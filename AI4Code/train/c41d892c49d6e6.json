{"cell_type":{"20f4d711":"code","35d2f573":"code","3cb7c503":"code","b51e4216":"code","aab01e3d":"code","ba5295b1":"code","5ae3703f":"code","827cd89c":"code","8169b5fe":"code","31172a76":"code","54ecc4fb":"code","29e7915f":"code","ea19c088":"code","4b30d4c0":"code","815c0283":"code","615fbe9f":"code","9d7efb87":"code","285e83cd":"code","065d7679":"code","714fb29d":"code","6bd10188":"code","66f0b2c6":"code","b4b1650d":"code","0d4c037d":"code","f2978b61":"code","b809bd57":"code","187daa29":"code","202b1bc0":"code","380bd034":"code","3b22c1c8":"code","04a2a13f":"code","4a73c0c3":"code","dab281b2":"code","b22bce79":"code","a41ff0d4":"code","846a6510":"code","773b6684":"code","7d558d22":"code","3dc4ba48":"code","e988948a":"code","b4795b3f":"markdown","69941d82":"markdown","fcb317aa":"markdown","3099bd5a":"markdown","b808aa6d":"markdown","7a62e3f2":"markdown","d378dfc8":"markdown","ad4f87ec":"markdown","b546342d":"markdown","90aa4822":"markdown","bde0d450":"markdown","5941f2e9":"markdown","0b847556":"markdown","103555e2":"markdown","8e645b19":"markdown","26f13e85":"markdown","2182aae1":"markdown","e69cd9da":"markdown","592749c4":"markdown","4badd529":"markdown"},"source":{"20f4d711":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","35d2f573":"from sklearn import datasets\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('ggplot')\niris=datasets.load_iris()","3cb7c503":"type(iris)","b51e4216":"print(iris.keys())","aab01e3d":"type(iris.data),type(iris.target)","ba5295b1":"iris.data.shape","5ae3703f":"print(iris.target_names)\nprint(iris.feature_names)","827cd89c":"x=iris.data\ny=iris.target","8169b5fe":"df= pd.DataFrame(x,columns=iris.feature_names)","31172a76":"df.head()","54ecc4fb":"_=pd.scatter_matrix(df,c=y,figsize=[8,8],s=150,marker='D')","29e7915f":"_=pd.plotting.scatter_matrix(df,c=y,figsize=[8,8],s=150,marker='D')","ea19c088":"plt.figure()\nsns.countplot(x='education', hue='party', data=df, palette='RdBu')\nplt.xticks([0,1], ['No', 'Yes'])\nplt.show()","4b30d4c0":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=6)\nknn.fit(iris['data'],iris['target'])\n","815c0283":"# Import KNeighborsClassifier from sklearn.neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Create arrays for the features and the response variable\ny = df['party'].values\nX = df.drop('party', axis=1).values\n\n# Create a k-NN classifier with 6 neighbors\nknn = KNeighborsClassifier(n_neighbors=6)\n\n# Fit the classifier to the data\nknn.fit(X,y)\n\n#using the model to predict values\nnew_prediction = knn.predict(X_new)\nprint(\"Prediction: {}\".format(new_prediction))\n","615fbe9f":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,Y_train,Y_test = \ntrain_test_split(Data, targetvariable, test_size=0.3, random_state = 21, stratify = y)\n","9d7efb87":"# Import necessary modules\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.model_selection import train_test_split\n\n# Create feature and target arrays\nX = digits.data\ny = digits.target\n\n# Split into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)\n\n# Create a k-NN classifier with 7 neighbors: knn\nknn = KNeighborsClassifier(n_neighbors = 7)\n\n# Fit the classifier to the training data\nknn.fit(X_train,y_train)\n\n# Print the accuracy\nprint(knn.score(X_test, y_test))\n","285e83cd":"# Setup arrays to store train and test accuracies\nneighbors = np.arange(1, 9)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n# Loop over different values of k\nfor i, k in enumerate(neighbors):\n    # Setup a k-NN Classifier with k neighbors: knn\n    knn = KNeighborsClassifier(n_neighbors=k)\n\n    # Fit the classifier to the training data\n    knn.fit(X_train,y_train)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n\n    #Compute accuracy on the testing set\n    test_accuracy[i] = knn.score(X_test, y_test)\n\n# Generate plot\nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()\n","065d7679":"from sklearn import datasets\nimport numpy as np\nimport pandas as pd\nboston = datasets.load_boston()\n","714fb29d":"print(type(boston))\nprint(boston.keys())","6bd10188":"print(boston.feature_names)","66f0b2c6":"boston_df = pd.DataFrame(boston.data,columns=boston.feature_names)","b4b1650d":"boston_df.head()","0d4c037d":"X=boston_df.values\ny=boston.target","f2978b61":"print(X.shape)\nprint(type(X))\nprint(y.shape)\nprint(type(y))","b809bd57":"plt.scatter(X[:,5],y)\nplt.ylabel('Value of house \/ 1000($)')\nplt.xlabel('Number of rooms')\nplt.margins(0.2)\nplt.show()","187daa29":"from sklearn import linear_model\nreg = linear_model.LinearRegression()\nreg.fit(X[:,5].reshape(-1,1),y.reshape(-1,1))\nprediction_space = np.linspace(min(X[:,5]),max(X[:,5])).reshape(-1,1)\nplt.scatter(X[:,5],y,color='blue')\nplt.plot(prediction_space,reg.predict(prediction_space),color='black',linewidth=3)\nplt.show()\n\nprint(reg.score(X[:,5].reshape(-1,1),y.reshape(-1,1)))","202b1bc0":"sns.heatmap(boston_df.corr(), square=True, cmap='RdYlGn')\nplt.show()","380bd034":"# Import necessary modules\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state=42)\n\n# Create the regressor: reg_all\nreg_all = LinearRegression()\n\n# Fit the regressor to the training data\nreg_all.fit(X_train,y_train)\n\n# Predict on the test data: y_pred\ny_pred = reg_all.predict(X_test)\n\n# Compute and print R^2 and RMSE\nprint(\"R^2: {}\".format(reg_all.score(X_test, y_test)))\nrmse = np.sqrt(mean_squared_error(y_test,y_pred))\nprint(\"Root Mean Squared Error: {}\".format(rmse))\n","3b22c1c8":"# Import the necessary modules\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\n# Create a linear regression object: reg\nreg = LinearRegression()\n\n# Compute 5-fold cross-validation scores: cv_scores\ncv_scores = cross_val_score(reg,X,y,cv=5)\n\n# Print the 5-fold cross-validation scores\nprint(cv_scores)\n\nprint(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)))","04a2a13f":"# Import Lasso\nfrom sklearn.linear_model import Lasso\n\n# Instantiate a lasso regressor: lasso\nlasso = Lasso(alpha=0.4,normalize=True)\n\n# Fit the regressor to the data\nlasso.fit(X,y)\n\n# Compute and print the coefficients\nlasso_coef = lasso.coef_\nprint(lasso_coef)\n\n# Plot the coefficients\nplt.plot(range(len(df_columns)), lasso_coef)\nplt.xticks(range(len(df_columns)), df_columns.values, rotation=60)\nplt.margins(0.02)\nplt.show()\n","4a73c0c3":"def display_plot(cv_scores, cv_scores_std):\n    fig = plt.figure()\n    ax = fig.add_subplot(1,1,1)\n    ax.plot(alpha_space, cv_scores)\n\n    std_error = cv_scores_std \/ np.sqrt(10)\n\n    ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\n    ax.set_ylabel('CV Score +\/- Std Error')\n    ax.set_xlabel('Alpha')\n    ax.axhline(np.max(cv_scores), linestyle='--', color='.5')\n    ax.set_xlim([alpha_space[0], alpha_space[-1]])\n    ax.set_xscale('log')\n    plt.show()","dab281b2":"# Import necessary modules\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\n\n# Setup the array of alphas and lists to store scores\nalpha_space = np.logspace(-4, 0, 50)\nridge_scores = []\nridge_scores_std = []\n\n# Create a ridge regressor: ridge\nridge = Ridge(normalize=True)\n\n# Compute scores over range of alphas\nfor alpha in alpha_space:\n\n    # Specify the alpha value to use: ridge.alpha\n    ridge.alpha = alpha\n    ridge.fit(X,y)\n    # Perform 10-fold CV: ridge_cv_scores\n    ridge_cv_scores = cross_val_score(ridge,X,y,cv=10)\n    \n    # Append the mean of ridge_cv_scores to ridge_scores\n    ridge_scores.append(np.mean(ridge_cv_scores))\n    \n    # Append the std of ridge_cv_scores to ridge_scores_std\n    ridge_scores_std.append(np.std(ridge_cv_scores))\n\n# Display the plot\ndisplay_plot(ridge_scores, ridge_scores_std)\n","b22bce79":"# Import necessary modules\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report \n\n# Create training and test set\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.4,random_state=42)\n\n# Instantiate a k-NN classifier: knn\nknn = KNeighborsClassifier(n_neighbors=6)\n\n# Fit the classifier to the training data\nknn.fit(X_train,y_train)\n\n# Predict the labels of the test data: y_pred\ny_pred = knn.predict(X_test)\n\n# Generate the confusion matrix and classification report\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n","a41ff0d4":"# Import the necessary modules\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)\n\n# Create the classifier: logreg\nlogreg = LogisticRegression()\n\n# Fit the classifier to the training data\nlogreg.fit(X_train,y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred = logreg.predict(X_test)\n\n# Compute and print the confusion matrix and classification report\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n","846a6510":"# Import necessary modules\nfrom sklearn.metrics import roc_curve\n\n# Compute predicted probabilities: y_pred_prob\ny_pred_prob = logreg.predict_proba(X_test)[:,1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()","773b6684":"# Import necessary modules\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score\n\n# Compute predicted probabilities: y_pred_prob\ny_pred_prob = logreg.predict_proba(X_test)[:,1]\n\n# Compute and print AUC score\nprint(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\n\n# Compute cross-validated AUC scores: cv_auc\ncv_auc = cross_val_score(logreg,X,y,cv=5,scoring='roc_auc')\n\n# Print list of AUC scores\nprint(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))\n","7d558d22":"# Import necessary modules\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\n# Setup the hyperparameter grid\nc_space = np.logspace(-5, 8, 15)\nparam_grid = {'C': c_space}\n\n# Instantiate a logistic regression classifier: logreg\nlogreg = LogisticRegression()\n\n# Instantiate the GridSearchCV object: logreg_cv\nlogreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n\n# Fit it to the data\nlogreg_cv.fit(X,y)\n\n# Print the tuned parameters and score\nprint(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \nprint(\"Best score is {}\".format(logreg_cv.best_score_))\n","3dc4ba48":"# Import necessary modules\nfrom scipy.stats import randint\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {\"max_depth\": [3, None],\n              \"max_features\": randint(1, 9),\n              \"min_samples_leaf\": randint(1, 9),\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# Instantiate a Decision Tree classifier: tree\ntree = DecisionTreeClassifier()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\ntree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n\n# Fit it to the data\ntree_cv.fit(X,y)\n\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\nprint(\"Best score is {}\".format(tree_cv.best_score_))\n","e988948a":"# Import necessary modules\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\n\n# Setup the pipeline steps: steps\nsteps = [('imputation', Imputer(missing_values='NaN', strategy='most_frequent', axis=0)),\n        ('SVM', SVC())]\n\n# Create the pipeline: pipeline\npipeline = Pipeline(steps)\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n\n# Fit the pipeline to the train set\npipeline.fit(X_train,y_train)\n\n# Predict the labels of the test set\ny_pred = pipeline.predict(X_test)\n\n# Compute metrics\nprint(classification_report(y_test, y_pred))\n","b4795b3f":"Measuring model performance\n\ncalculation of accuracy\n\nsplit data in train and validation set\n\nX_train,X_test,Y_train,Y_test = train_test_split(Data, targetvariable, test_size=0.3, random_state = 21, stratify = y)\n\n random_state --> seed for test train random data generation\n test_size -->\n stratify --> list or array containing the labels\n \n knn.score(X_test,Y_test)","69941d82":"GridSearchCV can be computationally expensive, especially if you are searching over a large hyperparameter space and dealing with multiple hyperparameters. A solution to this is to use **RandomizedSearchCV**\n\nNote that RandomizedSearchCV will never outperform GridSearchCV. Instead, it is valuable because it saves on computation time.\n\n","fcb317aa":"**Constructing heatmap to explore data and see correlation **","3099bd5a":"**Regression Analysis**\n\nusing built in datasets in sklearn package\n\nWe will import the boston dataset and will run regression on the data to predict target variable","b808aa6d":"Overfitting and underfitting","7a62e3f2":"linear regression --> chossing parameters\n\nridge\/lasso regression --> choosing alpha\n\nk-nearest neighbours --> choosing n_neighbours\n\nall these parameters are called hyperparameters \nchooing correct hyperparameters\n* try bunch of different parameters\n* fit them all separately\n* see how well each performs\n* choose the best one\n\n#Hyperparameter tuning\n\n\nUse Gridsearch CV \n\n* from sklearn.model_selection import GridSearchCV\n* param_grid = {}\n* knn = KNeighborClassifier()\n* knn_cv = GridSearchCV(knn, param_grid, cv=5)\n* knn_cv.fit(X,y)\n* knn_cv.best_params_\n* knn_cv.best_score_\n\n","d378dfc8":"Regularized Regression\n\nlinear regression --> minimize the loss function\nregularized regression --> \n    ridge regression\n    lasso function\n\nexample\n\n**Ridge**\n* from sklearn.linear_model import Ridge\n* X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=, random_state=)\n* ridge = Ridge(alpha=0.1, normalize=True)\n* ridge.fit(X_train, y_train)\n* ridge_pred = ridge.predict(X_test)\n* ridge.score(X_test,y_test)\n\n\n**Lasso**\n* from sklearn.linear_model import Lasso\n* X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=, random_state=)\n* lasso = Lasso(alpha=0.1, normalize=True)\n* lasso.fit(X_train, y_train)\n* lasso_pred = ridge.predict(X_test)\n* lasso.score(X_test,y_test)\n* \n* names= boston.drop('MEDV', axis=1).columns\n* lasso = Lasso(alpha=0.1)\n* lasso_coef = lasso.fit(X,y).coef_\n* _=plt.plot(range(len(names)),lasso_coef)\n* _=plt.xticks(range(len(names)),names,rotation=60)\n* _=plt.ylabel('Coefficients')\n* plt.show()\n\n\n","ad4f87ec":"%timeit cross_val_score(reg, X, y, cv = ____)  #run to get time","b546342d":"Larger the area under ROC curve better the model.\nknown as AUC (area under the curve)\n\nfrom sklearn.metrics import roc_auc_score\n\ny_pred_prob = logreg.predict(X_test)[:,1]\nroc_auc_score(y_test, y_pred_prob)\n\n* AUC using cross validation\n\nfrom sklearn.metrics import cross_val_score\ncv_scores = cv_val_score(logreg, X, y, cv=5, scoring = 'roc_auc')\n\n","90aa4822":"**Fine Tuning your model**\n\nClassification : generally ***accuracy*** is used\n\n\n**confusion matrix**\n1. Accuracy = tp + tn \/ (tp +tn +fp +fn)\n2. Precision =  tp \/ (tp + fp)\n3. Recall = tp \/ (tp + fn)\n4. F1 score =  \n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nprint( confusion_matrix( y_test, y_pred ) )\nprint( classification_report( y_test, y_pred ) )\n\n","bde0d450":"Fitting a regression model\n* from sklearn import linear_model\n* reg = linear_model.LinearRegression()\n* reg.fit(X_room,y)","5941f2e9":"**Train\/Test Split + Fit\/Predict\/Accuracy**","0b847556":"EDA in python\n* .head()\n* .info()\n* .describe()","103555e2":"Centring and scaling data\n\n\ndf.describe()\n\nNormalizing the data\n\nsubtract mean and divide by variance\n\nsubtract minimum and divide by range\n\nfrom sklearn.preprocessing import scale\n\nX_scaled = scale(X)\n\nScaling in a pipeline\nfrom sklearn.preprocessing import StandardScalar\n\n","8e645b19":"Classification\n* .fit()\n* .predict()","26f13e85":"**Cross Validation**\n1. split dataset info multiple folds (multiple subsets)\n2. keeping one fold as test data, run model on remaining folds.\n3. We will repeat this process multiple times, by keeping different folds aside and running the model on the remaining folds\n4. compute the metric of interest in this iterative process\n\nK-fold Cross Validation\n\nadvt:  removes the dependency of calculated metric data on test and validation set.\n\nfrom sklearn.model_selection import cross_val_score\nreg = linear_model.LinearRegression()\ncv_results = cross_val_score(reg, X, y, cv=5)\n","2182aae1":"**Logistic Regression**\n\nfrom sklearn.linear_model import LogisticRegression\n\nROC (Receiver operator characteristic) curve\n\nfrom sklearn.metrics import roc_curve\ny_pred_prob = logreg.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test,y_pred_prob)  fpr - false positive rate, tpr - true positive rate\n\n","e69cd9da":"***Lasso is great for feature selection, but when building regression models, Ridge regression should be your first choice.***","592749c4":"Holdout sample from the dataset\n\nThe idea is to tune the model's hyperparameters on the training set, and then evaluate its performance on the hold-out set which it has never seen before.\n\nscikit-learn :  OneHotEncoder()\n\npandas : get_dummies()\n\nImputing missing data\n\nfrom sklean.preprocessing import Imputer\n\nimp = Imputer(missing_values='NaN', strategy = 'mean', axis=0)\nimp.fit(X)\nX = imp.transform(X)\n\nImputing within a pipeline\n\nfrom \n\n\n","4badd529":"Ordinary least squares (OLS) :  Minimize the sum of squares of residuals"}}