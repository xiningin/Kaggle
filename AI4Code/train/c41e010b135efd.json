{"cell_type":{"baf14bd2":"code","7e8f8ad6":"code","75974624":"code","1984f6d6":"code","ffb69a2a":"code","50d2750c":"code","84ebdfa3":"code","9ef3bdd7":"code","5d8f6238":"code","4014b54b":"code","d11317d0":"code","26c1cf21":"code","d0a166aa":"code","68bacbae":"code","5ab0cff5":"code","540580ba":"code","2773b471":"code","50f704bc":"code","0af5d585":"code","0a6e206d":"code","7f160cb5":"code","e663ec3c":"code","7a317a08":"code","f9023d36":"code","0f298b8d":"code","9b4d3e2d":"code","cb1c3df9":"code","4023cb9c":"code","121b4873":"markdown","e1e55c73":"markdown","b6a9efc4":"markdown","49714f56":"markdown","86fb3473":"markdown","ef20cc5c":"markdown","2d304092":"markdown","0bf15623":"markdown","ffcba1bb":"markdown","00de1fa2":"markdown","eff80622":"markdown","36578550":"markdown","6714f1e2":"markdown","1f11de15":"markdown","0004d28c":"markdown","36af5a0c":"markdown","6ef4f7df":"markdown","747c2a67":"markdown","bf7db22c":"markdown","da756cfa":"markdown"},"source":{"baf14bd2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7e8f8ad6":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport spacy","75974624":"nlp = spacy.load('en_core_web_lg')\n\nreview_data = pd.read_csv('\/kaggle\/input\/medium-articles\/articles.csv')\nreview_data.head()","1984f6d6":"review_data.size","ffb69a2a":"data = pd.read_json('\/kaggle\/input\/medium-articles-as-json\/articles.json')\ndata.head()","50d2750c":"menu = [\"machine learning\", \"ML\"]","84ebdfa3":"from spacy.matcher import PhraseMatcher\n\nindex_of_review_to_test_on = 3\ntext_to_test_on = data.text.iloc[index_of_review_to_test_on]\n\n##print(text_to_test_on)\n\n# Load the SpaCy model\nnlp = spacy.blank('en')\n\n# Create the tokenized version of text_to_test_on\nreview_doc = nlp(text_to_test_on)\n\n# Create the PhraseMatcher object. The tokenizer is the first argument. Use attr = 'LOWER' to make consistent capitalization\nmatcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n\n# Create a list of tokens for each item in the menu\nmenu_tokens_list = [nlp(item) for item in menu]\n\n#print(menu_tokens_list)\n\n# Add the item patterns to the matcher. \n# Look at https:\/\/spacy.io\/api\/phrasematcher#add in the docs for help with this step\n# Then uncomment the lines below \n\n# \nmatcher.add(\"MENU\",            # Just a name for the set of rules we're matching to\n            None,              # Special actions to take on matched words\n            *menu_tokens_list  \n            )\n\n# Find matches in the review_doc\nmatches = matcher(review_doc)\nprint(matches)\n","9ef3bdd7":"for match in matches:\n    print(f\"Token number {match[2]}: {review_doc[match[1]:match[2]]}\")","5d8f6238":"nlp = spacy.load('en')\ndoc = nlp(data.text.iloc[4])","4014b54b":"print(f\"Token \\t\\tLemma \\t\\tStopword\".format('Token', 'Lemma', 'Stopword'))\nprint(\"-\"*40)\nfor token in doc[250:300]:\n    print(f\"{str(token)}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")","d11317d0":"nlp = spacy.load('en_core_web_lg')\narticles = pd.read_csv(\"\/kaggle\/input\/medium-articles\/articles.csv\")","26c1cf21":"def explain_text_entities(text):\n    doc = nlp(text)\n    for ent in doc.ents:\n        print(f'Entity: {ent}, Label: {ent.label_}, {spacy.explain(ent.label_)}')","d0a166aa":"explain_text_entities(articles['text'][9])","68bacbae":"one_sentence = articles['text'][0]\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent',jupyter=True)","5ab0cff5":"def redact_names(text):\n    doc = nlp(text)\n    redacted_sentence = []\n    for ent in doc.ents:\n        ent.merge()\n    for token in doc:\n        if token.ent_type_ == \"PERSON\":\n            redacted_sentence.append(\"[REDACTED_PERSON]\")\n        else:\n            redacted_sentence.append(token.string)\n    return \"\".join(redacted_sentence)","540580ba":"print(\"**Before**\")\none_sentence = articles['text'][0]\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent',jupyter=True)\nprint(\"**After**\")\none_sentence = redact_names(articles['text'][0])\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent',jupyter=True)\n","2773b471":"example_text = articles['text'][9]\ndoc = nlp(example_text)\nspacy.displacy.render(doc, style='ent', jupyter=True)\n\nfor idx, sentence in enumerate(doc.sents):\n    for noun in sentence.noun_chunks:\n        print(f\"sentence {idx+1} has noun chunk '{noun}'\")","50f704bc":"one_sentence = articles['text'][300]\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent', jupyter=True)\n\nfor token in doc:\n    print(token, token.pos_)","0af5d585":"text = articles['text'].str.cat(sep=' ')\n# spaCy enforces a max limit of 1000000 characters for NER and similar use cases.\n# Since `text` might be longer than that, we will slice it off here\nmax_length = 1000000-1\ntext = text[:max_length]\n\n# removing URLs and '&amp' substrings using regex\nimport re\nurl_reg  = r'[a-z]*[:.]+\\S+'\ntext   = re.sub(url_reg, '', text)\nnoise_reg = r'\\&amp'\ntext   = re.sub(noise_reg, '', text)","0a6e206d":"doc = nlp(text)\nitems_of_interest = list(doc.noun_chunks)\n# each element in this list is spaCy's inbuilt `Span`, which is not useful for us\nitems_of_interest = [str(x) for x in items_of_interest]\n# so we've converted it to string","7f160cb5":"import seaborn as sns\ndf_nouns = pd.DataFrame(items_of_interest, columns=[\"What is it about\"])\nplt.figure(figsize=(7,8))\nsns.countplot(y=\"What is it about\",\n             data=df_nouns,\n             order=df_nouns[\"What is it about\"].value_counts().iloc[:10].index)\nplt.show()\n","e663ec3c":"trump_topics = []\nfor token in doc:\n    if (not token.is_stop) and (token.pos_ == \"NOUN\") and (len(str(token))>2):\n        trump_topics.append(token)\n        \ntrump_topics = [str(x) for x in trump_topics]","7a317a08":"df_nouns = pd.DataFrame(trump_topics, columns=[\"Article deals with\"])\ndf_nouns\nplt.figure(figsize=(7,8))\nsns.countplot(y=\"Article deals with\",\n             data=df_nouns,\n             order=df_nouns[\"Article deals with\"].value_counts().iloc[:10].index)\nplt.show()","f9023d36":"trump_topics = []\nfor ent in doc.ents:\n    if ent.label_ not in [\"PERCENT\", \"CARDINAL\", \"DATE\"]:\n#         print(ent.text,ent.label_)\n        trump_topics.append(ent.text.strip())","0f298b8d":"df_ttopics = pd.DataFrame(trump_topics, columns=[\"Deals with\"])\nplt.figure(figsize=(7,8))\nsns.countplot(y=\"Deals with\",\n             data=df_ttopics,\n             order=df_ttopics[\"Deals with\"].value_counts().iloc[1:11].index)\nplt.show()\n# from collections import Counter\n# item_counter = Counter(items_of_interest)\n# item_counter.most_common()","9b4d3e2d":"from spacy.lang.en.stop_words import STOP_WORDS\nfrom wordcloud import WordCloud\nplt.figure(figsize=(10,5))\nwordcloud = WordCloud(background_color=\"white\",\n                      stopwords = STOP_WORDS,\n                      max_words=45,\n                      max_font_size=30,\n                      random_state=42\n                     ).generate(str(trump_topics))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","cb1c3df9":"from spacy.matcher import Matcher\ndoc = nlp(text)\nmatcher = Matcher(nlp.vocab)\nmatched_sents = [] # collect data of matched sentences to be visualized\n\ndef collect_sents(matcher, doc, i, matches, label='MATCH'):\n    \"\"\"\n    Function to help reformat data for displacy visualization\n    \"\"\"\n    match_id, start, end = matches[i]\n    span = doc[start : end]  # matched span\n    sent = span.sent  # sentence containing matched span\n    \n    # append mock entity for match in displaCy style to matched_sents\n    \n    if doc.vocab.strings[match_id] == 'TENSORFLOW':  # don't forget to get string!\n        match_ents = [{'start': span.start_char - sent.start_char,\n                   'end': span.end_char - sent.start_char,\n                   'label': 'TENSORFLOW'}]\n        matched_sents.append({'text': sent.text, 'ents': match_ents })\n    elif doc.vocab.strings[match_id] == 'MACHINE LEARNING':  # don't forget to get string!\n        match_ents = [{'start': span.start_char - sent.start_char,\n               'end': span.end_char - sent.start_char,\n               'label': 'MACHINE LEARNING'}]\n        matched_sents.append({'text': sent.text, 'ents': match_ents })\n    elif doc.vocab.strings[match_id] == 'WE':  # don't forget to get string!\n        match_ents = [{'start': span.start_char - sent.start_char,\n               'end': span.end_char - sent.start_char,\n               'label': 'WE'}]\n        matched_sents.append({'text': sent.text, 'ents': match_ents })\n    \n# declare different patterns\nrussia_pattern = [{'LOWER': 'MACHINE LEARNING'}, {'LEMMA': 'be'}, {'POS': 'ADV', 'OP': '*'},\n           {'POS': 'ADJ'}]\ndemocrats_pattern = [{'LOWER': 'TENSORFLOW'}, {'LEMMA': 'be'}, {'POS': 'ADV', 'OP': '*'},\n           {'POS': 'ADJ'}]\ni_pattern = [{'LOWER': 'WE'}, {'LEMMA': 'be'}, {'POS': 'ADV', 'OP': '*'},\n           {'POS': 'ADJ'}]\n\nmatcher.add('TENSORFLOW', collect_sents, democrats_pattern)  # add pattern\nmatcher.add('MACHINE LEARNING', collect_sents, russia_pattern)  # add pattern\nmatcher.add('WE', collect_sents, i_pattern)  # add pattern\nmatches = matcher(doc)\n\nspacy.displacy.render(matched_sents, style='ent', manual=True, jupyter=True,  options = {'colors': {'WE': '#6290c8', 'MACHINE LEARNING': '#cc2936', 'TENSORFLOW':'#f2cd5d'}})\n","4023cb9c":"print(matched_sents[:3])","121b4873":"Matching sentences","e1e55c73":"# **3. Named Entity Recognizer**\nWe are going to extract the entities and look at their explanations.","b6a9efc4":"# **2. Stop words and Lemma**\nWe are going to extract all the stop words and lemma (root words, example: walking -> walk) from the text.","49714f56":"As we can see, there are various useful entities like Date, Org, Cardinal etc. This will help us to extract a particular entity from long text or essays.","86fb3473":"# **1. Phrase Matching**\nWe are going to look for matches of certain terms using Spacy's PhraseMatcher.","ef20cc5c":"Let us extract all the matches of 'Machine Learning' and 'ML' in a particular article.","2d304092":"List of matched phrase with text id, start and end position in the text. ","0bf15623":"This gives us some idea that the author has talked more about data in this article.","ffcba1bb":"# **5. Parts of Speech Tagging : Noun chunks**\nLike entities, we can extract noun chunks from our text.","00de1fa2":"Now let us visualize the items of interest from our text with few lines of code. ","eff80622":"Similary, we can remove urls and other stop words from our text using prebuilt regular expressions.","36578550":"You can see the redacted words in this particular article.","6714f1e2":"# **4. Replacing particular entities**\nSpacy provides us provision to replace a particular type of entity throughout the text. For example, to hide confidential information or identity of people. ","1f11de15":"Now let us try out a fancy way of representing most used terms. This could be the gist or synopsis of this article.","0004d28c":"Converting the .csv file to Json file for convenience. ","36af5a0c":"As we could see, this visualization isnt very insightful. Let us try it differently.","6ef4f7df":"Pretty cool right :)","747c2a67":"This is pretty insightful, as we omit person, cardinal and date entities, we get more focus on organizations. This shows that the author has spoke more about Google, CNN and github. ","bf7db22c":"Let us do some text analytics using Spacy on our Medium articles dataset. ","da756cfa":"As you can see, we can get nouns, adjectives, adverbs, determiners, pronouns, prepositions, punctuations and so on from our text"}}