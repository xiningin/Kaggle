{"cell_type":{"9f6d2bf0":"code","3a3d5fd3":"code","6e65ba1b":"code","c0933dcb":"code","20a1e508":"code","6c6d14a9":"code","7217c058":"code","58066573":"code","3a0a8afb":"code","6ad62e1c":"code","9e636284":"code","c622b0fd":"code","d57d506e":"code","34c91fab":"code","7ed6e8eb":"code","dff6338c":"code","ef94d49c":"code","a923a622":"code","39ed4b9c":"markdown","8794acc4":"markdown","c0e181ea":"markdown","25c05408":"markdown","8c3a17c5":"markdown","f4fd4a67":"markdown","b34a9efc":"markdown","93b7171f":"markdown","159d5117":"markdown"},"source":{"9f6d2bf0":"pip install keras==2.2.5","3a3d5fd3":"!pip install tensorflow==1.14.0","6e65ba1b":"import tensorflow as tf","c0933dcb":"tf.__version__","20a1e508":"import keras","6c6d14a9":"keras.__version__","7217c058":"\nfrom keras.models import *\nfrom keras.layers import *\n","58066573":"class AddBeta(Layer):\n    def __init__(self  , **kwargs):\n        super(AddBeta, self).__init__(**kwargs)\n        \n    def build(self, input_shape):\n        if self.built:\n            return\n        \n        self.beta = self.add_weight(name='beta', \n                                      shape= input_shape[1:] ,\n                                      initializer='zeros',\n                                      trainable=True)\n       \n        self.built = True\n        super(AddBeta, self).build(input_shape)  \n        \n    def call(self, x, training=None):\n        return tf.add(x, self.beta)\n\n\nclass G_Guass(Layer):\n    def __init__(self , **kwargs):\n        super(G_Guass, self).__init__(**kwargs)\n        \n    def wi(self, init, name):\n        if init == 1:\n            return self.add_weight(name='guess_'+name, \n                                      shape=(self.size,),\n                                      initializer='ones',\n                                      trainable=True)\n        elif init == 0:\n            return self.add_weight(name='guess_'+name, \n                                      shape=(self.size,),\n                                      initializer='zeros',\n                                      trainable=True)\n        else:\n            raise ValueError(\"Invalid argument '%d' provided for init in G_Gauss layer\" % init)\n\n\n    def build(self, input_shape):\n        # Create a trainable weight variable for this layer.\n        self.size = input_shape[0][-1]\n\n        init_values = [0., 1., 0., 0., 0., 0., 1., 0., 0., 0.]\n        self.a = [self.wi(v, 'a' + str(i + 1)) for i, v in enumerate(init_values)]\n        super(G_Guass , self).build(input_shape)  # Be sure to call this at the end\n\n    def call(self, x):\n        z_c, u = x \n\n        def compute(y):\n            return y[0] * tf.sigmoid(y[1] * u + y[2]) + y[3] * u + y[4]\n\n        mu = compute(self.a[:5])\n        v  = compute(self.a[5:])\n\n        z_est = (z_c - mu) * v + mu\n        return z_est\n    \n    def compute_output_shape(self, input_shape):\n        return (input_shape[0][0], self.size)","3a0a8afb":"def batch_normalization(batch, mean=None, var=None):\n    if mean is None or var is None:\n        mean, var = tf.nn.moments(batch, axes=[0])\n    return (batch - mean) \/ tf.sqrt(var + tf.constant(1e-10))\n\n\ndef add_noise( inputs , noise_std ):\n    return Lambda( lambda x: x + tf.random.normal(tf.shape(x)) * noise_std  )( inputs )","6ad62e1c":"def get_ladder_network_fc(layer_sizes=[784, 1000, 500, 250, 250, 250, 10], \n     noise_std=0.3,\n     denoising_cost=[1000.0, 10.0, 0.10, 0.10, 0.10, 0.10, 0.10]):\n\n    L = len(layer_sizes) - 1  # number of layers\n\n    inputs_l = Input((layer_sizes[0],))  \n    inputs_u = Input((layer_sizes[0],))  \n\n    fc_enc = [Dense(s, use_bias=False, kernel_initializer='glorot_normal') for s in layer_sizes[1:] ]\n    fc_dec = [Dense(s, use_bias=False, kernel_initializer='glorot_normal') for s in layer_sizes[:-1]]\n    betas  = [AddBeta() for l in range(L)]\n\n    def encoder(inputs, noise_std  ):\n        h = add_noise(inputs, noise_std)\n        all_z    = [None for _ in range( len(layer_sizes))]\n        all_z[0] = h\n        \n        for l in range(1, L+1):\n            z_pre = fc_enc[l-1](h)\n            z =     Lambda(batch_normalization)(z_pre) \n            z =     add_noise (z, noise_std)\n            \n            if l == L:\n                h = Activation('softmax')(betas[l-1](z))\n            else:\n                h = Activation('relu')(betas[l-1](z))\n                \n            all_z[l] = z\n\n        return h, all_z\n\n    y_c_l, _ = encoder(inputs_l, noise_std)\n    y_l, _   = encoder(inputs_l, 0.0)  \n\n    y_c_u, corr_z  = encoder(inputs_u , noise_std)\n    y_u,  clean_z  = encoder(inputs_u , 0.0)  \n\n    # Decoder\n    d_cost = []  # to store the denoising cost of all layers\n    for l in range(L, -1, -1):\n        z, z_c = clean_z[l], corr_z[l]\n        if l == L:\n            u = y_c_u\n        else:\n            u = fc_dec[l]( z_est ) \n        u = Lambda(batch_normalization)(u)\n        z_est  = G_Guass()([z_c, u])  \n        d_cost.append((tf.reduce_mean(tf.reduce_sum(tf.square(z_est - z), 1)) \/ layer_sizes[l]) * denoising_cost[l])\n\n    u_cost = tf.add_n(d_cost)\n\n    y_c_l = Lambda(lambda x: x[0])([y_c_l, y_l, y_c_u, y_u, u, z_est, z])\n\n    tr_m = Model([inputs_l, inputs_u], y_c_l)\n    tr_m.add_loss(u_cost)\n    tr_m.compile(keras.optimizers.Adam(lr=0.02 ), 'categorical_crossentropy', metrics=['accuracy'])\n\n    tr_m.metrics_names.append(\"den_loss\")\n    tr_m.metrics_tensors.append(u_cost)\n\n    te_m = Model(inputs_l, y_l)\n    tr_m.test_model = te_m\n\n    return tr_m","9e636284":"from __future__ import print_function\n\nfrom keras.datasets import mnist\nimport keras\nimport random\nfrom sklearn.metrics import accuracy_score\nimport numpy as np","c622b0fd":"# get the dataset\ninp_size = 28*28 # size of mnist dataset \nn_classes = 10\n(x_train, y_train), (x_test, y_test) = mnist.load_data()","d57d506e":"x_train = x_train.reshape(60000, inp_size).astype('float32')\/255\nx_test  = x_test.reshape(10000,  inp_size).astype('float32')\/255\n\ny_train = keras.utils.to_categorical(y_train, n_classes)\ny_test  = keras.utils.to_categorical(y_test,  n_classes)","34c91fab":"# only select 100 training samples \nidxs_annot = range(x_train.shape[0])\nrandom.seed(0)\nidxs_annot = np.random.choice(x_train.shape[0], 100)","7ed6e8eb":"x_train_unlabeled = x_train\nx_train_labeled   = x_train[idxs_annot]\ny_train_labeled   = y_train[idxs_annot]","dff6338c":"n_rep = x_train_unlabeled.shape[0] \/\/ x_train_labeled.shape[0]\nx_train_labeled_rep = np.concatenate([x_train_labeled]*n_rep)\ny_train_labeled_rep = np.concatenate([y_train_labeled]*n_rep)","ef94d49c":"# initialize the model \nmodel = get_ladder_network_fc(layer_sizes=[inp_size, 1000, 500, 250, 250, 250, n_classes])","a923a622":"# train the model for 100 epochs\n#for _ in range(100):\nmodel.fit([x_train_labeled_rep, x_train_unlabeled], y_train_labeled_rep, epochs=100)\ny_test_pr = model.test_model.predict(x_test, batch_size=100)\nprint(\"Test accuracy : %f\" % accuracy_score(y_test.argmax(-1), y_test_pr.argmax(-1)))","39ed4b9c":"This code cell will run for 100 epochs and it took more than 4 hours on colab GPU, so I have not executed this code cell again. It will give a test accuarcy of 96%. ","8794acc4":"**This code is implemented according to the algorithm mentioned in the research paper, [Semi-Supervised learning with Ladder Networks](https:\/\/arxiv.org\/abs\/1507.02672), on MNIST dataset**\n\nThe code requires,\nkeras version = 2.2.5\ntensorflow version = 1.14.0\n\n\nTest accuracy = 96%\n\nAlso available on [Github](https:\/\/github.com\/NANDINI-star\/Semi-Supervised-Learning-with-Ladder-Networ) ","c0e181ea":"![result.png](attachment:61233dfe-f04f-41d7-b400-32b24638cd6b.png)","25c05408":"### [GITHUB LINK](https:\/\/github.com\/NANDINI-star\/Semi-Supervised-Learning-with-Ladder-Networ) Check this out to see the run scripts version.","8c3a17c5":"![image.png](attachment:88bce858-56af-44d9-ad57-43608304f040.png)\n\n","f4fd4a67":"Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data).","b34a9efc":"![image.png](attachment:236697c9-ad6e-4862-92d1-84726128b593.png)","93b7171f":"![image.png](attachment:0a776e1a-cc13-426a-860c-d137c468bb5e.png)","159d5117":"# SEMI-SUPERVISED LEARNING WITH LADDER NETWORK"}}