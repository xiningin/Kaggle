{"cell_type":{"9ba25487":"code","6d732aba":"code","f63888ac":"code","7f5c40fe":"code","72f80131":"code","30aa3c49":"code","db7602a1":"code","6d1b1634":"code","6c19bdb1":"code","0b058860":"code","3d9d7b0a":"code","7528c994":"code","b6e4c300":"markdown","a74fe0a9":"markdown","0bb5e994":"markdown","7aa1e61c":"markdown","968d3f2c":"markdown","c97e453e":"markdown","d8574edf":"markdown","a7d64c62":"markdown","e58166d5":"markdown"},"source":{"9ba25487":"#Importing the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","6d732aba":"#Importing the dataset\ndataset=pd.read_csv(\"..\/input\/mall-customers\/Mall_Customers.csv\")\nprint (\"Data shape {}\".format(dataset.shape))","f63888ac":"# Sample data\ndataset.head()","7f5c40fe":"# Data type of each feature \ndataset.dtypes","72f80131":"#we will find and remove the duplicate entries in the datset\ndataset.drop_duplicates(inplace = True)\nprint (\"Data shape after dropping duplicates {}\".format(dataset.shape))","30aa3c49":"#Checking for any null entries column wise, We can see that there are 0 null entries\nprint (pd.DataFrame(dataset.isnull().sum()))    ","db7602a1":"plt.scatter(dataset['Annual Income (k$)'],dataset['Spending Score (1-100)'],color='blue')\nplt.title('Clusters of customers')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend(loc='best')\nplt.show()","6d1b1634":"# Feature data\nX=dataset.iloc[:,[3,4]].values\nprint(X.shape)","6c19bdb1":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX= sc_X.fit_transform(X)","0b058860":"# Using the Elbow method to find the number of clusters\n\n#n_clusters is no.of clusters in a range\n#k-means++ is an random initialization methods for centriods to avoid random intialization trap,\n#max_iter is max no of iterations defined when k-means is running\n#n_init is no of times k-means will run with different initial centroids\nfrom sklearn.cluster import KMeans\nwcss=[] #With in cluster sum of squers(Inertia)\nfor i in range(1,11):\n    kmeans=KMeans(n_clusters=i, init=\"k-means++\",max_iter=300, n_init=10, random_state=0)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1,11),wcss)\nplt.title(\"The Elbow Method\")\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"wcss\")\nplt.show()","3d9d7b0a":"# Applying k-mean to dataset\nkmeans=KMeans(n_clusters=5, init=\"k-means++\", max_iter=300, n_init=10, random_state=0)\n# Predcit cluster labels\ny_kmeans=kmeans.fit_predict(X) #kmeans.labels_\nprint(y_kmeans)\nprint (\"Unique no of Clusters {}\".format(np.unique(y_kmeans)))","7528c994":"# Visualising the clusters\nplt.figure(figsize = (12, 8))\nplt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = \"red\", label = \"Customer Type 1\")\nplt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = \"blue\", label = \"Customer Type 2\")\nplt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = \"green\", label = \"Customer Type 3\")\nplt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 100, c = \"cyan\", label = \"Customer Type 4\")\nplt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = 100, c = \"magenta\", label = \"Customer Type 5\")\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = \"orange\", label = \"Centroids\")\nplt.title(\"Clusters of clients\")\nplt.xlabel(\"Annual Incom (K$)\")\nplt.ylabel(\"Spending Score (1-100)\")\nplt.legend()","b6e4c300":"### Clustering-of-Customers Based on Customer Behavior:\n* We need to group the customers based on their spending score for a supermarket mall and through membership cards using the provided customer data like Customer ID, age, gender, annual income and spending score. Spending Score is determined based on parameters like customer behavior and purchasing data. \n* We use k-means clustering algorithm to group the customers.  We can target customers with high spending score for any offers or promotions as they are likely to spend more money on purchase.","a74fe0a9":"### K-Means Algorithm:\n* Based on the Elbow method results i.e, plot we can see that at cluster=5 distortion goes rapidly so n_clusters=5. We apply K-means to the dataset with 5 clusters.","0bb5e994":"#### Data Visualization:\nWe will group the data based on their Annual Income and Speding Score assuming these two features helpful in clustering the customers. We can see this from the below scatter plot also.","7aa1e61c":"### Results Analysis:\n* Based on the analysis above results we visualize the data of clusters assigned.","968d3f2c":"### Elbow criterion method to find the optimal number of clusters (Choosing K) :\n* The algorithm described above finds the clusters and data set labels for a particular pre-chosen K. \n* To find the number of clusters in the data,we need to run the K-means clustering algorithm for a range of K values and compare the results. In general, there is no method for determining exact value of K, but an accurate estimate can be obtained using the following techniques. \n* One of the metrics that is commonly used to compare results across different values of K is the mean distance between data points and their cluster centroid(WCSS - With in Cluster Sum of Squares).\n* Since increasing the number of clusters will always reduce the distance to data points, increasing K will always decrease this metric, to the extreme of reaching zero when K is the same as the number of data points. \n* Elbow - a point representing an \"optimal\" number of clusters\n* Thus, this metric cannot be used as the sole target. Instead, mean distance to the centroid as a function of K is plotted and the \"Elbow point,\" where the rate of decrease sharply shifts(where distortion goes rapidly), can be used to roughly determine K\n.","c97e453e":"### Data Preprocessing:\n* We will look for duplicate records and drop them. We can see below there are no duplicate records in the data.\n* We wil find any null values in the data and drop them if any found. There are no null values found in the dataset.","d8574edf":"Standardizing features by removing the mean and scaling to unit variance as K-means is distance based algorithm, standardizing helps in run the algorithm faster and find the clusters efficiently.","a7d64c62":"### K-Means Clustering Introduction:\n* K-means clustering is an unsupervised alogithm, which is used to group or cluster the data when you have unlabeled data (i.e., data without defined categories or labels). \n* The goal of this algorithm is to find groups(clusters) of customers in the data. We will find the number of groups(K) using Elbow method. \n* The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on feature similarity.","e58166d5":"### Conclusion:\n\nFrom the above results we have clustered customers into 5 groups. We compare customer group against each other and target cutomer groups which makes most business sense ie., Customer Type 1, Customer Type 2, Customer Type 4 for profits as their spending score is high.\n\nCustomer Type 1 : These customers like to purchase average number of products with good price(moderate spending score)\n\nCustomer Type 2 : These customers like to purchase few products with high price(high spending score)\n\nCustomer Type 3 : These customers like to purchase more products with high price(high spending score)\n\nCustomer Type 4 : These customers like to purchase few products with low price(low spending score)\n\nCustomer Type 5 : These customers like to purchase more products with low price(low spending score)"}}