{"cell_type":{"e434cb69":"code","18512f5a":"code","98a67321":"code","d2fe953e":"code","51e4bb6a":"code","c63f2f7f":"code","88fe4fe7":"code","1eb6b76c":"code","75a60e4c":"code","d076238a":"code","b493b4f8":"code","9d022c24":"code","e8c05471":"code","767a311c":"code","b0b6a68c":"code","d845202d":"markdown","b27f61be":"markdown"},"source":{"e434cb69":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","18512f5a":"df=pd.read_csv('..\/input\/nifty-indices-dataset\/INDIAVIX.csv')\ndf.head()","98a67321":"df.info()","d2fe953e":"#Some data cleaning\nm=df['Previous'].median()\ndf['Previous']=df['Previous'].fillna(m)","51e4bb6a":"df.head()","c63f2f7f":"df['Date']=pd.to_datetime(df['Date'],format='%Y-%m-%d')","88fe4fe7":"df.info()","1eb6b76c":"import seaborn as sns\nsns.scatterplot(df['Date'],df['Change'])\nsns.scatterplot(df['Date'],df['%Change'])\n#The below diagram tells me about the volatility of the indices during 2016, 2020 and 2021\n#Maybe because of covid-19","75a60e4c":"from sklearn import datasets\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom sklearn import neighbors,metrics\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\nfrom datetime import date","d076238a":"#x is date\n#y is closing price\n\n#Splitting up the data\nx=df['Date']\ny=df['Close']\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)","b493b4f8":"#Setting up the model\n\nmodel = svm.SVR()\n\n#Ensure to reshape your data(becos if you don't there will be an error)\n\nx_train=np.array(x_train)\nx_train=x_train.reshape(-1,1)\n\ny_train=np.array(y_train)\ny_train=y_train.reshape(-1,1)\n\ny_test=np.array(y_test)\ny_test=y_test.reshape(-1,1)\n\nx_test=np.array(x_test)\nx_test=x_test.reshape(-1,1)\n\nmodel.fit(x_train,y_train)\nprint(model)","9d022c24":"#Looking at accuracy\npredictions = model.predict(x_test)\naccuracy=explained_variance_score(y_test, predictions)\nprint(\"predictions: \",predictions)\nprint('actual: ',y_test)\nprint('accuracy: ',accuracy)","e8c05471":"#Let us now try to use random forest regressor\nfrom sklearn.ensemble import RandomForestRegressor\nregr = RandomForestRegressor(n_estimators=200,max_depth=3, random_state=0)\nregr.fit(x_train,y_train)","767a311c":"predictions=regr.predict(x_test)\naccuracy=explained_variance_score(y_test, predictions)","b0b6a68c":"print(f'predictions:{predictions}')\nprint(f'accuracy:{accuracy}')","d845202d":"### This goes to show that random forest does a better job in making VIX predictions","b27f61be":"* sns.scatterplot(df['Date'],df['Change'])\n* Decreasing trend of prices observed \n* There was a huge price spike shortly after 2020(Prob. because investors wanted to take advantage of volatility)\n* There is also a cyclical process happening to the dataset"}}