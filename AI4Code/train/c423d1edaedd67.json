{"cell_type":{"6235b6d2":"code","306beb9f":"code","48dfaa68":"code","67b80723":"code","0ca59318":"code","3b4463dc":"code","f995508e":"code","fffa6191":"code","e4824748":"code","073cf169":"code","f9da72aa":"code","00743cac":"code","341dfbdc":"markdown","d281f17f":"markdown","95c17bd8":"markdown"},"source":{"6235b6d2":"import warnings; warnings.filterwarnings('ignore')\nimport numpy as np,pandas as pd,pylab as pl,h5py\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom IPython import display\nfrom keras.preprocessing import image as ksimage\nfrom keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\nfrom keras.metrics import top_k_categorical_accuracy,categorical_accuracy\nfrom keras.models import Sequential,load_model,Model\nfrom keras.layers import Dense,LSTM,GlobalAveragePooling1D,GlobalAveragePooling2D\nfrom keras.layers.advanced_activations import PReLU,LeakyReLU\nfrom keras.layers import Input,Activation,Flatten,Dropout,BatchNormalization\nfrom keras.layers import Conv2D,MaxPooling2D,GlobalMaxPooling2D\nfw='weights.best.letters.hdf5'","306beb9f":"# plotting of fitting histories for neural networks\ndef history_plot2(fit_history):\n    keys=list(fit_history.history.keys())[6:]\n    pl.figure(figsize=(12,10)); pl.subplot(211)\n    pl.plot(fit_history.history[keys[0]],\n            color='slategray',label='valid 1')\n    pl.plot(fit_history.history[keys[1]],\n            color='#4876ff',label='valid 2')\n    pl.xlabel(\"Epochs\"); pl.ylabel(\"Loss\")\n    pl.legend(); pl.grid(); pl.title('Loss Function')     \n    pl.subplot(212)\n    pl.plot(fit_history.history[keys[2]],\n            color='slategray',label='valid 1')\n    pl.plot(fit_history.history[keys[3]],\n            color='#4876ff',label='valid 2')\n    pl.xlabel(\"Epochs\"); pl.ylabel(\"Accuracy\")    \n    pl.legend(); pl.grid(); pl.title('Accuracy'); pl.show()\n# preprocessing functions \ndef ohe(x): \n    return OneHotEncoder(categories='auto')\\\n           .fit(x.reshape(-1,1)).transform(x.reshape(-1,1))\\\n           .toarray().astype('int64')\ndef tts(X,y): \n    x_train,x_test,y_train,y_test=\\\n    train_test_split(X,y,test_size=.2,random_state=1)\n    n=int(len(x_test)\/2)\n    x_valid,y_valid=x_test[:n],y_test[:n]\n    x_test,y_test=x_test[n:],y_test[n:]\n    return x_train,x_valid,x_test,y_train,y_valid,y_test","48dfaa68":"f=h5py.File('..\/input\/LetterColorImages_123.h5','r')\nkeys=list(f.keys()); keys","67b80723":"# creating image arrays and targets\nletters=u'\u0430\u0431\u0432\u0433\u0434\u0435\u0451\u0436\u0437\u0438\u0439\u043a\u043b\u043c\u043d\u043e\u043f\u0440\u0441\u0442\u0443\u0444\u0445\u0446\u0447\u0448\u0449\u044a\u044b\u044c\u044d\u044e\u044f'\nbackgrounds=np.array(f[keys[0]])\nlabels=np.array(f[keys[2]])\n# normalization of image arrays\nimages=np.array(f[keys[1]])\/255","0ca59318":"pl.figure(figsize=(2,3)); il=10**4\npl.title('Label: %s \\n'%letters[labels[il]-1]+\\\n         'Background: %s'%backgrounds[il],\n         fontsize=18)\npl.imshow(images[il]); pl.show()","3b4463dc":"# one-hot encoding\ncbackgrounds,clabels=ohe(backgrounds),ohe(labels)\nctargets=np.concatenate((clabels,cbackgrounds),axis=1)\ndisplay.display(pd.DataFrame([labels[97:103],clabels[97:103]]).T)\npd.DataFrame([clabels.shape,cbackgrounds.shape,ctargets.shape])","f995508e":"# splitting the data\nx_train,x_valid,x_test,\\\ny_train,y_valid,y_test=tts(images,ctargets)","fffa6191":"y_train_list=[y_train[:,:33],y_train[:,33:]]\ny_test_list=[y_test[:,:33],y_test[:,33:]]\ny_valid_list=[y_valid[:,:33],y_valid[:,33:]]","e4824748":"def multi_model():   \n    model_input=Input(shape=(32,32,3))\n    x=BatchNormalization()(model_input)    \n    x=Conv2D(32,(5,5),padding='same')(model_input)\n    x=LeakyReLU(alpha=.02)(x)\n    x=MaxPooling2D(pool_size=(2,2))(x)    \n    x=Dropout(.25)(x)    \n    x=Conv2D(256,(5,5),padding='same')(x)  \n    x=LeakyReLU(alpha=.02)(x)\n    x=MaxPooling2D(pool_size=(2,2))(x)    \n    x=Dropout(.25)(x)              \n    x=GlobalMaxPooling2D()(x)   \n    x=Dense(1024)(x) \n    x=LeakyReLU(alpha=.02)(x)\n    x=Dropout(.25)(x)       \n    y1=Dense(33,activation='softmax')(x)\n    y2=Dense(4,activation='softmax')(x)    \n    model=Model(inputs=model_input,outputs=[y1,y2])\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='adam',metrics=['accuracy'])\n    return model\nmulti_model=multi_model()","073cf169":"multi_model.summary()","f9da72aa":"checkpointer=ModelCheckpoint(filepath=fw,verbose=2,save_best_only=True)\nlr_reduction=ReduceLROnPlateau(monitor='val_loss',patience=5,verbose=2,factor=.75)\nestopping=EarlyStopping(monitor='val_loss',patience=20,verbose=2)\nhistory=multi_model.fit(x_train,y_train_list,\n                        epochs=200,batch_size=64,verbose=2,\n                        validation_data=(x_valid,y_valid_list),\n                        callbacks=[checkpointer,lr_reduction,estopping])","00743cac":"history_plot2(history)\n# loading the model weights with the best validation accuracy\nmulti_model.load_weights(fw)\nmulti_model.evaluate(x_test,y_test_list)","341dfbdc":"# Step 0. Code Modules, Links and Helpful Functions\n#### [\ud83d\udcd1 GitHub Repository](https:\/\/github.com\/OlgaBelitskaya\/deep_learning_projects\/tree\/master\/DL_PP2) & [\ud83d\udcd1 Colaboratory](https:\/\/colab.research.google.com\/drive\/1Z9Fz0OOi6bpWvH-H2OhExC9CkGPWBYZz)\n#### [\ud83d\udcd1 Full Version - Python](https:\/\/olgabelitskaya.github.io\/kaggle_letters.html) & [\ud83d\udcd1 Full Version - R](https:\/\/olgabelitskaya.github.io\/kaggle_letters_R.html) \n#### [\ud83d\udcd1 Deep Learning. P2: Multi-Label Classification. Letter Recognition](https:\/\/olgabelitskaya.github.io\/DL_PP2_Solutions_SMC.html)","d281f17f":"# \u2712\ufe0f Step 1. Loading and Preprocessing the Data","95c17bd8":"# \u2712\ufe0f Step 2. Defining a Multi-Label Classification Model"}}