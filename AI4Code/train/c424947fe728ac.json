{"cell_type":{"b571792f":"code","1d977f41":"code","92799d32":"code","dc308b4e":"code","c238e105":"code","e95a65ad":"code","b6cc41c3":"code","8538e92e":"code","b8987622":"code","2009fee0":"code","9e136fc5":"code","d81b18e6":"code","1ee75e19":"code","7ff0469f":"code","bd8ec9f8":"code","766a8a14":"code","92e808fb":"code","a142ad13":"code","f5a3de12":"code","c6ddc543":"code","5928db2d":"code","cdd6f0c1":"code","394fea13":"code","f2b22be3":"code","6fffa9e1":"code","d7c9b034":"code","3371e1a6":"code","6b5485fe":"code","60e0cd38":"code","ecef642b":"code","5e8d2bcb":"code","81628be9":"code","d127728d":"code","1a61414c":"code","fe0ada3d":"code","d73c61e3":"code","ad99f459":"code","6ff1b919":"code","4979bcf2":"code","b992a638":"code","fae6feb4":"code","03e084c4":"code","3a992d0c":"code","6a9bcf16":"code","2ba67e76":"code","267fe869":"code","b28b391b":"code","e6b4fdc1":"code","0fa75e95":"code","4f34e52b":"code","c62657d2":"code","b22ad429":"code","a9f9c717":"code","c778ea10":"code","b0fd5299":"code","0ddcee2f":"code","2e501be7":"code","fc2d9708":"code","b9f4bef4":"code","1a95f8d4":"code","ea74d926":"code","5fd59100":"code","658dcbaa":"code","538a038d":"code","ee9700d4":"markdown","00413902":"markdown","3af02b2d":"markdown","aba36fd7":"markdown","1c9b075d":"markdown","c531b21d":"markdown","fff91a9a":"markdown","7cc9f046":"markdown","347bbcf1":"markdown","29d55394":"markdown","09c87fba":"markdown","2f42b3a3":"markdown","c89cfe53":"markdown","b6859a2e":"markdown","30fba539":"markdown","be77a68b":"markdown","3b4630c8":"markdown","e02a82ca":"markdown","37b58076":"markdown","d915b7fb":"markdown","b50d8a1d":"markdown","562087ac":"markdown","9264c439":"markdown","35a89983":"markdown","9cc5de37":"markdown","471204f3":"markdown","719ccde9":"markdown","5bd5ddd3":"markdown","5779f8cd":"markdown","722675bc":"markdown","1871eebd":"markdown","943f1458":"markdown","0adec2fc":"markdown","5ca28f5f":"markdown","cae59ab2":"markdown"},"source":{"b571792f":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting data\nimport seaborn as sns # EDA\nfrom IPython.display import display, Markdown as md\nimport gc","1d977f41":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_colwidth', 500)\npd.set_option('display.unicode.ambiguous_as_wide', True)","92799d32":"seed_value= 30\n\n# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\nimport os\nos.environ['PYTHONHASHSEED']=str(seed_value)\n\n# 2. Set the `python` built-in pseudo-random generator at a fixed value\nimport random\nrandom.seed(seed_value)\n\n# 3. Set the `numpy` pseudo-random generator at a fixed value\nimport numpy as np\nnp.random.seed(seed_value)","dc308b4e":"PATH_IN = '..\/input\/home-credit-default-risk'","c238e105":"train = pd.read_csv(f'{PATH_IN}\/application_train.csv', low_memory=False)\ntest = pd.read_csv(f'{PATH_IN}\/application_test.csv', low_memory=False)","e95a65ad":"train.head(10).transpose()","b6cc41c3":"sns.countplot(train['TARGET'], palette=\"bwr\")\nplt.show()","8538e92e":"categorical_feat = [\n    'NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY',\n    'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE','NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE',\n    'FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE', 'FLAG_PHONE', 'FLAG_EMAIL',\n    'OCCUPATION_TYPE', 'REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY',\n    'WEEKDAY_APPR_PROCESS_START', 'HOUR_APPR_PROCESS_START', \n    'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION', 'LIVE_REGION_NOT_WORK_REGION', \n    'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY', 'ORGANIZATION_TYPE',\n    'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE',\n    'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3',\n    'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6',\n    'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9',\n    'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12',\n    'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15',\n    'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18',\n    'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21',\n]","b8987622":"# for feat in categorical_feat:\n#     fig_dims = (30, 5)\n#     fig, ax = plt.subplots(1, 2, figsize=fig_dims)\n#     train_feat_values = train[feat].unique()\n#     sns.countplot(train[feat], order=train_feat_values, ax=ax[0]).set_title(f'{feat}_TRAIN')\n#     sns.countplot(test[feat], order=train_feat_values, ax=ax[1]).set_title(f'{feat}_TEST')\n#     plt.show()","2009fee0":"numerical_feat = [\n    'CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY',\n    'AMT_GOODS_PRICE', 'REGION_POPULATION_RELATIVE', 'DAYS_BIRTH',\n    'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH', 'OWN_CAR_AGE',\n    'CNT_FAM_MEMBERS',\n    'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', \n    'APARTMENTS_AVG', 'BASEMENTAREA_AVG',\n    'YEARS_BEGINEXPLUATATION_AVG', 'YEARS_BUILD_AVG', 'COMMONAREA_AVG',\n    'ELEVATORS_AVG', 'ENTRANCES_AVG', 'FLOORSMAX_AVG', 'FLOORSMIN_AVG',\n    'LANDAREA_AVG', 'LIVINGAPARTMENTS_AVG', 'LIVINGAREA_AVG',\n    'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAREA_AVG', 'APARTMENTS_MODE',\n    'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE',\n    'COMMONAREA_MODE', 'ELEVATORS_MODE', 'ENTRANCES_MODE', 'FLOORSMAX_MODE',\n    'FLOORSMIN_MODE', 'LANDAREA_MODE', 'LIVINGAPARTMENTS_MODE',\n    'LIVINGAREA_MODE', 'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE',\n    'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI',\n    'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI', 'ELEVATORS_MEDI',\n    'ENTRANCES_MEDI', 'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI', 'LANDAREA_MEDI',\n    'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI', 'NONLIVINGAPARTMENTS_MEDI',\n    'NONLIVINGAREA_MEDI', 'TOTALAREA_MODE', 'OBS_30_CNT_SOCIAL_CIRCLE',\n    'DEF_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE',\n    'DEF_60_CNT_SOCIAL_CIRCLE', 'DAYS_LAST_PHONE_CHANGE',\n    'AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY',\n    'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON',\n    'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR'\n]","9e136fc5":"# for feat in numerical_feat:\n#     fig_dims = (30, 5)\n#     fig, ax = plt.subplots(1, 2, figsize=fig_dims)\n#     sns.kdeplot(train[feat], ax=ax[0], bw=1, shade=True).set_title(f'{feat}_TRAIN')\n#     sns.kdeplot(test[feat], ax=ax[1], bw=1, shade=True).set_title(f'{feat}_TEST')\n#     plt.show()","d81b18e6":"X_train = train.drop(columns=['TARGET'])\ny = train['TARGET']\nX_test = test.copy()\nprint(X_train.shape)\nprint(X_test.shape)","1ee75e19":"train = None\ntest = None\ngc.collect()","7ff0469f":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","bd8ec9f8":"missing_values_table(X_train)","766a8a14":"def to_nan(df, value, columns):\n    for column in columns:\n        df.loc[df[column]==value, column] = np.NaN\n    return df","92e808fb":"to_nan(X_train, 'XNA', columns=['CODE_GENDER', 'ORGANIZATION_TYPE'])\nto_nan(X_test, 'XNA', columns=['CODE_GENDER', 'ORGANIZATION_TYPE'])\ngc.collect()","a142ad13":"# DAYS_EMPLOYED\nto_nan(X_train, 365243, columns=['DAYS_EMPLOYED'])\nto_nan(X_test, 365243, columns=['DAYS_EMPLOYED'])\nX_train['DAYS_EMPLOYED'].hist()\nplt.show()\ngc.collect()","f5a3de12":"def flag_nan(df, df_test=None):\n    if df_test is None:\n        for column in df.columns:\n            if df[column].isna().values.any():\n                df[f'{column}_NaN'] = df[column].isna().astype(int)\n        return df\n    \n    for column in df.columns:\n        if df[column].isna().values.any() or df_test[column].isna().values.any():\n            df[f'{column}_NaN'] = df[column].isna().astype(int)\n            df_test[f'{column}_NaN'] = df_test[column].isna().astype(int)\n            \n            ","c6ddc543":"flag_nan(X_train, X_test)\nX_train.columns","5928db2d":"from sklearn.impute import SimpleImputer\n\n# Filling NaN for Categorical features:\n\ncate_imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=X_train[categorical_feat].mode())\nX_train[categorical_feat] = cate_imp.fit_transform(X_train[categorical_feat])\nX_test[categorical_feat] = cate_imp.transform(X_test[categorical_feat])\n\n# Filling NaN for Numerical features:\n\nnum_imp = SimpleImputer(missing_values=np.nan, strategy='median')\nX_train[numerical_feat] = num_imp.fit_transform(X_train[numerical_feat])\nX_test[numerical_feat] = num_imp.transform(X_test[numerical_feat])\n\ngc.collect()","cdd6f0c1":"def cyclic_encode(df, feature_name, max_val, mapping=None, df_test=None):\n    if mapping is not None:\n        df[feature_name] = df[feature_name].apply(mapping)\n        if df_test is not None:\n            df_test[feature_name] = df_test[feature_name].apply(mapping)\n            \n    df[f'{feature_name}_Sin'] = np.sin(2 * np.pi * df[feature_name] \/ max_val)\n    df[f'{feature_name}_Cos'] = np.cos(2 * np.pi * df[feature_name] \/ max_val)\n    df.drop(columns=feature_name, inplace=True)\n    \n    if df_test is not None:\n        df_test[f'{feature_name}_Sin'] = np.sin(2 * np.pi * df_test[feature_name] \/ max_val)\n        df_test[f'{feature_name}_Cos'] = np.cos(2 * np.pi * df_test[feature_name] \/ max_val)\n        df_test.drop(columns=feature_name, inplace=True)\n        return df, df_test\n    \n    return df","394fea13":"# WEEKDAY_APPR_PROCESS_START\nweekday_encoder = {    \n    'MONDAY': 0,\n    'TUESDAY': 1,\n    'WEDNESDAY': 2,\n    'THURSDAY': 3,\n    'FRIDAY': 4,\n    'SATURDAY': 5,\n    'SUNDAY': 6\n}\n\nX_train['FLAG_WEEKEND_APPR_PROCESS_START'] = X_train['WEEKDAY_APPR_PROCESS_START'].apply(lambda x: x in ('SATURDAY', 'SUNDAY')).astype(int)\nX_test['FLAG_WEEKEND_APPR_PROCESS_START'] = X_test['WEEKDAY_APPR_PROCESS_START'].apply(lambda x: x in ('SATURDAY', 'SUNDAY')).astype(int)\ncyclic_encode(df=X_train, df_test=X_test, feature_name='WEEKDAY_APPR_PROCESS_START', max_val=6.0, mapping=lambda x: weekday_encoder[x])\n\ncategorical_feat.remove('WEEKDAY_APPR_PROCESS_START')\n\n# HOUR_APPR_PROCESS_START\n\ncyclic_encode(df=X_train, df_test=X_test, feature_name='HOUR_APPR_PROCESS_START', max_val=23.0)\n\ncategorical_feat.remove('HOUR_APPR_PROCESS_START')\n\ngc.collect()","f2b22be3":"# Apply dummy encoding on Categorical features:\nX_all = pd.concat([X_train, X_test])\nX_all = pd.get_dummies(data=X_all, columns=categorical_feat, drop_first=True)\nX_all.head(10).transpose()","6fffa9e1":"X_train = X_all[:X_train.shape[0]]\nX_test = X_all[X_train.shape[0]:]\nprint(X_train.shape)\nprint(X_test.shape)\nX_all = None\ngc.collect()","d7c9b034":"# from sklearn.preprocessing import MinMaxScaler\n\n# scaler = MinMaxScaler(feature_range = (0, 1))\n# X_train = scaler.fit_transform(X_train)\n# X_test = scaler.transform(X_test)","3371e1a6":"from sklearn.model_selection import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(X_train.iloc[:,1:], y, test_size=0.15, stratify=y, random_state=seed_value)\nprint('Train shape: ', x_train.shape)\nprint('Valid shape: ', x_valid.shape)","6b5485fe":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\n\nrfr = RandomForestClassifier(\n    n_estimators=20,\n    max_features=0.5,\n    max_samples=0.5,\n    min_samples_leaf=125,\n    n_jobs=-1,\n    oob_score=True,\n    random_state=seed_value\n)\n\n%time rfr.fit(x_train, y_train)\n\n\nprint(f'ROC_AUC_Train: {roc_auc_score(y_train, rfr.predict_proba(x_train)[:, 1])}')\nprint(f'ROC_AUC_OOB: {roc_auc_score(y_train, rfr.oob_decision_function_[:, 1])}')\nprint(f'ROC_AUC_Valid: {roc_auc_score(y_valid, rfr.predict_proba(x_valid)[:, 1])}')","60e0cd38":"feature_importances = pd.DataFrame(\n    {'feature': X_train.columns[1:], 'importance': rfr.feature_importances_}\n).sort_values(by='importance', ascending=False)\n\nfeature_importances.head(15)","ecef642b":"rfr = None\nx_train, x_valid, y_train, y_valid = None, None, None, None\ngc.collect()","5e8d2bcb":"X_train['CREDIT_INCOME_PERCENT'] = X_train['AMT_CREDIT'] \/ (X_train['AMT_INCOME_TOTAL']+1)\nX_train['ANNUITY_INCOME_PERCENT'] = X_train['AMT_ANNUITY'] \/ (X_train['AMT_INCOME_TOTAL']+1)\nX_train['CREDIT_TERM'] = X_train['AMT_ANNUITY'] \/ (X_train['AMT_CREDIT']+1)\nX_train['DAYS_EMPLOYED_PERCENT'] = X_train['DAYS_EMPLOYED'] \/ (X_train['DAYS_BIRTH']+1)\nX_train['GOODS_INCOME_PERCENT'] = X_train['AMT_GOODS_PRICE']\/ (X_train['AMT_INCOME_TOTAL']+1)\nX_train['GOODS_CREDIT_PERCENT'] = X_train['AMT_GOODS_PRICE']\/ (X_train['AMT_CREDIT']+1)","81628be9":"X_test['CREDIT_INCOME_PERCENT'] = X_test['AMT_CREDIT'] \/ (X_test['AMT_INCOME_TOTAL']+1)\nX_test['ANNUITY_INCOME_PERCENT'] = X_test['AMT_ANNUITY'] \/ (X_test['AMT_INCOME_TOTAL']+1)\nX_test['CREDIT_TERM'] = X_test['AMT_ANNUITY'] \/ (X_test['AMT_CREDIT']+1)\nX_test['DAYS_EMPLOYED_PERCENT'] = X_test['DAYS_EMPLOYED'] \/ (X_test['DAYS_BIRTH']+1)\nX_test['GOODS_INCOME_PERCENT'] = X_test['AMT_GOODS_PRICE']\/ (X_test['AMT_INCOME_TOTAL']+1)\nX_test['GOODS_CREDIT_PERCENT'] = X_test['AMT_GOODS_PRICE']\/ (X_test['AMT_CREDIT']+1)","d127728d":"previous_application = pd.read_csv(f'{PATH_IN}\/previous_application.csv', low_memory=False)\nprevious_application.head(10).transpose()","1a61414c":"cyclic_encode(previous_application, 'WEEKDAY_APPR_PROCESS_START', max_val=6.0, mapping=lambda x: weekday_encoder[x])\ncyclic_encode(previous_application, 'HOUR_APPR_PROCESS_START', max_val=23.0).head()","fe0ada3d":"categorical = [\n    'NAME_CONTRACT_TYPE',\n    'FLAG_LAST_APPL_PER_CONTRACT', 'NFLAG_LAST_APPL_IN_DAY',\n    'NAME_CASH_LOAN_PURPOSE','NAME_CONTRACT_STATUS', 'NAME_PAYMENT_TYPE',\n    'CODE_REJECT_REASON', 'NAME_TYPE_SUITE', 'NAME_CLIENT_TYPE',\n    'NAME_GOODS_CATEGORY', 'NAME_PORTFOLIO', 'NAME_PRODUCT_TYPE',\n    'CHANNEL_TYPE', 'NAME_SELLER_INDUSTRY',\n    'NAME_YIELD_GROUP', 'PRODUCT_COMBINATION',\n    'NFLAG_INSURED_ON_APPROVAL']\n\nnumerical = [feat for feat in previous_application.columns[2:] if feat not in categorical]\nprint(numerical)","d73c61e3":"to_nan(previous_application, 'XNA', categorical)\nto_nan(previous_application, 365243, numerical)\nflag_nan(previous_application)\ngc.collect()","ad99f459":"from sklearn.impute import SimpleImputer\n\n# Filling NaN for Categorical features:\n\ncate_imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=previous_application[categorical].mode())\nprevious_application[categorical] = cate_imp.fit_transform(previous_application[categorical])\n\n# Filling NaN for Numerical features:\n\nnum_imp = SimpleImputer(missing_values=np.nan, strategy='mean')\nprevious_application[numerical] = num_imp.fit_transform(previous_application[numerical])\n\ncate_imp, num_imp = None, None\ngc.collect()","6ff1b919":"# Onehot\nprevious_application = pd.get_dummies(data=previous_application, columns=categorical, drop_first=True)\ncategorical = [feat for feat in previous_application.columns[2:] if feat not in numerical]","4979bcf2":"# Groupby\nPA_count = previous_application[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\nPA_count.columns = ['CNT_PREV']\n\nPA_num = previous_application[['SK_ID_CURR']+numerical].groupby('SK_ID_CURR').mean()\nPA_num.columns = ['mean_pa_' + col for col in PA_num.columns]\n\nPA_cat = previous_application[['SK_ID_CURR']+categorical].groupby('SK_ID_CURR').sum()\nPA_cat.columns = ['sum_pa_' + col for col in PA_cat.columns]\n\nprevious_application = PA_count.merge(right=PA_num, left_index=True, right_index=True).merge(right=PA_cat, left_index=True, right_index=True)\nprevious_application.shape","b992a638":"X_train = X_train.merge(right=previous_application, how='left', left_on='SK_ID_CURR', right_index=True)\nX_test = X_test.merge(right=previous_application, how='left', left_on='SK_ID_CURR', right_index=True)\nprint(X_train.shape)\nprint(X_test.shape)\n\nPA_count, PA_num, PA_cat, previous_application = None, None, None, None\ngc.collect()","fae6feb4":"bureau = pd.read_csv(f'{PATH_IN}\/bureau.csv', low_memory=False)\nbureau_balance = pd.read_csv(f'{PATH_IN}\/bureau_balance.csv', low_memory=False)","03e084c4":"bureau = bureau.merge(right=bureau_balance[['SK_ID_BUREAU', 'MONTHS_BALANCE']].groupby('SK_ID_BUREAU').count(), left_on='SK_ID_BUREAU', right_index=True)\nbureau.head(10).transpose()","3a992d0c":"categorical = ['CREDIT_ACTIVE', 'CREDIT_CURRENCY','CREDIT_TYPE']\nnumerical = [feat for feat in bureau.columns[2:] if feat not in  categorical]\n\nflag_nan(bureau)","6a9bcf16":"from sklearn.impute import SimpleImputer\n\n# Filling NaN for Categorical features:\n\ncate_imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=bureau[categorical].mode())\nbureau[categorical] = cate_imp.fit_transform(bureau[categorical])\n\n# Filling NaN for Numerical features:\n\nnum_imp = SimpleImputer(missing_values=np.nan, strategy='mean')\nbureau[numerical] = num_imp.fit_transform(bureau[numerical])\n\ncate_imp, num_imp = None, None\ngc.collect()","2ba67e76":"# Onehot\nbureau = pd.get_dummies(data=bureau, columns=categorical, drop_first=True)\ncategorical = [feat for feat in bureau.columns[2:] if feat not in numerical]","267fe869":"bureau.head()","b28b391b":"# Groupby\nbureau_count = bureau[['SK_ID_CURR', 'SK_ID_BUREAU']].groupby('SK_ID_CURR').count()\nbureau_count.columns = ['CNT_BUREAU']\n\nbureau_num = bureau[['SK_ID_CURR']+numerical].groupby('SK_ID_CURR').mean()\nbureau_num.columns = ['mean_br_' + col for col in bureau_num.columns]\n\nbureau_cat = bureau[['SK_ID_CURR']+categorical].groupby('SK_ID_CURR').sum()\nbureau_cat.columns = ['sum_br_' + col for col in bureau_cat.columns]\n\nbureau = bureau_count.merge(right=bureau_num, left_index=True, right_index=True).merge(right=bureau_cat, left_index=True, right_index=True)\nbureau.shape","e6b4fdc1":"X_train = X_train.merge(right=bureau, how='left', left_on='SK_ID_CURR', right_index=True)\nX_test = X_test.merge(right=bureau, how='left', left_on='SK_ID_CURR', right_index=True)\nprint(X_train.shape)\nprint(X_test.shape)\n\nbureau_count, bureau_num, bureau_cat, bureau = None, None, None, None\ngc.collect()","0fa75e95":"installments_payments = pd.read_csv(f'{PATH_IN}\/installments_payments.csv', low_memory=False)\ninstallments_payments.head(10).transpose()","4f34e52b":"# Count of previous installments\ncnt_inst = installments_payments[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\ninstallments_payments['SK_ID_PREV'] = installments_payments['SK_ID_CURR'].map(cnt_inst['SK_ID_PREV'])\n\n# Average values for all other variables in installments payments\navg_inst = installments_payments.groupby('SK_ID_CURR').mean()\navg_inst.columns = ['mean_i_' + f_ for f_ in avg_inst.columns]\n\nX_train = X_train.merge(right=avg_inst.reset_index(), how='left', on='SK_ID_CURR')\nX_test = X_test.merge(right=avg_inst.reset_index(), how='left', on='SK_ID_CURR')\n\ncnt_inst, avg_inst, installments_payments = None, None, None\ngc.collect()","c62657d2":"pcb = pd.read_csv(f'{PATH_IN}\/POS_CASH_balance.csv', low_memory=False)\npcb.head(10).transpose()","b22ad429":"# Count of pos cash for a given ID\npcb_count = pcb[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\npcb['SK_ID_PREV'] = pcb['SK_ID_CURR'].map(pcb_count['SK_ID_PREV'])\n\n# Average values for all other variables in pos cash\npcb_avg = pcb.groupby('SK_ID_CURR').mean()\n\nX_train = X_train.merge(right=pcb_avg.reset_index(), how='left', on='SK_ID_CURR')\nX_test = X_test.merge(right=pcb_avg.reset_index(), how='left', on='SK_ID_CURR')\n\npcb_count, pcv_avg, pcb = None, None, None\ngc.collect()","a9f9c717":"credit_card_balance = pd.read_csv(f'{PATH_IN}\/credit_card_balance.csv', low_memory=False)\ncredit_card_balance.head(10).transpose()","c778ea10":"# Count of previous credit card balance for a given ID\nnb_prevs = credit_card_balance[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\ncredit_card_balance['SK_ID_PREV'] = credit_card_balance['SK_ID_CURR'].map(nb_prevs['SK_ID_PREV'])\n\n# Average values of all other columns\navg_cc_bal = credit_card_balance.groupby('SK_ID_CURR').mean()\navg_cc_bal.columns = ['cc_bal_' + f_ for f_ in avg_cc_bal.columns]\n\nX_train = X_train.merge(right=avg_cc_bal.reset_index(), how='left', on='SK_ID_CURR')\nX_test = X_test.merge(right=avg_cc_bal.reset_index(), how='left', on='SK_ID_CURR')\n\nnb_prevs, avg_cc_bal, creadit_card_balance = None, None, None\ngc.collect()","b0fd5299":"from sklearn.impute import SimpleImputer\n\nall_imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0.0)\nX_train[:] = all_imp.fit_transform(X_train[:])\nX_test[:] = all_imp.transform(X_test[:])\n\nall_imp = None\ngc.collect()","0ddcee2f":"from sklearn.model_selection import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(X_train.iloc[:, 1:], y, test_size=0.15, stratify=y, random_state=seed_value)\nprint('Train shape: ', x_train.shape)\nprint('Valid shape: ', x_valid.shape)","2e501be7":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\n\nrfr = RandomForestClassifier(\n    n_estimators=20,\n    max_features=0.5,\n    max_samples=0.5,\n    min_samples_leaf=125,\n    n_jobs=-1,\n    oob_score=True,\n    random_state=seed_value\n)\n\n%time rfr.fit(x_train, y_train)\n\n\nprint(f'ROC_AUC_Train: {roc_auc_score(y_train, rfr.predict_proba(x_train)[:, 1])}')\nprint(f'ROC_AUC_Valid: {roc_auc_score(y_valid, rfr.predict_proba(x_valid)[:, 1])}')\nprint(f'ROC_AUC_OOB: {roc_auc_score(y_train, rfr.oob_decision_function_[:, 1])}')","fc2d9708":"feature_importances = pd.DataFrame(\n    {'feature': X_train.columns[1:], 'importance': rfr.feature_importances_}\n).sort_values(by='importance', ascending=False)\nfeature_importances","b9f4bef4":"drop_features = feature_importances.loc[feature_importances['importance'] < 0.000001, 'feature']\ndrop_features.count()","1a95f8d4":"X_train.drop(columns=drop_features, inplace=True)\nX_test.drop(columns=drop_features, inplace=True)\nprint(X_train.shape)\nprint(X_test.shape)","ea74d926":"rfr = RandomForestClassifier(\n    n_estimators=150,\n    max_features=0.5,\n    max_samples=0.5,\n    min_samples_leaf=125,\n    n_jobs=-1,\n    oob_score=True,\n    random_state=seed_value\n)\n\n%time rfr.fit(X_train.iloc[:, 1:], y)\n\nprint(f'ROC_AUC_Train: {roc_auc_score(y, rfr.predict_proba(X_train.iloc[:, 1:])[:, 1])}')\nprint(f'ROC_AUC_OOB: {roc_auc_score(y, rfr.oob_decision_function_[:, 1])}')","5fd59100":"feature_importances = pd.DataFrame(\n    {'feature': X_train.columns[1:], 'importance': rfr.feature_importances_}\n).sort_values(by='importance', ascending=False)\nfeature_importances.head(15)","658dcbaa":"y_predict = rfr.predict_proba(X_test.iloc[:, 1:])[:, 1]\nrandom_forest_with_feature_engineering_submission = pd.DataFrame({'SK_ID_CURR': X_test['SK_ID_CURR'].astype(int), 'TARGET': y_predict})\nrandom_forest_with_feature_engineering_submission.head()","538a038d":"random_forest_with_feature_engineering_submission.to_csv('random_forest_with_feature_engineering.csv', index=False)","ee9700d4":"# VI. Feature Engineering","00413902":"## 1. Target","3af02b2d":"## 4. Normalization\nThis step is necessary if we are about to use regression-based models such as logistic regression, SVM, neural network, etc.","aba36fd7":"## 2. Features from other tables","1c9b075d":"## 2. Random Forest","c531b21d":"## 2. Random Forest","fff91a9a":"### Pos Cash Balance","7cc9f046":"![image.png](attachment:image.png)","347bbcf1":"# III. Exploratory Data Analysis","29d55394":"### a. Observation of missing values","09c87fba":"### Bureau","2f42b3a3":"## 1. Train - valid splitting","c89cfe53":"### Credit Card Balance","b6859a2e":"# I. Environment settings","30fba539":"***This notebook is my personal practice based on many ideas and principles I have learnt in these tutorials:***\n* http:\/\/course18.fast.ai\/ml\n* https:\/\/www.kaggle.com\/willkoehrsen\/start-here-a-gentle-introduction\n\n\nI'd like to thank Jeremy Howard for his great effort on emphasizing the crucial importance of feature engineering in Data Science.\n\nI also want to thank Will Koehrsen for his wonderful notebooks of many elementary ML concepts.\n\nFinally, thank you, Mr. \u0110\u1ee9c Nguy\u1ec5n aka mathormad, for pointing me out the helpful course and this wonderful platform (Kaggle) for Data Scientists.","be77a68b":"## 3. Numerical features","3b4630c8":"### c. Imputation","e02a82ca":"# II. Dataset Overview","37b58076":"## 3. One-hot encoding categorical features\nMost ML models can't directly handle categorical features, so the very first step before applying any ML model to data which contains categorical features should be one-hot encoding or dummy encoding","d915b7fb":"### Previous Installments","b50d8a1d":"# IV. Data Preprocessing","562087ac":"### Previous Applications","9264c439":"# VII. Final model","35a89983":"## 2. Categorical features","9cc5de37":"## Target:\n\nIn this notebook, I will try to reach an optimized auc_roc score for my ***random forest*** by applying basic feature engineering techniques.","471204f3":"## 1. Train - Valid splitting","719ccde9":"In EDA step, we have also observed a lot of outliers in many numerical features. As for ourliers in numerical features, the safest approach should be replacing them with the mean or the median. We don't do that directly here. Instead we will treat them as missing values, and then impute them later.","5bd5ddd3":"### Filling NaN of merging tables","5779f8cd":"# V. Baseline Model","722675bc":"## 3. Submission","1871eebd":"## 2. Encoding cyclical features\nA common method for encoding cyclical data is to transform the data into two dimensions using a sine and consine transformation.\n\nWe can do that using the sin - cos transformations.","943f1458":"### b. Treating improper values or outliers as missing values\nAs we can see in EDA step, CODE_GENDER and ORGANIZATION_TYPE contain a weird value in train set, 'XNA'. This is probably a missing value we have to deal with. The safest way should be treating it as a missing value and impute it together with other missing values later.","0adec2fc":"## 1. Domain knowledge features","5ca28f5f":"# Home Credit Default Risk\n### *Can you predict how capable each applicant is of repaying a loan?*","cae59ab2":"## 1. Imputation of missing values (NaN) and outliers"}}