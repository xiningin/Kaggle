{"cell_type":{"751752a9":"code","ef7ee62b":"code","b263e156":"code","ced829ea":"code","77d2c407":"code","d127f6fa":"code","b15164dd":"code","1cb0927e":"code","4c9d5ac5":"code","5ba9cc3e":"code","6113b279":"code","cd617d8f":"code","97b0ab44":"code","ca6275b0":"code","e823ea0d":"code","b5537acb":"code","fe51d1b1":"code","4da63da1":"code","a0c19da0":"markdown","96da909f":"markdown"},"source":{"751752a9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ef7ee62b":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","b263e156":"torch.manual_seed(1)","ced829ea":"words_to_ix = {\"hello\": 0, \"world\": 1}\nembeds = nn.Embedding(2, 5)\nlookup_tensor = torch.tensor(words_to_ix[\"hello\"], dtype=torch.long)\nhello_embed = embeds(lookup_tensor)\nprint(hello_embed)","77d2c407":"context_size = 4\nembedding_dimension = 10\n\ntest_sentence = \"\"\"When forty winters shall besiege thy brow,\nAnd dig deep trenches in thy beauty's field,\nThy youth's proud livery so gazed on now,\nWill be a totter'd weed of small worth held:\nThen being asked, where all thy beauty lies,\nWhere all the treasure of thy lusty days;\nTo say, within thine own deep sunken eyes,\nWere an all-eating shame, and thriftless praise.\nHow much more praise deserv'd thy beauty's use,\nIf thou couldst answer 'This fair child of mine\nShall sum my count, and make my old excuse,'\nProving his beauty by succession thine!\nThis were to be new made when thou art old,\nAnd see thy blood warm when thou feel'st it cold.\"\"\".strip().split()","d127f6fa":"vocab = list(set(test_sentence))","b15164dd":"print(vocab[:10])","1cb0927e":"trigrams = [(test_sentence[i], [test_sentence[i+1], test_sentence[i+2], test_sentence[i-1], test_sentence[i-2]]) for i in range(2, len(test_sentence) - 2)]","4c9d5ac5":"trigrams[:10]","5ba9cc3e":"words_to_ix = {word: i for i, word in enumerate(vocab)}\nlist(words_to_ix.items())[:10]","6113b279":"class NGramLanguageModeller(nn.Module):\n    def __init__(self, vocab_size, embedding_dimension, context_size):\n        super(NGramLanguageModeller, self).__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_dimension)\n        self.linear1 = nn.Linear(context_size * embedding_dimension, 128)\n        self.linear2 = nn.Linear(128, vocab_size)\n        \n    def forward(self, inputs):\n        embeds = self.embeddings(inputs).view(1, -1)\n        out = F.relu(self.linear1(embeds))\n        out = self.linear2(out)\n        log_probs = F.log_softmax(out, dim=1)\n        return log_probs","cd617d8f":"losses = []\nloss_function = nn.NLLLoss()\nmodel = NGramLanguageModeller(len(vocab), embedding_dimension, context_size)\noptimizer = optim.SGD(model.parameters(), lr=0.0005)","97b0ab44":"for epoch in range(5000):\n    total_loss = 0\n    for target, context in trigrams:\n        context_ids = torch.tensor([words_to_ix[context_word] for context_word in context], dtype=torch.long)\n        model.zero_grad()\n        log_probs = model(context_ids)\n        loss = loss_function(log_probs, torch.tensor([words_to_ix[target]], dtype=torch.long))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    losses.append(total_loss)\n    if epoch % 100 == 0:\n        print(\"Epoch: {}; Loss: {}\".format(epoch, total_loss))        ","ca6275b0":"import matplotlib.pyplot as plt\nplt.plot(losses)\nplt.show()","e823ea0d":"model.embeddings(torch.tensor([words_to_ix[\"thy\"]], dtype=torch.long))","b5537acb":"test_sentence = \"\"\"When forty winters shall besiege thy brow,\nAnd dig deep trenches in thy beauty's field,\nThy youth's proud livery so gazed on now,\nWill be a totter'd weed of small worth held:\nThen being asked, where all thy beauty lies,\nWhere all the treasure of thy lusty days;\nTo say, within thine own deep sunken eyes,\nWere an all-eating shame, and thriftless praise.\nHow much more praise deserv'd thy beauty's use,\nIf thou couldst answer 'This fair child of mine\nShall sum my count, and make my old excuse,'\nProving his beauty by succession thine!\nThis were to be new made when thou art old,\nAnd see thy blood warm when thou feel'st it cold.\"\"\".strip().split()\n\nvocab = list(set(test_sentence))\nlen_vocab = len(vocab)","fe51d1b1":"import random\npairs = []\nfor i in range(20000):\n    j = random.randint(2, len(test_sentence) - 3)\n    pairs.append(((test_sentence[j], test_sentence[j + random.randint(-2, 2)]), 1.))\nprint(pairs[:10])","4da63da1":"random_pairs = []\nfor i in range(100000):\n    j = random.randint(0, len(test_sentence) - 1)\n    k = random.randint(0, len(test_sentence) - 1)\n    random_pairs.append(((test_sentence[j], test_sentence[k]), 0.))\nprint(random_pairs[:10])","a0c19da0":"## Skip Gram Model Training","96da909f":"## Word to Vec"}}