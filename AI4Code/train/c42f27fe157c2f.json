{"cell_type":{"4621fafb":"code","84f79264":"code","93bb0449":"code","3f8e37de":"code","e2c5d794":"code","2b5fbcc9":"code","cb6837f4":"code","50e1327a":"markdown","4d89e280":"markdown","3a124769":"markdown","9e337722":"markdown","d5b8c7c9":"markdown","0e4f2ef8":"markdown","f2897d41":"markdown","c5c20c7c":"markdown","04f2c643":"markdown"},"source":{"4621fafb":"pip install pickledb","84f79264":"import pandas\nimport json\nimport csv\nimport tensorflow_hub as hub\nimport numpy as np\nimport pickle\nimport pickledb\nimport random\nimport os","93bb0449":"\nclass EmbeddingStore:\n    embed = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/5\")\n\n    def __init__(self, filename):\n        self.embedding_file = filename\n        self.all_docs_embeddings = self.read_embeddings()\n        if self.all_docs_embeddings is None:\n            print(\"all_docs initialized to None\")\n\n    def print_embeddings(self):\n        if self.all_docs_embeddings is None:\n            print(\"Not initialized\")\n        else:\n            print(self.all_docs_embeddings.shape)\n\n    def save_embeddings(self):\n        f = open(self.embedding_file, 'wb')\n        pickle.dump(self.all_docs_embeddings, f)\n\n    def read_embeddings(self):\n        try:\n            f = open(self.embedding_file, 'rb')\n        except FileNotFoundError:\n            return None\n        try:\n            embeddings_ = pickle.load(f)\n        except EOFError:\n            embeddings_ = None\n        return embeddings_\n\n    def mergeInOrder(self, filename):\n        f = open(filename, 'rb')\n        try:\n            new_embeddings_ = pickle.load(f)\n        except EOFError:\n            new_embeddings_ = None\n        combined_embeddings_ = np.concatenate((self.all_docs_embeddings, new_embeddings_))\n        self.all_docs_embeddings = combined_embeddings_\n        print(\"Merged embeddings = {}\".format(len(self.all_docs_embeddings)))\n        self.save_embeddings()\n\n    def add_embeddings(self, new_embeddings_):\n        if self.all_docs_embeddings is None:\n            combined_embeddings_ = new_embeddings_\n        else:\n            combined_embeddings_ = np.concatenate((self.all_docs_embeddings, new_embeddings_))\n        self.all_docs_embeddings = combined_embeddings_\n\n    def dump(self):\n        self.save_embeddings()\n\n    def add_paras(self, new_paras_):\n        new_embeddings_ = self.embed(new_paras_)\n        self.add_embeddings(new_embeddings_)\n\n    def add_para(self, new_para_):\n        new_embeddings_ = self.embed([new_para_])\n        self.add_embeddings(new_embeddings_)\n\n    def add_para_new(self, new_para_):\n        t = tf.cast(t, tf.string)\n        t = tf.string_split([t], delimiter).values\n        e = self.embed(t)\n        e = tf.reduce_mean(e, axis=0)\n        self.add_embeddings(tf.squeeze(e))\n\n    def find_top_match(self, query):\n        #print(\"Query received = {}\".format(query))\n        query_embedding_ = self.embed(query)\n        #print(query_embedding_.shape) \n        #print(query_embedding_[0].shape)\n        #print(self.all_docs_embeddings.shape)\n        corr = np.inner(query_embedding_[0], self.all_docs_embeddings)\n        #print(corr)\n        value = np.amax(corr)\n        pos = np.where(corr == value)\n        #print(pos)\n        #print(\"Match = {} at index {}\".format(value, pos[0][0]))\n        return pos[0][0]\n\n    def find_top_k_match(self, query, k):\n        print(\"Query received = {}\".format(query))\n        query_embedding_ = self.embed(query)\n        print(query_embedding_.shape)\n        print(query_embedding_[0].shape)\n        print(self.all_docs_embeddings.shape)\n        corr = np.inner(query_embedding_[0], self.all_docs_embeddings)\n        print(corr)\n        values = np.argpartition(corr, -k)[-k:]\n        print(\"Top K matches for = {} at {}\".format(query, values))\n        return values\n\n    def find_top_k_match_with_conf(self, query, k):\n        query_embedding_ = self.embed(query)\n        corr = np.inner(query_embedding_[0], self.all_docs_embeddings)\n        indexes = np.argpartition(corr, -k)[-k:]\n        values = []\n        for i in indexes:\n            values.append((i,corr[i]))\n        #print(\"Top K matches for = {} at {}\".format(query, values))\n        return values\n\nclass ParaStore:\n\n    def __init__(self, docsfilename, embeddingsfile):\n        self.docfile = docsfilename\n        self.embeddingstore = EmbeddingStore(embeddingsfile)\n        self.all_docs = self.read_para()\n        self.nos_docs = self.all_docs.totalkeys()\n        print(\"There are {} docs in the store\".format(self.nos_docs))\n\n    # Merge the pickledbs \n    def mergeInOrder(self, filename, embeddingsfile):\n        doc2 = pickledb.load(filename, False)\n        rangekey = doc2.totalkeys()\n        for i in range(rangekey):\n            para = doc2.get(str(i))\n            self.all_docs.set(str(self.nos_docs), para)\n            self.nos_docs +=1\n        self.all_docs.dump()\n        self.embeddingstore.mergeInOrder(embeddingsfile)\n\n    def save_para(self):\n        self.all_docs.dump()\n\n    def read_para(self):\n        docs = pickledb.load(self.docfile, False)\n        return docs\n\n    # Changed this function to use the aid instead of text\n    def already_exists(self, new_para):\n        allkeys = self.all_docs.getall()\n        for key in allkeys:\n            candidate = self.all_docs.get(key)\n            if new_para[\"aid\"] == candidate[\"aid\"] and new_para[\"pid\"] == candidate[\"pid\"]:\n                print(\"({},{}) already in the doc store\".format(new_para[\"aid\"], new_para[\"pid\"]))\n            #if new_para[\"aid\"] == self.all_docs.get(key)[\"aid\"]:\n            #    print(\"{} already in the doc store\".format(new_para[\"aid\"]))\n                return True\n        return False\n\n    def add_para(self, new_para):\n        text = new_para[\"text\"]\n        if not self.already_exists(new_para):\n            self.all_docs.set(str(self.nos_docs), new_para)\n            #self.embeddingstore.add_para(text)\n            self.embeddingstore.add_para_new(text)\n            self.nos_docs = self.nos_docs+1\n            if self.nos_docs % 1000 == 0:\n                self.dump()\n\n    def add_paras(self, paras):\n        print(\"Adding {} paras\".format(len(paras)))\n        for i in range(len(paras)):\n            self.add_para(paras[i])\n\n    def dump(self):\n        self.all_docs.dump()\n        self.embeddingstore.dump()\n\n    def get_para(self, nos):\n        #return self.all_docs.get(str(nos+1))\n        return self.all_docs.get(str(nos))\n\n    def get_matching_para(self, query):\n        pos = self.embeddingstore.find_top_match(query)\n        match = self.get_para(pos)\n        return match\n\n    def get_matching_k_para(self, query, k):\n        positions = self.embeddingstore.find_top_k_match(query, k)\n        matches = {}\n        for i in positions:\n            match = self.get_para(i)\n            matches[str(i)] = match\n        return matches\n\n    def get_matching_k_para_with_conf(self, query, k):\n        positions = self.embeddingstore.find_top_k_match_with_conf(query, k)\n        #print(\"Input query = {}\".format(query))\n        matches = []\n        for pos in positions:\n            i = pos[0]\n            match = self.get_para(i)\n            matches.append((pos[0], pos[1], match))\n        return matches\n\n    def filter_k_randomly(self, matches, k):\n        kmatches = {}\n        print(\"Filtering {} from {} matches\".format(k, len(matches)))\n        if len(matches) <= k:\n            return matches\n        else:\n           indexes = random.sample(matches.keys(), k)\n        for i in indexes:\n            kmatches[i] = matches[i]\n        return kmatches\n\n    def applyFilter(self, filt, text):\n        for name in filt:\n            if text and name.lower() in text.lower():\n                return True\n        return False\n\n    def get_including_k_para(self, query, filt, k):\n        matches = {}\n        allkeys = self.all_docs.getall()\n        for key in allkeys:\n            candidate = self.all_docs.get(key)\n            if (query[0] in candidate[\"text\"]) and (self.applyFilter(filt, candidate[\"text\"])):\n                matches[key] = candidate\n        kmatches = self.filter_k_randomly(matches, k)\n        return kmatches","3f8e37de":"def openStore(docfile, embedding_file):\n    docstore = ParaStore(docfile, embedding_file)\n    return docstore\n\ndef getTopKMatch(dbfile, embedfile, query, k=1):\n    # open the store\n    query_vector = [query]\n    print(\"The query vector is {}\".format(query_vector))\n    docstore = openStore(dbfile, embedfile)\n    matches = docstore.get_matching_k_para(query_vector, k)\n    return matches\n\ndef getTopKMatchWithConf(dbfile, embedfile, query, k=1):\n    # open the store\n    query_vector = [query]\n    #print(\"The query vector is {}\".format(query_vector))\n    docstore = openStore(dbfile, embedfile)\n    matches = docstore.get_matching_k_para_with_conf(query_vector, k)\n    matches.sort(key = lambda x: x[1], reverse=True)\n    return matches\n\ndef findTitleURL(cord_uid):\n    df = pandas.read_csv(metadatafile)\n    cand = df.loc[df['cord_uid']== cord_uid]\n    title = cand[\"title\"].values[0]\n    url = cand[\"url\"].values[0]\n    return (title,url)\n\ndef findMatches(paragraph, paraFile, embeddingFile):\n    print(\"Paragraph = \" + paragraph)\n    topk = getTopKMatchWithConf(paraFile, embeddingFile, paragraph, 10)  \n    matches = []    \n    for cand in topk:\n        #print(cand)\n        match = {}\n        para = cand[2]\n        cord_uid = para[\"aid\"]\n        title, url = findTitleURL(cord_uid)\n        match[\"title\"] = title\n        match[\"url\"] = url\n        match[\"text\"] = para[\"text\"]\n        match[\"index\"] = str(cand[0])\n        matches.append(match)\n    return matches\n\ndef findSeed(taskFile, paraFile, embeddingFile):\n    seeddir = \"\/kaggle\/working\/seeds\/\"\n    if not os.path.isdir(seeddir):\n        os.mkdir(seeddir)\n    json_file = open(taskFile, 'r')\n    data = json.load(json_file)\n    keys = data.keys()\n    start = taskFile.find(\"Task\")\n    for key in sorted(keys):\n        \n        outfile = taskFile[start:taskFile.find(\"_query\")] + \"_\" + key + \"_match.csv\"\n        #print(\"Outputfile = {}\".format(outfile))\n        csvwriter = csv.writer(open(seeddir + outfile, 'w'))\n        paraText = data[key][\"paraText\"]\n        #print(paraText)\n        matches = findMatches(paraText, paraFile, embeddingFile)\n        for match in matches:\n            csvwriter.writerow([paraText, match[\"text\"], match[\"index\"], match[\"title\"], match[\"url\"]])\n            \n        \nparaFile = \"\/kaggle\/input\/cord19parauseembeddings\/cord19-para-clean.db\"\nembeddingFile = \"\/kaggle\/input\/cord19parauseembeddings\/cord19-para-clean.pkl\"\ntaskFile = \"\/kaggle\/input\/task-1-query\/Task_1_query.json\"\nmetadatafile = \"\/kaggle\/input\/cord19metadata\/metadata.csv\"\n\nfindSeed(taskFile, paraFile, embeddingFile)\n","e2c5d794":"pip install MulticoreTSNE","2b5fbcc9":"import pickle\nimport pickledb\nimport pandas\nimport sys\nimport time\nimport numpy as np\nimport scipy as sp\nimport scipy.sparse\nimport matplotlib.pyplot as plt\nimport os\n\nfrom sklearn.neighbors import kneighbors_graph\nfrom scipy import sparse\nfrom scipy.sparse.linalg import eigs\nfrom MulticoreTSNE import MulticoreTSNE as TSNE\nfrom sklearn.manifold import SpectralEmbedding\n\n\n\ndef read_embeddings(file):\n        try:\n            f = open(file, 'rb')\n        except FileNotFoundError:\n            return None\n        try:\n            embeddings_ = pickle.load(f)\n        except EOFError:\n            embeddings_ = None\n        return embeddings_\n\ndef read_para(self):\n        docs = pickledb.load(self.docfile, False)\n        return docs\n\ndef get_para(self, nos):\n        return self.docs.get(str(nos))\n\ndef make_graph(x, n_neighbors, mode, sparse_mode, sigma, knnfile, nrmd):\n    '''\n    ***\n    \n    inputs: \n\n    x: the data (np.array)\n    args: args: the input arguments to the script\n    nrmd: if we want the graph matrix to be normalized\n    outputs:\n\n    a: the diffusion matrix (scipy.csr_matrix)\n\n    ***\n    '''\n    n = x.shape[0]\n    print(knnfile)\n    # construct kneighbors graph from data\n\n    if (id(knnfile) == id('')):\n        print('computing knn\\n')\n\n        start_time = time.time()\n        a = kneighbors_graph(x, n_neighbors, mode='distance')\n        print(\"--- %s seconds ---\" % (time.time() - start_time))\n        print(a.shape)\n        with open('knn'+str(n_neighbors), 'wb') as wfile:\n            pickle.dump(a,wfile)\n    else:\n        with open('knn'+str(n_neighbors), 'rb') as rfile:\n            a = pickle.load(rfile)                           \n    # simmetrize it \n\n#print(a[[1,2],:])\n    a = a + a.transpose()\n#a = np.expm1(-a)\n#a = a-a.sign()\n#if args.sparse_mode: a = a + sparse.eye(n)\n\n#print(a[[1,2],:])\n\n    if (nrmd):\n\n        # get the un-normalized weight matrix\n        norm = (a * sigma).max(axis=1).todense()\n\n        norm = 1. \/ norm\n        a = - a.multiply(norm)\n        a = a.expm1()\n        a = a - a.sign()\n        if sparse_mode: a = a + sparse.eye(n)\n\n        # get the normalized weight matrix\n        p = a.sum(axis=1)\n        p = 1. \/ p\n        a = a.multiply(p)\n        assert a.sum() == n # sanity check\n    return a\n\n\nclass diff_vecs:  \n    def __init__(self, name, roll):  \n        self.name = name  \n        self.vec = vec \n   \n\n\ndef diffuse_labels(y=None, train_indices=None, g=None, t=1, class_=1):\n    '''\n    ***\n    \n    inputs: \n\n    y: the labels (np.array)\n    train_indices: the indices of the train dataset (list)\n    g: the graph (scipy.csr_matrix)\n    t: how many diffusion steps (int)\n\n    outputs:\n\n    signal: the soft labels from diffusion (np.array)\n\n    ***\n    '''\n    n = len(y)\n\n    # get training data labels and normalize in [-1,1]\n    y = y[train_indices]\n    y = 2 * (y == class_).astype(np.float) - 1\n    # get the signal to diffuse\n    signal = np.zeros((n))\n    signal[train_indices] = y\n\n    # diffuse t times \n    for _ in range(t):\n        signal = g.dot(signal)\n        signal[train_indices] = y\n    return signal\n\ndef plot_diff(title,fig_ax, x, signal,training_idx,text,color):\n    print('Plotting results')\n    fig_ax.scatter(x[:, 0], x[:, 1], s=2**2)\n    heatmap = signal[signal>0]\n    cmap = 'bwr'\n\n\n    for ix in training_idx:\n        print(ix)\n        fig_ax.text(x[ix,0],x[ix,1], str(ix))\n\n    fig_ax.set_title(title, fontsize='x-large')\n    fig_ax.scatter(x[signal>0,0], x[signal>0,1],color = 'red')\n\n    fig_ax.scatter(x[training_idx,0], x[training_idx,1], marker='s',  color = color)\n\n    return(fig_ax)\n\n\ndef findPara(pos, docs):\n    para = docs.get(str(pos))  # uncomment this line if pos is an integer\n    return para['text']\n\n\n# Function to lookup title for notebook\ndef findTitle(pos, paraFile=None, metadataFile=None):\n    if not paraFile:\n        paraFile = \"\/kaggle\/input\/embeddings\/cord19.db\"\n    if not metadataFile:\n        metadataFile = \"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\"\n        \n    docs = pickledb.load(paraFile, False)\n    para = docs.get(str(pos))\n    docid = para[\"aid\"]\n    metadata = pandas.read_csv(metadataFile)\n    cand = metadata[metadata[\"cord_uid\"] == docid]\n    title = cand[\"title\"].values[0]\n    return docid, title\n\n\ndef main(docs, data, seedFile, n_neighbors, mode, sparse_mode, sigma, knnfile, TSNE_done):\n      \n    n = data.shape[0]\n    print(n)\n\n    print('starting TSNE')\n    print(TSNE_done)\n    if (TSNE_done == 0):\n#        \tprint('Doing TSNE_para2')\n            X_embedded = TSNE(n_jobs=4).fit_transform(data)\n            with open('tsne_para_para2', 'wb') as wfile:\n                pickle.dump(X_embedded,wfile)\n            plt.scatter(X_embedded[:, 0], X_embedded[:, 1])\n            plt.show()\n    else:\n            with open(TSNE_file, 'rb') as rfile:\n                X_embedded = pickle.load(rfile)\n\n\n    #creating the graph and intiating diffusion:\n    print('Creating graph, and starting diffusion process\\n')\n\n    g=make_graph(X_embedded, n_neighbors, mode, sparse_mode, sigma, knnfile, nrmd = 1)\n    df = pandas.read_csv(seedFile,header=None)\n    meds = ['task x']\n    titles = ['tasks xx']\n    colors = ['green','yellow','pink','black','brown','grey']\n    count = 0\n\n    #fig, ax_array = plt.subplots(1, len(meds), sharex=True, sharey=True)\n\n\n    print('diffusion loop')\n    a = []\n    for med in meds:\n        seed_idx = df.values.astype(int)\n        y = np.ones(n, dtype=int)\n        y.astype(int)\n        signal = diffuse_labels(y=y, train_indices=seed_idx, g=g, t=titer, class_=1)\n        a=np.where(signal>0)\n        sd_id_id = np.where((seed_idx) == a[0])\n\n\n        print('recoeverd results indices: '+str(a))\n\n        color = colors[count]\n        count = count+1\n    outfilename = seedFile[seedFile.find(\"Seedfile\"):seedFile.find('.')]\n    outF = open(\"\/kaggle\/working\/diffusion\/\"+ outfilename + \"_res\" +\".txt\", \"w\")\n    print('doing '+ str(seedFile))\n    # print out the interactions\n    for i in a[0]:\n        #print('i'+str(i))\n        #print(type(i))\n        #id, tit = findTitle(i, paraFile='cord19.db', metadataFile='metadata.csv')\n        para = findPara(i, docs)\n        outF.write('id: '+ str(i) + ' and paragraph: ' + para +  '\\n\\n')\n        #print('id: '+ str(i) + ' and paragraph: ' + para +  '\\n')\n        #print('id: '+ id + ' and title: ' + tit +  '\\n')\n\n    #sys.exit(0)\n    print('done')\n    \n# Data files\nparaFile = \"\/kaggle\/input\/cord19parauseembeddings\/cord19-para-clean.db\"\nembeddingFile = \"\/kaggle\/input\/cord19parauseembeddings\/cord19-para-clean.pkl\"\nmetadatafile = \"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\"\nTSNE_file = \"\/kaggle\/input\/tsnepara\/tsne_para2\"\nseedDir = '\/kaggle\/input\/task1seeds'\n\n\n# Default parameters\nsigma = 1\nmode = 0\nsparse_mode = 0\nknnfile = ''\nn_neighbors = 4\nTSNE_done = 1\ntiter = 1\n\n\ndef processAllSeeds(dir, paraFile, embeddingFile, n_neighbors, mode, sparse_mode, sigma, knnfile, TSNE_done):\n    outputdir = \"\/kaggle\/working\/diffusion\/\"\n    if not os.path.isdir(outputdir):\n        os.mkdir(outputdir)\n    docs = pickledb.load(paraFile, False) \n    data = read_embeddings(embeddingFile)\n    for f in os.listdir(dir):\n        if f[0] == '.':\n            continue\n        seedFile = os.path.join(dir,f)\n        print(\"Invoking main with seedFile = {}\".format(seedFile))\n        main(docs, data, seedFile, n_neighbors, mode, sparse_mode, sigma, knnfile, TSNE_done)\n\nprocessAllSeeds(seedDir, paraFile, embeddingFile, n_neighbors, mode, sparse_mode, sigma, knnfile, TSNE_done)","cb6837f4":"import os\nimport csv\n\n# If using precomputed results\nresult_dir = \"\/kaggle\/input\/task1result\/\"\n# If using computed results from running the notebook cell for Diffusion\n#result_dir = \"\/kaggle\/working\/diffusion\/\"\n\noutput_dir = \"\/kaggle\/working\/\"\n\ndef lookupTask1SubTasks(nos):\n    if nos == 1:\n        subtask = \"Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery\"\n    elif nos == 2:\n        subtask = \"Prevalence of asymptomatic shedding and transmission (e.g., particularly children)\"\n    elif nos == 3:\n        subtask = \"Seasonality of transmission\"\n    elif nos == 4:\n        subtask = \"Physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic\/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding)\"\n    elif nos == 5:\n        subtask = \"Persistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood)\"\n    elif nos == 6:\n        subtask = \"Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic)\"\n    elif nos == 7:\n        subtask = \"Natural history of the virus and shedding of it from an infected person\"\n    elif nos == 8:\n        subtask = \"Implementation of diagnostics and products to improve clinical processes\"\n    elif nos == 9:\n        subtask = \"Disease models, including animal models for infection, disease and transmission\"\n    elif nos == 10:\n        subtask = \"Tools and studies to monitor phenotypic change and potential adaptation of the virus\"\n    elif nos == 11:\n        subtask = \"Immune response and immunity\"\n    elif nos == 12:\n        subtask = \"Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings\"\n    elif nos == 13:\n        subtask = \"Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\"\n    elif nos == 14:\n        subtask = \"Role of the environment in transmission\"\n    else:\n        subtask = \"Illegal subtask nos\"\n    \n    return subtask\n\ndef extractSubtask(infile):\n    index1 = infile.find(\"Subtask\")\n    start = index1 + len(\"Subtask\")\n    end = start+2\n    return infile[start:end]\n\ndef extractResultPara(line):\n    index1 = line.find(\"paragraph:\")\n    start = index1 + len(\"paragraph:\")\n    return line[start:].strip()\n\ndef text2CSV(infile, outfile):\n    csvwriter = csv.writer(open(outfile, 'w'))\n    in_file = open(infile, 'r')\n    subtasknos = extractSubtask(infile)\n    subtask_str = lookupTask1SubTasks(int(subtasknos)) \n    csvwriter.writerow([\"Subtask ID\", \"SubTask Description\", \"Matching Paragraph\"])\n    for line in in_file:\n        if len(line.strip()) <=0:\n            continue\n        match = extractResultPara(line)\n        csvwriter.writerow([subtasknos, subtask_str, match])\n        \n    \ndef convertDiffResultToCSV(dir, outdir):\n    for f in os.listdir(dir):\n        if f[0] == '.':\n            continue\n        infile = os.path.join(dir,f)\n        f = f[f.find('_')+1:]\n        filename = f.split('.')[0] + \".csv\"\n        outfile = os.path.join(outdir, filename)\n        print(\"Input file {} to Output file {}\".format(infile, outfile))\n        text2CSV(infile, outfile)\n\nconvertDiffResultToCSV(result_dir, \".\")","50e1327a":"# Approach\nWe intent to match a task description to paragraph that are semantically similar. To achieve that, we encode each task description and paragraphs of documents\/articles to an embedding vector space using a deep learning model (Universal Sentence Encoder [[1](https:\/\/arxiv.org\/abs\/1803.11175)]) and find semantic similarity between them. The Universal Sentence Encoder (USE) is an extremely popular model for encoding text, and used in many Natural Language Processing (NLP) tasks. We found that the semantic similarity in embedded space does not always find all the desired matches as in many cases the task description are either not specific or mention several diverse concepts. Hence we improve the obtained results using two fold approach. First we provide a feature that allows a user to select relevant paragraphs from the result of top-k matches to a given task description, we call them *seeds*, and recursively find semantically similar paragraph to these seeds. Second, we use the novel concept of \"Diffusion\" from active learning, on a k-nearest neighbor graph constructed using the USE embeddings of all the paragraphs of the documents.     ","4d89e280":"# Description\nWe have built a pipeline of the following processing modules\n\n1. **ETL** - extract a representation of each article in the CORD-19 data corpus,\n2. **Encoding** - compute the Universal Sentence Encoder (USE) encoding of the extracted representation of each article. (This is a computationaly expensive step which takes 2-3 hours. To save time we have precomputed embeddings and uploaded them as input data source. To use them instead of computing them, please skip the step marked as optional below and uncomment the filename pointing to the input data)\n3. **Seed Generation** - use the sub-task or a related question to find top-k matching articles from the CORD-19 data corpus\n4. **Representation** - use the deep-encoding to create a contextual graph\n5. **Diffusion** - using the top-k matches as seed, use the process of diffusion on the graph to find contextually similar document. Reports the sets of intersection, and a visualization.","3a124769":"![](http:\/\/)![image.png](attachment:image.png)\n\nThis is a joint project by Avinash Vyas and Dan Kushnir\n\n\n\u00a9 2020 Nokia Licensed under the BSD 3-Clause License SPDX-License-Identifier: BSD-3-Clause","9e337722":"# Diffusion","d5b8c7c9":"# Seed Generation\nWe have extracted the task and subtask descriptions and uploaded them as a data source (task-1-query). The following module take each task description (and subtasks within), compute their USE embeddings and finds top-10 semantically similar paragraphs in the USE embedding space using COSINE similarity. As demonstrated in the example below, this step itself yields interesting and relevant results for many of the subtasks. ","0e4f2ef8":"# Seed Selection\nThis is a manual step where an expert selects the paragraphs that best match the task description. To aid this step we have created a webinterface that allows the expert to go through the paragraphs returned by the seed generation step and select the ones that are best match to the task description. The user can find more matches by using the seeds as input to the seed generation phase. Example seed file for Task1 Subtask 2 :\n\n..\/input\/task1seeds\/Seedfile_Task_1_Subtask02.csv\n\n644146\n641032\n916895\n671694\n\nThe selected seeds are then used as input to our diffusion step as described below.","f2897d41":"# Encoding\nNext we show the code to compute the USE embeddings of all the paragraphs of the documents and the articles in the CORD-19 data set. *This s a computationaly expensive step which takes 2-3 hours. To save time we have precomputed embeddings and uploaded them as input data source.* ","c5c20c7c":"# Presentation and Output","04f2c643":"# Goal\nTo adderess the COVID-19 Open Research Dataset Challenge, we aim to provide a generic search tool that a researcher or a doctor can use to quickly find sections\/paragraphs of papers\/articles that are most relevant to a concept or question they have related to Covid-19 or other viruses studied in the past thus saving time and effort. In addition to finding the information relevant to the official tasks posed in the challenge, the tool allows one to find information related to orthogonal or subsequent questions e.g. What is the target of a drug x? or Which drugs have similar effect?. It can also be used to find correlation between two seemingly independent concepts. "}}