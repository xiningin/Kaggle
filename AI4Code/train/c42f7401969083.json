{"cell_type":{"01e77d55":"code","82d98be7":"code","58d8af43":"code","aea78446":"code","13ccfa67":"code","6890b294":"code","bfdb0857":"code","118a6f65":"code","eb402a82":"code","7f3756cb":"code","2e2062ae":"code","a007d5b3":"code","2f50d68a":"code","611f82b6":"code","2026e5ef":"code","e5deb0b2":"code","0c4b7c40":"code","18f094d5":"code","76de9538":"code","c1f14470":"code","401529ac":"code","03dd7366":"code","331bc5c1":"code","a7b940fe":"code","20bf7645":"code","441081ac":"code","f7e7382d":"code","d5d8e8cc":"code","6fca0d95":"code","574347b3":"code","5552b723":"code","cb1aedf8":"code","c3ced313":"code","5001a5e7":"code","d25ac4a5":"code","69b584cf":"code","0a29a4ee":"code","dce83cd5":"code","65974f00":"code","5d58825b":"code","f35d6434":"code","3a507a14":"code","f8423ed1":"code","65eb6061":"code","c2f0e1bd":"code","4fc7d2cd":"markdown","ded10663":"markdown","cc6960ec":"markdown","dd3102dd":"markdown","b3abcd5e":"markdown","9b260ec3":"markdown","73f50ddf":"markdown","d5ceb762":"markdown","55f91b8a":"markdown","58b08863":"markdown","0cf858db":"markdown","d4801eff":"markdown","2cc5a833":"markdown","120951e1":"markdown","d2fc994a":"markdown","7130f250":"markdown","8079afb8":"markdown","b9cab1ce":"markdown","e2615e93":"markdown","9c24d1e8":"markdown","8ef06dcb":"markdown","e0525c89":"markdown","40e3cd2d":"markdown","b0cab6fa":"markdown","e3c1458b":"markdown","c9403cf3":"markdown","fc60c8e9":"markdown","db363376":"markdown","624d25e6":"markdown","3b0be21b":"markdown","bdb64db9":"markdown","1b131907":"markdown"},"source":{"01e77d55":"import torch\nfrom torch import optim\nimport numpy as np\n\nfrom PIL import Image \n\nimport pickle\nimport numpy as np\nfrom skimage import io\n\nfrom tqdm import tqdm, tqdm_notebook\nfrom pathlib import Path\n\nfrom torchvision import transforms\nfrom torchvision import models\nfrom multiprocessing.pool import ThreadPool\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom torch.optim import lr_scheduler\n\nimport os\nimport pandas as pd\nimport time\n\nfrom matplotlib import colors, patches, pyplot as plt\nfrom matplotlib.font_manager import FontProperties\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)\n\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","82d98be7":"class SimpsonsDataset(Dataset):\n    \n    def __init__(self, files, mode):\n        super().__init__()\n        # dataset for download\n        self.files = sorted(files)\n        # dataset mode by model mode\n        self.mode = mode\n\n        if self.mode not in DATA_MODES:\n            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n            raise NameError\n\n        self.len_ = len(self.files)\n     \n        self.label_encoder = LabelEncoder()\n\n        if self.mode != 'test':\n            self.labels = [path.parent.name for path in self.files]\n            self.label_encoder.fit(self.labels)\n\n            with open('label_encoder.pkl', 'wb') as le_dump_file:\n                  pickle.dump(self.label_encoder, le_dump_file)\n                      \n    def __len__(self):\n        return self.len_\n      \n    def load_sample(self, file):\n        image = Image.open(file)\n        image.load()\n        return image\n  \n    def __getitem__(self, index, RESCALE_SIZE=256):\n        # transformation by mode and normalization\n        transformers = {\n            'train': \n            transforms.Compose([\n                      transforms.RandomResizedCrop(size=RESCALE_SIZE),\n                      transforms.RandomPerspective(distortion_scale=0.1),\n                      transforms.RandomRotation(degrees=15),\n                      transforms.ColorJitter(),\n                      transforms.RandomHorizontalFlip(),\n                      transforms.CenterCrop(size=RESCALE_SIZE),  # Imagenet standards\n                      transforms.ToTensor(),\n                      transforms.Normalize([0.485, 0.456, 0.406],\n                                           [0.229, 0.224, 0.225])            \n            ]),\n            'val':\n            transforms.Compose([\n                      transforms.Resize(size=RESCALE_SIZE+20),\n                      transforms.CenterCrop(size=RESCALE_SIZE),\n                      transforms.ToTensor(),\n                      transforms.Normalize([0.485, 0.456, 0.406],\n                                           [0.229, 0.224, 0.225])\n            ]),\n            'test':\n            transforms.Compose([\n                      transforms.Resize(size=RESCALE_SIZE+20),\n                      transforms.CenterCrop(size=RESCALE_SIZE),\n                      transforms.ToTensor(),\n                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n            ])\n        }\n\n        x = self.load_sample(self.files[index])\n\n        # mode -> transformer\n        if self.mode == 'train':\n            x = transformers['train'](x)\n            label = self.labels[index]\n            label_id = self.label_encoder.transform([label])\n            y = label_id.item()\n            return x, y\n\n        elif self.mode == 'val':\n            x = transformers['val'](x)\n            label = self.labels[index]\n            label_id = self.label_encoder.transform([label])\n            y = label_id.item()\n            return x, y\n\n        else:\n            x = transformers['test'](x)\n            return x","58d8af43":"# Read Annotation file\nannotation = pd.read_csv('..\/input\/the-simpsons-characters-dataset\/annotation.txt',header=None)\nannotation.columns = ['images', 'x1','y1','x2','y2','character']\nannotation.images = annotation.images.apply(lambda x: x.split('characters')[1])\nannotation","aea78446":"# Empty lists\ntraindir = '..\/input\/the-simpsons-characters-dataset\/simpsons_dataset\/'\ncategories = []\nimg_categories = []\nn_train = []\nhs = []\nws = []\n\n# Iterate through each category\nfor d in os.listdir(traindir):\n    if d == 'simpsons_dataset':\n        continue\n    print(d)\n    categories.append(d)\n\n    # Number of each image\n    train_imgs = os.listdir(traindir + d)\n    n_train.append(len(train_imgs))\n\n    # Find stats for train images\n    for i in train_imgs:\n        img_categories.append(d)\n        try:\n            img = Image.open(traindir + d + '\/' + i)\n        except:\n            #..\/input\/the-simpsons-characters-dataset\/simpsons_dataset\/rainier_wolfcastle\/\n            #train_imgs = os.listdir(traindir + i)\n            img = Image.open(traindir + i)\n        img_array = np.array(img)\n        # Shape\n        hs.append(img_array.shape[0])\n        ws.append(img_array.shape[1])\n\n# Dataframe of categories\ncat_df = pd.DataFrame({'category': categories,\n                       'n_train': n_train,\n                       }).sort_values('category')\n\n# Dataframe of training images\nimage_df = pd.DataFrame({\n    'category': img_categories,\n    'height': hs,\n    'width': ws\n})\n\ncat_df.sort_values('n_train', ascending=False, inplace=True)","13ccfa67":"cat_df.tail(10)","6890b294":"cat_df.to_csv('cat_info.csv')","bfdb0857":"plt.figure(figsize=(25, 10))\nax = sns.barplot(x = cat_df.category, y = cat_df.n_train)\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, horizontalalignment='right')\nNone","118a6f65":"print ('image_height_min =', image_df.height.min(), ', image_width_min =', image_df.width.min())\nprint ('image_height_max =', image_df.height.max(), ', image_width_max =', image_df.width.max())","eb402a82":"TRAIN_DIR = Path('..\/input\/the-simpsons-characters-dataset\/simpsons_dataset\/')\nTEST_DIR = Path('..\/input\/the-simpsons-characters-dataset\/kaggle_simpson_testset\/kaggle_simpson_testset\/')\n\ntrain_val_files = sorted(list(TRAIN_DIR.rglob('*.jpg')))\ntest_files = sorted(list(TEST_DIR.rglob('*.jpg')))","7f3756cb":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","2e2062ae":"# different modes of dataset \nDATA_MODES = ['train', 'val', 'test']\n\n# size for rescale\nSIZE = 256\n\n# set device\nDEVICE = torch.device(device)","a007d5b3":"from sklearn.model_selection import train_test_split\n\ntrain_val_labels = [path.parent.name for path in train_val_files]\ntrain_files, val_files = train_test_split(train_val_files, test_size=0.2, \\\n                                          stratify = train_val_labels, random_state = 1)","2f50d68a":"train_files_labels = [path.parent.name for path in train_files]\nval_files_labels = [path.parent.name for path in val_files]","611f82b6":"print('Train dataset len:', len(train_files_labels), 'n_classes:', len(set(train_files_labels)), '\\n'\n      'Val dataset len:', len(val_files_labels), 'n_classes:', len(set(val_files_labels)))","2026e5ef":"train_dataset = SimpsonsDataset(train_files, mode='train')\nval_dataset = SimpsonsDataset(val_files, mode = 'val')\ntest_dataset = SimpsonsDataset(test_files, mode = 'test')","e5deb0b2":"def imshow(inp, title=None, plt_ax=plt, default=False):\n    \"\"\"Imshow for tensors\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt_ax.imshow(inp)\n    if title is not None:\n        plt_ax.set_title(title)\n    plt_ax.grid(False)\n\n\n# grid of random images from train dataset\ndef imshow_grid(nrows, ncols, mode):\n    fig, ax = plt.subplots(nrows=nrows, ncols=ncols,figsize=(16, 8), \\\n                            sharey=True, sharex=True)\n    for fig_x in ax.flatten():\n        \n        if mode == 'train':\n            random_characters = int(np.random.uniform(0, len(train_dataset)))\n            img, label = train_dataset[random_characters]\n            img_label = \" \".join(map(lambda x: x.capitalize(),\\\n                     train_dataset.label_encoder.inverse_transform([label])[0].split('_')))\n        \n        elif mode == 'val':\n            random_characters = int(np.random.uniform(0, len(val_dataset)))\n            img, label = val_dataset[random_characters]\n            img_label = \" \".join(map(lambda x: x.capitalize(),\\\n                     val_dataset.label_encoder.inverse_transform([label])[0].split('_')))\n\n        imshow(img.data.cpu(), title=img_label,plt_ax=fig_x)","0c4b7c40":"imshow_grid(2, 4, 'train')","18f094d5":"imshow_grid(2, 4, 'val')","76de9538":"def fit_epoch(model, train_loader, criterion, optimizer):\n    running_loss = 0.0\n    running_corrects = 0\n    processed_data = 0\n\n    for inputs, labels in train_loader:\n        inputs = inputs.to(DEVICE)\n        labels = labels.to(DEVICE)\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        preds = torch.argmax(outputs, 1)\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n        processed_data += inputs.size(0)\n\n    # true loss and accuracy \n    train_loss = running_loss \/ processed_data \n    train_acc = running_corrects.cpu().numpy() \/ processed_data\n\n    return train_loss, train_acc","c1f14470":"def eval_epoch(model, val_loader, criterion):\n    model.eval() \n    running_loss = 0.0\n    running_corrects = 0\n    processed_data = 0\n\n    for inputs, labels in val_loader:\n        inputs = inputs.to(DEVICE)\n        labels = labels.to(DEVICE)\n\n        with torch.set_grad_enabled(False):\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            preds = torch.argmax(outputs, 1)\n\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n        processed_data += inputs.size(0)\n\n    val_loss = running_loss \/ processed_data \n    val_acc = running_corrects.cpu().numpy() \/ processed_data\n\n    return val_loss, val_acc","401529ac":"def train(train_dataset, val_dataset, model, epochs, batch_size, criterion, optimizer, scheduler):\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n    since = time.time()\n\n    best_model_wts = model.state_dict()\n    best_acc = 0.0\n\n\n    history = []\n    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n    val_loss {v_loss:0.4f} \\n train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n\n    with tqdm(desc='epoch', total=epochs) as pbar_outer:\n        for epoch in range(epochs):\n            train_loss, train_acc = fit_epoch(model, train_loader, criterion, optimizer)\n\n            scheduler.step()\n            \n            val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n\n            history.append((train_loss, train_acc, val_loss, val_acc))\n\n            if val_acc > best_acc:\n                best_acc = val_acc\n                best_model_wts = model.state_dict()\n\n            pbar_outer.update(1)\n            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss, v_loss=val_loss,\\\n                                           t_acc=train_acc, v_acc=val_acc))\n            \n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60\n    )) \n    print('Best val acc: {:4f}'.format(best_acc))\n\n    # best model's params\n    model.load_state_dict(best_model_wts)\n\n    return model, history","03dd7366":"def val_training(train_dataset, model, epochs, batch_size, criterion, optimizer, scheduler):\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n    since = time.time()\n\n    best_model_wts = model.state_dict()\n    best_acc = 0.0\n\n\n    history = []\n    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} train_acc {t_acc:0.4f}\"\n\n    with tqdm(desc='epoch', total=epochs) as pbar_outer:\n        for epoch in range(epochs):\n            train_loss, train_acc = fit_epoch(model, train_loader, criterion, optimizer)\n\n            scheduler.step()\n\n            history.append((train_loss, train_acc))\n\n            if train_acc > best_acc:\n                best_acc = train_acc\n                best_model_wts = model.state_dict()\n\n            pbar_outer.update(1)\n            tqdm.write(log_template.format(ep=epoch+1,\n                                           t_loss=train_loss, \n                                           t_acc=train_acc))\n            \n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60\n    )) \n    print('Best val acc: {:4f}'.format(best_acc))\n\n    # best model's params\n    model.load_state_dict(best_model_wts)\n\n    return model, history","331bc5c1":"def predict(model, test_loader):\n    with torch.no_grad():\n        logits = []\n\n        for inputs in test_loader:\n            inputs = inputs.to(DEVICE)\n            model.eval()\n            outputs = model(inputs).cpu()\n            logits.append(outputs)\n\n    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n    return probs","a7b940fe":"def show_loss(history):\n    loss, acc, val_loss, val_acc = zip(*history)\n    plt.figure(figsize=(12, 8))\n    plt.plot(loss, label=\"train\")\n    plt.plot(val_loss, label=\"val\")\n    plt.legend()\n    plt.show()","20bf7645":"vgg_16 = models.vgg16(pretrained=True)\nvgg_16","441081ac":"# freeze the layers\nlayers_to_unfreeze = 20\nfor param in vgg_16.features[:-layers_to_unfreeze].parameters():\n    param.requires_grad = False","f7e7382d":"# dense in_features\nnum_features = 25088\n\n# num of classes to classificztion\nn_classes = len(np.unique(train_val_labels))\n\n# replace fully-connected layer by our classification linear layer\nvgg_16.classifier = nn.Sequential(\n                    nn.Linear(in_features=num_features, out_features=4096, bias=True),\n                    nn.ReLU(inplace=True),\n                    nn.Dropout(p=0.5, inplace=False),\n                    nn.Linear(in_features=4096, out_features=4096, bias=True),\n                    nn.ReLU(inplace=True),\n                    nn.Dropout(p=0.5, inplace=False),\n                    nn.Linear(in_features=4096, out_features=n_classes)\n)\n\nvgg_16.to(DEVICE)\n\n# cost function\ncriterion_vgg = nn.CrossEntropyLoss()\n\n# AdamW as optimizer\noptimizer_vgg = optim.AdamW(list(vgg_16.features.parameters())[-layers_to_unfreeze:] + \n                            list(vgg_16.classifier.parameters()), lr=3e-4)\n\n# scheduler init\nscheduler_vgg = lr_scheduler.StepLR(optimizer_vgg, step_size=7, gamma=0.1)","d5d8e8cc":"%%time\nvgg_16, history = train(train_dataset, val_dataset, vgg_16, epochs=20, batch_size=64, \n                        criterion=criterion_vgg,\n                        optimizer=optimizer_vgg,\n                        scheduler=scheduler_vgg)","6fca0d95":"show_loss(history)","574347b3":"torch.save(vgg_16.state_dict(), 'vgg_16.pth')","5552b723":"from torchvision import models\nmobNet = models.mobilenet_v3_small(pretrained=True)\nmobNet","cb1aedf8":"num_features = 576\nn_classes = len(np.unique(train_files_labels))\nmobNet.classifier = nn.Sequential(\n                                  nn.Linear(num_features, 1024, bias=True),\n                                  nn.Hardswish(),\n                                  nn.Dropout(p=0.2, inplace=True),\n                                  nn.Linear(1024, n_classes, bias=True) \n)\n\nmobNet.to(DEVICE)\n\ncriterion_mobNet = nn.CrossEntropyLoss()\n\noptimizer_mobNet = optim.AdamW(mobNet.parameters(), lr=3e-4)\n\nscheduler_mobNet = lr_scheduler.StepLR(optimizer_mobNet, step_size=7, gamma=0.1)","c3ced313":"mobNet, history = train(train_dataset, val_dataset, mobNet, epochs=10, batch_size=64, \n                        criterion=criterion_mobNet,\n                        optimizer=optimizer_mobNet,\n                        scheduler=scheduler_mobNet\n                        )","5001a5e7":"mobNet, history = val_training(val_dataset, mobNet, epochs = 5, batch_size=64,\n                               criterion=criterion_mobNet,\n                               optimizer=optimizer_mobNet,\n                               scheduler=scheduler_mobNet)","d25ac4a5":"torch.save(mobNet.state_dict(), 'mobNetv3small.pth')","69b584cf":"mobNetlarge = models.mobilenet_v3_large(pretrained=True)\nmobNetlarge","0a29a4ee":"num_features = 960\nn_classes = len(np.unique(train_files_labels))\nmobNetlarge.classifier = nn.Sequential(\n                                  nn.Linear(num_features, 1280, bias=True),\n                                  nn.Hardswish(),\n                                  nn.Dropout(p=0.2, inplace=True),\n                                  nn.Linear(1280, n_classes, bias=True) \n)\n\nmobNetlarge.to(DEVICE)\n\ncriterion_mobNetlarge = nn.CrossEntropyLoss()\n\noptimizer_mobNetlarge = optim.AdamW(mobNetlarge.parameters(), lr=3e-4)\n\nscheduler_mobNetlarge = lr_scheduler.StepLR(optimizer_mobNetlarge, step_size=7, gamma=0.1)","dce83cd5":"mobNetlarge, history = train(train_dataset, val_dataset, mobNetlarge, epochs=15, batch_size=64, \n                        criterion=criterion_mobNetlarge,\n                        optimizer=optimizer_mobNetlarge,\n                        scheduler=scheduler_mobNetlarge\n                        )","65974f00":"show_loss(history)","5d58825b":"torch.save(mobNetlarge.state_dict(), 'mobNetLarge.pth')","f35d6434":"def predict_one_sample(model, inputs, device=DEVICE):\n    with torch.no_grad():\n        inputs = inputs.to(device)\n        model.eval()\n        logit = model(inputs).cpu()\n        probs = torch.nn.functional.softmax(logit, dim=-1).numpy()\n    return probs","3a507a14":"from sklearn.metrics import f1_score\ndef f1_val_score(num_samples, model):\n    idxs = list(map(int, np.random.uniform(0, len(val_dataset), num_samples)))\n    imgs = [val_dataset[id][0].unsqueeze(0) for id in idxs]\n\n    probs_ims = predict(model, imgs)\n\n    y_pred = np.argmax(probs_ims,-1)\n    actual_labels = [val_dataset[id][1] for id in idxs]\n\n    return f1_score(actual_labels, y_pred, average='micro')","f8423ed1":"label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))","65eb6061":"def preds_visual(nrows, ncols, model, label_encoder):\n\n    fig, ax = plt.subplots(nrows=nrows, ncols=ncols,figsize=(16, 8), \\\n                            sharey=True, sharex=True)\n    for fig_x in ax.flatten():\n        random_characters = int(np.random.uniform(0, len(val_dataset)))\n        im_val, label = val_dataset[random_characters]\n        img_label = \" \".join(map(lambda x: x.capitalize(),\\\n                    val_dataset.label_encoder.inverse_transform([label])[0].split('_')))\n\n\n\n        imshow(im_val.data.cpu(), title=img_label, plt_ax=fig_x)\n\n        actual_text = \"Actual : {}\".format(img_label)\n\n        fig_x.add_patch(patches.Rectangle((0, 231),256,35,color='white'))\n        font0 = FontProperties()\n        font = font0.copy()\n        font.set_family(\"sans-serif\")\n        prob_pred = predict_one_sample(model, im_val.unsqueeze(0))\n        predicted_proba = np.max(prob_pred)*100\n        y_pred = np.argmax(prob_pred)\n\n        predicted_label = label_encoder.classes_[y_pred]\n        predicted_label = predicted_label[:len(predicted_label)\/\/2] + '\\n' + predicted_label[len(predicted_label)\/\/2:]\n        predicted_text = \"{} : {:.0f}%\".format(predicted_label,predicted_proba)\n\n        fig_x.text(1, 231, predicted_text , horizontalalignment='left', fontproperties=font,\n                        verticalalignment='top',fontsize=9, color='black',fontweight='bold')","c2f0e1bd":"class SimpleCnn(nn.Module):\n  \n    def __init__(self, n_classes):\n        super().__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3),\n            nn.BatchNorm2d(8),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=96, kernel_size=3),\n            nn.BatchNorm2d(96),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        \n        self.out = nn.Sequential(\n            nn.Dropout(p=0.5),\n            nn.Linear(96 * 5 * 5, 96),\n            nn.Linear(96, n_classes)\n        )\n  \n  \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.conv5(x)\n\n        x = x.view(x.size(0), -1)\n        logits = self.out(x)\n        return logits","4fc7d2cd":"Function for visualization random image from train and val datasets","ded10663":"F1 score for random sample of validation data","cc6960ec":"saving model","dd3102dd":"# Train loop","b3abcd5e":"split on train and val datasets","9b260ec3":"# VGG_16","73f50ddf":"# EOF","d5ceb762":"# Visualization for predictions","55f91b8a":"Training","58b08863":"# Functions for model's training ","0cf858db":"Function to predict one image","d4801eff":"Epoch training","2cc5a833":"loss","120951e1":"Epoch validation","d2fc994a":"predictions visualization","7130f250":"# MobileNetV3_small\n","8079afb8":"# Statistic","b9cab1ce":"loop to train on validation dataset.\nI want to try this because we have unbalanced classes.\n\nupd. it doesn't really help.","e2615e93":"# Simpsons' class","9c24d1e8":"# Create datasets: train, val, test","8ef06dcb":"saving model","e0525c89":"probs prediction function","40e3cd2d":"# SimpleCNN","b0cab6fa":"[reference for statistic](https:\/\/github.com\/WillKoehrsen\/pytorch_challenge\/blob\/master\/Transfer%20Learning%20in%20PyTorch.ipynb) ","e3c1458b":"# Hi, kagglers!\n\n### This is my pet-research project, doing this to know more about web interactive app\n#### Feel free to comment and upvote\n### Also you can check streamlit version of models from this notebook, just this [APP](https:\/\/share.streamlit.io\/dailysergey\/streamlit-simpsons\/web_app.py) out ","c9403cf3":"# Functions for image visualization","fc60c8e9":"# Packages import","db363376":"# Have a look on train and val images.","624d25e6":"There are three classes with less than 10 samples.\n\nLet's visualizate this information.","3b0be21b":"# MobNet v3 large","bdb64db9":"# Data download","1b131907":"Loss visualization"}}