{"cell_type":{"60942443":"code","3f5abfff":"code","59a27a1f":"code","e4fc08cd":"code","aea22c35":"code","5b8ec8b5":"code","f00d571c":"code","1359bdfe":"code","385862f8":"code","1bafddd8":"code","09a9419f":"code","53626dce":"code","184dad45":"code","fc6fff82":"code","8c208bc5":"code","49589721":"code","7e157de9":"code","91824ec0":"code","b3158397":"code","955f8866":"code","3f1b3c65":"code","03f0655d":"code","d2011e7d":"code","41f62dbf":"code","8fbdd82b":"code","0c8d2f48":"code","c1ae2f3f":"code","88b611b7":"code","a1be6d97":"code","ec6d33c8":"code","58a9b91c":"code","453ed373":"code","b0923add":"code","33a1d544":"code","57d4c6eb":"markdown","f7976d7f":"markdown","37944d22":"markdown","ca2ab731":"markdown","4255f933":"markdown","c3b180af":"markdown","9a601e45":"markdown","5ab13b7b":"markdown","600526d5":"markdown","888e8b85":"markdown","8fec887b":"markdown","d6a0d1c4":"markdown","ff6afdb9":"markdown","928d8c83":"markdown","2e3161e2":"markdown","8c873f24":"markdown","ef4b9534":"markdown","4a66c504":"markdown","b483bfab":"markdown","b4bf9493":"markdown","2cb580f7":"markdown","2524948f":"markdown","ab6cf317":"markdown","41928f5a":"markdown","72640d9e":"markdown","76aed929":"markdown","9ad03988":"markdown"},"source":{"60942443":"# These are all the modules we'll be using later. Make sure you can import them\n# before proceeding further.\nfrom __future__ import print_function\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport sys\nfrom six.moves import cPickle as pickle\nimport pandas as pd\nimport gzip\nimport seaborn as sns\nimport string\nfrom time import time\nimport nltk\nfrom nltk.corpus import stopwords \nstops = set(stopwords.words(\"english\"))\nimport urllib.request as tr\n#urllib.request.urlretrieve.\n\n\nfrom IPython.display import display # Allows the use of display() for DataFrames\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\nFIG_SIZE = (14,8)\n# Config the matlotlib backend as plotting inline in IPython\n%matplotlib inline\nDATASET_NAME ='reviews_Musical_Instruments.json.gz'\n#Random state for classifiers\nRAN_STATE = 42","3f5abfff":"#removes extensions and adds pickle.\npickle_file_name = (DATASET_NAME[:-8]+'.pickle')\n#pickle file is saved binary format.\n\n#loads pickle if exists, extracts and pickles if it doesn't\nif os.path.exists(pickle_file_name):\n    print ('Pickled file already present, loading...')\n    data = pd.read_pickle(pickle_file_name)\n    print ('Pickle file loaded.')\nelse:\n    data = pd.read_json(\"..\/input\/amazon-musical-instruments\/reviews_Musical_Instruments.json\",\n                        lines=True,\n                        orient='columns')\n\n    data.to_pickle(pickle_file_name)\nprint(data.shape)\ndata.head()\n","59a27a1f":"#select the columns\ndf = data.iloc[:, [5,4,3]]\n\n#split numerator and denominator\ndf['helpful_numerator'] = df['helpful'].apply(lambda x: x[0])\ndf['helpful_denominator'] = df['helpful'].apply(lambda x: x[1])\n\n# delete un-needed 'helpful catagory\ndel df['helpful']\n\n#Check if we have any null values\nprint (df.isnull().sum())","e4fc08cd":"df.describe()","aea22c35":"#include reviews that have more than 10 helpfulness data point only\ndf1 = df[(df.helpful_denominator > 10)].copy()\ndf1.shape","5b8ec8b5":"#transform Helpfulness into a binary variable with 0.50 ratio\nthreshold = 0.5\ndf1.loc[:, 'Helpful'] = np.where(df1.loc[:, 'helpful_numerator'] \\\n                                 \/ df1.loc[:, 'helpful_denominator'] > threshold, 1, 0)\ndf1.head(3)","f00d571c":"#Check the balance\nprint ('Count:')\ndisplay(df1.groupby('Helpful').count())","1359bdfe":"#Visualize correlation of the data\ncorrelations = df1.corr()\nplt.figure(figsize = FIG_SIZE)\nplt.title(\"Heatmap of correlations in each catagory\")\n_ = sns.heatmap(correlations, vmin=0, vmax=1, annot=True)","385862f8":"df1","1bafddd8":"# convert text to lowercase\ndf1.loc[:, 'reviewText'] = df1['reviewText'].str.lower()\n\ndef remove_punctuation(text):\n    return text.translate(string.punctuation.translate( '\"'))\n\ndf1['reviewText']=df1['reviewText'].apply(lambda x: remove_punctuation(x))\ndf1['reviewText'].head(4)","09a9419f":"#tokenize text with Tfidf\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.stem.snowball import SnowballStemmer\n\n#create a stemmer\nstemmer = SnowballStemmer(\"english\")\n\n\n#define our own tokenizing function that we will pass into the TFIDFVectorizer. We will also stem the words here.\ndef tokens(x):\n    x = x.split()\n    stems = []\n    [stems.append(stemmer.stem(word)) for word in x]\n    return stems\n\n#define the vectorizer\nvectorizer = TfidfVectorizer(tokenizer = tokens, stop_words = 'english', ngram_range=(1, 1), min_df = 0.01)\n#fit the vectorizers to the data.\nfeatures = vectorizer.fit_transform(df1['reviewText'])\nfeatures","53626dce":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features,df1['Helpful'], test_size=0.2, random_state=RAN_STATE)","184dad45":"from sklearn.metrics import roc_auc_score, roc_curve\n\ndef train_classifier(clf, X_train, y_train):\n    ''' Fits a classifier to the training data. '''\n    \n    # Start the clock, train the classifier, then stop the clock\n    start = time()\n    clf.fit(X_train, y_train)\n    end = time()\n    \n    # Print the results\n    print (\"Trained model in {:.4f} seconds\".format(end - start))\n\n    \ndef predict_labels(clf, features, target):\n    ''' Makes predictions using a fit classifier based on roc_auc score. '''\n    \n    # Start the clock, make predictions, then stop the clock\n    start = time()\n    probas = clf.predict_proba(features)\n    end = time()\n    \n    # Print and return results\n    print (\"Made predictions in {:.4f} seconds.\".format(end - start))\n    return roc_auc_score(target.values, probas[:,1].T)\n\n\ndef train_predict(clf, X_train, y_train, X_test, y_test):\n    ''' Train and predict using a classifer based on roc_auc score. '''\n    \n    # Indicate the classifier and the training set size\n    print (\"Training a {} using a training set size of {}. . .\".format(clf.__class__.__name__, X_train.shape[0]))\n    \n    # Train the classifier\n    train_classifier(clf, X_train, y_train)\n    \n    # Print the results of prediction for both training and testing\n    print (\"ROC_AUC score for training set: {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n    print (\"ROC_AUC score for test set: {:.4f}.\\n\".format(predict_labels(clf, X_test, y_test)))\n    \ndef clf_test_roc_score(clf, X_train, y_train, X_test, y_test):\n    clf.fit(X_train, y_train)\n    probas = probas =clf.predict_proba(X_test)\n    return roc_auc_score(y_test, probas[:,1].T)","fc6fff82":"# Import the supervised learning models from sklearn\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n# Initialize the models using a random state were applicable.\nclf_list = [GaussianNB(), \n            AdaBoostClassifier(random_state = RAN_STATE), \n            RandomForestClassifier(random_state = RAN_STATE), \n            LogisticRegression(random_state = RAN_STATE),\n            DecisionTreeClassifier(random_state = RAN_STATE)]\nx_tr = X_train.toarray()\nx_te = X_test.toarray()\n\n\n# Set up the training set sizes for 100, 200 and 300 respectively.\ntrain_feature_list = [x_tr[0:5000],x_tr[0:10000],x_tr]\ntrain_target_list = [y_train[0:5000], y_train[0:10000], y_train]\n\n\n# Execute the 'train_predict' function for each of the classifiers and each training set size\nfor clf in clf_list:\n    for a, b in zip(train_feature_list, train_target_list):\n        train_predict(clf, a, b, x_te, y_test)","8c208bc5":"FIG_SIZE = (14,8)\n### Visualize all of the classifiers                                                               \nfor clf in clf_list:\n    x_graph = []\n    y_graph = []\n    for a, b in zip(train_feature_list, train_target_list):\n        y_graph.append(clf_test_roc_score(clf, a, b, x_te, y_test))\n        x_graph.append(len(a))\n    plt.scatter(x_graph,y_graph)\n    plt.plot(x_graph,y_graph, label = clf.__class__.__name__)\n\nplt.title('Comparison of Different Classifiers')\nplt.xlabel('Training Size')\nplt.ylabel('ROC_AUC score on test set')\nplt.legend(bbox_to_anchor=(1.6, 1.05))\nplt.figure(figsize=FIG_SIZE)             \nplt.show()","49589721":"#add Score column to features\nimport scipy as scipy\n\noverall = np.array(list(df1.overall))\noverall = overall.reshape(features.shape[0], 1)\n\nfeatures = scipy.sparse.hstack((features,scipy.sparse.csr_matrix(overall)))\n\nfeatures = scipy.sparse.csr_matrix(features)\nfeatures\n","7e157de9":"X_train2, X_test2, y_train, y_test = train_test_split(features, df1['Helpful'], test_size=0.2, random_state=RAN_STATE)","91824ec0":"from sklearn.model_selection import GridSearchCV,cross_validate,StratifiedKFold\n#make the grid search object\ngs2 = GridSearchCV(\n    estimator=LogisticRegression(),\n    param_grid={'C': [10**i for i in range(-5,5)], 'class_weight': [None, 'balanced']},\n    cv=StratifiedKFold(n_splits=5),\n    scoring='roc_auc'\n)\n\n#fit the grid search object to our new dataset\nprint ('Fitting grid search...')\ngs2.fit(X_train2, y_train)\nprint (\"Grid search fitted.\")","b3158397":"#print the grid search scores.\nprint (gs2.best_estimator_)\n#gs2.grid_scores_\ngs2.cv_results_","955f8866":"clf2 = gs2.best_estimator_\nprobas =clf2.predict_proba(X_test2)\nplt.figure(figsize = FIG_SIZE)\nplt.plot(roc_curve(y_test, probas[:,1])[0], roc_curve(y_test, probas[:,1])[1])\nplt.title('ROC Curve for Helpful Rating')\nplt.grid()\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.show\n\n# ROC\/AUC score\nprint ('ROC_AUC Score:',roc_auc_score(y_test, probas[:,1].T))","3f1b3c65":"random_numbers = range(1,101)\ntot = 0.0\nfor seed in random_numbers:\n    clf3 = LogisticRegression(random_state=seed)\n    clf3.fit(X_train2, y_train)\n    probas =clf3.predict_proba(X_test2)\n    tot += roc_auc_score(y_test, probas[:,1].T)\n    \nprint ('Average ROC_AUC Score for 1-100 random_state: {:.4f}'.format(tot\/100))","03f0655d":"clf = LogisticRegression()\nclf.fit(X_train,y_train)\nprobas = clf.predict_proba(X_test)\nclf2 = gs2.best_estimator_\nprobas2 =clf2.predict_proba(X_test2)\nplt.figure(figsize = FIG_SIZE)\n\nplt.plot(roc_curve(y_test, probas[:,1])[0], roc_curve(y_test, probas[:,1])[1], label = 'TFIDF')\nplt.plot(roc_curve(y_test, probas2[:,1])[0], roc_curve(y_test, probas2[:,1])[1], label = 'TFIDF + overall')\nplt.title('ROC Curve for Helpful Rating')\nplt.grid()\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\n\nplt.legend(bbox_to_anchor=(1.0, .5))\nplt.figure(figsize=FIG_SIZE)\nplt.show()\n\n# ROC\/AUC score\nprint ('ROC_AUC Score:',roc_auc_score(y_test, probas[:,1].T))","d2011e7d":"data = pd.read_csv('..\/input\/amazon-musical-instruments\/Musical_instruments_reviews.csv')\ndata.head()\n","41f62dbf":"#select the columns\ndf2 = data.iloc[:, [5,4]]\n\n#split numerator and denominator\n\n# delete un-needed 'helpful catagory\ndf2.head(10)","8fbdd82b":"conditions = [\n    (df2['overall'] <= 2),\n    (df2['overall'] == 3),\n    (df2['overall'] > 3)\n    ]\n\n# create a list of the values we want to assign for each condition\nvalues = ['Negative', 'Neutral', 'Positive']\n\n# create a new column and use np.select to assign values to it using our lists as arguments\ndf2['User_Sentiment'] = np.select(conditions, values)\n\n# display updated DataFrame\ndf2.head(10)","0c8d2f48":"df2.User_Sentiment.value_counts()","c1ae2f3f":"df2.dropna(\n    axis=0,\n    how='any',\n    thresh=None,\n    subset=None,\n    inplace=True\n)\ndf2.User_Sentiment.value_counts()","88b611b7":"# Import libraries\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\nfrom textblob import Word\n\n# Lower casing and removing punctuations\ndf2['reviewText'] = df2['reviewText'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ndf2['reviewText'] = df2['reviewText'].str.replace('[^\\w\\s]','')\n\nstop = stopwords.words('english')\ndf2['reviewText'] = df2['reviewText'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n\n# Lemmatization\n\ndf2['reviewText'] = df2['reviewText'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\ndf2.reviewText.head(5)","a1be6d97":"# Let's define our sentiment analyzer function:\npolarity_score =[]\ndef analyze_sentiment(cleaned_verified_reviews):\n    analysis = TextBlob(cleaned_verified_reviews)\n    if analysis.sentiment.polarity > 0.2:\n        polarity_score.append(analysis.sentiment.polarity)\n        return 'Positive'\n    elif analysis.sentiment.polarity >= -0.2 and analysis.sentiment.polarity <= 0.2 :\n        polarity_score.append(analysis.sentiment.polarity)\n        return 'Neutral'\n    else:\n        polarity_score.append(analysis.sentiment.polarity)\n        return 'Negative'","ec6d33c8":"df2['Review_Sentiment'] = df2['reviewText'].apply(lambda x: analyze_sentiment(x))\ndf2['Polarity_score'] = polarity_score\ndf2.head()\n","58a9b91c":"df2.Review_Sentiment.value_counts()","453ed373":"df2 = df2[df2.User_Sentiment != 'Neutral']\ndf2 = df2[df2.Review_Sentiment != 'Neutral']\nprint(df2.shape)\ndf2.head(10)","b0923add":"comparison_column = np.where(df2[\"User_Sentiment\"] == df2[\"Review_Sentiment\"], True, False)\ndf2[\"result\"] = comparison_column\ndf2.head()","33a1d544":"# Printing those results with 'Not Genuine' reviews ! \ndf2 = df2[df2.result != True]\nprint(df2.shape)\ndf2.head()","57d4c6eb":"# Text Feature Generation for Benchmark model\n\n* First we convert the text to lower case so we can run our natural language processing to produce features.\n* Remove punctuations. Removing punctuation may seem like it should not be done in the case of this problem,but it will cause the learning algorithm to behave poorly.","f7976d7f":"#### Now the next step is to divide the overall score into 3 categories . If user have given less than 3 star we categorize it as negative , for 3 star neutral and for more than 3 we mark it as Positive.","37944d22":"# Justification\n\nSo now our final model using 'overall' score and TFIDF features was able to score a value of 0.888 for the area under the ROC curve. Now if we compare this score to our benchmark model which scored 0.845 , this is a 4.3% improvement. As this was only the addition of one feature. The area under the AUC_ROC can be directly interpreted as accuracy.\n\nOur goal in the beginning of this project was to design a system to automatically classify new Reviews made on amazon products as 'helpful' or 'non-helpful'. Our optimum model will correctly do this 84.5% of the time. This means that 15.5% of the time, our system will not work. \n\n# Free-Form Visualization\n\nA important quality of this project is the effect of introducing a new key feature to a benchmark model. Mainly, we looked at the TFIDF features generated from amazon review text and added the 'overall_rating' that was given to the product by the reviewer. We used these features to predict how 'helpful' other users would find the review. Our results can be visualized as follows:\n","ca2ab731":"### We are for now concerned with only 'reviewText' and 'overall' . So we will select only those in our dataframe.","4255f933":"#### Now we will perform our algorithms using above function.\n#### We will train and test on three different training sizes in order to find which one will be our benchmark.","c3b180af":"# Amazon Review - Machine Learning Project\n\nMyself [Chetan Gadge](https:\/\/www.linkedin.com\/in\/chetan-gadge-7b1090189\/) along with [Shivam Parab](https:\/\/www.linkedin.com\/in\/shivam-parab-715b19199\/) are creating an ML based 'Prediction model of helpfulness' under the mentorship of  [Mr. Rocky Jagtiani](https:\/\/www.linkedin.com\/today\/author\/rocky-jagtiani-3b390649\/). This project was done as the final project for the Advanced ML course.\n\nIn this course we learned various concepts related to Machine Learning but mostly focused on Natural Language Processing (NLP). Concepts involved were text classification , summarization , LDA , LSA , Name Entity Recognition etc ...\n\nSo there exist two problems in front of us :\n## Problem Statements :-\n> 1. Pre-rate the reviews on their 'helpfulness' factor.\n> 2. To test Genuinity of 'reviewText' vs 'Score'.","9a601e45":"### Problem Statement 1 :- Pre-rate the review on their 'helpfulness' factor.\nThe problem is the poor quality of Amazon reviews at the top of the forum despite the \u201chelpfulness\u201d rating system. The problem is that new reviews to be placed at the top of the forum, for a chance to be rated by the community. The proposed solution to this problem is to use machine learning techniques to design a system that \u201cpre-rates\u201d new reviews on their \u201chelpfulness\u201d before they are given a position at the top of the forum. This way, poor quality reviews will be more unlikely to be shown at the top of the forum, as they do not get the \u201cfreepass\u201d because they are new. The proposed system will use a set of Amazon review data to train itself to predict a helpfulness classification (helpful, or not helpful) for new input data.","5ab13b7b":"# Preprocessing :-\n> * Remove Punctuations\n> * Lower Casing\n> * Remove Stopwords\n> * Lemmatization","600526d5":"### The amount in helpful_numerator and helpful_denominator wont matter to us much as they going to be transformed into labels later on.\n## Data Preprocessing\n* The count of helpful_denominator that has less than 10 ratings will first be trimmed out of the dataset.\n* There are huge number of reviews, and many do not face the need of people reading them . So these reviews could be good, but by chance, do not get read and rated, they will be rated negatively by our algorithms and affect negatively in our model to classify future \"good\" reviews.","888e8b85":"The average ROC_AUC score for 100 different random_states is equal to 0.888, the same as our optimum solution. In this regard, our solution can be considered robust.","8fec887b":"# Grid Search\n\nGrid search is a tuning technique that attempts to compute the optimum values of hyperparameters. It is an exhaustive search that is performed on a the specific parameter values of a model. The model is also known as an estimator.\n# Cross-Validation\n\nCross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.\n\nThe procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=5 becoming 5-fold cross-validation.\n\nCross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.\n\nIt is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train\/test split.","d6a0d1c4":"# Creating Model for Sentiment Analysis\nSo there are 4 types of Sentiment Analysis :-\n> * Fine-Grained Sentiment\n> * Emotion Detection Sentiment\n> * Aspect Based\n> * Intent Analysis\n\n#### But in this Case , we are going with Aspect Based Analyzer because usually, when analyzing sentiments of texts, let\u2019s say product reviews, you\u2019ll want to know which particular aspects or features people are mentioning in a positive, neutral, or negative way. That's where aspect-based sentiment analysis can help, for example in this text: \"The battery life of this camera is too short\", an aspect-based classifier would be able to determine that the sentence expresses a negative opinion about the feature battery life.\n\n#### Various techniques and methodologies have been developed to address automatically identifying the sentiment expressed in the text. In this project, I\u2019ll use TextBlob, a Python sentiment analysis library, to classify whether the reviews are positive, negative, or neutral.\n\n#### First, we\u2019ll import the TextBlob function from textblob library. I\u2019ll initialise the sentiment analyser from textblob, and then iterate over the 'reviewText' from the dataframe. I\u2019ll then calculate whether the compound sentiment score is above or beneath the thresholds so that we can assign them with the positive, negative, or neutral label.","ff6afdb9":"# Streamlit App :-\n   I have even created an app for the same depicting my model on [STREAMLIT](https:\/\/share.streamlit.io\/chetan-30\/musical-instruments_reviews-sentiment_analysis\/streamlit.py) where you can physically try my model and enter the reviews to try out the working of it .\n    ","928d8c83":"#### Now to be honest we are not concerned about the neutral reviews. In many cases we have seen user mentions couple of positive things about the product but is not fully satisified about the product and end up giving 3 stars as he dont want to mention negative things about it. But our model will rate it as Positive , so we dont want to contradict those reviews.\n\n#### Hence we are concerned only about those reviews which are termed as 'Positive' but are actually 'Negative' or vice-versa . So we are removing all those reviews which includes neutral in either of the column.","2e3161e2":"# Conclusion :- \n\n#### So we can clearly see 3rd review going as 'I love it , I used this for my Yamaha ypt-230 and it works great, I would recommend it to anyone' . User gave it a 2 star rating but clearly it is a positive review , Thus we need to filter them out as it can disturb overall ratings of that particular product and even new customers will be in a confused state.","8c873f24":"### So for the labels , we need to determine whether the 'reviewText' is helpful or not. So for that we need binary classification. For that we are going to set a threshold value (0.5) of 'helpful_numerator' divided by 'helpful_denominator'. In short , the ratio of people finding it helpful over amount of people rated. If it exceeds the threshold value , we can rate it as 'helpful = 1'  else 'helpful = 0'. ","ef4b9534":"# Refinement\n#### In order to improve our results we are going to try adding a new feature to our feature set. The 'overall' score category was shown before to have a correlation with 'helpful' or 0.46. Lets try adding it to the feature set and seeing how well the algorithm performs.","4a66c504":"#### So there were total 1024 neutral sentiments . Thus Removing them.\n\n#### So in the next cell we compare two columns , one 'User_Sentiments' rated by the user themselves and second 'Review_Sentiment' generated by our model by analysing user's reviews . Labelling it 'True' if they match concluding it as a genuine review where as labelling it 'False' if they dont match concluding it as a 'Sarcastic review' or 'not genuine'.","b483bfab":"#### In order to generate more features we will use the TF-IDF vectorizer from the sci-kit learn library. We will also use the NLTK library. The pre-processing methods that we are going to employ are as follows:\n\n> 1. Stemming : - Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form\u2014generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root.A stemmer for English operating on the stem cat should identify such strings as cats, catlike, and catty.\n\n> 2. Tokenizing : - Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens. It is essential to generate our TFIDF features.\n\n> 3. Removing Stop words :- Stop words are a set of commonly used words in any language. For example, in English, \u201cthe\u201d, \u201cis\u201d and \u201cand\u201d, would easily qualify as stop words. In NLP and text mining applications, stop words are used to eliminate unimportant words, allowing applications to focus on the important words instead.\n\n> 4. N-grams :- Makes groups of words that are 'n' long. E.g. the 2-grams for the sentence \"Red color Guitar\" are [Red, color] , [Red, Guitar] and [color, Guitar] . Weving more than that is computationally expensive for my computer. will stick to 2-grams and 1-grams for now. Having more than that is computationally expensive.\n\n#### Finally we will generate TF-IDF for each of the stemmed and tokenized words and ngrams. TF-IDF is short for Term Frequency Inverse Document frequency. TF-IDF is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining.\n\n>  TF: Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization: \n\n> TF(t) = (Number of times term t appears in a document) \/ (Total number of terms in the document).\n\n> IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:\n\n> IDF(t) = log_e(Total number of documents \/ Number of documents with term t in it). \n\nThe TF-IDF weight is the product of these two numbers. We will set min_df = 0.001 for getting rid of spelling mistakes.\nWords appearing infrequently are most likely typing errors and not interested for our algos thus reducing feature space and enabling our algorithm to work faster.","b4bf9493":"We can see the our optimized classifier is a LogisticRession with a 'C' parameter of 1 and a 'class_weight' = 'balanced'. This is the same as the default, meaning our optimization step did not actually change the parameters of the our of box model. We can now perform a prediction and measure it's results using a ROC visualization.","2cb580f7":"#### So here we are concerned about the column named 'helpful'. So the helpful score can be evaluated as whether the review was helpful for them or not . For e.g. [5 , 7] can be evaluated as there have been 7 total rating count out of which 5 is rated as helpful review and eventually 2 can considered as not helpful.\n#### So for the training ,  this label can be generated by dividing the \u2018helpful\u2019 ratings by the total ratings and check whether it exceeds a certain threshold value.\n#### So let's extract only those information we need for this project:\n*  The 'reviewText' will be used to generate features using natural language processing later.\n*  The 'score' will be used as a feature later for the final model but for now we keep it excluded for benchmark model.\n*  The 'helpful' rating will be used to generate our labels. We will train our model using these training labels. Predict the label using the test features, and measure the success of our model using the test labels . Will be explained in upcoming cells.","2524948f":"# Reference\n\n> Inferring networks of substitutable and complementary products. J. McAuley, R. Pandey, J. Leskovec Knowledge Discovery and Data Mining, 2015.\nImage-based recommendations on styles and substitutes J. McAuley, C. Targett, J. Shi, A. van den Hengel SIGIR, 2015.\n\n> HUI BWU Y. Anti-spam model based on semi-Naive Bayesian classification model. Journal of Computer Applications. 2009;29(3):903-904.\n\n> Liaw A. Weiner M. Classification and Regression by randomForest. R News. 2002;Vol 2(2):18-22.\n\n> Casanova R, Saldana S, Chew EY, Danis RP, Greven CM, et al. (2014) Application of Random Forests Methods to Diabetic Retinopathy Classification Analyses. PLoS ONE 9(6): e98587. doi: 10.1371\/journal.pone.0098587\n\n> Jones M. Viola A. Robust Real-Time Face Detection. International Journal of Computer Vision. 2004. pg 137\u2013154. http:\/\/blog.echen.me\/2011\/04\/27\/choosing-a-machine-learning-classifier\/\n\n> Lowri Williams : Sentiment Analysis: Aspect-Based Opinion Mining. https:\/\/towardsdatascience.com\/%EF%B8%8F-sentiment-analysis-aspect-based-opinion-mining-72a75e8c8a6d","ab6cf317":"### The benefit of splitting the data into testing and training sets is that it sets valuation of how well the model is performing before using it in the real world to make predictions.","41928f5a":"We have now generated all of the features that we are going to need.\n\n### Benchmark model\n\nIn order to establish a baseline for the project, we will look at all of the out of box models from sklearn.\n\nThe following algorithms were investigated:\n> * Gaussian Naive Bayes (GaussianNB)\n> * Decision Trees\n> * AdaBoost\n> * Random Forest\n> * Logistic Regression\n\nLets read about these algorithms in short :-\n> 1. Gaussian Navie Bayes :- In Gaussian Naive Bayes, continuous values associated with each feature are assumed to be distributed according to a Gaussian distribution. A Gaussian distribution is also called Normal distribution. When plotted, it gives a bell shaped curve which is symmetric about the mean of the feature values.\n\n> 2. Decision Trees :- A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.\n\n> 3. AdaBoostClassifier :- AdaBoost is an iterative ensemble method. AdaBoost classifier builds a strong classifier by combining multiple poorly performing classifiers so that you will get high accuracy strong classifier . Any machine learning algorithm can be used as base classifier if it accepts weights on the training set.\n\n> 4. RandomForest :- Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes or mean\/average prediction of the individual trees. \n\n> 5. Logistic Regression :- Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression).\n\nNow we will shuffle and split the data into 80% training and 20% testing.","72640d9e":"# Problem Statement 2 :- To test Genuinity of 'reviewText' vs 'Score'\n\nHave you ever met in your listing that there are 1 star reviews left by non-verified purchase \u201cbuyers\u201d with very positive comment or vice-versa with 5 star reviews with a negative comment.\n\nFor e.g. :-\n![image.png](attachment:image.png)\n\n\nFor my case, I am doubting those negative reviews are from the infringing seller who opens some buyers accounts to leave negative comment. Each account has only one review on their page, which is the one for my product.\n\nIt feels so disgusting.\n\n## Solution : - \n>#### 1. We will be using Sentiment Analysis on 'reviewText' and give our own rating based on whether the sentiment is Positive , Negative or Neutral . \n>#### 2. Then we will compare those with score which users have given while reveiewing . If both sentiments don't match we have to filter those out.\n\n### So what is Sentiment Analysis ?\n>#### Sentiment analysis (or opinion mining) is a natural language processing technique used to determine whether data is positive, negative or neutral. Sentiment analysis is often performed on textual data to help businesses monitor brand and product sentiment in customer feedback, and understand customer needs.","76aed929":"As expected, the Logistic Regression is the best algorithm in terms of accuracy for all test sizes. It's final score for the area under the ROC curve was 0.8453 and a sample size of ~19,000. In addition it is the fastest. The training speed and prediction speed were 0.068s and 0.0137s respectively for a sample size of ~19,000. As our system needs to consider the trade off between accuracy and speed, the Logistic Regression algorithm represents the ideal model for our benchmark. ","9ad03988":"#### So we can see clearly that Logistic Regression classifier did the best in our benchmark test. Not so suprising as we have mentioned above that logistic is a binary Classification and will work best on binary labels. It's interesting to compare the speed of the Logistic Regression to the other algorithms. It is quite a bit faster than all of the algorithms, and more accurate than each of them for a test size of 18,968. In order to make sure we are covering all of our bases, we should also do a visualization."}}