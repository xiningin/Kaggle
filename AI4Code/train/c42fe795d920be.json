{"cell_type":{"b0812d35":"code","43fdf6dc":"code","078730eb":"code","cc9747c9":"code","28dd6ad6":"code","e9fbe43e":"code","67acadc4":"code","01cfaec2":"code","c2c4f11d":"code","2f95d287":"code","323d906b":"code","4aa47b86":"code","83579245":"code","07473bce":"code","3a6e8888":"code","8e55346c":"code","d6baaedc":"code","1f7223c0":"code","10622da7":"code","5991aa76":"code","5ce99518":"code","846b3838":"code","36456162":"code","d0a257bb":"code","33357597":"code","46137e84":"code","6f9b726e":"code","a9b05a2c":"code","2a4121f9":"code","ef342b5b":"code","cbaedba1":"code","d4f4ff50":"code","d6f8f564":"code","292750c5":"code","a6977c71":"code","bd23270a":"code","3ddbe471":"code","2c04078a":"code","8d45ce8f":"code","7ddfd15a":"code","babd72b8":"code","5cb69c9c":"code","97c7c873":"code","23808b7d":"code","26f7e1ff":"code","b18c333e":"code","b8ee0db3":"code","4f91b0ed":"code","c7d5e375":"code","5735563e":"code","64b6e19a":"code","6ccd26e0":"code","11172bd6":"code","6dab7124":"code","48d9649e":"code","dbb09f15":"code","d37cbc55":"code","ba3dcfbd":"code","416e4e5b":"code","be82c319":"code","95158392":"code","8edba62f":"code","f65f1c5c":"code","7539f19a":"code","17d85d7c":"markdown","7e7e980f":"markdown","7f267839":"markdown","9ad1a34c":"markdown","4a41bc39":"markdown","b08e7b90":"markdown","c3847d57":"markdown","df688b58":"markdown","76ea8f7e":"markdown","01bd1c43":"markdown","f042d72a":"markdown","277aebe6":"markdown","8aa54fb6":"markdown","d67f5e47":"markdown","5b5eb13c":"markdown","c6eb1c7d":"markdown"},"source":{"b0812d35":"#Loading packages and Data\n\nfrom IPython.display import Image\nimport numpy as np # linear algebra\nimport pandas as pd# data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas import read_csv\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom datetime import datetime\nfrom random import random\nfrom math import sqrt\nfrom numpy import concatenate\nfrom numpy import array\nimport matplotlib.pyplot as plt\n\nfrom statsmodels.tsa.stattools import grangercausalitytests\nfrom statsmodels.tsa.vector_ar.var_model import VAR\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.vector_ar.vecm import coint_johansen\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom statsmodels.tsa.seasonal  import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n\n!pip install pyramid-arima\nfrom pyramid.arima import auto_arima\n\n\n#!pip install plotly==3.10.0\n\nfrom fbprophet import Prophet\n#from plotly.plotly import plot_mpl\n\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n","43fdf6dc":"\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\npd.plotting.register_matplotlib_converters()\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","078730eb":"#Loading csv\ndata=pd.read_csv('..\/input\/smart-home-dataset-with-weather-information\/HomeC.csv')","cc9747c9":"data.head()","28dd6ad6":"data.info()","e9fbe43e":"# removing the truncated record\ndata=data[:-1]\ndata.shape","67acadc4":"#new daterange in increments of minutes\ntime_index = pd.date_range('2016-01-01 05:00', periods=len(data),  freq='min')  \ntime_index = pd.DatetimeIndex(time_index)\ndata['time']=time_index","01cfaec2":"#changing column names before doing some calculation as they look weird with \"[kw]\"\ndata.columns=['time', 'use', 'gen', 'House overall', 'Dishwasher',\n       'Furnace 1', 'Furnace 2', 'Home office', 'Fridge',\n       'Wine cellar', 'Garage door', 'Kitchen 12',\n       'Kitchen 14', 'Kitchen 38', 'Barn', 'Well',\n       'Microwave', 'Living room', 'Solar', 'temperature',\n       'icon', 'humidity', 'visibility', 'summary', 'apparentTemperature',\n       'pressure', 'windSpeed', 'cloudCover', 'windBearing', 'precipIntensity',\n       'dewPoint', 'precipProbability']","c2c4f11d":"data['gen'].head()","2f95d287":"data['Solar'].head()","323d906b":"(data['gen']-data['Solar']).value_counts()","4aa47b86":"data=data.drop('gen',axis=1)","83579245":"(data['House overall']-data['use']).value_counts()","07473bce":"data=data.drop('House overall',axis=1)","3a6e8888":"#getting  hour, day,week, month from the date column\ndata['day']= data['time'].dt.day\ndata['month']= data['time'].dt.month\ndata['week']= data['time'].dt.week\ndata['hour']= data['time'].dt.hour","8e55346c":"import seaborn as sns\ndef visualize(label, cols):\n    fig,ax=plt.subplots(figsize=(14,8))\n    colour= ['red','green','blue','yellow']\n    for colour,col in zip(colour,cols):\n            data.groupby(label)[col].mean().plot(ax=ax,label=col,color=colour)\n    plt.legend()\n\n","d6baaedc":"visualize('hour',['Furnace 1','Furnace 2'])","1f7223c0":"visualize('day',['Furnace 1','Furnace 2'])","10622da7":"visualize('month',['Furnace 1','Furnace 2'])","5991aa76":"data['Furnace']= data['Furnace 1']+data['Furnace 2']\ndata=data.drop(['Furnace 1','Furnace 2'], axis =1)","5ce99518":"visualize('month',['Kitchen 12','Kitchen 14','Kitchen 38'])","846b3838":"visualize('week',['Kitchen 12','Kitchen 14','Kitchen 38'])","36456162":"visualize('day',['Kitchen 12','Kitchen 14','Kitchen 38'])","d0a257bb":"visualize('hour',['Kitchen 12','Kitchen 14','Kitchen 38'])","33357597":"data['Kitchen 38'].describe()","46137e84":"fig,ax=plt.subplots(2,2,figsize=(15,10))\n\ndata.groupby('hour')['Kitchen 38'].mean().plot(ax=ax[0,0],color='green',label= 'kitchen 38')\ndata.groupby('day')['Kitchen 38'].mean().plot(ax=ax[0,1],color='green',label= 'kitchen 38')\ndata.groupby('week')['Kitchen 38'].mean().plot(ax=ax[1,0],color='green',label= 'kitchen 38')\ndata.groupby('month')['Kitchen 38'].mean().plot(ax=ax[1,1],color='green',label= 'kitchen 38')\n\n                                                     \n\nplt.legend()","6f9b726e":"data['icon'].value_counts()","a9b05a2c":"data['summary'].value_counts()","2a4121f9":"data.groupby('summary')['Solar'].sum()","ef342b5b":"data=data.drop(['icon','summary'], axis =1)","cbaedba1":"data['cloudCover'].dtypes","d4f4ff50":"data['cloudCover'].head()","d6f8f564":"data['cloudCover'].value_counts()","292750c5":"data['cloudCover'].unique()","a6977c71":"data['cloudCover'].replace(['cloudCover'], method='bfill', inplace=True)\n","bd23270a":"data['cloudCover'].unique()","3ddbe471":"data['cloudCover']=data['cloudCover'].astype('float')","2c04078a":"data.info()","8d45ce8f":"data.index= data['time']\n#daily resampling\ndataD=data.resample('D').mean()","7ddfd15a":"dataD.info()","babd72b8":"#hourly resampling\ndataH=data.resample('H').mean()","5cb69c9c":"weathercols= ['temperature', 'humidity','visibility', 'apparentTemperature', 'pressure', 'windSpeed',\n       'cloudCover', 'windBearing', 'precipIntensity', 'dewPoint','precipProbability']\nHousecols = ['Dishwasher','Furnace', 'Home office', 'Fridge','Wine cellar', 'Garage door', 'Kitchen 12','Kitchen 14', \n             'Kitchen 38', 'Barn', 'Well','Microwave', 'Living room']\nuseweather=['use','temperature', 'humidity','visibility', 'apparentTemperature', 'pressure', 'windSpeed',\n       'cloudCover', 'windBearing', 'precipIntensity', 'dewPoint','precipProbability']\nsolarweather=['Solar','temperature', 'humidity','visibility', 'apparentTemperature', 'pressure', 'windSpeed',\n       'cloudCover', 'windBearing', 'precipIntensity', 'dewPoint','precipProbability']\nusesolar=['use','Solar']","97c7c873":"# load dataset\ndef series_visualize(data, cols):\n    dataset = data[cols]\n    values = dataset.values\n    # specify columns to plot    \n    groups = [i for i in range(len(cols))]\n    j = 1\n# plot each column\n    plt.figure(figsize=(18,13))\n    for group in groups:\n        plt.subplot(len(groups), 1, j)\n        plt.plot(values[:, group])\n        plt.title(dataset.columns[group], y=0.5, loc='right')\n        j += 1\n    plt.show()","23808b7d":"#series_visualize(dataH,Housecols)\nseries_visualize(dataD,Housecols)","26f7e1ff":"series_visualize(dataH,usesolar)","b18c333e":"series_visualize(dataD,usesolar)","b8ee0db3":"datause=dataD.iloc[:,0].values\n#fig,ax=plt.subplots(figsize=(15,10))\nplt.rcParams['figure.figsize'] = (14, 9)\nseasonal_decompose(dataD[['use']]).plot()\nresult = adfuller(datause)\nplt.show()","4f91b0ed":"X= dataD.iloc[:,0].values\nresult = adfuller(X)\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))","c7d5e375":"# split data into train and tests\ntrain=dataD[dataD['month']<12].iloc[:,0]\ntest=dataD[dataD['month']>=12].iloc[:,0]\nprint(\"train has {} records, test has {} records\".format(len(train),len(test)))","5735563e":"fig,ax=plt.subplots(figsize=(18,6))\ntrain.plot(ax=ax)\ntest.plot(ax=ax)\nplt.show()","64b6e19a":"\n# fit model withweekly seasonality \nmodel = ExponentialSmoothing(train.values,seasonal='add',seasonal_periods=7)\nmodel_fit = model.fit()\n\n# make prediction\n\ny = model_fit.forecast(len(test))\ny_predicted=pd.DataFrame(y,index=test.index,columns=['Holtwinter'])\n\nplt.figure(figsize=(16,8))\nplt.plot(test, label='Test')\nplt.plot(y_predicted, label='Holtwinter')\nplt.legend(loc='best')\nplt.show()","6ccd26e0":"rms = sqrt(mean_squared_error(test,y_predicted))\nprint(rms)","11172bd6":"from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n\n# Draw Plot\nfig, axes = plt.subplots(1,2,figsize=(16,3), dpi= 100)\nx=plot_acf(train.tolist(), lags=50,ax=axes[0])\ny=plot_pacf(train.tolist(), lags=50, ax=axes[1])\nplt.show()","6dab7124":"# first differencing\nfig, axes = plt.subplots(1,2,figsize=(16,3), dpi= 100)\nx=plot_acf(train.diff().dropna(), lags=50,ax=axes[0])\ny=plot_pacf(train.diff().dropna(), lags=50, ax=axes[1])\nplt.show()","48d9649e":"def split_sequence(sequence, n_steps_in, n_steps_out):\n    X, y = list(), list()\n    for i in range(len(sequence)):\n        # find the end of this pattern\n        end_ix = i + n_steps_in\n        out_end_ix = end_ix + n_steps_out\n        # check if we are beyond the sequence\n        if out_end_ix > len(sequence):\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)","dbb09f15":"# define input sequence\nraw_seq = train[:307].values.tolist()\n# choose a number of time steps\nn_steps_in, n_steps_out = 28, 16\n# split into samples\nX, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\n# reshape from [samples, timesteps] into [samples, timesteps, features]\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))\n# define model","d37cbc55":"#LSTM model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\nmodel.add(LSTM(100, activation='relu'))\nmodel.add(Dense(n_steps_out))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=50, verbose=0)","ba3dcfbd":"# demonstrate prediction\nx_input = train[307:].values\nx_input = x_input.reshape((1, n_steps_in, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","416e4e5b":"yhat=yhat.reshape(16,1)","be82c319":"\nforecast = pd.DataFrame(yhat,index = test.index,columns=['Prediction'])\n\n#plot the predictions for validation set\nplt.figure(figsize=(16,8))\nplt.plot(test, label='Valid')\nplt.plot(forecast, label='Prediction')\nplt.show()","95158392":"rms = sqrt(mean_squared_error(test,forecast))\nprint(rms)","8edba62f":"f = m.plot_components(forecast)\n","f65f1c5c":"predictions=pd.DataFrame(forecast[335:]['yhat'])\npredictions.index=test.index\n\nfig,ax=plt.subplots(figsize=(15,8))\ntest.plot(ax=ax)\npredictions.plot(ax=ax)","7539f19a":"rms = sqrt(mean_squared_error(test,forecast[['yhat']][335:]))\nprint(rms)","17d85d7c":"                                                            * * *","7e7e980f":"we need to impute 'cloudCover' with the nearest values as the records are taken in minute steps. We would use backward fill to replace","7f267839":"As there are lot of unique values, let us check what are they","9ad1a34c":"LSTM couldn't produce better predictions than Statistical models. Because, LSTMs need more data to tune their parameters. And, in our case, we have only 1 year data. Also, LSTM's are better at forecasting longterm not at shortterm","4a41bc39":" P-value < 0.05, the series is stationary ","b08e7b90":"### <div id= '3'>Univariate Model<\/div>","c3847d57":"Before building models, Let us check for datatypes that are not int or float","df688b58":"                             Hourly power usage and hourly solar power generation ","76ea8f7e":"we need to prepare dataset as a 3D matrix for LSTM from [samples, timesteps] to  [samples, timesteps, features] . Here, we have only 1 feature i.e., use,  and timesteps are the sequence of steps, here, we choose 28 timesteps and the output timesteps to predict as 16, because we need to validate on the Test set","01bd1c43":"Let us check how solar energy got produced in different days","f042d72a":"As expected clear, partly cloudy, drizzle, light rain days produced a lot more power than other days. Also the number of clear days outnumbered other days. So, this number would be large compared to other day's","277aebe6":"###  <div id= '3.4'>LSTM<\/div>","8aa54fb6":"Now we will check for 'cloudCover' column","d67f5e47":"There is consumption but very little comparing to other kitchens, we will keep them like that","5b5eb13c":"In the months of June, July,and August, \"office\", \"winecellar\", \"Fridge\" power consumption rose. And in December, January, February months Furnace's power consumption rose.","c6eb1c7d":"### <div id= '3.1'> HOLTZ-WINTERS Exponential Smoothing <\/div>"}}