{"cell_type":{"88e03dd7":"code","61c4300c":"code","7f7d04ca":"code","b8203963":"code","98a76c84":"code","d487273d":"code","5c4cd57e":"code","049f4281":"code","2c1aa677":"code","433a0e3c":"code","21c30784":"code","be9c715e":"code","60155e7b":"code","0815a4dc":"code","cea7c464":"code","a1df89e3":"code","64f98abe":"code","da7fc124":"code","6bc32ee4":"code","9996ef1d":"markdown","8099be95":"markdown","811cc9ed":"markdown","9a35bd52":"markdown","da27c64a":"markdown","a546e9e9":"markdown","bc360279":"markdown"},"source":{"88e03dd7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting\nfrom sklearn.model_selection import train_test_split, cross_validate # creat train and test datasets to modeling\nfrom sklearn.metrics import classification_report, f1_score, precision_score, recall_score # report and metrics modules \n\n\n# ML models upload\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Additional models\nimport xgboost as xgb, lightgbm as lgbm\n","61c4300c":"import warnings \nwarnings.filterwarnings('ignore')","7f7d04ca":"!pip install scikit-learn  -U","b8203963":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n#                 if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n#                     df[col] = df[col].astype(np.float16)\n#                 elif\n\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","98a76c84":"def show_proba_calibration_plots(y_predicted_probs, y_true_labels):\n    preds_with_true_labels = np.array(list(zip(y_predicted_probs, y_true_labels)))\n\n    thresholds = []\n    precisions = []\n    recalls = []\n    f1_scores = []\n\n    for threshold in np.linspace(0.1, 0.9, 50):\n        thresholds.append(threshold)\n        precisions.append(precision_score(y_true_labels, list(map(int, y_predicted_probs > threshold))))\n        recalls.append(recall_score(y_true_labels, list(map(int, y_predicted_probs > threshold))))\n        f1_scores.append(f1_score(y_true_labels, list(map(int, y_predicted_probs > threshold))))\n\n    scores_table = pd.DataFrame({'f1':f1_scores,\n                                 'precision':precisions,\n                                 'recall':recalls,\n                                 'probability':thresholds}).sort_values('f1', ascending=False).round(3)\n  \n    figure = plt.figure(figsize = (25, 12))\n\n    plt1 = figure.add_subplot(121)\n    plt1.plot(thresholds, precisions, label='Precision', linewidth=4)\n    plt1.plot(thresholds, recalls, label='Recall', linewidth=4)\n    plt1.plot(thresholds, f1_scores, label='F1', linewidth=4)\n    plt1.set_ylabel('Scores')\n    plt1.set_xlabel('Probability threshold')\n    plt1.set_title('Probabilities threshold calibration')\n    plt1.legend(bbox_to_anchor=(0.25, 0.25))   \n    plt1.table(cellText = scores_table.values,\n               colLabels = scores_table.columns, \n               colLoc = 'center', cellLoc = 'center', loc = 'bottom', bbox = [0, -1.1, 1, 1])\n\n    plt2 = figure.add_subplot(122)\n    plt2.hist(preds_with_true_labels[preds_with_true_labels[:, 1] == 0][:, 0], \n              label='Another class', color='royalblue', alpha=1)\n    plt2.hist(preds_with_true_labels[preds_with_true_labels[:, 1] == 1][:, 0], \n              label='Main class', color='darkcyan', alpha=0.8)\n    plt2.set_ylabel('Number of examples')\n    plt2.set_xlabel('Probabilities')\n    plt2.set_title('Probability histogram')\n    plt2.legend(bbox_to_anchor=(1, 1))\n\n    plt.show()","d487273d":"def get_classification_report(y_train_true, y_train_pred, y_test_true, y_test_pred):\n    print('TRAIN\\n\\n' + classification_report(y_train_true, y_train_pred))\n    print('TEST\\n\\n' + classification_report(y_test_true, y_test_pred))\n    print('CONFUSION MATRIX\\n')\n    print(pd.crosstab(y_test_true, y_test_pred))","5c4cd57e":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')","049f4281":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)\nsub = reduce_mem_usage(sub)","2c1aa677":"train.info()","433a0e3c":"base_features = list(train.columns) [1:-1]","21c30784":"X = train[base_features]\ny = train['target']\nX_test_fin = test[base_features]","be9c715e":"X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.30)","60155e7b":"seed = 0\nfold = 2","0815a4dc":"'''\nmodel_lgbm = lgbm.LGBMClassifier(\n    num_iterations=100,\n    objective = \"binary\",\n    num_leaves= 31,\n    feature_pre_filter = False\n    )\ndef score(X, y, model_lgbm, cv):\n    scoring = [\"roc_auc\"]\n    scores = cross_validate(\n        model_lgbm, X, y, scoring=scoring, cv=cv, return_train_score=True\n    )\n    scores = pd.DataFrame(scores).T\n    return scores.assign(\n        mean = lambda x: x.mean(axis=1),\n        std = lambda x: x.std(axis=1),\n    )\n\nscores = score(X, y, model_lgbm, cv=fold)\ndisplay(scores)\n'''","cea7c464":"'''\ndef score(X, y, model_lgbm, cv):\n    scoring = [\"roc_auc\"]\n    scores = cross_validate(\n        model_lgbm, X, y, scoring=scoring, cv=cv, return_train_score=True\n    )\n    scores = pd.DataFrame(scores).T\n    return scores.assign(\n        mean = lambda x: x.mean(axis=1),\n        std = lambda x: x.std(axis=1),\n    )\n\ntest_roc_auc_row = []\n\nfor num_iter in range(200, 700, 40):\n    for max_d in range (8, 15, 1):\n        model_lgbm = lgbm.LGBMClassifier(\n        num_iterations=num_iter,\n        objective = \"binary\",\n        feature_pre_filter = False,\n        max_depth = max_d\n        )\n\n        res = {}\n        res['num_iter'] = num_iter\n        res['max_depth'] = max_d\n        scores = score(X, y, model_lgbm, cv=fold)\n        res['test_roc_auc'] = scores.loc['test_roc_auc','mean']\n        print(num_iter, max_d, res['test_roc_auc'])\n\n        test_roc_auc_row.append(res)'''","a1df89e3":"'''df = pd.DataFrame(test_roc_auc_row)\ndf.sort_values(by='test_roc_auc', ascending=False).head(10)'''","64f98abe":"model_fin = lgbm.LGBMClassifier(num_iterations = 240,max_depth = 13, eval_metric='auc')\nmodel_fin.fit(X, y)","da7fc124":"predictions = model_fin.predict_proba(X_test_fin)\ny_pred_f = np.array([1 if x>=0.5 else 0 for x in predictions[:,1]])","6bc32ee4":"sub['target'] = y_pred_f\nsub.to_csv('submission.csv', index = 0)\nsub","9996ef1d":"# Discription\nThis notebook work with two simple ML model: KNN and LogisticRegression.\n\nNotebook plan:\n\n1. Modules import.\n2. Utils.\n3. LGBM parameters tuning and modeling.\n3. Full model training.","8099be95":"## Modules","811cc9ed":"## 4. Final mode train","9a35bd52":"Do not forget to update sklearn vesrsion.","da27c64a":"## 2. Utils\n\nWe use three difeerent functions:\n\n1. reduce_mem_usage - to deduce dataset memory size \n2. show_proba_calibration_plots - to visualize the key characteristics of learning outcomes\n3. get_classification_report - to see simple classification report","a546e9e9":"## Modeling\n","bc360279":"## Data load and work preparation\n\nDownload data and prepare it to modeling. "}}