{"cell_type":{"7976829c":"code","db3c881a":"code","ff62e938":"code","efa26091":"code","089e359a":"code","c9ac4269":"code","2d476ddb":"code","f5071e57":"code","57f5417c":"code","e8c9dde9":"code","d91d8f7b":"code","d5fcaf56":"code","38e65fc6":"markdown","da4f2946":"markdown","24d56975":"markdown","690a24bd":"markdown","55c54300":"markdown","086062ea":"markdown","9404c5e3":"markdown","32990952":"markdown","ce0ba543":"markdown"},"source":{"7976829c":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nimport pandas as pd\nimport json\nimport numpy as np\n","db3c881a":"def read_dataset(path):\n    return json.load(open(path)) \n\ntrain = read_dataset('..\/input\/train.json')\ntest = read_dataset('..\/input\/test.json')","ff62e938":"def generate_text(data):\n    text_data = [\" \".join(doc['ingredients']).lower() for doc in data]\n    return text_data ","efa26091":"train_text = generate_text(train)\ntest_text = generate_text(test)\ntarget = [doc['cuisine'] for doc in train]\n","089e359a":"tfidf = TfidfVectorizer(binary=True)\ndef tfidf_features(txt, flag):\n    if flag == \"train\":\n        x = tfidf.fit_transform(txt)\n    else:\n        x = tfidf.transform(txt)\n    x = x.astype('float16')\n    return x \nX = tfidf_features(train_text, flag=\"train\")\nX_test = tfidf_features(test_text, flag=\"test\")","c9ac4269":"lb = LabelEncoder()\ny = lb.fit_transform(target)\ny = keras.utils.to_categorical(y)","2d476ddb":"model = keras.Sequential()\nmodel.add(keras.layers.Dense(1000, kernel_initializer=keras.initializers.he_normal(seed=1), activation='relu', input_dim=3010))\nmodel.add(keras.layers.Dropout(0.81))\nmodel.add(keras.layers.Dense(1000, kernel_initializer=keras.initializers.he_normal(seed=2), activation='relu'))\nmodel.add(keras.layers.Dropout(0.81))\nmodel.add(keras.layers.Dense(20, kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=4), activation='softmax'))\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","f5071e57":"history = model.fit(X, y, epochs=20, batch_size=512, validation_split=0.1)\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model to disk\")\n","57f5417c":"print(history.history.keys())\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n","e8c9dde9":"predictions_encoded = model.predict(X_test)\npredictions_encoded.shape","d91d8f7b":"predictions = lb.inverse_transform([np.argmax(pred) for pred in predictions_encoded])\npredictions","d5fcaf56":"test_id = [doc['id'] for doc in test]\nsub = pd.DataFrame({'id': test_id, 'cuisine': predictions}, columns=['id', 'cuisine'])\nsub.to_csv('output.csv', index=False)","38e65fc6":"<h1>Build a model<\/h1>\n<p>You can monkey with value of dropout to see how looks underfit\/overfit (later). Also you can add other regularizers such as l1, l2.<\/p>\n<p>He initializer works better with relu.<\/p>","da4f2946":"<h1>Converting lists of ingredients to strings<\/h1>","24d56975":"<h1>Converting the list of strings to the matrix of vectors<\/h1>\n(to be fed our nn)","690a24bd":"<h1>Plotting learning curves<\/h1>\nLearning curves show us overting\/underfiting","55c54300":"<h1>Converting predicted vectors to names of cuisines<\/h1>","086062ea":"<h1>Training the model<\/h1>\n<p>Don't forget to turn on GPU<\/p>","9404c5e3":"<h1>Load and read dataset<\/h1>","32990952":"<h1>Submission<\/h1>","ce0ba543":"<h1>TF-IDF on text data<\/h1>"}}