{"cell_type":{"e45ea930":"code","da126646":"code","98fbfba3":"code","7d3febe3":"code","a7407c4a":"code","808caa4d":"code","083c0ff7":"code","7b8287a4":"code","dcd2abe1":"code","a0a93a69":"code","5fd6e50d":"code","f2d8ac2e":"code","56811ed5":"code","e49e2a14":"code","cbfc24d0":"code","f10cf906":"code","f253e694":"code","a6e002fa":"code","dc0806c7":"code","d9c7873b":"code","dddfec9a":"code","e4d7af94":"code","058eddbe":"code","db07bd8b":"code","7f858647":"code","2be194e6":"code","a883c6ba":"code","753350fe":"code","8e7ef952":"code","db13b286":"code","75c9664a":"code","177ac13e":"code","b3e9ca76":"code","65bad9a3":"code","3614f6ff":"code","eb158367":"code","c081ae03":"code","59386e6b":"code","8028c3d7":"code","242fb40e":"code","e8399531":"code","b140ad2b":"code","44a84f65":"code","c579ae55":"code","d42bc7a4":"code","0088816d":"code","c0e47cf8":"code","32170802":"code","537de1d3":"code","6b161352":"code","e4bc6ea6":"code","3ae24838":"code","e71850fd":"code","f84ad84a":"code","90044a41":"code","bed7eb2a":"code","3b937a16":"code","18c0b386":"code","2c97b603":"code","2c7c13da":"code","83f621ae":"code","0d1fc11f":"code","45b270a3":"code","13dde5ec":"code","6e35f2f8":"code","2d924570":"code","c4a36cff":"code","be712825":"code","3433f7ca":"code","a1881912":"code","fb7eb0a4":"code","86e20f4f":"code","10540d7d":"code","4e757b4b":"code","9a9c64b6":"code","ba203d48":"code","7d56a6db":"code","1c774a9f":"code","9b11ffd3":"code","11d5ede6":"code","6f501c1c":"code","b4d4c148":"code","7abc8d2d":"code","85a7db3d":"code","3c296c15":"code","d53c39ec":"code","2e3c4465":"code","9e11a20b":"code","ce167de6":"code","cfa37c50":"code","2054ac60":"code","1fea5220":"code","af8adfce":"code","100a4f85":"code","238ba3cd":"code","744ac0c0":"code","e5f307e7":"code","8fa7a0f3":"code","9c8268c2":"code","ea1c6bee":"code","77204906":"code","24477edc":"code","6ea100c7":"markdown","36414a0d":"markdown","1f0b2c3b":"markdown","d2ba4244":"markdown","e44ad473":"markdown","43e53d1f":"markdown","b2539dc4":"markdown","5ff31682":"markdown","fb06c949":"markdown","11ac2067":"markdown","5c4c34e2":"markdown","07ab4b47":"markdown","fb53754b":"markdown","ee08ed1f":"markdown","d9a2e398":"markdown","2a8a1e83":"markdown","63b73ad3":"markdown","842669a7":"markdown","7bdabda5":"markdown","db999b8b":"markdown","d68048e3":"markdown","07210084":"markdown","ce212ab5":"markdown","3ba0449b":"markdown","54464470":"markdown","14693aa9":"markdown","c7acee56":"markdown","630d12a1":"markdown","c18d2f28":"markdown","def85f26":"markdown","15232c1c":"markdown","00a9636f":"markdown","f499be22":"markdown","b1f69143":"markdown","1750dfeb":"markdown","069d8eed":"markdown","b044084f":"markdown","f777b431":"markdown","3c2fcb2b":"markdown","49b861c8":"markdown","d3096207":"markdown","6db4586d":"markdown","1fdc89bc":"markdown","ebace520":"markdown","30282846":"markdown","963e14e0":"markdown","b231f43d":"markdown","44e41780":"markdown","036aec3c":"markdown","bc1aa02b":"markdown","c07333ee":"markdown","03361bc6":"markdown","219c8cd0":"markdown","0f8f9946":"markdown","4950f3b0":"markdown","3a355c54":"markdown","a52cc153":"markdown","09aed04d":"markdown","c33aa192":"markdown"},"source":{"e45ea930":"# imports\nimport os\nimport pandas as pd","da126646":"HOUSING_PATH=os.path.join(\"..\/input\",\"california-housing-prices\")","98fbfba3":"def load_housing_data(housing_path=HOUSING_PATH):\n    csv_path = os.path.join(housing_path,\"housing.csv\")\n    return pd.read_csv(csv_path)","7d3febe3":"housing = load_housing_data()","a7407c4a":"housing.head()","808caa4d":"housing.columns","083c0ff7":"housing.info()","7b8287a4":"housing[\"ocean_proximity\"].value_counts()","dcd2abe1":"housing.describe()","a0a93a69":"%matplotlib inline\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50,figsize=(20,15))\nplt.show()","5fd6e50d":"import numpy as np\n# Fix the random seed\nnp.random.seed(42)","f2d8ac2e":"# Implementing basic train-test split\ndef split_train_test_data(total_data, test_frac=0.2):\n    # Shuffle the indices of the data randomly\n    shuffled_indices = np.random.permutation(len(total_data))\n    # Length of test set\n    test_set_len = int(test_frac*len(total_data))\n    # Test Indices\n    test_indices = shuffled_indices[:test_set_len]\n    # Train Indices\n    train_indices = shuffled_indices[test_set_len:]\n    \n    return total_data.iloc[train_indices], total_data.iloc[test_indices]","56811ed5":"# My split data\ntrain_set, test_set = split_train_test_data(housing)","e49e2a14":"# Split using Scikit-learn\nfrom sklearn.model_selection import train_test_split\n\nsk_train_set, sk_test_set = train_test_split(housing, test_size=0.2, random_state=42)","cbfc24d0":"# Comparing data splits\ntrain_set.head()","f10cf906":"sk_train_set.head()","f253e694":"# A more sophisticated split method: Keep the test set consistent even when new data is added to the total set\n# When there's new data added to the set, the test set will definitely have all of the original test data despite shuffling\nfrom zlib import crc32\n\ndef test_set_check(data_id, test_frac):\n    data_hash_value = crc32(np.int64(data_id)) & 0xffffffff\n    return data_hash_value < test_frac * 2**32","a6e002fa":"def split_by_id(total_data, test_frac, id_column):\n    ids = total_data[id_column]\n    ids_in_test_set = ids.apply(lambda id_: test_set_check(id_, test_frac))\n    \n    return total_data[~ids_in_test_set], total_data[ids_in_test_set]","dc0806c7":"# Create an id field for the dataset\n\n# Option 1) Add the index field\nhousing_with_id = housing.reset_index()\ntrain_set_stable, test_set_stable = split_by_id(housing_with_id, 0.2, \"index\")","d9c7873b":"train_set_stable.head()","dddfec9a":"# Option 2) Create a more stable id using latitude and longitude\nhousing_with_id = housing\nhousing_with_id[\"id\"] = housing[\"longitude\"]*1000 + housing[\"latitude\"]\ntrain_set_stable, test_set_stable = split_by_id(housing_with_id, 0.2, \"id\")","e4d7af94":"train_set_stable.head()","058eddbe":"# Create a income category to see how the income values are distributed\nhousing[\"income_cat\"] = pd.cut(housing[\"median_income\"], \n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels = [1, 2, 3, 4, 5])\n\nhousing[\"income_cat\"].hist()","db07bd8b":"from sklearn.model_selection import StratifiedShuffleSplit","7f858647":"split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]","2be194e6":"# Original data's distribution across categories\nhousing[\"income_cat\"].value_counts()\/len(housing)","a883c6ba":"# Stratified test set's distribution across categories\nstrat_test_set[\"income_cat\"].value_counts()\/len(strat_test_set)","753350fe":"# Drop the \"income_cat\" feature from the stratified datasets\n\nfor set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)\n    set_.drop(\"id\", axis=1, inplace=True)","8e7ef952":"# Create a copy of the stratified data for analysis\nhousing = strat_train_set.copy()","db13b286":"housing.plot(kind=\"scatter\", # Type of plot\n             x=\"longitude\",\n             y=\"latitude\",\n             alpha=0.4, # Transparency to visualize data easier\n             s=housing[\"population\"]\/100, # Scale the points by the population; Gives us a sense of density\n             label=\"Population\",\n             figsize=(20,14),\n             c=\"median_house_value\", # Color code by the house value; Gives us a sense of where homes cost more\n             cmap=plt.get_cmap(\"jet\"), # Blue indicates low values and Red indicates high values\n             colorbar=True)\n\nplt.legend()","75c9664a":"# Compute the correlation of the target variable (median housing price) with all other features\ncorr_matrix = housing.corr()\n\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","177ac13e":"from pandas.plotting import scatter_matrix\n\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n\n# The scatter matrix shows this relationship visually\nscatter_matrix(housing[attributes], figsize=(24,16))","b3e9ca76":"# Of these attributes, the correlation with median income is most interesting\nhousing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n             alpha=0.3,figsize=(12,8))","65bad9a3":"# These are some features recommended by the author and as can be seen below, the bedrooms_per_room feature could be useful\nhousing[\"rooms_per_household\"] = housing[\"total_rooms\"]\/housing[\"households\"] \nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]\/housing[\"total_rooms\"]\nhousing[\"population_per_household\"] = housing[\"population\"]\/housing[\"households\"]","3614f6ff":"# Recompute the Correlation\ncorr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","eb158367":"# Revert to the clean training set for the next steps\nhousing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels since this is our training set\nhousing_labels = strat_train_set[\"median_house_value\"].copy()","c081ae03":"# Extracting the rows with missing info\nincomplete_rows = housing[housing.isnull().any(axis=1)]\nincomplete_rows.head()","59386e6b":"incomplete_rows.dropna(subset=[\"total_bedrooms\"]) # Option 1: Drop the rows which have NaNs for this feature","8028c3d7":"incomplete_rows.drop(\"total_bedrooms\",axis=1).head() # Option 2: Drop the feature completely","242fb40e":"median_value = housing[\"total_bedrooms\"].median()\nincomplete_rows[\"total_bedrooms\"].fillna(median_value,inplace=True) # Option 3: Fill with something like the median value of the feature\nincomplete_rows.head()","e8399531":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")","b140ad2b":"housing_numeric = housing.drop(\"ocean_proximity\", axis=1)","44a84f65":"imputer.fit(housing_numeric)","c579ae55":"# Let's look at the statistics computed by the imputer\nimputer.statistics_","d42bc7a4":"# Does it match the statistics of the original data?\nhousing_numeric.median().values","0088816d":"# Transform the values in the training set\nhousing_numeric_transformed = imputer.transform(housing_numeric) ","c0e47cf8":"housing_tr = pd.DataFrame(housing_numeric_transformed, columns=housing_numeric.columns,\n                          index=housing_numeric.index)","32170802":"housing_tr.head()","537de1d3":"housing_cat = housing[[\"ocean_proximity\"]]\nhousing_cat.head(10)","6b161352":"from sklearn.preprocessing import OrdinalEncoder\n\nordinal_encoder = OrdinalEncoder()\nhousing_cat_enc = ordinal_encoder.fit_transform(housing_cat)\nhousing_cat_enc[:10]","e4bc6ea6":"ordinal_encoder.categories_ # labels run from 0 to 4","3ae24838":"from sklearn.preprocessing import OneHotEncoder\n\ncat_encoder = OneHotEncoder()\nhousing_cat_one_hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_one_hot","e71850fd":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# index of columns we are interested in\nrooms_idx, bedrooms_idx, population_idx, households_idx = 3, 4, 5, 6\n\n# Needs 3 functions to be defined without *args or **kwargs\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    \n    # Function 1\n    def __init__(self, add_bedrooms_per_room=True): \n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    \n    # Function 2\n    def fit(self, X, y=None):\n        return self # nothing to do here\n    \n    # Function 3 : Adding the features we tried out earlier\n    def transform(self, X):\n        rooms_per_household = X[:,rooms_idx]\/X[:,households_idx]\n        population_per_household = X[:, population_idx]\/X[:, households_idx]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:,bedrooms_idx]\/X[:,rooms_idx]\n            return np.c_[X, rooms_per_household,population_per_household,\n                         bedrooms_per_room]\n        else:\n            return np.c_[X,rooms_per_household,population_per_household]\n        \nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)\n        ","f84ad84a":"housing_extra_attribs = pd.DataFrame(housing_extra_attribs,\n                                     columns=list(housing.columns) + [\"rooms_per_household\", \"population_per_household\"],\n                                     index=housing.index)\n\nhousing_extra_attribs.head()","90044a41":"# Putting together all the steps of data processing into a pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler # For feature scaling so that all numeric attributes are in the same range\n\nnumeric_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy=\"median\")), # First Do this\n    ('attribs_adder', CombinedAttributesAdder()), # Next Do this\n    ('std_scaler', StandardScaler()), # Finally do this\n])\n\nhousing_numeric_tr = numeric_pipeline.fit_transform(housing_numeric)","bed7eb2a":"housing_numeric_tr","3b937a16":"from sklearn.compose import ColumnTransformer\n\nnumeric_attr = list(housing_numeric)\ncat_attr = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer([\n    (\"num\", numeric_pipeline, numeric_attr), # One pipeline for numeric data\n    (\"cat\", OneHotEncoder(), cat_attr), # One pipeline for categorical data\n])\n\nhousing_prepared = full_pipeline.fit_transform(housing) # One pipeline to rule them all ;) ","18c0b386":"housing_prepared","2c97b603":"housing_prepared.shape, housing.shape # Note that the extra columns are the features we added plus 5 for one-hot encoding","2c7c13da":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)","83f621ae":"# Evaluate on a few training examples\nsome_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data) # Pre-process the sample in the same way \nprint(\"Predictions: \", lin_reg.predict(some_data_prepared))","0d1fc11f":"print(\"Labels: \", list(some_labels))","45b270a3":"from sklearn.metrics import mean_squared_error\n\n# How good is the model on the training set? Look's like it's underfitting quite a bit\nlin_train_preds = lin_reg.predict(housing_prepared)\n\nlin_reg_mse = mean_squared_error(lin_train_preds, housing_labels)\nlin_reg_rmse = np.sqrt(lin_reg_mse)\nlin_reg_rmse","13dde5ec":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_prepared, housing_labels)","6e35f2f8":"tree_train_preds = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(tree_train_preds, housing_labels)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse # The Decision Tree is clearly overfitting!","2d924570":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\nforest_reg.fit(housing_prepared, housing_labels)","c4a36cff":"forest_train_preds = forest_reg.predict(housing_prepared)\nforest_mse = mean_squared_error(forest_train_preds, housing_labels)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse # Random Forest seems a lot better!","be712825":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)","3433f7ca":"def display_scores(scores):\n    print(\"Scores: \", scores)\n    print(\"Mean: \", scores.mean())\n    print(\"Standard Deviation: \", scores.std())\n    \ndisplay_scores(tree_rmse_scores) ","a1881912":"lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                              scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores) # Decision Tree performs worse than Linear Regression!","fb7eb0a4":"forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n                                scoring=\"neg_mean_squared_error\", cv=10, n_jobs=-1)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","86e20f4f":"from sklearn.model_selection import GridSearchCV\n\n# Specify the parameters you are interesting in trying out\nparam_grid = [\n    {'n_estimators': [3, 10, 30], 'max_features': [2,4,6,8]}, # Try a 3x4 combination first\n    {'bootstrap': [False], 'n_estimators': [3,10], 'max_features': [2,3,4]}, # Then try a 2x3 combination with bootstrapping set to False\n]\n\nforest_reg = RandomForestRegressor(random_state=42)\n\n# Train Across 5 folds so totally (12 + 6) * 5 = 90 rounds of training\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                          scoring=\"neg_mean_squared_error\",\n                          return_train_score=True,\n                          n_jobs=-1)\n\ngrid_search.fit(housing_prepared, housing_labels)\n","10540d7d":"grid_search.best_params_","4e757b4b":"grid_search.best_estimator_","9a9c64b6":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","ba203d48":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {\n    'n_estimators': randint(low=1, high=200),\n    'max_features': randint(low=1, high=8 ),\n}\n\nforest_reg = RandomForestRegressor(random_state=42)\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n                               n_iter=10, cv=5, scoring=\"neg_mean_squared_error\", # Note that we've asked it to run 10 times\n                               random_state=42, n_jobs=-1)\nrnd_search.fit(housing_prepared, housing_labels)","7d56a6db":"cvres = rnd_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","1c774a9f":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","9b11ffd3":"extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\ncat_encoder = full_pipeline.named_transformers_[\"cat\"]\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = numeric_attr + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True) # Let's look at the features in their order of importance","11d5ede6":"final_model = grid_search.best_estimator_\n\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\nY_test = strat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(final_predictions, Y_test)\nfinal_rmse = np.sqrt(final_mse)","6f501c1c":"final_rmse","b4d4c148":"# Let's compute a 95% confidence interval on our result\nfrom scipy import stats\n\nconfidence=0.95\nsquared_errors = (final_predictions - Y_test)**2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, \n                         loc=squared_errors.mean(),\n                         scale=stats.sem(squared_errors)))\n","7abc8d2d":"\nfrom sklearn.svm import SVR\n\nparam_grid_svm = [\n    {'kernel': ['linear'], 'C': [10., 30., 100.]},\n    {'kernel': ['rbf'], 'C': [1.0, 3.0, 10., 30.], \n     'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},    \n]\n\nsvm_reg = SVR()\ngrid_search_svm = GridSearchCV(svm_reg, param_grid_svm, cv=5, scoring=\"neg_mean_squared_error\", verbose=2, n_jobs=-1)\ngrid_search_svm.fit(housing_prepared, housing_labels)\n","85a7db3d":"\nsvm_mse = grid_search_svm.best_score_\nsvm_rmse = np.sqrt(-svm_mse)\nsvm_rmse # Looks much worse than the Random Forest Regressor\n","3c296c15":"\n# Best Parameters for SVM Regressor\ngrid_search_svm.best_params_\n","d53c39ec":"\nfrom scipy.stats import expon, reciprocal\n\nparam_distribs_svm = {\n    'kernel': ['linear', 'rbf'],\n    'C': reciprocal(20, 2000),\n    'gamma': expon(scale=1.0),\n}\n\nsvm_reg = SVR()\nrnd_search_svm = RandomizedSearchCV(svm_reg, param_distributions=param_distribs_svm,\n                                n_iter=26, cv=5, scoring='neg_mean_squared_error', \n                                verbose=2, random_state=42, n_jobs=-1)\nrnd_search_svm.fit(housing_prepared, housing_labels)\n","2e3c4465":"\nsvm_mse = rnd_search_svm.best_score_\nsvm_rmse = np.sqrt(-svm_mse)\nsvm_rmse # Seems a lot better right?\n","9e11a20b":"\nrnd_search_svm.best_params_\n","ce167de6":"# Let's see if we still have this data from the Random Forest we ran a while back\nfeature_importances","cfa37c50":"# Method to extract indices of the top-K most important features\ndef extract_top_k_indices(importances, k):\n    # Get the indices we care about\n    return np.sort(np.argpartition(importances,-k)[-k:]) ","2054ac60":"k=5\ntop_k_indices = extract_top_k_indices(feature_importances, k)\ntop_k_indices","1fea5220":"feature_names = np.array(attributes)[top_k_indices]\nfeature_names","af8adfce":"sorted(zip(feature_importances, attributes), reverse=True)[:k]","100a4f85":"class TopKFeatureSelector(BaseEstimator, TransformerMixin):\n    # function 1\n    def __init__(self, feature_importances, k):\n        self.k = k\n        self.feat_imp = feature_importances\n    # function 2\n    def fit(self, X, y=None):\n        self.top_k_indices = extract_top_k_indices(self.feat_imp, self.k)\n        return self\n    # function 3\n    def transform(self, X):\n        return X[:, self.top_k_indices]","238ba3cd":"feat_select_pipeline = Pipeline([\n    ('preparation', full_pipeline), # same one as before\n    ('feature_selection', TopKFeatureSelector(feature_importances, k)) # New block \n])","744ac0c0":"housing_top_k_prepared = feat_select_pipeline.fit_transform(housing)\nhousing_top_k_prepared[:3] ","e5f307e7":"housing_prepared[0:3, top_k_indices] # Check to see if we got it right","8fa7a0f3":"end_to_end_pipeline = Pipeline([\n    ('preparation', full_pipeline), # Prepare the Data\n    ('feature_selection', TopKFeatureSelector(feature_importances, k)), # Choose the best features\n    ('svm_reg', SVR(**rnd_search_svm.best_params_)) # Make Predictions\n])","9c8268c2":"end_to_end_pipeline.fit(housing, housing_labels)","ea1c6bee":"sample_data = housing.iloc[:5]\nsample_labels = housing_labels.iloc[:5]\n\nprint(\"Predictions: \", end_to_end_pipeline.predict(sample_data))\nprint(\"True Labels: \", list(sample_labels))","77204906":"param_grid_auto = [{\n    'preparation__num__imputer__strategy': ['median', 'most_frequent']\n}]\n\nauto_pipeline = Pipeline([\n    ('preparation', full_pipeline),\n    ('svm_reg', SVR(**rnd_search_svm.best_params_))\n])\n\ngrid_search_prep = GridSearchCV(auto_pipeline, param_grid_auto, cv=5,\n                                scoring='neg_mean_squared_error', verbose=2)\ngrid_search_prep.fit(housing, housing_labels)","24477edc":"grid_search_prep.best_params_","6ea100c7":"## Select and Train a Model\nNow that the data's prepared, let's use it to train some models and check performance","36414a0d":"Let's use the `head` method to take a look at the first 5 rows of the dataframe. Herein, each row represents one district in the dataset.","1f0b2c3b":"So what are the scores of each combination we tested?","d2ba4244":"### Adding a few features based on the correlation","e44ad473":"### 3. Try adding a transformer in the preparation pipeline to select only the most important attributes\nLet's first extract the \"Top K\" indices and then incorporate that into a custom transformer","43e53d1f":"## Read the data","b2539dc4":"# End to End Machine Learning Project\nUse the California census to dataset to build a model of housing prices in the state","5ff31682":"The house value seems to be heavily correlated to the median income and weakly negatively correlated with latitude","fb06c949":"The best hyperparameter combination can be found as:","11ac2067":"## Exercises\n","5c4c34e2":"### Visualizing Geographical Data\nLet's plot the data based on the latitude and longitude feature of each sample to see how the homes are distributed","07ab4b47":"#### Evaluate the Model","fb53754b":"We have the right features selected, so let's move on","ee08ed1f":"### Data Cleaning\nThere are three strategies commonly pursued for data cleaning as shown below:","d9a2e398":"### Standard Train Test Split","2a8a1e83":"### Looking for Correlations","63b73ad3":"### Custom Transformers in Scikit-Learn","842669a7":"#### Ordinal Encoding","7bdabda5":"There's a clear upward trend between the price and income.","db999b8b":"## Look at the structure of the data","d68048e3":"## Fine Tune your Model","07210084":"### Decision Tree","ce212ab5":"### Handling Text & Categorical Attributes\n","3ba0449b":"#### Evaluate the Model","54464470":"#### One Hot Encoding","14693aa9":"## Creating a Test Set\nLet's split the total data into training and test sets using different methods and compare results","c7acee56":"Preliminary insights:\n1. High density areas are Bay Area, Los Angeles and San Diego amongst others\n2. Housing prices seem to be related to the location of the home (near the ocean) and the density of population","630d12a1":"**Observations from the plots:**\n1. Some of the attributes such as `median_income`, `housing_median_age` and `median_house_value` (our target variable!) are clipped to a range\n2. Attributes have different scales\n3. Many histograms are tail-heavy, i.e., the distribution extends more to the right of the median than the left\n","c18d2f28":"### 4. Try creating a single pipeline that does the full data preparation plus the final prediction","def85f26":"### Analyze the Best Models and Their Errors\nLet's take a look at the feature importances of the best model so far, i.e., the Random Forest","15232c1c":"## Prepare the Data for Machine Learning Algorithms","00a9636f":"### Grid Search \nFind the best hyperparameters for the problem by trying out all combinations that you specify","f499be22":"#### Evaluate the Model","b1f69143":"### Transformation Pipelines\nA pipeline allows us to put all the preprocessing steps in sequence and perform them in the right order","1750dfeb":"### Stable Train Test Split","069d8eed":"### 2. Replace GridSearchCV with RandomizedSearchCV\n","b044084f":"### 5. Automatically explore some preparation options using GridSearchCV","f777b431":"There are 10 attributes, namely longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, median_house_value and ocean_proximity. Alternatively, you can see them via the `columns` member of the dataframe","3c2fcb2b":"### Evaluate the Model on the Test Set\nLet's see how our optimized model does on the data that actually matters!","49b861c8":"As can be seen above, the test set has roughly the same distribution of class categories as the training set. Now we can move on to inspecting the data for gaining insights","d3096207":"The `total_bedrooms` feature has some NaNs so let's use that to try out different cleaning strategies","6db4586d":"If we are filling missing values, `scikit-learn`'s `Imputer` can be used instead. However, it works only with numeric attributes so the text attributes have to be removed","1fdc89bc":"## Disclaimer & Credits\n\nIn this kernel, I have attempted to re-implement the code for the second chapter of **Aur\u00e9lien G\u00e9ron's** amazing book [Hands-on Machine Learning with Scikit-Learn, Keras and Tensorflow](https:\/\/github.com\/ageron\/handson-ml2). You can find his detailed jupyter notebooks for each chapter in the link mentioned before. This notebook is primarily a way for me to internalize the content shared in each chapter of the book, and I hope it is useful to you. \n\n\n**Note:** _The code and content here is contained in the notebooks linked above. I have done my best not to include anything present in his book but not present in the notebooks._","ebace520":"For the other features, we can use the `describe()` method which will give statistical information for each feature such as *count,mean,std,min,max* and *percentile* information","30282846":"## Discover and Visualize the Data to Gain Insights","963e14e0":"### Stratified Split","b231f43d":"Additionally, plotting the histogram for each numerical attribute helps get a quick feel of the data. ","44e41780":"### 1. Try a Support Vector Machine Regressor (SVR) with various hyperparameters. How does the best SVR predictor perform?\n","036aec3c":"Let's take a look at the distribution of the `ocean-proximity` feature using the `value_counts` methods since it is non-numeric","bc1aa02b":"Now let's see if the data has been split according the true representation of categories in the dataset","c07333ee":"### Linear Regression","03361bc6":"### Cross-Validation","219c8cd0":"### Randomized Search\nGive a range for each hyperparameter and it tries out a random combination of each hyperparameter by choosing random values for each one within the range","0f8f9946":"And the best estimator as:","4950f3b0":"### Random Forest ","3a355c54":"Similarly, we can view the combination of cv scores and parameters that got them","a52cc153":"But what about the `ocean_proximity` which is non-numeric?","09aed04d":"Using the `info` method, we can get information about the total number of rows, each feature's type and the number of *non-null* values. Below, we can see that there are some null values for the `ocean_proximity` feature and that there are **20640** rows in the dataset. Further, the `ocean_proximity` feature is non-numeric","c33aa192":"To ensure that the test set is representative of all categories in the whole dataset, we can use stratified sampling"}}