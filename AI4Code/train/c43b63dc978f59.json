{"cell_type":{"b1ff57c8":"code","59cf38b8":"code","14b75924":"code","7246d907":"code","b9355dbf":"markdown","4536eb11":"markdown","cddef05b":"markdown","177010c7":"markdown","ed6dcd7c":"markdown"},"source":{"b1ff57c8":"# imports\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom sklearn.mixture import GaussianMixture as GMM\n\n# suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# reading just the train data, I do not do estimations yet\ntrain = pd.read_csv(\n    '\/kaggle\/input\/tabular-playground-series-jan-2021\/train.csv', \n    index_col=0\n)","59cf38b8":"fig, ax = plt.subplots(2,1, figsize=(20, 5), sharex=True)\nsns.kdeplot(train.target, ax=ax[0])\nsns.boxplot(train.target, ax=ax[1], fliersize=10, **{'flierprops':{'alpha':.2}})\nfig.show()","14b75924":"# fitting the GMM model to the target data\nclf = (\n    GMM(\n        n_components=5, \n        max_iter=200, \n        random_state = 0\n    )\n    .fit(\n        np.array(train.target)\n        .reshape(-1, 1)\n    )\n)","7246d907":"# configuring plot\nplt.figure(figsize=(20,5))\nplt.title('The estimated gaussians behind the multimodal target data')\n\n# plotting the original kde (in blue)\nsns.kdeplot(train.target)\n\n# plotting the estimation (in red)\nxpdf = np.linspace(5,10,100000).reshape((-1,1))\ndensity = np.exp(clf.score_samples(xpdf))\nplt.plot(xpdf, density, '-r', alpha=.5)\n\n# plotting the estimated underlying normal distributions\nfor i in range(clf.n_components):\n    pdf = (\n        clf.weights_[i]\n        * stats.norm(\n            clf.means_[i, 0],\n            np.sqrt(clf.covariances_[i, 0])\n        ).pdf(xpdf)\n    )\n    plt.fill(xpdf, pdf, facecolor='gray',\n             edgecolor='none', alpha=0.3)\n\n# trimming the outlier at 0\nplt.xlim(5, 10)\n\n# putting the 5 distributions GMM has found into a DF\ngaussians = pd.concat(\n    [\n        pd.DataFrame(clf.means_, columns=['means']),\n        pd.DataFrame(clf.covariances_.squeeze(), columns=['covariances']),\n        pd.DataFrame(clf.weights_, columns=['weights'])\n    ],\n    axis=1)\n\nplt.table(\n    cellText=gaussians.values,\n    rowLabels=gaussians.index,\n    colLabels=gaussians.columns,\n    cellLoc = 'right', rowLoc = 'center', loc='bottom',\n    bbox=[.015,.45,.35,.5])\n\nplt.show()\n","b9355dbf":"### Plotting the actual data, the estimated normal distributions, and their sum","4536eb11":"### Trying to find the distributions that make up this curve\nThis looks like a few normal distributions on top of each other, and some outlier at around 0.\nThere is a distribution just for that, called [Gaussian Mixture Model](https:\/\/scikit-learn.org\/stable\/modules\/mixture.html). Now I try to model the target distribution (blue) with GMM (red).","cddef05b":"### Let's look at the shape of the target distribution\n","177010c7":"### Exploratory analysis of the 2021 tabular playground target data\n\nDumping the dataset into several ML algorithms then choosing the best one, then run a hyperparameter-tuning and automatic feature engineering do not teach you anything about the problem. \n\nI like to go manual and apply a lot of common sense and experimentation. To help this, I always visualize my hypotheses and findings. And I stare a lot at pairplots, sometimes distorted, coloured by different data aspects, zoomed in-and-out. I do this with my estimations and errors too, down to the individual record.","ed6dcd7c":"Honestly, I'm a bit dissapointed. I was hoping for a near-perfect fit from the GMM, but for the sake of the experiment, I push forward with this. So now I will try to find\/build features that are strong predictors to one of the 5 gaussians, and see, where that path takes us."}}