{"cell_type":{"9d658aef":"code","215eedfc":"code","58d22b39":"code","d8b219c0":"code","2c512efa":"code","3a44b5b5":"code","5835843b":"code","4fefd99b":"code","8f221c86":"code","0e6d2fcb":"code","ecda75ab":"code","07ff8bdc":"code","16dbc9aa":"code","a1ad4973":"code","6c1e2063":"code","2c557bc9":"code","61ca27b2":"code","98bc4c75":"code","f76057a3":"code","cb5143bf":"code","5aa96307":"code","ab2d91ce":"code","cb419678":"markdown","1d5c4265":"markdown","1f78f933":"markdown","bd993670":"markdown","23bca840":"markdown","c89530c7":"markdown","cd49edf2":"markdown","9366396e":"markdown","5ebb474d":"markdown"},"source":{"9d658aef":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nfrom itertools import chain\nfrom kaggle.competitions import twosigmanews\n\nenv = twosigmanews.make_env()\nprint('Done!')","215eedfc":"(market_train_df, news_train_df) = env.get_training_data()\nprint(f'market train df shape: {market_train_df.shape}')\nprint(f'news train df shape: {news_train_df.shape}')","58d22b39":"market_train_df.head()","d8b219c0":"market_train_df.info()","2c512efa":"missing_count = market_train_df.isna().sum()\nmissing_count","3a44b5b5":"plt.figure(figsize=(12,8))\nplt.bar(missing_count.index, missing_count.values)\nplt.xticks(rotation=45)\nplt.show()","5835843b":"# show number of missing values over time\n\nmissing_col = ['returnsClosePrevMktres1', 'returnsOpenPrevMktres1', \n               'returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\ndf_na = market_train_df[market_train_df.isnull().any(axis=1)]\nmissing_day = df_na.loc[:, missing_col].isnull().groupby(df_na.time).sum()\n\nf, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharey=True, figsize=(12,8))\nax1.plot(missing_day.index, missing_day.returnsOpenPrevMktres1)\nax1.set_ylim(0,100)\nax1.set_title('returnsClosePrevMktres1')\nax2.plot(missing_day.index, missing_day.returnsOpenPrevMktres1)\nax2.set_ylim(0,100)\nax2.set_title('returnsOpenPrevMktres1')\nax3.plot(missing_day.index, missing_day.returnsClosePrevMktres10)\nax3.set_ylim(0,200)\nax3.set_title('returnsClosePrevMktres10')\nax4.plot(missing_day.index, missing_day.returnsOpenPrevMktres10)\nax4.set_ylim(0,200)\nax4.set_title('returnsOpenPrevMktres10')\nplt.show()","4fefd99b":"print(f'number of unique asset Codes: {market_train_df.assetCode.unique().shape[0]}')\nprint(f'number of unique asset Names: {market_train_df.assetName.unique().shape[0]}')","8f221c86":"market_train_df.describe()","0e6d2fcb":"# histograme for log10(volume)\n\nplt.hist(market_train_df.volume.apply(lambda x: np.log10(x) if x!=0 else 0), bins=50)\nplt.title('volume')\nplt.show()","ecda75ab":"# there are some very high open(10k) and close value(1.5k) \nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10,4))\nax1.hist(market_train_df.open, bins=50, range=(0,500))\nax1.set_title('open')\nax2.hist(market_train_df.close, bins=50, range=(0,500))\nax2.set_title('close')\nplt.show()","07ff8bdc":"f, axes = plt.subplots(3, 3, sharey=True, figsize=(15,15))\nfor i in range(3):\n    for j in range(3):\n        axes[i,j].hist(market_train_df.iloc[:, 6+i*3+j], range=(-0.5,0.5), bins=50)\n        axes[i,j].set_title(market_train_df.columns[6+i*3+j])\nplt.show()","16dbc9aa":"news_train_df.head()","a1ad4973":"news_train_df.info()","6c1e2063":"# no missing value here\nnews_train_df.isna().sum().sum()","2c557bc9":"news_train_df.describe()","61ca27b2":"# plot histogram of all the numeric columns\nnews_train_df.select_dtypes(include=[np.number]).hist(figsize=(15,15))\nplt.show()","98bc4c75":"n_codes = len(set(chain(*news_train_df['assetCodes'].str.findall(f\"'([\\w\\.\/]+)'\"))))\nprint(f'number of unique asset Codes in news set: {n_codes}')\nprint(f'number of unique asset Names in news set: {news_train_df.assetName.unique().shape[0]}')\nprint('*'*50)\nprint(f'number of unique asset Codes in market set: {market_train_df.assetCode.unique().shape[0]}')\nprint(f'number of unique asset Names in market set: {market_train_df.assetName.unique().shape[0]}')","f76057a3":"days = env.get_prediction_days()\n(market_obs_df, news_obs_df, predictions_template_df) = next(days)\nprint(f'market_obs_df shape: {market_obs_df.shape}')\nprint(f'news_obs_df shape: {news_obs_df.shape}')\nprint(f'predictions_template_df shape: {predictions_template_df.shape}')","cb5143bf":"market_obs_df.head()","5aa96307":"news_obs_df.head()","ab2d91ce":"print(f'date in market set: {market_obs_df.time.dt.date.unique().tolist()}')\nprint(f'date in news set: {news_obs_df.time.dt.date.unique().tolist()}')","cb419678":"### missing values:\n- There are missing values in 4 returns columns spreadding out over lots of trading days. The reason is pointed out in the data description: \"The set of included instruments changes daily and is determined based on the amount traded and the availability of information. This means that there may be instruments that enter and leave this subset of data. There may therefore be gaps in the data provided, and this does not necessarily imply that that data does not exist.\"","1d5c4265":"### continuous variables","1f78f933":"- Most of the returns values are close to 0, although in extreme cases the number can be quite large. Also, the distributions of 10 day return are wider than 1day return, since it's over longer period.\n- Given the range of next 10 day return and the range of y (-1,1) the submission wants, consider using tanh(next 10 day return) as target for prediction. ","bd993670":"There are little more unique asset codes than names. It means there are cases that multiple asset codes correspond to the same asset name(there is an \"unknown\" asset name). Also note that the predictions are based on asset codes. ","23bca840":"## Let's take a look at market data","c89530c7":"## I haven't seen anyone doing EDA on test set yet, but there is something that may have been overlooked.\n\nEach time we call env.get_prediction_days(),  the Two Sigma method will spit out a set of market data in which each row is the data for a asset that we will need to make prediction. It also spit out the news **since the last trading day**. So as you can see, for 2017-1-3, the news set acutally contains news from 2016-12-30 to 2017-1-3(weekends and holidays). Also note that Two Sigma split days at 22:00, time after 22:00 and before 0:00 is considered the next day. So it looks like attention need to be paid when joining market and news set. Simply joining by date and assetCode may throw away useful information, or put 'future'(arguably) information in training data. ","cd49edf2":"It appears that there are much more asset codes and names in the news dataframe than the market dataframe. So there are assets in the news that are not included in market set. ","9366396e":"## Next,  news data","5ebb474d":"Thanks!"}}