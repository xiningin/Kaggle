{"cell_type":{"1dbd8f6e":"code","1ea64534":"code","0a3be6cc":"code","784a91a2":"code","a05fc6ab":"code","b37be17d":"code","12dd38e3":"code","c1552581":"code","fb1a6c62":"code","d5d4f002":"code","21d34d2f":"markdown","657a6602":"markdown","473b3329":"markdown","235bf2b8":"markdown","63700b63":"markdown","cd94e01c":"markdown","00fc4e03":"markdown"},"source":{"1dbd8f6e":"from tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom hyperopt import hp, Trials, fmin, tpe, STATUS_OK\nimport lightgbm as lgb\nimport ast\nimport csv\nimport pickle\n# Optional if you want to run it locally and inspect it in real time using Tensorboard\n#from tensorboardX import SummaryWriter \n\nout_file = 'LGB.csv'\nMAX_EVALS = 5 #This has been set to a small number for demonstration. Increase it!\nN_FOLDS = 5\npbar = tqdm(total=MAX_EVALS, desc=\"Hyperopt\")","1ea64534":"# Loading the data\ntrain = pd.read_csv('..\/input\/train.csv', parse_dates=['Dates'])\n\n# Wrangling the dataset\ntrain.drop_duplicates(inplace=True)\ntrain.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\n\nimp = SimpleImputer(strategy='mean')\n\nfor district in train['PdDistrict'].unique():\n    train.loc[train['PdDistrict'] == district, ['X', 'Y']] = imp.fit_transform(\n        train.loc[train['PdDistrict'] == district, ['X', 'Y']])\n\n# Feature Engineering\ndef feature_engineering(data):\n    data['Date'] = pd.to_datetime(data['Dates'].dt.date)\n    data['n_days'] = (\n        data['Date'] - data['Date'].min()).apply(lambda x: x.days)\n    data['Day'] = data['Dates'].dt.day\n    data['DayOfWeek'] = data['Dates'].dt.weekday\n    data['Month'] = data['Dates'].dt.month\n    data['Year'] = data['Dates'].dt.year\n    data['Hour'] = data['Dates'].dt.hour\n    data['Minute'] = data['Dates'].dt.minute\n    data['Block'] = data['Address'].str.contains('block', case=False)\n    \n    data.drop(columns=['Dates','Date','Address'], inplace=True)\n        \n    return data\n\ntrain = feature_engineering(train)\ntrain.drop(columns=['Descript','Resolution'], inplace=True)\n\n# Encoding Categorical Variables\nle1 = LabelEncoder()\ntrain['PdDistrict'] = le1.fit_transform(train['PdDistrict'])\n\nle2 = LabelEncoder()\ny = le2.fit_transform(train.pop('Category'))\n\n# Forming the dataset\ntrain_set = lgb.Dataset(\n    train, label=y, categorical_feature=['PdDistrict'], free_raw_data=False)","0a3be6cc":"def param_flatten(d, params={}):\n    \"\"\"Function that accepts a dictionary with nested dictionaries and returns a flattened dictionary\"\"\"\n    for key, value in d.items():\n        if not isinstance(value, dict):\n            params[key] = value\n        else:\n            param_flatten(value, params)\n            \n    return params","784a91a2":"def objective(params, n_folds=N_FOLDS):\n    \"\"\"Objective function for LightGBM Hyperparameter Optimization\"\"\"\n\n    # Keep track of evals\n    global ITERATION\n    ITERATION += 1\n    \n    # We need all the parameters in a flattened dictionary\n    params = param_flatten(params)\n\n    # Make sure parameters that need to be integers are integers\n    for key, value in params.items():\n        if key in ['num_leaves', 'min_data_in_leaf']:\n            params[key] = int(value)\n            \n    print(params)\n\n    # Perform n_folds cross validation.\n    # If you download this notebook you can add callbacks=[logspy] to use Tensorboard\n    try:\n        cv_results = lgb.cv(\n            params,\n            train_set,\n            num_boost_round=100,\n            nfold=n_folds,\n            early_stopping_rounds=10,\n            metrics='multi_logloss')\n\n        # Extract the best score\n        loss = min(cv_results['multi_logloss-mean'])\n        print('loss: ',loss)\n\n        # Boosting rounds that returned the highest cv score\n        epochs = np.argmin(cv_results['multi_logloss-mean']) + 1\n        \n        # Write to the csv file ('a' means append)\n        of_connection = open(out_file, 'a')\n        writer = csv.writer(of_connection)\n        writer.writerow([loss, params, ITERATION, epochs])\n\n        pbar.update()\n\n        # Dictionary with information for evaluation\n        return {\n            'loss': loss,\n            'params': params,\n            'iteration': ITERATION,\n            'epochs': epochs,\n            'status': STATUS_OK\n        }\n    except Exception as e:\n        print('EXCEPTION\\n')\n        print(e)\n        return{'status': 'fail'}","a05fc6ab":"space = {\n    'boosting':\n    hp.choice('boosting', [\n        {\n            'boosting': 'gbdt',\n            'max_delta_step': hp.quniform('gbdt_max_delta_step', 0, 2, 0.1),\n            'min_data_in_leaf': hp.quniform('gbdt_min_data_in_leaf', 10, 30,\n                                            1),\n            'num_leaves': hp.quniform('gbdt_num_leaves', 20, 40, 1)\n        },\n        {\n            'boosting': 'dart',\n            'max_delta_step': hp.quniform('dart_max_delta_step', 0, 2, 0.1),\n            'min_data_in_leaf': hp.quniform('dart_min_data_in_leaf', 10, 30,\n                                            1),\n            'num_leaves': hp.quniform('dart_num_leaves', 20, 40, 1),\n        },\n    ]),\n    'objective':\n    'multiclass',\n    'num_class':\n    39\n}","b37be17d":"def run_trials():\n    \"\"\"Function to run the trials and save the results after every iteration.\n    This is usefull in case you need to interupt the execution and continue from where you left.\"\"\"\n\n    trials_step = 1  # how many additional trials to do after loading saved trials. 1 = save after iteration\n    max_trials = 1  # initial max_trials. put something small to not have to wait\n\n    try:  # try to load an already saved trials object, and increase the max\n        trials = pickle.load(open(\"LGB.hyperopt\", \"rb\"))\n        print(\"Found saved Trials! Loading...\")\n        max_trials = len(trials.trials) + trials_step\n        print(\"Rerunning from {} trials to {} (+{}) trials\".format(\n            len(trials.trials), max_trials, trials_step))\n    except:  # create a new trials object and start searching\n        trials = Trials()\n\n    best = fmin(\n        fn=objective,\n        space=space,\n        algo=tpe.suggest,\n        max_evals=max_trials,\n        trials=trials)\n\n    print(\"Best:\", best)\n\n    # save the trials object\n    with open(\"LGB.hyperopt\", \"wb\") as f:\n        pickle.dump(trials, f)","12dd38e3":"#File to save first results\n\nof_connection = open(out_file, 'w')\nwriter = csv.writer(of_connection)\n\n# Write the headers to the file\nwriter.writerow(\n    ['loss', 'params', 'iteration', 'epochs'])\nof_connection.close()","c1552581":"ITERATION = 0","fb1a6c62":"while ITERATION <= MAX_EVALS:\n    run_trials()\npbar.close()","d5d4f002":"trials = pickle.load(open(\"LGB.hyperopt\", \"rb\"))\nresults = pd.DataFrame(trials.results)\n\nbayes_params = pd.DataFrame(columns = list(results.loc[0, 'params'].keys()),\n                            index = list(range(len(results))))\n\n# Add the results with each parameter a different column\nfor i, params in enumerate(results['params']):\n    bayes_params.loc[i, :] = list(params.values())\n    \nbayes_params['loss'] = results['loss']\nbayes_params['iteration'] = results['iteration']\nbayes_params.sort_values('loss', inplace=True)\n\nbayes_params.head()","21d34d2f":"# Bayesian Optimization \n### Objective function to minimize","657a6602":"# Importing libraries","473b3329":"### Function that flattens nested dictionaries","235bf2b8":"# Helper functions\n### (Optional) Function to write Tensorboard logs so that you can use Tensorboard to monitor the process\n```python\n# This is a Markdown cell it will not run\n# Convert it to a code cell if you download the Notebook and want to use Tensorboard\ndef logspy(env):\n    \"\"\" Function that writes logs that can be read by Tensorboard.\n    It has no use if you run this kernel on Kaggle\"\"\"\n    global t_writer\n    \n    if env.iteration == 0:\n        t_writer = SummaryWriter('..\/logs\/LGB'+str(ITERATION))\n        \n    t_writer.add_scalar('train', env.evaluation_result_list[0][1], env.iteration)\n    t_writer.add_scalar('val', env.evaluation_result_list[0][2], env.iteration)\n    \n    return\n```","63700b63":"### Space over which to search","cd94e01c":"# Importing and preprocessessing data","00fc4e03":"Note: This Notebook is an example of the Bayesian Optimization that took place during the hyperparameters of the model, and part of the [SF-Crime Analysis & Prediction](https:\/\/www.kaggle.com\/yannisp\/sf-crime-analysis-prediction). Please start there for a full analysis."}}