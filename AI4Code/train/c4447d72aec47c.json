{"cell_type":{"96cc47aa":"code","b85efcf1":"code","5115662e":"code","7f32ea02":"code","5af272ef":"code","e06a4a44":"code","57d63bb4":"code","b98d2dc0":"code","5080bcab":"code","a3f61e1a":"code","9d6e2420":"code","6d796c74":"code","cfd39890":"code","651fd8f6":"code","c52f35cc":"code","01133646":"code","b0a55641":"code","01a32cd0":"code","03df2c75":"code","104583c5":"code","b40348d5":"code","9661ee5d":"code","03ab8093":"code","e1483d4c":"code","3f2d849d":"code","3cf329d7":"code","89187a1e":"code","0c500167":"code","cff5c03b":"code","dc594fac":"code","675a3870":"code","fc5a7253":"code","3994d814":"code","f132d51d":"code","faa14b96":"code","7d663539":"code","b0c0e5c2":"code","833c85d8":"code","ff376b77":"code","b7b2e21d":"code","97bfb3f9":"code","ee0f6b62":"code","07a494c7":"code","5a8c90f5":"code","380dede0":"code","b53150a7":"code","31f3d919":"code","be12f653":"code","5cade700":"code","b6a5958f":"code","ea7fec59":"code","00550a82":"code","c516977c":"code","29d215e0":"code","5f9bfe3b":"code","86d2b687":"code","81097db1":"code","d83a714e":"code","f9414f68":"code","fb463857":"code","db9afcd6":"code","d0447b51":"code","d8d7670e":"code","fc90ae75":"code","629e06bb":"code","d50cec9d":"code","f6edf78b":"code","f6de9028":"code","37822441":"code","2ecee478":"code","6066f6f9":"code","0a49471f":"code","896e1559":"code","fab06928":"code","e1b7f858":"code","4aea0c48":"code","f5ff030d":"code","ca44a520":"code","03d4a2b5":"code","759e7817":"code","54b9982a":"code","7ac57e9f":"code","30b23186":"code","85318fe8":"code","68132543":"code","39dd5611":"code","617659ef":"code","0ea54b6e":"code","29ce440b":"code","2970a936":"code","ded99f73":"code","4fae731b":"code","495bd8a2":"code","adbc114c":"code","7a621eb6":"code","8a93b48b":"code","9f18bc95":"code","c291b873":"markdown","ccb48982":"markdown","65d69b46":"markdown","20467b24":"markdown","68b6ef53":"markdown","deedd80a":"markdown","7a9ceb1c":"markdown","52933831":"markdown","3f80d11c":"markdown","0ae8962a":"markdown","a11baf5f":"markdown","474c8745":"markdown","432169de":"markdown","8685a8a8":"markdown","cce14d33":"markdown","e0a844ad":"markdown","48b9cf7d":"markdown","e8d7ae07":"markdown","80f9b473":"markdown","1e635b8d":"markdown","3eb9cbbe":"markdown","9093a77e":"markdown","4fb70d29":"markdown","95b93a5f":"markdown","322157d9":"markdown","c86beadc":"markdown","9038125b":"markdown","5507938a":"markdown","9c07b84a":"markdown","f02ea7ff":"markdown","1e2c7a06":"markdown","706a3dca":"markdown","4b81f350":"markdown","3251f54c":"markdown","09215334":"markdown","bed4c06b":"markdown","c20f47f9":"markdown","fd8e6c72":"markdown","79d12cbf":"markdown","4b3f5449":"markdown","886592e6":"markdown","239abf71":"markdown"},"source":{"96cc47aa":"#importing the libraries\nfrom datetime import datetime \nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom scipy.sparse import csr_matrix\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import StandardScaler\nimport pickle\nfrom scipy.sparse import hstack\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n%matplotlib inline","b85efcf1":"#import gensim.downloader as api\n\n#w2v = api.load('word2vec-google-news-300')\n","5115662e":"df = pd.read_csv('..\/input\/love-in-the-time-of-screens\/data.csv')","7f32ea02":"df","5af272ef":"df.shape","e06a4a44":"df.columns","57d63bb4":"print(\"No of NULL values : \", sum(df.isnull().any()))","b98d2dc0":"duplicates = df.duplicated()\nprint(\"There are {} duplicate users in the data..\".format(sum(duplicates)))","5080bcab":"#univariate analysis of age\nsns.distplot(df['age'])\nplt.show()\nprint('='*50)\nprint(df['age'].describe())","a3f61e1a":"#univariate analysis on 'status'\nsns.countplot(df['status'])\nplt.grid()\nplt.show()\n\nprint('='*50)\nprint(df['status'].describe())","9d6e2420":"#univariate analysis on 'sex'\nsns.countplot(df['sex'])\nplt.show()\n\nprint('='*50)\nprint(df['sex'].describe())","6d796c74":"#univariate analysis on 'orientation'\nsns.countplot(df['orientation'])\nplt.show()\n\nprint('='*50)\nprint(df['orientation'].describe())","cfd39890":"#univariate analysis on 'drinks'\nsns.countplot(df['drinks'])\nplt.show()\n","651fd8f6":"#univariate analysis on 'drugs'\nsns.countplot(df['drugs'])\nplt.show()","c52f35cc":"#univariate analysis on 'height'\nsns.distplot(df['height'])\nplt.show()\nprint('='*50)\nprint(df['height'].describe())","01133646":"#univariate analysis on 'smokes'\nplt.figure(figsize = (8,5))\nsns.countplot(df['smokes'])\nplt.show()","b0a55641":"plt.figure(figsize = (8,5))\nsns.countplot(df['new_languages'])\nplt.show()","01a32cd0":"plt.figure(figsize = (15,7))\nsns.countplot(df['body_profile'])\nplt.show()","03df2c75":"sns.boxplot(x = df['education_level'])\nplt.grid()\nplt.show()\nprint('='*50)\nprint(df['education_level'].describe())","104583c5":"plt.figure(figsize = (10,7))\ndf['job'].value_counts().plot(kind = 'bar')\nplt.show()","b40348d5":"word_count = df['bio'].str.split().apply(len).value_counts()\nword_dict = dict(word_count)\nword_dict = dict(sorted(word_dict.items(), key=lambda kv: kv[1]))\n\n\nind = np.arange(len(word_dict))\nplt.figure(figsize=(20,8))\np1 = plt.bar(ind, list(word_dict.values()))\n\nplt.ylabel('Number of bios')\nplt.xlabel('Number of words in each bio')\nplt.title('Number of words per bio')\nplt.xticks(ind, list(word_dict.keys()))\nplt.show()","9661ee5d":"sns.distplot(word_count.values)\nplt.grid()\nplt.title('Distribution of number of words per bio')\nplt.xlabel('Number of words in each bio')\nplt.show()","03ab8093":"plt.figure(figsize = (15,7))\nsns.countplot(df['interests'])\nplt.xticks(rotation = 90)\nplt.show()","e1483d4c":"plt.figure(figsize = (7,5))\nsns.countplot(df['location_preference'])\nplt.show()","3f2d849d":"#removing the '\/' and whitespaces\ndf['job'] = df['job'].str.replace('\/','_')\ndf['job'] = df['job'].str.replace(' ','')\ndf['job'].value_counts()","3cf329d7":"df['location']","89187a1e":"#removing ',' and whitespaces\ndf['location'] = df['location'].str.replace(',', ' ')\ndf['location'].value_counts()","0c500167":"#removing whitespaces\ndf['pets'] = df['pets'].str.replace(' ', '_')\ndf['pets'].value_counts()","cff5c03b":"#language = list(train_df['language'].values)\ndef lang(language):\n    lang_list = []\n    for i in language:\n        temp = \"\"\n        for j in i.split(','): \n            j = j.replace(' ','')\n            temp += j.strip() + \" \" \n            temp = temp.replace('(','_')\n            temp = temp.replace(')','_')\n        lang_list.append(temp.strip())\n    return lang_list","dc594fac":"df['cleaned_language'] = lang(list(df['language'].values))\ndf.drop(['language'], axis=1, inplace=True)\ndf.head(2)","675a3870":"df['new_languages'] = df['new_languages'].str.replace(' ', '_')\ndf['new_languages'].value_counts()","fc5a7253":"df['body_profile'] = df['body_profile'].str.replace(' ', '_')\ndf['body_profile'].value_counts()","3994d814":"df['bio']","f132d51d":"# https:\/\/stackoverflow.com\/a\/47091490\/4084039\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","faa14b96":"# https:\/\/gist.github.com\/sebleier\/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","7d663539":"#printing a few random bios\nprint(10, df['bio'].values[10])\nprint('-'*50)\nprint(1033, df['bio'].values[1033])\nprint('-'*50)\nprint(44, df['bio'].values[44])\nprint('-'*50)\nprint(777, df['bio'].values[777])","b0c0e5c2":"def preprocess_text(text_data):\n    preprocessed_text = []\n    for sentence in tqdm(text_data):\n        sent = decontracted(sentence)\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        # https:\/\/gist.github.com\/sebleier\/554280\n        sent = ' '.join(e for e in nltk.word_tokenize(sent) if e.lower() not in stopwords)\n        preprocessed_text.append(sent.lower().strip())\n    return preprocessed_text","833c85d8":"preprocessed_bio = preprocess_text(df['bio'].values)","ff376b77":"#printing a few random bios\nprint(10, preprocessed_bio[10])\nprint('-'*50)\nprint(1033, preprocessed_bio[1033])\nprint('-'*50)\nprint(44, preprocessed_bio[44])\nprint('-'*50)\nprint(777, preprocessed_bio[777])","b7b2e21d":"df['bio'] = preprocessed_bio","97bfb3f9":"df['interests'] = df['interests'].str.replace(' ', '_')\ndf['interests'].value_counts()","ee0f6b62":"df['other_interests']=df['other_interests'].str.replace(' ','_')\ndf['other_interests'].value_counts()","07a494c7":"df['location_preference'] = df['location_preference'].str.replace(' ', '_')\ndf['location_preference'].value_counts()","5a8c90f5":"df['status'] = df['status'].str.replace(' ', '_')\ndf['status'].value_counts()","380dede0":"scaler = StandardScaler()\nscaler.fit(df['age'].values.reshape(-1, 1))\ndf['age']=scaler.transform(df['age'].values.reshape(-1, 1))","b53150a7":"df['age'].head(10)","31f3d919":"scaler = StandardScaler()\nscaler.fit(df['height'].values.reshape(-1, 1))\ndf['height']=scaler.transform(df['height'].values.reshape(-1, 1))","be12f653":"df['height'].head(10)","5cade700":"scaler = StandardScaler()\nscaler.fit(df['education_level'].values.reshape(-1, 1))\ndf['education_level']=scaler.transform(df['education_level'].values.reshape(-1, 1))\n","b6a5958f":"df['education_level'].head(10)","ea7fec59":"df.head(1)","00550a82":"if not os.path.isfile('..\/input\/love-in-the-time-of-screens\/train.csv'):\n    # create the dataframe and store it in the disk for offline purposes..\n    df.to_csv(\"train.csv\", index=False)","c516977c":"df = pd.read_csv(\"..\/input\/love-in-the-time-of-screens\/train.csv\")","29d215e0":"df.shape","5f9bfe3b":"len(df['bio'])","86d2b687":"d2v = Doc2Vec.load(\"..\/input\/love-in-the-time-of-screens\/d2v.model\")","81097db1":"#vectorising the df['bio']\nbio_vectorized = [d2v.docvecs[str(i)] for i in range(len(df['bio']))]","d83a714e":"np.asarray(bio_vectorized).shape","f9414f68":"#tokenized_locations = [word_tokenize(df['location'][i]) for i in range(len(df['location']))]","fb463857":"\ndef w2v_locations(location_i_tokenized):\n    vectorized_location_i = 0\n    for word in location_i_tokenized:\n        try:\n            vectorized_location_i += w2v[word]\n        except:\n            w2v[word] = np.zeros_like(w2v['word'])\n        finally:\n            vectorized_location_i += w2v[word]\n    return vectorized_location_i","db9afcd6":"#vectorized_locations = [w2v_locations(location_i_tokenized) for location_i_tokenized in tokenized_locations]","d0447b51":"#with open('vectorized_locs.pickle', 'wb') as f:\n #   pickle.dump(vectorized_locations, f)","d8d7670e":"with open('..\/input\/love-in-the-time-of-screens\/vectorized_locs.pickle' ,\"rb\", buffering=0) as f:\n    vectorized_locations = pickle.load(f)","fc90ae75":"np.asarray(vectorized_locations)","629e06bb":"df.head(1)","d50cec9d":"df.columns","f6edf78b":"#df['cleaned_language'] = df['cleaned_language'].str.replace('_', ' ')","f6de9028":"#tokenized_langs = [word_tokenize(df['cleaned_language'][i]) for i in range(len(df['cleaned_language']))]","37822441":"#vectorized_language = [w2v_locations(location_i_tokenized) for location_i_tokenized in tokenized_langs]","2ecee478":"#with open('vectorized_langs.pickle', 'wb') as f:\n #   pickle.dump(vectorized_language, f)","6066f6f9":"with open('..\/input\/love-in-the-time-of-screens\/vectorized_langs.pickle' ,\"rb\", buffering=0) as f:\n    vectorized_language = pickle.load(f)","0a49471f":"def ohe(feature):\n    vectorizer = CountVectorizer(binary = True)\n    vectorized_feature = vectorizer.fit_transform(feature.values)\n    return vectorized_feature\n","896e1559":"status_vec = ohe(df['status'])","fab06928":"orientation_vec = ohe(df['orientation'])","e1b7f858":"drinks_vec = ohe(df['drinks'])","4aea0c48":"drugs_vec = ohe(df['drugs'])","f5ff030d":"job_vec = ohe(df['job'])","ca44a520":"pets_vec = ohe(df['pets'])","03d4a2b5":"smokes_vec = ohe(df['smokes'])","759e7817":"new_languages_vec = ohe(df['new_languages'])","54b9982a":"body_vec = ohe(df['body_profile'])","7ac57e9f":"dropped_vec = ohe(df['dropped_out'])","30b23186":"interests_vec = ohe(df['interests'])","85318fe8":"other_interests_vec = ohe(df['other_interests'])","68132543":"location_pref_vec = ohe(df['location_preference'])","39dd5611":"df['sex'] = df['sex'].str.replace('m', 'male')\ndf['sex'] = df['sex'].str.replace('f', 'female')\ndf['sex']","617659ef":"sex_vec = ohe(df['sex'])","0ea54b6e":"X = hstack((np.asarray(df['age']).reshape(-1,1), status_vec, sex_vec, orientation_vec, drinks_vec, drugs_vec, np.asarray(df['height']).reshape(-1,1), job_vec, np.asarray(vectorized_locations), pets_vec, smokes_vec, new_languages_vec, body_vec, np.asarray(df['education_level']).reshape(-1,1), dropped_vec, np.asarray(bio_vectorized), interests_vec, other_interests_vec, location_pref_vec, np.asarray(vectorized_language))).tocsr()","29ce440b":"X.shape","2970a936":"u_u_similarities = cosine_similarity(X)","ded99f73":"u_u_similarities","4fae731b":"u_u_reco = u_u_similarities*100","495bd8a2":"#np.fill_diagonal(u_u_reco, 0)","adbc114c":"u_u_reco","7a621eb6":"index_values = list(df['user_id'])\ncolumn_values = list(df['user_id'])\ndf_to_submit = pd.DataFrame(data = u_u_reco,  \n                  index = index_values,  \n                  columns = column_values)\n","8a93b48b":"df_to_submit","9f18bc95":"df_to_submit.to_csv('submission.csv')","c291b873":"### 3.1.9 'other_interests'","ccb48982":"### 3.1.11 'status'","65d69b46":"### 2.9 'new_languges'","20467b24":"## 4.1 Doc2Vec on 'bio'","68b6ef53":"### 4.4 One hot encoding the rest of the features","deedd80a":"# 4. Vectorizing Categorial features","7a9ceb1c":"### 1.2 Handling duplicate users","52933831":"### 2.8 'smokes'","3f80d11c":"# 3. Data preprocessing\n## 3.1 Preprocessing Categorical features ","0ae8962a":"### 3.1.4 'language'","a11baf5f":"### 2.11 'education_level'","474c8745":"### 3.1.6 'body_profile'","432169de":"### 2.10 'body_profile'","8685a8a8":"### 1.1 Checking for NULL values","cce14d33":"### 2.15 'location_preference'","e0a844ad":"### 4.4 Concatenating all the features","48b9cf7d":"### 3.2.3 'education_level'","e8d7ae07":"### 2.1 'age'","80f9b473":"### 3.2.1 'age'","1e635b8d":"### 2.14 'interests'","3eb9cbbe":"### 2.12 'job'","9093a77e":"### 2.13 'bio'","4fb70d29":"# 2. Exploratory Data Analysis","95b93a5f":"### 3.1.7 'bio'","322157d9":"### 2.2 'status'","c86beadc":"### 3.1.1 'job'","9038125b":"### 4.3 Word2Vec on 'cleaned_language'","5507938a":"### 3.2.2 'height'","9c07b84a":"### 3.1.3 'pets'","f02ea7ff":"Let's try to visualise some of the features which seem relevant","1e2c7a06":"### 2.4 'orientation'","706a3dca":"## 3.2 Preprocessing Numerical Data","4b81f350":"### 2.7 'height'","3251f54c":"### 3.1.5 'new_languages'","09215334":"### 3.1.10 'location_preference'","bed4c06b":"### 2.6 'drugs'","c20f47f9":"### 3.1.8 'interests'","fd8e6c72":"### 2.3 'sex'","79d12cbf":"### 4.2 Word2Vec on 'location'","4b3f5449":"# 1. Data overview","886592e6":"### 3.1.2 'location'","239abf71":"### 2.5 'drinks'"}}