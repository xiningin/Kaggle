{"cell_type":{"095e97f8":"code","96c91fb1":"code","1d7d9db0":"code","5591b68c":"code","d9d3a2bd":"code","e726e686":"code","627bc226":"code","aaadf99f":"code","c07aa3ff":"code","8658bf54":"code","0fe9e141":"code","21ee7725":"code","fb368156":"code","544a0e69":"code","f9c213bd":"code","eb68b5dc":"code","f97dcda1":"code","66269b10":"code","56666f64":"code","19d054c0":"code","d985e061":"code","df72d9f5":"code","ae4effb3":"code","6a683c0b":"code","1884e4a5":"code","7d216234":"code","1aa592ee":"code","d985b8af":"code","1a154e72":"code","0282d542":"code","bef837de":"code","f3ffebf4":"code","6d9497ce":"code","991913d1":"code","216bc9e5":"code","a69feac0":"code","32974a02":"code","c755655b":"code","dfb167dc":"code","5a5192e2":"code","85bf6d30":"code","4cfff13f":"code","75c44e6e":"code","eff95758":"code","764d6171":"code","911121c8":"code","34fcf9f3":"code","50a162f8":"code","cd7e9a22":"markdown","c2d8f697":"markdown","78ecd129":"markdown","61834e89":"markdown","181e74c5":"markdown","b74116e9":"markdown","b9cbff49":"markdown","2c136215":"markdown","abf76921":"markdown","59dfba80":"markdown","60475134":"markdown"},"source":{"095e97f8":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nimport random\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.utils import np_utils\n%matplotlib inline\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.regularizers import L1L2\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import EarlyStopping\nimport tensorflow as tf\nfrom keras.models import load_model\nfrom tqdm import tqdm","96c91fb1":"# defining the path and classes.\ndirectory = '..\/input\/train'\ntest_directory = '..\/input\/test\/'\nclasses = ['c0','c1','c2','c3','c4','c5','c6','c7','c8','c9']","1d7d9db0":"def generate_path(*args,**kwargs):\n    import os\n    path_conv = \"\"\n    for i in range(len(args)):\n        \n        if len(args) == 1 :\n            path_conv = os.path.join(path_conv,args[i])     \n            return path_conv\n        elif i == (len(args) - 1):\n            return path_conv\n        \n        path_conv = os.path.join(path_conv,args[i],args[i+1])","5591b68c":"# provide the path and number of images to be displayed.\n# function plots those images.\ndef display_images(path,no_of_images):\n    count = 1\n    for img in os.listdir(path):\n        img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n        plt.imshow(img_array, cmap='gray')\n        plt.show()\n        count += 1\n        if(no_of_images < count):\n            break","d9d3a2bd":"# defining a shape to be used for our models.\nimg_size1 = 240\nimg_size2 = 240","e726e686":"class train_and_test:\n    def __init__(self,*args,**kwargs):\n        self.train_and_test = args\n        \n    # creating a training dataset.\n    def create_training_data(self,path,classes,img_size1,img_size2):\n            training_data = []\n            for img in tqdm(os.listdir(path)):\n                img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n                new_img = cv2.resize(img_array,(img_size2,img_size1))\n                if classes == 'c0':\n                    training_data.append([new_img,0])\n                elif classes == 'c1' :               \n                    training_data.append([new_img,1])\n                elif classes == 'c2' :               \n                    training_data.append([new_img,1])\n                elif classes == 'c3' :               \n                    training_data.append([new_img,1])\n                elif classes == 'c4' :               \n                    training_data.append([new_img,1])\n                elif classes == 'c5' :               \n                    training_data.append([new_img,1])\n                elif classes == 'c6' :               \n                    training_data.append([new_img,1])\n                elif classes == 'c7' :               \n                    training_data.append([new_img,1])\n                elif classes == 'c8' :               \n                    training_data.append([new_img,1])\n                elif classes == 'c9' :               \n                    training_data.append([new_img,1])\n            return training_data\n        \n    # Creating a test dataset.    \n    def create_testing_data(self,path,img_size1,img_size2):\n        testing_data = []       \n        for img in tqdm(os.listdir(path)):\n            img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n            new_img = cv2.resize(img_array,(img_size2,img_size1))\n            testing_data.append([img,new_img])\n        return testing_data","627bc226":"# Initializing the train and test classes for training and validation.\n_train_ = train_and_test()\n_test_ = train_and_test()","aaadf99f":"training_data_c0 = _train_.create_training_data(generate_path(directory,classes[0]),classes[0],img_size1,img_size2)\ntraining_data_c1 = _train_.create_training_data(generate_path(directory,classes[1]),classes[1],img_size1,img_size2)\ntraining_data_c2 = _train_.create_training_data(generate_path(directory,classes[2]),classes[2],img_size1,img_size2)\ntraining_data_c3 = _train_.create_training_data(generate_path(directory,classes[3]),classes[3],img_size1,img_size2)\ntraining_data_c4 = _train_.create_training_data(generate_path(directory,classes[4]),classes[4],img_size1,img_size2)\ntraining_data_c5 = _train_.create_training_data(generate_path(directory,classes[5]),classes[5],img_size1,img_size2)\ntraining_data_c6 = _train_.create_training_data(generate_path(directory,classes[6]),classes[6],img_size1,img_size2)\ntraining_data_c7 = _train_.create_training_data(generate_path(directory,classes[7]),classes[7],img_size1,img_size2)\ntraining_data_c8 = _train_.create_training_data(generate_path(directory,classes[8]),classes[8],img_size1,img_size2)\ntraining_data_c9 = _train_.create_training_data(generate_path(directory,classes[9]),classes[9],img_size1,img_size2)","c07aa3ff":"test_data = _test_.create_testing_data(generate_path(test_directory),img_size1,img_size2)","8658bf54":"# create train and test data for our model.\nclass features_and_labels:\n    # get all the arguments dynmically.\n    def __init__(self,*args,**kwargs):\n        self.features_and_labels = args\n        \n    # generate your features and labels.\n    def generate_features_and_label(self,_class1_,_class2_):\n        x = []\n        y = []\n        \n        for features, label in tqdm(_class1_):\n            x.append(features)\n            y.append(label)\n            \n        for features, label in tqdm(_class2_):\n            x.append(features)\n            y.append(label)\n            \n        return x,y\n    \n    # generate np_arrays for test.\n    def generate_npArray(self,_class1_,_class2_,img_size2,img_size1) :\n        x,y = self.generate_features_and_label(_class1_,_class2_)\n        np_array = np.array(x).reshape(-1,img_size2*img_size1)\n        return np_array, y\n    \n    # train and split your data.\n    def train_and_split(self,features,labels,test_size,random_state,num_class):\n        x_train,x_test,y_train,y_test = train_test_split(features,labels,test_size=test_size,random_state=random_state)\n        Y_train = np_utils.to_categorical(y_train,num_classes=num_class)\n        Y_test = np_utils.to_categorical(y_test,num_classes=num_class)\n        \n        return x_train,x_test,Y_train,Y_test\n    ","0fe9e141":"feature_label = features_and_labels()\n\nnp_array_c0c1,y_c0c1 = feature_label.generate_npArray(training_data_c0,training_data_c1,img_size2,img_size1)\nx_train_c0c1,x_test_c0c1,y_train_c0c1,y_test_c0c1 = feature_label.train_and_split(np_array_c0c1,y_c0c1,0.3,100,2)","21ee7725":"# initializing the logistic regression classifier.\noutput_dim = nb_classes = 2\nbatch_size = 128 \nnb_epoch = 100\nmodel_c0c1 = Sequential() \nmodel_c0c1.add(BatchNormalization())\nmodel_c0c1.add(Dense(output_dim, input_dim=240*240, activation='softmax')) \nmodel_c0c1.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\ncallbacks = [EarlyStopping(monitor='val_acc',patience=5,mode='max')]\nhistory_c0c1 = model_c0c1.fit(x_train_c0c1, y_train_c0c1, batch_size=batch_size, epochs=nb_epoch,verbose=1, validation_data=(x_test_c0c1, y_test_c0c1),callbacks=callbacks) ","fb368156":"model_c0c1.save_weights('.\/driverdistraction_Safe_vs_texting_right_weights.h5', overwrite=True)\nmodel_c0c1.save('.\/driverdistraction_lr_Safe_vs_texting_right.h5')","544a0e69":"# Plot training & validation accuracy values\nplt.plot(history_c0c1.history['acc'])\nplt.plot(history_c0c1.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history_c0c1.history['loss'])\nplt.plot(history_c0c1.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","f9c213bd":"np_array_c0c2, y_c0c2 = feature_label.generate_npArray(training_data_c0,training_data_c2,img_size2,img_size1)\nx_train_c0c2,x_test_c0c2,y_train_c0c2,y_test_c0c2 = feature_label.train_and_split(np_array_c0c2,y_c0c2,0.3,100,2)","eb68b5dc":"# initializing the logistic regression classifier.\noutput_dim = nb_classes = 2\nbatch_size = 128 \nnb_epoch = 25\nmodel_c0c2 = Sequential() \nmodel_c0c2.add(BatchNormalization())\nmodel_c0c2.add(Dense(output_dim, input_dim=240*240, activation='softmax')) \nmodel_c0c2.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\ncallbacks = [EarlyStopping(monitor='val_acc',patience=5,mode='max')]\nhistory_c0c2 = model_c0c2.fit(x_train_c0c2, y_train_c0c2, batch_size=batch_size, epochs=nb_epoch,verbose=1, validation_data=(x_test_c0c2, y_test_c0c2),callbacks=callbacks) ","f97dcda1":"model_c0c2.save_weights('.\/driverdistraction_talking_on_the_phone_right_weights.h5', overwrite=True)\nmodel_c0c2.save('.\/driverdistraction_lr_talking_on_the_phone_right.h5')","66269b10":"# Plot training & validation accuracy values\nplt.plot(history_c0c2.history['acc'])\nplt.plot(history_c0c2.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history_c0c2.history['loss'])\nplt.plot(history_c0c2.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","56666f64":"np_array_c0c3, y_c0c3 = feature_label.generate_npArray(training_data_c0,training_data_c3,img_size2,img_size1)\nx_train_c0c3,x_test_c0c3,y_train_c0c3,y_test_c0c3 = feature_label.train_and_split(np_array_c0c3,y_c0c3,0.3,100,2)","19d054c0":"# initializing the logistic regression classifier.\noutput_dim = nb_classes = 2\nbatch_size = 128 \nnb_epoch = 25\nmodel_c0c3 = Sequential() \nmodel_c0c3.add(BatchNormalization())\nmodel_c0c3.add(Dense(output_dim, input_dim=240*240, activation='softmax')) \nmodel_c0c3.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\ncallbacks = [EarlyStopping(monitor='val_acc',patience=5,mode='max')]\nhistory_c0c3 = model_c0c3.fit(x_train_c0c3, y_train_c0c3, batch_size=batch_size, epochs=nb_epoch,verbose=1, validation_data=(x_test_c0c3, y_test_c0c3),callbacks=callbacks) ","d985e061":"model_c0c3.save_weights('.\/driverdistraction_Safe_texting_left_weights.h5', overwrite=True)\nmodel_c0c3.save('.\/driverdistraction_lr_Safe_texting_left.h5')","df72d9f5":"# Plot training & validation accuracy values\nplt.plot(history_c0c3.history['acc'])\nplt.plot(history_c0c3.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history_c0c3.history['loss'])\nplt.plot(history_c0c3.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","ae4effb3":"np_array_c0c4, y_c0c4 = feature_label.generate_npArray(training_data_c0,training_data_c4,img_size2,img_size1)\nx_train_c0c4,x_test_c0c4,y_train_c0c4,y_test_c0c4 = feature_label.train_and_split(np_array_c0c4,y_c0c4,0.3,100,2)","6a683c0b":"# initializing the logistic regression classifier.\noutput_dim = nb_classes = 2\nbatch_size = 128 \nnb_epoch = 25\nmodel_c0c4 = Sequential() \nmodel_c0c4.add(BatchNormalization())\nmodel_c0c4.add(Dense(output_dim, input_dim=240*240, activation='softmax')) \nmodel_c0c4.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\ncallbacks = [EarlyStopping(monitor='val_acc',patience=5,mode='max')]\nhistory_c0c4 = model_c0c4.fit(x_train_c0c4, y_train_c0c4, batch_size=batch_size, epochs=nb_epoch,verbose=1, validation_data=(x_test_c0c4, y_test_c0c4),callbacks=callbacks) ","1884e4a5":"model_c0c4.save_weights('.\/driverdistraction_talking_on_the_phone_left_weights.h5', overwrite=True)\nmodel_c0c4.save('.\/driverdistraction_lr_talking_on_the_phone_left.h5')","7d216234":"# Plot training & validation accuracy values\nplt.plot(history_c0c4.history['acc'])\nplt.plot(history_c0c4.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history_c0c4.history['loss'])\nplt.plot(history_c0c4.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","1aa592ee":"np_array_c0c5, y_c0c5 = feature_label.generate_npArray(training_data_c0,training_data_c5,img_size2,img_size1)\nx_train_c0c5,x_test_c0c5,y_train_c0c5,y_test_c0c5 = feature_label.train_and_split(np_array_c0c5,y_c0c5,0.3,100,2)","d985b8af":"# initializing the logistic regression classifier.\noutput_dim = nb_classes = 2\nbatch_size = 128 \nnb_epoch = 25\nmodel_c0c5 = Sequential() \nmodel_c0c5.add(BatchNormalization())\nmodel_c0c5.add(Dense(output_dim, input_dim=240*240, activation='softmax')) \nmodel_c0c5.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\ncallbacks = [EarlyStopping(monitor='val_acc',patience=5,mode='max')]\nhistory_c0c5 = model_c0c5.fit(x_train_c0c5, y_train_c0c5, batch_size=batch_size, epochs=nb_epoch,verbose=1, validation_data=(x_test_c0c5, y_test_c0c5),callbacks=callbacks) ","1a154e72":"model_c0c5.save_weights('.\/driverdistraction_operating_the_radio_weights.h5', overwrite=True)\nmodel_c0c5.save('.\/driverdistraction_lr_operating_the_radio.h5')","0282d542":"# Plot training & validation accuracy values\nplt.plot(history_c0c5.history['acc'])\nplt.plot(history_c0c5.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history_c0c5.history['loss'])\nplt.plot(history_c0c5.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","bef837de":"np_array_c0c6, y_c0c6 = feature_label.generate_npArray(training_data_c0,training_data_c6,img_size2,img_size1)\nx_train_c0c6,x_test_c0c6,y_train_c0c6,y_test_c0c6 = feature_label.train_and_split(np_array_c0c6,y_c0c6,0.3,100,2)","f3ffebf4":"# initializing the logistic regression classifier.\noutput_dim = nb_classes = 2\nbatch_size = 128 \nnb_epoch = 30\nmodel_c0c6 = Sequential() \nmodel_c0c6.add(BatchNormalization())\nmodel_c0c6.add(Dense(output_dim, input_dim=240*240, activation='softmax')) \nmodel_c0c6.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\ncallbacks = [EarlyStopping(monitor='val_acc',patience=5,mode='max')]\nhistory_c0c6 = model_c0c6.fit(x_train_c0c6, y_train_c0c6, batch_size=batch_size, epochs=nb_epoch,verbose=1, validation_data=(x_test_c0c6, y_test_c0c6),callbacks=callbacks) ","6d9497ce":"model_c0c6.save_weights('.\/driverdistraction_drinking_weights.h5', overwrite=True)\nmodel_c0c6.save('.\/driverdistraction_lr_drinking.h5')","991913d1":"# Plot training & validation accuracy values\nplt.plot(history_c0c6.history['acc'])\nplt.plot(history_c0c6.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history_c0c6.history['loss'])\nplt.plot(history_c0c6.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","216bc9e5":"np_array_c0c7, y_c0c7 = feature_label.generate_npArray(training_data_c0,training_data_c7,img_size2,img_size1)\nx_train_c0c7,x_test_c0c7,y_train_c0c7,y_test_c0c7 = feature_label.train_and_split(np_array_c0c7,y_c0c7,0.3,100,2)","a69feac0":"# initializing the logistic regression classifier.\noutput_dim = nb_classes = 2\nbatch_size = 128 \nnb_epoch = 30\nmodel_c0c7 = Sequential() \nmodel_c0c7.add(BatchNormalization())\nmodel_c0c7.add(Dense(output_dim, input_dim=240*240, activation='softmax')) \nmodel_c0c7.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\ncallbacks = [EarlyStopping(monitor='val_acc',patience=5,mode='max')]\nhistory_c0c7 = model_c0c7.fit(x_train_c0c7, y_train_c0c7, batch_size=batch_size, epochs=nb_epoch,verbose=1, validation_data=(x_test_c0c7, y_test_c0c7),callbacks=callbacks) ","32974a02":"model_c0c7.save_weights('.\/driverdistraction_reach_behind_weights.h5', overwrite=True)\nmodel_c0c7.save('.\/driverdistraction_lr_reach_behind.h5')","c755655b":"# Plot training & validation accuracy values\nplt.plot(history_c0c7.history['acc'])\nplt.plot(history_c0c7.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history_c0c7.history['loss'])\nplt.plot(history_c0c7.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","dfb167dc":"np_array_c0c8, y_c0c8 = feature_label.generate_npArray(training_data_c0,training_data_c8,img_size2,img_size1)\nx_train_c0c8,x_test_c0c8,y_train_c0c8,y_test_c0c8 = feature_label.train_and_split(np_array_c0c8,y_c0c8,0.3,100,2)","5a5192e2":"# initializing the logistic regression classifier.\noutput_dim = nb_classes = 2\nbatch_size = 128 \nnb_epoch = 30\nmodel_c0c8 = Sequential() \nmodel_c0c8.add(BatchNormalization())\nmodel_c0c8.add(Dense(output_dim, input_dim=240*240, activation='softmax')) \nmodel_c0c8.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\ncallbacks = [EarlyStopping(monitor='val_acc',patience=5,mode='max')]\nhistory_c0c8 = model_c0c8.fit(x_train_c0c8, y_train_c0c8, batch_size=batch_size, epochs=nb_epoch,verbose=1, validation_data=(x_test_c0c8, y_test_c0c8),callbacks=callbacks) ","85bf6d30":"model_c0c8.save_weights('.\/driverdistraction_hair_and_makeup_weights.h5', overwrite=True)\nmodel_c0c8.save('.\/driverdistraction_lr_hair_and_makeup.h5')","4cfff13f":"# Plot training & validation accuracy values\nplt.plot(history_c0c8.history['acc'])\nplt.plot(history_c0c8.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history_c0c8.history['loss'])\nplt.plot(history_c0c8.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","75c44e6e":"np_array_c0c9, y_c0c9 = feature_label.generate_npArray(training_data_c0,training_data_c9,img_size2,img_size1)\nx_train_c0c9,x_test_c0c9,y_train_c0c9,y_test_c0c9 = feature_label.train_and_split(np_array_c0c9,y_c0c9,0.3,100,2)","eff95758":"# initializing the logistic regression classifier.\noutput_dim = nb_classes = 2\nbatch_size = 128 \nnb_epoch = 25\nmodel_c0c9 = Sequential() \nmodel_c0c9.add(BatchNormalization())\nmodel_c0c9.add(Dense(output_dim, input_dim=240*240, activation='softmax')) \nmodel_c0c9.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\ncallbacks = [EarlyStopping(monitor='val_acc',patience=5,mode='max')]\nhistory_c0c9 = model_c0c9.fit(x_train_c0c9, y_train_c0c9, batch_size=batch_size, epochs=nb_epoch,verbose=1, validation_data=(x_test_c0c9, y_test_c0c9),callbacks=callbacks) ","764d6171":"model_c0c9.save_weights('.\/driverdistraction_talking_to_the_passenger_weights.h5', overwrite=True)\nmodel_c0c9.save('.\/driverdistraction_lr_talking_to_the_passenger.h5')","911121c8":"# Plot training & validation accuracy values\nplt.plot(history_c0c9.history['acc'])\nplt.plot(history_c0c9.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history_c0c9.history['loss'])\nplt.plot(history_c0c9.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","34fcf9f3":"from sklearn.metrics import r2_score\ntest_data_ = np.array(test_data[3000][1]).reshape(-1,img_size2*img_size1)\nnew_img = cv2.resize(test_data[3000][1],(img_size2,img_size1))\nplt.imshow(new_img,cmap='gray')\nplt.show()\npred = model_c0c3.predict(test_data_)\n#r2_score(y_test, pred)\nprint(np.argmax(pred))","50a162f8":"pred","cd7e9a22":"## Different Distraction type\n    c0: safe driving\n    c1: texting - right\n    c2: talking on the phone - right\n    c3: texting - left\n    c4: talking on the phone - left\n    c5: operating the radio\n    c6: drinking\n    c7: reaching behind\n    c8: hair and makeup\n    c9: talking to passenger\n","c2d8f697":"## Creating training data for Safe vs hair_and_makeup","78ecd129":"## Creating training data for Safe vs operating_the_radio","61834e89":"## Creating training data for Safe vs talking_to_the_passenger","181e74c5":"## Creating training data for Safe vs texting_left","b74116e9":"## Creating training data for Safe vs talking_on_the_phone_right","b9cbff49":"## Creating training data for Safe vs texting_right","2c136215":"## Creating training data for Safe vs drinking","abf76921":"## Creating training data for Safe vs reach_behind","59dfba80":"## Creating training data for Safe vs talking_on_the_phone_left","60475134":"    c0: safe driving\n    c1: texting - right\n    c2: talking on the phone - right\n    c3: texting - left\n    c4: talking on the phone - left\n    c5: operating the radio\n    c6: drinking\n    c7: reaching behind\n    c8: hair and makeup\n    c9: talking to passenger"}}