{"cell_type":{"021e294b":"code","9dcf597d":"code","add32cff":"code","56cef1dd":"code","253a6941":"code","3622058e":"code","3a3494ca":"code","33b19d9c":"code","2372b3b7":"code","ca273574":"code","ac7fc30c":"code","f1bce9f4":"code","ebb54688":"code","0f8a5564":"code","76461d4e":"markdown"},"source":{"021e294b":"!apt-get install -y python-opengl\n!apt-get install -y ffmpeg\n\nfrom IPython.display import clear_output\nclear_output()\n\nfrom xvfbwrapper import Xvfb\n\nvdisplay = Xvfb(width=1280, height=740)\nvdisplay.start()\nprint('Done')","9dcf597d":"import gym\nimport pylab\nimport time\nimport itertools\nimport numpy as np\nimport seaborn as sns\nfrom statistics import mean\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation, rc\nfrom IPython.display import Math, HTML\n\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential, load_model, Model\n\nfrom keras import backend as K\n\nimport tensorflow as tf\ntf.compat.v1.disable_eager_execution()","add32cff":"GAMMA = 0.99\nEPISODES = 1000\nLR_ACTOR = 0.001\nLR_CRITIC = 0.005\n\nEPOCHS = 4\nMINI_BATCH_SIZE = 16\nBATCH_SIZE = EPOCHS * MINI_BATCH_SIZE\n\nCLIPPING = 0.1\nENTROPY_BETA = 0.001\n\n# smoothing factor\nALPHA = 0.95\n\nenv = gym.make('CartPole-v1')\nstate_size = env.observation_space.shape[0]\naction_size = env.action_space.n","56cef1dd":"def create_animation(frames):\n    rc('animation', html='jshtml')\n    fig = plt.figure()\n    plt.axis(\"off\")\n    im = plt.imshow(frames[0], animated=True)\n\n    def updatefig(i):\n        im.set_array(frames[i])\n        return im,\n\n    ani = animation.FuncAnimation(fig, updatefig, frames=len(frames), interval=20, blit=True)\n    display(HTML(ani.to_html5_video()))    \n    plt.close()    \n    \n    return ani","253a6941":"def plot_reward_history(rewards, lr_actor, lr_critic, save_to_file):\n    plt.figure(0)\n    plt.cla()\n    ax = sns.lineplot(data=np.array(rewards))\n    plt.show()\n    \n    if save_to_file:\n        plt.figure(0).savefig(str(lr_actor) + '-' + str(lr_critic) + '.png')","3622058e":"def normalize(x):\n    x -= x.mean()\n    x \/= (x.std() + 1e-8)\n    return x","3a3494ca":"def ppo_loss_function(advantage, old_prediction):\n    def loss(y_true, y_pred):\n        # y_true one hot encoded actions\n        # pred is a softmax vector. \n        # prob is the probability of the taken aciton.\n        prob = y_true * y_pred\n        old_prob = y_true * old_prediction\n\n        # create the ratio based on log probability\n        ratio = K.exp(K.log(prob + 1e-10) - K.log(old_prob + 1e-10))\n\n        clip_ratio = K.clip(ratio, min_value=(1 - CLIPPING), max_value=(1 + CLIPPING))\n        surrogate1 = ratio * advantage\n        surrogate2 = clip_ratio * advantage\n\n        # add the entropy loss to avoid getting stuck on local minima\n        entropy_loss = (prob * K.log(prob + 1e-10))\n        ppo_loss = -K.mean(K.minimum(surrogate1,surrogate2) + ENTROPY_BETA * entropy_loss)\n        return ppo_loss\n\n    return loss","33b19d9c":"class PPOAgent:\n    def __init__(self, state_size, action_size, gamma=GAMMA, lr_actor=LR_ACTOR, lr_critic=LR_CRITIC, load_model_from_file=False):\n        self.load_model_from_file = load_model_from_file\n\n        self.gamma = gamma\n        self.lr_actor = lr_actor\n        self.lr_critic = lr_critic\n\n        self.state_size = state_size\n        self.action_size = action_size\n        self.value_size = 1\n        \n        self.training_loss = []\n        \n        self.actor = self.build_actor()\n        self.critic = self.build_critic()\n\n        if self.load_model_from_file:\n            self.actor.load_weights('cartpole_actor.h5')\n            self.critic.load_weights('cartpole_critic.h5')\n     \n    def build_actor(self):\n        advantage = Input(shape=(1,), name='advantage_input')\n        old_prediction = Input(shape=(self.action_size,),name='old_prediction_input')\n        loss = ppo_loss_function(advantage=advantage, old_prediction=old_prediction)\n        \n        state_input = Input(shape=self.state_size, name='state_input')\n        serial = state_input\n        serial = Dense(24, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform')(serial)\n        output = Dense(self.action_size, activation='softmax', kernel_initializer='he_uniform')(serial)\n        \n        actor = Model(inputs=[state_input, advantage, old_prediction], outputs=[output])\n        actor.compile(loss=[loss], optimizer=Adam(lr=self.lr_actor))\n        # actor.summary()\n        return actor\n\n    def build_critic(self):\n        critic = Sequential()\n        critic.add(Dense(24, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n        critic.add(Dense(self.value_size, activation='linear', kernel_initializer='he_uniform'))\n        critic.compile(loss=\"mse\", optimizer=Adam(lr=self.lr_critic))\n        # critic.summary()\n        return critic\n    \n    def get_value(self, state):\n        state_expanded = np.reshape(state, [1, self.state_size])\n        return self.critic.predict(state_expanded)[0][0]\n\n    def get_action(self, state):\n        state_expanded = np.reshape(state, [1, self.state_size])\n        probability = self.actor.predict([state_expanded, np.zeros((1, 1)), np.zeros((1, self.action_size))])[0]\n        action_idx = np.random.choice(self.action_size, 1, p=probability)[0]\n        return action_idx, probability\n\n    def one_hot_ecode_actions(self, actions):\n        length = len(actions)\n        result = np.zeros((length, self.action_size))\n        result[range(length), actions] = 1\n        return result\n    \n    def train_model(self, states, advantages, actions, probabilities, gaes):\n        one_hot_encoded_actions = self.one_hot_ecode_actions(actions)\n\n        actor_loss = self.actor.fit(\n            [states, advantages, probabilities],\n            [one_hot_encoded_actions],\n            verbose=False, shuffle=True, epochs=EPOCHS, batch_size=MINI_BATCH_SIZE, validation_split=0.2)\n        \n        critic_loss = self.critic.fit(\n            [states],\n            [gaes],\n            verbose=False, shuffle=True, epochs=EPOCHS, batch_size=MINI_BATCH_SIZE, validation_split=0.2)\n        self.training_loss = [mean(actor_loss.history['val_loss']), mean(critic_loss.history['val_loss'])]\n\n    def save_models(self):\n        self.actor.save(\"cartpole_actor.h5\")\n        self.critic.save(\"cartpole_critic.h5\")","2372b3b7":"def record_episode(agent):\n    frames = []\n    done = False\n    \n    state = env.reset()\n\n    while not done:\n        action, _ = agent.get_action(state)\n        next_state, _, done, _ = env.step(action)\n        state = next_state\n        frames.append(env.render(mode=\"rgb_array\"))\n\n    ani = create_animation(frames)","ca273574":"def get_generalized_advantage_estimations(reward_mem, value_mem, mask_mem, next_state_value):\n    gae = 0\n    return_mem = []\n    episode_length = len(reward_mem)\n   \n    for t in reversed(range(episode_length)):\n        value = value_mem[t]\n        value_prime = next_state_value if (t+1) >= episode_length else value_mem[t+1]\n        \n        delta = reward_mem[t] + GAMMA * value_prime * mask_mem[t] - value\n        gae = delta + GAMMA * ALPHA * mask_mem[t] * gae\n        \n        return_value = gae + value  \n        return_mem.insert(0, return_value)\n        \n    return np.array(return_mem)","ac7fc30c":"def train_agent(nr_of_episodes=EPISODES, lr_actor=LR_ACTOR, lr_critic=LR_CRITIC, render=False):\n    scores = []\n    agent = PPOAgent(state_size, action_size, lr_actor=lr_actor, lr_critic=lr_critic)\n\n    value_mem = []\n    state_mem = []\n    action_mem = []\n    reward_mem = []\n    mask_mem = []\n    probability_mem = []\n    \n    try:\n        for episode_idx in range(nr_of_episodes):\n            done = False\n            score = 0\n            state = env.reset()\n\n            while not done:\n                value = agent.get_value(state)\n                action, probability = agent.get_action(state)\n                next_state, reward, done, _ = env.step(action)\n\n                value_mem.append(value)\n                state_mem.append(state)\n                action_mem.append(action)\n                reward_mem.append(reward)\n                mask_mem.append(0 if done == True else 1)\n                probability_mem.append(probability)\n\n                score += reward\n                state = next_state\n                \n                if done:\n                    scores.append(score)\n                    \n                    if (episode_idx % 50) == 49:\n                        mean_score = mean(scores[-10:])\n                        print(episode_idx+1, 'Mean score:', mean_score, ', loss:', agent.training_loss)\n\n                \n                if len(state_mem) >= BATCH_SIZE:\n                    # the value of this state is not yet added to the value memory\n                    next_state_value = agent.get_value(state)\n\n                    state_mem = np.array(state_mem)\n                    value_mem = np.array(value_mem)\n                    action_mem = np.array(action_mem)\n                    probability_mem = np.array(probability_mem)\n                    \n                    gaes = get_generalized_advantage_estimations(reward_mem, value_mem, mask_mem, next_state_value)\n                    advantages = gaes - value_mem\n                    advantages = normalize(advantages)\n  \n                    agent.train_model(state_mem, advantages, action_mem, probability_mem, gaes)\n                    \n                    # reset buffers\n                    value_mem = []\n                    state_mem = []\n                    action_mem = []\n                    reward_mem = []\n                    mask_mem = []\n                    probability_mem = []\n                    \n\n            # stop the training and save the model when ever the last X runs are close to the optimum\n            if len(scores) > 5 and np.mean(scores[-5:]) > 490:\n                agent.save_models()\n                return\n    finally:\n        plot_reward_history(scores, lr_actor=lr_actor, lr_critic=lr_critic, save_to_file=False)","f1bce9f4":"agent = PPOAgent(state_size, action_size)\nrecord_episode(agent)","ebb54688":"lrs_actor = [0.001]\nlrs_critic = [0.005]\n\nfor r in itertools.product(lrs_actor, lrs_critic):\n    lr_actor = r[0]\n    lr_critic = r[1]\n    print(\"LR Actor:\", lr_actor, \", LR Critic:\", lr_critic)\n    train_agent(nr_of_episodes=1000, lr_actor=lr_actor, lr_critic=lr_critic)","0f8a5564":"agent = PPOAgent(state_size, action_size, load_model_from_file=True)\nrecord_episode(agent)","76461d4e":"# References:\n* https:\/\/www.youtube.com\/watch?v=WxQfQW48A4A\n* https:\/\/www.kaggle.com\/thimac\/ppo-lunar-lander-reinforcement-learning\n* https:\/\/github.com\/nric\/ProximalPolicyOptimizationKeras\n* https:\/\/github.com\/rlcode\/reinforcement-learning\/tree\/master\/2-cartpole\/4-actor-critic"}}