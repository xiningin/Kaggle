{"cell_type":{"2f927e27":"code","fbaee860":"code","40f01859":"code","2e32373e":"code","bee453a8":"code","0ce2404e":"code","f83fd646":"code","d8704868":"code","a1313486":"code","641b29f9":"code","ebb5f2cd":"code","2d86823a":"code","c6027f8c":"code","1961cd49":"code","1dd8fbd0":"code","161190ac":"code","6292302d":"code","fbb2d05d":"code","748a1883":"code","e8fb6248":"code","cfd6e5d9":"markdown","e06631da":"markdown","edc9e733":"markdown","3735a3f3":"markdown","75f6dee4":"markdown","22aaed94":"markdown","e14240a0":"markdown","93135829":"markdown","3081bd57":"markdown","d4f0a7a0":"markdown","f8553bf0":"markdown","a306353a":"markdown","352f369d":"markdown","517047bc":"markdown","c29f022f":"markdown","14c3a01d":"markdown","5df904f4":"markdown","fb4b4171":"markdown","d5a1fafd":"markdown","91a42c59":"markdown"},"source":{"2f927e27":"!ls\n!pwd\n!ls \/\n!ls \/kaggle\/\n!ls \/kaggle\/input\/\n!ls \/kaggle\/input\/polytech-ds-2019\/\n!ls \/kaggle\/input\/models\/","fbaee860":"!pip install timm","40f01859":"!pip install efficientnet_pytorch","2e32373e":"import torch\nimport torch.nn as nn\nfrom torchvision import transforms\nfrom PIL import Image\nimport glob\nfrom IPython.display import display\nfrom IPython.display import FileLink\nimport torch.utils.model_zoo as model_zoo\nimport torchvision.models as models\nfrom tqdm import tqdm_notebook\nimport random\nimport timm\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nimport pandas as pd\nimport os\nos.chdir(r'\/kaggle\/working')","bee453a8":"# diretory of data\nimg_dir = [r\"\/kaggle\/input\/polytech-ds-2019\/polytech-ds-2019\/training\/\", \n           r\"\/kaggle\/input\/polytech-ds-2019\/polytech-ds-2019\/validation\/\"]\n\nclass trainDataset(torch.utils.data.Dataset):\n    def __init__(self, root_dir, train_list):\n        super().__init__()\n        self.root_dir = root_dir\n        self.train_list = train_list\n        self.img_names = [self.root_dir + os.sep + item for item in self.train_list]\n        self.labels = [int(item.split(os.sep)[1].split('_')[0]) for item in self.train_list]\n\n        self.transform = transforms.Compose([transforms.RandomHorizontalFlip(p=0.5),\n                                             #transforms.RandomRotation(180),\n                                             transforms.ColorJitter(brightness=(0.3 if random.random()<0.5 else False),\n                                                                    contrast=(0.2 if random.random()<0.5 else False),\n                                                                    saturation=(0.2 if random.random()<0.5 else False),\n                                                                    hue=(0.1 if random.random()<0.5 else False)),\n                                             transforms.RandomAffine(10, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=(0.15, 0), resample=False),\n                                             transforms.Resize((256, 256)),\n                                             transforms.CenterCrop((224, 224)),\n                                             transforms.ToTensor(),\n                                             transforms.Normalize([0.485, 0.456, 0.406],\n                                                                   [0.229, 0.224, 0.225])\n                                            ])\n\n    def __len__(self):\n        return len(self.img_names)\n\n    def __getitem__(self, i):\n        img = Image.open(self.img_names[i]).convert('RGB')\n        return self.transform(img), self.labels[i]\n\n\n\nclass valDataset(torch.utils.data.Dataset):\n    def __init__(self, root_dir, val_list):\n        super().__init__()\n        self.root_dir = root_dir\n        self.val_list = val_list\n        self.img_names = [self.root_dir + os.sep + item for item in self.val_list]\n        self.labels = [int(item.split(os.sep)[1].split('_')[0]) for item in self.val_list]\n\n        # PyTorch transforms\n        self.transform = transforms.Compose([transforms.Resize((256, 256)),\n                                             transforms.CenterCrop((224, 224)),\n                                             transforms.ToTensor(),\n                                             transforms.Normalize([0.485, 0.456, 0.406],\n                                                                  [0.229, 0.224, 0.225])\n                                            ])\n\n    def __len__(self):\n        return len(self.img_names)\n\n    def __getitem__(self, i):\n        img = Image.open(self.img_names[i]).convert('RGB')\n        return self.transform(img), self.labels[i]\n\n\ndef random_split_train_val(train_dir, val_dir, split_ratio):\n    train_names = ['training' + os.sep + item for item in os.listdir(train_dir)]\n    val_names = ['validation' + os.sep + item for item in os.listdir(val_dir)]\n    all_names = train_names + val_names\n    nums = len(all_names)\n    ratio = split_ratio\n    random.shuffle(all_names)\n    new_val = all_names[:round(nums * ratio)]\n    new_train = all_names[round(nums * ratio):]\n    return new_train, new_val\n\n\ntrain_dir = r\"\/kaggle\/input\/polytech-ds-2019\/polytech-ds-2019\/training\/\"\nval_dir = r\"\/kaggle\/input\/polytech-ds-2019\/polytech-ds-2019\/validation\/\"\nnew_train, new_val = random_split_train_val(train_dir, val_dir, 0.1)\ntrain_set = trainDataset(r\"\/kaggle\/input\/polytech-ds-2019\/polytech-ds-2019\/\", new_train)\nprint(\"training dataset size\",len(train_set))\nval_set = valDataset(r\"\/kaggle\/input\/polytech-ds-2019\/polytech-ds-2019\/\", new_val)\nprint(\"validation dataset size\",len(val_set))\n\ndef display_tensor(t):\n    trans = transforms.ToPILImage()\n    display(trans(t))\n\n\nfor i in range(10):\n    img_train, label_train = train_set[i]\n    img_val, label_val = val_set[i]\n    display_tensor(img_train)\n    print(\"class : \", label_train)\n    display_tensor(img_val)\n    print(\"class : \", label_val)","0ce2404e":"batch_size = 25\ntest_split = 0.1\n\ntrain_dl = torch.utils.data.DataLoader(train_set, batch_size=batch_size,shuffle=True)\nval_dl = torch.utils.data.DataLoader(val_set, batch_size=batch_size)\n\n\n# unfreezing function for certain layers, only used on ResNext        \ndef unfreeze_last_layers(model, feature_extracting):\n    for param in model.parameters():\n        param.requires_grad = False\n    next(model.layer4[1].conv1.parameters()).requires_grad = True\n    next(model.layer4[1].conv2.parameters()).requires_grad = True\n    next(model.layer4[1].conv3.parameters()).requires_grad = True\n    next(model.layer4[2].conv1.parameters()).requires_grad = True\n    next(model.layer4[2].conv2.parameters()).requires_grad = True\n    next(model.layer4[2].conv3.parameters()).requires_grad = True\n    next(model.layer4[0].conv1.parameters()).requires_grad = True\n    next(model.layer4[0].conv2.parameters()).requires_grad = True\n    next(model.layer4[0].conv3.parameters()).requires_grad = True\n    next(model.layer3[22].conv1.parameters()).requires_grad = True\n    next(model.layer3[22].conv2.parameters()).requires_grad = True\n    next(model.layer3[22].conv3.parameters()).requires_grad = True\n\n# model1 mixnet_l\nmodel = timm.create_model('mixnet_l', pretrained=True)\nmodel.classifier = nn.Linear(1536, 11)\nprint(model)\n\n# # model2 resnext101\n# model = models.resnext101_32x8d(pretrained=True)\n# # unfreezing last layers in resnext in order to use the bottom features and reduce calculation (resnext101 model is quite big), \n# # which also yielded a better accuracy in our experiments\n# unfreeze_last_layers(model)\n# fc_features = model.fc.in_features\n# model.fc = nn.Linear(fc_features, 11)\n# print(model)\n\n# # model3 mixnet_xl\n# model = timm.create_model('mixnet_xl', pretrained=True)\n# model.classifier = nn.Linear(1536, 11)\n# print(model)\n\n# # model4 inceptionV3\n# model = torchvision.models.inception_v3(pretrained=True, aux_logits=False, transform_input=False)\n# model.fc = nn.Linear(2048, 11)\n# model.cuda()","f83fd646":"torch.cuda.empty_cache()\nLEARNING_RATE = 0.01\nmodel.cuda()\nN_EPOCHS = 1\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = LEARNING_RATE, momentum=0.9, weight_decay=1e-4) \nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 6)","d8704868":"epoch_val_loss = [] \nepoch_val_acc = []\nepoch_train_loss = []\nepoch_train_acc = []\nflag = 0\nbest_val_acc = 0\nfor e in range(N_EPOCHS):\n    print(\"EPOCH:\",e)\n    running_loss = 0\n    running_accuracy = 0\n    model.train()\n    for i, batch in enumerate(tqdm_notebook(train_dl)):\n#         # quick check for max batch size\n#         if i == 5:\n#             break\n        x = batch[0]\n        labels = batch[1]\n        x = x.cuda()\n        labels = labels.cuda()\n\n        y = model(x)\n        loss = criterion(y, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        running_accuracy += (y.max(1)[1] == labels).sum().item()\n    \n    print(\"Training accuracy: {:.2f}%\".format(100*running_accuracy\/float(len(train_set))),\n          \"Training loss:\", running_loss\/float(len(train_dl)), \"learning rate:\", scheduler.get_lr()[0])\n    \n    scheduler.step()\n    epoch_train_acc.append(running_accuracy\/float(len(train_set)))\n    epoch_train_loss.append(running_loss\/float(len(train_set)))\n    \n\n    model.eval()\n    running_val_loss = 0\n    running_val_accuracy = 0\n    \n    for i, batch in enumerate(val_dl):\n        with torch.no_grad():\n#             # quick check for max batch size\n#             if i == 5:\n#                 break\n            x = batch[0]\n            labels = batch[1]\n            x = x.cuda()\n            labels = labels.cuda()\n            y = model(x)\n            loss = criterion(y, labels)\n            running_val_loss += loss.item()\n            running_val_accuracy += (y.max(1)[1] == labels).sum().item()\n    acc = running_val_accuracy\/float(len(val_set))\n    if acc > best_val_acc:\n        best_val_acc = acc\n        torch.save(model.state_dict(), '\/kaggle\/working\/model_mixnet_phase1.pkl')\n        flag = 0\n    print(\"Validation accuracy:{:.2f}%\".format(100* acc),\n          \"Validation loss:\", running_val_loss\/float(len(val_set)))\n    epoch_val_loss.append(running_val_loss\/len(val_set))\n    epoch_val_acc.append(running_val_accuracy\/len(val_set))\n\n\n# model=mixnet_xl, scheduler=CosineAnnealing, lr=0.01\n# Training accuracy: 99.87% Training loss: 0.0056521773303466446 learning rate: 0.006545084971874732\n# Validation accuracy:96.71% Validation loss: 0.0057580280123060035","a1313486":"def plot_accuracy(train_acc, val_acc):\n    plt.title('FOOD11')\n    plt.plot(train_acc)\n    plt.plot(val_acc)\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train_accuracy', 'validation_accuracy'], loc='best')\n    plt.show()\n    \ndef plot_loss(train_loss, val_loss):\n    plt.title('FOOD11')\n    plt.plot(train_loss)\n    plt.plot(val_loss)\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train_loss', 'validation_loss'], loc='best')\n    plt.show()","641b29f9":"plot_accuracy(epoch_train_acc, epoch_val_acc) \nplot_loss(epoch_train_loss, epoch_val_loss)","ebb5f2cd":"# restart_training = timm.create_model('mixnet_xl', pretrained=True)\nrestart_training = timm.create_model('mixnet_l', pretrained=True)\nrestart_training.classifier = nn.Linear(1536, 11)\nrestart_training.load_state_dict(torch.load('\/kaggle\/working\/model_mixnet_phase1.pkl'))\n\ntorch.cuda.empty_cache()\nLEARNING_RATE = 0.001\nrestart_training.cuda()\n\nN_EPOCHS = 1\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(restart_training.parameters(), lr = LEARNING_RATE, momentum=0.9, weight_decay=1e-4) \nMILESTONE = [20, 30, 40]\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONE, gamma=0.1)","2d86823a":"epoch_val_loss = [] \nepoch_val_acc = []\nepoch_train_loss = []\nepoch_train_acc = []\nbest_val_acc = 0\nflag = 0\nfor e in range(N_EPOCHS):\n    print(\"EPOCH:\",e)\n    running_loss = 0\n    running_accuracy = 0\n    restart_training.train()\n    for i, batch in enumerate(tqdm_notebook(train_dl)):\n#         # quick check\n#         if i == 5:\n#             break\n        x = batch[0]\n        labels = batch[1]\n        x = x.cuda()\n        labels = labels.cuda()\n        y = restart_training(x)\n        loss = criterion(y, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        running_accuracy += (y.max(1)[1] == labels).sum().item()\n    \n    print(\"Training accuracy: {:.2f}%\".format(100*running_accuracy\/float(len(train_set))),\n          \"Training loss:\", running_loss\/float(len(train_dl)), \"learning rate:\", scheduler.get_lr()[0])\n    \n    scheduler.step()\n    epoch_train_acc.append(running_accuracy\/float(len(train_set)))\n    epoch_train_loss.append(running_loss\/float(len(train_set)))\n    \n    restart_training.eval()\n\n    running_val_loss = 0\n    running_val_accuracy = 0\n    \n    for i, batch in enumerate(val_dl):\n        with torch.no_grad():\n#             # quick check for max batch size\n#             if i == 5:\n#                 break\n            x = batch[0]\n            labels = batch[1]\n            x = x.cuda()\n            labels = labels.cuda()\n            y = restart_training(x)\n            loss = criterion(y, labels)\n            running_val_loss += loss.item()\n            running_val_accuracy += (y.max(1)[1] == labels).sum().item()\n    acc = running_val_accuracy\/float(len(val_set))\n    if acc > best_val_acc:\n        best_val_acc = acc\n        torch.save(restart_training.state_dict(), '\/kaggle\/working\/model_mixnet_phase2.pkl')\n        flag = 0\n    print(\"Validation accuracy:{:.2f}%\".format(100* acc),\n          \"Validation loss:\", running_val_loss\/float(len(val_set)))\n    epoch_val_loss.append(running_val_loss\/len(val_set))\n    epoch_val_acc.append(running_val_accuracy\/len(val_set))\n","c6027f8c":"restart_training.eval()\ny_true = []\ny_pred = []\nfor i, batch in enumerate(val_dl):\n    with torch.no_grad():\n        x = batch[0]\n        labels = batch[1]\n        x = x.cuda()\n        labels = labels.cuda()\n        y = restart_training(x)        \n        y_true.extend(y.max(1)[1].tolist())\n        y_pred.extend(labels.tolist())\nprint(\"confusion matrix\")\nconfusion_matrix(y_true, y_pred)","1961cd49":"plot_accuracy(epoch_train_acc, epoch_val_acc) \nplot_loss(epoch_train_loss, epoch_val_loss)","1dd8fbd0":"!pip install timm","161190ac":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nimport torchvision\nimport os\nfrom PIL import Image\n# from efficientnet_pytorch import EfficientNet\nimport timm\nimport pandas as pd\nimport time\n\nclass food11_dataset_test(Dataset):\n\tdef __init__(self, root_dir, inp_list, img_transform=None):\n\t\tself.imgs = inp_list\n\t\tself.transform = img_transform\n\t\tself.img_dirs = [os.path.join(root_dir, img_dir) for img_dir in self.imgs]\n\n\tdef __len__(self):\n\t\treturn len(self.img_dirs)\n\n\tdef __getitem__(self, index):\n\t\timg = Image.open(self.img_dirs[index]).convert('RGB')\n\t\tif self.transform is not None:\n\t\t\timg = self.transform(img)\n\t\tname = self.imgs[index].split('.')[0]\n\t\treturn [img, name]\n    \ndef test_ensemble(test_dir, csv_dir):\n    transform_test = transforms.Compose([\n\t\ttransforms.Resize((RESIZE_SIZE, RESIZE_SIZE)),\n\t\ttransforms.CenterCrop((CROP_SIZE, CROP_SIZE)),\n\t\ttransforms.ToTensor(),\n\t\ttransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    test_list = os.listdir(test_dir)\n    test_dataset = food11_dataset_test(test_dir, test_list, transform_test)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE_VAL, shuffle=False, num_workers=4)\n    # print(test_dataset[0])\n    N_batch_test = len(test_loader)\n\n    ckpt_1 = r'\/kaggle\/input\/models\/mixnet_l2_best.pkl'\n    ckpt_2 = r'\/kaggle\/input\/models\/resnext101_best.pkl'\n    ckpt_3 = r'\/kaggle\/input\/models\/model_mixnet_best.pkl'\n    ckpt_4 = r'\/kaggle\/input\/models\/inceptionV3_epoch_30_model.pth'\n\n    \n    # mixnet_l\n    model_1 = timm.create_model('mixnet_l', pretrained=False)\n    model_1.classifier = nn.Linear(1536, 11)\n    model_1.load_state_dict(torch.load(ckpt_1))\n    model_1.cuda()\n    model_1.eval()\n    \n    # resnext_100\n    model_2 = torchvision.models.resnext101_32x8d(pretrained=False)\n    fc_features = model_2.fc.in_features\n    model_2.fc = nn.Linear(fc_features, 11)\n    model_2.load_state_dict(torch.load(ckpt_2))\n    model_2.cuda()\n    model_2.eval()\n\n    # # mixnet_xl\n    model_3 = timm.create_model('mixnet_xl', pretrained=False)\n    model_3.classifier = nn.Linear(1536, 11)\n    model_3.load_state_dict(torch.load(ckpt_3))\n    model_3.cuda()\n    model_3.eval()\n\n    # inception_v3\n    # model_4 = torchvision.models.inception_v3(pretrained=False, aux_logits=False, transform_input=False)\n    # model_4.fc = nn.Linear(2048, 11)\n    # model_4.load_state_dict(torch.load(ckpt_4))\n    # model_4.cuda()\n    # model_4.eval()\n\n    criterion = nn.CrossEntropyLoss()\n     \n    running_loss = 0\n    running_acc = 0\n    imgs = []\n    cate = []\n    start = time.time()\n    for i, batch in enumerate(test_loader):\n        x = batch[0].cuda()\n        y1 = model_1(x)\n        y2 = model_2(x)\n        y3 = model_3(x)\n        # y4 = model_4(x)\n        y_fuse = y1 + y2 + y3\n\n    # for i, batch in enumerate(test_loader):\n    #     x = batch[0].cuda()\n    #     y2 = model_2(x)\n\n        names = batch[1]\n        for name in names:\n            imgs.append(name)\n        for pred in y_fuse.max(1)[1]:\n            cate.append(' '+str(pred.item()))\n        print(\"Processing batch %d\/%d\" % (i, len(test_loader)))\n\n    end = time.time()\n    info = \"Test finished, elapsed_time=%.3f.\" % (end - start)\n    print(info)\n    df = pd.DataFrame()\n    df['Id'] = pd.Series(imgs)\n    df['Category'] = pd.Series(cate)\n    df.to_csv(csv_dir, index=False)\n    print(\"csv file wrote successfully.\")\n    \nif __name__ == \"__main__\":\n    RESIZE_SIZE = 342\n    CROP_SIZE = 299\n    BATCH_SIZE_VAL = 4\n\n    root_dir = \"\/kaggle\/input\/polytech-ds-2019\/polytech-ds-2019\/\"\n    test_dir = root_dir + \"kaggle_evaluation\/\"\n    csv_dir = \"result.csv\"\n    test_ensemble(test_dir, csv_dir)","6292302d":"# !rm \/kaggle\/working\/result.csv\n# !ls \/kaggle\/working\/","fbb2d05d":"!pip install timm","748a1883":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nimport torchvision\nimport os\nfrom PIL import Image\nimport time\n# from efficientnet_pytorch import EfficientNet\nimport timm\nimport pandas as pd","e8fb6248":"class food11_dataset_centercrop(torch.utils.data.Dataset):\n    def __init__(self, root_dir, val_list, img_transform=None):\n        super().__init__()\n        self.root_dir = root_dir\n        self.val_list = val_list\n        self.img_names = [self.root_dir + os.sep + item for item in self.val_list]\n        self.labels = [int(item.split(os.sep)[1].split('_')[0]) for item in self.val_list]\n        self.transform = img_transform\n\n    def __len__(self):\n        return len(self.img_names)\n\n    def __getitem__(self, i):\n        img = Image.open(self.img_names[i]).convert('RGB')\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, self.labels[i]\n    \n\nclass food11_dataset_fivecrop(Dataset):\n\tdef __init__(self, root_dir, inp_list, img_transform=None):\n\t\tself.imgs = inp_list\n\t\tself.transform = img_transform\n\t\tself.img_dirs = [os.path.join(root_dir, img_dir) for img_dir in self.imgs]\n\t\tself.labels = [int(img_name.split('\/')[1].split('_')[0]) for img_name in self.imgs]\n\n\tdef __len__(self):\n\t\treturn len(self.labels)\n\n\tdef __getitem__(self, index):\n\t\timg = Image.open(self.img_dirs[index]).convert('RGB')\n\t\tif self.transform is not None:\n\t\t\timg = self.transform(img)\n\t\tlabel = self.labels[index]\n\t\treturn img, label\n\n    \n    \ndef test_tencrop(test_dir, val_list, ckpt_dir):\n\ttransform_test = transforms.Compose([\n\t\ttransforms.Resize((RESIZE_SIZE, RESIZE_SIZE)),\n\t\ttransforms.TenCrop((CROP_SIZE, CROP_SIZE)),\n        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n\t\ttransforms.Lambda(lambda crops: torch.stack([transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(transforms.ToTensor()(crop)) for crop in crops]))\n        # transforms.ToTensor(),\n\t])\n\ttest_dataset = food11_dataset_fivecrop(test_dir, val_list, transform_test)\n\ttest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE_VAL, shuffle=False, num_workers=4)\n    # print(test_dataset[0])\n\tN_batch_test = len(test_loader)\n\n    # model = torchvision.models.inception_v3(pretrained=False, aux_logits=False, transform_input=False)\n    # model.fc = nn.Linear(2048, 11)\n    # state_dict = torch.load(ckpt_dir, map_location='cpu')\n    # model.load_state_dict(state_dict)\n\n\tmodel = timm.create_model('mixnet_xl', pretrained=False)\n\tmodel.classifier = nn.Linear(1536, 11)\n\tmodel.load_state_dict(torch.load(ckpt_dir))\n\tmodel.cuda()\n\tcriterion = nn.CrossEntropyLoss()\n\n\tmodel.eval()\n\trunning_loss = 0\n\trunning_acc = 0\n\tstart = time.time()\n\tfor i, batch in enumerate(test_loader):\n\t\tx = batch[0].cuda()\n\t\tlabels = batch[1].cuda()\n# \t\tprint(labels.size())\n        # print(x.size())\n\t\tbs, ncrops, c, h, w = x.size()\n\t\tresult = model(x.view(-1, c, h, w))\n\t\tresult_avg = result.view(bs, ncrops, -1).mean(1)\n\t\tloss = criterion(result_avg, labels)\n\t\trunning_loss += loss.item()\n\t\trunning_acc += (result_avg.max(1)[1] == labels).sum().item()\n\t\tprint(\"testing batch %d\/%d\" % (i, len(test_loader)))\n\n\tend = time.time()\n\ttop1_acc = running_acc \/ len(test_dataset)\n\tinfo = \"test result: val_loss=%.3f, top1_acc=%.3f, elapsed_time=%.3f\" % (\n\t\trunning_loss \/ len(test_loader), top1_acc, end-start)\n\tprint(info)\n\n\ndef test_fivecrop(test_dir, val_list, ckpt_dir):\n\ttransform_test = transforms.Compose([\n\t\ttransforms.Resize((RESIZE_SIZE, RESIZE_SIZE)),\n\t\ttransforms.FiveCrop((CROP_SIZE, CROP_SIZE)),\n        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n\t\ttransforms.Lambda(lambda crops: torch.stack([transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(transforms.ToTensor()(crop)) for crop in crops]))\n        # transforms.ToTensor(),\n\t])\n\n\ttest_list = os.listdir(test_dir)\n\ttest_dataset = food11_dataset_fivecrop(test_dir, val_list, transform_test)\n\ttest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE_VAL, shuffle=False, num_workers=4)\n    # print(test_dataset[0])\n\tN_batch_test = len(test_loader)\n\n    # model = torchvision.models.inception_v3(pretrained=False, aux_logits=False, transform_input=False)\n    # model.fc = nn.Linear(2048, 11)\n    # state_dict = torch.load(ckpt_dir, map_location='cpu')\n    # model.load_state_dict(state_dict)\n\n\tmodel = timm.create_model('mixnet_xl', pretrained=False)\n\tmodel.classifier = nn.Linear(1536, 11)\n\tmodel.load_state_dict(torch.load(ckpt_dir))\n\tmodel.cuda()\n\tcriterion = nn.CrossEntropyLoss()\n    \n\tmodel.eval()\n\trunning_loss = 0\n\trunning_acc = 0\n\tstart = time.time()\n\tfor i, batch in enumerate(test_loader):\n\t\tx = batch[0].cuda()\n\t\tlabels = batch[1].cuda()\n# \t\tprint(labels.size())\n        # print(x.size())\n\t\tbs, ncrops, c, h, w = x.size()\n\t\tresult = model(x.view(-1, c, h, w))\n\t\tresult_avg = result.view(bs, ncrops, -1).mean(1)\n\t\tloss = criterion(result_avg, labels)\n\t\trunning_loss += loss.item()\n\t\trunning_acc += (result_avg.max(1)[1] == labels).sum().item()\n\t\tprint(\"testing batch %d\/%d\" % (i, len(test_loader)))\n\n\tend = time.time()\n\ttop1_acc = running_acc \/ len(test_dataset)\n\tinfo = \"test result: val_loss=%.3f, top1_acc=%.3f, elapsed_time=%.3f\" % (\n\t\trunning_loss \/ len(test_loader), top1_acc, end-start)\n\tprint(info)\n\n    \ndef test_centercrop(test_dir, val_list, ckpt_dir):\n\ttransform_test = transforms.Compose([\n\t\ttransforms.Resize((RESIZE_SIZE, RESIZE_SIZE)),\n\t\ttransforms.CenterCrop((CROP_SIZE, CROP_SIZE)),\n\t\ttransforms.ToTensor(),\n\t\ttransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\t])\n\n\ttest_dataset = food11_dataset_centercrop(root_dir, val_list, transform_test)\n\ttest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE_VAL, shuffle=False, num_workers=4)\n    # print(test_dataset[0])\n\tN_batch_test = len(test_loader)\n\n    # model = torchvision.models.inception_v3(pretrained=False, aux_logits=False, transform_input=False)\n    # model.fc = nn.Linear(2048, 11)\n    # state_dict = torch.load(ckpt_dir, map_location='cpu')\n    # model.load_state_dict(state_dict)\n\n\tmodel = timm.create_model('mixnet_xl', pretrained=False)\n\tmodel.classifier = nn.Linear(1536, 11)\n\tmodel.load_state_dict(torch.load(ckpt_dir))\n\tmodel.cuda()\n\tcriterion = nn.CrossEntropyLoss()\n\n\n\tmodel.eval()\n\trunning_loss = 0\n\trunning_acc = 0\n\tstart = time.time()\n\tfor i, batch in enumerate(test_loader):\n\t\tx = batch[0].cuda()\n\t\tlabels = batch[1].cuda()\n# \t\tprint(labels.size())\n\t\ty = model(x)\n\t\tloss = criterion(y, labels)\n\t\trunning_loss += loss.item()\n\t\trunning_acc += (y.max(1)[1] == labels).sum().item()\n\t\tprint(\"testing batch %d\/%d\" % (i, len(test_loader)))\n\n\tend = time.time()\n\ttop1_acc = running_acc \/ len(test_dataset)\n\tinfo = \"test result: val_loss=%.3f, top1_acc=%.3f, elapsed_time=%.3f\" % (\n\t\trunning_loss \/ len(test_loader), top1_acc, end-start)\n\tprint(info)\n\n\n\n\nRESIZE_SIZE = 256\nCROP_SIZE = 224\nRAND_PROB = 0.5\nBATCH_SIZE_VAL = 1\n\nroot_dir = r\"\/kaggle\/input\/polytech-ds-2019\/polytech-ds-2019\/\"\nckpt_dir = r\"\/kaggle\/input\/models\/model_mixnet_best.pkl\"\n# \ttest_centercrop(root_dir, new_val, ckpt_dir)\n# \ttest_fivecrop(root_dir, new_val, ckpt_dir)\ntest_tencrop(root_dir, new_val, ckpt_dir)\n\n\n# cropsize=224\n# center crop test result: val_loss=0.132, top1_acc=0.970, elapsed_time=45.791\n# ten crop test result: val_loss=0.122, top1_acc=0.967, elapsed_time=380.180\n\n# cropsize=299\n# center no crop test result: val_loss=0.142, top1_acc=0.962, elapsed_time=33.212\n# center crop test result: val_loss=0.133, top1_acc=0.966, elapsed_time=35.501\n# five crop test result: val_loss=0.126, top1_acc=0.965, elapsed_time=449.507\n# ten crop test result: val_loss=0.123, top1_acc=0.966, elapsed_time=115.089","cfd6e5d9":"### Visualization of Loss and Accuracy","e06631da":"### Visualization of Loss and Accuracy","edc9e733":"# 3  Test-time Augmentation","3735a3f3":"### 1.4.2  Phase 2 Training","75f6dee4":"### 1.4.1  Phase 1 Training","22aaed94":"The original training and validation datasets contains 9866 and 3430 images respectively. The number of validation images are rather large, so we merged them together and re-split randomly. We used a split ratio of 10%.","e14240a0":"# 1  Model","93135829":"We restarted training with a different scheduler and smaller learning rate, with the best model obatined by CosineAnnealing (phase1), did fine-grained learning rate control using multistep scheduler and obtained the final best model (phase2).","3081bd57":"## 1.2  Datasets","d4f0a7a0":"Our training process contains two phases: \n\n1. Using the CosineAnnealing scheduler. As the learning rates for each epoch are calculated automatically and sometimes the minimizing is interupted by the increasing learning rate in the ascending period of the cosine function, we added the second phase of training, tuned the learning rate in a more controlable way. \n\n2. Using the MultiStep scheduler. From the optimal model obtained in phase 1, we restart the training with a smaller learning rate and obtained our final optimal model.","f8553bf0":"## contents\n**0   Preparation**\n\n**1   Model**\n\n**1.1   Packages**\n\n**1.2   Datasets**\n\n**1.3   Create Models**\n\n**1.4   Training and Validation **(Visulization and Confusion Matrix included)\n\n**2   Model Ensemble**\n\n**3   Test-time Augmentation**","a306353a":"### Confusion Matrix","352f369d":"## 1.4  Training and Validation","517047bc":"We did experiments on test-time augmentation, using five-crop and ten-crop transformation, but found no improvements in accuracy. So in our final version we keep the centercrop testing transformation.","c29f022f":"## 1.1  Packages","14c3a01d":"We trained different models including **InceptionV3**, **ResNext50**, **ResNext101**, **Mixnet_l**, **Mixnet_xl** and **EfficientNet_B5**. We used the pretrained models on ImageNet and finetuned them on the food11 dataset. \n\nThe first three models are loaded from Pytorch model zoo, among them Resnext101 has the best accuracy with the last 6 layers unfrozen (we didn't do elaborate experiments of this and this is the best we got). \n\nFor Mixnet (https:\/\/arxiv.org\/abs\/1907.09595) we unfroze all the layers. We thank to rwightman's work of his collection of the pytorch network implementation and the pre-trained models: https:\/\/github.com\/rwightman\/pytorch-image-models\n\nFor EfficientNet (https:\/\/arxiv.org\/abs\/1905.11946) we unfroze all the layers. Thanks to lukemelas of the pytorch implementation and pre-trained models of EfficientNet: https:\/\/github.com\/lukemelas\/EfficientNet-PyTorch","5df904f4":"# 0  Preparation","fb4b4171":"## 1.3 Create Models","d5a1fafd":"With the multiple trained models at hand, we did a simple ensembling of those models to see if there will be improvement.\n\nOur method of ensemble is averaging the predictions (output probabilities) of different models and obtain the final output prediction (averaging is simplified as calculating the sum of predictions in our code).\n\nThe pre-trained models are uploaded as \"datasets\" in kaggle (which we have set them to be public) and added into this notebook. They should be seen under \/kaggle\/input\/models\/. EfficientNet doesn't work well both individually and ensemble, so we didn't include nor upload the model.\n\nAmong all the ensemble combinations (tried by enumeration), the best accuracy is obtained by the ensemble of **(resnext101, mixnet_l, mixnet_xl)**. \n\nPlus, we occasionally observed an improvement by zooming in the input image to a larger size when ensemble testing (training: 224, testing: 299), so we kept this method in the final submission. However for individual models this doesn't hold all the time.\n\nThe final csv file is generated using our ensemble method.","91a42c59":"# 2  Model Ensemble"}}