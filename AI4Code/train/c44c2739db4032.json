{"cell_type":{"62a98ec5":"code","4d0777dc":"code","c7f9a02f":"code","761f5991":"code","c80789e5":"code","98a82bd1":"code","6df0743b":"code","28d6be00":"code","b16d0ace":"code","312bc43f":"code","6571caf6":"code","cadcdc3b":"code","4fc2fcff":"code","5638f10c":"code","97b3a30f":"code","af3a42aa":"code","b3b33a43":"code","d486a11f":"code","83906b84":"code","e61e1c12":"code","95ec8652":"code","d5de19c6":"markdown","a8ecbc06":"markdown","33b43db2":"markdown","13056a98":"markdown","f94d757f":"markdown","2bf62c7a":"markdown","532544ce":"markdown","d8806c98":"markdown","ebf30001":"markdown","99634a2e":"markdown","46aca328":"markdown","8977bfcc":"markdown","cb64b3c0":"markdown","a81dfa05":"markdown","60cee21c":"markdown","bf7cc544":"markdown","28a9d2b4":"markdown","ddc8f9cb":"markdown","73696b90":"markdown","42a087c0":"markdown","7867c207":"markdown","b33c9639":"markdown","a79b4318":"markdown","38a051e8":"markdown","d2bd902f":"markdown","1ff75311":"markdown","5d4b1e32":"markdown","cafe6f70":"markdown"},"source":{"62a98ec5":"import os\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport pydicom as dicom\nimport cv2\nimport ast\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","4d0777dc":"path = '\/kaggle\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/'\nos.listdir(path)","c7f9a02f":"train_data = pd.read_csv(path+'train_labels.csv')\nsamp_subm = pd.read_csv(path+'sample_submission.csv')","761f5991":"print('Samples train:', len(train_data))\nprint('Samples test:', len(samp_subm))","c80789e5":"train_data.head()","98a82bd1":"train_data[\"MGMT_value\"].value_counts().head(2).plot(kind = 'pie', autopct='%1.1f%%', figsize=(8, 8)).legend()","6df0743b":"train_data[\"MGMT_value\"].value_counts()","28d6be00":"samp_subm.head()","b16d0ace":"folder = str(train_data.loc[0, 'BraTS21ID']).zfill(5)\nfolder","312bc43f":"os.listdir(path+'train\/'+folder)","6571caf6":"print('Number of FLAIR images:', len(os.listdir(path+'train\/'+folder+'\/'+'FLAIR')))\nprint('Number of T1w images:', len(os.listdir(path+'train\/'+folder+'\/'+'T1w')))\nprint('Number of T1wCE images:', len(os.listdir(path+'train\/'+folder+'\/'+'T1wCE')))\nprint('Number of T2w images:', len(os.listdir(path+'train\/'+folder+'\/'+'T2w')))","cadcdc3b":"path_file = ''.join([path, 'train\/', folder, '\/', 'FLAIR\/'])\nimage = os.listdir(path_file)[0]\ndata_file = dicom.dcmread(path_file+image)\nimg = data_file.pixel_array","4fc2fcff":"print('Image shape:', img.shape)","5638f10c":"def plot_examples(row = 0, cat = 'FLAIR'): \n    folder = str(train_data.loc[row, 'BraTS21ID']).zfill(5)\n    path_file = ''.join([path, 'train\/', folder, '\/', cat, '\/'])\n    images = os.listdir(path_file)\n    \n    fig, axs = plt.subplots(1, 5, figsize=(30, 30))\n    fig.subplots_adjust(hspace = .2, wspace=.2)\n    axs = axs.ravel()\n    \n    for num in range(5):\n        data_file = dicom.dcmread(path_file+images[num])\n        img = data_file.pixel_array\n        axs[num].imshow(img, cmap='gray')\n        axs[num].set_title(cat+' '+images[num])\n        axs[num].set_xticklabels([])\n        axs[num].set_yticklabels([])\n        \nrow = 0\nplot_examples(row = row, cat = 'FLAIR')","97b3a30f":"plot_examples(row = row, cat = 'T1w')","af3a42aa":"plot_examples(row = row, cat = 'T1wCE')","b3b33a43":"plot_examples(row = row, cat = 'T2w')","d486a11f":"VGG_types = {\n    'VGG11' : [64, 'M', 128, 'M', 256, 256, 'M', 512,512, 'M',512,512,'M'],\n    'VGG13' : [64,64, 'M', 128, 128, 'M', 256, 256, 'M', 512,512, 'M', 512,512,'M'],\n    'VGG16' : [64,64, 'M', 128, 128, 'M', 256, 256,256, 'M', 512,512,512, 'M',512,512,512,'M'],\n    'VGG19' : [64,64, 'M', 128, 128, 'M', 256, 256,256,256, 'M', 512,512,512,512, 'M',512,512,512,512,'M']\n}\nclass VGGnet(nn.Module):\n    def __init__(self, model, in_channels=3, num_classes=10, init_weights=True):\n        super(VGGnet,self).__init__()\n        self.in_channels = in_channels\n\n        # create conv_layers corresponding to VGG type\n        self.conv_layers = self.create_conv_laters(VGG_types[model])\n\n        self.fcs = nn.Sequential(\n            nn.Linear(1536, 1536\/\/2),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.Linear(1536\/\/2, 1536\/\/2),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.Linear(1536\/\/2, num_classes),\n        )\n\n        # weight initialization\n        if init_weights:\n            self._initialize_weights()\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = x.view(x.size(0), -1)\n        x = self.fcs(x)\n        return x\n\n    # defint weight initialization function\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm3d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n    \n    # define a function to create conv layer taken the key of VGG_type dict \n    def create_conv_laters(self, architecture):\n        layers = []\n        in_channels = self.in_channels # 3\n\n        for x in architecture:\n            if type(x) == int: # int means conv layer\n                out_channels = x\n\n                layers += [nn.Conv3d(in_channels=in_channels, out_channels=out_channels,\n                                     kernel_size=(3,2,3), stride=(1,1,1), padding=(1,1,1)),\n                           nn.BatchNorm3d(x),\n                           nn.ReLU()]\n                in_channels = x\n            elif x == 'M':\n                layers += [nn.MaxPool3d(kernel_size=(2,2,2), stride=(2,2,2))]\n        \n        return nn.Sequential(*layers)","83906b84":"# define device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# creat VGGnet object\n# Choose between 'VGG11', 'VGG13', 'VGG16', 'VGG19'\nmodel = VGGnet('VGG16', in_channels=1, num_classes=1, init_weights=True).to(device)\nprint(model)","e61e1c12":"samp_subm.head()","95ec8652":"samp_subm.to_csv('submission.csv', index=False)","d5de19c6":"#### How to submit results ","a8ecbc06":"> # T1wCE Images ","33b43db2":"### Take overview of the Competition here \n\nhttps:\/\/www.kaggle.com\/c\/rsna-miccai-brain-tumor-radiogenomic-classification\/overview","13056a98":"# Read Dicom Files\nWe consider the first train sample.\n```\nTraining\/Validation\/Testing\n\u2502\n\u2514\u2500\u2500\u2500 00000\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500 FLAIR\n\u2502   \u2502   \u2502 Image-1.dcm\n\u2502   \u2502   \u2502 Image-2.dcm\n\u2502   \u2502   \u2502 ...\n\u2502   \u2502   \n\u2502   \u2514\u2500\u2500\u2500 T1w\n\u2502   \u2502   \u2502 Image-1.dcm\n\u2502   \u2502   \u2502 Image-2.dcm\n\u2502   \u2502   \u2502 ...\n\u2502   \u2502   \n\u2502   \u2514\u2500\u2500\u2500 T1wCE\n\u2502   \u2502   \u2502 Image-1.dcm\n\u2502   \u2502   \u2502 Image-2.dcm\n\u2502   \u2502   \u2502 ...\n\u2502   \u2502   \n\u2502   \u2514\u2500\u2500\u2500 T2w\n\u2502   \u2502   \u2502 Image-1.dcm\n\u2502   \u2502   \u2502 Image-2.dcm\n\u2502   \u2502   \u2502 .....\n\u2502   \n\u2514\u2500\u2500\u2500 00001\n\u2502   \u2502 ...\n\u2502   \n\u2502 ...   \n\u2502   \n\u2514\u2500\u2500\u2500 00002\n\u2502   \u2502 ...\n```","f94d757f":"### Read image","2bf62c7a":"### Files\ntrain\/ - folder containing the training files, with each top-level folder representing a subject\ntrain_labels.csv - file containing the target MGMT_value for each subject in the training data (e.g. the presence of MGMT promoter methylation)\ntest\/ - the test files, which use the same structure as train\/; your task is to predict the MGMT_value for each subject in the test data. NOTE: the total size of the rerun test set (Public and Private) is ~5x the size of the Public test set\nsample_submission.csv - a sample submission file in the correct format","532544ce":"### Image shape","d8806c98":"<div class=\"alert alert-block alert-success\">  \n    <center><h2><strong>\ud83d\udc68\u200d\ud83d\udcbb Getting Started with RSNA-MICCAI Brain Tumor Radiogenomic Classification<\/strong><\/h2><\/center>\n    <i><\/i>\n<\/div>","ebf30001":"<div class=\"alert alert-block alert-info\">  \n    <h3><strong>Guideline to submit the results<\/strong><\/h3>\n    <i><\/i>\n<\/div>","99634a2e":"> # Flair Images ","46aca328":"![](https:\/\/www.researchgate.net\/profile\/Max_Ferguson\/publication\/322512435\/figure\/fig3\/AS:697390994567179@1543282378794\/Fig-A1-The-standard-VGG-16-network-architecture-as-proposed-in-32-Note-that-only.png)","8977bfcc":"### Overview of dataset","cb64b3c0":"<div class=\"alert alert-block alert-danger\">  \n<h1>If you like my work, please upvote ^ \ud83d\udc4d my code so that i will be motivated to share more valuable and helpful work on this competition \ud83d\ude0d<\/h1>\n        <\/p>\n<\/div>","a81dfa05":"> # T1w Images ","60cee21c":"> # T2w Images ","bf7cc544":"### Path of dataset","28a9d2b4":"### Extract folder id of the first train sample","ddc8f9cb":"> **We submitted the predictions but we will come back soon with model training and a lot more.**","73696b90":"### The exact mpMRI scans included are:\n\n* Fluid Attenuated Inversion Recovery\n* T1-weighted pre-contrast (T1w)\n* T1-weighted post-contrast (T1Gd)\n* T2-weighted (T2)\n","42a087c0":"> **Check the submission.csv file so we will know the format that how can submit the predictions. Before we do so, it's worth reminding ourselves that this is a code-only competition, meaning that your submission file has to be generated in a script\/notebook. The submission.csv file demonstrated what kind of file needs to be produced:**","7867c207":"### Importing Libraries","b33c9639":"![](https:\/\/www.mdpi.com\/jcm\/jcm-10-01411\/article_deploy\/html\/images\/jcm-10-01411-g001.png)","a79b4318":"# Transfer Learning for Brain Tumor Radiogenomic Classification\u00b6","38a051e8":"### MGMT_value counts ","d2bd902f":"## More code is coming Soon. Please upvote if you like the work and you will get notifications with additions. \n\n<center><img src=\"https:\/\/thumbs.gfycat.com\/AshamedWeightyDachshund-max-1mb.gif\"><\/center>","1ff75311":"### Folders content","5d4b1e32":"#### Loading dataset","cafe6f70":"### Read the refrence paper for more ideas \n\nU.Baid, et al., \u201cThe RSNA-ASNR-MICCAI BraTS 2021 Benchmark on Brain Tumor Segmentation and Radiogenomic Classification\u201d, arXiv:2107.02314, 2021.\nhttps:\/\/arxiv.org\/abs\/2107.02314"}}