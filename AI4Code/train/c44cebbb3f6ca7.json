{"cell_type":{"2e2a0791":"code","48b3b8b4":"code","0e4bc38f":"code","14b09f49":"code","c4464ed4":"code","7d6ce81e":"code","022c4651":"code","7898cbba":"code","cd67a628":"code","cd00b02e":"code","8c9ad7d0":"markdown","2d4372d7":"markdown"},"source":{"2e2a0791":"import pandas as pd\ndata = pd.read_csv('..\/input\/named-entity-recognition-ner\/ner_dataset.csv', encoding= 'unicode_escape')\ndata.head()","48b3b8b4":"from itertools import chain\ndef get_dict_map(data, token_or_tag):\n    tok2idx = {}\n    idx2tok = {}\n    \n    if token_or_tag == 'token':\n        vocab = list(set(data['Word'].to_list()))\n    else:\n        vocab = list(set(data['Tag'].to_list()))\n    \n    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\n    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}\n    return tok2idx, idx2tok\ntoken2idx, idx2token = get_dict_map(data, 'token')\ntag2idx, idx2tag = get_dict_map(data, 'tag')","0e4bc38f":"data['Word_idx'] = data['Word'].map(token2idx)\ndata['Tag_idx'] = data['Tag'].map(tag2idx)\ndata_fillna = data.fillna(method='ffill', axis=0)\n# Groupby and collect columns\ndata_group = data_fillna.groupby(\n['Sentence #'],as_index=False\n)['Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))","14b09f49":"from sklearn.model_selection import train_test_split\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\ndef get_pad_train_test_val(data_group, data):\n\n    #get max token and tag length\n    n_token = len(list(set(data['Word'].to_list())))\n    n_tag = len(list(set(data['Tag'].to_list())))\n\n    #Pad tokens (X var)    \n    tokens = data_group['Word_idx'].tolist()\n    maxlen = max([len(s) for s in tokens])\n    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)\n\n    #Pad Tags (y var) and convert it into one hot encoding\n    tags = data_group['Tag_idx'].tolist()\n    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value= tag2idx[\"O\"])\n    n_tags = len(tag2idx)\n    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n    \n    #Split train, test and validation set\n    tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, train_size=0.8, random_state=42)\n    train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_,tags_,train_size =0.8, random_state=42)\n\n    print(\n        'train_tokens length:'  , len(train_tokens),\n        '\\ntrain_tags:         ', len(train_tags),\n        '\\ntest_tokens length: ', len(test_tokens),\n        '\\ntest_tags:          ', len(test_tags),\n        '\\nval_tokens:         ', len(val_tokens),\n        '\\nval_tags:           ', len(val_tags),\n    )\n    \n    return train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags\n\ntrain_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = get_pad_train_test_val(data_group, data)\n","c4464ed4":"import numpy as np\nimport tensorflow\nfrom tensorflow.keras import Sequential, Model, Input\nfrom tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\nfrom tensorflow.keras.utils import plot_model\nfrom numpy.random import seed\nseed(1)\ntensorflow.random.set_seed(13)","7d6ce81e":"input_dim = len(list(set(data['Word'].to_list())))+1\noutput_dim = 64\ninput_length = max([len(s) for s in data_group['Word_idx'].tolist()])\nn_tags = len(tag2idx)","022c4651":"def get_bilstm_lstm_model():\n    model = Sequential()\n\n    # Add Embedding layer\n    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n\n    # Add bidirectional LSTM\n    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n\n    # Add LSTM\n    model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n\n    # Add timeDistributed Layer\n    model.add(TimeDistributed(Dense(n_tags, activation=\"relu\")))\n\n    #Optimiser \n    # adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n\n    # Compile model\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model.summary()\n    \n    return model","7898cbba":"def train_model(X, y, model):\n    loss = list()\n    for i in range(50):\n        # fit model for one epoch on this sequence\n        hist = model.fit(X, y, batch_size=1024, verbose=1, epochs=1, validation_split=0.2)\n        loss.append(hist.history['loss'][0])\n    return loss","cd67a628":"results = pd.DataFrame()\nmodel_bilstm_lstm = get_bilstm_lstm_model()\nplot_model(model_bilstm_lstm)\nresults['with_add_lstm'] = train_model(train_tokens, np.array(train_tags), model_bilstm_lstm)","cd00b02e":"import spacy\nfrom spacy import displacy\nnlp = spacy.load('en_core_web_sm')\ntext = nlp(\"\"\"AI-based machine learning techniques are going beyond the cloud-based data center, as processing of vital IoT sensor data moves much closer to where the data first resides.\n\nThe move will be enabled by new artificial intelligence (AI)-equipped chips. These include embedded microcontrollers with narrower memory and power consumption requirements than GPUs (graphical processing units), FPGAs (field-programmable gate arrays) and other specialized IC types first used to answer data scientists\u2019 questions in the cloud data centers of Amazon Web Services, Microsoft and Google.\n\nIt was in these clouds that machine learning and related neural network use exploded. But the rise of IoT created a data onslaught that required edge-based machine learning as well.\n\nNow, cloud providers, Internet of Things (IoT) platform makers, and others see benefit in processing data at the edge before turning it over to the cloud for analytics.\n\nMaking AI decisions at the edge reduces latency and makes real-time response to sensor data more practical and possible. Still, what people call \u201cedge AI\u201d takes many forms. And how to power it with next-gen IoT presents challenges in terms of presenting good-quality actionable data.\n\nEdge Computing Workloads Grow\n\nEdge-based machine learning could drive significant growth of AI in the IoT market, which Mordor Intelligence estimates will grow at a 27.3% CAGR through to 2026.\n\nThat is buttressed by Eclipse Foundation IoT Group research in 2020, which pegged AI at 30% as the most commonly cited edge computing workload among IoT developers.\n\nFor many applications, replicating the endless racks of servers that enabled parallel machine learning on the cloud is not an option. IoT edge cases that benefit from local processing are many, and highlighted by varied cases of operations monitoring. The processors, for example, watch events triggered by pressure gauge changes on an oil rig, detection of an anomaly on a distant power line, or captured video surveillance of an issue at a factory.\n\nThe last case is one of those most widely pursued. Application of AI that parses image data at the edge has proved a fertile area. But there are many complex processing needs for event processing using IoT device-gathered data.\n\nThe Value of Edge Compute\n\nStill, cloud-based IoT analytics will endure, said Steve Conway, senior adviser, Hyperion Research. But the distance data must travel brings processing latency. Moving data to and from a cloud naturally creates lag; the round trip takes time.\n\n\u201cThere is something called the speed of light,\u201d Conway quips. \u201cAnd you cannot exceed it.\u201d As result, a hierarchy of processing is developing on the edge.\n\nOther than devices and board-level implementations, this hierarchy includes IoT gateways and data centers in manufacturing that expand architectural options available for next-generation IoT system development.\n\nIn the long view, edge AI architecture is yet another generational shift in data processing  focus  \u2013 but a key one, according to Saurabh Mishra, senior manager for product marketing at SAS\u2019s IoT and Edge division.\n\n\u201cThere is a progression here,\u201d he said. \u201cAt one time, the idea was centralizing your data. You can do that for certain industries and certain use cases \u2013 ones where data was already created in a context, such as in a data center,\u201d he said.\n\nIt\u2019s not really possible to efficiently \u2013 and economically \u2013 move that to the cloud for analysis,\u201d Mishra said, who noted that SAS has created validated edge IoT reference architectures on top of which customers can build AI and analytical applications. Striking a balance between cloud and edge AI will be a fundamental requirement, he said.\n\nFinding balance begins with consideration of the amount of data needed to run machine learning models, according to Fr\u00e9d\u00e9ric Desbiens, program manager, IoT and Edge Computing at the Eclipse Foundation. That is where the new intelligent processors come in.\n\n\u201cAI accelerators at the edge can do local processing before sending the data somewhere else. But, this requires you to think about the functional requirements, including the software stack and storage needed,\u201d Desbiens said.\n\nAI Edge Chip Abundance\n\nThe rise of cloud-based machine learning was influenced by the rise of the high-memory bandwidth GPU, often in the form of a NVIDIA semiconductor. That success drew the attention of other chip makers.\n\nIn-house AI-specific processors followed from hyperscale cloud-players Google, AWS and Microsoft.\n\nThat AI chip battle has been joined by leading lights such as AMD, Intel, Qualcomm, and ARM Technology (which, for its part, last year was acquired by NVIDIA).\n\nIn turn, embedded microprocessor and systems-on-a-chip mainstays like Maxim Integrated, NXP Semiconductors, Silicon Labs, STM Microelectronics and others began to focus on adding AI abilities to the edge.\n\nToday,  IoT and edge processing needs have attracted AI chip start-ups that include EdgeQ,  Graphcore, Hailo, Mythic and others. Processing on the edge is constrained. Barriers include memory available, energy consumed and cost, emphasizes Hyperion\u2019s Steve Conway.\n\n\u201cThe embedded processors are very important, as energy use is very important,\u201d Conway said. \u201cThe GPUs and CPUs are not tiny dies, and GPUs, particularly, use a ton of energy,\u201d he said, referring to the relatively large silicon form factors GPUs and CPUs can take on.\n\nMaking Neurals Fit the Part\n\nData movement is a factor in energy consumption on the edge, advises Kris Ardis, executive director of Maxim Integrated\u2019s microcontroller and software algorithm businesses. Recently, the company released the MAX78000, which pairs a low-power controller with a neural net processor that can run on battery-powered IoT devices.\n\n\u201cIf you can do a computation at the very edge, you save bandwidth, and communications power. The challenge is taking the neural net and making it fit in the part,\u201d Ardis said.\n\nIndividual IoT devices based on the chip can feed IoT gateways, which also have a useful part to play, combining rollups of data from devices, and further filtering data that may go to the cloud in order to analyze overall operations, he indicated.\n\nOther semiconductor device makers also are adjusting to a trend that sees compute moving nearer to where data is. They are part of the effort to broaden the capabilities of developers, even as their hardware choices grow.\n\nBill Pearson, vice president of Intel\u2019s IoT group admits there was a time when \u201cthe CPU was the answer to all problems.\u201d Trends like edge AI belie that now.\n\nHe uses the term \u201cXPU\u201d to represent a variety of chip types that support different uses. But, he adds, the variety should be supported by a single software application programming interface (API).\n\nTo aid software developers, Intel recently released Version 2021.2 of the OpenVINO toolkit for inference on edge systems. It provides a, common environment for development among Intel components including CPUs, GPUs, and Movidius Visual Processing Units. As well, Intel offers DevCloud for the Edge software to forecast performance of neural network inference on different Intel hardware, according to Pearson.\n\nThe drive to simplify is marked at GPU powerhouse NVIDIA too.\n\n\u201cThe industry has to make it easier for people that aren\u2019t AI specialists,\u201d said Justin Boitano, vice president and general manager for Enterprise and Edge Computing, NVIDIA.\n\nThat may take the form of NVIDIA Jetson, which includes a low-power ARM processor. Named with a nod to the \u201860s science-fiction cartoon series, Jetson is intended to provide GPU-accelerated parallel processing in mobile embedded systems.\n\nRecently, to ease vision system development, NVIDIA rolled out Jetson JetPack 4.5, which includes the first production version of its Vision Programming Interface (VPI).\n\nWith time, edge AI development chores will be handled more by IT shops, and less by AI researchers with deep knowledge of machine learning, Boitano said.\n\nThe Tiny ML That Roared\n\nThe skills needed to migrate machine learning methods from the vast cloud to the constrained edge device are not easily gained. But new software techniques are being applied to enable compact edge AI, while easing the task of the developer.\n\nIn fact, industry has experienced the rise of \u201cTiny ML\u201d approaches. These make do with less power and use limited memory, while achieving capable inference-operations-per-second ratings.\n\nVarious machine learning tooling to reduce edge processing requirements have emerged, including Apache MXNet,  Edge Impulse\u2019s EON, Facebook\u2019s Glow, Foghorn Lightning Edge ML, Google TensorFlow Lite, Microsoft ELL, OctoML\u2019s Octomizer and others.\n\nDown-sizing neural net processing is a main target here, and the techniques are several. Among these are quantization, binarization and pruning, according to Sastry Malladi, who is CTO at Foghorn, a maker of a software platform that supports a variety of edge and on-premises implementations.\n\nQuantization of neural net processing focuses on use of low bit-width math. Binarization, in turn, is used to reduce the complexity of computations. And, pruning is used to reduce the number of neural nodes that must be processed.\n\nMalladi admits that is a daunting gamut for most developers to traverse \u2013 especially across a range of hardware. The efforts behind Foghorn\u2019s Lightning platform, he said, are intended to abstract the complexity in machine learning on the edge.\n\nThe goal is to allow line operators and reliability engineers, for example, to work with drag-and-drop interfaces, rather than application programming interfaces and software development kits, which are less intuitive and require more coding knowledge.\n\nSoftware that simplifies development and runs across multiple types of edge AI hardware is also a focus for Edge Impulse, makers of a development platform for embedded machine learning.\n\nUltimately, machine learning maturation means some model miniaturization, according to Zach Shelby, CEO, Edge Impulse.\n\n\u201cOnce, the direction of the research was toward bigger and bigger models of more and more complexity,\u201d Shelby said. \u201cBut, as machine learning hit prime time, people started to care about efficiency again.\u201d That led to Tiny ML.\n\nSoftware that can work on existing IoT infrastructure is necessary, while supporting a path to new varieties of hardware, he said. Edge Impulse tools allow cloud-based modeling of algorithms and events on available hardware, Shelby continued, so that users can try different options before they make selections.\n\nKeep Your Eyes on Vision\n\nOn the edge, computer vision has become a prominent use case for AI, especially in the form of deep learning, which employs multiple layers of neural networks and unsupervised techniques to achieve results in image pattern recognition.\n\nVision system architecture is undergoing shifts today, as cameras on the very edge add processing capabilities via embedded hardware for deep learning, according to Forrester Research\u2019s Kjell Carlsson, principal analyst. But finding the best application targets can be a challenge.\n\n\u201cThe issue with AI on the edge is that you more frequently end up looking at use cases that are \u2018net new,\u2019\u201d he said.\n\nDeveloping these greenfield solutions has inherent risk, Carlsson said, so a helpful tactic is to focus on use cases that offer a high benefit to cost ratio, even if the pattern recognition accuracy might trail that of full-fledged existing systems.\n\nOverall, Carlsson said edge AI could help fulfill IoT\u2019s original promise, which has lagged at times as implementers sorted through myriad potential use cases.\n\n\u201cIoT on its own had some limitations. Now, with AI, machine learning and deep learning that makes IoT more applicable \u2013 as well as valuable,\u201d he said.\"\"\")\ndisplacy.render(text, style = 'ent', jupyter=True)","8c9ad7d0":"![](https:\/\/miro.medium.com\/max\/2972\/1*7DkqpU3E-E9yknyw9c7vCQ.png)","2d4372d7":"Named Entity means anything that is a real-world object such as a person, a place, any organisation, any product which has a name. \n\nIn Machine Learning Named Entity Recognition (NER) is a task of Natural Language Processing to identify the named entities in a certain piece of text.\n\nGrammarly identifies all the incorrect spellings and punctuations in the text and corrects it. But it does not do anything with the named entities, as it is also using the same technique."}}