{"cell_type":{"8364c016":"code","79e7ec94":"code","351423aa":"code","e6f56eed":"code","1ea7155f":"code","f68d0245":"code","c75f43d8":"code","059a6f5f":"code","5cd0ba4f":"code","7231f3ed":"code","a836d4d7":"code","75e24f1b":"code","8c3e2eae":"code","f2e8469c":"code","d52a977d":"code","1539ca81":"code","bbc03005":"code","0a87561b":"code","41b821b6":"code","b140a0b3":"code","779b4881":"code","424feffa":"code","acc66021":"code","7d3b1e91":"code","0e326a58":"code","72d4644d":"code","91f56173":"code","3fa4d961":"code","c8d26b90":"code","32dc8566":"code","78451dd5":"code","2b27d860":"code","319a0921":"code","c897b77e":"code","eba264e7":"code","a695374b":"code","6e6bea87":"code","f6280164":"code","45411ac5":"code","aeb9c044":"code","e7b5f00d":"code","6a8438f5":"code","7201fc14":"code","be60fef7":"code","0b89d60f":"code","756547da":"code","fc7370be":"code","7779db68":"code","44a6798a":"code","3f36dd21":"code","6504dadb":"code","eba2a632":"code","bbd533eb":"code","0bda8e01":"code","8ac9036a":"code","93dbe3d2":"code","7cb6b35e":"code","83193fc2":"code","acbd0719":"code","8cfb1edb":"code","03a1e2c2":"code","a21ffb3a":"code","d29692a2":"code","602451d2":"code","c4a915eb":"code","2a54ee55":"code","690a3178":"code","113646f5":"code","ac215329":"code","b8adce72":"code","b45cb250":"code","37d60948":"code","682ec2fd":"code","63595920":"code","d2fc18fc":"code","bc20854e":"code","f37f615b":"markdown","17d03d2f":"markdown","552e01af":"markdown","b2f16b75":"markdown","6719d2c3":"markdown","8d7c733e":"markdown","92a83297":"markdown","90cb2b78":"markdown","b13b5dfc":"markdown","d1028001":"markdown","0e33b721":"markdown","53fea23f":"markdown","1dbb876b":"markdown","adfc7c97":"markdown","92d4bfc7":"markdown","2a3be84a":"markdown","fab79540":"markdown","c3312564":"markdown","7cd36d33":"markdown","78bea4a9":"markdown","959dd5eb":"markdown","7af860ef":"markdown","3dfac8e1":"markdown","bc1b3da4":"markdown","35a50e0b":"markdown","f824b887":"markdown","42851333":"markdown","c0211e1b":"markdown","6ed39336":"markdown","5e68cc2f":"markdown","ef26fc3b":"markdown","75eb2c22":"markdown","ebf8d971":"markdown","4ce6628f":"markdown","dcf2ce68":"markdown"},"source":{"8364c016":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","79e7ec94":"train = pd.read_csv(\"..\/input\/youtube\/train_data.csv\")\ntrain_true = pd.read_csv(\"..\/input\/youtube\/train_data.csv\")\ntest_data = pd.read_csv(\"..\/input\/youtubetest\/test_data.csv\")\n\n#\u6700\u5f8c\u306e\u30c7\u30fc\u30bf\u7d50\u5408\u306e\u70baid\u306f\u5225\u3067\u4fdd\u5b58\u3057\u3066\u304a\u304f\niddf = test_data[[\"id\"]]","351423aa":"#\u76ee\u7684\u5909\u6570\u3092\u53ef\u8996\u5316\u3059\u308b\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.hist(train['y'], bins=20, log=True)\n#\u5916\u308c\u5024\u304c\u5b58\u5728\u3059\u308b","e6f56eed":"\n  #\u5916\u308c\u5024\u9664\u5916\n  #\u4e0a\u4f4d1%\u3060\u3051\u3092\u53d6\u308a\u9664\u304f\n#train = train[train[\"y\"] < np.percentile(train['y'], 99)]\n#train.reset_index(drop=True)\n#plt.hist(train['y'], bins=20, log=True)\n#train.shape\n  #\u5916\u308c\u5024\u304c\u306a\u304f\u306a\u3063\u305f\u4e8b\u304c\u5206\u304b\u308b","1ea7155f":"train.isnull().sum()\n","f68d0245":"train.dtypes","c75f43d8":"train.head()","059a6f5f":"#\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u5225\u3005\u3067\u51e6\u7406\u3059\u308b\u306e\u306f\u9762\u5012\u306a\u306e\u3067\u3001\u7d50\u5408\u3057\u3066\u524d\u51e6\u7406\u3057\u3066\u3044\u304f\ndata = pd.concat([train, test_data])\n\n#\u3042\u3068\u3067\u30c6\u30b9\u30c8\u3068\u8a13\u7df4\u3092\u5206\u5225\u3067\u304d\u308b\u69d8\u306b\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u9577\u3055\u3092\u4fdd\u5b58\ntrain_len = len(train[\"y\"])\nprint(train.shape, test_data.shape, data.shape)","5cd0ba4f":"#\u76f8\u95a2\u4fc2\u6570\u3092\u898b\u3066\u3001\u95a2\u4fc2\u306e\u3042\u308a\u305d\u3046\u306a\u8aac\u660e\u5909\u6570\u3092\u898b\u308b\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ncorrmat = data.corr()\nf, ax = plt.subplots(figsize =(10, 10))\nsns.heatmap(corrmat, vmax = .8, square = True)\n# \u4e0b\u306e\u76f8\u95a2\u4fc2\u6570\u306e\u56f3\u304b\u3089y\u3068\u306flikes,dislikes,comment_count\u304c\u304b\u308d\u3046\u3058\u3066\u9ad8\u3044\u3068\u5206\u304b\u308b","7231f3ed":"  #\u95a2\u4fc2\u306e\u3042\u308a\u305d\u3046\u306a\u8aac\u660e\u5909\u6570\u3092\u8db3\u3057\u5408\u308f\u305b\u3066\u3001\u5206\u5e03\u3092\u898b\u308b\n#train[\"sum_values\"] = train[\"dislikes\"] + train[\"likes\"] + train[\"comment_count\"]\n#plt.scatter(x = np.log1p(train[\"sum_values\"]), y = train[\"y\"])\n#plt.axvline(x=6.5, ymin=0, ymax=1,c = \"r\")\n#plt.axvline(x=10, ymin=0, ymax=1,c = \"r\")\n#plt.show()\n\n  #\u8996\u8074\u7387\u3092\u300c\u505c\u6ede\u671f\u300d\u3001\u300c\u4e0a\u6607\u671f\u300d\u3001\u300c\u4eba\u6c17\u6642\u671f\u300d\u306e3\u500b\u306b\u5206\u3051\u308c\u305d\u3046\n  #\u4ee5\u4e0b\u306e\u8d64\u7dda\u306e\u30dd\u30a4\u30f3\u30c8\u3067\u5206\u3051\u308b","a836d4d7":"#plt.scatter(x = np.log1p(train[\"comment_count\"]), y = train[\"y\"])\n#plt.axvline(x=4, ymin=0, ymax=1,c = \"r\")\n#plt.axvline(x=7, ymin=0, ymax=1,c = \"r\")\n#plt.show()\n  #\u30b3\u30e1\u30f3\u30c8\u3060\u3051\u3001\u76f8\u95a2\u4fc2\u6570\u304c\u5c11\u3057\u4f4e\u304b\u3063\u305f\u306e\u3067\u3001\u4e0d\u5b89\u306b\u306a\u3063\u3066\u53ef\u8996\u5316\u3057\u305f\u3060\u3051\u7b11","75e24f1b":"  #3\u500b\u3092\u8db3\u3057\u5408\u308f\u305b\u305f\u5909\u6570\u3092\u4f5c\u308b\n#import collections\n#data[\"sum_values\"] = data[\"dislikes\"] + data[\"likes\"] + data[\"comment_count\"]\n","8c3e2eae":"\n  #(-inf <6.5, 6.5 < 10, 10 < inf)\u306e3\u30b0\u30eb\u30fc\u30d7\u3067\u5206\u3051\u308b\n#bin_edges = [-float('inf'), 6.5, 10, float('inf')]\n\n\n#binned = pd.cut(data[\"sum_values\"], bin_edges, labels = False)\n  #\u3057\u3063\u304b\u308a\u5206\u3051\u3089\u308c\u3066\u3044\u308b\u4e8b\u304c\u5206\u304b\u308b\n#print(binned)\n\n\n  #\u305d\u308c\u305e\u308c\u306e\u30b0\u30eb\u30fc\u30d7\u306e\u500b\u6570\u3092\u898b\u308b\n  #part1\n#c = collections.Counter(binned)\n#print(c)\n\n  #part2\n#c = binned.value_counts().to_dict()\n#print(c)\n  #\u3069\u3063\u3061\u3092\u4f7f\u3063\u3066\u3082\u4e00\u7dd2\u7b11\n  #2\u306b\u5927\u5206\u504f\u3063\u3066\u3044\u308b\u304c\u4eca\u306f\u6c17\u306b\u3057\u306a\u3044\n\n#data[\"values_group\"] = binned\n#plt.hist(data[\"values_group\"])","f2e8469c":"data.head()","d52a977d":"#\u5404ID\u306b\u6240\u5c5e\u3057\u3066\u3044\u308by\u306e\u300c\u5e73\u5747\u5024\u3001\u6700\u5927\u5024\u3001\u6700\u5c0f\u5024\u3001\u6a19\u6e96\u504f\u5dee\u3001\u500b\u6570\u300d\nmean_ = train[[\"categoryId\", \"y\"]].groupby(\"categoryId\").mean().reset_index().rename({\"y\":\"mean\"}, axis=1)\nmax_ = train[[\"categoryId\", \"y\"]].groupby(\"categoryId\").max().reset_index().rename({\"y\":\"max\"}, axis=1)\nmin_ = train[[\"categoryId\", \"y\"]].groupby(\"categoryId\").min().reset_index().rename({\"y\":\"min\"}, axis=1)\nstd_ = train[[\"categoryId\", \"y\"]].groupby(\"categoryId\").std().reset_index().rename({\"y\":\"std\"}, axis=1)\ncount_ = train[[\"categoryId\", \"y\"]].groupby(\"categoryId\").count().reset_index().rename({\"y\":\"count\"}, axis=1)\n\n#\u5404ID\u306e\u90e8\u4f4d\u70b9\nq1_ = train[[\"categoryId\", \"y\"]].groupby(\"categoryId\").quantile(0.1).reset_index().rename({\"y\":\"q1\"}, axis=1)\nq25_ = train[[\"categoryId\", \"y\"]].groupby(\"categoryId\").quantile(0.25).reset_index().rename({\"y\":\"q25\"}, axis=1)\nq5_ = train[[\"categoryId\", \"y\"]].groupby(\"categoryId\").quantile(0.5).reset_index().rename({\"y\":\"q5\"}, axis=1)\nq75_ = train[[\"categoryId\", \"y\"]].groupby(\"categoryId\").quantile(0.75).reset_index().rename({\"y\":\"q75\"}, axis=1)\nq9_ = train[[\"categoryId\", \"y\"]].groupby(\"categoryId\").quantile(0.9).reset_index().rename({\"y\":\"q9\"}, axis=1)","1539ca81":"#\u76ee\u7684\u5909\u6570\u3092\u5bfe\u6570\u5909\u63db\u3057\u3066\u3001\u6b63\u5247\u5316\u3057\u3066\u304b\u3089\u30c9\u30ed\u30c3\u30d7\u3059\u308b\ntrain[\"y\"] = np.log1p(train[\"y\"])\ny = train[\"y\"]\ndel train[\"y\"]","bbc03005":"#\u578b\u3092bool\u65b9\u304b\u3089int\u306b\u5909\u63db\ndata[\"comments_disabled\"] = data[\"comments_disabled\"].astype(np.int16)\ndata[\"ratings_disabled\"] = data[\"ratings_disabled\"].astype(np.int16)\n#False = 0: True = 1\n\n# like dislike comment\n#data[\"likes2\"] = data[\"likes\"]**2\ndata[\"loglikes\"] = np.log(data[\"likes\"]+1)\n#data[\"dislikes2\"] = data[\"dislikes\"]**2\ndata[\"logdislikes\"] = np.log(data[\"dislikes\"]+1)\ndata[\"logcomment_count\"] = np.log(data[\"comment_count\"]+1)\n\ndata[\"sqrtlikes\"] = np.sqrt(data[\"likes\"])\n\n#\u3053\u3053\u306e+1\u306f\u4f55\u304b\u306e\u5fae\u8abf\u6574\uff1f\u308f\u304b\u3089\u3093\u3002\u3002\ndata[\"like_dislike_ratio\"] = data[\"likes\"]\/(data[\"dislikes\"]+1)\ndata[\"comments_like_ratio\"] = data[\"comment_count\"]\/(data[\"likes\"]+1)\ndata[\"comments_dislike_ratio\"] = data[\"comment_count\"]\/(data[\"dislikes\"]+1)\n\n# likes comments diable\ndata[\"likes_com\"] = data[\"likes\"] * data[\"comments_disabled\"]\ndata[\"dislikes_com\"] = data[\"dislikes\"] * data[\"comments_disabled\"]\ndata[\"comments_likes\"] = data[\"comment_count\"] * data[\"ratings_disabled\"]","0a87561b":"#tag\n#\u6b20\u640d\u5024\u3092\u51e6\u7406\ndata[\"tags\"].fillna(\"[none]\", inplace = True)\n\n#\"|\"\u3092\u542b\u3093\u3067\u3044\u308b\u6587\u3067\u3001\"|\"\u3067\u533a\u5207\u3063\u3066\u3001\u30ab\u30a6\u30f3\u30c8\u3057\u3066\u3001\u30bd\u30fc\u30c8\n#\u305d\u308c\u305e\u308c\u306etag\u306e\u51fa\u73fe\u56de\u6570\u3092\u8868\u3057\u3066\u3044\u308b\ntagdic = dict(pd.Series(\"|\".join(list(data[\"tags\"])).split(\"|\")).value_counts().sort_values())\n\n#tag\u306e\u500b\u6570\u3092\u30ab\u30a6\u30f3\u30c8\ndata[\"num_tags\"] = data[\"tags\"].astype(str).apply(lambda x: len(x.split(\"|\")))\n\n#\u6587\u5b57\u6570\ndata[\"length_tags\"] = data[\"tags\"].astype(str).apply(lambda x: len(x))\n\n#\u4f7f\u3063\u3066\u3044\u308btag\u3092tagdic\u3067\u30dd\u30a4\u30f3\u30c8\u5316\u3057\u3066\u3001\u30de\u30a4\u30ca\u30fc\u304b\u3001\u6709\u540d\u306atag\u3092\u4f7f\u3063\u3066\u3044\u308b\u304b\u3092\u5224\u5b9a\u3067\u304d\u308b\ndata[\"tags_point\"] = data[\"tags\"].apply(lambda tags: sum([tagdic[tag] for tag in tags.split(\"|\")]))","41b821b6":"data.head()","b140a0b3":"#\u65e5\u4ed8\u578b\u306b\u5909\u63db\ndata[\"publishedAt\"] = pd.to_datetime(data[\"publishedAt\"], utc=True)\n\ndata[\"publishedAt_year\"] = data[\"publishedAt\"].apply(lambda x: x.year)\ndata[\"publishedAt_month\"] = data[\"publishedAt\"].apply(lambda x: x.month)\ndata[\"publishedAt_day\"] = data[\"publishedAt\"].apply(lambda x: x.day)\ndata[\"publishedAt_hour\"] = data[\"publishedAt\"].apply(lambda x: x.hour)\ndata[\"publishedAt_minute\"] = data[\"publishedAt\"].apply(lambda x: x.minute)\n  #df[\"publishedAt_second\"] = df[\"publishedAt\"].apply(lambda x: x.second)\ndata[\"publishedAt_dayofweek\"] = data[\"publishedAt\"].apply(lambda x: x.dayofweek)","779b4881":"\n#df[\"collection_date_year\"] = df[\"collection_date\"].apply(lambda x: int(x[0:2]))\ndata[\"collection_date_month\"] = data[\"collection_date\"].apply(lambda x: int(x[3:5]))\ndata[\"collection_date_day\"] = data[\"collection_date\"].apply(lambda x: int(x[6:8]))\n\n#\"collection_date\"\u3092datetime\u578b\u306b\u5909\u63db\ndata[\"collection_date\"] = pd.to_datetime(\"20\"+data[\"collection_date\"], format=\"%Y.%d.%m\", utc=True)\n\n# delta","424feffa":"data.head()","acc66021":"#delta\n#\u516c\u958b\u65e5\u304b\u3089\u30c7\u30fc\u30bf\u53ce\u96c6\u65e5\u307e\u3067\u306e\u671f\u9593\ndata[\"delta\"] = (data[\"collection_date\"] - data[\"publishedAt\"]).apply(lambda x: x.days)\n\ndata[\"logdelta\"] = np.log(data[\"delta\"])\ndata[\"sqrtdelta\"] = np.sqrt(data[\"delta\"])\n\n\ndata[\"published_delta\"] = (data[\"publishedAt\"] - data[\"publishedAt\"].min()).apply(lambda x: x.days)\ndata[\"collection_delta\"] = (data[\"collection_date\"] - data[\"collection_date\"].min()).apply(lambda x: x.days)","7d3b1e91":"#\u6b20\u640d\u5024\u306f\u7a7a\u767d\u306f\u57cb\u3081\u308b\ndata[\"description\"].fillna(\" \", inplace=True)\n\n#\u5168\u3066\u3092\u5c0f\u6587\u5b57\u306b\u5909\u63db\u3057\u3066\u3001http\u3092\u30ab\u30a6\u30f3\u30c8\ndata[\"ishttp_in_dis\"] = data[\"description\"].apply(lambda x: x.lower().count(\"http\"))\n\n#description\u306e\u9577\u3055\ndata[\"len_description\"] = data[\"description\"].apply(lambda x: len(x))","0e326a58":"data[\"title\"].fillna(\" \", inplace=True)\n#\u6b20\u640d\u5024\u304c\u3042\u308b\u5834\u5408\u7a7a\u767d\u3067\u57cb\u3081\u308b\n\ndata[\"len_title\"] = data[\"title\"].apply(lambda x: len(x))\n#\u30bf\u30a4\u30c8\u30eb\u306e\u9577\u3055","72d4644d":"import unicodedata\n\n# \u6587\u5b57\u5217string\u306b1\u6587\u5b57\u3067\u3082\u300c\u3072\u3089\u304c\u306a\u300d\u300c\u30ab\u30bf\u30ab\u30ca\u300d\u300c\u6f22\u5b57\u300d\u306e\u3069\u308c\u304b\u304c\u542b\u307e\u308c\u3066\u3044\u308c\u3070True\u3092\u8fd4\u3057\u307e\u3059\ndef is_japanese(string):\n    for ch in string:\n        try:\n            name = unicodedata.name(ch) \n            if \"CJK UNIFIED\" in name \\\n            or \"HIRAGANA\" in name \\\n            or \"KATAKANA\" in name:\n                return True\n        except:\n            continue\n    return False\n\n# is japanese\ndata[\"isJa_title\"] = data[\"title\"].apply(lambda x: is_japanese(x))\ndata[\"isJa_tags\"] = data[\"tags\"].apply(lambda x: is_japanese(x))\ndata[\"isJa_description\"] = data[\"description\"].apply(lambda x: is_japanese(x))","91f56173":"data.head()","3fa4d961":"# is englosh\n#\u5168\u3066\u304c\u82f1\u6570\u5b57\u306e\u307f\u304b\u5224\u5b9a\u3059\u308b\n#\u6587\u5b57\u30b3\u30fc\u30c9\u3092\u6307\u5b9a\u3057\u306a\u3044\u3068\u3046\u307e\u304f\u5224\u5b9a\u3067\u304d\u306a\u3044\ndata[\"onEn_title\"] = data[\"title\"].apply(lambda x: x.encode('utf-8').isalnum())\ndata[\"onEn_tags\"] = data[\"tags\"].apply(lambda x: x.encode('utf-8').isalnum())\ndata[\"onEn_description\"] = data[\"description\"].apply(lambda x: x.encode('utf-8').isalnum())","c8d26b90":"import re\n# cotain englosh\n#findall\u306f\u30de\u30c3\u30c1\u3059\u308b\u3059\u3079\u3066\u306e\u90e8\u5206\u6587\u5b57\u5217\u3092\u30ea\u30b9\u30c8\u306b\u3057\u3066\u8fd4\u3059\n#\u307e\u305flen\u3092\u4f7f\u3063\u3066\u30ea\u30b9\u30c8\u306e\u9577\u3055\u3092\u56f3\u308b\u3053\u3068\u3067\u3001\u30de\u30c3\u30c1\u3057\u305f\u6587\u5b57\u306e\u9577\u3055\u3060\u3051\u3092\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u308b\n#x.lower\u3092\u4f7f\u3046\u7406\u7531\u306f\u51e6\u7406\u304c\u591a\u5c11\u65e9\u304f\u306a\u308b\u304b\u3089\uff01  (\u4f7f\u308f\u306a\u304f\u3066\u3082\u7d50\u679c\u306f\u5909\u308f\u3089\u306a\u3044)\ndata[\"conEn_title\"] = data[\"title\"].apply(lambda x: len(re.findall(r'[a-zA-Z0-9]', x.lower())))\ndata[\"conEn_tags\"] = data[\"tags\"].apply(lambda x: len(re.findall(r'[a-zA-Z0-9]', x.lower())))\ndata[\"conEn_description\"] = data[\"description\"].apply(lambda x: len(re.findall(r'[a-zA-Z0-9]', x.lower())))\n\n\n","32dc8566":"data.head()","78451dd5":"# Music\ndata[\"music_title\"] = data[\"title\"].apply(lambda x: \"music\" in x.lower())\ndata[\"music_tags\"] = data[\"tags\"].apply(lambda x: \"music\" in x.lower())\ndata[\"music_description\"] = data[\"description\"].apply(lambda x: \"music\" in x.lower())","2b27d860":"# Official\ndata[\"isOff\"] = data[\"title\"].apply(lambda x: \"fficial\" in x.lower())\ndata[\"isOffChannell\"] = data[\"channelTitle\"].apply(lambda x: \"fficial\" in x.lower())\ndata[\"isOffJa\"] = data[\"title\"].apply(lambda x: \"\u516c\u5f0f\" in x.lower())\ndata[\"isOffChannellJa\"] = data[\"channelTitle\"].apply(lambda x: \"\u516c\u5f0f\" in x.lower())\n\n#\u516c\u5f0f\u30a2\u30ab\u30a6\u30f3\u30c8\u3092\u77e5\u308b\u305f\u3081\u306b\u3001\"\u516c\u5f0f\"\u3068\"fficial\u306e2\u30d1\u30bf\u30fc\u30f3\u3092\u8003\u3048\u308b\n#ffical\u306e\u7406\u7531\u306f O\u304c\u5927\u6587\u5b57\u3068\u5c0f\u6587\u5b57\u3001\u7a7a\u767d\u304c\u3042\u308b\u5834\u5408\u304c\u3042\u308b\u304b\u3089","319a0921":"#CM\u306e\u5834\u5408\u306e\u51e6\u7406\ndata[\"cm_title\"] = data[\"title\"].apply(lambda x: \"cm\" in x.lower())\ndata[\"cm_tags\"] = data[\"tags\"].apply(lambda x: \"cm\" in x.lower())\ndata[\"cm_description\"] = data[\"description\"].apply(lambda x: \"cm\" in x.lower())","c897b77e":"#\u6700\u521d\u306b\u6c42\u3081\u305f\u5974\u306e\u7d50\u5408\ndata = data.merge(mean_, how='left', on=[\"categoryId\"])\ndata = data.merge(max_, how='left', on=[\"categoryId\"])\ndata = data.merge(min_, how='left', on=[\"categoryId\"])\ndata = data.merge(std_, how='left', on=[\"categoryId\"])\n  #data = df.merge(count_, how='left', on=[\"categoryId\"])\ndata = data.merge(q1_, how='left', on=[\"categoryId\"])\ndata = data.merge(q25_, how='left', on=[\"categoryId\"])\ndata = data.merge(q5_, how='left', on=[\"categoryId\"])\ndata = data.merge(q75_, how='left', on=[\"categoryId\"])\ndata = data.merge(q9_, how='left', on=[\"categoryId\"])","eba264e7":"# \u51fa\u73fe\u983b\u5ea6\n\nfor col in [\"categoryId\", \"channelTitle\"]:\n    freq = data[col].value_counts()\n    data[\"freq_\"+col] = data[col].map(freq)\nfreq","a695374b":"data.head()","6e6bea87":"#\u8868\u793a\u3055\u308c\u308b\u5217\u6570\u3068\u3001\u884c\u6570\u3092\u5897\u3084\u3057\u305f\n#\u524a\u9664\u3059\u308b\u5217\u306e\u78ba\u8a8d\u304c\u4e0d\u4fbf\u3060\u3063\u305f\u306e\u3067\u8ffd\u52a0\u3057\u305f\npd.set_option('display.max_columns', 68)\npd.set_option('display.max_rows', 100)","f6280164":"data.head()","45411ac5":"del data[\"id\"]\ndel data[\"video_id\"]\ndel data[\"title\"]\ndel data[\"publishedAt\"]\ndel data[\"channelId\"]\ndel data[\"channelTitle\"]\ndel data[\"collection_date\"]\ndel data[\"tags\"]\ndel data[\"thumbnail_link\"]\ndel data[\"description\"]\ndel data[\"y\"]","aeb9c044":"#\u3053\u3053\u306e\u6b20\u640d\u5730\u306f\u5168\u3066\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u7269\u306a\u306e\u3067\u504f\u3089\u306a\u3044\u69d8\u306b\u6700\u8fd1\u508d\u6cd5\u7528\u306e\u30c7\u30fc\u30bf\u3092\u4f5c\u308b\ndata_kmeans = data\ndata_kmeans[\"mean\"] = data[\"mean\"].fillna(np.mean(data[\"mean\"]))\ndata_kmeans[\"max\"] = data[\"max\"].fillna(np.mean(data[\"max\"]))\ndata_kmeans[\"min\"] = data[\"min\"].fillna(np.mean(data[\"min\"]))\ndata_kmeans[\"std\"] = data[\"std\"].fillna(np.mean(data[\"std\"]))\ndata_kmeans[\"q1\"] = data[\"q1\"].fillna(np.mean(data[\"q1\"]))\ndata_kmeans[\"q25\"] = data[\"q25\"].fillna(np.mean(data[\"q25\"]))\ndata_kmeans[\"q5\"] = data[\"q5\"].fillna(np.mean(data[\"q5\"]))\ndata_kmeans[\"q75\"] = data[\"q75\"].fillna(np.mean(data[\"q75\"]))\ndata_kmeans[\"q9\"] = data[\"q9\"].fillna(np.mean(data[\"q9\"]))","e7b5f00d":"from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\n#\u30b9\u30b1\u30fc\u30eb\u5909\u63db\nscalar = StandardScaler()\nscalar.fit(data)\ndata = pd.DataFrame(scalar.transform(data), columns=data.columns)\ndata_kmeans = pd.DataFrame(scalar.transform(data_kmeans), columns=data.columns)\n\nkmeans = KMeans(n_clusters=100, random_state=0)\nclusters = kmeans.fit(data_kmeans)\ndata[\"cluster\"] = clusters.labels_\nprint(data[\"cluster\"].unique())\ndata.head()","6a8438f5":"#PCA\nfrom sklearn.decomposition import PCA\nX = data\npca = PCA(n_components=2)\npca.fit(X)\nx_pca = pca.transform(X)\npca_df = pd.DataFrame(x_pca)\npca_df.reset_index(drop = True)\npca_df[\"cluster\"]= data[\"cluster\"].reset_index(drop = True)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfor i in pca_df[\"cluster\"].unique():\n    tmp = pca_df.loc[pca_df[\"cluster\"]==i]\n    plt.scatter(tmp[0], tmp[1])","7201fc14":"#UMAP\nimport umap\nimport matplotlib.cm as cm\num = umap.UMAP()\num.fit(data)\ndata_1 = um.transform(data)\nplt.scatter(data_1[:,0],data_1[:,1],c=clusters.labels_,cmap=cm.tab10)\nplt.colorbar()","be60fef7":"  #\u76f8\u95a2\u4fc2\u6570\u3092\u898b\u3066\u3001\u95a2\u4fc2\u306e\u3042\u308a\u305d\u3046\u306a\u8aac\u660e\u5909\u6570\u3092\u898b\u308b\n#X = data.iloc[:train_len, :]\n#corrmat_list = pd.concat([X,y],axis = 1)\n\n#import seaborn as sns\n#import matplotlib.pyplot as plt\n#corrmat = corrmat_list.corr()\n#k = 20\n\n#cols = corrmat.nlargest(k, 'y')['y'].index\n#cm = np.corrcoef(corrmat_list[cols].values.T)\n  #\u76f8\u95a2\u4fc2\u6570\u9ad8\u3044\u9806top10\u3092\u6c42\u3081\u305f\n\n#sns.set(font_scale = 1.25)\n#cm = sns.heatmap(cm, annot=True, square=True, fmt='.2f', annot_kws={'size': 0.001}, yticklabels=cols.values, xticklabels=cols.values)\n  #annot=\u30bb\u30eb\u306b\u5024\u3092\u51fa\u529b\n  #square=X,y\u3067\u6b63\u65b9\u5f62\n  #fmt = \u6587\u5b57\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3092\u6307\u5b9a\n  #annot_kws = \u6587\u5b57\u306e\u30b5\u30a4\u30ba\n\n#plt.show()\n#cols","0b89d60f":"#from sklearn.preprocessing import QuantileTransformer\n\n#sum_list = (data[cols[i]] for i in range(1,20))\n\n  #\u30e9\u30f3\u30af\u4ed8\u3051\u3067\u304d\u308b\u69d8\u306bndarry\u65b9\u306b\u3057\u3066\u578b\u3092\u5408\u308f\u305b\u308b\n#sum_values = [list(sum_list)]\n#sum_values = np.array(sum_values)\n\n  #\u5f62\u3092\u5408\u308f\u305b\u308b\n#sum_values = sum_values.reshape(-1,1) \n\n\n#transformer = QuantileTransformer(n_quantiles = 100, random_state = 0, output_distribution= \"normal\")\n#transformer.fit(data[cols[1:]])\n#data[cols[1:]] = transformer.transform(data[cols[1:]])","756547da":"#\u3042\u308b\u7a0b\u5ea6\u6b63\u5247\u5316\u3055\u308c\u3066\u3044\u308b\n#plt.hist(data[\"rank\"], bins = 250)\n#plt.xlim(-3, 3)\n#plt.show()","fc7370be":"#\u8a13\u9023\u7528\u3068\u8a55\u4fa1\u7528\u306b\u5206\u3051\u308b\nX = data.iloc[:train_len, :]\ntest = data.iloc[train_len:, :]\nprint(X.shape, y.shape,test.shape)","7779db68":"from sklearn.cluster import MiniBatchKMeans\n\nkmeans = MiniBatchKMeans(n_clusters=10, random_state = 0)\nkmeans.fit(X)\n\n  #\u5c5e\u3059\u308b\u30af\u30e9\u30b9\u30bf\u3092\u51fa\u529b\ncluster_train = kmeans.predict(X)\ncluster_test = kmeans.predict(test)\n  #\u5404\u30af\u30e9\u30b9\u30bf\u306e\u4e2d\u5fc3\u307e\u3067\u306e\u8ddd\u96e2\u3092\u51fa\u529b\ntrain_distances = kmeans.transform(X)\ntest_distances = kmeans.transform(test)\n\ncluster_train = pd.DataFrame(cluster_train)\ncluster_test = pd.DataFrame(cluster_test)\n\n  #\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u632f\u308a\u306a\u304a\u3055\u306a\u3044\u3068\u3001cluster\u3068\u6574\u5408\u6027\u306a\u3044\n#X.reset_index(drop=True, inplace = True)\ntest.reset_index(drop=True, inplace = True)\n\nX = pd.concat([X,pd.DataFrame(train_distances), cluster_train], axis = 1)\ntest = pd.concat([test, pd.DataFrame(test_distances), cluster_test], axis = 1)\n\n   #\u5217\u540d\u304c\u91cd\u8907\u3057\u3066\u3044\u3066\u3001xgboost\u304c\u30a8\u30e9\u30fc\u51fa\u3057\u305f\u306e\u3067\u3001\u91cd\u8907\u5217\u3092\u63a2\u3059\na = X.columns[X.columns.duplicated()]\n  #[0]\u2190\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3058\u3083\u306a\u304f\u3066\"0\"\u306e\u540d\u524d\u304c\u91cd\u8907\u3057\u3066\u3044\u308b\nprint(a)\n\ncol = X.columns.values\ncol[-1] = 'k-means'\nX.columns = col\n\ncol = test.columns.values\ncol[-1] = 'k-means'\ntest.columns = col\n  #\u3064\u3044\u3067\u306b\u60c5\u5831\u3092\u307f\u3066\u307f\u308b\nX[1].describe()\n  #max\u304c\u9ad8\u904e\u304e\u308b\u3002\u5916\u308c\u5024\u7684\u306a\u306e\u304c\u5b58\u5728\u3057\u3066\u3044\u308b\u53ef\u80fd\u6027\u304c\u9ad8\u3044\n    ","44a6798a":"from sklearn.model_selection import train_test_split\n#\u8a13\u7df4\u7528\u3068\u30c6\u30b9\u30c8\u7528\u306b\u5206\u3051\u308b\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0, shuffle = True)","3f36dd21":"print(X_train.shape, y_train.shape)","6504dadb":"from sklearn.model_selection import KFold\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\n","eba2a632":"# RMLSE \u3092\u8a08\u7b97\u3059\u308b\ndef rmsle_(y_valid, y_pred):\n    rmsle = np.sqrt(mean_squared_log_error(np.exp(y_valid), np.exp(y_pred)))\n    return rmsle\n\nparameters = {\n    'n_estimators' :[3,5,10,30,50],\n    'random_state' :[7,42],\n    'max_depth' :[3,5,8,10],\n}\n\n#\u4ea4\u5dee\u691c\u8a3c\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\nkfold_cv = KFold(n_splits = 5, shuffle=True)\n\n","bbd533eb":"#\u30b0\u30ea\u30c3\u30c9\u30b5\u30fc\u30c1\u3067\u826f\u3044\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u63a2\u3059\nGrid = GridSearchCV(estimator= RandomForestRegressor(), param_grid=parameters, cv=3)\nGrid.fit(X_train, y_train)\n\nbest_model = Grid.best_estimator_\ntrain_score = best_model.score(X_train, y_train)\ntest_score = best_model.score(X_test, y_test)\nprint(\"\u8a13\u7df4\u3067\u306e\u8a8d\u8b58\u7cbe\u5ea6:\" + str(train_score))\nprint(\"\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3067\u306e\u8a8d\u8b58\u7cbe\u5ea6   :\" + str(test_score))\n\nprint(Grid.best_estimator_)","0bda8e01":"#\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1\nfrom sklearn.metrics import mean_squared_log_error\ny_pred_test_RF = best_model.predict(X_test)\n\nRMSLE = np.sqrt(mean_squared_log_error(y_test, y_pred_test_RF))\n#\u8996\u8074\u7387\u306f0\u672a\u6e80\u306b\u306a\u3089\u306a\u3044\ny_pred_test_RF[y_pred_test_RF < 0] = 0\nprint(RMSLE)","8ac9036a":"a = test.isnull().sum()\na = pd.DataFrame(a)\na","93dbe3d2":"test[\"mean\"] = test[\"mean\"].fillna(np.mean(test[\"mean\"]))\ntest[\"max\"] = test[\"max\"].fillna(np.mean(test[\"max\"]))\ntest[\"min\"] = test[\"min\"].fillna(np.mean(test[\"min\"]))\ntest[\"std\"] = test[\"std\"].fillna(np.mean(test[\"std\"]))\ntest[\"q1\"] = test[\"q1\"].fillna(np.mean(test[\"q1\"]))\ntest[\"q25\"] = test[\"q25\"].fillna(np.mean(test[\"q25\"]))\ntest[\"q5\"] = test[\"q5\"].fillna(np.mean(test[\"q5\"]))\ntest[\"q75\"] = test[\"q75\"].fillna(np.mean(test[\"q75\"]))\ntest[\"q9\"] = test[\"q9\"].fillna(np.mean(test[\"q9\"]))","7cb6b35e":"test.isnull().sum()","83193fc2":"\ny_pred_eval_RF = best_model.predict(test)\ny_pred_eval_RF = np.exp(y_pred_eval_RF)\n#\u5bfe\u6570\u5909\u63db\u3057\u3066\u3044\u305fy\u3092\u623b\u3059","acbd0719":"df_sub_pred = pd.DataFrame(y_pred_eval_RF).rename(columns = {0:\"y\"})\ndf_sub_pred = pd.concat([test_data['id'], df_sub_pred['y']], axis=1)\ndf_sub_pred\ndf_sub_pred.to_csv(\"randomforest.csv\", index=False)","8cfb1edb":"import lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_log_error\n\n\n#\u8ab2\u984c:\u4eca\u306frmse\u3067\u6700\u9069\u5316\u3057\u3066\u3044\u308b\u304c\u3001\u30ab\u30b9\u30bf\u30e0\u30e1\u30c8\u30ea\u30c3\u30af\u3092\u4f7f\u3063\u3066\u3001rmsle\u3067\u6700\u9069\u5316\u3057\u305f\u3044\nlight_params = {'task': 'train', #\u4ed6\u306bpredict\u3001convert_model\u3001refit\u306a\u3069\u3082\u3042\u308b\n        'boosting_type': 'gbdt',\n        'objective': 'regression',\n        'metric': 'rmse', #\u8a55\u4fa1\u95a2\u6570\n        'verbosity': -1, #\u5b66\u7fd2\u9014\u4e2d\u3092\u8868\u793a\u3057\u306a\u3044,-1\u3067\u8868\u793a\u3057\u306a\u3044\n        \"seed\":42, #\u30b7\u30fc\u30c9\u5024\n        'learning_rate': 0.01, #\u5b66\u7fd2\u7387\n        'feature_fraction': 0.7, #\u5b66\u7fd2\u306e\u9ad8\u901f\u5316\u3068\u904e\u5b66\u7fd2\u306e\u6291\u5236\u306b\u4f7f\u7528\n        'num_leaves': 210, #\u30ce\u30fc\u30c9\u306e\u6570\u2192\u8449\u304c\u591a\u3044\u307b\u3069\u8907\u96d1\u306b\u306a\u308b\n        'bagging_fraction' : 0.8,# 0<= bagging_fraction <= 1 \u30ea\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u305b\u305a\u306b\u30c7\u30fc\u30bf\u306e\u4e00\u90e8\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u9078\u629e\u3057\u307e\u3059\n        'bagging_freq': 5, #\u30d0\u30ae\u30f3\u30b0\u306b\u5fc5\u8981\u306a\u5024\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u306f1,'bagging_fraction'\u3068\u30bb\u30c3\u30c8\u3067\u4f7f\u3046\n        'min_child_samples': 5} #\u6700\u5c0f\u30ce\u30fc\u30c9 \n\n\nxgb_params = {'learning_rate': 0.1,\n              'objective': 'reg:squarederror', #\u640d\u5931\u95a2\u6570\n              'eval_metric': 'rmse',#\u8a55\u4fa1\u95a2\u6570\n              'seed': 42,\n              'tree_method': 'hist',\n             'learning_rate': 0.01,\n             'num_leaves': 1000,}\n\n","03a1e2c2":"#\u4ea4\u5dee\u691c\u8a3c\u306e\u904e\u7a0b\u3092\u5165\u308c\u305f\u3044\u3002\u3002\u3002\n\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y , random_state = 0)\n\nlgb_train_data = lgb.Dataset(X_train, y_train)\nlgb_valid_data = lgb.Dataset(X_valid, y_valid)\nlgb_test = lgb.Dataset(test)\n\n\n\nmodel = lgb.train(light_params,\n                  lgb_train_data,\n                  valid_sets=[lgb_train_data, lgb_valid_data],\n                  early_stopping_rounds=100, #\u9023\u7d9a\u3067\u5024\u304c\u3042\u307e\u308a\u5909\u308f\u3089\u306a\u3044\u4e0a\u9650\n                  verbose_eval=500, #\u8a55\u4fa1\u904e\u7a0b\u3092\u8868\u793a\n                  num_boost_round=10000) #\u640d\u5931\u95a2\u6570\u304c\u6700\u5c0f\u306b\u306a\u308b\u6240\u304c\u671b\u307e\u3057\u3044\n\n\n","a21ffb3a":"y_pred_valid_lgb = model.predict(X_valid, num_iteration = model.best_iteration)\n\n#\u8996\u8074\u7387\u306f0\u3092\u4e0b\u56de\u3089\u306a\u3044\u306e\u3067\u4fee\u6b63\ny_pred_valid_lgb[y_pred_valid_lgb < 0] = 0\n\n#rmsle\u30b9\u30b3\u30a2\u3092\u5224\u5b9a\u3000\nprint(\"rmsle :\", rmsle_(y_valid, y_pred_valid_lgb))\n\n\ny_pred = model.predict(test,num_iteration=model.best_iteration)\ny_pred_lgb = np.exp(y_pred)","d29692a2":"#\u30c7\u30fc\u30bf\u306e\u91cd\u8981\u5ea6\nlgb.plot_importance(model, figsize=(12, 20))\nplt.show()","602451d2":"xgb_train_data = xgb.DMatrix(X_train, label=y_train)\nxgb_valid_data = xgb.DMatrix(X_valid, label = y_valid)\nxgb_test = xgb.DMatrix(test)\n\nnum_round = 10000 #\u5b66\u7fd2\u56de\u6570\n\n#evals \u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u3068\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u3092\u6e21\u3059\u3002 \u305d\u306e\u4e0a\u3067 evals_result \u30aa\u30d7\u30b7\u30e7\u30f3\u306b\u904e\u7a0b\u3092\u8a18\u9332\u3059\u308b\u305f\u3081\u306e\u8f9e\u66f8\u3092\u6e21\u3059\nevals_result = {}\n\n#\u5b66\u7fd2\u306f1428\u307e\u3067\u3057\u3066\u3044\u308b\n#\u8ab2\u984c:xgb.cv\u306e\u5c64\u5316\u691c\u8a3c\u3092\u5165\u308c\u308b\u4e8b\u306f\u3067\u304d\u306a\u3044\u304b\nbst = xgb.train(xgb_params, \n                xgb_train_data, \n                num_round,\n                evals=[(xgb_train_data, 'train'),(xgb_valid_data, 'eval')],\n                evals_result=evals_result,\n                early_stopping_rounds=100,\n                verbose_eval=500\n                )\n\ny_pred_valid_xgb = bst.predict(xgb_valid_data)\ny_pred_valid_xgb[y_pred_valid_xgb < 0] = 0\nprint(\"rmsle:\", rmsle_(y_valid, y_pred_valid_xgb))\n\n\ny_pred_xgb = np.exp(bst.predict(xgb_test))","c4a915eb":"#\u91cd\u8981\u5ea6\u306e\u53ef\u8996\u5316\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 20))\nxgb.plot_importance(bst, ax = ax)\n","2a54ee55":"#\u5b66\u7fd2\u904e\u7a0b\u306e\u53ef\u8996\u5316\ntrain_metric = evals_result['train']['rmse'] #\u8a13\u7df4\u30c7\u30fc\u30bf\nplt.plot(train_metric, label='train rmse')\n\neval_metric = evals_result['eval']['rmse'] #\u8a55\u4fa1\u30c7\u30fc\u30bf\nplt.plot(eval_metric, label='eval rmse')\n\nplt.grid()\nplt.legend()\nplt.xlabel('rounds')\nplt.ylabel('logloss')\nplt.show()\n\n#\u3053\u308c\u3067\u904e\u5b66\u7fd2 or \u30a2\u30f3\u30c0\u30fc\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u3057\u3066\u3044\u306a\u3044\u4e8b\u304c\u5206\u304b\u308b ","690a3178":"test.shape","113646f5":"print(\"lgb-rmsle\",rmsle_(y_valid, y_pred_valid_lgb))\nprint(\"xgb-rmsle\",rmsle_(y_valid, y_pred_valid_xgb))\nprint(\"lgb_xgb-rmsle\",rmsle_(y_valid, (y_pred_valid_lgb+y_pred_valid_xgb) \/ 2))\n#\u8a55\u4fa1\u304c\u9ad8\u3044\u9806\u306b\n#lgb\n#lgb_xgb\n#xgb_RF_lgb","ac215329":"#RF\u542b\u3093\u3060\u7d44\u307f\u5408\u308f\u305b\nprint(\"RF-rmsle\",rmsle_(y_valid, y_pred_test_RF))\nprint(\"lgb_RF-rmsle\",rmsle_(y_valid, (y_pred_valid_lgb+y_pred_test_RF) \/ 2))\nprint(\"xgb_RF-rmsle\",rmsle_(y_valid, (y_pred_valid_xgb+y_pred_test_RF) \/ 2))\nprint(\"xgb_RF_lgb-rmsle\",rmsle_(y_valid, (y_pred_valid_lgb+y_pred_test_RF+y_pred_valid_xgb) \/ 3))","b8adce72":"sub_df = pd.concat([iddf, pd.DataFrame(y_pred_lgb)], axis = 1)\nsub_df.columns = [\"id\", \"y\"]\nsub_df.to_csv(\"submission_lgb_sub.csv\", index=False)","b45cb250":"sub = y_pred_xgb + y_pred_lgb \/2\nsub_df = pd.concat([iddf, pd.DataFrame(sub)], axis = 1)\nsub_df.columns = [\"id\", \"y\"]\nsub_df.to_csv(\"submission_lgb_xgb.csv\", index=False)","37d60948":"sub = y_pred_xgb + y_pred_lgb + y_pred_eval_RF\/3\nsub_df = pd.concat([iddf, pd.DataFrame(sub)], axis = 1)\nsub_df.columns = [\"id\", \"y\"]\nsub_df.to_csv(\"submission_lgb_xgb.csv\", index=False)","682ec2fd":"#-----\u8ab2\u984c-------\n#UMAP\u306e\u30e2\u30c7\u30eb\u3092lgb\u304bxgb\u306b\u3044\u308c\u308b\u4e8b\u306f\u3067\u304d\u308b\u306e\u304b\n#\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u306e\u4ed5\u65b9\u3092\u5de5\u592b\u3059\u308b\n#xgb.cv\u3067\u4ea4\u5dee\u691c\u8a3c\u3092\u3044\u308c\u308b\u4e8b\u3063\u3066\u3067\u304d\u308b\u306e\u304b\n#\u30ab\u30b9\u30bf\u30e0\u30e1\u30c8\u30ea\u30c3\u30af\u3067rmsle\u8a55\u4fa1\u3067\u6700\u9069\u5316\u3059\u308b\n#lgb\u306e\u5b66\u7fd2\u904e\u7a0b\u3092\u53ef\u8996\u5316\u3059\u308b\n#\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5b8c\u5168\u7406\u89e3\u3059\u308b\n#\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u3092\u3059\u308b","63595920":"import lightgbm as lgb\nimport xgboost as xgb\ndef rmsle(preds, data):\n    y_true = data.get_label()\n    y_pred = preds\n    y_pred[y_pred<0] = 0\n    y_true[y_true<0] = 0\n    acc = np.sqrt(mean_squared_log_error(np.exp(y_true), np.exp(y_pred)))\n    # name, result, is_higher_better\n    return 'accuracy', acc, False\n\n# Optuna\u306e\u6700\u9069\u5316\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u4ee3\u5165\u3059\u308b\nlight_params = {'task': 'train', #\u4ed6\u306bpredict\u3001convert_model\u3001refit\u306a\u3069\u3082\u3042\u308b\n        'boosting_type': 'gbdt',\n        'objective': 'regression',\n        'metric': 'rmse', #\u8a55\u4fa1\u95a2\u6570\n        'verbosity': -1, #\u5b66\u7fd2\u9014\u4e2d\u3092\u8868\u793a\u3057\u306a\u3044,-1\u3067\u8868\u793a\u3057\u306a\u3044\n        \"seed\":42, #\u30b7\u30fc\u30c9\u5024\n        'learning_rate': 0.01, #\u5b66\u7fd2\u7387\n        'feature_fraction': 0.7, #\u5b66\u7fd2\u306e\u9ad8\u901f\u5316\u3068\u904e\u5b66\u7fd2\u306e\u6291\u5236\u306b\u4f7f\u7528\n        'num_leaves': 210} #\u30ce\u30fc\u30c9\u306e\u6570\u2192\u8449\u304c\u591a\u3044\u307b\u3069\u8907\u96d1\u306b\u306a\u308b\n               \nbest_params =  {'lambda_l1': 0.019918875912078603, 'lambda_l2': 0.002616688073257713, 'num_leaves': 219, 'feature_fraction': 0.6641013611124621, 'bagging_fraction': 0.7024199018549259, 'bagging_freq': 5, 'min_child_samples': 5}\n#best_params =  {}\nlight_params.update(best_params)\n\nxgb_params = {'learning_rate': 0.1,\n              'objective': 'reg:squarederror',\n              'eval_metric': 'rmse',\n              'seed': 42,\n              'tree_method': 'hist'}\nbest_params = {'learning_rate': 0.01665914389764044, 'lambda_l1': 4.406831762257336, 'num_leaves': 39}\n#best_params = {}\nxgb_params.update(best_params)\n\n\nFOLD_NUM = 11\nkf = KFold(n_splits=FOLD_NUM,\n           shuffle=True,\n           random_state=42)\n\nscores = []\nfeature_importance_df = pd.DataFrame()\npred_cv = np.zeros(len(test.index))\nnum_round = 10000\n\n#i\u306f\u4ea4\u5dee\u691c\u8a3c\u306e\u56de\u6570 tdx\u3068vdx\u306f\u30e9\u30f3\u30c0\u30e0\u306b\u9078\u3070\u308c\u305ftrain,test\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\nfor i, (tdx, vdx) in enumerate(kf.split(X, y)): \n    print(f'Fold : {i}')\n    ######LGB\n    #\u306a\u305cy.values\u306a\u306e\u304b\u3000\u3053\u308c\u3060\u3051\u308f\u304b\u308c\u3070\u3053\u3053\u306e\u30b3\u30fc\u30c9\u884c\u3051\u308b\uff01\n    X_train, X_valid, y_train, y_valid = X.iloc[tdx], X.iloc[vdx], y.values[tdx], y.values[vdx]\n\n    # LGB\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_valid = lgb.Dataset(X_valid, y_valid)\n    gbc = lgb.train(light_params, lgb_train, num_boost_round=num_round,\n                  valid_names=[\"train\", \"valid\"], valid_sets=[lgb_train, lgb_valid],\n                  #feval=rmsle,\n                  early_stopping_rounds=100, verbose_eval=500)\n    if i ==0:\n        importance_df = pd.DataFrame(gbc.feature_importance(), index=X.columns, columns=['importance'])\n    else:\n        importance_df += pd.DataFrame(gbc.feature_importance(), index=X.columns, columns=['importance'])\n    gbc_va_pred = np.exp(gbc.predict(X_valid, num_iteration=gbc.best_iteration))\n    gbc_va_pred[gbc_va_pred<0] = 0\n\n    # XGB\n    xgb_dataset = xgb.DMatrix(X_train, label=y_train)\n    xgb_test_dataset = xgb.DMatrix(X_valid, label=y_valid)\n    xgbm = xgb.train(xgb_params, xgb_dataset, 10000, evals=[(xgb_dataset, 'train'),(xgb_test_dataset, 'eval')],\n                      early_stopping_rounds=100, verbose_eval=500)\n    xgbm_va_pred = np.exp(xgbm.predict(xgb.DMatrix(X_valid)))\n    xgbm_va_pred[xgbm_va_pred<0] = 0\n    \n\n    # ENS\n    # lists for keep results\n    lgb_xgb_rmsle = []\n    lgb_xgb_alphas = []\n\n    for alpha in np.linspace(0,1,101): #(0 ~ 1\u307e\u3067\u30e9\u30f3\u30c0\u30e0\u306a\u6570\u3092100\u56de\u751f\u6210\u3059\u308b)\n        y_pred = alpha*gbc_va_pred + (1 - alpha)*xgbm_va_pred #\u3053\u306elgb\u3068xgb\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3081\u3061\u3083\u304f\u3061\u3083\u65e8\u3044\uff01\n        rmsle_score = np.sqrt(mean_squared_log_error(np.exp(y_valid), y_pred))\n        \n        #\u30b9\u30b3\u30a2\u3068\u30e9\u30f3\u30c0\u30e0\u306a\u6570\u5024\u3092\u683c\u7d0d\n        lgb_xgb_rmsle.append(rmsle_score)\n        lgb_xgb_alphas.append(alpha)\n    \n    #ndarray\u306b\u5909\u63db\n    lgb_xgb_rmsle = np.array(lgb_xgb_rmsle)\n    lgb_xgb_alphas = np.array(lgb_xgb_alphas)\n    \n    #rmsle\u304c\u4e00\u756a\u4f4e\u3044(\u30d9\u30b9\u30c8\u306a\u30b9\u30b3\u30a2)\u6642\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u5f97\u3066\u3001alpha\u3092\u683c\u7d0d\n    lgb_xgb_best_alpha = lgb_xgb_alphas[np.argmin(lgb_xgb_rmsle)]\n    \n    \n    print('best_rmsle=', lgb_xgb_rmsle.min())\n    print('best_alpha=', lgb_xgb_best_alpha)\n    \n    plt.plot(lgb_xgb_alphas, lgb_xgb_rmsle)\n    plt.title('f1_score for ensemble')\n    plt.xlabel('alpha')\n    plt.ylabel('f1_score')\n    plt.legend()\n\n    score_ = lgb_xgb_rmsle.min()\n    scores.append(score_)\n    #\u4e00\u756a\u826f\u304b\u3063\u305f\u30b9\u30b3\u30a2\u3092append\u3059\u308b\n\n    lgb_submission = np.exp(gbc.predict((test), num_iteration=gbc.best_iteration))\n    lgb_submission[lgb_submission<0] = 0\n\n    xgbm_submission = np.exp(xgbm.predict(xgb.DMatrix(test)))\n    xgbm_submission[xgbm_submission<0] = 0\n\n    submission = lgb_xgb_best_alpha*lgb_submission + (1 - lgb_xgb_best_alpha)*xgbm_submission #\u4e0a\u306e\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u306e\u4ed5\u65b9\u3068\u4e00\u7dd2\n    \n    #FOLD_NUM\u500b\u5206\u306e\u7b54\u3048\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u5272\u308b\n    #\u3053\u308c\u3092FOLD_NUM\u56de\u7e70\u308a\u8fd4\u3059\u306e\u3067pred_cv\u306f\u826f\u3044\u611f\u3058\u306e\u7b54\u3048\u306b\u306a\u308b\n    pred_cv += submission\/FOLD_NUM\n\nprint(\"##########\")\nprint(np.mean(scores))","d2fc18fc":"light_submission_df = pd.concat([iddf, pd.DataFrame(pred_cv)], axis=1)\nlight_submission_df.columns = [\"id\", \"y\"]\nlight_submission_df.to_csv(\"submission_lgb.csv\", index=False)\nprint(\"end\")","bc20854e":"\ntest.isnull().sum()","f37f615b":"# title,description, tags\u306b\u6ce8\u76ee","17d03d2f":"### lgb_xgb","552e01af":"# \u6700\u8fd1\u508d\u6cd5\u3067100\u500b\u306e\u30b0\u30eb\u30fc\u30d7\u5206\u3051\u3092\u3057\u3066\u307f\u308b","b2f16b75":"# \u7279\u5fb4\u91cf\u3092\u8ffd\u52a0\u3057\u3066\u3044\u304f","6719d2c3":"# publishedAt\u306b\u6ce8\u76ee\u3059\u308b","8d7c733e":"# Official\u306b\u6ce8\u76ee","92a83297":"# \u30ab\u30c6\u30b4\u30ea\u30fcID\u3068\u76ee\u7684\u5909\u6570\u306e\u95a2\u4fc2\u3092\u8ffd\u52a0\u3059\u308b","90cb2b78":"# LGB\u3068XGB\u3092\u4f7f\u3063\u3066\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb","b13b5dfc":"# \u30e9\u30f3\u30af\u4ed8\u3051\u3059\u308b (\u3053\u308c\u3082\u7cbe\u5ea6\u3092\u4e0b\u3052\u3066\u308b..","d1028001":"# comment_count, dislike, like\u306b\u6ce8\u76ee\u3057\u3066\u7279\u5fb4\u91cf\u3092\u4f5c\u308b","0e33b721":"# Music\u306b\u6ce8\u76ee","53fea23f":"### lgb\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1","1dbb876b":"# \u30c7\u30fc\u30bf\u7d50\u5408","adfc7c97":"# \u4e3b\u6210\u5206\u5206\u6790(PCA, UMAP)\u3092\u4f7f\u3063\u3066\u53ef\u8996\u5316","92d4bfc7":"### xgb\u30e2\u30c7\u30eb\u306e\u4f5c\u6210\u3068\u8a55\u4fa1","2a3be84a":"# \u53c2\u8003\u30ce\u30fc\u30c8\u306b\u3042\u308b\u901a\u308a\u306b\u7279\u5fb4\u91cf\u3092\u52a0\u3048\u308b","fab79540":"# 3\u500b\u306b\u5206\u985e\u3059\u308b(\u6539\u826f\u306e\u4e88\u77e5\u3042\u308a,,","c3312564":"# \u6b20\u640d\u5730\u3068\u30c7\u30fc\u30bf\u578b\u3092\u78ba\u8a8d\u3059\u308b","7cd36d33":"### lgb\u306e\u30e2\u30c7\u30eb\u4f5c\u6210","78bea4a9":"### \u30d1\u30e9\u30e1\u30fc\u30bf\u3068\u8a55\u4fa1\u95a2\u6570\u306e\u8a2d\u5b9a","959dd5eb":"# \u4e0a\u7d1a\u30e2\u30c7\u30eb","7af860ef":"# \"categoryId\", \"channelTitle\"\u306e\u51fa\u73fe\u983b\u5ea6","3dfac8e1":"# Title\u3082\u540c\u3058","bc1b3da4":"# MiniBatchKMeans\u3092\u4f7f\u3046","35a50e0b":"### lgb","f824b887":"# PublishedAt\u3068collection_date\u306e\u4e8c\u3064\u306e\u65e5\u4ed8\u30c7\u30fc\u30bf\u306b\u6ce8\u76ee","42851333":"# collection_date\u306b\u6ce8\u76ee","c0211e1b":"# tag\u306b\u6ce8\u76ee\u3059\u308b","6ed39336":"# # \u76ee\u7684\u5909\u6570\u306e\u5916\u308c\u5024\u3092\u524a\u9664(\u7cbe\u5ea6\u3092\u4e0b\u3052\u3066\u305f...","5e68cc2f":"# description\u306b\u6ce8\u76ee","ef26fc3b":"# CM\u306e\u5834\u5408","75eb2c22":"### xgb_lgb_RF","ebf8d971":"# \u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8 ","4ce6628f":"# \u6700\u5f8c\u306b\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u6b20\u640d\u5730\u3092\u5e73\u5747\u5024\u3067\u57cb\u3081\u3066\u304b\u3089\u8a55\u4fa1","dcf2ce68":"# \u5168\u3066\u306e\u8a55\u4fa1\u3092\u307e\u3068\u3081\u3066\u304a\u304f"}}