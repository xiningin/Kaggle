{"cell_type":{"48cb0c81":"code","0e567543":"code","7f16368f":"code","785713d6":"code","ff17bbf3":"code","8da08dc5":"code","db8adaec":"code","1595b78b":"code","ea2ca12e":"code","e1def8bc":"code","fae8b5b2":"code","95e701c9":"code","22d1070a":"code","c56cfd35":"code","b4ab289e":"code","b44a0d73":"code","170d1428":"code","1c28fca5":"code","6f4d7a6d":"code","f60cf250":"code","01fcf3ec":"code","0f43bf8d":"code","68579e16":"code","1abf3292":"code","ec218128":"code","883fc849":"code","eec30366":"code","df479d35":"code","0a20a294":"code","16b9f4ad":"code","22f9e1a4":"code","4c97a45e":"code","64489abe":"code","6494e51c":"markdown","209ed8b2":"markdown","6bb65bd7":"markdown","b95d7d99":"markdown","4fa5e170":"markdown","70e28a64":"markdown","7782738d":"markdown","a37e4c55":"markdown","a79edb07":"markdown","ef850fdd":"markdown","0c5e50ae":"markdown","df02e2b6":"markdown","122728df":"markdown","c89532b7":"markdown","65fa3718":"markdown","90eb3e32":"markdown","6744a088":"markdown","f434a2ec":"markdown","e3b04fcc":"markdown","912fb268":"markdown","e192b9a5":"markdown"},"source":{"48cb0c81":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0e567543":"import itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import NullFormatter\nimport pandas as pd\nimport numpy as np\nimport matplotlib.ticker as ticker\nimport seaborn as sns\nfrom sklearn import preprocessing\n%matplotlib inline","7f16368f":"df=pd.read_csv(\"..\/input\/prediction-of-loan-status\/data.csv\")\ndf.head()","785713d6":"df.shape","ff17bbf3":"df['due_date'] = pd.to_datetime(df['due_date'])\ndf['effective_date'] = pd.to_datetime(df['effective_date'])\ndf.head()","8da08dc5":"df['loan_status'].value_counts()","db8adaec":"bins = np.linspace(df.Principal.min(), df.Principal.max(), 10)\ng = sns.FacetGrid(df, col=\"Gender\", hue=\"loan_status\", palette=\"Set1\", col_wrap=2)\ng.map(plt.hist, 'Principal', bins=bins, ec=\"k\")\n\ng.axes[-1].legend()\nplt.show()","1595b78b":"\nbins = np.linspace(df.age.min(), df.age.max(), 10)\ng = sns.FacetGrid(df, col=\"Gender\", hue=\"loan_status\", palette=\"Set1\", col_wrap=2)\ng.map(plt.hist, 'age', bins=bins, ec=\"k\")\n\ng.axes[-1].legend()\nplt.show()","ea2ca12e":"\ndf['dayofweek'] = df['effective_date'].dt.dayofweek\nbins = np.linspace(df.dayofweek.min(), df.dayofweek.max(), 10)\ng = sns.FacetGrid(df, col=\"Gender\", hue=\"loan_status\", palette=\"Set1\", col_wrap=2)\ng.map(plt.hist, 'dayofweek', bins=bins, ec=\"k\")\ng.axes[-1].legend()\nplt.show()","e1def8bc":"df['weekend'] = df['dayofweek'].apply(lambda x: 1 if (x>3)  else 0)\ndf.head()","fae8b5b2":"df.groupby(['Gender'])['loan_status'].value_counts(normalize=True)","95e701c9":"df['Gender'].replace(to_replace=['male','female'], value=[0,1],inplace=True)\ndf.head()","22d1070a":"df.groupby(['education'])['loan_status'].value_counts(normalize=True)","c56cfd35":"\ndf[['Principal','terms','age','Gender','education']].head()","b4ab289e":"Feature = df[['Principal','terms','age','Gender','weekend']]\nFeature = pd.concat([Feature,pd.get_dummies(df['education'])], axis=1)\nFeature.drop(['Master or Above'], axis = 1,inplace=True)\nFeature.head()","b44a0d73":"\nX = Feature\nX[0:5]","170d1428":"y = df['loan_status'].values\ny[0:5]","1c28fca5":"\nX= preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]","6f4d7a6d":"from sklearn.model_selection import train_test_split\nX_trainset, X_testset, y_trainset, y_testset = train_test_split( X, y, test_size=0.2, random_state=4)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score\n\nfrom sklearn import metrics\n\nk_acc= []\n\nk = 1\nfor k in range(1, 20):\n   \n    neigh = KNeighborsClassifier(n_neighbors = k).fit(X_trainset,y_trainset)\n\n    yhat = neigh.predict(X_testset)\n  \n    print(\"Test set Accuracy: \", metrics.accuracy_score(y_testset, yhat), \" - k =\", k)\n    k_acc.append(metrics.accuracy_score(y_testset, yhat))\n    \nplt.plot(k_acc)\nplt.xlabel('k')\nplt.ylabel('accuracy')\nplt.show()","f60cf250":"# F1 SCORE AND JACCARD SCORE FOR KNN WITH 7 NEIGBOURS\nfrom sklearn.metrics import f1_score\n# from sklearn.metrics import jaccard_similarity_score\nfrom sklearn.metrics import log_loss\n\nneigh = KNeighborsClassifier(n_neighbors = 7).fit(X_trainset,y_trainset)\nknn_pred=neigh.predict(X_testset)\nprint(\"F1-SCORE:: \",f1_score( y_testset, knn_pred, average='weighted'))\nprint(\"Logloss score: \", log_loss(y_testset,neigh.predict_proba(X_testset)))","01fcf3ec":"# Import \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nX_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=0.2, random_state=4)\n# Print the shape of X_trainset and y_trainset. Ensure that the dimensions match\nprint(\"X-trainset-shape:: \", X_trainset.shape)\nprint(\"Y-trainset-shape:: \", y_trainset.shape)\nloanTree = DecisionTreeClassifier(criterion=\"entropy\",max_depth=6)\nloanTree # SHOWS DEFAULT PARAMS","0f43bf8d":"\nloanTree.fit(X_trainset,y_trainset)","68579e16":"predTree = loanTree.predict(X_testset)\n# Lets see the Output of 5 results\nprint (\"PREDICTED -- >\", predTree [0:10])\nprint (\"ACTUAL --> \", y_testset [0:10])","1abf3292":"\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score\n\nprint(\"DecisionTrees's Accuracy: \", metrics.accuracy_score(y_testset, predTree))\n\nprint(\"F1_score: \",f1_score(y_testset, predTree,average='weighted'))","ec218128":"from sklearn import svm\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nX_trainset, X_testset, y_trainset, y_testset = train_test_split( X, y, test_size=0.2, random_state=4)","883fc849":"clf = svm.SVC(kernel='linear')\nclf.fit(X_trainset, y_trainset) \nyhat = clf.predict(X_testset)\nprint(\"SVM linear kernel: accuracy \", metrics.accuracy_score(y_testset, yhat), \" \\n F1: \", f1_score(y_testset, yhat, average='weighted'),  )","eec30366":"\nclf = svm.SVC(kernel='poly')\nclf.fit(X_trainset, y_trainset) \nyhat = clf.predict(X_testset)\nprint(\"SVM polynomial kernel: accuracy \", metrics.accuracy_score(y_testset, yhat), \" \\nF1: \", f1_score(y_testset, yhat, average='weighted'))","df479d35":"\nclf = svm.SVC(kernel='rbf')\nclf.fit(X_trainset, y_trainset) \nyhat = clf.predict(X_testset)\nprint(\"SVM rbf kernel: accuracy \", metrics.accuracy_score(y_testset, yhat), \" \\nF1: \", f1_score(y_testset, yhat, average='weighted'))","0a20a294":"\nclf = svm.SVC(kernel='sigmoid')\nclf.fit(X_trainset, y_trainset) \nyhat = clf.predict(X_testset)\nprint(\"SVM sigmoid kernel: accuracy \", metrics.accuracy_score(y_testset, yhat), \" \\nF1: \", f1_score(y_testset, yhat, average='weighted'))","16b9f4ad":"from sklearn.linear_model import LogisticRegression\nLR = LogisticRegression(C=0.01, solver='newton-cg').fit(X_trainset,y_trainset)\nyhat_lr = LR.predict(X_testset)\nprint(\"LR newton-cg solver: accuracy \", metrics.accuracy_score(y_testset, yhat_lr), \" \\nF1: \", f1_score(y_testset, yhat_lr, average='weighted'))\nprint(\"Logloss score: \", log_loss(y_testset,LR.predict_proba(X_testset)))","22f9e1a4":"LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_trainset,y_trainset)\nyhat_lr = LR.predict(X_testset)\nprint(\"LR liblinear solver: accuracy \", metrics.accuracy_score(y_testset, yhat_lr), \" \\nF1: \", f1_score(y_testset, yhat_lr, average='weighted'))\nprint(\"Logloss score: \", log_loss(y_testset,LR.predict_proba(X_testset)))","4c97a45e":"LR = LogisticRegression(C=0.01, solver='saga').fit(X_trainset,y_trainset)\nyhat_lr = LR.predict(X_testset)\nprint(\"LR saga solver: accuracy \", metrics.accuracy_score(y_testset, yhat_lr), \" \\nF1: \", f1_score(y_testset, yhat_lr, average='weighted'))\nprint(\"Logloss score: \", log_loss(y_testset,LR.predict_proba(X_testset)))","64489abe":"\ntable = [[\"KNN\",0.7765,0.4671],[\"Decision Tree\",0.6466,'NA'],\n...          [\"SVM\",0.6914,'NA'],[\"LogisticRegression\",0.6914,0.4920]]\n\nfrom tabulate import tabulate\nfrom IPython.display import HTML\nHTML(tabulate(table, headers= ['Algorithm', 'F1-score','LogLoss'], tablefmt='html'))","6494e51c":"# Covert to data time object","209ed8b2":"RBF----->","6bb65bd7":"Linear--->","b95d7d99":"# Pre-processing:Features Selection\/extraction\nLets look at the day of the week people get the loan","4fa5e170":"Newton-cg solver--->","70e28a64":"# REPORT","7782738d":"**K Nearest Neighbor(KNN)**","a37e4c55":"# One Hot Encoding","a79edb07":"# Normalize Data","ef850fdd":"\nSaga solver----->","0c5e50ae":"Sigmoid--->\n","df02e2b6":"# Convert Categorical features to numerical values","122728df":"# Logistic Regression","c89532b7":"# Feature selection","65fa3718":"**Support Vector Machine**","90eb3e32":"**Decision Tree**","6744a088":"Polynomial---->","f434a2ec":"# Model Training","e3b04fcc":"# Data Visualization and Preprocessing","912fb268":"Liblinear solver---->","e192b9a5":"From here,you can see best test accuracy is with k=7 or 8.\n\nLet k=7"}}