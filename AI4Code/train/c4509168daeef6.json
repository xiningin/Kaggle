{"cell_type":{"1e9652da":"code","4583cdcd":"code","d21f0649":"code","fbb00290":"code","586c7cd0":"code","25dc1160":"code","05abc74e":"code","2139c4ca":"code","93362cd3":"code","dff53e38":"code","ad243678":"code","90d17c8f":"code","09ebbae0":"code","a3345469":"code","6f39078b":"code","934c5a5b":"code","31f816c9":"code","2cdb5219":"code","3e3c9dde":"code","ebc0907d":"code","1f2fea74":"code","7365b48f":"code","0e35aed5":"code","625a3054":"code","b7585988":"code","9cf1afb9":"code","0fbc11f4":"code","bf667335":"code","112acaf9":"code","16734882":"code","517ab969":"code","c1bcd27f":"code","d4c193b5":"code","66498bac":"code","87eb686b":"code","69dd6012":"code","f06d7c21":"code","eb8d3765":"code","1e21e9e2":"code","e85cbc05":"code","22755e01":"code","cba1a2d8":"code","11eb50c7":"markdown","244e92e2":"markdown","3c8acd0b":"markdown","f3fdf972":"markdown","b9dcdd04":"markdown","32f03f40":"markdown","17503ecf":"markdown","443ffa42":"markdown","8d7faa0b":"markdown","29dc6d43":"markdown","904ec131":"markdown","dcf2e782":"markdown","24b73f5f":"markdown","9a27e22e":"markdown","eb509a2a":"markdown","d88c6bfb":"markdown","24f1effc":"markdown","19dc44bf":"markdown","1dee6f01":"markdown","6f6cdfd0":"markdown","36ef33f5":"markdown","e570174d":"markdown","74a3e613":"markdown","44bbf4cc":"markdown","cd7702e5":"markdown","945af81e":"markdown","1b9580ac":"markdown","c96435d2":"markdown","6c8cc68a":"markdown","d9e97218":"markdown","44c52454":"markdown","f6f16577":"markdown","2ac01bac":"markdown","59f315f1":"markdown","4042c4b2":"markdown","6bd6fe3c":"markdown","5e2455f5":"markdown","34f1e381":"markdown"},"source":{"1e9652da":"import numpy as np\nimport scipy as sp\nimport pandas as pd\n\nimport matplotlib as mpl\n%matplotlib inline\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nimport pylab\n\nimport time\nfrom scipy.stats import pearsonr","4583cdcd":"ch = pd.read_csv('..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\nch.info()\nch.head()","d21f0649":"_ = (ch.groupby('Churn')['customerID'].count()\/ch['customerID'].count()).plot.bar()\n_ = plt.title('Proportion of Customers')\n_ = plt.ylabel('Proportion')\n_ = plt.xlabel('Left (Yes) or Remained (No)')\nprint('Overall Customer Churn percentage in the given dataset is {} %'.format(round(ch.Churn.replace({'No':0,'Yes':1}).mean()*100,2)))","fbb00290":"# Examine the rows with total charges blank\nch[ch['TotalCharges'] == ' ']","586c7cd0":"ch[ch['tenure'] == 0]","25dc1160":"ch.TotalCharges = ch.TotalCharges.apply(lambda x: 0  if x == ' ' else float(x))","05abc74e":"# Change No internet service to NoInt for brevity\nch[['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']] = ch[['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']].astype(str).replace({'No internet service': 'NoInt'})\nch['MultipleLines'] = ch['MultipleLines'].replace({'No phone service':'NoPh'})","2139c4ca":"# Split the customers into 4 groups of tenures and see their Churn Rate\n_ = plt.figure(figsize=(12,4))\n_ = plt.subplot(1,2,1)\nch['Tenure Group'] = 'Between 2 to 5 Years'\nch.loc[ch['tenure'] >59,'Tenure Group'] = 'More than 5 Years' \nch.loc[ch['tenure'] <= 24,'Tenure Group'] = '1 Year -> 2 Year'\nch.loc[ch['tenure'] <= 12,'Tenure Group'] = 'Less Than 1 Year'\nch['Ch10'] = ch['Churn'].replace({'Yes':1,'No':0})\nch_rate = ch.groupby('Tenure Group')['Ch10'].mean().sort_values(ascending=False)\n(round(100*ch_rate,2)).plot.bar(color='pink')\n\n# Evaluate the Revenue Loss per month\nch['revloss'] = ch['MonthlyCharges']*ch['Ch10']\n_ = plt.ylabel('Churn Percentage')\n_ = plt.title('Percentage Churn Vs Tenures')\n_ = plt.subplot(1,2,2)\nrevenue_group = ch.groupby('Tenure Group')['revloss'].sum().sort_values(ascending=False)\n(round(100*revenue_group\/revenue_group.sum(),2)).plot.bar(color='g')\n_ = plt.ylabel('Loss Percentage')\n_ = plt.title('Percentage Revenue loss\/Month Vs Tenure group')\nprint('Total Revenue Lost\/Month due to Churn: $',int(revenue_group.sum()))","93362cd3":"# Check the stats for numeic types within Churn and No Churn group\nnumvar = ['tenure','MonthlyCharges','TotalCharges']\nround(ch.groupby('Churn')[numvar].describe().T,2)","dff53e38":"# Plot the histogram of the tenure and see if it tells any story!\nfig, ax = plt.subplots(figsize=(14,5))\n_ = plt.subplot(1,2,1)\nsns.distplot(ch.loc[ch.Churn=='No','tenure'],hist=True,color='g',kde=False)\n_ = plt.title('Histogram of tenure values for loyal customers')\n_ = plt.xlabel('Tenure in Months')\n_ = plt.ylabel('People count')\n_ = plt.subplot(1,2,2)\nsns.distplot(ch.loc[ch.Churn=='Yes','tenure'],hist=True,kde=False)\n_ = plt.title('Histogram of tenure values for customers who left')\n_ = plt.xlabel('Tenure in Months')\n_ = plt.ylabel('People count')\nprint(\"Mean Tenure of Two groups\\n\",round(ch.groupby('Churn').tenure.mean(),2))\n_ = plt.figure()\n_ = ch[['Churn','tenure']].boxplot(by='Churn')","ad243678":"# Find the correlation between tenure*Monthly Vs TotalCharges\nprint(\"Correlation between Monthly*tenure Vs. Total Charges:\",pearsonr(ch.tenure*ch.MonthlyCharges,ch.TotalCharges))","90d17c8f":"ch['Temp'] = ch.tenure*ch.MonthlyCharges\nlm = ols('TotalCharges ~ Temp',ch).fit()\nlm.summary()","09ebbae0":"ch.drop(['Temp'],axis=1,inplace=True)","a3345469":"_ = pd.crosstab(ch.Contract,ch.Churn).plot.bar()\n_ = plt.title('Churn Count for Contract')\n_ = plt.ylabel('Churn\/No Churn Counts')\nprint('Mean Churn Across',ch.groupby('Contract')['Ch10'].mean())\n_ = pd.crosstab(ch.PhoneService,ch.Churn).plot.bar(color='cb')\n_ = plt.title('Churn Count for Phone Service')\n_ = plt.ylabel('Churn\/No Churn Counts')\n_ = pd.crosstab(ch.InternetService,ch.Churn).plot.bar(color='mr')\n_ = plt.title('Churn Count for Internet Service')\n_ = plt.ylabel('Churn\/No Churn Counts')\nprint('Mean Churn Across',ch.groupby('PhoneService')['Ch10'].mean())\nprint('Mean Churn Across',ch.groupby('InternetService')['Ch10'].mean())","6f39078b":"# Phone service is redundant.\nprint(\"Multiple Lines category counts:\\n\",ch.MultipleLines.value_counts())\nprint(\"Phone Lines category counts:\\n\",ch.PhoneService.value_counts())","934c5a5b":"y = ch.Ch10\nX = ch.drop(['customerID','Churn','Ch10','TotalCharges','PhoneService','Tenure Group','revloss'],axis=1,inplace=False).copy()\ntemp = ch[['tenure','MonthlyCharges','SeniorCitizen']]\nX.drop(['tenure','MonthlyCharges','SeniorCitizen'],axis=1,inplace=True)\nX = X.apply(lambda x: x.astype('category')).apply(lambda x: x.cat.codes)\nX[['tenure','MonthlyCharges','SeniorCitizen']] = temp\nX1 = X.copy() # Saving a copy","31f816c9":"# We will reduce all features to 2D by PCA.\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()  # Essential to see the effect of all\nX = sc.fit_transform(X)\nfig = plt.figure(figsize=(12,6))\npca = PCA()\nxx = pca.fit_transform(X)\nxs = xx[:,0]\nys = xx[:,1]\nfig.add_subplot(1,2,1)\n_ = plt.scatter(xs,ys,c=y)\n_ = plt.title('PCA analysis result by removing Total Charges')\n_ = plt.xlabel(\"PCA x component\")\n_ = plt.ylabel(\"PCA y component\")\nfig.add_subplot(1,2,2)\n_ = plt.bar(np.arange(pca.n_components_),100*np.round(pca.explained_variance_ratio_,4),color='m')\n_ = plt.xlabel(\"PCA Feature number\")\n_ = plt.ylabel(\"PCA Variance % \")\n_ = plt.title('Variance using PCA')\nprint(\"Percentage Variance by removing TotalCharges:\",100*np.round(pca.explained_variance_ratio_,4))","2cdb5219":"X1.drop(['MonthlyCharges'],axis=1,inplace=True)\nfig = plt.figure(figsize=(12,6))\nsc = StandardScaler()\nX1 = sc.fit_transform(X1)\nxx = pca.fit_transform(X1)\nxs = xx[:,0]\nys = xx[:,1]\nfig.add_subplot(1,2,1)\n_ = plt.scatter(xs,ys,c=y)\n_ = plt.title('PCA analysis result by dropping monthly charges')\n_ = plt.xlabel(\"PCA x component\")\n_ = plt.ylabel(\"PCA y component\")\nfig.add_subplot(1,2,2)\n_ = plt.bar(np.arange(pca.n_components_),100*np.round(pca.explained_variance_ratio_,4),color='m')\n_ = plt.xlabel(\"PCA Feature number\")\n_ = plt.ylabel(\"PCA Variance %\")\n_ = plt.title('Variance using PCA')\nprint(\"Percentage Variance by tenure and monthly charges:\",100*np.round(pca.explained_variance_ratio_,6))","3e3c9dde":"# Import KMeans Model\nfrom sklearn.cluster import KMeans\n\n# Graph and create 3 clusters of Customer Churn\nkmeans = KMeans(n_clusters=3,random_state=2)\nkmeans.fit(ch[ch.Churn=='Yes'][[\"tenure\",\"MonthlyCharges\"]])\n\nkmeans_colors = ['green' if c == 0 else 'blue' if c == 2 else 'red' for c in kmeans.labels_]\nfig, ax = plt.subplots(figsize=(10, 8))\nplt.subplot(2,1,1) #figsize=(10, 6))\nplt.scatter(x=\"tenure\",y=\"MonthlyCharges\", data=ch[ch.Churn=='Yes'],\n            alpha=0.25,color = kmeans_colors)\nplt.xlabel(\"Tenure in months \")\nplt.ylabel(\"Monthly Charges in Dollars\")\nplt.scatter(x=kmeans.cluster_centers_[:,0],y=kmeans.cluster_centers_[:,1],color=\"black\",marker=\"X\",s=100)\nplt.title(\"Clusters of Customers who switch\")\nprint(\"Cluster Centers for loyal customers are at:\")\nprint(\"Month, Dollars, Numbers\")\nprint(np.round(kmeans.cluster_centers_[0,:],2),(kmeans.labels_==0).sum())\nprint(np.round(kmeans.cluster_centers_[2,:],2),(kmeans.labels_==2).sum())\nprint(np.round(kmeans.cluster_centers_[1,:],2),(kmeans.labels_==1).sum())\n\nplt.subplot(2,1,2)\n\n# Graph and create 3 clusters of Customer Churn\nkmeans = KMeans(n_clusters=3,random_state=2)\nkmeans.fit(ch[ch.Churn=='No'][[\"tenure\",\"MonthlyCharges\"]])\n\nkmeans_colors = ['darkgreen' if c == 0 else 'orange' if c == 2 else 'purple' for c in kmeans.labels_]\n\nplt.scatter(x=\"tenure\",y=\"MonthlyCharges\", data=ch[ch.Churn=='No'],\n            alpha=0.25,color = kmeans_colors)\nplt.xlabel(\"Tenure in months \")\nplt.ylabel(\"Monthly Charges in Dollars\")\nplt.scatter(x=kmeans.cluster_centers_[:,0],y=kmeans.cluster_centers_[:,1],color=\"black\",marker=\"X\",s=100)\nplt.title(\"Clusters of Loyal customers\")\nprint(\"Cluster Centers for loyal customers are at:\")\nprint(\"Month  Dollars  Numbers\")\nprint(np.round(kmeans.cluster_centers_[1,:],2),(kmeans.labels_==1).sum())\nprint(np.round(kmeans.cluster_centers_[0,:],2),(kmeans.labels_==0).sum())\nprint(np.round(kmeans.cluster_centers_[2,:],2),(kmeans.labels_==2).sum())\n_ = plt.tight_layout()","ebc0907d":"X = ch.drop(['customerID','Churn','Ch10','TotalCharges','PhoneService','Tenure Group','revloss'],axis=1,inplace=False).copy()","1f2fea74":"cat_vars=['gender','Partner','Dependents','PaperlessBilling','MultipleLines','InternetService','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies','Contract','PaymentMethod']\nfor var in cat_vars:\n    cat_list='var'+'_'+var\n    cat_list = pd.get_dummies(ch[var], prefix=var)\n    X1=X.join(cat_list)\n    X=X1\nX.drop(cat_vars,axis=1,inplace=True) # Originals need to be dropped","7365b48f":"X.columns","0e35aed5":"X.drop(['MultipleLines_NoPh','InternetService_No','OnlineSecurity_No','OnlineBackup_No',\n        'DeviceProtection_No','TechSupport_No','StreamingTV_No','StreamingMovies_No',        \n        'gender_Male','Partner_No','Dependents_No','PaperlessBilling_No',\n        'Contract_Month-to-month','PaymentMethod_Credit card (automatic)'],axis=1,inplace=True)\nX.drop(['StreamingMovies_NoInt','StreamingTV_NoInt','TechSupport_NoInt','DeviceProtection_NoInt','OnlineBackup_NoInt','OnlineSecurity_NoInt'],axis=1,inplace=True)\nXLin = X[[ 'MultipleLines_No', 'MultipleLines_Yes','InternetService_Fiber optic', 'InternetService_DSL',\n         'OnlineSecurity_Yes','OnlineBackup_Yes', 'DeviceProtection_Yes', 'TechSupport_Yes','StreamingTV_Yes', 'StreamingMovies_Yes']]","625a3054":"# Fit Linear Regression for Monthly Charges using services\nfrom sklearn.linear_model import LinearRegression\nLinReg = LinearRegression(fit_intercept=False)\nyLin = ch.MonthlyCharges\nLinReg.fit(XLin,yLin)\npred = LinReg.predict(XLin)\nprint(\"R^2 of the fit:\",np.round(LinReg.score(XLin,yLin),3))\nprint(\"MSE of the model {:.2f}\".format(np.mean((pred - yLin) ** 2)))\nlincoeff = pd.DataFrame(np.round(LinReg.coef_,3),index=XLin.columns,columns=['$ Per month'])\nlincoeff.sort_values('$ Per month',ascending=False).plot.bar(color='orange')\nlincoeff.sort_values('$ Per month',ascending=False)","b7585988":"# Checking for verification of normality\nresid = pred-yLin\n_ = sm.qqplot(resid,line='r')\n_ = plt.title('Quantile Plot')\n_ = plt.figure()\n_ = sns.jointplot(pred,resid,color='r')\n_ = plt.title('Residual Plot')\nprint(\"Percentage of outliers:{:.2f}\".format(100*((abs(resid) > 2.25).sum())*resid.std()\/XLin.shape[0]))\n#(abs(resid) > resid.std()unt()\n#print(\"Indices of outlier points:\",list(np.argsort(abs(pred-ydev)).tail(10)))","9cf1afb9":"all_cust = round((np.sum(XLin,axis=0)*LinReg.coef_),2)\nindex1 = y > 0\nchurn_cust = round((np.sum(XLin.loc[index1,:],axis=0)*LinReg.coef_),2)\njoined = pd.concat([all_cust,churn_cust],axis=1)\njoined.columns = ['All Customers','Churn Customers']\njoined.plot.bar(width = 0.9)\n_ = plt.title('Monthly Income Bar Chart across services')\n_ = plt.ylabel('Monthly Income in Dollars')","0fbc11f4":"# We need to drop a few dummies to prevent correlations, in nonservice specific ones.\n# Plotting correlation for top 10 features\n# Ref : https:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html for colormap\nX.drop('MonthlyCharges',axis=1,inplace=True)\n_ = plt.figure(figsize=(16,12))\nmask = np.zeros_like(X.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(X.corr(),mask=mask,cmap='bwr')","bf667335":"from sklearn.metrics import confusion_matrix, classification_report,accuracy_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree","112acaf9":"# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n# clf - original classifier\n# parameters - grid to search over\n# X - usually your training X matrix\n# y - usually your training y \n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\nfrom sklearn.model_selection import GridSearchCV\n\ndef cv_optimize(clf, parameters, X, y, n_jobs=2, n_folds=5, score_func=None):\n    if score_func:\n        gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, n_jobs=n_jobs, scoring=score_func)\n    else:\n        gs = GridSearchCV(clf, param_grid=parameters, n_jobs=n_jobs, cv=n_folds)\n    gs.fit(X, y)\n    print(\"BEST\", gs.best_params_, gs.best_score_)\n    #print(gs.grid_scores_)\n    best = gs.best_estimator_\n    return best\n#------------------------------------------------------------------------------#\n# Function to plot ROC and find area under ROC                                 #\n#------------------------------------------------------------------------------#\ndef find_auc_score(clf,Xin,yin,color='b',name='LogReg',label=1,prob=1) :\n    '''Function to plot Receiver characteristics and find AUC'''\n    if prob == 1:\n        yscore = clf.predict_proba(Xin)\n    else :\n        yscore = clf.decision_function(Xin)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(yin, yscore[:,label],pos_label=label)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(false_positive_rate, true_positive_rate,color ,label='AUC '+name+' = %0.2f'% roc_auc)\n    plt.legend(loc='lower right')\n    plt.plot([0,1],[0,1],'r--')\n    plt.xlim([-0.01,1.01])\n    plt.ylim([-0.01,1.01])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    return roc_auc\n\n\ndef pre_process_Xy(Xarray,yarray,test_tr_split_size=0.4) :\n    '''Function to split given data into test and (train, dev) set'''\n    Xtr,Xdev,ytr,ydev = train_test_split(Xarray,yarray,test_size=test_tr_split_size,random_state=42,stratify=yarray)\n    return Xtr,Xdev,ytr,ydev\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n# Important parameters\n# indf - Input dataframe\n# featurenames - vector of names of predictors\n# targetname - name of column you want to predict (e.g. 0 or 1, 'M' or 'F', \n#              'yes' or 'no')\n# target1val - particular value you want to have as a 1 in the target\n# mask - boolean vector indicating test set (~mask is training set)\n# reuse_split - dictionary that contains traning and testing dataframes \n#              (we'll use this to test different classifiers on the same \n#              test-train splits)\n# score_func - we've used the accuracy as a way of scoring algorithms but \n#              this can be more general later on\n# n_folds - Number of folds for cross validation ()\n# n_jobs - used for parallelization\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n\ndef plot_train_test_error(clf,X,y,N=50):\n    '''This function plots Train and Test Accuracy for different lengths'''\n\n    training_error = np.empty([N,1])\n    dev_error = np.empty([N,1])\n    len_tr = int(X.shape[0]\/N)\n    re_ind = np.random.permutation(X.index)\n    X = X.reindex(re_ind)\n    y = y.reindex(re_ind)\n    for i in range(N) :\n        X1 = X[:(i+1)*len_tr]\n        y1 = y[:(i+1)*len_tr]\n        Xtr,Xte,ytr,yte = train_test_split(X1,y1,test_size=0.5,random_state=42,stratify=y1)\n        clf = clf.fit(Xtr, ytr)\n        training_error[i,0] = 1 - clf.score(Xtr, ytr)\n        dev_error[i,0] = 1 - clf.score(Xte, yte)\n    \n    plt.plot(np.arange(N)*len_tr,training_error.reshape(np.arange(N).shape),label='train error')\n    plt.plot(np.arange(N)*len_tr,dev_error.reshape(np.arange(N).shape),color='m',label='test error')\n    plt.title('Train Error and Test Error Vs Number of Samples used (train: test 1:1 ratio)')\n    plt.ylabel('Error rate')\n    plt.xlabel('Number of samples')\n    plt.legend(loc='best')\n    return\n    \ndef do_classify(clf, parameters, Xtr,ytr,Xdev,ydev, score_func=None, n_folds=5, n_jobs=2,model_name='LogReg',label=1,prob_dec=1):\n\n    if parameters:\n        clf = cv_optimize(clf, parameters, Xtr, ytr, n_jobs=n_jobs, n_folds=n_folds, score_func=score_func)\n    clf=clf.fit(Xtr, ytr)\n    training_accuracy = clf.score(Xtr, ytr)\n    test_accuracy = clf.score(Xdev, ydev)\n    print(\"############# based on standard predict ################\")\n    print(\"Accuracy on training data: %0.2f\" % (100*training_accuracy)+'%')\n    print(\"Accuracy on test data:     %0.2f\" % (100*test_accuracy)+'%')\n    print(\"confusion_matrix on dev data\")\n    ypred =  clf.predict(Xdev)\n    print(confusion_matrix(ydev,ypred))\n    print(\"classification report on dev data\")\n    print(classification_report(ydev,ypred))\n    print(\"########################################################\")\n  #  multi_auc_roc(clf,Xdev,ydev,prob=1)\n    auc_tr = find_auc_score(clf,Xtr,ytr,color='g',name=model_name+'_tr',label=label,prob=prob_dec) \n    auc_dev = find_auc_score(clf,Xdev,ydev,color='orange',name=model_name+'_dev',label=label,prob=prob_dec) \n    return clf,auc_tr,auc_dev\n","16734882":"%%time\n# Keep a copy to access columns\nXcpy = X.copy()\nX['tenure'] = X['tenure'].transform(lambda x: (x - x.mean()) \/ x.std())\nXtrain, Xdev, ytrain,ydev = train_test_split(X,y,test_size=0.4,stratify=y)\n# This is commented because hyperparameter tuning is not done currently.\n#Xdev, Xtest, ydev,ytest = train_test_split(Xt,yt,test_size=0.5,random_state=42,stratify=yt)\nparameters = {\"C\": [0.1,1,10,100,10000],\"class_weight\":['balanced',None]}\nlogreg,aucrf1,aucrf2 = do_classify(LogisticRegression(), parameters, Xtrain,ytrain,Xdev,ydev, score_func='recall', n_folds=5, n_jobs=2,label=1,prob_dec=1)","517ab969":"coeff=logreg.coef_\nintercept = logreg.intercept_\ncoeffs_b= logreg.coef_[0,np.argsort(abs(logreg.coef_[0,:]))[::-1]]\nnames_b = list(Xcpy.columns[np.argsort(abs(logreg.coef_[0,:]))[::-1]])\nlogfimp = pd.DataFrame(np.round(coeffs_b,3),index=names_b,columns=['Coeff value'])\n_ = logfimp.head(10).plot.bar(color='purple')\n_ = plt.title('Feature Importance (Log Reg)')\n_ = plt.ylabel('Coefficient value')\n_ = plt.xlabel('Features')\nlogfimp","c1bcd27f":"# Trying Feature Selection by limiting to 10\nfrom sklearn.feature_selection import RFE\nmodel = LogisticRegression(class_weight='balanced')\nrfe = RFE(model, 10)\nrfe = rfe.fit(Xtrain, ytrain)\n# After RFE has chosen, now do a prediction using that\nprint(\"Chosen Predictors:\",Xcpy.columns[rfe.support_])\nXp = Xcpy.loc[:,Xcpy.columns[rfe.support_]]\nXp = sc.fit_transform(Xp)\nXtrain1, Xt, ytrain1,yt = train_test_split(Xp,y,test_size=0.4,stratify=y)\nlogreg = LogisticRegression(class_weight='balanced')\nlogreg.fit(Xtrain1,ytrain1)\nyp = logreg.predict(Xt)\nprint(\"Report:\\n\",classification_report(yt,yp))\nprint(\"Dev Set Accuracy %\",np.round(accuracy_score(yt,yp)*100,2))\nprint(\"Train set Accuracy %\",np.round(accuracy_score(ytrain1,logreg.predict(Xtrain1))*100,2))\nyprob = logreg.predict_proba(Xt)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(yt, yprob[:,1],pos_label=1)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n_ = plt.title('Receiver Operating Characteristic')\n_ = plt.plot(false_positive_rate, true_positive_rate, 'b',label='AUC = %0.2f'% roc_auc)\n_ = plt.legend(loc='lower right')\n_ = plt.plot([0,1],[0,1],'r--')\n_ = plt.xlim([-0.01,1.01])\n_ = plt.ylim([-0.01,1.01])\n_ = plt.ylabel('True Positive Rate')\n_ = plt.xlabel('False Positive Rate')","d4c193b5":"# Coefficients with LogReg\nXp = Xcpy.loc[:,Xcpy.columns[rfe.support_]]\ncoeffs = logreg.coef_[0,np.argsort(abs(logreg.coef_[0,:]))[::-1]]\nnames = list(Xp.columns[np.argsort(abs(logreg.coef_[0,:]))[::-1]])\nprint(\"Coefficients and their values in decreasing importance\")\npd.DataFrame(np.round(coeffs,2),index=names,columns=['Coeff value'])","66498bac":"# To evaluate the extent of relation between churn and tenure, trying Logistic with tenure alone.\n# Recognizing that tenure and churn are just correlated, it may not be causation\n# We could verify if the coefficient with this is close to that obtained by the first \n# logistic regression that includes all variables\nlogreg_red = LogisticRegression(class_weight='balanced')\nXtrain, Xdev, ytrain,ydev = train_test_split(np.array(ch['tenure']),y,test_size=0.4,random_state=42,stratify=y)\nlogreg_red.fit(Xtrain.reshape(-1,1),ytrain)\nypred_red = logreg_red.predict_proba(Xdev.reshape(-1,1))\n_ = plt.plot(np.sort(ypred_red[:,1]),label = 'Probability values')\nypred = logreg_red.predict(Xdev.reshape(-1,1))\nypred_s = ypred[np.argsort(ypred_red[:,1])]\nspred = np.sort(ypred_red[:,1])\nvline = spred[ypred_s.argmax()]\nprint(\"Threshold Chosen for classification:\",round(vline,2))\nprint(\"Threshold Tenure:{} months\".format(round(0.997\/0.037)))\nprint(\"Max and Min Prob values:{} and {}\".format(round(ypred_red.max(),2),round(ypred_red.min(),2)))\n_ = plt.axhline(vline,color='k',linestyle='--',label = 'Threshold')\n_ = plt.scatter(np.arange(len(ypred)),ypred_s,color='m',marker='.',label = 'Predictions')\n_ = plt.legend(loc='best')\n_ = plt.xlabel('Test sample index')\n_ = plt.ylabel('Probability values')\n_ = plt.title('Probability Plot of Churn')\nprint(\"Train Set Accuracy :{:.2f}%\".format(100*accuracy_score(ytrain,logreg_red.predict(Xtrain.reshape(-1,1)))))\nprint(\"Dev Set Accuracy {:.2f}%\".format(100*accuracy_score(ydev,ypred)))\nprint(\"Report:\\n\",classification_report(ydev,ypred))\nprint(\"Coefficient:{}, Intercept:{}\".format(round(logreg_red.coef_[0,0],3),round(logreg_red.intercept_[0],3)))","87eb686b":"X_rf = X = ch.drop(['customerID','Churn','Ch10','TotalCharges','PhoneService','Tenure Group','revloss'],axis=1,inplace=False).copy()\ntemp = X_rf[['tenure','MonthlyCharges','SeniorCitizen']]\nX_rf = X_rf.drop(['tenure','MonthlyCharges','SeniorCitizen'],axis=1)\nX_rf = X_rf.apply(lambda x: x.astype('category')).apply(lambda x: x.cat.codes)\nX_rf[['tenure','MonthlyCharges','SeniorCitizen']] = temp\nXtrain, Xdev, ytrain,ydev = train_test_split(X_rf,y,test_size=0.4,stratify=y)","69dd6012":"%%time\nparameters = {\"max_depth\": [3,4,6,8,12], 'min_samples_leaf': [1,2,4,8],\"class_weight\":['balanced',None]}\ntr,aucrf1,aucrf2 = do_classify(DecisionTreeClassifier(), parameters, Xtrain,ytrain,Xdev,ydev, score_func='recall', n_folds=5, n_jobs=2,model_name='DecTree',label=1,prob_dec=1)","f06d7c21":"import graphviz\nfrom sklearn import tree\ndot_data = tree.export_graphviz(tr, out_file='dtree.dot', \n                         feature_names=X_rf.columns,  \n                         class_names=['N','Y'],  \n                         filled=True, rounded=True,  \n                         special_characters=True)\n#graph = graphviz.Source(dot_data)\n","eb8d3765":"%%time\nparameters = {\"max_depth\": [3,4,6,8,12,None], 'min_samples_leaf': [1,2,4,6],\"n_estimators\":[10,50,100,200],\"class_weight\":['balanced',None]}\nrf,aucrf1,aucrf2 = do_classify(RandomForestClassifier(), parameters, Xtrain,ytrain,Xdev,ydev, score_func='recall', n_folds=5, n_jobs=2,model_name='RandomForest',label=1,prob_dec=1)","1e21e9e2":"feature_labels = np.array(list(X_rf.columns))\n(pd.Series(rf.feature_importances_,index=feature_labels).sort_values(ascending=True)\/np.max(rf.feature_importances_)).plot.barh(color='purple',width=0.9)\n_ = plt.title('Normalized Feature Importance From Random Forest Classifier')\n_ = plt.axvline(0.05,linestyle='--',color='olive')\n_ = plt.text(0.05,7,'5% of the max',rotation=87,fontsize=16)\npd.DataFrame(rf.feature_importances_,index=feature_labels,columns=['Feature importance']).sort_values('Feature importance',ascending=False)","e85cbc05":"# Predictive Model to use\n# Assumes clean csv file\ndef cust_churn_prob_finder(coeff,intercept,csvfile):\n    df = pd.read_csv(csvfile,usecols=range(20))\n    df_monthly = df.MonthlyCharges\n    df.drop(['TotalCharges','customerID','PhoneService','MonthlyCharges'],axis=1,inplace=True)\n    cat_vars=['gender','Partner','Dependents','PaperlessBilling','MultipleLines','InternetService','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies','Contract','PaymentMethod']\n    for var in cat_vars:\n        cat_list='var'+'_'+var\n        cat_list = pd.get_dummies(ch[var], prefix=var)\n        X1=df.join(cat_list)\n        df=X1\n    df.drop(cat_vars,axis=1,inplace=True) # Originals need to be dropped\n    df.drop(['MultipleLines_NoPh','InternetService_No','OnlineSecurity_No','OnlineBackup_No',\n        'DeviceProtection_No','TechSupport_No','StreamingTV_No','StreamingMovies_No',        \n        'gender_Male','Partner_No','Dependents_No','PaperlessBilling_No',\n        'Contract_Month-to-month','PaymentMethod_Credit card (automatic)'],axis=1,inplace=True)\n    df.drop(['StreamingMovies_NoInt','StreamingTV_NoInt','TechSupport_NoInt','DeviceProtection_NoInt','OnlineBackup_NoInt','OnlineSecurity_NoInt'],axis=1,inplace=True)\n    df['tenure'] = df['tenure'].transform(lambda x: (x- x.mean())\/x.std())\n    prob_test = 1\/ (1+ np.exp(-np.dot(np.array(df),coeff[0,:])-intercept))\n    df['churn_prob'] = prob_test \n    df['charge*prob'] = df_monthly* prob_test \n    return df","22755e01":"df = cust_churn_prob_finder(coeff,intercept,'..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\nprint(\"Report\",classification_report(y,(df['churn_prob'] >= 0.5)))\nprint(\"Confusions\\n\",confusion_matrix(y,(df['churn_prob'] >= 0.5)))","cba1a2d8":"df_churn = df[df['churn_prob'] >= 0.5]\nindex1 = df_churn.sort_values('charge*prob',ascending=False).index\ntemp = y*(df['churn_prob'] >= 0.5)\nindex2 = ch.loc[(temp > 0),'MonthlyCharges'].index\nprint(\"Potential revenue savings:${}\".format(round(ch.loc[index2,'MonthlyCharges'].head(1485).sum())))","11eb50c7":"**Remarks:**\n+ The Model is a Good fit as shown by R^2 and MSE\n+ The monthly charges are proportional to services taken, no fixed charges.  \n+ Fiber optic internet is the most expensive service at \\$50, the double of DSL. \n+ Streaming Sevices are priced around \\$9.94, Other internet related services are around \\$\n+ A single phone line costs \\$20, an additional line costs \\$5\n+ We will next examine residuals just to make sure the assumptions hold true and hence results are reliable","244e92e2":"In the Real Scenario, Data Scientist can ask questions to the business entity on how the customers are charged. Here we need to find out the relation between services and monthly Charges. We could try fitting a Linear Regression Curve. The customers are assumed to be independent. There are enough number of samples. We can plot the residuals later and check for normality. If it is found to be normal, with number of outliers(points beyond 2 stds) < 5% and if it explains 99% of the variance (R2 measure), we will accept it.\n\nOur objective at this point is not prediction. It is to evaluate the coefficients of Linear regression, which tell us how much each service costs. This can later be used to quantify the gain in monthly revenue if some actions are taken which would result in retaining a customer.\n\nSince it is not a predictive Linear Regression, but an inference one, we need not split it into Test and train set. There is no need to standardize any variable too.\nWhen the first model was run, intercept came out to be < 10 cents. Hence ran this with intercept False","3c8acd0b":"### 8) A Final Note:\n\nIt was fun doing this small project. Made me understand what more I need to work on. In the process I learnt,\n+ Data Science Process\n+ To Apply Machine Learning basics studied in courses and text books. \n+ Metrics to measure the goodness of the model. Learnt that accuracy is definitely not a good enough measure in case of imbalanced classes. class_weight argument works magically by penalizing the mistakes in minority class more!\n\nThanks for taking time to go through this project. Feedback is highly appreciated. Please email [me](mailto:aparnack@gmail.com) to discuss your views","f3fdf972":"**Remark**: The Mean Rates printed above the plots only give some idea on the churn rate and that can give skewed perception on importance of set of people in a category.\n\nThe plots serve two purposes. They give counts and the relative counts of Churn Vs No churn. Across the categories, they tell us, which category has more members. This is helpful in deciding a few things. Let me explain with an example of Contract.\n\nThe month to month contract has high number of people in general and higher proportion of people Churning. The Contract with 2 years has the least members across categories and also within the category, relatively smaller proportion of the people Churn.\n\nSimilar Statements can be made about the other two plots. However, they do not give us an integrated picture of which combinations lead to certain customers' dissatisfaction. Hence we need a model to derive the relationship across them.","b9dcdd04":"### 6) Recommendations to retain the customers:\nA predictive model is given that ranks customers based on their probability of churn and the revenue that they bring.\n+ Use this model to prioritize whose concerns to be addressed first. Sometimes it might be case by case basis.\n+ Take the following actions immediately: \n  - Try striking a longer contract with new customers: two year or one year in that order of preference.\n  - Leverage the time to improve the quality of services, on the high cost ones like Fiber optic.\n  - Improve on the Technical support on all services like streaming, phone connection and internet. \nBe up-to-date with current technology.\n  - Collect customer feedback and act on it immediately to prevent new customer churn\n+ Next: It will be helpful to understand why churn started 5.5 years ago. Give more historical data to the data scientist for analysis.\n","32f03f40":"Even with tenure alone it gives about 64% overall accuracy, a mere 11% reduction compared to all features taken for the fitting.\n+ The coefficient value -0.037 is close to -0.030 obtained with the regression with all other variables included. This gives additional confidence.\n+ Churn is closely negatively correled with tenure.\n+ The curve ranges between 0.16 to 0.84, not between 0 to 1\n+ The threshold of 0.5 corresponds to tenure = 27 months => contract of two year is preferred.\n+ It should be noted that the sigmoid curve is almost linear between 0.66 and 0.23, corresponding to tenures 9 months and 60 months respectively. \n+ Before 9 months the curve is flat, implying, it takes lot of effort to make them subscribed for longer than a few months.\n+ After 60 months it is again flat at the other end, implying these customers have stabilized\n","17503ecf":"**Remark**: The PCA indicates that there are two dominant components which explain all the variance! My guess would be tenure and MonthlyCharges. But Monthly charges may be highly correlated with services, hence we will remove that and try again.","443ffa42":"### K-Means Clustering:\n\nClustering is done on tenure and Monthly Charges to get intuitive feel for the customer groups behavior. Churn population and non-Churn population are separately clustered into 3 and compared.","8d7faa0b":"### Decision Tree Classifier\n","29dc6d43":"### Random Forest Classifier\nThere is actually no need to drop correlated features for Random Forest. It selects best features at every node of every tree by itself. But from a business perspective, TotalCharges is related to monthly and tenure, we intentionally drop it to get the correct picture (better interpretation) on feature importance. Phone service is also dropped.","904ec131":"**Interpretation:** The negative coefficients indicate that customers with higher magnitude for negative coefficient tend to remain loyal and higher for positive coefficients indicate the opposite. ","dcf2e782":"### 7) Scope for future work:\n+ More predictive models could be tried, however, there is no guarantee of better accuracy, as we have seen similar accuracy witn logistic regression and random forest. This actually means most of the variance in the data is explained.\n+ One could collect more data through surveys, analyze them using NLP techniques and take more measures.\n+ There is a scope to collect historical data on company customers over a few decades, and fight out clear reason for customer drop happened 70 years ago.","24b73f5f":"#### Data Preparation\nThe remaining part does not need one hot encoding, and also categories need to be handled again.","9a27e22e":"Wow! That is good, except for a couple of strong squares. These will be okay.","eb509a2a":"Note that,\n\nTarget (Dependent) Variable: **Churn**\n\nFeature (Independent) Variables: 19 of them out of 21 shown above. Churn is target, Customer ID is just unique identity.\n\n**We can divide predictor variables into,**\n+ Service specific : \n  - Phone: PhoneService, MultipleLines\n  - Internet: InternetService, Online Security, Online backup, Streaming TV, Streaming Movies, Tech support, Device protection\n+ Person specific : gender, SeniorCitizen,Partner, Dependents, tenure (loyalty)\n+ Money specific: Monthly Charges, TotalCharges, Contract, Paperlessbilling, Payment Method\n\nIn a real scenario, we would get additional information from the business owner on relationship between charges and remaining factors. Still we will have to verify them from the data, because their assumptions on how they run business and the reality can differ. Hence,\n\n**The Questions to which we seek answers:**\n- Are monthly charges different for different contract types?\n- Are monthly charges solely dependent on the number\/type of the services?\n- Are there any discounts given to loyal customers?\n- Is there a correlation between monthly charges and churn?\n- Is there a correlation between tenure and churn?\n- Is there a correlation between certain type of services and churn?\n- Is there any person specific trends in churn?\n- Can we make predictions on likelihood of a customer churn given the predictor variables listed above\n\nand, Try quantifying these correlations into actionable items.\n\nFirst check what percentage of the given dataset has data for customers who switch and what percentage is loyal?","d88c6bfb":"### Logistc Regression Classifier\n\nThere are two numerical variables, and they don't need standardization. Result in terms of performance is found to be same with and without. Hence not doing.","24f1effc":"**Remarks: ** There is a clear distinction in the shape of the two histograms above. The percentiles are visible in the box plot. \n+ Customers who cut the contract are highly concentrated towards lesser tenure (<= 6 months). The number of customers in each higher bin progressively reduces.!\n+ The first peak in the first plot tells that there are many more new customers than those in each of the other bins except the last one.\n+ First bin to second bin, there is huge reduction, as some have Churned. There is slight reduction until bin centered at 40.\n+ Customers who have crossed 20 months are likely to remain loyal and hence the counts are almost same until bin centered 60.\n+ The last peak is a fact about the current dataset. It seems like there are many old customers (count > 850) who have stayed with the company's connection for more than 66 months. Suddenly there was a drop in number of customers and that is why there is less count in the bin previous to that. This clearly means, **Many customers switched to other companies about 5.5 years ago.** There could be a number of reasons for this:\n   - This business entity might not have been able to upgrade its services to state of the art technology. \n   - There was monopoly but suddenly a competitor company popped up and attracted its customers with inauguration offers.\n   - The progressive decrease from bin 65 to bin 60 to bin 55 and so on support the above points.\n   - We can not do further analysis on this currently, as there is no data for customers who left 5.5 years ago. We only know the number of such customers.","19dc44bf":"# **Customer Retention**\n *A Data Science Project By Aparna Shasty*","1dee6f01":"26.54% or 1869 of the 7043 records in the dataset belong to customers who switched to a competitor. This is a binary classification problem with moderately  imbalanced dataset. ","6f6cdfd0":"**Remarks: ** \nSeparability seems to have reduced, when monthly charges is removed.","36ef33f5":"**Remarks:** The model is having similar accuracy on dev and train set. Since the class weight is set to balanced, the model has adjusted for imbalance by sampling methods. In logistic regression model, one can not do better than this, once the training and dev set accuracy is equal. Ref: The book, Introduction to Statistical Learning, Chapter 2","e570174d":"All other variables are fine. Converting object data types to categories and doing one hot encoding are explained along the way. \n### **3) Exploratory Data Analysis, Data Story Telling**\n\nNow that we have clean data, let us quantify what impact this project could make, if all the customers who go away can be convinced to staying. We will also group tenure into 4 categories for better comprehension.","74a3e613":"### 5) Comparing The Results, Feature Importance: \nLogistic Regression is more interpretable, faster, gives better overall accuracy than Random Forest, with slight degradation in recall of Churn group.","44bbf4cc":"#### **Preparing the data for Predictive Analysis, by one-hot encoding**\nCreating one hot encoding to suit Linear\/Logistic regression is important for the correct interpretation of the features by the model. We do that below, and then drop original columns and the last column of each variable, as it is correlated to the others.","cd7702e5":"**Remark:**From the above, Phone service is a subset of multiple lines. It does not contain any new information. Hence we choose to omit it. \n\nWe can not rule out the possibility that the digits in customerID has some kind of information encoded. It can be taken up as future work. At this time, for ease of analysis, let us drop it.","945af81e":"TotalCharges should be removed, as it is just monthly charges accumulated till date, as shown by the regression model above.","1b9580ac":"### 9) References:\n\n+ Introduction to Statistical Learning Book by Gareth James et. al\n+ Machine Learning Course by Andrew Ng in Coursera\n+ How to plot ROC is [here](https:\/\/datamize.wordpress.com\/2015\/01\/24\/how-to-plot-a-roc-curve-in-scikit-learn\/)\n+ Colormap codes for heatmap [here](https:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html for colormap)\n+ [Here](https:\/\/towardsdatascience.com\/predict-customer-churn-with-r-9e62357d47b4) is another implementation of this problem by Susan Li in Towards Data Science blog","c96435d2":"+ In the churn group (first plot), blue cluster has the maximum count. The number is more than sum of other two clusters. Indicates that relatively new customers but those who have subscribed to more services are the ones who are more likely to leave.","6c8cc68a":"**Remarks**: Performance with RFE is slightly degraded. That is expected as only 10 predictors are used as opposed to double of that earlier.","d9e97218":"+ Mean tenure for Churn group is lower than the other group, which is as expected.\n+ Mean Monthly charges are higher for Churn group, this has to be analyzed further.\n+ Mean Total Charges is higher for loyal group than the churn group.","44c52454":"**Scikit-learn's RFE**: Scikit Learn has a nice package called Recursive feature elimination (RFE). Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of RFE is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.","f6f16577":"## Overview\n\n+ Recognizing\/defining the business problem\n+ Data Wrangling\n+ Exploratory Data Analysis (EDA) and Visualizations\n+ Data Storytelling\n+ Training and Testing Machine Learning models: Expect to see a few rarely discussed concepts like Recursive Feature Selection\n+ Recommendations to retain the customers\n+ Scope for future work\n+ A Final Note\n+ References\n\n### 1) Business Problem\n\nA telecom company has been affected by the increasing number of customers subscribing to the services of a competitor. It is much more expensive to attract new customer than retaining old customer. At the same time, spending too much on or spending on the wrong factor for retaining customer who has no intention to leave (or who was not leaving for that factor which was addressed) could be a waste of  money. Therefore it is important to identify the customer who has high probability of leaving and zero down on the reason for it. An analysis of the past records of the customers can give great insights on who might leave and what is the cause. The telecom company already has this data available and data scientist need not collect the data in this case. The data can be found in [IBM page](https:\/\/www.ibm.com\/communities\/analytics\/watson-analytics-blog\/guide-to-sample-datasets\/) : [Telecom Dataset]('https:\/\/community.watsonanalytics.com\/wp-content\/uploads\/2015\/03\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n\n### Step by Step approach to Predicting and Preventing Customer Churn Rate\n#### Quick Examination of the Dataset","2ac01bac":"tenure for all these 11 rows is 0. Churn is \"No\". One can interpret this as belonging to all new customers, if indeed these are the only rows with tenure = 0 too. This is verified to be true below. Hence we can set TotalCharges to 0, whenever tenure is 0.","59f315f1":"**Remarks**: The feature importance is not really matching with and without RFE. RFE is an overkill for problems with small number of features.","4042c4b2":"### 2) Data Wrangling\nThe output of the cell number 2 above gives all the data types and counts of non-null entries. Non-null does not mean valid entries. We need to ensure all of them have meaningful datatypes and valid entries. The TotalCharges is of type object, which means there is some non-numeric entry. It is expected to be float. Look at non-churn and churn group separately, as their dynamics might differ. Compute the I choose to impute the invalid entries with median of each group.\n\nSenior Citizen is a category variable, however it is given as int. It need not be converted, as it will be eventually converted back to numeric.","6bd6fe3c":"+ The business is losing approx $140k every month as per the current data!\n+ Customers less than 1 year have the highest churn and cause highest loss.\n+ The second and third place are interchanged in the two plots. \n\nA business would be interested in retaining the category that causes higher loss with immediate priority, as that will be more return on investment. ","5e2455f5":"+ The quantile Plot is nearly linear implying normality.\n+ Normal distribution is further confirmed by the histogram to the right of the second plot. (The residue Vs Monthly Charges)\n+ Correlation of monthly charges with residual is very small. Indicates good fit\n+ As a result, we include all other variables and drop MonthlyCharges from next predictive model","34f1e381":"### 4) Training and Testing Predictive Models\n \nThe goals of Predictive Model fitting are,\n+ To identify the customers with high probability of switching to competition\n+ To identify the major causes behind the tendency to leave (feature importance)\n+ To make predictions and assess the capacity of the model for future data (Predictive power)\n+ Use all the results and make recommendations to the telecom company.\n\n#### Metrics to assess the model:\nWe are interested in customers who have tendency to go away (i.e. label 1). We may not mind a few false alarms especially if the measures taken to retain them is relatively inexpensive compared to the loss due to missing the true alarms. The measures taken on the customers who had no intention to leave can result in improved customer satisfaction and hence long term benefits.\n\nWe want high recall on class 1. "}}