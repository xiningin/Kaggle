{"cell_type":{"245cc83c":"code","e9d7ba34":"code","101ce767":"code","f5b9a8ba":"code","ddf9bc4e":"code","0ddf2f50":"code","3c61496a":"code","0e08e43c":"code","d43ec026":"code","ba2fe7c3":"code","0cb344c3":"code","f4fa882e":"code","64389b8e":"code","99258605":"code","0b06a95e":"code","a844be29":"code","a8290cee":"code","9262b412":"code","16ef3b3a":"code","c272aa39":"code","c2b1ebb4":"code","bee351dd":"code","278da979":"code","7d98ed0b":"code","d178cf71":"code","0f115613":"code","ccbc21ce":"code","90ed980c":"code","3b11fb95":"code","1be80d01":"code","fc088ee9":"code","10230632":"code","716ce673":"code","abb6f5d6":"code","a1cb24e9":"code","742786bf":"code","152e4036":"markdown","3b5e53b8":"markdown","12674e81":"markdown","23d2deff":"markdown","46126afa":"markdown","0c193429":"markdown","2cd923c1":"markdown","9ac3ac27":"markdown","8d8d4bb3":"markdown","d5c60b43":"markdown","964af69a":"markdown","a7b50b60":"markdown","d82da4a3":"markdown"},"source":{"245cc83c":"cd ..\/input\/support-ml-competition-online-news-classification","e9d7ba34":"# Standard Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns #data visualization library similar to matplotlib\n\n# Model Selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier","101ce767":"\"\"\" \nfunction that prints shapes\n\"\"\"\ndef print_shape(**kwargs):\n  for name,item in kwargs.items():\n    print(f\"{name} shape : {item.shape}\\n\")","f5b9a8ba":"train = pd.read_csv(\"train.csv\")\ntest = pd.read_csv(\"test.csv\")\nsample_submission = pd.read_csv(\"sample_submission.csv\")","ddf9bc4e":"# Check data size\nprint(f\"The train data has {train.shape[1]} features and {train.shape[0]} rows\\n\")\nprint(f\"The test data has  {test.shape[1]} features and {test.shape[0]} rows\\n\")\n","0ddf2f50":"# Data shapes and first 5 rows\nprint_shape(trainDF=train, testDF=test, sampleDF=sample_submission)\ntrain.head()","3c61496a":"\"\"\" \nFirst check if there's any null values\nfor now we don't need to know their exact locations\nhowever if we find that there's null values we will to investigate more\n\"\"\"\ntrain.isnull().sum().sum()\n\n#luckily there was no null values","0e08e43c":"\"\"\"\n now we need to know what type of data we are working with\n this can be done with printing unique data type values\n \"\"\"\nprint(train.dtypes.value_counts())\n\n\"\"\"\nMost of the data is numerical except for one column\nif we check the data above we'll find that this column is \"tag\" column,\nwe'll also notice there's a \"tag_label\" column which means that \"tag\" column\nis already enccoded to numerical value, thus we can drop it safely\n\"\"\"\ntrain.drop(['tag'], axis=1, inplace= True)","d43ec026":"\"\"\"\nSeperate train data from target (X & Y)\nour target is to predict the data channel so this will be our Y\n\n\"\"\"\nX , Y = train.drop(['data_channel'],axis = 1) , train['data_channel']\nprint_shape(X=X, Y=Y)","ba2fe7c3":"# View basic info\ntrain.describe()","0cb344c3":"# DISABLED TEMPORARILY\n# # Features Distribution plot\n# i = 1\n# fig, ax = plt.subplots(figsize=(20,20))\n\n# for col in train.columns:\n  \n#   ax = fig.add_subplot(11, 5, i)\n#   sns.distplot(train[col])\n#   i+=1\n#   if i == 55:\n#     break\n# fig.show()","f4fa882e":"\"\"\"\nsince our goal is a multiclass classification,\nlet's see how data is distributed\nthis can help us check if there's a certain class that has more\nrows which can affect our final Model\n\"\"\"\nprint(Y.value_counts())\n\n# These line are just for styling the plot\nsns.set_style('white')\nplt.figure(figsize=(7,5))\nplt.title('Classes Distribution')\n\n#The Actual plot\nsns.distplot(a=Y, axlabel='Classes', kde=False)","64389b8e":"\"\"\" Now that we studied the classes, we need to find more infromation\nbetween the train data (X) and the target label, correlation can be a good start\nsince our data is numerical\n\"\"\"\n#calculate correlation\n# The reason we are using the absolute is because high correlation\n# whether positive or negative is useful\ncorrelations = abs(train[train.columns[0:]].corr()['data_channel'][:])\n\n#sort\ncorrelations = correlations.sort_values(ascending=False)\nprint(correlations.head(10))\n\n\"\"\" \n  we have the highest columns in correlation with the target and we can use\n  correlations.keys() to get their names to use later\n\"\"\"","99258605":"# Split data into train and validation data\n# this can give us intuition on how the model would do in data it hasn't been exposed to\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)\n","0b06a95e":"# Selecting top 10 features in correlation\nxLessFeatures = X[correlations.keys()[1:11]]","a844be29":"#Prepare model\nlogisticModel = LogisticRegression(max_iter=100)","a8290cee":"# Using Cross Validation to evaluate model without selected features\nscores = cross_val_score(logisticModel,X,Y)\nprint(f\"Average Score: {sum(scores) \/ 5}\")","9262b412":"# Using Cross Validation to evaluate model with selected features\nscores = cross_val_score(logisticModel, xLessFeatures, Y)\nprint(f\"Average Score: {sum(scores) \/ 5}\")","16ef3b3a":"\"\"\"\n  these lines are commented because we already scored with the cross validation\n  however they will be useful if we decided to use this as the final model\n\"\"\"\n# # Fitting model with splitted data\n# logisticModel.fit(X_train, Y_train)","c272aa39":"# Prepare Model\n  #svmModel = SVC()","c2b1ebb4":"# Using Cross Validation to evaluate model without selected features\n  #scores = cross_val_score(svmModel,X,Y, cv=3) \n# cv was changed from default 5 to 3 since SVM to quite a longer time than other models\n# however feel free to change it anytime\n  #print(f\"Average Score: {sum(scores) \/ 3}\")","bee351dd":"  #svmModel.fit(X_train, Y_train)","278da979":"  #svmModel.score(X_test,Y_test)","7d98ed0b":"treeModel = DecisionTreeClassifier(max_depth= 5)","d178cf71":"# Using Cross Validation to evaluate model without selected features\nscores = cross_val_score(treeModel,X,Y)\nprint(f\"Average Score: {sum(scores) \/ 5}\")","0f115613":"# Using Cross Validation to evaluate model with selected features\nscores = cross_val_score(treeModel,xLessFeatures,Y)\nprint(f\"Average Score: {sum(scores) \/ 5}\")","ccbc21ce":"\"\"\"\n  these lines are commented because we already scored with the cross validation\n  however they will be useful if we decided to use this as the final model\n\"\"\"\n# # Fitting model with splitted data\n# treeModel.fit(X_train, Y_train)\n","90ed980c":"# Fitting the final model on the whole data\nfinalModel = DecisionTreeClassifier(max_depth= 5)\nfinalModel.fit(X,Y)","3b11fb95":"# Drop the 'tag' column that we dropped from the train data earlier and another column 'id' that wasn't in the training\ntest.drop(['tag','Id'],axis = 1, inplace=True)","1be80d01":"# Predict\npredictions = finalModel.predict(test)","fc088ee9":"# Add prediction to sample submission file replacing the empty column\nsample_submission['data_channel'] = predictions","10230632":"# Make sure values added\nsample_submission['data_channel'].value_counts()","716ce673":"cd input\/support-ml-competition-online-news-classification\/","abb6f5d6":"cd ..\/..\/","a1cb24e9":"cd working","742786bf":"# Save the file\nsample_submission.to_csv('submission.csv',encoding='utf-8',index=False)","152e4036":"# Models\nWe'll fit models on all features and features with top 10 correlations and compare the results\n\n\nSince the problem at hand is a classification problem we'll be using classifier models such as Logistic Regression, SVM and Decision trees.\n\n\n\n","3b5e53b8":"## Predict","12674e81":"## Decision Tree","23d2deff":"From the above plot we can see that the class coded '3' has a low number of rows which we need to keep in mind","46126afa":"## Logistic regression\n","0c193429":"## Save Predictions","2cd923c1":"# Imports","9ac3ac27":"# EDA","8d8d4bb3":"# Cleaning","d5c60b43":"**You can find more details about the logic behind these models at:**\n\n[Logistic Regression](https:\/\/towardsdatascience.com\/logistic-regression-detailed-overview-46c4da4303bc)\n<br><br>\n[SVM](https:\/\/towardsdatascience.com\/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47)\n<br><br>\n[Decision Trees](https:\/\/towardsdatascience.com\/decision-tree-algorithm-explained-83beb6e78ef4#:~:text=Decision%20Tree%20algorithm%20belongs%20to%20the%20family%20of%20supervised%20learning%20algorithms.&text=The%20goal%20of%20using%20a,prior%20data(training%20data).)","964af69a":"# Load Data","a7b50b60":"## SVM\n(SVM probably still has higher accuracy that can be achieved by searching for optimal parameters)\nuncomment the model for testing","d82da4a3":"# Final Model"}}