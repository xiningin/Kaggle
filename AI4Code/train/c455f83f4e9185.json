{"cell_type":{"f8518b02":"code","15796a89":"code","5356416b":"code","bf7067a3":"code","31970cdc":"code","5882e74b":"code","51db51a1":"code","c1905f1b":"code","c07b217d":"code","3f494299":"code","2c6fde65":"code","be524b29":"code","9bf02278":"code","93f00693":"code","d78236d7":"code","16d2458b":"code","81abb389":"code","8c17bb7e":"code","6aed015b":"code","bee41437":"code","232f2748":"code","d93a516d":"code","80316266":"code","7ec66a40":"code","588f516c":"code","aac4a02e":"code","683d6697":"code","f1c868ca":"code","7c523d3b":"code","1df17f72":"code","12d5de6e":"code","5c3380ac":"code","09fdaa7c":"code","6b428993":"code","291ca5c9":"code","227474c0":"code","fab7d621":"code","86fa5ff2":"code","3f746b65":"code","86ac2da5":"markdown","89bc6523":"markdown","c30689eb":"markdown","df8ab7c7":"markdown","b790e2c5":"markdown","ed33963d":"markdown","7f2a3c5a":"markdown","31d29e0f":"markdown","9143d9ba":"markdown","b13710b9":"markdown","b4fdadb6":"markdown","1df8b3ca":"markdown","749ad629":"markdown","17dc0ed7":"markdown","0f5a0b33":"markdown","1496e0b3":"markdown","8046a8b3":"markdown","9934fa49":"markdown","40fb8900":"markdown"},"source":{"f8518b02":"import pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline","15796a89":"train_df = pd.read_csv('..\/input\/train.csv')","5356416b":"train_df.drop(['ID_code', 'target'], axis=1).nunique().sort_values()","bf7067a3":"f\"Min: {train_df['var_68'].min()} and max: {train_df['var_68'].max()}\"","31970cdc":"epoch_datetime = pd.datetime(1900, 1, 1)\ntrf_var_68_s = (train_df['var_68']*10000 - 7000 + epoch_datetime.toordinal()).astype(int)\ndate_s = trf_var_68_s.map(datetime.fromordinal)\ntrain_df['date'] = date_s\nsorted_train_df = train_df.drop('var_68', axis=1).sort_values('date')","5882e74b":"fig, ax = plt.subplots(1, 1, figsize=(20, 8))\nsorted_train_df.set_index('date')['var_0'].plot(ax=ax)","51db51a1":"fig, ax = plt.subplots(1, 1, figsize=(20, 8))\nsorted_train_df.set_index('date')['var_1'].plot(ax=ax)","c1905f1b":"fig, ax = plt.subplots(1, 1, figsize=(20, 8))\nsorted_train_df.set_index('date')['var_2'].plot(ax=ax)","c07b217d":"fig, ax = plt.subplots(1, 1, figsize=(20, 8))\nsorted_train_df.set_index('date')['target'].plot(ax=ax)","3f494299":"date_s.nunique()","2c6fde65":"f\"Train starts: {date_s.min()}, ends: {date_s.max()}\"","be524b29":"sorted_train_df['date'].dt.month.value_counts()","9bf02278":"sorted_train_df['date'].dt.month.value_counts().plot(kind='bar')","93f00693":"sorted_train_df['date'].dt.year.value_counts()","d78236d7":"sorted_train_df['date'].dt.year.value_counts().plot(kind='bar')","16d2458b":"sorted_train_df['date'].dt.dayofweek.value_counts()","81abb389":"sorted_train_df['date'].dt.dayofweek.value_counts().plot(kind='bar')","8c17bb7e":"fig, ax = plt.subplots(1, 1, figsize=(20, 8))\nsorted_train_df.groupby('date')['target'].agg(['std', 'mean', 'max', 'min']).plot(ax=ax)","6aed015b":"# In another cell signs the count is much bigger than the other statistics\nfig, ax = plt.subplots(1, 1, figsize=(20, 8))\nsorted_train_df.groupby('date')['target'].agg(['count']).plot(ax=ax)","bee41437":"test_df = pd.read_csv('..\/input\/test.csv')\nepoch_datetime = pd.datetime(1900, 1, 1)\ns = (test_df['var_68']*10000 - 7000 + epoch_datetime.toordinal()).astype(int)\ntest_df['date'] = s.map(datetime.fromordinal)\nsorted_test_df = test_df.drop('var_68', axis=1).sort_values('date')","232f2748":"f\"Test starts: {test_df['date'].min()} and ends: {test_df['date'].max()}\"","d93a516d":"test_df['date'].dt.year.value_counts().plot(kind='bar')","80316266":"test_df['date'].dt.month.value_counts().plot(kind='bar')","7ec66a40":"test_df['date'].dt.dayofweek.value_counts().plot(kind='bar')","588f516c":"fig, ax = plt.subplots(1, 1, figsize=(20, 8))\nsorted_test_df.groupby('date')['var_1'].agg(['count']).plot(ax=ax)","aac4a02e":"len(set(sorted_train_df['date']))","683d6697":"len(set(sorted_test_df['date']))","f1c868ca":"len(set(sorted_train_df['date']) & set(sorted_test_df['date']))","7c523d3b":"len(set(sorted_train_df['date']) - set(sorted_test_df['date']))","1df17f72":"len(set(sorted_test_df['date']) - set(sorted_train_df['date']))","12d5de6e":"set(sorted_test_df['date']) - set(sorted_train_df['date'])","5c3380ac":"fig, ax = plt.subplots(1, 1, figsize=(12, 8))\nsorted_train_df.groupby('date')['var_91'].count().plot(ax=ax, label=\"train\")\nsorted_test_df.groupby('date')['var_91'].count().plot(ax=ax, label=\"test\")\nax.legend()","09fdaa7c":"# Zoom on 2018\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\n(sorted_train_df.loc[lambda df: df.date.dt.year == 2018]\n               .groupby('date')['var_91']\n               .count()\n               .plot(ax=ax, label=\"train\"))\n(sorted_test_df.loc[lambda df: df.date.dt.year == 2018]\n               .groupby('date')['var_91']\n               .count()\n               .plot(ax=ax, label=\"test\"))\nax.legend()","6b428993":"# Zoom on 2018-1\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\n(sorted_train_df.loc[lambda df: (df.date.dt.year == 2018) & (df.date.dt.month == 1)]\n               .groupby('date')['var_91']\n               .count()\n               .plot(ax=ax, label=\"train\"))\n(sorted_test_df.loc[lambda df: (df.date.dt.year == 2018) & (df.date.dt.month == 1)]\n               .groupby('date')['var_91']\n               .count()\n               .plot(ax=ax, label=\"test\"))\nax.legend()","291ca5c9":"# Zoom on 2018-1\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\n(sorted_train_df.loc[lambda df: (df.date.dt.year == 2018) & (df.date.dt.month == 1)]\n               .groupby('date')['var_91']\n               .mean()\n               .plot(ax=ax, label=\"train\"))\n(sorted_test_df.loc[lambda df: (df.date.dt.year == 2018) & (df.date.dt.month == 1)]\n               .groupby('date')['var_91']\n               .mean()\n               .plot(ax=ax, label=\"test\"))\nax.legend()","227474c0":"overlapping_dates = set(sorted_train_df['date']) & set(sorted_test_df['date'])","fab7d621":"grouped_df = (sorted_train_df.loc[lambda df: df.date.isin(overlapping_dates)]\n                             .groupby('date')['target']\n                             .mean())","86fa5ff2":"grouped_df.plot(kind='hist', bins=100)","3f746b65":"grouped_df.to_csv('grouped_df.csv', index=False)","86ac2da5":"Idea to try: predict the mean of the target (using the date \nfor grouping) for the overlapping dates. ","89bc6523":"Two things to observe: \n    \n- data has been annonymized\n- it comes from a business setting\n\nThus, it is most likely (but not 100% sure) that some of the features\ncontain date-like information (and also categorical features but that's \nfor another day). \n\nHow to find potential columns? Let's try to sort the columns using the number of unique \nvalues. What's the heurestic behind this choice? \nWell there shouldn't be a lot of dates, maybe few thousand top.","c30689eb":"Alright, let's now explore this newly created column.","df8ab7c7":"==> Most of the dates overlap. ","b790e2c5":"This exploration is an attempt to discover some hidden things behind the annonymization.\nNothing is certain of course and this is so far specualative. \nUse this knowledge accordingly. ","ed33963d":"# What to do now?","7f2a3c5a":"Some of the things I will try to do: \n- Use this transformed column for a better temporal CV. Some ideas I have tried: stratification using years, day of weeks, and so on.\n- Transform other columns using this new one\n\nStay tuned for more insights. :)","31d29e0f":"Let's see if our observation transfers well to the test dataset.","9143d9ba":"So how to extract a date?\nWell, first, get ride of the decimal values.\nThen transform to a datetime object supposing that it is an ordinal datetime.\nTry different offsets until you get a meaningful date range.\nThat's it. Let's see this in action.","b13710b9":"# Test and train date column comparaison","b4fdadb6":"#\u00a0Some plots ","1df8b3ca":"#\u00a0What about the test?","749ad629":"#\u00a0Before you read","17dc0ed7":"=> I will thus use the `date` column to group rows. ","0f5a0b33":"==> Uniform day of week distribution. That's a good sign!","1496e0b3":"==> `var_68` has the least number of uniques, thus it **might** be a date-like column\n(it could also be a categorical column).\nThere is also a possibility that this small number of uniques is a coincidence due to the rounding to 4 decimal numbers (bonus question: could you compute the probability of this event?)","8046a8b3":"#\u00a0Preliminary work","9934fa49":"Thanks to this discussion for the observation: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/discussion\/84450\n\nIn this notebook, I transform this column and re-order the train dataset using this column, and see what \nhappens.","40fb8900":"# Date column exploration"}}