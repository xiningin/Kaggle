{"cell_type":{"48d64fba":"code","13c764a9":"code","b075a0fd":"code","9002992d":"code","5d3547fe":"code","fda800ea":"code","60e75e3b":"code","f924f6cf":"code","e6925501":"code","89b673a4":"code","09774355":"code","16d9b987":"code","04af578a":"code","dacc9b68":"code","53d3b891":"code","83050b01":"code","ba75f1d2":"code","0a17903c":"code","f8067252":"code","61cacefa":"code","9f15b46a":"code","bb8e8d7d":"code","11439936":"code","70aaf3d0":"code","9d794836":"code","b7e33069":"code","a260c3c7":"code","fedb0fa9":"code","f1d0e122":"code","fd8b8d41":"code","4da73190":"code","40647b3f":"code","b3f0db4a":"code","a1c094a9":"code","031adb29":"code","de4ebf5f":"code","4f5f85c4":"code","52869def":"code","f1bc0a30":"markdown","b68ec4ea":"markdown","ad66dc9a":"markdown","4469a904":"markdown","00c58f55":"markdown","1b5dc02d":"markdown","448cf461":"markdown","b192f7f8":"markdown","ed4ba71f":"markdown","04e4c3c1":"markdown","3a11ecfb":"markdown","17577c90":"markdown","b5d67ac1":"markdown","e5935e67":"markdown","ef504a40":"markdown","b4662373":"markdown","07320dcd":"markdown","9f4dfde8":"markdown","27501b1a":"markdown","1eb7279f":"markdown","06a83a10":"markdown","dfd2eee3":"markdown","714527c9":"markdown","f2e3254c":"markdown","f415b179":"markdown","e94454eb":"markdown","749c2514":"markdown","06804006":"markdown","85628ee8":"markdown","d0c31a2c":"markdown","39e55cdc":"markdown","8bd16aa6":"markdown","1b95e1fc":"markdown"},"source":{"48d64fba":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nimport missingno\nimport seaborn as sns\nimport plotly.express as px\nfrom IPython.display import display_html \n\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sns \nsns.set()\n\nfont = {'family' : 'cursive',\n          'weight' : 'bold',\n          'size'   : 8}\nplt.rc('font',**font)\nplt.rc('figure',figsize=(10,13))\npd.set_option('display.max_columns',30)","13c764a9":"## we have the data in csv format \n## so lets read it first\ndata = pd.read_csv('..\/input\/automobile-dataset\/Automobile_data.csv')\ndata.head() #shows top 5 rows","b075a0fd":"import string\n# dir(string)","9002992d":"special_char_cols = [] \nfor i in data.columns:\n    ## we print only those cols which contain special characters :--\n    if len(set(data[i].values).intersection(string.punctuation))>=1:\n        print(f'columns = {i} special char -->{set(data[i].values).intersection(string.punctuation)}')\n        special_char_cols.append(i)","5d3547fe":"data.isna().sum()\n# since the number of cols is just too much The outputs look damn messy","fda800ea":"# so symboling basically tells us whether a vehicle is safe or not -2 is safe ,  +3< is not safe\ndata.describe()","60e75e3b":"## we use a code of such format\n#USE THIS format--> print(data['normalized-losses'].replace('?',np.nan).isna().sum())\n\nfor i in special_char_cols:\n    data[i].replace('?',np.nan,inplace=True)","f924f6cf":"missingno.matrix(data) # to check missing values counts in diff. cols","e6925501":"\ndata['normalized-losses'] = data['normalized-losses'].astype(float)\n\nsns.kdeplot(data['normalized-losses'])\nplt.title('Before')\nplt.show()\n\ndata['normalized-losses'].mean()\n\n# replacing nan's with mean of the plot of normalised loss \ndata['normalized-losses'] = data['normalized-losses'].replace(np.nan,int(data['normalized-losses'].mean()+np.random.randint(1,6)))\n\nsns.kdeplot(data['normalized-losses'])\nplt.title('After')\nplt.show()","89b673a4":"cols = data.columns\nnan_cols = []","09774355":"def check_nan():\n        for i in data.columns:\n            if data[i].isna().sum()>0:\n                nan_cols.append(f'{i}')\n                print(f'data[{i}]')\nprint(check_nan())","16d9b987":"## i wanna impute the means in these *int* cols \nsubs = {\n    'bore':3.4,\n    'stroke':3,\n    'horsepower':100,\n    'peak-rpm':5000\n}\n\nfor cols,val in subs.items():\n    data[cols].fillna(val,inplace=True)","04af578a":"nums_col = []\ncols = data.columns\nfor i in cols:\n    if data[i].dtype != 'O':\n        # we plot only for the numeric cols\n        nums_col.append(i)\nprint(len(nums_col))","dacc9b68":"## 18 numeric cols\n## this worked for me tho\nfig = plt.figure(figsize=(16,14))\n\nfor i,col in zip(range(1,19),nums_col):\n    axes = plt.subplot(3,6,i)\n    axes.set_title(col)\n    sns.boxplot(y=data[col],ax=axes)\n    axes.get_xaxis().set_visible(False)\n    axes.set_ylabel('')\n    \nplt.show()","53d3b891":"## all I wanna do is to check out the number of categories in the columns in category type cols\n# The output looks mess \nfor i in data.columns:\n    if data[i].dtype == 'O':\n        print(f'{i}--> {data[i].value_counts()}')\n        print(20*'*')\n## there are columns in which there are nums but they are of type obj so we will convert them at last into the num cols","83050b01":"data['num-of-doors'].unique()\n","ba75f1d2":"no_to_map = {'one':1,'four':4,\n             'six':6,'five':5,\n             'three':3,'twelve':12, \n            'two':2, 'eight':8}\n\ndata['num-of-cylinders'] = data['num-of-cylinders'].map(no_to_map)\n","0a17903c":"#### change the num - of- cyl & num-of-doors strings to ints\ndata['num-of-doors'] = data['num-of-doors'].map(no_to_map)","f8067252":"data['num-of-doors'].fillna(data['num-of-doors'].mean(),inplace=True)","61cacefa":"## here, I wanna ceil the float cols the extra zeroes doesnt look good\nfor i in data.columns:\n    if data[i].dtype=='float64':\n        print(i)\n        data[i] = data[i].apply(np.ceil)","9f15b46a":"## these cols were'nt contributing mucho mucho \n## but these play a huge role in price determination \n## better take avgs of these AND DROP ORIGINALS\ndata['mpg'] = (data['city-mpg']+data['highway-mpg'])\/2\ndata.drop(['city-mpg','highway-mpg'],inplace=True,axis=1)\n\n#i replace the Nan's in each col. except price\n## now finally lets encode some of the categorical columns::::\n\ndata['fuel-type'] = data['fuel-type'].map({'gas':0,'diesel':1})\ndata['aspiration'] = data['aspiration'].map({'std':0,'turbo':1})\ndata['drive-wheels'] = data['drive-wheels'].map({'rwd':0,'fwd':1,'4wd':2})\n\n\n## I encoded them in order of what I found out after googling !! \ndata['engine-type'] = data['engine-type'].map({\n    'dohc':5, 'ohcv':4, 'ohc':3, 'l':1, 'rotor':0, 'ohcf':2 })","bb8e8d7d":"## since only have price column left with Nans'\ndata.dropna(inplace=True)\n## If some one can help me with this----\n# I wanted to drop nan using \n\n# data['price'].dropna(inplace=True)## this made the rows to 201 \n\n# but the other rows remained 205\n\n# how do i reset the index(to 201 rows) !!!!???\n","11439936":"## why is it like some columns are having int values and are of dtype==object\n## lets TRY convert those to int\n\ndata[['wheel-base','length','width','height',\n      'compression-ratio',\n      'horsepower','peak-rpm',\n      'price']] = data[['wheel-base','length',\n                        'width','height',\n                        'compression-ratio',\n                           'horsepower','peak-rpm',\n                           'price']].astype('int64')\n\ndata[['bore','stroke']] = data[['bore','stroke']].astype('float64')","70aaf3d0":"cols = data.columns\nnan_cols = []","9d794836":"def check_nan():\n        for i in data.columns:\n            if data[i].isna().sum()>0:\n                nan_cols.append(f'{i}')\n                print(f'data[{i}]')\nprint(check_nan())","b7e33069":"f = plt.figure()\ngrid = plt.GridSpec(1, 5, wspace=0.80, hspace=0.60)\n\n\ncmap1 = sns.color_palette('viridis')\nplt.subplot(grid[0,0:4])\nplt.xticks(rotation=90)\nsns.scatterplot(x=\"make\", y=\"price\",data=data,palette=cmap1)\nf.set_figheight(8.75)\nplt.subplot(grid[0,4])\nplt.axis('off')\nplt.grid(False)\nplt.text(0.10,0.30,'''Graphs like these help us only see \n    the relationship of the col with target price,\n    And analyse whats the relationship between these makers with the price,\n                \n                \n                \n                \n     It is evident from the daily life that makers like mercedes , BMW & porsch\u0113 cars costs more,\n    thats what we can evidently see in the chart \n\n        \n        \n        \n\n\n\n    Also small makers like toyota ,mitsubishi & Nissan are best sellers when it comes to selling \n    low price cars ''',fontsize=14,fontdict={'fontweight':'bold','fontfamily':'fantasy','fontstretch':'ultra-expanded'})\nplt.show()","a260c3c7":"data['fuel-system'].unique()","fedb0fa9":"## lets encode features of the other categorical cols and drop [make] col\ndata['body-style'] = data['body-style'].map({\n    'convertible':5,'hatchback':4,'sedan':3,'wagon':2,'hardtop':1 ## this is what i found from my google searches \n})\ndata['engine-location'] = data['engine-location'].map({\n    'front':1,\n    'rear':2 ## rear engine loc is found in very few top class cars hence more weightage\n})\n# data['fuel-system'] = data['fuel-system']###  I SERIOUSLY DO NOT KNOW HOW TO ENCODE THESE SO I DROP THESE\ndata.drop(['make','fuel-system'],axis=1,inplace=True)","f1d0e122":"data_styler = data.head().style.set_table_attributes('style=\"display:inline;font-family:verdana;title:\"').set_caption('Head of Data')\ndisplay_html(data_styler._repr_html_(),raw=True)","fd8b8d41":"data.to_csv('cleaned_data.csv',index=False)\n\n# loading cleaned data\ndata = pd.read_csv('cleaned_data.csv')\ndata.head()","4da73190":"mask = np.triu(data.corr())\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(16,18))\n\n# Generate a custom diverging colormap\ncmap = sns.color_palette(\"flare\", as_cmap=True)\n\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(data.corr(),annot=True, mask=mask, cmap=cmap,vmin=-1,vmax=1, center=0,\n            square=True, linewidths=False, cbar_kws={\"shrink\": 0.3})# shrink is used to change the size of the tiny color bar to the right \n","40647b3f":"## want to check corr between variables with other cols\nfrom scipy import stats\n## therefore I make a function To find corelation between two of them easily \ndef cor(col,tarcol,output=True):\n    corr , p_value = stats.pearsonr(data[col],data[tarcol])\n    if output==True:\n        print(f'cor between {col} <--> {tarcol}  :{corr}, & p_value ---> {p_value}')\n    return corr,p_value","b3f0db4a":"cor('length','wheel-base') \ncor('width','wheel-base')\ncor('wheel-base','bore')\ncor('wheel-base','curb-weight')\ncor('horsepower','engine-size')","a1c094a9":"for i in data.columns:\n    if ((i!='curb-weight')& (data[i].dtype!='O')):\n        correlation,p_value = cor('curb-weight',i,False)\n        if abs(correlation)>0.70:\n            print(f'corr. of curb-weight with {i}-->{correlation}')","031adb29":"cor('length','price')## we drop length \ncor('length','width')\ncor('length','curb-weight')\ncor('width','price')\ncor('curb-weight','width')\ncor('wheel-base','curb-weight')\ncor('curb-weight','price')### we only keep curb-weight and drop length,width,wheelbase\n## \n# cor('city-mpg','price')\n# cor('highway-mpg','price')\n# cor('city-mpg','highway-mpg') ## this is why i am dropping one of these (high correlation)\n###I DROPPED THEM EARLIER BASED ON THESE RESULTS\n##\ncor('engine-size','price')\ncor('horsepower','price')\n### check for compression ratio \n## we will drop this one too \ncor('fuel-type','compression-ratio')\ncor('fuel-type','price')\ncor('compression-ratio','price')# null hypo of no significance difference gets rejected p>0.05 \n\ncor('height','price')","de4ebf5f":"data.drop(['engine-type','peak-rpm','symboling','fuel-type','aspiration','compression-ratio','length','width','wheel-base','body-style','horsepower','height','normalized-losses','stroke','num-of-doors'],axis=1,inplace=True)","4f5f85c4":"# Generate a mask for the upper triangle\nmask = np.triu(data.corr())\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(16,18))\n\n# Generate a custom diverging colormap\ncmap = sns.color_palette(\"flare\", as_cmap=True)\n\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(data.corr(),annot=True, mask=mask, cmap=cmap,vmin=-1,vmax=1, center=0,\n            square=True, linewidths=False, cbar_kws={\"shrink\": 0.3})# shrink is used to change the size of the tiny color bar to the right \n","52869def":"data_styler = data.head().style.set_table_attributes('style=\"display:inline;font-family:verdana;title:\"').set_caption('Head of Data')\ndisplay_html(data_styler._repr_html_(),raw=True)","f1bc0a30":"better drop all these and then decide what to do of the other variables \nall of these are in high corr with each other","b68ec4ea":"It is quite evident from the corr values that we seriously need some to be removed \n* length\n* width\n* curb-weight \n* city-mpg\n* horsepower\n* compression ratio # since its cor with price is less \n\nTo choose which one to Drop we compare bivariate strength with the dependent variable \n\nso the one with the LOWER STRENGTH gets DROPPED ","ad66dc9a":"I tried this way too but this didnt worked out \n* fig,axes = plt.subplots(3,6,figsize=(16,10))\n* plt.tight_layout()\n\nafter this i was confused as to what should i do to iterate over it \n\nI dont know how to plot multi row grids  ","4469a904":"<p style='text-align:center;color:#c44dff;font-weight:800'> In Short We select only those variable which are MODERATELY CORELATED with price and do NOT VIOLATE rules of MULTICOLINEARITY<\/p>","00c58f55":"##### run this to check cols cross check cols categories\ndata.head(2)\n\ndata.info()","1b5dc02d":"<b><h3 style='text-align:center;color:purple'>  Well I dont think there are many outliers in this data <\/h3>\n<b><h3 style='text-align:center;color:purple'>These abnormalities actually makes model generalise better <\/h3>\n<b><h3 style='text-align:center;color:purple'>This small amount of variations are OK! (i seriously dont think of doing so) <\/h3><\/b><\/b><\/b>\n\nwell since this is my first NB i may be wrong\n\n--->(correct me if i am wrong)(PLEASSSEEEE!!)<-------","448cf461":"<p style='text-align:center;color:#ffd11a;font-weight:800'>Well we can see that peak-rpm , normalised-losses , num-of-doors and stroke <\/p>\n    \n<p style='text-align:center;color:#ffd11a;font-weight:800'>ALSO has nothing much to contribute better drop em tooo\n\n<p style='text-align:center;color:#ffd11a;font-weight:800'> More over symboling too also doesnt contribute much <\/p>","b192f7f8":"If you wanna Read My motive on writing this Notebook You Can See it At the End","ed4ba71f":"# EDA And Cleaning Of AutomobilesDataSet","04e4c3c1":"<h3 style='text-align:center;color:#ffcc66'>we can afford to drop prices and replace others nans with means respectively","3a11ecfb":"<h3> Considering Hypothesis<\/h3>\n\n<p style='text-align:center;color:#6699ff;font-weight:800'> We consider the null hypothesis as - no difference between the cols<\/p>\n\n<p style='text-align:center;color:#6699ff;font-weight:800'>The Alternate hypo. - there is significant difference between the cols <\/p>","17577c90":"I was trying to improve the graph by making the fonts bigger , & increasing the graph area \n\nbut i was not able to \n\ncan anyone help me in this one!!! ","b5d67ac1":"<p style='color:#3333ff'>Though the data is small but the number of independent variable is just way more than required ..\nThis may lead to cases of MultiColinearity between the columns","e5935e67":"<p style='text-align:center;color:#ffd11a;font-weight:800' > Other variables COR with Curb-Weight<\/p>","ef504a40":"# Now the Question is What is hell is Multicolinearity???","b4662373":"## Well On What Basis Do you Drop the COLUMNS ???","07320dcd":"<h3 style='text-align:center;color:#cc00ff'>I wanna check out something<\/h3> \n\n\n<ul style='color:#0099ff;fontfamily:monospace'>    \n<li>RELATION between different Makers Vs Prices<\/li>\n   <\/ul>\n","9f4dfde8":"<p style='text-align:center;color:#ffd11a;font-weight:800'> Comparing 'Pairs-Strength' with 'dependent varible' the one with lesser strength gets dropped<\/p>\n\n*dependent variable in this data is PRICE*","27501b1a":"<b><h3 style='text-align:center;color:#ffcc66'> So lets better check some MULTICOLINEARITY in data and DROP some COLS of the data<\/h3><b>","1eb7279f":" An approach I like is to save the desired cols OR values in data structures\n so that when we use subplots ALL YOU HAVE TO DO IS TO ENUMERATE OVER IT <<-;  hehe  ;->>\n here we used nums_col to do the same","06a83a10":"ITS STORY TIME \ud83d\ude04\ud83d\ude04\n\nMy Journeyy just like Everyone else is Full of 'Ups and Downs'\ud83d\ude04\ud83d\ude04\nIts been quiet a long time I did EDA of data , since I was Exploring the field of Deep Learning\n\nSo one day I was not able to remember the Basics of Data Analysis AND I went LIKE \"\" HOLY SHIT \"\" I mean I talk about deep learning and stuff and cant perform a basic EDA on Data \ud83e\udd26\u200d\ud83e\udd26\u200d\ud83e\udd26\u200d \ud83d\ude05\n\nI have spent a considerable amount of time in understanding deep learning\nso I thought now is the time to apply whatever you know and LETS SEE WHERE IT GOES\n\nI think I got a bit confused in concepts of statistical analysis so if you feel like\n\nsomethings wrong !!! feel free to leave a comment\n\nI HOPE YOU ENJOY SEEING THE NOTEBOOK AND LEARN SOMETHING FROM IT","dfd2eee3":"#### What Is CURB-WEIGHT\n##### curb-weight is the weight of the vehicle when no passengers are in there","714527c9":"<h3 style='text-align:center;color:#ffcc66'>Probably I can see that the Nan's in the PRICE column. are very less <\/h3>\n\n**Hence I Better Drop Them**","f2e3254c":"### The Approach\n\n*anyone having a better approach !! feel free to leave a comment*","f415b179":"<p style='text-align:center;color:#3333ff;font-weight:800'> WORK IN PROGRESS <\/p>\n\n<h2 style='text-align:center;font-weight:800'> Leave UPVOTE ! If you found this interesting <\/h2> \n\n### Resources USED :\n* [for subplots-guiding](https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/04.08-multiple-subplots.html)\n* checkout @andradaoltaneu on kaggle (her notebooks \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25) \n* [Credits to @drazen](https:\/\/www.kaggle.com\/drazen\/heatmap-with-sized-markers)\n* [R ANALYSIS\ud83d\udd25\ud83d\udd25](https:\/\/rstudio-pubs-static.s3.amazonaws.com\/200006_42952a32a0a94d0ba627cc66209fe9d7.html#:~:text=Engine%20Type,-The%20engine%20type&text=In%20this%20dataset%20we%20have%20seven%20engine%20types%3A%20dohc%20(Dual,and%20rotor%20(Rotary%20engine).)","e94454eb":"<b><h3 style='text-align:center;color:#ffcc66'> Lets Check for possible outliers in the Data <\/h3><\/b>\n","749c2514":"well you consider this as some replacable duplicate feature \n\nlets say for example:-- \n\nIf I find corelation between  'Age' And 'Number of years of experience' it will be Highly corelated \n\nThis means They depend on each other and DROPPING ONE OF THEM WONT EFFECT DATA MUCH \nThis is WHY !!!! \nwe are checking for multicolinearity in the data ","06804006":"\n<b>\n<h5 > In this NB Data Is Cleaned Thoroughly ; there is not much inference drawn from the data though<\/h5> \n<p style='color:#00000'>May be I think I will be Covering That in a seperate Notebook on Inference AND ML Modelling <\/p>\n    \n    To be honest ::\n\n    I have Made Some Mistakes Though Please leave a Comment And Give a valuable feedback\n    \n      *pls read the comments carefully*\n    *I guess you will find something new in them*","85628ee8":"<h3><b><p style='text-align:center;color:#ffcc66'> Guess What !! I never tried styling graphs in mpl<\/p><\/b>\n<b><p style='text-align:center;color:#ffcc66'>so I Tried it this time and I AM in awe <\/p><\/b>\n\n\n\n<p style='text-align:center'> Since normalised-losses has the \"Most Num of Missing values\" we Replace 'em' with their means <\/p>","d0c31a2c":"<b><p style=\"text-align:center\">We can see some columns having special characters in them instead of <\/p>\n    \n<b><p style='text-align:center'>NAN VALUES<\/p> \n    \n<b><H3 style='text-align:center'>The catch with these is they do not get detected when checking for Nan's\n<\/H3>\n\n*therefore , be careful*","39e55cdc":"these are correlated highly --> i.e. Existence of Multicolinearity Between Independent Variables\n\n    *length & width -- wheelbase*\n\n    *length | wheelbase --  curb-weight | enginesize | bore*\n\n    *horsepower -- enginesize*\n\n    *mpg both city & highway*\n    \nwe will calculate the p-values ,pearson_corelation and other stats to see which are to be removed and which are to be kept","8bd16aa6":"<h2 style='text-align:center;color:#ffcc66'>LETS SAVE THE DATA !!!!<\/h2> ","1b95e1fc":"### we replace this '?'  with Nan's"}}