{"cell_type":{"d08ace84":"code","e4616f4e":"code","7fc83897":"code","31c87ed6":"code","bdf140f2":"code","82eb67b8":"code","912b2bc2":"code","e6aa0ef8":"code","6437d104":"code","dc96aae8":"code","25df42ec":"code","5e6b5eb6":"code","cff0c685":"code","2a891c63":"code","37a46d7b":"markdown","62c331be":"markdown","26237ba5":"markdown","6123064f":"markdown","ea478e70":"markdown","c5e2c271":"markdown","37753d3a":"markdown","7bf957d7":"markdown","cce8a8d8":"markdown","94036a4a":"markdown","3c8b5590":"markdown","e9e4137a":"markdown","53e7b148":"markdown","a72a8772":"markdown","28b139db":"markdown","b65d9fbd":"markdown"},"source":{"d08ace84":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport inspect","e4616f4e":"%load_ext autoreload\n%autoreload 2","7fc83897":"!pip install python-chess  # Python-Chess is the Python Chess Package that handles the chess environment\n!pip install --upgrade git+https:\/\/github.com\/arjangroen\/RLC.git  # RLC is the Reinforcement Learning package","31c87ed6":"import chess\nfrom chess.pgn import Game\nimport RLC\nfrom RLC.capture_chess.environment import Board\nfrom RLC.capture_chess.learn import Q_learning\nfrom RLC.capture_chess.agent import Agent","bdf140f2":"board = Board()\nboard.board","82eb67b8":"board.layer_board[0,::-1,:].astype(int)","912b2bc2":"board = Board()\nagent = Agent(network='conv',gamma=0.1,lr=0.07)\nR = Q_learning(agent,board)\nR.agent.fix_model()\nR.agent.model.summary()","e6aa0ef8":"print(inspect.getsource(agent.network_update))","6437d104":"print(inspect.getsource(R.play_game))","dc96aae8":"pgn = R.learn(iters=750)","25df42ec":"reward_smooth = pd.DataFrame(R.reward_trace)\nreward_smooth.rolling(window=125,min_periods=0).mean().plot(figsize=(16,9),title='average performance over the last 125 steps')","5e6b5eb6":"with open(\"final_game.pgn\",\"w\") as log:\n    log.write(str(pgn))","cff0c685":"board.reset()\nbl = board.layer_board\nbl[6,:,:] = 1\/10  # Assume we are in move 10\nav = R.agent.get_action_values(np.expand_dims(bl,axis=0))\n\nav = av.reshape((64,64))\n\np = board.board.piece_at(20)#.symbol()\n\n\nwhite_pieces = ['P','N','B','R','Q','K']\nblack_piece = ['_','p','n','b','r','q','k']\n\ndf = pd.DataFrame(np.zeros((6,7)))\n\ndf.index = white_pieces\ndf.columns = black_piece\n\nfor from_square in range(16):\n    for to_square in range(30,64):\n        from_piece = board.board.piece_at(from_square).symbol()\n        to_piece = board.board.piece_at(to_square)\n        if to_piece:\n            to_piece = to_piece.symbol()\n        else:\n            to_piece = '_'\n        df.loc[from_piece,to_piece] = av[from_square,to_square]\n        \n        ","2a891c63":"df[['_','p','n','b','r','q']]","37a46d7b":"#### Implementation\n- I built two networks, A linear one and a convolutional one\n- The linear model maps the state (8,8,8) to the actions (64,64), resulting in over 32k trainable weights! This is highly inefficient because there is no parameter sharing, but it will work.\n- The convolutional model uses 2 1x1 convulutions and takes the outer product of the resulting arrays. This results in only 18 trainable weights! \n    - Advantage: More parameter sharing -> faster convergence\n    - Disadvantage: Information gets lost -> lower performance\n- For a real chess AI we need bigger neural networks. But now the neural network only has to learn to capture valuable pieces.","62c331be":"* ### Learned action values for capturing black (lower case) with white (upper case) pieces.\nUnderscore represents capturing an empty square","26237ba5":"## References\nReinforcement Learning: An Introduction  \n> Richard S. Sutton and Andrew G. Barto  \n> 1st Edition  \n> MIT Press, march 1998  \n\nRL Course by David Silver: Lecture playlist  \n> https:\/\/www.youtube.com\/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ  \n\nExperience Replay  \n> https:\/\/datascience.stackexchange.com\/questions\/20535\/what-is-experience-replay-and-what-are-its-benefits","6123064f":"[Notebook 1: Policy Iteration](https:\/\/www.kaggle.com\/arjanso\/reinforcement-learning-chess-1-policy-iteration)  \n[Notebook 2: Model-free learning](https:\/\/www.kaggle.com\/arjanso\/reinforcement-learning-chess-2-model-free-methods)  \n[Notebook 4: Policy Gradients](https:\/\/www.kaggle.com\/arjanso\/reinforcement-learning-chess-4-policy-gradients)","ea478e70":"#### Numerical representation of the pawns (layer 0)\nChange the index of the first dimension to see the other pieces","c5e2c271":"## Learned action values analysis\nSo what has the network learned? The code below checks the action values of capturing every black piece for every white piece. \n- We expect that the action values for capturing black pieces is similar to the (Reinfeld) rewards we put in our environment. \n- Of course the action values also depend on the risk of re-capture by black and the opportunity for consecutive capture. ","37753d3a":"## Reinforcement Learning Chess\nReinforcement Learning Chess is a series of notebooks where I implement Reinforcement Learning algorithms to develop a chess AI. I start of with simpler versions (environments) that can be tackled with simple methods and gradually expand on those concepts untill I have a full-flegded chess AI.","7bf957d7":"#### Q learning with a Q-network\n**Theory**\n- The Q-network is usually either a linear regression or a (deep) neural network. \n- The input of the network is the state (S) and the output is the predicted action value of each Action (in our case, 4096 values). \n- The idea is similar to learning with Q-tables. We update our Q value in the direction of the discounted reward + the max successor state action value\n- I used prioritized experience replay to de-correlate the updates. If you want to now more about it, check the link in the references\n> - I used fixed-Q targets to stabilize the learning process. ","cce8a8d8":"### The environment: Capture Chess\nIn this notebook we'll upgrade our environment to one that behaves more like real chess. It is mostly based on the Board object from python-chess.\nSome modifications are made to make it easier for the algorithm to converge:\n* There is a maximum of 25 moves, after that the environment resets\n* Our Agent only plays white\n* The Black player is part of the environment and returns random moves\n* The reward structure is not based on winning\/losing\/drawing but on capturing black pieces:\n    - pawn capture: +1\n    - knight capture: +3\n    - bishop capture: +3\n    - rook capture: +5\n    - queen capture: +9\n* Our state is represent by an 8x8x8 array\n    - Plane 0 represents pawns\n    - Plane 1 represents rooks\n    - Plane 2 represents knights\n    - Plane 3 represents bishops\n    - Plane 4 represents queens\n    - Plane 5 represents kings\n    - Plane 6 represents 1\/fullmove number (needed for markov property)\n    - Plane 7 represents can-claim-draw\n* White pieces have the value 1, black pieces are minus 1\n       \n","94036a4a":"#### Implementation","3c8b5590":"### The Agent\n* The agent is no longer a single piece, it's a chess player\n* Its action space consist of 64x64=4096 actions:\n    * There are 8x8 = 64 piece from where a piece can be picked up\n    * And another 64 pieces from where a piece can be dropped. \n* Of course, only certain actions are legal. Which actions are legal in a certain state is part of the environment (in RL, anything outside the control of the agent is considered part of the environment). We can use the python-chess package to select legal moves. (It seems that AlphaZero uses a similar approach https:\/\/ai.stackexchange.com\/questions\/7979\/why-does-the-policy-network-in-alphazero-work)","e9e4137a":"The PGN file is exported to the output folder. You can analyse is by pasting it on the [chess.com analysis board](https:\/\/www.chess.com\/analysis)","53e7b148":"#### Demo","a72a8772":"#### Import and Install","28b139db":"### Notebook III: Q-networks\nIn this notebook I implement an simplified version of chess named capture chess. In this environment the agent (playing white) is rewarded for capturing pieces (not for checkmate).  After running this notebook, you end up with an agent that can capture pieces against a random oponnent as demonstrated in the gif below. The main difference between this notebook and the previous one is that I use Q-networks as an alternative to Q-tables. Q-tables are nice and straightforward, but can only contain a limited amount of action values. Chess has state space complexity of 10<sup>47<\/sup>. Needless to say, this is too much information to put in a Q-table. This is where supervised learning comes in. A Q-network can represent a generalized mapping from state to action values.\n\n![](https:\/\/images.chesscomfiles.com\/uploads\/game-gifs\/90px\/green\/neo\/0\/cc\/0\/0\/aXFZUWpyN1Brc1BPbHQwS211WEhudkh6cXohMGFPMExPUTJNUTY4MDY1OTI1NFpSND8yOT85M1Y5MTA3MUxLQ3RDUkpDSjcwTE0wN293V0d6Rzc2cHhWTXJ6NlhzQVg0dUM0WGNNWDU,.gif)\n\n","b65d9fbd":"#### Board representation of python-chess:"}}