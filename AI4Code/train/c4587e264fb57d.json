{"cell_type":{"bde5b8d4":"code","9f5489ee":"code","51ad90a1":"code","c8d0767c":"code","37bf00e3":"code","7d714139":"code","895bcc65":"code","7f849b1a":"code","da037bce":"code","cb66a4b9":"code","a25d69ae":"code","ecc1536c":"code","4fd8ecd1":"code","ca09bd4e":"code","be37f935":"markdown","f4adc3bc":"markdown","bbe4da23":"markdown","190a2dc7":"markdown","0252f125":"markdown","b606cbbb":"markdown","2e788a6d":"markdown","f9aa5889":"markdown","1e99a5df":"markdown","5baad3c5":"markdown","a5056693":"markdown","0d438bdb":"markdown"},"source":{"bde5b8d4":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntrain_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-02\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-02\/eval.csv\")\n\nprint(train_data.info(), \"\\n\")\nprint(test_data.info())","9f5489ee":"train_data.drop(\"title\", axis=1, inplace=True)\nprint(train_data.columns)","51ad90a1":"print(train_data[\"esrb_rating\"].unique())","c8d0767c":"for column in train_data.drop([\"id\", \"esrb_rating\"], axis=1, inplace=False):\n    print(train_data[column].unique())\n\nprint(\"\\n\")\n\nfor column in test_data.drop(\"id\", axis=1, inplace=False):\n    print(test_data[column].unique())","37bf00e3":"for column in train_data.drop([\"id\", \"esrb_rating\"], axis=1, inplace=False):\n    print(column, \"\\n\")\n    print(train_data.groupby(\"esrb_rating\")[column].describe(), \"\\n\\n\")","7d714139":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntrain_data[\"esrb_rating\"] = le.fit_transform(train_data[\"esrb_rating\"])\n\nX_train = train_data.drop([\"id\", \"console\", \"use_of_drugs_and_alcohol\", \"esrb_rating\"], axis=1, inplace=False)\ny_train = train_data[\"esrb_rating\"]","895bcc65":"from sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\n\nlr = LogisticRegression(solver=\"liblinear\")\nlr_param_grid = {\"penalty\": [\"l1\", \"l2\"], \"C\": np.logspace(-4, 4, 10)}\n\nlr_search = GridSearchCV(lr, lr_param_grid)\n\nlr_search.fit(X_train, y_train)\n\nprint(lr_search.best_params_)\n\nlr_scores = pd.DataFrame(cross_val_score(lr_search.best_estimator_, X_train, y_train, cv=10))","7f849b1a":"from sklearn.svm import SVC\n\nsvc = SVC()\nsvc_param_grid = {'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}\n\nsvc_search = GridSearchCV(svc, svc_param_grid)\n\nsvc_search.fit(X_train, y_train)\n\nprint(svc_search.best_params_)\n\nsvc_scores = pd.DataFrame(cross_val_score(svc_search.best_estimator_, X_train, y_train, cv=10))","da037bce":"from sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier()\ntree_param_grid = {\"min_samples_split\" : [2, 20, 40], \"min_samples_leaf\": [1, 10, 20]}\n\ntree_search = GridSearchCV(tree, tree_param_grid)\n\ntree_search.fit(X_train, y_train)\n\nprint(tree_search.best_params_)\n\ntree_scores = pd.DataFrame(cross_val_score(tree_search.best_estimator_, X_train, y_train, cv=10))","cb66a4b9":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier()\nrf_param_grid = {\"n_estimators\" : [10, 50, 100], \"max_features\" : [5, 10, 20]}\n\nrf_search = GridSearchCV(rf, rf_param_grid)\n\nrf_search.fit(X_train, y_train)\n\nprint(rf_search.best_params_)\n\nrf_scores = pd.DataFrame(cross_val_score(rf_search.best_estimator_, X_train, y_train, cv=10))","a25d69ae":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn_param_grid = {\"n_neighbors\" : [3, 5, 10]}\n\nknn_search = GridSearchCV(knn, knn_param_grid)\n\nknn_search.fit(X_train, y_train)\n\nprint(knn_search.best_params_)\n\nknn_scores = pd.DataFrame(cross_val_score(knn_search.best_estimator_, X_train, y_train, cv=10))","ecc1536c":"print(\"Logistic Regression \\n\", lr_scores.describe())\nprint(\"\\nSVC \\n\", svc_scores.describe())\nprint(\"\\nDecision Tree \\n\", tree_scores.describe())\nprint(\"\\nRandom Forest \\n\", rf_scores.describe())\nprint(\"\\nKNN \\n\", knn_scores.describe())","4fd8ecd1":"sns.displot(lr_scores, legend=False)\nplt.title(\"Logistic Regression\")\nplt.xlabel(\"Accuracy\")\nplt.show()\n\nplt.clf()\nsns.displot(svc_scores, legend=False)\nplt.title(\"SVC\")\nplt.xlabel(\"Accuracy\")\nplt.show()\n\nplt.clf()\nsns.displot(tree_scores, legend=False)\nplt.title(\"Decision Tree\")\nplt.xlabel(\"Accuracy\")\nplt.show()\n\nplt.clf()\nsns.displot(rf_scores, legend=False)\nplt.title(\"Random Forest\")\nplt.xlabel(\"Accuracy\")\nplt.show()\n\nplt.clf()\nsns.displot(knn_scores, legend=False)\nplt.title(\"KNN\")\nplt.xlabel(\"Accuracy\")\nplt.show()","ca09bd4e":"X_test = test_data.drop([\"id\", \"console\", \"use_of_drugs_and_alcohol\"], axis=1, inplace=False)\n\npredictions = le.inverse_transform(svc_search.best_estimator_.predict(X_test))\n\noutput = pd.DataFrame({\"id\": test_data[\"id\"], \"esrb_rating\": predictions})\nprint(output.shape)\nprint(output.to_string())\noutput.to_csv(\"submission.csv\", index=False)","be37f935":"# Selecting the Best Model and Generating Submission Data\nThe distributions paint an interesting picture for the performance of the models. The Logistic Regression model is able to do relatively well, but the plurality of its scores are low. The SVC distribution is similar, although it's centered at a higher score than Logistic Regression and has a higher maximum. The Decision Tree distribution has a plurality of average scores. The Random Forest distribution attains relatively high scores and has more scores on the high end than the low. Finally, the KNN distribution is pitiful in comparison. Most of the scores are lower than the scores for the other models, and the minimum is remarkably low. Of these, SVC, Decision Tree, and Random Forest seem like good possible candidates to generate the final submission. I ultimately decided on using SVC to generate the submission data. ","f4adc3bc":"Of these features, most look they will be of some use. The first feature, console, actually seems to be similar to the case I described earlier where a feature has an even split of 1s and 0s for most ratings. Judging by the summary statistics, some features, like use_of_drugs_and_alcohol, have very few total 1s. Testing model accuracy with and without these questionable features may be useful. \n\nOther features, like strong_janguage and blood_and_gore have very clear associations with certain ratings. Features like this will always be included in the final model. \n\nAs for calculation of correlations between features, it would be both difficult to analyze and, by my thinking, not very fruitful. By definition, features like violence and mild_violence should have a strong correlation, but dropping correlated features in this case does not seem helpful. Consider two correlated features that each have strong predictive power for a game's rating, each being predictive for a different rating (E and T, for instance). If we drop one feature because of its strong correlation with the other, we may still lose useful information. The variable we have left may help us tell whether a game is T or not T, but it won't tell us which of the other ratings it may be. \n\n\nAnother small piece of data transformation is necessary for the models to work properly. When the data is trained, the ESRB rating needs to be label-encoded from their names to numbers that represent that category. This also means that when the model makes predictions, it will predict in the form of numbers. The same encoding that was done to change the ratings into numbers can be done in reverse to make the predicted numbers into the proper ratings.","bbe4da23":"Besides \"id\" and \"esrb_rating\", every entry in every column of both dataframes should be a zero or a one, as they are essentially Boolean values denoting whether a certain element was present in that video game. A value that isn't 0 or 1 might be construed as an outlier or a missing value. ","190a2dc7":"These statistics give us good information about the performance of the models. The best model seems to be SVC. Its mean score is highest, coming close with Random Forest, but its max value is highest by a good margin. It does, however, have the second highest standard deviation, meaning the results may not be consistent. The highest standard deviation belongs to the KNN model, which seems to be the worst performing model.\n\nIt may also be helpful to plot and visualize the distribution of scores across models. ","0252f125":"The \"esrb_rating\" column in the training data could also have non-null objects that we still might consider missing values. The only values that should be possible in that column are RP, EC, E, ET, T, M, and A. It does thankfully appear that the dataset follows these rules:","b606cbbb":"# Building the Models\nNow that we have our data prepared for the models, we need to build and evaluate the models. To model this data, I'll use Logistic Regression, a Support Vector Machine, a Decision Tree, a Random Forest, and K Nearest Neighbors. I'll also search through relevant hyperparameters for each model.","2e788a6d":"# Loading in the Data","f9aa5889":"Since there are no values in either dataset that aren't 0 or 1, they are all valid and contain no missing information or outliers.","1e99a5df":"# Missing Values and Outliers\nIt seems that there are no missing values in either the training or the evaluation datasets. The training dataset has 1421 entries and each column has 1421 non-null values. The evaluation dataset has 474 entries and each column has 474 non-null values. The \"title\" column could have non-null values that are nonsensical or whitespace, but since it isn't used in the evaluation dataset it doesn't even need to be considered. ","5baad3c5":"Kaggle Username: skylarmarosi","a5056693":"# Data Transformations and Feature Selection\/Engineering\nNow that we know all the data is valid, we need to decide what data to use when training the models. If a feature has a 50\/50 split of 0s and 1s for every ESRB rating, it's safe to say that feature will not be useful in determining the rating of unknown games. If a feature has all 0s for three of the ratings and all 1s for the other, that feature can tell us with high certainty that the game belongs to a certain rating.","0d438bdb":"Now that the models have all run through the training data and we've done some hyperparameter tuning, we can look at which model performed the best in cross validation. First we'll look at the summary statistics of the scores. "}}