{"cell_type":{"621dd073":"code","2d489597":"code","5cc446ce":"code","c9938aec":"code","336f90da":"code","89d68e0c":"code","b7b1ed04":"code","2c3060d2":"code","a5cbc207":"code","f107f1af":"code","ba933ddd":"code","99e352c9":"code","a8730815":"code","12f9a9ff":"code","0b11b878":"code","9c3624d0":"code","c7412b3c":"code","e5232b13":"code","77849aac":"code","07c20419":"code","5f274abe":"code","636f4245":"code","5d380f37":"code","a8449287":"code","76f1bcaf":"code","16203056":"code","a722c2c7":"code","cd8c2303":"code","91281d30":"code","89760c28":"code","c037b850":"code","1c7dc088":"code","8a99a3a8":"code","1764494b":"markdown","5eb46ab0":"markdown","2000dc90":"markdown","908011fc":"markdown","6845ba72":"markdown","a970d8ed":"markdown","e3a73d94":"markdown","76463ab6":"markdown","2ef7aedb":"markdown","2cb8fde7":"markdown","0f4a6ad4":"markdown","377647d1":"markdown","628fa1cc":"markdown","b895bbbc":"markdown","aa51db2f":"markdown","e75e0d2d":"markdown","e0ae5469":"markdown","18be795e":"markdown","ad7a00b2":"markdown","5152de04":"markdown","0a34f977":"markdown","cc48fa95":"markdown"},"source":{"621dd073":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2d489597":"from IPython.display import Image\nImage('..\/input\/image-digit\/mnist-3.0.1.png')","5cc446ce":"Image('..\/input\/minions\/minions.jpg')","c9938aec":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","336f90da":"train=pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntrain.head()","89d68e0c":"test=pd.read_csv('..\/input\/digit-recognizer\/test.csv')\ntest.head()","b7b1ed04":"train.shape","2c3060d2":"train.columns","a5cbc207":"test.columns","f107f1af":"l=train['label']\ntrain_df=train.drop(\"label\", axis=1)","ba933ddd":"l.shape, train_df.shape","99e352c9":"plt.figure(figsize=(7,7))\nidx = 1\n\ngrid_data = train_df.iloc[idx].to_numpy().reshape(28,28)  # reshape from 1d to 2d pixel array\nplt.imshow(grid_data, interpolation = \"none\", cmap = \"gray\")\nplt.show()\n\nprint(l[idx])","a8730815":"plt.figure(figsize=(7,7))\nidx = 3\n\ngrid_data = train_df.iloc[idx].to_numpy().reshape(28,28)  # reshape from 1d to 2d pixel array\nplt.imshow(grid_data, interpolation = \"none\", cmap = \"gray\")\nplt.show()\n\nprint(l[idx])","12f9a9ff":"\nlabels=l.head(15000)\ndata=train_df.head(15000)\n","0b11b878":"# Data-preprocessing: Standardizing the data\n\nfrom sklearn.preprocessing import StandardScaler\nstandarized_data=StandardScaler().fit_transform(data)\nprint(standarized_data.shape)","9c3624d0":"sample_data=standarized_data\n\ncovar_matrix=np.matmul(sample_data.T,sample_data)\n\nprint(\"the shape of variance matrix\",covar_matrix.shape)","c7412b3c":"from scipy.linalg import eigh\n\n# the parameter 'eigvals' is defined (low value to heigh value) \n# eigh function will return the eigen values in asending order\n# this code generates only the top 2 (782 and 783) eigenvalues.\n\nvalues, vectors=eigh(covar_matrix, eigvals=(782,783))\n\nprint(\"Shape of eigen values\", vectors.shape)\nvectors=vectors.T\n\nprint(\"updated shape of eigen vector\", vectors.shape)\n\n","e5232b13":"import matplotlib.pyplot as plt\nnew_coordinates = np.matmul(vectors, sample_data.T)\n\nprint (\" resultanat new data points' shape \", vectors.shape, \"X\", sample_data.T.shape,\" = \", new_coordinates.shape)","77849aac":"\n# appending label to the 2d projected data\nnew_coordinates = np.vstack((new_coordinates, labels)).T\n\n# creating a new data frame for ploting the labeled points.\ndataframe = pd.DataFrame(data=new_coordinates, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\nprint(dataframe.head())","07c20419":"df=pd.DataFrame()\ndf[\"1st\"]=[-5.558661,-5.043558,6.193635 ,19.305278]\ndf[\"2nd\"]=[-1.558661,-2.043558,2.193635 ,9.305278]\ndf['label']=[1,2,3,4]","5f274abe":"import seaborn as sns\nsns.FacetGrid(df, hue=\"label\", size=10).map(plt.scatter,'1st','2nd').add_legend()\nplt.show()","636f4245":"sns.scatterplot(x=\"1st\",y=\"2nd\",hue=\"label\", data=df)","5d380f37":"sns.FacetGrid(dataframe,hue=\"label\", size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.show()","a8449287":"sns.scatterplot(x=\"1st_principal\", y=\"2nd_principal\", legend=\"full\", hue=\"label\", data=dataframe)","76f1bcaf":"# initializing the pca\n\nfrom sklearn import decomposition \npca=decomposition.PCA()","16203056":"# configuring the parameteres\n# the number of components = 2\n\npca.n_components=2\npca_data=pca.fit_transform(sample_data)\n\nprint(\"shape of PCA reduced shape\", pca_data.shape)","a722c2c7":"from sklearn.manifold import TSNE\n\ndata_1000=standarized_data[0:1000,:]\nlabel_1000=labels[0:1000]\n\nmodel=TSNE(n_components=2,random_state=0)\n# configuring the parameteres\n# the number of components = 2\n# default perplexity = 30\n# default learning rate = 200\n# default Maximum number of iterations for the optimization = 1000\n\ntsne_data=model.fit_transform(data_1000)\n\ntsne_data=np.vstack((tsne_data.T, label_1000)).T\ntsne_df=pd.DataFrame(data=tsne_data,columns=(\"dim_1\",\"dim_2\", \"label\"))\n\nsns.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter, \"dim_1\",\"dim_2\").add_legend()\nplt.show()","cd8c2303":"model=TSNE(n_components=2, random_state=0, perplexity=50)\ntsne_data=model.fit_transform(data_1000)\n\n# creating a new data fram which help us in ploting the result data\ntsne_data=np.vstack((tsne_data.T, label_1000)).T\ntsne_df=pd.DataFrame(data=tsne_data, columns=(\"Dim_1\",\"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsns.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter,\"Dim_1\",\"Dim_2\").add_legend()\nplt.show()","91281d30":"#with different perplexity\nmodel=TSNE(n_components=2, random_state=0, perplexity=30)\ntsne_data=model.fit_transform(data_1000)\n\n# creating a new data fram which help us in ploting the result data\ntsne_data=np.vstack((tsne_data.T, label_1000)).T\ntsne_df=pd.DataFrame(data=tsne_data, columns=(\"Dim_1\",\"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsns.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter,\"Dim_1\",\"Dim_2\").add_legend()\nplt.show()","89760c28":"#with different perplexity\nmodel=TSNE(n_components=2, random_state=0, perplexity=10)\ntsne_data=model.fit_transform(data_1000)\n\n# creating a new data fram which help us in ploting the result data\ntsne_data=np.vstack((tsne_data.T, label_1000)).T\ntsne_df=pd.DataFrame(data=tsne_data, columns=(\"Dim_1\",\"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsns.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter,\"Dim_1\",\"Dim_2\").add_legend()\nplt.show()","c037b850":"#with different perplexity\nmodel=TSNE(n_components=2, random_state=0, perplexity=5)\ntsne_data=model.fit_transform(data_1000)\n\n# creating a new data fram which help us in ploting the result data\ntsne_data=np.vstack((tsne_data.T, label_1000)).T\ntsne_df=pd.DataFrame(data=tsne_data, columns=(\"Dim_1\",\"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsns.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter,\"Dim_1\",\"Dim_2\").add_legend()\nplt.title(\"With Perplexity 5\")\n\nplt.show()","1c7dc088":"#with different perplexity\nmodel=TSNE(n_components=2, random_state=0, perplexity=80)\ntsne_data=model.fit_transform(data_1000)\n\n# creating a new data fram which help us in ploting the result data\ntsne_data=np.vstack((tsne_data.T, label_1000)).T\ntsne_df=pd.DataFrame(data=tsne_data, columns=(\"Dim_1\",\"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsns.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter,\"Dim_1\",\"Dim_2\").add_legend()\nplt.title(\"With Perplexity 80\")\nplt.show()","8a99a3a8":"Image('..\/input\/thank-you\/download.jpg', height=1000, width=1000)","1764494b":"# [Seaborn](https:\/\/seaborn.pydata.org\/)","5eb46ab0":"#  [PCA using Scikit-Learn](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html)\n* [DIfference between PCS and t-SNE](https:\/\/www.kaggle.com\/questions-and-answers\/237497)","2000dc90":"# Ploting the 2d data points with seaborn\n* [Seaborn](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.linalg.eig.html)\n","908011fc":"# Covariance Matrix ","6845ba72":"# Observation \n* Insted of calculating as done above we can follow eassy steps\n* As mentioned down below\n* Eassy PCA implementation ","a970d8ed":"# Columns descriptions\n* There are 785 columns \n","e3a73d94":"# Projecting the original data sample on the plane \n* Formed by two principal eigen vectors by vector-vector multiplication.\n* **Appending label to the 2d projected data**","76463ab6":"# 2D Visualization using PCA\n* Principal Component Analysis(PCA), is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n* the idea of PCA is simple \u2014 reduce the number of variables of a data set, while preserving as much information as possible.\n# STEP BY STEP EXPLANATION OF PCA\n* STEP 1: STANDARDIZATION\n* STEP 2: COVARIANCE MATRIX COMPUTATION\n* STEP 3: COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX\n* STEP-4: IDENTIFY THE PRINCIPAL COMPONENTS\n* STEP 5: FEATURE VECTOR\n* STEP 6: VISUALIZE THE FEATURE","2ef7aedb":"# Dataset description\n* The data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine.\n* Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total.\n* Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker\n* This pixel-value is an integer between 0 and 255, inclusive.\n* The training data set, (train.csv), has 785 columns.\n* The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n* Each pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive.\n* To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. \n* Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).\n","2cb8fde7":"# Size and shape of the dataset ","0f4a6ad4":"# Conclusion\n* Perplexity is Hyper parameters \n* With different Perplexity we encounter the different visualization\n* Here is the some useful resources \n* [Resource 1](https:\/\/ranasinghiitkgp.medium.com\/t-sne-visualization-of-high-dimension-mnist-dataset-48fb23d1bafd)\n* [Resorce 2](https:\/\/distill.pub\/2016\/misread-tsne\/)","377647d1":"# Mnist Dataset ","628fa1cc":"# Table of content \n* Panda\n* Numpy \n* [Matplotlib](https:\/\/matplotlib.org\/)\n* [Seaborn](https:\/\/seaborn.pydata.org\/)\n* [Eigen Value](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.linalg.eig.html)\n* [PCA](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html)\n* [t-SNE](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.manifold.TSNE.html)\n* Conclusion","b895bbbc":"**Obs** - The visualization is not that good","aa51db2f":"**Inference** -\n* We can visualize the datset \n* But it is not that claer\n* 0s and 9's are well separated \n* But rest are overlapping with each other\n* PCA helps us to reduce the dimension \n* But the visulization is not upto that label for higher dimension \n* That's why We gonna use T-SNE \n * [PCA vs t-SNE](https:\/\/www.kaggle.com\/questions-and-answers\/237497)","e75e0d2d":" # [t-SNE using Scikit-Learn](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.manifold.TSNE.html)\n \n","e0ae5469":"# [Two eigen-values and corresponding eigen-vectors](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.linalg.eig.html) \n* As we can visualize only 2D data .\n* It is not possible to visualize the more number of dimension data\n* The purpose is to visalize the data and identify the digits","18be795e":"# Visualization \n* Imshow- Helps us to show the image","ad7a00b2":"# Dropping the columns in  order to obtains the insight ","5152de04":"# Change perplexity value","0a34f977":"# Standarization of the feature ","cc48fa95":"# Pawry time - Bring out all the Library "}}