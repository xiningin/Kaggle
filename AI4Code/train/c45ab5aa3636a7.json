{"cell_type":{"93c628a6":"code","4b50875b":"code","400482e8":"code","c538c39f":"code","2983a522":"code","d479d781":"code","a31d4ee8":"code","1640f182":"code","489e551c":"code","7bc3c5c3":"code","2238141b":"code","543226e8":"code","c390ede4":"code","6146d08e":"code","a9fc729c":"code","b875cc4a":"code","685cfb3d":"code","adeecea0":"code","a9c68142":"code","8dd73a55":"code","3fabe623":"code","3d2cd680":"code","8d88abb6":"code","541e41bd":"code","c17b800b":"code","bbf26eb6":"code","7d204638":"code","72e09b37":"code","7310dc0d":"code","c802402c":"code","40aee3a0":"code","1889bbef":"markdown","b4ead18b":"markdown","3986cd1f":"markdown","ed74dcee":"markdown","7d59b745":"markdown","c2908d04":"markdown","a0beabf8":"markdown","f2b4a236":"markdown","bc47a415":"markdown","2a957895":"markdown","1c5d8f65":"markdown","ca5f82bc":"markdown","09ab1699":"markdown","86001353":"markdown"},"source":{"93c628a6":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport nltk\nimport re\nimport os\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","4b50875b":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndf= pd.read_csv('\/kaggle\/input\/reddit-vaccine-myths\/reddit_vm.csv')\ndf.head(5)","400482e8":"df.info()","c538c39f":"df.isnull().sum()","2983a522":"df.describe()","d479d781":"df['year'] = pd.DatetimeIndex(df['timestamp']).year\ndf['month'] = pd.DatetimeIndex(df['timestamp']).month","a31d4ee8":"df.head()","1640f182":"pearsoncorrelation = df.corr(method = 'pearson')\nsns.heatmap(pearsoncorrelation,\n           xticklabels = pearsoncorrelation.columns,\n           yticklabels = pearsoncorrelation.columns,\n           cmap = 'RdBu_r',\n           annot = True,\n           linewidth = 0.5)","489e551c":"sns.heatmap(df.isnull())","7bc3c5c3":"sns.kdeplot(df['comms_num'],shade = True , color = 'Blue')","2238141b":"sns.countplot(data = df , x = 'year')","543226e8":"sns.countplot(data = df, x = 'month')","c390ede4":"sns.countplot(data = df, x = 'month' , hue = 'year')\nplt.style.use('seaborn-poster')","6146d08e":"sns.pairplot(df)","a9fc729c":"df.drop(columns=['comms_num','id','url','created','timestamp'], inplace=True)\ndf.head()","b875cc4a":"df['title'] = df['title'].astype(str)\ndf['body'] = df['body'].astype(str)\n\ndf['title'] = df['title'].apply(lambda word : \" \".join(word.lower() for word in word.split()))\ndf['body'] = df['body'].apply(lambda word: \" \".join(word.lower() for word in word.split()))","685cfb3d":"def remove_pattern(input_txt , pattern):\n    r = re.findall(pattern, input_txt)\n    for word in r:\n        input_txt = re.sub(word, \" \", input_txt)\n    return input_txt\ndf.head()","adeecea0":"df['clean_title'] = np.vectorize(remove_pattern)(df['title'],'@[\\w]')\ndf['clean_body'] = np.vectorize(remove_pattern)(df['body'],'@[\\w]')","a9c68142":"#df['clean_title'] = df['clean_title'].str.replace(\"[^a-za-z#]\",\"\")\n#df['clean_body'] = df['clean_body'].str.replace(\"[^a-za-z#]\",\"\")","8dd73a55":"df.head()","3fabe623":"df['clean_title'] = df['clean_title'].apply(lambda x : \" \".join([w for w in x.split() if len(w)>3]))\ndf['clean_body'] = df['clean_body'].apply(lambda x : \" \".join([w for w in x.split() if len(w)>3]))\n\n","3d2cd680":"df.head()","8d88abb6":"tokenized_title = df['clean_title'].apply(lambda x:x.split())\ntokenized_body = df['clean_body'].apply(lambda x:x.split())","541e41bd":"from nltk.stem.porter import PorterStemmer \nstemmer = PorterStemmer()\n\ntokenized_title = tokenized_title.apply(lambda sentence: [stemmer.stem(word) for word in sentence])\ntokenized_body = tokenized_body.apply(lambda sentence : [stemmer.stem(word)for word in sentence])","c17b800b":"for i in range(len(tokenized_title)):\n    tokenized_title[i] = \" \".join(tokenized_title[i])\n    \ndf['clean_title'] = tokenized_title","bbf26eb6":"for i in range(len(tokenized_body)):\n    tokenized_body[i] = \" \".join(tokenized_body[i])\n    \ndf['clean_body'] = tokenized_body","7d204638":"df.drop(columns = ['title','body'],inplace = True)\n","72e09b37":"all_words = \" \".join([sentence for sentence in df['clean_title']])\n\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width = 800 , height = 500 , random_state = 42 , max_font_size = 100).generate(all_words)\n\nplt.figure(figsize = (15,8))\nplt.imshow(wordcloud , interpolation = 'bilinear')\nplt.axis('off')\nplt.show()","7310dc0d":"all_words = \" \".join([sentence for sentence in df['clean_body']])\n\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width = 800 , height = 500 , random_state = 42 , max_font_size = 100).generate(all_words)\n\nplt.figure(figsize = (15,8))\nplt.imshow(wordcloud , interpolation = 'bilinear')\nplt.axis('off')\nplt.show()","c802402c":"all_words = \"\".join([sentence for sentence in df['clean_body'][df['score']>=5]])\n\nwordcloud = WordCloud(width = 800 , height = 500 , random_state = 42 , max_font_size = 100).generate(all_words)\n\nplt.figure(figsize = (15,8))\nplt.imshow(wordcloud , interpolation = 'bilinear')\nplt.axis('off')\nplt.show()","40aee3a0":"all_words = \"\".join([sentence for sentence in df['clean_body'][df['score']<5]])\n\nwordcloud = WordCloud(width = 800 , height = 500 , random_state = 42 , max_font_size = 100).generate(all_words)\n\nplt.figure(figsize = (15,8))\nplt.imshow(wordcloud , interpolation = 'bilinear')\nplt.axis('off')\nplt.show()","1889bbef":"Extracting year and month from timestamp","b4ead18b":"Tokenizing the cleaned columns , will be working on that only","3986cd1f":"Want to know the most impactful using the score of bodies","ed74dcee":"statistically impact","7d59b745":"Let's see the data","c2908d04":"Pearson correlation to know the best correlated columns , then we can drop the most in score","a0beabf8":"Don't want the duplicates","f2b4a236":"setting the bodies in lower cases","bc47a415":"Some visualizations ","2a957895":"Dropping the columns not required","1c5d8f65":"so many null values in url and body","ca5f82bc":"Need to improve these wordclouds now","09ab1699":"removing the patterns if any","86001353":"Stemming the similar words"}}