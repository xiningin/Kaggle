{"cell_type":{"7b1bcbf0":"code","368e1396":"code","f234f745":"code","9b02e556":"code","89481bf3":"code","21d04453":"code","907f0d79":"code","6b7562cf":"code","10170856":"code","e8d2b593":"code","46870e41":"code","7ffcced5":"code","944c6ae5":"code","235cf8ee":"code","f0aad809":"code","17af4987":"code","b2bd163e":"code","85ac37ed":"code","20e0224a":"code","d47dfb4d":"code","7f99db4d":"code","33295084":"code","a05f49fa":"markdown","568ee1a7":"markdown","65c80e2c":"markdown","6ba50c17":"markdown","bbee02cb":"markdown","6894559c":"markdown","428568c1":"markdown","984659ce":"markdown","3e8c37e8":"markdown","f1152cb3":"markdown","dba0d148":"markdown","dd23931b":"markdown","59c284ba":"markdown","2b08ef37":"markdown","e1ffea65":"markdown","d0b5df83":"markdown","fee900bf":"markdown","cb9a8bc6":"markdown","00f229a5":"markdown","69a33564":"markdown","0a690022":"markdown","b39c00c6":"markdown","7e9a161e":"markdown","f8e992b3":"markdown","bb7fae02":"markdown","23ca7448":"markdown","173d40e6":"markdown","353f0965":"markdown","1e045483":"markdown","8d4dd512":"markdown","3c733ed2":"markdown","8eb1afd6":"markdown","ae13800c":"markdown"},"source":{"7b1bcbf0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport pycountry\nlemmatizer = WordNetLemmatizer()\nstopword = set(stopwords.words('english'))","368e1396":"!pip install twython\nnltk.download('vader_lexicon')","f234f745":"df = pd.read_csv('..\/input\/omicron-rising\/omicron.csv')\n\n# first five rows\ndf.head()","9b02e556":"df.info()","89481bf3":"df.shape","21d04453":"# first 15 tweets\ndf['text'][:10].tolist()","907f0d79":"# change to date time format\ndf['date']=pd.to_datetime(df['date'])\n\n# tweets per hour\ntweets_per_hr = df['date'].dt.strftime('%H').value_counts().sort_index().to_frame(name='Count')\ntweets_per_hr['Hour']=tweets_per_hr.index\n\n# plot\nplt.figure(figsize=(12,7))\nax=sns.barplot(x='Hour', y='Count',data=tweets_per_hr, edgecolor='grey')\nax.bar_label(ax.containers[0])\nplt.title('Tweets per hour', size='xx-large')\nplt.show()","6b7562cf":"df['user_location'].value_counts()[:10]","10170856":"# excluding null values (where location is not specified) in user_location \nlocation = [loc for loc in df['user_location'] if type(loc)==str]\n\n# extracting country names from given location\ncountry_name = [country.name for loc in location for country in pycountry.countries if country.name in loc]\ncountry_name[:5]","e8d2b593":"# dictionary to count number of occurances of each country\ncount={}\nfor country in country_name:\n    count[country] = count.get(country, 0) + 1\n\n# Country vs tweets count\ncountry_df = pd.DataFrame({'Country': list(count.keys()),'Tweets Count': list(count.values())})\ncountry_df = country_df.sort_values(by = 'Tweets Count', ascending=False)\ncountry_df=country_df[:15] # top 15 countries\n\n# plot the data\nplt.figure(figsize=(20,8))\nplt.title('Country vs Tweets Count', size='xx-large')\nax = sns.barplot(x='Country', y='Tweets Count',data=country_df, palette='Blues_r', edgecolor='grey');\nax.bar_label(ax.containers[0])\nplt.show()","46870e41":"# preprocess text\ndef preprocess(text):\n    \n    # remove new lines\n    text = text.replace('\\n', ' ')\n    \n    # remove links\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', ' ', text)\n    \n    # remove hashtags at the end of text\n    text = re.sub('#(?!(?:hashtag)\\b)[\\w-]+(?=(?:\\s+#[\\w-]+)*\\s*$)', '',text)\n    \n    # remove handles\n    text = re.sub('@[\\w]+', '',text)\n    \n    # remove punctuations\n    punc ='''.?!,:;-_\u2014[](){}'\"`~|\\\/@#$%^&+=*'''\n    for i in text:\n        if i in punc:\n            text = text.replace(i, '') \n    \n    # remove extra spaces\n    re.sub(\"\\s\\s+\", \" \", text)\n    \n    # lower case\n    text = text.strip().lower()\n    \n    # lemmatization\n    text = [lemmatizer.lemmatize(word) for word in text.split(' ')]\n    text=\" \".join(text)\n    \n    # stopword removal\n    text = [word for word in text.split(' ') if word not in stopword]\n    text=\" \".join(text)\n\n    # replace covid19 with covid\n    text=text.replace('covid19','covid')\n    \n    return text\n\n\n# remove promotional tweets (with words 'subscribe' and 'subscription')\ndef no_spam(text):\n    if 'subscri' in text:\n        text=''\n    return text","7ffcced5":"# apply functions\ndf['text']= df['text'].apply(preprocess)\ndf['text']= df['text'].apply(no_spam)","944c6ae5":"# function\ndef demoji(text):\n    \n    # frequent emojis whhich will be kept\n    pattern = '\ud83d\ude24|\ud83d\ude21|\ud83d\ude20|\ud83d\ude11|\ud83d\ude44|\ud83e\udd28|\ud83d\ude36|\ud83d\ude31|\ud83d\ude40|\ud83d\ude32|\ud83d\ude13|\ud83d\ude30|\ud83d\ude22|\ud83d\ude25|\ud83d\ude2d|\ud83d\ude2a|\ud83e\udd15|\ud83d\ude14|\ud83d\ude23|\ud83d\ude41|\ud83d\ude12|\ud83d\ude16|\ud83d\ude15|\ud83e\udd74|\ud83e\udd12|\u2639\ufe0f|\ud83d\ude1e|\ud83d\ude37|\ud83e\udd27|\ud83d\ude27|\ud83d\ude28|\ud83d\ude29|\ud83e\udd7a|\ud83d\ude26|\ud83d\ude06|\ud83d\ude00|\ud83e\udd2d|\ud83e\udd29|\ud83d\ude0c|\ud83e\udd70|\ud83d\ude01|\ud83d\ude18|\ud83d\ude02|\ud83d\ude05|\ud83d\ude0a|\ud83d\ude1d|\ud83d\ude19|\ud83d\ude07'\n    for word in text:    \n        if re.match(pattern, word):\n            continue\n            \n        # remove all other non ascii characters\n        text=text.replace(word, re.sub('[^\\x00-\\x7f]','', word)).strip()\n        \n    return text\n\n# apply function\ndf['text']= df['text'].apply(demoji)","235cf8ee":"# funtion\ndef convert(text):\n    \n    # dictionary of emoji with their meaning\n    d = {'\ud83d\ude24':'frustrated','\ud83d\ude21':'angry','\ud83d\ude20':'angry','\ud83d\ude31':'horrified','\ud83d\ude40':'shock','\ud83d\ude32':'shock','\ud83d\ude44':'disapproval',\n         '\ud83e\udd28':'suspicion','\ud83d\ude36':'disappointment','\ud83d\ude13':'sad','\ud83d\ude30':'sad','\ud83d\ude22':'sad','\ud83d\ude25':'sad','\ud83d\ude2d':'sad','\ud83d\ude2a':'sad',\n         '\ud83e\udd15':'sad','\ud83d\ude14':'sad','\ud83d\ude23':'sad','\ud83d\ude41':'sad','\ud83d\ude12':'sad','\ud83d\ude16':'sad','\ud83d\ude15':'sad','\ud83e\udd74':'sad','\ud83e\udd12':'sad','\u2639\ufe0f':'sad',\n         '\ud83d\ude1e':'sad','\ud83d\ude37':'sick','\ud83e\udd27':'sick','\ud83d\ude27':'sad','\ud83d\ude28':'sad','\ud83d\ude29':'sad','\ud83e\udd7a':'sad','\ud83d\ude26':'sad','\ud83d\ude2b':'sad',\n         '\ud83d\ude06':'happy','\ud83d\ude00':'smile','\ud83e\udd2d':'embarrassment','\ud83e\udd29':'exciting','\ud83e\udd70':'affection','\ud83d\ude01':'smile','\ud83d\ude02':'laugh',\n         '\ud83d\ude05':'nervousness','\ud83d\ude0a':'smile','\ud83d\ude1d':'fun','\ud83d\ude19':'affection','\ud83d\ude07':'blessed'}\n    \n    for emoji, sentiment in d.items():\n        text=text.replace(emoji, sentiment)\n    return text\n\n# apply function\ndf['senti_text']= df['text'].apply(convert)","f0aad809":"# creating the text variable\ntext = \" \".join(tweet for tweet in df.text)\n\n# Creating word_cloud with text as argument in .generate() method\nword_cloud = WordCloud(collocations=False, background_color='white',\n                       max_words=50, stopwords=STOPWORDS, min_word_length=4,\n                       width=2048, height=1080).generate(text)\n\n# Display the generated Word Cloud\nplt.figure(figsize=(12,8))\nplt.imshow(word_cloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Omicron Tweets Word Cloud\\n', size='x-large')\nplt.savefig('.\/omicron_wordcloud.jpg',dpi=720) # saving the image\nplt.show()","17af4987":"# word count\nword_count = [len(text.split()) for text in df.text]\ndf['word_count'] = word_count\n\n# plot\nplt.figure(figsize=(15,7))\nax=sns.histplot(x='word_count', data=df, bins=20, color='#00acee')\nax.bar_label(ax.containers[0])\nplt.title('Number of words in text',size='xx-large')\nplt.xlabel('No:of words')\nplt.ylabel('No:of tweets')\nplt.show()","b2bd163e":"# excluding text with less than 3 words\ndf=df[df['word_count']>2]\n\n# excluding tweets with more than 16 words\ndf=df[df['word_count']<17]","85ac37ed":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\nSIA = SentimentIntensityAnalyzer()\n\ndf[\"Positive\"] = [SIA.polarity_scores(i)[\"pos\"] for i in df[\"senti_text\"]]\ndf[\"Neutral\"] = [SIA.polarity_scores(j)[\"neu\"] for j in df[\"senti_text\"]]\ndf[\"Negative\"] = [SIA.polarity_scores(k)[\"neg\"] for k in df[\"senti_text\"]]\n\ndf1 = df[[\"text\", \"Positive\",\"Neutral\", \"Negative\"]]\ndf1.head()","20e0224a":"# sums of sentiment scores\npos = sum(df1[\"Positive\"])\nneu = sum(df1[\"Neutral\"])\nneg = sum(df1[\"Negative\"])\n\n# calculates sentiment score\ndef sentiment_score(pos, neu, neg):\n    if (pos>neu) and (pos>neg):\n        print(\"Positive :)\")\n    elif (neu>pos) and (neu>neg):\n        print(\"Neutral :|\")\n    else:\n        print(\"Negative :(\")\n        \nsentiment_score(pos, neu, neg)","d47dfb4d":"# counting positive, neutral and negative tweets\n\nsentiments_nltk = []\n\nfor tweet in df.senti_text:\n    sentiment_dict = SIA.polarity_scores(tweet)\n    sentiment_dict.pop('compound', None)\n    sentiments_nltk.append(max(sentiment_dict , key=sentiment_dict.get))\n    \ndf['sentiment_nltk'] = sentiments_nltk\ndf['sentiment_nltk'].value_counts()","7f99db4d":"pos_tweets = \" \".join(sentiment for sentiment in df[df['sentiment_nltk']=='pos']['text'])\n\n# Creating word cloud of positive tweets\nstopwords_p = STOPWORDS\nstopwords_p.update(('omicron','covid', 'u','ha','amp','one','people','variant','mask'))\nword_cloud1 = WordCloud(collocations=False, background_color='white',\n                       max_words=20, stopwords=stopwords_p, #min_word_length=4,\n                       width=2048, height=1080).generate(pos_tweets)\n\n# Display the generated Word Cloud\nplt.figure(figsize=(10,5))\nplt.imshow(word_cloud1, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Positive Tweets Word Cloud\\n', size='x-large')\nplt.savefig('.\/wc_positive.jpg',dpi=720)\n\nplt.show()","33295084":"neg_tweets = \" \".join(sentiment for sentiment in df[df['sentiment_nltk']=='neg']['text'])\n\n# Creating word cloud of positive tweets\nstopwords_n = STOPWORDS\nstopwords_n.update(('vaccine','news','im','time','world','travel')) # \nword_cloud2 = WordCloud(collocations=False, background_color='white',\n                       max_words=20, stopwords=stopwords_n,\n                       width=2048, height=1080).generate(neg_tweets)\n\n# Display the generated Word Cloud\nplt.figure(figsize=(10,5))\nplt.imshow(word_cloud2, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Negative Tweets Word Cloud\\n', size='x-large')\nplt.savefig('.\/wc_negative.jpg',dpi=720)\nplt.show()","a05f49fa":"<p style=\"font-size:115%\"><strong>Sentiment analysis<\/strong> is the computational task of automatically determining what feelings a writer is expressing in text. <strong>Sentiment<\/strong> is often framed as a binary distinction (positive vs. negative), but it can also be a more fine-grained, like identifying the specific emotion an author is expressing (like fear, joy or anger).<\/p>\n\n<p style=\"font-size:115%\">Sentiment analysis is used for many applications, especially in business intelligence. Some examples of applications for sentiment analysis include:\n<ul>\n    <li><p style=\"font-size:115%\">Analyzing the social media discussion around a certain topic<\/p><\/li>\n    <li><p style=\"font-size:115%\">Evaluating survey responses<\/p><\/li>\n    <li><p style=\"font-size:115%\">Determining whether product reviews are positive or negative<\/p><\/li>\n<\/ul>    \n<\/p>\n<p style=\"font-size:115%\">Sentiment analysis is not perfect, and as with any automatic analysis of language, you will have errors in your results. It also cannot tell you why a writer is feeling a certain way. However, it can be useful to quickly summarize some qualities of text, especially if you have so much text that a human reader cannot analyze all of it.<\/p>\n\n<p style=\"font-size:115%\"><strong>Valence Aware Dictionary for sEntiment Reasoning<\/strong>, (or VADER), is a NLP algorithm that blended a sentiment lexicon approach as well as grammatical rules and syntactical conventions for expressing sentiment polarity and intensity. Vader is an open-sourced package within the Natural Language Toolkit (NLTK). <strong>NLTK.Vader<\/strong> is one of the more popular tools for sentiment analysis.<\/p>","568ee1a7":"<ul>\n    <li><h4>Tweets per hour<\/h4><\/li>\n<\/ul>","65c80e2c":"![](http:\/\/i.imgur.com\/hkZ7WPE.jpg)","6ba50c17":"Reference\n1. https:\/\/www.kaggle.com\/gpreda\/omicron-is-rising\n2. https:\/\/www.kaggle.com\/ludovicocuoghi\/how-are-people-reacting-to-omicron-on-twitter#NLTK-Sentiment-Analysis ","bbee02cb":"<ul>\n    <li><h4>Text column<\/h4><\/li>\n<\/ul>","6894559c":"<ul>\n    <li><h4>Load data<\/h4><\/li>\n<\/ul>","428568c1":"<p style=\"font-size:115%\">Now, we clean then text using a user defined function.<\/p>","984659ce":"<ul>\n    <li><h4>Number of words in text<\/h4><\/li>\n<\/ul>","3e8c37e8":"<h1 style=\"color:#189AB4;\"><strong>Text Preprocessing<\/strong><\/h1>","f1152cb3":"<p style=\"font-size:115%\">So most of the opinions were <strong>neutral<\/strong>, which means that people were <strong>sharing information<\/strong> about the Omicron variant instead of sharing any positive or negative opinions.<\/p>","dba0d148":"<ul>\n    <li><h4>Word Cloud for Negative Tweets<\/h4><\/li>\n<\/ul>","dd23931b":"#### Work in progress...","59c284ba":"<h1 style=\"color:#189AB4;\"><strong>Introduction<\/strong><\/h1>\n","2b08ef37":"<p style='font-size:115%'>Now we replace the emojis with their meaning<\/p>","e1ffea65":"<p style=\"font-size:115%\">We will have to extract country names from the user location<\/p>","d0b5df83":"<p style=\"font-size:115%\">\n    On 26 November 2021, WHO designated the variant B.1.1.529 a variant of concern, named <strong>Omicron<\/strong>, on the advice of WHO\u2019s Technical Advisory Group on Virus Evolution (TAG-VE).  This decision was based on the evidence presented to the TAG-VE that Omicron has several mutations that may have an impact on how it behaves, for example, on how easily it spreads or the severity of illness it causes. (<a href=\"https:\/\/www.who.int\/news\/item\/28-11-2021-update-on-omicron\">  source <\/a>)<\/p>\n    \n    \n<p style=\"font-size:115%\">In this project, we are applying sentiment analysis on the Twitter tweets that have hashtag <strong>#Omicron<\/strong><\/p>\n","fee900bf":"<ul>\n    <li><h4>Emoji<\/h4><\/li>\n<\/ul>","cb9a8bc6":"<ul>\n    <li><h4>Word Cloud<\/h4><\/li>\n<\/ul>","00f229a5":"<p style=\"font-size:115%\">In sentiment analysis use case, we need not remove the emojis or emoticons as it will convey some important information about the sentiment. Here we will remove all the non ascii characters except some common emojis<\/p>","69a33564":"<ul>\n    <li><h4>User locations<\/h4><\/li>\n<\/ul>","0a690022":"<ul>\n    <li><h4>Import libraries<\/h4><\/li>\n<\/ul>","b39c00c6":"<ul>\n    <li><h4>Basic text cleaning<\/h4><\/li>\n<\/ul>","7e9a161e":"<ul>\n    <li><h4>Word Cloud for Positive Tweets<\/h4><\/li>\n<\/ul>","f8e992b3":"<h1 style=\"color:#189AB4;\"><strong>Sentiment Analysis<\/strong><\/h1>","bb7fae02":"<p style=\"font-size:115%\">Dataset has <strong>60168<\/strong> rows and <strong>16<\/strong> columns.<\/p>","23ca7448":"<ul>\n    <li><h4>Overall Sentiment<\/h4><\/li>\n<\/ul>","173d40e6":"<ul>\n    <li><h4>Concise summary of the data<\/h4><\/li>\n<\/ul>","353f0965":"<ul>\n    <li><h4>Shape of the data<\/h4><\/li>\n<\/ul>","1e045483":"<p style=\"font-size:115%\">We will exclude some tweets<\/p>\n","8d4dd512":"#### **Thank You**","3c733ed2":"<h1 style=\"color:#189AB4;\"><strong>Exploratory Data Analysis<\/strong><\/h1>","8eb1afd6":"<p style=\"font-size:115%\">Text needs to be preprocessed, which will be taken care later<\/p>","ae13800c":"<p style=\"font-size:115%\">Most of the tweets are from <strong>India<\/strong><\/p>"}}