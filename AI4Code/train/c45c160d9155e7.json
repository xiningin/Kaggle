{"cell_type":{"6bc7feb5":"code","e03d09e8":"code","7a18b1a5":"code","ea2b8a9a":"code","f20339af":"code","784f6b27":"code","76269889":"code","50c42e1e":"code","df8185b2":"code","f01d5275":"code","47333dd1":"code","9939b420":"code","e1b7427f":"code","fdc86b0e":"code","206c8a30":"code","e55efeb5":"markdown","20d1b3aa":"markdown","c969f58b":"markdown","8bf99960":"markdown"},"source":{"6bc7feb5":"# If working in google colab need to run to make the data accessible \n\n# !pip install pytorch_pretrained_bert\n# from google.colab import drive\n# drive.mount('\/content\/gdrive')","e03d09e8":"!pip install pytorch_pretrained_bert # If running in kaggle kernel make sure internet is enabled (under settings\/Internet)\nimport torch\nfrom torch.optim import Adam\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom sklearn.model_selection import train_test_split\nfrom pytorch_pretrained_bert import BertTokenizer, BertConfig\nfrom pytorch_pretrained_bert import BertForSequenceClassification, BertAdam\n\nimport pandas as pd\nimport numpy as np","7a18b1a5":"# init_checkpoint_pt = '..\/pretrained_models\/bert\/cased_L-12_H-768_A-12\/'\n# DATA_PATH = \"\/content\/gdrive\/My Drive\/Colab Notebooks\/\"\nDATA_PATH = \"..\/input\/\"\nMAX_LEN = 350\nbs = 8","ea2b8a9a":"import pandas as pd\ndf = pd.read_csv(DATA_PATH + 'cp_challenge_train.csv')\ndf = df[:32] # TODO: Comment out - just to make sure everything runs....\nsentences = df.Plot","f20339af":"classes = list(set(df['label'].values))\nclas2idx = {t : i for i,t in enumerate(classes)}\ny = [clas2idx.get(l) for l in df.label.values]","784f6b27":"# tokenize the words\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\ntokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]","76269889":"def trp(l, n):\n    return l[:n] + [0]*(n-len(l))\n\n# bert get's id's and not words, so convert to that and padd (and trim) the sentences to by in MAX_LEN\ninput_ids = [trp(tokenizer.convert_tokens_to_ids(txt), MAX_LEN) for txt in tokenized_texts]\n# We can tell bery where we added words just for padding, it will help him embed better\nattention_masks = [[float(i>0) for i in ii] for ii in input_ids]","50c42e1e":"# split train test, use random_state so it will be the same split :)\ntr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, y, \n                                                            random_state=2018, test_size=0.1)\ntr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n                                             random_state=2018, test_size=0.1)","df8185b2":"# transform the vectors into something pytorch can read\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif device == 'cude':\n    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n\ntr_inputs = torch.tensor(tr_inputs)\nval_inputs = torch.tensor(val_inputs)\ntr_tags = torch.tensor(tr_tags)\nval_tags = torch.tensor(val_tags)\ntr_masks = torch.tensor(tr_masks)\nval_masks = torch.tensor(val_masks)\n\ntrain_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n\nvalid_data = TensorDataset(val_inputs, val_masks, val_tags)\nvalid_sampler = SequentialSampler(valid_data)\nvalid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)","f01d5275":"model = BertForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=len(clas2idx))","47333dd1":"\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.0}\n]\n\noptimizer = Adam(optimizer_grouped_parameters, lr=3e-5)","9939b420":"\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom tqdm import tqdm, trange\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nepochs = 3\nmax_grad_norm = 1.0\nfor _ in trange(epochs, desc=\"Epoch\"):\n    # TRAIN loop\n    model.train()\n    tr_loss = 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    for step, batch in enumerate(train_dataloader):\n        # add batch to gpu\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n#         print(\"forward pass\")\n        loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n#         print(\"backward pass\")\n        loss.backward()\n        # track train loss\n        tr_loss += loss.item()\n        nb_tr_examples += b_input_ids.size(0)\n        nb_tr_steps += 1\n        # gradient clipping\n        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n        # update parameters\n        optimizer.step()\n        model.zero_grad()\n    # print train loss per epoch\n    print(\"Train loss: {}\".format(tr_loss\/nb_tr_steps))\n    # VALIDATION on validation set\n    model.eval()\n    eval_loss, eval_accuracy, eval_f1 = 0, 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n    predictions , true_labels = [], []\n    for batch in valid_dataloader:\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n        with torch.no_grad():\n            tmp_eval_loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n            logits = logits.detach().cpu().numpy()\n            label_ids = b_labels.to('cpu').numpy()\n            predictions.extend([np.argmax(logits, axis=1)])\n            true_labels.append(label_ids)\n            tmp_eval_accuracy = accuracy_score(np.argmax(logits, axis=1), label_ids)\n            tmp_eval_f1 = f1_score(np.argmax(logits, axis=1), label_ids, average='weighted')\n            eval_loss += tmp_eval_loss.mean().item()\n            eval_accuracy += tmp_eval_accuracy\n            eval_f1 += tmp_eval_f1\n            nb_eval_examples += b_input_ids.size(0)\n            nb_eval_steps += 1\n    eval_loss = eval_loss\/nb_eval_steps\n    print(\"Validation loss: {}\".format(eval_loss))\n    print(\"Validation Accuracy: {}\".format(eval_accuracy\/nb_eval_steps))\n    print(\"Validation f1: {}\".format(eval_f1\/nb_eval_steps))\n    torch.save(model, 'bert_classifier')","e1b7427f":"df_test = pd.read_csv(DATA_PATH + 'challenge_testset.csv')\n\ntest_tokenized_texts = [tokenizer.tokenize(sent) for sent in df_test.Plot]\ntest_input_ids = [trp(tokenizer.convert_tokens_to_ids(txt), MAX_LEN) for txt in test_tokenized_texts]\ntest_attention_masks = [[float(i>0) for i in ii] for ii in test_input_ids]\nte_inputs = torch.tensor(test_input_ids)\nte_mask = torch.tensor(test_attention_masks)","fdc86b0e":"test_data = TensorDataset(te_inputs, te_mask)\ntest_sampler = SequentialSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=2)\npredictions = []\nfor batch in test_dataloader:\n    batch = tuple(t.to(device) for t in batch)\n    model_input, mask = batch\n    y_hat_proba = model(model_input, token_type_ids=None, attention_mask=mask)\n    y_hat_proba = y_hat_proba.detach().cpu().numpy()\n    predictions.extend(y_hat_proba)\n#     print(predictions)","206c8a30":"idx2class = {v:k for k,v in clas2idx.items()}\nidx2class[-1] = 'unknown'\n\ny_hat = list(np.argmax(predictions, axis=1))\ndf_test['predictions'] = [idx2class[i] for i in y_hat]\ndf_test[['imdbID', 'predictions']].to_csv(DATA_PATH + 'predictions.csv', index=False)","e55efeb5":"# Make predictions","20d1b3aa":"## Training","c969f58b":"this notebook is based on:  \nhttps:\/\/www.depends-on-the-definition.com\/named-entity-recognition-with-bert\/","8bf99960":"## Prepare data for bert\ntokenize  \npad \/ trim (make all same length)  \nconvert into pytorch objects"}}