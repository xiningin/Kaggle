{"cell_type":{"55c7eff9":"code","1fcd710c":"code","31559d21":"code","2c4303a7":"code","405fb880":"code","62a1c0eb":"code","885a766f":"code","d1527803":"code","5d854f85":"code","cb4895c6":"code","04078d28":"code","bc1b7213":"code","97200082":"code","893667d0":"code","9923a233":"code","0b284280":"code","af31d742":"code","7a14f9bf":"code","88f8c65f":"code","f59f6b2d":"code","17373666":"code","d650e89d":"code","169d3e94":"code","09f25dee":"code","f3487744":"code","4af09f62":"code","2a6e2ab6":"code","47851d80":"code","f1a73fa0":"code","1d039ff5":"code","8c7886c1":"code","60e85480":"code","49d951f3":"code","200e3500":"code","f763c268":"code","f3ddcdcb":"code","9673cfb4":"code","67104380":"code","a56aaf38":"code","07860e49":"code","1e5bd42c":"markdown","21495e28":"markdown","e831ac31":"markdown","ac3b4f5f":"markdown","12185774":"markdown","6524d40e":"markdown","de4a7695":"markdown","25a7eaa0":"markdown","8c2dd734":"markdown","82c83733":"markdown","7a6f22ed":"markdown"},"source":{"55c7eff9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1fcd710c":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(color_codes = True)\n%matplotlib inline\n\n\nfrom sklearn.preprocessing import LabelEncoder,MinMaxScaler , StandardScaler","31559d21":"data = pd.read_csv('\/kaggle\/input\/employee-future-prediction\/Employee.csv')\ndata.head()","2c4303a7":"## check Nan value\nfor i in data.columns:\n    print (i+\": \"+str(data[i].isna().sum()))","405fb880":"count = data.LeaveOrNot.value_counts()\ncount","62a1c0eb":"data.PaymentTier.unique()","885a766f":"data.head()","d1527803":"sns.barplot('City','ExperienceInCurrentDomain',data = data)","5d854f85":"sns.barplot('Education','ExperienceInCurrentDomain',data = data,hue = 'LeaveOrNot',palette = 'rocket')","cb4895c6":"sns.barplot('City','LeaveOrNot',data = data,hue = 'Education',palette = 'gnuplot')","04078d28":"sns.catplot('Gender','ExperienceInCurrentDomain',data = data,kind = 'bar',hue = 'LeaveOrNot',col = 'LeaveOrNot')\n","bc1b7213":"sns.catplot('Gender','Age',data = data,kind = 'bar',hue = 'LeaveOrNot',col = 'LeaveOrNot')","97200082":"sns.scatterplot('City','Age',data = data,hue = 'JoiningYear')","893667d0":"# Some of values are present in categorical form that we have convert into numerical form.\nlabel = LabelEncoder()\n\ndata['Education'] = label.fit_transform(data['Education'])\ndata['City'] = label.fit_transform(data['City'])\ndata['Gender'] = label.fit_transform(data['Gender'])\ndata['EverBenched'] = label.fit_transform(data['EverBenched'])","9923a233":"#Correlation shows you how each value is correlated with another values.\ncorrelation = data.corr()\n\n\nplt.figure(figsize = (20,8))\nsns.heatmap(correlation, annot = True,cmap = 'rocket')","0b284280":"# Values that are negative correlated that may affect on result while building model.\ndata = data.drop(['Gender','PaymentTier'],axis = True)\ndata.head()","af31d742":"x = data.drop('LeaveOrNot',axis = True)\nx","7a14f9bf":"y = data.iloc[:,-1]\ny","88f8c65f":"## Get the Leave and the Not_leave dataset \n\nLeave = data[data['LeaveOrNot']==0]\n\nNot_Leave = data[data['LeaveOrNot']==1]","f59f6b2d":"get_ipython().system('pip install imblearn')","17373666":"# Previsouly in data visualization we saw that data is imbalanced so to manage data we have to first manage data using imbalanced library.\n#!pip install imblearn\n#!pip install Delayed\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import NearMiss\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\nfrom pylab import rcParams","d650e89d":"# Implementing Oversampling for Handling Imbalanced \nsmk = SMOTETomek(random_state=42)\nX_res,y_res=smk.fit_resample(x,y)\n\nX_res.shape,y_res.shape ","169d3e94":"y_res.value_counts()","09f25dee":"# Normalize values.\nstandard = StandardScaler()\n\nstd_x = standard.fit_transform(X_res)","f3487744":"x_final = np.array(std_x)\ny_final = np.array(y_res)","4af09f62":"# Lets Check Validation method for creating train and test data.\nfrom sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=7)\nskf.get_n_splits(X_res, y_final)","2a6e2ab6":" for train_index, test_index in skf.split(x_final, y_final):\n        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n        X_train, X_test = x_final[train_index], x_final[test_index]\n        y_train, y_test = y_final[train_index], y_final[test_index]","47851d80":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(max_iter = 1500)\nclf.fit(X_train,y_train)\ny_predicted = clf.predict(X_test)\nscore = clf.score(X_test,y_test)\nprint(score)","f1a73fa0":"from sklearn.metrics import confusion_matrix\ncnf_matrix = confusion_matrix(y_test, y_predicted)\nnp.set_printoptions(precision=2)\ncnf_matrix","1d039ff5":"from sklearn.svm import SVC,NuSVC\nclf = SVC(kernel ='rbf',gamma = 'auto' )\nclf.fit(X_train,y_train)\ny_predicted = clf.predict(X_test)\nscore = clf.score(X_test,y_test)\nprint(score)","8c7886c1":"from sklearn.metrics import confusion_matrix\ncnf_matrix = confusion_matrix(y_test, y_predicted)\nnp.set_printoptions(precision=2)\ncnf_matrix","60e85480":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz,DecisionTreeRegressor","49d951f3":"clf = DecisionTreeClassifier()","200e3500":"from sklearn.model_selection import RandomizedSearchCV\n#Randomized Search CV\n\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\nsplitter = [\"best\", \"random\"]\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# max_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 23,33]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]\n","f763c268":"# Create the random grid\nrandom_grid = {'max_features': max_features,\n               'splitter':splitter,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\nprint(random_grid)","f3ddcdcb":"# Random search of parameters, using 3 fold cross validation, \n# search across 50 different combinations\nrf_random = RandomizedSearchCV(estimator = clf, param_distributions = random_grid,scoring='accuracy', n_iter = 50, cv = 3, verbose=2, random_state=42, n_jobs = 1)","9673cfb4":"rf_random.fit(X_train,y_train)","67104380":"rf_random.best_params_","a56aaf38":"clf = DecisionTreeClassifier(splitter = 'random',min_samples_split = 2,min_samples_leaf = 5,max_features = 'sqrt',max_depth =  15)\nclf.fit(X_train,y_train)\ny_predicted = clf.predict(X_test)\nscore = clf.score(X_test,y_test)\nprint(score)","07860e49":"from sklearn.metrics import confusion_matrix\ncnf_matrix = confusion_matrix(y_test, y_predicted)\nnp.set_printoptions(precision=2)\ncnf_matrix","1e5bd42c":"<a id=\"3\"><\/a>\n<h1 style='background:#83a9bf; border:0; color:black'><center>DATA VISUALIZATION<\/center><\/h1> ","21495e28":"<a id=\"8\"><\/a>\n<h1 style='background:#83a9bf; border:0; color:black'><center>CONCLUSION<\/center><\/h1> \n\n- We get better result on SVM. \n- you can increase your hyperparameter and play with it.\n- Try with Another Algorithm and check how it will perform and try to improve result.\n- For Solution I would request kindly check on my github profile, as some of the library is not working in kaggle.","e831ac31":"**<span style=\"color:#65634a;\"> If you liked this Notebook, please do upvote.<\/span>**\n\n**<span style=\"color:#65634a;\"> If you have any suggestions or questions, I am all ears!<\/span>**\n\n**<span style=\"color:#65634a;\">Best Wishes!<\/span>**","ac3b4f5f":"- As you can see that Using logistic regression true negative and false Negative are giving more numbers as compared to SVM, Lets try another method and hyperparameter.","12185774":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 style='background:#83a9bf; border:0; color:black'><center>INDEX<\/center><\/h1> ","6524d40e":"<a id=\"7\"><\/a>\n<h1 style='background:#83a9bf; border:0; color:black'><center>CONFUSION MATRIX<\/center><\/h1> ","de4a7695":"<a id=\"2\"><\/a>\n<h1 style='background:#83a9bf; border:0; color:black'><center>IMPORTING DATA<\/center><\/h1> \n","25a7eaa0":"<a id=\"5\"><\/a>\n<h1 style='background:#83a9bf; border:0; color:black'><center>TRAIN-TEST DATA<\/center><\/h1> ","8c2dd734":"<a id=\"6\"><\/a>\n<h1 style='background:#83a9bf; border:0; color:black'><center>BUILT MODEL<\/center><\/h1> ","82c83733":"<a id=\"4\"><\/a>\n<h1 style='background:#83a9bf; border:0; color:black'><center>DATA PREPROCESSING AND DATA MANIPULATING<\/center><\/h1> ","7a6f22ed":"<a id=\"1\"><\/a>\n<h1 style='background:#83a9bf; border:0; color:black'><center>LIBRARIES<\/center><\/h1>"}}