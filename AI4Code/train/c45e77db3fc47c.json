{"cell_type":{"1ca02397":"code","100cc2d5":"code","09ccd976":"code","62e12eb8":"code","40e12720":"code","ee550259":"code","0bc0375c":"code","a2d9a749":"code","b90e98cb":"code","7694035e":"code","1585ce81":"code","37c23f28":"code","ba2927a4":"code","3e0d8bee":"code","ced9e2f0":"code","25d9d441":"code","7ce23d2e":"code","11c1a756":"code","484bc7cc":"code","ec767a18":"code","ada94856":"code","1c4ef979":"code","5a0042b2":"code","700d11ec":"code","8d1d08fb":"code","e35a9d3f":"code","532b88a9":"code","27f547f3":"code","a4b2b00e":"code","23f9402d":"code","63e9ad02":"code","c35bae00":"code","daa606f2":"code","072c4c41":"code","8f7acc5b":"code","c293ebbf":"code","7996a2f8":"code","736d4322":"code","fc5fe83e":"code","b2e42672":"code","6cd55a23":"code","c6aa8100":"code","e4924e0f":"code","cfe8e252":"code","1f09bbcf":"code","c2df8910":"code","18a5a883":"code","fe3c4215":"code","70e7fc29":"code","b91a3009":"code","d0f38bdf":"code","187bbbc2":"code","5a48927c":"code","00399721":"code","0054bdff":"code","a618319d":"markdown","f01b6613":"markdown","b7d4f03d":"markdown","dca38b01":"markdown","e8fba30d":"markdown","4cc648bc":"markdown","2a81824e":"markdown","3206eee9":"markdown","01ab3ef9":"markdown","bc6fe934":"markdown","1605d225":"markdown","281f4357":"markdown","2dd4516c":"markdown","884f3752":"markdown","888d9301":"markdown","34d5bf3e":"markdown","d4658053":"markdown","adf8ad36":"markdown","b8d286e5":"markdown","1141b9ac":"markdown","bcf3a96e":"markdown","62ae98af":"markdown","f09759e9":"markdown","eda4bef5":"markdown","41dd2f03":"markdown","25aafb8e":"markdown","939af426":"markdown","c6ec7ef0":"markdown","2202f640":"markdown","b8fa33dc":"markdown","fee07dcd":"markdown","b9654791":"markdown","84725ec4":"markdown","80706505":"markdown"},"source":{"1ca02397":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline ","100cc2d5":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC","09ccd976":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection  import GridSearchCV","62e12eb8":"df = pd.read_csv(\"..\/input\/water-potability\/water_potability.csv\")\ndf.head()","40e12720":"df.tail()","ee550259":"df.describe().transpose()","0bc0375c":"plt.figure(figsize = (12,8))\n\nsns.heatmap(df.corr(),\n           annot = True,\n           cmap = 'magma')","a2d9a749":"df.select_dtypes(['object']).columns","b90e98cb":"df.duplicated().value_counts()","7694035e":"plt.figure(figsize = (8,6))\nsns.countplot(x = df['Potability'])","1585ce81":"perc = 100 * df['Potability'].value_counts() \/ len(df)\nperc","37c23f28":"len(df)","ba2927a4":"df.info()","3e0d8bee":"df.isnull().sum()","ced9e2f0":"null_percentages = df.isnull().sum() * 100 \/ len(df)\nnull_percentages.sort_values(ascending=False)","25d9d441":"# visualize null variables as heatmap\nplt.figure(figsize = (12, 6))\nsns.heatmap(df.isnull(),\n            yticklabels = False,\n            cbar = False,\n            cmap = 'viridis')","7ce23d2e":"all_null_rows = df[df.isnull().any(axis=1)]\nall_null_rows","11c1a756":"one_null_rows = df.loc[df.isnull().sum(1) == 1]\nlen(one_null_rows)","484bc7cc":"two_null_rows = df.loc[df.isnull().sum(1) == 2]\nlen(two_null_rows)","ec767a18":"three_null_rows = df.loc[df.isnull().sum(1) == 3]\nlen(three_null_rows)","ada94856":"cor_mat = df[['ph', 'Sulfate', 'Trihalomethanes', 'Potability']].corr()\ncor_mat","1c4ef979":"plt.figure(figsize = (10,6))\nsns.heatmap(cor_mat,\n            annot = True,\n            cmap = 'coolwarm')","5a0042b2":"# see https:\/\/www.dummies.com\/programming\/big-data\/data-science\/how-to-use-python-to-select-the-right-variables-for-data-science\/\n\nX = df.dropna().drop('Potability', axis = 1)\ny = df.dropna()['Potability']\n\n\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import f_regression\n\nSelector_f = SelectPercentile(f_regression, percentile=25)\nSelector_f.fit(X,y)\n\nfor n,s in zip(df.columns,Selector_f.scores_):\n print ('F-score: %3.2f for feature %s ' % (s,n))","700d11ec":"F_df = pd.DataFrame(Selector_f.scores_, df.drop('Potability', axis = 1).columns, columns = ['F-score']) # create F-score df\nF_df['cor_potability'] = df.corr()['Potability'].drop('Potability') # Add the correlation column with respect to 'Potability'\nF_df.sort_values(by = 'F-score', ascending = False)","8d1d08fb":"fig, (ax1, ax2) = plt.subplots(ncols=2, \n                               sharey = False, \n                               figsize = (14,6))\n\nsns.distplot(F_df['F-score'], \n             ax = ax1)\n\nsns.distplot(F_df['cor_potability'], \n             ax = ax2)","e35a9d3f":"null_rows_SO = df.loc[(df.isnull().sum(1) == 1) & (df['Sulfate'].isnull())]\n\nnull_rows_PH = df.loc[(df.isnull().sum(1) == 1) & (df['ph'].isnull())]\n\nnull_rows_TRI = df.loc[(df.isnull().sum(1) == 1) & (df['Trihalomethanes'].isnull())]\n\n\none_miss = pd.DataFrame([len(null_rows_SO), len(null_rows_PH), len(null_rows_TRI)], \n                    ['Sulfate','ph','Trihalomethanes'], \n                    columns = ['Sample Count'])\none_miss","532b88a9":"df.loc[(df.isnull().sum(1) == 1)].iloc[[0,1,58]]","27f547f3":"df = df.interpolate(limit_direction ='both') # selected 'both' --> to fill the edges too\nmissing_count = df.loc[(df.isnull().sum(1) > 0)]\nprint(\"Missing count: \", len(missing_count))","a4b2b00e":"df_copy = df.copy() # deep copy","23f9402d":"print(df_copy['Potability'].value_counts(), \"\\n\")\n\nplt.figure(figsize = (8,6))\nsns.countplot(x = df_copy['Potability'])","63e9ad02":"zero, one = df['Potability'].value_counts()\ndf_zero = df[df['Potability'] == 0]\ndf_one = df[df['Potability'] == 1]\n\ndf_sampled = df_one.sample(zero, replace=True)\ndf_sampled = pd.concat([df_zero, df_sampled], axis=0)\n\nprint(df_sampled['Potability'].value_counts(), \"\\n\")\n\nplt.figure(figsize = (8,6))\nsns.countplot(x = df_sampled['Potability'])","c35bae00":"zero, one = df['Potability'].value_counts()\ndf_zero = df[df['Potability'] == 0]\ndf_one = df[df['Potability'] == 1]\n\ndf_sampled = df_zero.sample(one)\ndf_sampled = pd.concat([df_one, df_sampled], axis=0)\n\nprint(df_sampled['Potability'].value_counts(), \"\\n\")\n\nplt.figure(figsize = (8,6))\nsns.countplot(x = df_sampled['Potability'])","daa606f2":"balance = False\n\nif balance: \n    df = df_sampled\nelse:\n    df = df_copy","072c4c41":"X = df.drop('Potability', axis = 1)\ny = df['Potability']\n\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size = 0.30,\n                                                    random_state = 101)","8f7acc5b":"scaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","c293ebbf":"log_model = LogisticRegression() \n\nlog_model.fit(X_train, y_train)\n\npredictions = log_model.predict(X_test)\n\nprint(classification_report(y_test, predictions))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(y_test, predictions))","7996a2f8":"dtree = DecisionTreeClassifier()\n\ndtree.fit(X_train, y_train)\n\npredictions = dtree.predict(X_test)\n\nprint(classification_report(y_test, predictions))\n\nprint(\"Confusion Matrix: \\n\", confusion_matrix(y_test, predictions))","736d4322":"rfc = RandomForestClassifier(n_estimators = 20)\n\nrfc.fit(X_train, y_train)\n\nrfc_pred = rfc.predict(X_test)\n\nprint(classification_report(y_test, rfc_pred))\n\nprint(\"Confusion Matrix: \\n\", confusion_matrix(y_test, rfc_pred))","fc5fe83e":"errors = []\n\nfor k in range(1,40):\n\n  knn = KNeighborsClassifier(n_neighbors = k)\n  knn.fit(X_train, y_train)\n  pred_k = knn.predict(X_test)\n\n  current_error = np.mean(pred_k != y_test)\n  errors.append(current_error)","b2e42672":"plt.figure(figsize = (10,6))\nplt.plot(range(1,40), \n           errors,\n           marker = 'o',\n           markerfacecolor='red',\n           markersize = 10)\nplt.title('Error Rate vs K')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","6cd55a23":"knn = KNeighborsClassifier(n_neighbors = 37) \nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\n\nprint(classification_report(y_test, predictions))\n\nprint(\"Confusion Matrix: \\n\", confusion_matrix(y_test, predictions))","c6aa8100":"param_grid = {'C': [0.1, 1, 10],\n              'gamma': [1, 0.1, 0.01]}\ngrid = GridSearchCV(SVC(),\n                    param_grid,\n                    verbose = 3)\ngrid.fit(X_train, y_train)","e4924e0f":"print(grid.best_params_, \"\\n\")\nprint(grid.best_estimator_)","cfe8e252":"grid_predictions = grid.predict(X_test)\n\nprint(classification_report(y_test, grid_predictions))\n\nprint(\"Confusion Matrix: \\n\", confusion_matrix(y_test, grid_predictions))","1f09bbcf":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping","c2df8910":"X = df.drop('Potability', axis = 1).values\ny = df['Potability'].values\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size = 0.30,\n                                                    random_state = 101)\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","18a5a883":"X_train.shape","fe3c4215":"model = Sequential()\n\nmodel.add(Dense(units = 9, activation = 'relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units = 18, activation = 'relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units = 36, activation = 'relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units = 1, activation = 'sigmoid'))\n\nmodel.compile(optimizer = 'adam',\n              loss = 'binary_crossentropy')","70e7fc29":"# better to --> minimize loss, maximize accuracy\nearly_stop = EarlyStopping(monitor = 'val_loss',\n                           mode = 'min',\n                           verbose = 1,\n                           patience = 25) # 25 more epochs","b91a3009":"model.fit(x = X_train,\n          y = y_train,\n          epochs = 250,\n          validation_data = (X_test, y_test),\n          callbacks = [early_stop],\n          verbose = 0) # silent","d0f38bdf":"losses = pd.DataFrame(model.history.history)\n\nprint(losses.head(), \"\\n\")\nprint(losses.tail())","187bbbc2":"losses.plot(figsize = (12, 8))","5a48927c":"predictions = model.predict_classes(X_test)\n\nprint(classification_report(y_test, predictions))\n\nprint(\"Confusion Matrix: \\n\", confusion_matrix(y_test, predictions))","00399721":"model_types = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'KNN', 'SVM', 'Simple NN']\naccuracies = [0.61, .58, .67, .66, .67, .67]\npd.DataFrame(accuracies, model_types, columns = ['Accuracy']).sort_values(by = 'Accuracy', ascending = False)","0054bdff":"plt.figure(figsize = (12, 6) )\nsns.barplot(model_types, accuracies)","a618319d":"This table shows the sample count for each of this attributes where the only just one value is missing. Remember how it looks like with some example samples:","f01b6613":"## KNN ","b7d4f03d":"## SVM with GridSearchCV","dca38b01":"Row count with only three missing cell.","e8fba30d":"# Train and Evaluate","4cc648bc":"Note: This is my first notebook on Kaggle after recently finished a Data Science course. I would be glad if you comment or criticize. Thanks for any advice or comment in advance.","2a81824e":"Put them together in a dataframe.","3206eee9":"Oversampling randomly","01ab3ef9":"Samples with one missing values compose majority as **1105 \/ 1265** corresponds to **~87%** of all samples with missing data. ","bc6fe934":"Undersampling randomly","1605d225":"I am **Alptug Aydin**.\n\nDate: 12.08.2021\n\nOutline:\n\n- Necessary imports\n\n- Data Loading\n\n- Exploring Data: \n    - Check if any categorical, duplicated or missing data\n    - Deeper look into missing data\n    - Imputing them with interpolation\n \n \n- Split, Scale, Balance\n    - Scale with MinMaxScaler\n    \n    \n- Train and test with various models: \n    - Logistic Regression\n    - Decision Tree\n    - Random Forest\n    - KNN\n    - SVM\n    - Simple NN\n","281f4357":"## Decision Tree","2dd4516c":"## Logistic Regression","884f3752":"# Loading and Exploring the Data","888d9301":"**No duplicates.**","34d5bf3e":"There is **no categorical data** to consider in pre-process phase.\n\n","d4658053":"Missing value counts.","adf8ad36":"\nDataset is **not** in perfect balance. \n\n- Not Potable 60.99%\n- Potable 39.01%\n\n","b8d286e5":"## Comparison plot","1141b9ac":"Row count with only one missing cell.","bcf3a96e":"Percentages of missing value containing samples.","62ae98af":"# Train Test Split and Scale","f09759e9":"This is my first notebook on Kaggle after recently finished a Data Science course. I would be glad if you comment or criticize. Thanks for any advice or comment in advance. ","eda4bef5":"Row count with only two missing cell.","41dd2f03":"Correlation and F values on 'Potability' are consistent. The **Solids** attribute has relatively more impact on **Potability** compare to **ph** and **Trihalomethanes**.","25aafb8e":"## Random Forest","939af426":"**ph, Sulfate, Trihalomethanes** columns contains null values. Let's visualize..","c6ec7ef0":"Before balancing","2202f640":"Set the 'balance' variable True if you want to continue with the desired balancing method. One of the sampling cells above must be executed already.","b8fa33dc":"As shown **23.84% of Sulfate, 14.99% of ph, 4.95% of Trihalomethanes** is missing. Before processing them, I want to examine a bit deeper. ","fee07dcd":"# Necessary Imports","b9654791":"## Handling Missing Values","84725ec4":"## Simple NN","80706505":"## Balancing"}}