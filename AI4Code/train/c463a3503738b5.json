{"cell_type":{"41ca09dc":"code","0b2eebd6":"code","dc6f981a":"code","0040c7b1":"code","71f8eedb":"code","969ffd5e":"code","f544b018":"code","65213fdc":"code","c793cd59":"code","01faee0d":"code","d19c13f9":"code","cfe4091f":"code","f4ec6e01":"code","dce85a5e":"code","b5f9967c":"code","0a191f21":"code","ac656b2a":"code","7f968f9b":"code","7c50074a":"code","95b5ff5e":"code","19cbb47b":"code","3ff52fe3":"code","af628420":"code","d688b862":"code","98e82bda":"code","790a6957":"code","16cbb0fe":"code","e8f5430a":"code","a3b8594c":"code","256b1f6b":"code","cdae87ee":"code","83bb0525":"code","0be413b1":"code","6bb6a413":"markdown","83572c40":"markdown","f7ba13ea":"markdown","03ce4657":"markdown","d7695b27":"markdown","b0cb3ab7":"markdown","30e84325":"markdown","61dbff80":"markdown","59e3b199":"markdown","423e4153":"markdown","4ab5131e":"markdown","2fe63cc6":"markdown","39eaf360":"markdown","8f416dff":"markdown","0f399c17":"markdown","0d0618f3":"markdown","02f4428d":"markdown","517cb6bd":"markdown","3c2ef0af":"markdown","97f44024":"markdown","b3d74e82":"markdown","d5a88c8a":"markdown","df4e3c17":"markdown","e862d004":"markdown"},"source":{"41ca09dc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n \n# Model Selection and utilities\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix\n\n# Model Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier","0b2eebd6":"#READING DATASET\ndf = pd.read_csv(\"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\")\ndf.head()","dc6f981a":"df.describe().T","0040c7b1":"df.isnull().sum()","71f8eedb":"df.info()","969ffd5e":"# Storing categorical and numerical features names in different Series\ncat_columns = [\"anaemia\",\"diabetes\",\"high_blood_pressure\",\"sex\",\"smoking\",\"DEATH_EVENT\"]\nnum_columns = pd.Series(df.columns)\nnum_columns = num_columns[~num_columns.isin(cat_columns)]","f544b018":"fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(10, 6))\ntitles = list(df[cat_columns])\n\nax_title_pairs = zip(axs.flat, titles)\n\nfor ax, title in ax_title_pairs:\n    sns.countplot(x=title, data=df, palette='muted', ax=ax)\n    ax.set_title(title)\n    ax.set_xlabel('')\n\nplt.tight_layout()","65213fdc":"df_grouped = df.groupby(by='DEATH_EVENT')\nfig, axs = plt.subplots(nrows=3, ncols=3, figsize=(15, 8))\ntitles = list(df[num_columns])\n\nax_title_pairs = zip(axs.flat, titles)\n\nfor ax, title in ax_title_pairs:\n  sns.distplot(df_grouped.get_group(0)[title], bins=10, ax=ax, label='No')\n  sns.distplot(df_grouped.get_group(1)[title], bins=10, ax=ax, label='Yes')\n  ax.legend(title='DEATH_EVENT')\n\naxs.flat[-1].remove()\naxs.flat[-2].remove()\nfig.tight_layout()","c793cd59":"X_raw = df.iloc[:,:-1].to_numpy()\ny_raw = df['DEATH_EVENT'].to_numpy()\n\nX_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_raw, y_raw, test_size = 0.2, random_state =1)\n\nresult_dict = {}","01faee0d":"def fit_model_with_grid_search(model, X_t, y_t, parameters, scoring='f1', verbose=1):\n  model = GridSearchCV(\n      model,\n      parameters,\n      scoring=scoring\n  )\n  \n  model.fit(X_t, y_t)\n  \n  if verbose:\n    print(f'\\nbest_params_: {model.best_params_}')\n    print(f'Mean cross-validated F1 score of the best_estimator: {model.best_score_:.4f}')\n      \n  return model\n\ndef print_metrics(cf, X_t, y_t):\n  y_pred = classifier.predict(X_t)\n\n  accuracy = accuracy_score(y_t, y_pred)\n  f1 = f1_score(y_t, y_pred, average='macro')\n  precision = precision_score(y_t, y_pred, average='macro')\n  recall = recall_score(y_t, y_pred, average='macro')\n  \n  print(f'\\nAccuracy (test set)\\t| {accuracy:.4f}')\n  print(f'F1 (test set)\\t\\t| {f1:.4f}')\n  print(f'Precision (test set)\\t| {precision:.4f}')\n  print(f'Recall (test set)\\t| {recall:.4f}\\n')\n  # print()\n  cm = confusion_matrix(y_t, y_pred)\n  plt.figure(figsize=(5,3))\n  sns.heatmap(cm,annot=True, linewidths=.5)\n  plt.show()\n\n  return {\n    'accuracy': accuracy,\n    'f1': f1,\n    'precision': precision,\n    'recall': recall,\n  }","d19c13f9":"print(\"*Logistic Regression*\")\n\nmodel_logistic_regression = LogisticRegression()\nparameters = {\n    'C': [0.01, 0.1, 1],\n}\nclassifier = fit_model_with_grid_search(\n    model_logistic_regression,\n    X_train_r,\n    y_train_r,\n    parameters,\n    scoring='f1',\n)\n\nresult_dict['Logistic Regression'] = print_metrics(classifier, X_test_r, y_test_r)","cfe4091f":"print(\"*K Nearest Neighours*\")\n\nmodel_knn = KNeighborsClassifier()\nparameters = {\n    \"n_neighbors\": list(range(2, 21)),\n    \"weights\": ['uniform', 'distance'],\n}\nclassifier = fit_model_with_grid_search(\n    model_knn,\n    X_train_r,\n    y_train_r,\n    parameters,\n    scoring='accuracy',\n)\n\nresult_dict['KNN'] = print_metrics(classifier, X_test_r, y_test_r)","f4ec6e01":"list1 = []\nfor neighbors in range(2,21):\n  classifier = KNeighborsClassifier(n_neighbors=neighbors)\n  classifier.fit(X_train_r, y_train_r)\n  y_pred = classifier.predict(X_test_r)\n  list1.append(accuracy_score(y_test_r, y_pred))\nplt.plot(list(range(2,21)), list1)\nplt.show()","dce85a5e":"print(\"*Support Vector Machine*\")\n\nmodel_svm = SVC()\nparameters = {\n    \"C\": [0.001, 0.01, 0.1, 1],\n}\nclassifier = fit_model_with_grid_search(\n    model_svm,\n    X_train_r,\n    y_train_r,\n    parameters,\n    scoring='accuracy',\n)\n\nresult_dict['SVM'] = print_metrics(classifier, X_test_r, y_test_r)","b5f9967c":"print(\"*Decision Tree Classsifier*\")\n\nmodel_decision = DecisionTreeClassifier()\nparameters = {\n  \"max_depth\": [1, 2, 3, 5, 10, None], \n  \"max_leaf_nodes\": list(range(2, 15)),\n  \"criterion\": [\"entropy\"],\n}\nclassifier = fit_model_with_grid_search(\n    model_decision,\n    X_train_r,\n    y_train_r,\n    parameters,\n    scoring='f1',\n)\n\nresult_dict['Decision Tree'] = print_metrics(classifier, X_test_r, y_test_r)","0a191f21":"print(\"*Random Forest Classifier*\")\nmodel_rand_forest = RandomForestClassifier()\nparameters = {\n    \"n_estimators\": list(range(10,21)),\n}\nclassifier = fit_model_with_grid_search(\n    model_rand_forest,\n    X_train_r,\n    y_train_r,\n    parameters,\n    scoring='f1',\n)\n\nresult_dict['Random Forest'] = print_metrics(classifier, X_test_r, y_test_r)","ac656b2a":"classifier = GaussianNB()\nclassifier.fit(X_train_r, y_train_r)\n\nprint(\"*Gaussian NaiveBayes*\")\nresult_dict['NaiveBayes'] = print_metrics(classifier, X_test_r, y_test_r)","7f968f9b":"ax, fig = plt.subplots(figsize=(12,12))\ncorr = df.corr()\nsns.heatmap(corr, vmin=-1, cmap='coolwarm', annot=True)\nplt.xticks(rotation=30, ha='right')\nplt.show()","7c50074a":"corr[abs(corr['DEATH_EVENT']) > 0.1]['DEATH_EVENT']","95b5ff5e":"# Feature Selection\n\nplt.rcParams['figure.figsize']=12,6 \nsns.set_style(\"darkgrid\")\n\nx1 = df.iloc[:, :-1]\ny1 = df.iloc[:,-1]\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nmodel = ExtraTreesClassifier()\nmodel.fit(x1,y1)\n# print(model.feature_importances_) \nfeat_importances = pd.Series(model.feature_importances_, index=x1.columns)\nfeat_importances.nlargest(12).plot(kind='barh')\nplt.show()","19cbb47b":"def plot_histogram(dataset, feature, color, title, labels):\n  fig = px.histogram(\n    dataset, \n    x=feature,\n    color=color, \n    marginal=\"box\",\n    hover_data=dataset.columns,\n    title = title, \n    labels = labels,\n    width=800,\n    template=\"plotly_white\",\n  )\n  fig.show()","3ff52fe3":"plot_histogram(df, 'age', 'DEATH_EVENT', 'AGE Vs DEATH_EVENT', {\"age\": \"AGE\"})","af628420":"plot_histogram(df, 'ejection_fraction', 'DEATH_EVENT', 'EJECTION FRACTION Vs DEATH_EVENT', {\"ejection_fraction\": \"EJECTION FRACTION\"})","d688b862":"plot_histogram(df, 'serum_sodium', 'DEATH_EVENT', 'SERUM SODIUM Vs DEATH_EVENT', {\"serum_sodium\": \"SERUM SODIUM\"})","98e82bda":"plot_histogram(df, 'serum_creatinine', 'DEATH_EVENT', 'SERUM CREATININE Vs DEATH_EVENT', {\"serum_creatinine\": \"SERUM CREATININE\"})","790a6957":"X = df[['ejection_fraction', 'serum_creatinine', 'serum_sodium', 'time', 'age']].to_numpy()\n\ny = df['DEATH_EVENT'].to_numpy()","16cbb0fe":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state =1)","e8f5430a":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","a3b8594c":"list1 = []\nfor neighbors in range(2,15):\n  classifier = KNeighborsClassifier(n_neighbors=neighbors)\n  classifier.fit(X_train, y_train)\n  y_pred = classifier.predict(X_test)\n  list1.append(accuracy_score(y_test,y_pred))\nplt.plot(list(range(2,15)), list1)\nplt.show()","256b1f6b":"print(\"*K Nearest Neighours (Transformed Data)*\")\n\nclassifier = KNeighborsClassifier(n_neighbors=11)\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n\nresult_dict['KNN(Selected Features)'] = print_metrics(classifier, X_test, y_test)","cdae87ee":"list1 = []\nfor c in [0.001, 0.01, 0.05, 0.1, 0.2, 0.4, 0.5, 0.75, 0.8, 0.9, 1]:\n  classifier = SVC(C = c)\n  classifier.fit(X_train, y_train)\n  y_pred = classifier.predict(X_test)\n  list1.append(accuracy_score(y_test,y_pred))\nplt.plot([0.001, 0.01, 0.05, 0.1, 0.2, 0.4, 0.5, 0.75, 0.8, 0.9, 1], list1)\nplt.show()","83bb0525":"print(\"*Support Vector machine (Transformed Data)*\")\n\nclassifier = SVC(C = 0.2)\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n\nresult_dict['SVM (Selected Features)'] = print_metrics(classifier, X_test, y_test)","0be413b1":"Results = pd.DataFrame(result_dict).T\nResults","6bb6a413":"### 6. Naive Bayes","83572c40":"Frequency distribution of Categorical Variables","f7ba13ea":"### 1. K Nearest Neighours","03ce4657":"## 4. Feature Selection","d7695b27":"Utility Functions","b0cb3ab7":"## 5. Model Training and Prediction on selected features","30e84325":"### 5. Random Forest","61dbff80":"### 2. SVM","59e3b199":"### 4. Decision Tree","423e4153":"![](https:\/\/patients.healthquest.org\/wp-content\/uploads\/2018\/05\/congestive-heart-failure-feature2.jpg)","4ab5131e":"## 1. Import Libraries and Dataset","2fe63cc6":"### 2. Extra Tree Classifier ","39eaf360":"## \u2665 Failure : Comparison of 6 classification models","8f416dff":"## 6. Model performance comparison","0f399c17":"## 2. Data analysis and Visualization","0d0618f3":"### 3. SVM","02f4428d":"### 1. Logistic Regression","517cb6bd":"Data Scaling","3c2ef0af":"## 3. Modelling on raw dataset","97f44024":"In this dataset:\n* It contains 299 rows (patient information).\n* It contains 13 columns (12 features and DEATH_EVENT target variable).\n* 10 features are integer type.\n* 3 features are float type.\n\n***FEATURES***\n\n**age**: age of patient\n\n**anaemia**: Decrease of red blood cells or hemoglobin\n\n**creatinine_phosphokinase**: Level of the CPK enzyme in the blood (mcg\/L)\n\n**diabetes**: If the patient has diabetes\n\n**ejection_fraction**: Percentage of blood leaving the heart at each contraction (percentage)\n\n**high_blood_pressure**: If the patient has hypertension platelets: Platelets in the blood\n\n**serum_creatinine**: Level of serum creatinine in the blood (mg\/dL)\n\n**serum_sodium**: Level of serum sodium in the blood (mEq\/L)\n\n**sex**: Woman or man (binary)\n\n**smoking**: If the patient smokes or not\n\n**time**: Follow-up period (days)\n\n**DEATH_EVENT**: If the patient deceased during the follow-up period","b3d74e82":"### 2. KNN","d5a88c8a":"Frequency distribution of Continuous Variables","df4e3c17":"In above correlation matrix, we see features relationship each other. This relationships can be useful to set up model. If the relationship how is close and is strong, it can be impact to use them in order to set up true model. In this dataset, we will look relationship of DEATH_EVENT with other features. If relationship between them is big from 0.1, This features can be important features,which heart attack triggers. ","e862d004":"### 1. Co-relation matrix"}}