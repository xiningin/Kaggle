{"cell_type":{"653ff3d0":"code","0f26ba38":"code","43383975":"code","8fafbf1b":"code","ac43c1a9":"code","ccc17bb5":"code","1973d00f":"code","67723f57":"code","c38513d7":"code","55483c35":"code","a556ca0d":"code","a70cef11":"code","6e958157":"code","1755b9a4":"code","1c7443b7":"code","4c5fda08":"code","90a2a240":"code","e96658f8":"code","b399f270":"code","7a066422":"code","c4dc9a31":"code","2573bc56":"code","e22dc84f":"code","460ded38":"code","4534ced5":"code","c354c721":"code","861cc221":"code","05862d3e":"code","ba444565":"code","fb5f4ef4":"code","ce87f8c0":"code","06485094":"code","9bed5b07":"code","62cd3eba":"markdown","9abea06b":"markdown","51d30c51":"markdown","2c5cb671":"markdown","7c49fe83":"markdown","4796454c":"markdown","6191edb5":"markdown","a621e1cc":"markdown","879ae168":"markdown","9199b7b5":"markdown","b519b354":"markdown"},"source":{"653ff3d0":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0f26ba38":"train = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\n\ntrain.shape","43383975":"train_target = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\n\ntrain_target.shape","8fafbf1b":"test = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\n\ntest.shape","ac43c1a9":"# From https:\/\/www.kaggle.com\/carlmcbrideellis\/moa-setting-ctl-vehicle-0-improves-score\n\ntrain.at[train['cp_type'].str.contains('ctl_vehicle'),train.filter(regex='-.*').columns] = 0.0\n\ntest.at[test['cp_type'].str.contains('ctl_vehicle'),test.filter(regex='-.*').columns] = 0.0","ccc17bb5":"train_size = train.shape[0]\n\ntraintest = pd.concat([train, test])\n\ntraintest = pd.concat([traintest, pd.get_dummies(traintest['cp_type'], prefix='cp_type')], axis=1)\ntraintest = pd.concat([traintest, pd.get_dummies(traintest['cp_time'], prefix='cp_time')], axis=1)\ntraintest = pd.concat([traintest, pd.get_dummies(traintest['cp_dose'], prefix='cp_dose')], axis=1)\n\ntraintest = traintest.drop(['cp_type', 'cp_time', 'cp_dose'], axis=1)\n\ntrain = traintest[:train_size]\ntest  = traintest[train_size:]\n\ndel traintest","1973d00f":"train.shape","67723f57":"from sklearn.preprocessing import StandardScaler\n\n\ng_columns = [ c for c in train.columns if 'g-' in c ]\n\nscaler = StandardScaler()\ntrain[g_columns] = scaler.fit_transform(train[g_columns])\ntest[g_columns] = scaler.transform(test[g_columns])","c38513d7":"x_train = train.drop('sig_id', axis=1)\n\ny_train = train_target.drop('sig_id', axis=1)\n\nx_test = test.drop('sig_id', axis=1)","55483c35":"options = {\n    'default': {\n        'features': list(x_train.columns)\n    }\n}","a556ca0d":"def make_x(option):\n    features = options[option]['features']\n    return x_train[features], x_test[features]","a70cef11":"from sklearn.feature_selection import RFECV\nimport lightgbm as lgb","6e958157":"params = {\n    'objective': 'binary',\n    'learning_rate': 0.05,\n    'max_depth': -1,\n    'num_leaves': 31,\n    'num_threads': 4,\n    'random_state': 42\n}","1755b9a4":"# feature_selector = RFECV(lgb.LGBMClassifier(**params),\n#                          step=10, min_features_to_select=500, scoring='neg_log_loss',\n#                          cv=3, verbose=1, n_jobs=-1)\n\n# feature_selector.fit(x_train, y_train['antiviral'])\n\n# print('Features selected:', feature_selector.n_features_)","1c7443b7":"# selected_features = [f for f in x_train.columns[feature_selector.ranking_ == 1]]\n\n# print(selected_features)","4c5fda08":"# from RFECV in Version 18 (500 features)\noptions['500'] = {\n    'features': ['g-0', 'g-1', 'g-2', 'g-3', 'g-4', 'g-5', 'g-6', 'g-7', 'g-8', 'g-9', 'g-10', 'g-11', 'g-12', 'g-13', 'g-14', 'g-15', 'g-16', 'g-17', 'g-18', 'g-19', 'g-20', 'g-21', 'g-22', 'g-23', 'g-24', 'g-25', 'g-26', 'g-27', 'g-28', 'g-29', 'g-30', 'g-31', 'g-32', 'g-33', 'g-34', 'g-35', 'g-36', 'g-37', 'g-38', 'g-39', 'g-40', 'g-41', 'g-42', 'g-43', 'g-44', 'g-45', 'g-46', 'g-47', 'g-48', 'g-49', 'g-50', 'g-51', 'g-52', 'g-53', 'g-54', 'g-55', 'g-56', 'g-57', 'g-58', 'g-59', 'g-60', 'g-61', 'g-62', 'g-63', 'g-64', 'g-65', 'g-66', 'g-67', 'g-68', 'g-69', 'g-70', 'g-71', 'g-72', 'g-73', 'g-74', 'g-75', 'g-76', 'g-77', 'g-78', 'g-79', 'g-80', 'g-81', 'g-82', 'g-83', 'g-84', 'g-85', 'g-86', 'g-87', 'g-88', 'g-89', 'g-90', 'g-91', 'g-92', 'g-93', 'g-94', 'g-95', 'g-96', 'g-97', 'g-98', 'g-99', 'g-100', 'g-101', 'g-102', 'g-103', 'g-104', 'g-105', 'g-106', 'g-107', 'g-108', 'g-109', 'g-110', 'g-111', 'g-112', 'g-113', 'g-114', 'g-115', 'g-116', 'g-117', 'g-118', 'g-121', 'g-122', 'g-123', 'g-125', 'g-126', 'g-127', 'g-128', 'g-129', 'g-130', 'g-131', 'g-132', 'g-133', 'g-139', 'g-144', 'g-149', 'g-152', 'g-153', 'g-154', 'g-155', 'g-158', 'g-161', 'g-162', 'g-165', 'g-166', 'g-167', 'g-168', 'g-169', 'g-170', 'g-171', 'g-172', 'g-173', 'g-174', 'g-175', 'g-176', 'g-177', 'g-178', 'g-179', 'g-180', 'g-182', 'g-185', 'g-188', 'g-189', 'g-191', 'g-192', 'g-193', 'g-194', 'g-195', 'g-196', 'g-197', 'g-198', 'g-199', 'g-200', 'g-201', 'g-202', 'g-203', 'g-204', 'g-209', 'g-211', 'g-213', 'g-214', 'g-217', 'g-220', 'g-221', 'g-222', 'g-223', 'g-225', 'g-226', 'g-229', 'g-233', 'g-239', 'g-240', 'g-243', 'g-244', 'g-246', 'g-247', 'g-250', 'g-254', 'g-255', 'g-256', 'g-257', 'g-259', 'g-261', 'g-262', 'g-265', 'g-269', 'g-270', 'g-271', 'g-273', 'g-279', 'g-282', 'g-284', 'g-286', 'g-289', 'g-292', 'g-294', 'g-295', 'g-298', 'g-299', 'g-300', 'g-303', 'g-306', 'g-310', 'g-311', 'g-312', 'g-314', 'g-316', 'g-317', 'g-318', 'g-320', 'g-321', 'g-323', 'g-326', 'g-327', 'g-328', 'g-332', 'g-333', 'g-335', 'g-339', 'g-340', 'g-341', 'g-346', 'g-347', 'g-351', 'g-353', 'g-355', 'g-357', 'g-360', 'g-364', 'g-365', 'g-370', 'g-375', 'g-383', 'g-385', 'g-388', 'g-389', 'g-390', 'g-391', 'g-392', 'g-399', 'g-401', 'g-402', 'g-403', 'g-404', 'g-409', 'g-417', 'g-423', 'g-424', 'g-425', 'g-426', 'g-428', 'g-429', 'g-430', 'g-431', 'g-433', 'g-434', 'g-438', 'g-439', 'g-440', 'g-441', 'g-443', 'g-444', 'g-445', 'g-446', 'g-447', 'g-448', 'g-449', 'g-450', 'g-451', 'g-452', 'g-453', 'g-454', 'g-455', 'g-456', 'g-460', 'g-461', 'g-463', 'g-465', 'g-467', 'g-470', 'g-473', 'g-474', 'g-475', 'g-476', 'g-481', 'g-483', 'g-485', 'g-486', 'g-489', 'g-490', 'g-495', 'g-500', 'g-501', 'g-502', 'g-504', 'g-509', 'g-513', 'g-516', 'g-517', 'g-518', 'g-527', 'g-531', 'g-535', 'g-540', 'g-542', 'g-543', 'g-544', 'g-547', 'g-548', 'g-549', 'g-552', 'g-556', 'g-558', 'g-562', 'g-566', 'g-571', 'g-572', 'g-573', 'g-574', 'g-578', 'g-584', 'g-585', 'g-590', 'g-592', 'g-595', 'g-596', 'g-600', 'g-602', 'g-603', 'g-605', 'g-606', 'g-611', 'g-612', 'g-615', 'g-617', 'g-618', 'g-619', 'g-620', 'g-623', 'g-624', 'g-625', 'g-626', 'g-627', 'g-628', 'g-629', 'g-630', 'g-631', 'g-632', 'g-633', 'g-634', 'g-635', 'g-636', 'g-637', 'g-638', 'g-639', 'g-640', 'g-643', 'g-645', 'g-646', 'g-648', 'g-651', 'g-654', 'g-657', 'g-658', 'g-659', 'g-660', 'g-661', 'g-662', 'g-663', 'g-664', 'g-665', 'g-666', 'g-668', 'g-670', 'g-672', 'g-673', 'g-674', 'g-675', 'g-677', 'g-678', 'g-679', 'g-685', 'g-689', 'g-695', 'g-697', 'g-698', 'g-699', 'g-701', 'g-702', 'g-703', 'g-704', 'g-707', 'g-709', 'g-710', 'g-712', 'g-713', 'g-714', 'g-715', 'g-721', 'g-722', 'g-724', 'g-725', 'g-726', 'g-734', 'g-736', 'g-737', 'g-740', 'g-742', 'g-743', 'g-745', 'g-747', 'g-750', 'g-751', 'g-752', 'g-753', 'g-756', 'g-757', 'g-758', 'g-759', 'g-760', 'g-761', 'g-762', 'g-763', 'g-764', 'g-766', 'g-767', 'g-768', 'g-769', 'g-770', 'g-771', 'c-0', 'c-1', 'c-2', 'c-3', 'c-4', 'c-5', 'c-11', 'c-12', 'c-14', 'c-19', 'c-20', 'c-21', 'c-22', 'c-23', 'c-25', 'c-27', 'c-32', 'c-35', 'c-36', 'c-37', 'c-40', 'c-47', 'c-54', 'c-55', 'c-57', 'c-58', 'c-61', 'c-62', 'c-63', 'c-65', 'c-66', 'c-71', 'c-74', 'c-75', 'c-77', 'c-78', 'c-79', 'c-80', 'c-81', 'c-82', 'c-83', 'c-84', 'c-85', 'c-86', 'c-87', 'c-88', 'c-89', 'c-90', 'c-91', 'c-92', 'c-93', 'c-97']\n}","90a2a240":"# from RFECV in Version 17 (600 features)\noptions['600'] = {\n    'features': ['g-0', 'g-1', 'g-2', 'g-3', 'g-4', 'g-5', 'g-6', 'g-7', 'g-8', 'g-9', 'g-10', 'g-11', 'g-12', 'g-13', 'g-14', 'g-15', 'g-16', 'g-17', 'g-18', 'g-19', 'g-20', 'g-21', 'g-22', 'g-23', 'g-24', 'g-25', 'g-26', 'g-27', 'g-28', 'g-29', 'g-30', 'g-31', 'g-32', 'g-33', 'g-34', 'g-35', 'g-36', 'g-37', 'g-38', 'g-39', 'g-40', 'g-41', 'g-42', 'g-43', 'g-44', 'g-45', 'g-46', 'g-47', 'g-48', 'g-49', 'g-50', 'g-51', 'g-52', 'g-53', 'g-54', 'g-55', 'g-56', 'g-57', 'g-58', 'g-59', 'g-60', 'g-61', 'g-62', 'g-63', 'g-64', 'g-65', 'g-66', 'g-67', 'g-68', 'g-69', 'g-70', 'g-71', 'g-72', 'g-73', 'g-74', 'g-75', 'g-76', 'g-77', 'g-78', 'g-79', 'g-80', 'g-81', 'g-82', 'g-83', 'g-84', 'g-85', 'g-86', 'g-87', 'g-88', 'g-89', 'g-90', 'g-91', 'g-92', 'g-93', 'g-94', 'g-95', 'g-96', 'g-97', 'g-98', 'g-99', 'g-100', 'g-101', 'g-102', 'g-103', 'g-104', 'g-105', 'g-106', 'g-107', 'g-108', 'g-109', 'g-110', 'g-111', 'g-112', 'g-113', 'g-114', 'g-115', 'g-116', 'g-117', 'g-118', 'g-119', 'g-120', 'g-121', 'g-122', 'g-123', 'g-124', 'g-125', 'g-126', 'g-127', 'g-128', 'g-129', 'g-130', 'g-131', 'g-132', 'g-133', 'g-134', 'g-135', 'g-136', 'g-137', 'g-138', 'g-139', 'g-140', 'g-141', 'g-142', 'g-143', 'g-144', 'g-145', 'g-146', 'g-147', 'g-148', 'g-149', 'g-150', 'g-151', 'g-152', 'g-153', 'g-154', 'g-155', 'g-156', 'g-157', 'g-158', 'g-159', 'g-160', 'g-161', 'g-162', 'g-163', 'g-164', 'g-165', 'g-166', 'g-167', 'g-168', 'g-169', 'g-170', 'g-171', 'g-172', 'g-173', 'g-174', 'g-175', 'g-176', 'g-177', 'g-178', 'g-179', 'g-180', 'g-181', 'g-182', 'g-183', 'g-184', 'g-185', 'g-186', 'g-187', 'g-188', 'g-189', 'g-190', 'g-191', 'g-192', 'g-193', 'g-194', 'g-195', 'g-196', 'g-197', 'g-198', 'g-199', 'g-200', 'g-201', 'g-202', 'g-203', 'g-204', 'g-205', 'g-206', 'g-207', 'g-208', 'g-209', 'g-210', 'g-211', 'g-212', 'g-213', 'g-214', 'g-215', 'g-217', 'g-220', 'g-221', 'g-222', 'g-223', 'g-225', 'g-226', 'g-229', 'g-233', 'g-239', 'g-240', 'g-241', 'g-242', 'g-243', 'g-244', 'g-245', 'g-246', 'g-247', 'g-250', 'g-254', 'g-255', 'g-256', 'g-257', 'g-259', 'g-261', 'g-262', 'g-265', 'g-269', 'g-270', 'g-271', 'g-272', 'g-273', 'g-279', 'g-282', 'g-284', 'g-286', 'g-289', 'g-292', 'g-294', 'g-295', 'g-298', 'g-299', 'g-300', 'g-303', 'g-306', 'g-310', 'g-311', 'g-312', 'g-314', 'g-316', 'g-317', 'g-318', 'g-320', 'g-321', 'g-323', 'g-326', 'g-327', 'g-328', 'g-332', 'g-333', 'g-335', 'g-339', 'g-340', 'g-341', 'g-346', 'g-347', 'g-351', 'g-353', 'g-355', 'g-357', 'g-360', 'g-364', 'g-365', 'g-370', 'g-375', 'g-383', 'g-385', 'g-388', 'g-389', 'g-390', 'g-391', 'g-392', 'g-399', 'g-401', 'g-402', 'g-403', 'g-404', 'g-409', 'g-417', 'g-423', 'g-424', 'g-425', 'g-426', 'g-428', 'g-429', 'g-430', 'g-431', 'g-433', 'g-434', 'g-438', 'g-439', 'g-440', 'g-441', 'g-443', 'g-444', 'g-445', 'g-446', 'g-447', 'g-448', 'g-449', 'g-450', 'g-451', 'g-452', 'g-453', 'g-454', 'g-455', 'g-456', 'g-460', 'g-461', 'g-463', 'g-465', 'g-467', 'g-470', 'g-473', 'g-474', 'g-475', 'g-476', 'g-481', 'g-483', 'g-485', 'g-486', 'g-489', 'g-490', 'g-495', 'g-500', 'g-501', 'g-502', 'g-504', 'g-509', 'g-513', 'g-516', 'g-517', 'g-518', 'g-527', 'g-531', 'g-535', 'g-540', 'g-542', 'g-543', 'g-544', 'g-547', 'g-548', 'g-549', 'g-552', 'g-556', 'g-558', 'g-562', 'g-566', 'g-571', 'g-572', 'g-573', 'g-574', 'g-578', 'g-584', 'g-585', 'g-590', 'g-592', 'g-595', 'g-596', 'g-599', 'g-600', 'g-601', 'g-602', 'g-603', 'g-604', 'g-605', 'g-606', 'g-607', 'g-608', 'g-609', 'g-610', 'g-611', 'g-612', 'g-613', 'g-614', 'g-615', 'g-616', 'g-617', 'g-618', 'g-619', 'g-620', 'g-621', 'g-622', 'g-623', 'g-624', 'g-625', 'g-626', 'g-627', 'g-628', 'g-629', 'g-630', 'g-631', 'g-632', 'g-633', 'g-634', 'g-635', 'g-636', 'g-637', 'g-638', 'g-639', 'g-640', 'g-643', 'g-645', 'g-646', 'g-647', 'g-648', 'g-651', 'g-654', 'g-657', 'g-658', 'g-659', 'g-660', 'g-661', 'g-662', 'g-663', 'g-664', 'g-665', 'g-666', 'g-668', 'g-670', 'g-672', 'g-673', 'g-674', 'g-675', 'g-676', 'g-677', 'g-678', 'g-679', 'g-680', 'g-685', 'g-688', 'g-689', 'g-690', 'g-691', 'g-695', 'g-697', 'g-698', 'g-699', 'g-700', 'g-701', 'g-702', 'g-703', 'g-704', 'g-707', 'g-708', 'g-709', 'g-710', 'g-711', 'g-712', 'g-713', 'g-714', 'g-715', 'g-716', 'g-717', 'g-718', 'g-719', 'g-720', 'g-721', 'g-722', 'g-723', 'g-724', 'g-725', 'g-726', 'g-728', 'g-729', 'g-730', 'g-731', 'g-732', 'g-733', 'g-734', 'g-736', 'g-737', 'g-739', 'g-740', 'g-742', 'g-743', 'g-744', 'g-745', 'g-747', 'g-748', 'g-749', 'g-750', 'g-751', 'g-752', 'g-753', 'g-756', 'g-757', 'g-758', 'g-759', 'g-760', 'g-761', 'g-762', 'g-763', 'g-764', 'g-766', 'g-767', 'g-768', 'g-769', 'g-770', 'g-771', 'c-0', 'c-1', 'c-2', 'c-3', 'c-4', 'c-5', 'c-10', 'c-11', 'c-12', 'c-13', 'c-14', 'c-15', 'c-16', 'c-17', 'c-18', 'c-19', 'c-20', 'c-21', 'c-22', 'c-23', 'c-24', 'c-25', 'c-26', 'c-27', 'c-32', 'c-35', 'c-36', 'c-37', 'c-40', 'c-43', 'c-47', 'c-48', 'c-52', 'c-53', 'c-54', 'c-55', 'c-56', 'c-57', 'c-58', 'c-61', 'c-62', 'c-63', 'c-64', 'c-65', 'c-66', 'c-67', 'c-68', 'c-69', 'c-70', 'c-71', 'c-72', 'c-73', 'c-74', 'c-75', 'c-76', 'c-77', 'c-78', 'c-79', 'c-80', 'c-81', 'c-82', 'c-83', 'c-84', 'c-85', 'c-86', 'c-87', 'c-88', 'c-89', 'c-90', 'c-91', 'c-92', 'c-93', 'c-94', 'c-97']\n}","e96658f8":"# from RFECV in Version 13 (700 features)\noptions['700'] = {\n    'features': ['g-0', 'g-1', 'g-2', 'g-3', 'g-4', 'g-5', 'g-6', 'g-7', 'g-8', 'g-9', 'g-10', 'g-11', 'g-12', 'g-13', 'g-14', 'g-15', 'g-16', 'g-17', 'g-18', 'g-19', 'g-20', 'g-21', 'g-22', 'g-23', 'g-24', 'g-25', 'g-26', 'g-27', 'g-28', 'g-29', 'g-30', 'g-31', 'g-32', 'g-33', 'g-34', 'g-35', 'g-36', 'g-37', 'g-38', 'g-39', 'g-40', 'g-41', 'g-42', 'g-43', 'g-44', 'g-45', 'g-46', 'g-47', 'g-48', 'g-49', 'g-50', 'g-51', 'g-52', 'g-53', 'g-54', 'g-55', 'g-56', 'g-57', 'g-58', 'g-59', 'g-60', 'g-61', 'g-62', 'g-63', 'g-64', 'g-65', 'g-66', 'g-67', 'g-68', 'g-69', 'g-70', 'g-71', 'g-72', 'g-73', 'g-74', 'g-75', 'g-76', 'g-77', 'g-78', 'g-79', 'g-80', 'g-81', 'g-82', 'g-83', 'g-84', 'g-85', 'g-86', 'g-87', 'g-88', 'g-89', 'g-90', 'g-91', 'g-92', 'g-93', 'g-94', 'g-95', 'g-96', 'g-97', 'g-98', 'g-99', 'g-100', 'g-101', 'g-102', 'g-103', 'g-104', 'g-105', 'g-106', 'g-107', 'g-108', 'g-109', 'g-110', 'g-111', 'g-112', 'g-113', 'g-114', 'g-115', 'g-116', 'g-117', 'g-118', 'g-119', 'g-120', 'g-121', 'g-122', 'g-123', 'g-124', 'g-125', 'g-126', 'g-127', 'g-128', 'g-129', 'g-130', 'g-131', 'g-132', 'g-133', 'g-134', 'g-135', 'g-136', 'g-137', 'g-138', 'g-139', 'g-140', 'g-141', 'g-142', 'g-143', 'g-144', 'g-145', 'g-146', 'g-147', 'g-148', 'g-149', 'g-150', 'g-151', 'g-152', 'g-153', 'g-154', 'g-155', 'g-156', 'g-157', 'g-158', 'g-159', 'g-160', 'g-161', 'g-162', 'g-163', 'g-164', 'g-165', 'g-166', 'g-167', 'g-168', 'g-169', 'g-170', 'g-171', 'g-172', 'g-173', 'g-174', 'g-175', 'g-176', 'g-177', 'g-178', 'g-179', 'g-180', 'g-181', 'g-182', 'g-183', 'g-184', 'g-185', 'g-186', 'g-187', 'g-188', 'g-189', 'g-190', 'g-191', 'g-192', 'g-193', 'g-194', 'g-195', 'g-196', 'g-197', 'g-198', 'g-199', 'g-200', 'g-201', 'g-202', 'g-203', 'g-204', 'g-205', 'g-206', 'g-207', 'g-208', 'g-209', 'g-210', 'g-211', 'g-212', 'g-213', 'g-214', 'g-215', 'g-216', 'g-217', 'g-218', 'g-219', 'g-220', 'g-221', 'g-222', 'g-223', 'g-224', 'g-225', 'g-226', 'g-227', 'g-228', 'g-229', 'g-230', 'g-231', 'g-232', 'g-233', 'g-234', 'g-235', 'g-236', 'g-237', 'g-238', 'g-239', 'g-240', 'g-241', 'g-242', 'g-243', 'g-244', 'g-245', 'g-246', 'g-247', 'g-248', 'g-249', 'g-250', 'g-251', 'g-252', 'g-253', 'g-254', 'g-255', 'g-256', 'g-257', 'g-258', 'g-259', 'g-260', 'g-261', 'g-262', 'g-263', 'g-264', 'g-265', 'g-266', 'g-267', 'g-268', 'g-269', 'g-270', 'g-271', 'g-272', 'g-273', 'g-274', 'g-275', 'g-279', 'g-282', 'g-284', 'g-285', 'g-286', 'g-287', 'g-288', 'g-289', 'g-290', 'g-291', 'g-292', 'g-293', 'g-294', 'g-295', 'g-296', 'g-297', 'g-298', 'g-299', 'g-300', 'g-301', 'g-302', 'g-303', 'g-304', 'g-305', 'g-306', 'g-307', 'g-310', 'g-311', 'g-312', 'g-314', 'g-316', 'g-317', 'g-318', 'g-319', 'g-320', 'g-321', 'g-322', 'g-323', 'g-324', 'g-325', 'g-326', 'g-327', 'g-328', 'g-332', 'g-333', 'g-335', 'g-339', 'g-340', 'g-341', 'g-342', 'g-343', 'g-344', 'g-345', 'g-346', 'g-347', 'g-348', 'g-349', 'g-350', 'g-351', 'g-352', 'g-353', 'g-354', 'g-355', 'g-357', 'g-360', 'g-364', 'g-365', 'g-370', 'g-375', 'g-383', 'g-385', 'g-388', 'g-389', 'g-390', 'g-391', 'g-392', 'g-399', 'g-401', 'g-402', 'g-403', 'g-404', 'g-405', 'g-406', 'g-407', 'g-409', 'g-417', 'g-423', 'g-424', 'g-425', 'g-426', 'g-428', 'g-429', 'g-430', 'g-431', 'g-433', 'g-434', 'g-435', 'g-436', 'g-438', 'g-439', 'g-440', 'g-441', 'g-443', 'g-444', 'g-445', 'g-446', 'g-447', 'g-448', 'g-449', 'g-450', 'g-451', 'g-452', 'g-453', 'g-454', 'g-455', 'g-456', 'g-460', 'g-461', 'g-463', 'g-465', 'g-467', 'g-470', 'g-473', 'g-474', 'g-475', 'g-476', 'g-481', 'g-483', 'g-485', 'g-486', 'g-489', 'g-490', 'g-495', 'g-500', 'g-501', 'g-502', 'g-504', 'g-509', 'g-513', 'g-516', 'g-517', 'g-518', 'g-527', 'g-531', 'g-535', 'g-540', 'g-542', 'g-543', 'g-544', 'g-547', 'g-548', 'g-549', 'g-552', 'g-556', 'g-558', 'g-562', 'g-566', 'g-571', 'g-572', 'g-573', 'g-574', 'g-578', 'g-584', 'g-585', 'g-590', 'g-592', 'g-595', 'g-596', 'g-599', 'g-600', 'g-601', 'g-602', 'g-603', 'g-604', 'g-605', 'g-606', 'g-607', 'g-608', 'g-609', 'g-610', 'g-611', 'g-612', 'g-613', 'g-614', 'g-615', 'g-616', 'g-617', 'g-618', 'g-619', 'g-620', 'g-621', 'g-622', 'g-623', 'g-624', 'g-625', 'g-626', 'g-627', 'g-628', 'g-629', 'g-630', 'g-631', 'g-632', 'g-633', 'g-634', 'g-635', 'g-636', 'g-637', 'g-638', 'g-639', 'g-640', 'g-641', 'g-642', 'g-643', 'g-644', 'g-645', 'g-646', 'g-647', 'g-648', 'g-649', 'g-650', 'g-651', 'g-652', 'g-654', 'g-655', 'g-656', 'g-657', 'g-658', 'g-659', 'g-660', 'g-661', 'g-662', 'g-663', 'g-664', 'g-665', 'g-666', 'g-667', 'g-668', 'g-670', 'g-672', 'g-673', 'g-674', 'g-675', 'g-676', 'g-677', 'g-678', 'g-679', 'g-680', 'g-685', 'g-687', 'g-688', 'g-689', 'g-690', 'g-691', 'g-692', 'g-695', 'g-697', 'g-698', 'g-699', 'g-700', 'g-701', 'g-702', 'g-703', 'g-704', 'g-705', 'g-706', 'g-707', 'g-708', 'g-709', 'g-710', 'g-711', 'g-712', 'g-713', 'g-714', 'g-715', 'g-716', 'g-717', 'g-718', 'g-719', 'g-720', 'g-721', 'g-722', 'g-723', 'g-724', 'g-725', 'g-726', 'g-727', 'g-728', 'g-729', 'g-730', 'g-731', 'g-732', 'g-733', 'g-734', 'g-735', 'g-736', 'g-737', 'g-738', 'g-739', 'g-740', 'g-741', 'g-742', 'g-743', 'g-744', 'g-745', 'g-746', 'g-747', 'g-748', 'g-749', 'g-750', 'g-751', 'g-752', 'g-753', 'g-754', 'g-755', 'g-756', 'g-757', 'g-758', 'g-759', 'g-760', 'g-761', 'g-762', 'g-763', 'g-764', 'g-765', 'g-766', 'g-767', 'g-768', 'g-769', 'g-770', 'g-771', 'c-0', 'c-1', 'c-2', 'c-3', 'c-4', 'c-5', 'c-6', 'c-7', 'c-10', 'c-11', 'c-12', 'c-13', 'c-14', 'c-15', 'c-16', 'c-17', 'c-18', 'c-19', 'c-20', 'c-21', 'c-22', 'c-23', 'c-24', 'c-25', 'c-26', 'c-27', 'c-28', 'c-29', 'c-30', 'c-31', 'c-32', 'c-33', 'c-34', 'c-35', 'c-36', 'c-37', 'c-40', 'c-41', 'c-42', 'c-43', 'c-44', 'c-45', 'c-46', 'c-47', 'c-48', 'c-49', 'c-50', 'c-51', 'c-52', 'c-53', 'c-54', 'c-55', 'c-56', 'c-57', 'c-58', 'c-59', 'c-60', 'c-61', 'c-62', 'c-63', 'c-64', 'c-65', 'c-66', 'c-67', 'c-68', 'c-69', 'c-70', 'c-71', 'c-72', 'c-73', 'c-74', 'c-75', 'c-76', 'c-77', 'c-78', 'c-79', 'c-80', 'c-81', 'c-82', 'c-83', 'c-84', 'c-85', 'c-86', 'c-87', 'c-88', 'c-89', 'c-90', 'c-91', 'c-92', 'c-93', 'c-94', 'c-95', 'c-96', 'c-97']\n}","b399f270":"import tensorflow as tf\nimport tensorflow_addons as tfa\n\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import accuracy_score\n\nimport tensorflow.keras.backend as K","7a066422":"def make_layer(x, units, dropout_rate):\n    t = tfa.layers.WeightNormalization(tf.keras.layers.Dense(units))(x)\n    # t = tf.keras.layers.Dense(units)(x)\n    t = tf.keras.layers.BatchNormalization()(t)\n    t = tf.keras.layers.Activation('relu')(t)\n    t = tf.keras.layers.Dropout(dropout_rate)(t)\n    return t\n\n\ndef make_model(data, units, dropout_rates):\n    \n    inputs = tf.keras.layers.Input(shape=(data.shape[1],))\n    x = tf.keras.layers.BatchNormalization()(inputs)\n\n    for i in range(len(units)):\n        u = units[i]\n        d = dropout_rates[i]\n        x = make_layer(x, u, d)\n       \n    y = tf.keras.layers.Dense(206, activation='sigmoid', name='dense_output')(x)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=y)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-5),\n                  metrics=['accuracy'])\n    return model","c4dc9a31":"def fit_predict(n_splits, x_train, y_train, units, dropout_rates, epochs, x_test, verbose, random_state):\n\n    histories = []\n    scores = []\n    y_preds = []\n\n    cv = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n    for train_idx, valid_idx in cv.split(x_train, y_train):\n\n        x_train_train = x_train.iloc[train_idx]\n        y_train_train = y_train.iloc[train_idx]\n        x_train_valid = x_train.iloc[valid_idx]\n        y_train_valid = y_train.iloc[valid_idx]\n\n        K.clear_session()\n\n        estimator = make_model(x_train, units, dropout_rates)\n\n        es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=10,\n                                              verbose=verbose, mode='min', restore_best_weights=True)\n\n        rl = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-5,\n                                                  mode='min', verbose=verbose)\n\n        history = estimator.fit(x_train_train, y_train_train,\n                                batch_size=128, epochs=epochs, callbacks=[es, rl],\n                                validation_data=(x_train_valid, y_train_valid),\n                                verbose=verbose)\n        \n        if x_test is not None:\n            y_part = estimator.predict(x_test)\n            y_preds.append(y_part)\n\n        histories.append(history)\n        scores.append(history.history['val_loss'][-1])\n    \n    if x_test is not None:\n        y_pred = np.mean(y_preds, axis=0)\n    else:\n        y_pred = None\n\n    score = np.mean(scores)\n    \n    return y_pred, histories, score","2573bc56":"import optuna\n\nfrom logging import CRITICAL\noptuna.logging.set_verbosity(CRITICAL)","e22dc84f":"def objective(trial):\n    \n    n_layers = trial.suggest_int('n_layers', 1, 5)\n    \n    units = []\n    dropout_rates = []\n    for i in range(n_layers):\n        u = trial.suggest_categorical('units_{}'.format(i+1), [1024, 512, 256, 128])\n        units.append(u)\n        r = trial.suggest_loguniform('dropout_rate_{}'.format(i+1), 0.1, 0.5)\n        dropout_rates.append(r)\n    \n    print('Units:', units, \"Dropout rates:\", dropout_rates)\n    \n    _, _, score = fit_predict(3, x_train, y_train, units, dropout_rates, 25, None, 0, 42)\n    return score\n\n\n# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=75)","460ded38":"# params = study.best_trial.params\n# params","4534ced5":"# from Optuna in Version 8\nparams = {\n    'n_layers': 5,\n    'units_1': 128,\n    'units_2': 256,\n    'units_3': 512,\n    'units_4': 256,\n    'units_5': 1024,\n    'dropout_rate_1': 0.3478936880741539,\n    'dropout_rate_2': 0.3478936880741539,\n    'dropout_rate_3': 0.3478936880741539,\n    'dropout_rate_4': 0.3478936880741539,\n    'dropout_rate_5': 0.3478936880741539\n}\n\noptions['default']['params'] = params","c354c721":"# from Optuna in Version 13\nparams = {\n    'n_layers': 3,\n    'units_1': 1024,\n    'units_2': 512,\n    'units_3': 256,\n    'dropout_rate_1': 0.4501813451502177,\n    'dropout_rate_2': 0.4501813451502177,\n    'dropout_rate_3': 0.4501813451502177\n}\n\noptions['500']['params'] = params\noptions['600']['params'] = params\noptions['700']['params'] = params","861cc221":"def fit_predict_option(option, random_state):\n    print('Option:', option)\n    \n    params = options[option]['params']\n    \n    n_layers = params['n_layers']\n    units = []\n    dropout_rates = []\n    for i in range(n_layers):\n        u = params['units_{}'.format(i+1)]\n        units.append(u)\n        d = params['dropout_rate_{}'.format(i+1)]\n        dropout_rates.append(d)\n\n    x_train_option, x_test_option = make_x(option)\n\n    # 7, 100, 2\n    y_pred, histories, score = fit_predict(7, x_train_option, y_train, units, dropout_rates, 100, x_test_option, 2, random_state)\n    \n    print('Score:', score)\n    \n    return y_pred, histories, score","05862d3e":"y_preds = []\n\nfor option in options:\n    y_pred, histories, score = fit_predict_option(option, 42)\n    y_preds.append(y_pred)\n    # break","ba444565":"len(y_preds)","fb5f4ef4":"y_pred = np.mean(y_preds, axis=0)\n\ny_pred.shape","ce87f8c0":"import matplotlib.pyplot as plt\n\n\nfig, axs = plt.subplots(2, 2, figsize=(18,18))\n\n# accuracy\nfor h in histories:\n    axs[0,0].plot(h.history['accuracy'], color='g')\naxs[0,0].set_title('Model accuracy - Train')\naxs[0,0].set_ylabel('Accuracy')\naxs[0,0].set_xlabel('Epoch')\n\nfor h in histories:\n    axs[0,1].plot(h.history['val_accuracy'], color='b')\naxs[0,1].set_title('Model accuracy - Test')\naxs[0,1].set_ylabel('Accuracy')\naxs[0,1].set_xlabel('Epoch')\n\n# loss\nfor h in histories:\n    axs[1,0].plot(h.history['loss'], color='g')\naxs[1,0].set_title('Model loss - Train')\naxs[1,0].set_ylabel('Loss')\naxs[1,0].set_xlabel('Epoch')\n\nfor h in histories:\n    axs[1,1].plot(h.history['val_loss'], color='b')\naxs[1,1].set_title('Model loss - Test')\naxs[1,1].set_ylabel('Loss')\naxs[1,1].set_xlabel('Epoch')\n\nfig.show()","06485094":"submission = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\n\ncolumns = list(submission.columns)\ncolumns.remove('sig_id')\n\nfor i in range(len(columns)):\n    submission[columns[i]] = y_pred[:,i]\n\nsubmission.to_csv('submission.csv', index=False)","9bed5b07":"submission.head()","62cd3eba":"## Feature engineering","9abea06b":"## OHE","51d30c51":"## Submit predictions","2c5cb671":"## Keras","7c49fe83":"## Prepare x, y","4796454c":"## Show graphs","6191edb5":"## Read data","a621e1cc":"## RFECV","879ae168":"## StandardScaler","9199b7b5":"# Keras\n\nFeatures:\n* StandardScaler for 'g-' features\n* RFECV with LightGBM (700, 600, 500 features)\n* Optuna\n* 7-fold CV (KFold)\n* Tune Adam and learning rate reduce\n* WeightNormalization\n* Accuracy, LogLoss visualization","b519b354":"## Optuna"}}