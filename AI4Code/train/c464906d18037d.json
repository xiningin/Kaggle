{"cell_type":{"e2cb5bb6":"code","120f6e08":"code","fbbfe7d0":"code","d116bb67":"code","777de8cd":"code","9d0e6ef1":"code","e9c8a053":"code","0ae18a98":"code","0c63b87a":"code","4fafe7e4":"code","7427a1df":"code","70a8fb17":"code","cf3cb7ab":"code","847b12e6":"code","cba6aaaa":"code","55be2f40":"code","a18a9517":"code","85ccbd75":"code","e14c19f7":"code","f09b3c3c":"code","837d23c0":"code","eba82eb6":"code","b7dc88d6":"code","50179883":"code","13325086":"code","1a3fb5ac":"code","ccf8f62c":"code","98d5bcbe":"code","0c0cb787":"code","b6a71156":"code","e5f1bc39":"code","8e57454c":"code","cf612781":"code","6777f385":"code","8dd022f3":"code","d57d0dcc":"code","056710e2":"code","234604be":"code","ba9f5c2a":"code","964c7f29":"code","5fe38a69":"code","ec58737e":"code","0f8da2b3":"code","b1e6edef":"code","f1691ae1":"code","b75af00e":"code","e3ce5fd3":"code","dee628db":"code","31b070a4":"code","420d6dac":"code","6166aeb9":"code","1ec474b6":"code","27a187d8":"code","0b9854bb":"code","231fa8a9":"code","1d609269":"code","75de1d77":"code","d76ba1a8":"code","e1ac37d9":"code","1f1d1f68":"code","e9895576":"code","80ce8e39":"code","17f7343c":"code","9b9e1672":"code","6f5ccfae":"code","356526ea":"code","749273e5":"code","bb652f0d":"code","8d8bd64e":"code","119bcf97":"code","bc16a95b":"code","fa3e8c4d":"code","4eb682ef":"code","8719b96d":"code","5be0d8bc":"code","fcac0c22":"code","c5347c4a":"code","be22b128":"code","f1e5e735":"code","ae53ee0b":"code","a0c235cf":"code","f66bf950":"code","cdf4eb0f":"code","3de6892c":"code","2322d0c8":"code","55edda2a":"code","86236826":"code","464f0a0d":"code","84a8db83":"code","7e39d275":"code","713951b0":"code","d2431fbd":"code","cc7bfe91":"code","4294d9f8":"code","daa3ca28":"code","3495c263":"code","4a8ba5da":"code","906b6763":"code","e0b2cbc3":"code","63116917":"code","b53ee94d":"code","4d67bae6":"code","f5b516e2":"code","f1b0ae98":"code","b123937f":"code","89d51e39":"code","3692ccb0":"code","6279a4a7":"code","c51aa7d6":"code","3c57deb2":"markdown","6247c84c":"markdown","8ec10283":"markdown","86f40981":"markdown","23c21415":"markdown","18aafb84":"markdown","6a4a0d2a":"markdown","26d0355f":"markdown","1e729dde":"markdown","2a8bdb2c":"markdown","1d2a5704":"markdown","ef9be080":"markdown","67aa9dbb":"markdown","9d3ade8b":"markdown","d79e8e87":"markdown","7f3c4968":"markdown","d7b96266":"markdown","c3a6d49c":"markdown","264e305b":"markdown","0cd66f23":"markdown","54bda2d6":"markdown","841d613f":"markdown","c19b67dc":"markdown","b7927f64":"markdown","0024bc33":"markdown","e5f2e6f1":"markdown","66a93b63":"markdown","562c5213":"markdown","d4d6e278":"markdown","8bed1dab":"markdown","f5d3ec68":"markdown","4ae8763b":"markdown","048d0b06":"markdown","ef01e6a8":"markdown","18b48ee5":"markdown","44b8f767":"markdown","30d5f346":"markdown","d3711eb0":"markdown","85bb4787":"markdown","7d3b7e27":"markdown","4275384b":"markdown","3ca435da":"markdown","61a2025f":"markdown","aa1b4253":"markdown","07007dd9":"markdown","fe38a987":"markdown","0c310c24":"markdown","9d2d1d30":"markdown","22d2a465":"markdown","f65e8a0b":"markdown","00d8d54c":"markdown","5e496579":"markdown","ea666fba":"markdown","82969120":"markdown","dd48f176":"markdown","0f29a769":"markdown","c9a0bc3b":"markdown","b29967b0":"markdown","11a80ed0":"markdown","60298a18":"markdown","46911f7e":"markdown","002c8cc8":"markdown","b18ec616":"markdown","24ef53cd":"markdown","e370b7f3":"markdown","025a9787":"markdown","c017897b":"markdown","6b89ddc5":"markdown","b9058e58":"markdown","535b8338":"markdown","a9007db2":"markdown","12f97e3b":"markdown","472fba61":"markdown","734401a1":"markdown","be368fc2":"markdown","7186397c":"markdown","27aac037":"markdown","21d99ea7":"markdown","34cdc7eb":"markdown","4f05e88c":"markdown","47d3688f":"markdown","eb5fedd2":"markdown","22dcda6d":"markdown","20275a18":"markdown","a66eb02e":"markdown","e909d532":"markdown","c51cc392":"markdown","08113297":"markdown","2dc4c971":"markdown","3af5f0c1":"markdown","30a5b190":"markdown","34b4c817":"markdown","9111b31c":"markdown","4320f2cd":"markdown","3a4ab9ef":"markdown","d77d78e1":"markdown"},"source":{"e2cb5bb6":"from IPython.display import YouTubeVideo\nYouTubeVideo(\"pN4HqWRybwk\")","120f6e08":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set(color_codes = True)\n\n\n#ignore warning messages \nimport warnings\nwarnings.filterwarnings('ignore')","fbbfe7d0":"pima = pd.read_csv('..\/input\/pimaindiansdiabetescsv\/pima-indians-diabetes.csv')","d116bb67":"pima","777de8cd":"pima.head()","9d0e6ef1":"pima.tail()","e9c8a053":"pima.shape","0ae18a98":"pima.dtypes","0c63b87a":"pima.info()","4fafe7e4":"pima.isnull().values.any()","7427a1df":"pima.describe().T","70a8fb17":"pima['class'].value_counts()","cf3cb7ab":"plt.figure(figsize=(10,7))\nsns.set(font_scale = 1.5)\nsns.countplot(x = 'class', data=pima, palette=\"Set2\")\nplt.ylabel('Number of People')","847b12e6":"plt.figure(figsize=(8,8))\npieC = pima['class'].value_counts()\nexplode = (0.05, 0)\ncolors = ['moccasin', 'coral']\nlabels = ['0 - Non Diabetic', '1 - Diabetic']\nsns.set(font_scale = 1.5)\nplt.pie(pieC, labels = ('0 - Non Diabetic', '1 - Diabetic'), autopct = \"%.2f%%\", explode = explode, colors = colors)\nplt.legend(labels, loc = 'lower left')","cba6aaaa":"pima[pima['Plas'] == 0]","55be2f40":"missingPlas = pima[pima['Plas'] == 0].shape[0]\nprint (\"Number of zeros in variable Plas (Glucose): \", missingPlas)","a18a9517":"missingPres = pima[pima['Pres'] == 0].shape[0]\nprint (\"Number of zeros in variable Pres (BloodPressure): \", missingPres)","85ccbd75":"missingSkin = pima[pima['skin'] == 0].shape[0]\nprint (\"Number of zeros in variable Skin (SkinThickness): \", missingSkin)","e14c19f7":"missingTest = pima[pima['test'] == 0].shape[0]\nprint (\"Number of zeros in variable Test (Insulin): \", missingTest)","f09b3c3c":"missingMass = pima[pima['mass'] == 0].shape[0]\nprint (\"Number of zeros in variable Mass (BMI): \", missingMass)","837d23c0":"pima_copy = pima.copy(deep = True)","eba82eb6":"pima_copy[['Plas','Pres','skin','test','mass']] = pima_copy[['Plas','Pres','skin','test','mass']].replace(0,np.NaN)\nprint(pima_copy.isnull().sum())","b7dc88d6":"pima.hist(figsize = (20,16),grid=True)","50179883":"pima.plot(kind= 'box' , subplots=True, layout=(3,3), sharex=False, sharey=False, figsize=(15,15))\nsns.set(font_scale = 1.5)","13325086":"fig, ax = plt.subplots(4,2, figsize=(16,16))\nsns.set(font_scale = 1)\nsns.distplot(pima.Plas, ax = ax[0,0], color = 'orange')\nsns.distplot(pima.Preg, ax = ax[0,1], color = 'red')\nsns.distplot(pima.Pres, ax = ax[1,0], color = 'seagreen')\nsns.distplot(pima.age, ax = ax[1,1], color = 'purple')\nsns.distplot(pima.mass, ax = ax[2,0], color = 'deeppink')\nsns.distplot(pima.pedi, ax = ax[2,1], color = 'brown')\nsns.distplot(pima.skin, ax = ax[3,0], color = 'royalblue')\nsns.distplot(pima.test, ax = ax[3,1], color = 'coral')","1a3fb5ac":"plt.figure(figsize=(15,6))\nsns.set(font_scale = 1.5)\nsns.distplot(pima['Plas'], kde = True, rug = True, color = 'orange')","ccf8f62c":"plt.figure(figsize=(15,6))\nsns.set(font_scale = 1.5)\nsns.boxplot(pima.Plas, color = 'orange')","98d5bcbe":"plt.figure(figsize=(15,6))\nsns.set(font_scale = 1.5)\nsns.distplot(pima['Pres'], kde = True, rug = True, color = 'seagreen')","0c0cb787":"plt.figure(figsize=(15,6))\nsns.set(font_scale = 1.5)\nsns.boxplot(pima.Pres, color = 'seagreen')","b6a71156":"plt.figure(figsize=(15,6))\nsns.set(font_scale = 1.5)\nsns.distplot(pima['skin'], kde = True, rug = True, color = 'royalblue')","e5f1bc39":"plt.figure(figsize=(15,6))\nsns.set(font_scale = 1.5)\nsns.boxplot(pima.skin, color = 'royalblue')","8e57454c":"plt.figure(figsize=(15,6))\nsns.set(font_scale = 1.5)\nsns.distplot(pima['test'], kde = True, rug = True, color = 'coral')","cf612781":"plt.figure(figsize=(15,6))\nsns.set(font_scale = 1.5)\nsns.boxplot(pima.test, color = 'coral')","6777f385":"plt.figure(figsize=(15,6))\nsns.set(font_scale = 1.5)\nsns.distplot(pima['mass'], kde = True, rug = True, color = 'deeppink')","8dd022f3":"plt.figure(figsize=(15,6))\nsns.set(font_scale = 1.5)\nsns.boxplot(pima.mass, color = 'deeppink')","d57d0dcc":"pima_copy['Plas'].fillna(pima_copy['Plas'].mean(), inplace = True)","056710e2":"pima_copy['Pres'].fillna(pima_copy['Pres'].mean(), inplace = True)","234604be":"pima_copy['skin'].fillna(pima_copy['skin'].median(), inplace = True)","ba9f5c2a":"pima_copy['test'].fillna(pima_copy['test'].median(), inplace = True)","964c7f29":"pima_copy['mass'].fillna(pima_copy['mass'].median(), inplace = True)","5fe38a69":"print(pima_copy.isnull().sum())","ec58737e":"pima_copy.describe().T","0f8da2b3":"pima_copy.plot(kind= 'box' , subplots=True, layout=(3,3), sharex=False, sharey=False, figsize=(15,15))\nsns.set(font_scale = 1.5)","b1e6edef":"plt.figure(figsize=(15,6))\nsns.set(font_scale = 1.5)\nsns.countplot(pima_copy['Preg'])\nplt.ylabel('Number of People')","f1691ae1":"print(\"Average number of children had by Pima woman: \", pima_copy['Preg'].mean())","b75af00e":"pima_copy['Preg'].median()","e3ce5fd3":"preg = pima_copy[pima_copy['Preg'] >= 1].shape[0]\nprint('Number of Pima Woman who had children: ', preg)","dee628db":"notPreg = pima_copy[pima_copy['Preg'] == 0].shape[0]\nprint('Number of Pima woman who did not have children: ', notPreg)","31b070a4":"pregPlusDiabetes = pima_copy[(pima_copy['Preg'] >= 1) & (pima_copy['class'] == 1)].shape[0]\nprint('Number of woman who have children and are diabetic: ',pregPlusDiabetes)","420d6dac":"pregPlusNotDiabetes = pima_copy[(pima_copy['Preg'] >= 1) & (pima_copy['class'] == 0)].shape[0]\nprint('Number of woman who have children and are not diabetic: ',pregPlusNotDiabetes)","6166aeb9":"notPregPlusDiabetes = pima_copy[(pima_copy['Preg'] == 0) & (pima_copy['class'] == 1)].shape[0]\nprint('Number of woman who do not have children and are diabetic: ',notPregPlusDiabetes)","1ec474b6":"notPregPlusNotDiabetes = pima_copy[(pima_copy['Preg'] == 0) & (pima_copy['class'] == 0)].shape[0]\nprint('Number of woman who do not have children and are not diabetic: ',notPregPlusNotDiabetes)","27a187d8":"corr = pima_copy.corr()\ncorr","0b9854bb":"plt.figure(figsize=(15,10))\nsns.set(font_scale = 1.5)\nsns.heatmap(corr, annot = True, cmap = 'plasma', vmin = -1, vmax = 1, linecolor='white', linewidths= 1)","231fa8a9":"print('Average Glucose for Pima woman who has diabetes: ', pima_copy[pima_copy['class'] == 1]['Plas'].mean())","1d609269":"print('Average Glucose for Pima woman who does not have diabetes: ', pima_copy[pima_copy['class'] == 0]['Plas'].mean())","75de1d77":"plt.figure(figsize=(15,7))\nsns.boxplot(pima_copy['class'],pima_copy['Plas'], palette=\"Set2\")\nsns.set(font_scale = 1.5)","d76ba1a8":"print('Average BMI for Pima woman who has diabetes: ', pima_copy[pima_copy['class'] == 1]['mass'].mean())","e1ac37d9":"print('Average BMI for Pima woman who does not have diabetes: ', pima_copy[pima_copy['class'] == 0]['mass'].mean())","1f1d1f68":"plt.figure(figsize=(15,6))\nsns.set(font_scale = 1.5)\nsns.boxplot(pima_copy['class'],pima_copy['age'], palette = \"Set3\")","e9895576":"oneOutcome = pima_copy[pima_copy['class'] == 1]\nprint(\"Minimum age of Pima woman who has Diabetes: \",oneOutcome['age'].min())","80ce8e39":"print(\"Maximum age of Pima woman who has Diabetes: \",oneOutcome['age'].max())","17f7343c":"zeroOutcome = pima_copy[pima_copy['class'] == 0]\nprint(\"Minimum age of Pima woman who does not have Diabetes: \",zeroOutcome['age'].min())","9b9e1672":"zeroOutcome = pima_copy[pima_copy['class'] == 0]\nprint(\"Maximum age of Pima woman who does not have Diabetes: \",zeroOutcome['age'].max())","6f5ccfae":"print('Average Age of Pima woman who has diabetes: ',pima_copy[pima_copy['class'] == 1]['age'].mean())","356526ea":"print('Average Age of Pima woman who does not have diabetes: ',pima_copy[pima_copy['class'] == 0]['age'].mean())","749273e5":"plt.figure(figsize=(15,6))\nsns.set(font_scale = 1.5)\nsns.countplot(x = 'Preg', hue = 'class', data = pima_copy, palette = 'Set2')","bb652f0d":"print('Average Skin Thickness of Pima woman who has diabetes: ', pima_copy[pima_copy['class'] == 1]['skin'].mean())","8d8bd64e":"print('Average Skin Thickness of Pima woman who does not have diabetes: ', pima_copy[pima_copy['class'] == 0]['skin'].mean())","119bcf97":"print('Average Insulin of Pima woman who has diabetes: ', pima_copy[pima_copy['class'] == 1]['test'].mean())","bc16a95b":"print('Average Insulin of Pima woman who does not have diabetes: ', pima_copy[pima_copy['class'] == 0]['test'].mean())","fa3e8c4d":"sns.set(font_scale = 1.5)\nsns.pairplot(data = pima_copy, hue = 'class', diag_kind = 'kde', palette = 'Set2')","4eb682ef":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve, auc","8719b96d":"from sklearn.model_selection import train_test_split\nX = pima_copy.drop('class', axis  = 1)\ny = pima_copy['class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 17)","5be0d8bc":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.size)\nprint(y_test.size)","fcac0c22":"model = LogisticRegression()","c5347c4a":"model.fit(X_train, y_train)","be22b128":"y_pred = model.predict(X_test)","f1e5e735":"confusion = metrics.confusion_matrix(y_test,y_pred)\nconfusion","ae53ee0b":"ylabel = [\"Actual [Non-Diab]\",\"Actual [Diab]\"]\nxlabel = [\"Pred [Non-Diab]\",\"Pred [Diab]\"]\n#sns.set(font_scale = 1.5)\nplt.figure(figsize=(15,6))\nsns.heatmap(confusion, annot=True, xticklabels = xlabel, yticklabels = ylabel, linecolor='white', linewidths=1)","a0c235cf":"print('Accuracy of Logistic Regression is: ', model.score(X_test,y_test) * 100,'%')","f66bf950":"print(classification_report(y_test,y_pred))","cdf4eb0f":"TP = confusion[1, 1]\nTN = confusion[0, 0]\nFP = confusion[0, 1]\nFN = confusion[1, 0]","3de6892c":"# print ('Precision: ', metrics.precision_score(y_test,y_pred))\nPrecision = TP \/ ( TP + FP )\nprint ('Precision: ', Precision)","2322d0c8":"Recall = TP \/ ( TP + FN )\nprint ('Recall: ', Recall)","55edda2a":"pima_copy['class'].value_counts()","86236826":"metrics.f1_score(y_test, y_pred)","464f0a0d":"Specificity = TN \/ ( TN + FP )\nprint ('Specificity: ', Specificity)","84a8db83":"Sensitivity = TP \/ ( TP + FN )\nprint ('Sensitivity: ', Sensitivity)","7e39d275":"Roc_Auc = metrics.roc_auc_score(y_test, y_pred)\nprint ('Roc Auc Score: ', Roc_Auc)","713951b0":"y_pred_prob = model.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)","d2431fbd":"plt.figure(figsize=(15,6))\nsns.set(font_scale = 1.5)\nplt.plot(fpr, tpr)\nplt.title('ROC Curve for Logistic Regression Diabetes Classifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)\nplt.xlim(0.0, 1.0)\nplt.ylim(0.0, 1.0)\nplt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')","cc7bfe91":"nbModel = GaussianNB()","4294d9f8":"nbModel.fit(X_train, y_train)","daa3ca28":"nb_y_pred = nbModel.predict(X_test)","3495c263":"nbConfusion = metrics.confusion_matrix(y_test, nb_y_pred)\nnbConfusion","4a8ba5da":"ylabel = [\"Actual [Non-Diab]\",\"Actual [Diab]\"]\nxlabel = [\"Pred [Non-Diab]\",\"Pred [Diab]\"]\n#sns.set(font_scale = 1.5)\nplt.figure(figsize=(15,6))\nsns.heatmap(nbConfusion, annot=True, xticklabels = xlabel, yticklabels = ylabel, linecolor='white', linewidths=1)","906b6763":"print('Accuracy of Naive Bayes Classifier is: ', nbModel.score(X_test,y_test) * 100,'%')","e0b2cbc3":"print(classification_report(y_test, nb_y_pred))","63116917":"nb_TP = nbConfusion[1, 1]\nnb_TN = nbConfusion[0, 0]\nnb_FP = nbConfusion[0, 1]\nnb_FN = nbConfusion[1, 0]","b53ee94d":"# print ('Precision: ', metrics.precision_score(y_test, nb_y_pred))\nnb_Precision = nb_TP \/ ( nb_TP + nb_FP)\nprint ('Precision: ', nb_Precision)","4d67bae6":"nb_Recall = nb_TP \/ ( nb_TP + nb_FN )\nprint ('Recall: ', nb_Recall)","f5b516e2":"pima_copy['class'].value_counts()","f1b0ae98":"metrics.f1_score(y_test, nb_y_pred)","b123937f":"nb_Specificity = nb_TN \/ ( nb_TN + nb_FP )\nprint ('Specificity: ', nb_Specificity)","89d51e39":"nb_Sensitivity = nb_TP \/ ( nb_TP + nb_FN )\nprint ('Sensitivity: ', nb_Sensitivity)","3692ccb0":"nb_Roc_Auc = metrics.roc_auc_score(y_test,nb_y_pred)\nprint ('Roc Auc Score: ', nb_Roc_Auc)","6279a4a7":"nb_y_pred_prob = nbModel.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = metrics.roc_curve(y_test, nb_y_pred_prob)","c51aa7d6":"plt.figure(figsize=(15,6))\nsns.set(font_scale = 1.5)\nplt.plot(fpr, tpr)\nplt.title('ROC Curve for Naive Bayes Diabetes Classifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)\nplt.xlim(0.0, 1.0)\nplt.ylim(0.0, 1.0)\nplt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')","3c57deb2":"**Sensitivity** (also called the **true positive rate**, the **recall**, or **probability of detection** in some fields) measures the proportion of actual positives that are correctly identified as such (e.g., the percentage of sick people who are correctly identified as having the condition).\n\nIf a person has a disease, how often will the test be positive (true positive rate)? ","6247c84c":"There are 500 non-diabetic Pima Indian women and 268 diabteic Pima Indian women. We can clearly see that, this dataset is unbalanced.\n\nIt is critical to know this for a few reasons:\n\n1. Algorithms like Logistic Regression assumes that the data is balanced. But if the data is unbalanced they will put more weight on the majority class. In this case, the non-diabetic class (500).\n\n2. Accuracy is not a useful metric now.\n\nFor example; We have 100 emails, 99 of which are spam emails and 1 is a good email. If we create an algorithm that always is going to put all emails in your spam folder, then we will have an accuracy of 99% or so. \nBut we will not be performing a great job because a good email will also be in your spam folder.\n\nBecause of this we will see other metrics such as Sensitivity, Specificity and Roc_Auc\n\nWe will use **Roc_Auc score as the metric of success**","8ec10283":"**Specificity** (also called the **true negative rate**) measures the proportion of actual negatives that are correctly identified as such (e.g., the percentage of healthy people who are correctly identified as not having the condition).\n\nIf a person does not have the disease how often will the test be negative (true negative rate)?","86f40981":"**c. DATA CLEANING**","23c21415":"Let us check how data is distributed for variables that have an invalid zero value\n\nSee if there are any outliers\n\nSee if data is normally distributed, left skewed or right skewed\n\nWe will use boxplot and distplot(Histogram)\n","18aafb84":"# 1. Logistic Regression","6a4a0d2a":"------------------------------------------------------------------------","26d0355f":"F1 = 2 * (Precision * Recall) \/ (Precision + Recall)\n\nF1 = 2 * (0.64 * 0.58) \/ (0.64 + 0.58)\n\nF1 = 0.61038961","1e729dde":"So it has a good **ROC AUC** score. There is **72%** chance that model will be able to distinguish between Diabetic class and Non-Diabetic class.\n\nBut, it fails tremendosly in the **sensitivity**. And the sensitivity, the recall is the thing that we want to improve. *Sensitivity: When the actual value is positive, how often is the prediction correct?*\n\nOr in other words. If you have diabetes, will it detect it?","2a8bdb2c":"**Analysis**:\n\n**Low recall, high precision**: \n    Indicates that we miss a lot of positive examples (high FN) but those we predict as positive are indeed positive (low FP).\n    \nIn other words, we miss a lot of Diabetic examples but those we predict as Diabetic are indeed Diabetic","1d2a5704":"**a. MODEL EVALUATION **","ef9be080":"The problem with sensitivity is that it will detect diabetes to people who dont have diabetes. But it is better that, than sending someone home telling them that they do not have diabetes.","67aa9dbb":"F1 = 2 * (Precision * Recall) \/ (Precision + Recall)\n\nF1 = 2 * (0.71 * 0.56) \/ (0.71 + 0.56)\n\nF1 = 0.63","9d3ade8b":"**a. MISSING VALUES**","d79e8e87":"Variables 'age' (Age), 'pedi' (DiabetesPedigreeFunction), 'Preg' (Pregnancies), 'skin' (SkinThickness), 'test' (Insulin) are right skewed that is:\n\nMean > Median\n","7f3c4968":"**Specificity** (also called the **true negative rate**) measures the proportion of actual negatives that are correctly identified as such (e.g., the percentage of healthy people who are correctly identified as not having the condition).\n\nIf a person does not have the disease how often will the test be negative (true negative rate)?","d7b96266":"dataset contains 768 observations and 9 variables","c3a6d49c":"---------------------------------------------------------","264e305b":"A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known.\n\nSo here:\n\nTN = 132\n\nTP = 45\n\nFN = 36\n\nFP = 18\n\nThis is how I interpret the confusion matrix: 0 = No Diabetes;  1 = Diabetes\n\n**TP**: Our model predicted 45 women as diabetic and in actual they were diabetic (Model was correct here)\n\n**TN**: Our model predicted 132 women as non-diabetic and in actual they were non-diabetic (Model was correct here)\n\n**FP**: Our model predicted 18 women as diabetic and in actual they were non-diabetic (Model was wrong here - \"Type 1 error\")\n\n**FN**: Our model predicted 36 women as non-diabetic and in actual they were diabetic (Model was wrong here - \"Type 2 error\")\n","0cd66f23":"A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known.\n\nSo here:\n\n**TN** = 124\n\n**TP** = 47\n\n**FN** = 34\n\n**FP** = 26\n\nThis is how I interpret the confusion matrix: 0 = No Diabetes; 1 = Diabetes\n\nTP: Our model predicted 47 women as diabetic and in actual they were diabetic (Model was correct here)\n\nTN: Our model predicted 124 women as non-diabetic and in actual they were non-diabetic (Model was correct here)\n\nFP: Our model predicted 26 women as diabetic and in actual they were non-diabetic (Model was wrong here - \"Type 1 error\")\n\nFN: Our model predicted 34 women as non-diabetic and in actual they were diabetic (Model was wrong here - \"Type 2 error\")Accuracy: Overall, how often is the classifier correct?","54bda2d6":"Data is cleaned now!","841d613f":"so as age increases the risk of being diabetes also increases","c19b67dc":"We can see from above plot that:\n\nPeople who do not have diabetes: 500\n\nPeople who have diabetes : 268\n","b7927f64":"-------------------------------------------------------------------------","0024bc33":"We can also verify the existence of any null values using following:","e5f2e6f1":"We have to predict whether or not the patients in the dataset have diabetes or not?\n\nSo let us check how many people have diabetes and how many of them do not\n","66a93b63":"# Step 3: Build a Model using Naive Bayes Classifier","562c5213":"From above I can say that, Pima women who have children have more possibility of being Diabetic","d4d6e278":"**Recall**: When the actual value is positive, how often is the prediction correct?\n    \nTP\/actual yes\n\nRecall = TP \/ (TP + FN)\n    \n= 45 \/ (45 + 36)\n        \n= 45 \/ 81\n    \n= 0.55555555555555555555555555555556\n\nWhen it's actually yes, how often does model predict yes?\n\nRecall is also known as \u201csensitivity\u201d and \u201ctrue positive rate\u201d (TPR).","8bed1dab":"For example; a zero value in 'Pres' (BloodPressure) variable does not make sense. Person would be dead I guess by now\n\nAlso there are zero values in 'Plas' (Glucose), 'Skin' (SkinThickness), 'Test' (Insulin), and 'Mass' (BMI) variables ","f5d3ec68":"--------------------------------------------------------","4ae8763b":"There is very less correlation between 'Preg' (Pregnancies) and 'class' (Outcome) that is they are weakly correlated\n\nPima woman with less number of children have low chance of diabetes\n","048d0b06":"**Sensitivity** (also called the **true positive rate**, the **recall**, or **probability of detection** in some fields) measures the proportion of actual positives that are correctly identified as such (e.g., the percentage of sick people who are correctly identified as having the condition).\n\nIf a person has a disease, how often will the test be positive (true positive rate)? ","ef01e6a8":"-------------------------","18b48ee5":"# Step 1: Read dataset","44b8f767":"**Another method to calculate total number of zeros in a column**","30d5f346":"**Analysis**\n\n1. The diagonal shows the distribution of the the dataset with the kernel density plots.\n\n2. The scatter-plots shows the relation between each and every attribute or features taken pairwise. Looking at the scatter-plots, we can say that no two attributes are able to clearly seperate the two outcome-class.","d3711eb0":"Build a predictive model so when a new person walks-in with these values of variables we feed it to the model and model tells us what is the probability of 0 or 1\n\nWe will use **\"Logistic Regression\"** and **\"Naive Bayes classifier\"** algorithm to predict 0 or 1\n","85bb4787":"\n**About the data**\n\nNumeric : Preg             : Number of times pregnant\n  \nNumeric : Plas             : Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n\nNumeric : Pres             : Diastolic blood pressure (mm Hg)\n\nNumeric : Skin             : Triceps skin fold thickness (mm)\n\nNumeric : Test             : 2-Hour serum insulin (mu U\/ml)\n\nNumeric : Mass             : Body mass index (weight in kg\/(height in m)^2)\n\nNumeric : Pedi             : Diabetes pedigree function\n\nNumeric : Age              : Age (years)\n\nNumeric : Class            : Class variable (0 or 1)","7d3b7e27":"**a. MODEL EVALUATION**","4275384b":"this tells me following:\n\nFor example; let us take 'Preg' (Pregnancies) variable\n1. count: Total number of entries or rows = 768 (no missing values)\n2. mean: Sum of all values of Pregnancies divide by 768 = 3.845052\n3. std: Standard Deviation - it measures how far data values are from their mean = 3.369578\n4. min: Minimum number of pregnancy = 0\n5. 25%: First Quartile (Q1) = 1\n6. 50%: Second Quartile (Q2) = 3\n7. 75%: Third Quartile (Q3) = 6\n8. max: Maximum number of pregnancies = 17\n\nAbove 5 number summary can be better explained using boxplot\n","3ca435da":"------------------------------------------------------------","61a2025f":"Replace NaN (earlier we replaced 0s with NaN in pima_copy dataframe) with Median or Mean\n\nVariables 'Plas' [Glucose] and 'Pres' [BloodPressure] do not have much outliers and we need to fill little data so we will use mean here\n\nVariables 'skin' [SkinThickness], 'test' [Insulin], and 'mass' [BMI] have much disparity and we need to fill more data so we will use median here\n","aa1b4253":"**AUC ROC** tells how much model is capable of distinguishing between classes.\n\nAUC is the Area under the Curve. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. \n\nBy analogy, Higher the AUC, better the model is at distinguishing between Pima Indian women with Diabetic and Non-Diabetic.\n\nThe ROC curve is plotted with **True Plot Rate (Sensitivity)** against the **False Plot Rate (1 - Specificity)** where TPR is on y-axis and FPR is on the x-axis","07007dd9":"We can see from above pie plot that:\n\n65.10% out of 768 Pima Indian women do not have diabetes\n\n34.90% out of 768 Pima Indian women have diabetes\n","fe38a987":"**Precision**: Precsion tells us about when model predicts yes, how often is it correct.\n\nTP\/predicted yes\n\nPrecision = TP \/ (TP + FP)\n\n= 45 \/ (45 + 18)\n\n= 45 \/ 63\n\n= 0.7142857142857143\n\nSo when our model predict 1 and actual it is 1 then it's precision is 71%. It should be high as possible.\n","0c310c24":"**e. BIVARIATE ANALYSIS**","9d2d1d30":"-------------------------------------------------","22d2a465":"Looks like variable 'Plas' [Glucose] has one outlier","f65e8a0b":"**b. Visualization for understanding and analysing the distribution of data for different variables**","00d8d54c":"**Precision**: Precsion tells us about when model predicts yes, how often is it correct.\n\nTP\/predicted yes\n\nPrecision = TP \/ (TP + FP)\n\n= 47 \/ (47 + 26)\n\n= 47 \/ 73\n\n= 0.64383561643835616438356164383562\n\nSo when our model predict 1 and actual it is 1 then it's precision is 64%. It should be high as possible.\n","5e496579":"Let us perform visulaization on **clean data (pima_copy)**","ea666fba":"The problem with sensitivity is that it will detect diabetes to people who dont have diabetes. But it is better that, than sending someone home telling them that they do not have diabetes.","82969120":"**f. Get shape of the DataFrame**","dd48f176":"**a. Import packages**","0f29a769":"The above plot tells us No. of Pregnancies vs No. of Pima Indian Women","c9a0bc3b":"# Step 2: Exploratory Data Analysis (EDA)","b29967b0":"**From above heatmap, we can conclude following:**\n\n1. There is no feature variable that has strong correlation with target 'class' (Outcome) as there is no +0.70 which indicates a strong uphill (positive) linear relationship\n\n2. Best predictor of target variable 'class' (Outcome) is 'Plas' (Glucose) --> 0.49 which is near to 0.50 which indicates a moderate uphill (positive) relationship\n\n3. Second best predictor of target variable 'class' (Outcome) is 'mass' (BMI) --> 0.31 which indicates a weak uphill (positive) linear relationship\n\n4. Correlation between 'mass' (BMI) and 'Skin' (SkinThickness) is 0.54 which indicates BMI increases with increase in Skin Thickness\n\n5. Correlation between 'age' (Age) and 'Preg' (Pregnancies) is 0.54 which indicates increase in age increases chances of having a child\n\n6. All the variables look to be uncorrelated. So we cannot eliminate any variable just by looking at the correlation matrix","11a80ed0":"**F-measure**\n\n\nIt is difficult to compare two models with low precision and high recall or vice versa. So to make them comparable, we use F-Score. F-score helps to measure Recall and Precision at the same time. It uses Harmonic Mean in place of Arithmetic Mean by punishing the extreme values more.\n\nIn other words the F1 score conveys the balance between the precision and the recall.","60298a18":"**AUC ROC** tells how much model is capable of distinguishing between classes.\n\nAUC is the Area under the Curve. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. \n\nBy analogy, Higher the AUC, better the model is at distinguishing between Pima Indian women with Diabetic and Non-Diabetic.\n\nThe ROC curve is plotted with **True Plot Rate (Sensitivity)** against the **False Plot Rate (1 - Specificity)** where TPR is on y-axis and FPR is on the x-axis","46911f7e":"**c. Display DataFrame**","002c8cc8":"'skin' [SkinThickness] has 227 zero invalid values that is why lower limit and Q1 (25th) quartile are same","b18ec616":"------------------------------------------------------------","24ef53cd":"**Problem Statement: **","e370b7f3":"-----------------------------------------------","025a9787":"**b. Import Data**","c017897b":"**Recall**: When the actual value is positive, how often is the prediction correct?\n    \nTP\/actual yes\n\nRecall = TP \/ (TP + FN)\n    \n= 47 \/ (47 + 34)\n        \n= 47 \/ 81\n    \n= 0.58024691358024691358024691358025\n\nWhen it's actually yes, how often does model predict yes?\n\nRecall is also known as \u201csensitivity\u201d and \u201ctrue positive rate\u201d (TPR).","6b89ddc5":"Pima woman who have diabetes has higher Glucose levels \n\nwhereas\n\nPima woman who does not have diabetes has lower Glucose levels","b9058e58":"**e. Show last 5 observations**","535b8338":"Replace zero (0) values with NaN values and then sum the NaN values in each column to know get count of NaN values","a9007db2":"**Analysis**","12f97e3b":"Thus we have replaced all NaN values with Mean or Median making data clean","472fba61":"**F-measure**\n\n\nIt is difficult to compare two models with low precision and high recall or vice versa. So to make them comparable, we use F-Score. F-score helps to measure Recall and Precision at the same time. It uses Harmonic Mean in place of Arithmetic Mean by punishing the extreme values more.\n\nIn other words the F1 score conveys the balance between the precision and the recall.","734401a1":"Can you build a machine learning model to accurately predict whether or not the patients in the dataset have diabetes or not?\n\n**Note: Use Naive Bayes Classifier**","be368fc2":"**Accuracy**: Overall, how often is the classifier correct?\n\nAccuracy = (TP + TN)\/ total\n     = (47 + 124) \/ (47 + 124 + 26 + 34)\n\n     =  171 \/ 231\n\n     = 0.74025974025974025974025974025974","7186397c":"------------------------------------------------------------","27aac037":"Above also shows that 768 entries and 9 columns.\n\nThere are no null values in any of the variable.\n\nBut there are values for some columns which does not make sense; they are marked as a zero (0) value","21d99ea7":"**Analysis**","34cdc7eb":"Same like 'SkinThickness', for 'Insulin', we replaced invalid zero minimum value with median so data is not representative. \n\nBut we can say that higher the insulin higher chances of getting diabetes. \n\nAlso insulin has a moderate correlation with Glucose\n\n\n\n\n\n","4f05e88c":"**Data Description:**\n\nThe datasets consists of several medical predictor variables and one target variable 'Class', Outcome. \n\nPredictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.","47d3688f":"**Analysis**:\n\n**Low recall, high precision**: \n    Indicates that we miss a lot of positive examples (high FN) but those we predict as positive are indeed positive (low FP).\n    \nIn other words, we miss a lot of Diabetic examples but those we predict as Daibetic are indeed Diabetic","eb5fedd2":"'test' [Insulin] has 374 zero invalid values that is why lower limit and Q1 (25th) quartile are same\n\nAlso there are many outliers","22dcda6d":"So it has a good **ROC AUC** score. There is **70%** chance that model will be able to distinguish between Diabetic class and Non-Diabetic class.\n\nBut, it fails tremendosly in the **sensitivity**. And the sensitivity, the recall is the thing that we want to improve. *Sensitivity: When the actual value is positive, how often is the prediction correct?*\n\nOr in other words. If you have diabetes, will it detect it?","20275a18":"**d. Show first 5 observations**","a66eb02e":"-----------------------------------------------","e909d532":"Since 'SkinThickness' had more than 227 zeros values and we replaced it with Median, data is not so representative. \n\nBut we can say that more the skin thickness more is the probability of getting diabetes","c51cc392":"I checked on Internet for BMI scale and here it is: \n\nUnderweight: BMI is less than 18.5\n\nNormal weight: BMI is 18.5 to 24.9\n\nOverweight: BMI is 25 to 29.9\n\n\n\nThus I can say that Pima women are Obese both who has diabetes and who does not have diabetes as their average is more than 30\n\nPima woman who has diabetes have more BMI as comapred to who does not have diabetes\n","08113297":"Variable 'Pres' [BloodPressure] has very few outliers","2dc4c971":"Let us calculate the average of children had by Pima Indian woman","3af5f0c1":"There are variables that have a minimum value of zero (0). \n\nOn some variables, a value of zero does not make sense and thus indicates missing value.\n\nFollowing variables have an invalid zero value:\n\n1. 'Pres' (BloodPressure)\n\n2. 'Plas' (Glucose)\n\n3. 'Skin' (SkinThickness)\n\n4. 'Test' (Insulin)\n\n5. 'Mass' (BMI)\n","30a5b190":"There are 500 non-diabetic Pima Indian women and 268 diabteic Pima Indian women. We can clearly see that, this dataset is unbalanced.\n\nIt is critical to know this for a few reasons:\n\n1. Algorithms like Logistic Regression assumes that the data is balanced. But if the data is unbalanced they will put more weight on the majority class. In this case, the non-diabetic class (500).\n\n2. Accuracy is not a useful metric now.\n\nFor example; We have 100 emails, 99 of which are spam emails and 1 is a good email. If we create an algorithm that always is going to put all emails in your spam folder, then we will have an accuracy of 99% or so. \nBut we will not be performing a great job because a good email will also be in your spam folder.\n\nBecause of this we will see other metrics such as Sensitivity, Specificity and Roc_Auc\n\nWe will use **Roc_Auc score as the metric of success**","34b4c817":"**Count of missing values(zeros) in above mentioned 5 variables**","9111b31c":"# 2. Naive Bayes Classifier","4320f2cd":"# Pima Indians Diabetes Dataset - Sanket Mayekar\n\n**Context:**\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset","3a4ab9ef":"**d. VISUALIZATION**","d77d78e1":"**Accuracy**: Overall, how often is the classifier correct?\n    \nAccuracy = (TP + TN)\/ total\n\n         = (45 + 132) \/ (45 + 132 + 18 + 36)\n         \n         =  177 \/ 231\n         \n         = 0.76623376623376623376623376623377"}}