{"cell_type":{"277c776d":"code","2932d6e0":"code","5bfce997":"code","3569a7f0":"code","49067ec9":"code","585a742f":"code","e158e990":"code","c1e3bab9":"code","58dd831f":"code","5971b38b":"code","7ba9eee4":"code","31a70f5a":"code","802bb57e":"code","88d115bd":"code","39a9ce6a":"code","35c41baa":"code","09ee61f9":"code","0b227ea1":"code","ca1a02cf":"code","c2e50f85":"markdown","59d54939":"markdown","5226ac44":"markdown","3f049e52":"markdown","e0e42183":"markdown","aee135d5":"markdown","5c928465":"markdown","42e51860":"markdown","714712ba":"markdown","c6b8500d":"markdown","2d72a85b":"markdown","59ff2af2":"markdown","1dccf91a":"markdown","91092d7d":"markdown","8364dbf5":"markdown"},"source":{"277c776d":"!pip -q install jax jaxlib\n\n%env JAX_ENABLE_X64=1\n%env JAX_PLATFORM_NAME=cpu\n\nimport jax.numpy as np\nfrom jax import grad, ops, jit, lax","2932d6e0":"def func_jax(x):\n    t = len(x)\n    f = np.zeros(t)\n    \n    #for i in range(1, t):\n    #    f = ops.index_update(f, i, x[i]+f[i-1])\n    \n    f = lax.scan(lambda f, i: (ops.index_update(f, i, x[i] + f[i-1]), None), f, np.arange(1, t))\n    \n    return np.sum(f[0])","5bfce997":"func_jax(np.ones(100))","3569a7f0":"%timeit grad(func_jax)(np.ones(10)).block_until_ready()","49067ec9":"%timeit grad(func_jax)(np.ones(100)).block_until_ready()","585a742f":"%timeit grad(func_jax)(np.ones(1000)).block_until_ready()","e158e990":"%timeit grad(func_jax)(np.ones(10000)).block_until_ready()","c1e3bab9":"%timeit jit(grad(func_jax))(np.ones(10)).block_until_ready()","58dd831f":"%timeit jit(grad(func_jax))(np.ones(100)).block_until_ready()","5971b38b":"%timeit jit(grad(func_jax))(np.ones(1000)).block_until_ready()","7ba9eee4":"%timeit jit(grad(func_jax))(np.ones(10000)).block_until_ready()","31a70f5a":"import torch","802bb57e":"def func_torch(x):\n    t = len(x)\n    f = torch.zeros(t)\n    for i in range(1, t):\n        f[i] = x[i] + f[i-1]\n    return f.sum()","88d115bd":"func_torch(torch.ones(100, dtype=torch.float64))","39a9ce6a":"def grad_torch(length):\n    x = torch.ones(length, requires_grad=True, dtype=torch.float64)\n    func_torch(x).backward()\n    return x.grad","35c41baa":"%timeit grad_torch(10)","09ee61f9":"%timeit grad_torch(100)","0b227ea1":"%timeit grad_torch(1000)","ca1a02cf":"%timeit grad_torch(10000)","c2e50f85":"Perform a sanity check.","59d54939":"Define a gradient function.","5226ac44":"Let's do the same with PyTorch.","3f049e52":"Measure performance of a gradient calculation for different array length. Run only one loop to exclude any caching if it exists. Also add a `.block_until_ready()` so we are not just timing dispatch time (due to asynchronous dispatch).","e0e42183":"Install and import JAX, enable usage of 64-bit floats and CPU for computations.","aee135d5":"# Performance of JAX vs PyTorch\n\nLet's compare how fast two libraries can calculate a gradient of the same function: JAX vs PyTorch. No hardware acceleration will be enabled, we will use just CPU (GPU is disabled in this notebook).","5c928465":"## PyTorch","42e51860":"Jitted versions have almost the same timings. ","714712ba":"Now measure performance.","c6b8500d":"A test function is still the same with a slight cosmetic modifications.","2d72a85b":"Looks like the problem is O(1) now!","59ff2af2":"PyTorch is fast, but looks like it solves the problem in O(n) time.","1dccf91a":"## JAX","91092d7d":"A sanity check again. Just a quick test to check that we implemented the same function.","8364dbf5":"Next we will define a toy function which will be used for our tests. It receives an array as input, performs some computations on it and returns a scalar. The problem is O(n) hard - the longer the input array, the linearly longer it takes to calculate the result.\n\nAs gnecula and mattjj kindly [explained](https:\/\/github.com\/google\/jax\/issues\/1832), it's better to use lax.scan in this case instead of a `for` loop, so it is commented out."}}