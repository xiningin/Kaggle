{"cell_type":{"31765142":"code","d98ca49f":"code","55d049ce":"code","7b7e13b1":"code","d1ef4319":"code","10c0d5ad":"code","bf6efa2c":"code","fed11f17":"code","7ec01af6":"code","33282a3f":"code","7d03385f":"code","8312906d":"code","0eb73c3a":"code","cb762bc8":"code","83d285b6":"code","d7569a13":"code","2a26acf6":"code","ca9a09e6":"code","bdbc0289":"code","aa6be5c7":"code","535c9c05":"code","138fb57a":"code","1ace5080":"code","a270a85e":"markdown","51fd6817":"markdown","aafaa8ad":"markdown","625f26a9":"markdown","f9faab94":"markdown","05e9a27c":"markdown","3266f297":"markdown","20eee545":"markdown","aa698f26":"markdown","a7d6d246":"markdown","750397c7":"markdown","465612aa":"markdown","a891b906":"markdown","0600c327":"markdown","08b9a859":"markdown","b147e272":"markdown","df6d5caf":"markdown"},"source":{"31765142":"import numpy as np\nimport itertools\nimport os\nimport shutil\nimport random\nimport glob\nimport matplotlib.pyplot as plt\nimport warnings\nimport tensorflow as tf\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.models import Model\n\nfrom tensorflow.keras.applications import vgg16\nfrom tensorflow.keras.applications import mobilenet\nfrom tensorflow.keras.applications import imagenet_utils\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing import image\n\nfrom tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\n\nfrom sklearn.metrics import confusion_matrix\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n%matplotlib inline","d98ca49f":"os.environ['PYTHONHASHSEED'] = '0'\nos.environ['CUDA_VISIBLE_DEVICES'] = ''\nnp.random.seed(27)\nrandom.seed(23)\ntf.random.set_seed(2723)","55d049ce":"import zipfile\nimport glob\n\nzip_file = glob.glob('..\/input\/dogs-vs-cats\/*.zip')  \nprint(zip_file)\n\ndef extract_zip(file):\n    with zipfile.ZipFile(file,\"r\") as zip_ref:\n        zip_ref.extractall(\"temp\")\n        \nfor files in zip_file:\n    extract_zip(files)","7b7e13b1":"train_path = 'train'\nvalid_path = 'valid'\ntest_path = 'test'\n\nif os.path.isdir('train\/dog') is False:\n    os.makedirs(train_path + '\/dog')\n    os.makedirs(train_path + '\/cat')\n    os.makedirs(valid_path + '\/dog')\n    os.makedirs(valid_path + '\/cat')\n    os.makedirs(test_path + '\/dog')\n    os.makedirs(test_path + '\/cat')\n\n    for i in random.sample(glob.glob('temp\/train\/cat*'), 500):\n        shutil.move(i, train_path + '\/cat')      \n    for i in random.sample(glob.glob('temp\/train\/dog*'), 500):\n        shutil.move(i, train_path + '\/dog')\n    for i in random.sample(glob.glob('temp\/train\/cat*'), 100):\n        shutil.move(i, valid_path + '\/cat')        \n    for i in random.sample(glob.glob('temp\/train\/dog*'), 100):\n        shutil.move(i, valid_path + '\/dog')\n    for i in random.sample(glob.glob('temp\/train\/cat*'), 50):\n        shutil.move(i, test_path + '\/cat')      \n    for i in random.sample(glob.glob('temp\/train\/dog*'), 50):\n        shutil.move(i, test_path + '\/dog')","d1ef4319":"train_batches = (ImageDataGenerator(preprocessing_function=vgg16.preprocess_input)\n                     .flow_from_directory(directory=train_path, \n                                          target_size=(224,224), \n                                          classes=['cat', 'dog'], \n                                          batch_size=10))\n\nvalid_batches = (ImageDataGenerator(preprocessing_function=vgg16.preprocess_input)\n                 .flow_from_directory(directory=valid_path, \n                                      target_size=(224,224), \n                                      classes=['cat', 'dog'], \n                                      batch_size=10))\n\ntest_batches = (ImageDataGenerator(preprocessing_function=vgg16.preprocess_input)\n                .flow_from_directory(directory=test_path, \n                                     target_size=(224,224), \n                                     classes=['cat', 'dog'], \n                                     batch_size=10, \n                                     shuffle=False))","10c0d5ad":"def plotImages(images_arr):\n    fig, axes = plt.subplots(1, 10, figsize=(20,20))\n    axes = axes.flatten()\n    for img, ax in zip(images_arr, axes):\n        ax.imshow(img)\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()","bf6efa2c":"imgs, labels = next(train_batches)\nplotImages(imgs)","fed11f17":"model = Sequential([\n    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding = 'same', input_shape=(224,224,3)),\n    MaxPool2D(pool_size=(2, 2), strides=2),\n    Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'),\n    MaxPool2D(pool_size=(2, 2), strides=2),\n    Flatten(),\n    Dense(units=2, activation='softmax')\n])\nmodel.summary()","7ec01af6":"model.compile(optimizer=Adam(learning_rate=0.0001),\n             loss=\"categorical_crossentropy\",\n             metrics=['accuracy'])\n\nmodel.fit(x=train_batches,\n         validation_data=valid_batches,\n         epochs=10,\n         verbose=2)","33282a3f":"predictions = model.predict(test_batches)\npredictions","7d03385f":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","8312906d":"predictions_labels = np.argmax(predictions, axis=1)\ncm = confusion_matrix(test_batches.classes, predictions_labels)\n\ncm_plot_labels = ['cat','dog']\nplot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')","0eb73c3a":"vgg16_model = vgg16.VGG16()\nvgg16_model.summary()","cb762bc8":"model = Sequential()\nfor layer in vgg16_model.layers[:-1]:\n    model.add(layer)\n\nfor layer in model.layers:\n    layer.trainable = False\n    \nmodel.add(Dense(units=2, activation='softmax'))\nmodel.summary()","83d285b6":"model.compile(optimizer=Adam(learning_rate=0.0001), \n              loss='categorical_crossentropy', \n              metrics=['accuracy'])\n\nmodel.fit(x=train_batches,\n          validation_data=valid_batches,\n          batch_size=10,\n          epochs=3,\n          verbose=2\n)","d7569a13":"predictions = model.predict(x=test_batches, verbose=0)\ncm = confusion_matrix(y_true=test_batches.classes, y_pred=np.argmax(predictions, axis=-1))\n\ncm_plot_labels = ['cat','dog']\nplot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')","2a26acf6":"mobile = mobilenet.MobileNet()\nmobile.summary()","ca9a09e6":"def prepareImage(file):\n    img = image.load_img(file, target_size=(224, 224))\n    img_array = image.img_to_array(img)\n    return mobilenet.preprocess_input(np.expand_dims(img_array, axis=0))\n\ndef getRandomImageFile():\n    return random.sample(glob.glob('temp\/test1\/*'), 1)[0]","bdbc0289":"img = getRandomImageFile()\nimg_prepared = prepareImage(img)\nplt.imshow(img_prepared[0])","aa6be5c7":"predictions = mobile.predict(img_prepared)\npredictions_label = imagenet_utils.decode_predictions(predictions)\npredictions_label","535c9c05":"gen = ImageDataGenerator(rotation_range=10, \n                         width_shift_range=0.1, \n                         height_shift_range=0.1, \n                         shear_range=0.15, \n                         zoom_range=0.1, \n                         channel_shift_range=10.,\n                         horizontal_flip=True)","138fb57a":"image_path = getRandomImageFile()\nimage_random = np.expand_dims(plt.imread(image_path),0)\nplt.imshow(image_random[0])","1ace5080":"aug_iter = gen.flow(image_random)\naug_images = [next(aug_iter)[0].astype(np.uint8) for i in range(10)]\nplotImages(aug_images)","a270a85e":"### Manage Directory\nSplit data into training, validating, and testing data. I only took 1000 images for training, 200 images for validation and 100 images for testing. This is to minimize computation because I just want to learn how to make CNN in Keras","51fd6817":"## Image Augmentation\nThis technique often used to increase the size of training data. This technique will manipulate the original image with zooming, rotating, fliping, etc. Image augmentation is also one way to overcome overfitting","aafaa8ad":"### MobileNet Model\nThe difference between this model and the VGG16 model is the size of the mode. VGG16 has a size 557MB but MobileNet only has 17MB. Very difference! Like the name, this model is very suitable for use in mobile applications. This model only has 4 million parameters but the accuracy only slightly difference from VGG16","625f26a9":"# Learn Convolutional Neural Network with Keras\n\n","f9faab94":"VGG16 has more then 138 millions parameters with output layer having 1000 nodes, this is because VGG16 is trained by ImageNet dataset with 1000 labels. I will adjust output layer so that it only has 2 nodes and I will to freeze every layers so the weights don't change while the model is trained","05e9a27c":"## Transfer Learning\n\nTransfer learning is a technique for using pre-trained model and fine-tune those parameter to perform a new task. So, we just load the model with its weights and add a new layers so that model can predict new data that don't seen before. There are so many pre-trained models, I only wanted to use two of them, VGG16 and MobileNet.\n\n### VGG16 Model\n","3266f297":"### Plot Images Function","20eee545":"### Load Data\nLoad data from directory and preprocessing with VGG16. The VGG16 model perfoms preprocessing only by substracting all color channels by their average","aa698f26":"### Confusion Matrix","a7d6d246":"Code in this notebook obtain from [Keras - Python Deep Learning Neural Network API Course](https:\/\/deeplizard.com\/learn\/playlist\/PLZbbT5o_s2xrwRnXk_yCPtnqqo4_u2YGL) or you can watch that course in [Youtube ](https:\/\/www.youtube.com\/watch?v=qFJeN9V1ZsI&t=2380s)","750397c7":"It can be seen that the model has perfect accuracy for training data but only has an accuray 0.61 for validation data so this model is overfitting.\n\n### Try To Predict Test Data","465612aa":"All images look saturated, this is the effect of VGG16 preprocessing\n\n## Make CNN Model From Scratch\nThis CNN scratch model has 2 convolutional layers and 2 max polling layers with `relu` activation fucntion. This model have 2 output nodes, this is because our data have two classes, namely dog and cat","a891b906":"### Extract Zip File","0600c327":"The `predict` method returns the probability of an image for each class","08b9a859":"### Set Seed to Reproducible Result","b147e272":"I'm not refining the model and just using the specified weights to predict the new image. You can fine-tuned this model like VGG16, as I have done before","df6d5caf":"It can be seen that with only with 3 epochs, the model has an accuracy of 0.98 for training data and validation data. This model not overfitting like our previous model."}}