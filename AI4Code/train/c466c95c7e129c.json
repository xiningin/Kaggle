{"cell_type":{"16b23fb4":"code","9ea54621":"code","a3e5175b":"code","866e116a":"code","22519732":"code","ff768208":"code","db7170da":"code","0d57e98d":"code","79785201":"code","fc19a594":"code","8aabfdb1":"code","d7171318":"code","15a799e3":"code","008eeea3":"code","2cef8994":"code","3b3bd686":"code","842d9bcd":"code","5f9d39c9":"code","5be9f05e":"code","a2062791":"code","5cd190a9":"code","de99b2cd":"code","90aad518":"code","30d8e574":"markdown","0c94f3be":"markdown"},"source":{"16b23fb4":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\n%matplotlib inline","9ea54621":"# Load MNIST dataset from tensorflow module\n\nimport tensorflow as tf\nmnist = tf.keras.datasets.mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train \/ 255.0, x_test \/ 255.0","a3e5175b":"# plot images of the dataset\n\nplt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(x_train[i], cmap=plt.cm.binary)\n    plt.xlabel(y_train[i])\nplt.show()","866e116a":"# reshape\/flatten the dataset \nx_train = x_train.reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2])\nx_test = x_test.reshape(x_test.shape[0] ,x_test.shape[1]*x_test.shape[2])","22519732":"x_train.shape","ff768208":"y_train.shape","db7170da":"# Convert the label from integer to one hot matrix\ny_train = np.eye(10)[y_train]\ny_train.shape","0d57e98d":"y_test = np.eye(10)[y_test]\ny_test.shape","79785201":"# Implementing sigmoid activation function\ndef sigmoid(x):\n    s=1\/(1+np.exp(-x))\n    return s","fc19a594":"# Implementing derivation of sigmoid activation function\ndef sigmoid_der(x):\n    s=1\/(1+np.exp(-x))\n    ds=s*(1-s)  \n    return ds","8aabfdb1":"# calculate the softmax of a vector\ndef softmax(v):\n    max = np.max(v,axis=1,keepdims=True)\n    e = np.exp(v-max)\n    f = e \/ np.sum(e,axis=1,keepdims=True)\n    return f","d7171318":"# Implementing derivation of softmax activation function\n\ndef softmax_der(vector):\n    v=softmax(vector)\n    return v*(1-v)\n    ","15a799e3":"# Predicting the result of the model on test dataset\ndef predict(x_test,w1,w2,b1,b2):\n    z1=np.dot(x_test,w1)+b1\n    a1=sigmoid(z1)\n    z2=np.dot(a1,w2)+b2\n    a2=softmax(z2)\n    \n    return a2","008eeea3":"# Implement Gradient descent with momentum\ndef optimizer(w,change,dw):\n    new_change = lr * dw + momentum * change\n    w = w - new_change\n    return w,new_change","2cef8994":"# Implemented Categorical cross entropy loss \ndef cross_entropy(y_true,y_pred):\n    loss=0.0\n    l= len(y_true)\n    \n    loss = (np.sum(-y_true * np.log(y_pred))\/l)\n    \n    return loss","3b3bd686":"# Calculated Accuracy of the result\ndef accuracy(y_true,y_pred):\n    acc=0.0\n    l= len(y_true)\n    y_pred = y_pred.round()\n    y_true = y_true.round()\n    \n    acc =(np.sum(y_true * y_pred)\/l)\n    return acc","842d9bcd":"def train(x_train,y_train,epochs,nlayer,loss):\n    \n    # Initializing the weight of the model( 2 layer neural network)\n    l=x_train.shape\n    w1=np.random.rand(l[1],nlayer)-0.5\n    w2=np.random.rand(nlayer,10)-0.5\n    b1=np.random.rand(1,nlayer)\n    b2=np.random.rand(1,10)\n    \n    # save the changes required for momentum in gradient descent\n    w1_change = 0.0\n    w2_change =0.0\n    b1_change = 0.0\n    b2_change=0.0\n    \n    \n    for i in range(epochs):        \n            \n        #Forward propagation of the model    \n        z1=np.dot(x_train,w1)+b1\n        a1=sigmoid(z1)\n        z2=np.dot(a1,w2)+b2\n        a2=softmax(z2)\n        \n        #Backward propagation of the model \n        dz2 = a2-y_train\n        dw2 = np.dot(a1.T,dz2)\/l[0]\n        db2 = np.sum(dz2,axis=0,keepdims=True)\/l[0]\n        dz1 = np.dot(dz2,w2.T)*sigmoid_der(z1)\n        dw1 = np.dot(x_train.T,dz1)\/l[0]\n        db1 = np.sum(dz1,axis=0,keepdims=True)\/l[0]\n        \n        #Gradient Descent with momentum Optimizer to update weights of the model\n        w1,w1_change = optimizer(w1,w1_change,dw1)\n        w2,w2_change = optimizer(w2,w2_change,dw2)\n        b1,b1_change = optimizer(b1,b1_change,db1)\n        b2,b2_change = optimizer(b2,b2_change,db2)\n        \n        \n        # Calculate loss on the train and test dataset\n        train_loss = cross_entropy(y_train,a2)\n        y_pred = predict(x_test,w1,w2,b1,b2)\n        test_loss = cross_entropy(y_test,y_pred)\n        \n        # Calculate accuracy on the train and test dataset\n        train_acc=accuracy(y_train,a2)\n        test_acc =accuracy(y_test,y_pred)\n        \n        loss[0].append(train_loss)\n        loss[1].append(test_loss)\n        print(\"epoch {} :  Training loss = {:.7} Testing loss = {:.7} Training accuracy = {:.5} Testing accucracy = {:.5}\"\n              .format(i+1,train_loss,test_loss,train_acc,test_acc))\n    \n    return w1,w2,b1,b2","5f9d39c9":"loss=[[],[]]\nmomentum =0.9\nlr=1\nw1,w2,b1,b2 = train(x_train,y_train,50,128,loss)","5be9f05e":"#plot the training and validation loss on the train dataset\nfrom matplotlib import pyplot\npyplot.title('Learning Curves')\npyplot.xlabel('Epoch')\npyplot.ylabel('Cross Entropy')\npyplot.plot(loss[0], label='train')\npyplot.plot(loss[1], label='val')\npyplot.legend()\npyplot.show()","a2062791":"y_pred = predict(x_test,w1,w2,b1,b2)\nmatrix = classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1))\nprint(matrix)","5cd190a9":"weight =[w1,w2,b1,b2]\nimport pickle\n\nwith open('weight_file', 'wb') as fp:\n    pickle.dump(weight, fp)","de99b2cd":"with open ('weight_file', 'rb') as fp:\n    weight = pickle.load(fp)","90aad518":"# Check on the random dataset\ntest = x_test[5]\nlabel = y_test[5]\npred = predict(test,weight[0],weight[1],weight[2],weight[3])\nprint(\"True result is {}    Predicted Result is {}\".format(np.argmax(label),np.argmax(pred)))","30d8e574":"#  Two-layer neural network from scratch In python\n\n","0c94f3be":"### Save the weights of the model"}}