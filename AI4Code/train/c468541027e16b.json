{"cell_type":{"0d59975d":"code","2a3f47b1":"code","e25946f8":"code","5c4012f9":"code","21c0e6bc":"code","493200c1":"code","0e7e8df6":"code","672fe8b2":"code","dd1f31a7":"code","6521ed05":"code","3b282edf":"code","6a9280a7":"code","c314ca9f":"code","c051286c":"code","b711c4bd":"code","b1c56a74":"code","7900f8c3":"code","70495ded":"code","b5a4f4ff":"code","edf8be00":"code","85a98fc3":"code","b3ded5da":"code","59587c1a":"code","83d06d27":"code","04659bdb":"code","148dc9ea":"code","6d2edae5":"code","2afe3e42":"code","6c36de68":"code","7ebf220f":"code","ac4738ed":"code","130869b3":"code","a6ffbd9f":"code","2e20d4d5":"code","bb41185b":"code","69d0fadd":"code","9038f4a6":"markdown","8c9d1f80":"markdown","cc0d1822":"markdown","09247aa0":"markdown","d215b010":"markdown","a41e5736":"markdown","6279d22e":"markdown","5f3383b9":"markdown","f89b8606":"markdown","2d12c90c":"markdown","07bbaa89":"markdown","da78ac25":"markdown","9cae37f5":"markdown","3cf8a9fc":"markdown","c334c7eb":"markdown","d18c7de1":"markdown","157e2b25":"markdown","2378ffc2":"markdown","09c19f1a":"markdown","78966ea6":"markdown","618a1f0a":"markdown","528e6fe7":"markdown","d8117227":"markdown","daf7f4d6":"markdown","b858e8f2":"markdown"},"source":{"0d59975d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2a3f47b1":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","e25946f8":"df = pd.read_csv('\/kaggle\/input\/loanprediction\/train_ctrUa4K.csv')\ndf.head()","5c4012f9":"df.describe()","21c0e6bc":"df['Property_Area'].value_counts()","493200c1":"df['ApplicantIncome'].hist(bins=50)","0e7e8df6":"df.boxplot(column='ApplicantIncome')","672fe8b2":"df.boxplot(column='ApplicantIncome', by = 'Education')","dd1f31a7":"df['LoanAmount'].hist(bins=50)","6521ed05":"df.boxplot(column='LoanAmount')","3b282edf":"temp1 = df['Credit_History'].value_counts(ascending=True)\ntemp2 = df.pivot_table(values='Loan_Status',index=['Credit_History'],aggfunc=lambda x: x.map({'Y':1,'N':0}).mean())\nprint ('Frequency Table for Credit History:') \nprint (temp1)\n\nprint ('\\nProbility of getting loan for each Credit History class:')\nprint (temp2)","6a9280a7":"import matplotlib.pyplot as plt\nplt.xlabel('Credit_History')\nplt.ylabel('Count of Applicants')\nplt.title(\"Applicants by Credit_History\")\ntemp1.plot(kind='bar')","c314ca9f":"temp2.plot(kind = 'bar')\nplt.xlabel('Credit_History')\nplt.ylabel('Probability of getting loan')\nplt.title(\"Probability of getting loan by credit history\")","c051286c":"temp1 = df['Married'].value_counts(ascending=True)\ntemp2 = df.pivot_table(values='Loan_Status',index=['Married'],aggfunc=lambda x: x.map({'Y':1,'N':0}).mean())\nprint ('Frequency Table for Credit History:') \nprint (temp1)\n\nprint ('\\nProbility of getting loan on the basis of martial status:')\nprint (temp2)","b711c4bd":"temp1 = df['Self_Employed'].value_counts(ascending=True)\ntemp2 = df.pivot_table(values='Loan_Status',index=['Self_Employed'],aggfunc=lambda x: x.map({'Y':1,'N':0}).mean())\nprint ('Frequency Table for Credit History:') \nprint (temp1)\n\nprint ('\\nProbility of getting loan on the basis of employment:')\nprint (temp2)","b1c56a74":"temp1 = df['Property_Area'].value_counts(ascending=True)\ntemp2 = df.pivot_table(values='Loan_Status',index=['Property_Area'],aggfunc=lambda x: x.map({'Y':1,'N':0}).mean())\nprint ('Frequency Table for Credit History:') \nprint (temp1)\n\nprint ('\\nProbility of getting loan on the basis of employment:')\nprint (temp2)","7900f8c3":"temp3 = pd.crosstab(df['Credit_History'], df['Loan_Status'])\ntemp3.plot(kind='bar', stacked=True, color=['red','blue'], grid=False)","70495ded":"temp3 = pd.crosstab(df['Married'], df['Loan_Status'])\ntemp3.plot(kind='bar', stacked=True, color=['red','blue'], grid=False)","b5a4f4ff":"temp3 = pd.crosstab(df['Self_Employed'], df['Loan_Status'])\ntemp3.plot(kind='bar', stacked=True, color=['red','blue'], grid=False)","edf8be00":"temp3 = pd.crosstab(df['Property_Area'], df['Loan_Status'])\ntemp3.plot(kind='bar', stacked=True, color=['red','blue'], grid=False)","85a98fc3":"df.apply(lambda x : sum(x.isnull()), axis=0)","b3ded5da":"df['LoanAmount'].fillna(df['LoanAmount'].mean(), inplace=True)","59587c1a":"df['Self_Employed'].value_counts()","83d06d27":"df['Self_Employed'].fillna('No', inplace=True)","04659bdb":"df['LoanAmount_log'] = np.log(df['LoanAmount'])\ndf['LoanAmount_log'].hist(bins=20)","148dc9ea":"df['TotalIncome']=df['ApplicantIncome'] + df['CoapplicantIncome']\ndf['TotalIncome_log'] = np.log(df['TotalIncome'])\ndf['TotalIncome_log'].hist(bins=20)","6d2edae5":"df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)\ndf['Married'].fillna(df['Married'].mode()[0], inplace=True)\ndf['Dependents'].fillna(df['Dependents'].mode()[0], inplace=True)\ndf['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mode()[0], inplace=True)\ndf['Credit_History'].fillna(df['Credit_History'].mode()[0], inplace=True)","2afe3e42":"var_mod = ['Gender','Married','Dependents','Education','Self_Employed','Property_Area','Loan_Status']\nle = LabelEncoder()\nfor i in var_mod:\n    df[i] = le.fit_transform(df[i])\ndf.dtypes","6c36de68":"#Generic function for making a classification model and accessing performance:\ndef classification_model(model, data, predictors, outcome):\n  #Fit the model:\n  model.fit(data[predictors],data[outcome])\n  \n  #Make predictions on training set:\n  predictions = model.predict(data[predictors])\n  \n  #Print accuracy\n  accuracy = metrics.accuracy_score(predictions,data[outcome])\n  print (\"Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n\n  #Perform k-fold cross-validation with 5 folds\n#   kf = KFold(data.shape[0], n_folds=5)\n  kf = KFold( n_splits = 5)\n  kf.get_n_splits(data.values)\n\n  error = []\n  for train, test in kf.split(data.values):\n    # Filter training data\n    train_predictors = (data[predictors].iloc[train,:])\n    \n    # The target we're using to train the algorithm.\n    train_target = data[outcome].iloc[train]\n    \n    # Training the algorithm using the predictors and target.\n    model.fit(train_predictors, train_target)\n    \n    #Record error from each cross-validation run\n    error.append(model.score(data[predictors].iloc[test,:], data[outcome].iloc[test]))\n \n  print (\"Cross-Validation Score : %s\" % \"{0:.3%}\".format(np.mean(error)))\n\n  #Fit the model again so that it can be refered outside the function:\n  model.fit(data[predictors],data[outcome]) ","7ebf220f":"outcome_var = ['Loan_Status']\nmodel = LogisticRegression()\npredictor_var = ['Credit_History']\n# print( type(df.values), df)\nclassification_model(model, df,predictor_var,outcome_var)","ac4738ed":"#We can try different combination of variables:\npredictor_var = ['Credit_History','Education','Married','Self_Employed','Property_Area']\nclassification_model(model, df,predictor_var,outcome_var)","130869b3":"model = DecisionTreeClassifier()\npredictor_var = ['Credit_History','Gender','Married','Education']\nclassification_model(model, df,predictor_var,outcome_var)","a6ffbd9f":"#We can try different combination of variables:\npredictor_var = ['Credit_History','Loan_Amount_Term','LoanAmount_log']\nclassification_model(model, df,predictor_var,outcome_var)","2e20d4d5":"model = RandomForestClassifier(n_estimators=100)\npredictor_var = ['Gender', 'Married', 'Dependents', 'Education',\n       'Self_Employed', 'Loan_Amount_Term', 'Credit_History', 'Property_Area',\n        'LoanAmount_log','TotalIncome_log']\nclassification_model(model, df,predictor_var,outcome_var)","bb41185b":"#Create a series with feature importances:\nfeatimp = pd.Series(model.feature_importances_, index=predictor_var).sort_values(ascending=False)\nprint (featimp)","69d0fadd":"model = RandomForestClassifier(n_estimators=25, min_samples_split=25, max_depth=7, max_features=1)\npredictor_var = ['TotalIncome_log','LoanAmount_log','Credit_History','Dependents','Property_Area']\nclassification_model(model, df,predictor_var,outcome_var)","9038f4a6":"We can see that there are missing values in LoanAmont, Loan_Amount_term and Credit_History\n\nNow lets see for categorical value","8c9d1f80":"For Married and UnMarried People","cc0d1822":"Filling The Remaining Missing Values","09247aa0":"# Decision Tree\nDecision tree is another method for making a predictive model. It is known to provide higher accuracy than logistic regression model. Read more about Decision Trees.","d215b010":"Now lets check the missing values in the dataset","a41e5736":"Before moving to fill the other missing values lets treat the extreme values in LoanAmount and Applicant Income\n\nInstead of treating the extreme values as outliers lets try a log transformation to nullify their effect","6279d22e":"On the Basis of Property Area","5f3383b9":"# Logistic Regression","f89b8606":"Let\u2019s use the top 5 variables for creating a model. Also, we will modify the parameters of random forest model a little bit:","2d12c90c":"We could also combine this plot in to one in a stacked chart.","07bbaa89":"Here we see that the accuracy is 100% for the training set. This is the ultimate case of overfitting and can be resolved in two ways:\n\nReducing the number of predictors\nTuning the model parameters\n\nLet\u2019s try both of these. First we see the feature importance matrix from which we\u2019ll take the most important features.","da78ac25":"It can be clearly seen that there are so many outliers for applicant income. May be it is related to education, so let us segregate applicant income with respect to education.","9cae37f5":"Let\u2019s try a few numerical variables:","3cf8a9fc":"So coming to Applicant Income, we shall combine it with Co-aaplicants as the people with lower income have strong co-applicants.","c334c7eb":"Here we observed that although the accuracy went up on adding variables, the cross-validation error went down. This is the result of model over-fitting the data. Let\u2019s try an even more sophisticated algorithm and see if it helps:","d18c7de1":"Distribution Analysis","157e2b25":"Thank You","2378ffc2":"Here we can see some extreme values.","09c19f1a":"As you can see 532 people are not self employed which is quite large when compared to employed people, so we fill the missing values with 'No'","78966ea6":"On the basis of Employment","618a1f0a":"# Random Forest\nAn advantage with Random Forest is that we can make it work with all the features and it returns a feature importance matrix which can be used to select features.","528e6fe7":"We can use all the features but it may lead to overfitting.\n\nSo, we shall use only those features which will give a high chance of getting loan.","d8117227":"There is no difference between mean incomes of graduates and non gradutes. There are higher number of graduates with very high incomes, appearing to be the outliers.","daf7f4d6":"Notice that although accuracy reduced, but the cross-validation score is improving showing that the model is generalizing well. Remember that random forest models are not exactly repeatable. Different runs will result in slight variations because of randomization. But the output should stay in the ballpark.","b858e8f2":"Here, too we can see many outliers. Both Applicant Income and Loan Amount require data mugging. Loan Amount has missing values too."}}