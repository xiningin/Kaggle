{"cell_type":{"dfb6b1e6":"code","96abf966":"code","8fa12b4a":"code","65311d3a":"code","2fdfe9e5":"code","5c13f3e5":"code","dd994faf":"code","98f5779d":"code","c59063e8":"code","dec11ba5":"code","ae7ca6e5":"code","2ec19e44":"code","513dbcf8":"code","d0306513":"code","ace1f3e1":"code","62ac5ba2":"code","de4f94ba":"code","8bf05946":"code","ea4774f2":"code","752b8d60":"code","22b7b8f0":"code","c6af65b7":"code","82ef8a5d":"code","dcbb7a4f":"code","80985077":"code","2723ce38":"code","181f8b9a":"code","e0f7033a":"code","c2e77f96":"code","65e6ea74":"code","9072cce0":"code","8b2c56a0":"code","c2c47c86":"code","2a96e03d":"code","e0318510":"code","59d67834":"code","cf3ed3c8":"code","aeaa71d3":"code","0a8f5940":"code","acfe0c32":"code","5a9f7fc7":"code","146d7151":"code","5c25e011":"code","912403b0":"code","c8d57e2e":"code","3837562f":"code","2420dace":"code","3a8bc980":"code","91cc14b1":"code","36d92791":"code","3ed2f092":"code","2f3275cc":"code","13226e89":"code","273f65c2":"code","70f1a1e8":"code","805642e4":"code","c6c0f4a0":"code","1ff5fd46":"code","a770a682":"code","c6987c09":"code","d86a0be0":"code","42f029ab":"code","d69a8d92":"code","7f9658cf":"code","d627643b":"code","3e02d91a":"code","cfd7fb02":"code","d27d2c3b":"code","3bca60e2":"code","f0992b1b":"code","a6e50050":"code","8d01e74a":"markdown","5c5f7b6a":"markdown","44e0f2c6":"markdown","25e51672":"markdown","618a2b0e":"markdown","8a126482":"markdown","a6ca6da6":"markdown","65b0ec32":"markdown","439822b7":"markdown","ed12f944":"markdown","1906ce9a":"markdown","386ca0ab":"markdown","de83a9d8":"markdown"},"source":{"dfb6b1e6":"import numpy as np\nimport pandas as pd\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nmit_test_data = pd.read_csv(\"..\/input\/mitbih_test.csv\", header=None)\nmit_train_data = pd.read_csv(\"..\/input\/mitbih_train.csv\", header=None)\n\nprint(\"MIT test dataset\")\nprint(mit_test_data.info())\nprint(\"MIT train dataset\")\nprint(mit_train_data.info())","96abf966":"mit_train_data[187].value_counts()","8fa12b4a":"mit_train_data.describe()","65311d3a":"#mit_train_data.iloc[19999].plot()","2fdfe9e5":"# take a random distribution\nsample = mit_test_data.sample(25)\n\n# remove the target column\nsampleX = sample.iloc[:,sample.columns != 187]\n\nimport matplotlib.pyplot as plt\n\nplt.style.use('classic')\n\n# plt samples\nfor index, row in sampleX.iterrows():\n    plt.plot(np.array(range(0, 187)) ,row)\n\nplt.xlabel(\"time\")\nplt.ylabel(\"magnitude\")\nplt.title(\"heartbeat reccording \\nrandom sample\")\n\nplt.show()\n\nplt.style.use(\"ggplot\")\n\nplt.title(\"Number of record in each category\")\n\nplt.hist(sample.iloc[:,sample.columns == 187].transpose())\nplt.show()","5c13f3e5":"# We will use the Seaborn library\nimport seaborn as sns\nsns.set()\n\n# Graphics in SVG format are more sharp and legible\n%config InlineBackend.figure_format = 'svg' ","dd994faf":"mit_train_data.columns","98f5779d":"columns = [0,1,2,3,4,5,6,187]","c59063e8":"# `pairplot()` may become very slow with the SVG format\n#%config InlineBackend.figure_format = 'png'\n#sns.pairplot(mit_train_data[columns]);","dec11ba5":"print(\"Train data\")\nprint(\"Type\\tCount\")\nprint((mit_train_data[187]).value_counts())\nprint(\"-------------------------\")\nprint(\"Test data\")\nprint(\"Type\\tCount\")\nprint((mit_test_data[187]).value_counts())","ae7ca6e5":"sns.countplot(x=187, data=mit_train_data);","2ec19e44":"sns.countplot(x=187, data=mit_test_data);","513dbcf8":"mit_train_data = mit_train_data.sample(frac=1)","d0306513":"from keras.utils import to_categorical\n\nprint(\"--- X ---\")\nX = mit_train_data.loc[:, mit_train_data.columns != 187]\nprint(X.head())\nprint(X.info())\n\nprint(\"--- Y ---\")\ny = mit_train_data.loc[:, mit_train_data.columns == 187]\ny = to_categorical(y)\n\nprint(\"--- testX ---\")\ntestX = mit_test_data.loc[:, mit_test_data.columns != 187]\nprint(testX.head())\nprint(testX.info())\n\nprint(\"--- testy ---\")\ntesty = mit_test_data.loc[:, mit_test_data.columns == 187]\ntesty = to_categorical(testy)","ace1f3e1":"from keras import backend as K\n    \ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n        Only computes a batch-wise average of recall.\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n \n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n        Only computes a batch-wise average of precision.\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","62ac5ba2":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation,BatchNormalization,Dropout\n\nmodel = Sequential()\n\nmodel.add(Dense(50, input_dim=187, init='normal', activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(30, init='normal', activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(5, activation='softmax'))\n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['acc',f1])\n\nhistory = model.fit(X, y, validation_split=0.2,epochs=100,shuffle=True,class_weight='auto')","de4f94ba":"print(\"Evaluation: \")\nmse, acc, F1 = model.evaluate(testX, testy)\nprint('mean_squared_error :', mse)\nprint('accuracy:', acc)\nprint('F1:', F1)","8bf05946":"history.history.keys()","ea4774f2":"plt.figure()\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.plot(history.history['f1'])\nplt.plot(history.history['val_f1'])\nplt.legend(labels=['loss','val_loss','f1','val_f1'],loc='best')\nplt.show()","752b8d60":"X.shape","22b7b8f0":"y.shape","c6af65b7":"X = np.expand_dims(X,2)\ntestX = np.expand_dims(testX,2)","82ef8a5d":"X.shape","dcbb7a4f":"from keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, Input, Flatten, SeparableConv1D\nfrom keras.layers import GlobalMaxPooling1D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.merge import Concatenate\nfrom keras.models import Model\n\nfrom keras import backend as K\nfrom keras.optimizers import Adam\nfrom keras.callbacks import LearningRateScheduler, ModelCheckpoint","80985077":"n_obs, feature, depth = X.shape\nbatch_size = 1024","2723ce38":"def build_model():\n    input_img = Input(shape=(feature, depth), name='ImageInput')\n    x = Conv1D(64, 3, activation='relu', padding='same', name='Conv1_1')(input_img)\n    x = Conv1D(64, 3, activation='relu', padding='same', name='Conv1_2')(x)\n    x = MaxPooling1D(2, name='pool1')(x)\n    \n    x = SeparableConv1D(64, 3, activation='relu', padding='same', name='Conv2_1')(x)\n    x = SeparableConv1D(64, 3, activation='relu', padding='same', name='Conv2_2')(x)\n    x = MaxPooling1D(2, name='pool2')(x)\n    \n    x = SeparableConv1D(128, 3, activation='relu', padding='same', name='Conv3_1')(x)\n    x = BatchNormalization(name='bn1')(x)\n    x = SeparableConv1D(128, 3, activation='relu', padding='same', name='Conv3_2')(x)\n    x = BatchNormalization(name='bn2')(x)\n    \n    x = SeparableConv1D(256, 3, activation='relu', padding='same', name='Conv3_3')(x)\n    x = MaxPooling1D(2, name='pool3')(x)\n    x = Dropout(0.6, name='dropout0')(x)\n    \n    x = Flatten(name='flatten')(x)\n    x = Dense(256, activation='relu', name='fc1')(x)\n    x = Dropout(0.6, name='dropout1')(x)\n    x = Dense(128, activation='relu', name='fc2')(x)\n    x = Dropout(0.5, name='dropout2')(x)\n    x = Dense(5, activation='softmax', name='fc3')(x)\n    \n    model = Model(inputs=input_img, outputs=x)\n    return model","181f8b9a":"model =  build_model()\n#model.summary()","e0f7033a":"model.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['acc',f1])","c2e77f96":"from keras.callbacks import ModelCheckpoint\ncheckpointer = ModelCheckpoint(filepath=\"\/tmp\/weights.hdf5\", verbose=1, save_best_only=True)\n","65e6ea74":"history = model.fit(X, y, validation_split=0.2,epochs=75,batch_size=batch_size,shuffle=True,class_weight='auto',callbacks=[checkpointer])","9072cce0":"print(\"Evaluation: \")\nmse, acc, F1 = model.evaluate(testX, testy)\nprint('mean_squared_error :', mse)\nprint('accuracy:', acc)\nprint('F1:', F1)","8b2c56a0":"model.save('cnn-0.985.h5')","c2c47c86":"y_pred = model.predict(testX, batch_size=1000)","2a96e03d":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, label_ranking_average_precision_score, label_ranking_loss, coverage_error \n\nprint(classification_report(testy.argmax(axis=1), y_pred.argmax(axis=1)))","e0318510":"import itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(testy.argmax(axis=1), y_pred.argmax(axis=1))\nnp.set_printoptions(precision=1)\n\n# Plot non-normalized confusion matrix\nplt.figure(figsize=(7, 7))\nplot_confusion_matrix(cnf_matrix, classes=['N', 'S', 'V', 'F', 'Q'],\n                      title='Confusion matrix')\nplt.show()","59d67834":"# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","cf3ed3c8":"#mit_train_data = mit_train_data.sample(frac=1)","aeaa71d3":"from keras.utils import to_categorical\n\nprint(\"--- X ---\")\nX = mit_train_data.loc[:, mit_train_data.columns != 187]\nprint(X.head())\nprint(X.info())\n\nprint(\"--- Y ---\")\ny = mit_train_data.loc[:, mit_train_data.columns == 187]\n#y = to_categorical(y)\n\nprint(\"--- testX ---\")\ntestX = mit_test_data.loc[:, mit_test_data.columns != 187]\nprint(testX.head())\nprint(testX.info())\n\nprint(\"--- testy ---\")\ntesty = mit_test_data.loc[:, mit_test_data.columns == 187]\ntesty = to_categorical(testy)","0a8f5940":"X.shape,y.shape","acfe0c32":"y = y.values.squeeze()","5a9f7fc7":"X = np.array(X)","146d7151":"C0 = np.argwhere(y == 0).flatten()\nC1 = np.argwhere(y == 1).flatten()\nC2 = np.argwhere(y == 2).flatten()\nC3 = np.argwhere(y == 3).flatten()\nC4 = np.argwhere(y == 4).flatten()","5c25e011":"print(C0.shape[0],C1.shape[0],C2.shape[0],C3.shape[0],C4.shape[0])","912403b0":"import random\nfrom scipy.signal import resample\n\ndef stretch(x):\n    l = int(187 * (1 + (random.random()-0.5)\/3))\n    y = resample(x, l)\n    if l < 187:\n        y_ = np.zeros(shape=(187, ))\n        y_[:l] = y\n    else:\n        y_ = y[:187]\n    return y_\n\ndef amplify(x):\n    alpha = (random.random()-0.5)\n    factor = -alpha*x + (1+alpha)\n    return x*factor\n\ndef augment(x):\n    result = np.zeros(shape= (5, 187))\n    for i in range(3):\n        if random.random() < 0.33:\n            new_y = stretch(x)\n        elif random.random() < 0.66:\n            new_y = amplify(x)\n        else:\n            new_y = stretch(x)\n            new_y = amplify(new_y)\n        result[i, :] = new_y\n    return result","c8d57e2e":"import matplotlib.pyplot as plt\nimport random\nplt.plot(X[0, :])\nplt.plot(amplify(X[0, :]))\nplt.plot(stretch(X[0, :]))\nplt.show()","3837562f":"result_C1 = np.apply_along_axis(augment, axis=1, arr=X[C1]).reshape(-1, 187)\nclass_C1 = np.ones(shape=(result_C1.shape[0],), dtype=int)*3\n\nresult_C3 = np.apply_along_axis(augment, axis=1, arr=X[C3]).reshape(-1, 187)\nclass_C3 = np.ones(shape=(result_C3.shape[0],), dtype=int)*3\n\n# result_C32 = np.apply_along_axis(augment, axis=1, arr=X[C3]).reshape(-1, 187)\n# class_C32 = np.ones(shape=(result_C32.shape[0],), dtype=int)*3\n\n# X = np.vstack([X, result_C1, result_C3])\n# y = np.hstack([y, class_C1, class_C3])\n\nX = np.vstack([X,  result_C3])\ny = np.hstack([y,  class_C3])","2420dace":"X.shape, y.shape","3a8bc980":"y = to_categorical(y)","91cc14b1":"from sklearn.utils import shuffle\nX, y = shuffle(X,y,random_state=0)","36d92791":"X = np.expand_dims(X,2)\ntestX = np.expand_dims(testX,2)","3ed2f092":"from keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, Input, Flatten, SeparableConv1D\nfrom keras.layers import GlobalMaxPooling1D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.merge import Concatenate\nfrom keras.models import Model\n\nfrom keras import backend as K\nfrom keras.optimizers import Adam\nfrom keras.callbacks import LearningRateScheduler, ModelCheckpoint\n\nn_obs, feature, depth = X.shape\nbatch_size = 1024","2f3275cc":"def build_model():\n    input_img = Input(shape=(feature, depth), name='ImageInput')\n    x = Conv1D(64, 3, activation='relu', padding='same', name='Conv1_1')(input_img)\n    x = Conv1D(64, 3, activation='relu', padding='same', name='Conv1_2')(x)\n    x = MaxPooling1D(2, name='pool1')(x)\n    \n    x = SeparableConv1D(128, 3, activation='relu', padding='same', name='Conv2_1')(x)\n    x = SeparableConv1D(128, 3, activation='relu', padding='same', name='Conv2_2')(x)\n    x = MaxPooling1D(2, name='pool2')(x)\n    \n    x = SeparableConv1D(256, 3, activation='relu', padding='same', name='Conv3_1')(x)\n    x = BatchNormalization(name='bn1')(x)\n    x = SeparableConv1D(256, 3, activation='relu', padding='same', name='Conv3_2')(x)\n    x = BatchNormalization(name='bn2')(x)\n    x = Dropout(0.3, name='dropout3-2')(x)\n    \n    x = SeparableConv1D(512, 3, activation='relu', padding='same', name='Conv3_3')(x)\n    x = MaxPooling1D(2, name='pool3')(x)\n    x = Dropout(0.3, name='dropout3-3')(x)\n    \n    x = Flatten(name='flatten')(x)\n    x = Dense(512, activation='relu', name='fc1')(x)\n    x = Dropout(0.6, name='dropout1')(x)\n    x = Dense(256, activation='relu', name='fc2')(x)\n    x = Dropout(0.5, name='dropout2')(x)\n    x = Dense(5, activation='softmax', name='fc3')(x)\n    \n    model = Model(inputs=input_img, outputs=x)\n    return model","13226e89":"model =  build_model()\nmodel.summary()","273f65c2":"model.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['acc',f1])","70f1a1e8":"from keras.callbacks import ModelCheckpoint\ncheckpointer = ModelCheckpoint(filepath=\"\/tmp\/weights-aug.hdf5\", monitor='val_f1', mode='max', verbose=1, save_best_only=True)","805642e4":"history = model.fit(X, y, validation_split=0.2,epochs=75,batch_size=batch_size*2,class_weight='auto',callbacks=[checkpointer])","c6c0f4a0":"model.load_weights('\/tmp\/weights-aug.hdf5')","1ff5fd46":"print(\"Evaluation: \")\nmse, acc, F1 = model.evaluate(testX, testy)\nprint('mean_squared_error :', mse)\nprint('accuracy:', acc)\nprint('F1:', F1)","a770a682":"K.set_value(model.optimizer.lr, 1e-4)\nmodel.fit(X, y, validation_split=0.2,epochs=30,batch_size=batch_size*2,class_weight='auto',callbacks=[checkpointer])","c6987c09":"K.set_value(model.optimizer.lr, 1e-5)\nmodel.fit(X, y, validation_split=0.2,epochs=30,batch_size=batch_size,class_weight='auto',callbacks=[checkpointer])","d86a0be0":"model.load_weights('\/tmp\/weights-aug.hdf5')\nprint(\"Evaluation: \")\nmse, acc, F1 = model.evaluate(testX, testy)\nprint('mean_squared_error :', mse)\nprint('accuracy:', acc)\nprint('F1:', F1)","42f029ab":"y_pred = model.predict(testX, batch_size=1000)","d69a8d92":"import itertools\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, label_ranking_average_precision_score, label_ranking_loss, coverage_error \n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(testy.argmax(axis=1), y_pred.argmax(axis=1))\n#np.set_printoptions(precision=0)\n\n# Plot non-normalized confusion matrix\nplt.figure(figsize=(5, 5))\nplot_confusion_matrix(cnf_matrix, classes=['N', 'S', 'V', 'F', 'Q'],\n                      title='Confusion matrix')\nplt.show()","7f9658cf":"# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","d627643b":"#K.set_value(model.optimizer.lr, 1e-3)\nhistory = model.fit(X, y, validation_data=(testX,testy),epochs=150,batch_size=batch_size*2,class_weight='auto',callbacks=[checkpointer])","3e02d91a":"model.load_weights('\/tmp\/weights-aug.hdf5')","cfd7fb02":"K.set_value(model.optimizer.lr, 1e-5)\nhistory = model.fit(X, y, validation_data=(testX,testy),epochs=30,batch_size=batch_size*2,class_weight='auto',callbacks=[checkpointer])","d27d2c3b":"model.load_weights('\/tmp\/weights-aug.hdf5')\nprint(\"Evaluation: \")\nmse, acc, F1 = model.evaluate(testX, testy)\nprint('mean_squared_error :', mse)\nprint('accuracy:', acc)\nprint('F1:', F1)","3bca60e2":"model.save(\"0.98887.hdf5\")","f0992b1b":"K.set_value(model.optimizer.lr, 1e-7)\nhistory = model.fit(X, y, validation_data=(testX,testy),epochs=30,batch_size=batch_size*2,class_weight='auto',callbacks=[checkpointer])","a6e50050":"y_pred = model.predict(testX, batch_size=1000)\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(testy.argmax(axis=1), y_pred.argmax(axis=1))\n#np.set_printoptions(precision=0)\n\n# Plot non-normalized confusion matrix\nplt.figure(figsize=(5, 5))\nplot_confusion_matrix(cnf_matrix, classes=['N', 'S', 'V', 'F', 'Q'],\n                      title='Confusion matrix')\nplt.show()","8d01e74a":"## Change LR train more","5c5f7b6a":"# \u6570\u636e\u589e\u5f3a","44e0f2c6":"## Try a network by using augmented data","25e51672":"# \u4f7f\u7528CNN\u8fdb\u4e00\u6b65\u4f18\u5316","618a2b0e":"# Separate features and targets","8a126482":"# ECG Heartbeat Categorization\n> By Ebby\n\nThe goal is to be able to classify heart disease from heartbeat signal. There is a lot of data, let's try to make sens out of it.","a6ca6da6":"# How many date we have in each category?","65b0ec32":"### \u8bad\u7ec3\u4e00\u4e2aBase Model \u786e\u5b9aBaseLine","439822b7":"> \u653e\u5927C1 C3 \u7c7b\uff0cC1\u7c7b\u653e\u59274\u500d\uff0cC3\u7c7b\u653e\u59278\u500d","ed12f944":"# Keras model to make prediction","1906ce9a":"## Random sample\n\nFor now let's take a random sample of the MIT train dataset.","386ca0ab":"## The last Step: Using all the training data and fitting more epochs","de83a9d8":"This seems to work pretty well!\n\nWork in progress"}}