{"cell_type":{"84b480ec":"code","8fb71981":"code","3f9c5618":"code","efd76348":"code","dd0933dd":"code","6b0b94c0":"code","4c133fb1":"code","592d6273":"code","ed2f15b6":"code","eb864667":"code","4fcca1ed":"code","26070061":"code","62c74b0d":"code","786e9f0c":"code","e7fa2a6a":"code","8bbd095a":"code","7483a40a":"code","8df24a41":"code","e92b7022":"code","6355e513":"code","ceec260d":"code","eb563fb3":"code","41469d61":"code","6ed0149e":"code","a0c00de4":"code","a0c3842a":"code","8257ad70":"code","85b8602e":"code","c18db9da":"code","7f43e7ef":"code","65f94844":"code","9ce13035":"code","c001601d":"code","fb84a558":"code","e37fc0ab":"code","cfa6291d":"code","da93cc8f":"code","6059a352":"code","49713bd3":"code","755e1bce":"code","6145dc1e":"code","1665cd83":"code","40918065":"code","2839ce9d":"code","c83638d2":"code","60f3d7e2":"code","dc4a05bc":"code","3ce98494":"code","d4446dd9":"markdown","e34207d7":"markdown","a848cb66":"markdown","01278aff":"markdown","52a5c4ff":"markdown","ade7a6d9":"markdown","23c3b0ec":"markdown","bf796284":"markdown","f849e8d9":"markdown","ee8e5ac4":"markdown","72b976a7":"markdown","6bceff03":"markdown","a3113c0a":"markdown","85cf8a2a":"markdown","e645aa25":"markdown","0fc40f2b":"markdown","07dbd402":"markdown","ccebda49":"markdown"},"source":{"84b480ec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport copy\nimport gc\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\nimport os\nfrom scipy.stats import norm, skew\nprint(os.listdir(\"..\/input\"))\n\nimport seaborn as sns\ncolor = sns.color_palette()\n\n# Any results you write to the current directory are saved as output.","8fb71981":"data = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nsub_df = pd.read_csv('..\/input\/sample_submission.csv')","3f9c5618":"plt.plot(data['ID'],data['y'])","efd76348":"data = data[data.y < 250]","dd0933dd":"plt.plot(data['ID'],data['y'])","6b0b94c0":"sns.distplot(data['y'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(data['y'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('target distribution')","4c133fb1":"from scipy import stats\n\nfig = plt.figure()\nres = stats.probplot(data['y'], plot=plt)\nplt.show()","592d6273":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ndata[\"y\"] = np.log1p(data[\"y\"])\n\n#Check the new distribution \nsns.distplot(data['y'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(data['y'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('target distribution')","ed2f15b6":"from scipy import stats\n\nfig = plt.figure()\nres = stats.probplot(data['y'], plot=plt)\nplt.show()","eb864667":"train_objs_num = len(data)\ndataset = pd.concat(objs=[data, test], axis=0)\ndataset = pd.get_dummies(dataset)\ndata = copy.copy(dataset[:train_objs_num])\ntest = copy.copy(dataset[train_objs_num:])","4fcca1ed":"useless_var = data.mean()[data.mean() == 0].index\ny_train = data.y\ndata.drop('y', axis = 1, inplace=True)\ndata.drop('ID', axis = 1, inplace= True)\ndata.drop(useless_var.values, axis = 1, inplace=True)\n\ntest.drop('y', axis = 1, inplace=True)\ntest.drop('ID', axis = 1, inplace= True)\ntest.drop(useless_var.values, axis = 1, inplace=True)","26070061":"depth1= ['X314', 'X315', 'X118', 'X232', 'X136', 'X127', 'X189', 'X261', 'X47', 'X236', 'X5_n', 'X5_w', 'X383', 'X204', 'X339', 'X275', 'X152', 'X50', 'X1_f', 'X6_e', 'X5_r', 'X104', 'X5_aa', 'X240', 'X206', 'X241', 'X95', 'X1_l', 'X113', 'X5_ag', 'X8_s', 'X5_v', 'X100', 'X13', 'X8_k', 'X6_c', 'X223', 'X114', 'X19', 'X115', 'X1_u', 'X80', 'X4_c', 'X68', 'X168', 'X306', 'X61', 'X350', 'X300', 'X132', 'X342', 'X1_c', 'X6_l', 'X267', 'X379', 'X23', 'X316', 'X8_j', 'X6_g', 'X5_m', 'X12', 'X0_z', 'X0_x', 'X77', 'X354', 'X196', 'X8_a', 'X283', 'X51', 'X8_g', 'X225', 'X292', 'X2_r', 'X148', 'X8_d', 'X8_x', 'X203', 'X5_k', 'X8_o', 'X101', 'X220', 'X191', 'X5_ab', 'X8_i', 'X8_w', 'X56', 'X65', 'X0_ab', 'X131', 'X224', 'X155', 'X181', 'X351', 'X0_au', 'X64', 'X75', 'X5_af', 'X105', 'X6_j', 'X117', 'X151', 'X177', 'X0_l', 'X362', 'X3_g', 'X5_l', 'X8_v', 'X285', 'X378', 'X244', 'X343', 'X228', 'X5_ae', 'X5_p', 'X273', 'X380', 'X5_ah', 'X27', 'X1_t', 'X6_h', 'X345', 'X1_h', 'X176', 'X116', 'X171', 'X1_aa', 'X6_d', 'X201', 'X122', 'X3_a', 'X5_ad', 'X178', 'X143', 'X38', 'X142', 'X14', 'X361', 'X1_q']\ndepth2= [('X118', 'X314'), ('X314', 'X315'), ('X232', 'X314'), ('X136', 'X315'), ('X127', 'X232'), ('X189', 'X315'), ('X232', 'X315'), ('X314', 'X47'), ('X261', 'X47'), ('X315', 'X47'), ('X232', 'X261'), ('X113', 'X118'), ('X118', 'X261'), ('X261', 'X5_r'), ('X236', 'X47'), ('X314', 'X61'), ('X232', 'X5_k'), ('X115', 'X261'), ('X206', 'X50'), ('X236', 'X261'), ('X5_w', 'X8_k'), ('X383', 'X50'), ('X315', 'X6_c'), ('X232', 'X47'), ('X241', 'X5_n'), ('X47', 'X5_w'), ('X236', 'X275'), ('X1_l', 'X5_aa'), ('X127', 'X136'), ('X261', 'X61'), ('X152', 'X5_n'), ('X143', 'X315'), ('X118', 'X5_w'), ('X118', 'X5_r'), ('X5_aa', 'X5_w'), ('X104', 'X118'), ('X152', 'X383'), ('X275', 'X5_n'), ('X118', 'X236'), ('X383', 'X5_n'), ('X351', 'X47'), ('X5_aa', 'X80'), ('X113', 'X261'), ('X240', 'X6_l'), ('X236', 'X314'), ('X13', 'X1_l'), ('X316', 'X6_e'), ('X115', 'X1_f'), ('X275', 'X339'), ('X114', 'X1_c'), ('X136', 'X232'), ('X127', 'X6_e'), ('X339', 'X47'), ('X47', 'X61'), ('X115', 'X314'), ('X168', 'X8_k'), ('X1_u', 'X47'), ('X151', 'X236'), ('X127', 'X47'), ('X152', 'X19'), ('X223', 'X6_g'), ('X118', 'X1_aa'), ('X104', 'X68'), ('X101', 'X152'), ('X306', 'X5_w'), ('X23', 'X8_k'), ('X113', 'X314'), ('X5_r', 'X5_w'), ('X47', 'X5_n'), ('X383', 'X6_l'), ('X127', 'X5_n'), ('X204', 'X5_n'), ('X6_l', 'X8_j'), ('X0_x', 'X47'), ('X300', 'X383'), ('X240', 'X8_s'), ('X5_n', 'X5_w'), ('X1_c', 'X5_r'), ('X100', 'X342'), ('X196', 'X5_n'), ('X47', 'X5_aa'), ('X0_z', 'X152'), ('X47', 'X5_ad'), ('X204', 'X383'), ('X379', 'X5_m'), ('X105', 'X232'), ('X19', 'X5_n'), ('X118', 'X1_q'), ('X261', 'X339'), ('X191', 'X339'), ('X350', 'X47'), ('X236', 'X240'), ('X5_r', 'X8_v'), ('X148', 'X95'), ('X127', 'X339'), ('X350', 'X5_n'), ('X132', 'X283'), ('X77', 'X8_j'), ('X1_l', 'X56'), ('X0_x', 'X1_u'), ('X12', 'X5_ag'), ('X1_f', 'X8_d'), ('X204', 'X236'), ('X204', 'X316'), ('X116', 'X47'), ('X113', 'X8_a'), ('X6_l', 'X77'), ('X204', 'X95'), ('X225', 'X383'), ('X100', 'X5_ab'), ('X100', 'X1_f'), ('X5_m', 'X8_s'), ('X19', 'X6_g'), ('X115', 'X8_s'), ('X339', 'X383'), ('X127', 'X1_f'), ('X118', 'X316'), ('X204', 'X5_aa'), ('X118', 'X8_a'), ('X115', 'X155'), ('X1_f', 'X5_v'), ('X104', 'X275'), ('X132', 'X1_l'), ('X1_f', 'X3_g'), ('X1_c', 'X315'), ('X23', 'X5_w'), ('X132', 'X339'), ('X116', 'X77'), ('X12', 'X383'), ('X2_r', 'X8_a'), ('X131', 'X5_aa'), ('X51', 'X5_k'), ('X5_ah', 'X8_g'), ('X204', 'X4_c'), ('X383', 'X5_v'), ('X362', 'X5_n'), ('X220', 'X342'), ('X204', 'X339'), ('X132', 'X354'), ('X132', 'X203'), ('X204', 'X240'), ('X383', 'X5_l'), ('X236', 'X292'), ('X1_c', 'X343'), ('X267', 'X95'), ('X292', 'X378'), ('X350', 'X5_l'), ('X345', 'X61'), ('X240', 'X8_o'), ('X117', 'X275'), ('X306', 'X5_n'), ('X100', 'X8_o'), ('X0_x', 'X224'), ('X5_n', 'X6_j'), ('X178', 'X61'), ('X236', 'X383'), ('X19', 'X5_m'), ('X236', 'X95'), ('X132', 'X5_v'), ('X132', 'X6_g'), ('X132', 'X236'), ('X19', 'X204'), ('X5_ag', 'X5_k'), ('X315', 'X6_g'), ('X240', 'X8_x'), ('X267', 'X316'), ('X383', 'X75'), ('X261', 'X383'), ('X105', 'X5_ag'), ('X267', 'X383'), ('X5_v', 'X5_w'), ('X65', 'X6_j'), ('X177', 'X267'), ('X342', 'X95'), ('X354', 'X6_j'), ('X306', 'X75'), ('X12', 'X95'), ('X115', 'X64'), ('X105', 'X114'), ('X181', 'X316'), ('X350', 'X380'), ('X383', 'X5_r'), ('X383', 'X8_x'), ('X292', 'X5_n'), ('X113', 'X3_a'), ('X142', 'X68'), ('X0_x', 'X351'), ('X292', 'X339'), ('X342', 'X5_m'), ('X1_l', 'X8_w'), ('X204', 'X47'), ('X151', 'X6_g'), ('X6_h', 'X75'), ('X240', 'X342'), ('X1_h', 'X64'), ('X12', 'X5_v'), ('X267', 'X51'), ('X342', 'X5_ag'), ('X176', 'X8_s'), ('X339', 'X4_c'), ('X171', 'X64'), ('X339', 'X8_d'), ('X206', 'X383'), ('X285', 'X350'), ('X342', 'X5_v'), ('X14', 'X68'), ('X1_u', 'X203'), ('X148', 'X244'), ('X203', 'X4_c'), ('X228', 'X8_w'), ('X342', 'X5_aa'), ('X12', 'X5_r'), ('X0_au', 'X339'), ('X132', 'X8_i'), ('X203', 'X267'), ('X0_au', 'X0_z'), ('X100', 'X23'), ('X350', 'X8_v'), ('X292', 'X8_d'), ('X177', 'X1_u'), ('X4_c', 'X5_w'), ('X148', 'X267'), ('X104', 'X95'), ('X339', 'X95'), ('X1_t', 'X5_ae'), ('X206', 'X8_i'), ('X228', 'X5_aa'), ('X315', 'X351'), ('X104', 'X342'), ('X5_p', 'X8_a'), ('X0_l', 'X4_c'), ('X104', 'X206'), ('X5_v', 'X8_d'), ('X5_m', 'X95'), ('X316', 'X5_ah'), ('X104', 'X5_w'), ('X1_t', 'X8_a'), ('X292', 'X5_ag'), ('X292', 'X5_af'), ('X1_u', 'X6_d'), ('X306', 'X4_c'), ('X0_ab', 'X4_c'), ('X5_aa', 'X5_ah'), ('X240', 'X275'), ('X240', 'X95'), ('X342', 'X5_af'), ('X273', 'X5_ae'), ('X0_au', 'X5_r'), ('X115', 'X177'), ('X0_ab', 'X204')]\ndepth3=[('X136', 'X314', 'X315'), ('X127', 'X232', 'X314'), ('X189', 'X314', 'X315'), ('X232', 'X314', 'X315'), ('X113', 'X118', 'X314'), ('X314', 'X315', 'X47'), ('X143', 'X314', 'X315'), ('X232', 'X314', 'X47'), ('X314', 'X315', 'X6_c'), ('X118', 'X1_q', 'X314'), ('X118', 'X261', 'X5_r'), ('X236', 'X314', 'X47'), ('X118', 'X1_aa', 'X314'), ('X261', 'X315', 'X47'), ('X232', 'X261', 'X5_k'), ('X118', 'X314', 'X5_r'), ('X118', 'X314', 'X8_a'), ('X118', 'X1_aa', 'X261'), ('X261', 'X5_r', 'X8_v'), ('X127', 'X232', 'X261'), ('X314', 'X47', 'X61'), ('X236', 'X261', 'X275'), ('X206', 'X383', 'X50'), ('X178', 'X314', 'X61'), ('X127', 'X136', 'X232'), ('X115', 'X1_f', 'X261'), ('X236', 'X261', 'X47'), ('X115', 'X261', 'X8_s'), ('X104', 'X118', 'X5_w'), ('X136', 'X232', 'X261'), ('X118', 'X236', 'X5_w'), ('X168', 'X5_w', 'X8_k'), ('X261', 'X47', 'X5_ad'), ('X23', 'X5_w', 'X8_k'), ('X275', 'X383', 'X5_n'), ('X261', 'X47', 'X5_w'), ('X261', 'X47', 'X61'), ('X152', 'X19', 'X5_n'), ('X261', 'X345', 'X61'), ('X261', 'X5_r', 'X5_w'), ('X127', 'X232', 'X47'), ('X261', 'X350', 'X47'), ('X113', 'X314', 'X47'), ('X13', 'X1_l', 'X5_aa'), ('X127', 'X232', 'X6_e'), ('X0_x', 'X1_u', 'X47'), ('X113', 'X261', 'X8_a'), ('X0_x', 'X351', 'X47'), ('X232', 'X51', 'X5_k'), ('X306', 'X47', 'X5_w'), ('X315', 'X351', 'X47'), ('X241', 'X47', 'X5_n'), ('X204', 'X316', 'X6_e'), ('X115', 'X1_f', 'X314'), ('X232', 'X5_ag', 'X5_k'), ('X240', 'X383', 'X6_l'), ('X114', 'X1_c', 'X5_r'), ('X127', 'X232', 'X5_n'), ('X113', 'X261', 'X3_a'), ('X115', 'X155', 'X314'), ('X241', 'X275', 'X5_n'), ('X383', 'X6_l', 'X8_j'), ('X101', 'X152', 'X383'), ('X47', 'X5_n', 'X5_w'), ('X19', 'X223', 'X6_g'), ('X47', 'X5_w', 'X8_k'), ('X196', 'X275', 'X5_n'), ('X47', 'X5_aa', 'X80'), ('X191', 'X275', 'X339'), ('X0_x', 'X224', 'X47'), ('X1_l', 'X47', 'X5_aa'), ('X261', 'X339', 'X47'), ('X236', 'X240', 'X275'), ('X5_aa', 'X5_w', 'X80'), ('X300', 'X383', 'X5_l'), ('X0_z', 'X152', 'X19'), ('X127', 'X232', 'X339'), ('X148', 'X267', 'X95'), ('X1_l', 'X5_aa', 'X5_w'), ('X152', 'X204', 'X383'), ('X204', 'X350', 'X5_n'), ('X1_l', 'X56', 'X5_aa'), ('X105', 'X232', 'X5_ag'), ('X204', 'X236', 'X47'), ('X105', 'X114', 'X232'), ('X132', 'X283', 'X339'), ('X19', 'X204', 'X5_n'), ('X151', 'X236', 'X275'), ('X339', 'X383', 'X47'), ('X127', 'X1_f', 'X232'), ('X204', 'X5_aa', 'X5_w'), ('X100', 'X342', 'X5_ab'), ('X100', 'X1_f', 'X5_v'), ('X6_l', 'X77', 'X8_j'), ('X131', 'X204', 'X5_aa'), ('X12', 'X5_ag', 'X5_r'), ('X225', 'X383', 'X5_r'), ('X118', 'X204', 'X316'), ('X204', 'X383', 'X5_v'), ('X104', 'X275', 'X339'), ('X116', 'X351', 'X47'), ('X1_f', 'X236', 'X8_d'), ('X132', 'X1_l', 'X5_v'), ('X1_f', 'X3_g', 'X5_v'), ('X132', 'X339', 'X354'), ('X152', 'X19', 'X204'), ('X204', 'X383', 'X50'), ('X12', 'X383', 'X5_l'), ('X204', 'X339', 'X5_n'), ('X1_c', 'X315', 'X5_r'), ('X5_aa', 'X5_r', 'X5_w'), ('X151', 'X236', 'X47'), ('X116', 'X13', 'X47'), ('X117', 'X275', 'X339'), ('X116', 'X6_l', 'X77'), ('X306', 'X362', 'X5_n'), ('X1_t', 'X2_r', 'X8_a'), ('X236', 'X383', 'X95'), ('X100', 'X342', 'X8_o'), ('X220', 'X342', 'X5_aa'), ('X19', 'X204', 'X5_m'), ('X5_aa', 'X5_ah', 'X8_g'), ('X132', 'X236', 'X5_v'), ('X1_u', 'X47', 'X95'), ('X100', 'X240', 'X342'), ('X1_c', 'X343', 'X5_r'), ('X5_aa', 'X5_w', 'X8_k'), ('X379', 'X5_m', 'X95'), ('X267', 'X316', 'X383'), ('X240', 'X6_l', 'X8_o'), ('X306', 'X5_n', 'X6_j'), ('X12', 'X342', 'X95'), ('X350', 'X380', 'X5_l'), ('X5_aa', 'X5_v', 'X5_w'), ('X261', 'X383', 'X5_r'), ('X204', 'X383', 'X8_x'), ('X19', 'X315', 'X6_g'), ('X306', 'X383', 'X75'), ('X292', 'X339', 'X5_n'), ('X240', 'X6_l', 'X8_x'), ('X1_l', 'X5_aa', 'X8_w'), ('X267', 'X383', 'X51'), ('X240', 'X342', 'X5_m'), ('X1_h', 'X241', 'X5_n'), ('X0_l', 'X379', 'X5_m'), ('X204', 'X47', 'X95'), ('X177', 'X1_u', 'X267'), ('X132', 'X203', 'X6_g'), ('X23', 'X5_aa', 'X5_w'), ('X181', 'X204', 'X316'), ('X176', 'X240', 'X8_s'), ('X104', 'X142', 'X68'), ('X306', 'X6_h', 'X75'), ('X339', 'X4_c', 'X8_d'), ('X0_ab', 'X65', 'X6_j'), ('X0_ab', 'X354', 'X6_j'), ('X115', 'X1_h', 'X64'), ('X204', 'X240', 'X95'), ('X5_m', 'X8_s', 'X95'), ('X12', 'X5_r', 'X5_v'), ('X104', 'X5_w', 'X68'), ('X132', 'X203', 'X4_c'), ('X151', 'X19', 'X6_g'), ('X292', 'X339', 'X378'), ('X342', 'X5_aa', 'X5_v'), ('X148', 'X244', 'X95'), ('X115', 'X177', 'X267'), ('X203', 'X267', 'X4_c'), ('X151', 'X236', 'X383'), ('X0_au', 'X0_z', 'X339'), ('X104', 'X206', 'X383'), ('X240', 'X8_s', 'X95'), ('X285', 'X350', 'X5_l'), ('X1_u', 'X203', 'X47'), ('X104', 'X14', 'X68'), ('X342', 'X4_c', 'X5_ag'), ('X171', 'X1_h', 'X64'), ('X204', 'X4_c', 'X5_w'), ('X228', 'X5_aa', 'X8_w'), ('X100', 'X23', 'X5_w'), ('X236', 'X292', 'X8_d'), ('X104', 'X339', 'X95'), ('X204', 'X240', 'X8_s'), ('X132', 'X203', 'X8_i'), ('X0_l', 'X5_m', 'X8_s'), ('X0_l', 'X204', 'X4_c'), ('X350', 'X5_l', 'X8_v'), ('X104', 'X206', 'X8_i'), ('X104', 'X342', 'X5_w'), ('X1_t', 'X5_p', 'X8_a'), ('X0_au', 'X273', 'X339'), ('X236', 'X292', 'X5_ag'), ('X316', 'X5_aa', 'X5_ah'), ('X1_u', 'X47', 'X6_d'), ('X0_ab', 'X306', 'X4_c'), ('X236', 'X5_v', 'X8_d'), ('X236', 'X292', 'X5_af'), ('X240', 'X275', 'X95'), ('X1_t', 'X273', 'X5_ae'), ('X0_au', 'X4_c', 'X5_r'), ('X0_ab', 'X204', 'X5_ag'), ('X342', 'X4_c', 'X5_af'), ('X122', 'X1_t', 'X5_ae'), ('X19', 'X1_h', 'X5_n'), ('X236', 'X292', 'X378'), ('X204', 'X240', 'X261'), ('X114', 'X228', 'X5_aa'), ('X177', 'X203', 'X27'), ('X0_au', 'X201', 'X4_c'), ('X203', 'X27', 'X8_g'), ('X0_l', 'X131', 'X38'), ('X0_l', 'X131', 'X361')]","62c74b0d":"data = data[depth1]\ntest = test[depth1]\nfor (a,b) in depth2[0:17]:\n    data.loc[:,a+'_+_'+b] = data[a].add(data[b])\n    data.loc[:,a+'_*_'+b] = data[a].mul(data[b])\n    #data.loc[:,a+'_-_'+b] = data[a].sub(data[b])\n    data.loc[:,a+'_abs(-)_'+b] = np.abs(data[a].sub(data[b]))\n    \n    test.loc[:,a+'_+_'+b] = test[a].add(test[b])\n    test.loc[:,a+'_*_'+b] = test[a].mul(test[b])\n    #test.loc[:,a+'_-_'+b] = test[a].sub(test[b])\n    test.loc[:,a+'_abs(-)_'+b] = np.abs(test[a].sub(test[b]))\n    #data.loc[:,a+'_max_'+b] = np.maximum(data[a],data[b])\n    #data.loc[:,a+'_min_'+b] = np.minimum(data[a],data[b])\n    \nfor (a,b,c) in depth3[0:13]:\n    data.loc[:,a+'_+_'+b+'_+_'+c] = data[a].add(data[b]).add(data[c])\n    test.loc[:,a+'_+_'+b+'_+_'+c] = test[a].add(test[b]).add(test[c])\n    data.loc[:,a+'_*_'+b+'_*_'+c] = data[a].mul(data[b]).mul(data[c])\n    test.loc[:,a+'_*_'+b+'_*_'+c] = test[a].mul(test[b]).mul(test[c])\n    data.loc[:,a+'_+_'+b+'_*_'+c] = data[a].add(data[b]).mul(data[c])\n    test.loc[:,a+'_+_'+b+'_*_'+c] = test[a].add(test[b]).mul(test[c])\n    data.loc[:,a+'_*_'+b+'_+_'+c] = data[a].mul(data[b]).add(data[c])\n    test.loc[:,a+'_*_'+b+'_+_'+c] = test[a].mul(test[b]).add(test[c])\n    \n    test.loc[:,a+'_-_'+b+'_+_'+c] = np.abs(test[a].sub(test[b])).add(test[c])\n    test.loc[:,a+'_-_'+b+'_*_'+c] = np.abs(test[a].sub(test[b])).mul(test[c])\n    test.loc[:,a+'_*_'+b+'_-_'+c] = test[a].mul(np.abs((test[b]).sub(test[c])))\n    test.loc[:,a+'_+_'+b+'_-_'+c] = test[a].add(np.abs((test[b]).sub(test[c])))\n    test.loc[:,a+'_-_'+b+'_-_'+c] = np.abs(np.abs(test[a].sub(test[b])).sub(test[c]))\n    \n    data.loc[:,a+'_-_'+b+'_+_'+c] = np.abs(data[a].sub(data[b])).add(data[c])\n    data.loc[:,a+'_-_'+b+'_*_'+c] = np.abs(data[a].sub(data[b])).mul(data[c])\n    data.loc[:,a+'_*_'+b+'_-_'+c] = data[a].mul(np.abs((data[b]).sub(data[c])))\n    data.loc[:,a+'_+_'+b+'_-_'+c] = data[a].add(np.abs((data[b]).sub(data[c])))\n    data.loc[:,a+'_-_'+b+'_-_'+c] = np.abs(np.abs(data[a].sub(data[b])).sub(data[c]))","786e9f0c":"from  itertools import combinations\nfrom sklearn.metrics import matthews_corrcoef\ncc = list(combinations(data.columns,2))\ndata_1 = [(c[0],c[1],abs(matthews_corrcoef(data[c[1]],data[c[0]]))) for c in cc]\n#data_1.columns = data_1.columns.map('_*_'.join)","e7fa2a6a":"to_drop = []\nfor (c1,c2,score) in data_1:\n    if score >0.95:\n        to_drop.append(c2)\nprint(to_drop)\ndata.drop(to_drop, axis=1, inplace = True)\ntest.drop(to_drop, axis=1, inplace = True)\n","8bbd095a":"data.shape","7483a40a":"from sklearn.model_selection import KFold, cross_val_score\nn_folds = 5\n\ndef r2_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(data.values)\n    r2= cross_val_score(model, data.values, y_train, scoring=\"r2\", cv = kf)\n    return(r2)\n","8df24a41":"model_xgb = xgb.XGBRegressor(alpha= 0, colsample_bytree= 1, eta= 0.005, reg_lambda= 0, max_depth= 4, min_child_weight= 0, n_estimators= 100)","e92b7022":"#'alpha': 0, 'colsample_bytree': 1, 'eta': 0.005, 'lambda': 0, 'max_depth': 4, 'min_child_weight': 0, 'n_estimators': 100\n#XGB Parameter Tuning\ngridParams = {\n    'eta': [0.005],\n    'max_depth': [3,4,5],\n    'n_estimators': [100,200],\n    'min_child_weight' : [0,0.25],\n    'colsample_bytree' : [0.8,1],\n    'lambda' : [0,0.2],\n    'alpha': [0]\n    }\n#grid = GridSearchCV(model_xgb, gridParams,verbose=1, cv=4,n_jobs=2,scoring = 'r2')\n# Run the grid\n#grid.fit(data, y_train)\n\n# Print the best parameters found\n#print(grid.best_params_)\n#print(grid.best_score_)","6355e513":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(max_depth= 6, min_samples_leaf= 2, min_samples_split= 30, n_estimators= 50)","ceec260d":"#RF Parameter Tuning\ngridParams = {\n    'max_depth': [3,6,7],\n    'n_estimators': [10,50,100],\n    'min_samples_leaf' : [2,10],\n    'min_samples_split' : [10,30,60]\n    }\n#grid = GridSearchCV(rf, gridParams,verbose=1, cv=4,n_jobs=2,scoring = 'r2')\n# Run the grid\n#grid.fit(data, y_train)\n\n# Print the best parameters found\n#print(grid.best_params_)\n#print(grid.best_score_)","eb563fb3":"model_lgb = lgb.LGBMRegressor(feature_fraction= 1, lambda_l1= 0, learning_rate= 0.05, max_depth= 3, min_data_in_leaf= 50, n_estimators= 500, num_leaves= 4)","41469d61":"#objective='regression', learning_rate=0.05, feature_fraction= 0.8, lambda_l1= 0, max_depth= 6, min_data_in_leaf= 50, n_estimators= 100, num_leaves= 8\n#lgb Parameter Tuning\ngridParams = {\n    'num_leaves': [4,6,8,10],\n    'max_depth': [2,3,6],\n    'n_estimators': [500,1000],\n    'feature_fraction' : [0.2,0.6,1],\n    'lambda_l1' : [0,0.5],\n    'min_data_in_leaf': [6,12,20,50],\n    'learning_rate' : [0.005, 0.05]\n    }\n#grid = GridSearchCV(model_lgb, gridParams,verbose=1, cv=4,n_jobs=2,scoring = 'r2')\n# Run the grid\n#grid.fit(data, y_train)\n\n# Print the best parameters found\n#print(grid.best_params_)\n#print(grid.best_score_)","6ed0149e":"from sklearn.kernel_ridge import KernelRidge\nKRR = KernelRidge(kernel='polynomial', alpha = 1, coef0=3, degree = 2)","a0c00de4":"#KRR Parameter Tuning\ngridParams = {\n    'alpha': [1,2,3],\n    'degree': [2,3,4],\n    'coef0' : [1,2,3],\n}\n#grid = GridSearchCV(KRR, gridParams,verbose=1, cv=4,n_jobs=2,scoring = 'r2')\n# Run the grid\n#grid.fit(data, y_train)\n\n# Print the best parameters found\n#print(grid.best_params_)\n#print(grid.best_score_)","a0c3842a":"from sklearn.svm import SVR\nsvr = SVR(C= 1, gamma= 0.001)","8257ad70":"#SVR Parameter Tuning\ngridParams = {\n    'gamma': [0.001,0.01,0.1],\n    'C': [1,10,100],\n}\n#grid = GridSearchCV(svr, gridParams,verbose=1, cv=4,n_jobs=2,scoring = 'r2')\n# Run the grid\n#grid.fit(data, y_train)\n\n# Print the best parameters found\n#print(grid.best_params_)\n#print(grid.best_score_)","85b8602e":"from sklearn.linear_model import Lasso\nlasso = Lasso(alpha =0.0005, random_state=1)","c18db9da":"#Lasso Parameter Tuning\ngridParams = {\n    'alpha': [0,0.0005,0.005],\n}\n#grid = GridSearchCV(lasso, gridParams,verbose=1, cv=4,n_jobs=2,scoring = 'r2')\n# Run the grid\n#grid.fit(data, y_train)\n\n# Print the best parameters found\n#print(grid.best_params_)\n#print(grid.best_score_)","7f43e7ef":"from sklearn.ensemble import ExtraTreesRegressor\net = ExtraTreesRegressor(max_depth= 5, min_samples_leaf= 40, n_estimators= 500)\n","65f94844":"#Extra Trees Parameter Tuning\ngridParams = {\n    'n_estimators': [500],\n    'max_depth': [2,5,6,7],\n    'min_samples_leaf' : [30,40,50],\n    #'min_samples_split': [0.2,0.6,0.9],\n}\n#grid = GridSearchCV(et, gridParams,verbose=1, cv=4,n_jobs=2,scoring = 'r2')\n# Run the grid\n#grid.fit(data, y_train)\n\n# Print the best parameters found\n#print(grid.best_params_)\n#print(grid.best_score_)\n","9ce13035":"from sklearn.ensemble import AdaBoostRegressor\nada = AdaBoostRegressor(learning_rate= 0.005, n_estimators= 50)","c001601d":"#Adaboost Parameter Tuning\ngridParams = {\n    'n_estimators': [30,50,80],\n    'learning_rate': [0.005, 0.05,0.5,1]\n}\n#grid = GridSearchCV(ada, gridParams,verbose=1, cv=4,n_jobs=2,scoring = 'r2')\n# Run the grid\n#grid.fit(data, y_train)\n\n# Print the best parameters found\n#print(grid.best_params_)\n#print(grid.best_score_)","fb84a558":"from catboost import CatBoostRegressor\ncat = CatBoostRegressor(iterations=3000,learning_rate=0.005,depth=2)","e37fc0ab":"#CatBoost Parameter Tuning\ngridParams = {\n            'iterations': [3000],\n            'learning_rate': [0.005],\n            'depth' : [2,4,6]\n}\n#grid = GridSearchCV(cat, gridParams,verbose=1, cv=4,n_jobs=2,scoring = 'r2')\n# Run the grid\n#grid.fit(data, y_train)\n\n# Print the best parameters found\n#print(grid.best_params_)\n#print(grid.best_score_)\n","cfa6291d":"from sklearn.ensemble import GradientBoostingRegressor\ngbr = GradientBoostingRegressor(n_estimators=1000,learning_rate=0.005,max_depth=3,alpha=0.1)","da93cc8f":"#GradientBoostingRegressor Parameter Tuning\ngridParams = {\n            'n_estimators': [1000],\n            'learning_rate': [0.005],\n            'max_depth' : [3,5,7],\n            'alpha':[0.1, 0.5]\n    \n}\n#grid = GridSearchCV(gbr, gridParams,verbose=1, cv=4,n_jobs=2,scoring = 'r2')\n# Run the grid\n#grid.fit(data, y_train)\n\n# Print the best parameters found\n#print(grid.best_params_)\n#print(grid.best_score_)","6059a352":"from sklearn.linear_model import ElasticNet\neNet = ElasticNet(alpha=0,l1_ratio=0,max_iter=5)","49713bd3":"#ElasticNEt Parameter Tuning\ngridParams = {\n            'alpha': [0],\n            'max_iter':[0,5,10,15],\n            'l1_ratio': [0]\n}\n#grid = GridSearchCV(eNet, gridParams,verbose=1, cv=4,n_jobs=2,scoring = 'r2')\n# Run the grid\n#grid.fit(data, y_train)\n\n# Print the best parameters found\n#print(grid.best_params_)\n#print(grid.best_score_)","755e1bce":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","6145dc1e":"averaged_models = AveragingModels(models = (KRR, model_lgb, rf, svr, model_xgb, lasso, eNet,ada, et, gbr, cat))\n\n#score = r2_cv(averaged_models)\n#print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","1665cd83":"averaged_models.fit(data.values,y_train)","40918065":"predictions = np.expm1(averaged_models.predict(test.values))","2839ce9d":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=8):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y.values[train_index]) \n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        \n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","c83638d2":"#meta_model = xgb.XGBRegressor(eta= 0.05, max_depth= 4, n_estimators= 100)","60f3d7e2":"\n\n#stacked_averaged_models = StackingAveragedModels(base_models = (KRR, model_lgb, rf, svr, model_xgb, lasso, ada, et),\n#                                                 meta_model = meta_model)\n","dc4a05bc":"#stacked_averaged_models.fit(data.values,y_train)\n#stacked_pred = np.expm1(stacked_averaged_models.predict(test.values))","3ce98494":"sub_df.drop('y', axis = 1, inplace = True)\nsub_df['y'] = predictions\nsub_df.to_csv(\"submission.csv\", index=False)\nsub_df","d4446dd9":"**ElasticNet**","e34207d7":"**XGBOOST**","a848cb66":"**Import data**","01278aff":"**Target Transformation**","52a5c4ff":"**Categorical variables One-Hot Encoding **","ade7a6d9":"**Extra Trees**","23c3b0ec":"**SVR**","bf796284":"**CatBoost**","f849e8d9":"**Models Averaging**","ee8e5ac4":"**GradientBoostingRegressor**","72b976a7":"**Remove outliers**","6bceff03":"**Lasso**","a3113c0a":"**Random forest**","85cf8a2a":"**LightGBM**","e645aa25":"**KernelRidge**","0fc40f2b":"**AdaBoost**","07dbd402":"**Features selection**","ccebda49":"**Features engineering**"}}