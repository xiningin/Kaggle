{"cell_type":{"4352cb8d":"code","7883a0c4":"code","22fa00ce":"code","47f2dd9a":"code","c445ffed":"code","a8bbcd48":"code","8227be4b":"code","24759215":"code","663de54f":"code","0718a69c":"code","217a55fd":"code","1221919b":"code","18fd6587":"code","d4442aaf":"code","2958f37f":"code","bdb58dc3":"code","cc37992e":"code","2691c0a5":"code","9e2ab288":"code","367bb1f4":"code","35bb37f6":"code","93c04ff9":"code","7a78fb80":"code","8568b2bb":"code","58692259":"code","cbc8065f":"code","236dd976":"code","693dc483":"code","fd92697f":"code","ae732cd4":"code","e31616cc":"code","b3d8a5db":"code","e5e39aea":"code","26c50f55":"markdown","ed2bcbb5":"markdown","5b461fcb":"markdown","adeae504":"markdown","184c7b29":"markdown","1ce742c6":"markdown","f7ada69a":"markdown","287849b6":"markdown","915b53bd":"markdown","1ce290f1":"markdown","5fb6a2ca":"markdown","aa62e71d":"markdown","f1be077f":"markdown","f1a22e04":"markdown","85a275bf":"markdown","82aba2f1":"markdown","528ba24c":"markdown","0fd4cfd3":"markdown","f21aa977":"markdown","840c072a":"markdown","e21f5b1d":"markdown","5eac4ed9":"markdown"},"source":{"4352cb8d":"import pandas as pd\nimport numpy as np\n\nfrom statsmodels.tsa.api import VAR\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.stattools import grangercausalitytests\nfrom statsmodels.tools.eval_measures import rmse, aic\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport datetime","7883a0c4":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', -1)\npd.plotting.register_matplotlib_converters()\nnp.set_printoptions(suppress=True)","22fa00ce":"ita_regional=pd.read_csv(\"..\/input\/covid19-in-italy\/covid19_italy_region.csv\")","47f2dd9a":"ita_regional.info()","c445ffed":"# Checking the percentage of missing data in each column\nper_missing = ita_regional.isna().sum()*100\/len(ita_regional)\nper_missing.sort_values(ascending=False)","a8bbcd48":"# Check for the period covered by the data (total # of days)\nita_regional['Date'] = pd.to_datetime(ita_regional['Date']).dt.normalize()\n(ita_regional.Date.max()-ita_regional.Date.min()) + datetime.timedelta(days=1)","8227be4b":"var_df = ita_regional.groupby('Date')[['HospitalizedPatients', 'IntensiveCarePatients', 'TotalHospitalizedPatients',\n                                      'HomeConfinement', 'CurrentPositiveCases', 'NewPositiveCases',\n                                      'Recovered', 'Deaths', 'TotalPositiveCases', 'TestsPerformed']].sum().reset_index()\nprint(\"df shape: \", var_df.shape)","24759215":"var_df.head()","663de54f":"# Droping columns who are part of other columns (e.g., \n#    TotalHospitalizedPatients = HospitalizedPatients + IntensiveCarePatients)\n\nvar_df.drop(['HospitalizedPatients', 'IntensiveCarePatients', 'NewPositiveCases', 'TotalPositiveCases', 'CurrentPositiveCases'],\n            axis=1, inplace=True)\n\nvar_df.head(n=5)","0718a69c":"type(var_df['Date'])","217a55fd":"fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(22,5))\n\nfor ycol, ax in zip(['TotalHospitalizedPatients', 'HomeConfinement',\n                                                  'Recovered', 'Deaths', 'TestsPerformed'], axes):\n\n    var_df.plot(kind='line', x='Date', y=ycol, ax=ax, alpha=0.5, color='r')","1221919b":"def grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False, maxlag=5):    \n    \n    \"\"\"Check Granger Causality of all possible combinations of the Time series.\n    The rows are the response variable, columns are predictors. \n\n    data      : pandas dataframe containing the time series variables\n    variables : list containing names of the time series variables.\n    \"\"\"\n    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n    for c in df.columns:\n        for r in df.index:\n            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n            min_p_value = np.min(p_values)\n            df.loc[r, c] = min_p_value\n    df.columns = [var + '_x' for var in variables]\n    df.index = [var + '_y' for var in variables]\n    return df  ","18fd6587":"grangers_causation_matrix(var_df, variables = ['TotalHospitalizedPatients', 'HomeConfinement',\n                                                  'Recovered', 'Deaths', 'TestsPerformed']) ","d4442aaf":"from statsmodels.tsa.vector_ar.vecm import coint_johansen\n\ndef cointegration_test(df, alpha=0.05): \n    \"\"\"Perform Johanson's Cointegration Test and Report Summary\"\"\"\n    out = coint_johansen(df,-1,5)\n    d = {'0.90':0, '0.95':1, '0.99':2}\n    traces = out.lr1\n    cvts = out.cvt[:, d[str(1-alpha)]]\n    def adjust(val, length= 6): return str(val).ljust(length)\n\n    # Summary\n    print('Name   ::  Test Stat > C(95%)    =>   Signif  \\n', '--'*20)\n    for col, trace, cvt in zip(df.columns, traces, cvts):\n        print(adjust(col), ':: ', adjust(round(trace,2), 9), \">\", adjust(cvt, 8), ' =>  ' , trace > cvt)","2958f37f":"cointegration_test(var_df[['TotalHospitalizedPatients', 'HomeConfinement',\n                                                  'Recovered', 'Deaths', 'TestsPerformed']])","bdb58dc3":"test_frec = 0.25\nn_test = round((len(var_df)) * test_frec)\ndf_train, df_test = var_df[0:-n_test], var_df[-n_test:]\n# df_train_copy = df_train.copy()\ndf_train.drop('Date',1, inplace=True)","cc37992e":"def adfuller_test(series, signif=0.05, name='', verbose=False):\n    \"\"\"Perform ADFuller to test for Stationarity of given series and print report\"\"\"\n    r = adfuller(series, autolag='AIC')\n    output = {'test_statistic':round(r[0], 4), 'pvalue':round(r[1], 4), 'n_lags':round(r[2], 4), 'n_obs':r[3]}\n    p_value = output['pvalue'] \n    def adjust(val, length= 6): return str(val).ljust(length)\n\n    # Print Summary\n    print(f'    Augmented Dickey-Fuller Test on \"{name}\"', \"\\n   \", '-'*47)\n    print(f' Null Hypothesis: Data has unit root. Non-Stationary.')\n    print(f' Significance Level    = {signif}')\n    print(f' Test Statistic        = {output[\"test_statistic\"]}')\n    print(f' No. Lags Chosen       = {output[\"n_lags\"]}')\n\n    for key,val in r[4].items():\n        print(f' Critical value {adjust(key)} = {round(val, 3)}')\n\n    if p_value <= signif:\n        print(f\" => P-Value = {p_value}. Rejecting Null Hypothesis.\")\n        print(f\" => Series is Stationary.\")\n    else:\n        print(f\" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.\")\n        print(f\" => Series is Non-Stationary.\")","2691c0a5":"# ADF Test on each column\nfor name, column in df_train.iteritems():\n    adfuller_test(column, name=column.name)\n    print('\\n')","9e2ab288":"# 1st difference\ndf_differenced = df_train.diff().dropna()","367bb1f4":"# ADF Test on each column\nfor name, column in df_differenced.iteritems():\n    adfuller_test(column, name=column.name)\n    print('\\n')","35bb37f6":"# 2nd Difference\ndf_differenced = df_differenced.diff().dropna()","93c04ff9":"# ADF Test on each column\nfor name, column in df_differenced.iteritems():\n    adfuller_test(column, name=column.name)\n    print('\\n')","7a78fb80":"model = VAR(df_differenced[['TotalHospitalizedPatients', 'HomeConfinement',\n                                                  'Recovered', 'Deaths', 'TestsPerformed']])\n\nfitted = model.fit(6)\nfitted.summary()","8568b2bb":"from statsmodels.stats.stattools import durbin_watson\nout = durbin_watson(fitted.resid)\n\nfor col, val in zip(var_df[['TotalHospitalizedPatients', 'HomeConfinement',\n                                                  'Recovered', 'Deaths', 'TestsPerformed']], out):\n    print(col, ':', round(val, 2))","58692259":"# Get the lag order\nlag_order = fitted.k_ar\n\n# Input data for forecasting\nforecast_input = df_differenced.values[-lag_order:]\nforecast_input","cbc8065f":"var_df_forecast = var_df[['TotalHospitalizedPatients', 'HomeConfinement',\n                                                  'Recovered', 'Deaths', 'TestsPerformed']]\n\nfc = fitted.forecast(y=forecast_input, steps=n_test)\ndf_forecast = pd.DataFrame(fc, index=var_df_forecast.index[-n_test:], columns=var_df_forecast.columns + '_2d')\ndf_forecast","236dd976":"def invert_transformation(df_train, df_forecast, second_diff=False, third_diff=False):\n    \"\"\"Revert back the differencing to get the forecast to original scale.\"\"\"\n    df_fc = df_forecast.copy()\n    columns = df_train.columns\n    for col in columns:        \n        # Roll back 3rd Diff\n        if third_diff:\n            df_fc[str(col)+'_2d'] = (df_train[col].iloc[-2]-df_train[col].iloc[-3]) + df_fc[str(col)+'_3d'].cumsum()\n        # Roll back 2nd Diff\n        if second_diff:\n            df_fc[str(col)+'_1d'] = (df_train[col].iloc[-1]-df_train[col].iloc[-2]) + df_fc[str(col)+'_2d'].cumsum()\n        # Roll back 1st Diff\n        df_fc[str(col)+'_forecast'] = df_train[col].iloc[-1] + df_fc[str(col)+'_1d'].cumsum()\n    return df_fc","693dc483":"df_results = invert_transformation(df_train, df_forecast, second_diff=True, third_diff=False)        \ndf_results.loc[:, ['TotalHospitalizedPatients_forecast', 'HomeConfinement_forecast',\n                                                  'Recovered_forecast', 'Deaths_forecast', 'TestsPerformed_forecast']]","fd92697f":"df_results","ae732cd4":"df_results['Date'] = var_df['Date'][13:17]\ndf_test.set_index('Date',inplace=True)","e31616cc":"fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(22,6))\n\nfor col, ax in zip(['TotalHospitalizedPatients', 'HomeConfinement',\n                                                  'Recovered', 'Deaths', 'TestsPerformed'], axes):\n\n    df_results.plot(kind='line', y=[col+'_forecast'], x='Date', ax=ax, alpha=0.5, color='r', legend=True).autoscale(axis='x',tight=True)\n    df_test[col][-n_test:].plot(legend=True, ax=ax)\n    ax.set_title(col + \": Forecast vs Actuals\")\nplt.tight_layout();","b3d8a5db":"from statsmodels.tsa.stattools import acf\ndef forecast_accuracy(forecast, actual):\n    mape = np.mean(np.abs(forecast - actual)\/np.abs(actual))  # MAPE\n    me = np.mean(forecast - actual)             # ME\n    mae = np.mean(np.abs(forecast - actual))    # MAE\n    mpe = np.mean((forecast - actual)\/actual)   # MPE\n    rmse = np.mean((forecast - actual)**2)**.5  # RMSE\n    corr = np.corrcoef(forecast, actual)[0,1]   # corr\n    mins = np.amin(np.hstack([forecast[:,None], \n                              actual[:,None]]), axis=1)\n    maxs = np.amax(np.hstack([forecast[:,None], \n                              actual[:,None]]), axis=1)\n    minmax = 1 - np.mean(mins\/maxs)             # minmax\n    return({'mape':mape, 'me':me, 'mae': mae, \n            'mpe': mpe, 'rmse':rmse, 'corr':corr, 'minmax':minmax})","e5e39aea":"print('Forecast Accuracy of: TotalHospitalizedPatients')\naccuracy_prod = forecast_accuracy(df_results['TotalHospitalizedPatients_forecast'].values, df_test['TotalHospitalizedPatients'])\nfor k, v in accuracy_prod.items():\n    print(k, ': ', round(v,4))\n\nprint('\\nForecast Accuracy of: HomeConfinement')\naccuracy_prod = forecast_accuracy(df_results['HomeConfinement_forecast'].values, df_test['HomeConfinement'])\nfor k, v in accuracy_prod.items():\n    print(k, ': ', round(v,4))\n\nprint('\\nForecast Accuracy of: Recovered')\naccuracy_prod = forecast_accuracy(df_results['Recovered_forecast'].values, df_test['Recovered'])\nfor k, v in accuracy_prod.items():\n    print(k, ': ', round(v,4))\n\nprint('\\nForecast Accuracy of: Deaths')\naccuracy_prod = forecast_accuracy(df_results['Deaths_forecast'].values, df_test['Deaths'])\nfor k, v in accuracy_prod.items():\n    print(k, ': ', round(v,4))\n\nprint('\\nForecast Accuracy of: TestsPerformed')\naccuracy_prod = forecast_accuracy(df_results['TestsPerformed_forecast'].values, df_test['TestsPerformed'])\nfor k, v in accuracy_prod.items():\n    print(k, ': ', round(v,4))","26c50f55":"> Vector autoregression (VAR) is a *stochastic process model used to capture the linear interdependencies among multiple time series*. VAR models generalize the univariate autoregressive model (AR model) by allowing for more than one evolving variable. All variables in a VAR enter the model in the same way: *each variable has an equation explaining its evolution based on its own lagged values, the lagged values of the other model variables, and an error term*.\n\nTaken from: [Vector autoregression](https:\/\/en.wikipedia.org\/wiki\/Vector_autoregression)","ed2bcbb5":"Results Visualization","5b461fcb":"**Practical Use**\n\nThe model predicts number a variety of parameters which can be translated into practical use.\nFor example, Predicting the number of patients that will need hospital care, can help the country to be better prepared towards what is expected.\nThe same holds for the prediction of number of tests to be performed, number of patients expected to be entered to home confinment and more.","adeae504":"**Basic EDA**","184c7b29":"**Forecasting**","1ce742c6":"**Related Work and Credits**\n\n[Analysis and Prediction on Coronavirus (Italy)](https:\/\/www.kaggle.com\/vanshjatana\/analysis-and-prediction-on-coronavirus-italy\/data)\n\nLarge parts of code snippets used for VAR modeling were taken from: [Vector Autoregression (VAR) \u2013 Comprehensive Guide with Examples in Python](https:\/\/www.machinelearningplus.com\/time-series\/vector-autoregression-examples-python\/)","f7ada69a":"**Reading Data**","287849b6":"**Imports**","915b53bd":"**VAR**","1ce290f1":"Modeling","5fb6a2ca":"**Introduction**","aa62e71d":"**Considering the length of our data, the results seems to be reasonable (altough not perfect :)).** \n\n**It might be the case that the model predictions will be better, as we get more updated data to feed into the model.** \n\n**In addition, I invite you to use this model (and modify it) in order to make similar predictions to other countries.**","f1be077f":"Train-Test Split","f1a22e04":"**Checking for Causlity**\n\n> Granger causality is a concept of causality derived from the notion that causes may not occur after effects and that *if one variable is the cause of another*, knowing the status on the cause at an earlier point in time can enhance prediction of the effect at a later point in time (Granger, 1969; L\u00fctkepohl, 2005, p. 41)\n\nTaken from: [Vector Autoregressive (VAR) Models and Granger Causality in Time Series Analysis in Nursing Research: Dynamic Changes Among Vital Signs Prior to Cardiorespiratory Instability Events as an Example](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC5161241\/)","85a275bf":"Choosing number of lags to be inserted into the model is a matter of trial and error, and can be changed according to the regression results (above), the durbin-watson test results (will be explained in a moment), and other metrics (e.g., RMSE, MAE, etc.)","82aba2f1":"The test *Null Hypothesis is that the coefficients of the corresponding past values are zero; That is the X does not cause Y*. \nThe P-values in the table are lesser than our significance level (0.05), which implies that the Null Hypothesis can be rejected.","528ba24c":"**Checking for Cointegration**\n\n> Cointegration tests analyze non-stationary time series\u2014 processes that have variances and means that vary over time. In other words, the method allows you to estimate the long-run parameters or equilibrium in systems with unit root variables (Rao, 2007).\n\nTaken from: [Cointegration: Definition, Examples, Tests](https:\/\/www.statisticshowto.datasciencecentral.com\/cointegration\/)\n\nMore information about python implementation and the test results can be found here:\n\n[Test](https:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.tsa.vector_ar.vecm.coint_johansen.html)\n\n[Results](https:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.tsa.vector_ar.vecm.JohansenTestResult.html#statsmodels.tsa.vector_ar.vecm.JohansenTestResult)","0fd4cfd3":"**Checking for Residuals' Autocorrelaotion**\n\nWe'll use Durbin-Watson test for this (denoted as *d*):\n\n> The value of d always lies between 0 and 4. \n> \n> d = 2 indicates no autocorrelation.\n> \n> If d < 2, there is evidence of positive serial correlation. As a rough rule of thumb, if d < 1.0, there may be cause > for alarm. Small values of d indicate successive error terms are positively correlated.\n> \n> If d > 2, successive error terms are negatively correlated. In regressions, this can imply an underestimation of the > level of statistical significance.\n\nTaken from (modified by the author): [Durbin\u2013Watson statistic](https:\/\/en.wikipedia.org\/wiki\/Durbin%E2%80%93Watson_statistic)\n","f21aa977":"As you can see, after 2 series differences, we have 2 stationary columns under significance level of 5%, 1 stationary column under significance level of 0.1%, and 2 non-stationary columns (under plausible significance level).\nThis is not ideal - however, because we're using \"short\" time series, I've decided to go on with only 2 diffrences and not to add more differences.  ","840c072a":"**Settings**","e21f5b1d":"Turning Forecasting into original values","5eac4ed9":"**Unit Root Test (checking for stationaity)**\n\n> In statistics, a unit root test tests whether a time series variable is non-stationary and possesses a unit root. *The null hypothesis is generally defined as the presence of a unit root and the alternative hypothesis is either stationarity*, trend stationarity or explosive root depending on the test used.\n\nTaken from: [Unit root test](https:\/\/en.wikipedia.org\/wiki\/Unit_root_test)"}}