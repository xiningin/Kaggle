{"cell_type":{"7b4581f9":"code","acb865e0":"code","ce62b9c1":"code","38663073":"code","694a1cd8":"code","67c72b19":"code","2d0f11dd":"code","2228d991":"code","65586880":"code","b065a49e":"code","3fbce987":"markdown","03d4f073":"markdown","b66436c2":"markdown","8bdef702":"markdown","76f3c8fd":"markdown","504220eb":"markdown","44501348":"markdown"},"source":{"7b4581f9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","acb865e0":"train = pd.read_csv('..\/input\/X_train.csv')\ntest = pd.read_csv('..\/input\/X_test.csv')\ny = pd.read_csv('..\/input\/y_train.csv')\nprint(train.head())\nprint(train.columns)\nprint(y.head())\nprint(\"Length of Train\", len(train))\nprint(\"Length of Y Labels\", len(y))","ce62b9c1":"# Standardize all Columns that are not ID's or measurement numbers\ncol = train.columns[3:]\nscaler = StandardScaler()\n# scale the columns that contain the data\nnew_df = scaler.fit_transform(train[col])\nnew_df = pd.DataFrame(new_df, columns=col)\n# Add back index\nnew_df[\"series_id\"] = train['series_id']\nnew_df.head()","38663073":"np.unique(y['surface'])","694a1cd8":"y = pd.read_csv('..\/input\/y_train.csv')\ny['surface'].value_counts().plot(kind='bar')","67c72b19":"le = LabelEncoder()\ny = le.fit_transform(y['surface'])\ny","2d0f11dd":"def change1(x):\n    return np.mean(np.abs(np.diff(x)))\n\ndef change2(x):\n    return np.mean(np.diff(np.abs(np.diff(x))))\n\ndef feature_extraction(raw_frame):\n    frame = pd.DataFrame()\n    raw_frame['angular_velocity'] = raw_frame['angular_velocity_X'] + raw_frame['angular_velocity_Y'] + raw_frame['angular_velocity_Z']\n    raw_frame['linear_acceleration'] = raw_frame['linear_acceleration_X'] + raw_frame['linear_acceleration_Y'] + raw_frame['linear_acceleration_Z']\n    raw_frame['velocity_to_acceleration'] = raw_frame['angular_velocity'] \/ raw_frame['linear_acceleration']\n    #raw_frame['acceleration_cumsum'] = raw_frame['linear_acceleration'].cumsum()\n    \n    for col in raw_frame.columns[3:]:\n        frame[col + '_mean'] = raw_frame.groupby(['series_id'])[col].mean()\n        frame[col + '_std'] = raw_frame.groupby(['series_id'])[col].std()\n        frame[col + '_max'] = raw_frame.groupby(['series_id'])[col].max()\n        frame[col + '_min'] = raw_frame.groupby(['series_id'])[col].min()\n        frame[col + '_max_to_min'] = frame[col + '_max'] \/ frame[col + '_min']\n        \n        # Change 1st order\n        frame[col + '_mean_abs_change'] = raw_frame.groupby('series_id')[col].apply(change1)\n        # Change 2nd order\n        #frame[col + '_mean_abs_change2'] = raw_frame.groupby('series_id')[col].apply(change2)\n        frame[col + '_abs_max'] = raw_frame.groupby('series_id')[col].apply(lambda x: np.max(np.abs(x)))\n    return frame\n\ntrain_df = feature_extraction(new_df)\nlen(train_df)","2228d991":"import lightgbm as lgb\nimport time\nnum_folds = 10\ntarget = y\n\nparams = {\n    'num_leaves': 18,\n    'min_data_in_leaf': 40,\n    'objective': 'multiclass',\n    'metric': 'multi_error',\n    'max_depth': 8,\n    'learning_rate': 0.01,\n    \"boosting\": \"gbdt\",\n    \"bagging_freq\": 5,\n    \"bagging_fraction\": 0.812667,\n    \"bagging_seed\": 11,\n    \"verbosity\": -1,\n    'reg_alpha': 0.2,\n    'reg_lambda': 0,\n    \"num_class\": 9,\n    'nthread': -1\n}\n\nt0 = time.time()\ntrain_set = lgb.Dataset(train_df, label=target)\neval_hist = lgb.cv(params, train_set, nfold=10, num_boost_round=9999,\n                   early_stopping_rounds=100, seed=19)\nnum_rounds = len(eval_hist['multi_error-mean'])\n# retrain the model and make predictions for test set\nclf = lgb.train(params, train_set, num_boost_round=num_rounds)\n\nprint(\"Timer: {:.1f}s\".format(time.time() - t0))","65586880":"predictions = clf.predict(train_df, parameters = None)","b065a49e":"y_pred = np.argmax(predictions, axis = 1)\nle.inverse_transform(y_pred)","3fbce987":"# Starter Code For CareerCon Challenge\n\"To fully understand and properly navigate a task, however, they need input about their environment. In this competition, you\u2019ll help robots recognize the floor surface they\u2019re standing on using data collected from Inertial Measurement Units (IMU sensors).\"","03d4f073":"## Encoding Y Labels\nWe see that we have 9 different surface types, and we should encode them so that our models can predict the label. ","b66436c2":"## Standardizing Columns","8bdef702":"## Loading Data\nWe will now load the train, test, and true labels. Notice that we are going to predict for each series_id, a surface label. We have 487680 X training rows and 3810 rows for our labels. ","76f3c8fd":"## Light Gradient Boosting\nWe will now try to classify using Light Gradient Boosting","504220eb":"## Extra Links for Further Exploring!\nPyTorch LSTM: https:\/\/www.kaggle.com\/artgor\/basic-pytorch-lstm\nComplete EDA w\/Model Analysis: https:\/\/www.kaggle.com\/artgor\/where-do-the-robots-drive\nCurrent Best Score: https:\/\/www.kaggle.com\/jesucristo\/1-robots-eda-rf-cv-predictions-0-73","44501348":"## Feature Engineering\nWe have 3810 rows to predict, so we aggregate our time-series data using the groupby function, to make 3810 rows. \n\nTaken from: https:\/\/www.kaggle.com\/jsaguiar\/surface-recognition-baseline"}}