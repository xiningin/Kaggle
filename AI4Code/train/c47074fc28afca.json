{"cell_type":{"b94ed1d1":"code","91191f4f":"code","baab0598":"code","07779596":"code","5cae619d":"code","d21df3ed":"code","683596cd":"code","f4cc5c71":"code","4fbe1191":"code","2bc82dc1":"code","d50608f7":"code","fe9ca7f9":"code","d3dc2a2e":"code","e913cf8a":"code","b8129046":"code","cf5dbd36":"code","20295baf":"code","1c33c578":"code","55d541d7":"code","b0ca56a7":"markdown","14215479":"markdown","88a79840":"markdown","39cee5c9":"markdown","98bc1dff":"markdown","f81d8c87":"markdown","aded64ef":"markdown","8ffb16be":"markdown","adf41b64":"markdown","9d9f841a":"markdown","be1d2fe7":"markdown","0496ba8a":"markdown","f307205e":"markdown","b7359aee":"markdown","5d8a039d":"markdown","457e7021":"markdown","0b10492e":"markdown","a91de40f":"markdown","44f6c7e2":"markdown","55cdad46":"markdown"},"source":{"b94ed1d1":"import numpy as np                                   # linear algebra\nimport pandas as pd                                  # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt                      # library used for plotting data\nfrom sklearn.model_selection import train_test_split # method used for splitting data set into trining and testing sets\nimport warnings                                      # libraries to deal with warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"numpy version:\", np.__version__)\nprint(\"pandas version:\", pd.__version__)","91191f4f":"raw_data_train = pd.read_csv(\"..\/input\/fashionmnist\/fashion-mnist_train.csv\")\nraw_data_test = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')","baab0598":"raw_data_train.head()","07779596":"len(raw_data_train)","5cae619d":"raw_data_train.dtypes.unique()","d21df3ed":"subset_1 = raw_data_train.iloc[:1000,1:]\nplt.subplots(figsize=(10,5))\nplt.hist(subset_1, bins=256, fc='k', ec='k',histtype='step')\nplt.show()","683596cd":"class_names = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\nplt.figure(figsize=(12,9))\nfor i in range(0,12):\n    plt.subplot(3,4,i+1)\n    image_resized = np.resize(raw_data_train.iloc[i,1:].values,(28,28))\n    plt.title(class_names[raw_data_train.iloc[i,0]])\n    plt.imshow(image_resized, cmap='gray', interpolation='none')\n    plt.axis('off')","f4cc5c71":"X = np.array(raw_data_train.iloc[:, 1:])\ny = pd.get_dummies(np.array(raw_data_train.iloc[:, 0]))\n\n# alternative:\n#from keras.utils import to_categorical\n#y = to_categorical(np.array(raw_data_train.iloc[:, 0]))","4fbe1191":"X_train, X_validate, y_train, y_validate = train_test_split(X, y, test_size=0.2, random_state=12)","2bc82dc1":"im_rows, im_cols = 28, 28\ninput_shape = (im_rows, im_cols, 1)\n\n# Test data\nX_test = np.array(raw_data_test.iloc[:, 1:])\ny_test = pd.get_dummies(np.array(raw_data_test.iloc[:, 0]))\n\n# train and validate sets\nX_train = X_train.reshape(X_train.shape[0], im_rows, im_cols, 1)\nX_validate = X_validate.reshape(X_validate.shape[0], im_rows, im_cols, 1)\nX_test = X_test.reshape(X_test.shape[0], im_rows, im_cols, 1)\n\n# normalisation\nX_train = X_train\/255\nX_validate = X_validate\/255\nX_test = X_test\/255\n\nprint(\"X_train shape:\", X_train.shape)\nprint(\"X_validate shape:\", X_validate.shape)\nprint(\"X_test shape:\", X_test.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"y_validate shape:\", y_validate.shape)\nprint(\"y_test shape:\", y_test.shape)","d50608f7":"import keras # main keras package\nfrom keras.models import Sequential # sequential model\nfrom keras.layers import Dropout, Flatten, AveragePooling2D # layers with layers operations\nfrom keras.layers import Dense,Conv2D  # layers types\nfrom keras.layers.normalization import BatchNormalization\n\nprint(\"Keras version:\", keras.__version__)","fe9ca7f9":"num_classes = 10\n\nmodel = Sequential()\n\nmodel.add(Conv2D(6, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\nmodel.add(AveragePooling2D())\n\nmodel.add(Conv2D(16, kernel_size=(3, 3), activation='relu'))\nmodel.add(AveragePooling2D())\n\nmodel.add(Flatten())\n\nmodel.add(Dense(120, activation='relu'))\n\nmodel.add(Dense(84, activation='relu'))\n\nmodel.add(Dense(num_classes, activation = 'softmax'))","d3dc2a2e":"model.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\"])","e913cf8a":"model.summary()","b8129046":"tracker = model.fit(X_train, y_train,\n                    batch_size=512,\n                    epochs=100,\n                    validation_data=(X_validate, y_validate),\n                    verbose=0)","cf5dbd36":"score = model.evaluate(X_test, y_test, verbose=0)","20295baf":"print('Test loss:', score[0])\nprint('Test accuracy:', score[1])","1c33c578":"fig, ax = plt.subplots(figsize = (8,6))\nax.plot(tracker.history[\"loss\"], label = \"training_loss\")\nax.plot(tracker.history[\"val_loss\"], label = \"val_loss\")\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss function\")\nax.legend(loc = 'upper center', shadow = True,)\nplt.show()","55d541d7":"fig, ax = plt.subplots(figsize=(8,6))\nax.plot(tracker.history[\"acc\"], label = \"training_accuracy\")\nax.plot(tracker.history[\"val_acc\"], label = \"val_accuracy\")\nplt.xlabel(\"epochs\")\nplt.ylabel(\"accuracy\")\nax.legend(loc = 'best', shadow = True,)\nplt.show()","b0ca56a7":"## <a id='4'>4. Tracking Learning<\/a>\n\nIt is possible to monitor the learning process of our model by accessing its history with *.history* attribute of previously defined *tracker*. It is worth looking at how the loss function was developing for both testing and validation sets. It is possible to spot if we have already a problem with overfitting - test set loss function will be decreasing while validation's loss function after reaching a minimum value starts to increase again.","14215479":"### <a id='3.5'>3.5 Fitting model to training data<\/a>\n\nAfter the model was checked it is time to fit it to the training data. I will store the fitted model in *tracker* variable to later investigate its history.","88a79840":"## <a id='1'>1. Exploring Data<\/a>","39cee5c9":"### <a id='3.1'>3.1 Importing necessary modules<\/a>","98bc1dff":"### <a id='2.1'>2.1 Changing dataframe to numpy arrays<\/a>\n\nThe images' labels are stored in the first column of the dataframe. These data are categorical so we have to one-hot encode labels: we can do it either by using keras [to_categorical](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/utils\/to_categorical), pandas [get_dummies](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.get_dummies.html) or sklearn [OneHotEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html) functions.","f81d8c87":"### <a id='3.2'>3.2 Defining the CNN architecture - LeNET<\/a>\n\nLeNet architecture does not contain dropouts or max pooling operations which are common in modern architectures. \n\nA **max pooling** layer reduces the number of parameters in the layer by extracting the maximum value from a \"patch\" of a specified size (e.g. 2X2 pixels) and reducing layers size in such a way. There are also option to use the minimum value or the average one.\n\nA **dropout** operation \"kills\" specified percentage of neurons (units) on a given layer to prevent overfitting.\nIn order to prevent overfitting, you can also use a Batch Normalisation but it is not recommended to use dropouts and batch normalisation in a single model. This is so-called \"disharmony\" of dropout and batch normalization.","aded64ef":"<a id='Top'><\/a>\n<center>\n<h1><u>Fashion MNIST - Building CNN in Keras step by step<\/u><\/h1>\n<\/center>\n<br>\n\n<!-- Start of Unsplash Embed Code - Centered (Embed code by @BirdyOz)-->\n<div style=\"width:60%; margin: 20px 20% !important;\">\n    <img src=\"https:\/\/images.unsplash.com\/photo-1512436991641-6745cdb1723f?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=720&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjEyMDd9\" class=\"img-responsive img-fluid img-med\" alt=\"assorted-color clothes lot \" title=\"assorted-color clothes lot \">\n    <div class=\"text-muted\" style=\"opacity: 0.5\">\n        <small><a href=\"https:\/\/unsplash.com\/photos\/R2aodqJn3b8\" target=\"_blank\">Photo<\/a> by <a href=\"https:\/\/unsplash.com\/@theburbgirl\" target=\"_blank\">@theburbgirl<\/a> on <a href=\"https:\/\/unsplash.com\" target=\"_blank\">Unsplash<\/a>, accessed 21\/02\/2020<\/small>\n    <\/div>\n<\/div>\n<!-- End of Unsplash Embed code -->\n                \n\nIn this kernel I will demonstrate how to build a simple Convolutional Neural Network (CNN) step by step using Keras, along with essentials explanations on the way. I hope you will enjoy reading this kernel (and don't forget to upvote it if you like it).\n\n**Introduction to Zalando Fashion MNIST**\n\nZalando is a Germany-based (Berlin) fashion e-commerce company operating in many European countries. Because astandard MNIST dataset id currently a trivial task for most of the state-of-art neural nets Zalando decided to create a bit more challenging set. Detailed information about \"Zalando Fashion MNIST\" initiative you can find on their [official GitHub profile](https:\/\/github.com\/zalandoresearch\/fashion-mnist).\n\n**Why CNN?**\n\nCNN is a class of Neural Networks excelling in the classification of images (and other tasks related to images). It is based on partially connected layers (in contrast to fully-connected) and is currently a state-of-the art algorithm to perform this image-related tasks. Note that it is also technically possible to use fully connected layers but it is sub-optimal approach due to a huge number of parameters it needs to learn.\n\n**Why Keras?**\n\nKeras is user-friendly, modular and high-level neural network APIs. It makes reading code, building models and learning very easy. It works with TensorFlow, CNTK, Theano, MXNet, and PlaidML. More about Keras you can read the official [keras webpage](https:\/\/keras.io\/) or in the [TensorFlow Guide to High-Level APIs](https:\/\/www.tensorflow.org\/guide\/keras).\n\n**CONTENT**\n- <a href='#1'>1. Exploring data<\/a>\n- <a href='#2'>2. Preparing data<\/a>\n     - <a href='#2.1'>2.1 Changing dataframe to Numpy arrays<\/a>\n     - <a href='#2.2'>2.2 Splitting data into train and validation sets<\/a>\n     - <a href='#2.2'>2.3 Reshaping and splitting data for Convolution Neural Network<\/a>\n- <a href='#3'>3. Building CNN<\/a>\n     - <a href='#3.1'>3.1 Importing necessary modules<\/a>\n     - <a href='#3.2'>3.2 Defining the CNN architecture<\/a>\n     - <a href='#3.3'>3.3 Compiling model<\/a>\n     - <a href='#3.4'>3.4 Summarising model<\/a>\n     - <a href='#3.5'>3.5 Fitting model to training data<\/a>\n     - <a href='#3.6'>3.6 Evaluating model with test data<\/a>\n- <a href='#4'>4. Tracking learning<\/a>","8ffb16be":"### <a id='3.4'>3.4 Summarising model<\/a>\n\nIt is worth to summarise the model to see how many trainable parameters our model contains - this has a direct impact on computational power required.","adf41b64":"## <a id='5'>5. References and learning resources<\/a>\n\n1. [Keras documentation](https:\/\/keras.io\/)\n2. [Tensorflow documentation](https:\/\/www.tensorflow.org\/) \n3. [Pytorch documentation](https:\/\/pytorch.org\/)\n4. [CNN theory on www.towarddatascience.com](https:\/\/towardsdatascience.com\/deep-learning-personal-notes-part-1-lesson-3-cnn-theory-convolutional-filters-max-pooling-dbe68114848e)","9d9f841a":"## <a id='2'>2. Preparing Data<\/a>\n\nIn this chapter a basic preparation of data will be conducted.","be1d2fe7":"Loading .csv files into pandas dataframe.","0496ba8a":"### <a id='3.3'>3.3 Compiling model<\/a>\n\nCNN model has now to be compiled. To do so I will use:\n* ADAM (ADAptive Moment estimation) **optimiser** - you can read more about this optimiser, e.g. [here](adaptive moment estimation). \n* My **loss function** (what it is you can read [here](https:\/\/machinelearningmastery.com\/loss-and-loss-functions-for-training-deep-learning-neural-networks\/)) will be *categorical_crossentropy* (other options are explained [here](https:\/\/gombru.github.io\/2018\/05\/23\/cross_entropy_loss\/) - careful math ahead!)\n* **Metric** will be *accuracy*","f307205e":"## <a id='3'>3. Building CNN<\/a>\n\nIn this chapter the CNN model will be built and run.","b7359aee":"### <a id='2.3'>2.3 Reshaping and splitting data for Convolution Neural Network<\/a>\n\n**Reshaping**: We have 28x28 pixels, black and white (1 channel only) pictures. Note that for a colored images we would have 3 channels (RGB).\n\n**Splitting**: To build a well-functioning CNN model you have to follow a procedure of splitting your data into three sets: training, validation and test. In this case, I will create the validation set explicitly but if data are shuffled you can use simply *validation_split* parameter when fitting the model. More about how to create a validation set and why you need it is explained nicely on YouTube [here](https:\/\/www.youtube.com\/watch?v=dzoh8cfnvnI&feature=youtu.be).","5d8a039d":"Desired types of data here are int64 or uint64.","457e7021":"It's worth to look at histogram of pixels distribution. I will do it for a subset of 1000 images using matplotlib histogram function (use parameter *histtype=\"step\"* to reduce the computational time).","0b10492e":"Let's visualise the first nine pictures in the raw training database. As each image is stored in a separate row we have to resize it from the flat array back to 28x28 array. Above each picture a abel will be displayed.\n\n| Class |    Label    | Class |    Label   |\n|:-----:|:-----------:|:-----:|:----------:|\n|   1   | T-shirt\/top |   6   |   Sandal   |\n|   2   |   Trouser   |   7   |    Shirt   |\n|   3   |   Pullover  |   8   |   Sneaker  |\n|   4   |    Dress    |   9   |     Bag    |\n|   5   |     Coat    |   10  | Ankle boot |","a91de40f":"Looking at the head of our database.","44f6c7e2":"### <a id='2.2'>2.2 Splitting data into train and validation sets<\/a>\n\nSklearn [train_test_split](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html) will be used to perform this operation. In this case, it will be more like train-validation split as this validation set will be used during the compilation of the model. The real test set is given to us in a separate .csv file.","55cdad46":"### <a id='3.6'>3.6 Evaluating model with a test data<\/a>\n\nNow the model will be evaluated on test sets."}}