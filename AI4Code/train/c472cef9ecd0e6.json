{"cell_type":{"e1248b67":"code","f3c7103c":"code","a2545a47":"code","ac9248d6":"code","e3bbbee1":"code","2baec284":"code","be3e5ac7":"code","b1fa4a1f":"code","e4809db6":"code","43c952e4":"code","4eecf2f5":"code","b9b18ee2":"code","8ee4faa1":"code","30643ec1":"code","cc3a93cb":"code","04bc0f35":"code","ccc45b1b":"code","60acf891":"code","5edc77c9":"code","93d81906":"code","105ec726":"markdown","c20ddb1d":"markdown","d85e74c8":"markdown","f44b8fa7":"markdown","e6f0bb7c":"markdown","3d2bd48a":"markdown","6ac314c1":"markdown","a95653bb":"markdown"},"source":{"e1248b67":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier","f3c7103c":"df = pd.read_csv('..\/input\/titanic\/train.csv')\n\ndf.head()","a2545a47":"df.info()","ac9248d6":"def preprocess(df):\n    \"\"\"\n        This function is defined to streamline all of the preprocessing steps we will use on the training dataset\n        and later on the testing dataset to make some predictions.\n        \n        In the function, we will perform the following steps:\n        - remove unnecessary columns\n        - convert string features into numeric\n        - fill null values with statistical average values\n    \"\"\"\n    \n    cols_to_drop = ['PassengerId', 'Name', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n    df = df.drop(cols_to_drop, axis = 1)\n    \n    encoder = LabelEncoder()\n    df['Sex'] = encoder.fit_transform(df['Sex'])\n    \n    # Since we have null values only for the Age column, we will replace the null values with the mean\n    df = df.fillna(df['Age'].mean())\n    \n    return df\n    ","e3bbbee1":"df_cleaned = preprocess(df)\n\ndf_cleaned.info()","2baec284":"# Segregate the target and feature columns\n\nX = df_cleaned.drop(['Survived'], axis = 1)\ny = df_cleaned['Survived']\n\nprint(X.shape)\nprint(y.shape)","be3e5ac7":"## Split the data into train and val sets\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3)","b1fa4a1f":"def entropy(col):\n    \"\"\"\n        This function calculates and returns the entropy for a particular column given as the only paramter.\n    \"\"\"\n    \n    # Get each unique class of the column and count for each class to calculate probabilites\n    counts = np.unique(col, return_counts = True)\n    \n    N = float(col.shape[0])\n    \n    entropy = 0.0\n    \n    for idx in counts[1]:\n        prob = idx\/N\n        entropy += -1 * prob * np.log2(prob)\n    \n    return entropy","e4809db6":"## Let's calcuate entropy for the target column in our dataset\n\nprint(entropy(y))","43c952e4":"## We'll create a synthetic data column with just the classes 0s and 1s\n## and see how the entropy of the system changes as we change the distrbution of the classes\n\nimport matplotlib.pyplot as plt\n\narray = np.zeros(10)\nentropies = [entropy(array)]\n\nfor i in range(10):\n    array[i] = 1\n    entropies.append(entropy(array))\n\nplt.figure(figsize = (10, 5))\nplt.plot(range(0,11), entropies)\nplt.xlabel(\"No. of 1s\")\nplt.ylabel(\"Entropy of the system\")\nplt.show()","4eecf2f5":"def split_data(x, fkey, fval):\n    \"\"\"\n        A function to split our dataset based on the key and threshold value\n        to create different systems and calculate information gain\n    \"\"\"\n    \n    x_right = pd.DataFrame([], columns = x.columns)\n    x_left = pd.DataFrame([], columns = x.columns)\n    \n    for idx in range(x.shape[0]):\n        val = x[fkey].loc[idx]\n        \n        if val > fval:\n            x_right = x_right.append(x.loc[idx])\n        else:\n            x_left = x_left.append(x.loc[idx])\n        \n    return x_left, x_right\n","b9b18ee2":"# An example of splitting the first 10 rows according to the Sex feature\nl, r = split_data(df_cleaned[:10], 'Sex', 0.5)\nprint(l)\nprint(r)","8ee4faa1":"def info_gain(x, fkey, fval):\n    \"\"\"\n        This function will calculate the information gain as we split the dataset\n        according to different feature columns to select the most useful features\n    \"\"\"\n    \n    left,right = split_data(x, fkey, fval)\n    \n    # Calculate percentages of samples in left and right\n    l = float(left.shape[0])\/x.shape[0]\n    r = float(right.shape[0])\/x.shape[0]\n    \n    # If all samples are in one side\n    if l == 0.0 or r == 0.0:\n        return -100000 # Min information gain\n    \n    i_gain = entropy(x['Survived']) - (l*entropy(left['Survived']) + r*entropy(right['Survived']))\n    \n    return i_gain","30643ec1":"# Let's calculate the information gain for each column when split by mean values\nfor col in X.columns:\n    print(col, \": \", info_gain(df_cleaned, col, df_cleaned[col].mean()))","cc3a93cb":"class DecisionTree:\n    \n    def __init__(self, depth = 0, max_depth = 5):\n        self.left = None\n        self.right = None        \n        self.fkey = None\n        self.fval = None\n        self.max_depth = max_depth\n        self.depth = depth\n        self.target = None\n        \n    def train(self, X_train):\n        features = X_train.columns\n        info_gains = []\n        \n        for feat in features:\n            i_gain = info_gain(X_train, feat, X_train[col].mean())\n            info_gains.append(i_gain)\n        \n        self.fkey = features[np.argmax(info_gains)]\n        self.fval = X_train[self.fkey].mean()\n        print(\"Current feature: \", self.fkey)\n        \n        # Split the data\n        left, right = split_data(X_train, self.fkey, self.fval)\n        left = left.reset_index(drop = True)\n        right = right.reset_index(drop = True)\n        \n        if left.shape[0] == 0 or right.shape[0] == 0:\n            if X_train['Survived'].mean() >= 0.5:\n                self.target = 'Survived'\n            else:\n                self.target = 'Dead'\n        \n        # Stop early when max depth reached\n        if self.depth >= self.max_depth:\n            if X_train['Survived'].mean() >= 0.5:\n                self.target = 'Survived'\n            else:\n                self.target = 'Dead'\n        \n        self.left = DecisionTree(depth = self.depth+1, max_depth = self.max_depth)\n        self.left.train(left)\n        \n        self.right = DecisionTree(depth = self.depth+1, max_depth = self.max_depth)\n        self.right.train(right)\n        \n        if X_train['Survived'].mean() >= 0.5:\n            self.target = 'Survived'\n        else:\n            self.target = 'Dead'\n        return","04bc0f35":"dtree = DecisionTreeClassifier(criterion='entropy')\ndtree.fit(X_train, y_train)","ccc45b1b":"preds = dtree.predict(X_val)\npreds","60acf891":"dtree.score(X_val, y_val)","5edc77c9":"test_df = pd.read_csv('..\/input\/titanic\/test.csv')\ntest_clean = preprocess(test_df)\n\ntest_predictions = pd.DataFrame(dtree.predict(test_clean), columns = ['Survived'])\n\nsubmission = pd.concat([test_df.PassengerId, test_predictions], axis = 1)\nsubmission","93d81906":"submission.to_csv('submission.csv', index = False)","105ec726":"## Load and preprocess the dataset","c20ddb1d":"## Implementing Decision Trees from Scratch\n\nContinuing with the tradition of coding different machine learning algorithms from scratch, we now move to decision trees. These are considered to be one of the most easily explainable machine learning algorithms as they are, in essence, a bunch of yes\/no question-based trees which derives information from the data present in terms of the target. We'llbe looking at it in a little more detail later.\n\nWe'll be using the Titanic dataset as our problem and try to predict the survivability of the passengers using the given dataset and our algorithm. ","d85e74c8":"## The time has come for Decision Trees","f44b8fa7":"The closer the value is to 1, the more random it is. For systems which have equally distributed classes, the entropy is maximum at 1.","e6f0bb7c":"## Import the libraries","3d2bd48a":"From our formula, we see see that Sex is the most important feature which seems intuitive as there were more female survivors than male since they were the ones evacuated first.","6ac314c1":"## Implementing Entropy and Information Gain\n\nThese are two most fundamental concepts in terms of decision trees' working. \n\n**Entropy** is generally defined as the randomness in the system at any given point. In mathematics, they are given by the formula\n\\begin{equation}\nH(S) = - \\Sigma P(c) * \\log{P(c)}\n\\end{equation} \nwhere P(c) is the probability of the class c in the system S.\n\n**Information Gain** is the amount of randomness lost when changing from one system to another. The difference in the relative entropy between systems is called the information gain. The higher its value, the less random the new state is.\nThe formula for IG is :\n\\begin{equation}\nIG = H(S) - \\Sigma \\frac{|S_v|}{|S|} H(S_v)\n\\end{equation} ","a95653bb":"## Let's get a little real and use Scikit-learn"}}