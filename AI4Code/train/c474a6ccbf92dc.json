{"cell_type":{"7ec10d28":"code","fc803408":"code","fc19af22":"code","703e51bf":"code","80a06a5e":"code","627dc2f0":"code","47c34cde":"code","568fffa7":"code","68851831":"code","849de517":"code","1c15d534":"code","b0cb4660":"code","92466b00":"code","2b7025b2":"code","eb707327":"code","451924f3":"code","a3c42045":"code","71316ef3":"code","4ff95fd4":"code","199893cf":"code","cac5ee86":"code","3d9c3c96":"code","ecb5b9d6":"code","13e3c429":"code","5602260e":"code","ce60585f":"code","659a256b":"code","111ccba8":"code","babd9219":"markdown"},"source":{"7ec10d28":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fc803408":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Input,GlobalMaxPool1D,Dropout\nfrom keras.utils import plot_model\nfrom tensorflow.keras.layers import Input,GlobalMaxPool1D,Dropout,concatenate\nfrom tensorflow.keras.models import Model\nfrom textblob import TextBlob\n\nimport nltk\nimport re\nfrom nltk.corpus import stopwords #corpus is collection of text\n#from nltk.stem.porter import PorterStemmer\nfrom nltk.stem import RSLPStemmer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, f1_score, roc_auc_score\n","fc19af22":"\n#excel_data_df = pandas.read_excel('records.xlsx', sheet_name='Employees')","703e51bf":"# Carregamento dos dados de treinamento - base de dados fake-news traduzida\ntrain = pd.read_excel('..\/input\/train-testxlsx\/train.xlsx')\ntrain.head()","80a06a5e":"train = train.iloc[:200,]\ntrain.shape","627dc2f0":"# Reemo\u00e7\u00e3o dos valores nulos - uso de texto como titulo e titulo como texto.\ntrain['text' ] = train['text' ].replace(np.nan, train['title'])\ntrain['title'] = train['title'].replace(np.nan, train['text' ])\ntrain.isnull().sum()","47c34cde":"# Separando a coluna com o label\nX_train=train.drop('label',axis=1)\ny_train=train['label']","568fffa7":"# Define o tamanho do vocabulario. Sera usado no embeding\nvo_size=500\nmessages=X_train.copy()\nmessages.reset_index(inplace=True)","68851831":"#Convertendo tudo para str\nfor i in range(0, len(messages)):\n    if not (type(messages['text'][i]) is str):\n        print(str(messages['text'][i]))","849de517":"stop_words = stopwords.words('portuguese') #stop words in Portugu\u00eas\nnewStopWords = ['The New York Times', 'de', 'do', 'para', 'no', 'na', 'nos' , 'nas', 'uma', 'um', \n                'umas','uns', 'a', 'Breitbart', 'New York','York', 'Times', 'The', 'New','the', 'The New',\n               ' York times', ' Times', ' times', 'York Times', 'breitbart', 'times', 'Times', ' Breitbart',\n               '- The New York Times','new','york' ]\nstop_words.extend(newStopWords)","1c15d534":"#Pre processamento da base - normaliza\u00e7\u00e3o das palavras com base em seus radicais\n#https:\/\/pythonspot.com\/nltk-stemming\/\n#ps_title =PorterStemmer()\n#ps_text =PorterStemmer()\nps_title =RSLPStemmer() # Removedor de sufixos da lingua portuguesa\nps_text =RSLPStemmer()\ncorpus_title = []\ncorpus_text = []\nfor i in range(0, len(messages)):\n    if not (type(messages['text'][i]) is str):\n        messages['text'][i] = str(messages['text'][i])\n    print(\"Status: %s \/ %s\" %(i, len(messages)), end=\"\\r\")\n    \n    #preproc title\n    review = re.sub('[^a-zA-Z]', ' ',messages['title'][i]) #Filtro de caracteres - Deixndo apenas texto\n    review = review.lower()\n    review = review.split()\n    \n    review = [ps_title.stem(word) for word in review if not word in stop_words]\n    review = ' '.join(review)\n    corpus_title.append(review)\n    \n    #preproc text\n    review = re.sub('[^a-zA-Z]', ' ',messages['text'][i])\n    review = review.lower()\n    review = review.split()\n    review = [ps_text.stem(word) for word in review if not word in stop_words]\n    review = ' '.join(review)\n    corpus_text.append(review)","b0cb4660":"# corpus_text - vetor com strings ['etc alt pal ...','process pul presid ...',...]","92466b00":"# Representa\u00e7\u00e3o One hot - Codifica\u00e7\u00e3o em n\u00fameros\n# Vetor de vetores com numeros - strings tranformadas em vetores - As redes neurais n\u00e3o trabalham com dados categoricos diretamente\nonehot_rep_title = [one_hot(words, vo_size) for words in corpus_title] #vo_size definido anteriormente como o tamanho do vocabulario\nonehot_rep_text = [one_hot(words, vo_size) for words in corpus_text] #one_hot codifica as palavras em vetores numericos preenchidos com zeros com exce\u00e7\u00e3o de um \u00fanico 1","2b7025b2":" # A camada de  Embedding do Keras  requer que todos os documentos tenham o mesmo tamanho.\n# Explica\u00e7\u00e3o do embeding: https:\/\/www.kaggle.com\/rajmehra03\/a-detailed-explanation-of-keras-embedding-layer\n# Uniformiza\u00e7\u00e3o do tamanho - preenchimento de lacunas com o 'padding'- os vetores passam a ter mesmo tamanho\n# Tamanhos definidos abaixo ser\u00e3o usadas no Input da rede neural\nsent_length_title = 20 # Tamanho da linha para o titulo\nsent_length_text = 1000 # Tamanho da linha para o corpo do texto\nembedded_doc_title=pad_sequences(onehot_rep_title, padding='pre', maxlen=sent_length_title)\nembedded_doc_text=pad_sequences(onehot_rep_text, padding='pre', maxlen=sent_length_text)","eb707327":"# Check do formato - y-train \u00e9 uma s\u00e9rie do Pandas\nprint(len(embedded_doc_title),y_train.shape)\nprint(len(embedded_doc_text),y_train.shape)","451924f3":"# Dados concluidos para o processamento pela rede neural - NN\n# Confirma\u00e7\u00e3o do tipo como array Numpy\n#Verifica\u00e7\u00e3o das dimens\u00f5es X e y\nX_final_title=np.array(embedded_doc_title)\nX_final_text=np.array(embedded_doc_text)\ny_final=np.array(y_train)\nprint(X_final_title.shape,y_final.shape)\nprint(X_final_text.shape,y_final.shape)","a3c42045":"# Modelagem\n# Embedding \u00e9 uma t\u00e9cnica usada para representar palavras em documentos como vetores. A representa\u00e7\u00e3o usa numeros reais tal que palavras similares semanticamente s\u00e3o mapeadas uma proxima \u00e0 outra.\n#Thus the embedding layer in Keras can be used when we want to create the embeddings to embed higher dimensional data into lower dimensional vector space.\n# the integer encoding for the word remains same in different docs. eg 'butter' is denoted by 31 in each and every document.\n# the vocab_size is specified large enough so as to ensure unique integer encoding for each and every word.\n# The Keras Embedding layer requires all individual documents to be of same length. Hence we wil pad the shorter documents with 0 for now. \n# Therefore now in Keras Embedding layer the 'input_length' will be equal to the length (ie no of words) of the document with maximum length or maximum number of words.\n# To pad the shorter documents I am using pad_sequences functon from the Keras library.\n\"\"\"\"\" Parametros da camada de embedding ---\n\n'input_dim' = tamanho do vocabulario. Ou seja, o numero de palavras unicas no vocabulario.\n\n'output_dim' = N\u00famero dimens\u00f5es resultante. Cada palavra ser\u00e1 representada por um vetor com estas dimens\u00f5es.\n\n\"\"\"\n\nembedding_vector_feature_title = 10\nembedding_vector_feature_text = 100\n\ninput_title = Input(shape=(sent_length_title,)) # define formato da entrada para o titulo. O tamanho (sent_length_title) foi definido anteriormente.\ninput_text = Input(shape=(sent_length_text,)) # define formato da entrada para o texto\n\nemb_title = Embedding(vo_size,embedding_vector_feature_title)(input_title) # primeiro argumento \u00e9 o tamanho do vocabulario. O segundo \u00e9 o tamanho do vetor do embeding\nlstm_title = LSTM(128, return_sequences=False)(emb_title)# https:\/\/towardsdatascience.com\/understanding-lstm-and-its-quick-implementation-in-keras-for-sentiment-analysis-af410fd85b47\n#LSTM supera outros modelos quando queremos aprendizado baseado em termos distantes no tempo.  A habilidade das LSTM  esquecer, lembrar e atualizar informa\u00e7\u00f5es a coloca \u00e0 frente das RNN.\n# a informa\u00e7\u00e3o de entrada \u00e9 sequencial\n#A camada LSTM transforma a sequencia de vetores em um vetor unico, condesando a informa\u00e7\u00e3o de toda a sequencia.\n\nemb_text = Embedding(vo_size,embedding_vector_feature_text)(input_text)\nlstm_text = LSTM(128, return_sequences=True)(emb_text) \n\n\nmax_pool_text = GlobalMaxPool1D()(lstm_text) # GlobalMaxpooling difere de maxpooling porque pool_length = tamanho da entrada.Promove a redu\u00e7\u00e3o da dimensionalidade para prevenir overfitting e diminuir o custo computacional.\n# In the layer of polling, we use the max-pooling mechanism to reduce the impact of noise that from the output of convolutional layer, at the same time, use it can reduce the feature dimension and prevent model overfitting.\ndropout_1_text = Dropout(0.1)(max_pool_text) # desliga 10# das celulas. Tamb\u00e9m minimiza o overfitting.\ndense_1_text = Dense(50, activation='relu')(dropout_1_text) # celula comum que combina as entradas\ndropout_2_text = Dropout(0.1)(dense_1_text)\n\nout = concatenate([lstm_title,dropout_2_text],axis=-1) # concatena titulo e saida de texto\noutput=Dense(1, activation='sigmoid')(out)\n\nmodel = Model(inputs=[input_title, input_text], outputs=output) # modelo com duas entradas\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # saida binaria.  entropia cruzada \u2013 indica a dist\u00e2ncia entre o que a rede acredita que essa distribui\u00e7\u00e3o deve ser e o que realmente deveria ser. \n# Pode ser mais \u00fatil em problemas nos quais os alvos s\u00e3o 0 e 1.\nprint(model.summary())","71316ef3":"# Get model plot\nplot_model(model, to_file='model_plot4.png', show_shapes=True, show_layer_names=True)","4ff95fd4":"# treinamento - O tamanho do batch define o tamanho do bloco a ser usado no treinamento antes de atualziar os parametros da rede. Blocos menores geram gradientes com comportamento mais erratico.\n# as epocas definem a quantidade de vezes que se passa por toda a base.\nhistory = model.fit(x=[X_final_title,X_final_text], y=y_final, batch_size=128, epochs=10, verbose=1, validation_split=0.2) ","199893cf":"history.history","cac5ee86":"# Carregamento dos dados de teste\ntest = pd.read_excel('..\/input\/train-testxlsx\/test.xlsx')\ntest.head()","3d9c3c96":"test = test.iloc[:100,]","ecb5b9d6":"# Replace NA\ntest['text'] = test['text'].replace(np.nan, test['title'])\ntest['title'] = test['title'].replace(np.nan, test['text'])\ntest.isnull().sum()","13e3c429":"# prepare test data for NN\nX_test=test\nmessages=X_test.copy()\nmessages.reset_index(inplace=True)\n\nps_title =RSLPStemmer()\nps_text =RSLPStemmer()\ncorpus_title = []\ncorpus_text = []\nfor i in range(0, len(messages)):\n    print(\"Status: %s \/ %s\" %(i, len(messages)), end=\"\\r\")\n    \n    #preproc title\n    review = re.sub('[^a-zA-Z]', ' ',messages['title'][i])\n    review = review.lower()\n    review = review.split()\n    \n    review = [ps_title.stem(word) for word in review if not word in stop_words]\n    review = ' '.join(review)\n    corpus_title.append(review)\n    \n    #preproc text\n    review = re.sub('[^a-zA-Z]', ' ',messages['text'][i])\n    review = review.lower()\n    review = review.split()\n    \n    review = [ps_text.stem(word) for word in review if not word in stop_words]\n    review = ' '.join(review)\n    corpus_text.append(review)\n\nonehot_rep_title = [one_hot(words, vo_size) for words in corpus_title]\nonehot_rep_text = [one_hot(words, vo_size) for words in corpus_text]\n\nsent_length_title = 20\nsent_length_text = 1000\nembedded_doc_title=pad_sequences(onehot_rep_title, padding='pre', maxlen=sent_length_title)\nembedded_doc_text=pad_sequences(onehot_rep_text, padding='pre', maxlen=sent_length_text)\n\nX_final_title=np.array(embedded_doc_title)\nX_final_text=np.array(embedded_doc_text)\nprint(X_final_title.shape)\nprint(X_final_text.shape)","5602260e":"# predi\u00e7\u00e3o final\ny_pred_final = model.predict ([X_final_title,X_final_text])\ny_prob = pd.DataFrame(y_pred_final)\ny_prob['0'] = 1 - y_prob[0]\ny_class = pd.DataFrame(y_prob.values.argmax(axis=-1))\ny_class[0] = np.where(y_class[0]==1, 0, 1)\ny_class.head()\nsubmit = pd.concat([test['id'].reset_index(drop=True), y_class], axis=1)\nsubmit.rename(columns={ submit.columns[1]: \"label\" }, inplace = True)\nsubmit.isnull().sum()","ce60585f":"# Salvar o modelo\nmodel.save_weights(\"model_multi.h5\")","659a256b":"# Gr\u00e1fico da acur\u00e1cia\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","111ccba8":"# Gr\u00e1fico da diferen\u00e7a (loss) \nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","babd9219":"\u201cAo se deparar com uma informa\u00e7\u00e3o impactante, os tomadores de decis\u00e3o precisam sempre buscar fontes para os dados e, principalmente, analisar se a not\u00edcia faz sentido dentro de um contexto\u201d"}}