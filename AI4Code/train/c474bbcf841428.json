{"cell_type":{"81a74972":"code","1c9c43ca":"code","0ee7cbe1":"code","408921ab":"code","0e969fe2":"code","a7bb6fe1":"code","f00f9079":"code","f8f6baeb":"code","f239a1b0":"code","0c706f3a":"markdown","fd7b483c":"markdown","936ce122":"markdown","9bc2e59c":"markdown","68966e66":"markdown","3cf3c7b7":"markdown","eed84902":"markdown","dce5cb0e":"markdown","4f49c275":"markdown"},"source":{"81a74972":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.tree import DecisionTreeClassifier as dtc\nfrom sklearn.ensemble import RandomForestClassifier as rfc\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import tree\n\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n","1c9c43ca":"train_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-spring-21-assignment-1\/train.csv\", index_col = 0)\ntest_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-spring-21-assignment-1\/test.csv\", index_col = 0)\n\nprint(train_data.describe())\nprint(train_data.info())\ntrain_data.sample(10)\n","0ee7cbe1":"features = train_data.drop(columns =\"Bankrupt\" )\ntarget = train_data[\"Bankrupt\"]","408921ab":"from scipy import stats\nzscores = np.abs(stats.zscore(features))\nzscores_std = np.where(temp > 3)\nprint(temp2[:])","0e969fe2":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=123)\nfor train_index, test_index in split.split(features, target):\n    train_features,test_features  = features.iloc[train_index], features.iloc[test_index]\n    train_targets, test_targets = target.iloc[train_index],target.iloc[test_index]  \n#print(type(train_index))","a7bb6fe1":"from sklearn.metrics import roc_auc_score, f1_score,accuracy_score\n\ndecision_tree = dtc( criterion = 'entropy')\ndecision_tree = decision_tree.fit(train_features, train_targets)\n\nplt.figure(figsize=(25,200))\ntree.plot_tree(decision_tree, feature_names = features.columns,filled=True )\npredictions = decision_tree.predict(test_features)\n\nprint(\"ROC AUC:\" + str(roc_auc_score(test_targets, predictions)))\nprint(\"F1 Score:\" + str(f1_score(test_targets, predictions)))\nprint(\"Accuracy:\" + str(accuracy_score(test_targets, predictions)))\n\n","f00f9079":"decision_tree = dtc( criterion = 'entropy')\ndecision_tree = decision_tree.fit(train_features, train_targets)\n\nplt.figure(figsize=(25,200))\ntree.plot_tree(decision_tree, feature_names = features.columns,filled=True )\npredictions = decision_tree.predict(test_features)\n\nprint(\"ROC AUC:\" + str(roc_auc_score(test_targets, predictions)))\nprint(\"F1 Score:\" + str(f1_score(test_targets, predictions)))\nprint(\"Accuracy:\" + str(accuracy_score(test_targets, predictions)))","f8f6baeb":"SEED = 1\nrf = rfc(random_state = SEED)\nrnd = rfc(max_depth= 6, max_features= 'sqrt', max_leaf_nodes=15, n_estimators= 100, criterion = 'entropy')\nrnd.fit(train_features,train_targets)\n\npred = rnd.predict_proba(test_data)[:,1]\n\n\nfile = pd.DataFrame({'Bankrupt' : pred})\nfile.to_csv('submission.csv', index = True, index_label = 'id')\nprint(pred)\n\n\n","f239a1b0":"rf_seg = rfc(random_state = SEED)\nrnd_seg = rfc(max_depth= 6, max_features= 'sqrt', max_leaf_nodes=15, n_estimators= 100, criterion = 'entropy')\nrnd_seg.fit(train_features,train_targets)\n\npred_seg = rnd.predict(test_features)\n\nprint(\"ROC AUC:\" + str(roc_auc_score(test_targets, pred_seg)))\nprint(\"F1 Score:\" + str(f1_score(test_targets, pred_seg)))\nprint(\"Accuracy:\" + str(accuracy_score(test_targets, pred_seg)))","0c706f3a":"* Loading Data from the Data Files provided\n* Using .info we can see that the number of rows that ate non-Null match the number of rows of Data","fd7b483c":"* Checking for outliers, after removing these outliers in total, Changing to remove with more than 3 standard deviations away, there was no signifigant change in roc Auc score","936ce122":"* Using Gridsearch I was able to find the best parameters for the model\n\n* results from gridsearch : best parans: {'max_depth': 6, 'max_features': 'sqrt', 'max_leaf_nodes': 15, 'n_estimators': 100}\n\n* printing out the decision tree and best scores\n\n","9bc2e59c":"**Lawrence OConnor on Kaggle**","68966e66":"* creating the best Random Forest tree used for the contest submission ,Built using the entire training set because more data is better\n","3cf3c7b7":"* Splitting the Training Data into training and testing set","eed84902":"* Creating the decision tree using the split test data and calculating requested scores","dce5cb0e":"* Calculating the requested scores using the decision tree with the split test set","4f49c275":"* Drop the bankrupt column to seperate features from target"}}