{"cell_type":{"4b0a6a72":"code","75bd9895":"code","33239380":"code","aab5eea0":"code","577e2cae":"code","1f456779":"code","bf13f99d":"code","0427e022":"code","5aac3259":"code","4a4abb83":"code","afb70c55":"code","6ba59f0f":"code","4002e0ee":"code","2060eb2d":"code","f5032701":"code","39e27b5b":"code","07b41954":"code","fe8554f3":"code","932ebb4a":"code","328a0041":"code","8c3b6daa":"code","9b239a85":"code","c3e28ebe":"code","93e8f136":"code","9240ed23":"code","891193f5":"code","f0f3fc39":"code","0e424d90":"code","870b8247":"code","7863533f":"code","bf0b566d":"code","86c58a30":"code","b88fd2ec":"code","bddfaf93":"code","c4935452":"code","11a44c80":"code","f0c893e4":"code","226f61a1":"code","b652cd61":"code","4af5b89c":"code","f5fc6dd6":"code","42db6179":"code","2194066f":"code","a2263507":"code","2e8d34c3":"code","1b3b5a92":"code","9fcb04bc":"code","6a939416":"code","6a30d24f":"code","ab5865ee":"code","1d770543":"code","048bab01":"code","3f1bfeab":"code","87ec18a9":"code","ec1bd1ff":"code","b664ec59":"code","4c01cf1a":"code","8f8e590a":"code","1d54df25":"code","5827ea99":"code","187014b0":"code","0eca4022":"code","e73bf825":"code","1231e066":"code","eb37ef7d":"code","28d21cd5":"code","7daa1f2d":"code","a0edf4c2":"code","4324e5f4":"code","5579fe2d":"code","0bbd406b":"code","4069c58e":"code","4bd7fd43":"code","2bf81a9a":"code","1c9d5a38":"code","29572bd0":"code","3d4aebe3":"code","a3d6d2cf":"code","13be183e":"code","c87bc6b9":"code","8524bb38":"code","07e1d37f":"code","7355f2af":"code","f90afd76":"code","89642f83":"code","e5364703":"code","6c6e4b51":"code","aa826a44":"code","fd93a091":"code","f3f58fc7":"code","31fcffd7":"code","94f6e4af":"code","6c0a0996":"code","f78cba52":"code","272a3d03":"code","da3cbea7":"markdown","a05632af":"markdown","201eba6f":"markdown","84a22a8f":"markdown","7d69e6b4":"markdown","03581026":"markdown","b189906a":"markdown","80f98c0f":"markdown","2e7efb29":"markdown","08455821":"markdown","05ad7563":"markdown","be0bb1ad":"markdown","3662b7ed":"markdown","1ca6c285":"markdown","6c9113a3":"markdown","2777ddb2":"markdown"},"source":{"4b0a6a72":"import pandas as pd\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab\nimport seaborn as sn\nfrom pandas.plotting import scatter_matrix\n\n%matplotlib inline\n","75bd9895":"df=pd.read_csv('..\/input\/HousePrice.csv')\nprint(df.shape)   #(1460, 81)\nprint(df.describe())","33239380":"#fig1:scatterplot\nfig,ax=plt.subplots()\ndf.plot(kind='scatter',x='GrLivArea',y='SalePrice',ax=ax)\n#alternatively,we can:\n#plt.scatter(df['GrLivArea'],df['SalePrice'])\n#plt.show()","aab5eea0":"\nHouseAge=df['YrSold']-df['YearBuilt']\n#sn.distplot(HouseAge,color='seagreen', kde=False)\nplt.barh(HouseAge,width=df[\"SalePrice\"],color=\"green\")\nplt.title(\"Sale Price vs Age of house\")\nplt.ylabel(\"Age of house\")\nplt.xlabel(\"Sale Price\");\n#We find that ","577e2cae":"plt.xticks(rotation=45) \nPriceperSF=df['SalePrice']\/df['GrLivArea']\nsn.barplot(df[\"Neighborhood\"],PriceperSF)\nplt.title(\"Sale Price per square feet vs Neighborhood\");","1f456779":"#fig3: zoning class vs saleprice\nfig,ax=plt.subplots()\nsn.violinplot(data=df[['MSZoning','SalePrice']],x='MSZoning',y='SalePrice',ax=ax)","bf13f99d":"sn.stripplot(x=\"HeatingQC\", y=\"SalePrice\",data=df,hue='CentralAir',jitter=True,dodge=True)\nplt.title(\"Sale Price vs Heating Quality\");","0427e022":"#fig4\nfig,ax=plt.subplots()\nfig=sn.boxplot(data=pd.concat([df['ExterQual'],df['SalePrice']],axis=1),x='ExterQual',y='SalePrice',ax=ax)","5aac3259":"#fig5: barchart\nfig,ax=plt.subplots()\nsn.barplot(data=df[['CentralAir','SalePrice']],x='CentralAir',y='SalePrice',ax=ax)","4a4abb83":"#fig6: scatterplot matrix\nscatter_matrix(df[['YrSold','SalePrice','TotalBsmtSF','OverallQual', 'GrLivArea']], diagonal='kde', alpha=0.3,figsize=(50,50))","afb70c55":"#correlation matirx\ncorr=df[['OverallQual','OverallCond','GarageCars','YearBuilt','SalePrice','LowQualFinSF','Fireplaces']].corr()\nmask=np.array(corr)\nmask[np.tril_indices_from(mask)] = False\nsn.heatmap(corr, mask=mask,vmax=.8, square=True,annot=True)","6ba59f0f":"#bubble plot\nplt.scatter(df['YearBuilt'],df['SalePrice'],c=df['OverallQual'],s=df['GrLivArea']*0.01,alpha=0.5)\nplt.show()","4002e0ee":"#missingvalue stastistics\ntotal = df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nprint(missing_data.head(20))","2060eb2d":"print(len(df[df['GarageArea']==0])) #81\nprint(len(df[df['TotalBsmtSF']==0])) #37\nprint(len(df[df['PoolArea']==0])) #1453  \nprint(len(df[df['Fireplaces']==0]))#690\n","f5032701":"for i in ['GarageType','GarageFinish','GarageCond','GarageQual']:\n    df[i]=df[i].fillna(\"None\")","39e27b5b":"df['GarageYrBlt']=df['GarageYrBlt'].fillna(0)","07b41954":"for i in ['BsmtExposure', 'BsmtFinType2','BsmtFinType1','BsmtCond', 'BsmtQual']:\n    df[i] = df[i].fillna(\"None\")","fe8554f3":"df['Electrical'] = df['Electrical'].fillna(df['Electrical'].mode()[0])","932ebb4a":"df[\"MasVnrType\"] = df[\"MasVnrType\"].fillna(\"None\")\ndf[\"MasVnrArea\"] = df[\"MasVnrArea\"].fillna(0)","328a0041":"df['FireplaceQu']=df['FireplaceQu'].fillna(\"None\")\ndf['Fence']=df['Fence'].fillna(\"None\")\ndf['MiscFeature'] = df['MiscFeature'].fillna(\"None\")\ndf['Alley'] = df['Alley'].fillna(\"None\")\ndf['PoolQC'] = df['PoolQC'].fillna(\"None\")\ndf['LotFrontage'] =df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\nprint(df.isnull().sum().sort_values(ascending=False).head())","8c3b6daa":"fig,ax=plt.subplots()\ndf.plot(kind='scatter',x='GrLivArea',y='SalePrice',ax=ax)","9b239a85":"#remove abnormal point that has large area but extremely low price\ndf=df[(df['GrLivArea']<4000) | (df['SalePrice']>200000)]\nfig,ax=plt.subplots()\ndf.plot(kind='scatter',x='GrLivArea',y='SalePrice',ax=ax)","c3e28ebe":"fig,ax=plt.subplots()\nfig=sn.boxplot(x='OverallQual',y='SalePrice',data=pd.concat([df['OverallQual'],df['SalePrice']],axis=1),ax=ax)\n#we can leave the above the range's outliers here since we are not sure whether there are any significant factors that affect the sale price","93e8f136":"\n#countSeries=df['MSZoning'].value_counts()\n#second=countSeries[1]\n#print(countSeries[countSeries == second].index[0])\ncolumnlist=list(df)\ncolumnlist.remove('Id')\ndroplist=[]\nfor i in columnlist:\n    count=df[i].value_counts()\n    first=count.values[0]\n    second=count.values[1]\n    r=float(second\/first)\n    if r<0.05:\n        droplist.append(i)\nprint(droplist)\n    ","9240ed23":"df=df.drop(droplist,1)\nprint(df.shape) #55 left","891193f5":"print(df.columns)","f0f3fc39":"data=df[['SalePrice','GrLivArea','GarageCars','RoofStyle','MSZoning','KitchenQual','CentralAir','TotalBsmtSF']]","0e424d90":"data=data.dropna()\nmissing = data.isnull().sum()\nprint(missing)","870b8247":"from scipy.stats import norm\nsn.distplot(data['SalePrice'],fit=norm_hist)\ndata['SalePrice']=np.log1p(data['SalePrice']) #Sale price is skewd to the right","7863533f":"sn.distplot(data['GarageCars'],fit=norm)","bf0b566d":"#import scipy\n#print(data.skew())\n#sn.distplot(data['GrLivArea'],fit=norm)\ndata['LogGrArea']=np.log1p(data['GrLivArea'])\nsn.distplot(data['LogGrArea'],fit=norm)","86c58a30":"data['LogBsmtSF']=np.log1p(data['TotalBsmtSF'])\nsn.distplot(data['LogBsmtSF'],fit=norm)","b88fd2ec":"#data=data.dropna()\n#sn.distplot(data['SalePrice'],fit=norm) #log transform of saleprice","bddfaf93":"print(data.dtypes)","c4935452":"data1=pd.get_dummies(data,drop_first=True)\nprint(data1.dtypes)","11a44c80":"from sklearn.model_selection import train_test_split","f0c893e4":"columnlist=list(data1.columns)\ncolumnlist.remove('SalePrice')\ncolumnlist.remove('GrLivArea')\ncolumnlist.remove('TotalBsmtSF')","226f61a1":"X_train, X_test, y_train, y_test = train_test_split(data1[columnlist],data1['SalePrice'],test_size=0.3, random_state=42)","b652cd61":"print(\"Training set::{}{}\".format(X_train.shape,y_train.shape))\nprint(\"Testing set::{}\".format(X_test.shape))","4af5b89c":"#FULL model\nfrom sklearn import  linear_model\nX_train=X_train.astype(float)\ny_train=pd.DataFrame(y_train)\ny_train = y_train.values.reshape(-1,1)\nlin_reg = linear_model.LinearRegression()\nlin_reg.fit(X_train,y_train)\nprint(lin_reg.coef_)","f5fc6dd6":"import statsmodels.api as sm","42db6179":"X_train2 = sm.add_constant(X_train, prepend=False)\nreg = sm.OLS(y_train,X_train2).fit()\nprint(reg.summary())","2194066f":"#check normality \nimport scipy.stats as stats\nstats.probplot(reg.resid, dist=\"norm\", plot=plt)\nplt.show()","a2263507":"plt.hist(reg.resid,100)","2e8d34c3":"plt.scatter(reg.fittedvalues,reg.resid)\nplt.title(\"Residual vs. fit plot\")\nplt.xlabel(\"fitted value\")\nplt.ylabel(\"residual\")","1b3b5a92":"## Assumption checking - constant variance assumption and mean-zero assumption\nplt.plot(reg.resid, '-')  # solid line\nplt.title(\"Residual vs. Observation number plot\")\nplt.xlabel(\"observation number\")\nplt.ylabel(\"residual\")","9fcb04bc":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n# X_train.shape[1] refers to the column number\nvif[\"features\"] = X_train.columns\nvif.round(1)","6a939416":"import time\ndef processSubset(feature_set):\n    # Fit model on feature_set and calculate AIC\n    X_select = X_train[list(feature_set)]\n    X = sm.add_constant(X_select, prepend=False)\n    model = sm.OLS(y_train,X)\n    regr = model.fit()\n    aic = regr.aic\n    return {\"model\":regr, \"AIC\":aic, \"predict\":feature_set}\ndef getBest(k):\n    \n    tic = time.time()\n    \n    results = []\n    \n    for combo in itertools.combinations(X_train.columns, k):\n        results.append(processSubset(combo))\n    \n    # Wrap everything up in a nice dataframe\n    models = pd.DataFrame(results)\n    \n    # Choose the model with the lowest AIC\n    best_model = models.loc[models['AIC'].argmin()]\n    \n    toc = time.time()\n    print(\"Processed\", models.shape[0], \"models on\", k, \"predictors in\", (toc-tic), \"seconds.\")\n    \n    # Return the best model, along with some other useful information about the model\n    return best_model\ndef forward(predictors):\n    \n    if 'const' in predictors:\n        predictors.remove('const')\n    \n    \n    # Pull out predictors we still need to process\n    remaining_predictors = [p for p in X_train.columns if p not in predictors]\n    \n    tic = time.time()\n    \n    results = []\n    \n    for p in remaining_predictors:\n        results.append(processSubset(predictors+[p]))\n    \n    # Wrap everything up in a nice dataframe\n    models = pd.DataFrame(results)\n    aic=[]\n    name=[]\n    predict=[]\n    for i in range(len(results)):\n        aic.append(results[i]['AIC'])\n        name.append(results[i]['model'])\n        predict.append(results[i]['predict'])\n        \n    \n    models['AIC']=aic\n    models['model']=name\n    models['predict']=predict\n    \n    # Choose the model with the lowest AIC\n    best_model = models.loc[models['AIC'].argmin()]\n    \n    toc = time.time()\n    print(\"Processed \", models.shape[0], \"models on\", len(predictors)+1, \"predictors in\", (toc-tic), \"seconds.\")\n    \n    # Return the best model, along with some other useful information about the model\n    return best_model","6a30d24f":"models_fwd = pd.DataFrame(columns=[\"AIC\", \"model\",\"predict\"])\n\ntic = time.time()\npredictors = []\n\nfor i in range(1,len(X_train.columns)+1):    \n    models_fwd.loc[i] = forward(predictors)\n    predictors = models_fwd.loc[i][\"model\"].model.exog_names\n\ntoc = time.time()\nprint(\"Total elapsed time:\", (toc-tic), \"seconds.\")","ab5865ee":"models_fwd","1d770543":"index = models_fwd['AIC'].argmin()\npredictor = models_fwd.loc[index, 'predict']\nprint(predictor)","048bab01":"## see the model after selection\nX_train3 = X_train[predictor]\nX_train3 = sm.add_constant(X_train3, prepend=False)\nreg_selection = sm.OLS(y_train,X_train3).fit()\nprint(reg_selection.summary())","3f1bfeab":"## For the model selected by stepwise selection, \n## using the k-fold cross validation (specifically 10-fold) to reduce overfitting affects\n# cross_val_predict function returns cross validated prediction values as fitted by the model object.\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nlin_reg_s = linear_model.LinearRegression()\nlin_reg_s = lin_reg_s.fit(X_train3, y_train)\npredicted = cross_val_predict(lin_reg_s, X_train3, y_train, cv=10)","87ec18a9":"## Assumption checking - check of homoscedasticity\n## residual plot\nfig, ax = plt.subplots()\nax.scatter(y_train, y_train-predicted)\nax.axhline(lw=2,color='black')\nax.set_xlabel('Observed')\nax.set_ylabel('Residual')\nplt.show()","ec1bd1ff":"r2_scores = cross_val_score(lin_reg_s, X_train3, y_train, cv=10)\nmse_scores = cross_val_score(lin_reg_s, X_train3, y_train, cv=10,scoring='neg_mean_squared_error')\nprint(\"R-squared::{}\".format(r2_scores))\nprint(\"MSE::{}\".format(mse_scores))","b664ec59":"fig, ax = plt.subplots()\nax.plot([i for i in range(len(r2_scores))],r2_scores,lw=2)\nax.set_xlabel('Iteration')\nax.set_ylabel('R-Squared')\nax.title.set_text(\"Cross Validation Scores, Avg:{}\".format(np.average(r2_scores)))\nplt.show()","4c01cf1a":"## test dataset\nX_test=X_test.astype(float)\nX_test2 = X_test[predictor]\nlin_reg_s_test = linear_model.LinearRegression()\nlin_reg_s_test = lin_reg_s.fit(X_test2, y_test)\ny_pred = lin_reg_s_test.predict(X_test2)\n#y_pred = pd.DataFrame(y_pred)\nresiduals = pd.DataFrame(y_test-y_pred)","8f8e590a":"from sklearn import metrics","1d54df25":"r2_score = lin_reg_s_test.score(X_test2,y_test)\nprint(\"R-squared::{}\".format(r2_score))\nprint(\"MSE: %.2f\" % metrics.mean_squared_error(y_test, y_pred))","5827ea99":"fig, ax = plt.subplots()\nax.scatter(y_test, residuals)\nax.axhline(lw=2,color='black')\nax.set_xlabel('Observed')\nax.set_ylabel('Residuals')\nax.title.set_text(\"Residual Plot with R-Squared={}\".format(np.average(r2_score)))\nplt.show()","187014b0":"print(\"MSE: {}\".format(metrics.mean_squared_error(y_test, y_pred)))","0eca4022":"X_test_vif = X_test2.copy()","e73bf825":"vif2 = pd.DataFrame()\nvif2[\"VIF Factor\"] = [variance_inflation_factor(X_test_vif.values, i) for i in range(X_test_vif.shape[1])]\nvif2[\"features\"] = X_test_vif.columns\nvif2.round(1)","1231e066":"selected =list(X_test_vif.columns)\nselected.remove('LogBsmtSF')\nselected.append('B_A_ratio')\nselected.remove('CentralAir_Y')\nprint(selected)","eb37ef7d":"X_train_1=X_train.copy()\nX_train_1['B_A_ratio']=X_train_1['LogBsmtSF']\/X_train_1['LogGrArea']\nX_train4 = X_train_1[selected]\n#X_train4['Bsmt_Gr_ratio']=X_train4['TotalBsmtSF']\/X_train4['GrLivArea']\nX_train4 = sm.add_constant(X_train4, prepend=False)\nreg_selection = sm.OLS(y_train,X_train4).fit()\nprint(reg_selection.summary())","28d21cd5":"lin_reg_s = linear_model.LinearRegression()\nX_train4=X_train4.drop(['const'],axis=1)\nlin_reg_s = lin_reg_s.fit(X_train4, y_train)\npredicted = cross_val_predict(lin_reg_s, X_train4, y_train, cv=10)","7daa1f2d":"fig, ax = plt.subplots()\nax.scatter(y_train, y_train-predicted)\nax.axhline(lw=2,color='black')\nax.set_xlabel('Observed')\nax.set_ylabel('Residual')\nplt.show()","a0edf4c2":"fig, ax = plt.subplots()\nax.plot([i for i in range(len(r2_scores))],r2_scores,lw=2)\nax.set_xlabel('Iteration')\nax.set_ylabel('R-Squared')\nax.title.set_text(\"Cross Validation Scores, Avg:{}\".format(np.average(r2_scores)))\nplt.show()","4324e5f4":"print(X_train4.shape)","5579fe2d":"## test dataset\n#X_test=X_test.astype(float)\n#X_test['TotalArea']=X_test['TotalBsmtSF']+X_test['GrLivArea']\nX_test_1=X_test.copy()\nX_test_1['B_A_ratio']=X_test_1['LogBsmtSF']\/X_test_1['LogGrArea']\nX_test3 = X_test_1[selected]\n#X_test3 = sm.add_constant(X_test3, prepend=False)\nprint(X_test3.shape)","0bbd406b":"#lin_reg_s_test1= linear_model.LinearRegression()\n#lin_reg_s_test1= lin_reg_s.fit(X_test3, y_test)\ny_pred1 = lin_reg_s.predict(X_test3)\n#y_pred = pd.DataFrame(y_pred)\ny_test=pd.DataFrame(y_test)\ny_test=y_test.values.reshape(-1,1)\nresiduals = pd.DataFrame(y_test-y_pred1)","4069c58e":"#r2_score=metrics.r2_score(y_test,y_pred1)\nr2_score = lin_reg_s.score(X_test3,y_test)\nprint(\"R-squared::{}\".format(r2_score))\nprint(\"MSE: %.2f\" % metrics.mean_squared_error(y_test, y_pred1))","4bd7fd43":"fig, ax = plt.subplots()\nax.scatter(y_test, residuals)\nax.axhline(lw=2,color='black')\nax.set_xlabel('Observed')\nax.set_ylabel('Residuals')\nax.title.set_text(\"Residual Plot with R-Squared={}\".format(np.average(r2_score)))\nplt.show()","2bf81a9a":"print(\"MSE: {}\".format(metrics.mean_squared_error(y_test, y_pred1)))","1c9d5a38":"X_test_vif_1 = X_test3.copy()\nvif3 = pd.DataFrame()\nvif3[\"VIF Factor\"] = [variance_inflation_factor(X_test_vif_1.values, i) for i in range(X_test_vif_1.shape[1])]\nvif3[\"features\"] = X_test_vif_1.columns\nvif3.round(1)","29572bd0":"#coefficient=pd.concat(pd.DataFrame(selected),pd.DataFrame(lin_reg_s.coef_))\ncoef_name=pd.DataFrame(selected)\ncoef_value=pd.DataFrame(lin_reg_s.coef_).transpose()\ncoefficient=pd.concat([coef_name,coef_value],axis=1)\nprint(coefficient)","3d4aebe3":"print(X_test3[['LogGrArea','B_A_ratio','GarageCars','KitchenQual_TA','MSZoning_RM', 'KitchenQual_Fa', 'KitchenQual_Gd', 'RoofStyle_Hip', 'MSZoning_FV', 'MSZoning_RL', 'MSZoning_RH']].corr())","a3d6d2cf":"#use test set for checking model assumption\nstats.probplot(reg_selection.resid, dist=\"norm\", plot=plt)\nplt.show()","13be183e":"plt.hist(reg_selection.resid,100)","c87bc6b9":"plt.scatter(reg_selection.fittedvalues,reg_selection.resid)\nplt.title(\"Residual vs. fit plot\")\nplt.xlabel(\"fitted value\")\nplt.ylabel(\"residual\")","8524bb38":"## Assumption checking - constant variance assumption and mean-zero assumption\nplt.plot(reg_selection.resid, '-')  # solid line\nplt.title(\"Residual vs. Observation number plot\")\nplt.xlabel(\"observation number\")\nplt.ylabel(\"residual\")","07e1d37f":"import graphviz\nfrom sklearn import preprocessing","7355f2af":"cat_attributes=pd.get_dummies(data[['RoofStyle','MSZoning','KitchenQual','CentralAir']])\ndata2=pd.concat([cat_attributes, data[['GrLivArea','GarageCars','TotalBsmtSF']]], axis=1)","f90afd76":"#data['SalePrice']=np.expm1(data['SalePrice'])","89642f83":"X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(data2,data['SalePrice'],test_size=0.3, random_state=42)","e5364703":"from sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor(max_depth=5)\ndtr.fit(X_train_reg,y_train_reg)","6c6e4b51":"dtr.score(X_train_reg,y_train_reg)","aa826a44":"y_pred_reg = dtr.predict(X_test_reg)","fd93a091":"from sklearn.metrics import mean_squared_error\nimport itertools\nprint(\"MSE::{}\".format(mean_squared_error(y_test_reg, y_pred_reg)))","f3f58fc7":"## display the tree\nimport pydotplus\nfrom IPython.display import Image \nfrom sklearn.externals.six import StringIO \nfrom sklearn.tree import export_graphviz","31fcffd7":"y_train_reg=y_train_reg.values.reshape((1020,1))","94f6e4af":"y_train_reg=pd.DataFrame(y_train_reg)","6c0a0996":"y_train_reg.columns =['SalePrice']","f78cba52":"from IPython.display import Image\ndot_data = StringIO()\nexport_graphviz(dtr, out_file=dot_data, feature_names=X_train_reg.columns,\n                         class_names=y_train_reg.columns,  \n                filled=True, rounded=True,\n                special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","272a3d03":"def plot_feature_importances(feature_importances, title, feature_names):\n    # Normalize the importance values \n    feature_importances = 100.0 * (feature_importances \/ max(feature_importances))\n\n    # Sort the values and flip them\n    index_sorted = np.flipud(np.argsort(feature_importances))\n\n    # Arrange the X ticks\n    pos = np.arange(index_sorted.shape[0]) + 0.5\n\n    # Plot the bar graph\n    plt.figure()\n    plt.bar(pos, feature_importances[index_sorted], align='center')\n    plt.xticks(pos, feature_names[index_sorted])\n    plt.xticks(rotation=45) \n    plt.ylabel('Relative Importance')\n    plt.title(title)\n    plt.show()  \nplot_feature_importances(dtr.feature_importances_,'Decision Tree regressor', X_train_reg.columns)\nim=dtr.feature_importances_.reshape(-1,1)\nimportance=pd.DataFrame(data=im,index=X_train_reg.columns)\nimportance.columns=['importance']\nimportance.sort_values(by='importance',inplace=True,ascending=False)\nprint(importance)","da3cbea7":"## Deal with missing values ","a05632af":"### Since the best model using step-wise selection (AIC) has multicollinearity problem, we need to deal with variables whose VIF > 10. Here,we may drop CentralAir_Y, and add interaction terms for LogGrArea and LogBsmtSF: 'B_A_ratio'=LogBsmtSF\/LogGrArea.","201eba6f":"# Step-wise Variable Selection using AIC","84a22a8f":"# Drop low contribution variables by ratios (2nd\/1st frequent) <0.05","7d69e6b4":"## Multiple Linear Regression","03581026":"## Load Dependencies and Configuration Settings","b189906a":"# Add constant term upon full model","80f98c0f":"# Poisson & Negaitve Binomial Model","2e7efb29":"## Load and View the Dataset","08455821":"# Data Cleaning","05ad7563":"# Exploratory data analysis","be0bb1ad":"GrLivArea's VIF >10 is not our concern because it's inevitable that it is colinear with its interaction term \"B_A_ratio\".\n\nMSZoning_RL's VIF >10 is due to the interdependence with other indicator variables e.g.MSZoning_RM, thus, there is no need to delete the whole categorical variable.\n\nTherefore, the model at this point has no multicollinearity problem for explanation purpose.\n","3662b7ed":"# Regression Tree","1ca6c285":"We cannot use Poisson or negative binomial model in our data, since the dependent variable is continuous, and is not a count of any number of times an event occurs. And both the Poisson and negative binomial model are used for modeling count data, which is different from our desired output.","6c9113a3":"# Data transformation","2777ddb2":"# Check outliers "}}