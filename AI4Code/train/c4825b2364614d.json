{"cell_type":{"c1900f9d":"code","68f466d6":"code","5533be78":"code","834f71e5":"code","25fa3b97":"code","7f3370df":"code","83fdcf52":"code","d7e5a2ba":"code","67a1c643":"code","550e01c7":"code","e61a3e65":"code","43f176f1":"code","807ba406":"code","58c2669f":"code","8779c5b3":"code","1ad6e2d8":"code","4dbd9cb6":"code","8b7a839d":"code","acce386f":"code","b763cb7b":"code","1fa6e10b":"code","df374164":"code","9fe45b11":"code","123565c8":"code","807292c4":"code","c527224c":"code","d55d2457":"code","3fd335a3":"code","0a08c5ed":"code","b822da8e":"code","290aa2e7":"code","c5d3d9af":"code","c3e9a455":"code","fb53b212":"code","3d6f099d":"code","b5949572":"code","ea2bfd24":"code","eb7829e5":"code","ac7b3113":"code","f2455362":"code","605c938b":"code","dd538b1c":"code","f9f3c183":"code","f3307a3c":"code","bc1cdf8c":"code","f29a1ab8":"code","b6e2f437":"code","2cec5ff5":"code","f24b1e05":"code","55c4c195":"code","17532a67":"code","61fb950f":"code","3f9b081c":"code","0b019a3a":"code","17bdf77c":"code","77e64ee1":"code","ee0c44c9":"code","68329614":"code","e14c614c":"code","d010f03a":"code","0ce5fdd4":"code","548c9a93":"code","a6cf0ece":"code","bec25cff":"code","91a721a3":"code","203aaf70":"code","8cd3b160":"code","57b2e2df":"code","877e3746":"code","bb3ff698":"code","8e7c4561":"code","8f5ec97e":"code","b5370140":"code","1e66fb1c":"markdown","845ce3a3":"markdown","5668c63b":"markdown","040459be":"markdown","6a09452c":"markdown","b9721d44":"markdown","5a2be8a6":"markdown","fb2e54ac":"markdown","645febde":"markdown","6aaa30b2":"markdown","90df935c":"markdown","a5149d2d":"markdown","d163d999":"markdown","d828b6e6":"markdown","3903238d":"markdown","cc8294d4":"markdown","2c02185e":"markdown","4daaed90":"markdown","aa96ffa3":"markdown","eead8ec8":"markdown","9a30d926":"markdown","350b526c":"markdown","9d814f81":"markdown","d004e02f":"markdown","7b2411ec":"markdown","5516a940":"markdown","c6ee1453":"markdown","9b265f40":"markdown","548219a5":"markdown","a8785480":"markdown","0c62d983":"markdown","d3cbb242":"markdown","b4c050f8":"markdown","b7585168":"markdown","9abd1e0a":"markdown","71284e2e":"markdown"},"source":{"c1900f9d":"# Standard imports\nimport numpy as np\nimport pandas as pd\n\n# Import text stuffs\nimport re\nimport string\n\n# Import visualisation tools\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Import NLTK stuffs\nfrom nltk.tokenize import word_tokenize, TreebankWordTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nimport nltk\nnltk.download('stopwords')\n\n# Import Vectorizers\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n# Import Models and Metrics\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors.nearest_centroid import NearestCentroid\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC","68f466d6":"# Importing database\ntrain_original = pd.read_csv(\"..\/input\/train.csv\")\ntest_original = pd.read_csv(\"..\/input\/test.csv\")","5533be78":"train = train_original.copy()\ntest = test_original.copy()\nfull = pd.concat([train,test], ignore_index=True, sort=False)","834f71e5":"# Having a look at the data\ntrain_original.head()","25fa3b97":"full.head()","7f3370df":"full.tail()","83fdcf52":"# Checking for data types and info \nfull.info()","d7e5a2ba":"# Checking Shapes\ntrainshape = train_original.shape\ntestshape = test_original.shape\nfullshape = full.shape\nprint(trainshape)\nprint(testshape)\nprint(fullshape)","67a1c643":"# Checking for nulls\ntrain_original.isnull().any()","550e01c7":"# Checking for personality types\ntrain_original['type'].unique()","e61a3e65":"# Checking the distibution of personality types\ntrain_original['type'].value_counts()","43f176f1":"# Having a look at one of the posts\ntrain_original.iloc[1163 ,1]","807ba406":"# Creating a data frame for us to play around with safely\nEDAdf = train_original.copy()","58c2669f":"EDAdf['post_length']=EDAdf['posts'].apply(lambda x :len(x.split('|||')))","8779c5b3":"len(EDAdf.iloc[0,1].split('|||'))","1ad6e2d8":"EDAdf[EDAdf['post_length']>50].head()","4dbd9cb6":"pattern = \"[\\\\|\\\\|\\\\|]+\"\nsplit_post = re.split(pattern, train_original.iloc[1163 ,1])\nsplit_post","8b7a839d":"EDAdf[EDAdf['post_length']>50].head()","acce386f":"# create new column with revised posts text\nEDAdf['posts_r'] = EDAdf['posts'].copy()\n\n# replaces post separators with empty space\nEDAdf['posts_r'] = EDAdf['posts_r'].apply(lambda x: x.replace('|||', ' '))\n\n\n# replace hyperlinks with 'URL'\nEDAdf['posts_r'] = EDAdf['posts_r'].apply(lambda x: re.sub\\\n                                                  (r'\\bhttps?:\\\/\\\/.*?[\\r\\n]*? ', 'URL ', x, flags=re.MULTILINE))\n\nEDAdf.head()","b763cb7b":"# Add columns for the total number of words (across 50 posts), and average words per post\nEDAdf['total_words'] = EDAdf['posts_r'].apply(lambda x: len(re.findall(r'\\w+', x)))\nEDAdf['avg_words_per_post'] = EDAdf['total_words'] \/ 50\nEDAdf['total_ellipsis'] = EDAdf['posts_r'].apply(lambda x: len(re.findall(\"[\/.\/.\/.]+\", x)))\nEDAdf['avg_ellipsis_per_post'] = EDAdf['total_ellipsis'] \/ 50\nEDAdf.head()","1fa6e10b":"# Average length of post overall\nEDAdf['avg_words_per_post'].mean()","df374164":"# Average length of post, by 16 types\nEDAdf['avg_words_per_post'].groupby(EDAdf['type']).mean()","9fe45b11":"split_post","123565c8":"train = train_original.copy()\ntest = test_original.copy()\nfull = pd.concat([train, test], ignore_index=True)\ntrain_visual = train_original.copy()","807292c4":"train_original['type'].value_counts()","c527224c":"fig, ax = plt.subplots(figsize= (15.0, 4.0))\nsns.set_palette(sns.color_palette(\"GnBu_d\", 16))\nsns.countplot(x = \"type\", data = train_visual, order=[\"INTJ\", \"INTP\", \"ENTJ\", \"ENTP\", \n                                               \"INFJ\", \"INFP\", \"ENFJ\", \"ENFP\", \n                                               \"ISTJ\", \"ISFJ\", \"ESTJ\", \"ESFJ\", \n                                               \"ISTP\", \"ISFP\", \"ESTP\", \"ESFP\"])\nplt.title(\"Distribution of poster's personality type\")\nplt.plot","d55d2457":"plt.figure(figsize=(15,10))\nsns.violinplot(x='type', y=EDAdf['posts'].apply(lambda x: len(x.split())\/50), data=EDAdf, inner=None, color='lightgray')\nsns.stripplot(x='type', y=EDAdf['posts'].apply(lambda x: len(x.split())\/50), data=EDAdf, size=4, jitter=True)","3fd335a3":"# Creating mapping for the different criterion\ntrain_visual[\"Mind\"] = train_visual[\"type\"].map(lambda x: 'Introversion' if x[0] == 'I' else 'Extroversion')\ntrain_visual[\"Energy\"] = train_visual[\"type\"].map(lambda x: 'Intuition' if x[1] == 'N' else 'Sensing')\ntrain_visual[\"Nature\"] = train_visual[\"type\"].map(lambda x: 'Thinking' if x[2] == 'T' else 'Feeling')\ntrain_visual[\"Tactics\"] = train_visual[\"type\"].map(lambda x: 'Judging' if x[3] == 'J' else 'Perceiving')","0a08c5ed":"train_visual.head()","b822da8e":"# Explore the counts for each axis of the types\nprint('Introversion (I) \u2013 Extroversion (E)', '\\n', train_visual['Mind'].value_counts(), '\\n')\nprint('Intuition (N) \u2013 Sensing (S)', '\\n', train_visual['Energy'].value_counts(), '\\n')\nprint('Thinking (T) \u2013 Feeling (F)', '\\n', train_visual['Nature'].value_counts(), '\\n')\nprint('Judging (J) \u2013 Perceiving (P)', '\\n', train_visual['Tactics'].value_counts(), '\\n')","290aa2e7":"sns.set_palette(sns.color_palette(\"GnBu_d\", 2))\nsns.countplot(x=\"Mind\", data=train_visual, order=[\"Introversion\", \"Extroversion\"])\nplt.ylim(0,6000)\nplt.title(\"Mind Distribution\")","c5d3d9af":"sns.set_palette(sns.color_palette(\"BuGn_r\", 2))\nsns.countplot(x=\"Energy\", data=train_visual, order=[\"Intuition\", \"Sensing\"])\nplt.ylim(0,6000)\nplt.title(\"Energy Distribution\")","c3e9a455":"sns.set_palette(sns.light_palette(\"purple\", reverse=True))\nsns.countplot(x=\"Nature\", data=train_visual, order=[\"Thinking\", \"Feeling\"])\nplt.ylim(0,6000)\nplt.title(\"Nature Distribution\")","fb53b212":"sns.set_palette(sns.color_palette(\"RdBu\"))\nsns.countplot(x=\"Tactics\", data=train_visual, order=[\"Judging\", \"Perceiving\"])\nplt.ylim(0,6000)\nplt.title(\"Tactics Distribution\")","3d6f099d":"# Splitting posts\nfull['posts'] = full['posts'].apply(lambda x: x.split('|||'))\nfull['posts'] = full['posts'].apply(lambda x: ' '.join(x))","b5949572":"# Mapping binary according to competition standard\nfull['mind'] = full[full['id'].isnull() == True]['type'].apply(lambda s: s[0])\nfull['mind'] = full[full['id'].isnull() == True]['mind'].map({'I':0,'E':1})\n\nfull['energy'] = full[full['id'].isnull() == True]['type'].apply(lambda s: s[1])\nfull['energy'] = full[full['id'].isnull() == True]['energy'].map({'S':0,'N':1})\n\nfull['nature'] = full[full['id'].isnull() == True]['type'].apply(lambda s: s[2])\nfull['nature'] = full[full['id'].isnull() == True]['nature'].map({'F':0,'T':1})\n\nfull['tactics'] = full[full['id'].isnull() == True]['type'].apply(lambda s: s[3])\nfull['tactics'] = full[full['id'].isnull() == True]['tactics'].map({'P':0,'J':1})","ea2bfd24":"# Removing the website addresses and replacing it with URL\nurl_pattern = r'https?:\\\/\\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&\/\/=]*)'\nurl_replace = r'URL'\nfull['posts'] = full['posts'].replace(to_replace = url_pattern, value = url_replace, regex = True)","eb7829e5":"# Making all words lower case\nfull['posts'] = full['posts'].str.lower()","ac7b3113":"# Removing punctuation function. Code from EDSA train\ndef remove_punctuation(post):\n    return ''.join([letter for letter in post if letter not in string.punctuation])\n\nfull['posts'] = full['posts'].apply(remove_punctuation)","f2455362":"# Tokenizing the posts\nfull['tokened'] = full['posts'].apply(TreebankWordTokenizer().tokenize)","605c938b":"# Stemming the posts\nstemmer = SnowballStemmer('english')\n\ndef stemmerizer(words, stemmer):\n    return [stemmer.stem(word) for word in words]\n\nfull['stemmed'] = full['tokened'].apply(stemmerizer, args=(stemmer, ))","dd538b1c":"# Removing stop words\ndef remove_stop_words(tokens):    \n    return [t for t in tokens if t not in stopwords.words('english')]\nfull['stopped'] = full['stemmed'].apply(remove_stop_words)","f9f3c183":"# Cleaned posts\nfull['clean'] = full['stopped'].apply(lambda x: ' '.join(x))","f3307a3c":"# Using a vectorizer on the data\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(full[full['id'].isnull() == True]['clean'])\ntestX = vectorizer.transform(full[full['id'].isnull() == False]['clean'])","bc1cdf8c":"print('Train shape is:', X.shape)\nprint('Test shape is:', testX.shape)","f29a1ab8":"# Train Test Split\ny = full[full['id'].isnull() == True]['type'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\nprint('X_train ', X_train.shape, '\\n', 'X_test ', X_test.shape, '\\n', 'y_train ', y_train.shape, '\\n', \n      'y_test', y_test.shape)","b6e2f437":"# Fit and score a Logistic Regression\n\nlgr = LogisticRegression()\nlgr.fit(X_train, y_train)\nprint(\"TRAINING SET\")\nprint(\"Accuracy: \", lgr.score(X_train, y_train))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_train, lgr.predict(X_train)))\nprint(\"Classification Report:\")\nprint(classification_report(y_train, lgr.predict(X_train)))\nprint(\"\")\n\nprint(\"TEST SET\")\nprint(\"Accuracy: \", lgr.score(X_test, y_test))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, lgr.predict(X_test)))\nprint(\"Classification Report:\")\nprint(classification_report(y_test, lgr.predict(X_test)))","2cec5ff5":"# Convert confusion matrix to a dataframe to prepare it for heatmapping\ncm_lgr = np.array(confusion_matrix(y_test, lgr.predict(X_test)))\n\ncm_logreg = pd.DataFrame(cm_lgr, index=['ENFJ', 'ENFP', 'ENTJ', 'ENTP', 'ESFJ', 'ESFP',\n                                       'ESTJ', 'ESTP', 'INFJ', 'INFP', 'INTJ', 'INTP',\n                                       'ISFJ', 'ISFP', 'ISTJ', 'ISTP'], \n                            columns=['predict_ENFJ','predict_ENFP','predict_ENTJ',\n                                     'predict_ENTP','predict_ESFJ','predict_ESFP',\n                                     'predict_ESTJ','predict_ESTP','predict_INFJ',\n                                     'predict_INFP','predict_INTJ','predict_INTP',\n                                     'predict_ISFJ','predict_ISFP','predict_ISTJ',\n                                     'predict_ISTP'])\ncm_logreg","f24b1e05":"# Create confusion matrix heatmap of Logistic Regression model \nfig, ax = plt.subplots(figsize=(14,10)) \nplt.title('Confusion Matrix for Logistic Regression', fontsize=16,\n          fontweight='bold', y=1.02)\nsns.heatmap(cm_logreg, robust=True, annot=True, linewidth=0.5, \n            fmt='', cmap='RdBu_r', vmax=303, ax=ax)\nplt.xticks(fontsize=12)\nplt.yticks(rotation=0, fontsize=12);","55c4c195":"# Fit and score a Random Forest Classifier\n# Parameters: 30 estimators, min 50 samples per leaf node, out-of-bag samples to estimate generalization accuracy\nrfc = RandomForestClassifier(n_estimators=30, min_samples_leaf=50, oob_score=True, n_jobs= -1, random_state=123)\nrfc.fit(X_train, y_train)\nprint(\"TRAINING SET\")\nprint(\"Accuracy: \", rfc.score(X_train, y_train))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_train, rfc.predict(X_train)))\nprint(\"Classification Report:\")\nprint(classification_report(y_train, rfc.predict(X_train)))\nprint(\"\")\n\nprint(\"TEST SET\")\nprint(\"Accuracy: \", rfc.score(X_test, y_test))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, rfc.predict(X_test)))\nprint(\"Classification Report:\")\nprint(classification_report(y_test, rfc.predict(X_test)))","17532a67":"# Convert confusion matrix to a dataframe to prepare it for heatmapping\ncm_rfc = np.array(confusion_matrix(y_test, rfc.predict(X_test)))\n\ncm_randomforest = pd.DataFrame(cm_rfc, index=['ENFJ', 'ENFP', 'ENTJ', 'ENTP', 'ESFJ', 'ESFP',\n                                       'ESTJ', 'ESTP', 'INFJ', 'INFP', 'INTJ', 'INTP',\n                                       'ISFJ', 'ISFP', 'ISTJ', 'ISTP'], \n                            columns=['predict_ENFJ','predict_ENFP','predict_ENTJ',\n                                     'predict_ENTP','predict_ESFJ','predict_ESFP',\n                                     'predict_ESTJ','predict_ESTP','predict_INFJ',\n                                     'predict_INFP','predict_INTJ','predict_INTP',\n                                     'predict_ISFJ','predict_ISFP','predict_ISTJ',\n                                     'predict_ISTP'])\ncm_randomforest","61fb950f":"# Create confusion matrix heatmap of Random Forest Classifier model \nfig, ax = plt.subplots(figsize=(14,10)) \nplt.title('Confusion Matrix for Random Forest Classifier', fontsize=16,\n          fontweight='bold', y=1.02)\nsns.heatmap(cm_randomforest, robust=True, annot=True, linewidth=0.5, \n            fmt='', cmap='RdBu_r', vmax=303, ax=ax)\nplt.xticks(fontsize=12)\nplt.yticks(rotation=0, fontsize=12);","3f9b081c":"# Fit and score a Nearest Centroid Classifier\nnc = NearestCentroid()\nnc.fit(X_train, y_train)\nprint(\"TRAINING SET\")\nprint(\"Accuracy: \", nc.score(X_train, y_train))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_train, nc.predict(X_train)))\nprint(\"Classification Report:\")\nprint(classification_report(y_train, nc.predict(X_train)))\nprint(\"\")\n\nprint(\"TEST SET\")\nprint(\"Accuracy: \", nc.score(X_test, y_test))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, nc.predict(X_test)))\nprint(\"Classification Report:\")\nprint(classification_report(y_test, nc.predict(X_test)))","0b019a3a":"# Convert confusion matrix to a dataframe to prepare it for heatmapping\ncm_nc = np.array(confusion_matrix(y_test, nc.predict(X_test)))\n\ncm_nearestcentroid = pd.DataFrame(cm_nc, index=['ENFJ', 'ENFP', 'ENTJ', 'ENTP', 'ESFJ', 'ESFP',\n                                       'ESTJ', 'ESTP', 'INFJ', 'INFP', 'INTJ', 'INTP',\n                                       'ISFJ', 'ISFP', 'ISTJ', 'ISTP'], \n                            columns=['predict_ENFJ','predict_ENFP','predict_ENTJ',\n                                     'predict_ENTP','predict_ESFJ','predict_ESFP',\n                                     'predict_ESTJ','predict_ESTP','predict_INFJ',\n                                     'predict_INFP','predict_INTJ','predict_INTP',\n                                     'predict_ISFJ','predict_ISFP','predict_ISTJ',\n                                     'predict_ISTP'])\ncm_nearestcentroid","17bdf77c":"# Create confusion matrix heatmap of Nearest Centroid model \nfig, ax = plt.subplots(figsize=(14,10)) \nplt.title('Confusion Matrix for Nearest Centroid Classifier', fontsize=16,\n          fontweight='bold', y=1.02)\nsns.heatmap(cm_nearestcentroid, robust=True, annot=True, linewidth=0.5, \n            fmt='', cmap='RdBu_r', vmax=303, ax=ax)\nplt.xticks(fontsize=12)\nplt.yticks(rotation=0, fontsize=12);","77e64ee1":"# Fit and score a Gaussian Naive Bayes\ngnb = GaussianNB()\ngnb.fit(X_train.toarray(), y_train)\nprint(\"TRAINING SET\")\nprint(\"Accuracy: \", gnb.score(X_train.toarray(), y_train))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_train, gnb.predict(X_train.toarray())))\nprint(\"Classification Report:\")\nprint(classification_report(y_train, gnb.predict(X_train.toarray())))\nprint(\"\")\n\nprint(\"TEST SET\")\nprint(\"Accuracy: \", gnb.score(X_test.toarray(), y_test))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, gnb.predict(X_test.toarray())))\nprint(\"Classification Report:\")\nprint(classification_report(y_test, gnb.predict(X_test.toarray())))","ee0c44c9":"# Convert confusion matrix to a dataframe to prepare it for heatmapping\ncm_gnb = np.array(confusion_matrix(y_test, gnb.predict(X_test.toarray())))\n\ncm_naivebayes = pd.DataFrame(cm_gnb, index=['ENFJ', 'ENFP', 'ENTJ', 'ENTP', 'ESFJ', 'ESFP',\n                                       'ESTJ', 'ESTP', 'INFJ', 'INFP', 'INTJ', 'INTP',\n                                       'ISFJ', 'ISFP', 'ISTJ', 'ISTP'], \n                             columns=['predict_ENFJ','predict_ENFP','predict_ENTJ',\n                                     'predict_ENTP','predict_ESFJ','predict_ESFP',\n                                     'predict_ESTJ','predict_ESTP','predict_INFJ',\n                                     'predict_INFP','predict_INTJ','predict_INTP',\n                                     'predict_ISFJ','predict_ISFP','predict_ISTJ',\n                                     'predict_ISTP'])\ncm_naivebayes","68329614":"# Create confusion matrix heatmap of Random Forest Classifier model \nfig, ax = plt.subplots(figsize=(14,10)) \nplt.title('Confusion Matrix for Gaussian Naive Bayes', fontsize=16,\n          fontweight='bold', y=1.02)\nsns.heatmap(cm_naivebayes, robust=True, annot=True, linewidth=0.5, \n            fmt='', cmap='RdBu_r', vmax=303, ax=ax)\nplt.xticks(fontsize=12)\nplt.yticks(rotation=0, fontsize=12);","e14c614c":"# Train-test splits, using type variables as target and posts variable as predictor\nyIE = full[full['id'].isnull() == True]['mind'].values\nyNS = full[full['id'].isnull() == True]['energy'].values\nyTF = full[full['id'].isnull() == True]['nature'].values\nyJP = full[full['id'].isnull() == True]['tactics'].values\n\n# Introversion - Extroversion\nX_train_IE, X_test_IE, y_train_IE, y_test_IE = train_test_split(X, yIE, test_size=0.2, random_state = 42)\n# Intuition - Sensing\nX_train_NS, X_test_NS, y_train_NS, y_test_NS = train_test_split(X, yNS, test_size=0.2, random_state = 42)\n# Thinking - Feeling\nX_train_TF, X_test_TF, y_train_TF, y_test_TF = train_test_split(X, yTF, test_size=0.2, random_state = 42)\n# Judging - Perceiving\nX_train_JP, X_test_JP, y_train_JP, y_test_JP = train_test_split(X, yJP, test_size=0.2, random_state = 42)","d010f03a":"# Hyperparemeter Tuning for Random Forest Classifier using GridSearchCV\nrfc = RandomForestClassifier(random_state = 42)\nparam_grid = { \n    'n_estimators': [10, 20, 50, 85],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy'],\n    'class_weight' : ['balanced', 'balanced_subsample']\n}\n\nrfcclf = GridSearchCV(rfc, param_grid, cv=5)\ngridrfcm = rfcclf.fit(X_train_IE, y_train_IE)\nprint('Best Mind n_estimators:', gridrfcm.best_estimator_.get_params()['n_estimators'])\nprint('Best Mind max_features:', gridrfcm.best_estimator_.get_params()['max_features'])\nprint('Best Mind max_depth:', gridrfcm.best_estimator_.get_params()['max_depth'])\nprint('Best Mind criterion:', gridrfcm.best_estimator_.get_params()['criterion'])\nprint('Best Mind class_weight:', gridrfcm.best_estimator_.get_params()['class_weight'])\nprint(\" \")\ngridrfce = rfcclf.fit(X_train_NS, y_train_NS)\nprint('Best Energy n_estimators:', gridrfce.best_estimator_.get_params()['n_estimators'])\nprint('Best Energy max_features:', gridrfce.best_estimator_.get_params()['max_features'])\nprint('Best Energy max_depth:', gridrfce.best_estimator_.get_params()['max_depth'])\nprint('Best Energy criterion:', gridrfce.best_estimator_.get_params()['criterion'])\nprint('Best Energy class_weight:', gridrfce.best_estimator_.get_params()['class_weight'])\nprint(\" \")\ngridrfcn = rfcclf.fit(X_train_TF, y_train_TF)\nprint('Best Nature n_estimators:', gridrfcn.best_estimator_.get_params()['n_estimators'])\nprint('Best Nature max_features:', gridrfcn.best_estimator_.get_params()['max_features'])\nprint('Best Nature max_depth:', gridrfcn.best_estimator_.get_params()['max_depth'])\nprint('Best Nature criterion:', gridrfcn.best_estimator_.get_params()['criterion'])\nprint('Best Nature class_weight:', gridrfcn.best_estimator_.get_params()['class_weight'])\nprint(\" \")\ngridrfct = rfcclf.fit(X_train_JP, y_train_JP)\nprint('Best Tactics n_estimators:', gridrfct.best_estimator_.get_params()['n_estimators'])\nprint('Best Tactics max_features:', gridrfct.best_estimator_.get_params()['max_features'])\nprint('Best Tactics max_depth:', gridrfct.best_estimator_.get_params()['max_depth'])\nprint('Best Tactics criterion:', gridrfct.best_estimator_.get_params()['criterion'])\nprint('Best Tactics class_weight:', gridrfct.best_estimator_.get_params()['class_weight'])","0ce5fdd4":"# Fit and score a Random Forest Classifier using the parameters identified by the grid search\nrfc = RandomForestClassifier(n_estimators = 85, max_features = 'auto', max_depth = 8, \n                             criterion = 'gini', class_weight = 'balanced', random_state = 42)\nrfc.fit(X_train_IE, y_train_IE)\nprint(\"TRAINING SET\")\nprint(\"Accuracy: \", rfc.score(X_train_IE, y_train_IE))\nprint(\"Confusion Matrix (counts):\")\nprint(confusion_matrix(y_train_IE, rfc.predict(X_train_IE)))\nprint(\"Confusion Matrix (percentages):\")\nprint(pd.DataFrame(confusion_matrix(y_train_IE, rfc.predict(X_train_IE))).apply(lambda x: x\/x.sum(),axis=1))\nprint(\"Classification Report:\")\nprint(classification_report(y_train_IE, rfc.predict(X_train_IE)))\nprint(\"\")\n\nprint(\"TEST SET\")\nprint(\"Accuracy: \", rfc.score(X_test_IE, y_test_IE))\nprint(\"Confusion Matrix (counts):\")\nprint(confusion_matrix(y_test_IE, rfc.predict(X_test_IE)))\nprint(\"Confusion Matrix (percentages):\")\nprint(pd.DataFrame(confusion_matrix(y_test_IE, rfc.predict(X_test_IE))).apply(lambda x: x\/x.sum(),axis=1))\nprint(\"Classification Report:\")\nprint(classification_report(y_test_IE, rfc.predict(X_test_IE)))","548c9a93":"# Convert confusion matrix to a dataframe to prepare it for heatmapping\ncm_rfc_IE = np.array(confusion_matrix(y_test_IE, rfc.predict(X_test_IE)))\n\ncm_randomforest_IE = pd.DataFrame(cm_rfc_IE, index=['Extroversion', 'Introversion'],\n                                 columns=['predict_Extroversion', 'predict_Introversion'])\\\n.apply(lambda x: x\/x.sum(),axis=1)\n\n# Create confusion matrix heatmap of Random Forest Classifier model \nfig, ax = plt.subplots(figsize=(6,4)) \nplt.title('Confusion Matrix for Random Forest Classifier: Introversion - Extroversion Axis', fontsize=16,\n          fontweight='bold', y=1.02)\nsns.heatmap(cm_randomforest_IE, robust=True, annot=True, linewidth=0.5, \n            fmt='', cmap='RdBu_r', vmax=1, ax=ax)\nplt.xticks(fontsize=12)\nplt.yticks(rotation=0, fontsize=12);","a6cf0ece":"# Fit and score a Random Forest Classifier using the parameters identified by the grid search\nrfc = RandomForestClassifier(n_estimators = 85, max_features = 'auto', max_depth = 8, \n                             criterion = 'gini', class_weight = 'balanced_subsample', random_state = 42)\nrfc.fit(X_train_NS, y_train_NS)\nprint(\"TRAINING SET\")\nprint(\"Accuracy: \", rfc.score(X_train_NS, y_train_NS))\nprint(\"Confusion Matrix (counts):\")\nprint(confusion_matrix(y_train_NS, rfc.predict(X_train_NS)))\nprint(\"Confusion Matrix (percentages):\")\nprint(pd.DataFrame(confusion_matrix(y_train_NS, rfc.predict(X_train_NS))).apply(lambda x: x\/x.sum(),axis=1))\nprint(\"Classification Report:\")\nprint(classification_report(y_train_NS, rfc.predict(X_train_NS)))\nprint(\"\")\n\nprint(\"TEST SET\")\nprint(\"Accuracy: \", rfc.score(X_test_NS, y_test_NS))\nprint(\"Confusion Matrix (counts):\")\nprint(confusion_matrix(y_test_NS, rfc.predict(X_test_NS)))\nprint(\"Confusion Matrix (percentages):\")\nprint(pd.DataFrame(confusion_matrix(y_test_NS, rfc.predict(X_test_NS))).apply(lambda x: x\/x.sum(),axis=1))\nprint(\"Classification Report:\")\nprint(classification_report(y_test_NS, rfc.predict(X_test_NS)))","bec25cff":"# Convert confusion matrix to a dataframe to prepare it for heatmapping\ncm_rfc_NS = np.array(confusion_matrix(y_test_NS, rfc.predict(X_test_NS)))\n\ncm_randomforest_NS = pd.DataFrame(cm_rfc_NS, index=['Intuision', 'Sensing'],\n                                 columns=['predict_Intuition', 'predict_Sensing'])\\\n.apply(lambda x: x\/x.sum(),axis=1)\n\n# Create confusion matrix heatmap of Random Forest Classifier model \nfig, ax = plt.subplots(figsize=(6,4)) \nplt.title('Confusion Matrix for Random Forest Classifier: Intuition - Sensing Axis', fontsize=16,\n          fontweight='bold', y=1.02)\nsns.heatmap(cm_randomforest_NS, robust=True, annot=True, linewidth=0.5, \n            fmt='', cmap='RdBu_r', vmax=1, ax=ax)\nplt.xticks(fontsize=12)\nplt.yticks(rotation=0, fontsize=12);","91a721a3":"# Fit and score a Random Forest Classifier using the parameters identified by the grid search\nrfc = RandomForestClassifier(n_estimators = 85, max_features = 'auto', max_depth = 8, \n                             criterion = 'gini', class_weight = 'balanced', random_state = 42)\nrfc.fit(X_train_TF, y_train_TF)\nprint(\"TRAINING SET\")\nprint(\"Accuracy: \", rfc.score(X_train_TF, y_train_TF))\nprint(\"Confusion Matrix (counts):\")\nprint(confusion_matrix(y_train_TF, rfc.predict(X_train_TF)))\nprint(\"Confusion Matrix (percentages):\")\nprint(pd.DataFrame(confusion_matrix(y_train_TF, rfc.predict(X_train_TF))).apply(lambda x: x\/x.sum(),axis=1))\nprint(\"Classification Report:\")\nprint(classification_report(y_train_TF, rfc.predict(X_train_TF)))\nprint(\"\")\n\nprint(\"TEST SET\")\nprint(\"Accuracy: \", rfc.score(X_test_TF, y_test_TF))\nprint(\"Confusion Matrix (counts):\")\nprint(confusion_matrix(y_test_TF, rfc.predict(X_test_TF)))\nprint(\"Confusion Matrix (percentages):\")\nprint(pd.DataFrame(confusion_matrix(y_test_TF, rfc.predict(X_test_TF))).apply(lambda x: x\/x.sum(),axis=1))\nprint(\"Classification Report:\")\nprint(classification_report(y_test_TF, rfc.predict(X_test_TF)))","203aaf70":"# Convert confusion matrix to a dataframe to prepare it for heatmapping\ncm_rfc_TF = np.array(confusion_matrix(y_test_TF, rfc.predict(X_test_TF)))\n\ncm_randomforest_TF = pd.DataFrame(cm_rfc_TF, index=['Thinking', 'Feeling'],\n                                 columns=['predict_Thinking', 'predict_Feeling'])\\\n.apply(lambda x: x\/x.sum(),axis=1)\n\n# Create confusion matrix heatmap of Random Forest Classifier model \nfig, ax = plt.subplots(figsize=(6,4)) \nplt.title('Confusion Matrix for Random Forest Classifier: Feeling - Thinking Axis', fontsize=16,\n          fontweight='bold', y=1.02)\nsns.heatmap(cm_randomforest_TF, robust=True, annot=True, linewidth=0.5, \n            fmt='', cmap='RdBu_r', vmax=1, ax=ax)\nplt.xticks(fontsize=12)\nplt.yticks(rotation=0, fontsize=12);","8cd3b160":"# Fit and score a Random Forest Classifier using the parameters identified by the grid search\nrfc = RandomForestClassifier(n_estimators = 85, max_features = 'auto', max_depth = 8, \n                             criterion = 'gini', class_weight = 'balanced', random_state = 42)\nrfc.fit(X_train_JP, y_train_JP)\nprint(\"TRAINING SET\")\nprint(\"Accuracy: \", rfc.score(X_train_JP, y_train_JP))\nprint(\"Confusion Matrix (counts):\")\nprint(confusion_matrix(y_train_JP, rfc.predict(X_train_JP)))\nprint(\"Confusion Matrix (percentages):\")\nprint(pd.DataFrame(confusion_matrix(y_train_JP, rfc.predict(X_train_JP))).apply(lambda x: x\/x.sum(),axis=1))\nprint(\"Classification Report:\")\nprint(classification_report(y_train_JP, rfc.predict(X_train_JP)))\nprint(\"\")\n\nprint(\"TEST SET\")\nprint(\"Accuracy: \", rfc.score(X_test_JP, y_test_JP))\nprint(\"Confusion Matrix (counts):\")\nprint(confusion_matrix(y_test_JP, rfc.predict(X_test_JP)))\nprint(\"Confusion Matrix (percentages):\")\nprint(pd.DataFrame(confusion_matrix(y_test_JP, rfc.predict(X_test_JP))).apply(lambda x: x\/x.sum(),axis=1))\nprint(\"Classification Report:\")\nprint(classification_report(y_test_JP, rfc.predict(X_test_JP)))","57b2e2df":"# Convert confusion matrix to a dataframe to prepare it for heatmapping\ncm_rfc_JP = np.array(confusion_matrix(y_test_JP, rfc.predict(X_test_JP)))\n\ncm_randomforest_JP = pd.DataFrame(cm_rfc_JP, index=['Judging', 'Perceiving'],\n                                 columns=['predict_Judging', 'predict_Perceiving'])\\\n.apply(lambda x: x\/x.sum(),axis=1)\n\n# Create confusion matrix heatmap of Random Forest Classifier model \nfig, ax = plt.subplots(figsize=(6,4)) \nplt.title('Confusion Matrix for Random Forest Classifier: Perceiving - Judging Axis', fontsize=16,\n          fontweight='bold', y=1.02)\nsns.heatmap(cm_randomforest_JP, robust=True, annot=True, linewidth=0.5, \n            fmt='', cmap='RdBu_r', vmax=1, ax=ax)\nplt.xticks(fontsize=12)\nplt.yticks(rotation=0, fontsize=12);","877e3746":"# Hyperparemeter Tuning for Logistic Regression\nlgr = LogisticRegression()\nlogisticpararemeters = {'C': (np.logspace(0, 4, 10)),\n                        'penalty': ('l1','l2')}\nlgrclf = GridSearchCV(lgr, logisticpararemeters, cv=5)\ngridlgrm = lgrclf.fit(X_train_IE, y_train_IE)\nprint('Best Mind Penalty:', gridlgrm.best_estimator_.get_params()['penalty'])\nprint('Best Mind C:', gridlgrm.best_estimator_.get_params()['C'])\ngridlgre = lgrclf.fit(X_train_NS, y_train_NS)\nprint('Best Energy Penalty:', gridlgre.best_estimator_.get_params()['penalty'])\nprint('Best Energy C:', gridlgre.best_estimator_.get_params()['C'])\ngridlgrn = lgrclf.fit(X_train_TF, y_train_TF)\nprint('Best Nature Penalty:', gridlgrn.best_estimator_.get_params()['penalty'])\nprint('Best Nature C:', gridlgrn.best_estimator_.get_params()['C'])\ngridlgrt = lgrclf.fit(X_train_JP, y_train_JP)\nprint('Best Tactics Penalty:', gridlgrt.best_estimator_.get_params()['penalty'])\nprint('Best Tactics C:', gridlgrt.best_estimator_.get_params()['C'])","bb3ff698":"# Final Logistic Model Best Score\nlgrm = LogisticRegression(C = 2.7825594022071245, penalty = 'l1')\nlgre = LogisticRegression(C = 2.7825594022071245, penalty = 'l1')\nlgrn = LogisticRegression(C = 1.0, penalty = 'l2')\nlgrt = LogisticRegression(C = 2.7825594022071245, penalty = 'l1')","8e7c4561":"# Fitting a Logistical Regression Model\nlgrm.fit(X_train_IE, y_train_IE)\nlgre.fit(X_train_NS, y_train_NS)\nlgrn.fit(X_train_TF, y_train_TF)\nlgrt.fit(X_train_JP, y_train_JP)","8f5ec97e":"# Making Prediction on test\nmindg = lgrm.predict(testX)\nenergyg = lgre.predict(testX)\nnatureg = lgrn.predict(testX)\ntacticsg = lgrt.predict(testX)","b5370140":"# Creating submission file\ntest_id = test['id']\nsubmission = pd.DataFrame({'id': test_id, 'mind': mindg, 'energy': energyg,\n                    'nature':natureg, 'tactics':tacticsg })\nsubmission.to_csv('submission.csv', index = False)","1e66fb1c":"Model: NearestCentroid","845ce3a3":"# Transforming the Data","5668c63b":"Removing the individual website addresses and replacing them all with just a URL because in our initial exploration of preprocessing doing anything with the urls like scrapping the domain names, or using keywords from YouTube videos and plugging them into the posts results in the most minimal amount of improvement and takes far too many resources. If you want a csv where we actually scraped the everything can be found [here](mailto:shabatjie@gmail.com). Thanks to Rob Fairon, and Joanne Moonsamy for the help with YouTube's API and the actually processing of all of this.","040459be":"#### Mind","6a09452c":"We're going to make a seperate dataframe so we can examine the data a little bit more and try to get a bit of insight and see where we can focus our efforts on getting the best accuracy we can","b9721d44":"# Visualizing","5a2be8a6":"Cleaned posts are without urls, punctuation, stop words which are the common words that generally create noise, and the are stemmed","fb2e54ac":"# Imports","645febde":"Making sure that there are actually 50 posts per user and an error was not made we checked to see if there were more than 50 posts after splitting it by the seperator that was used by the engineer that created the data source. Annoyingingly it looks like it there seems to be more than 50 posts by some users ","6aaa30b2":"# Modeling","90df935c":"# Predicting MBTI Personality Types from Forum Posts\n\n### Classification Sprint\n\nGroup 1 - EDSA JHB","a5149d2d":"It appears that all 16 personalities are represented. Though the distrubution is not balanced at all","d163d999":"# Preprocessing","d828b6e6":"# Conclusion\n\nWhile multiclass classification doesn't perform too badly it's quite clear that it's far better to classify the data based on each axis. Logistic Regression is definititely in its element here as it is very good at binary classification. Random Forest wasn't too bad either but as soon as you lift the estimators to something over a hundred you might as well go on vacation because even when you get back it won't be done.\n\n\nIt was interesting to note that the less you do and the simpler you keep thing the far better your sanity and the results are. Pulling YouTube keywords was an absolute pain and wasn't worth the effort. You also don't get nearly enough units through Google's API which meant it wasn't a simple mission. The computer also crashed multiple times handling that text. \n\n\nWhoever created this data set needs some serious talking to though because the choice of seperator is genuinely very poor. I found that classification of text and NLP\/Machine Learning is definitely not an exact science. There a lot that goes into that varies from dataset to dataset and sometimes no logic is the way to success.\n\n\nHappily done with all this and the final submission score on the Kaggle competition not using predict_proba was 5.01837. CSV for that will be attached with submission of this notebook. Thankfully this is now all over. It was horrid.","3903238d":"Having a look at the amount of words per comment further would probably give me some better insight into the personality types. Unfortunately due to a lack of resources at this time, mainly time, we can't go further into that. The amount of ellipsis per post would have been an interesting option to go into as well. However having a look at that random post above you can clearly see that each post was that ended with an ellipsis (...) was mainly due to the amount of characters per post being capped when the data was sourced. It would then appear that word per comment would also then be non valuable information as we can not be certain as to how long a post actually was a quite a bit of them were cut off.","cc8294d4":"# Exploratory Data Analysis","2c02185e":"## Overview\n\n#### Description:\n\nIn this challenge, we are required to build and train a model (or many models) capable of predicting a person's MBTI label using only what they posted in an online forum.\n\nThe challenge will require the use of Natural Language Processing to convert the data into machine learning format. This data will then be used to train a classifier capable of assigning MBTI labels to a person's online forum posts.\n\nRead more about the MBTI personality types [here](https:\/\/www.16personalities.com\/personality-types) OR, better yet, [take the test for yourself!](https:\/\/www.16personalities.com\/free-personality-test)\n\nEach MBTI personality type consists of four binary variables, they are: \n- **Mind**: Introverted (I) or Extraverted (E) \n- **Energy**: Sensing (S) or Intuitive (N) \n- **Nature**: Feeling (F) or Thinking (T) \n- **Tactics**: Perceiving (P) or Judging (J)\n\nEach person will have only one of the two categories for each variable above. Combining the four variables gives the final personality type. For example, a person who is Extraverted, Intuitive, Thinking and Judging will get the **ENTJ** personality type.\n\nWe will need to build and train a model that is capable of predicting labels for each of the four MBTI variables - i.e. predict four separate labels for each person which, when combined, results in that person's personality type.\n\n\n\n#### About MBTI:\n\nAccording to Carl G. Jung's theory of psychological types, people can be characterized by their preference of general attitude:\n\n- **Extraverted (E) vs. Introverted (I)**,\ntheir preference of one of the two functions of perception:\n\n- **Sensing (S) vs. Intuition (N)**,\nand their preference of one of the two functions of judging:\n\n- **Thinking (T) vs. Feeling (F)**\nThe three areas of preferences introduced by Jung are dichotomies (i.e. bipolar dimensions where each pole represents a different preference). Jung also proposed that in a person one of the four functions above is dominant \u2013 either a function of perception or a function of judging. Isabel Briggs Myers, a researcher and practitioner of Jung\u2019s theory, proposed to see the judging-perceiving relationship as a fourth dichotomy influencing personality type:\n\n- **Judging (J) vs. Perceiving (P)**\n\n\nThe first criterion, **Extraversion \u2013 Introversion (Mind)**, signifies the source and direction of a person\u2019s energy expression. An extravert\u2019s source and direction of energy expression is mainly in the external world, while an introvert has a source of energy mainly in their own internal world.\n\nThe second criterion, **Sensing \u2013 Intuition (Energy)**, represents the method by which someone perceives information. Sensing means that a person mainly believes information he or she receives directly from the external world. Intuition means that a person believes mainly information he or she receives from the internal or imaginative world.\n\nThe third criterion, **Thinking \u2013 Feeling (Nature)**, represents how a person processes information. Thinking means that a person makes a decision mainly through logic. Feeling means that, as a rule, he or she makes a decision based on emotion, i.e. based on what they feel they should do.\n\nThe fourth criterion, **Judging \u2013 Perceiving (Tactics)**, reflects how a person implements the information he or she has processed. Judging means that a person organizes all of his life events and, as a rule, sticks to his plans. Perceiving means that he or she is inclined to improvise and explore alternative options.\n\nAll possible permutations of preferences in the 4 dichotomies above yield 16 different combinations, or personality types, representing which of the two poles in each of the four dichotomies dominates in a person, thus defining 16 different personality types. Each personality type can be assigned a 4 letter acronym of the corresponding combination of preferences.\n\nSource can be found [here](http:\/\/www.humanmetrics.com\/personality\/type)\n\n\n#### Data:\n\n- The training data for this competition consists of 6505 rows, with two columns. Each row corresponds to one user from the Personality Cafe website forums. The posts column represents a user's posts, which include URLs, emojis and plain text. The type column contains the MBTI personality type for each user.\n\n- The testing data for this competition consists of 2169 rows, with two columns. Each row corresponds to one user from the Personality Cafe website forums. The posts column represents a user's posts, which include URLs, emojis and plain text. The id column contains an id for each user.\n\n- You can find the get the data from the competition page [here](https:\/\/www.kaggle.com\/c\/edsa-mbti\/data)\n\n\n\n#### Notes:\n\n*A lot* of inspiration for the flow and few of the the visualisations were from a wonderful [notebook](https:\/\/github.com\/myarolin\/Predicting_Personality_Type\/blob\/master\/Notebook_Personality_type.ipynb) on the MBTI dataset created by Maria Yarolin. Thank you for that ;)\n\n*Exploratory Data Analysis*: The data was seperated poorly which meant you couldn't analyse as well\n\n*Multiclass Classification*: Ran Logistic Regression, Random Forest Classifier, Nearest Centroid Classifier and Naive Bayes. Naive Bayes was slow and not worth it. Logistic Regression perfomed well. Random Forest basically only classified one type and Nearest Centroid didn't do as fell as I thought\n\n*Binary Classification*: Used this for submission. Only ran Logistic Regression and Random Forest. SVM took 4 and a half hours and wasn't worth it in the slightest and was thrown in the bin immediately\n\n*Conclusion*\nSee below for detail, basically Binary was better and used for submission. Don't use SVM unless you have a super computer and like disappointment","4daaed90":"#### Final Model and Submission","aa96ffa3":"CountVectorizer just counts the word frequencies. Simple as that.\n\nWith the TFIDF Vectorizer the value increases proportionally to count, but is offset by the frequency of the word in the corpus. - This is the IDF (inverse document frequency part). Because of this we chose to use the TFIDF Vectorizer to create more meaning between the words.","eead8ec8":"#### Multiclass Classification","9a30d926":"Creating some new colums so we can try get some insight into the posts themselves","350b526c":"Model: Random Forest Classifier","9d814f81":"Snowball Stemmer was chosen as opposed to Porter stemmer or Lancaster as we needed something that was quick and efficient and balanced in aggressiveness. Snowball Stemmer is also known as Porter2 which is just an improved version of the Porter Stemmer and one which the creater of the original even noted as being better than his. We chose not to do Lemmentization as we saw that it generally lost too much of the original words","d004e02f":"You can clearly see the bias towards Introversion and Intuition on this forums. It's heavily skewed towards that. We're now going to visualise this","7b2411ec":"Codes for the new columns:\n- I-E: Introversion (0) - Extroversion (1)\n- N-S: Intuition (1) - Sensing (0)\n- T-F: Thinking (1) - Feeling (0)\n- J-P: Judging (1) - Perceiving (0)\n\n*The weird mapping is because someone who designed the challenge mapped it weird. Yes I know... lovely*","5516a940":"#### Tactics","c6ee1453":"Model: Logistic Regression","9b265f40":"**Data Dictionary** \n\nVariables in original dataframes:\n- *type*: (string) four-letter Myers-Briggs Type Index (MBTI) code\n- *posts*: (string) text of up to fifty most recent posts to the *PersonalityCafe* forum\n- *id*: (float) unique number assigned to a user for competition testing purposes\n\nNew variables added for analysis in the *full*, *EDAdf* or *train_visual* datafile:\n- *I-E* or *Mind*: (string) code on Introversion-Extroversion axis, derived from *type*\n- *N-S* or *Energy*: (string) code on Intuition-Sensing axis, derived from *type*\n- *T-F* or *Nature*: (string) code on Thinking-Feeling axis, derived from *type*\n- *J-P* or *Tactics*: (string) code on Judging-Perceiving axis, derived from *type*\n- *tokened*: (string) tokenized post text, derived from *posts*\n- *stemmed*: (string) stemmed post text, derived from *tokened*\n- *stopped*: (string) post text with stop words removed, derived from *stemmed*\n- *clean*: (string) cleaned post text, derived from *stopped*\n- *posts_r*: (string) cleaned post text, derived from *posts*\n- *total_words*: (integer) number of total words across up to 50 posts, derived from *posts*\n- *avg_words_per_post*: (float) average number of words per post, derived from *posts* and *total words*","548219a5":"Model: Gaussian Naive Bayes","a8785480":"#### Energy","0c62d983":"#### Binary Classification","d3cbb242":"---","b4c050f8":"Thanks to  again Maria Yarolin for the code for these Confusion Matrixes","b7585168":"Hyperparameter Tuning for Random Forest Classifier for this portion of binary classification","9abd1e0a":"#### Nature","71284e2e":"We decided to split by a more accurate splitter with a Regular Expression. But after some closer inspection we could clearly see why the splitting was never going to work as accurately as hoped. The 3 pipe (|||) seperator chosen by the data source creater wasn't unique as some users had posts with multiple pipes in it. We also noticed that some users had less than the 50 posts or so. This isn't a train smash as the number doesn't really affect us much it's just a little bit of an annoyance and doesn't give us the best insight that we could gain. Nevertheless we're going to go by that initial assumption that each user has 50 posts each and have a closer look at word count and other stuff to see if that could help improve the score"}}