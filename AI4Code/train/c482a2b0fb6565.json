{"cell_type":{"6d9e2b9c":"code","6b8a7054":"code","e8634278":"code","9300ea98":"code","1e688524":"code","4a107e9b":"code","6c025f32":"code","ce111ee9":"code","f259f695":"code","a62342eb":"code","7f0c0e0e":"code","2eea5eb0":"markdown","75c388b2":"markdown","0b6b3382":"markdown","faced1bf":"markdown","30b559d5":"markdown","54a4e038":"markdown","1dc75c68":"markdown","24c33bb2":"markdown","76e72891":"markdown","2d11a91a":"markdown","d2905f86":"markdown","2805c96e":"markdown"},"source":{"6d9e2b9c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(\"Importing File: {} \".format(os.path.join(dirname, filename)))\n\n\n# Import Model Libraries \nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n\n# Import the data\nX = pd.read_csv(\"..\/input\/titanic\/train.csv\")\nX_test_full = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ngender_submissions = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\n\nprint(\"Setup Complete\")\n\n\n","6b8a7054":"survivors = X[X['Survived'] == True]\ndeceased = X[X['Survived'] == False]\n\nplt.figure(figsize=(10, 10))\n\nsns.relplot(x='Fare', y='Age', hue='Survived', data= X)\n\nplt.title('Titanic passengers: attributes')\nplt.xlabel('Fare ($)')\n\nplt.show()","e8634278":"plt.figure(figsize=(10, 10))\n\n\nsns.kdeplot(survivors.Fare.rename(\"Survivor's Fare\"))\nsns.kdeplot(deceased.Fare.rename(\"Deceaced's Fare\"))\n\nplt.title('Titanic passengers: Survivor Fare Density')\nplt.xlabel('Fare ($)')\n\nplt.show()","9300ea98":"plt.figure(figsize=(10, 10))\n\n\nsns.kdeplot(survivors.Age.rename(\"Survivor's Age\"))\nsns.kdeplot(deceased.Age.rename(\"Deceaced's Age\"))\n\nplt.title('Titanic passengers: Age Fare Density')\nplt.xlabel('Age (years)')\n\nplt.show()","1e688524":"plt.figure(figsize=(10, 10))\n\n\nsns.kdeplot(survivors.SibSp.rename(\"Survivors\"))\nsns.kdeplot(deceased.SibSp.rename(\"Deceaced\"))\n\nplt.title('Titanic passengers: Number of Family Memebers Travelling with Density')\nplt.xlabel('# number of siblings or spouces')\n\nplt.show()","4a107e9b":"plt.figure(figsize=(10, 10))\n\nsns.catplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", kind=\"bar\", data=X)\n\nplt.show()","6c025f32":"# remove rows w\/ missing target\nX.dropna(axis=0, subset=['Survived'], inplace=True)\n\n# seperate target from the training dataset \ny = X.Survived\nX.drop(['Survived'], axis=1, inplace=True)\n\n\n# break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n\n# get low cardinality columns\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n\n\n# select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n\n# one-hot encode the data (to shorten the code, we use pandas)\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_test = pd.get_dummies(X_test)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\nX_train, X_test = X_train.align(X_test, join='left', axis=1)\n\nmissing_val_count_by_column = (X_train.isnull().sum())\nprint (missing_val_count_by_column[missing_val_count_by_column > 0])\nX_train.head()","ce111ee9":"# check for missing values\nmissing_val_count_by_column = (X_train.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])\nprint('\\n' + '-'* 80 + '\\n')\n\n\n# get information to make educated guess on how best to impute this data\nprint(\"Folks with missing age\")\nprint(X_train[X_train.Age.isnull()].describe())\nprint('\\n' + '-'* 80 + '\\n')\n\nprint('General Population')\nprint(X_train.describe())","f259f695":"\ndef check_model (n_estimators):\n    \"\"\" Create a XGBoostClassifier using provided arguements and return the \n    model\"\"\"\n    # define model \n    model = XGBClassifier(n_estimators=n_estimators, n_jobs=4)\n    \n    # bundle preprocessing and modelling in pipeline\n    clf = Pipeline(steps=[('preporcessor', SimpleImputer(strategy='most_frequent')),\n                          ('model', model)\n                         ])\n    \n    # preprocess and train data, fit the model\n    clf.fit(X_train, y_train)\n    \n    # get predictions\n    preds = clf.predict(X_valid)\n    \n    # evaluate\n    return accuracy_score(preds, y_valid)\n\n\ntest_n_estimators = [i*25 for i in range(1, 50)]\n\nmodel_accuracy = {}\nfor test in test_n_estimators:\n    model_accuracy[test] = check_model(test)\n    \n\nplt.plot(list(model_accuracy.keys()), list(model_accuracy.values()))\nplt.xlabel('Number of Estimators')\nplt.ylabel('accuracy')\nplt.show()","a62342eb":"final_model = XGBClassifier(n_estimators=50, n_jobs=4)\n    \n# bundle preprocessing and modelling in pipeline\nclf = Pipeline(steps=[('preporcessor', SimpleImputer(strategy='most_frequent')),\n                          ('model', final_model)\n                         ])\n    \n# preprocess and train data, fit the model\nclf.fit(X_train, y_train)\n    \n# get predictions\npreds = clf.predict(X_test)\n\n\noutput = pd.DataFrame({'PassengerId': X_test.PassengerId, 'Survived': preds.astype(int)})\noutput.to_csv('submission_new_session.csv', index=False)\n\n","7f0c0e0e":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\n\n# seems like there might be a bug in my pipeline that its not imputing but \n    # this is round about solution\nputer = SimpleImputer()\nimputed_X_valid = pd.DataFrame(puter.fit_transform(X_valid))\n\n    \n#y_valid = SimpleImputer(y_valid, strategy='most_frequent')\n\nimputed_X_valid.columns = X_valid.columns\nperm = PermutationImportance(clf, random_state=1).fit(imputed_X_valid, y_valid)\neli5.show_weights(perm, feature_names = X_valid.columns.tolist())","2eea5eb0":"<a id='factors'><\/a>\n# Holistic Factors \nFactors which Appear to increase the chance of survival\n- Being Female\n- Being in First Class\/more presignous class\n- Being a young child\n- Not travelling with a large group (only 0-1 family members)","75c388b2":"# Titanic ML Competition\n\n\n\n\n## About\n\nOn April 15, 1912, the supposedly unsinkable ship the titanic struck an iceberg and sank. Tragically there wasn't enough lifeboards in order to save everyone on board. This resulted in the tragic death of 1502 out of 2224 passengers and crew. \n\nThe goal of this competetion is to use machine learning algorithms and data science techniques in order to predict the callous hand of fate. However, a pure predictive model doesn't give much historical insight into this events. As such, an additional goal of this investagation will be to shift out the determining factors which if you were a passenger on that fateful night, would improve your chances of survival.","0b6b3382":"Permutation Importance is a method of extracting meaning from a machine learning models as well as a method of debugging. The algorithm loops over ever column\/feature after the model has been trained. For each feature, it randomly mixes up the rows\/values. With this permuted input which is basically substituting nonsense in for the input to that feature, the algorithm calculates a loss function that determines by how much the accuracy of the model is degradated. The size of the degragation corresponds to the importance of the feature to determining the success of the model and thus yields a metric to determine the importance of the feature.\n\nAs we can see from the results, the number one determiner of whether a person was to survive this horrific tragidy is whether or not the passanger was a women. Intuitively this makes sense as the first instinct culturally in times of crisis is to save the women and the children. According to this metric, social economic status superseeds age in level of importance. I don't believe that this reflects truth and I believe that it is an artifact of my very naive imputation technique. Additionally it points out the opportunity for an is_child feature to be added to the model to improve model efficency. The model also points out that passengers departing from the ports of Cherbourg, and Queenstown had an advantage over those departing from Southhampton. This presumably corresponed to proximety to life boats. This therefore implies that taking the time to deal with the cabins feature would most likely yield improvements the model, but I think at this point I've already spent too much time working on this morbid problem.","faced1bf":"My method for imputing these missing values will be to manually fill in values for passengers I think are children and imputing the mean value for the rest.","30b559d5":"\n### Data Notes\n\n---------------------------------\n- pclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\n- age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n- sibsp: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n- parch: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.\n\n- Survived: 1 if they survived \/\/ 0 if not\n\n- Fare: Fare paid for the trip in ($) \n\n- Cabin: Cabin number\n\n- Embarked: which port they embarked from (S: Southhampton, C: Cherbourg, Q: Queenstown)","54a4e038":"\n# Data Visualizations\/Analysis","1dc75c68":"<a id='build-model'><\/a>\n# Build Model","24c33bb2":"<a id='conclusion'><\/a>\n# Conclusion\nThis final model yielded a final accuracy score of 78.5%. At the time of submitting this model it ranked within the top 18% of the competition, which I believe is a success for my first machine learning competition. However in terms of my learning journey it is just the tip of the iceberg.\n\nFurther improvements of my methods for the future are to implement a more nuiance method of imputation for missing values, implement better data cleaning in order to utilize the high cardinality categorical data that was ignored in this investagation, and to improve my visualization techniques in order to extract more information from the data","76e72891":"\n## Fetch Data\n","2d11a91a":"<a id='data-cleansing'><\/a>\n# Preliminary Data Cleansing \n\nFor simplicity sake I will drop any categorical columns which have a large cardinality. Although it is possible to deal with these large cardinality columns and they might provide additional information to our model, they will be largely cumbersome to deal with doing this doesn't neccisarially allign with my own personal learning objectives. \n\nIn addition, low cardinality categorical data will be one-hot-encoded in order to improve model performance. ","d2905f86":"# Analysize Determining Factors","2805c96e":"# Submit Final Model Predictions"}}