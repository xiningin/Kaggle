{"cell_type":{"20e54ca1":"code","de283321":"code","5b17ae8e":"code","6315a964":"code","0f277259":"code","9848effd":"code","c5ed7d38":"code","061bc1c5":"code","dd204fbb":"code","ad98b175":"code","dfcab21a":"code","2bf12db5":"code","69af281b":"code","3be2e078":"code","0905830b":"code","dda3636c":"code","9b28be2d":"code","cd0b3492":"code","8e8ea456":"code","2403b030":"code","aed698b5":"code","b7ea3a72":"code","e2babec6":"code","bd6c9a96":"code","f68d43bc":"code","f5acbb91":"code","ab9d28fb":"code","44e6778d":"code","3fb711d9":"code","1283d7ab":"code","56172ad6":"code","d0523b17":"code","c54b448f":"code","523b0077":"code","a1d9eef9":"code","bb97c03f":"code","7553aafe":"code","27b0e6ff":"code","bf6c85b8":"code","40aabc4c":"code","3a01ab44":"code","aaee9e77":"code","9a4076bd":"markdown","0dc9cbcc":"markdown","2d93011c":"markdown","e6664a78":"markdown","3494ac48":"markdown","aa4ea63c":"markdown","7da2151b":"markdown","457672a8":"markdown","5985bef9":"markdown","9a8693be":"markdown","cc342c0f":"markdown","c21eb26d":"markdown","ee0daefb":"markdown","45654673":"markdown","2cafe575":"markdown","b0aa6da5":"markdown","85c7feaa":"markdown","df18b89b":"markdown","e841e00a":"markdown","0410bbb4":"markdown","0b620734":"markdown","a4b8b2ec":"markdown","0a2d81c2":"markdown","0d95d748":"markdown","36a43db6":"markdown","e791bdec":"markdown","9b8001e6":"markdown","abbddc71":"markdown","20ed2c18":"markdown","53718a89":"markdown","b7466709":"markdown","e5d03432":"markdown","33a91aa2":"markdown","75770ba4":"markdown","b25faae4":"markdown","da743d14":"markdown","12602c03":"markdown","7aef3d25":"markdown","c5e42dc0":"markdown","8067f6cf":"markdown","2761cc39":"markdown","872538de":"markdown","8df83bf9":"markdown","88293cc3":"markdown","6dc5453c":"markdown","b9bce801":"markdown","3012498d":"markdown","6d34ec0c":"markdown"},"source":{"20e54ca1":"# LGB\nlgb_num_leaves_max = 255\nlgb_in_leaf = 50\nlgb_lr = 0.0001\nlgb_bagging = 7\n\n# XGB\nxgb_max_depth = 20\nxgb_min_child_weight = 75\nxgb_lr = 0.0005\nxgb_num_boost_round_max = 4000\n# without early_stopping_rounds\n\n# Set weight of models\nw_lgb = 0.6\nw_xgb = 0.3\nw_logreg = 1 - w_lgb - w_xgb\nw_logreg","de283321":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport eli5\n\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import GridSearchCV, KFold, train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix\nimport lightgbm as lgb\nimport xgboost as xgb\n\nimport gc\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5b17ae8e":"tourney_result = pd.read_csv('..\/input\/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament\/MDataFiles_Stage1\/MNCAATourneyCompactResults.csv')\ntourney_result = tourney_result[tourney_result['Season'] < 2015]\ntourney_seed = pd.read_csv('..\/input\/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament\/MDataFiles_Stage1\/MNCAATourneySeeds.csv')\ntourney_seed = tourney_seed[tourney_seed['Season'] < 2015]\ntourney_result = tourney_result.drop(['DayNum', 'WScore', 'LScore', 'WLoc', 'NumOT'], axis=1)\ntourney_result = pd.merge(tourney_result, tourney_seed, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\ntourney_result.rename(columns={'Seed':'WSeed'}, inplace=True)\ntourney_result = tourney_result.drop('TeamID', axis=1)\ntourney_result = pd.merge(tourney_result, tourney_seed, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\ntourney_result.rename(columns={'Seed':'LSeed'}, inplace=True)\ntourney_result = tourney_result.drop('TeamID', axis=1)","6315a964":"tourney_seed","0f277259":"def get_seed(x):\n    return int(x[1:3])\n\ntourney_result['WSeed'] = tourney_result['WSeed'].map(lambda x: get_seed(x))\ntourney_result['LSeed'] = tourney_result['LSeed'].map(lambda x: get_seed(x))","9848effd":"season_result = pd.read_csv('..\/input\/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament\/MDataFiles_Stage1\/MRegularSeasonCompactResults.csv')\nseason_result = season_result[season_result['Season'] < 2015]\nseason_win_result = season_result[['Season', 'WTeamID', 'WScore']]\nseason_lose_result = season_result[['Season', 'LTeamID', 'LScore']]\nseason_win_result.rename(columns={'WTeamID':'TeamID', 'WScore':'Score'}, inplace=True)\nseason_lose_result.rename(columns={'LTeamID':'TeamID', 'LScore':'Score'}, inplace=True)\nseason_result = pd.concat((season_win_result, season_lose_result)).reset_index(drop=True)\nseason_score = season_result.groupby(['Season', 'TeamID'])['Score'].sum().reset_index()","c5ed7d38":"tourney_result = pd.merge(tourney_result, season_score, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\ntourney_result.rename(columns={'Score':'WScoreT'}, inplace=True)\ntourney_result = tourney_result.drop('TeamID', axis=1)\ntourney_result = pd.merge(tourney_result, season_score, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\ntourney_result.rename(columns={'Score':'LScoreT'}, inplace=True)\ntourney_result = tourney_result.drop('TeamID', axis=1)","061bc1c5":"tourney_win_result = tourney_result.drop(['Season', 'WTeamID', 'LTeamID'], axis=1)\ntourney_win_result.rename(columns={'WSeed':'Seed1', 'LSeed':'Seed2', 'WScoreT':'ScoreT1', 'LScoreT':'ScoreT2'}, inplace=True)","dd204fbb":"tourney_lose_result = tourney_win_result.copy()\ntourney_lose_result['Seed1'] = tourney_win_result['Seed2']\ntourney_lose_result['Seed2'] = tourney_win_result['Seed1']\ntourney_lose_result['ScoreT1'] = tourney_win_result['ScoreT2']\ntourney_lose_result['ScoreT2'] = tourney_win_result['ScoreT1']","ad98b175":"tourney_win_result['Seed_diff'] = tourney_win_result['Seed1'] - tourney_win_result['Seed2']\ntourney_win_result['ScoreT_diff'] = tourney_win_result['ScoreT1'] - tourney_win_result['ScoreT2']\ntourney_lose_result['Seed_diff'] = tourney_lose_result['Seed1'] - tourney_lose_result['Seed2']\ntourney_lose_result['ScoreT_diff'] = tourney_lose_result['ScoreT1'] - tourney_lose_result['ScoreT2']","dfcab21a":"tourney_win_result['result'] = 1\ntourney_lose_result['result'] = 0\ntrain_df = pd.concat((tourney_win_result, tourney_lose_result)).reset_index(drop=True)\ntrain_df","2bf12db5":"season_result","69af281b":"tourney_result","3be2e078":"test_df = pd.read_csv('..\/input\/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament\/MSampleSubmissionStage1_2020.csv')\nsub = test_df.copy()","0905830b":"test_df","dda3636c":"test_df['Season'] = test_df['ID'].map(lambda x: int(x[:4]))\ntest_df['WTeamID'] = test_df['ID'].map(lambda x: int(x[5:9]))\ntest_df['LTeamID'] = test_df['ID'].map(lambda x: int(x[10:14]))","9b28be2d":"tourney_seed = pd.read_csv('..\/input\/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament\/MDataFiles_Stage1\/MNCAATourneySeeds.csv')\ntourney_seed = tourney_seed[tourney_seed['Season'] > 2014]","cd0b3492":"season_result = pd.read_csv('..\/input\/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament\/MDataFiles_Stage1\/MRegularSeasonCompactResults.csv')\nseason_result = season_result[season_result['Season'] > 2014]\nseason_win_result = season_result[['Season', 'WTeamID', 'WScore']]\nseason_lose_result = season_result[['Season', 'LTeamID', 'LScore']]\nseason_win_result.rename(columns={'WTeamID':'TeamID', 'WScore':'Score'}, inplace=True)\nseason_lose_result.rename(columns={'LTeamID':'TeamID', 'LScore':'Score'}, inplace=True)\nseason_result = pd.concat((season_win_result, season_lose_result)).reset_index(drop=True)\nseason_score = season_result.groupby(['Season', 'TeamID'])['Score'].sum().reset_index()","8e8ea456":"test_df = pd.merge(test_df, tourney_seed, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\ntest_df.rename(columns={'Seed':'Seed1'}, inplace=True)\ntest_df = test_df.drop('TeamID', axis=1)\ntest_df = pd.merge(test_df, tourney_seed, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\ntest_df.rename(columns={'Seed':'Seed2'}, inplace=True)\ntest_df = test_df.drop('TeamID', axis=1)\ntest_df = pd.merge(test_df, season_score, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\ntest_df.rename(columns={'Score':'ScoreT1'}, inplace=True)\ntest_df = test_df.drop('TeamID', axis=1)\ntest_df = pd.merge(test_df, season_score, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\ntest_df.rename(columns={'Score':'ScoreT2'}, inplace=True)\ntest_df = test_df.drop('TeamID', axis=1)","2403b030":"test_df['Seed1'] = test_df['Seed1'].map(lambda x: get_seed(x))\ntest_df['Seed2'] = test_df['Seed2'].map(lambda x: get_seed(x))\ntest_df['Seed_diff'] = test_df['Seed1'] - test_df['Seed2']\ntest_df['ScoreT_diff'] = test_df['ScoreT1'] - test_df['ScoreT2']\ntest_df = test_df.drop(['ID', 'Pred', 'Season', 'WTeamID', 'LTeamID'], axis=1)\ntest_df","aed698b5":"X = train_df.drop('result', axis=1)\ny = train_df.result","b7ea3a72":"params_lgb = {'num_leaves': lgb_num_leaves_max,\n              'min_data_in_leaf': lgb_in_leaf,\n              'objective': 'binary',\n              'max_depth': -1,\n              'learning_rate': lgb_lr,\n              \"boosting_type\": \"gbdt\",\n              \"bagging_seed\": lgb_bagging,\n              \"metric\": 'logloss',\n              \"verbosity\": -1,\n              'random_state': 42,\n             }","e2babec6":"NFOLDS = 10\nfolds = KFold(n_splits=NFOLDS)\n\ncolumns = X.columns\nsplits = folds.split(X, y)\ny_preds_lgb = np.zeros(test_df.shape[0])\ny_train_lgb = np.zeros(X.shape[0])\ny_oof = np.zeros(X.shape[0])\n\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = columns\n  \nfor fold_n, (train_index, valid_index) in enumerate(splits):\n    print('Fold:',fold_n+1)\n    X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    dtrain = lgb.Dataset(X_train, label=y_train)\n    dvalid = lgb.Dataset(X_valid, label=y_valid)\n\n    clf = lgb.train(params_lgb, dtrain, 10000, valid_sets = [dtrain, dvalid], verbose_eval=200)\n    \n    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n    \n    y_pred_valid = clf.predict(X_valid)\n    y_oof[valid_index] = y_pred_valid\n    \n    y_train_lgb += clf.predict(X) \/ NFOLDS\n    y_preds_lgb += clf.predict(test_df) \/ NFOLDS\n    \n    del X_train, X_valid, y_train, y_valid\n    gc.collect()","bd6c9a96":"params_xgb = {'max_depth': xgb_max_depth,\n              'objective': 'binary:logistic',\n              'min_child_weight': xgb_min_child_weight,\n              'learning_rate': xgb_lr,\n              'eta'      : 0.3,\n              'subsample': 0.8,\n              'lambda '  : 4,\n              'eval_metric': 'logloss',\n              'colsample_bytree ': 0.9,\n              'colsample_bylevel': 1\n              }","f68d43bc":"X","f5acbb91":"y","ab9d28fb":"# Thanks to https:\/\/www.kaggle.com\/khoongweihao\/ncaam2020-xgboost-lightgbm-k-fold-baseline\nNFOLDS = 10\nfolds = KFold(n_splits=NFOLDS)\n\ncolumns = X.columns\nsplits = folds.split(X, y)\n\ny_preds_xgb = np.zeros(test_df.shape[0])\ny_train_xgb = np.zeros(X.shape[0])\ny_oof_xgb = np.zeros(X.shape[0])\n\ntrain_df_set = xgb.DMatrix(X)\ntest_set = xgb.DMatrix(test_df)\n  \nfor fold_n, (train_index, valid_index) in enumerate(splits):\n    print('Fold:',fold_n+1)\n    X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    train_set = xgb.DMatrix(X_train, y_train)\n    val_set = xgb.DMatrix(X_valid, y_valid)\n    \n    clf = xgb.train(params_xgb, train_set, num_boost_round=xgb_num_boost_round_max, evals=[(train_set, 'train'), (val_set, 'val')], verbose_eval=100)\n    \n    y_train_xgb += clf.predict(train_df_set) \/ NFOLDS\n    y_preds_xgb += clf.predict(test_set) \/ NFOLDS\n    \n    del X_train, X_valid, y_train, y_valid\n    gc.collect()","44e6778d":"test_df.head()","3fb711d9":"%%time\n# Standardization for regression models\nscaler = StandardScaler()\ntrain_log = pd.DataFrame(\n    scaler.fit_transform(X),\n    columns=X.columns,\n    index=X.index\n)\ntest_log = pd.DataFrame(\n    scaler.transform(test_df),\n    columns=test_df.columns,\n    index=test_df.index\n)","1283d7ab":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(train_log, y)\ncoeff_logreg = pd.DataFrame(train_log.columns.delete(0))\ncoeff_logreg.columns = ['feature']\ncoeff_logreg[\"score_logreg\"] = pd.Series(logreg.coef_[0])\ncoeff_logreg.sort_values(by='score_logreg', ascending=False)","56172ad6":"# Eli5 visualization\neli5.show_weights(logreg)","d0523b17":"y_logreg_train = logreg.predict(train_log)\ny_logreg_pred = logreg.predict(test_log)","c54b448f":"# Showing Confusion Matrix\n# Thanks to https:\/\/www.kaggle.com\/marcovasquez\/basic-nlp-with-tensorflow-and-wordcloud\ndef plot_cm(y_true, y_pred, title, figsize=(7,6)):\n    y_pred = y_pred.round().astype(int)\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","523b0077":"# Showing Confusion Matrix for LGB model\nplot_cm(y, y_train_lgb, 'Confusion matrix for LGB model')","a1d9eef9":"# Showing Confusion Matrix for XGB model\nplot_cm(y, y_train_xgb, 'Confusion matrix for XGB model')","bb97c03f":"# Showing Confusion Matrix for Logistic Regression\nplot_cm(y, y_logreg_train, 'Confusion matrix for Logistic Regression')","7553aafe":"y_preds = w_lgb*y_preds_lgb + w_xgb*y_preds_xgb + w_logreg*y_logreg_pred","27b0e6ff":"# Showing Confusion Matrix for Merging solution\ny_train_preds = w_lgb*y_train_lgb + w_xgb*y_train_xgb + w_logreg*y_logreg_train\nplot_cm(y, y_train_preds, 'Confusion matrix for Merging solution')","bf6c85b8":"sub['Pred'] = y_preds\nsub.head()","40aabc4c":"sub.info()","3a01ab44":"sub['Pred'].hist()","aaee9e77":"sub.to_csv('submission.csv', index=False)","9a4076bd":"## 3. Download data & FE <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","0dc9cbcc":"## 6. Comparison and merging solutions <a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","2d93011c":"### Commit 8\n\n    params = {'num_leaves': 127,\n              'min_data_in_leaf': 10,\n              'objective': 'binary',\n              'max_depth': -1,\n              'learning_rate': 0.005,\n              \"boosting_type\": \"gbdt\",\n              \"bagging_seed\": 11,\n              \"metric\": 'logloss',\n              \"verbosity\": -1,\n              'random_state': 42,\n             }\n             \n**LB = 0.05038**","e6664a78":"## In this notebook, I'm just training some models (LGB, XGB etc.).\n\n## Commits from 33-th don't contain leaks for 2015-2019 and are trained only by data from 1985-2014.","3494ac48":"### Commit 6\n\n    params = {'num_leaves': 127,\n              'min_data_in_leaf': 50,\n              'objective': 'binary',\n              'max_depth': -1,\n              'learning_rate': 0.005,\n              \"boosting_type\": \"gbdt\",\n              \"bagging_seed\": 11,\n              \"metric\": 'logloss',\n              \"verbosity\": -1,\n              'random_state': 42,\n             }\n             \n**LB = 0.14726**","aa4ea63c":"### Commit 14\n\n    params = {'num_leaves': 127,\n              'min_data_in_leaf': 10,\n              'objective': 'binary',\n              'max_depth': -1,\n              'learning_rate': 0.01,\n              \"boosting_type\": \"gbdt\",\n              \"bagging_seed\": 7,\n              \"metric\": 'logloss',\n              \"verbosity\": -1,\n              'random_state': 42,\n             }\n             \n**LB = 0.04872**","7da2151b":"### Commit 46\n\n* LGB\n* lgb_num_leaves_max = 255\n* lgb_in_leaf = 50\n* lgb_lr = 0.0001\n* lgb_bagging = 7\n* \n* XGB\n* xgb_max_depth = 20\n* xgb_min_child_weight = 75\n* xgb_lr = 0.0004\n* xgb_n_estimators = 4000\n* \n* w_lgb = 0.6\n* w_xgb = 0.3\n* x_logreg = 0.1\n\n**LB = 0.56177**","457672a8":"### Merging solutions","5985bef9":"[Go to Top](#0)","9a8693be":"### Commit 1\n\n    params = {'num_leaves': 255,\n              'min_data_in_leaf': 100,\n              'objective': 'binary',\n              'max_depth': -1,\n              'learning_rate': 0.005,\n              \"boosting_type\": \"gbdt\",\n              \"bagging_seed\": 11,\n              \"metric\": 'logloss',\n              \"verbosity\": -1,\n              'random_state': 42,\n             }\n             \n**LB = 0.29445**","cc342c0f":"### Commit 22\n\n**LGB**\n\n    params_lgb = {'num_leaves': 63,\n                  'min_data_in_leaf': 10,\n                  'objective': 'binary',\n                  'max_depth': -1,\n                  'learning_rate': 0.005,\n                  \"boosting_type\": \"gbdt\",\n                  \"bagging_seed\": 7,\n                  \"metric\": 'logloss',\n                  \"verbosity\": -1,\n                  'random_state': 42,\n                 }\n\n**XGB**\n\n    params_xgb = {'max_depth':63,\n                  'objective':'binary:logistic',\n                  'min_child_weight': 10,\n                  'learning_rate': 0.005,\n                  'eta'      :0.3,\n                  'subsample':0.8,\n                  'lambda '  :4,\n                  'eval_metric':'logloss',\n                  'n_estimators':2000,\n                  'colsample_bytree ':0.9,\n                  'colsample_bylevel':1\n                  }\n                  \n    w_lgb = 0.48\n    w_xgb = 0.48\n    w_logreg = 1 - w_lgb - w_xgb\n    \n    y_preds = w_lgb*y_preds_lgb + w_xgb*y_preds_xgb + w_logreg*y_logreg_pred\n    \n\n**LB = 0.27677**","c21eb26d":"# Acknowledgements\n\nThis kernel uses such good kernels: \n* [Merging FE & Prediction - xgb, lgb, logr, linr](https:\/\/www.kaggle.com\/vbmokin\/merging-fe-prediction-xgb-lgb-logr-linr)\n* [Basic Starter Kernel](https:\/\/www.kaggle.com\/addisonhoward\/basic-starter-kernel-ncaa-men-s-dataset-2019)\n* [2020 Basic Starter Kernel](https:\/\/www.kaggle.com\/hiromoon166\/2020-basic-starter-kernel)\n* [March Madness 2020 NCAAM EDA and baseline](https:\/\/www.kaggle.com\/artgor\/march-madness-2020-ncaam-eda-and-baseline)\n* [March Madness 2020 NCAAM:Simple Lightgbm on KFold](https:\/\/www.kaggle.com\/ratan123\/march-madness-2020-ncaam-simple-lightgbm-on-kfold)\n* [NCAAM2020: XGBoost + LightGBM K-Fold (Baseline)](https:\/\/www.kaggle.com\/khoongweihao\/ncaam2020-xgboost-lightgbm-k-fold-baseline)","ee0daefb":"# My older commits:\n## The organizers warned that solutions with LB < 0.20 will be deleted, so I have a solution (with all data) with LB=0.20125.\n\n## Experienced participants in previous such competitions suggest that the best solution should be around 0.5, so I have a solution (commit 16) with LB = 0.49.\n\n","45654673":"### Commit 44\n\n* LGB\n* lgb_num_leaves_max = 255\n* lgb_in_leaf = 50\n* lgb_lr = 0.001\n* lgb_bagging = 7\n* \n* XGB\n* xgb_max_depth = 20\n* xgb_min_child_weight = 75\n* xgb_lr = 0.0004\n* xgb_n_estimators = 4000\n* \n* w_lgb = 0.6\n* w_xgb = 0.3\n* x_logreg = 0.1\n\n**LB = 0.55582**","2cafe575":"## 2. Import libraries <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","b0aa6da5":"### Commit 2\n\n    params = {'num_leaves': 127,\n              'min_data_in_leaf': 100,\n              'objective': 'binary',\n              'max_depth': -1,\n              'learning_rate': 0.005,\n              \"boosting_type\": \"gbdt\",\n              \"bagging_seed\": 11,\n              \"metric\": 'logloss',\n              \"verbosity\": -1,\n              'random_state': 42,\n             }\n             \n**LB = 0.29445**","85c7feaa":"## 1.3. Previous commits: LGB (all data) <a class=\"anchor\" id=\"1.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","df18b89b":"### Commit 45\n\n* LGB\n* lgb_num_leaves_max = 300\n* lgb_in_leaf = 20\n* lgb_lr = 0.001\n* lgb_bagging = 7\n* \n* XGB\n* xgb_max_depth = 20\n* xgb_min_child_weight = 50\n* xgb_lr = 0.0004\n* xgb_n_estimators = 4000\n* \n* w_lgb = 0.6\n* w_xgb = 0.3\n* x_logreg = 0.1\n\n**LB = 0.56801**","e841e00a":"### 4.1 LGB <a class=\"anchor\" id=\"4.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","0410bbb4":"## 1. My upgrade<a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","0b620734":"### Commit 38\n\n* LGB\n* lgb_num_leaves_max = 127\n* lgb_in_leaf = 10\n* lgb_lr = 0.001\n* lgb_bagging = 7\n* \n* XGB\n* xgb_max_depth = 35\n* xgb_min_child_weight = 75\n* xgb_lr = 0.0004\n* xgb_n_estimators = 4000\n* \n* w_lgb = 0.5\n* w_xgb = 0.3\n* x_logreg = 0.2\n\n**LB = 0.57203**","a4b8b2ec":"### 4.2 XGB <a class=\"anchor\" id=\"4.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","0a2d81c2":"### Commit 43\n\n* LGB\n* lgb_num_leaves_max = 200\n* lgb_in_leaf = 10\n* lgb_lr = 0.001\n* lgb_bagging = 7\n* \n* XGB\n* xgb_max_depth = 7\n* xgb_min_child_weight = 10\n* xgb_lr = 0.0001\n* xgb_n_estimators = 4000\n* \n* w_lgb = 0.8\n* w_xgb = 0.1\n* x_logreg = 0.1\n\n**LB = 0.61163**","0d95d748":"### Commit 40\n\n* LGB\n* lgb_num_leaves_max = 200\n* lgb_in_leaf = 10\n* lgb_lr = 0.01\n* lgb_bagging = 7\n* \n* XGB\n* xgb_max_depth = 7\n* xgb_min_child_weight = 10\n* xgb_lr = 0.01\n* xgb_n_estimators = 2000\n* \n* w_lgb = 0.8\n* w_xgb = 0.1\n* x_logreg = 0.1\n\n**LB = 0.73089**","36a43db6":"## Prepare Training Data","e791bdec":"I hope you find this kernel useful and enjoyable.","9b8001e6":"### Commit 9\n\n    params = {'num_leaves': 63,\n              'min_data_in_leaf': 10,\n              'objective': 'binary',\n              'max_depth': -1,\n              'learning_rate': 0.005,\n              \"boosting_type\": \"gbdt\",\n              \"bagging_seed\": 11,\n              \"metric\": 'logloss',\n              \"verbosity\": -1,\n              'random_state': 42,\n             }\n             \n**LB = 0.08311**","abbddc71":"## 5. Showing Confusion Matrices <a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","20ed2c18":"<a class=\"anchor\" id=\"0\"><\/a>\n# [Google Cloud & NCAA\u00ae ML Competition 2020-NCAAM](https:\/\/www.kaggle.com\/c\/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament)","53718a89":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n\n1. [My upgrade](#1)\n    -  [Commit now](#1.1)\n    -  [Previous commits: LGB (1985-2014)](#1.2)\n    -  [Previous commits: LGB (all data)](#1.3)\n    -  [Previous commits: Merging of LGB, XGB, Logistic Regression (all data)](#1.4)\n1. [Import libraries](#2)\n1. [Download data & FE](#3)\n1. [Models tuning](#4)\n    -  [LGB](#4.1)\n    -  [XGB](#4.2)    \n    -  [Logistic Regression](#4.3)\n1. [Showing Confusion Matrices](#5)\n1. [Comparison and merging solutions](#6)\n1. [Submission](#7)","b7466709":"### Commit 34\n\n* LGB\n* lgb_num_leaves_max = 127\n* lgb_in_leaf = 10\n* lgb_lr = 0.001\n* lgb_bagging = 7\n* \n* XGB\n* xgb_max_depth = 35\n* xgb_min_child_weight = 75\n* xgb_lr = 0.0004\n* xgb_n_estimators = 4000\n* \n* w_lgb = 0.55\n* w_xgb = 0.35\n* x_logreg = 0.1\n\n**LB = 0.56026**","e5d03432":"### Commit 20\n\n**LGB**\n\n    params_lgb = {'num_leaves': 127,\n                  'min_data_in_leaf': 10,\n                  'objective': 'binary',\n                  'max_depth': -1,\n                  'learning_rate': 0.0001,\n                  \"boosting_type\": \"gbdt\",\n                  \"bagging_seed\": 7,\n                  \"metric\": 'logloss',\n                  \"verbosity\": -1,\n                  'random_state': 42,\n                 }\n\n**XGB**\n\n    params_xgb = {'max_depth':63,\n                  'objective':'binary:logistic',\n                  'min_child_weight': 10,\n                  'learning_rate': 0.0001,\n                  'eta'      :0.3,\n                  'subsample':0.8,\n                  'lambda '  :4,\n                  'eval_metric':'logloss',\n                  'n_estimators':2000,\n                  'colsample_bytree ':0.9,\n                  'colsample_bylevel':1\n                  }\n                  \n    w_lgb = 0.48\n    w_xgb = 0.48\n    w_logreg = 1 - w_lgb - w_xgb\n    \n    y_preds = w_lgb*y_preds_lgb + w_xgb*y_preds_xgb + w_logreg*y_logreg_pred\n    \n\n**LB = 0.34462**","33a91aa2":"## 7. Submission <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","75770ba4":"### 4.3 Logistic Regression <a class=\"anchor\" id=\"4.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","b25faae4":"# Preparing testing data","da743d14":"### Commit 32 (as Commit 14 for all data)\n\nw_lgb = 1\n\n    params = {'num_leaves': 127,\n              'min_data_in_leaf': 10,\n              'objective': 'binary',\n              'max_depth': -1,\n              'learning_rate': 0.001,\n              \"boosting_type\": \"gbdt\",\n              \"bagging_seed\": 7,\n              \"metric\": 'logloss',\n              \"verbosity\": -1,\n              'random_state': 42,\n             }\n             \n**LB = 0.64267**","12602c03":"### Commit 27\n\n#### LGB\n* lgb_num_leaves_max = 100\n* lgb_in_leaf = 75\n* lgb_lr = 0.007\n* lgb_bagging = 7\n\n#### XGB\n* xgb_max_depth = 40\n* xgb_min_child_weight = 75\n* xgb_lr = 0.0004\n* xgb_n_estimators = 4000\n\n#### Set weight of models\n* w_lgb = 0.8\n* w_xgb = 0.15\n* w_logreg = 1 - w_lgb - w_xgb    \n\n**LB = 0.24733**","7aef3d25":"Your comments and feedback are most welcome.","c5e42dc0":"### Confusion Matrix","8067f6cf":"### Commit 15\n\n    params = {'num_leaves': 127,\n              'min_data_in_leaf': 10,\n              'objective': 'binary',\n              'max_depth': -1,\n              'learning_rate': 0.001,\n              \"boosting_type\": \"gbdt\",\n              \"bagging_seed\": 7,\n              \"metric\": 'logloss',\n              \"verbosity\": -1,\n              'random_state': 42,\n             }\n             \n**LB = 0.17908**","2761cc39":"### Commit 39\n\n* LGB\n* lgb_num_leaves_max = 200\n* lgb_in_leaf = 10\n* lgb_lr = 0.01\n* lgb_bagging = 7\n* \n* XGB\n* xgb_max_depth = 7\n* xgb_min_child_weight = 10\n* xgb_lr = 0.01\n* xgb_n_estimators = 2000\n* \n* w_lgb = 0.55\n* w_xgb = 0.35\n* x_logreg = 0.1\n\n**LB = 0.60422**","872538de":"### Commit 13\n\n    params = {'num_leaves': 127,\n              'min_data_in_leaf': 10,\n              'objective': 'binary',\n              'max_depth': -1,\n              'learning_rate': 0.01,\n              \"boosting_type\": \"gbdt\",\n              \"bagging_seed\": 11,\n              \"metric\": 'logloss',\n              \"verbosity\": -1,\n              'random_state': 42,\n             }\n             \n**LB = 0.04872**","8df83bf9":"Thanks to:\n* [March Madness 2020 NCAAM EDA and baseline](https:\/\/www.kaggle.com\/artgor\/march-madness-2020-ncaam-eda-and-baseline)\n* [March Madness 2020 NCAAM:Simple Lightgbm on KFold](https:\/\/www.kaggle.com\/ratan123\/march-madness-2020-ncaam-simple-lightgbm-on-kfold)","88293cc3":"## 1.1. Commit now <a class=\"anchor\" id=\"1.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","6dc5453c":"### Commit 42\n\n* LGB\n* lgb_num_leaves_max = 200\n* lgb_in_leaf = 10\n* lgb_lr = 0.01\n* lgb_bagging = 7\n* \n* XGB\n* xgb_max_depth = 7\n* xgb_min_child_weight = 10\n* xgb_lr = 0.001\n* xgb_n_estimators = 4000\n* \n* w_lgb = 0.8\n* w_xgb = 0.1\n* x_logreg = 0.1\n\n**LB = 0.74658**","b9bce801":"## 1.4. Previous commits: Merging of LGB, XGB, Logistic Regression (all data) <a class=\"anchor\" id=\"1.4\"><\/a>\n\n[Back to Table of Contents](#0.1)","3012498d":"## 4. Model tuning <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","6d34ec0c":"## 1.2. Previous commits: LGB (1985-2014) <a class=\"anchor\" id=\"1.2\"><\/a>\n\n[Back to Table of Contents](#0.1)"}}