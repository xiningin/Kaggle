{"cell_type":{"f6ac48cd":"code","efa488b2":"code","573ed204":"code","76484434":"code","993105e9":"code","1f70c0e6":"code","ed39e7a2":"code","60a56e28":"code","df146049":"code","c998ebea":"code","88eb0f5c":"code","c8cd0529":"code","50b5cdab":"code","76dd1db2":"code","765a49fd":"code","0833369e":"code","d3c959ce":"code","3e9593d9":"code","40c9cefe":"code","0241f55f":"code","78f5ebbc":"code","cc587a9e":"code","5ecf0fab":"code","e1e72e3f":"code","e7913972":"code","2b9e0d30":"code","0d02a466":"code","9bb83baf":"code","fc9e70e2":"code","a62fbbc4":"code","02c525bd":"code","896b449b":"code","fe00740d":"code","1e7303ad":"code","5bbb9d6c":"code","03e2b19e":"code","6ddfd050":"code","916f7155":"code","7e6f4ced":"code","c98a486b":"code","0906924c":"code","99a10e73":"code","f75decaf":"code","4d29f509":"code","aa13610e":"code","bf1688ae":"code","3854798a":"code","548ec373":"code","82138e9a":"code","92a7a5ca":"code","2bfab421":"code","8aa331c1":"code","94e790be":"code","acee6de7":"code","4e3e34bb":"code","cf1788d8":"code","ec12600a":"code","9fbff04d":"code","e9ee8170":"code","a7fb2aa1":"code","f548a742":"code","26b95c81":"code","b30a0df6":"code","07f18733":"code","ffef7f9e":"code","21d6ed71":"code","6d9350cf":"code","d9a9c875":"code","ada1ac29":"code","5984f2c2":"code","1b0f637d":"code","07780a22":"code","40b2f47f":"code","3a70f334":"code","f87f574f":"code","56bc1027":"code","dd2ce242":"code","aa2ff65e":"code","09762d8b":"code","57d6ae3d":"code","be42ebda":"code","780713b8":"code","14b7674d":"code","586a4166":"code","6bb1fd2a":"code","6b97c4dd":"code","2d2e6031":"code","1e22aca4":"code","13972600":"code","3bee797a":"code","2256c048":"code","6dac73dd":"code","f75f5e4c":"code","e393da50":"code","1da19d5f":"code","eb98f61f":"code","9fdc0d97":"code","0bbde949":"code","4d3ca866":"code","5f14c574":"code","97c5316f":"code","73b29ef5":"code","89221ba9":"code","1007d1c7":"code","118d7b8a":"code","3568740d":"code","b8e12638":"code","1c582fbf":"code","41012705":"code","1438ce5d":"code","b3aaa5c4":"code","5fdc266b":"code","044aeec1":"markdown","ca46d466":"markdown","af67d06b":"markdown","2672ec02":"markdown","c5c980b7":"markdown","f2923b15":"markdown","1a8c49c4":"markdown","7dc24c3f":"markdown","604df42a":"markdown","8adf83b1":"markdown","eeb2f2cc":"markdown","d87f94a2":"markdown","56eb4117":"markdown","9b1f234c":"markdown","515586da":"markdown","8a90435d":"markdown","e6f96833":"markdown","46c20589":"markdown","93bf2301":"markdown","c797ca6d":"markdown","0c747886":"markdown","e635d87d":"markdown","fa9ffd73":"markdown","3ea8ac81":"markdown","fe229684":"markdown","ce6fc2e1":"markdown","4102acc7":"markdown","e5d5bd9e":"markdown","49d7c031":"markdown","f58cee82":"markdown","eeeb5b58":"markdown","45de54bd":"markdown","c6c5384c":"markdown","84b064f2":"markdown","b7cdbc98":"markdown","d943d0a8":"markdown","ffc50e2d":"markdown","a9539306":"markdown"},"source":{"f6ac48cd":"import torch\nimport torchvision\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.transforms import ToTensor\nfrom torchvision.utils import make_grid\nfrom torch.utils.data import random_split\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom colorama import Fore, Back, Style\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","efa488b2":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\ndata = train.append(test)","573ed204":"train.shape","76484434":"test.shape","993105e9":"data.shape","1f70c0e6":"train.head()","ed39e7a2":"test.head()","60a56e28":"train.isnull().sum()","df146049":"test.isnull().sum()","c998ebea":"red = Fore.RED\ngrn = Fore.GREEN\nblu = Fore.BLUE\nylw = Fore.YELLOW\nwht = Fore.WHITE","88eb0f5c":"def plot_distribution(feature,color):\n    plt.figure(dpi=125)\n    sns.distplot(data[feature],color=color);\n    print(\"{}Max value of {} is {}\\n{}Min value of {} is {}\\n{}Mean value of {} is {}\\n{}Std value of {} is {}\\n{}Median value of {} is {}\".format(red,feature,train[feature].max(),blu,feature,train[feature].min(),grn,feature,train[feature].mean(),ylw,feature,train[feature].std(),wht,feature,train[feature].median()));","c8cd0529":"plot_distribution('Age','green')","50b5cdab":"sns.set(style = 'darkgrid')\nplt.figure(dpi=125)\nsns.countplot(x=train.Sex, hue=train.Survived, data=train,edgecolor = sns.color_palette('dark',2));","76dd1db2":"plt.figure(dpi=125)\nsns.countplot(x = train.Sex, hue = train.Pclass,data = train,edgecolor = sns.color_palette('dark',3));","765a49fd":"plt.figure(dpi=125)\nsns.countplot(x = train.Survived, hue = train.Pclass, data = train,edgecolor = sns.color_palette('dark',1));","0833369e":"plt.figure(dpi=125)\nsns.countplot(x = train.Pclass, hue = train.Survived, data = train,edgecolor = sns.color_palette('dark',5));","d3c959ce":"train.groupby('Pclass').Survived.value_counts()","3e9593d9":"plt.figure(dpi=125)\nsns.countplot(x = train.Embarked, hue = train.Sex, data = train,edgecolor = sns.color_palette('dark',6));","40c9cefe":"plt.figure(dpi=125)\nsns.countplot(x = train.Embarked, hue = train.Survived, data = train);","0241f55f":"train['family'] = train['SibSp'] + train['Parch'] + 1","78f5ebbc":"plt.figure(dpi=125)\nsns.countplot(x = train.family, hue = train.Survived, data = train);","cc587a9e":"train.groupby('family').Survived.value_counts()","5ecf0fab":"for i in range(len(train)):\n    if(train['family'][i] > 1):\n        train['family'][i] = 1\n    else:\n        train['family'][i] = 0","e1e72e3f":"plt.figure(dpi=125)\nsns.countplot(x = train.family, hue = train.Survived, data = train);","e7913972":"train['Name'] = train.Name.str.extract('([A-Za-z]+)\\.',expand = False)","2b9e0d30":"plt.figure(dpi=200)\nplt.xticks(size=5)\nsns.countplot(x = train.Name, hue = train.Survived, data = train);","0d02a466":"top6 = train['Name'].value_counts()[:6].index.to_list()\ntop6","9bb83baf":"train['Name'] = train['Name'].apply(lambda x: x if x in top6 else 'Other')","fc9e70e2":"train.groupby('Name').Survived.value_counts()","a62fbbc4":"train['Cabin'].fillna(0,inplace = True)","02c525bd":"train['Cabin'] = train['Cabin'].apply(lambda x: 1 if x != 0 else 0)","896b449b":"plt.figure(dpi = 125)\nsns.countplot(x = train.Cabin,hue = train.Survived, data = train)","fe00740d":"train.groupby('Cabin').Survived.value_counts()","1e7303ad":"plt.figure(dpi=125)\nplot_distribution('Fare','orange')","5bbb9d6c":"train['fare_val'] = pd.qcut(train['Fare'],5,labels=False)\ntrain['fare_val'] = train['fare_val'].astype(int)","03e2b19e":"#train['fare_val'] = 0\n#for i in range(len(train)):\n#    if(train['Fare'][i] > 40.0):\n#        train['fare_val'][i] = 1        ","6ddfd050":"train.groupby('fare_val').Survived.value_counts()","916f7155":"plt.figure(dpi=125)\nsns.countplot(x = train.fare_val, hue = train.Survived, data = train);","7e6f4ced":"for i in train.Ticket:\n    train['ticket'] = train['Ticket'].apply(lambda x: x.replace('\/','').replace('.','').split()[0] if not x.isdigit() else 'N' )","c98a486b":"train[\"ticket\"].unique()","0906924c":"plt.figure(dpi=125)\nplt.xticks(rotation = 'vertical')\nsns.countplot(x = train.ticket, hue = train.Survived, data = train);","99a10e73":"train['SibSp'] = pd.cut(train['SibSp'],bins=[0,1,100],labels=[0,1],right=False).astype(int)\n\ntrain['Parch'] = pd.cut(train['Parch'], bins=[0,1,100], labels=[0,1], right=False).astype(int)","f75decaf":"train.groupby('Parch').Survived.value_counts()","4d29f509":"#family\ntest['family'] = test['SibSp'] + test['Parch'] + 1\nfor i in range(len(test)):\n    if(test['family'][i] > 1):\n        test['family'][i] = 1\n    else:\n        test['family'][i] = 0\n    \n#Name\ntest['Name'] = test['Name'].apply(lambda x: x if x in top6 else 'Other')\n        \n#Cabin\ntest['Cabin'].fillna(0,inplace = True)\n\ntest['Cabin'] = test['Cabin'].apply(lambda x: 1 if x != 0 else 0)\n\n#ticket\nfor i in test.Ticket:\n    test['ticket'] = test['Ticket'].apply(lambda x: x.replace('\/','').replace('.','').split()[0] if not x.isdigit() else 'N' )\n\n#Fare\ntest['fare_val'] = pd.qcut(test['Fare'],5,labels=False)\ntest['fare_val'] = test['fare_val'].fillna(value = test['fare_val'].mode()[0])\ntest['fare_val'] = test['fare_val'].astype(int)\n#test['fare_val'] = 0\n#for i in range(len(test)):\n#    if(test['Fare'][i] > 40.0):\n#        test['fare_val'][i] = 1\n\n# SibSp and Parch\ntest['SibSp'] = pd.cut(test['SibSp'],bins = [0,1,200],labels=[0,1],right = False).astype(int)\ntest['Parch'] = pd.cut(test['Parch'],bins = [0,1,200],labels=[0,1],right = False).astype(int)","aa13610e":"features = [##'PassengerId',\n            'Pclass',\n            'Name',\n            'Sex',\n            'Age',\n            'SibSp',\n            'Parch',\n            #'family',#derived from SibSp & Parch\n            #'Ticket',\n            #'ticket',\n            ##'Fare',\n            'fare_val',#derived from Fare\n            #'Cabin',\n            #'Embarked'\n           ]\n\ntarget = ['Survived']","bf1688ae":"train[features].isnull().sum()","3854798a":"test[features].info()","548ec373":"test[features].isnull().sum()","82138e9a":"'''Age_mean = train['Age'].std()\n#train['Age'] = train['Age'].fillna(value = Age_mean)\n\nAge_mean_t = test['Age'].std()\n#test['Age'] = test['Age'].fillna(value = Age_mean_t)\nf\"'train',{Age_mean}, 'test',{Age_mean_t}\"''';","92a7a5ca":"'''Age_std = train['Age'].std()\ntrain['Age'] = train['Age'].fillna(value = Age_std)\n\nAge_std_t = test['Age'].std()\ntest['Age'] = test['Age'].fillna(value = Age_std_t)\nf\"'train',{Age_std}, 'test',{Age_std_t}\"''';","2bfab421":"Age_mean = data['Age'].mean()\nAge_std = data['Age'].std()\n\ntrain['Age'] = train['Age'].fillna(np.random.randint(Age_mean-Age_std,Age_mean+Age_std))\ntest['Age'] = test['Age'].fillna(np.random.randint(Age_mean-Age_std,Age_mean+Age_std))","8aa331c1":"from sklearn.preprocessing import LabelEncoder\n\nlbl = LabelEncoder()\n\ntrain['Sex'] = lbl.fit_transform(train[['Sex']].values.ravel())\ntest['Sex'] = lbl.fit_transform(test[['Sex']].values.ravel())","94e790be":"lbl2 = LabelEncoder()\ntrain['Name'] = lbl2.fit_transform(train[['Name']].values.ravel())\ntest['Name'] = lbl2.fit_transform(test[['Name']].values.ravel())","acee6de7":"#lbl3 = LabelEncoder()\n#train['Cabin'] = lbl3.fit_transform(train[['Cabin']].values.ravel())\n#test['Cabin'] = lbl3.fit_transform(test[['Cabin']].values.ravel())","4e3e34bb":"train['Embarked'] = train['Embarked'].fillna(value=train['Embarked'].mode()[0])\ntest['Embarked'] = test['Embarked'].fillna(value=test['Embarked'].mode()[0])\n","cf1788d8":"train_ds = train[features+target]\ntest_ds = test[features]","ec12600a":"#train_ds = pd.get_dummies(columns = ['Embarked'],data=train_ds,drop_first = False)\n#test_ds = pd.get_dummies(columns = ['Embarked'],data=test_ds,drop_first = False)","9fbff04d":"#train_ds = pd.get_dummies(columns = ['Pclass'],data=train_ds,drop_first = True)\n#test_ds = pd.get_dummies(columns = ['Pclass'],data=test_ds,drop_first = True)","e9ee8170":"#train_remove_tickets = ['_{}'.format(x) for x in train['ticket'].unique() if x not in test['ticket'].unique()]\n#test_remove_tickets = ['_{}'.format(x) for x in test['ticket'].unique() if x not in train['ticket'].unique()]","a7fb2aa1":"#print(f'train tickets to be removed:{train_remove_tickets}\\ntest tickets to be removed:{test_remove_tickets}')","f548a742":"#train_ds = pd.get_dummies(columns = ['ticket'],data=train_ds,prefix = '')\n#test_ds = pd.get_dummies(columns = ['ticket'],data=test_ds,prefix = '')","26b95c81":"#train_ds = train_ds.drop(train_remove_tickets,axis = 1)\n#test_ds = test_ds.drop(test_remove_tickets,axis = 1)","b30a0df6":"print(train_ds.head())\ntrain_ds.shape","07f18733":"print(test_ds.head())\ntest_ds.shape","ffef7f9e":"#train_ds = train_ds.drop(['Cabin_T'],axis = 1)","21d6ed71":"train_ds","6d9350cf":"from torch.nn import BCELoss\nfrom torch.utils.data.dataloader import DataLoader\nfrom tqdm import tqdm_notebook as tqdm\ntorch.manual_seed(7)","d9a9c875":"class Titanicnn(nn.Module):\n    def __init__(self,in_size,hidden_size1,hidden_size2,hidden_size3,hidden_size4,num_classes):\n        super().__init__()\n        self.linear1 = nn.Linear(in_size, hidden_size1)\n        self.linear2 = nn.Linear(hidden_size1, hidden_size2)\n        self.linear3 = nn.Linear(hidden_size2,hidden_size3)\n        self.linear4 = nn.Linear(hidden_size3,hidden_size4)\n        self.linear5 = nn.Linear(hidden_size4,num_classes)\n        self.dropout = nn.Dropout(0.2)\n    \n    def forward(self,xb):\n        xb = xb.view(-1,xb.size(1)).float()\n        out = self.linear1(xb)\n        #out = self.dropout(out)\n        out = F.relu(out)\n        out = self.linear2(out)\n        out = self.dropout(out)\n        #out = F.relu(out)\n        out = self.linear3(out)\n        #out = self.dropout(out)\n        out = F.relu(out)\n        out = self.linear4(out)\n        out = self.dropout(out)\n        #out = F.relu(out)\n        out = self.linear5(out)\n        out = self.dropout(out)\n        return out\n    \n    def training_step(self,batch):\n        features, labels = batch\n        out = self(features)\n        loss = F.cross_entropy(out, labels)\n        #loss_x = nn.CrossEntropyLoss()\n        #loss = loss_x(out,labels)\n        return loss\n    \n    def validation_step(self,batch):\n        features,labels = batch\n        out = self(features)\n        loss = F.cross_entropy(out, labels)\n        #loss_x = F.CrossEntropyLoss()\n        #loss = loss_x(out,labels)\n        acc = accuracy(out,labels)\n        return {'valid_loss': loss, 'valid_acc': acc}\n\n    def validation_epoch_end(self,outputs):\n        batch_losses = [x['valid_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()\n        batch_accs = [x['valid_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()\n        return {'valid_loss': epoch_loss.item(), 'valid_acc': epoch_acc.item()}\n\n    def epoch_end(self,epoch,result):\n        print(\"Epoch [{}], valid_loss: {:.4f}, valid_acc: {:.4f}\".format(epoch, result['valid_loss'], result['valid_acc']))","ada1ac29":"def accuracy(outputs, labels):\n    \n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))\n        \n    return preds","5984f2c2":"input_size = train_ds.shape[1] -1\nhidden_size1 = 128\nhidden_size2 = 1024\nhidden_size3 = 512\nhidden_size4 = 128\nnum_classes = 2","1b0f637d":"model = Titanicnn(input_size, hidden_size1=hidden_size1, hidden_size2= hidden_size2,hidden_size3 = hidden_size3,hidden_size4 = hidden_size4,num_classes = num_classes)\nmodel","07780a22":"for t in model.parameters():\n    print(t.shape)","40b2f47f":"def convert_to_tensors(ds, valid_size = 0.30,train_set=True):\n    \n    scaler = StandardScaler()\n    batch_size = ds.shape[0]\n    \n    if(train_set == True):\n        \n        targets_t = ds.Survived.values\n        features_t = ds.drop(labels = ['Survived'],axis = 1).values\n        \n        features_t = scaler.fit_transform(features_t)\n        \n        targetsTrain = torch.from_numpy(targets_t).type(torch.LongTensor)\n        featuresTrain = torch.from_numpy(features_t)\n        \n        train_tensor = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\n        \n        valid_size = round(len(train_tensor) * (valid_size))\n        train_size = len(train_tensor) - valid_size\n\n        ttrain_ds, tvalid_ds = random_split(train_tensor, [train_size, valid_size])\n        \n        print(\"train_size:- \",len(ttrain_ds),\"test_size:- \", len(tvalid_ds))\n              \n        train_loader = DataLoader(ttrain_ds, batch_size = batch_size, shuffle=True, num_workers=4, pin_memory=True)\n        valid_loader = DataLoader(tvalid_ds, batch_size = batch_size*2, num_workers=4, pin_memory=True)\n              \n        return train_loader,valid_loader\n    \n    else:\n        \n        featuresTest = ds.values\n        features_test = scaler.fit_transform(featuresTest)\n        featuresTensor = torch.from_numpy(features_test)\n        print(len(featuresTensor))\n        test_loader = DataLoader(featuresTensor, batch_size*2, num_workers=4, pin_memory = True)\n        \n        return test_loader\n        ","3a70f334":"train_ds.shape","f87f574f":"train_loader,valid_loader = convert_to_tensors(train_ds)","56bc1027":"for data, _ in train_loader:\n    print('data.shape:', data.shape)","dd2ce242":"for data, labels in train_loader:\n    outputs = model(data)\n    print(labels.shape)\n    #loss_x = nn.CrossEntropyLoss()\n    #loss = loss_x(outputs, labels)\n    loss = F.cross_entropy(outputs, labels)\n    acc = accuracy(outputs,labels)\n    print('Loss:', loss.item())\n    print('Initial Acc:',float(acc),\"%\")\n    break\n\nprint('outputs.shape : ', outputs.shape)\nprint('Sample outputs :\\n', outputs[:5].data)\nprint('Sample labels :\\n', labels[:5].data)","aa2ff65e":"torch.cuda.is_available()","09762d8b":"def get_default_device():\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')","57d6ae3d":"device = get_default_device()\ndevice","be42ebda":"def to_device(data,device):\n    if isinstance(data, (list,tuple)):\n        return [to_device(x,device) for x in data]\n    return data.to(device,non_blocking = True)","780713b8":"for xd, yd in train_loader:\n    print(xd.shape)\n    xds = to_device(xd, device)\n    print(xds.device)\n    break","14b7674d":"class DeviceDataLoader():\n    def __init__(self, ds, device):\n        self.ds = ds\n        self.device = device\n    \n    def __iter__(self):\n        for d in self.ds:\n            yield to_device(d,self.device)\n            \n    def __len__(self):\n        return len(self.ds)","586a4166":"train_loader = DeviceDataLoader(train_loader, device)\nvalid_loader = DeviceDataLoader(valid_loader, device)","6bb1fd2a":"for xd, yd in valid_loader:\n    print('xd.device:', xd.device)\n    print('yd:', yd)\n    break","6b97c4dd":"def evaluate(model, valid_loader):\n    outputs = [model.validation_step(batch) for batch in valid_loader]\n    return model.validation_epoch_end(outputs)","2d2e6031":"def fit(epochs, lr, model, train_loader, valid_loader, opt_func = torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(),lr)\n    for epoch in tqdm(range(epochs)):\n        # Training ==>\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            \n        # Validation ==>\n        result = evaluate(model,valid_loader)\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","1e22aca4":"model = Titanicnn(input_size,hidden_size1=hidden_size1, hidden_size2= hidden_size2, hidden_size3= hidden_size3,hidden_size4 = hidden_size4,num_classes = num_classes)\nto_device(model, device) #Load model into the GPU","13972600":"history = [evaluate(model,valid_loader)]\nhistory","3bee797a":"history += fit(20, 0.2, model, train_loader, valid_loader) #Learning Rate = 0.5","2256c048":"history += fit(40, 0.001, model, train_loader, valid_loader)","6dac73dd":"history += fit(100, 0.01, model, train_loader, valid_loader)","f75f5e4c":"history += fit(300, 0.01, model, train_loader, valid_loader)","e393da50":"history += fit(30, 0.001, model, train_loader, valid_loader)","1da19d5f":"test_loader = convert_to_tensors(test_ds,train_set= False)","eb98f61f":"def predict(pred_x,model):\n    for pred in pred_x:\n        y = model(pred)\n        _, y_  = torch.max(y, dim=1)\n    return y_","9fdc0d97":"test_loader_gpu = DeviceDataLoader(test_loader, device)","0bbde949":"test_preds = predict(test_loader_gpu,model)","4d3ca866":"final_pred = test_preds.cpu()","5f14c574":"print(final_pred.shape)","97c5316f":"passId = test[['PassengerId']].values","73b29ef5":"final_pred = final_pred.data.numpy()","89221ba9":"final_pred","1007d1c7":"sub = {'PassengerId':passId.ravel(), 'Survived':final_pred}","118d7b8a":"submission_csv = pd.DataFrame(sub)","3568740d":"submission_csv.head()","b8e12638":"submission_csv.to_csv('final_sub_titanic_pth.csv',index=False)","1c582fbf":"x = pd.read_csv(\".\/final_sub_titanic_pth.csv\")","41012705":"x.head()","1438ce5d":"filename = \".\/titanic_pytorch.pth\"","b3aaa5c4":"torch.save(model.state_dict(),filename)","5fdc266b":"loaded_model = Titanicnn(input_size,hidden_size1=hidden_size1, hidden_size2= hidden_size2, hidden_size3= hidden_size3,hidden_size4 = hidden_size4,num_classes = num_classes)\nto_device(model, device)\nloaded_model.load_state_dict(torch.load(filename))\nloaded_model.eval()","044aeec1":"Well there weren't fewer people in 1st class.. so we can conclude that people in first class did indeed had a higher chance and rate of survival","ca46d466":"The minimum fare is 0.0 which means there was\/were someone\/some people with a free ride in titanic(probably in 1st class) \ud83d\ude05\ud83d\ude05","af67d06b":"# Getting data \ud83d\udcbd","2672ec02":"#### Making same modifications to test dataset","c5c980b7":"## Prediction Function \ud83d\udd2e","f2923b15":"# Importing libraries\ud83d\udcda","1a8c49c4":"people with family had greater rate of survival","7dc24c3f":"## Name","604df42a":"Only people from 1st class had a greater survival rate compared to classes 2 and 3... maybe there were fewer people in 1st class","8adf83b1":"More people survived from Cherbourg compared to other two","eeb2f2cc":"Loading DataLoaders into the GPU","d87f94a2":"Most people survived were from 1st class.. so people in 1st class had a greater chance of survival","56eb4117":"### Family","9b1f234c":"As we can see we get loss of about .63 and an accuracy of .67","515586da":"# Creating a PyTorch Model \u2764\ufe0f\u200d\ud83d\udd25","8a90435d":"people with PC,SWPP,SCAH,SS,PP ticket survived the most","e6f96833":"People with greater fare had a higher rate of survival","46c20589":"### Ticket","93bf2301":"## Training Loop \ud83d\udd01","c797ca6d":"## Saving the Model \ud83d\udcbe","0c747886":"# Data Preprocessing \ud83d\uddc4\ufe0f","e635d87d":"As we know from the disaster.. women and children were the first to be evacuated.. mean age is 29.6, median age is 28.. both of which are >18 which suggests the people with missing ages are adults ... standard deviation is 14.5 which is <18 which suggests they are chilren..","fa9ffd73":"## Initial Prediction \ud83c\udf31","3ea8ac81":"As we can confirm females had a greater survival rate compared to males.","fe229684":"Device is cuda means we are using GPU","ce6fc2e1":"people with cabins were more likely to survive","4102acc7":"# EDA \ud83d\udcca","e5d5bd9e":"## Work in Progress \ud83d\udea7\ud83d\udea7","49d7c031":"## Loading the Model \ud83d\udd03","f58cee82":"### Fare","eeeb5b58":"This seems kinda random .. only people with family member size of 2,3,4 survived greater than the rest \ud83e\udd14\ud83e\udd14.","45de54bd":"## Checkout my other [**Notebook**](https:\/\/www.kaggle.com\/mdhamani\/titanic-getting-better-eda-top-14) with randomforest and xgboost Classifiers\n","c6c5384c":"### Moving PyTorch Tensors to Device(GPU)","84b064f2":"### Cabin","b7cdbc98":"There were many people from both genders in 3rd class(as expected)... the second most filled class was 1st class ... this maybe suggests huge price difference between classes.. not sure though \ud83e\udd37\u200d\u2642\ufe0f\ud83e\udd37\u200d\u2642\ufe0f","d943d0a8":"# My Submission \ud83d\ude4b\u200d\u2642\ufe0f","ffc50e2d":"# To-Do\ud83d\udccb\n## Tuning parameters \ud83e\udd37\u200d\u2642\ufe0f\ud83e\udd37\u200d\u2642\ufe0f\n## Make the PyTorch Model more accurate","a9539306":"# Using GPU for Training \ud83d\udc96\ud83e\udd2f"}}