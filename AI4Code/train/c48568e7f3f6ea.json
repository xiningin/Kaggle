{"cell_type":{"a10f7bed":"code","d75c5f70":"code","6a4b7b8d":"code","5fd3ee91":"code","4d95a456":"code","b3d9666c":"code","9ee586d4":"code","db5d7d4c":"code","bb3313a0":"code","c1fd9165":"code","d03d743a":"code","5450c588":"code","11df88eb":"code","f86199d1":"code","8568ac49":"code","96ddf220":"code","b2a337b7":"code","1f96a54d":"code","6a3768b6":"code","2cf38873":"code","5f300658":"code","e116bf39":"code","5b0c82cc":"code","f657c7f9":"code","5427323e":"code","63c174e7":"code","755fbc21":"code","8fbe09cf":"code","af204517":"code","a21c4672":"code","92d584f9":"code","a0d22d26":"code","c6131ebb":"code","e652e755":"code","dd910ffc":"code","824974a0":"code","b347e021":"code","947ae3b8":"code","f940385a":"code","e14459ff":"code","280fa7c7":"code","4c88d7d6":"code","c39635fc":"code","89c87271":"code","1ab48ec2":"code","dc24cf7c":"code","69338223":"code","f166e9b7":"code","4d981211":"code","2a6c3906":"code","e82821b6":"code","f8ebbb23":"code","0cdc1c19":"code","3a8b9e6b":"code","2b875de4":"code","0bd6721c":"code","2b69033e":"code","3a4db508":"code","64149ae7":"code","4460fc23":"code","3820c4a1":"code","1982a71c":"markdown","3980c2c9":"markdown","5da0edc4":"markdown","626e6904":"markdown","db68a27c":"markdown","32b23797":"markdown","5786eef8":"markdown","8819acc8":"markdown","d96fae8f":"markdown","68724557":"markdown","85526a0c":"markdown","39f65734":"markdown","c6c1bdb0":"markdown","fe40b6fe":"markdown","0eaedbbb":"markdown","4a8a97d2":"markdown","558e7210":"markdown","dcf87b16":"markdown","775f15cd":"markdown","3e76a5f6":"markdown","1c837982":"markdown","a2318fd4":"markdown","ae2ecb24":"markdown","4049eac6":"markdown","5ca59554":"markdown","6b106dea":"markdown","85e9184b":"markdown","5ad526f3":"markdown","644599ba":"markdown","e169266c":"markdown"},"source":{"a10f7bed":"!pip install openpyxl","d75c5f70":"#Load the librarys\nimport pandas as pd #To work with dataset\nimport numpy as np #Math library\nimport seaborn as sns #Graph library that use matplot in background\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\n# Import StandardScaler from scikit-learn\nfrom sklearn.preprocessing import StandardScaler,RobustScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.manifold import TSNE\nfrom datetime import datetime, date\nfrom sklearn.metrics import  accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nwarnings.filterwarnings('ignore')","6a4b7b8d":"# Read spreadsheet and assign it to swiss_loan\nswiss_loan= pd.read_excel('..\/input\/loan-credit-risk\/ATUCE_Case_study_data_2021.xlsx')\n# Create a boolean mask on whether each feature less than 40% missing values.\nmask = swiss_loan.isna().sum() \/ len(swiss_loan) < 0.4\n# Create a reduced dataset by applying the mask\nreduced_df =swiss_loan.loc[:, mask]\n\n# drop ID\nreduced_df.drop('ID', axis=1, inplace=True)\nreduced_df['Pays_corr']=reduced_df['Pays'].str.strip()\nreduced_df['Taux_corr']=reduced_df['Taux'].str.replace('%','').str.strip().str.replace(',','.').str.extract(r'(\\d+.\\d+)')\nreduced_df['Taux_corr'] = np.where(reduced_df['Taux_corr'].isnull(), 0, reduced_df['Taux_corr'])\nreduced_df['Taux_corr'] = pd.to_numeric(reduced_df['Taux_corr'], errors='coerce')\n# Specify the boundaries of the bins\nbins = [-np.inf,5.5,  6.5, 10]\n# Bin labels\nlabels = [ 'Low', 'Medium', 'High']\n# Bin the continuous variable ConvertedSalary using these boundaries\nreduced_df['Taux_corr_binned'] = pd.cut(reduced_df['Taux_corr'], \n                                         bins=bins,labels=labels )\n# Print the first 5 rows of the boundary_binned column\nreduced_df['Montant_corr']=reduced_df['Montant'].str.replace('\u20ac','').str.replace('\\xa0','').str.strip().str.replace('\\s+','')\nreduced_df['Montant_corr'] = np.where(reduced_df['Montant_corr'].isnull(), 0, reduced_df['Montant_corr'])\nreduced_df['Montant_corr'] = pd.to_numeric(reduced_df['Montant_corr'], errors='coerce')\nreduced_df['Niveau_risque_corr']=reduced_df['Niveau de risque'].str.rstrip().str.replace('\\s+','')\nEmprunteurs = reduced_df['Emprunteur']\n\n\nEmprunteurs_counts = Emprunteurs.value_counts()\n\n# Create a mask for only categories that occur less than 5 times\nmask = Emprunteurs.isin(Emprunteurs_counts[Emprunteurs_counts<5].index)\n# Label all other categories as Other\nreduced_df['Emprunteur'][mask] = 'Other'\nreduced_df['capital_social_corr']=reduced_df['capital social'].str.replace('\u20ac','').str.replace('\\xa0','').str.strip().str.replace('\\s+','')\nreduced_df['capital_social_corr'] = np.where(reduced_df['capital_social_corr'].isnull(), 0, reduced_df['capital_social_corr'])\nreduced_df['capital_social_corr'] = pd.to_numeric(reduced_df['capital_social_corr'], errors='coerce')\n\nreduced_df['Effectifse_corr']=reduced_df['effectifs'].str.rstrip().str.replace('\\s+','')\nreduced_df['Effectifse_corr'][reduced_df['Effectifse_corr'] == '-'] = np.nan\n\nreduced_df['Nombre_mois_p\u00e9riode16_corr']=reduced_df['Nombre de mois de la p\u00e9riode 16'].str.rstrip().str.replace('mois','').str.replace(',','.').str.replace('\\s+','').str.extract(r\"(\\d+\\.\\d+|\\d+)\")\nreduced_df['Nombre_mois_p\u00e9riode16_corr'][reduced_df['Nombre_mois_p\u00e9riode16_corr'] == '-'] = np.nan\nreduced_df['Nombre_mois_p\u00e9riode16_corr'] = pd.to_numeric(reduced_df['Nombre_mois_p\u00e9riode16_corr'], errors='coerce')\n\nreduced_df['Chiffre_Affaires_16_corr']=reduced_df.iloc[:,12].str.replace('\\xa0','').str.strip().str.replace('\\s+','')\nreduced_df['Chiffre_Affaires_16_corr'] = pd.to_numeric(reduced_df['Chiffre_Affaires_16_corr'], errors='coerce')\n\nreduced_df['Total_Bilan_16_corr']= reduced_df['Total Bilan 16'].str.replace('\\xa0','').str.strip().str.replace('\\s+','').str.extract(r\"([-+]?\\d*\\.*\\d+|\\d+)\")\nreduced_df['Total_Bilan_16_corr']= pd.to_numeric(reduced_df['Total_Bilan_16_corr'], errors='coerce')\n\nreduced_df['Capacit\u00e9_remboursement_FCCR_16_corr']= reduced_df['Capacit\u00e9 de remboursement (FCCR) 16'].str.replace('\\xa0','').str.strip().str.replace('\\s+','').str.replace(',','.').str.extract(r\"([-+]?\\d*\\.*\\d+|\\d+)\")\nreduced_df['Capacit\u00e9_remboursement_FCCR_16_corr']= pd.to_numeric(reduced_df['Capacit\u00e9_remboursement_FCCR_16_corr'], errors='coerce')\n\n\nreduced_df['Fonds_Propres_16_corr']= reduced_df['Fonds Propres 16'].str.replace('\\xa0','').str.strip().str.replace('\\s+','').str.replace(',','.').str.extract(r\"([-+]?\\d*\\.*\\d+|\\d+)\")\nreduced_df['Fonds_Propres_16_corr']= pd.to_numeric(reduced_df['Fonds_Propres_16_corr'], errors='coerce')\n\nreduced_df['Fonds_Propres_Total_Bilan_corr']= reduced_df['Fonds Propres \/ Total Bilan 16'].str.replace('\\xa0','').str.strip().str.replace('\\s+','').str.replace(',','.').str.replace('%','').str.extract(r\"([-+]?\\d*\\.*\\d+|\\d+)\")\nreduced_df['Fonds_Propres_Total_Bilan_corr']= pd.to_numeric(reduced_df['Fonds_Propres_Total_Bilan_corr'], errors='coerce')\n\nreduced_df['Dettes_Nettes_EBE_16_corr']= reduced_df['Dettes Nettes \/ EBE(* ann\u00e9es) 16'].str.replace('\\xa0','').str.strip().str.replace('\\s+','').str.replace(',','.').str.replace('*','').str.extract(r\"([-+]?\\d*\\.*\\d+|\\d+)\")\nreduced_df['Dettes_Nettes_EBE_16_corr']= pd.to_numeric(reduced_df['Dettes_Nettes_EBE_16_corr'], errors='coerce')\n\nreduced_df['DettesNettes_Fonds_propres_16_corr']= reduced_df['Dettes Nettes \/ Fonds propres 16'].str.replace('\\xa0','').str.strip().str.replace('\\s+','').str.replace(',','.').str.replace('%','').str.extract(r\"([-+]?\\d*\\.*\\d+|\\d+)\")\nreduced_df['DettesNettes_Fonds_propres_16_corr']= pd.to_numeric(reduced_df['DettesNettes_Fonds_propres_16_corr'], errors='coerce')","5fd3ee91":"reduced_df.columns","4d95a456":"list_to_keep= [ 'Pays_corr','Mois', 'Taux_corr', 'Taux_corr_binned', 'Montant_corr',\n       'Niveau_risque_corr','Emprunteur', 'capital_social_corr', 'Effectifse_corr','ann\u00e9e de cr\u00e9ation',\n       'Nombre_mois_p\u00e9riode16_corr', 'Chiffre_Affaires_16_corr',\n       'Total_Bilan_16_corr', 'Capacit\u00e9_remboursement_FCCR_16_corr',\n       'Fonds_Propres_16_corr', 'Fonds_Propres_Total_Bilan_corr',\n       'Dettes_Nettes_EBE_16_corr', 'DettesNettes_Fonds_propres_16_corr']","b3d9666c":"clean_reduced_df= reduced_df[list_to_keep].copy()\nclean_reduced_df.shape","9ee586d4":"clean_reduced_df.head(3)","db5d7d4c":"clean_reduced_df.info()","bb3313a0":"clean_reduced_df.describe()","c1fd9165":"clean_reduced_df.var(skipna = True)","d03d743a":"ax = sns.distplot(clean_reduced_df[\"Montant_corr\"])","5450c588":"# Print out the variance \nprint(clean_reduced_df['Montant_corr'].var())\n\n# Apply the log normalization function \nclean_reduced_df['Montant_corr_log'] = np.log(clean_reduced_df['Montant_corr'])\n\n# Check the variance of the normalized \nprint(clean_reduced_df['Montant_corr_log'].var())","11df88eb":"ax = sns.distplot(clean_reduced_df['Montant_corr_log'])","f86199d1":"ax = sns.distplot(clean_reduced_df[\"Chiffre_Affaires_16_corr\"])","8568ac49":"# Print out the variance \nprint(clean_reduced_df['Chiffre_Affaires_16_corr'].var())\n\n# Apply the log normalization function \nclean_reduced_df['Chiffre_Affaires_16_corr_log'] = np.log(clean_reduced_df['Chiffre_Affaires_16_corr'])\n\n# Check the variance of the normalized \nprint(clean_reduced_df['Chiffre_Affaires_16_corr_log'].var())","96ddf220":"ax = sns.distplot(clean_reduced_df['Chiffre_Affaires_16_corr_log'])","b2a337b7":"ax = sns.distplot(clean_reduced_df[\"capital_social_corr\"])","1f96a54d":"# Print out the variance\nprint(clean_reduced_df['capital_social_corr'].var())\n\n# Apply the log normalization function t\nclean_reduced_df['capital_social_corr_log'] = np.log(clean_reduced_df['capital_social_corr']+1)\n\n# Check the variance of the normalized \nprint(clean_reduced_df['capital_social_corr_log'].var())","6a3768b6":"ax = sns.distplot(clean_reduced_df[\"capital_social_corr\"])","2cf38873":"# select the float columns\nnum_columns = clean_reduced_df.select_dtypes(include=['int64','float64']).columns\n# Create a histogram\n# create distplots\nfor column in num_columns:\n    plt.figure()         \n    sns.distplot(clean_reduced_df[column])","5f300658":"# select non-numeric columns\ncat_columns = clean_reduced_df.select_dtypes(exclude=['int64','float64']).columns\ncat_columns","e116bf39":"fill_missing_then_one_hot_encoder = make_pipeline(\n    SimpleImputer(strategy='constant', fill_value='manquante',add_indicator=True),\n    OneHotEncoder(handle_unknown='ignore')\n)\n\nfill_missing_then_one_hot_encoder.fit(clean_reduced_df[cat_columns])\n\nx_encoded =fill_missing_then_one_hot_encoder.transform(clean_reduced_df[cat_columns])\n\nprint('C\\'est bon')","5b0c82cc":"x_encoded.toarray()","f657c7f9":"# This function converts given date to age\ndef age(creation):\n    born = int(creation)\n    today = date.today()\n    return today.year - born\n  \nclean_reduced_df['Age'] = clean_reduced_df['ann\u00e9e de cr\u00e9ation'].apply(age)\nclean_reduced_df[['Age','ann\u00e9e de cr\u00e9ation']]","5427323e":"# Create a boolean mask on whether each feature less than 40% missing values.\nmask = swiss_loan.isna().sum() \/ len(swiss_loan) < 0.4\n# Create a reduced dataset by applying the mask\nreduced_df =swiss_loan.loc[:, mask]\n\n# drop ID\nreduced_df.drop('ID', axis=1, inplace=True)\nreduced_df['Pays_corr']=reduced_df['Pays'].str.strip()\nreduced_df['Taux_corr']=reduced_df['Taux'].str.replace('%','').str.strip().str.replace(',','.').str.extract(r'(\\d+.\\d+)')\nreduced_df['Taux_corr'] = np.where(reduced_df['Taux_corr'].isnull(), 0.1, reduced_df['Taux_corr'])\nreduced_df['Taux_corr'] = pd.to_numeric(reduced_df['Taux_corr'], errors='coerce')\n# Specify the boundaries of the bins\nbins = [0.01,5.5,  6.5, 10]\n# Bin labels\nlabels = [ 'Low', 'Medium', 'High']\n# Bin the continuous variable ConvertedSalary using these boundaries\nreduced_df['Taux_corr_binned'] = pd.cut(reduced_df['Taux_corr'], \n                                         bins=bins,labels=labels )\n# Print the first 5 rows of the boundary_binned column\nreduced_df['Montant_corr']=reduced_df['Montant'].str.replace('\u20ac','').str.replace('\\xa0','').str.strip().str.replace('\\s+','')\nreduced_df['Montant_corr'] = np.where(reduced_df['Montant_corr'].isnull(), 0.1, reduced_df['Montant_corr'])\nreduced_df['Montant_corr'] = pd.to_numeric(reduced_df['Montant_corr'], errors='coerce')\nreduced_df['Niveau_risque_corr']=reduced_df['Niveau de risque'].str.rstrip().str.replace('\\s+','')\nEmprunteurs = reduced_df['Emprunteur']\n\n\nEmprunteurs_counts = Emprunteurs.value_counts()\n\n# Create a mask for only categories that occur less than 5 times\nmask = Emprunteurs.isin(Emprunteurs_counts[Emprunteurs_counts<5].index)\n# Label all other categories as Other\nreduced_df['Emprunteur'][mask] = 'Other'\nreduced_df['capital_social_corr']=reduced_df['capital social'].str.replace('\u20ac','').str.replace('\\xa0','').str.strip().str.replace('\\s+','')\nreduced_df['capital_social_corr'] = np.where(reduced_df['capital_social_corr'].isnull(), 0.1, reduced_df['capital_social_corr'])\nreduced_df['capital_social_corr'] = pd.to_numeric(reduced_df['capital_social_corr'], errors='coerce')\n\nreduced_df['Effectifse_corr']=reduced_df['effectifs'].str.rstrip().str.replace('\\s+','')\nreduced_df['Effectifse_corr'][reduced_df['Effectifse_corr'] == '-'] = np.nan\n\nreduced_df['Nombre_mois_p\u00e9riode16_corr']=reduced_df['Nombre de mois de la p\u00e9riode 16'].str.rstrip().str.replace('mois','').str.replace(',','.').str.replace('\\s+','').str.extract(r\"(\\d+\\.\\d+|\\d+)\")\nreduced_df['Nombre_mois_p\u00e9riode16_corr'][reduced_df['Nombre_mois_p\u00e9riode16_corr'] == '-'] = np.nan\nreduced_df['Nombre_mois_p\u00e9riode16_corr'] = pd.to_numeric(reduced_df['Nombre_mois_p\u00e9riode16_corr'], errors='coerce')\n\nreduced_df['Chiffre_Affaires_16_corr']=reduced_df.iloc[:,12].str.replace('\\xa0','').str.strip().str.replace('\\s+','')\nreduced_df['Chiffre_Affaires_16_corr'] = pd.to_numeric(reduced_df['Chiffre_Affaires_16_corr'], errors='coerce')\n\nreduced_df['Total_Bilan_16_corr']= reduced_df['Total Bilan 16'].str.replace('\\xa0','').str.strip().str.replace('\\s+','').str.extract(r\"([-+]?\\d*\\.*\\d+|\\d+)\")\nreduced_df['Total_Bilan_16_corr']= pd.to_numeric(reduced_df['Total_Bilan_16_corr'], errors='coerce')\n\nreduced_df['Capacit\u00e9_remboursement_FCCR_16_corr']= reduced_df['Capacit\u00e9 de remboursement (FCCR) 16'].str.replace('\\xa0','').str.strip().str.replace('\\s+','').str.replace(',','.').str.extract(r\"([-+]?\\d*\\.*\\d+|\\d+)\")\nreduced_df['Capacit\u00e9_remboursement_FCCR_16_corr']= pd.to_numeric(reduced_df['Capacit\u00e9_remboursement_FCCR_16_corr'], errors='coerce')\n\n\nreduced_df['Fonds_Propres_16_corr']= reduced_df['Fonds Propres 16'].str.replace('\\xa0','').str.strip().str.replace('\\s+','').str.replace(',','.').str.extract(r\"([-+]?\\d*\\.*\\d+|\\d+)\")\nreduced_df['Fonds_Propres_16_corr']= pd.to_numeric(reduced_df['Fonds_Propres_16_corr'], errors='coerce')\n\nreduced_df['Fonds_Propres_Total_Bilan_corr']= reduced_df['Fonds Propres \/ Total Bilan 16'].str.replace('\\xa0','').str.strip().str.replace('\\s+','').str.replace(',','.').str.replace('%','').str.extract(r\"([-+]?\\d*\\.*\\d+|\\d+)\")\nreduced_df['Fonds_Propres_Total_Bilan_corr']= pd.to_numeric(reduced_df['Fonds_Propres_Total_Bilan_corr'], errors='coerce')\n\nreduced_df['Dettes_Nettes_EBE_16_corr']= reduced_df['Dettes Nettes \/ EBE(* ann\u00e9es) 16'].str.replace('\\xa0','').str.strip().str.replace('\\s+','').str.replace(',','.').str.replace('*','').str.extract(r\"([-+]?\\d*\\.*\\d+|\\d+)\")\nreduced_df['Dettes_Nettes_EBE_16_corr']= pd.to_numeric(reduced_df['Dettes_Nettes_EBE_16_corr'], errors='coerce')\n\nreduced_df['DettesNettes_Fonds_propres_16_corr']= reduced_df['Dettes Nettes \/ Fonds propres 16'].str.replace('\\xa0','').str.strip().str.replace('\\s+','').str.replace(',','.').str.replace('%','').str.extract(r\"([-+]?\\d*\\.*\\d+|\\d+)\")\nreduced_df['DettesNettes_Fonds_propres_16_corr']= pd.to_numeric(reduced_df['DettesNettes_Fonds_propres_16_corr'], errors='coerce')\n\n\n# Apply the log normalization function \nreduced_df['Montant_corr_log'] = np.log(reduced_df['Montant_corr'])\n\n# Apply the log normalization function \nreduced_df['Chiffre_Affaires_16_corr_log'] = np.log(reduced_df['Chiffre_Affaires_16_corr'])\n# Apply the log normalization function t\nreduced_df['capital_social_corr_log'] = np.log(reduced_df['capital_social_corr']+1)\n# This function converts given date to age\ndef age(creation):\n    born = int(creation)\n    today = date.today()\n    return today.year - born\n  \nreduced_df['Age'] = reduced_df['ann\u00e9e de cr\u00e9ation'].apply(age)\nlist_to_keep= [ 'Pays_corr','Mois','Age' ,'Taux_corr', 'Taux_corr_binned', 'Montant_corr_log',\n       'Niveau_risque_corr','Emprunteur', 'capital_social_corr_log', 'Effectifse_corr',\n       'Nombre_mois_p\u00e9riode16_corr', \n       'Total_Bilan_16_corr', 'Capacit\u00e9_remboursement_FCCR_16_corr',\n       'Fonds_Propres_16_corr', 'Fonds_Propres_Total_Bilan_corr',\n       'Dettes_Nettes_EBE_16_corr', 'DettesNettes_Fonds_propres_16_corr','Chiffre_Affaires_16_corr_log']\nclean_reduced_df= reduced_df[list_to_keep].copy()\nclean_reduced_df.shape\n","63c174e7":"clean_reduced_df.info()","755fbc21":"clean_reduced_df.isnull().sum()","8fbe09cf":"# Create arrays for the features and the response variable\nclassification_cible = \"Taux_corr_binned\"\nregresiion_cible= \"Taux_corr\"\ny = clean_reduced_df['Taux_corr_binned']\nX = clean_reduced_df.drop(['Taux_corr','Taux_corr_binned'], axis=1)\n# Split the dataset and labels into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\nprint(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test.shape[0], X_train.shape[0], X_test.shape[1]))","af204517":"# select the float columns\nnum_columns = X.select_dtypes(include=['int64','float64']).columns\n# select non-numeric columns\ncat_columns = X.select_dtypes(exclude=['int64','float64']).columns","a21c4672":"X[num_columns].describe()","92d584f9":"X[['Total_Bilan_16_corr']].boxplot()","a0d22d26":"X[['Total_Bilan_16_corr']].describe()","c6131ebb":"quantile = X[num_columns].quantile(0.95)\nquantile","e652e755":"# Find the mean and standard dev\nstd = X[num_columns].std()\nmean = X[num_columns].mean()\n\n# Calculate the cutoff\ncut_off = std *3.5\nlower, upper = mean - cut_off, mean + cut_off\n\n# Trim the outliers\ntrimmed_df = X[(X[num_columns] < upper) \\\n                           & (X[num_columns] > lower)]\n\n# The trimmed box plot\nX[num_columns].boxplot()\nplt.show()","dd910ffc":"# The trimmed box plot\ntrimmed_df[num_columns].boxplot()\nplt.show()","824974a0":"trimmed_df.shape","b347e021":"trimmed_df.describe()","947ae3b8":"trimmed_df[['Total_Bilan_16_corr']].boxplot()","f940385a":"trimmed_df[['Total_Bilan_16_corr']].describe()","e14459ff":"fill_missing_then_one_hot_encoder = make_pipeline(\n    SimpleImputer(strategy='constant', fill_value='manquante',add_indicator=True),\n    OneHotEncoder(handle_unknown='ignore')\n)\nfill_missing_then_Standar_scaler = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n    StandardScaler()\n)\nfill_missing_then_RobustScaler = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n    RobustScaler()\n)\ndata_preprocess_StandardScaler = make_column_transformer(\n    ( fill_missing_then_one_hot_encoder , cat_columns),\n    ( fill_missing_then_Standar_scaler, num_columns)\n)\ndata_preprocess_RobustScaler = make_column_transformer(\n    ( fill_missing_then_one_hot_encoder , cat_columns),\n    ( fill_missing_then_RobustScaler, num_columns)\n)\nX_pre_StandardScaler_trim = data_preprocess_StandardScaler.fit_transform(trimmed_df)\nX_pre_RobustScaler_trim = data_preprocess_RobustScaler.fit_transform(trimmed_df)\nx_with_outlier_standard =data_preprocess_StandardScaler.fit_transform(X)\nx_with_outlier_Robust =data_preprocess_RobustScaler.fit_transform(X)\ny_pre = LabelEncoder().fit_transform(clean_reduced_df[['Taux_corr_binned']])\ny = clean_reduced_df['Taux_corr']","280fa7c7":"from sklearn.ensemble import IsolationForest\nisf = IsolationForest(n_jobs=-1, random_state=1)\nisf.fit(x_with_outlier_standard, y_pre)\nx = np.array(isf.predict(x_with_outlier_standard))\nunique, counts = np.unique(x, return_counts=True)\nprint(np.asarray((unique, counts)).T)","4c88d7d6":"lasso = Lasso(alpha =0.0005, random_state=1)\nlasso.fit(X_pre_StandardScaler_trim,y)\nlasso.score(X_pre_StandardScaler_trim,y)","c39635fc":"from sklearn.linear_model import LinearRegression\nLinearRegression = LinearRegression()\nLinearRegression.fit(X_pre_StandardScaler_trim,y)\nLinearRegression.score(X_pre_StandardScaler_trim,y)","89c87271":"\nLinearRegression.fit(x_with_outlier_standard,y)\nLinearRegression.score(x_with_outlier_standard,y)","1ab48ec2":"lasso = Lasso(alpha =0.0005, random_state=1)\nlasso.fit(x_with_outlier_standard,y)\nlasso.score(x_with_outlier_standard,y)","dc24cf7c":"lasso = Lasso(alpha =0.0005, random_state=1)\nlasso.fit(x_with_outlier_Robust,y)\nlasso.score(x_with_outlier_Robust,y)","69338223":"lasso.fit(X_pre_RobustScaler_trim,y)\nlasso.score(X_pre_RobustScaler_trim,y)","f166e9b7":"from sklearn.feature_selection import VarianceThreshold\n\n# Create a VarianceThreshold feature selector\nsel = VarianceThreshold(threshold=0.001)\ndf_numeric_r=X[num_columns]\n# Fit the selector to normalized head_df\nsel.fit(df_numeric_r \/ df_numeric_r.mean())\n\n# Create a boolean mask\nmask = sel.get_support()\n\n# Apply the mask to create a reduced dataframe\nreduced_df = df_numeric_r.loc[:, mask]\n\nprint(\"Dimensionality reduced from {} to {}.\".format(df_numeric_r.shape[1], reduced_df.shape[1]))","4d981211":"import seaborn as sns \nwarnings.filterwarnings(\"ignore\")\nsns.heatmap(clean_reduced_df.corr(), square=True, cmap='RdYlGn')","2a6c3906":"# Calculate the correlation matrix and take the absolute value\ncorr_matrix = clean_reduced_df.corr().abs()\n\n# Create a True\/False mask and apply it\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\ntri_df = corr_matrix.mask(mask)\n\n# List column names of highly correlated features (r > 0.95)\nto_drop = [c for c in tri_df.columns if any(tri_df[c] > 0.95)]\n\n# Drop the features in the to_drop list\nreduced_df = clean_reduced_df.drop(to_drop, axis=1)\nprint(to_drop)\nprint(\"The reduced dataframe has {} columns.\".format(reduced_df.shape[1]))","e82821b6":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nfill_missing_then_RobustScaler = make_pipeline( SimpleImputer(strategy='median',add_indicator=False),\n    RobustScaler())\n# Create the RFE with a LogisticRegression estimator and 10 features to select\nrfe_LR = RFE(estimator=LogisticRegression(), n_features_to_select = 10, verbose=1)\ny1 = LabelEncoder().fit_transform(clean_reduced_df[['Taux_corr_binned']])\nX1 = clean_reduced_df.drop(['Taux_corr','Taux_corr_binned'], axis=1)\ndf_numeric_1=fill_missing_then_RobustScaler.fit_transform(X1[num_columns])\n\n# Fits the eliminator to the data\nrfe_LR.fit(df_numeric_1, y1)\n\n# Print the features and their ranking (high = dropped early on)\nprint(dict(zip(X1[num_columns].columns, rfe_LR.ranking_)))\nmask_LR=rfe_LR.support_\n# Print the features that are not eliminated\nprint(X1[num_columns].columns[mask_LR])\n\n# Calculates the test set accuracy\nacc = accuracy_score(y1, rfe_LR.predict(df_numeric_1))\nprint(\"{0:.1%} accuracy on test set.\".format(acc))","f8ebbb23":"# Wrap the feature eliminator around the random forest model\nrfe_RFC = RFE(estimator=RandomForestClassifier(), n_features_to_select=10, verbose=1)\n\n# Fits the eliminator to the data\nrfe_RFC.fit(df_numeric_1, y1)\n\n# Create a mask using an attribute of rfe\nmask_RFC = rfe_RFC.support_\n\n# Apply the mask to the feature dataset X and print the result\nreduced_num_X = X1[num_columns].loc[:,mask_RFC]\nprint(reduced_num_X.columns)\n# Calculates the test set accuracy\nacc = accuracy_score(y1, rfe_RFC.predict(df_numeric_1))\nprint(\"{0:.1%} accuracy on test set.\".format(acc))","0cdc1c19":"# Select 10 features with RFE on a GradientBoostingRegressor\nrfe_gb = RFE(estimator=GradientBoostingRegressor(), \n             n_features_to_select=10, step=1, verbose=1)\nrfe_gb.fit(df_numeric_1, y1)\n# Calculate the R squared on the test set\n# Assign the support array to gb_mask\ngb_mask = rfe_gb.support_\nr_squared = rfe_gb.score(df_numeric_1, y1)\nprint('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n# Apply the mask to the feature dataset X and print the result\nreduced_num_X = X1[num_columns].loc[:,gb_mask]\nprint(reduced_num_X.columns)\n","3a8b9e6b":"# Sum the votes of the three models\nvotes = np.sum([mask_LR, mask_RFC, gb_mask], axis=0)\n\n# Create a mask for features selected by all 3 models\nmeta_mask = votes >= 3\n\n# Apply the dimensionality reduction on X\nX_reduced = X1[num_columns].loc[:, meta_mask]\nprint(X1[num_columns].columns)\nprint(X_reduced.columns)","2b875de4":"print(X_reduced.columns)","0bd6721c":"temp1=set(X1[num_columns].columns)\ns=set(X_reduced.columns)\ntemp3 = [x for x in temp1 if x not in s]\nprint(\"list to drop {}\".format(temp3))","2b69033e":"list(set(X1[num_columns].columns) - set(X_reduced.columns))","3a4db508":"X1[num_columns][temp3].describe()","64149ae7":"X_reduced = clean_reduced_df.drop(['Taux_corr','Taux_corr_binned', 'Fonds_Propres_16_corr','Chiffre_Affaires_16_corr_log' ,'Total_Bilan_16_corr', 'Nombre_mois_p\u00e9riode16_corr'], axis=1)\ny = clean_reduced_df['Taux_corr']\n# select the float columns\nnum_columns_red = X_reduced.select_dtypes(include=['int64','float64']).columns\n# select non-numeric columns\ncat_columns_red =X_reduced.select_dtypes(exclude=['int64','float64']).columns\n \nfill_missing_then_one_hot_encoder = make_pipeline(\n    SimpleImputer(strategy='constant', fill_value='manquante',add_indicator=True),\n    OneHotEncoder(handle_unknown='ignore')\n)\nfill_missing_then_Standar_scaler = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n    StandardScaler()\n)\nfill_missing_then_RobustScaler = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n    RobustScaler()\n)\ndata_preprocess_StandardScaler = make_column_transformer(\n    ( fill_missing_then_one_hot_encoder , cat_columns_red),\n    ( fill_missing_then_Standar_scaler, num_columns_red)\n)\ndata_preprocess_RobustScaler = make_column_transformer(\n    ( fill_missing_then_one_hot_encoder , cat_columns_red),\n    ( fill_missing_then_RobustScaler, num_columns_red)\n)\nx_with_outlier_Robust_reduced = data_preprocess_RobustScaler.fit_transform(X_reduced)\n\nLR = LinearRegression()\nLR.fit(x_with_outlier_Robust_reduced,y)\nLR.score(x_with_outlier_Robust_reduced,y)","4460fc23":"lasso = Lasso(alpha =0.0005, random_state=1)\nlasso.fit(x_with_outlier_Robust_reduced,y)\nlasso.score(x_with_outlier_Robust_reduced,y)","3820c4a1":"from sklearn.neighbors import KNeighborsRegressor as KNN\nknn=KNN(n_neighbors=51)\nknn.fit(x_with_outlier_Robust_reduced,y)\nknn.score(x_with_outlier_Robust_reduced,y)","1982a71c":"# 3.Feature Engineering\nFeature engineering is the act of taking raw data and extracting features from it that are suitable for tasks like machine learning\n## Encoding categorical variables\n     Pays_corr  \n     Taux_corr_binned \n     Niveau_risque_corr \n     Emprunteur \n     Effectifse_corr ","3980c2c9":"##  Handle  Outliers : num_clumns \n### Percentage based outlier removal\n\nOne way to ensure a small portion of data is not having an overly adverse effect is by removing a certain percentage of the largest and\/or smallest values in the column. This can be achieved by finding the relevant quantile and trimming the data using it with a mask. This approach is particularly useful if you are concerned that the highest values in your dataset should be avoided. When using this approach, you must remember that even if there are no outliers, this will still remove the same top N percentage from the dataset.","5da0edc4":"# Combining 3 feature selectors","626e6904":"# EDA After Cleaning :\n","db68a27c":"## Keep clean columns :","32b23797":"\n    \n## 1.Clean_Preprocess_selection Data","5786eef8":"#### Montant_corr_log","8819acc8":"# Let's see if trimming was efficient : ","d96fae8f":"# Data Preparation\nTasks\n\n    1.Data selection\n\n    2.Data preprocessing\n\n    3.Feature engineering\n\n    4.Dimensionality reduction","68724557":"# Mask2","85526a0c":"### Those 2 columns have big varinace let's see if we can reduce the variance  \n    Montant_corr  var:                         3.744913e+11\n    capital_social_corr  var :                 2.133567e+16\n    Chiffre_Affaires_16_corr var:              1.070443e+11","39f65734":"# Put all preprocess tasks ","c6c1bdb0":"#### Chiffre_Affaires_16_corr_log","fe40b6fe":"# Trim the outliers","0eaedbbb":"#### capital_social_corr_log","4a8a97d2":"**==> Reducing features does not improve the result for regression ...**\n\n**==> In the futur we can use : ,SelectKBest,chi2..   and Combining feature selectors**","558e7210":"# Final Pipe Preprocess ","dcf87b16":"# Mask3:\n","775f15cd":"# Create new cloumn age of the company :\n","3e76a5f6":"**==> we have 49 outliers in our data**","1c837982":"# Automatic Recursive Feature Elimination\n## Mask1","a2318fd4":"# Target vs predictors \n","ae2ecb24":"# Beware the outliers\n\nOutliers are extreme values that fall a long way outside of the other observations. In a small dataset, the impact of an outlier can be much greater, since it will have a heavy weight for the model:\n","4049eac6":"**=>In our case , trimming have not a real impact**","5ca59554":"# 2.Data preprocessing\n## Log normalization\n\nTransformation Normally distributed features are an assumption in Statistical algorithms. Deep learning & Regression-type algorithms also benefit from normally distributed data. Transformation is required to treat the skewed features and make them normally distributed. Right skewed features can be transformed to normality with Square Root\/ Cube Root\/ Logarithm transformation.\nHelps with skewness No predetermined range for scaled data Useful only **on non-zero, non-negative data**\n","6b106dea":"## Num Vs cat columns ","85e9184b":"# Create pipeline :Cat colmuns ","5ad526f3":"## Features with low variance\n\nNow use the VarianceThreshold feature selector to remove these features.","644599ba":"# 4.Dimensionality reduction\n## The curse of dimensionality\n\nIn fact, to avoid overfitting the number of observations should increase exponentially with the number of features. Since this becomes really problematic for high dimensional datasets this phenomenon is known as the curse of dimensionality. The solution to this is of course to apply dimensionality reduction.\n\nWhile removing outliers consists of deleting rows from the dataset, feature selection consists of deleting columns that do not contribute to the prediction. There is a wide variety of methods, such as analysis of its correlation with the target, importance analysis and recursive elimination.\n","e169266c":"##  Removing highly correlated features"}}