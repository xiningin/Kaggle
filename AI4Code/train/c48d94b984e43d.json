{"cell_type":{"59709ae1":"code","7cb92d7c":"code","431283a6":"code","42f44af1":"code","58dfb92e":"code","bc1ee285":"code","dd91390d":"code","3b94b02d":"code","ebc86e6d":"code","2556f939":"code","73992b88":"code","19080134":"code","586a95f4":"markdown"},"source":{"59709ae1":"# Importing core libraries\nimport numpy as np\nimport pandas as pd\nimport joblib\n\n# Importing from Scikit-Learn\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA","7cb92d7c":"# Loading data \nX_train = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\nX_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")","431283a6":"# Preparing data as a tabular matrix\ny_train = X_train.target\nX_train = X_train.set_index('id').drop('target', axis='columns')\nX_test = X_test.set_index('id')","42f44af1":"# Pointing out categorical features\ncategoricals = [item for item in X_train.columns if 'cat' in item]","58dfb92e":"# Dealing with categorical data using get_dummies\ndummies = pd.get_dummies(X_train.append(X_test)[categoricals])\nX_train[dummies.columns] = dummies.iloc[:len(X_train), :]\nX_test[dummies.columns] = dummies.iloc[len(X_train): , :]\ndel(dummies)","bc1ee285":"# Dealing with categorical data using OrdinalEncoder (only when there are 3 or more levels)\nordinal_encoder = OrdinalEncoder()\nX_train[categoricals[3:]] = ordinal_encoder.fit_transform(X_train[categoricals[3:]]).astype(int)\nX_test[categoricals[3:]] = ordinal_encoder.transform(X_test[categoricals[3:]]).astype(int)\nX_train = X_train.drop(categoricals[:3], axis=\"columns\")\nX_test = X_test.drop(categoricals[:3], axis=\"columns\")","dd91390d":"# Feature selection (https:\/\/www.kaggle.com\/lucamassaron\/tutorial-feature-selection-with-boruta-shap)\nimportant_features = ['cat1_A', 'cat1_B', 'cat5', 'cat8', 'cat8_C', 'cat8_E', 'cont0', \n                      'cont1', 'cont10', 'cont11', 'cont12', 'cont13', 'cont2', 'cont3', \n                      'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9']\n\ncategoricals = ['cat5', 'cat8']\n\nX_train = X_train[important_features]\nX_test = X_test[important_features]","3b94b02d":"# Stratifying the data\n\npca = PCA(n_components=16, random_state=0)\nkm = KMeans(n_clusters=32, random_state=0)\n\npca.fit(X_train)\nkm.fit(pca.transform(X_train))\n\nprint(np.unique(km.labels_, return_counts=True))\n\ny_stratified = km.labels_","ebc86e6d":"# Creating your folds for repeated use (for instance, stacking)\n\nfolds = 10\nseeds = [42, 0, 101]\nfold_idxs = list()\n\nfor seed in seeds:\n    skf = StratifiedKFold(n_splits=folds,\n                          shuffle=True, \n                          random_state=seed)\n\n    fold_idxs.append(list(skf.split(X_train, y_stratified)))","2556f939":"# Checking the produced folds\nfor j, fold_idxs_ in enumerate(fold_idxs):\n    print(f\"\\n--- seed={seeds[j]} ---\")\n    for k, (train_idx, validation_idx) in enumerate(fold_idxs_):\n        print(f\"fold {k} train idxs: {len(train_idx)} validation idxs: {len(validation_idx)} -> {validation_idx[:10]}\")","73992b88":"# Storing into the notebook for future use\njoblib.dump(fold_idxs, '.\/fold_idxs.job')","19080134":"# Retrieving from the notebook\nfold_idxs = joblib.load('.\/fold_idxs.job')","586a95f4":"In this competition there are a lot of discussions and notebooks on hyper-parameter tuning, XGBoost and stacking, since they are the most effective techniques.\n\nYet, something is missing. For instance, there little reasoning has been done on the best way to cross-validate.\n\nActually, if you watch carefully the data, it seems like data distributions are segregated into specific portions of space, something reminiscent ot me of the Madelon dataset created by Isabelle Guyon, one of the ideators of the Support Vector Machines (see for Guyon's contribution: https:\/\/www.kdnuggets.com\/2016\/07\/guyon-data-mining-history-svm-support-vector-machines.html for the Madelon dataset see instead: https:\/\/archive.ics.uci.edu\/ml\/datasets\/madelon).\n\nI therefore tried to stratifiy my folds based on a k-means clustering of the non-noisy data (see https:\/\/www.kaggle.com\/c\/30-days-of-ml\/discussion\/267931) and my local cv has become more reliable (very correlated with the public leaderboard) and my models are performing much better with cv prediction.\n\nTry it and let me know, if it works also on your models!\n\nHappy Kaggling!"}}