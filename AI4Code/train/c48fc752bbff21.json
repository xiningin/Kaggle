{"cell_type":{"60d91b3c":"code","d54ed719":"code","79bf7d97":"code","33b012c4":"code","d580b8fd":"code","658d4fec":"code","95044dd0":"code","ff82b67c":"code","0ed9c9c9":"code","7c0aab27":"code","cf19793a":"code","b4ba8b7c":"code","a0b8fe16":"code","5387868c":"code","5f661ee9":"code","a0d606cf":"code","7ae88680":"code","ea7284a4":"code","963e9fab":"code","4faaf3d5":"code","fd15b09f":"code","79fc46be":"code","2a17e0e5":"code","82558e95":"code","83451c14":"code","ab404f9e":"code","34a7475c":"code","424b039b":"code","4361c9eb":"code","f2b6d2d9":"code","37ddb6de":"code","5be9551d":"code","6bf6157e":"code","2030351e":"code","33b69e20":"code","5bee758f":"code","05e336e2":"code","c6b36fe9":"code","249b9f08":"code","0d5181f5":"code","edf44b10":"code","af82afda":"code","9478caef":"code","22839a66":"code","d946e171":"code","77ddaf7c":"code","317ad9ae":"code","eebef2e3":"code","54a16143":"code","2d39e35d":"code","d82c3b98":"code","3b91d4ac":"code","012a319c":"code","05398f16":"code","a2b1bb81":"code","5ac80bb3":"code","20a19349":"code","ab86f7a9":"code","e1097ed3":"code","e3371688":"code","45c671fc":"markdown","967b0f62":"markdown","667994ae":"markdown","7082f372":"markdown","056dee4c":"markdown","aca998c1":"markdown","10c53eb3":"markdown","88746f74":"markdown","8fbbbc9e":"markdown","cfae5adb":"markdown","f703172b":"markdown","11e8da89":"markdown","ca8a2515":"markdown","19d75399":"markdown","81879915":"markdown","27f742c3":"markdown","f6517200":"markdown","ee8c5710":"markdown","e932c23f":"markdown","21fc82f0":"markdown","83b22e0d":"markdown"},"source":{"60d91b3c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n# Import Libraries\nimport pandas as pd\nimport numpy  as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom statistics import mode\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n# import the metrics class\nfrom sklearn import metrics\nimport math\nimport matplotlib.pyplot as plt   \nimport seaborn as sns\nimport re","d54ed719":"#read train data_set\ntrain_data=pd.read_csv(\"..\/input\/titanic\/train.csv\")\n\n#read test data_set\ntest_data=pd.read_csv(\"..\/input\/titanic\/test.csv\")","79bf7d97":"# show train table data_set\ntrain_data","33b012c4":"# show test table data_set\ntest_data","d580b8fd":"# as we see train dataset columns = 12 and test dataset columns = 11, Survived column in test data is missing...\n#lets compare train data_set in columns and rows\ntrain_data.info()","658d4fec":"#lets compare test data_set in columns and rows\ntest_data.info()","95044dd0":"# lets discover how many values is null in train dataset column\ntrain_data.isnull().sum()","ff82b67c":"test_data.isnull().sum()","0ed9c9c9":"# as we see 86 null values in Ages column, 327 in cabin and 1 in Fare\nprint(\"the minimum age value in Age colum is:\",train_data[\"Age\"].min())","7c0aab27":"#so no zero value in Age column\nprint(\"the maximum age value in Age column is:\",train_data[\"Age\"].max())","cf19793a":"#replacing NAN values in Age column by the average Age, (train data)\n#creat a list called new_Age, include all Age column values\nnew_Age_train=pd.unique(train_data[\"Age\"])\n\n#get the mean value of Age column \nmean_Age_train= round(np.nanmean(new_Age_train),0)\nprint(\"Average of the Age column is =\", mean_Age_train)","b4ba8b7c":"# let us replace the NAN values in Age column of train dataset by the mean value \ntrain_data[\"Age\"] = train_data[\"Age\"].fillna(34)\ntrain_data[\"Age\"] \ntrain_data.info()","a0b8fe16":"#creat a list called new_Age, include all Age column values\nnew_Age_test=pd.unique(test_data[\"Age\"])\n\n#get the mean value of Age column \nmean_Age_test= round(np.nanmean(new_Age_test),0)\nprint(\"Average of the Age column is =\", mean_Age_test)","5387868c":"# let us replace the NAN values in Age column of test dataset by the mean value \ntest_data[\"Age\"] = test_data[\"Age\"].fillna(31)\ntest_data[\"Age\"]\ntest_data.info()","5f661ee9":"plt.figure(figsize = (3, 4)) # setting the size of the figure\nsns.set(style=\"darkgrid\")    # setting the style of my vidualizations\n#Creating a barplot with count of survived or not survived\nsns.countplot(x = 'Survived', data = train_data, palette= ['Red', 'Blue'])    \nplt.title(\"Survived (0 vs 1)\",size=20)   # setting the title of my plot\nplt.show()","a0d606cf":"Rate_survived=round(np.mean(train_data['Survived']),3)*100\nprint(\"the Survival Rate is:\",Rate_survived,\"%\")","7ae88680":"sns.set(style=\"darkgrid\")\nplt.figure(figsize = (3, 4))\nsns.countplot(x = 'Sex', data = train_data, palette= ['skyblue','pink'])    \nplt.title(\"Sex Gender (Male vs Female)\",size=20)   # setting the title of my plot\nplt.show()","ea7284a4":"total=train_data['Survived'].sum()\nmen=train_data[train_data['Sex']=='male']\nwomen=train_data[train_data['Sex']=='female']\nm=men['Sex'].count()\nw=women['Sex'].count()\nprint(\"male:\",m)\nprint(\"female:\",w)\nprint(\"Rate of women:\",round(w\/(m+w)*100))\nprint(\"Rate of men:\",round(m\/(m+w)*100))","963e9fab":"#Lets visualize the relation between survivors and Sex gender.\nplt.figure(figsize=(5,5))\nsns.countplot(x = 'Survived', hue = 'Sex', data = train_data)\nplt.title(\"Relation : Sex and Survived\",size=20)","4faaf3d5":"# plot distributions of Fare passengers who survived or death\nb = sns.FacetGrid( train_data, hue = 'Survived', aspect=4 )\nb.map(sns.kdeplot, 'Fare', shade= True )\nb.set(xlim=(0 , train_data['Fare'].max()))\nb.add_legend()","fd15b09f":"#Lets visualize the relation Pclass  and number of survivors\nplt.figure(figsize=(5,5))\nsns.countplot(x = 'Survived', hue = 'Pclass', data = train_data)\nplt.title(\"Relation : Pclass and Survived\",size=20)","79fc46be":"#Lets visualize the relation SibSp  and number of survivors\nplt.figure(figsize=(5,5))\nsns.countplot(x = 'Survived', hue = 'SibSp', data = train_data)\nplt.title(\"Relation : SibSp and Survived\",size=20)","2a17e0e5":"#Lets visualize the relation Parch and number of survivors\nplt.figure(figsize=(5,5))\nsns.countplot(x = 'Survived', hue = 'Parch', data = train_data)\nplt.title(\"Relation : Parch and Survived\",size=20)","82558e95":"#Lets visualize the relation Embarked and number of survivors\nplt.figure(figsize=(5,5))\nsns.countplot(x = 'Survived', hue = 'Embarked', data = train_data)\nplt.title(\"Relation : Embarked and Survived\",size=20)","83451c14":"plt.figure(figsize=(30,15))\nplt.subplot(235)\nplt.hist(x = [train_data[train_data['Survived']==1]['Age'], train_data[train_data['Survived']==0]['Age']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Age Histogram by Survival')\nplt.xlabel('Age (Years)')\nplt.ylabel('# of Passengers')\nplt.legend()","ab404f9e":"# plot distributions passengers age who survived or death\nc = sns.FacetGrid( train_data, hue = 'Survived', aspect=4 )\nc.map(sns.kdeplot, 'Age', shade= True )\nc.set(xlim=(0 , train_data['Age'].max()))\nc.add_legend()","34a7475c":"sns.heatmap(train_data.corr(), annot = True)","424b039b":"#as we see there is 2 missing values in Embarked column in train data\nprint(train_data.isnull().sum())","4361c9eb":"#lets fill the NAN values of embarked column \nsi = SimpleImputer(strategy=\"most_frequent\")\n\ntrain_data['Embarked'] = si.fit_transform(train_data['Embarked'].values.reshape(-1,1))\ntest_data['Embarked']  = si.fit_transform(test_data['Embarked'].values.reshape(-1,1))\n\n#train_data['Embarked'].head()\nprint(train_data.isnull().sum())","f2b6d2d9":"train_data[\"Sex\"][train_data[\"Sex\"] == \"male\"] = 0\ntrain_data[\"Sex\"][train_data[\"Sex\"] == \"female\"] = 1\n\ntest_data[\"Sex\"][test_data[\"Sex\"] == \"male\"] = 0\ntest_data[\"Sex\"][test_data[\"Sex\"] == \"female\"] = 1\n\ntrain_data[\"Embarked\"][train_data[\"Embarked\"] == \"S\"] = 0\ntrain_data[\"Embarked\"][train_data[\"Embarked\"] == \"C\"] = 1\ntrain_data[\"Embarked\"][train_data[\"Embarked\"] == \"Q\"] = 2\n\ntest_data[\"Embarked\"][test_data[\"Embarked\"] == \"S\"] = 0\ntest_data[\"Embarked\"][test_data[\"Embarked\"] == \"C\"] = 1\ntest_data[\"Embarked\"][test_data[\"Embarked\"] == \"Q\"] = 2","37ddb6de":"# to remember... stille 1 missing float value in Fare colum (test_data)\n#complete missing fare with median\n\nsi_Fare = SimpleImputer(missing_values=np.nan, strategy='mean')\ntest_data['Fare'] = si_Fare.fit_transform(test_data['Fare'].values.reshape(-1,1))\ntest_data.info()","5be9551d":"#lets scaling data in Fare and Age columns\n\nmmx = MinMaxScaler()\n\ntrain_data['Fare'] = mmx.fit_transform(train_data['Fare'].values.reshape(-1,1))\ntest_data['Fare']  = mmx.fit_transform(test_data['Fare'].values.reshape(-1,1))\n\ntrain_data['Age'] = mmx.fit_transform(train_data['Age'].values.reshape(-1,1))\ntest_data['Age'] = mmx.fit_transform(test_data['Age'].values.reshape(-1,1))\n\ntrain_data","6bf6157e":"# Searching for the titles and extracting them from the names column (train and test)\n\ntrain_data['Title'] = train_data['Name'].map(lambda x: re.compile(\"([A-Za-z]+)\\.\").search(x).group())\ntest_data['Title'] = test_data['Name'].map(lambda x: re.compile(\"([A-Za-z]+)\\.\").search(x).group())\nprint(train_data['Title'].unique())","2030351e":"print(test_data['Title'].unique())","33b69e20":"# so we have titles for Nobels like Master, Capt...and others for regular people..\n# so lets replace Nobels people by Dummy value 1 and regular people by Dummy value 0\ntitle_mapping = {'Mr.': 0, 'Mrs.': 0, 'Miss.': 0, 'Master.' : 1,'Don.': 1, 'Rev.' : 1,'Dr.' : 1,'Mme.': 0, 'Ms.': 0, 'Major.': 1,\n 'Lady.': 1, 'Sir.': 1, 'Mlle.': 0, 'Col.': 1, 'Capt.': 1, 'Countess.': 1, 'Jonkheer.': 1,'Dona.': 1,}\n\ntrain_data['Title'] = train_data['Title'].map(title_mapping)\ntrain_data['Title'] = train_data['Title'].fillna(0)\n\n    \nprint(train_data['Title'].unique())","5bee758f":"title_mapping = {'Mr.': 0, 'Mrs.': 0, 'Miss.': 0, 'Master.' : 1,'Don.': 1, 'Rev.' : 1,'Dr.' : 1,'Mme.': 0, 'Ms.': 0, 'Major.': 1,\n 'Lady.': 1, 'Sir.': 1, 'Mlle.': 0, 'Col.': 1, 'Capt.': 1, 'Countess.': 1, 'Jonkheer.': 1,'Dona.': 1,}\n\ntest_data['Title'] = test_data['Title'].map(title_mapping)\ntest_data['Title'] = test_data['Title'].fillna(0)\n\nprint(test_data['Title'].unique())","05e336e2":"for n, i in enumerate(train_data[\"SibSp\"]):\n    if i != 0:\n     train_data[\"SibSp\"][n] = 1\n\n\nfor m, k in enumerate(train_data[\"Parch\"]):\n    if k != 0:\n     train_data[\"Parch\"][m] = 1","c6b36fe9":"#lets drop Name, Ticket and Cabin columns from train data set \nDS_train=train_data.drop(columns=[\"Cabin\",\"Name\",\"Ticket\"])\nDS_train","249b9f08":"#lets drop Name, Ticket, Cabin, columns from test data set\nDS_test=test_data.drop(columns=[\"Cabin\",\"Name\",\"Ticket\"])\nDS_test","0d5181f5":"#lets re-ordered columns in train dataset\nDS_train = DS_train[['PassengerId','Sex','Pclass','Age','Fare','SibSp','Parch','Embarked','Title','Survived']]\nDS_train","edf44b10":"#lets re-ordered columns in test dataset\nDS_test = DS_test[['PassengerId','Sex','Pclass','Age','Fare','SibSp','Parch','Embarked','Title']]\nDS_test","af82afda":"#creat dataset in features and target variable\nfeature_columns = ['Sex','Pclass','Age','Fare','SibSp','Parch','Embarked','Title']\nX = DS_train[feature_columns] # training Features (Predectors)\ny = DS_train['Survived']     # Target variable\nX","9478caef":"y","22839a66":"# split X and y into training and testing sets\nX_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.30,random_state=0)\n\n#Here, the Dataset is broken into two parts in a ratio of 70:30. It means 70% data will \n#be used for model training and 30% for model testing.\n#First, import the Logistic Regression module and create a Logistic Regression classifier object using LogisticRegression() \n#function.Then, fit your model on the train set using fit() and perform prediction on the test set using predict().\n#instantiate the model (using the default parameters)\n\n#test the accuracy of Logistic Regression.\nlogreg = LogisticRegression(max_iter = 30000)\n\n# fit the model with data\nlogreg.fit(X_train,y_train)\ny_pred=logreg.predict(X_val)\n\n\nacc_LOG = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_LOG)","d946e171":"#Model Evaluation using Confusion Matrix\n#A confusion matrix is a table that is used to evaluate the performance of a classification model. \n#You can also visualize the performance of an algorithm. \n#The fundamental of a confusion matrix is the number of correct and incorrect predictions are summed up class-wise.\n\n\ncnf_matrix = metrics.confusion_matrix(y_val, y_pred)\ncnf_matrix","77ddaf7c":"# Here, you can see the confusion matrix in the form of the array object. \n# The dimension of this matrix is 2*2 because this model is binary classification. \n# You have two classes 0 and 1. Diagonal values represent accurate predictions, \n# while non-diagonal elements are inaccurate predictions. In the output, 139 and 73 are actual predictions, \n# and 29 and 27 are incorrect predictions.\n\nclass_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","317ad9ae":"accuracy=((139+73)\/(139+29+27+73))\nprint('accuracy is: ', (round(accuracy, 2)*100))","eebef2e3":"#test the accuracy of Support Vector Machines.\n\nsvc = SVC()\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_val)\nacc_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_svc)","54a16143":"# test the accuracy of Linear SVC\n\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, y_train)\ny_pred = linear_svc.predict(X_val)\nacc_linear_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_linear_svc)","2d39e35d":"#test the accuracy of Decision Tree\n\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(X_train, y_train)\ny_pred = decisiontree.predict(X_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_decisiontree)","d82c3b98":"# test the accuracy of Random Forest\nrandomforest = RandomForestClassifier()\nrandomforest.fit(X_train, y_train)\ny_pred = randomforest.predict(X_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)","3b91d4ac":"# test the accuracy of Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, y_train)\ny_pred = perceptron.predict(X_val)\nacc_perceptron = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_perceptron)","012a319c":"# test the accuracy of KNN or k-Nearest Neighbors\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_val)\nacc_knn = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_knn)","05398f16":"# test the accuracy of Stochastic Gradient Descent\nsgd = SGDClassifier()\nsgd.fit(X_train, y_train)\ny_pred = sgd.predict(X_val)\nacc_sgd = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_sgd)","a2b1bb81":"# test the accuracy of Gradient Boosting Classifier\ngbc = GradientBoostingClassifier()\ngbc.fit(X_train, y_train)\ny_pred = gbc.predict(X_val)\nacc_gbc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbc)","5ac80bb3":"# Gaussian Naive Bayes\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\ny_pred = gaussian.predict(X_val)\nacc_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gaussian)","20a19349":"from catboost import CatBoostClassifier, cv, Pool\n\nclf =CatBoostClassifier(eval_metric='Accuracy',use_best_model=True,random_seed=2)\nclf.fit(X_train,y_train, eval_set=(X_val,y_val), early_stopping_rounds=100,verbose=False)\n#,cat_features=cate_features_index,\ny_pred = clf.predict(X_val)\nacc_clf = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_clf)","ab86f7a9":"# Let's compare the accuracies of each model\n\nmodels = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier','Cat Boost Classifier'],\n    'Accuracy': [acc_svc, acc_knn, acc_LOG, \n              acc_randomforest, acc_gaussian, acc_perceptron,acc_linear_svc, acc_decisiontree,\n              acc_sgd, acc_gbc, acc_clf]})\nmodels.sort_values(by='Accuracy', ascending=False)","e1097ed3":"#set the output as a dataframe and convert to csv file named submission.csv\n\nids = DS_test['PassengerId']\npredictions = gbc.predict(DS_test.drop('PassengerId', axis=1))\n\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })","e3371688":"output.to_csv('submission.csv', index=False)","45c671fc":"I am a new in data science and machine learning, and will be attempting to work my way through the Titanic: \nMachine Learning from Disaster dataset. Please consider upvoting if this is useful to you! ","967b0f62":"Parch don't have big effect on numbers of survived people...","667994ae":"# Data Visualization","7082f372":"# Explore Dataset","056dee4c":"as we see 177 nul values in Ages column, 687 in cabin and 2 in Embarked AGE, \nand CABIN have the higest number of Null Values.\nso CABIN will not be of major help since most of the values are missing.\nlets discover how many values is null in test dataset column","aca998c1":"Embarked don't have big effect on numbers of survived people...","10c53eb3":"# Import Library","88746f74":"Numerical Features: Age (Continuous), Fare (Continuous), SibSp (Discrete), Parch (Discrete)\nCategorical Features: Survived, Sex, Embarked, Pclass\nAlphanumeric Features: Ticket, Cabin","8fbbbc9e":"After explored features in dataset,\nnow we shall draw close comparisons with \"SURVIVED\" feature,to help us to get clear idea about important features","cfae5adb":"SibSp don't have big effect on numbers of survived people...","f703172b":"Pclass: People of higher socioeconomic class have more chance to survive..almost, people not survived are in Pclass 3....","11e8da89":"the Gradient Boosting Classifier model will be used for the testing data.","ca8a2515":"as we see befor The SibSp and Parch dataset defines family relations... \nso no need to know how many family members are traveled with survivors...\nbut we need to know if travelers are alone or with someone else...\nso when SibSp and Parch values = 0, we will keep it.. and any other values we will replaced by 1..\nits mean travelers embarque with someone","19d75399":"now no missing value in Embarked...lets \nreplace S by 0, C to 1 and Q to 2 using maping","81879915":"Sex: Females have more chance to survive.","27f742c3":"approximately similar to 79.1","f6517200":"# Data Cleaning","ee8c5710":"# SUBMISSION FILE(choosing Gradient Boosting)","e932c23f":"# Data Modeling","21fc82f0":"Age: Young have more chance to survive.","83b22e0d":"Fare: when Fare value increase...people have more chance to survive."}}