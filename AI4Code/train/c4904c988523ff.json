{"cell_type":{"8fc29413":"code","92006cfb":"code","d97484fc":"code","89c9ed55":"code","7607ea3d":"code","ea83b2ad":"code","cc27210c":"code","f1ed9895":"markdown"},"source":{"8fc29413":"# Put your own project id here\nPROJECT_ID = 'kaggle-bq-test-248917'\n\n# create a client instance for your project\nfrom google.cloud import bigquery\nclient = bigquery.Client(project=PROJECT_ID, location=\"US\")","92006cfb":"# create small array & save it as a csv\n# (This is too small to warrent using BigQuery, it's\n# just here as an example)\nimport numpy\na = numpy.asarray([ [1,2,3], [4,5,6], [7,8,9] ])\nnumpy.savetxt(\"foo.csv\", a, delimiter=\",\")","d97484fc":"# create a new datset\nclient.create_dataset(\"new_dataset\")","89c9ed55":"# create a new table in that dataset ()\nclient.create_table(f\"{PROJECT_ID}.new_dataset.new_table\")","7607ea3d":"# some variables\nfilename = 'foo.csv' # this is the file path to your csv\ndataset_id = 'new_dataset'\ntable_id = 'new_table'\n\n# tell the client everything it needs to know to upload our csv\ndataset_ref = client.dataset(dataset_id)\ntable_ref = dataset_ref.table(table_id)\njob_config = bigquery.LoadJobConfig()\njob_config.source_format = bigquery.SourceFormat.CSV\njob_config.autodetect = True\n\n# load the csv into bigquery\nwith open(filename, \"rb\") as source_file:\n    job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n\njob.result()  # Waits for table load to complete.\n\n# looks like everything worked :)\nprint(\"Loaded {} rows into {}:{}.\".format(job.output_rows, dataset_id, table_id))","ea83b2ad":"# query (you won't want to use SELECT * unless your dataset is very small)\nquery = f\"\"\" SELECT * \n        FROM `{PROJECT_ID}.new_dataset.new_table`\"\"\"\n\n# Set up the query\nquery_job = client.query(query)\n\n# API request - run the query, and return a pandas DataFrame\ndata = query_job.to_dataframe()","cc27210c":"# check out the data we got back :)\ndata","f1ed9895":"This notebook will show you how to upload a .csv file as BigQuery dataset. \n\nIn order to upload a dataset, you'll need to link a GCP account with write permissions for a BigQuery project. You can see a tutorial on how to do that [in this blog post](http:\/\/blog.kaggle.com\/2019\/06\/24\/kaggle-kernels-notebooks-now-offers-bigquery\/). "}}