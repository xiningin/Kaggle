{"cell_type":{"b1046bba":"code","765d0063":"code","19335056":"code","57c54449":"code","3f3cfd96":"code","265b1e64":"code","8c717cae":"code","52330a12":"code","1b99b0f2":"code","691fd487":"code","2e157eee":"code","2821733f":"code","6e894848":"code","13831c28":"code","80b36cc3":"code","b46eaf00":"code","e5e62e56":"code","bb8f6c40":"code","27a90c1f":"code","d7859d37":"code","9be0ede4":"code","2b9211e7":"code","2f933c60":"code","3dad0154":"code","31387741":"code","6dd6685a":"markdown","bcd4e252":"markdown","6aefae99":"markdown","f03ff09c":"markdown","505c091a":"markdown","6af9a5d7":"markdown","61b22df2":"markdown","fbc0400b":"markdown","60aa9b75":"markdown","0f86b19a":"markdown","d84c469b":"markdown","76fc66f7":"markdown","4c4e7362":"markdown","daa718c4":"markdown","5ef46c55":"markdown","a5601684":"markdown","61869ef8":"markdown","5fbe298a":"markdown"},"source":{"b1046bba":"import io\nimport openpyxl\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","765d0063":"#For this exercise I understand it\u00b4s a better exercise if we do the analysis by country in order to have a more accurate analyzis by market\n\n# geo = input(\"Select the country you\u00b4d like to analyze (France \/ Germany \/ Spain): \")\n\nchurn_ds = pd.read_csv(\"..\/input\/predicting-churn-for-bank-customers\/Churn_Modelling.csv\", sep=\",\")\nchurn_ds = churn_ds.loc[churn_ds[\"Geography\"] == \"Germany\"]\n\nchurn_ds","19335056":"#Checking a dataset sample\n\npd.set_option(\"display.max_rows\", 100)\npd.set_option(\"display.max_columns\", 100)\npd.options.display.float_format=\"{:,.2f}\".format\nchurn_ds.sample(n=10, random_state=0)","57c54449":"#Checking dataset info by feature\n\nchurn_ds.info(verbose=True, null_counts=True)","3f3cfd96":"#Checking the existence of zeros in rows\n\n(churn_ds==0).sum(axis=0).to_excel(\"zeros_per_feature.xlsx\")\n(churn_ds==0).sum(axis=0)","265b1e64":"#Checking the existence of duplicated rows\n\nchurn_ds.duplicated().sum()","8c717cae":"#Checking data balancing (for classification)\n\ndata_balancing = pd.DataFrame()\ndata_balancing[\"Count\"] = churn_ds[\"Exited\"].value_counts()\ndata_balancing[\"Count%\"] = churn_ds[\"Exited\"].value_counts()\/churn_ds.shape[0]*100\n\ndata_balancing","52330a12":"#Checking basic statistical data by feature\n\nchurn_ds.describe(include=\"all\")","1b99b0f2":"#1\n\nchurn_ds[\"BalanceSalaryProportion\"] = (churn_ds[\"Balance\"]\/churn_ds[\"EstimatedSalary\"])\nchurn_ds.reset_index(drop=True)","691fd487":"#2\n\nchurn_ds.drop([\"RowNumber\", \"CustomerId\", \"Surname\", \"Geography\"], axis=1, inplace=True)","2e157eee":"#3\n\nchurn_ds = pd.concat([churn_ds, pd.get_dummies(churn_ds[\"Gender\"], prefix=\"Gender\")], axis=1)\n\nchurn_ds.to_excel(\"churn_ds_clean.xlsx\")","2821733f":"#Plotting Categorical Variables\n\nfig, ax = plt.subplots(1, 2)\nchurn_ds[\"Exited\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nchurn_ds[\"Exited\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Exited Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\nfig, ax = plt.subplots(1, 2)\nchurn_ds[\"Gender\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nchurn_ds[\"Gender\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Gender Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\nfig, ax = plt.subplots(1, 2)\nchurn_ds[\"HasCrCard\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nchurn_ds[\"HasCrCard\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"HasCrCard Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\nfig, ax = plt.subplots(1, 2)\nchurn_ds[\"IsActiveMember\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nchurn_ds[\"IsActiveMember\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"IsActiveMember Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)","6e894848":"#Plotting Numerical Variables\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"CreditScore Distribution\", fontsize=15)\nsns.distplot(churn_ds[\"CreditScore\"], ax=ax[0])\nsns.boxplot(churn_ds[\"CreditScore\"], ax=ax[1])\nsns.violinplot(churn_ds[\"CreditScore\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Age Distribution\", fontsize=15)\nsns.distplot(churn_ds[\"Age\"], ax=ax[0])\nsns.boxplot(churn_ds[\"Age\"], ax=ax[1])\nsns.violinplot(churn_ds[\"Age\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Tenure Distribution\", fontsize=15)\nsns.distplot(churn_ds[\"Tenure\"], ax=ax[0])\nsns.boxplot(churn_ds[\"Tenure\"], ax=ax[1])\nsns.violinplot(churn_ds[\"Tenure\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Balance Distribution\", fontsize=15)\nsns.distplot(churn_ds[\"Balance\"], ax=ax[0])\nsns.boxplot(churn_ds[\"Balance\"], ax=ax[1])\nsns.violinplot(churn_ds[\"Balance\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"NumOfProducts Distribution\", fontsize=15)\nsns.distplot(churn_ds[\"NumOfProducts\"], ax=ax[0])\nsns.boxplot(churn_ds[\"NumOfProducts\"], ax=ax[1])\nsns.violinplot(churn_ds[\"NumOfProducts\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"EstimatedSalary Distribution\", fontsize=15)\nsns.distplot(churn_ds[\"EstimatedSalary\"], ax=ax[0])\nsns.boxplot(churn_ds[\"EstimatedSalary\"], ax=ax[1])\nsns.violinplot(churn_ds[\"EstimatedSalary\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"BalanceSalaryProportion Distribution\", fontsize=15)\nsns.distplot(churn_ds[\"BalanceSalaryProportion\"], ax=ax[0])\nsns.boxplot(churn_ds[\"BalanceSalaryProportion\"], ax=ax[1])\nsns.violinplot(churn_ds[\"BalanceSalaryProportion\"], ax=ax[2])","13831c28":"#Alternatively using Profile Report to see variables statistics and correlations\n\n# from pandas_profiling import ProfileReport\n# profile = ProfileReport(churn_ds, title=\"Customer Index\")\n# profile.to_file(output_file=\"Customer_Churn.html\")","80b36cc3":"#Deleting original categorical columns\n\nchurn_ds.drop([\"Gender\"], axis=1, inplace=True)\n\n#Plotting a Heatmap\n\nfig, ax = plt.subplots(1, figsize=(25,25))\nsns.heatmap(churn_ds.corr(), annot=True, fmt=\",.2f\")\nplt.title(\"Heatmap Correlation\", fontsize=20)\nplt.tick_params(labelsize=12)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\n#Plotting a Pairplot\n\nsns.pairplot(churn_ds)","b46eaf00":"#Plotting a Feature Importance\n\nfrom xgboost import XGBClassifier\nfrom matplotlib import pyplot\n#Defining Xs and y\nX = churn_ds.drop([\"Exited\"], axis=1)\ny = churn_ds[\"Exited\"]\n#Defining the model\nmodel = XGBClassifier().fit(X, y)\n#Getting importance\nimportance = model.feature_importances_\n#Summarizing feature importance\nfor i,v in enumerate(importance):\n    print(\"Feature:{0:}, Score:{1:,.4f}\".format(X.columns[i], v))\n#Plotting feature importance\npd.Series(model.feature_importances_[::-1], index=X.columns[::-1]).plot(kind=\"barh\", figsize=(25,25))","e5e62e56":"#Defining Xs and y\n\nX = churn_ds[[\"NumOfProducts\", \"IsActiveMember\", \"Age\", \"Gender_Female\", \"Balance\", \"HasCrCard\", \"BalanceSalaryProportion\", \"EstimatedSalary\", \"CreditScore\", \"Tenure\"]]\ny = churn_ds[[\"Exited\"]]\n\n#Scaling all features\n\nfrom sklearn.preprocessing import MinMaxScaler\nsc_X = MinMaxScaler()\nX_scaled = sc_X.fit_transform(X)\nX_scaled = pd.DataFrame(X_scaled)\n\n#Setting train\/test split\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=0)","bb8f6c40":"#Creating a Logistic Regression model and checking its Metrics\n\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n\n#Trying different polynomial degrees\ndegrees = [1, 2, 3, 4, 5]\nprint(\"Testing degrees:\")\nfor a in degrees:\n    poly = PolynomialFeatures(degree=a)\n    X_train_degree = poly.fit_transform(X_train)\n    X_test_degree = poly.fit_transform(X_test)\n    model_lr = linear_model.LogisticRegression(max_iter=1000000000).fit(X_train_degree, y_train.values.ravel())\n    y_preds_train = model_lr.predict(X_train_degree)\n    y_preds_test = model_lr.predict(X_test_degree)\n    accuracy_train = accuracy_score(y_train, y_preds_train)\n    accuracy_test = accuracy_score(y_test, y_preds_test)\n    precision_train = precision_score(y_train, y_preds_train)\n    precision_test = precision_score(y_test, y_preds_test)\n    recall_train = recall_score(y_train, y_preds_train)\n    recall_test = recall_score(y_test, y_preds_test)\n    f1_train = f1_score(y_train, y_preds_train)\n    f1_test = f1_score(y_test, y_preds_test)\n    print(\"Train: Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_train, precision_train, recall_train, f1_train))\n    print(\"Test : Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\")\n\n#Choosing the best polynomial degree\nchosen_degree = 4\npoly = PolynomialFeatures(degree=chosen_degree)\n\n#Working on X_train & X_test in the polynomial chosen degree\nX_train_degree = poly.fit_transform(X_train)\nX_test_degree = poly.fit_transform(X_test)\n\n#Fitting to the model\nmodel_lr = linear_model.LogisticRegression(max_iter=1000000000).fit(X_train_degree, y_train.values.ravel())\nprint(f\"Linear Regression Intercept: {model_lr.intercept_}\")\nprint(f\"Linear Regression Coefficients: {model_lr.coef_}, \\n\")\n\n#Getting the predictions & Metrics\ny_preds_train = model_lr.predict(X_train_degree)\ny_preds_test = model_lr.predict(X_test_degree)\naccuracy_train = accuracy_score(y_train, y_preds_train)\naccuracy_test = accuracy_score(y_test, y_preds_test)\nprecision_train = precision_score(y_train, y_preds_train)\nprecision_test = precision_score(y_test, y_preds_test)\nrecall_train = recall_score(y_train, y_preds_train)\nrecall_test = recall_score(y_test, y_preds_test)\nf1_train = f1_score(y_train, y_preds_train)\nf1_test = f1_score(y_test, y_preds_test)\nprint(\"Chosen degree:\")\nprint(\"Train: Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_degree, accuracy_train, precision_train, recall_train, f1_train))\nprint(\"Test : Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_degree, accuracy_test, precision_test, recall_test, f1_test))\n# print(\"\\nConfusion matrix:\")\n# confusion_matrix = pd.crosstab(y_test, y_preds_test, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n# print(f\"{confusion_matrix}, \\n\")\n# sns.heatmap(confusion_matrix, annot=True, fmt='0f')\n\n#Visualizing y_pred in the dataset\nX_degree = poly.fit_transform(X_scaled)\ny_preds_all = model_lr.predict(X_degree)\nchurn_ds[\"Exited_predicted\"] = y_preds_all\nchurn_ds.to_excel(\"model_lr.xlsx\")","27a90c1f":"#Creating a SVM model and checking its Metrics\n\nfrom sklearn import svm\n\n#Fitting to the model\nmodel_svm = svm.SVC().fit(X_train, y_train.values.ravel())\n\n#Getting the predictions & Metrics\ny_preds_train = model_svm.predict(X_train)\ny_preds_test = model_svm.predict(X_test)\naccuracy_train = accuracy_score(y_train, y_preds_train)\naccuracy_test = accuracy_score(y_test, y_preds_test)\nprecision_train = precision_score(y_train, y_preds_train)\nprecision_test = precision_score(y_test, y_preds_test)\nrecall_train = recall_score(y_train, y_preds_train)\nrecall_test = recall_score(y_test, y_preds_test)\nf1_train = f1_score(y_train, y_preds_train)\nf1_test = f1_score(y_test, y_preds_test)\nprint(\"Train: Accuracy:{0:,.3f}, Precision:{1:,.3f}, Recall:{2:,.3f}, F1:{3:,.3f}\".format(accuracy_train, precision_train, recall_train, f1_train))\nprint(\"Test : Accuracy:{0:,.3f}, Precision:{1:,.3f}, Recall:{2:,.3f}, F1:{3:,.3f}\".format(accuracy_test, precision_test, recall_test, f1_test))\n# print(\"\\nConfusion matrix:\")\n# confusion_matrix = pd.crosstab(y_test, y_preds_test, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n# print(f\"{confusion_matrix}, \\n\")\n# sns.heatmap(confusion_matrix, annot=True, fmt='0f')\n\n#Visualizing y_pred in the dataset\ny_preds_all = model_svm.predict(X_scaled)\nchurn_ds[\"Exited_predicted\"] = y_preds_all\nchurn_ds.to_excel(\"model_svm.xlsx\")","d7859d37":"#Creating a Naive Bayes model and checking its Metrics\n\nfrom sklearn import naive_bayes\n\n#Fitting to the model\nmodel_nb = naive_bayes.MultinomialNB().fit(X_train, y_train.values.ravel())\n\n#Getting the predictions & Metrics\ny_preds_train = model_nb.predict(X_train)\ny_preds_test = model_nb.predict(X_test)\naccuracy_train = accuracy_score(y_train, y_preds_train)\naccuracy_test = accuracy_score(y_test, y_preds_test)\nprecision_train = precision_score(y_train, y_preds_train)\nprecision_test = precision_score(y_test, y_preds_test)\nrecall_train = recall_score(y_train, y_preds_train)\nrecall_test = recall_score(y_test, y_preds_test)\nf1_train = f1_score(y_train, y_preds_train)\nf1_test = f1_score(y_test, y_preds_test)\nprint(\"Train: Accuracy:{0:,.3f}, Precision:{1:,.3f}, Recall:{2:,.3f}, F1:{3:,.3f}\".format(accuracy_train, precision_train, recall_train, f1_train))\nprint(\"Test : Accuracy:{0:,.3f}, Precision:{1:,.3f}, Recall:{2:,.3f}, F1:{3:,.3f}\".format(accuracy_test, precision_test, recall_test, f1_test))\n# print(\"\\nConfusion matrix:\")\n# confusion_matrix = pd.crosstab(y_test, y_preds_test, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n# print(f\"{confusion_matrix}, \\n\")\n# sns.heatmap(confusion_matrix, annot=True, fmt='0f')\n\n#Visualizing y_pred in the dataset\ny_preds_all = model_nb.predict(X_scaled)\nchurn_ds[\"Exited_predicted\"] = y_preds_all\nchurn_ds.to_excel(\"model_nb.xlsx\")","9be0ede4":"#Creating a KNN model and checking its Metrics\n\nfrom sklearn import neighbors\n\n#Trying different neighbors\nn_neighbors = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\nprint(\"Testing neighbors:\")\nfor a in n_neighbors:\n    model_knn = neighbors.KNeighborsClassifier(n_neighbors=a).fit(X_train, y_train.values.ravel())\n    y_preds_train = model_knn.predict(X_train)\n    y_preds_test = model_knn.predict(X_test)\n    accuracy_train = accuracy_score(y_train, y_preds_train)\n    accuracy_test = accuracy_score(y_test, y_preds_test)\n    precision_train = precision_score(y_train, y_preds_train)\n    precision_test = precision_score(y_test, y_preds_test)\n    recall_train = recall_score(y_train, y_preds_train)\n    recall_test = recall_score(y_test, y_preds_test)\n    f1_train = f1_score(y_train, y_preds_train)\n    f1_test = f1_score(y_test, y_preds_test)\n    print(\"Train: Neighbors:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_train, precision_train, recall_train, f1_train))\n    print(\"Test : Neighbors:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\")\n\n#Choosing the best neighbor\nchosen_neighbor = 13\nmodel_knn = neighbors.KNeighborsClassifier(n_neighbors=chosen_neighbor).fit(X_train, y_train.values.ravel())\ny_preds_train = model_knn.predict(X_train)\ny_preds_test = model_knn.predict(X_test)\naccuracy_train = accuracy_score(y_train, y_preds_train)\naccuracy_test = accuracy_score(y_test, y_preds_test)\nprecision_train = precision_score(y_train, y_preds_train)\nprecision_test = precision_score(y_test, y_preds_test)\nrecall_train = recall_score(y_train, y_preds_train)\nrecall_test = recall_score(y_test, y_preds_test)\nf1_train = f1_score(y_train, y_preds_train)\nf1_test = f1_score(y_test, y_preds_test)\nprint(\"Chosen neighbors:\")\nprint(\"Train: Neighbors:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_neighbor, accuracy_train, precision_train, recall_train, f1_train))\nprint(\"Test : Neighbors:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_neighbor, accuracy_test, precision_test, recall_test, f1_test))\n# print(\"\\nConfusion matrix:\")\n# confusion_matrix = pd.crosstab(y_test, y_preds_test, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n# print(f\"{confusion_matrix}, \\n\")\n# sns.heatmap(confusion_matrix, annot=True, fmt='0f')\n\n#Visualizing y_pred in the dataset\ny_preds_all = model_knn.predict(X_scaled)\nchurn_ds[\"Exited_predicted\"] = y_preds_all\nchurn_ds.to_excel(\"model_knn.xlsx\")","2b9211e7":"#Creating a Random Forest model and checking its Metrics\n\nfrom sklearn import ensemble\n\n#Trying different depths\ndepths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\nprint(\"Testing depths:\")\nfor a in depths:\n    model_rf = ensemble.RandomForestClassifier(max_depth=a, random_state=0).fit(X_train, y_train.values.ravel())\n    y_preds_train = model_rf.predict(X_train)\n    y_preds_test = model_rf.predict(X_test)\n    accuracy_train = accuracy_score(y_train, y_preds_train)\n    accuracy_test = accuracy_score(y_test, y_preds_test)\n    precision_train = precision_score(y_train, y_preds_train)\n    precision_test = precision_score(y_test, y_preds_test)\n    recall_train = recall_score(y_train, y_preds_train)\n    recall_test = recall_score(y_test, y_preds_test)\n    f1_train = f1_score(y_train, y_preds_train)\n    f1_test = f1_score(y_test, y_preds_test)\n    print(\"Train: Depth:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_train, precision_train, recall_train, f1_train))\n    print(\"Test : Depth:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\")\n\n#Choosing the best depth\nchosen_depth = 8\nmodel_rf = ensemble.RandomForestClassifier(max_depth=chosen_depth, random_state=0).fit(X_train, y_train.values.ravel())\ny_preds_train = model_rf.predict(X_train)\ny_preds_test = model_rf.predict(X_test)\naccuracy_train = accuracy_score(y_train, y_preds_train)\naccuracy_test = accuracy_score(y_test, y_preds_test)\nprecision_train = precision_score(y_train, y_preds_train)\nprecision_test = precision_score(y_test, y_preds_test)\nrecall_train = recall_score(y_train, y_preds_train)\nrecall_test = recall_score(y_test, y_preds_test)\nf1_train = f1_score(y_train, y_preds_train)\nf1_test = f1_score(y_test, y_preds_test)\nprint(\"Chosen depth:\")\nprint(\"Train: Depth:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_depth, accuracy_train, precision_train, recall_train, f1_train))\nprint(\"Test : Depth:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_depth, accuracy_test, precision_test, recall_test, f1_test))\n# print(\"\\nConfusion matrix:\")\n# confusion_matrix = pd.crosstab(y_test, y_preds_test, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n# print(f\"{confusion_matrix}, \\n\")\n# sns.heatmap(confusion_matrix, annot=True, fmt='0f')\n\n#Visualizing y_pred in the dataset\ny_preds_all = model_rf.predict(X_scaled)\nchurn_ds[\"Exited_predicted\"] = y_preds_all\nchurn_ds.to_excel(\"model_rf.xlsx\")","2f933c60":"#Creating a XGBoost model and checking its Metrics\n\nfrom xgboost import XGBClassifier\n\n#Trying different depths\ndepths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\nprint(\"Testing depths:\")\nfor a in depths:\n    model_xgbc = XGBClassifier(max_depth=a, objective=\"multi:softmax\", num_class=4, random_state=0).fit(X_train, y_train.values.ravel())\n    y_preds_train = model_xgbc.predict(X_train)\n    y_preds_test = model_xgbc.predict(X_test)\n    accuracy_train = accuracy_score(y_train, y_preds_train)\n    accuracy_test = accuracy_score(y_test, y_preds_test)\n    precision_train = precision_score(y_train, y_preds_train)\n    precision_test = precision_score(y_test, y_preds_test)\n    recall_train = recall_score(y_train, y_preds_train)\n    recall_test = recall_score(y_test, y_preds_test)\n    f1_train = f1_score(y_train, y_preds_train)\n    f1_test = f1_score(y_test, y_preds_test)\n    print(\"Train: Depth:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_train, precision_train, recall_train, f1_train))\n    print(\"Test : Depth:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\")\n\n#Choosing the best depth\nchosen_depth = 1\nmodel_xgbc = XGBClassifier(max_depth=chosen_depth, objective=\"multi:softmax\", num_class=4, random_state=0).fit(X_train, y_train.values.ravel())\ny_preds_train = model_xgbc.predict(X_train)\ny_preds_test = model_xgbc.predict(X_test)\naccuracy_train = accuracy_score(y_train, y_preds_train)\naccuracy_test = accuracy_score(y_test, y_preds_test)\nprecision_train = precision_score(y_train, y_preds_train)\nprecision_test = precision_score(y_test, y_preds_test)\nrecall_train = recall_score(y_train, y_preds_train)\nrecall_test = recall_score(y_test, y_preds_test)\nf1_train = f1_score(y_train, y_preds_train)\nf1_test = f1_score(y_test, y_preds_test)\nprint(\"Chosen depth:\")\nprint(\"Train: Depth:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_depth, accuracy_train, precision_train, recall_train, f1_train))\nprint(\"Test : Depth:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_depth, accuracy_test, precision_test, recall_test, f1_test))\n# print(\"\\nConfusion matrix:\")\n# confusion_matrix = pd.crosstab(y_test, y_preds_test, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n# print(f\"{confusion_matrix}, \\n\")\n# sns.heatmap(confusion_matrix, annot=True, fmt='0f')\n\n#Visualizing y_pred in the dataset\ny_preds_all = model_xgbc.predict(X_scaled)\nchurn_ds[\"Exited_predicted\"] = y_preds_all\nchurn_ds.to_excel(\"model_xgbc.xlsx\")","3dad0154":"#Creating a Deep Learning model and checking its Metrics\n\nfrom keras import Sequential\nfrom keras.layers import Dense\n\n#Creating a model\nmodel_dl = Sequential()\n\n#Input and First Hidden Layer\nmodel_dl.add(Dense(units=256, activation=\"relu\", input_dim=X_train.shape[-1]))\n\n#Output Layer\nmodel_dl.add(Dense(units=1, activation=\"sigmoid\",))\n\n#Compiling the neural network\nmodel_dl.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"binary_accuracy\"])\n\n#Fitting to the model\nmodel_dl.fit(X_train, y_train.values.ravel(), epochs=250)\n\n#Getting the predictions & Metrics\ny_preds_train = model_dl.predict(X_train)\ny_preds_test = model_dl.predict(X_test)\naccuracy_train = accuracy_score(y_train, y_preds_train.round())\naccuracy_test = accuracy_score(y_test, y_preds_test.round())\nprecision_train = precision_score(y_train, y_preds_train.round())\nprecision_test = precision_score(y_test, y_preds_test.round())\nrecall_train = recall_score(y_train, y_preds_train.round())\nrecall_test = recall_score(y_test, y_preds_test.round())\nf1_train = f1_score(y_train, y_preds_train.round())\nf1_test = f1_score(y_test, y_preds_test.round())\nprint(\"Train: Accuracy:{0:,.3f}, Precision:{1:,.3f}, Recall:{2:,.3f}, F1:{3:,.3f}\".format(accuracy_train, precision_train, recall_train, f1_train))\nprint(\"Test : Accuracy:{0:,.3f}, Precision:{1:,.3f}, Recall:{2:,.3f}, F1:{3:,.3f}\".format(accuracy_test, precision_test, recall_test, f1_test))\n# print(\"\\nConfusion matrix:\")\n# from sklearn.metrics import confusion_matrix\n# confusion_matrix = confusion_matrix(y_test, y_preds_test)\n# print(f\"{confusion_matrix}, \\n\")\n# sns.heatmap(confusion_matrix, annot=True, fmt='.0f')\n\n#Visualizing y_pred in the dataset\ny_preds_all = model_dl.predict(X_scaled)\nchurn_ds[\"Exited_predicted\"] = y_preds_all\nchurn_ds.to_excel(\"model_dl.xlsx\")","31387741":"#Entering Xs\n\n# num_prods = int(input(\"Enter the client\u00b4s number of Products: \"))\n# is_act = str(input(\"Is the client an active member (Yes\/No)? \"))\n# if is_act == \"No\":\n#     is_act = 0\n# else:\n#     is_act = 1\n# age = float(input(\"Enter the client\u00b4s age: \"))\n# female = str(input(\"Enter the client gender (Male\/Female): \"))\n# if female == \"Male\":\n#     female = 0\n# else:\n#     female = 1\n# balance = float(input(\"Enter the client\u00b4s balance: \"))\n# has_cr_cd = str(input(\"Does the client have a credit card (Yes\/No)? \"))\n# if has_cr_cd == \"No\":\n#     has_cr_cd = 0\n# else:\n#     has_cr_cd = 1\n# estim_sal = float(input(\"Enter the client\u00b4s estimated salary: \"))\n# cred_score = int(input(\"Enter the client\u00b4s credit score: \"))\n# tenure = int(input(\"Enter the client\u00b4s tenure: \"))\n# bal_sal_prop= balance\/estim_sal\n\n#Defining Xs\n\n# X_mod_dep = pd.DataFrame({\"NumOfProducts\":[num_prods], \"IsActiveMember\": [is_act], \"Age\": [age], \n#                           \"Gender_Female\": [female], \"Balance\": [balance], \"HasCrCard\": [has_cr_cd], \n#                           \"BalanceSalaryProportion\": [bal_sal_prop], \"EstimatedSalary\": [estim_sal], \n#                           \"CreditScore\": [cred_score], \"Tenure\": [tenure]})\n#Choosing an specific client for testing:\nX_mod_dep = pd.DataFrame({\"NumOfProducts\":[4], \"IsActiveMember\": [0], \"Age\": [29], \n                          \"Gender_Female\": [1], \"Balance\": [115046], \"HasCrCard\": [1], \n                          \"BalanceSalaryProportion\": [0.9639], \"EstimatedSalary\": [119346], \n                          \"CreditScore\": [376], \"Tenure\": [4]})\n\n#Appending X_mod_dep to original X dataframe, so we can scale it all together next\n\nX_with_X_mode_dep = X.append(X_mod_dep)\nX_with_X_mode_dep.reset_index(drop=True)\n\n#Scaling all features\n\nfrom sklearn.preprocessing import MinMaxScaler\nsc_X = MinMaxScaler()\nX_scaled = sc_X.fit_transform(X_with_X_mode_dep)\nX_scaled = pd.DataFrame(X_scaled)\n\n#Recovering X_mod_dep row in dataframe after scaling\n\nX_mod_dep = X_scaled.tail(1)\n\n#Predicting results\n\nprediction = model_xgbc.predict(X_mod_dep).round()\nif prediction == 0:\n    prediction_answer = \"No\"\nelse:\n    prediction_answer = \"Yes\"\n\nprint(\"\")\nprint(f\"Is this client predicted as in risk of leaving the bank? {prediction_answer}.\")","6dd6685a":"# 9.4 KNN","bcd4e252":"# 1. Introduction: Business Goal & Problem Definition\n\nThis project\u00b4s goal is creating a model to identify the potential risk of clients leaving the company, what I understand is a crucial information for the business. The concerning dataset is called \"Predicting Churn for Bank Customers\" and it\u00b4s available in Kaggle. We\u00b4ll use the following 10 features below to create the model:\n\nIF YOU LIKE IT OR IF IT HELPS YOU SOMEHOW, COULD YOU PLEASE UPVOTE? THANK YOU VERY MUCH!!!\n\n* Credit Score\n* Geography\n* Gender\n* Age\n* Tenure\n* Balance\n* Number of products\n* Credit card possession\n* Is active member\n* Salary","6aefae99":"# 8. Data Modelling","f03ff09c":"# 6. Data Exploration","505c091a":"# 9.1 Logistic Regression","6af9a5d7":"# 9.5 Random Forest","61b22df2":"# 9.3 Naive Bayes","fbc0400b":"# 9.6 XGBoost","60aa9b75":"# 7. Correlations Analysis & Features Selection","0f86b19a":"# 9.2 SVM","d84c469b":"# 3. Data Collection","76fc66f7":"# 10. Model Deployment","4c4e7362":"# 4. Data Preliminary Exploration","daa718c4":"# 5. Data Preparation\n\n    We\u00b4ll perform the following:\n\n    1. Create a column that will be the \"Balance\" divided by the \"Salary\", what could potentially bring relevant information to the model\n\n\n    2. Remove columns that don\u00b4t add any value to the model: \"RowNumber\", \"CustomerId\", \"Surname\" and \"Geography\"\n\n\n    3. Convert categorical variables to dummies: \"Gender\"\n\n\n    * No duplicated rows found\n    * No outliers found","5ef46c55":"# 11. Conclusions\n\nIF YOU LIKE IT OR IF IT HELPS YOU SOMEHOW, COULD YOU PLEASE UPVOTE? THANK YOU VERY MUCH!!!\n\nIn this project we went through all the process from defining the business objective, collecting data, exploring features and distributions, treating data, understanding correlations, selecting relevant features, data modelling and presenting 7 different algorithms with metrics to select the best to predict the Customer\u00b4s risk of leaving our business, what\u00b4s crucial for the bank since with it we can start taking measures to avoid it, keeping the client portfolio. The chosen model was XGBoost, with around 83% accuracy.","a5601684":"# 9.7 Deep Learning","61869ef8":"# 9. Machine Learning Algorithms Implementation & Assessment","5fbe298a":"# 2. Importing Basic Libraries"}}