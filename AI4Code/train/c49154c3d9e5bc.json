{"cell_type":{"95dfc530":"code","f422d4b5":"code","67d0c7e4":"code","badc38eb":"code","6104fe7a":"code","c9f09f65":"code","d7885aa5":"code","e4d294a7":"code","bafb0778":"code","06717dfd":"code","70a62a9a":"code","accc73fc":"code","cb8f4c89":"code","ce8daed6":"code","7c21a5f4":"code","ef2d57b6":"code","03cc748b":"code","9adb3c22":"code","78c5cada":"code","9f066dac":"code","8efa5db1":"code","d9b298f8":"code","e6c8f371":"code","0fc36e66":"code","a40d0130":"code","2674bedf":"code","f6abae02":"code","f4aa1a62":"code","67a62e3f":"code","4964bdb6":"code","a956a229":"code","a93e5a6a":"code","19e0b318":"code","af1b0a91":"code","4b73d1ce":"code","f61dae20":"code","dd4d1f34":"code","8349d725":"code","b5911651":"code","bef2e6f8":"code","f8830ca4":"code","8de7ea58":"code","da42efdc":"code","8265a7f2":"code","f1dd9245":"code","c0fd67eb":"code","e3057649":"code","4dced7f3":"code","81f4da44":"code","76d14efc":"code","6c5df070":"code","29a66dad":"code","89262c81":"code","aaf9ff0d":"code","20c23590":"code","96bbb0cc":"code","12d4a2d8":"code","bcde82ad":"code","f21c28c6":"code","248b0e51":"code","7bbaa272":"markdown","4be90943":"markdown","dad486c5":"markdown","d34daaea":"markdown","a13cae25":"markdown","8544832f":"markdown","81d5a83b":"markdown","15224826":"markdown","89e438dd":"markdown","048ceb3f":"markdown","87fac648":"markdown","6f6ab7d5":"markdown","6bfb3466":"markdown","aa390e82":"markdown","03b8b73f":"markdown","daa55524":"markdown","7e8a68d1":"markdown","31903bb6":"markdown","41bb74bf":"markdown","8aad9782":"markdown","da69acd5":"markdown","b1248ca7":"markdown","bfee0445":"markdown","2fdbe191":"markdown","6f463933":"markdown","b03cc436":"markdown","b304100c":"markdown","c2ba09fc":"markdown","9807fc11":"markdown","dc8383c7":"markdown","18414b6c":"markdown","056acd31":"markdown","97d2e7ca":"markdown","e28d112b":"markdown","c95fd707":"markdown","270ea799":"markdown","7d72a333":"markdown","b34e3574":"markdown","50cd246f":"markdown","e278fe6f":"markdown","43df042f":"markdown","efccac34":"markdown","83023b03":"markdown","a9b17f50":"markdown","414ccae7":"markdown","ae787a2a":"markdown","d8e31c52":"markdown","1157298a":"markdown","6a53c57e":"markdown","46fa064a":"markdown","8a7caa4f":"markdown","94ed63a8":"markdown","b1bbf8a6":"markdown","17bcfa59":"markdown","2b85a527":"markdown","3de93b9c":"markdown","5e2eff29":"markdown","292bd6a0":"markdown"},"source":{"95dfc530":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\nfrom sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\nfrom sklearn.feature_selection import RFE\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.ensemble import RandomForestClassifier","f422d4b5":"#Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","67d0c7e4":"#Load preprocessed data\nfile_path = '\/kaggle\/input\/icu-admission-data-cleaning-and-exploration\/'\nfile_name = 'first_step_output_data.csv'\ndata = pd.read_csv(file_path + file_name, index_col = 0)\n\ndata.head()","badc38eb":"#Remove all data take after window 1\nmodel_1_cols = [x for x in data.columns if x[-1] not in [str(y) for y in range(2,6)]]\ndata_1 = data[model_1_cols]\n\ndata_1.head()","6104fe7a":"#Compute Pearson correlation\ndata_1_corr = data_1.corr()\n\nsns.heatmap(data_1_corr)\nplt.figure(figsize = (10, 8))","c9f09f65":"#Show correlation values in stacked format\ndef rank_correlation_score(data):\n    \n    #Stack correlation map into 3-columns format\n    stacked_corr = data.corr().stack().reset_index().rename(\n       columns = {'level_0': 'Feature_1',\n                  'level_1': 'Feature_2',\n                  0: 'Pearson_Corr'})\n    \n    #Remove redudant relationships\n    stacked_corr = stacked_corr.query('Feature_1 != Feature_2')\n    chained_feature_names = ['-'.join(np.sort(x)) for x in stacked_corr[['Feature_1', 'Feature_2']].values]\n    stacked_corr.loc[:,'Duplicate_Key'] = chained_feature_names\n    stacked_corr = stacked_corr.drop_duplicates(subset = 'Duplicate_Key').drop(columns = 'Duplicate_Key')\n\n    #Remove correlations to the target\n    stacked_corr = stacked_corr[stacked_corr['Feature_1'] != 'ICU']\n    stacked_corr = stacked_corr[stacked_corr['Feature_2'] != 'ICU']\n    \n    # Order absolute correlation strenght\n    stacked_corr['Pearson_Corr'] = abs(stacked_corr['Pearson_Corr'])\n    return stacked_corr.sort_values(by = 'Pearson_Corr', ascending = False)\n\nstacked_data_1_corr = rank_correlation_score(data_1)\nstacked_data_1_corr","d7885aa5":"#Filter very strong correlations\nstacked_data_1_corr[stacked_data_1_corr['Pearson_Corr'] > 0.99]","e4d294a7":"#Investigate MEAN\/MEDIAN correlations\nstacked_data_1_corr['MEASURE_FEATURE_1'] = [x.split('_')[0] for x in stacked_data_1_corr['Feature_1']]\nstacked_data_1_corr['MEASURE_FEATURE_2'] = [x.split('_')[0] for x in stacked_data_1_corr['Feature_2']]\nstacked_data_1_corr['TYPE_FEATURE_1'] = [x.split('_')[-2] for x in stacked_data_1_corr['Feature_1']]\nstacked_data_1_corr['TYPE_FEATURE_2'] = [x.split('_')[-2] for x in stacked_data_1_corr['Feature_2']]\n\nmean_median_corr = stacked_data_1_corr.query('MEASURE_FEATURE_1 == MEASURE_FEATURE_2')\nmean_median_corr = mean_median_corr.query('TYPE_FEATURE_1 != TYPE_FEATURE_2')\nmean_median_corr = mean_median_corr[mean_median_corr['TYPE_FEATURE_1'].isin(['MEDIAN', 'MEAN'])]\nmean_median_corr = mean_median_corr[mean_median_corr['TYPE_FEATURE_2'].isin(['MEDIAN', 'MEAN'])]\n\nrelevant_cols = ['Feature_1', 'Feature_2', 'Pearson_Corr']\nmean_median_corr[relevant_cols]","bafb0778":"#List columns to be removed\ncols_to_remove = ['BLOODPRESSURE_DIASTOLIC_MEDIAN_1', 'BLOODPRESSURE_SISTOLIC_MEDIAN_1', 'HEART_RATE_MEDIAN_1',\n                  'OXYGEN_SATURATION_MEDIAN_1', 'RESPIRATORY_RATE_MEDIAN_1', 'TEMPERATURE_MEDIAN_1']","06717dfd":"#Investigate DIFF\/DIFF_REL correlations\ndiff_corr = stacked_data_1_corr.query('MEASURE_FEATURE_1 == MEASURE_FEATURE_2')\ndiff_corr = diff_corr.query('TYPE_FEATURE_1 != TYPE_FEATURE_2')\ndiff_corr = diff_corr[diff_corr['TYPE_FEATURE_1'].isin(['DIFF', 'REL'])]\ndiff_corr = diff_corr[diff_corr['TYPE_FEATURE_2'].isin(['DIFF', 'REL'])]\n\ndiff_corr[relevant_cols]","70a62a9a":"#Add columns to the remove list\ncols_to_remove.extend(['BLOODPRESSURE_DIASTOLIC_DIFF_REL_1', 'BLOODPRESSURE_SISTOLIC_DIFF_REL_1', \n                       'HEART_RATE_DIFF_REL_1', 'OXYGEN_SATURATION_DIFF_REL_1', 'RESPIRATORY_RATE_DIFF_REL_1',\n                       'TEMPERATURE_DIFF_REL_1'])","accc73fc":"#Sort absolute correlations values to the target\ndata_1 = data_1.drop(columns = cols_to_remove)\ndata_1_target_corr = abs(data_1.corr()['ICU'])\n\ndata_1_target_corr[data_1_target_corr < 1].sort_values(ascending = False)","cb8f4c89":"data_1.head()","ce8daed6":"#Define function to encode features\ndef encode_feature(data, col):\n    new_cols = pd.get_dummies(data[col], prefix = col, prefix_sep = ':', drop_first = True)\n    return pd.concat([data.drop(columns = col), new_cols], axis = 1)","7c21a5f4":"#Encode AGE_PERCENTIL_1\ndata_1 = encode_feature(data_1, 'AGE_PERCENTIL_1')","ef2d57b6":"#Split data into train\/test and validation\nnp.random.seed(10)\n\ntarget_col = 'ICU'\nfeature_cols = data_1.drop(columns = ['ICU', 'PATIENT_VISIT_IDENTIFIER_1']).columns.values\n\nx_train, x_validation, y_train, y_validation = train_test_split(data_1[feature_cols], data_1[target_col],\n                                                                test_size = 0.1)","03cc748b":"#Define function to test algorithm\ndef score_model(estimator, train_data, validation_data, cv):\n    #Unpack data\n    x_train, y_train = train_data\n    x_validation, y_validation = validation_data\n    \n    #Perfomed cross-validation on train data\n    model_cv = cross_validate(estimator = estimator, X = x_train, y = y_train,\n                              scoring = ['accuracy', 'roc_auc'],\n                              cv = cv)\n    \n    #Apply model to validation data\n    estimator.fit(x_train, y_train)\n    y_pred = estimator.predict(x_validation)\n\n    #Print results\n    print('CV model accuracy:  %.3f +\/- %.3f'  %(model_cv['test_accuracy'].mean(), \n                                              model_cv['test_accuracy'].std()))\n    print('CV model roc_auc:  %.3f +\/- %.3f'  %(model_cv['test_roc_auc'].mean(), \n                                             model_cv['test_roc_auc'].std()))\n    print('Validation accuracy score: %.3f' %accuracy_score(y_validation, y_pred))\n    print('Validation ROC_AUC score: %.3f' %roc_auc_score(y_validation, y_pred))\n    \n    return estimator","9adb3c22":"#Test KNN model\nbaseline_model_1 = KNN(n_neighbors = 10, weights = 'distance')\nfitted_baseline_model_1 = score_model(estimator = baseline_model_1, \n                                      train_data = (x_train, y_train),\n                                      validation_data = (x_validation, y_validation),\n                                      cv = 10)","78c5cada":"#Test RandomForestClassifier model\nbaseline_model_2 = RandomForestClassifier()\nfitted_baseline_model_2 = score_model(estimator = baseline_model_2, \n                                      train_data = (x_train, y_train),\n                                      validation_data = (x_validation, y_validation),\n                                      cv = 10)","9f066dac":"#Plot feature importances for the Random Forest Model\nfeat_importances = pd.Series(data = fitted_baseline_model_2.feature_importances_,\n                             index = feature_cols).sort_values()\nfeat_importances.plot(kind = 'barh', figsize = (12, 11))","8efa5db1":"#Test RandomForest model for reduced dataset\nncols_to_keep = int(0.8 * len(feat_importances))\nreduced_feat_columns = feat_importances.nlargest(n = ncols_to_keep).index\n\nreduced_x_train = x_train[reduced_feat_columns]\nreduced_x_validation = x_validation[reduced_feat_columns]\n\nfitted_baseline_model_3 = score_model(estimator = baseline_model_2, \n                                      train_data = (reduced_x_train, y_train),\n                                      validation_data = (reduced_x_validation, y_validation),\n                                      cv = 10)AGE_PERCENTIL_1                        object","d9b298f8":"#Get RFE feature ranking and compare to RandomForestClassifier feature importance\nrfe_model = RandomForestClassifier(n_estimators = 10)\nfeature_selector = RFE(estimator = rfe_model, step = 1)\nfeature_selector.fit(x_train, y_train)\nfeature_ranking = pd.Series(data = feature_selector.ranking_, index = feature_cols).sort_values()\n\nimportance_scale = pd.concat([feature_ranking, feat_importances.rank(ascending = False)], axis = 1)\nimportance_scale = importance_scale.rename(columns = {0: 'RFE_ranking', 1: 'RFC_ranking'})\nimportance_scale.sort_values(by = ['RFE_ranking', 'RFC_ranking'])","e6c8f371":"#Test RandomForest model for RFE reduced dataset\ncols_to_keep = importance_scale[importance_scale['RFE_ranking'] == 1].index\nreduced_x_train = x_train[cols_to_keep]\nreduced_x_validation = x_validation[cols_to_keep]\n\nfitted_baseline_model_4 = score_model(estimator = baseline_model_2, \n                                      train_data = (reduced_x_train, y_train),\n                                      validation_data = (reduced_x_validation, y_validation),\n                                      cv = 10)","0fc36e66":"#Define hyperparameter space\nhyper_space = {\n    'n_estimators': [10, 100, 500],\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [3, 5, 10, None],\n    'max_features': ['sqrt', 'log2', None]\n}","a40d0130":"#Perform hyperparameter tuning by grid searching the defined space\ngrid_search = GridSearchCV(estimator = baseline_model_2, \n                           param_grid = hyper_space,\n                           scoring = 'roc_auc',\n                           cv = 10,\n                           n_jobs = 4,\n                           verbose = 1)\ngrid_search_results = grid_search.fit(x_train, y_train)","2674bedf":"#Look at the best performing set of hyperparameters and apply estimator on validation data\nprint(grid_search.best_params_)\n\nbest_gridsearch_model = grid_search.best_estimator_\nbest_gridsearch_model.fit(x_train, y_train)\ny_pred = best_gridsearch_model.predict(x_validation)\n\nprint('Validation accuracy: %.3f' %(accuracy_score(y_validation, y_pred)))\nprint('Validation ROC_AUC: %.3f' %(roc_auc_score(y_validation, y_pred)))","f6abae02":"data.head()","f4aa1a62":"#Split features into time variant and time constant\nfeatures_df = pd.DataFrame(data = data.columns.values, columns = ['Feature_name'])\nfeatures_df['Feature_group'] = [x[:-2] for x in features_df['Feature_name']]\n\nfeatures_df = features_df.join(on = 'Feature_group',\n                               other = features_df.groupby(by = 'Feature_group').count(),\n                               how = 'inner',\n                               rsuffix = '_count')\n\ntime_constant_features = features_df[features_df['Feature_name_count'] == 1]['Feature_name'].values\ntime_variant_features = [x for x in features_df['Feature_name'] if x not in time_constant_features]\n\ntime_constant_features = time_constant_features[time_constant_features != 'PATIENT_VISIT_IDENTIFIER_1']","67a62e3f":"#Define function to retrieve the two most recent time windows data\ndef last_two_windows_data(data, cols, reference_col):\n    \n    #Remove all records where there's only window-1 data available\n    reference_group = np.sort([x for x in cols if x[:-2] == reference_col[:-2]])\n    temp_data = data[data[reference_group[1]].isnull() == False].drop(columns = 'PATIENT_VISIT_IDENTIFIER_1')\n    \n    #Identify last two windows for each records\n    reference_df = temp_data[reference_group]\n    last_window = [len(reference_group) - x for x in reference_df.isnull().sum(axis = 1)]\n    last_window = [[x, y] for x, y in zip(reference_df.index, last_window)]\n    \n    df_list = []\n    #Extract desired columns for each record\n    for window in range(2,6):\n        index_portion = [x[0] for x in last_window if x[1] == window]\n        data_portion = temp_data[temp_data.index.isin(index_portion)]\n        \n        window_cols = [x for x in data_portion.columns if x[-1] in [str(window), str(window - 1)]]\n        data_portion = data_portion[window_cols]\n        \n        new_col_names = {x: x[:-1] + 'end' if x[-1] == str(window) else x[:-1] + 'begin' \n                         for x in data_portion.columns}\n        \n        df_list.append(data_portion.rename(columns = new_col_names))\n        \n    return pd.concat(df_list), last_window","4964bdb6":"#Get time variant features for the last two available time windows\ndata_2, last_window = last_two_windows_data(data = data.drop(columns = time_constant_features), \n                                            cols = time_variant_features, \n                                            reference_col = 'TEMPERATURE_MAX_1')\n\ndata_2.head()","a956a229":"#Merge time variant and time constant features together\ndata_2 = data_2.join(other = data[time_constant_features], how = 'inner')\ndata_2.head()","a93e5a6a":"#Plot last window distribution according to target value\nfig, axis = plt.subplots(1,2, figsize = (8, 10))\ncolors = ['#ff9999','#66b3ff','#99ff99','#ffcc99']\n\nfor target in data_2['ICU'].unique():\n    target_indices = data_2[data_2['ICU'] == target].index\n    labels, sizes = np.unique([x[1] for x in last_window if x[0] in target_indices], return_counts = True)\n    axis[target].pie(sizes, labels = labels, \n                     shadow = True, \n                     colors = colors,\n                     autopct = '%1.1f%%')\n    axis[target].set_title('ICU = %d' %target)\n\nfig.tight_layout(rect = [0, 0.4, 1, 1])\nplt.suptitle('Last Window Distribution vs. ICU value', fontsize = 20)\nplt.show()","19e0b318":"#Add last window information to dataset\ndata_2 = data_2.join(\n    other = pd.DataFrame(data = last_window, columns = ['INDEX', 'LAST_WINDOW']).set_index('INDEX'),\n    how = 'inner')","af1b0a91":"#Define feature groups to investigate and compute pearson correlation\ncols_to_investigate = np.unique([x for x in data_2.columns if x.split('_')[-1] in ('end', 'begin')])\n\ndata_2_corr = data_2[np.append(cols_to_investigate, 'LAST_WINDOW')].corr()['LAST_WINDOW']\ndata_2_corr = abs(data_2_corr)\n\nplt.figure(figsize = (8,4))\nsns.distplot(data_2_corr.values)\nplt.tight_layout()\nplt.title('Distribution of Pearson correlation coefficient with respect to LAST_WINDOW', fontsize = 15)","4b73d1ce":"#Select strongly LAST_WINDOW correlated features\ndata_2_corr = data_2_corr[(data_2_corr >= 0.5) & (data_2_corr < 1)]\ndata_2_corr.sort_values(ascending = False)","f61dae20":"#Plot TEMPERATURE_DIFF_REL_end, the one with the larger correlation coefficient\nfig, axis = plt.subplots(1, 2, figsize = (15, 5))\n\nsns.boxplot(x = 'LAST_WINDOW', y = 'TEMPERATURE_DIFF_REL_end', data = data_2, ax = axis[0])\nsns.boxplot(x = 'LAST_WINDOW', y = 'BLOODPRESSURE_DIASTOLIC_DIFF_REL_end', data = data_2, ax = axis[1])\n\n\nplt.suptitle('Investigating TEMPERATURE_DIFF_REL_end and BLOODPRESSURE_DIASTOLIC_DIFF_REL_end correlations')","dd4d1f34":"#Remove features strongly correlated to LAST_WINDOW\ncol_groups_to_remove = ['_'.join(x.split('_')[:-1]) for x in data_2_corr.index] \nremaining_cols = [x for x in data_2.columns if '_'.join(x.split('_')[:-1]) not in col_groups_to_remove]\n\ndata_2 = data_2[remaining_cols]\ndata_2.head()","8349d725":"#Rank feature correlations\ndata_2_corr = rank_correlation_score(data_2)\ndata_2_corr = data_2_corr[data_2_corr['Pearson_Corr'] < 1]\ndata_2_corr","b5911651":"data_2_corr[data_2_corr['Pearson_Corr'] >= 0.9]","bef2e6f8":"data_2_corr['Feature_1_Type'] = [x.split('_')[-1] for x in data_2_corr['Feature_1']]\ndata_2_corr['Feature_2_Type'] = [x.split('_')[-1] for x in data_2_corr['Feature_2']]\n\ntime_corr = data_2_corr[data_2_corr['Pearson_Corr'] >= 0.9]\ntime_corr = time_corr[data_2_corr['Feature_1_Type'] != data_2_corr['Feature_2_Type']]\n\ntime_corr.drop(columns = ['Feature_1_Type', 'Feature_2_Type'])","f8830ca4":"print('Percentage of records in which DESEASE GROUPING 3 value does not change: %2.1f%%' \\\n    %(100*len(data_2[data_2['DISEASE GROUPING 3_begin'] == data_2['DISEASE GROUPING 3_end']]) \/ len(data_2)))","8de7ea58":"#Define function to compute how the feature change relates to the target\ndef compute_likelyhood(data, col, target):\n    begin_col = col + '_begin'\n    end_col = col + '_end'\n    \n    change_data = data[data[begin_col] != data[end_col]]\n    no_change_data = data[data[begin_col] == data[end_col]]\n        \n    change_likelyhood = len(change_data[change_data[target] == 1]) \/ len(change_data)\n    no_change_likelyhood = len(no_change_data[no_change_data[target] == 1]) \/ len(no_change_data)\n    \n    print('Success probability on change: %2.1f%%' %(100 * change_likelyhood))\n    print('Success probability on no change: %2.1f%%'%(100 * no_change_likelyhood))","da42efdc":"compute_likelyhood(data_2, 'DISEASE GROUPING 3', 'ICU')","8265a7f2":"data_2['DISEASE GROUPING 3_begin'].unique()","f1dd9245":"binary_feature_groups = np.unique(['_'.join(x.split('_')[:-1]) for x in data_2.columns.values \n                                   if len(data_2[x].unique()) == 2], return_counts = True)\nbinary_feature_groups = [x for x,y  in zip(binary_feature_groups[0], binary_feature_groups[1]) if y > 1]\n\n#Selected desired correlations\nbinary_feat_time_corr = data_2_corr[data_2_corr['Feature_1_Type'] != data_2_corr['Feature_2_Type']]\nbinary_feat_time_corr['Feature_1_Type'] = ['_'.join(x.split('_')[:-1]) \n                                           for x in binary_feat_time_corr['Feature_1']]\nbinary_feat_time_corr['Feature_2_Type'] = ['_'.join(x.split('_')[:-1]) \n                                           for x in binary_feat_time_corr['Feature_2']]\n\nbinary_feat_time_corr = binary_feat_time_corr[\n    binary_feat_time_corr['Feature_1_Type'] == binary_feat_time_corr['Feature_2_Type']]\nbinary_feat_time_corr = binary_feat_time_corr[binary_feat_time_corr['Feature_1_Type'].isin(binary_feature_groups)]\nbinary_feat_time_corr","c0fd67eb":"#Remove '_begin' features for strongly time correlated group of attributes\ndata_2 = data_2.drop(columns = \n                     binary_feat_time_corr[binary_feat_time_corr['Feature_1'] != 'OTHER_begin']['Feature_1'])\ndata_2.head()","e3057649":"#Select correlations for features in the same time level\nsingle_time_step_corr = data_2_corr[data_2_corr['Feature_1_Type'] == data_2_corr['Feature_2_Type']]\nsingle_time_step_corr = single_time_step_corr[single_time_step_corr['Pearson_Corr'] >= 0.9]\nsingle_time_step_corr","4dced7f3":"#Plot BLOODPRESSURE_DIASTOLIC_MEAN_end vs. BLOODPRESSURE_DIASTOLIC_MEDIAN_end\nsns.jointplot('BLOODPRESSURE_DIASTOLIC_MEAN_end', 'BLOODPRESSURE_DIASTOLIC_MEDIAN_end', data_2, kind = 'hex')","81f4da44":"#Remove correlated features\ndata_2 = data_2.drop(columns = single_time_step_corr['Feature_2'].values)\ndata_2.head()","76d14efc":"#Encode remaining categorical feature\ndata_2 = encode_feature(data_2, 'AGE_PERCENTIL_1')","6c5df070":"#Split dataset into training and validation\nfeature_cols = data_2.drop(columns = ['ICU', 'LAST_WINDOW']).columns\nx_train, x_validation, y_train, y_validation = train_test_split(data_2[feature_cols], data_2['ICU'],\n                                                                test_size = 0.1,\n                                                                shuffle = True)","29a66dad":"#Cross-validate baseline model\nbaseline_model = RandomForestClassifier()\nfitted_baseline_model = score_model(estimator = baseline_model, \n                                    train_data = (x_train, y_train),\n                                    validation_data = (x_validation, y_validation), \n                                    cv = 5)","89262c81":"#Plot confusion matrix for validation data\nconfusion_matrix(y_validation, fitted_baseline_model.predict(x_validation))","aaf9ff0d":"#Compute baseline model score for a different random seed\nnp.random.seed(100)\n\nx_train, x_validation, y_train, y_validation = train_test_split(data_2[feature_cols], data_2['ICU'],\n                                                                test_size = 0.1,\n                                                                shuffle = True)\n\nfitted_baseline_model_2 = score_model(estimator = RandomForestClassifier(), \n                                    train_data = (x_train, y_train),\n                                    validation_data = (x_validation, y_validation), \n                                    cv = 5)","20c23590":"#Compute score for a KNN model\nfitted_baseline_model_3 = score_model(estimator = KNN(), \n                                    train_data = (x_train, y_train),\n                                    validation_data = (x_validation, y_validation), \n                                    cv = 5)","96bbb0cc":"#Plot feature importances for the Random Forest Model\nfeat_importances = pd.Series(data = fitted_baseline_model.feature_importances_,\n                             index = feature_cols).sort_values()\nfeat_importances.plot(kind = 'barh', figsize = (13, 14))","12d4a2d8":"#Remove the 30% least important features\nthrehshold_index = int(len(feat_importances) * 0.7)\nreduced_feature_cols = feat_importances.sort_values(ascending = False).index.values[:threhshold_index]\n\nscore_model(estimator = RandomForestClassifier(), \n            train_data = (x_train[reduced_feature_cols], y_train),\n            validation_data = (x_validation[reduced_feature_cols], y_validation), \n            cv = 5)","bcde82ad":"#Remove least important features\nnot_important_cols = [x for x in feature_cols if x not in reduced_feature_cols]\ndata_2 = data_2.drop(columns = not_important_cols)\ndata_2.head()","f21c28c6":"#Retrain model for reduced dataset\nfeature_cols = [x for x in feature_cols if x in data_2.columns.values]\n\nx_train, x_validation, y_train, y_validation = train_test_split(data_2[feature_cols], data_2['ICU'],\n                                                                test_size = 0.1,\n                                                                shuffle = True)\n\nmodel_2 = score_model(estimator = RandomForestClassifier(), \n                      train_data = (x_train, y_train),\n                      validation_data = (x_validation, y_validation), \n                      cv = 5)","248b0e51":"#Re-evaluate feature importances\nfeat_importances = pd.Series(data = model_2.feature_importances_,\n                             index = feature_cols).sort_values()\nfeat_importances.plot(kind = 'barh', figsize = (13, 14))","7bbaa272":"The results are now much better than before. This is a relief, considering our first model was not much better than simply randomly selecting which patients would go to the ICU. From here, there are several paths we can take:\n\n* Further investigate the dataset in order to remove or add features;\n* Tune the algorithm hyperparameters to see if we can achieve some accuracy improvement;\n* Discard this approach and proceed to a time-relevant model.\n\nFor now, the better course of action seems to be keep investigating our window-1 dataset. The first thing we can do is to look at the feature importances from the Random Forest model.","4be90943":"As we can see, even for the least strong correlation value, the attribute remains mostly unchanged for the two time levels in question. This raises an interesting possibility: could we use the variable change as feature? Let's see if the possible new attribute correlates to the target in any level. ","dad486c5":"As we can see, some feature present quite the correlation coefficient. Let's take a closer look at the ones in which the *Pearson_Corr* is larger than 0.99.","d34daaea":"Here, we have that some present nearly no importance to the model. These can be easily removed from our model. However, just to be safe, let's see if the model actually does not miss them.","a13cae25":"We have already performed some feature selection, and before we go any further, it could be wise to see how a model performs in our data as for this point. Since our dataset is not very large, and there are not that many features remaining, this can work as a baseline model. Afterwards, we can see if removing or engineering features can maintain or even improve from this first try.\n\nWe this first attemp, let's try and use a K Nearest Neighbors classification algorithm. Our choice is based on the fact the model could be somewhat easy to interpret. ","8544832f":"Now that we have already evaluated the most extreme correlation cases between features, let's see how these attributes relate to the target column.","81d5a83b":"## 4.3. Baseline Model","15224826":"Let's first look at our baseline model and find out which features were more relevant to the final result.","89e438dd":"## 3.3. First Attempt","048ceb3f":"### 4.2.1. Time Relevant Correlations","87fac648":"As we can see, most features are poorly correlated to LAST_WINDOW. Still a couple of them present correlation coeficient values higher than 0.5. Let's take a look at those.","6f6ab7d5":"We have already removed some of the most obvious case of not so relevant features. Before proceeding to other feature selection techniques, let's train a model to serve as our baseline for the next optimization steps. As we have obtained some good results using a Random Forest Classifier, we are going to stick to it for now.","6bfb3466":"Once again, no exceptions are found for the *DIFF\/DIFF_REL* correlation behavior. As in the previous table, the second and third records are to be neglected, since they are not actually comparing the same measurement type. For this duo, we choose the *DIFF_REL* features to remove.","aa390e82":"Well, the table shows there are no exceptions to our previous assertion. Still, there are two smaller correlations values above. If you look closer, however, *Feature_1* and *Feature_2* are not part of the same measurement. They probably just slipped through our filters and don't matter for this specific step.\n\nIn the end, this means we do not need both *MEAN* and *MEDIAN* features for a given measurement type. So, we are going to stick with the *MEAN* attributes for now.","03b8b73f":"Comparing theses results to the ones shown above for the entire dataset version of the model, it is hard to draw conclusions. In one hand, the cross-validation results do not present a relevant change. However, the validation resuts are clearly better.\n\nOn the context of a Kaggle competition, this might be enough for us to go forward and discard these features. Still, the data is not conclusive to the point where this is a no brainer. Then, we are keeping all the attributes for now.\n\nIf we still want to explore the possibility of reducing the amount of features in our dataset, a reasonable approach is to apply a Recursive Feature Selection method. It should not take some much time, since the dataset is small. To further reduce processing time issues, we can use a simplified version of the Random Forest algorithm.","daa55524":"Looking at both rankings, it is clear both feature selection approaches present some resemblence. This is not necessarily a godd thing, since our results on removing the least important features were not conclusive. Not let's try and see how the classification model performs if we use only the better half of the features according to the RFE algorithm.","7e8a68d1":"## 3.1. Correlations","31903bb6":"# 1. Import Libraries","41bb74bf":"There is no perceptible change on the accuracy and ROC_AUC scores. We can then assume removing these features will not hurt our predictive model.","8aad9782":"This is by far the best results we have so far. Just by tuning the algorithm parameters we were able to improve the validation accuracy roughly by 6 percentage points. On top of that, we have reached a point in which our model should be able to correctly predict patients that are going to demand an ICU bed in over 80% of the cases.","da69acd5":"We finally have it. Our dataset has been constructed in such a way that all the *_end* features represent the data from the last available time window. Additionally, the *_begin* attribute are related to the imediatelly previous window. As for the time constant features, they can be identified by the *_1* sufix.\n\nThere is, however, an important issue we need to address before we proceed into constructing a predictive model. Going back to the definition to the choice of the two most recent time window data, a pattern arises. In the cases in which the patient did not go to the ICU, we are always looking at windows 4 and 5. As for the records where the patient required ICU treatment, the last window could be anyone from 2 to 5. Let's illustrate this.","b1248ca7":"Looking at our data, there is still a single featute that requires some processing: *AGE_PERCENTIL_1*. Let's encoded right now and move on.","bfee0445":"Looking back at our original data, we must remember that some features do not present a time aspect, such as age and gender related attributes. These columns will be part of our final dataset, and no work is required on them at this point. The other features are the ones we are focusing on right now.","2fdbe191":"Again, the results appear to be slightly better then the whole-dataset model. Also again, the margin is not big enough for us to make any bold statements. \n\nFor now, this is where we stop in terms of feature selection. The two tests we performed did provide us with conclusive results. Additionally, we are not dealing with a enormous amount of attributes, specially considering how these attributes are derived from a few measurements. The next step, then, is to see how much we are able to improve accuracy by tuning the hyperparameters.","6f463933":"Up to this point, our use of correlation scores was focused on managing an specific issue regarding the problem definition. Now we are looking at the relationships between features to see if there is anything we can learn from our dataset.\n\nOn the previous modeling we conducted, a similar investigation was performed. However, as we are not necessaraly using window-1 data, it is better to explore this version of the data without any preconceptions. However, we can still learn from our mistakes. So, let's skip plotting another messy heatmap and jump straight to ranking the relationships by their correlation score.","b03cc436":"## 3.2. Feature Encoding","b304100c":"### 4.2.2. Time Irrelevant Correlations","c2ba09fc":"## 4.1. Preprocess Data","9807fc11":"Simply put, the results are not impressive, specially considering the frequency of true values in our target is around 46%. Before we make any important decision, however, let's see how another algorithm performs on this data.","dc8383c7":"This notebook is the follow-up to the [data processing and exploring perfomed earlier](https:\/\/www.kaggle.com\/epdrumond\/icu-admission-data-cleaning-and-exploration). So, we are not using the raw data, but a preprocessed version. Here we are going to focus on feature selection, feature engineering and the actual development of a predictive model.\n\nOn this last point, it is interesting to reflect on how our model could be used to help identify which patients will require ICU attention. It is safe to assume the sooner we are able to identify the patients who demand extra care, the better. So, one way to tackle this problem is:\n\n1. Create a classification model using only the first window data.\n\nOf course, we could be discarding valuable data by following this path. However, if the models proves to be accurate enough, we would be saving the medical staff a lot of time, as well as saving many lifes.\n\nAdditionally, we can use another approach to this problem and take advantage of the time series data. We just have to remember this chronological information is not composed of a lot of different time steps, specially considering we should discard data taken after the ICU admission. Given all that, we decided to use the following approach:\n\n2. Create a classification model using the data from the two most recent measurements for each patient.\n\nIn this work, we are going to follow these two paths and then compare the results to find out which one produces the best predictions.","18414b6c":"What we see here is that there are two measurement combinations which tend to present a strong correlation between each other. They are:\n\n* MEAN\/MEDIAN.\n* DIFF\/DIFF_REL\n\nWell, it is exactly surprising to see this behavior. Still, it is better have proof than to base our feature selecion in assumptions. We are going even deeper and try to see if this assessment is true in every case for out dataset. ","056acd31":"Looking at the above relationships, all the features in question are actually binary. The Pearson correlation evaluation, in that case, is not even the proper test to be applied. However, these results probably mean the feature value does not change from one time window to the other. Let's take the last feature, *DISEASE GROUPING 3* as an example to see if this is actually true.","97d2e7ca":"This first assessment on the relationship between reveals some interesting points. First of all, most of the more strongly correlated features are part of the *DIFF* and *DIFF_REL* feature groups. When ploting two of them, we see the correlation is really onto something. The feature distribution does change a lot according to the window in which the measurement was taken.\n\nAs we really want to be on the safe side, we are going to remove all of these features and their counterparts. To be clear, not only *TEMPERATURE_DIFF_REL_end* is being cut off, but also *TEMPERATURE_DIFF_REL_begin*. Later, should the need arise, we can revisit this decision.","e28d112b":"Besides the four previous correlations, there another three of the time variant type involving binary features. The last one, for the *OTHER* feature, is not relevant in this context, since its value is much smaller compared to the other ones. This leaves us with two extra relationship, both of which present very large correlation scores. We can adopt the procedure mentioned above: consider the *end* feature as the correct assessment and remove the *begin* feature.","c95fd707":"Looking at all the selected correlation for which the time step as the same for both features, we observe the pattern is similar to when we were doing this same procedure on the previous model. Most strong correlation are of the *MEAN\/MEDIAN* type, followed by some *DIFF\/DIFF_REL* and a few *DIFF\/MIN*. Before we decide on what do, let's how one of these correlations looks on a plot.","270ea799":"## 3.4. Feature Selection","7d72a333":"So far, the best model we have produced has derived from the Random Forest algorithm. This way, we are only going to spend time tuning its hyperparemeters. In order to reduce the time we are going to spend on this procedure, only a portion of the hyperparameter is going to be investigated. They are:\n\n* n_estimators\n* criterion\n* max_depth\n* max_features\n\nAs our dataset is very small, it is feasable to perform a grid search for the best set of hyperparameters.","b34e3574":"# 5. Conclusion","50cd246f":"This data does not provide us with a lot. However there's some observations we can make. First of all, the most strognly correlated feature, *AGE_ABOVE65_1*, confirms the elderly are more affected by the desease. Following, looking at the *DIFF* attributes, they are mostly positioned at weaker correlated half of our features. This is not actually a surprise considering how poorly distributed they are.","e278fe6f":"The graph above leads to two observations:\n\n1. *RESPIRATORY_RATE_MEAN_1* is by far the most important feature on the classification model.\n2. After *OXYGEN_SATURATION_MAX_1*, there is a significant dip on how important the attributes are for the algorithm.\n\nOne way of checking whether this assessment has any validity is to remove the least important feature from our dataset and see how the same algorithm behaves.","43df042f":"# 3. Model #1: Window-1 Data Only","efccac34":"## 3.5. Hyperparameters Tuning","83023b03":"The graph confirms what the Pearson correlation score indicated. The two features are highly dependent on each other. As we are talking about extremely high correlation values, all of them over 0.97, we can simply remove one feature for each pair. As for any remaining strong correlations we have not investigated, other feature selection procedures will be applied on the dataset, meaning our work on cleaning the data is not done yet.","a9b17f50":"# 2. Introduction","414ccae7":"It is very easy to see this graphical approach is not very helpful for this amount of features. So let's split our analysis in two. First we are looking at how these features relate to each other, excluding the target column.","ae787a2a":"Now we proceed into our second approach at trying to accurately determine which patients are going to the ICU. The idea is to take advantage of the time series aspect of the dataset. \n\nSuppose a couple of ICU beds just became vacant (hopefully because their former occupants got better), and the medical staff wants to determine which of the current patients is more likely to take these spots. In this scenario, it would be wise to look at the most recent available data to do the assessment. However, if we are able see how these patients have progressed to their current state, a better decision could be made.\n\nGiven this rational, our idea is to use the data from the last 2 available time windows to predict ICU admission.","d8e31c52":"# 4. Model #2: Classification from Last Two Available Window Data ","1157298a":"The results are surprisingly accurate for our first try. Honestly, almost to good to be true. Let's double check.","6a53c57e":"The data shows promissing results. There is no record of patient going to the ICU while having it *DISEASE GROUPING 3* value change over time. However, there is one issue that we must consider. For this very small dataset, the 1.2% percent of the records in which this change does occur represent solely 4 patients. You don't have to be an expert to see it is not very smart to make any decisions based on such a small sample size. These occurances, for all we know, could very well be the result of wrong initial assessment of the patient condition or previous diseases.\n\nGiven this, our choice will be to remove the *begin* features for the strong correlations we highlighed. Before we do it, though, we should check the other binary features, specially the remaining *DISEASE GROUPINGS*.","46fa064a":"At this point, most data cleaning has already been performed. We have also looked at the features individually. Let's now focus on investigating the relationships between features and the target. First, we are going to look at how they correlate to each other.","8a7caa4f":"From the pattern highlighed in the graph above, there is the reasonable concern a correlation might exist between the features distribution and the time window in which they were taken. The data exploration performed in [our previous notebook](https:\/\/www.kaggle.com\/epdrumond\/icu-admission-data-cleaning-and-exploration) only supports this assumption. It is then wise to look further into this matter and verify whether or not this is true. This is important because, in real life situations, our model could undervalue the likelyhood of a patient going to the ICU solely on the basis of how much he or she has already spent in the hospital.","94ed63a8":"It is tempting to keep removing the least important features. However, our model is based on a very small dataset. Removing features by labeling them not relevant to the model might be unwise, considering some relationships could arise as more data is included. Given this, we are going to limit ourselves to removing those attributes that did not really seem to add any value do the predictive model.","b1bbf8a6":"On this notebook, we have been able to create two predictive models for the ICU admission classification problem. The first one focused on the first available data for each patient, obtaining a fairly accurate model. The second model took a time series approach and used data from the last two time windows for each patient. The results for this approach were significantly better compared to the first one. For this model, we investigated the window correlation aspect and removed the most evidently dependent attributes. One indication of the success of this data processing step is how well the model was able to classify patients for both target values.\n\nOnce again, we must warn that working with a small datasets limits the extent of how certain we can be about our results. Still, we have achieved some promissing results for two fairly interpretable predictive models.\n\nIf got to this point of the notebook, my sincere thanks. Please leave a comment and tell me where I got it wrong and how I can improve.","17bcfa59":"## 4.2. Correlations","2b85a527":"Once again, we see some features are extremely correlated. Let's look deeper at these cases.","3de93b9c":"It looks like this is not a mistake. If we look at the accuracy score for the KNN model, it is clear, we are not just getting the same results regardless of the choice of algorithm. It seems all the work we had processing the data was actually worth the trouble.\n\nBut this is not the time to get overly excited. Let's explore our model further.","5e2eff29":"# 4.4. Feature Selection","292bd6a0":"Mostly, these stronger correlations can be grouped in two types:\n\n1. *Begin* features strongly correlated to *End* features for the same measurements (Time relevant correlations).\n2. Features correlated at the same time level (Time irrelevant correlations).\n\nNext, we are looking at these two groups and determine whether any feature engineering or selection can be performed there."}}