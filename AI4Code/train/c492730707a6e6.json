{"cell_type":{"e744495f":"code","8a1ad3cc":"code","e5c3708a":"code","540ad269":"code","3ff9eb4c":"code","8f817dde":"code","34ab90a1":"code","0fee6601":"code","ae28e774":"code","f3728b42":"code","c6fe1c95":"code","e686be28":"code","1ec84e19":"code","448a5cc9":"code","eaae49d7":"code","f5b784b1":"code","91569bd1":"code","22c87cfe":"code","7047be3c":"code","211ca5bf":"code","c4a866ab":"code","61c00f11":"code","ef77d8e5":"code","f16a3554":"code","8df8e6ec":"code","ba9134ee":"code","596b6ae1":"code","cafcb6ca":"code","51c0233a":"code","f4921402":"code","7a2b4f70":"code","0e73495c":"code","b554fa72":"code","e8a5cdae":"code","b79139d6":"code","f4a892a5":"code","932f03ff":"code","6860289c":"code","8b386103":"code","c7a2d5ae":"code","06600a1e":"code","3b4e3dc7":"markdown","f84e1926":"markdown","4ff8223a":"markdown","313c5e11":"markdown","50532f93":"markdown","06912895":"markdown","e668fbc0":"markdown","e4c05855":"markdown"},"source":{"e744495f":"import sys\nsys.path.append('..\/input\/iterativestratification')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","8a1ad3cc":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport warnings\nwarnings.filterwarnings('ignore')","e5c3708a":"from sklearn.preprocessing import QuantileTransformer","540ad269":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","3ff9eb4c":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","8f817dde":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","34ab90a1":"def make_PCA(train_features, test_features, columns=GENES, n_comps=600):\n    data = pd.concat([pd.DataFrame(train_features[columns]),\n                      pd.DataFrame(test_features[columns])])\n    data_pca = (PCA(n_components=n_comps, random_state=42).fit_transform(data[columns]))\n\n    train_pca = data_pca[:train_features.shape[0]]\n    test_pca = data_pca[-test_features.shape[0]:]\n\n    train_pca = pd.DataFrame(train_pca, columns=[f'pca_{columns[0][0]}-{i}' for i in range(n_comps)])\n    test_pca = pd.DataFrame(test_pca, columns=[f'pca_{columns[0][0]}-{i}' for i in range(n_comps)])\n\n    train_features = pd.concat((train_features, train_pca), axis=1)\n    test_features = pd.concat((test_features, test_pca), axis=1)\n    return train_features, test_features","0fee6601":"train_features, test_features = make_PCA(train_features, test_features)\ntrain_features, test_features = make_PCA(train_features, test_features, columns=CELLS, n_comps=50)","ae28e774":"from sklearn.feature_selection import VarianceThreshold\n\nvar_thresh = VarianceThreshold(0.9) \ndata = train_features.append(test_features)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]","f3728b42":"q_list = [0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.75, 0.8, 0.9]","c6fe1c95":"def make_knn_features(features, pre_name='G'):\n    q_list = [0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.75, 0.8, 0.9]\n    kNN_features = {}\n    for q in q_list:\n        kNN_features[f'q-{q}'] = features.quantile(q, axis=1).values\n    kNN_features['min'] = features.min(axis=1).values\n    kNN_features['max'] = features.max(axis=1).values\n    kNN_features['std'] = features.std(axis=1).values\n    kNN_features['mean'] = features.mean(axis=1).values\n    return pd.DataFrame(kNN_features, columns=kNN_features.keys())","e686be28":"TRAIN_MASK = (train_features['cp_type'] != 'ctl_vehicle')\nTEST_MASK = (test_features['cp_type'] != 'ctl_vehicle')\n\ng_knn_train_features = make_knn_features(train_features[GENES][TRAIN_MASK])\ng_knn_test_features = make_knn_features(test_features[GENES][TEST_MASK])\n\nc_knn_train_features = make_knn_features(train_features[CELLS][TRAIN_MASK])\nc_knn_test_features = make_knn_features(test_features[CELLS][TEST_MASK])\n\nknn_train_features = pd.concat([g_knn_train_features, c_knn_train_features], axis=1)\nknn_test_features = pd.concat([g_knn_test_features, c_knn_test_features], axis=1)","1ec84e19":"target_cols = train_targets_scored.drop('sig_id', axis=1).columns.values.tolist()","448a5cc9":"from sklearn.ensemble import RandomForestClassifier\n\nclfr = RandomForestClassifier(10)\nclfr.fit(X=knn_train_features, y=train_targets_scored[target_cols][train_features['cp_type']!='ctl_vehicle'])","eaae49d7":"clfr_res = clfr.predict(knn_train_features)","f5b784b1":"(clfr_res == train_targets_scored[target_cols][train_features['cp_type']!='ctl_vehicle'].values).mean()","91569bd1":"train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']])\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']])\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\ntrain_features.shape","22c87cfe":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train[train_targets_scored.columns]\ntrain.shape","7047be3c":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","211ca5bf":"train.head()","c4a866ab":"target_cols = target.drop('sig_id', axis=1).columns.values.tolist()","61c00f11":"def set_train_folds(train, target, fold_number_col_name=\"kfold\", n_splits=7, shuffle=True, random_state=42):\n    folds = train.copy()\n    mskf = MultilabelStratifiedKFold(n_splits=n_splits, \n                                     shuffle=shuffle, \n                                     random_state=random_state)\n\n    for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n        folds.loc[v_idx, fold_number_col_name] = int(f)\n\n    folds[fold_number_col_name] = folds[fold_number_col_name].astype(int)\n    return folds","ef77d8e5":"folds = set_train_folds(train, target)\nfolds.head()","f16a3554":"print(train.shape)\nprint(folds.shape)\nprint(test.shape)\nprint(target.shape)\nprint(sample_submission.shape)","8df8e6ec":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n    ","ba9134ee":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0.\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0.\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n\n","596b6ae1":"import torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","cafcb6ca":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.2619422201258426)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.2619422201258426)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x\n    \nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n        with torch.no_grad():\n            # true_dist = pred.data.clone()\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing \/ (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    ","51c0233a":"def encode_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data","f4921402":"feature_cols = [c for c in encode_data(folds).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\nlen(feature_cols)","7a2b4f70":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 7           \nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=1500\n","0e73495c":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = encode_data(folds)\n    test_ = encode_data(test)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n   \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n        \n        elif (EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n\n    )\n    \n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions\n","b554fa72":"def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ \/ NFOLDS\n        oof += oof_\n        \n    return oof, predictions","e8a5cdae":"SEED = [21,42] #<-- Update\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    \n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ \/ len(SEED)\n    predictions += predictions_ \/ len(SEED)\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions\n","b79139d6":"from sklearn.linear_model import LogisticRegression\n\n\npredictions2 = clfr.predict(knn_train_features)\nfinal_model = RandomForestClassifier(3)\nfinal_model.fit(np.hstack([train[target_cols], predictions2]), train_targets_scored[target_cols][TRAIN_MASK])","f4a892a5":"train_targets_scored","932f03ff":"len(target_cols)\n","6860289c":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ \/ target.shape[1]\n    \nprint(\"CV log_loss: \", score)\n    ","8b386103":"def log_loss_metric(y_true, y_pred):\n    metrics = []\n    for _target in train_targets_scored.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels = [0,1]))\n    return np.mean(metrics)","c7a2d5ae":"predictions_knn = clfr.predict(knn_test_features)\ntest[target_cols] = final_model.predict(np.hstack([test[target_cols], predictions_knn]))\nsub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","06600a1e":"sub.shape","3b4e3dc7":"# Dataset Classes","f84e1926":"# Feature Selection using Trees","4ff8223a":"# PCA features + Existing features","313c5e11":"# CV folds","50532f93":"# Single fold training","06912895":"## Features for kNN (different statistics)","e668fbc0":"# Preprocessing steps","e4c05855":"# Model"}}