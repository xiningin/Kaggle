{"cell_type":{"5d23923f":"code","4a9ecab1":"code","d0254c20":"code","d0698f3f":"code","88879f38":"code","5a04da2e":"code","60cb1da6":"code","9a5120d6":"code","c395f33d":"code","5f6f2b08":"code","aca85bfc":"code","f9008512":"code","92b152c5":"code","dc6cedd5":"code","7bc1e098":"code","82137188":"code","35f98e53":"code","ae8430ff":"code","3fc85642":"code","c69ec3a9":"code","655258a2":"code","6f80bdc4":"code","ae50b6e9":"code","f8e09edb":"code","8dcb6da9":"code","0ccf4961":"markdown","38c5f13f":"markdown","d5c2e797":"markdown","7c2c188d":"markdown","1ebd0915":"markdown","a246cdcf":"markdown","ab6ded25":"markdown","5d569a85":"markdown","9a2a22fe":"markdown","9357b749":"markdown","f87a8bca":"markdown","cee202b9":"markdown","e0506d16":"markdown"},"source":{"5d23923f":"# !pip install mglearn","4a9ecab1":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nimport numpy as np\nimport mglearn","d0254c20":"data = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndata.head()","d0698f3f":"class_count = data['Class'].value_counts()\nprint(\"Valid {:.3f}%\".format(class_count[0] \/ data.shape[0] * 100))\nprint(\"fraud {:.3f}%\".format(class_count[1] \/ data.shape[0] * 100))","88879f38":"sb.barplot([0,1], data['Class'].value_counts())\nplt.xticks([0,1], ['Valid', 'Fraud'])","5a04da2e":"# visualizing how each class behave in respect of time\nfig, axes = plt.subplots(15, 2, figsize=(20, 24))\n\nvalid = data[data['Class'] == 0]\nfraud = data[data['Class'] == 1]\nax = axes.ravel()\nfeatures = data.drop(['Class', 'Time'], axis=1).columns.to_list()\n\nfor i in range(29):\n    sb.scatterplot(valid['Time'], valid[features[i]], c=['red'], ax=ax[i])\n    sb.scatterplot(fraud['Time'], fraud[features[i]], ax=ax[i])","60cb1da6":"# correlation matrix\nmatrix_corr = data.corr()\nplt.figure(figsize=(20, 8))\nsb.heatmap(matrix_corr, annot=True, cmap='viridis')","9a5120d6":"corr_df = matrix_corr.loc['Class']\ncorr_df = corr_df.drop('Class').sort_values()\nplt.figure(figsize=(15,8))\nsb.barplot(corr_df, corr_df.index)","c395f33d":"corr_df.describe()","5f6f2b08":"selected_features = corr_df[np.abs(corr_df) > 0.018].index.to_list()\nselected_features","aca85bfc":"fig, axes = plt.subplots(ncols=5, nrows=6, figsize=(20, 8))\nfor i, feature, ax in zip(np.arange(30), data.columns.to_list(), axes.flat):\n    sb.distplot(data[feature], ax=ax)\n    ax.set_title(feature)","f9008512":"# Some algorithms may took hours to train\n# to cover that let's take some sample\n# of the data\nsample = data.sample(frac = 0.1, random_state=42)\nprint(\"sample size at 10% from original \", sample.shape[0])\nprint(\"Number of valid transactions \", sample[sample['Class'] == 0].shape[0])\nprint(\"Number of fraud transactions \", sample[sample['Class'] == 1].shape[0])","92b152c5":"# using PCA for visualization\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport mglearn # help visualize better\n\nX = data[selected_features].values\ny = data['Class'].values\n\n# scaling X\nX_scaled = StandardScaler().fit_transform(X)\n\n# get 2 components for 2D visualization\npca = PCA(n_components=2)\npca.fit(X_scaled)\n\nX_pca = pca.transform(X)\nprint(\"Original shape {} reduced shape {}\".format(X.shape, X_pca.shape))\n\nmglearn.discrete_scatter(X_pca[:, 0], X_pca[:, 1], y)","dc6cedd5":"# using TSNE for visualization\nfrom sklearn.manifold import TSNE\n\n# run the sample because it\n# could took a long time if using\n# whole dataset\nX = sample[selected_features]\ny = sample['Class']\n\nX_scaled = StandardScaler().fit_transform(X)\n\n# get 2 components for 2D visualization\ntsne = TSNE()\ntsne.fit(X_scaled)\n\nX_tsne = pca.transform(X)\nprint(\"Original shape {} reduced shape {}\".format(X.shape, X_tsne.shape))\n\nmglearn.discrete_scatter(X_tsne[:, 0], X_tsne[:, 1], y)","7bc1e098":"# baseline model\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import roc_auc_score, classification_report\nfrom sklearn.model_selection import train_test_split\n\nX, y = data[selected_features], data['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\ngboost = GradientBoostingClassifier(learning_rate=0.01)\ngboost.fit(X_train, y_train)\n\ny_decision_gboost = gboost.decision_function(X_test)\nscore_auc = roc_auc_score(y_test, y_decision_gboost)\nprint(\"AUC score \", score_auc)","82137188":"test_fraud = y_test[y_test == 1].count()\ntest_valid = y_test[y_test == 0].count()\nprint(\"Test valid \", test_valid)\nprint(\"Test fraud \", test_fraud)","35f98e53":"from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n\ny_pred_gboost = gboost.predict(X_test)\n\ndef show_performace(y_predicted, y_test):\n    print(\"number of errors %d\" % (y_predicted != y_test).sum())\n    print(\"accuracy score %f\" % accuracy_score(y_test, y_predicted))\n    print(\"f1 score : %.3f\" % f1_score(y_test, y_predicted))\n    print(classification_report(y_test, y_predicted, labels=[0,1]))\n    print(confusion_matrix(y_test, y_predicted))\n    \nshow_performace(y_pred_gboost, y_test)","ae8430ff":"from sklearn.metrics import precision_recall_curve\n\ndef show_precision_recall(y_decision, y_test):\n    precision, recall, thresholds = precision_recall_curve(\n                                    y_test, y_decision)\n\n    close_zero = np.argmin(np.abs(thresholds))\n    plt.plot(precision[close_zero], recall[close_zero], 'o', markersize=10,\n    label=\"threshold zero\", fillstyle=\"none\", c='k', mew=2)\n    plt.plot(precision, recall, label=\"precision recall curve\")\n    plt.ylabel(\"Recall\")\n    plt.xlabel(\"Precision\")\n    \nshow_precision_recall(y_decision_gboost, y_test)","3fc85642":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(random_state=41)\nrf.fit(X_train, y_train)\n\ny_decision_rf = rf.predict_proba(X_test)[:, 1]\nscore_auc = roc_auc_score(y_test, y_decision_rf)\nprint(\"AUC score \", score_auc)","c69ec3a9":"y_pred_rf = rf.predict(X_test)\n\nshow_performace(y_pred_rf, y_test)","655258a2":"show_precision_recall(y_decision_rf - 0.5, y_test)","6f80bdc4":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\ny_decision_logreg = logreg.decision_function(X_test)\nscore_auc = roc_auc_score(y_test, y_decision_logreg)\nprint(\"AUC score \", score_auc)","ae50b6e9":"y_pred_logreg = logreg.predict(X_test)\nshow_performace(y_pred_logreg, y_test)","f8e09edb":"show_precision_recall(y_decision_logreg, y_test)","8dcb6da9":"correct_ans = y_test[(y_test == y_pred_rf) & (y_test == 1)]\nfraud_test = y_test[y_test == 1]\n\ntotal_amount_fraud_detected = data.iloc[correct_ans]['Amount'].sum()\ntotal_amount_fraud = data.iloc[fraud_test]['Amount'].sum()\n\nsaved_loss_percentage = total_amount_fraud_detected\/total_amount_fraud * 100\n\nprint(\"Total amount of fraud detected {:.2f}\".format(total_amount_fraud_detected))\nprint(\"Total amount of fraud          {:.2f}\".format(total_amount_fraud))\nprint(\"Saved loss percentage          {:.2F}%\".format(saved_loss_percentage))","0ccf4961":"# Data Analysis","38c5f13f":"## Conclusion","d5c2e797":"## Creating Models","7c2c188d":"__Random Forest__","1ebd0915":"# Credit Card Fraud","a246cdcf":"Scatter plot above is a pretty much representative of how the fraud behaves. Looking at the Amount plot, the amount of fraud tend to be small comparing with the valids transactions, and also there are other features which separates the fraud and valid behaviour.\n\nTo get better understand how each feature influence the class, using correlation matrix is one of the way.","ab6ded25":"Fraud detection usually comes with highly imbalanced dataset, this is because of the Fraud is just like a valid normal transactions. However they may have some kind of pattern which could be different from valid transactions.","5d569a85":"## Data Visualization using PCA and TSNE","9a2a22fe":"We can see the above bar plot how much each features correlates with the class, and after that each relatively uncorrelated features could be removed.\n\nMachine learning models would performs better if the data follows the normal distributions or the bell curve and for this data set most of the features is already scaled since the uploader cannot showing the real features to protect user's privacy.","9357b749":"__Gradient Boosting__","f87a8bca":"After get better understanding at the data using some of simple plotting, may be we can get better visualization of what makes the fraud differents from valid transaction. Decomposisiton method like PCA and t-sne manifold learning could extract each features to help getting better visualization.\n\nBut, the first thing to remember is that the dataset itself is huge and some algorithms could take half an hour or more to get their job done, it's better to create a sample data to in compensation with computational capability.","cee202b9":"This is very insteresting, since all of the features are numerical and looks like many of them were scaled, most of our models performs quitely good. If looking at the ROC_AUC score Logistic regression give 0.97~ while Gradient boosting and Random forest scored 0.91~ and 0.93~ respectively. However, the f1-score tell differently. Random forest scored 0.85 with 31 incorrect answer while Gradient boosting score 0.75 with 51 incorrect answer and Logistic regression scored 0.72 with 56 incorrect answer.\n\nThis where the business requirements comes in, normally the business wants to take all of the frauds as possible, and in binary classification we can measure how good the model at prediciting both of the class in this case valid and fraud transaction. So, looking at the scores the Random forest is the winner when predicting the fraud class with f1-score 0.85, precision 0.95, and recall 0.78, with that said the amount of saved by the model using the test data is about 78.33%","e0506d16":"__Logistic Regression__"}}