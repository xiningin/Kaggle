{"cell_type":{"d03c93f9":"code","00fe434f":"code","95b0f729":"code","1e7e37a7":"code","b551b9d9":"code","5ea3e65a":"code","9aa2a357":"code","c8723be6":"code","e30c8d4a":"code","78b1d75f":"code","1d2f87be":"code","fae3eb67":"code","81ece68c":"code","97144adb":"code","f5b3ca0e":"code","3958d7b7":"code","7e1df826":"code","dbc37012":"code","13580314":"code","2703ab6d":"code","f2dbbb76":"code","8db284fb":"code","bb1070a4":"code","f90c7c65":"code","14e174d4":"code","64af8148":"code","1a89cd3b":"code","fca54145":"code","e84d3af9":"code","67b5942c":"code","18baaa57":"code","a5b7a7e3":"code","6524612c":"code","b7845aeb":"code","8d1766eb":"code","6525e8f6":"code","130b28a7":"code","f081c726":"code","12bcf69e":"code","6ac09ddf":"code","593804b5":"code","6851494d":"code","d5b0ca3e":"code","7bfbcaca":"code","2f3ae4cb":"code","855e8fa1":"code","5a068be6":"code","0518b65e":"code","4a1bf924":"code","0e437eb3":"code","3082c1c1":"code","f59153d9":"code","9cf06541":"code","ae61ec8f":"code","92d81b8b":"code","edb9f451":"code","334a2cdf":"code","d8acf04c":"code","738453e3":"code","3a5abb6b":"code","4f97bcce":"code","c144cfe6":"code","5c98241e":"code","f4affb7e":"markdown","8ecd96d8":"markdown","b0cb3bf4":"markdown","daee073b":"markdown","72af0a27":"markdown","1b670dd0":"markdown","a087aca1":"markdown","041788ad":"markdown","a18e9b49":"markdown","f7da43f5":"markdown","b3d55607":"markdown","ff1e8705":"markdown","55552e8f":"markdown","a19af47b":"markdown","82169b8f":"markdown","4322e8f4":"markdown","7ded0999":"markdown","e20ece45":"markdown","428b6aa5":"markdown","de657efc":"markdown","15845c13":"markdown","e257cff9":"markdown","78dc9c4d":"markdown","23f520ad":"markdown","d40058d8":"markdown","97a60c86":"markdown","528f584b":"markdown","de4c7af4":"markdown","f06e887e":"markdown","65f9c1f7":"markdown","b40892cd":"markdown","9f61511f":"markdown","b95d3907":"markdown","5ac4f17f":"markdown","06dc7518":"markdown","b22d0ad1":"markdown"},"source":{"d03c93f9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","00fe434f":"import numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\nfigure(num=None, figsize=(20, 10), dpi=80, facecolor='w', edgecolor='k')\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom datetime import datetime\nimport json\nimport os\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.stats import skew \nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n# models\nfrom xgboost import XGBRegressor\nimport warnings\n# Ignore useless warnings\nwarnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n\n# Avoid runtime error messages\npd.set_option('display.float_format', lambda x:'%f'%x)\n\n# make notebook's output stable across runs\nnp.random.seed(42)","95b0f729":"# Read CSVs\nfetch_from = '\/kaggle\/input\/energy-industry\/ml_case_training_data.csv'\ntrain = pd.read_csv(fetch_from)\n\nfetch_from = '\/kaggle\/input\/energy-industry\/ml_case_training_hist_data.csv'\ntrain_hist_data = pd.read_csv(fetch_from)\n\n\nfetch_from = '\/kaggle\/input\/energy-industry\/ml_case_training_output.csv'\noutput = pd.read_csv(fetch_from)\n","1e7e37a7":"train.shape","b551b9d9":"train.head()","5ea3e65a":"train_hist_data.shape","9aa2a357":"train_hist_data.head()","c8723be6":"output.head()","e30c8d4a":"train.isnull().sum().sum()","78b1d75f":"# Which columns have the most missing values?\ndef missing_data(df):\n    total = df.isnull().sum()\n    percent = (df.isnull().sum()\/train.isnull().count()*100)\n    missing_values = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in df.columns:\n        dtype = str(df[col].dtype)\n        types.append(dtype)\n    missing_values['Types'] = types\n    missing_values.sort_values('Total',ascending=False,inplace=True)\n    return(np.transpose(missing_values))\nmissing_data(train)","1d2f87be":"train_df =train.copy()","fae3eb67":"# Let's plot these missing values(%) vs column_names\nmissing_values_count = (train_df.isnull().sum()\/train.isnull().count()*100).sort_values(ascending=False)\nplt.figure(figsize=(15,10))\nbase_color = sns.color_palette()[0]\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)\nplt.xticks(rotation=90)\nsns.barplot(missing_values_count[:10].index.values, missing_values_count[:10], color = base_color)","81ece68c":"# Let's plot these missing values(%) vs column_names\nmissing_values_count = (train_hist_data.isnull().sum()\/train_hist_data.isnull().count()*100).sort_values(ascending=False)\nplt.figure(figsize=(15,10))\nbase_color = sns.color_palette()[0]\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)\nplt.xticks(rotation=90)\nsns.barplot(missing_values_count[:10].index.values, missing_values_count[:10], color = base_color)","97144adb":"train_hist_data_df = train_hist_data.copy()\nstatistics_data =(pd.DataFrame(train_hist_data_df.describe())).T\nstatistics_data","f5b3ca0e":"train_df.hist(bins=50, figsize=(20,15))\nplt.tight_layout(pad=0.4)\nplt.show()","3958d7b7":"train_df.describe()","7e1df826":"train_hist_data_df.hist(bins=50, figsize=(20,15))\nplt.tight_layout(pad=0.4)\nplt.show()","dbc37012":"output[\"churn\"] = output[\"churn\"].replace({0:\"stayed\",1:\"churned\"})\noutput[\"churn\"].head()","13580314":"output[\"churn\"].value_counts(normalize=True)","2703ab6d":"# create barplot to show the percentage in visual\nsns.countplot(data=output, x='churn', order=output.churn.value_counts().index)","f2dbbb76":"# Check negative values in price of power\nnegative_cols = ['price_p1_fix','price_p2_fix','price_p3_fix']\ntrain_hist_data_df[negative_cols].apply(abs)\n\ntrain_hist_data_df.describe()","8db284fb":"import missingno as msno\nmsno.bar(train_hist_data_df)","bb1070a4":"sorted = train_hist_data_df.sort_values(by = ['id','price_date'])\nmsno.matrix(sorted)","f90c7c65":"# Check the correlation of columns in heatmap\nmsno.heatmap(train_hist_data_df,cmap='YlGnBu')","14e174d4":"msno.dendrogram(train_hist_data_df)","64af8148":"#Check id with missing values on the other columns\ntrain_hist_data_df_null =train_hist_data_df[train_hist_data_df.isnull().any(axis=1)].index.values.tolist()\n\n#get the missing values data\nmissing_values = train_hist_data_df.iloc[train_hist_data_df_null,:]\n\nmissing_values.head()\n","1a89cd3b":"# Check amount of missing values \nmissing_values.shape","fca54145":"## Imputations of price columns\ntrain_hist_data_df.fillna(method=\"bfill\",inplace=True)\ntrain_hist_data_df.iloc[train_hist_data_df_null,3:9].head()\n                          ","e84d3af9":"train_hist_data_df.describe().T","67b5942c":"# merge historical data with ouput dataset on id column\ntrain_hist_output_df = train_hist_data_df.merge(output,on=[\"id\"])\ntrain_hist_output_df.head()","18baaa57":"msno.bar(train_df)","a5b7a7e3":"import matplotlib.patches as mpatches\nmsno.matrix(train_df)\ngray_patch = mpatches.Patch(color='gray', label='Not Missing')\nwhite_patch = mpatches.Patch(color='white', label='Missing')\nplt.legend(handles=[gray_patch, white_patch])\nplt.show()","6524612c":"sorted_main = train_df.sort_values('activity_new')\nmsno.matrix(sorted_main)","b7845aeb":"# check correlation among missing values \nmsno.heatmap(train_df,cmap='YlGnBu')","8d1766eb":"msno.dendrogram(train_df)","6525e8f6":"# Check the correlation between date_activ and date_first_activ either it could potentially change the values interchangably\ndate_active_first_compare = [\"date_activ\",\"date_first_activ\"]\ndate_compare =train_df[date_active_first_compare]\ndate_compare.isnull().sum()","130b28a7":"def compare_date(df,column):\n    date_compare = df[column]\n    date_compare_cc =date_compare.dropna(subset=[column[1]],how=\"any\",inplace=False)\n    date_compare_cc[column[0]].equals(date_compare_cc[column[1]])\n    date_compare_desc = date_compare_cc.describe()\n    return date_compare_desc\n\ncolumns = ['date_activ','date_first_activ']\ncompare_date(train_df,columns)","f081c726":"train_df_drop_columns = train_df.drop(labels=[\"activity_new\",\"campaign_disc_ele\"],axis=1)\n","12bcf69e":"dropna_columns = ['date_end','date_modif_prod','date_renewal','origin_up','pow_max','margin_gross_pow_ele',\n         'margin_net_pow_ele', 'net_margin','forecast_discount_energy','forecast_price_energy_p1',\n         'forecast_price_energy_p2','forecast_price_pow_p1']\ntrain_df_drop_columns.dropna(subset=dropna_columns,how=\"any\",inplace=True)\nmsno.matrix(train_df_drop_columns)\ngray_patch = mpatches.Patch(color='gray', label='Not Missing')\nwhite_patch = mpatches.Patch(color='white', label='Missing')\nplt.legend(handles=[gray_patch, white_patch])\nplt.show()","6ac09ddf":"sorted_main = train_df_drop_columns.sort_values('channel_sales')\nmsno.matrix(sorted_main)","593804b5":"# Check correlation\nmsno.heatmap(train_df_drop_columns,cmap='YlGnBu')","6851494d":"incomplete_columns = ['channel_sales','date_first_activ','forecast_base_bill_ele','forecast_base_bill_year','forecast_bill_12m','forecast_cons']\ncomplete_columns = [cols for cols in train_df_drop_columns.columns if cols not in incomplete_columns]\ntrain_df_cc = train_df_drop_columns[complete_columns]\n## fix negative values\nnumeric = [cols for cols in train_df_cc.columns if train_df_cc[cols].dtype == \"float64\" or train_df_cc[cols].dtype == \"int64\"]\ntrain_df_cc.loc[:,numeric] = train_df_cc.loc[:,numeric].apply(abs)\ntrain_df_cc.describe()","d5b0ca3e":"train_df_cc.has_gas","7bfbcaca":"## function for changing columns contents\ndef change_columns_type(df,col):\n    if col == \"has_gas\":\n        df[col] = df[col].replace({\"f\":\"Yes\",\"t\":\"No\"})\n    df[col].replace({1:\"Churned\",0:\"Stayed\"})\n            ","2f3ae4cb":"#Convert has_gas columns contents\nchange_columns_type(train_df_cc,\"has_gas\")","855e8fa1":"train_df_cc.head()","5a068be6":"# merge datasets \ntrain_df_cc_merged = train_df_cc.merge(right=output,on=[\"id\"])","0518b65e":"change_columns_type(train_df_cc_merged,\"churn\")","4a1bf924":"train_df_cc_merged.head()","0e437eb3":"train_df_cc_merged.shape","3082c1c1":"train_df_cc_merged.info()","f59153d9":"## convert date format into datetime\ndate_cols = [\"date_activ\",\"date_end\",\"date_modif_prod\",\"date_renewal\"]\nfor col in date_cols:\n    train_df_cc_merged[col] =pd.to_datetime(train_df_cc_merged[col])","9cf06541":"train_df_cc_merged.info()","ae61ec8f":"sns.heatmap(train_df_cc_merged.corr(), cmap=\"YlGnBu\")","92d81b8b":"from scipy.stats import zscore as zscore\n#calculate zscore of tenure\ntenure_zscores =zscore(train_df_cc_merged[\"num_years_antig\"])\ntenure_zscores ","edb9f451":"tenure_zscores.plot()","334a2cdf":"#Change the negative values of tenure zscores\nabs_tenure_zscores = np.abs(tenure_zscores)\nabs_tenure_zscores.plot()","d8acf04c":"churn_tenure = train_df_cc_merged[[\"churn\",\"num_years_antig\"]]\nchurn_tenure[\"zscore\"] = abs_tenure_zscores.tolist()\nchurn_tenure.head()","738453e3":"# remove outliers if zscores above 3\nchurn_tenure_filtered = churn_tenure[churn_tenure['zscore'] < 3]\n","3a5abb6b":"# Finding numeric features\nnumeric_dtypes = ['int64', 'float64']\nnumeric = []\nfor i in train_df_cc_merged.columns:\n    if train_df_cc_merged[i].dtype in numeric_dtypes:\n        if i in ['forecast_cons_12m',\"forecast_discount_energy\",\"cons_12m\",\"cons_gas_12m\",\"cons_last_month\",\"forecast_cons_year\",\"nb_prod_act\",\"pow_max\"]:\n            pass\n        else:\n            numeric.append(i)     \n# visualising some more outliers in the data values\nfig, axs = plt.subplots(ncols=2, nrows=1 ,figsize=(15, 60))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\nsns.color_palette(\"husl\", 8)\nfor i, feature in enumerate(list(train_df_cc_merged[numeric]), 1):\n    plt.subplot(len(list(numeric)), 3, i)\n    sns.boxplot(x=feature, y='churn', hue='churn', palette='Blues', data=train_df_cc_merged)\n        \n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('Churn', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(loc='best', prop={'size': 10})\n        \nplt.show()","4f97bcce":"elec_popular = train_df_cc_merged.loc[(train_df_cc_merged['churn']>='stayed') & (train_df_cc_merged['net_margin']>0),['id', 'origin_up','net_margin']]\n\nelec_popular.value_counts(subset=['origin_up'],normalize=True)","c144cfe6":"#Highest netting electricity subscription campaign\nelec_popular.groupby('origin_up')['net_margin'].agg('sum').sort_values(ascending=False)","5c98241e":"top_5_customers = train_df_cc_merged.loc[(train_df_cc_merged['churn']>='stayed') & (train_df_cc_merged['net_margin']>0),['id','num_years_antig','net_margin']]\ntop_5_customers.sort_values(by=['net_margin'],ascending=False).head()","f4affb7e":"there are three types of columns such as object,int64,and float. before transforming the columns, i will change the date columns type into datetime.\n","8ecd96d8":"We can also see there are a few negative values on price of power that do not make sense for the price. ","b0cb3bf4":"Based on the statistics data we can conclude as follows :\n- the average price of energy for the 1st period is $0.14\n\n- the average price of energy for the 2st period is $0.05 \n\n- the average price of energy for the 3st period is $0.03\n\n- the average price of power for the 1st period is $43.32 \n\n- the average price of power for the 2st period is $10.69\n\n- the average price of power for the 3st period is $6.45\n\n\n","daee073b":"We can see on the train dataset and historical datasets have many missing values. we can visualize and deal with with missing values whether we should remove them or impute with mean, median. We can use msno library to check the missing values. ","72af0a27":"## Missing Values","1b670dd0":"# Explore the data","a087aca1":"we can see the correlation between popular electricity with the net margin earned in 2015.","041788ad":"We can see based on churn columns that roughly 10% of customers are likely to churn based on the output dataset in the first 3 months.","a18e9b49":"The dendrogram above shows the deeper understanding about correlation of each features based on the top-down approach like net margin and margin_net_pole_ele that have higher similarity because of closer distance.","f7da43f5":"We change the negative values of price of energy into positive assuming there is any posibility of errors.","b3d55607":"# Check Missingness of Training data","ff1e8705":"there are 1359 missing values on the history dataset which is missing not at random(MNAR) because there is a relationship between missingness and its values. This indicates there is likely multicollinearity among the columns.","55552e8f":"We can see most of the features is skew and number of year of antiquity is 5 years followed by net margin which is roughly $217","a19af47b":"## Churn vs Tenure","82169b8f":"code of electricity campaign *lxidpiddsbxsbosboudacockeimpuepw* is the most popular origin up with 47% currrent customers followed by code  *kamkkxfxxuwbdslkwifmmcsiusiuosws*      ","4322e8f4":"Based on the graph above, we can conclude a few notes related missing values as follows:\n- activity_new is MCAR and has low correlation with any of the variables. we can drop this feature\n- Channel_sales is MAR and has correlation with a few variables. \n- campaign_disc_elec is MCAR on all the instances. This depicts that subscribers do not subscribe by campaign offers. \n- date_first_active is MAR because it could not be replaced by date_activ\n- margin_net_pole_ele has a strong correlation with margin_gross_pole_ele. it indicates multicollinearity. so does net_margin and margin_gross_pow_ele.\n- pow_up and pow_max are MCAR that indicates no correlation and depicts a few mising values. we can drop these variables.\n- Forecast_base_bill_ele, forecast_base_bill_year, forecast_bill_12m and forecast_cons have high correlation with date_first_active(score 1 in the heatmap).Due to the dates are not identical and it could not be replaced by date_activ, then it is MNAR(possibility of systematic missing values). \n","7ded0999":"we can see there is different way of comparing the dates between these two variables like for the date in february compared to july. It is going to make the data bias.","e20ece45":"# Introduction\n\n\n![energy.jpg](attachment:2a29e095-3e28-4de7-8aec-1a1220f37460.jpg)\n\nThis is a comprehensive Exploratory Data Analysis for the BCG Virtual Internship Program Provided by [Forage](https:\/\/www.theforage.com\/virtual-internships\/prototype\/Tcz8gTtprzAS4xSoK\/Open-Access%20Data%20Science%20&%20Advanced%20Analytics%20Virtual%20Experience%20Program?ref=PFRxbADb5emG73ZYp) with Python and Data Visualization libraries such as matplotlib and seaborn.\n\nThe goal of this program challenge is to predict the probability of customers churn on one of BCG clients called PowerCo. PowerCo is a company that focus on supplying gas and electricity for SME( Small Medium Enterprises) and residential customers. They want to derive an effective decision to the declining customers lately by collaboration with BCG. One hypothesis that likely to happen of the customers churn during period of January to March 2016 is the price sensitiviy and the issue of power-liberalization market in Europe. We, as consultant want to understand the data better and derive actionable insights through the hypothesis whether we should consider marketing strategy that PowerCo is trying to do by offering 20% discount to the customers churn. Is it an effective way to do or any other solutions that we can deliver to the client?. Because this is a classification problem, we will be using one or more classification algorithms such as Logistic Regression, Decision tree, or Random Forest by always checking a few importants parts such as overfitting or underfitting model.\n\nFor these reasons, We will analyze three datasets that we will believe can support the insights by following data :\n1. Historical customer data: Customer data such as usage, sign up date, forecasted usage etc\n2. Historical pricing data: variable and fixed pricing data etc\n3. Churn indicator: whether each customer has churned or not\n\nWe start the exploratory data analysis by loading the dataset using pandas, checking missing values, doing feature engineering,checking outliers and comparing between univariate and bivariate features,improving the model using ML Algorithms(Logistics regression, Decision Tree, Random Forest or Gradient Boosting) as classificication model. However, before diving into building model, we will be building exploratory data analysis for the first project. \n\n \nFile descriptions\n- ml_case_training_data.csv - the training set (contains 16096 records)\n- ml_case_training_hist_data.csv - the testing set (contains 193002 records)\n- ml_case_training_output.csv - a sample of output whether the clients churned or not <br>\n\nData fields\n- id contact id\n- activity_new category of the company's activity\n- campaign_disc_ele code of the electricity campaign the customer last subscribed to\n- channel_sales code of the sales channel\n- cons_12m electricity consumption of the past 12 months\n- cons_gas_12m gas consumption of the past 12 months\n- cons_last_month electricity consumption of the last month\n- date_activ date of activation of the contract\n- date_end registered date of the end of the contract\n- date_first_activ date of first contract of the client\n- date_modif_prod date of last modification of the product\n- date_renewal date of the next contract renewal\n- forecast_base_bill_ele forecasted electricity bill baseline for next month\n- forecast_base_bill_year forecasted electricity bill baseline for calendar year\n- forecast_bill_12m forecasted electricity bill baseline for 12 months\n- forecast_cons forecasted electricity consumption for next month\n- forecast_cons_12m forecasted electricity consumption for next 12 months\n- forecast_cons_year forecasted electricity consumption for next calendar year\n- forecast_discount_energy forecasted value of current discount\n- forecast_meter_rent_12m forecasted bill of meter rental for the next 12 months\n\n\n- forecast_price_energy_p1 forecasted energy price for 1st period\n- forecast_price_energy_p2 forecasted energy price for 2nd period\n- forecast_price_pow_p1 forecasted power price for 1st period\n- has_gas indicated if client is also a gas client\n- imp_cons current paid consumption\n- margin_gross_pow_ele gross margin on power subscription\n- margin_net_pow_ele net margin on power subscription\n- nb_prod_act number of active products and services\n- net_margin total net margin\n- num_years_antig antiquity of the client (in number of years)\n- origin_up code of the electricity campaign the customer first subscribed to\n- pow_max subscribed power\n- price_date reference date\n- price_p1_var price of energy for the 1st period\n- price_p2_var price of energy for the 2nd period\n- price_p3_var price of energy for the 3rd period\n- price_p1_fix price of power for the 1st period\n- price_p2_fix price of power for the 2nd period\n- price_p3_fix price of power for the 3rd period\n- churned has the client churned over the next 3 months\n\n\nI would like to say thank you for a great [tutorial](https:\/\/www.kaggle.com\/lavanyashukla01\/how-i-made-top-0-3-on-a-kaggle-competition) by @lavanyashukla01 for helping me create better visualization.","428b6aa5":"There are 93633 missing values of 16096 instances.","de657efc":"We can conclude the graph as follows by comparing churn with representative variables :\n- The median age of customers who are likely to churn is 4 years\n- The median age of customers who are likely to stay is 5 years\n- Customers are more likely to churn in the 4th year than the 7th \n- and plot how the other features are correlater to each other and Churn","15845c13":"## Most Popular Electricity campaign\n","e257cff9":"# Drop unneccesary  columns","78dc9c4d":"## History Dataset","23f520ad":"This is the end of the first part of BCG Advanced Analytics Data Exploration by Boston Consulting Group. You can join the program and improve your data science skills","d40058d8":"it looks like there is no missing values, but based on previus analysis estimated that more than 70% of price columns have missing values.","97a60c86":"## Numerical Features","528f584b":"most of the zscores values ranges from 1 to 3","de4c7af4":"Campaign_disc_ele,forest_bill_12m, date_first_activ,forest_cons,forecast_base_bill_ele, forecast_base_bill_year have more than 70% missing values.","f06e887e":"## Top 5 customers by tenure and net margin","65f9c1f7":"after doing data cleaning, we can see on the heatmap that the correlation among variables is small and it can avoid multicollinearity for building our machine learning model.","b40892cd":"# Check Train Dataset","9f61511f":"We validate the missingness after doing sorted by id and price_date columns and it estimated that the price columns are mising.","b95d3907":"Most of the features\/columns have more than 70% missing values.","5ac4f17f":"## Imputations","06dc7518":"# Dealing with Missing Values","b22d0ad1":"# Overview of training and history datasets"}}