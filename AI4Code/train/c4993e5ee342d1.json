{"cell_type":{"ed098ff5":"code","1d9a4938":"code","67f5d7ae":"code","30ee970a":"code","56248b37":"code","8b8310f9":"code","99f5eb48":"code","31b57e49":"code","ac5d24b8":"code","1e848ca0":"code","bb416a32":"code","5f543f66":"code","cba629de":"code","7542fc57":"code","db2159d9":"code","b185a765":"code","fbcdde9b":"code","be45f926":"code","26709f36":"code","584e2739":"code","03dd42a8":"code","850d75ea":"code","cb64f883":"code","76441cf4":"code","64d90784":"code","6e4a2581":"code","71b5feca":"code","ce6909bd":"code","3ad72227":"code","8bff6a09":"code","f5553b1b":"code","0ee08d03":"code","02e792c5":"code","2d559254":"code","4ae476c1":"code","5a78d1e4":"code","d768189b":"code","e74201ff":"code","cffd7596":"code","f4ce6c64":"code","40dad4e1":"code","fd8f7c53":"code","78de4e87":"code","64b84419":"code","04d09216":"code","368b8493":"code","0ec8d398":"code","2780a20d":"markdown","d7484214":"markdown","97f8c1f5":"markdown","433e1617":"markdown","99975b4c":"markdown","f3274af3":"markdown","1dbcedb2":"markdown","bdf923e0":"markdown","70201e3e":"markdown","919d1697":"markdown","b4472391":"markdown","1459072e":"markdown","b8efdb10":"markdown","16c78aa6":"markdown","0de81f6a":"markdown","8ff9a7cd":"markdown","1b6df756":"markdown","38f7c9d0":"markdown","3c184df0":"markdown","2b9ab3ac":"markdown","4bbbaaa1":"markdown","73879470":"markdown","faec43bb":"markdown","82cd0587":"markdown","85fa7474":"markdown","9555cd0b":"markdown","1b3efe6b":"markdown"},"source":{"ed098ff5":"import numpy as np\nimport pandas as pd \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport warnings \nwarnings.filterwarnings('ignore')","1d9a4938":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ngender_submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","67f5d7ae":"train.head(2)","30ee970a":"test.head(2)","56248b37":"# Save our target values as name 'label'\nlabel = train['Survived']","8b8310f9":"# Data describe \ntrain.describe(include='all')","99f5eb48":"# Missing Values \ntrain.isnull().sum()","31b57e49":"train.dtypes","ac5d24b8":"# Also check out the unique valeus in each columns\n\ndict = {}\nfor i in list(train.columns):\n    dict[i] = train[i].value_counts().shape[0]\n\npd.DataFrame(dict, index=['unique count']).transpose()","1e848ca0":"# drop columns that is not useful \ntrain = train.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\ntest = test.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\ntrain.head(2)","bb416a32":"train.head(2)","5f543f66":"# age, fare \n\nfig = plt.figure(figsize=(18,7))\ngs = fig.add_gridspec(1,2)\ngs.update(wspace=0.3, hspace=0.15)\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\n\n# title \nax0.text(0.5,0.5,\"[ Target Value ]\\n___________\\n\\nSurvived Rate\",\n        horizontalalignment = 'center',\n        verticalalignment = 'center',\n        fontsize = 28,\n        fontweight='bold',\n        fontfamily='serif',\n        color='#000000')\n\nax0.set_xticklabels([])\nax0.set_yticklabels([])\nax0.tick_params(left=False, bottom=False)\n\n\n# Survived count\nax_survived = ax1\nsns.countplot(x='Survived',color='gray', data=train,ax=ax_survived, palette='gist_gray')\nsns.despine()\n\nax0.spines[\"top\"].set_visible(False)\nax0.spines[\"left\"].set_visible(False)\nax0.spines[\"bottom\"].set_visible(False)\nax0.spines[\"right\"].set_visible(False)\n\n\nplt.show();","cba629de":"# Sex, Parch, Embarked, Pclass, SibSip \n\nfig = plt.figure(figsize=(18,15))\ngs = fig.add_gridspec(3,3)\n\n\n\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\nax2 = fig.add_subplot(gs[0,2])\nax3 = fig.add_subplot(gs[1,0])\nax4 = fig.add_subplot(gs[1,1])\n\nax_sex = ax0\nsns.countplot(x='Sex', data=train, ax=ax_sex, palette='gist_gray')\nsns.despine()\n\nax_parch = ax1\nsns.countplot(x='Parch',data=train, ax=ax_parch, palette='gist_gray')\nsns.despine()\n\nax_embarked = ax2\nsns.countplot(x='Embarked', data=train, ax=ax_embarked, palette='gist_gray')\nsns.despine()\n\nax_pclass = ax3\nsns.countplot(x='Pclass', data=train, ax=ax_pclass, palette='gist_gray')\nsns.despine()\n\nax_sibsip = ax4\nsns.countplot(x='SibSp',data=train, ax=ax_sibsip, palette='gist_gray')\nsns.despine()\n\n\n# Title & Subtitle    \nfig.suptitle('Categorical Features', fontweight='bold', fontsize=20)\nfig.text(s='Sex, Parch, Embarked, Pclass, SibSip ' , x=0.5, y=0.94,  ha='center',va='top')\n\n\nplt.show();","7542fc57":"# Sex, Parch, Embarked, Pclass, SibSip \n\nfig = plt.figure(figsize=(18,15))\ngs = fig.add_gridspec(3,3)\n\n\n\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\nax2 = fig.add_subplot(gs[0,2])\nax3 = fig.add_subplot(gs[1,0])\nax4 = fig.add_subplot(gs[1,1])\n\nax_sex = ax0\nsns.countplot(x='Sex',hue='Survived', data=train, ax=ax_sex, palette='gist_gray')\nsns.despine()\n\nax_parch = ax1\nsns.countplot(x='Parch',hue='Survived', data=train, ax=ax_parch, palette='gist_gray')\nsns.despine()\n\nax_embarked = ax2\nsns.countplot(x='Embarked', hue='Survived', data=train, ax=ax_embarked, palette='gist_gray')\nsns.despine()\n\nax_pclass = ax3\nsns.countplot(x='Pclass',hue='Survived', data=train, ax=ax_pclass, palette='gist_gray')\nsns.despine()\n\nax_sibsip = ax4\nsns.countplot(x='SibSp',hue='Survived', data=train, ax=ax_sibsip, palette='gist_gray')\nsns.despine()\n\n\n# Title & Subtitle    \nfig.suptitle('Categorical Features & Survived', fontweight='bold', fontsize=20)\nfig.text(s='Sex, Parch, Embarked, Pclass, SibSip ' , x=0.5, y=0.94,  ha='center',va='top')\n\n\nplt.show();","db2159d9":"# age, fare \n\nfig = plt.figure(figsize=(15,8))\ngs = fig.add_gridspec(2,3)\n\n\n\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\nax2 = fig.add_subplot(gs[1,0])\nax3 = fig.add_subplot(gs[1,1])\n\n\nax_sex = ax0\nsns.kdeplot(x='Age',color='gray', shade=True, alpha=.5, data=train,ax=ax_sex, palette='gist_gray')\nsns.despine()\n\nax_sex2 = ax2\nsns.boxenplot(x='Age',hue='Survived', data=train,   ax=ax_sex2, palette='gist_gray_r')\nsns.despine()\n\nax_sex = ax1\nsns.kdeplot(x='Fare',color='gray', shade=True, alpha=.5, data=train,ax=ax_sex, palette='gist_gray')\nsns.despine()\n\nax_fare2 = ax3\nsns.boxenplot(x='Fare',hue='Survived', data=train,   ax=ax_fare2, palette='gist_gray_r')\nsns.despine()\n\n# Title & Subtitle    \nfig.text(0.1, 1, '               Age                                   Fare', fontsize=20, fontweight='bold', fontfamily='serif', ha='left') \n\nplt.show();","b185a765":"# Sex, Parch, Embarked, Pclass, SibSip \n\nfig = plt.figure(figsize=(18,15))\ngs = fig.add_gridspec(3,3)\n\n\n\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\n\n\nax_sex = ax0\nsns.kdeplot(x='Age',hue='Survived', data=train,fill=True, alpha=.5,shade=True, ax=ax_sex, palette='gist_gray_r')\nsns.despine()\n\nax_parch = ax1\nsns.kdeplot(x='Fare',hue='Survived', data=train, fill=True, alpha=.5,shade=True, ax=ax_parch, palette='gist_gray_r')\nsns.despine()\n\n# Title & Subtitle    \nfig.suptitle('Continous Features & Survived', fontweight='bold', fontsize=20)\nfig.text(s='Age, Fare ' , x=0.5, y=0.94,  ha='center',va='top',fontsize=15)\n\n\nplt.show();","fbcdde9b":"# Sex, Parch, Embarked, Pclass, SibSip \n\nfig = plt.figure(figsize=(18,15))\ngs = fig.add_gridspec(3,3)\n\n\n\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\nax2 = fig.add_subplot(gs[0,2])\nax3 = fig.add_subplot(gs[1,0])\nax4 = fig.add_subplot(gs[1,1])\n\n\nax_sex = ax0\nax_sex.text(-0.45, 550,'Sex & Survived',fontweight='bold', fontsize=10)\nsns.countplot(x='Sex', hue='Survived', data=train, ax=ax_sex, palette='gist_gray')\nsns.despine()\n\nax_parch = ax1\nax_parch.text(-0.45, 400,'Sex & Pclass',fontweight='bold', fontsize=10)\nsns.countplot(x='Sex',hue='Pclass', data=train, ax=ax_parch, palette='gist_gray')\nsns.despine()\n\nax_sibsp = ax2\nax_parch.text(1.95, 400,'Sex & Sibsp',fontweight='bold', fontsize=10)\nsns.countplot(x='Sex', hue='SibSp', data=train, ax=ax_sibsp, palette='gist_gray')\nsns.despine()\n\n\nax_pclass = ax3\nax_pclass.text(-0.45, 530,'Sex & Parch',fontweight='bold', fontsize=10)\nsns.countplot(x='Sex', hue='Parch', data=train, ax=ax_pclass, palette='gist_gray')\nsns.despine()\n\nax_embarked = ax4\nax_embarked.text(-0.45, 480,'Sex & Embarked',fontweight='bold', fontsize=10)\nsns.countplot(x='Sex', hue='Embarked', data=train, ax=ax_embarked, palette='gist_gray')\nsns.despine()\n\n\n\n\n\n# Title & Subtitle    \nfig.suptitle('Correlation with Sex Feature', fontweight='bold', fontsize=20)\nfig.text(s='Survived, Pclass, Age, Parch, Embarked ' , x=0.5, y=0.94,  ha='center',va='top')\n\n\nplt.show();","be45f926":"# Sex, Parch, Embarked, Pclass, SibSip \n\nfig = plt.figure(figsize=(18,15))\ngs = fig.add_gridspec(3,3)\n\n\n\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\nax2 = fig.add_subplot(gs[0,2])\n\n\nax_sex = ax0\nsns.kdeplot(x='Age', hue='Sex', data=train, ax=ax_sex, palette='gist_gray_r', fill=True, alpha=.5, linewidth=0, shade=True)\nsns.despine()\n\n\ntrain.loc[train['Survived'] ==0]['Age'].plot.hist(ax=ax1, bins=20, edgecolor='black', color='lightgray')\nax1 = list(range(0, 80, 5))\n\n\ntrain.loc[train['Survived'] ==1]['Age'].plot.hist(ax=ax2, bins=20, edgecolor='black', color='gray')\nax2 = list(range(0, 80, 5))\n\n\n# Title & Subtitle    \nfig.suptitle('Correlation with Age Feature', fontweight='bold', fontsize=20)\nfig.text(s='                                                    Survived=0                                                Survived=1' , fontweight='bold',fontsize=15, x=0.5, y=0.94,  ha='center',va='top')\n   \n\nplt.show();","26709f36":"# Sex, Parch, Embarked, Pclass, SibSip \n\nfig = plt.figure(figsize=(18,15))\ngs = fig.add_gridspec(3,3)\n\n\n\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\nax2 = fig.add_subplot(gs[0,2])\n\n\nax_sex = ax0\nsns.distplot(train[train['Pclass']==1].Fare,ax=ax_sex, color='gray')\nax_sex.set_title('Fares in Pclass 1',fontweight='bold')\n\nax_sex2 = ax1\nsns.distplot(train[train['Pclass']==2].Fare,ax=ax_sex2, color='gray')\nax_sex2.set_title('Fares in Pclass 2',fontweight='bold')\n\nax_sex3 = ax2\nsns.distplot(train[train['Pclass']==3].Fare,ax=ax_sex3, color='gray')\nax_sex3.set_title('Fares in Pclass 3',fontweight='bold')\n\n\n\n# Title & Subtitle    \nfig.suptitle('Correlation with Fare & Pclass Feature', fontweight='bold', fontsize=20)\n   \n\nplt.show();","584e2739":"# Create a new features with Sibsp & Parch \ntrain['FamilySize'] = train['SibSp'] + train['Parch'] + 1\n\n# Sex, Parch, Embarked, Pclass, SibSip \n\nfig = plt.figure(figsize=(18,15))\ngs = fig.add_gridspec(3,3)\n\n\n\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\nax2 = fig.add_subplot(gs[0,2])\n\n\nsns.countplot(x='FamilySize', data=train,  ax=ax0, palette='gist_gray_r')\nsns.despine()\n\nsns.countplot(x='FamilySize',hue='Survived', data=train, ax=ax1, palette='gist_gray_r')\nsns.despine()\n\nsns.lineplot(x='FamilySize',y='Survived', data=train, ax=ax2, color='gray')\nsns.despine()\n\n# Title & Subtitle    \nfig.suptitle('FamilySize = SibSp + Parch', fontweight='bold', fontsize=20)\n\n\nplt.show();","03dd42a8":"fig = plt.figure(figsize=(16,8))\nsns.heatmap(train.corr(), annot=True, cmap='gray')\nfig.suptitle('Correlation with Every Features', fontweight='bold', fontsize=20)\n\nplt.show()","850d75ea":"train.head(2)","cb64f883":"train.isnull().sum()","76441cf4":"# filling Embarked missing values\ntrain['Embarked'].fillna('S', inplace=True)\ntrain.isnull().sum()","64d90784":"## Fill Age with the median age of similar rows according to Pclass, Parch and SibSp\n# Index of NaN age rows\nindex_NaN_age = list(train[\"Age\"][train[\"Age\"].isnull()].index)\n\nfor i in index_NaN_age :\n    age_med = train[\"Age\"].median()\n    age_pred = train[\"Age\"][((train['SibSp'] == train.iloc[i][\"SibSp\"]) & \n                             (train['Parch'] == train.iloc[i][\"Parch\"]) & \n                             (train['Pclass'] == train.iloc[i][\"Pclass\"]))].median()\n    \n    if not np.isnan(age_pred) :\n        train['Age'].iloc[i] = age_pred\n        \n    else :\n        train['Age'].iloc[i] = age_med","6e4a2581":"train.isnull().sum()","71b5feca":"train.dtypes","ce6909bd":"con_columns = [c for c,t in zip(train.dtypes.index, train.dtypes) if t == 'float64']\ncat_columns = [c for c,t in zip(train.dtypes.index, train.dtypes) if t == 'object']\n\nprint('Continous columns:', con_columns)\nprint('Categorical columns:', cat_columns)\n\n# Pclass, SibSp, Parch  left !","3ad72227":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ntrain[con_columns] = scaler.fit_transform(train[con_columns])\ntrain.head()","8bff6a09":"from sklearn.preprocessing import OneHotEncoder \n\nohe = OneHotEncoder(sparse=False)\nohe.fit(train[cat_columns])\n\n#check out encdoing\nohe.categories_","f5553b1b":"# put the new columns to our data \n\nohe_columns = list()\nfor lst in ohe.categories_:\n    ohe_columns += lst.tolist()\n    \nnew_train = pd.DataFrame(ohe.transform(train[cat_columns]), columns=ohe_columns)","0ee08d03":"new_train.head()","02e792c5":"# concat scaling and encoding features \ntrain = pd.concat([train, new_train], axis=1)\n\n# remove previous categorical features\ntrain = train.drop(columns=cat_columns)\n\n# check \ntrain.head()","2d559254":"# use dummies for pclass \npclass_columns = ['Pclass']\ntrain = pd.get_dummies(data=train, columns=pclass_columns)\ntrain.head()","4ae476c1":"# Create new features of Family Size \ntrain['single'] = train['FamilySize'].map(lambda s: 1 if s == 1 else 0)\ntrain['small'] = train['FamilySize'].map(lambda s: 1 if  s == 2  else 0)\ntrain['median'] = train['FamilySize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ntrain['large'] = train['FamilySize'].map(lambda s: 1 if s >= 5 else 0)","5a78d1e4":"train.head(2)","d768189b":"x = train.drop('Survived', axis=1).values\ny = train['Survived'].values\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.4, random_state=46,shuffle=True)\nx_valid, x_test, y_valid, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=46,shuffle=True)","e74201ff":"# import Library \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.metrics import f1_score","cffd7596":"# 1. LogisticRegression\n\nlr = LogisticRegression()\n\nlr.fit(x_train, y_train)\n\ny_pred = lr.predict(x_valid)\n\nprint(f\"Logistic Regression F1 Score: {f1_score(y_valid, y_pred, average='micro')}\")","f4ce6c64":"# 2. Support Vector Machine\n\nsvc = SVC(probability=True)\n\nsvc.fit(x_train, y_train)\n\ny_pred = svc.predict(x_valid)\n\nprint(f\"Support Vector Machine F1 Score: {f1_score(y_valid, y_pred, average='micro')}\")","40dad4e1":"# 3. Rnadom Forest\n\nrf = RandomForestClassifier()\n\nrf.fit(x_train, y_train)\n\ny_pred = rf.predict(x_valid)\n\nprint(f\"RandomForest F1 Score: {f1_score(y_valid, y_pred, average='micro')}\")","fd8f7c53":"# 4. XGBoost\n\nxgb = XGBClassifier()\n\nxgb.fit(x_train, y_train)\n\ny_pred = xgb.predict(x_valid)\n\nprint(f\"XGBoost F1 Score: {f1_score(y_valid, y_pred, average='micro')}\")","78de4e87":"# 5. LightGBM\n\nlgb = LGBMClassifier()\n\nlgb.fit(x_train, y_train)\n\ny_pred = lgb.predict(x_valid)\n\nprint(f\"LightGBM F1 Score: {f1_score(y_valid, y_pred, average='micro')}\")","64b84419":"# 6. KNeighborsClassifier \n\nknn = KNeighborsClassifier()\n\nknn.fit(x_train, y_train)\n\ny_pred = knn.predict(x_valid)\n\nprint(f\"KNeighborsClassifier F1 Score: {f1_score(y_valid, y_pred, average='micro')}\")","04d09216":"from sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nkfold = KFold(n_splits=10, random_state=2020, shuffle=True) # k=10, split the data into 10 equal parts\nxyz=[]\naccuracy=[]\nstd=[]\nclassifiers=['Logistic Regression',\n             'SVC',\n             'Random Forest',\n             'XGB',\n             'LGBM',\n             'KNeighbors']\n\nmodels=[LogisticRegression(),\n        SVC(),\n        RandomForestClassifier(),\n        XGBClassifier(),\n        LGBMClassifier(),\n        KNeighborsClassifier()]\n\nfor i in models:\n    model = i\n    cv_result = cross_val_score(model,x,y, cv = kfold,scoring = \"accuracy\")\n    cv_result=cv_result\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\nnew_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)       \nnew_models_dataframe2","368b8493":"new_models_dataframe2['CV Mean'].plot.barh(width=0.8)\nplt.title('Average CV Mean Accuracy')\nfig=plt.gcf()\nfig.set_size_inches(8,5)\nplt.show()","0ec8d398":"from sklearn.ensemble import AdaBoostClassifier\nada=AdaBoostClassifier(n_estimators=200, # 200\uac1c\ub77c\ub294 \uac83\uc740 200\uac1c\ub97c \ubd99\uc5ec\uc11c \uc774\uc81c \uc2e4\ud589\uc744 \ud574\uc900\ub2e4\ub294 \uac83\uc744 \ub9d0\ud574\uc8fc\ub294 \uac83\uc774\ub2e4 \n                       random_state=0,\n                       learning_rate=0.1)\nresult=cross_val_score(ada,x,y,cv=10,scoring='accuracy')\nprint('The cross validated score for AdaBoost is:',result.mean())","2780a20d":"First I am going to fill out the missing values. ","d7484214":"First drop the unnecessary columns","97f8c1f5":"Now we can seperate our columns into categorical features and continouse features.\n\n**Categorical Features**: Sex, Parch, Embarked\n\n**Ordinal Features**: Pclass\n\n**Continous Features**: age, fare\n\n**Discrete Feature**: SibSip\n\n**Target Feature**: Survived","433e1617":"Fill out the Embarked missing values with 'S'. Because the maximum passengers boarded from port S ","99975b4c":"# **Part2: Feature Engineering and Data Cleaning:**\n","f3274af3":"### 4) [Sibsp + Parch] -> Family Size ","1dbcedb2":"### Data Check ","bdf923e0":"## **Conclusion of EDA**:\n**Pclass** : the 1st class passengers have lot more chance to survive. 3rd class passengers survival rate seems very low .\n\n**Sex** : Male rate seems higher than women, but the survival rate of women is much more higher than male in every Pclass and age. \n\n**Age** : We can check out that children have higher chance to survive. \n\n**Embarked** : The chances of survival at C looks to be better than even though the majority of Pclass1 passengers got up at S.","70201e3e":"There is missing values in age, cabin, embarked. \n\nWe will going to fill this out later. Let's check out our data dtypes and unique valeus before EDA. I am going to do my EDA based on the dtypes. ","919d1697":"\n## **1) Modeling**\n#### 1) Logistic Regression\n\n#### 2) Support Vector Machines(Linear and radial)\n\n#### 3) Random Forest\n\n#### 4) LightGBM\n\n#### 5) KNeighborClassifier\n\n#### 6) XGBoost","b4472391":"Age distribution seems to be the same in Male and Female population","1459072e":"# **Part1: Exploratory Data Analysis(EDA):**\nRemember that we have to only fill out the missing values of age, and embarked columns. \n\nWe will figure out by exploratory data analysis. ","b8efdb10":"### 2) [Sex, Embarked] \n### - Categorical Features : Onehot Encdoing","16c78aa6":"## **2) Cross Validation**","0de81f6a":"## **3) Ensembling**","8ff9a7cd":"## Types Of Features\n\n### Categorical Features:\nA categorical variable is one that has two or more categories and each value in that feature can be categorised by them.For example, gender is a categorical variable having two categories (male and female). Now we cannot sort or give any ordering to such variables. They are also known as **Nominal Variables**.\n\n**Categorical Features in the dataset: Sex,Embarked.**\n\n### Ordinal Features:\nAn ordinal variable is similar to categorical values, but the difference between them is that we can have relative ordering or sorting between the values. For eg: If we have a feature like **Height** with values **Tall, Medium, Short**, then Height is a ordinal variable. Here we can have a relative sort in the variable.\n\n**Ordinal Features in the dataset: PClass**\n\n### Continous Feature:\nA feature is said to be continous if it can take values between any two points or between the minimum or maximum values in the features column.\n\n**Continous Features in the dataset: Age**","1b6df756":"## 1) **Data Cleaning**","38f7c9d0":"NOW! The feature engineering is clear! Next we are going to split the train-test set and go modeling ~!","3c184df0":"### Thanks a lot for having a look at this notebook. If you found this notebook useful, Please give me a **Upvote**.","2b9ab3ac":"We can check out the 'Survived' columns are not in test data. \n\nWhich it means that the 'Survived' column is our target value. ","4bbbaaa1":"# **[ Gray Theme ]   EDA for Prediction Titanic** \n![image.png](attachment:885d884e-ffba-44ea-95e9-1fd7df068464.png)\n\n\n# Contents of the Notebook:\n \n# Part1: Exploratory Data Analysis(EDA):\n### 1) Analysis of the features \n\n### 2) Finding relationship between features.\n\n### 3) Making a new feature for modeling.\n\n# Part2: Feature Engineering and Data Cleaning:\n\n### Converting features into suitable form for modeling.\n\n### 1) Continous Features  \n\n### 2) Categorical Features \n\n### 3) Else \n\n# Part3: Predictive Modeling\n\n### 1) Modling\n\n### 2) Cross Validation\n\n### 3) Ensembling\n","73879470":"# **Part3: Predictive Modeling**","faec43bb":"## Categorical & Continuous Features\n\n- Categorical Features - **One-hot-encoding**\n\n- Continuous Features - **StandardScaler**\n","82cd0587":"### 3) [Pclass]","85fa7474":"### 1) [Age, Fare] \n### - Continuous Features: StandardScaler","9555cd0b":"## 2) **Feature Engineering**","1b3efe6b":"We can check out that there is some outliers in the continous features."}}