{"cell_type":{"f105e9c5":"code","82736f59":"code","f685444a":"code","8f76e0c7":"code","cc03a396":"code","debdb8f8":"code","384f5da0":"code","9f101574":"code","78a3527f":"code","2c515798":"code","d6b3a574":"code","09d831e2":"code","966b461b":"code","fdcb5251":"code","60ea0c60":"code","fa13f247":"code","292696db":"code","f42c69e8":"code","64a2d58e":"code","cc7eaffd":"code","c8a6f210":"code","13ce390d":"code","aeabbe3f":"code","8ef0a906":"code","c1f12c5e":"code","765daaba":"code","74551029":"code","aace5af0":"code","d522de6a":"code","ed4ea121":"code","4fc75a0e":"code","ea18664c":"code","498eedfe":"code","e2cd2259":"code","f016eb1a":"code","8c6770e4":"code","dcac72e5":"code","881304cb":"code","d803e6a7":"code","56ec2205":"code","5dcc0e39":"code","d8b2e06a":"code","bf545218":"code","e830921a":"code","fee11826":"code","4353a848":"code","70dbca04":"code","e5bb146f":"code","6a3a175a":"code","1ee4231e":"code","0408f887":"code","8e56a31d":"code","04c773b7":"code","ef60b73b":"code","1c95ba49":"code","d95fd9c5":"code","2430beef":"code","e41e533e":"markdown","99a39274":"markdown","e03893b3":"markdown","c7195525":"markdown","fb6965ae":"markdown","b29e75b6":"markdown","d0f3c9af":"markdown","08996f8c":"markdown","1f30a698":"markdown","91ff67b1":"markdown","4da4d035":"markdown","f506048d":"markdown","9761172a":"markdown","dac2b1ac":"markdown","8cd7b70d":"markdown","7bf5aa88":"markdown","e9216727":"markdown","2b494dd7":"markdown","9bf26e92":"markdown","ddd237a7":"markdown","0d193552":"markdown","35b83088":"markdown","892dbd93":"markdown","df35ca22":"markdown","1a238ebc":"markdown","31e64be5":"markdown","57876b73":"markdown","bd81f588":"markdown","c7abd3c8":"markdown","44e5f866":"markdown","221ae930":"markdown","24126df3":"markdown","5e54da84":"markdown","fd5ddf92":"markdown","341713b4":"markdown","64aded98":"markdown","061f5b06":"markdown","95090a9f":"markdown","24fdf12d":"markdown","0b8992e0":"markdown","8cbde6ef":"markdown","19121cdf":"markdown","c5f43a04":"markdown","ec05620d":"markdown","fa2c35c1":"markdown","9e6c74fe":"markdown","ae0ab6b3":"markdown","33d2845b":"markdown","aa913439":"markdown","050cc0ca":"markdown","9e56e414":"markdown"},"source":{"f105e9c5":"# Ignore Warnings\nimport warnings\nfrom warnings import simplefilter\nwarnings.filterwarnings(\"ignore\")\n\n# Computational imports\nimport numpy as np   # Library for n-dimensional arrays\nimport pandas as pd  # Library for dataframes (structured data)\n\n# Helper imports\nimport os \nimport re\nimport time\nimport warnings\nfrom tqdm import tqdm\nimport datetime as dt\nfrom datetime import datetime\nimport scipy.stats as stats\nfrom pathlib import Path\n\n# ML\/DL imports\nfrom sklearn.preprocessing import MinMaxScaler, OrdinalEncoder, LabelEncoder, RobustScaler\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_probability as tfp\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, RepeatVector, TimeDistributed\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\n# Plotting imports\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as dates\nimport seaborn as sns\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode, iplot\n\n%matplotlib inline\ninit_notebook_mode(connected=True)\n\n# Set seeds to make the experiment more reproducible.\nfrom numpy.random import seed\nseed(1)\n\n# Allows us to see more information regarding the DataFrame\npd.set_option(\"display.max_rows\", 500)\npd.set_option(\"display.max_columns\", 500)","82736f59":"data_path = \"..\/input\/g-research-crypto-forecasting\/\"\n\ncrypto_df = pd.read_csv(data_path + 'train.csv')\nasset_details_df = pd.read_csv(data_path + 'asset_details.csv')","f685444a":"len(crypto_df[crypto_df[\"Asset_ID\"]==1].set_index(\"timestamp\"))","8f76e0c7":"btc = crypto_df[crypto_df[\"Asset_ID\"]==1].set_index(\"timestamp\") # Asset_ID = 1 for Bitcoin\nbtc_mini = btc.iloc[-200:] # Select recent data rows","cc03a396":"candle_stick_graph = go.Candlestick(x=btc_mini.index, open=btc_mini['Open'], high=btc_mini['High'], low=btc_mini['Low'], close=btc_mini['Close'])\nfig = go.Figure(data=[candle_stick_graph])\nfig.show()","debdb8f8":"crypto_df.isnull().sum()","384f5da0":"eth = crypto_df[crypto_df[\"Asset_ID\"] == 6].set_index(\"timestamp\") # Asset_ID = 6 for Ethereum\neth.info(show_counts =True)","9f101574":"eth.isna().sum()","78a3527f":"start_btc = btc.index[0].astype('datetime64[s]')\nend_btc = btc.index[-1].astype('datetime64[s]')\nstart_eth = eth.index[0].astype('datetime64[s]')\nend_eth = eth.index[-1].astype('datetime64[s]')\n\nprint('BTC data goes from', start_btc, 'to', end_btc)\nprint('Ethereum data goes from', start_eth, 'to', end_eth)","2c515798":"(eth.index[1:]-eth.index[:-1]).value_counts().head()","d6b3a574":"eth = eth.reindex(range(eth.index[0],eth.index[-1]+60,60),method='ffill')\neth.head(5)","09d831e2":"(eth.index[1:]-eth.index[:-1]).value_counts().head()","966b461b":"fig  = make_subplots(rows=1, cols=2, \n                    specs=[[{\"type\": \"scatter\"}, {\"type\": \"scatter\"}]],\n                    column_widths=[0.5, 0.5], vertical_spacing=0, horizontal_spacing=0.10,\n                    subplot_titles=(\"BTC vs Time\", \"ETH vs Time\"))\n\nfig.add_trace(go.Scatter(x=btc.index,y=btc.Close,name=\"BTC\"), row=1, col=1)\nfig.add_trace(go.Scatter(x=eth.index,y=eth.Close,name=\"ETH\"), row=1, col=2)\nfig.update_layout(autosize=True, margin=dict(b=0,r=20,l=20), template=\"plotly_dark\", title_font=dict(size=25, color='#8a8d93', family=\"Lato, sans-serif\"), \n                  width=900, height=500, title_text=\"Crypto vs Time\", font=dict(color='#8a8d93'),)\nfig.update_xaxes(title_text=\"Time (seconds)\")\nfig.update_yaxes(title_text=\"Closing Price ($)\")\nfig.show()","fdcb5251":"# auxiliary function, from datetime to timestamp\ntotimestamp = lambda s: np.int32(time.mktime(dt.datetime.strptime(s, \"%d\/%m\/%Y\").timetuple()))\n\n# create intervals\nbtc_mini_2021 = btc.loc[totimestamp('01\/06\/2021'):totimestamp('01\/07\/2021')]\neth_mini_2021 = eth.loc[totimestamp('01\/06\/2021'):totimestamp('01\/07\/2021')]","60ea0c60":"fig  = make_subplots(rows=2, cols=1, \n                    specs=[[{\"type\": \"scatter\"}], [{\"type\": \"scatter\"}]],\n                    column_widths=[0.5], vertical_spacing=0.35, horizontal_spacing=0,\n                    subplot_titles=(\"BTC vs Time (2021)\", \"ETH vs Time (2021)\"))\n\nfig.add_trace(go.Scatter(x=btc_mini_2021.index,y=btc_mini_2021.Close,name=\"BTC\"), row=1, col=1)\nfig.add_trace(go.Scatter(x=eth_mini_2021.index,y=eth_mini_2021.Close,name=\"ETH\"), row=2, col=1)\nfig.update_layout(autosize=True, margin=dict(b=0,r=20,l=20), template=\"plotly_dark\", title_font=dict(size=25, color='#8a8d93', family=\"Lato, sans-serif\"), \n                  width=900, height=500, title_text=\"Crypto vs Time\", font=dict(color='#8a8d93'),)\nfig.update_xaxes(title_text=\"Time (seconds)\")\nfig.update_yaxes(title_text=\"Closing Price ($)\")\nfig.show()","fa13f247":"# define function to compute log returns\ndef log_return(series, periods=1):\n    return np.log(series).diff(periods=periods)","292696db":"lret_btc_mini = log_return(btc_mini_2021.Close)[1:]\nlret_eth_mini  = log_return(eth_mini_2021.Close)[1:]\nlret_btc_mini.rename('lret_btc_mini', inplace=True)\nlret_eth_mini.rename('lret_eth_mini', inplace=True)\n\nplt.figure(figsize=(8,4))\nplt.plot(lret_btc_mini);\nplt.plot(lret_eth_mini);\nplt.show()","f42c69e8":"# join two asset in single DataFrame\nlret_btc_long = log_return(btc.Close)[1:]\nlret_eth_long = log_return(eth.Close)[1:]\nlret_btc_long.rename('lret_btc', inplace=True)\nlret_eth_long.rename('lret_eth', inplace=True)\ntwo_assets = pd.concat([lret_btc_long, lret_eth_long], axis=1)\n\n# group consecutive rows and use .corr() for correlation between columns\ncorr_time = two_assets.groupby(two_assets.index\/\/(10000*60)).corr().loc[:,\"lret_btc\"].loc[:,\"lret_eth\"]\n\ncorr_time.plot();\nplt.xticks([])\nplt.ylabel(\"Correlation\")\nplt.title(\"Correlation between BTC and ETH over time\");","64a2d58e":"# create dataframe with returns for all assets\nall_assets_2021 = pd.DataFrame([])\nfor asset_id, asset_name in zip(asset_details_df.Asset_ID, asset_details_df.Asset_Name):\n  asset = crypto_df[crypto_df[\"Asset_ID\"]==asset_id].set_index(\"timestamp\")\n  asset = asset.loc[totimestamp('01\/01\/2021'):totimestamp('01\/05\/2021')]\n  asset = asset.reindex(range(asset.index[0],asset.index[-1]+60,60),method='pad')\n  lret = log_return(asset.Close.fillna(0))[1:]\n  all_assets_2021 = all_assets_2021.join(lret, rsuffix=asset_name, how=\"outer\")","cc7eaffd":"plt.figure(figsize = (10,7))\nsns.heatmap(all_assets_2021.corr(), annot=True)\nplt.show()","c8a6f210":"moving_average = eth.Close.rolling(\n    window=365*24*60,       # 365-day window\n    center=True,            # puts the average at the center of the window\n    min_periods=182*24*60,  # choose about half the window size\n).mean()                    # compute the mean (could also do median, std, min, max, ...)\n\nax = eth.Close.plot(style=\".\", color=\"0.5\")\nmoving_average.plot(\n    ax=ax, linewidth=3, title=\"Tunnel Traffic - 365-Day Moving Average\", legend=False,\n);","13ce390d":"simplefilter(\"ignore\")\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True, figsize=(11, 5))\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\nplot_params = dict(\n    color=\"0.75\",\n    style=\".-\",\n    markeredgecolor=\"0.25\",\n    markerfacecolor=\"0.25\",\n    legend=False,\n)\n%config InlineBackend.figure_format = 'retina'\n\n\n# annotations: https:\/\/stackoverflow.com\/a\/49238256\/5769929\ndef seasonal_plot(X, y, period, freq, ax=None):\n    if ax is None:\n        _, ax = plt.subplots()\n    palette = sns.color_palette(\"husl\", n_colors=X[period].nunique(),)\n    ax = sns.lineplot(\n        x=freq,\n        y=y,\n        hue=period,\n        data=X,\n        ci=False,\n        ax=ax,\n        palette=palette,\n        legend=False,\n    )\n    ax.set_title(f\"Seasonal Plot ({period}\/{freq})\")\n    for line, name in zip(ax.lines, X[period].unique()):\n        y_ = line.get_ydata()[-1]\n        ax.annotate(\n            name,\n            xy=(1, y_),\n            xytext=(6, 0),\n            color=line.get_color(),\n            xycoords=ax.get_yaxis_transform(),\n            textcoords=\"offset points\",\n            size=14,\n            va=\"center\",\n        )\n    return ax","aeabbe3f":"eth[\"temp_date\"] = eth.index\neth[\"temp_date\"]","8ef0a906":"for idx in eth.index[:1000]:\n    new_date = datetime.utcfromtimestamp(idx).strftime('%Y-%m-%d %H:%M:%S')\n    eth[\"temp_date\"].loc[idx] = new_date","c1f12c5e":"eth.set_index(eth['temp_date'], inplace = True)  \neth.drop(labels = 'temp_date', axis = 1)\neth_seas_plot = eth.iloc[:1000, :]\neth_seas_plot.index = pd.to_datetime(eth_seas_plot.index)","765daaba":"X = pd.DataFrame(eth_seas_plot.Close.copy())\n\n# days within a week\nX[\"day\"] = eth_seas_plot.index.dayofweek  # the x-axis (freq)\nX[\"week\"] = eth_seas_plot.index.week  # the seasonal period (period)\nX[\"minute\"] = eth_seas_plot.index.minute\nX[\"second\"] = eth_seas_plot.index.second\n# days within a year\nX[\"dayofyear\"] = eth_seas_plot.index.dayofyear\nX[\"year\"] = eth_seas_plot.index.year\nfig, ax0 = plt.subplots(1, 1, figsize=(11, 6))\nseasonal_plot(X, y=\"Close\", period=\"week\", freq=\"minute\", ax=ax0)","74551029":"def split_sequences(sequences, timesteps, horizon):\n    Sequences, Targets = list(), list()\n    for i in range(len(sequences)):\n        # find the end of this pattern\n        end_ix = i + timesteps\n        out_end_ix = end_ix + horizon-1\n        # check if we are beyond the dataset\n        if out_end_ix > len(sequences):\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n        Sequences.append(seq_x)\n        Targets.append(seq_y)\n        show_shapes()\n    return array(X), array(y)","aace5af0":"def downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df","d522de6a":"def Normalize(list):\n    list = np.array(list)\n    low, high = np.percentile(list, [0, 100])\n    delta = high - low\n    if delta != 0:\n        for i in range(0, len(list)):\n            list[i] = (list[i]-low)\/delta\n    return  list,low,high\n\ndef FNoramlize(list,low,high):\n    delta = high - low\n    if delta != 0:\n        for i in range(0, len(list)):\n            list[i] = list[i]*delta + low\n    return list\n\ndef Normalize2(list,low,high):\n    list = np.array(list)\n    delta = high - low\n    if delta != 0:\n        for i in range(0, len(list)):\n            list[i] = (list[i]-low)\/delta\n    return  list","ed4ea121":"def show_shapes(Sequences, Targets): # this'll use inputs; can make yours to use local variable values\n    print(\"Expected: (num_samples, timesteps, channels)\")\n    print(\"Sequences: {}\".format(Sequences.shape))\n    print(\"Targets:   {}\".format(Targets.shape))   ","4fc75a0e":"def basic_eda(df):\n    print(\"-------------------------------TOP 5 RECORDS-----------------------------\")\n    print(df.head(5))\n    print()\n    \n    print(\"-------------------------------INFO--------------------------------------\")\n    print(df.info())\n    print()\n    \n    print(\"-------------------------------Describe----------------------------------\")\n    print(df.describe())\n    print()\n    \n    print(\"-------------------------------Columns-----------------------------------\")\n    print(df.columns)\n    print()\n    \n    print(\"-------------------------------Data Types--------------------------------\")\n    print(df.dtypes)\n    print()\n    \n    print(\"----------------------------Missing Values-------------------------------\")\n    print(df.isnull().sum())\n    print()\n    \n    print(\"----------------------------NULL values----------------------------------\")\n    print(df.isna().sum())\n    print()\n    \n    print(\"--------------------------Shape Of Data---------------------------------\")\n    print(df.shape)\n    print()\n    \n    print(\"============================================================================ \\n\")","ea18664c":"def add_features(df):\n    df['Upper_Shadow'] = df['High'] - np.maximum(df['Close'], df['Open'])\n    df['Lower_Shadow'] = np.minimum(df['Close'], df['Open']) - df['Low']\n    \n    df['spread'] = df['High'] - df['Low']\n    df['mean_trade'] = df['Volume']\/df['Count']\n    df['log_price_change'] = np.log(df['Close']\/df['Open'])\n    return df","498eedfe":"data_path = \"..\/input\/g-research-crypto-forecasting\/\"\n\ntrain = pd.read_csv(data_path + 'train.csv').set_index(\"timestamp\")\nassets = pd.read_csv(data_path + 'asset_details.csv')","e2cd2259":"assets_order = pd.read_csv('..\/input\/g-research-crypto-forecasting\/supplemental_train.csv').Asset_ID[:14]\nassets_order = dict((t,i) for i,t in enumerate(assets_order))\nassets_order","f016eb1a":"train_cut = train[15000000:]\n\ntrain_cut[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP','Target']] = \\\ntrain_cut[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP','Target']].astype(np.float32)\n\ntrain_cut.head(10)","8c6770e4":"print(np.sum(train_cut.isna()))\ntrain_cut['Target'] = train_cut['Target'].fillna(method = 'ffill')\nprint('\\n', np.sum(train_cut.isna()))","dcac72e5":"# VWAP column has -inf and inf values. VWAP_max and VWAP_min will be used for replacement\n    \nVWAP_max = np.max(train_cut[np.isfinite(train_cut.VWAP)].VWAP)\nVWAP_min = np.min(train_cut[np.isfinite(train_cut.VWAP)].VWAP)\nprint(VWAP_max, \"\\n\", VWAP_min)\n\ntrain_cut['VWAP'] = np.nan_to_num(train_cut.VWAP, posinf=VWAP_max, neginf=VWAP_min)","881304cb":"df = train_cut[['Asset_ID', 'Target']].copy()\ntimes = dict((t,i) for i,t in enumerate(df.index.unique()))\ndf['id'] = df.index.map(times)\ndf['id'] = df['id'].astype(str) + '_' + df['Asset_ID'].astype(str)\nids = df.id.copy()\n\ndel df","d803e6a7":"train_cut = add_features(train_cut)\ntrain_cut.shape","56ec2205":"scale_features = train_cut.columns.drop(['Asset_ID','Target'])\nRS = RobustScaler()\ntrain_cut[scale_features] = RS.fit_transform(train_cut[scale_features])","5dcc0e39":"ind = train_cut.index.unique()\ndef reindex(df):\n    df = df.reindex(range(ind[0],ind[-1]+60,60),method='nearest')\n    df = df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n    return df","d8b2e06a":"train_cut=train_cut.groupby('Asset_ID').apply(reindex).reset_index(0, drop=True).sort_index()\ntrain_cut.shape","bf545218":"# Matching records and marking generated rows as 'fake'\n\ntrain_cut['group_num'] = train_cut.index.map(times)\ntrain_cut = train_cut.dropna(subset=['group_num'])\ntrain_cut['group_num'] = train_cut['group_num'].astype('int')\n\ntrain_cut['id'] = train_cut['group_num'].astype(str) + '_' + train_cut['Asset_ID'].astype(str)\n\ntrain_cut['is_real'] = train_cut.id.isin(ids)*1\ntrain_cut = train_cut.drop('id', axis=1)","e830921a":"# Features values for 'non-real' rows are set to zeros\n\nfeatures = train_cut.columns.drop(['Asset_ID','group_num','is_real'])\ntrain_cut.loc[train_cut.is_real == 0, features] = 0.","fee11826":"# Sorting assets according to their order in the 'supplemental_train.csv'\n\ntrain_cut['asset_order'] = train_cut.Asset_ID.map(assets_order) \ntrain_cut=train_cut.sort_values(by=['group_num', 'asset_order'])\ntrain_cut.head(20)","4353a848":"train_cut['asset_order'] = train_cut['asset_order'].astype('float64')","70dbca04":"train_targets = train_cut['Target'].to_numpy().reshape(-1, 14)\n\nfeatures = train_cut.columns.drop(['Asset_ID', 'Target', 'group_num','is_real'])\ntrain_cut = train_cut[features]\n\ntrain_cut=np.array(train_cut)\ntrain_cut = train_cut.reshape(-1,14,train_cut.shape[-1])\ntrain_cut.shape","e5bb146f":"train_cut = train_cut.astype('float64')\ntrain_cut.shape","6a3a175a":"# timeseriesgenerator-like class, except it using target from the last timestep insteed of last+1\nclass sample_generator(keras.utils.Sequence):\n    def __init__(self, x_set, y_set, batch_size, length):\n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n        self.length = length\n        self.size = len(x_set)\n\n    def __len__(self):\n        return int(np.ceil(len(self.x) \/ float(self.batch_size)))\n\n    def __getitem__(self, idx):\n        batch_x=[]\n        batch_y=[]\n        for i in range(self.batch_size):\n            start_ind = self.batch_size*idx + i\n            end_ind = start_ind + self.length \n            if end_ind <= self.size:\n                batch_x.append(self.x[start_ind : end_ind])\n                batch_y.append(self.y[end_ind -1])\n\n        return np.asarray(batch_x).astype(\"float32\"), np.asarray(batch_y).astype(\"float32\")","1ee4231e":"#last 10% of the data are used as validation set\nX_train, X_test = train_cut[:-len(train_cut)\/\/10], train_cut[-len(train_cut)\/\/10:]\ny_train, y_test = train_targets[:-len(train_cut)\/\/10], train_targets[-len(train_cut)\/\/10:]","0408f887":"BATCH_SIZE=2**10\ntrain_generator = sample_generator(X_train, y_train, length=15, batch_size=BATCH_SIZE)\nval_generator = sample_generator(X_test, y_test, length=15, batch_size=BATCH_SIZE)\n\nprint(f'Sample shape: {train_generator[0][0].shape}')\nprint(f'Target shape: {train_generator[0][1].shape}')","8e56a31d":"#https:\/\/github.com\/tensorflow\/tensorflow\/issues\/37495\ndef MaxCorrelation(y_true,y_pred):\n    \"\"\"Goal is to maximize correlation between y_pred, y_true. Same as minimizing the negative.\"\"\"\n    mask = tf.math.not_equal(y_true, 0.)\n    y_true_masked = tf.boolean_mask(y_true, mask)\n    y_pred_masked = tf.boolean_mask(y_pred, mask)\n    return -tf.math.abs(tfp.stats.correlation(y_true_masked,y_pred_masked, sample_axis=None, event_axis=None))\n\ndef Correlation(y_true,y_pred):\n    return tf.math.abs(tfp.stats.correlation(y_pred,y_true, sample_axis=None, event_axis=None))\n\ndef masked_mse(y_true, y_pred):\n    mask = tf.math.not_equal(y_true, 0.)\n    y_true_masked = tf.boolean_mask(y_true, mask)\n    y_pred_masked = tf.boolean_mask(y_pred, mask)\n    return tf.keras.losses.mean_squared_error(y_true = y_true_masked, y_pred = y_pred_masked)\n\ndef masked_mae(y_true, y_pred):\n    mask = tf.math.not_equal(y_true, 0.)\n    y_true_masked = tf.boolean_mask(y_true, mask)\n    y_pred_masked = tf.boolean_mask(y_pred, mask)\n    return tf.keras.losses.mean_absolute_error(y_true = y_true_masked, y_pred = y_pred_masked)\n\ndef masked_cosine(y_true, y_pred):\n    mask = tf.math.not_equal(y_true, 0.)\n    y_true_masked = tf.boolean_mask(y_true, mask)\n    y_pred_masked = tf.boolean_mask(y_pred, mask)\n    return tf.keras.losses.cosine_similarity(y_true_masked, y_pred_masked)\n\ndef get_model(n_assets=14):  \n    x_input = keras.Input(shape=(train_generator[0][0].shape[1], n_assets, train_generator[0][0].shape[-1]))\n\n    branch_outputs = []\n        \n    for i in range(n_assets):\n            # Slicing the ith asset:\n        a = layers.Lambda(lambda x: x[:,:, i])(x_input)\n        a = layers.Masking(mask_value=0.,)(a)\n        a = layers.LSTM(units=32, return_sequences=True)(a)\n        a = layers.Dropout(0.2)(a)\n        a = layers.LSTM(units=16)(a)\n        a = layers.Dropout(0.2)(a)\n        branch_outputs.append(a)\n    \n    x = layers.Concatenate()(branch_outputs)\n    x = layers.Dense(units=128)(x)\n    out = layers.Dense(units=n_assets)(x)\n    \n    model = keras.Model(inputs=x_input, outputs=out)\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3), \n                  loss = masked_cosine,\n                  metrics=[Correlation]\n                 )\n    \n    return model \n    \nmodel=get_model()\nmodel.summary()","04c773b7":"#example with 3 assets for visibility\ntf.keras.utils.plot_model(get_model(n_assets=3), show_shapes=True)","ef60b73b":"tf.random.set_seed(10)\n\nestop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min',restore_best_weights=True)\nscheduler = keras.optimizers.schedules.ExponentialDecay(1e-3, (0.5*len(X_train)\/BATCH_SIZE), 1e-3)\nlr = keras.callbacks.LearningRateScheduler(scheduler, verbose = 1)\n    \nhistory = model.fit(train_generator, validation_data = (val_generator), epochs = 10, callbacks = [lr])","1c95ba49":"fig, ax = plt.subplots(1,2, figsize=(16,8))\n\nhistories = pd.DataFrame(history.history)\n\nepochs = list(range(1,len(histories)+1))\nloss = histories['loss']\nval_loss = histories['val_loss']\nCorrelation = histories['Correlation']\nval_Correlation = histories['val_Correlation']\n\nax[0].plot(epochs, loss, label = 'Train Loss')\nax[0].plot(epochs, val_loss, label = 'Val Loss')\nax[0].set_title('Losses')\nax[0].set_xlabel('Epoch')\nax[0].legend(loc='upper right')\n\nax[1].plot(epochs, Correlation, label = 'Train Correlation')\nax[1].plot(epochs, val_Correlation, label = 'Val Correlation')\nax[1].set_title('Correlations')\nax[1].set_xlabel('Epoch')\nax[1].legend(loc='upper right')\n\nfig.show()","d95fd9c5":"sns.set_style('darkgrid') # darkgrid, white grid, dark, white and ticks\ncolors = sns.color_palette('pastel') # Color palette to use\nplt.rc('axes', titlesize=18)     # fontsize of the axes title\nplt.rc('axes', labelsize=14)    # fontsize of the x and y labels\nplt.rc('xtick', labelsize=13)    # fontsize of the tick labels\nplt.rc('ytick', labelsize=13)    # fontsize of the tick labels\nplt.rc('legend', fontsize=13)    # legend fontsize\nplt.rc('font', size=13)          # controls default text sizes\n\n# # Set Matplotlib defaults\n# plt.style.use(\"seaborn-whitegrid\")\n# plt.rc(\"figure\", autolayout=True, figsize=(11, 5))\n# plt.rc(\n#     \"axes\",\n#     labelweight=\"bold\",\n#     labelsize=\"large\",\n#     titleweight=\"bold\",\n#     titlesize=16,\n#     titlepad=10,\n# )\n# plot_params = dict(\n#     color=\"0.75\",\n#     style=\".-\",\n#     markeredgecolor=\"0.25\",\n#     markerfacecolor=\"0.25\",\n#     legend=False,\n# )\n# %config InlineBackend.figure_format = 'retina'\n\n\ndef plot_loss(history):\n    fig, ax = plt.subplots(figsize=(10,6), tight_layout=True)\n    ax.plot(history.history['loss'], 'o-', color=\"#004C99\", linewidth=2)\n    ax.plot(history.history['val_loss'], 'o-', color=\"#D96552\",linewidth=2)\n    ax.set_facecolor(colors[-1])\n    plt.grid(b=True,axis = 'y')\n    ax.grid(b=True,axis = 'y')\n    plt.ylabel('Loss')\n    plt.xlabel('epoch')\n    plt.legend(['Train loss', 'Validation loss'], loc='upper right',prop={'size': 15})\n    plt.show()\n    \ndef plot_future(prediction, y_test):\n    fig, ax = plt.subplots(figsize=(10,6), tight_layout=True)\n    range_future = len(prediction)\n    ax.plot(np.arange(range_future), np.array(y_test),label='Actual',color=\"#004C99\")\n    ax.plot(np.arange(range_future),np.array(prediction),label='Prediction',color=\"#D96552\")\n    ax.set_facecolor(colors[-1])\n    plt.grid(b=True,axis = 'y')\n    ax.grid(b=True,axis = 'y')\n    plt.ylabel('USD')\n    plt.legend(loc='upper left',prop={'size': 15})\n    plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)   \n    plt.show()","2430beef":"plot_loss(history)","e41e533e":"I'm going to truncate a the training data just for the notebook's sake. Or else, the training would take an eternity,","99a39274":"Let's make a new series named ids, which stores what timestep the current row is part of. For example, for timestep xxxxxx (first timestep) of Cardano (#3), the ids for this row would be 1_3 (representing first timestep and asset id 3 which is Cardano).","e03893b3":"Let's add some extra features that will help with the prediction. We will be using the add_features function we defined previously.","c7195525":"## Reading and Preparing the Data\nLet's start by reading our data. We will store it in many dataframes.","fb6965ae":"This feature is specific to this competition. It automatically adds usefull features to the DataFrame.","b29e75b6":"## Plotting: Trend\nSimple trend plotting with rolling average. We used a 365 day window (in minutes, thus the *24*60)","d0f3c9af":"We notice that all timesteps aren't equally spaced. That is a problem we have to solve. We solve it by reindexing the index.","08996f8c":"## Plotting: BTC and ETC vs time\nWe plot both BTC and ETC coin side by side to analyze and observe if they have any similarities. To do so, we use plotly subplots.","1f30a698":"### Split Sequences\nA key component of time-series problem is splitting our input data into sequences that we can feed to our LSTM network. This sequences depend on the required timesteps and horizons. ","91ff67b1":"## Creating the LSTM Network\nWe are going to be creating a multivariate Parallel LSTM Network with a dense layer at the end. We are using dropout as a regularisation method to combat overfitting. Notice the lambda layer which allows to add our own functional layer. In this case, the lambda layer is used to slice the array to the respective crypto we want. We then loop with the for loop and create the same layer structure for each crypto. The hidden layer output is then concatenanted for each crypto and then the prediction it obtained through the dense layer. \n\n* Build the model. Simplified  structure:\n    - Lambda layer needed for assets separation\n    - Masking layer. Generated records (filled gaps) has zeros as features values, so they are not used in the computations\n    - LSTM or GRU layer\n    - Dropout as a regularisation method to combat overfitting\n    - Concatanate layer\n    - Dense Layer for Prediction (linear activation which is the default)","4da4d035":"## Plotting: Correlations","f506048d":"## Plotting: Seasonal Trend\nFirst we have a couple of functions that will allows us to easily plot the seasonal trend.","9761172a":"Let's check out the intervals between all timesteps and see if they are all homogenous. If they are not, we will have to fix them.","dac2b1ac":"We use plotly library and its CandleStick graph. ","8cd7b70d":"## Fixing Timesteps\nWe have to re-index the timestep (like we did for plotting). I chose to do it in a different fashion here just to practice different methods. We will be using the reindex() function to do so.","7bf5aa88":"## Split into sequence and target\nAfter sequencing and normalzing the data, we slice the data to create the input sequences and output targets.","e9216727":"## Stadardizing the training data\nHere we stadardize our training data with RobustScaler. As the anme states, robust scaler is robust to outlier data. This is good for random data points that dont' really follow the data distribution. This will increase models accuracy and speed.","2b494dd7":"## Training\/Fitting time\nWe can finally train our model with our training data. Let's see how it does.","9bf26e92":"## Reading and Preparing the Data\nLet's start by reading our data. We will store it in many dataframes.","ddd237a7":"We solved it by reindexing the dataframe and forward filling for all the missing values. ","0d193552":"Let's create a dict name assets_order which tracks the true asset order in the Dataframe (for example, in the DataFrame, asset order #3 (Cardano) starts and is followed by #2). This is important so that we can create a new feature in the training dataframe named asset order which goes from 0 -> 13 for one timestep and then resets to 0 for the next timestep.","35b83088":"# Introduction \n`V1.0.0`\n### Who am I\nJust a fellow Kaggle learner. I was creating this Notebook as practice and thought it could be useful to some others \n### Who is this for\nThis Notebook is for people that learn from examples. Forget the boring lectures and follow along for some fun\/instructive time :)\n### What can I learn here\nYou learn all the basics needed to create a rudimentary RNN\/LSTM Parallel Network. I go over a multitude of steps with explanations. Hopefully with these building blocks,you can go ahead and build much more complex models.\n\n### Things to remember\n+ Please Upvote\/Like the Notebook so other people can learn from it\n+ Feel free to give any recommendations\/changes. \n+ I will be continuously updating the notebook. Look forward to many more upcoming changes in the future.\n\n### You can also refer to these notebooks that have helped me as well:\n+ https:\/\/www.kaggle.com\/yamqwe\/g-research-lstm-starter-notebook#Training-%F0%9F%8F%8B%EF%B8%8F\n\n+ https:\/\/www.kaggle.com\/vmuzhichenko\/g-research-parallel-lstm-training\n","892dbd93":"## Splitting the training data into sequences\nIn this section, we split the training data into sequences that we can further feed into LSTM network. Notice that each sequence has many variables\/features making it a multivariate problem. To predict the next 1 days (our horizon), we are going to use the events that occureed 15 days ago.","df35ca22":"Finally, lets build a similar graph, but this time only observing 2021 data.","1a238ebc":"Now that we have set a new datetime index, we can proceed with the plotting.","31e64be5":"### Normalization \nThese functions are used to normalize our data. This aids with model performance and speed. You can also use the scikit-learn MinMaxScaler if you wish, it is up to you.","57876b73":"# Imports\nFirst let us start by importing the relevant libraries that we need.","bd81f588":"## Plotting model accuracy and loss\nThis step is very important since it allows you to see if your model is performing well as you train it. If it isn't, you will rather have to create new features, tune hyperparameters, modify the RNN network or cry.","c7abd3c8":"# Plotting\nPreparing the data and plotting different kinds of plots.","44e5f866":"### Show Shapes\nThis functions is used to quickly check the shapes of our numpy arrays. This is especially important to assure we have the right shape for our LSTM network.","221ae930":"## Removing fake records\nHere we find fake records. Fake records are timesteps where we do not have a record for a specific crypto. Fake records have all column values set to 0.","24126df3":"## Taking care of NaN (missing values) in the dataframe\nThere is multiple ways of doing this, I have chosen do it with the following method:","5e54da84":"There is indeed many missing values here, we will take care of them later.","fd5ddf92":"Before anything, we have to transform the index into a datetime format to be able to plot the seasonal plot.","341713b4":"We have equally spaced data now!","64aded98":"## Plotting: Heatmaps\nWe are going to plot the heatmap here with the seaborn library. The heatmap will show us visualy the correlation between different assets. The hotter (red) it is, the higher the correlation.","061f5b06":"## Plotting: Log Returns \nIn order to analyze price changes for an asset we can deal with the price difference. However, different assets exhibit different price scales, so that the their returns are not readily comparable. We can solve this problem by computing the percentage change in price instead, also known as the return. This return coincides with the percentage change in our invested capital.\n\nReturns are widely used in finance, however log returns are preferred for mathematical modelling of time series, as they are additive across time. Also, while regular returns cannot go below -100%, log returns are not bounded.\n\nTo compute the log return, we can simply take the logarithm of the ratio between two consecutive prices. The first row will have an empty return as the previous value is unknown, therefore the empty return data point will be dropped.","95090a9f":"Let us now use plot model method to validate our network visually.","24fdf12d":"Here we order the dataframe using the asset_order column we will create. This is to ensure that all new timesteps start with the same token (in our case, Asset_ID of 3 which is Cardano)","0b8992e0":"\n# Dataset Structure \n\n> **train.csv** - The training set\n> \n> 1.  timestamp - A timestamp for the minute covered by the row.\n> 2.  Asset_ID - An ID code for the cryptoasset.\n> 3.  Count - The number of trades that took place this minute.\n> 4.  Open - The USD price at the beginning of the minute.\n> 5.  High - The highest USD price during the minute.\n> 6.  Low - The lowest USD price during the minute.\n> 7.  Close - The USD price at the end of the minute.\n> 8.  Volume - The number of cryptoasset u units traded during the minute.\n> 9.  VWAP - The volume-weighted average price for the minute.\n> 10. Target - 15 minute residualized returns. See the 'Prediction and Evaluation section of this notebook for details of how the target is calculated.\n> 11. Weight - Weight, defined by the competition hosts [here](https:\/\/www.kaggle.com\/cstein06\/tutorial-to-the-g-research-crypto-competition)\n> 12. Asset_Name - Human readable Asset name.\n> \n>\n> **example_test.csv** - An example of the data that will be delivered by the time series API.\n> \n> **example_sample_submission.csv** - An example of the data that will be delivered by the time series API. The data is just copied from train.csv.\n> \n> **asset_details.csv** - Provides the real name and of the cryptoasset for each Asset_ID and the weight each cryptoasset receives in the metric.\n> \n> **supplemental_train.csv** - After the submission period is over this file's data will be replaced with cryptoasset prices from the submission period. In the Evaluation phase, the train, train supplement, and test set will be contiguous in time, apart from any missing data. The current copy, which is just filled approximately the right amount of data from train.csv is provided as a placeholder.\n>\n> - There are 14 coins in the dataset\n>\n> - There are 4 years  in the [full] dataset","8cbde6ef":"We do see a strong correlation between BTC and ETH. This is what we expect. The price of ETH is known to be strongly swayed and dependent on the price of BTC.","19121cdf":"## Helper Functions","c5f43a04":"The VMAP feature has some nan values. Let's take care of them by replacing by our defined VMAP_max, VMAP_min.","ec05620d":"### Downcasting\nThis functions is used to downcast our variables to types that take less memory. This helps with model performance and speed.","fa2c35c1":"Let's check more precisely on one crypto: Ethereum.","9e6c74fe":"### Exploraty Data Analysis for pandas\nThis functions is used to quickly check the basic attributes of our pandas DataFrame.","ae0ab6b3":"Let us see how many timesteps are in this dataframe.","33d2845b":"# Training and Predicting\nThis section contains all necessary steps to train and predict your model.","aa913439":"## Taking care of NaN (missing values) in the dataframe\nThis allows us to check how many null\/nan values each column has.\n","050cc0ca":"Let's make an inline functions that will take care of transforming string dates into posix timestamps.","9e56e414":"## Plotting: Candlestick plots\nBitcoin has Asset_ID of 1. We select only the recent 200 timesteps of data (-200: refers to LAST_TIMESTEP-200:LAST_TIMESTEP). We call it btc_mini since it is a miniscule portion of the whole dataframe"}}