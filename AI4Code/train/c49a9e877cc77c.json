{"cell_type":{"ba65d46c":"code","316f0710":"code","94600a8f":"code","4bc1d1d2":"code","c9f4bea7":"code","bab002a6":"code","fe30ca77":"code","53b6156d":"code","7d4939cc":"code","e26bc92d":"code","a9f89192":"code","5f6bf4ea":"code","8d567f5d":"code","e6a5eab0":"code","9f3d00e6":"code","17c745ec":"code","8e855a80":"code","7f0e6013":"code","02d4aecc":"code","2b53c57d":"code","1b77711f":"code","102fdb11":"code","ed0f4252":"code","adfa0df0":"code","62162382":"code","ea2ccb83":"code","d1075ea4":"code","60438e84":"code","639c3bff":"code","3d492fef":"code","c911a559":"code","e3e48d03":"code","f235defb":"code","61a3a830":"code","c4cc82fd":"code","5bbd366e":"code","eff0468c":"code","72c39ec4":"code","8802a293":"code","b5e335aa":"code","d50ab45a":"code","0bac572f":"markdown","84add8b1":"markdown","10ebead7":"markdown","1904f02f":"markdown","bcda2a4b":"markdown","82aae70d":"markdown","33cce61e":"markdown","5e7e0dfd":"markdown","cd67e6dc":"markdown","974015e4":"markdown","374d1265":"markdown","a0fda401":"markdown","d3fb4d1e":"markdown","1f6c1ea6":"markdown","7bb5bd2e":"markdown","1e2dfb06":"markdown","7d021820":"markdown","192cbc3e":"markdown","bdc0c4d4":"markdown","85501904":"markdown","5b8a8b67":"markdown","3726e13c":"markdown","ca4ac90e":"markdown","74189a6c":"markdown","e6f21ac0":"markdown","7e316c47":"markdown","e5e3f30f":"markdown","a632bbaf":"markdown","cbdd8af1":"markdown","32aab505":"markdown","9539e81a":"markdown","51612d37":"markdown","770477ab":"markdown","414a9068":"markdown","90a1926a":"markdown","1e805be8":"markdown","996f4c72":"markdown","b1aa9529":"markdown","d243249f":"markdown","4f832922":"markdown"},"source":{"ba65d46c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom scipy.stats import normaltest , skewtest ,kurtosistest\nfrom scipy.stats import chi2_contingency\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split , KFold\nfrom sklearn.metrics import accuracy_score\nimport xgboost\nimport csv as csv\nfrom xgboost import plot_importance\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import cross_val_score\n\n# train a logistic regression model on the training set\nfrom sklearn.linear_model import LogisticRegression\n\n# get ANOVA table as R like output\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nimport warnings\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","316f0710":"import warnings\nwarnings.simplefilter(\"ignore\", UserWarning)\nimport warnings\nwarnings.simplefilter(\"ignore\", FutureWarning)","94600a8f":"df=pd.read_csv('\/kaggle\/input\/hr-data-for-analytics\/HR_comma_sep.csv')","4bc1d1d2":"df.head()","c9f4bea7":"df.isnull().sum()","bab002a6":"df.info()","fe30ca77":"df.describe(include='all')","53b6156d":"categorical = ['promotion_last_5years' ,'left' ,'Work_accident' ,'salary' , 'sales']","7d4939cc":"for i in categorical :\n    df[i] = df[i].astype('category')","e26bc92d":"df.describe(include=['category'])","a9f89192":"df.describe().T","5f6bf4ea":"outliers = (df.select_dtypes(['float' , 'int'])\n .apply(lambda x: (x >  plt.boxplot(x)['whiskers'][1].get_ydata()[1] ) \n        | (x <  plt.boxplot(x)['whiskers'][0].get_ydata()[1] ) ))\nplt.close()\n","8d567f5d":"(outliers).mean()*100","e6a5eab0":" plt.boxplot(df['time_spend_company'])","9f3d00e6":"new_col =[]\nfor i in outliers.columns : \n    j =str(i) + '_outlier'\n    new_col.append(j)\n    \noutliers.columns = new_col","17c745ec":"df_cleaning= df.groupby(['left' , 'salary']).mean()\nnew_col =[]\nfor i in df_cleaning.columns : \n    j =str(i) + '_mean'\n    new_col.append(j)\n    \ndf_cleaning.columns = new_col\n\ndf_cleaning.reset_index(inplace=True)\n\ndf_cleaning","8e855a80":"df = pd.concat([df  ,outliers['time_spend_company_outlier'] ],axis=1).merge(df_cleaning.loc[:,['left' , 'salary','time_spend_company_mean']] , on = ['left' ,'salary'])\n              ","7f0e6013":"df['time_spend_company'] = np.where(df['time_spend_company_outlier'] == True,np.round(df['time_spend_company_mean'],0) , df['time_spend_company']  )","02d4aecc":"df.drop(columns=['time_spend_company_outlier' , 'time_spend_company_mean'] , inplace=True)","2b53c57d":"df.head()","1b77711f":"def show_values(axs, orient=\"v\", space=.01):\n    def _single(ax):\n        if orient == \"v\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() \/ 2\n                _y = p.get_y() + p.get_height() + (p.get_height()*0.01)\n                value = '{:.1f}'.format(p.get_height())\n                ax.text(_x, _y, value, ha=\"center\") \n        elif orient == \"h\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() + float(space)\n                _y = p.get_y() + p.get_height() - (p.get_height()*0.5)\n                value = '{:.1f}'.format(p.get_width())\n                ax.text(_x, _y, value, ha=\"left\")\n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _single(ax)\n    else:\n        _single(axs)","102fdb11":"for i in df.select_dtypes(['float' , 'int']).columns:\n    \n    print(str(i) + ' describe')\n    print(' ')\n\n    print(df[i].describe() )\n    print(' ')\n    sns.distplot(df[i] , )\n    stat, p = normaltest(df[i])\n    \n    \n    if p > 0.001:\n\t    print(str(i) +' Sample looks Gaussian (fail to reject H0)')\n    else:\n\t    print(str(i) + ' Sample does not look Gaussian (reject H0)')\n    \n    \n    \n    p = skewtest(df[i])[1]\n    \n    if p > 0.001:\n\t    print(str(i) +' Sample does not look skewed (fail to reject H0)')\n    else:\n\t    print(str(i) + ' Sample looks skewed (reject H0)')\n    \n    p = kurtosistest(df[i])[1]\n    \n    \n    if p > 0.001:\n\t    print(str(i) +' Sample does not look kurtosis (fail to reject H0)')\n    else:\n\t    print(str(i) + ' Sample looks kurtosis (reject H0)')\n    \n\n    print(' ')\n    sns.distplot(df[i] , )\n    plt.show()\n    print('____________________________')\n    print(' ')","ed0f4252":"for i in categorical :\n    df[i] = df[i].astype('category')\n    print(str(i) + ' describe')\n    print(' ')\n    show_values(sns.barplot(df[i].value_counts().index ,df[i].value_counts().values))\n    plt.xticks(rotation=45)\n\n    plt.show()","adfa0df0":"for i in df.select_dtypes(['float' , 'int']).columns:\n\n# Ordinary Least Squares (OLS) model\n    sns.boxplot(x='left', y=i, data=df, color='#99c2a2')\n    plt.show()\n    model = ols( '{} ~ C(left)'.format(i), data=df).fit()\n    print(sm.stats.anova_lm(model, typ=2))\n","62162382":"sns.heatmap(df.select_dtypes(['float' , 'int']).corr() , annot = True , vmin=-1, vmax=1, center= 0, cmap= 'RdYlGn')","ea2ccb83":"for i in df.select_dtypes(['category']).drop(columns=['left']).columns:\n    p =  (chi2_contingency(pd.crosstab(df[i], df['left']))[1])\n    if p < .001 :\n        print('employee choice regarding leaving gets affected by {} (reject H0)'.format(i))\n    else :\n        print('employee choice regarding leaving not affected by {} (fail to reject H0)'.format(i))\n\n    contigency_pct = pd.crosstab(df[i], df['left'], normalize='index')\n    sns.heatmap(contigency_pct, annot=True, cmap=\"YlGnBu\")\n    plt.show()\n\n    \n   ","d1075ea4":"X = pd.get_dummies(df , columns=['Work_accident',  'promotion_last_5years', 'sales', 'salary'] , drop_first=True).drop(columns=['left'])","60438e84":"Y = df['left']","639c3bff":"columns =X.columns ","3d492fef":"x_train , x_test , y_train , y_test = train_test_split(X , Y )","c911a559":"scaler  = preprocessing.MinMaxScaler()\n","e3e48d03":"x_train= scaler.fit_transform(x_train)\n\nx_test = scaler.fit_transform(x_test)","f235defb":"\n\n# instantiate the model\nlogreg = LogisticRegression( random_state=0)\n\n\n# fit the model\nlogreg.fit(x_train, y_train)","61a3a830":"y_pred_train = logreg.predict (x_train)","c4cc82fd":"accuracy_score(y_train,y_pred_train.reshape(-1,))","5bbd366e":"\ny_pred_test = logreg.predict (x_test)\n","eff0468c":"accuracy_score(y_test,y_pred_test.reshape(-1,))","72c39ec4":"rf = RandomForestClassifier(n_estimators=50)\nrf.fit(x_train, y_train)\n\nprint('Training Accuracy: {:.2f} '.format(rf.score(x_train, y_train)*100))","8802a293":"print('test Accuracy: {:.2f} '.format(rf.score(x_test, y_test)*100))","b5e335aa":"importances = rf.feature_importances_\nindices = np.argsort(importances)","d50ab45a":"plt.figure(figsize=(15,15))\nplt.title('Feature Importances')\nplt.barh(range(len(indices[-5:])), importances[indices][-5:]*100, color='b', align='center')\nplt.yticks(range(len(indices[-5:])), [columns[i] for i in indices[-5:]])\nplt.xlabel('Relative Importance')\nplt.show()","0bac572f":"* To fix this issue we get the average of time spend company across (whether left or not , and salary category ) \n\nbelow generating the average data","84add8b1":"#### A -  we check missing values issues : \n * We check presence of missing values ","10ebead7":"#### A - Relations between (Left) and scale variables :\n* We check how Scale variables change across (Left)\n* We check whether difference is valid","1904f02f":"for example the average satisfaction_level_mean among employees who did not leave and their salary is high is (0.651879) and so on","bcda2a4b":"#### First : one variable Analysis ","82aae70d":"#### B - Check correlation between scale variables \n * To get rid of milti collinearity (i.e if to dependent variables depend on each other)\n * Strong dependency if correlation is more than 70 % on both directions\n  ","33cce61e":"Generally , when we try the box-whiskers plot to check the presence of outlier values ","5e7e0dfd":"We define the formula to make plots with data labels","cd67e6dc":"* We define training and test set\n* Also we create dummy variables for categorical variables ","974015e4":"checking describtive statistics , to make sure about wrongly stored features or outliers","374d1265":"> Second : Running RandomForest Regression","a0fda401":"* We notice that there are no outliers in the data except in the vaiable (time_spend_company).\n\n* Despite they may be correct data but we remove them in order to normalize data as much as we can , as well for the sake of showing the process","d3fb4d1e":"We find that almost all variables differ significantly across (Left) categories except (number of projects ) and (average_monthly_hours)","1f6c1ea6":"#### Geathering Data","7bb5bd2e":"We have a quick look on data structure","1e2dfb06":"thus we find no missing values , then let's check how dfferent features are stored and whether or not datatypes are correct ","7d021820":"then we replace outlier values with average values","192cbc3e":"### Breif about Data :\n\nData is about Human resources in a specific company each record represents an employee\n\n#### Variables description : \n\nDependent variable \n* Left : 0 if employee did not leave , 1 if left company\n\nIndependent variables \n\n* satisfaction_level : means how much employee satisfied (0 less satisfied , 1 most satisfied)\n* last_evaluation : means employees' evaluation for last month (0 bad , 1 Excellent)\n* number_project : number of projects the employee worked on\n* average_montly_hours : average months employee spends at work per month\n* time_spend_company : years the employee spent in a company\n* Work_accident : 0 if he did not have an accident , 1 if had at least one\n* promotion_last_5years : 0 if he did not have any promotion in last 5 years , 1 if had at least one\n* sales : department in which employee works\n\n\n#### Our Question is :\n\nWhat are factors causing labour to leave and how we predict that","bdc0c4d4":"#### C - Relations between (Left) and category variables and how it varies across different categories of each variable","85501904":"### Third : Modelling","5b8a8b67":"### Importing necessary libraries and methods","3726e13c":"#### B - Modelling phase \n> first we try logestic regression","ca4ac90e":"#### A - checking the scale variables distribution :\n * We check if there is outliers\n * check the distrubution of the variable\n * check skewness and kurtosis","74189a6c":"Now , we merge the average (time_spend_company) per (left) category per(Salary) category , and whether or not the value is outlier into our dataset","e6f21ac0":"### Second : Two variables relations ","7e316c47":" > testing logestic regression","e5e3f30f":"* We find that all variables not normal so we need to transform them with log . minmax , or any other transformation \n\n* note that maximum variables in (time_spend_company) is 5 not 10 as before fixing outliers","a632bbaf":"we find a large  difference between the third quartile and the maximum value of (time_spend_company)","cbdd8af1":"#### A - preparing data for modelling :","32aab505":"#### C - Detecting and fixing Outliers Issues : \n * Now , we check the numeric features to find any outliers ","9539e81a":"#### Asessing Data (checking for cleaning and tidy issues)\n","51612d37":"#### B - Detecting and Fixing variables type  issues :\n\n* we find the problem that the variables (work accident , left , and promotion last 5 years) are stored as numeric despite they are binary (i.e expressing whether or not the employee left , pomoted in last five years , or had accident) so we have to make them categorical ","770477ab":"### First : Data Wrangling","414a9068":"so we fix that issue ","90a1926a":"* Scaling data (to get rid of skewness)","1e805be8":"* Finally : We get features importance","996f4c72":"#### B - Checking distribution of categorical variables \n   * we check frequencies of categories","b1aa9529":"### Second : Exploratory Data Analysis (EDA)","d243249f":"we find that there is no strong dependency between any 2 scale features","4f832922":"* Dividing data into train and test sets"}}