{"cell_type":{"7e60d76e":"code","59e885f5":"code","00710092":"code","f32b8e6a":"code","489867f9":"code","7f84599a":"code","5f6815da":"code","9333f9e4":"code","cb52ae58":"code","cfa56370":"code","c2a335aa":"code","0e7b96f1":"code","e6e1a44c":"code","cea41f3d":"code","e059192b":"code","d3726dd4":"code","40f64048":"markdown","c6aa8698":"markdown","886d90e1":"markdown","6090184e":"markdown","a596b36a":"markdown","9e6de8e2":"markdown","474fe5e8":"markdown","90bb351d":"markdown","4e644832":"markdown","07bc7761":"markdown"},"source":{"7e60d76e":"# Keras\nimport keras\nfrom keras import regularizers\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import (EarlyStopping, LearningRateScheduler,\n                             ModelCheckpoint, TensorBoard, ReduceLROnPlateau)\nfrom keras import losses, models, optimizers\nfrom keras.activations import relu, softmax\nfrom keras.layers import (Convolution2D, GlobalAveragePooling2D, BatchNormalization, Flatten, Dropout,\n                          GlobalMaxPool2D, MaxPool2D, concatenate, Activation, Input, Dense)\n\n# sklearn\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Other  \nfrom tqdm import tqdm, tqdm_pandas\nimport scipy\nfrom scipy.stats import skew\nimport librosa\nimport librosa.display\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport seaborn as sns\nimport glob \nimport os\nimport sys\nimport IPython.display as ipd  # To play sound in the notebook\nimport warnings\n# ignore warnings \nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","59e885f5":"'''\n1. Data Augmentation method   \n'''\ndef speedNpitch(data):\n    \"\"\"\n    Speed and Pitch Tuning.\n    \"\"\"\n    # you can change low and high here\n    length_change = np.random.uniform(low=0.8, high = 1)\n    speed_fac = 1.2  \/ length_change # try changing 1.0 to 2.0 ... =D\n    tmp = np.interp(np.arange(0,len(data),speed_fac),np.arange(0,len(data)),data)\n    minlen = min(data.shape[0], tmp.shape[0])\n    data *= 0\n    data[0:minlen] = tmp[0:minlen]\n    return data\n\n'''\n2. Extracting the MFCC feature as an image (Matrix format).  \n'''\ndef prepare_data(df, n, aug, mfcc):\n    X = np.empty(shape=(df.shape[0], n, 216, 1))\n    input_length = sampling_rate * audio_duration\n    \n    cnt = 0\n    for fname in tqdm(df.path):\n        file_path = fname\n        data, _ = librosa.load(file_path, sr=sampling_rate\n                               ,res_type=\"kaiser_fast\"\n                               ,duration=2.5\n                               ,offset=0.5\n                              )\n\n        # Random offset \/ Padding\n        if len(data) > input_length:\n            max_offset = len(data) - input_length\n            offset = np.random.randint(max_offset)\n            data = data[offset:(input_length+offset)]\n        else:\n            if input_length > len(data):\n                max_offset = input_length - len(data)\n                offset = np.random.randint(max_offset)\n            else:\n                offset = 0\n            data = np.pad(data, (offset, int(input_length) - len(data) - offset), \"constant\")\n\n        # Augmentation? \n        if aug == 1:\n            data = speedNpitch(data)\n        \n        # which feature?\n        if mfcc == 1:\n            # MFCC extraction \n            MFCC = librosa.feature.mfcc(data, sr=sampling_rate, n_mfcc=n_mfcc)\n            MFCC = np.expand_dims(MFCC, axis=-1)\n            X[cnt,] = MFCC\n            \n        else:\n            # Log-melspectogram\n            melspec = librosa.feature.melspectrogram(data, n_mels = n_melspec)   \n            logspec = librosa.amplitude_to_db(melspec)\n            logspec = np.expand_dims(logspec, axis=-1)\n            X[cnt,] = logspec\n            \n        cnt += 1\n    \n    return X\n\n\n'''\n3. Confusion matrix plot \n'''        \ndef print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n    '''Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n\n    Arguments\n    ---------\n    confusion_matrix: numpy.ndarray\n        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n        Similarly constructed ndarrays can also be used.\n    class_names: list\n        An ordered list of class names, in the order they index the given confusion matrix.\n    figsize: tuple\n        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n        the second determining the vertical size. Defaults to (10,7).\n    fontsize: int\n        Font size for axes labels. Defaults to 14.\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        The resulting confusion matrix figure\n    '''\n    df_cm = pd.DataFrame(\n        confusion_matrix, index=class_names, columns=class_names, \n    )\n    fig = plt.figure(figsize=figsize)\n    try:\n        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n    except ValueError:\n        raise ValueError(\"Confusion matrix values must be integers.\")\n\n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n    \n    \n'''\n# 4. Create the 2D CNN model \n'''\ndef get_2d_conv_model(n):\n    ''' Create a standard deep 2D convolutional neural network'''\n    nclass = 14\n    inp = Input(shape=(n,216,1))  #2D matrix of 30 MFCC bands by 216 audio length.\n    x = Convolution2D(32, (4,10), padding=\"same\")(inp)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPool2D()(x)\n    x = Dropout(rate=0.2)(x)\n    \n    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPool2D()(x)\n    x = Dropout(rate=0.2)(x)\n    \n    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPool2D()(x)\n    x = Dropout(rate=0.2)(x)\n    \n    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPool2D()(x)\n    x = Dropout(rate=0.2)(x)\n    \n    x = Flatten()(x)\n    x = Dense(64)(x)\n    x = Dropout(rate=0.2)(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Dropout(rate=0.2)(x)\n    \n    out = Dense(nclass, activation=softmax)(x)\n    model = models.Model(inputs=inp, outputs=out)\n    \n    opt = optimizers.Adam(0.001)\n    model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n    return model\n\n'''\n# 5. Other functions \n'''\nclass get_results:\n    '''\n    We're going to create a class (blueprint template) for generating the results based on the various model approaches. \n    So instead of repeating the functions each time, we assign the results into on object with its associated variables \n    depending on each combination:\n        1) MFCC with no augmentation  \n        2) MFCC with augmentation \n        3) Logmelspec with no augmentation \n        4) Logmelspec with augmentation\n    '''\n    \n    def __init__(self, model_history, model ,X_test, y_test, labels):\n        self.model_history = model_history\n        self.model = model\n        self.X_test = X_test\n        self.y_test = y_test             \n        self.labels = labels\n\n    def create_plot(self, model_history):\n        '''Check the logloss of both train and validation, make sure they are close and have plateau'''\n        plt.plot(model_history.history['loss'])\n        plt.plot(model_history.history['val_loss'])\n        plt.title('model loss')\n        plt.ylabel('loss')\n        plt.xlabel('epoch')\n        plt.legend(['train', 'test'], loc='upper left')\n        plt.show()\n\n    def create_results(self, model):\n        '''predict on test set and get accuracy results'''\n        opt = optimizers.Adam(0.001)\n        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n        score = model.evaluate(X_test, y_test, verbose=0)\n        print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n\n    def confusion_results(self, X_test, y_test, labels, model):\n        '''plot confusion matrix results'''\n        preds = model.predict(X_test, \n                                 batch_size=16, \n                                 verbose=2)\n        preds=preds.argmax(axis=1)\n        preds = preds.astype(int).flatten()\n        preds = (lb.inverse_transform((preds)))\n\n        actual = y_test.argmax(axis=1)\n        actual = actual.astype(int).flatten()\n        actual = (lb.inverse_transform((actual)))\n\n        classes = labels\n        classes.sort()    \n\n        c = confusion_matrix(actual, preds)\n        print_confusion_matrix(c, class_names = classes)\n    \n    def accuracy_results_gender(self, X_test, y_test, labels, model):\n        '''Print out the accuracy score and confusion matrix heat map of the Gender classification results'''\n    \n        preds = model.predict(X_test, \n                         batch_size=16, \n                         verbose=2)\n        preds=preds.argmax(axis=1)\n        preds = preds.astype(int).flatten()\n        preds = (lb.inverse_transform((preds)))\n\n        actual = y_test.argmax(axis=1)\n        actual = actual.astype(int).flatten()\n        actual = (lb.inverse_transform((actual)))\n        \n        # print(accuracy_score(actual, preds))\n        \n        actual = pd.DataFrame(actual).replace({'female_angry':'female'\n                   , 'female_disgust':'female'\n                   , 'female_fear':'female'\n                   , 'female_happy':'female'\n                   , 'female_sad':'female'\n                   , 'female_surprise':'female'\n                   , 'female_neutral':'female'\n                   , 'male_angry':'male'\n                   , 'male_fear':'male'\n                   , 'male_happy':'male'\n                   , 'male_sad':'male'\n                   , 'male_surprise':'male'\n                   , 'male_neutral':'male'\n                   , 'male_disgust':'male'\n                  })\n        preds = pd.DataFrame(preds).replace({'female_angry':'female'\n               , 'female_disgust':'female'\n               , 'female_fear':'female'\n               , 'female_happy':'female'\n               , 'female_sad':'female'\n               , 'female_surprise':'female'\n               , 'female_neutral':'female'\n               , 'male_angry':'male'\n               , 'male_fear':'male'\n               , 'male_happy':'male'\n               , 'male_sad':'male'\n               , 'male_surprise':'male'\n               , 'male_neutral':'male'\n               , 'male_disgust':'male'\n              })\n\n        classes = actual.loc[:,0].unique() \n        classes.sort()    \n\n        c = confusion_matrix(actual, preds)\n        print(accuracy_score(actual, preds))\n        print_confusion_matrix(c, class_names = classes)","00710092":"ref = pd.read_csv(\"\/kaggle\/input\/data-path\/Data_path.csv\")\nref.head()","f32b8e6a":"sampling_rate=44100\naudio_duration=2.5\nn_mfcc = 30\nmfcc = prepare_data(ref, n = n_mfcc, aug = 0, mfcc = 1)","489867f9":"# Split between train and test \nX_train, X_test, y_train, y_test = train_test_split(mfcc\n                                                    , ref.labels\n                                                    , test_size=0.25\n                                                    , shuffle=True\n                                                    , random_state=42\n                                                   )\n\n\n# one hot encode the target \nlb = LabelEncoder()\ny_train = np_utils.to_categorical(lb.fit_transform(y_train))\ny_test = np_utils.to_categorical(lb.fit_transform(y_test))\n\n# Normalization as per the standard NN process\nmean = np.mean(X_train, axis=0)\nstd = np.std(X_train, axis=0)\n\nX_train = (X_train - mean)\/std\nX_test = (X_test - mean)\/std\n\n# Build CNN model \nmodel = get_2d_conv_model(n=n_mfcc)\nmodel_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n                    batch_size=16, verbose = 2, epochs=20)","7f84599a":"results = get_results(model_history,model,X_test,y_test, ref.labels.unique())\nresults.create_plot(model_history)\nresults.create_results(model)\nresults.confusion_results(X_test, y_test, ref.labels.unique(), model)","5f6815da":"sampling_rate=44100\naudio_duration=2.5\nn_mfcc = 30\nmfcc_aug = prepare_data(ref, n = n_mfcc, aug = 1, mfcc = 1)","9333f9e4":"# Split between train and test \nX_train, X_test, y_train, y_test = train_test_split(mfcc_aug\n                                                    , ref.labels\n                                                    , test_size=0.25\n                                                    , shuffle=True\n                                                    , random_state=42\n                                                   )\n\n# one hot encode the target \nlb = LabelEncoder()\ny_train = np_utils.to_categorical(lb.fit_transform(y_train))\ny_test = np_utils.to_categorical(lb.fit_transform(y_test))\n\n# Normalization as per the standard NN process\n# mean = np.mean(X_train, axis=0)\n# std = np.std(X_train, axis=0)\n\n# X_train = (X_train - mean)\/std\n# X_test = (X_test - mean)\/std\n\n# Build CNN model \nmodel = get_2d_conv_model(n=n_mfcc)\nmodel_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n                    batch_size=16, verbose = 2, epochs=20)","cb52ae58":"results = get_results(model_history,model,X_test,y_test, ref.labels.unique())\nresults.create_plot(model_history)\nresults.create_results(model)\nresults.confusion_results(X_test, y_test, ref.labels.unique(), model)","cfa56370":"sampling_rate=44100\naudio_duration=2.5\nn_melspec = 60\nspecgram = prepare_data(ref, n = n_melspec, aug = 0, mfcc = 0)","c2a335aa":"# Split between train and test \nX_train, X_test, y_train, y_test = train_test_split(specgram\n                                                    , ref.labels\n                                                    , test_size=0.25\n                                                    , shuffle=True\n                                                    , random_state=42\n                                                   )\n\n\n\n# one hot encode the target \nlb = LabelEncoder()\ny_train = np_utils.to_categorical(lb.fit_transform(y_train))\ny_test = np_utils.to_categorical(lb.fit_transform(y_test))\n\n# Normalization as per the standard NN process\nmean = np.mean(X_train, axis=0)\nstd = np.std(X_train, axis=0)\n\nX_train = (X_train - mean)\/std\nX_test = (X_test - mean)\/std\n\n# Build CNN model \nmodel = get_2d_conv_model(n=n_melspec)\nmodel_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n                    batch_size=16, verbose = 2, epochs=20)","0e7b96f1":"results = get_results(model_history,model,X_test,y_test, ref.labels.unique())\nresults.create_plot(model_history)\nresults.create_results(model)\nresults.confusion_results(X_test, y_test, ref.labels.unique(), model)","e6e1a44c":"sampling_rate=44100\naudio_duration=2.5\nn_melspec = 60\naug_specgram = prepare_data(ref,  n = n_melspec, aug = 1, mfcc = 0)","cea41f3d":"# Split between train and test \nX_train, X_test, y_train, y_test = train_test_split(aug_specgram\n                                                    , ref.labels\n                                                    , test_size=0.25\n                                                    , shuffle=True\n                                                    , random_state=42\n                                                   )\n\n\n\n# one hot encode the target \nlb = LabelEncoder()\ny_train = np_utils.to_categorical(lb.fit_transform(y_train))\ny_test = np_utils.to_categorical(lb.fit_transform(y_test))\n\n# Normalization as per the standard NN process\nmean = np.mean(X_train, axis=0)\nstd = np.std(X_train, axis=0)\n\nX_train = (X_train - mean)\/std\nX_test = (X_test - mean)\/std\n\n# Build CNN model \nmodel = get_2d_conv_model(n=n_melspec)\nmodel_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n                    batch_size=16, verbose = 2, epochs=20)","e059192b":"results = get_results(model_history,model,X_test,y_test, ref.labels.unique())\nresults.create_plot(model_history)\nresults.create_results(model)\nresults.confusion_results(X_test, y_test, ref.labels.unique(), model)","d3726dd4":"results.accuracy_results_gender(X_test, y_test, ref.labels.unique(), model)","40f64048":"<a id=\"logmelonly\"><\/a>\n## 4. Log-melspectogram without augmentation\nThe accuracy is 60%, a slight drop from 64% without augmentation which is interesting and slightly unexpected. The plot of the logloss also indicates that it's converged. Suppose augmentation doesn't really do much for MFCC here... Note that due to some weird behaviour with the standardisation, I had to turn it off. So maybe that has some minor impact on the accuracy drop. \n\nNow, we're going to look at another type of feature call log-melspectogram. I've not covered it in this series but you can check out another kernel where I covered this feature quite extensively, right [here](https:\/\/www.kaggle.com\/ejlok1\/part-2-extracting-audio-features). But in short, if we treat the log-melspectogram like an image just like we did with the MFCC, then we can feed it into the same 2D CNN architecture that we built! Lets give it a try, without augmentation first...","c6aa8698":"# <center>Audio Emotion Recognition<\/center>\n## <center>Part 6 - 2D CNN implementation - 66% accuracy <\/center>\n#### <center> 12th September 2019 <\/center> \n#####  <center> Eu Jin Lok <\/center> ","886d90e1":"### 63% accuracy as well. Not too different from wihtout doing data augmentation. Slightly dissapointed the data augmentation didn't add much value, but then again I've only implemented just 1 simple version of the augmentation. Can't conclude firmly at this point if augmentation adds any value to it. Also, if augmentation is supposed to prevent overfitting (which is another topic on its own), then seeing a slight drop in accuracy is highly expected and welcomed.\n\n<a id=\"final\"><\/a>\n## Final thoughts\nOur advance implementation looks really good! All the 2D CNN models seems to do better than the 1D CNN model aproach that we took in previous parts of this series. In summary our accuracy from various approaches so far: \n\n- [1D CNN of simple MFCC mean](https:\/\/www.kaggle.com\/ejlok1\/audio-emotion-part-3-baseline-model) = __45%__\n- [1D CNN of simple MFCC mean with data augmentation](https:\/\/www.kaggle.com\/ejlok1\/audio-emotion-part-5-data-augmentation) = __48%__\n- [2D CNN of entire MFCC without augmentation](#MFCConly) = __64%__\n- [2D CNN of entire MFCC with Augmentation](#MFCCaug) = __60%__\n- [2D CNN of entire Log-melspectogram without augmentation](#logmelonly) = __63%__\n- [2D CNN of entire Log-melspectogram with augmentation ](#logmelaug) = __63%__\n\nA large improvement to the predictive accuracy was made from using the entire MFCC and log-melspectogram information, instead of just a reduced average to fit into a 1D CNN. And because we used a 2D CNN, we could take advantage of the entire information presented through the MFCC and mel-spectogram\n\nNotice that whilst the model isn't perfect at classifying the different emotions, its done pretty well at distinguishing the genders! ","6090184e":"# Introduction \nWe arrive to our advance implementation part of the speaker emotion classifier series. We're going to build a more advance modelling approach, and with confidence, I should see a drastic improvement of the accuracy of the emotion classifier.\n\nThe combination of the implementation above should not only improve our out-of-fold accuracy, but also ensure better generalisation to new dataset. However, whether it is practical enough for deployment or not, is a different story all together...\n\nBut the show has to carry on, so lets begin, the agenda below:\n1. [Custom functions](#custom)\n2. [MFCC without augmentation](#MFCConly) \n3. [MFCC with Augmentation](#MFCCaug)\n4. [Log-melspectogram without augmentation](#logmelonly)\n5. [Log-melspectogram with augmentation ](#logmelaug)\n6. [Final thoughts](#final)\n\n-------------------------------------\nIf you've followed my kernels and post this far, then I thank you for your audience and attention. But like the old saying, \"standing on the shoulders of giants\", I am only able to get this far because I've learnt from others. So before continuing on, I wanted to take the time here to say thanks to the following individuals who've taken the time to write these amazing notebooks, so that you and I can expedite our learning process. So they are:  \n\n- [Zafarullah](https:\/\/www.kaggle.com\/fizzbuzz\/beginner-s-guide-to-audio-data)\n\nWhen I first started out on learning about audio, Zafar's kernel here was the real launch pad for me. So thank you. Furthermore, alot of the CNN code I've implemented here are based on Zafar's original kernels [here](https:\/\/www.kaggle.com\/fizzbuzz\/beginner-s-guide-to-audio-data)\n\n- [Mitesh Puthran](https:\/\/github.com\/MITESHPUTHRANNEU\/Speech-Emotion-Analyzer)\n\nThe building of the audio emotion recognition series is inspired by this [repository](https:\/\/github.com\/MITESHPUTHRANNEU\/Speech-Emotion-Analyzer). I initially started out as a small experiment out of curiosity. And then it grew to become a multi-series kernel post in itself as I got deeper into the experiements. After this series is completed, I'm going to contribute back to Mitest's original github repo. Thank you Mitesh\n\n- [Reza Chu](https:\/\/towardsdatascience.com\/speech-emotion-recognition-with-convolution-neural-network-1e6bb7130ce3)\n\nThe data augmentation idea was brought about from this blogpost. The author also leveraged on Mitesh's code repository and build up on it like I did. But Reza really injected a whole new area of knowledge that I didn't know about. We're all famiiar with data augmentation methods on images, who would have thought (or maybe just me) that audio is no different! So go check out Reza's [post](https:\/\/towardsdatascience.com\/speech-emotion-recognition-with-convolution-neural-network-1e6bb7130ce3)\n\n-------------------------------------\n\nAnd finally, I want to thank the 4 authors for their excellent dataset, without it, writing this notebook could not have been possible. The original source of the dataset links are below:\n\n- [TESS](https:\/\/tspace.library.utoronto.ca\/handle\/1807\/24487)\n- [CREMA-D](https:\/\/github.com\/CheyneyComputerScience\/CREMA-D)\n- [SAVEE](http:\/\/kahlan.eps.surrey.ac.uk\/savee\/Database.html)\n- [RAVDESS](https:\/\/zenodo.org\/record\/1188976#.XYP8CSgzaUk)\n- [RAVDESS_Kaggle](https:\/\/www.kaggle.com\/uwrfkaggler\/ravdess-emotional-speech-audio)\n","a596b36a":" They are a few other ideas that I have stashed them for now but if anyone's keen to try it out themselves, then here's what else you can look at: \n- Try a few other datasets (eg. EmoDB)\n- Trim white noise (silence)\n- Apply the other data augmentation methods\n- Transfer learning from popular Image Recognition benchmarks - ResNet50 or XCEPTION or VGG19 for simplicity\n\nWe're getting close to the end of the series now so there might be two or three more left, I haven't quite decided on what to show but one of them will involve a concatenation of 2 Deep Learning networks into 1, kind of like an ensemble. it wouldn't be very Kaggle if we didn't do an ensemble right? I don't want to spoil the fun by revealing too much but it could be the next post or the post after. So stay tuned!","9e6de8e2":"In previous post you've seen the visualisation of the MFCC, where it captures all the core information of the audio file into a single image. Well, if an audio information can be interpreted as an image, then surely we can apply the same image recognition approaches like VGG19 or RESNET and the like? \n\nThe answer is yes. And is suprisingly very fast and accurate. Its not as accurate as when applying RNN type models on the audio wave itself. But its very close to its accuracy potential, and heaps faster. There's some assumptions and limitations depending on use cases of course. \n\nLets pick up the meta-data file again that has all our file path","474fe5e8":"<a id=\"MFCConly\"><\/a>\n## 2. MFCC without augmentation\nSo, what I'm about to do is use the entire MFCC data and treat it as an image, and push it through to a 2D CNN instead of a 1D CNN that we saw in previous parts. This will be without data augmentation for now. The convergence is very quick so instead of letting it run over 50 or more epochs, I'm going to just cut it at 20. ","90bb351d":"<a id=\"logmelaug\"><\/a>\n## 5. Log-melspectogram with augmentation \n63% accuracy for a log-melspectogram. Slighlty lower than MFCC but very close. Notice however from the logloss plot, the log-melspectogram accuracy potential hasn't quite plateau yet. It's likely that higher number of epochs the accuracy could surpass that of the MFCC, albeit slightly. Eitherways, very interesting! \n\nSo now let's see what happens if we apply the same data augmentation here. ","4e644832":"## 1. Custom functions \nSo we'll be building some custom functions to extract various features. If you've been following my series from the start, you'd be familiar with what these functions are doing. With the exception of the 2D CNN model which is new here. Further explanations ahead. \n\nThe 2D CNN takes in a 2D array of 30 MFCC bands by 216 audio length as input data. so just imagine it as a 30 x 216 pixel image. And just like in images, we could inlude a 3rd Dimension, but that's a topic for another time. It's got 4 convolution blocks of batch normalisation, max pooling and a dropout node. So your standard setup similar to VGG19, just not as deep. And we're using Adam for our optimiser. \n\nFor printing our results, i've created a class instead to encapsulate the various attributes. So that way I don't have to repeat the same code many times. ","07bc7761":"<a id=\"MFCCaug\"><\/a>\n## 3. MFCC with Augmentation \nGreat result with using MFCC and applying a 2D CNN on it! __64% overall accuracy__, that's a huge leap from the 48% that we saw in [Part 5](https:\/\/www.kaggle.com\/ejlok1\/audio-emotion-part-5-data-augmentation). \n\nNow lets see what happens when we add in the extra step of data augmentation...."}}