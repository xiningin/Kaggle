{"cell_type":{"f9913f1c":"code","e99e16de":"code","8f77bc4c":"code","3f70b178":"code","1b36f391":"code","e6a68792":"code","3f495ee8":"code","e216fde6":"code","d9015cbe":"code","d4c9f5ca":"code","ae3fdc30":"code","8be13ff2":"code","69674bee":"code","d15e3363":"code","f0a7ac31":"code","d52ffd37":"code","1895fa45":"code","528d25e9":"code","456f111c":"code","aad3d855":"code","0f6c2a82":"code","22e5ab08":"code","49a8cc06":"code","7f701304":"code","2704d6c0":"code","b1bd71f9":"code","8dc82171":"code","b1bd45cc":"code","35f0f671":"code","3e453042":"code","ccd73c74":"code","22c92db6":"code","2ef888e0":"code","58b1f235":"code","4d2ad730":"code","7caf1e76":"code","daaf54d0":"code","58955564":"code","230e7684":"code","48ff8b6e":"code","12858095":"code","ade7b557":"code","5c0cc86e":"code","a4c091e6":"code","71b80554":"code","d030042e":"code","66583f81":"code","b67909c1":"code","41a0dde3":"code","2a17b5d4":"code","0c08fd7e":"code","6f773435":"code","4330e270":"code","323bff3e":"code","8600db0e":"code","4bd573c9":"code","f4233fff":"code","87c34fd4":"code","d49d19f3":"code","e7b32a4e":"code","38c12468":"code","8f97a7fa":"code","d0a50a73":"code","117c5462":"code","f3f48298":"code","5843c858":"code","d9b71271":"code","89117290":"code","19aecf41":"code","4c8b6e78":"code","fa73152b":"code","95977d20":"code","e177d289":"code","78da5125":"code","dd512df8":"code","74e38599":"markdown","22a78c24":"markdown","497b8cd1":"markdown","0d198188":"markdown","903574e8":"markdown","3897dc58":"markdown","5f26b561":"markdown","ebbb9d5d":"markdown","06394def":"markdown","aa9b1f5a":"markdown","accb3786":"markdown","e7fef430":"markdown","61630d1d":"markdown","c3f7cc8d":"markdown","d2587621":"markdown","4425a60c":"markdown","5afe6465":"markdown","3e72d237":"markdown","3fe69ed1":"markdown","307c6345":"markdown","696a8a5f":"markdown","7fa2a63e":"markdown","53ca46e0":"markdown","e7df37aa":"markdown","38c82527":"markdown","084c2a35":"markdown","09ac2595":"markdown","7f6bc486":"markdown","044905b4":"markdown","44a6e342":"markdown","434af5eb":"markdown","6bd23f74":"markdown","0d8c2b7a":"markdown","f97fcd47":"markdown","9e7409bb":"markdown","fb714fad":"markdown","e48c9fe6":"markdown","1acfa51c":"markdown","8c8542e2":"markdown","dec899b8":"markdown","a5e5ebc7":"markdown","6ac49998":"markdown","3a5611bd":"markdown","63f75cb7":"markdown","752afd36":"markdown","b39dec6b":"markdown","de95d6ff":"markdown","281c1e17":"markdown","c71e3f39":"markdown","87164cf1":"markdown","6b29de7a":"markdown","220377ce":"markdown","4ecae190":"markdown","66d657bf":"markdown"},"source":{"f9913f1c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","e99e16de":"df=pd.read_csv('..\/input\/used-car-dataset-ford-and-mercedes\/ford.csv')","8f77bc4c":"df.head()","3f70b178":"df.info()","1b36f391":"df.isnull().sum()","e6a68792":"df.model.value_counts()","3f495ee8":"df.transmission.value_counts()","e216fde6":"pie2=pd.DataFrame(df['transmission'].value_counts())\npie2.reset_index(inplace=True)\npie2.plot(kind='pie', title='Pie chart of transmission type',y = 'transmission', \n          autopct='%1.1f%%', shadow=False, labels=pie2['index'], legend = False, fontsize=14, figsize=(12,12))","d9015cbe":"df.fuelType.value_counts()","d4c9f5ca":"pie3=pd.DataFrame(df['fuelType'].value_counts())\npie3.reset_index(inplace=True)\npie3=pie3.head(2)\npie3.loc[2]=['Hybrid, Electric or Other type',25]\npie3.plot(kind='pie', title='Pie chart of fuel type',y = 'fuelType',\n          autopct='%1.1f%%', shadow=False, labels=pie3['index'], legend = False, fontsize=14, figsize=(12,12))","ae3fdc30":"pd.crosstab(df['fuelType'], df['transmission'], \n            rownames=['fuelType'], colnames=['transmission']).sort_values(by='Manual',ascending=False)","8be13ff2":"df.describe()","69674bee":"df.hist(bins=30, figsize=(15,13))","d15e3363":"sns.boxplot(x='engineSize',data=df)","f0a7ac31":"df[df.engineSize>2.5].groupby(by='model').count()","d52ffd37":"sns.boxplot(x='mpg',data=df)","1895fa45":"df[df.mpg>90]","528d25e9":"df[(df.model==' Kuga') & (df.year==2020)]","456f111c":"sns.boxplot(x='tax',data=df)","aad3d855":"df[df.tax>350]","0f6c2a82":"df.year.plot.hist(bins=30)","22e5ab08":"df[df['year']>2020]","49a8cc06":"df[df.model.isin([' Fiesta'])].groupby(by='year').count()","7f701304":"df[df.mileage>54000].groupby(by='year').count()","2704d6c0":"df[((df['model']==' Fiesta') & (df['mileage']>54000)) & ((df['year']==2006) | (df['year']==2016))].groupby(by='year').count()","b1bd71f9":"df.iloc[17726,1]=2016","8dc82171":"df.year.min(),df.year.max()","b1bd45cc":"df.year.plot.hist(bins=25)","35f0f671":"sns.boxplot(x='mileage',data=df)","3e453042":"j=sns.regplot(x='mileage',y='price',data=df)\nj.set(ylim=(0, None))","ccd73c74":"g=sns.regplot(x='year',y='mileage',data=df)\ng.set(ylim=(0, None))","22c92db6":"df.price.plot.hist(bins=30)","2ef888e0":"sns.boxplot(x='price',data=df)","58b1f235":"df[df['price']>40000]","4d2ad730":"sns.pairplot(df)","7caf1e76":"data = {'Price':[df['price'].corr(df['year']),df['price'].corr(df['mileage']),df['price'].corr(df['tax']),\n                 df['price'].corr(df['mpg']),df['price'].corr(df['engineSize'])]}\n \npd.DataFrame(data, index=['Year','Mileage','Tax','Miles per galon','Engine Size'])","daaf54d0":"df.groupby(by='model').agg([np.min,np.mean,np.median,np.max])['price'].sort_values(by='mean',ascending=False)","58955564":"df.groupby(by='transmission').agg([np.min,np.mean,np.median,np.max])['price'].sort_values(by='mean',ascending=False)","230e7684":"df.transmission.value_counts()","48ff8b6e":"df.groupby(by='fuelType').agg([np.min,np.mean,np.median,np.max])['price'].sort_values(by='mean',ascending=False)","12858095":"df.fuelType.value_counts()","ade7b557":"df3=df.copy(deep=True)","5c0cc86e":"from sklearn.preprocessing import PolynomialFeatures","a4c091e6":"df_cat=df3[['model','transmission','fuelType']]  # Categorical columns\ndf_cat.head()","71b80554":"df_num=df3[['year','price','mileage','tax','mpg','engineSize']]   #Numerical columns","d030042e":"pf = PolynomialFeatures(degree=2, include_bias=False)\ndf3_pf = pf.fit_transform(df_num)","66583f81":"pd.DataFrame(df3_pf).head()","b67909c1":"X_new=pd.concat([pd.DataFrame(df3_pf), df_cat], axis=1)","41a0dde3":"X_new.head() ","2a17b5d4":"df.head()","0c08fd7e":"X_new.shape","6f773435":"one_hot_encode_cols = X_new.dtypes[X_new.dtypes == np.object]  # filtering by string categoricals\none_hot_encode_cols = one_hot_encode_cols.index.tolist()\none_hot_encode_cols","4330e270":"df2=X_new.copy(deep=True)","323bff3e":"df2 = pd.get_dummies(X_new, columns=one_hot_encode_cols, drop_first=True) #To avoid multicollinearity\ndf2.describe().T","8600db0e":"df2.shape","4bd573c9":"df2.head()","f4233fff":"y_col = 1\nX = df2.drop(y_col, axis=1)\ny = df2[y_col]","87c34fd4":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\ndef rmse(ytrue, ypredicted):\n    return np.sqrt(mean_squared_error(ytrue, ypredicted))","d49d19f3":"df9=df.copy(deep=True)","e7b32a4e":"df9 = pd.get_dummies(df9, columns=one_hot_encode_cols, drop_first=True) #To avoid multicollinearity","38c12468":"df9.head()","8f97a7fa":"y_col = 'price'\nXno_pf = df9.drop(y_col, axis=1)\nyno_pf = df9[y_col]","d0a50a73":"from sklearn.model_selection import train_test_split","117c5462":"X_train2, X_test2, y_train2, y_test2 = train_test_split(Xno_pf, yno_pf, test_size=0.3, random_state=42)","f3f48298":"from sklearn.linear_model import LinearRegression\n\nlinearRegression3 = LinearRegression().fit(X_train2, y_train2)\n\ny_pred3 = linearRegression3.predict(X_test2)\n\nprint('MSE: ',mean_squared_error(y_pred3,y_test2))\nprint('RMSE: ',rmse(y_pred3,y_test2))\nprint('Coefficient of determination: ',r2_score(y_pred3,y_test2))","5843c858":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","d9b71271":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","89117290":"from sklearn.preprocessing import StandardScaler\ns = StandardScaler()\n\nX_train_s = s.fit_transform(X_train)\nX_test_s = s.transform(X_test)","19aecf41":"from sklearn.linear_model import LinearRegression\n\nlinearRegression = LinearRegression().fit(X_train_s, y_train)\n\ny_pred = linearRegression.predict(X_test_s)\n\nprint('MSE: ',mean_squared_error(y_pred,y_test))\nprint('RMSE: ',rmse(y_pred,y_test))\nprint('Coefficient of determination: ',r2_score(y_pred,y_test))","4c8b6e78":"from sklearn.linear_model import RidgeCV\n\nalphas = [0.0005, 0.001, 0.003, 0.007, 0.009, 0.02]\n\nridgeCV = RidgeCV(alphas=alphas, cv=4).fit(X_train_s, y_train)\n\nridgeCV_pre = ridgeCV.predict(X_test_s)\n\nprint('Alpha found: ',ridgeCV.alpha_)\nprint('MSE ', mean_squared_error(ridgeCV_pre,y_test))\nprint('RMSE: ',rmse(ridgeCV_pre,y_test))\nprint('Coefficient of determination: ',r2_score(ridgeCV_pre,y_test))","fa73152b":"from sklearn.linear_model import LassoCV\n\nalphas2 = np.array([1e-6, 5e-6, 1e-5, 2e-5])\n\nlassoCV = LassoCV(alphas=alphas2, max_iter=5e4, cv=3).fit(X_train_s, y_train)\n\nlassoCV_pre = lassoCV.predict(X_test_s)\n\nprint('Alpha found: ',lassoCV.alpha_)\nprint('MSE ', mean_squared_error(lassoCV_pre,y_test))\nprint('RMSE: ',rmse(lassoCV_pre,y_test))\nprint('Coefficient of determination: ',r2_score(lassoCV_pre,y_test))","95977d20":"from sklearn.linear_model import ElasticNetCV\n\nl1_ratios = np.linspace(0.1, 0.9, 9)\n\nelasticNetCV = ElasticNetCV(alphas=alphas2, l1_ratio=l1_ratios, max_iter=1e4).fit(X_train_s, y_train)\nelasticNetCV_pre = elasticNetCV.predict(X_test_s)\n\nprint('Alpha found: ',elasticNetCV.alpha_)\nprint('l1_ratio: ', elasticNetCV.l1_ratio_)\nprint('MSE ', mean_squared_error(elasticNetCV_pre,y_test))\nprint('RMSE: ',rmse(elasticNetCV_pre,y_test))\nprint('Coefficient of determination: ',r2_score(elasticNetCV_pre,y_test))","e177d289":"data = {'Linear without tuning':[mean_squared_error(y_pred3,y_test),rmse(y_pred3,y_test),r2_score(y_pred3,y_test)],\n        'Linear with tuning':[mean_squared_error(y_pred,y_test),rmse(y_pred,y_test),r2_score(y_pred,y_test)],\n        'RidgeCV': [mean_squared_error(ridgeCV_pre,y_test),rmse(ridgeCV_pre,y_test),r2_score(ridgeCV_pre,y_test)],\n        'LassoCV': [mean_squared_error(lassoCV_pre,y_test),rmse(lassoCV_pre,y_test),r2_score(lassoCV_pre,y_test)],\n        'ElasticNetCV': [mean_squared_error(elasticNetCV_pre,y_test),rmse(elasticNetCV_pre,y_test),r2_score(elasticNetCV_pre,y_test)]}\n \npd.DataFrame(data, index=['MSE','RMSE','R2 score'])","78da5125":"fig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nlangs = ['Linear without tuning', 'Linear with tuning', 'RidgeCV', 'LassoCV', 'ElasticNetCV']\nstudents = [r2_score(y_pred3,y_test),r2_score(y_pred,y_test),r2_score(ridgeCV_pre,y_test),\n            r2_score(lassoCV_pre,y_test),r2_score(elasticNetCV_pre,y_test)]\nax.bar(langs,students)\nax.set_xticks(ticks=[0, 1, 2, 3, 4])\nax.set_xticklabels(['Linear without tuning', 'Linear with tuning', 'RidgeCV', 'LassoCV', 'ElasticNetCV'], rotation=90)\nax.set_xlabel('Models')\nax.set_ylabel('Coefficient of determination (R2 Score)')\nax.set(ylim=(0.7, None))\nplt.show()","dd512df8":"data =  ((mean_squared_error(y_pred3,y_test),rmse(y_pred3,y_test)), \n        (mean_squared_error(y_pred,y_test),rmse(y_pred,y_test)), \n        (mean_squared_error(ridgeCV_pre,y_test),rmse(ridgeCV_pre,y_test)), \n        (mean_squared_error(lassoCV_pre,y_test),rmse(lassoCV_pre,y_test)), \n        (mean_squared_error(elasticNetCV_pre,y_test), rmse(elasticNetCV_pre,y_test)))\n\ndim = len(data[0])\nw = 0.75\ndimw = w \/ dim\n\nfig, ax = plt.subplots()\nx = np.arange(len(data))\nfor i in range(len(data[0])):\n    y = [d[i] for d in data]\n    b = ax.bar(x + i * dimw, y, dimw, bottom=0.001)\n\nax.set_xticks(ticks=[0, 1, 2, 3, 4])\nax.set_xticklabels(['Linear without tuning', 'Linear with tuning', 'RidgeCV', 'LassoCV', 'ElasticNetCV'], rotation=90)\nax.set_yscale('log')\nax.set_xlabel('Models')\nax.set_ylabel('Logarithmic Error in US$')\n\ncolors = {'MSE':'blue', 'RMSE':'orange'}         \nlabels = list(colors.keys())\nhandles = [plt.Rectangle((0,0),1,1, color=colors[label]) for label in labels]\nplt.legend(handles, labels)\n\nplt.show()","74e38599":"**4. Lasso Regression with Cross-validation.**","22a78c24":"As we see above there are 43 cars with similar characteristics of the 'car 2060', 40 are of year 2016 whereas 3 are of year 2006, based on this we impute 2016 as the year of such car.","497b8cd1":"Ploting hystogram for each feature:","0d198188":"Values for tax are correct as outliers are cars with taxes reaching aproximately $570 which correspond to models such as: Mustang, Kuga, S-MAX.","903574e8":"Year variable:","3897dc58":"**Before encoding we must get polynomial features:**","5f26b561":"As we know after the polynomial transformation the name of every feature was changed by integer numbers, when building the machine learning model 'price' will be used as label and this one correspond to the column '1' on the dataframe above, so the following step will be spliting this into X and Y after the train-test split.","ebbb9d5d":"And finally our new concatenated dataset corresponds to:","06394def":"**Now encoding of categorical features:**  \nAs we only have nominal type one hot enconding function will be applied to these:","aa9b1f5a":"It's correct, just the outliers correspond to sport models which are in better conditions, also are well-known due to their high price such as Mustang and Focus.","accb3786":"Now let's do something similar for the categorical variables:","e7fef430":"Above we see that in general manual transmission cars tend to be cheaper and thus are much more frequent and preferred.","61630d1d":"As the limits of years are 1996 and 2020 we have to plot a new histogram with 25 bins to avoid gaps:","c3f7cc8d":"In both tables above the interval of years 2013-2017 was much more frequent and we assume there is a high probability that the car belongs to one of this years.","d2587621":"### Categorical variables: ","4425a60c":"Before building the different models let's declare some error metrics in order to compare the performace of each one:","5afe6465":"Focusing on the column 'mean' for each table we see a significant difference between each other, we could compute a t-test and levene's test to reject the hypothesis that the means are the same, but it's not needed as is clear the difference.","3e72d237":"After one hot encode the nominal categorical variables we expect to have a total of 55 features which come from: 27 polynomial + 22 model + 2 transmission + 4 fueltype.","3fe69ed1":"For mpg we can see that are some outliers and these correspond to a specific model 'Kuga year 2020' which is a hybrid car with 200 mpg and engine size of 2.5 litres, as the values for this feature are correct will be kept the same.","307c6345":"We see in the top of the table above are models which are known for justly its expensiveness and luxury. ","696a8a5f":"**Plotting R2 score for each model:**","7fa2a63e":"Mileage variable:","53ca46e0":"Let's see the type of each feature and how many non-null values contain:","e7df37aa":"Price variable:","38c82527":"Showing the min, mean, median and max of price for each model sorted by mean ascendingly:","084c2a35":"About engine size feature the values are correct as the outliers are models Mustang and Mondeo with engine size of 5 litres and 3.2 litres for model Ranger, these correspond to powerful cars often used in sport.","09ac2595":"**5. ElasticNet with ratios between 0.1 - 0.9 and Cross-validation.**","7f6bc486":"MPG variable:","044905b4":"Tax variable: ","44a6e342":"## Data Exploring and Cleaning ","434af5eb":"Clearly the feature engineering is a fundamental step in the aim to get the highest accuracies in the prediction by ML models, we can see a significant and big difference in the first model for this reason, about the other 4 models the metrics for each one is fairly good, they all have R2 scores greater than 0.9999, so we will focus on the MSE and RMSE because in these we can see a major difference. ElasticNetCV and LassoCV have the highest errors, this should be product of still overfitting data giving 13.97 for RMSE and both used considerably low alphas such as 1e-6, comparing to RidgeCV which used alpha equal to 1e-5 this one have the lowest error reaching 1.88 for RMSE and a measure a bit higher was for Linear Regression reaching 2.00.  \nFocusing on the best model 'Ridge' based on their metrics we would expect to have an error of US$1.88 when predicting the price of a car given the 9 features. Let's see the difference on performance a bit better by making a couple of interesting plots: ","6bd23f74":"### Numerical variables:","0d8c2b7a":"The following models will be built and compared using their corresponding error measurements:  \n1. Linear Regression without polynomial features nor standardization scaling.\n2. Linear Regression with engineered features.\n3. Ridge Regression with Cross-validation with engineered features.\n4. Lasso Regression with Cross-validation with engineered features.\n5. ElasticNet with ratios between 0.1 - 0.9 and Cross-validation with engineered features.","f97fcd47":"Let's try to find some record with similar characteristics as the car of year 2060:","9e7409bb":"Let's make a pairplot and focus on the second row 'price', along the columns we will see its regression plot with every numerical feature, for example we could early say there is a considerable correlation with year and mileage, whereas a weak correlation with miles per galon, engine size and tax.","fb714fad":"Model, transmission and fuel type doesn't seem to be binary nor ordinal categorical, instead nominal so let's see the value counts for each of these features:","e48c9fe6":"## Feature Engineering","1acfa51c":"Let's firstly split the dataset into numerical and categorical columns, so the first will be used in the argument of polynomial features function, and then will be concatenated with their corresponding categorical values:","8c8542e2":"As was shown above the features are nominal so we will need to one hot encode them in order to be used in a machine learning model.","dec899b8":"Let's compute and show in a table the correlation of the label price with each feature:","a5e5ebc7":"Let's check if the data of these first 5 rows correctly match with the prior dataset df:","6ac49998":"Let's build a table showing the error metrics for the three models:","3a5611bd":"There is car of the year 2060, obviously it's invalid and we found a way to impute the most appropiate value which is as following:  \n1. Retrieve the entire row for the cars of year 2060.  \n2. Using a multiple conditional selecting we will retrieve cars of model = 'Fiesta' and mileage > 54000.  \n3. The result of the prior step will give us lots of records so we assume that they actually wanted to write 2006 or 2016, so let's find which of these 2 years could make more sense.","63f75cb7":"We check that there are not null values in all features by using isnull function:","752afd36":"**1. Linear Regression without polynomial features nor standardization scaling.**","b39dec6b":"Engine size variable:","de95d6ff":"It's correct, are cars with long mileage and it's not strongly correlated to year, we can have cars of 2015 with very long mileage and the same for cars prior 2005.","281c1e17":"## Modeling:","c71e3f39":"From the 6 features we expect the polynomialfeatures function will generate a dataset of 27 columns in total excluding bias: ","87164cf1":"**Plotting MSE and RMSE in logarithmic scale for each model:**","6b29de7a":"Firstly plot the describe table to see if the measures make sense and then check one by one if outliers are correct:","220377ce":"About fuel type we can't conclude too much as in general hybrid, electric and other type tend to be more expensive but we only have 25 of these in total, whereas diesel or petrol vehicles are cheaper but also have outliers which stand out as those sport models that we have already seen.","4ecae190":"**3. Ridge Regression with Cross-validation.**","66d657bf":"**2. Linear Regression with polynomial features and standardized.**"}}