{"cell_type":{"3540e643":"code","d61fba6c":"code","d450d6b8":"code","44958283":"code","b566830a":"code","1b489329":"code","0e25b0af":"code","8c07254a":"code","d6a2c818":"code","1f855d79":"code","02155073":"code","aebc7958":"code","92367aaa":"code","a5c47331":"code","e025658d":"code","22b35c8b":"code","9b70a2ab":"code","4bebe04c":"code","956e3b14":"code","007e3dc0":"code","2c42492b":"code","cc1dedc9":"code","e3a7a457":"code","2ec81119":"code","e2eceff1":"code","80e25e11":"code","5072a4ae":"code","557c4600":"code","4c328b83":"code","dfae8d18":"code","176577f7":"code","22547a52":"code","a45f9425":"code","6c095d96":"code","5589fcc5":"code","1860cb6b":"markdown","571fdaa7":"markdown","238ab9c8":"markdown","d0f22670":"markdown","c1ee34c2":"markdown","fe95bd5c":"markdown","5c0738d6":"markdown","566fa472":"markdown","0ab7fe46":"markdown","14d75c4c":"markdown","b77e0dde":"markdown","7673ba27":"markdown"},"source":{"3540e643":"import numpy as np\nimport pandas as pd\nimport re\n\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor\nfrom sklearn.metrics import mean_absolute_error, make_scorer\nfrom sklearn.feature_selection import VarianceThreshold, RFE\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_regression, f_regression\nfrom sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsRegressor, RadiusNeighborsRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold, RepeatedStratifiedKFold\nfrom sklearn.metrics import mean_squared_error as mse\n \nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nimport matplotlib.style as style\nimport seaborn as sns\n\nimport folium\nfrom folium.plugins import FloatImage\n\nfrom lightgbm import LGBMRegressor\n\nimport warnings\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', 100)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d61fba6c":"PATH = \"..\/input\/real-time-advertisers-auction\"","d450d6b8":"df = pd.read_csv(f'{PATH}\/Dataset.csv')","44958283":"df.sample(20)","b566830a":"df['date'] = pd.to_datetime(df['date'])","1b489329":"# Formula to calculate CPM. Imported from the orginal notebook\ndef weird_division(n, d):\n    return n \/ d if d else 0\n\ndf['CPM'] = df.apply(lambda x: weird_division(((x['total_revenue'] * 100)),\n                                                x['measurable_impressions']) * 1000,\n                                                axis=1)\ndf = df[df['CPM'] >= 0]","0e25b0af":"# Separating the train and test dataframes\n\ntrain = df[df['date'] <= \"2019-06-21\"]\ntest = df[df['date'] > \"2019-06-21\"]\n\n# Now we drop target columns, columns linked to them and the quantile\ntest = test[test['CPM'] < test['CPM'].quantile(0.95)]\ntrain = train[train['CPM'] < train['CPM'].quantile(0.95)]\n\n# We set our target ('CPM' from test dataframe, also further we act as though we didn't have it)\n# and y_train ('CPM' from train dataframe)\ntarget = test['CPM']\ny = train['CPM']","8c07254a":"train.sample(10)","d6a2c818":"corr = train.corr()\nplt.figure(figsize=(14,10))\nsns.heatmap(data=corr,vmin=0, vmax=1, cmap=\"YlGnBu\",  square=True, annot= True)\nplt.show()","1f855d79":"# We drop the values that have been used to calculate our target\ntrain = train.drop(['total_revenue', 'CPM'], axis=1)\ntest = test.drop(['total_revenue', 'CPM'], axis=1)","02155073":"# Now we can remove values which are not correlated with the 'CPM'\ntrain.drop(['integration_type_id' , 'revenue_share_percent'], axis = 1, inplace=True)\ntest.drop(['integration_type_id' , 'revenue_share_percent'], axis = 1, inplace=True)","aebc7958":"train.os_id.unique()","92367aaa":"train.site_id.unique()","a5c47331":"train.ad_type_id.unique()","e025658d":"train.ad_unit_id.unique()","22b35c8b":"train.geo_id.unique()","9b70a2ab":"train.order_id.unique()","4bebe04c":"train.monetization_channel_id.unique()","956e3b14":"train.fillna(0, inplace = True)","007e3dc0":"train.columns","2c42492b":"# We get rid of some values to show a little bit of fair play (of course because the model still beats the target)\ncat_columns = ['site_id', 'ad_type_id', 'device_category_id', 'advertiser_id',\n               'order_id', 'line_item_type_id', 'os_id', 'monetization_channel_id']\n\n# We can further use other features so let's include and comment them in our case\nnum_columns = ['geo_id', 'ad_unit_id']#, 'total_impressions', 'viewable_impressions']\n\n\n# And then our first-guess df looks like this\nfg_df = train[cat_columns + num_columns]","cc1dedc9":"fg_df","e3a7a457":"def transform_data(df, num_cols, cat_cols):\n    transformed_df = df.copy()\n    \n    for col in cat_cols:\n        transformed_df[col] = transformed_df[col].astype('category')\n        transformed_df = pd.concat([transformed_df.drop(col, axis=1),\n                                    pd.get_dummies(transformed_df[col], prefix=col)], axis=1)\n        \n    transformed_df[num_cols] = transformed_df[num_cols].apply(\n        lambda x: np.log(x+1))\n    \n    scaler = MinMaxScaler()\n    transformed_df[num_cols] = scaler.fit_transform(transformed_df[num_cols])\n\n    return transformed_df\ntransformed_df = transform_data(fg_df, num_columns, cat_columns)","2ec81119":"transformed_df","e2eceff1":"def train_and_test(df, y, model, test_size=0.2):\n    target = y\n    features = df\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        features, target, test_size=test_size, random_state=42)\n\n    model.fit(X_train, y_train)  \n    y_pred = model.predict(X_test) \n    print('MSE test: %.3f' % (mse(y_test, y_pred)))\n    pred = np.round((y_pred) + 1, 1)\n    actual = np.round((y_test) + 1, 1)\n    plt.scatter(actual.to_numpy(), pred)\n    plt.title('Predicted vs. Actual', fontsize=18, fontweight='bold')\n    plt.xlabel('Actual')\n    plt.ylabel('Predicted')\n    plt.show()\n\n    return model","80e25e11":"# We use the LGBMRegressor as a model. For this particular case we can do it \"from the box\".\n# Anyway we leave some room for manoeuvre and comment possible attributes to be added\n\n# n = 10\nmodel = LGBMRegressor()# boosting_type=\"dart\", n_estimators=60, learning_rate=0.2, max_depth=n, num_leaves=2 ** n)\ntrained_model_LGBMR = train_and_test(transformed_df, y, model)","5072a4ae":"# Here we get the most important features for some further analytics (maybe for RnD department)\nfeat_imp = pd.Series(trained_model_LGBMR.feature_importances_,\n                     index=transformed_df.columns)\nfeat_imp.nlargest(30).plot(kind='barh', figsize=(10, 6))\nplt.xlabel('Relative Importance')\nplt.title(\"Feature importances\", fontsize=18, fontweight='bold')\nplt.show()","557c4600":"transformed_test = transform_data(test, num_columns, cat_columns)\ntrain_cols = transformed_df.columns.tolist()\ntest_cols = transformed_test.columns.tolist()\nintersection = []\nfor col in train_cols:\n    if col in test_cols:\n        intersection.append(col)\ntransformed_test = transform_data(test, num_columns, cat_columns)\ntransformed_test = transformed_test[intersection]\ntransformed_df = transformed_df[intersection]","4c328b83":"# To be sure we use cross-validation mechanism\nfeatures = transformed_df.copy()\n    \nX_train, X_test, y_train, y_test = train_test_split(\n        features, y, test_size=0.1, random_state=42)","dfae8d18":"scorer = make_scorer(mse)","176577f7":"scores = cross_val_score(trained_model_LGBMR, X_train, y_train, scoring=scorer, cv=5)\nscores","22547a52":"model = LGBMRegressor()# boosting_type=\"dart\", n_estimators=60, learning_rate=0.2, max_depth=n, num_leaves=2 ** n)\ntrained_model_LGBMR = train_and_test(transformed_df, y, model, 0.99)","a45f9425":"prediction = trained_model_LGBMR.predict(transformed_test)","6c095d96":"print('MSE test: %.3f' % (mse(target, prediction)))","5589fcc5":"# I want a credit for the auction theory.\n# Thank you in advance!","1860cb6b":"**We select columns for further work. We consider the least diverse features as categorical.**","571fdaa7":"**DATA PREPROCESSING AND ANALISYS**","238ab9c8":"**As we can see the results respond our expactations so we proceed**","d0f22670":"**We try to fetch some information by precisely looking at our features**","c1ee34c2":"**IMPORTS**","fe95bd5c":"**Cross-validation**","5c0738d6":"**Here is the notebook that has finished with the MSE score 4626.235**","566fa472":"**First let's try to make using only those features that are numeric though could be easily represented as the categoical\nFor that purpose the one-hot encoding with dummies would be used**","0ab7fe46":"**And now we get down to business**","14d75c4c":"**Still not bad**","b77e0dde":"**Final train on the whole dataset**","7673ba27":"**We fill the empty cells using diiferent variants.**"}}