{"cell_type":{"189a2059":"code","21764fe4":"code","48ddb53e":"code","712c832f":"code","80f7ba87":"code","94e04146":"code","1c535d1a":"code","284e5e5a":"code","9c8f74d3":"code","18bde866":"code","053080e6":"code","99757a00":"code","3dbacaeb":"code","137f1b10":"code","df090983":"code","f99565a8":"code","824fcc31":"code","6714c321":"code","b990a3ba":"code","9c3c4eee":"code","52e74cae":"code","c06b6dd6":"code","6e10589e":"code","88ba46fa":"code","758660c4":"code","41b31918":"code","efe86186":"code","4490f9b8":"code","a15e2d22":"code","150aa9f5":"code","720e091d":"code","0e521113":"code","8c1217a8":"code","96e6b2c2":"code","bc670417":"code","cca8c743":"code","e9e863d5":"code","ea15165e":"code","f0b1bde9":"code","95324c38":"code","1fba5e26":"code","1c980392":"code","0f7dd40b":"code","7537ddcf":"code","e21fcafa":"code","fc90ee59":"code","454ccc99":"code","5be9e490":"code","be9507a7":"code","cbbd2fdd":"code","4eebf386":"code","8fe50106":"code","4e80f01a":"code","63598679":"code","8efe6446":"code","f37be6d4":"code","b4098df2":"markdown","db0e691a":"markdown","a186715f":"markdown","a67b36da":"markdown","c1ba8b77":"markdown","a9c28660":"markdown","f05f3858":"markdown","4be30109":"markdown"},"source":{"189a2059":"# !pip install wandb\n!python --version","21764fe4":"import numpy as np \nimport pandas as pd \nimport tensorflow as tf\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport plotly.express as px \nfrom warnings import filterwarnings as filt \nimport IPython.display as ipd\nimport tensorflow as tf\nimport shutil\nimport os\nimport random\nimport wandb \nimport albumentations \n\ndef beep():\n    beep = np.sin(2*np.pi*400*np.arange(10000*2)\/10000)\n    return ipd.Audio(beep, rate=10000, autoplay=True)\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = (12, 6)\nfilt('ignore')","48ddb53e":"seed_value = 0\n\nos.environ['PYTHONHASHSEED']=str(seed_value)\n\nrandom.seed(seed_value)\nnp.random.seed(seed_value)\ntf.random.set_seed(seed_value)\n\nsession_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nsess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\ntf.compat.v1.keras.backend.set_session(sess)","712c832f":"base_path = '..\/input\/asl-signs'\nfor folder in os.listdir(base_path):\n    images = os.listdir(os.path.join(base_path, folder))\n    print(f'{folder} has {len(images)}')","80f7ba87":"def get_img_names_classes(base_path):\n    img_path = []\n    img_class = []\n    for folder in os.listdir(base_path):\n        dirname = os.path.join(base_path, folder)\n        img_dir = os.listdir(dirname)\n        n = len(img_dir)\n        \n        img_path += [os.path.join(folder, img) for img in img_dir]\n        img_class += [folder] * n\n        \n    return img_path, img_class\n\ndef get_dataset(path):\n    df   = tf.data.Dataset.list_files(file_pattern = path)\n    \n    imgs   = []\n    labels = []\n    for row in df.as_numpy_iterator():\n        label = tf.strings.split(row, os.path.sep)[3].numpy().decode('utf-8')\n        img_name = row.decode('utf-8')\n        \n        imgs.append(img_name)\n        labels.append(label)\n        \n    return pd.DataFrame({\n        'img_path': imgs.as_numpy_iterator(),\n        'target'  : labels.as_numpy_iterator()\n    })","94e04146":"%%time\n## creating a df \n\nbase_train_path = '..\/input\/asl-signs'\nimg_path, labels = get_img_names_classes(base_train_path)\ndf = pd.DataFrame({\n    'img_path' : img_path,\n    'target'   : labels\n})\n\ndf.head()","1c535d1a":"%%time\n\nlabels = sorted([i for i in df.target.unique() if len(i) == 1 or i == 'Nothing' or i == 'Space'])\ndataframe = pd.DataFrame(columns = ['img_path', 'target'])\n\nfor cls in labels:\n    sdf = df[df.target == cls]\n    dataframe = pd.concat([dataframe, sdf])\n    \ndf =  dataframe.copy()  \ndf.head()","284e5e5a":"df.target.unique().shape","9c8f74d3":"target_counts = df.target.value_counts()\npx.bar(x = target_counts.index, y = target_counts, color = target_counts.index, title = 'albhabet image counts')","18bde866":"df.shape","053080e6":"import cv2  \nimport PIL\n\ndef to_gray_scale(df, col, target, base_path, dirname = 'scaled_images'):\n    images = df[col]\n    target_names = df[target].unique()\n    \n    if 'scaled_images' not in os.listdir():\n        os.makedirs(dirname)\n        \n    create_dirs(dirname, target_names)\n    \n    for ind, img in enumerate(images):\n        path = os.path.join(base_path, img)\n        image = cv2.imread(path)\n        \n        gimage = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        new_path = os.path.join(dirname, img)\n        cv2.imwrite(new_path, gimage)\n        \n    print(f'Converted {ind + 1} images to Gray scale ...')\n    print(f'Saved the scaled images to {dirname} ...')\n    \n\ndef get_row_num(num, n = 5):\n    if num % n == 0:\n        return (num \/\/ n)\n    else:\n        return (num \/\/ n) + 1 \n\n    \ndef show_img(df, base_path, num_img = 9, cols = 5, cls = None):\n    if cls:\n        df = df[df.target == cls]\n        \n    df = df.sample(n = num_img)\n    ind = 1\n    plt.figure(figsize=(20, 8))\n    for _, rdf in df.iterrows():\n        rows = get_row_num(num_img, cols)\n        plt.subplot(rows, cols, ind)\n        ind += 1\n        path = os.path.join(base_path, rdf.img_path)\n        label = rdf.target\n        \n        image = plt.imread(path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        plt.imshow(image)\n        plt.title(f'{label}  |  {image.shape}')\n        plt.axis('off')\n        \ndef create_dirs(base_path, dirnames):\n    for ind, dirname in enumerate(dirnames):\n        if dirname not in os.listdir(base_path):\n            path = os.path.join(base_path, dirname)\n            os.makedirs(path)\n    print(f'Successfully created {ind + 1} folders ...')","99757a00":"show_img(df, base_train_path, 15);","3dbacaeb":"from keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nimport albumentations as A","137f1b10":"def alb_transformation(x):\n    transform = A.Compose([\n        A.CoarseDropout(max_holes = 12),\n#         A.ChannelShuffle(p=0.3),\n        A.GaussianBlur()\n    ])\n    \n#     x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n    y = transform(image = x)['image']\n\n    return y\n\ndef create_img_generator(df, base_path, color_mode, batch_size = 16, datagen_args = {\n    'rescale'            : 1.\/255,\n    'horizontal_flip'    : True,\n    'vertical_flip'      : True,\n    'rotation_range'     : 35,\n    'width_shift_range'  : 0.2,\n    'height_shift_range' : 0.2,\n    'zoom_range'         : 0.05\n}, shuffle = True):\n    generator = ImageDataGenerator(**datagen_args)\n    x_gen = generator.flow_from_dataframe(\n                                  dataframe = df, \n                                  directory = base_path, \n                                  x_col = 'img_path', \n                                  y_col = 'target',\n                                  color_mode = color_mode,\n                                  target_size=(300, 300),\n                                  class_mode = 'categorical', \n                                  batch_size = batch_size,\n                                  shuffle = shuffle\n                                 )\n    return x_gen\n\ndef show_img_generator(generator, num_imgs = 10, cols = 5):\n    images = generator[0][0]\n    labels = generator[0][1]\n    plt.figure(figsize = (20, 8))\n    for i in range(num_imgs):\n        \n        rows = get_row_num(num_imgs, cols)\n        plt.subplot(rows, cols, i + 1)\n        image = images[i]\n        plt.imshow(image)\n        cls = labels_inverse[np.argmax(labels[i])]\n        plt.title(f'{cls}  |  {image.shape}')\n        plt.axis('off')\n\ndef sample(x, y, frac = 0.2):\n    x, xt, y, yt = train_test_split(x, y, test_size = frac, stratify = y)\n    return x, xt, y, yt","df090983":"path = os.path.join(base_path, df.img_path.sample(1).values[0])\ni = plt.imread(path)\nni = alb_transformation(i)\nplt.subplot(1, 2, 1)\nplt.imshow(i)\nplt.title('normal')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(cv2.cvtColor(ni, cv2.COLOR_BGR2RGB))\nplt.title('transformed')\nplt.axis('off');","f99565a8":"df.shape[0] * 0.0345","824fcc31":"x = df.drop(['target'], axis = 1)\ny = df.target\nx_train, x_dev, y_train, y_dev = sample(x, y, 0.0345)\n\nprint(f'x train shape : {x_train.shape}')\nprint(f'y train shape : {y_train.shape}')\nprint(f'x dev shape   : {x_dev.shape}')\nprint(f'y dev shape   : {y_dev.shape}')","6714c321":"train_datagen_args = {\n    'rescale'            : 1.\/255,\n    'rotation_range'     : 30,\n    'horizontal_flip'    : True,\n    'shear_range'        : 0.2,\n    'preprocessing_function' : alb_transformation\n}\n\ndev_datagen_args = {\n    'rescale'            : 1.\/255,\n#     'preprocessing_function' : lambda x : cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n}\n\ntraindf = pd.concat([x_train, y_train], axis = 1)\ndevdf = pd.concat([x_dev, y_dev], axis = 1)\n\ntraindf.head()","b990a3ba":"devdf.shape","9c3c4eee":"train_datagen_args","52e74cae":"(281245 + 10050) * 0.0345","c06b6dd6":"train_gen = create_img_generator(traindf, base_train_path, 'grayscale', 128, train_datagen_args)\ndev_gen = create_img_generator(devdf, base_train_path, 'grayscale', 128, dev_datagen_args, shuffle = False)\ntrain_gen[0][1].shape","6e10589e":"sample_gen = create_img_generator(traindf.sample(n = 300), base_train_path, 'grayscale', 64, train_datagen_args)","88ba46fa":"train_gen.class_indices","758660c4":"dev_gen.class_indices","41b31918":"sample_gen.class_indices","efe86186":"labels_inverse = {v : k for k, v in sample_gen.class_indices.items()}","4490f9b8":"show_img_generator(train_gen, 15)","a15e2d22":"show_img_generator(dev_gen)","150aa9f5":"show_img_generator(sample_gen, 15)","720e091d":"from keras import Sequential\nfrom keras.layers import Dense, MaxPool2D, Conv2D, Flatten, Input, Dropout, BatchNormalization, GlobalMaxPooling2D\nfrom keras.applications import resnet, mobilenet_v2, inception_resnet_v2\nimport keras \nimport tensorflow as tf","0e521113":"128 * 2","8c1217a8":"def calculate_cnn_op_shape(im_shape, kernel_shape, padding, strides):\n    return 1 + (im_shape - kernel_shape + (2 * padding)) \/ strides\n\ndef CNN(ip_shape, op_shape):\n    \n    ip = Input(shape = ip_shape)\n    \n    x = Conv2D(filters = 64, kernel_size = (3,3), strides = 1, padding = 'same', activation = 'relu')(ip)\n    x = Conv2D(filters = 64, kernel_size = (3,3), strides = 1, padding = 'same', activation = 'relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPool2D(pool_size=(2,2))(x)\n    \n    x  = Conv2D(filters = 64, kernel_size = (3,3), strides = 1, padding = 'same', activation = 'relu')(x)\n    x  = Conv2D(filters = 64, kernel_size = (3,3), strides = 1, padding = 'same', activation = 'relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPool2D(pool_size=(2,2))(x)\n    \n    x  = Conv2D(filters = 128, kernel_size = (3,3), strides = 1, padding = 'same', activation = 'relu')(x)\n    x = BatchNormalization()(x)\n    \n    x = GlobalMaxPooling2D()(x)\n    \n    x = Dense(512, activation = 'relu')(x)\n    x = Dense(256, activation = 'relu')(x)\n    \n    op = Dense(op_shape , activation = 'softmax')(x)\n    \n    return keras.Model(inputs = ip, outputs = op)\n\n\ndef CustomModel(MODEL : keras.applications, ip : tuple, op : int):\n    base_model = MODEL(\n        include_top = False,\n        weights     = 'imagenet',\n        input_shape = ip,\n        pooling     = 'max'\n    )\n    \n    for layer in base_model.layers[:-1]:\n        layer.trainable = False\n        \n    x = Dense(1024, activation = 'relu')(base_model.output)\n    x = Dense(256, activation = 'relu')(x)\n    x = Dense(64,  activation = 'relu')(x)\n    x = Dense(op,  activation = 'softmax')(x)\n    \n    model = keras.Model(inputs = base_model.input, outputs = x)\n    \n    return model\n    \n\ndef plot(history, monitor):\n    his = pd.DataFrame(history.history)\n    l = [c for c in his.columns if 'loss' in c]\n    m = [c for c in his.columns if 'loss' not in c]\n    if monitor == 'loss':\n        his[l].plot(kind = 'line')\n    else:\n        his[m].plot(kind = 'line')  \n    \n    plt.title('metric comparison between training and validation')\n    \n    \ndef hardmax(y):\n    return np.argmax(y, axis = 1)","96e6b2c2":"1573888 + 262400 + 16448 + 1820","bc670417":"beep()","cca8c743":"import wandb \n\nwandb.login()","e9e863d5":"IMG_SHAPE = (300, 300, 1)\nBATCH_SIZE = 128\n\nwandb.init(\n    project=\"ASL-sign-classification\", \n    entity=\"jaabir\",\n    config={\n        \"learning_rate\": 0.0001,\n        \"epochs\"       : 4,\n        \"loss\"         : 'categorical_crossentropy',\n        \"optimizer\"    : 'adam',\n        'batch_size'   : BATCH_SIZE,\n        'architecture' : 'Vanilla-CNN'\n    }\n)\n\nconfig = wandb.config\n\n\n\n# model1 = CustomModel(inception_resnet_v2.InceptionResNetV2, IMG_SHAPE, 28)\n# model1 = keras.models.load_model('..\/input\/asl-model\/model.h5')\nmodel1 = CNN(IMG_SHAPE, 28)\nmodel1.summary()","ea15165e":"model1.compile(\n    loss = config.loss,\n    metrics = ['accuracy'],\n    optimizer = tf.keras.optimizers.Adam(learning_rate = config.learning_rate)\n)","f0b1bde9":"## callbacks \n\nfrom keras.callbacks import ReduceLROnPlateau, Callback, EarlyStopping, ModelCheckpoint\nfrom wandb.keras import WandbCallback\n\nlr_decay = ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.95,\n    patience=3,\n    verbose=1,\n    mode=\"min\",\n    min_lr=0.00001,\n)\n\ner_stopping = EarlyStopping(\n    monitor=\"val_loss\",\n    patience=4,\n    verbose=1,\n    mode=\"min\",\n    restore_best_weights= True,\n)\n\ncheckpoint = ModelCheckpoint(filepath='model.h5', \n                             monitor= 'val_loss',\n                             verbose=1, \n                             save_best_only=True,\n                             mode = 'min')\n\nclass StopTraining(Callback):\n    def __init__(self, thresh = 0.95, times = 3):\n        self.thresh = thresh\n        self.times = times\n        self.reached = 0\n        \n    def on_epoch_end(self, epoch, logs = {}):\n        if logs.get(\"val_accuracy\") >= self.thresh:\n            self.reached += 1\n            print(f\"Reached 95% accuracy {self.reached} \/ {self.times} ...\")\n            if self.reached >= self.times:\n                self.model.stop_training = True\n                print('Stopping the model ...')\n                \n                \nstopTr = StopTraining(times = 3, thresh = 0.965)","95324c38":"his = model1.fit_generator(\n    generator = train_gen,\n    validation_data = dev_gen,\n    epochs = config.epochs,\n    steps_per_epoch = train_gen.samples \/\/ train_gen.batch_size,\n    callbacks = [checkpoint, WandbCallback()]\n)","1fba5e26":"beep()","1c980392":"# plot(his, 'loss')\n# plot(his, 'metric')","0f7dd40b":"# model1.save('asl_model.h5')\n# model = keras.models.load_model('model.h5')\n# model.summary()\n# model1.evaluate_generator(dev_gen)","7537ddcf":"base_test_path = '..\/input\/asl-alphabet-test\/asl-alphabet-test'\nimg_path, labels = get_img_names_classes(base_test_path)\ntest_df = pd.DataFrame({\n    'img_path' : img_path,\n    'target'   : labels\n})\n\ntest_df = test_df[test_df.target != 'del']\ntest_df['target'] = test_df.target.str.capitalize()\ntest_df = test_df.sample(frac = 1)\n\ntest_df.head()","e21fcafa":"target_counts = test_df.target.value_counts()\npx.bar(x = target_counts.index, y = target_counts, color = target_counts.index, title = 'albhabet image counts')","fc90ee59":"test_df.target.unique().shape","454ccc99":"tgen = create_img_generator(test_df, base_test_path, 'grayscale', 128, {'rescale' : 1.\/255}, shuffle = False)","5be9e490":"show_img_generator(tgen, 15)","be9507a7":"[labels_inverse[i] for i in tgen.classes][:15]","cbbd2fdd":"y_true = test_df.target","4eebf386":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport seaborn as sns\n\ndef report(yt, pred):\n    print(f'Accuracy score :====> {accuracy_score(yt, pred)}')\n    print()\n    print(classification_report(yt, pred))\n    print()\n    plt.figure(figsize = (20, 8))\n    sns.heatmap(confusion_matrix(yt, pred), fmt = '.1f', annot = True)","8fe50106":"# model = keras.models.load_model('..\/input\/asl-model\/model-best.h5')\nmodel = model1\ntpred = hardmax(model.predict_generator(tgen, verbose = 1))\nyt = tgen.classes\nreport(yt, tpred)","4e80f01a":"model.evaluate_generator(dev_gen, verbose = 1)","63598679":"labels_inverse[4]","8efe6446":"beep()","f37be6d4":"model.optimizer, model.loss","b4098df2":"**Problem i am currently facing is lower accuracy in the testing data**","db0e691a":"#### transformed images (train)","a186715f":"all classes are almost equally distributed ","a67b36da":"### original images ","c1ba8b77":"**View Model Report : https:\/\/wandb.ai\/jaabir\/ASL-sign-classification**","a9c28660":"**View Model Report : https:\/\/wandb.ai\/jaabir\/ASL-sign-classification**","f05f3858":"### transformed images (sample)","4be30109":"### transformed images (dev)"}}