{"cell_type":{"5d94d011":"code","5e510a87":"code","61b96776":"code","ba08403f":"code","8cca29e1":"code","32c5e199":"code","d05edf33":"code","8a3efe4e":"code","370cbe0e":"code","dbf4ffc7":"code","7a220d7b":"code","a9cfe744":"code","e2c1d011":"code","d8cbeb0c":"code","fd7b46e3":"code","f8912172":"code","4980b65b":"code","bd68ec48":"code","97519707":"code","361de1ed":"code","dcc84ddf":"code","3e4ef9ca":"code","456a538c":"code","eb51fe43":"code","29bcdc26":"code","efb304ea":"code","c7cb356f":"code","a30f3739":"code","aacffbc3":"code","facee22f":"markdown","52781b8f":"markdown","25c0c589":"markdown"},"source":{"5d94d011":"# for TPU\n# !curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev","5e510a87":"# for TPU\n# import torch_xla\n# import torch_xla.core.xla_model as xm","61b96776":"!export CUDA_LAUNCH_BLOCKING=1","ba08403f":"import sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","8cca29e1":"import time\nimport math, random\nimport gc, os\nimport datetime\nimport numpy as np\nimport pandas as pd\nfrom sklearn.kernel_approximation import Nystroem\nfrom sklearn.decomposition import PCA, KernelPCA, TruncatedSVD\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import FeatureAgglomeration, AgglomerativeClustering, KMeans\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA \nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom hyperopt import hp, fmin, tpe, Trials\nfrom hyperopt.pyll.base import scope\nfrom tqdm.notebook import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')","32c5e199":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n\nss = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\n\ncols = [c for c in ss.columns.values if c != 'sig_id']\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","d05edf33":"def preprocess(df):\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n#     df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    del df['sig_id']\n    return df\n\n# def log_loss_metric(y_true, y_pred):\n#     metrics = []\n#     for _target in train_targets.columns:\n#         metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels = [0,1]))\n#     return np.mean(metrics)\n\ndef log_loss_metric(y_true, y_pred):\n    loss = 0\n    y_pred_clip = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    for i in range(y_true.shape[1]):\n        loss += - np.mean(y_true[:, i] * np.log(y_pred_clip[:, i]) + (1 - y_true[:, i]) * np.log(1 - y_pred_clip[:, i]))\n    return loss \/ y_true.shape[1]\n\ntrain = preprocess(train_features)\ntest = preprocess(test_features)\n\ndel train_targets['sig_id']\ndel train_targets_nonscored['sig_id']","8a3efe4e":"from sklearn.preprocessing import QuantileTransformer\n\nqt = QuantileTransformer(output_distribution = 'normal', random_state = 42)\nqt.fit(pd.concat([pd.DataFrame(train[GENES+CELLS]), pd.DataFrame(test[GENES+CELLS])]))\ntrain[GENES+CELLS] = qt.transform(train[GENES+CELLS])\ntest[GENES+CELLS] = qt.transform(test[GENES+CELLS])","370cbe0e":"from sklearn.decomposition import PCA\n\n# GENES\nn_comp_genes = 600  #<--Update\n\ndata = pd.concat([pd.DataFrame(train[GENES]), pd.DataFrame(test[GENES])])\npca_genes = PCA(n_components=n_comp_genes, random_state = 42)\ndata2 = pca_genes.fit_transform(data[GENES])\ntrain2 = data2[:train.shape[0]]; test2 = data2[-test.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp_genes)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp_genes)])\n\ntrain = pd.concat((train, train2), axis=1)\ntest = pd.concat((test, test2), axis=1)\n\n#CELLS\nn_comp_cells = 50  #<--Update\n\ndata = pd.concat([pd.DataFrame(train[CELLS]), pd.DataFrame(test[CELLS])])\npca_cells = PCA(n_components=n_comp_cells, random_state = 42)\ndata2 = pca_cells.fit_transform(data[CELLS])\ntrain2 = data2[:train.shape[0]]; test2 = data2[-test.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp_cells)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp_cells)])\n\ntrain = pd.concat((train, train2), axis=1)\ntest = pd.concat((test, test2), axis=1)","dbf4ffc7":"from sklearn.feature_selection import VarianceThreshold\n\nvar_thresh = VarianceThreshold(0.8)  #<-- Update\ndata = train.append(test)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 3:])\n\ntrain_transformed = data_transformed[ : train.shape[0]]\ntest_transformed = data_transformed[-test.shape[0] : ]\n\ntrain = pd.DataFrame(train[['cp_type','cp_time','cp_dose']].values.reshape(-1, 3),\\\n            columns=['cp_type','cp_time','cp_dose'])\n\ntrain = pd.concat([train, pd.DataFrame(train_transformed)], axis=1)\n\ntest = pd.DataFrame(test[['cp_type','cp_time','cp_dose']].values.reshape(-1, 3),\\\n            columns=['cp_type','cp_time','cp_dose'])\n\ntest = pd.concat([test, pd.DataFrame(test_transformed)], axis=1)\n\nprint(train.shape)\nprint(test.shape)","7a220d7b":"train_targets = train_targets.loc[train['cp_type'] == 0].reset_index(drop = True)\ntrain_targets_nonscored = train_targets_nonscored.loc[train['cp_type'] == 0].reset_index(drop = True)\ntrain = train.loc[train['cp_type'] == 0].reset_index(drop = True)\n\nprint(train.shape)","a9cfe744":"top_feats = np.arange(1, train.shape[1])\nprint(top_feats)","e2c1d011":"train.head()","d8cbeb0c":"import torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets, self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss\n    \nsbcewlogits = SmoothBCEwLogits(smoothing = 0.0008)","fd7b46e3":"def create_folds(num_starts, num_splits):\n    \n    folds = []\n    \n    # LOAD FILES\n    train_feats = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\n    scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\n    drug = pd.read_csv('\/kaggle\/input\/lish-moa\/train_drug.csv')\n    scored = scored.loc[train_feats['cp_type'] == 'trt_cp', :]\n    drug = drug.loc[train_feats['cp_type'] == 'trt_cp', :]\n    targets = scored.columns[1:]\n    scored = scored.merge(drug, on='sig_id', how='left') \n\n    # LOCATE DRUGS\n    vc = scored.drug_id.value_counts()\n    vc1 = vc.loc[vc <= 18].index.sort_values()\n    vc2 = vc.loc[vc > 18].index.sort_values()\n    \n    for seed in range(num_starts):\n\n        # STRATIFY DRUGS 18X OR LESS\n        dct1 = {}; dct2 = {}\n        skf = MultilabelStratifiedKFold(n_splits = num_splits, shuffle = True, random_state = seed)\n        tmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\n        for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[targets])):\n            dd = {k:fold for k in tmp.index[idxV].values}\n            dct1.update(dd)\n\n        # STRATIFY DRUGS MORE THAN 18X\n        skf = MultilabelStratifiedKFold(n_splits = num_splits, shuffle = True, random_state = seed)\n        tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop = True)\n        for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[targets])):\n            dd = {k:fold for k in tmp.sig_id[idxV].values}\n            dct2.update(dd)\n\n        # ASSIGN FOLDS\n        scored['fold'] = scored.drug_id.map(dct1)\n        scored.loc[scored.fold.isna(),'fold'] =\\\n            scored.loc[scored.fold.isna(),'sig_id'].map(dct2)\n        scored.fold = scored.fold.astype('int8')\n        folds.append(scored.fold.values)\n        \n        del scored['fold']\n        \n    return np.stack(folds)","f8912172":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(42)","4980b65b":"nfolds = 5\nnstarts = 3\nnepochs = 1000\nbatch_size = 128\nval_batch_size = 1024\nntargets = train_targets.shape[1]\ntargets = [col for col in train_targets.columns]\ncriterion_train = sbcewlogits\ncriterion_val = nn.BCELoss()\n\n# for GPU\/CPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# for TPU\n# device = xm.xla_device()\n# torch.set_default_tensor_type('torch.FloatTensor')","bd68ec48":"class SwishFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        y = x * torch.sigmoid(x)\n        return y\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x = ctx.saved_variables[0]\n        sigmoid = torch.sigmoid(x)\n        return grad_output * (sigmoid * (1 + x * (1 - sigmoid)))\nF_swish = SwishFunction.apply\n\nclass Swish(nn.Module):\n    def forward(self, x):\n        return F_swish(x)\n\nclass GroupLinear(nn.Module):\n    def __init__(self, in_dim, out_dim, group=4):\n        super(GroupLinear, self).__init__()\n        self.group = group\n        self.linear = nn.Linear(in_dim\/\/group, out_dim\/\/group)\n\n    def forward(self, x):\n        batch_size, dim = x.shape\n        g = self.group\n        x = x.reshape(batch_size, g, dim\/\/g)\n        x = self.linear(x)\n        x = x.reshape(batch_size, -1)\n        return x\n\nclass ResAdd(nn.Module):\n    def __init__(self, module):\n        super(ResAdd, self).__init__()\n        self.module = module\n\n    def forward(self, x):\n        x = self.module(x)+x\n        return x\n\n# class Net(nn.Module):\n#     def __init__(self, num_feats, num_targets, \n#                  hidden_units = [512, 128, 128, 256, 512], \n#                  dropout_rates = [0.22023758845836414, 0.44095268503187185, 0.33039674088592896, 0.17879173200029594, 0.31790411449084355], \n#                  num_groups = [16, 4, 16, 8]):\n#         super(Net, self).__init__()\n#         self.resnet = nn.Sequential(\n#             nn.BatchNorm1d(num_feats),\n#             nn.Dropout(dropout_rates[0]),\n#             nn.Linear(num_feats, hidden_units[0]),    # mix\n#             GroupLinear(hidden_units[0], hidden_units[1], num_groups[0]),   # group\n#             nn.BatchNorm1d(hidden_units[1]),\n#             Swish(),\n#             nn.Dropout(dropout_rates[1]),#\n\n#             ResAdd(nn.Sequential(\n#                 nn.Linear(hidden_units[1], hidden_units[2]),      # mix\n#                 GroupLinear(hidden_units[2], hidden_units[1], num_groups[1]), # group\n#                 nn.BatchNorm1d(hidden_units[1]),\n#                 Swish(),\n#                 nn.Dropout(dropout_rates[2]),\n#             )),\n#             ResAdd(nn.Sequential(\n#                 nn.Linear(hidden_units[1], hidden_units[3]),      # mix\n#                 GroupLinear(hidden_units[3], hidden_units[1], num_groups[2]), # group\n#                 nn.BatchNorm1d(hidden_units[1]),\n#                 Swish(),\n#                 nn.Dropout(dropout_rates[3]),\n#             )),\n#             ResAdd(nn.Sequential(\n#                 nn.Linear(hidden_units[1], hidden_units[4]),      # mix\n#                 GroupLinear(hidden_units[4], hidden_units[1], num_groups[3]), # group\n#                 nn.BatchNorm1d(hidden_units[1]),\n#                 Swish(),\n#                 nn.Dropout(dropout_rates[4]),\n#             )),\n#         )\n\n#         self.logit = nn.Linear(hidden_units[1], num_targets)\n\n#     def forward(self, x):\n#         x = self.resnet(x)\n#         return self.logit(x)\n\nclass Net(nn.Module):\n    def __init__(self, num_feats, num_targets, hidden_units = [512, 256], \n                 dropout_rates = [0.3, 0.2, 0.2], \n                 num_groups = [16, 8]):\n        super(Net, self).__init__()\n        self.resnet = nn.Sequential(\n            nn.BatchNorm1d(num_feats),\n            nn.Dropout(dropout_rates[0]),\n            nn.Linear(num_feats, hidden_units[0]),    # mix\n            GroupLinear(hidden_units[0], hidden_units[1], num_groups[0]),   # group\n            nn.BatchNorm1d(hidden_units[1]),\n            Swish(),\n            nn.Dropout(dropout_rates[1]),#\n\n            ResAdd(nn.Sequential(\n                nn.Linear(hidden_units[1], hidden_units[1]),      # mix\n                GroupLinear(hidden_units[1], hidden_units[1], num_groups[1]), # group\n                nn.BatchNorm1d(hidden_units[1]),\n                Swish(),\n                nn.Dropout(dropout_rates[2]),\n            )),\n            ResAdd(nn.Sequential(\n                nn.Linear(hidden_units[1], hidden_units[1]),      # mix\n                GroupLinear(hidden_units[1], hidden_units[1], num_groups[1]), # group\n                nn.BatchNorm1d(hidden_units[1]),\n                Swish(),\n                nn.Dropout(dropout_rates[2]),\n            )),\n#             ResAdd(nn.Sequential(\n#                 nn.Linear(hidden_units[1], hidden_units[1]),      # mix\n#                 GroupLinear(hidden_units[1], hidden_units[1], num_groups[1]), # group\n#                 nn.BatchNorm1d(hidden_units[1]),\n#                 Swish(),\n#                 nn.Dropout(dropout_rates[2]),\n#             )),\n#             ResAdd(nn.Sequential(\n#                 nn.Linear(hidden_units[1], hidden_units[1]),      # mix\n#                 GroupLinear(hidden_units[1], hidden_units[1], num_groups[1]), # group\n#                 nn.BatchNorm1d(hidden_units[1]),\n#                 Swish(),\n#                 nn.Dropout(dropout_rates[2]),\n#             )),\n        )\n\n        self.logit = nn.Linear(hidden_units[1], num_targets)\n\n    def forward(self, x):\n        x = self.resnet(x)\n        return self.logit(x)","97519707":"# dataset class\nclass MoaDataset(Dataset):\n    def __init__(self, df, targets, feats_idx, mode='train'):\n        self.mode = mode\n        self.feats = feats_idx\n        self.data = df[:, feats_idx]\n        if mode=='train':\n            self.targets = targets\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        if self.mode == 'train':\n            return torch.FloatTensor(self.data[idx]), torch.FloatTensor(self.targets[idx])\n        elif self.mode == 'test':\n            return torch.FloatTensor(self.data[idx]), 0","361de1ed":"ss_pseudo = pd.read_csv('..\/input\/drug-sub\/submission_pbest.csv').drop('sig_id', axis = 1)\npseudo_targets = ss_pseudo.loc[test['cp_type'] == 0, cols].values\npseudo_train = test.loc[test['cp_type'] == 0, test.columns].values","dcc84ddf":"train = train.values\ntest = test.values\ntrain_targets = train_targets.values","3e4ef9ca":"folds_split = create_folds(nstarts, nfolds)\nprint(folds_split)","456a538c":"for nums, seed in enumerate(range(nstarts)):\n    seed_everything(seed)\n#     kfold = MultilabelStratifiedKFold(n_splits = nfolds, random_state = seed, shuffle = True)\n    for n, foldno in enumerate(set(folds_split[nums])):\n        tr = folds_split[nums] != foldno\n        te = folds_split[nums] == foldno\n        start_time = time.time()\n        xtrain, xval = train[tr], train[te]\n        ytrain, yval = train_targets[tr], train_targets[te]\n        \n        # Pseudo Labeling\n        xtrain = np.concatenate([xtrain, pseudo_train])\n        ytrain = np.concatenate([ytrain, pseudo_targets])\n\n        train_set = MoaDataset(xtrain, ytrain, top_feats)\n        val_set = MoaDataset(xval, yval, top_feats)\n\n        dataloaders = {\n            'train': DataLoader(train_set, batch_size=batch_size, shuffle=True),\n            'val': DataLoader(val_set, batch_size=val_batch_size, shuffle=False)\n        }\n        \n        model = Net(len(top_feats), 206).to(device)\n        checkpoint_path = f'ResDT_{seed}_Fold_{n+1}.pt'\n        optimizer = optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 1e-5)\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.1, \n                                                         patience = 3, eps = 1e-4, verbose = False)\n        best_loss = {'train': np.inf, 'val': np.inf}\n        \n        es_count = 0\n        for epoch in range(nepochs):\n            epoch_loss = {'train': 0.0, 'val': 0.0}\n\n            for phase in ['train', 'val']:\n                if phase == 'train':\n                    model.train()\n                else:\n                    model.eval()\n\n                running_loss = 0.0\n\n                for i, (x, y) in enumerate(dataloaders[phase]):\n                    x, y = x.to(device), y.to(device)\n\n                    optimizer.zero_grad()\n\n                    with torch.set_grad_enabled(phase=='train'):\n                        if phase=='train':\n                            preds = model(x)\n                            loss = criterion_train(preds, y)\n                        else:\n                            preds = torch.sigmoid(model(x))\n                            loss = criterion_val(preds, y)\n\n                        if phase=='train':\n                            loss.backward()\n                            optimizer.step()\n#                             xm.optimizer_step(optimizer, barrier = True)\n\n                    running_loss += loss.item() \/ len(dataloaders[phase])\n\n                epoch_loss[phase] = running_loss\n    \n            scheduler.step(epoch_loss['val'])\n\n            if epoch_loss['val'] < best_loss['val']:\n                best_loss = epoch_loss\n                torch.save(model.state_dict(), checkpoint_path)\n                es_count = 0\n            else:\n                es_count += 1\n                \n#             print(\"Epoch {}\/{} - loss: {:5.5f} - val_loss: {:5.5f} - es: {}\".format(epoch+1, nepochs, epoch_loss['train'], epoch_loss['val'], es_count))\n            \n            if es_count > 10:\n                break\n        \n        print(\"[{}] - seed: {} - fold: {} - best val_loss: {:5.5f}\".format(str(datetime.timedelta(seconds = time.time() - start_time))[0:7], seed, n, best_loss['val']))","eb51fe43":"oof = np.zeros((len(train), nstarts, ntargets))\noof_targets = np.zeros((len(train), ntargets))\npreds = np.zeros((len(test), ntargets))","29bcdc26":"def mean_log_loss(y_true, y_pred):\n    metrics = []\n    for i, target in enumerate(targets):\n        metrics.append(log_loss(y_true[:, i], y_pred[:, i].astype(float), labels=[0,1]))\n    return np.mean(metrics)","efb304ea":"res = np.zeros(train_targets.shape)\nfor nums, seed in enumerate(range(nstarts)):\n    print(f\"Inference for seed {seed}\")\n    seed_targets = []\n    seed_oof = []\n    seed_preds = np.zeros((len(test), ntargets, nfolds))\n    \n    for n, foldno in enumerate(set(folds_split[nums])):\n        tr = folds_split[nums] != foldno\n        te = folds_split[nums] == foldno\n        xval, yval = train[te], train_targets[te]\n        fold_preds = []\n        fold_oof = []\n        \n        val_set = MoaDataset(xval, yval, top_feats)\n        test_set = MoaDataset(test, None, top_feats, mode='test')\n        \n        dataloaders = {\n            'val': DataLoader(val_set, batch_size=val_batch_size, shuffle=False),\n            'test': DataLoader(test_set, batch_size=val_batch_size, shuffle=False)\n        }\n        \n        checkpoint_path = f'ResDT_{seed}_Fold_{n+1}.pt'\n        model = Net(len(top_feats), 206).to(device)\n        model.load_state_dict(torch.load(checkpoint_path, map_location = device))\n        model.eval()\n        \n        for phase in ['val', 'test']:\n            for i, (x, y) in enumerate(dataloaders[phase]):\n                if phase == 'val':\n                    x, y = x.to(device), y.to(device)\n                elif phase == 'test':\n                    x = x.to(device)\n                \n                with torch.no_grad():\n                    batch_preds = torch.sigmoid(model(x))\n                    \n                    if phase == 'val':\n                        seed_targets.append(y)\n                        seed_oof.append(batch_preds)\n                        fold_oof.append(batch_preds)\n                    elif phase == 'test':\n                        fold_preds.append(batch_preds)\n        \n        fold_oof = torch.cat(fold_oof, dim=0).cpu().numpy()\n        print(f'Score of seed {seed} fold {n}:\\t', log_loss_metric(train_targets[te], fold_oof))\n        res[te] += fold_oof \/ nstarts\n        fold_preds = torch.cat(fold_preds, dim=0).cpu().numpy()\n        seed_preds[:, :, n] = fold_preds\n        \n    seed_targets = torch.cat(seed_targets, dim=0).cpu().numpy()\n    seed_oof = torch.cat(seed_oof, dim=0).cpu().numpy()\n    seed_preds = np.mean(seed_preds, axis=2)\n    \n#     print(\"Score for this seed {:5.5f}\".format(mean_log_loss(seed_targets, seed_oof)))\n    oof_targets = seed_targets\n    oof[:, seed, :] = seed_oof\n    preds += seed_preds \/ nstarts","c7cb356f":"print(f'Overall OOF Score:\\t', log_loss_metric(train_targets, res))\nnp.save('Net_oof.npy', res)","a30f3739":"# OOF CV Score With Control Group\ntr_targets = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv').drop('sig_id', axis = 1)\nres_all = np.zeros(tr_targets[cols].shape)\nres_all[train_features['cp_type'] == 0] = res.copy()\noverall_oof_score = log_loss_metric(tr_targets[cols].values, res_all)\nprint('OOF CV Score With Control Group:', overall_oof_score)","aacffbc3":"ss[targets] = preds\nss.loc[test_features['cp_type'] == 1, targets] = 0\nss.to_csv('submission.csv', index = False)\nnp.save('Net_sub.npy', ss.loc[test_features['cp_type'] == 0, cols].values)","facee22f":"# Features","52781b8f":"# Training\n\nThe model I use here is different from the one suggested in the paper. Here, I flatten all the outputs of the trees and pass a final dense layer. I found this approach can accelerate the convergence and generate better results.\n\nP.S. Using more trees significantly increases the training time but no obvious improvement on CV and LB.","25c0c589":"# Inference"}}