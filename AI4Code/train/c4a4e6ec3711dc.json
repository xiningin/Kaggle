{"cell_type":{"b26cb3f8":"code","675c93b5":"code","2edf7f5e":"code","fd2f2bac":"code","e981db24":"code","ce843f1b":"code","78fab097":"code","d2223c91":"code","395c5f87":"code","ecf16b39":"code","36ea097d":"code","5fd6588d":"code","ec61e230":"code","87aa1dc2":"code","814ffa42":"code","084e2f11":"code","ebf10c48":"code","fb3df26d":"code","312c92e7":"code","19af9380":"code","1b2b7658":"code","f348382c":"code","c7b96c0e":"code","2d3e0617":"code","85a61ea6":"code","cd5b3401":"markdown","f1beaf24":"markdown","2795faf1":"markdown","9ac121ae":"markdown","5ef228f4":"markdown","1c84b17b":"markdown","337e7581":"markdown","d57a5d51":"markdown","654e677c":"markdown","81f4e4dc":"markdown","39e38c81":"markdown","aae8422d":"markdown","5279fdf5":"markdown","98bfceaa":"markdown","7fb6dbbd":"markdown","c04fb65b":"markdown","2add8cb9":"markdown"},"source":{"b26cb3f8":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.rcParams[\"figure.figsize\"] = (15, 10)\nplt.rcParams[\"figure.dpi\"] = 125\nplt.rcParams[\"font.size\"] = 14\nplt.rcParams['font.family'] = ['sans-serif']\nplt.rcParams['font.sans-serif'] = ['DejaVu Sans']\nplt.style.use('ggplot')\nsns.set_style(\"whitegrid\", {'axes.grid': False})\nplt.rcParams['image.cmap'] = 'gray' # grayscale looks better","675c93b5":"from pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport os\nfrom skimage.io import imread as imread\nfrom skimage.util import montage\nfrom PIL import Image\nmontage_rgb = lambda x: np.stack([montage(x[:, :, :, i]) for i in range(x.shape[3])], -1)\nfrom skimage.color import label2rgb\nimage_dir = Path('..') \/ 'input' \/ 'satellite-images-of-hurricane-damage'\nmapping_file = Path('..') \/ 'input' \/ 'hurricane-damage-overview' \/ 'resnet_features.json'\nimage_df = pd.read_json(mapping_file)\nimage_df['damage_val'] = image_df['damage'].map(lambda x: x=='damage') \nimage_df.sample(3)","2edf7f5e":"image_df['data_split'].value_counts()","fd2f2bac":"from sklearn.model_selection import train_test_split\ntrain_df = image_df.query('data_split==\"train_another\"')\ntrain_df.reset_index(inplace=True)\nprint(train_df.shape[0], 'training images')","e981db24":"train_x_vec = np.stack(train_df['resnet_features'].values, 0)\ntrain_y_vec = np.stack(train_df['damage_val'], 0)\nprint(train_x_vec.shape, '->', train_y_vec.shape)","ce843f1b":"from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\ndef show_model_results(in_model, use_split=None, plot_type='swarm'):\n    fig, m_axs = plt.subplots(4, 2, figsize=(15, 30))\n    m_axs = m_axs.flatten()\n    all_rows = []\n    ax1 = m_axs[0]\n    \n    if use_split is None:\n        cur_df = image_df.copy()\n    else:\n        cur_df = image_df.query('data_split==\"{}\"'.format(use_split)) \n    \n    for c_split, example_df in cur_df.groupby('data_split'):\n        example_df = example_df.reset_index()\n        x_vec = np.stack(example_df['resnet_features'].values, 0)\n        y_vec = np.stack(example_df['damage_val'], 0)\n\n        valid_pred = in_model.predict(x_vec)\n        tpr, fpr, _ = roc_curve(y_vec[:], valid_pred[:])\n        auc = roc_auc_score(y_vec[:], valid_pred[:])\n        acc = accuracy_score(y_vec[:], valid_pred[:]>0.5)\n        ax1.plot(tpr, fpr, '.-', label='{}, AUC {:0.2f}, Accuracy: {:2.0%}'.format(c_split, auc, acc))\n        all_rows += [pd.DataFrame({'class': y_vec[:], 'prediction': np.clip(valid_pred[:], 0, 1), 'type': 'damage', \n                                  'split': c_split})]\n    \n    c_all_df = pd.concat(all_rows)\n        \n    # show example images\n    ax1.legend()\n    for (_, c_row), (c_ax) in zip(\n        example_df.sample(m_axs.shape[0]).iterrows(), \n                               m_axs[1:-1]):\n        \n        c_ax.imshow(imread(c_row['path']))\n        t_yp = in_model.predict(np.expand_dims(c_row['resnet_features'], 0))\n        c_ax.set_title('Class: {}\\n Damage Prediction: {:2.2%}'.format(c_row['damage'], t_yp[0]))\n        c_ax.axis('off')\n        \n        t_y = np.array(c_row['damage_val'])\n    \n    # nice dataframe of output\n    \n    ax1 = m_axs[-1]\n    if plot_type=='swarm':\n        # prevent overplotting\n        sns.swarmplot(data=c_all_df.sample(500) if c_all_df.shape[0]>1000 else c_all_df,\n                      hue='class', \n                      y='prediction', \n                      x='type', \n                      size=2.0, \n                      ax=ax1)\n    elif plot_type=='box':\n        sns.boxplot(data=c_all_df, hue='class', y='prediction', x='type', ax=ax1)\n    elif plot_type=='violin':\n        sns.violinplot(data=c_all_df, hue='class', y='prediction', x='type', ax=ax1)\n    ax1.set_ylim(-0.05, 1.05)\n    return c_all_df","78fab097":"from sklearn.neighbors import KNeighborsRegressor\nknn = KNeighborsRegressor(n_neighbors=1)\nknn.fit(train_x_vec, train_y_vec)","d2223c91":"show_model_results(knn, use_split='train_another', plot_type='box');","395c5f87":"fig, m_axs = plt.subplots(6, 4, figsize=(30, 40))\nfor (c_ax, c_feat_ax, d_ax, d_feat_ax), (_, c_row) in zip(m_axs, \n                            image_df.sample(m_axs.shape[0], random_state=2018).iterrows()):\n    \n    query_img = Image.open(c_row['path'])\n    idx_to_color = np.array(query_img.convert('P', palette='web').getpalette()).reshape((-1, 3))\/255.0\n    c_ax.imshow(query_img)\n    c_ax.set_title(c_row['location'][:25])\n    c_ax.axis('off')\n    c_feat_ax.bar(np.arange(2048), c_row['resnet_features'], edgecolor='k', linewidth=0.1)\n    c_feat_ax.set_xlabel('Color Id')\n    c_feat_ax.set_ylabel('Pixel Count')\n    c_feat_ax.set_title('Feature Vector')\n    \n    dist, idx = knn.kneighbors(np.expand_dims(c_row['resnet_features'], 0))\n    m_row = train_df.iloc[idx[0][0]]\n    matched_img = Image.open(m_row['path'])\n    \n    d_ax.imshow(matched_img)\n    d_ax.set_title('Closest Match\\n{}\\nDistance: {:2.1%}'.format(m_row['location'][:25], dist[0][0]))\n    d_ax.axis('off')\n    \n    d_feat_ax.bar(np.arange(2048), m_row['resnet_features'], edgecolor='k', linewidth=0.1)\n    d_feat_ax.set_xlabel('Color Id')\n    d_feat_ax.set_ylabel('Pixel Count')\n    d_feat_ax.set_title('Matched Feature')","ecf16b39":"show_model_results(knn);","36ea097d":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(train_x_vec, train_y_vec)","5fd6588d":"show_model_results(lr);","ec61e230":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.feature_selection import VarianceThreshold\nlr_pipe = make_pipeline(RobustScaler(), VarianceThreshold(0.99), LinearRegression())\nlr_pipe.fit(train_x_vec, train_y_vec)","87aa1dc2":"show_model_results(lr_pipe);","814ffa42":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.decomposition import PCA\ndt_pipe = make_pipeline(RobustScaler(), \n                        PCA(n_components=10), \n                        DecisionTreeRegressor(max_depth=5, min_samples_split=50))\ndt_pipe.fit(train_x_vec, train_y_vec)\nshow_model_results(dt_pipe);","084e2f11":"from sklearn.tree import export_graphviz\nimport graphviz\ndef show_tree(in_tree):\n    return graphviz.Source(export_graphviz(in_tree, out_file=None))\n\nshow_tree(dt_pipe.steps[-1][1])","ebf10c48":"from sklearn.ensemble import RandomForestRegressor\nrf_pipe = make_pipeline(RobustScaler(), RandomForestRegressor(n_estimators=200))\nrf_pipe.fit(train_x_vec, train_y_vec)\nshow_model_results(rf_pipe);","fb3df26d":"from xgboost import XGBRegressor\nxg_pipe = make_pipeline(RobustScaler(), \n                        XGBRegressor(objective='reg:linear'))\nxg_pipe.fit(train_x_vec, train_y_vec)","312c92e7":"show_model_results(xg_pipe);","19af9380":"from keras.models import load_model\nfrom PIL import ImageEnhance, Image\nresnet_model = load_model('..\/input\/hurricane-damage-overview\/feature_model.h5')\ndef color_count_feature(raw_image):\n    return resnet_model.predict(np.expand_dims(raw_image, 0))[0]","1b2b7658":"sample_images_df = image_df.query('data_split==\"validation_another\"').groupby('damage').apply(lambda x: x.sample(3)).reset_index(drop=True)","f348382c":"op_vals = [None, 0.15, 0.5, 2]\nfig, m_axs = plt.subplots(sample_images_df.shape[0], len(op_vals)+1, figsize=(25, 25))\n\nfor (_, c_row), n_axs in zip(\n        sample_images_df.iterrows(), m_axs):\n        \n        c_img = Image.open(c_row['path'])\n        \n        for c_op, c_ax in zip(op_vals, n_axs):\n            if c_op is None:\n                t_img = c_img\n                t_caption = 'Original: {} \\n'.format(c_row['damage'])\n            else:\n                t_img = ImageEnhance.Color(c_img).enhance(c_op)\n                t_caption = 'Saturation: {:2.1%}\\n'.format(c_op)\n            c_feat = color_count_feature(t_img)\n            c_ax.imshow(t_img)\n            t_yp = xg_pipe.predict(np.expand_dims(c_feat, 0))\n            c_ax.set_title('{}Model Score:{:2.1%}'.format(t_caption, t_yp[0]))\n            c_ax.axis('off')\n        \n        op_space = np.linspace(np.min(op_vals[1:]), np.max(op_vals[1:]), 50)\n        im_score = np.zeros_like(op_space)\n        \n        for i, c_op in enumerate(op_space):\n            t_img = ImageEnhance.Color(c_img).enhance(c_op)\n            c_feat = color_count_feature(t_img)\n            t_yp = xg_pipe.predict(np.expand_dims(c_feat, 0))[0]\n            im_score[i] = t_yp\n        n_axs[-1].plot(100*op_space, 100*im_score.clip(0, 1), '.-', lw=0.2)\n        n_axs[-1].set_ylabel('Model Prediction')\n        n_axs[-1].set_xlabel('Saturation')\n        n_axs[-1].set_ylim(-5, 105)\n            ","c7b96c0e":"op_vals = [None, 0.25, 1.5, 4]\nfig, m_axs = plt.subplots(sample_images_df.shape[0], len(op_vals)+1, figsize=(25, 25))\n\nfor (_, c_row), n_axs in zip(\n        sample_images_df.iterrows(), m_axs):\n        \n        c_img = Image.open(c_row['path'])\n        \n        for c_op, c_ax in zip(op_vals, n_axs):\n            if c_op is None:\n                t_img = c_img\n                t_caption = 'Original: {} \\n'.format(c_row['damage'])\n            else:\n                t_img = ImageEnhance.Contrast(c_img).enhance(c_op)\n                t_caption = 'Contrast: {:2.1%}\\n'.format(c_op)\n            c_feat = color_count_feature(t_img)\n            c_ax.imshow(t_img)\n            t_yp = xg_pipe.predict(np.expand_dims(c_feat, 0))\n            c_ax.set_title('{}Model Score:{:2.1%}'.format(t_caption, t_yp[0]))\n            c_ax.axis('off')\n        \n        op_space = np.linspace(np.min(op_vals[1:]), np.max(op_vals[1:]), 50)\n        im_score = np.zeros_like(op_space)\n        \n        for i, c_op in enumerate(op_space):\n            t_img = ImageEnhance.Contrast(c_img).enhance(c_op)\n            c_feat = color_count_feature(t_img)\n            t_yp = xg_pipe.predict(np.expand_dims(c_feat, 0))[0]\n            im_score[i] = t_yp\n        n_axs[-1].plot(100*op_space, 100*im_score.clip(0, 1), '.-', lw=0.2)\n        n_axs[-1].set_ylabel('Model Score')\n        n_axs[-1].set_xlabel('Image Contrast')\n        n_axs[-1].set_ylim(-5, 105)\n            ","2d3e0617":"op_vals = [None, 0.5, 1.5, 1\/0.5]\nfig, m_axs = plt.subplots(sample_images_df.shape[0], len(op_vals)+1, figsize=(25, 25))\n\nfor (_, c_row), n_axs in zip(\n        sample_images_df.iterrows(), m_axs):\n        \n        c_img = Image.open(c_row['path'])\n        \n        for c_op, c_ax in zip(op_vals, n_axs):\n            if c_op is None:\n                t_img = c_img\n                t_caption = 'Original: {} \\n'.format(c_row['damage'])\n            else:\n                t_img = ImageEnhance.Brightness(c_img).enhance(c_op)\n                t_caption = 'Brightness: {:2.1%}\\n'.format(c_op)\n            c_feat = color_count_feature(t_img)\n            c_ax.imshow(t_img)\n            t_yp = xg_pipe.predict(np.expand_dims(c_feat, 0))\n            c_ax.set_title('{}Model Score:{:2.1%}'.format(t_caption, t_yp[0]))\n            c_ax.axis('off')\n        \n        op_space = np.linspace(np.min(op_vals[1:]), np.max(op_vals[1:]), 50)\n        im_score = np.zeros_like(op_space)\n        \n        for i, c_op in enumerate(op_space):\n            t_img = ImageEnhance.Brightness(c_img).enhance(c_op)\n            c_feat = color_count_feature(t_img)\n            t_yp = xg_pipe.predict(np.expand_dims(c_feat, 0))[0]\n            im_score[i] = t_yp\n        n_axs[-1].plot(100*op_space, 100*im_score.clip(0, 1), '.-', lw=0.2)\n        n_axs[-1].set_ylabel('Model Score')\n        n_axs[-1].set_xlabel('Image Brightness')\n        n_axs[-1].set_ylim(-5, 105)\n            ","85a61ea6":"op_vals = [None, -2.5, 1.5, 5]\nfig, m_axs = plt.subplots(sample_images_df.shape[0], len(op_vals)+1, figsize=(25, 25))\n\nfor (_, c_row), n_axs in zip(\n        sample_images_df.iterrows(), m_axs):\n        \n        c_img = Image.open(c_row['path'])\n        \n        for c_op, c_ax in zip(op_vals, n_axs):\n            if c_op is None:\n                t_img = c_img\n                t_caption = 'Original: {} \\n'.format(c_row['damage'])\n            else:\n                t_img = ImageEnhance.Sharpness(c_img).enhance(c_op)\n                t_caption = 'Sharpness: {:2.1%}\\n'.format(c_op)\n            c_feat = color_count_feature(t_img)\n            c_ax.imshow(t_img)\n            t_yp = xg_pipe.predict(np.expand_dims(c_feat, 0))\n            c_ax.set_title('{}Model Score:{:2.1%}'.format(t_caption, t_yp[0]))\n            c_ax.axis('off')\n        \n        op_space = np.linspace(np.min(op_vals[1:]), np.max(op_vals[1:]), 50)\n        im_score = np.zeros_like(op_space)\n        \n        for i, c_op in enumerate(op_space):\n            t_img = ImageEnhance.Sharpness(c_img).enhance(c_op)\n            c_feat = color_count_feature(t_img)\n            t_yp = xg_pipe.predict(np.expand_dims(c_feat, 0))[0]\n            im_score[i] = t_yp\n        n_axs[-1].plot(100*op_space, 100*im_score.clip(0, 1), '.-', lw=0.2)\n        n_axs[-1].set_ylabel('Model Score')\n        n_axs[-1].set_xlabel('Image Sharpness')\n        n_axs[-1].set_ylim(-5, 105)\n            ","cd5b3401":"# Fancier Models\nHere we can use much fancier models like random forest to even further improve the performance","f1beaf24":"# The Simplist Model\nNearest Neighbor works by finding the most similar case from the training data using the feature vector. We can directly visualize this by showing which training image was being looked at.","2795faf1":"## Saturation","9ac121ae":"## Show the results\nWe get incredibly good, nearly perfect results! Are we done now? Time to build an app and sell it to google for $$$?","5ef228f4":"## Let's dig down a bit deeper, how does it work?","1c84b17b":"# More Complicated Models\nWe can try decision trees to get better results","337e7581":"# Goal\nThe goal is to make a simple model that can go from a satellite image to a prediction of how likely it is if there is any damage present\n\n## Setup\nWe basically take the precomputed color features and build simple models in order to determine if the image contains any damage [here](https:\/\/www.kaggle.com\/kmader\/hurricane-damage-overview). We try to create a balanced training group and a realistic validation group to know if the model is learning anything useful","d57a5d51":"# Linear Regression Model","654e677c":"## XGBoost\nOne of the most powerful classification tools","81f4e4dc":"Split up the groups so we can validate our model on something besides the direct training data","39e38c81":"# Model Sensitivity\nHow does changing the images slightly (saturation, contrast, brightness) affect the predictions of the model?","aae8422d":"# Sharpness \/ Blurriness","5279fdf5":"# Brightness","98bfceaa":"## Normalize the input\n\nWe can make a pipeline to normalize the input and remove bad features","7fb6dbbd":"## Use on the validation split","c04fb65b":"# Display Results Nicely\nWe want to have code to display our results nicely so we can see what worked well and what didn't","2add8cb9":"## Contrast"}}