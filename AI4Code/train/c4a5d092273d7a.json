{"cell_type":{"e1faf787":"code","99382c54":"code","a292dae4":"code","2af4c335":"code","e5dd96c0":"code","709b17cb":"code","e545478e":"code","5f0c9102":"code","242b3dc6":"code","0fcd1920":"code","f1be3337":"code","1ab7e2a5":"code","aa9dd850":"code","f8289f0c":"code","019d81b8":"code","9d9e1fa9":"code","a463351a":"code","1d0214bc":"code","87e6d4ff":"code","1b552b13":"code","200e4146":"code","33ccb463":"code","ddda78c9":"code","a536b712":"code","183c457c":"code","e56fabc6":"code","42ab4c74":"code","8813a2e8":"code","ebafffe9":"code","b157b812":"code","ba511f0e":"code","b02d639f":"code","69f07ab5":"code","e38019ca":"code","d04dc355":"code","72bafcf3":"code","3d5d4ad3":"code","6382d7b9":"code","33cc1e6a":"code","a5395442":"code","42723cb2":"code","48065d1d":"code","9aca8073":"code","0fd05f8b":"code","d99f6402":"code","d2a28437":"code","cd510ca1":"code","02cec212":"code","c3be0623":"code","9e0c2540":"code","a7a58e92":"code","0a6c8fa9":"code","8fd0d7eb":"code","62d4b695":"code","f44366a7":"code","825c1d93":"code","81d44fda":"code","8464a4a3":"code","06891d27":"code","de46b34d":"code","250efc08":"code","1bc093a6":"code","67db9768":"code","8f90d36a":"code","f84dee22":"code","42dcdf83":"code","0258e826":"code","dec491e9":"code","80e609dc":"code","3f61a15a":"code","0f1f8bb5":"code","b0a00cf2":"code","f71c84f0":"code","8fdc977a":"code","6694ba6d":"code","0411b795":"code","412536f8":"code","b34051d4":"code","4781481c":"code","eb0c9b96":"markdown","51e1ede0":"markdown","dd8da243":"markdown","28a0ae5f":"markdown","90fbdeb9":"markdown","7c5acde5":"markdown","39ccfe02":"markdown","022df01c":"markdown","d81cf7d8":"markdown","588852ee":"markdown","9b62dd8c":"markdown","06df0481":"markdown","16322a52":"markdown","8aa7ce1b":"markdown","8c6168ce":"markdown","6b6b0dc5":"markdown","0ed93d36":"markdown","81e14858":"markdown","cba4b5c6":"markdown","cbfe94e6":"markdown","407adf3c":"markdown","7d65ef8b":"markdown","db0433c3":"markdown","6e0f80ec":"markdown","0b8db753":"markdown","fc68b93d":"markdown"},"source":{"e1faf787":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cross_validation import ShuffleSplit,train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport statsmodels.api as sm\nfrom sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nimport folium\nfrom mpl_toolkits.basemap import Basemap","99382c54":"import os\nprint(os.listdir(\"..\/input\"))","a292dae4":"dc = pd.read_csv(\"..\/input\/DC_Properties.csv\")\ndc.head()","2af4c335":"dc.info()","e5dd96c0":"print(dc.isnull().sum())","709b17cb":"dc_na = (dc.isnull().sum() \/ len(dc)) * 100\ndc_na = dc_na.drop(dc_na[dc_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing %' :dc_na})\nmissing_data.head(20)","e545478e":"f, ax = plt.subplots(figsize=(15, 12))\nsns.barplot(y=dc_na.index, x=dc_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)\nplt.show()","5f0c9102":"lat = dc['LATITUDE'].values\nlon = dc['LONGITUDE'].values\n\n# 1. Draw the map background\nfig = plt.figure(figsize=(8, 8))\nm = Basemap(projection='lcc', resolution='l', \n            lat_0=39, lon_0=-78,\n            width=1E6, height=1.2E6)\nm.shadedrelief()\nm.drawcoastlines(color='gray')\nm.drawcountries(color='gray')\nm.drawstates(color='gray')\n\n# 2. scatter lat and long values\n\nm.scatter(lon, lat, latlon=True,\n          cmap='Reds', alpha=0.5)\nplt.show()","242b3dc6":"locations = dc[['LATITUDE', 'LONGITUDE']]\nlocationlist = locations.values.tolist()\nprint(len(locationlist))\nprint(locationlist[1])\nmap = folium.Map(location=[38.9146833, -77.04076447], zoom_start=12)\nfolium.Marker(locationlist[1]).add_to(map)\nmap","0fcd1920":"# map = folium.Map(location=[38.9146833, -77.04076447], zoom_start=12)\n# for point in range(0, len(locationlist)):\n#    folium.Marker(locationlist[point]).add_to(map)\n# map    ","f1be3337":"dc_clean=dc\ndc_clean.head()","1ab7e2a5":"dc_clean=dc_clean.drop(['NATIONALGRID','ASSESSMENT_SUBNBHD','CENSUS_BLOCK','X','Y','QUADRANT'], axis=1)","aa9dd850":"dc_clean.NUM_UNITS[dc_clean.NUM_UNITS.isnull()] =  dc_clean.NUM_UNITS.mode().iloc[0]","f8289f0c":"dc_clean.loc[dc_clean['YR_RMDL'].isnull(), 'YR_RMDL'] = dc_clean['EYB']","019d81b8":"dc_clean.loc[dc_clean['AYB'].isnull(), 'AYB'] = dc_clean['EYB']","9d9e1fa9":"dc_clean.STORIES=dc_clean.STORIES.round()\ndc_clean.STORIES[dc_clean.STORIES.isnull()] =  dc_clean.STORIES.mode().iloc[0]","a463351a":"print(dc_clean.GBA.describe())\ndc_clean.GBA[dc_clean.GBA.isnull()] =  dc_clean.GBA.mean()","1d0214bc":"dc_clean.STYLE[dc_clean.STYLE.isnull()] =  dc_clean.STYLE.mode().iloc[0]","87e6d4ff":"dc_clean.STRUCT[dc_clean.STRUCT.isnull()] =  dc_clean.STRUCT.mode().iloc[0]\ndc_clean.GRADE[dc_clean.GRADE.isnull()] =  dc_clean.GRADE.mode().iloc[0]\ndc_clean.CNDTN[dc_clean.CNDTN.isnull()] =  dc_clean.CNDTN.mode().iloc[0]\ndc_clean.EXTWALL[dc_clean.EXTWALL.isnull()] =  dc_clean.EXTWALL.mode().iloc[0]\ndc_clean.ROOF[dc_clean.ROOF.isnull()] =  dc_clean.ROOF.mode().iloc[0]\ndc_clean.INTWALL[dc_clean.INTWALL.isnull()] =  dc_clean.INTWALL.mode().iloc[0]\ndc_clean.KITCHENS[dc_clean.KITCHENS.isnull()] =  dc_clean.KITCHENS.mode().iloc[0]","1b552b13":"print(dc_clean.LIVING_GBA.describe())\ndc_clean.LIVING_GBA[dc_clean.LIVING_GBA.isnull()] =  dc_clean.LIVING_GBA.median()","200e4146":"dc_clean['CMPLX_NUM1'] = dc_clean['FULLADDRESS'].str[0:4]\ndc_clean.loc[dc_clean['CMPLX_NUM'].isnull(), 'CMPLX_NUM'] = dc_clean['CMPLX_NUM1']\ndc_clean.CMPLX_NUM[dc_clean.CMPLX_NUM.isnull()] =  dc_clean.CMPLX_NUM.mode().iloc[0]","33ccb463":"dc_clean.CITY[dc_clean.CITY.isnull()] =  'WASHINGTON'\ndc_clean.STATE[dc_clean.STATE.isnull()] =  'DC'","ddda78c9":"dc_clean.LATITUDE[dc_clean.LATITUDE.isnull()] =  38.9146833\ndc_clean.LONGITUDE[dc_clean.LONGITUDE.isnull()] =  -77.04076447","a536b712":"dc_clean=dc_clean.drop(['FULLADDRESS','CMPLX_NUM1'], axis=1)","183c457c":"dc_clean.head()","e56fabc6":"dc_clean.info()","42ab4c74":"unknown_data=dc_clean[dc_clean.PRICE.isnull()]","8813a2e8":"unknown_data.head()","ebafffe9":"dc_clean = dc_clean[dc_clean.PRICE.notnull()]\ndc_clean.head()","b157b812":"dc_clean.rename(columns={\"Unnamed: 0\": \"id\"}, inplace= True)\ndc_clean.set_index(\"id\", inplace = True)","ba511f0e":"dc_clean.head()","b02d639f":"unknown_data.rename(columns={\"Unnamed: 0\": \"id\"}, inplace= True)\nunknown_data.set_index(\"id\", inplace = True)\nunknown_data.head()","69f07ab5":"dc_clean = pd.get_dummies(dc_clean, prefix='HEAT_', columns=['HEAT'])\ndc_clean = pd.get_dummies(dc_clean, prefix='AC_', columns=['AC'])\ndc_clean = pd.get_dummies(dc_clean, prefix='QUALIFIED_', columns=['QUALIFIED'])\ndc_clean = pd.get_dummies(dc_clean, prefix='STYLE_', columns=['STYLE'])\ndc_clean = pd.get_dummies(dc_clean, prefix='STRUCT_', columns=['STRUCT'])\ndc_clean = pd.get_dummies(dc_clean, prefix='GRADE_', columns=['GRADE'])\ndc_clean = pd.get_dummies(dc_clean, prefix='CNDTN_', columns=['CNDTN'])\ndc_clean = pd.get_dummies(dc_clean, prefix='EXTWALL_', columns=['EXTWALL'])\ndc_clean = pd.get_dummies(dc_clean, prefix='ROOF_', columns=['ROOF'])\ndc_clean = pd.get_dummies(dc_clean, prefix='INTWALL_', columns=['INTWALL'])\ndc_clean = pd.get_dummies(dc_clean, prefix='SOURCE_', columns=['SOURCE'])\ndc_clean = pd.get_dummies(dc_clean, prefix='WARD_', columns=['WARD'])","e38019ca":"print(dc_clean.describe())","d04dc355":"def stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    \"\"\" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    \"\"\"\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.argmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included","72bafcf3":"X = dc_clean[['BATHRM', 'HF_BATHRM', 'NUM_UNITS', 'ROOMS', 'BEDRM', 'AYB',\n       'YR_RMDL', 'EYB', 'STORIES', 'SALE_NUM',\n       'GBA', 'BLDG_NUM', 'KITCHENS', 'FIREPLACES', 'USECODE', 'LANDAREA',\n         'LIVING_GBA',\n       'ZIPCODE', 'LATITUDE', 'LONGITUDE',\n       'CENSUS_TRACT', 'HEAT__Air Exchng', 'HEAT__Air-Oil',\n       'HEAT__Elec Base Brd', 'HEAT__Electric Rad', 'HEAT__Evp Cool',\n       'HEAT__Forced Air', 'HEAT__Gravity Furnac', 'HEAT__Hot Water Rad',\n       'HEAT__Ht Pump', 'HEAT__Ind Unit', 'HEAT__No Data',\n       'HEAT__Wall Furnace', 'HEAT__Warm Cool', 'HEAT__Water Base Brd',\n       'AC__0', 'AC__N', 'AC__Y', 'QUALIFIED__Q', 'QUALIFIED__U',\n       'STYLE__1 Story', 'STYLE__1.5 Story Fin', 'STYLE__1.5 Story Unfin',\n       'STYLE__2 Story', 'STYLE__2.5 Story Fin', 'STYLE__2.5 Story Unfin',\n       'STYLE__3 Story', 'STYLE__3.5 Story Fin', 'STYLE__3.5 Story Unfin',\n       'STYLE__4 Story', 'STYLE__4.5 Story Fin', 'STYLE__4.5 Story Unfin',\n       'STYLE__Bi-Level', 'STYLE__Default', 'STYLE__Outbuildings',\n       'STYLE__Split Foyer', 'STYLE__Split Level', 'STYLE__Vacant',\n       'STRUCT__Default', 'STRUCT__Multi', 'STRUCT__Row End',\n       'STRUCT__Row Inside', 'STRUCT__Semi-Detached', 'STRUCT__Single',\n       'STRUCT__Town End', 'STRUCT__Town Inside', 'GRADE__Above Average',\n       'GRADE__Average', 'GRADE__Excellent', 'GRADE__Exceptional-A',\n       'GRADE__Exceptional-B', 'GRADE__Exceptional-C',\n       'GRADE__Exceptional-D', 'GRADE__Fair Quality',\n       'GRADE__Good Quality', 'GRADE__Low Quality', 'GRADE__No Data',\n       'GRADE__Superior', 'GRADE__Very Good', 'CNDTN__Average',\n       'CNDTN__Default', 'CNDTN__Excellent', 'CNDTN__Fair', 'CNDTN__Good',\n       'CNDTN__Poor', 'CNDTN__Very Good', 'EXTWALL__Adobe',\n       'EXTWALL__Aluminum', 'EXTWALL__Brick Veneer',\n       'EXTWALL__Brick\/Siding', 'EXTWALL__Brick\/Stone',\n       'EXTWALL__Brick\/Stucco', 'EXTWALL__Common Brick',\n       'EXTWALL__Concrete', 'EXTWALL__Concrete Block', 'EXTWALL__Default',\n       'EXTWALL__Face Brick', 'EXTWALL__Hardboard',\n       'EXTWALL__Metal Siding', 'EXTWALL__Plywood', 'EXTWALL__SPlaster',\n       'EXTWALL__Shingle', 'EXTWALL__Stone', 'EXTWALL__Stone Veneer',\n       'EXTWALL__Stone\/Siding', 'EXTWALL__Stone\/Stucco',\n       'EXTWALL__Stucco', 'EXTWALL__Stucco Block',\n       'EXTWALL__Vinyl Siding', 'EXTWALL__Wood Siding', 'ROOF__Built Up',\n       'ROOF__Clay Tile', 'ROOF__Comp Shingle', 'ROOF__Composition Ro',\n       'ROOF__Concrete', 'ROOF__Concrete Tile', 'ROOF__Metal- Cpr',\n       'ROOF__Metal- Pre', 'ROOF__Metal- Sms', 'ROOF__Neopren',\n       'ROOF__Shake', 'ROOF__Shingle', 'ROOF__Slate', 'ROOF__Typical',\n       'ROOF__Water Proof', 'ROOF__Wood- FS', 'INTWALL__Carpet',\n       'INTWALL__Ceramic Tile', 'INTWALL__Default', 'INTWALL__Hardwood',\n       'INTWALL__Hardwood\/Carp', 'INTWALL__Lt Concrete',\n       'INTWALL__Parquet', 'INTWALL__Resiliant', 'INTWALL__Terrazo',\n       'INTWALL__Vinyl Comp', 'INTWALL__Vinyl Sheet',\n       'INTWALL__Wood Floor', 'SOURCE__Condominium',\n       'SOURCE__Residential', 'WARD__Ward 1', 'WARD__Ward 2',\n       'WARD__Ward 3', 'WARD__Ward 4', 'WARD__Ward 5', 'WARD__Ward 6',\n       'WARD__Ward 7', 'WARD__Ward 8']]\ny = dc_clean['PRICE']","3d5d4ad3":"result = stepwise_selection(X, y)\nprint('resulting features:')\nprint(result)","6382d7b9":"X = dc_clean[['WARD__Ward 3', 'QUALIFIED__Q', 'HEAT__Ht Pump', 'YR_RMDL', 'QUALIFIED__U', 'EYB', 'LIVING_GBA',\n              'STRUCT__Single', 'LATITUDE', 'FIREPLACES', 'GRADE__Very Good', 'GRADE__Good Quality', 'AYB', \n              'GRADE__Excellent', 'WARD__Ward 2', 'CENSUS_TRACT', 'LONGITUDE', 'SALE_NUM', 'WARD__Ward 4',\n              'ZIPCODE', 'WARD__Ward 7', 'STYLE__3 Story', 'GRADE__Superior', 'STRUCT__Semi-Detached', \n              'GRADE__Exceptional-C','BATHRM', 'HF_BATHRM', 'NUM_UNITS', 'ROOMS', 'BEDRM']]\ny = dc_clean['PRICE']","33cc1e6a":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8 , random_state=100)","a5395442":"corrmat = X_train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True,cmap = \"viridis\")\nplt.show()","42723cb2":"X_train_sm = X_train \nX_train_sm = sm.add_constant(X_train_sm)","48065d1d":"lm_sm = sm.OLS(y_train,X_train_sm).fit()\nlm_sm.params","9aca8073":"lm_sm.summary()","0fd05f8b":"X_test_sm = X_test\nX_test_sm = sm.add_constant(X_test_sm)\ny_pred_ols = lm_sm.predict(X_test_sm)\ny_pred_ols","d99f6402":"mse = mean_squared_error(y_test, y_pred_ols)\nr_squared = r2_score(y_test, y_pred_ols)\nprint('Mean Squared Error:' ,mse)\nprint('r square:',r_squared)","d2a28437":"ridge1 = Ridge(alpha = 4, normalize = True)\nridge1.fit(X_train, y_train)              # Fit a ridge regression on the training data\npred_ridge1 = ridge1.predict(X_test)           # Use this model to predict the test data\nprint(pd.Series(ridge1.coef_, index = X.columns))# Print coefficients\nprint(\"MSE for test\",mean_squared_error(y_test, pred_ridge1)) \nprint(\"R2 for test\",r2_score(y_test,pred_ridge1))","cd510ca1":"ridge2 = Ridge(alpha = 10**10, normalize = True)\nridge2.fit(X_train, y_train)             # Fit a ridge regression on the training data\npred_ridge2 = ridge2.predict(X_test)           # Use this model to predict the test data\nprint(pd.Series(ridge2.coef_, index = X.columns)) # Print coefficients\nprint(\"MSE for test\",mean_squared_error(y_test, pred_ridge2))\nprint(\"R2 for test\",r2_score(y_test,pred_ridge2))","02cec212":"ridge3 = Ridge(alpha = 0, normalize = True) # alpha = 0 is a linear regression\nridge3.fit(X_train, y_train)             # Fit a ridge regression on the training data\npred_ridge3 = ridge3.predict(X_test)            # Use this model to predict the test data\nprint(pd.Series(ridge3.coef_, index = X.columns)) # Print coefficients\nprint(\"MSE for test\",mean_squared_error(y_test, pred_ridge3)) \nprint(\"R2 for test\",r2_score(y_test,pred_ridge3))","c3be0623":"alphas = 10**np.linspace(10,-2,100)*0.5\nprint(alphas)\n","9e0c2540":"ridgecv = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error', normalize = True)\nridgecv.fit(X_train, y_train)\nprint(\"Ideal alpha\",ridgecv.alpha_)","a7a58e92":"\nridge4 = Ridge(alpha = ridgecv.alpha_, normalize = True)\nridge4.fit(X_train, y_train)\npred_ridge4 = ridge4.predict(X_test)            # Use this model to predict the test data\nprint(pd.Series(ridge4.coef_, index = X.columns))\nprint(\"MSE for test\",mean_squared_error(y_test, pred_ridge4))\nprint(\"R2 for test\",r2_score(y_test,pred_ridge4))","0a6c8fa9":"lasso = Lasso(max_iter = 10000, normalize = True)\ncoefs = []\n\nfor a in alphas:\n    lasso.set_params(alpha=a)\n    lasso.fit(X_train, y_train)\n    coefs.append(lasso.coef_)\n    \nax = plt.gca()\nax.plot(alphas*2, coefs)\nax.set_xscale('log')\nplt.axis('tight')\nplt.xlabel('alpha')\nplt.ylabel('weights')","8fd0d7eb":"lassocv = LassoCV(alphas = None, cv = 10, max_iter = 100000, normalize = True)\nlassocv.fit(X_train, y_train)\n\nlasso.set_params(alpha=lassocv.alpha_)\nlasso.fit(X_train, y_train)\npred_lasso = lasso.predict(X_test)            # Use this model to predict the test data\nprint(pd.Series(lasso.coef_, index = X.columns))\nprint(\"MSE for test\",mean_squared_error(y_test, pred_lasso))\nprint(\"R2 for test\",r2_score(y_test,pred_lasso))","62d4b695":"knn2 = KNeighborsRegressor(n_neighbors=2)\nknn2.fit(X_train, y_train)\nprint(\"R2 for Train dataset\",knn2.score(X_train, y_train))\npred_knn2 = knn2.predict(X_test)\nprint(\"MSE for test\",mean_squared_error(y_test,pred_knn2))\nprint(\"R2 for test\",r2_score(y_test,pred_knn2))","f44366a7":"knn5 = KNeighborsRegressor(n_neighbors=5)\nknn5.fit(X_train, y_train)\nprint(\"R2 for Train dataset\",knn5.score(X_train, y_train))\npred_knn5 = knn5.predict(X_test)\nprint(\"MSE for test\",mean_squared_error(y_test,pred_knn5))\nprint(\"R2 for test\",r2_score(y_test,pred_knn5))","825c1d93":"\nknn3 = KNeighborsRegressor(n_neighbors=3)\nknn3.fit(X_train, y_train)\nprint(\"R2 for Train dataset\",knn3.score(X_train, y_train))\npred_knn3 = knn3.predict(X_test)\nprint(\"MSE for test\",mean_squared_error(y_test,pred_knn3))\nprint(\"R2 for test\",r2_score(y_test,pred_knn3))","81d44fda":"knn4 = KNeighborsRegressor(n_neighbors=4)\nknn4.fit(X_train, y_train)\nprint(\"R2 for Train dataset\",knn4.score(X_train, y_train))\npred_knn4 = knn4.predict(X_test)\nprint(\"MSE for test\",mean_squared_error(y_test,pred_knn4))\nprint(\"R2 for test\",r2_score(y_test,pred_knn4))","8464a4a3":"#svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n#svr_rbf.fit(X_train,y_train)\n#pred = svr_rbf.predict(X_test)\n#print(\"MSE\",mean_squared_error(y_test,pred))\n#print(\"R2\",r2_score(y_test,pred))","06891d27":"#svr_rbf = SVR(kernel='linear', C=1e3)\n#svr_rbf.fit(X_train,y_train)\n#pred = svr_rbf.predict(X_test)\n#print(\"MSE\",mean_squared_error(y_test,pred))\n#print(\"R2\",r2_score(y_test,pred))","de46b34d":"regr1 = RandomForestRegressor(random_state=0, n_jobs=-1)\nmodel1 = regr1.fit(X_train, y_train)\nmodel1.score(X_train, y_train)\npred_rf1 = model1.predict(X_test)\nprint(\"MSE for test\",mean_squared_error(y_test,pred_rf1))\nprint(\"R2 for test\",r2_score(y_test,pred_rf1))","250efc08":"regr2 = RandomForestRegressor(n_estimators=50,random_state=0, n_jobs=-1)\nmodel2 = regr2.fit(X_train, y_train)\nmodel2.score(X_train, y_train)\npred_rf2 = model2.predict(X_test)\nprint(\"MSE for test\",mean_squared_error(y_test,pred_rf2))\nprint(\"R2 for test\",r2_score(y_test,pred_rf2))","1bc093a6":"regr3 = RandomForestRegressor(n_estimators=100,random_state=0, n_jobs=-1)\nmodel3 = regr3.fit(X_train, y_train)\nmodel3.score(X_train, y_train)\npred_rf3 = model3.predict(X_test)\nprint(\"MSE for test\",mean_squared_error(y_test,pred_rf3))\nprint(\"R2 for test\",r2_score(y_test,pred_rf3))","67db9768":"parameters = {\n    \"n_estimators\": [5, 10, 25, 50, 70, 100, 110, 150], # Test out various amounts of trees in the forest\n    #\"max_features\": [0.25] # Test amount of features\n}\nregr_grid = RandomForestRegressor()\ngrid_rf = GridSearchCV(regr_grid,parameters)","8f90d36a":"grid_rf.fit(X_train, y_train)","f84dee22":"grid_rf.best_params_","42dcdf83":"\nregr4 = RandomForestRegressor(n_estimators=25,random_state=0, n_jobs=-1)\nmodel4 = regr4.fit(X_train, y_train)\nmodel4.score(X_train, y_train)\npred_rf4 = model4.predict(X_test)\nprint(\"MSE for test\",mean_squared_error(y_test,pred_rf4))\nprint(\"R2 for test\",r2_score(y_test,pred_rf4))","0258e826":"feature_importances = pd.DataFrame(regr4.feature_importances_,\n                                   index = X_train.columns,\n                                    columns=['importance']).sort_values('importance',ascending=False)\nfeature_importances","dec491e9":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=feature_importances.index, y=feature_importances.importance,palette=\"deep\")\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Feature importance', fontsize=15)\nplt.title('Feature importance using Random Forest model', fontsize=15)","80e609dc":"gbr1=GradientBoostingRegressor(n_estimators=100) \ngbr1.fit(X_train, y_train) \npred_gbr1=gbr1.predict(X_test)\nprint(\"MSE for test\",mean_squared_error(y_test,pred_gbr1))\nprint(\"R2 for test\",r2_score(y_test,pred_gbr1))\nprint(\"Train score\",gbr1.score(X_train, y_train))\nprint(\"Test score\",gbr1.score(X_test, y_test))","3f61a15a":"feature_importances_gbr1 = pd.DataFrame(gbr1.feature_importances_,\n                                   index = X_train.columns,\n                                    columns=['importance']).sort_values('importance',ascending=False)\nfeature_importances_gbr1","0f1f8bb5":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=feature_importances_gbr1.index, y=feature_importances_gbr1.importance,palette=\"pastel\")\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Feature importance', fontsize=15)\nplt.title('Feature importance using GBR model', fontsize=15)","b0a00cf2":"gbr2 = GradientBoostingRegressor(alpha=0.9, init=None, learning_rate=0.1, loss='ls', max_depth=6, \n                          max_features=1.0, max_leaf_nodes=None, min_samples_leaf=3, \n                          min_samples_split=2, n_estimators=100, random_state=None, subsample=1.0, \n                          verbose=0, warm_start=False) \ngbr2.fit(X_train,y_train)\npred_gbr2=gbr2.predict(X_test)\nprint(\"MSE for test\",mean_squared_error(y_test,pred_gbr2))\nprint(\"R2 for test\",r2_score(y_test,pred_gbr2))\nprint(\"Train score\",gbr2.score(X_train, y_train))\nprint(\"Test score\",gbr2.score(X_test, y_test))","f71c84f0":"feature_importances_gbr2 = pd.DataFrame(gbr2.feature_importances_,\n                                   index = X_train.columns,\n                                    columns=['importance']).sort_values('importance',ascending=False)\nfeature_importances_gbr2","8fdc977a":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=feature_importances_gbr2.index, y=feature_importances_gbr2.importance,palette=\"bright\")\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Feature importance', fontsize=15)\nplt.title('Feature importance using GBR model', fontsize=15)","6694ba6d":"def GradientBooster(param_grid, n_jobs): \n    estimator = GradientBoostingRegressor()\n    cv = ShuffleSplit(X_train.shape[0], n_iter=10, test_size=0.2) \n    classifier = GridSearchCV(estimator=estimator, cv=cv, param_grid=param_grid, n_jobs=n_jobs)\n    classifier.fit(X_train, y_train)\n    print (\"Best Estimator learned through GridSearch\")\n    print (classifier.best_estimator_) \n    return cv, classifier.best_estimator_ ","0411b795":"param_grid={'n_estimators':[100], 'learning_rate': [0.1, 0.05, 0.02, 0.01], 'max_depth':[6,4],\n            'min_samples_leaf':[3,5,9,17], 'max_features':[1.0,0.3] } \nn_jobs=4 #Let's fit GBRT to the digits training dataset by calling the function we just created. \ncv,best_est=GradientBooster(param_grid, n_jobs) ","412536f8":"gbr3 = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n             learning_rate=0.1, loss='ls', max_depth=6, max_features=0.3,\n             max_leaf_nodes=None, min_impurity_decrease=0.0,\n             min_impurity_split=None, min_samples_leaf=3,\n             min_samples_split=2, min_weight_fraction_leaf=0.0,\n             n_estimators=100, n_iter_no_change=None, presort='auto',\n             random_state=None, subsample=1.0, tol=0.0001,\n             validation_fraction=0.1, verbose=0, warm_start=False)\ngbr3.fit(X_train,y_train)\npred_gbr3=gbr3.predict(X_test)\nprint(\"MSE for test\",mean_squared_error(y_test,pred_gbr3))\nprint(\"R2 for test\",r2_score(y_test,pred_gbr3))\nprint(\"Train score\",gbr3.score(X_train, y_train))\nprint(\"Test score\",gbr3.score(X_test, y_test))","b34051d4":"feature_importances_gbr3 = pd.DataFrame(gbr3.feature_importances_,\n                                   index = X_train.columns,\n                                    columns=['importance']).sort_values('importance',ascending=False)\nfeature_importances_gbr3","4781481c":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=feature_importances_gbr3.index, y=feature_importances_gbr3.importance,palette=\"muted\")\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Feature importance', fontsize=15)\nplt.title('Feature importance using GBR model', fontsize=15)","eb0c9b96":"## Splitting the train and test data","51e1ede0":"## Grid search to find the best model","dd8da243":"## Random Forest regression model and prediction by tuning number of estimators.","28a0ae5f":"## Best model using Random forest","90fbdeb9":"## Performing Stepwise regression to find the most significant variables","7c5acde5":"##  Best model using Ridge regression ","39ccfe02":"## Parameter tuning using Grid search","022df01c":"# From all the above models, we found Gradient boosting regression is the best model with R2 of 96.94%.","d81cf7d8":"### Commented because it is taking a huge time to execute marking 100k  records on the folium map)","588852ee":"# Missing Value Treatment","9b62dd8c":"### Including only the significant variables","06df0481":"## K nearest neighbour regression model and prediction for different number of N","16322a52":"## Plotting the location using basemap and Folium map","8aa7ce1b":"## Ridge regression model and Prediction","8c6168ce":"## Feature importance plot","6b6b0dc5":"## Creating the Dummy variable for object datatype variables","0ed93d36":"## OLS model and prediction","81e14858":"## Gradient Boosting regression model and prediction by tuning parameters","cba4b5c6":"## SVM model","cbfe94e6":"## Correlation matrix","407adf3c":"## Best model using GBR","7d65ef8b":"## Importing all the libraries required for the analysis","db0433c3":"## Calculating the missing value percentage in each column","6e0f80ec":"## Lasso Regression model and prediction","0b8db753":"## Best model using KNN ,n_neighbours = 3","fc68b93d":"## Feature importance plot"}}