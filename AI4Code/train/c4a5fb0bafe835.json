{"cell_type":{"9c2f386d":"code","e3728724":"code","344bb648":"code","c6dff643":"code","24ca131d":"code","9fb83105":"code","4716ab49":"code","7aa30604":"code","badcac4e":"code","981ccde5":"code","14d24171":"code","6dc6f474":"code","e2a6fd45":"code","55d50ecf":"code","f2027edf":"code","994662f1":"code","f0c448a3":"code","fc94eb35":"code","436db3c1":"code","19dcbbfe":"code","0667c96a":"code","99322c7a":"code","03be300a":"code","f7ad57cf":"code","6c4f4be7":"code","32af8c97":"code","7386e9db":"code","987fb3cf":"markdown","3657e03d":"markdown","fcc1170c":"markdown","6e9387a7":"markdown","efe48272":"markdown"},"source":{"9c2f386d":"import numpy as np\nimport pandas as pd \nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nimport keras_tuner as kt\nimport tensorflow as tf","e3728724":"batch_size = 128","344bb648":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-oct-2021\/train.csv\")","c6dff643":"train_set, val_set = train_test_split(train, test_size=0.15, random_state=42)","24ca131d":"portion = 0.01\ntrain_subset = train_set.iloc[np.random.choice(train_set.shape[0], int(train_set.shape[0] * portion))]\nval_subset = val_set.iloc[np.random.choice(val_set.shape[0], int(val_set.shape[0] * portion))]","9fb83105":"train_set.to_csv(\"train.csv\", index=False)\nval_set.to_csv(\"val.csv\", index=False)\ntrain_subset.to_csv(\"train_subset.csv\", index=False)\nval_subset.to_csv(\"val_subset.csv\", index=False)","4716ab49":"del train\ndel train_set\ndel val_set\ndel train_subset\ndel val_subset","7aa30604":"train_ds = tf.data.experimental.make_csv_dataset(\"train.csv\", batch_size=batch_size, label_name=\"target\").shuffle(512).cache().prefetch(1)\nval_ds = tf.data.experimental.make_csv_dataset(\"val.csv\", batch_size=batch_size, label_name=\"target\").cache().prefetch(1)\ntrain_sub_ds = tf.data.experimental.make_csv_dataset(\"train_subset.csv\", batch_size=batch_size, label_name=\"target\").cache().prefetch(1)\nval_sub_ds = tf.data.experimental.make_csv_dataset(\"val_subset.csv\", batch_size=batch_size, label_name=\"target\").cache().prefetch(1)\nsample_submission = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-oct-2021\/sample_submission.csv\")","badcac4e":"test_ds = tf.data.experimental.make_csv_dataset(\"\/kaggle\/input\/tabular-playground-series-oct-2021\/test.csv\", batch_size=1000, shuffle=False)","981ccde5":"categorical_columns = ['f22', 'f43', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284']\nnumerical_columns = ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241']","14d24171":"numeric_features = [tf.feature_column.numeric_column(item, dtype=tf.float32) for item in numerical_columns]\ncategorical_features = [tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list(key, [0, 1])) for key in categorical_columns]","6dc6f474":"features = numeric_features + categorical_features","e2a6fd45":"inputs = dict()\nfor feature in numeric_features:\n    inputs[feature.key] = tf.keras.Input(name=feature.key, shape=())\nfor feature in categorical_features:\n    inputs[feature.categorical_column.key] = tf.keras.Input(name=feature.categorical_column.key, shape=(), dtype=\"int32\")","55d50ecf":"def build_model(hp):\n    x = tf.keras.layers.DenseFeatures(features)(inputs)\n    for depth in range(hp.Choice('depth', [3, 4, 5, 6, 7, 8, 9, 10])):\n        x = keras.layers.Dense(\n            hp.Choice('width', [8, 16, 32, 64, 128, 256]), \n            activation='relu'\n        )(x)\n    output = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n    model = keras.Model(inputs=list(inputs.values()), outputs=output)\n    adam = keras.optimizers.Adam(learning_rate=hp.Float(\"learing_rate\", 1e-5, 5e-3))\n    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[\"accuracy\", keras.metrics.AUC()])\n    return model","f2027edf":"train_length = pd.read_csv(\"train_subset.csv\").shape[0]\nval_length = pd.read_csv(\"val_subset.csv\").shape[0]","994662f1":"train_steps = train_length \/\/ batch_size\nval_steps = val_length \/\/ batch_size","f0c448a3":"tuner = kt.RandomSearch(\n    build_model,\n    objective=kt.Objective(\"val_auc\", direction=\"max\"),\n    max_trials=50)\ntuner.search(train_sub_ds.take(train_steps), epochs=3, validation_data=val_sub_ds.take(val_steps))\nbest_model = tuner.get_best_models()[0]\nkeras.utils.plot_model(best_model, show_shapes=True)","fc94eb35":"best_hp = tuner.get_best_hyperparameters()[0]","436db3c1":"for param in [\"width\", \"depth\", \"learing_rate\"]:\n    print(\"%s:\"%(param), best_hp.get(param))","19dcbbfe":"keras.backend.clear_session()","0667c96a":"model = tuner.hypermodel.build(best_hp)","99322c7a":"model_checkpoint_path = \"model.h5\"\nearly_stopping = keras.callbacks.EarlyStopping(patience=5)\nmodel_checkpoint = keras.callbacks.ModelCheckpoint(model_checkpoint_path, save_best_only=True)\nreduce_lr =  keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=5, min_lr=0.00001)","03be300a":"train_steps = int(1000000 \/\/ batch_size * 0.85)\nval_steps = int(1000000 \/\/ batch_size * 0.15)","f7ad57cf":"history = model.fit(train_ds.take(train_steps), epochs=10, validation_data=val_ds.take(val_steps), callbacks=[early_stopping, model_checkpoint, reduce_lr])","6c4f4be7":"del train_ds\ndel val_ds","32af8c97":"pd.DataFrame(history.history).plot()","7386e9db":"test_steps = 500000 \/\/ 1000\ny_pred = model.predict(test_ds.take(test_steps)).reshape(-1)\nprint(y_pred.shape)\nsample_submission[\"target\"] = y_pred\nsample_submission.to_csv(\"submission.csv\", index=False)","987fb3cf":"## TPS Prediction with DNN and KerasTuner\nIn this notebook, I will build a DNN model to solve TPS problem with the help of KerasTuner. Since this dataset is so large, I will use a small dataset to do HyperParameter Tuning.","3657e03d":"## Conclusion\nEven if I choose only 1% of data for hyper parameter tuning, KerasTuner can still find a good architecture that gets a good result.","fcc1170c":"## Model Development","6e9387a7":"Here is best parameters:","efe48272":"## Submission"}}