{"cell_type":{"9343b2e9":"code","096b491a":"code","c8ceda7a":"code","90849d19":"code","68112ac7":"code","e7f41e21":"code","0e7675c1":"code","b98341ef":"code","359cbd7a":"code","1b01cde9":"code","bd0c0443":"code","105996b0":"code","c5bd5cbb":"code","0dc9d983":"code","68eb9eb9":"code","0d2f0ee9":"code","8446ea1a":"code","07dff9b2":"code","b272b709":"code","52cbc5cc":"code","e78cc3f1":"code","550f2cf3":"markdown","65281f1a":"markdown","3efc2ae7":"markdown","2c7ffd8f":"markdown","d18abf9d":"markdown","3f10fc81":"markdown","f09f2b4a":"markdown","37ccbe2b":"markdown","773604a6":"markdown","eec712de":"markdown"},"source":{"9343b2e9":"import pandas as pd\nimport numpy as np\nimport regex as re\nimport matplotlib.pyplot as plt","096b491a":"df = pd.read_csv(\"..\/input\/large-random-tweets-from-pakistan\/Random \"\n                   \"Tweets from Pakistan- Cleaned- Anonymous.csv\",encoding_errors = 'ignore')","c8ceda7a":"df.shape","90849d19":"df.head(5)","68112ac7":"df = df['full_text']","e7f41e21":"df = df.dropna()","0e7675c1":"# removing urdu\nreg = re.compile(r'[\\-\\u06ff]+', re.UNICODE)\ndf = df.apply(lambda x: re.sub(reg, \"\", x))","b98341ef":"\n\n# remove hyperlinks and #\ndf = df.apply(lambda x: re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', x))\ndf = df.apply(lambda x: re.sub(r'#', '', x))\ndf = df.apply(lambda x: re.sub(r'rt : ', '', x))","359cbd7a":"df = df.drop_duplicates()","1b01cde9":"df.shape","bd0c0443":"import nltk\nimport string \nfrom nltk.corpus import stopwords  \nfrom nltk.stem import PorterStemmer  \nfrom nltk.tokenize import TweetTokenizer\nimport emoji\nfrom nltk.stem import WordNetLemmatizer\n\nnltk.download('stopwords')\n\n# dictionary for lemmatization\nnltk.download('wordnet')","105996b0":"tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                           reduce_len=True)\ndf = df.apply(tokenizer.tokenize)","c5bd5cbb":"stopwords_english = stopwords.words('english')\n\ndef remove_stop(x):\n    return [y for y in x if y not in stopwords_english and y not in string.punctuation\n          and (len(y) > 1 or emoji.is_emoji(y)) ]\n\ndf = df.apply(remove_stop)","0dc9d983":"stemmer = PorterStemmer()\ndef stem(x):\n    return [stemmer.stem(y) for y in x]\n\nstemmed_tweets = df.apply(stem)","68eb9eb9":"from nltk.corpus import twitter_samples\n\nnltk.download('twitter_samples')\n\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n","0d2f0ee9":"def process_tweet(tweet):\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english')\n    # remove stock market tickers like $GE\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    # remove hyperlinks\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(r'#', '', tweet)\n    # tokenize tweets\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(tweet)\n    tweets_clean = []\n    for word in tweet_tokens:\n        if (word not in stopwords_english and  # remove stopwords\n                word not in string.punctuation):  # remove punctuation\n            # tweets_clean.append(word)\n            stem_word = stemmer.stem(word)  # stemming word\n            tweets_clean.append(stem_word)\n\n    return tweets_clean\n    ","8446ea1a":"def build_freqs(tweets, ys):\n    yslist = np.squeeze(ys).tolist()\n\n    freqs = {}\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1\n    return freqs\n","07dff9b2":"tweets = all_positive_tweets + all_negative_tweets\nlabels = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))\nfreqs = build_freqs(tweets, labels)","b272b709":"theta = np.array([[7e-08, 0.0005239, -0.00055517]])\n\ndef logits(tweet):\n\n    global freqs\n    i=0\n    x = np.zeros([1, 3])\n    x[0, 0] = 1\n    for token in tweet:\n\n        print(token)\n        if (token,1) in freqs.keys():\n            x[0,1] += freqs[(token,1)]\n        else: i+=1\n            \n        if (token,0) in freqs.keys():\n            x[0,2] += freqs[(token,0)]\n        else: i+=1\n    return x\n\ndef predict(tweet):\n    z = logits(tweet)\n    z = np.dot(theta,z.flatten())\n    return 1 \/ (1 + np.exp(-z))  #Sigmoid","52cbc5cc":"sample_tweet = df[72]\nsample_tweet\n","e78cc3f1":"if predict(sample_tweet) > 0.5:\n    print(\"tweet convays positive sentiment\")\n\nelse:\n    print(\"tweet has negative sentiment\")","550f2cf3":"### Stemming","65281f1a":"### Testing on our tweet dataset","3efc2ae7":"### Tokenizing tweets","2c7ffd8f":"### Removing empty values","d18abf9d":"## Filtering & Cleaning","3f10fc81":"### Getting Features from already labelled Tweets\n","f09f2b4a":"## Exploratory Data Analysis\n","37ccbe2b":"### Removing stop words","773604a6":"### Building positive and negative word frequencies","eec712de":"## NLP Preprocessing"}}