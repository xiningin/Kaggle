{"cell_type":{"a5ee895a":"code","807f32d0":"code","0707473c":"code","62daaac1":"code","5607edf5":"code","1852eabc":"code","bcc0157c":"code","4d90d83c":"code","fa8e31a0":"markdown","f6400734":"markdown","8d5f7216":"markdown","23a681c0":"markdown"},"source":{"a5ee895a":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport gc\nimport pickle\nimport time\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nfrom joblib import dump, load\n\n\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)","807f32d0":"import pkg_resources\nimport types\ndef get_imports():\n    for name, val in globals().items():\n        if isinstance(val, types.ModuleType):\n            # Split ensures you get root package, \n            # not just imported function\n            name = val.__name__.split(\".\")[0]\n\n        elif isinstance(val, type):\n            name = val.__module__.split(\".\")[0]\n\n        # Some packages are weird and have different\n        # imported names vs. system names\n        if name == \"PIL\":\n            name = \"Pillow\"\n        elif name == \"sklearn\":\n            name = \"scikit-learn\"\n\n        yield name\nimports = list(set(get_imports()))\n\nrequirements = []\nfor m in pkg_resources.working_set:\n    if m.project_name in imports and m.project_name!=\"pip\":\n        requirements.append((m.project_name, m.version))\n\nfor r in requirements:\n    print(\"{}=={}\".format(*r))","0707473c":"data = pd.read_pickle('..\/input\/eda-preprocessing-feature-engineering\/all_data.pkl')\n# Dropping the first 6 months because they were used for lags\ndata = data[data.date_block_num > 5]\ntest  = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv').set_index('ID')\n\n# dropping some of the columns that didn't give any improvement\ndropcols = [\n            \"item_cnt_month_lag_12\",\n            \"item_cnt_month_lag_12_adv\",\n            \"date_item_target_enc_lag_12\",\n            \"date_shop_target_enc_lag_12\",\n            \"date_city_target_enc_lag_1\",\n            \"date_city_target_enc_lag_2\",\n            \"date_city_target_enc_lag_3\",\n            \"date_type_target_enc_lag_1\",\n            \"date_subtype_target_enc_lag_1\",\n            \"new_item_cat_avg_lag_1\",\n            \"new_item_cat_avg_lag_2\",\n            \"new_item_cat_avg_lag_3\",\n            \"new_item_shop_cat_avg_lag_1\",\n            \"new_item_shop_cat_avg_lag_2\",\n            \"new_item_shop_cat_avg_lag_3\",\n           ]\n\n# Doing the time based train-val-test split\nX_train = data[data.date_block_num < 33].drop(['item_cnt_month']+dropcols, axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month']+dropcols, axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month']+dropcols, axis=1)\n\ndel data\ngc.collect()","62daaac1":"X_train.info()","5607edf5":"start_time = time.time()\nmodel = XGBRegressor(\n        max_depth=10,\n        n_estimators=1500,\n        min_child_weight=0.5, \n        colsample_bytree=0.8, \n        subsample=0.7, \n        eta=0.01,\n        tree_method='gpu_hist',\n        seed=0)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 100)\n\nprint(f\"training took {time.time() - start_time}s\")","1852eabc":"plot_features(model, (10,14))","bcc0157c":"# Predicting on the test set\n\nstart_time = time.time()\nY_test = model.predict(X_test).clip(0, 20)\nprint(f\"predicting on test set took {time.time() - start_time}s\")\n\n# Predicting on train set\nstart_time = time.time()\nY_train_pred = model.predict(X_train).clip(0, 20)\nprint(f\"Predicting on train set took {time.time() - start_time} s\")\n\n# Predicting on valid set\nstart_time = time.time()\nY_valid_pred = model.predict(X_valid).clip(0, 20)\nprint(f\"Predicting on valid set took {time.time() - start_time} s\")\n\n# Savin the predictions\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)\n\ntrain_preds = pd.DataFrame({\n    \"ID\": X_train.index, \n    \"item_cnt_month\": Y_train_pred\n})\ntrain_preds.to_csv('xgb_y_train.csv', index=False)\n\nvalid_preds = pd.DataFrame({\n    \"ID\": X_valid.index, \n    \"item_cnt_month\": Y_valid_pred\n})\nvalid_preds.to_csv('xgb_y_valid.csv', index=False)","4d90d83c":"# Saving the model to disk\ndump(model, 'xgb_model.joblib') ","fa8e31a0":"Using XGBRegressor with tuned parameters. Tried stacking, but didn't manage to get any improvement on the results so sticking with a single model","f6400734":"Submitting the predictions","8d5f7216":"Feature Importances:","23a681c0":"Features used:"}}