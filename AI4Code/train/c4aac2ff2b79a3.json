{"cell_type":{"76e54aae":"code","575d2544":"code","3662e525":"code","81b5f484":"code","bb487848":"code","9a613d75":"code","f4e37dd5":"code","25240972":"code","d9ebd19f":"code","c93f903f":"code","71c7466a":"code","09a6713e":"code","bf28554c":"code","c7c5743b":"code","7f687176":"code","5fd95aaa":"code","a5974d14":"code","f20b9c95":"code","06d5f377":"code","7346eb37":"code","7e8c16af":"code","0a39f5f5":"code","ddc54664":"code","4e2f0213":"code","cedfa7d0":"code","f2f57f86":"code","27d260e4":"code","18228cb0":"code","ec6d6b91":"code","2f89e6be":"code","7be55f6f":"code","19d2ab73":"code","5d23eb8c":"code","2c918a9e":"code","9d3545f4":"code","fdfd47d0":"code","e8991ce4":"code","f6ec242b":"code","6d1b057d":"code","3fc8ae42":"code","b61ba30a":"code","dd7cb5a2":"code","4a0f4838":"code","709743d7":"code","c453dbf6":"code","dfa84d40":"code","43e648b8":"code","a8aa9c66":"code","7d54f09f":"code","953d817a":"code","fb975396":"code","f386e551":"markdown","517a49d6":"markdown","7c61a2fd":"markdown","e32daa4b":"markdown","76fab692":"markdown","c2826806":"markdown","9cff0744":"markdown","4d9f17cb":"markdown","6e654c5c":"markdown","a32bced7":"markdown","cac7f544":"markdown","999a3c42":"markdown","0954f509":"markdown","dd62a68a":"markdown","fb7c1a2e":"markdown","fe7ac865":"markdown","85b369a4":"markdown","12d47cad":"markdown","e5ffc6b2":"markdown","b44294d5":"markdown","6be568b7":"markdown","15bf10f2":"markdown","9c93219f":"markdown","0129797c":"markdown","df567f5b":"markdown","61a1d5fd":"markdown","66162813":"markdown","ac6d888a":"markdown","a2c7b561":"markdown","38de1edd":"markdown","3b48311f":"markdown"},"source":{"76e54aae":"import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom sklearn.metrics import confusion_matrix\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n","575d2544":"raw_data = pd.read_csv('train.csv')\nraw_data.head()","3662e525":"#Checking for Null values \nraw_data.isnull().sum()\n","81b5f484":"cleaned_data=raw_data","bb487848":"#i check the data types of the features to better categorize between categorical varibles and numerical variables\ncleaned_data.dtypes","9a613d75":"# Droping all the catigorical featueres and creating a new dataframe with only the target, id and numerical values\ndata_num = cleaned_data.drop(['cat0','cat1','cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9','cat10','cat11','cat12','cat13','cat14','cat15','cat16','cat17','cat18'],axis=1)\ndata_num\n","f4e37dd5":"# Droping all the numerical featueres and creating a new dataframe with only the target, id and numerical values\ndata_cat = cleaned_data.drop(['cont0','cont1','cont2','cont3','cont4','cont5','cont6','cont7','cont8','cont9','cont10'],axis=1)\ndata_cat","25240972":"data_cat.describe(include='all')","d9ebd19f":"targets = cleaned_data['target']\ninputs = cleaned_data.drop(['target'], axis=1)","c93f903f":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(inputs,targets,test_size= 0.30,random_state=9)","71c7466a":"from feature_engine.encoding import CountFrequencyEncoder","09a6713e":"enc = CountFrequencyEncoder()","bf28554c":"enc.fit(x_train)","c7c5743b":"x_train_enc = enc.transform(x_train)","7f687176":"x_train_enc ","5fd95aaa":"from sklearn.feature_selection import SelectKBest","a5974d14":"bestfeatures = SelectKBest(k=20)\nfit = bestfeatures.fit(x_train_enc,y_train)","f20b9c95":"dfscore = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(x_train_enc.columns)","06d5f377":"feature = pd.concat([dfcolumns,dfscore],axis=1)\nfeature.columns = ['feature','Score']","7346eb37":"#Plot\nx_featued_sel = feature.nlargest(20, 'Score')","7e8c16af":"sns.barplot(y=x_featued_sel['feature'], x=x_featued_sel['Score'])","0a39f5f5":"##Dropping all low performing features\nx_cleaned = x_train_enc.drop(['cont0','cont4','cont7','cont9','cont10','cat2','cat3','cat5','cat7','cat8','cat10','cat12','cont2'],axis=1)\n","ddc54664":"x_cleaned","4e2f0213":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(x_cleaned)","cedfa7d0":"scaler.transform(x_cleaned)","f2f57f86":"from sklearn.linear_model import LogisticRegression\nlgr= LogisticRegression()\nlgr.fit(x_cleaned,y_train)","27d260e4":"y_pred_lgr = lgr.predict(x_cleaned)\n","18228cb0":"sc_reg_lgr=confusion_matrix(y_train, y_pred_lgr)\nsc_lgr = np.array(sc_reg_lgr)\naccuracy_score_lgr = (sc_lgr[0,0]+sc_lgr[1,1])\/sc_lgr.sum()\naccuracy_score_lgr","ec6d6b91":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\n\nknc = KNeighborsClassifier(n_neighbors=5)\nknc.fit(x_cleaned,y_train)","2f89e6be":"y_pred_knn = knc.predict(x_cleaned)","7be55f6f":"sc_conf_knc=confusion_matrix(y_train, y_pred_knn)\nsc_knc = np.array(sc_conf_knc)\naccuracy_score_knc = (sc_knc[0,0]+sc_knc[1,1])\/sc_knc.sum()\naccuracy_score_knc","19d2ab73":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(criterion='gini',max_depth=40,max_features='auto')\nrfc.fit(x_cleaned,y_train)","5d23eb8c":"y_pred_rfc = rfc.predict(x_cleaned)\ny_pred_rfc","2c918a9e":"sc_confr_rfc=confusion_matrix(y_train, y_pred_rfc)\nsc_rfc = np.array(sc_confr_rfc)\naccuracy_score_rfc = (sc_rfc[0,0]+sc_rfc[1,1])\/sc_rfc.sum()\naccuracy_score_rfc","9d3545f4":"#parameters\nfrom sklearn.model_selection import RandomizedSearchCV\nrandom_grid_par = {\n'criterion' : [\"gini\", \"entropy\"],\n'max_features': ['auto', 'sqrt'],\n'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n'min_samples_split': [2, 5, 10],\n'min_samples_leaf': [1, 2, 4],\n'bootstrap': [True, False]}","fdfd47d0":"rfc_rg = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid_par, cv = 10, verbose=2, n_jobs = 6)","e8991ce4":"rfc_rg.fit(x_cleaned,y_train)","f6ec242b":"rfc_rg.best_params_","6d1b057d":"rfc_rg.best_score_","3fc8ae42":"from sklearn.linear_model import RidgeClassifier\nrc = RidgeClassifier()\nrc.fit(x_cleaned,y_train)\n\n","b61ba30a":"y_pred_rc = rc.predict(x_cleaned)\ny_pred_rc","dd7cb5a2":"sc_conf_rc=confusion_matrix(y_train, y_pred_rc)\nsc_rc = np.array(sc_conf_rc)\naccuracy_score_rc = (sc_rc[0,0]+sc_rc[1,1])\/sc_rc.sum()\naccuracy_score_rc","4a0f4838":"test_data = pd.read_csv('test.csv')","709743d7":"enc.fit(test_data)\nx_test = enc.transform(test_data)","c453dbf6":"x_test = x_test.drop(['cont0','cont4','cont7','cont9','cont10','cat2','cat3','cat5','cat7','cat8','cat10','cat12','cont2'],axis=1)","dfa84d40":"pred_values = rc.predict(x_test)","43e648b8":"pred_values","a8aa9c66":"pid = test_data['id']\npid","7d54f09f":"output = pd.DataFrame(pid, columns=['id'])\noutput['target'] = pred_values\noutput","953d817a":"output.dtypes","fb975396":"output.to_csv(r'C:\\Users\\Hugo\\Desktop\\output.csv', index = False)","f386e551":"## Hyperparameter","517a49d6":"## Input and targets<a id=\"cell7\"><\/a>\n","7c61a2fd":"## KNeighborsClassifier","e32daa4b":"### Categorical Features Visualaziton \nThe first thing i notic is that a couple of the featus have high unique values, which make it diffcult for creating dummies, so \ni try and see how i can segement these categorial features","76fab692":"## Importaing relevant libraries<a id=\"cell3\"><\/a>","c2826806":"## Model Selection <a id=\"cell9\"><\/a>","9cff0744":"svc = SVC()\nsvc.fit(x_cleaned,y_train)\n\n","4d9f17cb":"## Scaling ","6e654c5c":"### Ridge Classifer","a32bced7":"## Pre-proccesing <a id=\"cell5\"><\/a>","cac7f544":"## The challange <a id=\"cell1\"><\/a>\n\n","999a3c42":"### Data Type Check","0954f509":"### Logistic model","dd62a68a":"## Variables Overview<a id=\"cell2\"><\/a>\n\nThe dataset is used for this competition is synthetic but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features.","fb7c1a2e":"grid_search_cv = GridSearchCV(SVC(), param_grid=grid, scoring='accuracy')","fe7ac865":"## Test data ","85b369a4":"### New numerical Dataframe","12d47cad":"## Introduction\nThis is my second competition submission, where i put together what i have learned during my data science bootcamp and what i have learned through kaggle Data scientist and its courses.\n\n## Table of Content \n    \n   \n   1. [The Challenge](#cell1)\n   2. [Variables Overview](#cell2)\n   2. [Importaing relevant libraries](#cell3)\n   3. [importing dataset](#cell4)\n   4. [Preprocessing dataset](#cell5)\n       - Dealing with missing values\n       - Data Type check\n       - New numerical Dataframe\n       - New categorical Dataframe\n       \n   5. [Feature Selection](#cell6)\n       - Numerical Features\n       - Categorical Features\n       - Creating Dummy Variables for Categorical features \n        \n        \n   6. [Input and targets](#cell7)\n   \n   7. [Train, test , Split](#cell8)\n   8. [Model Selection](#cell9)\n       - Logistic Regression\n       - Random Forest \n       - Ridge Classifier\n       ","e5ffc6b2":"grid = {\n    'c': [1. 10, 100, 100]\n    'kernel': ['rbf','linear']\n    'gama':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n}\n","b44294d5":"from sklearn.svm import SVC","6be568b7":"## Feature Enginering  Count Encoding","15bf10f2":"## Train, test , Split<a id=\"cell8\"><\/a>","9c93219f":"## Importing dataset<a id=\"cell4\"><\/a>","0129797c":"### Random Forest","df567f5b":"sc_conf_svc=confusion_matrix(y_train, y_pred_svc)\nsc_svc = np.array(sc_conf_svc)\naccuracy_score_svc = (sc_svc[0,0]+sc_svc[1,1])\/sc_svc.sum()\naccuracy_score_svc","61a1d5fd":"### New categorical Dataframe","66162813":"## SVC","ac6d888a":"### Numerical Visualazation","a2c7b561":"y_pred_svc = rc.predict(x_train)\ny_pred_svc","38de1edd":"## Feature Selection ","3b48311f":"### Hyper Parameter Tuning SVC"}}