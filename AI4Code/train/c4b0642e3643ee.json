{"cell_type":{"1e645929":"code","09623ab6":"code","52dea367":"code","09c7fc3f":"code","9f26a0bd":"markdown","47d1cb61":"markdown","c7f80eed":"markdown","1a0ac02d":"markdown"},"source":{"1e645929":"# Install required packages\nfrom distutils.dir_util import copy_tree\ncopy_tree(\"..\/input\/orthonet-libs\", \"..\/working\/\")\n!pip install efficientnet-pytorch > \/dev\/null\n\nimport os\nimport csv\nimport albumentations as A\nfrom collections import defaultdict\n\nimport torch.optim\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.optim.lr_scheduler import OneCycleLR\n\nfrom lib.data import CLASSES\nfrom lib.models import get_unet\nfrom lib.training import load_segmentation_transforms, cycle_seg_classifier, save_state\nfrom lib.models import CLASSIFIER_MODEL_GENERATORS\nfrom lib.datasets import OrthonetClassificationDataset","09623ab6":"# Set below to False to train all 7 different model architectures\nSTOP_AFTER_EFFICIENTNET = True","52dea367":"# Paths\nCSV_TRAIN_VAL = \"..\/input\/orthonet-data\/train.csv\"\nMODEL_DIR = \"..\/working\"\nDATA_PATH = \"..\/input\/orthonet-data\/orthonet data\/orthonet data\"\nUNET_PATH = \"..\/input\/orthonet-models\/seg_unet_255_0.9216422.pt\"\n\n# Other settings\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nWEIGHT_LOSS = True\nFOLD = 3\nBS_TRAIN = 16\nBS_VAL = 16\nN_WORKERS = 4\nN_EPOCHS = 300\nLEARNING_RATE = 1e-4\nWEIGHT_DECAY = 5e-4\n\nUSE_CLAHE = True  # Adaptive histogram normalisation after segmentation to accentuate implant features","09c7fc3f":"results_by_model_by_epoch = defaultdict(lambda: defaultdict(list))\n\nclahe_transform = A.Compose([A.CLAHE(p=1)]) if USE_CLAHE else None\n\nunet_model = get_unet(1, 1)\nunet_model.load_state_dict(torch.load(UNET_PATH)['state_dict'])\nunet_model = unet_model.to(DEVICE)\n\nfor model_type, model_generator in CLASSIFIER_MODEL_GENERATORS.items():\n\n    # Data\n    train_transforms, test_transforms = load_segmentation_transforms()\n    ds_train = OrthonetClassificationDataset('train', CSV_TRAIN_VAL, DATA_PATH, train_transforms)\n    ds_val = OrthonetClassificationDataset('val', CSV_TRAIN_VAL, DATA_PATH, test_transforms)\n    dl_train = DataLoader(ds_train, BS_TRAIN, shuffle=True, num_workers=N_WORKERS, pin_memory=True)\n    dl_val = DataLoader(ds_val, BS_VAL, shuffle=True, num_workers=N_WORKERS, pin_memory=True)\n\n    # Model\n    model = model_generator(n_in=2, n_out=len(CLASSES)).to(DEVICE)\n    optimizer = torch.optim.AdamW((p for p in model.parameters() if p.requires_grad), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = OneCycleLR(optimizer, max_lr=LEARNING_RATE*10, steps_per_epoch=len(dl_train), epochs=N_EPOCHS)\n    train_criterion = nn.CrossEntropyLoss(weight=ds_train.get_class_weights().to(DEVICE) if WEIGHT_LOSS else None)\n    test_criterion = nn.CrossEntropyLoss(weight=ds_train.get_class_weights().to(DEVICE) if WEIGHT_LOSS else None)\n\n    # Train\n    best_loss, best_path, last_save_path = 1e10, None, None\n\n    print(f\"Training {model_type}\")\n    for epoch in range(1, N_EPOCHS + 1):\n        train_loss, train_acc = cycle_seg_classifier('train', model, unet_model, dl_train, DEVICE, train_criterion, optimizer, scheduler, stack=True, clahe_transform=clahe_transform)\n        val_loss, val_acc = cycle_seg_classifier('test', model, unet_model, dl_val, DEVICE, test_criterion, optimizer, stack=True, clahe_transform=clahe_transform)\n\n        print(f\"Epoch {epoch:03d}\\t\\tTRAIN loss: {train_loss:.4f}\\tTRAIN acc: {train_acc:.4f}\\t\\tVAL loss: {val_loss:.4f}{'*' if val_loss < best_loss else ''}\\tVAL tacc: {val_acc:.4f}\")\n\n        state = {'epoch': epoch + 1,\n                 'state_dict': model.state_dict(),\n                 'optimizer': optimizer.state_dict(),\n                 'scheduler': scheduler}\n        save_path = os.path.join(MODEL_DIR, f\"segclass_s_{model_type}_{epoch}_{val_loss:.07f}.pt\")\n        best_loss, last_save_path = save_state(state, save_path, val_loss, best_loss, last_save_path)\n\n        results_by_model_by_epoch[model_type]['train_loss'].append(train_loss)\n        results_by_model_by_epoch[model_type]['train_acc'].append(train_acc)\n        results_by_model_by_epoch[model_type]['val_loss'].append(val_loss)\n        results_by_model_by_epoch[model_type]['val_acc'].append(val_acc)\n\n    with open(os.path.join(MODEL_DIR, f\"segclass_s_{model_type}_{best_loss}.txt\"), 'w') as f:\n        writer = csv.writer(f)\n        writer.writerow(results_by_model_by_epoch[model_type].keys())\n        writer.writerows(zip(*results_by_model_by_epoch[model_type].values()))\n        \n    if STOP_AFTER_EFFICIENTNET:\n        break","9f26a0bd":"### Settings","47d1cb61":"# Run","c7f80eed":"# Train the segmentation classifier networks\n\nThese networks are trained to classify the output of the UNet network, which we trained previously to segment out the orthopaedic prostheses present on an X-ray.\n\nAs with the simple classifier, 7 different network architectures are trained.\n\nThere are actually two inputs into these segmentation classifiers: the segmented out prostheses in 1 channel, and the full original radiograph as a second channel.","1a0ac02d":"### Install and import the required packages"}}