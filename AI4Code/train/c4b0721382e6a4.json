{"cell_type":{"92f7b1d3":"code","236bd64c":"code","f54f437e":"code","6859f672":"code","9d3520b6":"code","b1491e4f":"code","cbae9ae2":"code","62c4d826":"code","5c6592de":"code","41ebec7e":"code","d83b1684":"code","017737ab":"code","0c71217b":"code","9ebdaad6":"code","ab36bb1b":"code","da21d375":"code","9ec47299":"code","02d5c001":"code","cc96c90a":"code","da76063e":"code","bae95ab2":"code","5c6ed5eb":"code","6e3cef99":"code","6a91e8d1":"code","4b838310":"code","94db500d":"code","038a24c8":"code","c8403ffa":"code","56ac245f":"code","bf040cac":"code","d2772a52":"markdown","f6141acf":"markdown","a793311f":"markdown","4c442b0e":"markdown","73193d1a":"markdown","b9dff461":"markdown","1217970a":"markdown","a523a1e9":"markdown","d5e32fcb":"markdown","9654f036":"markdown","e53c9486":"markdown"},"source":{"92f7b1d3":"import pandas as pd\n\nsamplesub = pd.read_csv('\/kaggle\/input\/ml-challenge-tr-is-bankasi\/sampleSubmission.csv')\ntatil_data = pd.read_excel('\/kaggle\/input\/ozelveriler\/Resmi Tatiller.xlsx')\ndolar = pd.read_csv('\/kaggle\/input\/ozelveriler\/dolar.csv')\nenflasyon = pd.read_csv('\/kaggle\/input\/ozelveriler\/enflasyon.csv',sep=\"\\t\")","236bd64c":"data = pd.read_csv('\/kaggle\/input\/ml-challenge-tr-is-bankasi\/train.csv')\ndata.drop('Record_Count',axis=1,inplace=True)\ndata.reset_index(inplace=True)\ndata = data.rename(columns={'index':'ID'})\ndata.head()","f54f437e":"test = pd.read_csv('\/kaggle\/input\/ml-challenge-tr-is-bankasi\/test.csv')\ntest.drop(['Record_Count'],axis=1,inplace=True)\ntest.head()","6859f672":"data['YIL_AY_RFM']=data['YIL_AY'].astype(str).apply(lambda x: x + '01')\ndata['YIL_AY_RFM']=pd.to_datetime(data['YIL_AY_RFM'],format='%Y%m%d')\n\ntest['YIL_AY_RFM']=test['YIL_AY'].astype(str).apply(lambda x: x + '01')\ntest['YIL_AY_RFM']=pd.to_datetime(test['YIL_AY_RFM'],format='%Y%m%d')","9d3520b6":"custom_aggregation = {}\ncustom_aggregation[\"YIL_AY_RFM\"] = lambda x:x.iloc[0]\ncustom_aggregation[\"CUSTOMER\"] = lambda x:x.iloc[0]\ncustom_aggregation[\"ISLEM_TUTARI\"] = \"sum\"\n\n\nrfmTable = data.groupby(\"ID\").agg(custom_aggregation)","b1491e4f":"import datetime as dt\nNOW = dt.datetime(2019,1,1)\nrfmTable[\"Recency\"] = NOW - rfmTable[\"YIL_AY_RFM\"]\nrfmTable[\"Recency\"] = pd.to_timedelta(rfmTable[\"Recency\"]).astype(\"timedelta64[D]\")","cbae9ae2":"custom_aggregation = {}\n\ncustom_aggregation[\"Recency\"] = [\"min\", \"max\"]\ncustom_aggregation[\"YIL_AY_RFM\"] = lambda x: len(x)\ncustom_aggregation[\"ISLEM_TUTARI\"] = \"sum\"\n\nrfmTable_final = rfmTable.groupby(\"CUSTOMER\").agg(custom_aggregation)","62c4d826":"rfmTable_final.columns = [\"min_recency\", \"max_recency\", \"frequency\", \"monetary_value\"]\n\nquantiles = rfmTable_final.quantile(q=[0.25,0.5,0.75])\nquantiles = quantiles.to_dict()\nsegmented_rfm = rfmTable_final\n\ndef RScore(x,p,d):\n    if x <= d[p][0.25]:\n        return 1\n    elif x <= d[p][0.50]:\n        return 2\n    elif x <= d[p][0.75]: \n        return 3\n    else:\n        return 4\n    \ndef FMScore(x,p,d):\n    if x <= d[p][0.25]:\n        return 4\n    elif x <= d[p][0.50]:\n        return 3\n    elif x <= d[p][0.75]: \n        return 2\n    else:\n        return 1","5c6592de":"segmented_rfm['r_quartile'] = segmented_rfm['min_recency'].apply(RScore, args=('min_recency',quantiles,))\nsegmented_rfm['f_quartile'] = segmented_rfm['frequency'].apply(FMScore, args=('frequency',quantiles,))\nsegmented_rfm['m_quartile'] = segmented_rfm['monetary_value'].apply(FMScore, args=('monetary_value',quantiles,))\nsegmented_rfm['RFMScore'] = segmented_rfm.r_quartile.map(str) + segmented_rfm.f_quartile.map(str) + segmented_rfm.m_quartile.map(str)\nsegmented_rfm = segmented_rfm.reset_index()","41ebec7e":"data = pd.merge(data,segmented_rfm,on='CUSTOMER')\ndata.drop(columns=['r_quartile','f_quartile','m_quartile'],inplace=True)","d83b1684":"test = pd.merge(test,segmented_rfm,on='CUSTOMER')\ntest.drop(columns=['r_quartile','f_quartile','m_quartile'],inplace=True)","017737ab":"data['Y\u0131l'] = data['YIL_AY'].apply(lambda x:str(x)[0:4])\ndata['Ay'] = data['YIL_AY'].apply(lambda x:str(x)[-2:])\ndata.drop('YIL_AY',axis=1,inplace=True)\n\ntest['Y\u0131l'] = test['YIL_AY'].apply(lambda x:str(x)[0:4])\ntest['Ay'] = test['YIL_AY'].apply(lambda x:str(x)[-2:])\ntest.drop('YIL_AY',axis=1,inplace=True)\n\ntatil_data.drop(['g.1','g.2'],axis=1,inplace=True)\ntatil_data['Ay'] = tatil_data.date.apply(lambda x: str(x)[5:7])\ntatil_data['Y\u0131l'] = tatil_data.date.apply(lambda x: str(x)[0:4])\ntatil_data.Ay = tatil_data.Ay.astype(int)\ntatil_data.Y\u0131l = tatil_data.Y\u0131l.astype(int)\nnew_tatil = tatil_data.groupby(['Ay','Y\u0131l']).sum()['holiday']\nnew_tatil = new_tatil.reset_index()\n\ndata['Ay'] = data['Ay'].apply(lambda x: str(x[-1]) if str(x)[0]==\"0\" else str(x))\ntest['Ay'] = test['Ay'].apply(lambda x: str(x[-1]) if str(x)[0]==\"0\" else str(x))\n\ndata['Y\u0131l'] = data['Y\u0131l'].astype(int)\ndata['Ay'] = data['Ay'].astype(int)\n\ntest['Y\u0131l'] = test['Y\u0131l'].astype(int)\ntest['Ay'] = test['Ay'].astype(int)","0c71217b":"data = pd.merge(data, new_tatil, on=['Ay','Y\u0131l'],how=\"left\")\ntest = pd.merge(test, new_tatil, on=['Ay','Y\u0131l'],how=\"left\")","9ebdaad6":"data['holiday'] = data.holiday.fillna(0)\ndata['holiday'] = data.holiday.astype(int)\n\ntest['holiday'] = test.holiday.fillna(0)\ntest['holiday'] = test.holiday.astype(int)\n\ndolar = dolar[['Tarih','\u015eimdi']]\ndolar.columns = [\"Tarih\",\"Dolar_Kuru\"]\ndolar['Tarih'] = pd.to_datetime(dolar['Tarih'])\ndolar['Y\u0131l'] = dolar['Tarih'].apply(lambda x: str(x)[0:4])\ndolar['Ay'] = dolar['Tarih'].apply(lambda x: str(x)[5:7])\ndolar['Ay'] = dolar['Ay'].apply(lambda x: str(x[-1]) if str(x)[0]==\"0\" else str(x))\ndolar['Dolar_Kuru'] = dolar['Dolar_Kuru'].str.replace(\",\",\".\").astype(float)\ndolar.drop('Tarih',axis=1,inplace=True)\ndolar['Ay'] = dolar['Ay'].astype(int)\ndolar['Y\u0131l'] = dolar['Y\u0131l'].astype(int)\ndolar = dolar.groupby(['Y\u0131l','Ay']).mean().reset_index()","ab36bb1b":"data = pd.merge(data, dolar, on=['Ay','Y\u0131l'],how=\"left\")\ntest = pd.merge(test, dolar, on=['Ay','Y\u0131l'],how=\"left\")","da21d375":"enflasyon.columns=[\"Tarih\",\"Enflasyon\",\"Beklenti\",\"\u00d6nceki\",\"BUG\"]\nenflasyon.drop('BUG',axis=1,inplace=True)\nenflasyon = enflasyon[['Tarih','Enflasyon']]\n\nenflasyon['Tarih'] = pd.to_datetime(enflasyon['Tarih'])\nenflasyon['Y\u0131l'] = enflasyon['Tarih'].apply(lambda x: str(x)[0:4])\nenflasyon['Ay'] = enflasyon['Tarih'].apply(lambda x: str(x)[8:10])\nenflasyon['Ay'] = enflasyon['Ay'].apply(lambda x: str(x[-1]) if str(x)[0]==\"0\" else str(x))\nenflasyon.drop('Tarih',axis=1,inplace=True)\nenflasyon['Ay'] = enflasyon['Ay'].astype(int)\nenflasyon['Y\u0131l'] = enflasyon['Y\u0131l'].astype(int)","9ec47299":"data = pd.merge(data, enflasyon, on=['Ay','Y\u0131l'],how=\"left\")\ntest = pd.merge(test, enflasyon, on=['Ay','Y\u0131l'],how=\"left\")","02d5c001":"obj_cols=['SEKTOR','CUSTOMER']\ncolumns =['ISLEM_ADEDI']\n\nfor col in columns:\n    for feat in obj_cols:\n        data[f'{col}_mean_group_{feat}']=data[col]\/data.groupby(feat)[col].transform('mean')\n        data[f'{col}_max_group_{feat}']=data[col]\/data.groupby(feat)[col].transform('max')\n        data[f'{col}_min_group_{feat}']=data[col]\/data.groupby(feat)[col].transform('min')\n               \nfor col in columns:\n    for feat in obj_cols:\n        test[f'{col}_mean_group_{feat}']=test[col]\/test.groupby(feat)[col].transform('mean')\n        test[f'{col}_max_group_{feat}']=test[col]\/test.groupby(feat)[col].transform('max')\n        test[f'{col}_min_group_{feat}']=test[col]\/test.groupby(feat)[col].transform('min')","cc96c90a":"# Create correlation matrix\ncorr_matrix = data.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n# Drop features \ndata = data.drop(data[to_drop], axis=1)\ntest = test.drop(test[to_drop], axis=1)","da76063e":"categoricals = [\"ISLEM_TURU\"]\n\nfrom sklearn.preprocessing import LabelEncoder\nfor col in data[categoricals].columns:\n        \n    data[col] = data[col].astype(str)\n    test[col] = test[col].astype(str)\n        \n    le = LabelEncoder()\n    le.fit(list(data[col])+list(test[col]))\n    data[col] = le.transform(data[col])\n    test[col]  = le.transform(test[col])\n        \n    data[col] = data[col].astype('category')\n    test[col] = test[col].astype('category')","bae95ab2":"data = pd.get_dummies(data, columns=[\"SEKTOR\"], prefix=[\"Sektor\"] )\ntest = pd.get_dummies(test, columns=[\"SEKTOR\"], prefix=[\"Sektor\"] )","5c6ed5eb":"drop_columns = ['ID','YIL_AY_RFM','Y\u0131l','Ay']\n#drop_columns = ['ID','YIL_AY_RFM']\ndata = data.drop(columns=drop_columns)\ntest = test.drop(columns=drop_columns)","6e3cef99":"X = np.array(data.drop(['ISLEM_TUTARI'],axis=1))\ny = np.array(data.ISLEM_TUTARI.values)","6a91e8d1":"\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\ndtr = DecisionTreeRegressor(max_depth=12,min_samples_split=5,min_samples_leaf=6)\nx_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=1)\ndtr.fit(x_train,y_train)\n\ny_pred_dtr = dtr.predict(x_valid)\nprint('Test:{}'.format(np.sqrt(mean_squared_error(y_valid,y_pred_dtr))))","4b838310":"y_pred_train = model.predict(x_train)\nprint('Train Score: {}'.format(np.sqrt(mean_squared_error(y_tra\n                                                          in, y_pred_train))))","94db500d":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict, train_test_split\n\nparam_grid = {\"criterion\": [\"mse\"],\n              \"min_samples_split\": [2,7,10],\n              \"max_depth\": [5,8,12,15],\n              \"min_samples_leaf\": [2,7,10],\n              \"max_leaf_nodes\": [5,10,20],\n              }\ndtr2 = DecisionTreeRegressor()\n\nX_train, X_test, y_train , y_test = train_test_split(X, y, test_size=0.25, random_state=2)\nX_train, X_val,y_train , y_val = train_test_split(X_train, y_train, test_size=0.10, random_state=2)\n\ngrid_cv_dtm = GridSearchCV(dtr2, param_grid, cv=2,verbose=1,n_jobs=-1)\n\ngrid_cv_dtm.fit(X_train,y_train)\n\nprint('Best Score: {}'.format(grid_cv_dtm.best_score_))\nprint('Best parameters {}'.format(grid_cv_dtm.best_params_))","038a24c8":"dtr2 = DecisionTreeRegressor(max_depth=12,min_samples_split=5,min_samples_leaf=6)\n#dtr2 = grid_cv_dtm.best_estimator_\n\ny_pred_train = dtr2.predict(X_train)\nprint('Train Score: {}'.format(np.sqrt(mean_squared_error(y_train, y_pred_train))))\n\ny_pred_test = dtr2.predict(X_test)\nprint('Test Score: {}'.format(np.sqrt(mean_squared_error(y_test, y_pred_test))))\n\ny_pred_val = dtr2.predict(X_val)\nprint('Validation Score: {}'.format(np.sqrt(mean_squared_error(y_val, y_pred_val))))","c8403ffa":"cols = list(data.columns)\ncols.remove('ISLEM_TUTARI')\n\nfor importance, name in sorted(zip(dtr2.feature_importances_, cols),reverse=True):\n    print (name, importance)","56ac245f":"samplesub['Predicted'] = pd.Series(dtr2.predict(test))","bf040cac":"samplesub.to_csv('submission.csv',index=False)","d2772a52":"Tatil g\u00fcnleri, ayl\u0131k dolar verisi ve ayl\u0131k enflasyon verisini de\u011fi\u015fken olarak dataframemize ekliyoruz.","f6141acf":"Her bir sekt\u00f6r ve m\u00fc\u015fteri i\u00e7in i\u015flem adedinin minimum, maksimum ve ortalamas\u0131n\u0131 buluyoruz.","a793311f":"Gerekli verileri y\u00fckl\u00fcyoruz.\n* Dolar i\u00e7in d\u0131\u015f kaynak verisi: https:\/\/tr.investing.com\/currencies\/usd-try\n* Enflasyon i\u00e7in d\u0131\u015f kaynak verisi: https:\/\/www.tcmb.gov.tr\/wps\/wcm\/connect\/EN\/TCMB+EN\/Main+Menu\/Statistics\/Inflation+Data\/Consumer+Prices","4c442b0e":"Her bir m\u00fc\u015fterimiz i\u00e7in RFM analizi yap\u0131caz. RFM analizini detayl\u0131 incelemek isterseniz https:\/\/medium.com\/kaveai\/rfm-analizi-ve-kmeans-algoritmas\u0131yla-m\u00fc\u015fteri-segmentasyonu-fad0842945b1 yaz\u0131ma g\u00f6z atabilirsiniz.","73193d1a":"Korelasyonu 0.95'ten b\u00fcy\u00fck olan de\u011fi\u015fkenleri d\u00fc\u015f\u00fcr\u00fcyoruz.","b9dff461":"\u0130\u015flem t\u00fcr\u00fcn\u00fc Label Encoder kullanarak 0-1 haline getiriyoruz","1217970a":"# Model","a523a1e9":"> # \u00d6znitelik \u00c7\u0131kar\u0131m\u0131 ve RFM Analizi","d5e32fcb":"1. DecisionTreeRegressor modelimizi kuruyoruz. GridSearch ile parametre ara\u015ft\u0131rmas\u0131n\u0131 yap\u0131p sonu\u00e7 ald\u0131k fakat elimizle parametreleri ayarlay\u0131nca daha iyi sonu\u00e7 ald\u0131\u011f\u0131m\u0131zdan GridSearch'ten elde etti\u011fimiz parametreleri kullanmad\u0131k. \n1. Modelimizin ba\u015far\u0131s\u0131n\u0131 \u00f6l\u00e7mek i\u00e7in verimizi e\u011fitim, test ve validasyon olmak \u00fczere 3 e ay\u0131rd\u0131k.","9654f036":"Y\u0131l ve ay olarak ba\u015fka i\u015flemlerde kullanmak \u00fczere de\u011fi\u015fken olu\u015fturuyoruz.","e53c9486":"# Gerekli verisetlerini y\u00fckleme"}}