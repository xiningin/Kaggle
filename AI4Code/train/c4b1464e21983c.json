{"cell_type":{"ca4dbf50":"code","fce227cf":"code","f47e4449":"code","a1d6b9cd":"code","3382a582":"code","c0e3fc5f":"code","7fc63a6a":"code","1a77b92c":"code","c558c099":"code","6d4619ce":"code","dc368d25":"code","62a6e948":"code","2e26f9eb":"code","8bf1b340":"code","7c4dabab":"code","3a732217":"code","692fb76f":"code","982a9ea9":"code","54f68cad":"code","9db8ce1d":"code","dbb28f4c":"code","3e67d1e6":"code","06f764d3":"markdown","3f790f4d":"markdown","ebe14d2b":"markdown","fb9b0980":"markdown","87b16ce6":"markdown","68fad6e8":"markdown","1f343ecf":"markdown","9c4ea00c":"markdown","680e9075":"markdown","a1897657":"markdown","02ebdc1c":"markdown","126e3818":"markdown","4ff0029b":"markdown","89e7309f":"markdown","044038ed":"markdown","b7b8f90a":"markdown","11484a6d":"markdown"},"source":{"ca4dbf50":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory","fce227cf":"df = pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/all_sources_metadata_2020-03-13.csv')\ndf.shape","f47e4449":"df.head()","a1d6b9cd":"df.tail()","3382a582":"title = df.copy()\ntitle = title.dropna(subset=['title'])\ntitle['title'] = title['title'].str.replace('[^a-zA-Z]', ' ', regex=True)\ntitle['title'] = title['title'].str.lower()\ntitle.head()","c0e3fc5f":"title.tail()","7fc63a6a":"title['keyword_vaccine'] = title['title'].str.find('vaccine') \ntitle.head()","1a77b92c":"included_vaccine = title.loc[title['keyword_vaccine'] != -1]\nincluded_vaccine","c558c099":"shaid = []\nfor index, row in included_vaccine.iterrows():\n    id = str(row['sha']) + \".json\"\n    shaid.append(id)","6d4619ce":"import json\nimport os\ndatafiles = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if not (filename==''):\n            if(filename in shaid):\n                ifile = os.path.join(dirname, filename)\n                if ifile.split(\".\")[-1] == \"json\":\n                    datafiles.append(ifile)","dc368d25":"len(datafiles)","62a6e948":"ArrBodyText = []\nfor file in datafiles:\n    with open(file,'r')as f:\n        doc = json.load(f)\n    id = doc['paper_id'] \n    bodytext = ''\n    for item in doc['body_text']:\n        bodytext = bodytext + item['text']\n        \n    ArrBodyText.append({id:bodytext})","2e26f9eb":"ArrBodyText[20]","8bf1b340":"text_split = str(ArrBodyText[20]).split()\nlen(text_split)","7c4dabab":"#Identify common words\nfreq = pd.Series(' '.join(text_split).split()).value_counts()[:20]\nfreq","3a732217":"#Identify uncommon words\nfreq1 =  pd.Series(' '.join(text_split).split()).value_counts()[-20:]\nfreq1","692fb76f":"from nltk.stem.porter import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlem = WordNetLemmatizer()\nstem = PorterStemmer()\nword = \"inversely\"\nprint(\"stemming:\",stem.stem(word))\nprint(\"lemmatization:\", lem.lemmatize(word, \"v\"))","982a9ea9":"# Libraries for text preprocessing\nimport re\nimport nltk\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import RegexpTokenizer\n#nltk.download('wordnet') \nfrom nltk.stem.wordnet import WordNetLemmatizer","54f68cad":"stop_words = set(stopwords.words(\"english\"))\n\nnew_words = [\"using\", \"show\", \"result\", \"large\", \"also\", \"iv\", \"one\", \"two\", \"new\", \"previously\", \"shown\",\"et\",'al']\nstop_words = stop_words.union(new_words)","9db8ce1d":"corpus = []\nfor i in range(0, 4548):\n    #Remove punctuations\n    text = re.sub('[^a-zA-Z]', ' ', text_split[i])\n    \n    #Convert to lowercase\n    text = text.lower()\n    \n    #remove tags\n    text=re.sub(\"&lt;\/?.*?&gt;\",\" &lt;&gt; \",text)\n    \n    # remove special characters and digits\n    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n    \n    ##Convert to list from string\n    text = text.split()\n    \n    ##Stemming\n    ps=PorterStemmer()\n    #Lemmatisation\n    lem = WordNetLemmatizer()\n    text = [lem.lemmatize(word) for word in text if not word in  \n            stop_words] \n    text = \" \".join(text)\n    corpus.append(text)","dbb28f4c":"#View corpus item\ncorpus[222]","3e67d1e6":"#Word cloud\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\n#matplotlib inline\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stop_words,\n                          max_words=100,\n                          max_font_size=90, \n                          random_state=62\n                         ).generate(str(corpus))\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()\nfig.savefig(\"word1.png\", dpi=900)","06f764d3":"1. Objective.\n\n**Identify the files that are related to the vaccination process.**\n\nThe process will search within the metadata file and then specifically search for the filtered files.\n\nIn future updates we will use nlp tools to find which documents have positive results.\n\nThank what was explained in:\n\nhttps:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\n\nhttps:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv","3f790f4d":"4. Text Analysis\n\nWe will use the NLP library called NLTK, first we will make a split on an item of the specific arrangement to be able to separate the words.","ebe14d2b":"now we make the filter by the title field","fb9b0980":"we check the final lines of the dataframe","87b16ce6":"Data Exploration\nWe will now visualize the text corpus that we created after pre-processing to get insights on the most frequently used words.","68fad6e8":"If the result prompt -1, then the title doesn't contained the keyword.","1f343ecf":"2. Load Metadata","9c4ea00c":"Now we go through the selected files and create an arrangement with the body of the article.","680e9075":"Text pre-processing can be divided into two broad categories \u2014 noise removal & normalization. Data components that are redundant to the core text analytics can be considered as noise.","a1897657":"we check that the correct information will be loaded","02ebdc1c":"Creating a list of stop words and adding custom stopwords and Creating a list of custom stopwords","126e3818":"we check how many files crossed","4ff0029b":"now we go through the json files and we compare them with the metadata array","89e7309f":"Now that we have a filter, we will start to perform other text techniques\nto be continue...","044038ed":"we check the first lines of the dataframe","b7b8f90a":"To have a good initial filter, we will reform the title field, so that we can later filter on the subject of vaccines.","11484a6d":"Now that we have the filter for the articles that contain the vaccine title, we will filter the files and load them for further analysis, for this we create an array with the names of the sha that is part of the name of the json file."}}