{"cell_type":{"11a49e54":"code","91b812a0":"code","973d8380":"code","4a46fb44":"code","f8c90d1a":"code","29c06905":"code","e1366f37":"code","c26bf7d6":"code","10853774":"code","10797bcb":"code","fb4f3bb2":"code","12e5a9fb":"code","5086ad1c":"code","10406768":"code","ca97a9d4":"code","d325e63e":"code","da15b5cd":"code","57a31c90":"code","ef1b690a":"code","6b5cd53e":"code","c3cff879":"code","cfbe1bf4":"code","9d42ce57":"code","c98ee3e3":"code","f4c10228":"code","550b6d2e":"code","0acd6765":"code","bf52bd6a":"code","7301fad8":"code","63b998c3":"code","39395e2a":"code","9f5e3e3d":"code","cd96fe25":"code","99bb2cfc":"code","1c895274":"code","2bd11a80":"code","66cc1b70":"code","0308a3aa":"markdown","3882a8e9":"markdown","f1333205":"markdown","0caa6273":"markdown","55fed7b5":"markdown","2ac0d118":"markdown","7ccd1fd7":"markdown","8a905072":"markdown","a395be35":"markdown","8a2f7199":"markdown","96cf08f5":"markdown","868e9b36":"markdown","a2dbda21":"markdown","965fa534":"markdown","ce471e2e":"markdown","2ec74ca9":"markdown","f19292cc":"markdown","00496e20":"markdown","53014030":"markdown","68203e64":"markdown","52c5986a":"markdown","5b9141b6":"markdown","a4c280b5":"markdown","b59be702":"markdown","1bebd62d":"markdown","b4eb90cc":"markdown","7766f645":"markdown","d0711626":"markdown","5c2f5dd8":"markdown","610c9289":"markdown","510b66f7":"markdown","019b1463":"markdown","3182c38c":"markdown","62d80243":"markdown","2b34575b":"markdown","f475016b":"markdown","704d72f2":"markdown","cc9e5d3b":"markdown","a0520b5c":"markdown","5683124f":"markdown","0d57ce5c":"markdown","09440fb6":"markdown","62e8175b":"markdown","14ef075b":"markdown","6e711cbd":"markdown","388c750d":"markdown"},"source":{"11a49e54":"import pandas as pd\nimport numpy as np\n\nimport random\nnp.random.seed(123)\n\n# Reading the data\n#test = pd.read_csv('..\/input\/blight-ticket\/blight_test.csv', engine='python')\ntrain = pd.read_csv('..\/input\/blight-ticket\/blight_train.csv', engine='python')\n\nlatlons = pd.read_csv('..\/input\/blight-ticket\/latlons.csv', engine='python')\naddresses = pd.read_csv('..\/input\/blight-ticket\/addresses.csv', engine='python')\n\n","91b812a0":"train.head()","973d8380":"latlons.head()","4a46fb44":"addresses.head()","f8c90d1a":"\n# Remove compliances that are NaNs (not 1 or 0)\ntrain = train[(train['compliance'] == 1)|(train['compliance'] == 0)]\ntrain.reset_index(drop=True, inplace=True)","29c06905":"\n# Extract a new feature: the gap between the date when the ticket was issued and when the court hearing is supposed to happen:\ntrain['day_gap']= (pd.to_datetime(train['hearing_date']) - pd.to_datetime(train['ticket_issued_date'])).dt.days","e1366f37":"train.drop(['agency_name', 'inspector_name', 'violator_name',\n       'violation_street_number', 'violation_street_name',\n       'violation_zip_code', 'mailing_address_str_number',\n       'mailing_address_str_name', 'city', 'state', 'zip_code',\n       'non_us_str_code', 'country', 'ticket_issued_date', 'hearing_date',\n       'violation_code', 'violation_description',\n       'admin_fee', 'state_fee',\n       'clean_up_cost', 'payment_amount', 'balance_due',\n       'payment_date', 'payment_status', 'collection_status',\n       'grafitti_status', 'compliance_detail', 'fine_amount'], axis=1, inplace=True)","c26bf7d6":"train['disposition'].value_counts()","10853774":"train = train.loc[train['disposition'] != 'Responsible (Fine Waived) by Deter']","10797bcb":"train.isnull().sum()","fb4f3bb2":"train.dropna(inplace=True)","12e5a9fb":"# Split into training features and target values\nX0 = train.loc[:, ['ticket_id', 'disposition', 'late_fee', 'discount_amount',\n       'judgment_amount', 'day_gap']]\n\ny0 = train['compliance']","5086ad1c":"# Merge addresses and latitude and longitute with the training features\nX1 = pd.merge(X0, addresses, how='left', on='ticket_id')\nX2 = pd.merge(X1, latlons, how='left', on='address')\nX2.drop('address', axis=1, inplace=True)","10406768":"# Convert strings in 'disposition' to dummy values (0 and 1) # I could also try LabelEncoder()\ncolumns_for_dummies = X2[['disposition']]\nextra_dummies = pd.get_dummies(columns_for_dummies)\n\n# add the new dummy features \nX3 = pd.merge(X2, extra_dummies, how='left', left_index=True, right_index=True)\n\n# Get the final training set that only includes numeric types\nX4 = X3.drop(['ticket_id', 'disposition'], axis=1)\n\n# forward fill any other missing values\nX4.ffill(inplace=True)\n\nX4.head()","ca97a9d4":"from sklearn.model_selection import train_test_split\n\n# Split into training and testing data\nX_train, X_test, y_train, y_test = train_test_split(X4, y0, random_state=0)","d325e63e":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\n# then, apply the scaler to the test data:\nX_test_scaled = scaler.transform(X_test)","da15b5cd":"from sklearn.linear_model import LogisticRegression\nlogis = LogisticRegression().fit(X_train_scaled, y_train)","57a31c90":"from sklearn.metrics import confusion_matrix, classification_report, recall_score, precision_score\n# (1) obtain probabilities of observations belonging to class 1\nX_test_sc_probs = logis.predict_proba(X_test_scaled)[:, 1]\n\n# (2) use vals function to convert the probabilities to predictions based on a give threshold\n# If threshold is 0.5, then all observations with probability of 0.5 and higher will be assigned to class 1. Otherwise, class 0.\ndef vals (probabilities, threshold=0.5):\n    class_preds = np.where(probabilities >= threshold, 1, 0)\n    return class_preds\n\nX_test_sc_class = vals(X_test_sc_probs, 0.5)\n\n# (3) use the predictions and actual values to construct a confustion matrix and output the classification report.\nprint(confusion_matrix(y_test, X_test_sc_class))\nprint(classification_report(y_test, X_test_sc_class))","ef1b690a":"pd.DataFrame(y_test).value_counts()","6b5cd53e":"import matplotlib.pyplot as plt\n\nX_test_sc_probs = logis.predict_proba(X_test_scaled)[:, 1]\nrecalls = []\nprecisions = []\ndef vals_2 (probabilities, threshold=0.5):\n    class_preds = np.where(probabilities >= threshold, 1, 0)\n    rscore = recall_score(y_test, class_preds)\n    pscore = precision_score(y_test, class_preds)\n    \n    recalls.append(rscore)\n    precisions.append(pscore)\n\nfor i in np.arange(0.0, 1.0, 0.05):\n    vals_2(X_test_sc_probs, i)\n    \nplt.plot(np.arange(0.0, 1.0, 0.05), recalls, label=\"Recall\")\nplt.plot(np.arange(0.0, 1.0, 0.05), precisions, label=\"Precision\")\nplt.xlim([0.0, 1.01])\nplt.ylim([0.0, 1.01])\nplt.xlabel('Thresholds')\nplt.ylabel('Recall \/ Precisions')\nplt.legend()","c3cff879":"from sklearn.metrics import precision_recall_curve\n\nprecision, recall, thresholds = precision_recall_curve(y_test, X_test_sc_probs)\n\nclosest_zero = np.argmin(np.abs(thresholds-0.5))\nclosest_zero_p = precision[closest_zero]\nclosest_zero_r = recall[closest_zero]\n\nplt.figure()\nplt.xlim([0.0, 1.01])\nplt.ylim([0.0, 1.01])\nplt.plot(precision, recall, label='Precision-Recall Curve')\nplt.plot(closest_zero_p, closest_zero_r, 'o', markersize = 12, fillstyle = 'none', c='r', mew=3)\nplt.xlabel('Precision', fontsize=16)\nplt.ylabel('Recall', fontsize=16)\nplt.axes().set_aspect('equal')\nplt.show()","cfbe1bf4":"from sklearn.metrics import roc_curve, auc\n\nfpr_lr, tpr_lr, tts = roc_curve(y_test, X_test_sc_probs)\n\nroc_auc_lr = auc(fpr_lr, tpr_lr)\n\nplt.figure()\nplt.xlim([-0.01, 1.00])\nplt.ylim([-0.01, 1.01])\nplt.plot(fpr_lr, tpr_lr, lw=3, label=' (area = {:0.2f})'.format(roc_auc_lr))\nplt.xlabel('False Positive Rate', fontsize=16)\nplt.ylabel('True Positive Rate', fontsize=16)\nplt.title('ROC curve', fontsize=16)\nplt.legend(loc='lower right', fontsize=10)\nplt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\nplt.axes().set_aspect('equal')\nplt.show()","9d42ce57":"from sklearn.model_selection import GridSearchCV\n# list parameters and values we are interested in. \ngrid_values = {'C': [1, 10, 100], 'penalty':['l1', 'l2']}\n\n#GridSearch will go over each combination looking for the highest recall score, while using k-fold Cross-Validation on 5 folds\nlogis_cv = LogisticRegression(max_iter=1000, solver='liblinear' )\ngrid_lr = GridSearchCV(logis_cv, param_grid = grid_values, cv=5, scoring='recall')\ngrid_lr.fit(X_train, y_train)","c98ee3e3":"grid_lr.best_params_","f4c10228":"nb_lr = LogisticRegression(C=1, penalty='l1', max_iter=1000, solver='liblinear').fit(X_train_scaled, y_train)\nnb_lr_probs = nb_lr.predict_proba(X_test_scaled)[:, 1]\n\ndef vals (probabilities, threshold=0.5):\n    class_preds = np.where(probabilities >= threshold, 1, 0)\n    return class_preds\n\nX_best_test_sc_class = vals(nb_lr_probs, 0.5)\n\n# (3) use the predictions and actual values to construct a confustion matrix and output the classification report.\nprint(confusion_matrix(y_test, X_best_test_sc_class))\nprint(classification_report(y_test, X_best_test_sc_class))","550b6d2e":"precision, recall, thresholds = precision_recall_curve(y_test, nb_lr_probs)\n\nclosest_zero = np.argmin(np.abs(thresholds-0.5))\nclosest_zero_p = precision[closest_zero]\nclosest_zero_r = recall[closest_zero]\n\nplt.figure()\nplt.xlim([0.0, 1.01])\nplt.ylim([0.0, 1.01])\nplt.plot(precision, recall, label='Precision-Recall Curve')\nplt.plot(closest_zero_p, closest_zero_r, 'o', markersize = 12, fillstyle = 'none', c='r', mew=3)\nplt.xlabel('Precision', fontsize=16)\nplt.ylabel('Recall', fontsize=16)\nplt.axes().set_aspect('equal')\nplt.show()","0acd6765":"fpr_lr, tpr_lr, tts = roc_curve(y_test, nb_lr_probs)\n\nroc_auc_lr = auc(fpr_lr, tpr_lr)\n\nplt.figure()\nplt.xlim([-0.01, 1.00])\nplt.ylim([-0.01, 1.01])\nplt.plot(fpr_lr, tpr_lr, lw=3, label=' (area = {:0.2f})'.format(roc_auc_lr))\nplt.xlabel('False Positive Rate', fontsize=16)\nplt.ylabel('True Positive Rate', fontsize=16)\nplt.title('ROC curve', fontsize=16)\nplt.legend(loc='lower right', fontsize=10)\nplt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\nplt.axes().set_aspect('equal')\nplt.show()","bf52bd6a":"pd.DataFrame([X4.columns.values, nb_lr.coef_.T]).T","7301fad8":"from sklearn.ensemble import RandomForestClassifier\n\ngrid_values = {'n_estimators': [20, 50], 'max_features': [7, 9], 'max_depth': [3, 5, 10]}\n\ntemp_tf = RandomForestClassifier(n_jobs=6)\ngrid_tf = GridSearchCV(temp_tf, param_grid = grid_values, scoring='recall')\ngrid_tf.fit(X_train, y_train)","63b998c3":"grid_tf.best_params_","39395e2a":"tforest = RandomForestClassifier(n_estimators=50, n_jobs=6, max_depth=10, max_features=9).fit(X_train, y_train)","9f5e3e3d":"test_pred_probs = tforest.predict_proba(X_test)[:, 1]\n\n# (2) use vals function to convert the probabilities to predictions based on a give threshold\n# If threshold is 0.5, then all observations with probability of 0.5 and higher will be assigned to class 1. Otherwise, class 0.\ndef vals (probabilities, threshold=0.5):\n    class_preds = np.where(probabilities >= threshold, 1, 0)\n    return class_preds\n\nX_test_tf_class = vals(test_pred_probs, 0.5)\n\n# (3) use the predictions and actual values to construct a confustion matrix and output the classification report.\nprint(confusion_matrix(y_test, X_test_tf_class))\nprint(classification_report(y_test, X_test_tf_class))","cd96fe25":"X_test_sc_probs = tforest.predict_proba(X_test)[:, 1]\nrecalls = []\nprecisions = []\ndef vals_2 (probabilities, threshold=0.5):\n    class_preds = np.where(probabilities >= threshold, 1, 0)\n    rscore = recall_score(y_test, class_preds)\n    pscore = precision_score(y_test, class_preds)\n    \n    recalls.append(rscore)\n    precisions.append(pscore)\n\nfor i in np.arange(0.0, 1.0, 0.05):\n    vals_2(X_test_sc_probs, i)\n    \nplt.plot(np.arange(0.0, 1.0, 0.05), recalls, label=\"Recall\")\nplt.plot(np.arange(0.0, 1.0, 0.05), precisions, label=\"Precision\")\nplt.xlim([0.0, 1.01])\nplt.ylim([0.0, 1.01])\nplt.xlabel('Thresholds')\nplt.ylabel('Recall \/ Precisions')\nplt.legend()","99bb2cfc":"from sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\n\nprecision, recall, thresholds = precision_recall_curve(y_test, test_pred_probs)\n\nclosest_zero = np.argmin(np.abs(thresholds-0.5))\nclosest_zero_p = precision[closest_zero]\nclosest_zero_r = recall[closest_zero]\n\nplt.figure()\nplt.xlim([0.0, 1.01])\nplt.ylim([0.0, 1.01])\nplt.plot(precision, recall, label='Precision-Recall Curve')\nplt.plot(closest_zero_p, closest_zero_r, 'o', markersize = 12, fillstyle = 'none', c='r', mew=3)\nplt.xlabel('Precision', fontsize=16)\nplt.ylabel('Recall', fontsize=16)\nplt.axes().set_aspect('equal')\nplt.show()","1c895274":"from sklearn.metrics import roc_curve, auc\n\nfpr_lr, tpr_lr, tts = roc_curve(y_test, test_pred_probs)\n\nroc_auc_lr = auc(fpr_lr, tpr_lr)\n\nplt.figure()\nplt.xlim([-0.01, 1.00])\nplt.ylim([-0.01, 1.01])\nplt.plot(fpr_lr, tpr_lr, lw=3, label=' (area = {:0.2f})'.format(roc_auc_lr))\nplt.xlabel('False Positive Rate', fontsize=16)\nplt.ylabel('True Positive Rate', fontsize=16)\nplt.title('ROC curve', fontsize=16)\nplt.legend(loc='lower right', fontsize=10)\nplt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\nplt.axes().set_aspect('equal')\nplt.show()","2bd11a80":"pd.DataFrame([X4.columns, tforest.feature_importances_]).T.sort_values(1, ascending=False)","66cc1b70":"train.groupby(['compliance'])['late_fee'].mean()","0308a3aa":"As we can see, our data is imbalanced: we have too many observations of class 0: 92.86%. If instead of our model, we just assigned every observation to the majority (class 0), we would get an accuracy of 92.96% because we would get 37,021 (92.86%) observations right and the rest wrong. So, we cannot conclude that our model is any good judging from the accuracy only.\n\nInstead, let's look at precision and recall. \n\n**Precision** is how many observations we predicted to be class 1 and how many actually turned out to be class 1: TP\/(TP + FP) = 147\/(147 + 5) = 0.967 = 97%. This is great!\n\n**Recall** is how many observations were class 1 and how many we detected: TP\/(FN +TP) = 147\/(2698 + 147) = 0.05. This is terrible. This means that out of all people who paid their ticket (class 1), we have detected only 5%. If it were within the context of classifying tumors, it would mean that out of all patients with tumors, we have detected only 5%!\n\nOne way to increase recall is to change the threshold. However, by increasing recall, we will decrease precision. This is called a precision-recall tradeoff, where we have to either prioritize one at the cost of another or find a satisfying balance. Let's find precision and recall scores for different thresholds and plot them on a graph:\n","3882a8e9":"For reference, here is how to interpret the confusion matrix:\n![image.png](attachment:image.png)","f1333205":"Our newly extracted variable has 222 missing values. Those 222 entries won't have much impact because we have 150k entries. Hence, let's drop them:","0caa6273":"We also need to examine the data for typos:","55fed7b5":"# 3. Final conclusions and takeaways","2ac0d118":"At last, let's look at the coefficients of our model. This should tell us which features are the most influential. ","7ccd1fd7":"We can use the graph to evaluate our model's performance compared to a coin toss, whose TPR vs. FPR is the dashed blue line. The metric we use is Area Under the ROC Curve. For the coin toss, it's 0.50. For our model, it is 0.78. This seems like a good improvement from a coin toss. However, we are still unsatisfied with our recall score, so let's see if we can do better than that.","8a905072":"From this classification problem, we learned that: \n1. Increasing late_fee decreases the probability of people paying the blight ticket, while increasing discount_amount increases the probability.\n2. Accuracy is not the best metric to judge the performance. Choosing a metric will depend on the context.\n3. The logistic model had a low recall rate, which we tried to increase by tuning parameters C and changing the regularization method using GridSearchCV\n4. Random Forest model did much better at recall and prediction overall, but we sacrificed the interpretability.\n\n\nIf you have any questions, comments, or concerns, I would love to discuss them in the comments!\n\nIf you found this notebook useful, please upvote it :)","a395be35":"# 2.2 Random Forests","8a2f7199":"## 2.2.1 Random Forest Evaluation","96cf08f5":"We will also make use of the addresses and latlons datasets to get precise coordinates of the buildings that received a blight ticket. One hypothesis is that poorer neighborhoods would have a higher concentration of buildings receiving blight tickets. Therefore, it might be an influencial variable for our model.","868e9b36":"As we mentioned, by default, Logistic Regression applied L2 regularization to variables. It penalizes values that are too large. To ensure that all variables are treated and penalized equally, we have to convert them to the same scale from 0 to 1. This process is called Normalization. We will normalize our data in the cell below:","a2dbda21":"Let's split our training data into another training and testing data using sklearn library:**","965fa534":"# 2.1 Logistic Regression","ce471e2e":"Our recall for class 1 has increased from 0.05 to 0.08 just from tuning the model. However, the precision has dropped form 0.97 to 0.87. Let's also look at the changes in Precision-Recall and ROC Curves:","2ec74ca9":"Let us check if we have any missing values remaining:","f19292cc":"We can also look at the importance of features for our predicitons by calling the .feature_importances attribute. ","00496e20":"![Detroit-Blight-2-lead.jpg](attachment:Detroit-Blight-2-lead.jpg)","53014030":"Now, all variables are on the same scale from 0 to 1. Let's train our model on the normalized training data:","68203e64":"Random Forest Model allows us to specify multiple parameters, among which are:\n* n_estimators - the number of random decision trees we want to create\n* max_features - the maximum number of features a decision tree can use\n* max_depth - the maximum depth of each tree\n\nSo, from the onset, let's search for the best parameters using GridSearch. We will not specify too many values for our parameters because this would result in too many combinations we need to check.","52c5986a":"Now, let's assess the performance of our model on the test data using the same methods: Confusion Matrix and Classification Report","5b9141b6":"As we saw, 'disposition' has three categories: Responsible by Default, Responsible by Admission, and Responsible by Determination. For out model to use these categories, we need to represent them numerically. We can encode them as 0, 1, 2, but our model might interpret the difference between 0 and 1, and 1 and 2 as numerically significant. Instead, we will create a dummy column for each category:","a4c280b5":"# Predicting Blight Ticket Payments with Logistic Regression and Random Forest","b59be702":"Now, we can see that at threshold 0.5, our recall has increased from 0.08 with the Logistic model to 0.24 with Random Forest! This is a great improvement. From the graphs below, we will also see a better looking precision-recall graph and a higher Area Under the ROC Curve (increase from 0.78 to 0.81).","1bebd62d":"We can see that increasing the discount_amount is strongly associated with people paying their blight tickets. Otherwise, increasing the late_fee decreases the likelihood of people paying. These insights can be useful in decision making.","b4eb90cc":"Finally, let's look at the ROC graph. ROC Curve plots True Positive Rate vs. False Positive Rate for each threshold.","7766f645":"When having many variables, it is useful to think of potential features we can extract by combining them. For example, using the hearing_date and ticket_issued_date, we can find the gap between the two. If the gap is small, then the violator has less time to prepare and might get a larger fine. This might lead him\/her to be more reluctant about paying a larger fee.","d0711626":"As we see, those who don't pay for their blight ticket tend to have a higher late_fee, on average. However, we suspect that late_fee is the 'data from the future' because at the point of assigning the ticket, we can't know if the person will have received a late fee. We would have to look at the description of the variable on the Data Portal to learn about what late_fee really means. For now, we will leave it as it is.\n\nAlso, when examining the importance of features from a random forest model, we should remember one caveat. If there are correlated variables, then a random forest model will choose only one of those variables as important while significantly reducing the importance of the other variable, even though the other correlated variables may be as important. For more on this, follow [this link](https:\/\/blog.datadive.net\/selecting-good-features-part-iii-random-forests\/#:~:text=Random%20forest%20feature%20importance,impurity%20and%20mean%20decrease%20accuracy.).\n\nOne drawback of Random Forests is that we can't draw as many insights about the causal process. We know that late_fee is the most important feature, but we don't know its coefficient - does it increase or decrease the probability of people paying the blight ticket? As models get more complex, our ability to interpret results decreases, while the predictive power increases.","5c2f5dd8":"Now, we will train another model with the new parameters and see if our recall_score at threshold 0.5 has increased.","610c9289":"Let's analyze the output.\n\nAccuracy: How many observations did we predict correctly? To find accuracy, we take True Negatives + True Positives and divide by the total number of observations. We can find it manually from our confusion matrix: (37016 + 147)\/(37016 + 147 + 2698 + 5) = 0.9322. Looks great. We classified 93% of observations properly! However, this does not mean our model is any good. To understand why, let's take a look at the distribution of classes 0 and 1 in the data:\n","510b66f7":"Logistic Regression is one of the simplest classification models out there. It works well on simple data that can be separated by a line. It is rare for real-world data to be linearly separable, but it is worth trying a simpler model before a complex one.\n\nHere is a rough outline of how logistic regression works:\n\n![image.png](attachment:image.png)\n\n1. At first, it takes the data and finds a set of coefficients {B0, B1 ... Bn}, which would maximize the probability of observation of class 1 being class 1. \n2. We can also apply regularization - imposing implicit constraints on the model. Imposing constraints on the model makes it less likely to overfit. The two types of regularization are L1 (Ridge) and L2 (Lasso). L1 suits for data where we have many medium to largely significant variables. L2 is better for data with a few highly significant variables, and the rest are of low significance. By default, in sklearn, Logistic Regression uses L2.\n3. Once we have the coefficients, we plug in the variables into our model and find a number in the range from minus infinity to plus infinity.\n4. To convert the number to a probability, we use the logit function on the left.\n5. The logit function outputs the probability of the observation being class 1.","019b1463":"Let's evaluate our model's performance. In the cell below, we (1) obtain probabilities of observations belonging to class 1 and (2) use vals function to convert the probabilities to class predictions based on a given threshold. (3) Then, we use the predicted classes and actual labels to construct the confusion matrix and output the classification report.","3182c38c":"# 1. Cleaning Training Data and Extracting Features\n","62d80243":"Now, let's see the optimal parameters to achieve the highest recall score at the default threshold of 0.5:","2b34575b":"# 2. Modeling","f475016b":"As we mentioned above, Logistic Regression can use one of the two types of regularization: L1 or L2. We can also choose how strong the penalty from those regularizations should be. We can tune the strength of regularization with parameter C, wherein higher C values lead to a less constrained model. The default C is 1, and the default regularization method is L2. Let's use GridSearch to see if we can find combinations of C and L that would result in a better recall score:","704d72f2":"As we can see, we have 195 entries that are \"Responsible (Fine Waived) by Determination.\" This is a special case of Responsible by Determination. We could either remove the special case or make it the general one. Removing it would be better because 195 observations would not impact our model's performance, given big data frame size (159880).","cc9e5d3b":"Now, let's drop variables that (1) can't be used for prediction. Training the model on these variables wouldn't make sense if they are not present in the test data. Those variables can't be used for prediction because variables like 'payment_date' and 'payment_status' are information from the future. At the time of the ticket issue, we can't possibly know when the violator has paid the ticket. Including 'data from the future' leads to data leakage - when during the training phase, the model gets access to data, it's not supposed to know. \n\n(2) Also, we will assume that some values are not essential and drop them. We are also dropping the location-related variables because we will extract precise latitude and longitude from the *latlons* data frame, which contains coordinates for each building in the dataset.","a0520b5c":"## 2.1.1 Logistic Regression Evaluation","5683124f":"## 2.1.2 Logistic Regression and GridSearchCV","0d57ce5c":"Blight violations are issued by the city to individuals who allow their properties to remain in a deteriorated condition. Every year, the city of Detroit issues millions of dollars in fines to residents and every year, many of these fines remain unpaid. Enforcing unpaid blight fines is a costly and tedious process, so the city wants to know: how can we increase blight ticket compliance?\n\nThe first step in answering this question is understanding when and why a resident might fail to comply with a blight ticket. This is where predictive modeling comes in. In this notebook, our task is to **predict whether a given blight ticket will be paid on time.** We will create a function that trains a model to predict blight ticket compliance in Detroit using *train.csv.* Using this model, we will return a series of length 61001 with the data being the probability that each corresponding ticket from *test.csv* will be paid, and the index being the ticket_id.\n\nThe data comes from the [Detroit Open Data Portal.](http:\/\/)https:\/\/data.detroitmi.gov\/ \n\n**File Description:** \n* train.csv - the training set (all tickets issued 2004-2011)\n* test.csv - the test set (all tickets issued 2012-2016)\n* addresses.csv & latlons.csv - mapping from ticket id to addresses, and from addresses to lat\/lon coordinates. \n\n**Data fields:** \n\nFields we can use for prediction:\n* ticket_id - unique identifier for tickets\n* agency_name - Agency that issued the ticket\n* inspector_name - Name of inspector that issued the ticket\n* violator_name - Name of the person\/organization that the ticket was issued to\n* violation_street_number, violation_street_name, violation_zip_code - Address where the violation occurred\n* mailing_address_str_number, mailing_address_str_name, city, state, zip_code, non_us_str_code, country - Mailing address of the violator\n* ticket_issued_date - Date and time the ticket was issued\n* hearing_date - Date and time the violator's hearing was scheduled\n* violation_code, violation_description - Type of violation\n* disposition - Judgment and judgement type\n* fine_amount - Violation fine amount, excluding fees\n* admin_fee - \\$20 fee assigned to responsible judgments\n* state_fee - \\$10 fee assigned to responsible judgments\n* late_fee - 10% fee assigned to responsible judgments\n* discount_amount - discount applied, if any\n* clean_up_cost - DPW clean-up or graffiti removal cost\n* judgment_amount - Sum of all fines and fees\n* grafitti_status - Flag for graffiti violations\n\nFields we cannot use for prediction:\n* payment_amount - Amount paid, if any\n* payment_date - Date payment was made, if it was received\n* payment_status - Current payment status as of Feb 1 2017\n* balance_due - Fines and fees still owed\n* collection_status - Flag for payments in collections\n* compliance [target variable for prediction] \n*  Null = Not responsible\n*  0 = Responsible, non-compliant\n*  1 = Responsible, compliant\n* compliance_detail - More information on why each ticket was marked compliant or non-compliant","09440fb6":"We can see that as the precision increases, the recall decreases. If we want to prioritize recall, we can take a threshold of 0.1, but this would result in a terrible precision.\n\nA more common way to analyze recall vs. precision is using the precision-recall curve. Let's plot it below and indicate the 0.5 threshold with a red circle:","62e8175b":"Compliance, our target variable for prediction, has some NaN values, which are not useful for training the model. Let's remove them:","14ef075b":"The late_fee variable has the strongest positive association with the class 1. late_fee means that a 10% fee was assigned to people who have to pay the ticket, and the value is the amount in dollars that 10 % constitute. Let's examine the mean value for those who complied (class 1) and who didn't (class 0):","6e711cbd":"Now, let's split the predictor (X) and target (Y) variables:","388c750d":"Let's see if we can improve our recall score by using a more complex model - Random Forest Classifier. Random Forest is an ensemble of many decision trees.\n\n1. Each decision tree is trained using a slightly different sample size, called randomized bootstrap copies. **Bootstrapped Samples** pick a random set of rows from the training data with replacement, thus allowing for the same row to be picked multiple times at each selection. The resulting sample has the same sample size n as the original data, but some rows are missing, and some are repeated multiple times. Hence, every decision tree in the forest is trained on slightly different data.\n2. Each decision tree uses a randomized feature split, meaning that every tree will make use of different combinations of the features\n3. Additionally, we could impose constraints such as the max depth of every tree.\n\nIn the end, we have an ensemble of many decision trees that are somewhat different from each other. In this way, some decision trees randomly correct for mistakes of the others, resulting in good generalizability - the ability to apply the model to yet unseen data.\n\nOne cool thing about random forests is that we don't have to perform scaling and normalization, which saves us time and computational power."}}