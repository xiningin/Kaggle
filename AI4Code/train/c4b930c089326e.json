{"cell_type":{"49419dec":"code","0ce06528":"code","af4905f7":"code","8d434565":"code","aecbc27c":"code","048267bd":"code","c6960af0":"code","ea0018e3":"code","68a3d526":"code","ebac590b":"code","f0d0dd91":"code","318b131c":"code","1db23e68":"code","8ace9916":"code","c70bb8db":"code","eb26a3b6":"code","c01f0798":"code","612d9f89":"code","c689b9fb":"code","0b67ea57":"code","a6875715":"code","f7af0d28":"code","1c8a771f":"code","c9becca2":"code","c6f4d0ae":"code","bba2ef35":"code","a43448a2":"code","6f3d9627":"code","20800e9a":"code","aea94271":"code","0283b139":"code","6dd2d61b":"code","41dbed7d":"markdown","106791bc":"markdown","74dbf7a9":"markdown","6eecccc4":"markdown","2785934c":"markdown","66daaa1b":"markdown","9c472e59":"markdown","87a228e2":"markdown","97286786":"markdown","ed2844aa":"markdown","8f4f4116":"markdown","ca47adb8":"markdown","449edae6":"markdown","cd4b7ad2":"markdown","6ac2a006":"markdown","33dd6c4a":"markdown","71277f32":"markdown","39dd6b60":"markdown","046d076f":"markdown","3b0efa7c":"markdown","47399655":"markdown","7e93b5dc":"markdown","988f1aaa":"markdown","271d47fa":"markdown","10d61b5c":"markdown","c396df1c":"markdown","bd822c94":"markdown","397aca31":"markdown","e87baf4a":"markdown","9c499b2e":"markdown","ac261978":"markdown"},"source":{"49419dec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0ce06528":"import pandas as pd\n \nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\ntrain = pd.read_csv(\"..\/input\/sex-input-toy-data\/input.csv\")\ntest = pd.read_csv(\"..\/input\/test-class\/test.csv\")\ntrain_df = pd.DataFrame(train)\n","af4905f7":"# Input feature for training the classifier\nX_train = train_df[['height','weight','foot_size']]\nX_train","8d434565":"# output label class\n\ny_train = train_df[['sex']]\ny_train","aecbc27c":"\n# Input data is continous in nature, so we need to use gaussian NB classifier. \n# For Gaussian,  we need to compute the mean and variance of the input data. \n\ntrain.describe()","048267bd":"# model\nclf = GaussianNB()\n\n# cross validation of training dataset\nkfold = KFold(n_splits=4, random_state=100)\n\n# model fitting\nclf.fit(X_train, y_train)\n\nresults = cross_val_score(clf, X_train, y_train, cv=kfold)\nresults.mean()","c6960af0":"pred = clf.predict(test)\nprint (test,\" \" , \"belongs to class: \",pred)   # this gives the class : female as output. ","ea0018e3":"# function to calculate gaussian probability density function\n\nfrom math import sqrt\nfrom math import pi\nfrom math import exp\n\ntotal_entries =  y_train.count()\n\n# define custom function to calculate gaussian probability for continuous input features. \ndef calculate_gaussian_prob(x, mean , stdev):\n    exponent = exp(-((x-mean)**2 \/ (2 * stdev**2 )))\n    return (1 \/ (sqrt(2 * pi) * stdev)) * exponent\n\ncalculate_gaussian_prob(1.0, 1, 1)","68a3d526":"# prob_male = float((y_train.sex.values == 'male').sum() \/ total_entries)\n# prob_female = float((y_train.sex.values == 'female').sum() \/ total_entries)\n\n# prob_male, prob_female","ebac590b":"# test\n# h_mean = X_train['height'].mean()\n# w_mean = X_train['weight'].mean()\n# f_mean = X_train['foot_size'].mean()\n# h_std = X_train['height'].std()\n# w_std = X_train['weight'].std()\n# f_std = X_train['foot_size'].std()","f0d0dd91":"# posterior_male = prob_male*(calculate_gaussian_prob()","318b131c":"iris_data = pd.read_csv(\"..\/input\/iris-flower-dataset\/Iris_flower_dataset.csv\")\niris_data.head()","1db23e68":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n%matplotlib inline","8ace9916":"# scatter plot","c70bb8db":"# shape\nprint(iris_data.shape)","eb26a3b6":"iris_data.isnull().sum()","c01f0798":"print(iris_data.info())","612d9f89":"iris_data['Species'].unique()","c689b9fb":"iris_data[\"Species\"].value_counts()","0b67ea57":"iris_data.describe()","a6875715":"iris_data.columns","f7af0d28":"cols = iris_data.columns\nfeatures = cols[0:4]\nlabels = cols[4]\nprint(features)\nprint(labels)","1c8a771f":"# Seperating labels and features from the main dataset.\n\nX = iris_data.iloc[:,:-1].values\ny = iris_data.iloc[:,-1 ].values\nX","c9becca2":"# using train_test_split to seprate training and validation data\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.3, random_state = 10)\nX_train, y_train","c6f4d0ae":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n# Model\nclf = KNeighborsClassifier(n_neighbors=10)\n\n# cross validation of training dataset\nkfold = KFold(n_splits=4, random_state=100)\n\n# model fitting\nclf.fit(X_train, y_train)\n\nresults = cross_val_score(clf, X_train, y_train, cv=kfold)\nresults.mean()","bba2ef35":"\n# prediction\ny_pred = clf.predict(X_valid)\ny_pred","a43448a2":"# Summary of the predictions made by the classifier\nprint(classification_report(y_valid, y_pred))\nprint(confusion_matrix(y_valid, y_pred))","6f3d9627":"# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_valid))","20800e9a":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\n# model\nclf_g = GaussianNB()\n\n# cross validation of training dataset\nkfold = KFold(n_splits=4, random_state=100)\n\n# model fitting\nclf_g.fit(X_train, y_train)\n\nresults = cross_val_score(clf_g, X_train, y_train, cv=kfold)\nresults.mean()\n","aea94271":"# prediction\ny_pred = clf_g.predict(X_valid)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_valid, y_pred))\nprint(confusion_matrix(y_valid, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_valid))\n","0283b139":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\nclf_d = DecisionTreeClassifier()\n\n# cross validation of training dataset\nkfold = KFold(n_splits=4, random_state=100)\n\n# model fitting\nclf_d.fit(X_train, y_train)\n\nresults = cross_val_score(clf_g, X_train, y_train, cv=kfold)\nresults.mean()\n","6dd2d61b":"clf_d.fit(X_train, y_train)\n\ny_pred = clf_d.predict(X_valid)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_valid, y_pred))\nprint(confusion_matrix(y_valid, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_valid))","41dbed7d":"I have tried to explain the theoretical concepts of conditional probability and its use in Bayes Theorem. \nApplication of Bayes theorem in Naive Bayes classifier using toy dataset of gender classification. \n\nFuther usage of DecisionTree classifier, KNN and Gaussian NB has been is described on the very famous IRIS flower dataset. Please explore the below notebook and **upvote it. **\n\n","106791bc":"**Bayes's theorem** shows the relationship between a conditional probability and its inverse. It allows us to make inference from the probability of a hypothesis given the evidence to probability of the given hypothesis and vice versa. \n\nfrom conditional probability, we already know.\n\n       Conditional probability of event A given prior event B:   P(A\/B) = P(A \u2229 B) \/ P(B)\n                                                        Hence,   P(A \u2229 B) = P(A\/B) * P(B)   -   equation 1.\n                                                        \n                                            Similarly, P(B\/A) =  P(A \u2229 B) \/ P(A)\n                                            Hence,     P(A \u2229 B) = P(B\/A) * P(A)             -   equation 2.\n                                            \nFrom equation 1 and 2, \n\n                                      P(A\/B) * P(B) = P(B\/A) * P(A)\n          Bayes' theorem will be    P(A\/B) = ( P(B\/A) * P(A) )\/ P(B)\n            \n          P(A) is prior probability. It represents the pior knowledge of A before gathering of information.\n          P(B\/A) is likelihood probability.      ","74dbf7a9":"## Naive Bayes' Classifier","6eecccc4":"### Example 1\n\nLet's understand the concept with a basic example. \n\nWe throw toss 3 coins simultaneously. \nSo, the sample space will be \"THH, HTH, HHT, TTH, THT, HTT, TTT, HHH\". In total 8 events. \n\nConsider,event A of getting at least 2 heads. i.e. \"THH, HTH, HHT, HHH\" - 4 events. \n                        \n                        P(A) = 4\/8 = 1\/2\n                    \nEvent B is getting 1st coin as tail. Favourable elements will be \"THH, TTH, TTT, THT\".\n\n                        P(B) = 4\/8 = 1\/2\n                        \nHere, A intersection B is having 2 heads when already 1st tail is present. i.e \"THH\"\n                         \n                       P(A \u2229 B) = 1\/8\n\nWe need to find the conditional probability of getting 2 heads givent that 1st is tail. ie. P(A\/B)\n\n                        P(A\/B) = P(A \u2229 B) \/ P(B)\n                               = (1\/8) \/ (1\/2) = 1\/4\n                       Hence,  P(A\/B) = 1\/4                          ","2785934c":"Naive Bayes classifiers are a collection of classification algorithms based on Bayes\u2019 Theorem. It is not a single algorithm but a family of algorithms where all of them share a common principle, i.e. every pair of features being classified is independent of each other. \n\nIn real time scenarios, all the features might not be indepedent in nature. There is some level of correlation between them.  But, in Naive Bayes' classifiers, we assume that there is no correlation between them and they are strongly independent. This is a Naive assumption as in real life scenarios, some sort of correlation is present. Inspite of the fact, Naive Bayes classifier works very well.    \n\n* Given a dataset (training data), we learn (build) a statistical model.  That model is classifier.  \n\n* Each point in the training data is feature vector in the below form.\n            \n                <label, feature 1, feature 2, feature 3, ...  feature n>\nHere label is the class and features are the dimensions of the data points.  \n\n* If we are given a data point without the label, then use classifier to decide on its label.  \n","66daaa1b":"### Decision Tree","9c472e59":"### Naive Bayes","87a228e2":"## Conditional probability","97286786":"#### Sex classification using toy dataset.  \n\nWe will be classifying sex (male\/female) using Naive Bayes classifier. \nDataset consist of 4 columns.\n                    \n                    <height, weight, foot_size, sex>.  \n\n<height, weight, foot_size> will be input feature vector, based on which we will be training the classifier to give the label <sex: male?female>","ed2844aa":"#### KNN","8f4f4116":"\n#### Types of Naive Bayes Classifier:\n\nBernoulli Naive Bayes:\nHere, predictors are boolean variables. The parameters that we use to predict the class variable take up only values yes or no, for example if a word occurs in the text or not.\n\nMultinomial Naive Bayes:\nThis is mostly used for document classification problem, i.e whether a document belongs to the category of sports, politics, technology etc. The features\/predictors used by the classifier are the frequency of the words present in the document.\n\nGaussian Naive Bayes:\nWhen the predictors take up a continuous value and are not discrete, we assume that these values are sampled from a gaussian distribution.\n\nSince the way the values are present in the dataset changes, the formula for conditional probability changes to,\n\n![image.png](attachment:image.png)\n\nConclusion:\nNaive Bayes algorithms are mostly used in sentiment analysis, spam filtering, recommendation systems etc. They are fast and easy to implement but their biggest disadvantage is that the requirement of predictors to be independent. In most of the real life cases, the predictors are dependent, this hinders the performance of the classifier.","ca47adb8":"## Classification in IRIS flower dataset.","449edae6":"##### Inference:\nI can conclude that the Cross validation and accuracy in all the 3 classifiers i.e DecisionTree, Gaussian NB and KNN was same and perfect.","cd4b7ad2":"## Bayes' Theorem","6ac2a006":"The dataset has 4 features, which needs to be classied into 3 classes. \n\nClasses i.e. label:\n\n    Iris Versicolor\n    Iris Virginica\n    Iris Setosa\n\nFeatures:\n\n    sepal length\n    sepal width\n    petal length\n    petal width\n","33dd6c4a":" You are given a dataset which consists of the petal and sepal measurements of iris flowers. The problem is to find the category of flower (Iris Setosa, Iris Virginica, Iris Versicolor) given the measurements of a particular flower. Create models using KNN and Na\u00efve Bayes and find the best of the two models used. Use python to prepare the data and create the models and cross-validate the dataset. Please make sure you also use appropriate performance measures, error values, bias and variance to come to conclusion of the best dataset","71277f32":"Cross validation score for KNN and Gaussian NB was almost same. ","39dd6b60":"To print dataset columns, we can use columns atribute","046d076f":"### Example 2.\n\nSample space of throwing 2 dices simultaneously. So, Total of 36 favourable elements in the space. \n\nConsider, event A of gettng sum of dices an even integer.  \n                            \n                                 P(A) = 18\/36\nEvent B of getting the number on first dice as 3. So, total of 6 elements.  \n                \n                                 P(B) = 6\/36\n                                 \nNow, we need to find the conditional probability of getting of dices as an even integer given that the first dice produces number 3. Sample space of this event will be \"3,1\", \"3,3\", \"3,5\".  \n\n                                 P(A \u2229 B) = 3\/36\n                    \n                                 P(A\/B) = P(A \u2229 B) \/ P(B)\n                                       = (3\/36) \/ (6\/36)\n                              ** Hence, P(A\/B) = 1\/2**","3b0efa7c":"**Definition: ** If E and F are two events associated with sample space of a random experiment, the conditional probability of event (E) given event (F) has already occured.    \n                                \n                                P(E\/F) = P(E \u2229 F) \/ P(F), P(F) != 0.","47399655":"### Data Modeling","7e93b5dc":"#### ******If you feel this notebook has helped you in any manner, please upvote it.  **","988f1aaa":"We need to check for null values in the dataset.","271d47fa":"Implement alogrithm without using any library and test it on above case.\n\nOur aim is to find out the posterior probability of male or female for the \ngiven test feature data.\n\n height   weight   foot_size\n   6      130          8\n   \nposterior probability (label (male\/female)) = P(label)*p(height\/label)*p(weight\/label)*p(foot_size\/label) \/                   \n                                                                        evidence\n                                                                        \nIn this case, evidenace will be same for both the labels hence, no need to calculating it.  \n\np(feature\/label) will be the gaussian probability density function. \n\nBelow will be the final algorithm. \n\nif numerator posterior probability (male) > numerator posterior probability (female)\n    then feature data will belong to male class\nelse\nfemale class","10d61b5c":"### Exploratory Data Analysis EDA","c396df1c":"**Conditional probability ** of event E given that event F has already occured is expressed as the ratio of number of elementary events favourable to intersection of events E and F to number of elementary events favourable to F. \n \nP(E\/F) = Number of elementary events favourable to (E \u2229 F) \/ Number of elementary events favourable to (F)\n\n        = n(E \u2229 F) \/ n(F)\n        \nDividing numerator and denominator by number of events in the sample space i.e. n(S)\n        \n        = n(E \u2229 F)\/n(S) \/ n(F)\/n(S)\n   \nHence, conditional probability will be  **P(E\/F) = P(E \u2229 F) \/ P(F)**,  \nIt is valid only if P(F) != 0 i.e F != 0.\n","bd822c94":"Example of Bayes' theorem\n\n    Mode of transport              Chances of getting late     \n    * Car                                  50%\n    * Bus                                  20%\n    * Train                                1%\n    \nConsider that Bob is late one day. His boss wishes to estimate the probability that he travelled to work that day by Car. i.e. P(Car\/Late) >  this we need to find. Posterior probablity.  \n\nProbability of getting late given that mode was car, bus or train. These will be likehood probability. \n                                        \n                                        P(late\/car) = 0.50\n                                        P(late\/bus) = 0.20\n                                        P(late\/train) = 0.01\n                                        \nBoss does not know which mode of transportation Bob usually uses, so he gives a prior probability of 1 in 3 to each of the three possibilities. Prior probabalities will be as shown below:-\n\n                                    P(Bus) == P(Car) == P(Train) = 1\/3 = 0.33                                            \n                                        \nUsing Bayes' theorem, Probability of travelling by car given that Bob is already late. \n                                         \n         P(car\/late) = P(car)*P(late\/car) \/ P(car)*P(late\/car)+ P(bus)*P(late\/bus) +P(train)*P(late\/train)\n         P(car\/late) = (0.33 * 0.50) \/ ((0.33 * 0.50)+(0.33 * 0.20)+(0.33 * 0.01))\n         P(car\/late) = 0.7042","397aca31":"Reference:\n\n* [https:\/\/archive.ics.uci.edu\/ml\/datasets\/Iris](http:\/\/)\n* http:\/\/syllabus.cs.manchester.ac.uk\/ugt\/2018\/COMP24111\/lectures.php\n\n","e87baf4a":"### Data Preprocessing\n","9c499b2e":"To give a statistical summary about the dataset, we can use **describe()","ac261978":"To get the unique species "}}