{"cell_type":{"d28551f4":"code","a9baa03f":"code","3e770276":"code","dcbe033b":"code","b5aadea5":"code","2a5c03c1":"code","6f7e49b8":"code","a0be7974":"code","d95debde":"code","ac51890d":"code","1096c8af":"code","3dcac8e5":"code","44795365":"markdown"},"source":{"d28551f4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport h5py\nimport gc\nfrom sklearn.model_selection import KFold, train_test_split\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a9baa03f":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n                start_mem - end_mem) \/ start_mem))\n    return df\n\n\ndef generate_features(data: pd.DataFrame,\n                      batch_sizes: list,\n                      window_sizes: list) -> pd.DataFrame:\n    \"\"\"\n    Generate features for https:\/\/www.kaggle.com\/c\/liverpool-ion-switching\n\n    Generate various aggregations over the data.\n\n    Args:\n        window_sizes: window sizes for rolling features\n        batch_sizes: batch sizes for which features are aggregated\n        data: original dataframe\n\n    Returns:\n        dataframe with generated features\n    \"\"\"\n    for batch_size in batch_sizes:\n        data['batch'] = ((data['time'] * 10_000) - 1) \/\/ batch_size\n        data['batch_index'] = ((data['time'] * 10_000) - 1) - (data['batch'] * batch_size)\n        data['batch_slices'] = data['batch_index'] \/\/ (batch_size \/ 10)\n        data['batch_slices2'] = data['batch'].astype(str).str.zfill(3) + '_' + data['batch_slices'].astype(\n            str).str.zfill(3)\n        data['batch_slices3'] = data['batch_index'] \/\/ (batch_size \/ 5)\n        data['batch_slices4'] = data['batch'].astype(str).str.zfill(3) + '_' + data['batch_slices3'].astype(\n            str).str.zfill(3)\n        data['batch_slices5'] = data['batch_index'] \/\/ (batch_size \/ 2)\n        data['batch_slices6'] = data['batch'].astype(str).str.zfill(3) + '_' + data['batch_slices5'].astype(\n            str).str.zfill(3)\n\n        for agg_feature in ['batch', 'batch_slices2', 'batch_slices4', 'batch_slices6']:\n            data[f\"min_{agg_feature}_{batch_size}\"] = data.groupby(agg_feature)['signal'].transform('min')\n            data[f\"max_{agg_feature}_{batch_size}\"] = data.groupby(agg_feature)['signal'].transform('max')\n            data[f\"std_{agg_feature}_{batch_size}\"] = data.groupby(agg_feature)['signal'].transform('std')\n            data[f\"mean_{agg_feature}_{batch_size}\"] = data.groupby(agg_feature)['signal'].transform('mean')\n            data[f\"median_{agg_feature}_{batch_size}\"] = data.groupby(agg_feature)['signal'].transform('median')\n\n            data[f\"mean_abs_chg_{agg_feature}_{batch_size}\"] = data.groupby(agg_feature)['signal'].apply(\n                lambda x: np.mean(np.abs(np.diff(x))))\n            data[f\"abs_max_{agg_feature}_{batch_size}\"] = data.groupby(agg_feature)['signal'].apply(\n                lambda x: np.max(np.abs(x)))\n            data[f\"abs_min_{agg_feature}_{batch_size}\"] = data.groupby(agg_feature)['signal'].apply(\n                lambda x: np.min(np.abs(x)))\n\n            data[f\"min_{agg_feature}_{batch_size}_diff\"] = data[f\"min_{agg_feature}_{batch_size}\"] - data['signal']\n            data[f\"max_{agg_feature}_{batch_size}_diff\"] = data[f\"max_{agg_feature}_{batch_size}\"] - data['signal']\n            data[f\"std_{agg_feature}_{batch_size}_diff\"] = data[f\"std_{agg_feature}_{batch_size}\"] - data['signal']\n            data[f\"mean_{agg_feature}_{batch_size}_diff\"] = data[f\"mean_{agg_feature}_{batch_size}\"] - data['signal']\n            data[f\"median_{agg_feature}_{batch_size}_diff\"] = data[f\"median_{agg_feature}_{batch_size}\"] - data[\n                'signal']\n\n            data[f\"range_{agg_feature}_{batch_size}\"] = data[f\"max_{agg_feature}_{batch_size}\"] - data[\n                f\"min_{agg_feature}_{batch_size}\"]\n            data[f\"maxtomin_{agg_feature}_{batch_size}\"] = data[f\"max_{agg_feature}_{batch_size}\"] \/ data[\n                f\"min_{agg_feature}_{batch_size}\"]\n            data[f\"abs_avg_{agg_feature}_{batch_size}\"] = (data[f\"abs_min_{agg_feature}_{batch_size}\"] + data[\n                f\"abs_max_{agg_feature}_{batch_size}\"]) \/ 2\n\n            data[f'signal_shift+1_{agg_feature}_{batch_size}'] = data.groupby([agg_feature]).shift(1)['signal']\n            data[f'signal_shift-1_{agg_feature}_{batch_size}'] = data.groupby([agg_feature]).shift(-1)['signal']\n            data[f'signal_shift+2_{agg_feature}_{batch_size}'] = data.groupby([agg_feature]).shift(2)['signal']\n            data[f'signal_shift-2_{agg_feature}_{batch_size}'] = data.groupby([agg_feature]).shift(-2)['signal']\n\n            data[f\"signal_shift+1_{agg_feature}_{batch_size}_diff\"] = data[f\"signal_shift+1_{agg_feature}_{batch_size}\"] - data['signal']\n            data[f\"signal_shift-1_{agg_feature}_{batch_size}_diff\"] = data[f\"signal_shift-1_{agg_feature}_{batch_size}\"] - data['signal']\n            data[f\"signal_shift+2_{agg_feature}_{batch_size}_diff\"] = data[f\"signal_shift+2_{agg_feature}_{batch_size}\"] - data['signal']\n            data[f\"signal_shift-2_{agg_feature}_{batch_size}_diff\"] = data[f\"signal_shift-2_{agg_feature}_{batch_size}\"] - data['signal']\n\n        for window in window_sizes:\n            window = min(batch_size, window)\n\n            data[\"rolling_mean_\" + str(window) + '_batch_' + str(batch_size)] = \\\n                data.groupby('batch')['signal'].rolling(window=window).mean().reset_index()['signal']\n            data[\"rolling_std_\" + str(window) + '_batch_' + str(batch_size)] = \\\n                data.groupby('batch')['signal'].rolling(window=window).std().reset_index()['signal']\n            data[\"rolling_min_\" + str(window) + '_batch_' + str(batch_size)] = \\\n                data.groupby('batch')['signal'].rolling(window=window).min().reset_index()['signal']\n            data[\"rolling_max_\" + str(window) + '_batch_' + str(batch_size)] = \\\n                data.groupby('batch')['signal'].rolling(window=window).max().reset_index()['signal']\n\n            data[f'exp_Moving__{window}_{batch_size}'] = data.groupby('batch')['signal'].apply(\n                lambda x: x.ewm(alpha=0.5, adjust=False).mean())\n        data = reduce_mem_usage(data)\n    data.fillna(0, inplace=True)\n\n    return data\n\n\ndef read_data(path: str = ''):\n    \"\"\"\n    Read train, test data\n\n    Args:\n        path: path to the data\n\n    Returns:\n        two dataframes\n    \"\"\"\n    train_df = pd.read_csv(f'{path}\/train.csv')\n    test_df = pd.read_csv(f'{path}\/test.csv')\n    return train_df, test_df","3e770276":"directory = '\/kaggle\/input\/liverpool-ion-switching\/'\ntrain,test = read_data(directory)\nsample_submission = pd.read_csv(f'{directory}\/sample_submission.csv') ","dcbe033b":"batch_sizes = [25000]\nwindow_sizes = [10, 25, 50, 5000, 10000]\n\ngenerated_train = generate_features(train, batch_sizes, window_sizes)\n\ndel train\ngc.collect()","b5aadea5":"feats = generated_train.columns\nprint(feats)\nfeats = np.delete(feats,[2,3,4,5,6,7,8,9,10]) # Delete Target from features\ntarget = ['open_channels']","2a5c03c1":"generated_test = generate_features(test, batch_sizes, window_sizes)\n\ndel test\ngc.collect()","6f7e49b8":"import lightgbm as lgb\nparams = {'learning_rate': 0.05, 'max_depth': -1, 'num_leaves':200, 'metric': 'rmse', 'random_state': 42, 'n_jobs':-1, 'sample_fraction':0.33} ","a0be7974":"# Thanks to https:\/\/www.kaggle.com\/siavrez\/simple-eda-model\nfrom sklearn.metrics import confusion_matrix, f1_score, mean_absolute_error, make_scorer\ndef MacroF1Metric(preds, dtrain):\n    labels = dtrain.get_label()\n    preds = np.round(np.clip(preds, 0, 10)).astype(int)\n    score = f1_score(labels, preds, average = 'macro')\n    return ('MacroF1Metric', score, True)","d95debde":"x1, x2, y1, y2 = train_test_split(generated_train[feats], generated_train[target], test_size=0.3, random_state=42)\nmodel = lgb.train(params, lgb.Dataset(x1, y1), 2000,  lgb.Dataset(x2, y2), verbose_eval=100, early_stopping_rounds=100, feval=MacroF1Metric)\ndel x1, x2, y1, y2\ngc.collect()","ac51890d":"preds_ = model.predict(generated_test[feats], num_iteration=model.best_iteration)","1096c8af":"sample_submission['open_channels'] = np.round(np.clip(preds_, 0, 10)).astype(int)\nsample_submission.to_csv('submission.csv', index=False, float_format='%.4f')\ndisplay(sample_submission.head())","3dcac8e5":"fig =  plt.figure(figsize = (25,25))\naxes = fig.add_subplot(111)\nlgb.plot_importance(model,ax = axes,height = 0.5)\nplt.show();plt.close()","44795365":"# Ion 550 Features (LightGBM)\n\n### Credit for the Feature Engineering code goes to [artgor](https:\/\/www.kaggle.com\/artgor). I took his FE template and implemented a simple Light GBM model to see how these aggregated features perform. Note that I had to reduce the number of Batch Size and Window Size options in order to avoid memory issues, so it isn't the full 550 feature set. I plan on trying out different Batch and Window Sizes to see if I can't land on ones that lead to best performance.\n\n* ### For access to the original 550 feature code, see the dataset in this link: https:\/\/www.kaggle.com\/artgor\/ion-features"}}