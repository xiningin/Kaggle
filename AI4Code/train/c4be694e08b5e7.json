{"cell_type":{"78d65218":"code","672c1ba9":"code","074650fe":"code","aa00c496":"code","ff7bf6e5":"code","4bc72a0f":"code","04981bf2":"code","a4694232":"code","2dc5a40b":"code","c1721ccc":"code","063a9574":"code","31e8970e":"code","beb017e5":"code","0e25e697":"code","1af6afb1":"code","aae879b1":"code","04c93bea":"code","a2833c53":"code","d23e5a58":"code","77924c46":"code","34741f78":"code","abad7cd0":"code","0b801546":"code","a3e1b8e5":"code","cc1cf1ad":"code","64f4816c":"code","ff238757":"code","baf91c30":"code","bf1d4c5d":"code","becb45d8":"code","ae8c8c27":"code","842e64a3":"code","63f7c148":"code","da66c4cc":"code","67e31047":"code","8fe7b228":"code","bba00adf":"code","9521fcb7":"code","daa5efc9":"code","1dc77e3c":"code","41c6c967":"code","a16a4562":"code","fbde200f":"code","08224e1a":"markdown","5288fb3b":"markdown","e22fc993":"markdown","3c97d894":"markdown","2d20ff2e":"markdown","1b99a5b4":"markdown","854b2d13":"markdown","453e88d5":"markdown","f8a8d9e8":"markdown","71939519":"markdown","efcfe15b":"markdown","99ec0e79":"markdown","ce2f2063":"markdown","a6274245":"markdown","c3cf31e9":"markdown","82852bac":"markdown","5aa99800":"markdown","2c9ffee7":"markdown","43a871e0":"markdown","dc67468b":"markdown","fa81b228":"markdown","4e19c7be":"markdown","860272d0":"markdown","f67f9265":"markdown","071d3df3":"markdown","ba9ca6b9":"markdown","bbc196e6":"markdown","984e18d2":"markdown","3c2cee35":"markdown"},"source":{"78d65218":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","672c1ba9":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","074650fe":"pd.options.display.max_columns=100\npd.options.display.max_rows=100\npd.set_option('display.float_format','{:.2f}'.format)","aa00c496":"data = pd.read_csv(\"\/kaggle\/input\/company-bankruptcy-prediction\/data.csv\")\nprint(data.shape)\ndata.head(3)","ff7bf6e5":"data.info()","4bc72a0f":"data.describe()","04981bf2":"print(\"Column names before renaming\",\"\\n\", data.columns[:5],\"\\n\")\ndata.columns = data.columns.str.strip()\ndata.columns = data.columns.str.replace(\" \" ,\"_\")\ndata.rename(columns = {'Bankrupt?' :'Bankrupt' },inplace=True)\nprint(\"Column names after renaming\",\"\\n\",data.columns[:5])","a4694232":"data.drop(['Net_Income_Flag'],axis=1,inplace=True) ## drop constant columns","2dc5a40b":"data.duplicated(keep=False).sum()  ## no duplicates","c1721ccc":"data.isnull().values.sum() # check missing values","063a9574":"df1 = pd.DataFrame(data.Bankrupt.value_counts())\ndf2 = pd.DataFrame(100*data.Bankrupt.value_counts(normalize=True).astype(float))\ntab = df1.merge(df2,left_index=True,right_index=True).rename(columns = {\"Bankrupt_x\" : \"Count\" , \"Bankrupt_y\" : \"Percentage\"})\nprint(tab)","31e8970e":"plt.pie(tab['Count'], labels= [0,1])","beb017e5":"## fn to separate only-fractional & other columns\ndef get_fraction_valued_columns(df):\n    my_columns  = []\n    for col in df.columns:\n        if (data[col].max()<=1) & (data[col].min() >= 0):\n            my_columns.append(col)\n    return(my_columns)\n\nfractional_columns = get_fraction_valued_columns(df=data.drop(['Bankrupt'],axis=1))\nnon_fraction_columns = data.drop(['Bankrupt'],axis=1).columns.difference(fractional_columns)\nprint(\"# Fraction-only Columns\",len(fractional_columns),\"\\t\",\"# Other than Fraction-only Columns\", len(non_fraction_columns))","0e25e697":"data[non_fraction_columns].hist(figsize= (20,20),sharex=True,layout= (6,4))\nplt.show()","1af6afb1":"data[non_fraction_columns].boxplot(vert=False,figsize= (15,10))\nplt.subplots_adjust(left=0.25)\nplt.show()","aae879b1":"## Outlier handling techniques\nlog_transformed_cols = []\nfor col in data[non_fraction_columns].columns:\n    if (data[col].quantile(1) >= 100* data[col].quantile(0.99)) |  (sum(data[col] > data[col].quantile(0.99)) <= 10):\n        data[col] = np.log1p(data[col])\n        log_transformed_cols.append(col)\n        \n## Change names of log transformed column\nlog_names = \"log_\" + data[log_transformed_cols].columns\ndata.rename(columns={data[log_transformed_cols].columns[i]: log_names[i] for i in range(len(log_names))}, inplace = True)\n#data.columns","04c93bea":"print(\"The following features are log transformed after they fulfill outlier detection condition.\",\"\\n\\n\",log_transformed_cols)\ndata[log_names].boxplot(vert=False,figsize= (15,10))\nplt.subplots_adjust(left=0.25)\nplt.title(\"Boxplot of Outlier infected features after log transformation\")\nplt.show()","a2833c53":"X = data.drop([\"Bankrupt\"],axis=1)\ny = data.Bankrupt","d23e5a58":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectKBest, chi2 , mutual_info_classif \nfrom imblearn.over_sampling import SMOTE ","77924c46":"X_scaled = pd.DataFrame(StandardScaler(copy=False).fit_transform(X))\nX_scaled.columns = X.columns","34741f78":"mutual_info = mutual_info_classif(X= X_scaled,y= y)  # get mutual info all predictors\npruned_features = X.columns[np.where(mutual_info>0)]  # retain features only with mi >0\nX_scaled_pruned = X_scaled[pruned_features]","abad7cd0":"sm = SMOTE(random_state=123)\nX_sm , y_sm = sm.fit_resample(X_scaled_pruned,y)\n\nprint(f'''Shape of X before SMOTE:{X_scaled_pruned.shape}\nShape of X after SMOTE:{X_sm.shape}''',\"\\n\\n\")\n\nprint(f'''Target Class distributuion before SMOTE:\\n{y.value_counts(normalize=True)}\nTarget Class distributuion after SMOTE :\\n{y_sm.value_counts(normalize=True)}''')","0b801546":"from sklearn.model_selection import train_test_split,StratifiedKFold\nfrom sklearn import metrics","a3e1b8e5":"x_train , x_test , y_train ,y_test = train_test_split(X_sm,y_sm,test_size= 0.33)","cc1cf1ad":"from sklearn.linear_model import LogisticRegression\nlr_fit = LogisticRegression(penalty='elasticnet',solver='saga',l1_ratio=0.5,max_iter=10000).fit(x_train,y_train)","64f4816c":"lr_pred = lr_fit.predict_proba(x_test)  ## predicted probabilities\nlr_pred = lr_pred[:,1]  #prob(Bankrupt=0)\n\nlr_fpr, lr_tpr, _ = metrics.roc_curve(y_test,  lr_pred)  #fpr, tpr for AUC\nlr_auc = metrics.roc_auc_score(y_test,lr_pred)   #AUC\n\n## in order to collate FPR, TPR and AUC create a dataframe  \nresult_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\nresult_table.set_index('classifiers', inplace=True)\nresult_table = result_table.append({'classifiers':\"Logistic Regression\",\n                                        'fpr':lr_fpr, \n                                        'tpr':lr_tpr, \n                                        'auc':lr_auc}, ignore_index=True)\n\nprint('Logistic Regression AUC=%.3f' % (lr_auc))","ff238757":"plt.plot([0,1], [0,1], color='black', linestyle='--')\nplt.plot(lr_fpr,lr_tpr ,label=\"{}, AUC={:.3f}\".format(result_table.loc[0,'classifiers'], result_table.loc[0]['auc']))\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.xlabel('False Positive Rate(1-Specificity)')\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':10}, loc='lower right')\nplt.show()","baf91c30":"yhat = lr_fit.predict(x_test,)\nlabel = ['Fin.Stable', 'Fin.Unstable']\nreport = metrics.classification_report(y_test, yhat,target_names=label)\n\nprint(report)\nprint(\"Confusion Matrix :\", \"\\n\" ,metrics.confusion_matrix(y_test,yhat))","bf1d4c5d":"from sklearn import svm","becb45d8":"svc_fit = svm.SVC(C=1,kernel= 'rbf',degree=3,probability=True).fit(x_train,y_train)","ae8c8c27":"svc_pred = svc_fit.predict_proba(x_test)\nsvc_pred = svc_pred[:,1]\n\nsvc_fpr, svc_tpr, _ = metrics.roc_curve(y_test,  svc_pred)\nsvc_auc = metrics.roc_auc_score(y_test,svc_pred)\n\nresult_table = result_table.append({'classifiers':\"Support Vector Classifier\",\n                                        'fpr':svc_fpr, \n                                        'tpr':svc_tpr, \n                                        'auc':svc_auc}, ignore_index=True)\n\nprint('Support Vector Classifier AUC =%.3f' % (svc_auc))","842e64a3":"plt.plot([0,1], [0,1], color='black', linestyle='--')\nplt.plot(lr_fpr,lr_tpr ,label=\"{}, AUC={:.3f}\".format(result_table.loc[0,'classifiers'], result_table.loc[0]['auc']))\nplt.plot(svc_fpr,svc_tpr ,label=\"{}, AUC={:.3f}\".format(result_table.loc[1,'classifiers'], result_table.loc[1]['auc']))\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.xlabel('False Positive Rate(1-Specificity)')\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':10}, loc='lower right')\nplt.show()","63f7c148":"yhat = svc_fit.predict(x_test)\nlabel = ['Fin.Stable', 'Fin.Unstable']\nreport = metrics.classification_report(y_test, yhat,target_names=label)\n\nprint(report)\nprint(\"Confusion Matrix :\", \"\\n\" ,metrics.confusion_matrix(y_test,yhat))","da66c4cc":"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,AdaBoostClassifier","67e31047":"gbc = GradientBoostingClassifier(learning_rate=0.1,max_depth=2,random_state=123,subsample=0.8,n_estimators=600)\ngbc_fit = gbc.fit(x_train,y_train)","8fe7b228":"gbc_pred = gbc_fit.predict_proba(x_test)\ngbc_pred = gbc_pred[:,1]\n\ngbc_fpr, gbc_tpr, _ = metrics.roc_curve(y_test,  gbc_pred)\ngbc_auc = metrics.roc_auc_score(y_test,gbc_pred)\n\nresult_table = result_table.append({'classifiers':\"Gradient Boosted Classifier\",\n                                    'fpr':gbc_fpr, \n                                    'tpr':gbc_tpr, \n                                    'auc':gbc_auc}, ignore_index=True)\n\n\nprint('Gradient Boosted Classifier AUC=%.3f' % (gbc_auc))","bba00adf":"plt.plot([0,1], [0,1], color='black', linestyle='--')\nplt.plot(lr_fpr,lr_tpr ,label=\"{}, AUC={:.3f}\".format(result_table.loc[0,'classifiers'], result_table.loc[0]['auc']))\nplt.plot(svc_fpr,svc_tpr ,label=\"{}, AUC={:.3f}\".format(result_table.loc[1,'classifiers'], result_table.loc[1]['auc']))\nplt.plot(gbc_fpr,gbc_tpr ,label=\"{}, AUC={:.3f}\".format(result_table.loc[2,'classifiers'], result_table.loc[2]['auc']))\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.xlabel('False Positive Rate(1-Specificity)')\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':10}, loc='lower right')\nplt.show()","9521fcb7":"yhat = gbc_fit.predict(x_test)\nlabel = ['Fin.Stable', 'Fin.Unstable']\nreport = metrics.classification_report(y_test, yhat,target_names=label)\n\nprint(report)\nprint(\"Confusion Matrix :\", \"\\n\" ,metrics.confusion_matrix(y_test,yhat))","daa5efc9":"abc = AdaBoostClassifier(n_estimators=500,random_state=123,learning_rate=0.3)\nabc_fit = abc.fit(x_train,y_train)","1dc77e3c":"abc_pred = abc_fit.predict_proba(x_test)\nabc_pred = abc_pred[:,1]\n\nabc_fpr, abc_tpr, _ = metrics.roc_curve(y_test,  abc_pred)\nabc_auc = metrics.roc_auc_score(y_test,abc_pred)\n\nresult_table = result_table.append({'classifiers':\"Ada Boosted Classifier\",\n                                    'fpr':abc_fpr, \n                                    'tpr':abc_tpr, \n                                    'auc':abc_auc}, ignore_index=True)\n\nprint('Ada Boosted Classifier AUC=%.3f' % (abc_auc))","41c6c967":"plt.plot([0,1], [0,1], color='black', linestyle='--')\nplt.plot(lr_fpr,lr_tpr ,label=\"{}, AUC={:.3f}\".format(result_table.loc[0,'classifiers'], result_table.loc[0]['auc']))\nplt.plot(svc_fpr,svc_tpr ,label=\"{}, AUC={:.3f}\".format(result_table.loc[1,'classifiers'], result_table.loc[1]['auc']))\nplt.plot(gbc_fpr,gbc_tpr ,label=\"{}, AUC={:.3f}\".format(result_table.loc[2,'classifiers'], result_table.loc[2]['auc']))\nplt.plot(abc_fpr,abc_tpr ,label=\"{}, AUC={:.3f}\".format(result_table.loc[3,'classifiers'], result_table.loc[3]['auc']))\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.xlabel('False Positive Rate(1-Specificity)')\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':8}, loc='lower right')\nplt.show()","a16a4562":"yhat = abc_fit.predict(x_test)\nlabel = ['Fin.Stable', 'Fin.Unstable']\nreport = metrics.classification_report(y_test, yhat,target_names=label)\n\nprint(report)\nprint(\"Confusion Matrix :\", \"\\n\" ,metrics.confusion_matrix(y_test,yhat))","fbde200f":"var_imp = pd.concat([pd.DataFrame(X_scaled_pruned.columns),pd.DataFrame(np.transpose(gbc_fit.feature_importances_))],axis=1)\nvar_imp.columns = ['Features', 'Importance']\nvar_imp = var_imp.sort_values('Importance',ascending = False)\nplt.figure(figsize=(20,20))\nsns.barplot(y= 'Features',x= 'Importance',data= var_imp,orient='h')","08224e1a":"### Performance Metrics (Cutoff based)","5288fb3b":"## 1.3 Target class Imballance : SMOTE\n- only 3.2% of the companies has Bankrupted in the dataset,making it imballanced target class probelm.\n- Hence positive target class( Bankrupt=1) is under-represented.This could be challenging as lack of positive class in the train data may lead machine learning model to have poor performance in terms of detecting positive class in the unseen data.\n- SMOTE(Synthetic Minority Oversampling Techniwque) proposed by Chawla et al 2002, is a well applied technique to handle such scenerio.\n- SMOTE actually creates as many synthetic examples for minority class as are requirred so that finally two target class are well represented. It does so by synthesising samples that are close to the feature space ,for the minority target class.\nMore about [SMOTE](http:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/)","e22fc993":"### 1.2 Info , describe\n- All features are numeric\n- one features has 0 variance that is constant throughout\n- major features are in the range of 0-1\n- there are outlier infected features","3c97d894":"## 2.1 Logistic Regression\n- Will be using logistic regression as our benchmark model.\n- Combination of L1 & L2 regularization(Elasticnet)is applied.\n- With and Without Using SMOTE","2d20ff2e":"### 3.1 Target Class distribution\n- Target is heavily imballanced \n- Bankruptcy Rate is around 3.2%","1b99a5b4":"###  Performance metrics : (Cutoff Based )","854b2d13":"### Other than fration-only features : Histogram","453e88d5":"## 2.2 Support Vector Classifier","f8a8d9e8":"### Feature Importance","71939519":"## Final Comment : \n- Best Model for the data: Gradient Boosted Tree\n- GBC predicts with following score on test dataset\n  - 96% accuracy \n  - 99.5% AUC\n  - 97% f1 score  ","efcfe15b":"###  Performance metrics : (Cutoff based )","99ec0e79":"###  Performance metrics : (Cutoff independent )","ce2f2063":"###  Performance metrics : (Cutoff Independent )","a6274245":"###  Performance metrics : (Cutoff independent )","c3cf31e9":"###  Performance metrics : (Cutoff Independent )","82852bac":"## 2.3.1 Gradient Boosted Classifiers","5aa99800":"### 3.2  Outliers Handling\n- First separate all **94** features into two groups\n    - _fraction-only_ features (i.e. features having values in [0,1])\n    - _other than fraction-only_ features\n\n- **70** features are _fraction-only_ features where as **24** are _other than fraction-only_ features.\n\n- Outliers are mainly **present** in these 24 _\"other than fraction-only\"_ features\n\n- To explore the outliers nature, distribution of these 24 features, are obtained using :\n    - Histogram\n    - Boxplot","2c9ffee7":"### 2. Sanity Check\n#### Column Names:\n- Remove leading whitespaces from Column names\n- Replace \" \" with \"_\" in columnnames\n- Rename Target column\n\n#### Check constant Columns:\n- ***Net Income Flag*** is constant, hence drop\n- Rename ***Bankrupt?  -> Bankrupt***\n\n#### Check Duplicates\n - No duplicated values\n\n#### Check Missing Values\n- No missing values","43a871e0":"## Tree Based Ensamble Models\n- Gradient Boosted Classifier\n- Ada Boost Classifiers","dc67468b":" ### Other than fration-only features :  Boxplot","fa81b228":"# Workflow","4e19c7be":"### 1.1 Read - head  - shape\n- We have total 6819 records from 96 columns in the dataset,of which:\n    - 1 Target (Bankrupt?)\n    - 95 features","860272d0":"##  1.2 Feature Selection\n- In order to select features to be fed into the predictive model mutual information is useed.\n- Features with positive mutual information are retained for final model.","f67f9265":"# B. Modelling","071d3df3":"##  2.3.2 AdaBoost Classifier","ba9ca6b9":"###  Performance metrics : (Cutoff based )","bbc196e6":"### __Observations__\n- Major values are concentrated around starting ranges yet there are very high valued records.\n- Some features show outliers in top 1% values only.Few of such features are:\n    - Total_debt\/Total_net_worth \n    - Revenue_per_person\n    - Net_Value_Growth_Rate\n    - Revenue_Per_Share etc\n\n2. There are some features that have significant number of higher values, like:\n    - Current_Asset_Turnover_Rate\n    - Cash_Turnover_Rate\n\n- **These features may potentially seem to be outliers but not be practically so.Hence outliers here need to be handled more cautiously,simply eliminating records lying above some cutoff cant be applied for all throughout**. \n\n- We Need to have a rule: \n\n#### **Rule** :\n- Only __other than fraction-only__ features are considered for outlier inspection\n- Among them those meeting following conditions I decide to term them as outliers infected:\n   - 100th percentile value is atleast 100 times greater than 99th percentile.\n   - There are 10 or less records for 100th percentile.\n- For the features satisfying above outlier condition we replace them  as x-> log(1+x)\n- __N.B: This is just a rule of thumb I made based on the experience while playing with the data.__ \n","984e18d2":"## 1.1 Feature Scaling\n- Scale all features in order to have zero mean and unit variance","3c2cee35":"# A. Pre-processing "}}