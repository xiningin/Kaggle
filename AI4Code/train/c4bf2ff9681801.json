{"cell_type":{"67e8465d":"code","99acce85":"code","01af6c6c":"code","742072ed":"code","d47f4515":"code","3e3115aa":"code","28ff6e57":"code","0e421af4":"code","94a1c8e7":"code","50a63f4b":"code","e6cc111f":"code","4f7f2463":"code","ec52bd2f":"code","74eeb9bf":"code","a1fd5853":"code","d46660b6":"code","61c92eca":"code","683b52ab":"code","dc3cedd4":"code","3f48d5ce":"code","aea01802":"code","9f0f7bef":"code","fd84faa3":"code","6dd818df":"code","e8a04a1f":"code","52698033":"code","63ba6962":"code","8e1a8590":"code","51207c83":"code","dc4f51b3":"code","50428933":"code","2fad73df":"code","4fd5fe68":"code","81754a56":"markdown","e0419d8d":"markdown"},"source":{"67e8465d":"import pandas as pd\nimport numpy as np","99acce85":"df = pd.read_csv(\"..\/input\/cartrade-used-cars-data\/CarTrade.csv\")\ndf.head(10)","01af6c6c":"df.dtypes","742072ed":"issue_df = df[df.isnull().any(axis=1)]\nissue_df.head()","d47f4515":"df['Car_Price'] = df['Car_Price'].apply(lambda x: x.replace(',', ''))\ndf['Car_Price'] = df['Car_Price'].astype('int64')\ndf.head()","3e3115aa":"df['Car_Kms'] = df['Car_Kms'].apply(lambda x: x.split(' ')[0].replace(',', ''))\ndf.head(10)","28ff6e57":"df['Car_Kms'] = df['Car_Kms'].astype('int64')","0e421af4":"df.dtypes","94a1c8e7":"df.describe()","50a63f4b":"#Just checking to see if the right result is obtained after cleaning up Car_Model column\ncars = df['Car_Model'].unique()\ncars[0].strip('[]').strip('\\'\\' ')","e6cc111f":"df['Car_Model'] = df['Car_Model'].apply(lambda x: x.strip('[]').strip('\\'\\' '))\ndf.head(10)","4f7f2463":"df['Car_Company'] = df['Car_Model'].apply(lambda x: x.split(' ')[0])\n\ndf['Car_Company'] = df['Car_Company'].replace({'Land': 'Land Rover', 'MG': 'MG Hector'})","ec52bd2f":"df.head()","74eeb9bf":"#Cleaning up location column for no reason\ndf['Car_Location'] = df['Car_Location'].apply(lambda x: x.split(' ')[0].strip())\n\ndf.head()","a1fd5853":"df['Car_Location'] = df['Car_Location'].apply(lambda x: x.replace('Navi', 'Navi Mumbai'))","d46660b6":"df['Car_Location'].unique()","61c92eca":"df['model'] = df['Car_Model'].apply(lambda x: x.split(' '))\ndf['model'] = df['model'].apply(lambda x: ' '.join(x[1:2]))\ndf.head()","683b52ab":"df.loc[df['Car_Company'] == 'Land Rover', 'model'] = df['Car_Model'].apply(lambda x: x.split(' '))\ndf.loc[df['Car_Company'] == 'Land Rover', 'model'] = df['model'].apply(lambda x: ' '.join(x[2:5]))\ndf.head()","dc3cedd4":"#Just curious\nhighest_df = df.groupby('model').mean().sort_values(by='Car_Price', ascending=False)\nhighest_df","3f48d5ce":"df.groupby('model').mean().sort_values(by='Car_Year', ascending=False).tail(25)","aea01802":"df.head()","9f0f7bef":"new_df = df.drop_duplicates(keep='first')\nnew_df.shape","fd84faa3":"from category_encoders import TargetEncoder\nfinal_df = df.copy()","6dd818df":"tencoder = TargetEncoder()\nfinal_df['Car_Model'] = tencoder.fit_transform(final_df['Car_Model'], final_df['Car_Price'])\nfinal_df.head()","e8a04a1f":"def thedummies(col, dff):\n    temp = pd.get_dummies(dff[col], drop_first=True)\n    dff = dff.drop(col, axis=1)\n    dff = pd.concat([dff, temp], axis=1)\n    return dff","52698033":"final_df = thedummies('Car_Fuel', final_df)\nfinal_df = thedummies('Car_Company', final_df)\nfinal_df.head()","63ba6962":"final_df = final_df.drop(['Car_Location', 'model'], axis=1)\nfinal_df.head()","8e1a8590":"from sklearn.linear_model import LinearRegression\nimport sklearn.preprocessing\nfrom sklearn.model_selection import train_test_split","51207c83":"X = final_df.drop('Car_Price', axis=1).values\nX = sklearn.preprocessing.scale(X)\ny = final_df['Car_Price'].values","dc4f51b3":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7)","50428933":"clf = LinearRegression()\nclf.fit(X_train, y_train)","2fad73df":"clf.score(X_test, y_test)","4fd5fe68":"for xtest, ytest in zip(X_test, y_test):\n    print(f'Actual price: {ytest} Forecast price: {clf.predict([xtest])[0]}')","81754a56":"### Conclusion: Forecasts look relatively accurate but that might be because of the huge number of duplicate values. Dropping them would make the dataset way too small so I did not do that.","e0419d8d":"# This might be a dataset that is almost completely filled with duplicates"}}