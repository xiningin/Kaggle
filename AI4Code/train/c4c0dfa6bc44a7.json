{"cell_type":{"90811455":"code","c4b76128":"code","0b5c9e11":"code","af89447c":"code","10b39cea":"code","2f8455f2":"code","fbcaf33b":"code","f978d5af":"code","a4a80d9b":"code","30326443":"code","265f2020":"code","0724ae91":"code","d86852fc":"code","d4c32a29":"code","effffb27":"code","98a86fcc":"code","5e19af79":"code","ca9eb7b2":"code","755179b9":"code","3b5150fb":"code","a44bc6b1":"code","1dc9157c":"code","dd3189d8":"code","8e234dbe":"code","513d59ab":"code","90d27576":"code","107afeb2":"code","b66dc712":"code","cfa8506c":"code","41a6dc58":"code","46745807":"code","98556e7d":"code","3bb3cbd1":"code","38845340":"code","764516b2":"code","8a731ee3":"code","e2401c6d":"code","e02e67a0":"code","100ce54a":"code","304a3f10":"code","27953678":"code","1cae4f26":"code","4d1cb2b0":"code","7d3cfd37":"code","116d0d78":"code","92159181":"code","ad09c664":"code","37b89b6b":"code","01d2243b":"code","08495b21":"code","f13549cc":"code","4d51c332":"code","82964c5e":"code","6b73f62b":"code","08669777":"code","20adca84":"code","872ad47d":"code","881571b5":"code","a6eccd11":"code","b4b527da":"code","5c72a12b":"code","32a9279f":"code","7b2263f8":"code","c46c3dbc":"code","b8ee7826":"code","57e43f9b":"code","21d740f3":"code","8da69749":"code","093a7413":"code","37181766":"code","70790e52":"code","6efdd2a3":"code","1dcd6c83":"markdown","b53034d9":"markdown","fdcbfd4e":"markdown","703c5363":"markdown","ae720bb7":"markdown","e754f4a5":"markdown","9990dc99":"markdown","a1aa2ebe":"markdown","4c127e3a":"markdown","a02b5d18":"markdown","1491115b":"markdown","8d12673f":"markdown","ad6ed6b0":"markdown","5f4075a9":"markdown","3d89f898":"markdown","333ba0db":"markdown","21c2c961":"markdown","478807a8":"markdown","b700872c":"markdown","ef307f48":"markdown","9c6e630f":"markdown","d85a035d":"markdown","80b4e7f6":"markdown","bd2f5631":"markdown","4f199ce9":"markdown","3ac84887":"markdown","0c07fbb4":"markdown","2284b3af":"markdown","f6e12354":"markdown","e2b8cb50":"markdown","c88f1c02":"markdown","5a37e230":"markdown","ec825570":"markdown","7a076058":"markdown"},"source":{"90811455":"import os\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout","c4b76128":"df_accepted = pd.read_csv(\"\/kaggle\/input\/lending-club\/accepted_2007_to_2018q4.csv\/accepted_2007_to_2018Q4.csv\")\n\nprint(df_accepted.shape)","0b5c9e11":"df_accepted.loan_status.value_counts()","af89447c":"df_accepted = df_accepted[df_accepted.loan_status.isin(['Fully Paid', 'Charged Off'])]\n\nprint(df_accepted.shape)","10b39cea":"sns.countplot(x=\"loan_status\", data=df_accepted)","2f8455f2":"df_accepted.isnull().sum().sort_values(ascending=False)","fbcaf33b":"df_accepted.isnull().sum().sort_values(ascending=False).head(50)","f978d5af":"cols_to_drop = df_accepted.isnull().sum().sort_values(ascending=False).head(50).index\ndf_accepted = df_accepted.drop(columns=cols_to_drop)\ndf_accepted.shape","a4a80d9b":"df_accepted.isnull().sum().sort_values(ascending=False).head(50)","30326443":"cols_to_drop = df_accepted.isnull().sum().sort_values(ascending=False).head(45).index\ndf_accepted = df_accepted.drop(columns=cols_to_drop)\ndf_accepted.shape","265f2020":"nulls = df_accepted.isnull().sum().sort_values(ascending=False)\nnulls_percentage = (nulls\/df_accepted.shape[0]*100)\nnulls_percentage.head(15)","0724ae91":"df_accepted = df_accepted.dropna()\ndf_accepted.shape","d86852fc":"df_accepted.info()","d4c32a29":"plt.figure(figsize=(15,5))\nsns.histplot(df_accepted.loan_amnt, kde=False)","effffb27":"plt.figure(figsize=(12,10))\nmask = np.zeros_like(df_accepted.corr())\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(df_accepted.corr(), cmap='viridis', mask=mask, annot=False, square=True)","98a86fcc":"print(df_accepted.out_prncp.value_counts())\nprint(df_accepted.out_prncp_inv.value_counts())\nprint(df_accepted.policy_code.value_counts())","5e19af79":"cols_to_drop = ['out_prncp', 'out_prncp_inv', 'policy_code']\ndf_accepted = df_accepted.drop(columns=cols_to_drop)\ndf_accepted.shape","ca9eb7b2":"plt.figure(figsize=(12,10))\nmask = np.zeros_like(df_accepted.corr())\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(df_accepted.corr(), cmap='viridis', mask=mask, annot=False, square=True)","755179b9":"# unstacking correlation to check highest correlation pairs\n\nc = df_accepted.corr().abs()\nunstack_corr = c.unstack()\npairs = unstack_corr.sort_values(ascending=False)\n\ncorr_pairs = pd.DataFrame(pairs).drop_duplicates().reset_index()\ncorr_pairs.columns = ['feature1', 'feature2', 'corr']\n\n\n# check feature pairs above 0.8 in correlation\n\ncond1 = (corr_pairs.feature1 != corr_pairs.feature2)\ncond2 = (corr_pairs['corr']>0.80)\n\ncorr_pairs[cond1 & cond2]","3b5150fb":"sns.scatterplot(x=df_accepted.fico_range_low, y=df_accepted.fico_range_high)","a44bc6b1":"sns.scatterplot(x=df_accepted.funded_amnt, y=df_accepted.loan_amnt)","1dc9157c":"sns.scatterplot(x=df_accepted.funded_amnt, y=df_accepted.funded_amnt_inv)","dd3189d8":"sns.scatterplot(x=df_accepted.total_pymnt, y=df_accepted.installment)","8e234dbe":"cols_to_drop = ['fico_range_low', 'funded_amnt_inv', 'funded_amnt', 'total_pymnt_inv', 'total_pymnt', 'installment', 'collection_recovery_fee', 'total_rec_prncp', 'last_fico_range_low']\ndf_accepted = df_accepted.drop(columns=cols_to_drop)\ndf_accepted.shape","513d59ab":"values = df_accepted.loan_status.unique()\nencode = [1,0]\nd = dict(zip(values, encode))","90d27576":"df_accepted['loan_paid'] = df_accepted['loan_status'].map(d)\n\ndf_accepted = df_accepted.drop(columns=['loan_status'])","107afeb2":"# checking correlation of features to the new loan_paid column\n\n\nplt.figure(figsize=(15,7))\nnew_corr = df_accepted.corr().iloc[:-1,-1].sort_values()\nnew_corr.plot.bar(rot=90)","b66dc712":"df_accepted.head()","cfa8506c":"cat_cols = df_accepted.select_dtypes(include=['object']).columns\nprint(cat_cols)\ndf_accepted[cat_cols]","41a6dc58":"# droping constant columns\n\nprint(df_accepted.pymnt_plan.value_counts())\nprint(df_accepted.hardship_flag.value_counts())\n\ncols_to_drop = ['id', 'pymnt_plan', 'hardship_flag']\ndf_accepted = df_accepted.drop(columns=cols_to_drop)\ndf_accepted.shape","46745807":"cat_cols = df_accepted.select_dtypes(include=['object']).columns\nprint(cat_cols)","98556e7d":"print(df_accepted.term.value_counts())\n\n# convert term into either a 36 or 60 integer numeric data type\n\nl1 = df_accepted.term.unique()\nl2 = [36, 60]\nd = dict(zip(l1, l2))\n\ndf_accepted['term'] = df_accepted['term'].map(d)","3bb3cbd1":"df_accepted['term']","38845340":"cat_cols = df_accepted.select_dtypes(include=['object']).columns\nprint(cat_cols)","764516b2":"print(df_accepted.grade.value_counts())\nprint(df_accepted.sub_grade.value_counts())","8a731ee3":"# checking count per grade and loan_paid\n\nsns.countplot(x=df_accepted.grade, hue=df_accepted.loan_paid)","e2401c6d":"# checking count per subgrade and loan_paid\n\nl = list(df_accepted.sub_grade.unique())\nl.sort()\n\nplt.figure(figsize=(12,4))\nsns.countplot(x=df_accepted.sub_grade, order=l, hue=df_accepted.loan_paid, palette='coolwarm')","e02e67a0":"# as grade is part of sub_grade, so let's just drop the grade feature.\n\ndf_accepted = df_accepted.drop(columns='grade')","100ce54a":"df_accepted = pd.get_dummies(df_accepted, columns = ['sub_grade'], prefix='', prefix_sep='', drop_first=True)","304a3f10":"cat_cols = df_accepted.select_dtypes(include=['object']).columns\nprint(cat_cols)","27953678":"print(df_accepted.verification_status.unique())\nprint(df_accepted.application_type.unique())\nprint(df_accepted.initial_list_status.unique())\nprint(df_accepted.purpose.unique())","1cae4f26":"# for these columns, let's just create dummy variables, concatenate them with the original dataframe and drop original columns.\n\ndf_accepted = pd.get_dummies(df_accepted, columns = ['verification_status'], prefix='', prefix_sep='', drop_first=True)\ndf_accepted = pd.get_dummies(df_accepted, columns = ['application_type'], prefix='', prefix_sep='', drop_first=True)\ndf_accepted = pd.get_dummies(df_accepted, columns = ['initial_list_status'], prefix='', prefix_sep='', drop_first=True)\ndf_accepted = pd.get_dummies(df_accepted, columns = ['purpose'], prefix='', prefix_sep='', drop_first=True)","4d1cb2b0":"cat_cols = df_accepted.select_dtypes(include=['object']).columns\nprint(cat_cols)","7d3cfd37":"df_accepted.home_ownership.value_counts()","116d0d78":"df_accepted['home_ownership'] = df_accepted.home_ownership.replace('NONE', 'ANY')\ndf_accepted = pd.get_dummies(df_accepted, columns = ['home_ownership'], prefix='', prefix_sep='', drop_first=True)","92159181":"cat_cols = df_accepted.select_dtypes(include=['object']).columns\nprint(cat_cols)","ad09c664":"df_accepted.issue_d.head(10)","37b89b6b":"df_accepted = df_accepted.drop(columns=['issue_d', 'url', 'last_pymnt_d', 'last_credit_pull_d'])","01d2243b":"df_accepted.zip_code.value_counts()","08495b21":"df_accepted = df_accepted.drop(columns=['zip_code', 'addr_state'])","f13549cc":"cat_cols = df_accepted.select_dtypes(include=['object']).columns\nprint(cat_cols)","4d51c332":"print(df_accepted.disbursement_method.value_counts())\nprint(df_accepted.debt_settlement_flag.value_counts())","82964c5e":"values = df_accepted.debt_settlement_flag.unique()\nencode = [0,1]\nd = dict(zip(values, encode))\ndf_accepted['debt_settlement_flag_bin'] = df_accepted['debt_settlement_flag'].map(d)","6b73f62b":"df = df_accepted.head(100)","08669777":"new_corr = df[['debt_settlement_flag_bin','loan_paid']].corr()\nnew_corr.plot.bar(rot=90)","20adca84":"df_accepted.corr()['debt_settlement_flag_bin', 'loan_paid']","872ad47d":"df_accepted = pd.get_dummies(df_accepted, columns = ['disbursement_method'], prefix='', prefix_sep='', drop_first=True)\ndf_accepted = pd.get_dummies(df_accepted, columns = ['debt_settlement_flag'], prefix='', prefix_sep='', drop_first=True)","881571b5":"print(df_accepted.earliest_cr_line.value_counts())","a6eccd11":"df_accepted = df_accepted.drop(columns=['earliest_cr_line'])","b4b527da":"# sampling data\n\n# df = df_accepted.sample(frac=0.8,random_state=42)\n# print(len(df))\n\ndf = df_accepted.copy()\n\nX = df.loc[:, df.columns != 'loan_paid'].values\ny = df.loan_paid.values","5c72a12b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","32a9279f":"scaler = MinMaxScaler()\n\nX_train= scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nprint(X_train.shape)\nprint(X_test.shape)","7b2263f8":"# building the model\n\nmodel = Sequential()\nmodel.add(Dense(units=78,activation='relu'))\nmodel.add(Dense(units=39,activation='relu'))\nmodel.add(Dense(units=19,activation='relu'))\nmodel.add(Dense(units=8,activation='relu'))\nmodel.add(Dense(units=4,activation='relu'))\nmodel.add(Dense(units=1,activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","c46c3dbc":"model.fit(x=X_train, \n          y=y_train, \n          epochs=40,\n          batch_size=512,\n          validation_data=(X_test, y_test), verbose=1)","b8ee7826":"losses = pd.DataFrame(model.history.history)","57e43f9b":"losses[['loss','val_loss']].plot()","21d740f3":"losses[['accuracy','val_accuracy']].plot()","8da69749":"predictions = (model.predict(X_test) > 0.5).astype(\"int32\")\n\nprint(classification_report(y_test, predictions))","093a7413":"pd.DataFrame(confusion_matrix(y_test, predictions))","37181766":"import random\n\ndf_accepted = df_accepted.reset_index(drop=True)\n\nrandom.seed(101)\nrandom_ind = random.randint(0,len(df_accepted))\n\nnew_customer = df_accepted.drop('loan_paid',axis=1).iloc[random_ind]\nnew_customer","70790e52":"new_c = scaler.transform(new_customer.values.reshape(1,81))\n\nprint(f\"model prediction: {(model.predict(new_c) > 0.5).astype('int32')[0][0]}\")","6efdd2a3":"# checking if this customer paid his loan\n\ndf_accepted.loc[random_ind, 'loan_paid']","1dcd6c83":"for the sake of simplicity, let's consider only _Fully Paid_ and _Charged Off_ values:","b53034d9":"let's drop these columns, as it will not contribute to our goal.","fdcbfd4e":"let's now see the correlation between continuous variables:","703c5363":"Since we are going to predict loan_status, let's take a brief closer look on it:","ae720bb7":"There are many yellowish squares. This indicates almost perfect correlation between many variables. Let's dig deeper:","e754f4a5":"let's now take a look at loan_amnt histogram, to have an idea of the amount distribution:","9990dc99":"there are 3 columns with a visible different behaviour:\nout_prncp, out_prncp_inv and policy_code.","a1aa2ebe":"### feature earliest_cr_line","4c127e3a":"<a id=\"sec2\"><\/a>\n\n# Exploratory Data Analysis (and some initial pre-processing)","a02b5d18":"we still have many string categorical columns. Let's work on it:","1491115b":"wow, there is a bunch of columns almost full of missing values. Let's check the first 50 columns sorted by missing values:","8d12673f":"<a id=\"sec5\"><\/a>\n\n# Evaluate model","ad6ed6b0":"<a id=\"sec4\"><\/a>\n\n# Creating a Neural Network Model","5f4075a9":"now we are ready to start our modelling phase. First, let's apply a train-test-split and scale data before training.","3d89f898":"### features \"grade\" and \"sub_grade\"","333ba0db":"### features verification_status, application_type, initial_list_status, purpose ","21c2c961":"### features zip_code and addr_state","478807a8":"checking now null values per columns:","b700872c":"maybe we can go further, and eliminate columns up to title:","ef307f48":"# Introduction\n\nUsing historical data on loans from Lending Club - including information on whether or not the borrower defaulted (charge-off) - the main objective of this notebook will be to predict if a potential borrower is likely to pay back the loan.\n\nAfter explore and pre-process data, we are going to train a Neural Network binary classification model in order to achive this objective. Finally, we evaluate model performance and simulate its operation with a new customer prediction. \n\nNotebook sections:\n\n1. <a href=\"#sec1\"> Import data <\/a>\n2. <a href=\"#sec2\"> Exploratory Data Analysis <\/a>\n3. <a href=\"#sec3\"> Pre-processing <\/a>\n4. <a href=\"#sec4\"> Define and train Model <\/a>\n5. <a href=\"#sec5\"> Evaluate Model <\/a>\n\n\np.s. work inspired by the excellent <a href=\"https:\/\/www.udemy.com\/course\/complete-tensorflow-2-and-keras-deep-learning-bootcamp\/\"> Tf2 and Keras DL Bootcamp <\/a>","9c6e630f":"### feature \"term\"","d85a035d":"<a id=\"sec1\"><\/a>\n# Importing data","80b4e7f6":"<a id=\"sec3\"><\/a>\n\n# Pre-processing","bd2f5631":"as our target column is categorical (loan_status), let's create a new feature - loan_paid - that is 1 for 'Fully Paid' and 0 for 'Charged Off'","4f199ce9":"Let's perform scatterplot and visually check some relationships among these features:","3ac84887":"### simulating a new customer","0c07fbb4":"### feature home_ownership","2284b3af":"> These are constant columns. We can drop it either.","f6e12354":"### features issue_d, url, last_pymnt_d and last_credit_pull_d","e2b8cb50":"for the remaining columns with missing values, let's see the missing percentage:","c88f1c02":"### features disbursement_method and debt_settlement_flag","5a37e230":"let's check again:","ec825570":"Remaining columns have missing data points, but they account for less than 0.2% of the total data. \nLet's remove the rows that are missing those values in those columns with dropna():","7a076058":"It seems we have many \"duplicated\" information. Let's drop some columns, based on visual inspection at corr_pairs dataframe:"}}