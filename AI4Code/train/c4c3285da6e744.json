{"cell_type":{"9d92de99":"code","4cb5ab10":"code","14160b15":"code","1ead7b46":"code","166d7112":"code","741fe465":"code","0ab47c6c":"code","1319cc75":"code","d4ef024a":"code","54fb7b9e":"code","05d3bd40":"code","122a6b29":"code","b9cd750b":"code","789dc490":"code","c4d6a7e0":"code","9495e236":"code","c40de961":"code","85b38b2f":"code","45891a8a":"code","65854431":"code","d9904898":"code","3de7e674":"code","25e25893":"code","0d66e6b5":"code","0927b4e8":"code","d46c5230":"code","fc352f86":"code","4aa0843d":"code","c609f926":"code","d5779279":"code","57e03c6c":"code","4dbb3373":"code","216b2ca6":"code","2eb97d91":"code","d8bd4d48":"markdown","fdf3dfdf":"markdown","8ddc3ac9":"markdown","dfead302":"markdown","848b2c40":"markdown","8759a3b6":"markdown","b07a29ae":"markdown","4df902fa":"markdown","cfb82e78":"markdown","dc4c63a3":"markdown","723b9fde":"markdown","1425a528":"markdown","9e4de38f":"markdown","e354971e":"markdown","dac9f85d":"markdown","9e55b5ca":"markdown","83c3e870":"markdown","3f16f99d":"markdown","a08b4c94":"markdown","1d36d34f":"markdown","2eadee5b":"markdown","f00f8b79":"markdown","8252d3b7":"markdown","507aa6f1":"markdown","4210fb1e":"markdown","bd547b00":"markdown","595972dd":"markdown","533788ad":"markdown","fd151986":"markdown","6854910b":"markdown","676ff4c2":"markdown","d0c893fe":"markdown","b0df6bc7":"markdown","2d3dc3f9":"markdown","8ec77195":"markdown","00427a86":"markdown","27fb9f70":"markdown","b0802f88":"markdown","2d6e507a":"markdown","eeeb13c4":"markdown","3cc1458a":"markdown","bee628e4":"markdown","de0a88fe":"markdown","fd1790ca":"markdown"},"source":{"9d92de99":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4cb5ab10":"# Import matplotlib and seaborn\nimport matplotlib.pyplot as plt\nimport seaborn as sns","14160b15":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n\ndisplay(train.head())\n\nprint(train.info())\nprint(train.info())\nprint(train.describe())","1ead7b46":"# Visualize with a countplot\nsns.countplot(x=\"Survived\", data=train)\nplt.show()\n\n# Print the proportions\nprint(train[\"Survived\"].value_counts(normalize=True))","166d7112":"# Visualize with a countplot\nsns.countplot(x=\"Pclass\", hue=\"Survived\", data=train)\nplt.show()\n\n# Proportion of people survived for each class\nprint(train[\"Survived\"].groupby(train[\"Pclass\"]).mean())\n\n# How many people we have in each class?\nprint(train[\"Pclass\"].value_counts())","741fe465":"# Display first five rows of the Name column\ndisplay(train[[\"Name\"]].head())","0ab47c6c":"\n# Get titles\ntrain[\"Title\"] = train['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n\n# Print title counts\nprint(train[\"Title\"].value_counts())","1319cc75":"# Print the Surviving rates by title\nprint(train[\"Survived\"].groupby(train[\"Title\"]).mean().sort_values(ascending=False))","d4ef024a":"# Print the missing values in Age column\nprint(train[\"Age\"].isnull().sum())","54fb7b9e":"# Survived by age\nsns.distplot(train[train.Survived==1][\"Age\"],color=\"y\", bins=7, label=\"1\")\n\n# Death by age\nsns.distplot(train[train.Survived==0][\"Age\"], bins=7, label=\"0\")\nplt.legend()\nplt.title(\"Age Distribution\")\nplt.show()","05d3bd40":"# Visualize with a countplot\nsns.countplot(x=\"Sex\", hue=\"Survived\", data=train)\nplt.show()\n\n# Proportion of people survived for each class\nprint(train[\"Survived\"].groupby(train[\"Sex\"]).mean())\n\n# How many people we have in each class?\nprint(train[\"Sex\"].value_counts())","122a6b29":"print(train[\"SibSp\"].value_counts())\n\nprint(train[\"Parch\"].value_counts())\n\ntrain[\"family_size\"] = train[\"SibSp\"] + train[\"Parch\"]\n\nprint(train[\"family_size\"].value_counts())\n\n# Proportion of people survived for each class\nprint(train[\"Survived\"].groupby(train[\"family_size\"]).mean().sort_values(ascending=False))\n","b9cd750b":"# Print the first five rows of the Ticket column\nprint(train[\"Ticket\"].head(15))","789dc490":"# Get first letters of the tickets\ntrain[\"Ticket_first\"] = train[\"Ticket\"].apply(lambda x: str(x)[0])\n\n# Print value counts\nprint(train[\"Ticket_first\"].value_counts())\n\n# Surviving rates of first letters\nprint(train.groupby(\"Ticket_first\")[\"Survived\"].mean().sort_values(ascending=False))","c4d6a7e0":"# Print 3 bins of Fare column\nprint(pd.cut(train['Fare'], 3).value_counts())\n\n# Plot the histogram\nsns.distplot(train[\"Fare\"])\nplt.show()\n\n# Print binned Fares by surviving rate\nprint(train['Survived'].groupby(pd.cut(train['Fare'], 3)).mean())","9495e236":"# Print the unique values in the Cabin column\nprint(train[\"Cabin\"].unique())\n\n# Get the first letters of Cabins\ntrain[\"Cabin_first\"] = train[\"Cabin\"].apply(lambda x: str(x)[0])\n\n# Print value counts of first letters\nprint(train[\"Cabin_first\"].value_counts())\n\n# Surviving rate of Cabin first letters\nprint(train.groupby(\"Cabin_first\")[\"Survived\"].mean().sort_values(ascending=False))","c40de961":"# Make a countplot\nsns.countplot(x=\"Embarked\", hue=\"Survived\", data=train)\nplt.show()\n\n# Print the value counts\nprint(train[\"Embarked\"].value_counts())\n\n# Surviving rates of Embarked\nprint(train[\"Survived\"].groupby(train[\"Embarked\"]).mean())","85b38b2f":"# Load the train and the test datasets\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\nprint(test.info())","45891a8a":"# Put the mean into the missing value\ntest['Fare'].fillna(train['Fare'].mean(), inplace = True)","65854431":"from sklearn.impute import SimpleImputer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n# Imputers\nimp_embarked = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\nimp_age = IterativeImputer(max_iter=100, random_state=34, n_nearest_features=2)\n\n# Impute Embarked\ntrain[\"Embarked\"] = imp_embarked.fit_transform(train[[\"Embarked\"]])\ntest[\"Embarked\"] = imp_embarked.transform(test[[\"Embarked\"]])\n\n# Impute Age\ntrain[\"Age\"] = np.round(imp_age.fit_transform(train[[\"Age\"]]))\ntest[\"Age\"] = np.round(imp_age.transform(test[[\"Age\"]]))","d9904898":"from sklearn.preprocessing import LabelEncoder\n\n# Initialize a Label Encoder\nle = LabelEncoder()\n\n# Encode Sex\ntrain[\"Sex\"] = le.fit_transform(train[[\"Sex\"]].values.ravel())\ntest[\"Sex\"] = le.fit_transform(test[[\"Sex\"]].values.ravel())","3de7e674":"# Family Size\ntrain[\"Fsize\"] = train[\"SibSp\"] + train[\"Parch\"]\ntest[\"Fsize\"] = test[\"SibSp\"] + test[\"Parch\"]","25e25893":"# Ticket first letters\ntrain[\"Ticket\"] = train[\"Ticket\"].apply(lambda x: str(x)[0])\ntest[\"Ticket\"] = test[\"Ticket\"].apply(lambda x: str(x)[0])\n\n# Cabin first letters\ntrain[\"Cabin\"] = train[\"Cabin\"].apply(lambda x: str(x)[0])\ntest[\"Cabin\"] = test[\"Cabin\"].apply(lambda x: str(x)[0])","0d66e6b5":"# Titles\ntrain[\"Title\"] = train['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\ntest[\"Title\"] = test['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]","0927b4e8":"# Group the family_size column\ndef assign_passenger_label(family_size):\n    if family_size == 0:\n        return \"Alone\"\n    elif family_size <=3:\n        return \"Small_family\"\n    else:\n        return \"Big_family\"\n    \n# Group the Ticket column\ndef assign_label_ticket(first):\n    if first in [\"F\", \"1\", \"P\", \"9\"]:\n        return \"Ticket_high\"\n    elif first in [\"S\", \"C\", \"2\"]:\n        return \"Ticket_middle\"\n    else:\n        return \"Ticket_low\"\n    \n# Group the Title column    \ndef assign_label_title(title):\n    if title in [\"the Countess\", \"Mlle\", \"Lady\", \"Ms\", \"Sir\", \"Mme\", \"Mrs\", \"Miss\", \"Master\"]:\n        return \"Title_high\"\n    elif title in [\"Major\", \"Col\", \"Dr\"]:\n        return \"Title_middle\"\n    else:\n        return \"Title_low\"\n    \n# Group the Cabin column  \ndef assign_label_cabin(cabin):\n    if cabin in [\"D\", \"E\", \"B\", \"F\", \"C\"]:\n        return \"Cabin_high\"\n    elif cabin in [\"G\", \"A\"]:\n        return \"Cabin_middle\"\n    else:\n        return \"Cabin_low\"","d46c5230":"# Family size\ntrain[\"Fsize\"] = train[\"Fsize\"].apply(assign_passenger_label)\ntest[\"Fsize\"] = test[\"Fsize\"].apply(assign_passenger_label)\n\n# Ticket\ntrain[\"Ticket\"] = train[\"Ticket\"].apply(assign_label_ticket)\ntest[\"Ticket\"] = test[\"Ticket\"].apply(assign_label_ticket)\n\n# Title\ntrain[\"Title\"] = train[\"Title\"].apply(assign_label_title)\ntest[\"Title\"] = test[\"Title\"].apply(assign_label_title)\n\n# Cabin\ntrain[\"Cabin\"] = train[\"Cabin\"].apply(assign_label_cabin)\ntest[\"Cabin\"] = test[\"Cabin\"].apply(assign_label_cabin)","fc352f86":"train = pd.get_dummies(columns=[\"Pclass\", \"Embarked\", \"Ticket\", \"Cabin\",\"Title\", \"Fsize\"], data=train, drop_first=True)\ntest = pd.get_dummies(columns=[\"Pclass\", \"Embarked\", \"Ticket\", \"Cabin\", \"Title\", \"Fsize\"], data=test, drop_first=True)","4aa0843d":"target = train[\"Survived\"]\ntrain.drop([\"Survived\", \"SibSp\", \"Parch\", \"Name\", \"PassengerId\"], axis=1, inplace=True)\ntest.drop([\"SibSp\", \"Parch\", \"Name\",\"PassengerId\"], axis=1, inplace=True)","c609f926":"display(train.head())\ndisplay(test.head())\n\nprint(train.info())\nprint(test.info())","d5779279":"from sklearn.model_selection import train_test_split\n\n# Select the features and the target\nX = train.values\ny = target.values\n\n# Split the data info training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=34, stratify=y)","57e03c6c":"# Import Necessary libraries\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n\n\n\"\"\"\n# Initialize a RandomForestClassifier\nrf = RandomForestClassifier(random_state=34)\n\nparams = {'n_estimators': [50, 100, 200, 300, 350],\n          'max_depth': [3,4,5,7, 10,15,20],\n          'criterion':['entropy', 'gini'],\n          'min_samples_leaf' : [1, 2, 3, 4, 5, 10],\n          'max_features':['auto'],\n          'min_samples_split': [3, 5, 10, 15, 20],\n          'max_leaf_nodes':[2,3,4,5],\n          }\n\nclf = GridSearchCV(estimator=rf,param_grid=params,cv=10, n_jobs=-1)\n\nclf.fit(X_train, y_train.ravel())\n\nprint(clf.best_estimator_)\nprint(clf.best_score_)\n\n\"\"\"\n\n# Initialize a RandomForestClassifier\nrf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=4, max_features='auto',\n                       max_leaf_nodes=5, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=15,\n                       min_weight_fraction_leaf=0.0, n_estimators=350,\n                       n_jobs=None, oob_score=False, random_state=34, verbose=0,\n                       warm_start=False)\n\nrf.fit(X_train, y_train)\n\n# Predict from the test set\ny_pred = rf.predict(X_test)\n\n# Predict from the train set\ny_pred_train = rf.predict(X_train)\n\n# Print the accuracy with accuracy_score function\nprint(\"Accuracy Train: \", accuracy_score(y_train, y_pred_train))\n\n# Print the accuracy with accuracy_score function\nprint(\"Accuracy Test: \", accuracy_score(y_test, y_pred))\n\n# Print the confusion matrix\nprint(\"\\nConfusion Matrix\\n\")\nprint(confusion_matrix(y_test, y_pred))","4dbb3373":"# Create a pandas series with feature importances\nimportance = pd.Series(rf.feature_importances_,index=train.columns).sort_values(ascending=False)\n\nsns.barplot(x=importance, y=importance.index)\n# Add labels to your graph\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.title(\"Important Features\")\nplt.show()","216b2ca6":"last_clf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=4, max_features='auto',\n                       max_leaf_nodes=5, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=15,\n                       min_weight_fraction_leaf=0.0, n_estimators=350,\n                       n_jobs=None, oob_score=True, random_state=34, verbose=0,\n                       warm_start=False)\n\nlast_clf.fit(train, target)\nprint(\"%.4f\" % last_clf.oob_score_)","2eb97d91":"# Store passenger ids\nids = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")[[\"PassengerId\"]].values\n\n# Make predictions\npredictions = last_clf.predict(test.values)\n\n# Print the predictions\nprint(predictions)\n\n# Create a dictionary with passenger ids and predictions\ndf = {'PassengerId': ids.ravel(), 'Survived':predictions}\n\n# Create a DataFrame named submission\nsubmission = pd.DataFrame(df)\n\n# Display the first five rows of submission\ndisplay(submission.head())\n\n# Save the file\nsubmission.to_csv(\"submission.csv\", index=False)","d8bd4d48":"Ticket first letters and Cabin first letters are also needed","fdf3dfdf":"# 3. Machine Learning\nTo evaluate our model's performance, we need to split our train data into training and test sets.","8ddc3ac9":"It's time to use One Hot Encoding","dfead302":"In this notebook, you will find the road map that I followed to build a machine learning model for the competition. The model's predictions have placed me in top **6%** on the leaderboard (on 23th of Aug 20) with **80.143%** score.\n\nYou can find the whole code [here](https:\/\/github.com\/Bhasfe\/titanic)","848b2c40":"# Fare\nWe can plot a histogram to see Fare distribution","8759a3b6":"\nApparently, family size is important to survive. I am going to group them in feature engineering step like following <br>\n\n**big family** = if family size > 3 <br>\n**small family** = if family size > 0 and family size < =3 <br>\n**alone** = family size == 0","b07a29ae":"We also encode the sex column.","4df902fa":"I extracted only first letters of the tickets because I thought that they would indicate the ticket type.","cfb82e78":"# Ticket\nAt first, I thought that I would drop this column but after exploration I found useful features.","dc4c63a3":"# 2. Feature Engineering\nWe have learned a lot from exploratory data analysis. Now we can start feature engineering. Firstly, let's load the train and the test sets.","723b9fde":"I have used two types of Imputer from sklearn. Iterative imputer for age imputation, and Simple imputer ( with most frequent strategy) for Embarked","1425a528":"We can extract the titles from names.","9e4de38f":"Extract the titles from the names","e354971e":"According to surviving rates. I will group the Cabins like following\n\n**higher surviving rate**  = D, E, B, F, C <br>\n**neutral** = G, A<br> \n**lower surviving rate**   else <br>","dac9f85d":"We can look at the feature importances.","9e55b5ca":"Now, we need some helper functions to group our categories","83c3e870":"\nApparently, there is relationship between titles and surviving rate. In feature engineering part, I will group title by their surviving rates like following <br>\n\n**higher** = the Countess, Mlle, Lady, Ms , Sir, Mme, Mrs, Miss, Master  <br>\n**neutral** = Major, Col, Dr  <br>\n**lower** = Mr, Rev, Jonkheer, Don, Capt  <br>","3f16f99d":"Apply the functions.","a08b4c94":"# **Survived**<br>\nLet's start with Survived column. It contains integer 1 or 0 which correspond to surviving ( 1 = Survived, 0 = Not Survived)","1d36d34f":"# Cabin <br><br>\n![cabin](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/5\/5d\/Titanic_side_plan_annotated_English.png\/880px-Titanic_side_plan_annotated_English.png)\n\nI found this figure [wikiwand.com](https:\/\/www.wikiwand.com\/en\/Sinking_of_the_Titanic). The figure shows us the most affacted parts of the Titanic and the Cabin locations. Although there are many missing value in Cabin column, I decided to extract the Cabin information to try whether it works or not.","2eadee5b":"# **Pclass**<br>\nPclass column contains the socioeconomic status of the passengers. It might be predictive for our model<br>\n1 = Upper<br>\n2 = Middle<br>\n3 = Lower","f00f8b79":"# Age","8252d3b7":"There is also a correlation between ticket fares and surviving","507aa6f1":"1. Notes:\n    * There are some missing values in Age, Embarked and Cabin columns.\n    * We do not need PassengerId column\n    * The surviving rate is 38.3% in our dataset","4210fb1e":"The first letters of the tickets are correlated with surviving rate somehow. I am going to group them like following <br>\n\n**higher surviving rate** = F, 1, P , 9  <br>\n**neutral** = S, C, 2  <br>\n**lower surviving rate** = else <br>","bd547b00":"In EDA, we decided to use family size feature","595972dd":"There is one missing value in the Fare column of the test set. I imputed it by using mean.","533788ad":"Prepare the submission file","fd151986":"# Embarked\nEmbarked is a categorical features which shows us the port of embarkation. <br>\n\nC = Cherbourg, Q = Queenstown, S = Southampton","6854910b":"Finally, we can submit our predictions. I hope this notebook will be helpful for you!\nGoodluck!","676ff4c2":"Workflow:\n\n1. Exploratory Data Analysis.\n    * Surviving rate\n    * Pclass\n    * Name\n    * Sex\n    * Age\n    * SibSp, Parch\n    * Ticket\n    * Fare\n    * Cabin\n    * Embarked\n1. Feature Engineering\n    * Imputation on Embarked and Age columns\n    * Title extraction\n    * Ticket first letters\n    * Cabin first letters\n    * Encoding sex column\n    * Family size\n    * One Hot Encoding for all categorical variables\n1. Machine Learning\n    * Split data into train and test sets\n    * Initialize a Random Forest Classifier\n    * Hyperparameter Tuning with Grid Search\n    * Prediction","d0c893fe":"# Sex\nIs sex important for surviving?","b0df6bc7":"# **SibSp & Parch**\nSibSp = Sibling or Spouse number <br>\nParch = Parent or Children number <br><br>\n \nI decided to make a new feature called family size by summing the SibSp and Parch columns","2d3dc3f9":"\nObviously, there is a relationship between sex and surviving.","8ec77195":"As I expected, first class passengers have higher surviving rate. We will use this information in our training data.","00427a86":"\n# **Name <br>**\nAt a first glance, I thought that I would use the titles.","27fb9f70":"I have used GridSearchCV for tuning my Random Forest Classifier.I will not search parameters here. If you want you can check my [github](https:\/\/github.com\/Bhasfe\/titanic\/) repository.","b0802f88":"After hyperparameter tuning. I have fit the model with whole train data.","2d6e507a":"Drop the colums that are no longer needed","eeeb13c4":"# **1. Exploratory Data Analysis**","3cc1458a":"There are 177 missing values in Age column, we will impute them in Feature engineering part. Now, let's look at the distribution of ages","bee628e4":"No doubt, C has the higher surviving rate. We will definetely use this information.","de0a88fe":"\nIs there any relationship between titles and surviving","fd1790ca":"Final look"}}