{"cell_type":{"24cdb066":"code","860bdb83":"code","f425da7f":"code","aec34886":"code","d948a57b":"code","1edbec7b":"code","0fc33245":"code","3eae1e55":"code","209e0807":"code","196998c3":"code","d1aa96e5":"code","6a5cb06c":"code","8de64779":"code","8de53ab4":"code","209aef39":"code","45496bd7":"code","f9d8d338":"code","dad23ac4":"code","53745d71":"code","bdf9668c":"code","4ee88d4a":"code","5cb8cbf7":"code","8ed86030":"code","003533b2":"code","ecdca3d4":"code","b6ec4c72":"code","e6112539":"code","6f3ff7d5":"code","ca5f1485":"code","e0bf8fb9":"code","5b412725":"code","5161eacf":"code","f94272b8":"code","e54a3bab":"code","90ca7238":"code","7ebd9f94":"code","a9fcc2ff":"code","6ed31fe3":"code","e9610a4d":"code","bfaf905e":"code","1eb06faf":"code","8841ec8f":"markdown","25a9b94b":"markdown","f9d0f0f8":"markdown","83abe063":"markdown","910b1cad":"markdown"},"source":{"24cdb066":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","860bdb83":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","f425da7f":"train = pd.read_csv('\/kaggle\/input\/sf-crime\/train.csv.zip')\ntest = pd.read_csv('\/kaggle\/input\/sf-crime\/test.csv.zip', index_col='Id')\n","aec34886":"submission = pd.read_csv('\/kaggle\/input\/sf-crime\/sampleSubmission.csv.zip',index_col='Id')\nsubmission","d948a57b":"train","1edbec7b":"test","0fc33245":"Tename = list(test)\nTename","3eae1e55":"df_train = train[Tename]\ndf_train","209e0807":"df = pd.concat((df_train,test))","196998c3":"df","d1aa96e5":"target = train['Category']","6a5cb06c":"target.value_counts()","8de64779":"from sklearn.preprocessing import LabelEncoder\n\nLB = LabelEncoder() \ntarget = LB.fit_transform(target)\nprint(LB.classes_)","8de53ab4":"target","209aef39":"date = pd.to_datetime(df['Dates'])\ndf['Date'] = date.dt.date\ndf['Year'] = date.dt.year\ndf['Month'] = date.dt.month\ndf['Day'] = date.dt.day\ndf['Hour'] = date.dt.hour\ndf","45496bd7":"df.drop('Dates', axis=1, inplace=True)","f9d8d338":"df","dad23ac4":"year = df.groupby('Year').count().iloc[:,0]\nmonth = df.groupby('Month').count().iloc[:,0]\nhour = df.groupby('Hour').count().iloc[:,0]\ndayofweek = df.groupby('DayOfWeek').count().iloc[:, 0]\n\nfigure, axs = plt.subplots(2,2, figsize = (15,10))\n\nsns.barplot(x=year.index, y= year,ax = axs[0][0])\nsns.barplot(x=month.index, y= month,ax = axs[0][1])\nsns.barplot(x=hour.index, y= hour,ax = axs[1][0])\nsns.barplot(x=dayofweek.index, y= dayofweek,ax = axs[1][1])","53745d71":"lb = LabelEncoder()\ndf['PdDistrict'] = lb.fit_transform(df[\"PdDistrict\"])\ndf['DayOfWeek'] = lb.fit_transform(df[\"DayOfWeek\"])","bdf9668c":"df[\"Address\"].value_counts().head(20)","4ee88d4a":"df['Block'] = df['Address'].str.contains('Block')\ndf['ST'] = df['Address'].str.contains('ST')","5cb8cbf7":"df","8ed86030":"df['Block'] = lb.fit_transform(df[\"Block\"])\ndf['ST'] = lb.fit_transform(df[\"ST\"])","003533b2":"df","ecdca3d4":"df.drop(\"Address\", axis =1, inplace = True)","b6ec4c72":"print(df[\"X\"].min(), df[\"X\"].max())\nprint(df[\"Y\"].min(), df[\"Y\"].max())","e6112539":"print(len(df.loc[df[\"X\"] >= -120.5]))\nprint(len(df.loc[df[\"Y\"] >= 90]))","6f3ff7d5":"df.loc[df[\"X\"] >= -120.5, \"X\"] = df[df[\"X\"] < -120.5][\"X\"].median()\ndf.loc[df[\"Y\"] >= 90, \"Y\"] = df[df[\"Y\"] < -90][\"Y\"].median()","ca5f1485":"df['X+Y'] = df['X'] + df['Y']\ndf['X-Y'] = df['X'] - df['Y']\ndf","e0bf8fb9":"df.drop(\"Date\", axis = 1, inplace = True)\ndf","5b412725":"df","5161eacf":"new_train = df[:train.shape[0]]\nnew_test = df[train.shape[0]:]","f94272b8":"new_train","e54a3bab":"len(target)","90ca7238":"new_test","7ebd9f94":"del train, test","a9fcc2ff":"new_train.shape","6ed31fe3":"target.shape","e9610a4d":"import lightgbm as lgb\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.model_selection import cross_val_score\n\ndef lgb_evaluate(                \n                numLeaves,\n                maxDepth,\n                scaleWeight,\n                minChildWeight,\n                Subsample,\n                colSam,\n                train_x, train_y\n                ):\n    \n    clf = lgb.LGBMClassifier(\n        objective = 'multiclass',\n        metric= 'multi_logloss',\n        eval_metric= 'logloss',\n        reg_alpha= 0,\n        reg_lambda= 2,\n        bagging_fraction= 0.999,\n        min_split_gain= 0,\n        min_child_samples= 10,\n        subsample_freq= 3,\n        subsample_for_bin= 50000,\n        n_estimators= 9999999,\n        num_leaves= int(numLeaves),\n        max_depth= int(maxDepth),\n        scale_pos_weight= scaleWeight,\n        min_child_weight= minChildWeight,\n        subsample= Subsample)\n        colsample_bytree= colSam)\n    \n    \n    scores = cross_val_score(clf, train_x, train_y, cv=5, scoring='neg_log_loss')\n    print(scores)\n\n    return scores\n   \ndef bayesOpt(train_x, train_y):\n    def dtc_crossval(numLeaves,\n                    maxDepth,\n                    scaleWeight,\n                    minChildWeight,\n                    subsample,\n                    colSam):\n        return lgb_evaluate(\n            numLeaves= int(numLeaves),\n            maxDepth= int(maxDepth),\n            scaleWeight= scaleWeight,\n            minChildWeight= minChildWeight,\n            Subsample= subsample,\n            colSam= colSam,\n            train_x=train_x,\n            train_y=train_y,\n        )    \n    \n    lgbBO = BayesianOptimization(dtc_crossval,\n                                 {                                             \n                                    'numLeaves':  (5, 50),\n                                    'maxDepth': (2, 63),\n                                    'scaleWeight': (1, 10000),\n                                    'minChildWeight': (0.01, 70),\n                                    'subsample': (0.4, 1),                                                \n                                    'colSam': (0.4, 1)\n                                            })\n    \n\n    lgbBO.maximize(init_points=5, n_iter=5)\n\n    print(lgbBO.res['max'])\n    \n\nbayesOpt(new_train, target)","bfaf905e":"predictions = bst.predict(new_test)","1eb06faf":"submission = pd.DataFrame(predictions,columns=LB.inverse_transform(np.linspace(0, 38, 39, dtype='int16')),index=new_test.index)\n#submission.to_csv('LGB.csv', index_label='Id')","8841ec8f":"1. EDA\n1. Data Preprocess\n1. Single Model\n1. Feature Engineering\n1. Stacking\n1. submit","25a9b94b":"\uc0cc\ud504\ub780\uc2dc\uc2a4\ucf54 \uc704\ub3c4(x): 37.7272, \uacbd\ub3c4(y): -123.032","f9d0f0f8":"https:\/\/nurilee.com\/2020\/04\/03\/lightgbm-definition-parameter-tuning\/","83abe063":"III. \ubaa8\ub378\ub9c1\n\n","910b1cad":"\uc704\ub3c4\t32\u00b030'N ~ 42\u00b0N   \n\uacbd\ub3c4\t114\u00b08'W ~ 124\u00b024'W"}}