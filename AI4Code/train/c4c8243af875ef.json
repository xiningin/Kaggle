{"cell_type":{"ec0f0ca7":"code","7770a943":"code","f5486c42":"code","ef2f83cf":"code","6588b280":"code","84c4aee2":"code","8f713bb4":"code","774ce186":"code","dfeb5da2":"code","39f69ffb":"code","8ab21bda":"code","74cd2e14":"code","a33937d9":"code","41743c81":"code","0a6b0c63":"code","2235a452":"code","3c787c9a":"code","862d2681":"code","fc7f2b87":"code","9dbafdcb":"code","dd7520d5":"code","5193f272":"code","8085d2d2":"code","b00a77f8":"code","09a03ce9":"code","4ffc49bc":"code","c2e54a96":"code","bf5fa27d":"code","01a64200":"code","8951f38e":"code","e01f7bb2":"code","f9102af2":"code","350493de":"code","de1891e4":"code","d37f9013":"markdown","9abc305b":"markdown","db7d586c":"markdown","36710161":"markdown","389528ab":"markdown","7d15fc16":"markdown","d1aaea14":"markdown","c5fdbd0f":"markdown","2cd5c2ea":"markdown","3a77747d":"markdown","85bec817":"markdown","7bca0377":"markdown"},"source":{"ec0f0ca7":"!pip install scikit-learn  -U\n!pip install scikit-optimize -U","7770a943":"import itertools\nfrom math import ceil\nfrom functools import partial\n\nimport pandas as pd\nimport numpy as np\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom statsmodels.tsa.deterministic import DeterministicProcess\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\nimport seaborn as sns\nfrom sklearn.metrics import fbeta_score, make_scorer\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom xgboost import XGBRegressor\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","f5486c42":"# modified from https:\/\/www.kaggle.com\/c\/web-traffic-time-series-forecasting\/discussion\/36414\ndef smape_loss(y_true, y_pred, ne=False):\n    \"\"\"\n    SMAPE Loss\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n        Ground truth (correct) target values.\n    y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n        Estimated target values.\n    Returns\n    -------\n    loss : float or ndarray of floats\n        If multioutput is 'raw_values', then mean absolute error is returned\n        for each output separately.\n        If multioutput is 'uniform_average' or an ndarray of weights, then the\n        weighted average of all output errors is returned.\n        SMAPE output is non-negative floating point. The best value is 0.0.\n\n    \"\"\"\n    denominator = (np.abs(y_true) + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    if ne:\n        result = -np.mean(diff)\n    else:\n        result = np.mean(diff)\n    return result\n\n","ef2f83cf":"TRAIN_PATH = '\/kaggle\/input\/tabular-playground-series-jan-2022\/train.csv'\nTEST_PATH = '\/kaggle\/input\/tabular-playground-series-jan-2022\/test.csv'\n\nTRAIN_DTYPE = {\n        'row_id': 'uint32',\n        'country': 'category',\n        'store': 'category',\n        'product': 'category',\n        'num_sold': 'uint32'\n    }\nTEST_DTYPE = {\n        'row_id': 'uint32',\n        'country': 'category',\n        'store': 'category',\n        'product': 'category',\n    }\n\nCAT_COLS = ['country', 'store', 'product']\nTS_COLS = ['country', 'store', 'product']\nTARGET_COL = 'num_sold'\nDATE_COL = 'date'\nDATE_FREQ = 'D'\nID_COL = 'row_id'\n\nFOURIER_ORDER = 10\nLOG_TARGET = True","6588b280":"train = pd.read_csv(TRAIN_PATH, dtype=TRAIN_DTYPE, parse_dates=[DATE_COL], infer_datetime_format=True,)\ntest = pd.read_csv(TEST_PATH, dtype=TEST_DTYPE, parse_dates=[DATE_COL],infer_datetime_format=True,)\ntrain[DATE_COL] = train[DATE_COL].dt.to_period(DATE_FREQ)\ntest[DATE_COL] = test[DATE_COL].dt.to_period(DATE_FREQ)\ntrain = train.set_index(TS_COLS+[DATE_COL]).sort_index()\ntest = test.set_index(TS_COLS+[DATE_COL]).sort_index()\ndata = pd.concat([train,test])\ngdp = pd.read_csv(\"..\/input\/gdp-20152019-finland-norway-and-sweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv\", )\nholiday = pd.read_csv(\"..\/input\/nordic-holidays\/nordic_holidays.csv\",index_col=0, parse_dates=[DATE_COL],infer_datetime_format=True,)\nholiday[DATE_COL] = holiday[DATE_COL].dt.to_period(DATE_FREQ)\n","84c4aee2":"y = data.drop('row_id', axis=1)\ny = y.unstack(TS_COLS)","8f713bb4":"fourier_A = CalendarFourier(\"A\", order=FOURIER_ORDER)\nfourier_M = CalendarFourier(\"M\", order=FOURIER_ORDER)\nfourier_W = CalendarFourier(\"W\", order=FOURIER_ORDER)\n\n\ndp = DeterministicProcess(\n    index=y.index,\n    # the intercept term in linear regression\n    constant=False,\n    # The order of the tim trend to include, e.g. 1 means linear, 2 means quadratic\n    order=1,\n    # below two combines to create weekly seasonal indicator\n    seasonal=True, \n    period=7, \n    additional_terms=[fourier_A, fourier_M, fourier_W],\n    # drop the perfect collinearity column\n    drop=True,\n)\n\n\nX = dp.in_sample()","774ce186":"X.head(10)","dfeb5da2":"# # need to create long form for X (replicate this for all series)\nX = pd.merge(data.reset_index()[TS_COLS+[DATE_COL]].set_index('date'),\n        X,\n        how='left',\n        left_index=True,\n        right_index=True)","39f69ffb":"X_trend = X.reset_index().set_index([DATE_COL]+TS_COLS)[['trend']]\nX_seasonal = X.reset_index().set_index([DATE_COL]+TS_COLS).drop(['trend'],axis=1)","8ab21bda":"# one hot\nX_series = pd.DataFrame(index=X_seasonal.index).reset_index()[TS_COLS]\nX_series = X_series.apply(lambda x: '-'.join(x), axis=1).to_frame(name='series_name')\nX_series = X_series.set_index(X_seasonal.index)\nX_series = pd.get_dummies(X_series.series_name, drop_first=False)\n\n# # label\n# le = LabelEncoder()\n# X_series = pd.DataFrame(index=X_seasonal.index).reset_index()[TS_COLS]\n# X_series = X_series.apply(lambda x: '-'.join(x), axis=1).to_frame(name='series_name')\n# X_series = X_series.set_index(X_seasonal.index)\n# X_series = X_series.assign(series_name=lambda x: le.fit_transform(x.series_name))","74cd2e14":"X_non_trend = pd.merge(\n    X_seasonal,\n    X_series,\n    how='left',\n    left_index=True,\n    right_index=True)","a33937d9":"holiday = holiday.set_index(['date','country'])\nholiday = pd.get_dummies(holiday.holiday, drop_first=False)\n\nX_non_trend = pd.merge(\n    X_non_trend.reset_index(['store','product']),\n    holiday,\n    how='left',\n    left_index=True,\n    right_index=True\n)","41743c81":"X_non_trend = X_non_trend.reset_index().set_index([DATE_COL]+TS_COLS)","0a6b0c63":"results = []\nfor yr in gdp.year:\n    for ctry in ['Finland', 'Norway', 'Sweden']:\n        results.append([yr, ctry, np.squeeze(gdp[gdp.year==yr][f'GDP_{ctry}'])])\ngdp = pd.DataFrame(results, columns=['year', 'country', 'gdp']).set_index(['year', 'country'])\n\nX_trend = X_trend.reset_index()\nX_trend = X_trend.assign(year=X_trend['date'].dt.year).set_index(['year', 'country'])\nX_trend = pd.merge(\n    X_trend,\n    gdp,\n    how='left',\n    left_index=True,\n    right_index=True)\n\nX_trend['gdp'] = np.log(X_trend['gdp'])\nX_trend = X_trend.reset_index().set_index([DATE_COL]+TS_COLS).sort_index()\nX_trend.drop([\n    'trend', \n    'year',\n], axis=1, inplace=True)","2235a452":"y = y.stack(TS_COLS).sort_index()","3c787c9a":"X_trend.head(10)","862d2681":"X_non_trend.head(10)","fc7f2b87":"y.head(10)","9dbafdcb":"ne_smape_loss = partial(smape_loss, ne=True)\ncv_score = make_scorer(ne_smape_loss)\n\nclass BoostedHybrid:\n    def __init__(self, model_1, model_2, model_1_param_grid, model_2_param_grid, log_transform):\n        self.model_1 = model_1\n        self.model_2 = model_2\n        self.model_1_param_grid = model_1_param_grid\n        self.model_2_param_grid = model_2_param_grid\n        self.log_transform = log_transform\n\n    def fit(self, X_1, X_2, y):\n        self.model_1_gcv = GridSearchCV(\n            self.model_1, \n           self.model_1_param_grid, \n           cv=TimeSeriesSplit(), \n           verbose=3, \n           scoring=cv_score,\n        )\n                    \n        self.model_1_gcv.fit(X_1, self.log_transform_switch(y,inverse=False))\n        model_1_y_fit = self.log_transform_switch(self.model_1_gcv.best_estimator_.predict(X_1), inverse=True)\n        model_1_y_resid = y - model_1_y_fit\n        \n        self.model_2_gcv = GridSearchCV(\n            self.model_2, \n            self.model_2_param_grid, \n            cv=5, \n            verbose=3, \n#             scoring=cv_score,\n        )\n        self.model_2_gcv.fit(X_2, model_1_y_resid)\n        model_2_y_fit = self.model_2_gcv.best_estimator_.predict(X_2)\n        model_2_y_resid = model_1_y_resid - model_2_y_fit\n\n        self.model_1_y_fit = model_1_y_fit\n        self.model_1_y_resid = model_1_y_resid\n        self.model_2_y_fit = model_2_y_fit\n        self.model_2_y_resid = model_2_y_resid\n        \n        print(f\"model_1_best_params_: {self.model_1_gcv.best_params_}\")\n        print(f\"model_2_best_params_: {self.model_2_gcv.best_params_}\")\n\n    def predict(self, X_1, X_2):\n        y_pred = self.log_transform_switch(np.squeeze(self.model_1_gcv.best_estimator_.predict(X_1)), inverse=True)\n        y_pred += self.model_2_gcv.best_estimator_.predict(X_2)\n        return y_pred\n    \n    def log_transform_switch(self, value, inverse):\n        if not self.log_transform:\n            return value\n        else:\n            return np.expm1(value) if inverse else np.log1p(value)","dd7520d5":"y_train, y_test = y.loc[\"2015-01-01\":\"2018-12-31\"].num_sold, y.loc[\"2019-01-01\":\"2019-12-31\"].num_sold\nX_trend_train, X_trend_test =  X_trend.loc[\"2015-01-01\":\"2018-12-31\"], X_trend.loc[\"2019-01-01\":\"2019-12-31\"]\nX_non_trend_train, X_non_trend_test =  X_non_trend.loc[\"2015-01-01\":\"2018-12-31\"], X_non_trend.loc[\"2019-01-01\":\"2019-12-31\"]","5193f272":"trend_pipe = Pipeline(steps=[('preprocessor', StandardScaler()),\n                              ('ridge', Ridge())\n                             ])\ntrend_param_grid = [{\n    'ridge__alpha': [0.01, 0.1, 0.5, 1.0, 10.],\n              }]\n\n# trend_gcv = GridSearchCV(\n#     trend_pipe, \n#     trend_param_grid, \n#     cv=TimeSeriesSplit(), \n#     verbose=3, \n#     scoring=cv_score,\n#                   )\n# trend_gcv.fit(X_trend_train, y_train)\n# trend_best_model = trend_gcv.best_estimator_\n\n# print(f\"gcv.best_params_: {trend_gcv.best_params_}\")\n# print(f\"gcv.best_score_: {trend_gcv.best_score_}\")\n# print(f\"smaple_loss: {smape_loss(y_train, trend_best_model.predict(X_trend_train))}\")","8085d2d2":"non_trend_pipe = Pipeline(steps=[\n                              ('xgb', XGBRegressor())\n                             ])\nnon_trend_param_grid = {\n    'xgb__learning_rate': [.03,  0.1, ],\n    'xgb__n_estimators': [1000],\n}\n\n# non_trend_gcv = GridSearchCV(\n#     non_trend_pipe, \n#     non_trend_param_grid, \n#     cv=TimeSeriesSplit(),\n#     verbose=3, \n# #                    scoring=cv_score,\n#                   )\n# y_resid = y_train - trend_best_model.predict(X_trend_train)\n# non_trend_gcv.fit(X_non_trend_train, y_resid)\n# non_trend_best_model = non_trend_gcv.best_estimator_","b00a77f8":"boosted_model = BoostedHybrid(trend_pipe, non_trend_pipe, trend_param_grid, non_trend_param_grid, log_transform=LOG_TARGET)\nboosted_model.fit(X_trend_train, X_non_trend_train, y_train)\nprint(f\"smaple_loss: {smape_loss(y_train, boosted_model.predict(X_trend_train, X_non_trend_train))}\")","09a03ce9":"y_train_pred = boosted_model.predict(X_trend_train, X_non_trend_train)\ny_test_pred = np.ceil(boosted_model.predict(X_trend_test, X_non_trend_test))","4ffc49bc":"train_result = pd.DataFrame(index=X_trend_train.index)\ntrain_result['y_train'] = y_train\ntrain_result['y_train_pred'] = y_train_pred\ntrain_result = train_result.reset_index()","c2e54a96":"plt.figure(figsize = (16,16))\nax=sns.scatterplot(data=train_result, x=\"y_train_pred\", y=\"y_train\", legend='brief')\nplt.xlim(0, 3000)\nplt.ylim(0, 3000)","bf5fa27d":"def get_ts(df, date_start, date_end, country, store, product, set_index='date'):\n    ts_df = df.query(f\"date >='{date_start}' and date<='{date_end}' and country=='{country}' and store=='{store}' and product=='{product}'\")\n    if set_index:\n        ts_df = ts_df.set_index('date')\n    return ts_df","01a64200":"combinations = [train_result['country'].unique(),train_result['store'].unique(), train_result['product'].unique()]\ncombinations=list(itertools.product(*combinations))\n\nfig, axes = plt.subplots(int(ceil(len(combinations)\/3)), 3, sharex=False, sharey=False, figsize=(16*3, 9*3))\nfor ts_comb, ax in zip(combinations, axes.flatten()):\n    ts_df = get_ts(train_result,'2015-01-01','2018-12-31',*ts_comb)\n    ts_df['y_train'].plot(alpha=0.5, label='train',title='-'.join(ts_comb), ylabel=\"items sold\",ax=ax)\n    ts_df['y_train_pred'].plot(ax=ax, label='pred')","8951f38e":"resid = y_train-y_train_pred\nresid_df = pd.DataFrame(\n    {'resid':resid},\n    index=y_train.index\n)","e01f7bb2":"plt.figure(figsize = (16,9))\nsns.displot(data=resid_df, x=\"resid\", kde=True,stat='density')","f9102af2":"plt.figure(figsize = (16,9))\nax = sns.boxplot(y=resid_df[\"resid\"])","350493de":"plt.figure(figsize = (16,9))\nax = pd.Series(y_train_pred).hist(label='train',)\npd.Series(y_test_pred).hist(label='test', ax=ax)\nax.set_title('Distributions of predctions')\nplt.legend()","de1891e4":"submission = pd.DataFrame(index=X_trend_test.index)\nsubmission['num_sold'] = y_test_pred\nsubmission_output = pd.merge(submission, test, how='inner', left_index=True, right_index=True)\nsubmission_output = submission_output.reset_index(drop=True).sort_values(ID_COL)[[ID_COL, TARGET_COL]]\nsubmission_output.to_csv('submission.csv', index=False)\nsubmission_output.head()","d37f9013":"what to try next\n- stacking\/ensemble (catboost, etc)\n- hyperparams tuning strategy\n- use all features for both X1,X2 in boosted model\n- plot resid vs date \n- feature - moving average, ema, etc\n- feature - lag\n- feature - holiday effect instead of only the holiday","9abc305b":"## Submission","db7d586c":"## features","36710161":"#### gdp","389528ab":"#### holiday","7d15fc16":"log\n- ceil to round up prediction\n- long form seems to be better for mutli-series prediction than wide form since each point is a different sample, \nwhile in wide form the same feature set is used for all the different series\n- modified the boosted hybird class to use hyperparams tuning\n- log transform the target (for feature-transforming model) improves the result a bit\n- use gdp (since the model is using the same trend coeff for all the series) improves the score significantly\n- scoring (smaple vs default) in cv search: seems to return the same hyperparams","d1aaea14":"#### trend, seasonality","c5fdbd0f":"## Error analysis","2cd5c2ea":"trying:\n- hyperparams tuning strategy","3a77747d":"#### series label","85bec817":"## Model and Prediction","7bca0377":"#### visualize the data"}}