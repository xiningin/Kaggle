{"cell_type":{"6192092f":"code","1ecec4eb":"code","9e13e94a":"code","72fa591c":"code","fdb4fdb7":"code","e55264fc":"code","d65527c1":"code","bddba6d0":"code","6c5f2dd3":"code","520b9213":"code","1d47b10d":"code","0680d2c2":"code","425a701d":"code","9e148c69":"code","80dfaa10":"code","2da1ea8c":"code","488ad3f6":"code","9ed8cd06":"code","a754d423":"code","d9889d84":"code","b9f8b7a1":"code","68393aef":"code","9d0183ca":"code","659c347a":"code","c6ef4b95":"code","ca7738f6":"code","3c4dabdb":"code","4dcde127":"markdown","c402f117":"markdown","ecac728d":"markdown","963d7cc2":"markdown","81de5a19":"markdown","701ff597":"markdown"},"source":{"6192092f":"from sklearn import preprocessing\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import mean_squared_error\nfrom datetime import datetime\nfrom sklearn.model_selection import train_test_split\nfrom math import sqrt\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.datasets import load_boston\nfrom sklearn.datasets import make_regression\nfrom sklearn.preprocessing import scale\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import FeatureUnion\n\ntry:\n    from sklearn.compose import ColumnTransformer\nexcept ImportError:\n    from future_encoders import ColumnTransformer # Scikit-Learn < 0.20\ntry:\n    from sklearn.impute import SimpleImputer # Scikit-Learn 0.20+\nexcept ImportError:\n    from sklearn.preprocessing import Imputer as SimpleImputer\n    \n%matplotlib inline  ","1ecec4eb":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9e13e94a":"\ntrain =pd.read_csv('..\/input\/ashrae-energy-prediction\/train.csv')\nweather_train = pd.read_csv('..\/input\/ashrae-energy-prediction\/weather_train.csv')\nweather_test = pd.read_csv('..\/input\/ashrae-energy-prediction\/weather_test.csv')\nbuilding_meta = pd.read_csv('..\/input\/ashrae-energy-prediction\/building_metadata.csv')\nsubmission = pd.read_csv('..\/input\/ashrae-energy-prediction\/sample_submission.csv')","72fa591c":"building_meta.head(10)","fdb4fdb7":"#We found out that building meta data contains 14449 samples and 6 features \n#which are site_id,building_id,primary_use,square_feet,year_built and floor_count \nbuilding_meta.count()","e55264fc":"building_meta.info()","d65527c1":"nrows,ncolumns = building_meta.shape","bddba6d0":"#Here we check for any null values\nnullInYearBuilt = (building_meta['year_built'].isnull().sum())\/nrows*100\nprint (nullInYearBuilt)\nnullInFloorCount = (building_meta['floor_count']).isnull().sum()\/nrows*100\nprint (nullInFloorCount)\n#We see that about 53.4% of data in year_built are null and 75% in floor_count\n#This makes these two features difficult to use later on.","6c5f2dd3":"#Visualize the number of buildings in each site\nplt.figure(figsize=[10,6])\nplt.hist(building_meta['site_id'], bins=15, ec='black',color=\"#2196f3\")\nplt.xlabel('Site ID')\nplt.ylabel('No.of building')\nplt.show()","520b9213":"sns.distplot(building_meta['site_id'], bins=20)","1d47b10d":"#Distribution of other features\nplt.figure(figsize=[20,5])\nplt.xticks(fontsize=14)\nplt.subplot(1,4,1)\nplt.title(\"Primary Use\")\nplt.hist(building_meta['primary_use'],bins=25,ec=\"black\")\nplt.xticks(rotation=90)\nplt.subplot(1,4,2)\nplt.title(\"Square Feet\")\nplt.hist(building_meta['square_feet'],bins=20,edgecolor='white',ec=\"black\")\nplt.subplot(1,4,3)\nplt.title(\"Year Built\")\nplt.hist(building_meta['year_built'],bins=20,ec=\"black\")\nplt.subplot(1,4,4)\nplt.xlabel('Floor count')\nplt.hist(building_meta['floor_count'],bins=20,edgecolor='white',ec=\"black\")\nplt.show()\n\n#We get that there are 16 types of building that are mostly built after the 1950s\n#and most buildings are small comparing to the left most side as they are logarithmically distributed.","0680d2c2":"#Finding correlations between features\nbuilding_meta.corr()","425a701d":"mask =  np.zeros_like(building_meta['year_built square_feet floor_count'.split()].corr())\ntriangle_indices = np.triu_indices_from(mask)\nmask[triangle_indices] = True\nplt.figure(figsize =(16,10))\nsns.heatmap(building_meta['year_built square_feet floor_count'.split()].corr(),mask=mask, annot=True, annot_kws={\"size\":14})\nsns.set_style('white')\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()\n# We notice a strong positive correlation between square_feet and floor_count while the other features have small but positive correlation.","9e148c69":"train.head()\n#We see that this data contains our target variable (meter_reading) that is identified by \n#and a relation with building meta data (building_id)","80dfaa10":"train.shape","2da1ea8c":"#We find here that there are no missing data.\npd.isnull(train).sum()","488ad3f6":"#Finding total readings of each building and the correlations with other features.\ntotal_meter_readings = train[['building_id', 'meter_reading']].groupby('building_id').sum()\ntotal_meter_readings = building_meta.merge(total_meter_readings, on='building_id')\ntotal_meter_readings.rename(columns={'meter_reading': 'total_meter_readings'}, inplace=True)\ntotal_meter_readings.head(10)","9ed8cd06":"total_meter_readings['total_meter_readings'].plot(logy=True,style='.', xlabel=\"building\", ylabel=\"total meter reading\")\n#We find here that there are two points that are far away from the other points which might affect our predictions \n#that are considered as outlines hence,these points shall be removed.","a754d423":"minPoint = total_meter_readings.total_meter_readings.sort_values(ascending=True).index[0]\nmaxPoint = total_meter_readings.total_meter_readings.sort_values(ascending=True).index[-1]\ntotal_meter_readings[(total_meter_readings.index==minPoint)|(total_meter_readings.index==maxPoint)]\nprint(minPoint)\nprint(maxPoint)","d9889d84":"#Now let us explore the correlation between meter readings and the other features.\ntotal_meter_readings[(total_meter_readings.index!=minPoint) &(total_meter_readings.index!=maxPoint)][['square_feet', 'year_built', 'floor_count', 'total_meter_readings']].corr()\n#We find the total meter_readings have the strongest correlation with square feet and least with year_built.","b9f8b7a1":"trainMergeBuilding = building_meta.merge(train, on='building_id')\ntrainMergeBuilding['timestamp'] = pd.to_datetime(trainMergeBuilding['timestamp'])\ntrainMergeBuilding.head(10)","68393aef":"# Here we explore the average meter readings in a timeline per building type.\nprim_use_list = trainMergeBuilding['primary_use'].unique()\nfig, axes = plt.subplots(8, 2, figsize=(20, 35))\nfor ax, use in zip(axes.flat[0:], prim_use_list[0:]): \n    prim_use_df = trainMergeBuilding[trainMergeBuilding['primary_use']==use]\n    prim_use_daily = prim_use_df.groupby(['building_id', prim_use_df['timestamp'].dt.date])['meter_reading'].sum()\n    prim_use_daily = prim_use_daily.reset_index()\n    mean = prim_use_daily.groupby('timestamp')['meter_reading'].mean()\n    ax.plot(mean.index, mean)\n    ax.set_title(use)\nfig.add_subplot(111, frameon=False)\nplt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\nplt.grid(False)\nplt.xlabel('Time')\nplt.ylabel('Meter Reading (Daily Sum)', labelpad=20)\nplt.title('Time vs Meter Reading per building type', pad=30)\nplt.show()\ndel(prim_use_list)\ndel(prim_use_df)\ndel(prim_use_daily)\ndel(mean )\n# The data shown is as expected but in Education the outline is shown as follows hence, it will be removed.","9d0183ca":"edu_df = trainMergeBuilding[(trainMergeBuilding['primary_use']=='Education')&(trainMergeBuilding['building_id']!=minPoint)&(trainMergeBuilding['building_id']!=maxPoint)]\nedu_daily = edu_df.groupby(['building_id', edu_df['timestamp'].dt.date])['meter_reading'].sum()\nedu_daily = edu_daily.reset_index()\nedu_mean = edu_daily.groupby('timestamp')['meter_reading'].mean()\naxes[0, 0].plot(edu_mean.index, edu_mean)\naxes[0, 0].set_title('Education With No Outliers')\nmean = edu_daily.groupby('timestamp')['meter_reading'].mean()\nplt.plot(mean.index, mean)\nplt.show()\ndel(edu_df)\ndel(edu_daily)\ndel(edu_mean)\ndel(mean)","659c347a":"# We see that weather train has an hourly data and contains Null values\nweather_train.head(10)","c6ef4b95":"# Let us find out how many null values exist\nnullInAirTemp = (weather_train['air_temperature'].isnull().sum())\nprint (\"Null in Air Temperature: \",nullInAirTemp)\nnullInCloudCoverage = (weather_train['cloud_coverage'].isnull().sum())\nprint (\"Null in Cloud Coverage: \",nullInCloudCoverage)\nnullInDewTemp = (weather_train['dew_temperature'].isnull().sum())\nprint (\"Null in Dew Temperature: \",nullInDewTemp)\nnullInDepth = (weather_train['precip_depth_1_hr'].isnull().sum())\nprint (\"Null in Precip Depth : \",nullInDepth)\nnullInSeaPressure = (weather_train['sea_level_pressure'].isnull().sum())\nprint (\"Null in Sea Level Pressure: \",nullInSeaPressure)\nnullInWindDirection = (weather_train['wind_direction'].isnull().sum())\nprint (\"Null in Wind Direction: \",nullInWindDirection)\nnullInWindSpeed = (weather_train['wind_speed'].isnull().sum())\nprint (\"Null in Wind Speed: \",nullInWindSpeed)\n","ca7738f6":"del(trainMergeBuilding)","3c4dabdb":"# test['timestamp'] = pd.to_datetime(test.timestamp)\n# train['timestamp'] = pd.to_datetime(train.timestamp)\n# train_full = building_meta.merge(train, on='building_id').merge(weather_train, on=['site_id', 'timestamp'])\n# test_full = building_meta.merge(test, on='building_id').merge(weather_test, on=['site_id', 'timestamp'])\n# test_full.head()\n","4dcde127":"## Visualising Data - Historgrams, Distributions and Bar Charts for Building Meta Data","c402f117":"# Building Meta data\n###### We will find out how many feature variables and their distributions","ecac728d":"## Train Data","963d7cc2":"# Notebook Imports","81de5a19":"# Data Imports","701ff597":"## Weather Data"}}