{"cell_type":{"fb6eaace":"code","0c39f92a":"code","58396c3a":"code","9ea4ce22":"code","47c75f6b":"code","8292de6a":"code","53e1b391":"code","7b8d8d2e":"code","7a289d20":"code","3e4e0029":"code","5a650fc9":"code","9bdaaa2f":"code","7cb5597b":"code","4ad3107c":"code","28b931f6":"code","746ab5a1":"code","744aa24b":"code","6b4e95b5":"code","fcb44a1b":"code","574ceb0d":"code","6b7b40e6":"code","441a34ad":"markdown","0a998bc7":"markdown","857019e7":"markdown","6cbff9fe":"markdown","9f2e2964":"markdown","ad4ab5c4":"markdown","c17986ec":"markdown","4b3b15af":"markdown","fdcf7953":"markdown","fc6f16f2":"markdown","f4b094c7":"markdown","6f8b8f22":"markdown","07e0e523":"markdown","6534129d":"markdown","ad7dacfa":"markdown","dc13271e":"markdown","acbb02ae":"markdown","13530745":"markdown","9949d25f":"markdown","6d357f3c":"markdown"},"source":{"fb6eaace":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0c39f92a":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\nprint(\"We drop rows with more than 0 NA entries\\n\")\ntrain_data.info()","58396c3a":"train_data = train_data.drop(columns = \"Cabin\")\ntrain_data = train_data.dropna()\ntrain_data.info()","9ea4ce22":"train_data.head()","47c75f6b":"Y = train_data[\"Survived\"]\nplt.hist(Y)\nplt.show()","8292de6a":"df = train_data.drop(columns = [\"Survived\", \"Name\", \"Ticket\"])\ndf[\"female\"] = pd.get_dummies(df['Sex'])[\"female\"]\ndf.drop(columns = [\"Sex\",\"PassengerId\"], inplace = True)\none_hot = pd.get_dummies(df[\"Embarked\"])[[\"C\", \"Q\"]]\ndf = df.join(one_hot).drop(columns = \"Embarked\")","53e1b391":"import seaborn as sns\n\nmyBasicCorr = df.corr()\nsns.heatmap(myBasicCorr)\nprint(myBasicCorr)","7b8d8d2e":"from sklearn.decomposition import PCA\n\ndf_subset = df.copy()\ndata_subset = df_subset.to_numpy()\npca = PCA(n_components=3)\npca_result = pca.fit_transform(data_subset)\ndf_subset['pca-one'] = pca_result[:,0]\ndf_subset['pca-two'] = pca_result[:,1] \ndf_subset['pca-three'] = pca_result[:,2]\nprint('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n\nprint(pd.DataFrame(pca.components_,columns=df.columns,index = ['PC-1','PC-2', 'PC-3']))\n","7a289d20":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nfare1 = train_data.loc[train_data.Survived == 1][\"Fare\"]\nfare0 = train_data.loc[train_data.Survived == 0][\"Fare\"]\n\nplt.hist(fare1)\nplt.hist(fare0)\nplt.show()","3e4e0029":"from sklearn.model_selection import train_test_split\ntrain_df, valid_df, train_Y, valid_Y = train_test_split(df, Y, test_size=0.2, random_state=42)\nprint(train_df.shape, valid_df.shape)","5a650fc9":"train_X = train_df[\"Fare\"]\nvalid_X = valid_df[\"Fare\"]\n\ndef logit(x):\n    return 1\/(1+np.exp(-x))\n\n# Initialize weights randomly\nnp.random.seed(42)\ntheta0, theta1 = np.random.randn(2)[0], np.random.randn(2)[1]\n\n# Normalising X\ntrain_X = (train_X - np.mean(train_X))\/np.std(train_X)\nvalid_X = (valid_X - np.mean(valid_X))\/np.std(valid_X)\n\n# Substitute outliers\ntrain_X[train_X > 3*np.std(train_X)] = 3*np.std(train_X)\nvalid_X[valid_X > 3*np.std(valid_X)] = 3*np.std(valid_X)\n\n# Learning Rate 0.00001\nalpha = 10e-3\n\n# Number of iterations\nN = 10001\n# Initialize cost\ntrain_cost = np.zeros(N)\naccuracy = np.zeros(N)\nvalid_cost = np.zeros(N)\nfor i in range(N):\n    diff_train = logit(theta0 + theta1*train_X) - train_Y\n    diff_valid = logit(theta0 + theta1*valid_X) - valid_Y\n    train_cost[i] = 1\/len(train_X) * -np.sum(train_Y * np.log(logit(theta0 + theta1*train_X)) + (1 - train_Y) * np.log(1 - logit(theta0 + theta1*train_X)))\n    valid_cost[i] = 1\/len(valid_X) * -np.sum(valid_Y * np.log(logit(theta0 + theta1*valid_X)) + (1 - valid_Y) * np.log(1 - logit(theta0 + theta1*valid_X)))\n    accuracy[i] = 1 - np.sum(np.abs(diff_train))\/len(diff_train)\n    theta0 = theta0 - alpha*(np.sum(diff_train))\n    theta1 = theta1 - alpha*(np.sum(diff_train.dot(train_X)))\/len(diff_train)\n    if i % 250 == 0:\n        print(\"Iteration %d, training_cost: %2.2f, validation_cost: %2.2f, accuracy: %2.2f, theta0: %2.2f, theta1: %2.2f\" % (i, train_cost[i], valid_cost[i], accuracy[i], theta0, theta1))\n        #print(np.sum(train_Y))\n        #print(np.sum(logit(theta0 + theta1*train_X)))\n        #print(np.sum(diff_train))\n","9bdaaa2f":"plt.figure(figsize=(7,6))\nlo = plt.scatter(np.arange(N), train_cost, marker='x', color='r')\nll = plt.scatter(np.arange(N), valid_cost, marker='o', color='g')\n\nplt.legend((lo, ll, ),\n           ('Training', 'Validate'),\n           scatterpoints=1,\n           loc='lower right',\n           ncol=3,\n           fontsize=15)\nplt.show()","7cb5597b":"from sklearn.linear_model import LogisticRegression\ntrain_X = train_df[\"Fare\"]\n\n# Normalising X\ntrain_X = (train_X - np.mean(train_X))\/np.std(train_X)\n\n# Substitute outliers\ntrain_X[train_X > 3*np.std(train_X)] = 3*np.std(train_X)\n\nsk_train_X = np.c_[np.ones(len(train_X)), train_X  ]  # Add bias\nclf = LogisticRegression(random_state=0, solver = \"sag\").fit(sk_train_X, train_Y)\naccuracy = clf.score(sk_train_X, train_Y)\nprint(\"theta0: %2.2f, theta1: %2.2f, accuracy: %2.2f\" %(clf.coef_[0][0], clf.coef_[0][1], accuracy))","4ad3107c":"X = train_X\nY = train_Y\ncost = valid_cost\nz = logit(theta0 + theta1*X)\nerrors = Y - z\n\nRsq = np.sum(errors**2)\/np.sum((Y-np.mean(Y))**2)\nprint(\"R-squared: \", Rsq)\n#For multiple logistic regression\nFstat = Rsq*(len(Y)-2)\/1\n\nprint(\"F-stat: \", Fstat)\nplt.figure(figsize = (10,5))\nplt.plot()\n\nplt.subplot(1,2,1)\nplt.scatter(X, Y)\nplt.scatter(X, z)\nplt.title(\"Errors vs predicted\")\nplt.subplot(1,2,2)\nplt.scatter(z, errors)\nmeanline = np.mean(errors) * np.ones(len(z))\nplt.plot(z, meanline, 'r')\nplt.title(\"Cost function\")\nplt.show()\n","28b931f6":"def replaceOutliers(df):\n    for col in df.columns.to_list():\n        mean = df[col].mean()\n        std = df[col].std()\n        df[col] = np.where(df[col] >3*std, 3*std,df[col])\n        #outliers = (df[col] - mean).abs() > 3*std\n        #df[outliers] = np.nan\n        #df[col].fillna(3*std, inplace=True)\n    return df","746ab5a1":"train_X = train_df[[\"Fare\", \"Age\"]]\nvalid_X = valid_df[[\"Fare\", \"Age\"]]\n\ndef logit(x):\n    return 1\/(1+np.exp(-x))\n\n# Initialize weights randomly\nnp.random.seed(42)\ntheta0, theta1, theta2 = np.random.randn(3)[0], np.random.randn(3)[1], np.random.randn(3)[2]\n# Normalising X\ntrain_X = (train_X - np.mean(train_X, axis = 0))\/np.std(train_X, axis = 0)\nvalid_X = (valid_X - np.mean(valid_X, axis = 0))\/np.std(valid_X, axis = 0)\n# Substitute outliers\ntrain_X = replaceOutliers(train_X)\nvalid_X = replaceOutliers(valid_X)\n# Learning Rate 0.00001\nalpha = 10e-3\n\n# Number of iterations\nN = 10001\n# Initialize cost\ntrain_cost = np.zeros(N)\naccuracy = np.zeros(N)\nvalid_cost = np.zeros(N)\nfor i in range(N):\n    train_z = theta0 + theta1*train_X[\"Fare\"].values + theta2*train_X[\"Age\"].values\n    valid_z = theta0 + theta1*valid_X[\"Fare\"].values + theta2*valid_X[\"Age\"].values\n    diff_train = logit(train_z) - train_Y.values\n    diff_valid = logit(valid_z) - valid_Y.values\n    train_cost[i] = 1\/len(train_X) * -np.sum(train_Y * np.log(logit(train_z)) + (1 - train_Y) * np.log(1 - logit(train_z)))\n    valid_cost[i] = 1\/len(valid_X) * -np.sum(valid_Y * np.log(logit(valid_z)) + (1 - valid_Y) * np.log(1 - logit(valid_z)))\n    accuracy[i] = 1 - np.sum(np.abs(diff_train))\/len(diff_train)\n    theta0 = theta0 - alpha*(np.sum(diff_train))\n    theta1 = theta1 - alpha*(np.sum(diff_train.dot(train_X[\"Fare\"].values)))\/len(diff_train)\n    theta2 = theta2 - alpha*(np.sum(diff_train.dot(train_X[\"Age\"].values)))\/len(diff_train)\n    if i % 250 == 0:\n        print(\"I %d, t_cost: %2.2f, v_cost: %2.2f, accuracy: %2.2f, theta0: %2.2f, theta1: %2.2f, theta2: %2.2f\" % (i, train_cost[i], valid_cost[i], accuracy[i], theta0, theta1, theta2))\n        ","744aa24b":"plt.figure(figsize=(7,6))\nlo = plt.scatter(np.arange(N), train_cost, marker='x', color='r')\nll = plt.scatter(np.arange(N), valid_cost, marker='o', color='g')\n\nplt.legend((lo, ll, ),\n           ('Training', 'Validate'),\n           scatterpoints=1,\n           loc='lower right',\n           ncol=3,\n           fontsize=15)\nplt.show()","6b4e95b5":"train_X = train_df[[\"Fare\", \"Age\"]]\nvalid_X = valid_df[[\"Fare\", \"Age\"]]\n\n# Normalising X\ntrain_X = (train_X - np.mean(train_X, axis = 0))\/np.std(train_X, axis = 0)\nvalid_X = (valid_X - np.mean(valid_X, axis = 0))\/np.std(valid_X, axis = 0)\n\n# Substitute outliers\ntrain_X = replaceOutliers(train_X)\nvalid_X = replaceOutliers(valid_X)\n\nsk_train_X = np.c_[np.ones(len(train_X)), train_X  ]  # Add bias\nclf = LogisticRegression(random_state=0, solver = \"sag\").fit(sk_train_X, train_Y)\naccuracy = clf.score(sk_train_X, train_Y)\nprint(\"theta0: %2.2f, theta1: %2.2f, theta2: %2.2f, accuracy: %2.2f\" %(clf.coef_[0][0], clf.coef_[0][1], clf.coef_[0][2], accuracy))","fcb44a1b":"X = train_X\nY = train_Y\nz = logit(theta0 + theta1*X[\"Fare\"].values + theta2*X[\"Age\"].values)\nerrors = Y - z\n\nRsq = np.sum(errors**2)\/np.sum((Y-np.mean(Y))**2)\nAdjsutedRsq = 1 - (1-Rsq)*(len(Y) - 1)\/(len(Y) - 2-1)\nprint(\"R-squared: \", Rsq)\nprint(\"Adj R-squared: \", AdjsutedRsq)\n#For multiple logistic regression\nFstat = Rsq*(len(Y)-3)\/2\nprint(\"F-stat: \", Fstat)\nplt.figure(figsize = (10,5))\nplt.plot()\n\nplt.subplot(1,2,1)\nplt.scatter(Y, z)\nplt.title(\"Errors vs predicted\")\nplt.subplot(1,2,2)\nplt.scatter(z, errors)\nmeanline = np.mean(errors) * np.ones(len(z))\nplt.plot(z, meanline, 'r')\nplt.title(\"Cost function\")\nplt.show()\n","574ceb0d":"from sklearn.linear_model import LogisticRegression\ntrain_X = train_df[\"Fare\"]\n\n# Normalising X\ntrain_X = (train_X - np.mean(train_X))\/np.std(train_X)\n\n# Substitute outliers\ntrain_X[train_X > 3*np.std(train_X)] = 3*np.std(train_X)\n\nsk_train_X = np.c_[np.ones(len(train_X)), train_X  ]  # Add bias\nclf = LogisticRegression(random_state=0, solver = \"sag\").fit(sk_train_X, train_Y)\naccuracy = clf.score(sk_train_X, train_Y)\nprint(\"theta0: %2.2f, theta1: %2.2f, accuracy: %2.2f\" %(clf.coef_[0][0], clf.coef_[0][1], accuracy))","6b7b40e6":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nX_test = test_data[\"Fare\"]\n# Normalising X\nX_test = (X_test - np.nanmean(X_test))\/np.nanstd(X_test)\n\n# Substitute outliers\nX_test[X_test > 3*np.nanstd(X_test)] = 3*np.nanstd(X_test)\n\n# Replace NAN\nX_test = X_test.fillna(np.nanmean(X_test))\n\nsk_X_test = np.c_[np.ones(len(X_test)), X_test  ]  # Add bias\npredictions = clf.predict(sk_X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","441a34ad":"### Use PCA to identify the variables that explain maximum variance in data\nWe use 3 dimensions in PCA","0a998bc7":"### Separate training and label\nWe plot a histogram to see balance of data","857019e7":"There is no significant correlation between any of the variables","6cbff9fe":"It is clear \"Cabin\" does not have enough values. We remove the column, then remove rows that have null in \"Age\" and \"Embarked\"","9f2e2964":"### One-hot encoding the categorical variables\nWe one-hot encode the two categorical variables = \"Sex\" and \"Embarked\". We remove \"Name\", and \"Ticket\"","ad4ab5c4":"The classes are fairly balanced","c17986ec":"Error Analysis","4b3b15af":"### Plot a correlation heatmap between all variables to identify correlations, if any","fdcf7953":"Now we have all columns with Non-null values","fc6f16f2":"### Data Exploration\nWe check if some columns have null values.\nWe first remove columns with >10% NULL values, then remove the rows that may have null values","f4b094c7":"Our accuracy is still low, and R square has decreased after adding age, so we will keep only \"Fare\" in our final model","6f8b8f22":"\"Fare\" and \"Age\" explain > 99% of the variance in the data. The correlation between the variables > 0.1. We build the model using only those 2 variables.","07e0e523":"### 1. Logistic Regression\nWe code the model, not use the one provided by sklearn","6534129d":"### Building the model\nWe use the bottom-up approach. We use only one variable - \"Fare\"\nBuilding a bar graph between fare and Survived, we see passengers with a lower fare have a lower chance of surviving","ad7dacfa":"### Models used:\n1. Parametric: Logistic Regression\n2. Non-parametric: Decision Trees","dc13271e":"Accuracy is low. theta0 is close to 0 which is expected because we normalised our variables. theta1 is close to 1. We plot our training and validation scores to see if we are overfitting.","acbb02ae":"We are not overfitting, both training and validation losses have converged. So have the theta0 and theta1 values\n\n#### Comparing with sklearn logistic regression model","13530745":"#### Divide the training data into validation set","9949d25f":"We now add age into the logistic regression and see if we get any increase in the accuracy or decrease in cost","6d357f3c":"Sklearn model displayed the same coefficient theta1, theta0 is a little too high in our model"}}