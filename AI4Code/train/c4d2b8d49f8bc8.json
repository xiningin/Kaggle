{"cell_type":{"4885d590":"code","847372cf":"code","a9ab8c34":"code","4b377dfc":"code","3769da36":"code","959eb92b":"code","44bc4eae":"code","f7eac3c8":"code","183ca138":"code","bcffb2a4":"code","82413855":"code","a3a9299e":"code","d214747a":"code","fa2ce99e":"code","8f74c8a0":"code","bff06d9c":"code","7a2be38a":"code","79fd4bab":"code","4dd6c2ba":"code","f7a9fb91":"code","c3dc34b6":"code","c298ca9b":"code","1f29246b":"code","d033b0ee":"code","8186338b":"code","1a6a0cae":"code","fb928643":"code","15617957":"code","46b96850":"code","70c7ffe7":"code","1c62e2c9":"code","e2247d92":"code","01dc95a6":"code","b7d0fa51":"code","026dfffa":"code","34dcd522":"code","5a1c3080":"code","a07d3a80":"code","6ca07822":"code","42a42b2d":"code","8253891a":"code","9c762100":"code","561f3499":"code","3b375123":"code","53585b51":"code","d0a2827f":"code","f7a77905":"code","792c1738":"code","e170a63a":"markdown","79600d57":"markdown","eabb0eb1":"markdown","855b8624":"markdown","e3032793":"markdown","98f0235d":"markdown","76d05a62":"markdown","23304f53":"markdown","15f40486":"markdown","07d70910":"markdown","873ab161":"markdown"},"source":{"4885d590":"import sys\nsys.path = [\n    '..\/input\/efficientnet-pytorch\/EfficientNet-PyTorch\/EfficientNet-PyTorch-master',\n] + sys.path","847372cf":"# !pip install torchsummary","a9ab8c34":"from fastai import *\nfrom fastai.vision import *\nfrom fastai.callbacks.hooks import *\nfrom fastai.callbacks import *\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import cohen_kappa_score,confusion_matrix\nimport matplotlib.image as image\nfrom tqdm.notebook import tqdm\nimport os\nimport gc\nimport zipfile\nimport openslide\nimport cv2\nimport skimage.io as sk\nimport warnings\nimport albumentations as A\nfrom torchvision import transforms\n# from torchsummary import summary\nfrom sys import getsizeof\nwarnings.filterwarnings(\"ignore\")","4b377dfc":"device = torch.device('cuda')","3769da36":"tile_size = 256\nsz = image_size = 256\nN = n_tiles = 36\nbatch_size = 8\nnum_workers = 4\nTRAIN = '\/kaggle\/input\/prostate-cancer-grade-assessment\/train_images\/'","959eb92b":"path1 = Path('\/kaggle\/input\/panda-36-tiles')","44bc4eae":"sld = os.listdir(TRAIN)\nsld = [x[:-5] for x in sld]","f7eac3c8":"df_duplicates = pd.read_csv('..\/input\/duplicates-panda\/duplicates.csv')\n# df_duplicates.head()","183ca138":"duplicate_files = df_duplicates['file2'].tolist()","bcffb2a4":"df = pd.read_csv('\/kaggle\/input\/prostate-cancer-grade-assessment\/train.csv')\ndf = df[df['image_id'].isin(sld)]\ndf = df[~df['image_id'].isin(duplicate_files)]\ndf.columns = ['fn', 'data_provider', 'isup_grade', 'gleason_score']","82413855":"wsi_aug = A.Compose([\n    A.RandomCrop(height=10, width=10, p=0.2),\n    A.Rotate(limit=5, p=0.2)\n])","a3a9299e":"tile_aug = A.Compose([A.OneOf([\n                            A.RGBShift(p=1),\n                            A.RandomGamma(p=1),\n                        ], p=0.5),\n                        A.RandomBrightnessContrast(p=0.7),\n                        A.OneOf([\n                            A.RandomRotate90(p=1),\n                            A.Flip(p=1),\n                            A.Rotate(limit=10, border_mode=0, value=(255, 255, 255), p=1),\n                            A.ShiftScaleRotate(shift_limit=0.15, scale_limit=0.1, rotate_limit=10, border_mode=0, value=(255, 255, 255), p=1),\n                        ], p=0.25),\n                        A.OneOf([\n                            A.Cutout(num_holes=50, max_h_size=10, max_w_size=10, fill_value=0, p=1),\n                            A.Cutout(num_holes=70, max_h_size=7, max_w_size=7, fill_value=0, p=1),\n                            A.Cutout(num_holes=100, max_h_size=5, max_w_size=5, fill_value=0, p=1),\n                        ], p=0.2)])","d214747a":"df['kfold'] = -1\ndf = df.sample(frac=1.,random_state=2020).reset_index(drop=True)\nkf = StratifiedKFold(n_splits=5)\ny = df.isup_grade.values\nfor f,(t_,v_) in enumerate(kf.split(X=df,y=y)):\n    df.loc[v_,'kfold'] = f","fa2ce99e":"# df.head()","8f74c8a0":"import seaborn as sns\nsns.countplot(x=df[df.kfold==1].isup_grade);\nplt.title('Fold - 1: Images count');","bff06d9c":"def get_tiles(img, mode=0):\n        result = []\n        h, w, c = img.shape\n        pad_h = (tile_size - h % tile_size) % tile_size + ((tile_size * mode) \/\/ 2)\n        pad_w = (tile_size - w % tile_size) % tile_size + ((tile_size * mode) \/\/ 2)\n        img2 = np.pad(img,[[pad_h \/\/ 2, pad_h - pad_h \/\/ 2], [pad_w \/\/ 2,pad_w - pad_w\/\/2], [0,0]], constant_values=1)\n        img3 = img2.reshape(\n            img2.shape[0] \/\/ tile_size,\n            tile_size,\n            img2.shape[1] \/\/ tile_size,\n            tile_size,\n            3\n        )\n\n        img3 = img3.transpose(0,2,1,3,4).reshape(-1, tile_size, tile_size,3)\n        if len(img) < n_tiles:\n            img3 = np.pad(img3,[[0,n_tiles-len(img3)],[0,0],[0,0],[0,0]], constant_values=1)\n        idxs = np.argsort(img3.reshape(img3.shape[0],-1).sum(-1))[:n_tiles]\n        img3 = img3[idxs]\n        for i in range(len(img3)):\n            result.append({'img':1 - img3[i], 'idx':i})\n        return result","7a2be38a":"class TiffImageItemList(ImageList):\n    def open(self,fn):\n        path = '\/kaggle\/input\/prostate-cancer-grade-assessment\/train_images\/'\n        fl = path + str(fn)+'.tiff'\n        img = sk.MultiImage(fl)[1]\n        img = wsi_aug(**{'image':img})['image']\n        res = get_tiles(img)\n        imgs = []\n        for i in range(36):\n            im = res[i%len(res)]['img']\n            im = tile_aug(**{'image':im})['image']\n            imgs.append(im)\n        #imgs = np.array(imgs)\n        imgs = [torch.tensor(x) for x in imgs]\n        imgs = torch.div(imgs, 255.0)\n#         final_image = np.concatenate(np.array([np.concatenate(imgs[j:j+6],axis=1).astype(np.uint8) for j in range(0,36,6)]),axis=0)\n#         final_image = cv2.resize(final_image, dsize=(512, 512), interpolation=cv2.INTER_CUBIC)\n        \n#         return vision.Image(pil2tensor(final_image,np.float32).div_(255))\n        return imgs","79fd4bab":"mean, std = imagenet_stats\nmean = 1.0 - torch.tensor(mean)\nstd = 1.0 - torch.tensor(std)\ndef open_image(fn:PathOrStr, div:bool=True, convert_mode:str='RGB', cls:type=Image,\n        after_open:Callable=None)->Image:\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", UserWarning) # EXIF warning from TiffPlugin\n        x = sk.MultiImage(fn)[1]\n        trans = transforms.ToPILImage()\n        x = trans(x).convert(convert_mode)\n#         x = PIL.Image.open(fn).convert(convert_mode)\n    if after_open: x = after_open(x)\n    x = pil2tensor(x,np.float32)\n    if div: x.div_(255)\n    return x #invert image for zero padding\n\nclass MImage(ItemBase):\n    def __init__(self, imgs):\n        self.obj, self.data = \\\n          (imgs), [(imgs[i].data - mean[...,None,None])\/std[...,None,None] for i in range(len(imgs))]\n    \n    def apply_tfms(self, tfms,*args, **kwargs):\n        for i in range(len(self.obj)):\n            self.obj[i] = self.obj[i].apply_tfms(tfms, *args, **kwargs)\n            self.data[i] = (self.obj[i].data - mean[...,None,None])\/std[...,None,None]\n        return self\n    \n    def __repr__(self): return f'{self.__class__.__name__} {img.shape for img in self.obj}'\n    def to_one(self):\n        img = torch.stack(self.data,1)\n        img = img.view(3,-1,N,sz,sz).permute(0,1,3,2,4).contiguous().view(3,-1,sz*N)\n        return Image(1.0 - (mean[...,None,None]+img*std[...,None,None]))\n\nclass MImageItemList(ImageList):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n    \n    def __len__(self)->int: return len(self.items) or 1 \n    \n    def get(self, i):\n        fn = Path(self.items[i])\n#         fnames = [Path(str(fn)+'_'+str(i)+'.png')for i in range(N)]\n        fname = TRAIN + str(fn)+ '.tiff'\n        imgs = open_image(fname, convert_mode=self.convert_mode, after_open=self.after_open)\n        imgs = imgs.permute(1, 2, 0).numpy()\n        res = get_tiles(imgs)\n        imgs = [Image(torch.tensor(x['img']).permute(2, 0, 1)) for x in res]\n#         print(imgs)\n        return MImage(imgs)\n\n    def reconstruct(self, t):\n        return MImage([mean[...,None,None]+_t*std[...,None,None] for _t in t])\n    \n    def show_xys(self, xs, ys, figsize:Tuple[int,int]=(300,50), **kwargs):\n        rows = min(len(xs),8)\n        fig, axs = plt.subplots(rows,1,figsize=figsize)\n        for i, ax in enumerate(axs.flatten() if rows > 1 else [axs]):\n            xs[i].to_one().show(ax=ax, y=ys[i], **kwargs)\n        plt.tight_layout()\n        \n\n#collate function to combine multiple images into one tensor\ndef MImage_collate(batch:ItemsList)->Tensor:\n    result = torch.utils.data.dataloader.default_collate(to_data(batch))\n    if isinstance(result[0],list):\n        result = [torch.stack(result[0],1),result[1]]\n    return result","4dd6c2ba":"# x = sk.MultiImage('..\/input\/prostate-cancer-grade-assessment\/train_images\/0005f7aaab2800f6170c399693a96917.tiff')[1]\n# trans = transforms.ToPILImage()\n# x = trans(x).convert('RGB')\n# x = pil2tensor(x,np.float32)\n# x.div_(255)\n# z = x #invert image for zero padding\n# img = z.permute(1, 2, 0).numpy()","f7a9fb91":"# mode=0\n# result = []\n# h, w, c = img.shape\n# pad_h = (tile_size - h % tile_size) % tile_size + ((tile_size * mode) \/\/ 2)\n# pad_w = (tile_size - w % tile_size) % tile_size + ((tile_size * mode) \/\/ 2)\n# img2 = np.pad(img,[[pad_h \/\/ 2, pad_h - pad_h \/\/ 2], [pad_w \/\/ 2,pad_w - pad_w\/\/2], [0,0]], constant_values=1)\n# img3 = img2.reshape(\n#     img2.shape[0] \/\/ tile_size,\n#     tile_size,\n#     img2.shape[1] \/\/ tile_size,\n#     tile_size,\n#     3\n# )\n\n# img3 = img3.transpose(0,2,1,3,4).reshape(-1, tile_size, tile_size,3)\n# if len(img) < n_tiles:\n#     img3 = np.pad(img3,[[0,n_tiles-len(img3)],[0,0],[0,0],[0,0]], constant_values=1)\n# idxs = np.argsort(img3.reshape(img3.shape[0],-1).sum(-1))[:n_tiles]\n# img3 = img3[idxs]\n# for i in range(len(img3)):\n#     result.append({'img':1 - img3[i], 'idx':i})","c3dc34b6":"fig, ax = plt.subplots(6, 6, figsize = (22, 22))\nfor i,j in enumerate(ax.flatten()):\n    j.imshow(result[i]['img'])","c298ca9b":"trn_idx,val_idx = list(df[df.kfold!=4].index),list(df[df.kfold==4].index)\nrandom.shuffle(trn_idx)\nrandom.shuffle(val_idx)","1f29246b":"data = (MImageItemList.from_df(df,path='',cols='fn')\n                          .split_by_idxs(trn_idx,val_idx)\n                          .label_from_df(cols='isup_grade')\n                          .transform(get_transforms(flip_vert=True,max_rotate=15),size=sz,padding_mode='zeros')\n                          .databunch(num_workers=4,bs=batch_size)\n#                           .normalize(imagenet_stats)\n       )","d033b0ee":"data.show_batch()","8186338b":"# stats = ([0.785946], [0.45007266])\n# data_img = (ImageList.from_df(df,path1,folder='.',suffix='.png',cols='fn')\n#                 .split_by_idxs(trn_idx,val_idx)\n#                 .label_from_df(cols='isup_grade',)\n#                 .transform(get_transforms(do_flip=True), size=300)\n#                 .databunch(bs=batch_size).normalize(imagenet_stats))","1a6a0cae":"# data_img.show_batch(rows=3,figsize=(20,8),seed=2020)","fb928643":"len(data_img.train_ds), len(data_img.valid_ds), data_img.classes, data_img.train_ds[0][0].data[0].shape,data_img.c","15617957":"from efficientnet_pytorch import model as enet","46b96850":"pretrained_model = {\n    'efficientnet-b3': '..\/input\/efficientnet-pytorch\/efficientnet-b3-c8376fa2.pth'\n}\n\nenet_type = 'efficientnet-b3'\nout_dim = 6","70c7ffe7":"enet.EfficientNet.from_name('efficientnet-b3')","1c62e2c9":"class enetv2(nn.Module):\n    def __init__(self, backbone, out_dim):\n        super(enetv2, self).__init__()\n        self.enet = enet.EfficientNet.from_name(backbone)\n        self.enet.load_state_dict(torch.load(pretrained_model[backbone]))\n        nc = self.enet._fc.in_features\n        self.head = nn.Sequential(AdaptiveConcatPool2d(),Flatten(),nn.Linear(2*nc,512),\n                            nn.ReLU(),nn.BatchNorm1d(512), nn.Dropout(0.5),nn.Linear(512,out_dim))\n        self.myfc = nn.Linear(self.enet._fc.in_features, out_dim)\n        self.enet._fc = nn.Identity()\n\n    def extract(self, x):\n        return self.enet(x)\n\n    def forward(self, x):\n        shape = x[0].shape\n        x = torch.stack(x,1).view(-1,shape[1],shape[2],shape[3])\n        #x: bs*N x 3 x 128 x 128\n        x = self.extract(x)\n        #x: bs*N x C x 4 x 4\n        shape = x.shape\n        #concatenate the output for tiles into a single map\n        x = x.view(-1,n,shape[1],shape[2],shape[3]).permute(0,2,1,3,4).contiguous()\\\n          .view(-1,shape[1],shape[2]*n,shape[3])\n        #x: bs x C x N*4 x 4\n        x = self.head(x)\n        #x: bs x n\n        return x\n#         x = self.extract(x)\n#         x = self.myfc(x)\n#         return x","e2247d92":"arch = enetv2(enet_type, out_dim=out_dim)","01dc95a6":"kp = KappaScore()\nkp.weights = 'quadratic'","b7d0fa51":"learn = Learner(data_img, arch , metrics = [kp] , model_dir = '\/kaggle\/working\/')","026dfffa":"learn.lr_find()\nlearn.recorder.plot()","34dcd522":"gc.collect()","5a1c3080":"cb2 = SaveModelCallback(learn, monitor = 'kappa_score', every = 'improvement', mode='max', name = 'best_model_ft' )\ncb3 = ReduceLROnPlateauCallback(learn,  monitor = 'kappa_score', mode = 'max',factor = 0.2,patience = 4, min_delta = 0.01)","a07d3a80":"#learn.split([arch.myfc])","6ca07822":"# epochs = 4\n# learn.fit_one_cycle(epochs ,max_lr = 1e-3, callbacks = [cb2,cb3])","42a42b2d":"learn.unfreeze()\nlearn.fit_one_cycle(6 ,max_lr = 1e-3, callbacks = [cb2,cb3])","8253891a":"learn.recorder.plot_losses()","9c762100":"learn.load('best_model_ft');","561f3499":"learn.export('\/kaggle\/working\/panda.pkl')","3b375123":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()","53585b51":"# learn = learn.to_fp32()","d0a2827f":"# test_df = pd.read_csv('\/kaggle\/input\/prostate-cancer-grade-assessment\/test.csv')\n# df.drop('kfold', axis=1, inplace=True)\n# df.columns = ['image_id', 'data_provider', 'isup_grade', 'gleason_score']\n# data_dir = '..\/input\/prostate-cancer-grade-assessment'\n# image_folder = os.path.join(data_dir, 'test_images')\n# is_test = os.path.exists(image_folder)  # IF test_images is not exists, we will use some train images.\n# image_folder = image_folder if is_test else os.path.join(data_dir, 'train_images')\n\n# test = test_df if is_test else df.sample(n=100)","f7a77905":"# def image_test(fn,image_folder):     \n#     path2 = image_folder +'\/'\n#     fl = path2 + str(fn)+'.tiff'\n#     img = sk.MultiImage(fl)[1]\n#     res = get_tiles(img)\n#     imgs = []\n#     for i in range(36):\n#         im = res[i%len(res)]['img']\n#         imgs.append(im)\n#     imgs = np.array(imgs)\n#     final_image = np.concatenate(np.array([np.concatenate(imgs[j:j+6],axis=1).astype(np.uint8) for j in range(0,36,6)]),axis=0)\n#     final_image = cv2.resize(final_image, dsize=(300, 300), interpolation=cv2.INTER_CUBIC)\n#     return vision.Image(pil2tensor(final_image,np.float32).div_(255))","792c1738":"# ts_name = test.image_id.values\n# pred = np.zeros(len(ts_name))\n    \n# for j in tqdm(range(len(ts_name))):\n#     ans = int(learn.predict(image_test(ts_name[j],image_folder))[0])\n#     pred[j] = ans\n        \n# out = pd.DataFrame({'image_id':ts_name,'isup_grade':pred.astype(int)})\n# out.to_csv('submission.csv',index=False)","e170a63a":"## Databunch of Processed Images: Using fastai's own ImageList","79600d57":"### Metrics Kappa Score","eabb0eb1":"## Inference Kernel can be found [**here**](https:\/\/www.kaggle.com\/ianmoone0617\/panda-effnet-b3-inference-fastai-custom-imagelist)","855b8624":"## DataBunch of Custom TiffImageItemList ","e3032793":"# Data Processing for fastai \n* We have 2 options either we write a custom Imagelist function or\n* We first convert all images first then use then As we like.\n\nLater will take time at first but will Speed up process later. As Fastai datablock will not have to process large **.tiff** files every time","98f0235d":"## Custom Fastai TiffImageList to Directly Process Slides","76d05a62":"## Model Efficient-B3","23304f53":"# Prostate cANcer graDe Assessment (PANDA) Challenge\n### Prostate cancer diagnosis using the Gleason grading system","15f40486":"* Train and validation split","07d70910":"* I have converted the tiff files they can be found [**here**](https:\/\/www.kaggle.com\/ianmoone0617\/panda-36-tiles-resize)\n* Lets start with Custum ImageItem List first","873ab161":"## Stratified Kfold"}}