{"cell_type":{"de71b761":"code","c9d53ac1":"code","3f3407e4":"code","6c3c36a4":"code","123ac541":"code","d53b453c":"code","50f937dc":"code","ef3c6bd9":"code","e9a67570":"code","8d088e0e":"code","17ca1934":"code","92ce7d56":"code","c4bb7d9c":"code","59f027d4":"code","ff2c1b03":"code","2b091950":"code","6ec35baa":"code","6cc25011":"code","465eebee":"code","429cc6cb":"code","2f155948":"code","8e126d09":"markdown","0f0932be":"markdown","87f2c71b":"markdown","aaefcf87":"markdown","31a75b43":"markdown","6f6ff99a":"markdown","a9af83b8":"markdown","94c8d31c":"markdown","34b7f510":"markdown","0340363a":"markdown","99b1e431":"markdown","1bc92c38":"markdown","701c2e30":"markdown","eee9bb7a":"markdown","4c083e28":"markdown","ab43370b":"markdown","aecb9de3":"markdown","24800f0e":"markdown","036a4303":"markdown","812ee0ea":"markdown","9c5c5149":"markdown","ff7d8080":"markdown","1b906048":"markdown","09d8c61a":"markdown","18718f31":"markdown"},"source":{"de71b761":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import roc_auc_score\nimport tqdm\nfrom sklearn.linear_model import LogisticRegression","c9d53ac1":"# GET INDICIES OF REAL TEST DATA FOR FE\n#######################\n# TAKE FROM YAG320'S KERNEL\n# https:\/\/www.kaggle.com\/yag320\/list-of-fake-samples-and-public-private-lb-split\n\ntest_path = '..\/input\/test.csv'\ntrain_path = '..\/input\/train.csv'\n\ndf_test = pd.read_csv(test_path)\ndf_test.drop(['ID_code'], axis=1, inplace=True)\ndf_test = df_test.values\n\nunique_samples = []\nunique_count = np.zeros_like(df_test)\nfor feature in range(df_test.shape[1]):\n    _, index_, count_ = np.unique(df_test[:, feature], return_counts=True, return_index=True)\n    unique_count[index_[count_ == 1], feature] += 1\n\n# Samples which have unique values are real the others are fake\nreal_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\nsynthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n\nprint('Found',len(real_samples_indexes),'real test')\nprint('Found',len(synthetic_samples_indexes),'fake test')\n\n###################\n\nd = {}\nfor i in range(200): d['var_'+str(i)] = 'float32'\nd['target'] = 'uint8'\nd['ID_code'] = 'object'\n\ntrain = pd.read_csv('..\/input\/train.csv', dtype=d)\ntest = pd.read_csv('..\/input\/test.csv', dtype=d)\n\nprint('Loaded',len(train),'rows of train')\nprint('Loaded',len(test),'rows of test')\nprint('Found',len(real_samples_indexes),'real test')\nprint('Found',len(synthetic_samples_indexes),'fake test')\n\n###################\n\nd = {}\nfor i in range(200): d['var_'+str(i)] = 'float32'\nd['target'] = 'uint8'\nd['ID_code'] = 'object'\n\ntrain = pd.read_csv(train_path, dtype=d)\ntest = pd.read_csv(test_path, dtype=d)\n\nprint('Loaded',len(train),'rows of train')\nprint('Loaded',len(test),'rows of test')","3f3407e4":"# FREQUENCY ENCODE\ndef encode_FE(df,col,test):\n    cv = df[col].value_counts()\n    nm = col+'_FE'\n    df[nm] = df[col].map(cv)\n    test[nm] = test[col].map(cv)\n    test[nm].fillna(0,inplace=True)\n    if cv.max()<=255:\n        df[nm] = df[nm].astype('uint8')\n        test[nm] = test[nm].astype('uint8')\n    else:\n        df[nm] = df[nm].astype('uint16')\n        test[nm] = test[nm].astype('uint16')        \n    return\n\ntest['target'] = -1\ncomb = pd.concat([train,test.loc[real_samples_indexes]],axis=0,sort=True)\nfor i in range(200): \n    encode_FE(comb,'var_'+str(i),test)\ntrain = comb[:len(train)]; del comb\nprint('Added 200 new magic features!')","6c3c36a4":"df_train_data = train.drop(columns=['ID_code'])\ndf_test_data = test.drop(columns=['ID_code'])","123ac541":"import numpy as np\nimport scipy.interpolate as interpolate\n\ndef inverse_transform_sampling(data, n_bins, n_samples, draw_hist=False):\n    # This function returns samples with the same distribution of data\n    hist, bin_edges = np.histogram(data, bins=n_bins, density=True)\n    cum_values = np.zeros(bin_edges.shape)\n    cum_values[1:] = np.cumsum(hist*np.diff(bin_edges))\n    inv_cdf = interpolate.interp1d(cum_values, bin_edges)\n    r = np.random.rand(n_samples)\n    samples = inv_cdf(r)\n    if draw_hist:\n        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,4))\n        ax1.hist(data, n_bins, density=True)\n        ax1.set_title('Original Data Hist')\n        ax2.hist(samples, n_bins, density=True)\n        ax2.set_title('Sampled Data Hist')\n        plt.show()\n    return samples","d53b453c":"var_i = 82\ndata_0 = df_train_data[df_train_data['target']==0][f'var_{var_i}'].values\ndata_1 = df_train_data[df_train_data['target']==1][f'var_{var_i}'].values\nprint(f'For target 0 and var_{var_i}')\nsamples_0 = inverse_transform_sampling(data_0, 100, len(data_0), draw_hist=True)\nprint(f'For target 1 and var_{var_i}')\nsamples_1 = inverse_transform_sampling(data_1, 50, len(data_1), draw_hist=True)\ncount_1 = len(set(np.round(data_1, 4)))\ncount_sample_1 = len(set(np.round(samples_1, 4)))\ncount_0 = len(set(np.round(data_0, 4)))\ncount_sample_0 = len(set(np.array(samples_0*10000, dtype=int)))\nprint(f'Target 1: Unique original data count({count_1}) vs Unique sampled data({count_sample_1}): {count_1\/count_sample_1}')\nprint(f'Target 0: Unique original data count({count_0}) vs Unique sampled data({count_sample_0}): {count_0\/count_sample_0}')","50f937dc":"counts_1 = []\ncounts_0 = []\ncounts_sample_1 = []\ncounts_sample_0 = []\nfor var_i in tqdm.tqdm(range(200)):\n    data_0 = df_train_data[df_train_data['target']==0][f'var_{var_i}'].values\n    data_1 = df_train_data[df_train_data['target']==1][f'var_{var_i}'].values\n    samples_0 = inverse_transform_sampling(data_0, 100, len(data_0))\n    samples_1 = inverse_transform_sampling(data_1, 50, len(data_1))\n    count_1 = len(set(np.round(data_1, 4)))\n    count_sample_1 = len(set(np.round(samples_1, 4)))\n    count_0 = len(set(np.round(data_0, 4)))\n    count_sample_0 = len(set(np.round(samples_0, 4)))\n    counts_1.append(count_1)\n    counts_0.append(count_0)\n    counts_sample_1.append(count_sample_1)\n    counts_sample_0.append(count_sample_0)","ef3c6bd9":"ones_quotient = np.array(counts_1)\/np.array(counts_sample_1)\nprint(ones_quotient.mean(), ones_quotient.std())\nzeros_quotient = np.array(counts_0)\/np.array(counts_sample_0)\nprint(zeros_quotient.mean(), zeros_quotient.std())","e9a67570":"plt.plot(ones_quotient)\nplt.plot(zeros_quotient)\nplt.show()","8d088e0e":"plt.hist(zeros_quotient, 20)\nplt.hist(ones_quotient, 20)\nplt.show()","17ca1934":"def create_dataset(append_counts, decimals_ones=4, decimals_zeros=4,  N_ones = 20_000, N_zeros = 180_000, mean = 0, std = 3):\n    # Mean and std could be changed. The selected where to demostrate the effect\n\n    # Sample normal distribution variable and round it with decimals_ones decimals\n    mult = np.power(10, decimals_ones)\n    normal_x_ones = np.array(mult*np.random.normal(mean, std, (N_ones,1)), dtype=int)\/mult\n    # Append ones\n    data_ones = np.append(normal_x_ones, np.ones((N_ones,1)), axis=1)\n\n    # Sample normal distribution variable and round it with decimals_zeros decimals\n    mult = np.power(10, decimals_zeros)\n    normal_x_zeros = np.array(mult*np.random.normal(mean, std, (N_zeros,1)), dtype=int)\/mult\n    # Append zeros\n    data_zeros = np.append(normal_x_zeros, np.zeros((N_zeros,1)), axis=1)\n\n    # Append zeros with ones\n    data = np.append(data_zeros, data_ones, axis=0)\n    X_train = data[:,0].reshape(-1,1)\n    y_train = data[:,1]\n\n    if append_counts:\n        # Append counts\n        values, indexes, inv, count = np.unique(X_train, return_index=True, return_inverse=True, return_counts=True)\n        count_data = count[inv].reshape(-1,1)\n\n        X_train = np.append(X_train, count_data, axis=1)\n        \n    return X_train, y_train","92ce7d56":"X_train, y_train = create_dataset(append_counts=False, decimals_ones=4, decimals_zeros=4)\nprint('X_train:\\n', X_train)\nprint('y_train:\\n', y_train)","c4bb7d9c":"clf = LogisticRegression(solver='lbfgs')\nclf.fit(X_train, y_train)\nprint(f'Accuracy: {clf.score(X_train, y_train)}')\nprint(f'AUC ROC: {roc_auc_score(y_train, clf.predict_proba(X_train)[:,1])}')","59f027d4":"X_train, y_train = create_dataset(append_counts=True, decimals_ones=4, decimals_zeros=4)\nprint('X_train:\\n', X_train)\nprint('y_train:\\n', y_train)","ff2c1b03":"clf = LogisticRegression(solver='lbfgs')\nclf.fit(X_train, y_train)\nprint(f'Accuracy: {clf.score(X_train, y_train)}')\nprint(f'AUC ROC: {roc_auc_score(y_train, clf.predict_proba(X_train)[:,1])}')","2b091950":"X_train, y_train = create_dataset(append_counts=True, decimals_ones=4, decimals_zeros=3)\nprint('X_train:\\n', X_train)\nprint('y_train:\\n', y_train)","6ec35baa":"clf = LogisticRegression(solver='lbfgs')\nclf.fit(X_train, y_train)\nprint(f'Accuracy: {clf.score(X_train, y_train)}')\nprint(f'AUC ROC: {roc_auc_score(y_train, clf.predict_proba(X_train)[:,1])}')","6cc25011":"decimals_zeros = 3.999\nX_train, y_train = create_dataset(append_counts=True, decimals_ones=4, decimals_zeros=decimals_zeros)\nprint('X_train:\\n', X_train)\nprint('y_train:\\n', y_train)\nprint(f'Equivalent as multipling by: {np.power(10, decimals_zeros)} (more optimistic case than Santander)')","465eebee":"clf = LogisticRegression(solver='lbfgs')\nclf.fit(X_train, y_train)\nprint(f'Accuracy: {clf.score(X_train, y_train)}')\nprint(f'AUC ROC: {roc_auc_score(y_train, clf.predict_proba(X_train)[:,1])}')","429cc6cb":"X_train, y_train = create_dataset(append_counts=False, decimals_ones=4, decimals_zeros=3)\nprint('X_train:\\n', X_train)\nprint('y_train:\\n', y_train)","2f155948":"clf = LogisticRegression(solver='lbfgs')\nclf.fit(X_train, y_train)\nprint(f'Accuracy: {clf.score(X_train, y_train)}')\nprint(f'AUC ROC: {roc_auc_score(y_train, clf.predict_proba(X_train)[:,1])}')","8e126d09":"# Generate Santander Artificial Data\n\nIn this section we generate data with the same distribution of the data in Santander. For each var we estimate the pdf and sample it generating artificial data.\n\nThen we count the number of uniques values and compare them with the Original dataset","0f0932be":"As expected, in this case we can predicted based on count because of the difference in rounding","87f2c71b":"# General Idea of this notebook","aaefcf87":"> # Load Dataset, divide fake from real and add counts","31a75b43":"You can change var_i to select the column you want to sample","6f6ff99a":"## 4 decimals for ones, 4 decimals for zeros, Counts column present","a9af83b8":"## 4 decimals for ones, 4 decimals for zeros, Counts column not present","94c8d31c":"As expected, there is no prediction capabilty here","34b7f510":"I have been struggling on trying to understand why adding the count column to each var really improoves the AUC ROC. The fact that we got almost the same results with a [CNN](https:\/\/www.kaggle.com\/jganzabal\/cnn-independence-counts-magic-0-92174-private) than with LGBM when adding the Count columns made me try to understand why. At first I thought it had to do with some intentional data manipulation Santander people did, and that maybe variables had some categorical component.\n\nAfter some analysis I got into the conclusion that adding count works because of a rounding effect and the fact that the rounding is not exactly the same for data with target 1 and data with target 0.\n\nProbably for people who realize to add the Count column this was obvious, but crearly not for me. If you have any comments or new ideas, just let me know.\n\nBased on this idea, would it be possible to come up with another feature that helps? The 2nd place notebook [here](https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/discussion\/88939#latest-525018), uses different decimal roundings  \n\nTo sum up, the hipothesis I try to show here is that Count feature is a good predictor when data is drawn from the same distributions (Or similar ones) for both classes but, because of some different pre-processing for each class, the rounding effect generate some assimetry that can be used to enhance predictions","0340363a":"# Simulations","99b1e431":"## Not integer rounding\nThe effect in santander of course is not one complete decimal rounding, so we simulate rounding not a full decimal. I suspect the diference in rounding difference between ones and zeros in santander might has to do with some operation done before the final rounding\n\nA 4 decimal rounding can be though as:  \ncast_int(10.000*data)\/10.000\n\nA not full decimal rounding can be though as:  \ncast_int(9.900*data)\/9.900\n\nWe can try to estimate the \"rounding\" in Santader by finding the rounding that gives equal ratio of counts for both ones and zeros:\nIn my calculations I got about 9900","1bc92c38":"# Import libraries","701c2e30":"Here you can see that the number of uniques for sampled data and original data (Santander) are both similar for both targets, but it seems that target 0 for Santander data has more repetitions than expected (less uniques). The hipothesys is that it works as if there was some sort of \"more\" rounding for observations with target 0","eee9bb7a":"## Doing it for all vars","4c083e28":"## 4 decimals for ones, 3 decimals for zeros, Counts column present","ab43370b":"Also as expected, there is no prediction capabilty neither when you add counts if the rounding is the same","aecb9de3":"This is taken from https:\/\/www.kaggle.com\/yag320\/list-of-fake-samples-and-public-private-lb-split","24800f0e":"# Train model","036a4303":"Lets sample 2 random valiables with exact same gaussian distributions\n\nOne for target 1 (20.000 samples), and the other for target 0 (180.000 samples)\n\nThe expected results here will be that a Logistic regression can not predict anything if the two random variables are sampled from the same distributions, but if we round them differently it will","812ee0ea":"## Mean, stds and plots of ratios for all vars","9c5c5149":"## 4 decimals for ones, 3 decimals for zeros, Counts column NOT present\nThis is the last combination and the idea is to show that even though the decimals for ones are zeros are different, if you don't add the Counts columns, because they are from the same distribution, the predictor can't do anything","ff7d8080":"This is taken from https:\/\/www.kaggle.com\/cdeotte\/200-magical-models-santander-0-920\/comments, a must read kernel from @cdeotte","1b906048":"As expected, the AUC ROC does not improove","09d8c61a":"Still improoves the AUC ROC even though the rouning is really subtle","18718f31":"# TODO\n- Try to find new features other than count that improove the AUC ROC to then try them on Santanders dataset"}}