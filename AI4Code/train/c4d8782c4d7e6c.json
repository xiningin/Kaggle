{"cell_type":{"f6cd0240":"code","15de06cc":"code","ca627738":"code","add91faf":"code","5fb24d9c":"code","c022a800":"code","1b10506b":"code","be192642":"code","271fc3f1":"code","a366bd0f":"code","9e1709ff":"code","3cc632a8":"code","d6ae3c0c":"code","0171e376":"code","9a6dc226":"code","132bcbfe":"code","d6d3f708":"code","208c9b6a":"code","15b56b72":"code","266697db":"code","40d4c985":"code","e0998486":"code","861dbd84":"code","e363653d":"code","8d8817a3":"code","c3cebce1":"code","5d28a96e":"code","c098be0e":"code","ded487a7":"markdown","8464243f":"markdown","97993b55":"markdown","2480f5df":"markdown","8bce1cc9":"markdown","e782f645":"markdown","06db5db5":"markdown"},"source":{"f6cd0240":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","15de06cc":"import plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.datasets import load_digits\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nimport lightgbm as lgb\n#import optuna.integration.lightgbm as lgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ca627738":"pd.set_option('display.max_columns', 100)","add91faf":"sumple_submission = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv\")","5fb24d9c":"sumple_submission","c022a800":"df_train = pd.DataFrame(train)\ndf_train","1b10506b":"plt.figure(figsize=(10,6))\nsns.countplot(x='target', data=df_train, order=df_train['target'].value_counts().index)","be192642":"#df_train.info()","271fc3f1":"df_train.drop(columns=['id', 'target']).describe().T\\\n        .style.bar(subset=['mean'], color=px.colors.qualitative.G10[0])\\\n        .background_gradient(subset=['std'], cmap='Greens')\\\n        .background_gradient(subset=['50%'], cmap='BuGn')","a366bd0f":"df_test = pd.DataFrame(test)\ndf_test","9e1709ff":"#df_test.info()","3cc632a8":"df_test.drop(columns=['id']).describe().T\\\n        .style.bar(subset=['mean'], color=px.colors.qualitative.G10[0])\\\n        .background_gradient(subset=['std'], cmap='Greens')\\\n        .background_gradient(subset=['50%'], cmap='BuGn')","d6ae3c0c":"df_train['target'] = df_train['target'].map({'Class_1':0, 'Class_2':1, 'Class_3':2, 'Class_4':3})","0171e376":"df_train","9a6dc226":"df_test","132bcbfe":"df_train_corr = df_train.corr()\ndf_train_corr","d6d3f708":"plt.figure(figsize=(20,10))\nsns.heatmap(df_train_corr, vmin=-0.03, vmax=0.03, center=0, square=False, annot=False, cmap='coolwarm');","208c9b6a":"predictor_cols = []\nfor i in df_train_corr:\n    innerName = df_train_corr[i].name\n    if df_train_corr[i]['target'] > 0 or df_train_corr[i]['target'] < -0:\n        if innerName != 'id' and innerName != 'target':\n            predictor_cols.append(innerName)\n#predictor_cols","15b56b72":"x = pd.DataFrame(df_train[predictor_cols])\nt = pd.DataFrame(df_train['target'])\n\nx = np.array(x)\nt = np.array(t)\nt = t.ravel()\n\nx = x.astype('float32')\nt = t.astype('int32')","266697db":"# split data for train and test\nx_train, x_test, t_train, t_test = train_test_split(x, t, test_size=0.2, random_state=128)# (0.2) (29,41,55,68,70,122,128,155)","40d4c985":"# XGBoost\ndtrain = xgb.DMatrix(x_train, label=t_train)\ndtest = xgb.DMatrix(x_test, label=t_test)\nxgb_params = {\n        # \u591a\u5024\u5206\u985e\u554f\u984c(multi:softprob\uff1a\u5404\u30af\u30e9\u30b9\u306b\u5c5e\u3059\u308b\u78ba\u7387\u3001multi:softmax\uff1a\u4e88\u6e2c\u3057\u305f\u30af\u30e9\u30b9)\n        'objective': 'multi:softprob',\n        'num_class': 4,\n        'learning_rate': 0.05,\n        'eval_metric': 'mlogloss',\n        'subsample': 0.5,# 1\n        'colsample_bytree': 1,# 1\n        'colsample_bylevel': 1,# 1\n        'eta': 0.3,# 0.3\n        'reg_alpha': 0.7,# 0\n        'reg_lambda': 0.9,# 1\n        'max_depth': 3,# 6\n        'min_child_weight': 1# 1\n    }\n ","e0998486":"# Training\nevals = [(dtrain, 'train'), (dtest, 'eval')]\nevals_result = {}\nbst = xgb.train(xgb_params,\n                dtrain,\n                num_boost_round=10000,\n                early_stopping_rounds=10,\n                evals=evals,\n                evals_result=evals_result,\n                verbose_eval=10\n                )","861dbd84":"pred = bst.predict(dtest, ntree_limit=bst.best_ntree_limit)\npred_max = np.argmax(pred, axis=1)\n\n# Accuracy\nacc = accuracy_score(t_test, pred_max)\nprint('Accuracy:', acc)","e363653d":"# Feature importance\nfig, ax = plt.subplots(figsize=(10, 10))\nxgb.plot_importance(bst, ax=ax)","8d8817a3":"# Training performance\nplt.plot(evals_result['train']['mlogloss'], label='train')\nplt.plot(evals_result['eval']['mlogloss'], label='eval')\nplt.ylabel('Log loss')\nplt.xlabel('Boosting round')\nplt.title('Training performance')\nplt.legend()\nplt.show()","c3cebce1":"testData = pd.DataFrame(df_test[predictor_cols])\ntestData = np.array(testData)\ntestData = testData.astype('float32')\ntestData = xgb.DMatrix(testData)","5d28a96e":"result = bst.predict(testData)","c098be0e":"outputArray = []\nid = 100000\nfor i in range(len(result)):\n    predict = result[i]\n    innerArray = [id, predict[0], predict[1], predict[2], predict[3]]\n    outputArray.append(innerArray)\n    id += 1\ndf = pd.DataFrame(outputArray, columns=['id', 'Class_1', 'Class_2', 'Class_3', 'Class_4'])\ndf.to_csv(path_or_buf='submission.csv', index=False)\ndf","ded487a7":"# 4. Extract items with high correlation coefficient","8464243f":"# 7. Make submission file","97993b55":"# 5. Modeling","2480f5df":"# 6. Prediction","8bce1cc9":"# 1. Import data","e782f645":"# 3. Check the correlation between each item","06db5db5":"# 2. Preprocessing"}}