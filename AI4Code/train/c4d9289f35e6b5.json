{"cell_type":{"d548b863":"code","ae58c716":"code","fe9d1db9":"code","ec94c679":"code","c4a5788a":"code","32397e5f":"code","9aabb5a7":"code","831c1633":"code","06b1db9d":"code","4c303817":"code","fcc15278":"code","d8310cb5":"code","8c212ff8":"code","4664ce92":"code","d24420b6":"code","a4027dfc":"code","101861d4":"code","b240cc99":"code","b7817721":"code","2a5151ec":"code","afd0ebe2":"code","645503bd":"code","7f9eec33":"code","57947b20":"markdown","442bd46d":"markdown","a518326f":"markdown","bdc1a51e":"markdown","42a30150":"markdown","844f374a":"markdown","fc472acf":"markdown","c51d8752":"markdown","1cd3c4bd":"markdown","2d916595":"markdown","76c13c8f":"markdown","fbe171a0":"markdown","dc22984c":"markdown","247b0278":"markdown","a90dd303":"markdown","776f56d6":"markdown","368152f4":"markdown","ca826ee5":"markdown","6b787ba7":"markdown","5c0bfb24":"markdown","b2786669":"markdown","6597cefb":"markdown","e6fda7a1":"markdown","c7fd76a8":"markdown","efb58495":"markdown","b17b2080":"markdown","012acb5f":"markdown","14df936d":"markdown","f362cdcc":"markdown","3242bbde":"markdown","c5fd6a61":"markdown"},"source":{"d548b863":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport re\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","ae58c716":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntrain.drop(['location','keyword'],axis = 1,inplace = True)","fe9d1db9":"train.target.hist();\nprint(\"Negative target (not a disaster) target, % : {}\\nPoistive target (it is a disaster), %      : {}\".format(train.target.value_counts()[0]\/len(train)*100,\n                                                                                                                 train.target.value_counts()[1]\/len(train)*100))","ec94c679":"for i in range(50):\n    print(train.text[i])","c4a5788a":"def to_lowercase(text):\n    return text.lower()\n    # Alternativly:\n    # train.text = train.text.str.lower()\n    # train.text = train.text.apply(lambda x: x.lower())\n    \ntext = 'THIS Is a RaNdOm SenTence with CAPITAL LetTers'\nprint(text,'===>',to_lowercase(text))","32397e5f":"# remove urls tags\ndef remove_url(text):\n    return re.sub(r\"https?:\\\/\\\/t.co\\\/[A-Za-z0-9]+\", \"\", text)\n\ndef assign_user(text,remove = True):\n    if remove:\n        text = re.sub(r\"\\@[A-Za-z0-9]+\", \"\", text)\n    else:\n        text = re.sub(r\"\\@[A-Za-z0-9]+\", \"USER\", text)\n    return text\n\ntext1 = 'http:\/\/t.co\/lHYXEOHY6C'\ntext2 = '@alex reported a disaster'\n\nprint(text1,'===>',remove_url(text1))\nprint(text2,'===>',assign_user(text2))\nprint(text2,'===>',assign_user(text2,remove = False))","9aabb5a7":"def remove_accented_chars(text):\n    # https:\/\/www.kdnuggets.com\/2019\/04\/text-preprocessing-nlp-machine-learning.html\n    import unicodedata\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return text\n\ntext = 'S\u00f3m\u011b \u00c1cc\u011bnt\u011bd t\u011bxt'\n\nprint(text,'===>',remove_accented_chars(text))","831c1633":"# from the notebook from the begining\n\nabbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}\n\ndef expand_abrivviation(text,mapping = abbreviations):\n    \n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text\n\ntext = 'ynk wtf is going on'\n\nprint(text,'===>',expand_abrivviation(text))","06b1db9d":"# https:\/\/www.kaggle.com\/theoviel\/improve-your-score-with-some-text-preprocessing\n\ncontraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \n                       \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n                       \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \n                       \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n                       \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n                       \"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n                       \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \n                       \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n                       \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n                       \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \n                       \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \n                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \n                       \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \n                       \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \n                       \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n\ndef expand_contractions(text,mapping = contraction_mapping):\n    specials =[\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in specials:\n        text = text.replace(s,\"'\")\n    \n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text\n\ntext = \"y'all'd've to learn machine learning\"\nprint(text,'===>',expand_contractions(text))","4c303817":"def remove_special_characters(text):\n    pat = r'[^a-zA-z0-9.,!?\/:;\\\"\\'\\s]' \n    return re.sub(pat, '', text)\n\ntext = ' # & * are special characters'\nprint(text,'===>',remove_special_characters(text))","fcc15278":"def remove_punctuation(text):\n    import string\n    text = ''.join([c for c in text if c not in string.punctuation])\n    return text\n\ntext = ' Sentence, with: different. king of ; punctuations...'\nprint(text,'===>',remove_punctuation(text))","d8310cb5":"def remove_numbers(text):\n    import re\n    pattern = r'[^a-zA-z.,!?\/:;\\\"\\'\\s]+' \n    return re.sub(pattern, '', text)\n\ntext = ' 1984 We are only in the beggining of 2020'\nprint(text,'===>',remove_numbers(text))","8c212ff8":"def remove_extra_whitespace_tabs(text):\n    import re\n    pattern = r'^\\s*|\\s\\s*'\n    return re.sub(pattern, ' ', text).strip()\n\ntext = ' # & * are special characters'\ntext1 = remove_punctuation(text)\nprint(text1, 'containes extra spaces')\nprint(text,'===>',text1,'===>',remove_extra_whitespace_tabs(text1))","4664ce92":"def remove_emojify(text):\n    #https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove#Data-Cleaning\n    import re\n    regrex_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags = re.UNICODE)\n    return regrex_pattern.sub(r'',text)","d24420b6":"from nltk import word_tokenize          \nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords \n\ndef remove_stop(text):\n    return \" \".join ([word for word in word_tokenize(text) if not word in stopwords.words('english')])\n\n\ntext = 'Here I am trying to show you a sample sentence with stopwords filtration'\nprint(text,'===>',remove_stop(text),'----- am to you with are removed')\n\n\ndef stem(text):\n    porter = PorterStemmer()\n    return \" \".join([porter.stem(word) for word in word_tokenize(text)])\n\nprint('cat cats dog dogs categories','===>',stem('cat cats dog dogs categories'))\n\n\ndef lemma(text):\n    # Difference between stem and lemma is illustrated by 'categories'\n    lemma=WordNetLemmatizer()\n    return \" \".join([lemma.lemmatize(word) for word in word_tokenize(text)])\n\nprint('cat cats dog dogs categories','===>',lemma('cat cats dog dogs categories'))\n\ndef no_preprocessing(text):\n    # just a reference function\n    return text","a4027dfc":"from sklearn.model_selection               import train_test_split,cross_val_score\n\nfrom sklearn.feature_extraction.text       import CountVectorizer,TfidfVectorizer\nfrom sklearn.naive_bayes                   import MultinomialNB,GaussianNB\n\nfrom sklearn.linear_model                  import LogisticRegression\n\nfrom sklearn.metrics                       import f1_score\ndef ml_modeling(train):\n    \n    X = train['text']\n    y = train['target']\n\n    X_train,X_val,y_train,y_val = train_test_split(X,y,test_size = 0.2,stratify = y, shuffle = True,random_state = 42)\n\n        \n    vect    = CountVectorizer(min_df = 5, max_df = 0.9, ngram_range = (1,2))\n    X_train = vect.fit_transform(X_train)\n    X_val   = vect.transform(X_val)\n\n    clf = MultinomialNB()\n    \n    score = cross_val_score(clf,X_train,y_train,cv = 5, scoring = 'f1')\n\n    print('Mean f1 score         : {:.5} STD: {:.5}'.format(score.mean(),score.std()))\n\n    print('f1 score on train data: {:.5}'.format(f1_score(y_train,clf.fit(X_train,y_train).predict(X_train))))\n    print('f1 score on valid data: {:.5}'.format(f1_score(y_val,clf.fit(X_train,y_train).predict(X_val))))\n\n    return score.mean()\n","101861d4":"funcs = [no_preprocessing,\n         to_lowercase,\n         remove_url,\n         assign_user,\n         remove_stop,\n         remove_accented_chars,\n         expand_abrivviation,\n         expand_contractions,\n         remove_special_characters,\n         remove_punctuation,\n         remove_numbers,\n         remove_extra_whitespace_tabs,\n         remove_emojify,\n         stem,\n         lemma]\n\nnames = ['no_preprocessing',\n         'to_lowercase',\n         'remove_url',\n         'assign_user',\n         'remove_stop',\n         'remove_accented_chars',\n         'expand_abrivviation',\n         'expand_contractions',\n         'remove_special_characters',\n         'remove_punctuation',\n         'remove_numbers',\n         'remove_extra_whitespace_tabs',\n         'remove_emojify',\n         'stem',\n         'lemma']\n\n\nf1 = []\nfor fun,name in zip(funcs,names):\n    df = train.copy()                              # here we use only single function, therefore in every itteration we have fresh dataser\n    print('\\nFunction ',name.upper(),'\\n')\n    print()\n    df['text'] = df['text'].apply(lambda x: fun(x))\n    score = ml_modeling(df)\n    \n    f1.append(score)","b240cc99":"pos = np.arange(len(f1)) \nfig, ax = plt.subplots(figsize = (20,10))\nplot = ax.bar(pos,f1, 0.4)\nax.set_ylabel('f1_score')\nax.set_title('Effect of text preprocessing')\nax.set_xticks(pos)\nax.set_xticklabels(names, rotation = 90,fontsize = 20);\n","b7817721":"df = train.copy()\nf1 = []\nfor fun,name in zip(funcs,names):\n    print('Function ',name.upper())\n    df['text'] = df['text'].apply(lambda x: fun(x))\n    score= ml_modeling(df)\n    \n    f1.append(score)","2a5151ec":"pos = np.arange(len(f1)) \nfig, ax = plt.subplots(figsize = (20,10))\nplot = ax.plot(f1)\nax.set_ylabel('f1_score')\nax.set_title('Effect of cumulative text preprocessing')\nax.set_xticks(pos)\nax.set_xticklabels(names, rotation = 90,fontsize = 20);","afd0ebe2":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf\n\n\ndef nn_modeling(train):\n    \n    # This function was also used for the similar study\n    # Results were the same, there is no significant improvment with text preprocessing\n    \n    X = train['text']\n    y = train['target']\n\n    X_train,X_val,y_train,y_val = train_test_split(X,y,test_size = 0.2,stratify = y, shuffle = True,random_state = 42)\n    \n    text_token = Tokenizer()\n    text_token.fit_on_texts(X_train)\n\n    X_train_seq = text_token.texts_to_sequences(X_train)\n    X_train_pad = pad_sequences(X_train_seq, maxlen = 100)\n\n    X_val_seq = text_token.texts_to_sequences(X_val)\n    X_val_pad = pad_sequences(X_val_seq, maxlen = 100)\n\n    text_vocab_size = len(text_token.word_index)+1\n\n\n\n    model_seq = tf.keras.Sequential([\n        tf.keras.layers.Embedding(text_vocab_size,12,input_length = X_train.shape[0]),\n        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(8,dropout=0.25, recurrent_dropout=0.25)),\n        tf.keras.layers.Dense(1,activation = 'sigmoid')\n\n    ])\n\n    model_seq.compile(loss = 'binary_crossentropy',\n                      optimizer = 'adam',\n                      metrics = ['accuracy'])\n\n\n\n    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n\n    history = model_seq.fit(X_train_pad,y_train,\n                            epochs = 2,\n                            batch_size=32,                       \n                            validation_data=(X_val_pad,y_val),\n                            shuffle = True,\n                            verbose = 1)\n    \n    \n    pred = model_seq.predict(X_val_pad,verbose = 1)\n    pred_temp = np.where(pred.reshape(-1,) > 0.5,1,0)\n    scores = []\n    f1 = f1_score(y_val,pred_temp)\n\n    print('\\nF1 score is: {:.5}'.format(f1))\n\n    return f1,model_seq,text_token","645503bd":"funcs = [no_preprocessing,\n         to_lowercase,\n         remove_url,\n         assign_user,\n         remove_stop,\n         remove_accented_chars,\n         expand_abrivviation,\n         expand_contractions,\n         remove_special_characters,\n         remove_punctuation,\n         remove_numbers,\n         remove_extra_whitespace_tabs,\n         remove_emojify,\n         stem,]\n         #lemma]\n\nnames = ['no_preprocessing',\n         'to_lowercase',\n         'remove_url',\n         'assign_user',\n         'remove_stop',\n         'remove_accented_chars',\n         'expand_abrivviation',\n         'expand_contractions',\n         'remove_special_characters',\n         'remove_punctuation',\n         'remove_numbers',\n         'remove_extra_whitespace_tabs',\n         'remove_emojify',\n         'stem',]\n         #'lemma']\n\n\n\nfor fun,name in zip(funcs,names):\n\n    print('(TRAIN) Function ',name.upper(),'\\n')\n    train['text'] = train['text'].apply(lambda x: fun(x))\n        \ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntest.drop(['location','keyword'],axis = 1,inplace = True)         \n\nfor fun,name in zip(funcs,names):\n\n    print('(TEST) Function ',name.upper(),'\\n')\n    test['text'] = test['text'].apply(lambda x: fun(x))","7f9eec33":"_,model,tokenizer = nn_modeling(train)\n\nX_test = tokenizer.texts_to_sequences(test['text'])\nX_test = pad_sequences(X_test,maxlen = 100)\n\npred = model.predict(X_test)\npred = pred.reshape(-1,)\n\n\nsub = pd.DataFrame({'Id':test.id, 'target':np.where(pred > 0.5,1,0)})\nsub.to_csv('submission.csv',index = False)","57947b20":"# Conclussion","442bd46d":"Let`s import data and focus only on the 'text' column( without considering 'location' and 'keyword')","a518326f":"Let`s put all functions in the list and itterate.\n\nIn this section we are going to apply only single preprocessing function for the text column and we are going to output mean f1_score\n","bdc1a51e":"# 1. Single fucntion","42a30150":"This functions are are extrmly usefull in NLP and used to get rid of usless words(stop words) and normalize text a bit(Lemmitization,Stemming)\n\n\nThe steps in 3 technoques are very simillar:\n\n**For example stop word removing:\n    1. Tokenize a text\n        This is a sentence ==> ['This','is','a','sentence']   with  word_tokenize()\n    2. Itterate through the list and check if there are some stop words\n    3. Join not stopwords with .join()\n    \n**Stemming and Lemmitizing:\n    1. Tokenize a text\n        This is a sentence ==> ['This','is','a','sentence']   with  word_tokenize()\n    2. stemm or lemmitize\n    3. join \n        \n        ","844f374a":"# 5. Expand Contractions","fc472acf":"Now we are going to fix following (as it suggested by many sources):\n\n* Lower text\n* Remove  http:\/\/ and  https:\/\/\n* We will try to substiture @username by USER\n* Remove accedent character\n* Expand abbriviations   \n* Expand contractions\n* Remove special characters\n* Remove punctuation\n* Remove numbers\n* Remove double spaces and tabs\n* Remove emoji\n* Apply stemming (extract kind of root of the word);\n* Apply lemmatization (advanced stemming)\n","c51d8752":"# 7. Concluding with powerfull Stop words\/Lemmitization\/Stemming","1cd3c4bd":"Unfortuantely there is extrmly small improvment....\n\nMaybe we should apply them all.... Let`s try","2d916595":"# 4. Remove abbriviations","76c13c8f":"# 3. Remove accented chars","fbe171a0":"# 6. Some polishing cleaning","dc22984c":"In this notebook i would like to open discussion on the importance of text preprocessing in the frame of traditional machine learning.\nEven though there are plenty of extremly usefull preprocessing techniques availabe on kaggle.com, it is not so easy to find its effect on validation score.\nThe notebook containes list of the most common preprocessing tricks that supossed to improve model performance.\nHowever, as you will see later the outcome of huge efforts on preprocessing is not always obvious.\n\nI should highlight the purpose of this notebook is not to maximize LB (I wish) but to understand influence of common preprocessing techniques on validation score.\n\nI would appreciate if you give me useful feedback, clarification on this issue and would be happy to discuss why preprocessing did not help me in this competition.\n\nIn the notebook you can find simple functions for:\n\n* Lower text\n* Remove  http:\/\/ and  https:\/\/\n* Substiture @username by USER\n* Remove accedent character\n* Expand abbriviations   \n* Expand contractions\n* Remove special characters\n* Remove punctuation\n* Remove numbers\n* Remove double spaces and tabs\n* Remove emoji\n* Apply stemming;\n* Apply lemmatization\n\n\nIn the end you can find example of simple bideractional LSTM\n\n### If you find this kernel interesting please leave your comment and upvote.\n \n\nI would like to thanks following usefull references:\n\n* https:\/\/www.kaggle.com\/vanshjatana\/a-simple-guide-to-text-cleaning\n* https:\/\/www.kaggle.com\/theoviel\/improve-your-score-with-some-text-preprocessing\n* https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert\n* https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert\n* https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\n* https:\/\/www.kdnuggets.com\/2019\/04\/text-preprocessing-nlp-machine-learning.html\n* https:\/\/www.datacamp.com\/community\/tutorials\/stemming-lemmatization-python","247b0278":"# 1. Lowering","a90dd303":"In this case we apply all functions to the text","776f56d6":"In this notebook, I used common text preprocessing techniques to improve f1_score.\nUnfortunately, in my case, application of either single or cummulative preprocessing didn`t improve score significatly.\nSmall improvement( ~1%) was observed by removing stopwords and punctuation.\n\n\nAs i wrote in the beggining purpose of this notebook is not to reach high LB, at the moment I am extrmly currius why common text preprcessing teqniques didn`t help me\n\n\nTherefore if you have any explanation, suggestion please let me know.\n\nI would ne happy to discuss this point.\n\n","368152f4":"Let`s just fit RNN and submit result","ca826ee5":"In this chapter I am going to use cross_val_score to determine f1_score wthin 5 folds.\n\nI am using simple:\n    * CountVectorizer() with some defined parameters\n    * MultinomialNB() with default parameters\n    \n    \nOf course it makes sense to use more powerful classifiers and perfrom hypeparameters tuning, howvere since I am intresting only in effect of text preprocessing we can do it later\n","6b787ba7":"# Fitting RNN sequential","5c0bfb24":"Since there are some links in the text without simantic meaning it make sense to remove them\n\n\nMoreover, evereything after @ can be removed or replaced with USER tag","b2786669":"## Importing","6597cefb":"# Conventional Machine Learning","e6fda7a1":"We have got binary classification problem, where 0 is not a disaster tweet and 1 is a disaster.\nData is almost balanced (57 vs 43 %) and it makes live easier.\n\nLet`s have a look at first 50 text messages","c7fd76a8":"# 2. Cumulative","efb58495":"Let`s visualize","b17b2080":"The biproduct of re.sub could be excessive numbers of white spaces and tabs.\n\nIt is a good habbit to clean up them","012acb5f":"# Text Preprocessing Worth or Not?","14df936d":"Common practice is to lower all letters","f362cdcc":"# 2. Remove http https and remove\/substitute @username","3242bbde":"Define a model","c5fd6a61":"## First look at the data set"}}