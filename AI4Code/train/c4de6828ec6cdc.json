{"cell_type":{"bda8e4fb":"code","ec045408":"code","5b24044e":"code","6319233a":"code","ca912b54":"code","3e52c26d":"code","1e7b604d":"code","1c345102":"code","bc21b97f":"code","52b45f28":"code","6a142f2d":"code","c471809c":"code","a674c2c7":"code","0cddc952":"code","6868d75e":"code","bce7196f":"code","120a0eb6":"code","be6a72d7":"code","7005f684":"code","634cec21":"code","1e259073":"markdown","0b6d201f":"markdown","ac8eabdc":"markdown","e31c0877":"markdown","1143dbca":"markdown","286608cc":"markdown","d4ca053d":"markdown","c76617e1":"markdown","3061f7bd":"markdown","a641d026":"markdown","51d83927":"markdown","16f4b51b":"markdown","f332c12a":"markdown","a8449752":"markdown","74aba765":"markdown","c57b7289":"markdown","abd45833":"markdown","4c67598c":"markdown","75f8cd07":"markdown","0b9a815d":"markdown","b4929ac8":"markdown","cc485932":"markdown"},"source":{"bda8e4fb":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ec045408":"#class1\nx1 = np.random.normal(25,5,1000) \ny1 = np.random.normal(25,5,1000)\n#class2\nx2 = np.random.normal(55,5,1000) \ny2 = np.random.normal(60,5,1000)\n#class3\nx3 = np.random.normal(55,5,1000) \ny3 = np.random.normal(15,5,1000)\n\nx = np.concatenate((x1,x2,x3),axis=0) #x1,x2,x3 birle\u015ftir, data sample 3000\ny = np.concatenate((y1,y2,y3),axis=0) #y1,y2,y3 birle\u015ftir, data sample 3000\n\ndictionary = {\"x\":x,\"y\":y}\ndata1 = pd.DataFrame(dictionary)\n\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(15,6))\nax1.scatter(x1,y1,color=\"black\")\nax1.scatter(x2,y2,color=\"black\")\nax1.scatter(x3,y3,color=\"black\")\nax2.scatter(x1,y1)\nax2.scatter(x2,y2)\nax2.scatter(x3,y3)\nfig.suptitle(\"Before and Target of Clustering\")\nax1.set_title('None Clustering')\nax2.set_title('Target Clustering')\nplt.show()","5b24044e":"from sklearn.cluster import KMeans\nwcss = []\nwcss_values = range(1,15)\n\nfor k  in wcss_values:\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(data1)\n    wcss.append(kmeans.inertia_) # Her K de\u011feri i\u00e7in wcss de\u011feri\n\nplt.figure(figsize=[15,6])\nplt.plot(wcss_values,wcss)\nplt.xlabel(\"number of k (cluster) value\")\nplt.xticks(wcss_values)\nplt.ylabel(\"wcss\")\nplt.show()","6319233a":"kmeans2 = KMeans(n_clusters=3)\nclusters = kmeans2.fit_predict(data1) # fit et ve ard\u0131ndan predict et yani data \u00fczerinde clusterlar\u0131 uygula.\n\ndata1[\"label\"] = clusters\nplt.figure(figsize=[15,6])\nplt.title(\"After Clustering and last cluster centers\")\nplt.scatter(data1.x[data1.label==0],data1.y[data1.label==0],color=\"blue\")\nplt.scatter(data1.x[data1.label==1],data1.y[data1.label==1],color=\"red\")\nplt.scatter(data1.x[data1.label==2],data1.y[data1.label==2],color=\"green\")\nplt.scatter(kmeans2.cluster_centers_[:,0],kmeans2.cluster_centers_[:,1],color=\"black\") #cluster centers\nplt.show()","ca912b54":"#class1\nx1 = np.random.normal(25,5,100) \ny1 = np.random.normal(25,5,100)\n#class2\nx2 = np.random.normal(55,5,100) \ny2 = np.random.normal(60,5,100)\n#class3\nx3 = np.random.normal(55,5,100) \ny3 = np.random.normal(15,5,100)\n\nx = np.concatenate((x1,x2,x3),axis=0) #x1,x2,x3 birle\u015ftir, data sample 3000\ny = np.concatenate((y1,y2,y3),axis=0) #y1,y2,y3 birle\u015ftir, data sample 3000\n\ndictionary = {\"x\":x,\"y\":y}\ndata1 = pd.DataFrame(dictionary)\n\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(15,6))\nax1.scatter(x1,y1,color=\"black\")\nax1.scatter(x2,y2,color=\"black\")\nax1.scatter(x3,y3,color=\"black\")\nax2.scatter(x1,y1)\nax2.scatter(x2,y2)\nax2.scatter(x3,y3)\nfig.suptitle(\"Before and Target of Clustering\")\nax1.set_title('None Clustering')\nax2.set_title('Target Clustering')\nplt.show()","3e52c26d":"#%% Dendogram\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n#cluster i\u00e7ideki yay\u0131l\u0131mlar\u0131,varyanslar\u0131 \u015fekillendirebilece\u011fimiz, minimalimize edece\u011fmiz y\u00f6ntem.\nplt.figure(figsize=(15,5))\nmerg = linkage(data1, method=\"ward\") \ndendrogram(merg, leaf_rotation=90,orientation='top',truncate_mode='lastp',show_contracted=True)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidean distance\")\nplt.show()","1e7b604d":"from sklearn.cluster import AgglomerativeClustering\n\nhierartical_cluster = AgglomerativeClustering(n_clusters=3, affinity=\"euclidean\", linkage=\"ward\")\nclusters = hierartical_cluster.fit_predict(data1) #modeli hem olu\u015ftur hemde predict edip clusterlar\u0131 olu\u015ftur.\n\ndata1[\"label\"] = clusters\nplt.figure(figsize=(15,6))\nplt.title('After Clustering')\nplt.xlabel(\"x feature\")\nplt.ylabel(\"y feature\")\nplt.scatter(data1.x[data1.label==0],data1.y[data1.label==0],color=\"red\")\nplt.scatter(data1.x[data1.label==1],data1.y[data1.label==1],color=\"green\")\nplt.scatter(data1.x[data1.label==2],data1.y[data1.label==2],color=\"blue\")\nplt.show()","1c345102":"df = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\ndf.info()\n","bc21b97f":"df.Class.value_counts()\n# 1 = sahtekarl\u0131k durumu (Fraud)\n# 2 = Aksi durum         (OtherWise)","52b45f28":"df.describe()","6a142f2d":"data = df.drop(['Class','Time'],axis=1)\ndata.head()","c471809c":"data.isnull().all()","a674c2c7":"#standartla\u015ft\u0131rma kodu gelecek\ndata_loc = data.loc[:400:,['V11','V15']]\nscaler_data = (data_loc - np.mean(data_loc)) \/ np.std(data_loc)\ndata2 = scaler_data\ndata2.head()","0cddc952":"plt.scatter(data2.V11,data2.V15)\nplt.xlabel('V11')\nplt.ylabel('V15')\nplt.show()\n","6868d75e":"from sklearn.cluster import KMeans\nwcss = []\n\nfor k  in range(1,15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(data2)\n    wcss.append(kmeans.inertia_) # wcss value for each k value","bce7196f":"plt.figure(figsize=(15,6))\nplt.plot(range(1,15),wcss)\nplt.xlabel(\"number of k (cluster) value\")\nplt.ylabel(\"wcss\")\nplt.show()","120a0eb6":"kmeans2 = KMeans(n_clusters = 5)\nclusters = kmeans2.fit_predict(data2)\ndata2['label'] = clusters\nplt.figure(figsize=[10,4])\nplt.scatter(data2.V11,data2.V15,c = data2.label)\nplt.scatter(kmeans2.cluster_centers_[:,0],kmeans2.cluster_centers_[:,1],color=\"red\",linewidths=5)\nplt.xlabel('V11')\nplt.ylabel('V15')\nplt.show()","be6a72d7":"data3 = data.loc[:200:,['V11','V15']]\nplt.figure(figsize=[10,4])\nplt.scatter(data2.V11,data2.V15,color='black')\nplt.title(\"Before Clustering\")\nplt.xlabel('V11')\nplt.ylabel('V15')\nplt.show()","7005f684":"\n\nmerge_single = linkage(data3, method='single',metric='euclidean' )\nmerge_complete = linkage(data3, method='complete',metric='euclidean')\nmerge_average = linkage(data3, method='average',metric='euclidean' )\nmerge_ward = linkage(data3, method='ward',metric='euclidean' )\n\nfig, axes = plt.subplots(4, 1, figsize=(12, 24))\n\nd_single=dendrogram(merge_single,ax=axes[0] ,leaf_rotation=90,truncate_mode='lastp',show_contracted=True)\nd_complete=dendrogram(merge_complete, ax=axes[1],leaf_rotation=90,truncate_mode='lastp',show_contracted=True)\nd_average=dendrogram(merge_average, ax=axes[2],leaf_rotation=90,truncate_mode='lastp',show_contracted=True)\nd_ward=dendrogram(merge_ward, ax=axes[3],leaf_rotation=90,truncate_mode='lastp',show_contracted=True)\n\naxes[0].set_title('Single')\naxes[1].set_title('Complete')\naxes[2].set_title('Average')\naxes[3].set_title('Ward')\nplt.show()\n","634cec21":"from sklearn.cluster import AgglomerativeClustering\n\nhierartical_cluster = AgglomerativeClustering(n_clusters=3, affinity=\"euclidean\", linkage=\"complete\")\nclusters = hierartical_cluster.fit_predict(data3) #modeli hem olu\u015ftur hemde predict edip clusterlar\u0131 olu\u015ftur.\n\nplt.figure(figsize=(15,6))\ndata3[\"label\"] = clusters\nplt.scatter(data3.V11,data3.V15,c = data3.label)\nplt.show()","1e259073":"<a id=\"3.2\"><\/a>\n# 3.2. Standardization","0b6d201f":"<a id=\"4\"> <\/a>\n# Conclusion\nUsing the V11 and V15 features in the credit card fraud database, it has been reached that there are 5 different groups with the K Means algorithm and 2 different groups with the complete method (the farthest neighbor) from the hierarchical clustering algorithms.","ac8eabdc":"<a id=\"1.1\"><\/a>\n## 1.1. How to choose the right k parameter ? \n\n\n* Selection of K parameter is important. K parameter too low may cause incomplete grouping, too high\ngoing into more detail than necessary may cause it to group too much.\nThe way to evaluate the K parameter selection is done by a parameter known as WCSS.\nWCSS means the sum of intra-cluster frames.\n\nWCSS Formula for K = 3 value:\n \n    \n![](https:\/\/i.hizliresim.com\/kYMyBQ.png)\n\n\n    \n\n","e31c0877":"<a id=\"3.5\"><\/a>\n# 3.5. Hierarchical Clustering algorithms for credit card fraud\n\n\nIn this section, credit card fraudsters are divided into groups using hierarchical clustering algorithms.","1143dbca":"<a id=\"35b\"><\/a>\n# b. Clustering according to the \"complete method\"","286608cc":"<a id=\"2.1\"><\/a>\n## 2.1. what is dendrogram ?\n* The dendrogram illustrates how each cluster is composed by drawing a U-shaped link between a non-singleton cluster and its children. The top of the U-link indicates a cluster merge. The two legs of the U-link indicate which clusters were merged. The length of the two legs of the U-link represents the distance between the child clusters. It is also the cophenetic distance between original observations in the two children clusters.\n[For Reference](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.cluster.hierarchy.dendrogram.html)\n\n\n![Dendrogram](https:\/\/i.hizliresim.com\/C4k037.png)","d4ca053d":"# INTRODUCTION\n\n*  Our aim in this kernel will be learning some of the unsupervised learning algorithms and implementing them on the credit card fraud database.\n\n<b> Content <\/b>\n\nA. [UnSuperVised Learning](#1) <br>\n1. [What is KMeans Algorithm ?](#2) <br>\n    1.1. [How to choose the right k parameter ? (WCSS)](#1.1) <br>\n    1.2. [what is the best wcss value? (Elbow Method)](#1.2)<br>\n2. [What are hierarchical clustering algorithms \n(non-hierarchical) ?](#2) <br>\n    2.1. [what is dendrogram ?](#2.1) <br>\n    2.2. [Dendrogram Example](#2.2) <br>\n3. [Clustering for Credit Card Fraud](#3) <br>\n    3.1. [Data Pre-Processing](#3.1)<br>\n    3.2. [Standardization](#3.2)<br>\n    3.3. [Data Visualization](#3.3)<br><br>\n    3.4. [K means algorithm for credit card fraud (non-hierarchical)](#3.4)<br>\n    a. [What is best wcss value ?](#34a)<br>\n    b. [K Means](#34b)\n    \n    3.5. [Hierarchical Clustering algorithms for credit card fraud](#3.5) <br>\n    a. [Dendrograms for aggleomerative clustering](#35a)<br>\n    b. [Clustering according to the \"complete method\"](#35b)<br> \n4. [Conclusion](#4) <br>\n","c76617e1":"**According to the elbow rule, we decided that the most logical k value was 5 or 6**<br>\n<br>\n**Let's see the most recent cluster centers.**","3061f7bd":"<a id=\"2.2\"><\/a>\n## 2.2. Dendrogram Example\n\n* Dendrogram is a graph showing coupling of clusters. What are the implications for this chart?\nCutting is done according to a determined distance threshold value or similarity level. It helps us determine our k (n-cluster) parameter. This determined value is shown on the euclidean distance.\n* Or, as another method, distances are checked according to the Dendrogram. Observed distances should not be cut by a horizontal line.\n\n![Optimal Dendrogram](https:\/\/i.hizliresim.com\/fqwgTv.png)\n[For Reference](https:\/\/www.udemy.com\/course\/machine-learning-ve-python-adan-zye-makine-ogrenmesi-4\/)\n\n<b> truncate_mode = 'lastp' ==>  show only the last p merged clusters <br>\n    show_contracted = True  ==>  to get a distribution impression in truncated branches<br><\/b>\n* The parameter show_contracted allows us to draw black dots at the heights of those previous cluster merges, so we can still spot gaps even if we don't want to clutter the whole visualization. In our example we can see that the dots are all at pretty small distances when compared to the huge last merge at a distance of 180, telling us that we probably didn't miss much there.\n* As it's kind of hard to keep track of the cluster sizes just by the dots, dendrogram() will by default also print the cluster sizes in brackets () if a cluster was truncated\n","a641d026":"<a id=\"34b\"><\/a>\n## k means","51d83927":"We can obtain 2 clusters with the single method.  <br>\nWe can obtain 3 clusters with the complete method.<br>\nWe can obtain 2 clusters with the Average method.<br>\nWe can obtain 2 clusters with the Ward method.<br>\n\n<b>  I will make the linkage by considering the complete method, that is, the closest distance between the clusters. <\/b>\n","16f4b51b":"According to the elbow rule, we decided that the most logical k value was 3.<br>\n<b> As the data is generated randomly, the result may look different. <\/b>\n","f332c12a":"<a id=\"3.1\"><\/a>\n# 3.1. data pre-processing","a8449752":"<a id=\"3.3\"><\/a>\n# 3.3. Data Visualization\n\n\n<b> We will consider 2 features to be comfortable in terms of visualization : V11,V15 <\/b>","74aba765":"<a id=\"2\"><\/a>\n# 2. What are hierarchical clustering algorithms ?\n\n\n- Hierarchical clustering: It is based on the process of treating each point as a cluster at the beginning and joining them according to the criteria selected at the beginning or separating all the points at the beginning as a single cluster and then separating them according to the selected criteria.\n- In this section, we will examine only associative methods, namely agglomerative clustering methods.\n\n<b>Agglomerative clustering methods initially treat each point as a cluster. Then the distance between each cluster:<\/b><br>\nA.) Euclidean Distance (L2) <br>\nB.) Manhattan Distance (L1) <br>\nC.) Cosine Distance <br>\nD.) Pre-Calculated  <br>\nIt is calculated by selecting one of the specified metrics.\n \n\n<b>The next stage is the merge stage. For the merge process:<\/b><br>\nA.) <b>Single Linkage<\/b>: It is based on combining the closest octans.<br>\nB.) <b>Complete Linkage<\/b>: It is based on the joining of the most distant points.<br>\nC.) <b>Average Linkage<\/b>: Based on average distance between cluster pairs.<br>\nD.) <b>Ward's Linkage<\/b>: Linkage is envisaged by minimizing intra-cluster variances..<br>\n\n      Combination takes place by selecting one of these methods. Merge continues until a single cluster remains\n      or completed at the specified threshold.\n      Selecting the linkage method is related to the structure of the distance table to be updated. The linkageu method we choose determines what the distance table will organize.\n      \n\n<b>Steps of agglomerative hierarchical clustering algorithms:<\/b>\n1. Treat each data point as a cluster.\n2. Determine the distance of each set to each other with the selected distance metric (euclidean, manhattan, cosine) and create the distance table. Merge the two sets with the lowest distance.\n3. Update the distance table with the linkage method (single, complete, average, linkage, wards) selected for newly formed sets.\n4. Repeat steps 2 and 3 until you have a single set.\n\n\n<b>Use the dendrogram to visually see cluster steps and to better understand and interpret clustering.<\/b>\n ","c57b7289":"<a id=\"3.4\"><\/a>\n# 3.4. K means algorithm for credit card fraud","abd45833":"<a id=\"35a\"><\/a>\n# a. Dendrograms for aggleomerative clustering","4c67598c":"<a id=\"1\"><\/a>\n# 1. What is KMeans Algorithm (non-hierarchical clustering algorithm) ?\n\n* The K means algorithm is an algorithm that follows a number of paths to group the data to be found.\n    If we examine the following x (feature 1) and y (feature 2) two-dimensional example:\n        When we want to group the data below, we can say that the data can belong to 3 different groups at a glance,\n        but when the size of the data increases, for example, x, y, z ... when it reaches more than 2 dimensions, the human eye now uses this grouping process,\n        is unable to do.\n    It is not certain how many clusters and groups of data should be separated before a clustering operation. This is why the \"K\" parameter is used. The best value of this parameter is found by trying.\n    As I said earlier, we can give the k parameter 3 at a glance, but it will not be possible to obtain the k value at a glance when the data size increases.\n        \n<font color=\"red\">** K means algorithm steps: **<\/font>\n1. K value is determined.\n2. The center point is assigned to the number of random K values.\n3. The distance of each point from each center point is generally determined by the euclidean distance and the relevant point is assigned to the nearest center.\n4. Steps 2 and 3 are repeated until these center points are not moved.\n     ","75f8cd07":"<a id='A'><\/a>\n# A. UNSUPERVISED LEARNING\n\n\n* Unsupervised learning works on unlabeled data, unlike supervised learning.\n* The number of points to be reached at the beginning of unsupervised learning is not certain, but in the classification, the number of points to be reached is any of the class labels.\n* In unsupervised learning, the process of grouping the data is done by taking into consideration the similarities within itself.\n* Similarities are determined by distance criteria.\n--------------------------------------------------------------------------------------------------\n* Denetimsiz \u00f6\u011frenme, denetimli \u00f6\u011frenmeden farkl\u0131 olarak etiketsiz veriler \u00fczerinde \u00e7al\u0131\u015f\u0131r.\n* Denetimsiz \u00f6\u011frenmenin ba\u015flang\u0131c\u0131nda var\u0131lacak nokta say\u0131s\u0131 belirli de\u011fildir fakat s\u0131n\u0131fland\u0131rmada ise var\u0131lacak nokta say\u0131s\u0131 s\u0131n\u0131f etiketlerinden herhangi biridir.\n* Denetimsiz \u00f6\u011frenmede verilerin kendi i\u00e7indeki benzerliklerinin g\u00f6z\u00f6n\u00fcne al\u0131narak grupland\u0131r\u0131lmas\u0131 i\u015flemi yap\u0131l\u0131r.\n* Benzerliklerin belirlenmesi ise uzakl\u0131k \u00f6l\u00e7\u00fctleri ile sa\u011flan\u0131r.","0b9a815d":"<a id=\"1.2\"><\/a>\n## 1.2. what is the best wcss value (Elbow Metric)?\n\n* Elbow technique is used to determine the best wcss value.\nIf there is no significant decrease after a certain value according to the below technique logic, the most optimal value is the point where the decrease is stopped.\nAs seen in the graphic below, there is no significant decrease after 3. It is understood that the value of k is 3.\n\n![Elbow](https:\/\/i.hizliresim.com\/k9QMTH.png)\n\n    ","b4929ac8":"<a id=\"34a\"><\/a>\n## What is best wcss value ?","cc485932":"<a id=\"3\"><\/a>\n# 3. Clustering for Credit Card Fraud\n* In this section, two features are selected from the credit card fraud dataset: Kmeans algorithm and hierarchical clustering algorithms are applied for easy visualization."}}