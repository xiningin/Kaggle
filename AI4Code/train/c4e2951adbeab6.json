{"cell_type":{"c42a1ba6":"code","28f17cc4":"code","9830b3d4":"code","38e2c8a3":"code","ee2e4d04":"code","4fd0096d":"code","f4bde927":"code","5557bd21":"code","99a78b39":"code","f36a904a":"code","b12225d2":"code","3f517e13":"code","674e2291":"code","7b2d64d9":"code","1e4b4bcb":"code","2e1a85c3":"code","60576e6a":"code","f04d5622":"code","5ff14c95":"code","814d4a06":"code","3952500f":"code","0a98b870":"code","544f4032":"code","605075f3":"code","81281811":"code","5df45e80":"code","caac8828":"code","86114e01":"code","2652de86":"markdown","37cc55b4":"markdown","91738ea7":"markdown","925632dd":"markdown","bc108c5a":"markdown","e0b25a2c":"markdown","b0c59586":"markdown","26f8f5e5":"markdown","d87b97fa":"markdown","a34500b8":"markdown","0acad104":"markdown","e60ef621":"markdown","a3fc8f59":"markdown","1f52387a":"markdown","31430930":"markdown","fe0c5d0a":"markdown","c48efde8":"markdown","74b33075":"markdown","db3eec06":"markdown"},"source":{"c42a1ba6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","28f17cc4":"loans = pd.read_csv('\/kaggle\/input\/predicting-who-pays-back-loans\/loan_data.csv')","9830b3d4":"loans.head()","38e2c8a3":"loans.info()","ee2e4d04":"loans.isnull().sum()\n# NO MISSING VALUES\n# with no missing values, we'll turn to the columns and see if any should be dropped or edited.","4fd0096d":"# not fully paid, with a score of 1 or 0, will be our target value and the variable we want to predict","f4bde927":"# let's observe the one 'object' data type\nloans['purpose'].nunique()\n# With 7 unique values that may correlate with our target value, we'll turn it into a dummy variable","5557bd21":"purpose_ = pd.get_dummies(loans['purpose'],drop_first=True)\npublic_record  = pd.get_dummies(loans['pub.rec'],drop_first=True)","99a78b39":"# drop the original columns that we're replacing with dummy variables \nloans.drop(['purpose','pub.rec'],axis=1,inplace=True)","f36a904a":"# dummy decider\n# loans['pub.rec'].unique()","b12225d2":"loans = pd.concat([loans,purpose_,public_record],axis=1)\nloans.head()","3f517e13":"from sklearn.model_selection import train_test_split","674e2291":"X_train, X_test, y_train, y_test = train_test_split(loans.drop('not.fully.paid',axis=1), \n                                                    loans['not.fully.paid'], test_size=0.30)","7b2d64d9":"from sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train,y_train)","1e4b4bcb":"predictions = logmodel.predict(X_test)","2e1a85c3":"from sklearn.metrics import classification_report, confusion_matrix","60576e6a":"logistic_confusion_matrix = confusion_matrix(y_test,predictions)\nlogistic_classification_report = classification_report(y_test,predictions)\n\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))","f04d5622":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier()\ndtree.fit(X_train,y_train)","5ff14c95":"predictions = dtree.predict(X_test)","814d4a06":"from sklearn.metrics import classification_report,confusion_matrix","3952500f":"decision_tree_confusion_matrix = confusion_matrix(y_test,predictions)\ndecision_tree_classification_report = (classification_report(y_test,predictions))\n\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))","0a98b870":"from sklearn.ensemble import RandomForestClassifier","544f4032":"rfc = RandomForestClassifier(n_estimators=600)","605075f3":"rfc.fit(X_train,y_train)","81281811":"predictions = rfc.predict(X_test)","5df45e80":"from sklearn.metrics import classification_report,confusion_matrix","caac8828":"random_forests_confusion_matrix = confusion_matrix(y_test,predictions)\nrandom_forests_classification_report = classification_report(y_test,predictions)\n\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))","86114e01":"print(logistic_confusion_matrix) \nprint(logistic_classification_report)\n\nprint(decision_tree_confusion_matrix) \nprint(decision_tree_classification_report)\n\nprint(random_forests_confusion_matrix) \nprint(random_forests_classification_report)","2652de86":"# 1. Cleaning the Data","37cc55b4":"## The purpose of this notebook will only be to run and compare the results of the three models. Therefore, I won't be creating any visualizations. Prep work will involve cleaning the data. ","91738ea7":"# Winner = Random Forests ","925632dd":"## The Machine Learning Algorithms used are:\n1. Logistic Regression\n2. Decision Trees\n3. Random Forests","bc108c5a":"### That concludes our quick three-model showdown.\n\n### - Sergio A. Galeano\n\n### #fortheloveofdatascience","e0b25a2c":"### Train and Predict","b0c59586":"### Evaluate the Results","26f8f5e5":"# 2. :::Model Selection and Execution:::","d87b97fa":"# 2.0 Train Test Split","a34500b8":"# Let's compare the results of all three models together","0acad104":"### Train and Predict","e60ef621":"### Evaluate the Results ","a3fc8f59":"# 2.1 Logistic Regression","1f52387a":"# 2.3 Random Forests ","31430930":"###  An important point to note is that our model was heavily more balanced towards positive cases. Our target class was not balanced, which would expectedly result in a model's ability to better predidct the more highly represented instance of the variable. \n\n###  Interestingly, while Random Forests appears to be the winner, decision trees accurately identified the most amount of true positives.\n\n###  While logisitc models can be more intuitive, random forests are generally better than decision trees, at the cost of having a process more opaque to the data scientist.","fe0c5d0a":"# In this notebook I implemented three machine learning algorithms on loan data with the purpose of predicting which lenders would pay back or not.\n","c48efde8":"### Train and Predict","74b33075":"### Evaluate the Results","db3eec06":"# 2.2 Decision Trees"}}