{"cell_type":{"ca711817":"code","ef91aaa2":"code","dcf636e8":"code","8cb9986c":"code","268c8973":"code","002aff49":"code","1bc5db59":"code","52c5e390":"code","651de319":"code","ecc9c3a1":"code","20eb0742":"code","ab0cc69b":"code","fa4b64f3":"code","967c4abb":"code","fbb3150f":"code","4db5cb53":"code","5f9cc7af":"code","65e3ca7c":"code","b600585b":"code","30e77273":"code","df75a20d":"code","c504c8ae":"code","fff474fa":"code","79a74c5b":"code","bbe156d0":"code","77057055":"code","83d3f627":"code","e5d2974f":"code","70e83dc3":"code","b4f3a660":"code","5e06fb4e":"markdown","78426dfc":"markdown","20e9567e":"markdown","bf2367c2":"markdown","646f5ef4":"markdown","5e61b6e4":"markdown","983083fa":"markdown","820e0577":"markdown","494b89f1":"markdown","5d2e055c":"markdown","658c713e":"markdown","c311ae94":"markdown","4e47f891":"markdown","1edeb05a":"markdown","1d903fb3":"markdown"},"source":{"ca711817":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ef91aaa2":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns","dcf636e8":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\n\ndf.head()","8cb9986c":"df.isnull().values.any()","268c8973":"df['Amount'].describe()","002aff49":"not_fraud = len(df[df.Class == 0])\nfraud = len(df[df.Class == 1])\nfraud_percent = fraud \/ (fraud + not_fraud) * 100\n\nprint(f'There are {fraud} cases in the dataset')\nprint(f'The percentage of fraud ocurrences is {round(fraud_percent,4)}%')","1bc5db59":"\ncount_classes = df.value_counts(df['Class'], sort= True)\nlabels = ['Not Fraud', 'Fraud']\n\ncount_classes.plot(kind = 'bar', rot = 0)\nplt.title(\"Visualization of Labels\")\nplt.ylabel(\"Count\")\nplt.xticks(range(2), labels)\nplt.show()","52c5e390":"scaler = StandardScaler()\n\ndf['New_amount'] = scaler.fit_transform(df[\"Amount\"].values.reshape(-1, 1))\n\ndf.drop(['Amount','Time'], inplace = True, axis = 1)","651de319":"Y = df[\"Class\"]\nX = df.drop([\"Class\"], axis= 1)","ecc9c3a1":"train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size= 0.3, random_state= 42)\n\nprint(\"Shape of train_X: \", train_X.shape)\nprint(\"Shape of test_X: \", test_X.shape)","20eb0742":"from sklearn.metrics import confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report","ab0cc69b":"# Function to perform training with giniIndex.\ndef train_using_gini(X_train, X_test, y_train):\n  \n    # Creating the classifier object\n    clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,max_depth=3, min_samples_leaf=5)\n  \n    # Performing training\n    clf_gini.fit(X_train, y_train)\n    \n    return clf_gini\n      \n# Function to perform training with entropy.\ndef tarin_using_entropy(X_train, X_test, y_train):\n  \n    # Decision tree with entropy\n    clf_entropy = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100, max_depth = 3, min_samples_leaf = 5)\n  \n    # Performing training\n    clf_entropy.fit(X_train, y_train)\n    return clf_entropy","fa4b64f3":"# Function to make predictions\ndef prediction(X_test, clf_object):\n  \n    # Predicton on test with giniIndex\n    y_pred = clf_object.predict(X_test)\n    print(\"Predicted values:\")\n    print(y_pred)\n    \n    return y_pred\n      \n# Function to calculate accuracy\ndef cal_accuracy(y_test, y_pred):\n      \n    print(\"Confusion Matrix: \", confusion_matrix(y_test, y_pred))\n      \n    print (\"Accuracy : \",\n    accuracy_score(y_test,y_pred)*100)\n      \n    print(\"Report : \",\n    classification_report(y_test, y_pred))","967c4abb":"# Driver code\ndef main():\n      \n    # Building Phase\n    clf_gini = train_using_gini(train_X, test_X, train_Y)\n    clf_entropy = tarin_using_entropy(train_X, test_X, train_Y)\n      \n    # Operational Phase\n    print(\"Results Using Gini Index:\")\n      \n    # Prediction using gini\n    y_pred_gini = prediction(test_X, clf_gini)\n    cal_accuracy(test_Y, y_pred_gini)\n      \n    print(\"Results Using Entropy:\")\n    # Prediction using entropy\n    y_pred_entropy = prediction(test_X, clf_entropy)\n    cal_accuracy(test_Y, y_pred_entropy)\n      \n      \n# Calling main function\nif __name__==\"__main__\":\n    main()","fbb3150f":"clf_entropy = tarin_using_entropy(train_X, test_X, train_Y)\ny_pred_entropy = prediction(test_X, clf_entropy)\n\ncm = confusion_matrix(test_Y,y_pred_entropy)\n\nconf_matrix = pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\");\n\nTN = cm[0,0]\nTP = cm[1,1]\nFN = cm[1,0]\nFP = cm[0,1]\n\nsensitivity = TP\/float(TP+FN)\nspecificity = TN\/float(TN+FP)\n\nprint(f'Sensitivity = {round(sensitivity,3)}')\nprint(f'Specificity = {round(specificity,3)}')\nprint('Positive Likelihood Ratio = Sensitivity\/(1-Specificity) = ',sensitivity\/(1-specificity))\nprint('Negative likelihood Ratio = (1-Sensitivity)\/Specificity = ',(1-sensitivity)\/specificity)","4db5cb53":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport pandas_profiling\n\nfrom matplotlib import rcParams\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")","5f9cc7af":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size= 0.3, random_state= 42)\n\nX_train_scaled = scaler.fit_transform(X_train)\n\nX_test_scaled = scaler.transform(X_test)","65e3ca7c":"classifier = RandomForestClassifier(n_estimators=100) # Create the classifier\n\nclassifier.fit(X_train, Y_train) # Train it","b600585b":"Y_pred = classifier.predict(X_test) # Predict outputs\n\ncm2 = confusion_matrix(Y_test,Y_pred)\n\nconf_matrix2 = pd.DataFrame(data=cm2,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix2, annot=True,fmt='d',cmap=\"YlGnBu\");\n\nTN2 = cm2[0,0]\nTP2 = cm2[1,1]\nFN2 = cm2[1,0]\nFP2 = cm2[0,1]\n\nsensitivity2 = TP2\/float(TP2+FN2)\nspecificity2 = TN2\/float(TN2+FP2)\n\nprint(f'Sensitivity = {round(sensitivity2,3)}')\nprint(f'Specificity = {round(specificity2,3)}')\nprint('Positive Likelihood Ratio = Sensitivity\/(1-Specificity) = ',sensitivity2\/(1-specificity2))\nprint('Negative likelihood Ratio = (1-Sensitivity)\/Specificity = ',(1-sensitivity2)\/specificity2)","30e77273":"feature_importances_df = pd.DataFrame(\n    {\"feature\": list(X.columns), \"importance\": classifier.feature_importances_}\n).sort_values(\"importance\", ascending=False)\n\nfeature_importances_df","df75a20d":"# Visualization\n\nsns.barplot(x=feature_importances_df.feature, y=feature_importances_df.importance)\n\nplt.xlabel(\"Feature Importance Score\")\nplt.ylabel(\"Features\")\nplt.title(\"Visualizing Important Features\")\nplt.xticks(rotation=45, horizontalalignment=\"right\", fontweight=\"light\", fontsize=\"x-large\")\nplt.show()","c504c8ae":"# cargar datos con caracter\u00edsticas seleccionadas\nX_new = df.drop([\"V2\", \"V25\", \"V24\", \"V13\", \"V28\", \"V8\", \"V22\", \"V23\", \"New_amount\", \"Class\"], axis=1)\ny_new = df[\"Class\"]\n\n# estandarizar el conjunto de datos\nscaler = StandardScaler()\nX_scaled_new = scaler.fit_transform(X_new)\n\n# dividir en conjunto de entrenamiento y de prueba\nX_train_n, X_test_n, y_train_n, y_test_n = train_test_split(X_scaled_new, y_new, stratify=y_new, test_size=0.3, random_state=42)","fff474fa":"# New Classificator\nclf = RandomForestClassifier(n_estimators=100)\n\nclf.fit(X_train_n, y_train_n)","79a74c5b":"Y_pred_n = clf.predict(X_test_n) # Predict outputs\n\ncm3 = confusion_matrix(y_test_n,Y_pred_n)\n\nconf_matrix3 = pd.DataFrame(data=cm3,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix3, annot=True,fmt='d',cmap=\"YlGnBu\");\n\nTN3 = cm3[0,0]\nTP3 = cm3[1,1]\nFN3 = cm3[1,0]\nFP3 = cm3[0,1]\n\nsensitivity3 = TP3\/float(TP3+FN3)\nspecificity3 = TN3\/float(TN3+FP3)\n\nprint(f'Sensitivity = {round(sensitivity3,3)}')\nprint(f'Specificity = {round(specificity3,3)}')\nprint('Positive Likelihood Ratio = Sensitivity\/(1-Specificity) = ',sensitivity3\/(1-specificity3))\nprint('Negative likelihood Ratio = (1-Sensitivity)\/Specificity = ',(1-sensitivity3)\/specificity3)","bbe156d0":"from sklearn.linear_model import LogisticRegression\n\nX_train_l, X_test_l, Y_train_l, Y_test_l = train_test_split(X, Y, test_size= 0.3, random_state= 42)\n\nX_train_scaled_l = scaler.fit_transform(X_train_l)\n\nX_test_scaled_l = scaler.transform(X_test_l)","77057055":"# Train the model\n\nlog_reg = LogisticRegression()\n\nlog_reg.fit(X_train_scaled_l,Y_train_l)","83d3f627":"Y_pred_l = log_reg.predict(X_test_scaled_l)\n\ncm4 = confusion_matrix(Y_test_l,Y_pred_l)\n\nconf_matrix4 = pd.DataFrame(data=cm4,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix4, annot=True,fmt='d',cmap=\"YlGnBu\");\n\nTN4 = cm4[0,0]\nTP4 = cm4[1,1]\nFN4 = cm4[1,0]\nFP4 = cm4[0,1]\n\nsensitivity4 = TP4\/float(TP4+FN4)\nspecificity4 = TN4\/float(TN4+FP4)\n\nprint(f'Sensitivity = {round(sensitivity4,3)}')\nprint(f'Specificity = {round(specificity4,3)}')\nprint('Positive Likelihood Ratio = Sensitivity\/(1-Specificity) = ',sensitivity4\/(1-specificity4))\nprint('Negative likelihood Ratio = (1-Sensitivity)\/Specificity = ',(1-sensitivity4)\/specificity4)","e5d2974f":"from prettytable import PrettyTable\n\nsummary = PrettyTable(['Model','Accuracy * 100','Specificity','Sensitivity','PLR','NLR'])\nsummary.add_row(['Decision Tree', round(accuracy_score(test_Y, y_pred_entropy)*100,4), round(specificity,4), round(sensitivity,4), round(sensitivity\/(1-specificity),4), round((1-sensitivity)\/specificity,4)])\nsummary.add_row(['Random Forest', round(accuracy_score(Y_test,Y_pred)*100,4), round(specificity2,4), round(sensitivity2,4), round(sensitivity2\/(1-specificity2),4), round((1-sensitivity2)\/specificity2,4)])\nsummary.add_row(['Random Forest 2', round(accuracy_score(y_test_n,Y_pred_n)*100,4), round(specificity3,4), round(sensitivity3,4), round(sensitivity3\/(1-specificity3),4), round((1-sensitivity3)\/specificity3,4)])\nsummary.add_row(['Logistic Regression', round(accuracy_score(Y_test_l,Y_pred_l)*100,4), round(specificity4,4), round(sensitivity4,4), round(sensitivity4\/(1-specificity4),4), round((1-sensitivity4)\/specificity4,4)])\n\nprint(summary)","70e83dc3":"eval_metrics_graph = pd.DataFrame({\n                        'model': ['Decision Tree', 'Random Forest', 'Random Forest 2', 'Logistic Regression'],\n                        'Sensitivity': [round(sensitivity,4), round(sensitivity2,4), round(sensitivity3,4), round(sensitivity4,4)]\n                     })\neval_metrics_graph = eval_metrics_graph.sort_values('Sensitivity', ascending=False)","b4f3a660":"sns.barplot(x=eval_metrics_graph.model, y=eval_metrics_graph.Sensitivity, palette=\"rocket\")\n\nplt.xlabel(\"ML Model\")\nplt.ylabel(\"Sensitivity\")\nplt.title(\"Sensitivity comparison\")\nplt.xticks(rotation=45, horizontalalignment=\"right\", fontweight=\"light\", fontsize=\"x-large\")\nplt.figure(figsize=(10,5), dpi=100)\nplt.show()","5e06fb4e":"It seems that by deleting some variables we make the model perform worse than before...","78426dfc":"We conclude that the Decision Tree is the model that best fits the data based on the 'Sensitivity' metric, wich is the most importante one since it shows how good are we identifying possible frauds.","20e9567e":"### 2.3. Logistic Regression","bf2367c2":"This results are pretty good, but we'll see if we can improve it. We will check the importance of the different variables used in the prediction and try to discard them.","646f5ef4":"## Step 2: Training and Test the Models\n\nSplitting train and test datasets","5e61b6e4":"### 2.2. Random Forest","983083fa":"Check for null values","820e0577":"This is actually a binary classification problem as we have to predict only 1 of the 2 class labels. We can apply a variety of algorithms for this problem like Random Forest, Decision Tree, Support Vector Machine algorithms, etc.\n\nIn this machine learning project, we build Random Forest, Decision Tree and Logistig Regression classifiers and see which one works best. We address the \u201cclass imbalance\u201d problem by picking the best-performed model.","494b89f1":"## Step 1: Load and see the data","5d2e055c":"### 2.1. Decision Tree\n\nWe will perform the training with 2 metrics:\n\n* **Gini Index:** metric to measure how often a randomly chosen element would be incorrectly identified. It means an attribute with lower gini index should be preferred).\n* **Entropy:** measure of uncertainty of a random variable, it characterizes the impurity of an arbitrary collection of examples. The higher the entropy the more the information content.","658c713e":"We can see that using Entropy we have better classification results. Next I'll plot that confusion Matrix, just by fun:","c311ae94":"We're discarding the variables that have an importance of less than 0.013","4e47f891":"# Comparing Results\n\nWe will now see the results of the 4 different models, to see wich one performed better.","1edeb05a":"Plot with Matplotlib","1d903fb3":"Scale the \"Amount\" column and get X and Y sets"}}