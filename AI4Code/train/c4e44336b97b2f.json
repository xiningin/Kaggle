{"cell_type":{"38e35de9":"code","4e2b8c3b":"code","2f0314df":"code","8e521669":"code","747dfe41":"code","c37acd6d":"code","7a85425f":"code","c2fefb72":"code","0ca72e52":"code","c44a465d":"code","7aaf962e":"code","e80750d8":"code","fd82f84c":"code","fae2e2f2":"code","d04e9792":"markdown","78471fb4":"markdown","abd6926c":"markdown","96931eb3":"markdown","0b59f46b":"markdown","cb2e21ab":"markdown","461c39cf":"markdown","de56c9b4":"markdown","4c9f63ee":"markdown","b5ba31af":"markdown","2022e99f":"markdown","b9063506":"markdown","5fe60cd9":"markdown"},"source":{"38e35de9":"import numpy as np\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt","4e2b8c3b":"df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nprint()\nprint(\"Number of passengers: \" + str(len(df)))\nprint(\"Number of survivors: \" + str(sum(df['Survived'] == 1)))\nprint()\nprint(\"Variables: \")\nprint(df.columns.values)\nprint()\nfor column in df.columns.values:\n    print(\"Number of samples with missing \" + str(column) + \": \" + str(len(df[df[column].isnull()])))","2f0314df":"\ndf = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n\nmean_age = int (df['Age'].mean())\n\ndf['Age'] = df['Age'].fillna(mean_age)\ndf['Cabin'] = df['Cabin'].fillna('X')\n\n# Embarked is only NaN for the Am\u00e9lie Icard and Mrs. George Nelson, who boarded at Southampton\ndf['Embarked'] = df['Embarked'].fillna('S')\ndf.loc[df['Embarked'] == 'S', 'Embarked'] = 0\ndf.loc[df['Embarked'] == 'C', 'Embarked'] = 1\ndf.loc[df['Embarked'] == 'Q', 'Embarked'] = 2\n\ndf['Sex'] = df['Sex'].apply(lambda x : 0 if x == 'male' else 1)\ndf['Male'] = (df['Sex'] == 0).astype(int)\ndf['Female'] = (df['Sex'] == 1).astype(int)\n\ndf['FamilySize'] = df['SibSp'] + df['Parch']\n\ndf['Cabin'] = df['Cabin'].apply(lambda x : 0 if x == 'X' else 1)\ndf['HasCabin'] = (df['Cabin'] == 1).astype(int)\ndf['NoCabin'] = (df['Cabin'] == 0).astype(int)\n\ndf['Fare'] = df['Fare'].apply(lambda x : np.log(x) if x > 0 else 0)","8e521669":"from sklearn.ensemble import RandomForestClassifier\n\ndef fit_forest(x_train, y_train, estimators, depth):\n    forest = RandomForestClassifier(n_estimators=estimators, max_depth=depth, random_state=0)\n    forest.fit(x_train, y_train)\n    return forest","747dfe41":"def evaluate_forest(forest, x_test, y_test):\n    prediction = forest.predict(x_test)\n    results = [1 if y == y_hat else 0 for (y, y_hat) in zip(prediction, y_test)]\n    tp = sum([1 if (y_hat == 1 and y == 1) else 0 for (y_hat, y) in zip(prediction, y_test)])\n    fp = sum([1 if (y_hat == 0 and y == 1) else 0 for (y_hat, y) in zip(prediction, y_test)])\n    tn = sum([1 if (y_hat == 0 and y == 0) else 0 for (y_hat, y) in zip(prediction, y_test)])\n    fn = sum([1 if (y_hat == 1 and y == 0) else 0 for (y_hat, y) in zip(prediction, y_test)])\n    if tp + fp == 0:\n        precision = 0\n    else:\n        precision = tp \/ (tp+fp)\n    if tp + fn == 0:\n        recall = 0\n    else:\n        recall = tp \/ (tp + fn)\n    accuracy = sum(results)\/len(results)\n    return precision, recall, accuracy","c37acd6d":"def show_feature_importance(forest, x_train):\n    feature_names = x_train.columns.values\n    importances = forest.feature_importances_\n    std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n    \n    forest_importances = pd.Series(importances, index=feature_names)\n    \n    fig, ax = plt.subplots()\n    forest_importances.plot.bar(yerr=std, ax=ax)\n    ax.set_title(\"Feature importances using MDI\")\n    ax.set_ylabel(\"Mean decrease in impurity\")\n    fig.tight_layout()","7a85425f":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\n\nnum_splits = 5\nnum_estimators = 100\nmax_depth = 1\n\nrelevant_data = ['Pclass', 'Male', 'Female', 'HasCabin', 'NoCabin', 'Fare', 'FamilySize']\n\nx_df = df[relevant_data]\ny_df = df['Survived']\n\nkf = KFold(n_splits=num_splits)\n\nprecisions = []\nrecalls = []\naccuracies = []\n\nfor train_index, test_index in kf.split(x_df, y_df):\n    x_train = x_df.iloc[train_index]\n    y_train = y_df.iloc[train_index]\n    x_test = x_df.iloc[test_index]\n    y_test = y_df.iloc[test_index]\n    forest = fit_forest(x_train, y_train, num_estimators, max_depth)\n    precision, recall, accuracy = evaluate_forest(forest, x_test, y_test)\n    precisions.append(precision)\n    recalls.append(recall)\n    accuracies.append(accuracy)\n\nprint(\"Mean accuracy:\")\nprint(np.mean(accuracies))\n\nshow_feature_importance(forest, x_train)","c2fefb72":"def evaluate_svm(svm, x_test, y_test):\n    prediction = svm.predict(x_test)\n    results = [1 if y == y_hat else 0 for (y, y_hat) in zip(prediction, y_test)]\n    tp = sum([1 if (y_hat == 1 and y == 1) else 0 for (y_hat, y) in zip(prediction, y_test)])\n    fp = sum([1 if (y_hat == 0 and y == 1) else 0 for (y_hat, y) in zip(prediction, y_test)])\n    tn = sum([1 if (y_hat == 0 and y == 0) else 0 for (y_hat, y) in zip(prediction, y_test)])\n    fn = sum([1 if (y_hat == 1 and y == 0) else 0 for (y_hat, y) in zip(prediction, y_test)])\n    if tp + fp == 0:\n        precision = 0\n    else:\n        precision = tp \/ (tp+fp)\n    if tp + fn == 0:\n        recall = 0\n    else:\n        recall = tp \/ (tp + fn)\n    accuracy = sum(results)\/len(results)\n    return precision, recall, accuracy","0ca72e52":"from sklearn.svm import SVC\n\nrelevant_data = ['Pclass', 'Male', 'Female', 'HasCabin', 'NoCabin', 'Fare', 'FamilySize']\n\nnum_splits = 5\n\nkf = KFold(n_splits=num_splits)\n\nx_df = df[relevant_data]\n\nfor train_index, test_index in kf.split(x_df, y_df):\n    # Split data into train and test series\n    x_train = x_df.iloc[train_index]\n    y_train = y_df.iloc[train_index]\n    x_test = x_df.iloc[test_index]\n    y_test = y_df.iloc[test_index]\n    # Convert train and test series to np arrays\n    x_vectors = []\n    for index, row in x_train.iterrows():\n        x_vectors.append(np.array(row.values))\n    y_vectors = np.array(y_train.values)\n    x_test_vectors = []\n    for index, row in x_test.iterrows():\n        x_test_vectors.append(np.array(row.values))\n    y_test_vectors = np.array(y_test.values)\n    # train and test svm\n    svm = SVC(kernel='rbf')\n    svm.fit(x_vectors, y_vectors)    \n    precision, recall, accuracy = evaluate_svm(svm, x_test, y_test)\n    precisions.append(precision)\n    recalls.append(recall)\n    accuracies.append(accuracy)\n\nprint(\"Mean Accuracy: \" + str(np.mean(accuracies)))","c44a465d":"df_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","7aaf962e":"mean_age = int (df['Age'].mean())\n\ndf_test['Age'] = df_test['Age'].fillna(mean_age)\ndf_test['Cabin'] = df_test['Cabin'].fillna('X')\n\n    df_test['Embarked'] = df_test['Embarked'].fillna('S')\ndf_test.loc[df_test['Embarked'] == 'S', 'Embarked'] = 0\ndf_test.loc[df_test['Embarked'] == 'C', 'Embarked'] = 1\ndf_test.loc[df_test['Embarked'] == 'Q', 'Embarked'] = 2\n\ndf_test['Sex'] = df_test['Sex'].apply(lambda x : 0 if x == 'male' else 1)\ndf_test['Male'] = (df_test['Sex'] == 0).astype(int)\ndf_test['Female'] = (df_test['Sex'] == 1).astype(int)\n\ndf_test['FamilySize'] = df_test['SibSp'] + df_test['Parch']\n\ndf_test['Cabin'] = df_test['Cabin'].apply(lambda x : 0 if x == 'X' else 1)\ndf_test['HasCabin'] = (df_test['Cabin'] == 1).astype(int)\ndf_test['NoCabin'] = (df_test['Cabin'] == 0).astype(int)\n\ndf_test['Fare'] = df_test['Fare'].apply(lambda x : np.log(x) if x > 0 else 0)","e80750d8":"x_df_test = df_test[relevant_data]\n\ndf_test.set_index('PassengerId', inplace=True)","fd82f84c":"prediction = forest.predict(x_df_test)\n\ntest_results = pd.DataFrame(data=zip(df_test.index.values, prediction), columns=['PassengerId', 'Survived'])\ntest_results.set_index('PassengerId', inplace=True)","fae2e2f2":"test_results.to_csv('\/kaggle\/working\/results.csv')","d04e9792":"<ul>\n    <li> Correct missing Embarked values for Am\u00e9lie Icard and Mrs. George Nelson \n        <br>(based on info from: \n        <a href=\"url\"> https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/amelia-icard.html)<\/a>\n    <li> Replace missing 'Age' entries by the mean age\n    <li> Convert 'Cabin' entries to 1 or 0\n    <li> Convert categorical data to one-hot encoded columns\n    <li> Normalize 'Fare' data by taking the log of the values\n<\/ul>","78471fb4":"### Testing function for the SVM","abd6926c":"### Training function for the tree model","96931eb3":"# 1. Introduce data","0b59f46b":"### Testing function for the tree model using precision, recall and accuracy","cb2e21ab":"# Submission of test data","461c39cf":"## Training and testing the SVM model using 5-fold cross-validation","de56c9b4":"## Preprocess testing data","4c9f63ee":"# 4. SVM","b5ba31af":"# Purpose of this notebook\n\nI am getting started in the world of data science, so I set this notebook up as a type of sandbox to try out different approaches to the Titanic dataset. I want to focus on getting familiar with fundamental functions and frameworks (sklearn, pandas). I will likely come back to this notebook once in a while to try a new approach or improve on the current methods. At the moment of writing this, I have implemented a random forest approach and an SVM, though with modest feature engineering.\n\nCONTENTS:\n- Introduce Data\n- Preprocessing\n- Random Forest\n- SVM\n- Submit Results","2022e99f":"## Execute Training and Testing on 5-fold cross-validation","b9063506":"# 2. Preprocess data","5fe60cd9":"# 3. Random Forest Model"}}