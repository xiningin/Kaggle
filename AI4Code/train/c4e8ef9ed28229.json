{"cell_type":{"76648259":"code","acaa952e":"code","30555ba8":"code","617b3aa9":"code","fdf4ef84":"code","168edee6":"code","aa565122":"code","cdf121b3":"code","74fcbad5":"code","ead66e0d":"code","2074d34d":"code","c69523d0":"code","d0b69a81":"code","64011ec9":"code","9b5d5ae5":"code","d88c56a4":"code","7ce57d92":"code","68c9a8c0":"code","7f220d64":"code","ab7a4ad2":"code","bb12617b":"code","fe910ffa":"code","2f40ac4d":"code","f083e685":"code","f48928cb":"code","faf4b6fd":"code","bedb45d5":"code","724ac908":"code","a224f097":"code","3e6bfd56":"code","f630b87b":"code","b3743d63":"code","49a99c4a":"code","706fa8ef":"code","873e346b":"code","eb3994b5":"code","327358a0":"code","c7053259":"code","ac28cbee":"code","a2465dca":"code","a6acb8dd":"code","41ef2999":"code","9208206b":"code","162b95e2":"code","7ed1fdcc":"code","5dde9f97":"code","97839f90":"code","70d0a071":"code","8483d42c":"code","1a82335e":"code","3310b436":"code","ddbc5641":"code","5413670b":"code","6300a6aa":"code","f4173999":"code","42257c6e":"code","8de61b24":"code","57328e00":"code","84fd4475":"code","c2773a81":"code","f5989ed5":"code","e02ccf87":"code","08178a13":"code","805752e5":"code","4e3a396a":"code","162ebb5e":"code","0f91bd8c":"code","305b5b70":"code","8f3f5612":"code","ad1d02c6":"code","c5b6ad64":"code","7046c2aa":"code","41e63187":"code","e9815279":"code","13ea60eb":"code","ec0e044d":"code","7cf68779":"code","7dfba9c9":"code","e39fd690":"code","d3f0f41e":"code","106d5321":"code","5f45beb9":"code","85de4d7e":"code","1fcfc3d5":"code","1f8b6b0d":"code","357502c0":"code","cd27876f":"code","92e6693b":"code","ab9e1bfb":"code","607d0039":"code","0692dabd":"code","999c5fcc":"code","c8786b69":"code","8c37ffaa":"code","9f1c5441":"code","d3967c83":"code","402daa8e":"code","a067033e":"code","cc60650d":"code","0dff095c":"code","cedb2393":"code","b633736b":"code","35f28ba7":"code","d87e9af8":"code","3d82e95c":"code","670501cf":"code","ad4f82f3":"code","0781afd3":"markdown","3c6b979e":"markdown","c054946a":"markdown","0f309295":"markdown","29321e84":"markdown","9ed15e12":"markdown","38bab79e":"markdown","427642b3":"markdown"},"source":{"76648259":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","acaa952e":"import pandas_profiling","30555ba8":"train=pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest=pd.read_csv(\"..\/input\/titanic\/test.csv\")","617b3aa9":"#train.profile_report()","fdf4ef84":"#train.info()","168edee6":"#test.profile_report()","aa565122":"def missing_percentage(df):\n    \"\"\"This function takes a DataFrame(df) as input and returns two columns, total missing values and total missing values percentage\"\"\"\n    ## the two following line may seem complicated but its actually very simple. \n    total = df.isnull().sum().sort_values(ascending = False)[df.isnull().sum().sort_values(ascending = False) != 0]\n    percent = round(df.isnull().sum().sort_values(ascending = False)\/len(df)*100,2)[round(df.isnull().sum().sort_values(ascending = False)\/len(df)*100,2) != 0]\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])","cdf121b3":"missing_percentage(train)","74fcbad5":"missing_percentage(test)","ead66e0d":"def missing_value_count(df,feature):\n    total=pd.DataFrame(df.loc[:,feature].value_counts(dropna=False))\n    percent=round(total\/len(df)*100,2)\n    total.columns = [\"Total\"]\n    percent.columns = ['Percent']\n    return  pd.concat([total,percent],axis=1)","2074d34d":"missing_value_count(train,'Embarked')","c69523d0":"train[train.Embarked.isnull()]","d0b69a81":"train.loc[(train['Survived'] == 1) & (train['Pclass'] == 1) & (train['Sex']=='female' )].loc[:,'Embarked'].value_counts()","64011ec9":"train.Embarked.fillna('S',inplace=True)","9b5d5ae5":"#FOR imputation of cabin feature we concat the train and test set\nsurvivers=train.Survived","d88c56a4":"train.drop('Survived',inplace=True,axis=1)","7ce57d92":"all_data=pd.concat([train,test],ignore_index=False)","68c9a8c0":"all_data.Cabin.fillna('N',inplace=True)","7f220d64":"all_data.Cabin=[i[0] for i in all_data.Cabin]","ab7a4ad2":"missing_value_count(all_data,'Cabin')","bb12617b":"with_N=all_data[all_data.Cabin=='N']","fe910ffa":"without_N=all_data[all_data.Cabin!='N']","2f40ac4d":"all_data.groupby('Cabin').Fare.mean().sort_values(ascending=False)","f083e685":"all_data.Cabin.value_counts(ascending=False)","f48928cb":"def cabin_estimator(i):\n    \"\"\"Grouping cabin feature by the first letter\"\"\"\n    a = 0\n    if i<16:\n        a = \"G\"\n    elif i>=16 and i<27:\n        a = \"F\"\n    elif i>=27 and i<38:\n        a = \"T\"\n    elif i>=38 and i<47:\n        a = \"A\"\n    elif i>= 47 and i<53:\n        a = \"E\"\n    elif i>= 53 and i<54:\n        a = \"D\"\n    elif i>=54 and i<116:\n        a = 'C'\n    else:\n        a = \"B\"\n    return a","faf4b6fd":"with_N['Cabin']=with_N.Fare.apply(lambda x: cabin_estimator(x))","bedb45d5":"with_N.Cabin.unique()","724ac908":"## getting back train. \nall_data = pd.concat([with_N, without_N], axis=0)\n## PassengerId helps us separate train and test. \nall_data.sort_values(by = 'PassengerId', inplace=True)","a224f097":"all_data.Cabin.unique()","3e6bfd56":"## Separating train and test from all_data. \ntrain = all_data[:891]#exclude 891 means up to 890 index value\n\ntest = all_data[891:]","f630b87b":"train.info()","b3743d63":"train['Survived']=survivers","49a99c4a":"test[test.Fare.isnull()]","706fa8ef":"missing_value=test[(test.Pclass==3)&(test.Sex=='male')&(test.Embarked=='S')].Fare.mean()","873e346b":"missing_value","eb3994b5":"test.Fare.fillna(missing_value,inplace=True)","327358a0":"import seaborn as sns\nimport matplotlib.pyplot as plt","c7053259":"sns.barplot(x = \"Sex\", \n                 y = \"Survived\", \n                 data=train, \n#                 linewidth=5,\n #                capsize = .05,\n\n                )\n\nplt.title(\"Survived\/Non-Survived Passenger Gender Distribution\", fontsize = 25,loc = 'center', pad = 40)","ac28cbee":"plt.subplots(figsize = (15,8))\nax = sns.countplot(x = \"Sex\", \n                   hue=\"Survived\",\n                   data = train, \n                   linewidth=4,\n)\nplt.title(\"Passenger Gender Distribution - Survived vs Not-survived\", fontsize = 25, pad=40)\nleg = ax.get_legend()\nleg.set_title(\"Survived\")\nlegs = leg.texts\nlegs[0].set_text(\"No\")\nlegs[1].set_text(\"Yes\")\nplt.show()    ","a2465dca":"train.describe()","a6acb8dd":"#Heatmap\n#representing correlation \nplt.subplots(figsize = (15,12))\nsns.heatmap(train.corr(),annot=True,cmap = 'RdBu',linewidths=.9,linecolor='grey', center = 0,square=True)\nplt.title(\"Correlations Among Features\", y = 1.03,fontsize = 20, pad = 40)","41ef2999":"#Creating feature that conatins name length\ntrain['name_length']=[len(i) for i in train.Name]","9208206b":"test['name_length']=[len(i) for i in test.Name]","162b95e2":"def name_length_group(size):\n    a=''\n    if (size<=20):\n        a='short'\n    elif(size<=35):\n        a='medium'\n    elif (size<=45):\n        a='good'\n    else:\n        a='long'\n    return a\n                ","7ed1fdcc":"train['nlength_group']=train['name_length'].map(name_length_group)","5dde9f97":"test['nlength_group']=test['name_length'].map(name_length_group)","97839f90":"train['title']=[i.split('.')[0] for i in train.Name]","70d0a071":"train['title']=[i.split(',')[1] for i in train.title]","8483d42c":"test['title']=[i.split('.')[0] for i in test.Name]\ntest['title']=[i.split(',')[1] for i in test.title]","1a82335e":"train['title']=[i.replace('Ms','Miss') for i in train.title]\ntrain[\"title\"] = [i.replace('Mlle', 'Miss') for i in train.title]\ntrain[\"title\"] = [i.replace('Mme', 'Mrs') for i in train.title]\ntrain[\"title\"] = [i.replace('Dr', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Col', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Major', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Don', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Jonkheer', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Sir', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Lady', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Capt', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('the Countess', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Rev', 'rare') for i in train.title]\n","3310b436":"test['title'] = [i.replace('Ms', 'Miss') for i in test.title]\ntest['title'] = [i.replace('Dr', 'rare') for i in test.title]\ntest['title'] = [i.replace('Col', 'rare') for i in test.title]\ntest['title'] = [i.replace('Dona', 'rare') for i in test.title]\ntest['title'] = [i.replace('Rev', 'rare') for i in test.title]","ddbc5641":"test.title.unique()","5413670b":"train[['Sex','title']].groupby('title').count()","6300a6aa":"train['family_size']=train.SibSp+train.Parch+1\ntest['family_size']=test.SibSp+test.Parch+1","f4173999":"print(train.family_size.unique())\nprint(test.family_size.unique())","42257c6e":"def family_group(size):\n    a=''\n    if (size<=1):\n        a='loner'\n    elif (size<=4):\n        a='small'\n    else:\n        a='large'\n    return a","8de61b24":"train[\"family_group\"]=train.family_size.map(family_group)\ntrain.head()","57328e00":"test['family_group'] = test['family_size'].map(family_group)","84fd4475":"train['is_alone'] = [1 if i<2 else 0 for i in train.family_size]\ntest['is_alone'] = [1 if i<2 else 0 for i in test.family_size]","c2773a81":"train.drop(['Ticket'], axis=1, inplace=True)\ntest.drop(['Ticket'], axis=1, inplace=True)","f5989ed5":"train['calculated_fare']=train.Fare\/train.family_size\ntrain.head()","e02ccf87":"test['calculated_fare']=test.Fare\/test.family_size","08178a13":"bins=[0,4,10,20,45,600]\ngroup=['very_low','small','medium','high','very_high']\ntrain['fare_group']=pd.cut(train['calculated_fare'],bins,labels=group)\ntest['fare_group']=pd.cut(test['calculated_fare'],bins,labels=group)","805752e5":"train.drop(['PassengerId'], axis=1, inplace=True)\n\ntest.drop(['PassengerId'], axis=1, inplace=True)","4e3a396a":"train = pd.get_dummies(train, columns=['title',\"Pclass\", 'Cabin','Embarked','nlength_group', 'family_group', 'fare_group'], drop_first=False)","162ebb5e":"test = pd.get_dummies(test, columns=['title',\"Pclass\",'Cabin','Embarked','nlength_group', 'family_group', 'fare_group'], drop_first=False)\ntrain.drop(['family_size','Name', 'Fare','name_length'], axis=1, inplace=True)","0f91bd8c":"test.drop(['Name','family_size',\"Fare\",'name_length'], axis=1, inplace=True)","305b5b70":"train = pd.concat([train[[\"Survived\", \"Age\", \"Sex\",\"SibSp\",\"Parch\"]], train.loc[:,\"is_alone\":]], axis=1)\ntest = pd.concat([test[[\"Age\", \"Sex\"]], test.loc[:,\"SibSp\":]], axis=1)","8f3f5612":"from sklearn.ensemble import RandomForestRegressor","ad1d02c6":"def completing_age(df):\n    ## gettting all the features except survived\n    age_df = df.loc[:,\"Age\":] \n    \n    temp_train = age_df.loc[age_df.Age.notnull()] ## df with age values\n    temp_test = age_df.loc[age_df.Age.isnull()] ## df without age values\n    \n    y = temp_train.Age.values ## setting target variables(age) in y \n    x = temp_train.loc[:, \"Sex\":].values\n    \n    rfr = RandomForestRegressor(n_estimators=1500, n_jobs=-1)\n    rfr.fit(x, y)\n    \n    predicted_age = rfr.predict(temp_test.loc[:, \"Sex\":])\n    \n    df.loc[df.Age.isnull(), \"Age\"] = predicted_age\n    \n\n    return df","c5b6ad64":"train.Sex=train.Sex.apply(lambda x : 0 if x=='female' else 1)\ntest.Sex=test.Sex.apply(lambda x : 0 if x=='female' else 1)","7046c2aa":"completing_age(train)","41e63187":"completing_age(test);","e9815279":"## create bins for age\ndef age_group_fun(age):\n    a = ''\n    if age <= 1:\n        a = 'infant'\n    elif age <= 4: \n        a = 'toddler'\n    elif age <= 13:\n        a = 'child'\n    elif age <= 18:\n        a = 'teenager'\n    elif age <= 35:\n        a = 'Young_Adult'\n    elif age <= 45:\n        a = 'adult'\n    elif age <= 55:\n        a = 'middle_aged'\n    elif age <= 65:\n        a = 'senior_citizen'\n    else:\n        a = 'old'\n    return a\n        \n## Applying \"age_group_fun\" function to the \"Age\" column.\ntrain['age_group'] = train['Age'].map(age_group_fun)\ntest['age_group'] = test['Age'].map(age_group_fun)\n\n## Creating dummies for \"age_group\" feature. \ntrain = pd.get_dummies(train,columns=['age_group'], drop_first=True)\ntest = pd.get_dummies(test,columns=['age_group'], drop_first=True);\n\ntrain.drop('Age', axis=1, inplace=True)\ntest.drop('Age', axis=1, inplace=True)","13ea60eb":"X = train.drop(['Survived'], axis = 1)\ny = train[\"Survived\"]","ec0e044d":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = .33, random_state=0)","7cf68779":"headers = X_train.columns \n\nX_train.head()","7dfba9c9":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\n## transforming \"train_x\"\nX_train = sc.fit_transform(X_train)\n## transforming \"test_x\"\nX_test = sc.transform(X_test)\n\n## transforming \"The testset\"\ntest = sc.transform(test)","e39fd690":"pd.DataFrame(X_train, columns=headers).head()","d3f0f41e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_absolute_error,accuracy_score\nlogreg=LogisticRegression(solver='liblinear',penalty='l1')\nlogreg.fit(X_train,y_train)\ny_pred=logreg.predict(X_test)\nprint('Our accuracy score is :{}'.format(round(accuracy_score(y_pred,y_test),4)))","106d5321":"#use of cross-validation\nfrom sklearn.model_selection import StratifiedShuffleSplit,cross_val_score\ncv=StratifiedShuffleSplit(n_splits=10,test_size=.25,random_state=0)\n## saving the feature names for decision tree display\ncolumn_names = X.columns","5f45beb9":"X = sc.fit_transform(X)\n","85de4d7e":"accuracies = cross_val_score(LogisticRegression(solver='liblinear'), X,y, cv  = cv)","1fcfc3d5":"print (\"Cross-Validation accuracy scores:{}\".format(accuracies))\nprint (\"Mean Cross-Validation accuracy score: {}\".format(round(accuracies.mean(),5)))","1f8b6b0d":"from sklearn.model_selection import GridSearchCV, StratifiedKFold\nC_vals = [0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16,16.5,17,17.5,18]\npenalties = ['l1','l2']\ncv = StratifiedShuffleSplit(n_splits = 10, test_size = .25)\nparam = {'penalty': penalties, 'C': C_vals}\nlogreg = LogisticRegression(solver='liblinear')\ngrid = GridSearchCV(estimator=LogisticRegression(),param_grid = param,scoring = 'accuracy',n_jobs =-1,cv = cv)\ngrid.fit(X, y)","357502c0":"## Getting the best of everything. \nprint (grid.best_score_)\nprint (grid.best_params_)\nprint(grid.best_estimator_)","cd27876f":"### Using the best parameters from the grid-search.\nlogreg_grid = grid.best_estimator_\nlogreg_grid.score(X,y)","92e6693b":"from sklearn.neighbors import KNeighborsClassifier\n## calling on the model oject. \nknn = KNeighborsClassifier(metric='minkowski', p=2)\n## knn classifier works by doing euclidian distance \n\n\n## doing 10 fold staratified-shuffle-split cross validation \ncv = StratifiedShuffleSplit(n_splits=10, test_size=.25, random_state=2)\n\naccuracies = cross_val_score(knn, X,y, cv = cv, scoring='accuracy')\nprint (\"Cross-Validation accuracy scores:{}\".format(accuracies))\nprint (\"Mean Cross-Validation accuracy score: {}\".format(round(accuracies.mean(),3)))","ab9e1bfb":"k_range = range(1,31)\nk_scores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X,y, cv = cv, scoring = 'accuracy')\n    k_scores.append(scores.mean())\nprint(\"Accuracy scores are: {}\\n\".format(k_scores))\nprint (\"Mean accuracy score: {}\".format(np.mean(k_scores)))","607d0039":"from matplotlib import pyplot as plt\nplt.plot(k_range, k_scores)","0692dabd":"from sklearn.model_selection import GridSearchCV\n## trying out multiple values for k\nk_range = range(1,31)\n## \nweights_options=['uniform','distance']\n# \nparam = {'n_neighbors':k_range, 'weights':weights_options}\n## Using startifiedShufflesplit. \ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n# estimator = knn, param_grid = param, n_jobs = -1 to instruct scikit learn to use all available processors. \nknn_grid = GridSearchCV(KNeighborsClassifier(), param,cv=cv,verbose = False, n_jobs=-1)\n## Fitting the model. \nknn_grid.fit(X,y)\nknn_estimator= knn_grid.best_estimator_\nknn_estimator.score(X,y)","999c5fcc":"from sklearn.model_selection import RandomizedSearchCV\n## trying out multiple values for k\nk_range = range(1,31)\n## \nweights_options=['uniform','distance']\n# \nparam = {'n_neighbors':k_range, 'weights':weights_options}\n## Using startifiedShufflesplit. \ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30)\n# estimator = knn, param_grid = param, n_jobs = -1 to instruct scikit learn to use all available processors. \n## for RandomizedSearchCV, \nrgrid = RandomizedSearchCV(KNeighborsClassifier(), param,cv=cv,verbose = False, n_jobs=-1, n_iter=40)\n## Fitting the model. \nrgrid.fit(X,y)\nknn_restimator = rgrid.best_estimator_\nknn_restimator.score(X,y)","c8786b69":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(X, y)\ny_pred = gaussian.predict(X_test)\ngaussian_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(gaussian_accy)","8c37ffaa":"from sklearn.svm import SVC\nCs = [0.001, 0.01, 0.1, 1,1.5,2,2.5,3,4,5, 10] ## penalty parameter C for the error term. \ngammas = [0.0001,0.001, 0.01, 0.1, 1]\nparam_grid = {'C': Cs, 'gamma' : gammas}\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\nsv_grid = GridSearchCV(SVC(kernel = 'rbf', probability=True), param_grid, cv=cv) ## 'rbf' stands for gaussian kernel\nsv_grid.fit(X,y)\nsv_estimator = sv_grid.best_estimator_\nsv_estimator.score(X,y)","9f1c5441":"from sklearn.tree import DecisionTreeClassifier\nmax_depth = range(1,30)\nmax_feature = [21,22,23,24,25,26,28,29,30,'auto']\ncriterion=[\"entropy\", \"gini\"]\n\nparam = {'max_depth':max_depth, \n         'max_features':max_feature, \n         'criterion': criterion}\ndt_grid = GridSearchCV(DecisionTreeClassifier(), \n                                param_grid = param, \n                                 verbose=False, \n                                 cv=StratifiedKFold(n_splits=20, random_state=15, shuffle=True),\n                                n_jobs = -1)\ndt_grid.fit(X, y) \ndt_estimator = dt_grid.best_estimator_ \ndt_estimator.score(X,y)","d3967c83":"from sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.ensemble import RandomForestClassifier\nn_estimators = [140,145,150,155,160];\nmax_depth = range(1,10);\ncriterions = ['gini', 'entropy'];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n\n\nparameters = {'n_estimators':n_estimators,\n              'max_depth':max_depth,\n              'criterion': criterions\n              \n        }\nrf_grid = GridSearchCV(estimator=RandomForestClassifier(max_features='auto'),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\nrf_grid.fit(X,y) \nrf_estimator=rf_grid.best_estimator_\nrf_estimator.score(X,y)","402daa8e":"from sklearn.ensemble import BaggingClassifier\nn_estimators = [10,30,50,70,80,150,160, 170,175,180,185];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n\nparameters = {'n_estimators':n_estimators,\n              \n        }\ngrid = GridSearchCV(BaggingClassifier(base_estimator= None, ## If None, then the base estimator is a decision tree.\n                                      bootstrap_features=False),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\ngrid.fit(X,y) \ngrid.best_score_\ngrid.best_params_\ngrid.best_estimator_\nbagging_grid = grid.best_estimator_\nbagging_grid.score(X,y)\nbagging_grid = grid.best_estimator_\nbagging_estimator=grid.best_estimator_\nbagging_grid.score(X,y)","a067033e":"\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit,learning_curve\ndef plot_learning_curve(estimator1, X, y, ylim=None, cv=None,n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ax1 = plt.subplots()\n    # First Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"bagging classifier Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    return plt\ncv = StratifiedShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(bagging_estimator, X, y, cv=cv, n_jobs=4)","cc60650d":"\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit,learning_curve\ndef plot_learning_curve(estimator1, X, y, ylim=None, cv=None,n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ax1 = plt.subplots()\n    # First Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"random forest classifier Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    return plt\ncv = StratifiedShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(rf_estimator, X, y, cv=cv, n_jobs=4)","0dff095c":"\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit,learning_curve\ndef plot_learning_curve(estimator1, X, y, ylim=None, cv=None,n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ax1 = plt.subplots()\n    # First Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"decision tree classifier Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    return plt\ncv = StratifiedShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(dt_estimator, X, y, cv=cv, n_jobs=4)","cedb2393":"\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit,learning_curve\ndef plot_learning_curve(estimator1, X, y, ylim=None, cv=None,n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ax1 = plt.subplots()\n    # First Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"SVM classifier Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    return plt\ncv = StratifiedShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(sv_estimator, X, y, cv=cv, n_jobs=4)","b633736b":"\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit,learning_curve\ndef plot_learning_curve(estimator1, X, y, ylim=None, cv=None,n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ax1 = plt.subplots()\n    # First Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"knn classifier Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    return plt\ncv = StratifiedShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(knn_restimator, X, y, cv=cv, n_jobs=4)","35f28ba7":"test_prediction = sv_estimator.predict(test)","d87e9af8":"test_data=pd.read_csv(\"..\/input\/titanic\/test.csv\")","3d82e95c":"submission = pd.DataFrame({\n        \"PassengerId\":test_data.PassengerId,\n        \"Survived\": test_prediction\n    })","670501cf":"submission.PassengerId = submission.PassengerId.astype(int)\nsubmission.Survived = submission.Survived.astype(int)","ad4f82f3":"submission.to_csv(\"titanic_1st_submission.csv\", index=False)","0781afd3":"*.Embarked","3c6b979e":"Feature Engineering","c054946a":"1.Name","0f309295":"*Cabin","29321e84":"a.Embarked","9ed15e12":"Missing Vlaues","38bab79e":"Taking majority","427642b3":"Filling missing values "}}