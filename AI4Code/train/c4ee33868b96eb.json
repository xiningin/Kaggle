{"cell_type":{"f6ac6b0b":"code","2cfff4fb":"code","00fa2c8a":"code","9a844bd1":"code","0b464c2b":"code","7e152927":"code","79fa3778":"code","635b1e9b":"code","915c5e22":"code","1cc083ea":"code","88edc220":"code","dd1a726d":"code","64a5ccee":"code","65e1b527":"code","bc4abed6":"code","e6d0c1c9":"code","328ecc8e":"code","b4e557d3":"code","6d689a98":"code","556a63b2":"code","57099f4b":"markdown","c342e70c":"markdown","78bbff89":"markdown","da057e46":"markdown","fc4d6163":"markdown","9d9a7c92":"markdown"},"source":{"f6ac6b0b":"# data manipulation\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set()\n\n# sklearn models & tools\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import make_scorer\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nimport os\nprint(os.listdir(\"..\/input\"))","2cfff4fb":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/sample_submission.csv\")","00fa2c8a":"train.shape","9a844bd1":"X = train.drop(['ID_code','target'],axis=1)\ny = train['target']\n\nX_test = test.drop('ID_code',axis=1)","0b464c2b":"X['i_am_train'] = 1\nX_test['i_am_train'] = 0","7e152927":"full = pd.concat([X,X_test],axis=0)","79fa3778":"your_feature='var_68'\nyour_threshold=5.036\nfig, ax = plt.subplots(1,1,figsize=(20,5))\nplt.hist(full.loc[:, your_feature].values, bins=500);\nax.axvline(your_threshold, c=\"red\")","635b1e9b":"full['var68_bin_index'] = np.digitize(full['var_68'],plt.hist(full.loc[:, your_feature].values, bins=500)[1])","915c5e22":"# [242,259,226,275,210,194,291,178,307,162,146,130,113,81,97,323,339,355,371,388,404]","1cc083ea":"full['var_0' + \"_bin\"] = np.where(full.loc[:, 'var_0'] <= 12.1, 1, 0)\nfull['var_1' + \"_bin\"] = np.where(full.loc[:, 'var_1'] <= 7.3, 1, 0)\nfull['var_2' + \"_bin\"] = np.where(full.loc[:, 'var_2'] <= 12, 1, 0)\nfull['var_4' + \"_bin\"] = np.where(full.loc[:, 'var_4'] <= 14.1, 1, 0)\nfull['var_6' + \"_bin\"] = np.where(full.loc[:, 'var_6'] <= 5.6, 1, 0)\nfull['var_7' + \"_bin\"] = np.where(full.loc[:, 'var_7'] <= 10.55, 1, 0)\nfull['var_9' + \"_bin\"] = np.where(full.loc[:, 'var_9'] <= 8, 1, 0)\nfull['var_12' + \"_bin\"] = np.where(full.loc[:, 'var_12'] <= 14, 1, 0)\nfull['var_12_weirdos'] = 0\nfull.loc[np.round(full['var_12'],3).isin([13.554,13.555]), 'var_12_weirdos'] = 1\nfull['var_13' + \"_bin\"] = np.where(full.loc[:, 'var_13'] <= 9.7, 1, 0)\nfull['var_16' + \"_bin\"] = np.where(full.loc[:, 'var_16'] <= 10.6, 1, 0)\nfull['var_26' + \"_bin\"] = np.where(full.loc[:, 'var_26'] <= 8, 1, 0)\nfull['var_27' + \"_bin\"] = np.where(full.loc[:, 'var_27'] <= 2.1, 1, 0)\nfull['var_28' + \"_bin\"] = np.where(full.loc[:, 'var_28'] <= 5.55, 1, 0)\nfull['var_29' + \"_bin\"] = np.where(full.loc[:, 'var_29'] <= 11, 1, 0)\nfull['var_35' + \"_bin\"] = np.where(full.loc[:, 'var_35'] <= 14, 1, 0)\nfull['var_37' + \"_bin\"] = np.where(full.loc[:, 'var_37'] <= 11, 1, 0)\nfull['var_40' + \"_bin\"] = np.where(full.loc[:, 'var_40'] <= 10.8, 1, 0)\nfull['var_41' + \"_bin\"] = np.where(full.loc[:, 'var_41'] <= 9.2, 1, 0)\nfull['var_43' + \"_bin\"] = np.where(full.loc[:, 'var_43'] <= 10.95, 1, 0)\nfull['var_53' + \"_bin\"] = np.where(full.loc[:, 'var_53'] <= 7.74, 1, 0)\nfull['var_55' + \"_bin\"] = np.where(full.loc[:, 'var_55'] <= 10.1, 1, 0)\nfull['var_59' + \"_bin\"] = np.where(full.loc[:, 'var_59'] <= 7.37, 1, 0)\nfull['var_60' + \"_bin\"] = np.where(full.loc[:, 'var_60'] <= 12, 1, 0)\nfull['var_68_highcounts'] = 0\nfull.loc[full['var68_bin_index'].isin([242,259,226,275,210,194,291,178,307,162,146,130,113,81,97,323,339,355,371,388,404]), 'var_68_highcounts'] = 1\nfull['var_73' + \"_bin\"] = np.where(full.loc[:, 'var_73'] <= 6, 1, 0)\nfull['var_75' + \"_bin\"] = np.where(full.loc[:, 'var_75'] <= 3.3, 1, 0)\nfull['var_80' + \"_bin\"] = np.where(full.loc[:, 'var_80'] <= 3.15, 1, 0)\nfull['var_81' + \"_bin\"] = np.where(full.loc[:, 'var_81'] <= 9.5, 1, 0)\nfull['var_86' + \"_bin\"] = np.where(full.loc[:, 'var_86'] <= 2, 1, 0)\nfull['var_88' + \"_bin\"] = np.where(full.loc[:, 'var_88'] <= 4, 1, 0)\nfull['var_89' + \"_bin\"] = np.where(full.loc[:, 'var_89'] <= 12.2, 1, 0)\nfull['var_92' + \"_bin\"] = np.where(full.loc[:, 'var_92'] <= 4.7, 1, 0)\nfull['var_95' + \"_bin\"] = np.where(full.loc[:, 'var_95'] <= 1.2, 1, 0)\nfull['var_98' + \"_bin\"] = np.where(full.loc[:, 'var_98'] <= 0.3, 1, 0)\nfull['var_99' + \"_bin\"] = np.where(full.loc[:, 'var_99'] <= 3.6, 1, 0)\nfull['var_101' + \"_bin\"] = np.where(full.loc[:, 'var_101'] <= 2.5, 1, 0)\nfull['var_108' + \"_bin\"] = np.where(full.loc[:, 'var_108'] <= 14.2155, 1, 0)\nfull['var_115' + \"_bin\"] = np.where(full.loc[:, 'var_115'] <= 2.4, 1, 0)\nfull['var_123' + \"_bin\"] = np.where(full.loc[:, 'var_123'] <= 5.65, 1, 0)\nfull['var_126' + \"_bin\"] = np.where(full.loc[:, 'var_126'] <= 11.55, 1, 0)\nfull['var_129' + \"_bin\"] = np.where(full.loc[:, 'var_129'] <= 7.1, 1, 0)\nfull['var_131' + \"_bin\"] = np.where(full.loc[:, 'var_131'] <= 0.75, 1, 0)\nfull['var_134' + \"_bin\"] = np.where(full.loc[:, 'var_134'] <= 8.5, 1, 0)\nfull['var_135' + \"_bin\"] = np.where(full.loc[:, 'var_135'] <= 11, 1, 0)\nfull['var_139' + \"_bin\"] = np.where(full.loc[:, 'var_139'] <= 4, 1, 0)\nfull['var_141' + \"_bin\"] = np.where(full.loc[:, 'var_141'] <= 5, 1, 0)\nfull['var_145' + \"_bin\"] = np.where(full.loc[:, 'var_145'] <= 12.8, 1, 0)\nfull['var_150' + \"_bin\"] = np.where(full.loc[:, 'var_150'] <= 12.4, 1, 0)\nfull['var_151' + \"_bin\"] = np.where(full.loc[:, 'var_151'] <= 9, 1, 0)\nfull['var_153' + \"_bin\"] = np.where(full.loc[:, 'var_153'] <= 12.7, 1, 0)\nfull['var_157' + \"_bin\"] = np.where(full.loc[:, 'var_157'] <= 8, 1, 0)\nfull['var_158' + \"_bin\"] = np.where(full.loc[:, 'var_158'] <= 2, 1, 0)\nfull['var_163' + \"_bin\"] = np.where(full.loc[:, 'var_163'] <= 12.5, 1, 0)\nfull['var_164' + \"_bin\"] = np.where(full.loc[:, 'var_164'] <= 7.5, 1, 0)\nfull['var_166' + \"_bin\"] = np.where(full.loc[:, 'var_166'] <= 2.15, 1, 0)\nfull['var_168' + \"_bin\"] = np.where(full.loc[:, 'var_168'] <= 6.6, 1, 0)\nfull['var_173' + \"_bin\"] = np.where(full.loc[:, 'var_173'] <= 10, 1, 0)\nfull['var_175' + \"_bin\"] = np.where(full.loc[:, 'var_175'] <= 11.6, 1, 0)\nfull['var_176' + \"_bin\"] = np.where(full.loc[:, 'var_176'] <= 10.8, 1, 0)\nfull['var_177' + \"_bin\"] = np.where(full.loc[:, 'var_177'] <= 7.3, 1, 0)\nfull['var_180' + \"_bin\"] = np.where(full.loc[:, 'var_180'] <= 9, 1, 0)\nfull['var_181' + \"_bin\"] = np.where(full.loc[:, 'var_181'] <= 10.76, 1, 0)\nfull['var_186' + \"_bin\"] = np.where(full.loc[:, 'var_186'] <= 4, 1, 0)\nfull['var_187' + \"_bin\"] = np.where(full.loc[:, 'var_187'] <= 13, 1, 0)\nfull['var_188' + \"_bin\"] = np.where(full.loc[:, 'var_188'] <= 8, 1, 0)\nfull['var_191' + \"_bin\"] = np.where(full.loc[:, 'var_191'] <= 8.2, 1, 0)\nfull['var_194' + \"_bin\"] = np.where(full.loc[:, 'var_194'] <= 11.5, 1, 0)\nfull['var_195' + \"_bin\"] = np.where(full.loc[:, 'var_195'] <= 3, 1, 0)\nfull['var_196' + \"_bin\"] = np.where(full.loc[:, 'var_196'] <= 13.5, 1, 0)","88edc220":"full.head()","dd1a726d":"# Split back into train and test\nX = full.loc[full['i_am_train']==1]\nX_test = full.loc[full['i_am_train']==0]\n\ndel full","64a5ccee":"del X['i_am_train'], del X_test['i_am_train']","65e1b527":"X.shape","bc4abed6":"X_test.shape","e6d0c1c9":"import lightgbm as lgb\nimport time","328ecc8e":"params = {'num_leaves': 13,\n         'min_data_in_leaf': 80,\n         'objective': 'binary',\n         'max_depth': -1,\n         'learning_rate': 0.01,\n         'boosting': 'gbdt',\n         'bagging_freq': 5,\n         'bagging_fraction': 0.4,\n         'feature_fraction': 0.627,\n         'bagging_seed': 1337,\n         #'reg_alpha': 1.728910519108444,\n         #'reg_lambda': 4.9847051755586085,\n         'random_state': 1337,\n         'metric': 'auc',\n         'verbosity': -1,\n         #'subsample': 0.81,\n         #'min_gain_to_split': 0.01077313523861969,\n         #'min_child_weight': 19.428902804238373,\n          'min_sum_hessian_in_leaf': 10,\n         'num_threads': 4}","b4e557d3":"folds = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n\nprediction = np.zeros(len(X_test))\noofs = np.zeros(len(X))\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X,y)):\n    print('Fold', fold_n, 'started at', time.ctime())\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    train_data = lgb.Dataset(X_train, label=y_train)\n    valid_data = lgb.Dataset(X_valid, label=y_valid)\n        \n    model = lgb.train(params,train_data,num_boost_round=99999,\n                    valid_sets = [train_data, valid_data],verbose_eval=300,early_stopping_rounds = 1000)\n            \n    oofs[valid_index] = model.predict(X_valid, num_iteration = model.best_iteration)\n    prediction += model.predict(X_test, num_iteration=model.best_iteration)\/5","6d689a98":"sub = pd.DataFrame({\"ID_code\": test.ID_code.values})\nsub[\"target\"] = prediction\nsub.to_csv('submission_original_bin.csv', index=False)","556a63b2":"oof = pd.DataFrame({\"ID_code\": train.ID_code.values})\noof[\"target\"] = oofs\noof.to_csv(\"oofs_original_bin.csv\", index=False)","57099f4b":"## Loading packages <a class=\"anchor\" id=\"load\"><\/a>","c342e70c":"## Sneak a peek at the data <a class=\"anchor\" id=\"data\"><\/a>","78bbff89":"Ok, 200.000 rows and 202 features. ","da057e46":"### New feature importances","fc4d6163":"### Train","9d9a7c92":"## Our goal\n\nIn this competition we are asked to predict if a customer will make a transaction or not regardless of the amount of money transacted. Hence our goal is to solve a binary classification problem. In the data description you can see that the features given are numeric and anonymized. Furthermore the data seems to be artificial as they state that \"the data has the same structure as our real data\". \n\n### Table of contents\n\n1. [Loading packages](#load) (complete)\n2. [Sneak a peek at the data](#data) (complete)\n2. [What can we say about the target?](#target) (complete)\n3. [Can we find relationships between features?](#correlation) (complete)\n4. [Baseline submissions](#baselines)\n5. [Feature engineering](#engineering)"}}