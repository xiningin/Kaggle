{"cell_type":{"2cc629ac":"code","62da7016":"code","cd2adba5":"code","9ae2a4b2":"code","d43e954b":"code","7e200a8e":"code","5cc874af":"code","9545f688":"code","30f17d5a":"code","16f94640":"code","9e986a51":"code","a7105da6":"code","72ffb651":"code","0efd1c01":"code","2b6f2598":"code","08e07b97":"markdown","c9cbfbe5":"markdown","95ea98dc":"markdown","74a580f5":"markdown","fdaa6a71":"markdown","7a26a21b":"markdown","963e951e":"markdown","1bad52af":"markdown","d573d424":"markdown","358c8d2d":"markdown","916a91e9":"markdown","35800332":"markdown"},"source":{"2cc629ac":"import os\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nimport torchmetrics as tm\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch.optim as optim\nfrom torch import nn\nfrom PIL import Image\nfrom torchvision import transforms, models","62da7016":"vgg = models.vgg19(pretrained=True).features\nfor param in vgg.parameters():\n    param.requires_grad_(False)","cd2adba5":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nvgg.to(device)","9ae2a4b2":"ROOT = os.path.join('..', 'input', 'best-artworks-of-all-time')\nBATCH_SIZE = 4\nAVAILABLE_GPUS=min(1, torch.cuda.device_count())","d43e954b":"def load_image(path, shape=None):\n    image = Image.open(path).convert('RGB')\n    in_transform = transforms.Compose([\n                        transforms.Resize(600),\n                        transforms.CenterCrop(600),\n                        transforms.ToTensor(),\n                        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])  #https:\/\/pytorch.org\/vision\/stable\/transforms.html\n    image = in_transform(image)[:3,:,:].unsqueeze(0)\n    return image","7e200a8e":"image = load_image('..\/input\/best-artworks-of-all-time\/images\/images\/Francisco_Goya\/Francisco_Goya_13.jpg').to(device)\nstyle = load_image('..\/input\/best-artworks-of-all-time\/images\/images\/Claude_Monet\/Claude_Monet_50.jpg').to(device)","5cc874af":"def convert(t):\n    i = t.to(\"cpu\").clone().detach()\n    i = i.numpy().squeeze()\n    i = i.transpose(1,2,0)\n    i = i * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n    return i","9545f688":"plt.imshow(convert(image))","30f17d5a":"plt.imshow(convert(style))","16f94640":"def layers(image, model, layers=None):\n    if layers is None:\n        layers = {'0': 'conv1_1',\n                  '5': 'conv2_1',\n                  '10': 'conv3_1',\n                  '19': 'conv4_1',\n                  '21': 'conv4_2',\n                  '28': 'conv5_1'}\n    la = {}\n    x = image\n    for name, layer in model._modules.items():\n        x = layer(x)\n        if name in layers:\n            la[layers[name]] = x\n    return la","9e986a51":"#Initializing gram_matrix function for our tensor image   \ndef gram_m(tensor):\n        _,d,h,w = tensor.size()\n        tensor = tensor.view(d,h*w)  \n        gram = torch.mm(tensor,tensor.t())     \n        return gram  ","a7105da6":"image_l = layers(image, vgg)\nstyle_l = layers(style, vgg)\ngram_count = {layer: gram_m(style_l[layer]) for layer in style_l}\ntarget_image = image.clone().requires_grad_(True).to(device)","72ffb651":"style_weights = {'conv1_1': 0.1,\n                 'conv2_1': 0.1,\n                 'conv3_1': 0.1,\n                 'conv4_1': 0.1,\n                 'conv5_1': 0.1}\n\n#rovnovaha mezi obsahem a stylem\ncontent_weight = 1\nstyle_weight = 1e1","0efd1c01":"show_every = 100\n# iterace\noptimizer = optim.Adam([target_image], lr=0.005)\nsteps = 600  #Kolik iteraci?\n\nfor ii in range(1, steps+1):\n    target_features = layers(target_image, vgg)\n    content_loss = torch.mean((target_features['conv4_2'] - image_l['conv4_2'])**2)\n    style_loss = 0\n   \n    for layer in style_weights:\n        target_feature = target_features[layer]\n        target_gram = gram_m(target_feature)\n        _, d, h, w = target_feature.shape\n        style_gram = gram_count[layer]\n        layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n        style_loss += layer_style_loss \/ (d * h * w)\n        \n    # v\u00fdsledn\u00e1 ztr\u00e1ta\n    total_loss = content_weight * content_loss + style_weight * style_loss\n    \n    # update v\u00fdsledn\u00e9ho obr\u00e1zku\n    optimizer.zero_grad()\n    total_loss.backward()\n    optimizer.step()\n    \n    # Vykreslen\u00ed\n    if  ii % show_every == 0:\n        print('Ztr\u00e1ta: ', total_loss.item())\n        plt.imshow(convert(target_image))\n        plt.show()","2b6f2598":"plt.imshow(convert(target_image))","08e07b97":"Importy knihoven","c9cbfbe5":"Sta\u017een\u00ed a na\u010dten\u00ed vgg knihovny","95ea98dc":"Vychyt\u00e1vka pro zobrazen\u00ed, jak je postupn\u011b vytv\u00e1\u0159en v\u00fdsledn\u00fd obr\u00e1zek","74a580f5":"Definice gramovy matice (https:\/\/python3.foobrdigital.com\/gram-matrix\/)","fdaa6a71":"Namapov\u00e1n\u00ed gramovy matice na funkce","7a26a21b":"Zobrazen\u00ed postupn\u00e9 ztr\u00e1ty","963e951e":"V\u00fdsledek","1bad52af":"Metoda pro konvert z tenzoru, aby bylo mo\u017en\u00e9 zobrazit obr\u00e1zek pomoc\u00ed pyplotu","d573d424":"Na\u010dten\u00ed obr\u00e1zku, kter\u00fd bude modifikov\u00e1n + obr\u00e1zek ud\u00e1vaj\u00edc\u00ed styl","358c8d2d":"Up\u0159ednostn\u011bn\u00ed ur\u010dit\u00fdch vrstev p\u0159ed ostatn\u00edmi vrstvami t\u00edm, \u017ee ke ka\u017ed\u00e9 vrstv\u011b p\u0159i\u0159ad\u00edme ur\u010dit\u00e9 parametry hmotnosti.","916a91e9":"Medota pro transformaci obr\u00e1zk\u016f. Transformace do stejn\u00fdch rozm\u011br\u016f.","35800332":"Na\u010dten\u00ed datasetu"}}