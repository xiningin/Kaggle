{"cell_type":{"763d3d16":"code","04348c59":"code","c8ac3b99":"code","14f5a3ee":"code","b637fe79":"code","30cc62c3":"code","68f2fc8a":"code","b94f50b7":"code","6d140ab3":"code","72018a1f":"code","cd4f1901":"code","08ebcc93":"code","065791a2":"code","fc6d6995":"code","5206cbf9":"code","e4731870":"code","0a03f4de":"code","134309b9":"code","3f147a86":"code","d199a653":"code","9ece4c59":"code","b8476dbe":"code","a9030c43":"code","22bf767f":"code","39350765":"code","91671e8b":"code","003a84df":"code","93de6bc9":"code","273fb9ae":"code","5860bdd9":"code","51675e31":"code","5a7aef1d":"code","8cd6fc53":"code","ec475bc2":"code","888cbdb3":"code","a705441c":"code","a94f6b56":"code","3c83eaa0":"code","0fe70338":"code","1e483e29":"code","3ef1d6a9":"code","06943ab8":"code","f67247f0":"code","b2317bb5":"code","b6f2e063":"code","4cba12eb":"code","452fab43":"code","b7809d27":"code","145236b3":"code","d7dd70ef":"code","7f2fba5c":"code","49f9c454":"code","da124170":"code","eddd4f74":"code","1188f4b2":"code","78a9982e":"code","1c49693d":"code","bfe87b70":"code","8e24ff2a":"code","1b602abd":"code","927c8ae5":"code","02568ccd":"code","321cabc5":"code","053267f2":"code","eeba1506":"code","abdf7f73":"code","85e8b93f":"code","a2b96bc8":"markdown","2787a515":"markdown","4f5b6b62":"markdown","20ab041d":"markdown","3978070e":"markdown","73c0d9de":"markdown","d061c697":"markdown","0981b14a":"markdown","4ac1adc6":"markdown","3f65aa55":"markdown","f1fa4c25":"markdown","60d5a3ee":"markdown","f978befe":"markdown","13221663":"markdown","577a055d":"markdown","bc3847a4":"markdown","a0ff67d6":"markdown","4ea70995":"markdown","6cb9da2a":"markdown","18f04933":"markdown","4f2165dc":"markdown","2263bf11":"markdown","9058c653":"markdown","41e69037":"markdown","2166da2b":"markdown","a559cefb":"markdown","addc1fed":"markdown","d6c8d708":"markdown","89d779c6":"markdown","7573fd41":"markdown","f78f1c4e":"markdown","c9265f78":"markdown","138f435e":"markdown","9d65ddb9":"markdown","e98eaec0":"markdown","43b325c2":"markdown","c1580a61":"markdown","441845b8":"markdown","346c0184":"markdown","c0b820a7":"markdown","4e462386":"markdown","8c84775c":"markdown","18ed304f":"markdown","f3b7616d":"markdown","ea0399f6":"markdown"},"source":{"763d3d16":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS \nfrom datetime import datetime\nimport plotly\nplotly.offline.init_notebook_mode (connected = True)\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport squarify\nimport matplotlib.colors as mcolors\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import KernelPCA,PCA,TruncatedSVD\nimport spacy\nimport en_core_web_sm\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\nimport calendar\n# nlp = en_core_web_sm.load()\n# !python -m spacy download en_core_web_md","04348c59":"### Copied from justfortherec's answer: https:\/\/stackoverflow.com\/questions\/28931224\/adding-value-labels-on-a-matplotlib-bar-chart\n\n\ndef add_value_labels(ax, spacing=5):\n    \"\"\"Add labels to the end of each bar in a bar chart.\n\n    Arguments:\n        ax (matplotlib.axes.Axes): The matplotlib object containing the axes\n            of the plot to annotate.\n        spacing (int): The distance between the labels and the bars.\n    \"\"\"\n\n    # For each bar: Place a label\n    for rect in ax.patches:\n        # Get X and Y placement of label from rect.\n        y_value = rect.get_height()\n        x_value = rect.get_x() + rect.get_width() \/ 2\n\n        # Number of points between bar and label. Change to your liking.\n        space = spacing\n        # Vertical alignment for positive values\n        va = 'bottom'\n\n        # If value of bar is negative: Place label below bar\n        if y_value < 0:\n            # Invert space to place label below\n            space *= -1\n            # Vertically align label at top\n            va = 'top'\n\n        # Use Y value as label and format number with one decimal place\n        label = \"{:.1f}\".format(y_value)\n\n        # Create annotation\n        ax.annotate(\n            label,                      # Use `label` as label\n            (x_value, y_value),         # Place label at end of the bar\n            xytext=(0, space),          # Vertically shift label by `space`\n            textcoords=\"offset points\", # Interpret `xytext` as offset in points\n            ha='center',                # Horizontally center label\n            va=va)                      # Vertically align label differently for\n                                        # positive and negative values.","c8ac3b99":"df = pd.read_csv(\"..\/input\/ecommerce-data\/data.csv\",engine='python',parse_dates=['InvoiceDate'])\ndf_original = df.copy()\ndf.head()","14f5a3ee":"df.describe()","b637fe79":"sns.pairplot(df)","30cc62c3":"fig,ax = plt.subplots(nrows=2,figsize=(20,7))\nsns.boxplot(df[(df['InvoiceNo'].str[0]=='c')|(df['InvoiceNo'].str[0]=='C')]['Quantity'],ax=ax[0])\nsns.boxplot(df[(df['InvoiceNo'].str[0]=='c')|(df['InvoiceNo'].str[0]=='C')]['UnitPrice'],ax=ax[1])\nax[0].title.set_text(\"Cancelled Orders 'C' quantity and price distribution\")","68f2fc8a":"neg_qty = df[df[\"Quantity\"]<0]\nneg_qty_without_C = neg_qty[neg_qty[\"InvoiceNo\"].str[0]!=\"C\"]\nprint(\"Negative Qty without 'C' in InvoiceNo \\n Unit Prices: {} \\t CustomerIDs: {}\".format(neg_qty_without_C[\"UnitPrice\"].unique(),neg_qty_without_C[\"CustomerID\"].unique()))","b94f50b7":"def check_hypothesis_cancelled_order(df):\n    failed = 0\n    passed = 0\n    neg_qty = df[df[\"Quantity\"]<0]\n    pos_qty = df[~df[\"Quantity\"]<0]\n    for ind in neg_qty.index:\n        if(neg_qty['CustomerID'][ind]):\n            p = pos_qty[\n                (pos_qty['CustomerID'] == neg_qty['CustomerID'][ind])&\n                (pos_qty['Quantity'] <= abs(neg_qty['Quantity'][ind]))&\n                ((pos_qty['InvoiceDate'] - neg_qty['InvoiceDate'][ind]).dt.total_seconds()>=0)\n            ]\n            if(len(p)==0):\n                failed+=1\n            else:\n                passed+=1\n    if(failed>passed):\n        print(\"Hypothesis Rejected!\")\n        print(\"Failed Counts:\"+str(failed)+\" Passed Counts:\"+str(passed))\n        print(\"Approximately \"+str(int(failed\/(failed + passed)*100)) + \"% rows didn't satisfy the condition\")\n    else:\n        print(\"Hypothesis Accepted\")\n        print(\"Failed Counts:\"+str(failed)+\" Passed Counts:\"+str(passed))\n        print(\"Approximately \"+str(int(passed\/(failed + passed)*100)) + \"% rows satisfy the condition\")\n            ","6d140ab3":"check_hypothesis_cancelled_order(df)","72018a1f":"neg_price = df[df[\"UnitPrice\"]<0]\nneg_price","cd4f1901":"df.drop_duplicates(inplace=True)","08ebcc93":"fig,ax = plt.subplots(figsize=(14,2))\n((df.isnull().sum() \/ len(df))*100).plot.bar(ax=ax)\nadd_value_labels(ax)\nax.set_title('% of Null Values')\nplt.show()","065791a2":"x = pd.DataFrame(df.groupby(\"StockCode\")[\"Description\"].value_counts())\ny = x.droplevel(level=1).index\ny = y[y.duplicated()]\ntest = df[[\"StockCode\",\"Description\"]]\ntest = test.drop_duplicates()\ntest1 = test[test[\"StockCode\"].isin(y)]\ntest2 = pd.DataFrame(test1.groupby(\"StockCode\")[\"Description\"].value_counts())\ntest2.head(10)","fc6d6995":"fig, ax = plt.subplots(figsize=(15,4))\ngrouped = df.groupby(\"StockCode\")['Description'].unique()\ngrouped_counts = grouped.apply(lambda x: len(x)).sort_values(ascending=False)\ngrouped_counts.head(50).plot.bar(ax=ax)","5206cbf9":"df[df[\"StockCode\"]==\"20713\"][\"Description\"].unique()","e4731870":"# LOGIC: Description with max upper case letters = Product Name\n\ndef get_product_name(x):\n    max_upper_count = 0\n    product_name = ''\n    for i in x:\n        if(i==i):  #To Check for NaN\n            count = 0\n            for letter in i:\n                if(letter.isupper()):\n                    count = count+1\n            if count>max_upper_count:\n                max_upper_count = count\n                product_name = i\n    return product_name\n","0a03f4de":"grouped = df.groupby(\"StockCode\")['Description'].unique()\nlookup = grouped.apply(get_product_name)\n# lookup.to_excel('lookup_product_stockCode.xlsx')","134309b9":"df = df.join(other=lookup, on='StockCode', how='left', rsuffix='ProductName')\ndf = df.rename(columns={'DescriptionProductName':'ProductName'})","3f147a86":"# GETTING SIMILARITY BETWEEN THE Description AND ProductName\n# !pip install jellyfish\n# import jellyfish\nfrom difflib import SequenceMatcher\n\ndes = df['Description']\nprod = df['ProductName']\ndist = []\nfor d,p in zip(des, prod):\n    try:\n        dist.append(SequenceMatcher(None,d,p).ratio())\n#         dist.append(float(jellyfish.damerau_levenshtein_distance(d,p)))\n    except:\n        dist.append(0)\n        ","d199a653":"df['dist'] = dist\ndf[(df['dist']<0.3)&(df['dist']!=0)][['StockCode','Description','ProductName','dist']]","9ece4c59":"df[\"TotalPrice\"] = df[\"UnitPrice\"] * df[\"Quantity\"]","b8476dbe":"fig, ax = plt.subplots(figsize=(7,10))\nneg_qty = df[df[\"Quantity\"]<0]\nneg_qty[\"TotalPrice\"] = abs(neg_qty[\"TotalPrice\"])\nx = neg_qty[[\"ProductName\",\"TotalPrice\"]]\nx.groupby(\"ProductName\")[\"TotalPrice\"].sum().sort_values(ascending=True).tail(30).plot.barh(ax=ax)","a9030c43":"cancelled_df = df[df['InvoiceNo'].str[0]=='C']\ndf = df[~(df['InvoiceNo'].str[0]=='C')]\ncancelled_df = cancelled_df.reset_index(drop=True)","22bf767f":"fig, ax = plt.subplots(nrows=4, ncols=1,figsize=(15,20))\nrev = df[df['TotalPrice']>=0]\nrev['TransactionsCount'] = 1\nrev = rev.groupby(rev['InvoiceDate'].dt.date).agg({'TotalPrice':'sum',\n                                                  'Quantity': 'sum',\n                                                  'CustomerID': 'count',\n                                                  'TransactionsCount':'sum'})\nrev['10 Days Moving Average Revenue'] = rev['TotalPrice'].rolling(10).mean()\nrev['10 Days Moving Average Quantity'] = rev['Quantity'].rolling(10).mean()\nrev['10 Days Moving Transactions Count'] = rev['TransactionsCount'].rolling(10).mean()\ncust = df.groupby('CustomerID').first().reset_index()[['CustomerID','InvoiceDate']]\ncust = cust.groupby(cust.InvoiceDate.dt.date).agg({'CustomerID':'count'})\ncust['10 Days Moving Average Quantity'] = cust['CustomerID'].rolling(10).mean()\n\nsns.set_style(\"whitegrid\")\nsns.lineplot(data=rev[['TotalPrice','10 Days Moving Average Revenue']], palette='magma_r', linewidth=1.5, ax=ax[0],legend=False)\nax[0].legend(title='Revenue Trends', loc='upper left', labels=['Revenue', '10 Days Moving Average Revenue'])\nax[0].title.set_text('Revenue Trends')\nax[0].set_xlabel('')\n\nsns.lineplot(data=rev[['TotalPrice','10 Days Moving Average Quantity']], palette='ocean', linewidth=1.5, ax=ax[1])\nax[1].legend(title='Quantity Trends', loc='upper left', labels=['Quantity Sold', '10 Days Moving Average Quantity'])\nax[1].title.set_text('Quantity Sold Trends')\nax[1].set_xlabel('')\n\nsns.lineplot(data=cust, palette='cividis', linewidth=1.5, ax=ax[2])\nax[2].legend(title='New Customers Trends', loc='upper right', labels=['New Customers', '10 Days Moving Average New Customers'])\nax[2].title.set_text('New Customers Trends')\nax[2].set_xlabel('')\n\nsns.lineplot(data=rev[['TransactionsCount','10 Days Moving Transactions Count']], palette='twilight_shifted', linewidth=1.5, ax=ax[3])\nax[3].legend(title='Transactions Count Trend', loc='upper right', labels=['Transactions Count', '10 Days Moving Average Transactions Count'])\nax[3].title.set_text('Transactions Count Trends')\nax[3].set_xlabel('')\n\nplt.show()","39350765":"fig, ax = plt.subplots(nrows=4, ncols=1,figsize=(15,20))\nrev = df[(df['TotalPrice']>0)&(df['InvoiceDate'].dt.year==2011)]\nrev['Transactions Count'] = 1\nrev = rev.groupby(rev['InvoiceDate'].dt.month).agg({'TotalPrice':'sum',\n                                                  'Quantity': 'sum',\n                                                  'CustomerID': 'count',\n                                                'Transactions Count':'sum'})\nrev = rev.reset_index()\nrev['Month'] = rev['InvoiceDate'].apply(lambda x: calendar.month_abbr[x])\nrev = rev.rename({'TotalPrice':'Revenue'},axis=1)\ncust = df.groupby('CustomerID').first().reset_index()[['CustomerID','InvoiceDate']]\ncust = cust.groupby(cust.InvoiceDate.dt.month).agg({'CustomerID':'count'})\ncust = cust.reset_index()\ncust['Month'] = cust['InvoiceDate'].apply(lambda x: calendar.month_abbr[x])\n\n\nsns.set_style(\"whitegrid\")\nsns.barplot(data=rev, x=rev.Month, y='Revenue', palette='magma_r', ax=ax[0])\nax[0].title.set_text('Revenue by Months')\nadd_value_labels(ax[0])\n\nsns.barplot(data=rev, x=rev.Month, y='Quantity',  palette='ocean', ax=ax[1])\nax[1].title.set_text('Quantity Sold by Months')\nadd_value_labels(ax[1])\n\nsns.barplot(data=cust, x=cust.Month, y='CustomerID',  palette='cividis', ax=ax[2])\nax[2].title.set_text('New Customers by Months')\nfig.suptitle('Growth Month wise',fontsize=16)\nadd_value_labels(ax[2])\n\nsns.barplot(data=rev, x=rev.Month, y='Transactions Count',  palette='twilight_shifted', ax=ax[3])\nax[3].title.set_text('Transactions Count by Months')\nfig.suptitle('Growth Month wise',fontsize=16)\nadd_value_labels(ax[3])\n\nplt.show()","91671e8b":"sales_comp = df[(df['InvoiceDate'].dt.month==12)&(df['TotalPrice']>=0)][['InvoiceDate','TotalPrice','Quantity']]\nsales_comp['Transactions Count'] = 1\nsales_comp = sales_comp.groupby(sales_comp['InvoiceDate'].dt.year)[['TotalPrice','Quantity','Transactions Count']].sum()\nfig, ax = plt.subplots(nrows=1, ncols=3,figsize=(20,5))\n\nsns.set_style(\"whitegrid\")\nsns.barplot(data=sales_comp, x=sales_comp.index, y='TotalPrice', palette='magma_r', ax=ax[0])\nax[0].title.set_text('Revenue Comparision')\nax[0].set_ylabel('Revenue')\nax[0].set_xlabel('December of Year')\nadd_value_labels(ax[0])\n\nsns.barplot(data=sales_comp, x=sales_comp.index, y='Quantity',  palette='ocean', ax=ax[1])\nax[1].title.set_text('Quantity Sold Comparision')\nadd_value_labels(ax[1])\nax[1].set_xlabel('December of Year')\n\nsns.barplot(data=sales_comp, x=sales_comp.index, y='Transactions Count',  palette='twilight_shifted', ax=ax[2])\nax[2].title.set_text('Transactions Count Comparision')\nadd_value_labels(ax[2])\nax[2].set_xlabel('December of Year')\n\nfig.suptitle('Comparision for the month of December in 2020 and 2021',fontsize=16)\n\nplt.show()","003a84df":"print(\"Sales Revenue Difference: {:2.2f}% decline in revenue from 2010 \\nSales Quantity Difference: {:2.2f}% decline in quantity from 2010\".format(\n((sales_comp['TotalPrice'][2010] - sales_comp['TotalPrice'][2011]) \/ sales_comp['TotalPrice'][2010])*100,\n    ((sales_comp['Quantity'][2010] - sales_comp['Quantity'][2011]) \/ sales_comp['Quantity'][2010])*100\n))\n    ","93de6bc9":"fig, ax = plt.subplots(nrows=1, ncols=1,figsize=(15,5))\nsns.set_style(\"whitegrid\")\n\nweek = df[df['TotalPrice']>=0][['InvoiceDate','TotalPrice','Quantity']]\nweek = week.groupby(week['InvoiceDate'].dt.weekday)[['TotalPrice','Quantity']].sum()\nweek = week.reset_index()\nweek['Week'] = week['InvoiceDate'].apply(lambda x: calendar.day_name[x])\n\nsns.lineplot(data = week, x=week.Week, y='Quantity', marker='o', sort = False, ax=ax)\nax2 = ax.twinx()\nsns.barplot(data = week, x=week.Week, y='TotalPrice', alpha=0.5, ax=ax2)\nfig.suptitle('Revenue and Quantity by Sale Week Day Wise',fontsize=16)\nadd_value_labels(ax2)\n\nplt.show()","273fb9ae":"fig, ax = plt.subplots(nrows=2, ncols=1,figsize=(15,7))\nsns.set_style(\"whitegrid\")\n\nday = df[df['TotalPrice']>=0][['InvoiceDate','TotalPrice','Quantity']]\nday = day.groupby(day['InvoiceDate'].dt.hour)[['TotalPrice','Quantity']].sum()\n\nsns.barplot(data = day, x=day.index, y='TotalPrice', alpha=1, ax=ax[0])\nsns.lineplot(data = day, x=day.index, y='Quantity', marker='o', sort = False, ax=ax[1])\nfig.suptitle('Revenue and Quantity by Sale Hourwise',fontsize=16)\nadd_value_labels(ax[0])\nplt.show()","5860bdd9":"fig, ax = plt.subplots(nrows=2, ncols=1,figsize=(15,7))\nsns.set_style(\"whitegrid\")\n\ndate = df[df['TotalPrice']>=0][['InvoiceDate','TotalPrice','Quantity']]\ndate = date.groupby(date['InvoiceDate'].dt.day)[['TotalPrice','Quantity']].sum()\n\nsns.barplot(data = date, x=date.index, y='TotalPrice', alpha=1, ax=ax[0])\nsns.lineplot(data = date, x=date.index, y='Quantity', marker='o', sort = False, ax=ax[1])\nfig.suptitle('Revenue and Quantity by Sale Daywise',fontsize=16)\n\nplt.show()","51675e31":"fig, ax = plt.subplots(nrows=2, ncols=1,figsize=(15,7))\nsns.set_style(\"whitegrid\")\n\nq = df[(df['TotalPrice']>=0)&(df['InvoiceDate'].dt.year==2011)][['InvoiceDate','TotalPrice','Quantity']]\nq = q.groupby(q['InvoiceDate'].dt.quarter)[['TotalPrice','Quantity']].sum()\n\nsns.barplot(data = q, x=q.index, y='TotalPrice', alpha=0.7, ax=ax[0])\nsns.lineplot(data = q, x=q.index, y='Quantity', marker='o', sort = False, ax=ax[1])\nfig.suptitle('Revenue and Quantity by Sale Quarterly for 2011',fontsize=16)\nadd_value_labels(ax[0])\nax[1].set_xticklabels(['',1,'',2,'',3,'',4])\nplt.show()","5a7aef1d":"reg = df[df['TotalPrice']>=0].groupby('Country').agg({'TotalPrice':'sum',\n                                                  'Quantity': 'sum',\n                                                  'CustomerID': 'count'})","8cd6fc53":"fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(10,30))\ng1 = sns.barplot(x=reg['TotalPrice'], y=reg.index, alpha=1, ax=ax[0],palette='Reds', orient='h')\ng2 = sns.barplot(x=reg['Quantity'], y=reg.index, alpha=1, ax=ax[1], palette='Blues',orient='h')\ng3 = sns.barplot(x=reg['CustomerID'], y=reg.index, alpha=1, ax=ax[2], palette='Greens', orient='h')\nax[2].title.set_text('Customers Count by Country')\nax[2].set_xlabel(\"Customers (Log Scale)\")\nax[1].title.set_text('Quantity Sold by Country')\nax[1].set_xlabel(\"Quantity (Log Scale)\")\nax[0].title.set_text('Revenue by Country')\nax[0].set_xlabel(\"Revenue (Log Scale)\")\ng1.set_xscale(\"log\")\ng2.set_xscale(\"log\")\ng3.set_xscale(\"log\")\nplt.show()\n","ec475bc2":"reg = reg[reg.index!='United Kingdom']\nfig, ax = plt.subplots(nrows=3, ncols=1, figsize=(20,20))\n# Change color\nsquarify.plot(sizes=reg['TotalPrice'], label=[str(x)+'\\n'+str(y)+'K' for x,y in zip(reg.index,(reg['TotalPrice']\/1000).round(2))], alpha=.6, ax=ax[0], color=mcolors.CSS4_COLORS )\nax[0].title.set_text('Revenue by Country (Excluding UK)')\nsquarify.plot(sizes=reg['Quantity'], label=[str(x)+'\\n'+str(y)+'K' for x,y in zip(reg.index,(reg['Quantity']\/1000).round(2))], alpha=.6, ax=ax[1], color=mcolors.CSS4_COLORS )\nax[1].title.set_text('Quantity Sold by Country (Excluding UK)')\nr1 = reg[reg['CustomerID']!=0]\nsquarify.plot(sizes=r1['CustomerID'], label=[str(x)+'\\n'+str(y)+'K' for x,y in zip(r1.index,(r1['CustomerID']\/1000).round(2))], alpha=.6, ax=ax[2], color=mcolors.CSS4_COLORS )\nax[2].title.set_text('Customers Count by Country (Excluding UK)')\nax[0].axis('off')\nax[1].axis('off')\nax[2].axis('off')\nplt.show()","888cbdb3":"## Copied this beautiful piece of code from fabiendaniel's notebook\n## https:\/\/www.kaggle.com\/fabiendaniel\/customer-segmentation\n\n\nimport plotly.graph_objs as go\nimport warnings\nfrom plotly.offline import init_notebook_mode,iplot\ninit_notebook_mode(connected=True)\nwarnings.filterwarnings(\"ignore\")\n\ntemp = df[['CustomerID', 'InvoiceNo', 'Country']].groupby(['CustomerID', 'InvoiceNo', 'Country']).count()\ntemp = temp.reset_index(drop = False)\ncountries = temp['Country'].value_counts()\n\ndata = dict(type='choropleth',\nlocations = countries.index,\nlocationmode = 'country names', z = countries,\ntext = countries.index, colorbar = {'title':'Order no.'},\ncolorscale=[[0, 'rgb(224,255,255)'],\n            [0.01, 'rgb(166,206,227)'], [0.02, 'rgb(31,120,180)'],\n            [0.03, 'rgb(178,223,138)'], [0.05, 'rgb(51,160,44)'],\n            [0.10, 'rgb(251,154,153)'], [0.20, 'rgb(255,255,0)'],\n            [1, 'rgb(227,26,28)']],    \nreversescale = False)\n#_______________________\nlayout = dict(title='Number of orders per country',\ngeo = dict(showframe = True, projection={'type':'mercator'}))\n#______________\nchoromap = go.Figure(data = [data], layout = layout)\niplot(choromap, validate=False)","a705441c":"lastdate = datetime(2012,1,1)\ncleaned_dataset = df[df['TotalPrice']>=0]\nrecent = (lastdate - cleaned_dataset.groupby(\"CustomerID\")[\"InvoiceDate\"].last()).dt.days\nfrequent = cleaned_dataset.groupby(\"CustomerID\")[\"InvoiceDate\"].count()\nmonetary = cleaned_dataset.groupby(\"CustomerID\")[\"TotalPrice\"].sum()","a94f6b56":"recent_quantile = recent.quantile(q=[0.25,0.5,0.75])\nrecent_quantile","3c83eaa0":"frequent_quantile = frequent.quantile(q=[0.25,0.5,0.75])\nfrequent_quantile","0fe70338":"monetary_quantile = monetary.quantile(q=[0.25,0.5,0.75])\nmonetary_quantile","1e483e29":"rfm = pd.DataFrame(data=[recent,frequent,monetary])\nrfm = rfm.transpose()\nrfm.columns = [\"recent\",\"frequent\",\"monetary\"]\nrfm","3ef1d6a9":"def get_kmeans_wcss(data, n_limit=15):\n    wcss = [] #Within cluster sum of squares (WCSS)\n    for i in range(1,n_limit):\n        km = KMeans(init='k-means++', n_clusters=i, n_init=10)\n        km.fit(data)\n        wcss.append(km.inertia_)\n    plt.title(\"Elbow Method\")\n    plt.plot(range(1, n_limit), wcss)\n    plt.xlabel(\"Number of clusters\")\n    plt.ylabel(\"WCSS\")\n    return wcss","06943ab8":"_ = get_kmeans_wcss(rfm, n_limit=15)","f67247f0":"kmeans = KMeans(n_clusters=3, init = \"k-means++\", random_state=42)\nclustered_cust = kmeans.fit_predict(rfm)","b2317bb5":"fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(10,20))\nsns.boxplot(clustered_cust,recent, palette=\"cubehelix\",ax=ax[0])\nax[0].set(xlabel=\"Clusters\", ylabel = \"Recency in Number of Days\")\nax[0].title.set_text('Clusters on Recency')\nsns.boxplot(clustered_cust,frequent, palette=\"cubehelix\",ax=ax[1])\nax[1].set(xlabel=\"Clusters\", ylabel = \"Frequency in Number of Days\")\nax[1].title.set_text('Clusters on Frequency')\nsns.boxplot(clustered_cust,monetary, palette=\"cubehelix\",ax=ax[2])\nax[2].set(xlabel=\"Clusters\", ylabel = \"Spending Amount\")\nax[2].title.set_text('Clusters on Monetary')","b6f2e063":"rfm['Clusters'] = clustered_cust\nrfm.Clusters.value_counts()","4cba12eb":"comment_words = '' \nstopwords = set(STOPWORDS) \n\nfor val in df.ProductName: \n    \n    val = str(val) \n    tokens = val.split() \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n    \n    comment_words += \" \".join(tokens)+\" \"\n\nwordcloud = WordCloud(width = 1200, height = 600, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n\n# plot the WordCloud image\nplt.figure(figsize = (12, 6), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n\nplt.show() \n","452fab43":"df['HolidaySeason'] = 0\ndf.loc[df['InvoiceDate'].dt.month.isin([9,10,11]), 'HolidaySeason'] = 1","b7809d27":"d = df[df['InvoiceDate'].dt.year==2011]\nd['Transactions Count'] = 1\nlabels0 = ['Holiday Season Revenue', 'Non-Holiday Revenue']\nsizes0 = [d[d['HolidaySeason']==1]['TotalPrice'].sum(),\n         d[d['HolidaySeason']==0]['TotalPrice'].sum()\n        ]\n\nlabels1 = ['Holiday Season Quantity', 'Non-Holiday Quantity']\nsizes1 = [d[d['HolidaySeason']==1]['Quantity'].sum(),\n         d[d['HolidaySeason']==0]['Quantity'].sum()\n        ]\n\nlabels2 = ['Holiday Season Transactions Count', 'Non-Holiday Transactions Count']\nsizes2 = [d[d['HolidaySeason']==1]['Transactions Count'].sum(),\n         d[d['HolidaySeason']==0]['Transactions Count'].sum()\n        ]\n\nfig1, ax = plt.subplots(ncols=3,figsize=(18,5))\nax[0].pie(sizes0, labels=labels0, autopct='%1.1f%%', shadow=True)\nax[0].axis('equal')\nax[1].pie(sizes1, labels=labels1, autopct='%1.1f%%', shadow=True)\nax[1].axis('equal')\nax[2].pie(sizes2, labels=labels2, autopct='%1.1f%%', shadow=True)\nax[2].axis('equal')\nplt.show()","145236b3":"df['Transactions Count'] = 1\nl1 = df[df['HolidaySeason']==1].groupby('StockCode')['Transactions Count'].sum()\nl2 = df[df['HolidaySeason']==0].groupby('StockCode')['Transactions Count'].sum()\nx = pd.DataFrame(data=[l1,l2]).T\nx.columns = ['Season','Off-Season']\nx = x.fillna(0)\nx = x.reset_index()","d7dd70ef":"_ = get_kmeans_wcss(x[['Season','Off-Season']], n_limit=20)","7f2fba5c":"kmeans = KMeans(n_clusters=10, init = \"k-means++\", random_state=100)\nclustered_cust = kmeans.fit_predict(x[['Season','Off-Season']])\nx['cluster'] = clustered_cust","49f9c454":"plt.figure(figsize=(15,10))\ng1 = sns.scatterplot(x['Season'],x['Off-Season'],hue=x['cluster'],palette=\"deep\")\ng1.set_xscale(\"log\")\nplt.xlabel('Season Counts')\nplt.title('StockCodes: Seasonal and Off-seasonal counts by KNN clusters (Seasonal Count axis is in log scale)')\nplt.ylabel('Off-Season Counts')","da124170":"df[df['StockCode'].isin(x[x['cluster']==8]['index'])]['ProductName'].unique()","eddd4f74":"QUANTILE = [0.90]\nMAX_QUANTILE = [0.95]\nMIN_QUANTILE = [0.15]\nprint(x['Season'].quantile(QUANTILE))\nprint(x['Off-Season'].quantile(QUANTILE))\nx.loc[:,'Q-Region'] = 0\nx.loc[(x['Season']>x['Season'].quantile(QUANTILE).values[0])&(x['Off-Season']>x['Off-Season'].quantile(QUANTILE).values[0]),'Q-Region'] = 1\nx.loc[(x['Season']<=x['Season'].quantile(QUANTILE).values[0])&(x['Off-Season']>x['Off-Season'].quantile(QUANTILE).values[0]),'Q-Region'] = 2\nx.loc[(x['Season']<=x['Season'].quantile(QUANTILE).values[0])&(x['Off-Season']<=x['Off-Season'].quantile(QUANTILE).values[0]),'Q-Region'] = 3\nx.loc[(x['Season']>x['Season'].quantile(QUANTILE).values[0])&(x['Off-Season']<=x['Off-Season'].quantile(QUANTILE).values[0]),'Q-Region'] = 4","1188f4b2":"plt.figure(figsize=(15,10))\ng1 = sns.scatterplot(x['Season'],x['Off-Season'],hue=x['Q-Region'],palette=\"deep\")\n# g1.set_xscale(\"log\")\nplt.title('StockCodes: Seasonal and Off-seasonal counts by Quantile Regions (Seasonal Count axis is in log scale)')\nplt.xlabel('Season Counts')\nplt.ylabel('Off-Season Counts')","78a9982e":"df = df.merge(x,left_on=['StockCode'],right_on=['index']).drop(['index','Season','Off-Season'],axis=1).rename({'cluster':'ProductCluster'},axis=1)","1c49693d":"fig, ax = plt.subplots(nrows=2, ncols=1,figsize=(15,14))\nddf = df[(df['Quantity']>0)&(df[\"InvoiceDate\"].dt.year==2011)]\nd = df.groupby([ddf[\"InvoiceDate\"].dt.week,df['Q-Region']]).agg({'TotalPrice':'sum',\n                                                  'Quantity': 'sum',\n                                                  'CustomerID': 'count'}).reset_index()\nsns.lineplot(data=d,y='TotalPrice',x='InvoiceDate',hue='Q-Region',palette=\"deep\",ax=ax[0])\nsns.lineplot(data=d,y='Quantity',x='InvoiceDate',hue='Q-Region',palette=\"deep\",ax=ax[1])\nplt.show()","bfe87b70":"Q1 = df[df['Q-Region']==1]\nQ2 = df[df['Q-Region']==2]\nQ3 = df[df['Q-Region']==3]\nQ4 = df[df['Q-Region']==4]","8e24ff2a":"Q1.groupby(['StockCode','ProductName'])[['UnitPrice','TotalPrice','Quantity']].sum().sort_values(by='UnitPrice',ascending=False).head(10)","1b602abd":"Q2.groupby(['StockCode','ProductName'])[['UnitPrice','TotalPrice','Quantity']].sum().sort_values(by='UnitPrice',ascending=False).head(10)","927c8ae5":"Q3.groupby(['StockCode','ProductName'])[['UnitPrice','TotalPrice','Quantity']].sum().sort_values(by='UnitPrice',ascending=False).head(10)","02568ccd":"Q4.groupby(['StockCode','ProductName'])[['UnitPrice','TotalPrice','Quantity']].sum().sort_values(by='UnitPrice',ascending=False).head(10)","321cabc5":"fig, ax = plt.subplots(nrows=10, ncols=1,figsize=(15,70))\nddf = df[(df['Quantity']>0)&(df[\"InvoiceDate\"].dt.year==2011)]\nd = df.groupby([ddf[\"InvoiceDate\"].dt.week,df['ProductCluster']]).agg({'TotalPrice':'sum',\n                                                  'Quantity': 'sum',\n                                                  'CustomerID': 'count'}).reset_index()\nd = d.rename({'TotalPrice':'Revenue','InvoiceDate':'Weeks'},axis=1)\nsns.lineplot(data=d[d['ProductCluster']==0],y='Revenue',x='Weeks',hue='ProductCluster',palette=\"Greens\",ax=ax[0])\nsns.lineplot(data=d[d['ProductCluster']==1],y='Revenue',x='Weeks',hue='ProductCluster',palette=\"Reds\",ax=ax[1])\nsns.lineplot(data=d[d['ProductCluster']==2],y='Revenue',x='Weeks',hue='ProductCluster',palette=\"deep\",ax=ax[2])\nsns.lineplot(data=d[d['ProductCluster']==3],y='Revenue',x='Weeks',hue='ProductCluster',palette=\"tab10\",ax=ax[3])\nsns.lineplot(data=d[d['ProductCluster']==4],y='Revenue',x='Weeks',hue='ProductCluster',palette=\"tab10_r\",ax=ax[4])\nsns.lineplot(data=d[d['ProductCluster']==5],y='Revenue',x='Weeks',hue='ProductCluster',palette=\"prism\",ax=ax[5])\nsns.lineplot(data=d[d['ProductCluster']==6],y='Revenue',x='Weeks',hue='ProductCluster',palette=\"vlag\",ax=ax[6])\nsns.lineplot(data=d[d['ProductCluster']==7],y='Revenue',x='Weeks',hue='ProductCluster',palette=\"RdPu_r\",ax=ax[7])\nsns.lineplot(data=d[d['ProductCluster']==8],y='Revenue',x='Weeks',hue='ProductCluster',palette=\"RdPu_r\",ax=ax[8])\nsns.lineplot(data=d[d['ProductCluster']==9],y='Revenue',x='Weeks',hue='ProductCluster',palette=\"CMRmap\",ax=ax[9])\n# sns.lineplot(data=d,y='Quantity',x='InvoiceDate',hue='ProductCluster',palette=\"deep\",ax=ax[1])\nplt.show()","053267f2":"basket_Germany = df[df['Country']==\"Germany\"].groupby(['InvoiceNo', 'ProductName'])['Quantity'].sum().unstack().reset_index().fillna(0).set_index('InvoiceNo')\nbasket_EIRE = df[df['Country']==\"EIRE\"].groupby(['InvoiceNo', 'ProductName'])['Quantity'].sum().unstack().reset_index().fillna(0).set_index('InvoiceNo')\nbasket_UK = df[df['Country']==\"UK\"].groupby(['InvoiceNo', 'ProductName'])['Quantity'].sum().unstack().reset_index().fillna(0).set_index('InvoiceNo')\nbasket_France = df[df['Country']==\"France\"].groupby(['InvoiceNo', 'ProductName'])['Quantity'].sum().unstack().reset_index().fillna(0).set_index('InvoiceNo')","eeba1506":"def encode_units(x):\n    if x <= 0:\n        return 0\n    if x >= 1:\n        return 1\n\nbasket_Germany.drop('POSTAGE',axis=1,inplace=True)\nbasket_France.drop('POSTAGE',axis=1,inplace=True)\n\nbasket_Germany = basket_Germany.applymap(encode_units)\nbasket_EIRE = basket_EIRE.applymap(encode_units)\nbasket_UK = basket_UK.applymap(encode_units)\nbasket_France = basket_France.applymap(encode_units)","abdf7f73":"frequent_itemsets = apriori(basket_France, min_support=0.07, use_colnames=True)\nrules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\nrules.sort_values(['lift','support'],ascending=False).reset_index(drop=True)","85e8b93f":"frequent_itemsets = apriori(basket_Germany, min_support=0.07, use_colnames=True)\nrules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\nrules.sort_values(['lift','support'],ascending=False).reset_index(drop=True)","a2b96bc8":"## Feature Generation <a class=\"anchor\" id=\"3\"><\/a>\n### Creating Product StockCode Lookup <a class=\"anchor\" id=\"3.1\"><\/a>\n\nWe can see Description not only tells what the product is but in some cases it is either corrupted by vague values or describes the condition of product (for eg. \"wet\/rusty\"). We can create a product lookup which can tell us a good idea what product a specific ```StockCode``` indicates to.","2787a515":"**Inferences**\n\n* It seems that company\u2019s performance is improving in terms of revenue and sales, but as we are provided with just one year of data, it is hard to claim this for sure\n\n\n* There might be a seasonal rise during the end of the year.\n\n\n* In terms of new customer acquisition,  we can see a clear downward slope. Though, it can be explained by the fact that, with growing customer base, it is difficult to get more new customers. \n\n\n* We can compare December sales for 2010 and 2011 to get an insight\n","4f5b6b62":"**Segregation:** <br>\n* Majority of customers,  ~96% belong to cluster 0 (Which is not good)\n* Only ~1.14% of customers belong to cluster 1\n* ~2.74% of customers belong to cluster 2\n","20ab041d":"Let's see what products are there in Cluster 8 (It has great sales during both season and off-season)","3978070e":"## Preprocessing <a class=\"anchor\" id=\"2\"><\/a>\n### Duplicate Deletion <a class=\"anchor\" id=\"2.1\"><\/a>","73c0d9de":"#### Data Description <a class=\"anchor\" id=\"1.2.1\"><\/a>\n\nThis following information is taken from UCI Machine Learning Repository:<br>\n*This is a transnational data set which contains all the transactions occurring between **01\/12\/2010 and 09\/12\/2011** for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.*\n\n\n| Column | Description |\n| --- | :-- |\n| InvoiceNo | Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. **If this code starts with letter 'c', it indicates a cancellation.** |\n| StockCode | Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product. |\n| Description | Product (item) name. Nominal. |\n| Quantity | The quantities of each product (item) per transaction. Numeric. |\n| InvoiceDate | Invice Date and time. Numeric, the day and time when each transaction was generated. |\n| UnitPrice | Numeric, Product price per unit in sterling. |\n| CustomerID | Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer. |\n| Country | Country name. Nominal, the name of the country where each customer resides. |\n\n","d061c697":"* We can see that the customers do tend to buy between 9:00 hrs - 13:00 hrs. Infact, ~51 % of the sales by revenue is done in these hours\n\n* Also, there are no transactions are done before 6:00 hrs and after 20:00 hrs. It can be assumed that the store usually opens for 14 hours between 6:00 hrs and 20:00 hrs.\n","0981b14a":"Table of Contents \n> [Getting Started](#1)\n>> [Importing Libraries](#1.1)<br>\n>> [Reading Data](#1.2)\n>>> [Data Description](#1.2.1)<br>\n\n>> [Analysing -ve Quantity and -ve UnitPrice](#1.3)\n>>> [Negative Quantity](#1.3.1) <br>\n>>> [Negative Price](#1.3.2)\n\n> [Preprocessing](#2)\n>> [Duplicate Deletion](#2.1)<br>\n>> [Analysing null Values](#2.2)\n\n> [Feature Generation](#3)\n>> [Creating Product StockCode Lookup](#3.1)<br>\n>> [Feature Generation: Product Name](#3.2)<br>\n>> [Feature Generation: Total Price](#3.3)<br>\n\n> [Analysis](#4)\n>> [Q. Is company\u2019s performance improving or degrading over time?](#4.1)<br>\n>> [Q. What are some important trends visible in the sales data and insights?](#4.2)<br>\n>> [Regionwise Analysis](#4.2.1)<br>\n>> [Q. How can we measure our performance in terms of customer acquisition and building customer loyalty?](#4.3)\n>>> [RFM Analysis](#4.3.1)\n\n>> [Q. Can we take some initiatives based on the data to increase the sales? ](#4.4)<br>\n>> [Q. Based on data can we avoid out of stock situations?](#4.5)\n>>> [Are there any products which are sold more during christmas season?](#4.5.1)\n\n>> [Q.What kind of Customers are buying from us?](#4.6)","4ac1adc6":"# *That's All Folks!*","3f65aa55":"Elbow method suggests that ideal number of clusters would be 3 (knee point)","f1fa4c25":"Hmm... It is interesting to note that in France, (ALARM CLOCK BAKELIKE RED ) and (ALARM CLOCK BAKELIKE GREEN) are very frequently bought together. Similarly, (ALARM CLOCK BAKELIKE RED ) and (ALARM CLOCK BAKELIKE PINK) are often bought together.\nAlso, in Germany, (ROUND SNACK BOXES SET OF 4 FRUITS ) and (ROUND SNACK BOXES SET OF4 WOODLAND ) are most frequently bought together.\nIt is worth noting that all of these items are **essencially the same, i.e. ALARM CLOCK and SNACK BOXES**. \n\n##### This behavior in purchasing pattern shows us that the majority of transactions done by the Customers are more likely to be Wholesale Retailers than some individuals. ","60d5a3ee":"* RECENCY boxplot<br>\nCluster 0 have low recency, which is not appreciable. <br>\nCluster 1,2 are having high recency, suggesting that they are more prone to marketing<br>\n\n* FREQUENCY boxplot<br>\nCluster 0 has low frequency, which means customers are not doing transactions frequently<br>\nCluster 1,2 are having better frequency, suggesting that they are more satisfied in general<br>\n\n* MONETARY boxplot<br>\nCluster 0 has lowest value suggesting they are having lowest affordability<br>\nCluster 1 are heavy spenders<br>\nCluster 2 comes in middle<br>\n\nIn summary, <br>\n\n| Clusters | Recency | Frequency | Monetary |\n| --- | --- | --- | --- |\n| 0 | Have not visited recently | Least frequent | Least spending |\n| 1 | Most recently visited | Highest frequency | Spending Highest |\n| 2 | Recently visited | Decent frequency | Decent Spending |\n\n<br>\nNow let us see customer counts by clusters","f978befe":"### Reading Data <a class=\"anchor\" id=\"1.2\"><\/a>\n\nLet's have sneak peek to the data","13221663":"We don't need to bother for the negative quantity without C in invoice number as their unit price 0 only. They won't effect the calculations later on. These entries have no CustomerID assoiciated with them.<br><br>\nDo all -ve Quantity have 'C' initiated ```InvoiceNo```? <br>\n'C' is said to be denoting cancelled orders and hence have -ve quantity. But we need to analyse it to be sure.\n\n#### **HYPOTHESIS: Rows with -ve quantities mean that the order was previously ordered and cancelled later on.**\nIf this hypothesis is true, then for majority of negative quantity orders, there must exist an entry which follows following condition (Why majority? Because for some initial months because of data cut conditions won't satisfy):<br>\n1. CustomerID (if exists) must match\n2. Quantity ordered <= Quantity Cancelled\n3. Order date must before Order cancelled date","577a055d":"### Analysing the *null* values <a class=\"anchor\" id=\"2.2\"><\/a>","bc3847a4":"## Getting Started <a class=\"anchor\" id=\"1\"><\/a>\n### Importing Libraries <a class=\"anchor\" id=\"1.1\"><\/a>","a0ff67d6":"The purpose of this data analysis is to find key insights and derive meaning from it. We will try to derive some business impacting insights by trying to answer relevant questions like \n* Is company\u2019s performance improving or degrading over time?\n* What are some important trends visible in the sales data and insights?\n* How can we measure our performance in terms of customer acquisition and building customer loyalty? \n* Can we take some initiatives based on the data to increase the sales? \n* Based on data can we avoid out of stock situations?\n* What kind of customer do typically buy from us?\n\nThis dataset is orignally uploaded at https:\/\/archive.ics.uci.edu\/ml\/datasets\/online+retail# ","4ea70995":"Since, 56% rows satisfy our condition hence, we can surely say that -ve quantity entries are for some previous order cancellation ","6cb9da2a":"* It is interesting to note that Thursdays have the maximum sale by volume and as well as revenue. \n* Also, there are no transactions done on Saturdays. It is safe to assume that the store is closed on Saturdays.\n* Nearly, 42% of total sale is done on Thursdays and Tuesdays \n","18f04933":"We can see certain category of products get a sale spike during holiday seasons and during year end. For example products in cluster 7,8 and 1<br>\nMoreover there is a sudden demand on Product cluster 3 in the beginning of the year\n\n### Q. What kind of Customers are buying from us? <a class=\"anchor\" id=\"4.6\"><\/a>\n### Understanding Customer's Purchase Pattern <a class=\"anchor\" id=\"4.6.1\"><\/a>\nNow, we can also analyse the customer's purchasing pattern which can give us some insights about the customers. We can do this by using **Association Rule Learning** or ARL. These algorithms answer questions like \"People who bought this also bought...\" or \"Customers tend to buy A and B together more often\". We will use one of the famous ARL, Apriori Algorithm\n<br><br>\nBut first, let us segregate transactions in baskets based on countries (since, creating a single basket of all the transactions would be resource intensive. Also, for the same reason, we would be avoiding creation of United Kingdom's basket)","4f2165dc":"### Analysing -ve Quantity and -ve UnitPrice <a class=\"anchor\" id=\"1.3\"><\/a>\n#### Negative Quantity <a class=\"anchor\" id=\"1.3.1\"><\/a>","2263bf11":"We can see there are missing values in ```Description``` and ```CustomerID```. <br>\n**~25%** of values missing in ```CustomerID``` and **~0.3%** values are missing in ```Description```.<br>\nWe cannot do anything for ```CustomerID```, but let's check what can we do for ```Description```.","9058c653":"We can see `AMAZON FEE`, `Manual`, `Bank Charges`, `Postage` are some of the main contributors for negative priced entries","41e69037":"Before heading it is important to understand few terminology,<br>\nFirst, \n### Support:\nSupport of an itemset X is propotion of transaction in the data in which X appears. It shows the **popularity** of X i.e. <br>\n##### $$\\text{Support(X)} = \\frac{\\text{Number of Transactions in which X appears}}{\\text{Total Number of Transactions}} = \\frac{freq(X)}{N}$$\nin our case, for a given rule A->B (in other words if A is bought, B is also bought) \n##### $$\\text{Support(A,B)} = \\frac{\\text{Number of Transactions in which A and B both appears}}{\\text{Total Number of Transactions}} = \\frac{freq(A,B)}{N}$$\n\n### Confidence:\nConfidence for a given rule A->B is defined as frequency of A and B together in a basket divided by frequency of A:\n##### $$\\text{Confidence} = \\frac{freq(A,B)}{freq(A)}$$\nIt shows the **likelihood** of item B being purchased when A is purchased. We must also take its drawback into consideration viz. it takes popularity of A in consideration. If product B is also equally popular as A then there will be a higher probability that a transaction containing A will also contain B thus increasing the confidence\n\n### Lift:\nLift for a given rule A->B is defined as\n##### $$\\text{Lift} = \\frac{Support(A,B)}{Support(A) * Support(B))}$$\nThis signifies the likelihood of the itemset B being purchased when item A is purchased while taking into account the popularity of B. If the value of lift is greater than 1, it means that the itemset B is likely to be bought with itemset A, while a value less than 1 implies that itemset B is unlikely to be bought if the itemset A is bought.\n\n### Conviction:\nConviction for a given rule A->B is defined as\n##### $$\\text{Conviction(A,B)} = \\frac{1-Support(B)}{1-Confidence(A,B)}$$\nSay, the conviction value of 1.32 means that the rule A->B would be incorrect 32% more often if the association between A and B was an accidental chance.\n\n\n### Leverage:\nLeverage for a given rule A->B is defined as\n##### $$\\text{Leverage(A,B)} = P(A and B) - P(A)P(B)$$\n\n\" Leverage measures the difference of A and B appearing together in the data set and what would be expected if A and B where statistically dependent. The rational in a sales setting is to find out how many more units (items A and B together) are sold than expected from the independent sells. Using min. leverage thresholds at the same time incorporates an implicit frequency constraint. E.g., for setting a min. leverage thresholds to 0.01% (corresponds to 10 occurrence in a data set with 100,000 transactions) one first can use an algorithm to find all itemsets with min. support of 0.01% and then filter the found item sets using the leverage constraint. Because of this property leverage also can suffer from the rare item problem. \"\n\nHere is a good read on Apriori Algorithm Basics: <br>\nhttps:\/\/michael.hahsler.net\/research\/recommender\/associationrules.html<br>\nhttps:\/\/www.hackerearth.com\/blog\/developers\/beginners-tutorial-apriori-algorithm-data-mining-r-implementation\/","2166da2b":"### Feature Generation: Product Name <a class=\"anchor\" id=\"3.2\"><\/a>","a559cefb":"#### Negative Pricing <a class=\"anchor\" id=\"1.3.2\"><\/a>","addc1fed":"We can see that there are products which have varied ```Description``` but belong to same ```StockCode``` For example, for ```StockCode```= 20713, there are 8 unique descriptions (9 including null)","d6c8d708":"#### Holiday Analysis\nLet us see how holidays have impacted the sales figure","89d779c6":"* It is evident that there is increase in sales by both volume and revenue in the fourth quarter of the year 2011.\n* There is major sale in the month of November and December.\n* It could be because of Holiday season.\n\n### Regionwise Analysis <a class=\"anchor\" id=\"4.2.1\"><\/a>","7573fd41":"### Q.What are some important trends visible in the sales data and insights? <a class=\"anchor\" id=\"4.2\"><\/a>\n\n* After clustering the data in different forms, many interesting  purchasing patterns emerged. \n* Trends can be better known through the purchasing pattern of the customers.\n* We can prose certain hypothesis to check the purchasing pattern, like\n\n> Do customers tend to buy more on certain weekday? <br>\n> Do customers buy more at the start of the month?<br>\n> Do customers tend to buy more at a specific hour of the day?<br>\n","f78f1c4e":"Let's see Quantile based groupings.<br>\nLOGIC: Based on Seasonal and Off-Seasonal Counts for each ```StockCode```, we will create clusters called as ```Q-Regions``` and we will see how each region's sales have occurred during the whole year 2011\n\n| Quantile Region | Seasonal Sale (X-Axis) | Off-Seasonal Sale (Y-Axis) | Comment |\n| --- | --- | --- | --- |\n| 1 | High Sales | High Sales | Ever-green Products |\n| 2 | Low and Moderate Sales | High Sales | Off-Seasonal Products (Day-to-day products) |\n| 3 | Low and Moderate Sales | Low and Moderate Sales | Rarely and often used products |\n| 4 | Low and Moderate Sales | High Sales | Seasonal Products\/Giftings |\n\n<br>\nNote that because of choice of quantile point, distribution would be highly skewed towards Low and Moderate Sales. Reason of choosing such a high quantile is to segregate the Region-1 products effectively","c9265f78":"## Analysis <a class=\"anchor\" id=\"4\"><\/a>\nNow as we have cleaned our dataset and also generated required features, we are good to go for analysing our actual business problems\n### Q. Is company\u2019s performance improving or degrading over time? <a class=\"anchor\" id=\"4.1\"><\/a>\nCompany\u2019s performance can be measured in many terms. Here, we will be analysing the performance by:\n\n* Growth in Revenue by Sales\n\n* Growth in Sales Quantity\n\n* Growth in Customer Base \n","138f435e":"It is safe to say that almost 37% sale (by revenue,quantity and transactions) occurs in Holiday season (Sep-Dec) \n#### Are there any products which are sold more during christmas season? <a class=\"anchor\" id=\"4.5.1\"><\/a>\nWe have already seen a surge in sale during holiday season. Are there any specific ```StockCode``` contributing to it? Let's figure it out.\nWe will segregate the products in two ways using count of seasonal and off-seasonal sale:<br>\n1. Based on K-nn created groupings\n2. Based on Qunatile Range groupings<br><br>\nLet's see first KNN based groupings.<br>\nLOGIC: Based on Seasonal and Off-Seasonal Counts for each ```StockCode```, we will create clusters called as ```ProductClusters``` and we will see how each cluster's sales have occurred during the whole year 2011","9d65ddb9":"We will try to segregate customers based on RFM analysis. First, we'll create clusters and see characterstics of each cluster\n","e98eaec0":"Let us see few products of each quantiles","43b325c2":"### Feature Generation: Total Price <a class=\"anchor\" id=\"3.3\"><\/a>","c1580a61":"### Q. How can we measure our performance in terms of customer acquisition and building customer loyalty? <a class=\"anchor\" id=\"4.3\"><\/a>\n\n#### RFM Analysis <a class=\"anchor\" id=\"4.3.1\"><\/a>\n\nUsed to segregate the customers based on behaviors\n\nRFM helps in segregating:\n* the more recent the purchase, the more responsive the customer is to promotions\n* the more frequently the customer buys, the more engaged and satisfied they are\n* monetary value differentiates heavy spenders from low-value purchasers\n","441845b8":"#### Removing 'C' Products","346c0184":"# Complete E-Commerce Data Analysis: Business First Approach","c0b820a7":"It should be noted that there are -ve values in both ```UnitPrice``` and ```Quantity```. Is it for cancelled orders?","4e462386":"First, it is important to check whether if each ```StockCode``` has a unique ```Description```","8c84775c":"### Q. Based on data can we avoid out of stock situations? <a class=\"anchor\" id=\"4.5\"><\/a>\n\nWe have huge variety of products (~4000 unique products) and in order to make them managable, it is important for us to segregate the products into categories","18ed304f":"### Q. Can we take some initiatives based on the data to increase the sales? <a class=\"anchor\" id=\"4.4\"><\/a>\n\n* **Inititative 1**: We noticed that not just revenue and sales increased in November and December, but number of new customers also increased during aforementioned period. It gives testifies that, November and December is a peak season. It would be wise to increase marketing during this tenure\n\n* **Initiative 2**: POSTAL CHARGES and CARRIAGE CHARGES. Majority of transactions done in UK, Germany and in France is for Postage charges. Working with postal companies to reduce the postal charges can increase company\u2019s revenue and Customer base\n\n* **Initiative 3**: Coming up with loyalty program for cluster 1 and 2 category customers. It will appreciate customers for buying more products from the company\n\n* **Initiative 4**: ~16 % of all transactions are cancelled. This might be due to poor transaction interface infrastructure. Upgrading technical foreground of the company might increase the customer engagement. \n","f3b7616d":"* Major sales is been done from UK (~85%). \n* Almost 95% of all the transactions occurring are from Europe ","ea0399f6":"* Here, we can see that company\u2019s revenue by sales is decreased from 2010 by **22.4%**, suggesting that company\u2019s performance is degrading.\n* We get a similar picture in sales quantity as well. A net decrease of **13.4%**  in Sales quantity.\n"}}