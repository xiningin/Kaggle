{"cell_type":{"5f9b6c12":"code","38b01fc3":"code","25a0e0c6":"code","41f4f17b":"code","d286e229":"code","bdcc42ed":"code","879d2e1c":"code","58e333ba":"code","93c7084c":"code","a9a18902":"code","71ce205a":"code","8397fa93":"code","234b2d28":"code","a984e38e":"code","6263c7ef":"code","b80c2630":"code","1f90d042":"code","8b27eebf":"code","3f7bcabe":"code","0eb3dd4f":"code","d3fb7e0c":"code","383ba436":"code","33af121b":"code","7824f63c":"code","7d4fee89":"code","67c2c681":"code","6861b0b2":"code","2d04bcf5":"code","855f6f81":"code","fee7b2a1":"code","5fc313d6":"code","c79f7257":"code","76f01096":"code","8ea0509d":"code","0cdcc592":"code","49f5b30f":"code","2bf91b45":"code","d989de66":"code","54088a4c":"code","062f34e4":"code","09d0dce9":"code","f54a9c41":"code","bf8b299d":"code","f899f611":"code","dbef4664":"code","6474f2b7":"code","aa11d83e":"code","f8ddeb0c":"code","85d0585c":"code","6091e468":"code","9db45030":"code","596e8071":"code","0dae74a1":"code","777577ed":"code","ed9216e4":"code","ea2335cf":"code","8119160a":"code","aa3fe42f":"code","9638c1c6":"code","695dd2ea":"code","6671b148":"code","a84318a9":"code","6b3b900f":"code","afb20a9b":"code","f318ec85":"code","ef18896b":"code","7ea1463c":"code","22327896":"code","1b02cbc1":"code","2a4965f3":"code","c5adfaf9":"code","55bac52f":"code","5b199ab2":"code","13f87f89":"code","138e41e3":"code","8649efda":"code","a7040f37":"code","c2f7af60":"code","857e4d73":"code","51e7a36c":"code","9a15ff66":"code","eb64fb08":"code","eab3c916":"code","b0d47035":"code","da0f92f9":"code","80b8d530":"code","6e5eb93b":"code","ee6a1b00":"code","e105327f":"code","ba48c551":"code","23a9fbb3":"markdown","b6f6b665":"markdown","dfac6173":"markdown","08d5d93d":"markdown","b566b090":"markdown","d1ce4b43":"markdown","0ef85f4a":"markdown","a4906e9a":"markdown","e645a619":"markdown"},"source":{"5f9b6c12":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","38b01fc3":"## IMPORTING TOOLS\nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nfrom nltk.tokenize import word_tokenize, sent_tokenize \nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport re\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,adjusted_rand_score\n\nfrom sklearn.naive_bayes import MultinomialNB \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom xgboost import XGBClassifier\n%matplotlib inline","25a0e0c6":"## Reading and exploring on train data\u00b6\ntrain = pd.read_csv(\"..\/input\/Train-1554810061973.csv\")\nprint(train.shape)\ntrain.head()","41f4f17b":"### Reading and exploring on test data\ntest = pd.read_csv(\"..\/input\/Test-1555730055539.csv\")\nprint(test.shape)\ntest.head()","d286e229":"### Reading and exploring on Existing hotels Data\nexist = pd.read_csv(\"..\/input\/ExistingHotels_CustomerVisitsdata-1554810038262.csv\")\nprint(exist.shape)\nexist.head(3)","bdcc42ed":"### Reading and exploring on New_hotels Data\nNew = pd.read_csv(\"..\/input\/NewHotels_CutstomerVisitsdata-1554810098964.csv\")\nprint(New.shape)\nNew.head(2)","879d2e1c":"#### Reading and exploring on User_ratings data\u00b6\nratings = pd.read_csv(\"..\/input\/user_hotel_rating-1555730075105.csv\")\nprint(ratings.shape)\nratings.head(2)","58e333ba":"#### VISUALIZATIONS on Train Data\n# Let's look at the top 10 reviewed Hotels\ntop_reviewed_hotels = train.Hotelid.value_counts()\ntop_reviewed_hotels[:10].plot(kind='barh',figsize=(20,10),title=\"TOP 10 REVIEWED HOTELS on traindata\",legend=True,colormap=\"PiYG\",fontsize=15)\n_=plt.xlabel('HotelID',fontsize=20)\n_=plt.ylabel('Total No. of reviews',fontsize=20)\n","93c7084c":"### Checking the reviews count with respect to date column\ntrain['Date']=pd.to_datetime(train['Date'])\n\ndays = {0:'Mon',1:'Tues',2:'Weds',3:'Thurs',4:'Fri',5:'Sat',6:'Sun'}\n\ntrain[\"month\"]=train[\"Date\"].dt.month\ntrain[\"Year\"]=train[\"Date\"].dt.year\ntrain[\"Day\"]=train[\"Date\"].dt.day\ntrain[\"dayOftheweek\"] = train[\"Date\"].dt.dayofweek\ntrain['dayOftheweek'] = train['dayOftheweek'].apply(lambda x: days[x])\n","a9a18902":"Review_Day_Count = train['Day'].value_counts()\nplt.figure(figsize=(10,4))\nsns.barplot(Review_Day_Count.index, Review_Day_Count.values, alpha=0.8)\nplt.ylabel(\"Number Of Review\")\nplt.xlabel(\"Reviews By Days\")\nplt.title('Total reviews count by Day', loc='Center', fontsize=14)\nplt.show()\n\nReviews_Count_Month = train['month'].value_counts()\nplt.figure(figsize=(10,4))\nsns.barplot(Reviews_Count_Month.index, Reviews_Count_Month.values, alpha=0.8)\nplt.ylabel(\"Number Of Review\")\nplt.xlabel(\"Reviews By Months\")\nplt.title('Total reviews count by month', loc='Center', fontsize=14)\nplt.show()\n\nReviews_Year = train['Year'].value_counts()\nplt.figure(figsize=(10,4))\nsns.barplot(Reviews_Year.index, Reviews_Year.values, alpha=0.8)\nplt.ylabel(\"Number Of Review\")\nplt.xlabel(\"Reviews By Year\")\nplt.title('Total reviews count by Year', loc='Center', fontsize=14)","71ce205a":"Sentiment_count = train['Sentiment'].value_counts()\nplt.figure(figsize=(10,4))\nsns.barplot(Sentiment_count.index, Sentiment_count.values, alpha=0.8)\nplt.ylabel(\"Count\")\nplt.xlabel(\"Feedback Count\")\nplt.title('Feedback counts across the train data', loc='Center', fontsize=14)\nplt.show()","8397fa93":"#Top 10 feed back given hotels\ntop_fb_hotels = train.groupby('Hotelid')['Sentiment'].value_counts().sort_values(ascending=False).head(10)\ntop_fb_hotels.plot(kind=\"barh\",color=\"gold\",title=\"Top 10 feed back given hotels \",legend=True,figsize=(10,10))\n_=plt.xlabel('count')\n_=plt.ylabel('HotelID')\nplt.show()","234b2d28":"#Least 10 feed back given hotels\nleast_fb_hotels = train.groupby('Hotelid')['Sentiment'].value_counts().sort_values(ascending=True).head(10)\nleast_fb_hotels.plot(kind=\"barh\",color=\"grey\",title=\"Least 10 feed back given hotels \",legend=True,figsize=(10,10))\n_=plt.xlabel('count')\n_=plt.ylabel('HotelID')\nplt.show()","a984e38e":"# As we can observe the trend in summer holidays i.e from April to August are more when compared to all the other months in all the years\n###   On existing Hotels Data\n\nexist['AverageOverallRatingOfHotel']=exist['AverageOverallRatingOfHotel'].astype(\"float64\")\n\n#Worst Hotels\nworst_hotels =exist.groupby('Hotelid')['AverageOverallRatingOfHotel'].mean().sort_values(ascending=True).head(5)\nworst_hotels.plot(kind=\"barh\",color=\"green\",title=\"Worst Hotels \")\n_=plt.xlabel('AverageOverallRatingOfHotel')\n_=plt.ylabel('HotelID')\nplt.show()\n\n#Best Hotels\nbest_hotels = exist.groupby('Hotelid')['AverageOverallRatingOfHotel'].mean().sort_values(ascending=False).head(5)\nbest_hotels.plot(kind=\"barh\",color = \"pink\",title=\"Best Hotels \")\n_=plt.xlabel('AverageOverallRatingOfHotel')\n_=plt.ylabel('HotelID')\nplt.show()\n\n\n","6263c7ef":"## Preprocessing On Train data and Test Data \n\ntrain[\"reviewtext\"]=train[\"reviewtext\"].apply(lambda x:re.sub(\"[^A-Za-z]\", \" \", x.strip()))\ntest[\"reviewtext\"]=test[\"reviewtext\"].apply(lambda x:re.sub(\"[^A-Za-z]\", \" \", x.strip()))\n\n","b80c2630":"## Lower case\ntrain['reviewtext'] = train['reviewtext'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ntest['reviewtext'] = test['reviewtext'].apply(lambda x: \" \".join(x.lower() for x in x.split()))","1f90d042":"### Numbers Removal\ntrain['reviewtext'] = train['reviewtext'].str.replace('[\\d]', '')\ntest['reviewtext'] = test['reviewtext'].str.replace('[\\d]', '')","8b27eebf":"## ### Punctuation Removal\ntrain['reviewtext'] = train['reviewtext'].str.replace('[^\\w\\s]','')\ntest['reviewtext'] = test['reviewtext'].str.replace('[^\\w\\s]','')","3f7bcabe":"#### Stop words Removal\n\nstop = stopwords.words('english')\ntrain['reviewtext'] = train['reviewtext'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\ntest['reviewtext'] = test['reviewtext'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))","0eb3dd4f":"### Common word removal\n## On trainData\nfreq = pd.Series(' '.join(train['reviewtext']).split()).value_counts()[:10]\nfreq\n","d3fb7e0c":"freq = list(freq.index)\ntrain['reviewtext'] = train['reviewtext'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ntrain['reviewtext'].head()\n","383ba436":"## On test Data\n\nfreq1 = pd.Series(' '.join(test['reviewtext']).split()).value_counts()[:10]\nfreq1","33af121b":"freq1 = list(freq1.index)\ntest['reviewtext'] = test['reviewtext'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq1))\ntest['reviewtext'].head()","7824f63c":"### Rare words removal\n### On train Data\nfreq = pd.Series(' '.join(train['reviewtext']).split()).value_counts()[-10:]\nfreq","7d4fee89":"freq = list(freq.index)\ntrain['reviewtext'] = train['reviewtext'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ntrain['reviewtext'].head()","67c2c681":"## On test Data\nfreq1 = pd.Series(' '.join(test['reviewtext']).split()).value_counts()[-10:]\nfreq1","6861b0b2":"freq1 = list(freq1.index)\ntest['reviewtext'] = test['reviewtext'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq1))\ntest['reviewtext'].head()","2d04bcf5":"### Stemming\nst = PorterStemmer()\ntrain[\"reviewtext\"] = train['reviewtext'].apply(lambda x: \" \".join([st.stem(word)\n                                                                    for word in x.split()]))\ntest[\"reviewtext\"] = test['reviewtext'].apply(lambda x: \" \".join([st.stem(word)\n                                                                  for word in x.split()]))","855f6f81":"### Lemmatization\nLem = WordNetLemmatizer()\n\ntrain[\"reviewtext\"] = train['reviewtext'].apply(lambda x: \" \".join([Lem.lemmatize(word)\n                                                                    for word in x.split()]))\n\ntest[\"reviewtext\"] = test['reviewtext'].apply(lambda x: \" \".join([Lem.lemmatize(word)\n                                                                  for word in x.split()]))","fee7b2a1":"### Converting the text data into list\nText_data1 = train['reviewtext'].tolist() \nText_data2 = test['reviewtext'].tolist()","5fc313d6":"for i in range(len(Text_data1)):     \n    Text_data1[i]=re.sub(r'\\s+', ' ', Text_data1[i]) #Removing more than one white spaces in the sentence     \n    Text_data1[i]=re.sub('[\\d]', ' ',Text_data1[i]) \n    Text_data1[i]=re.sub(r'[^\\x00-\\x7F]+',' ',Text_data1[i])\n    \nText_data1","c79f7257":"for i in range(len(Text_data2)):     \n    Text_data2[i]=re.sub(r'\\s+', ' ', Text_data2[i])     \n    Text_data2[i]=re.sub('[\\d]', ' ',Text_data2[i]) \n    Text_data2[i]=re.sub(r'[^\\x00-\\x7F]+','',Text_data2[i])\n\n    Text_data2","76f01096":"## Splitting the Data\nX_train,X_test,y_train,y_test = train_test_split(Text_data1,train['Sentiment'],test_size=0.3,random_state=124) ","8ea0509d":"print(y_train.value_counts())\nprint(y_test.value_counts())","0cdcc592":"## Creating a Tfidf Matrix\ntfidf_transformer = TfidfVectorizer(ngram_range=(1,1),stop_words='english',max_features=350)\nX_train_tfidf = tfidf_transformer.fit_transform(X_train)\nprint(X_train_tfidf.shape)\n# Get the tfidf matrix for test documents\nX_test_tfidf = tfidf_transformer.transform(X_test) \nprint(X_test_tfidf.shape)\n\ntest_tfidf=tfidf_transformer.transform(Text_data2)","49f5b30f":"## MODEL BUILDING\n\n## Navie Bayes model\n\nnb_clf = MultinomialNB().fit(X_train_tfidf,y_train) \n\npred_train = nb_clf.predict(X_train_tfidf)  \npred_test = nb_clf.predict(X_test_tfidf) #predict on test data \n","2bf91b45":"print(accuracy_score(y_train,pred_train)) \nprint(accuracy_score(y_test,pred_test))","d989de66":"## LOGISTIC REGRESSION\nlogmod=LogisticRegression()\n\nlogmod.fit(X_train_tfidf,y_train)\n\npred_train_log = logmod.predict(X_train_tfidf)\npred_test_log = logmod.predict(X_test_tfidf)\ntest_pred = logmod.predict(test_tfidf)\n\nprint(\"Accuracy on train is:\",accuracy_score(y_train,pred_train_log))\nprint(\"Accuracy on test is:\",accuracy_score(y_test,pred_test_log))\n","54088a4c":"## Using Grid search Model\n\nparam_grid={\n    \"C\":[10,20],\n    \"max_iter\":[100,150]\n}\nparam_grid_model = GridSearchCV(estimator = logmod, param_grid = param_grid, cv =5,n_jobs=-1)","062f34e4":"param_grid_model.fit(X_train_tfidf,y_train)","09d0dce9":"y_preds_lr_train = param_grid_model.best_estimator_.predict(X_train_tfidf)\ny_preds_lr_test = param_grid_model.best_estimator_.predict(X_test_tfidf)\ntest_pred_gd = param_grid_model.best_estimator_.predict(test_tfidf)","f54a9c41":"print(\"Accuracy on train is:\",accuracy_score(y_train,y_preds_lr_train))\nprint(\"Accuracy on test is:\",accuracy_score(y_test,y_preds_lr_test))","bf8b299d":"### submission file \ntest_Id = test.Reviewid.copy()\nsubmission = pd.DataFrame()\nsubmission['Id'] = test_Id","f899f611":"submission['Sentiment'] = test_pred_gd\nsubmission.head()\nsubmission.to_csv('final_subsmission.csv', index=False)","dbef4664":"### These are the other models\n## DECISION TREES\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train_tfidf,y_train)\n\npred_train_dt = dtc.predict(X_train_tfidf)\npred_test_dt = dtc.predict(X_test_tfidf)\n\nprint(\"Accuracy on train is:\",accuracy_score(y_train,pred_train_dt))\n\nprint(\"Accuracy on test is:\",accuracy_score(y_test,pred_test_dt))\n\n","6474f2b7":"## RANDOM FOREST\n\nrfc=RandomForestClassifier()\nrfc.fit(X=X_train_tfidf,y=y_train)\n\npred_train_rf=rfc.predict(X_train_tfidf)\n\npred_test_rf=rfc.predict(X_test_tfidf)\n\nprint(\"Accuracy on train is:\",accuracy_score(y_train , pred_train_rf))\n\nprint(\"Accuracy on test is:\",accuracy_score(y_test , pred_test_rf))","aa11d83e":"# ADA BOOST\nAdaboost_model = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=3),\n    n_estimators = 600,\n    learning_rate = 1)\n# Train model\n%time Adaboost_model.fit(X_train_tfidf, y_train)\n\n# Predictions and Evaluations \npred_train_Ada = Adaboost_model.predict(X_train_tfidf)\n\npred_test_Ada = Adaboost_model.predict(X_test_tfidf)\n\nprint(\"Accuracy on train is:\",accuracy_score(y_train,pred_train_Ada ))\n\nprint(\"Accuracy on test is:\",accuracy_score(y_test,pred_test_Ada ))","f8ddeb0c":"### XG BOOST\nXGB_model = XGBClassifier()\n# training the xgboost classifier\n%time XGB_model.fit(X_train_tfidf, y_train)\n\npred_train_XG = XGB_model.predict(X_train_tfidf)\n\npred_test_XG = XGB_model.predict(X_test_tfidf)\n\nprint(\"Accuracy on train is:\",accuracy_score(y_train,pred_train_XG ))\n\nprint(\"Accuracy on test is:\",accuracy_score(y_test,pred_test_XG ))","85d0585c":"# TASK 3\n### Preprocessing on Existing Hotels Data and New Hotels Data\n# Verifying the NULL values\nprint(\"__________On existing hotels data _______________\")\nprint(exist.isnull().sum().sort_values(ascending=True))\nprint(\"__________On New hotels data _______________\")\nprint(New.isnull().sum().sort_values(ascending=True))","6091e468":"#### Separating the Averagepricing Column\nexist=exist.join(exist['AveragePricing'].str.split('$', 1, expand=True).rename(columns={ 1:'AvgPricing'}))\nexist = exist.drop(0,axis=1)\n\nNew=New.join(New['AveragePricing'].str.split('$', 1, expand=True).rename(columns={ 1:'AvgPricing'}))\nNew = New.drop(0,axis=1)\n","9db45030":"#### Creating a LengthOfReview Column\nexist['LengthofReview'] = exist['reviewtext'].apply(lambda x: len(str(x).split(\" \")))\n\nNew['LengthofReview'] = New['reviewtext'].apply(lambda x: len(str(x).split(\" \")))","596e8071":"#### Dropping the unnecessary columns\nexist = exist.drop([\"Date\",\"userid\",\"reviewtext\",\"AveragePricing\"],axis=1)\nNew = New.drop([\"Date\",\"userid\",\"reviewtext\",\"AveragePricing\"],axis=1)","0dae74a1":"#### Verifying the datatypes\nprint(\"___________Existing hotels Dtypes____________\")\nprint(exist.dtypes)\n\nprint(\"___________New hotels Dtypes____________\")\nprint(New.dtypes)","777577ed":"#### Datatypes conversion\nexist[\"AvgPricing\"] = exist[\"AvgPricing\"].astype(\"float64\")\nNew[\"AvgPricing\"] = New[\"AvgPricing\"].astype(\"float64\")","ed9216e4":"#### Verifying the unique values count on the data\nprint(\"___________Unique values count on existing hotels___________\")\nunique_counts = pd.DataFrame.from_records([(col, exist[col].nunique()) for col in exist.columns],\n                          columns=['Column_Name', 'Num_Unique']).sort_values(by=['Num_Unique'])\nprint(unique_counts)\n\n\nprint(\"___________Unique values count on New hotels___________\")\nunique_counts = pd.DataFrame.from_records([(col, New[col].nunique()) for col in New.columns],\n                          columns=['Column_Name', 'Num_Unique']).sort_values(by=['Num_Unique'])\nprint(unique_counts)\n","ea2335cf":"### Feature Engineering\n#### On Existing hotels Data\nMR = exist.groupby(['Hotelid']).mean().reset_index()\n\nMR.drop(['NoOfReaders',\"HelpfulToNoOfreaders\"],axis=1, inplace=True)\n\nMR1 = pd.DataFrame(exist.groupby('Hotelid')['NoOfReaders',\"HelpfulToNoOfreaders\"].sum().reset_index())","8119160a":"Existing_Hotels = pd.merge(MR1,MR,on=['Hotelid','Hotelid'])\nExisting_Hotels.dtypes","aa3fe42f":"#### On New Hotels Data\nMR2 = New.groupby(['Hotelid']).mean().reset_index()\n\nMR2.drop(['NoOfReaders',\"HelpfulToNoOfreaders\"],axis=1, inplace=True)\n\nMR3 = pd.DataFrame(New.groupby('Hotelid')['NoOfReaders',\"HelpfulToNoOfreaders\"].sum().reset_index())","9638c1c6":"New_Hotels = pd.merge(MR2,MR3,on=['Hotelid','Hotelid'])\nNew_Hotels.dtypes","695dd2ea":"#### Drop the Hotelid Column\nExisting_Hotels = Existing_Hotels.drop(\"Hotelid\",axis=1)\nNew_Hotels = New_Hotels.drop(\"Hotelid\",axis=1)","6671b148":"# CLUSTERING\n#### Standardizing the data\nstd = StandardScaler()\nstd.fit(Existing_Hotels)\nX_train_std= std.transform(Existing_Hotels)\nX_test_std= std.transform(New_Hotels)","a84318a9":"#### KMEANS Model\nkmeans = KMeans(n_clusters=2,random_state=99999)\n\nkmeans = kmeans.fit(X_train_std)\n\nlabels_train = kmeans.predict(X_train_std)\nlabels_test = kmeans.predict(X_test_std)\n\n# Centroid values\ncentroids = kmeans.cluster_centers_","6b3b900f":"## Checking the values\ncentroids","afb20a9b":"labels_train","f318ec85":"labels_test","ef18896b":"print(kmeans.cluster_centers_)\nprint(kmeans.inertia_)","7ea1463c":"####  Checking For Various Values of K\nwss= {}\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(X_train_std)\n    clusters = kmeans.labels_\n    wss[k] = kmeans.inertia_","22327896":"plt.figure()\nplt.plot(list(wss.keys()), list(wss.values()))\nplt.xlabel(\"Number of cluster\")\nplt.ylabel(\"WSS\")\nplt.show()","1b02cbc1":"wss","2a4965f3":"#### KMEANS Model with \"K\" value\nkmeans = KMeans(n_clusters=5,random_state=9999)\n\nkmeans.fit(Existing_Hotels)\n\nExisting_Hotels[\"cluster\"] = kmeans.predict(Existing_Hotels)\nNew_Hotels[\"cluster\"] = kmeans.predict(New_Hotels)\n\n# Centroid values\ncentroids = kmeans.cluster_centers_","c5adfaf9":"#### Checking for cluster Stability\nindices=Existing_Hotels.sample(frac=0.8,random_state=123).index\nprint(indices)","55bac52f":"#### Subsetting 80% of train data\nAlpha = Existing_Hotels.loc[indices,:]\nAlpha.shape","5b199ab2":"kmeans = KMeans(n_clusters=5,random_state=45)\nkmeans2=kmeans.fit(Alpha)\nprint(len(kmeans2.labels_))\nAlpha['cluster']=kmeans2.labels_","13f87f89":"g1=Existing_Hotels.loc[indices,'cluster']\ng2=Alpha.cluster","138e41e3":"#### Cluster Stability\nadjusted_rand_score(g1,g2)","8649efda":"print(\"__________On user ratings data _______________\")\nprint(ratings.isnull().sum().sort_values(ascending=True))","a7040f37":"#### Verifying the unique counts on the data\nprint(\"___________Unique values count on existing hotels___________\")\nunique_counts = pd.DataFrame.from_records([(col, ratings[col].nunique()) for col in ratings.columns],\n                          columns=['Column_Name', 'Num_Unique']).sort_values(by=['Num_Unique'])\nprint(unique_counts)","c2f7af60":"### Feature Engineering\n# calculating the mean ratings for each hotel\n  \nrating = pd.DataFrame(ratings.groupby('Hotelid')['OverallRating'].count())  \nrating[\"ratings_count\"] = rating[\"OverallRating\"]\nrating=rating.drop(\"OverallRating\",axis=1)","857e4d73":"# sorting based on count of ratings that each hotelId got  \n  \nrating.sort_values('ratings_count', ascending=False).head()","51e7a36c":"## Creating a Pivot table\n# Preparing data table for analysis  \n  \nratings_pivot = ratings.pivot_table(values='OverallRating', index='userid', columns='Hotelid')  \n  \nratings_pivot.head()","9a15ff66":"# we can calculate the correlation of the Hotelid column with all others and for this we can use the corrwith function\nX = ratings_pivot[\"hotel_510\"]  ","eb64fb08":"### Checking correlation for a Hotelid\nCorr = pd.DataFrame(ratings_pivot.corrwith(X)) \nCorr.rename(columns={0: 'corr'}, inplace=True)\nCorr.head()","eab3c916":"#### Joining the two required columns\nFinal_summary = Corr.join(rating)","b0d47035":"# These are the most similar Hotels  \n  \nFinal_summary.sort_values('corr', ascending=False).head(10)","da0f92f9":"corr_matrix = ratings_pivot.corr(method=\"pearson\")\ncorr_matrix.head()","80b8d530":"ratings_pivot.iloc[3].dropna().head()","6e5eb93b":"## We create now the list of all Hotels with all correlations multiplied by ratings (integers from 1 to 5). \nuser_corr = pd.Series()\n\nuserid=3\n\nfor Hotelid in ratings_pivot.iloc[userid].dropna().index:\n    corr_list = corr_matrix[Hotelid].dropna()*ratings_pivot.iloc[userid][Hotelid]\n    user_corr = user_corr.append(corr_list)\n    ","ee6a1b00":"\n## We make the groupby in order to not have duplicate Hotels and we also sum their rating: *\nuser_corr = user_corr.groupby(user_corr.index).sum()\nuser_corr.head()","e105327f":"## We now create a list of Hotels Visited to drop (if contained in our Series) *\nHotels_list = []\nfor i in range(len(ratings_pivot.iloc[userid].dropna().index)):\n    if ratings_pivot.iloc[userid].dropna().index[i] in user_corr:\n        Hotels_list.append( ratings_pivot.iloc[userid].dropna().index[i])\n    else:\n        pass\n\nuser_corr = user_corr.drop(Hotels_list)","ba48c551":"print(\"\\n These are the hotels which you have visited \\n\")\nfor i in ratings_pivot.iloc[userid].dropna().index:\n    print(i)\nprint(\"\\n We would suggest you to try these 5 Hotels: \\n\")\nfor i in user_corr.sort_values(ascending=False).index[:5]:\n    print(i)","23a9fbb3":"*  In our model we\u2019ll use the Hotel based system because we are considering that a user based system could be influenced by the change of film taste in the time by people and also because having less Hotels than Users, will fasten our calculations.","b6f6b665":"### Analysis of Hotel Reviews, Segmentation & Develop Recommender System\n\n#### Problem Statement\n* The Client is interested in performing feedback analysis using review text , cluster the Hotels based on the required columns and recommend the hotels to the users who hasn't visited the hotel yet based on the similar user ratings\n\n#### Business Understanding\n\n* Rating systems are meant to provide an accurate and objective assessment of accommodations.Website ratings are based on personal   opinion and individual guest experience.The users opinion helps to improve reputation,higher occupancy and better pricing of the hotel.Data Analytics can enable a good understanding of customers\u2019 perception brand positioning of hotel and thus helps in accurately aligning Hotel prices, placement and availability with each customer segment in an insightful way.\n\n#### The Tasks Of This Project :\n* To do Exploratory Data Analysis using visualizations\n\n* To Predict the feedback analysis to classify each feedback\n\n* To segment the hotels to enable the user to choose one of the hotels from the group related the selected hotels.\n\n* To build the recommender system based on user ratings\n\n### TABLE OF CONTENTS\n###### IMPORTING TOOLS\n\nREADING AND EXPLORING THE DATA\n\n* Reading and exploring on train data\n* Reading and exploring on test data\n* Reading and exploring on Existing hotels data\n* Reading and exploring on New hotels data\n* Reading and exploring on User_ratings data","dfac6173":"### Recommendtaions for Users\n### Let's recommend Hotels for the below user based on his previous visits to the hotels\n* These are the ratings given by \"user_21051\" to different hotels *","08d5d93d":"**** TASK1\n\n**** Visualizations\n\nVisualizations on train data\nVisualizations on existing hotels data\n\n\n**** TASK2\n\nPreprocessing On Train data and Test Data\n\n* Lower Case\n* Removing numbers\n* Removing punctuation\n* Removing Stop words\n* Removing Common words\n* Rare words removal\n* Stemming\n* Lemmatization\n* Converting the text data into list\n\n**** SPLITTING THE DATA\n\n##### CREATING A TF_IDF MATRIX\n\n### MODEL BUILDING\n\n* Naive Bayes Model\n* Logistic Regression model\n* Using Grid search Model\n* Decision Trees\n* Random Forest\n* AdaBoost\n* XGBoost\n\n# TASK3\n\n## Preprocessing on Existing Hotels Data and New Hotels Data\n\n* Verifying the NULL values\n* Separating the AveragePricing Column\n* Creating a LengthOfReview Column\n* Dropping the unnecessary columns\n* Verifying the datatypes\n* Datatypes conversion\n* Verifying the unique values count on the data\n#### FEATURE ENGINEERING\n\n* On Existing Hotels Data\n* On New Hotels Data\n* Drop the HotelId Column\n* CLUSTERING\n\n##### Standardizing the data\n* KMEANS Model\n* Checking the values\n* Checking For Various Values of K\n* KMEANS Model with \"K\" value\n* Checking for cluster Stability\n* Subsetting 80% of train data\n* Cluster Stability\n\n# TASK 4\n\n### Preprocessing on user ratings data\n\n* Checking for Null values\n* Verifying the unique counts on the data\n* Feature Engineering\n* Summary\n* Creating a Pivot table\n* Checking correlation for a Hotelid\n* Joining the two required columns\n* Pearson Correlation\n* Recommendations For Users\n","b566b090":"### CONCLUSIONS\n###### Summary\n* In this project i have used Logistic regression to do feedback analysis.Moreover, I have used ensemble classification techniques to enhance my prediction power based on available additional information and I found that the Logistic regression works best when compared to all the other ensemble models which i used.Then found the segments for Existing Hotels and New Hotels Data. I recommended to the users based on the similarities of ratings on previously visited hotels.\n* Some interesting things i had learnt are:\n* Using Pearson correlation technique , how to build the recommendation systems.\n* In clustering, feature engineering i had done like using groupby found the mean of all the columns with respect to hotelid\n* Sentiment Trend plots to understand about the sentiments count with respect to time\n\n### Business Recommendations\n* The hotels have demand during the holidays either may be summer or christmas holidays. So they have to provide reasonable pricing in the hotels cost and should maintain cleanliness.\n\n* Wonderful service at the hotel helps to increase the crowd and can become a brand for customers.\n\n* The user ratings mainly depends on the above cases and every hotel should try to focus on customer satisfaction for their business survival.","d1ce4b43":"### Recommendations to the user","0ef85f4a":"## PEARSON CORRELATION\n* Now let's do the correlation on the whole dataset to find the similarities between the hotels \n* we can also specify which correlation function to use, and in this case we\u2019ll use the Pearson formula.","a4906e9a":"## TASK 4\n## We have to recommend Hotels to the Users based on previous Experiences of visited hotels             \n### Preprocessing for userratings data\n#### Checking for Null values","e645a619":"* These models are overfitting , that's why i considered LOGISTIC REGRESSION model as the best model for feedback analysis"}}