{"cell_type":{"fb039179":"code","e458b7c3":"code","1c5c0dd4":"code","23105039":"code","bfb1c5dd":"code","d2fc4a69":"code","623580d3":"code","fbbf6bea":"code","991bfe40":"code","962e83af":"code","d5d49ffd":"code","05ef4a79":"code","265e1f7d":"code","5e6d150b":"code","2514dee1":"code","872efe64":"markdown","e1437508":"markdown","1d0bb3bd":"markdown","04702164":"markdown","c409b0a9":"markdown","d6192665":"markdown","2e0390fc":"markdown","ebe035f3":"markdown","36819ed7":"markdown","8f34102e":"markdown","7d43787d":"markdown","cbfceb13":"markdown"},"source":{"fb039179":"# All the libraries used will be collected here at the end","e458b7c3":"import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\ntrain = pd.read_pickle('\/kaggle\/input\/ubiquant-market-prediction-half-precision-pickle\/train.pkl')\ntrain.head()","1c5c0dd4":"train.shape","23105039":"train.dtypes","bfb1c5dd":"train.isnull().sum().sum()","d2fc4a69":"y = train['target']\nx = train.drop(['target'], axis=1)","623580d3":"x.shape, y.shape","fbbf6bea":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.7, random_state=4)","991bfe40":"from sklearn.ensemble import RandomForestRegressor\nrf_model = RandomForestRegressor(n_estimators = 100, random_state = 0)\n#rf_model.fit(x_train, y_train)","962e83af":"#rf_model.score(x_test, y_test)","d5d49ffd":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\n\nNN_model = Sequential()\n\n# The Input Layer :\nNN_model.add(Dense(128, kernel_initializer='normal',input_dim = x.shape[1], activation='relu'))\n\n# The Hidden Layers :\nNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\nNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\nNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n\n# The Output Layer :\nNN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n\n# Compile the network :\nNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\nNN_model.summary()","05ef4a79":"from keras.callbacks import ModelCheckpoint\n\ncheckpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \ncheckpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\ncallbacks_list = [checkpoint]","265e1f7d":"NN_model.fit(x_train, y_train, epochs=2, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)","5e6d150b":"#NN_model.predict(x_test)","2514dee1":"\"\"\"import ubiquant\nenv = ubiquant.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\nfor (test_df, sample_prediction_df) in iter_test:\n    sample_prediction_df['target'] =   NN_model.predict(test_df) # make your predictions here\n    env.predict(sample_prediction_df)   # register your predictions\"\"\"","872efe64":"> ## Data analysis","e1437508":"> ## Data Modeling","1d0bb3bd":"### This notebook is not yet completed.\n## Please support!","04702164":"### Hi all, the dataset is so huge so we have to find a way to read the data efficiently.\n\n### **Use Pickle**\n\n#### The dataset has been normalised and reduced in size.\n\n##### Still in building phase.\n\n### Using Deep neural network in the first commit","c409b0a9":"> ## Neural Network","d6192665":"### Making deep neural network","2e0390fc":"### Defining a checkpoint callback","ebe035f3":"### Model training","36819ed7":"#### Feels great to find out that it has 0 null values. This reduces our work.","8f34102e":"### Random Forest","7d43787d":"> ## Data Visualization","cbfceb13":"> ## Submission"}}