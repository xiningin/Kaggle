{"cell_type":{"6f9c4a0d":"code","78f890fe":"code","6da9db34":"code","5acd84db":"code","54c7bcb8":"code","9741fe3f":"code","4be89d30":"code","f2126f11":"code","4918de3d":"code","baa3e69d":"code","de4ff754":"code","956a81ac":"code","1dfc3995":"code","49257cfe":"code","a39e020a":"code","788723ca":"code","c784844b":"code","7d81186a":"code","02ed1d56":"markdown","c46f42ee":"markdown"},"source":{"6f9c4a0d":"import pandas as pd\nimport numpy as np\nimport os,cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\n\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.utils import plot_model\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.utils import to_categorical\nfrom keras.datasets import cifar10\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import Adam,SGD\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping, Callback, TensorBoard\nfrom keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, Dropout\nfrom keras.models import Sequential,Input,Model\n\nfrom keras.initializers import *\n%matplotlib inline","78f890fe":"batch_size = 32\nnum_classes = 10","6da9db34":"(train_images,train_labels),(test_images,test_labels) = cifar10.load_data()\ntrain_images.shape,test_labels.shape,test_images.shape,test_labels.shape","5acd84db":"fig, ax = plt.subplots(1,2,figsize=(15,5))\n\nsns.countplot(train_labels.ravel(),ax=ax[0])\nsns.countplot(test_labels.ravel(),ax=ax[1])\n\nax[0].set_title(\"Training Data\")\nax[1].set_title(\"Testing Data\");","54c7bcb8":"train_images = train_images.astype(\"float\")\ntest_images = test_images.astype(\"float\")\n\ntrain_labels = to_categorical(train_labels,num_classes)\ntest_labels = to_categorical(test_labels,num_classes)","9741fe3f":"train_images[0].shape","4be89d30":"from IPython.display import clear_output\n\nclass DrawingPlot(Callback):\n    \n    def on_train_begin(self,logs={}):\n        \n        self.loss = []\n        self.val_loss = []\n        self.accuracy = []\n        self.val_accuracy = []\n        self.logs = []\n        pass\n    \n    def on_epoch_end(self,epoch,logs={}):\n        \n        self.logs.append(logs)\n        self.loss.append(logs.get('loss'))\n        self.val_loss.append(logs.get('val_loss'))\n        self.accuracy.append(logs.get('accuracy'))\n        self.val_accuracy.append(logs.get('val_accuracy'))\n        \n        fig, ax = plt.subplots(1,2,figsize=(15,5))\n        ax[0].set_title('Loss')\n        ax[1].set_title(\"Accuracy\")\n        ax[0].plot(self.loss,label='Train Loss')\n        ax[0].plot(self.val_loss,label='Test loss')\n        ax[1].plot(self.accuracy,label='Train Accuracy')\n        ax[1].plot(self.val_accuracy,label='Test Accuracy')\n        \n        ax[0].legend(loc='upper right')\n        ax[1].legend(loc='lower right')\n        \n        plt.show()\n        pass\n    \nplot = DrawingPlot()","f2126f11":"def show_final_history(history):\n    \n    plt.style.use(\"ggplot\")\n    fig, ax = plt.subplots(1,2,figsize=(15,5))\n    ax[0].set_title('Loss')\n    ax[1].set_title(\"Accuracy\")\n    ax[0].plot(history.history['loss'],label='Train Loss')\n    ax[0].plot(history.history['val_loss'],label='Test loss')\n    ax[1].plot(history.history['accuracy'],label='Train Accuracy')\n    ax[1].plot(history.history['val_accuracy'],label='Test Accuracy')\n    \n    ax[0].legend(loc='upper right')\n    ax[1].legend(loc='lower right')\n    plt.show()\n    pass","4918de3d":"train_datagen = ImageDataGenerator(rescale=1.\/255,\n                                  horizontal_flip=True,\n                                  vertical_flip=True,\n                                  rotation_range=20)\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = train_datagen.flow(x=train_images,\n                                   y=train_labels,\n                                   batch_size=batch_size,\n                                   shuffle=True,\n                                   seed=42)\n\ntest_generator = test_datagen.flow(x=test_images,\n                                 y=test_labels,\n                                 batch_size=batch_size,\n                                 shuffle=True,\n                                 seed=42)","baa3e69d":"def identity_block(X,f,filters,stage,block):\n    \n    conv_name_base = 'res_'+str(stage)+block+'_branch'\n    bn_name_base = 'bn_'+str(stage)+block+'_branch'\n    \n    F1,F2,F3 = filters\n    \n    X_shortcut = X\n    \n    # First Component of Main Path\n    X = Conv2D(filters=F1,kernel_size=(3,3),strides=(1,1),\n               padding='same',name=conv_name_base+'2a',\n               kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(name=bn_name_base+'2a')(X)\n    X = Activation('relu')(X)\n    \n    # Second Component of Main Path\n    X = Conv2D(filters=F2,kernel_size=(f,f),strides=(1,1),\n              padding='same',name=conv_name_base+'2b',\n              kernel_initializer=glorot_uniform(seed=0))(X)\n    X = BatchNormalization(name=bn_name_base+'2b')(X)\n    X = Activation('relu')(X)\n    \n    # Third Component of Main Path\n    X = Conv2D(filters=F3,kernel_size=(3,3),strides=(1,1),\n              padding='same',name=conv_name_base+'2c',\n              kernel_initializer=glorot_uniform(seed=0))(X)\n    X = BatchNormalization(name=bn_name_base+'2c')(X)\n    \n    X = Add()([X,X_shortcut])\n    X = Activation('relu')(X)\n    \n    return X\n    pass","de4ff754":"def convolutional_block(X,f,filters,stage,block,s=2):\n    \n    conv_base_name = 'res_' + str(stage) + block + '_branch'\n    bn_base_name = 'bn_' + str(stage) + block + '_branch'\n    \n    F1,F2,F3 = filters\n    \n    X_shortcut = X\n    \n    ### MAIN PATH ###\n    # First component of main path\n    X = Conv2D(filters=F1,kernel_size=(3,3),strides=(s,s),\n              padding='same',name=conv_base_name+'2a',\n              kernel_initializer=glorot_uniform(seed=0))(X)\n    X = BatchNormalization(name=bn_base_name+'2a')(X)\n    X = Activation('relu')(X)\n    \n    # Second Component of main path\n    X = Conv2D(filters=F2,kernel_size=(f,f),strides=(1,1),\n              padding='same',name=conv_base_name+'2b',\n              kernel_initializer=glorot_uniform(seed=0))(X)\n    X = BatchNormalization(name=bn_base_name+'2b')(X)\n    X = Activation('relu')(X)\n    \n    # Third Component of main path\n    X = Conv2D(filters=F3,kernel_size=(3,3),strides=(1,1),\n              padding='same',name=conv_base_name+'2c',\n              kernel_initializer=glorot_uniform(seed=0))(X)\n    X = BatchNormalization(name=bn_base_name+'2c')(X)\n    \n    # Shortcut path\n    X_shortcut = Conv2D(filters=F3,kernel_size=(1,1),strides=(s,s),\n                       padding='same',name=conv_base_name+'1',\n                       kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n    X_shortcut = BatchNormalization(name=bn_base_name+'1')(X_shortcut)\n    \n    X = Add()([X,X_shortcut])\n    X = Activation('relu')(X)\n    \n    return X\n    pass","956a81ac":"def ResNet(input_shape,classes):\n    \n    X_input = Input(input_shape)\n    \n    # Zero Padding\n    X = ZeroPadding2D((3,3))(X_input)\n    \n    # Stage 1\n    X = Conv2D(8,(7,7),strides=(2,2),name='conv1',kernel_initializer=glorot_uniform(seed=0))(X)\n    X = BatchNormalization(name='bn_conv1')(X)\n    X = Activation('relu')(X)\n    X = MaxPooling2D((3,3),strides=(2,2))(X)\n    X = Dropout(0.25)(X)\n    \n    # Stage 2\n    X = convolutional_block(X,f=3,filters=[16,16,32],stage=2,block='A',s=1)\n    X = identity_block(X,3,[16,16,32],stage=2,block='B')\n    X = identity_block(X,3,[16,16,32],stage=2,block='C')\n    X = Dropout(0.25)(X)\n    \n    # Stage 3\n    X = convolutional_block(X,f=3,filters=[32,32,64],stage=3,block='A',s=2)\n    X = identity_block(X,f=3,filters=[32,32,64],stage=3,block='B')\n    X = identity_block(X,f=3,filters=[32,32,64],stage=3,block='C')\n    X = identity_block(X,f=3,filters=[32,32,64],stage=3,block='D')\n    X = Dropout(0.25)(X)\n    \n    # Stage 4\n    X = convolutional_block(X,f=3,filters=[64,64,128],stage=4,block='A',s=2)\n    X = identity_block(X,f=3,filters=[64,64,128],stage=4,block='B')\n    X = identity_block(X,f=3,filters=[64,64,128],stage=4,block='C')\n    X = identity_block(X,f=3,filters=[64,64,128],stage=4,block='D')\n    X = identity_block(X,f=3,filters=[64,64,128],stage=4,block='E')\n    X = identity_block(X,f=3,filters=[64,64,128],stage=4,block='F')\n    X = Dropout(0.25)(X)\n    \n    # Stage 5\n    X = convolutional_block(X,f=3,filters=[128,128,256],stage=5,block='A',s=1)\n    X = identity_block(X,f=3,filters=[128,128,256],stage=5,block='B')\n    X = identity_block(X,f=3,filters=[128,128,256],stage=5,block='C')\n    X = Dropout(0.25)(X)\n    \n    # Stage 6\n    X = convolutional_block(X,f=3,filters=[256,256,512],stage=6,block='A',s=2)\n    X = identity_block(X,f=3,filters=[256,256,512],stage=6,block='B')\n    X = identity_block(X,f=3,filters=[256,256,512],stage=6,block='C')\n    X = identity_block(X,f=3,filters=[256,256,512],stage=6,block='D')\n    X = Dropout(0.25)(X)\n    \n    # Average Pool Layer\n    X = AveragePooling2D((1,1),name=\"avg_pool\")(X)\n    \n    # Output layer\n    X = Flatten()(X)\n    X = Dense(classes,activation='softmax',name='fc'+str(classes),\n              kernel_initializer=glorot_uniform(seed=0))(X)\n    \n    model = Model(inputs=X_input,outputs=X,name='ResNet')\n    \n    return model\n    pass","1dfc3995":"model = ResNet(input_shape=(32,32,3),classes=num_classes)","49257cfe":"plot_model(model, to_file='model.png')\nSVG(model_to_dot(model).create(prog='dot', format='svg'))\n\nmodel.summary()","a39e020a":"opt = Adam(lr=0.0001)\n\nmodel.compile(optimizer=opt,loss=['categorical_crossentropy'],metrics=['accuracy'])","788723ca":"checkpoint = ModelCheckpoint(\"model_weights.h5\",monitor=\"val_accuracy\",verbose=1,save_best_only=True,\n                            mode=\"max\")\ntensorboard_callback = TensorBoard(\"logs\")","c784844b":"epochs = 200\n\nhistory = model.fit_generator(generator = train_generator,\n                              steps_per_epoch = train_generator.n\/\/batch_size,\n                              epochs = epochs,\n                              validation_data = test_generator,\n                              validation_steps = test_generator.n\/\/batch_size,\n                              callbacks = [checkpoint,tensorboard_callback],\n                              verbose = 1)","7d81186a":"show_final_history(history)","02ed1d56":"No class imbalance present in the dataset.","c46f42ee":"Plotting the loss and accuracy versus time"}}