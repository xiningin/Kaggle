{"cell_type":{"9633dbd3":"code","5cec6776":"code","4e502950":"markdown","f16460c3":"markdown","fdfedac7":"markdown","a0f5680b":"markdown","1e5e6a39":"markdown","16b0aa9d":"markdown"},"source":{"9633dbd3":"import numpy as np\nA = np.arange(20).reshape((5,4))\n\nprint(\"Given input: \")\nprint(A)\n\ndef dropout(X, drop_probability):\n    keep_probability = 1 - drop_probability\n    mask = np.random.uniform(0, 1.0, X.shape) < keep_probability\n    if keep_probability > 0.0:\n        scale = (1\/keep_probability)\n    else:\n        scale = 0.0\n    return mask * X * scale\n\nprint(\"\\n After Dropout: \")\nprint(dropout(A,0.5))","5cec6776":"#showing picture about dropout  \n\nfrom PIL import Image \nImage.open(\"..\/input\/dropout2\/download.png\") \n","4e502950":"> from keras.layers import Dropout\n\n> model.add(Dropout(rate, inputs= input))\n\n\n**(1) rate:**  Float between 0 and 1. Fraction of the input units to drop.\n\n**(2) inputs:** Input tensor ","f16460c3":"> tf.keras.callbacks.EarlyStopping(\n>     monitor=\"val_loss\",\n>     min_delta=0,\n>     patience=0,\n>     verbose=0,\n>     mode=\"auto\",\n>     restore_best_weights=False,\n> )\n\n\n**(1) monitor** Quantity to be monitored\n\n**(2) min_delta** Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.\n\n**(3) patience:** Number of epochs with no improvement after which training will be stopped.\n\n**(4) verbose:** verbosity mode. To discover the training epoch on which training was stopped, the \u201cverbose\u201d argument can be set to 1 Once stopped, the callback will print the epoch number.\n\n**(5) mode mode:** One of {\"auto\", \"min\", \"max\"}. In min mode, training will stop when the quantity monitored has stopped decreasing; in \"max\" mode it will stop when the quantity monitored has stopped increasing; in \"auto\" mode, the direction is automatically inferred from the name of the monitored quantity By default, mode is set to \u2018auto\u2018 and knows that you want to minimize loss or maximize accuracy.\n\n**(6)restore_best_weights:** Whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used.","fdfedac7":"# Dropout \n\n\n**Dropout** is a regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data Dropout can be easily implemented by randomly selecting nodes to be dropped-out with a given probability (e.g. .1%) each weight update cycle.","a0f5680b":"Most importantly Dropout is only used during the training of a model and is not used when evaluating the model.","1e5e6a39":"you could find example about early stopping in my Notebook here  \n\nhttps:\/\/www.kaggle.com\/mahmoudreda55\/textile-97-colab-work\n\n\n\n\n\n\n\n\n \n# If you like this Notebook\n# please Vote and comment ,thanks","16b0aa9d":"# EarlyStopping\n \nTraining too little will lead to underfit in train and test sets. Traning too much will have the overfit in training set and poor result in test sets\n\nStop training when a monitored metric has stopped improving."}}