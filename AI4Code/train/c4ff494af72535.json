{"cell_type":{"920b654c":"code","b707028c":"code","f0bc8833":"code","58a80065":"code","3cf8b9df":"code","bab6bb91":"code","b9eb5d74":"code","a4bd8482":"code","dbec98ca":"code","5090c068":"code","401627c2":"code","81d1c28c":"code","1bb08c5b":"code","4e6dc2c0":"code","fcf93e5d":"code","7396a443":"code","8de719c5":"code","01efc1c4":"code","93e09900":"code","87eeda4f":"code","cfd001e9":"code","39e989c5":"code","74f8d018":"code","07ef1203":"code","eb0f8eac":"code","b658ecb3":"code","52e9fb20":"code","fb61010e":"code","255c71b8":"code","40d8b1b6":"code","74c711f9":"code","ee257379":"code","744172c5":"code","7382903f":"markdown","19fa9026":"markdown","372644f4":"markdown","57a3fd0f":"markdown","408028d9":"markdown","17520a61":"markdown","f37b3968":"markdown","34f6f712":"markdown","79b82988":"markdown","e0ccbafe":"markdown","73b8b7e1":"markdown","18385df2":"markdown","32cf39d1":"markdown"},"source":{"920b654c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b707028c":"df=pd.read_csv(\"\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv\",encoding='latin1')\ndf.head()","f0bc8833":"df.shape","58a80065":"df[\"Sentiment\"].unique()","3cf8b9df":"sns.countplot(df[\"Sentiment\"])","bab6bb91":"# Keeping the important columns only and removing the rest \ndf=df[[\"OriginalTweet\",\"Sentiment\"]]","b9eb5d74":"df.head()","a4bd8482":"labels=['Extremely Negative', 'Extremely Positive', 'Negative', 'Neutral', 'Positive']\nsizes = [\n         df[df['Sentiment'] == 'Extremely Negative'].shape[0], \n         df[df['Sentiment'] == 'Extremely Positive'].shape[0],\n         df[df['Sentiment'] == 'Negative'].shape[0], \n         df[df['Sentiment'] == 'Neutral'].shape[0],\n         df[df['Sentiment'] == 'Positive'].shape[0]\n        ]\nplt.pie(sizes,labels=labels, data=df, autopct='%1.2f%%', shadow=True, startangle=90)\nplt.title(\"Sentiments percentages in train data\")\nplt.axis(\"equal\")","dbec98ca":"#Combining the two sentiments as a single sentiment.\nfor i in [\"Extremely Negative\",\"Negative\"]:\n    df.loc[df[\"Sentiment\"]==i,\"Sentiment\"]=\"Negative\"","5090c068":"#Combining the two sentiments as a single sentiment.\nfor j in [\"Extremely Positive\",\"Positive\"]:\n    df.loc[df[\"Sentiment\"]==j,\"Sentiment\"]=\"Positive\"","401627c2":"#Now you can see that there are only three sentiments.\ndf[\"Sentiment\"].unique()","81d1c28c":"sns.countplot(df[\"Sentiment\"])","1bb08c5b":"labels=['Negative', 'Neutral', 'Positive']\nsizes = [\n         \n         df[df['Sentiment'] == 'Negative'].shape[0], \n         df[df['Sentiment'] == 'Neutral'].shape[0],\n         df[df['Sentiment'] == 'Positive'].shape[0]\n        ]\nplt.pie(sizes,labels=labels, data=df, autopct='%1.2f%%', shadow=True, startangle=90)\nplt.title(\"Sentiments percentages in train data\")\nplt.axis(\"equal\")","4e6dc2c0":"X=df[\"OriginalTweet\"]\ny=df[\"Sentiment\"]","fcf93e5d":"import re\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras.preprocessing.text import one_hot","7396a443":"messages=X.copy()","8de719c5":"messages.shape","01efc1c4":"messages.head()","93e09900":"ps=PorterStemmer()","87eeda4f":"corpus=[]\nfor i in range(len(messages)):\n    #removing everything other than alphabets\n    review=re.sub(\"[^a-zA-Z]\",\" \", str(messages[i]))\n    #remove urls\n    text = re.sub(r'http\\S+', \" \", str(messages[i]))\n    #remove mentions\n    text = re.sub(r'@\\w+',' ', str(messages[i]))\n    #remove hastags\n    text = re.sub(r'#\\w+', ' ', str(messages[i]))\n    #remove html tags\n    text = re.sub('r<.*?>',' ', str(messages[i]))\n    #Lowering the tweets\n    review=review.lower()\n    #Converting into a list\n    review=review.split()\n    #Removing the Stopwords\n    review=[ps.stem(word) for word in review if not word in stopwords.words(\"english\")]\n    #Joining the list \n    review=\" \".join(review)\n    corpus.append(review)","cfd001e9":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ny=le.fit_transform(y)","39e989c5":"#Setting the vocabulary size\nvoc_size=5000","74f8d018":"#we will do one hot encoding for the corpus. It is alloting every word an index according to the vocabulary size\nonehot=[one_hot(words,voc_size) for words in corpus]","07ef1203":"from tensorflow.keras.preprocessing.sequence import pad_sequences","eb0f8eac":"#if length of sentence is not 20 than it will ad 0 in front of sentence such that length becomes 20\nembedded_docs=pad_sequences(onehot,padding=\"pre\",maxlen=305)","b658ecb3":"from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GlobalAveragePooling1D, Input, GlobalMaxPool1D\nfrom keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential\nfrom keras.utils.np_utils import to_categorical","52e9fb20":"model = Sequential([\n    Embedding(voc_size+1,305, input_length=len(embedded_docs[0])),\n    Dropout(0.5),\n    Bidirectional(LSTM(200, return_sequences=True)),\n    Dropout(0.5),\n    GlobalMaxPool1D(),\n    Dropout(0.5),\n    Dense(3, activation='softmax')\n])\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","fb61010e":"len(embedded_docs),y.shape","255c71b8":"#Converting y to categorical with 3 features\ny = to_categorical(y,3)","40d8b1b6":"#Creating new independent and dependent variables\nimport numpy as np\nX_final=np.array(embedded_docs)\ny_final=np.array(y)","74c711f9":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X_final,y_final,test_size=0.3,random_state=0)","ee257379":"history=model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=5,batch_size=64)","744172c5":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","7382903f":"# Training the Model","19fa9026":"# Train Test Split","372644f4":"# Combining similar Sentiments","57a3fd0f":"# Visualizing unique Sentiments","408028d9":"# Encoding the dependant variable ","17520a61":"# Unique Values and Counts of Sentiments","f37b3968":"# Data Preprocessing","34f6f712":"# Importing NLP libraries","79b82988":"![image.png](attachment:ccaf1b90-dade-46f0-a108-e0c3622759b6.png)","e0ccbafe":"**Different Sentiments with values**\n\nExtremely Negative --> 0\n\nExtremely Postive --> 1\n\nNegative --> 2\n\nNeutral --> 3\n\nPositive --> 4","73b8b7e1":"# Model Creation","18385df2":"# Import Dataset","32cf39d1":"# Visualising the results"}}