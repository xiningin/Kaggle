{"cell_type":{"1fb08433":"code","03765df4":"code","df61c42d":"code","be56e52e":"code","6fcbe3b7":"code","fd2fd98e":"code","c8345468":"code","6cb7b8d1":"code","1ace0ec6":"code","3911cd13":"code","dbbdebd9":"code","78747ef0":"code","2a72ad67":"code","28ab88c0":"code","ba62401c":"code","f6f07f2d":"code","289cd013":"code","04a62e19":"code","a14e3aa6":"code","160459b0":"code","49ccced7":"code","3dadb658":"code","f6b0ed31":"code","eaa66bff":"code","3a6404e6":"code","50fc305b":"code","ea506f2f":"code","d0e140e4":"code","714d1db7":"code","0e732dd9":"code","76c5a62d":"code","29b25a5d":"code","642d28de":"code","2e60f812":"code","086c31a2":"code","c775d7b5":"code","c2c66beb":"code","3b02549b":"code","aaa7673a":"code","c02d02cd":"code","7d6f9608":"code","e315f283":"code","575732fc":"code","cc29d0ab":"code","3f4e1fdb":"code","ff979c88":"code","9cdfb17b":"code","a5d2ea74":"code","aef01d48":"code","3ce67faa":"code","e95bdec1":"code","d5714b06":"code","ec39d9fe":"code","ff43974f":"code","d3fb2cbb":"code","4c8c9ea1":"code","6fbc8daa":"code","befac1dc":"code","36dd60ca":"code","0351535f":"code","481e9d18":"code","dec0b3cd":"code","92d9d834":"code","c95cdea1":"code","eb88e775":"code","2008e253":"code","780e4c81":"code","e5521d01":"code","855193ba":"code","6db70c25":"code","ea0811d6":"code","63dd7d48":"code","837a95f2":"code","0db4d4dd":"code","6ed2b56e":"code","efe8a668":"code","ad4c547f":"code","cb755d6b":"code","308e82eb":"code","f76675e1":"code","d5bd9af7":"code","0e2194d4":"code","c9e22eeb":"code","6cb288fa":"code","9d522deb":"code","1ec8423d":"code","a289691c":"code","fb581d55":"code","c05d4285":"code","d5bd4005":"code","155c1d7f":"code","60617131":"code","9b64758f":"code","096941a4":"code","241a7c39":"code","d3656f64":"code","2b7eb2e7":"code","7be45a42":"code","5a6c8570":"code","bbdcb425":"code","dbfeecdd":"code","8e1b75ad":"code","c9576be4":"code","e22673e3":"code","1b7d74f5":"code","8ce797a1":"code","76644a55":"code","73580db0":"code","773905f5":"code","51281d17":"code","a2d13404":"code","db104378":"code","dd1029ba":"markdown","aa91d933":"markdown","d4088d7e":"markdown","5c46c1ba":"markdown","8c78dbe4":"markdown","c54243ca":"markdown","0723881a":"markdown","770657e3":"markdown","99c3036b":"markdown","3e248881":"markdown","5f6185e4":"markdown","bc5d9cea":"markdown","35fbfebe":"markdown","742e91eb":"markdown","27970d9a":"markdown","f14de8ea":"markdown","a46c70bd":"markdown","e83209f4":"markdown","de90450c":"markdown","e6960eee":"markdown","2df1674a":"markdown","8ac83488":"markdown","7c100d29":"markdown","680a5498":"markdown","60ae4a9b":"markdown","2d0fab95":"markdown","a9928799":"markdown","a2279255":"markdown","aaa7d33d":"markdown","5eb25d92":"markdown","7a7cd37f":"markdown","83f2643f":"markdown","d1e20534":"markdown","e9483998":"markdown","3275aed1":"markdown","634136f9":"markdown","e58e58f7":"markdown"},"source":{"1fb08433":"!pip3 install pyforest","03765df4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport scipy.stats as stats\n#this is used for automatically importing libraries\nimport pyforest\n\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder, StandardScaler, PowerTransformer, MinMaxScaler, LabelEncoder, RobustScaler\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold, KFold, cross_val_predict, train_test_split, GridSearchCV, cross_val_score, cross_validate\n\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge,ElasticNet\n\nfrom sklearn.metrics import plot_confusion_matrix, r2_score, mean_absolute_error, mean_squared_error, classification_report, confusion_matrix, accuracy_score, classification_report\n\nfrom sklearn.metrics import make_scorer, precision_score, precision_recall_curve, plot_precision_recall_curve, plot_roc_curve, roc_auc_score, roc_curve, f1_score, accuracy_score, recall_score\n\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.compose import make_column_transformer\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\nfrom sklearn.svm import SVR\n\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostClassifier\n\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, f_classif, f_regression, mutual_info_regression\n\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom xgboost import plot_importance\n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.tree import plot_tree\n\nfrom sklearn.impute import SimpleImputer, KNNImputer\n\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.svm import SVC\n\n\n\n#importing plotly and cufflinks in offline mode\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.warn(\"this will not show\")\nplt.rcParams[\"figure.figsize\"] = (10,6)\npd.set_option('max_colwidth',200)\n\n# pd.set_option('display.max_rows', 100) # if you wish to see more rows rather than default, just uncomment this line.\npd.set_option('display.max_columns', 200)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\nimport colorama\nfrom colorama import Fore, Style  # maakes strings colored\n# !pip3 install termcolor\nfrom termcolor import colored","df61c42d":"#Function for checking missing data\ndef missing_data (df):\n    missing_number = df.isnull().sum().sort_values(ascending=False)\n    missing_percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n    return missing_values","be56e52e":"# Function for insighting summary information about the column\n\ndef first_looking(col):\n    print(\"column name    : \", col)\n    print(\"--------------------------------\")\n    print(\"per_of_nulls   : \", \"%\", round(df[col].isnull().sum()\/df.shape[0]*100, 2))\n    print(\"num_of_nulls   : \", df[col].isnull().sum())\n    print(\"num_of_uniques : \", df[col].nunique())\n    print(df[col].value_counts(dropna = False))","6fcbe3b7":"# Function for examining scores\n#install from slearn\ndef train_val(y_train, y_train_pred, y_test, y_pred):\n    \n    scores = {\"train_set\": {\"Accuracy\" : accuracy_score(y_train, y_train_pred),\n                            \"Precision\" : precision_score(y_train, y_train_pred),\n                            \"Recall\" : recall_score(y_train, y_train_pred),                          \n                            \"f1\" : f1_score(y_train, y_train_pred)},\n    \n              \"test_set\": {\"Accuracy\" : accuracy_score(y_test, y_pred),\n                           \"Precision\" : precision_score(y_test, y_pred),\n                           \"Recall\" : recall_score(y_test, y_pred),                          \n                           \"f1\" : f1_score(y_test, y_pred)}}\n    \n    return pd.DataFrame(scores)","fd2fd98e":"df0 = pd.read_csv(\"..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")","c8345468":"df = df0.drop([\"id\"], axis =\"columns\")\ndf","6cb7b8d1":"df.head()","1ace0ec6":"df.tail()","3911cd13":"df.shape","dbbdebd9":"print(\"There is\", df.shape[0], \"observation and\", df.shape[1], \"columns in the dataset\")","78747ef0":"df.columns","2a72ad67":"df.info()","28ab88c0":"df.describe().T","ba62401c":"df.describe(include =\"object\").T","f6f07f2d":"df.nunique()","289cd013":"for col in df.select_dtypes(include=[np.number]).columns:\n  print(f\"{col} has {df[col].nunique()} unique value\")","04a62e19":"#check duplicate values \ndf.duplicated().value_counts()","a14e3aa6":"#Use define function to check missing data \nmissing_data(df)","160459b0":"first_looking(\"stroke\")","49ccced7":"#plot the target variable\nsns.countplot(x=\"stroke\", data=df)\nplt.legend([\"No\",\"Yes\"])\nplt.title(\"Stroke\")","3dadb658":"#Gender show bar plot\nsns.set(style='whitegrid')\nax=sns.barplot(x=df['stroke'].value_counts().index,y=df['stroke'].value_counts().values,palette=\"Blues_d\",hue=[0,1])\nplt.legend(loc=8)\nplt.xlabel('Stroke')\nplt.ylabel('Counts')\nplt.title('Show of Stroke Bar Plot')\nplt.show()","f6b0ed31":"df['stroke'].iplot(kind='hist')","eaa66bff":"print(df[\"stroke\"].value_counts())\ndf[\"stroke\"].value_counts().plot(kind=\"pie\", autopct='%1.1f%%', figsize=(7,7));","3a6404e6":"#To make things more clear because looking at the pie chart gives us the understanding\ny = df['stroke']\nprint(f'Percentage of stroke: % {round(y.value_counts(normalize=True)[1]*100,2)} --> \\\n({y.value_counts()[1]} cases for stroke)\\nPercentage of NOT stroke: % {round(y.value_counts(normalize=True)[0]*100,2)} --> ({y.value_counts()[0]} cases for NOT stroke)')","50fc305b":"#Describe >> create a table >> background to make color\ndf[df['stroke']==0].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu')","ea506f2f":"df[df['stroke']==1].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu')","d0e140e4":"#skewness refers to the geometry shape : negative and positive skewness\nprint( f\"Skewness: {df['stroke'].skew()}\")\n#K = 0 >> the shape is normal >> center at the middle \n#K < 0 the shape is flatten >> highly dispersed\n#k > 0 the shape is peaks sharply\nprint( f\"Kurtosis: {df['stroke'].kurtosis()}\")","714d1db7":"#Since we have just examined the target variable, we can drop to make our data cleaner\nnumerical= df.drop(['stroke'], axis=1).select_dtypes('number').columns\n\ncategorical = df.select_dtypes('object').columns\n\nprint(f'Numerical Columns:  {df[numerical].columns}')\nprint('\\n')\nprint(f'Categorical Columns: {df[categorical].columns}')","0e732dd9":"for col in numerical:\n  print(f\"{col} has {df[col].nunique()} unique value\")","76c5a62d":"df[numerical].head().T","29b25a5d":"df[numerical].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu')","642d28de":"df[numerical].iplot(kind='hist');","2e60f812":"df[numerical].iplot(kind='histogram', subplots=True,bins=50)","086c31a2":"for i in numerical:\n    df[i].iplot(kind=\"box\", title=i, boxpoints=\"all\", color='lightseagreen')","c775d7b5":"index = 0\nplt.figure(figsize=(20,20))\nfor feature in numerical:\n    if feature != \"stroke\":\n        index += 1\n        plt.subplot(4, 3, index)\n        sns.boxplot(x='stroke', y=feature, data=df)","c2c66beb":"sns.pairplot(df, hue=\"stroke\", palette=\"inferno\", corner=True);","3b02549b":"skew_vals = df.skew().sort_values(ascending=False)\nskew_vals","aaa7673a":"skew_limit = 0.5 # This is our threshold-limit to evaluate skewness. Overall below abs(1) seems acceptable for the linear models. \nskew_vals = df.skew()\nskew_cols = skew_vals[abs(skew_vals)> skew_limit].sort_values(ascending=False)\nskew_cols ","c02d02cd":"\nfor skew in skew_vals:\n    if -0.5 < skew < 0.5:\n        print (\"A skewness value of\", '\\033[1m', Fore.GREEN, skew, '\\033[0m', \"means that the distribution is approx.\", '\\033[1m', Fore.GREEN, \"symmetric\", '\\033[0m')\n    elif  -0.5 < skew < -1.0 or 0.5 < skew < 1.0:\n        print (\"A skewness value of\", '\\033[1m', Fore.YELLOW, skew, '\\033[0m', \"means that the distribution is approx.\", '\\033[1m', Fore.YELLOW, \"moderately skewed\", '\\033[0m')\n    else:\n        print (\"A skewness value of\", '\\033[1m', Fore.RED, skew, '\\033[0m', \"means that the distribution is approx.\", '\\033[1m', Fore.RED, \"highly skewed\", '\\033[0m')","7d6f9608":"kurtosis_vals = df.kurtosis().sort_values(ascending=False)\nkurtosis_vals","e315f283":"#Calculating Kurtosis \n\nkurtosis_limit = 7 # This is our threshold-limit to evaluate skewness. Overall below abs(1) seems acceptable for the linear models.\nkurtosis_vals = df.kurtosis()\nkurtosis_cols = kurtosis_vals[abs(kurtosis_vals) > kurtosis_limit].sort_values(ascending=False)\nkurtosis_cols","575732fc":"plt.figure(figsize=(12, 8))\nsns.heatmap(df.corr(), annot=True)\nplt.xticks(rotation=45);","cc29d0ab":"df_temp = df.corr()\n\ncount = \"Done\"\nfeature =[]\ncollinear=[]\nfor col in df_temp.columns:\n    for i in df_temp.index:\n        if (df_temp[col][i]> .9 and df_temp[col][i] < 1) or (df_temp[col][i]< -.9 and df_temp[col][i] > -1) :\n                feature.append(col)\n                collinear.append(i)\n                print(Fore.RED + f\"\\033[1mmulticolinearity alert in between\\033[0m {col} - {i}\")\n        else:\n            print(f\"For {col} and {i}, there is NO multicollinearity problem\") \n\nprint(\"\\033[1mThe number of strong corelated features:\\033[0m\", count) ","3f4e1fdb":"for col in categorical:\n  print(f\"{col} has {df[col].nunique()} unique value\")","ff979c88":"df[categorical].head().T","9cdfb17b":"df[categorical].describe().T","a5d2ea74":"for i in categorical:\n    df[i].iplot(kind=\"box\", title=i, boxpoints=\"all\", color='lightseagreen')","aef01d48":"df[categorical].iplot(kind='hist');","3ce67faa":"df[categorical].iplot(kind='histogram',subplots=True,bins=50)","e95bdec1":"index = 0\nplt.figure(figsize=(20,20))\nfor feature in categorical:\n    if feature != \"stroke\":\n        index += 1\n        plt.subplot(4, 3, index)\n        sns.countplot(data = df, x =feature, hue = \"stroke\")","d5714b06":"#Continue to EDA life !!!\n#Describe three variables","ec39d9fe":"#scatter\nsns.scatterplot(data=df, x='age', y='avg_glucose_level', hue='stroke', alpha=0.7)\n","ff43974f":"fig = px.scatter(df, x='age', y='avg_glucose_level', color='stroke', opacity=0.5)\nfig.show()","d3fb2cbb":"#Histogram\nfig = px.histogram(df, x=\"age\", y=\"avg_glucose_level\", color=\"gender\")\nfig.show()","4c8c9ea1":"fig = px.histogram(df, x=\"age\", y=\"avg_glucose_level\", color=\"stroke\", marginal=\"box\")\nfig.show()","6fbc8daa":"\nfig, ax = plt.subplots(1, 2, figsize=(28, 6))\n\n# boxplot\nsns.boxplot(x='stroke', y='age', data=df, ax=ax[0])\n\n# violinplot\nsns.violinplot(x='stroke', y='age', data=df, ax=ax[1])\n\nplt.show()","befac1dc":"# type 2 : hue parameter\nfig, ax = plt.subplots(1, 2, figsize=(18, 6))\nsns.violinplot(x='gender', y='age', hue='stroke', data=df, ax=ax[0])\n\n\n# type 3 : hue + split\nsns.violinplot(x='gender', y='age', hue='stroke', data=df, split=True, ax=ax[1])\nplt.show()","36dd60ca":"# type 2 : color add violin\nfig = px.violin(df, x='gender', y='age', \n                color='stroke'\n            )\nfig.show()","0351535f":"#joint plot \n# type 2 : many types (reg, hex, kde)\nsns.jointplot(data=df, x='age', y='avg_glucose_level', kind='reg', color='skyblue')\nsns.jointplot(data=df, x='age', y='avg_glucose_level', kind='hex', color='gold')\nsns.jointplot(data=df, x='age', y='avg_glucose_level', kind='kde', color='forestgreen' )\nplt.show()","481e9d18":"#type4 : marginal_x & marginal_y\nfig = px.scatter(df, x='age', y='avg_glucose_level', marginal_y=\"rug\", marginal_x=\"histogram\")\nfig.show()","dec0b3cd":"dfn = df.copy()\ndfn['total'] = 1\nfig = px.sunburst(dfn, path=[\"smoking_status\",'work_type'], values='total')\nfig.show()","92d9d834":"df.head()","c95cdea1":"df.info()","eb88e775":"df['bmi'].fillna(value = round(df['bmi'].mean(),1), inplace = True)","2008e253":"df['gender'].replace(\"Other\", \"Female\", inplace = True)","780e4c81":"df['bmi'].replace([71.9,78.0,97.6,92.0], round(df['bmi'].mean(),1), inplace = True)","e5521d01":"df['work_type'].value_counts()","855193ba":"df['work_type'].replace(\"Never_worked\",\"Govt_job\", inplace = True)","6db70c25":"df.shape","ea0811d6":"df.sample(10)","63dd7d48":"df = pd.get_dummies(df, drop_first=True)","837a95f2":"df.shape","0db4d4dd":"df.head()","6ed2b56e":"X = df.drop([\"stroke\"], axis=1)\ny = df[\"stroke\"]","efe8a668":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(sampling_strategy='minority')\nX_sm, y_sm = sm.fit_resample(X, y)\n\ny_sm.value_counts()","ad4c547f":"X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.2, stratify = y_sm, random_state = 101)","cb755d6b":"scaler = MinMaxScaler()\nscaler","308e82eb":"X_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","f76675e1":"# General Insights\n\ndef model_first_insight(X_train, y_train, class_weight, solver='liblinear'):\n    # Logistic Regression\n    log = LogisticRegression(random_state=101, class_weight=class_weight)\n    log.fit(X_train, y_train)\n    \n    # Decision Tree\n    decision_tree = DecisionTreeClassifier(criterion = 'entropy', random_state=101, class_weight=class_weight)\n    decision_tree.fit(X_train, y_train)\n   \n    # Random Forest\n    random_forest = RandomForestClassifier(n_estimators=10, criterion = 'entropy', random_state=101, class_weight=class_weight)\n    random_forest.fit(X_train, y_train)\n    \n    # KNN\n    knn = KNeighborsClassifier(n_neighbors = 3)\n    knn.fit(X_train, y_train) \n   \n    # SVC\n    svc = SVC(random_state=101, class_weight=class_weight)\n    svc.fit(X_train, y_train) \n    \n    # XGB\n    xgb = XGBClassifier(random_state=101, class_weight=class_weight)\n    xgb.fit(X_train, y_train)\n    \n    # AdaBoosting\n    ab = AdaBoostClassifier(n_estimators=50, random_state=101)\n    ab.fit(X_train, y_train)\n    \n    # GB GradientBoosting\n    gb = GradientBoostingClassifier(random_state=101)\n    gb.fit(X_train, y_train)\n    \n    # Model Accuracy on Training Data\n    print(f\"\\033[1m1) Logistic Regression Training Accuracy:\\033[0m {log.score(X_train, y_train)}\")\n    print(f\"\\033[1m2) SVC Training Accuracy:\\033[0m {svc.score(X_train, y_train)}\")    \n    print(f\"\\033[1m3) Decision Tree Training Accuracy:\\033[0m {decision_tree.score(X_train, y_train)}\")\n    print(f\"\\033[1m4) Random Forest Training Accuracy:\\033[0m {random_forest.score(X_train, y_train)}\")\n    print(f\"\\033[1m5) KNN Training Accuracy:\\033[0m {knn.score(X_train, y_train)}\")\n    print(f\"\\033[1m6) GradiendBoosting Training Accuracy:\\033[0m {gb.score(X_train, y_train)}\")\n    print(f\"\\033[1m7) AdaBoosting Training Accuracy:\\033[0m {ab.score(X_train, y_train)}\")\n    print(f\"\\033[1m8) XGBoosting Training Accuracy:\\033[0m {xgb.score(X_train, y_train)}\")\n    \n    return log, svc, decision_tree, random_forest, knn, gb, ab, xgb","d5bd9af7":"def models(X_train, y_train, class_weight):\n    \n    # Logistic Regression\n    log = LogisticRegression(random_state=101, class_weight=class_weight, solver='liblinear')\n    log.fit(X_train, y_train)\n    \n    # Decision Tree\n    decision_tree = DecisionTreeClassifier(criterion = 'entropy', random_state=101, class_weight=class_weight)\n    decision_tree.fit(X_train, y_train)\n    \n    # Random Forest\n    random_forest = RandomForestClassifier(n_estimators=10, criterion = 'entropy', random_state=101, class_weight=class_weight)\n    random_forest.fit(X_train, y_train)\n    # KNN\n    knn = KNeighborsClassifier(n_neighbors = 3)\n    knn.fit(X_train, y_train) \n   \n    # SVC\n    svc = SVC(random_state=101, class_weight=class_weight)\n    svc.fit(X_train, y_train) \n    \n    # XGB\n    xgb = XGBClassifier(random_state=101, class_weight=class_weight)\n    xgb.fit(X_train, y_train)\n    \n    # AdaBoosting\n    ab = AdaBoostClassifier(n_estimators=50, random_state=101)\n    ab.fit(X_train, y_train)\n    \n    # GB GradientBoosting\n    gb = GradientBoostingClassifier(random_state=101)\n    gb.fit(X_train, y_train)\n    \n    # Model Accuracy on Training Data\n    print(f\"\\033[1m1) Logistic Regression Training Accuracy:\\033[0m {log}\")\n    print(f\"\\033[1m2) SVC Training Accuracy:\\033[0m {svc}\")    \n    print(f\"\\033[1m3) Decision Tree Training Accuracy:\\033[0m {decision_tree}\")\n    print(f\"\\033[1m4) Random Forest Training Accuracy:\\033[0m {random_forest}\")\n    print(f\"\\033[1m5) KNN Training Accuracy:\\033[0m {knn}\")\n    print(f\"\\033[1m6) GradiendBoosting Training Accuracy:\\033[0m {gb}\")\n    print(f\"\\033[1m7) AdaBoosting Training Accuracy:\\033[0m {ab}\")\n    print(f\"\\033[1m8) XGBoosting Training Accuracy:\\033[0m {xgb}\")\n  \n    return log.score(X_train, y_train), svc.score(X_train, y_train),decision_tree.score(X_train, y_train),random_forest.score(X_train, y_train),knn.score(X_train, y_train),gb.score(X_train, y_train),ab.score(X_train, y_train),xgb.score(X_train, y_train)","0e2194d4":"def models_accuracy(X_Set, y_Set):    \n    Scores = pd.DataFrame(columns = [\"LR_Acc\", \"SVC_Acc\", \"DT_Acc\", \"RF_Acc\", \"KNN_Acc\", \"GB_Acc\", \"AB_Acc\", \"XGB_Acc\"])\n\n    print(\"\\033[1mBASIC ACCURACY\\033[0m\")\n    Basic = [log_acc, svc_acc, decision_tree_acc, random_forest_acc, knn_acc, gb_acc, ab_acc, xgb_acc] = models(X_train, y_train, None)\n    Scores.loc[0] = Basic\n\n    print(\"\\n\\033[1mSCALED ACCURACY WITHOUT BALANCED\\033[0m\")    \n    Scaled = [log_acc, svc_acc, decision_tree_acc, random_forest_acc, knn_acc, gb_acc, ab_acc, xgb_acc] = models(X_train_scaled, y_train, None)\n    Scores.loc[1] = Scaled\n\n    \n    print(\"\\n\\033[1mBASIC ACCURACY WITH BALANCED\\033[0m\")\n    Balanced = [log_acc, svc_acc, decision_tree_acc, random_forest_acc, knn_acc, gb_acc, ab_acc, xgb_acc] = models(X_train, y_train, \"balanced\")\n    Scores.loc[2] = Balanced\n\n    print(\"\\n\\033[1mSCALED ACCURACY WITH BALANCED\\033[0m\")    \n    Scaled_Balanced = [log_acc, svc_acc, decision_tree_acc, random_forest_acc, knn_acc, gb_acc, ab_acc, xgb_acc] = models(X_train_scaled, y_train, \"balanced\")\n    Scores.loc[3] = Scaled_Balanced\n\n    Scores.set_axis(['Basic', 'Scaled', 'Balanced', 'Scaled_Balanced'], axis='index', inplace=True)\n    #Scores.style.background_gradient(cmap='RdPu')\n\n    return Scores.style.applymap(lambda x: \"background-color: pink\" if x<0.6 or x == 1 else \"background-color: lightgreen\")\\\n                       .applymap(lambda x: 'opacity: 40%;' if (x < 0.8) else None)\\\n                       .applymap(lambda x: 'color: red' if x == 1 or x <=0.8 else 'color: darkblue')","c9e22eeb":"models_accuracy(X_train, y_train)","6cb288fa":"Scores = pd.DataFrame(columns = [\"LR_Acc\", \"SVC_Acc\", \"DT_Acc\", \"RF_Acc\", \"KNN_Acc\", \"GB_Acc\", \"AB_Acc\", \"XGB_Acc\"])\n\nprint(\"\\033[1mBASIC ACCURACY\\033[0m\")\nBasic = [log_acc, svc_acc, decision_tree_acc, random_forest_acc, knn_acc, gb_acc, ab_acc, xgb_acc] = models(X_train, y_train, None)\nScores.loc[0] = Basic\n\nprint(\"\\n\\033[1mSCALED ACCURACY WITHOUT BALANCED\\033[0m\")    \nScaled = [log_acc, svc_acc, decision_tree_acc, random_forest_acc, knn_acc, gb_acc, ab_acc, xgb_acc] = models(X_train_scaled, y_train, None)\nScores.loc[1] = Scaled\n\nprint(\"\\n\\033[1mBASIC ACCURACY WITH BALANCED\\033[0m\")\nBalanced = [log_acc, svc_acc, decision_tree_acc, random_forest_acc, knn_acc, gb_acc, ab_acc, xgb_acc] = models(X_train, y_train, \"balanced\")\nScores.loc[2] = Balanced\n\nprint(\"\\n\\033[1mSCALED ACCURACY WITH BALANCED\\033[0m\")    \nScaled_Balanced = [log_acc, svc_acc, decision_tree_acc, random_forest_acc, knn_acc, gb_acc, ab_acc, xgb_acc] = models(X_train_scaled, y_train, \"balanced\")\nScores.loc[3] = Scaled_Balanced\n\nScores.set_axis(['Basic', 'Scaled', 'Balanced', 'Scaled_Balanced'], axis='index', inplace=True)","9d522deb":"accuracy_scores = Scores.style.applymap(lambda x: \"background-color: pink\" if x<0.6 or x == 1 else \"background-color: lightgreen\")\\\n                              .applymap(lambda x: 'opacity: 40%;' if (x < 0.8) else None)\\\n                              .applymap(lambda x: 'color: red' if x == 1 or x <=0.8 else 'color: darkblue')\n\naccuracy_scores","1ec8423d":"accuracy_scores","a289691c":"operations = [(\"scaler\", MinMaxScaler()), (\"power\", PowerTransformer()), (\"log\", LogisticRegression(random_state=101))]","fb581d55":"# Defining the pipeline object for LogisticClassifier\n\npipe_log_model = Pipeline(steps=operations)","c05d4285":"pipe_log_model.get_params()","d5bd4005":"pipe_log_model.fit(X_train, y_train)\ny_pred = pipe_log_model.predict(X_test)\ny_train_pred = pipe_log_model.predict(X_train)","155c1d7f":"print(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","60617131":"pipe_scores = cross_validate(pipe_log_model, X_train, y_train, scoring = ['accuracy', 'precision','recall','f1'], cv = 10)\ndf_pipe_scores = pd.DataFrame(pipe_scores, index = range(1, 11))\n\ndf_pipe_scores","9b64758f":"df_pipe_scores.mean()[2:]","096941a4":"# evaluate the pipeline\n\n# from sklearn.model_selection import RepeatedStratifiedKFold\n\ncv = RepeatedStratifiedKFold(n_splits=10, random_state=101)\nn_scores = cross_val_score(pipe_log_model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n\nprint(f'Accuracy: Results Mean : %{round(n_scores.mean()*100,3)}, Results Standard Deviation : {round(n_scores.std()*100,3)}')","241a7c39":"print('Accuracy: %.3f (%.3f)' % (n_scores.mean(), n_scores.std()))","d3656f64":"accuracy_scores","2b7eb2e7":"LR_model = LogisticRegression() # Since Basic accuracy outcome gives the best model accuracy results, we will implement it \nLR_model.fit(X_train_scaled, y_train)\ny_pred = LR_model.predict(X_test_scaled)\ny_train_pred = LR_model.predict(X_train_scaled)\n\nlog_f1 = f1_score(y_test, y_pred)\nlog_acc = accuracy_score(y_test, y_pred)\nlog_recall = recall_score(y_test, y_pred)\nlog_auc = roc_auc_score(y_test, y_pred)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\n\nplot_confusion_matrix(LR_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)\n#Precision refers to prediction\n#Recall refers to true value!","7be45a42":"y_pred_proba = LR_model.predict_proba(X_test_scaled)\ny_pred_proba","5a6c8570":"test_data = pd.concat([X_test.set_index(y_test.index), y_test], axis=1)\ntest_data[\"pred\"] = y_pred\ntest_data[\"pred_proba\"] = y_pred_proba[:, 1]\ntest_data.sample(10)","bbdcb425":"#Train each folder!\nlog_xvalid_model = LogisticRegression()\n\nlog_xvalid_model_scores = cross_validate(log_xvalid_model, X_train_scaled, y_train, scoring = ['accuracy', 'precision','recall',\n                                                                          'f1'], cv = 10)\nlog_xvalid_model_scores = pd.DataFrame(log_xvalid_model_scores, index = range(1, 11))\n\nlog_xvalid_model_scores","dbfeecdd":"log_xvalid_model_scores.mean()[2:]","8e1b75ad":"#This is used to find the best parameters!\npenalty = [\"l1\", \"l2\", \"elasticnet\"]\nl1_ratio = np.linspace(0, 1, 20)\nC = np.logspace(0, 10, 20)\n\nparam_grid = {\"penalty\" : penalty,\n             \"l1_ratio\" : l1_ratio,\n             \"C\" : C}","c9576be4":"LR_grid_model = LogisticRegression(solver='saga', max_iter=5000, class_weight = \"balanced\")\n\nLR_grid_model = GridSearchCV(LR_grid_model, param_grid = param_grid)","e22673e3":"LR_grid_model.fit(X_train_scaled, y_train)","1b7d74f5":"print(colored('\\033[1mBest Parameters of GridSearchCV for LR Model:\\033[0m', 'blue'), colored(LR_grid_model.best_params_, 'cyan'))\nprint(\"--------------------------------------------------------------------------------------------------------------------\")\nprint(colored('\\033[1mBest Estimator of GridSearchCV for LR Model:\\033[0m', 'blue'), colored(LR_grid_model.best_estimator_, 'cyan'))","8ce797a1":"y_pred = LR_grid_model.predict(X_test_scaled)\ny_train_pred = LR_grid_model.predict(X_train_scaled)\n\nlog_grid_f1 = f1_score(y_test, y_pred)\nlog_grid_acc = accuracy_score(y_test, y_pred)\nlog_grid_recall = recall_score(y_test, y_pred)\nlog_grid_auc = roc_auc_score(y_test, y_pred)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\nprint(classification_report(y_test, y_pred))\nprint(\"\\033[1m--------------------------------------------------------\\033[0m\")\n\nplot_confusion_matrix(LR_grid_model, X_test_scaled, y_test)\n\ntrain_val(y_train, y_train_pred, y_test, y_pred)","76644a55":"plot_roc_curve(LR_model, X_test_scaled, y_test, response_method='auto');","73580db0":"plot_precision_recall_curve(LR_model, X_test_scaled, y_test);","773905f5":"fp_rate, tp_rate, thresholds = roc_curve(y_test, y_pred_proba[:, 1])","51281d17":"optimal_idx = np.argmax(tp_rate - fp_rate)\noptimal_threshold = thresholds[optimal_idx]\noptimal_threshold","a2d13404":"roc_curve = {\"fp_rate\":fp_rate, \"tp_rate\":tp_rate, \"thresholds\":thresholds}\ndf_roc_curve = pd.DataFrame(roc_curve)\ndf_roc_curve","db104378":"df_roc_curve.iloc[optimal_idx]","dd1029ba":"**4.5 - The Examination of Categorical Features**","aa91d933":"# 5) TRAIN | TEST SPLIT & HANDLING IMBALANCED DATA","d4088d7e":"# 6) FEATURE SCALING","5c46c1ba":"* 1) id: unique identifier\n* 2) gender: \"Male\", \"Female\" or \"Other\"\n* 3) age: age of the patient\n* 4) hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n* 5) heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n* 6) ever_married: \"No\" or \"Yes\"\n* 7) work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n* 8) Residence_type: \"Rural\" or \"Urban\"\n* 9) avg_glucose_level: average glucose level in blood\n* 10) bmi: body mass index\n* 11) smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n* 12) stroke: 1 if the patient had a stroke or 0 if not\n* *Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient","8c78dbe4":"# 4) EXPLORATORY DATA ANALYSIS (EDA) & VISUALIZATION","c54243ca":"**4.4 - The Examination of Skewness & Kurtosis**","0723881a":"**4.6 - Dummy Variables Operation and handling missing data**","770657e3":"# 7) MODELLING & MODEL PERFORMANCE","99c3036b":"**6.3 Handling with Skewness with PowerTransform & Checking Model Accuracy Scores**","3e248881":"**3.1 Reading the Data**","5f6185e4":"*  Feature scaling (Normalization) is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.\n* \n* For machine learning, in general, it is necessary to normalize features so that no features are arbitrarily large (centering) and all features are on the same scale (scaling).\n* \n* In general, algorithms that exploit distances or similarities (e.g. in the form of scalar product) between data samples, such as K-NN and SVM, are sensitive to feature transformations. So it is generally useful, when you are solving a system of equations, least squares, etc, where you can have serious issues due to rounding errors.\n* \n* However, Graphical-model based classifiers, such as Fisher LDA or Naive Bayes, as well as Decision trees and Tree-based ensemble methods (RF, XGB) are invariant to feature scaling, but still, it might be a good idea to rescale\/standardize your data.\n* \n* NOTE: XGBoost actually implements a second algorithm too, based on linear boosting. Scaling will make a difference there","bc5d9cea":"**4.2 - The Examination of Target Variable**","35fbfebe":"**2.3 What the Problem is**","742e91eb":"**4.1 - A General Looking at the Data**","27970d9a":"* In the given study, we have a binary classification problem.\n* We will make a prection on the target variable - Strokes\n* Lastly we will build a variety of Classification models and compare the models giving the best prediction on strokes","f14de8ea":"# 3) ANALYSIS","a46c70bd":"# 1) LIBRARIES NEEDED IN THE STUDY","e83209f4":"**4.3 - The Examination of Numerical Features**","de90450c":"* We are now ready to train our models.\n* \n* After determining related Classifiers from the scikit-learn framework, we can create and and fit them to our training dataset. Models are fit using the scikit-learn API and the model.fit() function.\n* \n* Then we can make predictions using the fit model on the test dataset. To make predictions we use the scikit-learn function model.predict().","e6960eee":"**1.1 User Defined Functions**","2df1674a":"**5.2 Train | Test Split**","8ac83488":"# 2) Data","7c100d29":"> SPECIAL NOTE: When we examine the results after handling with skewness, it's clear to assume that handling with skewness could NOT make any contribution to our model when comparing the results obtained by LogisticClassifier without using PowerTransform. So, for the next steps in this study, we will continue not handling with skewness assuming that it's useless for the results.","680a5498":"According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.\nThis dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about the patient.","60ae4a9b":"**7.1.b Cross-Validating Logistic Regression (LR) Model**","2d0fab95":"**2.2 About the Features**","a9928799":"**7.1.e The Determination of The Optimal Treshold**","a2279255":"*First let's take a close look at the models' accuracy scores for comparing the results given by Scaled, Not Scaled, Balanced and Not Balanced models.*","aaa7d33d":"**6.2 General Insights Before Going Further**","5eb25d92":"**2.1 Context**\n   ","7a7cd37f":"**5.1 Handling imbalance data**","83f2643f":"*Let's look at the best parameters & estimator found by GridSearchCV.*","d1e20534":"**7.1.a Modelling Logistic Regression (LR) with Default Parameters**","e9483998":"**6.1 The Implementation of Scaling**","3275aed1":"**7.1.d ROC (Receiver Operating Curve) and AUC (Area Under Curve)**","634136f9":"**7.1.c Modelling Logistic Regression (LR) with Best Parameters Using GridSeachCV**","e58e58f7":"***7.1 The Implementation of Logistic Regression (LR)***"}}