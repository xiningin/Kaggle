{"cell_type":{"bd06f454":"code","358de457":"code","a6fd6eb7":"code","0b269513":"code","7bac552c":"code","6ab7533c":"code","0e784215":"code","207e007e":"code","f370fdf0":"code","76576b31":"code","c3fdf95a":"code","f3f50734":"code","e037056e":"code","d78558df":"code","79ebb5f9":"code","a2ab2896":"code","f64e6d1e":"code","e1574539":"code","a7e93a0c":"code","67847a0d":"code","77804f72":"code","003199a2":"code","e83c041c":"code","92b8a418":"code","4096a5fd":"code","e64d5433":"code","eaac58d9":"code","3dae23cd":"code","fc09e8d3":"code","3e5c8b1e":"code","68cb9e71":"code","90a44808":"code","72e25bc3":"code","ecad4607":"code","2e6eb9d5":"code","01056379":"code","c03233e6":"code","c1ddeb08":"code","67f0915c":"code","86059274":"code","b93941e7":"code","f903ce7f":"code","6449994b":"markdown","5147d364":"markdown","065edd87":"markdown","80cdb4e3":"markdown","88eddaba":"markdown","b66d8550":"markdown","58533d7b":"markdown","b836df66":"markdown","bd7e76ca":"markdown","19e3220d":"markdown","688baf0a":"markdown","9df1b457":"markdown"},"source":{"bd06f454":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nimport datetime\nfrom kaggle.competitions import nflrush\nimport tqdm\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nimport keras\n\nfrom keras.callbacks import EarlyStopping\nfrom keras import backend as K\nfrom collections import Counter\nfrom sklearn.model_selection import RepeatedKFold\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Input, Dense, Embedding, Concatenate, Flatten, BatchNormalization, Dropout\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.utils.vis_utils import plot_model\nfrom keras import metrics\n\nsns.set_style('darkgrid')\nmpl.rcParams['figure.figsize'] = [15,10]","358de457":"train = pd.read_csv('..\/input\/nfl-big-data-bowl-2020\/train.csv', dtype={'WindSpeed': 'object'})","a6fd6eb7":"off_form = train['OffenseFormation'].unique()\ntrain['OffenseFormation'].value_counts()","0b269513":"train = pd.concat([train.drop(['OffenseFormation'], axis=1), pd.get_dummies(train['OffenseFormation'], prefix='Formation')], axis=1)\ndummy_col = train.columns","7bac552c":"def strtoseconds(txt):\n    txt = txt.split(':')\n    ans = int(txt[0])*60 + int(txt[1]) + int(txt[2])\/60\n    return ans","6ab7533c":"train['GameClock'] = train['GameClock'].apply(strtoseconds)","0e784215":"train['PlayerHeight'] = train['PlayerHeight'].apply(lambda x: 12*int(x.split('-')[0])+int(x.split('-')[1]))","207e007e":"train['TimeHandoff'] = train['TimeHandoff'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\ntrain['TimeSnap'] = train['TimeSnap'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))","f370fdf0":"train['TimeDelta'] = train.apply(lambda row: (row['TimeHandoff'] - row['TimeSnap']).total_seconds(), axis=1)","76576b31":"train['PlayerBirthDate'] = train['PlayerBirthDate'].apply(lambda x: datetime.datetime.strptime(x, \"%m\/%d\/%Y\"))","c3fdf95a":"seconds_in_year = 60*60*24*365.25\ntrain['PlayerAge'] = train.apply(lambda row: (row['TimeHandoff']-row['PlayerBirthDate']).total_seconds()\/seconds_in_year, axis=1)","f3f50734":"train = train.drop(['TimeHandoff', 'TimeSnap', 'PlayerBirthDate'], axis=1)","e037056e":"train['WindSpeed'] = train['WindSpeed'].apply(lambda x: x.lower().replace('mph', '').strip() if not pd.isna(x) else x)","d78558df":"train['WindSpeed'].value_counts()","79ebb5f9":"#let's replace the ones that has x-y by (x+y)\/2\n# and also the ones with x gusts up to y\ntrain['WindSpeed'] = train['WindSpeed'].apply(lambda x: (int(x.split('-')[0])+int(x.split('-')[1]))\/2 if not pd.isna(x) and '-' in x else x)\ntrain['WindSpeed'] = train['WindSpeed'].apply(lambda x: (int(x.split()[0])+int(x.split()[-1]))\/2 if not pd.isna(x) and type(x)!=float and 'gusts up to' in x else x)","a2ab2896":"def str_to_float(txt):\n    try:\n        return float(txt)\n    except:\n        return -1","f64e6d1e":"train['WindSpeed'] = train['WindSpeed'].apply(str_to_float)","e1574539":"train.drop('WindDirection', axis=1, inplace=True)","a7e93a0c":"train['PlayDirection'] = train['PlayDirection'].apply(lambda x: x is 'right')","67847a0d":"train['Team'] = train['Team'].apply(lambda x: x.strip()=='home')","77804f72":"train['GameWeather'] = train['GameWeather'].str.lower()\nindoor = \"indoor\"\ntrain['GameWeather'] = train['GameWeather'].apply(lambda x: indoor if not pd.isna(x) and indoor in x else x)\ntrain['GameWeather'] = train['GameWeather'].apply(lambda x: x.replace('coudy', 'cloudy').replace('clouidy', 'cloudy').replace('party', 'partly') if not pd.isna(x) else x)\ntrain['GameWeather'] = train['GameWeather'].apply(lambda x: x.replace('clear and sunny', 'sunny and clear') if not pd.isna(x) else x)\ntrain['GameWeather'] = train['GameWeather'].apply(lambda x: x.replace('skies', '').replace(\"mostly\", \"\").strip() if not pd.isna(x) else x)","003199a2":"weather_count = Counter()\nfor weather in train['GameWeather']:\n    if pd.isna(weather):\n        continue\n    for word in weather.split():\n        weather_count[word]+=1\n        \nweather_count.most_common()[:15]","e83c041c":"def map_weather(txt):\n    ans = 1\n    if pd.isna(txt):\n        return 0\n    if 'partly' in txt:\n        ans*=0.5\n    if 'climate controlled' in txt or 'indoor' in txt:\n        return ans*3\n    if 'sunny' in txt or 'sun' in txt:\n        return ans*2\n    if 'clear' in txt:\n        return ans\n    if 'cloudy' in txt:\n        return -ans\n    if 'rain' in txt or 'rainy' in txt:\n        return -2*ans\n    if 'snow' in txt:\n        return -3*ans\n    return 0","92b8a418":"train['GameWeather'] = train['GameWeather'].apply(map_weather)","4096a5fd":"train['IsRusher'] = train['NflId'] == train['NflIdRusher']\ntrain.drop(['NflId', 'NflIdRusher'], axis=1, inplace=True)","e64d5433":"train = train.sort_values(by=['PlayId', 'Team', 'IsRusher']).reset_index()","eaac58d9":"train.drop(['GameId', 'PlayId', 'index', 'IsRusher', 'Team'], axis=1, inplace=True)","3dae23cd":"cat_features = []\nfor col in train.columns:\n    if train[col].dtype =='object':\n        cat_features.append(col)\n        \ntrain = train.drop(cat_features, axis=1)","fc09e8d3":"train.fillna(-999, inplace=True)","3e5c8b1e":"players_col = []\nfor col in train.columns:\n    if train[col][:22].std()!=0:\n        players_col.append(col)","68cb9e71":"X_train = np.array(train[players_col]).reshape(-1, 11*22)","90a44808":"play_col = train.drop(players_col+['Yards'], axis=1).columns\nX_play_col = np.zeros(shape=(X_train.shape[0], len(play_col)))\nfor i, col in enumerate(play_col):\n    X_play_col[:, i] = train[col][::22]","72e25bc3":"X_train = np.concatenate([X_train, X_play_col], axis=1)\ny_train = np.zeros(shape=(X_train.shape[0], 199))\nfor i,yard in enumerate(train['Yards'][::22]):\n    y_train[i, yard+99:] = np.ones(shape=(1, 100-yard))","ecad4607":"__all__ = ['RAdam']\n\nclass RAdam(keras.optimizers.Optimizer):\n    \"\"\"RAdam optimizer.\n    # Arguments\n        learning_rate: float >= 0. Learning rate.\n        beta_1: float, 0 < beta < 1. Generally close to 1.\n        beta_2: float, 0 < beta < 1. Generally close to 1.\n        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n        decay: float >= 0. Learning rate decay over each update.\n        weight_decay: float >= 0. Weight decay for each param.\n        amsgrad: boolean. Whether to apply the AMSGrad variant of this\n            algorithm from the paper \"On the Convergence of Adam and\n            Beyond\".\n        total_steps: int >= 0. Total number of training steps. Enable warmup by setting a positive value.\n        warmup_proportion: 0 < warmup_proportion < 1. The proportion of increasing steps.\n        min_lr: float >= 0. Minimum learning rate after warmup.\n    # References\n        - [Adam - A Method for Stochastic Optimization](https:\/\/arxiv.org\/abs\/1412.6980v8)\n        - [On the Convergence of Adam and Beyond](https:\/\/openreview.net\/forum?id=ryQu7f-RZ)\n        - [On The Variance Of The Adaptive Learning Rate And Beyond](https:\/\/arxiv.org\/pdf\/1908.03265v1.pdf)\n    \"\"\"\n\n    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999,\n                 epsilon=None, decay=0., weight_decay=0., amsgrad=False,\n                 total_steps=0, warmup_proportion=0.1, min_lr=0., **kwargs):\n        learning_rate = kwargs.pop('lr', learning_rate)\n        super(RAdam, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n            self.learning_rate = K.variable(learning_rate, name='learning_rate')\n            self.beta_1 = K.variable(beta_1, name='beta_1')\n            self.beta_2 = K.variable(beta_2, name='beta_2')\n            self.decay = K.variable(decay, name='decay')\n            self.weight_decay = K.variable(weight_decay, name='weight_decay')\n            self.total_steps = K.variable(total_steps, name='total_steps')\n            self.warmup_proportion = K.variable(warmup_proportion, name='warmup_proportion')\n            self.min_lr = K.variable(min_lr, name='min_lr')\n        if epsilon is None:\n            epsilon = K.epsilon()\n        self.epsilon = epsilon\n        self.initial_decay = decay\n        self.initial_weight_decay = weight_decay\n        self.initial_total_steps = total_steps\n        self.amsgrad = amsgrad\n\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n\n        if self.initial_decay > 0:\n            lr = lr * (1. \/ (1. + self.decay * K.cast(self.iterations, K.dtype(self.decay))))\n\n        t = K.cast(self.iterations, K.floatx()) + 1\n\n        if self.initial_total_steps > 0:\n            warmup_steps = self.total_steps * self.warmup_proportion\n            decay_steps = K.maximum(self.total_steps - warmup_steps, 1)\n            decay_rate = (self.min_lr - lr) \/ decay_steps\n            lr = K.switch(\n                t <= warmup_steps,\n                lr * (t \/ warmup_steps),\n                lr + decay_rate * K.minimum(t - warmup_steps, decay_steps),\n            )\n\n        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='m_' + str(i)) for (i, p) in enumerate(params)]\n        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='v_' + str(i)) for (i, p) in enumerate(params)]\n\n        if self.amsgrad:\n            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='vhat_' + str(i)) for (i, p) in enumerate(params)]\n        else:\n            vhats = [K.zeros(1, name='vhat_' + str(i)) for i in range(len(params))]\n\n        self.weights = [self.iterations] + ms + vs + vhats\n\n        beta_1_t = K.pow(self.beta_1, t)\n        beta_2_t = K.pow(self.beta_2, t)\n\n        sma_inf = 2.0 \/ (1.0 - self.beta_2) - 1.0\n        sma_t = sma_inf - 2.0 * t * beta_2_t \/ (1.0 - beta_2_t)\n\n        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n\n            m_corr_t = m_t \/ (1.0 - beta_1_t)\n            if self.amsgrad:\n                vhat_t = K.maximum(vhat, v_t)\n                v_corr_t = K.sqrt(vhat_t \/ (1.0 - beta_2_t))\n                self.updates.append(K.update(vhat, vhat_t))\n            else:\n                v_corr_t = K.sqrt(v_t \/ (1.0 - beta_2_t))\n\n            r_t = K.sqrt((sma_t - 4.0) \/ (sma_inf - 4.0) *\n                         (sma_t - 2.0) \/ (sma_inf - 2.0) *\n                         sma_inf \/ sma_t)\n\n            p_t = K.switch(sma_t >= 5, r_t * m_corr_t \/ (v_corr_t + self.epsilon), m_corr_t)\n\n            if self.initial_weight_decay > 0:\n                p_t += self.weight_decay * p\n\n            p_t = p - lr * p_t\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    @property\n    def lr(self):\n        return self.learning_rate\n\n    @lr.setter\n    def lr(self, learning_rate):\n        self.learning_rate = learning_rate\n\n    def get_config(self):\n        config = {\n            'learning_rate': float(K.get_value(self.learning_rate)),\n            'beta_1': float(K.get_value(self.beta_1)),\n            'beta_2': float(K.get_value(self.beta_2)),\n            'decay': float(K.get_value(self.decay)),\n            'weight_decay': float(K.get_value(self.weight_decay)),\n            'epsilon': self.epsilon,\n            'amsgrad': self.amsgrad,\n            'total_steps': float(K.get_value(self.total_steps)),\n            'warmup_proportion': float(K.get_value(self.warmup_proportion)),\n            'min_lr': float(K.get_value(self.min_lr)),\n        }\n        base_config = super(RAdam, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","2e6eb9d5":"def crps(y_true, y_pred):\n    return K.mean(K.square(y_true - K.cumsum(y_pred, axis=1)), axis=1)","01056379":"def create_model():\n    numerical_inputs = Input(shape=(X_train.shape[1],)) \n    x = Dense(X_train.shape[1], activation='relu')(numerical_inputs)\n    x = BatchNormalization()(x)\n    \n    logits = Dense(512,activation='relu')(x)\n    logits = Dropout(0.3)(logits)\n    \n    logits = Dense(256,activation='relu')(logits)\n    logits = Dropout(0.2)(logits)\n    \n    logits = Dense(256,activation='relu')(logits)\n    out = Dense(199, activation='softmax')(logits)\n    \n    model = Model(inputs = numerical_inputs, outputs=out)\n    return model","c03233e6":"def train_model(x_tr, y_tr, x_vl, y_vl):\n    model = create_model()\n    \n    er = EarlyStopping(patience=15, min_delta=1e-4, restore_best_weights=True, monitor='val_loss')\n    \n    model.compile(optimizer=RAdam(warmup_proportion=0.1, min_lr=1e-5), loss=crps, metrics=[metrics.mae])\n    model.fit(x_tr, y_tr, epochs=100, callbacks=[er], validation_data=[x_vl, y_vl])\n    return model","c1ddeb08":"scaler = StandardScaler() \nX_train = scaler.fit_transform(X_train)\n\nrkf = RepeatedKFold(n_splits=5, n_repeats=5)\nmodels = []\n\nfor tr_idx, vl_idx in rkf.split(X_train, y_train):\n    \n    x_tr, y_tr = X_train[tr_idx], y_train[tr_idx]\n    x_vl, y_vl = X_train[vl_idx], y_train[vl_idx]\n    \n    model = train_model(x_tr, y_tr, x_vl, y_vl)\n    models.append(model)","67f0915c":"def make_pred(df, sample, env, models):\n    df['OffenseFormation'] = df['OffenseFormation'].apply(lambda x: x if x in off_form else np.nan)\n    df = pd.concat([df.drop(['OffenseFormation'], axis=1), pd.get_dummies(df['OffenseFormation'], prefix='Formation')], axis=1)\n    missing_cols = set( dummy_col ) - set( test.columns )-set('Yards')\n    for c in missing_cols:\n        df[c] = 0\n    df = df[dummy_col]\n    df.drop(['Yards'], axis=1, inplace=True)\n    df['GameClock'] = df['GameClock'].apply(strtoseconds)\n    df['PlayerHeight'] = df['PlayerHeight'].apply(lambda x: 12*int(x.split('-')[0])+int(x.split('-')[1]))\n    df['TimeHandoff'] = df['TimeHandoff'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n    df['TimeSnap'] = df['TimeSnap'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n    df['TimeDelta'] = df.apply(lambda row: (row['TimeHandoff'] - row['TimeSnap']).total_seconds(), axis=1)\n    df['PlayerBirthDate'] = df['PlayerBirthDate'].apply(lambda x: datetime.datetime.strptime(x, \"%m\/%d\/%Y\"))\n    seconds_in_year = 60*60*24*365.25\n    df['PlayerAge'] = df.apply(lambda row: (row['TimeHandoff']-row['PlayerBirthDate']).total_seconds()\/seconds_in_year, axis=1)\n    df['WindSpeed'] = df['WindSpeed'].apply(lambda x: x.lower().replace('mph', '').strip() if not pd.isna(x) else x)\n    df['WindSpeed'] = df['WindSpeed'].apply(lambda x: (int(x.split('-')[0])+int(x.split('-')[1]))\/2 if not pd.isna(x) and '-' in x else x)\n    df['WindSpeed'] = df['WindSpeed'].apply(lambda x: (int(x.split()[0])+int(x.split()[-1]))\/2 if not pd.isna(x) and type(x)!=float and 'gusts up to' in x else x)\n    df['WindSpeed'] = df['WindSpeed'].apply(str_to_float)\n    df['PlayDirection'] = train['PlayDirection'].apply(lambda x: x is 'right')\n    df['Team'] = df['Team'].apply(lambda x: x.strip()=='home')\n    indoor = \"indoor\"\n    df['GameWeather'] = df['GameWeather'].apply(lambda x: indoor if not pd.isna(x) and indoor in x else x)\n    df['GameWeather'] = df['GameWeather'].apply(lambda x: x.lower().replace('coudy', 'cloudy').replace('clouidy', 'cloudy').replace('party', 'partly').replace('clear and sunny', 'sunny and clear').replace('skies', '').replace(\"mostly\", \"\").strip() if not pd.isna(x) else x)\n    df['GameWeather'] = df['GameWeather'].apply(map_weather)\n    df['IsRusher'] = df['NflId'] == df['NflIdRusher']\n    \n    df = df.sort_values(by=['PlayId', 'Team', 'IsRusher']).reset_index()\n    df = df.drop(['TimeHandoff', 'TimeSnap', 'PlayerBirthDate', 'WindDirection', 'NflId', 'NflIdRusher', 'GameId', 'PlayId', 'index', 'IsRusher', 'Team'], axis=1)\n    cat_features = []\n    for col in df.columns:\n        if df[col].dtype =='object':\n            cat_features.append(col)\n\n    df = df.drop(cat_features, axis=1)\n    df.fillna(-999, inplace=True)\n    X = np.array(df[players_col]).reshape(-1, 11*22)\n    play_col = df.drop(players_col, axis=1).columns\n    X_play_col = np.zeros(shape=(X.shape[0], len(play_col)))\n    for i, col in enumerate(play_col):\n        X_play_col[:, i] = df[col][::22]\n    \n    X = scaler.transform(np.concatenate([X, X_play_col], axis=1))\n    y_pred = np.mean([model.predict(X) for model in models], axis=0)\n    \n    y_pred[0] = np.cumsum(y_pred[0], axis=0).clip(0, 1)\n    y_pred[:, -1] = np.ones(shape=(y_pred.shape[0], 1))\n    y_pred[:, 0] = np.zeros(shape=(y_pred.shape[0], 1))\n    \n    env.predict(pd.DataFrame(data=y_pred,columns=sample.columns))\n    return y_pred","86059274":"env = nflrush.make_env()","b93941e7":"for test, sample in tqdm.tqdm(env.iter_test()):\n    make_pred(test, sample, env, models)","f903ce7f":"env.write_submission_file()","6449994b":"## About this notebook","5147d364":"## Time handoff and snap and Player BirthDate","065edd87":"## Team","80cdb4e3":"## Offense formation","88eddaba":"## Game Weather","b66d8550":"# Model","58533d7b":"I've created this notebook originally to test a simple idea: what would happen if I used [prashantkikani's architecture](https:\/\/www.kaggle.com\/prashantkikani\/nfl-starter-mlp-feature-engg) with [miklgr500's solution](https:\/\/www.kaggle.com\/miklgr500\/fork-of-neural-networks-radam-repeatkfold)? \n\nThe result was interesting, so I decided to share it.\n\nHowever, an idea crossed my mind. I wanted to use softmax as the output's activation function because it made a lot more sense to me. Since I didn't know how to implement it, I asked for [help](https:\/\/www.kaggle.com\/c\/nfl-big-data-bowl-2020\/discussion\/112991). [@christoffer](https:\/\/www.kaggle.com\/christoffer) answered me, taught me how do to it (thank you!) and the custom loss function you see in ths notebook was his idea.\n\nHope this notebook helps you!","b836df66":"## NflId NflIdRusher","bd7e76ca":"## Wind Speed and Direction","19e3220d":"## Game Clock","688baf0a":"## Player height","9df1b457":"## PlayDirection"}}