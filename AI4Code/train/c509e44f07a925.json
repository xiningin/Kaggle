{"cell_type":{"3b088653":"code","576937f3":"code","8baa2473":"code","b6280d95":"code","d5c8e2b3":"code","a2cb3b79":"code","38cb6df1":"code","9a3079f5":"code","07281efc":"code","b18d356a":"code","0815a1ce":"code","4093e1c9":"code","a26eb72c":"code","3caf51d2":"code","045eda58":"code","5f92f746":"code","036bcf9a":"code","952a976d":"code","515d40d6":"code","78d08601":"code","eca9607a":"code","16460346":"code","9f2b8154":"code","e1a74475":"code","4e9a2956":"code","6cd22442":"code","5517009f":"code","80d94e47":"code","557f6efb":"code","57b16b83":"code","39240877":"code","b37fade3":"code","33a0dac8":"code","e459403d":"code","4a2b5e43":"code","b60a362c":"code","8fa1fff4":"code","7a879f03":"code","1dc0cc1e":"code","26a72c06":"code","c86e76b2":"code","3d903a3f":"code","b7833d03":"code","bc059bab":"code","41caca64":"code","850cb595":"code","6277c386":"code","13bbd8a1":"code","614efb7b":"code","6ada0a1c":"code","a25f97fc":"code","5e15973d":"code","da75bca8":"code","08959ab8":"code","91b98432":"code","b8c8db38":"code","1dca116c":"code","a5c1520a":"code","469eb620":"code","48ab23f1":"code","25ba6022":"code","7bbd2b5c":"code","078a783e":"code","902d578c":"markdown","acd8f8f4":"markdown","506751dd":"markdown","9bfc19ee":"markdown","9c9e7089":"markdown","514ba565":"markdown","fa3e7262":"markdown","f532923e":"markdown","73199b3f":"markdown","cf3e02f4":"markdown","a4905fa0":"markdown","4cd24551":"markdown","24461d8d":"markdown","372eb2bb":"markdown","a6662b8d":"markdown","f3809c7c":"markdown","ca18aff1":"markdown","2dc61517":"markdown","4267fad8":"markdown","47ab8cf3":"markdown","bb4f63ff":"markdown","d16bbd44":"markdown","5c281764":"markdown","221639ce":"markdown","8447dbfa":"markdown","f27cbf46":"markdown","8feb10b1":"markdown"},"source":{"3b088653":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')","576937f3":"# Importing preprocessed data \n\n# Train data\ntrain = pd.read_csv('..\/input\/flight-fare-prediction\/processed_trainData.csv')\n\n# Test data\ntest = pd.read_csv('..\/input\/flight-fare-prediction\/processed_testData.csv')","8baa2473":"train.head()","b6280d95":"test.head()","d5c8e2b3":"# Let's check the shape and columns\n\ntrain.shape","a2cb3b79":"test.shape","38cb6df1":"train.columns","9a3079f5":"test.columns","07281efc":"# Unecessary column added to the dataset called \"Unnamed: 0\", let's drop that\n\ntrain.drop([\"Unnamed: 0\"],axis=1, inplace=True)","b18d356a":"train.shape","0815a1ce":"train.columns","4093e1c9":"# Same for the test data\n\ntest.drop([\"Unnamed: 0\"], axis=1, inplace=True)\ntest.shape","a26eb72c":"test.columns","3caf51d2":"train.shape","045eda58":"test.shape","5f92f746":"plt.figure(figsize=(18,18))\nsns.heatmap(train.corr(), annot=True, cmap=\"Blues\")\nplt.title(\"Correlation\")\nplt.show()","036bcf9a":"X = train.drop([\"Price\"], axis=1)\nX.columns","952a976d":"y = train[\"Price\"]\ny.head()","515d40d6":"from sklearn.ensemble import ExtraTreesRegressor\nselection = ExtraTreesRegressor()\nselection.fit(X,y)","78d08601":"print(selection.feature_importances_)","eca9607a":"# Plot graph of feature importances for better visualization\n\nplt.figure(figsize=(12,8))\nfeat_importances = pd.Series(selection.feature_importances_,index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.title(\"Important Features from the data\")\nplt.show()","16460346":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)","9f2b8154":"from sklearn.ensemble import RandomForestRegressor\nrf_reg = RandomForestRegressor()\nrf_reg.fit(X_train, y_train)","e1a74475":"y_pred_rf = rf_reg.predict(X_test)","4e9a2956":"# Model Score on training set\n\nrf_reg.score(X_train, y_train)","6cd22442":"# Model Score on testing set\n\nrf_reg.score(X_test, y_test)","5517009f":"sns.distplot(y_test-y_pred_rf)\nplt.show()","80d94e47":"plt.scatter(y_test, y_pred_rf, alpha=0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","557f6efb":"from sklearn import metrics","57b16b83":"print(\"MAE: \", metrics.mean_absolute_error(y_test, y_pred_rf))\nprint(\"MSE: \", metrics.mean_squared_error(y_test, y_pred_rf))\nprint(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_test, y_pred_rf)))","39240877":"# r2_score\n\nmetrics.r2_score(y_test, y_pred_rf)","b37fade3":"from xgboost import XGBRegressor\n\nxgbR = XGBRegressor()\n\nxgbR.fit(X_train, y_train)","33a0dac8":"y_pred_xgb =  xgbR.predict(X_test)","e459403d":"# Model Score on training set\n\nxgbR.score(X_train, y_train)","4a2b5e43":"# Model Score testing set\n\nxgbR.score(X_test, y_test)","b60a362c":"sns.distplot(y_test - y_pred_xgb)\nplt.show()","8fa1fff4":"plt.scatter(y_test, y_pred_xgb, alpha = 0.5)\nplt.xlabel('y_test')\nplt.ylabel('y_pred')\nplt.show()","7a879f03":"print(\"MAE: \", metrics.mean_absolute_error(y_test, y_pred_xgb))\nprint(\"MSE: \", metrics.mean_squared_error(y_test, y_pred_xgb))\nprint(\"RMSE:\", np.sqrt(metrics.mean_squared_error(y_test, y_pred_xgb)))","1dc0cc1e":"# r2_score\n\nmetrics.r2_score(y_test, y_pred_xgb)","26a72c06":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV","c86e76b2":"# RandomizedSearchCV on RandomForestRegressor\n\n# number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start=100, stop = 1200, num=12)]\n# number of features to consider at even split\nmax_features = ['auto', 'sqrt']\n# maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num=6)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","3d903a3f":"# create random grid\n\nrandom_grid = {\n                'n_estimators': n_estimators,\n                'max_features': max_features,\n                'max_depth' : max_depth,\n                'min_samples_split': min_samples_split,\n                'min_samples_leaf': min_samples_leaf\n}","b7833d03":"# Random search of parameters, using 5 fold cross validation, \n# search across 100 different combinations\nrf_random = RandomizedSearchCV(estimator = rf_reg, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2)","bc059bab":"rf_random.fit(X_train, y_train)","41caca64":"# metrics.SCORERS.keys()","850cb595":"rf_random.best_params_","6277c386":"# xgboost with GridSearchCV\n\n# xgbR.get_params()","13bbd8a1":"#learning_rate \n# learning_rate = [0.001, 0.1, 0.002]\n\nmax_depth = [4,8,6]\n\nn_estimators = [200, 300, 800]\n\nn_jobs = [5,7]\n","614efb7b":"xgb_grid = {\n#             'base_score': base_score,\n            'max_depth': max_depth,\n            'n_estimators': n_estimators,\n            'n_jobs': n_jobs\n}","6ada0a1c":"xgb_grid = GridSearchCV(estimator=xgbR, param_grid=xgb_grid, scoring='neg_mean_squared_error',cv=5, verbose=2)","a25f97fc":"xgb_grid.fit(X_train, y_train)","5e15973d":"xgb_grid.best_params_","da75bca8":"rf_random.best_params_","08959ab8":"# RandomForestRegressor with best parameters\nrandomForest = RandomForestRegressor(n_estimators=200, min_samples_split=15, min_samples_leaf=1, max_features='auto', max_depth=25)\nrandomForest.fit(X_train, y_train)","91b98432":"# XGBoostRegressor with best parameters\nxgbreg = XGBRegressor(max_depth=4, n_estimators=300, n_jobs=5)\nxgbreg.fit(X_train, y_train)","b8c8db38":"# creating a function that predict the score on X_train, X_test, y_train and y_test\ndef model_score(model):\n    print(\"Score on training set: \")\n    print(model.score(X_train, y_train))\n    print(\"\\nScore testing set: \")\n    print(model.score(X_test, y_test))","1dca116c":"print(\"Score of RandomForest Regressor: \")\nmodel_score(randomForest)","a5c1520a":"print(\"Score of XGBRegressor: \")\nmodel_score(xgbreg)","469eb620":"y_pred_randomForest = randomForest.predict(X_test)","48ab23f1":"y_pred_xgbreg = xgbreg.predict(X_test)","25ba6022":"def check_score(y_pred_model):\n    print(\"MAE: \", metrics.mean_absolute_error(y_test, y_pred_model))\n    print(\"MSE: \", metrics.mean_squared_error(y_test, y_pred_model))\n    print(\"RMSE:\", np.sqrt(metrics.mean_squared_error(y_test, y_pred_model)))\n    print(\"\\nR2 Score: \")\n    print(metrics.r2_score(y_test, y_pred_model))","7bbd2b5c":"check_score(y_pred_randomForest)","078a783e":"check_score(y_pred_xgbreg)","902d578c":"Let's check feature importance","acd8f8f4":"First of all let's check the correlation between independent and dependent attributes.","506751dd":"# Give upvotes to both the notebooks","9bfc19ee":"- RandomForestRegressor","9c9e7089":"## Model Evaluation","514ba565":"Uneccesary column from both the datase had been dropped.","fa3e7262":"## This is a second part of Flight Fare Prediction project.\n\nCheck the first part of this notebook here: **[Flight Fare - 1 (EDA+ Visuals](https:\/\/www.kaggle.com\/karan842\/flight-fare-1-eda-visuals)**","f532923e":"Model Evaluation on **RandomForestRegressor**","73199b3f":"## Parametrized mdoel tunning\n\n1. RandomForestRegressor\n2. XGBRegressor","cf3e02f4":"## Fitting model using Random Forest and XGBoost\n\n1. Split dataset into train and test set \n2. Using two regression ML algorithms such as:\n    - Random Forest Regression\n    - XGBRegressor\n3. Model Evaluation","a4905fa0":"## Model evaluation with the help of visualization","4cd24551":"### Feature Selection\nFinding out the best feature which will contribute and have good relation with target variable.","24461d8d":"#### Important feature using ExtraTreesRegressor","372eb2bb":"## Importing Essential Libraries","a6662b8d":"# Flight Fare Prediction \n\nWhile training the model, different aspects are importnant therefore we're going to perform below methods:\n\n- Feature Selection\n- Hyperparamter Tunning\n- Modeol Building","f3809c7c":"Now we have a two variables **X** and **y** ","ca18aff1":" Above results gives us best observations for better model to predict flight fares\n \n **XGBRegressor** is the better option. It has low errors and pretty good r2_score than **RandomForestRegressor**","2dc61517":" Extremely Randomized Trees (or Extra-Trees) is an ensemble learning method. The method creates extra trees randomly in sub-samples of datasets to improve the predictivity of the model. By this approach, the method reduces the variance. The method averages the outputs from the decision trees.","4267fad8":"### Sperating Independent and Dependend variables","47ab8cf3":"Above graph showing us the correlation between columns of the train data.","bb4f63ff":"## Model evaluation with the help of visualization","d16bbd44":"- XGBoostRegressor","5c281764":"Model Evaluation on **XGBRegressor**","221639ce":"Below function will return **MAE, MSE, RMSE and r2_Score**","8447dbfa":"## Report(observation):","f27cbf46":"### Trainig models with best params","8feb10b1":"# Hyperparameter Tunning\n\n1. RandomizedSearchCV \n2. GridSearchCV \n\nFrom above 2 options RandomizedSearchCV is fast so for equality we can use this tunning method on both algorithms."}}