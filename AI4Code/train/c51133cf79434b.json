{"cell_type":{"317fdf35":"code","510233f7":"code","bbf8c13a":"code","7375a953":"code","161da6c8":"code","69b80622":"code","ea3c3e4e":"code","63f7e018":"code","490ab765":"code","48d1b79d":"code","4cfd36a8":"code","e17d66cf":"code","ab3a7d04":"code","d47df5f0":"code","aefaf5a7":"code","2ff28d67":"code","0d9e84d7":"code","10e9df65":"code","1d8fa057":"code","72170b5e":"code","6df558be":"code","9091fe9e":"code","3c342432":"code","5aeaf218":"markdown","bf0ee8bd":"markdown","d4e4b37b":"markdown","a487ca80":"markdown","10c13896":"markdown","c8c1f1b3":"markdown","23ca88b6":"markdown","9ff05627":"markdown","605a702c":"markdown","c0f67276":"markdown","d1a853d8":"markdown","0256a604":"markdown","790ee7df":"markdown","8b31ccb9":"markdown","31b3427e":"markdown","5916e4e6":"markdown","c780c95a":"markdown","3f1c1eaa":"markdown","72b7e0cc":"markdown","f05cf0ab":"markdown","19a99dae":"markdown","4f268d65":"markdown","4f6b18d7":"markdown"},"source":{"317fdf35":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","510233f7":"# Import libraries -\n\n# Visualization\nimport shap\n%matplotlib inline\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\n# Data Wrangling\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns',50)\n\n# Model Creation\nfrom xgboost import XGBRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom category_encoders import OneHotEncoder, OrdinalEncoder\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split, cross_val_score, validation_curve, GridSearchCV\nfrom sklearn.metrics import roc_curve, plot_roc_curve, mean_absolute_error, mean_squared_error, accuracy_score\n\n# Warnings Ignore\nimport warnings\nwarnings.filterwarnings(\"ignore\")","bbf8c13a":"# Read in data\ndf = pd.read_csv('..\/input\/energy-consumption-generation-prices-and-weather\/energy_dataset.csv')\n\n# Preview of data\ndf.head(5)","7375a953":"# .Describe to show overview of data\ndf.describe()","161da6c8":"# .Info to show datatype, nulls, and count\ndf.info()","69b80622":"# Null % check amongst columns\nround((df.isnull().sum()\/len(df)*100),2)","ea3c3e4e":"# DF.corr to show correlation of values\ndf.corr()","63f7e018":"# Correlation of columns to target variable \ncorrelations = df.corr(method='pearson')\nprint(correlations['price actual'].sort_values(ascending=False).to_string())","490ab765":"# Assign Variable to drop columns\nzero_val_cols = ['generation marine',\n                 'generation geothermal',\n                 'generation fossil peat',\n                 'generation wind offshore',\n                 'generation fossil oil shale',\n                 'forecast wind offshore eday ahead',\n                 'generation fossil coal-derived gas',\n                 'generation hydro pumped storage aggregated']\n\n# Drop Columns with zero values\nheat_map_features = df.drop(columns=zero_val_cols,axis=1)\n\n# Set Figure Size\nplt.figure(figsize=(15,12.5))\n\n# .corr heatmap of df to visualize correlation & show plot\nsns.heatmap(round(heat_map_features.corr(),1),annot=True,cmap='Blues',linewidth=0.9)\nplt.show();","48d1b79d":"# Figure Size\nplt.figure(figsize=(15,10))\n\n# Hist graph to show distribution of target variable\nsns.histplot(df,x='price actual');","4cfd36a8":"# Wrangle function to clean data\ndef wrangle(filepath):\n    \n    '''\n                 ,,,,,,,\n      (\\-\"\"\"-\/) \/       \\\n       |     | \/         \\\n       \\ 0 0 \/\/           \\\n        \\_o_\/\/       \/\\   \/\n       \/`   `\\      |  \\,\/  \n      \/       \\     |  \n      \\ (   ) \/     |\n     \/ \\_)-(_\/ \\    |\n    |  \/_____\\  |  \/\n    \\  \\ N.C \/ \/  \/\n     \\ '.___.' \/ \/\n    .'  \\-=-\/  '.\n   \/   \/`   `\\   \\\n  (\/\/.\/       \\.\\\\)\n   `\"`         `\"`\n   \n    '''\n    \n    # Read in the data, parse dates, and set the index\n    df = pd.read_csv(filepath,parse_dates=['time'],index_col='time')\n \n    # Rename columns by replacing all - or blank space with _\n    df.columns = df.columns.str.replace(' ','_').str.replace('-','_')\n\n    # Make the index DT\n    df.index = pd.to_datetime(df.index, utc=True)    \n\n    # Drop all columns with data leakage, or 90% + null\n    df.drop(columns=['price_day_ahead',\n                     'generation_marine',\n                     'total_load_forecast',\n                     'generation_geothermal',\n                     'generation_fossil_peat',\n                     'generation_wind_offshore',\n                     'forecast_solar_day_ahead',\n                     'generation_fossil_oil_shale',\n                     'forecast_wind_onshore_day_ahead',\n                     'forecast_wind_offshore_eday_ahead',\n                     'generation_fossil_coal_derived_gas',\n                     'generation_hydro_pumped_storage_aggregated'],inplace=True)\n    \n    # Drop Outlier row 2014 for plotting\n    df = df.drop(pd.Timestamp('2014-12-31 23:00:00+00:00')) \n    \n    # Sort index\n    df = df.sort_index()\n    \n    # Set conditional satements for filtering times of month to season value\n    condition_winter = (df.index.month>=1)&(df.index.month<=3)\n    condtion_spring = (df.index.month>=4)&(df.index.month<=6)\n    condition_summer = (df.index.month>=7)&(df.index.month<=9)\n    condition_automn = (df.index.month>=10)@(df.index.month<=12)\n    \n    # Create column in dataframe that inputs the season based on the conditions created above\n    df['season'] = np.where(condition_winter,'winter',\n                            np.where(condtion_spring,'spring',\n                                     np.where(condition_summer,'summer',\n                                              np.where(condition_automn,'automn',np.nan))))\n\n    return df\n\n# Applying the wrangle function to the dataset\ndf=wrangle('..\/input\/energy-consumption-generation-prices-and-weather\/energy_dataset.csv')","e17d66cf":"# Figure showing Price per total load\nfig = px.scatter(df,x='total_load_actual',\n                 y='price_actual',\n                 facet_col='season',\n                 opacity=0.1,\n                 title='Price Per KW Hour Compaired To Total Energy Genereated Per Season',\n                 animation_frame=df.index.year)\n\n# Figure customizations\nfig.update_traces(marker=dict(size=12,\n                              line=dict(width=2,\n                                        color='darkslateblue')),\n                  selector=dict(mode='markers'))","ab3a7d04":"# Create Target variable\ntarget='price_actual'\n\n# Split data into feature matrix and target vector\ny,X=df[target],df.drop(columns=target)\n\n# split data into train \/ validation sets\nX_train,X_val,y_train,y_val = train_test_split(X,y,test_size=.2,random_state=42)","d47df5f0":"# Assign variables for baselines and calculate baselines\ny_pred = [y_train.mean()]*len(y_train)\nmean_baseline_pred = y_train.mean()\nbaseline_mae = mean_absolute_error(y_train,y_pred)\nbaseline_rmse = mean_squared_error(y_train,y_pred,squared=False)\n\n# Print statement to show all baseline values\nprint('Mean Price Per KW\/h Baseline Pred:', mean_baseline_pred)\nprint('-------------------------------------------------------------------')\nprint('Baseline Mae:',baseline_mae)\nprint('-------------------------------------------------------------------')\nprint('Baseline RMSE:',baseline_rmse)","aefaf5a7":"# One Hot Encoder to transform Seasons column\nonehot = OneHotEncoder(use_cat_names=True)\nonehot_fit = onehot.fit(X_train)\nXT_train = onehot.transform(X_train)\nXT_val = onehot.transform(X_val)\n\n# Simple imputer to fill nan values, then transform sets\nsimp = SimpleImputer(strategy='mean')\nsimp_fit = simp.fit(XT_train)\nXT_train = simp.transform(XT_train)\nXT_val = simp.transform(XT_val)\n\n# Assigning model variables\nmodel_lr=LinearRegression()\nmodel_r=Ridge()\n\n# Fitting models\nmodel_r.fit(XT_train,y_train);\nmodel_lr.fit(XT_train,y_train);\n\n# Def to check model metrics of baseline performance\ndef check_metrics(model):\n    print(model)\n    print('===================================================================')\n    print('Training MAE:', mean_absolute_error(y_train,model.predict(XT_train)))\n    print('-------------------------------------------------------------------')\n    print('Validation MAE:', mean_absolute_error(y_val,model.predict(XT_val)))\n    print('-------------------------------------------------------------------')\n    print('Validation R2 score:', model.score(XT_val,y_val))\n    print('===================================================================')\nmodel = [model_r,model_lr]\nfor m in model:\n  check_metrics(m)","2ff28d67":"# Ordinal Encoder to transform Seasons column\nordinal = OrdinalEncoder()\nordinal_fit = ordinal.fit(X_train)\nXT_train = ordinal.transform(X_train)\nXT_val = ordinal.transform(X_val)\n\n# Simple imputer to fill nan values, then transform sets\nsimp = SimpleImputer(strategy='mean')\nsimp_fit = simp.fit(XT_train)\nXT_train = simp.transform(XT_train)\nXT_val = simp.transform(XT_val)\n\n# Assigning model variables\nmodel_rfr = RandomForestRegressor()\nmodel_xgbr=XGBRegressor()\n\n# Fitting models\nmodel_rfr.fit(XT_train,y_train);\nmodel_xgbr.fit(XT_train,y_train);\n\n# Def to check model metrics of baseline performance\ndef check_metrics(model):\n    print(model)\n    print('===================================================================')\n    print('Training MAE:', mean_absolute_error(y_train,model.predict(XT_train)))\n    print('-------------------------------------------------------------------')\n    print('Validation MAE:', mean_absolute_error(y_val,model.predict(XT_val)))\n    print('-------------------------------------------------------------------')\n    print('Validation R2 score:', model.score(XT_val,y_val))\n    print('===================================================================')\nmodel = [model_xgbr,model_rfr]\nfor m in model:\n  check_metrics(m)","0d9e84d7":"# Pipeline variable for RandomSearch\npipe_rs_xgb = make_pipeline(OrdinalEncoder(),\n                            SimpleImputer(),\n                            XGBRegressor(random_state=42,\n                                         n_jobs=-1))\n\n# Params for RandomSearch\nparamajama = {'simpleimputer__strategy':['meadian','mean'],\n              'xgbregressor__max_depth':range(5,35,5),\n              'xgbregressor__learning_rate':np.arange(0.2,1,0.1),\n              'xgbregressor__booster':['gbtree','gblinear','dart'],\n              'xgbregressor__min_child_weight':range(1,10,1),\n              'xgbregressor__gamma':np.arange(0,1,0.1),\n              'xgbregressor__max_delta_step':np.arange(0,1,0.1),\n              'xgbregressor__subsample':np.arange(0.5,1,0.1)}\n\n# RandomSearch Model\nmodel_rs_xgbr = RandomizedSearchCV(pipe_rs_xgb,\n                                   param_distributions = paramajama,\n                                   n_iter=20,\n                                   n_jobs=-1)\n\n# Model fit\nmodel_rs_xgbr.fit(X_train,y_train);\n\n# Check model metrics\nprint('Training MAE:', mean_absolute_error(y_train,model_rs_xgbr.predict(X_train)))\nprint('-------------------------------------------------------------------')\nprint('Validation MAE:', mean_absolute_error(y_val,model_rs_xgbr.predict(X_val)))\nprint('-------------------------------------------------------------------')\nprint('R2 score:', model_rs_xgbr.score(X_val,y_val))\nprint('===================================================================')\nmodel_rs_xgbr.best_params_","10e9df65":"# Pipeline variable for RandomSearch\npipe_rs_rfr = make_pipeline(OrdinalEncoder(),\n                            SimpleImputer(),\n                            RandomForestRegressor(random_state=42,\n                                                  n_jobs=-1))\n\n# Params for RandomSearch\npramajams = {'simpleimputer__strategy':['mean','meadian'],\n             'randomforestregressor__max_depth':range(5,35,5),\n             'randomforestregressor__n_estimators':range(25,200,10),\n             'randomforestregressor__max_samples':np.arange(0.2,1,0.1),\n             'randomforestregressor__max_features':['sqrt','log2'],\n             'randomforestregressor__min_samples_split':np.arange(2,5,1)}\n\n# RandomSearch Model\nmodel_rs_rfr = RandomizedSearchCV(pipe_rs_rfr,\n                                  param_distributions=pramajams,\n                                  n_iter=20,\n                                  n_jobs=-1)\n\n# Model fit\nmodel_rs_rfr.fit(X_train,y_train);\n\n# Check model metrics\nprint('Training MAE:', mean_absolute_error(y_train,model_rs_rfr.predict(X_train)))\nprint('-------------------------------------------------------------------')\nprint('Validation MAE:', mean_absolute_error(y_val,model_rs_rfr.predict(X_val)))\nprint('-------------------------------------------------------------------')\nprint('R2 score:', model_rs_rfr.score(X_val,y_val))\nprint('===================================================================')\nmodel_rs_rfr.best_params_","1d8fa057":"# Pipeline variable for GridSearch\n# pipe_it_up = make_pipeline(OrdinalEncoder(),\n#                            SimpleImputer(),\n#                            RandomForestRegressor(random_state=42,n_jobs=-1))\n\n# Params for GridSearch\n# pramajams = {'randomforestregressor__max_depth':range(0,35,5),\n#              'randomforestregressor__n_estimators':range(100,200,10),\n#              'randomforestregressor__max_samples':np.arange(0,1,0.1),\n#              'randomforestregressor__max_features':['sqrt','log2'],\n#              'randomforestregressor__criterion':['mse','mae'],\n#              'randomforestregressor__min_samples_split':np.arange(2,5,1)}\n\n# GridSearch Model\n# model_rfr_two = GridSearchCV(pipe_it_up,\n#                              param_grid=pramajams,\n#                              n_jobs=-1)\n\n# Model fit\n# model_rfr_two.fit(X_train,y_train)\n\n# Check model metrics\n# print('Training MAE:',mean_absolute_error(y_train,model_rfr_two.predict(X_train)))\n# print('-------------------------------------------------------------------')\n# print('Validation MAE:',mean_absolute_error(y_val,model_rfr_two.predict(X_val)))\n# print('-------------------------------------------------------------------')\n# print('R2 score:',model_rfr_two.sore(X_val,y_val))\n# print('===================================================================')\n# model_rfr_two.best_params_","72170b5e":"#Set samp variable to show features when plotting \nsamp = pd.DataFrame(XT_val,columns=ordinal_fit.get_feature_names())","6df558be":"# Shap waterfall plot showing feature importance\nexplainer = shap.TreeExplainer(model_xgbr)\nshap_values=explainer(samp.head(1))\nshap.plots.waterfall(shap_values[0])","9091fe9e":"# Shap force plot also showing feature importance\nexplainer = shap.TreeExplainer(model_xgbr)\nshap_values = explainer.shap_values(samp.head(1))\nshap.initjs()\nshap.force_plot(base_value = explainer.expected_value,\n                shap_values=shap_values,\n                features=samp.head(1))","3c342432":"# Permutation importance for features used in XGBR model\nperm = PermutationImportance(model_xgbr,random_state=42).fit(XT_val,y_val)\neli5.show_weights(perm, feature_names = samp.columns.tolist())","5aeaf218":"## 2.7. Results -\n\n* I ran grid search model for 1.5 days and pulled the plug because of time running thin. I would like to re-run my model post project completion to see if I can find an improvement in r^2 score.\n    * Since the default model outperformed all random searches I'm interested to see if any tuning is needed, or if the default model truly scores the highest.","bf0ee8bd":"## 2.4.1. XGB RandomSearch Conclusions -\n\n(20 random searches with 100 iter. Scaled down for kaggle notebook import) \n* RandomSearch model out performed the default with tuning.\n* Highest score listed - .8991\n    * Highest scoring model.\n* I'm interested in grid search performance post project to see if the model can outperform the random searches.\n\n### Best Score -\n* Training MAE: 0.18182096506754555\n* Validation MAE: 3.143150404572878\n* R2 score: 0.8991961935947261\n\n#### Params -\n*  'xgbregressor__subsample': 0.7,\n*  'xgbregressor__min_child_weight': 4,\n*  'xgbregressor__max_depth': 25,\n*  'xgbregressor__max_delta_step': 0.0,\n*  'xgbregressor__learning_rate': 0.2,\n*  'xgbregressor__gamma': 0.5,\n*  'xgbregressor__booster': 'dart',\n*  'simpleimputer__strategy': 'mean'}\n \n### Second Best Score -\n* Training MAE: 0.22435663695989874\n* Validation MAE: 3.6606957501289457\n* R2 score: 0.8619067640247029","d4e4b37b":"## 3.2 Feature\/Permutation Importance Conclusions - \n\n* Since shap doesn't seem to agree with RandomForestRegressor.. these are the results for the second best model.\n* Generation biomass seems to have the highest importance, followed by generation nuclear.\n* The feature engineered column seems to have the highest permutation importance value, followed by generation fossil gas.","a487ca80":"## 2.6. Grid Search RandomForestRegressor -","10c13896":"## 2.2. Baselines -","c8c1f1b3":"## 2.1. Split Data -","23ca88b6":"## 1.3. Data Cleaning -","9ff05627":"## 3.1 Feature Importance - ","605a702c":"## 1.1. Data Exploration -","c0f67276":"## Data Contents -\n\nThis dataset contains four years of electrical consumption, generation, pricing, and weather data for Spain. Consumption and generation data was retrieved from ENTSOE a public portal for Transmission Service Operator (TSO) data. Settlement prices were obtained from the Spanish TSO Red Electric Espa\u00f1a. \n\n## Column Meanings - \n\n* Time: Datetime index localized to CET\n* Generation biomass: biomass generation in MW\n* Generation fossil brown coal\/lignite: coal\/lignite generation in MW\n* Generation fossil coal-derived gas: coal gas generation in MW\n* Generation fossil gas: gas generation in MW\n* Generation fossil hard coal: coal generation in MW\n* Generation fossil oil: oil generation in MW\n* Generation fossil oil shale: shale oil generation in MW\n* Generation fossil peat: peat generation in MW\n* Generation geothermal: geothermal generation in MW\n* Generation hydro pumped storage aggregated: hydro1 generation in MW\n* Generation hydro pumped storage consumption: hydro2 generation in MW\n* Generation hydro run-of-river and poundage: hydro3 generation in MW\n* Generation hydro water reservoir: hydro4 generation in MW\n* Generation marine: sea generation in MW\n* Generation nuclear: nuclear generation in MW\n* Generation other: other generation in MW\n* Generation other renewable: other renewable generation in MW\n* Generation solar: solar generation in MW\n* Generation waste: waste generation in MW\n* Generation wind offshore: wind offshore generation in MW\n* Generation wind onshore: wind onshore generation in MW\n* Gorecast solar day ahead: forecasted solar generation\n* Gorecast wind offshore eday ahead: forecasted offshore wind generation\n* Gorecast wind onshore day ahead: forecasted onshore wind generation\n* Total load forecast: forecasted electrical demand\n* Total load actual: actual electrical demand\n* Price day ahead: forecasted price EUR\/MWh\n* Price actual: price in EUR\/MWh","d1a853d8":"# Predicting the Price of Electricty with Machine Learning","0256a604":"# 2. ML Model Creation -","790ee7df":"## 1.2. Data Exploration Findings -\n* Normal distrubution of target variable.\n* Low percentage of nan values in columns without 100% nan.\n* Lots of columns with 0 values for certain energy genreation types.\n* Found very high correlation between some columns.\n* All data seems to be numeric.","8b31ccb9":"# 3. Model Visualizations -","31b3427e":"## 2.3. Default Model Creation -","5916e4e6":"## 2.3.1. Default Model Results (Without Feature Engineering) -\n\n#### Ridge -\n* Training MAE: 8.472823575635683\n* Validation MAE: 8.63383328893237\n* R2 score: 0.35638687883377895\n\n#### LinearRegression - \n* Training MAE: 8.472823557244569\n* Validation MAE: 8.633833278291645\n* R2 score: 0.3563868756924954\n\n#### XGBRegressor -\n*  Training MAE: 3.545327560560801\n*  Validation MAE: 4.6979849457995755\n*  R2 score: 0.7950966649998653\n\n#### RandomForestRegressor -\n*  Training MAE: 1.4598095162420663\n*  Validation MAE: 3.9252298502111427\n*  R2 score: 0.8349323474716535\n\n### 2.3.2. Defualt Model Observations -\n\n* RandomForestRegressor seems to out perform any other model.\n* Ridge Regression \/ Linear Regression have very similar outputs with no parameter tuning.\n\n### 2.3.3. Default Model + Feature Engineering Observations -\n\n* Increase of validation scores from 3-7%\n    * HUGE improvement","c780c95a":"\"Real-time electricity pricing models can potentially lead to economic and environmental advantages compared to the current common flat rates. In particular, they can provide end users with the opportunity to reduce their electricity expenditures by responding to pricing that varies with different times of the day. However, recent studies have revealed that the lack of knowledge among users about how to respond to time-varying prices as well as the lack of effective building automation systems are two major barriers for fully utilizing the potential benefits of real-time pricing tariffs. We tackle these problems by proposing an optimal and automatic residential energy consumption scheduling framework which attempts to achieve a desired trade-off between minimizing the electricity payment and minimizing the waiting time for the operation of each appliance in household in presence of a real-time pricing tariff combined with inclining block rates . Our design requires minimum effort from the users and is based on simple linear programming computations. Moreover, we argue that any residential load control strategy in real-time electricity pricing environments requires price prediction capabilities. This is particularly true if the utility companies provide price information only one or two hours ahead of time. Simulation results show that the combination of the proposed energy consumption scheduling design and the price predictor filter leads to significant reduction not only in users' payments but also in the resulting peak-to-average ratio in load demand for various load scenarios. Therefore, the deployment of the proposed optimal energy consumption scheduling schemes is beneficial for both end users and utility companies.\" -[Amir-Hamed Mohsenian-Rad](https:\/\/ieeexplore.ieee.org\/abstract\/document\/5540263)\n\n\n","3f1c1eaa":"## 2.5. Random Forest Regressor RandomSearch -","72b7e0cc":"## 2.5.1. Random Forest Regressor RandomSearch Conclusions -\n\n(20 random searches with 100 iter. Scaled down for kaggle notebook import) \n* Default model out performed the RandomSearch with tuning.\n* Highest score listed - .8795\n* Model did not out perform the default model.\n\n### Best Score -\n\n* Training MAE: 1.4004700781578163\n* Validation MAE: 3.371529047081576\n* R2 score: 0.8795676540229065\n\n#### Params -\n*  'simpleimputer__strategy': 'mean',\n*  'randomforestregressor__n_estimators': 115,\n*  'randomforestregressor__min_samples_split': 2,\n*  'randomforestregressor__max_samples': 0.9000000000000001,\n*  'randomforestregressor__max_features': 'sqrt',\n*  'randomforestregressor__max_depth': 25\n \n### Second Best Score -\n\n* Training MAE: 1.4371681333507431\n* Validation MAE: 3.398423017100202\n* R2 score: 0.877617516249465","f05cf0ab":"![](https:\/\/i.pinimg.com\/originals\/0c\/ab\/1c\/0cab1c6c171b4e0d71ec822600f5923b.jpg)","19a99dae":"## 1.4. Data Cleaning Notes -\n* Removed forecasted columns to prevent data leakage.\n* Removed all columns with 0 fill for all values.\n* Removed only row from 2014 for plotting purposes.\n* Feature engineered seasons using time of year.","4f268d65":"# 1. Data Exploration \/ Clean - \n\n","4f6b18d7":"## 2.4. XGB RandomSearch -"}}