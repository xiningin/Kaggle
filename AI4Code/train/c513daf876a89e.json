{"cell_type":{"07e9f1f9":"code","20801eb7":"code","65768620":"code","7b5428db":"code","687ea382":"code","a1493783":"code","c73ea37e":"code","0b0ab23b":"code","d1dc4979":"code","930b652d":"code","9392d967":"code","d78dbdf1":"code","5f45e1d8":"code","97f76c94":"code","bc9383c9":"code","de08aed0":"code","6a40c155":"code","a3294d2f":"code","27d55527":"code","72658a30":"code","bfc70316":"code","f99ca70d":"code","e1623093":"code","9d11ad6d":"code","61d46ab2":"code","60d038ad":"code","75dd403e":"code","1c87b05c":"code","06c4d27a":"code","5bdc0ba4":"code","ccc6ec92":"code","7b295780":"code","c70167d8":"code","22de1571":"code","c23e38a4":"markdown","5e7398fe":"markdown","d83a1a05":"markdown","dc3764ea":"markdown","f37ddde8":"markdown","0c5e7a0f":"markdown","9eb0cc28":"markdown","698e2e55":"markdown","46fa1b43":"markdown"},"source":{"07e9f1f9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nimport os\nprint(os.listdir(\"..\/input\"))\nsns.set_style('dark')\nsns.set_context('talk')\n# Any results you write to the current directory are saved as output.","20801eb7":"df = pd.read_csv('..\/input\/creditcard.csv', encoding = 'latin1')","65768620":"df.info()","7b5428db":"df.shape","687ea382":"df.isnull().values.any()\n","a1493783":"print((\"Distribution of fraudulent points: {:.2f}%\".format(len(df[df['Class']==1])\/len(df)*100)))\nsns.countplot(df['Class'])\nplt.title('Class Distribution')\nplt.xticks(range(2),['Normal','Fraud'])\nplt.show()\n","c73ea37e":"normal = df[df['Class']==0]\nfraud = df[df['Class']==1]\nprint(\"Normal datapoints: \", normal.shape[0])\nprint(\"Fraud datapoints: \", fraud.shape[0])","0b0ab23b":"normal['Amount'].describe()","d1dc4979":"fraud['Amount'].describe()","930b652d":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize = (10,10) )\nf.suptitle('Amount per transaction by class')\n\nbins = 10\n\nax1.hist(fraud.Amount, bins = bins)\nax1.set_title('Fraud')\n\nax2.hist(normal.Amount, bins = bins)\nax2.set_title('Normal')\n\nax1.grid()\nax2.grid()\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.xlim((0, 20000))\nplt.yscale('log')\nplt.show();","9392d967":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10,10))\nf.suptitle('Time of transaction vs Amount by class')\n\nax1.scatter(fraud.Time, fraud.Amount, marker='.')\nax1.set_title('Fraud')\nax1.grid()\nax2.scatter(normal.Time, normal.Amount, marker='.')\nax2.set_title('Normal')\nax2.grid()\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Amount')\nplt.show()","d78dbdf1":"data = df.drop(['Time'], axis =1)","5f45e1d8":"X_train, X_test = train_test_split(data, test_size=0.2, random_state=42)\nX_train = X_train[X_train.Class == 0]\nX_train = X_train.drop(['Class'], axis=1)\ny_test = X_test['Class']\nX_test = X_test.drop(['Class'], axis=1)\nX_train = X_train\nX_test = X_test\nprint(X_train.shape)\n\nprint(X_test.shape)\nprint(y_test.shape)","97f76c94":"scaler = StandardScaler().fit(X_train.Amount.values.reshape(-1,1))\nX_train['Amount'] = scaler.transform(X_train.Amount.values.reshape(-1,1))\nX_test['Amount'] = scaler.transform(X_test.Amount.values.reshape(-1,1))","bc9383c9":"X_train.shape","de08aed0":"from keras.layers import Input, Dense\nfrom keras import regularizers\nfrom keras.models import Model, load_model\nfrom keras.callbacks import ModelCheckpoint, TensorBoard","6a40c155":"input_dim = X_train.shape[1]\nencoding_dim = 14\n","a3294d2f":"input_layer = Input(shape=(input_dim,))\nencoder = Dense(encoding_dim, activation=\"tanh\", \n                activity_regularizer=regularizers.l1(10e-5))(input_layer)\nencoder = Dense(int(encoding_dim \/ 2), activation=\"relu\")(encoder)\n\ndecoder = Dense(int(encoding_dim \/ 2), activation='tanh')(encoder)\ndecoder = Dense(input_dim, activation='relu')(decoder)\n\nautoencoder = Model(inputs=input_layer, outputs=decoder)","27d55527":"X_train.shape","72658a30":"nb_epoch = 100\nbatch_size = 32\n\nautoencoder.compile(optimizer='adam', \n                    loss='mean_squared_error', \n                    )\n\ncheckpointer = ModelCheckpoint(filepath=\"model.h5\",\n                               verbose=0,\n                               save_best_only=True)\ntensorboard = TensorBoard(log_dir='.\/logs',\n                          histogram_freq=0,\n                          write_graph=True,\n                          write_images=True)\n\nhistory = autoencoder.fit(X_train, X_train,\n                    epochs=nb_epoch,\n                    batch_size=batch_size,\n                    shuffle=True,\n                    validation_split=0.3,\n                    verbose=1,\n                    callbacks=[checkpointer, tensorboard]).history","bfc70316":"plt.figure(figsize = (10,5))\nplt.plot(history['loss'], label = 'Training Loss')\nplt.plot(history['val_loss'], label = 'CV Loss')\nplt.title(\"Model Loss\")\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.grid()\nplt.legend()\nplt.show()","f99ca70d":"autoencoder = load_model('model.h5')","e1623093":"predictions = autoencoder.predict(X_test)","9d11ad6d":"predictions.shape","61d46ab2":"mse = np.mean(np.power(X_test - predictions, 2), axis=1)\nerror_df = pd.DataFrame({'reconstruction_error': mse, 'true_class':y_test})","60d038ad":"error_df.groupby(['true_class']).describe()","75dd403e":"plt.figure(figsize = (10,5))\nsns.distplot(error_df[error_df['true_class']==0]['reconstruction_error'], bins = 5, label = 'Normal')\nsns.distplot(error_df[error_df['true_class']==1]['reconstruction_error'], bins=5, label = 'Fraud')\nplt.legend()\nplt.show()\n","1c87b05c":"from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n                             roc_curve, recall_score, classification_report, f1_score,\n                             precision_recall_fscore_support, roc_auc_score)","06c4d27a":"threshold = 1.4","5bdc0ba4":"groups = error_df.groupby('true_class')\nfig, ax = plt.subplots(figsize = (20,8))\n\nfor name, group in groups:\n    ax.plot(group.index, group.reconstruction_error, marker='+', ms=10, linestyle='',\n            label= \"Fraud\" if name == 1 else \"Normal\")\nax.hlines(threshold, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\nax.legend()\nplt.title(\"Reconstruction error for different classes\")\nplt.ylabel(\"Reconstruction error\")\nplt.xlabel(\"Data point index\")\nplt.grid()\nplt.show();\n","ccc6ec92":"fpr, tpr, thres = roc_curve(error_df.true_class, error_df.reconstruction_error)\nplt.plot(fpr, tpr, label = 'AUC') \nplt.plot([0,1], [0,1], ':', label = 'Random') \nplt.legend() \nplt.grid() \nplt.ylabel(\"TPR\") \nplt.xlabel(\"FPR\") \nplt.title('ROC') \nplt.show() ","7b295780":"LABELS = ['Normal', 'Fraud']\nthreshold = 2\ny_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\nconf_matrix = confusion_matrix(error_df.true_class, y_pred)\nsns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\", cmap='Greens');\nplt.title(\"Confusion matrix\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()\n","c70167d8":"## Chosen metric is AUC ROC as data is imbalanced\nprint(\"Area under ROC : \", roc_auc_score(error_df.true_class,y_pred ))","22de1571":"print(classification_report(error_df.true_class,y_pred))","c23e38a4":"## Distribution Analysis:","5e7398fe":"Here we are going to use Autoencoder and train it on X_train having only normal points, so when it learns the internal pattern of normal transactions we can use it to distinguish between normal and fraud data points. ","d83a1a05":"## Reading Data:","dc3764ea":"## Data Modeling","f37ddde8":"## Model Evaluation:","0c5e7a0f":"As the Data is highly imbalanced with one class being only 0.17%  We are taking this as an anomaly detection problem. ","9eb0cc28":"## Class wise Analysis:","698e2e55":"## Model Training:","46fa1b43":"mse here is basically the reconstruction error, if the model has learned the normal datapoints well, then this error should be less on normal points, and high on fraud points. "}}