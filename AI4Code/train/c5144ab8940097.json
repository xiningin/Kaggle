{"cell_type":{"eb7978b9":"code","91d16861":"code","2c930116":"code","1dc688fc":"code","53215d58":"code","66dedaa6":"code","49fb66c0":"code","76abf6b1":"code","1d98e57d":"code","5b302bb2":"code","a0c9fecb":"code","0cc4432d":"code","37b40bce":"code","cefe54cf":"code","e6776bc4":"code","eed3ba01":"code","d52abd32":"code","521c1e78":"markdown","a14a85d7":"markdown","fd31e403":"markdown","3c0078e9":"markdown","7fba03d8":"markdown","1d961a0c":"markdown","0a363411":"markdown","023452e1":"markdown"},"source":{"eb7978b9":"import os\nimport gc\nimport cv2\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tqdm import tqdm\n\nprint('TensorFlow version: %s' % tf.__version__)","91d16861":"ROOT = '..\/input\/bee-vs-wasp\/kaggle_bee_vs_wasp\/'\nSEED = 42\nEPOCHS = 50\nBATCH_SIZE = 32\nIMG_SIZE = 256\n\n\ndf = pd.read_csv(ROOT+'labels.csv')\ndf.head()","2c930116":"def seed_everything(seed):\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n\nseed_everything(SEED)","1dc688fc":"for idx in tqdm(df.index):    \n    df.loc[idx,'path']=df.loc[idx,'path'].replace('\\\\', '\/') \n    \ndf.head()","53215d58":"labels = list(df['label'].unique())\ny = list(df['label'].value_counts())\nplt.pie(y, labels=labels, autopct='%1.1f%%', startangle=90)\nplt.title('Unique values of the original data')\nplt.show()","66dedaa6":"labels = list(df['photo_quality'].unique())\nx = range(0, 2)\ny = list(df['photo_quality'].value_counts())\nplt.bar(x, y, tick_label=labels)\nplt.title('High quality photos in original data')\n\nplt.show()","49fb66c0":"def img_plot(df, label):\n    df = df.query('label == @label')\n    imgs = []\n    for path in df['path'][:9]:\n        img = cv2.imread(ROOT+path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        imgs.append(img)\n    f, ax = plt.subplots(3, 3, figsize=(15,15))\n    for i, img in enumerate(imgs):\n        ax[i\/\/3, i%3].imshow(img)\n        ax[i\/\/3, i%3].axis('off')\n        ax[i\/\/3, i%3].set_title('label: %s' % label)\n    plt.show()\n    \n    \nimg_plot(df, label='bee')","76abf6b1":"img_plot(df, label='wasp')","1d98e57d":"img_plot(df, label='insect')","5b302bb2":"img_plot(df, label='other')","a0c9fecb":"# select only high quality photos\ndf = df.query('photo_quality == 1')\ndf['label'].value_counts()","0cc4432d":"train_df = df.query('is_validation == 0 & is_final_validation == 0').reset_index(drop=True)\nval_df = df.query('is_validation == 1').reset_index(drop=True)\ntest_df = df.query('is_final_validation == 1').reset_index(drop=True)","37b40bce":"def create_datasets(df, img_size):\n    imgs = []\n    for path in tqdm(df['path']):\n        img = cv2.imread(ROOT+path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, (img_size,img_size))\n        imgs.append(img)\n        \n    imgs = np.array(imgs, dtype='float32')\n    imgs = imgs \/ 255.0\n    df = pd.get_dummies(df['label'])\n    return imgs, df\n\n\ntrain_imgs, train_df = create_datasets(train_df, IMG_SIZE)\nval_imgs, val_df = create_datasets(val_df, IMG_SIZE)\ntest_imgs, test_df = create_datasets(test_df, IMG_SIZE)","cefe54cf":"def build_model(img_size, n):\n    inp = Input(shape=(img_size,img_size,n))\n    resnet = tf.keras.applications.ResNet50(input_shape=(img_size,img_size,n), \n                                            weights='imagenet', \n                                            include_top=False)\n    x = resnet(inp)\n    x = GlobalAveragePooling2D()(x)\n    x = Dropout(0.3)(x)\n    x = Dense(3, activation='softmax')(x)\n    model = tf.keras.Model(inputs=inp, outputs=x) \n    opt = tf.keras.optimizers.SGD(momentum=0.9)\n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\nmodel = build_model(IMG_SIZE, 3)\nmodel.summary()","e6776bc4":"def get_lr_callback(batch_size=32, plot=False):\n    start_lr = 3e-4\n    def step_decay(epoch):\n        drop = 0.5\n        epochs_drop = 10.0\n        lr = start_lr * math.pow(drop, math.floor((1+epoch)\/epochs_drop))\n        return lr\n    \n    lr_callback = tf.keras.callbacks.LearningRateScheduler(step_decay)\n    if plot == True:\n        rng = [i for i in range(EPOCHS)]\n        y = [step_decay(x) for x in rng]\n        plt.plot(rng, y)\n        plt.xlabel('epoch', size=14)\n        plt.ylabel('learning_rate', size=14)\n        plt.title('Training Schedule', size=16)\n        plt.show()\n        \n    return lr_callback\n\n\nget_lr_callback(plot=True)","eed3ba01":"es_callback = tf.keras.callbacks.EarlyStopping(patience=10, \n                                               verbose=1, \n                                               restore_best_weights=True)\n\nhistory = model.fit(train_imgs, \n                    train_df, \n                    batch_size=BATCH_SIZE, \n                    epochs=EPOCHS, \n                    callbacks=[es_callback, get_lr_callback(BATCH_SIZE)], \n                    validation_data=(val_imgs, val_df))\n\n\npd.DataFrame(history.history)[['accuracy', 'val_accuracy']].plot()\npd.DataFrame(history.history)[['loss', 'val_loss']].plot()\nplt.show()\n\ngc.collect()","d52abd32":"model.evaluate(test_imgs, test_df) ","521c1e78":"## Create datasets","a14a85d7":"![bee_img](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F842582%2F1437912%2Fkaggle_bee_vs_wasp%2Fbee1%2F10457143913_149e654aaa_n.jpg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1599423647&Signature=Xi2gXuRGTFXpu1zjDZWjkjXE8%2FENXgSwWR3g0Lixt%2BF9tU%2FITLalVxY8SFHcogZ95BHm7yJQatXMFAI26Db2UjkJGeQFao3qBJwGO4r0YjM%2BhTrt7b0zHHJkO070%2BoFOe5zYA5ZZJNk5dDEONu%2FrN0H4IZ14XJixr6%2Fad8YaZYR%2BZvPZ9hC83WK8bHSJHOJCIcCdnrKqgKO2aa%2BCLugF29TG%2F21xkk4Q6ORCyM4SHRZR6vH8rH2OCF2GmLofxbZcIoI9IXQR2Jp4sdaet15E6aO4g5GpfdOxHaJv5koOmJ5jqUTNYBA1C7OYM2Oxi2zV1O7jLAAv9lJWdKuoEAQC4A%3D%3D)","fd31e403":"## set configurations and read metadata","3c0078e9":"<h1><font size=\"6\">Bee or Wasp?<\/font><\/h1>","7fba03d8":"![](http:\/\/)\n\n\n\n\n\n\nThis notebook uses dataset published by [George Ray] to show a simple baseline.\n\nThanks for publishing a great dataset.\n\n[George Ray]:https:\/\/www.kaggle.com\/jerzydziewierz","1d961a0c":"## Import libraries","0a363411":"## Build the model","023452e1":"## Preprocess and plot\n\nCheck the dataset and preprocess it."}}