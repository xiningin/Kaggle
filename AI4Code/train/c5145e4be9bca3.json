{"cell_type":{"731a3461":"code","93e2673d":"code","2cc4b0ef":"code","b942b322":"code","6ed7faee":"code","a10d6a29":"code","45976a9f":"code","d7ddc1e7":"code","7d8f4841":"code","ad325125":"code","0b05495d":"code","e9b14231":"code","3cc4c949":"code","1b003fc3":"code","a1f22df4":"code","123ef4d9":"code","c84f7b16":"code","1a680987":"code","67d1ec1e":"code","cc1a5a29":"code","bc965a4f":"markdown"},"source":{"731a3461":"from kaggle_secrets import UserSecretsClient","93e2673d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2cc4b0ef":"columns = ['date','open','high','low','close','volume']\ndata = pd.read_csv(\"\/kaggle\/input\/bitcoin-time-series\/bitcoin_usd.csv\", names=columns, header='infer', skiprows=1, parse_dates=['date'], index_col='date')\ndata = data.rename_axis('date').reset_index()\ndata.head()","b942b322":"from statsmodels.nonparametric.smoothers_lowess import lowess\nimport scipy.stats as st\nfrom scipy.stats import norm\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\ndef plot(data, x, y, title=\"\", xLabel=\"\" , yLabel='Precio USD', dpi=100):\n    plt.figure(figsize=(20,7), dpi=dpi)\n    plt.plot(x, y, color='tab:red')\n    plt.gca().set(title=title, xLabel=xLabel, yLabel=yLabel)\n    plt.show()\n\nplot(data, x=data.date, y=data.high, title='Bitcoin - Precio mas alto en el mercado de Julio de 2018 a Enero de 2021', xLabel='n = ' + str(data.high.count()))\n\nbatch_size = int(data.high.count()) # tama\u00f1o de la muestra = 1000\nm = int(batch_size*0.05) #tam >= 15 (5% del total de datos = 50)\nj = int(batch_size\/m) \nsigma = 0.2\nMj = np.arange(j, batch_size, j) # indices para c\u00e1lculo de medias \nX_m = data.iloc[Mj].date\n\ny_m_open = (data.iloc[Mj].open)\/(data.open.max()) #valores de la serie de tiempo (ini)\ny_m_high = (data.iloc[Mj].high)\/(data.high.max()) #valores de la serie de tiempo (altos)\ny_m_low = (data.iloc[Mj].low)\/(data.low.max()) #valores de la serie de tiempo (bajos)\ny_m_close = (data.iloc[Mj].close)\/(data.close.max()) #valores de la serie de tiempo (cierre)\n\nfor i in range(len(y_m_high)+1):\n    y_m_median = (y_m_high[:i]+y_m_low[:i])\/2\n\ndef plot_dist(data, x, y1, y2, y3, title=\"\", xLabel=\"\" , yLabel=\"\", dpi=100):\n    plt.figure(figsize=(20,7), dpi=dpi)\n    plt.plot(x, y1, color='tab:red')\n    plt.plot(x, y2, color='tab:blue')\n    plt.plot(x, y3, color='tab:cyan')\n    plt.gca().set(title=title, xLabel=xLabel, yLabel=yLabel)\n    plt.show()\n\nplot_dist(y_m_median, x=X_m, y1=y_m_high, y2=y_m_low, y3=y_m_median, title='Media ponderada entre precio m\u00e1s alto y el m\u00e1s bajo para Mu (normalizados)', xLabel = 'm = '+ str(m))\n\ny_m_df = pd.DataFrame(y_m_median)\ny_m_df.columns = ['median']\n\n#Mu suavizadas\ndata_soft = pd.DataFrame(lowess((data.high\/data.high.max()), np.arange(len(data.high)), frac=0.05)[:, 1], index=data.date, columns=['high'])\ny_m_median_soft = data_soft.iloc[Mj].high\n\nX = np.array(y_m_median.index).reshape((-1,1))\ny = np.array(y_m_median_soft)\n\n## LS \nregresion = linear_model.LinearRegression(normalize=True, fit_intercept=True).fit(X,y)\ncoef_det = regresion.score(X,y)\nweight_b0 = regresion.intercept_ #scalar y en x=0\nweight_b1 = regresion.coef_ #array dif x_i  x_i+1\nw0 = np.array(weight_b0).reshape(1)\n\ny_rl = [weight_b0]\nfor i in range(len(X)-1):\n    y_rl.append(y_rl[i-1]+weight_b1)\n\n## Ridge\nclf = Ridge(alpha=0.9)\nclf.fit(X, y)\ny_C = clf.predict(X)  \ncoef0 = y_C.min()\ncoef1 = (y_C[0]-y_C[1])\n\n## S = 0.02\nplt.figure(figsize=(20,7), dpi=100)\nplt.plot(X_m, y_rl, color='red')\nplt.plot(X_m, y_C, color='gray')\nplt.plot(X_m, y_m_median_soft, 'o', color='blue')\ndata_soft['high'].plot(title=\"S - 0.05, m - 50 :: Regresi\u00f3n lineal y ajuste Ridge \") \nplt.show()","6ed7faee":"# Phi_ij \ndef phi(data, x, y, sigma=0.2):\n    for i in range(len(data)+1):\n        Phi_alpha = (1\/(2*(sigma**2)))*(((x[:i])-(y[:i]))**2) #y \n    return(Phi_alpha)\n\nPhi_alpha = phi(y_m_median_soft, x=y_m_median.index, y=y_m_median_soft)\n\nplot(Phi_alpha, x=y_m_median.index, y=Phi_alpha, title='y = Phi*alpha + epsilon', xLabel='n = ' + str(Phi_alpha.count()), yLabel=\"valores optimizados + error\")","a10d6a29":"#LS = ||y - phi*alpha ||^2\n\ndef LS(data, y, phi_trans):\n    for i in range(len(data)+1):\n        alpha_min = (np.abs(y[:i]-phi_trans[:i]))**2\n    return(alpha_min)\n\nalpha_min = LS(y_m_median_soft, y=y_m_median_soft, phi_trans=Phi_alpha)\nplot(alpha_min, x=y_m_median.index, y=alpha_min, title='y = LS', xLabel='n = ' + str(alpha_min.count()), yLabel=\"minimos cuadrados\")","45976a9f":"X_date = np.array(X_m).reshape(-1,1)\nX = np.array(y_m_median.index).reshape(-1,1)\ny = np.array(y_m_median_soft)\n\n## Coeficientes Ridge\nfor i in range (len(y_m_median_soft)): \n    theta = y_m_median_soft-(coef1*j)\n","d7ddc1e7":"f_params={'kappa' : 0.05, \n          'X'     : X, \n          'y'     : y}\n\ngd_params = {'alpha'          : 0.95, \n             'alphaADADELTA'  : 0.7,\n             'alphaADAM'      : 0.95,\n             'alphaNADAM'     : 0.95,\n             'nIter'          : 50,\n             'batch_size'     : 49,\n             'eta'            : 0.9,\n             'eta1'           : 0.9,\n             'eta2'           : 0.999}\n\nixhel_params = {'sigma': 0.2,\n                'X': X_m,\n                'y': y_m_median}\n","7d8f4841":"#Gradientes\ndef grad_quadratic(theta, f_params):\n    X = f_params['X']\n    y = f_params['y']\n    err=theta[0]*X+theta[1]-y  \n    partial0=err\n    partial1=X*partial0\n    gradient= np.concatenate((partial1, partial0), axis=1)\n    return np.sum(gradient, axis=1)\n\ndef grad_exp(theta, f_params):\n    kappa= f_params['kappa']\n    X    = f_params['X']\n    y    = f_params['y']\n    err=theta[0]*X+theta[1]-y\n    partial0=err*np.exp(-kappa*err**2)\n    partial1=X*partial0\n    gradient= np.concatenate((partial1, partial0), axis=1)\n    return np.mean(gradient, axis=1)\n\ndef grad_nu(theta, f_params):\n    sigma = f_params['kappa']\n    x     = f_params['X']\n    y     = f_params['y']\n    for i in range(len(x)):\n        Phi_alpha = (-1\/(2*(sigma**2)))*((X[i]-y[:i])**2) #y \n        gradient = np.array(Phi_alpha).reshape(-1,1)\n    return np.mean(gradient, axis=0)","ad325125":"#Descenso de gradiente estoc\u00e1stico\n\ndef SGD(theta=[], grad=None, gd_params=[], f_params=[]):\n    (high,dim) = f_params['X'].shape\n    batch_size = gd_params['batch_size']\n    nIter      = gd_params['nIter']\n    alpha      = gd_params['alpha'] \n    Theta=[]\n    for t in range(nIter):\n        # Set of sampled indices\n        smpIdx = np.random.randint(low=0, high=high, size=batch_size, dtype='int32')\n        # sample \n        smpX = f_params['X'][smpIdx]\n        smpy = f_params['y'][smpIdx]\n        # parametros de la funcion objetivo\n        smpf_params ={'kappa' : f_params['kappa'], \n                      'X'     : smpX , \n                      'y'     : smpy}\n        p = grad(theta,f_params=smpf_params)\n        theta = theta - alpha*p\n        Theta.append(theta)\n    return np.array(Theta)\n\nThetaSGD = SGD(theta=theta, grad=grad_exp, \n               gd_params=gd_params, f_params=f_params)\n\n","0b05495d":"Tmax=50\nplt.figure(figsize=(20,7))\nplt.plot(ThetaSGD[:Tmax,-1], '.')\nplt.title('SGD')\n","e9b14231":"#Descenso de gradiente estoc\u00e1stico acelerado de tipo Nesterov\n\ndef NAG(theta=[], grad=None, gd_params={}, f_params={}):\n\n    nIter = gd_params['nIter']\n    alpha = gd_params['alpha'] \n    eta   = gd_params['eta']\n    p     = np.zeros(theta.shape)\n    Theta=[]\n    \n    for t in range(nIter):\n        pre_theta = theta - (2.0*alpha*p)\n        g = grad(pre_theta, f_params=f_params)\n        p = g + eta*p\n        theta = theta - (alpha*p)\n        Theta.append(theta)\n    return np.array(Theta)\n","3cc4c949":"ThetaNAG = NAG(theta=theta, grad=grad_exp, \n               gd_params=gd_params, f_params=f_params)\nTmax=50\nplt.figure(figsize=(20,7))\nplt.plot(ThetaNAG[ :Tmax,0], '.')\nplt.title('NAG')","1b003fc3":"#AdaDelta\n\ndef ADADELTA(theta=[], grad=None, gd_params={}, f_params={}):\n   \n    epsilon  = 1e-10\n    nIter    = gd_params['nIter']\n    alpha    = gd_params['alphaADADELTA'] \n    eta      = gd_params['eta']\n    G        = np.zeros(theta.shape)\n    g        = np.zeros(theta.shape) \n    Theta=[]\n    for t in range(nIter):\n        g = grad(theta, f_params=f_params)\n        G = (eta*g**2) + ((1-eta)*G)\n        p = 1.0\/(np.sqrt(G)+epsilon)*g\n        theta = theta - (alpha * p)\n        Theta.append(theta)\n    return np.array(Theta)\n","a1f22df4":"ThetaADADELTA = ADADELTA(theta=theta, grad=grad_exp, \n                         gd_params=gd_params, f_params=f_params)\nTmax=100\nplt.figure(figsize=(20,7))\nplt.plot(ThetaADADELTA[1,:Tmax], '.')\nplt.title('ADADELTA')","123ef4d9":"#ADAM\n\ndef ADAM(theta=[], grad=None, gd_params={}, f_params={}):\n   \n    epsilon= 1e-8\n    nIter    = gd_params['nIter']\n    alpha    = gd_params['alphaADAM'] \n    eta1     = gd_params['eta1']\n    eta2     = gd_params['eta2']\n    p        = np.zeros(theta.shape)\n    v        = 0.0\n    Theta    = []\n    eta1_t = eta1\n    eta2_t = eta2\n    \n    for t in range(nIter):\n        g  = grad(theta, f_params=f_params)\n        p  = eta1*p + (1.0-eta1)*g\n        v  = eta2*v + (1.0-eta2)*(g**2)\n        pg = p\/(1.-eta1_t)\n        vg = v\/(1.-eta2_t)\n        theta = (theta - alpha * pg) \/ (np.sqrt(vg)+epsilon)\n        eta1_t *= eta1\n        eta2_t *= eta2\n        Theta.append(theta)\n    return np.array(Theta)","c84f7b16":"ThetaADAM = ADAM(theta=theta, grad=grad_exp, \n                 gd_params=gd_params, f_params=f_params)\nTmax=50\nplt.figure(figsize=(20,7))\nplt.plot(ThetaADAM[:Tmax,0], '.')\nplt.title('ADAM')","1a680987":"#NADAM\n\ndef NADAM(theta=[], grad=None, gd_params={}, f_params={}):\n   \n    epsilon= 1e-8\n    nIter    = gd_params['nIter']\n    alpha    = gd_params['alphaNADAM'] \n    eta1     = gd_params['eta1']\n    eta2     = gd_params['eta2']\n    p        = np.zeros(theta.shape)\n    v        = 0.0\n    Theta    = []\n    eta1_t = eta1\n    eta2_t = eta2\n   \n    for t in range(nIter):\n        g  = grad(theta, f_params=f_params)+alpha\n        p  = g + eta1*p\n        v  = eta2*v + (1.0-eta2)*(g**2)\n        pg = p\/(1.-eta1_t)\n        vg = v\/(1.-eta2_t)\n        theta = (theta - alpha * pg) \/ (np.sqrt(vg)+epsilon)\n        eta1_t *= eta1\n        eta2_t *= eta2\n        Theta.append(theta)\n    return np.array(Theta)","67d1ec1e":"ThetaNADAM = NADAM(theta=theta, grad=grad_exp, \n                 gd_params=gd_params, f_params=f_params)\nTmax=50\nplt.figure(figsize=(20,7))\nplt.plot(ThetaNADAM[:Tmax,0], '.')\nplt.title('NADAM')","cc1a5a29":"import matplotlib as mpl\nfrom mpl_toolkits.mplot3d import Axes3D\n\nmpl.rcParams['legend.fontsize'] = 14\n\nfig = plt.figure(figsize=(15,15))\nax = fig.gca(projection='3d')\nnIter=np.expand_dims(np.arange(ThetaSGD.shape[0]),-1) \nTmax=100\n\nax.plot(ThetaSGD[:Tmax,0], ThetaSGD[:Tmax,0], nIter[:Tmax,0], label='SGD')\nax.plot(ThetaNAG[:Tmax,0], ThetaNAG[:Tmax,0], nIter[:Tmax,0], label='NAG')\nax.plot(ThetaADADELTA[:Tmax,0], ThetaADADELTA[:Tmax,0], nIter[:Tmax,0], label='ADADELTA')\nax.plot(ThetaADAM[:Tmax,0], ThetaADAM[:Tmax,0], nIter[:Tmax,0], label='ADAM')\nax.plot(ThetaNADAM[:Tmax,0], ThetaADAM[:Tmax,0], nIter[:Tmax,0], label='NADAM')\nax.legend()\nax.set_title(r'Trayectorias los par\u00e1metros calculados con distintos algoritmos')\nax.set_xlabel(r'$\\theta_1$')\nax.set_ylabel(r'$\\theta_0$')\nax.set_zlabel('Iteraci\u00f3n')\nplt.show()","bc965a4f":"** borrador **"}}