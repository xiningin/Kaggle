{"cell_type":{"355dc715":"code","f888f2ac":"code","3b559058":"code","69facd68":"code","edd0d206":"code","f1e32be1":"code","58a8e96e":"code","74cb6eee":"code","78a57fee":"code","6e858d7c":"code","b31038f0":"code","58bf2c9b":"code","acb97f6b":"code","420efea9":"code","b8b5d90c":"code","d1485895":"code","88951a2d":"code","0fe71085":"code","fcad4b55":"code","1d50fc8a":"code","db5d6f25":"code","07f6ae42":"code","e55e7122":"code","5974db0a":"code","899b2f39":"code","b70ad848":"code","d27c9174":"code","f075da81":"code","695c3eff":"code","73d9181c":"code","d5d836e6":"code","e43bf4b2":"code","f7242649":"code","6e1d6642":"code","7a438dc9":"code","c3fd8902":"code","423f9e5c":"code","7791fd98":"code","c93ce6f6":"code","52811e03":"code","a32022d5":"code","fdbfc635":"code","f1da3805":"code","a1dd1ab6":"code","27417179":"code","dfc57576":"code","23fd9ca8":"code","1fa0f4a7":"code","5803e82d":"code","15a9ff6f":"code","371275dc":"code","06da6c93":"code","92190c9b":"code","886a2475":"code","90f3bc38":"code","4bec3b2f":"code","fe86d8f2":"code","21c0f820":"code","4a110834":"code","5dfbedbc":"code","660d1bd0":"code","6208a6c3":"code","2d96f557":"code","1ceceaf6":"code","134f1a9b":"code","f87371ff":"code","728a82c0":"code","1eaacb93":"code","372a4ee2":"code","00322f00":"code","6bef515b":"code","40439372":"code","bad11eb4":"code","ac92cc42":"code","b375b9db":"code","a0f1455b":"code","312f22d7":"code","0931fdf4":"code","06510ce8":"code","830db528":"code","f1e41408":"code","857e2604":"code","37a3a91b":"code","9f968a1e":"code","251809dd":"code","aca99d75":"code","15451e48":"code","182a0bd1":"code","75efa562":"code","21fdf189":"code","7f7c8290":"code","6e0b3f2e":"code","ecea864f":"code","b764cb16":"code","bd7d110d":"markdown","0fc1a7f9":"markdown","38e8e79b":"markdown","0fa5aaad":"markdown","0fc432e3":"markdown","8703ccd7":"markdown","fa2a1ab9":"markdown","65e2de0b":"markdown","fdb9a190":"markdown","ed483fce":"markdown","f4808234":"markdown","4738389f":"markdown","4eb7e619":"markdown","4c082634":"markdown","45e1c08d":"markdown","6d18a5ee":"markdown","87887457":"markdown","806261f6":"markdown","1d84b8a9":"markdown","9f3e654a":"markdown","70d71d3a":"markdown","d3a48b18":"markdown","a706713b":"markdown","ffb256f8":"markdown","9009d2e7":"markdown","87913ee4":"markdown","23389584":"markdown","2ac00db2":"markdown","be99dd9e":"markdown","0e4a5620":"markdown","e830926a":"markdown","88d8287c":"markdown","e11ddc97":"markdown","63357cf0":"markdown","937a9852":"markdown","a1a5e11b":"markdown","da0cb5c3":"markdown","674dc3f1":"markdown","16ea7d52":"markdown","b5d0943e":"markdown","449dd70b":"markdown","29eb35e1":"markdown","554566c8":"markdown","5f961afd":"markdown","6088dece":"markdown","bb7d9196":"markdown","e0f84c97":"markdown","af14988c":"markdown","547ab05b":"markdown","52b0b3a0":"markdown","44435462":"markdown","2d2d3b39":"markdown","40da51be":"markdown","bfddfe14":"markdown","c0f0b2c0":"markdown","873d3b04":"markdown","4b7f19f6":"markdown","781e5e6e":"markdown","d22b2f08":"markdown","9f4f476a":"markdown","eaf0f404":"markdown","c45df087":"markdown","a9d6b239":"markdown","f918cb17":"markdown","6ae2a848":"markdown","8480bfcd":"markdown","921d77d8":"markdown","4c3228c0":"markdown","473dbffd":"markdown","c4430462":"markdown","9ff0d691":"markdown","5f3839fe":"markdown","403f2300":"markdown","b913b30f":"markdown","10172c9e":"markdown","a256dc9f":"markdown"},"source":{"355dc715":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n#warnings.filterwarnings(\"ignore\")\n#warnings.filterwarnings(module='sklearn*', action='ignore', category=DeprecationWarning)\n#warnings.filterwarnings(action='once')\n\nfrom sklearn.utils.testing import ignore_warnings\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","f888f2ac":"# This is used to avoid any warnings.\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn","3b559058":"#We are defining some functions which help in plotting and training\ndef get_best_score(model):\n    \n    print(model.best_score_)    \n    print(model.best_params_)\n    print(model.best_estimator_)\n    \n    return model.best_score_\n\n\ndef plot_feature_importances(model, columns):\n    nr_f = 10\n    imp = pd.Series(data = model.best_estimator_.feature_importances_, \n                    index=columns).sort_values(ascending=False)\n    plt.figure(figsize=(7,5))\n    plt.title(\"Feature importance\")\n    ax = sns.barplot(y=imp.index[:nr_f], x=imp.values[:nr_f], orient='h')","69facd68":"df_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")","edd0d206":"df_train.head()","f1e32be1":"df_test.head()","58a8e96e":"df_train.info()","74cb6eee":"df_test.info()","78a57fee":"fig, ax = plt.subplots(figsize=(9,5))\nsns.heatmap(df_train.isnull(), cbar=False, cmap=\"YlGnBu_r\")\nplt.show()","6e858d7c":"fig, ax = plt.subplots(figsize=(9,5))\nsns.heatmap(df_test.isnull(), cbar=False, cmap=\"YlGnBu_r\")\nplt.show()","b31038f0":"# We are assigning a variable for the columns.\ncols = ['Survived', 'Sex', 'Pclass', 'SibSp', 'Parch', 'Embarked']","58bf2c9b":"nr_rows = 2\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        \n        i = r*nr_cols+c       \n        ax = axs[r][c]\n        sns.countplot(df_train[cols[i]], hue=df_train[\"Survived\"], ax=ax)\n        ax.set_title(cols[i], fontsize=14, fontweight='bold')\n        ax.legend(title=\"survived\", loc='upper center') \n        \nplt.tight_layout()   ","acb97f6b":"bins = np.arange(0, 80, 5)\ng = sns.FacetGrid(df_train, row='Sex', col='Pclass', hue='Survived', margin_titles=True, size=3, aspect=1.1)\ng.map(sns.distplot, 'Age', kde=False, bins=bins, hist_kws=dict(alpha=0.6))\ng.add_legend()  \nplt.show()  ","420efea9":"df_train['Fare'].max()","b8b5d90c":"bins = np.arange(0, 550, 50)\ng = sns.FacetGrid(df_train, row='Sex', col='Pclass', hue='Survived', margin_titles=True, size=3, aspect=1.1)\ng.map(sns.distplot, 'Fare', kde=False, bins=bins, hist_kws=dict(alpha=0.6))\ng.add_legend()  \nplt.show()  ","d1485895":"sns.barplot(x='Pclass', y='Survived', data=df_train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Pclass\")\nplt.show()","88951a2d":"sns.barplot(x='Sex', y='Survived', hue='Pclass', data=df_train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Pclass and Sex\")\nplt.show()","0fe71085":"sns.barplot(x='Embarked', y='Survived', data=df_train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Embarked Port\")\nplt.show()","fcad4b55":"sns.barplot(x='Embarked', y='Survived', hue='Pclass', data=df_train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Embarked Port\")\nplt.show()","1d50fc8a":"sns.countplot(x='Embarked', hue='Pclass', data=df_train)\nplt.title(\"Count of Passengers as function of Embarked Port\")\nplt.show()","db5d6f25":"sns.boxplot(x='Embarked', y='Age', data=df_train)\nplt.title(\"Age distribution as function of Embarked Port\")\nplt.show()","07f6ae42":"sns.boxplot(x='Embarked', y='Fare', data=df_train)\nplt.title(\"Fare distribution as function of Embarked Port\")\nplt.show()","e55e7122":"cm_surv = [\"darkgrey\" , \"lightgreen\"]","5974db0a":"fig, ax = plt.subplots(figsize=(13,7))\nsns.swarmplot(x='Pclass', y='Age', hue='Survived', split=True, data=df_train , palette=cm_surv, size=7, ax=ax)\nplt.title('Survivals for Age and Pclass ')\nplt.show()","899b2f39":"fig, ax = plt.subplots(figsize=(13,7))\nsns.violinplot(x=\"Pclass\", y=\"Age\", hue='Survived', data=df_train, split=True, bw=0.05 , palette=cm_surv, ax=ax)\nplt.title('Survivals for Age and Pclass ')\nplt.show()","b70ad848":"g = sns.factorplot(x=\"Pclass\", y=\"Age\", hue=\"Survived\", col=\"Sex\", data=df_train, kind=\"swarm\", split=True, palette=cm_surv, size=7, aspect=.9, s=7)","d27c9174":"g = sns.factorplot(x=\"Pclass\", y=\"Age\", hue=\"Survived\", col=\"Sex\", data=df_train, kind=\"violin\", split=True, bw=0.05, palette=cm_surv, size=7, aspect=.9, s=7)","f075da81":"for df in [df_train, df_test] :\n    \n    df['FamilySize'] = df['SibSp'] + df['Parch'] +1\n    \n    df['Alone']=0\n    df.loc[(df.FamilySize==1),'Alone'] = 1\n    \n    df['NameLen'] = df.Name.apply(lambda x : len(x)) \n    df['NameLenBin']=np.nan\n    for i in range(20,0,-1):\n        df.loc[ df['NameLen'] <= i*5, 'NameLenBin'] = i\n    \n    \n    df['Title']=0\n    df['Title']=df.Name.str.extract(r'([A-Za-z]+)\\.') #lets extract the Salutations\n    df['Title'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],\n                    ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)","695c3eff":"print(df_train[['NameLen' , 'NameLenBin']].head(10))","73d9181c":"grps_namelenbin_survrate = df_train.groupby(['NameLenBin'])['Survived'].mean().to_frame()\ngrps_namelenbin_survrate","d5d836e6":"plt.subplots(figsize=(10,6))\nsns.barplot(x='NameLenBin' , y='Survived' , data = df_train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of NameLenBin\")\nplt.show()","e43bf4b2":"fig, ax = plt.subplots(figsize=(9,7))\nsns.violinplot(x=\"NameLenBin\", y=\"Pclass\", data=df_train, hue='Survived', split=True, \n               orient=\"h\", bw=0.2 , palette=cm_surv, ax=ax)\nplt.show()","f7242649":"g = sns.factorplot(x=\"NameLenBin\", y=\"Survived\", col=\"Sex\", data=df_train, kind=\"bar\", size=5, aspect=1.2)","6e1d6642":"grps_title_survrate = df_train.groupby(['Title'])['Survived'].mean().to_frame()\ngrps_title_survrate","7a438dc9":"plt.subplots(figsize=(10,6))\nsns.barplot(x='Title' , y='Survived' , data = df_train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of Title\")\nplt.show()","c3fd8902":"pd.crosstab(df_train.FamilySize,df_train.Survived).apply(lambda r: r\/r.sum(), axis=1).style.background_gradient(cmap='summer_r')","423f9e5c":"plt.subplots(figsize=(10,6))\nsns.barplot(x='FamilySize' , y='Survived' , data = df_train)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival as function of FamilySize\")\nplt.show()","7791fd98":"for df in [df_train, df_test]:\n\n    # Title\n    df['Title'] = df['Title'].fillna(df['Title'].mode().iloc[0])\n\n    # Age: use Title to fill missing values\n    df.loc[(df.Age.isnull())&(df.Title=='Mr'),'Age']= df.Age[df.Title==\"Mr\"].mean()\n    df.loc[(df.Age.isnull())&(df.Title=='Mrs'),'Age']= df.Age[df.Title==\"Mrs\"].mean()\n    df.loc[(df.Age.isnull())&(df.Title=='Master'),'Age']= df.Age[df.Title==\"Master\"].mean()\n    df.loc[(df.Age.isnull())&(df.Title=='Miss'),'Age']= df.Age[df.Title==\"Miss\"].mean()\n    df.loc[(df.Age.isnull())&(df.Title=='Other'),'Age']= df.Age[df.Title==\"Other\"].mean()\n    df = df.drop('Name', axis=1)\n\n\n","c93ce6f6":"# Embarked\ndf_train['Embarked'] = df_train['Embarked'].fillna(df_train['Embarked'].mode().iloc[0])\ndf_test['Embarked'] = df_test['Embarked'].fillna(df_test['Embarked'].mode().iloc[0])\n\n# Fare\ndf_train['Fare'] = df_train['Fare'].fillna(df_train['Fare'].mean())\ndf_test['Fare'] = df_test['Fare'].fillna(df_test['Fare'].mean())","52811e03":"for df in [df_train, df_test]:\n    \n    df['Age_bin']=np.nan\n    for i in range(8,0,-1):\n        df.loc[ df['Age'] <= i*10, 'Age_bin'] = i\n        \n    df['Fare_bin']=np.nan\n    for i in range(12,0,-1):\n        df.loc[ df['Fare'] <= i*50, 'Fare_bin'] = i        \n    \n    # convert Title to numerical\n    df['Title'] = df['Title'].map( {'Other':0, 'Mr': 1, 'Master':2, 'Miss': 3, 'Mrs': 4 } )\n    # fill na with maximum frequency mode\n    df['Title'] = df['Title'].fillna(df['Title'].mode().iloc[0])\n    df['Title'] = df['Title'].astype(int)        ","a32022d5":"df_train_ml = df_train.copy()\ndf_test_ml = df_test.copy()\n\npassenger_id = df_test_ml['PassengerId']","fdbfc635":"df_train_ml.info()","f1da3805":"df_test_ml.info()","a1dd1ab6":"df_train_ml = pd.get_dummies(df_train_ml, columns=['Sex', 'Embarked', 'Pclass'], drop_first=True)\ndf_test_ml = pd.get_dummies(df_test_ml, columns=['Sex', 'Embarked', 'Pclass'], drop_first=True)\n\ndf_train_ml.drop(['PassengerId','Name','Ticket', 'Cabin', 'Age', 'Fare_bin'],axis=1,inplace=True)\ndf_test_ml.drop(['PassengerId','Name','Ticket', 'Cabin', 'Age', 'Fare_bin'],axis=1,inplace=True)\n\n#df_train_ml.drop(['PassengerId','Name','Ticket', 'Cabin', 'Age_bin', 'Fare_bin'],axis=1,inplace=True)\n#df_test_ml.drop(['PassengerId','Name','Ticket', 'Cabin', 'Age_bin', 'Fare_bin'],axis=1,inplace=True)\n","27417179":"df_train_ml.dropna(inplace=True)","dfc57576":"for df in [df_train_ml, df_test_ml]:\n    df.drop(['NameLen'], axis=1, inplace=True)\n\n    df.drop(['SibSp'], axis=1, inplace=True)\n    df.drop(['Parch'], axis=1, inplace=True)\n    df.drop(['Alone'], axis=1, inplace=True)","23fd9ca8":"df_train_ml.head()","1fa0f4a7":"df_test_ml.fillna(df_test_ml.mean(), inplace=True)\ndf_test_ml.head()","5803e82d":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# for df_train_ml\nscaler.fit(df_train_ml.drop(['Survived'],axis=1))\nscaled_features = scaler.transform(df_train_ml.drop(['Survived'],axis=1))\ndf_train_ml_sc = pd.DataFrame(scaled_features) # columns=df_train_ml.columns[1::])\n\n# for df_test_ml\ndf_test_ml.fillna(df_test_ml.mean(), inplace=True)\n#scaler.fit(df_test_ml)\nscaled_features = scaler.transform(df_test_ml)\ndf_test_ml_sc = pd.DataFrame(scaled_features) # , columns=df_test_ml.columns)","15a9ff6f":"df_train_ml_sc.head()","371275dc":"df_test_ml_sc.head()","06da6c93":"df_train_ml.head()","92190c9b":"X = df_train_ml.drop('Survived', axis=1)\ny = df_train_ml['Survived']\nX_test = df_test_ml\n\nX_sc = df_train_ml_sc\ny_sc = df_train_ml['Survived']\nX_test_sc = df_test_ml_sc","886a2475":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import tree\n\n\nfrom sklearn.metrics import accuracy_score\n","90f3bc38":"from sklearn.model_selection import cross_val_score","4bec3b2f":"svc = SVC(gamma = 0.01, C = 100)\nscores_svc = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\nprint(scores_svc)\nprint(scores_svc.mean())","fe86d8f2":"svc = SVC(gamma = 0.01, C = 100)\nscores_svc_sc = cross_val_score(svc, X_sc, y_sc, cv=10, scoring='accuracy')\nprint(scores_svc_sc)\nprint(scores_svc_sc.mean())","21c0f820":"rfc = RandomForestClassifier(max_depth=5, max_features=6)\nscores_rfc = cross_val_score(rfc, X, y, cv=10, scoring='accuracy')\nprint(scores_rfc)\nprint(scores_rfc.mean())","4a110834":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom scipy.stats import uniform","5dfbedbc":"model = SVC()\nparam_grid = {'C':uniform(0.1, 5000), 'gamma':uniform(0.0001, 1) }\nrand_SVC = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=100)\nrand_SVC.fit(X_sc,y_sc)\nscore_rand_SVC = get_best_score(rand_SVC)","660d1bd0":"param_grid = {'C': [0.1,10, 100, 1000,5000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']}\nsvc_grid = GridSearchCV(SVC(), param_grid, cv=10, refit=True, verbose=1)\nsvc_grid.fit(X_sc,y_sc)\nsc_svc = get_best_score(svc_grid)","6208a6c3":"pred_all_svc = svc_grid.predict(X_test_sc)\n\nsub_svc = pd.DataFrame()\nsub_svc['PassengerId'] = df_test['PassengerId']\nsub_svc['Survived'] = pred_all_svc\nsub_svc.to_csv('svc.csv',index=False)","2d96f557":"knn = KNeighborsClassifier()\nleaf_range = list(range(3, 15, 1))\nk_range = list(range(1, 15, 1))\nweight_options = ['uniform', 'distance']\nparam_grid = dict(leaf_size=leaf_range, n_neighbors=k_range, weights=weight_options)\nprint(param_grid)\n\nknn_grid = GridSearchCV(knn, param_grid, cv=10, verbose=1, scoring='accuracy')\nknn_grid.fit(X, y)\n\nsc_knn = get_best_score(knn_grid)","1ceceaf6":"pred_all_knn = knn_grid.predict(X_test)\n\nsub_knn = pd.DataFrame()\nsub_knn['PassengerId'] = df_test['PassengerId']\nsub_knn['Survived'] = pred_all_knn\nsub_knn.to_csv('knn.csv',index=False)","134f1a9b":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier()\n\nparam_grid = {'min_samples_split': [4,7,10,12]}\ndtree_grid = GridSearchCV(dtree, param_grid, cv=10, refit=True, verbose=1)\ndtree_grid.fit(X_sc,y_sc)\n\nprint(dtree_grid.best_score_)\nprint(dtree_grid.best_params_)\nprint(dtree_grid.best_estimator_)","f87371ff":"pred_all_dtree = dtree_grid.predict(X_test_sc)\n\nsub_dtree = pd.DataFrame()\nsub_dtree['PassengerId'] = df_test['PassengerId']\nsub_dtree['Survived'] = pred_all_dtree\nsub_dtree.to_csv('dtree.csv',index=False)","728a82c0":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\n\nparam_grid = {'max_depth': [3, 5, 6, 7, 8], 'max_features': [6,7,8,9,10],  \n              'min_samples_split': [5, 6, 7, 8]}\n\nrf_grid = GridSearchCV(rfc, param_grid, cv=10, refit=True, verbose=1)\nrf_grid.fit(X_sc,y_sc)\nsc_rf = get_best_score(rf_grid)","1eaacb93":"plot_feature_importances(rf_grid, X.columns)","372a4ee2":"pred_all_rf = rf_grid.predict(X_test_sc)\n\nsub_rf = pd.DataFrame()\nsub_rf['PassengerId'] = df_test['PassengerId']\nsub_rf['Survived'] = pred_all_rf\nsub_rf.to_csv('rf.csv',index=False)","00322f00":"from sklearn.ensemble import ExtraTreesClassifier\nextr = ExtraTreesClassifier()\n\nparam_grid = {'max_depth': [6,7,8,9], 'max_features': [7,8,9,10],  \n              'n_estimators': [50, 100, 200]}\n\nextr_grid = GridSearchCV(extr, param_grid, cv=10, refit=True, verbose=1)\nextr_grid.fit(X_sc,y_sc)\nsc_extr = get_best_score(extr_grid)","6bef515b":"pred_all_extr = extr_grid.predict(X_test_sc)\n\nsub_extr = pd.DataFrame()\nsub_extr['PassengerId'] = df_test['PassengerId']\nsub_extr['Survived'] = pred_all_extr\nsub_extr.to_csv('extr.csv',index=False)","40439372":"from sklearn.ensemble import GradientBoostingClassifier\ngbdt = GradientBoostingClassifier()\n\nparam_grid = {'n_estimators': [50, 100], \n              'min_samples_split': [3, 4, 5, 6, 7],\n              'max_depth': [3, 4, 5, 6]}\ngbdt_grid = GridSearchCV(gbdt, param_grid, cv=10, refit=True, verbose=1)\ngbdt_grid.fit(X_sc,y_sc)\nsc_gbdt = get_best_score(gbdt_grid)","bad11eb4":"plot_feature_importances(gbdt_grid, X.columns)","ac92cc42":"pred_all_gbdt = gbdt_grid.predict(X_test_sc)\n\nsub_gbdt = pd.DataFrame()\nsub_gbdt['PassengerId'] = df_test['PassengerId']\nsub_gbdt['Survived'] = pred_all_gbdt\nsub_gbdt.to_csv('gbdt.csv',index=False)","b375b9db":"from xgboost import XGBClassifier\nxgb = XGBClassifier()\nparam_grid = {'max_depth': [5,6,7,8], 'gamma': [1, 2, 4], 'learning_rate': [0.1, 0.2, 0.3, 0.5]}\n\nwith ignore_warnings(category=DeprecationWarning):\n    xgb_grid = GridSearchCV(xgb, param_grid, cv=10, refit=True, verbose=1)\n    xgb_grid.fit(X_sc,y_sc)\n    sc_xgb = get_best_score(xgb_grid)","a0f1455b":"plot_feature_importances(xgb_grid, X.columns)","312f22d7":"with ignore_warnings(category=DeprecationWarning):\n    pred_all_xgb = xgb_grid.predict(X_test_sc)\n\nsub_xgb = pd.DataFrame()\nsub_xgb['PassengerId'] = df_test['PassengerId']\nsub_xgb['Survived'] = pred_all_xgb\nsub_xgb.to_csv('xgb.csv',index=False)","0931fdf4":"from sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier()\n\nparam_grid = {'n_estimators': [30, 50, 100], 'learning_rate': [0.08, 0.1, 0.2]}\nada_grid = GridSearchCV(ada, param_grid, cv=10, refit=True, verbose=1)\nada_grid.fit(X_sc,y_sc)\nsc_ada = get_best_score(ada_grid)\n\npred_all_ada = ada_grid.predict(X_test_sc)","06510ce8":"sub_ada = pd.DataFrame()\nsub_ada['PassengerId'] = df_test['PassengerId']\nsub_ada['Survived'] = pred_all_ada\nsub_ada.to_csv('ada.csv',index=False)","830db528":"from catboost import CatBoostClassifier\ncat=CatBoostClassifier()\n\nparam_grid = {'iterations': [100, 150], 'learning_rate': [0.3, 0.4, 0.5], 'loss_function' : ['Logloss']}\n\ncat_grid = GridSearchCV(cat, param_grid, cv=10, refit=True, verbose=1)\ncat_grid.fit(X_sc,y_sc, verbose=False)\nsc_cat = get_best_score(cat_grid)\n\npred_all_cat = cat_grid.predict(X_test_sc)","f1e41408":"sub_cat = pd.DataFrame()\nsub_cat['PassengerId'] = df_test['PassengerId']\nsub_cat['Survived'] = pred_all_cat\nsub_cat.to_csv('cat.csv',index=False)","857e2604":"import lightgbm as lgb\nlgbm = lgb.LGBMClassifier(silent=False)\nparam_grid = {\"max_depth\": [8,10,15], \"learning_rate\" : [0.008,0.01,0.012], \n              \"num_leaves\": [80,100,120], \"n_estimators\": [200,250]  }\nlgbm_grid = GridSearchCV(lgbm, param_grid, cv=10, refit=True, verbose=1)\nlgbm_grid.fit(X_sc,y_sc, verbose=True)\nsc_lgbm = get_best_score(lgbm_grid)\n\npred_all_lgbm = lgbm_grid.predict(X_test_sc)","37a3a91b":"sub_lgbm = pd.DataFrame()\nsub_lgbm['PassengerId'] = df_test['PassengerId']\nsub_lgbm['Survived'] = pred_all_lgbm\nsub_lgbm.to_csv('lgbm.csv',index=False)","9f968a1e":"from sklearn.ensemble import VotingClassifier","251809dd":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\n\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(random_state=1)\nclf3 = GaussianNB()\n\neclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')\n\nparams = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200],}\n\nwith ignore_warnings(category=DeprecationWarning):\n    votingclf_grid = GridSearchCV(estimator=eclf, param_grid=params, cv=10)\n    votingclf_grid.fit(X_sc,y_sc)\n    sc_vot1 = get_best_score(votingclf_grid)","aca99d75":"clf4 = GradientBoostingClassifier()\nclf5 = SVC()\nclf6 = RandomForestClassifier()\n\neclf_2 = VotingClassifier(estimators=[('gbdt', clf4), \n                                      ('svc', clf5), \n                                      ('rf', clf6)], voting='soft')\n\nparams = {'gbdt__n_estimators': [50], 'gbdt__min_samples_split': [3],\n          'svc__C': [10, 100] , 'svc__gamma': [0.1,0.01,0.001] , 'svc__kernel': ['rbf'] , 'svc__probability': [True],  \n          'rf__max_depth': [7], 'rf__max_features': [2,3], 'rf__min_samples_split': [3] } \n\nwith ignore_warnings(category=DeprecationWarning):\n    votingclf_grid_2 = GridSearchCV(estimator=eclf_2, param_grid=params, cv=10)\n    votingclf_grid_2.fit(X_sc,y_sc)\n    sc_vot2_cv = get_best_score(votingclf_grid_2)","15451e48":"with ignore_warnings(category=DeprecationWarning):    \n    pred_all_vot2 = votingclf_grid_2.predict(X_test_sc)\n\nsub_vot2 = pd.DataFrame()\nsub_vot2['PassengerId'] = df_test['PassengerId']\nsub_vot2['Survived'] = pred_all_vot2\nsub_vot2.to_csv('vot2.csv',index=False)","182a0bd1":"from mlxtend.classifier import StackingClassifier","75efa562":"# Initializing models\nclf1 = xgb_grid.best_estimator_\nclf2 = gbdt_grid.best_estimator_\nclf3 = rf_grid.best_estimator_\nclf4 = svc_grid.best_estimator_\n\nlr = LogisticRegression()\nst_clf = StackingClassifier(classifiers=[clf1, clf1, clf2, clf3, clf4], meta_classifier=lr)\n\nparams = {'meta-logisticregression__C': [0.1,1.0,5.0,10.0] ,\n          #'use_probas': [True] ,\n          #'average_probas': [True] ,\n          'use_features_in_secondary' : [True, False]\n         }\n\nwith ignore_warnings(category=DeprecationWarning):\n    st_clf_grid = GridSearchCV(estimator=st_clf, param_grid=params, cv=5, refit=True)\n    st_clf_grid.fit(X_sc, y_sc)\n    sc_st_clf = get_best_score(st_clf_grid)\n    pred_all_stack = st_clf_grid.predict(X_test_sc)","21fdf189":"list_scores = [sc_knn, sc_rf, sc_extr, sc_svc, sc_gbdt, sc_xgb, \n               sc_ada, sc_cat, sc_lgbm, sc_vot2_cv, sc_st_clf]\n\nlist_classifiers = ['KNN','RF','EXTR','SVC','GBDT','XGB',\n                    'ADA','CAT','LGBM','VOT2','STACK']","7f7c8290":"score_subm_svc   = 0.80861\nscore_subm_vot2  = 0.78947\nscore_subm_ada   = 0.78468\nscore_subm_lgbm  = 0.78468\nscore_subm_rf    = 0.77990\nscore_subm_xgb   = 0.77033\nscore_subm_dtree = 0.76076\nscore_subm_extr  = 0.76076\nscore_subm_gbdt  = 0.74641\nscore_subm_knn   = 0.69856\n             \n\nscore_subm_cat = 0.5                #TODO: int\n","6e0b3f2e":"fig, ax = plt.subplots()\nfig.set_size_inches(10,7)\nsns.barplot(x=list_scores, y=list_classifiers, ax=ax)\nplt.xlabel('Score: Accuracy')\nplt.show()","ecea864f":"predictions = {'KNN': pred_all_knn, 'RF': pred_all_rf, 'EXTR': pred_all_extr, \n               'SVC': pred_all_svc, 'GBDT': pred_all_gbdt, 'XGB': pred_all_xgb, \n               'ADA': pred_all_ada, 'CAT': pred_all_cat, 'LGBM': pred_all_lgbm, \n               'VOT2': pred_all_vot2, 'STACK': pred_all_stack}\ndf_predictions = pd.DataFrame(data=predictions) \ndf_predictions.corr()","b764cb16":"plt.figure(figsize=(9, 9))\nsns.set(font_scale=1.25)\nsns.heatmap(df_predictions.corr(), linewidths=1.5, annot=True, square=True, \n                fmt='.2f', annot_kws={'size': 10}, \n                yticklabels=df_predictions.columns , xticklabels=df_predictions.columns\n            )\nplt.yticks(rotation=0)\nplt.show()","bd7d110d":"**convert categorical to numerical : get_dummies**","0fc1a7f9":"**Chance to survive increases with length of name for all Passenger classes**","38e8e79b":"### Review: k fold cross validation  \njust a short review of this technique that we already studied in the first kernel","0fa5aaad":"### lightgbm LGBM","0fc432e3":"### New Feature: Title","8703ccd7":"### Decision Tree","fa2a1ab9":"**submission for knn**","65e2de0b":"### First Voting  \nfor the first voting ensemble I use three simple models (LR, RF, GNB)","fdb9a190":"### CatBoost\nlibrary for gradient boosting on decision trees with categorical features support","ed483fce":"# Part 0: Imports, Functions","f4808234":"### KNN","4738389f":"### StackingClassifier","4eb7e619":"# Part 1: Exploratory Data Analysis","4c082634":"### VotingClassifier","45e1c08d":"### Gradient Boost Decision Tree GBDT \n","6d18a5ee":"**some useful functions**","87887457":"### Comparison plot for best models","806261f6":"### Bar and Box plots","1d84b8a9":"### eXtreme Gradient Boosting - XGBoost","9f3e654a":"### SVC : GridSearchCV","70d71d3a":"**This is my second notebook for the Titanic classification competition.**\n\nIf you are new to Machine Learning, have a look at  **[my first Titanic notebook](https:\/\/www.kaggle.com\/dejavu23\/titanic-survival-for-beginners-eda-to-ml)** where  I studied the  \nbasics of EDA with Pandas and Matplotlib and how to do Classification with the scikit-learn library.  ","d3a48b18":"### Swarm and Violin plots\nAlthough the following swarm and violin plots show the same data like the countplots or distplots before,  \nthey can reveal ceratin details that disappear in other plots. However, it takes more time to study these plots in detail.","a706713b":"Passengers embarked in \"C\" had largest proportion of Pclass 1 tickets.  \nAlmost all Passengers embarked in \"Q\" had Pclass 3 tickets.  \nFor every class, the largest count of Passengers  embarked in \"S\".","ffb256f8":"### ExtraTreesClassifier","9009d2e7":"**References**  \nWhile this notebook contains some work work based on my ideas, it is also a collection of approaches  \nand techniques from these kaggle notebooks:","87913ee4":"But survival rate alone is not good beacuse its uncertainty depends on the number of samples.  \nWe also need to consider the total number (count) of passengers that embarked.","23389584":"Here, the high survival rate for kids in Pclass 2 is easily observed.  \nAlso, it becomes more obvious that for passengers older than 40 the best chance to survive is in Pclass 1,  \nand smallest chance in Pclass 3   ","2ac00db2":"### Ada Boost  ","be99dd9e":"### RFC, features not scaled  ","0e4a5620":"**submission for random forest**","e830926a":"Mean fare for Passengers embarked in \"C\" was higher.","88d8287c":"### Standard Scaler","e11ddc97":"As we know from the first Titanic kernel, survival rate decreses with Pclass.  \nThe hue parameter lets us see the difference in survival rate for male and female. ","63357cf0":"**Looks like there is very strong correlation of Survival rate and Name length**","937a9852":"With this kernel we studied EDA with **Seaborn** including some unusual plots like violin and swarm.   \nBased on the EDA we filled missing values according to related features and developed new features   \n(**Feature Engineering**) to improve model performance.  \nIn Part 3 we learned basics of applying **ensemble models** for classification like **Boosting**, **Stacking** and **Voting**.   \nFor this we applied libraries like: **sklearn, mlxtend, lightgbm, catboost, xgboost**","a1a5e11b":"### SVC, features not scaled  \nSupport Vector Machine Classifier","da0cb5c3":"### Second Voting\n\nfor the 2nd voting ensemble I use the three models (together with the optimal parameters found by GridSearchCV)  \nthat had the best test score based on the cross validations above ","674dc3f1":"### Correlation of prediction results","16ea7d52":"**In the following we apply GridSearchCV and RandomizedSearchCV for these Classification models:**  \n**KNN, Decision Tree, Random Forest, SVC**","b5d0943e":"Default mode for seaborn barplots is to plot the mean value for the category.  \nAlso, the standard deviation is indicated.  \nSo, if we choose Survived as y-value, we get a plot of the survival rate as function   \nof the categories present in the feature chosen as x-value.","449dd70b":"### SVC : RandomizedSearchCV","29eb35e1":"**submission for ExtraTreesClassifier**","554566c8":"**loading the data**","5f961afd":"**Boxplot**","6088dece":"## Feature Engineering\n**New Features: 'FamilySize'  ,  'Alone' , 'NameLen' and 'Title'**","bb7d9196":"## Data Wrangling","e0f84c97":"### SVC, features scaled  ","af14988c":"### Random Forest","547ab05b":"Of the 891 passengers in df_test, less than 350 survive.  \nMuch more women survive than men.  \nAlso, the chance to survive is much higher in Pclass 1 and 2 than in Class 3.  \nSurvival rate for passengers travelling with SibSp or Parch is higher than for those travelling alone.  \nPassengers embarked in C and Q are more likely to survie than those embarked in S.","52b0b3a0":"Passengers embarked in \"S\" had the lowest survival rate, those embarked in \"C\" the highest.  \nAgain, with hue we see the survival rate as function of Embarked and Pclass.","44435462":"## Hyperparameter tuning with RandomizedSearchCV and GridSearchCV","2d2d3b39":"**RandomizedSearchCV  and GridSearchCV apply k fold cross validation on a chosen set of parameters**\n**and then find the parameters that give the best performance.**  \nFor GridSearchCV, all possible combinations of the specified parameter values are tried out, resulting in a parameter grid.  \nFor RandomizedSearchCV, a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter.","40da51be":"# Part 2: Data Wrangling and Feature Engineering","bfddfe14":"This violinplot shows exactly the same info like the swarmplot before.","c0f0b2c0":"### New Feature: Family size","873d3b04":"### Seaborn heatmaps  \nmissing data in df_train and df_test","4b7f19f6":"### Seaborn Countplots  \nfor all categorical columns","781e5e6e":"**submission for GradientBoostingClassifier**","d22b2f08":"Best chances to survive for male passengers was in Pclass 1 or being below 5 years old.  \nLowest survival rate for female passengers was in Pclass 3 and being older than 40.  \nMost passengers were male, in Pclass 3 and between 15-35 years old.","9f4f476a":"<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/6e\/St%C3%B6wer_Titanic.jpg\" width=\"520\">","eaf0f404":"### **Conclusion**","c45df087":"**Increase of survival rate with length of name most important for male passengers**","a9d6b239":"### submission scores","f918cb17":"**submission for svc**","6ae2a848":"### New Feature: NameLenBin","8480bfcd":"Looks like the feature Cabin has lots of missing data, also some data for Age and Embarked is missing.  \nLets plot the seaborn heatmap of the isnull matrix for the train and test data","921d77d8":"**submission for decision tree**","4c3228c0":"### Bining for Age and Fare, convert Title to numerical","473dbffd":"### scores from GridSearchCV","c4430462":"**Fill NaN with mean or mode**","9ff0d691":"# Titanic Survival: Seaborn and Ensembles\n\n\n**[Part 0: Imports, Functions](#Part-0:-Imports,-Functions)** \n\n**[Part 1: Exploratory Data Analysis](#Part-1:-Exploratory-Data-Analysis)** \n\n* Seaborn [heatmaps](#Seaborn-heatmaps) : missing data in df_train and df_test\n* Seaborn [countplots](#Seaborn-Countplots) : Number of (Non-)Survivors as function of features\n* Seaborn [distplots](#Seaborn-Distplots) : Distribution of Age and Fare as function of Pclass, Sex and Survived  \n* [Bar and Box plots](#Bar-and-Box-plots) for categorical features : Pclass and Embarked\n* Seaborn [violin and swarm plots](#Swarm-and-Violin-plots) : Survivals as function of Age, Pclass and Sex\n\n**[Part 2: Data Wrangling and Feature Engineering](#Part-2:-Data-Wrangling-and-Feature-Engineering)**  \n\n* [Feature Engineering](#Feature-Engineering): include new features to improve the performance of the classifiers and to fill missing values:  \nFamily size, Alone, Name length, Title\n* [Data Wrangling](#Data-Wrangling): fill NaN, convert categorical to numerical, [Standard Scaler](#Standard-Scaler), create X, y and X_test for Part 3\n\n\n**[Part 3: Optimization of Classifier parameters, Boosting, Voting and Stacking](#Part-3:-Optimization-of-Classifier-parameters,-Boosting,-Voting-and-Stacking)**  \n\n* Review: [k fold cross validation](#Review:-k-fold-cross-validation) for SVC and Random Forest: \n * SVC, features not scaled \n * SVC, features scaled \n * Random Forest Classifier, RFC, features not scaled \n* Hyperparameter tuning with GridSearchCV and RandomizedSearchCV for:  \n * Support Vector Machine Classifier, [SVC](#SVC-:-RandomizedSearchCV) \n * K Nearest Neighbor, [KNN](#KNN)\n * [Decision Tree](#Decision-Tree)\n * [Random Forest Classifier](#Random-Forest), RFC\n\n* study Ensemble models like Boosting, Stacking and Voting:\n\n * [ExtraTreesClassifier](#ExtraTreesClassifier)\n * Gradient Boost Decision Tree - [GBDT](#Gradient-Boost-Decision-Tree-GBDT)\n * eXtreme Gradient Boosting - [XGBoost](#eXtreme-Gradient-Boosting---XGBoost)   \n * Adaptive Boosting - [AdaBoost](#Ada-Boost)\n * [CatBoost](#CatBoost)\n * lightgbm [LGBM](#lightgbm-LGBM)\n * Voting: [VotingClassifier 1](#First-Voting), [VotingClassifier 2](#Second-Voting)  \n * Stacking : [StackingClassifier](#StackingClassifier)  \n* Compare Classifier performance based on the validation score : [comparison plot](#Comparison-plot-for-best-models)\n* Correlation of prediction results : [correlation matrix](#Correlation-of-prediction-results)","5f3839fe":"# Part 3: Optimization of Classifier parameters, Boosting, Voting and Stacking","403f2300":"**double-check for missing values**","b913b30f":"Highest survival rate (>0.9) for women in Pclass 1 or 2.  \nLowest survival rate (<0.2) for men in Pclass 3.","10172c9e":"**Disribution of Fare as function of Pclass, Sex and Survived**","a256dc9f":"### Seaborn Distplots \n**Distribution of Age as function of Pclass, Sex and Survived**"}}