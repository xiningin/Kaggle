{"cell_type":{"e07dd636":"code","ae82a583":"code","dc448f25":"code","b4b7ef87":"code","810048f3":"code","c2947a84":"code","3caecfd5":"code","dfa135e8":"code","cb948268":"code","809aab80":"code","22ba6eea":"code","2bfcb9f3":"code","ea0c20c9":"code","f7bcf9a4":"code","1a4bf49b":"code","70dddee8":"code","d071e549":"code","fe95e4a3":"code","cffb7c22":"code","ccea4fb6":"code","63217b01":"code","a025af12":"code","ecd65224":"code","4848af0d":"code","5fb92b72":"code","f71ae361":"code","b3bde747":"markdown","ee5bbbd7":"markdown","0325c346":"markdown","c1cd5977":"markdown","4564f019":"markdown"},"source":{"e07dd636":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ae82a583":"# import main libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","dc448f25":"# import data set\n\ndf = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","b4b7ef87":"# first five rows of data set\ndf.head(5)","810048f3":"# last five rows of dataset\ndf.tail(5)","c2947a84":"# data information\n\ndf.info()","3caecfd5":"# lets check does data contains null values \ndf.isnull().sum()","dfa135e8":"# lets check how the values are deviated from mean and figure out how to fill bmi features nan values\ndf.describe()","cb948268":"# so we can fill nan values in bmi column with mean because data is in numeric and the values perfectly divated from mean \ndf['bmi']=df['bmi'].fillna(df['bmi'].mean())","809aab80":"# lets convert strings into numeric format using label encoder\n\nfrom sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()\n\ndf['gender']=label.fit_transform(df['gender'])\ndf['ever_married']= label.fit_transform(df['ever_married'])\ndf['work_type']= label.fit_transform(df['work_type'])\ndf['Residence_type']= label.fit_transform(df['Residence_type'])\ndf['smoking_status']= label.fit_transform(df['smoking_status'])","22ba6eea":"df.head(5)","2bfcb9f3":"df=df.drop('id', axis=1)","ea0c20c9":"plt.figure(figsize =(7,5))\ntarget = [len(df[df['stroke']==0]), len(df[df['stroke']==1])]\nlabels = ['no stroke', 'stroke']\ncolors = ['green','red']\nexplode =(0.05,0.1)\n\nplt.pie(target,explode= explode,labels = labels, colors = colors,autopct='%4.2f%%', shadow = True,startangle=45)\nplt.title('stroke percentage')\nplt.axis('equal')\nplt.show()","f7bcf9a4":"# lets visualize the dataset using bins \n\nplt.figure(figsize=(15,15))\n\nfor i, column in enumerate(df,1):\n    plt.subplot(4,4,i)\n    df[df['stroke']==0][column].hist(bins=35, color='blue',label='no stroke', alpha = 0.8)\n    df[df['stroke']==1][column].hist(bins=35,color='red',label =' stroke', alpha =0.8)\n    plt.legend()\n    plt.xlabel(column)","1a4bf49b":"# lets see correlation with feaature using correlation function","70dddee8":"cor = df.corr()\nfeatures = cor.index\nplt.figure(figsize=(15,15))\n\nheat = sns.heatmap(df[features].corr(),annot= True, cmap='RdYlGn')","d071e549":"# first we will train the data set\n\nX= df.drop(['stroke'],axis=1)\ny=df['stroke']\nX","fe95e4a3":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train,y_test=train_test_split(X,y,test_size=0.30,random_state=25)","cffb7c22":"#smote  function\n\nfrom imblearn.over_sampling import SMOTE\nsmote =SMOTE(random_state=2)\nX_train,y_train=smote.fit_resample(X_train,y_train.ravel())","ccea4fb6":"from xgboost import XGBClassifier","63217b01":"xg= XGBClassifier(objective='binary:logistic',max_depth=25,n_estimators=150,learning_rate =0.05,\n                  eta=0.01,random_state=5,use_label_encoder=False,eval_metric='logloss')\nxg= xg.fit(X,y)","a025af12":"predict =xg.predict(X_test)","ecd65224":"xg.score(X_test,y_test)","4848af0d":"# lets check model performance using f1 score \nfrom sklearn.metrics import f1_score,confusion_matrix, classification_report\nf1= f1_score(y_test,predict)\nf1","5fb92b72":"cm = confusion_matrix(y_test,predict)\ncm","f71ae361":"print(classification_report(y_test,predict))","b3bde747":"# we can clearly see that 95% is not effected by stroke and 5 percentage effected by stroke data set is not balanced","ee5bbbd7":"## we know that data set is not balanced so by using smote function we will figure it out","0325c346":"#                 THANKS YOU","c1cd5977":"# clearly we can see that which feature is more correlated with stroke ","4564f019":"### we can see that some the features are object lets convert them into integer before that we will fill null values and make the data more worthable"}}