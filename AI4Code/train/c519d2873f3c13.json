{"cell_type":{"736e10b2":"code","dfc8aaaa":"code","9e655632":"code","cfdbb8bd":"code","0b3f483e":"code","29140bdd":"code","50eee8b7":"code","db59479b":"code","2fff886f":"code","d4f8c22e":"code","c46a23f2":"code","5fa264c2":"code","5303ff22":"code","2d01c65b":"code","3000f935":"code","30a77027":"code","073f4790":"code","7f65315c":"code","84455d4b":"code","301cf8c1":"code","3f398d0f":"code","e2232d5a":"code","db43c10a":"code","2a7d4c5b":"code","8709fb1a":"code","eb681e02":"code","68e34d06":"code","4c574088":"code","8739a6a4":"code","23ef1f91":"code","c2857206":"code","be675b41":"code","14d92880":"code","b3943b09":"markdown","ef09345f":"markdown","1355cf2e":"markdown","353762f9":"markdown","93b7c3a5":"markdown","718b1a7f":"markdown","7010f4d5":"markdown","8d7add66":"markdown","4adce288":"markdown","f758be49":"markdown","24b1d77c":"markdown","e9921f3f":"markdown","e590e86a":"markdown","1f4ce903":"markdown","c1c3ca56":"markdown","3e8d3c99":"markdown","4b4dbb79":"markdown","783d6013":"markdown","9f88716d":"markdown","992e6ddf":"markdown","0c74909d":"markdown","efacb089":"markdown","69ea8740":"markdown","873172d5":"markdown","bd717b38":"markdown","0dcd331f":"markdown","cbf9a07c":"markdown","0854a7d9":"markdown","4d153aec":"markdown","7c78e681":"markdown"},"source":{"736e10b2":"# Data Processing\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import KFold,cross_validate, cross_val_score, train_test_split, GridSearchCV\nfrom sklearn.pipeline import make_pipeline, Pipeline\n\n# Data Visualizing\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Data Modeling\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, BayesianRidge, LassoLarsIC\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.svm import SVR\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n\n# Data Evalutation\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, roc_curve\n\n# Math\nimport math\nfrom scipy.stats import norm\nfrom scipy import stats\n\n# Warning Removal\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)","dfc8aaaa":"# Let's import and put the train and test datasets in  pandas dataframe\ntrain = pd.read_csv('..\/input\/house-prices-data\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-data\/test.csv')\n\n# Drop the 'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n# Concatenate both train and test dataset\nall_data = pd.concat((train, test)).reset_index(drop=True)","9e655632":"# Check the overall insight of the dataset\nall_data.describe()","cfdbb8bd":"# Check to see Null values and Data Type of each feature\nall_data.info(verbose=True)","0b3f483e":"# Target is continuous variable, using correlation to know which features may help prediction\nall_data.corr()['SalePrice'].sort_values(ascending = False)","29140bdd":"fig = plt.figure()\nsns.scatterplot(x=train['OverallQual'], y=train['SalePrice'])\n\nfig = plt.figure()\nsns.boxplot(x=train['OverallQual'], y=train['SalePrice'])\n\nfig = plt.figure()\nsns.scatterplot(x=train['GrLivArea'], y=train['SalePrice'])","50eee8b7":"# Remove outliers in dataset\ntrain.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index, inplace=True)\n\n# df[(df['GrLivArea']>4500) & (df['SalePrice']<300000)]","db59479b":"sns.scatterplot(x=train['GrLivArea'], y=train['SalePrice'], hue=train['OverallQual'])","2fff886f":"sns.scatterplot(x=train['ExterQual'], y=train['SalePrice'])\n\nfig = plt.figure()\nsns.scatterplot(x=train['GrLivArea'], y=train['SalePrice'], hue=train['ExterQual'])","d4f8c22e":"sns.distplot(train['SalePrice'])\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","c46a23f2":"#We use the numpy fuction log1p which applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n# plot the histogram.\nsns.distplot(train['SalePrice'], hist=True, kde=True, fit=norm, color='#e74c3c')\n\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","5fa264c2":"zero_count = (train.isnull()).sum().sort_values(ascending=False) # (df == 0).sum() # \nzero_count_df = pd.DataFrame(zero_count)\nzero_count_df.drop('SalePrice', axis=0, inplace=True)\nzero_count_df.columns = ['count_0']\n\n# https:\/\/stackoverflow.com\/questions\/31859285\/rotate-tick-labels-for-seaborn-barplot\/60530167#60530167\nsns.set(style='whitegrid')\nplt.figure(figsize=(13,8))\nsns.barplot(x=zero_count_df.index, y=zero_count_df['count_0'])\nplt.xticks(rotation=90)","5303ff22":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train['SalePrice']\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","2d01c65b":"# MSSubClass: missing value means No Building class\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n\n# The size and area of individual house in the same neighborhood tend to be similar,\n# thus filling missing value with the median neighborhood LotFronttage \nall_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].apply(lambda x: x.fillna(x.median()))\n\n# Alley: NA means No Alley\nall_data['Alley'].fillna('None', inplace=True)\n\n# MasVnrType,MasVnrArea: NA means no masonry veneer for these houses\nall_data['MasVnrType'].fillna('None', inplace=True)\nall_data['MasVnrArea'] = all_data.groupby('MasVnrType')['MasVnrArea'].apply(lambda x: x.fillna(x.median()))\n\n# BsmtQual,BsmtCond,BsmtExposure: \n# Using Tableau Prep or visualization to check their distribution and fill with most common values.\nall_data['BsmtQual'].fillna('TA', inplace=True)\nall_data['BsmtCond'].fillna('TA', inplace=True)\nall_data['BsmtExposure'].fillna('No', inplace=True)\n\n# BsmtFinType1, BsmtFinType2:\n# Using Tableau Prep or visualization to check their distribution and fill with most common values.\n# BsmtFinSF1,BsmtFinSF2,BsmtUnfSF:\n# Using groupby on BsmtFinSF1 and BsmtFinSF2 to fill nan with median value.\n# BsmtUnfSF: means either having no Bsmt or being built. Therefor fill nan with either None or median\nall_data['BsmtFinType1'].fillna('Rec', inplace=True)\nall_data['BsmtFinSF1'] = all_data.groupby('BsmtFinType1')['BsmtFinSF1'].apply(lambda x: x.fillna(x.median()))\nall_data['BsmtFinType2'].fillna('Rec', inplace=True)\nall_data['BsmtFinSF2'] = all_data.groupby('BsmtFinType2')['BsmtFinSF2'].apply(lambda x: x.fillna(x.median()))\nall_data['BsmtUnfSF'] = all_data['BsmtUnfSF'].fillna(all_data['BsmtUnfSF'].median())\n\n# There is a certain correlation between GrLivArea and TotalBsmtSF. \n# Taking all non-value of GrLivArea and TotalBsmtSF into equation: average of GrLivArea\/TotalBsmtSF\nall_data['TotalBsmtSF'] = all_data['TotalBsmtSF'].fillna(all_data['GrLivArea']\/1.5)\n\n# Electrical: fill nan with most common value\nall_data['Electrical'].fillna(all_data['Electrical'].mode()[0], inplace=True)\n\n# BsmtFullBath,BsmtHalfBath: fill nan with most common value\nall_data['BsmtFullBath'].fillna(all_data['BsmtFullBath'].mode()[0], inplace=True)\nall_data['BsmtHalfBath'].fillna(all_data['BsmtHalfBath'].mode()[0], inplace=True)\n\n# Functional: fill nan with most common value\nall_data['Functional'].fillna(all_data['Functional'].mode()[0], inplace=True)\n\n# FireplaceQu: fill nan with most common value\nall_data['FireplaceQu'].fillna('None', inplace=True)\n\n# GarageType: fill nan with None, according to description\nall_data['GarageType'].fillna('No', inplace=True)\n# GarageYrBlt: Garage is either built in the same year with the house or no Garage at all.\nall_data['GarageYrBlt'].fillna(all_data['YearBuilt'], inplace=True)\n\n# GarageFinish: fill nan with no for not having finish\n# GarageCars, GarageArea: fill nan with median value\n# GarageQual, GarageCond: fill nan based on OveralQual distribution\nall_data['GarageFinish'].fillna('No', inplace=True)\nall_data['GarageCars'].fillna(all_data['GarageCars'].median(), inplace=True)\nall_data['GarageArea'].fillna(all_data['GarageArea'].median(), inplace=True)\nall_data['GarageQual'].fillna('TA', inplace=True)\nall_data['GarageCond'].fillna('TA', inplace=True)\n\n# PoolQC,Fence,MiscFeature: fill nan with None \nall_data['PoolQC'].fillna('None', inplace=True)\nall_data['Fence'].fillna('None', inplace=True)\nall_data['MiscFeature'].fillna('None', inplace=True)\n\n# Exterior1st,Exterior2nd,SaleType: fill nan with the most common value\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'].fillna(all_data['SaleType'].mode()[0], inplace=True)\n\n# Utilities: Having the majority of AllPub, and a few of NoSeWa. Thus removing is safe\nall_data = all_data.drop(['Utilities'], axis=1)","3000f935":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)","30a77027":"# Getting dummies categorical features\nall_data = pd.get_dummies(all_data)\nprint(all_data.shape)","073f4790":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","7f65315c":"# Pipeline for Scaler + Model: https:\/\/stackoverflow.com\/questions\/43366561\/use-sklearns-gridsearchcv-with-a-pipeline-preprocessing-just-once\n# Metrics in GridSearchCV: https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n\ndef Best_Parameters_model(est, para):\n    model_table = {}\n    \n    MLA_name = est.__class__.__name__\n    model_table['Model Name'] = MLA_name\n\n    pipe = make_pipeline(RobustScaler(), GridSearchCV(estimator=est,\n                                              param_grid=para,\n                                              scoring='neg_root_mean_squared_error',\n                                              cv=10,\n                                              verbose=0, refit=True))\n    pipe_result = pipe.fit(train, y_train)\n\n    model_table['Best Test Accuracy Mean'] = pipe_result[1].best_score_\n    model_table['Best Parameters'] = pipe_result[1].best_params_\n        \n    return model_table\n\n# ridge = Ridge(random_state=0)\n# ridge_para = {'alpha':[0.9, 0.5, 0.1, 0.01, 0.001]}\n# Best_Parameters_model(ridge, ridge_para)\n\n# ridge = Ridge(random_state=0)\n# ridge_para = {'alpha':[0.9, 0.5, 0.1, 0.01, 0.001]}\n# Best_Parameters_model(ridge, ridge_para)\n\n# ENet = ElasticNet(random_state=0)\n# ENet_para = {'alpha': [0.1, 0.01, 0.0001],\n#             'l1_ratio': [0.9, 0.5, 0.1]}\n# Best_Parameters_model(ENet, ENet_para)","84455d4b":"def rmsle_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, train, y_train, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse.mean())","301cf8c1":"lasso = Lasso(alpha=0.0008,random_state=0)\nrmsle_cv(lasso)","3f398d0f":"ridge = Ridge(alpha=15,random_state=0)\nrmsle_cv(ridge)","e2232d5a":"ENet = ElasticNet(alpha=0.01, l1_ratio=0.05,random_state=0)\nrmsle_cv(ENet)","db43c10a":"lasso_coeficient = Lasso(alpha=0.0008,random_state=0).fit(train, y_train).coef_\nprint(\"Lasso picked \" + str(sum(lasso_coeficient != 0)) + \" variables and eliminated the other \" +  str(sum(lasso_coeficient == 0)) + \" variables\")\n\nridge_coeficient = Ridge(alpha=15,random_state=0).fit(train, y_train).coef_\nprint(\"Lasso picked \" + str(sum(ridge_coeficient != 0)) + \" variables and eliminated the other \" +  str(sum(ridge_coeficient == 0)) + \" variables\")\n\nENet_coeficient = ElasticNet(alpha=0.01, l1_ratio=0.05,random_state=0).fit(train, y_train).coef_\nprint(\"Lasso picked \" + str(sum(ENet_coeficient != 0)) + \" variables and eliminated the other \" +  str(sum(ENet_coeficient == 0)) + \" variables\")","2a7d4c5b":"# Tuning hyperparameter: kaggle.com\/duonghoanvu1\/houseprice\/edit\nGBoost = GradientBoostingRegressor(criterion='friedman_mse',\n                                   learning_rate=0.01,\n                                   loss= 'huber',\n                                   max_depth=4,\n                                   max_features='sqrt',\n                                   min_samples_split=5,\n                                   n_estimators=4000,\n                                   subsample=0.9,\n                                   random_state = 0)\nrmsle_cv(GBoost)","8709fb1a":"# Tuning hyperparameter: kaggle.com\/duonghoanvu1\/houseprice1\/edit\nXGBoost = XGBRegressor(n_estimators=2500,\n                     max_depth=5,\n                     num_leaves=30,\n                     learning_rate=0.01,\n                     booster='gbtree',\n                     objective='reg:squarederror',\n                     subsample=0.7,\n                     colsample_bytree=0.8,\n                     gamma=0.001,\n                     random_state=0, \n                     n_jobs=-1)\nrmsle_cv(XGBoost)","eb681e02":"# Tuning hyperparameter: kaggle.com\/duonghoanvu1\/houseprice2\/edit\nLGBM = LGBMRegressor(n_estimators=2500,\n                     max_depth=5,\n                     num_leaves=30,\n                     learning_rate=0.01,\n                     boosting_type='gbdt',\n                     objective='regression',\n                     subsample=0.55,\n                     colsample_bytree=0.70,\n                     reg_alpha=0.01,\n                     random_state=0, \n                     n_jobs=-1)\nrmsle_cv(LGBM)","68e34d06":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","4c574088":"averaged_models = AveragingModels(models = (ridge, lasso, ENet, GBoost))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f}\\n\".format(score))","8739a6a4":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","23ef1f91":"averaged_models.fit(train.values, y_train)\naveraged_models_pred = averaged_models.predict(train.values)\nprint(rmsle(y_train, averaged_models_pred))","c2857206":"XGBoost.fit(train, y_train)\nxgb_pred = XGBoost.predict(train)\n#xgb_pred = np.expm1(xgb_pred)\nprint(rmsle(y_train, xgb_pred))","be675b41":"##### Generate average_model\nLGBM.fit(train, y_train)\nLGBM_pred = LGBM.predict(train)\nprint(rmsle(y_train, LGBM_pred))","14d92880":"ensemble = averaged_models_pred*0.70 + xgb_pred*0.15 + LGBM_pred*0.15\nprint(rmsle(y_train, ensemble))","b3943b09":"### Transforming data types\n- Some numerical features are actually categorical features\n- Some other features, which are numerical but having low correlation with the SalePrice. Thus leaving it be and let Tree-based model handle them.","ef09345f":"#### Plot Nan\/Null values","1355cf2e":"##### Generate average_model","353762f9":"### CROSS VALIDATION\n- As having small dataset, using cross validation is a good technique. The setback of this method is computational expensive.\n- If having large dataset, we prefer to use train-test split in which 70% of training and 30% of test dataset.","93b7c3a5":"##### Define Root Mean Square Error function","718b1a7f":"##### Generate LightGBM model","7010f4d5":"#### Tree-based family: GradientBoost, XGBoost, LightGBM","8d7add66":"- ExteralQual with Excellent or Good condition, SalePrice is higher.","4adce288":"### MODELS","f758be49":"##### Coeffiect Elimination","24b1d77c":"# LIBRARY","e9921f3f":"### Ensembling StackingModel, XGBoost, and LightGBM","e590e86a":"#### Using Best_Parameters_model function to find out the best parameters of model on cross validation","1f4ce903":"# House Prices Prediction\n### Vu Duong\n#### May 2020\n\n","c1c3ca56":"### EXPLORATORY","3e8d3c99":"##### Generate XGBoost model","4b4dbb79":"# MODELING","783d6013":"### VISUALIZATION","9f88716d":"##### Observation:\n- Those models are sensitive to outliers and feature scalling, so applying StandardizationScaler or RobustScaler may help.\n- Having experiment showed RobustScaller generates better result for removing outliers.\n- Applying appropriate regularization value on each model as a way to handle collinearity, noise filter and overfitting problem, therefore improving accuracy in prediction.\n- Lasso applies L1 regularization, Ridge applies L2 regularization while ElasticNet is a combination of Lasso and Ridge.","992e6ddf":"# Credit\nThis work is inspired by multiple great sources done before:\n- https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n- https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n- https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset\n- https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models","0c74909d":"### Stacking \/ Averaging Models\n- Building a new class to extend and reuse scikit-learn with our model .","efacb089":"#### Observation: OverallQual, Living Area, ExteralQual, SalePrice\n- Higher OverallQual values are, higher SalePrice is.\n- Larger Living Area is, higher SalePrice is. Thus outliers happen when there is a large living area with low price.\n- SalePrice is right-skewed, thus applying log or log1p to turn the feature into normal curve so as to remove outliers.\n- Those features look linear to the target, SalePrice","69ea8740":"#### Cross Validation Score","873172d5":"### DATA IMPUTATION\n- mean\n- median\n- mode\n- correlation with other columns","bd717b38":"##### Model Score","0dcd331f":"# FEATURE ENGINEERING","cbf9a07c":"##### Final Result","0854a7d9":"##### Observation:\n- Do not apply RobustScaler as the algorithm itself does feature selection and pruning tree to be robust to outliers.\n- The important part is tuning hyperparameters.","4d153aec":"#### Linear Regression family: Lasso, Ridge, ElasticNet ","7c78e681":"##### Observation\n- The stacking\/average approach really improves the score"}}