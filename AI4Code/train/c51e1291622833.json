{"cell_type":{"ac2aa42f":"code","57a432f8":"code","243ce8d8":"code","11f8571d":"code","29562470":"code","c95747fd":"code","596cc2a3":"code","6339f672":"code","f39476ac":"code","e760a0a3":"code","83d75aae":"code","1e1cbcb8":"code","adf6ae8d":"code","25b47047":"code","12125b43":"code","60901216":"code","04ba9fc2":"code","d645cac0":"code","dedd261b":"code","0d2e0ebc":"code","bf5e2c61":"code","b88397d3":"code","03d69c5a":"code","e2b87c1e":"code","d0408a63":"code","2904b6a7":"code","29b8b3ca":"code","14ecfd5c":"code","c3bdd6c7":"code","88db3e5b":"code","f2abc2c0":"code","928b902f":"code","c93f82b7":"code","8978f3a7":"code","06749853":"code","6bf8b1e5":"code","634cdc60":"code","6abe76f8":"code","36057bc5":"markdown","1a5bbda7":"markdown","e58b11fc":"markdown","e846d9ed":"markdown","29fc34b7":"markdown","c9df2655":"markdown","a924f3aa":"markdown","ae975012":"markdown","029a8a55":"markdown"},"source":{"ac2aa42f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns","57a432f8":"#Importing Data\ndf = pd.read_csv(\"..\/input\/heart.csv\")","243ce8d8":"df.columns","11f8571d":"#Improving Names\nnew_columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholestoral',\n               'fasting_blood_sugar', 'resting_electrocardiographic_results', \n               'maximum_heart_rate', 'exercise_induced_angina', 'oldpeak', 'slope', \n               'ca','thal','target']","29562470":"#Replacing Columns Names\nfor i in range(0, len(df.columns)):\n    df.columns.values[i] = new_columns[i] ","c95747fd":"df_user = pd.DataFrame(np.arange(0, len(df)),columns = ['patient'])\ndf = pd.concat([df_user,df], axis=1)","596cc2a3":"df.info()","6339f672":"df.head(5)","f39476ac":"df.tail(5)","e760a0a3":"df.describe()","83d75aae":"df['target'].value_counts()","1e1cbcb8":"#Verifying Null Values\nsns.heatmap(data=df.isnull(), yticklabels=False, cbar=False, cmap='viridis')","adf6ae8d":"df.isna().sum()","25b47047":"df.isna().any()","12125b43":"#Defyning X and y\nX = df.drop(['patient','target'], axis=1)\ny = df['target']","60901216":"sns.set(style = 'ticks', color_codes=True)\nsns.pairplot(data=df, hue='target',  \n             vars=['resting_blood_pressure',\n                                          'cholestoral',\n                                          'maximum_heart_rate',\n                                          'oldpeak', 'age'])","04ba9fc2":"sns.countplot(data=df, x='target')","d645cac0":"plt.figure(figsize=(15,10))\nsns.heatmap(data= df.corr(), annot=True, cmap='viridis')","dedd261b":"fig, ax = plt.subplots(4,2, figsize=(16,16))\nsns.distplot(df.age, bins = 20, ax=ax[0,0]) \nsns.distplot(df.resting_blood_pressure, bins = 20, ax=ax[0,1]) \nsns.distplot(df.chest_pain_type, bins = 20, ax=ax[1,0]) \nsns.distplot(df.cholestoral, bins = 20, ax=ax[1,1]) \nsns.distplot(df.maximum_heart_rate, bins = 20, ax=ax[2,0])\nsns.distplot(df.oldpeak, bins = 20, ax=ax[2,1])\nsns.distplot(df.slope, bins = 20, ax=ax[3,0]) \nsns.distplot(df.thal, bins = 20, ax=ax[3,1]) ","0d2e0ebc":"df2 = df[['resting_blood_pressure','cholestoral','maximum_heart_rate','oldpeak']]\nfig = plt.figure(figsize=(15, 12))\nplt.suptitle('Histograms of Numerical Columns', fontsize=20)\nfor i in range(df2.shape[1]):\n    plt.subplot(4, 2, i + 1)\n    f = plt.gca()\n    f.set_title(df2.columns.values[i])\n\n    vals = np.size(df2.iloc[:, i].unique())\n    if vals >= 100:\n        vals = 100\n    \n    plt.hist(df2.iloc[:, i], bins=vals, color='#3F5D7D')\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    ","bf5e2c61":"#Correlation with independent variable\nX.corrwith(df.target).plot.bar(figsize = (15, 10), title = \"Correlation with Target\", fontsize = 10,grid = True)","b88397d3":"## Correlation Matrix\nsns.set(style=\"white\")\n\n# Compute the correlation matrix\ncorr = df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(10, 10))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\n","03d69c5a":"## Pie Plots \ndf2 = df[['sex', 'chest_pain_type', 'fasting_blood_sugar',\n                    'resting_electrocardiographic_results', 'exercise_induced_angina', 'slope',\n                    'ca', 'thal', 'target',\n                                        ]]\nfig = plt.figure(figsize=(20, 15))\nplt.suptitle('Pie Chart Distributions', fontsize=20)\nfor i in range(1, df2.shape[1] + 1):\n    plt.subplot(6, 3, i)\n    f = plt.gca()\n    f.axes.get_yaxis().set_visible(False)\n    f.set_title(df2.columns.values[i - 1])\n   \n    values = df2.iloc[:, i - 1].value_counts(normalize = True).values\n    index = df2.iloc[:, i - 1].value_counts(normalize = True).index\n    plt.pie(values, labels = index, autopct='%1.1f%%')\n    plt.axis('equal')\nfig.tight_layout(rect=[0, 0.03, 1, 0.95])","e2b87c1e":"#Splitting the dataset into the traing set and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, stratify=y, random_state=0)","d0408a63":"#Feature scaling\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train2 = pd.DataFrame(sc_X.fit_transform(X_train))\nX_test2 = pd.DataFrame(sc_X.transform(X_test))\nX_train2.columns = X_train.columns.values\nX_test2.columns = X_test.columns.values\nX_train2.index = X_train.index.values\nX_test2.index = X_test.index.values\nX_train = X_train2\nX_test = X_test2\n","2904b6a7":"X_train.head(5)","29b8b3ca":"X_test.head(5)","14ecfd5c":"#Model Building (Logistic Regression)\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state=0, penalty='l1')\nclassifier.fit(X_train, y_train)","c3bdd6c7":"#Predicting Test set\ny_pred = classifier.predict(X_test)\nfrom sklearn.metrics import accuracy_score, f1_score,recall_score,precision_score, confusion_matrix\nacc = accuracy_score(y_test,y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test,y_pred)\nresults = pd.DataFrame([['Logistic Regression (Lasso)', acc,prec,rec,f1]],columns=['Model', 'Accuracy', 'Precision', 'Recall','F1 Score'])\nresults","88db3e5b":"## EXTRA: Confusion Matrix\ncm = confusion_matrix(y_test, y_pred) # rows = truth, cols = prediction\ndf_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.4)\nsns.heatmap(df_cm, annot=True, fmt='g')\nprint(\"Test Data Accuracy: %0.4f\" % accuracy_score(y_test, y_pred)) ","f2abc2c0":"#Plotting Cumulative Accuracy Profile (CAP)\ny_pred_prob = classifier.predict_proba(X_test)\nfrom scipy import integrate\ndef capcurve(y_values, y_preds_proba):\n    num_pos_obs = np.sum(y_values)\n    num_count = len(y_values)\n    rate_pos_obs = float(num_pos_obs) \/ float(num_count)\n    ideal = pd.DataFrame({'x':[0,rate_pos_obs,1],'y':[0,1,1]})\n    xx = np.arange(num_count) \/ float(num_count - 1)\n    \n    y_cap = np.c_[y_values,y_preds_proba]\n    y_cap_df_s = pd.DataFrame(data=y_cap)\n    y_cap_df_s = y_cap_df_s.sort_values([1], ascending=False).reset_index(level = y_cap_df_s.index.names, drop=True)\n    \n    print(y_cap_df_s.head(20))\n    \n    yy = np.cumsum(y_cap_df_s[0]) \/ float(num_pos_obs)\n    yy = np.append([0], yy[0:num_count-1]) #add the first curve point (0,0) : for xx=0 we have yy=0\n    \n    percent = 0.5\n    row_index = int(np.trunc(num_count * percent))\n    \n    val_y1 = yy[row_index]\n    val_y2 = yy[row_index+1]\n    if val_y1 == val_y2:\n        val = val_y1*1.0\n    else:\n        val_x1 = xx[row_index]\n        val_x2 = xx[row_index+1]\n        val = val_y1 + ((val_x2 - percent)\/(val_x2 - val_x1))*(val_y2 - val_y1)\n    \n    sigma_ideal = 1 * xx[num_pos_obs - 1 ] \/ 2 + (xx[num_count - 1] - xx[num_pos_obs]) * 1\n    sigma_model = integrate.simps(yy,xx)\n    sigma_random = integrate.simps(xx,xx)\n    \n    ar_value = (sigma_model - sigma_random) \/ (sigma_ideal - sigma_random)\n    \n    fig, ax = plt.subplots(nrows = 1, ncols = 1)\n    ax.plot(ideal['x'],ideal['y'], color='grey', label='Perfect Model')\n    ax.plot(xx,yy, color='red', label='User Model')\n    ax.plot(xx,xx, color='blue', label='Random Model')\n    ax.plot([percent, percent], [0.0, val], color='green', linestyle='--', linewidth=1)\n    ax.plot([0, percent], [val, val], color='green', linestyle='--', linewidth=1, label=str(val*100)+'% of positive obs at '+str(percent*100)+'%')\n    \n    plt.xlim(0, 1.02)\n    plt.ylim(0, 1.25)\n    plt.title(\"CAP Curve - a_r value =\"+str(ar_value))\n    plt.xlabel('% of the data')\n    plt.ylabel('% of positive obs')\n    plt.legend()\n    ","928b902f":"capcurve(y_test,y_pred_prob[:,1])","c93f82b7":"#Applying K-folds validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator= classifier, X=X_train, y=y_train, cv=10)\naccuracies.mean()\naccuracies.std()\nprint('Logistic Regression (Lasso) Accuracy: %0.3f (+\/- %0.3f)' % (accuracies.mean(), accuracies.std() * 2))","8978f3a7":"#Analyzing the coefficients\npd.concat([pd.DataFrame(X_train.columns, columns = [\"features\"]),\n           pd.DataFrame(np.transpose(classifier.coef_), columns = [\"coef\"])\n           ],axis = 1)","06749853":"#Feature selection \n#Recursive feature elimination\nfrom sklearn.feature_selection import RFE\n\n#Select best feature \nrfe = RFE(classifier, n_features_to_select= None)\nrfe = rfe.fit(X_train, y_train)\n\n#Summarize the selection of the attributes\nprint(rfe.support_)\nprint(rfe.ranking_)\nX_train.columns[rfe.support_]\n","6bf8b1e5":"\n# New Correlation Matrix\nsns.set(style=\"white\")\n\n# Compute the correlation matrix\ncorr = X_train[X_train.columns[rfe.support_]].corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(18, 15))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})  ","634cdc60":"# Fitting Model to the Training Set\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state=0, penalty= 'l1')\nclassifier.fit(X_train[X_train.columns[rfe.support_]], y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test[X_train.columns[rfe.support_]])\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Logistic Regression RFE (Lasso)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)\nresults","6abe76f8":"y_pred_prob = classifier.predict_proba(X_test[X_train.columns[rfe.support_]])\ncapcurve(y_test,y_pred_prob[:,1])","36057bc5":"# Introduction","1a5bbda7":"# Cumulative Accuracy Profile (CAP)","e58b11fc":"# Feature Selection","e846d9ed":"Which independet varible is import for make a prediction? Which weight has the features on our model? In this kernel, we will try figure out these questions. For this, we will use Logistic Regression method. Due Logistic Regression has been using Linear Regression as methodology, Recursive Feature Elimation (RFE) will be the perfect method for our case.","29fc34b7":"For feature selection, we wil use the Recursive Feature Elimination (RFE). More about Recursive Feature Elimination (RFE) [here](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFE.html)","c9df2655":"# Visualizing Data","a924f3aa":"# Accuracy Paradox","ae975012":"\nAccuracy is not the best way to measure a perfomance of model. It\u00b4s because Accuracy Paradox.\nMore about Accuracy Paradox [here](http:\/\/towardsdatascience.com\/accuracy-paradox-897a69e2dd9b).\n","029a8a55":"For figure out Accuracy Paradox, we will use the Cumulative Accuracy Profile (CAP). More about Cumulative Accuracy Profile (CAP) [here](http:\/\/en.wikipedia.org\/wiki\/Cumulative_accuracy_profile)"}}