{"cell_type":{"b76c7f1a":"code","a3256871":"code","a72d7c6b":"code","6e437252":"code","1c434d99":"code","586eafe7":"code","3cc990a3":"code","5aa21b15":"code","3cb30ab2":"code","bb46613e":"code","9c4dc52c":"code","d3022fe0":"code","d765b33f":"code","b6e1a4f9":"code","8798a835":"code","a357f0e9":"code","3246be1a":"code","006e9a68":"code","9a3dec70":"code","78990a95":"code","984a7042":"code","e1a0851b":"code","62d17ce7":"code","724f8501":"code","9975d9e7":"code","b910128c":"code","7b7b95dd":"code","e436fbe0":"code","1a551bd4":"code","43aa0fc0":"code","56c065e8":"code","83fba8a1":"code","7f2bb219":"code","dc6684f1":"code","33bf6f2f":"code","b6856589":"code","1a1ceb17":"code","1dec26ec":"code","31b85a47":"code","ad79d26c":"code","93dc10e2":"code","73ba9898":"code","fb7a8d88":"code","5d39deee":"code","7298e802":"code","10dc4ddc":"code","0491832c":"code","cadc25be":"code","e2e0576b":"code","cb6d7f5f":"code","279192ac":"code","541b6be8":"code","0acc2759":"code","57d50173":"code","b512f3a3":"code","fea242ca":"code","22695aef":"code","9ac8a743":"code","a68f902d":"markdown","b4ff6882":"markdown","82a19b7e":"markdown","4331e62b":"markdown","b1e91cb3":"markdown","372b8c8b":"markdown","135ab8c4":"markdown","3904762d":"markdown","1dd699e6":"markdown","5f6e76b3":"markdown","12888637":"markdown","67af6e7d":"markdown","086a7e4e":"markdown","8454c62f":"markdown","1d978d81":"markdown","06da72e7":"markdown","8b11b1b1":"markdown","bf7f3692":"markdown","56fde0ad":"markdown","15ed6530":"markdown","5958525c":"markdown","a613261b":"markdown","0a03f1f8":"markdown","d1fe1d06":"markdown","142f4425":"markdown","f1786f2f":"markdown"},"source":{"b76c7f1a":"#importing the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","a3256871":"path = '\/kaggle\/input\/fish-market\/Fish.csv'","a72d7c6b":"df = pd.read_csv(path)","6e437252":"df.head(3)","1c434d99":"df.describe(include = 'all')","586eafe7":"df.info()","3cc990a3":"#plotting a distribution plot\nsns.distplot(df['Weight']) #we can see those few outliers shown by the longer right tail of the distribution","5aa21b15":"#Removing the top 1% of the observation will help us to deal with the outliers\nq = df['Weight'].quantile(0.99)\ndf = df[df['Weight']<q]\n\nsns.distplot(df['Weight']) ","3cb30ab2":"sns.distplot(df['Length1']) #we can see those few outliers shown by the longer right tail of the distribution","bb46613e":"#Removing the top 1% of the observation will help us to deal with the outliers\nq = df['Length1'].quantile(0.99)\ndf = df[df['Length1']<q]\n\nsns.distplot(df['Length1'])","9c4dc52c":"sns.distplot(df['Length2']) #we can see those few outliers shown by the longer right tail of the distribution","d3022fe0":"#Removing the top 1% of the observation will help us to deal with the outliers\nq = df['Length2'].quantile(0.99)\ndf = df[df['Length2']<q]\n\nsns.distplot(df['Length2'])","d765b33f":"sns.distplot(df['Length3']) #we can see those few outliers shown by the longer right tail of the distribution","b6e1a4f9":"#Removing the top 1% of the observation will help us to deal with the outliers\nq = df['Length3'].quantile(0.99)\ndf = df[df['Length3']<q]\n\nsns.distplot(df['Length3'])","8798a835":"#We need to reset index of the dataframe after droppinh those observation\ndf.reset_index(drop = True, inplace = True)","a357f0e9":"df.describe()","3246be1a":"sns.distplot(df['Height']) #we can see those few outliers shown by the longer right tail of the distribution","006e9a68":"#Removing the top 1% of the observation will help us to deal with the outliers\nq = df['Height'].quantile(0.99)\ndf = df[df['Height']<q]\n\nsns.distplot(df['Height'])","9a3dec70":"sns.distplot(df['Width']) #we can see those few outliers shown by the longer right tail of the distribution","78990a95":"#Removing the top 1% of the observation will help us to deal with the outliers\nq = df['Width'].quantile(0.99)\ndf = df[df['Width']<q]\n\nsns.distplot(df['Width'])","984a7042":"df.describe()","e1a0851b":"df.columns.values","62d17ce7":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\n# the target column (in this case 'weight') should not be included in variables\n#Categorical variables already turned into dummy indicator may or maynot be added if any\nvariables = df[['Length1', 'Length2', 'Length3', 'Height','Width']]\nX = add_constant(variables)\nvif = pd.DataFrame()\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range (X.shape[1]) ]\nvif['features'] = X.columns\nvif","724f8501":"df.drop(['Length1','Length2'], axis = 1, inplace = True)","9975d9e7":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\nvariables = df[['Length3', 'Height','Width']]\nX = add_constant(variables)\nvif = pd.DataFrame()\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range (X.shape[1]) ]\nvif['features'] = X.columns\nvif","b910128c":"fig,(ax1,ax2,ax3) = plt.subplots(1,3, figsize =(15,3))\nax1.scatter(df['Length3'], df['Weight'])\nax1.set_title('Length3 and weight')\n\nax2.scatter(df['Height'], df['Weight'])\nax2.set_title('Height and weight')\n\nax3.scatter(df['Width'], df['Weight'])\nax3.set_title('Width and weight')","7b7b95dd":"#Creating a new column in our dataset containing log-of-weight\ndf['log_weight'] = np.log(df['Weight'])","e436fbe0":"#RE plotting the graphs but this time using 'log_weight' as our target variable\nfig,(ax1,ax2,ax3) = plt.subplots(1,3, figsize =(15,3))\nax1.scatter(df['Length3'], df['log_weight'])\nax1.set_title('Length3 and log_weight')\n\nax2.scatter(df['Height'], df['log_weight'])\nax2.set_title('Height and log_weight')\n\nax3.scatter(df['Width'], df['log_weight'])\nax3.set_title('Width and log_weight')","1a551bd4":"#Creating new columns to hold the logs of the variables\ndf['log_length3'] = np.log(df['Length3'])\n\ndf['log_width'] = np.log(df['Width'])\n\ndf['log_height'] = np.log(df['Height'])","43aa0fc0":"fig,(ax1,ax2,ax3) = plt.subplots(1,3, figsize =(15,3))\nax1.scatter(df['log_length3'], df['log_weight'])\nax1.set_title('log_weight and log_length 1')\nax2.scatter(df['log_width'], df['log_weight'])\nax2.set_title('log_width and log_weight')\nax3.scatter(df['log_height'], df['log_weight'])\nax3.set_title('log_height and log_weight')","56c065e8":"#Getting the present variables in our dataframe\ndf.columns.values","83fba8a1":"#Dropping those columns that as been logged\ndf = df.drop(['Length3', 'Height', 'Width', 'Weight'], axis = 1)","7f2bb219":"df.describe()","dc6684f1":"df[df['log_weight'].apply(lambda x: x < 1000)].sort_values('log_weight', ascending = True)","33bf6f2f":"#Drop a row with index number 40\ndf.drop([40], inplace = True)\n\n#Resetting the index after dropping a row\ndf.reset_index(drop = True, inplace = True)","b6856589":"df = pd.get_dummies(df, drop_first = True)","1a1ceb17":"df.head()","1dec26ec":"#Declaring independent variable i.e x\n#Declaring Target variable i.e y\ny = df['log_weight']\nx = df.drop(['log_weight'], axis = 1)","31b85a47":"scaler = StandardScaler() #Selecting the standardscaler\nscaler.fit(x)#fitting our independent variables","ad79d26c":"scaled_x = scaler.transform(x)#scaling","93dc10e2":"#Splitting our data into train and test dataframe\nx_train,x_test, y_train, y_test = train_test_split(scaled_x, y , test_size = 0.2, random_state = 47)","73ba9898":"reg = LinearRegression()#Selecting our model\nreg.fit(x_train,y_train)","fb7a8d88":"#predicting using x_train\ny_hat = reg.predict(x_train)","5d39deee":"#Plotting y_train vs our predicted value(y_hat)\nfig, ax = plt.subplots()\nax.scatter(y_train, y_hat)","7298e802":"#Residual graph\nsns.distplot(y_train - y_hat)\nplt.title('Residual Graph')","10dc4ddc":"#R2\nreg.score(x_train, y_train)","0491832c":"#Intercept of the regression line\nreg.intercept_","cadc25be":"#Coefficient\nreg.coef_","e2e0576b":"#Predicting with x_test\ny_hat_test = reg.predict(x_test)","cb6d7f5f":"reg.score(x_test, y_test)","279192ac":"#Plotting predicted value against y_test\nplt.scatter(y_test, y_hat_test, alpha=0.5)\nplt.show()","541b6be8":"#Creating a summary table containing coefficients for each variable\nsummary = pd.DataFrame( data = x.columns.values, columns = ['Features'] )\nsummary['Weight'] = reg.coef_\nsummary","0acc2759":"#Creating a new dataframe\ndf1 = pd.DataFrame( data = np.exp(y_hat_test), columns = ['Predictions'] )","57d50173":"#Resetting index to match the index of y_test with that of the dataframe\ny_test = y_test.reset_index(drop = True)","b512f3a3":"#target column will hold our predicted values\ndf1['target'] = np.exp(y_test)\n","fea242ca":"#Substrating predictions from target to get the difference in value\ndf1['Residual'] = df1['target'] - df1['Predictions']\n\n#Difference in percentage\ndf1['Difference%'] = np.absolute(df1['Residual']\/ df1['target'] * 100)","22695aef":"df1.describe()","9ac8a743":"df1.sort_values('Difference%')","a68f902d":"### INTRODUCTION\n\n#### Predicting the weight of fish through linear regression\n#### Steps taken in preprocessing includes Data cleaning, Assumption check, Outliers Removal, Standardization etc\n\n### SIDE NOTE\n#### You can leave your question about any unclear part in the comment section\n#### Any correction will be highly welcomed","b4ff6882":"### WEIGHT INTERPRETATION","82a19b7e":"#### Our graph generally shows a normal distribution but with a  longer left tail and a sligthly longer right tail\n#### which means that our model tends to over estimate the target(a much higher value is predicted) a lot","4331e62b":"#### If you find this notebook useful don't forget to upvote. #Happycoding","b1e91cb3":"### Standardization\n\n#### Standardizing helps to give our independent varibles a more standard and relatable numeric scale, it also helps in improving model accuracy","372b8c8b":"### CONCLUSION\n#### Let's take a closer look at the expected and predicted values","135ab8c4":"#### From the plot above we can see that our model was pretty decent in predicting \n### 4. No Endogeneity assumption\n#### Lets plot the residual graph to check for No Endogeneity assumption","3904762d":"#### The above graph gives a straight line plot Therefore our Linearity assumption as been satified\n### 3. The next assumption to check is Normality and Homoscedasticity\n#### Normality is assumed while from the last graphs we can see that homoscedasticity as been achieved\n\n\n\n#### We are going to use the new log columns we created inplace of the former variables so let's drop those former variables","1dd699e6":"#### This dataset is clean","5f6e76b3":"### Dummy Variables\n#### Species is a categorical variable so we need  to turn it into a dummy indicator before we can perform our regression","12888637":"#### After dropping those variables and running the code again the result shows that Non of the variables are having a vif >= 10\n\n### 2.The next assumption we will like to check is LINEARITY\n#### Let's plot each numerical variable against our target variable 'Weight' and check to see if the resulting plot is linear","67af6e7d":"### DEALING WITH MISSING VALUES","086a7e4e":"#### From the the table above log_weight seems to have a minimum value of -inf(infinity) which is useless\n#### Let's locate and drop the row having the -inf value","8454c62f":"#### Our minimum Difference in % is 1.09 while our maximum is 22.88","1d978d81":"#### Considering the summary table the higher the weight the higher the impact that means log_length3 is the most impactful feature\n#### A positive weight shows an increase in log_weight and weight respectively\n#### A negative weight shows a decrease in log_weight and weight respectively","06da72e7":"####  We notice that  from the table above the values with the highest Diffence% have both negative and positive residual values\n#### That can also mean that our model tends both overestimate and underestimate its target\n#### Over all our model is a very good model","8b11b1b1":"####  The resulting plots above gives us a rather curve line\n#### Let's try to correct this by taking the log of  our target variable 'Weight'","bf7f3692":"### ASSUMPTION CHECK\n### 1.The first assumption we will check is No multicollinearity\n#### The minimum variance inflation factor(vif) will be 10","56fde0ad":"### LOADING THE DATA","15ed6530":"#### The above plots till leads to a not so straight line, next we take the log of the numerical variables","5958525c":"### DATASET ANALYSIS AND OUTLIERS REMOVAL","a613261b":"####  This plot shows that our model prediction is quite close to the expected values with an R2 of 99.3%","0a03f1f8":"#### Our model is explaining 99.5% of the variabilty of the data which is quite excellent","d1fe1d06":"#### from the table above 'Length1' and  'Length2' are highly correlated with the rest of variables.\n#### Let's drop them and run the code again","142f4425":"#### we will plot the distribution of all  the numeric variables in other to be able to identify outliers and any other abnormalities\n#### Outliers will be dealt with by removing either top 1% or the bottom 1%","f1786f2f":"### MULTIPLE LINEAR REGRESSION\n#### It is time to create our model\n#### We will split our dataframe into two, one part for training the other for testing"}}