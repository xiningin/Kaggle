{"cell_type":{"f484c9d9":"code","a47082d2":"code","2d334f72":"code","f6d60f3b":"code","e47c6015":"code","b5a8f860":"code","634ed018":"code","1271e226":"code","5de802ad":"code","66b7183f":"code","84e3228e":"code","2f162e34":"code","de58975a":"code","5351af39":"code","665b8e58":"code","be379daf":"code","e3753514":"code","2fd0384d":"code","4fdb1bfe":"code","c9e8f3de":"code","b90b0c15":"code","1875b8aa":"code","5533b46f":"code","d0363d4c":"markdown"},"source":{"f484c9d9":"!pip install git+https:\/\/github.com\/artemmavrin\/focal-loss.git","a47082d2":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.utils import plot_model\nfrom warnings import filterwarnings\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer, LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nfilterwarnings('ignore')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'","2d334f72":"from focal_loss import SparseCategoricalFocalLoss\nfrom tensorflow.keras import Sequential, Model\nfrom tensorflow.keras.layers import Dense, BatchNormalization, Flatten, Input, Concatenate, Dropout\nfrom tensorflow.keras.utils import to_categorical\nimport tensorflow_addons as tfa","f6d60f3b":"def plot_keras_history(history, measures):\n    \"\"\"\n    history: Keras training history\n    measures = list of names of measures\n    \"\"\"\n    rows = len(measures) \/\/ 2 + len(measures) % 2\n    fig, panels = plt.subplots(rows, 2, figsize=(15, 5))\n    plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.4, wspace=0.2)\n    try:\n        panels = [item for sublist in panels for item in sublist]\n    except:\n        pass\n    for k, measure in enumerate(measures):\n        panel = panels[k]\n        panel.set_title(measure + ' history')\n        panel.plot(history.epoch, history.history[measure], label=\"Train \"+measure)\n        panel.plot(history.epoch, history.history[\"val_\"+measure], label=\"Validation \"+measure)\n        panel.set(xlabel='epochs', ylabel=measure)\n        panel.legend()\n        \n    plt.show(fig)","e47c6015":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","b5a8f860":"train = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\")","634ed018":"# source: https:\/\/www.kaggle.com\/remekkinas\/tps-12-nn-tpu-pseudolabeling-0-95661\npseudolabels = pd.read_csv(\"..\/input\/tps12-pseudolabels\/tps12-pseudolabels_v2.csv\")","1271e226":"print(\"The target class distribution:\")\nprint((train.groupby('Cover_Type').Id.nunique() \/ len(train)).apply(lambda p: f\"{p:.3%}\"))","5de802ad":"# Droping Cover_Type 5 label, since there is only one instance of it\ntrain = train[train.Cover_Type != 5]","66b7183f":"# remove unuseful features\ntrain = train.drop([ 'Soil_Type7', 'Soil_Type15'], axis=1)\npseudolabels = pseudolabels.drop([ 'Soil_Type7', 'Soil_Type15'], axis=1)\ntest = test.drop(['Soil_Type7', 'Soil_Type15'], axis=1)\n\n# extra feature engineering\ndef r(x):\n    if x+180>360:\n        return x-180\n    else:\n        return x+180\n\ndef fe(df):\n    df['EHiElv'] = df['Horizontal_Distance_To_Roadways'] * df['Elevation']\n    df['EViElv'] = df['Vertical_Distance_To_Hydrology'] * df['Elevation']\n    df['Aspect2'] = df.Aspect.map(r)\n    ### source: https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/293373\n    df[\"Aspect\"][df[\"Aspect\"] < 0] += 360\n    df[\"Aspect\"][df[\"Aspect\"] > 359] -= 360\n    df.loc[df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n    df.loc[df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n    df.loc[df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n    df.loc[df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n    df.loc[df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n    df.loc[df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\n    ########\n    df['Highwater'] = (df.Vertical_Distance_To_Hydrology < 0).astype(int)\n    df['EVDtH'] = df.Elevation - df.Vertical_Distance_To_Hydrology\n    df['EHDtH'] = df.Elevation - df.Horizontal_Distance_To_Hydrology * 0.2\n    df['Euclidean_Distance_to_Hydrolody'] = (df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2)**0.5\n    df['Manhattan_Distance_to_Hydrolody'] = df['Horizontal_Distance_To_Hydrology'] + df['Vertical_Distance_To_Hydrology']\n    df['Hydro_Fire_1'] = df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Fire_Points']\n    df['Hydro_Fire_2'] = abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Fire_Points'])\n    df['Hydro_Road_1'] = abs(df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Roadways'])\n    df['Hydro_Road_2'] = abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Roadways'])\n    df['Fire_Road_1'] = abs(df['Horizontal_Distance_To_Fire_Points'] + df['Horizontal_Distance_To_Roadways'])\n    df['Fire_Road_2'] = abs(df['Horizontal_Distance_To_Fire_Points'] - df['Horizontal_Distance_To_Roadways'])\n    df['Hillshade_3pm_is_zero'] = (df.Hillshade_3pm == 0).astype(int)\n    return df\n\ntrain = fe(train)\ntest = fe(test)\npseudolabels = fe(pseudolabels)\n\n# Summed features pointed out by @craigmthomas (https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/292823)\nsoil_features = [x for x in train.columns if x.startswith(\"Soil_Type\")]\nwilderness_features = [x for x in train.columns if x.startswith(\"Wilderness_Area\")]\n\ntrain[\"soil_type_count\"] = train[soil_features].sum(axis=1)\npseudolabels[\"soil_type_count\"] = pseudolabels[soil_features].sum(axis=1)\ntest[\"soil_type_count\"] = test[soil_features].sum(axis=1)\n\ntrain[\"wilderness_area_count\"] = train[wilderness_features].sum(axis=1)\npseudolabels[\"wilderness_area_count\"] = pseudolabels[wilderness_features].sum(axis=1)\ntest[\"wilderness_area_count\"] = test[wilderness_features].sum(axis=1)","84e3228e":"train = reduce_mem_usage(train)\npseudolabels = reduce_mem_usage(pseudolabels)\noriginal_len = len(train)\ntrain = pd.concat([train, pseudolabels], axis=0)","2f162e34":"y = train.Cover_Type.values - 1\nX = train.drop(\"Cover_Type\", axis=1).set_index(\"Id\").values.astype(np.float32)\nXt = test.set_index(\"Id\").values.astype(np.float32)","de58975a":"import gc\ndel([train, test, pseudolabels])\n_ = [gc.collect() for i in range(5)]","5351af39":"le = LabelEncoder()\ntarget = le.fit_transform(y)\n\n_, classes_num = np.unique(target, return_counts=True)","665b8e58":"### create baseline-model\ndef get_model(layers=[8], targets=7, dropout_rate=0.0, skip_layers=True, \n              batchnorm=True, activation='selu', kernel_initializer=\"lecun_normal\"):\n    \n    inputs_sequence = Input(shape=(X.shape[1]))\n    x = Flatten()(inputs_sequence)\n\n    skips = list()\n    for layer, nodes in enumerate(layers):\n        x = Dense(nodes, kernel_initializer=kernel_initializer, activation=activation)(x)\n        if batchnorm is True:\n            x = BatchNormalization()(x)\n        if layer != (len(layers) - 1):\n            if dropout_rate > 0:\n                x = Dropout(rate=dropout_rate)(x)\n            skips.append(x)\n    \n    if skip_layers is True:\n        x = Concatenate(axis=1)([x] + skips)\n    else:\n        del(skips)\n        \n    output_class = Dense(targets, activation='softmax', \n                         kernel_regularizer=tf.keras.regularizers.l2(l2=0.03))(x)\n\n    model = Model(inputs=inputs_sequence, outputs=output_class)\n    \n    return model","be379daf":"dnn_params = {'layers': [128, 64, 64, 64], \n              'batchnorm': True, \n              'skip_layers': True, \n              'targets': len(le.classes_)}\n\nmodel = get_model(**dnn_params)\nmodel.summary()","e3753514":"plot_model(\n    model, \n    to_file='baseline.png', \n    show_shapes=True,\n    show_layer_names=True\n)","2fd0384d":"try:\n    # detect and init the TPU\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    # instantiate a distribution strategy\n    tf_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print(\"Running on TPU:\", tpu.master())\nexcept:\n    tf_strategy = tf.distribute.get_strategy()\n    print(f\"Running on {tf_strategy.num_replicas_in_sync} replicas\")\n    print(\"Number of GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","4fdb1bfe":"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n### define callbacks\nearly_stopping = EarlyStopping(\n    monitor='val_acc', \n    min_delta=0, \n    patience=10, \n    verbose=0,\n    mode='max', \n    baseline=None, \n    restore_best_weights=True\n)\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_acc', \n    factor=0.5,\n    patience=5,\n    mode='max'\n)","c9e8f3de":"N_FOLDS = 20\n\n### cross-validation \ncv = KFold(n_splits=N_FOLDS, shuffle=True, random_state=1)\n\npredictions = np.zeros((len(Xt), len(le.classes_)))\noof = np.zeros((original_len, len(le.classes_)))\nscores = list()\n\nwith tf_strategy.scope():\n    for fold, (idx_train, idx_valid) in enumerate(cv.split(X, y)):\n        \n        idx_valid = idx_valid[idx_valid<original_len]\n        X_train, y_train = X[idx_train, :], target[idx_train]\n        X_valid, y_valid = X[idx_valid, :], target[idx_valid]\n        \n        ss = RobustScaler()\n        X_train = ss.fit_transform(X_train)\n        X_valid = ss.transform(X_valid)\n\n        model = get_model(**dnn_params)\n        \n        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)    \n    \n        model.compile(\n            optimizer='adam',\n            loss=tf.keras.losses.SparseCategoricalCrossentropy(), #SparseCategoricalFocalLoss(gamma=2.), tf.keras.losses.SparseCategoricalCrossentropy()\n            metrics=['acc']\n        )\n\n        print('**'*20)\n        print(f\"Fold {fold+1} || Training\")\n        print('**'*20)\n\n        history = model.fit(\n            X_train, y_train,\n            validation_data=(X_valid, y_valid),\n            batch_size=1024*2,\n            epochs=150,\n            verbose=1,\n            shuffle=True,\n            callbacks=[\n                early_stopping,\n                reduce_lr\n            ]\n        )\n        \n        plot_keras_history(history, ['loss', 'acc'])\n        \n        print(f\"Best training accuracy: {np.max(history.history['acc']):0.5f}\")\n        print(f\"Best validation accuracy: {np.max(history.history['val_acc']):0.5f}\")\n        scores.append(np.max(history.history['val_acc']))\n\n        oof[idx_valid] = model.predict(X_valid, batch_size=4096) \n\n        predictions += model.predict(ss.transform(Xt), batch_size=4096)\n        \n        del([X_train, y_train, X_valid, y_valid])\n        gc.collect()","b90b0c15":"print(f\"Average cv accuracy: {np.mean(scores):0.5f} (std={np.std(scores):0.5f})\")","1875b8aa":"submission.Cover_Type = le.inverse_transform(np.argmax(predictions, axis=1)) + 1\nsubmission.to_csv(\"submission.csv\", index=False)","5533b46f":"oof = pd.DataFrame(oof, columns=[f\"prob_{i}\" for i in le.classes_])\noof.insert(loc=0, column='Id', value=range(len(oof)))\noof.to_csv(\"oof.csv\", index=False)","d0363d4c":"In this notebook, I demonstrate how to use the multiclass focal loss that should help you score better with such imbalanced classes. The focal loss function is from https:\/\/github.com\/artemmavrin\/focal-loss\/blob\/master\/docs\/source\/index.rst\n\nThe focal loss is a loss that has been devised for object detection problems where the background is more prominent than the objects to be detected. \n\n![](https:\/\/github.com\/Atomwh\/FocalLoss_Keras\/raw\/master\/images\/fig1-focal%20loss%20results.png)\n\nAs you increase the gamma value, you put more emphasis on hard to classify examples. There is clearly a trade-off for this (high gamma values can be detrimental), but overall if you set the right value it should perform much better than using other tricks for imbalanced data.\n\nThis notebook owes quite a lot of ideas from \"TPSDEC21-01-Keras Quickstart\" (https:\/\/www.kaggle.com\/ambrosm\/tpsdec21-01-keras-quickstart) by @ambrosm please consider upvoting also his work.\n\nIt also implements the feature engineering suggested by @aguschin (see my post https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/291839 for all the references)."}}