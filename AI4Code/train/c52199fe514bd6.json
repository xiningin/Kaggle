{"cell_type":{"6de21dae":"code","cdb94ca5":"code","c561c8eb":"code","255fe358":"code","d5f037f2":"code","0b32ba7c":"code","33671f72":"code","99bb694a":"code","c83ad96c":"code","0f7fa5f2":"code","de353f21":"code","63fd74ec":"code","d709ddb6":"code","7bfe27c1":"code","181d0f72":"code","8754e420":"markdown","75f14e28":"markdown"},"source":{"6de21dae":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import linear_model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\nimport matplotlib.pyplot as plt\nimport datetime\nfrom sklearn import metrics\nimport seaborn as sns\nimport sklearn.ensemble as ske\nimport gc\nfrom sklearn.linear_model import LinearRegression,SGDRegressor,ElasticNet,Ridge# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nDATA_PATH = \"..\/input\/ashrae-energy-prediction\/\"\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cdb94ca5":"#Loading data\nWeather_Train = pd.read_csv(DATA_PATH + 'weather_train.csv')\nTrain= pd.read_csv(DATA_PATH +'train.csv')\nBuilding= pd.read_csv(DATA_PATH +'building_metadata.csv')","c561c8eb":"#Function That reduces the used memory\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    return df","255fe358":"#Reducing the needed data's memory usage\nTrain = reduce_mem_usage(Train)\nWeather_Train = reduce_mem_usage(Weather_Train)\nBuilding = reduce_mem_usage(Building)","d5f037f2":"#Merging tables\nresults = Building.merge(Train,left_on='building_id',right_on='building_id',how='left')\ndata = results.merge(Weather_Train,left_on=['site_id','timestamp'],right_on=['site_id','timestamp'],how='left')\ndata.head()","0b32ba7c":"# Add and Drop Features\ndata = data.drop(columns=['year_built', 'floor_count', 'wind_direction', 'dew_temperature'])\n\n#Function that is used to clean memory from deleted data\ngc.collect()","33671f72":"#fixing timestamp and taking only the day and the month and then dropping timestamp column\ndata[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"])\ndata[\"day\"]= data[\"timestamp\"].dt.day\ndata[\"month\"]= data[\"timestamp\"].dt.month\ndata= data.drop(\"timestamp\", axis = 1) ","99bb694a":"#Change data type to float 32 for filling NA value before transforming them into int for smooth modeling processing\ndata['wind_speed'] = data['wind_speed'].astype('float32')\ndata['air_temperature'] = data['air_temperature'].astype('float32')\ndata['precip_depth_1_hr'] = data['precip_depth_1_hr'].astype('float32')\ndata['cloud_coverage'] = data['cloud_coverage'].astype('float32')","c83ad96c":"#Filling Null Values\ndata['precip_depth_1_hr'].fillna(data['precip_depth_1_hr'].mean(), inplace = True)\ndata['cloud_coverage'].fillna(data['cloud_coverage'].mean(), inplace = True)\ndata['wind_speed'].fillna(data['wind_speed'].mean(), inplace=True)\ndata['air_temperature'].fillna(data['air_temperature'].mean(), inplace=True)\n\n# Printing the sum of nulls inside the columns\ndata.isnull().sum()","0f7fa5f2":"# Here column 'primaty_use' was treated by get_dummies function and get_dummies is used for data manipulation\ndata_linearR = pd.get_dummies(data,columns = ['primary_use'])","de353f21":"#printing the columns of \"data_linearR\"\ndata_linearR.columns","63fd74ec":"#Using the important features\nXD =data_linearR[['building_id', 'meter', 'air_temperature', 'wind_speed', 'precip_depth_1_hr', 'cloud_coverage',\n       'square_feet', 'primary_use_Education', 'primary_use_Entertainment\/public assembly',\n       'primary_use_Food sales and service', 'primary_use_Healthcare',\n       'primary_use_Lodging\/residential',\n       'primary_use_Manufacturing\/industrial', 'primary_use_Office',\n       'primary_use_Other', 'primary_use_Parking',\n       'primary_use_Public services', 'primary_use_Religious worship',\n       'primary_use_Retail', 'primary_use_Services',\n       'primary_use_Technology\/science', 'primary_use_Utility',\n       'primary_use_Warehouse\/storage', 'month', 'day']]\n\n# Create target variable\nYD = data_linearR['meter_reading']\n\n# Train, test, split\nXD_train,XD_test, YD_train, YD_test = train_test_split(XD,YD, test_size = .20, random_state= 0)","d709ddb6":"#applying RandomForestRegressor and fitting the model\nRidge = Ridge(alpha=1.0)\nRidge.fit(XD,YD)","7bfe27c1":"#calculating accuracy of Randomforest model\nRidge.score(XD,YD)","181d0f72":"#getting predictions for the test\nYR_pred = Ridge.predict(XD_test)\nYR_pred","8754e420":"* **Memory usage reduction**","75f14e28":"* **Importing data**"}}