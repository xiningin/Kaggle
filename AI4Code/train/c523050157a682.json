{"cell_type":{"0044faf9":"code","263cd631":"code","b088dbe7":"code","a63b549d":"code","a7172982":"code","68f17fa3":"code","add9d260":"code","4e2271b0":"code","27d6417f":"code","8de460b4":"code","34b55c83":"code","d3b0ced4":"code","1eeeaed6":"code","d22e46d2":"code","3d37758c":"code","ec455e2a":"code","5fa62c28":"code","f430476d":"code","9c886eba":"code","131573e5":"code","db11f256":"code","5a354b93":"code","48b8b603":"code","3823e67c":"code","6a03fb3a":"code","ca0634da":"code","05c58349":"code","1dce0d17":"code","7499f7d3":"markdown","940206e9":"markdown","f9cf3d1f":"markdown","60e3dee0":"markdown","19737fb5":"markdown","38f1c145":"markdown","1007007b":"markdown","130c44db":"markdown","77bd078f":"markdown","0c1f2ba0":"markdown","e163774d":"markdown","e6a9743d":"markdown","9a29f7cb":"markdown","d80475d5":"markdown","2654c2ed":"markdown","e083cc12":"markdown","ab714655":"markdown","69552e84":"markdown","7089033f":"markdown","43acb3b8":"markdown","41e157bf":"markdown"},"source":{"0044faf9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","263cd631":"import warnings\nwarnings.filterwarnings('ignore')\nfrom scipy.optimize import minimize\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.metrics import log_loss","b088dbe7":"train = pd.read_csv('..\/input\/otto-group-product-classification-challenge\/train.csv')\ntest = pd.read_csv('..\/input\/otto-group-product-classification-challenge\/test.csv')\nsample = pd.read_csv('..\/input\/otto-group-product-classification-challenge\/sampleSubmission.csv')","a63b549d":"train.head(3)","a7172982":"train.info()","68f17fa3":"g_target = train.groupby(['target']).size().reset_index(name='counts')\n\nplt.figure(figsize=(12,5))\nplt.title('The Distribution of Target in Training Dataset')\nax = sns.barplot(x=\"target\", y=\"counts\", data=g_target)\nplt.show()","add9d260":"target = train['target']\ntrain.drop(['id','target'],axis=1,inplace=True)\ntrain.shape","4e2271b0":"testId = test['id']\ntest.drop('id',axis=1,inplace=True)\ntest.shape","27d6417f":"X = train\ny = target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 7)","8de460b4":"def get_accuracy_score(y_actual, y_pred):\n    return accuracy_score(y_actual, y_pred)\n\ndef get_log_loss_score(y_actual, y_pred):\n    return log_loss(y_actual, y_pred)\n\ndef train_test_model(clf, X_train, X_test, y_train, y_test, scoring_name='Accuracy'):\n    print('%s training result:' % (str(clf)))\n    clf.fit(X_train, y_train)\n    score_train, score_test = 0, 0\n    if scoring_name == 'Log_Loss':\n        score_train = get_log_loss_score(y_train, clf.predict_proba(X_train))\n        score_test = get_log_loss_score(y_test, clf.predict_proba(X_test))\n    else:\n        score_train = get_accuracy_score(y_train, clf.predict(X_train))\n        score_test = get_accuracy_score(y_test, clf.predict(X_test))\n    \n    print('- %s on train dataset %.2f%%' % (scoring_name, score_train * 100))\n    print('- %s on test dataset %.2f%%' % (scoring_name, score_test * 100))\n    print()\n    return {\n        'clf': clf,\n        'scores': {'train': score_train, 'test': score_test}\n    }\n\ndef grid_search(clf, param_grid, param_name):\n    # Grid Search for learning rate\n    #learning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n    #param_grid = dict(learning_rate = learning_rate, n_estimators = [150])\n    kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 7)\n    grid_search = GridSearchCV(clf, param_grid, scoring = 'neg_log_loss'\n                               , n_jobs = -1, cv = kfold, verbose = 1,  refit = True)\n    result = grid_search.fit(X_train, y_train)\n    print('The best paramter is %s ' % result.best_params_)\n    print('The bcore is %f' % result.best_score_)\n    print()\n    plt.errorbar(param_grid[param_name], result.cv_results_['mean_test_score']\n                 , yerr = result.cv_results_['std_test_score'])\n    plt.xlabel(param_name)\n    plt.ylabel('Log Loss')\n    plt.show()","34b55c83":"rfc = RandomForestClassifier(n_jobs=-1)\nrfc_result = train_test_model(rfc, X_train, X_test, y_train ,y_test, scoring_name='Log_Loss')\n\nlr = LogisticRegression(n_jobs=-1)\nlr_result = train_test_model(lr, X_train, X_test, y_train ,y_test, scoring_name='Log_Loss')\n\nlgbm = LGBMClassifier(n_jobs=-1, objective='multiclass')\nlgbm_result = train_test_model(lgbm, X_train, X_test, y_train ,y_test, scoring_name='Log_Loss')","d3b0ced4":"df_scores = {\n    'model': ['Random Forest', 'Random Forest'\n              , 'Logistic Regression', 'Logistic Regression'\n              , 'LightGBM', 'LightGBM'],\n    'type': ['train', 'test', 'train', 'test', 'train', 'test'],\n    'scores': [rfc_result['scores']['train'], rfc_result['scores']['test']\n               , lr_result['scores']['train'], lr_result['scores']['test']\n               , lgbm_result['scores']['train'], lgbm_result['scores']['test']\n              ]\n}","1eeeaed6":"plt.figure(figsize=(12,5))\nplt.title('The Log-Loss Scores')\nax = sns.barplot(x='model', y='scores', hue='type', data=df_scores)\nplt.show()","d22e46d2":"rfc = RandomForestClassifier(n_jobs=-1)\nrfc_params = {'n_estimators': range(10, 100, 20), 'max_depth' : range(15, 50, 5)}\n\nlr = LogisticRegression(n_jobs=-1)\nlr_params = {'C': [0.1,1,10,100]}\n\nlgbm = LGBMClassifier(n_jobs=-1, objective='multiclass')\nlgbm_params = {'min_child_weight': [1, 2, 4, 8], 'max_depth': range(5, 30, 10), 'learning_rate': [0.01, 0.1, 0.2, 0.3]}","3d37758c":"# RandomForest: n_estimators\nparams = {'n_estimators': rfc_params['n_estimators']}\ngrid_search(rfc, params, 'n_estimators')","ec455e2a":"# RandomForest: max_depth\nparams = {'max_depth': rfc_params['max_depth'], 'n_estimators':[90]}\ngrid_search(rfc, params, 'max_depth')","5fa62c28":"# LogisticRegression: C\nparams = {'C': lr_params['C']}\ngrid_search(lr, params, 'C')","f430476d":"# LightGBM: min_child_weight\nparams = {'min_child_weight': lgbm_params['min_child_weight']}\ngrid_search(lgbm, params, 'min_child_weight')","9c886eba":"# LightGBM: max_depth\nparams = {'max_depth': lgbm_params['max_depth'], 'min_child_weight': [4]}\ngrid_search(lgbm, params, 'max_depth')","131573e5":"# LightGBM: learning_rate\nparams = {'learning_rate': lgbm_params['learning_rate'], 'min_child_weight': [4], 'max_depth':[15]}\ngrid_search(lgbm, params, 'learning_rate')","db11f256":"clfs = []\n\nrfc = RandomForestClassifier(n_jobs=-1, n_estimators=90, max_depth=30)\nrfc_result_post = train_test_model(rfc, X_train, X_test, y_train ,y_test, scoring_name='Log_Loss')\nclfs.append(rfc_result_post['clf'])\n\nlr = LogisticRegression(n_jobs=-1, C=0.1)\nlr_result_post = train_test_model(lr, X_train, X_test, y_train ,y_test, scoring_name='Log_Loss')\nclfs.append(lr_result_post['clf'])\n\nlgbm = LGBMClassifier(n_jobs=-1, objective='multiclass'\n                     , min_child_weight=4, max_depth = 15, learning_rate = 0.1)\nlgbm_result_post = train_test_model(lgbm, X_train, X_test, y_train ,y_test, scoring_name='Log_Loss')\nclfs.append(lgbm_result_post['clf'])","5a354b93":"df_rfc = {\n    'name': ['default_model', 'default_model', 'tuned_model', 'tuned_model'],\n    'type': ['train', 'test', 'train', 'test'],\n    'scores': [rfc_result['scores']['train'], rfc_result['scores']['test']\n               , rfc_result_post['scores']['train'], rfc_result_post['scores']['test']\n              ]\n}\n\ndf_lr = {\n    'name': ['default_model', 'default_model', 'tuned_model', 'tuned_model'],\n    'type': ['train', 'test', 'train', 'test'],\n    'scores': [lr_result['scores']['train'], lr_result['scores']['test']\n               , lr_result_post['scores']['train'], lr_result_post['scores']['test']\n              ]\n}\n\ndf_lgbm = {\n    'name': ['default_model', 'default_model', 'tuned_model', 'tuned_model'],\n    'type': ['train', 'test', 'train', 'test'],\n    'scores': [lgbm_result['scores']['train'], lgbm_result['scores']['test']\n               , lgbm_result_post['scores']['train'], lgbm_result_post['scores']['test']\n              ]\n}","48b8b603":"plt.figure(figsize=(16, 5))\n\nplt.subplot(1, 3, 1)\nplt.title('Log-Loss Scores for Random Forest')\nax = sns.barplot(x='name', y='scores', hue='type', data=df_rfc)\n\nplt.subplot(1, 3, 2)\nplt.title('Log-Loss Scores for Logistic Regression')\nax = sns.barplot(x='name', y='scores', hue='type', data=df_lr)\n\nplt.subplot(1, 3, 3)\nplt.title('Log-Loss Scores for Light GBM')\nax = sns.barplot(x='name', y='scores', hue='type', data=df_lgbm)\nplt.show()","3823e67c":"predictions = []\nfor clf in clfs:\n    predictions.append(clf.predict_proba(X_test))\n\ndef log_loss_func(weights):\n    ''' scipy minimize will pass the weights as a numpy array '''\n    final_prediction = 0\n    for weight, prediction in zip(weights, predictions):\n            final_prediction += weight*prediction\n\n    return log_loss(y_test, final_prediction)\n    \n#the algorithms need a starting value, right not we chose 0.5 for all weights\n#its better to choose many random starting points and run minimize a few times\nstarting_values = [0.5 for i in range(len(predictions))]\n#adding constraints and a different solver\ncons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n#weights are bound between 0 and 1\nbounds = [(0,1)]*len(predictions)\n\nres = minimize(log_loss_func, starting_values, method='SLSQP', bounds=bounds, constraints=cons)\n\nprint('Ensamble Score: {best_score}'.format(best_score=res['fun']))\nprint('Best Weights: {weights}'.format(weights=res['x']))","6a03fb3a":"df_summary = pd.DataFrame(data={\n    'models': ['Random Forest(Tuned)', 'Logistic Regression(Tuned)'\n              , 'Light GBM(Tuned)', 'Ensembling Model'],\n    'scores': [rfc_result_post['scores']['test']\n               , lr_result_post['scores']['test']\n               , lgbm_result_post['scores']['test']\n               , res['fun']\n              ]\n}) \ndf_summary","ca0634da":"for clf in clfs:\n    preds = clf.predict_proba(test)\n    lpreds = clf.predict_proba(test)\n    gpreds = clf.predict_proba(test)\n    xpreds = clf.predict_proba(test)","05c58349":"coef_1 = res['x'][0] \ncoef_2 = res['x'][1] \ncoef_3 = res['x'][2]\nfinalPreds = coef_1*preds + coef_2*lpreds + coef_3*xpreds","1dce0d17":"pred = pd.DataFrame(finalPreds, index=sample.id.values, columns=sample.columns[1:])\npred.to_csv('prediction.csv', index_label='id')","7499f7d3":"Next, let's evaluate the performance of the tuned models.","940206e9":"## Part1: The overviews of data","f9cf3d1f":"## 2 Model training with default setting","60e3dee0":"The charts below show the scores of Log-Loss. The gaps between train and test scores were smaller after hyperparameter tuning. The tuned models were less overfitting. However, I still had room to do further improvement. ","19737fb5":"### Light GBM\n**min_child_weight:** Minimum sum of instance weight (hessian) needed in a child (leaf).","38f1c145":"### Logistic Regression\n\n**C:** regularization strength","1007007b":"**max_depth:** the maximum depth of the tree. (default: None)","130c44db":"This competition has already ended, but we still can submit our predictions to see the scores. Top places have score of 0.38.\n\nMy ensemble model got a score of 0.48983.","77bd078f":"We have 93 features describing products of Otto Group. This a multiple classification problem and the challenge is to classify products into correct categories. In this notebook, I focused on modelling. I experimented with **building an ensemble model** by training different models on the dataset and having each model make predictions individually. The metric I used is log loss.\n\nThe highlights are:\n1. I selected three models: Random Forest, Logistic Regression, and LightGBM. Trained individually with **GridSearchCV** to find the best models.\n2. I used **optimization algorithm to find the best ensemble weights** to combine three prediction results to a final prediction.\n3. The scores of log loss for each modeling showed that the ensemble model was better than any individual model.\n\n![image.png](attachment:image.png)\n\n------\nThe outlines of this notebook:\n- **Part0:** Imports\n- **Part1:** The overviews of data\n- **Part2:** Model training with default setting\n- **Part3:** Hyperparameters tuning\n    - 3.1 Random Forest\n    - 3.2 Logistic Regression\n    - 3.3 Light GBM\n- **Part4:** Optimizing ensemble weights\n- **Part5:** Submission","0c1f2ba0":"## Part5: Submission","e163774d":"Train 3 models with default setting and set the evaluation metric as Log Loss.","e6a9743d":"### 3.1 Random Forest\n\n**n_estimators:** the maximum number of weak learner","9a29f7cb":"The smaller the Log-Loss is the better model performs and the chart shows the training\/testing for each models. We can see that:\n- Random Forest and XGBoost are overfitting.\n- Althought Logistic Regression is not over\/underfitting, its training & testing scores are the worst.\n\n**ACTION-ITEM:** reduce overfitting by hyperparameter tuning in the next part.","d80475d5":"The table below shows Log-Loss. I learned that the ensembling model was better than three other individual models.","2654c2ed":"## Part4: Optimizing ensemble weights\nI used optimization algorithm to find the best weights to ensamble three models.","e083cc12":"**learning_rate** ","ab714655":"Prepare train\/test dataset for model training","69552e84":"## Part3: Hyperparameters tuning\nI tuned a parameter at a time and plotted paires of Log-Loss score and its standard deviaton for each candidate value of parameter to find the best value by balancing Log-Loss scores and its standard deviaton.","7089033f":"**max_depth:** the maximum depth of the tree. ","43acb3b8":"Define some common functions","41e157bf":"## Part0: Imports"}}