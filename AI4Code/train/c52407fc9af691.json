{"cell_type":{"b223bcb7":"code","813c6dee":"code","3d7621e8":"code","5fd13461":"code","69ec5a21":"code","2f389582":"code","77865df6":"code","4c9527c6":"code","8a9c1568":"code","ffd4514b":"code","ab56ed1d":"code","6c1746dc":"code","d5f4818d":"code","7c6eac76":"code","64437ed2":"code","f00e9ff6":"code","bcf700c6":"code","0c8dd1eb":"code","3d3e3511":"code","0d8ed582":"code","a9fbee1e":"code","648d17d1":"code","39d26efb":"code","c627ae34":"code","797f31ba":"code","72077e5a":"markdown","5a994f46":"markdown","addc9e9c":"markdown","bac2232f":"markdown","fb2477f6":"markdown","9bf60577":"markdown","5b8a7780":"markdown","1841e767":"markdown","c5c109cd":"markdown","4a7f9ac6":"markdown","6d1e96f5":"markdown","e46a72ed":"markdown","a0543d0d":"markdown","0fc81a79":"markdown","d96a5573":"markdown","85d0da0f":"markdown","9345f05a":"markdown"},"source":{"b223bcb7":"import pandas as pd       \nimport matplotlib as mat\nimport matplotlib.pyplot as plt    \nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\n\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nfrom cuml.ensemble import RandomForestRegressor\n\nseed = 42","813c6dee":"df_train = pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col = 'id')\nX_test = pd.read_csv('..\/input\/30-days-of-ml\/test.csv', index_col = 'id')\nsubmission = pd.read_csv('..\/input\/30-days-of-ml\/sample_submission.csv')","3d7621e8":"df_train","5fd13461":"df_train.info()","69ec5a21":"df_train.describe()","2f389582":"X_test","77865df6":"X_test.info()","4c9527c6":"X_test.describe()","8a9c1568":"for col in df_train.select_dtypes('object').columns:\n    print(col, '\\n')\n    print(df_train[col].value_counts(), '\\n')","ffd4514b":"for col in X_test.select_dtypes('object').columns:\n    print(col, '\\n')\n    print(X_test[col].value_counts(), '\\n')","ab56ed1d":"X_train = df_train.copy().drop('target', axis = 1)\nY_train = df_train['target'].copy()","6c1746dc":"encoding_map = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6,\n                'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14}\n\nfor col in X_test.select_dtypes('object').columns:\n    X_train[col] = X_train[col].map(encoding_map).astype('int')\n    X_test[col] = X_test[col].map(encoding_map).astype('int')","d5f4818d":"X_train.info()","7c6eac76":"X_test.info()","64437ed2":"for col in X_train.select_dtypes('int').columns:\n    print(col, '\\n')\n    print(X_train[col].value_counts(), '\\n')","f00e9ff6":"for col in X_test.select_dtypes('int').columns:\n    print(col, '\\n')\n    print(X_test[col].value_counts(), '\\n')","bcf700c6":"for col in X_train.select_dtypes('float64').columns:\n    X_train[col] = X_train[col].astype( np.float32 )\n    X_test[col] = X_test[col].astype( np.float32 )\n    \nY_train = Y_train.astype( np.float32 )\n\nfor col in X_train.select_dtypes('int64').columns:\n    X_train[col] = X_train[col].astype( np.float32 )\n    X_test[col] = X_test[col].astype( np.float32 )\n    \nX_train.info()","0c8dd1eb":"X_test.info()","3d3e3511":"rf_model = RandomForestRegressor(n_estimators = 500, random_state = seed, n_streams = 1, bootstrap = False\n                                 , max_depth = 12, max_features=7, min_samples_split = 10, min_samples_leaf = 5\n                                 , accuracy_metric = 'mse')","0d8ed582":"def cv_function (X_train, Y_train, model, splits = 10, seed = seed):\n    \n    print('seed:', seed)\n\n    kfold = KFold(n_splits = splits, shuffle=True, random_state = seed)\n    rmse = []\n   \n    cv_pred = np.zeros(len(X_train))\n    \n    for idx in kfold.split(X=X_train, y=Y_train):\n        train_idx, test_idx = idx[0], idx[1]\n        xtrain = X_train.iloc[train_idx]\n        ytrain = Y_train.iloc[train_idx]\n        xtest = X_train.iloc[test_idx]\n        ytest = Y_train.iloc[test_idx]\n        \n        # fit model for current fold\n        model.fit(xtrain, ytrain)\n\n        #create predictions\n        preds = model.predict(xtest)\n        cv_pred[test_idx] = preds\n                              \n        # calculate and append accuracy\n        fold_rmse = mean_squared_error(ytest,preds, squared=False)\n        print(\"RMSE: {0:0.5f}\". format(fold_rmse))\n        rmse.append(fold_rmse)\n        \n    print (np.mean(rmse))\n    return cv_pred","a9fbee1e":"%%time\nrf_cvpred = cv_function(X_train, Y_train, rf_model)","648d17d1":"def prediction (X_train, Y_train, model, X_test, seed = seed):\n    \n    print('seed:', seed)\n        \n    kfold = KFold(n_splits = 10, shuffle=True, random_state = seed)\n    \n    y_pred = np.zeros(len(X_test))\n    train_oof = np.zeros(len(X_train))\n    \n    for idx in kfold.split(X=X_train, y=Y_train):\n        train_idx, val_idx = idx[0], idx[1]\n        xtrain = X_train.iloc[train_idx]\n        ytrain = Y_train.iloc[train_idx]\n        xval = X_train.iloc[val_idx]\n        yval = Y_train.iloc[val_idx]\n        \n        # fit model for current fold\n        model.fit(xtrain, ytrain)\n\n        #create predictions\n        y_pred += model.predict(X_test)\/kfold.n_splits\n        print(y_pred)\n               \n        val_pred = model.predict(xval)\n        # getting out-of-fold predictions on training set\n        train_oof[val_idx] = val_pred\n\n        # calculate and append rmsle\n        rmse = mean_squared_error(yval,val_pred, squared=False)\n        print('RMSE : {}'.format(rmse))\n  \n    return y_pred, train_oof","39d26efb":"pred, train_oof = prediction (X_train, Y_train, rf_model, X_test)","c627ae34":"train_oof = pd.DataFrame(train_oof, columns = ['target'])\ntrain_oof.to_csv('train_oof.csv', index=False)\n\ntrain_oof","797f31ba":"submission['target'] = pred\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission","72077e5a":"## <center>If you find this notebook useful, support with an upvote!<center>","5a994f46":"## <center>If you find this notebook useful, support with an upvote!<center>","addc9e9c":"To use Random Forest on GPU, we import it from the cuML package with:\n>from cuml.ensemble import RandomForestRegressor\n\nWarning: cuML is only native on Kaggle notebooks running on GPU. If you try to import the model with the GPU turned off, you\u2019ll get the following error message:\n>ModuleNotFoundError: No module named 'cuml'","bac2232f":"It took less than 3min for our model to run all the 10 iterations. With the current parameters, running just one iteration on CPU would take a lot longer than that. Based on that, we can clearly see how useful the cuML package can be for datasets such as this one.","fb2477f6":"All features have numerical values now, with no missing values.\n\nLet's check the new values.","9bf60577":"## Categorical Features ","5b8a7780":"## Importing Packages and Datasets + First Look at the Data","1841e767":"There are some categorical features in this dataset. Let\u2019s take a look at them.","c5c109cd":"Now we are good to go. Let\u2019s define our model.","4a7f9ac6":"# <center>30 Days of Machine Learning<center>\n## <center>Random Forest on GPU (RAPIDS\/cuML)<center>\n---\nThe goal of this notebook is to show how to run a Random Forest Regressor on GPU for improved speed in larger datasets, such as this one.  Since Sklearn doesn\u2019t offer any support for GPU, it is necessary to find an alternative package to perform this task. Enter \u2018cuML\u2019. The cuML package ([link](https:\/\/docs.rapids.ai\/api\/cuml\/stable\/)), developed by the RAPIDS team, is a suite that mirrors Sklearn\u2019 API with the advantage of providing the option to run its models on GPU.\n    \nIt is important to point out that I won\u2019t be performing a hyperparameter tunning in this notebook, and that a higher RMSE was already expected in comparison to a well implemented XGBoost model. The intent is to simply provide a way for people to test the Random Forest model in this dataset within a more \u2018reasonable\u2019 running time.\n    \nThis work is based on the following notebook (suggested by [@miwojc](https:\/\/www.kaggle.com\/miwojc) on the \u201830 days\u2019 Discord):\n* [RandomForest on GPU in 3 minutes](https:\/\/www.kaggle.com\/titericz\/randomforest-on-gpu-in-3-minutes) by [Giba](https:\/\/www.kaggle.com\/titericz)\n    \n---\n    \nIf you're looking for an EDA notebook in this dataset, here is my suggestion:\n* [30 Days of ML - EDA](https:\/\/www.kaggle.com\/dwin183287\/30-days-of-ml-eda) by [Sharlto Cope](https:\/\/www.kaggle.com\/dwin183287)\n    \nMy other notebooks in this competition:\n* [30 Days of Machine Learning: Starter - Auto EDA + Base XGBoost (GPU)](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/30days-starter-auto-eda-base-xgboost-gpu)\n* [30 Days of Machine Learning: LightGBM Model (+ Blend with XGBoost)](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/30days-lightgbm-blend-with-xgboost)\n* [30 Days of Machine Learning: Final Stacking](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/30days-final-stacking)","6d1e96f5":"## Making Predictions","e46a72ed":"Let\u2019s call our CV function and see how long it takes to run all the 10 iterations.","a0543d0d":"We will be testing our model on a 10-fold Cross-Validation procedure.","0fc81a79":"Now that we know our model works, we can finish our notebook using it to make predictions on the test dataset and to create the submission file.","d96a5573":"We need to encode the categorical features before using them in our model. For this step, I\u2019ll encode them applying the map function.","85d0da0f":"## Random Forest on GPU (cuML)","9345f05a":"If we were going to run the model on CPU, we would already be good to go. However, to run the cuML model on GPU, the data type must be set to float32. Let's do this. "}}