{"cell_type":{"cccb7458":"code","c186df9a":"code","f19c62d4":"code","e1542673":"code","787790cf":"code","3aa5924c":"code","c9ab12fb":"code","6a7b90f6":"code","5ce4dcde":"code","b13c8ea0":"code","b948354e":"code","a7a3902b":"code","24f0d785":"code","eca10aa4":"code","1bd32277":"code","f48a6a2a":"code","73544cc9":"code","35284eab":"code","cd5fa9fd":"code","663e3730":"code","d3ed6aa4":"code","cf3180b0":"code","bfd57af8":"code","7fbc127d":"code","3f339d7a":"markdown","47ac98be":"markdown","61ebc85f":"markdown","ba26222c":"markdown","1d373e7a":"markdown","7ef02a9c":"markdown","ccf207c5":"markdown","ff9bec4f":"markdown","49e10fae":"markdown","35af7f65":"markdown","4bb96c04":"markdown"},"source":{"cccb7458":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c186df9a":"import keras\nfrom keras.preprocessing.text import Tokenizer\n\nfrom keras.layers import Dropout, Dense,Input,Embedding,Flatten, MaxPooling1D, Conv1D\nfrom keras.models import Sequential,Model\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import metrics\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.datasets import fetch_20newsgroups\nfrom keras.layers.merge import Concatenate\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nfrom keras.layers import Dropout, Dense,Input,Embedding,Flatten, MaxPooling1D, Conv1D\nfrom keras.models import Sequential,Model\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.datasets import fetch_20newsgroups\nfrom keras.layers.merge import Concatenate\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nfrom keras.backend import concatenate","f19c62d4":"# for bert model\n!pip install ktrain\nimport ktrain\nfrom ktrain import text","e1542673":"df = pd.read_csv('\/kaggle\/input\/a-fake-news-dataset-around-the-syrian-war\/FA-KES-Dataset.csv',encoding='latin1')\ndf.info()","787790cf":"# df['article_title'].apply(lambda x: ' ') is used to create a space between two column text\ntexts = np.array(df['article_title'] + df['article_title'].apply(lambda x: ' ') + df['article_content'])\ntarget = df['labels']\n# texts[0], target[0]","3aa5924c":"target_names = np.unique(target).tolist()\ntarget_names","c9ab12fb":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\ntexts, target, test_size=0.3, random_state=42)\n\nX_train = X_train.tolist()\nX_test = X_test.tolist()\ny_train = y_train.tolist()\ny_test = y_test.tolist()","6a7b90f6":"(X_train,  y_train), (X_test, y_test), preproc = text.texts_from_array(x_train=X_train, y_train=y_train,\n                                                                       x_test=X_test, y_test=y_test,\n                                                                       class_names=target_names,\n                                                                       preprocess_mode='bert',\n                                                                       maxlen=350, \n                                                                       max_features=35000)","5ce4dcde":"# you can disregard the deprecation warnings arising from using Keras 2.2.4 with TensorFlow 1.14.\nmodel = text.text_classifier('bert', train_data=(X_train, y_train), preproc=preproc)\nlearner = ktrain.get_learner(model, train_data=(X_train, y_train), batch_size=6)","b13c8ea0":"learner.fit_onecycle(2e-5, 5)","b948354e":"learner.validate(val_data=(X_test, y_test), class_names=target_names)","a7a3902b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(texts, target, test_size=0.3, random_state=42)","24f0d785":"# tokenization\nMAX_NB_WORDS = 3000\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS) # based on the word frequency, num_words-1 words will be kept\ntokenizer.fit_on_texts(texts)","eca10aa4":"# word indexing\nword_index = tokenizer.word_index\nprint(\"Data type of word index: {} and length of dictionary {}\".format(type(word_index),\n                                                                       len(word_index)) )","1bd32277":"# vectorization\nsequences_train = tokenizer.texts_to_sequences(X_train)\nsequences_test = tokenizer.texts_to_sequences(X_test)\nprint(\"Sequence: \", sequences_train[0])","f48a6a2a":"# padding\nMAX_SEQUENCE_LENGTH = 1600\ntexts_train = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH) # default value truncate the previous sequence\ntexts_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH) # default value truncate the previous sequence\nprint(\"Padding Result: \", texts_train[0])","73544cc9":"# assignment of word embedding\n## access pretrained word embedding file\npath='\/kaggle\/input\/gloveicg\/glove\/Glove\/glove.6B.300d.txt'\nembeddings_index = {}\nf = open(path, encoding=\"utf8\")\nfor line in f:\n    values = line.split()\n    word = values[0]\n    try:\n        coefs = np.asarray(values[1:], dtype='float32')\n    except:\n        pass\n    embeddings_index[word] = coefs\nf.close()\n\n## build word embedding matrix based on word index\nEMBEDDING_DIM = 300\nembedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        if len(embedding_matrix[i]) !=len(embedding_vector):\n            print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])),\n                                 \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n                                 \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n            exit(1)\n        embedding_matrix[i] = embedding_vector\n\nembedding_matrix.shape # words, vector length","35284eab":"print(\"Number of samples, Max sequence length, embedding dim\")\nlen(df), MAX_SEQUENCE_LENGTH, EMBEDDING_DIM","cd5fa9fd":"embedding_layer = Embedding(len(word_index) + 1,\n                                EMBEDDING_DIM,\n                                weights=[embedding_matrix],\n                                input_length=MAX_SEQUENCE_LENGTH,\n                                trainable=True)","663e3730":"# sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n# embedded_sequences = embedding_layer(sequence_input)\n# flat_1 = Flatten()(embedded_sequences)\n# flat_1.shape\n# embedded_sequences.shape","d3ed6aa4":"dropout=0.5\nnclasses = 2\n\n# input layer and embedding layer\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='input_embedding')\nembedded_sequences = embedding_layer(sequence_input)\nflat_1 = Flatten(name='flatten_1')(embedded_sequences)\n# dense layer and dropout layer\nl_dense = Dense(512, activation='relu', name='dense_layer_1')(flat_1)\nl_dropout = Dropout(dropout)(l_dense)\n\n### prediction\npreds = Dense(nclasses, activation='softmax', name='prediction_layer')(l_dropout)\n\nmodel = Model([sequence_input], preds)\nmodel.compile(loss='sparse_categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])","cf3180b0":"from keras.utils.vis_utils import plot_model\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","bfd57af8":"model.fit(texts_train, y_train,\n              batch_size=128,\n              epochs=5,\n          validation_data =(texts_test, y_test),\n          verbose=True)","7fbc127d":"loss, accuracy = model.evaluate(texts_test, y_test, verbose=0)\nprint(\"Loss value: {}, accuracy:{}\".format(loss, accuracy))","3f339d7a":"### Split Data","47ac98be":"### Build and Train CNN Model","61ebc85f":"### Split Data","ba26222c":"### Preprocess Data and Extract Features for Bert Model","1d373e7a":"## Bert Model","7ef02a9c":"### Preprocess Data and Extract Features for CNN Model\n1. tokenization\n2. word indexing\n3. vectorization\n4. padding\n5. assignment of word embedding","ccf207c5":"Define feature text and target","ff9bec4f":"## CNN Model","49e10fae":"To predict fake news, trained a pretrained Bert model and CNN model. ","35af7f65":"### Build and Train Bert Model","4bb96c04":"## Import Data"}}