{"cell_type":{"974cd28e":"code","5034928d":"code","78db6946":"code","26eb2cfa":"code","958bd7a1":"code","ebc38ff8":"code","86645eba":"code","8e0378ba":"code","8af3777b":"code","270f9d55":"code","061b4290":"code","07c58791":"code","b0cf7aac":"code","e2358e1f":"code","0a781059":"code","507f2a96":"code","a70bb8d8":"code","146eb650":"code","87ea258a":"markdown","37ae7b6c":"markdown","4b4ec7d1":"markdown","9963b3f3":"markdown","496db30f":"markdown","67452692":"markdown","9d744746":"markdown","27b7c8dc":"markdown","6323fc07":"markdown","36143283":"markdown","27bdd71b":"markdown","7c32bb9c":"markdown","8d843c31":"markdown","4165a78c":"markdown","44576531":"markdown","ef412a4e":"markdown","517db873":"markdown","7639df6c":"markdown","2657bcd4":"markdown","b9fd80ad":"markdown","a7c1f79d":"markdown"},"source":{"974cd28e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5034928d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('darkgrid')\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import f1_score as f1\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score, precision_score\nimport scikitplot as skplt\nfrom collections import Counter\n\nplt.rc('figure',figsize=(18,9))\n%pip install imbalanced-learn\nfrom imblearn.over_sampling import SMOTE","78db6946":"credit_data = pd.read_csv('\/kaggle\/input\/credit-card-customers\/BankChurners.csv')\ncredit_data = credit_data[credit_data.columns[:-2]]\ncredit_data.head(3)","26eb2cfa":"X = credit_data.drop('Attrition_Flag', axis=1)\ny = credit_data['Attrition_Flag']","958bd7a1":"plt.figure(figsize = (18,10))\nplt.title(\"Age with Churned or not\", fontsize = 30)\nsns.countplot(data = credit_data, x = credit_data[\"Customer_Age\"], hue = \"Attrition_Flag\")","ebc38ff8":"fig, ax = plt.subplots(figsize=(7, 7))\ncount = Counter(y)\nax.pie(count.values(), labels=count.keys(), autopct=lambda p:f'{p:.2f}%')\nax.set_title('Percentage of existing and attrited\/churned customers')\nplt.show()","86645eba":"#countplot\nplt.figure(figsize = (12,8))\nplt.title(\"Gender wise Existing and Churned customers\")\nsns.countplot(data = credit_data, x = credit_data[\"Gender\"], hue = \"Attrition_Flag\")","8e0378ba":"gender = credit_data.loc[credit_data[\"Attrition_Flag\"] == \"Attrited Customer\", [\"Education_Level\"]].value_counts()\ngender_normal = credit_data.loc[credit_data[\"Attrition_Flag\"] == \"Attrited Customer\", [\"Education_Level\"]].value_counts().tolist()\ngender_churned = credit_data.loc[credit_data[\"Attrition_Flag\"] == \"Existing Customer\", [\"Education_Level\"]].value_counts().tolist()\nfig, ax = plt.subplots(1, 2, dpi = 200, figsize = (15,8))\nax[0].set_title(\"Education Level in Existing Customers\")\nax[0].pie(x = gender_normal, labels = gender.index, autopct='%.2f%%')\nax[1].set_title(\"Education Level in Churned Customers\")\nax[1].pie(x = gender_churned, labels = gender.index, autopct='%.2f%%')","8af3777b":"fig, ax = plt.subplots(figsize=(15, 8))\nsns.heatmap(X.corr(), annot=True)\nplt.show()","270f9d55":"# #--------------let's convert some categorical variables into numerical--------------\n\n# #ordinal to numerical\nmap_education_level = {'High School':1,'Graduate':3,'Uneducated':0,'College':2,'Post-Graduate':4,'Doctorate':5}\nmap_income_level = {'$60K - $80K':3,'Less than $40K':1, '$80K - $120K':4,'$40K - $60K':2,'$120K +':5}\nmap_card_category = {'Blue':1,'Gold':3,'Silver':2,'Platinum':4}\nX['Education_Level'].replace(map_education_level,inplace=True)\nX['Income_Category'].replace(map_income_level,inplace=True)\nX['Card_Category'].replace(map_card_category,inplace=True)\n\n\n# #hot encoding of gender category\nX.insert(2,'Gender_M',X['Gender'],True)\nX.rename({'Gender':'Gender_F'},axis=1,inplace=True)\nX['Gender_M'].replace({'M':1,'F':0},inplace=True)\nX['Gender_F'].replace({'M':0,'F':1},inplace=True)\n\n\n# #hot encoding of marital status\nX.insert(7,'Single',X['Marital_Status'],True)\nX.insert(7,'Divorced',X['Marital_Status'],True)\nX.insert(7,'Unknown',X['Marital_Status'],True)\nX.rename({'Marital_Status':'Married'},axis=1,inplace=True)\nX['Married'].replace({'Single':0, 'Married':1, 'Divorced':0, 'Unknown':0},inplace=True)\nX['Single'].replace({'Single':1, 'Married':0, 'Divorced':0, 'Unknown':0},inplace=True)\nX['Divorced'].replace({'Single':0, 'Married':0, 'Divorced':1, 'Unknown':0},inplace=True)\nX['Unknown'].replace({'Single':0, 'Married':0, 'Divorced':0, 'Unknown':1},inplace=True)\n\n\ny.replace({'Existing Customer':0, 'Attrited Customer':1},inplace=True)\n\nX.head()","061b4290":"plt.hist(X.loc[X['Income_Category']!='Unknown']['Income_Category'])   # income is rightly skewed. so central value is median\nplt.show()\n\nplt.hist(X.loc[X['Education_Level']!='Unknown']['Education_Level'])   # education is normally distributed. so central value is mean\nplt.show()\n\n#Missing values in education column\neducatedDF = X.loc[X['Education_Level']!='Unknown']\nmean_education = educatedDF['Education_Level'].mean()\nX['Education_Level'].replace({'Unknown':mean_education},inplace=True)\n\n#Missing values in income column\nsalariedDF = X.loc[X['Income_Category']!='Unknown']\nmedian_salaries = salariedDF['Income_Category'].median()\nX['Income_Category'].replace({'Unknown':median_salaries},inplace=True)\n\nX.head()","07c58791":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)","b0cf7aac":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","e2358e1f":"#-----Upsampling----\nfrom sklearn.utils import resample\nfrom collections import Counter\n\nprint(\"Before Upsampling:-\")\nprint(Counter(y_train))\n\n\n# Let's use SMOTE to oversample\nfrom imblearn.over_sampling import SMOTE\noversample = SMOTE()\nX_train_upsampled, y_train_upsampled = oversample.fit_resample(X_train,y_train)\n\nprint(\"After Upsampling:-\")\nprint(Counter(y_train_upsampled))","0a781059":"classifier = RandomForestClassifier(n_estimators = 50, random_state = 0)\nclassifier.fit(X_train_upsampled, y_train_upsampled)\n\n# Predicting result for training set and validation set\npredict_val_rf = classifier.predict(X_test)\n\n# Model Performance\nprint(\"Accuracy : \", accuracy_score(y_test, predict_val_rf) *  100)\nprint(\"Recall : \", recall_score(y_test, predict_val_rf) *  100)\nprint(\"Precision : \", precision_score(y_test, predict_val_rf) *  100)\nprint(confusion_matrix(y_test, predict_val_rf))\nprint(classification_report(y_test, predict_val_rf))","507f2a96":"model = XGBClassifier()\nmodel.fit(X_train_upsampled, y_train_upsampled)\n\n# Predicting result for training set and validation set\npredict_val_rf2 = model.predict(X_test)\n\n# Model Performance\nprint(\"Accuracy : \", accuracy_score(y_test, predict_val_rf2) *  100)\nprint(\"Recall : \", recall_score(y_test, predict_val_rf2) *  100)\nprint(\"Precision : \", precision_score(y_test, predict_val_rf2) *  100)\nprint(confusion_matrix(y_test, predict_val_rf2))\nprint(classification_report(y_test, predict_val_rf2))","a70bb8d8":"gaussian = GaussianNB()\ngaussian.fit(X_train_upsampled, y_train_upsampled)\n\n# Predicting result for training set and validation set\npredict_val_rf2 = gaussian.predict(X_test)\n\n# Model Performance\nprint(\"Accuracy : \", accuracy_score(y_test, predict_val_rf2) *  100)\nprint(\"Recall : \", recall_score(y_test, predict_val_rf2) *  100)\nprint(\"Precision : \", precision_score(y_test, predict_val_rf2) *  100)\nprint(confusion_matrix(y_test, predict_val_rf2))\nprint(classification_report(y_test, predict_val_rf2))","146eb650":"svc = SVC()\nsvc.fit(X_train_upsampled, y_train_upsampled)\n\n# Predicting result for training set and validation set\npredict_val_rf2 = svc.predict(X_test)\n\n# Model Performance\nprint(\"Accuracy : \", accuracy_score(y_test, predict_val_rf2) *  100)\nprint(\"Recall : \", recall_score(y_test, predict_val_rf2) *  100)\nprint(\"Precision : \", precision_score(y_test, predict_val_rf2) *  100)\nprint(confusion_matrix(y_test, predict_val_rf2))\nprint(classification_report(y_test, predict_val_rf2))","87ea258a":"## Load the dataset","37ae7b6c":"## Building Classification models\n","4b4ec7d1":"## Scaling Numeric Values","9963b3f3":"## DATA PROCESSING\n","496db30f":"## Upsampling using SMOTE","67452692":"As we can see only 16% of the data samples represent churn customers, so we will use SMOTE to upsample the churn samples to match them with the regular customer sample size in order to give the later selected models a better chance of catching on small details which will almost definitely be missed out with such a size difference.","9d744746":"2. XG Boost","27b7c8dc":"## Load all the required libraries","6323fc07":"1. Random Forest","36143283":"Convert all the categorical variables into numerical variable","27bdd71b":"## Exploratory Data Analysis","7c32bb9c":"**4. Education Level in Existing and Churned Customers**","8d843c31":"We can see that the distribution of customer ages in our dataset follows a fairly normal distribution, thus further use of the age feature can be done with the normality assumption.","4165a78c":"The features in our dataset have some correlation, for example, Months_on_book and Customer_age, Avg_Utilization_Ratio and Total_Revolving_Bal.","44576531":"**2. Percentage of existing and attrited customers**","ef412a4e":"## Data Split","517db873":"4. Support Vector Machines","7639df6c":"1. **Age distribution**","2657bcd4":"**3. Gender wise Existing and Churned customers**","b9fd80ad":"## Handling missing values","a7c1f79d":"3. Naive Bayes"}}