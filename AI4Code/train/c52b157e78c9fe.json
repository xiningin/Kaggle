{"cell_type":{"cebb7c7f":"code","043c44df":"code","6290bca4":"code","3c39cce2":"code","77584d9e":"code","99d41bf7":"code","053c3bf9":"code","6b26aa03":"code","0e10c095":"code","049351e6":"code","02de39bf":"code","14a423c1":"code","c6e47c36":"code","b3556130":"code","70f759d9":"code","47d18007":"code","47b5ebe5":"markdown","e3ab01fb":"markdown","998a79d7":"markdown","208a56bf":"markdown","6631cfb3":"markdown","cddac0df":"markdown","ef95f24c":"markdown"},"source":{"cebb7c7f":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split\n\nimport time\nimport os\n","043c44df":"##import the data\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/test.csv\")\n","6290bca4":"train.head()","3c39cce2":"test.head()","77584d9e":"print('train data shape:',train.shape)\nprint('test data shape:',test.shape)\n","99d41bf7":"##misssing values count\nprint(\"train x data null value sum:\", train.isnull().sum().sum())\nprint(\"train y data null value sum:\", test.isnull().sum().sum())","053c3bf9":"## Separate features and targets \ndf_x = train.iloc[:,1:15]\ndf_y = train.target","6b26aa03":"# correlation map\ncorr_data = df_x.corr()\n\nplt.figure(figsize=(10,10))\nsns.heatmap(corr_data,square = True,vmax = 0.8)","0e10c095":"plot = plt.boxplot(df_x.T)","049351e6":"plot = plt.boxplot(test.iloc[:,1:].T)","02de39bf":"def IQR(dist):\n    return np.percentile(dist, 75) - np.percentile(dist, 25)\n\ndef handle_outliers(series):\n    \n    IQR_data = IQR(series)\n    percentile_75 = np.percentile(series, 75)\n    percentile_25 = np.percentile(series, 25)\n    \n    for i in range(series.shape[0]):\n        \n        if series[i] > percentile_75 + 1.5*IQR_data:\n            series[i] = percentile_75 + 1.5*IQR_data\n            \n        if series[i] < percentile_25 - 1.5*IQR_data:\n            series[i] = percentile_25 - 1.5*IQR_data\n            \n    return series\n\n            \ndf_x['cont7'] = handle_outliers(df_x['cont7'] )\ndf_x['cont9'] = handle_outliers(df_x['cont9'] )\n\ntest['cont7'] = handle_outliers(test['cont7'] )\ntest['cont9'] = handle_outliers(test['cont9'] )    \n    \n    ","14a423c1":"%matplotlib inline\ncolumn_reshape = np.array(df_x.columns).reshape(2,7)\n\nfig, ax = plt.subplots(2,7,figsize = (20,5))\nfor i in range(ax.shape[0]):\n    for j in range(ax.shape[1]):\n        plot = ax[i,j].hist(df_x[column_reshape[i,j]],bins = 50, density = True, color = 'purple')\n        ax[i,j].set_title(column_reshape[i,j])\n        ax[i,j].axis('off')\n","c6e47c36":"##decrease Skewness\ndf_x['cont7'] = np.log1p(df_x['cont7'])\ndf_x['cont11'] = np.log1p(df_x['cont11'])\ndf_x['cont12'] = np.log1p(df_x['cont12'])\n\ntest['cont7'] = np.log1p(test['cont7'])\ntest['cont11'] = np.log1p(test['cont11'])\ntest['cont12'] = np.log1p(test['cont12'])","b3556130":"##Split the train and validate data set\ntrainX, testX, trainY, testY = train_test_split(df_x,df_y,test_size=0.18, random_state=2021)","70f759d9":"ts = time.time()\n\nmodel = XGBRegressor(\n    max_depth=20,\n    n_estimators=300,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n\nmodel.fit(\n    trainX, \n    trainY, \n    eval_metric=\"rmse\", \n    eval_set=[(testX, testY)], \n    verbose=True, \n    early_stopping_rounds = 30)\n\ntime.time() - ts","47d18007":"prediction_xgb  = model.predict(test.iloc[:,1:])\n\nresults = pd.Series(prediction_xgb,name=\"target\")\n\nsubmission = pd.concat([test.iloc[:,0],results],axis = 1)\n\nsubmission.to_csv(\"submission_xgb.csv\",index=False)","47b5ebe5":"#### Start","e3ab01fb":"> ####  Now I have basic understanding of how data looks like. Then I would like to check if exists any correlations among 14 features ","998a79d7":"> #### distribution ","208a56bf":"> #### If exists outliers","6631cfb3":"## Train XG Boost model","cddac0df":"> #### Seems feature 11 and 12 have strong correlations, maybe can do some explorations later","ef95f24c":"#### handle outliers"}}