{"cell_type":{"44719d95":"code","d928887c":"code","ce8e0693":"code","f4129864":"code","5c13359e":"code","77792f98":"code","d4972998":"code","ba458c05":"code","3d9faca9":"code","8a6be970":"code","21855fe6":"code","0796205a":"code","ba3d123a":"code","ae07515f":"code","73f52009":"code","a33c933e":"code","b66fa9ce":"code","8a1a4014":"markdown","e903425c":"markdown","4b8bb9b9":"markdown"},"source":{"44719d95":"!pip install lyft-dataset-sdk -q","d928887c":"from datetime import datetime\nfrom functools import partial\nimport glob\nfrom multiprocessing import Pool\n\n# Disable multiprocesing for numpy\/opencv. We already multiprocess ourselves, this would mean every subprocess produces\n# even more threads which would lead to a lot of context switching, slowing things down a lot.\nimport os\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport pandas as pd\nimport cv2\nfrom PIL import Image\nimport numpy as np\nfrom tqdm import tqdm, tqdm_notebook\nimport scipy\nimport scipy.ndimage\nimport scipy.special\nfrom scipy.spatial.transform import Rotation as R\nfrom pathlib import Path\n\nfrom lyft_dataset_sdk.lyftdataset import LyftDataset,LyftDatasetExplorer\nfrom lyft_dataset_sdk.utils.data_classes import LidarPointCloud, Box, Quaternion\nfrom lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix\nimport time\nfrom lyft_dataset_sdk.utils.map_mask import MapMask","ce8e0693":"!ln -s \/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/test_images images\n!ln -s \/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/test_maps maps\n!ln -s \/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/test_lidar lidar","f4129864":"class LyftTestDataset(LyftDataset):\n    \"\"\"Database class for Lyft Dataset to help query and retrieve information from the database.\"\"\"\n\n    def __init__(self, data_path: str, json_path: str, verbose: bool = True, map_resolution: float = 0.1):\n        \"\"\"Loads database and creates reverse indexes and shortcuts.\n        Args:\n            data_path: Path to the tables and data.\n            json_path: Path to the folder with json files\n            verbose: Whether to print status messages during load.\n            map_resolution: Resolution of maps (meters).\n        \"\"\"\n\n        self.data_path = Path(data_path).expanduser().absolute()\n        self.json_path = Path(json_path)\n\n        self.table_names = [\n            \"category\",\n            \"attribute\",\n            \"sensor\",\n            \"calibrated_sensor\",\n            \"ego_pose\",\n            \"log\",\n            \"scene\",\n            \"sample\",\n            \"sample_data\",\n            \"map\",\n        ]\n\n        start_time = time.time()\n\n        # Explicitly assign tables to help the IDE determine valid class members.\n        self.category = self.__load_table__(\"category\")\n        self.attribute = self.__load_table__(\"attribute\")\n        \n        \n        self.sensor = self.__load_table__(\"sensor\")\n        self.calibrated_sensor = self.__load_table__(\"calibrated_sensor\")\n        self.ego_pose = self.__load_table__(\"ego_pose\")\n        self.log = self.__load_table__(\"log\")\n        self.scene = self.__load_table__(\"scene\")\n        self.sample = self.__load_table__(\"sample\")\n        self.sample_data = self.__load_table__(\"sample_data\")\n        \n        self.map = self.__load_table__(\"map\")\n\n        # Initialize map mask for each map record.\n        for map_record in self.map:\n            map_record[\"mask\"] = MapMask(self.data_path \/ map_record[\"filename\"], resolution=map_resolution)\n\n        if verbose:\n            for table in self.table_names:\n                print(\"{} {},\".format(len(getattr(self, table)), table))\n            print(\"Done loading in {:.1f} seconds.\\n======\".format(time.time() - start_time))\n\n        # Initialize LyftDatasetExplorer class\n        self.explorer = LyftDatasetExplorer(self)\n        # Make reverse indexes for common lookups.\n        self.__make_reverse_index__(verbose)\n        \n    def __make_reverse_index__(self, verbose: bool) -> None:\n        \"\"\"De-normalizes database to create reverse indices for common cases.\n        Args:\n            verbose: Whether to print outputs.\n        \"\"\"\n\n        start_time = time.time()\n        if verbose:\n            print(\"Reverse indexing ...\")\n\n        # Store the mapping from token to table index for each table.\n        self._token2ind = dict()\n        for table in self.table_names:\n            self._token2ind[table] = dict()\n\n            for ind, member in enumerate(getattr(self, table)):\n                self._token2ind[table][member[\"token\"]] = ind\n\n        # Decorate (adds short-cut) sample_data with sensor information.\n        for record in self.sample_data:\n            cs_record = self.get(\"calibrated_sensor\", record[\"calibrated_sensor_token\"])\n            sensor_record = self.get(\"sensor\", cs_record[\"sensor_token\"])\n            record[\"sensor_modality\"] = sensor_record[\"modality\"]\n            record[\"channel\"] = sensor_record[\"channel\"]\n\n        # Reverse-index samples with sample_data and annotations.\n        for record in self.sample:\n            record[\"data\"] = {}\n            record[\"anns\"] = []\n\n        for record in self.sample_data:\n            if record[\"is_key_frame\"]:\n                sample_record = self.get(\"sample\", record[\"sample_token\"])\n                sample_record[\"data\"][record[\"channel\"]] = record[\"token\"]\n\n        # Add reverse indices from log records to map records.\n        if \"log_tokens\" not in self.map[0].keys():\n            raise Exception(\"Error: log_tokens not in map table. This code is not compatible with the teaser dataset.\")\n        log_to_map = dict()\n        for map_record in self.map:\n            for log_token in map_record[\"log_tokens\"]:\n                log_to_map[log_token] = map_record[\"token\"]\n        for log_record in self.log:\n            log_record[\"map_token\"] = log_to_map[log_record[\"token\"]]\n\n        if verbose:\n            print(\"Done reverse indexing in {:.1f} seconds.\\n======\".format(time.time() - start_time))","5c13359e":"level5data = LyftTestDataset(data_path='.', json_path='\/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/test_data', verbose=True)\n# Our code will generate data, visualization and model checkpoints, they will be persisted to disk in this folder\nARTIFACTS_FOLDER = \".\/artifacts\"\nos.makedirs(ARTIFACTS_FOLDER, exist_ok=True)\nclasses = [\"car\", \"motorcycle\", \"bus\", \"bicycle\", \"truck\", \"pedestrian\", \"other_vehicle\", \"animal\", \"emergency_vehicle\"]","77792f98":"sample_sub = pd.read_csv('..\/input\/3d-object-detection-for-autonomous-vehicles\/sample_submission.csv')\nsample_sub.head()","d4972998":"def create_transformation_matrix_to_voxel_space(shape, voxel_size, offset):\n    \"\"\"\n    Constructs a transformation matrix given an output voxel shape such that (0,0,0) ends up in the center.\n    Voxel_size defines how large every voxel is in world coordinate, (1,1,1) would be the same as Minecraft voxels.\n    \n    An offset per axis in world coordinates (metric) can be provided, this is useful for Z (up-down) in lidar points.\n    \"\"\"\n    \n    shape, voxel_size, offset = np.array(shape), np.array(voxel_size), np.array(offset)\n    \n    tm = np.eye(4, dtype=np.float32)\n    translation = shape\/2 + offset\/voxel_size\n    \n    tm = tm * np.array(np.hstack((1\/voxel_size, [1])))\n    tm[:3, 3] = np.transpose(translation)\n    return tm\n\ndef transform_points(points, transf_matrix):\n    \"\"\"\n    Transform (3,N) or (4,N) points using transformation matrix.\n    \"\"\"\n    if points.shape[0] not in [3,4]:\n        raise Exception(\"Points input should be (3,N) or (4,N) shape, received {}\".format(points.shape))\n    return transf_matrix.dot(np.vstack((points[:3, :], np.ones(points.shape[1]))))[:3, :]","ba458c05":"def car_to_voxel_coords(points, shape, voxel_size, z_offset=0):\n    if len(shape) != 3:\n        raise Exception(\"Voxel volume shape should be 3 dimensions (x,y,z)\")\n        \n    if len(points.shape) != 2 or points.shape[0] not in [3, 4]:\n        raise Exception(\"Input points should be (3,N) or (4,N) in shape, found {}\".format(points.shape))\n\n    tm = create_transformation_matrix_to_voxel_space(shape, voxel_size, (0, 0, z_offset))\n    p = transform_points(points, tm)\n    return p\n\ndef create_voxel_pointcloud(points, shape, voxel_size=(0.5,0.5,1), z_offset=0):\n\n    points_voxel_coords = car_to_voxel_coords(points.copy(), shape, voxel_size, z_offset)\n    points_voxel_coords = points_voxel_coords[:3].transpose(1,0)\n    points_voxel_coords = np.int0(points_voxel_coords)\n    \n    bev = np.zeros(shape, dtype=np.float32)\n    bev_shape = np.array(shape)\n\n    within_bounds = (np.all(points_voxel_coords >= 0, axis=1) * np.all(points_voxel_coords < bev_shape, axis=1))\n    \n    points_voxel_coords = points_voxel_coords[within_bounds]\n    coord, count = np.unique(points_voxel_coords, axis=0, return_counts=True)\n        \n    # Note X and Y are flipped:\n    bev[coord[:,1], coord[:,0], coord[:,2]] = count\n    \n    return bev\n\ndef normalize_voxel_intensities(bev, max_intensity=16):\n    return (bev\/max_intensity).clip(0,1)","3d9faca9":"bev_shape = (336, 336, 3)\ntarget_im = np.zeros(bev_shape, dtype=np.uint8)\n\ndef move_boxes_to_car_space(boxes, ego_pose):\n    \"\"\"\n    Move boxes from world space to car space.\n    Note: mutates input boxes.\n    \"\"\"\n    translation = -np.array(ego_pose['translation'])\n    rotation = Quaternion(ego_pose['rotation']).inverse\n    \n    for box in boxes:\n        # Bring box to car space\n        box.translate(translation)\n        box.rotate(rotation)\n        \ndef scale_boxes(boxes, factor):\n    \"\"\"\n    Note: mutates input boxes\n    \"\"\"\n    for box in boxes:\n        box.wlh = box.wlh * factor\n\ndef draw_boxes(im, voxel_size, boxes, classes, z_offset=0.0):\n    for box in boxes:\n        # We only care about the bottom corners\n        corners = box.bottom_corners()\n        corners_voxel = car_to_voxel_coords(corners, im.shape, voxel_size, z_offset).transpose(1,0)\n        corners_voxel = corners_voxel[:,:2] # Drop z coord\n\n        class_color = classes.index(box.name) + 1\n        \n        if class_color == 0:\n            raise Exception(\"Unknown class: {}\".format(box.name))\n\n        cv2.drawContours(im, np.int0([corners_voxel]), 0, (class_color, class_color, class_color), -1)","8a6be970":"def visualize_lidar_of_sample(sample_token, axes_limit=80):\n    sample = level5data.get(\"sample\", sample_token)\n    sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n    level5data.render_sample_data(sample_lidar_token, axes_limit=axes_limit)\n    \n# Don't worry about it being mirrored.\nvisualize_lidar_of_sample(sample_sub.loc[0,'Id'])","21855fe6":"# Some hyperparameters we'll need to define for the system\nvoxel_size = (0.4, 0.4, 1.5)\nz_offset = -2.0\nbev_shape = (336, 336, 3)\n\n# We scale down each box so they are more separated when projected into our coarse voxel space.\nbox_scale = 0.8\n\nNUM_WORKERS = os.cpu_count() * 3\n\n# \"bev\" stands for birds eye view\n# test_data_folder = os.path.join(ARTIFACTS_FOLDER, \"bev_test_data\")\ntest_data_folder = '\/kaggle\/working\/artifacts'","0796205a":"def prepare_testing_data_for_scene(sample_token, output_folder=test_data_folder,\n                                   bev_shape=bev_shape, voxel_size=voxel_size, z_offset=z_offset,\n                                   box_scale=box_scale):\n    \"\"\"\n    Given a sample token (in a scene), output rasterized input volumes in birds-eye-view perspective.\n\n    \"\"\"\n    \n#     while sample_token:\n        \n    sample = level5data.get(\"sample\", sample_token)\n    \n\n    sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n    lidar_data = level5data.get(\"sample_data\", sample_lidar_token)\n    lidar_filepath = level5data.get_sample_data_path(sample_lidar_token)\n    \n    \n\n    ego_pose = level5data.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n    calibrated_sensor = level5data.get(\"calibrated_sensor\", lidar_data[\"calibrated_sensor_token\"])\n    \n\n\n    global_from_car = transform_matrix(ego_pose['translation'],\n                                       Quaternion(ego_pose['rotation']), inverse=False)\n    \n\n    car_from_sensor = transform_matrix(calibrated_sensor['translation'], Quaternion(calibrated_sensor['rotation']),\n                                        inverse=False)\n    \n    \n    lidar_pointcloud = LidarPointCloud.from_file(lidar_filepath)\n    \n    lidar_pointcloud.transform(car_from_sensor)\n\n    bev = create_voxel_pointcloud(lidar_pointcloud.points, bev_shape, voxel_size=voxel_size, z_offset=z_offset)\n    bev = normalize_voxel_intensities(bev)\n\n    bev_im = np.round(bev*255).astype(np.uint8)\n\n    cv2.imwrite(os.path.join(output_folder, \"{}_input.png\".format(sample_token)), bev_im)","ba3d123a":"# # to debug..\n# # doesn't work right now\n# os.makedirs(test_data_folder, exist_ok=True)\n# # NUM_WORKERS = 4\n# pool = Pool(NUM_WORKERS)\n# for _ in tqdm_notebook(pool.imap(partial(prepare_testing_data_for_scene,\n#                        output_folder=test_data_folder, bev_shape=bev_shape, voxel_size=voxel_size, z_offset=z_offset, box_scale=box_scale),sample_sub.loc[:,'Id'].values)\n#                       ,total=len(sample_sub)):\n#     pass\n\n# pool.close()\n# del pool","ae07515f":"for token in tqdm_notebook(sample_sub.loc[:,'Id'].values):\n    prepare_testing_data_for_scene(token)","73f52009":"!tar -czf lyft3d_bev_test_data.tar.gz .\/artifacts\/ ","a33c933e":"!du -h lyft3d_bev_test_data.tar.gz","b66fa9ce":"!rm -r .\/artifacts","8a1a4014":"Above is an example of what the input for our network will look like. It's a top-down projection of the world around the car (the car faces to the right in the image). The height of the lidar points are separated into three bins, which visualized like this these are the RGB channels of the image.","e903425c":"This kernel generates BEV (Bird's Eye View) images for the test data set. We will be feeding these image into NN for test set predictions. Instead of processing all the images everytime we want to make a prediction, it's better to generate it once and reuse it.\n\nYou can find Guido Zuidof's [original kernel](https:\/\/www.kaggle.com\/gzuidhof\/reference-model) here.","4b8bb9b9":"As input for our network we voxelize the LIDAR points. That means that we go from a list of coordinates of points, to a X by Y by Z space."}}