{"cell_type":{"58d1c00f":"code","f3818010":"code","2ff7f09c":"code","1d8d48e5":"code","2152a03e":"code","b7498d50":"code","7a5050f4":"code","ba81c659":"code","a63f1322":"code","c5740fba":"code","945dd47d":"code","f268eafc":"code","7e1e8041":"code","0429cd84":"code","647d76c0":"code","75c25821":"code","12609281":"code","e65dea7c":"code","502310d3":"code","69dd668f":"code","ea0d4392":"code","08ef13fa":"code","beb7a940":"code","c0412129":"code","f5bb8c26":"code","966e1028":"code","5103662b":"code","bc20cfa6":"code","341e7056":"code","2b2ae32a":"code","de315007":"code","994770ad":"code","16ea4ef9":"code","b639e100":"code","bb9a001d":"code","71500956":"code","738b6b23":"code","0e233ef1":"code","32e926db":"code","2d22ebc0":"code","59e7b579":"code","43d9b56e":"code","b92e912e":"code","ee96fadc":"code","bcccfdd1":"code","520e4707":"code","243ccf4a":"code","94c375e1":"code","1793a047":"code","b232e0f5":"code","f92a63a1":"code","4ba9600b":"code","99c6c087":"code","57348ba1":"code","e2246b5e":"code","b5a810e0":"code","b9c6f4f4":"code","ea199a21":"code","67ba85c9":"code","4213db07":"code","5ce1cbe2":"markdown","fbf2c8a7":"markdown","d33b3abf":"markdown","7b289d2b":"markdown","61e22e34":"markdown","8e2cdc61":"markdown","ba109a04":"markdown","4bcf3eed":"markdown","4ffaaaf9":"markdown","07bfb60c":"markdown","2c596da9":"markdown","7f44314d":"markdown","8ebda3ed":"markdown","6266cb6a":"markdown","2086d6e4":"markdown","9010a32a":"markdown","59836d7e":"markdown"},"source":{"58d1c00f":"!pip install  xgboost==1.4.1","f3818010":"# Data Visualization \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Data PreProcessing and Feature Engineering\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nimport re\n\n\n# Model Building \nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom  xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n# import dataset\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\nimport warnings  # For hiding warnings\nfrom termcolor import cprint # For making colorful printing texts\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nsns.set_style(\"darkgrid\")","2ff7f09c":"# import  datasets in (csv) format using pandas \n\ntrain = pd.read_csv('..\/input\/titanic\/train.csv') # reading training set (data use for training model)\n\ntest  = pd.read_csv('..\/input\/titanic\/test.csv')  # reading testing set (data use for testing model)\n\ndataset = train.append(test, ignore_index=True) # combine train and test data to preprocessing data easier","1d8d48e5":"# Showing first samples of train set\ntrain.head()","2152a03e":"# Showing first samples of test data\ntest.head()","b7498d50":"# Showing Count of null Values and Data type of Train & Test data\ntrain.info()\n\ncprint('*'*42,'green')  # print colorfull text with cprint \n\ntest.info()","7a5050f4":"# using .describe() to understand better  numeric data of train data\ntrain.describe()","ba81c659":"# using .describe() to understand better numeric data of test data\ntest.describe()","a63f1322":"cprint('Null Values in Training Data :','green')\nprint(train.isnull().sum()) # showing null values of train data\ncprint('Null Values in Test Data :','green')\nprint(test.isnull().sum()) # showing null values  of test data\n\nplt.figure(figsize=(18,18))\n\n# using .heatmap() from seaborn to visualize null values\nplt.subplot(221)\nsns.heatmap(train.isnull(), yticklabels = False, cmap='plasma')\nplt.title('Null Values Of Train Data ',size=15);\nplt.subplot(222)\nsns.heatmap(test.isnull(), yticklabels = False)\nplt.title('Null Values Of Test Data',size=15);","c5740fba":"cprint('Percent Of Survived :','green')\n# Showing  Percentage of survivors of Both gender Male\/Female\nprint(train[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False))\n\ncprint('Count Of Male\/Female :','green')\n# Showing  Count of Both gender Male\/Female\nprint(train.groupby('Sex').size())\n\nfig, (axis1,axis2) = plt.subplots(1,2,figsize=(14,6))\n\n# using .countplot() from seaborn to visualize  Count of each gender and survived percent\nsns.countplot(x = 'Survived', hue='Sex', data=train, ax =  axis1)\naxis2.set_title('Number of passenger Survived By Gender')\n\nsns.countplot(x='Sex',data=train,hue='Sex', ax = axis2)\naxis1.set_title('Number of passenger did\/didnt Survived By Gender')","945dd47d":"# using .catplot() from  seaborn to  visualize count of survived gender  in each Pclass\nsns.catplot(x = 'Sex', y = 'Survived', data = train, kind = 'bar', col = 'Pclass')","f268eafc":"cprint('Percent Of Survived :','green')\n\n# Showing  Percentage of survivors of Each Embarked\nprint(train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False))\n\ncprint('Count Of Each Embarked  :','green')\n\n# Showing  Count  of Pepole  of Each Embarked\nprint(train.groupby('Embarked').size())\n\nfig, (axis1,axis2) = plt.subplots(1,2,figsize=(14,6))\n\n# Showing  count of survived of Each Embarked\nsns.countplot('Embarked', hue = 'Survived', data = train,ax=axis1)\naxis1.set_title('Number of passenger did\/didnt Survived in each Pclass')\nsns.countplot(x='Survived',data=train,hue='Embarked', ax = axis2)\naxis2.set_title('Number of passenger Survived in each Pclass')\n","7e1e8041":"sns.catplot(x = 'Embarked', y = 'Survived', kind = 'bar', data = train, col = 'Sex')","0429cd84":"plt.figure(figsize = (14, 6))\n# showing distribute of age column with .distplot() of seaborn\nsns.distplot(train['Age'])\nplt.title('Age Distribution of passengers',)","647d76c0":"# showing mean age of Male\/Female using boxplot\nsns.catplot(x = 'Sex', y = 'Age', kind = 'box', data = train, height = 5, aspect = 2)\nplt.title('Mean Value of Age of each gender',)","75c25821":"# showing mean age of each geneder of each Ticket class (Pclass) using boxplot\nsns.catplot(x = 'Sex', y = 'Age', kind = 'box', data = train, col = 'Pclass')\n#plt.title('Mean Value of Age of each male\/femlae in  Pclasses')\nplt.suptitle('Mean Value of Age of each male\/femlae in  Pclasses')","12609281":"plt.figure(figsize = (14, 6))\n# showing mean age of each Ticket class (Pclass) using boxplot\nsns.boxplot(x='Pclass',y='Age',data=train)\nplt.title('Mean Value of Age of each  Pclass Passengers (Train Data)',)","e65dea7c":"plt.figure(figsize = (14, 6))\n# showing mean age of each Ticket class (Pclass) using boxplot for test data\nsns.boxplot(x='Pclass',y='Age',data=test)\nplt.title('Mean Value of Age of each  Pclass Passengers (Test Data)',)","502310d3":"plt.figure(figsize = (14, 6))\n# showing histogram of Paid Fare using .hist() of matplotlib \nplt.hist('Fare',data=train, bins = 60)\nplt.title('Difference in the amount of \u00a0Fare paid')","69dd668f":"cprint('Percent Of Survived :','green')\n# Showing  Percentage of survivors of Siblings or  couples \nprint(train[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False))\ncprint('Count Of Each Embarked  :','green')\n# Showing  count of   Siblings or  couples \nprint(train.groupby('SibSp').size())\nplt.figure(figsize = (14, 6))\n# using .countplot() of seaborn to  visualize  Count survived Siblings or  couples \nsns.countplot(x = 'SibSp', data = train, hue = 'Survived')\nplt.title('Number of Siblings or  couples  Survived by gender')","ea0d4392":"cprint('Percent Of Survived :','green')\n# Showing  Percentage of survivors of Parents \/children   \nprint(train[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False))\ncprint('Count Of Each Embarked  :','green')\n# Showing  count of   Parents \/children   \nprint(train.groupby('Parch').size())\n# using .countplot() of seaborn to  visualize  Count survived Siblings or  couples \nsns.catplot(x = 'Parch', y = 'Survived', data = train, hue = 'Sex', kind = 'bar', height = 6, aspect = 2)\nplt.title('Number of Siblings or  couples  Survived')","08ef13fa":"plt.figure(figsize = (10, 6))\n# using .heatmap() of seaborn to understand better relationship of variables \nsns.heatmap(train.corr(), annot=True)\nplt.title('Corelation Matrix');","beb7a940":"train.head()","c0412129":"# replace Gender with 0 and 1  in test & train data\n\ndataset.Sex[dataset.Sex == 'male'] = 0 # repalce male with 0\ndataset.Sex[dataset.Sex == 'female'] = 1 # repalce female with 1","f5bb8c26":"# Convert Embarked into dummies using pd.get_dummies()\ndummies  = pd.get_dummies(dataset.Embarked)\ndataset = pd.concat([dataset,dummies],axis='columns')\ndataset.drop(['Embarked'],axis='columns',inplace=True)","966e1028":"# replace Nan values of Cabin with U (Unknown)\ndataset.Cabin = dataset.Cabin.fillna('U')","5103662b":"# Using Regex to find letters and categories better cabin \ndataset.Cabin = dataset.Cabin.map(lambda x:re.compile(\"([a-zA-Z])\").search(x).group())","bc20cfa6":"dataset.groupby('Cabin').size()","341e7056":"# replace Cabins with numbers \ncabin_dictionary = {'A':1 , 'B':2, 'C':3 , 'D':4 , 'E':5 , 'F':6 , 'G':7 , 'T':8 , 'U':9}\n\ndataset = dataset.replace({'Cabin':cabin_dictionary})","2b2ae32a":"# combaine number of siblings and  spouses  to get  FamilySize \ndataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1","de315007":"# replace missing Fare value of test data with median of Fares \ndataset.Fare = dataset.Fare.fillna(dataset.Fare.median())","994770ad":"# drop unused columns of test and train data \ndataset = dataset.drop(['Name','SibSp', 'Parch', 'Ticket'], axis = 1)","16ea4ef9":"# split train  and test set \ntrain_main = dataset[dataset['Survived'].notna()] # train set Survived != null\ntest_main  = dataset[dataset['Survived'].isnull()] # test set Survived == null","b639e100":"# replace Missing values of age row with mean age of each Ticket class (Pclass) of train data\ndef clean_training_age(columns) :\n    Age = columns[0]\n    Pclass = columns[1]\n    \n    if pd.isnull(Age):\n        if Pclass ==1 :\n            return 37\n        elif  Pclass == 2 :\n            return 29\n        else : \n            return 24\n    else : \n        return Age","bb9a001d":"# replace Missing values of age  with mean age of each Ticket class (Pclass) of test data\ndef clean_test_age(columns) :\n    Age = columns[0]\n    Pclass = columns[1]\n    \n    if pd.isnull(Age):\n        if Pclass ==1 :\n            return 44\n        elif  Pclass == 2 :\n            return 27\n        else : \n            return 23\n    else : \n        return Age","71500956":"# fill age missing values with mean age of each Pclass according to the  boxplot\ntrain_main.Age = train_main[['Age','Pclass']].apply(clean_training_age,axis=1) \ntest_main.Age = test_main[['Age','Pclass']].apply(clean_test_age,axis=1) ","738b6b23":"# drop Survived of  test data \ntest_main.drop('Survived',axis =1,inplace=True)","0e233ef1":"train_main.head()","32e926db":"test_main.head()","2d22ebc0":"# split independent and dependent variables\nX_train = train_main.drop('Survived',axis=1) # independent\ny_train = train_main.Survived # dependent","59e7b579":"# scale our independent variables  with help  StandardScaler\nfrom sklearn.preprocessing import StandardScaler \nscaler = StandardScaler()\nX_train_scale = scaler.fit_transform(X_train)\ntest_main_scale  = scaler.transform(test_main) ","43d9b56e":"lrc = LogisticRegression()\nlrc.fit(X_train_scale,y_train)","b92e912e":"y_pred_lrc = lrc.predict(test_main_scale).astype(int)","ee96fadc":"svm = SVC()\nsvm.fit(X_train_scale,y_train)","bcccfdd1":"y_pred_svm = svm.predict(test_main_scale).astype(int)","520e4707":"knn = KNeighborsClassifier()\nknn.fit(X_train_scale,y_train)","243ccf4a":"y_pred_knn = knn.predict(test_main_scale).astype(int)","94c375e1":"nb = GaussianNB()\nnb.fit(X_train,y_train)","1793a047":"y_pred_nb = nb.predict(test_main).astype(int)","b232e0f5":"dt = DecisionTreeClassifier()\ndt.fit(X_train,y_train)","f92a63a1":"y_pred_dt = dt.predict(test_main).astype(int)","4ba9600b":"rf = RandomForestClassifier()\nrf.fit(X_train,y_train)","99c6c087":"y_pred_rf = rf.predict(test_main).astype(int)","57348ba1":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\nmodel = Sequential()\n\nmodel.add(Dense(15, activation = 'tanh', input_dim = 10))\n\nmodel.add(Dense(1, activation = 'sigmoid'))\n\noptimizer = SGD(lr = 0.01, momentum = 0.9)\nmodel.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n\nmodel.fit(X_train_scale, y_train, batch_size = 10, epochs = 50)","e2246b5e":"yp =  model.predict(test_main_scale)\ny_pred_ann = []\nfor element in yp:\n    if element > 0.5:\n        y_pred_ann.append(1)\n    else:\n        y_pred_ann.append(0)","b5a810e0":"gbc = GradientBoostingClassifier()\ngbc.fit(X_train,y_train)","b9c6f4f4":"y_pred_gbc = gbc.predict(test_main).astype(int)","ea199a21":"xgb = XGBClassifier(colsample_bylevel= 0.9,\n                    colsample_bytree = 0.8, \n                    gamma=0.99,\n                    max_depth= 5,\n                    min_child_weight= 1,\n                    n_estimators= 8,\n                    nthread= 5,\n                    random_state= 0,\n                    )\nxgb.fit(X_train_scale,y_train)","67ba85c9":"y_pred_xgb = xgb.predict(test_main_scale).astype(int)","4213db07":"final_data_1 = {'PassengerId': test_main.PassengerId, 'Survived': y_pred_xgb}\nsubmission_1 = pd.DataFrame(data=final_data_1)\nsubmission_1.to_csv('submission_xgb.csv',index =False)","5ce1cbe2":"## [GradientBoostingClassifier](http:\/\/scikitlearn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html)","fbf2c8a7":"## [Naive Bayes](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.GaussianNB.html)","d33b3abf":"# Libraries used in the project\n\n- [seaborn](https:\/\/seaborn.pydata.org\/)\n- [matplotlib](https:\/\/matplotlib.org\/)\n- [numpy](https:\/\/numpy.org\/)\n- [pandas](https:\/\/pandas.pydata.org\/)\n- [termcolor](https:\/\/pypi.org\/project\/termcolor\/)\n- [sklearn](https:\/\/scikit-learn.org\/stable\/)\n- [xgboost](https:\/\/xgboost.readthedocs.io\/en\/latest\/index.html)\n- re\n- warnings","7b289d2b":"## Cleaning and  Data Preprocessing and Feature Engineering Data \n\n This is the most important part of the project in my opinion, your data  can be compared to a dirty and old house that you have to clean and change its decoration and add new rooms to it , The more beautiful and new and bigger  your house is ,  higher (more accurate)  it will be sold :D\n#### 1) OneHot Encoding Gender \n\n#### 2) Convert Embarked into dummies\n\n#### 3) Fill Missing Values of Cabin and categorise it better\n\n#### 4) Convert Cabins into numbers \n\n#### 5) Create a new feature as FamilySize\n\n#### 6) Drop unused columns\n\n#### 7) Fill Missing Values of Fare\n\n#### 8) Fill Missing Values of Age","61e22e34":"## [XGBClassifier](https:\/\/xgboost.readthedocs.io\/en\/latest\/index.html)","8e2cdc61":"## [Support vector machine](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html)","ba109a04":"## [K nearest neighbors](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html)","4bcf3eed":"## [Decision Tree](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html)","4ffaaaf9":"## Output\n### save output as csv","07bfb60c":"# Data Visualization \nthis part realy helped me to understand data better especially for (feature engineering and data cleaning) for example what's the percent of survived and mean age of each Pclass and ... , with help of seaborn and matplotlib to visualize the data","2c596da9":"## [LogisticRegression](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html)","7f44314d":"# Understanding The Data \nIn this part , I tried to get a background of the data that im gonna use like what is the data types , how many missing values our data have ,  and describe the numeric variables of the data wuth help of (Pandas,Numpy,Python)","8ebda3ed":"# Building and Evaluating Model\nNow it's time to sell our fresh house and our house  buyers or customers are Models  (ML algorithms) whoever offers a higher price (more accuracy) can buy the house, note that house customers based on the house (data) and house features (data features) and house usage purpose (classification or regression)","6266cb6a":"## Scaling Data\nTip : usually we dont StandardScale data with dummies and OneHot encoded varibels include but  in this case it doesn't matter use the standardscaler  because the scaling is independently for each colum","2086d6e4":"## [Artificial Neural Network](https:\/\/keras.io\/)","9010a32a":"# Titanic Project Classification\nIn this notebook, I tried to understand the relationship between variables and classify who survived and  who did not . with help of  (1 - data visualization  2 - feature Engineering  3 - model building )\n\n\n## Overview \n### 1) Understand the data (Shape , missing values , data types , ...)\n\n### 2) Data Visualization  (Histograms,box plots)\n\n### 3) Data Preprocessing and Feature Engineering\n\n### 4) Model Building ","59836d7e":"## [RandomForest](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html)"}}