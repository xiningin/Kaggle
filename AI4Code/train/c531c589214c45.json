{"cell_type":{"4c462db5":"code","c70410e4":"code","a64b3eeb":"code","7f9c36d9":"code","ef4e14e7":"code","a2d7de09":"code","363cf947":"code","a86407e2":"code","0ed6612e":"code","b0a92447":"code","0513c79a":"code","3b613db7":"code","a0ec602c":"code","93d3cd9e":"code","7b74a880":"code","5670d0fd":"code","a68023b0":"code","77b52334":"code","82a5a813":"code","11d5b11f":"code","be24248b":"code","d7ac3e9f":"code","8bf2bdc3":"code","ea15c471":"code","512c1b00":"code","cc0dd29b":"markdown","cd4f3797":"markdown","8d182bfc":"markdown","0da2b7c6":"markdown","e7b133ec":"markdown","8067e207":"markdown","1a504c27":"markdown","6a66f5c1":"markdown","9ed4ce50":"markdown","d2c638de":"markdown","2673a1b0":"markdown","bf442734":"markdown","8921f1f4":"markdown","ea2dd403":"markdown"},"source":{"4c462db5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.cluster import KMeans as kmeans\nimport tensorflow as tf\nfrom keras.layers import Dense,Dropout,GlobalAveragePooling2D,BatchNormalization\nfrom  keras import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator,img_to_array,load_img\nfrom scipy.cluster.vq import whiten\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport cv2\n%matplotlib inline\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c70410e4":"train_dir = '..\/input\/intel-image-classification\/seg_train\/seg_train\/'\ntest_dir = '..\/input\/intel-image-classification\/seg_test\/seg_test\/'\npred_dir = '..\/input\/intel-image-classification\/seg_pred\/seg_pred\/'","a64b3eeb":"os.listdir(train_dir)","7f9c36d9":"album = pd.DataFrame(columns = [\"id\",\"label\",\"scene\"])\ndirs = ['mountain', 'street', 'buildings', 'sea', 'forest', 'glacier']\nfor direc in dirs:\n    for file in os.listdir(train_dir + direc):\n        if direc in ['sea', 'forest', 'glacier','mountain']:\n            scene = \"nature\"\n        else:\n            scene = \"manmade\"\n        album = album.append({\"id\":file,\"label\":direc,\"scene\":scene},ignore_index = True)\nalbum.head()","ef4e14e7":"plt.figure(figsize=(25,6))\nplt.subplot(1,2,1)\nsns.countplot(x=\"label\", data=album)\nplt.subplot(1,2,2)\nsns.countplot(x=\"scene\", data=album)\nplt.show()","a2d7de09":"test_album = pd.DataFrame(columns = [\"id\",\"label\"])\ndirs = ['mountain', 'street', 'buildings', 'sea', 'forest', 'glacier']\nfor direc in dirs:\n    for file in os.listdir(test_dir + direc):\n        if direc in ['sea', 'forest', 'glacier','mountain']:\n            scene = \"nature\"\n        else:\n            scene = \"manmade\"\n        test_album = test_album.append({\"id\":file,\"label\":direc,\"scene\":scene},ignore_index = True)\ntest_album.head()","363cf947":"plt.figure(figsize=(25,6))\nplt.subplot(1,2,1)\nsns.countplot(x=\"label\", data=test_album)\nplt.subplot(1,2,2)\nsns.countplot(x=\"scene\", data=test_album)\nplt.show()","a86407e2":"plt.figure(figsize = (20,16))\nn = 0\nfor label in dirs:\n    n += 1\n    img = (album[\"id\"].where(album[\"label\"] == label).dropna()).iloc[0]\n    plt.subplot(3,2,n)\n    plt.subplots_adjust(hspace = 0.4,wspace = 0.2)\n    path = train_dir + label + '\/' + img\n    plt.imshow(Image.open(path))\n    plt.xlabel((img_to_array(Image.open(path))).shape)\n    plt.title(label)\nplt.show()","0ed6612e":"def extract(img):\n    r,g,b = [],[],[]\n    for row in img[:2]:\n        for tmp_r,tmp_g,tmp_b in row:\n            r.append(tmp_r)\n            g.append(tmp_g)\n            b.append(tmp_b)\n    sred = whiten(r)\n    sblue = whiten(b)\n    sgreen = whiten(g)\n    \n    img_df = pd.DataFrame({'red':r,'blue':b,'green':g,'sred':sred,'sblue':sblue,'sgreen':sgreen})\n    clf = kmeans(n_clusters=3,random_state=0).fit(img_df[[\"sred\",\"sgreen\",\"sblue\"]])\n    centers = clf.cluster_centers_\n    # To display the dominant colors, convert the colors of the cluster centers to their raw values and then \n    # converted them to the range of 0-1, using the following formula: converted_pixel = standardized_pixel * pixel_std \/ 255\n    colors = []\n    r_std,g_std,b_std = img_df[[\"red\",\"green\",'blue']].std()\n    for center in centers:\n        sr,sg,sb = center\n        colors.append(\n            (\n                sr * r_std\/255,\n                sg * g_std\/255,\n                sb * b_std\/255\n            )\n        )\n    plt.imshow([colors])\n    plt.show()\n    plt.clf()","b0a92447":"img = plt.imread(\"..\/input\/intel-image-classification\/seg_pred\/seg_pred\/10004.jpg\")\nextract(img)\nplt.imshow(img)\nplt.show()","0513c79a":"def generate(train_dir,test_dir):\n    train_datagen = ImageDataGenerator(rescale=1.\/255,horizontal_flip=True,validation_split = 0.2)\n    test_datagen = ImageDataGenerator(rescale=1.\/255)\n    \n    train_gen = train_datagen.flow_from_directory(\n      directory = train_dir,\n      subset=\"training\",\n      target_size = (150,150),\n      shuffle = True,\n      class_mode = 'categorical',\n      batch_size=500)\n    \n    val_gen = train_datagen.flow_from_directory(\n      directory = train_dir,\n      subset=\"validation\",\n      shuffle = True,\n      class_mode = 'categorical',\n      target_size=(150,150),\n      batch_size=500)\n    \n    test_gen = test_datagen.flow_from_directory(\n      directory = test_dir,\n      shuffle = True,\n      class_mode = 'categorical',\n      target_size = (150,150),\n      batch_size = 500)\n    \n    return train_gen,val_gen,test_gen","3b613db7":"train_gen,valid_gen,test_gen = generate(train_dir,test_dir)","a0ec602c":"from keras.applications import DenseNet121\ndef get_model(arch):\n    base_model = arch(include_top = False,\n                        input_shape = (150,150,3),\n                        weights = \"imagenet\")\n    base_model.trainable = False\n    print(\"Length of Model: \",len(list((base_model.layers))))\n    return base_model","93d3cd9e":"def plot_history(history):\n    plt.figure(figsize=(25,10))\n    # summarize history for accuracy\n    plt.subplot(1,2,1)\n    plt.plot(history.history['categorical_accuracy'])\n    plt.plot(history.history['val_categorical_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('categorical_accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    # summarize history for loss\n    plt.subplot(1,2,2)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('categorical_accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()","7b74a880":"def hypertune(train_gen,valid_gen,test_gen,arch = DenseNet121,lr = 0.005,m =128,n =512):    \n    base_model = get_model(arch)\n    gal = GlobalAveragePooling2D()(base_model.output)\n    d1 = Dense(units = m,activation = 'relu')(gal)\n    drop1 = Dropout(0.2)(d1)\n    d2 = Dense(units = n,activation = 'relu')(drop1)\n    drop2 = Dropout(0.4)(d2)\n    bn = BatchNormalization()(drop2)\n    output = Dense(units = 6,activation = \"softmax\")(bn)\n    model = tf.keras.Model(inputs = base_model.input,outputs = output)\n    model.compile(optimizer = tf.keras.optimizers.Adam(lr = lr),loss = \"categorical_crossentropy\",metrics = [\"categorical_accuracy\"])\n    history = model.fit(train_gen,validation_data = valid_gen,batch_size = 500,steps_per_epoch = 11230 \/\/ 500,epochs = 5)\n    plot_history(history)\n    loss,acc = model.evaluate(test_gen)\n    print(f\"Model Loss: {loss}, Model Accuracy: {acc}\")\n    return model","5670d0fd":"model = hypertune(train_gen,valid_gen,test_gen)","a68023b0":"classes = list(train_gen.class_indices)\nclasses","77b52334":"x = plt.imread('..\/input\/intel-image-classification\/seg_train\/seg_train\/street\/1000.jpg')\nplt.imshow(x)\nx = x\/255\nx = np.expand_dims(x,axis = 0)\nx.shape\nprint(classes[np.argmax(model.predict(x))])","82a5a813":"pred_album = pd.DataFrame(columns = [\"id\",\"label\"])","11d5b11f":"for file in os.listdir(pred_dir):\n    path = pred_dir + file\n    x = img_to_array(Image.open(path))\n    x = x\/255\n    x = np.expand_dims(x,axis = 0)\n    pred = classes[np.argmax(model.predict(x))]\n    pred_album = pred_album.append({\"id\":file,\"label\":pred},ignore_index = True)","be24248b":"showcase = pred_album.iloc[:16,:]\nplt.figure(figsize = (20,16))\nn = 0\nfor image,label in zip(list(showcase[\"id\"]),list(showcase[\"label\"])):\n    n += 1\n    plt.subplot(4,4,n)\n    plt.subplots_adjust(hspace= 0.4,wspace = 0.4)\n    path = pred_dir + image\n    plt.imshow(Image.open(path))\n    plt.title('{}'.format(label))","d7ac3e9f":"pred_album.to_csv('answers.csv')","8bf2bdc3":"def plot_layer(lname,y = None):\n    layer = model.get_layer(lname)\n    extractor = tf.keras.Model(inputs = model.input,outputs = layer.output)\n    if y is None:\n        y = (layer.output).shape[3]\n    print(f\"What happened in the {len(list(extractor.layers)) - 1}th layer? \\n\")\n    feature_maps = extractor.predict(sample_img)\n    plt.figure(figsize = (20,16))\n    n = 0\n    for i in range(y):\n        n += 1\n        plt.subplot(y\/8,8,n)\n        plt.subplots_adjust()\n        # plot filter channel in grayscale\n        plt.imshow(feature_maps[0, :, :, n-1], cmap='gray')\n        # show the figure\n    plt.show()","ea15c471":"path = \"..\/input\/intel-image-classification\/seg_train\/seg_train\/buildings\/10006.jpg\"\nsample_img = load_img(path, target_size=(150,150,3))\nplt.imshow(sample_img)\nplt.show()\nsample_img = np.expand_dims(sample_img, axis=0)\nsample_img = tf.keras.applications.densenet.preprocess_input(sample_img)\nplot_layer(\"conv1\/relu\")\nplot_layer(\"conv2_block1_0_relu\")\nplot_layer(\"conv2_block1_1_relu\")","512c1b00":"plot_layer(\"relu\",16)","cc0dd29b":"**Checking Distribution of Test Album**","cd4f3797":"# Model","8d182bfc":"# About the Data","0da2b7c6":"The role of the ConvNet is to reduce the images into a form which is easier to process, without losing features which are critical for getting a good prediction.[[1]](https:\/\/towardsdatascience.com\/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2)\n>  The Network has reduced the size of the features from 150 to 75 to 38, while making sure to retain as many features as possible\n\n> It will get smaller and smaller until the image is no longer recognisable to the human eye\n\nExample: ","e7b133ec":"**Make Test DataFrame**","8067e207":"**Context**\n\nThis is image data of Natural Scenes around the world.\n\n**Content**\n\nThis Data contains around 25k images of size 150x150 distributed under 6 categories.\n{'buildings' -> 0,\n'forest' -> 1,\n'glacier' -> 2,\n'mountain' -> 3,\n'sea' -> 4,\n'street' -> 5 }\n\nThe Train, Test and Prediction data is separated in each zip files. There are around 14k images in Train, 3k in Test and 7k in Prediction.\nThis data was initially published on https:\/\/datahack.analyticsvidhya.com by Intel to host a Image classification Challenge.\n\n**Acknowledgements**\n\nThanks to https:\/\/datahack.analyticsvidhya.com for the challenge and Intel for the Data\n\nPhoto by Jan B\u00f6ttinger on Unsplash\n\n**Inspiration**\n\nWant to build powerful Neural network that can classify these images with more accuracy.\n\nVisualise how conv networks work","1a504c27":"**Make Train DataFrame**","6a66f5c1":"All images are of size (150,150,3)","9ed4ce50":"**Seeing Sample Images from each class**","d2c638de":"# Thank You! Hope you liked my notebook, if you did, please do leave an upvote.","2673a1b0":"> Its funny how neural networks prefer this to full images, but the fact is, the neural network trains to reduce the image to this size, by selecting the best filters to apply on them.\n\n> In the above layer, which is the 8th layer from the end of the model, there were a total of 1024 features.\n\n> The neural network remembers  these filters and not the features. This is a good explanantion I found online.\n\n    A filter is a matrix with the same dimension as our sliding window, e.g. 3x3. At each position of our sliding window, a mathematical operation is performed, the so called convolution. During convolution, each pixel value in our window is multiplied with the value at the respective position in the filter matrix and the sum of all multiplications is calculated. This result is called the dot product. Depending on what values the filter contains at which position, the original image will be transformed in a certain way, e.g. sharpen, blur or make edges stand out. You can find great visualizations on setosa.io.\n\n    To be precise, filters are collections of kernels so that, if we work with color images, we have 3 channels. The 3 dimensions from the channels will all get one kernel, which together create the filter. Each filter will only calculate one output value, the dot product mentioned earlier. The learning part of CNNs comes into play with these filters. Similar to learning weights in a MLP, CNNs will learn the most optimal filters for recognizing specific objects and patterns. But a CNN doesn\u2019t only learn one filter, it learns multiple filters. In fact, it even learns multiple filters in each layer! Every filter learns a specific pattern, or feature. That\u2019s why these collections of parallel filters are the so called stacks of feature maps or activation maps. We can visualize these activation maps to help us understand what the CNN learn along the way, but this is a topic for another lesson. \n   [[2]](https:\/\/www.shirin-glander.de\/2019\/01\/how_cnns_learn\/) ","bf442734":"# Data Preprocessing","8921f1f4":"# Color palette extraction using K-Means [Attribute](https:\/\/www.kaggle.com\/ravichaubey1506\/extracting-dominant-color-of-an-image)","ea2dd403":"**Checking Distribution of Training Album**"}}