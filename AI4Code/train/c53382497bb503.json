{"cell_type":{"84fc536f":"code","3e917d11":"code","3b95fb29":"code","f80c4eb1":"code","037b216f":"code","071d9749":"code","e8519707":"code","ca8c6b2e":"code","28df7dee":"code","fd7b2fb9":"code","7ed19eb2":"code","b8cee6b9":"code","019591f1":"code","42f52b8f":"code","c02609b7":"code","7210d001":"code","06534958":"code","bede1214":"code","ba6234d7":"code","c519b30d":"code","2d4ebf53":"code","290050a2":"code","2a791f4c":"code","2fd1f001":"code","4f4356a6":"code","09653f85":"code","2f767071":"code","81c76739":"code","87e0452f":"code","3c5747c9":"code","1a427da6":"code","591424bd":"code","3bb0676c":"code","c63913d6":"code","1f643513":"code","64c9ba6c":"code","e1127dcc":"code","952c15a9":"code","ea894386":"markdown","bd29b8cf":"markdown","99b578b2":"markdown","67d1005d":"markdown","4dc6150a":"markdown","329f5edf":"markdown","53630c9c":"markdown","9674e24c":"markdown","320e2f78":"markdown","59107dd4":"markdown","c9fbec03":"markdown","eaedd32a":"markdown","19a49d48":"markdown","1afed181":"markdown","370d11ef":"markdown","53501e8d":"markdown","0024559e":"markdown","efc2eb51":"markdown","dfd0ca2b":"markdown","342969b4":"markdown","d9b09ac8":"markdown","3268d7cc":"markdown","e4e24e4a":"markdown","8c37c035":"markdown","8f2f64d4":"markdown","6ff97c29":"markdown","6b85a569":"markdown","508d999f":"markdown","4fe6bcef":"markdown","4b4ed146":"markdown","d5c899fe":"markdown"},"source":{"84fc536f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3e917d11":"train_data = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","3b95fb29":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport spacy\nimport re","f80c4eb1":"missing_values_count = train_data.isnull().sum()\nmissing_values_count","037b216f":"total_cells = np.product(train_data.shape)\ntotal_missing = missing_values_count.sum()\n\n# percent of data that is missing\npercent_missing = (total_missing\/total_cells) * 100\nprint(percent_missing)","071d9749":"train_data['text'].str.len().hist()","e8519707":"train_data['text'].str.split().map(lambda x: len(x)).hist()","ca8c6b2e":"train_data['text'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x)).hist()","28df7dee":"len(train_data['text'].unique())","fd7b2fb9":"\n# Dictionary of English Contractions\ncontractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n                     \"you've\": \"you have\"}\n\n# Regular expression for finding contractions\ncontractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n\n# Function for expanding contractions\ndef expand_contractions(text,contractions_dict=contractions_dict):\n  def replace(match):\n    return contractions_dict[match.group(0)]\n  return contractions_re.sub(replace, text)\n\n# Expanding Contractions in the reviews\ntrain_data.text=train_data.text.apply(lambda x:expand_contractions(x))","7ed19eb2":"train_data.text=train_data.text.apply(lambda x: x.lower())","b8cee6b9":"nlp = spacy.load(\"en_core_web_sm\",exclude=[\"ner\"])\n\ntrain_data['lemmatized']=train_data.text.apply(\n    lambda x: ' '.join(\n        [\n            token.lemma_ for token in list(nlp(x)) if (\n                token.is_stop==False and \n                token.is_punct==False and\n                (\n                    (token.is_alpha and token.is_ascii) or\n                    token.is_space\n                )\n            )\n        ]\n    )\n)","019591f1":"count = 0\nneg_count = 0\nfor row,val in train_data.iterrows():    \n    if(\"http\" in val.lemmatized and val.target == 1):\n        count+=1\n    elif(\"http\" in val.lemmatized and val.target == 0):\n        neg_count+=1\nprint(count)\nprint(neg_count)","42f52b8f":"train_data['cleaned']=train_data['lemmatized'].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\ntrain_data['cleaned']=train_data['cleaned'].apply(lambda x:re.sub(r\"@\\S+\", \"\", x))\ntrain_data['cleaned']=train_data['cleaned'].apply(lambda x:\" \".join(x.split()))\n","c02609b7":"c = 0\nd = 0\nfor rows,val in train_data.iterrows():\n    if(pd.isnull(val.keyword) and val.target == 1):\n        c+=1\n    elif(pd.isnull(val.keyword) and val.target == 0):\n        d+=1\nprint(c)\nprint(d)","7210d001":"c = 0\nd = 0\nfor rows,val in train_data.iterrows():\n    if(pd.notnull(val.location) and val.target == 1):\n        c+=1\n    elif(pd.notnull(val.location) and val.target == 0):\n        d+=1\nprint(c)\nprint(d)","06534958":"train_data['cleaned'].str.split().map(lambda x: len(x)).hist()","bede1214":"max_words = train_data['cleaned'].str.split().map(lambda x: len(x))\nmax_words","ba6234d7":"train_data['cleaned'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x)).hist()","c519b30d":"dictionary = set()\nfor val in train_data['cleaned'].str.split():\n    dictionary = dictionary.union(set(val))\n    \nlen(dictionary)","2d4ebf53":"from collections import Counter\n\n## Build a dictionary that maps words to integers\ncounts = Counter(dictionary)\nvocab = sorted(counts, key=counts.get, reverse=True)\nvocab_to_int = {dictionary: ii for ii, dictionary in enumerate(vocab,1)} ","290050a2":"train_data['sequence'] = train_data['cleaned'].apply(lambda x : [vocab_to_int[word] for word in x.split()])","2a791f4c":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_data['sequence'], train_data['target'], test_size = 0.3, random_state = 1)","2fd1f001":"y_train = np.expand_dims(y_train, axis=1)\ny_test = np.expand_dims(y_test, axis=1)\n\nprint(X_train.shape)\nprint(X_test.shape)","4f4356a6":"from tensorflow.keras.preprocessing.sequence import pad_sequences\npadded_inputs = pad_sequences(X_train, maxlen=max_words)\npadded_inputs_test = pad_sequences(X_test, maxlen=max_words)","09653f85":"print(padded_inputs.shape)\nprint(padded_inputs_test.shape)","2f767071":"import tensorflow as tf\nfrom tensorflow.compat.v1 import ConfigProto\nfrom tensorflow.compat.v1 import InteractiveSession\n\nconfig = ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = InteractiveSession(config=config)","81c76739":"# Model paramateres\nnum_distinct_words = len(dictionary)\nno_of_features = 256\nmax_sequence_length = max_words","87e0452f":"from tensorflow.keras.models import Sequential\nmodel = Sequential()","3c5747c9":"from tensorflow.keras.layers import Embedding\nmodel.add(\n    Embedding(num_distinct_words,no_of_features,input_length=max_words)\n)","1a427da6":"from tensorflow.keras.layers import LSTM\nmodel.add(\n    LSTM((4))\n)","591424bd":"from tensorflow.keras.layers import Flatten\nmodel.add(\n    Flatten()\n)","3bb0676c":"from tensorflow.keras.layers import Dense\nmodel.add(\n    Dense(1,activation='sigmoid')\n)","c63913d6":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import BinaryCrossentropy\nmodel.compile(\n    optimizer = Adam(),\n    loss = BinaryCrossentropy(),\n    metrics = ['accuracy']    \n)","1f643513":"model.summary()","64c9ba6c":"print(padded_inputs.shape)\nprint(y_train.shape)\nx = np.random.random((1000, 32))\ny = np.random.random((1000, 1))\nhistory = model.fit(\n    x,\n    y,\n    batch_size = 32,\n    epochs = 30\n)","e1127dcc":"test_results = model.evaluate(\n    padded_inputs_test,\n    y_test,\n    verbose = 1\n)","952c15a9":"print(f'Test results - Loss: {test_results[0]} - Accuracy: {100*test_results[1]}%')","ea894386":"### We will now add an LSTM to learn the context, we will use 10 layers","bd29b8cf":"### Now we compile the model using:\n#### Adam Optimizer\n#### Binary Cross Entropy Loss function\n#### Accuracy Metrics","99b578b2":"### Once the model is trained, we want to check the predictions with out our test data","67d1005d":"### Analyse if there are any links in the text, check if they have any information gain","4dc6150a":"### Generate the word embeddings dictionary: vocab_to_int","329f5edf":"For the data, perform padding to ensure the vectors are of same length","53630c9c":"### Analyse the other columns (features): keyword and location","9674e24c":"### On your passed input data (train) and labels (train), we now train (fit) the model to learn weights","320e2f78":"### Now we have all the word embeddings which we will put on a Tensorflow + Keras with LSTM and Dense Sequential model","59107dd4":"### No of characters in each record","c9fbec03":"### Expand Contractions\n","eaedd32a":"### Before passing it to the dense layer, we want to flatten the input, so add a Flatten layer","19a49d48":"### They dont have much impact, \n#### \"location\" has ~2500 missing records, so we discard the column\n#### \"keyword\" has only 61 missing records, we could do away with these records and use this feature.","1afed181":"### No of Words in each record","370d11ef":"## How much Data is missing?","53501e8d":"# Perform Data cleaning","0024559e":"# Get insights about the data (text corpus)","efc2eb51":"### We will perform word embeddings on the data using the Embedding layer","dfd0ca2b":"### Remove stop words and non ascii characters words \n### Also lemmatize the words using spacy","342969b4":"### Convert input data into word embeddings","d9b09ac8":"### We will add a Dense Layer with 1 layer and a sigmoid activation to get us a probabilistic answer for disaster classification","3268d7cc":"### Filter out generate the complete dictionary","e4e24e4a":"### We will build a sequential model using Keras and tensorflow","8c37c035":"### Remove all links and extra spaces","8f2f64d4":"### No of Unique Words in the entire corpus","6ff97c29":"### Average word length in each record","6b85a569":"### Summarize the model","508d999f":"### Make the data  lowercase","4fe6bcef":"### Figure out the confusion matrix\/ accuracy\/ recall\/ precision etc","4b4ed146":"### Define Model Parameters","d5c899fe":"### Split the data into training and test data"}}