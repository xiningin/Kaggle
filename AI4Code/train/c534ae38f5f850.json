{"cell_type":{"615cec9a":"code","75426565":"code","c3fb9444":"code","c06ec5de":"code","4d0c03a2":"code","a168ef76":"code","1ec7503d":"code","4c655135":"code","a38bd4b5":"code","56de71d4":"code","e84c3a44":"code","8b7b076b":"code","c40e2ed8":"code","581dcf1d":"code","66c56106":"code","af484c1e":"code","9461dc03":"code","b68db087":"code","8f22f7ea":"code","9d6e1bdd":"code","74dc59a6":"code","293cc84f":"code","fb16d5fd":"code","2f6f56ff":"code","b3a00cc0":"code","86fc21af":"code","842847ce":"code","74016bc7":"code","5770ce1c":"code","c6c34234":"code","038bd6c3":"code","0d9bd6ba":"code","f1fdc9e0":"code","45df4392":"code","1a6e71c8":"code","8da46ecf":"code","062c1ff0":"code","0aff1472":"code","39fa50c0":"code","eb0cff2c":"code","a0b5b048":"code","8a80e50f":"code","bfb1b984":"code","11d597ae":"code","d7c529c2":"code","36a4860b":"code","016018a4":"code","c6370195":"code","22afddc2":"code","98a2293a":"code","a29d0b55":"code","4ad16a80":"code","83640b30":"code","05a4ed88":"code","62ea5485":"code","b313b3d1":"code","e6aa4313":"code","ba6920a8":"code","f16cead5":"code","77bd317f":"code","3dc9d709":"code","5aea9ad6":"code","fc38a65c":"code","7169dd13":"code","152ce494":"code","4e170280":"code","4cf7f01c":"code","818bf758":"code","e4c1da33":"code","3d01e1e0":"code","5bb3f57c":"code","e3ff0688":"markdown","9dd79494":"markdown","cf758882":"markdown","44f7d652":"markdown","1e21b954":"markdown","a7a1e8a6":"markdown","82c86a37":"markdown"},"source":{"615cec9a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","75426565":"# importing visualization libraries\n\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt","c3fb9444":"# Loading the training data\n\ntrain_df = pd.read_csv(\"\/kaggle\/input\/Kannada-MNIST\/train.csv\")","c06ec5de":"# Loading the testing data\n\ntest_df = pd.read_csv(\"\/kaggle\/input\/Kannada-MNIST\/test.csv\")","4d0c03a2":"train_df.head()","a168ef76":"test_df.head()","1ec7503d":"# dimensions of training data\n\ntrain_df.shape","4c655135":"# dimensions of testing data\n\ntest_df.shape","a38bd4b5":"# basic meta data about training data \n\ntrain_df.info()","56de71d4":"# basic meta data about test data\n\ntest_df.info()","e84c3a44":"# distribution of the target label\n\ntrain_df['label'].value_counts()","8b7b076b":"# visualizing the target variable distribution from the training dataset\n\ntrain_df['label'].value_counts().plot(kind='bar')","c40e2ed8":"# importing keras libraries\nimport keras\nimport sklearn\n\nfrom keras.preprocessing import image\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Flatten, BatchNormalization, Activation, Dropout, LeakyReLU, Input, Conv2D, MaxPooling2D\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras import optimizers\nfrom keras.layers.merge import concatenate\nfrom keras import utils\nfrom keras.utils import plot_model\n\n# Splitting the data into train and test\nfrom sklearn.model_selection import train_test_split","581dcf1d":"# number of images per batch\nBATCH_SIZE = 64\n\n# number of epochs or no.f times the data has to be projected towards model\nNP_EPOCHS = 50\n\n# number of output classes\nNP_CLASSES  = 10\n\n# split the data into train and validation use 20% of data for validation\nVALIDATION_SPLIT = 0.20\n\n# optimizer has been passed to the model\nOPTIMIZER = optimizers.RMSprop()","66c56106":"# dividing the training data into train and target\n\n# training data\ntrain = train_df.drop('label', axis=1)\n\n# target data\ntarget = train_df['label']","af484c1e":"# test data\n\ntest = test_df.drop('id', axis=1)","9461dc03":"train.head()","b68db087":"test.head()","8f22f7ea":"# sample target values\n\ntarget.head()","9d6e1bdd":"# Normalizing the train data\n\ntrain \/= 255","74dc59a6":"train.head()","293cc84f":"# Normalizing the test data\n\ntest \/= 255","fb16d5fd":"test.head()","2f6f56ff":"# reshaping the training data\n\ntrain = train.values.reshape(-1, 28, 28, 1)","b3a00cc0":"# taking a sample image from traing data\n\nplt.imshow(train[42][:, :, 0])","86fc21af":"# reshaping the test data\n\ntest = test.values.reshape(-1, 28, 28, 1)","842847ce":"# taking a sample image from test data\n\nplt.imshow(test[42][:, :, 0])","74016bc7":"# converting the target value into categorical values\n\ntarget = utils.to_categorical(target, NP_CLASSES)","5770ce1c":"# sample target encoded values\n\ntarget[:2]","c6c34234":"# Output activation function\nNP_OUTPUT_FUNC = \"softmax\"\n\n# Neuron activation fuction\nNP_DESCRIMINATOR_FUNC = \"sigmoid\"\n\n# 15% Drouput or Dropping the signal or connection between the layers\nNP_DROPOUT_FIRST = 0.15\n\n# 10% Dropout or Dropping the signal or connetion between the layers\nNP_DROPOUT_SECOND = 0.10","038bd6c3":"# Input shape\ninput_shape = (28, 28, 1)\n\n# Kernel Size\nNP_KERNEL_SIZE = 3","0d9bd6ba":"# Code to create Left branch of the network\nleft_input = Input(shape=input_shape)\nx = left_input\nNP_FILTERS = 32\n\nfor i in range(3):\n    x = Conv2D(filters=NP_FILTERS, kernel_size=NP_KERNEL_SIZE, padding=\"same\")(x)\n    x = LeakyReLU(alpha=0.15)(x)\n    x = BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001, center=True,\n                           scale=True, beta_initializer=\"zeros\", gamma_initializer=\"ones\",\n                           moving_mean_initializer=\"zeros\", moving_variance_initializer=\"one\",\n                           beta_regularizer=None, gamma_regularizer=None,\n                           beta_constraint=None, gamma_constraint=None)(x)\n    x = Conv2D(filters=NP_FILTERS, kernel_size=NP_KERNEL_SIZE)(x)\n    x = LeakyReLU(alpha=0.15)(x)\n    x = BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001, center=True,\n                           scale=True, beta_initializer=\"zeros\", gamma_initializer=\"ones\",\n                           moving_mean_initializer=\"zeros\", moving_variance_initializer=\"one\",\n                           beta_regularizer=None, gamma_regularizer=None,\n                           beta_constraint=None, gamma_constraint=None)(x)\n    x = Dropout(NP_DROPOUT_FIRST)(x)\n    x = MaxPooling2D()(x)\n    NP_FILTERS *= 2","f1fdc9e0":"# Code to create Right branch of the network\n\nright_input = Input(shape=input_shape)\ny = right_input\nNP_FILTERS = 32\n\nfor i in range(3):\n    y = Conv2D(filters=NP_FILTERS, kernel_size=NP_KERNEL_SIZE, padding=\"same\")(y)\n    y = LeakyReLU(alpha=0.15)(y)\n    y = BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001, center=True,\n                           scale=True, beta_initializer=\"zeros\", gamma_initializer=\"ones\",\n                           moving_mean_initializer=\"zeros\", moving_variance_initializer=\"one\",\n                           beta_regularizer=None, gamma_regularizer=None,\n                           beta_constraint=None, gamma_constraint=None)(y)\n    y = Conv2D(filters=NP_FILTERS, kernel_size=NP_KERNEL_SIZE)(y)\n    y = LeakyReLU(alpha=0.15)(y)\n    y = BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001, center=True,\n                           scale=True, beta_initializer=\"zeros\", gamma_initializer=\"ones\",\n                           moving_mean_initializer=\"zeros\", moving_variance_initializer=\"one\",\n                           beta_regularizer=None, gamma_regularizer=None,\n                           beta_constraint=None, gamma_constraint=None)(y)\n    y = Dropout(NP_DROPOUT_FIRST)(y)\n    y = MaxPooling2D()(y)\n    NP_FILTERS *= 2","45df4392":"# Concatinating both Left and Right branches of the Y network\n\nnetwork = concatenate([x, y])","1a6e71c8":"# Sending the the above concatenated result to a Dense network\n\nnetwork = Flatten()(network)\nnetwork = Dense(512)(network)\nnetwork = LeakyReLU(alpha=0.10)(network)\nnetwork = BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001, center=True, scale=True, beta_initializer=\"zeros\",\n                             gamma_initializer=\"ones\", moving_mean_initializer=\"zeros\",\n                             moving_variance_initializer=\"one\", beta_regularizer=None, gamma_regularizer=None,\n                             beta_constraint=None, gamma_constraint=None)(network)\nnetwork = Dropout(NP_DROPOUT_SECOND)(network)\n\nnetwork = Dense(256)(network)\nnetwork = LeakyReLU(alpha=0.10)(network)\nnetwork = BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001, center=True,\n                             scale=True, beta_initializer=\"zeros\", gamma_initializer=\"ones\",\n                             moving_mean_initializer=\"zeros\", moving_variance_initializer=\"one\",\n                             beta_regularizer=None, gamma_regularizer=None,\n                             beta_constraint=None, gamma_constraint=None)(network)\nnetwork = Dropout(NP_DROPOUT_SECOND)(network)\n\noutput = Dense(NP_CLASSES)(network)\noutput = Activation(NP_OUTPUT_FUNC)(output)","8da46ecf":"# Constructing the model \n\nmodel = Model([left_input, right_input], output)","062c1ff0":"# Summary of the model\n\nmodel.summary()","0aff1472":"# Plotting the model\n\nplot_model(model, to_file=\"y-network.png\", show_shapes=True, show_layer_names=True)","39fa50c0":"# Creating optimizer: Using RMSProp optimizer\n\n# optimizer = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\noptimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06, decay=0.0)","eb0cff2c":"# Compiling the model\n\n# compiling the model\n\nmodel.compile(\n    optimizer=optimizer,\n    loss=\"categorical_crossentropy\",\n    metrics=[\"categorical_accuracy\"]\n)","a0b5b048":"# Splitting the training data into train and validation using sklearn libraries\n\nX_train, X_val, y_train, y_val = train_test_split(train, target, test_size=0.20, random_state=123456789)","8a80e50f":"print(\"X training data shape: {}\".format(X_train.shape))\nprint(\"Y training data shape: {}\".format(y_train.shape))","bfb1b984":"print(\"X validation data shape: {}\".format(X_val.shape))\nprint(\"Y validation data shape: {}\".format(y_val.shape))","11d597ae":"# creating a generator to get the data as small batches in a lazy format - Data augumentation\n\ngenerator = image.ImageDataGenerator(\n    featurewise_center=True,\n    featurewise_std_normalization=True,\n    rotation_range=10,\n    zoom_range=0.1,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    vertical_flip=False,\n    horizontal_flip=False)","d7c529c2":"num_train_sequences = len(X_train)\nnum_val_sequences = len(X_val)\n\nprint(\"# training sequences: {}\".format(num_train_sequences))\nprint(\"# validation sequences: {}\".format(num_val_sequences))","36a4860b":"# calculating number of training and validation steps per epoch\n# for training\nif (num_train_sequences % BATCH_SIZE) == 0:\n    steps_per_epoch = int(num_train_sequences \/ BATCH_SIZE)\nelse:\n    steps_per_epoch = int(num_train_sequences \/ BATCH_SIZE) + 1\n    \n# for validation    \nif (num_val_sequences % BATCH_SIZE) == 0:\n    validation_steps = int(num_val_sequences \/ BATCH_SIZE)\nelse:\n    validation_steps = int(num_val_sequences \/ BATCH_SIZE) + 1    \n    \nprint(\"# number of steps required for training: {}\".format(steps_per_epoch))\nprint(\"# number of steps required for validation: {}\".format(validation_steps))","016018a4":"# importing datatime package\nimport datetime\n\ncurrent_dt_time = datetime.datetime.now()\nmodel_name = 'model_init' + '_' + str(current_dt_time).replace(' ', '').replace(':', '_') + '\/'\n\nif not os.path.exists(model_name):\n    os.mkdir(model_name)","c6370195":"!ls -lrt","22afddc2":"# call back: To save the best model based on validation categorical accuracy score\n\nfile_path = model_name + \"model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5\"\ncheckpoint = ModelCheckpoint(filepath=file_path, monitor='val_categorical_accuracy', verbose=1, save_best_only=True,\n                             save_weights_only=False, mode='auto', period=1)","98a2293a":"# call back: ReduceOnPlateau - If a plateau found\n \nLR = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=2, min_lr=0.000001, verbose=1, cooldown=1)","a29d0b55":"# call back: Exponential Decay\n\ndef exponential_decay_fn(epoch):\n    return 0.01 * 0.1**(epoch \/ 20)","4ad16a80":"# call back: Exponential Decay\n\ndef exponential_decay(lr, s):\n    def exponential_decay_fn(epoch):\n        return lr * 0.1**(epoch \/ s)\n    return exponential_decay_fn\n\nexponential_decay_fn = exponential_decay(lr=0.001, s=20)","83640b30":"lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)","05a4ed88":"# adding all callbacks\n\ncallbacks = [\n    # Model checkpoint\n    checkpoint, \n    # ReduceOnPlateau\n    LR, \n    # Learning Rate Scheduler\n    lr_scheduler]","62ea5485":"# fitting the model\nVERBOSE = 1\nhistory = model.fit_generator(generator.flow([X_train, X_train], y_train, batch_size=BATCH_SIZE), \n                             validation_data=generator.flow([X_val, X_val], y_val, batch_size=BATCH_SIZE),\n                             epochs=NP_EPOCHS,\n                             verbose=VERBOSE,\n                             steps_per_epoch=steps_per_epoch,\n                             validation_steps=validation_steps,\n                             class_weight=None,\n                             initial_epoch=0,\n                             callbacks=callbacks)","b313b3d1":"# finding the best model\n\n# best accuracy found\n# best model: saving model to model_init_2019-07-0108_08_42.188814\/model-00009-0.21558-0.93310-0.12834-0.96155.h5\n\nvalues = {}\nmodels = os.listdir(model_name)\n\nfor model in models:\n    converted = model.replace(\".h5\", \"\")\n    accuracy = float(converted.split(\"-\")[-1])\n    values.update({accuracy: model})\n    \nkey = max(values, key = values.get)\nbest = values.get(key)\n\n# Best model found among all the saved models\nprint(\"Best model found: {}\".format(best))","e6aa4313":"# stored history information\n\nhistory.history.keys()","ba6920a8":"plt.plot(history.history['categorical_accuracy'])\nplt.plot(history.history['val_categorical_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","f16cead5":"# summarize history for loss\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","77bd317f":"# loading the above found model\nfrom keras.models import load_model\n\nmodel_path = model_name + best\nprint(\"Absolute path found: {}\".format(model_path))","3dc9d709":"# loading the model\n\nmodel = load_model(model_path)","5aea9ad6":"# summary of the loaded model\n\nmodel.summary()","fc38a65c":"# predicting on test data\n\npredictions = model.predict([test, test])","7169dd13":"# sample predictions\n\npredictions[:5]","152ce494":"# converting the above prediction into target classes\n\nconversion = np.argmax(predictions, axis=1)","4e170280":"# converting into series\n\nresult = pd.Series(conversion, name=\"label\")","4cf7f01c":"# total length of result\n\nprint(\"No.f Id's available in the test dataset: {}\".format(len(test_df['id'])))\nprint(\"No.f predictions done by the model on test data: {}\".format(len(result)))","818bf758":"# creating a dataframe called submission to submit the data\n\nsubmission = pd.concat([test_df['id'], result],axis = 1)","e4c1da33":"submission.head()","3d01e1e0":"# creating submission file\n\nsubmission.to_csv(\"submission.csv\", index=False)","5bb3f57c":"!ls -lrt","e3ff0688":"##### Creating the Left branch of Y network","9dd79494":"##### Creating Call backs","cf758882":"##### Creating the Right branch of the Y network","44f7d652":"#### Model Construction","1e21b954":"#### Normalizing the training data and test data","a7a1e8a6":"###### Model Building Parameters","82c86a37":"#### Building a Model (Y network) using Keras CNN Conv2D (Functional API)\n\n1. We'll use Keras CNN 2D, to create Left and Right network of our Y network (i.e., Left and Right branches). Once it's created then we'll concatenate and send output to a Dense Network.\n2. Try to fit the above model using the train and validation data and save the best model.\n3. Using the best model saved, try to predict the target label of the test data and submit the same."}}