{"cell_type":{"c7ce7880":"code","00dcd61c":"code","084b1c02":"code","d359afed":"code","dd9a54c2":"code","d73c53b8":"code","ed3e69dd":"code","35be6d3b":"code","490e5443":"code","0a8ac2a9":"code","37be8e20":"code","059440d8":"code","d569b2b8":"code","8c78867c":"code","9456cf51":"code","4e4ee37a":"markdown","a73326a1":"markdown","6e17df9c":"markdown","1300cf21":"markdown","3dc62ffc":"markdown","db108d2b":"markdown","cbb66574":"markdown","86226d96":"markdown","51c2c856":"markdown"},"source":{"c7ce7880":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))","00dcd61c":"ted_df = pd.read_csv('..\/input\/ted_main.csv')\ntranscript_df = pd.read_csv('..\/input\/transcripts.csv')","084b1c02":"ted_df.head(3)","d359afed":"transcript_df.head(3)","dd9a54c2":"from nltk.stem import PorterStemmer #for word stemming\nfrom nltk.stem import WordNetLemmatizer #for word lemmatizing\nfrom nltk import pos_tag #for word lemmatizing\nfrom nltk.corpus import wordnet, stopwords #for word subject analyzing and stopwords removal\nfrom nltk.tokenize import sent_tokenize, word_tokenize #for tokenizing\nfrom string import punctuation\n\nimport tqdm\n#https:\/\/github.com\/tqdm\/tqdm #processing bar module\nimport gc\nimport re","d73c53b8":"def nltk2wn_tag(nltk_tag):\n    if nltk_tag.startswith('J'):\n        return wordnet.ADJ\n    elif nltk_tag.startswith('V'):\n        return wordnet.VERB\n    elif nltk_tag.startswith('N'):\n        return wordnet.NOUN\n    elif nltk_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return None","ed3e69dd":"def lemmatize_sentence(sentence):\n    '''\n    read sentences from the dataset and return lemmatized_sentences\n    '''\n    lemmatizer = WordNetLemmatizer()\n    #There are some sentences linked together by the dot, we should separate them\n    sentence = re.sub(r'[.](?=\\w)', '. ', sentence)\n    nltk_tagged = pos_tag(word_tokenize(sentence))\n    #stop words: all punctuation and common stop words (https:\/\/gist.github.com\/sebleier\/554280)\n    stop_words = set(stopwords.words('english') + list(punctuation))\n    #update some into the stop_words set\n    stop_words.update(['us','ve','nt','re','ll','wo','ca','m','s','t','``','...','-','\u2014',' ','laughter','applause', 'ok', 'oh'])\n    wn_tagged = map(lambda x: (x[0], nltk2wn_tag(x[1])), nltk_tagged)\n    res_words = []\n    for word, tag in wn_tagged:\n        #word after word_tokenize would be like: I'm => [I, 'm]\n        #so it's necessary to remove \"'\" to make it possible to match the words with the stop_words set\n        word = word.replace(\"'\", \"\")\n        #remove stop words\n        if word.lower() in stop_words: continue\n        if tag is None:\n            res_words.append(word)\n        else:\n            res_words.append(lemmatizer.lemmatize(word, tag))\n    return \" \".join(res_words)","35be6d3b":"for i in tqdm.tqdm(range(len(transcript_df['transcript']))):\n    #check attribute first\n    sentence = transcript_df.iloc[i, 0]\n    #do word lemmatizing\n    sentence = lemmatize_sentence(sentence)\n    transcript_df.iloc[i, 0] = sentence","490e5443":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(use_idf=True,\n                        ngram_range=(1,1), # considering only 1-grams\n                        min_df = 0.007,     # cut words present in less than 0.7% of documents\n                        max_df = 0.07)      # cut words present in more than 7% of documents \ntfidf = vectorizer.fit_transform(transcript_df['transcript'])\nprint(tfidf.shape)","0a8ac2a9":"# Let's make a function to call the top ranked words in a vectorizer\ndef rank_words(terms, feature_matrix):\n    sums = feature_matrix.sum(axis=0)\n    data = []\n    for col, term in enumerate(terms):\n        data.append( (term, sums[0,col]) )\n    ranked = pd.DataFrame(data, columns=['term','rank']).sort_values('rank', ascending=False)\n    return ranked\n\nranked = rank_words(terms=vectorizer.get_feature_names(), feature_matrix=tfidf)\nranked.head()","37be8e20":"# Let's visualize a word cloud with the frequencies obtained by idf transformation\ndic = {ranked.loc[i,'term'].upper(): ranked.loc[i,'rank'] for i in range(0,len(ranked))}\n\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(background_color='white',\n                      max_words=100,\n                      colormap='Reds').generate_from_frequencies(dic)\nfig = plt.figure(1,figsize=(12,10))\nplt.imshow(wordcloud,interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","059440d8":"### Get Similarity Scores using cosine similarity\nfrom sklearn.metrics.pairwise import cosine_similarity\nsim_unigram=cosine_similarity(tfidf)","d569b2b8":"#create a column to record the vid number (index)\ntranscript_df['index'] = transcript_df.index.astype(str)\ndef get_similar_articles(x):\n    return \",\".join(transcript_df['index'].loc[x.argsort()[-5:-1]])\ntranscript_df['similar_articles_unigram']=[get_similar_articles(x) for x in sim_unigram]","8c78867c":"for url in transcript_df.iloc[[0,663,730,1233,338], 1]:\n    print(url)","9456cf51":"','.join(re.findall('(?<=\\'id\\': )\\d+',ted_df['related_talks'][0]))\nfor url in transcript_df.iloc[[0,865,1738,2276,892,1232],1]:\n    print(url)","4e4ee37a":"**GOAL**\n\nSince we can easily do some sorting\/filtering to find out the talks we feel interested, such as speaker names, number of comments, duration, ratings, tags, and popularity (views), I'm kind of interested in analyzing the transcript data, building another related talks recommendation algorithm, and comparing it with the related_talks column in ted_main.csv.","a73326a1":"**1. Define nltk processing method**\n\nAlthough word vectors and semantic trees are very popular and widely applied nowadays, my first idea is to do BoW (Bag-of-Words) counting and compute similarity. Word vectors might also worth trying, but let's start with simpler solution.","6e17df9c":"The most kaggle datasets I played are supervised learning, but this ted-talks dataset isn't labelled. It is very interesting that I may try some EDA and find out how I can search for some talks which have similar topics.","1300cf21":"Actually, I really don't understand why TED thinks these talks are related...... :P\n\nHope you guys find this kernel usful. Thanks to https:\/\/www.kaggle.com\/gunnvant\/building-content-recommender-tutorial and https:\/\/www.kaggle.com\/adelsondias\/ted-talks-topic-models , they are really inspiring and awesome!","3dc62ffc":"**2.2 BoW algorithm (Tf-Idf)**\n\nTo build the recommendation system, I'd like to pick up some similar but not too frequent or mere key words\/tags. After some trial and error, I find that words presentation between 0.7% and 7% might provide a good filter.\n","db108d2b":"**2. Word data preprocessing**\n\nBefore conducting word counting and analysis, I want to lemmatize the lectures and remove stopwords to make the dataset simpler.","cbb66574":"It's pretty cool that the recommendation system picks one talk which is given by the same speaker!","86226d96":"Reference:\n\nhttps:\/\/textminingonline.com\/dive-into-nltk-part-iv-stemming-and-lemmatization\n\nhttps:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/stemming-and-lemmatization-1.html\n\nhttps:\/\/simonhessner.de\/lemmatize-whole-sentences-with-python-and-nltks-wordnetlemmatizer\/\n\nhttps:\/\/gist.github.com\/sebleier\/554280\n\nhttps:\/\/www.kaggle.com\/gunnvant\/building-content-recommender-tutorial\n\nhttps:\/\/www.kaggle.com\/adelsondias\/ted-talks-topic-models\n\nhttps:\/\/en.wikipedia.org\/wiki\/Similarity_measure","51c2c856":"**2.1 Word Lemmatizing and Tokenizing**\n\nSince I would like to apply BoW to build the algorithm, I think word lemmatizing is preferable to word stemming. To avoid the lemmatizing function from doing all the same regardless its subject, I also apply pos_tag function from nltk."}}