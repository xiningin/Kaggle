{"cell_type":{"d2ded644":"code","dc08dc27":"code","4e828e60":"code","b4d6ba79":"code","2331304b":"code","f90df245":"code","dfe3d211":"code","dcb95c7a":"code","c8642820":"code","a7d88cad":"code","1b6e1bea":"code","7a0b61c9":"code","70595b72":"code","c6e6a162":"markdown","79d3eb50":"markdown","202318f1":"markdown","c9ad4b4f":"markdown","f3907b8e":"markdown","507d0e86":"markdown","2191d027":"markdown","85447b22":"markdown","7dccc139":"markdown"},"source":{"d2ded644":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")","dc08dc27":"data_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = data_gen.flow_from_directory(#TODO - Insert train directory,\n                                              shuffle=True,\n                                              batch_size=16,\n                                              target_size=(100, 100))","4e828e60":"from tensorflow.keras.layers import Flatten, Dense, LeakyReLU, Input\n\ndef build_sequential_model():\n    model = tf.keras.Sequential()\n    model.add(Input(shape=(100, 100, 3)))\n    model.add(Flatten())\n    model.add(Dense(256))\n    model.add(LeakyReLU(0.3))\n    model.add(Dense(128))\n    model.add(LeakyReLU(0.3))\n    model.add(Dense(64))\n    model.add(LeakyReLU(0.3))\n    model.add(Dense(33, activation=#TODO - Insert string containing activation func name))\n    \n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n    return model\n\n\nmodel1 = build_sequential_model()\ntf.keras.utils.plot_model(model1, 'model1.png')\nmodel1.summary()\n","b4d6ba79":"history = model1.fit(#TODO - pass in the train data, epochs=3)","2331304b":"import IPython\nmodel2 = build_sequential_model()\noptimizer = #TODO - create optimizer object, preferably Adam, with lr=0.001\n\n\n@tf.function\ndef train_step(x, y):\n    with tf.GradientTape() as tape:\n        y_pred = model2(x)\n        loss_func = tf.keras.losses.CategoricalCrossentropy()\n        loss = loss_func(#TODO - pass in ground truth values and prediction values)\n    \n    grads = tape.gradient(loss, #TODO - What are you differentiating with respect to?)\n    optimizer.apply_gradients(zip(grads, model2.trainable_variables))\n    return loss\n\n\nnum_training_steps = train_generator.n\/\/train_generator.batch_size\n\nfor epoch in range(3):\n    for step in range(num_training_steps):\n        (x, y) = train_generator.next() #generates new batch of data\n        loss = train_step(x, y)\n        IPython.display.clear_output(wait=True)\n        print(f'Epoch {epoch+1}:\\nStep {step+1} of {num_training_steps}\\nLoss: {float(loss)}')\n        ","f90df245":"model2.compile(metrics=[#TODO - use accuracy metric])\nprint(f'Accuracy: {model2.evaluate(train_generator)[1]}')","dfe3d211":"def build_functional_model():\n    input_layer = Input(shape=(100, 100, 3))\n    \n    dense1 = Dense(256)(input_layer)\n    l_relu1 = LeakyReLU(0.3)(#TODO)\n    \n    dense2 = Dense(128)(#TODO)\n    l_relu2 = LeakyReLU(0.3)(#TODO)\n    \n    dense3 = Dense(64)(#TODO)\n    l_relu3 = LeakyReLU(0.3)(#TODO)\n    \n    output = Dense(33, activation='softmax')(#TODO)\n    return tf.keras.Model(inputs=input_layer, outputs=output)\n\nfunc_model = build_functional_model()\ntf.keras.utils.plot_model(func_model, 'func_model.png')\nfunc_model.summary()","dcb95c7a":"def multioutput_problem():\n    input_layer = Input(shape=(100, 100, 3))\n    \n    dense1 = Dense(256)(#TODO)\n    l_relu1 = LeakyReLU(0.3)(#TODO)\n    \n    dense2 = Dense(128)(#TODO)\n    l_relu2 = LeakyReLU(0.3)(#TODO)\n    \n    dense3 = Dense(64)(#TODO)\n    l_relu3 = LeakyReLU(0.3)(#TODO)\n    \n    output1 = Dense(33, activation='softmax', name='Object_Class')(#TODO)\n    output2 = Dense(4, name='Bounding_Box')(#TODO)\n    return tf.keras.Model(inputs=input_layer, outputs=[#TODO, #TODO])  \n\nmulti_out = multioutput_problem()\ntf.keras.utils.plot_model(multi_out, 'multi_output_model.png')\nmulti_out.summary()","c8642820":"import os\ncats = os.listdir('..\/input\/nmlo-contest-4\/train\/train')\nname_2_ind = {}\nfor i, cat in enumerate(cats):\n    name_2_ind[cat] = i\ntrain_dir = '..\/input\/nmlo-contest-4\/train\/train'\ndf = {'filename': [], 'label': []}\nfor cat in cats:\n    folder = os.path.join(train_dir, cat)\n    for file in os.listdir(folder):\n        img_path = os.path.join(folder, file)\n        df['filename'].append(img_path)\n        df['label'].append(name_2_ind[cat])\n        \n        \ndf = pd.DataFrame(df)\ndf.head()","a7d88cad":"def generator(files, labels):\n    def callable_generator():\n        for file, label in zip(files, labels):\n            img = tf.keras.preprocessing.image.load_img(file, target_size=(#TODO))\n            img_arr = tf.keras.preprocessing.image.img_to_array(img)\/255.\n            yield img_arr, #TODO - return one-hot encoded label\n    return callable_generator","1b6e1bea":"train_dataset = tf.data.Dataset.from_generator(generator(df['filename'], df['label']),\n                                              (tf.float32, tf.float32), ((100, 100, 3), (33)))","7a0b61c9":"# shuffle the data, put data into batches, use .repeat() to make sure each instance is used multiple times\ntrain_dataset = train_dataset.shuffle(100).batch(#TODO-Batch Size?).repeat() \ntrain_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)","70595b72":"new_model = build_sequential_model()\nSTEPS_PER_EPOCH = len(df['filename'])\/\/16\n\nhistory = new_model.fit(#TODO - Pass in the train dataset, epochs=5, steps_per_epoch=#TODO)","c6e6a162":"Hopefully, by now, you have a general understanding of the Functional and Sequential APIs for building keras models. With this knowledge, you can build very complex models and you can easily try out any creative model that you think of. \n\n**More About Data Loading**\n\nAt the beginning of this lab, we used a tool called the ```ImageDataGenerator``` to load in all of the data. Of course, this assumes that our data is presented very neatly and Kaggle is probably the only source that organizes the data this way. In general, when you access data from other sources, you may have to create custom methods to load in the data effectively. The important thing to remember is that the data should be compatible with the TensorFlow model. \n\nLuckily, we have the ```tf.data.Dataset``` API that we can use to load in messier data. All we have to do is write one generator, and the batches and shuffling will be done automatically. ","79d3eb50":"## Evaluate Your Model!","202318f1":"# Using Built-In ```ImageDataGenerator```\n\nUsing the ImageDataGenerator from keras is the easiest way to load in the data when dealing with image classification models. Note that our data must be organized very specifically in order for this to work. ","c9ad4b4f":"### Summary\n\nIn this quick lab, you got to go through modelling a real dataset using TensorFlow and Keras. Of course, distinguishing between fruits is a simple task and we did not have to experiment\/preprocess that much to get a descent accuracy. Nevertheless, we went through several fundamental tasks that can be acheived using TensorFlow including loading in data, constructing models using two different APIs, and training using ```.fit``` or audodiff. \n\nThere is a lot more to TensorFlow that we did not cover in this lab, so you should definitely learn more. The library seems relatively simple, but it can accomplish complex tasks which was shown by the AlphaFold project. Checkout how AlphaFold used TensorFlow to design their breakthrough research [here](https:\/\/github.com\/deepmind\/alphafold)!","f3907b8e":"# Functional API\n\nFrom here on, we will use the ```.fit()``` method to train our TensorFlow models, but keep in mind its limitations. We will now explore a new way to build models using the Functional API. Each layer of a neural network can be thought of as a function of the previous layer. We will see how we can construct unique models using this technique.","507d0e86":"**Read more about tf.data.Dataset [here](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset)!**\n\nUsing a custom generator to load data is only one way to create a TensorFlow dataset. There are so many more options that make data loading much easier. ","2191d027":"## Multioutput Scenario\n\nHere, we will see an example of where the functional API can be very useful. If we need multiple outputs for our ","85447b22":"**Which one did better?**\n\nTheoretically, both of them should be around the same because identical optimizers, learning rates, and loss functions were used. However, there is always randomness in machine learning which is why experimentation is crucial. ","7dccc139":"# Train with ```tf.GradientTape()```\n\nUsing autodifferentiation to train your model is slightly more complex; however, this technique may be necessary when training architectures such as Generative Adversarial Networks or other advanced networks. \n\nDocumentation: [www.tensorflow.org\/guide\/autodiff](https:\/\/www.tensorflow.org\/guide\/autodiff)"}}