{"cell_type":{"c407ac0f":"code","5e02e8c7":"code","eb475c6f":"code","1d42039a":"code","2fe0f392":"code","0beb36cd":"code","911d30f8":"code","fc46e57a":"code","7040278f":"code","38421d1e":"code","c080fdfb":"code","deaf9ade":"code","b8dfa95d":"code","7d2e9365":"code","c3cd93f2":"code","4260b887":"code","ece5bfa7":"code","bacdd507":"code","8b2eac05":"code","9bd51aec":"code","70eaec2c":"code","7e9fb795":"code","fc23ac5a":"code","8cc14d4d":"code","bdf096b1":"code","8cb45740":"code","820bb600":"code","a470a04e":"code","28e728f5":"markdown","011730b9":"markdown","12c51f1b":"markdown","bda9d178":"markdown","0b5aca81":"markdown","1cdf11ad":"markdown"},"source":{"c407ac0f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5e02e8c7":"import seaborn as sns\npd.set_option('display.max_rows', 1000)\nimport matplotlib.pyplot as plt\nimport math\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\nfrom sklearn.metrics import confusion_matrix, roc_auc_score ,roc_curve,auc\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import Counter\n%pip install ppscore # installing ppscore, library used to check non-linear relationships between our variables\nimport ppscore as pps # importing ppscore\nimport string\nfrom sklearn.linear_model import LogisticRegression\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\nSEED =45\nsm = SMOTE(random_state=SEED)\nsmk=SMOTETomek(random_state=SEED)\nrus = RandomUnderSampler(random_state=SEED)\nros = RandomOverSampler(random_state=SEED)\nadasyn = ADASYN(random_state=SEED)\nlogreg = LogisticRegression()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nplt.style.use('fivethirtyeight')\n%pylab inline","eb475c6f":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","1d42039a":"# take a look for features\ntrain.sample(10)","2fe0f392":"## script to compare feature influence\ndef basic_analysis(df1, df2):\n    '''the function compares the average values of  2 dataframes'''\n    b = pd.DataFrame()\n    b['First df_mean'] = round(df1.mean(),2)\n    b['Second df_mean'] = round(df2.mean(),2)\n    c = (b['First df_mean']\/b['Second df_mean'])\n    if [c<=1]:\n        b['Variation, %'] = round((1-((b['First df_mean']\/b['Second df_mean'])))*100)\n    else:\n        b['Variation, %'] = round(((b['First df_mean']\/b['Second df_mean'])-1)*100)\n        \n    b['Influence'] = np.where(abs(b['Variation, %']) <= 9, \"feature's effect on the target is not defined\", \n                              \"feature value affects the target\")\n\n    return b","0beb36cd":"## thanks to @Nadezda Demidova  https:\/\/www.kaggle.com\/demidova\/titanic-eda-tutorial-with-seaborn\ntrain.loc[train['PassengerId'] == 631, 'Age'] = 48\n\n# Passengers with wrong number of siblings and parch\ntrain.loc[train['PassengerId'] == 69, ['SibSp', 'Parch']] = [0,0]\ntest.loc[test['PassengerId'] == 1106, ['SibSp', 'Parch']] = [0,0]","911d30f8":"## checking for Survived dependence of Sex feature\ntrain[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","fc46e57a":"## checking for Survived dependence of Pclass feature\ntrain[[\"Pclass\", \"Survived\"]].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","7040278f":"## checking for Survived dependence of Sex feature\ntrain[[\"Embarked\", \"Survived\"]].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","38421d1e":"## checking for Survived dependence of SibSp feature\ntrain[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","c080fdfb":"## checking for Survived dependence of Parch feature\ntrain[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","deaf9ade":"## let's concatenate test and train datasets excluding ID and Target features\ndf = pd.concat((train.loc[:,'Pclass':'Embarked'], test.loc[:,'Pclass':'Embarked'])).reset_index(drop=True)","b8dfa95d":"## for Age imputation let's check its dependence from Pclass\npd.DataFrame(df.groupby('Pclass')['Age'].describe())","7d2e9365":"# New Title feature\ndf['Title'] = df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ndf['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndf['Title'] = df['Title'].replace('Mlle', 'Miss')\ndf['Title'] = df['Title'].replace('Ms', 'Miss')\ndf['Title'] = df['Title'].replace('Mme', 'Mrs')\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\ndf['Title'] = df['Title'].map(title_mapping)\ndf['Title'] = df['Title'].fillna(0)\n\n##dropping Name feature\ndf = df.drop(['Name'], axis=1)\n\n# Convert 'Sex' variable to integer form!\ndf[\"Sex\"][df[\"Sex\"] == \"male\"] = 0\ndf[\"Sex\"][df[\"Sex\"] == \"female\"] = 1\ndf[\"Sex\"] = df[\"Sex\"].astype(int)\n\n## Age tuning:\ndf['Age'] = df.groupby(['Pclass'])['Age'].transform(lambda x: x.fillna(x.median()))\ndf[\"Age\"] = df[\"Age\"].astype(int)\n\n# df['Age_cat'] = pd.qcut(df['Age'],q=[0, .16, .33, .49, .66, .83, 1], labels=False, precision=1)\n\n# Ticket tuning\ntickets = []\nfor i in list(df.Ticket):\n    if not i.isdigit():\n        tickets.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(\" \")[0])\n    else:\n        tickets.append(\"x\")\ndf[\"Ticket\"] = tickets\n# df['Ticket_Frequency'] = df.groupby('Ticket')['Ticket'].transform('count')\ndf = pd.get_dummies(df, columns= [\"Ticket\"], prefix = \"T\")\n\n\n## Fare tuning:\ndf['Fare'] = df.groupby(\"Pclass\")['Fare'].transform(lambda x: x.fillna(x.median())) \ndf['Zero_Fare'] = df['Fare'].map(lambda x: 1 if x == 0 else (0))\n\n\ndef fare_category(fr): \n    if fr <= 7.91:\n        return 1\n    elif fr <= 14.454 and fr > 7.91:\n        return 2\n    elif fr <= 31 and fr > 14.454:\n        return 3\n    return 4\n\ndf['Fare_cat'] = df['Fare'].apply(fare_category)\n\n# Replace missing values with 'U' for Cabin\ndf['Cabin'] = df['Cabin'].fillna('U')\nimport re\n# Extract first letter\ndf['Cabin'] = df['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\ncabin_category = {'A':9, 'B':8, 'C':7, 'D':6, 'E':5, 'F':4, 'G':3, 'T':2, 'U':1}\n# Mapping 'Cabin' to group\ndf['Cabin'] = df['Cabin'].map(cabin_category)\n\n\ndf[\"Embarked\"] = df[\"Embarked\"].fillna(\"C\")\ndf[\"Embarked\"][df[\"Embarked\"] == \"S\"] = 1\ndf[\"Embarked\"][df[\"Embarked\"] == \"C\"] = 2\ndf[\"Embarked\"][df[\"Embarked\"] == \"Q\"] = 3\ndf[\"Embarked\"] = df[\"Embarked\"].astype(int)\n\n# New 'familySize' feature & dripping 2 features:\ndf['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n\ndf['SmallF'] = df['FamilySize'].map(lambda s: 1 if  s == 2  else 0)\ndf['MedF']   = df['FamilySize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ndf['LargeF'] = df['FamilySize'].map(lambda s: 1 if s >= 5 else 0)\n\ndf['Senior'] = df['Age'].map(lambda s:1 if s>70 else 0)\ndf['FareCat_Sex'] = df['Fare_cat']*df['Sex']\ndf['Pcl_Sex'] = df['Pclass']*df['Sex']\ndf['Pcl_Title'] = df['Pclass']*df['Title']\ndf['Title_Sex'] = df['Title']*df['Sex']\ndf['Rich_woman'] = 0\ndf['Men_3Class'] = 0\ndf.loc[(df['Pclass']<=2) & (df['Sex']==0), 'Rich_woman'] = 1\ndf.loc[(df['Pclass']==3) & (df['Sex']==1), 'Men_3Class'] = 1\ndf['Rich_woman'] = df['Rich_woman'].astype(np.int8)\ndf['Alone'] = [1 if i == 1 else 0 for i in df['FamilySize']]\n","c3cd93f2":"## adding new featrures\ndef des_stat_feat(df):\n    df = pd.DataFrame(df)\n    dcol= [c for c in df.columns if df[c].nunique()>=10]\n    d_median = df[dcol].median(axis=0)\n    d_mean = df[dcol].mean(axis=0)\n    q1 = df[dcol].apply(np.float32).quantile(0.25)\n    q3 = df[dcol].apply(np.float32).quantile(0.75)\n    \n    #Add mean and median column to data set having more then 3 categories\n    for c in dcol:\n        df[c+str('_median_range')] = (df[c].astype(np.float32).values > d_median[c]).astype(np.int8)\n        df[c+str('_mean_range')] = (df[c].astype(np.float32).values > d_mean[c]).astype(np.int8)\n        df[c+str('_q1')] = (df[c].astype(np.float32).values < q1[c]).astype(np.int8)\n        df[c+str('_q3')] = (df[c].astype(np.float32).values > q3[c]).astype(np.int8)\n    return df\n\ndf = des_stat_feat(df) ","4260b887":"def reduce_memory_usage(df):\n    \"\"\" The function will reduce memory of dataframe\n    Note: Apply this function after removing missing value\"\"\"\n    intial_memory = df.memory_usage().sum()\/1024**2\n    print('Intial memory usage:',intial_memory,'MB')\n    for col in df.columns:\n        mn = df[col].min()\n        mx = df[col].max()\n        if df[col].dtype != object:            \n            if df[col].dtype == int:\n                if mn >=0:\n                    if mx < np.iinfo(np.uint8).max:\n                        df[col] = df[col].astype(np.uint8)\n                    elif mx < np.iinfo(np.uint16).max:\n                        df[col] = df[col].astype(np.uint16)\n                    elif mx < np.iinfo(np.uint32).max:\n                        df[col] = df[col].astype(np.uint32)\n                    elif mx < np.iinfo(np.uint64).max:\n                        df[col] = df[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)\n            if df[col].dtype == float:\n                df[col] =df[col].astype(np.float32)\n    \n    red_memory = df.memory_usage().sum()\/1024**2\n    print('Memory usage after complition: ',red_memory,'MB')\n    \nreduce_memory_usage(df)","ece5bfa7":"#creating matrices for feature selection:\nX_train = df[:train.shape[0]]\nX_test_fin = df[train.shape[0]:]\ny = train.Survived\nX_train['Survived'] = y\ndf = X_train","bacdd507":"# check and remove constant columns\ncolsToRemove = []\nfor col in df.columns:\n    if df[col].std() == 0.00: \n        colsToRemove.append(col)\n        \n# remove constant columns in the training set\ndf.drop(colsToRemove, axis=1, inplace=True)\nX_test_fin.drop(colsToRemove, axis=1, inplace=True)\nprint(\"Removed `{}` Constant Columns\\n\".format(len(colsToRemove)))\nprint(colsToRemove)","8b2eac05":"df.describe().T","9bd51aec":"## distribution and checking for outliers in numeric features\nimport matplotlib.pyplot as plt\nfeatures = df[['Age', 'SibSp', 'Parch', 'Fare']].columns\nfor i in features:\n    sns.boxplot(x=\"Survived\",y=i,data=df)\n    plt.title(i+\" by \"+\"Survived\")\n    plt.show()","70eaec2c":"## distribution of cat features\ncat_features = df[['Pclass', 'Sex', 'Embarked', 'Cabin', 'Title']].columns\nfor i in cat_features:\n    sns.barplot(y=\"Survived\",x=i,data=df)\n    plt.title(i+\" by \"+\"Survived\")\n    plt.show()","7e9fb795":"def spearman(frame, features):\n    spr = pd.DataFrame()\n    spr['feature'] = features\n    spr['spearman'] = [frame[f].corr(frame['Survived'], 'spearman') for f in features]\n    spr = spr.sort_values('spearman')\n    plt.figure(figsize=(6, 0.25*len(features)))\n    sns.barplot(data=spr, y='feature', x='spearman', orient='h')\n    \nfeatures = df.drop(['Survived'], axis=1).columns\nspearman(df, features)","fc23ac5a":"def basic_details(df):\n    b = pd.DataFrame()\n    b['Missing value, %'] = round(df.isnull().sum()\/df.shape[0]*100)\n    b['N unique value'] = df.nunique()\n    b['dtype'] = df.dtypes\n    return b\nbasic_details(df)","8cc14d4d":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.over_sampling import RandomOverSampler\n\nX = df.drop('Survived', axis=1)\ny = df.Survived\n\nx_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=SEED)\nx_train, y_train= ros.fit_resample(x_train, y_train)\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\nd_test = xgb.DMatrix(X_test_fin)\n","bdf096b1":"params = {\n        'objective':'binary:hinge',\n        'max_depth':13, # first pair to tune from 0 to 13\n        'learning_rate':0.1, # 4th to tune\n        'eval_metric':'auc',\n        'min_child_weight':7, # first pair to tune from 0 to 12\n        'subsample':0.8,\n        'colsample_bytree':0.8,\n        'seed':29,\n#         'reg_lambda':1,\n        'reg_alpha':0, ## 3rd to tune \n        'gamma':0, # second for tune with step 0.1 from 0 to 0.5\n        'scale_pos_weight':1,\n        'n_estimators': 5000,\n        'nthread':-1\n}\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\nnrounds=10000  \nmodel = xgb.train(params, d_train, nrounds, watchlist, early_stopping_rounds=50, \n                           maximize=True, verbose_eval=10)\nfrom sklearn import metrics\n\ny_pred = model.predict(d_valid)\nprint('Accuracy :{0:0.5f}'.format(metrics.accuracy_score(y_valid, y_pred))) \nprint('AUC : {0:0.5f}'.format(metrics.roc_auc_score(y_valid, y_pred)))\nprint('Precision : {0:0.5f}'.format(metrics.precision_score(y_valid, y_pred)))\nprint('Recall : {0:0.5f}'.format(metrics.recall_score(y_valid, y_pred)))\nprint('F1 : {0:0.5f}'.format(metrics.f1_score(y_valid, y_pred)))","8cb45740":"fig,ax = plt.subplots(figsize=(15,20))\nxgb.plot_importance(model,ax=ax,max_num_features=20,height=0.8,color='g')\n# plt.tight_layout()\nplt.show()","820bb600":"xgb.to_graphviz(model)","a470a04e":"sub = pd.DataFrame()\nsub['PassengerId'] = test['PassengerId']\nsub['Survived'] = model.predict(d_test)\nsub['Survived'] = sub['Survived'].apply(lambda x: 1 if x>0.8 else 0)\nsub.head()","28e728f5":"Loading datasets...","011730b9":"Outlier detection","12c51f1b":"Distribution of cat features....","bda9d178":"Loading libraries...","0b5aca81":"Final model's accuracy:","1cdf11ad":"Model Tuning...."}}