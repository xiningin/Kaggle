{"cell_type":{"f8eac691":"code","ee46f439":"code","55aa8493":"code","cb280dec":"code","ed2ef1aa":"code","ac964708":"code","b21af4e4":"code","b6558186":"code","df5b2416":"code","328c0633":"code","546b723e":"code","7d264195":"code","7ab52d21":"code","9d9a831d":"code","b34b8f8b":"code","aeac75bc":"code","0d607e4d":"code","6b2fa26d":"code","88c6da43":"code","97da5568":"code","3826c032":"code","98fd5a19":"code","9b018137":"code","d35e4f3e":"code","863b72d5":"code","69ebb774":"code","763b7c3b":"code","7f94ef2b":"code","7696786e":"code","581ba62d":"code","3cbb6442":"code","af6081bb":"code","b1d51aba":"code","dbb3de02":"code","b262f34b":"code","870b4514":"code","ee4cea2b":"code","45377971":"code","8be96563":"code","93757ae9":"code","85c4c10e":"code","ecf56163":"code","d8dc4f96":"code","35285f4c":"code","ec4e2475":"code","580c05dd":"code","56717e64":"code","5d668816":"code","e027d02f":"code","24b956f3":"code","9112c89d":"code","83d5f24e":"code","d75ba8ec":"code","a90f21b4":"code","319f30f6":"markdown","1d47180a":"markdown","c15cd6a1":"markdown","358e3a5b":"markdown","c8a2cc38":"markdown","4d02bbb7":"markdown","517f8592":"markdown","64a903f7":"markdown","da57a016":"markdown","64c4fea8":"markdown","d4b47ff8":"markdown","bfb10a19":"markdown","b5cc3041":"markdown","a950ef22":"markdown","969b6208":"markdown","51da10c5":"markdown","27e9b37a":"markdown","074ad07f":"markdown","c1b4a3cd":"markdown","3d4a726a":"markdown","4e4f2345":"markdown","f7b7dd5d":"markdown","bca60038":"markdown","d16232e0":"markdown","a03d6a5c":"markdown","83a36246":"markdown","c03141b2":"markdown","f0d46008":"markdown","9002033e":"markdown","21018f4f":"markdown","b97ffe79":"markdown","faa30375":"markdown"},"source":{"f8eac691":"pip install autocorrect","ee46f439":"pip install spacy==3.0.5","55aa8493":"!python -m spacy download en_core_web_sm","cb280dec":"pip install spacytextblob","ed2ef1aa":"import warnings\nimport numpy as np \nimport pandas as pd \nimport os\nimport re\nimport nltk\nfrom nltk.corpus import abc\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import *\nfrom nltk.stem.snowball import *\nfrom nltk.util import ngrams\nimport string\nimport spacy\nfrom spacy import displacy\nfrom spacytextblob.spacytextblob import SpacyTextBlob\nfrom autocorrect import Speller\nfrom sklearn import preprocessing\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom wordcloud import WordCloud, STOPWORDS\nimport gensim\nfrom gensim.models.word2vec import Text8Corpus\nfrom gensim.models import KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom textblob import TextBlob\nfrom IPython.core.display import HTML\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","ac964708":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b21af4e4":"data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\nprint('Total number of entries in the train dataset are:', len(data))\ndata.head()","b6558186":"data.shape","df5b2416":"data.info()","328c0633":"data.dtypes","546b723e":"data.isnull().sum()","7d264195":"data.isna()","7ab52d21":"data = data[['id', 'text']]\ndata","9d9a831d":"text = data['text']\ntext","b34b8f8b":"sents1 = sent_tokenize(text[1000])\nprint(f'Sentence Tokenization using NLTK: \\n {text[1000]} => {sents1}')","aeac75bc":"words1 = word_tokenize(sents1[0])\nprint(f'Word Tokenization using NLTK: \\n {sents1[0]} => {words1}')","0d607e4d":"sp = spacy.load('en_core_web_sm')","6b2fa26d":"print(f'Word Tokenization using SpaCy: \\n\\n{sp(text[0])} =>\\n')\n\nwords2 = sp(text[0])\nfor word in words2:\n    print(word)","88c6da43":"nltk.download('stopwords')","97da5568":"tokens1 = [word for word in words1 if not word in stopwords.words('english')] \nprint(f'Stopword Removal using NLTK: \\n{words1} => {tokens1}')","3826c032":"spacy_stopwords = sp.Defaults.stop_words\nprint(f'SpaCy Stopwords: \\n{spacy_stopwords}')","98fd5a19":"tokens2 = [word for word in words2 if word not in spacy_stopwords] \nprint(f'Stopword Removal using SpaCy: \\n{words2} => {tokens2}')","9b018137":"from string import punctuation\nprint(f'Delimiters in English: \\n{punctuation}')","d35e4f3e":"tokens1 = [word for word in words1 if word not in punctuation]\nprint(f'Delimiter Removal: \\n{words1} => {tokens1}')","863b72d5":"sent_with_html = \"<head> <title> Natural Language Processing <\/title> <\/head>\"\nremove_html = re.compile('<.*?>')\n\nprint(f\"Removing HTML tags: \\n{sent_with_html} => {re.sub(remove_html, '', sent_with_html).strip()}\")","69ebb774":"spell = Speller()\n\nprint('Spelling Correction:\\n')\nfor token in tokens1:\n    print(f'{token} => {spell(token)}')\n","763b7c3b":"spell = Speller()\nmisspelled = ['Calandar', 'neccessary', 'recieve', 'adress', 'misteak']\n\nprint('Tokens before and after Spelling Correction (Own Example):\\n')\nfor token in misspelled:\n    print(f'{token} : {spell(token)}')\n","7f94ef2b":"porterStemmer = PorterStemmer()\nstemWords1 = [porterStemmer.stem(word) for word in tokens1]\n\nprint(f'Tokens after Stemming using Porter Stemmer: \\n{stemWords1}')","7696786e":"snowballStemmer = SnowballStemmer('english')\nstemWords2 = [snowballStemmer.stem(word) for word in tokens1]\n\nprint(f'Tokens after Stemming using Snowball Stemmer: \\n{stemWords2}')","581ba62d":"wordNetLemmatizer = WordNetLemmatizer()\nlemmaWords1 = [wordNetLemmatizer.lemmatize(word) for word in tokens1]\n\nprint(f'Tokens after Lemmatization using WordNet Lemmatizer: \\n{tokens1} => {lemmaWords1}')","3cbb6442":"lemmaWords2 = [word.lemma_ for word in tokens2]\n\nprint(f'Tokens after Lemmatization using SpaCy Lemmatization: \\n{tokens2} => {lemmaWords2}')","af6081bb":"animals = ['dog', 'cat', 'mouse', 'dog', 'lion', 'lion', 'mouse', 'tiger', 'rat', 'dog']\n\nlabel_encoder = preprocessing.LabelEncoder()\ndata = pd.DataFrame({'Labels' : animals, 'Label Encoder Values' : label_encoder.fit_transform(animals)})\n\nprint(\"Label Encoder\")\ndata.style.background_gradient(cmap = 'BrBG')","b1d51aba":"animals = np.array(['dog', 'cat', 'mouse', 'dog', 'lion', 'lion', 'mouse', 'tiger', 'rat', 'dog'])\n\nohe = preprocessing.OneHotEncoder()\nresult = ohe.fit_transform(animals.reshape(-1,1)).toarray()\n\ndata = pd.DataFrame(result.astype(int))\ndata['Labels'] = animals\n\nprint(\"One Hot Encoder\")\ndata.style.background_gradient(cmap = 'Wistia')","dbb3de02":"tagged_tokens1 = nltk.pos_tag(tokens1)\n\nprint(f'POS tagging using NLTK: \\n{tokens1} => {tagged_tokens1}')","b262f34b":"tagged_tokens2 = [word.pos_ for word in tokens2]\n\nprint('POS tagging using SpaCy: \\n')\nfor i in range(len(tagged_tokens2)):\n    print(f'{tokens2[i]} : {tagged_tokens2[i]}')","870b4514":"n_grams1 = ngrams(tokens1, 2)\nn_grams1 = [ ' '.join(grams) for grams in n_grams1]\n\nprint(f'N-Gram using NLTK (n = 2): \\n{tokens1} => {n_grams1}')","ee4cea2b":"n_gram_finder = nltk.collocations.TrigramCollocationFinder.from_words(tokens1)\n\nprint(f'Most Common N-Gram Finder using NLTK (n = 3): \\n{tokens1} => {n_gram_finder.ngram_fd.most_common(2)}')","45377971":"word_count = {}\n\nfor word in tokens1:\n    \n    if word not in word_count.keys():\n        word_count[word] = 1\n    else:\n        word_count[word] += 1\n        \nprint(f'Bag of Words: \\n{tokens1} => {word_count}')","8be96563":"stopwords = set(STOPWORDS)\n\nwc = WordCloud(width = 900, height = 500, background_color = 'white', random_state = 10).generate(text[1])\n\nplt.title('Word Cloud for Bag Of Words')\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","93757ae9":"def color(val):\n    \n    color = 'mediumaquamarine' if val > 0 else ''\n    return 'background-color: %s' % color","85c4c10e":"count_vectorizer = CountVectorizer()\ntext_list = list(text[0:10])\n\ntf = count_vectorizer.fit_transform(text_list)\n\ntf_feature_names = count_vectorizer.get_feature_names()\n\nprint('Term Frequency of Document')\ndf = pd.DataFrame(tf.toarray(), columns = tf_feature_names) \/ len(tf_feature_names)\ndf.style.set_caption(\"Term Frequency of Document\")\ndf.style.applymap(color)","ecf56163":"tfidf_vectorizer = TfidfVectorizer()\ntext_list = list(text[0:10])\n\ntfidf = tfidf_vectorizer.fit_transform(text_list)\n\ntfidf_feature_names = tfidf_vectorizer.get_feature_names()\n\nprint('Term Frequency - Inverse Document Frequency of Document')\ndf = pd.DataFrame(tfidf.toarray(), columns = tfidf_feature_names)\ndf.style.set_caption(\"Term Frequency - Inverse Document Frequency of Document\")\ndf.style.applymap(color)","d8dc4f96":"grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n\nparser = nltk.RegexpParser(grammar)\nparse_tree = parser.parse(tagged_tokens1)\n\nprint(f'Dependency Parse Tree using NLTK: \\n\\n{sents1[0]} => \\n\\n{parse_tree}')","35285f4c":"print('Dependency Parse Tree using SpaCy')\n\ndisplacy.render(sp(text[1000]), style = \"dep\", jupyter = True, options = {\"distance\" : 100})","ec4e2475":"ner_chunk = nltk.ne_chunk(tagged_tokens1)\n\nprint(f'Named Entity Recognition using NLTK: \\n\\n{sents1[0]} =>\\n')\n\nfor x in str(ner_chunk).split('\\n'):\n    if '\/NN' in x:\n        print(x)","580c05dd":"ner_sent = sp(text[1])\n\nprint(f'Named Entity Recognition using SpaCy: \\n\\n{ner_sent} =>\\n')\n\nfor ent in ner_sent.ents:\n    print(ent.text, '-', ent.label_)","56717e64":"print('Named Entity Recognition Visualization using SpaCy')\ndisplacy.render(ner_sent, style = 'ent', jupyter = True)","5d668816":"warnings.filterwarnings('ignore')\n\nword2vec = gensim.models.Word2Vec(abc.sents())\nembedding1 = word2vec.most_similar(tokens1[-1])\nvector1 = word2vec[tokens1[-1]]\n\nprint(f'Word Embeddibng using Word2Vec: \\n\\nMost Similar Words => {embedding1} \\n\\nWord Vector => {vector1}')","e027d02f":"pca = PCA(n_components = 2)\n\nX = word2vec[word2vec.wv.vocab]\nX = X[ : 10, : ]\n\nresult = pca.fit_transform(X)\nwords = list(word2vec.wv.vocab)\nwords = words[ : 10]\n\nplt.figure(figsize = (15, 20))\nplt.title('Word Embedding using PCA')\nplt.scatter(result[ : , 0], result[ : , 1])\nfor i, word in enumerate(words):\n    plt.annotate(word, xy = (result[i, 0], result[i, 1]))\nplt.show()","24b956f3":"input_file = '..\/input\/glove6b\/glove.6B.100d.txt'\noutput_file = '..\/glove.6B.100d.txt.word2vec'\n\nglove2word2vec(input_file, output_file)\n\nmodel = KeyedVectors.load_word2vec_format(output_file, binary = False)\nresult = model.most_similar(positive = ['king', 'woman'], negative = ['man'], topn = 1)\n\nprint(f'Word Embedding using GloVe: \\n{result}')","9112c89d":"sia = SentimentIntensityAnalyzer()\npolarity_scores1 = sia.polarity_scores(text[10])\n\nprint(f\"Sentiment Analysis using NLTK: \\n{text[10]} => {polarity_scores1}\")","83d5f24e":"x = np.array([[sia.polarity_scores(word)['compound'] for word in word_tokenize(text[10])]])\nannot = np.array([word_tokenize(text[10])])\n\nplt.figure(figsize = (10, 3))\nsns.heatmap(x, annot = annot, fmt = '')\nplt.title('Heatmap for Sentiment Analysis')\nplt.show()","d75ba8ec":"sp.add_pipe('spacytextblob')\nsenti_sent = sp(text[20])\n\nprint(f\"SpaCy using TextBlob: \\n\\n{text[20]} =>\\n\")\nprint(f\"Sentiment Analysis: {senti_sent._.polarity}\")\nprint(f\"Sentiment Subjectivity: {senti_sent._.subjectivity}\")\nprint(f\"Sentiment Assessment: {senti_sent._.assessments}\")\n","a90f21b4":"sub = sp(text[20])\nx = np.array([[word._.subjectivity for word in sub]])\nannot = np.array([word_tokenize(text[20])[ : -1]])\n\nplt.figure(figsize = (6, 3))\nsns.heatmap(x, annot = annot, fmt = '')\nplt.title('Heatmap for Subjectivity Detection')\nplt.show()","319f30f6":"# **Stemming**\n\nStemming applies algorithmic rules to extract the stem out of the derived word simply. The words produced by this step do not have any essential meaning, but they are simply a bunch of letters put together without affixes. For example, the word \u201cbeautiful\u201d is stemmed to \u201cBeauti\u201d.\n\n![09-min.jpg](attachment:09-min.jpg)","1d47180a":"# **Removal of Tags**\n\nDuring web scraping, the data is scraped from web pages residing on the website, and they contain HTML tags. These tags do not provide any necessary information and hence, can be erased. For example, a tag like < body > (Body Tag) is deleted.\n\n![SEO-Meta-Tags-min.jpg](attachment:SEO-Meta-Tags-min.jpg)","c15cd6a1":"# **Named Entity Recognition**\n\nNER - Named Entity Recognition is the process of extracting proper nouns or proper noun phrases. For example, in the sentence 'Robert is interested in Amazon', the entities 'Robert' (Name) and 'Amazon' (Organization) are selected.\n\n![do-biomedical-ner-with-nlp-min.png](attachment:do-biomedical-ner-with-nlp-min.png)","358e3a5b":"<div class = \"alert alert-info\" style = 'color:black'> \ud83d\udd26 For Word Embedding using GloVe, the embedding representation 'glove.6B.100d.txt' is used! <\/div>","c8a2cc38":"# **N-Gram**\n\nN-gram is a language model widely used in NLP and is applied to statistical problems involving text and audio. It is a probabilistic model that predicts the next series of words. For example, in the sentence, \u201cThe movie was boring.\u201d Unigram processes the text as [\u201cThe\u201d, \u201cmovie\u201d, \u201cwas\u201d, \u201cboring\u201d]. Bi-gram processes the text as [\u201cThe movie\u201d, \u201cmovie was\u201d, \u201cwas boring\u201d]. Tri-gram processes the text as [\u201cThe movie was\u201d, \u201cmovie was boring\u201d]\n\n![7356.1569499094-min.png](attachment:7356.1569499094-min.png)","4d02bbb7":"# **FEATURE ENGINEERING**","517f8592":"# **Stopword Removal**\n\nThe dataset may contain words like \u2018after,\u2019 \u2018every\u2019 and \u2018I.\u2019 These words are not relevant to important NLP applications like the sentiment detection process. Thereby, these words can be deleted to minimize the burden on the system.\n\n![stopwords-min.jpg](attachment:stopwords-min.jpg)","64a903f7":"# **Import Packages**","da57a016":"# **DATA PREPROCESSING**","64c4fea8":"# **ENVIRONMENT SETUP**","d4b47ff8":"# **Word Tokenization**\n\nWord tokenization is the same as sentence tokenization. But, rather than applying it to\nsentences, it is used on words so that individual words are separated as items in a\nlist. For example, in the sentence, \"Chennai is humid,\u201d the result is [\"Chennai,\u201d \u201cis,\u201d\n\u201chumid\u201d].\n\n![fig2-min-min.png](attachment:fig2-min-min.png)","bfb10a19":"# **Encoding**\n\nEncoding is the process of encrypting data in a format that computers can understand. Humans comprehend natural language. However, a machine is capable of decoding only 0s and 1s. Encoding converts text to digits. For example, the words 'positive' and 'negative' are mapped to the numbers '0' and '1'.\n\n![1_Yp6r7m82IoSnnZDPpDpYNw-min.png](attachment:521f4c95-586c-4a8a-ba38-b54fbcf99120.png)","b5cc3041":"# **Spell Check**\n\nWhile communicating online, people use shorthand, urban dictionary words, and slang in their language. Standard dictionaries do not hold these words, so lookups into these dictionaries lead to inconsistency in data. Therefore it is essential to correct spelling.\n\n![1_6lG-4g_jIIco25FPyRa1AQ-min.png](attachment:1_6lG-4g_jIIco25FPyRa1AQ-min.png)","a950ef22":"<div class=\"alert alert-info\" style = 'color:black'>\n  \ud83e\udd29 Visualizing Word Embedding with Word2Vec using PCA! <\/div>","969b6208":"# **Content**\n\n* **ENVIRONMENT SETUP**\n    * Install Packages\n    * Import Packages\n    \n\n* **DATA PREPROCESSING**\n    * Data Loading\n    * Preliminary Analysis\n    * Sentence Tokenization\n    * Word Tokenization\n    * Stopword Removal\n    * Removal of Tags\n    * Delimiter Removal\n    * Spell Check \n    * Stemming\n    * Lemmatization\n    \n    \n* **FEATURE ENGINEERING**\n    * Encoding    \n    * POS Tagger\n    * N-GRAM\n    * Bag Of Words\n    * TF\n    * TF-IDF\n    * Dependency Parser\n    * Named Entity Recognition\n    * Word Embedding\n    * Sentiment Analysis\n    * Subjectivity Detection","51da10c5":"# **Install Packages**","27e9b37a":"# **Delimiter Removal**\n\nDelimiters are removed to reduce the size of the dataset as they do not supply any vital information in some cases. A Few delimiters are question marks (?), full stops (.), and exclamation marks (!). For example, after delimiter removal the sentence 'I am cold!' becomes 'I am cold'. \n\n![bfbe0b98b48faee19456e72e2fc61641-min.jpg](attachment:bfbe0b98b48faee19456e72e2fc61641-min.jpg)","074ad07f":"# **Term Frequency - Inverse Document Frequency**\n\nTF-IDF \u2013 Term Frequency-Inverse Document Frequency is described as the importance of a word in a document, which is proportional to the number of times the word appears in the document. For example, the word \u201cFruit\u201d appears in 100 of 10000 documents and the term frequency is 5 then the TF-IDF is 0.05 * log(10000\/100) = 5 * 2 = 10.\n\n![0_7r2GKRepjh5Fl41r-min.png](attachment:0_7r2GKRepjh5Fl41r-min.png)","c1b4a3cd":"# **Lemmatization**\n\nLemmatization is similar to stemming but it add context to bring out the true meaning. It groups inflected forms of words to be interpretd as a single root word. For example, the word \u201cbeautiful\u201d is stemmed to \u201cBeauty\u201d unlike \u201cBeauti\u201d.\n\n![stemvslemma.png](attachment:ec28f8ab-2d2f-4c47-a82f-27ebc94d3c0e.png)","3d4a726a":"# **Bag of Words**\n\nThe bag of words carries out sentence tokenization and word tokenization. After that, it counts the number of appearances of each word. For example, in a sentence, \u201cIt is nice but horrid, and that\u2019s not a nice thing.\u201d The word \u201cnice\u201d is extracted and countered with two occurrences.\n\n![image-20190906164045-2-min.jpeg](attachment:image-20190906164045-2-min.jpeg)","4e4f2345":"# **Word Embedding**\n\nWord embedding is the process of converting text into numerical data. Since computers work on only numbers and cannot accurately compute data in the form of strings, words are embedded into a unique integer or numerical code. For example, the word \u201cSevere\u201d is fixed as a number \u201c3214\u201d.\n\n![we1-min.png](attachment:we1-min.png)","f7b7dd5d":"# **Preliminary Analysis**","bca60038":"# **Term Frequency**\n\nTF \u2013 Term Frequency is described as the number of times that a term occurs in a document. It considers all the terms of equal importance. For example, the word \u201cFruit\u201d appears five times in a document of 100 words, then the TF for \u201cFruit\u201d is 5\/100 = 0.05.\n\n![term_frequency_after_stopword_removal-min.png](attachment:term_frequency_after_stopword_removal-min.png)","d16232e0":"<div class = \"alert alert-success\" style='color:black'> \ud83c\udf1f SpaCy Version 3.0! <\/div>","a03d6a5c":"# **Sentence Tokenization**\n\nFor any corpus, we first divide a huge entity into smaller entities so that they can be treated individually. Tokenization also does a similar task but upon sentences in text. First, the text is broken down into sentences and that is further broken down into words. The input is given as text or a corpus. The output generates a list of sentences. For example, in the text, \"I love dogs. I have a dog\", the output is [\"I love dogs,\u201d I have a dog\u201d]\n\n![Screenshot%20%2868%29.png](attachment:Screenshot%20%2868%29.png)","83a36246":"# **Sentiment Analysis**\n\nSentimental analysis plays a significant role in determining the polarity of a review or a comment. It is used to know whether the person is talking about something in a positive way or a negative way. It can be classified broadly into positive, negative, and neutral. For example, on a tourism website, a person leaves a remark stating, \u201cThere are beautiful tourist spots in Switzerland. \u201cThe word \u2018beautiful\u2019 is positive as it describes Switzerland as pretty.\n\n![sentiment-analysis-min.png](attachment:sentiment-analysis-min.png)","c03141b2":"# **POS Tagger**\n\nPOS tagger is parts of speech tagger that is an in-built function found in a standard library. It tags the words in the sentences according to the grammar of the langauge. For example, in the text, \u201cThe pizza was disgusting but the location was beautiful\u201d, the result after implementing POS tagger will be [\u201cThe [DT]\u201d, \u201cpizza [NN]\u201d, \u201cis [VB]\u201d, \u201cdisgusting [VBG]\u201d, \u201cbut [CC]\u201d, \u201cthe [DT]\u201d, \u201clocation [NN]\u201d, \u201cwas [VBD], \u201cbeautiful [JJ]].\n\n![1_fRjvBbgzo90x0MZdXZT82A%20%281%29-min.png](attachment:1_fRjvBbgzo90x0MZdXZT82A%20%281%29-min.png)","f0d46008":"# **Subjectivity Detection**\n\nSubjectivity Detection relies upon the answer to the question \"Is it based on facts or opinions?\". For example, the sentence 'I love cats' is subjective and the sentence 'Cats have tails' is objective.\n\n![192159-6-XL-EO-BLOG-MO-min.jpg](attachment:192159-6-XL-EO-BLOG-MO-min.jpg)","9002033e":"# **Description**\n\n**This is a starter notebook for NLP in text processing. It covers Data Preprocessing and Feature Engineering in comprehensive details. The code is implemented in popular NLP libraries namely NLTK, SpaCy, and occasionaly TextBlob. Please upvote and share the notebook if you found it helpful in any way. Thank you in advance!.**\n\n![0_zKRz1UgqpOZ4bvuA-min.png](attachment:0_zKRz1UgqpOZ4bvuA-min.png)![1_ax2uBqfp963n4PQVqmGplQ-min.png](attachment:1_ax2uBqfp963n4PQVqmGplQ-min.png)![textblob-logo-min.png](attachment:textblob-logo-min.png)","21018f4f":"# **Dependency Parser**\n\nStanford dependency parser establishes the relationship between entities in the language using grammatical rules. The output of the parser is a tree structure that is annotated. For example, in the sentence \u201cThe funny boy joked,\u201d \u201cfunny\u201d is an adjective for the noun \u201cboy.\u201d\n\n![Syntax+Parsing+with+CoreNLP+and+NLTK.png](attachment:Syntax+Parsing+with+CoreNLP+and+NLTK.png)","b97ffe79":"# **NATURAL LANGUAGE PROCESSING**\n\n**Natural Language Processing and Machine Learning make it possible to build robust models with the storage capacity and processing power available to us today. Natural Language Processing concepts deal with processing human langauge while discovering patterns, relationships and, semantics present in large amounts of data.**\n\n![nlp-important-use-cases-min.png](attachment:nlp-important-use-cases-min.png)","faa30375":"# **Data Loading**"}}