{"cell_type":{"eaaf8e1a":"code","1a2614d4":"code","8e3dae39":"code","b27f2848":"code","9fd85068":"code","ab638a1f":"code","f583eabd":"code","ac19aadc":"code","24206a20":"code","60933322":"code","a9e27035":"code","5656ff0b":"code","2ca8224d":"code","f8130dcf":"code","74411b88":"markdown","cf4ffdbe":"markdown","687d54f7":"markdown"},"source":{"eaaf8e1a":"# All libraries we will need\nimport pandas as pd # To store data as a dataframe\nimport requests # to get the data of an url\nprint('Setup complete!')","1a2614d4":"# We need the url of the page we are gonna scrape, we will start with the first page\nurl = 'https:\/\/www.kaggle.com\/rankings.json?group=notebooks&page=1&pageSize=20'\nresponse = requests.get(url) # Get content of page","8e3dae39":"import json\n\n# Get json object, take a look at how data is structured\njson_resp = json.loads(response.text)\n#json_resp # Uncomment to see structure ","b27f2848":"#Counts for tier\njson_resp['counts']","9fd85068":"# We need to sum the first 3 tiers, the ones that count for the datasets ranking (grandmaster, master, expert)\nrankingTiers = json_resp['counts'][0:3]\nrankingTiers","ab638a1f":"# We need the number of rankers in notebooks to know how many pages of 20 (every page has a fixed size of 20 users) we need to scrappe.\nnumberOfRankers = 0\nfor tier in rankingTiers:\n    numberOfRankers += tier['count']\nprint(numberOfRankers)","f583eabd":"# Round up using ceil as we need an extra page for the last ones\nimport math\n\nnumPagsToScrappe = math.ceil(numberOfRankers \/ 20)\nnumPagsToScrappe","ac19aadc":"# Get the kagglers users data \nusersList = json_resp['list']\nusersList[0] # Show first user of the list","24206a20":"# Data to extract\n\ncurrentRanking = []\ndisplayName = []\nthumbnailUrl = []\nuserId = []\nuserUrl = []\ntier = []\npoints = []\njoined = []\ntotalGoldMedals = []\ntotalSilverMedals = []\ntotalBronzeMedals = []","60933322":"baseURL = 'https:\/\/www.kaggle.com\/rankings.json?group=discussion&pageSize=20&page='\n\nfor page in range(1, numPagsToScrappe + 1): # Page query starts at 1, and its a range()  function so we need to add 1 \n    # We need the url of the page we are gonna scrape\n    pageToScrape = baseURL + str(page) # To acces the multiple pages\n    resp = requests.get(pageToScrape) # Get content of page\n    json_response = json.loads(resp.text) # Get JSON object \n    jsonRespListUsers = json_response['list'] # Get list of users of the JSON object\n    \n    for user in range(0, len(jsonRespListUsers)):\n        currentRanking.append(jsonRespListUsers[user]['currentRanking'])\n        displayName.append(jsonRespListUsers[user]['displayName'])\n        #thumbnailUrl.append(jsonRespListUsers[user]['thumbnailUrl'])\n        userId.append(jsonRespListUsers[user]['userId'])\n        userUrl.append(jsonRespListUsers[user]['userUrl'])\n        tier.append(jsonRespListUsers[user]['tier'])\n        points.append(jsonRespListUsers[user]['points'])\n        joined.append(jsonRespListUsers[user]['joined'])\n        totalGoldMedals.append(jsonRespListUsers[user]['totalGoldMedals'])\n        totalSilverMedals.append(jsonRespListUsers[user]['totalSilverMedals'])\n        totalBronzeMedals.append(jsonRespListUsers[user]['totalBronzeMedals'])\n","a9e27035":"# Create dataFrame with the information we have\ntopKagglersDiscussion = pd.DataFrame({\n    'displayName':displayName,\n    'currentRanking':currentRanking,\n    #'thumbnailUrl':thumbnailUrl,\n    'userId':userId,\n    'userUrl':userUrl,\n    'tier':tier,\n    'points':points,\n    'userJoinDate':joined,\n    'totalGoldMedals':totalGoldMedals,\n    'totalSilverMedals':totalSilverMedals,\n    'totalBronzeMedals':totalBronzeMedals\n\n})","5656ff0b":"# First 7 rows of the dataframe\ntopKagglersDiscussion.head(7)","2ca8224d":"# We check the sizes of the dataframe\ntopKagglersDiscussion.shape","f8130dcf":"# Build csv\ntopKagglersDiscussion.to_csv('topKagglersDiscussion.csv', index=False)","74411b88":"In the next tutorials we will see that similar pages have very similar data formats, so you can extract data with the same code.\n\nThe only thing that changes between this scrapping and the last one is the query group:\n\ngroup=discussion","cf4ffdbe":"Data Mining - Web Scrapping: The saga:\n1. https:\/\/www.kaggle.com\/ajpass\/data-mining-web-scrapper-vol-1-pokedex # Scrapping a pokedex, all pokemon with all stats\n2. https:\/\/www.kaggle.com\/ajpass\/data-mining-web-scrapping-vol-2-pokedex-pandas # Scrapping a pokedex, using a diferent method than number 1, easier but in case a column has multiple values, in some cases you may need to do some cleaning. Result similar as 1, only changes: Type and ID. \n3. https:\/\/www.kaggle.com\/ajpass\/data-mining-web-scrapper-vol-3-sudoku-to-string # Extract sudokus data and transform it to string\n4. https:\/\/www.kaggle.com\/ajpass\/data-mining-web-scrapping-vol-66-kaggle-datasets # It's about scrapping the top 20 kagglers in datasets ranking\n5. https:\/\/www.kaggle.com\/ajpass\/data-mining-web-scrapping-vol-4-kaggle-datasets2 # Second take at kaggle datasets scrapping \n6. https:\/\/www.kaggle.com\/ajpass\/data-mining-web-scrapping-vol-5-kaggle-notebooks # Scrap top notebook kagglers\n7. https:\/\/www.kaggle.com\/ajpass\/web-scrapping-vol-6-kaggle-discussions # Scrap top discussion kagglers","687d54f7":"To understand how this notebook work check this tutorial: https:\/\/www.kaggle.com\/ajpass\/data-mining-web-scrapping-vol-4-kaggle-datasets2"}}