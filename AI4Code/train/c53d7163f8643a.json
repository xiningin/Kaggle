{"cell_type":{"cbdc4699":"code","61c4a46a":"code","196b56c6":"code","c06b97c4":"code","f85dd3c1":"code","31162440":"code","b6a7c057":"code","ffec87b2":"code","2513de3c":"code","13581515":"code","6259a91b":"code","29a13c3d":"code","bba22dff":"code","75fcaed1":"code","545159e8":"code","7fd96e6a":"code","e071ab76":"code","06369f58":"code","f9979e4f":"code","30bbedac":"code","15d357a2":"code","0312fbd5":"code","e60c4f11":"code","70d658a1":"code","6fcad0c6":"code","2b26f93e":"code","cde0d407":"code","a0ea7353":"code","04cadcdc":"code","712c6c9a":"code","abb90401":"code","52a2c7cb":"markdown","a29f095e":"markdown","7f95d12a":"markdown"},"source":{"cbdc4699":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy import stats\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n\nplt.style.use('ggplot')","61c4a46a":"dirpath = '..\/input\/house-prices-advanced-regression-techniques\/train.csv'\ndf = pd.read_csv(dirpath)\ndf.head()","196b56c6":"df['SalePrice'].describe()","c06b97c4":"plt.figure(figsize=(9,6))\nsns.distplot(df['SalePrice'])\nplt.ylabel('Density')\nplt.show()","f85dd3c1":"print('Skewness : %f' % df['SalePrice'].skew())\nprint('Kurtosis : %f' % df['SalePrice'].kurt())","31162440":"#Check for above grade living area and sale price\ndef plots_scatter(df, var='GrLivArea', target='SalePrice'):\n    data = pd.concat([df[var], df[target]], axis=1)\n    data.plot.scatter(x=var, y=target, \n                      figsize=(9,6),ylim=(0,800000), alpha=0.3)\n    plt.show()\n\nplots_scatter(df, var='GrLivArea')","b6a7c057":"# Check for total rooms vs sale price\ndef plots_box(df, var='LotShape', target='SalePrice', size=(9,6)):\n    fig, ax = plt.subplots(figsize=size)\n    fig = sns.boxplot(x=var, y=target, data=df)\n    fig.axis(ymin=0, ymax=800000)\n    plt.show()\n    \nplots_box(df, var='TotRmsAbvGrd')","ffec87b2":"# Check for correlation on overall data\ncorr = df.corr()\nplt.figure(figsize=(15,9))\nsns.heatmap(corr, vmax=.8, square=True)","2513de3c":"cols = corr.nlargest(10, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, \n                 annot=True, square=True, \n                 fmt='.2f', annot_kws={'size': 10}, \n                 yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","13581515":"sns.set()\nvar = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageArea', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(df[var], size=2.5)\nplt.show()","6259a91b":"def plots(df, var, label='Density', fit=None):\n    fig, axes = plt.subplots(1,2, figsize=(15,6))\n    \n    sns.distplot(df[var], fit=fit, ax = axes[0])\n    axes[0].set(ylabel = label)\n    \n    stats.probplot(df[var], plot=axes[1])\n    plt.show()\n\ndf2 = df.copy()\nplots(df2, var='SalePrice', fit=norm)","29a13c3d":"# Use data transformation \ndf2['SalePrice'] = np.log(df2['SalePrice'])\nplots(df2, var='SalePrice', label='Density (log scale)', fit=norm)","bba22dff":"plots(df2, var='TotalBsmtSF', label='Density', fit=norm)","75fcaed1":"df2['TotalBsmtSF'] = np.log1p(df2['TotalBsmtSF'])\nplots(df2[df2['TotalBsmtSF'] > 0], var='TotalBsmtSF', label='Density (log scale)', fit=norm)","545159e8":"def check_missing_value(df, plot=False):\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = total \/ len(df)\n    missing_data = pd.concat([total, percent], keys=['Total', 'Percent'], axis=1)\n    missing_data = missing_data[missing_data['Total'] > 0]\n   \n    print('List missing value')\n    print('')\n    print(missing_data)\n    \n    if plot == True:\n        plt.figure(figsize=(14,6))\n        sns.barplot(x=missing_data.index, y=missing_data['Percent'])\n        plt.xticks(rotation=75)\n        plt.ylabel('Probability')\n        plt.show()\n    \n    return missing_data","7fd96e6a":"missing_data = check_missing_value(df, plot=True)","e071ab76":"from sklearn.impute import SimpleImputer\n\ndef handle_missing_value(df, missing_data, drop=False, threshold=0.5):\n    \n    df_copy = df.copy()\n    \n    # Dropping columns that contains mostly missing_data\n    # Set it to True and change the threshold if you consider it to delete the columns\n    if drop == True:\n        missing = missing_data[missing_data['Percent'] > threshold]\n        ms = [c for c in missing_data.index if c not in missing.index]\n        df_copy.drop(missing.index, axis=1,inplace=True)\n        print('Dropped index')\n        print(missing.index)\n    else:\n        ms = missing_data.index\n        \n        \n    # Get the columns\n    missing_num = [c for c in ms if df_copy[c].dtypes != \"object\"]\n    missing_obj = [c for c in ms if c not in missing_num]\n    \n    # Handle for missing value of numerical data\n    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n    df_copy[missing_num] = imputer.fit_transform(df_copy[missing_num])\n    \n    # Replace missing value of categorical data with the one that mostly used\n    for obj in missing_obj:\n        group = df_copy.groupby(obj)['Id'].count()\n        index = group.index.to_list()\n        names = index[np.argmax(group)]\n        \n        df_copy[obj] = df_copy[obj].fillna(names)\n    \n    return df_copy","06369f58":"from scipy.stats import skew\n\ndef log_transform(df):\n    \n    # Take index names for skewed > 0.75\n    num_indx = df.dtypes[df.dtypes != 'object'].index\n    skewed = df[num_indx].apply(lambda x: skew(x))\n    skewed = skewed[skewed > 0.75].index\n    \n    df[skewed] = np.log1p(df[skewed])\n    \n    return df","f9979e4f":"df_new = handle_missing_value(df, missing_data, drop=True, threshold=0.5)\ndf_new = log_transform(df_new)\ndf_ID = df_new['Id']\ndf_new.drop('Id', axis=1, inplace=True)","30bbedac":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn import metrics","15d357a2":"# Encoding the data\nXdata = df_new[df_new.columns[:-1]]\nXdata = pd.get_dummies(Xdata)\n\nydata = df_new['SalePrice']","0312fbd5":"X_train, X_test, y_train, y_test = train_test_split(Xdata, ydata, test_size=0.3, random_state=43)\nprint('Shape training data : X train {} \/ y train {}'.format(X_train.shape, y_train.shape))\nprint('Shape test data : X test {} \/ y test {}'.format(X_test.shape, y_test.shape))","e60c4f11":"def evaluate(y_true, y_pred):\n\n    # Mean absolute error\n    mae = metrics.mean_absolute_error(y_true, y_pred)\n    \n    # Root mean squared error\n    rmse = np.sqrt(metrics.mean_squared_error(y_true, y_pred))\n    \n    # R2 squared\n    r2 = metrics.r2_score(y_true, y_pred)\n    \n    # Print the results\n    \n    print('Mean Absolute Error     : {}'.format(mae))\n    print('Root Mean Squared Error : {}'.format(rmse))\n    print('R squared Error         : {}'.format(r2))\n\n    return None\n","70d658a1":"params_ridge = {'alpha' : [0.05, 0.1, 0.3, 0.7, 1.2],\n                'solver' : ['auto', 'svd', 'cholesky']}\n\nparams_lasso = {'alpha' : [1, 0.1, 0.001, 0.0005]}\n\nparams_forest = {'n_estimators' : [10, 30, 50, 100],\n                 'min_samples_leaf' : [1, 10]}\n\nmodel = [Ridge(), Lasso(), RandomForestRegressor(random_state=43)]\n\nparams = [params_ridge, params_lasso, params_forest]\n\nfor i in range(len(model)):\n    gridcv = GridSearchCV(model[i], param_grid=params[i], cv=5, n_jobs=-1)\n    gridcv.fit(X_train, y_train)\n    y_pred = gridcv.predict(X_test)\n    \n    print('')\n    print(type(model[i]).__name__)\n    print('')\n    evaluate(y_test, y_pred)\n    print('')\n    print('Best score', gridcv.best_score_)\n    print('Best params', gridcv.best_params_)","6fcad0c6":"# Use the best estimator\nlasso = Lasso(alpha=0.0005).fit(X_train, y_train)\n\ncoef = pd.Series(lasso.coef_, index=X_train.columns)\nprint(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","2b26f93e":"imp_coef = pd.concat([coef.sort_values().head(10),\n                     coef.sort_values().tail(10)])\nimp_coef.plot(kind = \"barh\", figsize=(9,9))\nplt.title(\"Coefficients in the Lasso Model\")","cde0d407":"preds = pd.DataFrame({\"preds\":lasso.predict(X_test), \"true\":y_test})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\npreds.plot(x = \"preds\", y = \"residuals\",\n           kind = \"scatter\", figsize=(8,6),\n           alpha=0.3)\nplt.ylabel('Residuals')\nplt.xlabel('Preds')\nplt.show()","a0ea7353":"import xgboost as xgb\n\ndtrain = xgb.DMatrix(X_train, label = y_train)\ndtest = xgb.DMatrix(X_test)\n\nparams = {\"max_depth\":2, \"eta\":0.1}\nmodel = xgb.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)","04cadcdc":"model.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot(figsize=(9,6))","712c6c9a":"model_xgb = xgb.XGBRegressor(n_estimators=360, \n                             max_depth=2, \n                             learning_rate=0.1)\nmodel_xgb.fit(X_train, y_train)\n\nxgb_preds = np.expm1(model_xgb.predict(X_test))\nlasso_preds = np.expm1(lasso.predict(X_test))","abb90401":"predictions = pd.DataFrame({\"xgb\":xgb_preds, \"lasso\":lasso_preds})\npredictions.plot(x = \"xgb\", \n                 y = \"lasso\", \n                 kind = \"scatter\",\n                 figsize=(9,6), alpha=0.4)\nplt.xlabel('XGB')\nplt.ylabel('Lasso')\nplt.show()","52a2c7cb":"### Missing value","a29f095e":"### Get insight from data","7f95d12a":"### Predicting"}}