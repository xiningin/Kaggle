{"cell_type":{"3ec4a212":"code","8e7bc9e7":"code","c282d18e":"code","146f45db":"code","903d4c49":"code","40777447":"code","7966a23c":"code","5d68ab7b":"code","8c4c5279":"code","9eee637e":"code","b249e6ff":"code","31073a61":"code","77eb31e9":"code","37eb6c6f":"code","d367cf68":"code","b1916f99":"code","a730769c":"code","e71868c8":"code","b708b89a":"code","9832e14c":"code","a2868b93":"markdown","595423ea":"markdown","e387b07f":"markdown","2ce51470":"markdown","5147c2f0":"markdown","f5ad0892":"markdown","52c5c5bd":"markdown","a26a8c82":"markdown","d6b980d4":"markdown","998b7b84":"markdown","1102bf8a":"markdown","b83f9e36":"markdown","108d2d11":"markdown","cac124be":"markdown","377bbf03":"markdown","9ece8586":"markdown","e783e8cc":"markdown","ef92f523":"markdown","895004bd":"markdown","c8676a5d":"markdown","d9a712ce":"markdown","62c49b9c":"markdown","1e42a65d":"markdown"},"source":{"3ec4a212":"import os\nimport math\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport albumentations as A\nimport tensorflow as tf\nfrom tensorflow.keras.applications import mobilenet_v2 as tf_mobilenet_v2\nfrom tensorflow.keras import layers as tf_layers\nfrom tensorflow.keras import models as tf_models\nfrom tensorflow.keras import callbacks as tf_callbacks\nfrom sklearn import metrics as sk_metrics\nfrom sklearn import model_selection as sk_model_selection","8e7bc9e7":"BASE_DIR = \"..\/input\/lego-minifigures-classification\/\"\nPATH_INDEX = os.path.join(BASE_DIR, \"index.csv\")\nPATH_TEST = os.path.join(BASE_DIR, \"test.csv\")\nPATH_METADATA = os.path.join(BASE_DIR, \"metadata.csv\")","c282d18e":"config = {\n    \"seed\": 42,\n    \n    \"valid_size\": 0.3,\n    \n    \"image_size\": (512, 512),\n    \"train_batch_size\": 4,\n    \"valid_batch_size\": 1,\n    \"test_batch_size\": 1,\n    \n    \"model\": \"mobilenet_v2\",\n    \"max_epochs\": 50,\n    \"patience_stop\": 3,\n    \"path_to_save_model\": \"best.hdf5\",\n    \"callbacks_monitor\": \"val_loss\",\n    \n}","146f45db":"def set_seed(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    tf.random.set_seed(seed_value)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed_value)\n    os.environ[\"TF_DETERMINISTIC_OPS\"] = \"true\"\n    \n\nset_seed(config[\"seed\"])","903d4c49":"# Read information about dataset\ndf = pd.read_csv(PATH_INDEX)\n\ntmp_train, tmp_valid = sk_model_selection.train_test_split(\n    df, \n    test_size=config[\"valid_size\"], \n    random_state=config[\"seed\"], \n    stratify=df['class_id'],\n)\n\n\ndef get_paths_and_targets(tmp_df):\n    # Get file paths\n    paths = tmp_df[\"path\"].values\n    # Create full paths (base dir + concrete file name)\n    paths = list(map(lambda x: os.path.join(BASE_DIR, x), paths))\n    # Get labels\n    targets = tmp_df[\"class_id\"].values\n    \n    return paths, targets\n\n\n# Get train file paths and targets\ntrain_paths, train_targets = get_paths_and_targets(tmp_train)\n\n# Get valid file paths and targets\nvalid_paths, valid_targets = get_paths_and_targets(tmp_valid)\n\ndf_test = pd.read_csv(PATH_TEST)\n# Get test file paths and targets\ntest_paths, test_targets = get_paths_and_targets(df_test)","40777447":"# Total number of classes in the dataset\ndf_metadata = pd.read_csv(PATH_METADATA)\nn_classes = df_metadata.shape[0]\nprint(\"Number of classes: \", n_classes)","7966a23c":"class DataGenerator(tf.keras.utils.Sequence):\n    def __init__(\n        self, paths, targets, image_size=(224, 224), batch_size=64, \n        shuffle=True, transforms=None, preprocess=None,\n    ):\n        # the list of paths to files\n        self.paths = paths\n        # the list with the true labels of each file\n        self.targets = targets\n        # images size\n        self.image_size = image_size\n        # batch size (the number of images)\n        self.batch_size = batch_size\n        # if we need to shuffle order of files\n        # for validation we don't need to shuffle, for training - do\n        self.shuffle = shuffle\n        # Augmentations for our images. It is implemented with albumentations library\n        self.transforms = transforms\n        # Preprocess function for the pretrained model. \n        # CHANGE IT IF USING OTHER THAN MOBILENETV2 MODEL\n        self.preprocess = preprocess\n        \n        # Call function to create and shuffle (if needed) indices of files\n        self.on_epoch_end()\n        \n    def on_epoch_end(self):\n        # This function is called at the end of each epoch while training\n        \n        # Create as many indices as many files we have\n        self.indexes = np.arange(len(self.paths))\n        # Shuffle them if needed\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n            \n    def __len__(self):\n        # We need that this function returns the number of steps in one epoch\n        \n        # How many batches we have\n        return len(self.paths) \/\/ self.batch_size\n    \n    \n    def __getitem__(self, index):\n        # This function returns batch of pictures with their labels\n        \n        # Take in order as many indices as our batch size is\n        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n        \n        # Take image file paths that are included in that batch\n        batch_paths = [self.paths[k] for k in indexes]\n        # Take labels for each image\n        batch_y = [self.targets[k] - 1 for k in indexes]\n        batch_X = []\n        for i in range(self.batch_size):\n            # Read the image\n            img = cv2.imread(batch_paths[i])\n            # Resize it to needed shape\n            img = cv2.resize(img, self.image_size)\n            # Convert image colors from BGR to RGB\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            # Apply transforms (see albumentations library)\n            if self.transforms:\n                img = self.transforms(image=img)[\"image\"]\n            # Apply preprocess\n            if self.preprocess:\n                img = self.preprocess(img)\n            \n            batch_X.append(img)\n            \n        return np.array(batch_X), np.array(batch_y)\n","5d68ab7b":"def get_train_transforms():\n    return A.Compose(\n        [\n            A.ShiftScaleRotate(\n                p=1.0, \n                shift_limit=(-0.1, 0.1), \n                scale_limit=(-0.2, 0.2), \n                rotate_limit=(-30, 30), \n                border_mode=4\n            ),\n            A.CoarseDropout(\n                p=0.5, \n                max_holes=100, \n                max_height=50, \n                max_width=50, \n                min_holes=10, \n                min_height=10, \n                min_width=10,\n                fill_value=0,\n            ),\n            A.CoarseDropout(\n                p=0.5, \n                max_holes=100, \n                max_height=50, \n                max_width=50, \n                min_holes=10, \n                min_height=10, \n                min_width=10,\n                fill_value=255,\n            ),\n            A.HorizontalFlip(p=0.5),\n            A.RandomContrast(limit=(-0.3, 0.3), p=0.5),\n            A.RandomBrightness(limit=(-0.4, 0.4), p=0.5),\n            A.Blur(p=0.25),\n        ], \n        p=1.0\n    )","8c4c5279":"# Initialize the train data generator\ntrain_generator = DataGenerator(\n    train_paths, \n    train_targets, \n    batch_size=config[\"train_batch_size\"], \n    image_size=config[\"image_size\"],\n    shuffle=True, \n    transforms=get_train_transforms(),\n    preprocess=tf_mobilenet_v2.preprocess_input,\n)\n\n# Initialize the valid data generator\nvalid_generator = DataGenerator(\n    valid_paths, \n    valid_targets, \n    image_size=config[\"image_size\"],\n    batch_size=config[\"valid_batch_size\"], \n    shuffle=False,\n    preprocess=tf_mobilenet_v2.preprocess_input,\n)\n\n# Initialize the test data generator\ntest_generator = DataGenerator(\n    test_paths, \n    test_targets, \n    image_size=config[\"image_size\"],\n    batch_size=config[\"test_batch_size\"], \n    shuffle=False,\n    preprocess=tf_mobilenet_v2.preprocess_input,\n)","9eee637e":"def denormalize_image(image):\n    return ((image + 1) * 127.5).astype(int)\n\n# Let's visualize some batches of the train data\nplt.figure(figsize=(16, 16))\nind = 0\nfor i_batch in range(len(train_generator)):\n    images, labels = train_generator[i_batch]\n    for i in range(len(images)):\n        plt.subplot(5, 5, ind + 1)\n        ind += 1\n        plt.imshow(denormalize_image(images[i]))\n        plt.title(f\"class: {labels[i]}\")\n        plt.axis(\"off\")\n        if ind >= 25:\n            break\n    if ind >= 25:\n        break","b249e6ff":"plt.figure(figsize=(16, 16))\nind = 0\nfor i_batch in range(len(valid_generator)):\n    images, labels = valid_generator[i_batch]\n    for i in range(len(images)):\n        plt.subplot(5, 5, ind + 1)\n        ind += 1\n        plt.imshow(denormalize_image(images[i]))\n        plt.title(f\"class: {labels[i]}\")\n        plt.axis(\"off\")\n        if ind >= 25:\n            break\n    if ind >= 25:\n        break","31073a61":"def init_mobilenet_v2(n_classes):\n    # We take pretrained MobileNetV2 (see Keras docs)\n    base_model = tf_mobilenet_v2.MobileNetV2()\n    x = base_model.layers[-2].output\n    # Take penultimate layer of the MobileNetV2 model and connect this layer with Dropout\n    x = tf_layers.Dropout(.5)(x)\n    # Add additional Dense layer, with number of neurons as number of our classes\n    # Use softmax activation because we have one class classification problem\n    outputs = tf_layers.Dense(n_classes, activation=\"softmax\")(x)\n    # Create model using MobileNetV2 input and our created output\n    model = tf_models.Model(base_model.inputs, outputs)\n    \n    return model\n\n\ndef compile_model(model, optimizer, loss, metrics):\n    # Compile model using Adam optimizer and categorical crossentropy loss\n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n    return model\n\n\nmodels_constructor = {\n    \"mobilenet_v2\": init_mobilenet_v2,\n}","77eb31e9":"# checkpoint to saving the best model by validation loss\ncallback_save = tf_callbacks.ModelCheckpoint(\n    config[\"path_to_save_model\"],\n    monitor=config[\"callbacks_monitor\"],\n    save_best_only=True,\n)\n\n# checkpoint to stop training if model didn't improve valid loss for 3 epochs\ncallback_early_stopping = tf_callbacks.EarlyStopping(\n    monitor=config[\"callbacks_monitor\"],\n    patience=config[\"patience_stop\"],\n)","37eb6c6f":"model = models_constructor[config[\"model\"]](n_classes)\ncompile_model(\n    model,\n    tf.keras.optimizers.Adam(0.0001), \n    \"sparse_categorical_crossentropy\", \n    [\"accuracy\"],\n)\n\n# Train model using data generators\nhistory = model.fit(\n    train_generator,\n    validation_data=valid_generator,\n    epochs=config[\"max_epochs\"],\n    callbacks=[callback_save, callback_early_stopping],\n    verbose=1,\n)","d367cf68":"def plot_history(param=\"loss\"):\n    plt.figure(figsize=(6, 6))\n    if param == \"loss\":\n        plt.plot(history.history[\"loss\"], label=\"train loss\")\n        plt.plot(history.history[\"val_loss\"], label=\"valid loss\")\n        plt.ylabel(\"Loss value\", fontsize=15)\n    elif param == \"accuracy\":\n        plt.plot(history.history[\"accuracy\"], label=\"train acc\")\n        plt.plot(history.history[\"val_accuracy\"], label=\"valid acc\")\n        plt.ylim(0, 1)\n        plt.ylabel(\"Accuracy score\", fontsize=15)\n    plt.xticks(fontsize=14)\n    plt.xlabel(\"Epoch number\", fontsize=15)\n    plt.yticks(fontsize=14)\n    plt.legend(fontsize=15)\n    plt.grid()\n    plt.show()\n\n\n# Visualize train and valid loss \nplot_history(\"loss\")\n# Visualize train and valid accyracy \nplot_history(\"accuracy\")","b1916f99":"# Load the best model (we create for checkpoint to save the best model)\nmodel = tf_models.load_model(config[\"path_to_save_model\"])","a730769c":"# Save the model predictions and true labels\ny_pred = []\ny_test = []\nfor _X_test, _y_test in test_generator:\n    y_pred.extend(model.predict(_X_test).argmax(axis=-1))\n    y_test.extend(_y_test)\n\n# Calculate needed metrics\nprint(f\"Accuracy score on test data: {sk_metrics.accuracy_score(y_test, y_pred)}\")\nprint(f\"Macro F1 score on test data: {sk_metrics.f1_score(y_test, y_pred, average='macro')}\")","e71868c8":"# Load metadata to get classes people-friendly names\nlabels = df_metadata[\"minifigure_name\"].tolist()\n\n# Calculate confusion matrix\nconfusion_matrix = sk_metrics.confusion_matrix(y_test, y_pred)\ndf_confusion_matrix = pd.DataFrame(confusion_matrix, index=labels, columns=labels)\n\n# Show confusion matrix\nplt.figure(figsize=(12, 12))\nsn.heatmap(df_confusion_matrix, annot=True, cbar=False, cmap=\"Oranges\", linewidths=1, linecolor=\"black\")\nplt.xlabel(\"Predicted labels\", fontsize=15)\nplt.xticks(fontsize=12)\nplt.ylabel(\"True labels\", fontsize=15)\nplt.yticks(fontsize=12);","b708b89a":"# Save image, label, prediction for false predictions \nerror_images = []\nerror_label = []\nerror_pred = []\nerror_prob = []\nfor _X_test, _y_test in test_generator:\n    pred = model.predict(_X_test).argmax(axis=-1)\n    if pred[0] != _y_test:\n        error_images.extend(_X_test)\n        error_label.extend(_y_test)\n        error_pred.extend(pred)\n        error_prob.extend(model.predict(_X_test).max(axis=-1))","9832e14c":"# Visualize missclassified samples\nw_size = 3\nh_size = math.ceil(len(error_images) \/ w_size)\nplt.figure(figsize=(16, h_size * 4))\nfor ind, image in enumerate(error_images):\n    plt.subplot(h_size, w_size, ind + 1)\n    plt.imshow(denormalize_image(image))\n    pred_label = labels[error_pred[ind]]\n    pred_prob = error_prob[ind]\n    true_label = labels[error_label[ind]]\n    plt.title(f\"predict: {pred_label} ({pred_prob:.2f})\\ntrue: {true_label}\", fontsize=12)\n    plt.axis(\"off\")","a2868b93":"<a id=\"2\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>Data reading<center><h2>","595423ea":"<a id=\"3\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>Data generator<center><h2>","e387b07f":"The directory to the dataset","2ce51470":"<a id=\"10\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>Model training<center><h2>","5147c2f0":"DataGenerator allows you not to load the entire dataset to memory at once, but to do it in batches     \nEach time we have only one batch of pictures in memory","f5ad0892":"![](https:\/\/i.imgur.com\/4cPQlEN.jpg)","52c5c5bd":"<a id=\"13\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>Error analysis - Confusion matrix<center><h2>","a26a8c82":"Albumentations augmentations for the train data.       \nWe don't need this transformations for the validation.   \n[albumentations-demo](https:\/\/albumentations-demo.herokuapp.com\/) ","d6b980d4":"<a id=\"7\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>Data visualizations (valid samples)<center><h2>","998b7b84":"# TensorFlow Modeling for [LEGO Minifigures Classification](https:\/\/www.kaggle.com\/ihelon\/lego-minifigures-classification) dataset\n\nThis is the guide about using pre-trained models in TensorFlow and Keras frameworks.   \nWe will use the MobileNetV2 model to predict which Minifigure is in the image.   ","1102bf8a":"<a id=\"5\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>Train and valid generators<center><h2>","b83f9e36":"<a id=\"6\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>Data visualizations (train samples)<center><h2>","108d2d11":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:blue; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Quick Navigation<\/center><\/h3>\n\n* [1. Configurations](#1)\n* [2. Data reading](#2)\n* [3. Data generator](#3)\n* [4. Augmentations](#4)\n* [5. Train and valid generators](#5)\n* [6. Data visualizations (train samples)](#6)   \n* [7. Data visualizations (valid samples)](#7)   \n* [8. Model initialization](#8)   \n* [9. Checkpoints initialization](#9)\n* [10. Model training](#10)\n* [11. Train logs](#11)\n* [12. Final test score check](#12)\n* [13. Error analysis - Confusion matrix](#13)\n* [14. Error analysis - Misclassified samples](#14)","cac124be":"<a id=\"8\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>Model initialization<center><h2>","377bbf03":"<a id=\"1\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>Configurations<center><h2>","9ece8586":"<a id=\"4\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>Augmentations<center><h2>","e783e8cc":"Try to set random seet that our experiment repeated between (We have some problem to set seed with GPU in Kaggle)","ef92f523":"<a id=\"9\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>Checkpoints initialization<center><h2>","895004bd":"<a id=\"12\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>Final test check<center><h2>","c8676a5d":"<a id=\"11\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>Train logs<center><h2>","d9a712ce":"<a id=\"14\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>Error analysis - Misclassified samples<center><h2>","62c49b9c":"We use simple MobileNetV2 model as a backbone","1e42a65d":"Let's define all out configs and parameters in the one place"}}