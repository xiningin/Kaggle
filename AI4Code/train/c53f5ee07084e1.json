{"cell_type":{"f1f801ef":"code","a1b3dab7":"code","62417ba5":"code","02ad9d4b":"code","5626c1fa":"code","ad60c42b":"code","ab6cfb2a":"code","c02a080b":"code","17627e47":"code","0399e015":"code","93ec79ab":"code","8b8350d8":"code","cb55e5dd":"code","6b71b687":"code","ea720788":"code","49880040":"code","555fd5a7":"code","12fd400e":"code","4caab61e":"code","459d592c":"code","3f2ac274":"code","3e18b8af":"code","20fc7c7c":"code","4d380de7":"code","056a8a02":"code","196ccf87":"code","de2cfd7d":"code","1462b2b0":"code","3e008e29":"code","60a06264":"code","1f5586d6":"code","629ba7a8":"code","6844851a":"code","04042a9f":"code","5bdbdaee":"code","8662a594":"code","be00cb4d":"code","3f70b246":"code","6ef2574d":"code","e60ea046":"code","06de392d":"code","ed5a26d6":"code","b4176cf0":"code","2a6f2808":"code","b4eecc90":"code","05dace9f":"code","094c2d5d":"code","7890bbd0":"code","d32c3c31":"code","5cbb725f":"code","213dedaa":"code","95a3cdb3":"code","7ad8939e":"code","3be0758d":"code","03d43a9a":"code","df4f30b9":"code","527ccbea":"code","0eafd9e4":"code","bcaa1045":"code","04d733c5":"code","5f2ad5ee":"code","cf8bdf28":"code","26f0039b":"code","3364e9f0":"code","e91659f5":"code","d06ff066":"code","6910e671":"code","f2073cdb":"code","788187ab":"code","37f4f953":"code","479f8e43":"markdown","e0f798da":"markdown","3aea34d6":"markdown","9d9dcff1":"markdown","f87b7a17":"markdown","a5fca063":"markdown","0d19a421":"markdown","5346f31e":"markdown","0d1a93e6":"markdown","036ba6e4":"markdown","3be546db":"markdown","8fade791":"markdown","fdf8a721":"markdown","6ca0e776":"markdown","bd5793ff":"markdown","8ec5f0ec":"markdown","be761541":"markdown","2d45009d":"markdown","8c96fc28":"markdown","ac2b2302":"markdown","74aa4cc7":"markdown","ebcf4f29":"markdown","21ffe06c":"markdown","6f7cc2af":"markdown","5c11d3d8":"markdown","b57c9b20":"markdown","a5dd3c33":"markdown","7879b8c5":"markdown","164f0c34":"markdown","632a64b5":"markdown","8f00f3c1":"markdown","92e48d23":"markdown","e3b8b95c":"markdown","b9683866":"markdown","62c936c3":"markdown","c849e6aa":"markdown","f9f4ba34":"markdown","2e512edc":"markdown","21d6c39e":"markdown","f4026b31":"markdown","910b0893":"markdown","9d49c33c":"markdown","64eaf0e7":"markdown"},"source":{"f1f801ef":"import pandas as pd\nimport seaborn as sns\nsns.set(style = \"darkgrid\")\nimport matplotlib.pyplot as plt\npd.set_option(\"display.max_row\", 304)\npd.set_option(\"display.max_column\", 14)","a1b3dab7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","62417ba5":"df = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")","02ad9d4b":"df.head()","5626c1fa":"data = df.copy()","ad60c42b":"data.columns","ab6cfb2a":"data['target'].value_counts(normalize = True)","c02a080b":"plt.figure(facecolor = \"white\")\ndata['target'].value_counts().plot.pie(labels = [\"risque\", \"non risque\"])","17627e47":"data.shape","0399e015":"data.dtypes","93ec79ab":"variables_continues = 'oldpeak'\nvariables_discretes = ['age', 'trestbps', 'chol', 'thalach']\nvariables_categorielles = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']","8b8350d8":"sns.heatmap(data.isna())","cb55e5dd":"plt.figure(figsize=(14, 15))\nsns.histplot(x = 'oldpeak', hue = 'target', data = data, kde = True)","6b71b687":"for column in variables_discretes:\n    plt.figure(figsize=(14, 15))\n    sns.histplot(x = column, hue = 'target', data = data, kde = True)","ea720788":"for column in variables_categorielles:\n    print(f'{column}:\\n{data[column].value_counts(normalize=True)}')","49880040":"for column in variables_categorielles:\n    print(pd.crosstab(data['target'], data[column]))","555fd5a7":"for column in variables_categorielles:\n    plt.figure(figsize=(12, 12))\n    sns.heatmap(pd.crosstab(data['target'], data[column]), annot=True, fmt = 'd')","12fd400e":"for column in variables_discretes:\n    print(\"correlation entre variable oldpeak et variable {} :\\n {}\".format(column, data[[\"oldpeak\", column]].corr()))","4caab61e":"sns.histplot(x = \"age\", hue = \"exang\", data= data, kde = True)","459d592c":"sns.histplot(x = \"oldpeak\", hue = \"exang\", data= data, kde = True)","3f2ac274":"for column in variables_discretes:\n    plt.figure()\n    sns.histplot(x = column, hue = \"exang\", data= data, kde = True)\n    ","3e18b8af":"variables_non_categorielles = variables_discretes + [\"oldpeak\"]","20fc7c7c":"for column in variables_non_categorielles:\n    plt.figure()\n    sns.histplot(x = column, hue = \"cp\", data= data, kde = True)","4d380de7":"for column in variables_non_categorielles:\n    plt.figure()\n    sns.histplot(x = column, hue = \"sex\", data= data, kde = True)","056a8a02":"for column in variables_non_categorielles:\n    plt.figure()\n    sns.histplot(x = column, hue = \"fbs\", data= data, kde = True)","196ccf87":"for column in variables_non_categorielles:\n    plt.figure()\n    sns.histplot(x = column, hue = \"restecg\", data= data, kde = True)","de2cfd7d":"for column in variables_non_categorielles:\n    plt.figure()\n    sns.histplot(x = column, hue = \"slope\", data= data, kde = True)","1462b2b0":"for column in variables_non_categorielles:\n    plt.figure()\n    sns.histplot(x = column, hue = \"ca\", data= data, kde = True)","3e008e29":"for column in variables_non_categorielles:\n    plt.figure()\n    sns.histplot(x = column, hue = \"thal\", data= data, kde = True)","60a06264":"sns.pairplot(data[variables_discretes])","1f5586d6":"sns.clustermap(data[variables_discretes].corr())","629ba7a8":"for column in variables_discretes+[\"oldpeak\"]:\n    plt.figure()\n    sns.boxplot(x = \"target\", y = column, data= data)\n    print(f\"for column {column}\")\n    print(\"limitations values upper {}\".format(data[column].quantile(0.75)+1.5*(data[column].quantile(0.75) - data[column].quantile(0.25))))\n    print(\"limitations values lesser {}\".format(data[column].quantile(0.25)-1.5*(data[column].quantile(0.75) - data[column].quantile(0.25))))","6844851a":"class ClassificationDataframeTraitements(object):\n    \"\"\"Classe concue pour faire des traitements sur un DataFrame\n    \"\"\"\n    def __init__(self, dataframe):\n        self.__dataframe = dataframe.copy()\n        self.__target = None\n    \n    @property\n    def dataframe(self):\n        return self.__dataframe\n\n    @dataframe.setter\n    def dataframe(self, df):\n        self.__dataframe = df\n    @property \n    def target(self):\n        return self.__target\n    \n    @target.setter\n    def target(self, target_name):\n        \"\"\"Fonction qui permet de definir le target du dataframe\n        Args:\n            target_name(str): Le nom du target\n        \"\"\"\n        self.__dataframe['target'] = self.__dataframe[target_name]\n        if target_name != \"target\":\n            self.__dataframe.drop(columns = target_name, axis = 1, inplace = True)\n        self.__target = self.__dataframe['target']\n        \n    def get_limitations(self, colonne, dataframe = None):\n        \"\"\"Cette fonction permet de calculer les limites au dessus ou en dessous des quelles\n        les donn\u00e9es sont consid\u00e9r\u00e9es comme aberrantes.\n        Args :\n            colonne(str): Le nom de la colonne dont on souhaite calculer les limitations\n        Returns :\n            limits(tuple): Les diff\u00e9rentes limitations\n        \"\"\"\n        if dataframe is None:\n            dataframe = self.dataframe\n        if not colonne in dataframe.columns:\n            raise ValueError(f\"La colonne {colonne} ne correspond \u00e0 aucune colonne du dataframe\")\n        if self.target is None:\n            raise ValueError(f\"Le target n'est pas encore d\u00e9fini\")\n        limitations = []\n        for category in self.target.unique():\n            colonne_df = dataframe[dataframe['target'] == category][colonne]\n            quantile_1 = colonne_df.quantile(0.25)\n            quantile_3 = colonne_df.quantile(0.75)\n            ecart_quantile = quantile_3 - quantile_1\n            limitation1 = quantile_1 - 1.5 * ecart_quantile\n            limitation2 = quantile_3 + 1.5 * ecart_quantile\n            limitations.append((limitation1, limitation2))\n        return limitations\n            \n    def sep_according_to_column(self, colonne):\n        \"\"\"Fonction qui permet de separer les donnees selon les categories du target\n        \"\"\"\n        if not colonne in self.dataframe.columns:\n            raise ValueError(f\"La colonne {colonne} ne correspond \u00e0 aucune colonne du dataframe\")\n        dataframes = []\n        for category in self.dataframe[colonne].unique():\n            dataframes.append(self.dataframe[self.dataframe[colonne] == category])\n        return dataframes\n    \n    def fisher_test(self, dataframe1, dataframe2, colonnes, alpha):\n        \"\"\"Fonction qui fait permet de faire le test d'hypothese sur des colonnes\n        du dataset\n        \"\"\"\n        if dataframe1.shape[0] <= dataframe2.shape[0]:\n            dataframe2 = dataframe2.sample(dataframe1.shape[0])\n        else:\n            dataframe1 = dataframe1.sample(dataframe2.shape[0])\n            \n        from scipy.stats import ttest_ind\n        for colonne in colonnes:\n            stat, p = ttest_ind(dataframe1[colonne], dataframe2[colonne])\n            if  p < alpha:\n                print(f\"{colonne:-<30} : H0 rejet\u00e9e\")\n            else:\n                print(f\"{colonne:-<30} : H0 non rejet\u00e9e\")\n            \n    # Les m\u00e9thodes de preprocessing de base\n    def split_dataset(self):\n        \"\"\"Fonction qui divise le dataset en trainset et testset\"\"\"\n        from sklearn.model_selection import train_test_split\n        train_set, test_set =  train_test_split(self.dataframe, test_size = 0.2, random_state = 0)\n        return train_set, test_set\n    \n    def encodage(self, dataframe):\n        \"\"\"Encode les donn\u00e9es categorielles\"\"\"\n        for column in dataframe.columns:\n            if dataframe[column].dtype == \"object\":\n                dataframe[column] = dataframe[column].astype(\"category\").cat.codes\n        return dataframe\n    \n    def delete_datas_depassing_limitations(self, dataframe, colonnes, target_name):\n        \"\"\"Fonction qui permet d'eliminer les donn\u00e9es aberrantes sur une liste de colonnes\"\"\"\n        for colonne in colonnes:\n            limitations = self.get_limitations(colonne, dataframe)\n            dataframe1 = dataframe[(dataframe[target_name] == 1) & (dataframe[colonne] > limitations[0][0]) & (dataframe[colonne] < limitations[0][1])]\n            dataframe2 = dataframe[(dataframe[target_name] == 0) & (dataframe[colonne] > limitations[1][0]) & (dataframe[colonne] < limitations[1][1])]\n            dataframe = pd.concat((dataframe1, dataframe2), axis = 0)\n        return dataframe\n        \n    def nettoyage(self, dataframe, colonnes, target_name):\n        \"\"\"Fonction qui nettoire les donnees manquantes\"\"\"\n        if dataframe.isna().sum().sum() != 0:\n            dataframe.dropna(axis = 0, inplace = True)\n        dataframe = self.delete_datas_depassing_limitations(dataframe, colonnes, target_name)\n        return dataframe\n    \n    def drop_columns(self, dataframe, columns):\n        \"\"\"Fonction qui permet de supprimer des colonnes\"\"\"\n        dataframe.drop(columns = columns, axis = 1, inplace = True)\n        return dataframe\n    \n    def select_features(self, model, X_train, y_train):\n        \"\"\"Fonction permettant de choisir les variables les plus performants\"\"\"\n        from sklearn.feature_selection import SelectFromModel\n        selector = SelectFromModel(model)\n        selector.fit(X_train, y_train)\n        print(f\"supports :\\n {selector.get_support()}\")\n        return selector\n    \n    def preprocessing(self, dataframe, target_name, columns = []):\n        \"\"\"Fonction de preprocessing\"\"\"\n        dataframe = self.nettoyage(dataframe, [\"trestbps\"], target_name)\n        if columns != []:\n            self.drop_columns(dataframe, columns)\n        if \"object\" in data.dtypes.tolist():\n            dataframe = self.encodage(dataframe)\n        y = dataframe[target_name]\n        X = dataframe.drop(columns=target_name)\n        return X, y\n    \n    def get_simple_model(self, X_train, y_train):\n        \"\"\"Fonction qui effectue un premier entrainement\"\"\"\n        from sklearn.tree import DecisionTreeClassifier\n        from sklearn.svm import SVC\n        model = DecisionTreeClassifier()\n        model.fit(X_train, y_train)\n        return model\n    \n    def multiple_model(self, X_train, y_train, X_test, y_test):\n        \"\"\"Fonction qui effectue plusieurs entrainement a partir de plusieurs modeles\"\"\"\n        from sklearn.pipeline import make_pipeline\n        from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n        from sklearn.svm import SVC\n        from sklearn.neighbors import KNeighborsClassifier\n        from sklearn.preprocessing import RobustScaler\n        from sklearn.linear_model import LogisticRegression\n        \n        Forest = make_pipeline(RobustScaler(), RandomForestClassifier(random_state = 0))\n        Adaboost = make_pipeline(RobustScaler(), AdaBoostClassifier(random_state = 0))\n        Svc = make_pipeline(RobustScaler(), SVC(random_state = 0))\n        KNeighbors = make_pipeline(RobustScaler(), KNeighborsClassifier())\n        Logistic = make_pipeline(RobustScaler(), LogisticRegression())\n        \n        models_dict = {\"Forest\" : Forest,\n                       \"Adaboost\" : Adaboost,\n                       \"SVC\" : Svc,\n                       \"KNN\" : KNeighbors,\n                       \"Logistic\" : Logistic\n                      }\n        \n        for key, value in models_dict.items():\n            print(f\"{'-'*15}\\nFor Model {key} :\")\n            value.fit(X_train, y_train)\n            self.evaluation(value, key, X_test, y_test, X_train, y_train)\n        return models_dict\n        \n        \n    def evaluation(self, model, model_name, X_test, y_test, X_train, y_train):\n        \"\"\"Fonction d'evaluation de base\"\"\"\n        from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n        from sklearn.model_selection import learning_curve\n        import numpy as np\n        y_pred = model.predict(X_test)\n        print(f\"La precision du model : {accuracy_score(y_test, y_pred)}\")\n        print(f\"Confusion matrix : \\n{confusion_matrix(y_test, y_pred)}\")\n        print(f\"Classification_report : \\n{classification_report(y_test, y_pred)}\")\n        N, train_score, test_score = learning_curve(model, X_train, y_train, cv = 5, train_sizes = np.linspace(0.1, 1, 10))\n        plt.figure(figsize = (13, 14))\n        plt.plot(N, train_score.mean(axis = 1), label = \"train_score\")\n        plt.plot(N, test_score.mean(axis = 1), label = \"test_score\")\n        plt.title(f\"Evaluation model {model_name}\")\n        plt.legend()\n        ","04042a9f":"# Verifions les resultats obtenus avec la colonne thalach\ntraitements = ClassificationDataframeTraitements(data)\ntraitements.target = \"target\"\ntraitements.get_limitations(\"thalach\")","5bdbdaee":"for column in variables_discretes+[\"oldpeak\"]:\n    plt.figure(figsize=(12, 13), facecolor=\"white\")\n    limitations = traitements.get_limitations(column)\n    limitation1 = limitations[0]\n    limitation2 = limitations[1]\n    sns.boxplot(x = \"target\", y = column, data= data)\n    plt.title(f\"Boxplot pour la colonne {column}\", fontweight = 40, fontsize = 15)\n    plt.hlines(y = limitation1, xmin = -0.5, xmax=1.5, colors=[\"orange\"])\n    plt.hlines(y = limitation2, xmin = -0.5, xmax=1.5, colors=[\"blue\"])\n    ","8662a594":"dataframe_no_abnorm = traitements.delete_datas_depassing_limitations(data, variables_discretes+[\"oldpeak\"], 'target')","be00cb4d":"for column in variables_discretes+[\"oldpeak\"]:\n    plt.figure(figsize=(12, 13), facecolor=\"white\")\n    limitations = traitements.get_limitations(column)\n    limitation1 = limitations[0]\n    limitation2 = limitations[1]\n    sns.boxplot(x = \"target\", y = column, data= dataframe_no_abnorm)\n    plt.title(f\"Boxplot pour la colonne {column}\", fontweight = 40, fontsize = 15)\n    plt.hlines(y = limitation1, xmin = -0.5, xmax=1.5, colors=[\"orange\"])\n    plt.hlines(y = limitation2, xmin = -0.5, xmax=1.5, colors=[\"blue\"])","3f70b246":"df_risque, df_non_risque = traitements.sep_according_to_column(\"target\")","6ef2574d":"traitements.fisher_test(df_risque, df_non_risque, variables_non_categorielles, 0.02)","e60ea046":"df_ang, df_non_ang = traitements.sep_according_to_column(\"exang\")","06de392d":"traitements.fisher_test(df_ang, df_non_ang, variables_non_categorielles, 0.02)","ed5a26d6":"df_2_slope, df_1_slope, df_no_slope = traitements.sep_according_to_column('slope')","b4176cf0":"traitements.fisher_test(df_1_slope, df_no_slope, variables_non_categorielles, 0.02)","2a6f2808":"traitements.fisher_test(df_2_slope, df_no_slope, variables_non_categorielles, 0.02) ","b4eecc90":"traitements.fisher_test(df_1_slope, df_2_slope, variables_non_categorielles, 0.02)","05dace9f":"df_2_restecg, df_1_restecg, df_no_restecg = traitements.sep_according_to_column('restecg')","094c2d5d":"traitements.fisher_test(df_1_restecg, df_no_restecg, variables_non_categorielles, 0.02)","7890bbd0":"traitements.fisher_test(df_2_restecg, df_no_restecg, variables_non_categorielles, 0.02) ","d32c3c31":"traitements.fisher_test(df_1_restecg, df_2_restecg, variables_non_categorielles, 0.02)","5cbb725f":"train_set, test_set = traitements.split_dataset()","213dedaa":"train_set.shape","95a3cdb3":"test_set.shape","7ad8939e":"X_train, y_train = traitements.preprocessing(train_set, \"target\", ['sex', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thalach', 'age', 'trestbps'])","3be0758d":"X_test, y_test = traitements.preprocessing(test_set, \"target\", ['sex', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thalach', 'age', 'trestbps'])","03d43a9a":"# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.svm import SVC\n# selector = traitements.select_features(DecisionTreeClassifier(), X_train, y_train)","df4f30b9":"# data.columns.drop(['sex', 'fbs', 'restecg', 'exang', 'slope', 'target'])[selector.get_support()]","527ccbea":"# X_train = selector.transform(X_train)\n# X_train.shape","0eafd9e4":"# X_test = selector.transform(X_test)\n# X_test.shape","bcaa1045":"model = traitements.get_simple_model(X_train, y_train)","04d733c5":"traitements.evaluation(model, \"DecisionTree\", X_test, y_test, X_train, y_train)","5f2ad5ee":"models_dict = traitements.multiple_model(X_train, y_train, X_test, y_test)","cf8bdf28":"traitements.evaluation(models_dict[\"Logistic\"], \"Logistic\", X_test, y_test, X_train, y_train)","26f0039b":"import numpy as np\nfrom sklearn.model_selection import GridSearchCV\nmodel = models_dict[\"Logistic\"]","3364e9f0":"model.get_params()","e91659f5":"hyper_params = {\n    \"logisticregression__max_iter\" : np.arange(95, 110),\n    \"logisticregression__penalty\" : ['l2', 'none']\n}","d06ff066":"grid_model = GridSearchCV(model, param_grid=hyper_params, scoring=\"accuracy\", cv = 5)","6910e671":"grid_model.fit(X_train, y_train)","f2073cdb":"grid_model.best_params_","788187ab":"grid_model.best_score_","37f4f953":"traitements.evaluation(grid_model.best_estimator_, \"Logistic\", X_test, y_test, X_train, y_train)","479f8e43":"### Variables discretes\/discretes","e0f798da":"## Analyses plus pouss\u00e9es ","3aea34d6":"#### Pour la variable ca","9d9dcff1":"## Relation target\/variables discr\u00e8tes","f87b7a17":"### Analyse plus pouss\u00e9e des valeurs aberrantes","a5fca063":"### Relation variables non categorielles \/ autres variables categorielles ","0d19a421":"### Optimisation du model LogisticRegression : Utilisation de GridSearchCV ","5346f31e":"#### exang\/variables","0d1a93e6":"### Test d'hypothese (H0)","036ba6e4":"## Idenfication de la target","3be546db":"### Relation autres variables discretes\/exang","8fade791":"Premier test pour la variable restecg : 0\/1","fdf8a721":"### Relation oldpeak\/variables discretes","6ca0e776":"#### Pour la variable sex : Autre analyse","bd5793ff":"Deuxi\u00e8me test pour variable slope : 0\/2","8ec5f0ec":"## Relation target\/variables categorielles","be761541":"### Preprocessing de base","2d45009d":"#### Pour la variable restecg","8c96fc28":"## Objectifs :\n- Quel est l'objectif \u00e0 atteindre : pr\u00e9dire si une personne est confront\u00e9e a un risque d'arret cardiaque en fonction des tests cliniques\n- Metrics : Le metrique Accuracy est valable dans ce cas de figure car le taux du risque d'arret cardiaque et le taux du non risque d'arret cardiaque sont presque les memes. Le but sera d'atteindre un score compris entre 80 et 90 %","ac2b2302":"### Separation du dataset en train set et test set","74aa4cc7":"#### slope\/variables","ebcf4f29":"### Relation age\/exang","21ffe06c":"Troisi\u00e8me test pour la variable slope : 1\/2\n","6f7cc2af":"### Relation oldpeak\/exang","5c11d3d8":"#### Pour la variable cp","b57c9b20":"#### Pour la variable thal","a5dd3c33":"Premier test pour la variable restecg : 0\/2\n","7879b8c5":"Troisi\u00e8me test pour la variable slope : 1\/2","164f0c34":"# Heart Attack predictions","632a64b5":"## Model optimisation : Utilisation du pipeline\n### Conclusion : Le LogisticRegression model donne de meilleures performances en diminuant consid\u00e9rablement l'overfitting tout en gardant un bon score","8f00f3c1":"#### Pour la variable slope","92e48d23":"#### Target\/variables","e3b8b95c":"### Donn\u00e9es abberantes","b9683866":"## Exploratory and Data Analysis :\n### Analyse de forme :\n- Identification de la target : Le target est d\u00e9j\u00e0 identifi\u00e9. Il est compos\u00e9 de deux valeurs 0 pour non pr\u00e9sence et 1 pour pr\u00e9sence de risque. Le target est presque \u00e9quilibr\u00e9 avec 46 % de non risque d'arret cardiaque et 54 % de risque d'arret cardiaque\n- Nombre de lignes et de colonnes : Nous avons 14 variables dans notre dataset et 303 patients diagnostiqu\u00e9s. Le target est le 14 i\u00e8me variable\n- Types de variables : Le dataset est d\u00e9j\u00e0 trait\u00e9 pour les variables cat\u00e9gorielle (encodage des variables categorielles). Les variables categorielles sont tout de meme identifiables, il s'agit des variables : sex, cp, fbs, restecg, exang, slope, ca, thal. Les autres variables sont discr\u00e8tes : age, trestbps, chol, thalach et une variable continue : oldpeak\n- Identification des valeurs manquantes : pas de donn\u00e9es manquantes\n\n### Analyse de fond:\n- Relation target et variables quantitatives continues : variable oldpeak (ST depression au niveau de l'electrocardiogramme, caus\u00e9 le plus souvent par la maladie coronarienne, ou coronaropathie, ou insuffisance coronarienne, qui est une maladie des art\u00e8res qui vascularisent le c\u0153u) agit sur le risque d'arret cardiaque.\n- Relation target et variables quantitatives discr\u00e8tes : \n   - Pour la variable age, les plus jeunes ont plus de risque de crise cardiaque que les plus ag\u00e9s.\n   - Le resting blood pressure et le serum cholestoral presentent presque les memes distributions pour le risque ou non d'arret cardiaque. Le serum cholestoral lui presente une petite difference de distribution. Donc elle influence legerement le risque d'arret cardiaque. Le fait qu'ils presentent plus de nombre de patients avec risque d'arret cardiaque est normal vu que la proportion de patients avec un risque cardiaque est plus grand dans le dataset.\n   - La variable thalach indique que si elle est de grande valeur alors il y a plus de risque d'arret cardiaque\n- Relation target et variables categorielles : \n   - Pour le sex on ne peut rien dire dessus, car le nombre d'hommes est sup\u00e9rieur au nombre de femmes (68 % \/ 32 %). Mais il on remarque un certain d\u00e9s\u00e9quilibre au niveau des patients avec non risque. Les hommes ont plus de chance de ne pas etre atteint de crise cardiaque. Mais cela doit etre etudi\u00e9 plus en profondeur\n   - Pour la variable cp, on remarque que la douleur a la poitrine de type 0 est plus pr\u00e9sente chez les patients avec non risque d'arret cardiaque contrairement aux type 1, 2 et 3\n   - Au niveau de la vitesse de propagation du sucre dans le sang on remarque un taux desiquilibr\u00e9 entre les patients avec un fbs > 120 mg\/dl de sang et ceux avec un fbs < 120 mg\/dl de sang. Ceux qui ont un fbs de type 1 (superieur a 120 mg\/dl) ont presque les memes taux de risque d'arret contrairement a ceux qui ont un fbs de type 0 (dont le taux d'arret cardiaque est plus nombreux). \n   - Au niveau des resultats de l'electrocardiographe on remarque un net desiquilibre les types 0, 1 et le type 2 qui presente un nombre de patients tres faible, donc c'est un cas rare. Cependant le type 0 et 2 presentent un taux plus grand de non risque d'arret cardiaque contrairement au type 1 qui, semble t'il, presente un plus grand taux de risque d'arret cardiaque que de non risque.\n   - Au niveau de la presence d'angine lors d'un exercice physique, on remarque un plus grand nombre de patients avec pas de risque d'angine avec qui le taux de risque d'arret cardiaque est plus important. Mais cela peut avoir une relation avec l'age des individus et le risque d'arret cardiaque peut etre caus\u00e9 par d'autre variables. Une analyse suppl\u00e9mentaire est n\u00e9cessaire.\n   - Au niveau du nombre de pics ou slope visualis\u00e9s sur l'electrogramme lors d'un exercice on remarque que 1 pic ou 2 pics sont les plus courants au sein des patients. Cependant 2 pics peut indiquer un risque d'arret cardiaque comme le montre le taux important de risque contrairement au cas ou on observe 1 pic. \n   - Au niveau du nombre de vaisseaux sanguins majeurs, on remarque que les patients avec pas de vaisseau majeur ont plus de risque d'arret cardiaque\n   - Au niveau du defaut d'hemoglobines dans le sang on remarque des taux disproportionn\u00e9s entre les differentes categories. La categorie 2 donne plus de risque d'arret cardiaque contrairement aux autres.\n\n### Premi\u00e8re conclusion :\n- La plupart des variables influence le risque d'arret cardiaque\n- Les patients avec un oldpeak plus faible on plus de risque d'arret cardiaque\n- Les patients plus ag\u00e9s ont plus de risque d'arret cardiaque\n- Les variables resting blood pressure et serum cholestoral ont moins d'influence sur le risque d'arret cardiaque\n- Les patients avec un heart rate achieved plus important ont plus de risque d'arret cardiaque\n- La variable age peut servir a analyser certaines variables categorielles (correlation entre age et la presence d'angine lors d'un exercice physique)\n- Les variables cp, fbs, restecg, slope, ca et tha ont une certaine influence sur le risque d'arret cardiaque\n\n### Analyses plus pouss\u00e9es :\n- Relation oldpeak\/variables discretes : La variable oldpeak n'est corr\u00e9l\u00e9e avec aucune variable discr\u00e8te\n- Relation age\/exang (etayer la these selon laquelle la variable exang n'est pas fiable) : la variable n'influence pas la presence d'angine. ++ Relation oldpeak\/exang : La variable oldpeak influence la presence d'angine. ++ Relation autres_variables_discrete\/exang : La variable thalach influence egalement la presence d'angine. On peut supposer que la presence d'angine est caus\u00e9 non pas par l'age mais par la presence d'une maladie coronarienne. \n- Relation variables non categorielles\/ autres variables categorielles : La relation entre les variables discretes et les autres variables categorielles nous permettront de savoir s'il y a des variables categorielles non utiles dans le dataset.\n  - Les distributions des variables non categorielles suivant les variables cp, ca et thal varient de differentes manieres. Ces quatres variables categorielles sont tr\u00e8s utiles car elle determine la distribution des variables non categorielles, ont doit les garder.\n  - Toutes les variables non categorielles ont les memes distributions pour les differentes valeurs prises par la variable sex. La variable n'est pas utiles \u00e0 notre analyse. Meme chose pour la variable fbs.\n  - Seules les variables non categorielles oldpeak, thalach et age ont des distributions differentes pour les differentes valeurs prises par slope. Ont doit garder les variables non categorielles oldpeak, thalach et age car elles ont une grande importance dans la repartition des variables categorielles. \n  - La variable restecg n'agit consid\u00e9rablement que sur les distributions des variables thalach et chol.\n   \n- Relation variables categorielle \/ variable categorielle : ##\n- Relation variables discretes \/ variables discretes : Aucune correlation existante entre deux variables discretes\n- Donn\u00e9es abberantes pour les variables non categorielles : Les variables trestbps, chol, thalach comportent des donn\u00e9es aberrantes\n- Analyses plus pouss\u00e9es des donn\u00e9es aberrantes : Pour les variables oldpeak, thalach et testbf, nous nous risquon a perdre des donn\u00e9es importantes en essayant de supprimer certaines donn\u00e9es aberrantes si on ne tiens pas en compte des valeurs de notre target et qu'on les supprime de maniere empirique.\n\n### Tests d'hypoth\u00e8ses (H0) :\n- H0 : Les taux de oldpeak, age et thalach sont les memes pour les patients ayant un risque d'arret cardiaque et les patients n'ayant pas de risque d'arret cardiaque.\n  - Les taux de toutes les variables non categorielles \u00e0 l'exception de la variable chol ne sont pas les memes pour les patients ayant un risque d'arret cardiaque et les patients n'ayant pas de risque d'arret cardiaque. \n  - Toutefois apres plusieurs tests de fisher, on remarque que la variable resting blood pressure aussi comporte des taux pour les deux cas. Donc les deux variables chol et trestbps peuvent etre consid\u00e9r\u00e9es comme n'agissant pas sur le risque d'arret cardiaque, oubien qu'elles agissent tres faiblement et donc n'on pas beaucoup d'influence sur le target.\n- H0 : les taux de thalach et oldpeak sont les memes pour les patients atteints d'angine et ceux non atteints\n  - Les taux de thalach et oldpeak ne sont pas les memes pour les patients atteints d'angine et ceux non atteints\n- H0 : les taux de thalach, oldpeak et age sont les memes pour les modalit\u00e9s prises par la variable slope\n  - Apres avoir effectuer trois tests et croiser les avoir croiser on peut en conclure que les taux de thalach, oldpeak et age sont differents pour les differentes modalit\u00e9s prises par la varible slope\n- H0 : Les taux de thalach et chol sont les memes pour les modalit\u00e9s prises par la variable restecg\n  - Les taux de thalach, chol, trestbps et age ne sont pas les memes pour les differentes modalit\u00e9s prises par la variable restecg\n\n### Deuxi\u00e8me conclusion :\n- Toutes les variables non categorielles \u00e0 l'exception de la variable, resting blood pressure, sont discrimantes. Toutefois on note que les variables non categorielles n'ont pas une forte correlation entre elles. La variable exang \u00e0 une relation avec les variables oldpeak (pr\u00e9sence de maladie coronarienne) et thalach, la variable slope \u00e0 une relation avec les variables oldpeak, thalach et age et la variable restecg a une relation avec les variables age, trestbps, chol et thalach. Les variables \u00e0 \u00e9liminer sont donc {sex, restecg, fbs, exang, slope}. Le dataset ne contient pas de donnees manquantes donc il sera plus facile a traiter. L'elimination de donn\u00e9es aberrantes doit etre faite avec precaution (en prenant en compte chaque modalit\u00e9 du target).","62c936c3":"## Relation target\/variables continues","c849e6aa":"## Types des variables","f9f4ba34":"Premier test pour variable slope : 0\/1","2e512edc":"#### Pour la variable fbs","21d6c39e":"## Preprocessing : Evaluation, idea, code\n### Conclusion 1 : On obtiens un score de 88 % pour l'accuracy apr\u00e8s avoir supprim\u00e9 les valeurs aberrantes et garder les variables {'age', 'cp', 'chol', 'thalach', 'oldpeak', 'ca', 'thal'}. Malheureusement on est en pr\u00e9sence d'un overfitting\n### Conclusion 2 : Apres une analyse plus pouss\u00e9es, on finit par \u00e9liminer dans la premi\u00e8re \u00e9tape du preprocessing les variables 'sex', 'fbs', 'restecg', 'exang', 'slope'. En gardant uniquement la variable thal l'overfitting disparait. Cependant on obtiens un meilleur score au niveau du test set en gardant au moins deux \u00e0 trois variables utiles (cp, oldpeak et thal par exemple apres multiples Features selection)\n### Conclusion final : Les variables cp, oldpeak et thal suffisent pour avoir un bon score tout en minimisant l'overfitting","f4026b31":"## Identification des valeurs manquantes","910b0893":"## Nombre de lignes et de colonnes","9d49c33c":"### Modelisation","64eaf0e7":"#### restecg\/variables"}}