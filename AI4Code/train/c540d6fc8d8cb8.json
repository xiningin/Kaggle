{"cell_type":{"41f6df71":"code","c21352db":"code","3eb4921f":"code","03977335":"code","3dc5012c":"code","32d9255a":"code","6236f7d9":"code","1821d8b0":"code","684b1d4b":"code","413b1168":"code","5870eb3f":"code","4633ec3e":"code","c9ecbe7b":"code","fc6d373f":"code","cfc28da2":"code","5799a191":"code","b6531835":"code","6196e81c":"code","47539128":"code","f2102201":"code","0e2f2033":"code","6119bc8c":"code","112d1495":"code","37432aba":"code","7dad75f1":"code","d839a758":"code","decfda4d":"code","5ae6245a":"code","4463faef":"code","6571ea72":"code","86d967a0":"code","e3d37f06":"code","765095fc":"code","f4183698":"code","dc4d7152":"code","e5f8153f":"code","eb2fe1e8":"code","42ad921e":"code","bc4f1e66":"code","0b9747d9":"code","855d2363":"code","5d4f39f3":"code","8737841e":"code","1a6a3cbf":"code","2703ab06":"code","00f9942c":"code","6bf9e54c":"code","bb261507":"code","32cfaaa2":"code","cb3f40c6":"code","e1d3843f":"code","4f5ea4b0":"code","90447ea8":"code","f4b71670":"code","c9ee2393":"code","270a49d7":"markdown","f03f1b93":"markdown","7c64ab0e":"markdown","4b8515a6":"markdown","74a82959":"markdown","209485cf":"markdown","18760687":"markdown","7e098b1d":"markdown","ca9357bf":"markdown","8301789a":"markdown","60f5a79e":"markdown","e1f72c7a":"markdown","86d2a958":"markdown","0d017421":"markdown","3f47bd91":"markdown","6885d421":"markdown","da8d681e":"markdown","bd1c3792":"markdown","3fe53c29":"markdown","7073875c":"markdown","6d950497":"markdown","a9a0f5fd":"markdown","b8853ce6":"markdown","8fc15fb1":"markdown","cae9ac9f":"markdown","cd3818bd":"markdown","6dc6a0ac":"markdown","bbe62a8a":"markdown"},"source":{"41f6df71":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c21352db":"training = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\ntraining['train_test'] = 1\ntest['train_test'] = 0\ntest['Survived'] = np.NaN\n\nall_data = pd.concat([training, test])","3eb4921f":"import matplotlib.pyplot as plt\n\n%matplotlib inline\n\ntest.dtypes","03977335":"training.info()","3dc5012c":"training.describe()","32d9255a":"# Let's find out the unique dtypes in the training set:\n\ntraining.dtypes.unique()","6236f7d9":"# Split the numerical and categorical data types\n\n### This way is not helpful as there are some columns which are numerical but we don't want them to be in the numerical dataset, like the id.\n# df_numerical = training.select_dtypes(exclude = 'object')\n# df_categorical = training.select_dtypes(exclude = ['int64', 'float64'])\n\n### Let's do it manually:\n\ndf_numerical = training[['Age', 'SibSp', 'Parch', 'Fare']]\ndf_categorical = training[['Survived', 'Pclass', 'Sex', 'Ticket', 'Cabin', 'Embarked']]","1821d8b0":"df_numerical.dtypes.unique()","684b1d4b":"df_categorical.dtypes.unique()","413b1168":"# Distributions for all numeric variables:\n\nfor i in df_numerical.columns:\n    plt.hist(df_numerical[i])\n    plt.title(i)\n    plt.show()","5870eb3f":"df_numerical.corr()","4633ec3e":"# Plotting a heatmap for correlations:\nimport seaborn as sns\n\nplt.figure(figsize = (5, 6))\nsns.heatmap(df_numerical.corr(), annot = True, vmin=-1, vmax=1, center= 0, cmap= 'coolwarm', linewidths=3, linecolor='black')","c9ecbe7b":"# Let's have a look at a pivot table, this will tell us the avg value for the column with respect to their survival.\n\npd.pivot_table(training, index = 'Survived', values = ['Age', 'SibSp', 'Parch', 'Fare'])","fc6d373f":"for i in df_categorical:\n    sns.barplot(df_categorical[i].value_counts().index, df_categorical[i].value_counts()).set_title(i)\n    plt.show()","cfc28da2":"sns.countplot(x = 'Survived', hue = 'Sex', data = training)","5799a191":"sns.countplot(x = 'Survived', hue = 'Pclass', data = training)","b6531835":"sns.countplot(x = 'Survived', hue = 'Embarked', data = training)","6196e81c":"df_categorical.Cabin","47539128":"# Some entries in the cabin column have multiple values assigned to them (eg. C23 C25 C27). We are analysing if having more than one cabin caused survival.\n\ntraining['cabin_multiple'] = training.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\ntraining['cabin_multiple'].value_counts()","f2102201":"sns.countplot(x = 'Survived', hue = 'cabin_multiple', data = training)","0e2f2033":"#Create categories based on the cabin DONT FORGET TO TEST THIS IN THE END!!!!!!\n\ntraining['cabin_adv'] = training.Cabin.apply(lambda x: str(x)[0])","6119bc8c":"print(training.cabin_adv.value_counts())\nsns.countplot(x = 'Survived', hue = 'cabin_adv', data = training)","112d1495":"training['Ticket'].value_counts()","37432aba":"# Split the numeric tickets and non-numeric tickets\n\ntraining['numeric_ticket'] = training.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ntraining['ticket_letters'] = training.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.', '').replace('\/','').lower() if len(x.split(' ')[:-1]) > 0 else 0)","7dad75f1":"# Let's see how many tickets are numeric:\n\ntraining['numeric_ticket'].value_counts()","d839a758":"# Now let's see how many different letters the tickets had:\n\npd.set_option(\"max_rows\", None) # This will not bound the values to be just 5\ntraining['ticket_letters'].value_counts()","decfda4d":"sns.countplot(x = 'Survived', hue = 'numeric_ticket', data = training) # this will tell the impact of having numeric ticket on survival.","5ae6245a":"# It wouldn't be a good idea to plot a distribution for the ticket_letters as there are too many letter to show on a graph.\n\n# Let's make a pivot table instead.\n\npd.pivot_table(training, index = 'Survived', columns = 'ticket_letters', values = 'Ticket', aggfunc = 'count')","4463faef":"# Let's have a look at different names that our dataset contains.\n\ntraining['Name'].head()","6571ea72":"# The title is on the position second in the column 'Name', let's take out the title and put it in a seperate column.\n\ntraining['name_title'] = training.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())","86d967a0":"training['name_title']","e3d37f06":"# First we will apply the same transformations as the training set on the all_data (complete dataset).\n\nall_data['cabin_multiple'] = all_data.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\nall_data['cabin_adv'] = all_data.Cabin.apply(lambda x: str(x)[0])\nall_data['numeric_ticket'] = all_data.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\nall_data['ticket_letters'] = all_data.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\nall_data['name_title'] = all_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n\n# Drop the rows where the Embarked has null values.\nall_data.dropna(subset=['Embarked'],inplace = True)","765095fc":"# Take care of the missing values\n\nall_data.Age = all_data.Age.fillna(training.Age.mean())\nall_data.Fare = all_data.Fare.fillna(training.Fare.median()) #### change this to mean and check later on....\n\n# Normalize the SibSp column\nall_data['norm_sibsp'] = np.log(all_data.SibSp+1)\nall_data['norm_sibsp'].hist()","f4183698":"# Normalize the Fare column\nall_data['norm_fare'] = np.log(all_data.Fare+1)\nall_data['norm_fare'].hist()","dc4d7152":"all_data","e5f8153f":"# Convert the type of Pclass column into string\nall_data.Pclass = all_data.Pclass.astype(str)\n\n# Convert the categorical column Pclass into dummies (0 or 1) [Can also use OneHotEncoder provided by sklearn]\nall_dummies = pd.get_dummies(all_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'norm_fare', 'Embarked', 'cabin_adv', 'cabin_multiple', 'numeric_ticket', 'name_title', 'train_test']])\n\n# Split the all_data dataset into train and test data (The way they were)\nX_train = all_dummies[all_dummies.train_test == 1].drop(['train_test'], axis =1)\nX_test = all_dummies[all_dummies.train_test == 0].drop(['train_test'], axis =1)\ny_train = all_data[all_data.train_test==1].Survived\ny_train.shape","eb2fe1e8":"all_dummies","42ad921e":"# Scale data \nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nall_dummies_scaled = all_dummies.copy()\nall_dummies_scaled[['Age','SibSp','Parch','norm_fare']]= scale.fit_transform(all_dummies_scaled[['Age','SibSp','Parch','norm_fare']])\nall_dummies_scaled\n\nX_train_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 1].drop(['train_test'], axis =1)\nX_test_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 0].drop(['train_test'], axis =1)\n\ny_train = all_data[all_data.train_test==1].Survived","bc4f1e66":"from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC","0b9747d9":"# Naive Bayes\ngnb = GaussianNB()\ncv = cross_val_score(gnb, X_train_scaled, y_train, cv = 5)\nprint(cv)\nprint(cv.mean())","855d2363":"# Logistic Regression\nlr = LogisticRegression()\ncv = cross_val_score(lr, X_train_scaled, y_train, cv = 5)\nprint(cv)\nprint(cv.mean())","5d4f39f3":"# Decision Trees\ndt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt, X_train, y_train, cv = 5)\nprint(cv)\nprint(cv.mean())","8737841e":"# K-Nearest Neighbors\nknn = KNeighborsClassifier()\ncv = cross_val_score(knn, X_train, y_train, cv = 5)\nprint(cv)\nprint(cv.mean())","1a6a3cbf":"# Random Forest\nrf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf, X_train_scaled, y_train, cv = 5)\nprint(cv)\nprint(cv.mean())","2703ab06":"# Support Vector Classifier\nsvc = SVC(probability = True)\ncv = cross_val_score(svc, X_train_scaled, y_train, cv = 5)\nprint(cv)\nprint(cv.mean())","00f9942c":"# XG Boost\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier(random_state = 1)\ncv = cross_val_score(xgb, X_train_scaled, y_train, cv = 5)\nprint(cv)\nprint(cv.mean())","6bf9e54c":"# Voting Classifier\nfrom sklearn.ensemble import VotingClassifier\nvoting_clf = VotingClassifier(estimators = [('lr', lr), ('knn', knn), ('rf', rf), ('gnb', gnb), ('svc', svc), ('xgb', xgb)], voting = 'soft')\ncv = cross_val_score(voting_clf, X_train_scaled, y_train, cv = 5)\nprint(cv)\nprint(cv.mean())","bb261507":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Write a function that tells us the performance:\ndef clf_performance(classifier, model_name):\n    print(model_name)\n    print('Best Score: ' + str(classifier.best_score_))\n    print('Best Parameters: ' + str(classifier.best_params_))","32cfaaa2":"# Logistic Regression\nlr = LogisticRegression()\nparam_grid = {'max_iter' : [2000],\n              'penalty' : ['l1', 'l2'],\n              'C' : np.logspace(-4, 4, 20),\n              'solver' : ['liblinear']}\n\nclf_lr = GridSearchCV(lr, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_lr = clf_lr.fit(X_train_scaled,y_train)\nclf_performance(best_clf_lr,'Logistic Regression')","cb3f40c6":"# K-Nearest Neighbors\nknn = KNeighborsClassifier()\nparam_grid = {'n_neighbors' : [3,5,7,9],\n              'weights' : ['uniform', 'distance'],\n              'algorithm' : ['auto', 'ball_tree','kd_tree'],\n              'p' : [1,2]}\nclf_knn = GridSearchCV(knn, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_knn = clf_knn.fit(X_train_scaled,y_train)\nclf_performance(best_clf_knn,'KNN')","e1d3843f":"# Support Vector Classifier\nsvc = SVC(probability = True)\nparam_grid = tuned_parameters = [{'kernel': ['rbf'], 'gamma': [.1,.5,1,2,5,10],\n                                  'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['linear'], 'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['poly'], 'degree' : [2,3,4,5], 'C': [.1, 1, 10, 100, 1000]}]\nclf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_svc = clf_svc.fit(X_train_scaled,y_train)\nclf_performance(best_clf_svc,'SVC')","4f5ea4b0":"# Random Forest Classifier\n\nrf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [400,450,500,550],\n                'criterion':['gini','entropy'],\n                'bootstrap': [True],\n                'max_depth': [15, 20, 25],\n                'max_features': ['auto','sqrt', 10],\n                'min_samples_leaf': [2,3],\n                'min_samples_split': [2,3]}\n                                  \nclf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_rf = clf_rf.fit(X_train_scaled,y_train)\nclf_performance(best_clf_rf,'Random Forest')","90447ea8":"best_rf = best_clf_rf.best_estimator_.fit(X_train_scaled,y_train)\nfeat_importances = pd.Series(best_rf.feature_importances_, index=X_train_scaled.columns)\nfeat_importances.nlargest(20).plot(kind='barh')","f4b71670":"# XG Boost\n\nxgb = XGBClassifier(random_state = 1)\n\nparam_grid = {\n    'n_estimators': [450,500,550],\n    'colsample_bytree': [0.75,0.8,0.85],\n    'max_depth': [None],\n    'reg_alpha': [1],\n    'reg_lambda': [2, 5, 10],\n    'subsample': [0.55, 0.6, .65],\n    'learning_rate':[0.5],\n    'gamma':[.5,1,2],\n    'min_child_weight':[0.01],\n    'sampling_method': ['uniform']\n}\n\nclf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_xgb = clf_xgb.fit(X_train_scaled,y_train)\nclf_performance(best_clf_xgb,'XGB')","c9ee2393":"y_hat_xgb = best_clf_xgb.best_estimator_.predict(X_test_scaled).astype(int)\nxgb_submission = {'PassengerId': test.PassengerId, 'Survived': y_hat_xgb}\nsubmission_xgb = pd.DataFrame(data=xgb_submission)\nsubmission_xgb.to_csv('xgb_submission3.csv', index=False)","270a49d7":"## Now we will be plotting the categorical columns:","f03f1b93":"### Let's visualize the impact of these features on their survival.","7c64ab0e":"##### Both the cases have kind of same ratio so does not seem like having a numeric ticket had any imapact on the survival.","4b8515a6":"# Let's now look at some correlations:","74a82959":"# Optimization\/Model Tuning","209485cf":"### 1) Cabin - Simplify cabins (evaluated if cabin letter (cabin_adv) or the purchase of tickets across multiple cabins (cabin_multiple) impacted survival)","18760687":"# Now let's try to feed this data to various Models and see the result:","7e098b1d":"### 2) Tickets - Do different ticket types impact survival rates?\n##### Let's try to understand the tickets values better!","ca9357bf":"# Feature Engineering\n\n#### Having a lot of columns does not mean that we need to use all of them, it's a smart move to remove the columns that are not needed. That is what we are going to do next.","8301789a":"##### Most of the survivers embarked from Southhampton, followed by Cherbourg and Queenstown","60f5a79e":"# Data Exploration\n\n## 1) For numerical data:\n    # Made histograms to understand distributions.\n    # corrplot\n    # Pivot table comparing survival rate across numerical variables.\n    \n## 2) For Categorical Data:\n    # made bar charts to understand balance of classes.\n    # made pivot tables to understand relationship with survival.","e1f72c7a":"## This notebook takes reference from Ken Jee's youtube video titled 'Beginner Kaggle Data Science Project Walk-Through'","86d2a958":"##### We can see that females survived more than males.","0d017421":"### Now let's put this to use, let's see if this had any impact on the survival","3f47bd91":"##### This plot shows us that people who had first class tickets survived the most, followed by third and then second in the last.","6885d421":"##### The value counts tells us that majority of data in the cabin column is missing (when it is 0) and most of the people had one cabin only.","da8d681e":"##### The plot shows if the first letter of the cabin had any impact on the survival.","bd1c3792":"## Plotting some distributions:","3fe53c29":"## Now let's plot the same analysis for a better intution:","7073875c":"# Date preprocessing for Model","6d950497":"## Project Planning\n\n#### Understand the nature of the data: .info(), .describe()\n#### Histograms and boxplots\n#### value counts\n#### Missing data\n#### Correlation between the metrics\n#### Explore interesting themes:\n* Did wealty survive?\n* By location?\n* Age scatterplot with ticket price\n* Young and wealthy variable\n* Total spent?\n\n#### Feature Engineering\n#### Preprocess data together or use a transformer.\n#### Scaling\n#### Model Baseline\n#### Model comparison with CV\n\n\n##### Let's get started!","a9a0f5fd":"##### Takeaway from here is that if someone had more cabins assigned to them, their survival was less likely.","b8853ce6":"# Overview:\n\n#### 1). Understand the shape of the data (Histograms, box plots, etc.)\n#### 2). Data Cleaning\n#### 3). Data Exploration\n#### 4). Feature Engineering\n#### 5). Data Preprocessing for Model\n#### 6). Basic Model Building\n#### 7). Model Tuning\n#### 8). Ensemble Model Building\n#### 9). Results","8fc15fb1":"#### Some conclusions we may make here are: People who survived paid more, were younger, Had parent\/children on deck, had lesser sibling\/spouse","cae9ac9f":"##### Nothing too relavent here also.","cd3818bd":"\n\n### 3) Does a person's title relate to survival rates?","6dc6a0ac":"### Some conclusion we can make here are:\n    # People who survived were less than the people who could not.\n    # Majority of people had third class ticked, then first then second.\n    # There were more males on the deck than females.\n    # Most people embarked from Southhampton followed by Cherbourg and Queenstown.","bbe62a8a":"### Conclusions we can make at this point is that only Age columns has the normal distribution. We will have to normalize other features which we think are important."}}