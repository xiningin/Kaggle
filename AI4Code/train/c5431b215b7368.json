{"cell_type":{"59f5d84c":"code","ffc18e78":"code","415fb4df":"code","34751587":"code","c34b5b12":"code","75515cb2":"code","dc1e094d":"code","1778422e":"code","0b931b91":"code","7f6c65d7":"code","947c0a40":"code","f228de58":"code","22606511":"code","3db16423":"code","a2aa1466":"code","2902403f":"code","11df8149":"code","8745670e":"code","ff3e0bee":"code","05b88a78":"code","891a97d3":"code","99442453":"code","3627d8b3":"code","d090192f":"code","3c40575e":"code","1e514ff9":"code","a0b7fba2":"code","a1d6faeb":"code","3fb24712":"code","1ba07b9d":"code","b2bebbcb":"code","a67ef092":"code","2b715899":"code","dfc5f93f":"code","c2d1e53f":"code","07064d63":"code","75d01e8e":"markdown","2f1d226c":"markdown","9d6c31f3":"markdown","e49e13eb":"markdown","5be84ff8":"markdown","7a347932":"markdown","08e4a0bc":"markdown","f076a7e9":"markdown","8d5c5e0e":"markdown","a7ad0d7a":"markdown","519498fd":"markdown","067e53b9":"markdown","dcb669f8":"markdown","ed3c5159":"markdown","35dccccd":"markdown","a1bd093b":"markdown","d2c91eef":"markdown","d5c80cbf":"markdown","58088ff1":"markdown","56873fbb":"markdown","a3e1f3c8":"markdown","5791c2e3":"markdown","92e90a7e":"markdown","aff932ef":"markdown","c7a15c68":"markdown","f62fd5c5":"markdown","3e2de5b3":"markdown","496ecb34":"markdown","17473507":"markdown","d54730ba":"markdown","eeefb0ec":"markdown","1b240e1f":"markdown"},"source":{"59f5d84c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n# Ploting and visualisations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.express as px \nfrom plotly.offline import download_plotlyjs,init_notebook_mode, iplot\nimport plotly.tools as tls \nimport plotly.figure_factory as ff \npy.init_notebook_mode(connected=True)\n# ----------------------- #\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ffc18e78":"# Products\nproducts = pd.read_csv(\"\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning\/products_info.csv\")\n\n# Districts\ndistricts = pd.read_csv(\"\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv\")","415fb4df":"# checking missing values if any\ndisplay(products.info(),products.head(),products.isnull().sum())","34751587":"display(districts.info(),districts.head(),districts.isnull().sum())","c34b5b12":"products['Product Name'].unique()","75515cb2":"# Define missing plot to detect all missing values in dataset\ndef missing_plot(dataset, key) :\n    null_feat = pd.DataFrame(len(dataset[key]) - dataset.isnull().sum(), columns = ['Count'])\n    percentage_null = pd.DataFrame((dataset.isnull().sum())\/len(dataset[key])*100, columns = ['Count'])\n    percentage_null = percentage_null.round(2)\n\n    trace = go.Bar(x = null_feat.index, y = null_feat['Count'] ,opacity = 0.8, text = percentage_null['Count'],  textposition = 'auto',marker=dict(color = '#7EC0EE',\n            line=dict(color='#000000',width=1.5)))\n\n    layout = dict(title =  \"Missing Values (count & %)\")\n\n    fig = dict(data = [trace], layout=layout)\n    py.iplot(fig)","dc1e094d":"missing_plot(products,'Product Name')","1778422e":"missing_plot(districts,'district_id')","0b931b91":"\ndef target_count(data,column):\n    trace = go.Bar( x = data[column].value_counts().values.tolist(),\n    y = data[column].unique(),\n    orientation = 'h',\n    text = data[column].value_counts().values.tolist(),\n    textfont=dict(size=20),\n    textposition = 'auto',\n    opacity = 0.5,marker=dict(colorsrc='tealrose',\n            line=dict(color='#000000',width=1.5))\n    )\n    layout = (dict(title= \"EDA of {} column\".format(column),\n                  autosize=True,height=800,))\n    fig = dict(data = [trace], layout=layout)\n    \n    py.iplot(fig)\n\n# --------------- donut chart to show there percentage -------------------- # \n\ndef target_pie(data,column):\n    trace = go.Pie(labels=data[column].unique(),values=data[column].value_counts(),\n                  textfont=dict(size=15),\n                   opacity = 0.5,marker=dict(\n                   colorssrc='tealrose',line=dict(color='#000000', width=1.5)),\n                   hole=0.6)\n                  \n    layout = dict(title=\"Dounat chart to see %age of individual elements\")\n    fig = dict(data=[trace],layout=layout)\n    py.iplot(fig)\n","7f6c65d7":"#target_count(products,'Product Name')\n#target_pie(products,'Product Name')","947c0a40":"products['Product Name'].value_counts()","f228de58":"# checking the frequence of the 'Product name' \nfreq = products.groupby(['Product Name']).count() \nfreq","22606511":"#products['Provider\/Company Name'].unique()\n\nfreq = products['Provider\/Company Name'].value_counts()[:20]\nfreq","3db16423":"freq = products.groupby(['Provider\/Company Name']).count()\nfreq.sort_values(by=['Product Name'], ascending=False )[:10]","a2aa1466":"target_pie(freq,'Product Name')","2902403f":"#Sector(s)\nproducts['Sector(s)'].unique()","11df8149":"freq = products.groupby(['Sector(s)','Provider\/Company Name']).count()\nfreq.sort_values(by=['Product Name'], ascending=False )","8745670e":"pk = products[products['Sector(s)'] == 'PreK-12']\n\npk = pk.groupby(['Provider\/Company Name']).count()\npk.sort_values(by=['Product Name'], ascending=False )[:5]","ff3e0bee":"pkH = products[products['Sector(s)'] == 'PreK-12; Higher Ed']\n\npkH = pkH.groupby(['Provider\/Company Name']).count()\npkH.sort_values(by=['Product Name'], ascending=False )[:5]","05b88a78":"pkH = products[products['Sector(s)'] == 'PreK-12; Higher Ed; Corporate']\n\npkH = pkH.groupby(['Provider\/Company Name']).count()\npkH.sort_values(by=['Product Name'], ascending=False )[:5]","891a97d3":"pkH = products[products['Sector(s)'] == 'Corporate']\n\npkH = pkH.groupby(['Provider\/Company Name']).count()\npkH.sort_values(by=['Product Name'], ascending=False )","99442453":"pkH = products[products['Sector(s)'] == 'Higher Ed; Corporate']\n\npkH = pkH.groupby(['Provider\/Company Name']).count()\npkH.sort_values(by=['Product Name'], ascending=False )","3627d8b3":"products['Primary Essential Function'].unique()\nfreq = products.groupby(['Provider\/Company Name','Primary Essential Function']).count()\n\nfreq.sort_values(by=['Product Name'], ascending=False )[:10]","d090192f":"products['Primary Essential Function'].unique()\nfreq = products.groupby(['Primary Essential Function']).count()\nfreq.sort_values(by=['Product Name'], ascending=False )","3c40575e":"products['Primary Essential Function'].unique()\nfreq = products.groupby(['Primary Essential Function','Provider\/Company Name']).count()\nfreq.sort_values(by=['Product Name'], ascending=False )[:10]","1e514ff9":"import plotly.express as px\ndf = px.data.tips()\nsunb_data = products[['Primary Essential Function','Provider\/Company Name','Sector(s)']]\nsunb_data = sunb_data.dropna()\nsunb_data = sunb_data.groupby(['Primary Essential Function','Provider\/Company Name']).size().reset_index(name='count')\n#sunb_data.sort_values(by=['count'], ascending=False )[:10]\nfig = px.sunburst(sunb_data, path=['Primary Essential Function','Provider\/Company Name'], values='count')\nfig.show()","a0b7fba2":"df = products[['Product Name','Primary Essential Function','Provider\/Company Name','Sector(s)']]\ndf = df.dropna()\ndf = df.groupby(['Provider\/Company Name','Product Name','Primary Essential Function','Sector(s)']).size().reset_index(name='count')\ndf.sort_values(by=['count'], ascending=False )","a1d6faeb":"## Lets see the representation of states this shows which states were more involved in learning... \npx.histogram(districts, x='state', barmode='group').update_xaxes(categoryorder='total ascending')","3fb24712":"#fig = px.pie(districts, values='state', names='state', title='Which state was most active')\n#iplot(fig)\ndef PIE(state):\n    trace = go.Pie(labels=districts[state].unique(),values=districts[state],\n                      textfont=dict(size=15),\n                       opacity = 0.5,marker=dict(\n                       colorssrc='tealrose',line=dict(color='#000000', width=1.5)),\n                       hole=0.6)\n\n    layout = dict(title=\"Which state was most active\")\n    fig = dict(data=[trace],layout=layout)\n    py.iplot(fig)\nPIE('state')","1ba07b9d":"# get names of indexes for which districts['state'].isnull() as state,locale both have 57 rows that are totally empty as seen \n# during previous data analysis\nindex_names = districts[ districts['state'].isnull()].index\n  \n# drop these row indexes\n# from dataFrame\ndistricts.drop(index_names, inplace = True)\n\ndistricts","b2bebbcb":"## Lets also see locality was more involved in learning... \ntarget_count(districts,'locale')\ntarget_pie(districts,'locale')","a67ef092":"px.histogram(districts, x='county_connections_ratio', barmode='group')","2b715899":"px.histogram(districts, x='pct_black\/hispanic', barmode='group')","dfc5f93f":"px.histogram(districts, x='pct_free\/reduced', barmode='group')","c2d1e53f":"px.histogram(districts, x='pp_total_raw', barmode='group')","07064d63":"import glob  \npath = \"\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\/*.csv\"\n\ndfs = []\nfor fname in glob.glob(path):\n    dfs.append(pd.read_csv(fname))\n\n# Concatenate all data into one DataFrame\nbig_frame = pd.concat(dfs, ignore_index=True)\nbig_frame\n\n## lp_id and product LP_ID can be merged and we can regroup according to dates and then analysis which company was being favoured as the days passe using engagement_index col. ","75d01e8e":"## This is first part I still need to go through the other data sheets. Hope till now it was an interesting notebook.If you like the work do show your support and stay tuned for more detailed exploration. Thank you once again.","2f1d226c":"## Similarly lets look at other columns also.","9d6c31f3":"## The above table and dougnut chart gives the percentage of people who have taken the course. \n* ### 89% people have taken different courses. \n* ### 0.69% people have taken by google courses. \n* ### 6.21% people have been a part of companies like Houghton,Microsoft, Learning A-z etc with 3+ people following it. ","e49e13eb":"### Here there are data points that are missing but still a little more work has to be done. \n### Removal of NaN values has to be done with.But will deal with it later.","5be84ff8":"## People from Suburbs are more active on these platforms to learn followed by Rural,City and Town people.","7a347932":"## Definatly we have to look into the districts dataframe. But we will look at it later. First lets go through products table","08e4a0bc":"* ## Google LLC covers the major sector like PreK-12; Higher Ed; Corporate ","f076a7e9":"## Lets also look at which company is focusing on which 'Primary Essential Function'","8d5c5e0e":"## Lets also see other way around to 'Primary Essential Function'","a7ad0d7a":"## So Below companies Primary Essential Function are the following. Thus we can also say that the following companies these products are better for learning. \n* #### Google LLC\tLC\/CM\/SDO - Other\n* #### LC - Content Creation & Curation\n* #### Learning A-Z\tLC - Courseware & Textbooks\n* #### IXL Learning\tLC - Digital Learning Platforms\n* #### Houghton Mifflin Harcourt\tLC - Courseware & Textbooks\n* #### Curriculum Associates\tLC - Digital Learning Platforms\n* #### Google LLC\tLC - Sites, Resources & Reference\n* #### CM - Classroom Engagement & Instruction - Communication & Messaging\n* #### Dictionary.com\tLC - Sites, Resources & Reference - Thesaurus & Dictionary\n* #### The College Board\tLC - Study Tools - Test Prep & Study Skills","519498fd":"### Note: Higher Ed; Corporate , Corporate are taken by Weebly,Qualtrics respectively","067e53b9":"### Lets explore more by regrouping the date as 'Provider'","dcb669f8":"## Next we are going to combine all the dataframes to collectively analysis the data and try to answer the important questions asked. Thanks for staying connected. Will be updating the notebook with futher analysis soon. \n## Do comment and share your views with me. Thank you once again. ","ed3c5159":"## We can see that many people used these companies resoursers for :-\n* ### LC - Digital Learning Platforms\n* ### LC - Sites, Resources & Reference\n* ### LC - Content Creation & Curation\n* ### LC - Study Tools\n## Due to being locked in homes people also learned more about 'Content Creation & Curation' thats an intresting thing to see.","35dccccd":"### Note: PreK-12 is covered by Learning A-Z,Curriculum Associates,The College Board,IXL Learning,Houghton Mifflin Harcourt","a1bd093b":"### Note: McGraw-Hill PreK-12,Houghton Mifflin Harcourt,Google LLC cover 'PreK-12; Higher Ed' sector","d2c91eef":"## Here we can see the 'Primary Essential Function' and then the 'Provider\/Company Name' and their count..\n## From the above graph we can learn a lot about which product product is being used and which companies product is being utilized.\n* ### Digital Learning Platform was the most used 'Primary Essential Function' and companies like Circumulative Associative, IXL learning,Teaching.com were mostly used.\n* ### Similarly for Sites,and references Google LLC was mostly refered. \n* ### Again For Content creation Google,Adobe and Autodesk was used more.","d5c80cbf":"## Similarly going through other columns we can see the following trend","58088ff1":"## Now lets explore districts datasheet. \n### We already know there are many NaN values so we have to deal with them but before that we have to see if we can do any imputaions or we will have to simply drop them. ","56873fbb":"### Note: Google, Houghton,Microsoft, Learning A-z etc are the most used courses as seen below.","a3e1f3c8":"## Lets remove NaN values","5791c2e3":"## Below I have divided the data on the bases of Sector and then grouping then by 'Provider\/Company Name' to learn more.\n## Here I can find which companies are focusing which sectors. ","92e90a7e":"## Note: This notebook is about learning what data we have a little of cleaning and re-arranging the items. Do stay tuned to see more of it. Also I will first be looking a individual datasheets given and then try to merge them together to start answering the questions asked above.\n## Also please comment or give suggestions on how to improve it further. Thank you.","aff932ef":"### Here some data points are missing but still alot is there to work on. ","c7a15c68":"# LearnPlatform COVID-19 Impact on Digital Learning\nUse digital learning data to analyze the impact of COVID-19 on student learning\n\n* Challenge\nWe challenge the Kaggle community to explore (1) the state of digital learning in 2020 and (2) how the engagement of digital learning relates to factors such as district demographics, broadband access, and state\/national level policies and events.\n\n* We encourage you to guide the analysis with questions that are related to the themes that are described above (in bold font). Below are some examples of questions that relate to our problem statement:\n    * What is the picture of digital connectivity and engagement in 2020?\n    * What is the effect of the COVID-19 pandemic on online and distance learning, and how might this also evolve in the future?\n    * How does student engagement with different types of education technology change over the course of the pandemic?\n    * How does student engagement with online learning platforms relate to different geography? Demographic context (e.g., race\/ethnicity, ESL, learning disability)? Learning context? Socioeconomic status?\n    * Do certain state interventions, practices or policies (e.g., stimulus, reopening, eviction moratorium) correlate with the increase or decrease online engagement?","f62fd5c5":"### Google LLC,Microsoft,Autodesk, Inc,Adobe Inc.,ZOOM VIDEO COMMUNICATIONS, INC coveres 'PreK-12; Higher Ed; Corporate' Sector","3e2de5b3":"## Now we can also see which course is utilised my people and which company is providing it..","496ecb34":"## Let's also see the Sector's people have selected from.","17473507":"## Lets create a Sunburst plot for more details just click on the element you want to know about\n## The main purpose to use this plot was to be able to group major info together ","d54730ba":"## Check missing points in Percentages","eeefb0ec":"### Connecticut was the most active one followed by Utah, Massachusetts and Illinios.   ","1b240e1f":"### From Product Name we will only get individual courses used but learning about Provider\/Company Name is benificial as a company can have several different products, So we might look at it. "}}