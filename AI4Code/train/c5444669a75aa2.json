{"cell_type":{"66e4e643":"code","e25e596f":"code","7bfadc10":"code","73b14fcf":"code","47853f6a":"code","03cf94e9":"code","1049fab4":"code","b92fc33a":"code","aa509f70":"code","0774ef40":"code","1b5a872b":"code","3c5516c0":"code","e11e7ca9":"code","12a00952":"code","8b257c00":"code","013e81c3":"code","907003df":"code","609c4534":"code","b5cb13fc":"code","55d2608b":"code","a85bb1a3":"code","d603745f":"code","05074ab7":"code","d9044fce":"code","c728ace8":"code","806ef35a":"markdown","33d396c8":"markdown","85856de7":"markdown","8d352c49":"markdown","e02fc3d6":"markdown","86e56153":"markdown","2a9dd75f":"markdown","1f826fe0":"markdown","9fa04438":"markdown","382da18b":"markdown","2b222bdb":"markdown","20314d5e":"markdown","5f697cba":"markdown","5c37e1ab":"markdown","cac58bc6":"markdown","cd518935":"markdown","8ed3099e":"markdown","a57c5e2c":"markdown","ce31e5e2":"markdown","335e58fe":"markdown","47087a2c":"markdown","c2795ab2":"markdown","a0574176":"markdown","c7fda64a":"markdown","dfae196c":"markdown","73f7b287":"markdown","d50af0d2":"markdown"},"source":{"66e4e643":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","e25e596f":"df = pd.read_csv('..\/input\/churn-prediction\/Churn.csv')\ndf.head()","7bfadc10":"print('ROWS: ', df.shape[0])\nprint('COLUMNS: ', df.shape[1])\nprint('nombre columnas: ', df.columns.tolist())\nprint('valores \u00fanicos por columna: ', df.nunique())\nprint('missing values: ', df.isnull().sum().values)\nprint('tipo de columnas: ',df.dtypes)\n\ndata_cat = df.select_dtypes('object').columns.tolist()\nfor i in data_cat:\n    print(df[i].value_counts())","73b14fcf":"df['Churn'].value_counts()","47853f6a":"df['TotalCharges']=df['TotalCharges'].replace(' ',np.nan)\ndf['TotalCharges']=df['TotalCharges'].astype(float)\ndf['TotalCharges']=df['TotalCharges'].fillna(df['TotalCharges'].mean())\n\nnointernet_cols = ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']\nfor i in nointernet_cols:\n    df[i]=df[i].replace('No internet service','No')\n\ndf['MultipleLines']=df['MultipleLines'].replace('No phone service','No')\n\ndf[\"SeniorCitizen\"] = df[\"SeniorCitizen\"].replace({1:\"Yes\",0:\"No\"})\n\n#We will use the numeric variable tenure for the model, but in order to have some insights we create this segment.\ndef tenure_tramo(df):\n    if df['tenure'] <=12:\n        return 'Tenure_0-12'\n    elif (df['tenure']>12) & (df['tenure'] <= 24) :\n        return 'Tenure_12-24'\n    elif (df['tenure'] > 24) & (df['tenure'] <= 48) :\n        return 'Tenure_24-48'\n    elif (df['tenure'] > 48) & (df['tenure'] <= 60) :\n        return 'Tenure_48-60'\n    elif df['tenure'] > 60 :\n        return 'Tenure_gt_60'\n    \ndf['tenure_group'] = df.apply(lambda df:tenure_tramo(df),axis = 1)","03cf94e9":"from plotly.offline import plot \nimport plotly.graph_objs as go\nimport plotly.offline as py\n\n#piechart function\ndef piecharts (columna):\n    trace_churn = go.Pie(labels = churn[columna].value_counts().keys().tolist() ,\n               values = churn[columna].value_counts().values.tolist() ,\n               marker = dict(colors =  [ 'royalblue' ,'lime'],\n                             line = dict(color = \"white\",\n                                         width =  3.6)\n                            ),\n               rotation = 90,\n               hoverinfo = \"label+value+text\",\n               domain  = dict(x = [0,.48]),\n               name= \"Churn customers\",\n               hole = .7\n              )\n\n    trace_nochurn = go.Pie(labels = no_churn[columna].value_counts().keys().tolist() ,\n               values = no_churn[columna].value_counts().values.tolist() ,\n               marker = dict(colors =  [ 'royalblue' ,'lime'],\n                             line = dict(color = \"white\",\n                                         width =  3.6)\n                            ),\n               rotation = 90,\n               hoverinfo = \"label+value+text\",\n               domain  = dict(x = [0.52,1]),\n               name= \"Non Churn customers\",\n               hole = .7\n              )\n                             \n    layout = go.Layout(dict(title = columna + \" distribution\",\n                            plot_bgcolor  = \"rgb(243,243,243)\",\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                            annotations = [dict(text = \"churn customers\",\n                                                font = dict(size = 20),\n                                                showarrow = False,\n                                                x = .17, y = .5),\n                                           dict(text = \"Non churn customers\",\n                                                font = dict(size = 20),\n                                                showarrow = False,\n                                                x = .85,y = .5\n                                               )\n                                          ]\n                           )\n                      )\n\n    data = [trace_churn,trace_nochurn]\n    fig = go.Figure(data = data,layout = layout)\n    py.iplot(fig)","1049fab4":"churn=df[df['Churn']=='Yes']\nno_churn=df[df['Churn']=='No']","b92fc33a":"data_cat = df.select_dtypes('object').columns.tolist()\ndata_cat = [x for x in data_cat if x not in 'customerID']","aa509f70":"for i in data_cat:\n    piecharts(i)","0774ef40":"#plot numerical variables\ndef HISTO (columna):\n    trace_churn = go.Histogram(x = churn[columna] ,\n               histnorm = \"percent\",\n               marker = dict(line = dict(color = \"black\",\n                                         width =  0.6)\n                            ),\n               name= \"Churn customers\",\n               opacity = .9\n              )\n\n    trace_nochurn = go.Histogram(x = no_churn[columna] ,\n               histnorm = \"percent\",\n               marker = dict(line = dict(color = \"black\",\n                                         width =  0.6)\n                            ),\n               name= \"Non churn customers\",\n               opacity = .9\n              )\n                             \n    layout = go.Layout(dict(title = columna + \" distribution\",\n                            plot_bgcolor  = \"rgb(243,243,243)\",\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                             title = columna,\n                                             zerolinewidth=1,\n                                             ticklen=5,\n                                             gridwidth=2\n                                            ),\n                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                             title = \"percent\",\n                                             zerolinewidth=1,\n                                             ticklen=5,\n                                             gridwidth=2\n                                            ),\n                           )\n                      )\n\n    data = [trace_churn,trace_nochurn]\n    fig = go.Figure(data = data,layout = layout)\n    py.iplot(fig)","1b5a872b":"data_num =  df.select_dtypes(['int64','float64']).columns.tolist()\nfor i in data_num:\n    HISTO(i)","3c5516c0":"df[data_num].describe().T","e11e7ca9":"df=df.drop('customerID',axis=1)\n\ntarget_col=['Churn']\n \ndata_num =  df.select_dtypes(['int64','float64']).columns.tolist()\ndata_cat = df.select_dtypes(['object']).columns.tolist()","12a00952":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nstd=StandardScaler()\nscaled=std.fit_transform(df[data_num])\nscaled=pd.DataFrame(scaled,columns=data_num)\n\ndf_original=df.copy()\ndf=df.drop(data_num,axis=1)\n\ndf=df.merge(scaled,left_index=True,right_index=True,how='left')","8b257c00":"le=LabelEncoder()\nfor i in data_cat:\n    df[i]=le.fit_transform(df[i])   \n                                                                    \ndf=pd.get_dummies(data=df,columns=data_cat,drop_first=True)","013e81c3":"df['Churn']=df['Churn_1']\ndf=df.drop('Churn_1',axis=1)","907003df":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score,recall_score\nfrom yellowbrick.classifier import DiscriminationThreshold","609c4534":"train,test=train_test_split(df,test_size=.25, random_state =111)\n\ncols    = [i for i in df.columns if i not in target_col]\n\ntrain_X = train[cols]\ntrain_Y = train[target_col]\ntest_X  = test[cols]\ntest_Y  = test[target_col]","b5cb13fc":"from imblearn.over_sampling import SMOTE\n\nos_smote = SMOTE(random_state = 0)\ntrain_X_smote,train_Y_smote = os_smote.fit_sample(train_X,train_Y)\ntrain_X_smote = pd.DataFrame(data = train_X_smote,columns=cols)\ntrain_Y_smote = pd.DataFrame(data = train_Y_smote,columns=target_col)","55d2608b":"from sklearn.metrics import cohen_kappa_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier","a85bb1a3":"logit = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)\n\ndt_classifier = DecisionTreeClassifier()\n\nknn = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n           weights='uniform')\n\nrfc   = RandomForestClassifier()\n\ngnb = GaussianNB(priors=None)\n\nsvc_lin  = SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n               decision_function_shape='ovr', degree=3, gamma=1.0, kernel='linear',\n               max_iter=-1, probability=True, random_state=None, shrinking=True,\n               tol=0.001, verbose=False)\n\nsvc_rbf  = SVC(C=1.0, kernel='rbf', \n               degree= 3, gamma=1.0, \n               coef0=0.0, shrinking=True,\n               probability=True,tol=0.001,\n               cache_size=200, class_weight=None,\n               verbose=False,max_iter= -1,\n               random_state=None)\n\nlgbm_c = LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n                        learning_rate=0.5, max_depth=7, min_child_samples=20,\n                        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n                        n_jobs=-1, num_leaves=500, objective='binary', random_state=None,\n                        reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n                        subsample_for_bin=200000, subsample_freq=0)\n\nxgc = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                    colsample_bytree=1, gamma=0, learning_rate=0.9, max_delta_step=0,\n                    max_depth = 7, min_child_weight=1, missing=None, n_estimators=100,\n                    n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n                    reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n                    silent=True, subsample=1)","d603745f":"from plotly.offline import plot\nimport plotly.figure_factory as ff\n\ndef model_report(model,training_x,testing_x,training_y,testing_y,name) :\n    model.fit(training_x,training_y)\n    predictions  = model.predict(testing_x)\n    accuracy     = accuracy_score(testing_y,predictions)\n    recallscore  = recall_score(testing_y,predictions)\n    precision    = precision_score(testing_y,predictions)\n    roc_auc      = roc_auc_score(testing_y,predictions)\n    f1score      = f1_score(testing_y,predictions) \n    kappa_metric = cohen_kappa_score(testing_y,predictions)\n    \n    df = pd.DataFrame({\"Model\"           : [name],\n                       \"Accuracy_score\"  : [accuracy],\n                       \"Recall_score\"    : [recallscore],\n                       \"Precision\"       : [precision],\n                       \"f1_score\"        : [f1score],\n                       \"Area_under_curve\": [roc_auc],\n                       \"Kappa_metric\"    : [kappa_metric],\n                      })\n    return df\n\n#outputs for every model\nmodel1 = model_report(logit,train_X_smote,test_X,train_Y_smote,test_Y,\n                      \"Logistic Regression\")\n\nmodel4 = model_report(dt_classifier,train_X_smote,test_X,train_Y_smote,test_Y,\n                      \"Decision Tree\")\n\nmodel5 = model_report(knn,train_X_smote,test_X,train_Y_smote,test_Y,\n                      \"KNN Classifier\")\n\nmodel6 = model_report(rfc,train_X_smote,test_X,train_Y_smote,test_Y,\n                      \"Random Forest Classifier\")\n\nmodel7 = model_report(gnb,train_X_smote,test_X,train_Y_smote,test_Y,\n                      \"Naive Bayes\")\n\nmodel8 = model_report(svc_lin,train_X_smote,test_X,train_Y_smote,test_Y,\n                      \"SVM Classifier Linear\")\n\nmodel9 = model_report(svc_rbf,train_X_smote,test_X,train_Y_smote,test_Y,\n                      \"SVM Classifier RBF\")\n\nmodel10 = model_report(lgbm_c,train_X_smote,test_X,train_Y_smote,test_Y,\n                      \"LGBM Classifier\")\n\nmodel11 = model_report(xgc,train_X_smote,test_X,train_Y_smote,test_Y,\n                      \"XGBoost Classifier\")\n\n#concat all models\nmodel_performances = pd.concat([model1,\n                                model4,model5,model6,\n                                model7,model8,model9,\n                                model10,model11],axis = 0).reset_index()\n\nmodel_performances = model_performances.drop(columns = \"index\",axis =1)\n\ntable  = ff.create_table(np.round(model_performances,4))\npy.iplot(table)","05074ab7":"def output_tracer(metric,color) :\n    tracer = go.Bar(y = model_performances[\"Model\"] ,\n                    x = model_performances[metric],\n                    orientation = \"h\",name = metric ,\n                    marker = dict(line = dict(width =.7),\n                                  color = color)\n                   )\n    return tracer\n\nlayout = go.Layout(dict(title = \"Model performances\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"metric\",\n                                     zerolinewidth=1,\n                                     ticklen=5,gridwidth=2),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                        margin = dict(l = 250),\n                        height = 780\n                       )\n                  )\n\n\ntrace1  = output_tracer(\"Accuracy_score\",\"#6699FF\")\ntrace2  = output_tracer('Recall_score',\"red\")\ntrace3  = output_tracer('Precision',\"#33CC99\")\ntrace4  = output_tracer('f1_score',\"lightgrey\")\ntrace5  = output_tracer('Kappa_metric',\"#FFCC99\")\n\ndata = [trace1,trace2,trace3,trace4,trace5]\nfig = go.Figure(data=data,layout=layout)\npy.iplot(fig)","d9044fce":"import matplotlib.pyplot as plt\nimport itertools\n\nlst    = [logit,dt_classifier,knn,rfc,\n          gnb,svc_lin,svc_rbf,lgbm_c,xgc]\n\nlength = len(lst)\n\nmods   = ['Logistic Regression',\n          'Decision Tree','KNN Classifier','Random Forest Classifier',\"Naive Bayes\",\n          'SVM Classifier Linear','SVM Classifier RBF', 'LGBM Classifier',\n          'XGBoost Classifier']\n\nfig = plt.figure(figsize=(13,15))\nfig.set_facecolor(\"#F3F3F3\")\nfor i,j,k in itertools.zip_longest(lst,range(length),mods) :\n    plt.subplot(4,3,j+1)\n    predictions = i.predict(test_X)\n    conf_matrix = confusion_matrix(predictions,test_Y)\n    sns.heatmap(conf_matrix,annot=True,fmt = \"d\",square = True,\n                xticklabels=[\"not churn\",\"churn\"],\n                yticklabels=[\"not churn\",\"churn\"],\n                linewidths = 2,linecolor = \"w\",cmap = \"Set1\")\n    plt.title(k,color = \"b\")\n    plt.subplots_adjust(wspace = .3,hspace = .3)","c728ace8":"st    = [logit,dt_classifier,knn,rfc,\n          gnb,svc_lin,svc_rbf,lgbm_c,xgc]\n\nlength = len(lst)\n\nmods   = ['Logistic Regression',\n          'Decision Tree','KNN Classifier','Random Forest Classifier',\"Naive Bayes\",\n          'SVM Classifier Linear','SVM Classifier RBF', 'LGBM Classifier',\n          'XGBoost Classifier']\n\nplt.style.use(\"dark_background\")\nfig = plt.figure(figsize=(12,16))\nfig.set_facecolor(\"#F3F3F3\")\nfor i,j,k in itertools.zip_longest(lst,range(length),mods) :\n    qx = plt.subplot(4,3,j+1)\n    probabilities = i.predict_proba(test_X)\n    predictions   = i.predict(test_X)\n    fpr,tpr,thresholds = roc_curve(test_Y,probabilities[:,1])\n    plt.plot(fpr,tpr,linestyle = \"dotted\",\n             color = \"royalblue\",linewidth = 2,\n             label = \"AUC = \" + str(np.around(roc_auc_score(test_Y,predictions),3)))\n    plt.plot([0,1],[0,1],linestyle = \"dashed\",\n             color = \"orangered\",linewidth = 1.5)\n    plt.fill_between(fpr,tpr,alpha = .4)\n    plt.fill_between([0,1],[0,1],color = \"k\")\n    plt.legend(loc = \"lower right\",\n               prop = {\"size\" : 12})\n    qx.set_facecolor(\"k\")\n    plt.grid(True,alpha = .15)\n    plt.title(k,color = \"b\")\n    plt.xticks(np.arange(0,1,.3))\n    plt.yticks(np.arange(0,1,.3))","806ef35a":"* **Second transformation:** Encoding categorical variables","33d396c8":"### Model performance metrics","85856de7":"Since we've seen that the number of customers that churned (1.869) is significantly higher than the number of non-churned (5.174) . **We are going to use smote to oversample the churned class** ","8d352c49":"# STEP 1 : DATA PREPROCESSING","e02fc3d6":"here is a link with all you need to know about [binary classification metrics](https:\/\/towardsdatascience.com\/the-ultimate-guide-to-binary-classification-metrics-c25c3627dd0a):","86e56153":"# STEP 0 : EXPLORATORY DATA ANALYSIS (EDA)","2a9dd75f":"Any comments or suggestions are very welcome.","1f826fe0":"### Conclusions and next steps","9fa04438":"# STEP 2 : MODELLING","382da18b":"### Compare confusion matrices","2b222bdb":"### Compare ROC Curves matrices","20314d5e":"Let's take a look at how **numerical variables** plot when we compare churn and non-churn customers:","5f697cba":"Ok, we begin by importing a set of packages we'll be needing:","5c37e1ab":"Let's get an idea of what the **distribution of our numerical variables** looks like:","cac58bc6":"Let's take a look at how **categorical variables** plot when we compare churn and non-churn customers:","cd518935":"# GUIDE FOR BINARY CLASSIFICATION PROBLEMS","8ed3099e":"First of all, let's take a look a our data.","a57c5e2c":"### Models","ce31e5e2":"In this notebook I basically simplify the great work done in the following notebook:\n* [Telecom Customer Churn Prediction](https:\/\/www.kaggle.com\/pavanraj159\/telecom-customer-churn-prediction\/notebook)\n\n","335e58fe":"* It would be interesting to try without using SMOTE\n* Once we figure out which algorithm seems to work best, we could try using GridSearch to find the best parameters\n* We could use k fold cross validation for a more robust solution","47087a2c":"### Compare model metrics","c2795ab2":"* **First transformation:** Normalizing numeric variables","a0574176":"The problem we will be solving is this:  [Telco Customer Churn](https:\/\/www.kaggle.com\/blastchar\/telco-customer-churn)","c7fda64a":"We can already see some little transformations we can make, so before we do any exploratory analysis, let's get them out of the way.\nApparently we don't have any missing data, but there are some columns that are not of the right type. **'SeniorCitizen'** should be an 'object' and **'TotalCharges'** a 'float'. Let's apply this and other transformations that seem appropriate:","dfae196c":"Importing the data:","73f7b287":"Quick look at the numbers of the target variable **'Churn'**","d50af0d2":"In this notebook I will solve a classification problem with two targets. **The goal for this notebook is to give a idea of the steps we need to follow to solve a binary classification problem** and to show some interesting techniques to do it."}}