{"cell_type":{"b82e8272":"code","8ddb96a0":"code","e5f9c539":"code","46a41304":"code","a848b7a1":"code","aaa86dd3":"code","a058b6d9":"code","7a6479bd":"code","53c9d6b9":"code","cd20460d":"code","4ffafd3a":"code","ac3916d6":"code","4f83f9ae":"code","f6cf91ee":"code","b0d2f89b":"code","67fdb9e9":"code","70179f1c":"code","c9443c1e":"code","1adf935b":"code","945cb341":"code","6b51eb05":"code","ab82782c":"code","b69fba89":"code","3c86c5e1":"code","3429b573":"code","490984e3":"code","61b52734":"code","a7b1489f":"code","64172e6b":"code","7627e89c":"code","aad7eb54":"code","abe66ee1":"code","f5d8d6f0":"code","d8d0bdef":"code","2de67fd4":"code","4ca06611":"code","ae04f21d":"code","58d60fc3":"markdown","91c2dae3":"markdown","73f55608":"markdown","32461de6":"markdown","b163bd9b":"markdown","1aa24000":"markdown","ead35693":"markdown","99c132fc":"markdown","d807c37c":"markdown","9f9dafdd":"markdown","0a2c4adf":"markdown","903bfd65":"markdown","7ed10e58":"markdown","0f3f9961":"markdown","a7838049":"markdown","ee2809dc":"markdown","6940aa6f":"markdown","67c14efc":"markdown","8c9b3b6c":"markdown","9beb9394":"markdown","225db538":"markdown","cec3ad88":"markdown","47bf60cc":"markdown","21582983":"markdown","b4f323d3":"markdown","2ff9d6fa":"markdown","93e68aa1":"markdown","6467e162":"markdown","25f937f6":"markdown","0762df0e":"markdown","ad5a389d":"markdown","7e2ec450":"markdown","39df6c0d":"markdown","aa31eddf":"markdown","45a2d0b5":"markdown","eb5680c9":"markdown"},"source":{"b82e8272":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","8ddb96a0":"test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n\n# Para evitar perder informa\u00e7\u00f5es possivelmente \u00fateis,\n# \u00e9 interessantes fazer uma c\u00f3pia do banco de dados de treino\ntrain_c = train.copy()\n\ntrain.describe(include = 'all')","e5f9c539":"print('Train data columns')\nprint(train.columns)\nprint('-'*100)\nprint('Test data columns')\nprint(test.columns)","46a41304":"train.head()","a848b7a1":"print('Train data types')\nprint(train.info())\nprint('-'*50)\nprint('Test data types')\nprint(test.info())","aaa86dd3":"print('Train data NaN Values')\nprint(train.isnull().sum())\nprint('-'*50)\nprint('Test data NaN Values')\nprint(test.isnull().sum())\n","a058b6d9":"# Retirando as features sem utilidade\nfor data in [train_c, test]:\n    data = data.drop(['Cabin','Ticket'], axis = 1, inplace = True)\n\n#podemos retirar \"PassengerId\" do banco de dados de teste\ntrain_c = train_c.drop([\"PassengerId\"], axis = 1)","7a6479bd":"#Preenchendo valores nulos\nfor data in [train_c, test]:\n    data['Age'].fillna(data['Age'].median(), inplace = True)\n    data['Fare'].fillna(data['Fare'].median(), inplace = True)\n    data['Embarked'].fillna(data['Embarked'].mode()[0], inplace = True)\n    \nprint('Train data NaN Values')\nprint(train_c.isnull().sum())\nprint('-'*50)\nprint('Test data NaN Values')\nprint(test.isnull().sum())","53c9d6b9":"for data in [train_c,test]:\n    #Utilizando cut para obter intervalos com o mesmo tamanho (https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.cut.html)\n    data['Age_group'] = pd.cut(data['Age'].astype(int), 5)\n    \n    #Utilizando qcut para obter intervalos com o mesmo n\u00famero de elementos (https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.qcut.html)\n    data['Fare_group'] = pd.qcut(data['Fare'], 6)","cd20460d":"#Relembrando quais Features s\u00e3o categ\u00f3ricas\ntrain_c.info()","4ffafd3a":"#Realizando One-Hot-encode\nfrom sklearn.preprocessing import OneHotEncoder\n\nonehot = OneHotEncoder(sparse = False)\n\n#Criando fun\u00e7\u00e3o para realizar o encoding das vari\u00e1veis\ndef OH_encoder(data,cols):\n    data_encoded = data.copy()\n    for col in cols:      \n        #Criando colunas para One-hot-encode\n        OH_cols = pd.DataFrame(onehot.fit_transform(data[[col]]),dtype = 'int')\n        \n        #Nomeando as colunas\n        OH_cols.columns = onehot.get_feature_names([col])\n        \n        #Adicionando as novas colunas codificadas ao dataframe\n        data_encoded = data_encoded.drop([col], axis = 1)\n        data_encoded = pd.concat([data_encoded,OH_cols], axis = 1)\n    return data_encoded\n\n#Realizando o encoding dos dataframes\ntrain_encoded = OH_encoder(train_c,[\"Sex\",\"Embarked\"])\ntest_encoded = OH_encoder(test,[\"Sex\",\"Embarked\"])","ac3916d6":"#Realizando Label encoding\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel = LabelEncoder()\n\nfor data in [train_encoded,test_encoded]:\n    data[\"Age_group_code\"] = label.fit_transform(data[\"Age_group\"])\n    data[\"Fare_group_code\"] = label.fit_transform(data[\"Fare_group\"])\n    data = data.drop([\"Age_group\",\"Fare_group\",\"Name\"], axis = 1, inplace = True)\n    \ntrain_encoded.head()","4f83f9ae":"#Criando e avaliando modelo\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nrandomforest_model1 = RandomForestClassifier(random_state = 0)\n\nX_model1 = train_encoded.drop('Survived', axis = 1)\ny_model1 = train_encoded['Survived']\n\nX_train,X_valid,y_train,y_valid = train_test_split(X_model1,y_model1,test_size = 0.2,random_state = 0)\n\nrandomforest_model1.fit(X_train,y_train)\npreds = randomforest_model1.predict(X_valid)\n\naccuracy = round(accuracy_score(preds,y_valid)*100,2)\nprint(accuracy)","f6cf91ee":"#Restaurando valores nulos originais do dataframe train_c nas colunas \"Age\" e \"Embarked\"\ntrain_c_na = train_c.copy()\ntrain_c_na = train_c_na.drop([\"Age\",\"Embarked\"], axis = 1)\ntrain_c_na = pd.concat([train_c_na,train[[\"Age\",\"Embarked\"]]],axis = 1)\n\ntrain_c_na.isnull().sum()","b0d2f89b":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\ndef pipeline_maker(data, encoder, model, numerical_imputer = SimpleImputer(),\n                   categorical_imputer = SimpleImputer(strategy = 'most_frequent')):\n    \n    X_pipe = data.drop('Survived', axis = 1)\n    y_pipe = data['Survived']\n\n    X_train_pipe,X_valid_pipe,y_train_pipe,y_valid_pipe = train_test_split(X_pipe,y_pipe,\n                                                                           test_size = 0.2,\n                                                                           random_state = 0)\n\n    #Criando os passos do preprocessor, para features num\u00e9ricas e categ\u00f3ricas\n    num_transf = numerical_imputer\n    cat_transf = Pipeline(steps = [('imputer', categorical_imputer),\n                                               ('encoder',encoder)])\n    preprocessor = ColumnTransformer(transformers = [\n                                                ('num',num_transf,['Age']),\n                                                ('cat',cat_transf,['Age_group','Fare_group',\n                                                                          'Sex','Embarked'])])\n    \n    #Criando pipeline e gerando predi\u00e7\u00f5es\n    pipe = Pipeline(steps = [('preprocessor', preprocessor),\n                                         ('model', model)])\n    pipe.fit(X_train_pipe,y_train_pipe)\n    preds_pipe = pipe.predict(X_valid_pipe)\n\n    #Avaliando as predi\u00e7\u00f5es\n    accuracy = round(accuracy_score(preds_pipe,y_valid_pipe)*100,2)\n    return accuracy, preds_pipe","67fdb9e9":"#Imputers\nmean_imputer = SimpleImputer()\nmostfreq_imputer = SimpleImputer(strategy = 'most_frequent')\nmedian_imputer = SimpleImputer(strategy = 'median')\nzero_imputer = SimpleImputer(strategy = 'constant', fill_value = 0)\nimputers = [mean_imputer, mostfreq_imputer, median_imputer, zero_imputer]\n\n#Encoders\nfrom sklearn.preprocessing import OrdinalEncoder\nonehot_encoder = OneHotEncoder(sparse = False)\nordinal_encoder = OrdinalEncoder()\nencoders = [onehot_encoder, ordinal_encoder]\n\n#model\nrandomforest = RandomForestClassifier(random_state = 0)","70179f1c":"for encoder in encoders:\n    for imputer in imputers:\n        accuracy,_ = pipeline_maker(train_c_na, encoder, randomforest, numerical_imputer = imputer)\n        print(\"pipe_\"+str(encoder)+'_'+str(imputer)+'='+str(accuracy))","c9443c1e":"from sklearn.model_selection import cross_val_score\n#Criando fun\u00e7\u00e3o para gerar avalia\u00e7\u00e3o\ndef cross_validation(model, X, y):\n    scores = cross_val_score(model, X, y, cv = 5, scoring = 'accuracy')\n    return round(scores.mean()*100,2)\n\n#Gerando avalia\u00e7\u00e3o do modelo mesclado \naccuracy_crossval = cross_validation(randomforest_model1, X_model1, y_model1)\nprint(accuracy_crossval)","1adf935b":"from sklearn.ensemble import GradientBoostingClassifier\n\n#Criando Fun\u00e7\u00e3o Gradient Boosting classifier para gerar e avaliar um modelo \ndef make_gbc(X,y):\n    gbclass = GradientBoostingClassifier(random_state = 0, n_iter_no_change = 100) \n    gbclass.fit(X,y)\n    score = cross_validation(gbclass, X, y)\n    return score, gbclass\n\nX_model2 = X_model1\ny_model2 = y_model1\n\nscore,gbclass = make_gbc(X_model2,y_model2)\nprint(score)","945cb341":"#Obtendo a pontua\u00e7\u00e3o MI de cada feature\nfrom sklearn.feature_selection import mutual_info_classif\n\n#Definindo as fun\u00e7\u00f5es discretas\ndiscrete_features = train_encoded.drop(['Age','Fare'], axis = 1)\n\n#Fu\u00e7\u00e3o para fazer gr\u00e1ficos de MI score\ndef make_MI (X, y, discrete_features):\n    \n    #Produzindo MI scores\n    mi_scores = mutual_info_classif(X, y, discrete_features = discrete_features )\n    mi_scores = pd.Series(mi_scores, index = X.columns)\n    mi_scores = mi_scores.sort_values(ascending = False)\n    \n    #Prduzindo gr\u00e1fico\n    sns.set_style('whitegrid')\n    scores = mi_scores.sort_values(ascending = True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n    \n    return mi_scores\n\nX = train_encoded.drop(['Survived'],axis = 1)\ny = train_encoded['Survived']\n\nmake_MI(X, y, discrete_features)","6b51eb05":"fig, axs = plt.subplots(1,2, figsize = (18,7))\n\n#Criando gr\u00e1ficos da distribui\u00e7\u00e3o de \"Fare\"\naxs[0].set_title('Distribui\u00e7\u00e3o das Taxas')\nsns.histplot(x = 'Fare', kde = True, data = train_c, ax = axs[0]);\naxs[1].set_title('Rela\u00e7\u00e3o Entre Taxas e Sobreviventes ')\nsns.histplot(x = 'Fare',hue = 'Survived', kde = True, data = train_c, ax = axs[1]);","ab82782c":"fig, axs = plt.subplots(2,2, figsize = (18,7), sharex = True, sharey = True)\n\n#Criando gr\u00e1ficos da distribui\u00e7\u00e3o de \"Fare\"\naxs[0,0].set_title('Sexo em rela\u00e7\u00e3o \u00e0 idade')\nsns.histplot(x = 'Age',hue = 'Sex', kde = True, data = train_c, ax = axs[0,0], palette = \"YlOrRd\");\n\naxs[0,1].set_title('Distribui\u00e7\u00e3o de sobreviventes por idade, comparando os sexos')\nsns.histplot(data = train_c.loc[train_c['Survived']==1], x = 'Age', hue = 'Sex', kde = True, ax = axs[0,1], palette = \"YlOrRd_r\");\n\naxs[1,0].set_title('Distribui\u00e7\u00e3o de sobreviv\u00eancia de homens')\nsns.histplot(data = train_c.loc[train_c['Sex']=='male'], x = 'Age', hue = 'Survived', kde = True, ax = axs[1,0]);\n\naxs[1,1].set_title('Distribui\u00e7\u00e3o de sobreviv\u00eancia de mulheres')\nsns.histplot(data = train_c.loc[train_c['Sex']=='female'], x = 'Age', hue = 'Survived', kde = True, ax = axs[1,1]);","b69fba89":"# Adicionando \"Name\" \u00e0 train_encoded\ntrain_encoded = pd.concat([train_encoded,train_c['Name']], axis = 1)\n\nfor data in [train_encoded,test]:\n    #Utilizando o m\u00e9todo string split do pandas para retirar informa\u00e7\u00f5es de texto \n    #(https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.Series.str.split.html)\n    data['Title'] = data['Name'].str.split(',',expand = True)[1].str.split('.',expand = True)[0]\n    data.drop(\"Name\", axis = 1, inplace = True)\n\n#Demonstrando quais os valores da coluna\nprint(train_encoded['Title'].value_counts())","3c86c5e1":"for data in [train_encoded,test]:\n    #Criando Feature para contar n\u00famero de integrantes da fam\u00edlia\n    data['Familysize'] = data['SibSp'] + data['Parch']\n    \n    #Criando Feature para identificar se a pessoa est\u00e1 ou n\u00e3o sozinha no navio\n    data['IsAlone'] = 1\n    for i in range(len(data.index)):\n        if data.loc[i,'Familysize'] != 0:\n            data.loc[i,'IsAlone'] = 0\n\ntrain_encoded.head()","3429b573":"#Visualizando a intera\u00e7\u00e3o entre \"Fare\" e \"Age\"\nsns.scatterplot(x='Fare',y='Age',data = train_c);","490984e3":"from sklearn.cluster import KMeans\n\n#Criando dataframe com \"Age\" e \"Fare\" sem outliers\nfare_age_group = train_c.loc[train_c['Fare'] < 500,['Fare','Age']]\n\n#Criando feature a partir da intera\u00e7\u00e3o entre \"Age\" e \"Fare\"\nkmeans = KMeans(n_clusters = 6, random_state = 0)\nfare_age_group['Fare_age_group'] = kmeans.fit_predict(fare_age_group)\nfare_age_group['Fare_age_group'] = fare_age_group['Fare_age_group'].astype('int')\n\nfare_age_group","61b52734":"#Adicionando feature criada ao banco de dados de treino\ntrain_c['Fare_age_group'] = fare_age_group['Fare_age_group']\n\n#Criando visualiza\u00e7\u00e3o para entendimento da nova feature\nfig, axs = plt.subplots(1,2, figsize = (15,7))\nsns.barplot(x = 'Fare_age_group', y = 'Survived', data = train_c, ax = axs[1], palette = \"rainbow_r\")\nsns.scatterplot(x='Fare',y='Age', hue = 'Fare_age_group', data = train_c, ax = axs[0],palette = \"rainbow_r\" );","a7b1489f":"#Descobrindo quem s\u00e3o so Outliers\ntrain_c.loc[train_c['Fare_age_group'].isnull() == True,:]","64172e6b":"#Selecionando um grupo para os Outliers\ntrain_c.loc[train_c['Fare_age_group'].isnull() == True,['Fare_age_group']] = 1\ntrain_c.loc[[258,679,737]]","7627e89c":"#Adicionando a nova feature aos banco de dado train_encoded\ntrain_encoded['Fare_age_group'] = train_c['Fare_age_group']\ntrain_encoded","aad7eb54":"from scipy import stats\nfrom sklearn.preprocessing import MinMaxScaler\n\n#Criando scaller \nscaller = MinMaxScaler(copy = False)\n\n#Criando dataframe para gravar as features ap\u00f3s realizar scalling\nstand_data = pd.DataFrame()\n\n#Loop para a realiza\u00e7\u00e3o do scalling\nfor column in ['Fare','Age']:\n    stand_data[[column]] = train_c[[column]]\n    stand_data[[column]] = scaller.fit_transform(stand_data[[column]])\n    \n#Gerando gr\u00e1ficos para mostrar a distribui\u00e7\u00e3o das features ap\u00f3s o scalling \nfig, axs = plt.subplots(1,2, figsize = (18,7))\n\nsns.histplot(x = 'Age', kde = True, data = stand_data, ax = axs[0]);\nsns.histplot(x = 'Fare', kde = True, data = stand_data, ax = axs[1]);\n","abe66ee1":"from sklearn.decomposition import PCA\n\n#Realizando o PCA de \"Fare\" e \"Age\" \npca = PCA()\npca_data = pca.fit_transform(stand_data)\n\n#Criando um dataframe para armazenar as features criadas a partir do PCA\ncolumns = ['PC1','PC2']\npca_data = pd.DataFrame(pca_data, columns = columns)\n\n#Criando dataframe para identificar como os eixos foram afetados pelo PCA\nloadings = pd.DataFrame(pca.components_.T, columns = columns, index = [\"Fare\",\"Age\"])\nloadings","f5d8d6f0":"#Gerando MI scores das features PC1 e PC2\nmake_MI(pca_data, train_c[\"Survived\"], discrete_features = False)","d8d0bdef":"from category_encoders import MEstimateEncoder\n\nencoder = MEstimateEncoder(cols = ['Title'], m = 5)\n\ntitle = encoder.fit_transform(train_encoded['Title'], train_encoded['Survived'])\ntitle","2de67fd4":"\nfig, ax = plt.subplots(figsize = (9,7))\n\nsns.distplot(train_encoded['Survived'], kde=False, norm_hist=True)\nsns.kdeplot(title['Title'], color='r', ax=ax)\nax.set_xlabel(\"Survived\")\nax.legend(labels=['Title', 'Survived']);","4ca06611":"train_encoded['Title-encoded'] = title\ntrain_encoded.info()","ae04f21d":"X_enc = train_encoded.loc[:,['Fare_age_group','Title-encoded']]\ny_enc = train_encoded['Survived']\nmake_MI(X_enc, y_enc, discrete_features = False )","58d60fc3":"### Criando novas Features a partir de opera\u00e7\u00f5es matem\u00e1ticas\n* Criaremos uma feature com o tamanho da fam\u00edlia, somando o n\u00famero de pais e filhos com o numero de parentes dos passageiros a bordo\n* Outra Feature que ser\u00e1 criada ser\u00e1 a que ir\u00e1 identificar se o passageiro esta sozinho ou n\u00e3o, a partir do n\u00famero de pessoas na fam\u00edlia","91c2dae3":"# Data Cleaning\n### A partir daqui a an\u00e1lise ser\u00e1 mais aprofundada em rela\u00e7\u00e3o \u00e0 primeira vers\u00e3o do c\u00f3digo\n\n### Retirar Features n\u00e3o \u00fateis: \n   * Algumas Features podem ser retiradas do banco de dados, na caluna \"Cabin\", por exemplo, existem um numero muito alto de valores nulos em ambos os dataset, ent\u00e3o sua retirada \u00e9 justific\u00e1vel\n   * No cado da Feature \"Ticket\", seus valores s\u00e3o uma mistura entre numeros e string, al\u00e9m de que n\u00e3o parece acrescentar informa\u00e7\u00e3o relevante ao modelo, a primeira vista, o que tamb\u00e9m justifica sua retirada","73f55608":"Pode-se notar que o n\u00famero de mulheres sobreviventes supera a de homens em praticamente todas as idades, a pesar de o numero total de homens superar o de mulheres tamb\u00e9m por toda a distribui\u00e7\u00e3o. Otro ponto importante \u00e9 que a distribui\u00e7\u00e3o das idades dos passageiros a bordo se aproxima de uma normal, j\u00e1 que quantidade de pessoas com idade menor que a m\u00e9dia \u00e9 pr\u00f3xima \u00e0 quantidade com idade maior.","32461de6":"Pode-se perceber que estas pessoas tem a mesma faixa de idade, entre 35 e 36 anos, e o valor de sua taxa \u00e9 o mesmo, 512.33, ou seja, s\u00e3o adultos com uma taxa muit alta, portanto, o grupo que mais se adequa \u00e9 o grupo 1. Outro fator a ser levado em considera\u00e7\u00e3o \u00e9 o alto \u00edndice de sobreviv\u00eancia desse grupo e como os 3 passageiros estudados sobreviveram, eles seguem o padr\u00e3o, diminuindo a vari\u00e2ncia da feature ","b163bd9b":"## Criando Pipelines\nNeste passo ser\u00e3o utilizadas pipelines para facilitar a contru\u00e7\u00e3o de novos modelos e teste de estrat\u00e9gias distintas de manipula\u00e7\u00e3o de dados\n* Restaurar valores nulos originais\n* Criar imputers para novas estrat\u00e9gias de substitui\u00e7\u00e3o de NaN\n* Criar column transformers para testar novas maneiras de lidar com vari\u00e1veis categ\u00f3ricas\n* Criar fun\u00e7\u00e3o que construa pipelines com diferentes estrat\u00e9gias\n* testar e comparar quais estrat\u00e9gias geraram melhores predi\u00e7\u00f5es","1aa24000":"### Fun\u00e7\u00e3o para avaliar estrat\u00e9gias\nAgora, ir\u00e1 ser criada uma fun\u00e7\u00e3o que construa e avalie modelos utilizando pipeline a partir dos par\u00e2metros: \n* ```data```: Banco de dados a partir do qual ser\u00e1 produzido o modelo\n* ```encoder```: a estrat\u00e9gia de encoding de vari\u00e1veis categ\u00f3ricas, como one-hot ou label encoder\n* ```model```: o algor\u00edtimo que ir\u00e1 produzir o modelo, inicialmente ser\u00e1 o random forest classifier\n* ```numerical_imputer```: a estrat\u00e9gia usada para substituir valores nulos nas features num\u00e9ricas, como \"mean\" ou \"median\"\n* ```categorical_imputer```: estrat\u00e9gia para preencher valores nulos nas features categ\u00f3ricas, aqui ser\u00e1 usada apenas \"most frequent\" que \u00e9 substituir pelo valor que mais aparece no banco de dados","ead35693":"### Avali\u00e7\u00e3o dos testes\nCom o valor da precis\u00e3o dos modelo produzidos nas pipelines pode-se concluir que nenhuma das estrat\u00e9gias, utilizadas separadamente, \u00e9 superior \u00e0 mescla entre elas. Portanto, a manipula\u00e7\u00e3o utilizada anteriormente, por enquanto, \u00e9 a melhor maneira de produzir um modelo que conseguimos.  ","99c132fc":"Como pode-se notar, a pontua\u00e7\u00e3o MI dessas features \u00e9 muito baixa, a pesar de PC2 ser levemente mais determinante para o target que PC1, n\u00e3o faz sentido acrescentar qualquer uma delas ao banco de dados de teste ","d807c37c":"## Target Encoding\nO banco de dados que esta sendo utilizado para o treino dos algor\u00edtimos e gera\u00e7\u00e3o do modelo de IA \u00e9 train_encoded, nele, em todas as features categ\u00f3ricas j\u00e1 foi realizado alguma estrat\u00e9gia de ecoding, seja one-hot ou label, h\u00e1 apenas uma que permanece como object data type, que \u00e9 \"Title\". Como nela existem v\u00e1rios valores diferentes com uma quantidade bem variada de ocorr\u00eancias, sendo que alguns, como \"Mr\" e \"Miss\", aparecem diversas vezes, e outros, como \"Capt\" e \"Don\", aparecem apenas uma vez, utilizar M estimate como target encoder, parece promissor.","9f9dafdd":"## Criando novas features a partir de existentes\n### Retirar informa\u00e7\u00f5es dos nomes registrados\nA feature \"Names\" cont\u00e9m valores que podem ser confusos, j\u00e1 que existem varias maneiras de se denominar as pessoas, como apelidos e abrevia\u00e7\u00f5es. Contudo, h\u00e1 uma informa\u00e7\u00e3o que pode ser \u00fatil, que \u00e9 o t\u00edtulo da pessoa registrada, como \"Miss.\" ou \"Mr.\", portanto agora ir\u00eamos obter essas informa\u00e7\u00f5es criando uma nova feature\n* Adicionar feature \"Name\" a train_encoded\n* Criar Feature \"Title\", contendo apenas o t\u00edtulo das pessoas, a partir da Feature \"Name\"\n* Retirar feature \"Name\" de train_encoded e test\n* Mostrar quais s\u00e3o os t\u00edtulos obtidos\n","0a2c4adf":"Pelos loadings, \u00e9 poss\u00edvel notar que os eixos praticamente n\u00e3o interagiram, j\u00e1 que PC1 representa, quase completamente, a feature \"Age\" e PC2 representa \"Fare\", ou seja, n\u00e3o houve um intera\u00e7\u00e3o vis\u00edvel entre os eixos","903bfd65":"Pelo gr\u00e1fico \"Distribui\u00e7\u00e3o das Taxas\" \u00e9 poss\u00edvel perceber que a maioria das taxas foram de menos de 50 dolares, contudo, ao passar deste valor, a partir do gr\u00e1fico \"Rela\u00e7\u00e3o Entre Taxas e Sobreviventes\", pode-se notar que o n\u00famero de sobreviv\u00eantes supera, em m\u00e9dia, o n\u00famero de mortes. Outro fato que vale a oenas ser destacado, \u00e9 que \"Fare\" n\u00e3o possui distribui\u00e7\u00e3o normal.","7ed10e58":"## Cross Validation\nPara melhor avaliar o mdelo constru\u00eddo at\u00e9 agora, ser\u00e1 utilizado o cross validation. A partir de suas pontua\u00e7\u00f5es ser\u00e1 feita a m\u00e9dia para avaliar o melhor modelo que constr\u00edmos at\u00e9 agora e os pr\u00f3ximos que ser\u00e3o feitos","0f3f9961":"### Criar categorias para vari\u00e1veis cont\u00ednuas:\n   * As vari\u00e1veis \"Age\" e \"Fare\", s\u00e3o cont\u00ednuas dessa maneira pode ser \u00fatil agrupa-las de alguma forma para facilitar sua visualiza\u00e7\u00e3o e o treinamento do modelo\n   * Para \"Age\" agruparemos em intervalos de idade previamente determinados, ent\u00e3o cada grupo ter\u00e1 uma quantidade de dados diferente do outro\n   * No caso de \"Fare\" o agrupamento ocorrer\u00e1 de forma que todos os intervalos contenham o mesmo n\u00famero de dados, ou seja, o tamanho do intervalo pode variar, j\u00e1 que a distribui\u00e7\u00e3o dos valores se aproxima da normal ","a7838049":"# Feature Engineering\nAgora ocorrer\u00e1 a cria\u00e7\u00e3o de algumas features a partir das ionforma\u00e7\u00f5es j\u00e1 presentes no banco de dados, essas nocvas colunas ser\u00e3o criadas com base em an\u00e1lises estat\u00edsticas que ser\u00e3o realizadas nesta parte ","ee2809dc":"# Aprofundando a an\u00e1lise\n### A partir de agora, come\u00e7a a constru\u00e7\u00e3o e avalia\u00e7\u00e3o de alguns modelos utilizando diferentes estrat\u00e9gias de manipula\u00e7\u00e3o de dados e algor\u00edtimos de Machine Learning\n### Realizando encoding das Features categ\u00f3ricas\n* Primeiramente \u00e9 necess\u00e1rio relembrar quais s\u00e3o as features categ\u00f3ricas, principalemnte pelo fato de que foram criadas novas colunas\n* Definir Features que em que ir\u00e3o ser feitos ou Lable encode ou One-Hot-encode\n* Criar uma fun\u00e7\u00e3o que avalie um modelo de acordo com o enconde de cada Feature","6940aa6f":"Nota-se que, nos grupos 0, 4 e 5, a taxa \u00e9 menor que 50, nestes grupos, a chance de sobreviver \u00e9 notavelmente mais baixa do que nos outros 3, que representam uma taxa maior que 50. Outro fator a ser notado \u00e9 que, o grupo 5, que represnta uma taxa ligeiramente maior que o 4, demonstra uma chance maior de sobrevivencia, isso tamb\u00e9m se deve ao fato de que este grupo contem as pessoas mais jovens, incluindo crian\u00e7as, que, normalmente, s\u00e3o salvas primeiro.","67c14efc":"### Teste de estrat\u00e9gias\nA paritir da fun\u00e7\u00e3o de cria\u00e7\u00e3o de pipelines e das estrat\u00e9gias de encoding e imputing das features, \u00e9 poss\u00edvel testar e avaliar a predi\u00e7\u00e3o feita por cada uma, separadamente. Com isso pode-se avaliar se alguma delas \u00e9 melhor para o modelo, comparando com a estrat\u00e9gia mesclada que j\u00e1 foi utilizada. ","8c9b3b6c":"## Realizando MI Score:\nAssim como no minicurso sobre [mutual information](https:\/\/www.kaggle.com\/ryanholbrook\/mutual-information) do kaggle, ser\u00e1 criado e analisado um gr\u00e1fico que demonstre a depend\u00eancia entre as features do banco de dados e o target","9beb9394":"### Features com valores nulos:\n   * As Features \"Age\", \"Embarked\" e \"Fare\", tamb\u00e9m apresentam NaN, contudo, cont\u00e9m informa\u00e7\u00f5es extremamente importantes para a constru\u00e7\u00e3o do modelo, logo, n\u00e3o ser\u00e3o retiradas por completo e seus valores nulos ser\u00e3o preenchidos\n   * Nas Features \"Age\" e \"Fare\", podemos substituir os valores nulos pela mediana, j\u00e1 que s\u00e3o vari\u00e1veis cont\u00ednuas\n   * J\u00e1 para a Feature \"Embarked\", podemos substituir pela moda, pois dessa forma ir\u00e1 interferir menos no treinamento do nosso modelo","225db538":"### 1. Stadardizing data\n* Importar a biblioteca [MinMaxScaler](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html) do sklearn\n* Criar dataframe para armazenar e visualizar os dados ap\u00f3s realiza\u00e7\u00e3o do standardizing\n* Realizar o scalling nas features \"Age\" e \"Fare\", do banco de dados de treino\n* Realizar PCA e analisar os loadings resultantes\n* Gerar e analisar o MI score das features criadas com PCA","cec3ad88":"## Importar e explorar os dados\n1. Transformar arquivos em dataframes\n2. Obter informa\u00e7\u00f5es estatisticas do arquvio de treino ","47bf60cc":"## Gradient Boosting\nAgora ser\u00e1 utilizado outro algor\u00edtimo para modelagem e predi\u00e7\u00e3o. XGBoost classifier funciona com as mesmas caracter\u00edsticas do XGBoost regressor, contudo \u00e9 usado para predi\u00e7\u00f5es de numeros discretos, como as predi\u00e7\u00f5es que queremos.","21582983":"## Principal Component Analysis\nPCA, assim como clustering, tamb\u00e9m \u00e9 unsupervised. Para facilitar sua aplica\u00e7\u00e3o tamb\u00e9m seram usados as features \"Age\" e \"Fare\", j\u00e1 que s\u00e3o cont\u00ednuas. Contudo, essas features n\u00e3o representam a mesma grandeza, portanto, ser\u00e1 necess\u00e1rio realizar o scalling, para que seja poss\u00edvel utilizar PCA para compara-las e avaliar se ser\u00e1 \u00faltil criar uma nova feature a partir de alguma intera\u00e7\u00e3o entre elas","b4f323d3":"### 1. Criar nova features a partir da intera\u00e7\u00e3o entre \"Fare\" e \"Age\"\n* Analisar distribui\u00e7\u00e3o de \"Age\" em rela\u00e7\u00e3o a \"Fare\"\n* Verificar a exist\u00eancia, e remover, outliers que possam prejudicar a cria\u00e7\u00e3o de clusters\n* Criar uma feature que seja o agrupamento da intera\u00e7\u00e3o entre \"Fare\" e \"Age\"\n* Visualizar e analisar gr\u00e1ficos que demonstrem como essa feature se comporta","2ff9d6fa":"# Data Analysis\n1. Quais s\u00e3o as features categ\u00f3ricas e quais s\u00e3o as num\u00e9ricas\n2. O tipo de dado de cada feature\n3. Quais features contem NaN","93e68aa1":"As features \"Name\", \"Sex\", \"Embarked\", \"Age_group\", \"Fare_group\", n\u00e3o s\u00e3o num\u00e9ricas.\nDessa forma pode-se afirmar que as outras 4 features s\u00e3o categ\u00f3ricas em que vale a penas Realizar encode.\n* Features Categ\u00f3ricas: ```Sex```, ```Embarked```, ```Age_group```, ```Fare_group```\n* As features \"Age_group\" e \"Fare_group\" tem uma sequ\u00eancia clara, o que justifica Lable encode como primeira abordagem, nas outras pode-se usar One-Hot-encode. mais a frente ser\u00e3o testadas e avaliadas outras abordagens para o encode dessas Features  ","6467e162":"## Clustering\nClustering, \u00e9 uma estrat\u00e9gia que n\u00e3o utiliza o target no seu funcionamento, ou seja, \u00e9 considerada unsupervised.\nExistem duas features com valores reais no banco de dados, s\u00e3o elas \"Age\" e \"Fare\", para ambos os casos foram criadas features para agrupa-las de forma que os algor\u00edtmos de machine learning capturem melhor seus padr\u00f5es. Anteriormente foram utilizados os m\u00e9todos cut e qcut para relizar este processo, agora isso ser\u00e1 feito por meio de [Clustering com K-means](https:\/\/www.kaggle.com\/ryanholbrook\/clustering-with-k-means). Esse m\u00e9todo possibilitar\u00e1 que possa ser feito um agrupamento da intera\u00e7\u00e3o entre \"Fare\" e \"Age\", facilitando a an\u00e1lise da chance de sobreviv\u00eancia de um passageiro.  ","25f937f6":"Pode-se perceber que existem tres passageiros que pagaram uma Taxa muito maior que o resto, mais de 500, neste caso, esses dados s\u00e3o considerados outliers, j\u00e1 que est\u00e3o extremamente fora do padr\u00e3o, logo devem ser inicialmente retirados para a cria\u00e7\u00e3o dos clusters, depois, devem ser estudados para que sejam incluidos em um grupo, manualmente.","0762df0e":"### Avaliando precis\u00e3o do modelo\nAgora ser\u00e1 criado e avaliado um modelo com as novas Features, dessa forma poder\u00eamos avaliar se a an\u00e1lise e manipula\u00e7\u00e3o mais aprofundada resultou em uma melhor precis\u00e3o do modelo.","ad5a389d":"## Analisando Resultados do MI score\nPode-se notar que a fature \"Fare\" obteve o melhor resultado, as feature \"Sex\" e \"Age\" tamb\u00e9m demonstraram uma rela\u00e7\u00e3o relativamente alta com o target, dessa forma \u00e9 interessante gerar gr\u00e1ficos dessas features para que possam ser analisadas. Para faciliar a constru\u00e7\u00e3o dos gr\u00e1ficos, ser\u00e1 utilizado o banco de dados de treino original\n### 1. Distribui\u00e7\u00e3o da feature \"Fare\" e rela\u00e7\u00e3o dela com \"Survived\"\n* Criar gr\u00e1fico utilizando ```sns.distplot``` para analizar a distribui\u00e7\u00e3o das taxas pagas pelos passageiros\n* Criar gr\u00e1fico relacionando as taxas com a quantidade de pessoas que sobreviveram ou n\u00e3o\n* Utilizar ```plt.subplots``` para mostrar gr\u00e1fico um do lado do outro","7e2ec450":"### 2. Lidando com outliers\nAgora ser\u00e3o estudados os 3 passageiros que n\u00e3o foram agrupados, dessa forma ser\u00e1 selecionado um grupo manualmente para eles, com base nas semelha\u00e7as com os grupos j\u00e1 criados","39df6c0d":"### Separa\u00e7\u00e3o de Features\n1. *Categ\u00f3ricas:* Sex, Embarked\n2. *Num\u00e9ricas:* Age(Cont\u00ednuo), Sibsp(discreto), Parch(discreto), Fare(cont\u00ednuo), Pclass(discreto)","aa31eddf":"Lembrando que a avalia\u00e7\u00e3o da primeira ver\u00e7\u00e3o do modelo teve uma pontu\u00e7\u00e3o de 66.43, pode-se concluir que a an\u00e1lise e manipula\u00e7\u00e3o mais aprofundada de dados feita at\u00e9 aqui, gerou um n\u00edvel de precis\u00e3o bem maior. A partir de agora v\u00e3o ser testados outras estrat\u00e9gias e modelos, que ser\u00e3o avaliados comparando com produzido acima, afim de elaborar a melhor estrat\u00e9gia para a previs\u00e3o mais correta poss\u00edvel dos resultados","45a2d0b5":"### Definindo estrat\u00e9gias\nPara testar as estrat\u00e9gias, ir\u00e1 ser ciada uma lista de imputers e de encoders\n* ***Imputers:*** \n    * Mean: substitui a o valor nulo pela m\u00e9dia\n    * Most_frequent: substitui pelo valor mais frequente na feature\n    * Median: substitui pela mediana\n    * Zero: substitui todos os valores nulos por 0\n    * Todas essas estrat\u00e9gias est\u00e3o sipon\u00edveis [neste link](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.SimpleImputer.html)\n* ***Encoders:***\n    * One-hot encoder\n    * Ordinal encoder: funciona de forma semelhante ao label encoder, contudo \u00e9 direcionado \u00e0s features, diferente do lebal encoder que \u00e9 direcionado ao target. Mais informa\u00e7\u00f5es [neste link](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder)","eb5680c9":"### 2. Avaliar a influ\u00eancia do sexo e idade do passageiro, na chance de sobrevivencia\n* Criar gr\u00e1fico que mostre a distribui\u00e7\u00e3o do sexo em rela\u00e7\u00e3o \u00e0 idade do passageiro\n* Gr\u00e1fico que indique a distribui\u00e7\u00e3o de sobreviventes comparando homens e mulheres\n* Dois gr\u00e1ficos, 1 com a distribui\u00e7\u00e3o dos homen que morreram em rela\u00e7\u00e3o \u00e0 idade e um com as mulheres"}}