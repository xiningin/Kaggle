{"cell_type":{"20e62a88":"code","9bff0d35":"code","32fab33b":"code","ff688d7e":"code","47ec4cb5":"code","b2a19115":"code","03f9e2ac":"code","dd885ef0":"code","a6afc73b":"code","da17733f":"code","c66f7317":"code","62545f59":"code","587c0984":"code","243cd4e8":"code","3dd83f37":"code","383ea751":"code","bde5830f":"code","515ec40b":"code","994a9a6c":"code","20ca31e0":"code","739250c7":"code","d796660d":"code","b1cb33e4":"code","2cdd1e5d":"code","2b1a7c9d":"code","57381103":"code","8a8f68a6":"code","1d6c49f6":"code","39b04640":"code","5fdcc877":"code","11f00c29":"code","f36eeed0":"code","bbdbaa38":"code","16541715":"code","f841ebc7":"code","c90530e6":"code","9892bb05":"code","40e0352c":"code","51a003a9":"code","dbc3deaa":"code","c1624015":"code","d7e5b0a3":"code","00c1a77d":"code","74da5394":"code","df97c440":"code","8bb25ca1":"markdown","410f7aac":"markdown","38b47d13":"markdown","5028ba0f":"markdown","a21a0b19":"markdown","85b17656":"markdown","3553afa0":"markdown","8eefc389":"markdown","bbb5a45d":"markdown","bc387d88":"markdown","0fe492a7":"markdown","c5d8eb25":"markdown","7d9e6519":"markdown","59789cbd":"markdown","f745bed5":"markdown","ee9ae0b7":"markdown","70f743a5":"markdown","fad19af5":"markdown","0be5e5ba":"markdown","0cc5e2de":"markdown","709f5086":"markdown","76a42ea9":"markdown","cdd3c972":"markdown","be32f365":"markdown","03d88ca3":"markdown","686f5b05":"markdown","0d93579f":"markdown"},"source":{"20e62a88":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport sklearn\nimport math\nfrom IPython.display import display, HTML\n\n%matplotlib inline\nplt.style.use('seaborn')","9bff0d35":"# Salvando o arquivo de teste csv em um DataFrame\ndf_train = pd.read_csv(\"..\/input\/atividade-regressao-PMR3508\/train.csv\", na_values = '?')","32fab33b":"# Exibindo o formato do df\ndf_train.shape","ff688d7e":"# Usando o comando .info() para obter algumas informa\u00e7\u00f5es b\u00e1sicas do df\ndf_train.info()","47ec4cb5":"# Vendo quais s\u00e3o os tipos das colunas\u00c7\ndf_train.dtypes","b2a19115":"# Usando a fun\u00e7\u00e3o describe para ter uma no\u00e7\u00e3o inicial dos valores num\u00e9ricos e como est\u00e3o distribu\u00eddos na base.\ndf_train.describe()","03f9e2ac":"# Visualizando as 5 primeiras linhas do Dataframe:\ndf_train.head()","dd885ef0":"df_train.drop('Id', axis=1).hist(bins=50, figsize=(15,15))\n# df_train.hist(bins=50, figsize=(15,15))\nplt.show()","a6afc73b":"lat = df_train['latitude']\nlong = df_train['longitude']\n\n\n\nplt.figure(figsize=(19,10))\nplt.scatter(lat,long,c=df_train[\"median_house_value\"],cmap=\"plasma\")\nplt.xlabel(\"Latitude\")\nplt.ylabel(\"Longitude\")\n\nlat_sf = 37.773972\nlong_sf = -122.431297\nplt.scatter(lat_sf,long_sf,c=\"green\",s=1000,marker=\"s\",label=\"San Francisco\")\n\nlat_la = 34.052235\nlong_la = -118.243683\nplt.scatter(lat_la,long_la,c=\"red\",s=1000,marker=\"d\",label=\"Los Angeles\")\n\nplt.legend(labelspacing=3,handletextpad=1,prop={'size': 15})\n\n","da17733f":"plt.scatter(x=df_train['median_income'],y=df_train['median_house_value'])","c66f7317":"plt.scatter(x=df_train['median_age'],y=df_train['median_house_value'])","62545f59":"plt.scatter(x=df_train['total_rooms'],y=df_train['median_house_value'])","587c0984":"df_train['rooms_per_household'] = df_train['total_rooms']\/df_train['households']\ndf_train['bedrooms_per_room'] = df_train['total_bedrooms']\/df_train['total_rooms']\ndf_train['population_per_household'] = df_train['population']\/df_train['households']\ndf_train","243cd4e8":"plt.scatter(x=df_train['rooms_per_household'],y=df_train['median_house_value'])","3dd83f37":"plt.scatter(x=df_train['bedrooms_per_room'],y=df_train['median_house_value'])","383ea751":"plt.scatter(x=df_train['population_per_household'],y=df_train['median_house_value'])","bde5830f":"#Funcao para calcular a distancia euclidiana entre duas coordenadas A e B em lat\/long\ndef distancia_euclidiana2(LatA,LongA,LatB,LongB):\n    try:\n        dist = (6371*(np.arccos(np.cos(np.radians(90-LatB))*np.cos(np.radians(90-LatA))+np.sin(np.radians(90-LatB))*np.sin(np.radians(90-LatA))*np.cos(np.radians(LongB-LongA))))*1.15)\n    except:\n        dist = 100000\n    \n    dist_min = np.min(dist)\n    return dist_min","515ec40b":"df_train['distance_to_sf'] = df_train.apply(lambda row: distancia_euclidiana2(lat_sf,long_sf,row['latitude'],row['longitude']),axis=1)","994a9a6c":"df_train['distance_to_la'] = df_train.apply(lambda row: distancia_euclidiana2(lat_la,long_la,row['latitude'],row['longitude']),axis=1)","20ca31e0":"corr_train = df_train.corr()\ncorr_train['median_house_value'].sort_values(ascending=False)","739250c7":"Y = df_train[\"median_house_value\"].copy()\n\nX = df_train.drop(['Id','median_house_value'], axis=1).copy()","d796660d":"from sklearn.preprocessing import StandardScaler\n\n# Criando nosso StandardScaler\nscaler = StandardScaler()\n\nX = scaler.fit_transform(X)\n\nX","b1cb33e4":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score","2cdd1e5d":"def regress(regressor,cv): \n    reg = regressor\n    reg.fit(X, Y)\n\n\n    scores = cross_val_score(reg, X, Y,\n                        scoring=\"neg_mean_squared_error\", cv=cv)\n    scores\n\n    rmse_scores = np.sqrt(-scores)\n    \n    return rmse_scores\n    \n    ","2b1a7c9d":"%%time\n\nfrom sklearn.linear_model import LinearRegression\n\n\nrmse_scores = regress(LinearRegression(),10)\nprint(\"Mean:\\t\\t \", rmse_scores.mean(), \"\\nStandard Deviation:\", rmse_scores.std())","57381103":"from sklearn.ensemble import RandomForestRegressor","8a8f68a6":"%%time\n\nrmse_scores = regress(RandomForestRegressor(),10)\n\nprint(\"Mean:\\t\\t \", rmse_scores.mean(), \"\\nStandard Deviation:\", rmse_scores.std())","1d6c49f6":"from sklearn.neighbors import KNeighborsRegressor\n\nrmse_scores = regress(KNeighborsRegressor(n_neighbors=11),10)\n                      \nprint(\"Mean:\\t\\t \", rmse_scores.mean(), \"\\nStandard Deviation:\", rmse_scores.std())","39b04640":"%%time\n\nbest_score = 0.0\n\nfor k in range(5, 30):\n    rmse_scores = regress(KNeighborsRegressor(n_neighbors=k),cv=10)\n    score = np.mean(rmse_scores)\n    \n    if score > best_score:\n        best_k = k\n        best_score = score\n        \nprint(\"the best score is {}, using k = {}\".format(best_score, best_k))","5fdcc877":"%%time\n\nbest_score = 99999.0\n\nfor cv in range(5, 30):\n    rmse_scores = regress(KNeighborsRegressor(n_neighbors=29),cv)\n    score = np.mean(rmse_scores)\n    \n    if score < best_score:\n        best_k = k\n        best_score = score\n        \nprint(\"the best score is {}, using k = {}\".format(best_score, best_k))","11f00c29":"%%time\n\nbest_score = 99999.0\n\nfor cv in range(5, 20):\n    rmse_scores = regress(LinearRegression(),cv)\n    score = np.mean(rmse_scores)\n    \n    if score < best_score:\n        best_k = k\n        best_score = score\n        \nprint(\"the best score is {}, using k = {}\".format(best_score, best_k))","f36eeed0":"from sklearn.model_selection import GridSearchCV\nparam_grid = [\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]}]","bbdbaa38":"forest_reg = RandomForestRegressor()\n\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                          scoring='neg_mean_squared_error')\ngrid_search.fit(X, Y)","16541715":"best_regressor = grid_search.best_estimator_","f841ebc7":"# Salvando o arquivo de teste csv em um DataFrame\ndf_test = pd.read_csv(\"..\/input\/atividade-regressao-PMR3508\/test.csv\", na_values = '?')","c90530e6":"df_test","9892bb05":"df_test['rooms_per_household'] = df_test['total_rooms']\/df_test['households']\ndf_test['bedrooms_per_room'] = df_test['total_bedrooms']\/df_test['total_rooms']\ndf_test['population_per_household'] = df_test['population']\/df_test['households']\ndf_test","40e0352c":"df_test['distance_to_sf'] = df_test.apply(lambda row: distancia_euclidiana2(lat_sf,long_sf,row['latitude'],row['longitude']),axis=1)","51a003a9":"df_test['distance_to_la'] = df_test.apply(lambda row: distancia_euclidiana2(lat_la,long_la,row['latitude'],row['longitude']),axis=1)","dbc3deaa":"# Y  = df_test['median_house_values'].copy()\n\nX = df_test.drop(['Id'], axis=1).copy()","c1624015":"from sklearn.preprocessing import StandardScaler\n\n# Criando nosso StandardScaler\nscaler = StandardScaler()\n\nX = scaler.fit_transform(X)\n\nX","d7e5b0a3":"prediction = best_regressor.predict(X)\nprediction","00c1a77d":"submission = pd.DataFrame()\nsubmission[0] = df_test['Id']\nsubmission[1] = prediction\nsubmission.columns = ['Id','median_house_value']","74da5394":"submission.head()","df97c440":"submission.to_csv('submission.csv',index = False)","8bb25ca1":"### 4.2. Random Forest","410f7aac":"Aqui podemos ver que a base tem 10 colunas que descrevem algumas caracter\u00edsticas das das casas nas regi\u00f5es. Pela informa\u00e7\u00e3o de \"Non-Null Count\" podemos ver que n\u00e3o existem valores faltantes.","38b47d13":"Usando a fun\u00e7\u00e3o describe para analisar a distribui\u00e7\u00e3o dos dados podemos notar que as colunas apresentam desvio padr\u00e3o alto, o que faz sentido, existe uma grande varia\u00e7\u00e3o no padr\u00e3o das casas e uma distribui\u00e7\u00e3o n\u00e3o homog\u00eanea entre cidades.","5028ba0f":"A base \u00e9 composta somente por valores num\u00e9ricos, exceto pela latitude e longitude, que s\u00e3o float64, as outras colunas s\u00e3o int64.","a21a0b19":"## 2.1. Visualizando no mapa","85b17656":"# 1. Prepara\u00e7\u00e3o dos dados","3553afa0":"Pode-se notar que h\u00e1 claramente uma concentra\u00e7\u00e3o de pre\u00e7os mais altos em regi\u00f5es perto da costa. Assim como regi\u00f5es pr\u00f3ximas de cidades refer\u00eancia como S\u00e3o Francisco e Los Angeles.","8eefc389":"## 3.1. Normaliza\u00e7\u00e3o ","bbb5a45d":"### 4.1. Regress\u00e3o Linear","bc387d88":"Iremos repetir os tratamentos que foram feitos na base de treino na base de teste e ent\u00e3o fazer a predi\u00e7\u00e3o para o arquivo de submiss\u00e3o","0fe492a7":"## 2.4. Observando correla\u00e7\u00e3o","c5d8eb25":"RandomForest continua sendo o melhor regressor, ent\u00e3o vamos us\u00e1-lo em ua etapa de fine-tuning","7d9e6519":"Podemos cruzar algumas das features definidas para obter valores mais apropriados para a an\u00e1lise do pre\u00e7o, como \"C\u00f4modos por Casa\". \"Quartos por C\u00f4modo\" e \"Pessoas por Casa\".","59789cbd":"### 4. Ajustando Hiperpar\u00e2metros","f745bed5":"Iremos ler o arquivo com os dados a serem an\u00e1lisado, como ele est\u00e1 constru\u00eddo e como os dados est\u00e3o respresentados dentro dele.","ee9ae0b7":"### 4.3. KNN","70f743a5":"Pode-se notar alguns pontos pelos histogramas:\n- households: H\u00e1 uma mior concentra\u00e7\u00e3o de regi\u00f5es com menos de 1000 casas.\n- latitude e longitude: percebe-se que h\u00e1 dois picos em cada um dos gr\u00e1ficos, pode-se concluir que s\u00e3o duas regi\u00f5es de maior concentra\u00e7\u00e3o de casas.\n- median_age e median_house_value: apresentam valores outliers no extremo m\u00e1ximo.\n- outros gr\u00e1ficos mostram novamente uma concentra\u00e7\u00e3o de valores mais baixos, o que faz sentido dado o contexto imobili\u00e1rio da base de dados - existem mais casa menores do que grandes imov\u00e9is.","fad19af5":"### 4.2. Executando dados de teste","0be5e5ba":"***\n\n# PMR3508 - Aprendizado de M\u00e1quina e Reconhecimento de Padr\u00f5es (2020)\n\n## Atividade Regress\u00e3o: California Housing\n\n### Gustavo Henrique de Oliveira (PMR3508-2020-54)\n\n***","0cc5e2de":"## 1.1. Importando bibliotecas","709f5086":"## 4. Regress\u00e3o","76a42ea9":"Somado ao que se pode ver pelos gr\u00e1ficos, pode-se ver que \"median_income\" tem correla\u00e7\u00e3o alta com o pre\u00e7o da casa.","cdd3c972":"## 2.3. Adicionando novas features","be32f365":"## 3. Pr\u00e9-processamento ","03d88ca3":"# 2. An\u00e1lise Explorat\u00f3ria","686f5b05":"## 1.2. Estrutura da base","0d93579f":"## 2.2. Analisando distribui\u00e7\u00f5es em rela\u00e7\u00e3o ao pre\u00e7o"}}