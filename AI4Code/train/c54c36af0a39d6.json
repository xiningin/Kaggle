{"cell_type":{"7057716b":"code","f6352402":"code","2d879fb7":"code","02d39aa2":"code","27cd9f02":"code","0a6cdd28":"code","37ae921c":"code","8ab2c52f":"code","7d5a2b75":"code","398e99f4":"code","fd1cb6b5":"code","1210043d":"code","49c18936":"code","abca2d04":"code","37beedea":"code","b16d1738":"code","34f054b4":"code","6453550c":"code","a8031e7e":"code","6fceaac4":"code","2fb240b2":"code","4dac122b":"code","7725506f":"code","fd2a1b25":"code","14fb5ec6":"code","86725223":"code","9f803258":"code","f2b2d1e5":"code","28b9ec65":"code","f4254b78":"code","fe569567":"code","cbeeb9b4":"code","e2f1af9e":"markdown","8e99134a":"markdown","67325f1d":"markdown","71c21c79":"markdown","92e0b990":"markdown","44151e7a":"markdown","016af7a4":"markdown","8f7f9630":"markdown","cd3a2dfa":"markdown","d64db3c6":"markdown","6928b947":"markdown","f2a7f217":"markdown","4ab6fa4b":"markdown","37ac8c91":"markdown","ec7f1743":"markdown","0532b95a":"markdown","4b03874f":"markdown","471f88a3":"markdown","166de6f8":"markdown","5756504f":"markdown"},"source":{"7057716b":"\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom tqdm import tqdm","f6352402":"df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","2d879fb7":"df.head()","02d39aa2":"df.shape, test.shape","27cd9f02":"sns.countplot(df[\"target\"]);","0a6cdd28":"df[\"target\"].value_counts(normalize = True) #normalized value counts","37ae921c":"def len_plot(data, name):\n  length = [len(sent.split()) for sent in data]\n  plt.hist(length)\n  plt.title(name)","8ab2c52f":"len_plot(df[df[\"target\"]==0][\"text\"], \"Not Disaster\") #passing non disaster tweets","7d5a2b75":"len_plot(df[df[\"target\"]==1][\"text\"], \"Disaster\") #passing disaster tweets","398e99f4":"X = df[\"text\"] # indpendent\ny = df[\"target\"] # dependent\ny = np.array(y) # converting into array","fd1cb6b5":"def unique_words(text):\n  unique_words_list = []\n  for sent in tqdm(text):\n    for word in sent.split():\n      if word.lower() not in unique_words_list:\n        unique_words_list.append(word.lower())\n      else:\n        pass\n  return unique_words_list\nun_words = unique_words(X)","1210043d":"print(\"Total number of unique words :\",len(un_words))","49c18936":"un_words[:50]","abca2d04":"SYMBOL1 = \"#\"\nwords_sym1 = [word for word in un_words if word.startswith(SYMBOL1)]\nlen(words_sym1)","37beedea":"words_sym1[:50]","b16d1738":"# words with starting letter \"@\"\nSYMBOL2 = \"@\"\nwords_sym2 = [word for word in un_words if word.startswith(SYMBOL2)]\nlen(words_sym2)","34f054b4":"words_sym2[:50]","6453550c":"def remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)","a8031e7e":"from nltk.stem import WordNetLemmatizer\nwl = WordNetLemmatizer()","6fceaac4":"def preprocessing(text):\n  \n  tweets = []\n  for tweet in tqdm(text):\n    tweet = tweet.lower() # converting to lower case\n    tweet =  remove_urls(tweet) # url removing\n    tweet = re.sub(r'@\\w+',  '', tweet).strip() # removing the words start with \"@\"\n    tweet = re.sub(\"[^a-zA-Z0-9 ']\", \"\", tweet) # removing unwanted symbols\n    tweet = tweet.split()\n    tweet1 = [wl.lemmatize(word) for word in tweet if word not in set(stopwords.words(\"english\"))] #lemmatization and stopwrds removal\n    tweet1 = \" \".join(tweet1)\n    tweets.append(tweet1)\n  return tweets\n\ntweets = preprocessing(X)","2fb240b2":"tweets[:50]","4dac122b":"# importing libraries\nimport tensorflow as tf\ntf.__version__\n\nfrom tensorflow.keras.layers import (Embedding,\n                                     LSTM,\n                                     Dense,\n                                     Dropout,\n                                     GlobalMaxPool1D,\n                                     BatchNormalization)\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import one_hot","7725506f":"VOC_SIZE = 30000\nonehot_repr = [one_hot(words, VOC_SIZE) for words in tweets]\nonehot_repr[100:110]","fd2a1b25":"# finding sentence length for each tweets\nword_length = []\nfor i in onehot_repr:\n  word_length.append(len(i))\n\nlen(word_length)","14fb5ec6":"word_length[1100:1150]","86725223":"# plotting graph (length of the tweets vs Numbers)\nplt.hist(word_length)\nplt.xlabel(\"Length of Words\")\nplt.ylabel(\"Nos\")\nplt.show()","9f803258":"max(word_length) # lenth of the longest tweet","f2b2d1e5":"SENT_LENGTH = 15\nembedded_docs = pad_sequences(onehot_repr, padding=\"post\", maxlen=SENT_LENGTH)\nembedded_docs","28b9ec65":"def create_model():\n  VECT_FEATURES = 32\n  model = Sequential()\n  model.add(Embedding(VOC_SIZE,\n                      VECT_FEATURES,\n                      input_length=SENT_LENGTH))\n  model.add(LSTM(100, return_sequences = True))\n  model.add(GlobalMaxPool1D())\n  model.add(BatchNormalization())\n  model.add(Dropout(0.5))\n  model.add(Dense(10, activation=\"relu\"))\n  model.add(Dropout(0.2))\n  model.add(Dense(1, activation = \"sigmoid\"))\n  return model\n","f4254b78":"model = create_model()\nmodel.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"]) # compiling\nmodel.summary() #summary","fe569567":"history = model.fit(embedded_docs, y, epochs=6, batch_size=32)","cbeeb9b4":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\nax1.plot(history.history[\"accuracy\"])\nax1.set_title(\"Accuracy\")\nax1.set_xlabel(\"Epochs\")\nax1.set_ylabel(\"Accuracy\")\n\nax2.plot(history.history[\"loss\"])\nax2.set_title(\"Loss\")\nax2.set_xlabel(\"Epochs\")\nax2.set_ylabel(\"Loss\")\nplt.show()","e2f1af9e":"#### Plotting the graph of model accuracy and loss","8e99134a":"#### So, let's start our code by importing libraries required","67325f1d":"#### Function for url removing","71c21c79":"#### words with starting letter \"@\"","92e0b990":"### LSTM","44151e7a":"#### Lemmatizer","016af7a4":"#### Padding the Sequence","8f7f9630":"#### Function for finding the number of unique words in our dataset","cd3a2dfa":"#### Preprocessing function","d64db3c6":"Since many of the words starting with \"@\" doesn't give any impact to our model accuracy, so we need to remove it","6928b947":"#### Function for model creation","f2a7f217":"#### Separating input and output features","4ab6fa4b":"#### As it is a twitter dataset, it contains several words starting with \"@\" and \"#\". Let's find this words","37ac8c91":"#### Plotting the number of  disaster and non disaster tweets","ec7f1743":"#### Training the Model","0532b95a":"#### words with starting letter \"#\"","4b03874f":"#### Creating a function for plotting a histogram (for length of tweets)","471f88a3":"#### Let's import the Dataset","166de6f8":"#### Performing onehot encoding","5756504f":"## Aim\n#### Aim of this notebook is to create model for classifying the tweets into disaster and non disaster using LSTM"}}