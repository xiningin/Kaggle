{"cell_type":{"c5da1ec6":"code","7da88a60":"code","17e8c080":"code","e04210c7":"code","da3394ce":"code","04526559":"code","214b1011":"code","d12642d0":"code","c648cf14":"code","e3e4668e":"code","a3f31f1d":"code","3c8d0f60":"code","9b322152":"code","fe838362":"code","ad16d1a9":"code","7f8af9ed":"code","d9bee37f":"code","f5de50ff":"code","160fab7f":"code","3cc4e3c0":"code","5f0415b8":"code","ab6e13eb":"code","5b4ccaf3":"code","e00f7c72":"code","16f273af":"code","1300fb4b":"code","d0ff9381":"code","8e44da4a":"code","be154d40":"code","10c6eb7d":"code","10dbd091":"code","15164996":"code","16e91e2e":"code","3204f201":"code","0fefb3ec":"code","acc4aa6b":"code","fd2183e7":"code","88b5ee93":"code","3fb2cb13":"code","f98595d8":"code","d2d95b6d":"code","6d968ac3":"markdown","1b4dd890":"markdown","1d69dafe":"markdown","85a1fda0":"markdown","4bac74cf":"markdown","0fc3df06":"markdown","ca2635db":"markdown","49a13219":"markdown","7f25a20b":"markdown","d3a9fc7b":"markdown","2c3de674":"markdown","fc5ea41c":"markdown","2ddf0b21":"markdown","0d296331":"markdown","fd2e18df":"markdown","2efe67e4":"markdown","3c8a0a43":"markdown","da765392":"markdown","68f54c91":"markdown","67597473":"markdown","c0bad745":"markdown","926684d7":"markdown","c6937c02":"markdown","5cb3d3af":"markdown","ff1415e0":"markdown","aa5fb3b9":"markdown","8d8631f6":"markdown","8a3e9088":"markdown","6d7035f7":"markdown","a31ec80d":"markdown","eef36d6b":"markdown","13355dbf":"markdown","89ab228e":"markdown","33885b8f":"markdown","009703f4":"markdown","1d8fb83f":"markdown","7076a0f4":"markdown","8e748537":"markdown","ced9478b":"markdown","c109300f":"markdown","4972850f":"markdown","b7bbaa46":"markdown","b4715569":"markdown","19bc6fb6":"markdown","ef912c15":"markdown","b025b714":"markdown","929bc6bd":"markdown","3b16c9d0":"markdown","19a0e0b7":"markdown","89864a2e":"markdown","21d5e7f5":"markdown","c935263d":"markdown","ed701e0b":"markdown"},"source":{"c5da1ec6":"import altair as alt\nimport itertools\nimport keras\nimport math\nimport numpy as np\nimport pandas as pd\nimport re\nimport string\nimport spacy\nimport tensorflow as tf\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom keras.models import Sequential\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Altair cannot do word cloud so I use Matplotlib just to render the image\nimport matplotlib.pyplot as plt\nplt.style.use(\"fivethirtyeight\")\n%matplotlib inline\n%config InlineBackend.figure_format = \"retina\"","7da88a60":"webmddf = pd.read_csv(\"..\/input\/webmd-drug-reviews-dataset\/webmd.csv\")\nwebmddf.head(3)","17e8c080":"df = webmddf[[\"Age\", \"Condition\", \"Drug\", \"DrugId\", \"Satisfaction\", \"Sex\", \"Reviews\"]]","e04210c7":"df.isnull().sum()","da3394ce":"df = df.dropna()","04526559":"for col in df.columns:\n    if df[col].dtype.kind == \"O\":\n        df[col] = df[col].str.strip()","214b1011":"data = [[col, df[col].nunique()] for col in df.columns.difference([\"Reviews\"])]\nuniques = pd.DataFrame(data=data, columns=[\"columns\", \"num of unique values\"])\n\nbars = (alt.Chart()\n           .mark_bar(size=25, \n                     color=\"#FFAA00\",\n                     strokeWidth=1,\n                     stroke=\"white\",\n                     strokeOpacity=0.7)\n           .encode(x=alt.X(shorthand=\"num of unique values:Q\",\n                           scale=alt.Scale(type=\"log\"),\n                           axis=alt.Axis(title=\"num of unique values, log scaled\")),\n                   y=alt.Y(\"columns:O\", sort=\"-x\"),\n                   tooltip=(\"num of unique values:Q\",\n                            \"columns:O\",),\n                   color=alt.Color(\"num of unique values\",\n                                   scale=alt.Scale(scheme=\"lightgreyteal\",\n                                                   type=\"log\")))\n           .properties(title='Unique Values'))\n\ntext = (alt.Chart()\n           .mark_text(align=\"left\",\n                      baseline=\"middle\",\n                      dx=3)\n           .encode(x=alt.X(shorthand=\"num of unique values:Q\"),\n                   y=alt.Y(\"columns:O\",\n                           axis=alt.Axis(title=\"columns\",\n                                         grid=False),\n                           sort=\"-x\"),\n                   text=\"num of unique values:Q\"))\n\nchart = ((alt.layer(bars, text, data=uniques)\n             .configure(background='#11043a')\n             .configure_title(font=\"Arial\",\n                              fontSize=18,\n                              color=\"#e6f3ff\",\n                              dy=-10)\n             .configure_text(color=\"white\")\n             .configure_legend(titleFontSize=12,\n                               titleColor=\"white\",\n                               tickCount=10,\n                               titleOpacity=0.8,\n                               labelColor=\"white\",\n                               labelOpacity=0.7,\n                               titlePadding=10)\n             .configure_axis(titleFontSize=13,\n                             titlePadding=20,\n                             titleColor=\"white\",\n                             titleOpacity=0.8,\n                             labelColor=\"white\",\n                             labelOpacity=0.7,\n                             labelFontSize=11,\n                             tickOffset=0,\n                             grid=True,\n                             gridOpacity=0.15)\n             .configure_view(strokeWidth=0)\n             .properties(height=200, width=680)))\n\nchart","d12642d0":"def missing_values(df):\n    \"\"\"Returns a summary of missing values in df\"\"\"\n    nrows = df.shape[0]\n    data = []\n    \n    def pct(n, total):\n        return round(n\/total, 2)\n    \n    for col in df.columns:\n\n        # string (Object) type columns\n        if df[col].dtype.kind == \"O\":\n            df[col] = df[col].str.strip()\n            nulls = df[df[col] == \"\"][col].count()\n            nulls += df[col].isnull().sum()\n\n        # numerical (int) type columns\n        elif df[col].dtype.kind == \"i\":\n            nulls = df[col].isnull().sum()\n\n        pctofnulls = pct(nulls, nrows)\n        data.extend(\n            [{\"column\": col, \"pct\": 1-pctofnulls, \"num of records\": nrows-nulls, \"type\": \"not missing\"},\n             {\"column\": col, \"pct\": pctofnulls, \"num of records\": nulls, \"type\": \"missing\"}])\n    \n    return pd.DataFrame(data)\n\nmissing = missing_values(df)\n\nbars = (alt.Chart()\n           .mark_bar(size=25, \n                     strokeWidth=1,\n                     stroke=\"white\",\n                     strokeOpacity=0.7,\n                     )\n           .encode(x=alt.X(\"sum(num of records)\",\n                           axis=alt.Axis(title=\"number of records\",\n                                         grid=True)), \n                   y=alt.Y(\"column:O\",\n                           axis=alt.Axis(title=\"columns\")),\n                   tooltip=(\"column\", \"type\", \"num of records:Q\",\n                            alt.Tooltip(\"pct:Q\", format=\".1%\")),\n                   color=alt.Color(\"type\",\n                                   scale=alt.Scale(range=[\"#11043a\", \"#648bce\"])))\n           .properties(title=\"Missing Values\"))\n\ntext = (alt.Chart()\n           .mark_text(align=\"right\",\n                      dx=-1)\n           .encode(x=alt.X(\"sum(num of records)\", \n                           stack=\"zero\"),\n                   y=alt.Y(\"column\"),\n                   color=alt.Color(\"type\",\n                                   legend=None,\n                                   scale=alt.Scale(range=[\"white\"])),\n                   text=alt.Text(\"pct\", format=\".0%\")))\n\n(alt.layer(bars, text, data=missing)\n    .configure(background='#11043a')\n    .configure_title(font=\"Arial\",\n                     fontSize=18,\n                     color=\"#e6f3ff\",\n                     dy=-10)\n    .configure_text(color=\"white\")\n    .configure_legend(titleFontSize=12,\n                      titleColor=\"white\",\n                      tickCount=10,\n                      titleOpacity=0.8,\n                      labelColor=\"white\",\n                      labelOpacity=0.7,\n                      titlePadding=10)\n    .configure_axis(titleFontSize=13,\n                    titlePadding=20,\n                    titleColor=\"white\",\n                    titleOpacity=0.8,\n                    labelFontSize=11,\n                    labelColor=\"white\",\n                    labelOpacity=0.7,\n                    tickOffset=0,\n                    grid=False,\n                    gridOpacity=0.15)\n    .configure_view(strokeWidth=0)\n    .resolve_scale(color='independent')\n    .properties(height=300, width=680))","c648cf14":"for col in [\"Age\", \"Condition\", \"Sex\", \"Reviews\"]:\n    df = df[(df[col].astype(bool) & df[col].notnull())]","e3e4668e":"print(df[\"Satisfaction\"].value_counts())\ndf = df[df[\"Satisfaction\"] <= 5]","a3f31f1d":"def relabel(x):\n    return 0 if x < 3 else 1 if x == 3 else 2\n\ndf[\"Satisfaction\"] = df[\"Satisfaction\"].apply(relabel)","3c8d0f60":"print(df[\"Sex\"].value_counts())","9b322152":"drugs = {}\nfor drugid, drug in df[[\"DrugId\", \"Drug\"]].itertuples(index=False):\n    drugs.setdefault(drugid, set()).add(drug)\ndrugs = {k:list(v) for k,v in drugs.items()}\n\ndrugs_with_more_names = {k:list(v) for k,v in drugs.items() if len(v) > 1}\nfor k,v in dict(itertools.islice(drugs_with_more_names.items(), 10)).items():\n    print(f\"{k:10}: {list(v)[:2]}\")","fe838362":"value_count_per_condition = df[\"Condition\"].value_counts()\nvalue_count_per_condition_norm = df[\"Condition\"].value_counts(normalize=True)\nunique_drugs_per_condition = df.groupby(\"Condition\")[\"DrugId\"].apply(set).to_frame().reset_index()\nunique_drugs_per_condition.columns = [\"condition\", \"unique_drugs\"]\n\ntempdf = pd.DataFrame({\"condition\": value_count_per_condition.index, \n                       \"condition_freq\": value_count_per_condition.values,\n                       \"condition_freq_norm\": value_count_per_condition_norm.values})\n\ntempdf = pd.merge(tempdf, unique_drugs_per_condition, on=\"condition\")","ad16d1a9":"def mrange(*args, ceiling=True):\n    \"\"\"Returns money range generator, yields 1, 2, 5, 10, 20, 50...\"\"\"\n    f = lambda x: (((x - 1) % 3)**2 + 1) * 10**((x-1)\/\/3)\n    if len(args) == 1:\n        start, stop = 1, args[0]\n    else:\n        start, stop = max(1, args[0]), args[1]\n    c = 1\n    x = f(c)\n    while x < start:\n        c += 1\n        x = f(c)\n    while True:\n        yield x\n        c += 1\n        x = f(c)\n        if x > stop:\n            break\n    if ceiling:\n        yield x\n\ndef roundup(x, nearest=1000):\n    \"\"\"Rounds x to the nearest 1000 or the optional argument.\"\"\"\n    return int(math.ceil(x \/ float(nearest))) * nearest\n\nceiling = roundup(value_count_per_condition[0]) + 1\nbins = [0] + [x for x in mrange(20, ceiling)]\nlabels = [str(x) for x in bins[1:]]\nbinlabels = pd.cut(tempdf[\"condition_freq\"], bins=bins, labels=labels)\nconddf = tempdf.assign(bin=binlabels.values)","7f8af9ed":"topN = 15\n\ndata = conddf[:topN][[\"condition\", \"condition_freq\", \"condition_freq_norm\"]]\n\nbars = (alt.Chart(title=f\"Top {topN} Conditions\")\n           .mark_bar(size=20,\n                     strokeWidth=1,\n                     stroke=\"white\",\n                     strokeOpacity=0.7,\n                     xOffset=-1)\n           .encode(x=alt.X(\"condition\", sort=\"-y\"),\n                   y=alt.Y(\"condition_freq:Q\",\n                           axis=alt.Axis(title=\"number of reviews\",\n                                         grid=True)), \n                   tooltip=(\"condition\",\n                            \"condition_freq:Q\",\n                            alt.Tooltip(\"condition_freq_norm:Q\", format=\".1%\")),\n                   color=alt.Color(\"condition_freq:Q\",\n                                   scale=alt.Scale(scheme=\"lightgreyteal\",\n                                                   type=\"log\"))))\n\ntext = (alt.Chart()\n           .mark_text(align=\"center\",\n                      baseline=\"bottom\",\n                      dx=-1, dy=-3)\n           .encode(x=alt.X(\"condition\", sort=\"-y\"),\n                   y=alt.Y(\"condition_freq:Q\"),\n                   size = alt.SizeValue(9),\n                   text=alt.Text(\"condition_freq_norm:Q\", format=\".1%\")))\n\nchart = (alt.layer(bars, text, data=data)\n            .configure(background='#11043a')\n            .configure_title(font=\"Arial\",\n                             fontSize=18,\n                             color=\"#e6f3ff\",\n                             dy=-10)\n            .configure_text(color=\"white\")\n            .configure_legend(title=None,\n                              titleFontSize=12,\n                              titleColor=\"white\",\n                              tickCount=5,\n                              titleOpacity=0.8,\n                              labelColor=\"white\",\n                              labelOpacity=0.7,\n                              titlePadding=10)\n            .configure_axis(titleFontSize=13,\n                            titlePadding=20,\n                            titleColor=\"white\",\n                            titleOpacity=0.8,\n                            labelFontSize=11,\n                            labelColor=\"white\",\n                            labelOpacity=0.7,\n                            #labelAngle=45,\n                            tickOffset=0,\n                            grid=False,\n                            gridOpacity=0.15)\n            .configure_view(strokeWidth=0)\n            .properties(height=300, width=700))\nchart","d9bee37f":"# this aggregates the sets of unique_drugs which fall into the same bin and counts the number of elements\naggr_sets = lambda x: sum(1 for n in set.union(*x))\n\ndata = (conddf.groupby(\"bin\")\n              .agg({\"condition\": \"count\", \"condition_freq\": \"sum\",\n                    \"condition_freq_norm\": \"sum\", \"unique_drugs\": aggr_sets})\n              .reset_index())\ndata.columns = [\"bin\", \"condition_count\", \"condition_freq_sum\",\n                \"condition_freq_norm_sum\", \"unique_drugs_count\"]\n\nbars = (alt.Chart(title=\"Distribution Of Condition Frequency And Drug Use\")\n           .mark_bar(size=20,\n                     strokeWidth=1,\n                     stroke=\"white\",\n                     strokeOpacity=0.7,\n                     xOffset=-1)\n           .encode(x=alt.X(shorthand=\"bin:Q\",\n                           scale=alt.Scale(round=False, type=\"log\"),\n                           axis=alt.Axis(title=\"binned condition counts\",\n                                         grid=False,\n                                         orient=\"bottom\")),\n                   y=alt.Y(shorthand=\"condition_freq_sum:Q\",\n                           scale=alt.Scale(type=\"log\"),\n                           axis=alt.Axis(title=\"sum of condition counts and unique drug use, log scaled\")),\n                   tooltip=(\"bin:Q\",\n                            \"condition_count:Q\",\n                            \"condition_freq_sum:Q\", \n                            alt.Tooltip(\"condition_freq_norm_sum:Q\", format=\".1%\"),\n                                        \"unique_drugs_count:Q\"),\n                   color=alt.Color(\"condition_count:Q\",\n                                   scale=alt.Scale(scheme=\"lightgreyteal\",\n                                                   type=\"log\"))))\n\ntext = (alt.Chart()\n           .mark_text(align=\"center\",\n                      baseline=\"bottom\",\n                      dx=-1, dy=-3)\n           .encode(x=alt.X(\"bin:Q\"),\n                   y=alt.Y(\"condition_freq_sum:Q\"),\n                   size = alt.SizeValue(9),\n                   text=alt.Text(\"condition_freq_norm_sum:Q\", format=\".1%\")))\n\nline = (alt.Chart()\n           .mark_line(color=\"red\",\n                      xOffset=-1,\n                      size=1)\n           .encode(x=alt.X(\"bin:Q\"),\n                   y=alt.Y(\"unique_drugs_count:Q\")))\n\npoint = (alt.Chart()\n            .mark_point(color=\"red\",\n                        xOffset=-1,\n                        size=15,\n                        shape=\"diamond\")\n            .encode(x=alt.X(\"bin:Q\"),\n                    y=alt.Y(\"unique_drugs_count:Q\"),\n                    tooltip=(\"unique_drugs_count:Q\")))\n\nchart = (alt.layer(bars, line, text, point, data=data[data[\"condition_count\"] > 0])\n            .configure(background='#11043a')\n            .configure_title(font=\"Arial\",\n                             fontSize=18,\n                             color=\"#e6f3ff\",\n                             dy=-10)\n            .configure_text(color=\"white\")\n            .configure_legend(title=None,\n                              titleFontSize=12,\n                              titleColor=\"white\",\n                              tickCount=10,\n                              titleOpacity=0.8,\n                              labelColor=\"white\",\n                              labelOpacity=0.7,\n                              titlePadding=10)\n            .configure_axis(titleFontSize=13,\n                            titlePadding=20,\n                            titleColor=\"white\",\n                            titleOpacity=0.8,\n                            labelColor=\"white\",\n                            labelOpacity=0.7,\n                            tickOffset=0,\n                            grid=True,\n                            gridOpacity=0.15)\n            .configure_view(strokeWidth=0)\n            .properties(height=300, width=700))\n\nchart.resolve_scale(color=\"independent\")","f5de50ff":"data = (df.groupby([\"Age\", \"Satisfaction\"])\n          .agg({\"Reviews\": \"count\"})\n          .reset_index()).sort_values([\"Age\", \"Satisfaction\"], ascending=True)\n#data['Cumulative_Reviews'] = data.groupby(['Age'])['Reviews'].apply(lambda x: x.cumsum())\n\nbars = (alt.Chart(data=data, title=\"Distribution of Reviews Over Age\")\n           .mark_bar(size=40,\n                     strokeWidth=0.5,\n                     stroke=\"white\")\n           .encode(x=alt.X('Age:O',\n                           axis=alt.Axis(title=\"Age groups\", grid=False)),\n                   y=alt.Y('Reviews:Q', stack='zero',\n                           scale=alt.Scale(type=\"linear\"),\n                           axis=alt.Axis(title=\"num of reviews\")),\n                   order=alt.Order('Satisfaction', sort='ascending'),\n                   color=alt.Color(\"Satisfaction:Q\",\n                                   scale=alt.Scale(scheme=\"lightgreyteal\",\n                                                   bins=[0,1,2,3],\n                                                   reverse=False))))\n\ntext = (alt.Chart(data=data[data[\"Reviews\"] > 1500])\n           .mark_text(align=\"center\",\n                      baseline=\"middle\",\n                      dx=0, dy=5)\n           .encode(x=alt.X(\"Age:O\"),\n                   y=alt.Y(\"Reviews:Q\", stack='zero'),\n                   size = alt.SizeValue(9),\n                   text=\"Reviews:Q\",\n                   color=alt.condition(alt.datum.Satisfaction > 1,\n                                          alt.value(\"white\"),\n                                          alt.value(\"black\"))))\n    \nchart = (alt.layer(bars, text)\n            .configure(background=\"#11043a\")\n            .configure_title(font=\"Arial\",\n                             fontSize=18,\n                             color=\"#e6f3ff\",\n                             dy=-10)\n            .configure_text(color=\"white\")\n            .configure_legend(titleFontSize=12,\n                              titleColor=\"white\",\n                              tickCount=10,\n                              titleOpacity=0.8,\n                              labelColor=\"white\",\n                              labelOpacity=0.7,\n                              titlePadding=10)\n            .configure_axis(titleFontSize=13,\n                            titlePadding=20,\n                            titleColor=\"white\",\n                            titleOpacity=0.8,\n                            labelColor=\"white\",\n                            labelOpacity=0.7,\n                            labelAngle=0,\n                            tickOffset=0,\n                            grid=True,\n                            gridOpacity=0.15)\n            .configure_view(strokeWidth=0)\n            .properties(height=300, width=700)\n)\nchart","160fab7f":"data = (df.groupby([\"DrugId\"])\n          .agg({\"Reviews\": \"count\", \"Satisfaction\": \"mean\"})\n          .reset_index()\n          .sort_values([\"Reviews\"], ascending=False))\ndata[\"Drug\"] = data[\"DrugId\"].map(drugs)\n\nalt.data_transformers.disable_max_rows()\nscatter = (alt.Chart(title=\"Distribution Of Reviews Over Satisfaction\")\n            .mark_point(color=\"#648bce\")\n            .encode(x=alt.X('Satisfaction:Q',\n                            axis=alt.Axis(title=\"Mean Satisfaction\",\n                                          grid=False)),\n                    y=alt.Y('Reviews:Q',\n                             scale=alt.Scale(type=\"log\"),\n                             axis=alt.Axis(title=\"Number of Reviews, log scaled\")),\n                    size='Reviews:Q',\n                    color=alt.Color(\"Satisfaction:Q\",\n                                   scale=alt.Scale(scheme=\"lightgreyteal\",\n                                                   type=\"linear\")),\n                    tooltip=['DrugId', 'Drug', 'Reviews',\n                              alt.Tooltip(\"Satisfaction\", format=\".3\")])\n            .interactive())\n\nchart = (alt.layer(scatter, data=data[data[\"Reviews\"] > 20])\n            .configure(background='#11043a')\n            .configure_title(font=\"Arial\",\n                             fontSize=18,\n                             color=\"#e6f3ff\",\n                             dy=-10)\n            .configure_text(color=\"white\")\n            .configure_legend(titleFontSize=12,\n                              titleColor=\"white\",\n                              tickCount=6,\n                              titleOpacity=0.8,\n                              labelColor=\"white\",\n                              labelOpacity=0.7,\n                              titlePadding=10)\n            .configure_axis(titleFontSize=13,\n                            titlePadding=20,\n                            titleColor=\"white\",\n                            titleOpacity=0.8,\n                            labelFontSize=11,\n                            labelColor=\"white\",\n                            labelOpacity=0.7,\n                            labelAngle=0,\n                            tickOffset=0,\n                            grid=False,\n                            gridOpacity=0.15)\n            .configure_view(strokeWidth=0)\n            .properties(height=300, width=700)\n)\nchart","3cc4e3c0":"indexes = np.random.randint(df.shape[0], size=3)\nprint(\" \".join(df[\"Reviews\"].iloc[indexes].tolist()))","5f0415b8":"%%time\n\nnlp = spacy.load(\"en\", disable=[\"ner\", \"parser\"])\nSTOPWORDS = set(ENGLISH_STOP_WORDS).union(set(stopwords.words(\"english\")))\n\ndef clean_review(text, STOPWORDS=STOPWORDS, nlp=nlp):\n    \"\"\"Cleans up text\"\"\"\n    \n    def rep_emo(text, placeholder_pos=' happyemoticon ', placeholder_neg=' sademoticon '):\n        \"\"\"Replace emoticons\"\"\"\n        # Credit https:\/\/github.com\/shaheen-syed\/Twitter-Sentiment-Analysis\/blob\/master\/helper_functions.py\n        emoticons_pos = [\":)\", \":-)\", \":p\", \":-p\", \":P\", \":-P\", \":D\",\":-D\", \":]\", \":-]\", \";)\", \";-)\",\n                         \";p\", \";-p\", \";P\", \";-P\", \";D\", \";-D\", \";]\", \";-]\", \"=)\", \"=-)\", \"<3\"]\n        emoticons_neg = [\":o\", \":-o\", \":O\", \":-O\", \":(\", \":-(\", \":c\", \":-c\", \":C\", \":-C\", \":[\", \":-[\",\n                         \":\/\", \":-\/\", \":\\\\\", \":-\\\\\", \":n\", \":-n\", \":u\", \":-u\", \"=(\", \"=-(\", \":$\", \":-$\"]\n\n        for e in emoticons_pos:\n            text = text.replace(e, placeholder_pos)\n\n        for e in emoticons_neg:\n            text = text.replace(e, placeholder_neg)   \n        return text\n\n    def rep_punct(text):\n        \"\"\"Replace all punctuation with space\"\"\"\n        for c in string.punctuation:\n            text = text.replace(c, \" \")\n        return text\n\n    def rem_stop_num(text):\n        \"\"\"Remove stop words and anything starting with number\"\"\"\n        return \" \".join(word for word in text.split() if word not in STOPWORDS and not word[0].isdigit())\n\n    def lemmatize(text):\n        \"\"\"Return lemmas of tokens in text\"\"\"\n        return \" \".join(tok.lemma_.lower().strip() for tok in nlp(text) if tok.lemma_ != \"-PRON-\")  \n\n    return lemmatize(rem_stop_num(rep_punct(rep_emo(text))))\n\nmldf = df[[\"Satisfaction\", \"Reviews\"]]\nmldf[\"Reviews\"] = mldf[\"Reviews\"].apply(clean_review)\n\n# remove any rows with new empty strings following the clean-up\nmldf[\"Reviews\"].replace(\"\", np.nan, inplace=True)\nmldf.dropna(inplace=True)\n# adding indexes as \"index\" column for later use to recreate same splits \nmldf.reset_index(inplace=True)","ab6e13eb":"print(\" \".join(mldf[\"Reviews\"].iloc[indexes].tolist()))","5b4ccaf3":"del df","e00f7c72":"negdf = mldf[mldf[\"Satisfaction\"] == 0]\nnegatives = []\nfor review in negdf[\"Reviews\"]:\n    negatives.append(review)\nnegatives = pd.Series(negatives).str.cat(sep=\" \")\n\nwordcloud = WordCloud(width=1600, height=800, max_font_size=200).generate(negatives)\nplt.figure(figsize=(12,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","16f273af":"posdf = mldf[mldf[\"Satisfaction\"] == 2]\npositives = []\nfor review in posdf[\"Reviews\"]:\n    positives.append(review)\npositives = pd.Series(positives).str.cat(sep=\" \")\n\nwordcloud = WordCloud(width=1600, height=800, max_font_size=200).generate(positives)\nplt.figure(figsize=(12, 10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","1300fb4b":"train_set, test_set = train_test_split(mldf, test_size=0.25, random_state=0, stratify=mldf[\"Satisfaction\"])\ntrain_index = train_set.index\ntest_index = test_set.index\nprint(train_set.shape)\nprint(test_set.shape)","d0ff9381":"def confusion_matrix_altair(labels, predictions):\n    \"\"\"Returns Altair heatmap as confusion matrix\"\"\"\n    \n    alt.data_transformers.disable_max_rows()\n    source = pd.DataFrame([labels, predictions]).T\n    source.columns=[\"True\", \"Predicted\"]\n\n    # Configure base chart\n    base = (alt.Chart(source, title=\"Confusion Matrix\")\n               .transform_aggregate(count=\"count()\",\n                                    groupby=[\"True\", \"Predicted\"])\n               .transform_joinaggregate(total=\"sum(count)\")\n               .transform_calculate(pct=\"datum.count \/ datum.total\")\n               .encode(x=alt.X(\"Predicted:O\", scale=alt.Scale(paddingInner=0)),\n                       y=alt.Y(\"True:O\", scale=alt.Scale(paddingInner=0)),\n                       tooltip=(alt.Tooltip(\"pct:Q\", format=\".1%\"))))\n    # Configure heatmap\n    heatmap = (base.mark_rect()\n                   .encode(color=alt.Color(\"count:Q\",\n                           scale=alt.Scale(scheme=\"blues\"),\n                           legend=alt.Legend(direction=\"vertical\"))))\n    # Configure text\n    text = (base.mark_text(baseline=\"middle\")\n                .encode(text=\"count:Q\",\n                        color=alt.condition(alt.datum.count > 10000,\n                                            alt.value(\"white\"),\n                                            alt.value(\"black\"))))\n    # Draw the chart\n    chart = ((heatmap + text)\n                .configure(background=\"#11043a\")\n                .configure_title(fontSize=18,\n                                 color=\"#e6f3ff\",\n                                 dy=-20)\n                .configure_text(color=\"white\",\n                                fontSize=14)\n                .configure_legend(titleFontSize=12,\n                                  titleColor=\"white\",\n                                  titleOpacity=0.8,\n                                  labelColor=\"white\",\n                                  labelOpacity=0.7,\n                                  titlePadding=10)\n                .configure_axis(titleFontSize=14,\n                                titlePadding=20,\n                                titleColor=\"white\",\n                                titleOpacity=0.8,\n                                labelFontSize=13,\n                                labelColor=\"white\",\n                                labelOpacity=0.7,\n                                labelAngle=0)\n                .configure_view(strokeWidth=0)\n                .properties(height=400, width=400)\n            )\n    return chart","8e44da4a":"%%time\nvectorizer = CountVectorizer(max_features=2500, min_df=10, max_df=0.8)\nX_train = vectorizer.fit_transform(train_set[\"Reviews\"]).toarray()\nX_test = vectorizer.transform(test_set[\"Reviews\"]).toarray()\ny_train = train_set[\"Satisfaction\"].values\ny_test = test_set[\"Satisfaction\"].values","be154d40":"%%time\nmodel = MultinomialNB(alpha=1.0)\nmodel.fit(X_train, y_train)","10c6eb7d":"acc_train = accuracy_score(y_train, model.predict(X_train))\nprint(f\"\\nAccuracy in train set: {acc_train:.2}\")\npredictions = model.predict(X_test)\nacc_test = accuracy_score(y_test, predictions)\nprint(f\"\\nAccuracy in test  set: {acc_test:.2}\\n\")\nprint(classification_report(y_test, predictions))\nconfusion_matrix_altair(y_test, predictions)","10dbd091":"%%time\ntrain_set = train_set[train_set[\"Satisfaction\"] != 1]\ntest_set = test_set[test_set[\"Satisfaction\"] != 1]\nprint(train_set.shape)\nprint(test_set.shape)\n\nvectorizer = CountVectorizer(max_features=2500, min_df=10, max_df=0.8)\nX_train = vectorizer.fit_transform(train_set[\"Reviews\"]).toarray()\nX_test = vectorizer.transform(test_set[\"Reviews\"]).toarray()\ny_train = train_set[\"Satisfaction\"].values\ny_test = test_set[\"Satisfaction\"].values\n\nmodel = MultinomialNB(alpha=1.0)\nmodel.fit(X_train, y_train)\n\nacc_train = accuracy_score(y_train, model.predict(X_train))\nprint(f\"\\nAccuracy in train set: {acc_train:.2}\")\npredictions = model.predict(X_test)\nacc_test = accuracy_score(y_test, predictions)\nprint(f\"\\nAccuracy in test  set: {acc_test:.2}\\n\")\nprint(classification_report(y_test, predictions))\nconfusion_matrix_altair(y_test, predictions)","15164996":"%%time\ntrain_set = mldf.loc[train_index]\ntest_set  = mldf.loc[test_index]\nprint(train_set.shape)\nprint(test_set.shape)\n\nvectorizer = TfidfVectorizer(max_features=2500, min_df=10, max_df=0.8)\nX_train = vectorizer.fit_transform(train_set[\"Reviews\"]).toarray()\nX_test = vectorizer.transform(test_set[\"Reviews\"]).toarray()\ny_train = train_set[\"Satisfaction\"].values\ny_test = test_set[\"Satisfaction\"].values","16e91e2e":"%%time\nmodel = RandomForestClassifier(min_samples_split=6, random_state=0)\nmodel.fit(X_train, y_train)","3204f201":"acc_train = accuracy_score(y_train, model.predict(X_train))\nprint(f\"\\nAccuracy in train set: {acc_train:.2}\")\npredictions = model.predict(X_test)\nacc_test = accuracy_score(y_test, predictions)\nprint(f\"\\nAccuracy in test  set: {acc_test:.2}\\n\")\nprint(classification_report(y_test, predictions))\nconfusion_matrix_altair(y_test, predictions)","0fefb3ec":"%%time\ntrain_set = train_set[train_set[\"Satisfaction\"] != 1]\ntest_set = test_set[test_set[\"Satisfaction\"] != 1]\nprint(train_set.shape)\nprint(test_set.shape)\n\nvectorizer = TfidfVectorizer(max_features=2500, min_df=10, max_df=0.8)\nX_train = vectorizer.fit_transform(train_set[\"Reviews\"]).toarray()\nX_test = vectorizer.transform(test_set[\"Reviews\"]).toarray()\ny_train = train_set[\"Satisfaction\"].values\ny_test = test_set[\"Satisfaction\"].values\n\nmodel = RandomForestClassifier(min_samples_split=6, random_state=0)\nmodel.fit(X_train, y_train)","acc4aa6b":"acc_train = accuracy_score(y_train, model.predict(X_train))\nprint(f\"\\nAccuracy in train set: {acc_train:.2}\")\npredictions = model.predict(X_test)\nacc_test = accuracy_score(y_test, predictions)\nprint(f\"\\nAccuracy in test  set: {acc_test:.2}\\n\")\nprint(classification_report(y_test, predictions))\nconfusion_matrix_altair(y_test, predictions)","fd2183e7":"%%time\nX_train = mldf.loc[train_index]\nX_test = mldf.loc[test_index]\ny_train = mldf.loc[train_index]\ny_test = mldf.loc[test_index]\n\nX_train = X_train[X_train[\"Satisfaction\"] != 1][\"Reviews\"].values\nX_test = X_test[X_test[\"Satisfaction\"] != 1][\"Reviews\"].values\ny_train = y_train[y_train[\"Satisfaction\"] != 1][\"Satisfaction\"].values\ny_test = y_test[y_test[\"Satisfaction\"] != 1][\"Satisfaction\"].values\n\nnum_words = 2500\nmaxlen = 200\n\ntokenizer = Tokenizer(num_words=num_words, split=\" \", lower=False)\ntokenizer.fit_on_texts(X_train)\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n\nX_train = pad_sequences(X_train, maxlen=maxlen)\nX_test = pad_sequences(X_test, maxlen=maxlen)\ny_train = pd.get_dummies(y_train).values\ny_test = pd.get_dummies(y_test).values","88b5ee93":"%%time\nembedding_vector_length = 100\n\nmodel = Sequential()\nmodel.add(Embedding(num_words, embedding_vector_length, input_length=maxlen))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(embedding_vector_length))\nmodel.add(Dense(2, activation=\"softmax\"))\nmodel.compile(loss = \"categorical_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])\nprint(model.summary())","3fb2cb13":"%%time\nepochs = 15\nbatch_size = 64\nhistory = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1,\n                    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=2, min_delta=0.0001)])","f98595d8":"accr = model.evaluate(X_test, y_test)\nprint(\"Loss in test set: {:0.3f}\\nAccuracy in test set: {:0.3f}\\n\".format(accr[0], accr[1]))\npredictions = model.predict_classes(X_test, batch_size = batch_size)\nlabels = np.argmax(y_test, axis=1)\nprint(classification_report(labels, predictions))\nconfusion_matrix_altair(labels, predictions)","d2d95b6d":"def predict_sentiment(text):\n    cleaned_text = tokenizer.texts_to_sequences([clean_review(text)])\n    padded_text = pad_sequences(cleaned_text, maxlen=maxlen)\n    return \"Positive\" if model.predict_classes(padded_text)[0] else \"Negative\"\n\npredict_sentiment(\"The drug is expensive but it is worth every cent.\")","6d968ac3":"I move on and try another ML algorithm, the RandomForestClassifier from sklearn. In addition I am also switching from the Bag of Words vectorization approch to TF-IDF. I set up TfidfVectorizer with the same parameters as the CountVectorizer before. The labels (y_train, y_test) have not changed so I do not touch them.","1b4dd890":"For a quick visualization I use word clouds. Word clouds have an appeal that is hard to deny, they are engaging and easy for the brain to digest. They are here to stay but they definitely should not be considered insightful but more like a fun amidst the hard work. Let's see first the word cloud built from the \"Negative\" reviews. I use Matplotlib because Altair which is based on Vega-Lite does not support this type of visualization as opposed to Vega which does.","1d69dafe":"Let's take stock of the number of unique values in each column except `Reviews`.","85a1fda0":"#### Representing `Reviews` in Numeric Form\n\nFor a machine to be able to work with text the content needs to be converted to and represented by numbers. There are three main approaches to do this. The Bag of Words, the TF-IDF and the Word2Vec. The first step is to create a vocabulary of all the unique words found in the documents. Here a Review is a document. In the Bag of Words approach each Review is converted to a vector of same size, which is the size of the vocabulary, where each position in the vector represents a word from the vocabulary and its value is the frequency of the word found in the document or zero if the word does not occur in the document. \n\nThe TF-IDF method not only assigns fixed weight to an occurrence of a word (\"TF\" or Term Frequency) but it also looks at how many times the word also occurs in other documents (\"IDF\" or Inverse Document Frequency) and assigns a value to a vector position which is the combination of the two for a given word.\n\nFirst, I want to build a reference model and for that I chose the multinomial Naive Bayes classifier. The multinomial distribution normally requires integer features, which is a vector what the Bag of Words algorithm produces but in practice fractional values, what the TF-IDF method makes, may work as well. For a basic model I chose the Bag of Words approach. To perform the vectorization I use CountVectorizer from sklearn.\n\nLet us proceed splitting the dataset into training (75%) and test (%25) sets. I specify the \"stratify\" parameter to make sure that after the split the proportion of the `Satisfaction` categories remain the same in both sets as was before the split.","4bac74cf":"WebMD is an organization which provides information, support and reference material about health subjects through a team of doctors and health experts across a broad range of specialty areas. This dataset was acquired by scraping [WebMD](https:\/\/www.webmd.com\/drugs\/2\/index) website until Mar 2020. The main objective of this notebook was to perform sentiment analysis on the reviews to predict if the reviewer was satisfied with the drug he or she rated or not. As per [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Sentiment_analysis):\n> Sentiment analysis (also known as opinion mining or emotion AI) refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information.\n\nI used Python 3.7, Altair for visualization because I have not used it before, Keras for ML and spaCy for lemmatization.","0fc3df06":"I also create a function to visualize the confusion matrix in Altair.","ca2635db":"### Conclusion\n\nOne of the most common natural language processing tasks is text classification. There are multiple Machine Learning algorithms that can be deployed to perform sentiment analysis. In this notebook I showed three different methods to build models which can predict the sentiment of a user based on what he or she writes. I only scratched the surface of NLP. In the subsequent notebook I will attempt to predict the condition of the reviewer from the review, sex and age.","49a13219":"For the purpose of my visualizations, I am going to group (bin) condition types based on their frequencies. The bin size will vary and follow a quasi logarithmic scale, which I call as \"money range\" mimicking the coin and note values in circulation in most countries (1, 2, 5, 10, 20, 50, 100, etc), starting from 20. In other words conditions with 20 reviews or less will be grouped together in the first bin, conditions with 21-50 reviews go into the second bin and so on.","7f25a20b":"I also strip leading and trailing white spaces from string type data. These white spaces often conceal the fact that there is nothing useful in the data cell simply because white spaces are data as well.","d3a9fc7b":"### Importing libraries","2c3de674":"As I suspected the predicting power of the model has become as much better as the number of neutral misclassifications made it worse in the previous model.","fc5ea41c":"The RandomForestClassifier has many hyperparameters. I left all of them on their default values except the  \"min_samples_split\" which I increased from 2 to 6 to lessen the chance of overfitting the model to the training data. Also I decided not to perform hyperparameter tuning through a grid of parameter ranges this time.","2ddf0b21":"The same goes for `Sex` but apparently an earlier cleaning step already removed those rows which has null values.","0d296331":"Now I turn my attention to `Drug` and `DrugId`.","fd2e18df":"It is not too bad but there are plenty of characters which convey little or no information about the sentiment of the reviewers. On the other hand there can be many signs which potentially carry hints about the outcome of the rating, for example emoticons are ubiquitous in any modern textual content on the Internet nowadays. I am going to perform the following steps:\n\n- convert positive emoticons to the word \"happyemoticon\" and negative emoticons to \"sademoticon\"\n- remove puctuations, which are none alphanumerical characters\n- remove stopwords and anything starting with numbers\n- lemmatize the text","2efe67e4":"The test accuracy is ~83% which is less than that of the training accuracy. This suggests the model is slightly overfitted because it performs better on the training set than on the test set. The difference is not much however.","3c8a0a43":"Let's remove them.","da765392":"### Importing dataset","68f54c91":"I delete the original dataframe because I do not need it anymore and I have memory constraints.","67597473":"I did warn you that word clouds may not be too revealing, didn't I?","c0bad745":"### References","926684d7":"In the code cell above I set `num_words` to 2500, so the interal vocabulary only keeps the 2500 most frequently used words. Then the tokenizer is fitted with the training data. I have seen quite a few publications where the authors fit the tokenizer to the entire dataset and subsequently split the dataset into training and test sets. I think that work flow is flawed because information can leak into the training process from the data which will be used later in the testing phase.\n\nPadding makes sure that each feature vector will have the same `maxlen` size. Sequences shorter than `maxlen` will get padded and sequences longer will get truncated.\n\nLet's build the LSTM model which is made up of a sequence of neural network layers.","c6937c02":"The model accuracy on the training set is ~87%. Let's see how it peforms on the test set by passing it through the evaluate method. ","5cb3d3af":"The scatter plot suggests a pretty homogenous distribution. We know there are much less \"Neutral\" reviews in the dataset than the other reviews, therefore the mean `Satisfaction` mostly comes from the average of \"Positive\" and \"Negative\" reviews. Perhaps there are a few more larger bubbles above the mean rating 1 (\"Neutral\") than below it.","ff1415e0":"To make life easier I simplify the categories and group them as follows:\n- `Satisfaction` 1 and 2 &rarr; 0 (Negative)\n- `Satisfaction` 3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&rarr; 1 (Neutral)\n- `Satisfaction` 4 and 5 &rarr; 2 (Positive)","aa5fb3b9":"Before I plot the grouped conditions let's look at the Top 15 most often occurring condition types.","8d8631f6":"#### RandomForestClassifier with TfidfVectorizer","8a3e9088":"#### Prediction\n\nLet's perform a test and predict the sentiment on a new review.","6d7035f7":"Let\u2019s check the number of null values.","a31ec80d":"And the result","eef36d6b":"https:\/\/en.wikipedia.org\/wiki\/Sentiment_analysis","13355dbf":"Not bad, we have gone from previous 76% to 83% accuracy without the \"Neutral\" reviews.","89ab228e":"From the chart it seems certain `Age` groups do not seem to take or review drugs, which is understandable given their young age. Also after the simplification of `Satisfaction` from 5 to 3 categories the number of \"Neutral\" reviews is now very low compared to those in the \"Positive\" and \"Negative\" categories. The most active reviewers are from Age group '45-54' and '55-64'.\n\nFinally let's plot the distribution of Satisfaction ratings.","33885b8f":"# Sentiment analysis on drug reviews","009703f4":"Instead of trying out another traditional ML algorithm I want to experiment with Neural Networks, more specifically a so called \"LSTM\" model. Long Short Term Memory networks (LSTMs) are a special type of Recurrent Neural Networks (RNNs), capable of learning long-term dependencies, that is remembering information for long periods and that is what they can do by default very well. LSTM solves the vanishing gradient problem of RNN networks.\n\nThe process is the same as before. The Tokenizer splits up the text, creates a vocabulary and turns the text into a sequence of numbers (vectors).","1d8fb83f":"In the next step I'll check individual columns. I begin with `Satisfaction`. I print then carry forward only the rows which have correct values.","7076a0f4":"Indeed, the chart above reveals about 12% new missing values in `Reviews`, 7% in `Sex` and 3% in `Age`. `Condition` also has a few (0.01%) empty cells. I remove all the rows with missing values.","8e748537":"### Cleaning\n\nFor the purpose of this kernel I will use only the following columns.","ced9478b":"It turns out the reason why `Drug` has more values than `DrugId` is that some drugs are sold is different forms, like cream, pill, gel, etc. I am going to be using `DrugId` so I do not care about this now. ","c109300f":"Now I evaluate the model on the tes test.","4972850f":"### Pre-processing `Reviews` for ML\nI select three random `Reviews` to see how they look before the cleaning.","b7bbaa46":"From the chart above it appears that:\n\n- there are many `Condition` types, a closer look may be required to see the distribution of reviews for each. \n- there are more `Drug` than `DrugId` values, which suggests some kind of peculiarity in the way drugs are named.\n- the values of `Satisfaction` is supposed to be from 1 to 5. Some reviews may have wrong or missing values.\n- `Sex` also has more values than what I am going to consider possible.\n\nBefore I explore the individual columns, I would like to see how many missing values are in the dataset. Previously I already deleted the null value rows but I also deleted leading and trailing white space which may have created new empty cells.","b4715569":"There is a great deal of misclassified \"Neutral\" reviews, amounting to ~9% but in overall we gained +12% in model accuracy. How about removing the \"Neutral\" reviews and training the model again.","19bc6fb6":"To evaluate the performance of the models, I use the traditional classification metrics such as accuracy, F1 measure and confusion matrix.","ef912c15":"The first 5 conditions make up about 35% of all conditions, with condition type \"Other\" leading the list which is just as broad of a condition as it can be. The second most common category is \"Pain\" which is again very generic.\n\nLet's see now that chart which I mentioned and prepaired the dataset for earlier.","b025b714":"This plot shows a lot, so I explain it a bit. Note that it is log scaled.\n- On the far right is the bin of conditions with number of reviews between 20,000-50,000, in this case containing just one condition, as the previous chart already depicted, which has 43,449 reviews and it is the Condition \"Other\". This group accounts for 14.8% of the total samples and 3,667 different drugs (DrugId) are reviewed in it.\n- On the far left bin are the conditions with 20 or less reviews and there are 1,022 of them adding up to 5,531 records in total which is 1.9% of the dataset. There are 1,311 unique drugs reviewed in this bin.\n- In the middle are scattered a few other bins of varying sizes but generally featuring much less conditions in each especially in the bin 1001-2000 and above. For example in the bin 10,000 (conditions with number of reviews between 5,000-10,000) there are only 4 distict conditions with 24,422 records (8.3%). There were 256 distict drugs reviewed in this group which is the lowest of all.\n\nIn summary around 40% of the dataset contains reviews for 1652 conditions with less than 2,000 reviewer per condition and around 43% of reviews rate 9 conditions with an average of 14,000 reviewer per condition. \n\nLet's see now how `Satisfaction` is looking across the Age groups.","929bc6bd":"Finally, before I tackle the `Reviews` column I explore `Condition` a bit. I would like to see the frequency of the various conditions both in value counts and in normalized form and the number of unique drugs (`DrugId`) used to treat each condition.","3b16c9d0":"And now from the \"Positive\" reviews.","19a0e0b7":"In the code cell above set up the vectorizer with vocabulary size of 2500 words (`max_features`) and  with two additional parameters. Words when occur less than 10 times (`min_df`) in the entire corpus (in all the `Reviews`) are not considered important and are excluded automatically. On the other side of the scale if a word occurs in 80% (`max_df`) or more in all of the `Reviews` then it is also not considered useful enough for classification because it is so common and therefore are ignored.\n\nThe reason why I did split first and \"fit_transform\" only on the training set afterwards is to make sure the vocabulary is learned strictly from the training set. The test set was only transformed using the already fitted vectorizer. Let's proceed fitting the MultinomialNB model. I leave the \"alpha\" at default value 1.0. The idea behind this hyperparameter is to solve the problem of zero probability. In other words if a word is not encountered in the Review the conditional probability of that word (which is 0) does not make the whole probability 0.","89864a2e":"In the code cell above, I created a Sequential() model and added the following layers:\n- Embedding layer with `input_dim` of `num_words` and `output_dim` of `embedding_vector_length`. This layer can be understood as a lookup table that maps from integer indices (of specific words in the vocabulary) to dense vectors (their embeddings)\n- SpatialDropout1D later which performs the same function as Dropout, however it drops entire 1D feature maps instead of individual elements. This layer helps prevent overfitting. Here 20% of the input units will be drop\n- LSTM, which saves information for later, thus preventing older signals from gradually vanishing during processing, I do not set dropout here because I do not want this layer to \"forget\" the past. \n- Dense layer with `softmax` activation to flatten the LSTM output\n\nWhile I compile our model I use the \"adam\" optimizer, \"categorical_crossentropy\" loss function and \"accuracy\" as metrics. 10% of the training data will be used to find the training accuracy of the model. I am also going to add a callback which will monitor the loss value and if that does not improve for 2 epochs more than 0.0001 then it will stop the fitting process.","21d5e7f5":"The model's accuracy is ~65%. Not only it missed ~13% on \"Neutral\" (1) reviews but sadly it misclassified twice as many in the \"Negative\" (0) and \"Positive\" (2) categories as well. Let's see if we can regain some of the losses by eliminating \"Neutral\" reviews and classifing only \"Negative\" and \"Positive\" feedbacks.","c935263d":"With this done let's check how the same random `Reviews` look now.","ed701e0b":"#### Recurrent Neural Network (LSTM)"}}