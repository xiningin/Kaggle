{"cell_type":{"12bfdfd3":"code","213da5a9":"code","e9355d41":"code","98f242a2":"code","dacf0b15":"code","a7a91087":"code","bc64b015":"code","9bc3d9b2":"code","551b17b4":"code","d6a6956c":"code","e005d069":"code","4f40920f":"code","134281a0":"code","8b7038ac":"code","b8869b54":"code","aa8556c5":"code","0367c438":"markdown","562c9aea":"markdown","4b2400c8":"markdown","4a566dbc":"markdown","f6cb8952":"markdown","9fc2c93d":"markdown","57f7034d":"markdown","943adcf3":"markdown","210fc14f":"markdown","fac1711f":"markdown","0d8d1161":"markdown","77058d95":"markdown"},"source":{"12bfdfd3":"import gc\nimport glob\nimport os\nimport cv2\nimport random\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport imageio as im\nfrom keras import models\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.optimizers import adam\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.utils import np_utils\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sns\nimport matplotlib\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/\"))\n\n# Any results you write to the current directory are saved as output.","213da5a9":"# load images dataset\ndef loadImagesData(glob_path):\n    images = []\n    names = []\n    for img_path in glob.glob(glob_path):\n        # load\/resize images with cv2\n        names.append(os.path.basename(img_path))\n        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        images.append(img) # already 32x32\n    return (images,names)\n# map of training label to list of images\ntrainData = {}\nnamesData = {}\nfor label in os.listdir('..\/input\/train\/'):\n    (images,names) = loadImagesData(f\"..\/input\/train\/{label}\/*.jpg\")\n    trainData[label] = images\n    namesData[label] = names\nprint(\"train labels:\", \",\".join(trainData.keys()))\nprint(len(trainData['train']))\n# show some data\nplt.figure(figsize=(4,2))\ncolumns = 4\nfor i in range(0,8):\n    plt.subplot(8 \/ columns + 1, columns, i + 1)\n    plt.imshow(trainData['train'][i])\nplt.show()","e9355d41":"train_meta = pd.read_csv('..\/input\/train.csv')\nprint(train_meta.shape)\nprint(train_meta.has_cactus.value_counts())\n# lookup table of name to has_cactus\nlookupY = {}\nfor i in range(0,len(train_meta)):\n    row = train_meta.iloc[i,:]\n    lookupY[row.id] = row.has_cactus\ntrain_meta.head()","98f242a2":"# build x\/y dataset\ntrainList = []\nmaxCount = 4364 # number of has_cactus = 0\ncounts = {'0':0,'1':0}\nfor (i,image) in enumerate(trainData['train']):\n    label = lookupY[namesData['train'][i]]\n    counts[str(label)] = 1 + counts[str(label)]\n    if counts[str(label)] < maxCount:\n        trainList.append({\n            'label': label,\n            'data': image\n        })\n# shuffle dataset\nrandom.shuffle(trainList)\n# dataframe and display\ntrain_df = pd.DataFrame(trainList)\ngc.collect()\nprint(train_df.shape)\nprint(train_df.label.value_counts())\ntrain_df.head()","dacf0b15":"# encode training data\ndata_stack = np.stack(train_df['data'].values)\ndfloats = data_stack.astype(np.float)\nall_x = np.multiply(dfloats, 1.0 \/ 255.0)\nall_x.shape","a7a91087":"all_y = np.array(train_df.label).astype(np.float)\nall_y[0:5]","bc64b015":"# split test\/training data\ntrain_x,test_x,train_y,test_y=train_test_split(all_x,all_y,test_size=0.2,random_state=7)\nprint(train_x.shape,test_x.shape)","9bc3d9b2":"# x,y and rotation data augmentation\ndatagen = ImageDataGenerator(\n    featurewise_center=False,  # set input mean to 0 over the dataset\n    samplewise_center=False,  # set each sample mean to 0\n    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n    samplewise_std_normalization=False,  # divide each input by its std\n    rotation_range=60,  # randomly rotate images in the range (degrees, 0 to 180)\n    zoom_range=0.2, # zoom images\n    horizontal_flip=True,  # randomly flip images\n    vertical_flip=True)  # randomly flip images\ndatagen.fit(train_x)","551b17b4":"# create the network\nnum_filters = 8\ninput_shape = train_x.shape[1:]\noutput_shape = 1\n# model\nm = Sequential()\ndef tdsNet(m):\n    m.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=input_shape))\n    m.add(Conv2D(16, kernel_size=3, activation='relu'))\n    m.add(Flatten())\n    m.add(Dense(units = output_shape, activation='sigmoid'))\ntdsNet(m)\n# compile adam with decay and use binary_crossentropy for single category dataset\nm.compile(optimizer = 'nadam',\n          loss = 'binary_crossentropy', \n          metrics = ['accuracy'])\n# show summary\nm.summary()","d6a6956c":"# train model\nbatch_size = 32\nhistory = m.fit_generator(datagen.flow(train_x, train_y,\n                          batch_size=batch_size),\n                          steps_per_epoch= (train_x.shape[0] \/\/ batch_size),\n                          epochs = 4,\n                          validation_data=(test_x, test_y),\n                          workers=4)","e005d069":"# create the network\nnum_filters = 8\ninput_shape = train_x.shape[1:]\noutput_shape = 1\n# model\nm = Sequential()\ndef cnnNet(m):\n    m.add(Conv2D(30, kernel_size=3, activation='relu', input_shape=input_shape))\n    m.add(MaxPooling2D(2,2))\n    m.add(Conv2D(15, kernel_size=3, activation='relu'))\n    m.add(MaxPooling2D(2,2))\n    m.add(Dense(7, activation='relu')) # <7 stops working, but higher values do nothing\n    m.add(Flatten())\n    m.add(Dense(units = output_shape, activation='sigmoid'))\ncnnNet(m)\n# compile adam with decay and use binary_crossentropy for single category dataset\nm.compile(optimizer = 'nadam',\n          loss = 'binary_crossentropy', \n          metrics = ['accuracy'])\n# show summary\nm.summary()","4f40920f":"# train model\nbatch_size = 32\nhistory = m.fit_generator(datagen.flow(train_x, train_y,\n                          batch_size=batch_size),\n                          steps_per_epoch= (train_x.shape[0] \/\/ batch_size),\n                          epochs = 4,\n                          validation_data=(test_x, test_y),\n                          workers=4)","134281a0":"# build complete x\/y dataset\ntrainList = []\nfor (i,image) in enumerate(trainData['train']):\n    label = lookupY[namesData['train'][i]]\n    trainList.append({\n        'label': label,\n        'data': image\n    })\n# shuffle dataset\nrandom.shuffle(trainList)\n# dataframe and display\ntrain_df = pd.DataFrame(trainList)\ngc.collect()\n# encode training data\ndata_stack = np.stack(train_df['data'].values)\ndfloats = data_stack.astype(np.float)\nall_x = np.multiply(dfloats, 1.0 \/ 255.0)\nall_x.shape\nall_y = np.array(train_df.label).astype(np.float)\n# split test\/training data\ntrain_x,test_x,train_y,test_y=train_test_split(all_x,all_y,test_size=0.2,random_state=7)\nprint(train_x.shape,test_x.shape)","8b7038ac":"# continue training model\nbatch_size = 64\nhistory = m.fit_generator(datagen.flow(train_x, train_y,\n                          batch_size=batch_size),\n                          steps_per_epoch= (train_x.shape[0] \/\/ batch_size),\n                          epochs = 4,\n                          validation_data=(test_x, test_y),\n                          workers=4)","b8869b54":"# check sample submission format\npd.read_csv('..\/input\/sample_submission.csv').head()","aa8556c5":"# output predicted submission csv\n(test_images, test_names) = loadImagesData(f\"..\/input\/test\/test\/*.jpg\")\ndata_stack = np.stack(test_images)\ndfloats = data_stack.astype(np.float32)\nunknown_x = np.multiply(dfloats, 1.0 \/ 255.0)\n# predict\npredicted = np.ravel(m.predict(unknown_x))\nsubmission_df = pd.DataFrame({'id':test_names,'has_cactus':predicted})\nsubmission_df.to_csv('submission.csv', index=False)\nlen(submission_df)","0367c438":"Here I write the function for loading and preprocessing the image data. There's only one target category in this dataset so I show the first 8 images. ","562c9aea":"Here I define the data augmenter. I use x,y and rotation as the images were taken from aerial and thus can vary in these ways.","4b2400c8":"Since we use binary_crossentropy, the y category data just needs to be made as floats","4a566dbc":"Encode x data as numpy stack","f6cb8952":"Checking out the train.csv data, use value_counts to check relative number of 0 and 1 has_cactus values","9fc2c93d":"I found I could do slightly better and have fewer trainable parameters if I use max pooling layers and a dense layer at the end.","57f7034d":"Make the training\/validation split to measure training accuracy","943adcf3":"Output predictions file","210fc14f":"Build a dataframe of all the x and y data. I then build a more even dataset of 50% 0 and 1 to help the training process.","fac1711f":"I'm basing this kaggle on my previous [Plant Seedling - Simple CNN](https:\/\/www.kaggle.com\/masonblier\/plant-seedling-simple-cnn) kaggle notebook. ","0d8d1161":"Starting with the simple stacked 3x3 conv net from Towards Data Science","77058d95":"Finish training on the rest of the data"}}