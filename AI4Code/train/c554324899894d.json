{"cell_type":{"4bed6c4a":"code","7354f219":"code","7e77ec58":"code","42f0827d":"code","f3b03ee4":"code","8327172d":"code","01188243":"code","d14d2829":"code","e518f19f":"code","c9523e31":"code","2eb0563f":"code","8ef151f2":"code","5c7403ee":"code","460e7f5a":"code","8b2ff9de":"code","b8eefc7f":"code","9753952a":"code","ba88308a":"code","2c107c66":"code","fd5cf1cb":"code","1cf3c955":"code","7602293a":"code","5e2f2d1e":"markdown","1dabcab1":"markdown","148047ed":"markdown","9f1b0d91":"markdown","aeb1c889":"markdown","9753750c":"markdown","6fdb03cb":"markdown","c3f77c9d":"markdown","5c95f3a2":"markdown","e9a177ac":"markdown","6bde5148":"markdown","890bab88":"markdown"},"source":{"4bed6c4a":"import numpy as np\nimport scipy.optimize as op\nimport pandas as pd \nimport matplotlib.pyplot as plt\n","7354f219":"dfTrain = pd.read_csv('..\/input\/Train.csv')   #Training Dataset\ndfTest = pd.read_csv('..\/input\/Test.csv')   #Test Dataset\ndfValid = pd.read_csv('..\/input\/Valid.csv') #Validation Dataset\ndfTrain.head()","7e77ec58":"dfTrain.plot(x='X',y='Y',kind='scatter')","42f0827d":"def extractFeatures(df):\n    df_Features=df.iloc[:,0:1]\n    df_Label=df.iloc[:,1:2]\n    X=df_Features.values\n    Y=df_Label.values\n    return X,Y","f3b03ee4":"X,Y=extractFeatures(dfTrain)\nXval,Yval=extractFeatures(dfValid)\nXtest,Ytest=extractFeatures(dfTest)","8327172d":"def mapFeature(X,degree):\n    \n    sz=getThetaSizeFromDegree(X,degree)\n    out=np.ones((X.shape[0],sz))\n\n    sz=X.shape[1]\n    if (sz==2):\n        X1=X[:, 0:1]\n        X2=X[:, 1:2]\n        col=1\n        for i in range(1,degree+1):        \n            for j in range(0,i+1):\n                out[:,col:col+1]= np.multiply(np.power(X1,i-j),np.power(X2,j))    \n                col+=1\n        return out\n    else:\n        for i in range(1,degree+1):        \n            out[:,i:i+1]= np.power(X,i)\n    \n    return out","01188243":"def getThetaSizeFromDegree(X,degree):\n    sz=X.shape[1]\n    if (sz==2):\n        sz=(degree+1)*(degree+2)\/2\n        sz=int(sz)\n    else:\n         sz=degree+1\n    return sz","d14d2829":"def getDegreeFromTheta(theta,X):\n    sz=theta.shape[0]\n    if (X.shape[1]==2):\n        degree=(np.sqrt(sz*8+1)-3)\/2\n        degree=int(degree)\n    else:\n         degree=sz-1\n    return degree","e518f19f":"def featureNormalize(X):\n    mu = np.mean(X, axis=0)\n    sigma = np.std(X, axis=0)\n    #Handle Bias Vector\n    if(mu[0]==1 and sigma[0]==0):\n        mu[0]=0\n        sigma[0]=1\n    X_norm = X - mu\n    X_norm = X_norm\/sigma\n    return X_norm, mu, sigma","c9523e31":"def computeCost(theta,X,y,regLambda):\n    m,n = X.shape\n    theta.shape = (n,1)\n\n    h=np.matmul( X,theta)                      #Hypothesis\n    err=h-y\n    errSqr=err**2\n    J=(1.0\/(2.0*m))* np.sum(errSqr)\n    \n    \n    regularized_theta=np.concatenate((np.zeros((1,1)) , theta[1:,:]),axis=0)\n    J=J +regLambda* (1.0\/(2.0*m)) *(np.sum(regularized_theta**2))\n \n    \n    return J\n","2eb0563f":"\ndef initTheta(X,degree):\n    size=getThetaSizeFromDegree(X,degree)\n    #newTheta=np.random.rand(size, 1)*1000\n    newTheta=np.zeros((size, 1))\n    return  newTheta\n\ndef gradientDescent(X, y, theta, alpha, iterations,degree,regLambda):        \n    m=len(y)\n    I=np.zeros((iterations,1),dtype=float)\n    J=np.zeros((iterations,1),dtype=float)\n    for k in range(iterations):\n        h=np.matmul( X,theta)                      #Hypothesis\n        err=h-y\n        d=np.matmul(err.T,X)  \n        g=  alpha*((1.0\/m)*d)              #Derivative\n        g=g.T     #Theta Itrations  \n        regularized_theta=np.concatenate((np.zeros((1,1)) , theta[1:,:]),axis=0)\n        g=g+ (regLambda\/m)*regularized_theta \n        I[k]=k*1.0\n        J[k]=computeCost(theta,X,y,regLambda)\n        theta=theta-g\n    \n    \n    \n    plt.plot(I, J,color='r')\n    return theta\n\n####################################################################\ndef addBiasVector(X):\n    return np.concatenate((np.ones((X.shape[0],1)),X),axis=1)","8ef151f2":"def computeGradient(theta,X,y,regLambda):\n    m,n = X.shape\n    theta.shape = (n,1) \n    h=np.matmul( X,theta)                      #Hypothesis\n    err=h-y\n    d=np.matmul(err.T,X)  \n    g=  (1.0\/m)*d\n    g=g.T\n    regularized_theta=np.concatenate((np.zeros((1,1)) , theta[1:,:]),axis=0)\n        \n    g=g + (regLambda\/m)*regularized_theta\n\n    \n\n    return g.flatten()","5c7403ee":"def optimizedGradientDescent(X, y,degree,regLambda):        \n    maxiter = 200\n    theta =np.zeros((X.shape[1], 1))\n    Result = op.minimize(fun = computeCost, x0 = theta,  args = (X, y,regLambda), options={'disp': False, 'maxiter':maxiter}, method=\"L-BFGS-B\",jac = computeGradient)\n    optimal_theta = Result.x\n    return optimal_theta","460e7f5a":"def predict(theta,X,mu, sigma):\n    degree=getDegreeFromTheta(theta,X)\n    X=mapFeature(X,degree)\n    X = X - mu\n    X = X\/sigma\n    Py=np.matmul(X, theta)\n    return Py","8b2ff9de":"def plotHypothesis(theta,X,y,regLambda,mu, sigma):\n    degree=getDegreeFromTheta(theta,X)\n    plt.scatter(X,y) \n    plt.title(\"Lambda=\"+str(regLambda)+\",Degree=\"+str(degree))\n    x_min, x_max = X[:, 0].min()-1 , X[:, 0].max()+1 \n    u = np.linspace(x_min, x_max, 100)\n    u.shape=(len(u),1) \n    v=predict(theta,u,mu, sigma) \n    plt.plot(u, v,color='r')","b8eefc7f":"plt.figure(figsize=(20,10))\nregLambdaList=[0,0,0,0,0,0,0,1,3,5,10,100]\ndegreeList=[1,2,3,4,5,6,8,8,8,8,8,8]\nfor i in range(len(regLambdaList)):\n    regLambda=regLambdaList[i]\n    degree=degreeList[i]\n    Xp=mapFeature(X,degree)    #Polynomial\n    Xn, mu, sigma = featureNormalize(Xp)  # Normalize\n    theta = optimizedGradientDescent(Xn, Y, degree,regLambda)  \n    #Without Lib   \n    #theta = gradientDescent(Xn, y, theta,alpha,iter,degree,regLambda)\n    plt.subplot(2 , int(len(regLambdaList)\/2 +0.5), i+1)\n    plotHypothesis(theta,X,Y,regLambda,mu, sigma)\nplt.show()","9753952a":"def plotLearningCurve(Xtrain, ytrain, Xval, yval, degree,regLambda):\n    m = len(Xtrain)\n    Xtrain_p=mapFeature(Xtrain,degree)    #Polynomial\n    Xtrain_n, mu, sigma = featureNormalize(Xtrain_p)  # Normalize\n\n    Xval_p=mapFeature(Xval,degree)    #Polynomial\n    Xval_n = Xval_p - mu\n    Xval_n = Xval_n\/sigma\n    \n    training_error = np.zeros((m, 1))\n    validation_error   = np.zeros((m, 1))\n\n    for i in range(m):\n        #Learning\n        Current_Xtrain=Xtrain_n[0:i+1]\n        Current_ytrain=ytrain[:i+1]\n        theta = optimizedGradientDescent(Current_Xtrain, Current_ytrain,degree,regLambda)        \n        training_error[i]=computeCost(theta,Current_Xtrain,Current_ytrain,0)\n        validation_error[i]=computeCost(theta,Xval_n,yval,0)\n    \n    plt.plot(range(1,m+1), training_error)\n    plt.plot( range(1,m+1), validation_error)\n    plt.title('Learning Curve (Lambda = '+str(regLambda)+',Degree='+str(degree)+')')  \n    plt.legend(('Training', 'Cross Validation'))   \n    plt.xlabel(\"Training\")\n    plt.ylabel(\"Error\/Cost\")\n    return\n","ba88308a":"plt.figure(figsize=(20,10))\nregLambdaList=[0,0,0.01,1]\ndegreeList=[1,8,8,8]\nfor i in range(len(regLambdaList)):\n    regLambda=regLambdaList[i]\n    degree=degreeList[i]\n    plt.subplot(2 , int(len(regLambdaList)\/2 +0.5), i+1)\n    plotLearningCurve(X,Y,Xval,Yval,degree,regLambda)\nplt.show()","2c107c66":"def plotValidationCurveForLambda(Xtrain, ytrain, Xval, yval, degree,regLambdaList):\n    \n    Xtrain_p=mapFeature(Xtrain,degree)    #Polynomial\n    Xtrain_n, mu, sigma = featureNormalize(Xtrain_p)  # Normalize\n\n    Xval_p=mapFeature(Xval,degree)    #Polynomial\n    Xval_n = Xval_p - mu\n    Xval_n = Xval_n\/sigma\n    \n    training_error = np.zeros((len(regLambdaList), 1))\n    validation_error   = np.zeros((len(regLambdaList), 1))\n\n    for i in range(len(regLambdaList)):\n        regLambda=regLambdaList[i]\n        theta = optimizedGradientDescent(Xtrain_n, ytrain, degree,regLambda)\n        training_error[i]=computeCost(theta,Xtrain_n,ytrain,0)\n        validation_error[i]=computeCost(theta,Xval_n,yval,0)\n    \n    plt.plot(regLambdaList, training_error)\n    plt.plot( regLambdaList, validation_error)\n    plt.title('Validation Curve (Degree='+str(degree)+')')  \n    plt.legend(('Training', 'Cross Validation'))   \n    plt.xlabel(\"Lambda\")\n    plt.ylabel(\"Error\/Cost\")\n    return","fd5cf1cb":"plt.figure(figsize=(20,10))\ndegreeList=[1,2,5,8]\nregLambdaList=[0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]\nfor i in range(len(degreeList)):\n    degree=degreeList[i]\n    plt.subplot(2 , int(len(degreeList)\/2 +0.5), i+1)\n    plotValidationCurveForLambda(X,Y,Xval,Yval,degree,regLambdaList)\nplt.show()","1cf3c955":"def plotFinalCurve(Xtrain, ytrain, Xtest, ytest, degree,regLambda):\n    Xp=mapFeature(Xtrain,degree)    #Polynomial\n    Xn, mu, sigma = featureNormalize(Xp)  # Normalize\n    theta = optimizedGradientDescent(Xn, ytrain, degree,regLambda)        \n    Xtest_p=mapFeature(Xtest,degree)    #Polynomial\n    Xtest_n = Xtest_p - mu\n    Xtest_n = Xtest_n\/sigma\n    testErr=computeCost(theta,Xtest_n,ytest,0)\n    #PLOT   \n    X=np.concatenate((Xtrain,Xtest),axis=0)\n    y=np.concatenate((ytrain,ytest),axis=0)\n    x_min, x_max = X[:, 0].min()-1 , X[:, 0].max()+1 \n    u = np.linspace(x_min, x_max, 100)\n    u.shape=(len(u),1) \n    v=predict(theta,u,mu, sigma) \n    plt.plot(u, v,color='r')\n    plt.scatter(Xtrain,ytrain) \n    plt.scatter(Xtest,ytest)\n    plt.title(\"Test data Lambda=\"+str(regLambda ) +\" , degree=\"+str(degree)+\" with Error=\"+str(round(testErr,4)))\n    plt.legend((\"Regression(Lambda=3,degree=8)\",\"Training Data\",\"Test Data\"))\n    return","7602293a":"plt.figure(figsize=(10,5))\ndegree=8\nregLambda=3\nplotFinalCurve(X,Y,Xtest,Ytest,degree,regLambda)\nplt.show()","5e2f2d1e":"<h2>Extract Input Feature to <b>X <\/b>and Label to <b>y<\/b>","1dabcab1":"<H1>Read Data from CSV","148047ed":"# Helper Functions","9f1b0d91":"<h3> Gradient Descent Algorithm with Regularization <\/h3>\n<p>\nWe start with assumpution equation (Called hypothesis) which can fit above data points.   \n<p>\n$h(x) = w_0 + w_1 x$\n<\/p> \nThe two coefficients with initial guess (i.e. $w_0$ and $w_1$) of $h(x)$ will be fed into the algorithm.\nThen Program will start from initial guess and then iterate steps to find the best fit.\n\n<p>\n Our objective is to minimize Loss.\n    <p>\n $ L(W)=   \\hat{Y}-Y$  Where  $\\hat{Y}=h(X)$\n <\/p>\nSince Loss can negative or postive, we need to minimize the absolute values ( OR Mean squared) Loss so we define Loss\/Cost function as follows","aeb1c889":"<h3>Cost\/Loss Function with Regularization<\/h3>\nWe can measure the accuracy of our hypothesis function by using a cost function. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from x's and the actual output y's.\n\n$L(W) = \\dfrac {1}{2n} \\displaystyle \\sum _{i=1}^n \\left ( \\hat{Y}_{i}- Y_{i} \\right)^2+\\frac{1}{n} \\frac{\\lambda}{2} \\sum_{j} w_{j}^{2}$\n<p>\n$L(w_0, w_1)  = \\dfrac {1}{2n} \\displaystyle \\sum _{i=1}^n \\left (h(x_{i}) - y_{i} \\right)^2 +\\frac{1}{n} \\frac{\\lambda}{2}  \\sum_{j} w_{j}^{2}$ \n\nThis Loss\/cost function is also called the \"Squared error function\", or \"Mean squared error\". The mean is halved $\\left(\\frac{1}{2}\\right)$as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the $\\frac{1}{2}$  term.","9753750c":"<h5> Visualize Data","6fdb03cb":"# Plotting Learning Curve","c3f77c9d":"# Final Plot and Test Error","5c95f3a2":"# Regularization\nRegularized Linear Regression using Gradient Descent Algorithm","e9a177ac":"We minimize Loss by taking the derivative (the tangential line to a function) of our cost\/loss function. The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost\/loss function in the direction with the steepest descent. The size of each step is determined by the parameter \u03b1($alpha$), which is called the learning rate. The direction in which the step is taken is determined by the partial derivative of $L(w_0,w_1)$. \n\nThe gradient descent algorithm is:\n\nrepeat until convergence:<p>\n{<p>\n&nbsp;&nbsp;    $w_0 := w_0 - \\alpha \\frac{\\partial}{\\partial w_0} L(w_0, w_1) + \\frac{\\lambda}{n} \\sum_{j} w_{j}$<p>\n&nbsp;&nbsp;    $w_1 := w_1 - \\alpha \\frac{\\partial}{\\partial w_1} L(w_0, w_1) + \\frac{\\lambda}{n} \\sum_{j} w_{j}$<p>\n}\n\nOR<p>\n$\\begin{align*} \\text{repeat until convergence: } \\lbrace & \\newline w_0 := & w_0 - \\alpha \\frac{1}{n} \\sum\\limits_{i=1}^{n}(h_w(x_{i}) - y_{i}) + \\frac{\\lambda}{n} \\sum_{j} w_{j}\\newline w_1 := & w_1 - \\alpha \\frac{1}{n} \\sum\\limits_{i=1}^{n}\\left((h_w(x_{i}) - y_{i}) x_{i}\\right) + \\frac{\\lambda}{n} \\sum_{j} w_{j}\\newline \\rbrace& \\end{align*}$","6bde5148":"# Plotting Validation Curve","890bab88":"# Plotting With Different Regularization Parameters and degree"}}