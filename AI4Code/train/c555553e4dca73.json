{"cell_type":{"bb4b0ec0":"code","c2122253":"code","3e806ff8":"code","2d1ec2c9":"code","a692fdb7":"code","62c75cf9":"code","62f92c90":"code","655402c5":"code","1ebc1aa6":"code","c787448e":"code","d95d6f17":"code","bed9d686":"code","1087a81a":"code","dddbea06":"code","e1dfbf81":"code","a519e812":"code","c2b76544":"code","9b4bb707":"code","60f756d0":"code","52a58013":"code","d438fe5f":"code","17805bdb":"code","c34938fb":"code","b8f1a88f":"code","66e7b497":"code","09662d5a":"code","940cb81d":"code","67b5c270":"markdown","c37ba0c7":"markdown","bad11978":"markdown","6c050455":"markdown","bd99611f":"markdown","5f3c1c75":"markdown","f22bc5e2":"markdown","94343bb4":"markdown","24020a95":"markdown","2640d24a":"markdown","9d3c9d1f":"markdown","09584942":"markdown","f9bb5ed5":"markdown","8dc9d50b":"markdown","c320f534":"markdown","a211dcc4":"markdown","5d9cebff":"markdown","86743bac":"markdown","2e61126e":"markdown","9bd7019d":"markdown","17aebb09":"markdown","e0d83d7a":"markdown"},"source":{"bb4b0ec0":"# Importando todas as bibliotecas\n\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scikitplot as skplt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier","c2122253":"#Carregando o banco\ndf = pd.read_csv('\/kaggle\/input\/hmeq-data\/hmeq.csv')\ndf.head(20)","3e806ff8":"#verificando se o tipo dos dados no banco est\u00e1 em conformidade com o dicion\u00e1rio\ndf.info()","2d1ec2c9":"#verificando a quantidade de valores faltantes dos campos\ndf.isnull().sum()","a692fdb7":"#Boxplots dos valores das d\u00edvidas contra\u00eddas em fun\u00e7\u00e3o da raz\u00e3o da hipoteca. Est\u00e3o comparados pelo pagamento ou n\u00e3o.\nsns.boxplot(x=\"REASON\", y=\"LOAN\",\n            hue=\"BAD\", palette=[\"m\", \"g\"],\n            data=df)\nsns.despine(offset=10, trim=True)","62c75cf9":"#Scatterplot do valor da propriedade com o valor atual da hipoteca\nplt.plot(range(500000))\nax = sns.scatterplot(x=\"VALUE\", y=\"MORTDUE\",\n                     hue=\"BAD\",\n                     data=df)\nplt.xlim(0, 500000)\nplt.ylim(0, 500000)\nplt.gca().set_aspect('equal', adjustable='box')\nplt.draw()","62f92c90":"#Embonecando as vari\u00e1veis categ\u00f3ricas\ndf = pd.get_dummies(df, columns=['REASON','JOB'])\ndf.head().T","655402c5":"#Definindo as vari\u00e1veis independentes\nfeats = [c for c in df.columns if c not in ['BAD']]","1ebc1aa6":"#Separando a base em treino e teste\ntrain, valid = train_test_split(df, test_size=0.2, random_state=42)\n\ntrain.shape, valid.shape","c787448e":"#Rodando o primeiro modelo\nrf = RandomForestClassifier(n_estimators=200, random_state=42)\nrf.fit(train[feats], train['BAD'])","d95d6f17":"#Imputa\u00e7\u00e3o\ndf.fillna(-1, inplace=True)\ndf.isnull().sum()","bed9d686":"#Refazendo a separa\u00e7\u00e3o\ntrain, valid = train_test_split(df, test_size=0.2, random_state=42)\n\ntrain.shape, valid.shape","1087a81a":"#Rodando o primeiro modelo novamente\nrf = RandomForestClassifier(n_estimators=200, random_state=42)\nrf.fit(train[feats], train['BAD'])","dddbea06":"#Aplicando o modelo na base de valida\u00e7\u00e3o e verificando a acur\u00e1cia\npreds_val = rf.predict(valid[feats])\n\naccuracy_score(valid['BAD'], preds_val)","e1dfbf81":"#Mostrando a matriz de confus\u00e3o\nskplt.metrics.plot_confusion_matrix(valid['BAD'],preds_val)","a519e812":"#Testando o limitador de profundidade da \u00e1rvore\nfor i in range(1,11,1):\n    rft = RandomForestClassifier(n_estimators=200, random_state=42, max_depth=i)\n    rft.fit(train[feats], train['BAD'])\n    pred_teste = rft.predict(valid[feats])\n    print(str(i)+\" de profundidade: \"+str(accuracy_score(valid['BAD'], pred_teste)))","c2b76544":"#Testando o n\u00famero de estimadores\nfor i in range(1000,100,-100):\n    rft = RandomForestClassifier(n_estimators=i, random_state=42)\n    rft.fit(train[feats], train['BAD'])\n    pred_teste = rft.predict(valid[feats])\n    print(str(i)+\": \"+str(accuracy_score(valid['BAD'], pred_teste)))","9b4bb707":"#Verificando o desbalanceio da vari\u00e1vel dependente\ndf['BAD'].value_counts()","60f756d0":"#Testando colocar pesos nas possibilidades pagadores para atacar o desbalanceio\nclass_weight = dict({1:4, 0:1})\nrdf = RandomForestClassifier(bootstrap=True,\n            class_weight=class_weight, \n            criterion='gini',\n            max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=4, min_samples_split=10,\n            min_weight_fraction_leaf=0.0, n_estimators=200,\n            oob_score=False,\n            random_state=42,\n            verbose=0, warm_start=False)\n\nrdf.fit(train[feats], train['BAD'])\n\npred_teste = rdf.predict(valid[feats])\nprint(accuracy_score(valid['BAD'], pred_teste))","52a58013":"# cria o vetor de notas, mostra e mostra a m\u00e9dia\nscores = cross_val_score(rf, df[feats], df['BAD'], n_jobs=-1, cv=5)\n\nscores, scores.mean()","d438fe5f":"# cria um objeto xgb\nxgb = XGBClassifier(n_estimators=200, n_jobs=-1, random_state=42, learning_rate=0.05)","17805bdb":"#Usa o cross validation como antes, mas com o xgb\nscores = cross_val_score(xgb, df[feats], df['BAD'], n_jobs=-1, cv=5)\n\nscores, scores.mean()","c34938fb":"#Cria um dicion\u00e1rio com os tipos de par\u00e2metros que ser\u00e3o testados\ngrid_param = {\n    'n_estimators': [100, 300, 500, 800, 1000],\n    'criterion': ['gini', 'entropy'],\n    'bootstrap': [True, False]\n}","b8f1a88f":"#Cria o objeto grid search utilizando o dicion\u00e1rio anterior\ngd_sr = GridSearchCV(estimator=rf,\n                     param_grid=grid_param,\n                     scoring='accuracy',\n                     cv=5,\n                     n_jobs=-1)","66e7b497":"#Treina o modelo testando combina\u00e7\u00f5es de todos os par\u00e2metros (demorado)\ngd_sr.fit(df[feats], df['BAD'])","09662d5a":"#Mostra os melhores par\u00e2metros\nbest_parameters = gd_sr.best_params_\nprint(best_parameters)","940cb81d":"#Mostra a acur\u00e1cia com os melhores par\u00e2metros\nbest_result = gd_sr.best_score_\nprint(best_result)","67b5c270":"## Modelagem\n","c37ba0c7":"Vemos que a profundidade da \u00e1rvore n\u00e3o melhora o modelo. Limit\u00e1-la diminuiu a acur\u00e1cia em compara\u00e7\u00e3o com o nosso valor original","bad11978":"**Vemos na matriz de confus\u00e3o os seguintes dados:**\n<br\/>**Sensibilidade:** 0,72 (190\/(190+75)) -> Capacidade de acertar os positivos\n<br\/>**Especificidade:** 0,97 (897\/(897+30)) -> Capacidade de acertar os negativos\n<br\/>**Acur\u00e1cia:** 0,91 (897+190)\/1192) -> N\u00famero de acertos totais frente ao tamanho do teste\n<br\/><br\/>Surpreendentemente parece se tratar de um bom modelo, mas vamos tentar melhor\u00e1-lo ainda mais.","6c050455":"Uma alternativa a ter mexido nos par\u00e2metros na m\u00e3o como fizemos l\u00e1 em cima \u00e9 o Grid Search. Vamos demonstr\u00e1-lo abaixo com o random forest.","bd99611f":"O n\u00famero de estimadores j\u00e1 contribui alguma coisa com o modelo. Com 1000 estimadores temos uma melhora de 0,0008 na acur\u00e1cia. Mas o desempenho n\u00e3o \u00e9 linear como na profundidade. Ao subir de 200 para 300 temos uma piora, fica um tempo estagnado, em 800 temos o valor inicial novamente e uma pequena melhora em 1000.","5f3c1c75":"## Cross Validation","f22bc5e2":"Como verificamos l\u00e1 em cima, existem muitos valores faltantes na base e o modelo n\u00e3o vai funcionar dessa forma. Partiremos ent\u00e3o para a imputa\u00e7\u00e3o.","94343bb4":"Nesse caso n\u00e3o temos base de teste verdadeira (sem a vari\u00e1vel dependente), ent\u00e3o separaremos a base somente em treino e valida\u00e7\u00e3o","24020a95":"## Conclus\u00e3o","2640d24a":"## Grid Search","9d3c9d1f":"Percebemos que os inadimplentes pegaram empr\u00e9stimos em valores menores quando o motivo era melhoria da propriedade. J\u00e1 no motivo consolida\u00e7\u00e3o de d\u00e9bito, a massa de dados \u00e9 bastante parecida entre os pagadores, com os inadimplentes tendo ainda uma mediana um pouco menor. Em todos os boxplots percebemos grandes quantidades de outliers a maior.","09584942":"Em vez de dividir em treino e teste como fizemos, poderiamos ter feito o cross validation logo de cara, para ter uma no\u00e7\u00e3o da qualidade do modelo pretendido e at\u00e9 mesmo comparar dois tipos de modelos diferentes. Vamos ver essa compara\u00e7\u00e3o entre o random forest e o XGB","f9bb5ed5":"## Dicion\u00e1rio dos dados","8dc9d50b":"Foram testados diversos pesos, mas o m\u00e1ximo de acur\u00e1cia alcan\u00e7ada com o peso 3 para o mau pagador foi a mesma do modelo original. Deixei com peso 4 para ilustrar que o esfor\u00e7o n\u00e3o adiantou muito.","c320f534":"**VARI\u00c1VEL** - TIPO - DESCRI\u00c7\u00c3O\n<br\/>**BAD** - num\u00e9rico - assume 1 se o cliente n\u00e3o cummpriu com o empr\u00e9stimo e 0 se cumpriu com o pagamento \n<br\/>**LOAN** - num\u00e9rico - valor total da d\u00edvida que o cliente quer contrair\n<br\/>**MORTDUE** - num\u00e9rico - valor atual da hipoteca\n<br\/>**VALUE** - num\u00e9rico - valor da propriedade hipotecada\n<br\/>**REASON** - categ\u00f3rico - raz\u00e3o da hipoteca. Se DebtCon \u00e9 consolida\u00e7\u00e3o de d\u00edvida e se HomeImp e para melhoria da propriedade\n<br\/>**JOB** - categ\u00f3rico - emprego do devedor. Seis categorias pr\u00f3prias\n<br\/>**YOJ** - num\u00e9rico - anos no emprego\n<br\/>**DEROG** - num\u00e9rico - n\u00famero de relat\u00f3rios de inconformidades\n<br\/>**DELINQ** - num\u00e9rico - n\u00famero de linhas de cr\u00e9dito inadimplentes\n<br\/>**CLAGE** - n\u00famerico - idade da linha de cr\u00e9dito mais antiga em meses\n<br\/>**NINQ** - num\u00e9rico - n\u00famero de linhas de cr\u00e9dito recentes\n<br\/>**CLNO** num\u00e9rico - total de linhas de cr\u00e9dito\n<br\/>**DEBTINC** - n\u00famerico - raz\u00e3o entre o d\u00e9bito e as entradas do cliente","a211dcc4":"Todas as observa\u00e7\u00f5es acima da linha identidade possuem um valor de hipoteca maior que o pr\u00f3prio valor da propriedade hipotecada. Curiosamente, a maioria n\u00e3o \u00e9 \"m\u00e1 pagadora\".","5d9cebff":"Os desempenhos do xgb n\u00e3o foram t\u00e3o bons quanto da random forest","86743bac":"   Num pr\u00f3ximo modelo podemos utilizar o cross validation logo de cara para decidir que tipo de modelo usar e ter uma ideia da m\u00e9dia de acur\u00e1cia dos modelos mesmo sem separar a base. Al\u00e9m disso, para melhorar o modelo podemos testar v\u00e1rias combina\u00e7\u00f5es de par\u00e2metros com o Grid Search at\u00e9 achar os melhores par\u00e2metros. Logicamente falando essa seria a maneira mais organizada de fazer as coisas. Entretanto foi bom ter feito a separa\u00e7\u00e3o antes de tudo isso. Em primeiro lugar, porque ilustra o caminho de aprendizado da disciplina, mas principalmente porque mostra o poder que uma separa\u00e7\u00e3o feita de determinada forma tem na modelagem (provavelmente mostra na realidade a for\u00e7a do overfitting). \n\n   Outros par\u00e2metros poderiam ter sido testados no grid search, como class_weight, max_features, max_leaf_nodes, min_impurity_decrease, min_impurity_split, min_samples_leaf, min_samples_split, min_weight_fraction_leaf, etc.\n    \n   De qualquer forma, para todos os efeitos, o modelo de random forest gerado com a base separada foi o que desempenhou melhor, com impressionantes 91,2% de acur\u00e1cia.","2e61126e":"Podemos ver que atrav\u00e9s do cross validation conseguimos modelos com acur\u00e1cias piores do que a separa\u00e7\u00e3o que foi feita com o seed 42. Demos sorte! Vamos testar agora cross validation + xgb","9bd7019d":"O valor de mal pagadores \u00e9 de 20% dos registros (1189\/5960). Vamos testar aplicar pesos diferentes para esses registros","17aebb09":"# Atividade avaliativa para a disciplina DMMLII\n## Aluno: Hermes Araujo","e0d83d7a":"## An\u00e1lise explorat\u00f3ria"}}