{"cell_type":{"6e067af7":"code","f4fcd005":"code","2a771cd8":"code","aab74710":"code","2d7ca970":"code","d5d54216":"code","511ea032":"code","813b134d":"code","777de664":"code","0af97222":"code","953dc9b0":"code","eb1a171b":"code","8083f2a9":"code","996558fa":"code","f532822b":"code","520bf0e0":"code","7baee44f":"code","9d741eac":"code","b8edab8a":"code","d50ad5be":"code","27d4cfff":"code","3514eb31":"code","d57332c5":"code","ba033090":"code","6233c9a6":"code","812758f5":"code","42348d54":"code","801ace51":"code","95187eab":"code","d2aef5ff":"code","62005b20":"code","1cc9162b":"code","01c8dc15":"code","fd7b5707":"code","64c82781":"code","66247445":"code","2ee38d94":"code","d8b17c04":"code","f572bc84":"code","78d02867":"code","90c3603f":"markdown","80b2fcc5":"markdown","9723c17e":"markdown","3b80ca8e":"markdown","f02ba85c":"markdown","66e86086":"markdown","24fc1c2f":"markdown","ab61309a":"markdown","549d7408":"markdown","11e0401a":"markdown","eb146eb1":"markdown","e8889004":"markdown","220df45d":"markdown","cda16207":"markdown","354c2a88":"markdown","90298b20":"markdown","703263e0":"markdown","86adc4a1":"markdown","9d689bd0":"markdown","5fe74e2d":"markdown","e883be8e":"markdown","fa95e7ce":"markdown","7ed14c0a":"markdown","cfd75f7b":"markdown","1f88bf2b":"markdown","dbc8cce0":"markdown"},"source":{"6e067af7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f4fcd005":"data = pd.read_csv('..\/input\/costa-rican-household-poverty-prediction\/train.csv')","2a771cd8":"data.head()","aab74710":"data.info()","2d7ca970":"for label, content in data.items():\n    if pd.api.types.is_string_dtype(content):\n        print(label)","d5d54216":"df = data.copy()","511ea032":"print(df['Target'].value_counts())\ndf['Target'].value_counts().plot(kind='bar')","813b134d":"def preprocess_data(df):    \n    # Fill numeric rows with the median\n    df.drop('Id', axis=1)\n    df.set_index('Id', inplace=True)\n    for label, content in df.items():\n        if pd.api.types.is_numeric_dtype(content):\n            if pd.isnull(content).sum():\n                #df[label+\"_is_missing\"] = pd.isnull(content)\n                df[label] = content.fillna(content.median())\n                \n        # Turn categorical variables into numbers\n        if not pd.api.types.is_numeric_dtype(content):\n            #df[label+\"_is_missing\"] = pd.isnull(content)\n            # We add the +1 because pandas encodes missing categories as -1\n            df[label] = pd.Categorical(content).codes+1        \n    \n    return df\npreprocess_data(df)","777de664":"X = df.drop('Target', axis=1)\ny = df['Target']","0af97222":"X","953dc9b0":"y","eb1a171b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","8083f2a9":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier().fit(X_train, y_train)\nrf_pred = rf.predict(X_test)\nprint('R2 score is : {:.2f}'.format(accuracy_score(y_test, rf_pred)))\nprint('\\n')\nprint(\"Classification Report : \")\nprint(classification_report(y_test,rf_pred))","996558fa":"from sklearn.ensemble import ExtraTreesClassifier\netc = ExtraTreesClassifier().fit(X_train, y_train)\netc_pred = etc.predict(X_test)\nprint('R2 score is : {:.2f}'.format(accuracy_score(y_test, etc_pred)))\nprint('\\n')\nprint(\"Classification Report : \")\nprint(classification_report(y_test,etc_pred))","f532822b":"etc.feature_importances_","520bf0e0":"import seaborn as sns\n\n# Helper function for plotting feature importance\ndef plot_features(columns, importances, n=10):\n    df = (pd.DataFrame({\"features\": columns,\n                        \"feature_importance\": importances})\n          .sort_values(\"feature_importance\", ascending=False)\n          .reset_index(drop=True))\n    \n    sns.barplot(x=\"feature_importance\",\n                y=\"features\",\n                data=df[:n],\n                orient=\"h\")\nplot_features(X_train.columns, etc.feature_importances_)","7baee44f":"new_data = df[['meaneduc','SQBmeaned','hogar_nin','SQBhogar_nin','cielorazo',\n               'qmobilephone','idhogar','overcrowding','r4t1','SQBdependency']]","9d741eac":"new_data","b8edab8a":"X_train, X_test, y_train, y_test = train_test_split(new_data,y, test_size=0.2, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","d50ad5be":"etc = ExtraTreesClassifier().fit(X_train, y_train)\netc_pred = etc.predict(X_test)\nprint('R2 score is : {:.2f}'.format(accuracy_score(y_test, etc_pred)))\nprint('\\n')\nprint(\"Classification Report : \")\nprint(classification_report(y_test,etc_pred))","27d4cfff":"test = pd.read_csv('..\/input\/costa-rican-household-poverty-prediction\/test.csv')","3514eb31":"test_df = test.copy()","d57332c5":"test_df.info()","ba033090":"for label, content in test_df.items():\n    if pd.api.types.is_string_dtype(content):\n        print(label)","6233c9a6":"preprocess_data(test_df)","812758f5":"pred = rf.predict(test_df)\npred = pd.DataFrame(pred)\npred.to_csv('submission.csv')","42348d54":"pred","801ace51":"Id = test['Id']\nId = pd.DataFrame(Id)","95187eab":"Id","d2aef5ff":"subs = pd.concat([id, pred], ignore_index=True, axis=1)","62005b20":"subs","1cc9162b":"subs.rename(columns={'0':'ID','1':'Target'}, inplace=True)","01c8dc15":"subs.columns = ['Id','Target']","fd7b5707":"subs.drop","64c82781":"s = subs.copy()","66247445":"s.reset_index(drop=True, inplace=True)","2ee38d94":"s.set_index('Id', inplace=True)","d8b17c04":"subs = s","f572bc84":"subs","78d02867":"subs.to_csv('submission.csv')","90c3603f":"One of the main characteristic a data scientist must have is to write a clean, readable block of code.\n\nWe can do that by defining functions, like this:","80b2fcc5":"Looking at categorical features.","9723c17e":"Ok, we have 5 categorical features, let's set `Id` as index and drop it.\n\nBetter yet, since we've already done this while training our model using `preprocess_data`, let's use the same function here.","3b80ca8e":"## Feature Engineering\n\nLet's split our data into X & Y, so that we can later split it into train and validation sets.","f02ba85c":"Nice, still works like a charm.","66e86086":"Hmmm, this looks good. The model seems to be learning well as you can look at the f1-scores for each classes.\n\nLet's take a look at other algorithms before jumping to conclusions.","24fc1c2f":"It's similar to our training data.","ab61309a":"## Loading the data","549d7408":"Looking at the information of test data...","11e0401a":"WOWW ! There's an improvement. I mean just look at the f1-scores.\n\nI've tried with different models but, this seems to work really good. So I'll keep this model.","eb146eb1":"Again, making a copy of test set.","e8889004":"## ExtraTreesClassifier","220df45d":"## Random Forest","cda16207":"Let's take a look at different dtypes in the dataframe.","354c2a88":"After trying with different ML algorithms, I feel that these two work like a charm on this dataset.","90298b20":"Wait a minute, our work isn't done yet.\n\nA good data scientist should be able to build a model, which can produce amazing results even with lesser data.\nAs we can see `ExtraTreesClassifier` did really good let's see the what are top 10 important features, and see if we can achieve the same results as compared to using all the features.","703263e0":"I know this is not readable, so let's visualize it.","86adc4a1":"Hmmm, top 10 features according to our model are : \n                                                    \n`'meaneduc',\n'SQBmeaned',\n'hogar_nin',\n'SQBhogar_nin',\n'cielorazo',\n'qmobilephone',\n'idhogar',\n'overcrowding',\n'r4t1',\n'SQBdependency'`.\n                                                    \nSo let's just use these 10 features and see if the model still works good.","9d689bd0":"Let's look at the distribution of our target variable.","5fe74e2d":"Beautiful, everything looks good.\n\nNow let's predict it on our test set.","e883be8e":"Hmmm, looks like there are 5 objects, 130 integers, and 8 floats.\n\nLet's take a look at our categorical features.","fa95e7ce":"Ok, look's like our target variable is unbalanced.","7ed14c0a":"Now It's time to predict on the test data.","cfd75f7b":"Now this is a very important step, always make a copy of your original dataframe, so that if something goes wrong, we still have a back-up data.","1f88bf2b":"This looks clean enough, now there are different approaches to any given problem, always try with different approaches before finalizing an approach.","dbc8cce0":"There are 5 categorical features, including `Id`, which we will set as index later."}}