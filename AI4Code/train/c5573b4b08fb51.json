{"cell_type":{"10f1b217":"code","7cccb64f":"code","62566cee":"code","5481029a":"code","7e365156":"code","acdf2c24":"code","42a361c6":"code","a90b9f95":"code","179fd61a":"code","0e0ed605":"code","6f577b85":"code","82557b07":"code","c52022f1":"code","68d6764d":"code","4a17b4b3":"code","e5dd81db":"code","99402ea3":"code","a537460c":"code","3d3653ae":"code","040e6364":"code","1033e0b6":"code","8f2b452a":"code","6af17531":"code","49b34210":"code","b808e482":"code","b2e3f575":"code","cbc616e7":"code","cf1f743f":"code","9c35f97d":"code","ef542fb1":"code","4a2a4621":"code","7a032786":"code","510e0e89":"code","726af27c":"code","0d6579a0":"code","83c71ee8":"code","e00b71fc":"code","7095bdaa":"code","17e6a915":"code","fb96c402":"code","ff842603":"code","adcf94ca":"code","da464e85":"code","2ef31eb0":"code","cdffa9fb":"code","35718c62":"code","212b1289":"code","4814e3cd":"code","95892ecd":"code","22adb224":"code","89a372ae":"code","0be6af3c":"code","a1968128":"code","b52cadda":"code","5389bb01":"code","abdaf2a7":"code","bc824dc7":"code","ff8ffb2e":"code","8a658428":"code","34ba8e70":"code","f0f8c198":"code","c0b682f2":"code","3c085d42":"code","b12a72ce":"code","045405fb":"code","89593c94":"code","cc36c0fe":"code","b8f6865f":"code","6688ba65":"code","dec68fce":"markdown","6757eada":"markdown","650812a2":"markdown","48042a14":"markdown","1d2bd97a":"markdown","4b00b51b":"markdown","8775a9d2":"markdown","9afb3255":"markdown","2aed9d3c":"markdown","ad8b534c":"markdown","a987fe2a":"markdown","43e76165":"markdown","c4193dc1":"markdown"},"source":{"10f1b217":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7cccb64f":"import plotly.offline as pyoff\nimport plotly.graph_objs as go\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import classification_report","62566cee":"import matplotlib.pyplot as plt\nimport seaborn as sns","5481029a":"true = pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv')\nfake = pd.read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv')","7e365156":"true.head()","acdf2c24":"fake.head()","42a361c6":"true.info()","a90b9f95":"fake.info()","179fd61a":"true['Target'] = 0\nfake['Target'] = 1","0e0ed605":"df = pd.concat([true, fake])","6f577b85":"patternDel = \"http\"\nfilter1 = df['date'].str.contains(patternDel)\n\ndf = df[~filter1]","82557b07":"pattern = \"Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec\"\nfilter2 = df['date'].str.contains(pattern)\n\ndf = df[filter2]","c52022f1":"df['date'] = pd.to_datetime(df['date'])","68d6764d":"df.head()","4a17b4b3":"plt.figure(figsize=(10,10))\nsns.countplot(df['subject'], hue='Target', data=df)","e5dd81db":"copy = df.copy()","99402ea3":"copy = copy.sort_values(by = ['date'])","a537460c":"copy = copy.reset_index(drop=True)","3d3653ae":"copy","040e6364":"copy1 = copy[copy['Target'] == 1]\ncopy1 = copy1.groupby(['date'])['Target'].count()","1033e0b6":"copy1 = pd.DataFrame(copy1)\ncopy1.head()","8f2b452a":"copy0 = copy[copy['Target'] == 0]\ncopy0 = copy0.groupby(['date'])['Target'].count()","6af17531":"copy0 = pd.DataFrame(copy0)\ncopy0.head()","49b34210":"plot_data = [\n    go.Scatter(\n        x=copy0.index,\n        y=copy0['Target'],\n        name='True',\n        #x_axis=\"OTI\",\n        #y_axis=\"time\",\n    ),\n    go.Scatter(\n        x=copy1.index,\n        y=copy1['Target'],\n        name='Fake'\n    )\n    \n]\nplot_layout = go.Layout(\n        title='Day-wise',\n        yaxis_title='Count',\n        xaxis_title='Time',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)\n","b808e482":"from wordcloud import WordCloud,STOPWORDS","b2e3f575":"plt.figure(figsize = (20,20))\nwc = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate((\" \".join(df_[df_.target == 1].news)))\nplt.imshow(wc,interpolation = 'bilinear')","cbc616e7":"plt.figure(figsize = (20,20))\nwc = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate((\" \".join(df_[df_.target == 0].news)))\nplt.imshow(wc,interpolation = 'bilinear')","cf1f743f":"from nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string","9c35f97d":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation","ef542fb1":"df_['news'] = df['text'] + df['title'] + df['subject']\ndf_['target'] = df['Target']","4a2a4621":"\ndf_.head()","7a032786":"def cleaner(phrase):\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can't\", 'can not', phrase)\n  \n  # general\n    phrase = re.sub(r\"n\\'t\",\" not\", phrase)\n    phrase = re.sub(r\"\\'re'\",\" are\", phrase)\n    phrase = re.sub(r\"\\'s\",\" is\", phrase)\n    phrase = re.sub(r\"\\'ll\",\" will\", phrase)\n    phrase = re.sub(r\"\\'d\",\" would\", phrase)\n    phrase = re.sub(r\"\\'t\",\" not\", phrase)\n    phrase = re.sub(r\"\\'ve\",\" have\", phrase)\n    phrase = re.sub(r\"\\'m\",\" am\", phrase)\n    \n    return phrase","510e0e89":"from bs4 import BeautifulSoup\nfrom tqdm import tqdm\nimport re\n\nimport nltk\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords","726af27c":"stop = set(stopwords.words('english'))","0d6579a0":"cleaned_text = []\n\nfor sentance in tqdm(df_['news'].values):\n    sentance = str(sentance)\n    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    sentance = cleaner(sentance)\n    sentance = re.sub(r'[?|!|\\'|\"|#|+]', r'', sentance)\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stop)\n    cleaned_text.append(sentance.strip())","83c71ee8":"df_['news'] = cleaned_text","e00b71fc":"df_.head()","7095bdaa":"import gensim\nimport nltk","17e6a915":"X = []\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\nfor par in df_['news'].values:\n    tmp = []\n    sentences = nltk.sent_tokenize(par)\n    for sent in sentences:\n        sent = sent.lower()\n        tokens = tokenizer.tokenize(sent)\n        filtered_words = [w.strip() for w in tokens if w not in stop and len(w) > 1]\n        tmp.extend(filtered_words)\n    X.append(tmp)","fb96c402":"w2v_model = gensim.models.Word2Vec(sentences=X, size=150, window=5, min_count=2)","ff842603":"w2v_model.wv.most_similar(positive = 'trump')","adcf94ca":"w2v_model.wv.most_similar('hillary')","da464e85":"w2v_model.wv.similarity('donaldtrump', 'hillaryclinton')","2ef31eb0":"w2v_model.wv.similarity('donaldtrump', 'mikepence')","cdffa9fb":"w2v_model.wv.doesnt_match(['trump', 'hillary', 'sanders'])","35718c62":"w2v_model.wv.doesnt_match(['donaldtrump', 'hillary', 'mikepence'])","212b1289":"w2v_model.wv.most_similar(positive = ['woman','trump'], negative=['hillary'], topn=3)","4814e3cd":"w2v_model.wv.most_similar(positive = ['hillary','trump'], negative=['mikepence'], topn=3)","95892ecd":"w2v_model.wv.most_similar(positive = ['hillary','trump'], negative=['america'], topn=3)","22adb224":"X = df_['news']\ny = df_['target']","89a372ae":"from sklearn.model_selection import train_test_split\nX_Train, X_test, y_Train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)","0be6af3c":"X_train, X_cross, y_train, y_cross = train_test_split(X_Train, y_Train, test_size=0.1, random_state=42)","a1968128":"tf_idf=TfidfVectorizer(min_df=5,use_idf=True,ngram_range=(1,2))\ntf_idf.fit(X_train)\nTrain_TFIDF = tf_idf.transform(X_train)\nCrossVal_TFIDF = tf_idf.transform(X_cross)\nTest_TFIDF= tf_idf.transform(X_test)","b52cadda":"alpha_set=[0.0001,0.001,0.01,0.1,1,10,100,1000]\n\nTrain_AUC_TFIDF = []\nCrossVal_AUC_TFIDF = []\n\n\nfor i in alpha_set:\n    naive_b=MultinomialNB(alpha=i)\n    naive_b.fit(Train_TFIDF, y_train)\n    Train_y_pred =  naive_b.predict(Train_TFIDF)\n    Train_AUC_TFIDF.append(roc_auc_score(y_train,Train_y_pred))\n    CrossVal_y_pred =  naive_b.predict(CrossVal_TFIDF)\n    CrossVal_AUC_TFIDF.append(roc_auc_score(y_cross,CrossVal_y_pred))","5389bb01":"Alpha_set=[]\nfor i in range(len(alpha_set)):\n    Alpha_set.append(np.math.log(alpha_set[i]))","abdaf2a7":"plt.plot(Alpha_set, Train_AUC_TFIDF, label='Train AUC')\nplt.scatter(Alpha_set, Train_AUC_TFIDF)\nplt.plot(Alpha_set, CrossVal_AUC_TFIDF, label='CrossVal AUC')\nplt.scatter(Alpha_set, CrossVal_AUC_TFIDF)\nplt.legend()\nplt.xlabel(\"alpha : hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.show()","bc824dc7":"optimal_alpha=alpha_set[CrossVal_AUC_TFIDF.index(max(CrossVal_AUC_TFIDF))]\nprint(optimal_alpha)","ff8ffb2e":"Classifier2 = MultinomialNB(alpha=optimal_alpha)\nClassifier2.fit(Train_TFIDF, y_train)","8a658428":"print (\"Accuracy on Train is: \", accuracy_score(y_train,Classifier2.predict(Train_TFIDF)))\n\nprint (\"Accuracy on Test is: \", accuracy_score(y_test,Classifier2.predict(Test_TFIDF)))","34ba8e70":"from sklearn import metrics\nprint(metrics.classification_report(y_test,Classifier2.predict(Test_TFIDF)))","f0f8c198":"plot_confusion_matrix(Classifier2, Test_TFIDF, y_test ,display_labels=['0','1'],cmap=\"Blues\",values_format = '')","c0b682f2":"c=[0.0001,0.001,0.01,0.1,1,10,100,1000]\nTrain_AUC_TFIDF = []\nCrossVal_AUC_TFIDF = []\nfor i in c:\n    logreg = LogisticRegression(C=i,penalty='l2')\n    logreg.fit(Train_TFIDF, y_train)\n    Train_y_pred =  logreg.predict(Train_TFIDF)\n    Train_AUC_TFIDF.append(roc_auc_score(y_train ,Train_y_pred))\n    CrossVal_y_pred =  logreg.predict(CrossVal_TFIDF)\n    CrossVal_AUC_TFIDF.append(roc_auc_score(y_cross,CrossVal_y_pred))","3c085d42":"C=[]\nfor i in range(len(c)):\n    C.append(np.math.log(c[i]))","b12a72ce":"plt.plot(C, Train_AUC_TFIDF, label='Train AUC')\nplt.scatter(C, Train_AUC_TFIDF)\nplt.plot(C, CrossVal_AUC_TFIDF, label='CrossVal AUC')\nplt.scatter(C, CrossVal_AUC_TFIDF)\nplt.legend()\nplt.xlabel(\"lambda : hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.show()","045405fb":"optimal_inverse_lambda=c[CrossVal_AUC_TFIDF.index(max(CrossVal_AUC_TFIDF))]\nprint(pow(optimal_inverse_lambda,-1))","89593c94":"Classifier=LogisticRegression(C=optimal_inverse_lambda,penalty='l2')\nClassifier.fit(Train_TFIDF, y_train)","cc36c0fe":"print (\"Accuracy is: \", accuracy_score(y_test,Classifier.predict(Test_TFIDF)))","b8f6865f":"from sklearn import metrics\nprint(metrics.classification_report(y_test,Classifier.predict(Test_TFIDF)))","6688ba65":"plot_confusion_matrix(Classifier, Test_TFIDF, y_test ,display_labels=['0','1'],cmap=\"Blues\")","dec68fce":"***Basket of Deplorables***","6757eada":"1. **Logistic Regression using L2 Penalty turns out to be the winner with an accuracy of more than 99.6%**\n2. **Multinomial Naive Bayes also did a commendable job with the classification**","650812a2":"We had already seen that the presence of 'Subject' was highly inconsistent, the subjects were very target specific which might the reason for overfitting. \n**I would prefer the Naive Bayes model without the 'Subject' feature as a perfect model.**","48042a14":"**Logistic Regression**","1d2bd97a":"**Highly imbalanced representation which might create problems later**","4b00b51b":"Multinomial naive Bayes","8775a9d2":"**When I first tried, I didn't include the 'subject' column and the model performed similar for MNB model, but the Logistic Regression Model could only achieve an accuracy of 90%. I guess the model suffers from overfitting due to the 'subject' feature**","9afb3255":"Modelling","2aed9d3c":"**Perfecto**","ad8b534c":"We can safely say that data is highly representative of the real scenario. \n1. Emergence of fake news before US 2016 Elections\n2. Huge amount of tweets on 9\/Nov 2016 - Trump's Victory\n3. 7th April 2017 - US missile attack on Syria","a987fe2a":"Credits - **https:\/\/www.kaggle.com\/sreshta140\/is-it-authentic-or-not\/notebook**","43e76165":"Ohh My Gawd **'Hillary and her friends at FOX'**","c4193dc1":"Please upvote and comment"}}