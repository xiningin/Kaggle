{"cell_type":{"eb3057bd":"code","d4197210":"code","89d01b0e":"code","7e81626b":"code","6c0ab76b":"code","c792c582":"code","7e0fd54f":"code","5b4f8129":"code","badfa9d9":"code","207bffc7":"code","3c4103ec":"code","9fc4dfaa":"code","9b4a0bf1":"code","bd9fad96":"code","02ae65ac":"code","da2b3ea6":"code","a930a8cd":"code","88e5c088":"code","b19f725e":"code","ccbba730":"code","73ba4bb4":"code","04eebca6":"code","baee216c":"code","5f6ad470":"code","31702b91":"code","f0676aaa":"code","961d64b8":"code","11eec6bd":"code","23379396":"code","5c9a4fb4":"code","dbc80e61":"code","de9e3a6c":"code","7a1eb73b":"code","cfd86256":"code","8001b912":"code","144c4a1e":"code","ebb382bc":"code","ffe1a423":"code","328b5808":"code","152312bc":"code","d12ecea6":"code","0b90421c":"code","3837793c":"code","42d249a8":"code","4c1cfe1c":"code","5ec65eac":"markdown","46c5a4af":"markdown","2a5cf66e":"markdown","ed686bcb":"markdown","5a155174":"markdown","75ca3345":"markdown","843c6a5a":"markdown","f52c4e89":"markdown","d58fc4db":"markdown","419d026b":"markdown","0d69377e":"markdown","52b8836b":"markdown","599f4b70":"markdown","fb50a1f4":"markdown","ae02e493":"markdown","b9bd50fc":"markdown","05c683cd":"markdown","00805a05":"markdown","50cc1d89":"markdown","b9d55534":"markdown","339a17c1":"markdown","84dc8a65":"markdown","cefbb979":"markdown","b1179035":"markdown","994250fc":"markdown"},"source":{"eb3057bd":"import sys\nimport os\n!git clone https:\/\/www.github.com\/matterport\/Mask_RCNN.git ..\/Mask_RCNN\nos.chdir('..\/Mask_RCNN')","d4197210":"!pip install -r requirements.txt\n!pip install tensorflow-gpu==1.15\n!pip install keras==2.2.5","89d01b0e":"!conda install -c anaconda --yes cudatoolkit\n!conda install -c anaconda --yes cudnn","7e81626b":"ROOT_PATH='\/kaggle\/working'\nMASK_PATH='\/kaggle\/working\/Mask_RCNN'\nMODEL_PATH='\/kaggle\/working\/Mask_RCNN\/logs'\nTRAIN_PATH='\/kaggle\/input\/kermany2018\/oct2017\/OCT2017 \/train\/DRUSEN'\nTEST_PATH='\/kaggle\/input\/kermany2018\/oct2017\/OCT2017 \/test\/DRUSEN'\nVAL_PATH='\/kaggle\/input\/kermany2018\/oct2017\/OCT2017 \/val\/DRUSEN'\nJSON_PATH='\/kaggle\/input\/labels'","6c0ab76b":"import json\nimport datetime\nimport numpy as np\nimport skimage.draw\nfrom imgaug import augmenters as iaa\nimport pandas as pd\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Root directory of the project\nROOT_DIR = os.path.abspath(ROOT_PATH)\n\n# Import Mask RCNN\nsys.path.append(ROOT_DIR)  # To find local version of the library\nfrom mrcnn.config import Config\nfrom mrcnn import model as modellib, utils\n\n# Path to trained weights file\nCOCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, \"mask_rcnn.h5\")\n\n# Directory to save logs and model checkpoints, if not provided\n# through the command line argument --logs\nDEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")","c792c582":"class OCTConfig(Config):\n   \n    NAME = \"OCT\"\n\n    # We use a GPU with 12GB memory, which can fit two images.\n    # Adjust down if you use a smaller GPU.\n    IMAGES_PER_GPU = 2\n\n    # Number of classes (including background)\n    # TODO CHANGE\n    NUM_CLASSES = 1 + 3  # Background + ObereSchicht + Membran + UntereSchicht\n\n    # Number of training steps per epoch\n    STEPS_PER_EPOCH = 125\n\n    DETECTION_MIN_CONFIDENCE = 0.75\n\n    TRAIN_ROIS_PER_IMAGE = 200","7e0fd54f":"class OCTDataset(utils.Dataset):\n\n    def load_oct(self, subset):\n\n        self.add_class(\"oct\", 1, \"pigment\")  # adjusted here\n        self.add_class(\"oct\", 2, \"soft\")  # adjusted here\n        self.add_class(\"oct\", 3, \"hard\")  # adjusted here\n\n        dataset_dir = \"\"\n        # Train or validation dataset?\n        if subset == \"train\":\n            dataset_dir=TRAIN_PATH\n        if subset == \"val\":\n            dataset_dir=VAL_PATH\n             \n        annotations = json.load(open(os.path.join(JSON_PATH, \"via_export_json-\"+subset+\".json\")))\n        annotations = list(annotations.values())  # don't need the dict keys\n        annotations = [a for a in annotations if a['regions']]\n\n        numberImages = 0\n\n        # Add images\n        for a in annotations:\n\n            if type(a['regions']) is dict:\n                polygons = [r['shape_attributes'] for r in a['regions'].values()]\n            else:\n                polygons = [r['shape_attributes'] for r in a['regions']]\n\n            class_names_str = [r['region_attributes']['druse'] for r in a['regions']]\n            class_name_nums = []\n\n            for i in class_names_str:\n                if i == 'pigment':\n                    class_name_nums.append(1)\n                if i == 'soft':\n                    class_name_nums.append(2)\n                if i == 'hard':\n                    class_name_nums.append(3)\n\n            image_path = os.path.join(dataset_dir, a['filename'])\n            \n            # Thanks to MAX\n            image_path = image_path.replace(\"val\",\"train\")\n            \n            image = skimage.io.imread(image_path)\n            height, width = image.shape[:2]\n            numberImages = numberImages+1\n\n            self.add_image(\n                \"oct\",\n                image_id=a['filename'],  # use file name as a unique image id\n                path=image_path,\n                width=width, height=height,\n                polygons=polygons,\n                class_list=np.array(\n                    class_name_nums))  # UNSURE IF  I CAN JUST ADD THIS  HERE. OTHERWISE NEED  TO MODIFY DATASET UTIL\n\n            self.dataset_size = numberImages\n\n    def load_mask(self, image_id):\n        # If not a balloon dataset image, delegate to parent class.\n        image_info = self.image_info[image_id]\n        if image_info[\"source\"] != \"oct\":  # adjusted here\n            return super(self.__class__, self).load_mask(image_id)\n\n        # Convert polygons to a bitmap mask of shape\n        # [height, width, instance_count]\n        info = self.image_info[image_id]\n        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n                        dtype=np.uint8)\n        for i, p in enumerate(info[\"polygons\"]):\n            # Get indexes of pixels inside the polygon and set them to 1\n            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n            mask[rr, cc, i] = 1\n\n        class_array = info['class_list']\n        return mask.astype(np.bool), class_array\n\n    def image_reference(self, image_id):\n        \"\"\"Return the path of the image.\"\"\"\n        info = self.image_info[image_id]\n        if info[\"source\"] == \"oct\":  # adjusted here\n            return info[\"path\"]\n        else:\n            super(self.__class__, self).image_reference(image_id)","5b4f8129":"weights = \"coco\"\nlogs=\"logs\"\ncommand = \"train\"\n\n# Configurations\nif command == \"train\":\n    config = OCTConfig()\nconfig.display()\n\n# Create model\nif command == \"train\":\n    model = modellib.MaskRCNN(mode=\"training\", config=config,\n                              model_dir=logs)\n\nmodel.keras_model.summary()\n\n# Select weights file to load\nif weights.lower() == \"coco\":\n    weights_path = COCO_WEIGHTS_PATH\n    # Download weights file\n    if not os.path.exists(weights_path):\n        utils.download_trained_weights(weights_path)\nelif weights.lower() == \"last\":\n    # Find last trained weights\n    weights_path = model.find_last()\nelse:\n    weights_path = weights\n\n# Load weights\nprint(\"Loading weights \", weights_path)\nif weights.lower() == \"coco\":\n    # Exclude the last layers because they require a matching\n    # number of classes\n    model.load_weights(weights_path, by_name=True, exclude=[\n        \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n        \"mrcnn_bbox\", \"mrcnn_mask\"])\nelse:\n    model.load_weights(weights_path, by_name=True)\n","badfa9d9":"augmentation = iaa.SomeOf((0, 2), [\n        iaa.Flipud(0.5),\n        iaa.Fliplr(0.5),  # horizontal flips\n        iaa.Crop(percent=(0, 0.1)),  # random crops\n\n        # Make some images brighter and some darker.\n        # In 20% of all cases, we sample the multiplier once per channel,\n        # which can end up changing the color of the images.\n        iaa.Multiply((0.8, 1.2), per_channel=0.2),\n\n        # Apply affine transformations to each image.\n        # Scale\/zoom them, translate\/move them, rotate them and shear them.\n        iaa.Affine(\n            scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n            translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n            rotate=(-25, 25),\n            shear=(-8, 8)\n        )\n    ], random_order=True)\n\nimg=mpimg.imread(\"\/kaggle\/input\/kermany2018\/oct2017\/OCT2017 \/train\/DRUSEN\/DRUSEN-9201573-14.jpeg\")\nimggrid = augmentation.draw_grid(img, cols=5, rows=2)\nplt.figure(figsize=(30, 12))\n_ = plt.imshow(imggrid.astype(int))","207bffc7":"# Training dataset.\nprint('preparing training set')\ndataset_train = OCTDataset()\ndataset_train.load_oct(\"train\")\ndataset_train.prepare()\n\n# Validation dataset\nprint('preparing val set')\ndataset_val = OCTDataset()\ndataset_val.load_oct(\"val\")\ndataset_val.prepare()","3c4103ec":"# starting_epoch = model.epoch\nepoch = int(dataset_train.dataset_size \/ (config.STEPS_PER_EPOCH * config.BATCH_SIZE) * 100)\nepochs_warmup = epoch\nepochs_heads = 3 * epoch\nepochs_stage4 = 3 * epoch\nepochs_all =  epoch\nepochs_breakOfDawn = 3 * epoch\n\nprint(\"> Training Schedule: \\\n    \\nwarmup: {} epochs \\\n    \\nheads: {} epochs \\\n    \\nstage4+: {} epochs \\\n    \\nall layers: {} epochs \\\n    \\ntill the break of Dawn: {} epochs\".format(\n    epochs_warmup,epochs_heads,epochs_stage4,epochs_all,epochs_breakOfDawn))","9fc4dfaa":"%%time\n## Training - WarmUp Stage\nprint(\"> Warm Up all layers\")\nmodel.train(dataset_train, dataset_val,\n            learning_rate=config.LEARNING_RATE \/ 10,\n            epochs=epochs_warmup,\n            layers='all',\n            augmentation=augmentation)\n\nhistory = model.keras_model.history.history","9b4a0bf1":"%%time\n# Training - Stage 1\nprint(\"> Training network heads\")\nmodel.train(dataset_train, dataset_val,\n            learning_rate=config.LEARNING_RATE,\n            epochs= epochs_warmup,\n            layers='heads',\n            augmentation=augmentation)\n\nnew_history = model.keras_model.history.history\nfor k in new_history: history[k] = history[k] + new_history[k]","bd9fad96":"%%time\n# Training - Stage 2\n# Finetune layers  stage 4 and up\nprint(\"> Fine tune {} stage 4 and up\".format(config.BACKBONE))\nmodel.train(dataset_train, dataset_val,\n            learning_rate=config.LEARNING_RATE,\n            epochs=epochs_stage4,\n            layers=\"4+\",\n            augmentation=augmentation)\n\nnew_history = model.keras_model.history.history\nfor k in new_history: history[k] = history[k] + new_history[k]","02ae65ac":"%%time\n# Training - Stage 3\n# Fine tune all layers\nprint(\"> Fine tune all layers\")\nmodel.train(dataset_train, dataset_val,\n            learning_rate=config.LEARNING_RATE \/ 10,\n            epochs=epochs_all,\n            layers='all',\n            augmentation=augmentation)\n\nnew_history = model.keras_model.history.history\nfor k in new_history: history[k] = history[k] + new_history[k]","da2b3ea6":"epochs = range(1, len(history['loss'])+1)\npd.DataFrame(history, index=epochs)","a930a8cd":"plt.figure(figsize=(21,11))\n\nplt.subplot(231)\nplt.plot(epochs, history[\"loss\"], label=\"Train loss\")\nplt.plot(epochs, history[\"val_loss\"], label=\"Valid loss\")\nplt.legend()\nplt.subplot(232)\nplt.plot(epochs, history[\"rpn_class_loss\"], label=\"Train RPN class ce\")\nplt.plot(epochs, history[\"val_rpn_class_loss\"], label=\"Valid RPN class ce\")\nplt.legend()\nplt.subplot(233)\nplt.plot(epochs, history[\"rpn_bbox_loss\"], label=\"Train RPN box loss\")\nplt.plot(epochs, history[\"val_rpn_bbox_loss\"], label=\"Valid RPN box loss\")\nplt.legend()\nplt.subplot(234)\nplt.plot(epochs, history[\"mrcnn_class_loss\"], label=\"Train MRCNN class ce\")\nplt.plot(epochs, history[\"val_mrcnn_class_loss\"], label=\"Valid MRCNN class ce\")\nplt.legend()\nplt.subplot(235)\nplt.plot(epochs, history[\"mrcnn_bbox_loss\"], label=\"Train MRCNN box loss\")\nplt.plot(epochs, history[\"val_mrcnn_bbox_loss\"], label=\"Valid MRCNN box loss\")\nplt.legend()\nplt.subplot(236)\nplt.plot(epochs, history[\"mrcnn_mask_loss\"], label=\"Train Mask loss\")\nplt.plot(epochs, history[\"val_mrcnn_mask_loss\"], label=\"Valid Mask loss\")\nplt.legend()\n\nplt.show()\n\nbest_epoch = np.argmin(history[\"val_loss\"])\nscore = history[\"val_loss\"][best_epoch]\nprint(f'Best Epoch:{best_epoch+1} val_loss:{score}')","88e5c088":"import os\nimport sys\nimport random\nimport math\nimport re\nimport time\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom glob import glob\n\n# Import Mask RCNN\nsys.path.append(ROOT_DIR)  # To find local version of the library\nfrom mrcnn import utils\nfrom mrcnn import visualize\nfrom mrcnn.visualize import display_images\nimport mrcnn.model as modellib\nfrom mrcnn.model import log\n\n%matplotlib inline \n\n# Directory to save logs and trained model\nMODEL_DIR = MODEL_PATH","b19f725e":"config = OCTConfig()\nOCT_DIR = os.path.join(ROOT_DIR, \"\")\nclass InferenceConfig(config.__class__):\n    # Run detection on one image at a time\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n\nconfig = InferenceConfig()\nconfig.DETECTION_MIN_CONFIDENCE = 0.75\nconfig.display()\n\nDEVICE = \"\/cpu:0\"  # \/cpu:0 or \/gpu:0\nTEST_MODE = \"inference\"\n\ndef get_ax(rows=1, cols=1, size=16):\n    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n    return ax","ccbba730":"# Load validation dataset\ndataset = OCTDataset()\ndataset.load_oct(\"val\")\n\n# Must call before using the dataset\ndataset.prepare()\n\nprint(\"Images: {}\\nClasses: {}\".format(len(dataset.image_ids), dataset.class_names))","73ba4bb4":"with tf.device(DEVICE):\n    model = modellib.MaskRCNN(mode=\"inference\", model_dir='\/kaggle\/Mask_RCNN\/logs\/',\n                              config=config)\n\nweights_path = model.find_last()\n\n# Load weights\nprint(\"Loading weights \", weights_path)\nmodel.load_weights(weights_path, by_name=True)","04eebca6":"#Display results\nnumberPictures=min(len(dataset.image_ids),5)\nax = get_ax(rows=numberPictures,cols=2)\n\nfor i in range(0,numberPictures):\n    image_id = random.choice(dataset.image_ids)\n    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n        modellib.load_image_gt(dataset, config, image_id, use_mini_mask=False)\n    info = dataset.image_info[image_id]\n    print(\"image ID: {}.{} ({}) {}\".format(info[\"source\"], info[\"id\"], image_id, \n                                           dataset.image_reference(image_id)))\n\n    # Run object detection\n    results = model.detect([image], verbose=1)\n\n    r = results[0]\n\n    visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n                                dataset.class_names, r['scores'], ax=ax[i,0],\n                                title=\"Predictions \")\n    \n    image2=mpimg.imread(dataset.image_reference(image_id))\n    ax[i,1].set_title(\"Blank\")\n    ax[i,1].imshow(image2)\n        \n    log(\"gt_class_id\", gt_class_id)\n    log(\"gt_bbox\", gt_bbox)\n    log(\"gt_mask\", gt_mask)","baee216c":"# Generate RPN trainig targets\n# target_rpn_match is 1 for positive anchors, -1 for negative anchors\n# and 0 for neutral anchors.\ntarget_rpn_match, target_rpn_bbox = modellib.build_rpn_targets(\n    image.shape, model.anchors, gt_class_id, gt_bbox, model.config)\nlog(\"target_rpn_match\", target_rpn_match)\nlog(\"target_rpn_bbox\", target_rpn_bbox)\n\npositive_anchor_ix = np.where(target_rpn_match[:] == 1)[0]\nnegative_anchor_ix = np.where(target_rpn_match[:] == -1)[0]\nneutral_anchor_ix = np.where(target_rpn_match[:] == 0)[0]\npositive_anchors = model.anchors[positive_anchor_ix]\nnegative_anchors = model.anchors[negative_anchor_ix]\nneutral_anchors = model.anchors[neutral_anchor_ix]\nlog(\"positive_anchors\", positive_anchors)\nlog(\"negative_anchors\", negative_anchors)\nlog(\"neutral anchors\", neutral_anchors)\n\n# Apply refinement deltas to positive anchors\nrefined_anchors = utils.apply_box_deltas(\n    positive_anchors,\n    target_rpn_bbox[:positive_anchors.shape[0]] * model.config.RPN_BBOX_STD_DEV)\nlog(\"refined_anchors\", refined_anchors, )\n\nvisualize.draw_boxes(image, boxes=positive_anchors, refined_boxes=refined_anchors, ax=get_ax())\n","5f6ad470":"# Run RPN sub-graph\npillar = model.keras_model.get_layer(\"ROI\").output  # node to start searching from\n\n# TF 1.4 and 1.9 introduce new versions of NMS. Search for all names to support TF 1.3~1.10\nnms_node = model.ancestor(pillar, \"ROI\/rpn_non_max_suppression:0\")\nif nms_node is None:\n    nms_node = model.ancestor(pillar, \"ROI\/rpn_non_max_suppression\/NonMaxSuppressionV2:0\")\nif nms_node is None: #TF 1.9-1.10\n    nms_node = model.ancestor(pillar, \"ROI\/rpn_non_max_suppression\/NonMaxSuppressionV3:0\")\n\nrpn = model.run_graph([image], [\n    (\"rpn_class\", model.keras_model.get_layer(\"rpn_class\").output),\n    (\"pre_nms_anchors\", model.ancestor(pillar, \"ROI\/pre_nms_anchors:0\")),\n    (\"refined_anchors\", model.ancestor(pillar, \"ROI\/refined_anchors:0\")),\n    (\"refined_anchors_clipped\", model.ancestor(pillar, \"ROI\/refined_anchors_clipped:0\")),\n    (\"post_nms_anchor_ix\", nms_node),\n    (\"proposals\", model.keras_model.get_layer(\"ROI\").output),\n])\n\nlimit = 100\nsorted_anchor_ids = np.argsort(rpn['rpn_class'][:,:,1].flatten())[::-1]\nvisualize.draw_boxes(image, boxes=model.anchors[sorted_anchor_ids[:limit]], ax=get_ax())","31702b91":"# Show top anchors with refinement. Then with clipping to image boundaries\nlimit = 50\nax = get_ax(1, 2)\npre_nms_anchors = utils.denorm_boxes(rpn[\"pre_nms_anchors\"][0], image.shape[:2])\nrefined_anchors = utils.denorm_boxes(rpn[\"refined_anchors\"][0], image.shape[:2])\nrefined_anchors_clipped = utils.denorm_boxes(rpn[\"refined_anchors_clipped\"][0], image.shape[:2])\nvisualize.draw_boxes(image, boxes=pre_nms_anchors[:limit],\n                     refined_boxes=refined_anchors[:limit], ax=ax[0])\nvisualize.draw_boxes(image, refined_boxes=refined_anchors_clipped[:limit], ax=ax[1])","f0676aaa":"# Show refined anchors after non-max suppression\nlimit = 50\nixs = rpn[\"post_nms_anchor_ix\"][:limit]\nvisualize.draw_boxes(image, refined_boxes=refined_anchors_clipped[ixs], ax=get_ax())","961d64b8":"# Show final proposals\n# These are the same as the previous step (refined anchors \n# after NMS) but with coordinates normalized to [0, 1] range.\nlimit = 50\n# Convert back to image coordinates for display\nh, w = config.IMAGE_SHAPE[:2]\nproposals = rpn['proposals'][0, :limit] * np.array([h, w, h, w])\nvisualize.draw_boxes(image, refined_boxes=proposals, ax=get_ax())","11eec6bd":"# Get input and output to classifier and mask heads.\nmrcnn = model.run_graph([image], [\n    (\"proposals\", model.keras_model.get_layer(\"ROI\").output),\n    (\"probs\", model.keras_model.get_layer(\"mrcnn_class\").output),\n    (\"deltas\", model.keras_model.get_layer(\"mrcnn_bbox\").output),\n    (\"masks\", model.keras_model.get_layer(\"mrcnn_mask\").output),\n    (\"detections\", model.keras_model.get_layer(\"mrcnn_detection\").output),\n])\n","23379396":"# Get detection class IDs. Trim zero padding.\ndet_class_ids = mrcnn['detections'][0, :, 4].astype(np.int32)\ndet_count = np.where(det_class_ids == 0)[0][0]\ndet_class_ids = det_class_ids[:det_count]\ndetections = mrcnn['detections'][0, :det_count]\n\nprint(\"{} detections: {}\".format(\n    det_count, np.array(dataset.class_names)[det_class_ids]))\n\ncaptions = [\"{} {:.3f}\".format(dataset.class_names[int(c)], s) if c > 0 else \"\"\n            for c, s in zip(detections[:, 4], detections[:, 5])]\nvisualize.draw_boxes(\n    image, \n    refined_boxes=utils.denorm_boxes(detections[:, :4], image.shape[:2]),\n    visibilities=[2] * len(detections),\n    captions=captions, title=\"Detections\",\n    ax=get_ax())","5c9a4fb4":"# Get input and output to classifier and mask heads.\nmrcnn = model.run_graph([image], [\n    (\"proposals\", model.keras_model.get_layer(\"ROI\").output),\n    (\"probs\", model.keras_model.get_layer(\"mrcnn_class\").output),\n    (\"deltas\", model.keras_model.get_layer(\"mrcnn_bbox\").output),\n    (\"masks\", model.keras_model.get_layer(\"mrcnn_mask\").output),\n    (\"detections\", model.keras_model.get_layer(\"mrcnn_detection\").output),\n])\n","dbc80e61":"# Get detection class IDs. Trim zero padding.\ndet_class_ids = mrcnn['detections'][0, :, 4].astype(np.int32)\ndet_count = np.where(det_class_ids == 0)[0][0]\ndet_class_ids = det_class_ids[:det_count]\ndetections = mrcnn['detections'][0, :det_count]\n\nprint(\"{} detections: {}\".format(\n    det_count, np.array(dataset.class_names)[det_class_ids]))\n\ncaptions = [\"{} {:.3f}\".format(dataset.class_names[int(c)], s) if c > 0 else \"\"\n            for c, s in zip(detections[:, 4], detections[:, 5])]\nvisualize.draw_boxes(\n    image, \n    refined_boxes=utils.denorm_boxes(detections[:, :4], image.shape[:2]),\n    visibilities=[2] * len(detections),\n    captions=captions, title=\"Detections\",\n    ax=get_ax())\n","de9e3a6c":"# Proposals are in normalized coordinates. Scale them\n# to image coordinates.\nh, w = config.IMAGE_SHAPE[:2]\nproposals = np.around(mrcnn[\"proposals\"][0] * np.array([h, w, h, w])).astype(np.int32)\n\n# Class ID, score, and mask per proposal\nroi_class_ids = np.argmax(mrcnn[\"probs\"][0], axis=1)\nroi_scores = mrcnn[\"probs\"][0, np.arange(roi_class_ids.shape[0]), roi_class_ids]\nroi_class_names = np.array(dataset.class_names)[roi_class_ids]\nroi_positive_ixs = np.where(roi_class_ids > 0)[0]\n\n# How many ROIs vs empty rows?\nprint(\"{} Valid proposals out of {}\".format(np.sum(np.any(proposals, axis=1)), proposals.shape[0]))\nprint(\"{} Positive ROIs\".format(len(roi_positive_ixs)))\n\n# Class counts\nprint(list(zip(*np.unique(roi_class_names, return_counts=True))))","7a1eb73b":"# Display a random sample of proposals.\n# Proposals classified as background are dotted, and\n# the rest show their class and confidence score.\nlimit = 200\nixs = np.random.randint(0, proposals.shape[0], limit)\ncaptions = [\"{} {:.3f}\".format(dataset.class_names[c], s) if c > 0 else \"\"\n            for c, s in zip(roi_class_ids[ixs], roi_scores[ixs])]\nvisualize.draw_boxes(image, boxes=proposals[ixs],\n                     visibilities=np.where(roi_class_ids[ixs] > 0, 2, 1),\n                     captions=captions, title=\"ROIs Before Refinement\",\n                     ax=get_ax())","cfd86256":"# Class-specific bounding box shifts.\nroi_bbox_specific = mrcnn[\"deltas\"][0, np.arange(proposals.shape[0]), roi_class_ids]\nlog(\"roi_bbox_specific\", roi_bbox_specific)\n\n# Apply bounding box transformations\n# Shape: [N, (y1, x1, y2, x2)]\nrefined_proposals = utils.apply_box_deltas(\n    proposals, roi_bbox_specific * config.BBOX_STD_DEV).astype(np.int32)\nlog(\"refined_proposals\", refined_proposals)\n\n# Show positive proposals\n# ids = np.arange(roi_boxes.shape[0])  # Display all\nlimit = 5\nids = np.random.randint(0, len(roi_positive_ixs), limit)  # Display random sample\ncaptions = [\"{} {:.3f}\".format(dataset.class_names[c], s) if c > 0 else \"\"\n            for c, s in zip(roi_class_ids[roi_positive_ixs][ids], roi_scores[roi_positive_ixs][ids])]\nvisualize.draw_boxes(image, boxes=proposals[roi_positive_ixs][ids],\n                     refined_boxes=refined_proposals[roi_positive_ixs][ids],\n                     visibilities=np.where(roi_class_ids[roi_positive_ixs][ids] > 0, 1, 0),\n                     captions=captions, title=\"ROIs After Refinement\",\n                     ax=get_ax())","8001b912":"keep = np.where(roi_class_ids > 0)[0]\nprint(\"Keep {} detections:\\n{}\".format(keep.shape[0], keep))\n\n# Remove low confidence detections\nkeep = np.intersect1d(keep, np.where(roi_scores >= config.DETECTION_MIN_CONFIDENCE)[0])\nprint(\"Remove boxes below {} confidence. Keep {}:\\n{}\".format(\n    config.DETECTION_MIN_CONFIDENCE, keep.shape[0], keep))","144c4a1e":"# Apply per-class non-max suppression\npre_nms_boxes = refined_proposals[keep]\npre_nms_scores = roi_scores[keep]\npre_nms_class_ids = roi_class_ids[keep]\n\nnms_keep = []\nfor class_id in np.unique(pre_nms_class_ids):\n    # Pick detections of this class\n    ixs = np.where(pre_nms_class_ids == class_id)[0]\n    # Apply NMS\n    class_keep = utils.non_max_suppression(pre_nms_boxes[ixs], \n                                            pre_nms_scores[ixs],\n                                            config.DETECTION_NMS_THRESHOLD)\n    # Map indicies\n    class_keep = keep[ixs[class_keep]]\n    nms_keep = np.union1d(nms_keep, class_keep)\n    print(\"{:22}: {} -> {}\".format(dataset.class_names[class_id][:20], \n                                   keep[ixs], class_keep))\n\nkeep = np.intersect1d(keep, nms_keep).astype(np.int32)\nprint(\"\\nKept after per-class NMS: {}\\n{}\".format(keep.shape[0], keep))","ebb382bc":"# Show final detections\nixs = np.arange(len(keep))  # Display all\n# ixs = np.random.randint(0, len(keep), 10)  # Display random sample\ncaptions = [\"{} {:.3f}\".format(dataset.class_names[c], s) if c > 0 else \"\"\n            for c, s in zip(roi_class_ids[keep][ixs], roi_scores[keep][ixs])]\nvisualize.draw_boxes(\n    image, boxes=proposals[keep][ixs],\n    refined_boxes=refined_proposals[keep][ixs],\n    visibilities=np.where(roi_class_ids[keep][ixs] > 0, 1, 0),\n    captions=captions, title=\"Detections after NMS\",\n    ax=get_ax())","ffe1a423":"display_images(np.transpose(gt_mask, [2, 0, 1]), cmap=\"Blues\")","328b5808":"# Get predictions of mask head\nmrcnn = model.run_graph([image], [\n    (\"detections\", model.keras_model.get_layer(\"mrcnn_detection\").output),\n    (\"masks\", model.keras_model.get_layer(\"mrcnn_mask\").output),\n])\n\n# Get detection class IDs. Trim zero padding.\ndet_class_ids = mrcnn['detections'][0, :, 4].astype(np.int32)\ndet_count = np.where(det_class_ids == 0)[0][0]\ndet_class_ids = det_class_ids[:det_count]\n\nprint(\"{} detections: {}\".format(\n    det_count, np.array(dataset.class_names)[det_class_ids]))","152312bc":"# Masks\ndet_boxes = utils.denorm_boxes(mrcnn[\"detections\"][0, :, :4], image.shape[:2])\ndet_mask_specific = np.array([mrcnn[\"masks\"][0, i, :, :, c] \n                              for i, c in enumerate(det_class_ids)])\ndet_masks = np.array([utils.unmold_mask(m, det_boxes[i], image.shape)\n                      for i, m in enumerate(det_mask_specific)])\nlog(\"det_mask_specific\", det_mask_specific)\nlog(\"det_masks\", det_masks)","d12ecea6":"display_images(det_mask_specific[:4] * 255, cmap=\"Blues\", interpolation=\"none\")","0b90421c":"display_images(det_masks[:4] * 255, cmap=\"Blues\", interpolation=\"none\")","3837793c":"# Get activations of a few sample layers\nactivations = model.run_graph([image], [\n    (\"input_image\",        tf.identity(model.keras_model.get_layer(\"input_image\").output)),\n    (\"res2c_out\",          model.keras_model.get_layer(\"res2c_out\").output),\n    (\"res3c_out\",          model.keras_model.get_layer(\"res3c_out\").output),\n    (\"res4w_out\",          model.keras_model.get_layer(\"res4w_out\").output),  # for resnet100\n    (\"rpn_bbox\",           model.keras_model.get_layer(\"rpn_bbox\").output),\n    (\"roi\",                model.keras_model.get_layer(\"ROI\").output),\n])","42d249a8":"# Input image (normalized)\n_ = plt.imshow(modellib.unmold_image(activations[\"input_image\"][0],config))","4c1cfe1c":"# Backbone feature map\ndisplay_images(np.transpose(activations[\"res2c_out\"][0,:,:,:4], [2, 0, 1]), cols=4)","5ec65eac":"## Filter Low Confidence Detections","46c5a4af":"## RPN Targets","2a5cf66e":"## Visualize Activations\n","ed686bcb":"## Augmentation","5a155174":"## Apply Bounding Box Refinement","75ca3345":"## Load Model","843c6a5a":"## Pfade","f52c4e89":"## Run Detection","d58fc4db":"**Helpful-Links:**\n* https:\/\/github.com\/matterport\/Mask_RCNN","419d026b":"##  Proposal Classification","0d69377e":"# Identifikation von Drusenarten in OCT-Images\n### Unter der Verwendung des matterplot Mask-R-CNN Ansatzes","52b8836b":"## Configuration","599f4b70":"## Generating Masks","fb50a1f4":"## Per-Class Non-Max Suppression","ae02e493":"## Metrics","b9bd50fc":"#  Training","05c683cd":"#  Configurations","00805a05":"# Import","50cc1d89":"# Setup Umgebung","b9d55534":"## RPN Predictions","339a17c1":"## Step by Step Detection","84dc8a65":"## Load Validation Dataset\n","cefbb979":"## Load Model","b1179035":"# Dataset","994250fc":"# Result"}}