{"cell_type":{"76e53f94":"code","1e3de623":"code","41ac8f9b":"code","71e0cb80":"code","e3629dc8":"code","0af4e4fc":"code","798c5051":"code","ff61e88c":"code","37f33b53":"code","80829e2f":"code","411ef323":"code","2bef4afc":"code","f78870fa":"code","24ee7406":"code","f8cda2e7":"code","55f42076":"code","1d4fb536":"code","3933b6da":"code","4d4cb4b7":"code","562ce938":"code","f1579cb7":"code","48c4fa54":"code","5546787e":"code","89c9870c":"code","dff6b0f0":"code","ec62f901":"code","619b1f84":"markdown","0bd94fad":"markdown","7885c851":"markdown","5718f4fe":"markdown","ebba5aab":"markdown","76774de6":"markdown","288256c3":"markdown","e020858c":"markdown","ec988af7":"markdown","da5e30a9":"markdown","93fb5ecb":"markdown","25c35c4d":"markdown","dad0bb15":"markdown","e110fc08":"markdown","188a469a":"markdown","108c449b":"markdown","f31f27f8":"markdown","42e591fb":"markdown","eb99ea3c":"markdown","a1fc3c1b":"markdown","cfcd9ca8":"markdown"},"source":{"76e53f94":"# Temel K\u00fct\u00fcphaneleri \u00e7al\u0131\u015fma ortam\u0131na ekleyerek ba\u015flayal\u0131m\n\nimport numpy as np\n\nimport pandas as pd\n\nfrom pandas import Series, DataFrame\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split","1e3de623":"train = pd.read_csv('..\/input\/Train_UWu5bXk.csv')\ntest = pd.read_csv('..\/input\/Test_u94Q5KV.csv')","41ac8f9b":"# linear regression algoritmas\u0131n\u0131 sklearn k\u00fct\u00fcphanesinden y\u00fckl\u00fcyoruz.\n\nfrom sklearn.linear_model import LinearRegression\n\nlreg = LinearRegression()\n\n#Veriyi par\u00e7al\u0131yoruz, \u00e7\u00fcnk\u00fc \u00e7apraz do\u011frulama yapaca\u011f\u0131z\n\nX = train.loc[:,['Outlet_Establishment_Year','Item_MRP']]\n\nx_train, x_cv, y_train, y_cv = train_test_split(X,train.Item_Outlet_Sales)","71e0cb80":"#Modelimizi e\u011fitebiliriz.\n\nlreg.fit(x_train,y_train)\n\npred = lreg.predict(x_cv)\n\n#Hatalar\u0131n karesel ortalamas\u0131na bak\u0131yoruz\nmse = np.mean((pred - y_cv)**2)","e3629dc8":"mse","0af4e4fc":"# Katsay\u0131lara bir g\u00f6z atal\u0131m(Yani denklemdeki thetalar\u0131m\u0131z)\ncoeff = DataFrame(x_train.columns)\n\ncoeff['Coefficient Estimate'] = Series(lreg.coef_)\n\ncoeff","798c5051":"lreg.score(x_cv,y_cv)\n#Not: Sizin yapaca\u011f\u0131n\u0131z \u00e7al\u0131\u015fmada Rkare de\u011feri farkl\u0131 \u00e7\u0131kabilir.","ff61e88c":"train['Item_Weight'].fillna((train['Item_Weight'].mean()), inplace = True)\ntrain['Outlet_Establishment_Year'].fillna((train['Outlet_Establishment_Year'].mean()), inplace = True)","37f33b53":"train.head()","80829e2f":"#\u015eimdi Modelimizi tekrar kontrol edelim art\u0131k eksik de\u011fer olan bir s\u00fctun kalmad\u0131\u011f\u0131ndan bu i\u015flemei yapabiliriz.\nX = train.loc[:,['Outlet_Establishment_Year', 'Item_MRP', 'Item_Weight']]\n\n#Elimizdeki veriyi tekrar par\u00e7al\u0131yoruz.\nx_train, x_cv, y_train, y_cv = train_test_split(X, train.Item_Outlet_Sales)\n#\u015eimdi de Modeli E\u011fitelim\nlreg.fit(x_train,y_train)","411ef323":"#Modeli e\u011fitti\u011fimize g\u00f6re g\u00f6re \u015eimdi tahminlerde bulunal\u0131m\npred = lreg.predict(x_cv)","2bef4afc":"#\u015eimdi de Ortalama Kare Hatas\u0131'na(mse)'ye bakal\u0131m'\nmse = np.mean((pred - y_cv)**2)\n\nmse","f78870fa":"# \u015eimdi Katsay\u0131lar\u0131 kontrol edelim bakal\u0131m nas\u0131l bir korelasyon ili\u015fkileri var.\ncoeff = DataFrame(x_train.columns)\n\ncoeff['Coefficient Estimate'] = Series(lreg.coef_)\n\ncoeff","24ee7406":"#Son duruma g\u00f6re R-Square de\u011ferimize bakal\u0131m ve tahmin yetene\u011fimizin ne kadar oldu\u011funu g\u00f6relim.\n\nlreg.score(x_cv, y_cv)","f8cda2e7":"# Regresyon modeli i\u00e7in veri \u00f6n i\u015fleme ad\u0131mlar\u0131\n# Eksik de\u011ferleri \u00f6ncelikle ele al\u0131yoruz ve eksik alanlar\u0131 dolduruyoruz.\ntrain['Item_Visibility'] = train['Item_Visibility'].replace(0,np.mean(train['Item_Visibility']))\ntrain['Outlet_Establishment_Year'] =2013 - train['Outlet_Establishment_Year']\n\ntrain['Outlet_Size'].fillna('Small', inplace = True)\n\n#Yeni bir de\u011fi\u015fken olu\u015fturarak kategorik de\u011fi\u015fkenleri s\u00fcrekli de\u011fi\u015fkenlere \u00e7eviriyoruz(one -hot-encoding).\nmylist = list(train.select_dtypes(include =['object']).columns)\n\ndummies = pd.get_dummies(train[mylist], prefix = mylist)\n\ntrain.drop(mylist, axis = 1, inplace = True)\n\nX = pd.concat([train, dummies], axis = 1)","55f42076":"from sklearn.linear_model import LinearRegression\n# importing linear regression\nlreg = LinearRegression()\n\n# for cross validation\n\nfrom sklearn.model_selection import train_test_split\n\nX = train.drop('Item_Outlet_Sales',1)\n\nx_train, x_cv, y_train, y_cv = train_test_split(X,train.Item_Outlet_Sales, test_size =0.3)","1d4fb536":"lreg.fit(x_train, y_train)","3933b6da":"# Linear modelimizi e\u011fitelim\n\nlreg.fit(x_train,y_train)\n\n# calculating mse\n\nmse = np.mean((x_cv - y_cv)**2)\n\nmse\n\n# evaluation using r-square\n\nlreg.score(x_cv, y_cv)","4d4cb4b7":"predictors = x_train.columns\n\ncoef = Series(lreg.coef_,predictors).sort_values()\n\ncoef.plot(kind='bar', title='Modal Coefficients')","562ce938":"from sklearn.linear_model import Ridge\n\n##training the model\n\nridgeReg = Ridge(alpha=0.05, normalize=True)\n\nridgeReg.fit(x_train,y_train)\n\npred = ridgeReg.predict(x_cv)\n\n##calculating mse\n\nmse = np.mean((pred - y_cv)**2)","f1579cb7":"mse","48c4fa54":"from sklearn.linear_model import Ridge\n\n##training the model\n\nridgeReg = Ridge(alpha=0.5, normalize=True)\n\nridgeReg.fit(x_train,y_train)\n\npred = ridgeReg.predict(x_cv)\n\n##calculating mse\n\nmse = np.mean((pred - y_cv)**2)\nmse","5546787e":"from sklearn.linear_model import Ridge\n\n##training the model\n\nridgeReg = Ridge(alpha=5, normalize=True)\n\nridgeReg.fit(x_train,y_train)\n\npred = ridgeReg.predict(x_cv)\n\n##calculating mse\n\nmse = np.mean((pred - y_cv)**2)\nmse","89c9870c":"from sklearn.linear_model import Lasso\n\nlassoReg = Lasso(alpha=0.3, normalize=True)\n\nlassoReg.fit(x_train,y_train)\n\npred = lassoReg.predict(x_cv)\n\n#calculating mse\n\nmse = np.mean((pred - y_cv)**2)\n\nmse\n\nlassoReg.score(x_cv,y_cv)","dff6b0f0":"from sklearn.linear_model import ElasticNet\n\nENreg = ElasticNet(alpha=1, l1_ratio=0.5, normalize=False)\n\nENreg.fit(x_train,y_train)\n\npred_cv = ENreg.predict(x_cv)\n\n#calculating mse\n\nmse = np.mean((pred_cv - y_cv)**2)\n\n\nENreg.score(x_cv,y_cv) #R-Squar","ec62f901":"mse","619b1f84":"## 12. Lasso regresyonu\nLASSO ((Least Absolute Shrinkage Selector Operator))En Az Mutlak B\u00fcz\u00fclme Se\u00e7ici Operat\u00f6r\u00fc), ridge'e olduk\u00e7a benzer, ancak b\u00fcy\u00fck mart veriseti problemimizde uygulayarak aralar\u0131ndaki fark\u0131 anlayabiliyoruz.","0bd94fad":"## 8. Polynomial Regression\nPolinom regresyonu, ba\u011f\u0131ms\u0131z de\u011fi\u015fkenin maksimum g\u00fcc\u00fcn\u00fcn 1'den fazla oldu\u011fu bir ba\u015fka regresyon \u015feklidir. Bu regresyon tekni\u011finde, en uygun \u00e7izgi do\u011frusal de\u011fil onun yerine e\u011fri bi\u00e7iminde k\u0131vr\u0131ml\u0131d\u0131r.\n\nKuadratik regresyon veya ikinci dereceden polinomlu regresyon, a\u015fa\u011f\u0131daki denklemde verilir:\n\nY =\u03981 + \u03982x + \u03983x2\n\n\u015eimdi a\u015fa\u011f\u0131da verilen \u00e7\u0131kt\u0131ya bir g\u00f6z atal\u0131m.\n![polinimial_regresyon.png](attachment:polinimial_regresyon.png)\n\nA\u00e7\u0131k\u00e7a, ikinci dereceden denklem, verilere basit do\u011frusal denklemden daha iyi uyar. Bu durumda, kuadratik regresyonun R-kare de\u011ferinin basit do\u011frusal regresyondan daha b\u00fcy\u00fck olaca\u011f\u0131n\u0131 m\u0131 d\u00fc\u015f\u00fcn\u00fcyorsunuz? Kesinlikle evet, \u00e7\u00fcnk\u00fc ikinci dereceden regresyon verilere do\u011frusal regresyondan daha iyi uyar. \u0130kinci dereceden ve k\u00fcbik polinomlar yayg\u0131n olsa da, daha y\u00fcksek dereceli polinomlar\u0131 da ekleyebilirsiniz.\n\nA\u015fa\u011f\u0131daki \u015fekil 6 dereceli bir polinom denkleminin davran\u0131\u015f\u0131n\u0131 g\u00f6stermektedir.\n![polinimial_regresyon_6derece.png](attachment:polinimial_regresyon_6derece.png)\n\nDolay\u0131s\u0131yla, veri k\u00fcmesine uymas\u0131 i\u00e7in daha y\u00fcksek dereceli polinomlar\u0131 kullanman\u0131n her zaman daha iyi oldu\u011funu mu d\u00fc\u015f\u00fcn\u00fcyor musunuz? Bu d\u00fc\u015f\u00fcnceniz i\u00e7in \u00fczg\u00fcn\u00fcm. Temel olarak, e\u011fitim verilerimize uygun bir model olu\u015fturduk ancak e\u011fitim setinin d\u0131\u015f\u0131ndaki de\u011fi\u015fkenler aras\u0131ndaki ger\u00e7ek ili\u015fkiyi tahmin edemiyoruz. Bu nedenle modelimiz test verileri \u00fczerinde zay\u0131f bir performans sergiliyor. Bu soruna a\u015f\u0131r\u0131 uydurma(overfitting) denir. Ayr\u0131ca modelin y\u00fcksek varyansa ve d\u00fc\u015f\u00fck \u00f6nyarg\u0131ya sahip oldu\u011funu s\u00f6yl\u00fcyoruz.\n\n![overunder1.png](attachment:overunder1.png)\nBenzer \u015fekilde, eksiklik_az uyum(under-fitting) denilen ba\u015fka bir problemimiz var, bu bizim modelimiz e\u011fitim verisine uymad\u0131\u011f\u0131nda veya yeni verilere genelleme yap\u0131ld\u0131\u011f\u0131nda meydana gelir.\n\n![overunder21.png](attachment:overunder21.png)\n\nModelimiz y\u00fcksek \u00f6nyarg\u0131 ve d\u00fc\u015f\u00fck de\u011fi\u015fkenli\u011fe sahip oldu\u011funda yetersizdir.","7885c851":"\nBu durumda, model 2'den \u00e7ok daha k\u00fc\u00e7\u00fck olan bir mse = 1.973,797.57 de\u011feri elde ettik. Bu da bize g\u00f6steriyorki, iki \u00f6zellik yard\u0131m\u0131yla tahmin etmek \u00e7ok daha do\u011fru sonu\u00e7lar verecektir.(ilk tahminde mse de\u011feri =2.875.386) Do\u011frusal regresyon modelinin katsay\u0131lar\u0131na bir g\u00f6z atal\u0131m.","5718f4fe":"Bu \u00e7al\u0131\u015fmada, R\u00b2 %32'dir, yani bunun anlam\u0131 \u015fudur; sat\u0131\u015flardaki varyans\u0131n yaln\u0131zca % 32'si Kurulu\u015f y\u0131l\u0131 ve MRP de\u011fi\u015fkenleri taraf\u0131ndan a\u00e7\u0131klanmaktad\u0131r.\n\nDi\u011fer bir ifade ile, kurulu\u015f y\u0131l\u0131n\u0131 ve M_R_P'yi biliyorsan\u0131z, sat\u0131\u015flar hakk\u0131nda do\u011fru bir tahmin yapmak i\u00e7in %32'lik bir bilginiz var demektir.\n\n\u015eimdi, modelime bir \u00f6zellik daha eklersem, modelim de\u011ferleri ger\u00e7ek de\u011ferine daha yak\u0131n tahmin edebilir mi? R-Square'in de\u011feri yeni ekleyece\u011fimiz \u00f6zellik ile artacak m\u0131 bunu kontrol edelim.\n\n{**\"Varyans:** Verilerin aritmetik ortalamadan sapmalar\u0131n\u0131n karelerinin toplam\u0131d\u0131r. Yani Standart Sapma'n\u0131n karesidir.}\n\n**Model 4 - Daha fazla de\u011fi\u015fkenli do\u011frusal regresyon(Multivariate Linear Regression)**\nBir de\u011fi\u015fken yerine birden fazla de\u011fi\u015fken kullanarak \u00f6\u011felerin sat\u0131\u015flar\u0131 hakk\u0131nda do\u011fru tahminlerde bulunma yetene\u011fimizi geli\u015ftirdi\u011fini \u00f6\u011frendik.\n\n\u00d6yleyse, yeni durumda ba\u015fka bir \u00f6zelli\u011fi yani 'a\u011f\u0131rl\u0131k' \u00f6zelli\u011fini de ekleyelim. \u015eimdi bu \u00fc\u00e7 \u00f6zellik ile bir regresyon modeli olu\u015ftural\u0131m bakal\u0131m sonu\u00e7lara etkisi nas\u0131l olacak?","ebba5aab":"## Modelimizi Olu\u015ftural\u0131m","76774de6":"## 9. Regresyon modellerinde yanl\u0131l\u0131k ve varyans(Bias & Variance)\nBu \u00f6nyarg\u0131 ve varyans asl\u0131nda ne anlama geliyor? Bunu ok\u00e7uluk hedefleri \u00f6rne\u011fiyle anlayal\u0131m.\n\n![bias-variance.png](attachment:bias-variance.png)\nDiyelim ki \u00e7ok do\u011fru bir modelimiz var, e\u011fer b\u00f6yle ise modelimizin hatas\u0131 d\u00fc\u015f\u00fck olacakt\u0131r, yani ilk \u015fekilde g\u00f6sterildi\u011fi gibi d\u00fc\u015f\u00fck \u00f6nyarg\u0131(bias) ve d\u00fc\u015f\u00fck de\u011fi\u015fkenlik(varyans) anlam\u0131na gelir. T\u00fcm veri noktalar\u0131 bo\u011fa g\u00f6z\u00fcn\u00fcn i\u00e7ine girer. Benzer \u015fekilde, varyans artarsa, veri noktam\u0131z\u0131n yay\u0131lmas\u0131n\u0131n artaca\u011f\u0131n\u0131 ve bunun da daha az do\u011fru tahminle sonu\u00e7land\u0131\u011f\u0131n\u0131 s\u00f6yleyebiliriz. \u00d6nyarg\u0131 artt\u0131k\u00e7a, \u00f6ng\u00f6r\u00fclen de\u011ferimiz ile hata aras\u0131ndaki fark artar ve g\u00f6zlenen de\u011ferler artar.\n\nM\u00fckemmel bir modele sahip olmak i\u00e7in \u015fimdi \u00f6nyarg\u0131 ve varyans\u0131n nas\u0131l de\u011ferlendirece\u011fiz? A\u015fa\u011f\u0131daki resme bakarak anlamaya \u00e7al\u0131\u015fal\u0131m. \n\n![model-complex.png](attachment:model-complex.png)\n\nModelimize daha fazla parametre ekledi\u011fimizde karma\u015f\u0131kl\u0131\u011f\u0131 da artar, bu da varyans\u0131n artmas\u0131 ve \u00f6nyarg\u0131n\u0131n azalt\u0131lmas\u0131 demektir. Yani fazla uydurma(overfitting) ile sonu\u00e7lan\u0131r. Bu nedenle, yanl\u0131l\u0131\u011f\u0131m\u0131zdaki azalman\u0131n varyanstaki art\u0131\u015fa e\u015fit oldu\u011fu modelimizde bir optimum nokta bulmam\u0131z gerekiyor. Uygulamada, bu noktay\u0131 bulmak i\u00e7in analitik bir yol yoktur. Peki y\u00fcksek sapma veya y\u00fcksek \u00f6nyarg\u0131 ile nas\u0131l ba\u015fa \u00e7\u0131k\u0131l\u0131r?\n\nAz \u00f6\u011frenme(underfitting) ya da y\u00fcksek \u00f6nyarg\u0131n\u0131n \u00fcstesinden gelmek i\u00e7in, temel olarak modelimize yeni parametreler ekleyebiliriz; b\u00f6ylece model karma\u015f\u0131kl\u0131\u011f\u0131 artar ve y\u00fcksek \u00f6nyarg\u0131 azal\u0131r.\n\n\u015eimdi, bir regresyon modeli i\u00e7in Overfitting'i nas\u0131l yenebiliriz?\n\nTemel olarak, fazla uydurman\u0131n(Overfitting) \u00fcstesinden gelmek i\u00e7in iki y\u00f6ntem vard\u0131r,\n\n    - Model karma\u015f\u0131kl\u0131\u011f\u0131n\u0131 azalt\u0131n(Reduce Model complexity)\n    - D\u00fczenlile\u015ftirme(Regularization) yap\u0131n \n\nD\u00fczenlile\u015ftirme hakk\u0131nda ve modelinizi daha genel hale getirmek i\u00e7in nas\u0131l kullanaca\u011f\u0131n\u0131z\u0131 tart\u0131\u015faca\u011f\u0131z.","288256c3":"\u015eimdi Yukar\u0131daki \u00e7\u0131kt\u0131lar\u0131 de\u011ferlendirelim; Hem Ridge'den hem de Lasso'dan \u00e7ok daha az R-Square'in de\u011ferini oldu\u011funu g\u00f6r\u00fcyoruz. Neden oldu\u011funu d\u00fc\u015f\u00fcnelim. Bu \u00e7\u00f6k\u00fc\u015f\u00fcn arkas\u0131ndaki sebep, temel olarak, \u00e7ok fazla \u00f6zelli\u011fe sahip olmad\u0131\u011f\u0131m\u0131zd\u0131r. Elastik regresyon genellikle b\u00fcy\u00fck bir veri setine sahip oldu\u011fumuzda iyi \u00e7al\u0131\u015f\u0131r.\nNot, burada iki parametre alfa ve l1_ratio vard\u0131r. \u00d6ncelikle elastik a\u011fda ne oldu\u011funu ve Ridge ve Lasso'dan nas\u0131l farkl\u0131 oldu\u011funu tart\u0131\u015fal\u0131m.\n\nElastik a\u011f, temel olarak, hem L1 hem de L2 d\u00fczenlemesinin bir birle\u015fimidir. Yani elastik a\u011f\u0131 biliyorsan\u0131z, parametreleri ayarlayarak hem Ridge'i hem de Lasso'yu uygulayabilirsiniz. Dolay\u0131s\u0131yla hem L1 hem de L2 ceza terimini kullan\u0131r, bu nedenle denklemi a\u015fa\u011f\u0131daki gibi g\u00f6r\u00fcn\u00fcr:!\n\n![elastik_net.png](attachment:elastik_net.png)\n\nPeki L1 ve L2 ceza s\u00fcresini kontrol etmek i\u00e7in lambdalar\u0131 nas\u0131l ayarlayaca\u011f\u0131z? Bir \u00f6rnek ile anlayal\u0131m; G\u00f6letten bal\u0131k tutmaya \u00e7al\u0131\u015f\u0131yorsunuz ve sadece bir a\u011f\u0131n\u0131z var, o zaman ne yapard\u0131n\u0131z? Net olarak rasgele atar m\u0131s\u0131n? Hay\u0131r, asl\u0131nda etrafta bir tane bal\u0131k g\u00f6rene kadar bekleyeceksiniz, daha sonra temel olarak t\u00fcm bal\u0131k grubunu toplamak i\u00e7in a\u011f\u0131 bu y\u00f6ne f\u0131rlatacaks\u0131n\u0131z. Dolay\u0131s\u0131yla, birbirleriyle korele olsalar bile, yine de t\u00fcm gruba bakmak istiyoruz.\n\nElastik regresyon benzer \u015fekilde \u00e7al\u0131\u015f\u0131r. Diyelim ki, bir veri setinde bir grup korelasyonlu ba\u011f\u0131ms\u0131z de\u011fi\u015fkenimiz var, o zaman elastik a\u011f basit\u00e7e bu ili\u015fkili de\u011fi\u015fkenlerden olu\u015fan bir grup olu\u015fturacakt\u0131r. \u015eimdi e\u011fer bu grubun de\u011fi\u015fkenlerinden herhangi biri g\u00fc\u00e7l\u00fc bir tahminci ise (yani ba\u011f\u0131ml\u0131 de\u011fi\u015fkenle g\u00fc\u00e7l\u00fc bir ili\u015fkiye sahip olmak anlam\u0131na gelir), o zaman t\u00fcm grubu modele dahil edece\u011fiz, \u00e7\u00fcnk\u00fc di\u011fer de\u011fi\u015fkenleri atlamak (Lasso'da yapt\u0131\u011f\u0131m\u0131z gibi) olabilir. Yorumlama kabiliyeti bak\u0131m\u0131ndan baz\u0131 bilgilerin kaybedilmesiyle sonu\u00e7lan\u0131r, bu da d\u00fc\u015f\u00fck bir model performans\u0131na yol a\u00e7ar.\n\nBu nedenle, yukar\u0131daki koda bakarsan\u0131z, modeli tan\u0131mlarken alfa ve l1_ratio tan\u0131mlamam\u0131z gerekir. Alpha ve l1_ratio, L1 ve L2 cezalar\u0131n\u0131 ayr\u0131 ayr\u0131 kontrol etmek istiyorsan\u0131z, buna g\u00f6re ayarlayabilece\u011finiz parametrelerdir.","e020858c":"Dolay\u0131s\u0131yla, modelimizde hafif bir geli\u015fme oldu\u011funu g\u00f6rebiliyoruz, \u00e7\u00fcnk\u00fc R -square'\u0131n de\u011feri artt\u0131r\u0131ld\u0131. Ridge'in hiperparametresi olan alfa de\u011ferinin, model taraf\u0131ndan otomatik olarak \u00f6\u011frenilmedi\u011fini, bunun yerine manuel olarak ayarlanmas\u0131 gerekti\u011fini hat\u0131r\u0131m\u0131zda tutal\u0131m.\n\nBurada alfa = 0.05'i g\u00f6z \u00f6n\u00fcne ald\u0131k. Fakat, farkl\u0131 alfa de\u011ferlerini d\u00fc\u015f\u00fcnelim ve her durum i\u00e7in katsay\u0131lar\u0131 \u00e7izelim.\n\n![Ridge0.051.png](attachment:Ridge0.051.png)\n![ridge0.51.png](attachment:ridge0.51.png)\n![ridge51.png](attachment:ridge51.png)\n![ridge101.png](attachment:ridge101.png)\n\nAlfa de\u011ferini art\u0131rd\u0131k\u00e7a katsay\u0131lar\u0131n b\u00fcy\u00fckl\u00fc\u011f\u00fcn\u00fcn azald\u0131\u011f\u0131n\u0131, de\u011ferlerin s\u0131f\u0131ra ula\u015ft\u0131\u011f\u0131n\u0131 ancak mutlak s\u0131f\u0131ra d\u00fc\u015fmedi\u011fini g\u00f6rebilirsiniz.\n\nAncak, her alfa i\u00e7in R-kare hesaplarsan\u0131z, R-kare de\u011ferinin alfa = 0,05'te maksimum olaca\u011f\u0131n\u0131 g\u00f6rece\u011fiz. Bu y\u00fczden ak\u0131ll\u0131ca bir de\u011fer aral\u0131\u011f\u0131nda yineleyerek ve bize en d\u00fc\u015f\u00fck hata veren alfa de\u011ferini kullanarak se\u00e7meliyiz.\n\n\u00d6yleyse, \u015fimdi nas\u0131l uygulanaca\u011f\u0131 hakk\u0131nda bir fikriniz var ama biz matematik taraf\u0131na da bir g\u00f6z atal\u0131m. \u015eimdiye kadar bizim fikrimiz temel olarak maliyet fonksiyonunu en aza indirgemekti, \u00f6yle ki \u015fimdi \u00f6ng\u00f6r\u00fclen de\u011ferler istenen sonuca \u00e7ok daha yak\u0131n.\n\nBurada fark ederseniz, ceza terimi olarak bilinen fazladan bir terim ile kar\u015f\u0131la\u015f\u0131yoruz. Burada verilen \u03bb, asl\u0131nda s\u0131rt(ridge) fonksiyonundaki alfa parametresi ile belirtilir. Bu y\u00fczden alfa de\u011ferlerini de\u011fi\u015ftirerek, temel olarak ceza terimini kontrol ediyoruz. Alfa de\u011ferleri ne kadar y\u00fcksek olursa, ceza o kadar b\u00fcy\u00fckt\u00fcr ve bu nedenle katsay\u0131lar\u0131n b\u00fcy\u00fckl\u00fc\u011f\u00fc azal\u0131r.\n\n    -\u00d6nemli noktalar:\n    -Parametreleri k\u00fc\u00e7\u00fclt\u00fcr, bu nedenle \u00e7o\u011funlukla \u00e7oklu ba\u011flant\u0131y\u0131 \u00f6nlemek i\u00e7in kullan\u0131l\u0131r.\n    -Model karma\u015f\u0131kl\u0131\u011f\u0131n\u0131, katsay\u0131l\u0131 b\u00fcz\u00fclme ile azalt\u0131r.\n    -L2 d\u00fczenlile\u015ftirme tekni\u011fini kullan\u0131r. (bu \u00e7al\u0131\u015fmada daha sonra tart\u0131\u015faca\u011f\u0131z) \u015eimdi d\u00fczenlile\u015ftirmeyi(regularization) de kullanan ba\u015fka bir regresyon tekni\u011fi t\u00fcr\u00fcn\u00fc inceleyelim.","ec988af7":"## 14. D\u00fczenleme Tekniklerinin T\u00fcrleri \nHat\u0131rlayal\u0131m, hem Ridge'de hem de Lasso'da bir ceza terimi ekledik, ancak her iki durumda da terim farkl\u0131yd\u0131. Ridge'de, theta'n\u0131n karelerini, Lasso'da ise theta'n\u0131n mutlak de\u011ferini kulland\u0131k. \u00d6yleyse neden sadece bu ikisi, ba\u015fka olas\u0131l\u0131klar olamaz m\u0131?\n\nAsl\u0131nda, taraf\u0131ndan belirtilen normalle\u015ftirme teriminde, parametrenin farkl\u0131 s\u0131ralama d\u00fczenleriyle farkl\u0131 olas\u0131 d\u00fczenleme se\u00e7enekleri vard\u0131r. Bu daha genel olarak Lp d\u00fczenleyici olarak bilinir.\n\nBaz\u0131lar\u0131n\u0131 \u00e7izerek g\u00f6rselle\u015ftirmeye \u00e7al\u0131\u015fal\u0131m. G\u00f6rselle\u015ftirmeyi kolayla\u015ft\u0131rmak i\u00e7in, onlar\u0131 2D uzayda \u00e7izelim. Bunun i\u00e7in iki parametremiz oldu\u011funu varsayal\u0131m. \u015eimdi, diyelim ki p = 1 terimimiz var. Bu \u00e7izgi denklemini \u00e7izemez miyiz? Benzer \u015fekilde, p'nin farkl\u0131 de\u011ferleri i\u00e7in \u00e7izim a\u015fa\u011f\u0131da verilmi\u015ftir.\n\n![Regularizasyon_teknikleri_.png](attachment:Regularizasyon_teknikleri_.png)\n\n\nYukar\u0131daki grafiklerde eksen parametreleri belirtir (\u03981 ve \u03982). Onlar\u0131 tek tek inceleyelim.\n\nP = 0.5 i\u00e7in, yaln\u0131zca di\u011fer parametre \u00e7ok k\u00fc\u00e7\u00fckse, bir parametrenin yaln\u0131zca b\u00fcy\u00fck de\u011ferlerini alabiliriz. P = 1 i\u00e7in, parametredeki art\u0131\u015f\u0131n di\u011ferindeki d\u00fc\u015f\u00fc\u015fle tam olarak dengelendi\u011fi mutlak de\u011ferlerin toplam\u0131n\u0131 elde ederiz. P = 2 i\u00e7in bir daire al\u0131yoruz ve daha b\u00fcy\u00fck p de\u011ferleri i\u00e7in yuvarlak kare \u015fekline yakla\u015f\u0131yor.\n\nEn yayg\u0131n kullan\u0131lan iki d\u00fczenleme, i\u00e7inde L1 ve L2 d\u00fczenlemesi olarak bilinen p = 1 ve p = 2 de\u011ferlerine sahibiz.\n\nA\u015fa\u011f\u0131da verilen \u015fekle dikkatlice bak\u0131n. Mavi \u015fekil normalle\u015ftirme terimini ifade eder ve mevcut di\u011fer \u015fekil en k\u00fc\u00e7\u00fck kare hatam\u0131z\u0131 (veya veri terimini) belirtir.\n\n\u0130lk rakam L1 ve ikincisi L2 d\u00fczenlenmesi i\u00e7indir. Siyah nokta, en k\u00fc\u00e7\u00fck kare hatas\u0131n\u0131n bu noktada en aza indirildi\u011fini ve g\u00f6r\u00fcld\u00fc\u011f\u00fc gibi, ondan hareket ettik\u00e7e kuadrik olarak artt\u0131\u011f\u0131n\u0131 ve t\u00fcm parametrelerin s\u0131f\u0131r oldu\u011fu ba\u015flang\u0131\u00e7ta d\u00fczenlenme s\u00fcresini en aza indirdi\u011fini g\u00f6steriyor.\n\n\u015eimdi soru \u015fu ki, maliyet fonksiyonumuz hangi noktada minimum olacak? Cevap, kuadrik olarak artt\u0131\u011f\u0131 i\u00e7in, her iki terimin toplam\u0131, ilk kesi\u015fti\u011fi noktada en aza indirilecektir.\n\n![Regularizasyon_teknikleri2_.png](attachment:Regularizasyon_teknikleri2_.png)\n\nL2 normalizasyon e\u011frisine bir bak\u0131n. L2 d\u00fczenleyicinin olu\u015fturdu\u011fu \u015fekil bir daire oldu\u011fundan, ondan uzakla\u015ft\u0131k\u00e7a d\u00f6rtl\u00fc olarak artar. L2 optimum (temel olarak kesi\u015fme noktas\u0131d\u0131r) eksen \u00e7izgileri \u00fczerine ancak minimum MSE (ortalama kare hatas\u0131 veya \u015fekildeki siyah nokta) ayn\u0131 zamanda tam olarak eksendeyken d\u00fc\u015febilir. Ancak L1 durumunda, L1 optimum ekseni eksen \u00e7izgisinde olabilir, \u00e7\u00fcnk\u00fc konturu keskindir ve bu nedenle eksen \u00fczerinde d\u00fc\u015fme olas\u0131l\u0131\u011f\u0131 y\u00fcksek olan etkile\u015fim noktas\u0131 vard\u0131r. Bu nedenle, minimum MSE eksende olmasa bile, eksen \u00e7izgisinde kesi\u015fmek m\u00fcmk\u00fcnd\u00fcr. Kav\u015fak noktas\u0131 eksenlerin \u00fczerine d\u00fc\u015ferse, seyrek olarak bilinir.\n\nBu nedenle, L1 modelimizi depolamak ve hesaplamak i\u00e7in daha verimli hale getiren ve ayr\u0131ca \u00f6zelli\u011fin \u00f6nemini kontrol etmede yard\u0131mc\u0131 olan bir miktar sparite seviyesi sunar, \u00e7\u00fcnk\u00fc \u00f6nemli olmayan \u00f6zellikler tam olarak s\u0131f\u0131ra ayarlanabilir.","da5e30a9":"## 11. Ridge Regresyonu\n\u00d6ncelikle yukar\u0131daki problemimize uygulayal\u0131m ve sonu\u00e7lar\u0131m\u0131z\u0131 lineer regresyon modelimizden daha iyi performans g\u00f6sterip g\u00f6stermedi\u011fini kontrol edelim.","93fb5ecb":"## 10. D\u00fczenleme(Regularizasyon)\nModelinizi haz\u0131rlad\u0131n\u0131z, \u00e7\u0131kt\u0131n\u0131z\u0131 tahmin ettiniz. \u00d6yleyse neden d\u00fczenlile\u015fmeye \u00e7al\u0131\u015fmam\u0131z gerekiyor? Bu gerekli mi?\n\nBir yar\u0131\u015fmaya kat\u0131ld\u0131\u011f\u0131n\u0131z\u0131 ve bu problemde s\u00fcrekli bir de\u011fi\u015fkeni tahmin etmeniz gerekti\u011fini varsayal\u0131m. B\u00f6ylece do\u011frusal regresyon uygulad\u0131n\u0131z ve \u00e7\u0131kt\u0131n\u0131z\u0131 tahmin ettiniz. \u0130\u015fte bu kadar! Sen lider panosundas\u0131n o da g\u00fczel ama baz\u0131 i\u015fler ters gidiyor gibi...!!!. Ama beklemedi\u011finiz \u015fey \u015fu ki, yine lider panosunda sizden ba\u015fka bir\u00e7ok insan var. Ama her \u015feyi do\u011fru yapt\u0131\u011f\u0131m\u0131za g\u00f6re bu nas\u0131l m\u00fcmk\u00fcn olabilir?\n\n  - \u201cHer \u015fey m\u00fcmk\u00fcn oldu\u011funca basit yap\u0131lmal\u0131, ancak daha basit\u00e7e olmamal\u0131 - Albert Einstein\u201d- \n\nYapt\u0131\u011f\u0131m\u0131z i\u015flem \u00e7ok basitti, herkeste bunu yapt\u0131, \u015fimdi de daha \u00e7ok basitle\u015ftirmeye bakal\u0131m. Bu y\u00fczden, kodumuzu d\u00fczenlile\u015ftirme(regularize) yard\u0131m\u0131 ile optimize etmeye \u00e7al\u0131\u015faca\u011f\u0131z.\n\nD\u00fczenleme i\u00e7in yapt\u0131\u011f\u0131m\u0131z \u015fey normalde ayn\u0131 say\u0131da \u00f6zelli\u011fi s\u00fcrd\u00fcrmemiz, fakat katsay\u0131lar\u0131n b\u00fcy\u00fckl\u00fc\u011f\u00fcn\u00fc azaltmam\u0131zd\u0131r yani j'leri. Katsay\u0131lar\u0131 azaltmak bize nas\u0131l yard\u0131mc\u0131 olacak?\n\nA\u015fa\u011f\u0131daki regresyon modelimizdeki \u00f6zellik katsay\u0131lar\u0131na bir g\u00f6z atal\u0131m.\n\n![linear1_regularize.png](attachment:linear1_regularize.png)\n\nOutlet_Identifier_OUT027 ve Outlet_Type_Supermarket_Type3 (son 2 pozitif katsay\u0131) katsay\u0131lar\u0131n\u0131n geri kalan katsay\u0131larla kar\u015f\u0131la\u015ft\u0131r\u0131ld\u0131\u011f\u0131nda \u00e7ok daha y\u00fcksek oldu\u011funu g\u00f6rebiliriz. Bu nedenle, bir malzemenin toplam sat\u0131\u015flar\u0131 bu iki \u00f6zellikten daha fazla etkilenecektir.\n\nModelimizdeki katsay\u0131lar\u0131n b\u00fcy\u00fckl\u00fc\u011f\u00fcn\u00fc nas\u0131l azaltabiliriz? Bu ama\u00e7la, bu sorunun \u00fcstesinden gelmek i\u00e7in d\u00fczenlile\u015fmeyi kullanan farkl\u0131 tipte regresyon tekniklerine sahibiz. \u00d6yleyse onlar\u0131 tart\u0131\u015fal\u0131m.","25c35c4d":"## Biti\u015f Notlar\u0131\nUmar\u0131m \u015fimdi, do\u011frusal regresyonun arkas\u0131ndaki bilimi ve bunun nas\u0131l uygulanaca\u011f\u0131n\u0131 ve modelinizi geli\u015ftirmek i\u00e7in nas\u0131l daha iyi duruma getirece\u011fimizi anlamaya ba\u015flad\u0131k.\n\n\u201cBilgi hazinedir ve uygulama bunun anahtar\u0131d\u0131r\u201d\n\nBu nedenle, baz\u0131 problemleri \u00e7\u00f6zerek ellerimizi kirletelim. Bunun ii\u00e7in \"Big mart sat\u0131\u015f\" problemiyle ba\u015flayabilir ve modelinizi baz\u0131 \u00f6zellik m\u00fchendisli\u011fi ile geli\u015ftirmeye \u00e7al\u0131\u015fabilirsiniz.\n\nNot: Bu \u00e7al\u0131\u015fma \u00f6rne\u011fi ekteki[link](https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/a-comprehensive-guide-for-linear-ridge-and-lasso-regression\/)'den faydalan\u0131larak haz\u0131rlanm\u0131\u015ft\u0131r.","dad0bb15":"Asl\u0131nda biz ;\n\nAlpha = a + b ve l1_ratio = a \/ (a+b)\nburada, a ve b a\u011f\u0131rl\u0131klar\u0131 s\u0131ras\u0131yla L1 ve L2 terimine verilmi\u015ftir. B\u00f6ylece, alfa ve l1_ratio de\u011ferlerini de\u011fi\u015ftirdi\u011fimizde a ve b, L1 ve L2 aras\u0131ndaki de\u011fi\u015f toku\u015fu kontrol edecek \u015fekilde ayarlan\u0131r:\n\na (L1 terimi) + b (L2 terimi)\n\nAlfa (veya a + b) = 1 olsun ve \u015fimdi a\u015fa\u011f\u0131dakileri g\u00f6z \u00f6n\u00fcnde bulundurun:\n\n        - L1_ratio = 1 ise, l1_ratio form\u00fcl\u00fcne bakarsak,\n        - l1_ratio'nun sadece a = 1 ise b = 0 anlam\u0131na gelirse 1'e e\u015fit olabilece\u011fini g\u00f6rebiliriz. \n        Bu nedenle, bir Lasso olacak.\n        - Benzer \u015fekilde e\u011fer l1_ratio = 0 ise, a = 0 anlam\u0131na gelir. O zaman ceza bir Ridge cezas\u0131 olacak. 0 ile 1 aras\u0131ndaki 1_ratio i\u00e7in ceza, Ridge ve Lasso kombinasyonudur.\n\n\u00d6yleyse alfa ve l1_ratio'yu ayarlayal\u0131m ve a\u015fa\u011f\u0131da verilen katsay\u0131lar\u0131n alanlar\u0131ndan anlamaya \u00e7al\u0131\u015fal\u0131m.\n\nArt\u0131k Ridge, Lasso ve elastik a\u011f regresyonu hakk\u0131nda temel bir bilgimiz var. Fakat bu s\u0131rada, temelde iki d\u00fczenleme t\u00fcr\u00fc olan iki terimle L1 ve L2 ile kar\u015f\u0131la\u015ft\u0131k. \u00d6zetle, Lasso ve Ridge'\u0131n \u00f6zetlenmesi, s\u0131ras\u0131yla L1 ve L2 d\u00fczeninin do\u011frudan uygulanmas\u0131d\u0131r.\n\nAncak hala bilmek istiyorsan\u0131z, a\u015fa\u011f\u0131da onlar\u0131n arkas\u0131ndaki kavram\u0131 a\u00e7\u0131klad\u0131m, bunada isterseniz bakabilirsiniz.","e110fc08":"Yukar\u0131daki sonu\u00e7tan anla\u015f\u0131laca\u011f\u0131 \u00fczere, mse'nin daha da azald\u0131\u011f\u0131n\u0131 g\u00f6r\u00fcyoruz. R-kare de\u011ferinde ise bir art\u0131\u015f var, bu sonu\u00e7 bize g\u00f6steriyorki yeni de\u011ferin a\u011f\u0131rl\u0131\u011f\u0131n\u0131 ilave etmenin modelimiz i\u00e7in yararl\u0131 oldu\u011fu anlam\u0131na m\u0131 geliyor? \u015eimdi bunu inceleyelim.\n\n### Ayarlanm\u0131\u015f R- Kare(Adjusted R-Squared)\nR2'nin tek sak\u0131ncas\u0131, modelimize yeni \u00f6ng\u00f6r\u00fcc\u00fcler (X) eklenirse, R2'nin yaln\u0131zca y\u00fckselmesi veya sabit kalmas\u0131, ancak asla d\u00fc\u015fmemesidir. Modelimizin karma\u015f\u0131kl\u0131\u011f\u0131n\u0131 art\u0131rarak daha do\u011fru hale getirip getirmedi\u011fimizi kontrol edelim.\n\nD\u00fczeltilmi\u015f R-Kare, modeldeki \u00f6ng\u00f6r\u00fcc\u00fclerin(X) say\u0131s\u0131na g\u00f6re ayarlanm\u0131\u015f, R-Kare'nin de\u011fi\u015ftirilmi\u015f \u015feklidir. Modelin serbestlik derecesini i\u00e7erir. D\u00fczeltilmi\u015f R-Kare sadece yeni terim model do\u011frulu\u011funu iyile\u015ftirirse artar.\n\n*Denklemdeki harflerin anlamlar\u0131;\n\nR2 = R kare \u00f6rnek say\u0131s\u0131\n\np = Tahmin edicilerin say\u0131s\u0131\n\nN = Toplam \u00f6rnek say\u0131s\u0131\n\n## 7. Tahmin i\u00e7in t\u00fcm \u00f6zellikleri kullanma\n\u015eimdi t\u00fcm \u00f6zellikleri i\u00e7eren bir model yapal\u0131m. Regresyon modellerini olu\u015ftururken sadece s\u00fcrekli \u00f6zellikleri kulland\u0131k. Bunun nedeni, kategorik de\u011fi\u015fkenleri do\u011frusal regresyon modelinde kullanmadan \u00f6nce farkl\u0131 \u015fekilde ele almam\u0131z gerekti\u011fidir. Onlar\u0131 uygun hale getirmek i\u00e7in farkl\u0131 teknikler var, burada bir s\u0131cak kodlama(one-hot encoding) kulland\u0131k (bir kategorik de\u011fi\u015fkenin her s\u0131n\u0131f\u0131n\u0131 bir \u00f6zellik olarak d\u00f6n\u00fc\u015ft\u00fcr\u00fcyoruz). Bunun d\u0131\u015f\u0131nda \u00e7\u0131k\u0131\u015f boyutu i\u00e7in eksik olan de\u011ferleri de belirledik.","188a469a":"## 13. Elastik Net Regresyon\nTeori k\u0131sm\u0131na girmeden \u00f6nce, bunu b\u00fcy\u00fck mart sat\u0131\u015f problemine uygulayal\u0131m. Ridge ve Lasso'dan daha iyi performans g\u00f6sterir mi? Hadi kontrol edelim!","108c449b":"# Giri\u015f\n\n\u0130stanbul'da S\u00fcpermarket zincirlerinden birinde operasyon m\u00fcd\u00fcrl\u00fc\u011f\u00fc yapan arkada\u015flar\u0131mdan biriyle konu\u015fuyordum. Tart\u0131\u015fmam\u0131z \u00fczerine, ma\u011fazalar zincirinin bayram mevsimi (Kurban&Ramazan Bayramlar\u0131) ba\u015flamadan \u00f6nce yapmas\u0131 gereken haz\u0131rl\u0131klar hakk\u0131nda konu\u015fmaya ba\u015flad\u0131k.\n\nBana, hangi \u00fcr\u00fcn\u00fcn kolay sataca\u011f\u0131n\u0131 ve hangilerinin satmayaca\u011f\u0131n\u0131 tahmin etmenin ne kadar \u00f6nemli oldu\u011funu s\u00f6yledi. K\u00f6t\u00fc bir karar, m\u00fc\u015fterilerinizi rakip ma\u011fazalara y\u00f6nlendirebilir ve yeni \u00fcr\u00fcnler aramaya b\u0131rakabilir. Buradaki zorluk bitmiyor - \u00fcr\u00fcnlerin farkl\u0131 konumlardaki ma\u011fazalar i\u00e7in ve farkl\u0131 t\u00fcketim tekniklerine sahip t\u00fcketiciler i\u00e7in \u00e7e\u015fitli kategorilerdeki sat\u0131\u015flar\u0131n\u0131 tahmin etmeniz gerekiyor.\n\nArkada\u015f\u0131m zorlu\u011fu anlat\u0131rken, i\u00e7imdeki veri bilimcisi g\u00fcl\u00fcmsemeye ba\u015flad\u0131! Niye ya dedim? Bug\u00fcnk\u00fc \u00e7al\u0131\u015fmam\u0131zda, size regresyon modelleri ve yukar\u0131da belirtilen sorunlar\u0131n tahmin problemlerini \u00e7\u00f6zmek i\u00e7in nas\u0131l kullan\u0131labilece\u011fi hakk\u0131nda bilmeniz gereken her \u015feyi anlamaya \u00e7al\u0131\u015faca\u011f\u0131z.\n\nK\u00fc\u00e7\u00fck bir beyin jimnasti\u011fi... Bir ma\u011fazan\u0131n sat\u0131\u015f\u0131n\u0131n etkilenece\u011fini, d\u00fc\u015f\u00fcnebildi\u011finiz t\u00fcm fakt\u00f6rleri listelemek i\u00e7in bir dakikan\u0131z\u0131 ay\u0131r\u0131n. Her fakt\u00f6r i\u00e7in, bu fakt\u00f6r\u00fcn \u00e7e\u015fitli \u00fcr\u00fcnlerin sat\u0131\u015f\u0131n\u0131 neden ve nas\u0131l etkileyece\u011fi konusunda bir hipotez olu\u015fturun. \u00d6rne\u011fin, \u00fcr\u00fcnlerin sat\u0131\u015f\u0131n\u0131n ma\u011fazan\u0131n konumuna ba\u011fl\u0131 olmas\u0131n\u0131 d\u00fc\u015f\u00fcn\u00fcn, \u00e7\u00fcnk\u00fc her b\u00f6lgedeki yerel halk farkl\u0131 ya\u015fam tarzlar\u0131na sahip olacak. Etiler'de bir marketin sataca\u011f\u0131 bayram \u00e7ikolatas\u0131 miktar\u0131 ile Esenyurt'taki benzer ma\u011fazan\u0131n \u00e7ikolata sat\u0131\u015f  t\u00fcr\u00fc ve miktar\u0131 ayn\u0131 olmayacakt\u0131r.\n\nBenzer \u015fekilde akl\u0131n\u0131za gelebilecek t\u00fcm olas\u0131 fakt\u00f6rleri listeleyin.\n\nMa\u011fazan\u0131z\u0131n konumu, \u00fcr\u00fcnlerin mevcudiyeti, ma\u011fazan\u0131n b\u00fcy\u00fckl\u00fc\u011f\u00fc, \u00fcr\u00fcnle ilgili belirli \u00f6zellik, bir \u00fcr\u00fcn i\u00e7in yap\u0131lan reklamlar, ma\u011fazada ki yerle\u015fimi, sat\u0131\u015flar\u0131n\u0131z\u0131n ba\u011fl\u0131 olaca\u011f\u0131 baz\u0131 \u00f6zellikler olabilir.\n\nNe kadar farkl\u0131 \u015feyler d\u00fc\u015f\u00fcnebildiniz? 15'ten azsa, daha fazla zaman ay\u0131r\u0131n ve tekrar d\u00fc\u015f\u00fcn\u00fcn! Bu problem \u00fczerinde \u00e7al\u0131\u015fan tecr\u00fcbeli bir veri bilimcisi i\u00e7in muhtemelen onlarca ve y\u00fczlerce farkl\u0131 durum d\u00fc\u015f\u00fcnebilir.\n\nBu d\u00fc\u015f\u00fcnce akl\u0131mda, size b\u00f6yle bir veri seti verelim - \"The Big Mart Sales\". Veri setinde, bir zincirin \u00c7oklu \u00e7\u0131k\u0131\u015flar\u0131 i\u00e7in Sat\u0131\u015f bilgisine sahip sat\u0131\u015flar\u0131m\u0131z var.\n\n- Konu Ba\u015fl\u0131klar\u0131m\u0131z.\n- 1- Tahmin i\u00e7in basit bir model olu\u015fturma (Simple models for Prediction)\n- 2- Do\u011frusal Regresyon(Linear Regression)\n- 3- En iyi \u00e7izgi uygumu (The Line of Best Fit)\n- 4- Dereceli Azalma(Gradient Descent)\n- 5- Tahmin i\u00e7in Do\u011frusal regrasyon algoritmas\u0131n\u0131n kullan\u0131lmas\u0131(Using Linear Regression for prediction)\n- 6- Modelin olu\u015fturulmas\u0131 ve R kare &Ayarlanm\u0131\u015f R kare de\u011ferlerinin bulunmas\u0131(Evaluate your Model \u2013 R square and Adjusted R squared)\n- 7- Veri setinde bulunan t\u00fcm ba\u011f\u0131ms\u0131z de\u011fi\u015fkenleri kullanarak tahmin yapma(Using all the features for prediction)\n- 8- Polynom Regresyonu(Polynomial Regression)\n- 9-  Bias and Variance\n- 10- Regularization\n- 11- Ridge Regression\n- 12- Lasso Regression\n- 13- ElasticNet Regression\n- 14- Regularizasyon t\u00fcrleri (Types of Regularization Techniques)","f31f27f8":"\n**#ValueError: Input contains NaN, infinity or a value too large for dtype('float64').** \u00d6\u011fe a\u011f\u0131rl\u0131klar\u0131(Item_weight) s\u00fctununun baz\u0131 eksik de\u011ferleri oldu\u011fundan, bir hata \u00fcretiyor. \u00d6yleyse di\u011fer bo\u015f olmayan giri\u015flerin ortalamas\u0131n\u0131 alarak bo\u015f olan alanlara ortalama de\u011feri atayal\u0131m.","42e591fb":"G\u00f6rd\u00fc\u011f\u00fcm\u00fcz gibi, modelimizin hem(mse) de\u011feri hem de R-karesinin de\u011feri artt\u0131r\u0131ld\u0131. Bu nedenle, Lasso modeli hem do\u011frusal hem de Ridge'den daha iyi tahmin ediyor oldu\u011funu g\u00f6rebiliyoruz.\n\nYine alfa de\u011ferini de\u011fi\u015ftirelim ve katsay\u0131lar\u0131 nas\u0131l etkiledi\u011fini g\u00f6relim.\n            ![lasso0.051.png](attachment:lasso0.051.png)\n\n            ![lasso0.51.png](attachment:lasso0.51.png)\n            \n            \nBu nedenle, k\u00fc\u00e7\u00fck alfa de\u011ferlerinde bile, katsay\u0131lar\u0131n b\u00fcy\u00fckl\u00fc\u011f\u00fcn\u00fcn \u00e7ok azald\u0131\u011f\u0131n\u0131 g\u00f6rebiliriz. Grafiklere bakarak, Ridge ile Lasso fonksiyonlar\u0131 aras\u0131nda bir fark bulabilir misiniz?\n\nAlfa de\u011ferini art\u0131rd\u0131k\u00e7a katsay\u0131lar\u0131n s\u0131f\u0131ra yakla\u015ft\u0131\u011f\u0131n\u0131 g\u00f6r\u00fcyoruz, ancak Lasso fonksiyonunda, daha k\u00fc\u00e7\u00fck alfalarda bile, katsay\u0131lar\u0131m\u0131z mutlak s\u0131f\u0131ra d\u00fc\u015f\u00fcyor. Bu nedenle, Lasso sadece baz\u0131 \u00f6zellikleri se\u00e7erken di\u011ferlerinin katsay\u0131lar\u0131n\u0131 s\u0131f\u0131ra indirir. Bu \u00f6zellik, \u00f6zellik se\u00e7imi olarak bilinir ve Ridge fonksiyonunda yoktur.\n\n    - Lasso regresyonunun ard\u0131ndaki matematik, Ridge'e benzedi\u011fini g\u00f6rebilirsinis, sadece fark\u0131n theta(\u0398) kareleri eklemek yerine, \u0398 mutlak de\u011ferini ekledi\u011fimizi fark ediyoruz.\n    \n    ![lasso_math_formul.png](attachment:lasso_math_formul.png)\n\nBurada da \u03bb, de\u011feri Lasso fonksiyonunda alfaya e\u015fit olan bir hipermetredir.\n\nLasso i\u00e7in \u00d6nemli noktalar:\n\n  - L1 d\u00fczenlile\u015ftirme tekni\u011fini kullan\u0131r (bu makalenin ilerleyen k\u0131s\u0131mlar\u0131nda ele al\u0131nacakt\u0131r)\n  - Genellikle daha fazla say\u0131da \u00f6zelli\u011fe sahip oldu\u011fumuzda kullan\u0131l\u0131r, \u00e7\u00fcnk\u00fc otomatik olarak \u00f6zellik se\u00e7imi yapar.\nArt\u0131k Ridge ve Lasso regresyonu hakk\u0131nda temel bir anlay\u0131\u015fa sahip oldu\u011fumuza g\u00f6re, geni\u015f bir veri setine sahip oldu\u011fumuz bir \u00f6rne\u011fi d\u00fc\u015f\u00fcnelim, 10,000 \u00f6zelli\u011fe sahip oldu\u011funu varsayal\u0131m. Ve baz\u0131 ba\u011f\u0131ms\u0131z \u00f6zelliklerin di\u011fer ba\u011f\u0131ms\u0131z \u00f6zellikler ile ili\u015fkili oldu\u011funu biliyoruz. \u00d6yleyse, hangi regresyonun kullan\u0131ld\u0131\u011f\u0131n\u0131 d\u00fc\u015f\u00fcn\u00fcn, Rigde mi, Lasso mu?\n\nTek tek tart\u0131\u015fal\u0131m. E\u011fer Ridge regresyonu uygularsak, t\u00fcm \u00f6zelliklerini koruyacak, katsay\u0131lar\u0131 daraltacakt\u0131r. Ancak sorun \u015fu ki, model 10.000 \u00f6zellik oldu\u011fundan hala karma\u015f\u0131k kalacakt\u0131r, bu nedenle d\u00fc\u015f\u00fck model performans\u0131na neden olabilir.\n\nRidge yerine lasso regresyonu uygularsak, Lasso regresyonu ile ilgili temel problem, korelasyonlu de\u011fi\u015fkenlere sahip oldu\u011fumuzda, sadece bir de\u011fi\u015fkeni korur ve di\u011fer korelasyonlu de\u011fi\u015fkenleri s\u0131f\u0131ra ayarlar. Bu muhtemelen modelimizde daha d\u00fc\u015f\u00fck do\u011frulukla sonu\u00e7lanan bir miktar bilgi kayb\u0131na yol a\u00e7acakt\u0131r.\n\n\u00d6yleyse bu sorunun \u00e7\u00f6z\u00fcm\u00fc nedir? Asl\u0131nda, temel olarak bir Ridge ve Lasso regresyonunun bir melezi olan *elastik net* regresyon olarak bilinen ba\u015fka bir regresyon t\u00fcr\u00fcne sahibiz. \u00d6yleyse gelin elastik net fonksiyonunu anlamaya \u00e7al\u0131\u015fal\u0131m.","eb99ea3c":"(MRP)MaterialRequirementPlanning'nin y\u00fcksek bir katsay\u0131ya sahip oldu\u011funu g\u00f6r\u00fcyoruz, yani fiyat\u0131 y\u00fcksek olan \u00fcr\u00fcnlerin dat\u0131\u015flar\u0131 daha iyi diyebiliriz.\n\n## 6. Modelinizi De\u011ferlendirme - R kare ve ayarlanm\u0131\u015f R-kare\nEvet \u015fimdi sizce model ne kadar do\u011fru? Bunu kontrol edebilmemiz i\u00e7in herhangi bir de\u011ferlendirme metri\u011fimiz var m\u0131? Asl\u0131nda R-Square olarak bilinen bir metri\u011fimiz var. \u015eimdi R-Square metri\u011fini inceleyelim.\n\nR-Square:\nY (ba\u011f\u0131ml\u0131 de\u011fi\u015fken) i\u00e7indeki toplam de\u011fi\u015fimin ne kadar\u0131n\u0131n X (ba\u011f\u0131ms\u0131z de\u011fi\u015fken) taraf\u0131ndan a\u00e7\u0131klanaca\u011f\u0131n\u0131 belirler. Matematiksel olarak \u015fu \u015fekilde yaz\u0131labilir: ![R_Squared.png](attachment:R_Squared.png)\n\nR-karenin de\u011feri her zaman 0 ile 1 aras\u0131ndad\u0131r, burada 0, modelin hedef de\u011fi\u015fkenindeki(Y) herhangi bir de\u011fi\u015fkenli\u011fi a\u00e7\u0131klamad\u0131\u011f\u0131 anlam\u0131na gelir ve 1 de\u011feri ise model hedef de\u011fi\u015fkeni tam olarak a\u00e7\u0131klad\u0131\u011f\u0131 anlam\u0131na gelmektedir.\n\u015eimdi gelin R-Square'i kontrol edelim,","a1fc3c1b":"## 1. Tahmin i\u00e7in basit modeller Olu\u015fturma\nBa\u015flamak i\u00e7in birka\u00e7 basit yol kullanarak tahminlerde bulunmaya \u00e7al\u0131\u015fal\u0131m. Sizce bir mal\u0131n sat\u0131\u015f\u0131n\u0131 tahmin etmenin en basit yolu ne olurdu?\n\n  **-Model 1 - Ortalama sat\u0131\u015flar:**\nHerhangi bir makine \u00f6\u011frenmesi bilgisi olmadan bile, bir \u00f6\u011fenin sat\u0131\u015f\u0131n\u0131 tahmin etmek zorunda kal\u0131rsan\u0131z tahmin olarak son birka\u00e7 g\u00fcn\u00fcn ortalamas\u0131 olaca\u011f\u0131n\u0131 s\u00f6yleyebiliriz. (ay\/hafta)\n\nBa\u015flamak i\u00e7in iyi bir d\u00fc\u015f\u00fcnce ama ayn\u0131 zamanda bir soruyu da beraberinde getiriyor - bu model ne kadar iyi?\n\nBu durum modelimizin ne kadar iyi oldu\u011funu de\u011ferlendirebilece\u011fimiz \u00e7e\u015fitli yollar oldu\u011funu ortaya koyuyor. En yayg\u0131n yol Ortalama Karesel Hata(mse). \u015eimdi gelin modelimizi nas\u0131l \u00f6l\u00e7ece\u011fimizi anlayal\u0131m.\n\n*Hatal\u0131 Tahmin Sak\u0131ncalar\u0131*\nBir modelin ne kadar iyi oldu\u011funu de\u011ferlendirmek i\u00e7in, yanl\u0131\u015f tahminlerin etkisini anlayal\u0131m. Sat\u0131\u015flar\u0131 olabilece\u011finden daha y\u00fcksek tahmin edersek, ma\u011faza fazla miktarda para harcayarak gereksiz stok d\u00fczenlemelerine gidebilir. Di\u011fer taraftan, \u00e7ok d\u00fc\u015f\u00fck tahmin edersek sat\u0131\u015f f\u0131rsatlar\u0131n\u0131 kaybedebiliriz.\n\nDolay\u0131s\u0131yla, hatay\u0131 hesaplaman\u0131n en basit yolu, \u00f6ng\u00f6r\u00fclen(predicted) ve ger\u00e7ekle\u015fen(actual) de\u011ferlerdeki fark\u0131 hesaplamak olacakt\u0131r. Ancak, basit\u00e7e onlar\u0131 eklersek, iptal ederler, bu y\u00fczden eklemeden \u00f6nce bu hatalar\u0131n karelerini al\u0131r\u0131z. Veri puanlar\u0131n\u0131n say\u0131s\u0131na ba\u011fl\u0131 olmamas\u0131 gerekti\u011finden, ortalama bir hatay\u0131 hesaplamak i\u00e7in onlar\u0131 veri noktalar\u0131n\u0131n say\u0131s\u0131na b\u00f6lmemiz gerekiyor. \n![snip1.png](attachment:snip1.png)\n\nBu ortalama kare hatas\u0131 olarak bilinir.\n\n\u0130\u015fte e1, e2\u2026. (errors) ger\u00e7ek ve \u00f6ng\u00f6r\u00fclen de\u011ferler aras\u0131ndaki farkt\u0131r.\n\n \n\n\u00d6yleyse, ilk modelimizde ortalama kare hatas\u0131 ne olurdu bakal\u0131m ? T\u00fcm veri noktalar\u0131 i\u00e7in ortalamay\u0131 tahmin ederken, ortalama bir kare hata elde ediyoruz = 29,111,799. G\u00f6r\u00fcld\u00fc\u011f\u00fc kadar\u0131 ile \u00e7ok b\u00fcy\u00fck bir hataya benziyor. Ortalama de\u011feri basit\u00e7e tahmin etmek o kadar da kolay olmayabilir.\n\nHatay\u0131 azaltmak i\u00e7in bir \u015feyler yapabilecek miyiz g\u00f6relim.\n\n**Model 2 - B\u00f6lgeye G\u00f6re Ortalama Sat\u0131\u015f:** Bir \u00fcr\u00fcn\u00fcn sat\u0131\u015f\u0131nda yerin hayati bir rol oynad\u0131\u011f\u0131n\u0131 biliyoruz. \u00d6rne\u011fin, \u0130stanbul'da otomobil sat\u0131\u015flar\u0131n\u0131n Nev\u015fehir'deki sat\u0131\u015flardan \u00e7ok daha y\u00fcksek olaca\u011f\u0131n\u0131 s\u00f6yleyebiliriz. Bu nedenle, \u2018Outlet_Location_Type\u2019 s\u00fctununun verilerini kullanal\u0131m.\nDolay\u0131s\u0131yla, her bir yer(b\u00f6lge) t\u00fcr\u00fc i\u00e7in ortalama sat\u0131\u015flar\u0131 hesaplayal\u0131m ve buna g\u00f6re tahmin yapal\u0131m.\n\nAyn\u0131s\u0131n\u0131 tahmin ederken, \u00f6nceki durumumuzdan daha az olan mse = 28,775,386 al\u0131yoruz. B\u00f6ylece karakteristik bir konum kullanarak hatay\u0131 azaltt\u0131\u011f\u0131m\u0131z\u0131 fark edebiliriz ama \u00e7okta b\u00fcy\u00fck bir azalma olmad\u0131.\n\n\u015eimdi, sat\u0131\u015flar\u0131n ba\u011fl\u0131 olaca\u011f\u0131 birden fazla \u00f6zellik varsa ki vard\u0131r. Bu bilgileri kullanarak sat\u0131\u015flar\u0131 nas\u0131l tahmin edebilece\u011fiz? Bu noktada Do\u011frusal gerileme(Linear Regression) bizim yard\u0131m\u0131m\u0131za yeti\u015fiyor.\n\n## 2. Do\u011frusal Regresyon\nDo\u011frusal regresyon, yorday\u0131c\u0131(tahminsel-predictive) modelleme i\u00e7in en basit ve en yayg\u0131n kullan\u0131lan istatistiksel tekniktir. Temel olarak bize, bizim \u00f6zelliklerimizin [bu \u00e7al\u0131\u015fmada sat\u0131\u015flar] ba\u011f\u0131ml\u0131 oldu\u011fu ba\u011f\u0131ms\u0131z de\u011fi\u015fkenler olarak sahip oldu\u011fumuz bir denklem verir.\n\nPeki denklem neye benziyor? Do\u011frusal regresyon denklemi \u015f\u00f6yle g\u00f6r\u00fcn\u00fcr: \n\nBurada, ba\u011f\u0131ml\u0131 de\u011fi\u015fkenimiz (Sat\u0131\u015f) olarak Y sahibiz, X\u2019ler ba\u011f\u0131ms\u0131z de\u011fi\u015fkenlerdir ve di\u011fer Theta de\u011ferleri ise a\u011f\u0131rl\u0131k katsay\u0131lar\u0131d\u0131r. Katsay\u0131lar temel olarak \u00f6zelliklerine, \u00f6nemlerine g\u00f6re verilen a\u011f\u0131rl\u0131klard\u0131r. \u00d6rne\u011fin, bir \u00f6\u011fenin sat\u0131\u015f\u0131n\u0131n, ma\u011fazan\u0131n b\u00fcy\u00fckl\u00fc\u011f\u00fcne k\u0131yasla konum t\u00fcr\u00fcne daha fazla ba\u011f\u0131ml\u0131 olaca\u011f\u0131na inan\u0131rsak, bu, birinci \u00f6neme sahip bir \u015fehirdeki sat\u0131\u015flar\u0131n 3.\u00f6neme sahip bir \u015fehirdekinden daha k\u00fc\u00e7\u00fck bir sat\u0131\u015f olmas\u0131na ra\u011fmen a\u011f\u0131rl\u0131\u011f\u0131n\u0131n daha fazla olaca\u011f\u0131 anlam\u0131na gelir. Bu nedenle, konum t\u00fcr\u00fc katsay\u0131s\u0131 depo boyutundan daha fazla olacakt\u0131r.\n\n\u00d6yleyse, \u00f6ncelikle sadece bir \u00f6zellik olan(single variate), yani sadece bir ba\u011f\u0131ms\u0131z de\u011fi\u015fkeni olan lineer regresyonu anlamaya \u00e7al\u0131\u015fal\u0131m. Dolay\u0131s\u0131yla denklemimiz \u015f\u00f6yle olur, ![linear_regresyon_formul_singleVariate.png](attachment:linear_regresyon_formul_singleVariate.png)\n\nBu denkleme, basit do\u011frusal regresyon denlemi denir. \u201c\u03980\u201d do\u011frunun Y eksenini kesti\u011fi noktay\u0131 ve , \u2018\u03981' \u00e7izginin e\u011fimini temsil eder. Sat\u0131\u015f ve MRP aras\u0131ndaki a\u015fa\u011f\u0131daki grafi\u011fe g\u00f6z atal\u0131m.\n\n![satis_MRP_Grafik.png](attachment:satis_MRP_Grafik.png)\n\n\u015ea\u015f\u0131rt\u0131c\u0131 bir \u015fekilde, bir \u00fcr\u00fcn\u00fcn sat\u0131\u015flar\u0131n\u0131n MRP'deki art\u0131\u015fla artt\u0131\u011f\u0131n\u0131 g\u00f6rebiliriz. Bu nedenle noktal\u0131 k\u0131rm\u0131z\u0131 \u00e7izgi, regresyon \u00e7izgimizi veya en uygun \u00e7izgiyi temsil eder. Evet i\u015fte arad\u0131\u011f\u0131m\u0131z soru \u015fu ki, biz bu \u00e7izgiyi nas\u0131l bulaca\u011f\u0131z?\n\n## 3. En Uygun \u00c7izgiyi Bulma\nA\u015fa\u011f\u0131da g\u00f6rebilece\u011finiz gibi, Sat\u0131\u015flar\u0131 MRP'lerine g\u00f6re tahmin etmek i\u00e7in kullan\u0131labilecek bir\u00e7ok sat\u0131r olabilir. Peki, en uygun hatt\u0131 veya regresyon hatt\u0131n\u0131 nas\u0131l se\u00e7eriz?\n\n![sales_MRP_line_fit.png](attachment:sales_MRP_line_fit.png)\n\nEn uygun \u00e7izginin as\u0131l amac\u0131, \u00f6ng\u00f6r\u00fclen de\u011ferlerimizin ger\u00e7ek de\u011ferlerimizden \u00e7ok uzak olan de\u011ferleri tahmin etmenin bir anlam\u0131 olmad\u0131\u011f\u0131 i\u00e7in ger\u00e7ek de\u011ferlerimizle ya da g\u00f6zlenen de\u011ferlerle daha yak\u0131n olmas\u0131 gerekti\u011fidir. Ba\u015fka bir deyi\u015fle, bizim taraf\u0131m\u0131zdan \u00f6ng\u00f6r\u00fclen de\u011ferler ile g\u00f6zlemlenen de\u011ferler aras\u0131ndaki fark\u0131 asgariye indirgeme e\u011filimidir. Hatan\u0131n grafiksel g\u00f6sterimi a\u015fa\u011f\u0131da g\u00f6sterilmi\u015ftir. Bu hatalar art\u0131klar(residuals) olarak da adland\u0131r\u0131l\u0131r. Art\u0131klar, \u00f6ng\u00f6r\u00fclen(predicted) ile ger\u00e7ek(actual) de\u011fer aras\u0131ndaki fark\u0131 g\u00f6steren dikey \u00e7izgilerle belirtilir.\n\n![residuals.png](attachment:residuals.png)\n\nTamam, \u015fimdi as\u0131l amac\u0131m\u0131z\u0131n hatay\u0131 bulmak ve onu en aza indirgemek oldu\u011funu biliyoruz. Fakat ondan \u00f6nce, hatay\u0131 hesaplamak i\u00e7in ilk k\u0131s\u0131m ile nas\u0131l ba\u015f edece\u011fimizi d\u00fc\u015f\u00fcnelim. Hatan\u0131n bizim taraf\u0131m\u0131zdan tahmin edilen de\u011fer ile g\u00f6zlenen de\u011fer aras\u0131ndaki fark oldu\u011funu zaten biliyoruz. Hatay\u0131 hesaplayabilece\u011fimiz \u00fc\u00e7 yolu ele alal\u0131m:\n\n    - **Art\u0131klar\u0131n toplam\u0131 (\u2211 (Y - s (X)))** - Olumlu ve olumsuz hatalar\u0131n iptal edilmesine neden olabilir.\n    - **Art\u0131klar\u0131n mutlak de\u011ferinin toplam\u0131 (\u2211 | Y-h (X) |)** - mutlak de\u011fer hatalar\u0131n iptalini \u00f6nler\n    - **Art\u0131klar\u0131n karelerinin toplam\u0131 (\u2211 (Yh (X)) 2)** - Uygulamada \u00e7o\u011funlukla kullan\u0131lan y\u00f6ntemdir, \u00e7\u00fcnk\u00fc burada daha k\u00fc\u00e7\u00fck hatayla kar\u015f\u0131la\u015ft\u0131r\u0131ld\u0131\u011f\u0131nda daha y\u00fcksek hata de\u011ferini daha fazla cezaland\u0131r\u0131r\u0131z, burada b\u00fcy\u00fck hatalar aras\u0131nda \u00f6nemli bir fark vard\u0131r. Ve en iyi \u00e7izgiyi ay\u0131rt etmeyi ve se\u00e7meyi kolayla\u015ft\u0131ran k\u00fc\u00e7\u00fck hatalar y\u00f6ntemi ile belirleyebiliriz.\n    \nBu nedenle, bu art\u0131klar\u0131n karelerinin toplam\u0131 \u015fu \u015fekildedir:\n\n![SSresiduals_1.png](attachment:SSresiduals_1.png)\n\nburada, h (x) bizim taraf\u0131m\u0131zdan tahmin edilen de\u011ferdir, h (x) = \u03981 * x + \u03980, y ger\u00e7ek de\u011ferlerdir ve m ise e\u011fitim setindeki sat\u0131rlar\u0131n say\u0131s\u0131d\u0131r(yani g\u00f6zlemlerin say\u0131s\u0131).\n\n**Maliyet \u0130\u015flevi(The Cost Function)**\n\u015eimdi, sat\u0131\u015flar\u0131n daha y\u00fcksek olaca\u011f\u0131n\u0131 \u00f6ng\u00f6rd\u00fck ve d\u00fckkan\u0131n boyutunu art\u0131rd\u0131k. Ancak d\u00fckkan\u0131n b\u00fcy\u00fckl\u00fc\u011f\u00fcn\u00fcn artmas\u0131na ra\u011fmen, bu ma\u011fazadaki sat\u0131\u015flar\u0131n o kadar artmad\u0131\u011f\u0131n\u0131 fark ettik, \u015fimdi ne yapmam\u0131z gerekir? Bu durumda d\u00fckkan\u0131n b\u00fcy\u00fckl\u00fc\u011f\u00fcn\u00fcn artt\u0131r\u0131lmas\u0131nda uygulanan maliyetin, bize olumsuz sonu\u00e7lar verdi\u011fini g\u00f6rd\u00fck \u015fimdi bu olumsuz sonu\u00e7lar\u0131 nas\u0131l de\u011ferlendirmeliyiz?\n\n\u015eimdi sonucumuz olumsuz oldu\u011fundan dolay\u0131 bu maliyetleri minimize etmemiz gerekiyor. Bu nedenle, temel olarak modelin hatas\u0131n\u0131 tan\u0131mlamak ve \u00f6l\u00e7mek i\u00e7in kullan\u0131lan bir maliyet fonksiyonunu tan\u0131t\u0131yoruz.\n\n![cost_function.png](attachment:cost_function.png)\n\nE\u011fer bu denkleme dikkatlice bakarsan\u0131z, denklem kare hatalar\u0131n toplam\u0131na benzer, burada matemati\u011fi kolayla\u015ft\u0131rmak i\u00e7in form\u00fcl 1 \/ 2m \u00e7arpan\u0131 ile \u00e7arp\u0131l\u0131r.\n\nDolay\u0131s\u0131yla, \u00f6ng\u00f6r\u00fcm\u00fcz\u00fc iyile\u015ftirmek i\u00e7in maliyet i\u015flevini(cost function) en aza indirmemiz gerekiyor. Bu ama\u00e7la gradyan ini\u015f(Gradient Descent) algoritmas\u0131n\u0131 kullan\u0131yoruz. Haydi bakal\u0131m \u015fimdi de Gradient Descent(dereceli ini\u015f) algoritmas\u0131n\u0131n nas\u0131l \u00e7al\u0131\u015ft\u0131\u011f\u0131n\u0131 anlayal\u0131m, \u00e7\u00fcnk\u00fc i\u015flemlerimizde a\u00e7\u0131k bir nokta b\u0131rakmadan yap\u0131lanlar\u0131 tam olarak anlamal\u0131y\u0131zki bilinmeyen hi\u00e7 bir nokta kalmas\u0131n ve konuya hakimiyetimiz tam olsun.\n\n## 4. Dereceli \u0130ni\u015f(Gradient Descent)\nBir \u00f6rnek ile ele alal\u0131m, elimizde bir deknlem var ve bu denklemin minimum de\u011ferini bulmam\u0131z gerekiyor,\n\nY = 5x + 4x ^ 2. Matematikte, bu denklemin t\u00fcrevini x'e g\u00f6re al\u0131p onu s\u0131f\u0131ra e\u015fitlersek bu denklemin minumum de\u011feriniz buluruz. Bu nedenle, bu de\u011feri de\u011fi\u015ftirmek bize bu denklemin minimum de\u011ferini verebilir.\n\nDereceli ini\u015f(gradient descent) benzer \u015fekilde \u00e7al\u0131\u015f\u0131r. Maliyet fonksiyonunun minimum olaca\u011f\u0131 bir nokta bulmak i\u00e7in tekrarl\u0131 olarak g\u00fcncellenir. Dereceli ini\u015f fonksiyonunu derinlemesine incelemek istiyorsan\u0131z, [bu makaleyi](https:\/\/www.analyticsvidhya.com\/blog\/2017\/03\/introduction-to-gradient-descent-algorithm-along-its-variants\/) g\u00f6zden ge\u00e7irmenizi tavsiye ederim.\n\n## 5. Tahmin i\u00e7in Do\u011frusal Regresyon Kullan\u0131m\u0131\n\u015eimdi, b\u00fcy\u00fck mart sat\u0131\u015f(big mart sales) sorunumuz i\u00e7in Sat\u0131\u015flar\u0131 tahmin etmek amac\u0131yla Do\u011frusal Regresyon kullanmaya ba\u015flayal\u0131m.\n\n **Model 3 - Do\u011frusal Regresyona Giri\u015f:** \u00d6nceki durumdan \u00e7\u0131kard\u0131\u011f\u0131m\u0131z sonu\u00e7lara g\u00f6re, e\u011fer do\u011fru \u00f6zellikleri kullan\u0131rsak tahmin do\u011frulu\u011fumuzu art\u0131raca\u011f\u0131n\u0131 biliyoruz bu da bize g\u00f6steriyorki yapaca\u011f\u0131m\u0131z i\u015fte e\u011fer yeterli alan bilgisine sahip isek kuraca\u011f\u0131m\u0131z modelde kullanaca\u011f\u0131m\u0131z \u00f6zellikleri se\u00e7mekte daha kolay olacakt\u0131r. \u015eimdi, sat\u0131\u015flar\u0131 tahmin etmek i\u00e7in MRP ve Ma\u011faza kurulu\u015f y\u0131l\u0131 olarak iki \u00f6zellik kullanal\u0131m.\n\u015eimdi, sadece bu iki \u00f6zelli\u011fi dikkate alarak python'da do\u011frusal bir regresyon modeli olu\u015ftural\u0131m.","cfcd9ca8":"A\u00e7\u0131k\u00e7as\u0131, hem mse hem de R-kare'de b\u00fcy\u00fck bir geli\u015fme oldu\u011funu g\u00f6rebiliyoruz, bu da modelimizin \u015fimdi ger\u00e7ek de\u011ferlere daha yak\u0131n de\u011ferler tahmin edebilece\u011fi anlam\u0131na geliyor.\n\n### Modeliniz i\u00e7in do\u011fru \u00f6zellikleri se\u00e7me\nY\u00fcksek boyutlu bir veri k\u00fcmesine sahip oldu\u011fumuzda, baz\u0131 de\u011fi\u015fkenler gereksiz bilgi verebilece\u011finden t\u00fcm de\u011fi\u015fkenleri kullanmak olduk\u00e7a verimsiz olacakt\u0131r. Ba\u011f\u0131ml\u0131 de\u011fi\u015fkeni iyi a\u00e7\u0131klayabilmemizin yan\u0131 s\u0131ra bize do\u011fru bir model veren do\u011fru de\u011fi\u015fken setini se\u00e7memizde \u00f6nem arzetmektedir. Model i\u00e7in do\u011fru de\u011fi\u015fken k\u00fcmesini se\u00e7menin birden fazla yolu vard\u0131r. Bunlardan ilki alan bilgisi olacakt\u0131r. \u00d6rne\u011fin, sat\u0131\u015flar\u0131 \u00f6ng\u00f6r\u00fcrken, pazarlama \u00e7abalar\u0131n\u0131n sat\u0131\u015flar\u0131 olumlu y\u00f6nde etkilemesi gerekti\u011fini ve modelinizde \u00f6nemli bir \u00f6zellik oldu\u011funu biliyoruz. Ayr\u0131ca, se\u00e7ti\u011fimiz de\u011fi\u015fkenlerin kendi aralar\u0131nda ili\u015fkilendirilmemesi gerekti\u011fine de dikkat etmeliyiz.\n\nDe\u011fi\u015fkenleri manuel olarak se\u00e7mek yerine, ileri veya geri se\u00e7imi kullanarak bu i\u015flemi otomatikle\u015ftirebiliriz.\n\n    - \u0130leriye do\u011fru se\u00e7im, modeldeki en \u00f6nemli yorday\u0131c\u0131yla(predictor) ba\u015flar ve her ad\u0131m i\u00e7in yeni de\u011fi\u015fken ekler.\n    - Geriye do\u011fru ekleme, modeldeki t\u00fcm \u00f6ng\u00f6r\u00fcc\u00fclerle(predictors) ba\u015flar ve her ad\u0131m i\u00e7in en az anlaml\u0131 de\u011fi\u015fkeni kald\u0131r\u0131r. Kriterler, R-kare, t-stat vb. Gibi herhangi bir istatistiksel \u00f6l\u00e7\u00fcme ayarlanabilir.\n\n### Regresyon Alanlar\u0131n\u0131n Yorumlanmas\u0131\nA\u015fa\u011f\u0131daki grafikte huniye benzer bir \u015fekil g\u00f6r\u00fcyoruz. Hata terimlerinde sabit olmayan varyans\u0131n varl\u0131\u011f\u0131, heteroskedastisite(de\u011fi\u015fen varyans sorunu) ile sonu\u00e7lan\u0131r. Hata terimlerinin (art\u0131klar\u0131n) varyans\u0131n\u0131n sabit olmad\u0131\u011f\u0131n\u0131 a\u00e7\u0131k\u00e7a g\u00f6rebiliriz. Genel olarak, sabit olmayan varyans, ayk\u0131r\u0131 de\u011ferlerin veya a\u015f\u0131r\u0131 kald\u0131ra\u00e7 de\u011ferlerinin varl\u0131\u011f\u0131nda ortaya \u00e7\u0131kar. Bu de\u011ferler \u00e7ok fazla a\u011f\u0131rl\u0131k al\u0131r, bu nedenle modelin performans\u0131n\u0131 orant\u0131s\u0131z \u015fekilde etkiler. Bu durum ortaya \u00e7\u0131kt\u0131\u011f\u0131nda, \u00f6rnek d\u0131\u015f\u0131na \u00e7\u0131kma \u00f6ng\u00f6r\u00fcs\u00fcn\u00fcn g\u00fcven aral\u0131\u011f\u0131 ger\u00e7ek d\u0131\u015f\u0131 geni\u015f ya da dar olma e\u011filimindedir.\n\nArt\u0131k(residual) ve uygulanan de\u011ferleri \u015fekle bakarak kolayca kontrol edebiliriz. E\u011fer heteroskedastisite mevcutsa, \u015fekil a\u015fa\u011f\u0131 da g\u00f6sterildi\u011fi gibi bir huni \u015fekli modeli sergileyecektir. Bu, model taraf\u0131ndan yakalanmayan verilerdeki do\u011frusal olmayan i\u015faretlerini g\u00f6sterir. Varsay\u0131mlar\u0131n ayr\u0131nt\u0131l\u0131 bir \u015fekilde anla\u015f\u0131lmas\u0131 ve regresyon parsellerinin yorumlanmas\u0131 i\u00e7in [bu makaleyi](https:\/\/www.analyticsvidhya.com\/blog\/2016\/07\/deeper-regression-analysis-assumptions-plots-solutions\/) g\u00f6zden ge\u00e7irmenizi \u015fiddetle tavsiye ederim.\n\n![residual-plot.png](attachment:residual-plot.png)\n\nBu do\u011frusal olmayan etkileri yakalamak i\u00e7in, polinom regresyonu olarak bilinen ba\u015fka bir regresyon tipine sahibiz. \u00d6yleyse \u015fimdi Polinom regrasyonu anlamaya \u00e7al\u0131\u015fal\u0131m.\n\n### Regresyon Alanlar\u0131n\u0131n Yorumlanmas\u0131\u00b6\nArt\u0131k(residual) vs e\u011fitilen de\u011ferler grafi\u011fine bak\u0131n\u0131z."}}