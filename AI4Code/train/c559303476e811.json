{"cell_type":{"163a6c6c":"code","0ea27a8b":"code","5575d623":"code","4964548f":"code","bb4a4019":"code","c30f5ad7":"code","15bf9150":"code","4ab3de6e":"code","b4d72c24":"code","da893aa6":"code","d51df31a":"code","2ca10e58":"code","2747191d":"code","0b20e921":"code","08d91250":"code","ef42add8":"code","f17e5e05":"code","5864a5de":"code","f2748561":"code","267bb153":"code","d63ffa74":"code","c68561d5":"code","ce84411e":"code","ad58cb33":"code","2d50af78":"code","8595bd72":"code","d8c663f0":"code","0d14812e":"code","e441d2a0":"code","b41fb032":"code","09fece00":"code","a31717e3":"code","bc49ca20":"code","d44de36b":"code","9a029045":"code","cc9b0360":"code","37789daf":"code","ac98de3a":"code","8564e82b":"code","38af3db9":"code","9a631bc3":"code","7c123b15":"code","c5b81b21":"code","12bb62d1":"markdown","4be65186":"markdown","425753d5":"markdown","bf67f10e":"markdown","1f53057f":"markdown","9328ab9a":"markdown","b006ee21":"markdown","3215487f":"markdown","620f9d19":"markdown","22392c73":"markdown","54232f08":"markdown","f573b679":"markdown","d8573d02":"markdown","f67777ae":"markdown","b50fd842":"markdown","bce1aad3":"markdown","099e2344":"markdown","325912ed":"markdown","908e2bc3":"markdown","0bfef5dc":"markdown","77f15345":"markdown","a8b766ee":"markdown","cf64907c":"markdown","225f614c":"markdown","af130589":"markdown","0fbf2488":"markdown","59cff33a":"markdown","9bb53231":"markdown","e1563e67":"markdown","34253470":"markdown","238b9067":"markdown"},"source":{"163a6c6c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0ea27a8b":"import numpy as np \nimport pandas as pd \nimport os\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport pyLDAvis.sklearn\nimport json\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom PIL import Image\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use(\"ggplot\")\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.decomposition import PCA\nimport string  \nimport spacy\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\nfrom spacy.lang.en import English\n\nimport sklearn.metrics as metrics\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nfrom sklearn.neighbors.nearest_centroid import NearestCentroid\nfrom sklearn.cluster import AgglomerativeClustering, KMeans\nimport importlib\n\n","5575d623":"with open('\/kaggle\/input\/CORD-19-research-challenge\/metadata.readme', 'r') as f:\n    data = f.read()\n    print(data)","4964548f":"dirs = ['\/kaggle\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv\/',\n        '\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/',\n        '\/kaggle\/input\/CORD-19-research-challenge\/noncomm_use_subset\/noncomm_use_subset\/',\n        '\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license\/']\n\nfilenames=[]\ndocs =[]\nfor d in dirs:\n    for file in os.listdir(d):\n        filename = d +file\n        j = json.load(open(filename, 'rb'))\n        \n        paper_id =j['paper_id']\n        #date =j['date']\n        title = j['metadata']['title']\n        authors = j['metadata']['authors']\n        list_authors =[]\n        for author in authors:\n            if(len(author['middle'])==0):\n                middle =\"\"\n            else :\n                middle = author['middle'][0]\n            _authors =author['first']+ \" \"+ middle +\" \"+ author['last']\n            list_authors.append(_authors)\n            \n        try :\n            abstract =  j['abstract'][0]['text']\n        except :\n            abstract =\" \"\n        \n        full_text =\"\"\n        for text in  j['body_text']:\n            full_text += text['text']\n        \n        docs.append([paper_id,title,list_authors,abstract,full_text])\n\ndf = pd.DataFrame(docs,columns=['paper_id','title','list_authors','abstract','full_text'])\ndf.to_csv('\/kaggle\/working\/data.csv')\ndf.head()","bb4a4019":"\ndf['abstract_cleaned'] = df['abstract'].apply(lambda x: x.lower())\ndf['abstract_cleaned'] = df['abstract_cleaned'].apply(lambda x: x.translate(string.punctuation))\ndf['abstract_cleaned'] = df['abstract_cleaned'].apply(lambda x: x.translate(string.digits))\n\ndf['full_text_cleaned'] = df['full_text'].apply(lambda x: x.lower())\ndf['full_text_cleaned'] = df['full_text_cleaned'].apply(lambda x: x.translate(string.punctuation))\ndf['full_text_cleaned'] = df['full_text_cleaned'].apply(lambda x: x.translate(string.digits))\n","c30f5ad7":"spacy_nlp = spacy.load('en_core_web_sm')\n\nspacy_nlp.Defaults.stop_words |= {\"et\", \"al\", \"novel\", \"data\", \"study\", \"studies\", \"research\", \"authors\"}\n\nspacy_stopwords = spacy_nlp.Defaults.stop_words\n\ndf['full_text_cleaned'] = df['full_text_cleaned'].apply(lambda x: ' '.join([word for word in x.split() if word not in (spacy_stopwords)]))\ndf['abstract_cleaned'] = df['abstract_cleaned'].apply(lambda x: ' '.join([word for word in x.split() if word not in (spacy_stopwords)]))","15bf9150":"def get_top_n_words(corpus, n=None):\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_n_words(df['abstract_cleaned'], 20)\n    \ndf2 = pd.DataFrame(common_words, columns = ['abstract_cleaned' , 'count'])\ndf2.groupby('abstract_cleaned').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 words in abstract')","4ab3de6e":"common_words = get_top_n_words(df['full_text_cleaned'], 20)\n    \ndf2 = pd.DataFrame(common_words, columns = ['full_text_cleaned' , 'count'])\ndf2.groupby('full_text_cleaned').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 words in full text')","b4d72c24":"df['abstract_word_count'] = df['abstract_cleaned'].apply(lambda x: len(x.strip().split()))\ndf['body_word_count'] = df['full_text_cleaned'].apply(lambda x: len(x.strip().split()))\ndf.head()\ndf.shape","da893aa6":"df.isnull().sum()\n# later in this work we found duplicates with differents id's we drop them by title\n# we keep first as biorxiv papers has more information and abstractS\ndf = df.drop_duplicates(subset='title', keep='first')","d51df31a":"for key in ['abstract_cleaned','title','full_text_cleaned']:\n    total_words = df[key].values\n    wordcloud = WordCloud(width=1800, height=1200).generate(str(total_words))\n    plt.figure( figsize=(30,10) )\n    plt.title ('Wordcloud' + key)\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.show()","2ca10e58":"df[['abstract_word_count']].plot(kind='box', title='Boxplot of Word Count', figsize=(10,6))\nplt.show()\n\ndf[['body_word_count']].plot(kind='box', title='Boxplot of Word Count', figsize=(10,6))\nplt.show()","2747191d":"print(df['abstract_word_count'].mean(),df['abstract_word_count'].std())\nprint(df['body_word_count'].mean(),df['body_word_count'].std())","0b20e921":"features  = 5000\n# TODO: probar con TFIDF\ntf_vectorizer = CountVectorizer(max_features=features, stop_words='english', min_df=10)\nX_tf = tf_vectorizer.fit_transform(df['abstract'])\ntf_feat_name = tf_vectorizer.get_feature_names()","08d91250":"topics = 7\nlda_model = LatentDirichletAllocation(learning_method='online',random_state=23,n_components=topics)\nlda_output =lda_model.fit_transform(X_tf)","ef42add8":"# preparing for plotting pyLDAvis\n%matplotlib inline\npyLDAvis.enable_notebook()\n# plotting lda\npyLDAvis.sklearn.prepare(lda_model, X_tf, tf_vectorizer)\n# if you want to save it \n# P=pyLDAvis.sklearn.prepare(lda_model, X_tf, tf_vectorizer)\n# pyLDAvis.save_html(p, 'lda.html')","f17e5e05":"# func from Anish Pandey\ndef visualizing_topic_cluster(model,stop_len,feat_name):\n    topic={}\n    for index,topix in enumerate(model.components_):\n        topic[index]= [feat_name[i] for i in topix.argsort()[:-stop_len-1:-1]]\n    \n    return topic\n\ntopic_lda =visualizing_topic_cluster(lda_model,10,tf_feat_name)\n# printing \nlen([print('Topic '+str(key),topic_lda[key]) for key in topic_lda])","5864a5de":"# dirty as fuck, removing words that appears in same topic\nimport copy\ntopic_lda_2 = topic_lda\nfor key in topic_lda_2:\n    for element in topic_lda_2[key]:\n        if element in ['cell','cells','viral','virus','respiratory','study','infection','acute']:\n            topic_lda_2[key].remove(str(element))\n[print('Topic '+str(key),topic_lda_2[key]) for key in topic_lda_2]","f2748561":"# lets see if our topics are correlated, first mixing topics with our df\ncolumns=['Topic'+ i for i in list(map(str,list(topic_lda.keys())))]\nlda_df =pd.DataFrame(lda_output,columns=columns).apply(lambda x : np.round(x,3))\nlda_df['Major_topic'] =lda_df[columns].idxmax(axis=1).apply(lambda x: int(x[-1]))\nlda_df['keyword'] = lda_df['Major_topic'].apply(lambda x: topic_lda[x])\nlda_df.head()","267bb153":"# plotting results\nimport seaborn as sns\nplt.figure(figsize=(10,10))\nsns.heatmap(abs(lda_df[columns].corr()),annot=True,fmt ='0.2f',cmap=\"YlGnBu\")\nplt.title(\" Correlation Plot\")","d63ffa74":"features  = 5000\ntfidf_vectorizer = TfidfVectorizer(max_features=features, stop_words=\"english\")\ntfidf_vectorizer.fit(df['abstract'])\nfeatures_tf_idf = tfidf_vectorizer.transform(df['abstract'])","c68561d5":"X = features_tf_idf\ncls = MiniBatchKMeans(n_clusters=7, random_state=0)\nY = cls.fit_predict(X)","ce84411e":"#Function for reducing (PCA) and visualizing cluster results\ndef reducing_and_visualizing_cluster_results(X,Y,centers):\n    # reduce the features to 2D\n    pca = PCA(n_components=2)\n    reduced_features = pca.fit_transform(X)\n    # reduce the cluster centers to 2D\n    reduced_cluster_centers = pca.transform(centers)\n    plt.scatter(reduced_features[:,0], reduced_features[:,1], c=Y)\n    plt.scatter(reduced_cluster_centers[:, 0], reduced_cluster_centers[:,1], marker='x', s=200, c='b')\n    ","ad58cb33":"reducing_and_visualizing_cluster_results(X.toarray(),Y,cls.cluster_centers_)","2d50af78":"#function for training and creating word embedding model\ndef create_word_embedings(texts,method):\n    documents=[]\n    texts = clean_text(texts)\n    for line in texts:\n        documents.append(line.split())\n    # build vocabulary and train model\n    if(str(method)==\"cbow\"):\n        model = Word2Vec(documents, size=50, window=5, min_count=5, workers=10)\n    else:\n        model = Word2Vec(documents, size=50, window=5, min_count=5, workers=10,sg=1)\n    model.train(documents, total_examples=len(documents), epochs=10)\n    return model","8595bd72":"#function for cleaning and homogenizing the texts\ndef clean_text(text):\n    text = text.apply(lambda x: re.sub('[^a-zA-Z0-9\\s]', ' ', x))\n    text = text.apply(lambda x: re.sub(' +',' ', x))\n    text = text.apply(lambda x: x.lower())\n    return text","d8c663f0":"texts = df['abstract']\nmodel = create_word_embedings(texts,'cobw')\nword_vectors = model.wv\ndel model","0d14812e":"word_vectors.most_similar(positive = 'coronavirus', topn=10)","e441d2a0":"tokens = [token for token in word_vectors.vocab]\nX = [word_vectors[token] for token in word_vectors.vocab]\n(len(X),len(tokens))","b41fb032":"silhouette_list = []\nk_list = ['2','3','4','5','6','7','8','9','10','15','20','25','30']\nfor p in k_list:\n    clusterer = AgglomerativeClustering(n_clusters=int(p), linkage='ward')\n    clusterer.fit(X)\n    # The higher (up to 1) the better\n    s = round(metrics.silhouette_score(X, clusterer.labels_), 4)\n    silhouette_list.append(s)\n    # The higher (up to 1) the better\n\nplt.figure(figsize=(10, 7))  \np = plt.plot(k_list,silhouette_list)\n#plt.plot(['3','3'], [0,0.12], color=\"red\", linewidth=2.5, linestyle=\"--\")\nplt.show()","09fece00":"clusterer = AgglomerativeClustering(n_clusters=5,  linkage='ward')\nY = clusterer.fit_predict(X)","a31717e3":"cls_NC = NearestCentroid()\ncls_NC.fit(X, Y)","bc49ca20":"reducing_and_visualizing_cluster_results(X,Y,cls_NC.centroids_)","d44de36b":"labels = [word_vectors.similar_by_vector(center,topn = 5) for center in cls_NC.centroids_]\nlabels","9a029045":"# function returns WSS score for a list of k values\ndef calculate_WSS(points,func):\n  sse = []\n  k_list = ['2','3','4','5','6','7','8','9','10','15','20','25','30']\n  module = importlib.import_module('sklearn.cluster')\n  function = getattr(module, func)\n  for k in k_list:\n    kmeans = function(n_clusters = int(k)).fit(points)\n    centroids = kmeans.cluster_centers_\n    pred_clusters = kmeans.predict(points)\n    curr_sse = 0\n    \n    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n    for i in range(len(points)):\n      curr_center = centroids[pred_clusters[i]]\n      curr_sse += np.linalg.norm(np.array(points[i])-np.array(curr_center)) ** 2\n      \n    sse.append(curr_sse)\n  return sse","cc9b0360":"sse = calculate_WSS(X,'KMeans')\nplt.figure(figsize=(10, 7))  \np = plt.plot(k_list,sse)\nplt.show()","37789daf":"cls_KM =  KMeans(n_clusters=3, n_jobs=4, verbose=10)\ncls_KM.fit(X)\nY = cls_KM.predict(X)","ac98de3a":"reducing_and_visualizing_cluster_results(X,Y,cls_KM.cluster_centers_)","8564e82b":"labels = [word_vectors.similar_by_vector(center,topn = 5) for center in cls_KM.cluster_centers_]\nlabels","38af3db9":"sse = calculate_WSS(X,'MiniBatchKMeans')\nplt.figure(figsize=(10, 7))  \np = plt.plot(k_list,sse)\nplt.show()","9a631bc3":"cls_MBKM = MiniBatchKMeans(n_clusters=3)\ncls_MBKM.fit(X)\nY = cls_MBKM.predict(X)","7c123b15":"reducing_and_visualizing_cluster_results(X,Y,cls_MBKM.cluster_centers_)","c5b81b21":"labels = [word_vectors.similar_by_vector(center,topn = 5) for center in cls_MBKM.cluster_centers_]\nlabels","12bb62d1":"Once the WE have been created, we are going to create clusters of words in order to detect clusters of topics. First, we will try using AgglomerativeClustering algorithm. Before creating the model, we are going to perform a quick analysis to obtain the optimal number of clusters (k). In this case, we will use the Silhouette Method.","4be65186":"Now we get the most similar words (top=10) to \"coronavirus\" word.","425753d5":"Extract all the data","bf67f10e":"The results look quite promising. Following we get the five most representative words for each cluster taking into account the Word2Vec model previously created.","1f53057f":"In this case, the clustering looks slightly worse than the above case. Notwithstanding, the results still look well. Eventually, we get the representative words for each cluster.","9328ab9a":"We are going to use the NearestCentroid classifier to get the centroids of each cluster.","b006ee21":"Let's see which words are the most common in the abstract and full text and try to eliminate them if they are not descriptive for the problem","3215487f":"# Clustering with KMeans and WE","620f9d19":"The results obtained using WE clearly outperform the results reached by TF-IDF. However, there are some ideas that could be interesting in order to improve topic analysis:\n\n* At first glance, we can see that the pre-processing step can be improved (cleaning, filtering, etc.).\n* Considering the use of n-grams for texts analyzing.\n* Using another sort of model (Doc2Vec, FastText) for text representation in a low-dimensional space.\n* Improving the method of selecting the most representative words.\n* Considenring the extraction of association rules per detected topic.","22392c73":"Now we remove stop words and those that are very common or very rare.","54232f08":"# COVID19 DATA IMPORT","f573b679":"## Boxplot: Word Count","d8573d02":"\n\nIn this section of the notebook we will try to visualize the data in an appropriate way to try to obtain relevant information from it that will help in the subsequent process. \n\n## Tag Cloud\n\nFor tag cloud the first step is add the Word Count Column","f67777ae":"# Clustering using word embeddings (WE).\n\nIt can be seen that results using tf-idf do not look pretty promising. Now we are going to try with word embeddings-based feature vectors. First, we are going to train and obtain the WE for the abstracts. We will use the Word2Vec model and CBOW architecture.","b50fd842":"Again, the first step will be to obtain the optimal number of clusters. We are going to use the Elbow Method as in the previous case.","bce1aad3":"# Clustering\n\nWe are going to create a clustering model to check if the clusters are correlated with the obtained topics. First we get the features based on the tf-idf weight of the text.","099e2344":"We chose k=3.","325912ed":"We have chosen k=3 as the optimal number of clusters.","908e2bc3":"# Topic Modelling with countvectorizer and lda\n\nTo try to answer the tasks we are going to get the topics discussed in the papers.","0bfef5dc":"Getting the five most similar words for each centroid.","77f15345":"# Clustering with MiniBathKMeans and WE.","a8b766ee":"# EDA & Text Cleaning\n\nFirst we're going to eliminate numbers, we're going to make everything small, and we're going to eliminate punctuation marks","cf64907c":"# Clustering with AgglomerativeClustering and WE.","225f614c":"We are using the code created by Simo Bouss for data import","af130589":"It can be seen that the highest value apart k=2 is reached for k=5. In this case, we chose k=5 as the optimal number of clusters. We have tried with different configurations of metrics and linkages and the better results were obtained with the Euclidean distance and the Ward method.  ","0fbf2488":"As in the previous case, the first step will be to obtain the optimal number of clusters. In this case, we are going to use the Elbow Method.","59cff33a":"We can see that words of the same cluster are close to each other, although there are still overlaps. Thus we are going to try to improve the results using another clustering algorithm.","9bb53231":"We are going to add visualization of the clusters, first we will have to reduce the dimensionality.","e1563e67":"Now we train and obtain the clusters, we will use in a first version the number of topics and later we will do experiments to refine the k number of clusters.","34253470":" Loading libreries","238b9067":"Look at the directory"}}