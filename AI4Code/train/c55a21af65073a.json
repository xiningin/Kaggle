{"cell_type":{"c1058da1":"code","25c484d2":"code","10d24e52":"code","e61ec349":"code","f04cf579":"code","7a4b6a63":"code","044737ce":"code","929b7a92":"code","0e28ccea":"code","feee1a85":"code","c92226b9":"code","27fd5743":"code","746d354c":"code","8b5f16e8":"code","e273fa7f":"code","83b52770":"code","18a0ce4e":"code","0900e974":"code","f294e77a":"code","759bcb11":"code","e6d0f8f5":"code","1dd3b515":"code","7020b2f8":"code","9cf18c7d":"code","b0edd85d":"code","3f6fe5da":"code","a4924161":"code","c839db4c":"code","74ad82ad":"code","7a7cc493":"code","1f86c93d":"code","5d4e94fb":"code","e3cf1e73":"code","49b509d0":"code","54296419":"code","fb5bed9d":"code","81140878":"code","9bd2a715":"code","10ee4420":"code","c65383f0":"code","c25cccbd":"code","2795014f":"code","3713f542":"code","900128c7":"code","544e15ee":"markdown","2f6c5d8b":"markdown","c5089bf8":"markdown","9beed72d":"markdown","c79536f5":"markdown","0470398b":"markdown"},"source":{"c1058da1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","25c484d2":"def mean_absolute_percentage_error(y_true, y_pred):\n    \"\"\"MAPE\"\"\"\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","10d24e52":"import numpy as np\nimport scipy as sp\nimport pandas as pd\nfrom pandas import DataFrame, Series\nimport re\nimport sys\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom category_encoders import OrdinalEncoder, OneHotEncoder, TargetEncoder\nfrom tqdm import tqdm_notebook as tqdm\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport lightgbm as lgb","e61ec349":"df_train = pd.read_csv('..\/input\/sales-prediction-of-clothes-in-e-commerce\/train.csv', index_col=0)\n#df_train = pd.read_csv('..\/input\/homework-for-students2\/train.csv', index_col=0)\ndf_test = pd.read_csv('..\/input\/sales-prediction-of-clothes-in-e-commerce\/test.csv', index_col=0)\ny_train = df_train.units_sold\nX_train = df_train.drop(['units_sold'], axis=1)\n# X_train = df_train\nX_test = df_test\n# \u30bf\u30fc\u30b2\u30c3\u30c8\u3092\u5bfe\u6570\u306b\u5909\u63db\ny_train = y_train.apply(np.log1p)","f04cf579":"y_train","7a4b6a63":"y_train.value_counts()","044737ce":"y_train.describe()","929b7a92":"y_train.isnull().sum()","0e28ccea":"df_train.head(10)","feee1a85":"df_train.units_sold.value_counts()","c92226b9":"# \u5fc5\u8981\u305d\u3046\u306a\u30c6\u30ad\u30b9\u30c8\u306e\u7279\u5fb4\u91cf\u53d6\u308a\u51fa\u3057\u3001\u524a\u9664\nTXT_train = X_train.tags.copy()\nTXT_test = X_test.tags.copy()\n\nTXT_title_train = X_train.title.copy()\nTXT_title_test = X_test.title.copy()\n\nX_train.drop(['title','tags','merchant_title', 'merchant_id'],axis=1,inplace=True)\nX_test.drop(['title','tags','merchant_title', 'merchant_id'],axis=1,inplace=True)","27fd5743":"# null\u306e\u6570\u306e\u7279\u5fb4\u91cf\u4f5c\u6210\nson_train = X_train.isnull().sum(axis=1)\nson_test = X_test.isnull().sum(axis=1)\nson_train.name = 'null_num'\nson_test.name = 'null_num'\n# null\u306e\u5408\u8a08\u3092\u7279\u5fb4\u91cf\u3068\u3057\u3066\u7d50\u5408\nX_train = pd.concat([X_train, son_train], axis=1) \nX_test = pd.concat([X_test, son_test], axis=1) ","746d354c":"X_train.null_num.value_counts()","8b5f16e8":"len(X_train)","e273fa7f":"# dtype\u304cobject\uff08\u6570\u5024\u3067\u306a\u3044\u3082\u306e\uff09\u306e\u30ab\u30e9\u30e0\u540d\u3068\u30e6\u30cb\u30fc\u30af\u6570\u3092\u78ba\u8a8d\nnum_cols = []\ncat_cols = []\nother_cols = []\nfor col in X_train.columns:\n    if X_train[col].dtype == 'float':\n        num_cols.append(col)\n    elif X_train[col].dtype == 'int':\n        num_cols.append(col)\n    elif X_train[col].dtype == 'object':\n        cat_cols.append(col)\n    else:\n        else_cols.append(col)\n    print(col, '\u30e6\u30cb\u30fc\u30af\u6570:',X_train[col].nunique(), '\u6b20\u640d\u5024:',X_train[col].isnull().sum())","83b52770":"num_cols","18a0ce4e":"X_train[num_cols].isnull().sum()","0900e974":"X_train[num_cols].nunique()","f294e77a":"# \u30b9\u30b3\u30a2\u306e\u5408\u8a08\nX_train['rating_score'] = 5*X_train.rating_five_count + 4*X_train.rating_four_count + 3*X_train.rating_three_count + 2*X_train.rating_two_count + 1*X_train.rating_one_count\nX_test['rating_score']= 5*X_test.rating_five_count + 4*X_test.rating_four_count + 3*X_test.rating_three_count + 2*X_test.rating_two_count + 1*X_test.rating_one_count","759bcb11":"# AVG_Score\nX_train['avg_score'] = X_train['rating_score'] \/ (X_train.rating_five_count + X_train.rating_four_count + X_train.rating_three_count + X_train.rating_two_count + X_train.rating_one_count)\nX_test['avg_score']= X_test['rating_score'] \/ (X_test.rating_five_count + X_test.rating_four_count + X_test.rating_three_count + X_test.rating_two_count + X_test.rating_one_count)","e6d0f8f5":"X_train","1dd3b515":"cat_cols","7020b2f8":"X_train[cat_cols].isnull().sum()","9cf18c7d":"X_train[cat_cols].nunique()","b0edd85d":"# \u4eca\u5ea6\u306fTarget Encoding\ntarget = 'units_sold'\nX_temp = pd.concat([X_train, y_train], axis=1)\n\nfor col in cat_cols:\n\n    # X_test\u306fX_train\u3067\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3059\u308b\n    summary = X_temp.groupby([col])[target].mean()\n    X_test[col] = X_test[col].map(summary) \n\n\n    # X_train\u306e\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u3092oof\u3067\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3059\u308b\n#     skf = StratifiedKFold(n_splits=5, random_state=71, shuffle=True)\n    skf = KFold(n_splits=5, random_state=71, shuffle=True)\n    enc_train = Series(np.zeros(len(X_train)), index=X_train.index)\n\n    for i, (train_ix, val_ix) in enumerate((skf.split(X_train, y_train))):\n        X_train_, _ = X_temp.iloc[train_ix], y_train.iloc[train_ix]\n        X_val, _ = X_temp.iloc[val_ix], y_train.iloc[val_ix]\n\n        summary = X_train_.groupby([col])[target].mean()\n        enc_train.iloc[val_ix] = X_val[col].map(summary)\n        \n    X_train[col]  = enc_train","3f6fe5da":"X_train","a4924161":"X_train.fillna(X_train.median(), inplace=True)\nX_test.fillna(X_train.median(), inplace=True)","c839db4c":"X_train","74ad82ad":"# tag\nTXT_train = TXT_train.str.lower()\nTXT_test = TXT_test.str.lower()\n\nTXT_train.fillna('#', inplace=True)\nTXT_test.fillna('#', inplace=True)\n\n### tfidf\u3067\u5358\u8a9e\u306e\u51fa\u73fe\u983b\u5ea6\u306b\u5909\u63db\u3057\u307e\u3059\u3002\u306a\u304a\u3001idf\u306fTree\u7cfb\u3067\u306f\u3042\u307e\u308a\u610f\u5473\u304c\u3042\u308a\u307e\u305b\u3093\u304c\u3001\u7dda\u5f62\u30e2\u30c7\u30eb\u3084NN\u3067\u306f\u6709\u7528\u3067\u3059\u306e\u3067\u3001\u3053\u3053\u3067\u306f\u9069\u7528\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\ntfidf = TfidfVectorizer(max_features=20, use_idf=True)\n\ntfidf.fit(TXT_train) # \u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u30dc\u30ad\u30e3\u30d6\u30e9\u30ea\n\nTXT_train_enc = tfidf.transform(TXT_train) # \u5358\u8a9e\u306a\u3044\u3057\u305d\u306e\u9023\u306a\u308a\uff08or \u6587\u5b57\u306a\u3044\u3057\u305d\u306e\u9023\u306a\u308a\uff09\u3092\u30ab\u30a6\u30f3\u30c8\u3057\u3066\u30d9\u30af\u30c8\u30eb\u5316\nTXT_test_enc = tfidf.transform(TXT_test)\n\n#\u758e\u884c\u5217\u304c\u5e30\u3063\u3066\u304d\u307e\u3059\u3002\nTXT_train_enc\n\nX_train_sp = sp.sparse.hstack([X_train.values, TXT_train_enc]) # \u6570\u5024+\u30ab\u30c6\u30b4\u30ea\u3068\u6a2a\u65b9\u5411\u306b\u30ac\u30c3\u30c1\u30e3\u30f3\u30b3\u3059\u308b(hstack)\nX_test_sp = sp.sparse.hstack([X_test.values, TXT_test_enc])","7a7cc493":"tfidf.get_feature_names()","1f86c93d":"# title\nTXT_title_train = TXT_title_train.str.lower()\nTXT_title_test = TXT_title_test.str.lower()\n\nTXT_title_train.fillna('#', inplace=True)\nTXT_title_test.fillna('#', inplace=True)\n\n### tfidf\u3067\u5358\u8a9e\u306e\u51fa\u73fe\u983b\u5ea6\u306b\u5909\u63db\u3057\u307e\u3059\u3002\u306a\u304a\u3001idf\u306fTree\u7cfb\u3067\u306f\u3042\u307e\u308a\u610f\u5473\u304c\u3042\u308a\u307e\u305b\u3093\u304c\u3001\u7dda\u5f62\u30e2\u30c7\u30eb\u3084NN\u3067\u306f\u6709\u7528\u3067\u3059\u306e\u3067\u3001\u3053\u3053\u3067\u306f\u9069\u7528\u3057\u3066\u304a\u304d\u307e\u3059\u3002\n\ntfidf = TfidfVectorizer(max_features=20, use_idf=True)\n\ntfidf.fit(TXT_title_train) # \u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u30dc\u30ad\u30e3\u30d6\u30e9\u30ea\n\nTXT_title_train_enc = tfidf.transform(TXT_title_train) # \u5358\u8a9e\u306a\u3044\u3057\u305d\u306e\u9023\u306a\u308a\uff08or \u6587\u5b57\u306a\u3044\u3057\u305d\u306e\u9023\u306a\u308a\uff09\u3092\u30ab\u30a6\u30f3\u30c8\u3057\u3066\u30d9\u30af\u30c8\u30eb\u5316\nTXT_title_test_enc = tfidf.transform(TXT_title_test)\n\n#\u758e\u884c\u5217\u304c\u5e30\u3063\u3066\u304d\u307e\u3059\u3002\nTXT_train_enc\n\nX_train_sp = sp.sparse.hstack([X_train_sp, TXT_title_train_enc]) # \u6570\u5024+\u30ab\u30c6\u30b4\u30ea\u3068\u6a2a\u65b9\u5411\u306b\u30ac\u30c3\u30c1\u30e3\u30f3\u30b3\u3059\u308b(hstack)\nX_test_sp = sp.sparse.hstack([X_test_sp, TXT_title_test_enc])","5d4e94fb":"tfidf.get_feature_names()","e3cf1e73":"X_test","49b509d0":"X_temp = pd.concat([X_train, y_train], axis=1)\ndf_corr = X_temp.corr()\nprint(df_corr.units_sold)\n# print(df_corr.grade, df_corr.sub_grade)","54296419":"# # # \u4eca\u5ea6\u306f\u30ab\u30c6\u30b4\u30ea\u7279\u5fb4\u91cf\u3082\u8ffd\u52a0\u3057\u3066\u30e2\u30c7\u30ea\u30f3\u30b0\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n# # # CV\u3057\u3066\u30b9\u30b3\u30a2\u3092\u898b\u3066\u307f\u308b\u3002\u5c64\u5316\u62bd\u51fa\u3067\u826f\u3044\u304b\u306f\u5225\u9014\u3088\u304f\u8003\u3048\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002\nscores = []\ny_preds = []\nX = X_train_sp.tocsr()\ny= y_train.reset_index(drop=True)\n# skf = StratifiedKFold(n_splits=3, random_state=71, shuffle=True)\nskf = KFold(n_splits=3, random_state=71, shuffle=True)\n\nfor i, (train_ix, test_ix) in tqdm(enumerate(skf.split(X,y))):\n    X_train_, y_train_ = X[train_ix], y_train.values[train_ix]\n    X_val, y_val = X[test_ix], y_train.values[test_ix]\n    \n    hyper_params = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'mape',\n    }\n    gbm = lgb.LGBMRegressor(**hyper_params)\n    gbm.fit(X_train_, y_train_,\n        eval_set=[(X_val, y_val)],\n        eval_metric='mape',\n        early_stopping_rounds=1000)\n    y_pred = gbm.predict(X_val, num_iteration=gbm.best_iteration_)\n    \n    score = mean_absolute_percentage_error(y_val, y_pred)\n    scores.append(score)\n    y_pred_t = gbm.predict(X_test_sp, num_iteration=gbm.best_iteration_)\n    y_preds.append(y_pred_t)\n    print('CV Score of Fold_%d is %f' % (i, score))","fb5bed9d":"print('avg_score:',(scores[0]+ scores[1] + scores[2])\/3)","81140878":"len(y_preds[0])","9bd2a715":"len(X_test)","10ee4420":"# \u5bfe\u6570\u306e\u9006\u95a2\u6570\ny_pred = (np.expm1(y_preds[0]) + np.expm1(y_preds[1]) + np.expm1(y_preds[2]) )\/3","c65383f0":"y_pred","c25cccbd":"df_train.units_sold.value_counts()","2795014f":"# # \u30a8\u30a4\u30e4\n# y_pred[y_pred > 15000] = 20000\n# y_pred[(15000 >= y_pred) & (y_pred > 7500)] = 10000\n# y_pred[(7500 >= y_pred) & (y_pred > 2500)] = 5000\n# y_pred[(2500 >= y_pred) & (y_pred > 750)] = 1000\n# y_pred[(750 >= y_pred) & (y_pred > 350)] = 500\n# y_pred[(350 >= y_pred) & (y_pred > 150)] = 200\n# y_pred[(150 >= y_pred) & (y_pred >= 0)] = 100","3713f542":"y_pred","900128c7":"# sample submission\u3092\u8aad\u307f\u8fbc\u3093\u3067\u3001\u4e88\u6e2c\u5024\u3092\u4ee3\u5165\u306e\u5f8c\u3001\u4fdd\u5b58\u3059\u308b\nsubmission = pd.read_csv('..\/input\/sales-prediction-of-clothes-in-e-commerce\/sample_submission.csv', index_col=0)\n\nsubmission.units_sold = y_pred\nsubmission.to_csv('submission.csv')","544e15ee":"# \u30ab\u30c6\u30b4\u30ea\u7279\u5fb4\u91cf","2f6c5d8b":"# \u30e2\u30c7\u30ea\u30f3\u30b0","c5089bf8":"# \u30bf\u30fc\u30b2\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0","9beed72d":"# \u30c6\u30ad\u30b9\u30c8\u7279\u5fb4\u91cf","c79536f5":"# \u76f8\u95a2","0470398b":"# \u6570\u5024\u7279\u5fb4\u91cf"}}