{"cell_type":{"af0c3869":"code","c2a0f975":"code","8a64d6e5":"code","b9b3179f":"code","8efdb09b":"code","3eb9ef52":"code","480e160d":"code","d2636af6":"code","31e15eb6":"code","0b6353cd":"code","b5b20f5b":"code","a6aa4d93":"code","7a1fc80a":"code","7dc6cbee":"code","f2a8fc0d":"code","775769b4":"code","94b8e868":"code","653e9235":"code","d51e9024":"code","f00f60bd":"code","d27133a3":"code","97ab665f":"code","157b1bb5":"code","f4a7f86c":"code","9b6794c4":"code","53bd9e6a":"code","8d525aef":"code","3cda3021":"code","63ef6db9":"code","bb70a304":"code","8b26b6ce":"code","cf6f4d41":"code","1194de15":"code","e4c05426":"code","cd94668c":"code","1acf750d":"code","c09a8b7c":"code","89a6bd91":"code","f0bcf4a9":"code","79227276":"code","532cf5a3":"code","30be7ce1":"code","0d32e9e0":"code","a8486686":"code","1b5290ca":"code","457d6d35":"code","4d4b7730":"code","e1393d09":"code","5b527c69":"code","640083aa":"code","e1720391":"code","1f7fb7d2":"code","06825837":"code","3dea6d36":"code","ec2a04a4":"code","e9606098":"code","369e9063":"markdown","343f6876":"markdown","f2993bf7":"markdown","042fb4a3":"markdown","46a9ab75":"markdown","b0812dc6":"markdown","3065cc24":"markdown","96084aa2":"markdown","74e4b1e8":"markdown","97235643":"markdown","58ccf01c":"markdown","094ec097":"markdown","27dec38b":"markdown","112e602d":"markdown","e7ea3116":"markdown","7ac845e4":"markdown","834d0856":"markdown","3f428a24":"markdown","645a8052":"markdown","b8c4dc03":"markdown","64c30a8a":"markdown","dc7953b8":"markdown","14c21617":"markdown","8104c597":"markdown","2df15e2a":"markdown","4f7ef805":"markdown","1675446d":"markdown","db7c59cb":"markdown","cfa940cb":"markdown","975316e3":"markdown","28f23ce1":"markdown","9c297086":"markdown","3acc5839":"markdown","a69efa69":"markdown","966a7736":"markdown"},"source":{"af0c3869":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c2a0f975":"from __future__ import absolute_import, division, print_function, unicode_literals","8a64d6e5":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\n\nimport sklearn\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nimport os\nimport tempfile\n\n\npd.set_option('display.max_columns', None)\n\n%matplotlib inline","b9b3179f":"mpl.rcParams['figure.figsize'] = (12, 10)\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']","8efdb09b":"df = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","3eb9ef52":"df.info()","480e160d":"df.head()","d2636af6":"df['Class'].value_counts()","31e15eb6":"sns.countplot('Class', data=df)\nplt.title('Class Lables Distributions \\n 0: No Fraud   1: Fraud', fontsize=16);","0b6353cd":"clear_df = df.copy()\n\nclear_df.pop('Time')\n\neps=0.001\nclear_df['Log Ammount'] = np.log(clear_df.pop('Amount')+eps)","b5b20f5b":"\ntrain_df, test_df = train_test_split(clear_df, test_size=0.2)\n\ntrain_df, val_df = train_test_split(train_df, test_size=0.2)\n\ntrain_labels = np.array(train_df.pop('Class'))\nbool_train_labels = train_labels != 0\nval_labels = np.array(val_df.pop('Class'))\ntest_labels = np.array(test_df.pop('Class'))\n\ntrain_features = np.array(train_df)\nval_features = np.array(val_df)\ntest_features = np.array(test_df)","a6aa4d93":"scaler = StandardScaler()\n\ntrain_features = scaler.fit_transform(train_features)\nval_features = scaler.transform(val_features)\ntest_features = scaler.transform(test_features)\n\nprint('Training labels shape:', train_labels.shape)\nprint('Validation labels shape:', val_labels.shape)\nprint('Test labels shape:', test_labels.shape)\n\ntrain_features = np.clip(train_features, -5, 5)\nval_features = np.clip(val_features, -5, 5)\ntest_features = np.clip(test_features, -5, 5)\n\nprint('\\nTraining features shape:', train_features.shape)\nprint('Validation features shape:', val_features.shape)\nprint('Test features shape:', test_features.shape)","7a1fc80a":"pos_df = pd.DataFrame(train_features[ bool_train_labels], columns = train_df.columns)\nneg_df = pd.DataFrame(train_features[~bool_train_labels], columns = train_df.columns)\n\nsns.jointplot(pos_df['V5'], pos_df['V6'],\n              kind='hex', xlim = (-5,5), ylim = (-5,5))\nplt.suptitle(\"Positive distribution\")\n\nsns.jointplot(neg_df['V5'], neg_df['V6'],\n              kind='hex', xlim = (-5,5), ylim = (-5,5))\n_ = plt.suptitle(\"Negative distribution\")","7dc6cbee":"METRICS = [\n      keras.metrics.TruePositives(name='tp'),\n      keras.metrics.FalsePositives(name='fp'),\n      keras.metrics.TrueNegatives(name='tn'),\n      keras.metrics.FalseNegatives(name='fn'), \n      keras.metrics.BinaryAccuracy(name='accuracy'),\n      keras.metrics.Precision(name='precision'),\n      keras.metrics.Recall(name='recall'),\n      keras.metrics.AUC(name='auc'),\n]","f2a8fc0d":"\ndef make_model(metrics = METRICS, output_bias=None):\n  if output_bias is not None:\n    output_bias = tf.keras.initializers.Constant(output_bias)\n  model = keras.Sequential([\n      keras.layers.Dense(16, activation='relu', input_shape=(train_features.shape[-1],)),\n      keras.layers.Dropout(0.5),\n      keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias),\n  ])\n\n  model.compile(\n      optimizer=keras.optimizers.Adam(lr=1e-3),\n      loss=keras.losses.BinaryCrossentropy(),\n      metrics=metrics)\n\n  return model","775769b4":"EPOCHS = 100\nBATCH_SIZE = 2048\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_auc', \n    verbose=1,\n    patience=10,\n    mode='max',\n    restore_best_weights=True)","94b8e868":"model = make_model()\nmodel.summary()","653e9235":"model.predict(train_features[:10])","d51e9024":"results = model.evaluate(train_features, train_labels, batch_size=BATCH_SIZE, verbose=0)\nprint(\"Loss: {:0.4f}\".format(results[0]))","f00f60bd":"neg, pos = np.bincount(clear_df['Class'])\ninitial_bias = np.log([pos\/neg])\ninitial_bias","d27133a3":"model = make_model(output_bias = initial_bias)\nmodel.predict(train_features[:10])","97ab665f":"results = model.evaluate(train_features, train_labels, batch_size=BATCH_SIZE, verbose=0)\nprint(\"Loss: {:0.4f}\".format(results[0]))","157b1bb5":"initial_weights = os.path.join(tempfile.mkdtemp(),'initial_weights')\nmodel.save_weights(initial_weights)","f4a7f86c":"model = make_model()\nmodel.load_weights(initial_weights)\nmodel.layers[-1].bias.assign([0.0])\nzero_bias_history = model.fit(\n    train_features,\n    train_labels,\n    batch_size=BATCH_SIZE,\n    epochs=20,\n    validation_data=(val_features, val_labels), \n    verbose=0)","9b6794c4":"odel = make_model()\nmodel.load_weights(initial_weights)\ncareful_bias_history = model.fit(\n    train_features,\n    train_labels,\n    batch_size=BATCH_SIZE,\n    epochs=20,\n    validation_data=(val_features, val_labels), \n    verbose=0)","53bd9e6a":"def plot_loss(history, label, n):\n  plt.semilogy(history.epoch,  history.history['loss'],\n               color=colors[n], label='Train '+label)\n  plt.semilogy(history.epoch,  history.history['val_loss'],\n          color=colors[n], label='Val '+label,\n          linestyle=\"--\")\n  plt.xlabel('Epoch')\n  plt.ylabel('Loss')\n  \n  plt.legend()","8d525aef":"plot_loss(zero_bias_history, \"Zero Bias\", 0)\nplot_loss(careful_bias_history, \"Careful Bias\", 1)","3cda3021":"model = make_model()\nmodel.load_weights(initial_weights)\nbaseline_history = model.fit(\n    train_features,\n    train_labels,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks = [early_stopping],\n    validation_data=(val_features, val_labels))","63ef6db9":"def plot_metrics(history):\n  metrics =  ['loss', 'auc', 'precision', 'recall']\n  for n, metric in enumerate(metrics):\n    name = metric.replace(\"_\",\" \").capitalize()\n    plt.subplot(2,2,n+1)\n    plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n    plt.plot(history.epoch, history.history['val_'+metric],\n             color=colors[0], linestyle=\"--\", label='Val')\n    plt.xlabel('Epoch')\n    plt.ylabel(name)\n    if metric == 'loss':\n      plt.ylim([0, plt.ylim()[1]])\n    elif metric == 'auc':\n      plt.ylim([0.8,1])\n    else:\n      plt.ylim([0,1])\n\n    plt.legend()","bb70a304":"plot_metrics(baseline_history)","8b26b6ce":"train_predictions_baseline = model.predict(train_features, batch_size=BATCH_SIZE)\ntest_predictions_baseline = model.predict(test_features, batch_size=BATCH_SIZE)","cf6f4d41":"def plot_cm(labels, predictions, p=0.5):\n  cm = confusion_matrix(labels, predictions > p)\n  plt.figure(figsize=(5,5))\n  sns.heatmap(cm, annot=True, fmt=\"d\")\n  plt.title('Confusion matrix @{:.2f}'.format(p))\n  plt.ylabel('Actual label')\n  plt.xlabel('Predicted label')\n\n  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n  print('Total Fraudulent Transactions: ', np.sum(cm[1]))","1194de15":"baseline_results = model.evaluate(test_features, test_labels,\n                                  batch_size=BATCH_SIZE, verbose=0)\nfor name, value in zip(model.metrics_names, baseline_results):\n  print(name, ': ', value)\nprint()\n\nplot_cm(test_labels, test_predictions_baseline)","e4c05426":"def plot_roc(name, labels, predictions, **kwargs):\n  fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n\n  plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n  plt.xlabel('False positives [%]')\n  plt.ylabel('True positives [%]')\n  plt.xlim([-0.5,20])\n  plt.ylim([80,100.5])\n  plt.grid(True)\n  ax = plt.gca()\n  ax.set_aspect('equal')","cd94668c":"plot_roc(\"Train Baseline\", train_labels, train_predictions_baseline, color=colors[0])\nplot_roc(\"Test Baseline\", test_labels, test_predictions_baseline, color=colors[0], linestyle='--')\nplt.legend(loc='lower right');","1acf750d":"total = neg + pos\nweight_for_0 = (1 \/ neg)*(total)\/2.0 \nweight_for_1 = (1 \/ pos)*(total)\/2.0\n\nclass_weight = {0: weight_for_0, 1: weight_for_1}\n\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))","c09a8b7c":"weighted_model = make_model()\nweighted_model.load_weights(initial_weights)\n\nweighted_history = weighted_model.fit(\n    train_features,\n    train_labels,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks = [early_stopping],\n    validation_data=(val_features, val_labels),\n \n    class_weight=class_weight) ","89a6bd91":"plot_metrics(weighted_history)","f0bcf4a9":"train_predictions_weighted = weighted_model.predict(train_features, batch_size=BATCH_SIZE)\ntest_predictions_weighted = weighted_model.predict(test_features, batch_size=BATCH_SIZE)","79227276":"weighted_results = weighted_model.evaluate(test_features, test_labels,\n                                           batch_size=BATCH_SIZE, verbose=0)\nfor name, value in zip(weighted_model.metrics_names, weighted_results):\n  print(name, ': ', value)\nprint()\n\nplot_cm(test_labels, test_predictions_weighted)","532cf5a3":"plot_roc(\"Train Baseline\", train_labels, train_predictions_baseline, color=colors[0])\nplot_roc(\"Test Baseline\", test_labels, test_predictions_baseline, color=colors[0], linestyle='--')\n\nplot_roc(\"Train Weighted\", train_labels, train_predictions_weighted, color=colors[1])\nplot_roc(\"Test Weighted\", test_labels, test_predictions_weighted, color=colors[1], linestyle='--')\n\n\nplt.legend(loc='lower right');","30be7ce1":"pos_features = train_features[bool_train_labels]\nneg_features = train_features[~bool_train_labels]\n\npos_labels = train_labels[bool_train_labels]\nneg_labels = train_labels[~bool_train_labels]","0d32e9e0":"ids = np.arange(len(pos_features))\nchoices = np.random.choice(ids, len(neg_features))\n\nres_pos_features = pos_features[choices]\nres_pos_labels = pos_labels[choices]\n\nres_pos_features.shape","a8486686":"resampled_features = np.concatenate([res_pos_features, neg_features], axis=0)\nresampled_labels = np.concatenate([res_pos_labels, neg_labels], axis=0)\n\norder = np.arange(len(resampled_labels))\nnp.random.shuffle(order)\nresampled_features = resampled_features[order]\nresampled_labels = resampled_labels[order]\n\nresampled_features.shape","1b5290ca":"BUFFER_SIZE = 100000\n\ndef make_ds(features, labels):\n  ds = tf.data.Dataset.from_tensor_slices((features, labels))#.cache()\n  ds = ds.shuffle(BUFFER_SIZE).repeat()\n  return ds\n\npos_ds = make_ds(pos_features, pos_labels)\nneg_ds = make_ds(neg_features, neg_labels)","457d6d35":"for features, label in pos_ds.take(1):\n  print(\"Features:\\n\", features.numpy())\n  print()\n  print(\"Label: \", label.numpy())","4d4b7730":"resampled_ds = tf.data.experimental.sample_from_datasets([pos_ds, neg_ds], weights=[0.5, 0.5])\nresampled_ds = resampled_ds.batch(BATCH_SIZE).prefetch(2)","e1393d09":"for features, label in resampled_ds.take(1):\n  print(label.numpy().mean())","5b527c69":"resampled_steps_per_epoch = np.ceil(2.0*neg\/BATCH_SIZE)\nresampled_steps_per_epoch","640083aa":"resampled_model = make_model()\nresampled_model.load_weights(initial_weights)\n\n# Reset the bias to zero, since this dataset is balanced.\noutput_layer = resampled_model.layers[-1] \noutput_layer.bias.assign([0])\n\nval_ds = tf.data.Dataset.from_tensor_slices((val_features, val_labels)).cache()\nval_ds = val_ds.batch(BATCH_SIZE).prefetch(2) \n\nresampled_history = resampled_model.fit(\n    resampled_ds,\n    epochs=EPOCHS,\n    steps_per_epoch=resampled_steps_per_epoch,\n    callbacks = [early_stopping],\n    validation_data=val_ds)","e1720391":"plot_metrics(resampled_history )","1f7fb7d2":"resampled_model = make_model()\nresampled_model.load_weights(initial_weights)\n\n# Reset the bias to zero, since this dataset is balanced.\noutput_layer = resampled_model.layers[-1] \noutput_layer.bias.assign([0])\n\nresampled_history = resampled_model.fit(\n    resampled_ds,\n    # These are not real epochs\n    steps_per_epoch = 20,\n    epochs=10*EPOCHS,\n    callbacks = [early_stopping],\n    validation_data=(val_ds))","06825837":"plot_metrics(resampled_history)","3dea6d36":"train_predictions_resampled = resampled_model.predict(train_features, batch_size=BATCH_SIZE)\ntest_predictions_resampled = resampled_model.predict(test_features, batch_size=BATCH_SIZE)","ec2a04a4":"resampled_results = resampled_model.evaluate(test_features, test_labels,\n                                             batch_size=BATCH_SIZE, verbose=0)\nfor name, value in zip(resampled_model.metrics_names, resampled_results):\n  print(name, ': ', value)\nprint()\n\nplot_cm(test_labels, test_predictions_weighted)","e9606098":"plot_roc(\"Train Baseline\", train_labels, train_predictions_baseline, color=colors[0])\nplot_roc(\"Test Baseline\", test_labels, test_predictions_baseline, color=colors[0], linestyle='--')\n\nplot_roc(\"Train Weighted\", train_labels, train_predictions_weighted, color=colors[1])\nplot_roc(\"Test Weighted\", test_labels, test_predictions_weighted, color=colors[1], linestyle='--')\n\nplot_roc(\"Train Resampled\", train_labels, train_predictions_resampled,  color=colors[2])\nplot_roc(\"Test Resampled\", test_labels, test_predictions_resampled,  color=colors[2], linestyle='--')\nplt.legend(loc='lower right')","369e9063":"Now we need normalize the input features.\nWe will use the sklearn StandardScaler.","343f6876":"## Evaluate metrics","f2993bf7":"### Checkpoint the initial weights","042fb4a3":"### Confirm that the bias fix helps - 20 epochs","46a9ab75":"## Class weights","b0812dc6":"Before moving on, confirm quick that the careful bias initialization actually helped.","3065cc24":"## Plot the ROC","96084aa2":"Drop the `Time` column and take the log of the `Amount` column to reduce its range.","74e4b1e8":"## Define the model & compile","97235643":"## Check training history","58ccf01c":"## Evaluate metrics\n","094ec097":"## Evaluate metrics","27dec38b":"Split the dataset into train, validation, and test sets. \n","112e602d":"## Re-train\n\nBecause training is easier on the balanced data, the above training procedure may overfit quickly. \n\nSo break up the epochs to give the `callbacks.EarlyStopping` finer control over when to stop training.","e7ea3116":"## Train the model","7ac845e4":"## Baseline model","834d0856":"## Read data using pandas\nPandas is a Python library. Can be used to download CSVs into a dataframe.","3f428a24":"This initial loss is many times less than if would have been with naive initilization.\n\nThis way the model doesn't need to spend the first few epochs just learning that positive examples are unlikely. This also makes it easier to read plots of the loss during training.","645a8052":"## Look at the data distribution","b8c4dc03":"## Oversampling\nA related approach would be to resample the dataset by oversampling the minority class.","64c30a8a":"## Train on the oversampled data","dc7953b8":"## Train a model with class weights","14c21617":"## Re-check training history","8104c597":"### Using Numpy ","2df15e2a":"## Plot the ROC","4f7ef805":"### Using tf.data ","1675446d":"## Plot the ROC","db7c59cb":"The goal is to identify fradulent transactions, but you don't have very many of those positive samples to work with, so you would want to have the classifier heavily weight the few examples that are available. You can do this by passing Keras weights for each class through a parameter. These will cause the model to \"pay more attention\" to examples from an under-represented class.","cfa940cb":"## Check the class label imbalance","975316e3":"### Optional: Set the correct initial bias.","28f23ce1":"## Prepare and normalize data for Keras","9c297086":"## Understanding useful metrics\n\nNotice that there are a few metrics defined above that can be computed by the model that will be helpful when evaluating the performance.\n\n\n\n*   **False** negatives and **false** positives are samples that were **incorrectly** classified\n*   **True** negatives and **true** positives are samples that were **correctly** classified\n*   **Accuracy** is the percentage of examples correctly classified\n>   $\\frac{\\text{true samples}}{\\text{total samples}}$\n*   **Precision** is the percentage of **predicted** positives that were correctly classified\n>   $\\frac{\\text{true positives}}{\\text{true positives + false positives}}$\n*   **Recall** is the percentage of **actual** positives that were correctly classified\n>   $\\frac{\\text{true positives}}{\\text{true positives + false negatives}}$\n*   **AUC** refers to the Area Under the Curve of a Receiver Operating Characteristic curve (ROC-AUC). This metric is equal to the probability that a classifier will rank a random positive sample higher than than a random negative sample.\n","3acc5839":"## Define the metrics","a69efa69":"## Check training history","966a7736":"## Check training history"}}