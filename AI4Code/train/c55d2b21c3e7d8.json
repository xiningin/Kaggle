{"cell_type":{"e2cdb4c6":"code","fc2972ab":"code","cdf88051":"code","616fab43":"code","932394db":"code","80b3b7f7":"code","79f21936":"code","bb7ee141":"code","cf611c13":"code","bdfd9c4d":"code","bb08ded0":"code","7587cb79":"code","9c90301e":"code","a73dc205":"code","34362d5b":"code","c06a7f9f":"code","466a7f5a":"code","975d5dc1":"code","9dfd613c":"code","2cb37b14":"code","aa1a515f":"code","0ca01b2f":"code","5f678379":"code","45dd4410":"code","5ec72f07":"code","af02a346":"code","b8f78054":"code","555fc018":"code","863a0791":"code","7ed457e5":"code","40e2204b":"code","c88b684f":"code","43b6ca09":"code","1a148793":"code","cc354b79":"code","ae8ee3aa":"code","3179699e":"code","adbe81c6":"code","2135214f":"code","dfc85dd5":"code","d791ed10":"code","e7bd63a4":"code","d862bc26":"code","ef2cb084":"code","44ff4ed0":"code","41aac68e":"code","10f101b9":"code","2e0003cb":"code","5e23c7a6":"markdown","26fd08ef":"markdown","4f759b17":"markdown","0c66b25c":"markdown","ef0ad2bb":"markdown","64c2a76f":"markdown","16cb9508":"markdown","15b5bf34":"markdown","6c3d5f0a":"markdown","7c6611d3":"markdown","091e4cfc":"markdown","1c51c897":"markdown","7d19a1dd":"markdown","e0496c1f":"markdown","465afd5c":"markdown","a3ef6f9d":"markdown","7c6055c9":"markdown","af88a9a2":"markdown","591862b0":"markdown","52c9f34c":"markdown","c5a39e45":"markdown","0bbbfbf5":"markdown","39760cdd":"markdown","f999bcbb":"markdown","d336d5b4":"markdown","25e2cf90":"markdown","df406ff9":"markdown"},"source":{"e2cdb4c6":"from numpy.random import seed\nseed(101)\nfrom tensorflow import set_random_seed\nset_random_seed(101)\n\nimport pandas as pd\nimport numpy as np\n\nimport tensorflow\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nimport os\nimport cv2\n\nimport imageio\nimport skimage\nimport skimage.io\nimport skimage.transform\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","fc2972ab":"# Number of samples we will have in each class.\nSAMPLE_SIZE = 250\n\n# The images will all be resized to this size.\nIMAGE_SIZE = 96","cdf88051":"os.listdir('..\/input\/nonsegmentedv2')","616fab43":"# get a list of image folders\nfolder_list = os.listdir('..\/input\/nonsegmentedv2')\n\ntotal_images = 0\n\n# loop through each folder\nfor folder in folder_list:\n    # set the path to a folder\n    path = '..\/input\/nonsegmentedv2\/' + str(folder)\n    # get a list of images in that folder\n    images_list = os.listdir(path)\n    # get the length of the list\n    num_images = len(images_list)\n    \n    total_images = total_images + num_images\n    # print the result\n    print(str(folder) + ':' + ' ' + str(num_images))\n    \nprint('\\n')\n# print the total number of images available\nprint('Total Images: ', total_images)\n    ","932394db":"# Create a new directory to store all available images\nall_images_dir = 'all_images_dir'\nos.mkdir(all_images_dir)\n","80b3b7f7":"# check that the new directory has been created\n!ls","79f21936":"# This code copies all images from their seperate folders into the same \n# folder called all_images_dir.\n\n\nfolder_list = os.listdir('..\/input\/nonsegmentedv2')\n\nfor folder in folder_list:\n    \n    # create a path to the folder\n    path = '..\/input\/nonsegmentedv2\/' + str(folder)\n\n    # create a list of all files in the folder\n    file_list = os.listdir(path)\n\n    # move the 0 images to all_images_dir\n    for fname in file_list:\n\n        # source path to image\n        src = os.path.join(path, fname)\n        \n        # Change the file name because many images have the same file name.\n        # Add the folder name to the existing file name.\n        new_fname = str(folder) + '_' + fname\n        \n        # destination path to image\n        dst = os.path.join(all_images_dir, new_fname)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n\n","bb7ee141":"# Check how many images are in all_images_dir.\n# Should be 5539.\n\nlen(os.listdir('all_images_dir'))","cf611c13":"# Get a list of all images in the all_images_dir folder.\nimage_list = os.listdir('all_images_dir')\n\n# Create the dataframe.\ndf_data = pd.DataFrame(image_list, columns=['image_id'])\n\ndf_data.head()","bdfd9c4d":"\n# Each file name has this format:\n# Loose Silky-bent_377.png\n\n# This function will extract the class name from the file name of each image.\ndef extract_target(x):\n    # split into a list\n    a = x.split('_')\n    # the target is the first index in the list\n    target = a[0]\n    \n    return target\n\n\n# create a new column called 'target'\ndf_data['target'] = df_data['image_id'].apply(extract_target)\n\ndf_data.head()","bb08ded0":"df_data.shape","7587cb79":"# source: https:\/\/www.kaggle.com\/gpreda\/honey-bee-subspecies-classification\n\n\ndef draw_category_images(col_name,figure_cols, df, IMAGE_PATH):\n    \n    \"\"\"\n    Give a column in a dataframe,\n    this function takes a sample of each class and displays that\n    sample on one row. The sample size is the same as figure_cols which\n    is the number of columns in the figure.\n    Because this function takes a random sample, each time the function is run it\n    displays different images.\n    \"\"\"\n    \n\n    categories = (df.groupby([col_name])[col_name].nunique()).index\n    f, ax = plt.subplots(nrows=len(categories),ncols=figure_cols, \n                         figsize=(4*figure_cols,4*len(categories))) # adjust size here\n    # draw a number of images for each location\n    for i, cat in enumerate(categories):\n        sample = df[df[col_name]==cat].sample(figure_cols) # figure_cols is also the sample size\n        for j in range(0,figure_cols):\n            file=IMAGE_PATH + sample.iloc[j]['image_id']\n            im=cv2.imread(file)\n            ax[i, j].imshow(im, resample=True, cmap='gray')\n            ax[i, j].set_title(cat, fontsize=16)  \n    plt.tight_layout()\n    plt.show()","9c90301e":"IMAGE_PATH = 'all_images_dir\/'\n\ndraw_category_images('target',4, df_data, IMAGE_PATH)","a73dc205":"# What is the class distribution?\n\ndf_data['target'].value_counts()","34362d5b":"\n# Get a list of classes\ntarget_list = os.listdir('..\/input\/nonsegmentedv2')\n\nfor target in target_list:\n\n    # Filter out a target and take a random sample\n    df = df_data[df_data['target'] == target].sample(SAMPLE_SIZE, random_state=101)\n    \n    # if it's the first item in the list\n    if target == target_list[0]:\n        df_sample = df\n    else:\n        # Concat the dataframes\n        df_sample = pd.concat([df_sample, df], axis=0).reset_index(drop=True)\n","c06a7f9f":"# Display the balanced classes.\n\ndf_sample['target'].value_counts()","466a7f5a":"# train_test_split\n\n# stratify=y creates a balanced validation set.\ny = df_sample['target']\n\ndf_train, df_val = train_test_split(df_sample, test_size=0.10, random_state=101, stratify=y)\n\nprint(df_train.shape)\nprint(df_val.shape)","975d5dc1":"# Train set class distribution\n\ndf_train['target'].value_counts()","9dfd613c":"# Val set class distribution\n\ndf_val['target'].value_counts()","2cb37b14":"folder_list = os.listdir('..\/input\/nonsegmentedv2')\n\nfolder_list","aa1a515f":"# Create a new directory\nbase_dir = 'base_dir'\nos.mkdir(base_dir)\n\n\n#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n\n# now we create 2 folders inside 'base_dir':\n\n# train_dir\n    # Maize\n    # Fat Hen\n    # Shepherd\u2019s Purse\n    # Common Chickweed\n    # Cleavers\n    # Charlock\n    # Loose Silky-bent\n    # Small-flowered Cranesbill\n    # Black-grass\n    # Scentless Mayweed\n    # Sugar beet\n    # Common wheat\n\n# val_dir\n    # Maize\n    # Fat Hen\n    # Shepherd\u2019s Purse\n    # Common Chickweed\n    # Cleavers\n    # Charlock\n    # Loose Silky-bent\n    # Small-flowered Cranesbill\n    # Black-grass\n    # Scentless Mayweed\n    # Sugar beet\n    # Common wheat\n\n\n# create a path to 'base_dir' to which we will join the names of the new folders\n# train_dir\ntrain_dir = os.path.join(base_dir, 'train_dir')\nos.mkdir(train_dir)\n\n# val_dir\nval_dir = os.path.join(base_dir, 'val_dir')\nos.mkdir(val_dir)\n\n\n# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\n\n# create new folders inside train_dir\n\nfor folder in folder_list:\n    \n    folder = os.path.join(train_dir, str(folder))\n    os.mkdir(folder)\n\n\n# create new folders inside val_dir\n\nfor folder in folder_list:\n    \n    folder = os.path.join(val_dir, str(folder))\n    os.mkdir(folder)","0ca01b2f":"# check that the folders have been created\n\nos.listdir('base_dir\/train_dir')","5f678379":"# Set the id as the index in df_data\ndf_data.set_index('image_id', inplace=True)","45dd4410":"df_data.head()","5ec72f07":"# Get a list of train and val images\ntrain_list = list(df_train['image_id'])\nval_list = list(df_val['image_id'])\n\n# Transfer the train images\n\nfor image in train_list:\n    \n    # the id in the csv file does not have the .tif extension therefore we add it here\n    fname = image\n    # get the label for a certain image\n    folder = df_data.loc[image,'target']\n    \n    \n    # source path to image\n    src = os.path.join(all_images_dir, fname)\n    # destination path to image\n    dst = os.path.join(train_dir, folder, fname)\n    \n    # resize the image and save it at the new location\n    image = cv2.imread(src)\n    image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n    # save the image at the destination\n    cv2.imwrite(dst, image)\n        \n    \n\n# Transfer the val images\n\nfor image in val_list:\n    \n    # the id in the csv file does not have the .tif extension therefore we add it here\n    fname = image\n    # get the label for a certain image\n    folder = df_data.loc[image,'target']\n    \n\n    # source path to image\n    src = os.path.join(all_images_dir, fname)\n    # destination path to image\n    dst = os.path.join(val_dir, folder, fname)\n    \n    # resize the image and save it at the new location\n    image = cv2.imread(src)\n    image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n    # save the image at the destination\n    cv2.imwrite(dst, image)\n\n    ","af02a346":"# get a list of image folders\nfolder_list = os.listdir('base_dir\/train_dir')\n\ntotal_images = 0\n\n# loop through each folder\nfor folder in folder_list:\n    # set the path to a folder\n    path = 'base_dir\/train_dir\/' + str(folder)\n    # get a list of images in that folder\n    images_list = os.listdir(path)\n    # get the length of the list\n    num_images = len(images_list)\n    \n    total_images = total_images + num_images\n    # print the result\n    print(str(folder) + ':' + ' ' + str(num_images))\n    \nprint('\\n')\n# print the total number of images available\nprint('Total Images: ', total_images)","b8f78054":"# get a list of image folders\nfolder_list = os.listdir('base_dir\/val_dir')\n\ntotal_images = 0\n\n# loop through each folder\nfor folder in folder_list:\n    # set the path to a folder\n    path = 'base_dir\/val_dir\/' + str(folder)\n    # get a list of images in that folder\n    images_list = os.listdir(path)\n    # get the length of the list\n    num_images = len(images_list)\n    \n    total_images = total_images + num_images\n    # print the result\n    print(str(folder) + ':' + ' ' + str(num_images))\n    \nprint('\\n')\n# print the total number of images available\nprint('Total Images: ', total_images)","555fc018":"# End of Data Preparation\n### ================================================================================== ###\n# Start of Model Building","863a0791":"train_path = 'base_dir\/train_dir'\nvalid_path = 'base_dir\/val_dir'\n\n\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = 10\nval_batch_size = 10\n\n\ntrain_steps = np.ceil(num_train_samples \/ train_batch_size)\nval_steps = np.ceil(num_val_samples \/ val_batch_size)","7ed457e5":"datagen = ImageDataGenerator(rescale=1.0\/255)\n\ntrain_gen = datagen.flow_from_directory(train_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=train_batch_size,\n                                        class_mode='categorical')\n\nval_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=val_batch_size,\n                                        class_mode='categorical')\n\n# Note: shuffle=False causes the test dataset to not be shuffled\ntest_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=1,\n                                        class_mode='categorical',\n                                        shuffle=False)","40e2204b":"# Source: https:\/\/www.kaggle.com\/fmarazzi\/baseline-keras-cnn-roc-fast-5min-0-8253-lb\n\nkernel_size = (3,3)\npool_size= (2,2)\nfirst_filters = 32\nsecond_filters = 64\nthird_filters = 128\n\ndropout_conv = 0.3\ndropout_dense = 0.3\n\n\nmodel = Sequential()\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu', \n                 input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)))\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size)) \nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(dropout_dense))\nmodel.add(Dense(12, activation = \"softmax\"))\n\nmodel.summary()","c88b684f":"model.compile(Adam(lr=0.0001), loss='categorical_crossentropy', \n              metrics=['accuracy'])\n","43b6ca09":"filepath = \"model.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n                             save_best_only=True, mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=3, \n                                   verbose=1, mode='max', min_lr=0.00001)\n                              \n                              \ncallbacks_list = [checkpoint, reduce_lr]\n\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n                    validation_data=val_gen,\n                    validation_steps=val_steps,\n                    epochs=40, verbose=1,\n                   callbacks=callbacks_list)","1a148793":"# get the metric names so we can use evaulate_generator\nmodel.metrics_names","cc354b79":"# Print the validation loss and accuracy.\n\n# Here the best epoch will be used.\nmodel.load_weights('model.h5')\n\nval_loss, val_acc = \\\nmodel.evaluate_generator(test_gen, \n                        steps=len(df_val))\n\nprint('val_loss:', val_loss)\nprint('val_acc:', val_acc)","ae8ee3aa":"# display the loss and accuracy curves\n\nimport matplotlib.pyplot as plt\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()","3179699e":"# make a prediction\npredictions = model.predict_generator(test_gen, steps=len(df_val), verbose=1)","adbe81c6":"predictions.shape","2135214f":"# This is how to check what index keras has internally assigned to each class. \ntest_gen.class_indices\n","dfc85dd5":"# Put the predictions into a dataframe.\n# The columns need to be ordered to match the output of the previous cell\n\nclass_dict = train_gen.class_indices\n\n# Get a list of the dict keys.\ncols = class_dict.keys()\n\ndf_preds = pd.DataFrame(predictions, columns=cols)\n\ndf_preds.head()","d791ed10":"# Get the labels of the test images.\n\ntest_labels = test_gen.classes","e7bd63a4":"# Source: Scikit Learn website\n# http:\/\/scikit-learn.org\/stable\/auto_examples\/\n# model_selection\/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n# selection-plot-confusion-matrix-py\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n    \n    # set the size of the figure here\n    plt.figure(figsize=(15,10))\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=80) # set x-axis text angle here\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","d862bc26":"# argmax returns the index of the max value in a row\ncm = confusion_matrix(test_labels, predictions.argmax(axis=1))","ef2cb084":"# Define the labels of the class indices. These need to match the \n# order shown above.\ncm_plot_labels = cols\n\nplot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')\n","44ff4ed0":"from sklearn.metrics import classification_report\n\n# Generate a classification report\n\n# Get the true labels\ny_true = test_gen.classes\n\n# For this to work we need y_pred as binary labels not as probabilities\ny_pred_binary = predictions.argmax(axis=1)\n\nreport = classification_report(y_true, y_pred_binary, target_names=cm_plot_labels)\n\nprint(report)","41aac68e":"!pip install tensorflowjs","10f101b9":"# Use the command line conversion tool to convert the model\n\n!tensorflowjs_converter --input_format keras model.h5 tfjs_model\/model","2e0003cb":"# Delete all_images_dir and base_dir directory to prevent a Kaggle error.\n# Kaggle allows a max of 500 files to be saved.\n\nshutil.rmtree('all_images_dir')\nshutil.rmtree('base_dir')","5e23c7a6":"### Set Up the Generators","26fd08ef":"### Transfer the images into the folders\u00b6","4f759b17":"### Create the train and  val sets\n","0c66b25c":"### How many images are there in each folder?","ef0ad2bb":"### A Weed Detection Web App","64c2a76f":"### Introduction\n\nThe goal of this kernel is to build a Keras cnn model to classify 12 kinds of plant seedlings.\n\nWe will use the V2 Plant Seedlings Dataset. This dataset contains 5,539 images distributed between 12 classes. The images show plant seedlings at different growth stages. Of the 12 classes, 3 classes are crop seedlings and 9 are weed seedlings. \n\nThe images are in different sizes. We will resize all images to 96x96 and use only 250 images from each class. We won't do any image augmentation. \n\nThis kernel will focus on:\n\n- Creating the folder structure that Keras generators need.\n- Creating generators to feed the images from the folders into the model.\n- Model building and training.\n- Assessing the quality of the model by generating a confusion matrix and a classification report.\n\n### Results\n\nThis simple model will produce a validation accuracy that is greater than 90% and an F1 score of approximately 0.75. \n\n***\n\n","16cb9508":"**Recall **= Given a class, will the classifier be able to detect it?<br>\n**Precision** = Given a class prediction from a classifier, how likely is it to be correct?<br>\n**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.\n","15b5bf34":"### Plot the Training Curves","6c3d5f0a":"### What folders are available?\n\nThe images are grouped into 12 folders by plant specie. ","7c6611d3":"### Conclusion","091e4cfc":"### Create the Model Architecture","1c51c897":"### Check how many val images are in each folder","7d19a1dd":"### Create a dataframe containing all the information","e0496c1f":"### Balance the class distribution\nWe will use 250 images from each class.","465afd5c":"### Convert the model from Keras to Tensorflowjs\nThis conversion needs to be done so that the model can be loaded into the web app.","a3ef6f9d":"### Evaluate the model using the val set","7c6055c9":"### Copy all images into one directory\nThis will make it easier to work with this data.","af88a9a2":"### Check how many train images are in each folder","591862b0":"### Make a prediction on the val set\nWe need these predictions to print the Confusion Matrix and calculate the F1 score.","52c9f34c":"### Create a Directory Structure","c5a39e45":"It may be possible to improve the performance of this model by doing the following:<br>\n- using more of the available data\n- using a larger image size\n- doing image augmentation\n- using a pre-trained model\n- parameter tuning\n\nThank you for reading. Merry Christmas.","0bbbfbf5":"### Display a random sample of 4 train images for each class\n\nHere we will see what the images in each class look like. Take note of the similar appearance between Black-grass and Loose Silky-bent. We will see later in the confusion matrix and classification report that the model will struggle to seperate these two classes.","39760cdd":"### Train the Model","f999bcbb":"### Create a Confusion Matrix","d336d5b4":"From the confusion matrix and classification report we see that the model is mis-classifying many Black-grass images as Loose Silky-bent. This may be because these plant seedlings look similar. Sheperd's Purse is another class that the model is struggling to classify correctly. One possible soluton may be to add more Black-grass and Sheperd's Purse images to the training set.\n\nBy generating the confusion matrix and F1 score we can see how the model is performing on a class by class basis. The accuracy score alone cannot give us these insights into the models strengths and weaknesses. ","25e2cf90":"I've built a prototype web app using this model. This may be something that farmers could use to detect weed seedlings. The user is able to submit a photo of a seedling and get an instant prediction indicating what kind of seedling it is. The app will probably not generalize very well but this shows how easy it is for an ordinary person to build an Ai product using the technology that's available today. \n\nAll the code is available on Github. The technology that enables this app to work is new. Therefore, I recommend using the latest version of the Chrome browser. When using Safari for example, you may see a message indicating that the model is loading but the app may actually be frozen.\n\nWeb app:<br>\nhttp:\/\/plant.test.woza.work\/<br>\nGithub:<br>\nhttps:\/\/github.com\/vbookshelf\/Weed-Detector\n\n\n\n","df406ff9":"### Create a Classification Report"}}