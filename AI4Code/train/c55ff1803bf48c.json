{"cell_type":{"9e7e6171":"code","794c7551":"code","18619d1d":"code","c465534a":"code","a708f260":"code","7b56adb3":"code","99cb1dac":"code","144c5a28":"code","4e03be8f":"code","d2e1f9de":"code","e4a047aa":"code","0c956888":"code","262d85e6":"code","5f25f3b8":"code","1fce9ed3":"code","70ab5b61":"code","ae325594":"code","0ed7bf32":"code","80e32000":"code","4d60305b":"code","fcaa520b":"code","4ec53d81":"code","bd406276":"code","085366d1":"code","b6d5615e":"code","be8f5152":"code","9582cc98":"code","b7a040cf":"code","3c95a505":"code","bf90733b":"code","067e940e":"code","dc5d477c":"code","8c630858":"code","05787ae7":"code","cd1e7e66":"code","45f317d8":"code","f666ded1":"code","d9eb485c":"code","2ccbf549":"code","cd0aaeba":"code","e470a148":"code","9573e83f":"code","7f54c62e":"code","1ed65857":"code","48fb77fc":"code","60408c21":"code","c2f94638":"code","4d0818ca":"code","d04f2a02":"code","8d777700":"code","e071917c":"code","1764acdf":"code","9f5d721b":"code","0c00a915":"code","6da0d9bb":"code","f6d7bd31":"code","a3aa16c4":"code","66f959a9":"code","a10baa72":"code","60abd87a":"code","6b60c038":"code","6d07e5a9":"code","5cf24738":"code","de48bd2a":"code","3c9899a2":"code","afd00622":"code","cedd6c25":"code","fff4b5c1":"code","698f7a90":"code","ad491725":"code","801386de":"code","59fc571e":"code","9aa8f6dc":"code","ca593662":"code","44197c2c":"code","65237fad":"code","ef9fc8ee":"code","aa9abcf0":"code","65a7b03a":"code","fd6d9783":"code","cf1d57cc":"code","0ea1b9b6":"code","a4fb888d":"code","c982906d":"code","261456c3":"code","8488fc18":"code","1d507335":"code","308f7c8f":"code","ee156f89":"code","9ae24d9f":"code","1f1678a8":"code","1cf64c9a":"code","4bcf6c08":"code","14823c3f":"code","4dac05a6":"code","08465bb5":"code","2b7cb88c":"code","62120879":"code","679342bc":"code","ea46f5e8":"code","4018233a":"code","e536bacd":"code","2571550f":"code","57d5e4d1":"code","cfe0a0ba":"code","b3efdff1":"code","2602578c":"code","a3739171":"code","297839af":"code","d7e90cab":"code","1f910af1":"code","509edfc4":"code","c04b875f":"code","df25a7f4":"code","7ef24713":"code","b809083c":"code","8eac3598":"code","a7d746c8":"code","c3014143":"code","d3ac1869":"code","159a36c6":"code","b0554c77":"code","b09c3a70":"code","7fc43986":"code","9468fd8f":"code","5e46953f":"code","05967be5":"code","681d70c9":"code","d683cf3d":"code","93651033":"code","4fdf2207":"code","4f032d41":"code","0ca64c42":"code","34a3c2b5":"code","68a0227d":"code","ce601a57":"code","b264452a":"code","57cf89d8":"markdown","4dd4c931":"markdown","ba098c36":"markdown","9ca2232d":"markdown","dadb6b49":"markdown","9175c881":"markdown","e83fce91":"markdown","5be569cf":"markdown","d0572b4f":"markdown","b40de884":"markdown","352006a5":"markdown","adc40c19":"markdown","7694d1f5":"markdown","416beac1":"markdown","75ca3e8f":"markdown","d76cd7e1":"markdown","a9880e39":"markdown","21f4fcd2":"markdown","d1febdf2":"markdown","7a2f2364":"markdown","8cac35a8":"markdown","8ba4345b":"markdown","6aa02a6f":"markdown","0d219246":"markdown","a2dad788":"markdown","ef11709e":"markdown","b6093424":"markdown","e48beb6a":"markdown","85e1007e":"markdown","2ab75ba6":"markdown","f3076fa7":"markdown","b8fb8217":"markdown","2a036b26":"markdown","1bc6178b":"markdown","368221bb":"markdown","f6733540":"markdown","9d4bf6c7":"markdown","d91d49a9":"markdown","82b97642":"markdown","9e7af854":"markdown","7bf5386b":"markdown","54a62a25":"markdown","05d4d11f":"markdown","d567d305":"markdown","3656d5d5":"markdown","dc9a4276":"markdown","8fa952e2":"markdown","f157cc34":"markdown","df31e7ee":"markdown","ea7b1353":"markdown","47b8e114":"markdown","a941e2bf":"markdown","15914e2e":"markdown","bc708c80":"markdown","8e25d19d":"markdown","9cc56114":"markdown","eaa66c89":"markdown","d25c9362":"markdown","a4d0fa50":"markdown","6bbf6dc4":"markdown","a050190b":"markdown","287a892e":"markdown","c210bbb7":"markdown","1c196f71":"markdown","e70e0df2":"markdown","5a47912b":"markdown","a081db19":"markdown","2fdfab1a":"markdown","e940c29b":"markdown","aa2f0b06":"markdown","0a51067c":"markdown","4aba8587":"markdown","f8bb9ae3":"markdown","e75a0c74":"markdown","36512c9c":"markdown","f50ff17f":"markdown","dfba2578":"markdown","c5277477":"markdown","d1713d8f":"markdown","b58c24ab":"markdown","c6c8c6c0":"markdown","2e7961d0":"markdown","52245198":"markdown","cc706c15":"markdown","bca8abbf":"markdown","67b32c58":"markdown","750c0689":"markdown","058ef393":"markdown","149b42a5":"markdown","2314242f":"markdown","61104996":"markdown","98897935":"markdown","dee0ffcb":"markdown","4dff2fcc":"markdown","552d649f":"markdown","da005718":"markdown","270e27c1":"markdown","21dfe818":"markdown","856deb32":"markdown","9b823d21":"markdown","b9cc8291":"markdown","fe57aa79":"markdown","ebad15e1":"markdown","123ac8d4":"markdown","9791efb4":"markdown","3940a0df":"markdown","61537435":"markdown","156f30de":"markdown","3d36c303":"markdown","9e6c2c24":"markdown","e8d55125":"markdown","dfdf7dbd":"markdown","7d003596":"markdown","8dc027b7":"markdown","7c8642ea":"markdown","c8da14fb":"markdown","17809450":"markdown","26bd8222":"markdown","3f385f36":"markdown","13502eef":"markdown","26ea48a6":"markdown","aff88501":"markdown","fdf8458b":"markdown","9477b2ba":"markdown","adf7b4c8":"markdown","911fc522":"markdown","019c9a81":"markdown","44e33349":"markdown","9845301a":"markdown","b95af98d":"markdown","3637cd06":"markdown","c1af31ae":"markdown","14399f7c":"markdown","a983cf32":"markdown"},"source":{"9e7e6171":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.offline as py\nimport plotly.graph_objs as go\nfrom sklearn.preprocessing import MinMaxScaler","794c7551":"Churn_train = pd.read_csv(r\"..\/input\/traindata\/train.csv\")\nChurn_train.head()","18619d1d":"Churn_test = pd.read_csv(r\"..\/input\/testdata\/test.csv\")\nChurn_test.head()","c465534a":"print (\"Dataset  : TRAINING set \")\nprint (\"Rows     : \" ,Churn_train.shape[0])\nprint (\"Columns  : \" ,Churn_train.shape[1])\nprint (\"\\nFeatures : \\n\" ,Churn_train.columns.tolist())\nprint (\"\\nTotal Missing values :  \", Churn_train.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",Churn_train.nunique())\nChurn_train.head(3)","a708f260":"print (\"Dataset  : TESTING set \")\nprint (\"Rows     : \" ,Churn_test.shape[0])\nprint (\"Columns  : \" ,Churn_test.shape[1])\nprint (\"\\nFeatures : \\n\" ,Churn_test.columns.tolist())\nprint (\"\\nTotal Missing values :  \", Churn_test.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",Churn_test.nunique())\nChurn_test.head(3)","7b56adb3":"print(\"Quantity of missing values per variable in Train and Test set\")\nprint(\"\\n\")\nprint(Churn_train.isna().sum())\nprint(\"\\n\")\nprint(Churn_test.isna().sum())\nprint(\"\\n\")\nprint(\"Percentage of missing values per variable in Train and Test set\")\nprint(\"\\n\")\nprint(Churn_train.isna().sum()\/len(Churn_train)*100)\nprint(\"\\n\")\nChurn_train.isna().sum()\/len(Churn_train)*100","99cb1dac":"print(\"Duplicate records in Training: \", Churn_train[Churn_train.duplicated(keep=False)])\nprint(\"\\n\")\nprint(\"Duplicate records in Training: \", Churn_test[Churn_test.duplicated(keep=False)])","144c5a28":"Churn_train_ID=Churn_train['Employee_ID']\nprint(\"Duplicate IDs: \", Churn_train_ID[Churn_train_ID.duplicated(keep=False)])","4e03be8f":"Churn_train.set_index('Employee_ID', inplace=True)\nChurn_train.head(3)","d2e1f9de":"Churn_test.set_index('Employee_ID', inplace=True)\nChurn_test.head(3)","e4a047aa":"Churn_train.info()","0c956888":"Churn_train.describe(include=['O']).T","262d85e6":"print(Churn_train['Gender'].value_counts())\nprint('\\n')\nprint(Churn_train['Marital_status'].value_counts())\nprint('\\n')\nprint(Churn_train['Department'].value_counts())\nprint('\\n')\nChurn_train['Churn_risk'].value_counts()","5f25f3b8":"#labels\nlab = Churn_train[\"Churn_risk\"].value_counts().keys().tolist()\n#values\nval = Churn_train[\"Churn_risk\"].value_counts().values.tolist()\n\ntrace = go.Pie(labels = lab ,\n               values = val ,\n               marker = dict(colors =  [ 'cornflowerlblue' ,'lightblue', 'red'],\n                             line = dict(color = \"white\",\n                                         width =  1.3)\n                            ),\n               rotation = 90,\n               hoverinfo = \"label+value+text\",\n               hole = .5\n              )\nlayout = go.Layout(dict(title = \"Employee Churn_risk in data\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                       )\n                  )\n\ndata = [trace]\nfig = go.Figure(data = data,layout = layout)\npy.iplot(fig)","1fce9ed3":"df = Churn_train.copy()\nId_col     = ['Employee_ID']\n\nsummary = (df[[i for i in df.columns if i not in Id_col]].\n           describe().transpose().reset_index())\n\nsummary = summary.rename(columns = {\"index\" : \"feature\"})\nsummary = np.around(summary,3)\n\nval_lst = [summary['feature'], summary['count'],\n           summary['mean'],summary['std'],\n           summary['min'], summary['25%'],\n           summary['50%'], summary['75%'], summary['max']]\n\ntrace  = go.Table(header = dict(values = summary.columns.tolist(),\n                                line = dict(color = ['white']),\n                                fill = dict(color = ['steelblue']),\n                               ),\n                  cells  = dict(values = val_lst,\n                                line = dict(color = ['white']),\n                                fill = dict(color = [\"yellow\",'lavender'])\n                               ),\n                  columnwidth = [200,60,100,100,60,60,80,80,80])\nlayout = go.Layout(dict(title = \"Numerical Variable Summary\"))\nfigure = go.Figure(data=[trace],layout=layout)\npy.iplot(figure)","70ab5b61":"#Binning variables\ndef overtime_lab(df) :\n    if df[\"Overtime\"] <= 9.5 :\n        return \"Overtime_0-9.5\"\n    elif (df[\"Overtime\"] > 9.5) & (df[\"Overtime\"] <= 10.3 ):\n        return \"Overtime_9.5-10.3\"\n    elif (df[\"Overtime\"] > 10.3) & (df[\"Overtime\"] <= 11.3) :\n        return \"Overtime_10.3-11.3\"\n    elif df[\"Overtime\"] > 14.2:\n        return \"Overtime_gt_14.2\"\ndf[\"overtime_group\"] = df.apply(lambda df:overtime_lab(df), axis = 1)\n\ndef tenure_lab(df) :\n    if df[\"Tenure\"] <= 8 :\n        return \"Tenure_0-8\"\n    elif (df[\"Tenure\"] > 8) & (df[\"Tenure\"] <= 13 ):\n        return \"Tenure_8-13\"\n    elif (df[\"Tenure\"] > 13) & (df[\"Tenure\"] <= 17) :\n        return \"Tenure_13-17\"\n    elif (df[\"Tenure\"] > 17) & (df[\"Tenure\"] <= 41) :\n        return \"Tenure_17-41\"\n    elif df[\"Tenure\"] > 41 :\n        return \"Tenure_gt_41\"\ndf[\"tenure_group\"] = df.apply(lambda df:tenure_lab(df),axis = 1)\n\ndef age_lab(df) :\n    if df[\"Age\"] <= 33 :\n        return \"young_adults\"\n    elif (df[\"Age\"] > 33) & (df[\"Age\"] <= 74 ):\n        return \"older_adults\"\n    elif df[\"Age\"] > 74 :\n        return \"Age_gt_4\"\ndf[\"age_group\"] = df.apply(lambda df:age_lab(df), axis = 1)\n  \n\n#Separating churn and non churn employee\nchurn_low     = df[df[\"Churn_risk\"] == \"low\"]\nchurn_medium  = df[df[\"Churn_risk\"] == \"medium\"]\nchurn_high    = df[df[\"Churn_risk\"] == \"high\"]\n\n#Separating catagorical and numerical columns\nId_col     = ['Employee_ID']\ntarget_col = [\"Churn_risk\"]\ncat_cols   = df.nunique()[df.nunique() < 8].keys().tolist()\ncat_cols   = [x for x in cat_cols if x not in target_col]\nnum_cols   = [x for x in df.columns if x not in cat_cols + target_col + Id_col]","ae325594":"#employee churn in age\ntg_low  =  churn_low[\"age_group\"].value_counts().reset_index()\ntg_low.columns  = [\"age_group\",\"count\"]\ntg_medium =  churn_medium[\"age_group\"].value_counts().reset_index()\ntg_medium.columns = [\"age_group\",\"count\"]\ntg_high =  churn_high[\"age_group\"].value_counts().reset_index()\ntg_high.columns = [\"age_group\",\"count\"]\n\n#bar - low churn\ntrace1 = go.Bar(x = tg_low[\"age_group\"]  , y = tg_low[\"count\"],\n                name = \"Low Churn Employee\",\n                marker = dict(line = dict(width = .5,color = \"black\")),\n                marker_color='lightblue',\n                opacity = .9)\n\n#bar - medium churn\ntrace2 = go.Bar(x = tg_medium[\"age_group\"] , y = tg_medium[\"count\"],\n                name = \"Medium Churn Employee\",\n                marker = dict(line = dict(width = .5,color = \"black\")),\n                marker_color='cornflowerblue',\n                opacity = .9)\n\n#bar - high churn\ntrace3 = go.Bar(x = tg_high[\"age_group\"] , y = tg_high[\"count\"],\n                name = \"High Churn Employee\",\n                marker = dict(line = dict(width = .5,color = \"black\")),\n                marker_color='red',\n                opacity = .9)\n\nlayout = go.Layout(dict(title = \"Employee churn in age_group\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"age_group\",\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"count\",\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                       )\n                  )\ndata = [trace1, trace2, trace3]\nfig  = go.Figure(data=data,layout=layout)\npy.iplot(fig)","0ed7bf32":"#labels\nlab = df[\"age_group\"].value_counts().keys().tolist()\n#values\nval = df[\"age_group\"].value_counts().values.tolist()\n\ntrace = go.Pie(labels = lab ,\n               values = val ,\n               marker = dict(colors =  [ 'lightcoral', 'maroon'],\n                             line = dict(color = \"white\",\n                                         width =  1.3)\n                            ),\n               rotation = 90,\n               hoverinfo = \"label+value+text\",\n               hole = .5\n              )\nlayout = go.Layout(dict(title = \"Employee age_group in data\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                       )\n                  )\n\ndata = [trace]\nfig = go.Figure(data = data,layout = layout)\npy.iplot(fig)","80e32000":"#employee churn in Kids\ntg_low  =  churn_low[\"Kids\"].value_counts().reset_index()\ntg_low.columns  = [\"Kids\",\"count\"]\ntg_medium =  churn_medium[\"Kids\"].value_counts().reset_index()\ntg_medium.columns = [\"Kids\",\"count\"]\ntg_high =  churn_high[\"Kids\"].value_counts().reset_index()\ntg_high.columns = [\"Kids\",\"count\"]\n\n#bar - low churn\ntrace1 = go.Bar(x = tg_low[\"Kids\"]  , y = tg_low[\"count\"],\n                name = \"Low Churn Employees\",\n                marker = dict(line = dict(width = .5,color = \"black\")),\n                marker_color='lightblue',\n                opacity = .9)\n\n#bar - medium churn\ntrace2 = go.Bar(x = tg_medium[\"Kids\"] , y = tg_medium[\"count\"],\n                name = \"Medium Churn Employees\",\n                marker = dict(line = dict(width = .5,color = \"black\")),\n                marker_color='cornflowerblue',\n                opacity = .9)\n\n#bar - high churn\ntrace3 = go.Bar(x = tg_high[\"Kids\"] , y = tg_high[\"count\"],\n                name = \"High Churn Employees\",\n                marker = dict(line = dict(width = .5,color = \"black\")),\n                marker_color='red',\n                opacity = .9)\n\nlayout = go.Layout(dict(title = \"Employee churn in Kids\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"Kids\",\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"count\",\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                       )\n                  )\ndata = [trace1, trace2, trace3]\nfig  = go.Figure(data=data,layout=layout)\npy.iplot(fig)","4d60305b":"#Employee churn in tenure groups\ntg_low  =  churn_low[\"tenure_group\"].value_counts().reset_index()\ntg_low.columns  = [\"tenure_group\",\"count\"]\ntg_medium =  churn_medium[\"tenure_group\"].value_counts().reset_index()\ntg_medium.columns = [\"tenure_group\",\"count\"]\ntg_high =  churn_high[\"tenure_group\"].value_counts().reset_index()\ntg_high.columns = [\"tenure_group\",\"count\"]\n\n#bar - low churn\ntrace1 = go.Bar(x = tg_low[\"tenure_group\"]  , y = tg_low[\"count\"],\n                name = \"Low Churn Employees\",\n                marker = dict(line = dict(width = .5,color = \"black\")),\n                marker_color='lightblue',\n                opacity = .9)\n\n#bar - medium churn\ntrace2 = go.Bar(x = tg_medium[\"tenure_group\"] , y = tg_medium[\"count\"],\n                name = \"Medium Churn Employees\",\n                marker = dict(line = dict(width = .5,color = \"black\")),\n                marker_color='cornflowerblue',\n                opacity = .9)\n\n#bar - high churn\ntrace3 = go.Bar(x = tg_high[\"tenure_group\"] , y = tg_high[\"count\"],\n                name = \"High Churn Employees\",\n                marker = dict(line = dict(width = .5,color = \"black\")),\n                marker_color='red',\n                opacity = .9)\n\nlayout = go.Layout(dict(title = \"Employee churn in tenure groups\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"tenure group\",\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"count\",\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                       )\n                  )\ndata = [trace1, trace2, trace3]\nfig  = go.Figure(data=data,layout=layout)\npy.iplot(fig)","fcaa520b":"# Overtime by Churn Risk\ntg_low  =  churn_low[\"overtime_group\"].value_counts().reset_index()\ntg_low.columns  = [\"overtime_group\",\"count\"]\ntg_medium =  churn_medium[\"overtime_group\"].value_counts().reset_index()\ntg_medium.columns = [\"overtime_group\",\"count\"]\ntg_high =  churn_high[\"overtime_group\"].value_counts().reset_index()\ntg_high.columns = [\"overtime_group\",\"count\"]\n\n#bar - low churn\ntrace1 = go.Bar(x = tg_low[\"overtime_group\"]  , y = tg_low[\"count\"],\n                name = \"Low Churn Employees\",\n                marker = dict(line = dict(width = .5,color = \"black\")),\n                marker_color='lightblue',\n                opacity = .9)\n\n#bar - medium churn\ntrace2 = go.Bar(x = tg_medium[\"overtime_group\"] , y = tg_medium[\"count\"],\n                name = \"Medium Churn Employees\",\n                marker = dict(line = dict(width = .5,color = \"black\")),\n                 marker_color='cornflowerblue',\n                opacity = .9)\n\n#bar - high churn\ntrace3 = go.Bar(x = tg_high[\"overtime_group\"] , y = tg_high[\"count\"],\n                name = \"High Churn Employees\",\n                marker = dict(line = dict(width = .5,color = \"black\")),\n                marker_color='red',\n                opacity = .9)\n\nlayout = go.Layout(dict(title = \"Employee overtime_group\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"tenure group\",\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"count\",\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                       )\n                  )\ndata = [trace1, trace2, trace3]\nfig  = go.Figure(data=data,layout=layout)\npy.iplot(fig)","4ec53d81":"# all numerical varibles by Churn_risk\ndef histogram(column) :\n    trace1 = go.Histogram(x  = churn_low[column],\n                          histnorm= \"percent\",\n                          name = \"Low Churn Employees\",\n                          marker = dict(line = dict(width = .5,color = \"black\")),\n                          marker_color='lightblue',\n                          opacity = .9) \n    \n    trace2 = go.Histogram(x  = churn_medium[column],\n                          histnorm = \"percent\",\n                          name = \"Medium Churn Employees\",\n                          marker = dict(line = dict(width = .5,color = \"black\")),\n                          marker_color='cornflowerblue',\n                          opacity = .9)\n                         \n    trace3 = go.Histogram(x  = churn_high[column],\n                          histnorm = \"percent\" ,\n                          name = \"High Churn Employees\",\n                          marker = dict(line = dict(width = .5,color = \"black\")),\n                          marker_color='red',\n                          opacity = .9)\n    \n    data = [trace1,trace2, trace3]\n    layout = go.Layout(dict(title =column + \" distribution in employee churn\",\n                            plot_bgcolor  = \"rgb(243,243,243)\",\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                             title = column,\n                                             zerolinewidth=1,\n                                             ticklen=5,\n                                             gridwidth=3\n                                            ),\n                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                             title = \"percent\",\n                                             zerolinewidth=1,\n                                             ticklen=5,\n                                             gridwidth=3\n                                            ),\n                           )\n                      )\n    fig  = go.Figure(data=data,layout=layout)\n    \n    py.iplot(fig)\n    \n#for all categorical columns plot histogram    \nfor i in num_cols :\n    histogram(i)","bd406276":"import plotly.figure_factory as ff\nfrom plotly.offline import iplot\n\n\n# prepare data\n\ndatascatter = Churn_train[[\"Age\",\"Days_off\", \"Emails\", 'Tenure']]\ndatascatter[\"index\"] = np.arange(1,len(datascatter)+1)\n# scatter matrix\nfig = ff.create_scatterplotmatrix(datascatter, diag='box', index='index',colormap='Portland',\n                                  colormap_type='cat',\n                                  height=700, width=700)\niplot(fig)","085366d1":"# Scatterplot by emails & bonus by Churn Risk\nChurn_train[['Bonus', 'Overtime']]\n\n\ndef plot_Churn_risk_scatter(Churn_risk,color) :\n    tracer = go.Scatter(x = Churn_train[Churn_train[\"Churn_risk\"] == Churn_risk][\"Bonus\"],\n                        y = Churn_train[Churn_train[\"Churn_risk\"] == Churn_risk][\"Overtime\"],\n                        mode = \"markers\",marker = dict(line = dict(color = \"black\",\n                                                                   width = .2),\n                                                       size = 4 , color = color,\n                                                       symbol = \"diamond-dot\",\n                                                      ),\n                        name = \"Employee Churn - \" + Churn_risk,\n                        opacity = .9\n                       )\n    return tracer\n\n\ntrace1 = plot_Churn_risk_scatter(\"low\",\"lightblue\")\ntrace2 = plot_Churn_risk_scatter(\"medium\",\"cornflowerblue\")\ntrace3 = plot_Churn_risk_scatter(\"high\",\"red\")\n\ndata   = [trace1,trace2,trace3] \n\n#layout\ndef layout_title(title) :\n    layout = go.Layout(dict(title = title,\n                            plot_bgcolor  = \"rgb(243,243,243)\",\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                         title = \"Bonus\",\n                                         zerolinewidth=1,ticklen=5,gridwidth=2),\n                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                         title = \"Overtime\",\n                                         zerolinewidth=1,ticklen=5,gridwidth=2),\n                            height = 600\n                           )\n                      )\n    return layout\n\nlayout  = layout_title(\"Bonus & Overtime by Churn_risk\")\nfig = go.Figure(data = data,layout = layout)\npy.iplot(fig)","b6d5615e":"#Emails, Days_Off and Tenure by Churn Risk\n\nchurn_df = Churn_train.copy()\n#Drop tenure column\n\ntrace1 = go.Scatter3d(x = churn_low[\"Emails\"],\n                      y = churn_low[\"Days_off\"],\n                      z = churn_low[\"Tenure\"],\n                      mode = \"markers\",\n                      name = \"Low Churn Employees\",\n                      marker = dict(size = 1,color = \"lightblue\")\n                     )\ntrace2 = go.Scatter3d(x = churn_medium[\"Emails\"],\n                      y = churn_medium[\"Days_off\"],\n                      z = churn_medium[\"Tenure\"],\n                      name = \"Medium Churn Employees\",\n                      mode = \"markers\",\n                      marker = dict(size = 1,color= \"cornflowerblue\")\n                     )\ntrace3 = go.Scatter3d(x = churn_high[\"Emails\"],\n                      y = churn_high[\"Days_off\"],\n                      z = churn_high[\"Tenure\"],\n                      name = \"High Churn Employees\",\n                      mode = \"markers\",\n                      marker = dict(size = 1,color= \"red\")\n                     )\n\n\n\nlayout = go.Layout(dict(title = \"Emails, Days_off & Tenure by Churn_risk\",\n                        scene = dict(camera = dict(up=dict(x= 0 , y=0, z=0),\n                                                   center=dict(x=0, y=0, z=0),\n                                                   eye=dict(x=1.25, y=1.25, z=1.25)),\n                                     xaxis  = dict(title = \"Emails\",\n                                                   gridcolor='rgb(255, 255, 255)',\n                                                   zerolinecolor='rgb(255, 255, 255)',\n                                                   showbackground=True,\n                                                   backgroundcolor='rgb(230, 230,230)'),\n                                     yaxis  = dict(title = \"Days_off\",\n                                                   gridcolor='rgb(255, 255, 255)',\n                                                   zerolinecolor='rgb(255, 255, 255)',\n                                                   showbackground=True,\n                                                   backgroundcolor='rgb(230, 230,230)'\n                                                  ),\n                                     zaxis  = dict(title = \"Tenure\",\n                                                   gridcolor='rgb(255, 255, 255)',\n                                                   zerolinecolor='rgb(255, 255, 255)',\n                                                   showbackground=True,\n                                                   backgroundcolor='rgb(230, 230,230)'\n                                                  )\n                                    ),\n                        height = 700,\n                       )\n                  )\n                  \n\ndata = [trace1,trace2,trace3]\nfig  = go.Figure(data = data,layout = layout)\npy.iplot(fig)","be8f5152":"import plotly.express as px\n\ndef check_outliers(column):\n    '''\n    This function will show the outliers for a column\n    '''\n    print(column.describe())\n    p1 = column.quantile(q=0.01)\n    p25 = column.quantile(q=0.25)\n    p75 = column.quantile(q=0.75)\n    p99 = column.quantile(q=0.99)\n    iqr = p75 - p25\n    mu = np.mean(column)\n    sigma = np.std(column)\n    print('Variable:', column.name)\n    print('\\n')\n    print('Percentile outliers: ')\n    print('Valid interval: [', p1, ', ', p99, ']')\n    print('Number of lower outliers: ', np.sum(column < p1))\n    print('Number of upper outliers: ', np.sum(column > p99))\n    print('Total Percentile Outliers: ', np.sum(column > p99)+np.sum(column < p1),\n          \" (\", round(((np.sum(column > p99)+np.sum(column < p1))\/5200*100),2), \"%)\")\n    print('\\n')\n    print('Tukey\u2019s fence outliers: ')\n    print('Valid interval: [', p25 - 1.5*iqr, ', ', p75 + 1.5*iqr, ']')\n    print('Number of lower outliers: ', np.sum(column < p25 - 1.5*iqr))\n    print('Number of upper outliers: ', np.sum(column > p75 + 1.5*iqr))\n    print('Total Tueky Fences Outliers: ', np.sum(column < p25 - 1.5*iqr)+np.sum(column > p75 + 1.5*iqr),\n         \" (\", round(((np.sum(column < p25 - 1.5*iqr)+np.sum(column > p75 + 1.5*iqr))\/5200*100),2), \"%)\")\n    print('\\n')\n    print('Standard deviation outliers: ')\n    print('Valid interval: [', mu - 3*sigma, ', ', mu + 3*sigma, ']')\n    print('Number of lower outliers: ', np.sum(column < mu - 3*sigma))\n    print('Number of upper outliers: ', np.sum(column > mu + 3*sigma))\n    print('Total STD Outliers: ', np.sum(column < mu - 3*sigma)+np.sum(column > mu + 3*sigma),\n          \" (\", round(((np.sum(column < mu - 3*sigma)+np.sum(column > mu + 3*sigma))\/5200*100),2), \"%)\")\n    print('\\n')","9582cc98":"check_outliers(Churn_train['Age'])\nfig = px.box(Churn_train, y=\"Age\", points=\"all\", color_discrete_sequence =['lawngreen'])\nfig.show()","b7a040cf":"check_outliers(Churn_train['Days_off'])\nfig = px.box(Churn_train, y=\"Days_off\", points=\"all\", color_discrete_sequence =['lightgreen'])\nfig.show()","3c95a505":"check_outliers(Churn_train['Rotations'])\nfig = px.box(Churn_train, y=\"Rotations\", points=\"all\", color_discrete_sequence =['lime'])\nfig.show()","bf90733b":"check_outliers(Churn_train['Satis_leader'])\nfig = px.box(Churn_train, y=\"Satis_leader\", points=\"all\", color_discrete_sequence =['springgreen'])\nfig.show()","067e940e":"check_outliers(Churn_train['Satis_team'])\nfig = px.box(Churn_train, y=\"Satis_team\", points=\"all\", color_discrete_sequence =['darkseagreen'])\nfig.show()","dc5d477c":"check_outliers(Churn_train['Emails'])\nfig = px.box(Churn_train, y=\"Emails\", points=\"all\", color_discrete_sequence =['turquoise'])\nfig.show()","8c630858":"check_outliers(Churn_train['Tenure'])\nfig = px.box(Churn_train, y=\"Tenure\", points=\"all\", color_discrete_sequence =['teal'])\nfig.show()","05787ae7":"check_outliers(Churn_train['Bonus'])\nfig = px.box(Churn_train, y=\"Bonus\", points=\"all\", color_discrete_sequence =['green'])\nfig.show()","cd1e7e66":"check_outliers(Churn_train['Distance'])\nfig = px.box(Churn_train, y=\"Distance\", points=\"all\", color_discrete_sequence =['forestgreen'])\nfig.show()","45f317d8":"check_outliers(Churn_train['Kids'])\nfig = px.box(Churn_train, y=\"Kids\", points=\"all\", color_discrete_sequence =['darkgreen'])\nfig.show()","f666ded1":"check_outliers(Churn_train['Overtime'])\nfig = px.box(Churn_train, y=\"Overtime\", points=\"all\", color_discrete_sequence =['olive'])\nfig.show()","d9eb485c":"#correlation\ncorrelation = Churn_train.corr()\n#tick labels\nmatrix_cols = correlation.columns.tolist()\n#convert to array\ncorr_array  = np.array(correlation)\n\n#Plotting\ntrace = go.Heatmap(z = corr_array,\n                   x = matrix_cols,\n                   y = matrix_cols,\n                   colorscale = \"greens\"\n                   )\n\n\nlayout = go.Layout(dict(title = \"Pearson Correlation Matrix for numerical Variables\",\n                        autosize = False,\n                        height  = 720,\n                        width   = 800,\n                        margin  = dict(r = 0 ,l = 210,\n                                       t = 25,b = 210,\n                                      ),\n                        yaxis   = dict(tickfont = dict(size = 9)),\n                        xaxis   = dict(tickfont = dict(size = 9))\n                       )\n                  )\n\ndata = [trace]\nfig = go.Figure(data=data,layout=layout)\npy.iplot(fig)                ","2ccbf549":"Churn_train_median = Churn_train.copy()\nChurn_train_median.fillna(Churn_train_median.median(), inplace = True)","cd0aaeba":"Churn_train_median_test = Churn_test.copy()\nChurn_train_median_test.fillna(Churn_train_median.median(), inplace = True)","e470a148":"print(Churn_train_median.isna().sum())\nChurn_train_median_test.isna().sum()","9573e83f":"#Subgroups grouped by Gender, Marital Status & Department - Median - Train Set\nmed_age = Churn_train.groupby(['Gender', 'Marital_status', 'Department'])['Age'].transform('median')\nmed_days = Churn_train.groupby(['Gender', 'Marital_status', 'Department'])['Days_off'].transform('median')\nmed_rot = Churn_train.groupby(['Gender', 'Marital_status', 'Department'])['Rotations'].transform('median')\nmed_ld = Churn_train.groupby(['Gender', 'Marital_status', 'Department'])['Satis_leader'].transform('median')\nmed_tm = Churn_train.groupby(['Gender', 'Marital_status', 'Department'])['Satis_team'].transform('median')\nmed_dist = Churn_train.groupby(['Gender', 'Marital_status', 'Department'])['Distance'].transform('median')\nmed_kids = Churn_train.groupby(['Gender', 'Marital_status', 'Department'])['Kids'].transform('median')\n# Missing values filled - Train set\nChurn_train['Age'].fillna(med_age, inplace=True)\nChurn_train['Days_off'].fillna(med_days, inplace=True)\nChurn_train['Rotations'].fillna(med_rot, inplace=True)\nChurn_train['Satis_leader'].fillna(med_ld, inplace=True)\nChurn_train['Satis_team'].fillna(med_tm, inplace=True)\nChurn_train['Distance'].fillna(med_dist, inplace=True)\nChurn_train['Kids'].fillna(med_kids, inplace=True)\n# Missing values filled - Test Set\nChurn_test['Age'].fillna(med_age, inplace=True)\nChurn_test['Days_off'].fillna(med_days, inplace=True)\nChurn_test['Rotations'].fillna(med_rot, inplace=True)\nChurn_test['Satis_team'].fillna(med_tm, inplace=True)\n# Subgroups for persisting missing values\nmed_age2 = Churn_train.groupby(['Gender', 'Marital_status'])['Age'].transform('median')\nmed_dist2 = Churn_train.groupby(['Gender', 'Marital_status'])['Distance'].transform('median')\nmed_tm2 = Churn_train.groupby(['Gender', 'Marital_status'])['Satis_team'].transform('median')\n# Missing values filled - Train set\nChurn_train['Age'].fillna(med_age2, inplace=True)\nChurn_train['Distance'].fillna(med_dist2, inplace=True)\n# Missing values filled - Test set\nChurn_test.fillna(Churn_train.median(), inplace = True)","7f54c62e":"print(Churn_train.isna().sum())\nChurn_test.isna().sum()","1ed65857":"Churn_train = pd.concat([Churn_train, pd.get_dummies(Churn_train['Marital_status'])], axis=1)\nChurn_train = pd.concat([Churn_train, pd.get_dummies(Churn_train['Gender'])], axis=1)\nChurn_train = pd.concat([Churn_train, pd.get_dummies(Churn_train['Department'])], axis=1)","48fb77fc":"print(Churn_train.columns)\nprint(Churn_train.shape)\nChurn_train.head(3)","60408c21":"Churn_test = pd.concat([Churn_test, pd.get_dummies(Churn_test['Marital_status'])], axis=1)\nChurn_test = pd.concat([Churn_test, pd.get_dummies(Churn_test['Gender'])], axis=1)\nChurn_test = pd.concat([Churn_test, pd.get_dummies(Churn_test['Department'])], axis=1)","c2f94638":"print(Churn_test.columns)\nprint(Churn_test.shape)\nChurn_test.head(3)","4d0818ca":"# Variable creation for the train set\nChurn_train['Tenure_Age'] = (Churn_train['Tenure']\/12)\/Churn_train['Age']\nChurn_train['Rotations_Tenure']=Churn_train['Rotations']\/Churn_train['Tenure']\nChurn_train['Distance_Overtime']=Churn_train['Distance']*Churn_train['Overtime']\nChurn_train['Overtime_Bonus']=Churn_train['Overtime']\/np.where(Churn_train['Bonus']>0,Churn_train['Bonus'],1)\n\n# Variable creation for the test set\nChurn_test['Tenure_Age'] = (Churn_test['Tenure']\/12)\/Churn_test['Age']\nChurn_test['Rotations_Tenure']=Churn_test['Rotations']\/Churn_test['Tenure']\nChurn_test['Distance_Overtime']=Churn_test['Distance']*Churn_test['Overtime']\nChurn_test['Overtime_Bonus']=Churn_test['Overtime']\/np.where(Churn_test['Bonus']>0,Churn_test['Bonus'],1)","d04f2a02":"print(\"Quantity of missing values per variable in Train and Test set - after new variables\")\nprint(\"\\n\")\nprint(Churn_train.isna().sum())\nprint(\"\\n\")\nprint(Churn_test.isna().sum())\nprint(\"\\n\")","8d777700":"check_outliers(Churn_train['Tenure_Age'])\nfig = px.box(Churn_train, y=\"Tenure_Age\", points=\"all\", color_discrete_sequence =['lightsalmon'])\nfig.show()","e071917c":"check_outliers(Churn_train['Rotations_Tenure'])\nfig = px.box(Churn_train, y=\"Rotations_Tenure\", points=\"all\", color_discrete_sequence =['lightcoral'])\nfig.show()","1764acdf":"check_outliers(Churn_train['Distance_Overtime'])\nfig = px.box(Churn_train, y=\"Distance_Overtime\", points=\"all\", color_discrete_sequence =['indianred'])\nfig.show()","9f5d721b":"check_outliers(Churn_train['Overtime_Bonus'])\nfig = px.box(Churn_train, y=\"Overtime_Bonus\", points=\"all\", color_discrete_sequence =['darkred'])\nfig.show()","0c00a915":"data = Churn_train.drop(['Churn_risk', 'Gender','Marital_status', 'Department'], axis=1)\nprint(\"Final dataframe shape: \", data.shape)\nprint(\"\\n\")\nprint(\"Independent Variables that could be used for modelling:\")\nprint(\"\\n\")\ndata.info()","6da0d9bb":"Churn_train_risk = Churn_train[['Churn_risk']].copy()\nChurn_train_risk.head()","f6d7bd31":"dataTest = Churn_test.drop(['Gender','Marital_status', 'Department'], axis=1)\nprint(\"Final dataframe shape: \", dataTest.shape)\nprint(\"\\n\")\nprint(\"Independent Variables that could be used for predictions:\")\nprint(\"\\n\")\ndataTest.info()","a3aa16c4":"Data_clip_perc = data.copy()\nLowP = Data_clip_perc.quantile(0.01)\nHighP = Data_clip_perc.quantile(0.99)\nData_clip_perc = Data_clip_perc.clip(LowP, HighP, axis=1)\nData_clip_perc.describe().T","66f959a9":"Data_clip_percTest = dataTest.copy()\nData_clip_percTest = Data_clip_percTest.clip(LowP, HighP, axis=1)\nData_clip_percTest.describe().T","a10baa72":"# Min Max applied to data without removing outliers\nData_with_outliers = data.copy()\nData_with_outliersMinMaxInstance = MinMaxScaler().fit(Data_with_outliers)\nData_with_outliersMinMax = Data_with_outliersMinMaxInstance.transform(Data_with_outliers)\nData_with_outliersMinMax","60abd87a":"# Min Max applied to data with outliers removed by percentile method\nData_clip_percMinMaxInstance = MinMaxScaler().fit(Data_clip_perc)\nData_clip_percMinMax = Data_clip_percMinMaxInstance.transform(Data_clip_perc)\nData_clip_percMinMax","6b60c038":"# Min Max applied to target without removing outliers\nData_with_outliersTest = dataTest.copy()\nData_with_outliersMinMaxTest = Data_with_outliersMinMaxInstance.transform(Data_with_outliersTest)\nData_with_outliersMinMaxTest","6d07e5a9":"# Min Max applied to target with outliers removed by percentile method\nData_clip_percMinMaxTest = Data_clip_percMinMaxInstance.transform(Data_clip_percTest)\nData_clip_percMinMaxTest","5cf24738":"Churn_train = pd.read_csv(r\"..\/input\/traindata\/train.csv\")\nChurn_train.set_index('Employee_ID', inplace=True)","de48bd2a":"Churn_train_risk = Churn_train[['Churn_risk']].copy()\nChurn_train.drop('Churn_risk', axis=1, inplace =True)","3c9899a2":"Churn_train_median = Churn_train.copy()\nChurn_train_median.fillna(Churn_train_median.median(), inplace = True)\nChurn_train_median_class = Churn_train.copy()\n\nmed_age = Churn_train_median_class.groupby(['Gender', 'Marital_status', 'Department'])['Age'].transform('median')\nmed_days = Churn_train_median_class.groupby(['Gender', 'Marital_status', 'Department'])['Days_off'].transform('median')\nmed_rot = Churn_train_median_class.groupby(['Gender', 'Marital_status', 'Department'])['Rotations'].transform('median')\nmed_ld = Churn_train_median_class.groupby(['Gender', 'Marital_status', 'Department'])['Satis_leader'].transform('median')\nmed_tm = Churn_train_median_class.groupby(['Gender', 'Marital_status', 'Department'])['Satis_team'].transform('median')\nmed_dist = Churn_train_median_class.groupby(['Gender', 'Marital_status', 'Department'])['Distance'].transform('median')\nmed_kids = Churn_train_median_class.groupby(['Gender', 'Marital_status', 'Department'])['Kids'].transform('median')\n\nChurn_train_median_class['Age'].fillna(med_age, inplace=True)\nChurn_train_median_class['Days_off'].fillna(med_days, inplace=True)\nChurn_train_median_class['Rotations'].fillna(med_rot, inplace=True)\nChurn_train_median_class['Satis_leader'].fillna(med_ld, inplace=True)\nChurn_train_median_class['Satis_team'].fillna(med_tm, inplace=True)\nChurn_train_median_class['Distance'].fillna(med_dist, inplace=True)\nChurn_train_median_class['Kids'].fillna(med_kids, inplace=True)\n\nmed_age2 = Churn_train_median_class.groupby(['Gender', 'Marital_status'])['Age'].transform('median')\nmed_dist2 = Churn_train_median_class.groupby(['Gender', 'Marital_status'])['Distance'].transform('median')\n\nChurn_train_median_class['Age'].fillna(med_age2, inplace=True)\nChurn_train_median_class['Distance'].fillna(med_dist2, inplace=True)","afd00622":"Churn_train_median_nv = Churn_train_median.copy()\nChurn_train_median_nv['Tenure_Age']=(Churn_train_median_nv['Tenure']\/12\/Churn_train_median_nv['Age']).copy()\nChurn_train_median_nv['Rotations_Tenure']=(Churn_train_median_nv['Tenure']\/(Churn_train_median_nv['Rotations']+1)).copy()\nChurn_train_median_nv['Distance_Overtime']=(Churn_train_median_nv['Distance']*Churn_train_median_nv['Overtime']).copy()\nChurn_train_median_nv['Overtime_Bonus']=(Churn_train_median_nv['Overtime']\/np.where(Churn_train_median_nv['Bonus']>0,Churn_train_median_nv['Bonus'],1)).copy()\n\nChurn_train_median_class_nv = Churn_train_median_class.copy()\nChurn_train_median_class_nv['Tenure_Age']=(Churn_train_median_class_nv['Tenure']\/12\/Churn_train_median_class_nv['Age']).copy()\nChurn_train_median_class_nv['Rotations_Tenure']=(Churn_train_median_class_nv['Tenure']\/(Churn_train_median_class_nv['Rotations']+1)).copy()\nChurn_train_median_class_nv['Distance_Overtime']=(Churn_train_median_class_nv['Distance']*Churn_train_median_class_nv['Overtime']).copy()\nChurn_train_median_class_nv['Overtime_Bonus']=(Churn_train_median_class_nv['Overtime']\/np.where(Churn_train_median_class_nv['Bonus']>0,Churn_train_median_class_nv['Bonus'],1)).copy()","cedd6c25":"Churn_train_median_dummies = pd.get_dummies(Churn_train_median)\nChurn_train_median_class_dummies = pd.get_dummies(Churn_train_median_class)\nChurn_train_median_nv_dummies = pd.get_dummies(Churn_train_median_nv)\nChurn_train_median_class_nv_dummies = pd.get_dummies(Churn_train_median_class_nv)","fff4b5c1":"Low_median = Churn_train_median_dummies.quantile(0.01)\nHigh_median = Churn_train_median_dummies.quantile(0.99)\nChurn_train_median_dummies_pct = Churn_train_median_dummies.clip(Low_median, High_median, axis=1)\n\nLow_median_class = Churn_train_median_class_dummies.quantile(0.01)\nHigh_median_class = Churn_train_median_class_dummies.quantile(0.99)\nChurn_train_median_class_dummies_pct = Churn_train_median_class_dummies.clip(Low_median_class, High_median_class, axis=1)\n\nLow_nv_median = Churn_train_median_nv_dummies.quantile(0.01)\nHigh_nv_median = Churn_train_median_nv_dummies.quantile(0.99)\nChurn_train_median_nv_dummies_pct = Churn_train_median_nv_dummies.clip(Low_nv_median, High_nv_median, axis=1)\n\nLow_nv_median_class = Churn_train_median_class_nv_dummies.quantile(0.01)\nHigh_nv_median_class = Churn_train_median_class_nv_dummies.quantile(0.99)\nChurn_train_median_class_nv_dummies_pct = Churn_train_median_class_nv_dummies.clip(Low_nv_median_class, High_nv_median_class, axis=1)","698f7a90":"Churn_train_median_dummies_01 = MinMaxScaler().fit(Churn_train_median_dummies).transform(Churn_train_median_dummies)\nChurn_train_median_dummies_pct_01 = MinMaxScaler().fit(Churn_train_median_dummies_pct).transform(Churn_train_median_dummies_pct)\nChurn_train_median_class_dummies_01 = MinMaxScaler().fit(Churn_train_median_class_dummies).transform(Churn_train_median_class_dummies)\nChurn_train_median_class_dummies_pct_01 = MinMaxScaler().fit(Churn_train_median_class_dummies_pct).transform(Churn_train_median_class_dummies_pct)\nChurn_train_median_nv_dummies_01 = MinMaxScaler().fit(Churn_train_median_nv_dummies).transform(Churn_train_median_nv_dummies)\nChurn_train_median_nv_dummies_pct_01 = MinMaxScaler().fit(Churn_train_median_nv_dummies_pct).transform(Churn_train_median_nv_dummies_pct)\nChurn_train_median_class_nv_dummies_01 = MinMaxScaler().fit(Churn_train_median_class_nv_dummies).transform(Churn_train_median_class_nv_dummies)\nChurn_train_median_class_nv_dummies_pct_01 = MinMaxScaler().fit(Churn_train_median_class_nv_dummies_pct).transform(Churn_train_median_class_nv_dummies_pct)","ad491725":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC \nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, multilabel_confusion_matrix\nimport time\nimport warnings\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.over_sampling import SMOTE\nwarnings.filterwarnings('ignore')","801386de":"scoring =  make_scorer(f1_score,average='micro')\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)","59fc571e":"model = LogisticRegression()\nmax_iter = [10, 100, 300]\nmulti_class = ['ovr', 'multinomial']\nsolver = ['newton-cg', 'lbfgs', 'sag', 'saga']\n# define grid search\ngrid = dict(max_iter=max_iter, multi_class=multi_class, solver=solver)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring=scoring,error_score=0)\ngrid_result = grid_search.fit(Churn_train_median_dummies_01,Churn_train_risk)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n# FINAL1 features = train.drop(['id', 'target'],axis=1).columns.values\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","9aa8f6dc":"model = GaussianNB()\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ncross_val_score(model,Churn_train_median_dummies_01,Churn_train_risk, cv=cv, scoring=scoring).mean()","ca593662":"model = DecisionTreeClassifier()\ncriterion=['gini','entropy']\nmax_features = ['sqrt', 'log2']\nmax_depth=[2,60,150]\nmax_leaf_nodes=[6,160,600]\n# define grid search\ngrid = dict(criterion=criterion, max_features=max_features, max_depth=max_depth, max_leaf_nodes=max_leaf_nodes)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring=scoring,error_score=0)\ngrid_result = grid_search.fit(Churn_train_median_dummies_01,Churn_train_risk)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","44197c2c":"model = RandomForestClassifier()\nn_estimators=[10,50,100,200]\nmax_features = ['sqrt','log2']\nbootstrap = [True,False]\n# define grid search\ngrid = dict(n_estimators=n_estimators, max_features=max_features, bootstrap=bootstrap)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring=scoring,error_score=0)\ngrid_result = grid_search.fit(Churn_train_median_dummies_01,Churn_train_risk)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","65237fad":"cross_val_score(ExtraTreesClassifier(max_features= 'log2', n_estimators= 200, bootstrap= False),Churn_train_median_dummies_01,Churn_train_risk, cv=cv, scoring=scoring).mean()","ef9fc8ee":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(Churn_train_median_dummies, Churn_train_risk, test_size = 0.2, random_state=5, stratify = Churn_train_risk)","aa9abcf0":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=0)\nX_sm, y_sm = sm.fit_resample(X_train, y_train)","65a7b03a":"model = RandomForestClassifier(max_features= 'log2', n_estimators= 200, bootstrap= False).fit(X_sm,y_sm) \ny_pred = model.predict(X_test)\n# how did our model perform?\ncount_misclassified = (y_test['Churn_risk'] != y_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\nfrom sklearn import metrics\nprint(f1_score(y_test['Churn_risk'], y_pred, average='micro'))","fd6d9783":"from imblearn.over_sampling import ADASYN\nada = ADASYN(random_state=0)\nX_ada, y_ada = ada.fit_resample(X_train, y_train)","cf1d57cc":"model = RandomForestClassifier(max_features= 'log2', n_estimators= 200, bootstrap= False).fit(X_ada,y_ada) \ny_pred = model.predict(X_test)\n# how did our model perform?\ncount_misclassified = (y_test['Churn_risk'] != y_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\nfrom sklearn import metrics\nprint(f1_score(y_test['Churn_risk'], y_pred, average='micro'))","0ea1b9b6":"model = SVC()\nkernel=['linear', 'poly', 'rbf'] \ndecision_function_shape=['ovo', 'ovr']\n# define grid search\ngrid = dict( kernel = kernel,decision_function_shape=decision_function_shape)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring=scoring,error_score=0)\ngrid_result = grid_search.fit(Churn_train_median_dummies_01,Churn_train_risk)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","a4fb888d":"model = KNeighborsClassifier()\nn_neighbors=[5,10,15] \n\n# define grid search\ngrid = dict( n_neighbors=n_neighbors)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring=scoring,error_score=0)\ngrid_result = grid_search.fit(Churn_train_median_dummies_01,Churn_train_risk)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","c982906d":"model = MLPClassifier()\nactivation= ['identity', 'logistic', 'tanh', 'relu']\nsolver=['lbfgs', 'sgd', 'adam']\n\n# define grid search\ngrid = dict( activation=activation, solver=solver)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring=scoring,error_score=0)\ngrid_result = grid_search.fit(Churn_train_median_class_dummies,Churn_train_risk)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","261456c3":"model = MLPClassifier()\nactivation= ['logistic']\nsolver=['lbfgs']\nlearning_rate=['constant', 'invscaling', 'adaptive']\n\n# define grid search\ngrid = dict( activation=activation, solver=solver, learning_rate=learning_rate)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring=scoring,error_score=0)\ngrid_result = grid_search.fit(Churn_train_median_class_dummies,Churn_train_risk)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","8488fc18":"model = MLPClassifier()\nactivation= ['logistic']\nsolver=['lbfgs']\nlearning_rate=['invscaling']\nhidden_layer_sizes = [(50),(10,10,10), (100)]\n\n# define grid search\ngrid = dict( activation=activation, solver=solver, learning_rate=learning_rate, hidden_layer_sizes=hidden_layer_sizes)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring=scoring,error_score=0)\ngrid_result = grid_search.fit(Churn_train_median_class_dummies,Churn_train_risk)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","1d507335":"from sklearn.ensemble import AdaBoostClassifier\ncross_val_score(AdaBoostClassifier(),Churn_train_median_class_dummies,Churn_train_risk, cv=cv, scoring=scoring).mean()","308f7c8f":"cross_val_score(GradientBoostingClassifier(),Churn_train_median_class_dummies,Churn_train_risk, cv=cv, scoring=scoring).mean()","ee156f89":"from sklearn.ensemble import VotingClassifier\nclf1=LogisticRegression(max_iter= 10, multi_class= 'multinomial', solver= 'saga')\nclf2=DecisionTreeClassifier(criterion= 'gini', max_depth= 60, max_features= 'sqrt', max_leaf_nodes= 600)\nclf3=RandomForestClassifier(n_estimators=200, max_features ='log2', bootstrap = False)\nclf4=ExtraTreesClassifier(max_features= 'log2', n_estimators= 200, bootstrap= False)\nclf5=SVC(probability=True)\nclf6= GradientBoostingClassifier()\nclf7=MLPClassifier(activation= 'logistic', solver= 'lbfgs',learning_rate= 'invscaling',hidden_layer_sizes= 100)\neclf = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('rf', clf3),('ert',clf4),('svc',clf5),('gb',clf6),('MLP', clf7)],voting='soft')\ncross_val_score(eclf, Churn_train_median_class_dummies,Churn_train_risk['Churn_risk'] ,scoring=scoring, cv=10).mean()","9ae24d9f":"model = RandomForestClassifier(n_estimators=200, max_features ='log2', bootstrap = False, random_state = 1)\ncross_val_score(model,Churn_train_median_dummies,Churn_train_risk['Churn_risk'], cv=cv, scoring=scoring).mean()","1f1678a8":"cross_val_score(model,Churn_train_median_class_dummies,Churn_train_risk['Churn_risk'], cv=cv, scoring=scoring).mean()","1cf64c9a":"cross_val_score(model,Churn_train_median_nv_dummies,Churn_train_risk['Churn_risk'], cv=cv, scoring=scoring).mean()","4bcf6c08":"cross_val_score(model,Churn_train_median_dummies_pct,Churn_train_risk['Churn_risk'], cv=cv, scoring=scoring).mean()","14823c3f":"cross_val_score(model,Churn_train_median_dummies_01,Churn_train_risk['Churn_risk'], cv=cv, scoring=scoring).mean()","4dac05a6":"Churn_test = pd.read_csv(r\"..\/input\/testdata\/test.csv\")","08465bb5":"Churn_test.set_index('Employee_ID', inplace=True)","2b7cb88c":"Churn_complete=Churn_train.append(Churn_test)","62120879":"Churn_complete.fillna(Churn_complete.median(), inplace=True)","679342bc":"Churn_complete_dummies = pd.get_dummies(Churn_complete)\nfrom sklearn.preprocessing import MinMaxScaler\nChurn_complete_dummies_01 = pd.DataFrame(MinMaxScaler().fit(Churn_complete_dummies).transform(Churn_complete_dummies), index=list(Churn_complete_dummies.index),columns=list(Churn_complete_dummies))","ea46f5e8":"def plot_feature_importances(model):\n    n_features = Churn_train_median_class_dummies.shape[1]\n    plt.figure(figsize=(20,10))\n    plt.barh(range(n_features), model.feature_importances_, align='center')\n    plt.yticks(np.arange(n_features), Churn_train_median_dummies.columns)\n    plt.xlabel(\"Feature importance\")\n    plt.ylabel(\"Feature\")\n    plt.show()\nplot_feature_importances\nmodel.fit(Churn_train_median_class_dummies,Churn_train_risk['Churn_risk'])\nplot_feature_importances(model)","4018233a":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.pipeline import make_pipeline\n\n# define feature selection\nfs = SelectKBest(score_func=f_classif, k=18)\nclf = RandomForestClassifier(n_estimators=200, max_features ='log2', bootstrap = False, random_state=1)\nrf_kbest = make_pipeline(fs, clf)\n\ncross_val_score(rf_kbest,Churn_train_median_dummies,Churn_train_risk['Churn_risk'], cv=cv, scoring=scoring).mean()","e536bacd":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.pipeline import make_pipeline\n\n# define feature selection\nfs = SelectKBest(score_func=f_classif, k=19)\nclf = RandomForestClassifier(n_estimators=200, max_features ='log2', bootstrap = False, random_state=1)\nrf_kbest = make_pipeline(fs, clf)\n\ncross_val_score(rf_kbest,Churn_train_median_dummies,Churn_train_risk['Churn_risk'], cv=cv, scoring=scoring).mean()","2571550f":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.pipeline import make_pipeline\n\n# define feature selection\nfs = SelectKBest(score_func=f_classif, k=20)\nclf = RandomForestClassifier(n_estimators=200, max_features ='log2', bootstrap = False, random_state=1)\nrf_kbest = make_pipeline(fs, clf)\n\ncross_val_score(rf_kbest,Churn_train_median_dummies,Churn_train_risk['Churn_risk'], cv=cv, scoring=scoring).mean()","57d5e4d1":"def avg_score(model):\n    # apply kfold\n    cv = StratifiedKFold(n_splits=5, random_state=1)\n    # create lists to store the results from the different models \n    score_train = []\n    score_test = []\n    timer = []\n   \n    for train_index, test_index in cv.split(Churn_train_median_dummies_01,Churn_train_risk['Churn_risk']):\n        # get the indexes of the observations assigned for each partition\n        X_train, X_test = pd.DataFrame(Churn_train_median_dummies_01).iloc[train_index], pd.DataFrame(Churn_train_median_dummies_01).iloc[test_index]\n        y_train, y_test = Churn_train_risk['Churn_risk'].iloc[train_index], Churn_train_risk['Churn_risk'].iloc[test_index]\n        # start counting time\n        begin = time.perf_counter()\n        # fit the model to the data\n        model.fit(X_train, y_train)\n        # finish counting time\n        end = time.perf_counter()\n        # check the mean accuracy for the train\n        value_train = f1_score(y_train,model.predict(X_train),average = 'micro')\n        # check the mean accuracy for the test\n        value_test = f1_score(y_test,model.predict(X_test),average = 'micro')\n        # append the accuracies, the time and the number of iterations in the corresponding list\n        score_train.append(value_train)\n        score_test.append(value_test)\n        timer.append(end-begin)\n        \n    # calculate the average and the std for each measure (accuracy, time and number of iterations)\n    avg_time = round(np.mean(timer),3)\n    avg_train = round(np.mean(score_train),3)\n    avg_test = round(np.mean(score_test),3)\n    std_time = round(np.std(timer),2)\n    std_train = round(np.std(score_train),2)\n    std_test = round(np.std(score_test),2)\n        \n    return str(avg_time) + '+\/-' + str(std_time), str(avg_train) + '+\/-' + str(std_train),\\\nstr(avg_test) + '+\/-' + str(std_test)","cfe0a0ba":"def show_results1(df, *args):\n    \"\"\"\n    Receive an empty dataframe and the different models and call the function avg_score\n    \"\"\"\n    count = 0\n    # for each model passed as argument\n    for arg in args:\n        # obtain the results provided by avg_score\n        time, avg_train, avg_test = avg_score(arg)\n        # store the results in the right row\n        df.iloc[count] = time, avg_train, avg_test\n        count+=1\n    return df","b3efdff1":"model = RandomForestClassifier(n_estimators=200, max_features ='log2', bootstrap = False, random_state=1)\ndf = pd.DataFrame(columns = ['Time','Train','Test'], index = ['200_log2'])\nshow_results1(df, model)","2602578c":"depth3 = RandomForestClassifier(max_depth = 3)\ndepth5 = RandomForestClassifier(max_depth = 5)\ndepth7 = RandomForestClassifier(max_depth = 7)\nleaves10 = RandomForestClassifier(max_leaf_nodes = 10)\nleaves20 = RandomForestClassifier(max_leaf_nodes = 20)\nleaves30 = RandomForestClassifier(max_leaf_nodes = 30)","a3739171":"df = pd.DataFrame(columns = ['Time','Train','Test'], index = ['depth3','depth5','depth7','leaves10','leaves20','leaves30'])\nshow_results1(df, depth3,depth5,depth7,leaves10,leaves20,leaves30)","297839af":"depth12 = RandomForestClassifier(max_depth = 12)\ndepth15 = RandomForestClassifier(max_depth = 15)\ndepth17 = RandomForestClassifier(max_depth = 17)","d7e90cab":"df = pd.DataFrame(columns = ['Time','Train','Test'], index = ['depth12','depth15','depth17'])\nshow_results1(df, depth12,depth15,depth17)","1f910af1":"RFDataset100=Churn_train_risk.copy()\nfor i in range (100):\n    model = RandomForestClassifier(max_depth=10, random_state = 10*i)\n    model.fit(Churn_train_median_dummies_01,Churn_train_risk['Churn_risk'])\n    for j in range (3):\n        RFDataset100['RF'+str(i)+str(j)] = pd.DataFrame(data=model.predict_proba(Churn_train_median_dummies_01)[:,j], index=list(Churn_train_median_dummies.index))\nRFDataset100","509edfc4":"def avg_score_mlprf(model):\n    # apply kfold\n    cv = StratifiedKFold(n_splits=5, random_state=1)\n    # create lists to store the results from the different models \n    score_train = []\n    score_test = []\n    timer = []\n   \n    for train_index, test_index in cv.split(RFDataset100.drop('Churn_risk', axis = 1),Churn_train_risk['Churn_risk']):\n        # get the indexes of the observations assigned for each partition\n        X_train, X_test = RFDataset100.drop('Churn_risk', axis = 1).iloc[train_index], RFDataset100.drop('Churn_risk', axis = 1).iloc[test_index]\n        y_train, y_test = Churn_train_risk['Churn_risk'].iloc[train_index], Churn_train_risk['Churn_risk'].iloc[test_index]\n        # start counting time\n        begin = time.perf_counter()\n        # fit the model to the data\n        model.fit(X_train, y_train)\n        # finish counting time\n        end = time.perf_counter()\n        # check the mean accuracy for the train\n        value_train = f1_score(y_train,model.predict(X_train),average = 'micro')\n        # check the mean accuracy for the test\n        value_test = f1_score(y_test,model.predict(X_test),average = 'micro')\n        # append the accuracies, the time and the number of iterations in the corresponding list\n        score_train.append(value_train)\n        score_test.append(value_test)\n        timer.append(end-begin)\n        \n    # calculate the average and the std for each measure (accuracy, time and number of iterations)\n    avg_time = round(np.mean(timer),3)\n    avg_train = round(np.mean(score_train),3)\n    avg_test = round(np.mean(score_test),3)\n    std_time = round(np.std(timer),2)\n    std_train = round(np.std(score_train),2)\n    std_test = round(np.std(score_test),2)\n        \n    return str(avg_time) + '+\/-' + str(std_time), str(avg_train) + '+\/-' + str(std_train),\\\nstr(avg_test) + '+\/-' + str(std_test)","c04b875f":"def show_results2(df, *args):\n    \"\"\"\n    Receive an empty dataframe and the different models and call the function avg_score\n    \"\"\"\n    count = 0\n    # for each model passed as argument\n    for arg in args:\n        # obtain the results provided by avg_score\n        time, avg_train, avg_test = avg_score_mlprf(arg)\n        # store the results in the right row\n        df.iloc[count] = time, avg_train, avg_test\n        count+=1\n    return df","df25a7f4":"model = MLPClassifier(hidden_layer_sizes=50)\ndf = pd.DataFrame(columns = ['Time','Train','Test'], index = ['mlp_rf'])\nshow_results2(df, model)","7ef24713":"cv = StratifiedKFold(n_splits=5, random_state=1)\nX = Churn_complete_dummies_01[:5200]\ncolumns = list()\nmodel = RandomForestClassifier(max_depth=9)\n\n\nfor i in range (50):\n    for j in range (3):\n        columns.append('RF'+str(i)+str(j))\nProbas = pd.DataFrame(index=list(Churn_train.index),columns=columns)\n        \nfor i in range (50):\n    model =  RandomForestClassifier(max_depth=9, random_state=i)\n    for train_index, test_index in cv.split(X,Churn_train_risk['Churn_risk']):\n        # get the indexes of the observations assigned for each partition\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = Churn_train_risk['Churn_risk'].iloc[train_index], Churn_train_risk['Churn_risk'].iloc[test_index]\n        model.fit(X_train, y_train)\n        for j in range (3):\n            Probas['RF'+str(i)+str(j)][list(X_test.index)]=pd.DataFrame(model.predict_proba(X_test), index = list(X_test.index))[j]\n        \nProbas","b809083c":"def avg_score_mlprf(model):\n    # apply kfold\n    cv = StratifiedKFold(n_splits=5, random_state=1)\n    # create lists to store the results from the different models \n    score_train = []\n    score_test = []\n    timer = []\n   \n    for train_index, test_index in cv.split(Probas,Churn_train_risk['Churn_risk']):\n        # get the indexes of the observations assigned for each partition\n        X_train, X_test = Probas.iloc[train_index], Probas.iloc[test_index]\n        y_train, y_test = Churn_train_risk['Churn_risk'].iloc[train_index], Churn_train_risk['Churn_risk'].iloc[test_index]\n        # start counting time\n        begin = time.perf_counter()\n        # fit the model to the data\n        model.fit(X_train, y_train)\n        # finish counting time\n        end = time.perf_counter()\n        # check the mean accuracy for the train\n        value_train = f1_score(y_train,model.predict(X_train),average = 'micro')\n        # check the mean accuracy for the test\n        value_test = f1_score(y_test,model.predict(X_test),average = 'micro')\n        # append the accuracies, the time and the number of iterations in the corresponding list\n        score_train.append(value_train)\n        score_test.append(value_test)\n        timer.append(end-begin)\n        \n    # calculate the average and the std for each measure (accuracy, time and number of iterations)\n    avg_time = round(np.mean(timer),3)\n    avg_train = round(np.mean(score_train),3)\n    avg_test = round(np.mean(score_test),3)\n    std_time = round(np.std(timer),2)\n    std_train = round(np.std(score_train),2)\n    std_test = round(np.std(score_test),2)\n        \n    return str(avg_time) + '+\/-' + str(std_time), str(avg_train) + '+\/-' + str(std_train),\\\nstr(avg_test) + '+\/-' + str(std_test)","8eac3598":"def show_results3(df, *args):\n    \"\"\"\n    Receive an empty dataframe and the different models and call the function avg_score\n    \"\"\"\n    count = 0\n    # for each model passed as argument\n    for arg in args:\n        # obtain the results provided by avg_score\n        time, avg_train, avg_test = avg_score_mlprf(arg)\n        # store the results in the right row\n        df.iloc[count] = time, avg_train, avg_test\n        count+=1\n    return df","a7d746c8":"model = MLPClassifier()\ndf = pd.DataFrame(columns = ['Time','Train','Test'], index = ['mlp_rf'])\nshow_results3(df, model)","c3014143":"from sklearn.ensemble import StackingClassifier\n\n# define the base models\nlevel0 = list()\nfor i in range (10):\n    level0.append(('lr'+str(i), RandomForestClassifier(max_depth = 10, random_state = 10*i)))\n\n# define meta learner model\nlevel1 = MLPClassifier()\n# define the stacking ensemble\nmodel = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)","d3ac1869":"df = pd.DataFrame(columns = ['Time','Train','Test'], index = ['mlp_rf'])\nshow_results1(df, model)","159a36c6":"Churn_test_median_class = Churn_test.copy()\nmed_age = Churn_test_median_class.groupby(['Gender', 'Marital_status', 'Department'])['Age'].transform('median')\nmed_days = Churn_test_median_class.groupby(['Gender', 'Marital_status', 'Department'])['Days_off'].transform('median')\nmed_rot = Churn_test_median_class.groupby(['Gender', 'Marital_status', 'Department'])['Rotations'].transform('median')\nmed_tm = Churn_test_median_class.groupby(['Gender', 'Marital_status', 'Department'])['Satis_team'].transform('median')\nChurn_test_median_class['Age'].fillna(med_age, inplace=True)\nChurn_test_median_class['Days_off'].fillna(med_days, inplace=True)\nChurn_test_median_class['Rotations'].fillna(med_rot, inplace=True)\nChurn_test_median_class['Satis_team'].fillna(med_tm, inplace=True)\nChurn_test_median_class_dummies=pd.get_dummies(Churn_test_median_class)\nChurn_test_median_class_01 = pd.DataFrame(MinMaxScaler().fit(Churn_train_median_class_dummies).transform(Churn_test_median_class_dummies), index=list(Churn_test_median_class_dummies.index),columns=list(Churn_test_median_class_dummies))","b0554c77":"model = RandomForestClassifier(n_estimators=200, max_features ='log2', bootstrap = False, random_state=1)\nmodel.fit(Churn_train_median_class_dummies_01,Churn_train_risk['Churn_risk'])\ntest_pred= pd.DataFrame(data=model.predict(Churn_test_median_class_01), index=list(Churn_test.index),columns=['Churn_risk'])\ntest_pred['Churn_risk'].value_counts()","b09c3a70":"fs = SelectKBest(score_func=f_classif, k=19)\nclf = RandomForestClassifier(n_estimators=200, max_features ='log2', bootstrap = False, random_state=1)\nrf_kbest = make_pipeline(fs, clf)\nrf_kbest.fit(Churn_complete_dummies_01[:5200],Churn_train_risk['Churn_risk'])\ntest_pred= pd.DataFrame(data=rf_kbest.predict(Churn_complete_dummies_01[5200:]), index=list(Churn_test.index),columns=['Churn_risk'])\ntest_pred['Churn_risk'].value_counts()","7fc43986":"sm = SMOTE(random_state=0)\nX_sm, y_sm = sm.fit_resample(Churn_complete_dummies_01[:5200],Churn_train_risk)\nfs = SelectKBest(score_func=f_classif, k=19)\nclf = RandomForestClassifier(n_estimators=200, max_features ='log2', bootstrap = False, random_state=1)\nrf_kbest = make_pipeline(fs, clf)\nrf_kbest.fit(X_sm,y_sm)\ntest_pred_smote= pd.DataFrame(data=rf_kbest.predict(Churn_complete_dummies_01[5200:]), index=list(Churn_test.index),columns=['Churn_risk'])\ntest_pred_smote['Churn_risk'].value_counts()","9468fd8f":"model = RandomForestClassifier(n_estimators=100, max_features ='sqrt', bootstrap = False, random_state=2)\nmodel.fit(Churn_train_median_class_dummies_01,Churn_train_risk['Churn_risk'])\ntest_pred_f1= pd.DataFrame(data=model.predict(Churn_test_median_class_01), index=list(Churn_test.index),columns=['Churn_risk'])\ntest_pred_f1['Churn_risk'].value_counts()","5e46953f":"test_pred= pd.DataFrame(data=np.where(test_pred_smote['Churn_risk']=='high',test_pred_smote['Churn_risk'],test_pred_f1['Churn_risk']), index=list(Churn_test.index),columns=['Churn_risk'])\ntest_pred['Churn_risk'].value_counts()","05967be5":"fs = SelectKBest(score_func=f_classif, k=19)\nclf = RandomForestClassifier(n_estimators=200, max_features ='log2', bootstrap = False, random_state=1)\nrf_kbest = make_pipeline(fs, clf)\nrf_kbest.fit(Churn_complete_dummies_01[:5200],Churn_train_risk['Churn_risk'])\ntest_proba= pd.DataFrame(data=rf_kbest.predict_proba(Churn_complete_dummies_01[5200:]), index=list(Churn_test.index))\ntest_proba['Churn_risk']=(pd.DataFrame(data=np.where(test_proba[0]>.25,'high',np.where(test_proba[1]>.45,'low','medium')), index=list(Churn_test.index)))\ntest_pred=pd.DataFrame(test_proba['Churn_risk'])\ntest_pred['Churn_risk'].value_counts()","681d70c9":"#test_pred.to_csv('Group5_VersionXX.csv')","d683cf3d":"cv = StratifiedKFold(n_splits=10, random_state=1)\nX = Churn_complete_dummies_01[:5200]\nsm = SMOTE(random_state=0)\nfs = SelectKBest(score_func=f_classif, k=19)\nclf = RandomForestClassifier(n_estimators=200, max_features ='log2', bootstrap = False, random_state=1)\nmodel = make_pipeline(fs, clf)\n\n\n\ncolumns = ['RF_SM0','RF_SM1','RF_SM2','RF0','RF1','RF2']\nProbas = pd.DataFrame(index=list(Churn_train.index),columns=columns)\n        \nfor train_index, test_index in cv.split(X,Churn_train_risk['Churn_risk']):\n        # get the indexes of the observations assigned for each partition\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = Churn_train_risk['Churn_risk'].iloc[train_index], Churn_train_risk['Churn_risk'].iloc[test_index]\n        X_sm, y_sm = sm.fit_resample(X_train,y_train)\n        model.fit(X_sm,y_sm)\n        for i in range (3):\n            Probas['RF_SM'+str(i)][list(X_test.index)]=pd.DataFrame(model.predict_proba(X_test), index = list(X_test.index))[i]\nfor train_index, test_index in cv.split(X,Churn_train_risk['Churn_risk']):\n        # get the indexes of the observations assigned for each partition\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = Churn_train_risk['Churn_risk'].iloc[train_index], Churn_train_risk['Churn_risk'].iloc[test_index]\n        model.fit(X_train,y_train)\n        for i in range (3):\n            Probas['RF'+str(i)][list(X_test.index)]=pd.DataFrame(model.predict_proba(X_test), index = list(X_test.index))[i]\n        \nProbas","93651033":"model = LogisticRegression()\ndf = pd.DataFrame(columns = ['Time','Train','Test'], index = ['mlp_rf'])\nshow_results3(df, model)","4fdf2207":"X_sm, y_sm = sm.fit_resample(Churn_complete_dummies_01[:5200],Churn_train_risk)\n\nfs = SelectKBest(score_func=f_classif, k=19)\nclf = RandomForestClassifier(n_estimators=200, max_features ='log2', bootstrap = False, random_state=1)\nmodel = make_pipeline(fs, clf)\nmodel.fit(X_sm,y_sm)\n\n\ncolumns = ['RF_SM0','RF_SM1','RF_SM2','RF0','RF1','RF2']\nProbas_test = pd.DataFrame(index=list(Churn_complete_dummies_01[5200:].index),columns=columns)\n\nfor i in range (3):\n        Probas_test['RF_SM'+str(i)][list(Churn_complete_dummies_01[5200:].index)]=pd.DataFrame(model.predict_proba(Churn_complete_dummies_01[5200:]), index = list(Churn_complete_dummies_01[5200:].index))[i]\nmodel2=make_pipeline(fs, clf)\nmodel2.fit(Churn_complete_dummies_01[:5200],Churn_train_risk)\n        \nfor i in range (3):\n        Probas_test['RF'+str(i)][list(Churn_complete_dummies_01[5200:].index)]=pd.DataFrame(model2.predict_proba(Churn_complete_dummies_01[5200:]), index = list(Churn_complete_dummies_01[5200:].index))[i]\nProbas_test     ","4f032d41":"model = LogisticRegression()\nmodel.fit(Probas,Churn_train_risk)\ntest_pred=pd.DataFrame(data=model.predict(Probas_test), index=list(Churn_test.index),columns=['Churn_risk'])","0ca64c42":"test_pred= pd.DataFrame(data=test_pred, index=list(Churn_test.index),columns=['Churn_risk'])\ntest_pred['Churn_risk'].value_counts()","34a3c2b5":"print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nfrom scipy import interp\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.multiclass import OneVsRestClassifier\n\n# Binarize the output\ny = label_binarize(Churn_train_risk, classes=[0, 1, 2])\nn_classes = y.shape[1]\n\n#Dataset\nChurn_train_risk_dummies = pd.get_dummies(Churn_train_risk)\nChurn_numpy = Churn_train_risk_dummies.to_numpy()\n\n# shuffle and split training and test sets\nX_train, X_test, y_train, y_test = train_test_split(Probas, Churn_numpy, test_size=.8,\n                                                    random_state=0)\n\n# Learn to predict each class against the other\n\nclassifier = OneVsRestClassifier(LogisticRegression())\ny_score =  classifier.fit(X_train, y_train).predict_proba(X_test)\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])","68a0227d":"# First aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n# Then interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(n_classes):\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n# Finally average it and compute AUC\nmean_tpr \/= n_classes\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n# Plot all ROC curves\nplt.figure(figsize=(15,10))\n\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\nlabels = (['High', 'Low', 'Medium'])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(labels[i], roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Churn Risk Classes ROC Curves')\nplt.legend(loc=\"lower right\")\nplt.show()","ce601a57":"# Plot average ROC curves\nplt.figure(figsize=(15,10))\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]),\n         color='deeppink', linestyle=':', linewidth=4)\n\nplt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label='macro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"macro\"]),\n         color='navy', linestyle=':', linewidth=4)\n\n\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Average ROC Curves')\nplt.legend(loc=\"lower right\")\nplt.show()","b264452a":"import plotly.figure_factory as ff\n\ncv = StratifiedKFold(n_splits=5, random_state=1)\n\n#gives model report in dataframe\ndef model_report(model,train_x,train_y,name) :\n    scoring_a =  make_scorer(accuracy_score)\n    scoringr=make_scorer(recall_score,average='micro')\n    scoringp=make_scorer(precision_score,average='micro')\n    scoringf = make_scorer(f1_score,average='micro')\n    accuracy     = cross_val_score(model,train_x,train_y, cv=cv, scoring=scoring_a).mean()\n    recallscore  = cross_val_score(model,train_x,train_y, cv=cv, scoring=scoringr).mean()\n    precision    = cross_val_score(model,train_x,train_y, cv=cv, scoring=scoringp).mean()\n    f1score      = cross_val_score(model,train_x,train_y, cv=cv, scoring=scoringf).mean()\n    \n    \n    df = pd.DataFrame({\"Model\"           : [name],\n                       \"Accuracy_score\"  : [accuracy],\n                       \"Recall_score\"    : [recallscore],\n                       \"Precision\"       : [precision],\n                       \"f1_score\"        : [f1score],\n                                             })\n    return df\n\n#outputs for every model\n\nclf0 = LogisticRegression()\nclf1=RandomForestClassifier(n_estimators=200, max_features ='log2', bootstrap = False)\nclf2=ExtraTreesClassifier(max_features= 'log2', n_estimators= 200, bootstrap= False)\nclf3= GradientBoostingClassifier()\nclf4=MLPClassifier(activation= 'logistic', solver= 'lbfgs',learning_rate= 'invscaling',hidden_layer_sizes= 100)\nclf5=SVC(probability=True)\nclf6=LogisticRegression(max_iter= 10, multi_class= 'multinomial', solver= 'saga')\nclf7=DecisionTreeClassifier(criterion= 'gini', max_depth= 60, max_features= 'sqrt', max_leaf_nodes= 600)\n\nmodel0 = model_report(clf0,Probas,Churn_train_risk,\n                      \"Logistic Reg. on top of RFs\")\nmodel1 = model_report(clf1,Churn_train_median_class_dummies_01,Churn_train_risk,\n                      \"Random Forest\")\nmodel2 = model_report(clf2,Churn_train_median_class_dummies_01,Churn_train_risk,\n                      \"Extra Randomized Trees\")\nmodel3 = model_report(clf3,Churn_train_median_class_dummies_01,Churn_train_risk,\n                      \"Gradient Boosting\")\nmodel4 = model_report(clf4,Churn_train_median_class_dummies_01,Churn_train_risk,\n                      \"MLP\")\nmodel5 = model_report(clf5,Churn_train_median_class_dummies_01,Churn_train_risk,\n                      \"Support Vector\")\nmodel6 = model_report(clf6,Churn_train_median_class_dummies_01,Churn_train_risk,\n                      \"Logistic Regression\")\nmodel7 = model_report(clf7,Churn_train_median_class_dummies_01,Churn_train_risk,\n                      \"Decision Tree\")\n\n\n#concat all models\nmodel_performances = pd.concat([model0,model1,model2,model3,\n                                model4,model5,model6,\n                                model7],axis = 0).reset_index()\n\nmodel_performances = model_performances.drop(columns = \"index\",axis =1)\n\ntable  = ff.create_table(np.round(model_performances,4))\n\npy.iplot(table)\n\nmodel_performances\ndef output_tracer(metric,color) :\n    tracer = go.Bar(y = model_performances[\"Model\"] ,\n                    x = model_performances[metric],\n                    orientation = \"h\",name = metric ,\n                    marker = dict(line = dict(width =.7),\n                                  color = color)\n                   )\n    return tracer\n\nlayout = go.Layout(dict(title = \"Model performances\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"metric\",\n                                     zerolinewidth=1,\n                                     ticklen=5,gridwidth=2),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                        margin = dict(l = 250),\n                        height = 780\n                       )\n                  )\n\n\ntrace1  = output_tracer(\"Accuracy_score\",\"midnightblue\")\ntrace2  = output_tracer('Recall_score',\"cornflowerblue\")\ntrace3  = output_tracer('Precision',\"lightsteelblue\")\ntrace4  = output_tracer('f1_score',\"yellow\")\n\ndata = [trace1,trace2,trace3,trace4]\nfig = go.Figure(data=data,layout=layout)\npy.iplot(fig)","57cf89d8":" <hr>\n<a class=\"anchor\" id=\"modelling\">\n\n## <font> 5. Modeling <\/font> \n\n<\/a>\n\n* [5.1. Model Selection](#modelselection)\n* [5.2. Training Dataframe](#trainingdataframe)\n* [5.3. Features Importance and Selecting K Best](#selectingkbest)\n* [5.4. Overfitting Checking](#overfittingchecking)\n* [5.5. Trying to reduce overfitting with a MLP on top of 100 Random Forests](#mlponrf)\n* [5.6. Building the Test Set Predictions DataFrame](#buildingthemodel)\n* [5.7. Final Strategy: Logistics Regression on top of Random Forests](#final)","4dd4c931":"The different variables are explored using summary statistics and visualizations, in order to understand the behavior of different variables in the churn predictions.","ba098c36":"<a class=\"anchor\" id=\"completedataframe\">\n\n### 5.2.1. Creating a Complete Training Dataframe <\/font> \n\n<\/a>\n","9ca2232d":"****The Extremely Randomized Tree was tested for parameters in the same manner as Random Forest.","dadb6b49":"<a class=\"anchor\" id=\"different thresholds\">\n\n#### 5.6.5. Saving in a csv file <\/font> \n\n<\/a>\n","9175c881":"The dataframes must include a transformation where the target variable is removed from the independent variables. This transformation is explored to understand final datasets.","e83fce91":"In this example a random forest with  (n_estimators=200, max_features ='log2', bootstrap = False, random_state=1) was used, and trained with the dataframe scaled 0-1 with missing values replaced by the median of each class of instances.\nThe same changes were made  to the test set and created the data frame after fitting the model and predicting the values.","5be569cf":"In order to achieve the final datasets that will be tested and used in the modelling stage, a series of transformations are performed by mixing the different preprocessing steps described previously to create multiple dataframes. These will be used to find the best results for models.\n\n> __1.__   Importing the data clean and setting the index, as described in section *3. Data Understanding*.\n\n> __2.__   Creating the data and target dataframes, as described in section *4.3. Data and Target Sets*.\n\n> __3.__   Applying the missing values treatments explored in section *4.1.1. Missing Values*.\n\n> __4.__   Creating the new variables explored in section *4.2.2. New Features*.\n\n> __5.__   Getting the dummy variables explored in section *4.2.1. Dummy Variables*.\n\n> __6.__   Applying outlier treatment explored in section *4.4. Outliers*.\n\n> __7.__   Scaling the dataframes as explored in section *4.5. Scaling*.\n","d0572b4f":" <hr>\n<a class=\"anchor\" id=\"finalmodel\">\n\n### <font> 6.3. Final Model Selection <\/font> \n\n<\/a>","b40de884":"Multi Layer Perceptron is a type of artificial neural network that can be used for multiclass classification. It uses layers of neurons to understand the data and make classifications. The following hyperparameters were tested using grid search:\n- activation: testing activation functions between identity, logistic, tanh and relu\n- solver: between lbfgs, sgd and adam\n- learning rates: between constant, inverse scaling and adaptive\n- hidden layer size: iterating between different amounts of hidden layers and neurons.\n\nAs it takes a long time to run each one of the cross validations for a neural network, we decided to split the grid searches.","352006a5":"The f1 scores were still good, but lower than the previous one, without oversamplampling techniques.","adc40c19":"Gradient Boosting is a boosting technique that also uses weak classifiers to make a stronger prediction. This technique was also explored for churn risk predictions.","7694d1f5":" <hr>\n<a class=\"anchor\" id=\"conclusions\">\n\n## <font> 7. Conclusion <\/font> \n\n<\/a>","416beac1":"Adaptive boosting is a modelling technique that uses weak classifiers combined to make a stronger classification method. Here, the technique was explored for the problem at hand.","75ca3e8f":"...making predictions with a model that had a high f1_score...","d76cd7e1":"<a class=\"anchor\" id=\"gradient\">\n\n##### 5.1.8.2. Gradient Boosting <\/font> \n\n<\/a>","a9880e39":"After having decided to use the random forest algorithm, each one of the created training datasets was tested in this model, sometimes changing the parameters.\nThe idea is to decide how to deal with missing values, if new varibles should be added added, if and how should outliers should be treated and if the data should be scaled. ","21f4fcd2":"<a class=\"anchor\" id=\"buildingthemodel\">\n\n#### 5.6.2. Predictions from random forest with 19 best features selection using Churn_complete [:5200] as training Dataset <\/font> \n\n<\/a>\n","d1febdf2":"<a class=\"anchor\" id=\"extremelyrandomized\">\n\n##### 5.1.4.1. Extremely Randomized Trees<\/font> \n\n<\/a>","7a2f2364":"The following insights are found using these transformations:\n- Both transformations are applicable to create different dataframes.\n- Using the Median by Class method could enrich our data by using imputed values from similar records.\n- Missing values could persist after applying the Median by Class method, so alternative classes must be used.","8cac35a8":"Logistic Regression is naturally a binary classification algorithm. However, multiclass parameters can be used to increase the scope to multiclass classification, like One-versus-Rest (OVR) and Multinomial. \n\nThe parameters that were tested in grid search for logistic regression are:\n- max iterations: Between 10, 100 and 300\n- multi-class: One versus Rest and Multinomial\n- solver: between newton-cg, lbfgs, sag and saga.","8ba4345b":"<a class=\"anchor\" id=\"trainingdataframe\">\n\n### 5.2. Deciding the Training Dataframe <\/font> \n\n<\/a>\n","6aa02a6f":"<a class=\"anchor\" id=\"randomforest\">\n\n#### 5.1.4. Random Forest <\/font> \n\n<\/a>","0d219246":"The following insights were gained from the exploration of numerical variables:\n- The average age of employees is 33, with a range between 18 and 73 years. At least 50% of all employees have between 30 and 35 years. By binning the age groups into young adults and older adults, it can be seen that the proportion of employees in each churn category varies, showing that there are differences in churn risk between age groups. \n- All VeryBestCompany's employees took at least 2 days off last year, with the maximum amount of days off being 32, and the average being 6. Looking at the distribution by churn risk, it can be seen that low risk employees have taken more days off in comparison to high and medium risk employees, who have taken less days off.\n- At least 75% of employees have had a rotation, with the maximum number of rotations being 4. However there are a number of employees with zero rotations.\n- Level of Satisfaction with the leader is quite low, with at least 75% of employees having a score below 1.2. However there are employees with the maximum score allowed (10 out of 10). Distribution of churn risk is scattered through all levels of satisfaction.\n- Level of Satisfaction with the team is also quite low, with 75% of employees having a satisfaction level below 1.04. The maximum satisfaction score with the team is 9.776. Lowest scores of satisfaction seem to be related to higher churn risk.\n- The amount of emails received per day vary a lot, with a minimum number of 1 email per day, and a maximum number of 146 emails per day. The average of emails per day is 30. Higher amounts of emails per day seem to be associated to higher churn risk.\n- 75% of employees have been working on the company for less than 18 months. The newest employee has been working for less than a month, while the highest tenure is 40 months. The average tenure is 1 year. Analyzing the churn risk under binned categories for tenure, it is noticeable that the highest share for high and medium churn risk is between 8 and 13 months of Tenure, while low churn risk seems to be greater in after 17 months of tenure.\n- At least 75% of employees received less than 0.7% of bonus, with some of them having negative values for the bonus. The maximum bonus received is 4.89% of salary. The percentage of bonus has a direct effect on churn risk, with lower earning employees having a higher churn risk than employees who received a bigger bonus.\n- The average distance from home to work is 3.2 kms, with the shortest distance being 2.74 kms and the longest distance 4.01 kms. The distance from home does not seem to be discriminative in regards to churn risk.\n-  The amount of kids per employee household ranges between none and 2, with 54.5% of employees having at least one child. The amount of children does not seem to be discriminative in predicting employee churn.\n- Employees in the VeryBestCompany have been working at least 8 hours per month in overtime, with the maximum number of hours worked per month being 14.2. In average, employees worked 10.48 overtime hours per month. Analyzing binned categories for overtime allows finding that this variable is discriminative in regards to churn risk, with higher overtime increasing the risk.","a2dad788":"<a class=\"anchor\" id=\"dataandtarget\">\n\n### 4.3. Data and Target Sets <\/font> \n\n<\/a>","ef11709e":"<a class=\"anchor\" id=\"scaling\">\n\n### 4.5. Scaling <\/font> \n\n<\/a>","b6093424":"<a class=\"anchor\" id=\"dummyvariables\">\n\n#### 4.2.1. Dummy Variables <\/font> \n\n<\/a>\n\nCategorical variables are transformed into dummy variables in order to be used in the algorithms.","e48beb6a":"<a class=\"anchor\" id=\"decisiontrees\">\n\n#### 5.1.3. Decision Trees <\/font> \n\n<\/a>","85e1007e":"|Number|Dataset name| Dataset Description | \n|---|---|---|\n|1|__Churn_train_median__| Dataset with missing values filled with median, outliers present, not scaled, without dummy variables, no new features|\n|2|__Churn_train_median_class__| Dataset with missing values filled with median by class, outliers present, not scaled, without dummy variables, no new features|\n|3|__Churn_train_median_nv__| Dataset with missing values filled with median, outliers present, not scaled, without dummy variables, with new features|\n|4|__Churn_train_median_class_nv__| Dataset with missing values filled with median by class, outliers present, not scaled, without dummy variables, with new features|\n|5|__Churn_train_median_dummies__| Dataset with missing values filled with median, outliers present, not scaled, with dummy variables, no new features|\n|6|__Churn_train_median_class_dummies__| Dataset with missing values filled with median, outliers present, not scaled, with dummy variables, no new features|\n|7|__Churn_train_median_nv_dummies__| Dataset with missing values filled with median, outliers present, not scaled, with dummy variables, with new features|\n|8|__Churn_train_median_class_nv_dummies__| Dataset with missing values filled with median by class, outliers present, not scaled, with dummy variables, with new features|\n|9|__Churn_train_median_dummies_01__| Dataset with missing values filled with median, outliers present, scaled between 0 and 1, with dummy variables, no new features|\n|10|__Churn_train_median_dummies_pct_01__| Dataset with missing values filled with median, outliers clipped, scaled between 0 and 1, with dummy variables, no new features|\n|11|__Churn_train_median_class_dummies_01__| Dataset with missing values filled with median by class, outliers present, scaled between 0 and 1, with dummy variables, no new features|\n|12|__Churn_train_median_class_dummies_pct_01__| Dataset with missing values filled with median by class, outliers clipped, scaled between 0 and 1, with dummy variables, no new features|\n|13|__Churn_train_median_nv_dummies_01__| Dataset with missing values filled with median, outliers present, scaled between 0 and 1, with dummy variables, with new features|\n|14|__Churn_train_median_nv_dummies_pct_01__| Dataset with missing values filled with median, outliers clipped, scaled between 0 and 1, with dummy variables, with new features|\n|15|__Churn_train_median_class_nv_dummies_01__| Dataset with missing values filled with median by class, outliers present, scaled between 0 and 1, with dummy variables, with new features|\n|16|__Churn_train_median_class_nv_dummies_pct_01__| Dataset with missing values filled with median by class, outliers clipped, scaled between 0 and 1, with dummy variables, with new features|","2ab75ba6":"__2.1. Problem Description__\n\nThe Very Best Company is starting to develop a problem with Employee churn. Even though the company belongs to the \"Top 100 places to work\", the problem is growing and requires attention. The reasons for employee churn are not yet known, and even though measures have been taken by HR, identifying churn risk by performing employee interviews becomes expensive and time consuming. The company has detected that some people are more likely to leave, and because of this, they want a data mining model that can successfully predict an employee's churn risk.\n\n__2.2. Data Mining Goal__\n\n- Build a predictive model that answers the question \u201cWhat people are more likely to quit their position at the company?\u201d \n- Use multiclass classification techniques to answer the above question.\n","f3076fa7":"<a class=\"anchor\" id=\"neuralnetworks\">\n\n#### 5.1.7. Neural Networks <\/font> \n\n<\/a>","b8fb8217":" <hr>\n<a class=\"anchor\" id=\"datapreprocessing\">\n\n## <font> 4. Data Preprocessing <\/font> \n\n<\/a>\n\n* [4.1. Missing Values](#missingvalues)\n* [4.2. New Variables](#newvariables)\n* [4.3. Data and Target Sets](#dataandtarget)\n* [4.4. Outliers](#outliers2)\n* [4.5. Scaling](#scaling)\n* [4.6. Final Datasets](#finaldatasets)","2a036b26":"> This is not the best way to run a grid search, because this method doesn't try every possibility, however it is better for computational reasons.","1bc6178b":"<a class=\"anchor\" id=\"correlations\">\n\n### 3.5. Correlation Analysis <\/font> \n\n<\/a>","368221bb":"<a class=\"anchor\" id=\"different thresholds\">\n\n#### 5.6.4. Using different thresholds <\/font> \n\n<\/a>\n","f6733540":"Additionally, other ensemble methods, as AdaBoost, GradientBoosting and a Soft Voting Classifier, were tested","9d4bf6c7":"* [1. Introduction](#introduction)\n* [2. Business Understanding](#businessunderstanding)\n* [3. Data Understanding](#dataunderstanding)\n* [4. Data Preprocessing](#datapreprocessing)\n* [5. Modelling](#modelling)\n* [6. Evaluation](#evaluation)\n* [7. Conclusions](#conclusions)","d91d49a9":"Correlation Analysis allows understanding the relationship between predictors, and between each predictor and the target variable. Certain models are affected by multicollinearity, and because of this, the training dataset needs to be treated to remove variables. Additionally, understanding the variability between predictor and target allows identifying which predictors have a bigger importance for this task.\n\nA threshold of 0.7 will be used to evaluate correlations between variables.","82b97642":" <hr>\n<a class=\"anchor\" id=\"businessunderstanding\">\n\n## <font > 2. Business Understanding <\/font> \n\n<\/a>","9e7af854":"<a class=\"anchor\" id=\"buildingthemodel\">\n\n#### 5.6.3. Compilation of predictions, using different models to predict different classes <\/font> \n\n<\/a>\n","7bf5386b":"<a class=\"anchor\" id=\"final\">\n\n### 5.7. Final Strategy: A Logistic Regression on top of probabilities of two Random Forests (with and without SMOTE) <\/font> \n\n<\/a>\n","54a62a25":"<a class=\"anchor\" id=\"rfadasyn\">\n\n##### 5.1.4.3 Random Forest with Adasyn <\/font> \n\n<\/a>","05d4d11f":"As the dataset was very unbalanced, it interesting to create a model that was good predicting the smallest class (high churn risk) and then merge it with a model that had a high f1 score, but that did not predict many high churn employees.","d567d305":"<a class=\"anchor\" id=\"buildingthemodel\">\n\n#### 5.6.1. Predictions from random forest (200 estimators) using Churn_train_median_class_dummies_01 as training Dataset <\/font> \n\n<\/a>\n","3656d5d5":"Outliers need to be removed for certain algorthms, and so, a method of percentiles is explored to clip outliers and transform their values to the upper and lower limit. Data is explored briefly to evaluate consistency.","dc9a4276":"After doing some cross validations, 19 features were selected that had the best mean f1 score.","8fa952e2":"Support Vector Machines are algorithms used for binary classification. They work on optimizing the maximum margin between classes in order to make predictions. By using One versus One (OVO) or One versus Rest (OVR) approaches, multiclass classification can be performed using this type of modelling technique. The following hyperparameters were evaluated in grid search:\n- kernel: between linear, poly and rbf\n- decision function shape: either OVO or OVR.","f157cc34":"Having found that the random forest was the best method to predict the employees churn risk and knowing how unbalanced the dataset was, it was decided to evaluate the f1 score of the random forest after running Synthetic Minority Oversampling TEchnique (SMOTE) on the train dataset.","df31e7ee":"And, finally, the hidden layer sizes dictionary.","ea7b1353":"<a class=\"anchor\" id=\"rfsmote\">\n\n##### 5.1.4.2. Random Forest with Smote <\/font> \n\n<\/a>","47b8e114":"Predictive Analytics is a wide concept that includes different statistical techniques looking to make predictions about unknown events. It bases the predictions on existing historical information, aiming at learning decision criteria that enables predicting a result for unknown examples. It includes Predictive Modeling, where tasks can be focused on predicting either a continuous target variable or a categorical target variable. Predicting categories is called Classification, where problems can be of a binary nature or involve three or more possible values as target categories.\n\nIn this project, a multiclass predictive classification task will be performed to identify an employee's churn risk. Various multiclass classification techniques will be used to train different models, followed by the evaluation and analysis of each model's performance. The Cross Industry Standard Process for Data Mining, known as CRISP-DM, will be followed during the entire process.","a941e2bf":"<a class=\"anchor\" id=\"supportvectormachines\">\n\n#### 5.1.5. Support Vector Machines <\/font> \n\n<\/a>","15914e2e":"__3.3.3. Numerical Variables:__ Plots and summary statistics are used to understand the behavior of numerical variables. To do this exploration, a copy of the original set is used in order to create binned variables that help graphical exploration - using the Plotly libraries.","bc708c80":"<a class=\"anchor\" id=\"outliers2\">\n\n### 4.4. Outliers <\/font> \n\n<\/a>","8e25d19d":"The Churn_complete_dummies_01 dataset was created successfully. Now, to train and test the models we needed to select only the correct rows from the train [:5200] and test [5200:] datasets.","9cc56114":"So the test set was prepared with the predictions from base models and then the final predictions were created using the logistic regression","eaa66c89":"<a class=\"anchor\" id=\"logisticregression\">\n\n#### 5.1.1. Logistic Regression <\/font> \n\n\n<\/a>","d25c9362":"<a class=\"anchor\" id=\"missingvalues\">\n\n### 4.1. Missing Values <\/font> \n\n<\/a>\n\n* [4.1.1. Missing Values by Median](#missingvaluesmedian)\n* [4.1.2. Missing Values by Class Median](#missingvaluesclass)","a4d0fa50":"<a class=\"anchor\" id=\"buildingthemodel\">\n\n### 5.6. Building the Test Set Predictions DataFrames  <\/font> \n\n<\/a>\n","6bbf6dc4":"The selected scaling method for this project is MinMax between 0 and 1, having the different datasets scaled and explored. This transformation needs to be applied on different datasets before modelling.","a050190b":"The following insights were discovered from correlation analysis:\n\n- There are no variables in the dataset that exceed the threshold of 0.7 of correlation, meaning that there is no need to remove variables to avoid multicollinearity.\n- The most related variables are Overtime and Bonus, and Tenure and Emails. However both relations are on the threshold and therefore do not require treatment.","287a892e":" <hr>\n<a class=\"anchor\" id=\"introduction\">\n\n## <font> 1. Introduction <\/font> \n\n<\/a>","c210bbb7":" <hr>\n<a class=\"anchor\" id=\"dataunderstanding\">\n\n## <font> 3. Data Understanding <\/font> \n\n<\/a>\n\n* [3.1. Setup](#setup)\n* [3.2. Data Quality Assessment](#dataquality)\n* [3.3. Data Exploration](#dataexploration)\n* [3.4. Outliers and Distributions](#outliers)\n* [3.5. Correlation Analysis](#correlations)","1c196f71":"__3.3.1. Data Types:__ The dataframe's composition is analyzed, finding that 15 variables form this set. 4 of them are categorical and 9 of them are numerical. ","e70e0df2":"K Nearest Neighbors is an instance based classifier that can be used for multiclass tasks. It bases its predictions on new observations by comparing a number of neighboring observations with similar characteristics, along with those observations' target level. The amount of near neighbors affect the quality of predictions, and because of this, the nearest neighbor hyperparameter was used on grid search:\n- nearest neighbors: between 5, 10 and 15.","5a47912b":"Random Forests are part of the ensemble classifiers who generally perform very well in multiclass tasks. They consist of multiple decisions trees that work on classification based on data subsets and feature subsets, with a final convergence based on the overall predictions. The number of decision tree estimators and max features are some of the hyperparameters that affect Random Forest performance.\n\nThe grid search for random forest includes evaluation of the following hyperparameters:\n- Number of estimators: between 10, 50, 100 and 200\n- max features between square root and logarithm in base 2\n- bootstrap: either true or false","a081db19":"#  <font color='#FF5733'> Multiclass Churn Prediction (imbalanced data set) <\/font> ","2fdfab1a":"<a class=\"anchor\" id=\"selectingkbest\">\n\n### 5.3. Features Importance and Selecting K Best <\/font> \n\n<\/a>\n","e940c29b":"Transformations regarding new variables are explored in order to use them as potential discriminators for churn risk. First, dummy variables are created from the categorical variables. Then, new variables are created and described, checking for missing values and outliers.","aa2f0b06":"As shown in sections 5.7 and 6 this stacked classifier, created with a Logistic Regression running on top of the probabilities generated by two Random Forests, one with the unbalanced DataSet and the other with a synthetic balanced dataset, brought excellent results, having high accuracy in the prediction of the three classes.\n\n\nAdditionally, it presents the best results regarding the set of metrics used for model evaluation and the highest predictions in Kaggle. \n\nNote: 28 files were submitted and it was decided to choose the ones that seemed less overfitted and more balanced for final evaluation in Kaggle.","0a51067c":"Moreover, the SelectKBest function was used to identify the feature importance, ","4aba8587":"Soft voting is a method that uses the average of the probabilities in ensemble methods to define a class. This technique was explored for churn risk classification.","f8bb9ae3":"The following insights were gained from the exploration of categorical variables:\n- 75% of employees are male, while 25% are female.\n- 99.6% of all employees have a couple, with only 0.4% being either single or widowed. It is possible that Marital Status will require treatment to reclassify \"Single\" and \"Widow\" into a single category.\n- The largest department in the company is Sales, with 38% of all employees belonging to this area. All other departments have a similar amount of employees, ranging between 635 and 652 people - and making up for around 12.4% each.\n- Regarding the target variable, it is noticeable that the set is highly unbalanced between the 3 output classes. The \"medium\" class has the highest share - with 60.2% of all the dataset. It is followed by the \"low\" class, with 36.7% of the dataset. The lowest share belongs to the \"high\" class, with only 3.12% of all the dataset. It is possible that oversampling \/ undersampling techniques will be required to balance the set.\n\n","e75a0c74":"|Name| Synthetic Variable Definition | Variables Used |\n|---|---|---|\n|Tenure_Age| The ratio between tenure in years and the age of the employee allows understanding the percentage of life time that the employee has been working in VeryBestCompany. It is constructed under the hypothesis that employees with higher values for this variable might have lower churn risk, as they are stable and loyal to the company | Tenure, Age |\n|Rotations_Tenure| The ratio between amount of rotations and employee tenure allows understanding the average amount of time per position for each employee. It is based on the hypothesis that employees with higher values for this variable might have higher churn risk, as they have had less promotions during their work contract | Tenure, Rotations |\n|Distance_Overtime| The product of distance and overtime is a synthetic variable that aims at measuring the impact of doing overtime for employees depending on the distance they must travel to get home after a shift. It is based on the hypothesis that employees with higher values for this variable might have higher churn risk, as they have unfavorable welfare conditions | Distance, Overtime |\n|Overtime_Bonus| The ratio of overtime and bonus describes the fairness in overtime work based on the bonus, assuming that employees will be more willing to work overtime if the bonus is better. This hypothesis is based on the assumption that lower values for this variable might represent lower churn risk employees. | Bonus, Overtime |\n\n","36512c9c":"Both the train and the test set have missing values that require treatment. The amount of missing values is small, however a proper method must be used as to not alter our data. 2 alternatives are explored and analyzed.","f50ff17f":"Insights from data scaling:\n- After applying scaling through the MinMax method, no errors or inconsistencies are found on either train or test sets.\n- Scaling is tested both on clipped and unclipped data, allowing both transformations to be used in final datasets.","dfba2578":"Insights from data and target sets:\n- There is a maximum of 27 possible variables used for modelling, depending on the dataframe used for each model.","c5277477":"__3.2.2. Duplicate rows:__ both datasets are checked for duplicate records, with no duplicates found.","d1713d8f":"...in the end both of them were merged, taking high values from one model and low and medium from the other.","b58c24ab":"Unfortunately, it can be seen that the model was totally overfitted. In order to decrease overfitting by limiting the depth and the number of leaves, two new models were created with the two parameters.","c6c8c6c0":"In this stage, data is preprocessed to create the final datasets to be used in the modelling phase. Given that different algorithms will be used, different datasets will be prepared in order to comply with different algorithm's specifications and test the optimal transformations based on performance and results.\n\nDuring the first 5 sections all possible transformations will be explored and assessed. The final section is used to combine different transformations with the purpose of creating the different training dataframes for the Modelling stage.","2e7961d0":"<a class=\"anchor\" id=\"newfeatures\">\n\n#### 4.1.2. New Features  <\/font> \n\n<\/a>\n\nNew variables are explored to describe churn risk and increase prediction power. After the new variables are created, they are checked for missing values and outliers.\n\nThe variable description is as follows:","52245198":"<a class=\"anchor\" id=\"softvoting\">\n\n##### 5.1.8.3. Soft Voting <\/font> \n\n<\/a>","cc706c15":"<a class=\"anchor\" id=\"modelselection\">\n\n### 5.1. Model Selection <\/font> \n\n* [5.1.1. Logistic Regression](#logisticregression)\n* [5.1.2. Naive Bayes](#naivebayes)\n* [5.1.3. Decision Trees](#decisiontrees)\n* [5.1.4. Random Forest](#randomforest)\n* [5.1.4.1. Extremely Randomized Trees](#extremelyrandomized)\n* [5.1.4.2. Random Forest with SMOTE](#rfsmote)\n* [5.1.4.3. Random Forest with ADASYN](#rfadasyn)\n* [5.1.5. Support Vector Machines](#supportvectormachines)\n* [5.1.6. Nearest Neighbor](#nearestneighbor)\n* [5.1.7. Neural Networks](#neuralnetworks)\n* [5.1.8. Other Ensemble Methods](#ensemblemethods)\n* [5.1.8.1. Adaboost](#adaboost)\n* [5.1.8.2. Gradient Boosting](#gradient)\n* [5.1.8.3. Soft Voting Classifier](#softvoting)\n\n<\/a>","bca8abbf":"<a class=\"anchor\" id=\"naivebayes\">\n\n#### 5.1.2. Naive Bayes <\/font> \n\n<\/a>","67b32c58":"<a class=\"anchor\" id=\"adaboost\">\n\n##### 5.1.8.1. Adaboost <\/font> \n\n<\/a>","750c0689":"<a class=\"anchor\" id=\"dataexploration\">\n\n### 3.3. Data Exploration <\/font> \n\n<\/a>","058ef393":"<a class=\"anchor\" id=\"newvariables\">\n\n### 4.2. New Variables <\/font> \n\n<\/a>\n\n* [4.2.1. Dummy Variables](#dummyvariables)\n* [4.2.2. New Features](#newfeatures)","149b42a5":"**Very Best Company**\n\nVery Best Company is faced with a problem regarding employee churn. With the use of historical data, it was possible to create a model that allows the company to predict employee churn with approximately 80% of accuracy and F1 score. For each employee, the model is able to assign a category between high, medium and low churn risk. Based on this model, it can be predicted that around 1% of employees will have a high churn risk, while 65% of employees will have a medium churn risk. These employees should become the focus of the company regarding retention and working environment conditions. With the right actions, Very Best Company can reduce the overall churn risk and keep its place in the \u201cTop 100 places to work\u201d.\n\n**Predictive Modeling**\n\nPredictive Modeling is one of the most important data mining tasks. It allows using past events to train models and predict outcomes based on historical data. This project used employee data to predict each employee\u2019s risk of churning by using a variety of modeling techniques.  Models were compared based on performance and complexity by contrasting reality and predictions.\n\nUnderstanding the business problem and the relations between the variables is fundamental for a good result. It allows making the right decisions regarding data transformations during the preprocessing stage. Next to that, decisions made on the modeling stage have an important effect on models\u2019 results. Opting for robust cross validation methods and selecting the right set of metrics becomes crucial to model selection and evaluation. Finally, having the right tools for hyper-parameter fine-tuning also allows a practical evaluation of a wide range of all possible combinations and outputs.\n\nWorking on this project allowed understanding how to apply different predictive methods and techniques to successfully develop a viable model in a business scenario. Visualizations were used to understand the problem at hand, followed by multiple transformations required to fit the data to each modeling technique. Repeated K Fold was used as a cross validation method, evaluating models based on the F1 score. With the use of Grid Search and K Best, parameter fine-tuning was performed on several algorithms, like Logistic Regression, Random Forest or Multilayer Perceptron. With this, it was possible to conclude that the best model for our business scenario was a Stacked Classifier, using as base models two random forests, each one runnin in different DataSets (one unbalanced and another syntethically balanced) with a Logistic Regression as Meta model.\n\nAt the end, the combination of techniques and algorithms allowed us to achieve the goal of predicting an employee\u2019s churn risk with a satisfying output performance \u2013 while applying the different concepts learned during the semester.","2314242f":"A grid search was used for each of the models that were tested, in order to pick the best model and then fine tune it, testing different parameters and preprocessed data frames.\n\nFirst, the cross validation method and scoring method were defined, in order to be used in the grid search. The make_scorer and RepeatedStratifiedKFold functions are used for these purposes. Repeated stratified k fold was used to decrease possible overfitting during evaluation. ","61104996":"__3.2.4. Setting the index:__ the index is set to be the Employee ID.","98897935":"<a class=\"anchor\" id=\"missingvaluesclass\">\n\n#### 4.1.2. Filling Missing Values by Class Median <\/font> \n\n<\/a>\n\nMissing values are filled in by imputing the value from similar characteristics of the record. Employees are grouped by gender, marital status and department, and the median of these sub-groups is used to treat the missing fields in the different variables. \n\nFor records that persist with missing values, additional subgroups are created. \n\nBoth train and test set are checked to guarantee no null values are present in the final dataset.","dee0ffcb":"<a class=\"anchor\" id=\"setup\">\n\n### 3.1. Setup <\/font> \n\n<\/a>","4dff2fcc":" <hr>\n<a class=\"anchor\" id=\"roc\">\n\n### <font> 6.1. ROC Curves <\/font> \n\n<\/a>","552d649f":"After testing each dataframe, the Churn_train_median_dummies_01 was seen to perform the best. This training dataset has missing values replaced by the median of the feature in the train set, without new variables, scaled in range 0 to 1 and without any outlier treatment.\n\nTest set must be prepared for predictions using these characteristics. Accordingly, a new dataframe was created, scaling and replacing outliers after merging the train and test dataset. The objective was to improve the accuracy of the scaling and the median of the variables by adding the instances of the test dataset to be considered while these steps were calculated, and make sure that the same tansformations are applied in both the training and testing set.","da005718":"The results were much worse than they previouly were (with the incorrect model), but it was interesting to see that we could create a stacking model less overfitted than the one produced by the StackingClassifier.","270e27c1":"The results weren't good because predictions were used for the base models after training the test dataset with itself, so it was very easy for the model to make the predictions. This mistake was managed by first, making the predictions to the base model with cross validations and later, using the StackingClassifier.","21dfe818":"The last strategy was to create a stacking classifier, using as input for a Logistic Regression the probabilities of class belonging predicted by random forests (one in the unbalanced Dataset and the other on a Dataset balanced through SMOTE).","856deb32":"<a class=\"anchor\" id=\"dataquality\">\n\n### 3.2. Data Quality Assessment <\/font> \n\n<\/a>","9b823d21":"In this stage, the key characteristics of the dataset are examined by performing summary statistics, correlation analysis and visualization techniques. This allows understanding the behavior of each predictor variable and its relation to the target variable. Additionally, data quality is assessed to identify outliers, missing values or inaccuracies. ","b9cc8291":"Regarding Data Quality, it is important to keep in mind:\n- The datasets require few preprocessing to improve quality - only needing missing values treatment.\n- No duplicate rows or IDs were found.\n- The training dataset has an index set to the Employee ID.","fe57aa79":"The results seemed really good, as they did not seem overfitted nor underfitted, with an excellent micro average f1 score.","ebad15e1":"Additionally, a Random Forest with Adaptive Synthetic Sampling Method for Imbalanced Data (ADASYN) was also used to evaluate performance with synthetic samples that bring balance to classes.","123ac8d4":"* After seeing how overfitted the model was and submitting predictions based on less overfitted models with a decrease in average f1_score, 100 random forests were created, with max_depth = 10 and next running a neural network on top of it. It was tested with the predictions and with the probabilities as input for the second layer model.","9791efb4":"Insights from Outlier clipping:\n- No inconsistencies were found from applying a clipping method by percentiles on both the train and test set.","3940a0df":"__3.2.3. Duplicate IDs:__ given that the employee ID will be the index of the dataframe, it is important to check if there is duplicity in employee's IDs. No duplicity was found, allowing to set the dataframe's index.","61537435":" <hr>\n<a class=\"anchor\" id=\"metrics\">\n\n### <font> 6.2. Main Models Average Metrics <\/font> \n\n<\/a>","156f30de":"Naive Bayes is an algorithm that can be used for multiclass classification that uses Bayes' theorem to classify observations. In this section, a Gaussian Naive Bayes Classifier is used to make predictions and evaluate this modelling technique.","3d36c303":"Decision Trees can be used for multiclass tasks, applying inferred rules to predict new observations. Several parameters affect decision tree performance, such as the attribute selection measure or the max depth allowed in splitting nodes and edges. Grid search was used to mix different hyperparameters, including:\n- criterion: between gini and entropy\n- max features: defined by either square root or logarithm in base 2\n- max depth: between 2, 60 and 150\n- max leaf nodes: between 6, 160 and 600","9e6c2c24":"From the imported datasets' overview, it is noticeable that:\n- The training set consists of 5.200 records and 16 columns, while the test set consists of 1300 records and 15 columns.\n- Both sets match variable names and quantities, with the exception of the target variable - which is not included in the test set. \n- Both sets have an employee ID that can be set as index.\n- Both sets have missing values that will need to be treated.","e8d55125":"All models were compared with the average F1 Scores after cross validations. Moreover, ROC Curves and other metrics were evaluated for the best ones. Here, we can see an example, using our best model according to average f1 score in Kaggle, the one we have just seen above.","dfdf7dbd":"__3.2.1. Missing Values:__ both datasets are checked for missing values, finding that both sets have missing values on different variables. However, they account for a small percentage of the data and can be fixed in Data Preprocessing tasks. ","7d003596":"<a class=\"anchor\" id=\"finaldatasets\">\n\n### 4.6. Final Datasets <\/font> \n\n\n<\/a>","8dc027b7":"Data Quality Assessment is performed to both dataframes by analyzing missing values, duplicate records and duplicate indexes.","7c8642ea":"The target dataset is called Churn_Train_Risk and it includes the 3 possible risk categories.","c8da14fb":" <hr>\n<a class=\"anchor\" id=\"evaluation\">\n\n## <font> 6. Evaluation <\/font> \n\n<\/a>\n\n* [6.1. ROC Curves](#roc)\n* [6.2. Main Models Average Metrics](#metrics)\n* [6.3. Final Model Selection](#finalmodel)","17809450":"The different models are organized by algorithm family. Within each family different variations of the models are used to predict the churn risk.\n\nFirst the setup for all models is done by importing the required libraries. ","26bd8222":"This combination of Random Forests and a Neural Network on top of it seemed really interesting because it ended with a good average F1_score for the predictions on the train and test splits.\nIt wasn't clear how it was going to behave in the predictions to the test set, because predictions will be made over predictions, increasing the uncertainty. ","3f385f36":"A function was defined to automatically check the amount of outliers per variable using 3 methods: percentile outliers, tukey's fences and standard deviation. Additionally, a boxplot is used to graphically illustrate outliers per variable.","13502eef":"The following insights were found on outliers and distributions:\n- All numerical variables have outliers detected by at least one of the 3 methods used. \n- Tukey\u2019s fences method detected in average a larger percentage of outliers (in average 3.87% per variable). In some variables like Age, Days Off, Rotations and SatisTeam, the percentage of outliers detected was above 3%, meaning that using this method to clip outliers will modify more data than expected. It is not recommended to use this method.\n- Standard Deviation method detected in average the least amount of outliers (in average 0.65% per variable). No amount of outliers exceeded the threshold of 3% of data, meaning that it is a viable method to apply.\n- Percentile Method detected an intermediate amount of outliers in the data (1.23% in average per variable). No amount of outliers exceeded the threshold of 3%, meaning this method is viable to use. Given that this is an intermediate approach between the 3 methods used, clipping will be performed using percentile\u2019s lower and upper limits. However, alternate datasets may be used to test results with the Standard Deviation method.","26ea48a6":"\n\nA revision was made on the average f1 score for the model predictions to train dataset itself.","aff88501":"The Random Forest algorithm was selected as best performing algorythm, in combination with \"Select K Best\" in order to reduce the number of features used, after seeing above that some features have very small importance.","fdf8458b":"Insights from Model Selection:\n- From the different grid searches done on the model families and hyperparameter fine tuning, and iterating between the different dataframes, it can be concluded that the best modelling technique for this problem is through Random Forest. This technique will be used to improve further.\n- Grid search is a powerful tool for parameter fine-tuning. The possibilities for optimization are very wide, as multiple combinations of models can be explored quickly and with reliable results. \n- Even though oversampling techniques tend to optimize performance in unbalanced datasets, our use of synthetic oversampling did not improve results in comparison to the unbalanced classes. Possible new attempts might be performed afterwards.","9477b2ba":"In this example the specific training rows from the Complete Dataset were used (merge of Training and Test Datasets) to train the model and the test rows to predict the churn risk. As model, a selection of 19 best features was used and the random forest used above.","adf7b4c8":"<a class=\"anchor\" id=\"mlponrf\">\n\n### 5.5. Trying to reduce overfitting with a MLP on top of 100 Random Forests <\/font> \n\n<\/a>\n","911fc522":"The following insights were found from the new variable transformations:\n- Dummy variables for the categorical records work without any limitation.\n- No missing values were found for the newly created variables.\n- Additionally, it was found that Tukey's fences is not suitable for outlier removal, while percentile method and standard deviation method maintain outlier percentage under a 3% threshold.\n- There are correlations among some of the newly created variables, which means that certain algorithms should work with variables dropped or with dataframes that do not have \"new variable\" transformations.\n- Both dummy and new variables are viable transformations for the final dataframes.","019c9a81":"The final datasets that will be used for modeling are described in the following table.","44e33349":"<a class=\"anchor\" id=\"missingvaluesmedian\">\n\n#### 4.1.1. Filling Missing Values by Median <\/font> \n\n<\/a>\n\nMissing values are filled in using each variable's median. After treatment, both train and test set are checked to guarantee no null values are present after the transformation.","9845301a":"<a class=\"anchor\" id=\"ensemblemethods\">\n\n#### 5.1.8. Other Ensemble Methods <\/font> \n\n<\/a>","b95af98d":"<a class=\"anchor\" id=\"outliers\">\n\n### 3.4. Outliers <\/font> \n\n<\/a>","3637cd06":"<a class=\"anchor\" id=\"overfittingchecking\">\n\n### 5.4. Overfitting Checking <\/font> \n\n<\/a>\n","c1af31ae":"__3.3.2. Categorical Variables:__ Plots and summary statistics are used to understand the categorical variables.  ","14399f7c":"The data is imported, along with the different libraries in Python. 2 separate dataframes are used: one for training and another one for testing.","a983cf32":"<a class=\"anchor\" id=\"nearestneighbor\">\n\n#### 5.1.6. Nearest Neighbor <\/font> \n\n<\/a>"}}