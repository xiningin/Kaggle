{"cell_type":{"afa815e4":"code","74804965":"code","94a8bfc3":"code","0376ab0a":"code","1ab55c04":"code","c0ab591a":"code","087d17c6":"code","8aaa9560":"code","8338753c":"code","83e3b3d3":"code","616d34c2":"code","265a63a0":"code","be4c9c0e":"code","4ae67201":"code","3e12b004":"code","07a188f6":"code","c3c08152":"code","41270ecc":"code","fb22120a":"code","d7bbf09b":"code","4fd6d458":"code","8f5a83b6":"code","34db4f06":"code","14eafef6":"code","3e844c7e":"code","5927839d":"code","668a65a3":"code","18f91d59":"code","64d29bb6":"code","7ccf2984":"code","e8197de6":"code","ea50e04e":"code","ddae957c":"code","9d91c64f":"code","be5fd682":"code","7e6a606f":"markdown","de2d4b4d":"markdown","a27b093f":"markdown","3818a19a":"markdown"},"source":{"afa815e4":"# !git clone --depth 1 -b v2.5.0 https:\/\/github.com\/tensorflow\/models.git\n# !pip install -Uqr models\/official\/requirements.txt\n# !pip install lime","74804965":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport sys\nsys.path.append('models')\n\nfrom official.nlp.data import classifier_data_lib\nfrom official.nlp.bert import tokenization\nfrom official.nlp import optimization\n\nfrom lime import lime_text\nfrom lime.lime_text import LimeTextExplainer","94a8bfc3":"print(\"TF Version: \", tf.__version__)\nprint(\"Eager mode: \", tf.executing_eagerly())\nprint(\"Hub version: \", hub.__version__)\nprint(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")","0376ab0a":"TARGET = \"target\"\nSEED = 42\nclass_names = ['Normal', 'disaster']\n\nTRAIN_PATH = \"..\/input\/nlp-getting-started\/train.csv\"\nTEST_PATH = \"..\/input\/nlp-getting-started\/test.csv\"\n\n\n\ndf = pd.read_csv(TRAIN_PATH)\ndf_test = pd.read_csv(TEST_PATH)\ndf_original = df.copy()\ndf.head()","1ab55c04":"df[TARGET].plot(kind='hist', title='Target distribution');","c0ab591a":"df[TARGET].value_counts() \/ len(df)","087d17c6":"df['text'].apply(len).plot(kind='hist', title='length distribution');","8aaa9560":"df.shape","8338753c":"replacement_patterns = [\n    (r'won\\'t', 'will not'),\n    (r'can\\'t', 'cannot'),\n    (r'i\\'m', 'i am'),\n    (r'ain\\'t', 'is not'),\n    (r'(\\w+)\\'ll', '\\g<1> will'),\n    (r'(\\w+)n\\'t', '\\g<1> not'),\n    (r'(\\w+)\\'ve', '\\g<1> have'),\n    (r'(\\w+)\\'s', '\\g<1> is'),\n    (r'(\\w+)\\'re', '\\g<1> are'),\n    (r'(\\w+)\\'d', '\\g<1> would'),\n]\n\nclass RegexpReplacer(object):\n    # Replaces regular expression in a text.\n    def __init__(self, patterns=replacement_patterns):\n        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n    \n    def replace(self, text):\n        s = text\n        \n        for (pattern, repl) in self.patterns:\n            s = re.sub(pattern, repl, s)\n        \n        return s\n\nclass SpellingReplacer(object):\n    \"\"\" Replaces misspelled words with a likely suggestion based on shortest\n    edit distance\n    \"\"\"\n    def __init__(self, dict_name='en', max_dist=2):\n        self.spell_dict = enchant.Dict(dict_name)\n        self.max_dist = max_dist\n    \n    def replace(self, word):\n        if self.spell_dict.check(word):\n            return word\n        \n        suggestions = self.spell_dict.suggest(word)\n        \n        if suggestions and edit_distance(word, suggestions[0]) <= self.max_dist:\n            return suggestions[0]\n        else:\n            return word\n\ndef clean_tweet(text) :\n    # remove urls\n    #text = df.apply(lambda x: re.sub(r'http\\S+', '', x))\n    # text = re.sub(r'http\\S+', ' ', text)\n\n    # replace contractions\n    replacer = RegexpReplacer()\n    text = replacer.replace(text)\n\n\n\n    return text\n\ndf['text'] = df['text'].apply(clean_tweet)","83e3b3d3":"df_original[TARGET].value_counts()","616d34c2":"# max_class_count = max(df[TARGET].value_counts())\n# min_class_count = min(df[TARGET].value_counts())\n# diff_class_count = max_class_count - min_class_count\n# df_disaster = df[df[TARGET] == 1]\n# df_sample = df_disaster.sample(diff_class_count, random_state=SEED)\n\n# df = df.append(df_sample)\n\n# df[TARGET].value_counts()","265a63a0":"df_train, df_val = train_test_split(df, random_state=SEED, test_size=0.1, stratify=df[TARGET].values)\n# df_val, df_test = train_test_split(remaining, random_state=SEED, test_size=0.5, stratify=remaining[TARGET].values)\ndf_train.shape, df_val.shape, df_test.shape","be4c9c0e":"with tf.device(\"\/cpu:0\"):\n  data_train = tf.data.Dataset.from_tensor_slices((df_train['text'], df_train[TARGET]))\n  data_val = tf.data.Dataset.from_tensor_slices((df_val['text'], df_val[TARGET]))\n  data_test = tf.data.Dataset.from_tensor_slices((df_test['text']))","4ae67201":"# model_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/2\"\nmodel_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/2\"\nbert_layer = hub.KerasLayer(model_url, trainable=True)","3e12b004":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","07a188f6":"text_length = df['text'].apply(lambda x: len(tokenizer.wordpiece_tokenizer.tokenize(x)))\n\nplt.figure(figsize=(10, 8))\nsns.distplot(text_length)\nprint(f'max lenth of text: {max(text_length)}')\nprint(f'avg lenth of text: {(text_length.mean())}');","c3c08152":"label_list = [0, 1] # Label categories\nmax_seq_length = 40 # maximum length of (token) input sequences\nbatch_size = 32","41270ecc":"def to_feature(text, label, label_list=label_list, max_seq_length=max_seq_length, tokenizer=tokenizer):\n  example = classifier_data_lib.InputExample(guid=None, text_a = text.numpy(), text_b=None, label = label.numpy())\n  feature = classifier_data_lib.convert_single_example(0, example, label_list,\n                                    max_seq_length, tokenizer)\n  \n  return (feature.input_ids, feature.input_mask, feature.segment_ids, feature.label_id)","fb22120a":"def map_feature(text, label):\n  input_ids, input_mask, segment_ids, label_id = tf.py_function(to_feature, inp=[text, label], \n                                Tout=[tf.int32, tf.int32, tf.int32, tf.int32])\n\n  input_ids.set_shape([max_seq_length])\n  input_mask.set_shape([max_seq_length])\n  segment_ids.set_shape([max_seq_length])\n  label_id.set_shape([])\n\n  x = {\n        'input_word_ids': input_ids,\n        'input_mask': input_mask,\n        'input_type_ids': segment_ids\n    }\n  return (x, label_id)","d7bbf09b":"with tf.device(\"\/cpu:0\"):\n  data_train = (data_train.map(map_feature, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n                          .shuffle(1000, seed=SEED)\n                          .batch(batch_size, drop_remainder=False)\n                          .prefetch(tf.data.experimental.AUTOTUNE))\n  \n  data_val = (data_val.map(map_feature, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n                          .batch(batch_size, drop_remainder=False)\n                          .prefetch(tf.data.experimental.AUTOTUNE))\n  \n  # data_test = (data_test.map(map_feature, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n  #                         .batch(batch_size, drop_remainder=True)\n  #                         .prefetch(tf.data.experimental.AUTOTUNE))","4fd6d458":"def to_feature_test(text, label_list=label_list, max_seq_length=max_seq_length, tokenizer=tokenizer):\n  example = classifier_data_lib.InputExample(guid=None, text_a = text.numpy(), text_b=None, label = None)\n  feature = classifier_data_lib.convert_single_example(0, example, None,\n                                    max_seq_length, tokenizer)\n  return (feature.input_ids, feature.input_mask, feature.segment_ids)\n\ndef map_feature_test(text):\n  input_ids, input_mask, segment_ids= tf.py_function(to_feature_test, inp=[text], \n                                  Tout=[tf.int32, tf.int32, tf.int32])\n  input_ids.set_shape([max_seq_length])\n  input_mask.set_shape([max_seq_length])\n  segment_ids.set_shape([max_seq_length])\n  x = {\n          'input_word_ids': input_ids,\n          'input_mask': input_mask,\n          'input_type_ids': segment_ids\n      }\n  return x\nwith tf.device(\"\/cpu:0\"):\n\n  data_test = (data_test.map(map_feature_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n                      .batch(batch_size, drop_remainder=False)\n                      .prefetch(tf.data.experimental.AUTOTUNE))","8f5a83b6":"def create_model(max_seq_length):\n  input_word_ids = tf.keras.layers.Input(shape=(max_seq_length, ), dtype=tf.int32, name=\"input_word_ids\")\n  input_mask = tf.keras.layers.Input(shape=(max_seq_length, ), dtype=tf.int32, name=\"input_mask\")\n  input_type_ids = tf.keras.layers.Input(shape=(max_seq_length, ), dtype=tf.int32, name=\"input_type_ids\")\n\n  pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])\n  # output = bert_layer([input_word_ids, input_mask, input_type_ids])\n  # pooled_output = output['pooled_output']\n  output = pooled_output\n\n  # output = tf.keras.layers.Dropout(0.3)(output)\n  output = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"finale_output\")(output)\n\n  model = tf.keras.Model( inputs={\n        'input_word_ids': input_word_ids,\n        'input_mask': input_mask,\n        'input_type_ids': input_type_ids\n    },\n    outputs=output)\n  return model","34db4f06":"print(f\"If we only predict ones: {df_val['target'].mean()}\\nIf we only predict zeros: {1 - df_val['target'].mean()}\")","14eafef6":"model = create_model(max_seq_length)\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\nmodel.summary()","3e844c7e":"%%time\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\ncheckpoint = ModelCheckpoint('bert_model.h5', monitor='val_accuracy', save_best_only=True)\n\ncallbacks = [checkpoint]\n\nepochs = 5\nhistory = model.fit(data_train,\n                    validation_data=data_val,\n                    epochs=epochs, callbacks=callbacks,\n                    # verbose=1\n                    )","5927839d":"model.load_weights('bert_model.h5')\nmodel.evaluate(data_val, verbose=1)","668a65a3":"preds = model.predict(data_val)\npreds = preds.round().astype(int)","18f91d59":"from sklearn.metrics import f1_score\n\nf1_score(df_val[TARGET], preds)","64d29bb6":"df_val_with_preds = df_val.copy()\ndf_val_with_preds['preds'] = preds\ndf_val_with_preds.reset_index(inplace=True)\ndf_wrong_preds = df_val_with_preds.query(\"target != preds\").reset_index()\ndf_wrong_preds","7ccf2984":"def new_predict(X):\n  test_data = tf.data.Dataset.from_tensor_slices((X))\n  test_data = (test_data.map(map_feature_test).batch(1))\n  pred = model.predict(test_data)\n  return np.hstack([1 - pred, pred])","e8197de6":"exp = LimeTextExplainer(class_names=class_names, random_state=SEED)","ea50e04e":"idx = 0\nexplained = exp.explain_instance(df_wrong_preds.iloc[idx]['text'], new_predict, num_features=5, top_labels=1, num_samples=100)\nexplained.show_in_notebook(text=df_val_with_preds.iloc[idx]['text'])","ddae957c":"train_loss = history.history['loss']         # train loss\ntrain_acc = history.history['accuracy']      # train accuracy\nval_loss = history.history['val_loss']       # validation loss\nval_acc = history.history['val_accuracy']    # validation accuracy\n\n# Plotting \nplt.figure(figsize=(20, 8))   # figure size\n\nplt.subplot(1, 2, 1)          # first plot: loss plot\n# line plot\nplt.plot( train_loss, label='train loss')   # train loss line plot\nplt.plot( val_loss, label='val loss')       # validation loss line plot\n\nplt.title('Loss')     # plot title\nplt.legend()          # to display labels\n\nplt.subplot(1, 2, 2)         # second plot: accuracy plot\n# line plot\nplt.plot(train_acc, label='train accuracy')    # train accuracy line plot\nplt.plot(val_acc, label='val accuracy')        # validation accuracy line plot\n\nplt.title('Accuracy')    # plot title\nplt.legend()             # to display labels\nplt.show();","9d91c64f":"test_pred = model.predict(data_test)\nsubmission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\nsubmission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('sub.csv', index=False)","be5fd682":"submission","7e6a606f":"#Analysis","de2d4b4d":"#Modeling","a27b093f":"#Preprocessing","3818a19a":"#Submission"}}