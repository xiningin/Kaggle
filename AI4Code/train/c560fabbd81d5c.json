{"cell_type":{"dfa7c912":"code","bfa393ec":"code","f861f0fc":"code","5ad0a741":"code","6680bc19":"code","016a09c0":"code","a98dfecf":"code","4c833a31":"code","237bf51c":"code","002f42a0":"code","e33f3edb":"code","05aef992":"code","f3a2b82e":"code","7f83de69":"code","21201ed6":"code","7ed808a8":"code","cd3ac80f":"code","caf2df4e":"code","e4343e07":"code","3df11b97":"code","4ea9a6e9":"code","a7cbb92a":"markdown","fefbc90b":"markdown","e5f35d57":"markdown","205218b7":"markdown","4c63adc5":"markdown","bf958a7c":"markdown"},"source":{"dfa7c912":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bfa393ec":"from tensorflow.keras.applications import VGG19\n# importing the dependencies\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing import image\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","f861f0fc":"conv_base = VGG19(weights='imagenet',\n                 include_top=False,\n                 input_shape=(150,150,3))","5ad0a741":"conv_base.summary()","6680bc19":"model = models.Sequential()\nmodel.add(conv_base)\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(256,activation = 'relu'))\nmodel.add(layers.Dense(1,activation='sigmoid'))\nmodel.summary()\n\n","016a09c0":"print(\"This is no. of trainable weights before freezing the conv base :\", len(model.trainable_weights))\nconv_base.trainable = False\nprint(\"The no of trainable weights after freezing conv Base :\", len(model.trainable_weights))","a98dfecf":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import optimizers","4c833a31":"import os, shutil\n# our dataset (only the train folder)\n\noriginal_dataset_dir = \"..\/input\/dogs-vs-cats\/train\/train\"    # we were asked to work with train part only for practice.\n\nprint('total images in train folder: ', len(os.listdir(original_dataset_dir)))\n# Create a Directory where we\u2019ll store our dataset\nbase_dir = \"..\/dog-cat-small\"\nos.mkdir(\"..\/dog-cat-small\")\n\n# directories for the training, validation and test splits\ntrain_dir = os.path.join(base_dir, 'train')\nos.mkdir(train_dir)\nvalidation_dir = os.path.join(base_dir, 'validation')\nos.mkdir(validation_dir)\ntest_dir = os.path.join(base_dir, 'test')\nos.mkdir(test_dir)\n\n# directory with training cat pictures\ntrain_cats_dir = os.path.join(train_dir, 'cats')\nos.mkdir(train_cats_dir)\n\n# directory with training dog pictures\ntrain_dogs_dir = os.path.join(train_dir, 'dogs')\nos.mkdir(train_dogs_dir)\n\n# directory with validation cat pictures\nvalidation_cats_dir = os.path.join(validation_dir, 'cats')\nos.mkdir(validation_cats_dir)\n\n# directory with validation dog pictures\nvalidation_dogs_dir = os.path.join(validation_dir, 'dogs')\nos.mkdir(validation_dogs_dir)\n\n# directory with test cat pictures\ntest_cats_dir = os.path.join(test_dir, 'cats')\nos.mkdir(test_cats_dir)\n\n# directory with test dog pictures\ntest_dogs_dir = os.path.join(test_dir, 'dogs')\nos.mkdir(test_dogs_dir)\n\n# copies the first 8750 cat images to train_cats_dir\nfnames = ['cat.{}.jpg'.format(i) for i in range(8750)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(train_cats_dir, fname)\n    shutil.copyfile(src, dst)\n\n# copies the next 2500 cat images to validation_cats_dir\nfnames = ['cat.{}.jpg'.format(i) for i in range(8750, 11250)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(validation_cats_dir, fname)\n    shutil.copyfile(src, dst)\n\n# copies the next 1250 cat images to test_cats_dir\nfnames = ['cat.{}.jpg'.format(i) for i in range(11250, 12500)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(test_cats_dir, fname)\n    shutil.copyfile(src, dst)\n\n# copies the first 8750 dog images to train_dogs_dir\nfnames = ['dog.{}.jpg'.format(i) for i in range(8750)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(train_dogs_dir, fname)\n    shutil.copyfile(src, dst)\n\n# copies the first 2500 dog images to validation_dogs_dir\nfnames = ['dog.{}.jpg'.format(i) for i in range(8750, 11250)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(validation_dogs_dir, fname)\n    shutil.copyfile(src, dst)\n\n# copies the first 1250 dog images to test_dogs_dir\nfnames = ['dog.{}.jpg'.format(i) for i in range(11250, 12500)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(test_dogs_dir, fname)\n    shutil.copyfile(src, dst)","237bf51c":"# Seeing the content count of the splits\nprint('total training cat images:', len(os.listdir(train_cats_dir)))\nprint('total training dog images:', len(os.listdir(train_dogs_dir)))\nprint('total validation cat images:', len(os.listdir(validation_cats_dir)))\nprint('total validation dog images:', len(os.listdir(validation_dogs_dir)))\nprint('total test cat images:', len(os.listdir(test_cats_dir)))\nprint('total test dog images:', len(os.listdir(test_dogs_dir)))","002f42a0":"train_datagen = ImageDataGenerator(\n                rescale = 1.\/255,\n                rotation_range = 40,\n                width_shift_range = 0.2,\n                height_shift_range = 0.2,\n                shear_range = 0.2,\n                zoom_range = 0.2,\n                horizontal_flip = True,\n                fill_mode = 'nearest')\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n                  train_dir,\n                  target_size  = (150,150),\n                  batch_size = 20,\n                  class_mode = 'binary')  \n\nvalid_generator = test_datagen.flow_from_directory(\n                  validation_dir,\n                  target_size  = (150,150),\n                  batch_size = 20,\n                  class_mode = 'binary')  \n","e33f3edb":"model.compile(loss='binary_crossentropy',optimizer = optimizers.RMSprop(lr=2e-5),metrics=['acc'])","05aef992":"batch_size= 20","f3a2b82e":"history = model.fit_generator(\n          train_generator,\n          validation_data= valid_generator,\n          validation_steps= 50,\n          epochs = 10)","7f83de69":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']","21201ed6":"epochs = range(1,11)\nplt.plot(epochs,acc,'bo',label = 'Training Acc')\nplt.plot(epochs,val_acc,'b',label = 'Validation Acc')\nplt.title('Training and Validation Accuracy')\nplt.legend()\nplt.figure()","7ed808a8":"plt.plot(epochs,loss,'bo',label = 'Training Loss')\nplt.plot(epochs,val_loss,'b',label = 'Validation Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.figure()\n","cd3ac80f":"#Unfreezing last 3 Convnet because they contain minutes features which are important:\n\nconv_base.trainable = True\n\nset_trainable = False\nfor layer in conv_base.layers:\n    if layer.name =='block5_conv1':\n        set_trainable = True\n    if set_trainable:\n        layer.trainable = True\n    else:\n        layer.trainable = False","caf2df4e":"model.compile(loss='binary_crossentropy',\n              optimizer=optimizers.RMSprop(lr=1e-5),\n              metrics=['acc'])\nhistory = model.fit_generator(\n          train_generator,\n          steps_per_epoch=100,\n          epochs=100,\n          validation_data=valid_generator,\n          validation_steps=50)","e4343e07":"epochs = range(1,101)\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']","3df11b97":"def smooth_curve(points, factor=0.8):\n    smoothed_points = []\n    for point in points:\n        if smoothed_points:\n            previous = smoothed_points[-1]\n            smoothed_points.append(previous * factor + point * (1 - factor))\n        else:\n            smoothed_points.append(point)\n    return smoothed_points\nplt.plot(epochs,smooth_curve(acc), 'bo', label='Smoothed training acc')\nplt.plot(epochs,smooth_curve(val_acc), 'b', label='Smoothed validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs,\nsmooth_curve(loss), 'bo', label='Smoothed training loss')\nplt.plot(epochs,\nsmooth_curve(val_loss), 'b', label='Smoothed validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()\n\n","4ea9a6e9":"test_generator = test_datagen.flow_from_directory(test_dir,target_size=(150, 150),batch_size=20,class_mode='binary')\ntest_loss, test_acc = model.evaluate_generator(test_generator, steps=50)\nprint('test acc: ', test_acc)","a7cbb92a":"# Using fine tuning\n","fefbc90b":"**You can see that How our accuracy increased boom by fine tuning the conv layers and increasing the no. of epochs**<br>\nYou can increase by some more by using grid search CV to find best hyperparameters.","e5f35d57":"**We can basically fine tune our model by tuning last 3 Conv Layers by not freezing them up.**<br>\nLets try to understand why won't we tune the whole Conv Layers of VGG19 model:<br>\n\n1. Earlier layers in the convolutional base encode more-generic, reusable features,\n   whereas layers higher up encode more-specialized features. It\u2019s more useful to\n   fine-tune the more specialized features, because these are the ones that need to\n   be repurposed on your new problem. There would be fast-decreasing returns in\n   fine-tuning lower layers.\n   \n2. The more parameters you\u2019re training, the more you\u2019re at risk of overfitting.\n   The convolutional base has 15 million parameters, so it would be risky to\n   attempt to train it on your small dataset.\n\n\n","205218b7":"# Please give upvote if you like notebook. ","4c63adc5":"Only the weights of two Dense layers that added will be trained. Thats a total of four weights tensor two per layer i.e that main weight matrix and the bias vector.\n","bf958a7c":"Freezing a layer or set of layers means preventing their weights from being updated during training."}}