{"cell_type":{"69415c26":"code","11bdb263":"code","9c0ccf89":"code","4b24be3e":"code","7d27f397":"code","e4417678":"code","fdf8974c":"code","adc4edbd":"code","eeeb644d":"code","3874f14e":"code","da0dc14c":"code","c52a27e5":"code","2bca1d8e":"code","817d32a3":"code","8a647ebf":"code","a178e7d7":"code","71c3768f":"markdown","1531bc09":"markdown"},"source":{"69415c26":"from keras.initializers import he_normal, normal\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier","11bdb263":"df = pd.read_csv('..\/input\/spambase\/realspambase.data', header=None)\n\n# features first 57 columns\n# last column is labels\nX_features, y_labels = df.iloc[:, :57].values, df.iloc[:, 57].values\n\n# Scale the features\nX_features = StandardScaler().fit_transform(X_features)\n\nx_train, x_test, y_train, y_test = train_test_split(X_features, y_labels, test_size=0.1, random_state=15)\n\ncallbacks = [\n    ReduceLROnPlateau(),\n    EarlyStopping(patience=4),\n    ModelCheckpoint(filepath='model-conv1d.h5', save_best_only=True)\n]","9c0ccf89":"def model(activation='tanh', units_per_layer=10, n_hidden_layers=2):\n    model = Sequential()\n    # First hidden layer, taking 57 features as inputs\n    model.add(Dense(input_dim=57,\n                    units=units_per_layer,\n                    kernel_initializer='uniform',\n                    activation=activation))\n\n    # Additional hidden layers\n    for i in range(1, n_hidden_layers):\n        model.add(Dense(units=units_per_layer,\n                    kernel_initializer='uniform',\n                    activation=activation))\n\n\n    # binary classification layer\n    model.add(Dense(units=1,\n                    kernel_initializer='uniform',\n                    activation='sigmoid',\n                    name='output'))\n\n    model.compile(optimizer='adam', loss='binary_crossentropy',\n                  metrics=['acc'])\n\n    #print(model.summary())\n\n    return model","4b24be3e":"def model_eval(model, X_test, y_test):\n\n    predicted_probs = model.predict(X_test, verbose=0)\n    predicted_classes = model.predict_classes(X_test, verbose=0)\n\n    # sklearn metrics require 1D array of actual and predicted vals\n    # need to transform the data into 1D from 2D\n\n    predicted_probs = predicted_probs[:, 0]\n    predicted_classes = predicted_classes[:, 0]\n\n    # accuracy: (tp + tn) \/ (p + n)\n    accuracy = accuracy_score(y_test, predicted_classes)\n    print('Accuracy: {}'.format(accuracy))\n    # precision tp \/ (tp + fp)\n    precision = precision_score(y_test, predicted_classes)\n    print('Precision: {}'.format(precision))\n    # recall: tp \/ (tp + fn)\n    recall = recall_score(y_test, predicted_classes)\n    print('Recall: {}'.format(recall))\n    # f1: 2 tp \/ (2 tp + fp + fn)\n    f1 = f1_score(y_test, predicted_classes)\n    print('F1 score: {}'.format(f1))\n    \n    return accuracy","7d27f397":"def cross_val(chosen_model):\n\n    model = KerasClassifier(chosen_model, verbose=True)\n    \n    activation=['tanh', 'relu']\n    units_per_layer=[10, 20]\n    n_hidden_layers=[2, 3, 4]\n    epochs=[20]\n\n    param_grid = dict(activation=activation, units_per_layer=units_per_layer, \n                      n_hidden_layers=n_hidden_layers, epochs=epochs)\n\n    grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,n_jobs=-1,\n                              cv=5, verbose=5, n_iter=12, scoring=['accuracy', 'f1', 'precision', 'recall'], refit='accuracy')\n    grid_result = grid.fit(x_train, y_train)\n\n    test_accuracy = grid.score(x_train, y_train)\n\n    print(grid_result.best_score_)\n    print(grid_result.best_params_)\n\n    print(test_accuracy)\n\n    return grid_result.best_params_","e4417678":"def plot_model(history):\n\n    # training & validation accuracy\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    # training & validation loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()","fdf8974c":"from sklearn.metrics import roc_curve","adc4edbd":"def plot_roc_curve(model, X_test, y_test):\n    y_score = model.predict(X_test, verbose=0)\n    fpr, tpr, _ = roc_curve(y_test, y_score)\n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr)\n    ax.set_ylabel('True Positive Rate')\n    ax.set_xlabel('False Positive Rate')\n    ax.set_title('ROC Curve')    ","eeeb644d":"model_original = model()\nhistory = model_original.fit(x_train, y_train, validation_split=0.10, epochs=20)\noriginal_testing_acc = model_eval(model_original, x_test, y_test)\nplot_model(history)\nplot_roc_curve(model_original, x_test, y_test)","3874f14e":"print('Original Model:\\nTraining Acc: {:0.4}\\nValidation Acc: {:0.4}\\nTesting Acc: {:0.4}'.format(history.history['acc'][-1], history.history['val_acc'][-1], original_testing_acc))","da0dc14c":"param_results = cross_val(model)\nprint(param_results)","c52a27e5":"model_optimized = model(activation=param_results['activation'], units_per_layer=param_results['units_per_layer'], n_hidden_layers=param_results['n_hidden_layers'])\nmodel_history = model_optimized.fit(x_train, y_train, validation_split=0.10, epochs=param_results['epochs'])\noptimized_acc = model_eval(model_optimized, x_test, y_test)\nplot_model(model_history)\nplot_roc_curve(model_optimized, x_test, y_test)","2bca1d8e":"print('Optimized Model:\\nTraining Acc: {:0.4}\\nValidation Acc: {:0.4}\\nTesting Acc: {:0.4}'.format(model_history.history['acc'][-1], model_history.history['val_acc'][-1], optimized_acc))","817d32a3":"#Decision Tree\nclf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100, max_depth=2, min_samples_leaf=7)\nclf_gini.fit(x_train, y_train)\npred_gini = clf_gini.predict(x_test)\nprint(\"DECISION TREE\")\nprint(\"Accuracy\")\nprint (accuracy_score(pred_gini, y_test))\nprint(\"Report\")\nprint(classification_report(pred_gini, y_test))","8a647ebf":"#Random Forest\nprint(\"RANDOM FOREST\")\nclf_random_forest = RandomForestClassifier(max_depth=2, random_state=0)\nclf_random_forest.fit(x_train, y_train)\npred_random_forest = clf_random_forest.predict(x_test)\nprint(\"Accuracy\")\nprint(accuracy_score(pred_random_forest, y_test))\nprint(\"Report\")\nprint(classification_report(pred_random_forest, y_test))","a178e7d7":"#K-Nearest Neighbor\nprint(\"K-NEAREST NEIGHBORS\")\nclf_knn = KNeighborsClassifier(n_neighbors=3)\nclf_knn.fit(x_train, y_train)\npred_knn = clf_knn.predict(x_test)\nprint(\"Accuracy\")\nprint(accuracy_score(pred_knn, y_test))\nprint(\"Report\")\nprint(classification_report(pred_knn, y_test))","71c3768f":"## Spam Classifier with Hyper-parameter Optimization","1531bc09":"#### Execution"}}