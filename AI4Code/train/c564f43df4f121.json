{"cell_type":{"b86f6b4a":"code","b6007439":"code","c22571a2":"code","d15d065c":"code","71d4b02a":"code","54560a70":"code","3ce03f89":"code","d5f52a69":"code","eaad959b":"code","8169cab0":"code","cc2ac8b3":"code","02dc3c0f":"code","e195fde6":"code","1cc3858f":"code","f13b6ba3":"code","42685ec7":"code","f86cfa7e":"code","7663dabe":"code","41712ff2":"code","d677d239":"code","5f7120ac":"code","71ecc672":"code","fc53676a":"code","5de73d22":"code","ed6fce71":"code","be4965f0":"code","211d52b5":"code","3dd5d6ab":"code","4aa57a52":"code","90a3b5c7":"code","1a6aa4f3":"code","e4f25ee9":"markdown","33565b47":"markdown","9ca9742b":"markdown","765602aa":"markdown","6edc6ec0":"markdown","5477b6a2":"markdown","67fc5dcc":"markdown","8b78b6bb":"markdown","d78f3f98":"markdown"},"source":{"b86f6b4a":"# Environment setup\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))","b6007439":"# Load Data\nurl = \"\/kaggle\/input\/titanic\/train.csv\"\ntrain_data = pd.read_csv(url)","c22571a2":"train_data.info()\ntrain_data.head()","d15d065c":"# Let's check the possibilities of the column \"Embarked\":\ntrain_data[\"Embarked\"].unique()","71d4b02a":"# and of the column \"Pclass\":\ntrain_data[\"Pclass\"].unique()","54560a70":"#Let's make a copy of the main training data, not to lose data by mistake.\ntd = train_data.copy()","3ce03f89":"# Removing \"name\" and \"ticket\" columns\ntd.drop(['Name','Ticket'],axis=1,inplace=True)","d5f52a69":"td['Cabin'] = td['Cabin'].apply(lambda x: 0 if type(x)==float else 1)\n\ntd['Sex'] = pd.get_dummies(td['Sex'],drop_first=1)\n\nembarked = pd.get_dummies(td['Embarked'],drop_first=1)\n#In the new emabarked variable, the two NaN values will be treated as 'C'.\n\ntd = pd.concat([td,embarked],axis=1)\ntd.drop(['Embarked'],axis=1,inplace=True)","eaad959b":"def age_estimator(CSA):\n    pclass = CSA['Pclass']\n    sex = CSA['Sex']\n    age = CSA['Age']\n    \n    df_test = CSA[CSA.isnull()['Age']==False]\n    \n    PS11 = np.median(df_test[(df_test['Pclass']==1) & (df_test['Sex']==1)]['Age'])\n    PS10 = np.median(df_test[(df_test['Pclass']==1) & (df_test['Sex']==0)]['Age'])\n    PS21 = np.median(df_test[(df_test['Pclass']==2) & (df_test['Sex']==1)]['Age'])\n    PS20 = np.median(df_test[(df_test['Pclass']==2) & (df_test['Sex']==0)]['Age'])\n    PS31 = np.median(df_test[(df_test['Pclass']==3) & (df_test['Sex']==1)]['Age'])\n    PS30 = np.median(df_test[(df_test['Pclass']==3) & (df_test['Sex']==0)]['Age'])\n    \n    age_update = []\n    for p in range(0,CSA.shape[0]):\n        age_update.append(age.iloc[p])\n        if age.isnull().iloc[p]:\n            if pclass.iloc[p] == 1:\n                if sex.iloc[p] == 1:\n                    age_update[p] = PS11\n                else:\n                    age_update[p] = PS10\n            elif pclass.iloc[p] == 2:\n                if sex.iloc[p] == 1:\n                    age_update[p] = PS21\n                else:\n                    age_update[p] = PS20\n            else:\n                if sex.iloc[p] == 1:\n                    age_update[p] = PS31\n                else:\n                    age_update[p] = PS30\n    \n    return pd.DataFrame(age_update)","8169cab0":"td['Age'] = age_estimator(td)","cc2ac8b3":"td.info()\ntd.head()","02dc3c0f":"import seaborn as sns\nsns.displot(td[(td['Fare']>5) & (td['Fare']<40)],x='Fare',hue='Pclass')\nsns.displot(td[(td['Age']>20) & (td['Age']<50)],x='Age',hue='Pclass')","e195fde6":"td.loc[td['Fare']<=10,'Fare'] = 3\ntd.loc[(td['Fare']>10) & (td['Fare']<=25),'Fare'] = 2\ntd.loc[td['Fare']>25,'Fare'] = 1","1cc3858f":"td.loc[td['Age']<=16,'Age'] = 0\ntd.loc[(td['Age']>16) & (td['Age']<=25),'Age'] = 3\ntd.loc[(td['Age']>25) & (td['Age']<=35),'Age'] = 2\ntd.loc[(td['Age']>35) & (td['Age']<60),'Age'] = 1\ntd.loc[td['Age']>=60,'Age'] = 1","f13b6ba3":"#Loading Data\nurl = \"\/kaggle\/input\/titanic\/test.csv\"\ntest_data = pd.read_csv(url)\n#Preparation\ntest_data.drop(['Name','Ticket'],axis=1,inplace=True)\ntest_data['Cabin'] = test_data['Cabin'].apply(lambda x: 0 if type(x)==float else 1)\ntest_data['Sex'] = pd.get_dummies(test_data['Sex'],drop_first=1)\nembarked = pd.get_dummies(test_data['Embarked'],drop_first=1)\ntest_data = pd.concat([test_data,embarked],axis=1)\ntest_data.drop(['Embarked'],axis=1,inplace=True)\ntest_data['Age'] = age_estimator(td)\n#Preprocessing\ntest_data.loc[test_data['Fare']<=10,'Fare'] = 3\ntest_data.loc[(test_data['Fare']>10) & (test_data['Fare']<=25),'Fare'] = 2\ntest_data.loc[test_data['Fare']>25,'Fare'] = 1\ntest_data.loc[test_data['Age']<=16,'Age'] = 0\ntest_data.loc[(test_data['Age']>16) & (test_data['Age']<=25),'Age'] = 3\ntest_data.loc[(test_data['Age']>25) & (test_data['Age']<=35),'Age'] = 2\ntest_data.loc[(test_data['Age']>35) & (test_data['Age']<60),'Age'] = 1\ntest_data.loc[test_data['Age']>=60,'Age'] = 1","42685ec7":"td.columns","f86cfa7e":"features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Q', 'S']\nX_train = td[features]\nX_test = test_data[features]\ny_train = td['Survived']","7663dabe":"#Let's check the test data\nX_test.info()","41712ff2":"#One data of \"Fare\" is NaN. Let's assign the value of \"Pclass\" to it.\nidx = X_test[pd.isnull(X_test['Fare'])].index.tolist()\n#print(idx)\nX_test['Fare'].loc[idx] = X_test['Pclass'].loc[idx]","d677d239":"from sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier","5f7120ac":"def err_report(y_true,y_predict):\n    print(confusion_matrix(y_true,y_predict))\n    print(\"\\n\")\n    print(classification_report(y_true,y_predict))","71ecc672":"#Logistic Regression\nLR = LogisticRegression(max_iter=1000)\nLR.fit(X_train,y_train)\n#y_LR = pd.Series(LR.predict(X_train))\ny_LR = LR.predict(X_train)\nerr_report(y_train,y_LR)","fc53676a":"#K-Nearest Neighbors\n\n#First, let's decide on n_neighbors\nmean_error = []\nmax_N = 20\nfor i in range(1,max_N):\n    KN = KNeighborsClassifier(n_neighbors=i)\n    KN.fit(X_train,y_train)\n    KN_pred = KN.predict(X_train)\n    #err = np.sqrt(sum((y_train-KNN_pred)^2))\n    err = np.mean(KN_pred != y_train)\n    mean_error.append(err)\nplt.figure(figsize=(10,6))\nplt.plot(range(1,max_N),mean_error,linestyle='dashed', color='red',\n         marker='o', markerfacecolor='none', markersize=10)\nplt.title('Mean Error vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Mean Error')","5de73d22":"#Now, let's use the most accurate one (n=3)\nKN = KNeighborsClassifier(n_neighbors=3)\nKN.fit(X_train,y_train)\ny_KN = KN.predict(X_train)\nerr_report(y_train,y_KN)","ed6fce71":"#Decision Tree\nDT = DecisionTreeClassifier()\nDT.fit(X_train,y_train)\ny_DT = DT.predict(X_train)\nerr_report(y_train,y_DT)","be4965f0":"#Random Forest\n\n#First, let's decide on n_estimator\nmean_error = []\nmax_N = 50\nfor i in range(1,max_N):\n    RF = RandomForestClassifier(n_estimators=i)\n    RF.fit(X_train,y_train)\n    RF_pred = RF.predict(X_train)\n    #err = np.sqrt(sum((y_train-KNN_pred)^2))\n    err = np.mean(RF_pred != y_train)\n    mean_error.append(err)\nplt.figure(figsize=(10,6))\nplt.plot(range(1,max_N),mean_error,linestyle='dashed', color='red',\n         marker='o', markerfacecolor='none', markersize=10)\nplt.title('Mean Error vs. Forest Size')\nplt.xlabel('Size')\nplt.ylabel('Mean Error')","211d52b5":"#Let's go for size of 40, where the error is relatively stable and low.\nRF = RandomForestClassifier(n_estimators=40)\nRF.fit(X_train,y_train)\ny_RF = RF.predict(X_train)\nerr_report(y_train,y_RF)","3dd5d6ab":"#AdaBoost\nAB = AdaBoostClassifier(n_estimators=40)\nAB.fit(X_train,y_train)\ny_AB = AB.predict(X_train)\nerr_report(y_train,y_AB)","4aa57a52":"#Support Vector Machine\nSV = SVC()\nSV.fit(X_train,y_train)\ny_SV = SV.predict(X_train)\nerr_report(y_train,y_SV)","90a3b5c7":"#Multi-layer Perceptron\nMP = MLPClassifier(solver='lbfgs',learning_rate='adaptive',max_iter=1000,tol=0.005)\nMP.fit(X_train,y_train)\ny_MP = MP.predict(X_train)\nerr_report(y_train,y_MP)","1a6aa4f3":"prediction = SV.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': prediction})\noutput.to_csv('my_submission_SV.csv', index=False)","e4f25ee9":"# **4. Reporting**\n\nExporting different data (with and without pre-processing of data), the highest score (0.77) was on \"SVC\" for the pre-processed data, which is close to the score of (0.76) of \"Logistic Regression\" of the ***not*** pre-precessed data.","33565b47":"# **1. Introduction**\n\nLet's try different classifiers on solving this problem.","9ca9742b":"**Age**\n\nThe **Age** column consists of many NaNs. While there are many ways to guess the unknown data, I'd go for the median for the sets of {'Pclass','Sex'}.****","765602aa":"# **2.4. Defining X and y**\n\nNow, that the data are ready for the processing, let's put them into two variables X and y.","6edc6ec0":"# **2.2. Data Preprocessing**\n\nNow, all data are in hand but possible to simplify, for example, the ***Age*** and ***Fare*** data. Using the following figures, it will be clearer.\nPlus, we shouldn't forget to add categories for ***children (age<16)*** and ***elderly (age>60)***.","5477b6a2":"# **3. Data Processing: calling classifiers**\n\nLet's try different classifiers:\n1. LR: Logistic Regression\n2. KN: K-Nearest Neighbors*\n3. DT: Decision Tree\n4. RF: Random Forest*\n5. AB: AdaBoost\n6. SV: Support Vector Machine\n7. MP: Multi-layer Perceptron\n\n( * ) The sizing of these methods are decided using a complementary function.\n\n***note***: the way the errors are defined is not acceptable. A simple remedy is to apply \".train_test_split\" on the training data. In here, however, I used all training data for both training and testing the accuracy of the fitted classifiers!","67fc5dcc":"# **2. Data Investigation**\n\nLet's open the training data set, and have a glance on it.\n","8b78b6bb":"# **2.1. Data Preparation**\nAmong different data, I guess that the passengers' **Name** and **Ticket** did not affect their survival!\n\n**Cabin**: it is not clear to me, but if it shows whether a cabin was assigned to the passenger, its data should be changed to a boolean -> 0: not assigned | 1: assigned\n\n**Embarked**: 889 of data are either S, C or Q and 2 are nan. The first 3 can be mapped into 0, 1 and 2, respectively. For the last one (nan), let's figure it out later.\n\n***Categorical data***: it is possible to map categorical data on a predefined set of variables via user-defined functions; however, it can be cumborsome. For example, for working with column **Sex** and **Embarked**, one can use **.map()**, or even **lambda** functions:\n\n* td['Sex'].apply(lambda x: 0 if x=='female' else 1) **with: female:0 and male:1**\n* td['Embarked'].apply(lambda x: 0 if x=='S' else (1 if x=='C' else (2 if x=='Q' else 3))) **with: S\/C\/Q\/NAN = 0\/1\/2\/3**\n\nOne obvious disadvantage is the amount work for doing the job. An easier way is to work with ***pd.get_dummies*** to convert categorical data into dummy\/indicator variables. It should be noted that the in this case, generally, one set of data become redundant. For example: **Sex** generates two columns of **male** and **female**, which are complementary, and, in this case, one can be safely ignored.\n\n**Writing a function**: Perhaps, it would be beneficial to write this section as a function, so that both train and test data can be easily prepared. For here, though, let's repeat the steps for both sets.\n","d78f3f98":"# **2.3. The test data**\n\nNow, let's repeat the steps for the test data set."}}