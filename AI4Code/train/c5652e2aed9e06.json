{"cell_type":{"d84f5680":"code","a74c65f3":"code","2e8167a3":"code","e33906e7":"code","0f6bda9b":"code","8fb90e25":"code","8047786c":"code","89a35d5c":"code","cd17c2ee":"code","79412570":"code","ca4db2b9":"code","284e7ac7":"markdown","a415eccb":"markdown","0aeef9f9":"markdown","03ee2887":"markdown","f7339dfe":"markdown","d2b3b054":"markdown","590eddf5":"markdown","3658229f":"markdown","08f4b00e":"markdown","5be8fb7f":"markdown","db3e61ba":"markdown","e9762b12":"markdown"},"source":{"d84f5680":"#!\/usr\/bin\/python3\n# coding=utf-8\n#===========================================================================\n# This is a minimal script to perform a classification \n# using the logistic regression classifier from scikit-learn \n# Carl McBride Ellis (18.IV.2020)\n#===========================================================================\n#===========================================================================\n# load up the libraries\n#===========================================================================\nimport pandas as pd\nimport numpy  as np\n\n#===========================================================================\n# read in the data\n#===========================================================================\ntrain_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data  = pd.read_csv('..\/input\/titanic\/test.csv')\n\n#===========================================================================\n# features we use\n#===========================================================================\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\"]\n\n#===========================================================================\n# for the features that are categorical we use pd.get_dummies:\n# \"Convert categorical variable into dummy\/indicator variables.\"\n#===========================================================================\nX_train  = pd.get_dummies(train_data[features])\ny_train  = train_data[\"Survived\"]\nX_test   = pd.get_dummies(test_data[features])\n\n#===========================================================================\n# perform the classification\n#===========================================================================\nfrom sklearn.linear_model import LogisticRegression\n# we use the default Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm\nclassifier = LogisticRegression(solver='lbfgs',fit_intercept=False)\nclassifier.fit(X_train, y_train)\n\n#===========================================================================\n# use the model to predict 'Survived' for the test data\n#===========================================================================\npredictions   = classifier.predict(X_test)\nprobabilities = classifier.predict_proba(X_test)\n\n#===========================================================================\n# write out CSV submission file\n#===========================================================================\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, \n                       'Survived': predictions})\noutput.to_csv('submission.csv', index=False)","a74c65f3":"print(classifier.coef_)","2e8167a3":"import eli5","e33906e7":"eli5.show_weights(classifier, feature_names = X_train.columns.tolist())","0f6bda9b":"def sigmoid(x):\n    return 1\/(1 + np.exp(-x))\n\nimport matplotlib.pyplot as plt\n\nx = np.arange(-6,6,0.05) \ny = sigmoid(x)\nfig, ax = plt.subplots(1, 1, figsize=(9, 4))\nax.plot(x,y, lw=3)\n#ax.set(xticklabels=[]) \n#ax.set(yticklabels=['9:00','10:00','11:00','12:00','13:00','14:00','15:00' ,'16:00'])  \nax.set_title(\"S(x)\", fontsize=18)\nplt.grid(True, color='w', linestyle='-', linewidth=2)\nplt.gca().patch.set_facecolor('0.8')\nplt.show();","8fb90e25":"test_data.iloc[79]","8047786c":"X_test.iloc[79]","89a35d5c":"predictions[79]","cd17c2ee":"probabilities[79:80]","79412570":"p_one = sigmoid(2.53528797*1 + 0.97048816*0 + 0.67457572*1 + 0.46849080*0\n              - 0.03364347*0 - 0.11317645*0 - 0.20849115*0 - 0.83364195*3)\nprint(p_one)","ca4db2b9":"eli5.show_prediction(classifier, X_test.iloc[79],  show_feature_values=True)","284e7ac7":"# Titanic explainability: Why me? asks Miss Doyle\n\nSometimes pure predictive capacity is not the only thing we ask of our machine learning models, and increasingly it is just as important to be able explain how we arrived at our predictions.\n\n### Explainability and the GDPR\nBeing able to easily explain how a model works, or how a decision was made based on the model, is not a mere intellectual nicety; in fact the [EU General Data Protection Regulation (GDPR) 2016\/679](https:\/\/eur-lex.europa.eu\/eli\/reg\/2016\/679), Article 15(1)(h) states:\n\n> \"*The data subject shall have the right to obtain... ...meaningful information about the logic involved, as well as the significance and the envisaged consequences of such processing*\"\n\nalso, in Article 22:\n\n> \"*The data subject shall have the right to obtain... ...human intervention on the part of the controller, to express his or her point of view and to contest the decision.*\"\n\nIn order to comply with this, the data scientist must be able to clearly explain how any decision was originally arrived at. \n\nNon-compliance with the GDPR by a company can result in serious consequences, and it is part of the job of a data scientist to mitigate such risks for their employers. (For those interested the website [GDPR Enforcement Tracker](https:\/\/www.enforcementtracker.com\/) has a partial list of fines that have been imposed).\n\nThe following is a small demonstration script which applies the [logistic regression classifier from scikit-learn](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html) to the [Titanic data set](https:\/\/www.kaggle.com\/c\/titanic). ","a415eccb":"Here we can see that our model predicts that Miss. Doyle indeed survives (*i.e.* a prediction of 1):","0aeef9f9":"How was this value arrived at? Let us look at the raw probabilites for the prediction","03ee2887":"and the data that we actually used in our model","f7339dfe":"We can now see that our model is given by\n\n\n$$ p(1) = S(2.53528797(\\mathrm{Sex\\_female}) \n+ 0.97048816(\\mathrm{Embarked\\ at\\ Cherbourg}) \n+ 0.67457572(\\mathrm{Embarked\\ at\\ Queenstown})\n+ 0.4684908(\\mathrm{Embarked\\ at\\ Southampton})\n- 0.03364347(\\mathrm{Parch}) \n- 0.11317645(\\mathrm{Sex\\_male}) \n- 0.20849115(\\mathrm{SibSp}) \n- 0.83364195(\\mathrm{Pclass}))$$\n\n\nwhere $S$ is the [sigmoid function](https:\/\/en.wikipedia.org\/wiki\/Sigmoid_function)\n\n$$ S(x) = \\frac{1}{1+\\exp(-x)}$$\n\nwhich graphically looks like","d2b3b054":"we can see that the probability of a 0 is 0.32983358, and conversly, the probability of a 1 is 0.67016642. Let us run the numbers through the model by hand","590eddf5":"where the coefficients are known as the *weights*. The positive weights are in green and the negative weights are in red","3658229f":"Given that our [discrimination threshold](https:\/\/www.kaggle.com\/carlmcbrideellis\/discrimination-threshold-false-positive-negative) is set to 0.5, and given that 0.67 > 0.5 then our binarized result is a 1. Thanks to ELI5 we can see this all presented in a nice table","08f4b00e":"## We now have our model!\n...but what does it look like?","5be8fb7f":"# Why me? A look at Miss. Doyle\nLet us look at an individual prediction. We have been asked by Miss. Doyle to explain to her the reason, according to our machine learning model, that she survived. Here is all the data we have on Miss. Doyle","db3e61ba":"where we can see visually that being in 3rd class was actually a large factor against surviving, however, being female was a big factor in favor of surviving, as too was embarking at Queenstown, which leads to a final overall positive survival prediction for Miss. Doyle.\n\nWith this data we can now clearly explain to Miss. Doyle how we arrived at our prediction.\n\n# Related links\n* [Logistic regression](https:\/\/en.wikipedia.org\/wiki\/Logistic_regression) on Wikipedia\n* [Sigmoid function](https:\/\/en.wikipedia.org\/wiki\/Sigmoid_function) on Wikipedia\n* [Logistic Regression classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html) on Scikit-learn\n* [ELI5 documentation](https:\/\/eli5.readthedocs.io\/en\/latest\/index.html)\n\nkaggle notebooks\n\n* [\"Explainability, collinearity, and the VIF\"](https:\/\/www.kaggle.com\/carlmcbrideellis\/explainability-collinearity-and-the-vif)\n* [\"GPU accelerated SHAP values: Jane Street example\"](https:\/\/www.kaggle.com\/carlmcbrideellis\/gpu-accelerated-shap-values-jane-street-example)","e9762b12":"this is our model, however it is a little cryptic. Let us use [ELI5 library](https:\/\/eli5.readthedocs.io\/en\/latest\/index.html) to make this more readable"}}