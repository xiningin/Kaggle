{"cell_type":{"15a7c6a3":"code","04bc3630":"code","fac2f4ad":"code","8e1a1cba":"code","09ad51e8":"code","ca248590":"code","a148e05d":"code","d724f928":"code","c1ab146a":"code","3c848ca3":"code","2b69264b":"code","a84404f4":"code","02359e68":"code","9fa126cd":"code","2971e853":"code","e78a45ca":"code","76d0beaf":"code","a93152dd":"code","f992c3f1":"code","4fdbec32":"code","3072078e":"code","871207e7":"code","08f2da89":"code","237f9df4":"code","8a53d5fc":"code","4d36a85d":"code","d424a638":"code","1c6f8824":"code","0604aaf6":"code","77b155bf":"code","cf0d7a4e":"code","62ffb7bd":"code","25dabfd6":"code","3a808419":"code","0a5b9d7a":"code","bf7a5c65":"code","c214ee83":"code","1a7f0b54":"code","8d83ba56":"code","6792f096":"code","fb666e72":"code","3dd0e186":"code","517da48d":"markdown","6db80a87":"markdown","fab6866d":"markdown","3de0945f":"markdown","88d20446":"markdown","e5d36813":"markdown","a8a4408a":"markdown","3a5199d2":"markdown","73861dcb":"markdown","3ea1a191":"markdown"},"source":{"15a7c6a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","04bc3630":"import json\nimport plotly.express as px\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom nltk.tokenize import wordpunct_tokenize\nfrom tqdm import tqdm","fac2f4ad":"train = pd.read_csv('\/kaggle\/input\/dmia-dl-nlp-2019\/train.csv')","8e1a1cba":"train.head()","09ad51e8":"# \u0447\u0438\u0442\u0430\u0435\u043c \nwith open('\/kaggle\/input\/dmia-dl-nlp-2019\/main_category_mapper.json') as f:\n    main_cat2id = json.load(f)\n    \n# \u0438\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u043c\nid2main_cat = {value: key for key, value in main_cat2id.items()}","ca248590":"id2main_cat","a148e05d":"# \u0442\u043e\u0436\u0435 \u0441\u0430\u043c\u043e\u0435 \u0441 \u0434\u043e\u043f \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044f\u043c\u0438\nwith open('\/kaggle\/input\/dmia-dl-nlp-2019\/sub_category_mapper.json') as f:\n    sub_cat2id = json.load(f)\n    \nid2sub_cat = {value: key for key, value in sub_cat2id.items()}","d724f928":"# \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u043c id \u0432 \u0441\u0442\u0440\u043e\u043a\u0443\ntrain.main_category = train.main_category.map(id2main_cat)\ntrain.sub_category = train.sub_category.map(id2sub_cat)","c1ab146a":"train.head()","3c848ca3":"val_counts_main = pd.DataFrame(train.main_category.value_counts())\nval_counts_main.reset_index(inplace=True)\nval_counts_main.columns = ['category', 'n_entries']\nfig = px.bar(val_counts_main, x='category', y='n_entries')\nfig.show()","2b69264b":"val_counts_sub = pd.DataFrame(train.sub_category.value_counts())\nval_counts_sub.reset_index(inplace=True)\nval_counts_sub.columns = ['category', 'n_entries']\nfig = px.bar(val_counts_sub, x='category', y='n_entries')\nfig.show()","a84404f4":"train['char_len'] = train.question.map(len)","02359e68":"train['token_len'] = train.question.map(lambda x: len(wordpunct_tokenize(x)))","9fa126cd":"train.head()","2971e853":"plt.figure(figsize=(16, 12))\nplt.title('Distplot question char len')\nsns.distplot(train.char_len)","e78a45ca":"plt.figure(figsize=(16, 12))\nplt.title('Distplot question token len')\nsns.distplot(train.token_len)","76d0beaf":"unsupervised = pd.read_csv('\/kaggle\/input\/dmia-dl-nlp-2019\/unsupervised.csv')","a93152dd":"unsupervised","f992c3f1":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score, confusion_matrix","4fdbec32":"# \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u043c \u043a\u043b\u0430\u0441\u0441 \u0432 \u0438\u043d\u0434\u0435\u043a\u0441\ntrain['target'] = train.main_category.map(main_cat2id)","3072078e":"train.head()","871207e7":"x_train, x_validation, y_train, y_validation = train_test_split(train.question, train.target, test_size=0.15)","08f2da89":"x_validation.shape","237f9df4":"# \u043e\u0441\u0442\u0430\u0432\u0438\u043c \u0442\u043e\u043b\u044c\u043a\u043e 75000 \u0441\u0430\u043c\u044b\u0445 \u0447\u0430\u0441\u0442\u044b\u0445 \u0441\u043b\u043e\u0432\n# \u043a\u0430\u0436\u0434\u043e\u0435 \u0441\u043b\u043e\u0432\u043e - \u0444\u0438\u0447\u0430\nvectorizer = TfidfVectorizer(max_features=75000)","8a53d5fc":"x_train_vectorized = vectorizer.fit_transform(x_train)\nx_validation_vectorized = vectorizer.transform(x_validation)","4d36a85d":"# \u0443 \u043d\u0430\u0441 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0430\u0441\u044c \u0440\u0430\u0437\u0440\u0435\u0436\u0435\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u0432\u0441\u0435 75 000 \u0441\u043b\u043e\u0432 \u043f\u043e\u0447\u0442\u0438 \u0442\u043e\u0447\u043d\u043e \u043d\u0435 \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0442\u0441\u044f \u0432 \u043e\u0434\u043d\u043e\u043c \u0442\u0435\u043a\u0441\u0442\u0435\nx_train_vectorized","d424a638":"sample = x_train_vectorized[0].toarray()[0]","1c6f8824":"# \u043d\u0430 \u043a\u0430\u0436\u0434\u044b\u0439 \u043f\u0440\u0438\u043c\u0435\u0440 \u0443 \u043d\u0430\u0441 \u0432\u0435\u043a\u0442\u043e\u0440 \u0438\u0437 75000\nsample.shape","0604aaf6":"# \u0438 \u0442\u043e\u043b\u044c\u043a\u043e \u043d\u0430 \u0441\u0442\u043e\u043b\u044c\u043a\u043e \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d \u043d\u0430\u0448 \u0432\u0435\u043a\u0442\u043e\u0440\n(sample > 0).sum() * 100 \/ sample.shape[0]","77b155bf":"log_reg = LogisticRegression(solver='lbfgs', multi_class='auto')","cf0d7a4e":"log_reg.fit(x_train_vectorized, y_train)","62ffb7bd":"predicted_train = log_reg.predict(x_train_vectorized)\npredicted_validation = log_reg.predict(x_validation_vectorized)","25dabfd6":"f1_train = f1_score(y_true=y_train, y_pred=predicted_train, average='micro')\nf1_test = f1_score(y_true=y_validation, y_pred=predicted_validation, average='micro')\n\nf'F1 train: {f1_train:.3f} | test: {f1_test:.3f}'","3a808419":"classes = [id2main_cat[n] for n in range(len(id2main_cat))]","0a5b9d7a":"def plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=True,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    fig, ax = plt.subplots(figsize=(18, 18))\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax","bf7a5c65":"plot_confusion_matrix(y_validation, predicted_validation, classes)","c214ee83":"test = pd.read_csv('\/kaggle\/input\/dmia-dl-nlp-2019\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/dmia-dl-nlp-2019\/sample_submission.csv')","1a7f0b54":"text_vectorized = vectorizer.transform(test.question)","8d83ba56":"test_prediction = log_reg.predict(text_vectorized)","6792f096":"sample_submission.main_category = test_prediction","fb666e72":"sample_submission","3dd0e186":"sample_submission.to_csv('submission.csv', index=False)","517da48d":"# \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u043b\u0438\u043d \u0432 \u0442\u043e\u043a\u0435\u043d\u0430\u0445 \u0438 \u0441\u0438\u043c\u0432\u043e\u043b\u0430\u0445","6db80a87":"# \u0413\u043b\u0430\u0432\u043d\u0430\u044f \u0438 \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044f \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b id\n\u0423 \u043d\u0430\u0441 \u0435\u0441\u0442\u044c \u0444\u0430\u0439\u043b\u044b main_category_mapper.json \u0438 sub_category_mapper.json  \n### main_category_mapper.json\n\u0421\u043b\u043e\u0432\u0430\u0440\u044c, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u043a\u043b\u044e\u0447 - \u044d\u0442\u043e \u0441\u0442\u0440\u043e\u043a\u043e\u0432\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0433\u043b\u0430\u0432\u043d\u043e\u0439 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0438, \u0430 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 - id \u0433\u043b\u0430\u0432\u043d\u043e\u0439 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0438\n### sub_category_mapper.json\n\u0421\u043b\u043e\u0432\u0430\u0440\u044c, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u043a\u043b\u044e\u0447 - \u044d\u0442\u043e \u0441\u0442\u0440\u043e\u043a\u043e\u0432\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0438, \u0430 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 - id \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0438","fab6866d":"![alt text](https:\/\/i.redd.it\/icok5mempnd21.jpg \"Logo Title Text 1\")","3de0945f":"![alt text](https:\/\/i.kym-cdn.com\/entries\/icons\/mobile\/000\/002\/456\/tearingmeapartlisa.jpg \"Logo Title Text 1\")","88d20446":"## \u0422\u0435\u043f\u0435\u0440\u044c \u043a\u043b\u0430\u0441\u0441\u044b \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b \u0441\u0442\u0440\u043e\u043a\u0430\u043c\u0438","e5d36813":"# \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432","a8a4408a":"# \u041d\u0435\u0440\u0430\u0437\u043c\u0435\u0447\u0435\u043d\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435","3a5199d2":"# \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 train","73861dcb":"# \u0412\u0435\u043a\u0442\u043e\u0440\u0438\u0437\u0443\u0435\u043c \u043d\u0430\u0448\u0438 \u0442\u0435\u043a\u0441\u0442\u044b\n\ntf-idf \u0447\u0430\u0441\u0442\u043e \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0449\u0438\u0439\u0441\u044f \u0432 NLP \u043c\u0435\u0442\u043e\u0434 \u0432\u0435\u043a\u0442\u043e\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439. \u041f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435 \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0447\u0438\u0442\u0430\u0442\u044c \u0437\u0434\u0435\u0441\u044c: https:\/\/ru.wikipedia.org\/wiki\/TF-IDF  \n\u041c\u044b \u0431\u0435\u0440\u0435\u043c \u0435\u0433\u043e \u043a\u0430\u043a \u0431\u0435\u0439\u0437\u043b\u0430\u0439\u043d, \u043f\u0440\u0438\u044f\u0442\u043d\u043e \u0431\u0443\u0434\u0435\u0442 \u0437\u043d\u0430\u0442\u044c \u043f\u0440\u043e \u043d\u0435\u0433\u043e, \u043d\u043e \u043d\u0435 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043a\u0440\u0438\u0442\u0438\u0447\u043d\u044b\u043c \u0434\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u0437\u0430\u0434\u0430\u043d\u0438\u044f. \u0412 \u043d\u0435\u043c \u043c\u043d\u043e\u0433\u043e \u043c\u0438\u043d\u0443\u0441\u043e\u0432, \u043d\u043e \u0431\u0435\u0437\u0443\u0441\u043b\u043e\u0432\u043d\u044b\u0439 \u043f\u043b\u044e\u0441 - \u043f\u0440\u043e\u0441\u0442\u043e\u0442\u0430.","3ea1a191":"# \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u043f\u0440\u043e\u0441\u0442\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c"}}