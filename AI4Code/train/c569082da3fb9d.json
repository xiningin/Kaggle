{"cell_type":{"a4dea5c1":"code","df760d3c":"code","84b23d96":"code","9dd624c1":"code","ee6de20f":"code","56d3d49e":"code","39163fa4":"code","86de6317":"code","0be993b6":"code","ae390306":"code","f46a0e9d":"code","03debe99":"code","0eb457da":"code","49783200":"code","480d7fd4":"code","16d40d08":"code","7085c4b4":"code","f870be8f":"code","f27e0cc2":"code","ea914dab":"code","7b64d899":"markdown","e49ad1d6":"markdown","11d94bed":"markdown","ec7a3192":"markdown","517f750c":"markdown","b966b2e6":"markdown","53beb6ea":"markdown","64cf7478":"markdown","385e2747":"markdown","d032e21e":"markdown"},"source":{"a4dea5c1":"import numpy as np \nimport pandas as pd \nimport os, gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nimport lightgbm as lgb\nimport xgboost as xgb\nimport optuna\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, GridSearchCV, train_test_split\nfrom sklearn.linear_model import LinearRegression\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","df760d3c":"PATH = '..\/input\/tabular-playground-series-jan-2021\/'\n\ntrain = pd.read_csv(PATH + 'train.csv')\ntest = pd.read_csv(PATH + 'test.csv')\nsample = pd.read_csv(PATH + 'sample_submission.csv')\n\nprint(train.shape, test.shape)","84b23d96":"train.head(10)","9dd624c1":"test.head(10)","ee6de20f":"train.info()","56d3d49e":"test.info()","39163fa4":"fig, ax = plt.subplots(1, 2, figsize=(16, 6))\nsns.distplot(train['target'], ax=ax[0])\nsns.boxplot(train['target'], ax=ax[1])","86de6317":"train.describe()","0be993b6":"FEATURES = train.drop(['id', 'target'], 1).columns\nFEATURES","ae390306":"fig, ax = plt.subplots(7, 2, figsize=(16, 40))\nax = ax.flatten()\n\nfor k, i in enumerate(FEATURES):\n    sns.distplot(train[i], ax=ax[k], hist=False, label='train')\n    sns.distplot(test[i], ax=ax[k], hist=False, label='test')","f46a0e9d":"x = train.corr()\nplt.figure(figsize=(12,12))\nsns.heatmap(x, annot=True)","03debe99":"fig, ax = plt.subplots(7, 2, figsize=(16, 40))\nax = ax.flatten()\n\nfor k, i in enumerate(FEATURES):\n    sns.boxplot(train[i], ax=ax[k])","0eb457da":"train['train'] = 1\ntest['train'] = 0\ntarget = train.target","49783200":"combined_df = pd.concat([train, test], 0)\ncombined_df = combined_df.sort_values(by='id', ascending=True)\ncombined_df","480d7fd4":"for i in FEATURES:\n    combined_df[f'{i}_lag_1'] = combined_df[i].shift(1)\n    combined_df[f'{i}_lag_5'] = combined_df[i].shift(5)\n    combined_df[f'{i}_lag_10'] = combined_df[i].shift(10)\n    \n    combined_df[f'{i}_lag_-1'] = combined_df[i].shift(-1)\n    combined_df[f'{i}_lag_-5'] = combined_df[i].shift(-5)\n    combined_df[f'{i}_lag_-10'] = combined_df[i].shift(-10)\n    \n    \n    combined_df[f'{i}_50_rl_max'] = combined_df[i].rolling(window=50).max()\n    combined_df[f'{i}_50_rl_max'] = combined_df[i].rolling(window=50).min()\n    combined_df[f'{i}_50_rl_max'] = combined_df[i].rolling(window=50).std()\n    combined_df[f'{i}_50_rl_max'] = combined_df[i].rolling(window=50).mean()\n    combined_df[f'{i}_50_rl_max'] = combined_df[i].rolling(window=50).median()\n    \n    combined_df[f'{i}_20_rl_max'] = combined_df[i].rolling(window=20).max()\n    combined_df[f'{i}_20_rl_max'] = combined_df[i].rolling(window=20).min()\n    combined_df[f'{i}_20_rl_max'] = combined_df[i].rolling(window=20).std()\n    combined_df[f'{i}_20_rl_max'] = combined_df[i].rolling(window=20).mean()\n    combined_df[f'{i}_20_rl_max'] = combined_df[i].rolling(window=20).median()\n    \n    \n    combined_df[f'{i}_10_rl_max'] = combined_df[i].rolling(window=10).max()\n    combined_df[f'{i}_10_rl_max'] = combined_df[i].rolling(window=10).min()\n    combined_df[f'{i}_10_rl_max'] = combined_df[i].rolling(window=10).std()\n    combined_df[f'{i}_10_rl_max'] = combined_df[i].rolling(window=10).mean()\n    combined_df[f'{i}_10_rl_max'] = combined_df[i].rolling(window=10).median()\ncombined_df","16d40d08":"cv = KFold(n_splits=5, shuffle=True)\ncv","7085c4b4":"X = combined_df[combined_df['train'] == 1].drop(['id', 'target', 'train'], 1)\ny = combined_df[combined_df['train'] == 1].target\nprint(X.shape, y.shape)","f870be8f":"NUM_BOOST_ROUNDS = 10000\nEARLY_STOPPING_ROUNDS = 1000\nVERBOSE_EVAL = 1\n\noof_df = train[['id', 'target']].copy()\nfold_ = 1\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)\n\ndef objective(trial):\n        train_set = lgb.Dataset(X_train, y_train)\n        val_set = lgb.Dataset(X_val, y_val)\n        \n        param = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"verbosity\": 1,\n        \"boosting_type\": \"gbdt\",\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.005),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 128, 512),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 31),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.7, 0.9),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.7, 0.9),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n        }\n        \n        \n        model = lgb.train(param,\n                          train_set)\n        val_preds = model.predict(X_val, num_iteration=model.best_iteration)\n        scc = math.sqrt(mean_squared_error(val_preds, y_val))\n        return -1*scc\n    \nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=10)\ntrial = study.best_trial\nprint(trial.params)\ntrial.params['metric']  = 'rmse'","f27e0cc2":"for train_idx, val_idx in cv.split(X, y):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n    \n    train_set = lgb.Dataset(X_train, y_train)\n    val_set = lgb.Dataset(X_val, y_val)\n    \n    \n    model = lgb.train(trial.params,\n                          train_set,\n                          num_boost_round=NUM_BOOST_ROUNDS,\n                          early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n                          verbose_eval=VERBOSE_EVAL,\n                          valid_sets=[train_set, val_set]\n                          )\n    \n    val_preds = model.predict(X_val, num_iteration=model.best_iteration)\n    test_preds = model.predict(combined_df[combined_df['train'] == 0].drop(['id', 'target', 'train'], 1), num_iteration=model.best_iteration)\n\n    oof_df.loc[oof_df.iloc[val_idx].index, 'oof'] = val_preds\n    sample[f'fold{fold_}'] = test_preds\n    \n    score = mean_squared_error(oof_df.loc[oof_df.iloc[val_idx].index]['target'], oof_df.loc[oof_df.iloc[val_idx].index]['oof'])\n    print(math.sqrt(score))\n    fold_ += 1","ea914dab":"print(math.sqrt(mean_squared_error(oof_df.target, oof_df.oof)))\nsample['target'] = sample.drop(['id', 'target'], 1).mean(axis=1)\nsample[['id', 'target']].to_csv('submission.csv', index = False)","7b64d899":"1. There is a correlation cluster from cont6 to cont13, but the highest value of correlation coefficient is 0.83, so no need to drop any features.\n\n2. Features are not correlated to the target.\n\nLet's check feature-wise outliers","e49ad1d6":"All the features are multimodal with varying number of peaks. The feature distributions from train and test set are almost same.  Let's check the correlations","11d94bed":"The target variable has a bimodel distribution and outliers are present. We'll be using LightGBM so no need for transformations.","ec7a3192":"## <a>Loading Packages and Data<\/a>","517f750c":"There are no missing values present train and test. Also, all the feature columns are float type.","b966b2e6":"Both train and test are medium sized datasets. Let's take a look at the train set.\n","53beb6ea":"## <a>Introduction<\/a>\n\nWelcome to this new competition series by Kaggle. This is somewhat in between basic playground competitions and competitive featured ones. \n\nIn this competition, we are given a regression task. We will be predicting a continuous target based on a number of feature columns given in the data. All of the feature columns, cont1 - cont14 are continuous.\n\nLet's get started.","64cf7478":"## <a>Model<\/a>","385e2747":"## <a>EDA<\/a>\n\nLet's first check the distribution of target variable.\n","d032e21e":"## <a>Feature Engineering<\/a>"}}