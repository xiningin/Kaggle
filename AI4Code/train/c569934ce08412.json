{"cell_type":{"fc950268":"code","90916709":"code","e9da8201":"code","b1bf1867":"code","f3d3885a":"code","d7c5396a":"code","2c6460a6":"code","7b3c48d3":"code","f6a47042":"code","1d5a8769":"code","21e1193e":"code","07c77919":"code","af2fd1b9":"code","b1839dd3":"code","51d09e7c":"code","e799596b":"code","48f8d16a":"code","f17e0ca6":"code","5933f792":"code","d5fbbf4b":"code","ed7f2e46":"code","154b3b9a":"code","0da9b013":"code","edac7aa0":"code","0e5e842c":"code","fecfc7e7":"code","b64977f2":"code","9f4173ce":"code","79f81f52":"code","c32ce969":"code","2d763102":"code","0f445ee1":"code","bc3ea67f":"code","2b1ea623":"code","0bec4165":"code","a1d879b6":"code","faa254ee":"code","8c156186":"code","12853ff1":"markdown","9097da0d":"markdown","b5eb492b":"markdown"},"source":{"fc950268":"from xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split,cross_val_score, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, f1_score\nimport pandas as pd\nimport time\nimport numpy as np","90916709":"df = pd.read_csv(\"\/kaggle\/input\/csgo-round-winner-classification\/csgo_round_snapshots.csv\")\n\nX = df.drop(columns=['round_winner'])\nX = pd.get_dummies(X)\ny = df['round_winner']\ny = y.map({'CT': 1, 'T': 0}).astype(int)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)","e9da8201":"import time\nstart = time.time()\nclassfier = LGBMClassifier()\nclassfier.fit(X=X_train, y=Y_train.values.ravel())\npredicted=classfier.predict(X_test)\nprint('Classification of the result is:')\nprint(f1_score(Y_test, predicted))\nend = time.time()\nprint('Execution time is:')\nprint(end - start)","b1bf1867":"lgb=LGBMClassifier()\nstart = time.time()\n#Define the parameters\nparameters = {'num_leaves':[1000,1250,1500], 'num_iterations':[50,100,150],'max_depth':[-1],\n             'learning_rate':[0.05,0.1,0.15],'reg_alpha':[0.01]}\nparameters['random_state']=[42]","f3d3885a":"#Define the scoring\ngscv=GridSearchCV(lgb,parameters,scoring='accuracy',verbose=0,n_jobs=-1)\ngscv.fit(X=X_train, y=Y_train.values.ravel())\nprint(gscv.best_params_)\npredicted=gscv.predict(X_test)\nprint('Classification of the result is:')\nprint(f1_score(Y_test, predicted))\nend = time.time()\nprint('Execution time is:')\nprint(end - start)\n","d7c5396a":"rscv=RandomizedSearchCV(lgb,parameters,scoring='accuracy',n_iter=10,verbose=0,n_jobs=-1)\nrscv.fit(X=X_train, y=Y_train.values.ravel())\nprint(rscv.best_params_)\npredicted=rscv.predict(X_test)\nprint('Classification of the result is:')\nprint(f1_score(Y_test, predicted))\nend = time.time()\nprint('Execution time is:')\nprint(end - start)","2c6460a6":"from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n\nlgb_reg_params = {\n    'num_leaves':            hp.choice('num_leaves', np.arange(500, 2000, 1, dtype=int)),\n    'min_child_samples':     hp.choice('num_iterations', np.arange(40, 3001, dtype=int)),\n    'max_depth':             hp.choice('max_depth',np.arange(1, 200, 1, dtype=int)),\n    'learning_rate':         hp.uniform('learning_rate',0.05,0.3),\n    'reg_alpha':             hp.uniform('reg_alpha',0.005,0.05),\n}\n\n\n\n\ndef f(params):\n    global X_train,y_train\n    params['random_state']=42\n    cumulative_f1_score = []\n    lgbm = LGBMClassifier(n_jobs=-1,early_stopping_rounds=None,**params)\n    skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n    \n    X,y = X_train.values , Y_train.values\n    \n    \n    for train_index, test_index in skf.split(X, y):\n        X_train_, X_test_ = X[train_index], X[test_index]\n        y_train_, y_test_ = y[train_index], y[test_index]\n        lgbm.fit(X_train_,y_train_)\n        cumulative_f1_score.append(f1_score(lgbm.predict(X_test_),y_test_))\n        \n\n    return 1 - np.array(cumulative_f1_score).mean()\n\ntrials = Trials()\nresult = fmin(\n    fn=f,                           # objective function\n    space=lgb_reg_params,   # parameter space\n    algo=tpe.suggest,               # surrogate algorithm\n    max_evals=6,                # no. of evaluations\n    trials=trials                   # trials object that keeps track of the sample results (optional)\n)\nprint(result)","7b3c48d3":"import time\nstart = time.time()\nhs = LGBMClassifier(**result)\nhs.fit(X=X_train, y=Y_train.values.ravel())\npredicted=hs.predict(X_test)\nprint('Classification of the result is:')\nprint(f1_score(Y_test, predicted))\nend = time.time()\nprint('Execution time is:')\nprint(end - start)","f6a47042":"result","1d5a8769":"params = {'reg_alpha': 0.01, 'num_leaves': 1000, 'min_child_samples': 15, 'max_depth': -1, 'learning_rate': 0.15}","21e1193e":"start = time.time()\nclassfier = LGBMClassifier(**params)\nclassfier.fit(X=X_train, y=Y_train.values.ravel())\npredicted=classfier.predict(X_test)\nprint('Classification of the result is:')\nprint(accuracy_score(Y_test, predicted))\nend = time.time()\nprint('Execution time is:')\nprint(end - start)","07c77919":"!pip install shap==0.35.0","af2fd1b9":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm\nimport shap\nshap.initjs()","b1839dd3":"explainer = shap.TreeExplainer(classfier)","51d09e7c":"shap_values = explainer.shap_values(X_test)","e799596b":"shap_values[1].shape","48f8d16a":"X_test.shape","f17e0ca6":"len(shap_values[1][0])","5933f792":"np.mean(Y_test)","d5fbbf4b":"explainer.expected_value","ed7f2e46":"proba = classfier.predict_proba(X_test)","154b3b9a":"proba","0da9b013":"i = 0\nprint(f\"Predicted class: {predicted[i]}\")\nprint(f\"Predicted probabilities: {proba[i]}\")\nshap.force_plot(explainer.expected_value[1], shap_values[1][i,:], X_test.iloc[i,:])","edac7aa0":"i = 1\nprint(f\"Predicted class: {predicted[i]}\")\nprint(f\"Predicted probabilities: {proba[i]}\")\nshap.force_plot(explainer.expected_value[1], shap_values[1][i,:], X_test.iloc[i,:])","0e5e842c":"i = 2\nprint(f\"Predicted class: {predicted[i]}\")\nprint(f\"Predicted probabilities: {proba[i]}\")\nshap.force_plot(explainer.expected_value[1], shap_values[1][i,:], X_test.iloc[i,:])\n","fecfc7e7":"i = 2\nprint(f\"Predicted class: {predicted[i]}\")\nprint(f\"Predicted probabilities: {proba[i]}\")\nshap.decision_plot(explainer.expected_value[1], shap_values[1][i,:], X_test.iloc[i,:])","b64977f2":"i = 27091\nprint(f\"Predicted class: {predicted[i]}\")\nprint(f\"Predicted probabilities: {proba[i]}\")\nshap.force_plot(explainer.expected_value[1], shap_values[1][i,:], X_test.iloc[i,:])\n","9f4173ce":"i = 27091\nprint(f\"Predicted class: {predicted[i]}\")\nprint(f\"Predicted probabilities: {proba[i]}\")\nshap.decision_plot(explainer.expected_value[1], shap_values[1][i,:], X_test.iloc[i,:])","79f81f52":"i = 27091\nprint(f\"Predicted class: {predicted[i]}\")\nprint(f\"Predicted probabilities: {proba[i]}\")\nshap.waterfall_plot(explainer.expected_value[1], shap_values[1][i,:], X_test.iloc[i,:])","c32ce969":"len(shap_values)","2d763102":"X_test.iloc[i,:]","0f445ee1":"proba","bc3ea67f":"abs(proba[:,0]-.5)","2b1ea623":"np.argsort(abs(proba[:,0]-.5))\n","0bec4165":"feature_imp = pd.DataFrame(sorted(zip(classfier.feature_importances_,X.columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 25))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.show()","a1d879b6":"shap.summary_plot(shap_values[1], X_test)","faa254ee":"shap.dependence_plot('ct_armor', shap_values[1], X_test)","8c156186":"X_test.columns","12853ff1":"## Parametry modelu Lightgbm Classifier\n- num_leaves : Parametr ten kontroluje z\u0142o\u017cono\u015b\u0107 modelu. Okre\u015bla on maksymaln\u0105 z\u0142o\u017cono\u015b\u0107 pojedynczego modelu w ramach boostingu. Nieadekwatnie du\u017ce warto\u015bci tego parametru mog\u0105 powodowa\u0107 tak zwany overfitting. natomiast jego zbyt ma\u0142e warto\u015bci niewystarczaj\u0105co skomplikowany model\n![](https:\/\/ichi.pro\/assets\/images\/max\/724\/1*AZsSoXb8lc5N6mnhqX5JCg.png)\n- num_iterations : Parametr ten okre\u015bla, przez ile iteracji model boostingowy b\u0119dzie trenowyany. Ma to wyp\u0142yw an dok\u0142adno\u015b\u0107 modelu i czas trenowania, ale zwiesza szanse na overfitting.\n- max_depth : Ten parametr okre\u015bla maksymaln\u0105 g\u0142\u0119boko\u015b\u0107 drzewa. Konsekwencji w przypadku jego niewystarczaj\u0105cej warto\u015bci s\u0105 takie same jak w przypadku num_leaves. Ponadto parametr ten wp\u0142ywa na czas treningu, jak i optymaln\u0105 warto\u015b\u0107 num_leaves, jako i\u017c s\u0105 one od siebie zale\u017cne.\n- learning_rate : Parametr ten okre\u015bla, jak bardzo b\u0142\u0105d zostanie poprawiony w przypadku pojedynczej iteracji. Przy niskim learning rate i du\u017cej liczbie iteracji wsp\u00f3\u0142czynnik ten zwi\u0119kszy dok\u0142adno\u015b\u0107 modelu przy skomplikowanych danych.\n- reg_alpha : Parametr ten odpowiada za regularyzacje cech pod wzgl\u0119dem ich przydatno\u015bci. Im cech proporcjonalnie mniej przydatna, tym mniej b\u0119dzie brana pod uwag\u0119 przy budowie modelu.\n","9097da0d":"## [Hyperopt](https:\/\/hyperopt.github.io\/hyperopt\/#documentation)\nOstatni\u0105 om\u00f3wion\u0105 metod\u0105 jest hyperopt - biblioteka polegaj\u0105ca na bayesowskim podej\u015bciu do optymalizacji. Podej\u015bcie to polega na uwzgl\u0119dnianiu poprzednich warto\u015b\u0107 funkcji straty dla wybranych parametr\u00f3w. Dzieje si\u0119 to dzi\u0119ki tworzeniu modelu probabilistycznego bazuj\u0105cego na wybranych obserwacjach i pr\u00f3bie przewidzenia nast\u0119pnego zestawu parametr\u00f3w bardziej odpowiedniego dla wybranego problemu. Tworzona funkcja nazywana jest funkcja zast\u0119pcza\/modelem zast\u0119pczym, gdy\u017c stara si\u0119 ona przewidzie\u0107 zachowanie modelu, bazuj\u0105c na parametrach, pomijaj\u0105c jego faktyczne dzia\u0142anie.\n<br>\n\n\n![](https:\/\/miro.medium.com\/max\/875\/1*01OV9s_DYaK8k-cwa0bk3A.png)\n\n<br>\n\n![](https:\/\/miro.medium.com\/max\/875\/1*31TpvO5XO_VGaZG0m3FgVg.png)","b5eb492b":"## [GridSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html)\nGridsearchCV nale\u017cy do tak zwanych wyczerpuj\u0105cych wyszukiwa\u0144. Oznacza to, i\u017c przeszukiwane s\u0105 wszystkie kombinacje parametr\u00f3 by wybra\u0107 ten najlepszy. Ma to wady w postaci d\u0142ugiego czasu wyszukiwa\u0144 jak i wy\u017cszej szansy na tak zwany overfitting poprzez parametry. Do zalet grid searchu nale\u017cy jego mo\u017cliwo\u015b\u0107 ur\u00f3wnoregleania, gdy\u017c nast\u0119pna iteracja nie zale\u017cy od poprzedniej.\n## [RandomizedSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html)\nRandomizedSearchCV Jest to metoda optymalizacji losuj\u0105ca wybrane parametry z okre\u015blonej dystrybucji. W przypadku braku okre\u015blenia dystrybucji wybierana jest tak zwana dystrybucja r\u00f3wnomierna. Spos\u00f3b ten mo\u017ce dawa\u0107 lepsze rezultaty w przypadku gdy istniej\u0105 niepowi\u0105zane lub te\u017c ma\u0142o wa\u017cne cechy, gdy\u017c ka\u017cde nowe losowanie b\u0119dzie najprawdopodobniej zawiera\u0107 inn\u0105 warto\u015b\u0107 wa\u017cnych zmiennych.\n\n![](https:\/\/miro.medium.com\/max\/1838\/1*o2rMCJKUcpqRBFfHZ3Jkfg.png)"}}