{"cell_type":{"d0bb5c1f":"code","e827d753":"code","65ec3758":"code","d3bd9ce3":"code","61871455":"code","de80e5c3":"code","e1d46fc0":"code","8e852de6":"code","c8dc91c1":"code","bb64c07c":"code","0adec4d1":"code","100ddedd":"code","eeeb5bf8":"code","1e4ae401":"code","629ccbac":"code","de115eda":"code","353085be":"code","1981ca09":"code","cdc45ec7":"code","806471a9":"code","2662f2fa":"code","aece6a79":"code","d4f3256d":"code","41dc7e67":"code","91130eb6":"code","c0cfc663":"code","b048e995":"code","b548c987":"code","3db27ab5":"code","bfc15dac":"code","b60284c0":"markdown","5c091280":"markdown","0ed876d4":"markdown","99152f3c":"markdown","6b41f1a5":"markdown","e0b3a5d0":"markdown","98a33bb4":"markdown","f429673b":"markdown","90475666":"markdown","bdedb4d2":"markdown","c35d4d4e":"markdown","505b32e8":"markdown","effd9751":"markdown","7e635965":"markdown","8fb615a3":"markdown","4753b795":"markdown","48fb9150":"markdown","a36ed5e9":"markdown","12304676":"markdown","61d3b966":"markdown"},"source":{"d0bb5c1f":"import numpy as np\nfrom numpy import array\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport string\nimport os\nimport glob\nfrom PIL import Image\nfrom time import time\n\nfrom keras import Input, layers\nfrom keras import optimizers\nfrom keras.optimizers import Adam\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing import image\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import LSTM, Embedding, Dense, Activation, Flatten, Reshape, Dropout\nfrom keras.layers.wrappers import Bidirectional\nfrom keras.layers.merge import add\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.inception_v3 import preprocess_input\nfrom keras.models import Model\nfrom keras.utils import to_categorical","e827d753":"token_path = \"..\/input\/8kflickrdataset\/Flickr_Data\/Flickr_TextData\/Flickr8k.lemma.token.txt\"\ntrain_images_path = '..\/input\/8kflickrdataset\/Flickr_Data\/Flickr_TextData\/Flickr_8k.trainImages.txt'\ntest_images_path = '..\/input\/8kflickrdataset\/Flickr_Data\/Flickr_TextData\/Flickr_8k.testImages.txt'\nimages_path = '..\/input\/8kflickrdataset\/Flickr_Data\/Images\/'\nglove_path = '..\/input\/glove6b200d\/'\n\ndoc = open(token_path,'r').read()\nprint(doc[:410])","65ec3758":"descriptions = dict()\nfor line in doc.split('\\n'):\n        tokens = line.split()\n        if len(line) > 2:\n          image_id = tokens[0].split('.')[0]\n          image_desc = ' '.join(tokens[1:])\n          if image_id not in descriptions:\n              descriptions[image_id] = list()\n          descriptions[image_id].append(image_desc)","d3bd9ce3":"table = str.maketrans('', '', string.punctuation)\nfor key, desc_list in descriptions.items():\n    for i in range(len(desc_list)):\n        desc = desc_list[i]\n        desc = desc.split()\n        desc = [word.lower() for word in desc]\n        desc = [w.translate(table) for w in desc]\n        desc_list[i] =  ' '.join(desc)","61871455":"pic = '102351840_323e3de834.jpg'\nx=plt.imread(images_path+pic)\nplt.imshow(x)\nplt.show()\ndescriptions['102351840_323e3de834']","de80e5c3":"pic = '1001773457_577c3a7d70.jpg'\nx=plt.imread(images_path+pic)\nplt.imshow(x)\nplt.show()\ndescriptions['1001773457_577c3a7d70']","e1d46fc0":"vocabulary = set()\nfor key in descriptions.keys():\n        [vocabulary.update(d.split()) for d in descriptions[key]]\nprint('Original Vocabulary Size: %d' % len(vocabulary))","8e852de6":"lines = list()\nfor key, desc_list in descriptions.items():\n    for desc in desc_list:\n        lines.append(key + ' ' + desc)\nnew_descriptions = '\\n'.join(lines)\n","c8dc91c1":"doc = open(train_images_path,'r').read()\ndataset = list()\nfor line in doc.split('\\n'):\n    if len(line) > 1:\n      identifier = line.split('.')[0]\n      dataset.append(identifier)\n\ntrain = set(dataset)","bb64c07c":"img = glob.glob(images_path + '*.jpg')\ntrain_images = set(open(train_images_path, 'r').read().strip().split('\\n'))\ntrain_img = []\nfor i in img: \n    if i[len(images_path):] in train_images:\n        train_img.append(i)\n\ntest_images = set(open(test_images_path, 'r').read().strip().split('\\n'))\ntest_img = []\nfor i in img: \n    if i[len(images_path):] in test_images: \n        test_img.append(i)","0adec4d1":"print(\"Number of train images: \", len(train_img))\nprint(\"Number of test images: \", len(test_img))","100ddedd":"train_descriptions = dict()\nfor line in new_descriptions.split('\\n'):\n    tokens = line.split()\n    image_id, image_desc = tokens[0], tokens[1:]\n    if image_id in train:\n        if image_id not in train_descriptions:\n            train_descriptions[image_id] = list()\n        desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n        train_descriptions[image_id].append(desc)","eeeb5bf8":"all_train_captions = []\nfor key, val in train_descriptions.items():\n    for cap in val:\n        all_train_captions.append(cap)","1e4ae401":"word_count_threshold = 10\nword_counts = {}\nnsents = 0\nfor sent in all_train_captions:\n    nsents += 1\n    for w in sent.split(' '):\n        word_counts[w] = word_counts.get(w, 0) + 1\nvocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n\nprint('Vocabulary = %d' % (len(vocab)))","629ccbac":"ixtoword = {}\nwordtoix = {}\nix = 1\nfor w in vocab:\n    wordtoix[w] = ix\n    ixtoword[ix] = w\n    ix += 1\n\nvocab_size = len(ixtoword) + 1","de115eda":"all_desc = list()\nfor key in train_descriptions.keys():\n    [all_desc.append(d) for d in train_descriptions[key]]\nlines = all_desc\nmax_length = max(len(d.split()) for d in lines)\n\nprint('Description Length: %d' % max_length)","353085be":"embeddings_index = {} \nf = open(os.path.join(glove_path, 'glove.6B.200d.txt'), encoding=\"utf-8\")\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs","1981ca09":"embedding_dim = 200\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, i in wordtoix.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","cdc45ec7":"model = InceptionV3(weights='imagenet')","806471a9":"model_new = Model(model.input, model.layers[-2].output)","2662f2fa":"def preprocess(image_path):\n    img = image.load_img(image_path, target_size=(299, 299))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    return x","aece6a79":"def encode(image):\n    image = preprocess(image) \n    fea_vec = model_new.predict(image) \n    fea_vec = np.reshape(fea_vec, fea_vec.shape[1])\n    return fea_vec\n\nencoding_train = {}\nfor img in train_img:\n    encoding_train[img[len(images_path):]] = encode(img)\ntrain_features = encoding_train\n\nencoding_test = {}\nfor img in test_img:\n    encoding_test[img[len(images_path):]] = encode(img)","d4f3256d":"inputs1 = Input(shape=(2048,))\nfe1 = Dropout(0.5)(inputs1)\nfe2 = Dense(256, activation='relu')(fe1)\n\ninputs2 = Input(shape=(max_length,))\nse1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\nse2 = Dropout(0.5)(se1)\nse3 = LSTM(256)(se2)\n\ndecoder1 = add([fe2, se3])\ndecoder2 = Dense(256, activation='relu')(decoder1)\noutputs = Dense(vocab_size, activation='softmax')(decoder2)\n\nmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\nmodel.summary()","41dc7e67":"model.layers[2].set_weights([embedding_matrix])\nmodel.layers[2].trainable = False","91130eb6":"model.compile(loss='categorical_crossentropy', optimizer='adam')","c0cfc663":"def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n    X1, X2, y = list(), list(), list()\n    n=0\n    # loop for ever over images\n    while 1:\n        for key, desc_list in descriptions.items():\n            n+=1\n            # retrieve the photo feature\n            photo = photos[key+'.jpg']\n            for desc in desc_list:\n                # encode the sequence\n                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n                # split one sequence into multiple X, y pairs\n                for i in range(1, len(seq)):\n                    # split into input and output pair\n                    in_seq, out_seq = seq[:i], seq[i]\n                    # pad input sequence\n                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n                    # encode output sequence\n                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n                    # store\n                    X1.append(photo)\n                    X2.append(in_seq)\n                    y.append(out_seq)\n\n            if n==num_photos_per_batch:\n                yield ([array(X1), array(X2)], array(y))\n                X1, X2, y = list(), list(), list()\n                n=0","b048e995":"epochs = 30\nbatch_size = 64\nsteps = len(train_descriptions)\/\/batch_size\n\ngenerator = data_generator(train_descriptions, train_features, wordtoix, max_length, batch_size)\nmodel.fit(generator, epochs=epochs, steps_per_epoch=steps, verbose=1)","b548c987":"def greedySearch(photo):\n    in_text = 'startseq'\n    for i in range(max_length):\n        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        yhat = model.predict([photo,sequence], verbose=0)\n        yhat = np.argmax(yhat)\n        word = ixtoword[yhat]\n        in_text += ' ' + word\n        if word == 'endseq':\n            break\n\n    final = in_text.split()\n    final = final[1:-1]\n    final = ' '.join(final)\n    return final","3db27ab5":"def beam_search_predictions(image, beam_index = 3):\n    start = [wordtoix[\"startseq\"]]\n    start_word = [[start, 0.0]]\n    while len(start_word[0][0]) < max_length:\n        temp = []\n        for s in start_word:\n            par_caps = sequence.pad_sequences([s[0]], maxlen=max_length, padding='post')\n            preds = model.predict([image,par_caps], verbose=0)\n            word_preds = np.argsort(preds[0])[-beam_index:]\n            # Getting the top <beam_index>(n) predictions and creating a \n            # new list so as to put them via the model again\n            for w in word_preds:\n                next_cap, prob = s[0][:], s[1]\n                next_cap.append(w)\n                prob += preds[0][w]\n                temp.append([next_cap, prob])\n                    \n        start_word = temp\n        # Sorting according to the probabilities\n        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n        # Getting the top words\n        start_word = start_word[-beam_index:]\n    \n    start_word = start_word[-1][0]\n    intermediate_caption = [ixtoword[i] for i in start_word]\n    final_caption = []\n    \n    for i in intermediate_caption:\n        if i != 'endseq':\n            final_caption.append(i)\n        else:\n            break\n\n    final_caption = ' '.join(final_caption[1:])\n    return final_caption","bfc15dac":"pic = '2398605966_1d0c9e6a20.jpg'\nimage = encoding_test[pic].reshape((1,2048))\nx=plt.imread(images_path+pic)\nplt.imshow(x)\nplt.show()\n\nprint(\"Greedy Search:\",greedySearch(image))\nprint(\"Beam Search, K = 3:\",beam_search_predictions(image, beam_index = 3))\nprint(\"Beam Search, K = 5:\",beam_search_predictions(image, beam_index = 5))\nprint(\"Beam Search, K = 7:\",beam_search_predictions(image, beam_index = 7))\nprint(\"Beam Search, K = 10:\",beam_search_predictions(image, beam_index = 10))","b60284c0":"# Greedy and Beam Search","5c091280":"Now we save all the training and testing images in train_img and test_img lists respectively:","0ed876d4":"source \n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/11\/create-your-own-image-caption-generator-using-keras\/","99152f3c":"Now let\u2019s save the image id\u2019s and their new cleaned captions in the same format as the token.txt file:","6b41f1a5":"# Evaluation","e0b3a5d0":"We also need to find out what the max length of a caption can be since we cannot have captions of arbitrary length.","98a33bb4":"# Glove Embeddings","f429673b":"# Data loading and Preprocessing","90475666":"Next, let\u2019s train our model for 30 epochs. ","bdedb4d2":"Create a list of all the training captions:","c35d4d4e":"# Model Building and Training","505b32e8":"create a vocabulary of all the unique words present across all the 8000*5 (i.e. 40000) image captions in the data set. We have 8828 unique words across all the 40000 image captions.","effd9751":"Before training the model we need to keep in mind that we do not want to retrain the weights in our embedding layer (pre-trained Glove vectors).","7e635965":"We load all the training image id\u2019s in a variable train from the \u2018Flickr_8k.trainImages.txt\u2019 file:","8fb615a3":"Next, we make the matrix consisting of our vocabulary and the 200-d vector.","4753b795":"# Import the required libraries ","48fb9150":"Next, compile the model using Categorical_Crossentropy as the Loss function and Adam as the optimizer.","a36ed5e9":"Now we create two dictionaries to map words to an index and vice versa. Also, we append 1 to our vocabulary since we append 0\u2019s to make all captions of equal length.","12304676":"# Model Training","61d3b966":"Now, we load the descriptions of the training images into a dictionary. However, we will add two tokens in every caption, which are \u2018startseq\u2019 and \u2018endseq\u2019:"}}