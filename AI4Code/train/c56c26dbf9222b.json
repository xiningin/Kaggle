{"cell_type":{"23878900":"code","e50cb4ec":"code","971365c1":"code","05e3d321":"code","dffb61c8":"code","10da2fd6":"code","39488f5b":"code","ae9e16e0":"code","4379e77f":"code","e5e1400a":"code","5d81266d":"code","d823e4ac":"code","e64a609f":"code","dd1eaab0":"code","0d64f1a6":"code","a52b5985":"code","daf48d32":"code","ab3968c9":"code","9b6c8845":"code","dda273a2":"code","e9f0e45f":"markdown","a7ce9a54":"markdown","77fef499":"markdown","5be54713":"markdown","6cc86115":"markdown","ca6040ef":"markdown","ca76eddf":"markdown","38edf4e2":"markdown","1645e90f":"markdown"},"source":{"23878900":"!pip install ..\/input\/sacremoses\/sacremoses-master\/\n!pip install ..\/input\/transformers\/transformers-master\/","e50cb4ec":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nDATA_DIR = '..\/input\/google-quest-challenge'","971365c1":"!ls ..\/input","05e3d321":"sub = pd.read_csv(f'{DATA_DIR}\/sample_submission.csv')\nsub.head()","dffb61c8":"target_columns = sub.columns.values[1:].tolist()\ntarget_columns","10da2fd6":"train = pd.read_csv(f'{DATA_DIR}\/train.csv')\ntrain.head()","39488f5b":"test = pd.read_csv(f'{DATA_DIR}\/test.csv')\ntest.head()","ae9e16e0":"import torch\n#import torch.utils.data as data\nfrom torchvision import datasets, models, transforms\nfrom transformers import *\nfrom sklearn.utils import shuffle\nimport random\nfrom math import floor, ceil\nfrom sklearn.model_selection import GroupKFold\n\nMAX_LEN = 512\n#MAX_Q_LEN = 250\n#MAX_A_LEN = 259\nSEP_TOKEN_ID = 102\n\nclass QuestDataset(torch.utils.data.Dataset):\n    def __init__(self, df, train_mode=True, labeled=True):\n        self.df = df\n        self.train_mode = train_mode\n        self.labeled = labeled\n        #self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.tokenizer = BertTokenizer.from_pretrained('..\/input\/bert-base-uncased\/')\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        token_ids, seg_ids = self.get_token_ids(row)\n        if self.labeled:\n            labels = self.get_label(row)\n            return token_ids, seg_ids, labels\n        else:\n            return token_ids, seg_ids\n\n    def __len__(self):\n        return len(self.df)\n\n    def select_tokens(self, tokens, max_num):\n        if len(tokens) <= max_num:\n            return tokens\n        if self.train_mode:\n            num_remove = len(tokens) - max_num\n            remove_start = random.randint(0, len(tokens)-num_remove-1)\n            return tokens[:remove_start] + tokens[remove_start + num_remove:]\n        else:\n            return tokens[:max_num\/\/2] + tokens[-(max_num - max_num\/\/2):]\n\n    def trim_input(self, title, question, answer, max_sequence_length=MAX_LEN, \n                t_max_len=30, q_max_len=239, a_max_len=239):\n        t = self.tokenizer.tokenize(title)\n        q = self.tokenizer.tokenize(question)\n        a = self.tokenizer.tokenize(answer)\n\n        t_len = len(t)\n        q_len = len(q)\n        a_len = len(a)\n\n        if (t_len+q_len+a_len+4) > max_sequence_length:\n\n            if t_max_len > t_len:\n                t_new_len = t_len\n                a_max_len = a_max_len + floor((t_max_len - t_len)\/2)\n                q_max_len = q_max_len + ceil((t_max_len - t_len)\/2)\n            else:\n                t_new_len = t_max_len\n\n            if a_max_len > a_len:\n                a_new_len = a_len \n                q_new_len = q_max_len + (a_max_len - a_len)\n            elif q_max_len > q_len:\n                a_new_len = a_max_len + (q_max_len - q_len)\n                q_new_len = q_len\n            else:\n                a_new_len = a_max_len\n                q_new_len = q_max_len\n\n\n            if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n                raise ValueError(\"New sequence length should be %d, but is %d\" \n                                 % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n\n            t = t[:t_new_len]\n            q = q[:q_new_len]\n            a = a[:a_new_len]\n\n        return t, q, a\n        \n    def get_token_ids(self, row):\n        t_tokens, q_tokens, a_tokens = self.trim_input(row.question_title, row.question_body, row.answer)\n\n        tokens = ['[CLS]'] + t_tokens + ['[SEP]'] + q_tokens + ['[SEP]'] + a_tokens + ['[SEP]']\n        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n        if len(token_ids) < MAX_LEN:\n            token_ids += [0] * (MAX_LEN - len(token_ids))\n        ids = torch.tensor(token_ids)\n        seg_ids = self.get_seg_ids(ids)\n        return ids, seg_ids\n    \n    def get_seg_ids(self, ids):\n        seg_ids = torch.zeros_like(ids)\n        seg_idx = 0\n        first_sep = True\n        for i, e in enumerate(ids):\n            seg_ids[i] = seg_idx\n            if e == SEP_TOKEN_ID:\n                if first_sep:\n                    first_sep = False\n                else:\n                    seg_idx = 1\n        pad_idx = torch.nonzero(ids == 0)\n        seg_ids[pad_idx] = 0\n\n        return seg_ids\n\n    def get_label(self, row):\n        #print(row[target_columns].values)\n        return torch.tensor(row[target_columns].values.astype(np.float32))\n\n    def collate_fn(self, batch):\n        token_ids = torch.stack([x[0] for x in batch])\n        seg_ids = torch.stack([x[1] for x in batch])\n    \n        if self.labeled:\n            labels = torch.stack([x[2] for x in batch])\n            return token_ids, seg_ids, labels\n        else:\n            return token_ids, seg_ids\n\ndef get_test_loader(batch_size=4):\n    df = pd.read_csv(f'{DATA_DIR}\/test.csv')\n    ds_test = QuestDataset(df, train_mode=False, labeled=False)\n    loader = torch.utils.data.DataLoader(ds_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=ds_test.collate_fn, drop_last=False)\n    loader.num = len(df)\n    \n    return loader\n        \ndef get_train_val_loaders(batch_size=4, val_batch_size=4, ifold=0):\n    df = pd.read_csv(f'{DATA_DIR}\/train.csv')\n    df = shuffle(df, random_state=1234)\n    #split_index = int(len(df) * (1-val_percent))\n    gkf = GroupKFold(n_splits=5).split(X=df.question_body, groups=df.question_body)\n    for fold, (train_idx, valid_idx) in enumerate(gkf):\n        if fold == ifold:\n            df_train = df.iloc[train_idx]\n            df_val = df.iloc[valid_idx]\n            break\n\n    #print(df_val.head())\n    #df_train = df[:split_index]\n    #df_val = df[split_index:]\n\n    print(df_train.shape)\n    print(df_val.shape)\n\n    ds_train = QuestDataset(df_train)\n    train_loader = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=ds_train.collate_fn, drop_last=True)\n    train_loader.num = len(df_train)\n\n    ds_val = QuestDataset(df_val, train_mode=False)\n    val_loader = torch.utils.data.DataLoader(ds_val, batch_size=val_batch_size, shuffle=False, num_workers=0, collate_fn=ds_val.collate_fn, drop_last=False)\n    val_loader.num = len(df_val)\n    val_loader.df = df_val\n\n    return train_loader, val_loader\n\ndef test_train_loader():\n    loader, _ = get_train_val_loaders(4, 4, 1)\n    for ids, seg_ids, labels in loader:\n        print(ids)\n        print(seg_ids.numpy())\n        print(labels)\n        break\ndef test_test_loader():\n    loader = get_test_loader(4)\n    for ids, seg_ids in loader:\n        print(ids)\n        print(seg_ids)\n        break","4379e77f":"test_train_loader()","e5e1400a":"from transformers import *\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass QuestModel(nn.Module):\n    def __init__(self, n_classes=30):\n        super(QuestModel, self).__init__()\n        self.model_name = 'QuestModel'\n        self.bert_model = BertModel.from_pretrained('..\/input\/bert-base-uncased\/')    \n        self.fc = nn.Linear(768, n_classes)\n\n    def forward(self, ids, seg_ids):\n        attention_mask = (ids > 0)\n        layers, pool_out = self.bert_model(input_ids=ids, token_type_ids=seg_ids, attention_mask=attention_mask)\n        #print(layers[-1][0].size())\n        #print(pool_out.size())\n\n        #out = F.dropout(layers[-1][:, 0, :], p=0.2, training=self.training)\n        out =  F.dropout(pool_out, p=0.2, training=self.training)\n        logit = self.fc(out)\n        return logit\n    \ndef test_model():\n    x = torch.tensor([[1,2,3,4,5, 0, 0], [1,2,3,4,5, 0, 0]])\n    seg_ids = torch.tensor([[0,0,0,0,0, 0, 0], [0,0,0,0,0, 0, 0]])\n    model = QuestModel()\n\n    y = model(x, seg_ids)\n    print(y)","5d81266d":"!ls ..\/input","d823e4ac":"def create_model(model_file):\n    model = QuestModel()\n    model.load_state_dict(torch.load(model_file))\n    model = model.cuda()\n    #model = DataParallel(model)\n    return model\n\ndef create_models():\n    models = []\n    for i in range(5):\n        model = create_model(f'..\/input\/quest-models\/best_{i}.pth')\n        model.eval()\n        models.append(model)\n    return models","e64a609f":"from tqdm import tqdm\nimport torch\ndef predict(models, test_loader):\n    all_scores = []\n    with torch.no_grad():\n        for ids, seg_ids in tqdm(test_loader, total=test_loader.num \/\/ test_loader.batch_size):\n            ids, seg_ids = ids.cuda(), seg_ids.cuda()\n            scores = []\n            for model in models:\n                outputs = torch.sigmoid(model(ids, seg_ids)).cpu()\n                scores.append(outputs)\n            all_scores.append(torch.mean(torch.stack(scores), 0))\n\n    all_scores = torch.cat(all_scores, 0).numpy()\n    \n    return all_scores","dd1eaab0":"test_loader = get_test_loader(batch_size=32)","0d64f1a6":"models = create_models()","a52b5985":"preds = predict(models, test_loader)","daf48d32":"preds[:1]","ab3968c9":"sub[target_columns] = preds","9b6c8845":"sub.to_csv('submission.csv', index=False)","dda273a2":"sub.head()","e9f0e45f":"### Generate Submission","a7ce9a54":"## Build Model","77fef499":"### Required Imports\n\nI've added imports that will be used in training too","5be54713":"As we are not allowed to use internet I've created required datasets and commands to setup Hugging Face Transformers setup in offline mode. You can find the required github codebases in the datasets.\n\n* sacremoses dependency - https:\/\/www.kaggle.com\/axel81\/sacremoses\n* transformers - https:\/\/www.kaggle.com\/axel81\/transformers","6cc86115":"**Pytorch BERT baseline**","ca6040ef":"In this version, I convert https:\/\/www.kaggle.com\/akensert\/bert-base-tf2-0-minimalistic into pytorch version","ca76eddf":"### Install HuggingFace transformers & sacremoses dependency","38edf4e2":"**Please upvote the kernel if you find it helpful**","1645e90f":"### Define dataset"}}