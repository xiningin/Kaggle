{"cell_type":{"49038dd7":"code","b34d4847":"code","7196e07c":"code","a3753f96":"code","f92f7460":"code","144961ab":"code","d57953f7":"code","2c3872de":"code","005b7b69":"code","cb17cf9b":"code","ae7bafc6":"code","a593d1bd":"markdown","1b9e3062":"markdown","394dfa33":"markdown","54e735c1":"markdown","1ed9a3f9":"markdown","aba6d21c":"markdown","18b997ad":"markdown","be8680af":"markdown","567c4ae5":"markdown","a445677c":"markdown","ec59f688":"markdown","7f93598b":"markdown","02b4b7c9":"markdown"},"source":{"49038dd7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# also load the following for this project\nfrom collections import Counter\nimport random  # import random library\nimport PIL  # import Pillow library\nfrom PIL import Image  # import Image functionality\nimport torch  # import pytorch\nfrom torch import nn\nfrom sklearn import metrics  # import evaluation metrics\n\n\n# import seaborn and matplotlib.pyplot for data visualisation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimage_dir = []\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/sheep-breed-classification\/SheepFaceImages\/'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        image_dir.append(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b34d4847":"images = []  # variable to store the image data as 3D numpy arrays\nlabels = []  # variable to store the label of each image\nunique_labels = list(set([str(directory[57:].split('\/')[0]) for directory in image_dir]))  # unique labels\nfor d in image_dir:  # iterate over file directory\n    if np.asarray(Image.open(d)).shape == (181, 156, 3):  # serves to remove different sized images\n        images.append(np.asarray(Image.open(d)) \/ 255)  # append images as scaled 3D numpy arrays\n        labels.append(unique_labels.index(d[57:].split('\/')[0]))  # append classifications as encoded labels","7196e07c":"# a function that will shuffle the data\ndef shuffle_data(x_vals, y_vals):\n\n    # convert the x and y values to tuples, and then shuffle it to randomise the order of variables\n    data = [(x, y) for x, y in zip(x_vals, y_vals)]\n    random.shuffle(data)\n    new_x_vals, new_y_vals = zip(*data)\n    \n    return list(new_x_vals), list(new_y_vals)\n\n# a function to tensorize the data\ndef tensorize_data(x_vals, y_vals):\n    \n    x_tensor = []\n    y_tensor = []\n    for x, y in zip(x_vals, y_vals):\n        x_tensor.append(torch.from_numpy(x).float())\n        y_tensor.append(torch.from_numpy(np.array(y)).long())\n                \n    return x_tensor, y_tensor","a3753f96":"# shuffle the data\nx_values, y_values = shuffle_data(images, labels)\n\n# tensorize the data\nx_tensor, y_tensor = tensorize_data(x_values, y_values)","f92f7460":"# split the train\/test split\nsplit = 279 # define the index upon which to split\nx_train = x_tensor[:-split]\ny_train = y_tensor[:-split]\nx_test = x_tensor[-split:]\ny_test = y_tensor[-split:]\nprint(len(x_train), len(x_test))","144961ab":"# define our sheep classification model\nclass SheepClassifier(nn.Module):   \n    def __init__(self):\n        super(SheepClassifier, self).__init__()\n\n        # the CNN component\n        self.cnn_component = nn.Sequential(\n            nn.Conv2d(181, 70, kernel_size=2),  #  2D convolution\n            nn.BatchNorm2d(70),  # batch normalization\n            nn.ReLU(inplace=True),  # ReLU\n            nn.MaxPool2d(kernel_size=2, stride=2),  # 2D max pooling\n        )\n\n        # the MLP fully connected layer\n        self.cnn_output_linear = nn.Sequential(\n            nn.Dropout(0.02),\n            nn.Linear(70 * 77 * 1, 800),\n            nn.ReLU(),\n            nn.Dropout(0.02),\n            nn.Linear(800, 200),\n            nn.ReLU(),\n            nn.Dropout(0.02),\n            nn.Linear(200, 75),\n            nn.ReLU(),\n            nn.Linear(75, 4)\n        )\n\n    # defining the forward pass    \n    def forward(self, x):\n        \n        # pass through the CNN component\n        cnn_output = self.cnn_component(x)\n        \n        # flatten outputs\n        cnn_output_reshaped = cnn_output.view(cnn_output.shape[0], cnn_output.shape[1]\n                                              * cnn_output.shape[2] * cnn_output.shape[3])\n        \n        # pass flattened output through the MLP fully connected layer\n        output = self.cnn_output_linear(cnn_output_reshaped)\n        \n        return output\n    \nsheep_classifier = SheepClassifier()  # define the model\nprint(sheep_classifier)  # print the model to summarise it","d57953f7":"loss_fn = nn.CrossEntropyLoss()  # loss function\noptimizer = torch.optim.Adam(sheep_classifier.parameters(), lr=0.000025)  # optimizer\nepochs = 100  # number of times we repeat the training process\nbatch_size = 50  # batch number\n\nx_val, y_val = x_train[-batch_size:], y_train[-batch_size:]  # validation set \nx_train, y_train = x_train[:-batch_size], y_train[:-batch_size]  # new training set\n\n# form validation batch in advance\nval_x_batch = None\nval_y_batch = None\nfor v in range(batch_size):\n    if v % batch_size == 0:\n        val_x_batch = torch.unsqueeze(x_val[v], dim=0)\n        val_y_batch = torch.unsqueeze(y_val[v], dim=0)\n    else:\n        val_x_batch = torch.cat((val_x_batch, torch.unsqueeze(x_val[v], dim=0)), dim=0)\n        val_y_batch = torch.cat((val_y_batch, torch.unsqueeze(y_val[v], dim=0)), dim=0)\n\n# now start the training process\nfor i in range(epochs):\n    x_batch = None\n    y_batch = None\n    for z in range(len(x_train)):\n        # batching done within the training process\n        if z % batch_size == 0:\n            x_batch = torch.unsqueeze(x_train[z], dim=0)\n            y_batch = torch.unsqueeze(y_train[z], dim=0)\n        else:\n            x_batch = torch.cat((x_batch, torch.unsqueeze(x_train[z], dim=0)), dim=0)\n            y_batch = torch.cat((y_batch, torch.unsqueeze(y_train[z], dim=0)), dim=0)            \n        \n        if x_batch.shape[0] == batch_size:\n            preds = sheep_classifier(x_batch)  # feed batch to the model\n            single_loss = loss_fn(preds, y_batch)  # calculate loss from the batch\n            \n            # validation set evaluation\n            with torch.no_grad():  # disable autograd engine\n                val_preds = sheep_classifier(val_x_batch)  # feed val batch to the model\n                val_loss = loss_fn(val_preds, val_y_batch)  # calculate loss from the val batch\n\n            optimizer.zero_grad()  # zero the gradients\n            single_loss.backward()  # backpropagate through the model\n            optimizer.step()  # update parameters\n        \n    if i%5 == 0:\n        print(f'epoch: {i:5} training loss: {single_loss.item():10.8f} validation loss: {val_loss.item():10.8f}')","2c3872de":"# change the model into its evaluation setting\nsheep_classifier.eval()\n\n# concatenate along new first dimension\nx_test_stack = None\n\n# x_test_stack\nfor i in enumerate(x_test):\n    if x_test_stack == None:\n        x_test_stack = torch.unsqueeze(i[1], dim=0)\n    else:\n        x_test_stack = torch.cat((x_test_stack, torch.unsqueeze(i[1], dim=0)), dim=0)\n\n# y_test_stack\ny_test_stack = None\n\n# populate the y_test_stack variable\nfor z in enumerate(y_test):\n    if y_test_stack == None:\n        y_test_stack = torch.unsqueeze(z[1], dim=0)\n    else:\n        y_test_stack = torch.cat((y_test_stack, torch.unsqueeze(z[1], dim=0)), dim=0)","005b7b69":"model_preds = sheep_classifier(x_test_stack)  # input testing data\nmodel_preds = np.argmax(model_preds.detach().numpy(), axis=1)\naccuracy = metrics.accuracy_score(y_test_stack, model_preds)  # check accuracy\nprint(f'Overall testing accuracy is {round((accuracy * 100), 2)}% (2 d.p)')","cb17cf9b":"# a function to present images of sheep within the testing set as well as \n# the model's prediction and the correct label\ndef result_presentation(x_test, y_test, prediction, labels):\n    \n    ran = random.sample(range(0, x_test.shape[0]), 6)\n    \n    # Set up the matplotlib figure\n    f, ax = plt.subplots(2, 3, figsize=(10,10))\n    \n    ax[0, 0].imshow(x_test[ran[0]].numpy())\n    ax[0, 0].set_title(\"Predicted: \" + labels[prediction[ran[0]]] + \n                       \"\\nTruth: \" + labels[y_test[ran[0]]], fontsize=15)\n\n    ax[0, 1].imshow(x_test[ran[1]].numpy())\n    ax[0, 1].set_title(\"Predicted: \" + labels[prediction[ran[1]]] + \n                       \"\\nTruth: \" + labels[y_test[ran[1]]], fontsize=15)\n    \n    ax[0, 2].imshow(x_test[ran[2]].numpy())\n    ax[0, 2].set_title(\"Predicted: \" + labels[prediction[ran[2]]] + \n                       \"\\nTruth: \" + labels[y_test[ran[2]]], fontsize=15)\n    \n    ax[1, 0].imshow(x_test[ran[3]].numpy())\n    ax[1, 0].set_title(\"Predicted: \" + labels[prediction[ran[3]]] + \n                       \"\\nTruth: \" + labels[y_test[ran[3]]], fontsize=15)\n    \n    ax[1, 1].imshow(x_test[ran[4]].numpy())\n    ax[1, 1].set_title(\"Predicted: \" + labels[prediction[ran[4]]] + \n                       \"\\nTruth: \" + labels[y_test[ran[4]]], fontsize=15)\n    \n    ax[1, 2].imshow(x_test[ran[5]].numpy())\n    ax[1, 2].set_title(\"Predicted: \" + labels[prediction[ran[5]]] + \n                       \"\\nTruth: \" + labels[y_test[ran[5]]], fontsize=15)\n    \n    f.tight_layout(pad=1.5)\n    \n    return\n\n# call the function\nresult_presentation(x_test_stack, y_test_stack, model_preds, unique_labels)","ae7bafc6":"# define a function that calculates the accuracy of each sheep breed\ndef accuracy_by_breed(y_test, predictions, label):\n    \n    correct = 0\n    labels = 0\n    for truth, predicted in zip(y_test.tolist(), predictions):\n        #print(truth, predicted)\n        if truth == label:\n            labels += 1\n        \n        if truth == label and predicted == label:\n            correct += 1\n                                    \n    return (correct \/ labels) * 100 \n\n# determine the accuracy of testing predictions by sheep breed\nlabel_0 = accuracy_by_breed(y_test_stack, model_preds, 0)\nlabel_1 = accuracy_by_breed(y_test_stack, model_preds, 1)\nlabel_2 = accuracy_by_breed(y_test_stack, model_preds, 2)\nlabel_3 = accuracy_by_breed(y_test_stack, model_preds, 3)\n\n# plot as a barchart\nsns.set(rc={'figure.figsize': (45.0, 20.0)})\nsns.set_context(\"notebook\", font_scale=4.5, rc={\"lines.linewidth\": 0.5})\nax = sns.barplot(x=unique_labels,\n                 y=[label_0, label_1, label_2, label_3])\nax.set_ylabel('Classification accuracy (%)', labelpad=40, fontsize=65)\nax.set_xlabel('Sheep Breed', labelpad=40, fontsize=65)\nplt.title(\"A Barplot comparing the model classification \\naccuracy by sheep breed\", fontsize=100)","a593d1bd":"### Stage 4: Model Training\n\nStage 4 involves training the model by (i) feeding it batches of training data, (ii) evaluating its output against the corresponding labels and (iii) backpropagating through the model and updating parameters in order to optimize performance.\n\nThe following hyperparameters were used:\n* Number of Epochs: 100\n* Loss Function: Cross Entropy Loss\n* Optimizer: Adam's Optimizer\n* Learning rate: 0.000025\n\nA validation set is also used in order to check for overfitting.","1b9e3062":"## Conclusion\n\nOverall, this deep learning CNN based model appears competent at classifying sheep breeds with a mean accuracy score of 90.35% and accuracy values ranging from 87.81-91.40%.\n\nAs mentioned in the introduction, this was my first attempt at both using CNNs and Computer Vision so any feedback and criticism will be greatly appreciated!","394dfa33":"Generally from re-runs, the model appears to perform better with classifying both the Suffolk and White Suffolk sheep breeds, whilst it struggles more with both the Poll Dorset breed and particularly the Marino breed. However, the performance of the model across the sheep breeds may vary in subsequent re-runs from other users.","54e735c1":"The model is then fed the stacked (concatenated) testing data, and the model outputs are compared to the actual labels using an accuracy score:","1ed9a3f9":"### Stage 2: Training\/Testing Dataset Splitting\n\nThe second stage involves splitting the data into training\/testing datasets. The former is used to train the model by enabling it to learn from the data and adjust its parameters in order to generalize effectively, whilst the latter is used in order to provide a final evaluation on the model's performance.\n\nThe x and y tensors are split by index, whereby the training sets (x_train and y_train) variables represent the first 1400 values within the x_tensor and y_tensor variables, and the testing sets (x_test and y_test) represent the last 279 values within the x_tensor and y_tensor variables. Since the ordering of values were shuffled in Stage 1 (using the 'shuffle_data()' function), the training and testing sets should both contain a fairly even split of x\/y values corresponding to each of the 4 sheep breeds.","aba6d21c":"### Stage 1: Data Preparation\n\nThe first stage involves preparing the data for the subsequent steps. Specifically, this involves first extracting & preprocessing the data from its raw format into a format that can be understood by a machine (step 1.1), and then shuffling and tensorizing the data (step 1.2).\n\n#### 1.1 Data Extraction & Preprocessing\n\nThe first step involves extracting the data from the directory and preprocessing it so that it can be appropriately understood by a machine. With this in mind, the images within the directory are iterated over, with the 'images' variable being populated with the imagery data, and the 'labels' variable with the classification label.\n\nSpecifically at each iteration, the image is first converted to a 3D numpy array, and each value is divided by 255 which serves to scale the data and thus accelerate the calculations associated with DL, whilst the value appended to the 'labels' variable is encoded with a value that corresponds to the sheep breed classification.\n\nAdditionally, one of the 1680 images was of different size (all others were 181 x 156 x 3). In this case the image was discarded.","18b997ad":"#### 1.2 Data shuffling and Tensorization\n\nThe second step of stage 1 involves shuffling and tensorizing the data. The shuffling is done using the function 'shuffle_data()', and is required in order to make sure that the training and testing sets (discussed later) both contain a fairly even split of all four sheep types. The tensorization is done using the 'tensorize_data()' function, which converts the x values into 3D tensors of type 'float', and the y values into tensors of type 'long'.","be8680af":"### Stage 5: Model Testing & Evaluation\n\nWith the model being fully trained, the model is then fed unseen testing data and its performance evaluated.\n\nThe testing data is first concatenated into batches in order to feed the model all the data at once, as such:","567c4ae5":"From 6 re-runs, the mean accuracy score was 90.35% with the scores ranging from 87.81-91.40%. However, subsequent re-runs of this kernel by other users may yield scores outside of this range due to (i) each re-run producing a different split of training\/testing set images due to the shuffling function, and (ii) the model's parameters being randomly initialised upon definition.","a445677c":"## Introduction\n\nThis notebook presents a Convolutional Neural Networks (CNNs) based Deep Learning (DL) approach to classifying the breed (4 different breeds) of sheep based upon imagery which exhibited a mean classification accuracy of 90.35% across 6 re-runs of the kernel.\n\nThis was my first time using CNNs and Computer Vision so any feedback\/criticism would be greatly appreciated!\n\nThe notebook follows a workflow with 5 clear stages:\n\n**1) Data Preparation**\n\n**2) Training\/Testing Dataset Splitting**\n\n**3) Model Definition**\n\n**4) Model Training**\n\n**5) Model Testing & Evaluation**","ec59f688":"The function below plots 6 randomly selected images of sheep within the testing set, as well as the model's prediction and the correct label.","7f93598b":"### Stage 3: Model Definition\n\nFollowing the splitting of the training\/testing datasets, the model is then defined. The model is built in PyTorch, a DL framework, and is composed of a CNN layer and an MLP fully connected layer. \n\nThe CNN layer applies (i) a 2D convolution over an input signal (nn.Conv2d()), (ii) batch normalization over the resulting 4D input (nn.BatchNorm2d()), (iii) the ReLU non-linear unit function elementwise over the output of the 2D convolution and batch normalization, and (iv) a 2D max pooling on the output of the three previous components.\n    \nThe MLP fully connected layer takes the flattened CNN layer output and feeds it through an MLP which feaures three hidden layers of width 800, 200 and 75 (each featuring a ReLU unit function) which map the 5390 inputs to 4 outputs which correspond to the four sheep breeds.","02b4b7c9":"In order to better understand the successes and limitations of this model, the accuracy by sheep breed was calculated and plotted as a bar chart in order to see how\/if the model's accuracy at classifying all four different sheep breeds is consistent across breeds, or whether it displays better performance with some as opposed to others. The code is outlined below:"}}