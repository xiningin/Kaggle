{"cell_type":{"48ddb829":"code","00039f05":"code","2e022191":"code","c09d0805":"code","51b7d8be":"code","527785d9":"code","b91409e1":"code","498c3119":"code","e12cd00c":"code","83ceb652":"code","83833bdb":"code","050e947c":"code","9817c547":"code","b3986666":"code","81f47806":"code","04657913":"code","e6d1a545":"code","38e876e6":"code","28a7a478":"code","23751282":"code","ff871ed9":"code","398ce784":"code","e75ea87f":"code","695c43ae":"code","b25f2697":"code","c428ef7c":"code","bc65c16d":"code","0d6e1a97":"code","9f1c7c67":"code","86e3d1b2":"code","83307388":"code","5805bbcd":"code","cc89f4f9":"code","78fccfb9":"code","4155fd3b":"code","c99a946b":"code","bb1260cc":"code","62f0ec1c":"code","50032680":"code","fd51bbac":"code","1e69eda5":"code","aca636ca":"code","f54ac30b":"code","d9922a4a":"markdown","f3df8291":"markdown","5c104dfe":"markdown","dc4f593e":"markdown","164c1faf":"markdown","8501ffaf":"markdown","0715dcb6":"markdown","a4c40f97":"markdown","3d6c2a03":"markdown","f4e86773":"markdown","53f9c2c7":"markdown","700ed7b1":"markdown","34067c9c":"markdown","85b72c51":"markdown","73187120":"markdown","b6097b2d":"markdown","0f08f079":"markdown","e8eafa03":"markdown","72f6efd9":"markdown","998a25d4":"markdown"},"source":{"48ddb829":"%matplotlib inline\nimport collections\nimport gensim\nimport pandas\nimport nltk.corpus\nimport nltk.sentiment\nimport sklearn.linear_model\nimport textblob\nimport random\nimport numpy\nimport sklearn.metrics\nimport sklearn.ensemble\nimport seaborn\nimport re\n\nsentence_splitter=re.compile(u\"\"\"[.?!]['\"]*\\s+\"\"\",re.UNICODE)\n","00039f05":"def sentence_structure_features(document):\n    return ['_'.join((pos for (word,pos) in sentence.pos_tags))\n            for sentence in textblob.blob.TextBlob(document).sentences]","2e022191":"class SentenceStructureCorpus(object):\n    def __init__(self):\n        lies=pandas.read_csv(\"..\/input\/fake.csv\")\n        n_lies=lies.shape[0]\n        self.vader=nltk.sentiment.vader.SentimentIntensityAnalyzer()\n        print(\"Converting Fake News corpus\")\n        self.data=[sentence_structure_features('{0}\\n{1}'.format(row['title'],row['text']))\n                   for (index,row) in lies.iterrows()]\n        sentiments=[self.analyse_sentiments('{0}\\n{1}'.format(row['title'],row['text']))\n                    for (index,row) in lies.iterrows()]\n        reuters=nltk.corpus.reuters\n        print('Converting Reuters corpus')\n        self.data.extend([sentence_structure_features(reuters.raw(fileid))\n                          for fileid in reuters.fileids()])\n        sentiments.extend([self.analyse_sentiments(reuters.raw(fileid))\n                           for fileid in reuters.fileids()])\n        self.sentiments=numpy.array(sentiments)\n        self.N=len(self.data)\n        self.labels=numpy.ones(self.N)\n        self.labels[:n_lies]=0\n        self.test_sample=random.sample(range(self.N),self.N\/\/10)\n        print(\"Creating dictionary\")\n        self.dictionary=gensim.corpora.dictionary.Dictionary(self.data)\n        \n    def __iter__(self):\n        return (self.dictionary.doc2bow(document) for document in self.data)\n                          \n    def analyse_sentiments(self,document):\n        valences=numpy.array([[sent['pos'],sent['neg'],sent['neu']]\n                             for sent in (self.vader.polarity_scores(sentence)\n                                          for sentence in sentence_splitter.split(document))])\n        return valences.sum(axis=0)\n    \n    def training_data(self):\n        return [self.dictionary.doc2bow(document) for (i,document) in enumerate(self.data)\n                if i not in self.test_sample]\n                \n    def training_labels(self):\n        return self.labels[[i for i in range(self.N) if i not in self.test_sample]]\n    \n    def training_sentiments(self):\n        return self.sentiments[[i for i in range(self.N) if i not in self.test_sample]]\n    \n    def test_sentiments(self):\n        return self.sentiments[self.test_sample]\n                \n    def test_data(self):\n        return [self.dictionary.doc2bow(self.data[i])\n                for i in self.test_sample]\n            \n    def test_labels(self):\n        return self.labels[self.test_sample]","c09d0805":"ssf=SentenceStructureCorpus()\nprint(\"Training LSI\")\nlsi=gensim.models.lsimodel.LsiModel(ssf)","51b7d8be":"vectors=gensim.matutils.corpus2dense(lsi[ssf.training_data()],lsi.num_topics).T\nclassifier=sklearn.linear_model.LogisticRegression()\nprint(\"Training classifier\")\nclassifier.fit(vectors,ssf.training_labels())\nprint(\"Testing classifier\")\nconfusion=sklearn.metrics.confusion_matrix(ssf.test_labels(),\n                                           classifier.predict(gensim.matutils.corpus2dense(lsi[ssf.test_data()],\n                                                                                           lsi.num_topics).T))\nseaborn.heatmap(confusion,annot=True)","527785d9":"def precision(cm):\n    return cm[1,1]\/cm[:,1].sum()\n\ndef recall(cm):\n    return cm[1,1]\/cm[1].sum()\n\ndef accuracy(cm):\n    return (cm[0,0]+cm[1,1])\/cm.sum()\n\ndef matthews(cm):\n    return (cm[0,0]*cm[1,1]-cm[1,0]*cm[0,1])\/numpy.sqrt(cm[0].sum()*cm[1].sum()*cm[:,0].sum()*cm[:,1].sum())","b91409e1":"precision(confusion)","498c3119":"recall(confusion)","e12cd00c":"accuracy(confusion)","83ceb652":"matthews(confusion)","83833bdb":"sentiment_classifier=sklearn.linear_model.LogisticRegression()\nsentiment_classifier.fit(ssf.training_sentiments(),ssf.training_labels())\nconfusion=sklearn.metrics.confusion_matrix(ssf.test_labels(),\n                                           sentiment_classifier.predict(ssf.test_sentiments()))\nseaborn.heatmap(confusion,annot=True)","050e947c":"precision(confusion)","9817c547":"recall(confusion)","b3986666":"accuracy(confusion)","81f47806":"matthews(confusion)","04657913":"enhanced_vectors=numpy.hstack([vectors,ssf.training_sentiments()])\ncombined_classifier=sklearn.linear_model.LogisticRegression()\nprint(\"Training classifier\")\ncombined_classifier.fit(enhanced_vectors,ssf.training_labels())\nprint(\"Testing classifier\")\nenhanced_test_vectors=numpy.hstack([gensim.matutils.corpus2dense(lsi[ssf.test_data()],\n                                                                 lsi.num_topics).T,\n                                    ssf.test_sentiments()])\nconfusion=sklearn.metrics.confusion_matrix(ssf.test_labels(),\n                                           combined_classifier.predict(enhanced_test_vectors))\nseaborn.heatmap(confusion,annot=True)","e6d1a545":"precision(confusion)","38e876e6":"recall(confusion)","28a7a478":"accuracy(confusion)","23751282":"matthews(confusion)","ff871ed9":"forest0=sklearn.ensemble.RandomForestClassifier(n_estimators=100)\nforest0.fit(vectors,ssf.training_labels())\nconfusion=sklearn.metrics.confusion_matrix(ssf.test_labels(),\n                                           forest0.predict(gensim.matutils.corpus2dense(lsi[ssf.test_data()],\n                                                                                           lsi.num_topics).T))\nseaborn.heatmap(confusion,annot=True)","398ce784":"precision(confusion)","e75ea87f":"recall(confusion)","695c43ae":"accuracy(confusion)","b25f2697":"matthews(confusion)","c428ef7c":"forest1=sklearn.ensemble.RandomForestClassifier(n_estimators=100)\nforest1.fit(ssf.training_sentiments(),ssf.training_labels())\nconfusion=sklearn.metrics.confusion_matrix(ssf.test_labels(),\n                                           forest1.predict(ssf.test_sentiments()))\nseaborn.heatmap(confusion,annot=True)","bc65c16d":"precision(confusion)","0d6e1a97":"recall(confusion)","9f1c7c67":"accuracy(confusion)","86e3d1b2":"matthews(confusion)","83307388":"forest2=sklearn.ensemble.RandomForestClassifier(n_estimators=100)\nforest2.fit(enhanced_vectors,ssf.training_labels())\nconfusion=sklearn.metrics.confusion_matrix(ssf.test_labels(),\n                                           forest2.predict(enhanced_test_vectors))\nseaborn.heatmap(confusion,annot=True)","5805bbcd":"precision(confusion)","cc89f4f9":"recall(confusion)","78fccfb9":"accuracy(confusion)","4155fd3b":"matthews(confusion)","c99a946b":"keys = collections.defaultdict(int)\nfor doc in ssf:\n    for (key,count) in doc:\n        keys[key]+=1\npandas.Series(keys).value_counts()","bb1260cc":"repeated_keys = collections.defaultdict(int)\nfor doc in ssf:\n    repeated_keys[len([key for (key,value) in doc if keys[key]>1])]+=1\npandas.Series(repeated_keys).sort_values()","62f0ec1c":"stopwords = nltk.corpus.stopwords.words(\"english\")\nstopwords","50032680":"def sentence_structure_features(document):\n    blob = textblob.blob.TextBlob(document)\n    return ['_'.join((pos for (word,pos) in sentence.pos_tags))\n            for sentence in textblob.blob.TextBlob(document).sentences] +[word.lower() \n                                                                          for word in blob.words\n                                                                          if  word.lower() in stopwords]\nssf2 = SentenceStructureCorpus()\ntransform = gensim.models.LsiModel(ssf2,id2word=ssf2.dictionary)\ntraining_data = gensim.matutils.corpus2dense(transform[ssf2.training_data()],\n                                             transform.num_topics).T\ntest_data = gensim.matutils.corpus2dense(transform[ssf2.test_data()],\n                                         transform.num_topics).T\nclassifier = sklearn.linear_model.LogisticRegression()\nclassifier.fit(training_data,ssf2.training_labels())\nconfusion = sklearn.metrics.confusion_matrix(ssf2.test_labels(),\n                                             classifier.predict(test_data))\nseaborn.heatmap(confusion,annot=True)","fd51bbac":"precision(confusion)","1e69eda5":"recall(confusion)","aca636ca":"accuracy(confusion)","f54ac30b":"matthews(confusion)","d9922a4a":"This is then a simple and accurate estimate of whether an article is stylistically more similar to fake news or a genuine news source. However, it is necessary to see how well it would perform on articles from different sources. There is now another fake news dataset on Kaggle, which has enabled me to perform [a replication study](https:\/\/www.kaggle.com\/petebleackley\/the-grammar-of-truth-and-lies-part-2).","f3df8291":"Random Forests does slightly worse with sentiments than with sentence structure features. Precision is 76%, recall is 75%, accuracy is 78% and Matthews correlation is 56%. ","5c104dfe":"First, let us train a classifier on the sentence Structure Features","dc4f593e":"We can see that this classifier rejects more `False` articles, at the expense of rejecting more `True` ones. Precision is 71%, recall is 88%, accuracy 79% and Matthews 59%.","164c1faf":"Now let us try using the sentiments to classifiy the articles.","8501ffaf":"From this we can see that around half the documents in the sample have no features in common with any other documents.\n\nIf there were a set of features that only occurred in the Fake News sample, but no features that were unique to the Reuters sample, this would explain the behaviour of the Logistic Regression model with sentence structure features. Truth would be the null hypothesis, which the model would default to if features distinctive to Fake News were not detected.\n\nNow consider stop words. These are words that mainly carry grammatical rather than semantic meaning, and thus have the same properties of content independence that made sentence structures interesting. They will also be found in all documents, so will give a much denser feature space. It should be noted that they are often used in stylometric analysis to identify the authors of disputed documents.  ","0715dcb6":"Let us now redefine the Sentence Structure Features function to include stopwords, and rerun the analysis.","a4c40f97":"Performance in a slight improvement on that of sentiment alone. Precision is 74%, recall is 90%, accuracy is 81% and Matthews is 64%.","3d6c2a03":"The confusion matrix shows that almost all `True` articles are correctly classified, but that just over half the `False` articles are classified as `True`","f4e86773":"Now let's try Random Forests on sentiments","53f9c2c7":"In order to classify whether articles are Fake News or not, a corpus of news articles from a reliable source is needed. For this kernel, I'm using the Reuters Corpus, which is avaialble from NLTK. Fortunately, this contains a similar number of articles to the Fake News Corpus, thus avoiding balancing issues.\n\nIn order to classify articles as `True` or `False`, I want to avoid the use of any features directly based on the vocabulary of the messages. This is because\n1. Using vocabulary-based features risks introducing bias into the model. The Reuters Corpus covers different dates, and hence different news stories, from the Fake News corpus, and so this would be likely to mean that vocabulary terms associated with stories found in one corpus but not the other would bias the model.\n2. Features not based on vocabulary will generalize more easily.\n3. Features not based on vocabulary will be harder to fake.\n\nThe features I have used are\n1. Sentence Structure Features. For each sentence in an article, create a string by concatenating the grammatical roles of each word in the sentence, as extracted with the `textblob` library. These features are very sparse, so their dimensionality will be reduced with Latent Semantic Indexing.\n2. Sentiment features, extracted with VADER.\n\nClassification uses Logistic Regression and Random Forests.","700ed7b1":"One potential problem with Sentence Structure Features is that they are very sparse. Let us examine the distribution of the number of documents a given feature appears in.","34067c9c":"Now let's see how Random Forests perform with combined sentence structure features and sentiments.","85b72c51":"For this classifier, precision is 84%, recall is 88%, accuracy is 87% and Matthews correlation is 74% - slightly (but not significantly) better than sentence structure features alone.","73187120":"Now let's try combining grammatical and sentiment features.","b6097b2d":"We see that many features appear only in a single document. Now let us examine the number of features a given document has in common with other documents in the sample.","0f08f079":"Precision is 90%, Recall is 96%, Accuracy is 93% and Matthews coefficient is 87%","e8eafa03":"While combining the two sets of features does give a better result that either set of features alone, it is only a small improvement, and sentiment is still doing most of the work. A more sophisticated means of combining the features may be able to exploit the strengths of both.\nFirst, let's try Random Forests with sentence structure features.","72f6efd9":"Precision is 82%, recall is 87%, accuracy is 86% and Matthews correlation is 71%","998a25d4":"Precision is 61%, recall is 96%, accuracy is 70% and Matthews Correlation Coefficient is 50%, ie this classifier is 50% better than guesswork."}}