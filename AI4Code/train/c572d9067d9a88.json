{"cell_type":{"11ffe763":"code","4d74a707":"code","e40b0e8d":"code","e568b74e":"code","8b4257a4":"code","1d12601e":"code","1495238b":"code","e21c4848":"code","84f788b6":"code","eb405b9d":"markdown","03bbe4a7":"markdown","52d5f98b":"markdown","92a1195e":"markdown","64790e2f":"markdown"},"source":{"11ffe763":"!pip uninstall -y typing\n!pip install ..\/input\/pytorchtabnetpretraining\/pytorch_tabnet-2.0.1-py3-none-any.whl\n!pip install \/kaggle\/input\/iterative-stratification\/iterative-stratification-master\/","4d74a707":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nimport numpy as np\nimport pandas as pd \n\nimport os\nimport random\nimport sys\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nfrom tqdm import tqdm\nfrom sklearn.metrics import log_loss\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\nfrom sklearn.metrics import roc_auc_score\n","e40b0e8d":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(42)","e568b74e":"data_path = \"..\/input\/lish-moa\/\"\ntrain = pd.read_csv(data_path+'train_features.csv')\ntrain.drop(columns=[\"sig_id\"], inplace=True)\n\ndrug = pd.read_csv(data_path +'train_drug.csv')\n\ntrain_targets_scored = pd.read_csv(data_path+'train_targets_scored.csv')\n#train_targets_scored.drop(columns=[\"sig_id\"], inplace=True)\ntargets = train_targets_scored.columns[1:]\ntest = pd.read_csv(data_path+'test_features.csv')\ntest.drop(columns=[\"sig_id\"], inplace=True)\n\nsubmission = pd.read_csv(data_path+'sample_submission.csv')\n\nremove_vehicle = True\n\nif remove_vehicle:\n    kept_index = train['cp_type']=='trt_cp'\n    train = train.loc[kept_index].reset_index(drop=True)\n    train_targets_scored = train_targets_scored.loc[kept_index].reset_index(drop=True)\n\ntrain[\"cp_type\"] = (train[\"cp_type\"]==\"trt_cp\") + 0\ntrain[\"cp_dose\"] = (train[\"cp_dose\"]==\"D1\") + 0\n\ntest[\"cp_type\"] = (test[\"cp_type\"]==\"trt_cp\") + 0\ntest[\"cp_dose\"] = (test[\"cp_dose\"]==\"D1\") + 0\n\nX_test = test.values\n\n\n# Taken from Chris Deotte folds\n\nSEED = 42\n\nNB_SPLITS = 5\n\n# Taken from Chris : https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/195195\nscored = train_targets_scored.merge(drug, on='sig_id', how='left') \n# LOCATE DRUGS\nvc = scored.drug_id.value_counts()\nvc1 = vc.loc[vc<=18].index.sort_values()\nvc2 = vc.loc[vc>18].index.sort_values()\n\n# STRATIFY DRUGS 18X OR LESS\ndct1 = {}; dct2 = {}\nskf = MultilabelStratifiedKFold(n_splits=NB_SPLITS, shuffle=True, \n          random_state=SEED)\ntmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\nfor fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n    dd = {k:fold for k in tmp.index[idxV].values}\n    dct1.update(dd)\n\n# STRATIFY DRUGS MORE THAN 18X\nskf = MultilabelStratifiedKFold(n_splits=NB_SPLITS, shuffle=True, \n          random_state=SEED)\ntmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop=True)\nfor fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n    dd = {k:fold for k in tmp.sig_id[idxV].values}\n    dct2.update(dd)\n\n# ASSIGN FOLDS\nscored['fold'] = scored.drug_id.map(dct1)\nscored.loc[scored.fold.isna(),'fold'] =\\\n    scored.loc[scored.fold.isna(),'sig_id'].map(dct2)\nscored.fold = scored.fold.astype('int8')\n","8b4257a4":"from pytorch_tabnet.metrics import Metric\nfrom sklearn.metrics import roc_auc_score, log_loss\n\nclass LogitsLogLoss(Metric):\n    \"\"\"\n    LogLoss with sigmoid applied\n    \"\"\"\n\n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):\n        \"\"\"\n        Compute LogLoss of predictions.\n\n        Parameters\n        ----------\n        y_true: np.ndarray\n            Target matrix or vector\n        y_score: np.ndarray\n            Score matrix or vector\n\n        Returns\n        -------\n            float\n            LogLoss of predictions vs targets.\n        \"\"\"\n        logits = 1 \/ (1 + np.exp(-y_pred))\n        aux = (1-y_true)*np.log(1-logits+1e-15) + y_true*np.log(logits+1e-15)\n        return np.mean(-aux)","1d12601e":"from pytorch_tabnet.pretraining import TabNetPretrainer\n\nBS=1024\nMAX_EPOCH=101\n\nN_D = 128\nN_A = 32\nN_INDEP = 1\nN_SHARED = 1\nN_STEPS = 3\n\ntabnet_params = dict(n_d=N_D, n_a=N_A, n_steps=N_STEPS,  #0.2,\n                         n_independent=N_INDEP, n_shared=N_SHARED,\n                         lambda_sparse=0., optimizer_fn=torch.optim.Adam,\n                         optimizer_params=dict(lr=2e-2),\n                         mask_type=\"entmax\",\n                         scheduler_params=dict(mode=\"min\",\n                                               patience=5,\n                                               min_lr=1e-5,\n                                               factor=0.9,),\n                         scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,                         \n                         verbose=10,\n                         )\n\npretrainer = TabNetPretrainer(**tabnet_params)\n\npretrainer.fit(X_train=test.values[:,1:], #  np.vstack([train.values[:,1:], test.values[:,1:]])\n          eval_set=[train.values[:,1:]],\n          max_epochs=MAX_EPOCH,\n          patience=20, batch_size=BS, virtual_batch_size=128, #128,\n          num_workers=0, drop_last=True,\n          pretraining_ratio=0.8)","1495238b":"scores_auc_all= []\ntest_cv_preds = []\n\noof_preds = []\noof_targets = []\nscores = []\nscores_auc = []\nNB_FOLD = 5\nfor fold_nb in range(NB_FOLD):\n    train_idx = scored[scored.fold!=fold_nb].index\n    val_idx = scored[scored.fold==fold_nb].index\n\n    print(\"FOLDS : \", fold_nb)\n    if fold_nb >= NB_FOLD:\n        break\n    ## model\n    X_train, y_train = train.values[train_idx, 1:], train_targets_scored.values[train_idx, 1:].astype(float) #[:,simple_tasks].astype(float)\n    X_val, y_val = train.values[val_idx, 1:], train_targets_scored.values[val_idx, 1:].astype(float) # [:,simple_tasks].astype(float)\n    MAX_EPOCH=51\n    BS=1024\n\n    tabnet_params = dict(n_d=N_D, n_a=N_A, n_steps=N_STEPS,\n                         n_independent=N_INDEP, n_shared=N_SHARED,\n                         gamma=1.0,\n                         lambda_sparse=0., optimizer_fn=torch.optim.Adam, # \n                         optimizer_params=dict(lr=2e-2, # 2e-2\n                                               weight_decay=1e-5\n                                              ),\n                         mask_type=\"entmax\",\n#                          scheduler_params=dict(mode=\"min\",\n#                                                patience=5,\n#                                                min_lr=1e-5,\n#                                                factor=0.9,),\n#                          scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                         scheduler_params=dict(max_lr=0.05,\n                                              steps_per_epoch=int(X_train.shape[0] \/ BS),\n                                              epochs=MAX_EPOCH,\n                                              is_batch_level=True),\n                         scheduler_fn=torch.optim.lr_scheduler.OneCycleLR,\n                         verbose=10,\n                         )\n\n    model = TabNetRegressor(**tabnet_params)\n\n    model.fit(X_train=X_train,\n              y_train=y_train,\n              eval_set=[(X_val, y_val)],\n              eval_name = [\"val\"],\n              eval_metric = [\"logits_ll\"],\n              max_epochs=MAX_EPOCH,\n              patience=20, batch_size=BS, virtual_batch_size=128, #128,\n              num_workers=1, drop_last=True,\n              from_unsupervised=pretrainer,\n              # use binary cross entropy as this is not a regression problem              \n              loss_fn=torch.nn.functional.binary_cross_entropy_with_logits)\n        ## save oof to compute the CV later \n    preds_val = model.predict(X_val)\n    # Apply sigmoid to the predictions\n    preds =  1 \/ (1 + np.exp(-preds_val))\n    score = np.min(model.history[\"val_logits_ll\"])\n    scores.append(score)\n    oof_preds.append(preds)\n    oof_targets.append(y_val)\n\n#     name = cfg.save_name + f\"_fold{fold_nb}\"\n#     model.save_model(name)    \n\n    # preds on test\n    preds_test = model.predict(X_test[:,1:])\n    test_cv_preds.append(1 \/ (1 + np.exp(-preds_test)))\n\noof_preds_all = np.concatenate(oof_preds)\noof_targets_all = np.concatenate(oof_targets)\ntest_preds_all = np.stack(test_cv_preds)","e21c4848":"aucs = []\nfor task_id in range(oof_preds_all.shape[1]):\n    aucs.append(roc_auc_score(y_true=oof_targets_all[:, task_id],\n                              y_score=oof_preds_all[:, task_id]))\nprint(f\"Overall AUC : {np.mean(aucs)}\")\nprint(f\"Average CV : {np.mean(scores)}\")","84f788b6":"all_feat = [col for col in submission.columns if col not in [\"sig_id\"]]\nsubmission[all_feat] = test_preds_all.mean(axis=0)\n# set control to 0\nsubmission.loc[test['cp_type']==0, submission.columns[1:]] = 0\nsubmission.to_csv('submission.csv', index=None)","eb405b9d":"## Package installation","03bbe4a7":"# Data and minimal preprocessing","52d5f98b":"## Cross validation starting from pretrained weights","92a1195e":"# Self Supervision with pytorch-tabnet\n\n## What is self supervision?\n\nIn machine learning, we are talking about self supervision when you train a supervised algorithm with labels which you created artificially.\nIt's often a reconstruction game, where you don't have labels but you mask some information from the data itself to the model and then try to predict the information you masked. \n\nSome examples:\n- for computer vision, the labels can be \"what is the rotation degree applied to this image?\" \"can you reconstruct the pixels missing on this patch?\"\n- from video clip: \"which one of this to frame comes first in the video?\"\n- tabular data: \"can you reconstruct the missing features?\"\n\n## Why self supervision?\n\nVery often, labelling data is either expensive or very time consuming. Only a small percentage of the available data have labels, and you can only use this small portion of data with supervised algorithms. With the help of self supervision you can use the information of your unlabeled data to pretrain your model. What you are hoping for is that in order to be able to solve the puzzle you created on unlabelled data, your model will need to learn some fundamental concept from your data.\n\n## How does TabNetPretrainer works?\n\nThe current implementation is very close to the one proposed in tabnet research paper, the only difference being that I decided to take the mean of the given loss per batch in order to get consistant results with different batch sizes (instead of summing everything). See original paper : https:\/\/arxiv.org\/abs\/1908.07442\n\nThere is a new class named `TabNetPretrainer` which globally have the same parameters than `TabNetRegressor`, `TabNetClassifier` or `TabNetMultiTaskClassifier`.\nThere is only one more parameter for the `fit` method which is `pretraining_ratio`, a floating point between 0 and 1 that decides how hard is the puzzle you are trying to solve. If `pretraining_ratio=0.8` it means you are asking the model to reconstruct 80% of the feature from 20% you did not mask. Masks are random and created during training at the batch level.\n\nYou need to call `fit` on some dataset to perform self supervised pretraining. You can also give another dataset in order to perform early stopping to avoid overfitting.\n\nThen you can start from the pretrained weights by giving the pretrainer to `from_unsupervised` parameter in the `fit` (note that this would work for `TabNetRegressor`, `TabNetClassifier` or `TabNetMultiTaskClassifier` as only the final layer differs for those networks).\n\nNote : this version of code solves the retraining bug mentionned here : https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/196830\nNote 2 : you won't be able to resuse previously saved pytorch-tabnet model with this version because the body of the network has been refactorize in order to keep the code clean with this addition.\n\n## Where does the code come from?\nThe pytorchtabnetpretraining dataset used here is just a copy paste of the code from this Pull Request : https:\/\/github.com\/dreamquark-ai\/tabnet\/pull\/220\n\nThis has not been merged yet because I consider that it has not been tested nor reviewed enough. You are using this at your own risk (but I'm confident that things are working as expected). I'll merge the PR in a few days after careful review, please feel free to share your thoughts about this either in the comments here or directly in the PR : https:\/\/github.com\/dreamquark-ai\/tabnet\/pull\/220\n\n## What is the boost on performance?\n\nI haven't spend much time on this competition, so I don't have a strong pipeline to share.\n\nHowever I tried to follow the CV scheme from Chris Deotte (https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/195195) so that people can compare this more easily.\n\nFrom the very small experience I did with the same params and 5 fold CV (without using control group):\n- No pretraining : CV 0.01841, LB 0.01951\n- With pretraining (pretraining_ratio 0.8): CV 0.01754, LB 0.01887\n\nSo it might be useful for some of you who have a stronger pipeline!\nPlease let me know if it helped!\n","64790e2f":"## Pretraining\n\nThis is where I pretrain a model.\n\nNote that here I decided to pretrain on test data and validate on train data.\nThis means that the pretraining is different in local version and committed version.\n\nI did this in the hope that the model will perform better on private set if it has been pretrain with it."}}