{"cell_type":{"ec2f0115":"code","58b1a36a":"code","4a067ce0":"code","e01bfe47":"code","f252b204":"code","50c35ab3":"code","aec16cae":"code","a09f2794":"code","7ba1f061":"code","126e4aed":"code","0fea670f":"code","971c1e41":"code","48c11fc7":"code","27a75c36":"code","d301f381":"code","717efc01":"code","2e8955b5":"code","11628911":"code","ebaf75ca":"code","84db731e":"code","1817b5c9":"code","7031adfa":"code","fbdaf9f2":"code","50d113ce":"code","f9c4f35e":"code","567c7f83":"code","452b759d":"code","3736d383":"code","ce4663a4":"code","2bef2b00":"code","d107923d":"code","18e76fe2":"code","1e0e5dd3":"code","e89bad53":"code","9e9784fc":"code","6120f7ec":"code","6def05ea":"code","1f835e99":"code","92f9a25d":"code","96c44a61":"code","11cba63a":"code","572fbd5b":"code","a8ce5361":"code","5f371155":"code","c000e960":"code","f952a7be":"code","89b518ce":"code","fbab165f":"code","c3a9c1ce":"code","031308b5":"code","6221134d":"code","2ad254ee":"code","a5bf0e0f":"code","cc852aae":"code","92ce2cdd":"code","d30eed87":"code","731de5a9":"code","31437a59":"code","3b3903c7":"code","024e21d7":"code","c0dcbdb7":"code","df3730a7":"code","5c9669cd":"code","81ff5a82":"code","4b889464":"code","c43adc76":"code","92409e94":"code","a11ad444":"code","e45d55e7":"code","6caf5ded":"code","9a59cf98":"code","c3e565d4":"code","262b925b":"code","f0ab3b1d":"code","a5795362":"code","8b4d132b":"code","a54d1a3d":"code","217f33ba":"code","236513b4":"markdown","4a4329b3":"markdown","b915b0fe":"markdown","5e39c4ae":"markdown","7d4a21a2":"markdown","03250e9d":"markdown","65945e50":"markdown","b4c4ddb9":"markdown","56aac14b":"markdown","ea688589":"markdown","36d9709c":"markdown","2a82cb6a":"markdown","40fc89ae":"markdown","06d1709e":"markdown","b4aba5c9":"markdown","2385d8f1":"markdown","25910d1d":"markdown","41e66897":"markdown","ee626606":"markdown","8819983a":"markdown","d300929c":"markdown","957206c8":"markdown","75a62561":"markdown","4305b446":"markdown","8b817c39":"markdown","47be123e":"markdown","095011dd":"markdown","0ecb983d":"markdown","cc540324":"markdown","f2e339c3":"markdown","43341dc9":"markdown","f8b8205c":"markdown","5c73d627":"markdown","02d811e5":"markdown"},"source":{"ec2f0115":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.simplefilter(action='ignore')\nimport gc\nimport os\nimport time\n\nimport plotly\nimport plotly.graph_objs as go\nplotly.offline.init_notebook_mode(connected = True)\n\n","58b1a36a":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4a067ce0":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\nprint(\"{} observations and {} features in train set.\".format(train_df.shape[0],train_df.shape[1]))\nprint(\"{} observations and {} features in test set.\".format(test_df.shape[0],test_df.shape[1]))","e01bfe47":"train_df.head(5)","f252b204":"test_df = pd.read_csv('..\/input\/test.csv')\ntest_df.head(5)","50c35ab3":"active_month_series_traindf = train_df['first_active_month'].value_counts()\nactive_month_series_testdf = test_df['first_active_month'].value_counts()\n\ntrace0 = go.Scatter(\n        x = active_month_series_traindf.index,\n        y = active_month_series_traindf.values,\n        name = 'train set')\n\ntrace1 = go.Scatter(\n        x = active_month_series_testdf.index,\n        y = active_month_series_testdf.values,\n        name = 'test set')\n\nplotly.offline.iplot({\n    \"data\": [trace0, trace1],\n    \"layout\": go.Layout(title=\"First Active month in Train & Test data\")\n})","aec16cae":"train_df['target'] = pd.to_numeric(train_df['target'], errors = 'ignore')\ntrain_df['target'].dtypes","a09f2794":"print(\"maximum loyalt score is {}.\".format(train_df['target'].max()))\nprint(\"minimum loyalt score is {}.\".format(train_df['target'].min()))\nprint(\"mean loyalt score is {}.\".format(train_df['target'].mean()))\nprint(\"std loyalt score is {}.\".format(train_df['target'].std()))","7ba1f061":"train_df['target'].describe()","126e4aed":"train_df.describe()","0fea670f":"target_bins = pd.cut(train_df['target'],[-35,-30,-20,-10,-5,0,5,10,20,30])\nLoyalt_score = train_df.groupby(target_bins)['target'].agg(['count']).reset_index()\n\n\nfig, ax = plt.subplots(1,2,figsize=(15,5))\nax[0].set_title('Bar plot of Loyalt Score distribution')\nvis1 = sns.countplot(x=target_bins, data=Loyalt_score, ax = ax[0])\n\n\nax[1].set_title('Histogram of Loyalt Score distribution')\nvis2 = sns.distplot(train_df['target'].values, bins=50, kde=False, color=\"red\", ax = ax[1])","971c1e41":"fig, ax = plt.subplots(1,3, figsize=(20,5))\nvis1 = sns.countplot(x = 'feature_1', data = train_df, ax = ax[0])\nvis2 = sns.countplot(x = 'feature_2', data = train_df, ax = ax[1])\nvis2 = sns.countplot(x = 'feature_3', data = train_df, ax = ax[2])\nax[0].set_title('Feature_1 in train set')\nax[1].set_title('Feature_2 in train set')\nax[2].set_title('Feature_3 in train set')","48c11fc7":"from plotly import tools \n\nfeature1_traindf = train_df['feature_1'].value_counts()\nfeature1_testdf = test_df['feature_1'].value_counts()\nfeature2_traindf = train_df['feature_2'].value_counts()\nfeature2_testdf = test_df['feature_2'].value_counts()\nfeature3_traindf = train_df['feature_3'].value_counts()\nfeature3_testdf = test_df['feature_3'].value_counts()\n\n\nfig = tools.make_subplots(rows=1, cols=3)\n\ntrace0 = go.Bar(x = feature1_traindf.index, y = feature1_traindf.values, name = 'feature1 train set')\ntrace1 = go.Bar(x = feature1_testdf.index, y = feature1_testdf.values, name = 'feature1 test set')\ntrace2 = go.Bar(x = feature2_traindf.index, y = feature2_traindf.values, name = 'feature2 train set')\ntrace3 = go.Bar(x = feature2_testdf.index, y = feature2_testdf.values, name = 'feature2 test set')\ntrace4 = go.Bar(x = feature3_traindf.index, y = feature3_traindf.values, name = 'feature3 train set')\ntrace5 = go.Bar(x = feature3_testdf.index, y = feature3_testdf.values, name = 'feature3 test set')\n\n\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 1, 2)\nfig.append_trace(trace4, 1, 3)\nfig.append_trace(trace5, 1, 3)\n\n\nfig['layout'].update(height=400, width=1100, title='Feature 1, 2 3 in train and test set')\nplotly.offline.iplot(fig, filename='stacked-subplots')","27a75c36":"merchants_df = pd.read_csv('..\/input\/merchants.csv')\nprint('merchant datasets')\nmerchants_df.head(5)","d301f381":"print(\"{} observations and {} features in merchants data.\".format(merchants_df.shape[0],merchants_df.shape[1]))","717efc01":"sale_range = merchants_df['most_recent_sales_range'].value_counts()\npurchanse_range = merchants_df['most_recent_purchases_range'].value_counts()\n\nplotly.offline.iplot({\n    \"data\": [go.Scatter(x = sale_range.index, y = sale_range.values, name = 'sales range'),\n            go.Scatter(x = purchanse_range.index, y = purchanse_range.values, name = 'purchases range')],\n    \"layout\": go.Layout(title=\"most_recent_sales_purchases_range\")\n})","2e8955b5":"lag3 = merchants_df['active_months_lag3'].value_counts()\nlag6 = merchants_df['active_months_lag6'].value_counts()\nlag12 = merchants_df['active_months_lag12'].value_counts()\n\nplotly.offline.iplot({\n    \"data\": [go.Scatter(x = lag3.index, y = lag3.values, name = 'lag3'),\n            go.Scatter(x = lag6.index, y = lag6.values, name = 'lag6'),\n            go.Scatter(x = lag12.index, y = lag12.values, name = 'lag12')],\n    \"layout\": go.Layout(title=\"active_months_lag3, lag6, lag12 data\")\n})","11628911":"hist_df = pd.read_csv('..\/input\/historical_transactions.csv')\nhist_df.head(5)","ebaf75ca":"print(\"{} observations and {} features in history merchant data.\".format(hist_df.shape[0],hist_df.shape[1]))","84db731e":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if df[col].dtypes == 'object':\n            df[col] = df[col].astype('category')\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","1817b5c9":"new_trans_df = pd.read_csv(\"..\/input\/new_merchant_transactions.csv\")\nnew_trans_df.head()","7031adfa":"historical_transactions = reduce_mem_usage(hist_df)\nnewMerchant_transactions = reduce_mem_usage(new_trans_df)\ngc.collect()","fbdaf9f2":"historical_transactions.dtypes","50d113ce":"histPurchase_amount = pd.DataFrame({'hist_total_transactions' : historical_transactions.groupby(['card_id','authorized_flag'])['purchase_amount'].size()}).reset_index()\nhistPurchase_amount.head(5)","f9c4f35e":"authorised_count = pd.DataFrame({'hist_authorised_count' : histPurchase_amount.groupby('card_id')['authorized_flag'].count()}).reset_index()\nhistPurchase_amount = pd.merge(authorised_count, histPurchase_amount, on = 'card_id', how = 'left')\nhistPurchase_amount.head(5)","567c7f83":"newPurchase_amount = pd.DataFrame({'new_total_transactions' : newMerchant_transactions.groupby(['card_id','authorized_flag'])['purchase_amount'].size()}).reset_index()\nnewPurchase_amount.head(5)","452b759d":"newAuthorised_count = pd.DataFrame({'new_authorised_count' : newPurchase_amount.groupby('card_id')['authorized_flag'].count()}).reset_index()\nnewPurchase_amount = pd.merge(newAuthorised_count, newPurchase_amount, on = 'card_id', how = 'left')\nnewPurchase_amount.head(5)","3736d383":"print(\"Unauthorised entries in new Transactions are: {}\\nOnly Unauthorised entries in history Transactions are: {}\"\n      .format(newPurchase_amount.loc[newPurchase_amount['authorized_flag'] == 'N'].shape[0],\n               histPurchase_amount.loc[((histPurchase_amount['authorized_flag'] == 'N') & (histPurchase_amount['hist_authorised_count'] != 2))].shape[0]))","ce4663a4":"# All card holders has atleast once authorised entires so.\ndel histPurchase_amount\ndel newPurchase_amount\ngc.collect()","2bef2b00":"historical_transactions.describe()","d107923d":"historical_installments = pd.DataFrame({'hist_total_installments': historical_transactions.groupby(['card_id'])['installments'].value_counts()}).reset_index()\nhistorical_installments.head(5)","18e76fe2":"newM_installments = pd.DataFrame({'newM_total_installments': newMerchant_transactions.groupby(['card_id'])['installments'].value_counts()}).reset_index()\nnewM_installments.head(5)","1e0e5dd3":"train_df = pd.merge(train_df, historical_installments, on=\"card_id\", how=\"left\")\ntest_df = pd.merge(test_df, historical_installments, on=\"card_id\", how=\"left\")\ntrain_df = pd.merge(train_df, newM_installments, on=\"card_id\", how=\"left\")\ntest_df = pd.merge(test_df, newM_installments, on=\"card_id\", how=\"left\")\ntrain_df.head(5)","e89bad53":"bins = pd.cut(train_df['installments_x'],[0,1,2,3,4,5,6,7,10,20,50,100,900,1000])\n#as the highest number of transaction is 1137\nhist_installments_data = train_df.groupby(bins)['installments_x'].agg(['count','sum','mean','std','min','max']).reset_index()\nhist_installments_data","9e9784fc":"plt.figure(figsize=(18,5))\nsns.boxplot(x=bins, y=train_df['target'], data=hist_installments_data, showfliers=False)\nplt.xticks(rotation='90')\nplt.ylabel('target\/loyalt score')\nplt.title('loyalt score based on number of installments in history transactions data')","6120f7ec":"bins = pd.cut(train_df['installments_y'],[0,1,2,3,4,5,6,7,10,20,50,100,900,1000])\n#as the highest number of transaction is 1137\nnewM_installments_data = train_df.groupby(bins)['installments_y'].agg(['count','sum','mean','std','min','max']).reset_index()\nnewM_installments_data","6def05ea":"plt.figure(figsize=(18,5))\nsns.boxplot(x=bins, y=train_df['target'], data=newM_installments_data, showfliers=False)\nplt.xticks(rotation='90')\nplt.ylabel('target\/loyalt score')\nplt.title('loyalt score based on number of installments in new Merchant Data')","1f835e99":"histPurchase_amount = pd.DataFrame({'hist_total_transactions' : historical_transactions.groupby(['card_id'])['purchase_amount'].size()}).reset_index()\nhistPurchase_amount.head(5)","92f9a25d":"newPurchase_amount = pd.DataFrame({'new_total_transactions' : newMerchant_transactions.groupby(['card_id'])['purchase_amount'].size()}).reset_index()\nnewPurchase_amount.head(5)","96c44a61":"histPurchase_amount.columns = ['card_id','hist_total_transactions']\nnewPurchase_amount.columns = ['card_id','new_total_transactions']\n\ntrain_df = pd.merge(train_df, histPurchase_amount, on = 'card_id', how = 'left')\ntest_df = pd.merge(test_df, histPurchase_amount, on = 'card_id', how = 'left')\ntrain_df = pd.merge(train_df, newPurchase_amount, on = 'card_id', how = 'left')\ntest_df = pd.merge(test_df, newPurchase_amount, on = 'card_id', how = 'left')\n\ntrain_df.head(5)","11cba63a":"s1 = train_df.groupby(['hist_total_transactions'])['target'].mean()\ns2 = train_df.groupby(['new_total_transactions'])['target'].mean()\nplotly.offline.iplot({\n    \"data\": [go.Scatter(x = s1.index, y = s1.values, name = 'hist_transactions'),\n            go.Scatter(x = s2.index, y = s2.values, name = 'new_transactions')],\n    \"layout\": go.Layout(title=\"Number of Transactions in History and New Data vs. Loyalt score based on Purchase amount\")\n})","572fbd5b":"bins = pd.cut(train_df['hist_total_transactions'],[0,10,20,30,40,50,60,70,80,100,120,150,300, 500,1000,1500,2000,3000])\n#as the highest number of transaction is 1137\nhist_data = train_df.groupby(bins)['hist_total_transactions'].agg(['count','sum','mean','std','min','max']).reset_index()\nhist_data","a8ce5361":"hist_data.columns = ['card_id', 'count_hist_transaction', \"sum_hist_transaction\", \"mean_hist_transaction\", \"std_hist_transaction\",\n                     \"min_hist_transaction\", \"max_hist_transaction\"]\ntrain_df = pd.merge(train_df, hist_data, on=\"card_id\", how=\"left\")\ntest_df = pd.merge(test_df, hist_data, on=\"card_id\", how=\"left\")","5f371155":"plt.figure(figsize=(18,5))\nsns.boxplot(x=bins, y=train_df['target'], data=train_df)\nplt.xticks(rotation='90')\nplt.ylabel('target\/loyalt score')\nplt.title('loyalt score based on number of purchased amount in new history Data')","c000e960":"(train_df.loc[train_df['hist_total_transactions'] > 1500]).head()","f952a7be":"bins = pd.cut(train_df['hist_total_transactions'],[1000,1500,2000,3000])\n#as the highest number of transaction is 1137\nhist_data = train_df.groupby(bins)['hist_total_transactions'].agg(['mean']).reset_index()\nplt.figure(figsize=(15,5))\nsns.boxplot(x=bins, y=train_df['target'], data=train_df, showfliers=False)\nplt.xticks(rotation='90')\nplt.ylabel('target\/loyalt score')\nplt.title('loyalt score based on purchase amount')","89b518ce":"(train_df.loc[(train_df['first_active_month'] == '2017-01')]).head()","fbab165f":"historical_transactions['purchase_date'] = pd.to_datetime(historical_transactions['purchase_date'], infer_datetime_format=False)","c3a9c1ce":"historical_transactions['month_yr'] = historical_transactions['purchase_date'].apply(lambda x: x.strftime('%B-%Y'))\nhistorical_transactions.head(5)","031308b5":"history_repeat_business = pd.DataFrame({'hist_transactions' : historical_transactions.groupby('card_id')['month_yr'].value_counts()}).reset_index()\nhistory_repeat_business.head(5)","6221134d":"bins1 = pd.cut(history_repeat_business['hist_transactions'],[0,10,20,30,40,50,60,70,80,100,120,150,1000,1500])\n#as the highest number of transaction is 1137\nhist_data = history_repeat_business.groupby(bins1)['hist_transactions'].agg(['count','sum'])\nhist_data","2ad254ee":"plt.figure(figsize=(15,5))\nsns.countplot(x = bins1, data = hist_data)","a5bf0e0f":"cnt_srs = history_repeat_business['month_yr'].value_counts()\ncnt_srs","cc852aae":"#month_year data sorting.\nmonth_yr = [\"February-2018\",\"January-2018\",\"January-2017\", \"February-2017\", \"March-2017\", \"April-2017\", \"May-2017\", \"June-2017\", \n          \"July-2017\", \"August-2017\", \"September-2017\", \"October-2017\", \"November-2017\", \"December-2017\"]\nhistory_repeat_business['month_yr'] = pd.Categorical(history_repeat_business['month_yr'], categories=month_yr, ordered=True)\nhistory_repeat_business.sort_values(by='month_yr')","92ce2cdd":"plotly.offline.iplot({\n    \"data\": [go.Scatter(x = cnt_srs.index[::-1], y = cnt_srs.values[::-1], name = 'transactions')],\n    \"layout\": go.Layout(title=\"Number of transactions in year 2017\")\n})","d30eed87":"#history_repeat_business = pd.DataFrame({'hist_transactions' : historical_transactions.groupby('card_id')['month_yr'].value_counts()}).reset_index()\nhistory_repeat_business.loc[history_repeat_business['hist_transactions'] >= 500]","731de5a9":"'''L = ['year', 'month', 'day', 'dayofweek', 'dayofyear', 'weekofyear', 'quarter']\nhist_date_data = historical_transactions.join(pd.concat((getattr(historical_transactions['purchase_date'].dt, i).rename(i) for i in L), axis=1))\n#new_date_data = new_date_data.join(pd.concat((getattr(new_date_data['purchase_date'].dt, i).rename(i) for i in L), axis=1))\nhist_date_data.head(5)'''","31437a59":"'''months_map = {1: 'Jan', 2: 'Feb', 3:'March', 4:'Apr', 5:'May', 6:'June', 7:'July', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'}\nday_map = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4:'Friday', 5:'Saturday', 6: 'Sunday'}\nhist_date_data['month'] = hist_date_data['month'].apply(lambda x: months_map[x])\nhist_date_data['dayofweek'] = hist_date_data['dayofweek'].apply(lambda x: day_map[x])\nhist_day_data = pd.DataFrame({'hist_transactions' : hist_date_data.groupby('card_id')['dayofweek'].value_counts()}).reset_index()\n\n#day_srs = hist_date_data.groupby('card_id')['dayofweek'].value_counts()\nhist_day_data.head(5)'''","3b3903c7":"'''new_trans_df = pd.read_csv(\"..\/input\/new_merchant_transactions.csv\")\nnew_trans_df.head()'''","024e21d7":"'''newMerchant_transactions = reduce_mem_usage(new_trans_df)\ngc.collect()'''","c0dcbdb7":"newMerchant_transactions['purchase_date'] = pd.to_datetime(newMerchant_transactions['purchase_date'], infer_datetime_format=False)","df3730a7":"newMerchant_transactions['month_yr'] = newMerchant_transactions['purchase_date'].apply(lambda x: x.strftime('%B-%Y'))\nnewMerchant_transactions.head(5)","5c9669cd":"newMerchant_transactions = pd.DataFrame({'new_merchant_transactions' : newMerchant_transactions.groupby('card_id')['month_yr'].size()}).reset_index()\nnewMerchant_transactions.head(5)","81ff5a82":"train_df = pd.merge(train_df, newMerchant_transactions, on = 'card_id', how = 'left')\ntest_df = pd.merge(test_df, newMerchant_transactions, on = 'card_id', how = 'left')","4b889464":"train_df.head(5)","c43adc76":"bins = pd.cut(newMerchant_transactions['new_merchant_transactions'],[0,10,20,30,40,50,60,70,80])\n#as the highest number of transaction is 1137\nnewM_data = newMerchant_transactions.groupby(bins)['new_merchant_transactions'].agg(['count','sum','mean','std','min','max']).reset_index()\nnewM_data","92409e94":"bins = pd.cut(newMerchant_transactions['new_merchant_transactions'],[0,10,20,30,40,50,60,70,80])\n#as the highest number of transaction is 1137\nnewM_data = newMerchant_transactions.groupby(bins)['new_merchant_transactions'].agg(['count','sum','mean','std','min','max']).reset_index()\nnewM_data","a11ad444":"newM_data.columns = [\"card_id\", \"count_newM_trans\", \"sum_newM_trans\", \"mean_newM_trans\", \"std_newM_trans\", \"min_newM_trans\", \"max_newM_trans\"]\ntrain_df = pd.merge(train_df, newM_data, on=\"card_id\", how=\"left\")\ntest_df = pd.merge(test_df, newM_data, on=\"card_id\", how=\"left\")","e45d55e7":"train_df.head(5)","6caf5ded":"plt.figure(figsize=(15,5))\nsns.countplot(x = bins, data = train_df)\nplt.ylabel('target\/loyalt score')","9a59cf98":"bins = pd.cut(train_df['new_merchant_transactions'],[0,10,20,30,40,50,60,70,80,100,120])\nnewM_srs = train_df.groupby(bins)['target'].value_counts()\nplt.figure(figsize=(15,5))\nsns.boxplot(x=bins, y='target', data=train_df, showfliers=False)\nplt.ylabel('target\/loyalt score')\nplt.xticks(rotation = '90')\nplt.title('new Merchant transaction distribution by target\/loyalty score values')\nplt.show()","c3e565d4":"train_df['first_active_month'] = pd.to_datetime(train_df['first_active_month'], infer_datetime_format=False)","262b925b":"test_df['first_active_month'] = pd.to_datetime(test_df['first_active_month'], infer_datetime_format=False)","f0ab3b1d":"train_df['year'] = train_df['first_active_month'].dt.year\ntest_df['year'] = test_df['first_active_month'].dt.year\ntrain_df['month'] = train_df['first_active_month'].dt.year\ntest_df['month'] = test_df['first_active_month'].dt.month","a5795362":"cols = ['feature_1', 'feature_2', 'feature_3','installments_x', 'hist_total_installments', 'installments_y',\n       'newM_total_installments', 'hist_total_transactions',\n       'new_total_transactions', 'count_hist_transaction',\n       'sum_hist_transaction', 'mean_hist_transaction', 'std_hist_transaction',\n       'min_hist_transaction', 'max_hist_transaction',\n       'new_merchant_transactions', 'count_newM_trans', 'sum_newM_trans',\n       'mean_newM_trans', 'std_newM_trans', 'min_newM_trans', 'max_newM_trans',\n       'year', 'month']","8b4d132b":"from sklearn import model_selection, preprocessing, metrics\nimport lightgbm as lgb\n\nx_train = train_df[cols]\nx_test = test_df[cols]\ny_train = train_df[cols].values\ny_test = test_df[cols].values\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\n#---Build LGB Model-----\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\n\n\n\ndef run_lgb(train_X, train_y, val_X, val_y, test_X):\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"num_leaves\" : 30,\n        \"min_child_weight\" : 50,\n        \"learning_rate\" : 0.05,\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.7,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 2018,\n        \"verbosity\" : -1,\n        'max_depth' : 10,\n        'n_estimators' : 200,\n        'min_child_samples': 399, \n        'min_child_weight': 0.1\n    }\n    \n    train_data=lgb.Dataset(train_X,label=train_y)\n    valid_data=lgb.Dataset(val_X,label=val_y)\n \n    evals_result = {}\n    model = lgb.train(params, train_data, 1000, valid_sets=[valid_data], early_stopping_rounds=100, verbose_eval=100, evals_result=evals_result)\n    \n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    return pred_test_y, model, evals_result\n\ntrain_X = train_df[cols]\ntest_X = test_df[cols]\ntrain_y = train_df['target'].values\n\npred_test = 0\nkf = model_selection.KFold(n_splits=5, random_state=2018, shuffle=True)\nfor dev_index, val_index in kf.split(train_df):\n    dev_X, val_X = train_X.loc[dev_index,:], train_X.loc[val_index,:]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    \n    pred_test_tmp, model, evals_result = run_lgb(dev_X, dev_y, val_X, val_y, test_X)\n    pred_test += pred_test_tmp\npred_test \/= 5.","a54d1a3d":"fig, ax = plt.subplots(figsize=(12,10))\nlgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nax.grid(False)\nplt.title(\"LightGBM - Feature Importance\", fontsize=15)\nplt.show()","217f33ba":"sub_df = pd.DataFrame({\"card_id\":test_df[\"card_id\"].values})\nsub_df[\"target\"] = pred_test\nsub_df.to_csv(\"lgb_baseline.csv\", index=False)","236513b4":"#### 2.2.1 Analysing first active month data in train and test dataset\n","4a4329b3":"### 2. Data Analysis\n\n#### 2.1 Train and Test Data Analysis","b915b0fe":"Business is growing overall in year 2017-2018. Greate news promotion offers and discount schemes are working reasonably good.","5e39c4ae":"Card holders whose first active month is 2017, generally has higher loyalt score. they have feature variations too. however their purchase amount count is average.","7d4a21a2":"Thanks for stopping by. If you like my kernel, please upvote.","03250e9d":"### Observation :\n\n#### From given set of merchants data, most recent sale and purchase ratio is quite similar. Which is a good sign for any business.\n    ","65945e50":"## Objective:\n\npredict a loyalty score for each card_id represented in test.csv and sample_submission.csv.\n\n\n## Introduction about company and data provided\n\n**Elo**, one of the largest payment brands in Brazil, has built partnerships with merchants in order to offer promotions or discounts to cardholders. \n\nWe need to find out\n\n### Do these promotions work for either the consumer or the merchant? \n\n### Do customers enjoy their experience? \n\n### Do merchants see repeat business? \n\nPersonalization is key.\n\n## File descriptions\n\n**train.csv** - the training set\n\n**test.csv** - the test set\n\n**sample_submission.csv** - a sample submission file in the correct format - contains all card_ids you are expected to predict for.\n\n**historical_transactions.csv** - up to 3 months' worth of historical transactions for each card_id\n\n**merchants.csv** - additional information about all merchants \/ merchant_ids in the dataset.\n\n**new_merchant_transactions.csv**  - two months' worth of data for each card_id containing ALL purchases that card_id made at merchant_ids that were not visited in the historical data.","b4c4ddb9":"### 2.3.2 (c) Analysing Loyalt Score based on purchased amount field.","56aac14b":"### 3. Baseline Model\n\nLet us build a baseline model using the new features.","ea688589":"### Observation\n\nAs expected aggregation by 'count','mean' or 'std' their distribution across loyalt score remains same based on the purchase amount range here.","36d9709c":"### 2.2.3 Analysing feature_1, feature_2 and feature_3 distribution in train and test dataset","2a82cb6a":"### Observation: \n   #### In History transaction data there are individual card holder and their entry as Authorised as well Unauthorised flag shows there were no restrictions or conditions for authorised transactions only. where in new merchant transaction has might be that condition and thus all entries are authorised only.\n   \n   #### So, we can total the hist_total_transactions of individual card holder and ignore their authorised flag. \n\n#### All card holders has atleast once authorised entires so.","40fc89ae":"### 2.3 History Merchant Data Analysis","06d1709e":"### Observation: \n\n#### This shows overall business is growing. To be more precise Number of Loyalt card holders are overall increasing every year. ","b4aba5c9":"To check repeate business, I tried calculating through monthly total transactions groupby card-Id. however I have experienced more memory consumtion. lets analyze impact of authorized_flag, number of installments, purchase amount.","2385d8f1":"### Observations: \n\n#### Looks like each card having authorised and unauthorised transactions entries. so, now its important to check weather any card holder has done only unauthorised transactions?","25910d1d":"As we can see, this file is resonably big and took much memory space. thus to do further analysis, it's important to reduce memory usage.\n\nThere's a way to optimise for the reading issue\n\n* Load objects as categories.\n* Binary values are switched to int8\n* Binary values with missing values are switched to float16 (int does not understand nan)\n* 64 bits encoding are all switched to 32, or 16 of possible","41e66897":"### Observations: \n\nBusiness is gradually increasing every quater for individual merchants.\n\nit means promotion offers are working well for loyalt cards. Lets check how good the repeate business from history data.","ee626606":"#### For LightGBM theory Understanding followed [Pushkar Mandot](http:\/\/https:\/\/medium.com\/@pushkarmandot\/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc).\n\n#### For implementation of LightGBM baseline model, took help from [SRK](http:\/\/https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-elo) kernel.\n","8819983a":"### 2.3.2 (b) Analysing installments field.","d300929c":"### Observations\n\n#### There is nearly equal distribution of target values based on installments upto 20 installments.\n#### 999 installment and their counts are bit strange.","957206c8":"Lets check there are any card holder which has only Unauthorised entry. or it is just that in history transactions there were\nno such authorised condition for transaction or loyalt score.","75a62561":"### Lets understand this graph bit more. \nLoyalt score or target is based on many parameters. i.e\n\n* Number of transactions ,\n* Purchase amount for each transactions ,\n* Category they buy,\n* Time period they activated and offers that time. etc,\n\nHere, loyalt score for bin range >1000 is different then usual. lets see what difference it speaks and decide weather this information or features are useful for prediction or not?","4305b446":"### 2.4 New Merchant Data Analysis","8b817c39":"### 1. Data Exploration","47be123e":"Wow! with very less memory usage, we have dataframe ready to inspect. Thanks to [Ashish Patel's kernel](http:\/\/www.kaggle.com\/ashishpatel26\/lightgbm-gbdt-rf-baysian-ridge-reg-lb-3-61).\nGlad can continue my work now with less memory usage now.","095011dd":"#### 2.3.2 (a) Analysing authorised_flag fields and their impact on business. ","0ecb983d":"Mercent can offer special discounts or promotional offers to the card holders based on number of transactions or based on amount spend.","cc540324":"### Observations: \n\n* Loyalt score shows normal distribution between -10 and 10.\n* Majority Loyalt score is between -5 and 5. \n* few data has <-30 values which are suspecious.\n\nHow they have calculated Loyalt Score or Target values, can have various parameters.\n\nHowever, Lets check given feature_1, feature_2 and feature_3 in train and test set.","f2e339c3":"### 2.2.2 Analysing Target values in train dataset","43341dc9":"Not getting much detail information, just that feature_1, feature_2 and feature_3 distribution pattern in test and train data is overall same.","f8b8205c":"### 2.2 Merchant Data Analysis","5c73d627":"####  2.3.1 Repeat Business","02d811e5":"We can clearly see here monthly repete business by observing multiple transactions for individual card holders. \nTarget value or Loyalt score is generally high here in 2017."}}