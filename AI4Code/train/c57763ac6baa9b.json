{"cell_type":{"904f7b64":"code","be57c096":"code","7ee857d3":"code","6693497f":"code","df0ce83d":"code","e0cb7a1e":"code","e7f4b5e7":"code","67a6bf3d":"code","05a9513a":"code","0b9ae0c0":"code","c78a9279":"code","11135e50":"code","4e8c2298":"code","27f8de08":"code","b7ac1d3a":"code","f46b7a2e":"code","9762e4bc":"code","e7fe0077":"code","1c7f9200":"code","c947c8d5":"code","b921b6b3":"code","ff3ee4c4":"code","b78efcb0":"code","f4d8717d":"code","689ca42e":"code","16af13de":"code","1bde60a3":"code","b5d74328":"code","ffe6790e":"code","77dd4974":"code","8792c6c8":"code","58f0c7b2":"code","cc0acc2a":"markdown","56177d4f":"markdown","892a599e":"markdown","b24873fc":"markdown","0940d61b":"markdown","ee0dc785":"markdown","4466083d":"markdown"},"source":{"904f7b64":"import os\nimport warnings\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import display\nfrom pandas.api.types import CategoricalDtype\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split","be57c096":"train_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\",index_col = \"Id\")\ntest_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\",index_col = \"Id\")","7ee857d3":"train_df.head()","6693497f":"train_df.isnull().sum()","df0ce83d":"\nsns.barplot(x = train_df[\"YrSold\"] , y = train_df[\"SalePrice\"])\n","e0cb7a1e":"sns.barplot(x = train_df[\"OverallQual\"] , y = train_df[\"SalePrice\"])","e7f4b5e7":"plt.figure(figsize=(18,10))\n\nsns.scatterplot(x = train_df[\"YearBuilt\"] , y = train_df[\"SalePrice\"])","67a6bf3d":"def make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","05a9513a":"X = train_df.copy()\nX = X.fillna(\"Missing\")\n\n\ny = X.pop(\"SalePrice\")\n\n\n\n# Label encoding for categoricals\nfor colname in X.select_dtypes(\"object\"):\n    X[colname], _ = X[colname].factorize()\n\n# All discrete features should now have integer dtypes (double-check this before using MI!)\ndiscrete_features = X.dtypes == int\nX.head()","0b9ae0c0":"mi_scores = make_mi_scores(X, y, discrete_features)\nmi_scores[:10]  # show a few features with their MI scores","c78a9279":"plt.figure(figsize=(18,10))\nsns.scatterplot(x = train_df[\"SalePrice\"], y = train_df[\"Neighborhood\"])","11135e50":"sns.regplot(x = train_df[\"GarageArea\"] , y = train_df[\"SalePrice\"])","4e8c2298":"mi_scores[len(mi_scores)-10:]","27f8de08":"sns.barplot(x = train_df[\"PoolQC\"],y = train_df[\"SalePrice\"])","b7ac1d3a":"sns.scatterplot(x = train_df[\"YrSold\"],y = train_df[\"SalePrice\"])","f46b7a2e":"sns.barplot(x = train_df[\"Condition2\"],y = train_df[\"SalePrice\"])","9762e4bc":"sns.barplot(x = train_df[\"MoSold\"],y = train_df[\"SalePrice\"])","e7fe0077":"t = train_df.copy()\nt = t.fillna(\"Missing\")\nsns.swarmplot(x = t[\"Alley\"],y = train_df[\"SalePrice\"])\n","1c7f9200":"X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)","c947c8d5":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","b921b6b3":"print(\"MAE Score: \")\nprint(score_dataset(X_train_full, X_valid_full, y_train, y_valid))","ff3ee4c4":"X_train_full.head()","b78efcb0":"train_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\",index_col = \"Id\")\ntest_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\",index_col = \"Id\")","f4d8717d":"def label_encode(df):\n    X = df.copy()\n    for name in df.select_dtypes(\"number\"):\n        X[name] = X[name].fillna(0)\n    for name in df.select_dtypes(\"category\"):\n        X[name] = X[name].fillna(\"None\")\n        \n    for colname in X.select_dtypes(\"object\"):\n        X[colname], _ = X[colname].factorize()\n        \n    \n    \n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n        \n    return X","689ca42e":"def pca_function(X):\n    X = (X - X.mean(axis=0)) \/ X.std(axis=0)\n\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca , X_pca , loadings","16af13de":"class CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  \n        self.cv_ = KFold(n_splits=5)\n\n   \n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) \/ len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","1bde60a3":"def create_features(train_df , test_df):\n    X = train_df.copy()\n    y = X.pop(\"SalePrice\")\n    \n    \n    X_test = test_df.copy()\n    \n    X = pd.concat([X, test_df])\n    \n    X = label_encode(X)\n   \n    pca , X_pca , loadings = pca_function(X)\n    X[\"PC1\"] = X_pca[\"PC1\"]\n    X[\"PC2\"] = X_pca[\"PC2\"]\n    X[\"PC5\"] = X_pca[\"PC5\"]\n    \n    X_test = X.loc[test_df.index, :]\n    X.drop(test_df.index, inplace=True)\n    \n    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n    \n    X = X.join(encoder.fit_transform(X, y, cols=[\"Neighborhood\"]))\n    X_test = X_test.join(encoder.transform(X_test))\n    \n    return X, X_test","b5d74328":"def score_dataset(X, y, model=XGBRegressor()):\n  \n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    log_y = np.log(y)\n    score = cross_val_score(\n        model, X, log_y, cv=5, scoring=\"neg_mean_squared_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score","ffe6790e":"#train_df, test_df = load_data()\n\nX_train , X_test = create_features(train_df,test_df)\n#print(X_train.head())\ny_train = train_df.loc[:, \"SalePrice\"]\n\nscore_dataset(X_train, y_train)","77dd4974":"X_train ,X_test = create_features(train_df , test_df)\ny_train = train_df.loc[:, \"SalePrice\"]\n\nxgb_params = dict(\n    max_depth=6,           # maximum depth of each tree - try 2 to 10\n    learning_rate=0.01,    # effect of each tree - try 0.0001 to 0.1\n    n_estimators=1000,     # number of trees (that is, boosting rounds) - try 1000 to 8000\n    min_child_weight=1,    # minimum number of houses in a leaf - try 1 to 10\n    colsample_bytree=0.7,  # fraction of features (columns) per tree - try 0.2 to 1.0\n    subsample=0.7,         # fraction of instances (rows) per tree - try 0.2 to 1.0\n    reg_alpha=0.5,         # L1 regularization (like LASSO) - try 0.0 to 10.0\n    reg_lambda=1.0,        # L2 regularization (like Ridge) - try 0.0 to 10.0\n    num_parallel_tree=1,   # set > 1 for boosted random forests\n    gpu_id = 0,\n)\n\nxgb = XGBRegressor(**xgb_params)\nscore_dataset(X_train, y_train, xgb)","8792c6c8":"import optuna\n\ndef objective(trial):\n    xgb_params = dict(\n        max_depth=trial.suggest_int(\"max_depth\", 2, 10),\n        learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n        n_estimators=trial.suggest_int(\"n_estimators\", 1000, 8000),\n        min_child_weight=trial.suggest_int(\"min_child_weight\", 1, 10),\n        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n        subsample=trial.suggest_float(\"subsample\", 0.2, 1.0),\n        reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-4, 1e2, log=True),\n        reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n    )\n    xgb = XGBRegressor(**xgb_params)\n    return score_dataset(X_train, y_train, xgb)\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=5)\nxgb_params = study.best_params","58f0c7b2":"X_train ,X_test = create_features(train_df , test_df)\ny_train = train_df.loc[:, \"SalePrice\"]\n\nxgb = XGBRegressor(**xgb_params)\n\nxgb.fit(X_train, np.log(y))\npredictions = np.exp(xgb.predict(X_test))\noutput = pd.DataFrame({'Id': X_test.index, 'SalePrice': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","cc0acc2a":"## Search for better features on the data ","56177d4f":"## Some Features seem to be ok even though it has low MI score lets evaluate our data\n","892a599e":"## Imports","b24873fc":"## House Price Predictions","0940d61b":"## Loading the Data","ee0dc785":"## EDA Step","4466083d":"## As we can see, MI scores are handy for feature selection\n## Now Let's look at the worst MI scored data, and see what we can do to increase their effectiveness  "}}