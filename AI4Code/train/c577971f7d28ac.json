{"cell_type":{"e3f615a1":"code","41dd8d6d":"code","49c73bf6":"code","aea7bff2":"code","aed78ead":"code","23d91985":"code","eda03564":"code","9516760b":"code","736a936e":"code","be870100":"code","e5ca64ed":"code","056418a6":"code","ed55c32a":"code","ff2b1e51":"code","e2107c92":"code","93660f8a":"code","6ab93b7c":"markdown","9e47e2d8":"markdown","c87c0d17":"markdown","0357a696":"markdown","736e06bd":"markdown","6091f331":"markdown","15ee2092":"markdown"},"source":{"e3f615a1":"# Sync Notebook with VS Code\n!git clone https:\/\/github.com\/sarthak-314\/toxic\nimport sys; sys.path.append('toxic')\n\nfrom src import *\nfrom src.tflow import *","41dd8d6d":"%%hyperparameters \n\n## Model Architecture ## \nmax_seq_len: 128 # Use between 64-196\nbatch_size: 16 # Can go upto 256, but I am using low batch size for regularization\n    \nbackbone: roberta-base # You can try other backbones from huggingface like albert-base-v2, roberta-large, etc.\nattention_dropout: 0.125 # default: 0.10 for roberta | 0 for albert-base-v2 \n\nhidden_dropout: 0.15 # default: 0.10\nhidden_layers: [256, 64, 16, 4] # Dense layers with mish activation\n\n## Model Compile ## \noptimizer:\n    _target_: AdamW # Only AdamW Implemented for now\n    weight_decay: 3e-5 \n    epsilon: 1e-6 \n    beta_1: 0.9\n    beta_2: 0.999\n    max_grad_norm: 10.0 # Roberta does not use max grad norm, but uses higher weight decay\n    use_swa: True\n    use_lookahead: False\n    # average_decay: 0.999\n    # dynamic_decay: True\n    \n    \n# LR Schedule \n# Use plot_first_epoch to visualize the LR schedule curve with different values \nwarmup_epochs: 1\nwarmup_power: 1.5\nlr_cosine:\n    max_lr: 16e-6\n    min_lr: 0\n    lr_gamma: 0.75\n    num_cycles: 2\n    step_gamma: 2\n        \n## Model Training ## \nmax_epochs: 20\ncheckpoints_per_epoch: 4 # Multiple checkpoints per epoch\nearly_stop_patience: 0.5\n\n# Loss Function # \nlow_agree_margin: 0.001 # 0.001 margin for 14k comments ~ distance of 14 comments\nhigh_agree_margin: 0.01 # 0.01 margin ~ distance of 140 comments\n\n## Data ## \nfold: 3\nrandom_state: 69420\nadd_special_tokens: False # Add Special tokens for the features\nadd_old_hard_pos: True # Augment the data by adding hard positives from 2016 competition data","49c73bf6":"# (Optional) Download hyperparameters for this run by clicking in the link below. \nfrom IPython.display import FileLink\nFileLink('experiment.yaml')","aea7bff2":"FEATURES = ['severe_toxic', 'identity_hate', 'threat', 'toxic', 'insult', 'obscene']\nSPECIAL_TOKENS = [\n    '[SEVERE_TOXIC]', '[IDENTITY_HATE]', '[THREAT]', '[TOXIC]', '[INSULT]', '[OBSCENE]', \n    '[SOFT]', # Soft labels for the features generated by a model trained on 2016 data\n    '[HARD]', # Human annotated hard labels \n]\n\nFEAT_TO_TOKEN = {feat: token for feat, token in zip(FEATURES, SPECIAL_TOKENS)}","aed78ead":"# bfloat16: False to disable mixed precision \n# jit_compile: True to enable JIT compliation (Use for larger training datasets)\n\n# !wandb login 'xxxxxxxx' # Your WandB API Code Here. Uncomment this line to use WandB\nSTRATEGY = tf_accelerator(bfloat16=True, jit_compile=False)\nset_seed(HP.random_state)\n\nwith STRATEGY.scope(): \n    backbone = TFAutoModel.from_pretrained(\n        HP.backbone, \n        attention_probs_dropout_prob=HP.attention_dropout, \n    )\ntokenizer = AutoTokenizer.from_pretrained(\n    HP.backbone, \n    additional_special_tokens=SPECIAL_TOKENS, \n)","23d91985":"backbone.config","eda03564":"def add_old_hard_positive(): \n    old = pd.read_csv('..\/input\/toxic-public-dataframes\/old_pseudo_label.csv')\n    old['y'] = old.loc[:, 'toxic': 'identity_hate'].sum(axis=1)\n    neg = old[old.y==0]\n    pos = pd.concat([old[old.identity_hate==1], old[old.severe_toxic==1]])\n    \n    dd = {'more_toxic': [], 'less_toxic': []}\n    for i in range(len(pos)-1): \n        dd['more_toxic'].append(pos.iloc[i].comment_text)\n        dd['less_toxic'].append(neg.iloc[i].comment_text)\n    df = pd.DataFrame(dd)\n    df['margin'] = HP.high_agree_margin\n    return df","9516760b":"def add_special_tokens_LT(row):\n    text = row.less_toxic \n    for feat in reversed(FEATURES): \n        if row[f'LT_{feat}'] > 0.5: \n            text = f'{FEAT_TO_TOKEN[feat]} ' + text\n    if (row.LT_toxic == -1) or (1 < row.LT_toxic < 0): \n        text = '[SOFT] ' + text\n    else: \n        text = '[HARD] ' + text\n    return text\n\ndef add_special_tokens_MT(row):\n    text = row.more_toxic \n    for feat in reversed(FEATURES): \n        if row[f'MT_{feat}'] > 0.5: \n            text = f'{FEAT_TO_TOKEN[feat]} ' + text\n    if (row.MT_toxic == -1) or (1 < row.MT_toxic < 0): \n        text = '[SOFT] ' + text\n    else: \n        text = '[HARD] ' + text\n    return text\n\ndef add_feature_values(df): \n    old = pd.read_csv('\/kaggle\/input\/toxic-public-dataframes\/old_pseudo_label.csv')\n    old_dict = old.set_index('comment_text').to_dict()\n    for feat in FEATURES: \n        df[f'MT_{feat}'] = df.more_toxic.map(old_dict[feat])\n        df[f'LT_{feat}'] = df.less_toxic.map(old_dict[feat])\n    return df","736a936e":"df = pd.read_csv('\/kaggle\/input\/toxic-public-dataframes\/valid.csv')\ndfc = pd.read_csv('\/kaggle\/input\/toxic-public-dataframes\/comments.csv')\ndf = add_feature_values(df)\n\ndef add_special_tokens(df): \n    df = df.fillna(-1)\n    if not HP.add_special_tokens: \n        return df\n    df.more_toxic = df.apply(add_special_tokens_MT, axis=1)\n    df.less_toxic = df.apply(add_special_tokens_LT, axis=1)\n    return df\n\ndef build_train(): \n    df['margin'] = df.agree.apply(lambda a: HP.low_agree_margin if a<1 else HP.high_agree_margin)\n    train = df[df.fold!=HP.fold]\n    if HP.add_old_hard_pos: \n        train = pd.concat([train, add_old_hard_positive()])\n    return add_special_tokens(train)\n\ndef build_valid(): \n    df['margin'] = df.agree.apply(lambda a: HP.low_agree_margin if a<1 else HP.high_agree_margin)\n    valid = df[df.fold==HP.fold]\n    return add_special_tokens(valid)\n\ntrain, valid = build_train(), build_valid()\ntrain\nvalid","be870100":"%%time\ndef tokenize_text(text): \n    return tokenizer(\n        text, \n        max_length=HP.max_seq_len, \n        padding='max_length', \n        truncation=True, \n    )\n\ndef convert_to_features(example_batch): \n    M_tokenized = tokenize_text(example_batch['more_toxic'])\n    L_tokenized = tokenize_text(example_batch['less_toxic'])\n    return {\n        'MT_ids': M_tokenized['input_ids'], \n        'MT_mask': M_tokenized['attention_mask'], \n        'LT_ids': L_tokenized['input_ids'], \n        'LT_mask': L_tokenized['attention_mask'], \n    }\n\ndef dataset_to_tfds(dataset):\n    dataset.set_format(type='numpy')\n    \n    model_inputs = {\n        'MT_ids': dataset['MT_ids'].astype(np.int32), \n        'MT_mask': dataset['MT_mask'].astype(np.int32), \n        'LT_ids': dataset['LT_ids'].astype(np.int32), \n        'LT_mask': dataset['LT_mask'].astype(np.int32), \n        'margin': dataset['margin'].astype(np.float32),\n    }\n    ds = tf.data.Dataset.from_tensor_slices(model_inputs)\n    return ds\n\ndef df_to_tfds(df, is_valid=False): \n    raw_dataset = datasets.Dataset.from_pandas(df[['less_toxic', 'more_toxic', 'margin']])\n    processed_dataset = raw_dataset.map(\n        convert_to_features, \n        batched=True, \n        batch_size=1024, \n        num_proc=4, \n        desc='Running tokenizer on the dataset', \n    )\n    ds = dataset_to_tfds(processed_dataset)\n    if not is_valid:\n        ds = ds.shuffle(len(processed_dataset), reshuffle_each_iteration=True).repeat()\n    ds = ds.batch(HP.batch_size)\n    if is_valid: \n        ds = ds.cache()\n    steps = len(processed_dataset)\/\/HP.batch_size + 1\n    return ds.prefetch(tf.data.AUTOTUNE), steps\n\ntrain_ds, train_steps = df_to_tfds(train, is_valid=False)\nvalid_ds, valid_steps = df_to_tfds(valid, is_valid=True)","e5ca64ed":"def margin_ranking_loss(MT_pred, LT_pred, margin): \n    MT_pred, LT_pred, margin = tf.cast(MT_pred, tf.float32), tf.cast(LT_pred, tf.float32), tf.cast(margin, tf.float32)\n    return tf.math.maximum(margin + (LT_pred-MT_pred), 0)\n\nclass ToxicModel(tf.keras.Model):     \n    @tf.function\n    def train_step(self, data): \n        with tf.GradientTape() as tape:\n            MT_pred = self((data['MT_ids'], data['MT_mask']), training=True)\n            LT_pred = self((data['LT_ids'], data['LT_mask']), training=True)\n            \n            loss = self.compiled_loss(MT_pred, LT_pred, regularization_losses=self.losses)\n            loss = tf.cast(loss, tf.float32)\n            loss += margin_ranking_loss(MT_pred, LT_pred, data['margin'])\n            \n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        self.compiled_metrics.update_state(MT_pred, LT_pred)\n        return {m.name: m.result() for m in self.metrics}\n    \n    @tf.function\n    def test_step(self, data):\n        MT_pred = self((data['MT_ids'], data['MT_mask']), training=False)\n        LT_pred = self((data['LT_ids'], data['LT_mask']), training=False)\n        self.compiled_loss(MT_pred, LT_pred, regularization_losses=self.losses)\n        self.compiled_metrics.update_state(MT_pred, LT_pred)\n        return {m.name: m.result() for m in self.metrics}\n    \ndef bert_initalizer(initializer_range = 0.02):\n    return tf.keras.initializers.TruncatedNormal(stddev=initializer_range)\n\ndef build_hidden_layer(hidden_layer_units, hidden_dropout): \n    layers = []\n    for units in hidden_layer_units: \n        layers.append(tf.keras.layers.Dropout(hidden_dropout))\n        layers.append(tf.keras.layers.Dense(\n            units, \n            activation=tfa.activations.mish, \n            kernel_initializer=bert_initalizer(backbone.config.initializer_range)\n        ))\n    return tf.keras.Sequential(layers, name='hidden_layer')\n    \n    \ndef build_model(backbone): \n    input_ids = tf.keras.Input((HP.max_seq_len,), dtype=tf.int32)\n    attention_mask = tf.keras.Input((HP.max_seq_len,), dtype=tf.int32)\n    \n    backbone_outputs = backbone(\n        input_ids=input_ids, \n        attention_mask=attention_mask, \n        return_dict=True,\n    )\n    \n    if 'pooler_output' in backbone_outputs: \n        x = backbone_outputs.pooler_output\n    else: \n        # TODO: Take average instead \n        x = tf.squeeze(backbone_outputs.last_hidden_state[:, -1:, :], axis=1)\n    hidden_layer = build_hidden_layer(HP.hidden_layers, HP.hidden_dropout)\n    x = hidden_layer(x)\n    \n    score_layer = tf.keras.Sequential([\n        tf.keras.layers.Dropout(HP.hidden_dropout), \n        tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=bert_initalizer(backbone.config.initializer_range)), \n    ], name='score')\n    \n    model = ToxicModel([input_ids, attention_mask],  outputs=score_layer(x))\n    model.trainable = True\n    return model\n\nwith STRATEGY.scope(): \n    model = build_model(backbone)","056418a6":"from src.tflow.factory import lr_scheduler_factory, plot_first_epoch\nHP.steps_per_execution = 1024 # Set None to debug\n\ndef loss_fn(y_more_toxic, y_less_toxic):\n    return tf.math.maximum(1e-4 + (y_less_toxic-y_more_toxic), 0)\n\ndef MT(MT_pred, LT_pred): \n    return tf.reduce_mean(MT_pred)\ndef LT(MT_pred, LT_pred): \n    return tf.reduce_mean(LT_pred)\n    \ndef accuracy(MT_y, LT_y): \n    correct = tf.math.reduce_sum(tf.where(MT_y>LT_y, 1, 0))\n    wrong = tf.math.reduce_sum(tf.where(LT_y>=MT_y, 1, 0))\n    return correct \/ (correct + wrong)\n\ndef model_compile(): \n    lr_scheduler = lr_scheduler_factory(HP.warmup_epochs, HP.warmup_power, HP.lr_cosine, train_steps+4)\n    optimizer = optimizer_factory(HP.optimizer, lr_scheduler)\n    \n    model.compile(\n        optimizer=optimizer, \n        loss=loss_fn, \n        metrics=[accuracy, MT, LT], \n        steps_per_execution=HP.steps_per_execution, \n        run_eagerly=HARDWARE == 'CPU', \n    )    \nwith STRATEGY.scope(): \n    model_compile()","ed55c32a":"accuracy_checkpoint = tf.keras.callbacks.ModelCheckpoint('checkpoint_acc.h5', monitor='val_accuracy', mode='max', save_weights_only=True, save_best_only=True, verbose=1)\nearly_stop = tf.keras.callbacks.EarlyStopping(patience=HP.early_stop_patience)\n\nhistory = model.fit(\n    train_ds, steps_per_epoch=train_steps\/\/HP.checkpoints_per_epoch+1, epochs=HP.max_epochs*HP.checkpoints_per_epoch, \n    validation_data=valid_ds, validation_steps=valid_steps-1, \n    callbacks=[accuracy_checkpoint], \n)","ff2b1e51":"# # Save best model to WandB\n# with STRATEGY.scope(): \n#     model.load_weights('checkpoint_acc.h5')\n#     model.save_weights('valid_only_robertab_val75xx_3rdDec.h5')\n#     wandb.save('valid_only_robertab_val75xx_3rdDec.h5')","e2107c92":"# def build_dummy_model(backbone):\n#     input_ids = tf.keras.Input((HP.max_seq_len,), dtype=tf.int32)\n#     attention_mask = tf.keras.Input((HP.max_seq_len,), dtype=tf.int32)\n#     backbone_outputs = backbone(\n#         input_ids=input_ids, \n#         attention_mask=attention_mask, \n#         return_dict=True,\n#     )\n#     if 'pooler_output' in backbone_outputs: \n#         x = backbone_outputs.pooler_output\n#     else: \n#         x = tf.squeeze(backbone_outputs.last_hidden_state[:, -1:, :], axis=1)\n#     score_layer = tf.keras.layers.Dense(1)\n#     model = tf.keras.Model([input_ids, attention_mask],  outputs=score_layer(x))\n#     return model\n\n# def add_wandb_backbones(wandb_weights):\n#     backbone_weights = []\n#     for run, weights in tqdm(wandb_weights): \n#         wandb.restore(weights, run)\n#         temp_model = build_dummy_model(backbone)\n#         backbone.save_weights(weights)\n#         backbone_weights.append(weights)\n#     return backbone_weights\n\n# def add_huggingface_backbones(huggingface_backbones): \n#     huggingface_backbone_weights = []\n#     for huggingface_model in huggingface_backbones: \n#         bb = TFAutoModel.from_pretrained(huggingface_model)\n#         backbone_weights = f\"{huggingface_model.replace('\/', '_')}.h5\"\n#         bb.save_weights(backbone_weights)\n#         huggingface_backbone_weights.append(backbone_weights)\n#     return huggingface_backbone_weights\n\n# backbone.save_weights('init_backbone_weights.h5')\n# model.get_layer('hidden_layer').save_weights('init_hidden_layer_weights.h5')\n\n# BACKBONE_WEIGHTS = ['init_backbone_weights.h5']\n# # WandB Backbone Weights\n# if HP.backbone == 'roberta-base': \n#     WANDB_WEIGHTS = [\n#         # LB: 0.805 | Trained on Old Only Data\n#         ('uncategorized\/runs\/35gso3iz', 'nolapval755_oldonlyv2_robertab_17thDec.h5'), \n          # Best LB: 0.831: Trained on old data with pseudo labels. \n#     ]\n#     BACKBONE_WEIGHTS += add_wandb_backbones(WANDB_WEIGHTS)\n\n# # Huggingface Backbones \n# if HP.backbone == 'roberta-base': \n#     HUGGINGFACE_BACKBONES = [\n#         'cardiffnlp\/twitter-roberta-base-hate', \n#         'cardiffnlp\/twitter-roberta-base-offensive', \n#     ]\n#     BACKBONE_WEIGHTS += add_huggingface_backbones(HUGGINGFACE_BACKBONES)","93660f8a":"# TRIALS = 100\n# max_max_accuracy = 0\n# for i in tqdm(range(TRIALS)): \n#     tf.keras.backend.clear_session()\n#     print('-'*50, f'RUN #{i} ', '-'*50)\n    \n#     # Load Weights\n#     with STRATEGY.scope(): \n#         backbone_weights = random.choice(BACKBONE_WEIGHTS)\n#         print('backbone_weights: ', red(backbone_weights))\n#         backbone.load_weights(backbone_weights)\n        \n#     # Build Model \n#     HP.max_seq_len = random.choice([64, 96, 128, 192, 256])\n#     HP.hidden_layers = random.choice([\n#         [256, 64, 16, 4],\n#         [256], [64], \n#     ])\n#     HP.hidden_dropout = random.uniform(0, 0.20)\n#     print('max_seq_len:', blue(HP.max_seq_len))\n#     print('hidden_layers:', blue(HP.hidden_layers))\n#     print('hidden_dropout:', blue(HP.hidden_dropout))\n#     with STRATEGY.scope(): model = build_model(backbone)\n    \n    \n#     # OPTIMIZER KWARGS \n#     HP.optimizer.weight_decay = random.choice([1e-4, 3e-4, 1e-4, 3e-5, 1e-5])\n#     HP.optimizer.epsilon = random.choice([3e-6, 1e-6, 3e-7, 1e-7, 3e-7, 1e-8])\n#     HP.optimizer.beta_1 = random.uniform(0.875, 0.925)\n#     HP.optimizer.beta_2 = random.choice([0.995, 0.999, 0.9995])\n#     HP.optimizer.max_grad_norm = random.choice([0.3, 1.0, 3, 10, 100])\n#     if 'roberta' in HP.backbone: \n#         HP.optimizer.max_grad_norm = random.choice([1.0, 3, 10, 30, 100, 1000])\n#         HP.optimizer.weight_decay = random.choice([1e-3, 3e-4, 1e-4, 3e-5])\n#     HP.optimizer.use_swa = random.choice([True, True, False])\n#     # HP.optimizer.moving_average = random.choice([0.99, 0.999])\n#     # HP.optimizer.dynamic_decay = random.choice([True, False])\n#     print('weight_decay:', blue(HP.optimizer.weight_decay))\n#     print('epsilon:', blue(HP.optimizer.epsilon))\n#     print('beta_1:', blue(HP.optimizer.beta_1))\n#     print('beta_2:', blue(HP.optimizer.beta_2))\n#     print('max_grad_norm:', blue(HP.optimizer.max_grad_norm))\n#     print('use_swa:', blue(HP.optimizer.use_swa))\n#     # print('moving_average:', blue(HP.optimizer.moving_average))\n#     # print('dynamic_decay:', blue(HP.optimizer.dynamic_decay))\n    \n#     # LR KWARGS \n#     HP.warmup_epochs = random.choice([0.0625, 0.125, 0.25, 0.5, 0.75, 1, 1.5, 2, 3])\n#     HP.warmup_power = random.choice([1, 1.25, 1.5, 2])\n#     HP.lr_cosine.max_lr = random.uniform(0, 8e-5)\n#     max_lr = HP.lr_cosine.max_lr\n#     HP.lr_cosine.min_lr = random.choice([0, max_lr\/10, max_lr\/3, max_lr])\n#     HP.lr_cosine.lr_gamma = random.uniform(0.5, 1)\n#     HP.lr_cosine.step_gamma = random.choice([2, 3])\n#     print('warmup_epochs:', blue(HP.warmup_epochs))\n#     print('warmup_power:', blue(HP.warmup_power))\n#     print('max_lr:', blue(HP.lr_cosine.max_lr))\n#     print('min_lr:', blue(HP.lr_cosine.min_lr))\n#     print('lr_gamma:', blue(HP.lr_cosine.lr_gamma))\n#     print('step_gamma:', blue(HP.lr_cosine.step_gamma))\n    \n#     # Loss Function\n#     HP.high_agree_margin = random.choice([0.1, 0.01, 0.001])\n#     HP.low_agree_margin = random.choice([0.05, 0.01, 0.001, 0.0001])\n#     HP.low_agree_margin = min(HP.low_agree_margin, HP.high_agree_margin)\n#     print('high_agree_margin:', blue(HP.high_agree_margin))\n#     print('low_agree_margin:', blue(HP.low_agree_margin))\n#     # HP.steps_per_execution=None\n#     with STRATEGY.scope(): model_compile()\n    \n#     # Model Training \n#     HP.checkpoints_per_epoch = random.randint(1, 3)\n#     HP.max_epochs = 100\n#     HP.batch_size = random.choice([8, 16, 32, 64])\n#     print('checkpoints_per_epoch:', blue(HP.checkpoints_per_epoch))\n#     print('batch_size:', blue(HP.batch_size))\n    \n    \n#     # Data HParams\n#     HP.add_old_hard_pos = random.choice([True, False])\n#     HP.add_test_hard_pos = random.choice([True, False])\n#     HP.add_special_tokens = random.choice([True, False, False])\n#     print('add_old_hard_pos:', blue(HP.add_old_hard_pos))\n#     print('add_test_hard_pos:', blue(HP.add_test_hard_pos))\n#     print('add special tokens: ', red(HP.add_special_tokens))\n    \n#     train, valid = build_train(), build_valid()\n#     train_ds, train_steps = df_to_tfds(train, is_valid=False)\n#     valid_ds, valid_steps = df_to_tfds(valid, is_valid=True)\n    \n#     accuracy_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n#         f'checkpoint_acc_{i}.h5', monitor='val_accuracy', mode='max', save_weights_only=True, save_best_only=True, verbose=1, \n#     )\n#     history = model.fit(\n#         train_ds, steps_per_epoch=train_steps\/\/HP.checkpoints_per_epoch, epochs=HP.max_epochs*HP.checkpoints_per_epoch, \n#         validation_data=valid_ds, validation_steps=valid_steps-1,\n#         callbacks = [\n#             tf.keras.callbacks.EarlyStopping(patience=5*HP.checkpoints_per_epoch), accuracy_checkpoint\n#         ]\n#     )\n#     max_acc = max(history.history['val_accuracy'])\n#     print('Maximum accuracy for the run: ', max_acc)\n#     if max_acc < max_max_accuracy: \n#         max_max_accuracy = max_acc\n#         print(f'NEW BEST ACCURACY ({i}th run): ', max_max_accuracy)","6ab93b7c":"# Toxic: Valid Bi-Encoder Train\n\nTrain Bi-Encoder on Annotated data from validation_data.csv\n\n---\n### <a href='#hyperparameters'> Hyperparameters <\/a> | <a href='#training'> Training <\/a>","9e47e2d8":"## Data Factory\n---\n### <a href='#training'> Training <\/a> | <a href='#hyperparameters'> Hyperparameters <\/a> \n<a name='data-factory'>","c87c0d17":"### Weights Loader","0357a696":"### Hyperparameter Optimization\n---","736e06bd":"#### GitHub Repo: https:\/\/github.com\/sarthak-314\/toxic\n\nThe GitHub Repo contains all the boilerplate reusable code. I use it for all the notebooks in this competition. Some of it's features: \n- __Auto Imports__: Imports all the useful libraries. No need for manual imports like import pandas as pd, import tensorflow as tf. \n- __Sepration of Hyperparameters and Logic__: Seperates hyperparameters from the core logic. You can define all the hyperparameters in a yaml fashion. This makes it easy to run multiple experiments without changing the code. You can also download the hyperparameter file easily.  ","6091f331":"### Training\n---\n#### <a href='#hyperparameters'> Hyperparameters <\/a> | <a href='#dataframes'> Dataframes <\/a> | <a href='#data-factory'> Data Factory <\/a> \n\n<a name='training'>","15ee2092":"## Hyperparameters\n---\n### <a href='#training'> Training <\/a> | <a href='#data-factory'> Data Factory <\/a> \n\n<a name='hyperparameters'>"}}