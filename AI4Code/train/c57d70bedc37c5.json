{"cell_type":{"fd7b534c":"code","0d895a5a":"code","9112a9bb":"code","c66ffc14":"code","d2d45af7":"code","161a6c9a":"code","14e2b842":"code","66994db2":"code","e7c7c85a":"code","b3b41585":"code","bc138cff":"code","8b12ba86":"code","26b84370":"code","f3e99d3d":"code","c64b7209":"code","650f3073":"code","52e2eec4":"code","18949509":"code","bf6c5f35":"code","6adbe1c1":"code","c6f022f0":"code","dbaf1de9":"code","c25a3037":"code","80778640":"code","88f27093":"code","94f6229c":"code","4af376bb":"code","dea4cc8b":"code","268b5805":"code","8d6e1f58":"code","075f5186":"code","afd6cce9":"code","a06912c5":"code","777d92a9":"code","8686f1d1":"code","bb46044c":"markdown","bda32f29":"markdown","dfad18ec":"markdown","375bb9ad":"markdown","cb168dfe":"markdown","89e4b9d1":"markdown","cb639370":"markdown","da9d7cbe":"markdown","1750f35e":"markdown","57f5ff85":"markdown","f4cf8548":"markdown","39424c9f":"markdown","031d1143":"markdown","1c9231ff":"markdown","d23a5253":"markdown","2ae5a981":"markdown","644b544e":"markdown","26c57b13":"markdown","4f4da85d":"markdown","335f7544":"markdown","ea47c1a5":"markdown","b625c34a":"markdown","929569a2":"markdown","77c0663f":"markdown","d7198315":"markdown","34986312":"markdown","72d54787":"markdown","79e8b105":"markdown","c6c1ba39":"markdown","7b142491":"markdown","28427576":"markdown","f5968e06":"markdown","1fda4491":"markdown","1e44abb0":"markdown","c1217e6f":"markdown","96c90134":"markdown","ec30564d":"markdown","d12bda6d":"markdown","339cbb88":"markdown","ce2489c7":"markdown","3b61164b":"markdown"},"source":{"fd7b534c":"# for performing mathematical operations\nimport numpy as np \n\n# for data processing, CSV file I\/O \nimport pandas as pd \n\n# for plotting and visualozing data\nimport matplotlib.pyplot as plt \nimport seaborn as sns","0d895a5a":"# read the data from the excel file into a dataframe\ndataset = pd.read_excel('..\/input\/covid19\/dataset.xlsx', index_col=0)","9112a9bb":"# checking first thirty rows of our dataset\npd.set_option(\"display.max_rows\",500)\npd.set_option(\"display.max_columns\",500)\ndataset.head(30)","c66ffc14":"# checking the shape of my dataset\ndataset.shape","d2d45af7":"# extracting information from the dataset for the predictor and target variables\ndataset.info()","161a6c9a":"# Understanding the fields of the dataset on the basis of statistical variables\ndataset.describe(include=\"all\").T","14e2b842":"# finding out the number of positive and negative SARS-Cov-2 Cases\nprint(\"\\nNumber of Positive and Negative Cases of SARS-COV-2\")\ndataset['SARS-Cov-2 exam result'].value_counts()","66994db2":"# dropping the selected columns \ndataset.drop(columns=['Patient addmited to regular ward (1=yes, 0=no)',\n                      'Patient addmited to semi-intensive unit (1=yes, 0=no)',\n                      'Patient addmited to intensive care unit (1=yes, 0=no)'], inplace=True)","e7c7c85a":"# looking for null values\ntotal_null_values = dataset.isnull().sum().sort_values(ascending=False) \nnot_null_values = dataset.notnull().sum().sort_values(ascending=False) \nnull_values_percentage = (dataset.isnull().sum()\/dataset.notnull().count().sort_values(ascending=False)) * 100\n\n# concating the calculated values with the data frame of null values\ndataset_missing_values = pd.concat({'Null': total_null_values, 'Not Null': not_null_values, 'Percentage': null_values_percentage}, axis=1)\n\n# view the newly formed dataframe\ndataset_missing_values","b3b41585":"sns.set(style=\"whitegrid\")\n\n# initialize the matplotlib figure\nfig, axs = plt.subplots(figsize=(20,8))\n\n# plot the Total Missing Values\nsns.set_color_codes(\"muted\")\nsns.barplot(x=dataset_missing_values.index, y=\"Percentage\", data=dataset_missing_values, color=\"g\")\n\n# customizing Bar Graph\nplt.xticks(rotation='90')\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Missing Data in our Dataset', fontsize=20)","bc138cff":"# finding those columns that completely doesn't have any values\ndataset_missing_values[dataset_missing_values['Percentage']==100]","8b12ba86":"# finding those columns that doesn't have any values more than 5 values\ndataset_missing_values[dataset_missing_values['Not Null'] <= 6]","26b84370":"# dropping the selected columns \ndataset.drop(columns=['Mycoplasma pneumoniae','Urine - Sugar','Prothrombin time (PT), Activity','D-Dimer','Fio2 (venous blood gas analysis)','Urine - Nitrite','Vitamin B12'], inplace=True)","f3e99d3d":"# replace NaNs by 0\ndataset = dataset.fillna(0)","c64b7209":"dataset.replace('not_detected', 0, inplace=True)\ndataset.replace('detected', 0, inplace=True)\ndataset.replace('absent', 0, inplace=True)\ndataset.replace('present', 1, inplace=True)\ndataset.replace('negative', 0, inplace=True)\ndataset.replace('positive', 1, inplace=True)","650f3073":"# Our Dataset\ndataset","52e2eec4":"# visualize the relationship between the features and the target using scatterplots\nfig, axs = plt.subplots(3, 3, figsize=(20, 20))\ndataset.plot(kind='scatter', x='SARS-Cov-2 exam result', y='Hemoglobin',ax=axs[0,0], c='red')\ndataset.plot(kind='scatter',  x='SARS-Cov-2 exam result', y='Hematocrit', ax=axs[0,1], c='green')\ndataset.plot(kind='scatter', x='SARS-Cov-2 exam result', y='Platelets',ax=axs[0,2], c='blue')\ndataset.plot(kind='scatter',  x='SARS-Cov-2 exam result', y='Eosinophils', ax=axs[1,0], c='orange')\ndataset.plot(kind='scatter', x='SARS-Cov-2 exam result', y='Red blood Cells',ax=axs[1,1], c='purple')\ndataset.plot(kind='scatter',  x='SARS-Cov-2 exam result', y='Lymphocytes', ax=axs[1,2], c='pink')\ndataset.plot(kind='scatter', x='SARS-Cov-2 exam result', y='Leukocytes',ax=axs[2,0], c='blue')\ndataset.plot(kind='scatter',  x='SARS-Cov-2 exam result', y='Basophils', ax=axs[2,1], c='red')\ndataset.plot(kind='scatter',  x='SARS-Cov-2 exam result', y='Monocytes', ax=axs[2,2], c='green')","18949509":"# visualize the relationship between the data points using heatmap\ncorr_matrix = abs(dataset.corr())\n\n# correlation with target variable\ncorr_target = corr_matrix[\"SARS-Cov-2 exam result\"]\n\n# selecting highly correlated features\nrelevant_features = [\"Platelets\",\"Leukocytes\",\"Eosinophils\",\"Monocytes\",\"Hemoglobin\",\"Segmented\",\"ctO2 (arterial blood gas analysis)\",\"pCO2 (arterial blood gas analysis)\",\"HCO3 (venous blood gas analysis)\"]\n\n# plotting the heatmap\nfig, axs = plt.subplots(figsize=(18, 10))\nsns.heatmap(abs(dataset[relevant_features].corr()), yticklabels=relevant_features, xticklabels=relevant_features, vmin = 0.0, square=True, annot=True, vmax=1.0, cmap='OrRd')","bf6c5f35":"# visualize positive cases vs negative cases\ndataset_negative = dataset['SARS-Cov-2 exam result'] == 0\ndataset_positive = dataset['SARS-Cov-2 exam result'] == 1\n\n# data to plot\nlabels = 'Positive Cases', 'Negative Cases'\nsizes = [dataset_positive.sum(), dataset_negative.sum()]\ncolors = ['lightcoral', 'lightskyblue']\n# explode 1st slice\nexplode = (0.1, 0) \n\nfig, axs = plt.subplots(figsize=(14, 7))\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\nautopct='%1.1f%%', shadow=True, startangle=140)\n\nplt.axis('equal')\nplt.show()","6adbe1c1":"# import the KNeighborsClassifier module\nfrom sklearn.neighbors import KNeighborsClassifier","c6f022f0":"# instantiating KNeighborsClassifier\nknn = KNeighborsClassifier()","dbaf1de9":"# for splitting data into training and testing data\nfrom sklearn.model_selection import train_test_split","c25a3037":"# defining target variables \ntarget = dataset['SARS-Cov-2 exam result']\n\n# defining predictor variables \nfeatures = dataset.select_dtypes(exclude=[object])\n\n# assigning the splitting of data into respective variables\nX_train,X_test,y_train,y_test = train_test_split(features, target, test_size=0.3, random_state=42, stratify = target)","80778640":"print(\"Number of samples in train set: %d\" % y_train.shape)\nprint(\"Number of positive samples in train set: %d\" % (y_train == 1).sum(axis=0))\nprint(\"Number of negative samples in train set: %d\" % (y_train == 0).sum(axis=0))\nprint()\nprint(\"Number of samples in test set: %d\" % y_test.shape)\nprint(\"Number of positive samples in test set: %d\" % (y_test == 1).sum(axis=0))\nprint(\"Number of negative samples in test set: %d\" % (y_test == 0).sum(axis=0))","88f27093":"# to display the HTML representation of an object.\nfrom IPython.display import display_html\n\nX_train_data = X_train.describe().style.set_table_attributes(\"style='display:inline'\").set_caption('Summary of Training Data')\nX_test_data = X_test.describe().style.set_table_attributes(\"style='display:inline'\").set_caption('Summary of Testing Data')\n\n# to display the summary of both training and testing data, side by side for comparison \ndisplay_html(X_train_data._repr_html_(), raw = True)\ndisplay_html(X_test_data._repr_html_(), raw = True)","94f6229c":"# for exhaustive search over specified parameter values for an estimator\nfrom sklearn.model_selection import GridSearchCV","4af376bb":"# assigning the dictionary of variables whose optimium value is to be retrieved\nparam_grid = {'n_neighbors' : np.arange(1,50)}","dea4cc8b":"# performing Grid Search CV on knn-model, using 5-cross folds for validation of each criteria\nknn_cv = GridSearchCV(knn, param_grid, cv=5)","268b5805":"# training the model with the training data and best parameter\nknn_cv.fit(X_train,y_train)","8d6e1f58":"# finding out the best parameter chosen to train the model\nprint(\"The best paramter we have is: {}\" .format(knn_cv.best_params_))\n\n# finding out the best score the chosen parameter achieved\nprint(\"The best score we have achieved is: {}\" .format(knn_cv.best_score_))","075f5186":"# predicting the values using the testing data set\ny_pred = knn_cv.predict(X_test)","afd6cce9":"# the score() method allows us to calculate the mean accuracy for the test data\nprint(\"The score accuracy for training data is: {}\" .format(knn_cv.score(X_train,y_train)))\nprint(\"The score accuracy for testing data is: {}\" .format(knn_cv.score(X_test,y_test)))","a06912c5":"# for performance metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix","777d92a9":"# call the classification_report and print the report\nprint(classification_report(y_test, y_pred))","8686f1d1":"# call the confusion_matrix and print the matrix\nprint(confusion_matrix(y_test, y_pred))","bb46044c":"## Model Performance","bda32f29":"Fortunately, we have dropped the values of the selected unncessary columns from our dataset.\n\n### Dealing with the Case of Missing Values\nThe dataset with which we are about to train our model has many missing values due to unaccountable reasons. There are many ways which are used to deal with such missing values. Let's investigate them.","dfad18ec":"## Hyperparameter Tuning - Adjust important parameters using Grid Search CV\nParameters that needs to be specified before fitting (training) a model are called Hyperparameters, like *n_neighbors* in our case. So, the process of choosing the optimal hyperparameters for the learning algorithm is called Hyperparameter Tuning.\n\nWe will be using the *Grid Search Cross Validation* technique to choose the hyperparameter (n_neighbors) that would perform the best, for our model.\n\n**What is Grid Search Cross Validation technique?**\n\nAs explained by Wikipedia:\n\n> The traditional way of performing hyperparameter optimization has been grid search, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set or evaluation on a held-out validation set.\nSince the parameter space of a machine learner may include real-valued or unbounded value spaces for certain parameters, manually set bounds and discretization may be necessary before applying grid search.","375bb9ad":"# K-Nearest Neighbors Classifier for diagnosis of COVID-19","cb168dfe":"### Importing Data using Pandas Library","89e4b9d1":"The n_neighbor is set as **7** and the accuracy level is **99.5696%**","cb639370":"### Exploring X_train, X_test, y_train, y_test\n","da9d7cbe":"### Exploration of Data\nWe have loaded our required dataset, now we will see it's first five rows to check how our data looks.","1750f35e":"### Best Parameter and Best Score\nLet's look for the value of the required parameter, chosen to be the most efficient for the model. Along with the score that it achieved.","57f5ff85":"### Importing KNeighborsClassifier using Sklearn Library\nNow we will use K-Nearest Neighbors Classification to predict a new record on the basis of this data. ","f4cf8548":"## Splitting Training and Testing Data\nIn order to evaluate our model later for perfomance metrics or factors, we will split our data set into two groups.\n- Training Data, consisting of 70 percent of data on which we will train our model\n- Testing Data, consisting of remaing 30 percent of data on which we will perform evaluation","39424c9f":"## Summary","031d1143":"### Defining the Parameter Grid","1c9231ff":"### Importing classification_report and confusion_matrix using Sklearn Library\nWe will use classification_report and confusion_matrix to view the performance factors of our model.","d23a5253":"In classification problems, *accuracy* is the commonly used metric to evaluate the performance of the model, which means the fraction of the correct predictions.","2ae5a981":"### Cleaning Data - Dropping Unnecessary Columns\n\nThe data have 3 columns: \n- Patient addmited to regular ward (1=yes, 0=no)\"\n- Patient addmited to semi-intensive unit (1=yes, 0=no)\n- Patient addmited to intensive care unit (1=yes, 0=no)\n\nThe values of these columns would contribute nothing to our analysis. To handle this, we will drop the values of these columns from the dataset.","644b544e":"Although the array is printed without headings, but you can see that the majority of the predictions fall on the diagonal line of the matrix (which are correct predictions).\n\nFinally, we have trained our model and it's successfully predicting correct values with maximum accuracy. ","26c57b13":"This metric quantifies the fraction of the correct predictions. Thus, the accuracy of the model for training is **99.59%** and to predict the values is **99.70%**.","4f4da85d":"## Data Preparation\n\nIn this section, required dataset is imported, explored and cleaned to make it available in the right format for the implementation of K Nearest Neighbors Classification. \n\nData Analysis techniques used in this section includes: \n- Importing data set using Pandas \n- Exploring data to find features and target \n- Handling missing or corrupted values in the data\n- Visualizing data using Matplotlib to explore relationships\n\n*You can skip this section if you want to play with data yourself.*\n ","335f7544":"## Goal for this Notebook\n\nThis notebook aims to develop a model that would predict confirmed **COVID-19 cases** among suspected cases. This model would be trained using the provided data set containing the results of laboratory tests, using the **K Nearest Neighbors Algorithm** and the optimized parameters.","ea47c1a5":"### Instantiate the model\nNow we will create a knn-model classifier for making predictions.","b625c34a":"## Implementing K-Nearest Neighbors Algorithm\n","929569a2":"### Investigating Data for Predictors and Target\nIn the dataset provided, we have extracted the information regarding the columns and rows. Rows are refered as *observations*. Each column in this data set tells us something about each of our observations, like Hematocrit, Hemoglobin, or Platelets. These columns are called *Features* or *Predictor Variables* of our dataset. Column \"SARS-Cov-2 exam result\" classifies our dataset and predicts whether or not a person is a COVID-19 patient, and is considered as *Target Variable*. \n\n**Target** \n- SARS-Cov-2 exam result\n\nAlso, we have been provided with **5644 observations**, ranged from 0 to 5643. ","77c0663f":"### Visualize the relationship between the features and the target variable\n\nWe will visualize this data on plots using target variable as x-axis and predictor variables as y-axis for further investigation and to explore the relationship between them.","d7198315":"In this notebook, first, we examined the data to understand it and extracted information required to know the *features* and *target*. Then, data cleaning is performed to transform the data in the right and useful format. After that, we have visualized the data to explore the relationship between the data points. We have splitted our data into testing and training data sets, as well. Later, we have intantiated the KNeighbors classifier and performed hyperparameter tuning to figure out the best and optimum parameter for training our model. Subsequently, we have evaluated our model predictions using different performance metrics and achieved maximum accuracy.","34986312":"### Importing GridSearchCV using Sklearn Library\nSince, we are using the grid search cross validation technique for hyperparameter tuning, we will first import it from the Sklearn Library.","72d54787":"#### Data Handling\n\n- Importing Data with Pandas\n- Cleaning Data\n- Exploring Data through Visualizations with Matplotlib\n\n#### Supervised Machine Learning Technique\n- K Nearest Neighbors Classification \n    \n#### Hyperparameter Tuning\n- Grid Search Cross Validation\n    \n#### Model Performance\n- Score \n- Classification Report\n- Confusion Matrix\n\n#### Required Packages\n- NumPy\n- Pandas\n- Scikit Learn\n- Matplotlib\n- Seaborn\n\n#### Dataset\nThis dataset contains anonymized data from patients seen at the Hospital Israelita Albert Einstein, at S\u00e3o Paulo, Brazil, and who had samples collected to perform the SARS-CoV-2 RT-PCR and additional laboratory tests during a visit to the hospital.\n<br \/>\nReference : *einsteindata4u\/covid19?*","79e8b105":"## Testing the Model\nApply the model. For supervised algorithms, this is predict.","c6c1ba39":"### Dataset Details\n\nWe will be using the CSV format file called **COVID-19_Dataset** to train our model. The CSV file contains rows (observations) describing various parameters and factors defined with the help of columns (features). The dataset contains more than 5K  observations. ","7b142491":"At this point, we have a classifier and paramerter_grid, so we can perform the Grid Search Cross Validation.","28427576":"### Assignment of X_train, X_test, y_train, y_test","f5968e06":"#### Let's replace all our 'NaN' values with 0.","1fda4491":"By this we know that, we have **5086** Negative yet, **558** Postive cases.","1e44abb0":"### Training the Classifier\nThe idea is to tune the model's hyperparameter on the training set and then evaluate later it's performance on the hold-out set. So, using the training set that we have obtained before, that is, X_train and y_train, we will now fit the model.","c1217e6f":"### Importing train_test_split using Sklearn Library\nWe will use train_test_split to split our data set into training and testing data.","96c90134":"**What is K-Nearest Neighbor Algorithm?**\n\nAs explained by Wikipedia:\n\n> In pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression: \n- In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. \n- In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.\n\nk-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until function evaluation.","ec30564d":"Now, this dataset is in the most right format for our targeted algorithm. Although, there may be more methodologies to make it more manageable but let's continue with this.","d12bda6d":"To begin with, let's make all the necessary imports to start working.","339cbb88":"We have **110** columns and **5644** rows in our dataset.","ce2489c7":"#### Let's replace all our affirmative\/positive values to 1 and all negative values to 0","3b61164b":"What can be percieved from this is, we have **5** columns with 100% null values. Also, we have **3** more columns that contains only 1 or upto 6 values. There is not much that they contribute to our analysis, so let's eliminate them."}}