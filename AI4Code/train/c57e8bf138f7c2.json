{"cell_type":{"f28513dd":"code","8b8bb5a2":"code","9339bfd2":"code","c426fe89":"code","0b7fafdd":"code","18f57680":"code","b5d1b886":"code","1f504322":"code","80c58e76":"code","be45a2fb":"code","2e612ab4":"code","e3560cef":"code","498b4dd7":"code","49b9b4d4":"code","75ae6627":"code","2b4c11c5":"code","9fab07af":"code","0d825641":"code","09a0e7a4":"code","a26d915e":"code","2bbacc9b":"code","52656e3e":"code","e2024904":"code","0866e729":"code","7d9cae7e":"code","2d32caf8":"markdown","96b1eb13":"markdown","bff99b1a":"markdown","0e2618a2":"markdown","76ee344e":"markdown","b85a1c7e":"markdown","c512d566":"markdown","56b89c9f":"markdown","2c11274d":"markdown","83307482":"markdown","b3e3232c":"markdown","0ec91632":"markdown","5a9b479b":"markdown","d0569379":"markdown","ae37b80e":"markdown","6fbc1f9a":"markdown","05169547":"markdown","5c0e46d4":"markdown"},"source":{"f28513dd":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport math\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import *\nfrom keras_preprocessing.image import ImageDataGenerator\n\nimport zipfile \n\nimport cv2\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom mlxtend.plotting import plot_confusion_matrix\n\nfrom keras import models\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.utils import to_categorical\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model","8b8bb5a2":"path = '\/kaggle\/input\/challenges-in-representation-learning-facial-expression-recognition-challenge\/'\nos.listdir(path)","9339bfd2":"data = pd.read_csv(path+'icml_face_data.csv')\ndata.columns = ['emotion', 'Usage', 'pixels']      # Added by rb\ntrain = pd.read_csv(path+'train.csv')\ntest = pd.read_csv(path+'test.csv')","c426fe89":"data.head()","0b7fafdd":"train.head()","18f57680":"test.head()","b5d1b886":"data['Usage'].value_counts()","1f504322":"def prepare_data(data):\n    image_array = np.zeros(shape=(len(data), 48, 48, 1))\n    image_label = np.array(list(map(int, data['emotion'])))\n\n    for i, row in enumerate(data.index):\n        image = np.fromstring(data.loc[row, 'pixels'], dtype=int, sep=' ')\n        image = np.reshape(image, (48, 48)) \n        image_array[i, :, :, 0] = image \/ 255\n\n    return image_array, image_label\n\ndef vis_training(hlist, start=1):\n    \n    loss = np.concatenate([h.history['loss'] for h in hlist])\n    val_loss = np.concatenate([h.history['val_loss'] for h in hlist])\n    acc = np.concatenate([h.history['accuracy'] for h in hlist])\n    val_acc = np.concatenate([h.history['val_accuracy'] for h in hlist])\n    \n    epoch_range = range(1,len(loss)+1)\n\n    plt.figure(figsize=[12,6])\n    plt.subplot(1,2,1)\n    plt.plot(epoch_range[start-1:], loss[start-1:], label='Training Loss')\n    plt.plot(epoch_range[start-1:], val_loss[start-1:], label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.legend()\n\n    plt.subplot(1,2,2)\n    plt.plot(epoch_range[start-1:], acc[start-1:], label='Training Accuracy')\n    plt.plot(epoch_range[start-1:], val_acc[start-1:], label='Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend()\n\n    plt.show()","80c58e76":"emotions = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}","be45a2fb":"full_train_images, full_train_labels = prepare_data(data[data['Usage']=='Training'])\ntest_images, test_labels = prepare_data(data[data['Usage']!='Training'])\n\nprint(full_train_images.shape)\nprint(full_train_labels.shape)\nprint(test_images.shape)\nprint(test_labels.shape)","2e612ab4":"train_images, valid_images, train_labels, valid_labels =\\\n    train_test_split(full_train_images, full_train_labels, test_size=0.2, random_state=1)\n\nprint(train_images.shape)\nprint(valid_images.shape)\nprint(train_labels.shape)\nprint(valid_labels.shape)","e3560cef":"N_train = train_labels.shape[0]\n\nsel = np.random.choice(range(N_train), replace=False, size=16)\n\nX_sel = train_images[sel, :, :, :]\ny_sel = train_labels[sel]\n\nplt.figure(figsize=[12,12])\nfor i in range(16):\n    plt.subplot(4,4,i+1)\n    plt.imshow(X_sel[i,:,:,0], cmap='binary_r')\n    plt.title(emotions[y_sel[i]])\n    plt.axis('off')\nplt.show()","498b4dd7":"%%time \n\ncnn = Sequential()\n\ncnn.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(48,48,1)))\ncnn.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\ncnn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\ncnn.add(Dropout(0.25))\ncnn.add(BatchNormalization())\n\ncnn.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))\ncnn.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))\ncnn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\ncnn.add(Dropout(0.25))\ncnn.add(BatchNormalization())\n\ncnn.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same'))\ncnn.add(Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same'))\ncnn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\ncnn.add(Dropout(0.5))\ncnn.add(BatchNormalization())\n\ncnn.add(Flatten())\n\ncnn.add(Dense(512, activation='relu'))\ncnn.add(Dropout(0.5))\ncnn.add(Dense(512, activation='relu'))\ncnn.add(Dropout(0.5))\ncnn.add(Dense(512, activation='relu'))\ncnn.add(Dropout(0.5))\n\ncnn.add(Dense(7, activation='softmax'))\n\ncnn.summary()","49b9b4d4":"%%time \n\nopt = keras.optimizers.Adam(lr=0.001)\ncnn.compile(loss='sparse_categorical_crossentropy',\n                  optimizer=opt, metrics=['accuracy'])","75ae6627":"%%time \n\nh1 = cnn.fit(train_images, train_labels, batch_size=256, epochs=30, verbose=1, \n                   validation_data =(valid_images, valid_labels)) ","2b4c11c5":"vis_training([h1])","9fab07af":"%%time \nkeras.backend.set_value(cnn.optimizer.learning_rate, 0.0001)\n\nh2 = cnn.fit(train_images, train_labels, batch_size=256, epochs=30, verbose=1, \n                   validation_data =(valid_images, valid_labels)) ","0d825641":"vis_training([h1, h2])","09a0e7a4":"cnn.save('first_model_57.h5')","a26d915e":"test_prob = cnn.predict(test_images)\ntest_pred = np.argmax(test_prob, axis=1)\ntest_accuracy = np.mean(test_pred == test_labels)\n\nprint(test_accuracy)","2bbacc9b":"conf_mat = confusion_matrix(test_labels, test_pred)\n\npd.DataFrame(conf_mat, columns=emotions.values(), index=emotions.values())","52656e3e":"fig, ax = plot_confusion_matrix(conf_mat=conf_mat,\n                                show_normed=True,\n                                show_absolute=False,\n                                class_names=emotions.values(),\n                                figsize=(8, 8))\nfig.show()","e2024904":"print(classification_report(test_labels, test_pred, target_names=emotions.values()))","0866e729":"class GradCAM:\n    def __init__(self, model, classIdx, layerName=None):\n        self.model = model\n        self.classIdx = classIdx\n        self.layerName = layerName\n        if self.layerName is None:\n            self.layerName = self.find_target_layer()\n            \n    def find_target_layer(self):\n        for layer in reversed(self.model.layers):\n            if len(layer.output_shape) == 4:\n                return layer.name\n        raise ValueError(\"Could not find 4D layer. Cannot apply GradCAM.\")\n        \n    def compute_heatmap(self, image, eps=1e-8):\n        gradModel = Model(\n            inputs=[self.model.inputs],\n            outputs=[self.model.get_layer(self.layerName).output,self.model.output]\n       )\n           \n        with tf.GradientTape() as tape:\n            inputs = tf.cast(image, tf.float32)\n            (convOutputs, predictions) = gradModel(inputs)\n            loss = predictions[:, self.classIdx]\n            grads = tape.gradient(loss, convOutputs)\n\n            castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n            castGrads = tf.cast(grads > 0, \"float32\")\n            guidedGrads = castConvOutputs * castGrads * grads\n            convOutputs = convOutputs[0]\n            guidedGrads = guidedGrads[0]\n\n            weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n            cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n\n            (w, h) = (image.shape[2], image.shape[1])\n            heatmap = cv2.resize(cam.numpy(), (w, h))\n            numer = heatmap - np.min(heatmap)\n            denom = (heatmap.max() - heatmap.min()) + eps\n            heatmap = numer \/ denom\n            heatmap = (heatmap * 255).astype(\"uint8\")\n        return heatmap\n\n    def overlay_heatmap(self, heatmap, image, alpha=0.5,\n        colormap = cv2.COLORMAP_VIRIDIS):\n        heatmap = cv2.applyColorMap(heatmap, colormap)\n        output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n        return (heatmap, output)","7d9cae7e":"plt.figure(figsize=[16,16])\nfor i in range(36):\n    img = test_images[i,:,:,0]\n    p_dist = cnn.predict(img.reshape(1,48,48,1))\n    k = np.argmax(p_dist)\n    p = np.max(p_dist)\n\n    cam = GradCAM(cnn, k)\n    heatmap = cam.compute_heatmap(img.reshape(1,48,48,1))\n\n    plt.subplot(6,6,i+1)\n    plt.imshow(img, cmap='binary_r')\n    plt.imshow(heatmap, alpha=0.5, cmap='coolwarm')\n    plt.title(f'{emotions[test_labels[i]]} - ({emotions[k]} - {p:.4f})')\n    plt.axis('off')\nplt.tight_layout()\nplt.show()","2d32caf8":"## 2.1 Load the Data","96b1eb13":"## 2.2 Prepare the data","bff99b1a":"# 1.Load Libraries","0e2618a2":"> **Helping functions**","76ee344e":"# 2. Data","b85a1c7e":"## 4.1 - Generate Test Predictions and Calculating Accuracy","c512d566":"So we see that data is the sum of all the Train set, test set (private test and public test). ","56b89c9f":"## 4.3 Class Activation Maps","2c11274d":"Trying to save the model ","83307482":"# 4. Evaluating the model ","b3e3232c":"# 3.CNN Model","0ec91632":"## 3.3 Save Model","5a9b479b":"## 3.1 Training Run 1","d0569379":"Defining train and test data. ","ae37b80e":"## 4.2 Classification report ","6fbc1f9a":"## 2.3 Display Sample of Images ","05169547":"## 3.2 Training Run 2","5c0e46d4":"## 4.2 Confusion matrix"}}