{"cell_type":{"59bf440f":"code","c646093d":"code","bf53715f":"code","475b7873":"code","5c411b7c":"code","ee4986eb":"code","feb4f4cc":"code","a288fc5c":"code","fde67ec2":"code","503bd053":"code","fc7d52e4":"code","5a42afe8":"code","654280bd":"code","8be798ff":"code","882f2ecb":"code","f0d97e9d":"code","f26f902a":"code","ae4263b2":"code","8ef34a88":"code","7ce7d1b3":"code","6cc1f024":"code","d7d34aaf":"code","1c6fd861":"code","8b5597db":"code","bc2ed2c5":"code","3a296304":"code","95adf1ef":"code","e5de8405":"code","1de72af5":"code","6bfbab04":"code","cb952f42":"code","5deca530":"code","66d1ba52":"code","6ee52173":"code","1fdbeda5":"code","fbd8bc85":"code","fadc071c":"code","16e8beb2":"code","7891bf06":"code","de7b4623":"code","fb3f763c":"code","d57eb14e":"code","681e1020":"code","1fdcb936":"code","d00e9d68":"code","51950120":"code","62f2cf87":"code","84192c54":"code","feda9db6":"code","968d77a9":"code","f82a2c4f":"code","1e0f47dc":"code","d588b966":"code","935a7282":"code","1267093e":"code","37555a1e":"code","247b8f7d":"code","ee8dcabb":"code","f97b263d":"code","2a496d99":"code","3d1c6b7e":"code","eff8a4c2":"code","441452b1":"code","2128dceb":"code","db5e380f":"code","5bef01d6":"code","761c74d7":"code","e3637da0":"code","d35a8b0e":"markdown","7125de8d":"markdown","f9f9bdaf":"markdown","114b1670":"markdown","70b89cdb":"markdown","216d4658":"markdown","02f2a94c":"markdown","b034ea52":"markdown","7fe5766e":"markdown","08bae6af":"markdown","470b0212":"markdown","48cc21f1":"markdown","28f993fb":"markdown","87b89c8a":"markdown","d074aa54":"markdown","0158c2fc":"markdown","99655b5f":"markdown","bb2b725b":"markdown","0225057f":"markdown","19afc546":"markdown","880fde48":"markdown","705273c3":"markdown","ce779854":"markdown","5b27bfc5":"markdown","4527ac41":"markdown","2a568014":"markdown","0bb7acc3":"markdown","6e1277f1":"markdown","1d0ddd5f":"markdown","5d94c359":"markdown","7356b2fe":"markdown","88ce18ed":"markdown","e530d5df":"markdown","0b63f167":"markdown","fba1aa84":"markdown","349b4244":"markdown","c6389bcd":"markdown","d296f264":"markdown","79ea78db":"markdown","22120e70":"markdown","181eb947":"markdown","e5f73304":"markdown","3e5d943c":"markdown","89476c7d":"markdown","e196977f":"markdown","3dab903c":"markdown","bd287429":"markdown","d869ad47":"markdown","ea4c6486":"markdown","7e683317":"markdown","f21523ed":"markdown","a321cb8d":"markdown","03e3cbce":"markdown","6b9e7295":"markdown","77464138":"markdown","c41aafc8":"markdown","2bf08090":"markdown","c35b8005":"markdown","be180fe7":"markdown","834721ed":"markdown","cd0e1f3e":"markdown","6ff78cfd":"markdown","497f494d":"markdown","7948220e":"markdown","d1a25964":"markdown","c4745727":"markdown","4a17d555":"markdown","0985a822":"markdown","a9a211af":"markdown","4d239a12":"markdown","83b88f41":"markdown","1957ae7e":"markdown","e331b1cb":"markdown","9015ac92":"markdown","629e04ed":"markdown","c35d4a8e":"markdown","05974baa":"markdown","7d0d4486":"markdown","e313a86f":"markdown","0aca5b70":"markdown","3308f4fc":"markdown","d48ab861":"markdown","69bff449":"markdown","9b664a32":"markdown","bd6e980d":"markdown","256e89b8":"markdown","dd5d1406":"markdown","dabe87a8":"markdown","2ef1bb87":"markdown","8622e8c2":"markdown","9e981251":"markdown","231cf04d":"markdown","6b3dee17":"markdown","cb2d46c6":"markdown","d4578400":"markdown","051b19aa":"markdown"},"source":{"59bf440f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c646093d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import RandomForestClassifier","bf53715f":"train = pd.read_csv('..\/input\/porto-seguro-safe-driver-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/porto-seguro-safe-driver-prediction\/test.csv')","475b7873":"print(train.head())","5c411b7c":"print(train.tail())","ee4986eb":"print(train.shape)","feb4f4cc":"train.drop_duplicates()\nprint(train.shape)","a288fc5c":"print(test.shape)","fde67ec2":"print(train.info())","503bd053":"data = []\n\nfor f in train.columns:\n    if f == 'target':\n        role = 'target'\n    elif f == 'id':\n        role = 'id'\n    else:\n        role = 'input'\n\n    if 'bin' in f or f == 'target':\n        level = 'binary'\n    elif 'cat' in f or f == 'id':\n        level = 'nominal'\n    elif train[f].dtype == 'float64':\n        level = 'interval'\n    elif train[f].dtype == 'int64':\n        level = 'ordinal'\n        \n    keep = True\n    if f == 'id':\n        keep = False\n    \n    dtype = train[f].dtype\n    \n    f_dict = {\n        'varname':f,\n        'role':role,\n        'level':level,\n        'keep':keep,\n        'dtype':dtype\n    }\n    \n    data.append(f_dict)\n    \nmeta = pd.DataFrame(data, columns=['varname', 'role','level','keep', 'dtype'])\nmeta.set_index('varname', inplace=True)\nmeta.to_csv('.\/meta.csv')","fc7d52e4":"meta[(meta.level == 'nomial') & (meta.keep)].index","5a42afe8":"pd.DataFrame({'count':meta.groupby(['role', 'level'])['role'].size()}).reset_index()","654280bd":"v = meta[(meta.level=='interval') & (meta.keep)].index # \uc778\ub371\uc2a4 \ucd94\ucd9c\n\ntrain[v].describe()\n","8be798ff":"'''\n           ps_reg_01      ps_reg_02      ps_reg_03      \ncount  595212.000000  595212.000000  595212.000000     \nmean        0.610991       0.439184       0.551102         \nstd         0.287643       0.404264       0.793506       \nmin         0.000000       0.000000      -1.000000      \n25%         0.400000       0.200000       0.525000      \n50%         0.700000       0.300000       0.720677       \n75%         0.900000       0.600000       1.000000       \nmax         0.900000       1.800000       4.037945       \n'''","882f2ecb":"'''\n           ps_car_13      ps_car_14      ps_car_15 \ncount  595212.000000  595212.000000  595212.000000    \nmean        0.813265       0.276256       3.065899       \nstd         0.224588       0.357154       0.731366       \nmin         0.250619      -1.000000       0.000000   \n25%         0.670867       0.333167       2.828427        \n50%         0.765811       0.368782       3.316625        \n75%         0.906190       0.396485       3.605551 \nmax         3.720626       0.636396       3.741657       \n\n'''","f0d97e9d":"'''\n          ps_calc_02     ps_calc_03  \ncount  595212.000000  595212.000000  \nmean        0.449589       0.449849  \nstd         0.286893       0.287153  \nmin         0.000000       0.000000  \n25%         0.200000       0.200000  \n50%         0.400000       0.500000  \n75%         0.700000       0.700000  \nmax         0.900000       0.900000  \n'''","f26f902a":"v = meta[(meta.level=='ordinal') & (meta.keep)].index # \uc778\ub371\uc2a4 \ucd94\ucd9c\n\ntrain[v].describe()","ae4263b2":"v = meta[(meta.level=='binary') & (meta.keep)].index\n\ntrain[v].describe()","8ef34a88":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf_train = pd.read_csv('..\/input\/porto-seguro-safe-driver-prediction\/train.csv')\n\ntarget_count = df_train.target.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\nplt.title('Count (target)')\nsns.countplot(x='target', data=df_train)\nplt.show()","7ce7d1b3":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Remove 'id' and 'target' columns\nlabels = df_train.columns[2:]\n\nX = df_train[labels]\ny = df_train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","6cc1f024":"model = XGBClassifier()\nmodel.fit(X_train[['ps_calc_01']], y_train)\ny_pred = model.predict(X_test[['ps_calc_01']])\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","d7d34aaf":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n\nconf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint('Confusion matrix:\\n', conf_mat)\n\nlabels = ['Class 0', 'Class 1']\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nsns.heatmap(conf_mat,cmap=plt.cm.Blues)\nax.set_xlabel([''] + labels)\nax.set_ylabel([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","1c6fd861":"# \uac01 class\ubcc4 \uac1c\uc218 count\ncount_class_0, count_class_1 = df_train.target.value_counts()\n\ndf_class_0 = df_train[df_train['target'] == 0]\ndf_class_1 = df_train[df_train['target'] == 1]\n\nprint(count_class_0)\nprint(count_class_1)","8b5597db":"# target \uc774 1\uc778 \ub370\uc774\ud130 \uac1c\uc218\ub9cc\ud07c sampling\n# df_class_0 \uc5d0\uc11c count_class_1 \uac1c\uc218 \ub9cc\ud07c\uc758 \uc218\ub85c sampling \ud574\uc11c \uadf8 \ub370\uc774\ud130\ub9cc \uac00\uc9c0\uace0 train data\ub97c \ub2e4\uc2dc \uad6c\uc131\ndf_class_0_under = df_class_0.sample(n=count_class_1)\ndf_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n\nprint('Random under-sampling:')\nprint(df_test_under.target.value_counts())","bc2ed2c5":"sns.countplot(x=df_test_under['target'])\nplt.title('Count (target)')\nplt.show()","3a296304":"# \ub370\uc774\ud130\uac00 \uc801\uc740 df_class_1 \uc758 \ub370\uc774\ud130\ub97c \uc911\ubcf5\uc744 \ud5c8\uc6a9\ud558\uc5ec count_class_0 \ub9cc\ud07c\uc758 \uc218\ub85c sampling\ndf_class_1_over = df_class_1.sample(count_class_0, replace=True)\ndf_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n\nprint('Random over-sampling:')\nprint(df_test_over.target.value_counts())","95adf1ef":"sns.countplot(x=df_test_over['target'])\nplt.title('Count (target)')\nplt.show()","e5de8405":"# make_classification method \ub97c \uc774\uc6a9\ud574 \uc784\uc758\uc758 \uc791\uc740 unbalanced data \ub97c \ub9cc\ub4e4\uc5b4\uc900\ub2e4.\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(\n    n_classes=2, class_sep=1.5, weights=[0.9, 0.1],\n    n_informative=3, n_redundant=1, flip_y=0,\n    n_features=20, n_clusters_per_class=1,\n    n_samples=100, random_state=10\n)\n\ndf = pd.DataFrame(X)\ndf['target'] = y\ndf.target.value_counts().plot(kind='bar', title='Count (target)')\nplt.show()","1de72af5":"# \uc2dc\uac01\ud654\ub97c \uc704\ud55c \ud568\uc218\n\nimport numpy as np\n\ndef plot_2d_space(X, y, label='Classes'):\n    colors = ['#1F77B4', '#FF7F0E']\n    markers = ['o', 's']\n    for l, c, m in zip(np.unique(y), colors, markers):\n        plt.scatter(\n            X[y==l, 0],\n            X[y==l, 1],\n            c=c, label=l, marker=m\n        )\n    plt.title(label)\n    plt.legend(loc='upper right')\n    plt.show()","6bfbab04":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX = pca.fit_transform(X)\n\nplot_2d_space(X, y, 'Imbalanced dataset (2 PCA components)')","cb952f42":"from imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler()\nX_rus, y_rus = rus.fit_resample(X,y)\n\nprint('Removed indexes:', rus.sample_indices_)\n\nplot_2d_space(X_rus, y_rus, 'Random under-sampling')","5deca530":"from imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler()\nX_ros, y_ros = ros.fit_resample(X, y)\n\nprint(X_ros.shape[0] - X.shape[0], 'new random picked points')\n\nplot_2d_space(X_ros, y_ros, 'Random over-sampling')","66d1ba52":"from imblearn.under_sampling import TomekLinks\n\ntl = TomekLinks(sampling_strategy='majority')\n\nX_tl, y_tl = tl.fit_resample(X, y)\n\nprint('Removed indexes:', tl.sample_indices_)\n\nplot_2d_space(X_tl, y_tl, 'Tomek links under-sampling')","6ee52173":"from imblearn.under_sampling import ClusterCentroids\n\ncc = ClusterCentroids(sampling_strategy={0:10})\nX_cc, y_cc = cc.fit_resample(X, y)\n\nplot_2d_space(X_cc, y_cc, 'Cluster Centroids under-sampling')","1fdbeda5":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(sampling_strategy='minority')\nX_sm, y_sm = smote.fit_resample(X, y)\n\nplot_2d_space(X_sm, y_sm, 'SMOTE over-sampling')","fbd8bc85":"from imblearn.combine import SMOTETomek\n\nsmt = SMOTETomek()\nX_smt, y_smt = smt.fit_resample(X, y)\n\nplot_2d_space(X_smt, y_smt, 'SMOTE + Tomek links')","fadc071c":"desired_apriori=0.10\n\n# \uac01 target \uac12\uc5d0 \ub300\ud55c index \uac00\uc838\uc624\uae30\nidx_0 = train[train.target == 0].index\nidx_1 = train[train.target == 1].index\n\n# target \uac12\uc5d0 \ub300\ud55c \uac1c\uc218 \uac00\uc838\uc624\uae30\nnb_0 = len(train.loc[idx_0])\nnb_1 = len(train.loc[idx_1])\n\n\n\n# Calculate the undersampling rate and resulting number of records with target = 0\nundersampling_rate = ((1-desired_apriori)*nb_1)\/(nb_0*desired_apriori)\n# \uc704 \uc2dd\uc774 \uc5b4\ub5bb\uac8c \ub098\uc654\ub294 \uc9c0 \uc798 \ubaa8\ub974\uaca0\uc74c...\nundersampled_nb_0 = int(undersampling_rate*nb_0)\nprint('Rate to undersample records with target=0: {}'.format(undersampling_rate))\nprint('Number of records with target=0 after undersampling: {}'.format(undersampled_nb_0))\n\n# undersampled_nb_0 \uac1c\uc218\ub9cc\ud07c class 0 \uc758 index \ubf51\uae30\nundersampled_idx = shuffle(idx_0, random_state=37, n_samples=undersampled_nb_0)\n\n# train \uc73c\ub85c \uc4f8 index \ub9ac\uc2a4\ud2b8\ub85c \ub9cc\ub4e4\uc5b4\uc11c \ud569\uce58\uae30\nidx_list = list(undersampled_idx) + list(idx_1)\n\n# train \uc5d0\uc11c index\ub97c \ubf51\uc544 \uc0c8\ub85c\uc6b4 train data \ub9cc\ub4e4\uae30\ntrain = train.loc[idx_list].reset_index(drop=True)","16e8beb2":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.target.values\n\n\nvars_with_missing = []\n\nfor f in train.columns:\n    # train[f] == -1 \uc778 \ub370\uc774\ud130 \ud504\ub808\uc784\uc758 [f] column\uc5d0\uc11c \ub370\uc774\ud130 \uac1c\uc218\ub97c count\n    missing = train[train[f] == -1][f].count()\n\n    if missing > 0:\n        vars_with_missing.append(f)\n        missing_perc = missing \/ train.shape[0]\n\n        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missing, missing_perc))\n\nprint('In total, there are {} variables with missing values'.format(len(vars_with_missing)))","7891bf06":"vars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\ntrain.drop(vars_to_drop, inplace=True, axis=1)\ntest.drop(vars_to_drop, inplace=True, axis=1)\n\n# meta \ub370\uc774\ud130 update\nmeta.loc[(vars_to_drop), 'keep'] = False\n\n# \uc5d0\ub7ec\ub294 \uc774\ubbf8 \uc9c0\uc6cc\uc84c\ub294\ub370 \ud55c \ubc88 \ub354 \uc2e4\ud589 \uc2dc\ucf1c\uc11c \ub098\uc634...","de7b4623":"# SimpleImputer\uc758 axis \ub9e4\uac1c\ubcc0\uc218\uac00 \uc0ac\ub77c\uc9d0. \uae30\ubcf8 \ub3d9\uc791 \ubc29\uc2dd\uc740 Imputer \ud074\ub798\uc2a4\uc758 axis=0 \uc77c \ub54c\uc640 \uac19\ub2e4.\nmean_imp = SimpleImputer(missing_values=-1, strategy='mean')\nmode_imp = SimpleImputer(missing_values=-1, strategy='most_frequent')\n\ntrain['ps_reg_03'] = mean_imp.fit_transform(train[['ps_reg_03']]).ravel() # ravel() \uc740 1\ucc28\uc6d0\uc73c\ub85c \ubcc0\ud658\ud558\ub294 \uc5ed\ud560\ntrain['ps_car_12'] = mean_imp.fit_transform(train[['ps_car_12']]).ravel()\ntrain['ps_car_14'] = mean_imp.fit_transform(train[['ps_car_14']]).ravel()\ntrain['ps_car_11'] = mode_imp.fit_transform(train[['ps_car_11']]).ravel()\n\n# SimpleImputer\uc758 axis \ub9e4\uac1c\ubcc0\uc218\uac00 \uc0ac\ub77c\uc9d0. \uae30\ubcf8 \ub3d9\uc791 \ubc29\uc2dd\uc740 Imputer \ud074\ub798\uc2a4\uc758 axis=0 \uc77c \ub54c\uc640 \uac19\ub2e4.\nmean_imp = SimpleImputer(missing_values=-1, strategy='mean')\nmode_imp = SimpleImputer(missing_values=-1, strategy='most_frequent')\n\ntest['ps_reg_03'] = mean_imp.fit_transform(test[['ps_reg_03']]).ravel() # ravel() \uc740 1\ucc28\uc6d0\uc73c\ub85c \ubcc0\ud658\ud558\ub294 \uc5ed\ud560\ntest['ps_car_12'] = mean_imp.fit_transform(test[['ps_car_12']]).ravel()\ntest['ps_car_14'] = mean_imp.fit_transform(test[['ps_car_14']]).ravel()\ntest['ps_car_11'] = mode_imp.fit_transform(test[['ps_car_11']]).ravel()\n","fb3f763c":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    dist_values = len(train[f].unique())\n    print('Variable {} has {} distinct values'.format(f, dist_values))","d57eb14e":"ex_list = [(\"\ub0a8\uc790\", 1), (\"\uc5ec\uc790\", 1), (\"\uc5ec\uc790\", 1), (\"\uc5ec\uc790\", 0), (\"\ub0a8\uc790\", 0)]\n\nex = pd.DataFrame(data = ex_list, columns = [\"\uc131\ubcc4\", \"target\"])","681e1020":"\uc131\ubcc4_mean = ex.groupby(\"\uc131\ubcc4\")[\"target\"].mean()\n\n\uc131\ubcc4_mean","1fdcb936":"def add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\n\ndef target_encode(trn_series = None, tst_series=None, target = None, min_samples_leaf = 1,\n                  smoothing = 1, noise_level = 0):\n\n    # assert \ubb38\uc740 \ud504\ub85c\uadf8\ub7a8\uc758 \ub0b4\ubd80 \uc810\uac80\uc774\ub2e4. \ud45c\ud604\uc2dd\uc774 \ucc38\uc774 \uc544\ub2c8\uba74 AssertionError \uc608\uc678\uac00 \ubc1c\uc0dd\ud55c\ub2e4.\n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    \n    temp = pd.concat([trn_series, target], axis=1)\n    '''\n    ps_car_11_cat  target\n0                  26       0\n1                 104       0\n2                  15       0\n3                  74       0\n4                  78       0\n...               ...     ...\n216935            104       1\n216936              5       1\n216937            104       1\n216938            104       1\n216939             57       1\n\n    '''\n\n    # trn_series.name \uc740 \uc5ec\uae30\uc11c ps_car_11_cat, target.name \uc740 \uc5ec\uae30\uc11c target\n    # ps_car_11_cat \uc758 \uac19\uc740 \uac12\ub07c\ub9ac \ubb36\uace0 \ubb36\uc5c8\uc744 \ub54c\uc758 target \uac12\uc758 \ud3c9\uade0\uacfc class \ubcc4 \uac1c\uc218\ub97c count \ud55c\ub2e4.\n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n\n    # \uc624\ubc84\ud53c\ud305 \ubc29\uc9c0\ub97c \uc704\ud574 smoothing \uc744 \ucd94\uac00\ud55c\ub2e4.\n    # https: \/\/ en.wikipedia.org \/ wiki \/ Exponential_smoothing\n    # smoothing \ubcc0\uc218\ub294 alpha \ub97c \uc758\ubbf8\n\n    smoothing = 1 \/ (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) \/ smoothing))\n    prior = target.mean()\n    # https: \/\/ www.kaggle.com \/ vprokopev \/ mean - likelihood - encodings - a - comprehensive - study\n    # Where  pc  is a target mean for a category,  nc  is a number of samples in a category,\n    # pglobal  is a global target mean\n    # and  \u03b1  is a regularisation parameter that can be viewed as a size of a category you can trust.\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n\n    # mean \uacfc count column\uc740 \ud544\uc694 \uc5c6\uc5b4\uc11c \uc0ad\uc81c\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n\n\n    # Apply averages to trn and tst series\n    # merge \ud568\uc218\ub294 \ub450 \ub370\uc774\ud130 \ud504\ub808\uc784\uc744 \uac01 \ub370\uc774\ud130\uc5d0 \uc874\uc7ac\ud558\ub294 \uace0\uc720\uac12(key) \uc744 \uae30\uc900\uc73c\ub85c \ubcd1\ud569\ud560 \ub54c \uc0ac\uc6a9\ud55c\ub2e4.\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name), # ps_car_11_cat \uc758 \uac12\ub4e4\uacfc\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n\n    # on=trn_series.name -> ps_car_11_cat \uc5d0\uc11c \uac19\uc740 \uac12\uc744 join\n    # how='left' -> trn_series.to_frame(trn_series.name) \uc744 \uae30\uc900\uc73c\ub85c merge\n    # \uc989, train data \uc758 ps_car_11_cat \uc5d0\uc11c \uac01 \uace0\uc720\uac12\uc5d0 \ud574\ub2f9\ud558\ub294 \ud3c9\uade0 \uac12\uc744 \ub123\uc5b4\uc8fc\ub294 \uacfc\uc815\n    # Nan\uc774 \ubc1c\uc0dd\ud55c \uacbd\uc6b0, \uc804\uccb4 target\uc5d0 \ub300\ud55c \ud3c9\uade0\uc744 \ub123\uc5b4\uc900\ub2e4.\n\n\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index\n    \n    \n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n\n    # pd.merge \uac00 index\ub97c \uc720\uc9c0\ud558\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0 index\ub97c \uc800\uc7a5\ud574\ub454\ub2e4.\n    ft_tst_series.index = tst_series.index\n\n\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","d00e9d68":"train_encoded, test_encoded = target_encode(train[\"ps_car_11_cat\"],\n                             test[\"ps_car_11_cat\"],\n                             target=train.target,\n                             min_samples_leaf=100,\n                             smoothing=10,\n                             noise_level=0.01)\n\ntrain['ps_car_11_cat_te'] = train_encoded\ntrain.drop('ps_car_11_cat', axis=1, inplace=True)\nmeta.loc['ps_car_11_cat', 'keep'] = False  # drop \ud588\uc73c\ubbc0\ub85c, meta \ub370\uc774\ud130\ub97c \uc5c5\ub370\uc774\ud2b8 \uc2dc\ucf1c\uc900\ub2e4.\ntest['ps_car_11_cat_te'] = test_encoded\ntest.drop('ps_car_11_cat', axis=1, inplace=True)","51950120":"meta.to_csv('meta.csv')\n\nv = meta[((meta.level == 'nominal') & (meta.keep))].index\n\nfor f in v:\n    fig, ax = plt.subplots(figsize=(10,7))\n\n    # \uac01 column \ubcc4 target=1 \uc758 percentage \uacc4\uc0b0\n    # \uac01 column \uacfc target \uac12\uc744 \ub530\ub85c \ucd94\ucd9c\ud574\uc11c\n    # \ub9cc\ub4e4\uc5b4\uc9c4 dataframe \uc744 column \uc5d0\uc11c\uc758 \uac19\uc740 \uac12\uc744 \uae30\uc900\uc73c\ub85c groupby \ub97c \ud558\uc5ec, target \uc758 \ud3c9\uade0\uc744 \uacc4\uc0b0\n    '''\n            ps_car_09_cat  target\n0                   2     0.0\n1                   2     0.0\n2                   2     0.0\n3                   0     0.0\n4                   0     0.0\n...               ...     ...\n216935              2     1.0\n216936              2     1.0\n216937              1     1.0\n216938              2     1.0\n216939              2     1.0\n    '''\n    # \uc704\uc640 \uac19\uc774 \ub9cc\ub4e4\uc5b4\uc9c0\uba74, \uc774\ub97c ps_car_09_cat \uc5d0\uc11c \uac19\uc740 \uac12\uc73c\ub85c groupby \ud558\uc5ec target \uac12\uc744 \ud3c9\uade0\ub0b8\ub2e4.\n    cat_perc = train[[f, 'target']].groupby([f],as_index=False).mean()\n    '''\n    ps_car_09_cat    target\n0             -1  0.226087\n1              0  0.092161\n2              1  0.149783\n3              2  0.099636\n4              3  0.096570\n5              4  0.135338\n    '''\n    cat_perc.sort_values(by='target', ascending=False, inplace=True)\n    # barplot\n    # target mean \uc744 \ub0b4\ub9bc\ucc28\uc21c\uc73c\ub85c \uadf8\ub9bc\n    sns.barplot(ax=ax, x=f, y='target', data=cat_perc, order=cat_perc[f])\n    plt.ylabel('% target', fontsize=18)\n    plt.xlabel(f, fontsize=18)\n    plt.tick_params(axis='both', which='major', labelsize=18)\n    plt.show()","62f2cf87":"plt.clf() # plt \ucd08\uae30\ud654","84192c54":"def corr_heatmap(v):\n    correlations = train[v].corr()\n\n    fig, ax = plt.subplots(figsize=(10, 10))\n    sns.heatmap(correlations, cmap='Blues', vmax=1.0, center=0, fmt='.2f',\n                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .75}, ax=ax)\n    plt.show()\n    \nv = meta[(meta.level == 'interval') & (meta.keep)].index\ncorr_heatmap(v)","feda9db6":"s = train.sample(frac=0.1) # \ub370\uc774\ud130\uc758 10% sampling","968d77a9":"sns.lmplot(x='ps_reg_02', y='ps_reg_03', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","f82a2c4f":"sns.lmplot(x='ps_car_12', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","1e0f47dc":"sns.lmplot(x='ps_car_12', y='ps_car_14', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","d588b966":"sns.lmplot(x='ps_car_15', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","935a7282":"v = meta[(meta.level == 'ordinal') & (meta.keep)].index\ncorr_heatmap(v)","1267093e":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\nprint('Before dummification we have {} variables in train'.format(train.shape[1]))\nprint('Before dummification we have {} variables in test'.format(test.shape[1]))\ntrain = pd.get_dummies(train, columns=v, drop_first=True)\ntest = pd.get_dummies(test, columns=v, drop_first=True)\n\nprint('After dummification we have {} variables in train'.format(train.shape[1]))\nprint('After dummification we have {} variables in test'.format(test.shape[1]))","37555a1e":"v = meta[(meta.level == 'interval') & (meta.keep)].index\npoly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\ninteractions = pd.DataFrame(data=poly.fit_transform(train[v]), columns=poly.get_feature_names(v))\ninteractions.drop(v, axis=1, inplace=True)  # \uc6d0\ub798 column\uc740 \uc81c\uac70","247b8f7d":"# interaction variables\ub97c train data\uc640 \ud569\uce5c\ub2e4.\n\nprint('Before creating interactions we have {} variables in train'.format(train.shape[1]))\ntrain = pd.concat([train, interactions], axis=1)\ntest = pd.concat([test, interactions], axis=1)\nprint('After creating interactions we have {} variables in train'.format(train.shape[1]))","ee8dcabb":"selector = VarianceThreshold(threshold=0.01)\nselector.fit(train.drop(['id', 'target'], axis=1)) # id \uc640 target \uc744 \uc81c\uc678\ud558\uace0 fit \ud55c\ub2e4.\n\n# get_support() \ud558\uba74 True,False\uac12\uc774 \ub098\uc624\ub294\ub370 \uc774\uc911\uc5d0\uc11c False\uac12\ub9cc \ubc18\ud658\ud558\uae30\uc704\ud574 f\ud568\uc218\uc9c0\uc815\nf = np.vectorize(lambda x : not x)\n\nv = train.drop(['id','target'],axis=1).columns[f(selector.get_support())]\n\nprint('{} variables have too low variance.'.format(len(v)))\nprint('These variables are {}'.format(list(v)))","f97b263d":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\n\nid_test = test['id']\n\nfeat_labels = X_train.columns\n\nrf = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\n\nrf.fit(X_train, y_train)\nimportances = rf.feature_importances_\n\nindices = np.argsort(rf.feature_importances_)[::-1]\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30,feat_labels[indices[f]], importances[indices[f]]))","2a496d99":"sfm = SelectFromModel(rf, threshold='median', prefit=True)\nprint('Number of features before selection: {}'.format(X_train.shape[1]))\n\nn_features = sfm.transform(X_train).shape[1]\nprint('Number of features after selection: {}'.format(n_features))\nselected_vars = list(feat_labels[sfm.get_support()])\n\ntrain = train[selected_vars + ['target']]\ntest = test[selected_vars]","3d1c6b7e":"'''\ntrain_without_target = train.drop(['target'],axis=1)\n\nscaler = StandardScaler()\ntrain_scaled = scaler.fit_transform(train.drop(['target'],axis=1))\ntrain_scaled = pd.DataFrame(train_scaled, columns=train_without_target.columns, index=list(train.index.values))\ntrain = pd.concat([train_scaled, train['target']], axis=1)\n\nscaler_test = StandardScaler()\ntest_scaled = scaler_test.fit_transform(test)\ntest = pd.DataFrame(test_scaled, columns=test.columns, index=list(test.index.values))\n'''","eff8a4c2":"X = train.drop(['target'],axis=1).values\ny = train['target'].values\n\nX_test = test.values","441452b1":"def eval_gini(y_true, y_pred):\n    # \uc2e4\uc81c \uac12\uacfc \uc608\uce21 \uac12\uc774 \ub3d9\uc77c\ud55c \ud06c\uae30\uc778\uc9c0 \ud655\uc778\ud569\ub2c8\ub2e4 (\ub2e4\ub978 \uac12\uc73c\ub85c \uc778\ud574 \uc624\ub958\uac00 \ubc1c\uc0dd \ud568).\n    assert y_true.shape == y_pred.shape\n\n    n_samples = y_true.shape[0] # \ub370\uc774\ud130 \uc218\n    L_mid = np.linspace(1 \/ n_samples, 1, n_samples) # \ub300\uac01\uc120 \uac12\n    # 1 \/ n_samples \uc5d0\uc11c 1 \uae4c\uc9c0 n_samples \ub9cc\ud07c\uc758 \uac1c\uc218\uc758 \uc218\ub97c \uc0dd\uc131\n\n    # 1) \uc608\uce21 \uac12\uc5d0 \ub300\ud55c \uc9c0\ub2c8 \uacc4\uc218\n    pred_order = y_true[y_pred.argsort()] # y_pred \ud06c\uae30\ub85c y_true \uac12 \uc815\ub82c\n    L_pred = np.cumsum(pred_order) \/ np.sum(pred_order) # Lorentz Curve  # cumsum \uc740 \ub204\uc801\ud569\uc744 \uc758\ubbf8\n    G_pred = np.sum(L_mid - L_pred) # \uc608\uce21\uac12\uc5d0 \ub300\ud55c \uc9c0\ub2c8 \uacc4\uc218 (A \uc601\uc5ed)\n\n    # 2) \uc608\uce21\uc774 \uc644\ubcbd \ud560 \ub54c\uc758 \uc9c0\ub2c8 \uacc4\uc218\n    true_order = y_true[y_true.argsort()] # y_pred \ud06c\uae30\ub85c y_true \uac12 \uc815\ub82c\n    L_true = np.cumsum(true_order) \/ np.sum(true_order) # Lorentz Curve\n    G_true = np.sum(L_mid - L_true) # \uc608\uce21\uc774 \uc644\ubcbd \ud560 \ub54c\uc758 \uc9c0\ub2c8 \uacc4\uc218 (A+B \uc601\uc5ed)\n\n    # \uc815\uaddc\ud654 \ub41c \uc9c0\ub2c8 \uacc4\uc218\n    return G_pred \/ G_true\n\ndef gini_lgb(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'gini', eval_gini(labels, preds), True\n\n\ndef gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'gini', eval_gini(labels, preds)","2128dceb":"from sklearn.model_selection import StratifiedKFold\n\n# Stratified K Fold Cross-Verifier \ub9cc\ub4e4\uae30 (5\ub4f1\ubd84 \ud574 \uc90c)\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1991)","db5e380f":"max_params_lgb = {'bagging_fraction': 0.8043696643500143, # \ub370\uc774\ud130\ub97c \uc0d8\ud50c\ub9c1\ud558\ub294 \ube44\uc728\ub85c, \ud2b8\ub9ac\uac00 \ucee4\uc838\uc11c \uacfc\ub300\uc801\ud569\ub418\ub294 \uac83\uc744 \uc81c\uc5b4\ud55c\ub2e4.\n 'feature_fraction': 0.6829323879981047, # \uac1c\ubcc4 \ud2b8\ub9ac\ub97c \ud559\uc2b5\ud560 \ub54c\ub098\ub2e4 \ubb34\uc791\uc704\ub85c \uc120\ud0dd\ud558\ub294 \ud53c\ucc98\uc758 \ube44\uc728\n 'lambda_l1': 0.9264555612104627, # L1 \uaddc\uc81c (\ud074\uc218\ub85d \uacfc\uc801\ud569 \uac10\uc18c)\n 'lambda_l2': 0.9774233689434216, # L2 \uaddc\uc81c (\ud074\uc218\ub85d \uacfc\uc801\ud569 \uac10\uc18c)\n 'min_child_samples': 10, # \ub9ac\ud504 \ub178\ub4dc\uac00 \ub418\uae30 \uc704\ud55c \ucd5c\uc18c\ud55c\uc758 \uc0d8\ud50c \ub370\uc774\ud130 \uc218\n 'min_child_weight': 125.68433948868649, \n                  # \ud2b8\ub9ac\uc5d0\uc11c \ucd94\uac00\uc801\uc73c\ub85c \uac00\uc9c0\ub97c \ub098\ub20c\uc9c0 \uacb0\uc815\ud558\uae30 \uc704\ud574 \ud544\uc694\ud55c \ub370\uc774\ud130\ub4e4\uc758 weight \ucd1d\ud569 \ud574\ub2f9 \uac12\uc774 \ud074\uc218\ub85d \ubd84\ud560\uc774 \uc798 \uc548 \uc77c\uc5b4\ub09c\ub2e4.\n 'num_leaves': 28, # \ud558\ub098\uc758 \ud2b8\ub9ac\uac00 \uac00\uc9c8 \uc218 \uc788\ub294 \ucd5c\ub300 \ub9ac\ud504 \uac1c\uc218 \n 'objective': 'binary', # \uc190\uc2e4\ud568\uc218 \uc815\uc758\n 'learning_rate': 0.01, # \ud559\uc2b5\ub960\n 'bagging_freq': 1, # bagging \ube48\ub3c4\n                  # 0\uc740 bagging \ube44\ud65c\uc131\ud654\ub97c \uc758\ubbf8\ud558\uace0 k\ub294 k \ubc18\ubcf5\ub9c8\ub2e4 bagging\uc744 \uc218\ud589\ud568\uc744 \uc758\ubbf8\ud55c\ub2e4.\n 'verbosity': 0, \n 'random_state': 1991}\n\nimport lightgbm as lgbm\n\n# OOF-trained model \ub85c \uac80\uc99d \ub370\uc774\ud130 \ub300\uc0c1 \uac12\uc744 \uc608\uce21\ud558\uae30 \uc704\ud55c 1\ucc28\uc6d0 \ud655\ub960 \ubc30\uc5f4\noof_val_preds_lgb = np.zeros(X.shape[0])\n# OOF-trained model \ub85c \uac80\uc99d \ub370\uc774\ud130 \ub300\uc0c1 \uac12\uc744 \uc608\uce21\ud558\uae30 \uc704\ud55c 1\ucc28\uc6d0 \ud655\ub960 \ubc30\uc5f4\noof_test_preds_lgb = np.zeros(X_test.shape[0])\n\n# OOF \uc5d0 \uc758\ud55c Train, validate, and predict models\nfor idx, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n    # The phrase that separates each fold.\n    print('#' * 40, f'Fold {idx + 1} out of {folds.n_splits}', '#' * 40)\n\n    # train data, valid data \uc124\uc815\n    X_train, y_train = X[train_idx], y[train_idx]  # Train data\n    X_valid, y_valid = X[valid_idx], y[valid_idx]  # Valid data\n\n    # Create lgbm dataset\n    dtrain = lgbm.Dataset(X_train, y_train)  # lgbm train dataset\n    dvalid = lgbm.Dataset(X_valid, y_valid)  # lgbm valid dataset\n\n    # Train LightGBM\n    lgb_model = lgbm.train(params=max_params_lgb,  # \ucd5c\uc801\uc758 Hyper-parameters\n                           train_set=dtrain,  # Train data\n                           num_boost_round=1500,  # boosting \ubc18\ubcf5 \ud69f\uc218\n                           valid_sets=dvalid,  # model \uc131\ub2a5 \ud3c9\uac00\ub97c \uc704\ud55c Valid data\n                           feval=gini_lgb,  # validation \uc744 \uc704\ud55c evaluation metrics\n                           early_stopping_rounds=150,  # Early stopping \uc870\uac74 (150 \ud68c \ub3d9\uc548 loss\uc758 \uac10\uc18c\uac00 \uc5c6\uc73c\uba74 \ud559\uc2b5 \uc911\ub2e8)\n                           verbose_eval=100)\n\n    # \ubaa8\ub378\uc758 \uc131\ub2a5\uc774 \uac00\uc7a5 \uc88b\uc740 \uacbd\uc6b0 boosting \ubc18\ubcf5 \ud69f\uc218\n    best_iter = lgb_model.best_iteration\n    # test data \ub97c \uc774\uc6a9\ud55c predict probabilities\n    # (predict \ud55c \uac12\ub4e4\uc758 \ud3c9\uade0)\n    oof_test_preds_lgb += lgb_model.predict(X_test,\n                                            num_iteration=best_iter) \/ folds.n_splits\n    # \ubaa8\ub378 \uc131\ub2a5 \ud3c9\uac00\ub97c \uc704\ud55c OOF \uc608\uce21\n    oof_val_preds_lgb[valid_idx] += lgb_model.predict(X_valid, num_iteration=best_iter)\n\n    # OOF \uc608\uce21\uc5d0 \ub300\ud55c \uc815\uaddc\ud654\ub41c \uc9c0\ub2c8 \uacc4\uc218\n    gini_score = eval_gini(y_valid, oof_val_preds_lgb[valid_idx])\n    print(f'Fold {idx + 1} gini score: {gini_score}\\n')\n","5bef01d6":"max_params_xgb = {'colsample_bytree': 0.8927325521002059, # \ud2b8\ub9ac \uc0dd\uc131\uc5d0 \ud544\uc694\ud55c \ud53c\ucc98(\uce7c\ub7fc)\ub97c \uc784\uc758\ub85c \uc0d8\ud50c\ub9c1\ud558\ub294 \ub370 \uc0ac\uc6a9\ub41c\ub2e4.\n                  # \ub9e4\uc6b0 \ub9ce\uc740 \ud53c\ucc98\uac00 \uc788\ub294 \uacbd\uc6b0 \uacfc\uc801\ud569\uc744 \uc870\uc815\ud558\ub294 \ub370 \uc0ac\uc6a9\ub41c\ub2e4.\n 'gamma': 9.766883037651555, # \ud2b8\ub9ac\uc758 \ub9ac\ud504 \ub178\ub4dc\ub97c \ucd94\uac00\uc801\uc73c\ub85c \ub098\ub20c\uc9c0\ub97c \uacb0\uc815\ud560 \ucd5c\uc18c \uc190\uc2e4 \uac10\uc18c \uac12\uc774\ub2e4.\n                  # \ud574\ub2f9 \uac12\ubcf4\ub2e4 \uc190\uc2e4(loss) \uc774 \uac10\uc18c\ub41c \uacbd\uc6b0\uc5d0 \ub9ac\ud504 \ub178\ub4dc\ub97c \ubd84\ub9ac\ud55c\ub2e4. \uac12\uc774 \ud074\uc218\ub85d \uacfc\uc801\ud569 \uac10\uc18c \ud6a8\uacfc\uac00 \uc788\ub2e4.\n 'max_depth': 7, # \ud2b8\ub9ac \uae30\ubc18 \uc54c\uace0\ub9ac\uc998\uc758 max_depth \uc640 \uac19\ub2e4. 0\uc744 \uc9c0\uc815\ud558\uba74 \uae4a\uc774 \uc81c\ud55c\uc774 \uc5c6\ub2e4. max_depth\uac00 \ub192\uc73c\uba74 \uacfc\uc801\ud569 \uac00\ub2a5\uc11c\uc774 \ub192\ub2e4.\n                  # \ubcf4\ud1b5 3~10 \uc0ac\uc774\uc758 \uac12\uc744 \uc801\uc6a9\ud55c\ub2e4.\n 'min_child_weight': 6.0577898395058085, \n                  # \ud2b8\ub9ac\uc5d0\uc11c \ucd94\uac00\uc801\uc73c\ub85c \uac00\uc9c0\ub97c \ub098\ub20c\uc9c0 \uacb0\uc815\ud558\uae30 \uc704\ud574 \ud544\uc694\ud55c \ub370\uc774\ud130\ub4e4\uc758 weight \ucd1d\ud569 \ud574\ub2f9 \uac12\uc774 \ud074\uc218\ub85d \ubd84\ud560\uc774 \uc798 \uc548 \uc77c\uc5b4\ub09c\ub2e4.\n 'reg_alpha': 8.136089122187865, # L1 \uaddc\uc81c (feature\uc758 \uac1c\uc218\uac00 \ub9ce\uc744 \uacbd\uc6b0 \uc801\uc6a9\uc744 \uac80\ud1a0\ud558\uba70, \uac12\uc774 \ud074\uc218\ub85d \uacfc\uc801\ud569 \uac10\uc18c \ud6a8\uacfc\uac00 \uc788\ub2e4.)\n 'reg_lambda': 1.385119327658532, # L2 \uaddc\uc81c (feature\uc758 \uac1c\uc218\uac00 \ub9ce\uc744 \uacbd\uc6b0 \uc801\uc6a9\uc744 \uac80\ud1a0\ud558\uba70, \uac12\uc774 \ud074\uc218\ub85d \uacfc\uc801\ud569 \uac10\uc18c \ud6a8\uacfc\uac00 \uc788\ub2e4.)\n 'scale_pos_weight': 1.5142072116395773, # \ud2b9\uc815 \uac12\uc73c\ub85c \uce58\uc6b0\uce5c \ube44\ub300\uce6d\ud55c \ud074\ub798\uc2a4\ub85c \uad6c\uc131\ub41c \ub370\uc774\ud130 \uc138\ud2b8\uc758 \uade0\ud615\uc744 \uc720\uc9c0\ud558\uae30 \uc704\ud55c \ud30c\ub77c\ubbf8\ud130\uc774\ub2e4.\n 'subsample': 0.717425859940308, # \ud2b8\ub9ac\uac00 \ucee4\uc838\uc11c \uacfc\uc801\ud569\ub418\ub294 \uac83\uc744 \uc81c\uc5b4\ud558\uae30 \uc704\ud574 \ub370\uc774\ud130\ub97c \uc0d8\ud50c\ub9c1\ud558\ub294 \ube44\uc728\uc744 \uc9c0\uc815\ud55c\ub2e4.\n 'objective': 'binary:logistic', # \uc190\uc2e4\ud568\uc218 \uc9c0\uc815\n 'learning_rate': 0.05, # \ud559\uc2b5\ub960\n 'random_state': 1991}\n\nimport xgboost as xgb\n\n# OOF-trained model \ub85c \uac80\uc99d \ub370\uc774\ud130 \ub300\uc0c1 \uac12\uc744 \uc608\uce21\ud558\uae30 \uc704\ud55c 1\ucc28\uc6d0 \ud655\ub960 \ubc30\uc5f4\noof_val_preds_xgb = np.zeros(X.shape[0])\n# OOF-trained model \ub85c \uac80\uc99d \ub370\uc774\ud130 \ub300\uc0c1 \uac12\uc744 \uc608\uce21\ud558\uae30 \uc704\ud55c 1\ucc28\uc6d0 \ud655\ub960 \ubc30\uc5f4\noof_test_preds_xgb = np.zeros(X_test.shape[0])\n\n# OOF \uc5d0 \uc758\ud55c Train, validate, and predict models\nfor idx, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n    # The phrase that separates each fold.\n    print('#' * 40, f'Fold {idx + 1} out of {folds.n_splits}', '#' * 40)\n\n    # train data, valid data \uc124\uc815\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_valid, y_valid = X[valid_idx], y[valid_idx]\n\n    # xgboost dmatrix \ub9cc\ub4e4\uae30\n    dtrain = xgb.DMatrix(X_train, y_train)\n    dvalid = xgb.DMatrix(X_valid, y_valid)\n    dtest = xgb.DMatrix(X_test)\n\n    watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n\n    xgb_model = xgb.train(params=max_params_xgb,\n                          dtrain=dtrain,\n                          num_boost_round=1000,\n                          evals=watchlist,\n                          maximize=True,\n                          feval=gini_xgb,\n                          early_stopping_rounds=150,\n                          verbose_eval=100)\n\n    # \ubaa8\ub378\uc758 \uc131\ub2a5\uc774 \uac00\uc7a5 \uc88b\uc740 \uacbd\uc6b0 boosting \ubc18\ubcf5 \ud69f\uc218\n    best_iter = xgb_model.best_iteration\n    # test data \ub97c \uc774\uc6a9\ud55c predict probabilities\n    # (predict \ud55c \uac12\ub4e4\uc758 \ud3c9\uade0)\n    oof_test_preds_xgb += xgb_model.predict(dtest,\n                                            ntree_limit=best_iter) \/ folds.n_splits\n    # \ubaa8\ub378 \uc131\ub2a5 \ud3c9\uac00\ub97c \uc704\ud55c OOF \uc608\uce21\n    oof_val_preds_xgb[valid_idx] += xgb_model.predict(dvalid, ntree_limit=best_iter)\n\n    # OOF \uc608\uce21\uc5d0 \ub300\ud55c \uc815\uaddc\ud654\ub41c \uc9c0\ub2c8 \uacc4\uc218\n    gini_score = eval_gini(y_valid, oof_val_preds_xgb[valid_idx])\n    print(f'Fold {idx + 1} gini score: {gini_score}\\n')\n\nprint('LightGBM OOF Gini Score:', eval_gini(y, oof_val_preds_lgb))\nprint('XGBoost OOF Gini Score:', eval_gini(y, oof_val_preds_xgb))","761c74d7":"oof_test_preds = oof_test_preds_lgb * 0.6 + oof_test_preds_xgb * 0.4","e3637da0":"sub = pd.DataFrame()\nsub['id'] = id_test\nsub['target'] = oof_test_preds\n\nsub.set_index('id')\n\nsub.to_csv('submission.csv', index=False)","d35a8b0e":"### gini evaluation\n\n![image.png](attachment:image.png)\n\n* A\uc601\uc5ed = \ub85c\ub80c\uce20 \uace1\uc120\uacfc \uc644\uc804\uade0\ud615 \ub300\uac01\uc120\uacfc\uc758 \uc0ac\uc774 = \ubd88\ud3c9\ub4f1 \uba74\uc801\n* B\uc601\uc5ed = \uc0bc\uac01\ud615 \uc804\uccb4\uba74\uc801 - A\uc601\uc5ed\n* \uc9c0\ub2c8\uacc4\uc218 $${\\displaystyle \uc9c0\ub2c8\uacc4\uc218 G={{A} \\over {A+B}}}$$\n\n* $$\uc18c\ub4dd \uc644\uc804\ud3c9\ub4f1 = 0 , {\\displaystyle \uc9c0\ub2c8\uacc4\uc218 G={{0} \\over {0+B}}}$$\n* \ub85c\ub80c\uce20\uace1\uc120\uc774 \uc644\uc804\uade0\ud615 \ub300\uac01\uc120\uc5d0 \uc218\ub834\ud558\uc5ec \uc77c\uce58\ub420 \ub54c A\uc601\uc5ed\uc740 B\uc601\uc5ed\uc5d0 \uc758\ud574 \uc5c6\uc5b4\uc9c4\ub2e4\uace0 \ubcfc\uc218\uc788\ub2e4.\n* $$\uc18c\ub4dd \uc644\uc804\ubd88\ud3c9\ub4f1 = 1 , {\\displaystyle G={{A} \\over {A+0}}}$$\n* \ub85c\ub80c\uce20\uace1\uc120\uc774 \uc218\uc9c1\uc120\uc5d0 \uc218\ub834\ud558\uc5ec \uc77c\uce58\ub420 \ub54c B\uc601\uc5ed\uc740 A\uc601\uc5ed\uc5d0 \uc758\ud574 \uc5c6\uc5b4\uc9c4\ub2e4\uace0 \ubcfc\uc218\uc788\ub2e4.\n\n\n<br>\n\n\ucd9c\ucc98: [https:\/\/ko.wikipedia.org\/wiki\/%EC%A7%80%EB%8B%88_%EA%B3%84%EC%88%98](https:\/\/ko.wikipedia.org\/wiki\/%EC%A7%80%EB%8B%88_%EA%B3%84%EC%88%98)","7125de8d":"### insight\n\n* ps_car_11_cat \ub9cc 104\uac1c\ub85c \ub9ce\uc740 distinct value \ub4e4\uc774 \ub9ce\ub2e4\ub294 \uac83\uc744 \ubcfc \uc218 \uc788\ub2e4.\n* \uc774\ub807\uac8c distinct value \ub4e4\uc774 \ub9ce\uc740 \uacbd\uc6b0 mean encoding\uc744 \uc801\uc6a9\ud55c\ub2e4.","f9f9bdaf":"#### 1. ps_reg_02 and ps_reg_03","114b1670":"## Resampling\n\n\ub9e4\uc6b0 \ubd88\uade0\ud615\ud55c \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\uae30 \uc704\ud574 \ub110\ub9ac \ucc44\ud0dd\ub418\ub294 \uae30\uc220\uc744 Resampling \uc774\ub77c\uace0 \ud55c\ub2e4. <br>\n\ub2e4\uc218 \ud074\ub798\uc2a4\uc5d0\uc11c \uc0d8\ud50c\uc744 \uc81c\uac70\ud558\uac70\ub098(under-sampling) \uc18c\uc218 \ud074\ub798\uc2a4\uc5d0\uc11c \ub354 \ub9ce\uc740 example \uc744 \ucd94\uac00\ud558\ub294 \ubc29\uc2dd(over-sampling)\uc774 \uc788\ub2e4. <br>\n<br>\n\nsampling\uc744 \ud1b5\ud574 balancing class \ub4e4\uc744 \ub9cc\ub4e4\uc5b4 \uc900\ub2e4\ub294 \uc7a5\uc810\uc774 \uc788\uc9c0\ub9cc \ub2e8\uc810\ub3c4 \uc874\uc7ac\ud55c\ub2e4. <br>\n* over-sampling\uc740 \uc18c\uc218\uc758 \ud074\ub798\uc2a4\uc5d0\uc11c \uc784\uc758\uc758 record(\ud589) \uc744 \ubcf5\uc81c\ud558\uae30 \ub54c\ubb38\uc5d0 over-fitting\uc744 \uc720\ubc1c\ud55c\ub2e4. <br>\n* under-sampling\uc740 \ub2e4\uc218\uc758 \ud074\ub798\uc2a4\uc5d0\uc11c \uc784\uc758\uc758 record\ub97c \uc81c\uac70\ud558\ub294 \uac83\uc774\ubbc0\ub85c \uc815\ubcf4 \uc190\uc2e4\uc774 \ubc1c\uc0dd\ud560 \uc218 \uc788\ub2e4. <br>","70b89cdb":"# Meta Data","216d4658":"#### Ensemble prediction","02f2a94c":"\uc544\ub798\ub294 VarianceThreshold \uc0ac\uc6a9 \uc608\uc2dc\uc774\ub2e4.","b034ea52":"* role: input, ID, target -> \uc5ed\ud560 <br>\n* level: nominal, interval, ordinal, binary -> \ub370\uc774\ud130 \uc885\ub958 <br>\n* keep: True or False <br>\n* dtype: int, float, str","7fe5766e":"## \uc5ec\ub7ec\uac00\uc9c0 imbalanced classes \ub2e4\ub8e8\ub294 \ubc29\ubc95","08bae6af":"### 4) Under-sampling: Cluster Centroids\n\n\uc774 \uae30\uc220\uc740 clustering \ubc29\ubc95\uc744 \uae30\ubc18\uc73c\ub85c centroid\ub97c \uc0dd\uc131\ud558\uc5ec under-sampling\uc744 \uc218\ud589\ud55c\ub2e4. <br>\n\uc815\ubcf4\ub97c \ubcf4\uc874\ud558\uae30 \uc704\ud574 \ub370\uc774\ud130\ub294 \uc720\uc0ac\uc131\uc5d0 \ub530\ub77c \uadf8\ub8f9\ud654 \ub41c\ub2e4.<br>\n\n<br>\n\uc544\ub798 \uc608\uc2dc\uc5d0\uc11c sampling_strategy parameter \ub85c {0: 10} \ub515\uc154\ub108\ub9ac\ub97c \ub118\uaca8\uc8fc\ub294\ub370, \uc774\ub294 \ub2e4\uc218 \ud074\ub798\uc2a4\uc778 0\uc744 10\uac1c\ub9cc \ub0a8\uae30\uace0, \uc18c\uc218 \ud074\ub798\uc218 1\uc740 \uadf8\ub300\ub85c \ub454\ub2e4.","470b0212":"#### 4. ps_car_13 and ps_car_15","48cc21f1":"# training\n\nEnsemble \uc744 \uc774\uc6a9\ud558\uc5ec \ud574 \ubcf4\uc558\ub2e4.","28f993fb":"#### 3. ps_car_12 and ps_car_14","87b89c8a":"role \uacfc level \uc5d0 \ub530\ub978 \uac1c\uc218","d074aa54":"## 2. Creating interaction variables\n\n\uc0ac\uc774\ud0b7\ub7f0\uc758 PolynomialFeatures\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828 \uc138\ud2b8\uc5d0 \uc788\ub294 \uac01 \ud2b9\uc131\uc744 \uc81c\uacf1\ud558\uc5ec \uc0c8\ub85c\uc6b4 \ud2b9\uc131\uc73c\ub85c \ucd94\uac00\ud55c \ud6c8\ub828 \ub370\uc774\ud130\ub97c \ub9cc\ub4e4\uc5b4\ubcf4\uc790 <br>\n<br>\nQ : 2\ucc28\ud56d \ubcc0\uc218\ub97c \ub9cc\ub4dc\ub294 \uc774\uc720\ub294 ? \uc5b4\ub5a4\uacbd\uc6b0\uc5d0 ? <br>\nA : \ub370\uc774\ud130\ub4e4\uac04\uc758 \ud615\ud0dc\uac00 \ube44\uc120\ud615\uc77c\ub54c \ub370\uc774\ud130\uc5d0 \uac01 \ud2b9\uc131\uc758 \uc81c\uacf1\uc744 \ucd94\uac00\ud574\uc11c \ud2b9\uc131\uc774 \ucd94\uac00\ub41c \ube44\uc120\ud615 \ub370\uc774\ud130\ub97c \uc120\ud615\ud68c\uadc0 \ubaa8\ub378\ub85c \ud6c8\ub828\uc2dc\ud0a4\ub294 \ubc29\ubc95 <br>","0158c2fc":"### Modeling\n\n\uac01 \ubaa8\ub378\uc5d0 \ub300\ud574\uc11c cross validation\uc744 \uc9c4\ud589\ud558\uace0 <br>\nLGBM \uc5d0 0.6 \uc758 \uac00\uc911\uce58\ub97c, XBG \uc5d0\ub294 0.4\uc758 \uac00\uc911\uce58\ub97c \uc8fc\uc5b4 Ensemble \ud559\uc2b5\uc744 \ud55c\ub2e4. <br>","99655b5f":"#### 1. LGBM","bb2b725b":"### 1) imblearn \uc744 \uc774\uc6a9\ud574 under-sampling \uc744 \uc218\ud589\ud55c\ub2e4.","0225057f":"\uc704\uc640 \uc544\ub798 \uacb0\uacfc \ubaa8\ub450 96.35% \uc815\ud655\ub3c4\ub97c \ubcf4\uc778\ub2e4. \uc5ed\uc2dc \ub192\uc740 \uc815\ud655\ub3c4\ub294 \uadf8\uc800 \ucc29\uac01\uc774\uc5c8\ub2e4. <br>\n\ubd88\uade0\ud615 \ub370\uc774\ud130 \uc138\ud2b8\uc5d0 \uc0ac\uc6a9\ub418\ub294 metric \uc744 \uc120\ud0dd\ud558\ub294 \uac83\uc740 \ub9e4\uc6b0 \uc911\uc694\ud558\ub2e4. <br>\n\n\uc774 competition\uc5d0\uc11c evaluation metric\uc740 \ubd88\uade0\ud615 \ub370\uc774\ud130 \uc138\ud2b8\uc5d0 \ub300\ud574 \ubcf4\ub2e4 \uac15\ub825\ud55c <br>\nmetric\uc778 \uc815\uaddc\ud654\ub41c \uc9c0\ub2c8 \uacc4\uc218\uc774\uba70, random guessing(\ubb34\uc791\uc704 \ucd94\uce21)\uc758 \uacbd\uc6b0 \uc57d 0\uc5d0\uc11c \ucd5c\uace0\uc810\uc758 \uacbd\uc6b0 \uc57d 0.5\uae4c\uc9c0\uc774\ub2e4. <br>","19afc546":"### reg variable \uc5d0 \ub300\ud55c insight\n\n* ps_reg_03 \ub9cc missing values\ub97c \uac00\uc9c0\uace0 \uc788\ub2e4.\n* reg \ubcc0\uc218\uac04\uc758 \ucd5c\ub300 \ucd5c\uc18c \uc0ac\uc774 \ubc94\uc704\uac00 \uc11c\ub85c \ub2e4\ub974\uae30 \ub54c\ubb38\uc5d0 scaling\uc744 \uc0ac\uc6a9\ud574 \ubcfc \uc218 \uc788\ub2e4.\n* \uadf8\ub7ec\ub098, \uc6b0\ub9ac\uac00 \uc0ac\uc6a9\ud558\uace0\uc790 \ud558\ub294 classifier\uc5d0 \ub530\ub77c\uc11c scaling \uc0ac\uc6a9 \uc5ec\ubd80\/\uc5b4\ub5a4 scaling\uc744 \uc0ac\uc6a9\ud560 \uc9c0 \ub2ec\ub77c\uc9c4\ub2e4.\n\n<br>\n\n#### scaler \ubcc4 \ud6a8\uacfc\nhttps:\/\/homeproject.tistory.com\/entry\/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%8A%A4%EC%BC%80%EC%9D%BC%EB%A7%81-Data-Scaling","880fde48":"data \uc885\ub958\uac00 nomial \uc774\uace0, drop \ud558\uc9c0 \uc54a\ub294 \uc778\ub371\uc2a4 \ucd94\ucd9c","705273c3":"## Meta Data \uc774\uc6a9\ud574\uc11c describe() \uc801\uc6a9\ud560 \ubcc0\uc218 \ubf51\uae30","ce779854":"> # \ub370\uc774\ud130 \ud0d0\uc0c9","5b27bfc5":"#### submission","4527ac41":"### \uc720\uc9c0\ud560 \uc0c1\uad00 \ubcc0\uc218\ub97c \uc5b4\ub5bb\uac8c \uacb0\uc815\ud560 \uc218 \uc788\uc744\uae4c?\n\n* \ubcc0\uc218\uc5d0 \ub300\ud574 PCA \ub97c \uc218\ud589\ud558\uc5ec \ucc28\uc6d0\uc744 \uc904\uc77c \uc218 \uc788\ub2e4.\n* \uadf8\ub7ec\ub098 correlated variables\uac00 \uc801\uae30 \ub54c\ubb38\uc5d0, PCA\ub97c \uc9c4\ud589\ud558\uc9c0 \uc54a\ub294\ub2e4.","2a568014":"### 6) Over-sampling followed by under-sampling\n\nSMOTE \uacfc Tomek links \uae30\uc220\uc744 \uc0ac\uc6a9\ud558\uc5ec over-sampling \uacfc under-sampling \uc758 \uc870\ud569\uc744 \uc218\ud589\ud55c\ub2e4.","0bb7acc3":"## \uc911\ubcf5\ub41c row\uac00 \uc788\ub294\uc9c0 \ud655\uc778\ud558\uae30 (\uc644\uc804\ud788 \uac19\uc740 \ub370\uc774\ud130\uac00 \uc788\ub294\uc9c0 \ud655\uc778)","6e1277f1":"get_support \ub97c \uc774\uc6a9\ud558\uc5ec \uc0ac\uc6a9\ud560 \ub370\uc774\ud130\ub97c \ubf51\uc544\ub0b8\ub2e4.","1d0ddd5f":"* \ub0a8\uc790\uc758 \uacbd\uc6b0 2\uac1c\uc758 \ub370\uc774\ud130\uc5d0\uc11c target\uac12\uc774 1\uacfc0 \uc774\ubbc0\ub85c 0.5\uac00 \ub098\uc624\uace0,\n* \uc5ec\uc790\uc758 \uacbd\uc6b0 3\uac1c\uc758 \ub370\uc774\ud130\uc5d0\uc11c target\uac12\uc774 1\uc774 2\uac1c 0\uc774 1\uac1c\uc774\ubbc0\ub85c 0.6667\uc774 \ub098\uc628\ub2e4.\n* \uc774 \uac12\uc73c\ub85c \ud574\ub2f9 unique\uac12\uc744 \uc778\ucf54\ub529 \ud574\uc900\ub2e4.\n\n\uc774\uc640 \uac19\uc740 \ubc29\uc2dd\uc744 mean-encoding \uc774\ub77c\uace0 \ud55c\ub2e4.","5d94c359":"### 2. binary variables","7356b2fe":"## categorical variable \uc5d0 \ub300\ud55c describe\n\n\uc55e\uc5d0\uc11c categorical variable \uc5d0 \ub300\ud574\uc11c \uc774\ub7ec\ud55c \uacc4\uc0b0\uc740 \ubb34\uc758\ubbf8\ud558\ub2e4\uace0 \ud588\uc73c\ub098, \uadf8\ub798\ub3c4 \ud55c \ubc88 describe\ub97c \ud1b5\ud574 \uad00\ucc30\ud574 \ubcf8\ub2e4.","88ce18ed":"null \uac12\uc774 -1\ub85c \ub300\ucc44 \ub3fc \uc788\uc5b4\uc11c null \uac12\uc774 \uc5c6\ub2e4\uace0 \ub098\uc634. <br>\nbinary \uac12\uc740 \uc774\ubbf8 \uc218\ub85c \ubcc0\ud658\ub3fc \uc788\uc74c. <br>\n\uc774\ud6c4\uc5d0 categorical feature \ub4e4\ub85c dummy variable \ub9cc\ub4e4 \ud544\uc694 \uc788\uc74c. <br>","e530d5df":"### mean-encoding \uc608\uc2dc\n\n\uc544\ub798\uc640 \uac19\uc740 \ub370\uc774\ud130\uac00 \uc788\ub2e4\uace0 \uac00\uc815\ud574\ubcf4\uc790","0b63f167":"### car variable \uc5d0 \ub300\ud55c insight\n\n* ps_car_12, ps_car_14 \uac00 missing data\ub97c \uac00\uc9c0\uace0 \uc788\uace0\n* \ucd5c\ub300 \ucd5c\uc18c \uc0ac\uc774 \ubc94\uc704\uac00 \ub2e4\ub974\uae30 \ub54c\ubb38\uc5d0 scaling \uc801\uc6a9 \uac00\ub2a5","fba1aa84":"* ps_car_03_cat, ps_car_05_cat : \uacb0\uce21\uce58\uac00 \ub108\ubb34 \ub9ce\uc544 \uc9c0\uc6cc\ubc84\ub9b0\ub2e4.\n\n* \uacb0\uce21\uac12\uc774 \uc788\ub294 \ub2e4\ub978 categorical \ub370\uc774\ud130\uc758 \uacbd\uc6b0 \uacb0\uce21\uac12 -1 \uc744 \uadf8\ub300\ub85c \ub458 \uc218 \uc788\ub2e4.\n\n* ps_reg_03 (continuous): 18% \uc758 \uacb0\uce21\uce58\uac00 \uc788\ub2e4. \ud3c9\uade0\uc73c\ub85c \ucc44\uc6cc\uc900\ub2e4.\n* ps_car_11 (ordinal) : 5\uac1c\uc758 \uacb0\uce21\uce58\uac00 \uc788\ub2e4. \ucd5c\ube48\uac12\uc73c\ub85c \ucc44\uc6cc\uc900\ub2e4.\n* ps_car_12 (continuous): 2\uac1c\uc758 \uacb0\uce21\uce58\uac00 \uc788\ub2e4. \ud3c9\uade0\uc73c\ub85c \ucc44\uc6cc\uc900\ub2e4.\n* ps_car_14 (continuous) : 7%\uc758 \uacb0\uce21\uce58\uac00 \uc788\ub2e4. \ud3c9\uade0\uc73c\ub85c \ucc44\uc6cc\uc900\ub2e4.","349b4244":"#### 2. ps_car_12 and ps_car_13","c6389bcd":"#### 2. XGboost","d296f264":"### metric \ud568\uc815\n\n\ucd08\ubcf4\uc790\uac00 \ubd88\uade0\ud615 \ub370\uc774\ud130 \uc138\ud2b8\ub97c \ucc98\ub9ac\ud560 \ub54c \ubc1c\uc0dd\ud558\ub294 \uc8fc\uc694 \ubb38\uc81c \uc911 \ud558\ub098\ub294 \ubaa8\ub378\uc744 \ud3c9\uac00\ud558\ub294\ub370 \uc0ac\uc6a9\ub418\ub294 metric\uacfc \uad00\ub828\uc774 \uc788\ub2e4. <br>\naccuracy_score \uc640 \uac19\uc740 \ub354 \uac04\ub2e8\ud55c \uce21\uc815 \ud56d\ubaa9\uc744 \uc0ac\uc6a9\ud558\uba74 \uc624\ud574\uc758 \uc18c\uc9c0\uac00 \uc0dd\uae34\ub2e4.<br>\n\ubd84\ub958\uae30\uac00 \ud559\uc2b5\uc744 \ud558\uc9c0 \uc54a\uace0 \uac00\uc7a5 \uc218\uac00 \ub9ce\uc740 class\ub97c \uc608\uce21\ud558\uba74, \uc5ec\uc804\ud788 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \uac16\uac8c \ub41c\ub2e4.<br>\n\n\uc608\ub97c \ub4e4\uc5b4 100\uac1c\uc758 \ub370\uc774\ud130 \uc911 96\uac1c\uac00 1\uc774\uace0 4\uac1c\uac00 0\uc778\ub370, \ubd84\ub958\uae30\ub294 \uadf8\ub0e5 \ubaa8\ub4e0 \ub370\uc774\ud130\ub97c 1\ub85c \uc608\uce21\ud588\uc744 \ubfd0\uc778\ub370 96% \uc758 \uc815\ud655\ub3c4\uac00 \ub098\uc624\ub294 \uac83\uc774\ub2e4.","79ea78db":"## 4. Checking the cardinality of the categorical variables\n\ncardinality \ub780, variable \uc5d0 \uc788\ub294 \uace0\uc720\ud55c \uac12\ub4e4\uc758 \uac1c\uc218 (unique \ud55c \uac12\uc758 \uac1c\uc218)\ub97c \ub9d0\ud55c\ub2e4. <br>\n\uc774\ud6c4\uc5d0 dummy variable \uc744 \ub9cc\ub4e4 \uac83\uc774\ubbc0\ub85c, \uace0\uc720 \uac12\uc774 \ub9ce\uc740 variable \uc774 \uc788\ub294\uc9c0 \ud655\uc778\ud574\uc57c \ud55c\ub2e4. <br>\n\uc774\ub7ec\ud55c \ubcc0\uc218\ub294 \ub9ce\uc740 dummy variable \uc744 \uc0dd\uc131\ud558\ubbc0\ub85c \ub2e4\ub974\uac8c \ucc98\ub9ac\ud574\uc57c \ud55c\ub2e4.","22120e70":"### confusion matrix\n\nxgboost\uc5d0 \uc758\ud574 \uc608\uce21\uc774 \uc5b4\ub5bb\uac8c \ub410\ub294\uc9c0 \ubcf4\uae30 \uc704\ud574 confusion matrix \ub97c \ubcf8\ub2e4","181eb947":"### Parameter\n\n1. degree: \ucc28\uc218\n2. interaction_only: \uc0c1\ud638\uc791\uc6a9 \ud56d\ub9cc \ucd9c\ub825 (x1, x2\uc77c \ub54c \uc790\uc2e0\uc758 \uc81c\uacf1\ud56d\uc740 \ubb34\uc2dc\ud558\uace0 x1x2 \ub9cc \ucd9c\ub825)\n3. include_bias: \uc0c1\uc218\ud56d \uc0dd\uc131 \uc5ec\ubd80","e5f73304":"\ud558\ub098\uc758 feature \ub9cc\uc744 \uc0ac\uc6a9\ud574\uc11c accuracy\uac00 \uc5b4\ub5bb\uac8c \ub098\uc624\ub294 \uc9c0 \ubd24\ub2e4.","3e5d943c":"\ucc38\uace0\n* [https:\/\/www.kaggle.com\/bertcarremans\/data-preparation-exploration](https:\/\/www.kaggle.com\/bertcarremans\/data-preparation-exploration)\n* [https:\/\/www.kaggle.com\/jeanbai\/resampling-strategies-for-imbalanced-datasets](https:\/\/www.kaggle.com\/jeanbai\/resampling-strategies-for-imbalanced-datasets)\n* [https:\/\/www.kaggle.com\/whtngus4759\/porto-seguro-for-beginner](https:\/\/www.kaggle.com\/whtngus4759\/porto-seguro-for-beginner)\n* [https:\/\/www.kaggle.com\/kongnyooong\/porto-seguro-eda-for-korean](https:\/\/www.kaggle.com\/kongnyooong\/porto-seguro-eda-for-korean)\n* [https:\/\/www.kaggle.com\/vprokopev\/mean-likelihood-encodings-a-comprehensive-study](https:\/\/www.kaggle.com\/vprokopev\/mean-likelihood-encodings-a-comprehensive-study)\n* [https:\/\/www.kaggle.com\/werooring\/top-9th-lightgbm-xgboost-ensemble](https:\/\/www.kaggle.com\/werooring\/top-9th-lightgbm-xgboost-ensemble)","89476c7d":"# feature selection\n\n\ubd84\ub958 \uc54c\uace0\ub9ac\uc998\uc774 keep \ud560 feature \ub97c \uc120\ud0dd\ud558\ub3c4\ub85d \ud558\uae30\ub3c4 \ud558\uc9c0\ub9cc, <br>\n\uc6b0\ub9ac\uac00 \uc120\ud0dd\ud574\uc8fc\ub294 \ubc29\ubc95\ub3c4 \uc788\ub2e4. <br>\n\uc774\ub294 \ubd84\uc0b0\uc774 0\uc774\uac70\ub098 \uc791\uc740 feature\ub97c \uc81c\uac70\ud574 \uc900\ub2e4.<br>\nSklearn\uc5d0\ub294 VarianceThreshold \ub97c \uc774\uc6a9\ud558\uba74 \ud3b8\ub9ac\ud558\uac8c \uc81c\uac70\ud560 \uc218 \uc788\ub2e4.<br>\n<br>\n\uadf8\ub7ec\ub098, \uc774 \ub370\uc774\ud130\uc5d0\uc11c\ub294 \ubd84\uc0b0\uc774 0\uc778 \ubcc0\uc218\uac00 \uc5c6\uace0,<br>\n\ubd84\uc0b0\uc774 1% \ubbf8\ub9cc\uc778 feature\ub97c \uc81c\uac70\ud558\uba74 31\uac1c\uc758 \ubcc0\uc218\uac00 \uc81c\uac70\ub41c\ub2e4.<br>\nfeature \uac00 \uc801\uc740 \uc0c1\ud0dc\uc778\ub370, 31\uac1c\uc758 feature\ub97c \uc9c0\uc6cc\ubc84\ub9ac\uba74 \ub354 feature\uac00 \uc5c6\uae30 \ub54c\ubb38\uc5d0,<br>\n\uc774 \ub370\uc774\ud130\uc5d0\uc11c\ub294 \uc801\uc6a9\ud558\uc9c0 \uc54a\ub294\ub2e4.<br>\n<br>\n<br>\n Q : \ubd84\uc0b0\uc774 \uc791\uc740 \ubcc0\uc218\ub97c \uc81c\uac70\ud574\uc57c\ud558\ub294 \uc774\uc720 ?<br>\n<br>\n A : \uc608\uce21\ubaa8\ub378\uc5d0\uc11c \uc911\uc694\ud55c \ud2b9\uc131\uc774\ub780, \ud0c0\uac9f\uacfc\uc758 \uc0c1\uad00\uad00\uacc4\uac00 \ud070 \ud2b9\uc131\uc744 \uc758\ubbf8\ud55c\ub2e4.<br>\n    \uadf8\ub7f0\ub370 \uc0c1\uad00\uad00\uacc4\uc5d0 \uc55e\uc11c \uc5b4\ub5a4 \ud2b9\uc131\uc758 \uac12 \uc790\uccb4\uac00 \ud45c\ubcf8\uc5d0 \ub530\ub77c \uadf8\ub2e4\uc9c0 \ubcc0\ud558\uc9c0 \uc54a\ub294\ub2e4\uba74,<br>\n    \uc608\uce21\uc5d0 \ubcc4 \ub3c4\uc6c0\uc774 \ub418\uc9c0 \uc54a\uc744 \uac00\ub2a5\uc131\uc774 \ub192\ub2e4.<br>\n    (ex. \ub0a8\uc790\ub97c \uc0c1\ub300\ub85c\ud55c \uc124\ubb38\uc870\uc0ac \ub370\uc774\ud130\uc5d0\uc11c \ub0a8\uc790 \ub77c\ub294 \uc131\ubcc4\ud2b9\uc131\uc740 \ubb34\uc758\ubbf8\ud568.)<br>\n    \ub530\ub77c\uc11c, \ud45c\ubcf8 \ubcc0\ud654\uc5d0 \ub530\ub978 \ub370\uc774\ud130 \uac12\uc758 \ubcc0\ud654\ub7c9 \uc989, \ubd84\uc0b0\uc774 \uae30\uc900\uce58\ubcf4\ub2e4\ub0ae\uc740 \ud2b9\uc131\uc740 \uc81c\uac70\ud558\ub294\uac83<br>\n<br>\n\ucd9c\ucc98: https:\/\/www.kaggle.com\/whtngus4759\/porto-seguro-for-beginner","e196977f":"describe \ub97c \uc774\uc6a9\ud574\uc11c \ud3c9\uade0, \ud45c\uc900\ud3b8\ucc28 \ub4f1\ub4f1\uc744 \uacc4\uc0b0\ud560 \uc218 \uc788\uc9c0\ub9cc, categorical variable \uc5d0 \ub300\ud574\uc11c \uc774\ub7ec\ud55c \uacc4\uc0b0\uc740 \uc758\ubbf8\uac00 \uc5c6\ub2e4. <br>\n\uadf8\ub7ec\ubbc0\ub85c, \uc774\ud6c4\uc5d0 categorical variable\uc740 \uc2dc\uac01\ud654\ud55c\ub2e4.","3dab903c":"### insight\n\n* \ud3c9\uade0\uc744 \ud1b5\ud574 \ub300\ubd80\ubd84\uc758 \ubcc0\uc218\uc5d0\uc11c \uac12\ub4e4\uc774 0\uc784\uc744 \uc54c \uc218 \uc788\uace0, \uc774\ub294 \ub9e4\uc6b0 imbalance\ud55c \ub370\uc774\ud130 \uc784\uc744 \uc54c \uc218 \uc788\ub2e4.","bd287429":"### \uc544\ub798 \ucf54\ub4dc\ub294 noise\uc640 smoothing\uc744 \ucd94\uac00\ud588\ub2e4.","d869ad47":"# feature scailing \n\nfeature scailing \uc744 \uc801\uc6a9\ud558\uba74 \ub354 \uc798 \ub3d9\uc791\ud558\ub294 \uacbd\uc6b0\ub3c4 \uc788\ub2e4.","ea4c6486":"## 2. ps_car_03_cat, ps_car_05_cat \uc9c0\uc6b0\uae30","7e683317":"# Selecting features with a Random Forest and SelectFromModel\n\n* \ubd84\uc0b0\uc744 \uae30\ubc18\uc73c\ub85c feature\ub97c \uc120\ud0dd\ud558\uba74 \ub9ce\uc740 \ubcc0\uc218\ub97c \uc783\uac8c \ub41c\ub2e4.\n* \uadf8\ub7ec\ub098, \ubcc0\uc218\uc758 \uac1c\uc218\uac00 \ub9ce\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0 classifier \uac00 \uc120\ud0dd\ud558\ub3c4\ub85d \ud55c\ub2e4.\n\n<br>\n\n* sklearn \uc740 \ub2e4\ub978 feature selection \uae30\ub2a5\uc744 \uc81c\uacf5\ud558\ub294\ub370,\n* \uc774\ub294 SelectFromModel \uc744 \uc774\uc6a9\ud558\uc5ec classifier\uac00 \ucd5c\uc0c1\uc758 feature\ub97c \uc120\ud0dd\ud558\ub3c4\ub85d \ud558\ub294 \uac83\uc774\ub2e4.\n* \uc544\ub798 \ubc29\ubc95\uc740 Random Forest \ub97c \uc774\uc6a9\ud55c \ubc29\uc2dd\uc774\ub2e4.","f21523ed":"## data loading","a321cb8d":"\uc784\uc758\ub85c \ub9cc\ub4e0 \ub370\uc774\ud130\uac00 \uace0\ucc28\uc6d0 \ub370\uc774\ud130\uc774\uae30 \ub54c\ubb38\uc5d0 PCA\ub97c \uc774\uc6a9\ud558\uc5ec 2\ucc28\uc6d0\uc73c\ub85c \ucc28\uc6d0 \ucd95\uc18c\ub97c \ud574\uc900\ub2e4.","03e3cbce":"* smoothing \uc2dd\n\n![image.png](attachment:image.png)\n\n[https: \/\/ www.kaggle.com \/ vprokopev \/ mean - likelihood - encodings - a - comprehensive - study](http:\/\/)\n\n","6b9e7295":"## 3. Imputer\ub97c \uc774\uc6a9\ud574\uc11c \uacb0\uce21\uce58 \ucc44\uc6cc\uc8fc\uae30","77464138":"### pair plot\n\nseaborn \uc5d0\ub294 \ubcc0\uc218\uac04\uc758 (\uc120\ud615) \uad00\uacc4\ub97c \uc2dc\uac01\ud654 \ud558\ub294 \uba87 \uac00\uc9c0 \ud3b8\ub9ac\ud55c plot\uc774 \uc788\ub2e4. <br>\npairplot \uc744 \uc774\uc6a9\ud574\uc11c \ubcc0\uc218 \uac04\uc758 \uad00\uacc4\ub97c \uc2dc\uac01\ud654 \ud560 \uc218 \uc788\ub2e4. <br>\nheat map \uc5d0\uc11c \uc774\ubbf8 \uc0c1\uad00\uad00\uacc4\uac00 \ub192\uc740 \uac01 \ubcc0\uc218\ub97c \ubcf4\uc5ec\uc92c\uae30 \ub54c\ubb38\uc5d0 \uc0c1\uad00\uad00\uacc4\uac00 \ub192\uc740 \ubcc0\uc218\ub97c \uc704\uc8fc\ub85c pairplot \uc744 \uadf8\ub9b0\ub2e4. <br>\n\n<br>\n\n\ucc38\uace0: \ubaa8\ub4e0 \ub370\uc774\ud130\ub85c plot \ud558\uae30 \uc2dc\uac04\uc774 \ub108\ubb34 \ub9ce\uc774 \uac78\ub9ac\ubbc0\ub85c 10% \ub9cc sampling \ud574\uc11c \uc801\uc6a9\uc2dc\ucf1c\uc900\ub2e4.","c41aafc8":"### calc variable \uc5d0 \ub300\ud55c insight\n\n* missin data\ub294 \uc5c6\uc74c.\n* \ucd5c\ub300\uac12\uc774 0.9\uc784\uc744 \uc54c \uc218 \uc788\ub2e4.\n* 3\uac1c\uc758 _calc \ubcc0\uc218\ub4e4\uc774 \uac19\uc740 \ubd84\ud3ec\ub97c \uac00\uc9c0\uace0 \uc788\uc74c\uc744 \uc54c \uc218 \uc788\ub2e4.","2bf08090":"\ub370\uc774\ud130 \uad00\ub9ac\ub97c \uc6a9\uc774\ud558\uac8c \ud558\uae30 \uc704\ud574\uc11c \ubcc0\uc218\uc5d0\ub300\ud55c meta\uc815\ubcf4\ub97c DataFrame\uc5d0 \uc800\uc7a5 \ud560 \uac83\uc774\ub2e4. <br>\n\uc774\ub294 \uc774\ud6c4 \ubd84\uc11d, \uc2dc\uac01\ud654, \ubaa8\ub378\ub9c1\uc2dc\uc5d0 \uc720\uc6a9\ud558\ub2e4. ","c35b8005":"## data tail-5 \ub370\uc774\ud130 \ubcf4\uae30","be180fe7":"### DataFream.sample \uc744 \uc774\uc6a9\ud558\uc5ec \uac01 \ud074\ub798\uc2a4\uc5d0 \ub300\ud574 random sample\uc744 \uc5bb\ub294 \ubc29\ubc95","834721ed":"### insight\n\n* \uadf8\ub798\ud504\ub85c \ubcfc \uc218 \uc788\ub4ef\uc774, \ub450 \ubcc0\uc218\uac00 \uc120\ud615 \uad00\uacc4\ub97c \uc774\ub8ec\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4.\n* hue parameter\ub97c \uc774\uc6a9\ud558\uc5ec traget=0\uacfc target=1 \uc758 regression line\uc774 \ube44\uc2b7\ud568\uc744 \uc54c \uc218 \uc788\ub2e4.","cd0e1f3e":"* \ubd84\ub958\uae30\uac00 \ubaa8\ub4e0 \ub370\uc774\ud130\ub97c 1\ub85c \uc608\uce21 \ud588\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4.\n* \ubaa8\ub4e0 \ub370\uc774\ud130\ub97c 1\ub85c \uc608\uce21 \ud588\uc74c\uc5d0\ub3c4 96% \uc758 accuracy \uac00 \ub098\uc654\uc74c\uc744 \uc54c \uc218 \uc788\ub2e4.","6ff78cfd":"# \ub2e4\uc2dc \ub3cc\uc544\uc640\uc11c\n### \uc5ec\uae30\uc11c\ub294 \ud070 training set\uc744 \uc0ac\uc6a9\ud558\uae30 \ub54c\ubb38\uc5d0, undersampling\uc744 \uc774\uc6a9\ud55c\ub2e4.","497f494d":"## ordinal variables \uc5d0 \ub300\ud574 correlation \uc870\uc0ac","7948220e":"* smoothing \uc2dd\uc5d0\uc11c alpha \uc5d0 \ub300\ud55c \uc2dd\n\n![image.png](attachment:image.png)\n\n[https: \/\/ en.wikipedia.org \/ wiki \/ Exponential_smoothing](http:\/\/)\n","d1a25964":"### insight\n\n* \uadf8\ub798\ud504\ub97c \ud1b5\ud574 -1 \uac12 (\uacb0\uce21\uce58 \uac12)\uc774 \ub9ce\uc744\uc218\ub85d \uace0\uac1d\uc774 \ubcf4\ud5d8\uc744 \uccad\uad6c\ud560 \uac00\ub2a5\uc131\uc774 \ub192\ub2e4\ub294 \uac83\uc744 \ubcfc \uc218 \uc788\ub2e4.\n* \ud639\uc740 -1\uc774 \uc5c6\ub294 \uacbd\uc6b0 \ubcf4\ud5d8 \uccad\uad6c \uac00\ub2a5\uc131\uc774 \uc5c6\ub2e4\ub294 \uac83\ub3c4 \uc54c \uc218 \uc788\ub2e4.\n* \uadf8\ub7ec\ubbc0\ub85c, -1\uc774 \uc720\uc758\ubbf8\ud55c \ud2b9\uc9d5\uc744 \ubcf4\uc5ec\uc11c, \ucd5c\ube48\uac12\uc73c\ub85c \ub300\uccb4\ud558\ub294 \ub300\uc2e0 -1\uac12\uc744 \uadf8\ub300\ub85c \ub450\ub294\uac8c \ubc14\ub78c\uc9c1\ud558\ub2e4.\n* \ub354 \ub9ce\uc740 insight\uc740 https:\/\/www.kaggle.com\/kongnyooong\/porto-seguro-eda-for-korean \ucc38\uace0","c4745727":"# Feature engineering\n\n## 1. encoding\n\n* \uace0\uc720\uac12 (unique)\uc774 \ub9ce\uc740 variable\uc740 mean-encoding\uc744 \ud574\uc92c\ub2e4.\n* \ub098\uba38\uc9c0 \ubcc0\uc218\ub4e4\uc5d0 \ub300\ud574\uc11c\ub294 dummy variable\uc744 \ub9cc\ub4e4\uc5b4\uc900\ub2e4. (one-hot endcoding)\n* categorical variable\uc5d0 \uc21c\uc11c\ub97c \ubd80\uc5ec\ud558\uc9c0 \uc54a\uace0, \uace0\uc720\uac12\uc774 \uc801\uc5b4 \ucc28\uc6d0\uc774 \ub9ce\uc774 \ub298\uc5b4\ub098\uc9c0 \uc54a\ub294\ub2e4.","4a17d555":"## insight 2\n\n* \uc804\ubc18\uc801\uc73c\ub85c, interval \ubcc0\uc218\ub4e4\uc758 \ubc94\uc704\uac00 \ud06c\uac8c \ucc28\uc774\uac00 \ub098\uc9c0 \uc54a\ub294 \uac83\uc73c\ub85c \ubcf4\uc544, \uc544\ub9c8\ub3c4 log \ubcc0\ud658 \uac19\uc740 \ub370\uc774\ud130 \ubcc0\ud658\uc774 \uc801\uc6a9\ub41c \ub370\uc774\ud130\uc774\uc9c0 \uc544\ub2d0\uae4c \uc0dd\uac01\ud55c\ub2e4.\n","0985a822":"# imabalaced classes \ub2e4\ub8e8\uae30\n\n\ntargert=1 \uc778 \uac83\uc758 \ube44\uc728\uc774 target=0 \ubcf4\ub2e4 \ub9e4\uc6b0 \uc801\ub2e4. <br>\n\uc774\ub85c \uc778\ud574 accuracy\uac00 \uc88b\uc544 \ubcf4\uc77c \uc218\ub294 \uc788\uc73c\ub098, \uc0ac\uc2e4 \uadf8\ub807\uac8c \ubcfc \uc218\ub294 \uc5c6\uace0 \ubaa8\ub378\uc758 \ud559\uc2b5\uc774 \uc81c\ub300\ub85c \uc774\ub8e8\uc5b4\uc9c0\uc9c0 \uc54a\uc744 \ud655\ub960\uc774 \ub192\ub2e4. <br>\n<br>\n\uc774\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574 \ub098\uc628 \uac1c\ub150\uc774 under-sampling \uacfc over-sampling \uc774\ub2e4.\n<br>\n\n\n* under-sampling: \ubd88\uade0\ud615\ud55c \ub370\uc774\ud130 \uc14b\uc5d0\uc11c \ub192\uc740 \ube44\uc728\uc744 \ucc28\uc9c0\ud558\ub358 \ud074\ub798\uc2a4\uc758 \uc218\ub97c \uc904\uc784\uc73c\ub85c\uc368 \ub370\uc774\ud130\uc758 \ubd88\uade0\ud615\uc744 \ud574\uc18c\ud558\ub294 \uc544\uc774\ub514\uc5b4\uc774\ub2e4. <br>\n  \ud558\uc9c0\ub9cc, \uc774 \ubc29\ubc95\uc740 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub418\ub294 \uc804\uccb4 \ub370\uc774\ud130 \uc218\ub97c \uae09\uaca9\ud558\uac8c \uac10\uc18c\uc2dc\ucf1c \uc624\ud788\ub824 \uc131\ub2a5\uc774 \ub5a8\uc5b4\uc9c8 \uc218 \uc788\ub2e4.\n<br>\n\n* over-sampling: \ub0ae\uc740 \ube44\uc728 \ud074\ub798\uc2a4\uc758 \ub370\uc774\ud130 \uc218\ub97c \ub298\ub9bc\uc73c\ub85c\uc368 \ub370\uc774\ud130 \ubd88\uade0\ud615\uc744 \ud574\uc18c\ud558\ub294 \uc544\uc774\ub514\uc5b4\uc774\ub2e4. <br>\n  \uc774 \ubc29\ubc95\uc740 \uac00\ub2a5\ud558\ub2e4\uba74 \uc5b8\ub354 \uc0d8\ud50c\ub9c1\ubcf4\ub2e4 \ud6e8\uc52c \uc88b\uc740 \ud574\uacb0\ucc45\uc774\uc9c0\ub9cc, \ubb38\uc81c\ub294 \uc5b4\ub5bb\uac8c \uc5c6\ub358 \ub370\uc774\ud130\ub97c \uc0dd\uc131\ud558\ub290\ub0d0 \uc774\ub2e4.","a9a211af":"### 3) Under-sampling: Tomek links\n\nTomek links \ub294 \ub9e4\uc6b0 \uac00\uae4c\uc6b4 instance \uc30d\uc774\uc9c0\ub9cc \ubc18\ub300 \ud074\ub798\uc2a4\uc774\ub2e4. <br>\n\uac01 \uc30d\uc758 \ub2e4\uc218 \ud074\ub798\uc2a4 \uc778\uc2a4\ud134\uc2a4\ub97c \uc81c\uac70\ud558\uba74 \ub450 \ud074\ub798\uc2a4 \uc0ac\uc774\uc758 \uacf5\uac04\uc774 \ub298\uc5b4\ub098 classification\uc5d0 \uc6a9\uc774\ud574 \uc9c4\ub2e4. <br>\n![image.png](attachment:image.png)\n\n<br>\nsampling_strategy='majority' \ub97c \uc774\uc6a9\ud574\uc11c \uad6c\ud604\ud55c\ub2e4.<br>","4d239a12":"## \uc5ec\ub7ec\uac00\uc9c0 encoding \ubc29\uc2dd\n\n\ubc94\uc8fc\ud615 \ubcc0\uc218\ub97c \uc778\ucf54\ub529\ud558\ub294 \ubc29\ubc95\uc5d0\uc11c \uc77c\ubc18\uc801\uc73c\ub85c one-hot-encoding, Label-encoding \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud55c\ub2e4. <br>\n\n* one-hot-encoding\uc740 \ubcc0\uc218\uc5d0 \uc21c\uc11c(\ub192\uace0 \ub0ae\uc74c)\uc774 \ubd80\uc5ec\ub418\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0 target\uc744 \uc608\uce21\ud558\ub294\ub370 \uc601\ud5a5\uc744 \uc8fc\uc9c4 \uc54a\uc9c0\ub9cc\n  unique\uac12\uc774 \ub9ce\uc744 \uacbd\uc6b0 \uad49\uc7a5\ud788 sparse\ud55c \ubca1\ud130\uac00 \uc0dd\uc131\ub418\uace0, \ub610\ud55c feature\ub3c4 \uae30\ud558\uae09\uc218\uc801\uc73c\ub85c \ub298\uc5b4\ub098\uae30 \ub54c\ubb38\uc5d0 cost\uac00 \ub298   \uc5b4\ub098\uace0\n  \ucc28\uc6d0\uc758 \uc800\uc8fc\uc5d0 \ube60\uc9c8 \ud655\ub960\uc774 \ub192\uc544\uc9c4\ub2e4.\n\n* Label-encoding\uc758 \uacbd\uc6b0 feature\uac00 \uc720\uc9c0\ub418\uae30 \ub54c\ubb38\uc5d0 cost\uac00 \uc801\uace0 \ucc28\uc6d0\uc744 \uc2e0\uacbd\uc4f8 \ud544\uc694\uac00 \uc5c6\uc9c0\ub9cc\n  \uac01\uac01 unique \uac12\uc5d0 \ub9e4\ud551\ub418\ub294 \uc22b\uc790\uc5d0 \uc21c\uc11c\uac00 \uc0dd\uaca8\ubc84\ub9ac\uae30 \ub54c\ubb38\uc5d0 target \uc608\uce21\uc5d0 \uc601\ud5a5\uc744 \uc904 \uc218 \uc788\ub2e4.\n\n* \uc774 \ucee4\ub110\uc5d0\uc11c\ub294 mean-encoding\uc774\ub77c\ub294 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud588\ub2e4.\n\n\n### mean-encoding\n\n* \uce90\uae00\uc5d0\uc11c\ub294 mean encoding, frequency encoding \ub4f1 \ubc29\ubc95\ub4e4\uc744 \ub9ce\uc774 \uc0ac\uc6a9\ud55c\ub2e4\uace0 \ud55c\ub2e4.\n\n* \ucc28\uc6d0\uc758 \uc800\uc8fc\uc5d0 \ube60\uc9c8 \ud655\ub960\uc774 \uc5c6\uace0 \ube60\ub974\ub2e4\ub294 \uc7a5\uc810\uc774 \uc788\ub2e4.\n\n* \uac00\uc7a5 \ud070 \uc7a5\uc810\uc73c\ub85c\ub294 target\uacfc\uc758 mean\uc744 \ucde8\ud574\uc8fc\uc5c8\uae30 \ub54c\ubb38\uc5d0 \uc77c\uc885\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \uac00\uc9c0\uac8c \ub41c\ub2e4.\n\n* \ud558\uc9c0\ub9cc \uce58\uba85\uc801\uc778 \ub2e8\uc810\uc774 \uc788\ub2e4.\n\n* \ubc14\ub85c Data Leakage \ubb38\uc81c\uc640 \uc624\ubc84\ud53c\ud305 \ubb38\uc81c\uc774\ub2e4.\n\n* test \ubc0f \ucd94\ud6c4 \uc218\uc9d1\ub420 \ub370\uc774\ud130 \uc14b\uc5d0 \ub300\ud55c target \uac12\uc744 \uc54c\uc9c0 \ubabb\ud558\uae30 \ub54c\ubb38\uc5d0 target\uc5d0 \ub300\ud55c \ud3c9\uade0\uc744 train \ub370\uc774\ud130 \uc14b\uc73c\ub85c\ub9cc \uc801\uc6a9\uc2dc\ucf1c\uc57c \ud55c\ub2e4.\n\n* \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 test \uc14b\uc758 target \uac12\uc744 \uc0ac\uc6a9\ud558\uac8c \ub418\uba74 data leakage \ubb38\uc81c\uc5d0 \ube60\uc9c0\uac8c\ub418\uace0, \n  train \uc14b\uc758 target \uac12\ub9cc \uc0ac\uc6a9\ud558\uac8c \ub418\uba74 \uc624\ubc84\ud53c\ud305\ub418\ub294 \ub51c\ub808\ub9c8\uac00 \uc788\ub2e4.\n\n* \ud2b9\ud788 train, test \uc14b\uc758 unique\uac12 \ubd84\ud3ec\uac00 \ud06c\uac8c \ub2e4\ub97c \ub54c \uc624\ubc84\ud53c\ud305 \ubb38\uc81c\ub294 \ucee4\uc9c0\uac8c \ub41c\ub2e4.\n\n* ex. train \uc14b\uc758 \ub0a8\uc790 95\uba85 \uc5ec\uc7905\uba85, test \uc14b\uc758 \ub0a8\uc790 50\uba85, \uc5ec\uc790 50\uba85\n\n\uc544\ub798 \ubc29\ubc95\uc744 \ubcf4\uc544, train \uc14b\uc758 target \uac12\ub9cc \uc0ac\uc6a9\ud55c \uac83 \uac19\ub2e4...","83b88f41":"## \uac01\uac01\uc758 variable\uc5d0 \ub300\ud55c \uc815\ubcf4 \uac00\uc838\uc624\uae30","1957ae7e":"## test data \uc5d0 train data\uc640 \uac19\uc740 variable(column) \uc758 \uc218\ub97c \uac00\uc9c0\uace0 \uc788\ub294\uc9c0 \ud655\uc778","e331b1cb":"## 2. interval variable\n\n* interval variable \ub07c\ub9ac\uc758 correlation \uc744 \ubcf8\ub2e4.\n* heatmap\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubcf8\ub2e4.","9015ac92":"### insight\n\n* ps_car_11 \ub9cc \uacb0\uce21\uce58\uac00 \uc874\uc7ac\ud568\uc744 \uc54c \uc218 \uc788\ub2e4.\n* \ub2e4\ub978 \ubc94\uc704\ub97c \uac00\uc9c0\uace0 \uc788\uc744 \uacbd\uc6b0 scaling \uc801\uc6a9 \uac00\ub2a5 ","629e04ed":"### insight\n\n\ubcc0\uc218\ub4e4 \uc0ac\uc774\uc5d0 \uac15\ud55c correlation\uc774 \ubcf4\uc778\ub2e4.\n\n* ps_reg_02 and ps_reg_03 (0.7)\n* ps_car_12 and ps_car13 (0.67)\n* ps_car_12 and ps_car14 (0.58)\n* ps_car_13 and ps_car15 (0.67)","c35d4a8e":"### Python imbalanced-learn module\n\nscientific literature \uc5d0\uc11c\ub294 \ubcf4\ub2e4 \uc815\uad50\ud55c Resampling \uae30\ubc95\uc774 \ub9ce\uc774 \uc81c\uc548\ub418\uc5c8\ub2e4. <br>\n<br>\n\uc5d0\ub97c\ub4e4\uc5b4, \n* under-sampling \uc758 \uacbd\uc6b0 \n  \ub2e4\uc218 \ud074\ub798\uc2a4\uc758 record(\ud589 \ub370\uc774\ud130)\ub97c clustering \ud558\uace0 \uac01 cluster \uc5d0\uc11c record\ub97c \uc81c\uac70\ud558\uc5ec under-sampling\uc744 \uc218\ud589\ud558\uc5ec\uc815\ubcf4\ub97c \ubcf4\uc874\ud560 \uc218 \uc788\ub2e4. \n\n* over-sampling \uc758 \uacbd\uc6b0\n  \uc18c\uc218 \ud074\ub798\uc2a4 record \uc758 \uc815\ud655\ud55c \ubcf5\uc218\ubcf8\uc744 \ub9cc\ub4dc\ub294 \ub300\uc2e0 \ubcf5\uc0ac\ubcf8\uc5d0 \uc791\uc740 \ubcc0\ud615\uc744 \ub3c4\uc785\ud558\uc5ec \ub354 \ub2e4\uc591\ud55c \ud569\uc131 \uc0d8\ud50c\uc744 \ub9cc\ub4e4 \uc218 \uc788\ub2e4.\n\nPython \ub77c\uc774\ube0c\ub7ec\ub9ac imbalanced-learn\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc774\ub7ec\ud55c Resampling \uae30\uc220 \uc911 \uc77c\ubd80\ub97c \uc801\uc6a9 \ud574 \ubcfc \uc218 \uc788\ub2e4. <br>\nscikit-learn\uacfc \ud638\ud658\ub418\uba70 scikit-learn-contrib \ud504\ub85c\uc81d\ud2b8\uc758 \uc77c\ubd80\uc785\ub2c8\ub2e4. <br>\n\n<br>\nanaconda prompt \uc5d0\uc11c <br>\n\npip install imblearn \uc744 \ud1b5\ud574 \ub2e4\uc6b4 \ubc1b\ub294\ub2e4. <br>\n\n\uac00\uc0c1 \ud658\uacbd\uc744 \ub9cc\ub4e4\uc5b4\uc11c \uc4f0\uace0 \uc788\ub2e4\uba74 'activate \uac00\uc131\ud658\uacbd \uc774\ub984' \uc73c\ub85c \uac00\uc0c1\ud658\uacbd \ud65c\uc131\ud654\ub97c \uc2dc\ud0a4\uace0 \uc124\uce58\ud558\uba74 \ub41c\ub2e4. <br>","05974baa":"\uc911\ubcf5\ub41c row\uac00 \uc5c6\uc74c\uc744 \uc54c \uc218 \uc788\ub2e4.","7d0d4486":"### \ucd9c\ub825\uacfc \uc2dc\uac01\ud654\ub97c \ud1b5\ud574 \ub370\uc774\ud130\uac00 \ub9e4\uc6b0 imbalance \ud568\uc744 \uc54c \uc218 \uc788\ub2e4.","e313a86f":"### 5) Over-sampling: SMOTE\n\nSMOTE\ub294 \uc801\uc740 \ub370\uc774\ud130 \uc14b\uc5d0 \uc788\ub294 \uac1c\ubcc4 \ub370\uc774\ud130\ub4e4\uc758 K \ucd5c\uadfc\uc811 \uc544\uc6c3(K Nearest Neighbor)\uc744 \ucc3e\uc544\uc11c, <br>\n\uc774 \ub370\uc774\ud130\uc640 K\uac1c \uc774\uc6c3\ub4e4\uc758 \ucc28\uc774\ub97c \uc77c\uc815 \uac12\uc73c\ub85c \ub9cc\ub4e4\uc5b4\uc11c \uae30\uc874 \ub370\uc774\ud130\uc640 \uc57d\uac04 \ucc28\uc774\uac00 \ub098\ub294 \uc0c8\ub85c\uc6b4 \ub370\uc774\ud130\ub4e4\uc744 \uc0dd\uc131\ud558\ub294 \ubc29\uc2dd\uc774\ub2e4.\n<br>\n\n*  \uc54c\uace0\ub9ac\uc998\n\n    1. \uc18c\uc218 \ub370\uc774\ud130 \uc911 \ud2b9\uc815 \ubca1\ud130 (\uc0d8\ud50c)\uc640 \uac00\uc7a5 \uac00\uae4c\uc6b4 \uc774\uc6c3 \uc0ac\uc774\uc758 \ucc28\uc774\ub97c \uacc4\uc0b0\ud55c\ub2e4.\n    2. \uc774 \ucc28\uc774\uc5d0 0\uacfc 1\uc0ac\uc774\uc758 \ub09c\uc218\ub97c \uacf1\ud55c\ub2e4.\n    3. \ud0c0\uac9f \ubca1\ud130\uc5d0 \ucd94\uac00\ud55c\ub2e4.\n    4. \ub450 \uac1c\uc758 \ud2b9\uc815 \uae30\ub2a5 \uc0ac\uc774\uc758 \uc120\ubd84\uc744 \ub530\ub77c \uc784\uc758\uc758 \uc810\uc744 \uc120\ud0dd\ud560 \uc218 \uc788\ub2e4.\n\n\ucd9c\ucc98:\nhttps:\/\/mkjjo.github.io\/python\/2019\/01\/04\/smote_duplicate.html","0aca5b70":"## 1. Random under sampling\n\n\npandas.DataFrame.sample: <br>\n\nhttps:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.sample.html <br>\n\n-> Return a random sample of items from an axis of object. <br>","3308f4fc":"\ud558\ub098\uac00 \ub204\ub77d \ub3fc \uc788\ub294 \uac83 \ucc98\ub7fc \ubcf4\uc774\uc9c0\ub9cc, target \uac12\uc774 \ub204\ub77d \ub3fc \uc788\ub294\uac70\uc5ec\uc11c \uad1c\ucc2e\ub2e4.","d48ab861":"------------------------------------------------------------------------------------ <br>\n\n\uc5ec\uae30\uc11c \ubd80\ud130\ub294 Porto Seguro\u2019s Safe Driver Prediction \uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uace0 \uc7a0\uc2dc \uc784\uc758\uc758 \ub370\uc774\ud130\ub97c \ub9cc\ub4e4\uc5b4\uc11c \uc0ac\uc6a9\ud55c\ub2e4. <br>\n\uc0ac\uc6a9 \uc608\uc2dc\ub97c \ubcf4\uc5ec\uc8fc\uae30 \uc704\ud568\uc774\ub2e4.","69bff449":"## shape \ubcf4\uae30","9b664a32":"## data head-5 \ub370\uc774\ud130 \ubcf4\uae30","bd6e980d":"\uc778\ucf54\ub529\ud560 \ubc94\uc8fc\ud615 \ubcc0\uc218\uc640 target\uc744 groupby\ud574\uc900 \ud6c4 \ud3c9\uade0\uac12\uc744 \ucde8\ud574\uc900\ub2e4.","256e89b8":"### 2) imblearn \uc744 \uc774\uc6a9\ud574 over-sampling \uc744 \uc218\ud589\ud55c\ub2e4.","dd5d1406":"* PolynomialFeatures \ub97c \uc0ac\uc6a9\ud55c\ub2e4.\n\n* \uc785\ub825\uac12 x\ub97c \ub2e4\ud56d\uc2dd\uc73c\ub85c \ubcc0\ud658 (x >> [1, x, x^2, x^3...])\n\n* \uc5f4\uc758 \uac1c\uc218\uac00 2\uac1c\ub77c\uba74 (x1, x2 >> [1, x1, x2, x1^2, x2^2, x1x2])\n\n* poly.get_feature_names\ub97c \ud1b5\ud574 \ud3b8\ud558\uac8c feature\uc758 \uc774\ub984\uc744 \uc9c0\uc815\ud574\uc904 \uc218 \uc788\ub2e4.\n\n\ucd9c\ucc98: https:\/\/www.kaggle.com\/kongnyooong\/porto-seguro-eda-for-korean","dabe87a8":"## insight 1\n\n1. \uc774\ub97c \ud1b5\ud574 \ud3c9\uade0\uacfc \uc911\uc559\uac12\uc758 \ucc28\uc774, \ucd5c\uc18c \ucd5c\ub300\ub97c \ubcf4\uc544 \ub370\uc774\ud130\uac00 \uc5b4\ub5a4 \uc2dd\uc758 \ubd84\ud3ec\ub97c \uac00\uc9c0\uace0 \uc788\ub294\uc9c0 \ub300\ucda9 \ucd94\uc815\ud560 \uc218 \uc788\ub2e4.\n2. \ucd5c\uc18c\uac12\uc744 \uad00\ucc30\ud558\uc5ec \ucd5c\uc18c\uac12\uc774 -1\uc774 \uc874\uc7ac\ud558\ub294 column\uc5d0 \uacb0\uce21\uce58\uac00 \uc788\uc74c\uc744 \uc54c \uc218 \uc788\ub2e4.","2ef1bb87":"### mean-encoding \uc801\uc6a9 \uc2dc\ucf1c\uc8fc\uae30.","8622e8c2":"## 2. Random over-sampling","9e981251":"# Data Quality Checks\n\n## 1. \uacb0\uce21\uce58 \ubcf4\uae30","231cf04d":"\uc5ec\uae30\uc11c\ub294 Random forest \uc758 feature importance \ub97c \uae30\uc900\uc73c\ub85c feature selection\uc744 \uc9c4\ud589\ud55c\ub2e4. <br>\n<br>\nSklearn\uc758 SelectFromModel\uc744 \uc0ac\uc6a9\ud558\uba74 \uc720\uc9c0\ud560 \ubcc0\uc218 \uac1c\uc218\ub97c \uc9c0\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.<br>\n<br>\nfeature importance \uc758 \uc784\uacc4\uac12\uc744 \uc815\ud560 \uc218\ub3c4 \uc788\ub2e4.<br>\n\uadf8\ub7ec\ub098 \uc5ec\uae30\uc11c\ub294 \ub2e8\uc21c\ud558\uac8c \uc0c1\uc704 50% \uc758 \ubcc0\uc218\ub97c \ud0dd\ud55c\ub2e4.<br>","6b3dee17":"### insight\n\nordinal variable \uc758 \uacbd\uc6b0 \uc0c1\uad00\uad00\uacc4\uac00 \ub9ce\uc9c0 \uc54a\ub2e4.","cb2d46c6":"### 1. ordinal variables ","d4578400":"## meta \ub370\uc774\ud130 \uc0ac\uc6a9\ud574\ubcf4\ub294 \uc5f0\uc2b5","051b19aa":"# Exploratory Data Visualization\n\n## 1. \ubc94\uc8fc\ud615 \ub370\uc774\ud130 (Categorical variables)\n\n\ubc94\uc8fc\ud615 \ub370\uc774\ud130\uc640 target=1 \uc778 \uace0\uac1d\uc758 \ube44\uc728\uc744 \uc0b4\ud3b4\ubcf8\ub2e4."}}