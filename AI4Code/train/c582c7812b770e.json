{"cell_type":{"caa9a37b":"code","f5a68cf9":"code","15dc3b00":"code","525f222b":"code","cd347467":"code","20f93d9d":"code","aea992f3":"code","b1237b63":"code","256acc7b":"code","d6bb04fa":"code","662d4185":"code","7b274e9a":"code","855b3744":"code","e3111529":"code","aec2d39e":"code","a946bf3f":"code","bd9a07ae":"code","dc4fc41d":"code","a7888a2b":"code","7d7d3f39":"code","b32e9bb3":"code","8209eae6":"code","c5508740":"code","4d998c02":"code","144f3a6e":"code","ca6e97a8":"code","bb9da69c":"code","a81e030d":"code","dfba87bc":"code","80661411":"code","65107f77":"code","30d108e5":"code","eb0b3c1a":"code","97e21be3":"code","4d05750d":"code","87ba52f8":"code","afe0b193":"code","87e4e897":"code","d1555799":"code","404d6e63":"code","c8a68e04":"code","1d73e53d":"code","f54283d7":"code","c142c2a2":"code","ec6fc9a5":"code","f63eca8f":"code","975bbe07":"code","5fe9994d":"code","d671d059":"code","8a2c000f":"code","2c3707cb":"code","909b150a":"code","b95f10e7":"code","6eec053a":"code","1d4d86e9":"code","5cbe2dff":"code","e64ee564":"code","83caf50f":"code","650af389":"code","566b977b":"code","4c13162d":"code","0169ac8d":"code","6c5fcc12":"code","2583f46e":"code","ef1e6913":"code","ee91ec75":"code","a2303fab":"code","efb5f313":"code","cd03de1a":"code","9be6748a":"code","1413ad6a":"code","43653826":"code","f70fbfdf":"code","e38094dd":"code","06997336":"code","77930ba8":"code","0f41d74e":"code","1d50ee8d":"code","4596eab2":"code","74f74f09":"code","c7de2e15":"code","3a543b69":"code","1415b116":"code","45e14515":"code","467d2165":"code","38397657":"code","481d83aa":"code","22111adb":"code","8a295c89":"code","af919ed8":"code","b09ade20":"code","9ca6bf92":"code","2bf7b40f":"code","f922c497":"code","d046a032":"code","81257629":"code","b01f91de":"code","d94e6b44":"code","c99df3fc":"code","99dff84b":"code","9d5831d7":"code","f99f9d39":"code","b2b92280":"code","f651a0ac":"code","c0887f93":"code","cf3f2739":"code","e5069183":"code","e7db5ef0":"code","930ff3c6":"code","3536104f":"code","3b07f8a0":"code","9ce87d07":"code","0450ece9":"code","c95d1e97":"code","7f7a0071":"code","526f2e4f":"code","dba4a2ef":"code","fa181725":"code","e217919a":"code","2bc88a43":"code","988643f4":"code","fe74a7e1":"code","95ad41c0":"code","7380d54c":"code","e7f87bb1":"code","d16b63a5":"code","498770d5":"code","0ddee292":"code","54c8785d":"code","c04caa68":"code","e247d661":"code","3f8c7106":"code","95e83829":"code","cc5c2034":"code","1a15ae10":"code","fd0dd9ed":"code","3507a2d6":"code","ab8888ab":"code","0089e745":"code","caaffc5b":"code","9cc5d0a8":"code","a63457b4":"code","31cbb638":"code","64ecc407":"code","835c5c3f":"code","8d8d4c2f":"code","5c471185":"code","bbca9f7d":"code","9210a6bd":"code","2577b5fe":"code","312ffe39":"code","6cd3e208":"code","d837fb1f":"code","526ad4f6":"code","16d057d5":"code","1d721928":"code","1a42f4ff":"code","8845edb4":"code","1c700183":"code","d486e575":"code","30c65591":"code","f7465d3c":"code","0f616498":"code","f50e0afd":"code","5865ab91":"code","0a28ee76":"code","ea25ccdb":"code","95977b3b":"code","a3bac17c":"code","617875fa":"code","f4420261":"code","b6a4a61a":"code","ba442349":"code","17d85c58":"code","45086056":"code","2ea56e39":"code","9b7c57b4":"code","dc662f3a":"code","75455e78":"code","f00f1823":"code","df48055b":"code","11fa4855":"code","8afaf222":"code","79dd188b":"code","e3a5277f":"code","ce940747":"code","fddcd7fa":"code","8cc75e20":"code","a43bc39e":"code","35f02a06":"code","94a38c9d":"code","6e52afe9":"code","73994586":"code","9c3df6a4":"code","e707ebc5":"code","e3e6a393":"code","93b00c08":"code","a3d124d9":"code","10e38b13":"code","6b83b4af":"code","314a8348":"code","c2b84313":"code","660156ad":"code","2384ca4e":"code","3a8c6a48":"code","6040d863":"code","565073b2":"code","27deb06b":"code","80291b93":"code","f24640ff":"code","7778def2":"code","525a816c":"code","fef15a1f":"code","678526e0":"code","00b1c5af":"code","6c40bc1e":"code","477d4508":"code","2b8429ab":"code","0efec248":"code","01a59b47":"code","a1c214b8":"code","86db4ced":"code","316f8cec":"code","84a4393c":"code","d44de59f":"code","c45d05d1":"code","110021f7":"code","6bccc07c":"code","b6f4a20b":"code","bfd609a8":"code","af953a0c":"code","62aecfa9":"code","bc4bb7ce":"code","9dfddfa9":"code","344638fb":"code","066c4d25":"code","3c0ac27d":"code","a6a8a989":"code","56dc6688":"code","1beb1235":"code","1e28f96c":"code","03b40709":"code","3e411545":"code","55b7df61":"code","1b9408a4":"code","944e0b17":"code","923bc3b2":"code","5ab24f40":"code","141ad4f4":"code","af16567c":"code","4094f1e4":"code","a1a492c1":"code","ac923ad8":"code","4094640a":"code","500e8782":"code","ff678fcb":"code","e6420051":"code","dfa37c26":"code","992aaf6b":"code","e238b36d":"code","77976bb4":"code","d634e429":"code","8cb1e33c":"code","db356efb":"code","ca8d6a2d":"code","6abec285":"code","ffa877a9":"code","c5853742":"code","eb9082f6":"code","2d8dfc2e":"code","591c338f":"markdown","6ce5ed0d":"markdown","5266df3d":"markdown","a8e04ff4":"markdown","b2928735":"markdown","c6715e82":"markdown","798ed9f3":"markdown","04da9cca":"markdown","0c3395e3":"markdown","77f3e59f":"markdown","6cd230ad":"markdown","b65edd6c":"markdown","038742ef":"markdown","796ab875":"markdown","a9e107b1":"markdown","09dfb04c":"markdown","293db53b":"markdown","18563dc6":"markdown","91248164":"markdown","735a4773":"markdown","adc6dee6":"markdown","14a04024":"markdown","238c0c1b":"markdown","0fa782cb":"markdown","46074812":"markdown","428b7e62":"markdown","ac418ca5":"markdown","108de3b7":"markdown","3d352b85":"markdown","f975ba00":"markdown","5954cb1a":"markdown","88dbfbac":"markdown","40eafa55":"markdown","621de1d0":"markdown","b9784f1f":"markdown","1c8754f9":"markdown","62b79023":"markdown","c1697774":"markdown","78dc2cf9":"markdown","913d3ae0":"markdown","4ee98812":"markdown","896c9199":"markdown","be6e0019":"markdown","edcbbd91":"markdown","2f57eb4a":"markdown","40b6053d":"markdown","3d186bc3":"markdown","91140d1f":"markdown","3a8fe2b1":"markdown","c9b50d8f":"markdown","4449fc75":"markdown","7a096e1b":"markdown","9f31fc94":"markdown","9568a22a":"markdown","832901ca":"markdown","f0e87481":"markdown","dc6a166e":"markdown","36ee580f":"markdown","6fe7ecf6":"markdown","c73bdfe3":"markdown","dfa49495":"markdown","25d67814":"markdown","751b1915":"markdown","b06f24a4":"markdown","7179a59a":"markdown","27a97b61":"markdown","395058a3":"markdown","1498d693":"markdown","f7101861":"markdown","57485fd6":"markdown","19b0a08a":"markdown","7a1bc782":"markdown","f3d4af92":"markdown","0ed03f61":"markdown","07b65a4d":"markdown","e3d6a2f3":"markdown","a4c883eb":"markdown","15e986a9":"markdown","38c652e8":"markdown","7b9ddb05":"markdown","0005733c":"markdown","4bf281ac":"markdown","b4f52408":"markdown","3b1592ba":"markdown","2c4d59d2":"markdown","02fdd17f":"markdown","97cbd4f1":"markdown","1f962315":"markdown","e00f36ac":"markdown","9373a917":"markdown","91733691":"markdown","ca7ad87c":"markdown","d1ea496b":"markdown","d8ee592a":"markdown","c893d833":"markdown","3afd239c":"markdown","247d9909":"markdown","b661545f":"markdown","4ae96dbd":"markdown","7c2e06cb":"markdown","16cf51dc":"markdown","42fa1362":"markdown","0ce0faac":"markdown","17f882bb":"markdown","aa59899b":"markdown","29d298cd":"markdown","52ee3bdc":"markdown","734d36e4":"markdown","fb03f107":"markdown","90532340":"markdown","4bb1ee6d":"markdown","d1b97f66":"markdown","467637b5":"markdown","cd5e875c":"markdown","497a6389":"markdown","764fd845":"markdown","a2f09786":"markdown","0182c327":"markdown","66db7892":"markdown","95e4e174":"markdown","8e3a70d7":"markdown","6d282d41":"markdown","20c19aa5":"markdown"},"source":{"caa9a37b":"!pip install lifetimes","f5a68cf9":"import pandas as pd\nfrom datetime import datetime\nstart_time = datetime.now()\n\n# need to install these libraries the first time you run the notebook\n# %pip install --user -U nltk\n# %conda install -c conda-forge wordcloud\n# %conda install -c plotly plotly\n# %pip install pandas-profiling\n# %pip install mlxtend\n# %pip install surprise\n# %pip install spotlight\n# %pip install scipy == 1.1.0\n# %pip install lifetimes\n\n#Because of issues with pandas profiling versions when installed using pip\/conda, this code is run to resolve it\n# %pip install https:\/\/github.com\/pandas-profiling\/pandas-profiling\/archive\/master.zip\n    \nimport nltk\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom lifetimes import BetaGeoFitter\nfrom scipy.special import logsumexp\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport matplotlib.cm as cm\nimport itertools\n# import pandas_profiling\nimport plotly.graph_objs as go\nimport math\n\n# from spotlight.interactions import Interactions\nfrom pathlib import Path\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom sklearn import preprocessing, model_selection, metrics, feature_selection\nfrom sklearn.model_selection import GridSearchCV, learning_curve\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import neighbors, linear_model, svm, tree, ensemble\nfrom scipy.spatial.distance import cdist\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.decomposition import PCA\nfrom IPython.display import display, HTML\nfrom datetime import datetime\nfrom plotly.offline import init_notebook_mode,iplot\nfrom sklearn.manifold import TSNE\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\n\ninit_notebook_mode(connected=True)\nwarnings.filterwarnings(\"ignore\")\n# plt.rcParams[\"patch.force_edgecolor\"] = True\nplt.style.use('fivethirtyeight')\nmpl.rc('patch', edgecolor = 'dimgray', linewidth=1)\n%matplotlib inline\nprint('LOAD TIME: ',datetime.now() - start_time) # Load duration about 2 minutes","15dc3b00":"# read the datafile\ndf_initial = pd.read_csv('..\/input\/dataskss\/data (7) (1).csv',encoding=\"ISO-8859-1\",\n                         dtype={'CustomerID': str,'InvoiceID': str})\nprint('Dataframe dimensions:', df_initial.shape)\n#______\ndf_initial['InvoiceDate'] = pd.to_datetime(df_initial['InvoiceDate'])\n#____________________________________________________________\n# gives some info on columns types and numer of null values\ntab_info=pd.DataFrame(df_initial.dtypes).T.rename(index={0:'column type'})\ntab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()).T.rename(index={0:'null values (nb)'}))\ntab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()\/df_initial.shape[0]*100).T.\n                         rename(index={0:'null values (%)'}))\ndisplay(tab_info)\n#__________________\n# show first lines\ndisplay(df_initial[:5])","525f222b":"# print first 5 rows\ndf_initial.head(5)","cd347467":"# Output a profile of the original, raw data\n# profile = pandas_profiling.ProfileReport(df_initial)\n# profile.to_file('Initial data profile.html')","20f93d9d":"## Look at entries with no customer ID\nno_cust = df_initial[df_initial[\"CustomerID\"].isnull() == True]\nno_cust.head()","aea992f3":"#print last 5 rows from the dataset\nno_cust.tail()","b1237b63":"#count the values \nno_cust[\"Country\"].value_counts()","256acc7b":"## Check to make sure that the CustomerIDs don't go as high as the InvoiceNo.\npd.DataFrame(df_initial[\"CustomerID\"].value_counts().reset_index().sort_values(by = \"index\",ascending = False))","d6bb04fa":"pd.DataFrame(df_initial[\"InvoiceNo\"].value_counts().reset_index().sort_values(by = \"index\",ascending = False))","662d4185":"# 1. study the empty Customer IDs seperately:\nno_cust","7b274e9a":"# StockCode\nplt.figure()\ntemp = pd.DataFrame(no_cust[\"StockCode\"].value_counts())\nplt.bar(temp.index[:15], temp.StockCode[:15])\nplt.xticks(rotation = 90)\nplt.xlabel(\"StockCode\")\nplt.ylabel(\"Count\")\nplt.title(\"Count of StockCode\")\nplt.show()","855b3744":"# 3. Description\nplt.figure()\ntemp = pd.DataFrame(no_cust[\"Description\"].value_counts())\nplt.bar(temp.index[:15], temp.Description[:15])\nplt.xticks(rotation = 90)\nplt.xlabel(\"Description\")\nplt.ylabel(\"Count\")\nplt.title(\"Count of Description\")\nplt.show()","e3111529":"# 4. Quantity\nplt.figure()\nplt.hist(no_cust[\"Quantity\"], bins = 100)\nplt.yscale(\"log\")\nplt.xlabel(\"Quantity\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of Quantity\")\nplt.show()","aec2d39e":"# Zoom in on the area between -2000 to 2000\ntemp = no_cust[(no_cust[\"Quantity\"]<2000) & (no_cust[\"Quantity\"]>-2000)]\nplt.figure()\nplt.hist(temp[\"Quantity\"], bins = 100)\nplt.yscale(\"log\")\nplt.xlabel(\"Quantity\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of Quantity\")\n\nplt.show()","a946bf3f":"# 5. InvoiceDate\nplt.figure()\ntemp = pd.DataFrame(no_cust[\"InvoiceDate\"].value_counts()).reset_index()\nplt.bar(temp[\"index\"][:15].astype(str), temp.InvoiceDate[:15])\nplt.xticks(rotation = 90)\nplt.xlabel(\"InvoiceDate\")\nplt.ylabel(\"Count\")\nplt.title(\"Count of Date\")\nplt.show()","bd9a07ae":"# 6. UnitPrice\nplt.figure()\nplt.hist(no_cust[\"UnitPrice\"], bins = 100)\n#plt.xticks(rotation = 90)\n#plt.xlim(-200,200)\nplt.yscale(\"log\")\nplt.xlabel(\"Price\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of UnitPrice\")\nplt.show()","dc4fc41d":"# Zoom in on the area between 0 to 2000\ntemp = no_cust[(no_cust[\"UnitPrice\"]<2000) & (no_cust[\"UnitPrice\"]>0)]\nplt.figure()\nplt.hist(temp[\"UnitPrice\"], bins = 100)\nplt.yscale(\"log\")\nplt.xlabel(\"Price\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of UnitPrice\")\nplt.show()","a7888a2b":"# 7. CustomerID\nplt.figure()\ntemp = pd.DataFrame(no_cust[\"CustomerID\"].value_counts())\nplt.bar(temp.index[:15], temp.CustomerID[:15])\nplt.xticks(rotation = 90)\nplt.xlabel(\"CustomerID\")\nplt.ylabel(\"Count\")\nplt.title(\"Count of CustomerID\")\nplt.show()","7d7d3f39":"# 8. Country\nplt.figure()\ntemp = pd.DataFrame(no_cust[\"Country\"].value_counts())\nplt.bar(temp.index[:15], temp.Country[:15])\nplt.yscale(\"log\")\nplt.xticks(rotation = 90)\nplt.xlabel(\"Country\")\nplt.ylabel(\"Count\")\nplt.title(\"Count of Country\")\nplt.show()","b32e9bb3":"# or, 2. alternatively, just drop the orders with no customer ID:\ndf_initial.dropna(axis = 0, subset = ['CustomerID'], inplace = True)\nprint('Dataframe dimensions:', df_initial.shape)","8209eae6":"df_initial.dropna(axis = 0, subset = ['CustomerID'], inplace = True)\nprint('Dataframe dimensions:', df_initial.shape)\n#____________________________________________________________\n# gives some info on columns types and numer of null values\ntab_info=pd.DataFrame(df_initial.dtypes).T.rename(index={0:'column type'})\ntab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()).T.rename(index={0:'null values (nb)'}))\ntab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()\/df_initial.shape[0]*100).T.\n                         rename(index={0:'null values (%)'}))\ndisplay(tab_info)","c5508740":"# drop duplicate entries.\nprint('Duplicate entries: {}'.format(df_initial.duplicated().sum()))\ndf_initial.drop_duplicates(inplace = True)","4d998c02":"# 1. InvoiceNo. - who made the biggest orders\ndf_initial[\"InvoiceNo\"].value_counts()","144f3a6e":"plt.figure()\ntemp = pd.DataFrame(df_initial[\"InvoiceNo\"].value_counts())\nplt.bar(temp.index[:15], temp.InvoiceNo[:15])\nplt.xticks(rotation = 90)\nplt.xlabel(\"InvoiceNo\")\nplt.ylabel(\"Count\")\nplt.title(\"Count of InvoiceNo\")\nplt.show()","ca6e97a8":"# StockCode\nplt.figure()\ntemp = pd.DataFrame(df_initial[\"StockCode\"].value_counts())\nplt.bar(temp.index[:15], temp.StockCode[:15])\nplt.xticks(rotation = 90)\nplt.xlabel(\"StockCode\")\nplt.ylabel(\"Count\")\nplt.title(\"Count of StockCode\")\nplt.show()","bb9da69c":"# 3. Description\nplt.figure()\ntemp = pd.DataFrame(df_initial[\"Description\"].value_counts())\nplt.bar(temp.index[:15], temp.Description[:15])\nplt.xticks(rotation = 90)\nplt.xlabel(\"Description\")\nplt.ylabel(\"Count\")\nplt.title(\"Count of Description\")\nplt.show()","a81e030d":"# 4. Quantity\nplt.figure()\nplt.hist(df_initial[\"Quantity\"], bins = 100)\nplt.yscale(\"log\")\nplt.xlabel(\"Quantity\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of Quantity\")\nplt.show()","dfba87bc":"# Zoom in on the area between -5000 to 5000\ntemp = df_initial[(df_initial[\"Quantity\"]<5000) & (df_initial[\"Quantity\"]>-5000)]\nplt.figure()\nplt.hist(temp[\"Quantity\"], bins = 100)\nplt.yscale(\"log\")\nplt.xlabel(\"Quantity\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of Quantity\")\n\nplt.show()","80661411":"# 5. InvoiceDate\nplt.figure()\ntemp = pd.DataFrame(df_initial[\"InvoiceDate\"].value_counts()).reset_index()\nplt.bar(temp[\"index\"][:15].astype(str), temp.InvoiceDate[:15])\nplt.xticks(rotation = 90)\nplt.xlabel(\"InvoiceDate\")\nplt.ylabel(\"Count\")\nplt.title(\"Count of Date\")\nplt.show()","65107f77":"# 6. UnitPrice\nplt.figure()\nplt.hist(df_initial[\"UnitPrice\"], bins = 100)\n#plt.xticks(rotation = 90)\n#plt.xlim(-200,200)\nplt.yscale(\"log\")\nplt.xlabel(\"Price\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of UnitPrice\")\nplt.show()","30d108e5":"# Zoom in on the area between 0 to 2000\ntemp = df_initial[(df_initial[\"UnitPrice\"]<2000) & (df_initial[\"UnitPrice\"]>0)]\nplt.figure()\nplt.hist(temp[\"UnitPrice\"], bins = 100)\nplt.yscale(\"log\")\nplt.xlabel(\"Price\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of UnitPrice\")\nplt.show()","eb0b3c1a":"# 7. CustomerID\nplt.figure()\ntemp = pd.DataFrame(df_initial[\"CustomerID\"].value_counts())\nplt.bar(temp.index[:15], temp.CustomerID[:15])\nplt.xticks(rotation = 90)\nplt.xlabel(\"CustomerID\")\nplt.ylabel(\"Count\")\nplt.title(\"Count of CustomerID\")\nplt.show()","97e21be3":"# 8. Country\nplt.figure()\ntemp = pd.DataFrame(df_initial[\"Country\"].value_counts())\nplt.bar(temp.index[:15], temp.Country[:15])\nplt.yscale(\"log\")\nplt.xticks(rotation = 90)\nplt.xlabel(\"Country\")\nplt.ylabel(\"Count\")\nplt.title(\"Count of Country\")\nplt.show()","4d05750d":"temp = df_initial[['CustomerID', 'InvoiceNo', 'Country']].groupby(['CustomerID', 'InvoiceNo', 'Country']).count()\ntemp = temp.reset_index(drop = False)\ncountries = temp['Country'].value_counts()\nprint('Number of countries in the data: {}'.format(len(countries)))","87ba52f8":"data = dict(type='choropleth',\nlocations = countries.index,\nlocationmode = 'country names', z = countries,\ntext = countries.index, colorbar = {'title':'Order nb.'},\ncolorscale=[[0, 'rgb(224,255,255)'],\n            [0.01, 'rgb(166,206,227)'], [0.02, 'rgb(31,120,180)'],\n            [0.03, 'rgb(178,223,138)'], [0.05, 'rgb(51,160,44)'],\n            [0.10, 'rgb(251,154,153)'], [0.20, 'rgb(255,255,0)'],\n            [1, 'rgb(227,26,28)']],    \nreversescale = False)\n#_______________________\nlayout = dict(title='Number of orders per country',\ngeo = dict(showframe = True, projection={'type':'mercator'}))\n#______________\nchoromap = go.Figure(data = [data], layout = layout)\niplot(choromap, validate=False)","afe0b193":"# I also explored which countries accounted for the most unique customers\nforhist = temp.groupby('Country')['CustomerID'].nunique().reset_index().sort_values('CustomerID', ascending = False)\nforhist.reset_index(inplace = True)\nforhist.drop('index', axis = 1, inplace = True)\ndisplay(forhist.head(10))","87e4e897":"# String to be searched in start of string  \nsearch =\"C\"\n# boolean series returned \ndf_initial[df_initial[\"InvoiceNo\"].str.startswith(search)]","d1555799":"pd.DataFrame([{'products': len(df_initial['StockCode'].value_counts()),    \n               'transactions': len(df_initial['InvoiceNo'].value_counts()),\n               'customers': len(df_initial['CustomerID'].value_counts()),  \n              }], columns = ['products', 'transactions', 'customers'], index = ['quantity'])","404d6e63":"temp = df_initial.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['InvoiceDate'].count()\nnb_products_per_basket = temp.rename(columns = {'InvoiceDate':'Number of products'})\nnb_products_per_basket[:10].sort_values('CustomerID')","c8a68e04":"nb_products_per_basket['order_canceled'] = nb_products_per_basket['InvoiceNo'].apply(lambda x:int('C' in x))\ndisplay(nb_products_per_basket[:5])\n#______________________________________________________________________________________________\nn1 = nb_products_per_basket['order_canceled'].sum()\nn2 = nb_products_per_basket.shape[0]\nprint('Number of cancellation transactions among all orders: {}\/{} ({:.2f}%) '.format(n1, n2, n1\/n2*100))","1d73e53d":"display(df_initial.sort_values('CustomerID')[:5])","f54283d7":"df_check = df_initial[df_initial['Quantity'] < 0][['CustomerID','Quantity',\n                                                   'StockCode','Description','UnitPrice']]\nfor index, col in  df_check.iterrows():\n    if df_initial[(df_initial['CustomerID'] == col[0]) & (df_initial['Quantity'] == -col[1]) \n                & (df_initial['Description'] == col[2])].shape[0] == 0: \n        print(df_check.loc[index])\n        print(15*'-'+'>'+' HYPOTHESIS NOT FULFILLED')\n        break","c142c2a2":"df_check = df_initial[(df_initial['Quantity'] < 0) & (df_initial['Description'] != 'Discount')][\n                                 ['CustomerID','Quantity','StockCode',\n                                  'Description','UnitPrice']]\nfor index, col in  df_check.iterrows():\n    if df_initial[(df_initial['CustomerID'] == col[0]) & (df_initial['Quantity'] == -col[1]) \n                & (df_initial['Description'] == col[2])].shape[0] == 0: \n        print(index, df_check.loc[index])\n        print(15*'-'+'>'+' HYPOTHESIS NOT FULFILLED')\n        break","ec6fc9a5":"# try to match all cancellation orders with all previous purchase orders\ncancels=df_initial[(df_initial['Description']!='Discount')&(df_initial['Quantity']<0)].copy()\ncancels['cancel_idx']=cancels.index\ndf_match=df_initial.copy()\ndf_match['match_idx']=df_match.index\ndf_match=df_match[df_match['Quantity']>0]\nmg=cancels.merge(df_match[['InvoiceDate','CustomerID','StockCode','Quantity','match_idx']],how='left',\n              left_on=['CustomerID','StockCode'],right_on=['CustomerID','StockCode'])\nmatched=mg[mg['InvoiceDate_x']>mg['InvoiceDate_y']]\nmatched['match_idx']=matched['match_idx'].astype(int)\nmatched['Quantity_y']=matched['Quantity_y'].astype(int)\n# sort data to make sure matching starts with the latest purchase order\nmatched=matched.sort_values(['cancel_idx','InvoiceDate_y'],ascending=[1,0])","f63eca8f":"# since one purchase order can be matched to multiple cancellation orders,\n# it can happen that one purchase order is accounted for too many cancellation orders\n# e.g. the purchase order only listed 8 units in quantity, \n# but is used to match multiple cancallation orders of which the quantities sum up to more than 8\n# in other words, this purchase order is double counted\n# the following codes handle this problem\ncount=1\nwhile True:\n    # if a customer returns items that were bought through multiple purchases\n    # start accounts for the returned quantity from the latest order\n    # until all returned\/canceled quantity is accounted for and linked to a purchase order\n    # it is possible that not all the canceled quantity can be accounted for\n    matched['tot_bought']=matched.groupby('cancel_idx')['Quantity_y'].cumsum()\n    matched['tot_bought_lag1']=matched.groupby('cancel_idx')['tot_bought'].shift(1).fillna(0)\n    matched['QuantityCanceled']=matched[['Quantity_x','Quantity_y','tot_bought','tot_bought_lag1']].\\\n                                 apply(lambda x: min(-x[0],x[1]) if x[3]==0 else \\\n                                       (x[1] if x[2]<=-x[0] else \\\n                                       (0 if x[3]>=-x[0] else -x[0]-x[3])), axis=1).astype(int)\n    # add a column to track the total quantity a given purchae order has been used to account for a cancellation\n    # if the total quantity counted exceeds the number of units that were bought in this order\n    # remove this purchase order from the candidate lists of all cancellation orders from this point on\n    # iterate one more time to link cancellations to purchases\n    matched['tot_QuantityCanceled']=matched.groupby('match_idx')['QuantityCanceled'].cumsum()\n    double_count=len(matched[matched['tot_QuantityCanceled']>matched['Quantity_y']])\n    print('Iteration {} double counted: {} times'.format(count, double_count))\n    if double_count>0:\n        matched=matched[matched['tot_QuantityCanceled']<=matched['Quantity_y']]\n        count+=1\n    else:\n        print('End iteration')\n        break","975bbe07":"tot_canceled=matched.groupby('cancel_idx')['Quantity_x'].mean()*(-1)\ntot_bought=matched.groupby('cancel_idx')['Quantity_y'].sum()\n# some but not all units of the cancellation are accounted for and linked to purchase orders\npartial_doubtful_entry=tot_canceled[tot_canceled>tot_bought].index.tolist()\n# all units of the cancellation are accounted for and linked to purchase orders\nentry_to_remove=tot_canceled[tot_canceled<=tot_bought].index.tolist()\n# no previous purchase that match the cancellation can be found\ndoubtful_entry=list(set(mg['cancel_idx'].unique())-set(partial_doubtful_entry)-set(entry_to_remove))","5fe9994d":"df_cleaned = df_initial.copy(deep = True)\ndf_cleaned=df_cleaned.merge(pd.DataFrame(matched[['match_idx','tot_QuantityCanceled']].groupby('match_idx').max()),\n                            how='left',left_index=True, right_index=True)\ndf_cleaned.rename(columns={'tot_QuantityCanceled':'QuantityCanceled'},inplace=True)\ndf_cleaned['QuantityCanceled']=df_cleaned['QuantityCanceled'].fillna(0).astype(int)","d671d059":"df_cleaned.head()","8a2c000f":"print(f\"Entry_to_remove: {len(entry_to_remove)} ({len(entry_to_remove)\/len(df_initial)*100:.2f}%)\")\nprint(f\"Doubtful_entry: {len(doubtful_entry)} ({len(doubtful_entry)\/len(df_initial)*100:.2f}%)\")\nprint(f\"Partial_doubtful_entry: {len(partial_doubtful_entry)} ({len(partial_doubtful_entry)\/len(df_initial)*100:.2f}%)\")","2c3707cb":"df_cleaned.drop(entry_to_remove, axis = 0, inplace = True)\ndf_cleaned.drop(doubtful_entry, axis = 0, inplace = True)\ndf_cleaned.drop(partial_doubtful_entry, axis = 0, inplace = True)\nremaining_entries = df_cleaned[(df_cleaned['Quantity'] < 0) & (df_cleaned['StockCode'] != 'D')]\nprint(\"number of entries to delete: {}\".format(remaining_entries.shape[0]))","909b150a":"# Output a profile of the cleaned data\n# profile = pandas_profiling.ProfileReport(df_cleaned)\n# profile.to_file('Cleaned data profile.html')","b95f10e7":"df_cleaned['Month'] = df_cleaned[\"InvoiceDate\"].map(lambda x: x.month)\ndf_cleaned['Weekday'] = df_cleaned[\"InvoiceDate\"].map(lambda x: x.weekday())\ndf_cleaned['Day'] = df_cleaned[\"InvoiceDate\"].map(lambda x: x.day)\ndf_cleaned['Hour'] = df_cleaned[\"InvoiceDate\"].map(lambda x: x.hour)","6eec053a":"df_cleaned['Month'].value_counts()","1d4d86e9":"df_cleaned['Weekday'].value_counts()","5cbe2dff":"df_cleaned['Day'].value_counts()","e64ee564":"df_cleaned['Hour'].value_counts()","83caf50f":"plt.hist(df_cleaned['Month'], bins=12)","650af389":"plt.hist(df_cleaned['Weekday'], bins=7)","566b977b":"plt.hist(df_cleaned['Day'], bins=31)","4c13162d":"plt.hist(df_cleaned['Hour'], bins=24)","0169ac8d":"list_special_codes = df_cleaned[df_cleaned['StockCode'].str.contains('^[a-zA-Z]+', regex=True)]['StockCode'].unique()\nlist_special_codes","6c5fcc12":"for code in list_special_codes:\n    print(\"{:<15} -> {:<30}\".format(code, df_cleaned[df_cleaned['StockCode'] == code]['Description'].unique()[0]))","2583f46e":"print(\"{} entries removed\".format(len(df_cleaned[df_cleaned['StockCode'].isin(list_special_codes)])))\ndf_cleaned=df_cleaned[-df_cleaned['StockCode'].isin(list_special_codes)]","ef1e6913":"df_cleaned['TotalPrice'] = df_cleaned['UnitPrice'] * (df_cleaned['Quantity'] - df_cleaned['QuantityCanceled'])\ndf_cleaned.sort_values('CustomerID')[:5]","ee91ec75":"revenue_per_countries = df_cleaned.groupby([\"Country\"])[\"TotalPrice\"].sum().sort_values()\nrevenue_per_countries.plot(kind='bar', figsize=(15,12))\nplt.yscale('log')\nplt.title(\"Revenue per Country\")","a2303fab":"#___________________________________________\n# sum of purchases \/ user and orders\ntemp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['TotalPrice'].sum()\nbasket_price = temp.rename(columns = {'TotalPrice':'Basket Price'})\n#_____________________\n# order date\ndf_cleaned['InvoiceDate_int'] = df_cleaned['InvoiceDate'].astype('int64')\ntemp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['InvoiceDate_int'].mean()\ndf_cleaned.drop('InvoiceDate_int', axis = 1, inplace = True)\nbasket_price.loc[:, 'InvoiceDate'] = pd.to_datetime(temp['InvoiceDate_int'])\n#______________________________________\n# selection of significant entries:\n# basket_price = basket_price[basket_price['Basket Price'] > 0]\nbasket_price.sort_values('CustomerID')[:6]","efb5f313":"# Show distribution of basket prices in pie chart. \n# Purchase count\nprice_range = [0, 50, 100, 200, 500, 1000, 5000, 50000]\ncount_price = []\nfor i, price in enumerate(price_range):\n    if i == 0: continue\n    val = basket_price[(basket_price['Basket Price'] < price) &\n                       (basket_price['Basket Price'] > price_range[i-1])]['Basket Price'].count()\n    count_price.append(val)\n# Representation of the number of purchases \/ amount        \nplt.rc('font', weight='bold')\nf, ax = plt.subplots(figsize=(11, 6))\ncolors = ['yellowgreen', 'gold', 'wheat', 'c', 'violet', 'royalblue','firebrick']\nlabels = [ '{}<.<{}'.format(price_range[i-1], s) for i,s in enumerate(price_range) if i != 0]\nsizes  = count_price\nexplode = [0.0 if sizes[i] < 100 else 0.0 for i in range(len(sizes))]\nax.pie(sizes, explode = explode, labels=labels, colors = colors,\n       autopct = lambda x:'{:1.0f}%'.format(x) if x > 1 else '',\n       shadow = False, startangle=0)\nax.axis('equal')\nf.text(0.5, 1.01, \"Distribution of order amounts\", ha='center', fontsize = 18);","cd03de1a":"# make histogram distributions of the basket prices \nplt.hist(basket_price['Basket Price'], bins=20)\nplt.yscale('log')","9be6748a":"xmax = 1000\n# plt.yscale('log')\ntemp = basket_price[basket_price['Basket Price'] <= xmax]\nplt.hist(temp['Basket Price'], bins=50)\nplt.show()","1413ad6a":"df_cleaned.Quantity.describe()","43653826":"is_noun = lambda pos: pos[:2] == 'NN'\ndef keywords_inventory(dataframe, colonne = 'Description'):\n    stemmer = nltk.stem.SnowballStemmer(\"english\")\n    keywords_roots  = dict()  # collect the words \/ root\n    keywords_select = dict()  # association: root <-> keyword\n    category_keys   = []\n    count_keywords  = dict()\n    icount = 0\n    for s in dataframe[colonne]:\n        if pd.isnull(s): continue\n        lines = s.lower()\n        tokenized = nltk.word_tokenize(lines)\n        nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n        \n        for t in nouns:\n            t = t.lower() ; racine = stemmer.stem(t)\n            if racine in keywords_roots:                \n                keywords_roots[racine].add(t)\n                count_keywords[racine] += 1                \n            else:\n                keywords_roots[racine] = {t}\n                count_keywords[racine] = 1\n    \n    for s in keywords_roots.keys():\n        if len(keywords_roots[s]) > 1:  \n            min_length = 1000\n            for k in keywords_roots[s]:\n                if len(k) < min_length:\n                    clef = k ; min_length = len(k)            \n            category_keys.append(clef)\n            keywords_select[s] = clef\n        else:\n            category_keys.append(list(keywords_roots[s])[0])\n            keywords_select[s] = list(keywords_roots[s])[0]\n                   \n    print(\"Nb of keywords in variable '{}': {}\".format(colonne,len(category_keys)))\n    return category_keys, keywords_roots, keywords_select, count_keywords","f70fbfdf":"df_produits = pd.DataFrame(df_initial['Description'].unique()).rename(columns = {0:'Description'})","e38094dd":"keywords, keywords_roots, keywords_select, count_keywords = keywords_inventory(df_produits)","06997336":"list_products = []\nfor k,v in count_keywords.items():\n    list_products.append([keywords_select[k],v])\nlist_products.sort(key = lambda x:x[1], reverse = True)","77930ba8":"liste = sorted(list_products, key = lambda x:x[1], reverse = True)\n#_______________________________\nplt.rc('font', weight='normal')\nfig, ax = plt.subplots(figsize=(7, 25))\ny_axis = [i[1] for i in liste[:125]]\nx_axis = [k for k,i in enumerate(liste[:125])]\nx_label = [i[0] for i in liste[:125]]\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 13)\nplt.yticks(x_axis, x_label)\nplt.xlabel(\"Nb. of occurences\", fontsize = 18, labelpad = 10)\nax.barh(x_axis, y_axis, align = 'center',color='red')\nax = plt.gca()\nax.invert_yaxis()\n#_______________________________________________________________________________________\nplt.title(\"Words occurence\",bbox={'facecolor':'k', 'pad':5}, color='orange',fontsize = 25)\nplt.show()","0f41d74e":"# use predefined list of stopwords from NLTK package\nnltk_stopwords = set(stopwords.words('english'))\n# basic function that removes matched stopwords from a dictionary object\ndef remove_stopwords(corpus: dict):\n    temp_dict = {} # create a temporary dictionary\n    tokens = keywords_select.keys()\n    filtered_tokens = list(filter(lambda token: token not in nltk_stopwords, tokens)) # filter for stopwords\n    for i in filtered_tokens: # update the temporary dictionary\n        temp_dict.update({i: i})\n    print(f'{len(keywords_select.keys())-len(temp_dict.keys())} stopwords removed')\n    return temp_dict","1d50ee8d":"keywords_select = remove_stopwords(keywords_select) # also prints how many stopwords were removed","4596eab2":"# reflect the change on 'count_keywords' dict, too\ncount_keywords = {k: v for k, v in count_keywords.items() if k in keywords_select.keys()}","74f74f09":"# remove some very common words\nlist_products = []\nfor k,v in count_keywords.items():\n    word = keywords_select[k]\n    if word in ['pink', 'blue', 'tag', 'green', 'orange','candl','design','vintag',\\\n                'box', 'sign', 'vintage', 'glass', 'light', 'candle', 'flower']: continue\n    if len(word) < 5 or v < 13: continue\n    if ('+' in word) or ('\/' in word): continue\n    list_products.append([word, v])\n#______________________________________________________    \nlist_products.sort(key = lambda x:x[1], reverse = True)\nprint('preserved words:', len(list_products))","c7de2e15":"liste = sorted(list_products, key = lambda x:x[1], reverse = True)\n#_______________________________\nplt.rc('font', weight='normal')\nfig, ax = plt.subplots(figsize=(7, 25))\ny_axis = [i[1] for i in liste[:125]]\nx_axis = [k for k,i in enumerate(liste[:125])]\nx_label = [i[0] for i in liste[:125]]\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 13)\nplt.yticks(x_axis, x_label)\nplt.xlabel(\"Nb. of occurences\", fontsize = 18, labelpad = 10)\nax.barh(x_axis, y_axis, align = 'center',color='red')\nax = plt.gca()\nax.invert_yaxis()\n#_______________________________________________________________________________________\nplt.title(\"Words occurence\",bbox={'facecolor':'k', 'pad':5}, color='orange',fontsize = 25)\nplt.show()","3a543b69":"liste_produits = df_cleaned['Description'].unique()\nX = pd.DataFrame()\nfor key, occurence in list_products:\n    X.loc[:, key] = list(map(lambda x:int(key.upper() in x), liste_produits))\nX","1415b116":"threshold = [0, 1, 2, 3, 5, 10]\nlabel_col = []\nfor i in range(len(threshold)):\n    if i == len(threshold)-1:\n        col = '.>{}'.format(threshold[i])\n    else:\n        col = '{}<.<{}'.format(threshold[i],threshold[i+1])\n    label_col.append(col)\n    X.loc[:, col] = 0\n\ndef categorize_price(price):\n    j=0\n    while price > threshold[j+1]:\n        j+=1\n        if j+1 == len(threshold): break\n    return label_col[j]\n\nprod_prices=df_cleaned[['Description','UnitPrice']].groupby('Description').mean()\nprod_prices['PriceRange']=prod_prices['UnitPrice'].apply(categorize_price)\nfor col in label_col:\n    prods=prod_prices[prod_prices['PriceRange']==col].index\n    X.loc[[list(liste_produits).index(prod) for prod in prods],col]=1\nX","45e14515":"# Return frequency of products by price range. \nX.iloc[:,-6:].sum()\n# print(\"{:<8} {:<20} \\n\".format('gamme', 'nb. produits') + 20*'-')\n# for i in range(len(threshold)):\n#     if i == len(threshold)-1:\n#         col = '.>{}'.format(threshold[i])\n#     else:\n#         col = '{}<.<{}'.format(threshold[i],threshold[i+1])    \n#     print(\"{:<10}  {:<20}\".format(col, X.loc[:, col].sum()))","467d2165":"prod_prices.groupby('PriceRange').size()","38397657":"def calculate_cosine_similarity(a,b):\n    \"\"\"Calculates the cosine similarity between two vectors\"\"\"\n    dot_product=np.dot(a,b)\n    len_a=np.sqrt(np.dot(a,a))\n    len_b=np.sqrt(np.dot(b,b))\n    return dot_product\/(len_a*len_b+1e-9)\n\ndef get_closest_center(v,centers):\n    \"\"\"Find the center that given vector is closest to based on cosine similarity\"\"\"\n    distances=[calculate_cosine_similarity(c,v) for c in centers]\n    return distances.index(max(distances))\n\ndef get_center(c):\n    \"\"\"Calculate the center of a list of vectors by averaging the vectors after normalizing them to unit length\"\"\"\n    return np.array([[i\/np.sqrt(sum(i*i))] for i in c]).mean(axis=0)[0]\n\nclass KMeans_cosine(object):\n    \"\"\"KMeans clustering based on cosine distance\"\"\"\n    def __init__(self,vectors,k,max_iter=30,min_sample=10,init_centers=None):\n        self.vectors=vectors\n        self.n=len(self.vectors)\n        self.max_iter=max_iter\n        self.min_sample=min_sample\n        if init_centers is None:\n            self.k=k\n            self.centers=self.vectors[np.random.choice(self.n,self.k)]\n        else:\n            self.centers=init_centers\n            self.k=len(init_centers)\n        self.clusters=[[] for c in range(self.k)]\n        self.labels=[]\n        \n    def update_cluster(self):\n        \"\"\"Assign vectors to the centers they are closest to\"\"\"\n        labels=[]\n        for v in self.vectors:\n            index=get_closest_center(v,self.centers)\n            labels.append(index)\n            self.clusters[index].append(v)\n        self.labels=labels\n    \n    def update_center(self):\n        \"\"\"Calculate the center of a list of vectors\"\"\"\n        if min([len(c) for c in self.clusters])<self.min_sample:\n            self.centers=self.vectors[np.random.choice(self.n,self.k)]\n            self.clusters=[[] for c in range(self.k)]\n            self.labels=[]\n            return True\n        \n        new_centers=[]\n        for c in self.clusters:\n            center=get_center(c)\n            new_centers.append(center)\n        \n        new_centers=np.array(new_centers)\n        if (self.centers==new_centers).all():\n            return False\n        \n        self.centers=new_centers\n        self.clusters=[[] for c in range(self.k)]\n        return True\n        \n    def fit(self):\n        \"\"\"Iteratively move centers until they have stabilized or the maximum number of iteration has reached\"\"\"\n        self.update_cluster()\n        count=0\n        while self.update_center():\n            self.update_cluster()\n            count+=1\n            if count==self.max_iter: \n                break","481d83aa":"# generate the cosine distance matrix\ndist_matrix=cdist(X,X,'cosine')","22111adb":"# iter_n=100\n# avg_silhouette_scores={}\n# for n_clusters in range(3,9):\n#     tot_score=0\n#     for i in range(iter_n):\n#         kmeans = KMeans_cosine(X.values,n_clusters,min_sample=50)\n#         kmeans.fit()\n#         clusters=kmeans.labels\n#         silhouette_avg = silhouette_score(dist_matrix, clusters, metric='precomputed')\n#         tot_score+=silhouette_avg\n#     avg_score=tot_score\/iter_n\n#     avg_silhouette_scores[n_clusters]=avg_score\n#     print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", avg_score)","8a295c89":"n_clusters=6\nsilhouette_avg=0\n# init_centers=pd.read_csv('init_centers.csv').values\n\nniter = 0\nwhile silhouette_avg<0.53:\n    #init_centers is provided to replicate one optimal solution I found through many iterations\n#     kmeans = KMeans_cosine(X.values,n_clusters,min_sample=50,init_centers=init_centers)\n    kmeans = KMeans_cosine(X.values,n_clusters,min_sample=50)\n    kmeans.fit()\n    clusters=kmeans.labels\n    silhouette_avg = silhouette_score(dist_matrix, clusters, metric='precomputed')\n    print(niter, \"The average silhouette_score is :\", silhouette_avg)\n    niter = niter+1\nclusters=np.array(clusters)","af919ed8":"prods=pd.DataFrame([liste_produits,clusters]).T\nprods.columns=['prod','cluster']\nprods['cluster'].value_counts()","b09ade20":"prods[prods['cluster']==0].head(20)","9ca6bf92":"prods[prods['cluster']==1].head(20)","2bf7b40f":"prods[prods['cluster']==2].head(20)","f922c497":"prods[prods['cluster']==3].head(20)","d046a032":"prods[prods['cluster']==4].head(20)","81257629":"prods[prods['cluster']==5].head(20)","b01f91de":"cluster_price=X[['0<.<1', '1<.<2', '2<.<3', '3<.<5','5<.<10', '.>10']].copy()\ncluster_price['cluster']=clusters\ncluster_price.groupby('cluster').mean()","d94e6b44":"def graph_component_silhouette(n_clusters, lim_x, mat_size, sample_silhouette_values, clusters):\n#     plt.rcParams[\"patch.force_edgecolor\"] = True\n    plt.style.use('fivethirtyeight')\n    mpl.rc('patch', edgecolor = 'dimgray', linewidth=1)\n    #____________________________\n    fig, ax1 = plt.subplots(1, 1)\n    fig.set_size_inches(8, 8)\n    ax1.set_xlim([lim_x[0], lim_x[1]])\n    ax1.set_ylim([0, mat_size + (n_clusters + 1) * 10])\n    y_lower = 10\n    for i in range(n_clusters):\n        #___________________________________________________________________________________\n        # Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n        ith_cluster_silhouette_values = sample_silhouette_values[clusters == i]\n        ith_cluster_silhouette_values.sort()\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n        cmap = cm.get_cmap(\"Spectral\")\n        color = cmap(float(i) \/ n_clusters)        \n        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values,\n                           facecolor=color, edgecolor=color, alpha=0.8)\n        #____________________________________________________________________\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.03, y_lower + 0.5 * size_cluster_i, str(i), color = 'red', fontweight = 'bold',\n                bbox=dict(facecolor='white', edgecolor='black', boxstyle='round, pad=0.3'))\n        #______________________________________\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  ","c99df3fc":"#____________________________________\n# define individual silouhette scores\nsample_silhouette_values = silhouette_samples(dist_matrix, clusters,metric=\"precomputed\")\n#__________________\n# and do the graph\ngraph_component_silhouette(n_clusters, [-0.05, 0.7], len(X), sample_silhouette_values, clusters)","99dff84b":"liste = pd.DataFrame(liste_produits)\nliste_words = [word for (word, occurence) in list_products]\noccurence = [dict() for _ in range(n_clusters)]\nfor i in range(n_clusters):\n    liste_cluster = liste.loc[clusters == i]\n    for word in liste_words:\n        if word in ['art', 'set', 'heart', 'pink', 'blue', 'tag',]: continue\n        occurence[i][word] = sum(liste_cluster.loc[:, 0].str.contains(word.upper()))","9d5831d7":"#________________________________________________________________________\ndef random_color_func(word=None, font_size=None, position=None,\n                      orientation=None, font_path=None, random_state=None):\n    h = int(360.0 * tone \/ 255.0)\n    s = int(100.0 * 255.0 \/ 255.0)\n    l = int(100.0 * float(random_state.randint(70, 120)) \/ 255.0)\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n#________________________________________________________________________\ndef make_wordcloud(liste, increment):\n    ax1 = fig.add_subplot(4,2,increment)\n    words = dict()\n    trunc_occurences = liste[0:150]\n    for s in trunc_occurences:\n        words[s[0]] = s[1]\n    #________________________________________________________\n    wordcloud = WordCloud(width=1000,height=400, background_color='lightgrey', \n                          max_words=1628,relative_scaling=1,\n                          color_func = random_color_func,\n                          normalize_plurals=False)\n    wordcloud.generate_from_frequencies(words)\n    ax1.imshow(wordcloud, interpolation=\"bilinear\")\n    ax1.axis('off')\n    plt.title('cluster n\u00ba{}'.format(increment-1))\n#________________________________________________________________________\nfig = plt.figure(1, figsize=(14,14))\ncolor = [0, 160, 130, 95, 280, 40, 330, 110, 25]\nfor i in range(n_clusters):\n    list_cluster_occurences = occurence[i]\n\n    tone = color[i] # define the color of the words\n    liste = []\n    for key, value in list_cluster_occurences.items():\n        liste.append([key, value])\n    liste.sort(key = lambda x:x[1], reverse = True)\n    make_wordcloud(liste, i+1)            ","f99f9d39":"# Perform PCA to subsequently check distinctness between clusters.\nmatrix=X.values\npca = PCA()\npca.fit(matrix)\npca_samples = pca.transform(matrix)","b2b92280":"fig, ax = plt.subplots(figsize=(14, 5))\nsns.set(font_scale=1)\nplt.step(range(matrix.shape[1]), pca.explained_variance_ratio_.cumsum(), where='mid',\n         label='cumulative explained variance')\nsns.barplot(np.arange(1,matrix.shape[1]+1), pca.explained_variance_ratio_, alpha=0.5, color = 'green',\n            label='individual explained variance')\nplt.xlim(0, 100)\nax.set_xticklabels([s if int(s.get_text())%2 == 0 else '' for s in ax.get_xticklabels()])\nplt.ylabel('Explained variance', fontsize = 14)\nplt.xlabel('Principal components', fontsize = 14)\nplt.legend(loc='upper left', fontsize = 13);","f651a0ac":"pca = PCA(n_components=50)\nmatrix_9D = pca.fit_transform(matrix)\nmat = pd.DataFrame(matrix_9D)\nmat['cluster'] = pd.Series(clusters)","c0887f93":"import matplotlib.patches as mpatches\nsns.set_style(\"white\")\nsns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2.5})\nLABEL_COLOR_MAP = {0:'r', 1:'gold', 2:'b', 3:'k', 4:'c', 5:'g'}\nlabel_color = [LABEL_COLOR_MAP[l] for l in mat['cluster']]\nfig = plt.figure(figsize = (15,8))\nincrement = 0\nfor ix in range(4):\n    for iy in range(ix+1, 4):    \n        increment += 1\n        ax = fig.add_subplot(2,3,increment)\n        ax.scatter(mat[ix], mat[iy], c= label_color, alpha=0.4) \n        plt.ylabel('PCA {}'.format(iy+1), fontsize = 12)\n        plt.xlabel('PCA {}'.format(ix+1), fontsize = 12)\n        ax.yaxis.grid(color='lightgray', linestyle=':')\n        ax.xaxis.grid(color='lightgray', linestyle=':')\n        ax.spines['right'].set_visible(False)\n        ax.spines['top'].set_visible(False)\n        \n        if increment == 9: break\n    if increment == 9: break\n        \n#_______________________________________________\n# I set the legend: abreviation -> airline name\ncomp_handler = []\nfor i in range(5):\n    comp_handler.append(mpatches.Patch(color = LABEL_COLOR_MAP[i], label = i))\n\nplt.legend(handles=comp_handler, bbox_to_anchor=(1.1, 0.97), \n           title='Cluster',\n           shadow = True, frameon = True, framealpha = 1,\n           fontsize = 13, bbox_transform = plt.gcf().transFigure)\n\nplt.show()","cf3f2739":"# Append product category per above clustering to dataframe.\ncorresp = dict()\nfor key, val in zip (liste_produits, clusters):\n    corresp[key] = val \n#__________________________________________________________________________\ndf_cleaned['categ_product'] = df_cleaned.loc[:, 'Description'].map(corresp)\ndf_cleaned","e5069183":"for i in range(6):\n    col = 'categ_{}'.format(i)        \n    df_temp = df_cleaned[df_cleaned['categ_product'] == i]\n    price_temp = df_temp['UnitPrice'] * (df_temp['Quantity'] - df_temp['QuantityCanceled'])\n    price_temp = price_temp.apply(lambda x:x if x > 0 else 0)\n    df_cleaned.loc[:, col] = price_temp\n    df_cleaned[col].fillna(0, inplace = True)\n#__________________________________________________________________________________________________\ndf_cleaned[['InvoiceNo', 'Description', 'categ_product', 'categ_0', \n            'categ_1', 'categ_2', 'categ_3','categ_4','categ_5']][:5]","e7db5ef0":"df_cleaned[['categ_0', 'categ_1', 'categ_2', 'categ_3','categ_4','categ_5']].sum()","930ff3c6":"# Create dataframe which combines purchase information for each distinct Invoice Number. \n#___________________________________________\n# sum of purchases \/ user & order\ntemp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['TotalPrice'].sum()\nbasket_price = temp.rename(columns = {'TotalPrice':'Basket Price'})\n#____________________________________________________________\n# percentage of the price of the order \/ product category\nfor i in range(n_clusters):\n    col = 'categ_{}'.format(i) \n    temp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)[col].sum()\n    basket_price.loc[:, col] = temp.loc[:, col]\n#_____________________\n# order date\ndf_cleaned['InvoiceDate_int'] = df_cleaned['InvoiceDate'].astype('int64')\ntemp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['InvoiceDate_int'].mean()\ndf_cleaned.drop('InvoiceDate_int', axis = 1, inplace = True)\nbasket_price.loc[:, 'InvoiceDate'] = pd.to_datetime(temp['InvoiceDate_int'])\n#______________________________________\n# selection of significant entries:\nbasket_price = basket_price[basket_price['Basket Price'] > 0]\nbasket_price.sort_values('CustomerID', ascending = True)[:5]","3536104f":"print(basket_price['InvoiceDate'].min(), '->',  basket_price['InvoiceDate'].max())","3b07f8a0":"temp1 = datetime(2011,10,1)\ntemp = np.datetime64(datetime(2011,10,1))\nprint(temp)","9ce87d07":"type(temp)","0450ece9":"print(basket_price['InvoiceDate'])","c95d1e97":"set_entrainement = basket_price[basket_price['InvoiceDate'] < np.datetime64(datetime(2011,10,1))]\nset_test         = basket_price[basket_price['InvoiceDate'] >= np.datetime64(datetime(2011,10,1))]","7f7a0071":"basket_price.head()","526f2e4f":"# Group all relevent information together for each customer.\n#________________________________________________________________\n# number of visits and stats on the cart amount \/ users\ntransactions_per_user=basket_price.groupby(by=['CustomerID'])['Basket Price'].agg(['count','min','max','mean','sum'])\nfor i in range(n_clusters):\n    col = 'categ_{}'.format(i)\n    transactions_per_user.loc[:,col] = basket_price.groupby(by=['CustomerID'])[col].sum() \/\\\n                                            transactions_per_user['sum']*100\n\ndf_cleaned['CanceledValue']=df_cleaned['QuantityCanceled']*df_cleaned['UnitPrice']\ntransactions_per_user=transactions_per_user.merge(\n    df_cleaned[['CustomerID','CanceledValue']].groupby('CustomerID').sum(),\n    left_index=True,right_index=True)\ntransactions_per_user['canceled%']=transactions_per_user['CanceledValue']\/transactions_per_user['sum']*100\ntransactions_per_user['canceled%']=transactions_per_user['canceled%'].apply(lambda x: 100 if x>100 else x)\n\ntransactions_per_user=transactions_per_user.merge(\n    df_cleaned[['Description','CustomerID']].groupby('CustomerID').nunique().\\\n    rename(columns={'Description':'unique_prod'}),\n    left_index=True,right_index=True)\n\ntransactions_per_user.drop(columns='CanceledValue',inplace=True)\ntransactions_per_user.index.name = None\ntransactions_per_user.reset_index(drop = False, inplace = True)\n\n# transactions_per_user.drop(columns='CustomerID', inplace=True)\ntransactions_per_user.rename(columns={'index':'CustomerID'}, inplace=True)\ntransactions_per_user.sort_values('CustomerID', ascending = True)[:5]","dba4a2ef":"last_date = basket_price['InvoiceDate'].max().date()\nfirst_registration = pd.DataFrame(basket_price.groupby(by=['CustomerID'])['InvoiceDate'].min())\nlast_purchase      = pd.DataFrame(basket_price.groupby(by=['CustomerID'])['InvoiceDate'].max())\ntest  = first_registration.applymap(lambda x:(last_date - x.date()).days)\ntest2 = last_purchase.applymap(lambda x:(last_date - x.date()).days)\ntransactions_per_user.loc[:, 'LastPurchase'] = test2.reset_index(drop = False)['InvoiceDate']\ntransactions_per_user.loc[:, 'FirstPurchase'] = test.reset_index(drop = False)['InvoiceDate']\ntransactions_per_user[:5]","fa181725":"print('\"Canceled %\" 95 quantile:',transactions_per_user['canceled%'].quantile(0.95))\ntransactions_per_user['canceled%'].hist()\nplt.yscale('log')","e217919a":"print('\"Unique Products\" 95 quantile:',transactions_per_user['unique_prod'].quantile(0.95))\ntransactions_per_user['unique_prod'].hist(bins=50)\nplt.yscale('log')","2bc88a43":"print('\"Count\" 95 quantile:',transactions_per_user['count'].quantile(0.95))\ntransactions_per_user['count'].hist(bins=20)\nplt.yscale('log')","988643f4":"print('\"Mean\" 95 quantile:',transactions_per_user['mean'].quantile(0.95))\ntransactions_per_user['mean'].hist(bins=50)\nplt.yscale('log')","fe74a7e1":"transactions_per_user.describe()","95ad41c0":"transactions_per_user.quantile(0.97)","7380d54c":"transactions_per_user['count']=transactions_per_user['count'].clip(upper=20)\ntransactions_per_user['min']=transactions_per_user['min'].clip(upper=1000)\ntransactions_per_user['max']=transactions_per_user['max'].clip(upper=2000)\ntransactions_per_user['mean']=transactions_per_user['mean'].clip(upper=1500)\ntransactions_per_user['canceled%']=transactions_per_user['canceled%'].clip(upper=15)\ntransactions_per_user['unique_prod']=transactions_per_user['unique_prod'].clip(upper=300)","e7f87bb1":"transactions_per_user.plot.scatter('min','max',color='yellow')","d16b63a5":"transactions_per_user.plot.scatter('LastPurchase','FirstPurchase',color='green')","498770d5":"temp = transactions_per_user[transactions_per_user['FirstPurchase'] > 200]\nfig = plt.hist(temp['FirstPurchase'], bins = 100,color='violet')","0ddee292":"transactions_per_user.plot.scatter('categ_0','categ_1',color='red')","54c8785d":"n1 = transactions_per_user[transactions_per_user['count'] == 1].shape[0]\nn2 = transactions_per_user.shape[0]\nprint(\"number of customers with a single purchase: {:<2}\/{:<5} ({:<2.2f}%)\".format(n1,n2,n1\/n2*100))\nprint(f\"number of customers with more than one purchases: {n2-n1}\")","c04caa68":"selected_customers = transactions_per_user[transactions_per_user['count'] > 1].copy(deep = True)\nsingle_buyers=transactions_per_user[transactions_per_user['count'] == 1]\ncustomer_comparison=pd.DataFrame([single_buyers.mean().values,selected_customers.mean().values],\n                                    columns=single_buyers.columns,\n                                    index=['one_time_buyers','multi_time_buyers']).drop(columns='CustomerID')\ncustomer_comparison","e247d661":"list_cols = ['count','min','max','mean','categ_0','categ_1','categ_2','categ_3','categ_4', 'categ_5',\n             'canceled%','unique_prod']\n#_____________________________________________________________\nmatrix = selected_customers[list_cols].to_numpy()","3f8c7106":"scaler = StandardScaler()\nscaler.fit(matrix)\nprint('variables mean values: \\n' + 90*'-' + '\\n' , scaler.mean_)\nscaled_matrix = scaler.transform(matrix)","95e83829":"pca = PCA()\npca.fit(scaled_matrix)\npca_samples = pca.transform(scaled_matrix)","cc5c2034":"fig, ax = plt.subplots(figsize=(14, 5))\nsns.set(font_scale=1)\nplt.step(range(matrix.shape[1]), pca.explained_variance_ratio_.cumsum(), where='mid',\n         label='cumulative explained variance')\nsns.barplot(np.arange(1,matrix.shape[1]+1), pca.explained_variance_ratio_, alpha=0.5, color = 'g',\n            label='individual explained variance')\nplt.xlim(0, 10)\n\nax.set_xticklabels([s if int(s.get_text())%2 == 0 else '' for s in ax.get_xticklabels()])\n\nplt.ylabel('Explained variance', fontsize = 14)\nplt.xlabel('Principal components', fontsize = 14)\nplt.legend(loc='best', fontsize = 13);","1a15ae10":"for n in range(6,12):\n    kmeans = KMeans(init='k-means++', n_clusters = n, n_init=100)\n    kmeans.fit(scaled_matrix)\n    clusters_clients = kmeans.predict(scaled_matrix)\n    silhouette_avg = silhouette_score(scaled_matrix, clusters_clients)\n    print('number of clusters is ',n, '  silhouette score: {:<.3f}'.format(silhouette_avg))\n    print(pd.DataFrame(pd.Series(clusters_clients).value_counts(), columns = ['nb. de clients']).T,'\\n')","fd0dd9ed":"n_clusters_cust= 8\nkmeans = KMeans(init='k-means++', n_clusters = n_clusters_cust, n_init=100)\nkmeans.fit(scaled_matrix)\nclusters_clients = kmeans.predict(scaled_matrix)\nsilhouette_avg = silhouette_score(scaled_matrix, clusters_clients)\nprint('silhouette score: {:<.3f}'.format(silhouette_avg))","3507a2d6":"pd.DataFrame(pd.Series(clusters_clients).value_counts(), columns = ['nb. de clients']).T","ab8888ab":"pca = PCA(n_components=6)\nmatrix_3D = pca.fit_transform(scaled_matrix)\nmat = pd.DataFrame(matrix_3D)\nmat['cluster'] = pd.Series(clusters_clients)","0089e745":"import matplotlib.patches as mpatches\n\nsns.set_style(\"white\")\nsns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2.5})\n\nLABEL_COLOR_MAP = {0:'r', 1:'tan', 2:'b', 3:'k', 4:'c', 5:'g', 6:'deeppink', 7:'skyblue', 8:'darkcyan', 9:'orange',\n                   10:'yellow', 11:'tomato', 12:'seagreen'}\nlabel_color = [LABEL_COLOR_MAP[l] for l in mat['cluster']]\n\nfig = plt.figure(figsize = (12,10))\nincrement = 0\nfor ix in range(6):\n    for iy in range(ix+1, 6):   \n        increment += 1\n        ax = fig.add_subplot(4,3,increment)\n        ax.scatter(mat[ix], mat[iy], c= label_color, alpha=0.5) \n        plt.ylabel('PCA {}'.format(iy+1), fontsize = 12)\n        plt.xlabel('PCA {}'.format(ix+1), fontsize = 12)\n        ax.yaxis.grid(color='lightgray', linestyle=':')\n        ax.xaxis.grid(color='lightgray', linestyle=':')\n        ax.spines['right'].set_visible(False)\n        ax.spines['top'].set_visible(False)\n        \n        if increment == 12: break\n    if increment == 12: break\n        \n#_______________________________________________\n# I set the legend: abreviation -> airline name\ncomp_handler = []\nfor i in range(n_clusters):\n    comp_handler.append(mpatches.Patch(color = LABEL_COLOR_MAP[i], label = i))\n\nplt.legend(handles=comp_handler, bbox_to_anchor=(1.1, 0.9), \n           title='Cluster', \n           shadow = True, frameon = True, framealpha = 1,\n           fontsize = 13, bbox_transform = plt.gcf().transFigure)\n\nplt.tight_layout()","caaffc5b":"sample_silhouette_values = silhouette_samples(scaled_matrix, clusters_clients)\n#____________________________________\n# define individual silouhette scores\nsample_silhouette_values = silhouette_samples(scaled_matrix, clusters_clients)\n#__________________\n# and do the graph\ngraph_component_silhouette(n_clusters_cust, [-0.15, 0.55], len(scaled_matrix), sample_silhouette_values, clusters_clients)","9cc5d0a8":"# We draw the TSNE plot to visualize the eight customer clusters.\nplt.figure(figsize=(20,15))\ntsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=500)\ntsne_ft = tsne.fit_transform(scaled_matrix)\nkm_data = pd.DataFrame(kmeans.predict(scaled_matrix), columns=['y'])\nkm_data['tsne-1'] = tsne_ft[:, 0]\nkm_data['tsne-2'] = tsne_ft[:, 1]\nsns.scatterplot(x=\"tsne-1\", y=\"tsne-2\",hue=km_data[\"y\"].tolist(), palette=sns.color_palette(\"hls\", 8),\n    data=km_data, legend=\"full\", alpha=0.5)","a63457b4":"selected_customers.loc[:, 'cluster'] = clusters_clients","31cbb638":"merged_df = pd.DataFrame()\nfor i in range(n_clusters_cust):\n    test = pd.DataFrame(selected_customers[selected_customers['cluster'] == i].mean())\n    test = test.T.set_index('cluster', drop = True)\n    test['size'] = selected_customers[selected_customers['cluster'] == i].shape[0]\n    merged_df = pd.concat([merged_df, test])\n#_____________________________________________________\nmerged_df.drop('CustomerID', axis = 1, inplace = True)\nprint('number of customers:', merged_df['size'].sum())\n\nmerged_df = merged_df.sort_values('sum')","64ecc407":"liste_index = []\nfocused_cols=['categ_0', 'categ_1', 'categ_2', 'categ_3', 'categ_4', 'categ_5', \n              'canceled%', 'unique_prod','sum']\nfor col in focused_cols:\n    idx=merged_df[col].idxmax()\n    if idx not in liste_index:\n        liste_index.append(idx)\nliste_index_reordered = liste_index.copy()\nliste_index_reordered += [ s for s in merged_df.index if s not in liste_index]\nmerged_df = merged_df.reindex(index = liste_index_reordered)\nmerged_df = merged_df.reset_index(drop = False)\nmerged_df['cluster']=merged_df['cluster'].astype(int)\nmerged_df","835c5c3f":"def _scale_data(data, ranges):\n    (x1, x2) = ranges[0]\n    d = data[0]\n    return [(d - y1) \/ (y2 - y1) * (x2 - x1) + x1 for d, (y1, y2) in zip(data, ranges)]\n\nclass RadarChart():\n    def __init__(self, fig, location, sizes, variables, ranges, n_ordinate_levels = 6):\n\n        angles = np.arange(0, 360, 360.\/len(variables))\n\n        ix, iy = location[:] ; size_x, size_y = sizes[:]\n        \n        axes = [fig.add_axes([ix, iy, size_x, size_y], polar = True, \n        label = \"axes{}\".format(i)) for i in range(len(variables))]\n\n        _, text = axes[0].set_thetagrids(angles, labels = variables)\n        \n        for txt, angle in zip(text, angles):\n            if angle > -1 and angle < 181:\n                txt.set_rotation(angle - 90)\n            else:\n                txt.set_rotation(angle - 270)\n        \n        for ax in axes[1:]:\n            ax.patch.set_visible(False)\n            ax.xaxis.set_visible(False)\n            ax.grid(\"off\")\n        \n        for i, ax in enumerate(axes):\n            grid = np.linspace(*ranges[i],num = n_ordinate_levels)\n            grid_label = [\"\"]+[\"{:.0f}\".format(x) for x in grid[1:-1]]\n            ax.set_rgrids(grid, labels = grid_label, angle = angles[i])\n            ax.set_ylim(*ranges[i])\n        \n        self.angle = np.deg2rad(np.r_[angles, angles[0]])\n        self.ranges = ranges\n        self.ax = axes[0]\n                \n    def plot(self, data, *args, **kw):\n        sdata = _scale_data(data, self.ranges)\n        self.ax.plot(self.angle, np.r_[sdata, sdata[0]], *args, **kw)\n\n    def fill(self, data, *args, **kw):\n        sdata = _scale_data(data, self.ranges)\n        self.ax.fill(self.angle, np.r_[sdata, sdata[0]], *args, **kw)\n\n    def legend(self, *args, **kw):\n        self.ax.legend(*args, **kw)\n        \n    def title(self, title, *args, **kw):\n        self.ax.text(0.9, 1, title, transform = self.ax.transAxes, *args, **kw)\n","8d8d4c2f":"# fig = plt.figure(figsize=(10,8))\n# attributes = ['count', 'mean', 'sum', 'canceled%','unique_prod',\n#               'categ_0', 'categ_1', 'categ_2', 'categ_3', 'categ_4','categ_5']\n# ranges = [[0.01, 10], [0.01, 1500], [0.01, 10000], [0.01, 15], [0.01,100],\n#           [0.01, 50], [0.01, 50], [0.01, 50], [0.01, 50], [0.01, 50], [0.01, 50]]\n# index  = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n# n_groups = n_clusters_cust ; i_cols = 3\n# i_rows = n_groups\/\/i_cols\n# size_x, size_y = (1\/i_cols), (1\/i_rows)\n# for ind in range(n_clusters_cust):\n#     ix = ind%3 ; iy = i_rows - ind\/\/3\n#     pos_x = ix*(size_x + 0.05) ; pos_y = iy*(size_y + 0.05)            \n#     location = [pos_x, pos_y]  ; sizes = [size_x, size_y] \n#     #______________________________________________________\n#     data = np.array(merged_df.loc[index[ind], attributes])    \n#     radar = RadarChart(fig, location, sizes, attributes, ranges)\n#     radar.plot(data, color = 'b', linewidth=2.0)\n#     radar.fill(data, alpha = 0.2, color = 'b')\n#     radar.title(title = 'cluster n\u00ba{}\\ncnt:{}'.format(index[ind],merged_df.loc[index[ind], \"size\"]), color = 'r')\n#     ind += 1 ","5c471185":"cvm=transactions_per_user[['sum','count','max','unique_prod','canceled%']].copy()\ncvm.rename(columns={'count':'num_visits','max':'max_order'},inplace=True)\nprint(\"Number of customers:\",len(cvm))\ncvm.describe()","bbca9f7d":"def f_score(x,mid,c,max_f):\n    return 1+(max_f-1)\/(1+math.e**(-(x-mid)\/c))","9210a6bd":"var='num_visits'\nplt.figure(figsize=(10,5))\nplt.hist(cvm[var],bins=20)\n# cvm[var].hist(bins=20)\nplt.xlabel(var,fontsize=15)\nplt.ylabel('Count',fontsize=15)\nplt.show()","2577b5fe":"var='num_visits'\nmid=9\nc=1.5\nmax_f=1.3\ncvm['f1']=cvm[var].apply(lambda x:f_score(x,mid,c,max_f))\nplt.figure(figsize=(10,5))\nplt.scatter(data=cvm,x=var,y='f1')\nplt.xlabel(var,fontsize=15)\nplt.ylabel('f1',fontsize=15)\n# cvm.plot.scatter(var,'f1')\nplt.show()","312ffe39":"var='max_order'\nplt.figure(figsize=(10,5))\nplt.hist(cvm[var],bins=50)\nplt.xlabel(var,fontsize=15)\nplt.ylabel('Count',fontsize=15)\nplt.show()","6cd3e208":"var='max_order'\nmid=900\nc=180\nmax_f=1.1\ncvm['f2']=cvm[var].apply(lambda x:f_score(x,mid,c,max_f))\nplt.figure(figsize=(10,5))\nplt.scatter(data=cvm,x=var,y='f2')\nplt.xlabel(var,fontsize=15)\nplt.ylabel('f2',fontsize=15)\nplt.show()","d837fb1f":"var='unique_prod'\nplt.figure(figsize=(10,5))\nplt.hist(cvm[var],bins=50)\nplt.xlabel(var,fontsize=15)\nplt.ylabel('Count',fontsize=15)\nplt.show()","526ad4f6":"var='unique_prod'\nmid=130\nc=25\nmax_f=1.2\ncvm['f3']=cvm[var].apply(lambda x:f_score(x,mid,c,max_f))\nplt.figure(figsize=(10,5))\nplt.scatter(data=cvm,x=var,y='f3')\nplt.xlabel(var,fontsize=15)\nplt.ylabel('f3',fontsize=15)\nplt.show()","16d057d5":"var='canceled%'\nplt.figure(figsize=(10,5))\nplt.hist(cvm[var],bins=15)\nplt.xlabel(var,fontsize=15)\nplt.ylabel('Logscale of Count',fontsize=15)\nplt.yscale('log')\nplt.show()","1d721928":"cvm['CVM']=cvm['sum']*cvm['f1']*cvm['f2']*cvm['f3']\ncvm.describe()","1a42f4ff":"xhigh=8000\ntemp=cvm[cvm['CVM']<xhigh]['CVM']\nplt.figure(figsize=(15,8))\nplt.hist(temp,bins=50,edgecolor='blue')\nplt.yscale('log')\nplt.ylim(1,2000)\nplt.xlim(-350,xhigh)\nplt.axvline(cvm['CVM'].quantile(0.25),color='black',linewidth=4)\nplt.axvline(cvm['CVM'].quantile(0.5),color='black',linewidth=4)\nplt.axvline(cvm['CVM'].quantile(0.75),color='black',linewidth=4)\nplt.annotate(f\"Group C\\n   <{cvm['CVM'].quantile(0.25):.1f}\",\n             (-300,1100),fontweight='bold',fontsize=15)\nplt.annotate(f\"Group B\\n>{cvm['CVM'].quantile(0.5):.1f}\",\n             (720,1100),fontweight='bold',fontsize=15)\nplt.annotate(f\"Group A\\n>{cvm['CVM'].quantile(0.75):,.1f}\",\n             (1780,1100),fontweight='bold',fontsize=15)\nplt.xlabel('CVM',fontsize=18)\nplt.ylabel('Log Scale of Count',fontsize=18)\nplt.savefig('value_groups.png')\nplt.show()","8845edb4":"df_cleaned['InvoiceDateNew'] = df_cleaned['InvoiceDate'].dt.date","1c700183":"df_cleaned['InvoiceDateNew'] = df_cleaned['InvoiceDate'].dt.date","d486e575":"groupby=df_cleaned.groupby('CustomerID').count()['InvoiceNo']","30c65591":"df_cleaned.head()","f7465d3c":"df_cleaned['Adjusted_Quantity']=df_cleaned['Quantity']-df_cleaned['QuantityCanceled']\ndf_cleaned.sort_values('Adjusted_Quantity')","0f616498":"df_cleaned['Total']=df_cleaned['Adjusted_Quantity']*df_cleaned['UnitPrice']\nCancellation=df_cleaned[df_cleaned['InvoiceNo'].str.contains('C')].groupby('CustomerID').count()['InvoiceNo'].reset_index().rename(columns={'InvoiceNo':'Cancels'})\nCancellation.head()","f50e0afd":"frequency=pd.DataFrame(df_cleaned.groupby('CustomerID').nunique()['InvoiceNo']).reset_index().rename(columns={'InvoiceNo':'Frequency'})\nfrequencyadj=pd.merge(frequency,Cancellation,how='left').fillna(0)\nfrequencyadj['Adjusted_Frequency']=frequencyadj['Frequency']-frequencyadj['Cancels']\nfrequencyadj['Adjusted_Frequency']=frequencyadj['Adjusted_Frequency'].replace(1,0)\nfrequencyadj.sort_values('Adjusted_Frequency')","5865ab91":"frequencyadj['Adjusted_Frequency'].hist()\nplt.yscale('log')\nplt.title('Adjusted Frequency Histogram\\nin Log Scale')","0a28ee76":"frequencyadj.describe()","ea25ccdb":"from datetime import datetime, timedelta\nage=pd.DataFrame((max(df_cleaned['InvoiceDate'])+timedelta(1))-df_cleaned.groupby('CustomerID').min()['InvoiceDate']).reset_index().rename(columns={'InvoiceDate':'Customer_Age'})\nage['Customer_Age']=pd.to_numeric(age['Customer_Age'].dt.days, downcast='integer')\nage","95977b3b":"recency=pd.DataFrame(df_cleaned['CustomerID'].unique()).rename(columns={0:'Customer_ID'})\nrecency","a3bac17c":"%%time\n# this cell is slow\nresult={}\nresult\nfor n in recency['Customer_ID']:\n  try:\n    outcome=(df_cleaned[df_cleaned['CustomerID']==str(n)]['InvoiceDateNew'].unique()[-1]-df_cleaned[df_cleaned['CustomerID']==str(n)]['InvoiceDateNew'].unique()[-2]).days\n    result[n]=outcome\n  except:\n    result[n]=0","617875fa":"recency1 = pd.DataFrame(list(result.items()),columns = ['CustomerID','Recency']) \nrecency1.describe()","f4420261":"from datetime import datetime, timedelta\ntemp=pd.DataFrame((max(df_cleaned['InvoiceDate'])+timedelta(1))-df_cleaned.groupby('CustomerID').max()['InvoiceDate']).reset_index().rename(columns={'InvoiceDate':'Days_Since_Purchase'})\ntemp2=pd.DataFrame((max(df_cleaned['InvoiceDate'])+timedelta(1))-df_cleaned.groupby('CustomerID').min()['InvoiceDate']).reset_index().rename(columns={'InvoiceDate':'First_Purchase'})\nrecency=pd.merge(temp,temp2,how='left')\nrecency['Days_Since_Purchase'] = pd.to_numeric(recency['Days_Since_Purchase'].dt.days, downcast='integer')\nrecency['First_Purchase'] = pd.to_numeric(recency['First_Purchase'].dt.days, downcast='integer')\nrecency=pd.merge(recency1,recency,how='inner')","b6a4a61a":"recency=pd.merge(recency1,recency[['CustomerID','First_Purchase']])","ba442349":"recency['Recency'].hist(bins=20)\nplt.title('Recency Distribution in Log Sacle')\nplt.yscale('log')","17d85c58":"cancelation=df_cleaned[df_cleaned['InvoiceNo'].str.contains('C')].groupby('CustomerID').count()['InvoiceNo'].reset_index()","45086056":"cancel_order=df_cleaned[df_cleaned['InvoiceNo'].str.contains('C')]['InvoiceNo']\nnon_cancel=df_cleaned[df_cleaned['InvoiceNo'].isin(cancel_order)==False]\ncanceled=df_cleaned[df_cleaned['InvoiceNo'].isin(cancel_order)==True]\nnon_cancel=non_cancel.groupby('CustomerID').sum()['Total'].reset_index().rename(columns={'Total':'Non_Cancel_Total'})\ncanceled=canceled.groupby('CustomerID').sum()['Total'].reset_index().rename(columns={'Total':'Cancel_Total'})\ntotaldf=pd.merge(non_cancel,canceled,how='left').fillna(0)\ntotaldf['Adjusted_Total']=totaldf['Non_Cancel_Total']-totaldf['Cancel_Total']\ntotaldf=totaldf[['CustomerID','Adjusted_Total']]\ntotaldf.describe()","2ea56e39":"plt.hist(totaldf[totaldf['Adjusted_Total']<=6000]['Adjusted_Total'],bins=20)\nplt.title('Adjusted Total Amount Distribution')","9b7c57b4":"quantity=df_cleaned.groupby('CustomerID').mean()['Adjusted_Quantity'].reset_index().rename(columns={'Adjusted_Quantity':'Average_Quantity'})\nquantity.describe()\nquantity[quantity['Average_Quantity']<1500].hist('Average_Quantity',bins=20,color='green')\nplt.title('Average Quantity\\n in Log Scale')\nplt.yscale(\"log\")","dc662f3a":"quantity[quantity['Average_Quantity']<=100].hist('Average_Quantity',bins=20,color='green')\nplt.title('Average Quantity Distribution')","75455e78":"nmid_f1=125\nc_f1=25\nrecency['F_Recency']=1+(1.25-1)\/(1+np.exp(-(recency['Recency']-nmid_f1)\/c_f1))\nrecency[recency['Recency']<=500].plot.scatter('Recency','F_Recency',color='red')\nplt.title('F(Recency) Transition Graph\\nMidpoint 125 Spread 25')","f00f1823":"frequency=df_cleaned.groupby('CustomerID').nunique('InvoiceNo')['InvoiceNo'].reset_index().rename(columns={'InvoiceNo':'Frequency'})\nfrequency['Frequency']=frequency['Frequency']-1\nfrequency\nrecency=recency[['CustomerID','Recency','F_Recency']]\nrecency.max()","df48055b":"frequency.describe()","11fa4855":"frequency[frequency['Frequency']<=100].hist(bins=20)\nplt.title('Frequency Distribution\\n in Log Scale')\nplt.yscale('log')","8afaf222":"nmid_f1=20\nc_f1=5\nfrequencyadj['F_Frequency']=1+(1.3-1)\/(1+np.exp(-(frequencyadj['Adjusted_Frequency']-nmid_f1)\/c_f1))\nfrequencyadj[frequencyadj['Adjusted_Frequency']<=80].plot.scatter('Adjusted_Frequency','F_Frequency')\nplt.title('F(Frequency) Transition Graph\\nMidpoint 20 Spread 5')","79dd188b":"data=pd.merge(frequency[['CustomerID','Frequency']],recency)\ndata=pd.DataFrame(data)\ndata=pd.merge(data,totaldf,how='inner')\ndata=pd.merge(data,frequencyadj[['CustomerID','Adjusted_Frequency','F_Frequency']],how='inner')\ndata","e3a5277f":"data['Frequency'].plot(kind='hist', bins=25)\nprint(data['Frequency'].describe())\nprint(sum(data['Frequency'] == 0)\/float(len(data)))","ce940747":"data=pd.merge(data,age,how='inner')\ndata['F_Frequency'].max()","fddcd7fa":"frequencyadj=frequencyadj[['CustomerID','Adjusted_Frequency']]\nfrequencyadj['Adjusted_Frequency']=frequencyadj['Adjusted_Frequency'].replace(1,0)","8cc75e20":"data=pd.merge(data,quantity,how='inner')","a43bc39e":"data['CVM'] = cvm['CVM']","35f02a06":"data['CVM_Group']=np.nan\nfor n in range(0,len(data)):\n  if data['CVM'][n]<=318.8:\n    data['CVM_Group'][n]=1\n  elif data['CVM'][n]<=730.1:\n    data['CVM_Group'][n]=2\n  else:\n    data['CVM_Group'][n]=3","94a38c9d":"field_means = data.mean()\nKnn_table = data.groupby('CVM_Group').mean()\nKnn_norm_table = Knn_table \/ field_means\nKnn_norm_table_transpose = Knn_norm_table.transpose()\nplt.figure(figsize=(15,15))\nsns.heatmap(Knn_norm_table_transpose, cmap='gnuplot', annot=True, vmin=0, vmax=2)","6e52afe9":"len(data[data['Recency']==0])\/len(data)","73994586":"for n in range(0,len(data)):\n  if data['Adjusted_Frequency'][n]==0:\n    data['Recency'][n]=0","9c3df6a4":"from lifetimes import BetaGeoFitter\nfrom scipy.special import logsumexp\nbgf = BetaGeoFitter(penalizer_coef=0.0)\nbgf.fit(data['Adjusted_Frequency'], data['Recency'], data['Customer_Age'],)\nprint(bgf)","e707ebc5":"from lifetimes.plotting import plot_frequency_recency_matrix\nimport matplotlib.pyplot as plt\nfig = plt.figure(figsize=(12,6))\nplot_frequency_recency_matrix(bgf)","e3e6a393":"from lifetimes.plotting import plot_probability_alive_matrix\nfig = plt.figure(figsize=(12,6))\nplot_probability_alive_matrix(bgf)","93b00c08":"from lifetimes.plotting import *\nfrom lifetimes.utils import *\n# from lifetimes.estimation import *","a3d124d9":"cancel_order=df_cleaned[df_cleaned['InvoiceNo'].str.contains('C')]\nnon_cancel=df_cleaned[df_cleaned['InvoiceNo'].isin(cancel_order)==False]\ntemp=non_cancel.groupby('CustomerID').count()['InvoiceNo'].reset_index()\ntemp['One_Timer']=np.nan\nfor n in range(0,len(temp)):\n  if temp['InvoiceNo'][n]==1:\n    temp['One_Timer'][n]='One_Time'\n  else:\n    temp['One_Timer'][n]='Repeat'\ntemp=temp[['CustomerID','One_Timer']]\ndata=pd.merge(data,temp,how='inner')","10e38b13":"\nplt.bar(data['One_Timer'], data['Adjusted_Total'],color='orange')\nplt.title('One Time Purchaser v.s. Repeat Customers\\nTotal Amount Comparison', fontsize=14)\nplt.xlabel('One_Timer', fontsize=14)\nplt.ylabel('Adjusted_Total', fontsize=14)\nplt.grid(True)\nplt.show()\n","6b83b4af":"df_new=pd.merge(df_cleaned,temp,how='left')\ndf_new['Weekday']=df_new['InvoiceDate'].dt.day_name()\nweekday=df_new.groupby(['CustomerID','Weekday']).sum().reset_index()","314a8348":"df_new=df_new[df_new['Total']>0]\ndf_new=df_new[df_new['Total']<=6000]\ng=sns.catplot(x=\"Weekday\", y=\"Total\", hue=\"One_Timer\",kind='bar', data=df_new)\n\ng.fig.set_figwidth(20)\ng.fig.set_figheight(8)\nplt.title('One Time Customer v.s. Repeat Customer\\nComparison (without Returns)')","c2b84313":"citizen=df_cleaned[['CustomerID','Country']].drop_duplicates()\ndata_citizen=pd.merge(citizen,data,how='right')","660156ad":"top10=pd.DataFrame(df_new.groupby('Country').count()['InvoiceNo'].reset_index().sort_values('InvoiceNo',ascending=False)[:6]['Country'])\ndf_top10=pd.merge(data_citizen,top10,how='inner')","2384ca4e":"g = sns.catplot(data=df_top10, x='Country', kind=\"box\",\n                y='CVM', hue='One_Timer')\ng.fig.set_figwidth(40)\ng.fig.set_figheight(15)\nplt.ylim(0,200000)\nplt.title('Customer Value Distribution Among Top 6 Countries\\nwith Most Orders Placed',fontsize=26)","3a8c6a48":"df_cleaned[df_cleaned['Country']=='EIRE'].groupby('CustomerID').count()","6040d863":"top10=pd.DataFrame(df_new.groupby('Country').nunique()['CustomerID'].reset_index().sort_values('CustomerID',ascending=False)[:6]['Country'])\ndf_top10=pd.merge(data_citizen,top10,how='inner')","565073b2":"g = sns.catplot(data=df_top10, x='Country', kind=\"bar\",\n                y='CVM', hue='One_Timer')\nplt.ylim(0,20000)\ng.fig.set_figwidth(40)\ng.fig.set_figheight(15)\nplt.title('Customer Value Distribution Among Top 6 Countries\\nwith Most Customers Capped at $20,000 CVM',fontsize=26)","27deb06b":"data.groupby(['CVM_Group','One_Timer']).mean()['Adjusted_Total']","80291b93":"plt.figure(figsize=(10,5))\np=data.groupby(['CVM_Group','One_Timer']).mean()['Adjusted_Total'].unstack().plot(kind='bar')\nplt.title('Customer Value Measure by 25% Percentile')","f24640ff":"df_new.groupby(['Weekday','One_Timer']).sum()['Total']","7778def2":"ar_df = (df_cleaned.groupby(['InvoiceNo', 'Description'])['Quantity'].sum().unstack().reset_index().fillna(0).set_index('InvoiceNo'))\n\ndef encoding(x):\n    if x <= 0:return 0      \n    if x >= 1:return 1\n        \nitemsets = ar_df.applymap(encoding)\nfrequent_sets = apriori(itemsets, min_support=0.025, use_colnames=True)\nassoc_rules = association_rules(frequent_sets, min_threshold=0.4)","525a816c":"frequent_sets","fef15a1f":"#the following item antecedent and consequents pairs ordered by confidence\n#can be of help to formulate sales strategies such as item bundling\nassoc_rules","678526e0":"!pip install scikit-surprise","00b1c5af":"from surprise import SVD,accuracy, Dataset, Reader\nfrom surprise.model_selection import cross_validate,train_test_split","6c40bc1e":"rs_df = pd.DataFrame(df_cleaned.groupby(['CustomerID','StockCode'])['Quantity'].sum()).reset_index()\nrs_df","477d4508":"reader = Reader()\nrs_df_matrix = Dataset.load_from_df(rs_df[['CustomerID', 'StockCode', 'Quantity']], reader)","2b8429ab":"trainset, testset = train_test_split(rs_df_matrix, test_size=0.25)","0efec248":"algo = SVD()\nalgo.fit(trainset)\npredictions = algo.test(testset)","01a59b47":"accuracy.rmse(predictions)","a1c214b8":"#let's take a close look at how this can make recommendations for a specific customer\n#for the sake of illustration, we will use customer id 13268\niids = rs_df['StockCode'].unique()\niids13268 = rs_df.loc[rs_df.CustomerID=='13268','StockCode']\n#remove the items that 13268 has already purchased\niids_to_pred = np.setdiff1d(iids, iids13268)\ntest_df = [['13268',iid, 5] for iid in iids_to_pred] #\npredictions = algo.test(testset)\npredictions[0]","86db4ced":"pred_ratings = np.array([pred.est for pred in predictions])\n#find the index of the max predicted rating\ni_max = pred_ratings.argmax()\n#use this to find the corresponding iid\niid = iids_to_pred[i_max]\nprint(\"Top item for customer 13268 has stockcode {0}, with predicted rating {1}\".format(iid, pred_ratings[i_max]))","316f8cec":"invoices = (df_cleaned.groupby(['InvoiceNo', 'Description'])['Quantity']\n                      .sum().unstack().reset_index().fillna(0)\n                      .set_index('InvoiceNo'))","84a4393c":"%%time\n# this cell takes a while\nbasket_sets = invoices.applymap(lambda x: 1 if x >= 1 else 0)\nfrequent_itemsets = apriori(basket_sets, min_support=0.02, use_colnames=True)","d44de59f":"# below is a list of all frequent itemsets bought by customers\nfrequent_itemsets","c45d05d1":"# below is a list of all frequent itemsets that are a combination of 2 or more items\n# bought by customers\nfrequent_itemsets[frequent_itemsets.itemsets.apply(len) > 1]","110021f7":"rules = association_rules(frequent_itemsets, min_threshold=0.5)\nrules","6bccc07c":"items = df_cleaned['StockCode'].unique()\nitems2idx = dict(zip(items, range(len(items))))\nidx2item = dict(zip(range(len(items)), items))","b6f4a20b":"pip install spotlight","bfd609a8":"from spotlight.spotlight.interactions import Interactions","af953a0c":"# I couldn't get spotlight to run so I commented out the next 6 cells\n\ninteractions = Interactions(df_cleaned.CustomerID.values.astype(int),\n                            df_cleaned.StockCode.map(items2idx).values.astype(int),\n                            df_cleaned.Quantity.values.astype(float))","62aecfa9":"model = ExplicitFactorizationModel(loss='regression',\n                                   embedding_dim=128,  # latent dimensionality\n                                   n_iter=10,  # number of epochs of training\n                                   batch_size=1024,  # minibatch size\n                                   l2=1e-9,  # strength of L2 regularization\n                                   learning_rate=1e-3,\n                                   use_cuda=torch.cuda.is_available())","bc4bb7ce":"train, test = random_train_test_split(interactions)\nmodel.fit(train, verbose=True)","9dfddfa9":"train_rmse = rmse_score(model, train)\ntest_rmse = rmse_score(model, test)\n","344638fb":"class Class_Fit(object):\n    def __init__(self, clf, params=None):\n        if params:            \n            self.clf = clf(**params)\n        else:\n            self.clf = clf()\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def grid_search(self, parameters, Kfold):\n        self.grid = GridSearchCV(estimator = self.clf, param_grid = parameters, cv = Kfold)\n        \n    def grid_fit(self, X, Y):\n        self.grid.fit(X, Y)\n        \n    def grid_predict(self, X, Y):\n        self.predictions = self.grid.predict(X)\n        print(\"Precision: {:.2f} % \".format(100*metrics.accuracy_score(Y, self.predictions)))\n        ","066c4d25":"columns = ['mean', 'categ_0', 'categ_1', 'categ_2', 'categ_3', 'categ_4' ]\nX = selected_customers[columns]\nY = selected_customers['cluster']","3c0ac27d":"X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, train_size = 0.8)","a6a8a989":"svc = Class_Fit(clf = svm.LinearSVC)\nsvc.grid_search(parameters = [{'C':np.logspace(-2,2,10)}], Kfold = 3)\n# svc.grid.best_params_","56dc6688":"svc.grid_fit(X = X_train, Y = Y_train)","1beb1235":"svc.grid_predict(X_test, Y_test)","1e28f96c":"def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n    #_________________________________________________\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n    #_________________________________________________\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    #_________________________________________________\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","03b40709":"class_names = [i for i in range(11)]\ncnf_matrix = confusion_matrix(Y_test, svc.predictions) \nnp.set_printoptions(precision=2)\nplt.figure(figsize = (8,8))\nplot_confusion_matrix(cnf_matrix, classes=class_names, normalize = False, title='Confusion matrix')","3e411545":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 10)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","55b7df61":"g = plot_learning_curve(svc.grid.best_estimator_,\n                        \"SVC learning curves\", X_train, Y_train, ylim = [1.01, 0.5],\n                        cv = 5,  train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5,\n                                                0.6, 0.7, 0.8, 0.9, 1])","1b9408a4":"lr = Class_Fit(clf = linear_model.LogisticRegression)\nlr.grid_search(parameters = [{'C':np.logspace(-2,2,20)}], Kfold = 3)\nlr.grid_fit(X = X_train, Y = Y_train)\nlr.grid_predict(X_test, Y_test)\nlr.grid.best_params_","944e0b17":"g = plot_learning_curve(lr.grid.best_estimator_, \"Logistic Regression learning curves\", X_train, Y_train,\n                        ylim = [1.01, 0.6], cv = 5, \n                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","923bc3b2":"knn = Class_Fit(clf = neighbors.KNeighborsClassifier)\nknn.grid_search(parameters = [{'n_neighbors': np.arange(5,25,1)}], Kfold = 3)\nknn.grid_fit(X = X_train, Y = Y_train)\nknn.grid_predict(X_test, Y_test)\nknn.grid.best_params_","5ab24f40":"g = plot_learning_curve(knn.grid.best_estimator_, \"Nearest Neighbors learning curves\", X_train, Y_train,\n                        ylim = [1.01, 0.6], cv = 5, \n                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","141ad4f4":"tr = Class_Fit(clf = tree.DecisionTreeClassifier)\ntr.grid_search(parameters = [{'criterion' : ['entropy', 'gini'], 'max_features' :['sqrt', 'log2']}], Kfold = 3)\ntr.grid_fit(X = X_train, Y = Y_train)\ntr.grid_predict(X_test, Y_test)\ntr.grid.best_params_","af16567c":"g = plot_learning_curve(tr.grid.best_estimator_, \"Decision tree learning curves\", X_train, Y_train,\n                        ylim = [1.01, 0.5], cv = 5, \n                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","4094f1e4":"rf = Class_Fit(clf = ensemble.RandomForestClassifier)\nparam_grid = {'criterion' : ['entropy', 'gini'], 'n_estimators' : [100, 125, 150],\n               'max_features' :['sqrt', 'log2']}\nrf.grid_search(parameters = param_grid, Kfold = 3)\nrf.grid_fit(X = X_train, Y = Y_train)\nrf.grid_predict(X_test, Y_test)\nrf.grid.best_params_","a1a492c1":"g = plot_learning_curve(rf.grid.best_estimator_, \"Random Forest learning curves\", X_train, Y_train,\n                        ylim = [1.01, 0.6], cv = 5, \n                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","ac923ad8":"# from sklearn.tree import DecisionTreeClassifier\nada = Class_Fit(clf = AdaBoostClassifier)\n# ada = Class_Fit(clf = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(max_depth=3)))\nparam_grid = {'n_estimators' : [40,60,80]}\nada.grid_search(parameters = param_grid, Kfold = 3)\nada.grid_fit(X = X_train, Y = Y_train)\nada.grid_predict(X_test, Y_test)\nada.grid.best_params_","4094640a":"g = plot_learning_curve(ada.grid.best_estimator_, \"AdaBoost learning curves\", X_train, Y_train,\n                        ylim = [1.01, 0.3], cv = 5, \n                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","500e8782":"gb = Class_Fit(clf = ensemble.GradientBoostingClassifier)\nparam_grid = {'n_estimators' : [1000], 'max_depth' : [4]}\ngb.grid_search(parameters = param_grid, Kfold = 3)\ngb.grid_fit(X = X_train, Y = Y_train)\ngb.grid_predict(X_test, Y_test)\ngb.grid.best_params_","ff678fcb":"g = plot_learning_curve(gb.grid.best_estimator_, \"Gradient Boosting learning curves\", X_train, Y_train,\n                        ylim = [1.01, 0.5], cv = 5, \n                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","e6420051":"rf_best  = ensemble.RandomForestClassifier(**rf.grid.best_params_)\ngb_best  = ensemble.GradientBoostingClassifier(**gb.grid.best_params_)\nsvc_best = svm.LinearSVC(**svc.grid.best_params_)\ntr_best  = tree.DecisionTreeClassifier(**tr.grid.best_params_)\nknn_best = neighbors.KNeighborsClassifier(**knn.grid.best_params_)\nlr_best  = linear_model.LogisticRegression(**lr.grid.best_params_)","dfa37c26":"votingC = ensemble.VotingClassifier(estimators=[('rf', rf_best),('gb', gb_best),\n                                                ('knn', knn_best)], voting='soft') ","992aaf6b":"votingC = votingC.fit(X_train, Y_train)","e238b36d":"predictions = votingC.predict(X_test)\nprint(\"Precision: {:.2f} % \".format(100*metrics.accuracy_score(Y_test, predictions)))","77976bb4":"basket_price_test = set_test.copy(deep = True)","d634e429":"transactions_per_user_test=basket_price_test.groupby(by=['CustomerID'])['Basket Price'].agg(['count','min','max','mean','sum'])\nfor i in range(5):\n    col = 'categ_{}'.format(i)\n    transactions_per_user_test.loc[:,col] = basket_price_test.groupby(by=['CustomerID'])[col].sum() \/\\\n                                            transactions_per_user_test['sum']*100\n\ntransactions_per_user_test.reset_index(drop = False, inplace = True)\nbasket_price_test.groupby(by=['CustomerID'])['categ_0'].sum()\n\n#_______________________\n# Correcting time range\ntransactions_per_user_test['count'] = 5 * transactions_per_user_test['count']\ntransactions_per_user_test['sum']   = transactions_per_user_test['count'] * transactions_per_user_test['mean']\n\ntransactions_per_user_test.sort_values('CustomerID', ascending = True)[:5]","8cb1e33c":"transactions_per_user_test.shape","db356efb":"list_cols = ['count','min','max','mean','categ_0','categ_1','categ_2','categ_3','categ_4']\n#_____________________________________________________________\nmatrix_test = transactions_per_user_test[list_cols]\nmatrix_test['dummy1'] =matrix_test['count']\nmatrix_test['dummy2'] =matrix_test['count']\nmatrix_test['dummy3'] =matrix_test['count']\nmatrix_test.head()","ca8d6a2d":"matrix_test.to_numpy()\nscaled_test_matrix = scaler.transform(matrix_test)\nmatrix_test = matrix_test[list_cols]\nmatrix_test","6abec285":"Y = kmeans.predict(scaled_test_matrix)","ffa877a9":"columns = ['mean', 'categ_0', 'categ_1', 'categ_2', 'categ_3', 'categ_4' ]\nX = transactions_per_user_test[columns]","c5853742":"classifiers = [(svc, 'Support Vector Machine'),\n                (lr, 'Logostic Regression'),\n                (knn, 'k-Nearest Neighbors'),\n                (tr, 'Decision Tree'),\n                (rf, 'Random Forest'),\n                (gb, 'Gradient Boosting')]\n#______________________________\nfor clf, label in classifiers:\n    print(30*'_', '\\n{}'.format(label))\n    clf.grid_predict(X, Y)","eb9082f6":"predictions = votingC.predict(X)\nprint(\"Precision: {:.2f} % \".format(100*metrics.accuracy_score(Y, predictions)))","2d8dfc2e":"stop_time = pd.datetime.now()\nprint('duration: ', stop_time - start_time)","591c338f":"in order to create a representation of the various clusters:","6ce5ed0d":"** c\/ _Customers morphotype_**\n\nAt this stage, I have verified that the different clusters are reasonably disjoint (at least, in a global way). It remains to understand the behavior of the customers in each cluster. To do so, I start by adding to the `selected_customers` dataframe a variable that defines the cluster to which each client belongs:","5266df3d":"# 2. Remove Cancelled Orders and Look at Remaining Records\n\nFirst of all, I count the number of transactions corresponding to canceled orders:","a8e04ff4":"then I can test the quality of the prediction with respect to the test data:","b2928735":"\n\n\n** a \/ _Report via the PCA_ **\n\nThere is a certain disparity in the sizes of different groups that have been created. Hence I will now try to understand the content of these clusters in order to validate (or not) this particular separation. At first, I use the result of the PCA:","c6715e82":"____\n#### Consumer Order Combinations\n\nIn a second step, I group together the different entries that correspond to the same user. I thus determine the number of purchases made by the user, as well as the minimum, maximum, average amounts and the total amount spent during all the visits. For each customer, I also added the total dollar value of all his or her cancelled items as a percentage of total purchase amount, as well as the number of distinct products bought.","798ed9f3":"By sales revenue, category 0, 1 and 3 are the top three categories.","04da9cca":"The execution of this function returns three values:\n- `keywords`: the list of extracted keywords\n- `keywords_roots`: a dictionary where the keys are the keywords roots and the values are the lists of words associated with those roots\n- `count_keywords`: dictionary listing the number of times every word is used\n\nAt this point, I convert the `count_keywords` dictionary into a list, to sort the keywords according to their occurences:","0c3395e3":"The cosine kmeans method I constructed is somewhat unstable. Thus I performed 100 iterations for n_clusters from 3 to 8 to get a more stablized picture. The results are:\n- For n_clusters = 3 The average silhouette_score is : 0.25673884714979833\n- For n_clusters = 4 The average silhouette_score is : 0.32045152822154677\n- For n_clusters = 5 The average silhouette_score is : 0.3406799276763692\n- For n_clusters = 6 The average silhouette_score is : 0.38970683534465045\n- For n_clusters = 7 The average silhouette_score is : 0.3687191429652772\n- For n_clusters = 8 The average silhouette_score is : 0.35286912641983464\n\nI chose 6 for it has the highest average silhouette score across all iterations. After that, I ran a few more iterations to find an near-optimal model with an average silhouette score above a particular threshold.","77f3e59f":"Each line in this matrix contains a consumer's buying habits. At this stage, it is a question of using these habits in order to define the category to which the consumer belongs. These categories have been established in Section 4. ** At this stage, it is important to bear in mind that this step does not correspond to the classification stage itself**. Here, we prepare the test data by defining the category to which the customers belong. However, this definition uses data obtained over a period of 2 months (via the variables ** count **, ** min **, ** max ** and ** sum **). The classifier defined in Section 5 uses a more restricted set of variables that will be defined from the first purchase of a client.\n\nHere it is a question of using the available data over a period of two months and using this data to define the category to which the customers belong. Then, the classifier can be tested by comparing its predictions with these categories. In order to define the category to which the clients belong, I recall the instance of the `kmeans` method used in section 4. The` predict` method of this instance calculates the distance of the consumers from the centroids of the 11 client classes and the smallest distance will define the belonging to the different categories:","6cd230ad":"___\n### Support Vector Machine Classifier (SVC)\n\nThe first classifier I use is the SVC classifier. In order to use it, I create an instance of the `Class_Fit` class and then call` grid_search()`. When calling this method, I provide as parameters:\n- the hyperparameters for which I will seek an optimal value\n- the number of folds to be used for cross-validation","b65edd6c":"Using it, I create a representation of the most common keywords:","038742ef":"Finally, I define two additional variables that give the number of days elapsed since the first purchase (** FirstPurchase **) and the number of days since the last purchase (** LastPurchase **):","796ab875":"On these few lines, we see that when an order is canceled, we have another transaction in the dataframe, mostly identical except for the **Quantity** and **InvoiceDate** variables. I decide to check if this is true for all the entries.\nTo do this, I decide to locate the entries that indicate a negative quantity and check if there is *systematically* an order indicating the same quantity (but positive), with the same description (**CustomerID**, **Description** and **UnitPrice**):","a9e107b1":"# 3. Make Product Categories\n\nIn the dataframe, products are uniquely identified through the **StockCode** field. A short description of the products is given in the **Description** field. In this section, I intend to use the content of this latter variable in order to group the products into different categories.\n\n___\n### Look at Products Description\n\nAs a first step, I extract from the **Description** field the information that will prove useful. To do this, I use the following function:","09dfb04c":"___\n### Let's vote !\n\nFinally, the results of the different classifiers presented in the previous sections can be combined to improve the classification model. This can be achieved by selecting the customer category as the one indicated by the majority of classifiers. To do this, I use the `VotingClassifier` method of the `sklearn` package. As a first step, I adjust the parameters of the various classifiers using the *best* parameters previously found:","293db53b":"Once more, we find that the initial hypothesis is not verified. Hence, cancellations do not necessarily correspond to orders that would have been made beforehand.\n\nAt this point, I decide to create a new field in the dataframe that indicates if part of the order has been canceled. For the cancellations without counterparts, a few of them are probably due to the fact that the buy orders were performed before December 2010 (the point of entry of the database). Below, I make a census of the canceled orders and check for the existence of counterparts:","18563dc6":"In the following, I will create clusters of customers. In practice, before creating these clusters, it is interesting to define a base of smaller dimension allowing to describe the `scaled_matrix` matrix. In this case, I will use this base in order to create a representation of the different clusters and thus verify the quality of the separation of the different groups. I therefore perform a PCA beforehand:","91248164":"where the $a_ {i, j}$ coefficient  is 1 if the description of the product $i$ contains the word $j$, and 0 otherwise. Here I have a row for each product and a column for each keyword. I fill in the entries with zero or 1, depending on whether that product has that keyword in its description.","735a4773":"We see that the initial hypothesis is not fulfilled because of the existence of a  '_Discount_' entry. I check again the hypothesis but this time discarding the '_Discount_' entries:","adc6dee6":"#### Data encoding\n\nThe dataframe `transactions_per_user` contains a summary of all the commands that were made. Each entry in this dataframe corresponds to a particular client. I use this information to characterize the different types of customers and only keep a subset of variables:","14a04024":"#### Examine the clusters","238c0c1b":"___\n###\u00a0Creation of customers categories","0fa782cb":"and I represent the amount of variance explained by each of the components:","46074812":"___\n# 4. Make Customer Categories\n\n### Formatting data\n\nIn the previous section, the different products were grouped in five clusters. In order to prepare the rest of the analysis, a first step consists in introducing this information into the dataframe. To do this, I create the categorical variable **categ_product** where I indicate the cluster of each product :","428b7e62":"###\u00a0Gradient Boosting Classifier","ac418ca5":"\nAt this point, I define clusters of clients from the standardized matrix that was defined earlier and using the `k-means` algorithm from` scikit-learn`. I choose the number of clusters based on the silhouette score and I find that a pretty good score is obtained with 8 clusters:","108de3b7":"Finally, I re-organize the content of the dataframe by ordering the different clusters: first, in relation to the amount wpsent in each product category and then, according to canceled%, number of unique products, and the total amount spent:","3d352b85":"The $X$ matrix indicates the words contained in the description of the products using the *one-hot-encoding* principle. In practice, I have found that introducing the price range results in more balanced groups in terms of element numbers.\nHence, I add 6 extra columns to this matrix, where I indicate the price range of the products:","f975ba00":"Finally, as anticipated in Section 5.8, it is possible to improve the quality of the classifier by combining their respective predictions. At this level, I chose to mix *Random Forest*, *Gradient Boosting* and *k-Nearest Neighbors* predictions because this leads to a slight improvement in predictions:","5954cb1a":"Compared to multi-time buyers, one-time buyers have a lower canceled value percentage and slighly smaller average order size. In terms of product mix, there doesn't seem to be a big diffference.","88dbfbac":"In practice, the different variables I selected have quite different ranges of variation and before continuing the analysis, I  create a matrix where these data are standardized:","40eafa55":"## 6. Examine Item Co-occurence and Make Recommendation\ninspired by https:\/\/pbpython.com\/market-basket-analysis.html#:~:text=Association%20rules%20are%20normally%20written,%7BBeer%7D%20is%20the%20consequent.","621de1d0":"We see that the number of components required to explain the data is extremely important: we need more than 100 components to explain 90% of the variance of the data. So we find that the space contains lots of variance in many non correlated directions, and PCA doesn't really help that much. In practice, I decide to keep only a limited number of components since this decomposition is only performed to visualize the data:","b9784f1f":"Then, I load the data. Once done, I also give some basic informations on the content of the dataframe: the type of the various fields, the number of null values and their percentage with respect to the total number of entries:","1c8754f9":"from which I create the following representation:","62b79023":"and train it:","c1697774":"### k-Nearest Neighbors","78dc2cf9":"Since the goal is to define the class to which a client belongs and this, as soon as its first visit, I only keep the variables that describe the content of the basket, and do not take into account the variables related to the frequency of visits or variations of the basket price over time:","913d3ae0":"Even with fine tuning and many iterations, we can still find many words appearing in various clusters. This imperfect result is expected because at the end of the day machines can only do what we tell them to do, in our case here, that is to find similar words based on how they are spelled, not what they mean. The machine has no way to know \"cushion\" is closer to \"cabinet\" than it is to \"bracelet\". In real world, it's very unlikely a business will do unsupervised product segmentation like this. \n\n** c \/ _Principal Component Analysis_ **\n\nIn order to ensure that these clusters are reasonably distinct, I look at their composition in a relevant low dimensional space. Given the large number of variables of the initial matrix, I first perform a PCA:","4ee98812":"The list that was obtained contains more than 1400 keywords and the most frequent ones appear in more than 200 products. However, while examinating the content of the list, I note that some names are useless. Others are do not carry information, like colors. Therefore, I discard these words from the analysis that follows and also, I decide to consider only the words that appear more than 13 times.","896c9199":"###\u00a0Random Forest","be6e0019":"The first lines of this list shows several things worthy of interest:\n- the existence of entries with the prefix C for the **InvoiceNo** variable: this indicates transactions that have been canceled\n- the existence of users who only came once and only purchased one product (e.g. n\u00ba12346)\n- the existence of frequent users that buy a large number of items at each order","edcbbd91":"It remains only to examine the predictions of the different classifiers that have been trained in section 5:","2f57eb4a":"____\n#### Create clusters of products\n\nIn this section, I will group the products into different classes. In the case of matrices with binary encoding, cosine similarity\/distance is a commonly-used measure of distance. Since **kmeans** method of sklearn uses a Euclidean distance, I defined my own functions to perform kmeans clustering using cosine distance. In order to define (approximately) the number of clusters that best represents the data, I use the silhouette score as a measure of the \"quality\" of the segmentation.","40b6053d":"It can be seen, for example, that the first 5 clusters correspond to a strong preponderance of purchases in a particular category of products. Other clusters will differ from basket averages (** mean **), the total sum spent by the clients (** sum **) or the total number of visits made (** count **).\n\n","3d186bc3":"Then, I plot the learning curve to have a feeling of the quality of the model:","91140d1f":"### Look at Countries","3a8fe2b1":"Then, I convert the dataframe into a matrix and retain only variables that define the category to which consumers belong. At this level, I recall the method of normalization that had been used on the training set:","c9b50d8f":"### A simple implementation on the dataset","4449fc75":"I capped the long tails at around 97% quantile.","7a096e1b":"___\n### Defining product categories ","9f31fc94":"OK, therefore, by removing these entries we end up with a dataframe filled at 100% for all variables! Finally, I check for duplicate entries and delete them:","9568a22a":"We see that there are several types of peculiar transactions, connected e.g. to port charges or bank charges. For better product segmentation, I will remove such transactions.\n\n\n___\n#### Basket Price","832901ca":"from which I represent the leanring curve of the SVC classifier:","f0e87481":"This function takes as input the dataframe and analyzes the content of the **Description** column by performing the following operations:\n\n- extract the names (proper, common) appearing in the products description\n- for each name, I extract the root of the word and aggregate the set of names associated with this particular root\n- count the number of times each root appears in the dataframe\n- when several words are listed for the same root, I consider that the keyword associated with this root is the shortest name (this systematically selects the singular when there are singular\/plural variants)\n\nThe first step of the analysis is to retrieve the list of products:","dc6a166e":"I checked the number of elements in each class and did a sanity check by simply looking at the products in a cluster.","36ee580f":"___\n## Exploring the content of fields\n\nThis dataframe contains 8 fields that correspond to: \n\n**InvoiceNo**: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.  <br>\n**StockCode**: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product. <br>\n**Description**: Product (item) name. Nominal. <br>\n**Quantity**: The quantities of each product (item) per transaction. Numeric.\t<br>\n**InvoiceDate**: Invice Date and time. Numeric, the day and time when each transaction was generated. <br>\n**UnitPrice**: Unit price. Numeric, Product price per unit in sterling. <br>\n**CustomerID**: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer. <br>\n**Country**: Country name. Nominal, the name of the country where each customer resides.<br>\n\n___\n### Field Statistics","6fe7ecf6":"# 5. Make Customer Value Measures","c73bdfe3":"For numeric fields, I look at the distribution of values. For categorical fields, look at the top 15 most occurring values. ","dfa49495":"Once this list is created, I use the function I previously defined in order to analyze the description of the various products:","25d67814":"____\n## 7. Classification of Customers - still under construction\n\nIn this part, the objective will be to adjust a classifier that will classify consumers in the different client categories that were established in the previous section. The objective is to make this classification possible at the first visit. To fulfill this objective, I will test several classifiers implemented in `scikit-learn`. First, in order to simplify their use, I define a class that allows to interface several of the functionalities common to these different classifiers.\n\nNOTE - there are design issues around these models. Did we do proper training\/testing? what are we predicting? Caution is needed to beiieve anything beyond here, but it's good examples of using predictive models on this data.","751b1915":"This is a lot of orders to just drop\/ignore. Before deleting the entries that are not assigned to a particular customer, it is worth taking a look at these entries and explore reasons on why they are not associated with any customers.","b06f24a4":"It can be seen, for example, that the first 5 clusters correspond to a strong preponderance of purchases in a particular category of products. Other clusters will differ from basket averages (**mean**), the total sum spent by the clients (**sum**) or the total canceled value as a percent of total spent (**canceled%**).","7179a59a":"** b\/ _Word Cloud_**\n\nNow we can have a look at the type of objects that each cluster represents. In order to obtain a global view of their contents, I determine which keywords are the most frequent in each of them","27a97b61":"### Association Rules Metrics\nSupport: indicates how frequently an antecedent X (i.e.,{Lamps & Bedside Tables}) has appeared in the whole dataset. \n\nConfidence: indicates how often the rule turns out to be true, in our case, what proportion of the antecedent X {Lamps & Bedside Tables} has resulted in the purchase of Y (Beds). \n\nLift: denotes the ratio of: observed Support\/expected Support if X and Y are independent. If Lift = 1, this would mean that no rule can be applied since the occurrences of the two events are independent. If Lift > 1, it implies the degree to which the Y is dependent on X. If Lift < 1, then it means X and Y are substitute to each other (i.e., the presence of one itemset negatively impacts the presence of the other)\n\nConviction: can be interpreted as the ratio of the expected frequency that X occurs without Y (i.e, the frequency that the rule makes an incorrect prediction) if X and Y were independent divided by the observed frequency of incorrect predictions.","395058a3":"Note that when defining the `votingC` classifier, I only used a sub-sample of the whole set of classifiers defined above and only retained the *Random Forest*, the *k-Nearest Neighbors* and the *Gradient Boosting* classifiers. In practice, this choice has been done with respect to the performance of the classification carried out in the next section.\n\n___\n## Testing predictions\n\nIn the previous section, a few classifiers were trained in order to categorize customers. Until that point, the whole analysis was based on the data of the first 10 months. In this section, I test the model  the last two months of the dataset, that has been stored in the `set_test` dataframe:","1498d693":"___\n#### Creation of customer categories","f7101861":"and show the result on a chloropleth map:","57485fd6":"We see that the dataset is largely dominated by orders made from the UK.\n\n___\n###\u00a0Look at Customers and products\n\nThe dataframe contains $\\sim$400,000 entries. What are the number of users and products in these entries ?","19b0a08a":"# 1. Read and Explore Data","7a1bc782":"While looking at the number of null values in the dataframe, it is interesting to note that $\\sim$25% of the entries are not assigned to a particular customer. With the data available, it is impossible to impute values for the user and these entries are thus useless for the current exercise. So I delete them from the dataframe:","f3d4af92":"At first, I look at the number of customers in each cluster:","0ed03f61":"Each entry of the dataframe indicates prices for a single kind of product. Hence, orders are split on several lines. I collect all the purchases made during a single order to recover the total order price:","07b65a4d":"We note that the number of cancellations is quite large ($\\sim$16% of the total number of transactions).\nNow, let's look at the first lines of the dataframe:","e3d6a2f3":"Finally, I randomly split the dataset in train and test sets:","a4c883eb":"Then, I define a classifier that merges the results of the various classifiers:","15e986a9":"### Building Recommender System Using Scikit-Surprise","38c652e8":"Finally, in order to prepare the execution of the classifier, it is sufficient to select the variables on which it acts:","7b9ddb05":"   \n|   | mot 1  |  ...  | mot j  | ...  | mot N  |\n|:-:|---|---|---|---|---|\n| product 1  | $a_{1,1}$  |     |   |   | $a_{1,N}$  |\n| ...        |            |     | ...  |   |   |\n|product i   |    ...     |     | $a_{i,j}$    |   | ...  |\n|...         |            |     |  ... |   |   |\n| product M  | $a_{M,1}$  |     |   |   | $a_{M,N}$   |","0005733c":"### Association Rules Mining\nThis data mining method will find the top co-occurance items and find provide metrics on how strong the found pairs are based on a measure. For example how confident are you, based on the dataset that if a customer buys beer he\/she will also buy diapers?","4bf281ac":"___\n#### Learning curve\n\nA typical way to test the quality of a fit is to draw a learning curve. In particular, this type of curves allow to detect possible drawbacks in the model, linked for example to over- or under-fitting. This also shows to which extent the mode could benefit from a larger data sample. In order to draw this curve, I use the [scikit-learn documentation code again](http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_learning_curve.html#sphx-glr- self-examples-model-selection-pad-learning-curve-py)","b4f52408":"## Observations:\n* Custmer measure values for the 3.0 is repeated more , in the sense more number of customers are repeatedly busying  or ordering something.\n* CVM Group 1 has less repeated number of customers .\n* for 2.0 One time customerspercentage is more when compared to other","3b1592ba":"I create a new field that indicates the total price of every purchase:","2c4d59d2":"95% customers returned less than 10% of their purchase in value, purchased less than 205 different products and visited the shop less than 13 times.","02fdd17f":"### AdaBoost Classifier","97cbd4f1":"#### Separation of data over time\n\nThe dataframe `basket_price` contains information for a period of 12 months. Later, one of the objectives will be to develop a model capable of characterizing and anticipating the habits of the customers visiting the site and this, from their first visit. In order to be able to test the model in a realistic way, I split the data set by retaining the first 10 months to develop the model and the following two months to test it:","1f962315":"From the exploration, although there is no particular invoice or product associated with no Customer ID, majority of the entry is from the UK. I think dropping the entries without investigating is a rushed decision because one possible reason that these entries don't have an associated Customer ID is because the customer is a new customer (i.e. not assigned an ID yet). I think a more reasonable choice would be to assign an independent Customer ID such as the InvoiceNo (since items with the same InvoiceNo would be assumed to be purchased by the same customer).","e00f36ac":"and I output the result as wordclouds:","9373a917":"## Observations\n* From above we can observe that from more number of customers and orders from United Kingdom country\n* Least number of customer's are from Bahrain country Compare to all other countries\n","91733691":"#### Look at StockCode\n\nAbove, it has been seen that some values of the ** StockCode ** field indicate a particular transaction (i.e. D for _Discount_). I check the contents of this field by looking for the set of codes that would contain only letters:","ca7ad87c":"____\n#### Data encoding\n\nNow I will use these keywords to create groups of product. Firstly, I define the $X$ matrix as:","d1ea496b":"Then, I average the contents of this dataframe by first selecting the different groups of clients. This gives access to, for example, the average baskets price, the number of visits or the total sums spent by the clients of the different clusters. I also determine the number of clients in each group (variable ** size **):","d8ee592a":"This allows to have a global view of the content of each cluster:","c893d833":"a list of item antecedent and consequents paired rank ordered by confidence store might employ strategies for either bundle these items, or make the pair  of items physically far away from each other instore so customers looking for the second item (to complete the set) needs to walk across the whole store therefore more chances to \"accidentally\" find new items to buy.","3afd239c":"Finally, we can create a prediction for this model:","247d9909":"# Customer segmentation\n\nThis notebook aims at analyzing the content of an E-commerce database that lists purchases made by $\\sim$4000 customers over a period of one year (from 2010\/12\/01 to 2011\/12\/09). Based on this analysis, I develop a model that allows to anticipate the purchases that will be made by a new customer, during the following year and this, from its first purchase. <br>\n___\n**1. Read and Explore Data**\n\n**2. Remove cancelled Orders and Look at Remaining Records**\n\n**3. Make Product Categories**\n   \n**4. Make Customer Categories**\n\n**5. Make Customer Value Measure**\n   \n**6. Examine Item Co-occurence and Make Recommendations**\n\n**7. Classification of Customers (under construction)**","b661545f":"## Two choice of what to do with the orders with no Customer number. Use one or the other below:","4ae96dbd":"** d \/ _Customers morphology_ **\n\nFinally, I created a representation of the different morphotypes. To do this, I define a class to create \"Radar Charts\" (which has been adapted from this [kernel](https:\/\/www.kaggle.com\/yassineghouzam\/don-t-know-why-employees-leave -read-this)):","7c2e06cb":"We see that the quantity canceled is greater than the sum of the previous purchases.","16cf51dc":"___\n#### Grouping products\n\nIn a second step, I decide to create the **categ_N** variables (with $ N \\in [0: 5]$) that contains the amount spent in each product category:","42fa1362":"In this section, we use the lifetime quantity a customer buys an item as the metric to build a recommender system that gives recommendation based on the items a customer buys. Specifically we use the scikit-surprise package for this task. More info on the package can be found at http:\/\/surpriselib.com\/","0ce0faac":"It can be seen that the vast majority of orders concern relatively large purchases given that $\\sim$65% of purchases give prices in excess of \u00a3 200. There's interesting aspects to this distribution, for example a spike arounf 300, and a rise around 100. ","17f882bb":"It can be seen that the data has 4372 users and that they bought 3684 different products. The total number of transactions carried out is of the order of $\\sim$22'000.\n\nNow I will determine the number of products purchased in every transaction:","aa59899b":"Up to now, the information related to a single order was split over several lines of the dataframe (one line per product). I decide to collect the information related to a particular order and put in in a single entry. I therefore create a new dataframe that contains, for each order, the amount of the basket, as well as the way it is distributed over the 5 categories of products:","29d298cd":"and then check for the amount of variance explained by each component:","52ee3bdc":"It's worth noting that because the majority of the matrix is sparse and a product must be categorized to one of the price baskets, the clustering result is dominated by the price range. In one extreme iteration, the six clusters are exactly separated by the price range.","734d36e4":"From this representation, it can be seen, for example, that the first principal component allow to separate the tiniest clusters from the rest. More generally, we see that there is always a representation in which two clusters will appear to be distinct.\n\n** b\/ _Score de silhouette intra-cluster_ **\n\nAs with product categories, another way to look at the quality of the separation is to look at silouhette scores within different clusters:","fb03f107":"### Explicit recommender systems\nUsing the quantity customers buy as an explicit measure of \"rating\", this factorization model will recommend items based on given customerID.\n\nThis recommendation model can help pinpoint specific items to recommend to specific customer.","90532340":"###### ___\n## Conclusion\n\nThe work described in this notebook is based on a database providing details on purchases made on an E-commerce platform over a period of one year. Each entry in the dataset describes the purchase of a product, by a particular customer and at a given date. In total, approximately $\\sim$4000 clients appear in the database. Given the available information, I decided to develop a classifier that allows to anticipate the type of purchase that a customer will make, as well as the number of visits that he will make during a year, and this from its first visit to the E-commerce site.\n\nThe first stage of this work consisted in describing the different products sold by the site, which was the subject of a first classification. There, I grouped the different products into 6 main categories of goods. In a second step, I performed a classification of the customers by analyzing their consumption habits over a period of 10 months. I have classified clients into 11 major categories based on the type of products they usually buy, the number of visits they make and the amount they spent during the 10 months. Once these categories established, I finally trained several classifiers whose objective is to be able to classify consumers in one of these 8 categories and this from their first purchase. For this, the classifier is based on variables which are:\n- ** mean **: amount of the basket of the current purchase\n- ** categ_N ** with $N \\in [0: 4]$: percentage spent in product category with index $N$\n\nFinally, the quality of the predictions of the different classifiers was tested over the last two months of the dataset. The data were then processed in two steps: first, all the data was considered (ober the 2 months) to define the category to which each client belongs, and then, the classifier predictions were compared with this category assignment. I then found that 75% of clients are awarded the right classes.\nThe performance of the classifier therefore seems correct given the potential shortcomings of the current model. In particular, a bias that has not been dealt with concerns the seasonality of purchases and the fact that purchasing habits will potentially depend on the time of year (for example, Christmas ). In practice, this seasonal effect may cause the categories defined over a 10-month period to be quite different from those extrapolated from the last two months. In order to correct such bias, it would be beneficial to have data that would cover a longer period of time.","4bb1ee6d":"A customer category of particular interest is that of customers who make only one purchase. I find that this type of customer represents 1\/3 of the customers listed. When doing customer segmentation, I'll put all these \"one-time-buyers\" into their own segment.","d1b97f66":"On this curve, we can see that the train and cross-validation curves converge towards the same limit when the sample size increases. This is typical of modeling with low variance and proves that the model does not suffer from overfitting. Also, we can see that the accuracy of the training curve is correct which is synonymous of a low bias. Hence the model does not underfit the data.\n\n___\n### Logistic Regression\n\nI now consider the logistic regression classifier. As before, I create an instance of the `Class_Fit` class, adjust the model on the training data and see how the predictions compare to the real values:","467637b5":"###\u00a0Decision Tree","cd5e875c":"# Itemsets Analysis and Customer Buying Habits","497a6389":"Among all the cancellation entries, the lines listed in the `entry_to_remove` list are the cancellation orders in which all the units canceled are fully accounted for. The lines listed in the `doubtful_entry` list correspond to the cancellations that no matching purchase order beforehand can be found. The lines listed in the ` partial_doubtful_entry` list represent cancellation orders in which the units canceled are partially accounted. In practice, I decide to delete all of these entries, which count respectively for  $\\sim$1.9%, 0.3% and 0.01% of the dataframe entries.\n\nNow all transactions that are not discount but have negative quantities have been removed.","764fd845":"In a first step, I regroup reformattes these data according to the same procedure as used on the training set. However, I am correcting the data to take into account the difference in time between the two datasets and weights the variables ** count ** and ** sum ** to obtain an equivalence with the training set:","a2f09786":"\n** a \/ _Silhouette intra-cluster score_ **\n\nIn order to have an insight on the quality of the classification, we can represent the silhouette scores of each element of the different clusters. This is the purpose of the next figure which is taken from the [sklearn documentation](http:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_silhouette_analysis.html):","0182c327":"Once this instance is created, I adjust the classifier to the training data:","66db7892":"___\n#### Confusion matrix\n\nThe accuracy of the results seems to be correct. Nevertheless, let us remember that when the different classes were defined, there was an imbalance in size between the classes obtained. In particular, one class contains around 40% of the clients. It is therefore interesting to look at how the predictions and real values compare to the breasts of the different classes. This is the subject of the confusion matrices and to represent them, I use the code of the [sklearn documentation](http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html):","95e4e174":"and to choose the appropriate ranges, I check the number of products in the different groups:","8e3a70d7":"#### Formulas for metrics\n- support(A->C) = support(A+C) [aka 'support'], range: [0, 1]\n\n- confidence(A->C) = support(A+C) \/ support(A), range: [0, 1]\n\n- lift(A->C) = confidence(A->C) \/ support(C), range: [0, inf]\n\n- leverage(A->C) = support(A->C) - support(A)*support(C),\nrange: [-1, 1]\n\n- conviction = [1 - support(C)] \/ [1 - confidence(A->C)],\nrange: [0, inf]","6d282d41":"The goal of constructing an association rules framework is to find interesting sales relations between various items within the transaction dataset. Ultimately, it will help us discover items people will most likely to buy given the goods in the previous purchase. Consider a scenario where the transaction data is organized into a large matrix which contains the transaction IDs and the names of purchased item. In each entry a value of 1 is assigned if the item is present in the corresponding transaction, 0 if otherwise. For example, if an association rule is defined as {Lamps, Bedside Tables} => {Beds}, indicating that if lamps and bedside tables are purchased, customers will also buy beds.","20c19aa5":"Here, I quickly look at the countries from which orders were made:"}}