{"cell_type":{"dfbc4a08":"code","97bf6abd":"code","8b1b264d":"code","8be6bb0e":"code","4f3d91c0":"code","173f959c":"code","e4706433":"code","acb7600b":"code","a1fbbe84":"code","c83c8dd0":"code","32c6ea8e":"code","fd651f4a":"code","61d4ffe0":"code","98f07735":"code","e628724b":"code","a736b0cc":"code","18ae5869":"code","79b29f58":"code","fc34d304":"code","e5935e29":"code","b7f27763":"code","96eab5fd":"code","278fd462":"code","e50c2351":"code","f9c03c96":"code","c8b91f2f":"code","67ccb362":"code","683512ca":"code","673fb938":"code","ec8679f6":"code","55751c6f":"code","ac6b44b4":"code","3aad1f0e":"markdown","fa3d8648":"markdown","f7fcfc4c":"markdown","de541bc6":"markdown","fb78136a":"markdown","e4f58ddc":"markdown","fd5ec4f3":"markdown","8aa943b1":"markdown","9a37c2ac":"markdown","8a1f38c8":"markdown","71a9a58b":"markdown","92b37494":"markdown","81393683":"markdown","505a7894":"markdown","c6535da5":"markdown","c773c7e3":"markdown","8edd40b9":"markdown","c088cd8f":"markdown","5c6999bd":"markdown","c494e2f9":"markdown","9a4e1b2d":"markdown","112fea80":"markdown","d51d4bbc":"markdown","8a520892":"markdown","c136a166":"markdown","46b0e107":"markdown","69b08499":"markdown","f00fb7ab":"markdown","6674bd28":"markdown","03b0d801":"markdown","60e1790e":"markdown","7fdd74f4":"markdown","29015c13":"markdown","30e896f4":"markdown","e5f43001":"markdown","7f7ffd86":"markdown","122684d1":"markdown","38928a45":"markdown","df4d0e06":"markdown","90525174":"markdown","66893c77":"markdown","b8fd75a2":"markdown"},"source":{"dfbc4a08":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","97bf6abd":"import seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score","8b1b264d":"traindf=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntestdf=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","8be6bb0e":"traindf.info()","4f3d91c0":"traindf.describe()","173f959c":"traindf.head(10)","e4706433":"survive = 'Alive'\nnotsurvive = 'Dead'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nwomen = traindf[traindf['Sex']=='female']\nmen = traindf[traindf['Sex']=='male']\nax = sns.histplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survive, ax = axes[0], kde =False)\nax = sns.histplot(women[women['Survived']==0].Age.dropna(),color=\"red\", bins=40, label = notsurvive, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Female')\n\nax = sns.histplot(men[men['Survived']==0].Age.dropna(),color=\"red\", bins=40, label = notsurvive, ax = axes[1], kde = False)\nax = sns.histplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survive, ax = axes[1], kde = False)\nax.legend()\nax.set_title('Male')","acb7600b":"sns.barplot(x='Pclass', y='Survived', data=traindf)\n\nsurvived = traindf.loc[traindf.Pclass == 1][\"Survived\"]\nprint(\"The people survived Pclass 1 = \",sum(survived),\" out of \",len(survived))\n\nsurvived = traindf.loc[traindf.Pclass == 2][\"Survived\"]\nprint(\"The people survived Pclass 2 = \",sum(survived),\" out of \",len(survived))\n\nsurvived = traindf.loc[traindf.Pclass == 3][\"Survived\"]\nprint(\"The people survived Pclass 3 = \",sum(survived),\" out of \",len(survived))","a1fbbe84":"traindf['Embarked'].describe()","c83c8dd0":"FacetGrid = sns.FacetGrid(traindf, row='Embarked', height=4.5, aspect=1.6)\nFacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )\nFacetGrid.add_legend()","32c6ea8e":"plt.figure(figsize=(8, 5))\nsns.set_style('whitegrid')\nsns.countplot(x = 'Embarked',hue='Survived',data=traindf, palette = \"Set2\" )","fd651f4a":"d = [traindf, testdf]\nfor data in d:\n    data['related'] = data['SibSp'] + data['Parch'] \n    data.loc[data['related'] > 0, 'alone'] = 1\n    data.loc[data['related'] == 0, 'alone'] = 0\n    data['alone'] = data['alone'].astype(int)\ntraindf['alone'].value_counts()","61d4ffe0":"axes = sns.pointplot(x='related',y='Survived', data=traindf)","98f07735":"traindf['Cabin'] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in traindf['Cabin'] ])\ntestdf['Cabin'] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in testdf['Cabin'] ])","e628724b":"plt.figure(figsize=(8, 5))\ng = sns.catplot(y=\"Survived\",x=\"Cabin\",data=traindf,kind=\"bar\",order=['A','B','C','D','E','F','G','X'])","a736b0cc":"traindf[\"Cabin\"] = traindf[\"Cabin\"].map({\"X\":0, \"A\":1, \"B\" : 2 , \"C\":3, \"D\":4, \"E\":5, \"F\":6, \"G\":7,\"T\":0})\ntraindf[\"Cabin\"] = traindf[\"Cabin\"].astype(int)\ntestdf[\"Cabin\"] = testdf[\"Cabin\"].map({\"X\":0, \"A\":1, \"B\" : 2 , \"C\":3, \"D\":4, \"E\":5, \"F\":6, \"G\":7,\"T\":0})\ntestdf[\"Cabin\"] = testdf[\"Cabin\"].astype(int)","18ae5869":"data = [traindf, testdf]\nfor dataset in data:\n    mean = traindf[\"Age\"].mean()\n    std = testdf[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = traindf[\"Age\"].astype(int)\n    ","79b29f58":"val = 'S'\nd = [traindf, testdf]\nfor dataset in d:\n    dataset['Embarked'] = dataset['Embarked'].fillna(val)\n    \nfor dataset in d:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)","fc34d304":"genders = {\"male\": 0, \"female\": 1}\nports = {\"S\": 1, \"C\": 2, \"Q\": 3}\nd = [traindf, testdf]\n\nfor dataset in d:\n    dataset['Sex'] = dataset['Sex'].map(genders)\n    \nfor dataset in d:\n    dataset['Embarked'] = dataset['Embarked'].map(ports)","e5935e29":"d = [traindf, testdf]\nfor dataset in d:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6\n","b7f27763":"d = [traindf, testdf] \nfor dataset in d: \n    dataset.loc[ dataset['Fare'] <= 7.00, 'Fare'] = 0 \n    dataset.loc[(dataset['Fare'] > 7.00) & (dataset['Fare'] <= 8.00), 'Fare'] = 1 \n    dataset.loc[(dataset['Fare'] > 8.00) & (dataset['Fare'] <= 14.0), 'Fare']   = 2 \n    dataset.loc[(dataset['Fare'] > 14) & (dataset['Fare'] <= 26), 'Fare']   = 3 \n    dataset.loc[(dataset['Fare'] > 26) & (dataset['Fare'] <= 55.5), 'Fare']   = 4 \n    dataset.loc[ dataset['Fare'] > 55.5, 'Fare'] = 5 \n    dataset['Fare'] = dataset['Fare'].astype(int)","96eab5fd":"d = [traindf, testdf]\n\nfor dataset in d:\n    dataset['Fare_Per_Person'] = dataset['Fare']\/(dataset['related']+1)\n    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)","278fd462":"for dataset in d:\n    dataset['Pclass_Embarked'] = dataset['Embarked']*(dataset['Pclass'])","e50c2351":"for dataset in d:\n    dataset['Age_Pclass'] = dataset['Age']*(dataset['Pclass'])","f9c03c96":"traindf=traindf.drop(['PassengerId','Name','Ticket','alone'],axis=1)\ntestdf=testdf.drop(['Name','Ticket','alone'],axis=1)\ntraindf.head()","c8b91f2f":"testdf.head()","67ccb362":"X_train = traindf.drop(\"Survived\", axis=1)\nY_train = traindf[\"Survived\"]\nX_test  = testdf.drop(\"PassengerId\", axis=1).copy()","683512ca":"rf = RandomForestClassifier(n_estimators=100)\nrf.fit(X_train, Y_train)\n\nYpred =rf.predict(X_test)\n\nrf.score(X_train, Y_train)\nscore  = round(rf.score(X_train, Y_train) * 100, 2)\nscore","673fb938":"from sklearn.metrics import confusion_matrix\nsub=pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nconfusion_matrix(sub[\"Survived\"], Ypred)\n","ec8679f6":"rf = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(rf, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","55751c6f":"submission = pd.DataFrame({'PassengerId':testdf['PassengerId'],'Survived':Ypred})\nsubmission.head()","ac6b44b4":"submission.to_csv('Titanic_Predictions.csv',index=False)\nprint('File Saved.')","3aad1f0e":"#### If a passenger have more than 1 and less than 3, they have increasing chance of survival. With three relatives, you have the best survival rate. Whereas after three, a passenger have a pretty low survival rate.","fa3d8648":"### Change Cabin alphabet into integers for building model","f7fcfc4c":"### Filling up the missing values of Age feature using random values which is based on mean and std value.","de541bc6":"## Cabin Vs Survived","fb78136a":"# Embarked Vs Pclass","e4f58ddc":"#### These graph shows that there is a high probability for women to survive the sinking than men. For women, the highest percent of women who might be saved were around 15 to 42 years old.\n\n#### For men, it's relatively low whereas the infants might stand a chance for survival.","fd5ec4f3":"# Cabin","8aa943b1":"### Saving the predicted file","9a37c2ac":"#### We can use SibSp and Parch to see whether the survival rate increases or decreases if relatives or family is present on the ship. We will make a new feature to determine whether a passenger is alone or not.\n\n#### I still don't know we will need it or not but we have follow all the leads","8a1f38c8":"#### Let's separate the train and test sets","71a9a58b":"# Data Analysis and Vizualisation","92b37494":"## Let's turn Age and Fare feature into groups which will helps later on. Just be careful wih dividing the groups, make it equal.","81393683":"#### Building the model ","505a7894":"### For Fare feature, we can't just divide it in the usual way. If we did, more tah 80% of values will fall under in the first group. Thus, we will use qcut() because qcut tries to divide up the underlying data into equal sized bins. This is based on percentile on distribution of data and not the number of bins.","c6535da5":"#### Earlier I thought i moght drop this row but there is a great way to make this a feature useful. As the starting of eac Cabin has a alphabet. This alphabet represent the deck where the passenger is staying. It might prove a second-best lead till now. Let X be the missing values.","c773c7e3":"> #### d = [traindf, testdf]\n> for dataset in d:\n> > fare = pd.qcut(dataset['Fare'], q=' ',duplicates='drop')        \n> #### fare\n\n#### I didn't put this code in this notebook.","8edd40b9":"#### The survival rate highly depends on the gender of the passenger. In these graphs, it shows that on port Q and S, there is a higher probabailty of survival for women. Whereas, at port C, men has higher survival rate.\n\n#### Let us see how many survived or not at each embarked to see things more clearer.","c088cd8f":"# Embarked Vs Sex Vs Pclass","5c6999bd":"## Impoerting the Libraries","c494e2f9":"# Predictions of survival pattern using Titanic Dataset","9a4e1b2d":"The sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nLet's build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).","112fea80":"# Relative Vs Survived","d51d4bbc":"### Making all the other features to integer for using in making models","8a520892":"# Conclusion ","c136a166":"#### Let's see how many we did predict right","46b0e107":"#### Let's drop the features we don't need in training and testing dataframes. I also noticed that Alone feature is not that important. So, i also dropped it. ","69b08499":"### Pclass times Embarked","f00fb7ab":"### Fare Per Person","6674bd28":"## Creating new features after see the data and how they are co-related tp each other\n","03b0d801":"### I am used different models and Random Forest Classifier is the best suited for now.","60e1790e":"## Reading the data in","7fdd74f4":"#### 220 predictions are true positves and 112 are true negatives. Around 82% accuracy can be found using these features and this model.","29015c13":"## Let's use K-Fold Cross Validation","30e896f4":"# Age Vs Sex","e5f43001":"#### The higher Pclass has the best survival rate than other Pclasses. Whereas Pclass 3 has a low probability of survival.","7f7ffd86":"#### The training dataset has null and missing values in Cabin, Age and Embarked. The Cabin row has the most missing datas. I might drop this row as it's missing more than 70% of the data.","122684d1":"# Let's build the model","38928a45":"#### Our model has a acuuracy of 82% with standard deviation of 3% which means that the acuuracy of this model can differ +-4%.","df4d0e06":"### It's a great dataset to start Machine Learning journey. The features can be made more accurate. I dropped the Ticket and Name feature, but further analysis can prove that those features are more important. The model can be improved a lot. Feel free to explore on you own. During data pre-processing session, the missing values can be handled more efficiently and accurately and new features can be created to better performance of the model.\n\n### More extensive research and comparing and plotting feature against others and remove the other unnecessary features will make improvement. I didn't use hyperparameter  for Random Forest Classifier as i am still learning about it. Extensive use of hyperparameter tuning on the model can improve the overall performance.","90525174":"### Age times Pclass","66893c77":"### Filling up the missing values of Embarked with the common value and for Fare, i am just putting zero","b8fd75a2":"# Data Pre-processing "}}