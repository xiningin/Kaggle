{"cell_type":{"1a86685b":"code","ead5756a":"code","8c6ca984":"code","621c7ed4":"code","4d7c01a4":"code","2dd5e61c":"code","33b54d14":"code","23ac951d":"code","3d7b4862":"code","69c17b60":"code","35ddcda1":"code","9eddd7a9":"code","b65c8e7a":"code","fd4360e0":"markdown","8c3db01c":"markdown","caaf3b42":"markdown","e4f07eee":"markdown","d82d528a":"markdown","94366a4f":"markdown","a1a6361b":"markdown","16e3caf2":"markdown","dfe3149f":"markdown","ea2c983e":"markdown","a9687d38":"markdown","454fc546":"markdown"},"source":{"1a86685b":"import spacy\nfrom spacy import displacy\n\n# Load English tokenizer, tagger, parser and NER\nnlp_sm = spacy.load(\"en_core_web_lg\")","ead5756a":"def print_entities(pipeline, text):\n    \n    # Create a document \n    document = pipeline(text)\n    \n    # Entity text & label extraction\n    for entity in document.ents:\n        print(entity.text + '->', entity.label_)\n        \n    # Show entities in pretty manner\n    #displacy.render(document, jupyter=True, style='ent')","8c6ca984":"short_text = \"\"\"\nIn the search for life beyond Earth, NASA's Curiosity rover has been on a nearly decade-long mission to determine if Mars was ever habitable for living organisms.\nA new analysis of sediment samples collected by the rover revealed the presence of carbon -- and the possible existence of ancient life on the red planet is just one potential explanation for why it may be there.\nCarbon is the foundation for all of life on Earth, and the carbon cycle is the natural process of recycling carbon atoms. On our home planet, carbon atoms go through a cycle as they travel from the atmosphere to the ground and back to the atmosphere. Most of our carbon is in rocks and sediment and the rest is in the global ocean, atmosphere and organisms, according to NOAA, or the National Oceanic and Atmospheric Administration\"\"\"\n\n\nlong_text = \"\"\"\nThey represent a stake in a phenomenally successful company. In terms of impact on the everyday lives of hundreds of millions of people, Apple\u2019s founders and their successors stand alongside Henry Ford and Alexander Graham Bell. The company\u2019s key products have entered the language, whether the Mac computer, the iPhone, the Apple Watch and, of course, iTunes, described as the world\u2019s biggest juke-box. There was even a 2015 film about the company that took its name from co-founder Steve Jobs.\nFollow Apple (NASDAQ: AAPL) publications and financial reports in their Investor Relations Page.\nGood news for consumers, undoubtedly, and good news also for investors. Apple\u2019s recent results, covering the three months to December 31 2016, saw the company\u2019s chief financial officer Luca Maestri announce: \u2018We returned nearly $15 billion to investors through share re-purchases and dividends during the quarter.\u2019 The quarterly dividend itself was 57 cents a share, identical to the dividend for the previous three quarters and up on the 52 cents paid for each of the four quarters before that.\nBusiness is brisk at Apple. On January 31, Tim Cook, Apple\u2019s chief executive, said of the last three months of 2016: \u2018We\u2019re thrilled to report that our holiday quarter results generated Apple\u2019s highest quarterly revenue ever, and broke multiple records along the way. We sold more iPhones than ever before and set all-time revenue records for iPhone, Services, Mac and Apple Watch.\u2019\"\"\"","621c7ed4":"print_entities(nlp_sm, short_text)","4d7c01a4":"#document_2 = nlp_sm(long_text)\n#print_entities(document_2)\nprint_entities(nlp_sm, long_text)","2dd5e61c":"def visualize_entities(model, text):\n    \n    # Create a document \n    document = model(text)\n        \n    # Show entities in pretty manner\n    displacy.render(document, jupyter=True, style='ent')\n    #from spacy import displacy\n    #displacy.render(doc, style='dep', jupyter=True, options={'distance': 130})","33b54d14":"# On the short text\nvisualize_entities(nlp_sm, short_text)","23ac951d":"# On the long text\nvisualize_entities(nlp_sm, long_text)","3d7b4862":"#!python3 -m spacy download en_core_web_sm\n\n# Load the spacy transformer (roberta-base) model\nroberta_nlp = spacy.load(\"en_core_web_sm\")","69c17b60":"# Entities on short text\nprint_entities(roberta_nlp, short_text)","35ddcda1":"# Entities on long text\nprint_entities(roberta_nlp, long_text)","9eddd7a9":"# On the short text\nvisualize_entities(roberta_nlp, short_text)","b65c8e7a":"# On the short text\nvisualize_entities(roberta_nlp, long_text)","fd4360e0":"<p style=\"font-size:18px; font-family:verdana; line-height: 1.7em\">RoBERTa stands for Robustly Optimized BERT Pre-training Approach. It was presented by researchers at Facebook and Washington University. The goal of this paper was to optimize the training of BERT architecture in order to take lesser time during pre-training.\n    \n<p style=\"font-size:18px; font-family:verdana; line-height: 1.7em\">Modifications to BERT:<br>\nRoBERTa has almost similar architecture as compare to BERT, but in order to improve the results on BERT architecture, the authors made some simple design changes in its architecture and training procedure. These changes are:\nRemoving the Next Sentence Prediction (NSP) objective\nTraining with bigger batch sizes & longer sequences\nDynamically changing the masking pattern","8c3db01c":"<p style=\"font-size:18px; font-family:verdana; line-height: 1.7em\">We can modify our print_entities function by adding the displacy function in order to have a nice visualization of all the entities","caaf3b42":"<p style=\"font-size:40px; font-family:verdana; line-height: 1.7em; font-weight: 550;\"> NER With Traditional spaCy and spaCy Transformers - RoBERTa","e4f07eee":"<blockquote><p style=\"font-size:18px; color:#D4AF37; font-family:verdana;\">\nRule-based Parsing, python<\/br>\nDictionary lookups,<\/br>\nPOS Tagging,<\/br>\nDependency Parsing.<\/blockquote>","d82d528a":"<blockquote><p style=\"font-size:18px; color:#D4AF37; font-family:verdana;\">1. Named Entity Recognition is one of the key entity detection methods in NLP.\n<p style=\"font-size:18px; color:#D4AF37; font-family:verdana;\">2. Named entity recognition is a natural language processing technique that can automatically scan entire articles and pull out some fundamental entities in a text and classify    them into predefined categories. Entities are, Organizations, Quantities, Monetary values, Percentages, People\u2019s names, Company names,Geographic locations, Product names, Dates and times, Amounts of money, Names of events.\n<p style=\"font-size:18px; color:#D4AF37; font-family:verdana;\">3. In simple words, Named Entity Recognition is the process of detecting the named entities such as person names, location names, company names, etc from the text.\n<p style=\"font-size:18px; color:#D4AF37; font-family:verdana;\">4. With the help of named entity recognition, we can extract key information to understand the text, or merely use it to extract important information to store in a database.","94366a4f":"# <p style=\"background-color:#D4AF37;font-family:verdana;color:#00000;font-size:130%;text-align:center;border-radius:60px 50px;\">spaCy Transformer - roBERTa<\/p>","a1a6361b":"<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em\">Named-entity recognition (NER) (also known as (named) entity identification, entity chunking, and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.<\/p>","16e3caf2":"<p style=\"background-color:#D4AF37;font-family:verdana;font-size:20px;line-height:1.7em;text-align:center;border-radius:5px 5px\">Entities are the most important chunks of a particular sentence such as noun phrases, verb phrases, or both. Generally, Entity Detection algorithms are ensemble models of:","dfe3149f":"<p style=\"font-size:18px; font-family:verdana; line-height: 1.7em\">\nDatasets Used:<br>\nThe following are the datasets used to train ROBERTa model:<br><br>\n<code>BOOK CORPUS and English Wikipedia dataset:<\/code> This data also used for training BERT architecture, this data contains 16GB of text.\n<code>CC-NEWS:<\/code> This data contains 63 million English news articles crawled between September 2016 and February 2019. The size of this dataset is 76 GB after filtering.<br>\n<code>OPENWEBTEXT:<\/code> This dataset contains web content extracted from the URLs shared on Reddit with at least 3 upvotes. The size of this dataset is 38 GB.<br>\n<code>STORIES:<\/code> This dataset contains a subset of Common Crawl data filtered to match the story-like style of Winograd NLP task. This dataset contains 31 GB of text.","ea2c983e":"<p style=\"font-size:18px; font-family:verdana; line-height: 1.7em\">Experimentation Data<br>\nWe will be performing the entity extraction of two different text data, a short text and a much longer one respectively (from CNN and Etoro) and finally, compare the performance of traditional spaCy and roBERTa.","a9687d38":"<p style=\"font-size:18px; font-family:verdana; line-height: 1.7em\">For Example,<br>\nAt the party Thrusday night at Chateau Marmont, Cate Blanchett barely made it up in the elevator.<br>\n<p style=\"font-size:18px; font-family:verdana; line-height: 1.7em\">\n    <code>Date:<\/code> Thursday, <code>Time:<\/code> night, <code>Location:<\/code> Chateau Marmont, <code>Person:<\/code> Cate Blanchett","454fc546":"# <p style=\"background-color:#D4AF37;font-family:verdana;color:#00000;font-size:130%;text-align:center;border-radius:60px 50px;\">Traditional spaCy<\/p>"}}