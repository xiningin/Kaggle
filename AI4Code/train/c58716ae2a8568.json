{"cell_type":{"44e162a0":"code","106f5dec":"code","428e4565":"code","d7a78388":"code","6d4e36cf":"code","489e72d9":"code","2d71e4cd":"code","54f1481e":"code","979c460a":"code","fc615662":"code","b4deb5b1":"code","b342845f":"code","c33327df":"code","892887f4":"code","a25ae3b3":"code","1b808d56":"code","2b98334c":"code","c5aef00a":"code","ead6ea51":"code","11283fae":"code","90900ce1":"code","667cae2f":"code","367f92ed":"code","41fe7908":"code","1a72e44d":"code","60bb0fc5":"code","7f001082":"code","47eed6a1":"code","ac222563":"code","4fe53b20":"code","3f1b4e47":"code","e854ef61":"code","c25e7c72":"code","9fa3de08":"markdown","12470359":"markdown","f3291c65":"markdown","419c8c53":"markdown","2f709ebd":"markdown","ce2c18da":"markdown","35b20e12":"markdown","db53fb20":"markdown","9ea25d49":"markdown","52716d00":"markdown","0b8d0512":"markdown","7614f3df":"markdown","fb67ade8":"markdown","93bc4053":"markdown","2f6533f2":"markdown","f221bf28":"markdown","b626b91b":"markdown"},"source":{"44e162a0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","106f5dec":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report, plot_roc_curve\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\nimport plotly.express as px\nsns.set_theme(style='darkgrid')\n","428e4565":"df = pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\ndf.head()","d7a78388":"df.info()","6d4e36cf":"df.isnull().sum()","489e72d9":"df.describe()\n","2d71e4cd":"# check for how many womens are prone to heart-attack\nwomen_stroke = df.loc[df.sex == 0]['output']\nwomen_stroke_percentage = sum(women_stroke)\/len(women_stroke)\nprint('The % of womens prone to heart-attack: {}%'.format(women_stroke_percentage*100))","54f1481e":"women = df[df['sex']==0]\nmen = df[df['sex']==1]\n","979c460a":"def cp_density(women, men):\n    plt.figure(figsize=(16,6))\n    sns.kdeplot(women['cp'], label = 'Women cp values', shade = True, color='red')\n    sns.kdeplot(men['cp'], label = 'men cp values', shade = True, color = 'blue')\n    plt.legend()\n    plt.title('Cp values comparision', weight='bold')\n    plt.xlabel('Cp values')\n    \ndef resting_blood_pressure_comparision(women, men):\n    plt.figure(figsize=(16,6))\n    sns.kdeplot(women['trtbps'], label = 'Women rbp values', shade = True, color='red')\n    sns.kdeplot(men['trtbps'], label = 'men rbp values', shade = True, color = 'blue')\n    plt.title('Resting blood pressure values comparision', weight='bold')\n    plt.legend()\n    plt.xlabel('trtbps values')\n    \ndef cholestarol(women, men):\n    plt.figure(figsize=(16,6))\n    sns.kdeplot(women['chol'], label = 'Women cholestoral values', shade = True, color='green')\n    sns.kdeplot(men['chol'], label = 'men cholestoral values', shade = True, color = 'black')\n    plt.title('Cholestarol values comparision', weight='bold')\n    plt.legend()\n    plt.xlabel('Cholestral values')\n    \ndef resting_electrocardiographic(women, men):\n    plt.figure(figsize=(16,6))\n    sns.kdeplot(women['restecg'], label = 'Women resting electrocardiographic values', shade = True, color='green')\n    sns.kdeplot(men['restecg'], label = 'men resting electrocardiographic values', shade = True, color = 'black')\n    plt.title('resting electrocardiographic values comparision', weight='bold')\n    plt.legend()\n    plt.xlabel('resting electrocardiographic values')\n\ndef maximum_heartrate_achieved(women, men):\n    plt.figure(figsize=(16,6))\n    sns.kdeplot(women['thalachh'], label = 'Women cp values', shade = True, color='black')\n    sns.kdeplot(men['thalachh'], label = 'men cp values', shade = True, color = 'yellow')\n    plt.title('Maximum heart-rate achieved values comparision', weight='bold')\n    plt.legend()\n    plt.xlabel('Maximum heart-rate achieved values')\n    ","fc615662":"cp_density(women, men)","b4deb5b1":"resting_blood_pressure_comparision(women, men)","b342845f":"cholestarol(women, men)","c33327df":"resting_electrocardiographic(women, men)","892887f4":"maximum_heartrate_achieved(women, men)","a25ae3b3":"plt.figure(figsize=(10,10))\nsns.pairplot(df, hue='output')","1b808d56":"df.head()","2b98334c":"def eda_for_several_attributes(data):\n    features = ['slp','thalachh','age','caa']\n    for var in features:\n        plt.figure(figsize=(16,6))\n        g = sns.scatterplot(data = data, x = 'oldpeak', y = data[var], hue = 'output')\n        plt.title(var, weight='bold')\n        plt.grid(linestyle = '--', axis = 'y')\n        plt.show()\n            \neda_for_several_attributes(df)","c5aef00a":"# check for the age and sex\ndef check_the_risk(data):\n    for var in ['sex','age']:\n        plt.figure(figsize=(16,8))\n        g = sns.countplot(data=data, x = data[var], hue = data.output, palette='OrRd')\n        plt.grid(linestyle = '--', axis='y')\n        plt.title(var)\n        \n        for i in ['top','left']:\n            g.spines[i].set_visible(False)\n            \ncheck_the_risk(df)","ead6ea51":"def find_correlational_map(data):\n    plt.figure(figsize=(16,12))\n    sns.heatmap(data.corr(), annot=True, cmap='OrRd')\n    plt.title('Correlational Map', weight='bold')\n    print('---'*50)\n    print(data.corr().output.sort_values(ascending = False))\n    plt.tight_layout()\n    \nfind_correlational_map(df)","11283fae":"df.head()","90900ce1":"X = df.iloc[:,:-1].values\ny = df.iloc[:,-1].values","667cae2f":"X.shape, y.shape","367f92ed":"# Let's split the date into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\nX_train.shape, X_test.shape","41fe7908":"y_train.shape, y_test.shape","1a72e44d":"# let's create a pipline \npipeline = make_pipeline(RobustScaler()) # creating pipeline for model building\n\nLR = make_pipeline(pipeline, LogisticRegression(random_state=0)) # LogisticRegression pipeline\nDT = make_pipeline(pipeline, DecisionTreeClassifier(random_state=0)) # DecisionTree Classifier pipeline\nRF = make_pipeline(pipeline, RandomForestClassifier(random_state=0)) # RandomForest Classifier pipeline\nAC = make_pipeline(pipeline, AdaBoostClassifier(random_state=0)) # Adaboost Classifier pipeline\nNB = make_pipeline(pipeline, GaussianNB()) # Naive bayes pipeline\nKN = make_pipeline(pipeline, KNeighborsClassifier()) # KNeighbor pipeline\nSV = make_pipeline(pipeline, SVC(random_state=0)) # Support vector pipeline","60bb0fc5":"# creating model_dict\nmodel_dictionary = {\n    'Logistic_Regression':LR,\n    'DecisionTree_Classifier':DT,\n    'RandomForest_classifier':RF,\n    'Adaboost_Classifier':AC,\n    'Naivebayes_Classifier':NB,\n    'KNeighbors_classifier':KN,\n    'Support_Vector':SV\n}","7f001082":"print(model_dictionary)","47eed6a1":"# define a function to fit the model and return it's accuracy, classification report and confusion matrix\ndef model_fitting(model):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print('The accuracy score of the model is: {}%'.format(accuracy_score(y_test, y_pred)* 100))\n    print('-----'*20)\n    print(confusion_matrix(y_test, y_pred))\n    print(classification_report(y_test, y_pred))","ac222563":"for name, model in model_dictionary.items():\n    print('---'*10)\n    print(name)\n    model_fitting(model)","4fe53b20":"model = AdaBoostClassifier(random_state=0)\nmodel.fit(X_train, y_train)","3f1b4e47":"y_pred = model.predict(X_test)\naccuracy_score(y_test, y_pred)","e854ef61":"def find_confusion_matrix(y_test, y_pred):\n    cm = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, cmap='OrRd')\n    plt.title('Confusion Matrix', weight='bold')\n    print(classification_report(y_test, y_pred))\n    plot_roc_curve(model, X_test, y_test)\n    \n    \nfind_confusion_matrix(y_test, y_pred)","c25e7c72":"print('The accuracy of the model is: {}%'.format(round(accuracy_score(y_test, y_pred)*100, 2)))","9fa3de08":"* Men's Resting blood pressure values has the probable density of 120.\n* Women's Resting blood pressure values has the probable density of 135.","12470359":"* This shows that most of the men's are free from heart-attacks.\n* Both the genders show's spike for non-anginal pain. Women has the great tendency for non-anginal chest pain.","f3291c65":"# **Data Splitting**","419c8c53":"# **EDA (Exploratory Data Analysis)**","2f709ebd":"# **Selecting The Best Model**","ce2c18da":"* Predominently women shows probable or definite left ventricular hypertrophy by Estes.","35b20e12":"Check for the following relationship;\n* cp\n* trtbps\n* chol\n* restecg\n* thalachh\n* oldpeak","db53fb20":"There is a spike in for heart-attack at the age of 41, 44, 51, 52,54.","9ea25d49":"# **Correlational Map**","52716d00":"There are lot of women's who have achived maximum heart-rate than men.","0b8d0512":"As we can see:\n* Ada boost has got 90% accuracy with only 6 misclassified classes.\n* It' has a precision of 0.86 for classes 0 and 0.94 for classes 1, which is better than all other algorithms.","7614f3df":"* As we can see here that, cholestoral values of range 200 - 270 falls majorly under womens category. \n* It's interesting that only the men's are showing values above 350.","fb67ade8":"# **Model Building**","93bc4053":"# **THANK YOOOOUU!!!!!!!!!**","2f6533f2":"Let's use Adaboost Model","f221bf28":"Surprisingly there are 75% of the women are prone to heart-attack","b626b91b":"* The less maximum heart-rate achieved the less chances of heart-attack.\n* The slp value of 1 shows feable chances of heart-attack."}}