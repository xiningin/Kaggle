{"cell_type":{"25c73edb":"code","314099f2":"code","1fac4e38":"code","9ff81a88":"code","ec5a62b8":"code","5afcc223":"code","7e70f397":"code","514ebac5":"code","8776bdcd":"code","e37f5424":"code","54090821":"code","73287acc":"code","1fdc453d":"code","6ec3fa71":"code","17c817bc":"code","d8470666":"code","c601fddf":"code","6a49aa71":"code","e64cab5f":"code","17cb176f":"code","0f5966d8":"code","584e739e":"code","b7785b3d":"code","9f9c4964":"code","2b812efd":"code","d6483ccd":"code","7c197c06":"code","3f08b439":"code","73f19d0f":"code","be9d370a":"code","9a702192":"code","cec3aade":"code","4774db2a":"code","676b6843":"code","efbc8f40":"code","1b1a0b8b":"code","ae8c8dfc":"code","e61826cd":"code","c7b2fbf1":"markdown","0e48803a":"markdown","ad09c086":"markdown","9dc87d36":"markdown","b91bff2f":"markdown"},"source":{"25c73edb":"import pandas as pd\nfrom sklearn.cluster import KMeans","314099f2":"dataset_train1 = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv', index_col='id')\ndataset_test1 = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv', index_col='id')\ny = dataset_train1.target\ny = pd.DataFrame(y)\ndataset_train = dataset_train1.drop(['target'], axis=1)","1fac4e38":"#dataset_test1['num_missing_std'] = dataset_test1.isna().std(axis=1).astype('float')\n#dataset_test1['mean'] = dataset_test1.mean(axis=1)\n#dataset_test1['median'] = dataset_test1.median(axis=1)\n#dataset_test1['std'] = dataset_test1.std(axis=1)\n#dataset_test1['mad'] = dataset_test1.mad(axis=1)\n#dataset_test1['var'] = dataset_test1.var(axis=1)\n#dataset_test1['skew'] = dataset_test1.skew(axis=1) \n#dataset_test1['max'] = dataset_test1.abs().max(axis=1)\n#dataset_test1['min'] = dataset_test1.abs().min(axis=1) ","9ff81a88":"for col in dataset_train.columns:\n    dataset_train[col] = dataset_train[col].astype('float32')\n    dataset_test1[col] = dataset_test1[col].astype('float32')\ndataset_train.dtypes","ec5a62b8":"model_kmeans = KMeans(n_clusters=5, random_state=59)\nmodel_kmeans.fit(dataset_train);\n\nclusters_train = model_kmeans.predict(dataset_train)\nclusters_test  = model_kmeans.predict(dataset_test1)\n\ndataset_train['fe_cluster'] = clusters_train\ndataset_test1['fe_cluster']  = clusters_test","5afcc223":"dataset_train = pd.get_dummies(dataset_train, columns=['fe_cluster'])\ndataset_test1 = pd.get_dummies(dataset_test1, columns=['fe_cluster'])","7e70f397":"dataset_train['fe_mean']        = dataset_train.mean(axis=1)\ndataset_test1['fe_mean']         = dataset_test1.mean(axis=1)\ndataset_train['fe_median']      = dataset_train.median(axis=1)\ndataset_test1['fe_median']       = dataset_test1.median(axis=1)\ndataset_train['fe_min']         = dataset_train.min(axis=1)\ndataset_test1['fe_min']          = dataset_test1.min(axis=1)\ndataset_train['fe_max']         = dataset_train.max(axis=1)\ndataset_test1['fe_max']          = dataset_test1.max(axis=1)\ndataset_train['fe_skew']        = dataset_train.skew(axis=1)\ndataset_test1['fe_skew']         = dataset_test1.skew(axis=1)\ndataset_train['sum1']        = dataset_train.sum(axis=1)\ndataset_test1['sum1']         = dataset_test1.sum(axis=1)","514ebac5":"dataset_train['f0_f1'] = dataset_train['f0'] + dataset_train['f1']\ndataset_test1['f0_f1'] = dataset_test1['f0'] + dataset_test1['f1']\ndataset_train['f1_f2'] = dataset_train['f2'] + dataset_train['f1']\ndataset_test1['f1_f2'] = dataset_test1['f2'] + dataset_test1['f1']","8776bdcd":"for col in dataset_train.columns:\n    dataset_train[f'{col}^2'] = dataset_train[col]**2\n    dataset_test1[f'{col}^2'] = dataset_test1[col]**2","e37f5424":"dataset_train","54090821":"import tensorflow as tf","73287acc":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","1fdc453d":"from sklearn.preprocessing import QuantileTransformer, KBinsDiscretizer, PowerTransformer","6ec3fa71":"#from sklearn.impute import SimpleImputer\n#imputer = SimpleImputer(strategy='median')\n#dataset_train = imputer.fit_transform(dataset_train)\n#dataset_test = imputer.transform(dataset_test1)","17c817bc":"#pt = PowerTransformer()\n#dataset_train = pt.fit_transform(dataset_train)\n#dataset_test = pt.transform(dataset_test)","d8470666":"#qt = QuantileTransformer(n_quantiles=300, output_distribution='uniform')\n#dataset_train = qt.fit_transform(dataset_train)\n#dataset_test = qt.transform(dataset_test)","c601fddf":"from sklearn.preprocessing import MinMaxScaler\nx_scaler = MinMaxScaler()\ndataset_train_sc = x_scaler.fit_transform(dataset_train)\ndataset_test_sc = x_scaler.transform(dataset_test1)","6a49aa71":"from tensorflow import keras\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Embedding\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Conv1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import load_model\n#from tensorflow.keras.engine.input_layer import Input\nfrom tensorflow.keras.layers import MaxPooling1D, GaussianNoise\nfrom tensorflow.keras.layers import GlobalAveragePooling1D\nfrom tensorflow.keras.layers import BatchNormalization","e64cab5f":"from tensorflow.keras.constraints import max_norm","17cb176f":"shape = len(dataset_train_sc[0])","0f5966d8":"def model_builder(lr):\n    \"\"\"\u041c\u043e\u0434\u0435\u043b\u044c \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438 \u0434\u043b\u044f \u0430\u0433\u0435\u043d\u0442\u0430\"\"\"\n    inputA = keras.Input(shape=(shape))\n    #line = GaussianNoise(0.00001)(inputA)\n    line = Reshape((shape,1))(inputA)\n    \n    line = Conv1D(filters=32, kernel_size=2, padding='same', activation='swish')(line)\n    line = BatchNormalization()(line)\n    line = Dropout(0.4)(line)\n    \n    line = Conv1D(filters=64, kernel_size=2, activation='swish')(line)\n    line = BatchNormalization()(line)\n    line = Dropout(0.4)(line)\n    \n    line = Conv1D(filters=128, kernel_size=2, activation='swish')(line)\n    line = BatchNormalization()(line)\n    line = Dropout(0.4)(line)\n    \n    #line = MaxPooling1D(pool_size=2)(line)\n    \n    line = Flatten()(line)\n    #line = Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l1(0.000001))(line)\n    #line = Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l1(0.000001))(line)\n    #line = Dropout(0.1)(line)\n    #line = Dense(128, activation='relu')(line)\n    #line = Dense(32, activation='relu')(line)\n    #line = Dense(32, activation='relu',kernel_constraint=max_norm(2.),)(line)\n    #line = Dense(256, activation='swish')(inputA)\n    #line = Dropout(0.5)(line)\n    #line = Dense(128, activation='swish')(line)\n    #line = Dropout(0.5)(line)\n    line = Dense(256, activation='swish')(line)\n    line = Dropout(0.4)(line)\n    line = Dense(128, activation='swish')(line)\n    line = Dropout(0.4)(line)\n    line = Dense(64, activation='swish')(line)\n    line = Dropout(0.4)(line)\n    line = Dense(32, activation='swish')(line)\n    \n    #line = Dense(16, activation='relu')(line)\n    \n    outputA = Dense(units=1, activation=\"sigmoid\", kernel_regularizer=keras.regularizers.l1(0.01))(line)\n    model = Model(inputs=inputA, outputs=outputA)\n    #model = keras.models.load_model('models\/model2')\n    model.compile(loss = 'binary_crossentropy', optimizer = Adam(lr=lr), metrics=[tf.keras.metrics.AUC(name='auc'), 'binary_accuracy'],)\n    return model","584e739e":"lr=0.0001\nwith strategy.scope():\n    model = model_builder(lr)","b7785b3d":"model.summary()","9f9c4964":"checkpoint_filepath = 'best.h5'\nsave_model_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=False,\n    monitor='val_auc',\n    mode='max',\n    verbose=1,\n    save_best_only=True)","2b812efd":"with strategy.scope():\n    model = load_model('best.h5')","d6483ccd":"#model = load_model('..\/input\/tps0921-pre-nn\/best_81204.h5')","7c197c06":"preds = model.predict(dataset_test_sc)","3f08b439":"#for i in range(len(preds)):\n#    preds[i] = 1 if preds[i] >= 0.5 else 0","73f19d0f":"len(dataset_train_sc)","be9d370a":"N_split = int(0.15 * len(dataset_train_sc))\ndataset_sc_TRAIN = dataset_train_sc[:-N_split, :]\ndataset_sc_VAL = dataset_train_sc[-N_split:, :]\ny_TRAIN = y[:-N_split]\ny_VAL = y[-N_split:]","9a702192":"import numpy as np\nN = 200000\nbig_dataset_X = np.concatenate([dataset_sc_TRAIN, dataset_test_sc[:N,:]], axis=0)\nlen(big_dataset_X)","cec3aade":"big_dataset_y = np.concatenate([y_TRAIN, preds[:N,:]], axis=0)\nlen(big_dataset_y)","4774db2a":"#N = len(preds)\/(len(preds)+len(y_TRAIN))\n#N","676b6843":"from tensorflow.keras.callbacks import ReduceLROnPlateau\nreduce_lr = ReduceLROnPlateau(monitor='val_auc', factor=0.2, patience=10, min_lr=0.001, verbose=1, mode='max')","efbc8f40":"EPOCHS = 50\n#EPOCHS = 1\nmodel.fit(\n    big_dataset_X, big_dataset_y,\n    validation_data=(dataset_sc_VAL, y_VAL), epochs=EPOCHS, callbacks=[save_model_callback, reduce_lr], batch_size=4096, shuffle=True)","1b1a0b8b":"EPOCHS = 100\n#EPOCHS = 1\nmodel.fit(\n    dataset_sc_TRAIN, y_TRAIN,\n    validation_data=(dataset_sc_VAL, y_VAL), epochs=EPOCHS, callbacks=[save_model_callback, reduce_lr], batch_size=2048, shuffle=True)","ae8c8dfc":"with strategy.scope():\n    model = load_model('best.h5')","e61826cd":"preds = model.predict(dataset_test_sc)\noutput = pd.DataFrame({'Id': dataset_test1.index,'target': preds[:,0]})\npath = 'sample_submission.csv'\noutput.to_csv(path, index=False)\noutput ","c7b2fbf1":"real training","0e48803a":"pseudo training","ad09c086":"repeated pseudo training","9dc87d36":"repeat few times 2 cells above","b91bff2f":"Below is the compiled list of the all feature engineering ideas tried out so far.<br>\n\nX['num_missing'] = X.isna().sum(axis=1) # Number of NaN columns <br>\nX['num_missing_std'] = X.isna().std(axis=1).astype('float') # Standard deviation of NaN columns count<br>\nX['mean'] = X.mean(axis=1) # Mean of row values<br>\nX['median'] = X.median(axis=1) # Median<br>\nX['std'] = X.std(axis=1) # Standard Deviation<br>\nX['mad'] = X.mad(axis=1) # Mean Absolute Deviation<br>\nX['var'] = X.var(axis=1) # Variance<br>\nX['skew'] = X.skew(axis=1) # Skewness<br>\nX['max'] = X.abs().max(axis=1) # Max of row values<br>\nX['min'] = X.abs().min(axis=1) # Min of row values<br>"}}