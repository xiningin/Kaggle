{"cell_type":{"b97648c8":"code","76f8cf1c":"code","69fe3f46":"code","c79576bd":"code","50aa0539":"code","759c6d09":"code","a1901e11":"code","8d44d643":"code","ae87e45a":"code","c0212afd":"code","7fcd5957":"code","9dff063d":"code","04eebb83":"code","4d9dfa09":"code","c1df62f1":"code","9345c836":"code","0530c7a5":"markdown","4c1efba2":"markdown","a2a002c6":"markdown"},"source":{"b97648c8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","76f8cf1c":"!pip install pyspark","69fe3f46":"from pyspark import SparkContext\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.sql import SparkSession ,Row\nfrom pyspark.sql.functions import col\nfrom pyspark.sql import SQLContext\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.sql.types import StructType,StructField,IntegerType\nimport matplotlib.pyplot as plt\n%matplotlib inline","c79576bd":"appName=\"Collaborative Filtering with PySpark\"\n\n#initialize the spark session\nspark = SparkSession.builder.appName(appName).getOrCreate()\n\n#get sparkcontext from the sparksession\nsc = spark.sparkContext\nsqlContext = SQLContext(sc)","50aa0539":"df_review = sqlContext.read.json('\/kaggle\/input\/yelp-dataset\/yelp_academic_dataset_review.json')","759c6d09":"df_review.show()","a1901e11":" df_business = sqlContext.read.json('..\/input\/yelp-dataset\/yelp_academic_dataset_business.json')","8d44d643":"df_business.show()","ae87e45a":"df_business = df_business.select(\"business_id\",\"name\", \"stars\", \n                                 \"review_count\", \"attributes\", \n                                 \"categories\", \"city\").withColumnRenamed(\"stars\", \"stars_restaurant\")","c0212afd":"df_business.show()","7fcd5957":"df_business = df_business.filter((df_business['city'] == 'Pine Castle') & (df_business.categories.contains('Restaurants'))).drop('city')","9dff063d":"df_business.show()","04eebb83":"indexer = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in ['business_id', 'user_id']]\npipeline = Pipeline(stages=indexer)\ntransformed = pipeline.fit(df_review).transform(df_review)\ntransformed.select(['business_id', 'user_id','business_id_index', 'user_id_index'])","4d9dfa09":"(training, test) = transformed.randomSplit([0.8, 0.2])\n","c1df62f1":"als=ALS(maxIter=5,\n        regParam=0.09,\n        rank=25,\n        userCol=\"user_id_index\",\n        itemCol=\"business_id_index\",\n        ratingCol=\"stars\",\n        coldStartStrategy=\"drop\",\n        nonnegative=True)\n\nmodel=als.fit(training)","9345c836":"evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"stars\",predictionCol=\"prediction\")\npredictions=model.transform(test)\nrmse=evaluator.evaluate(predictions)\nprint(\"RMSE=\"+str(rmse))","0530c7a5":"For this first version of the model, we restrict our dataset to businesses that are restaurants and in the city of Atlanta. Therefore we apply the function filter in the columns city and category to enforce this restriction. ","4c1efba2":"From the table df_review, we want only the reviews from the selected table above. We use an inner join of df_review and df_business to eliminate all other businesses from the table reviews.","a2a002c6":"Our dataset contains many columns, and most of them won\u2019t be used in this project. From the table df_business, we want only the essential features of businesses such as the id, name, stars, category and so on."}}