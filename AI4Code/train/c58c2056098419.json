{"cell_type":{"49c6314e":"code","de99c23b":"code","f048806c":"code","b7be8f93":"code","28a3ff41":"code","1f59bf29":"code","5932c20b":"code","36af234f":"code","6bf6ae97":"code","11f6b0de":"code","89bed9fe":"code","7589d1a6":"code","a2a44597":"code","32ea2f71":"markdown","7e95598c":"markdown","05a6a642":"markdown","87711366":"markdown","92ed0e0a":"markdown","f1ed5643":"markdown"},"source":{"49c6314e":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import model_selection","de99c23b":"class Options :\n  FastTestRead = False #reads 100 rows from file if set to true\n  DrawEpochLoss = True #draws a graph of loss and accuracy of each epoch\n  PlotAgeDiffDistribution = True\n  RunCrossValidation = True\n  \n  FeatureFilter = []\n  PrintAllChartPatterns = False\n\n  TransformToIntegerScores = True\n  AllowedOrb = 7\n  MaximumScore = 10\n  ScoreValidation = False\n\n  TrainDataSize = 0.6\n  TestDataSize = 0.2\n  ValidationDataSize = (1 - TrainDataSize - TestDataSize)\n\n  CrossValidationFoldCount = 5\n  CrossValidationRandomState = 20180818\n  CrossValidationShowAllFoldGraphs = True\n  \n  KerasDimensions = 405\n  KerasEpochs = 100 if not FastTestRead else 10\n  KerasBatchSize = 4500\n  KerasModelCheckpointSave = True\n  KerasEarlyStopping = False\n  KerasReduceLearningRate = False\n  KerasVerbose = 0\n  KerasCheckPointForNEpoch = 30 #Save weights for every N epochs\n\n  SkipColumns = 6 #skip first n columns from the file\n  \n  PrintChartPatternsForRow = -1 #-1 set row no \n\n  ReadNumberOfRowsFromFile = 100 if FastTestRead else None\n\n  GlobalData = []","f048806c":"class StringUtil :\n    @staticmethod\n    def isBlank(sting):\n      if isinstance(sting, str) and sting and sting.strip():\n          #sting is not None AND sting is not empty or blank\n          return False\n      #sting is None OR sting is empty or blank\n      return True","b7be8f93":"class AspectUtil :\n\n    @staticmethod\n    def get_binary_score(orb, feature_name) :\n      if StringUtil.isBlank(orb) :\n        return 0\n\n      return float(orb)\n\n\n    @staticmethod\n    def get_aspect_score(orb, feature_name):\n      if StringUtil.isBlank(orb) :\n        return 0\n\n      forb = float(orb)\n\n      if (abs(forb) > Options.AllowedOrb) :\n        return 0\n\n      MaxMeaningfulOrb = 7\n\n      result = -(float(Options.MaximumScore) \/ float(MaxMeaningfulOrb)) * abs(float(orb)) + Options.MaximumScore\n      result = abs(result)\n      return result","28a3ff41":"class DataValidation:\n    \n    @staticmethod\n    def validation(dsConcat, X, y):\n        DataValidation.printChartPatternsForValidation(dsConcat, Options.PrintChartPatternsForRow)\n\n    @staticmethod\n    def printChartPatternsForValidation(dataset, rowNo):\n        if (rowNo <= 0):\n          return\n\n        print(dataset.iloc[rowNo,0])\n    \n        for i in range(len(dataset.columns)) :\n            if (isBlank(str(dataset.iloc[rowNo, i])) == False) :\n                print(dataset.columns[i] + \" \" + str(dataset.iloc[rowNo, i]))\n\n    @staticmethod\n    def getPatternIndex(dataset, patternName):\n        for i in range(len(dataset)):\n            columnName = dataset[i]\n            if (columnName.find(patternName) != -1) :\n              return i\n        return -1\n\n\n    @staticmethod\n    def getPatternIndexes(dataset, patternName):\n        indexes = []\n\n        for i in range(len(dataset)):\n            columnName = dataset[i]\n            if (columnName.find(patternName) != -1) :\n              indexes.append(i)\n\n        return indexes\n\n\n    @staticmethod\n    def unimportantFeatures(Xa, y, feature_names):\n\n        resultIndexes = []\n\n        for i in range(len(feature_names)):\n            pattern = feature_names[i]\n            posPatternCount, negPatternCount = DataValidation.printChartPatternCounts(Xa, y, feature_names, pattern)\n            diffRatio = 0\n\n            if (negPatternCount != 0):\n              diffRatio = ((posPatternCount - negPatternCount) * 100) \/ (negPatternCount)\n              diffRatio = abs(diffRatio)\n\n            if (diffRatio < 1) :\n              resultIndexes.append(i)\n\n        return resultIndexes\n\n    @staticmethod\n    def processAllChartPatternCountDifferences(Xa, y, feature_names, toPrint):\n\n        resultDict = {}\n        resultDetailed = {}\n\n        for i in range(len(feature_names)):\n            pattern = feature_names[i]\n            posPatternCount, negPatternCount = DataValidation.printChartPatternCounts(Xa, y, feature_names, pattern)\n            diffRatio = 0\n\n            if (negPatternCount != 0):\n              diffRatio = ((posPatternCount - negPatternCount) * 100) \/ (negPatternCount)\n              diffRatio = abs(diffRatio)\n\n            resultDetailed[pattern] = { 'diffRatio' : diffRatio, 'posPatternCount' : posPatternCount, 'negPatternCount' : negPatternCount }\n            resultDict[pattern] = diffRatio\n\n        sorted_dicdata = sorted(resultDict.items(), key = operator.itemgetter(1), reverse = True)\n\n        if (toPrint) : \n          for item in sorted_dicdata:\n            print(item[0], item[1])\n\n          for key, value in resultDetailed.items():\n            print(key, \"diffRatio\", value.get('diffRatio'), \"posPatternCount: \", value.get('posPatternCount'), \"negPatternCount: \", value.get('negPatternCount'))\n\n        return sorted_dicdata\n\n    @staticmethod\n    def printChartPatternCounts(Xa, y, feature_names, patternName):\n        posCount = 0 #positive group pattern count\n        negCount = 0 #negative group pattern count\n        i = DataValidation.getPatternIndex(feature_names, patternName)\n        rowCount = len(Xa)\n\n        if (isinstance(Xa, (list, tuple, np.ndarray)) == False) :\n          for j in range(rowCount):\n              if (Xa.iloc[j, i] == 1):\n                  colValue = y[j]\n            \n                  if (colValue == 1):\n                    posCount += 1\n                  elif (colValue == 0):\n                    negCount += 1\n        else :\n          for j in range(rowCount):\n              if (isBlank(Xa[j][i]) == False):\n                  colValue = y[j]\n            \n                  if (colValue == 1):\n                    posCount += 1\n                  elif (colValue == 0):\n                    negCount += 1\n\n        return posCount, negCount\n\n\n    @staticmethod\n    def plot_age_diff_distribution(dsConcat) :\n      if (Options.PlotAgeDiffDistribution == False) :\n        return\n\n      age_diffs = dsConcat[\"BMinusAAgeDifferenceYears\"].values\n      age_len = len(age_diffs)\n      half = int(age_len \/ 2)\n      \n      # Plot the histograms\n      fig, ax = plt.subplots()\n      ax.hist(age_diffs[0:half], bins = 100, alpha=0.5, label='Real couples')\n      ax.hist(age_diffs[half:age_len], bins = 100, alpha=0.3, label='Fake couples')\n      ax.set_xlabel('Age difference (MaleAge-FemaleAge)')\n      ax.set_ylabel('Chart count')\n      plt.legend(loc='best')\n      plt.show()\n      return\n\n\n\n    @staticmethod\n    def drawOrb(Xa, y):\n        return\n        toDrawPositive = []\n        toDrawNegative = []\n        patternName = 'AB-VenCnjMar'\n\n        i = DataValidation.getPatternIndex(feature_names, patternName)\n        rowCount = len(Xa)\n\n        for j in range(rowCount):\n            if (isBlank(Xa[j][i]) == False):\n                colValue = y[j]\n            \n                if (colValue == 1):\n                  toDrawPositive.append(float(Xa[j][i]))\n                elif (colValue == 0):\n                  toDrawNegative.append(float(Xa[j][i]))\n\n        plt.hist(toDrawNegative, bins= 10)  \n        plt.title(\"Histogram\")\n        plt.axis([-10, 10, 0, 40])\n        plt.show()\n\n\n\n    @staticmethod\n    def print_score(is_real, pattern_name, data_txt, curr_score, aspect_score, coefficient):\n      if (Options.ScoreValidation == False or curr_score == 0) :\n        return\n\n      print ( (\"Real:%s  Pattern:%s  CurrScore:%f  Txt:%s  AspectScore:%f  Coeff:%f\") % (is_real, pattern_name, curr_score, data_txt, aspect_score, coefficient) )\n\n\n    @staticmethod\n    def print_data_summary(msg, X_train, y_train, X_validate, y_validate, X_test, y_test) :\n      print(msg)\n      print((\"Train rows: %i      Validate rows: %i      Test rows: %i\") % (len(X_train), len(X_validate), len(X_test)) )\n      print((\"Train columns: %i   Validate columns: %i   Test columns: %i\") % (len(X_train[0]), len(X_validate[0]), len(X_test[0])) )\n","1f59bf29":"import time\nimport pandas as pd\n\nclass DataReader :\n    \n    @staticmethod\n    def readDataSet(filename, positives):\n        pds = pd.read_csv(filename, '\\t', header=0, nrows=Options.ReadNumberOfRowsFromFile)\n        rowCount = len(pds['Chart_A_Name'])\n        newColVal = 1 if positives else 0\n        pds['Result'] = pd.Series(newColVal, index=pds.index)\n        return pds, rowCount\n    \n    @staticmethod\n    def combineDataSet(filesArray):\n        dsPositives = DataReader.readDataSet(filesArray['positives'], True)\n        dsRndNegatives = DataReader.readDataSet(filesArray['negatives'], False)\n        frames = [dsPositives[0], dsRndNegatives[0]]\n        dsConcat = pd.concat(frames)\n        return dsConcat, dsPositives, dsRndNegatives\n\n\n    @staticmethod\n    def columnFiltered(colName, filterList):\n      if StringUtil.isBlank(colName):\n          return True\n    \n      for i in range(len(filterList)):\n         if colName.find(filterList[i]) != -1 :\n          return True\n    \n      return False\n\n\n    @staticmethod\n    def fixColumnNames(ds):\n      for i in range(len(ds.columns.values)):\n          ds.columns.values[i] = ds.columns.values[i].replace(\"_\", \"\")\n          ds.columns.values[i] = ds.columns.values[i].replace(\" \", \"-\")\n\n\n    @staticmethod\n    def filterColumns(ds, featureFilterList):\n      colToFilter = []\n      for i in range(len(ds.columns.values)):\n          if DataReader.columnFiltered(ds.columns.values[i], featureFilterList):\n              colToFilter.append(i)\n      return colToFilter\n\n\n    @staticmethod\n    def satisfyOrb(orb) : \n      if (Options.AllowedOrb >= abs(float(orb))) :\n        return True\n      else:\n        return False\n\n\n\n    #poor performance code to retain column names (using arrays would be faster)\n    @staticmethod\n    def xDataSetToBinaryScores(X):\n        for i in range(len(X.values)):\n            for j in range(len(X.iloc[i])):\n                if (StringUtil.isBlank(X.iat[i, j]) or isAspect(X.iat[i, j], X.values[i])):\n                    X.iat[i,j] = 0\n                else:\n                    X.iat[i,j] = 1\n\n    @staticmethod\n    def xArrayToBinaryScores(X, feature_names):\n        for j in range(len(feature_names)):\n            for i in range(len(X)):\n                if (StringUtil.isBlank(X[i, j])):\n                    X[i, j] = 0\n                elif (DataReader.satisfyOrb(X[i, j]) == False):\n                    X[i, j] = 0\n                else:\n                    X[i, j] = 1\n\n    @staticmethod\n    def xArrayToAspectScores(X, feature_names):\n        for j in range(len(feature_names)):\n            for i in range(len(X)):\n                X[i, j] =  AspectUtil.get_aspect_score(X[i, j], feature_names[j])\n                    \n\n\n    @staticmethod\n    def transform_to_scores(X_train, X_validate, X_test, feature_names, cast_as_type = 'int64'):\n      if (Options.TransformToIntegerScores) :\n        print(\"Transforming to \", cast_as_type, \" scores.\")\n        DataReader.xArrayToAspectScores(X_train, feature_names)\n        DataReader.xArrayToAspectScores(X_validate, feature_names)\n        DataReader.xArrayToAspectScores(X_test, feature_names)\n      else :\n        print(\"Transforming to binary scores.\")\n        DataReader.xArrayToBinaryScores(X_train, feature_names)\n        DataReader.xArrayToBinaryScores(X_validate, feature_names)\n        DataReader.xArrayToBinaryScores(X_test, feature_names)\n\n      X_train = X_train.astype(cast_as_type)\n      X_validate = X_validate.astype(cast_as_type)\n      X_test = X_test.astype(cast_as_type)\n\n      return X_train, X_validate, X_test\n\n\n    @staticmethod\n    def convert_to_array(dataframe, convert_features) :\n      feature_names = None\n      columnLen = dataframe.shape[1]\n\n      if (convert_features) : \n        feature_names = dataframe.columns[(Options.SkipColumns + 1):columnLen-1]\n      \n      Xa = dataframe.iloc[:, (Options.SkipColumns + 1):columnLen-1].values\n      y = dataframe.iloc[:, columnLen - 1].values\n\n      return Xa, y, feature_names\n\n\n    #contains all aux methods after dataset read and before score conversion\n    @staticmethod\n    def prepare_dataset(fileNames):\n        start_time = time.time()\n    \n        dsConcat, dsPositives, dsRndNegatives = DataReader.combineDataSet(fileNames)\n        colsToFilter = DataReader.filterColumns(dsConcat, Options.FeatureFilter)\n        dsConcat.drop(dsConcat.columns[colsToFilter], axis=1, inplace=True)\n        DataReader.fixColumnNames(dsConcat)\n        keepAsDataSet = False\n\n        columnLen = dsConcat.shape[1]\n\n        feature_names = None\n    \n        if (keepAsDataSet):\n            X = dsConcat.iloc[:, (Options.SkipColumns + 1):columnLen]\n            y = dsConcat.iloc[:, columnLen - 1]\n            ceateFeatureMap(X.columns)\n            DataReader.xDataSetToBinaryScores(X)\n        else: #convert to array for performance\n            Xa, y, feature_names = DataReader.convert_to_array(dsConcat, True)\n            \n            if (Options.PrintAllChartPatterns) : #placed here because of array processing\n              sorted_dicdata = DataValidation.processAllChartPatternCountDifferences(Xa, y, feature_names, True)\n            \n            DataValidation.drawOrb(Xa, y)\n\n            if (Options.TransformToIntegerScores) :\n              DataReader.xArrayToAspectScores(Xa, feature_names)\n            else :\n              DataReader.xArrayToBinaryScores(Xa, feature_names)\n        \n            X = pd.DataFrame(data=Xa, columns=feature_names)\n    \n    \n        elapsed_time = time.time() - start_time\n        print((\"%f seconds elapsed during dataset read.\") % elapsed_time)\n    \n        return dsConcat, dsPositives, dsRndNegatives, X, y, feature_names","5932c20b":"import numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\n\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import confusion_matrix\n\n\n\nclass ClassifierInput:\n\n  def __init__(self, no, name, lib_type, creator) :\n      self.no = no\n      self.name = name\n      self.library_type = lib_type\n      self.creator = creator\n\n\n  def allocate(self):\n      self.classifier = self.creator()\n      return self\n\n\n  def set_classification_params(self, paramz) :\n      self.classificationParams = paramz\n\n      if (self.library_type == \"sklearn\") :\n        self.classifier.set_params(**paramz)\n\n\n  def set_vectors(self, dfConcat, dfPositives, dfRndNegatives, X, y, feature_names) :\n      self.X = X\n      self.y = y\n      self.dfConcat = dfConcat #Concatenated dataframe of dsPositives and dsNegatives\n      self.dfPositives = dfPositives #Only Positive samples\n      self.dfRndNegatives = dfRndNegatives #Only Negative samples\n      self.feature_names = feature_names #Feature names\n\n\n  def shuffle(self) : \n    self.dfConcat = self.dfConcat.sample(frac=1).reset_index(drop=True)\n    self.X, self.y, self.feature_names = DataReader.convert_to_array(self.dfConcat, True)\n\n\n  def predict(self, X_test, y_test, dataset_name='') :\n    y_pred = self.classifier.predict(X_test)\n    predictions = [np.round(value) for value in y_pred]\n    accuracy = accuracy_score(y_test, predictions)\n    print(\"Accuracy On Test Data %s : %.2f%%\" % (dataset_name, accuracy * 100.0))\n    return accuracy, y_pred\n\n\n  def predict_proba(self, X_test) :\n    y_pred_prob = self.classifier.predict_proba(X_test)\n    return y_pred_prob\n\n\n\n  # An area under the ROC curve of value A, for example means that, a randomly\n  # selected case from the group\n  # with the target equals Y has a score larger than that for a randomly chosen\n  # case from the group with\n  # the target equals N in A% of the time.\n  def draw_roc_curve(self, X_test, y_test, title_prefix='') :\n    y_pred_prob = self.predict_proba(X_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_prob[:, 1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n\n    plt.figure(1)\n    plt.plot([0, 1], [0, 1], linestyle='--', lw=1, color='r', label='Luck', alpha=.8)\n    plt.plot(false_positive_rate, true_positive_rate, label=title_prefix + self.name + ' (area = {:.3f})'.format(roc_auc))\n    plt.xlabel('False positive rate (1-specificity)')\n    plt.ylabel('True positive rate (sensitivity)')\n    plt.title('ROC curve')\n    plt.legend(loc='best')\n    plt.show()\n\n\n\n  def confusion_matrix(self, y_test, y_pred) :\n    cnf_matrix = confusion_matrix(y_test, y_pred)\n    np.set_printoptions(precision=2)\n    return cnf_matrix\n\n\n  def draw_confusion_matrix_data(self, y_test, y_pred, class_names, title, normalize=False) :\n    cnf_matrix = self.confusion_matrix(y_test, y_pred)\n    self.draw_confusion_matrix(cnf_matrix, classes=class_names, normalize = normalize,\n                      title='Confusion matrix')\n\n  def draw_confusion_matrix(self, cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix')\n\n    print(cm)\n\n    plt.figure()\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n\n\n\n  def draw_cv_mean(self, cv_accuracy) :\n    x = np.arange(1,len(cv_accuracy) + 1,1)\n    y = cv_accuracy    \n    y_mean = [np.mean(y)] * len(x)\n    \n    fig,ax = plt.subplots()\n    # Plot the data\n    data_line = ax.plot(x,y, label='CV Fold Accuracy', marker='o')\n    # Plot the average line\n    mean_line = ax.plot(x,y_mean, label='Mean', linestyle='--')\n    legend = ax.legend(loc='upper right')\n    plt.show()\n\n\n  def print_cv_details(self, cv_accuracy) :\n    print(\"\\nCross validation average accuracy -> %.2f%% (STD : +\/- %.2f%%)\" % (100 * np.mean(cv_accuracy), 100 * np.std(cv_accuracy)))\n\n","36af234f":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport keras\nimport os.path\n\nfrom keras.callbacks import ModelCheckpoint\n\n\ndef baseline_model() :\n    from keras.models import Sequential\n    from keras.layers import Dense, Dropout, GaussianNoise, BatchNormalization\n    from keras.constraints import max_norm\n    from keras.optimizers import RMSprop\n    model = Sequential()\n    model.add(Dropout(0.42, input_shape=(Options.KerasDimensions,)))\n    model.add(Dense(50, kernel_initializer='normal', activation='sigmoid'))\n    model.add(Dropout(0.79))\n    rms = RMSprop(lr = 0.00050)\n    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer=rms, metrics=['accuracy'])\n    return model\n\n\n\nclass NeuralNetClassifier(ClassifierInput): #class to prevent exception when there is no keras installation\n  def allocate(self) :\n    import tensorflow as tf\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    session = tf.Session(config=config)\n    from keras.wrappers.scikit_learn import KerasClassifier\n    self.creator = KerasClassifier\n    self.classifier = self.creator(build_fn=baseline_model, epochs=Options.KerasEpochs, batch_size=Options.KerasBatchSize, verbose=Options.KerasVerbose)\n    return self\n\n\n\n  def fit(self, X_train, y_train, **kwargs) :\n\n    Options.KerasDimensions = len(X_train[0])\n        \n    callbacks = []\n    \n    saved_files = os.listdir('.')\n    for item in saved_files:\n      if item.endswith(\".hdf5\"):\n        os.remove(os.path.join('.', item))\n    \n    filepath=\"nn_weights-{epoch:02d}.hdf5\"\n\n    if (Options.KerasModelCheckpointSave) : \n      checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=Options.KerasVerbose, save_weights_only=False, save_best_only=False, mode='max', period = Options.KerasCheckPointForNEpoch)\n      callbacks.append(checkpoint)\n\n    if (Options.KerasEarlyStopping) : \n      early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=7, verbose=Options.KerasVerbose, mode='auto')\n      callbacks.append(early_stop)\n\n    if (Options.KerasReduceLearningRate) : \n      reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001, verbose = Options.KerasVerbose)\n      callbacks.append(reduce_lr)\n\n    self._fitted = self.classifier.fit(X_train, y_train, batch_size = Options.KerasBatchSize, epochs = Options.KerasEpochs, verbose = Options.KerasVerbose,\n          validation_data = kwargs['validation_data'], callbacks=callbacks)\n\n    print(\"Fit done.\")\n    return self._fitted\n\n   \n  def print_summary(self) :\n    return\n\n\n  def draw_epoch_loss(self, X_test, y_test):\n    if Options.DrawEpochLoss == False :\n      return\n\n    #from neuralnetclassifier import baseline_model\n    temp_test_model = baseline_model()\n    test_over_time = []\n    test_result = []\n\n    for i in range(len(self._fitted.history['acc'])):\n      saved_file = \"nn_weights-%02d.hdf5\" % ((int(i)+Options.KerasCheckPointForNEpoch))\n      if i % Options.KerasCheckPointForNEpoch == 0 and os.path.isfile(saved_file) : \n          temp_test_model.load_weights(saved_file)\n          test_result = temp_test_model.evaluate(X_test, y_test, verbose=Options.KerasVerbose)\n          # 0 is loss; 1 is accuracy\n          test_over_time.append(test_result)\n      else :\n          test_over_time.append(test_result)\n\n    fig, (ax1, ax2) = plt.subplots(2, 1)\n    ax1.plot(range(len(self._fitted.history['loss'])), self._fitted.history['loss'],linestyle='-', color='blue',label='Training', lw=2)\n    ax1.plot(range(len(np.array(test_over_time)[:,0])), np.array(test_over_time)[:,0], linestyle='-', color='green',label='Test', lw=2)\n    ax2.plot(range(len(self._fitted.history['acc'])), self._fitted.history['acc'],linestyle='-', color='blue',label='Training', lw=2)\n    ax2.plot(range(len(np.array(test_over_time)[:,1])), np.array(test_over_time)[:,1], linestyle='-', color='green',label='Test', lw=2)\n    leg = ax1.legend(bbox_to_anchor=(0.7, 0.9), loc=2, borderaxespad=0.,fontsize=13)\n    ax1.set_xticklabels('')\n    #ax1.set_yscale('log')\n    ax2.set_xlabel('# Epochs',fontsize=14)\n    ax1.set_ylabel('Loss',fontsize=14)\n    ax2.set_ylabel('Accuracy',fontsize=14)\n    plt.show()\n\n  ","6bf6ae97":"\nfrom sklearn.feature_selection import RFE\n#from sklearn.linear_model import RandomizedLasso\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\n\nclass Classification :\n\n    ClassifiersToRun = [ 11 ]\n\n    Classifiers = [\n                   ClassifierInput(1, \"LR\", \"sklearn\", LogisticRegression),\n                   ClassifierInput(2, 'RF', \"sklearn\", RandomForestClassifier),\n                   None,\n                   ClassifierInput(4, 'SVC', \"sklearn\", SVC),\n                   ClassifierInput(5, 'NB', \"sklearn\", GaussianNB),\n                   ClassifierInput(6, 'LDA', \"sklearn\", LinearDiscriminantAnalysis),\n                   ClassifierInput(7, 'CART', \"sklearn\", DecisionTreeClassifier),\n                   ClassifierInput(8, 'KNN', \"sklearn\", KNeighborsClassifier),\n                   ClassifierInput(9, 'ExtraTrees', \"sklearn\", ExtraTreesClassifier),\n                   None,\n                   NeuralNetClassifier(11, 'NN', 'keras', None)\n                  ]\n\n    ClassifierParameters = [\n                              (1, {} ),\n                              (2, {'n_estimators' : 1000, 'max_depth' : 4, 'bootstrap' : False, 'verbose' : 1, 'criterion': 'gini' }),\n                              (3, {'min_split_loss' : 5, 'n_estimators' : 1000, 'subsample' : 0.5, 'reg_lambda' : 10 }),\n                              (4, {} ),\n                              (5, {} ),\n                              (6, {} ),\n                              (7, {} ),\n                              (8, {} ),\n                              (9, {'n_estimators' : 50, 'min_samples_split' : 2, 'max_depth' : 3 } ),\n                              (10, {} ),\n                              (11, {} ),\n                           ]\n\n    @staticmethod\n    def get_classifier(no):\n      classifierInput = Classification.Classifiers[no-1].allocate()\n      paramz = Classification.ClassifierParameters[no-1][1]\n      classifierInput.set_classification_params(paramz)\n      return classifierInput\n\n","11f6b0de":"DataFiles = [{'positives': '..\/input\/gauq-couples-aspects-REAL-7deg-20000-noa2b-cdata4.csv', \n             'negatives': '..\/input\/gauq-couples-aspects-RANDOMIZED-7deg-20000-noa2b-cdata4.csv'},\n            ]\n\ndsConcat, dsPositives, dsRndNegatives, X, y, feature_names = DataReader.prepare_dataset(DataFiles[0])\n\ni = 0 #removed while loop from main code for demonstration\nclassifierModel = Classification.get_classifier(Classification.ClassifiersToRun[i])\nclassifierModel.set_vectors(dsConcat, dsPositives, dsRndNegatives, X, y, feature_names)","89bed9fe":"DataValidation.plot_age_diff_distribution(dsConcat)\n#the distributions should look identical","7589d1a6":"kf = StratifiedKFold(n_splits = Options.CrossValidationFoldCount, shuffle = False, random_state = Options.CrossValidationRandomState) \ncv_index = 0\ncv_results = []\ncv_accuracy = []\n\n\nfor train_index, test_index in kf.split(classifierModel.X, classifierModel.y):\n  X_train_df, X_test_df = classifierModel.dfConcat.iloc[train_index, :], classifierModel.dfConcat.iloc[test_index, :]\n\n  X_train, y_train, feature_names = DataReader.convert_to_array(X_train_df, True)\n  X_test, y_test, feature_names_t = DataReader.convert_to_array(X_test_df, True)\n\n  X_train, X_validate, y_train, y_validate = model_selection.train_test_split(X_train, y_train, test_size=\n                                                                              (Options.ValidationDataSize \/ (Options.ValidationDataSize + Options.TrainDataSize)))\n\n  X_train, X_validate, X_test = DataReader.transform_to_scores(X_train, X_validate, X_test, feature_names, 'float')\n\n  summary_msg = (\"\\nSummary for cross validation fold \" + str(cv_index + 1))\n  DataValidation.print_data_summary(summary_msg, X_train, y_train, X_validate, y_validate, X_test, y_test)\n\n  classifierModel.print_summary()\n  fitted = classifierModel.fit(X_train, y_train, validation_data=(X_validate, y_validate))\n  accuracy, y_pred = classifierModel.predict(X_test, y_test)\n\n  if (cv_index == 0 or Options.CrossValidationShowAllFoldGraphs) :\n    classifierModel.draw_confusion_matrix_data(y_test, y_pred, ['Random', 'Real'], 'Confusion matrix', normalize=False)\n    classifierModel.draw_confusion_matrix_data(y_test, y_pred, ['Random', 'Real'], 'Confusion matrix (normalized)', normalize=True)\n    classifierModel.draw_roc_curve(X_test, y_test)\n    classifierModel.draw_epoch_loss(X_test, y_test)\n\n  cv_accuracy.append(accuracy)\n\n  cv_index = cv_index + 1\n\n","a2a44597":"classifierModel.draw_cv_mean(cv_accuracy)\nclassifierModel.print_cv_details(cv_accuracy)","32ea2f71":"# Definition of the aspects","7e95598c":"An astrological aspect is a geometrical angle between any two celestial bodies projected unto a plane. Aspects are generally meausured by the angular distance in degrees and minutes of ecliptic longitude between two points, as viewed from a location. Note that aspects are usually projected on to the ecliptic, which is different from the angle between right ascension of the two astronomical bodies. Aspects viewed from the earth are called geocentric aspects, whereas those viewed from the sun known as the heliocentric aspects.\n\nPtolemaic aspects are the five major aspects [Conjunction (1), Opposition (1\/2), Trine (1\/3), Square (1\/4) and Sextile (1\/6)] as defined by Ptolemy who wrote the oldest surviving texts on astrology in the western astrological tradition around 120 AD. In addition, there are two other angles [Semisextile (1\/12) and Quincunx (5\/12)] which are called minor aspects. Johannes Kepler in the 17th century, introduced new aspects such as the semi-sextile (1\/12), semi-square (1\/8), quintile (1\/5), biquintile (2\/5) and sesquidrate (3\/8).\n\nPtolemy defined the aspects in his book Harmonica -a treatise on music, and extended his theory to the workings of the macrocosm. He imagined the musical octave covering half of the ecliptic circle. When two planets were in opposition, they form an octave interval and other aspects represent intervals accordingly.\n\n\n![astro%20aspects.png](attachment:astro%20aspects.png)","05a6a642":"# Version Summary","87711366":". #4: Dropout added to input layer.","92ed0e0a":"One of the classical assertions of astrology is that it can disclose the affinity -or lack of it- between a man and a woman. Eysenck and Nias (1982). In Western astrology there are four main concepts, namely: 1) Aspects 2) Signs 3) Houses 4) Midpoints. However, full analysis of these concepts will lead to an astronomical number of features. Thus, analysis of the 405 aspects between male and female charts were conducted.\n\nIf aspects do not contain any information, a standard binary classification task of real and randomised non-real couples will lead to a 50% accuracy rate with a small error margin, that is when the classification classes are balanced.\n\nAlso, the sparsity of the features should be noted with 92% to 96% of the columns were unfilled. \n\nVarious algorithms were unable to detect any stable patterns to build a classifier. (e.g. Logistic regression, random forest, xgboost, neural network classifiers). In this notebook, stratified cross validation average accuracy was 50.46% (STD : +\/- 0.31%) as expected.\n\nData is taken from \nhttp:\/\/cura.free.fr\/gauq\/CURA_ALL_PARTNERS_Tab_delimited.txt  \nhttp:\/\/cura.free.fr\/gauq\/1506_GAUQUELIN_MARRIED.pdf\n\n","f1ed5643":"# Draft Summary\n"}}