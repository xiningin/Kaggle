{"cell_type":{"e806dd78":"code","a9bbe6fc":"code","5c2194ec":"code","1732e1f3":"code","a6aa3974":"code","0778e926":"code","313ddae2":"code","5dcc39b2":"code","e20799c4":"code","991166f2":"code","c4c3c5a1":"code","e96a0208":"code","d9805257":"code","223e4d07":"code","a82a8528":"code","8a1ef55d":"code","f20dee78":"code","576923f3":"code","0348aa9d":"code","99763d5c":"code","fc21451e":"code","0394499d":"code","56a1a884":"code","e82a2c02":"code","78dd82f2":"code","0c6d3afd":"code","4225b8ee":"code","f4164c93":"code","5b5d2c78":"code","3bb09c4b":"code","69125c56":"code","ef1cd495":"code","f0a877ba":"markdown","ab39e8e2":"markdown","c49f45b4":"markdown","ed20958b":"markdown","2c3d6706":"markdown","d84b241c":"markdown","92266c92":"markdown","98772679":"markdown","28c24fd3":"markdown","a999d161":"markdown","e11900e5":"markdown","143235bc":"markdown","2e10bf06":"markdown","1a97c78b":"markdown","707602dd":"markdown","99f873ff":"markdown","36e3ebc1":"markdown","f65a7464":"markdown","7270651f":"markdown","4e14aec3":"markdown","06200d4d":"markdown","8e3ec708":"markdown","40981285":"markdown","9a090803":"markdown","feb786c6":"markdown","2c7e053c":"markdown","e4772f29":"markdown","07da5e2a":"markdown","575f38d3":"markdown","4852071e":"markdown","09537740":"markdown","816b24f2":"markdown","b1c62dfe":"markdown","3b9e16c4":"markdown"},"source":{"e806dd78":"# General Purpose Libraries\nimport numpy as np\nnp.random.seed(1)\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\n\n# Text Processing Libraries\nimport spacy\nimport re\nimport string\nfrom wordcloud import WordCloud\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize\n\n# Scikit Learn\nfrom sklearn.model_selection import train_test_split\n\n# TensorFlow\nimport tensorflow_hub as hub\nimport tensorflow as tf\ntf.random.set_seed(1)\n\n# Setting Pandas Display Option\npd.set_option('display.max_colwidth', 500)\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')","a9bbe6fc":"# Reading Data Files\ntrain = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\nprint(\"Train Shape :\", train.shape)\nprint(\"Test Shape :\", test.shape)","5c2194ec":"# Viewing top rows of training set\ntrain.head()","1732e1f3":"# Viewing top rows of test set\ntest.head()","a6aa3974":"train.isnull().sum()","0778e926":"# Proportion of classes\ntrain['target'].value_counts(normalize = True)","313ddae2":"# Visualize the classes\nsns.countplot('target', data = train)","5dcc39b2":"# Unique Words Count\ntrain.keyword.value_counts()","e20799c4":"# Visualize top 20 keywords\nsns.barplot(y=train['keyword'].value_counts()[:20].index,x=train['keyword'].value_counts()[:20])","991166f2":"# Visualize bottom 10 keywords\nsns.barplot(y=train['keyword'].value_counts()[-20:].index,x=train['keyword'].value_counts()[-20:])","c4c3c5a1":"# Unique Words Count\ntrain.location.value_counts()","e96a0208":"# Visualize top 20 locations\nsns.barplot(y = train['location'].value_counts()[:20].index,x = train['location'].value_counts()[:20])","d9805257":"# Visualize bottom 20 locations\nsns.barplot(y=train['location'].value_counts()[-20:].index,x=train['location'].value_counts()[-20:])","223e4d07":"train['text'].sample(5)","a82a8528":"# Real tweets indicating disaster\nreal_tweets = train[train['target']==1]['text']\nreal_tweets.values[0:5]","8a1ef55d":"# Not Real (Fake) tweets\nfake_tweets = train[train['target']==0]['text']\nfake_tweets.values[0:5]","f20dee78":"# Custom Function for Text Cleaning\n\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","576923f3":"# Applying the cleaning function to both test and training datasets\ntrain['cleaned_text'] = train['text'].apply(lambda x: clean_text(x))\ntest['cleaned_text'] = test['text'].apply(lambda x: clean_text(x))\n\n# Take a look at the cleaned text\ntrain['cleaned_text'].head()","0348aa9d":"!pip install nlppreprocess\nfrom nlppreprocess import NLP\n\nnlp = NLP()\n\ntrain['stopwords_cleaned'] = train['cleaned_text'].apply(nlp.process)\ntest['stopwords_cleaned'] = test['cleaned_text'].apply(nlp.process)  ","99763d5c":"# Import spaCy's language model\nen_model = spacy.load('en', disable=['parser', 'ner'])\n\n# function to lemmatize text\ndef lemmatization(texts):\n    output = []\n    for i in texts:\n        s = [token.lemma_ for token in en_model(i)]\n        output.append(' '.join(s))\n    return output","fc21451e":"# Applying the lemmatization function to both test and training datasets\ntrain['lemmatized_text'] = lemmatization(train['stopwords_cleaned'])\ntest['lemmatized_text'] = lemmatization(test['stopwords_cleaned'])","0394499d":"# Let's take a look at the cleaned & lemmatized text\ntrain.head(5)","56a1a884":"# Wordcloud for train set\nplt.figure(figsize = (12,9))\nwordcloud = WordCloud(min_font_size = 6,  max_words = 200 , width = 1000 , height = 600).generate(\" \".join(train['stopwords_cleaned']))\nplt.imshow(wordcloud,interpolation = 'bilinear')","e82a2c02":"# Wordcloud for test set\nplt.figure(figsize = (12,9))\nwordcloud = WordCloud(min_font_size = 6,  max_words = 200 , width = 1000 , height = 600).generate(\" \".join(test['stopwords_cleaned']))\nplt.imshow(wordcloud,interpolation = 'bilinear')","78dd82f2":"# Splitting training & testing set\nx_train, x_test, y_train, y_test = train_test_split(train['stopwords_cleaned'], train['target'],  \n                                                    test_size = 0.2, random_state = 1)","0c6d3afd":"# Build a Keras Sequential Model using Pre-trained Universal Sentence Encoder as the Input Layer\nhub_layer = hub.KerasLayer('https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4', \n                        input_shape = [],\n                        output_shape = [512],\n                        dtype = tf.string, \n                        trainable = True)\n\nmodel = tf.keras.models.Sequential()\nmodel.add(hub_layer)\nmodel.add(tf.keras.layers.Dense(128, activation = 'relu'))\nmodel.add(tf.keras.layers.Dense(32, activation = 'relu'))\nmodel.add(tf.keras.layers.Dense(1, activation = 'sigmoid'))\nmodel.summary()","4225b8ee":"# Complile the Model\nmodel.compile(optimizer = 'adam', \n              loss = 'binary_crossentropy', \n              metrics = ['accuracy'])","f4164c93":"# Train the Model \nmodel.fit(x_train, \n          y_train, \n          epochs = 1,\n          validation_data = (x_test, y_test))","5b5d2c78":"# Predict on Test Set\npred = model.predict_classes(test['stopwords_cleaned'])","3bb09c4b":"# Load Submission File and \nsubmission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\nsubmission['target'] = pred\nsubmission.to_csv('submission.csv', index=False)","69125c56":"submission.head()","ef1cd495":"submission.to_csv(\"submission.csv\", index = False)","f0a877ba":"Let's use this model to predict on our test set now and see how it performs.","ab39e8e2":"### Missing Values","c49f45b4":"## Loading Data\nLet's begin by loading the dataset in our environment and then take a peek at the dataset.","ed20958b":"## Wordcloud\nLet's visualize wordcloud for train and test set to see the distribution of words!","2c3d6706":"#### *text*\nThis is our main variable which contains actual tweets from different users. Let's take a look at some random tweets to get a sense of how the data is and what kind of pre-processing is required.","d84b241c":"### How Does TensorFlow Hub Help Us?\n**TensorFlow Hub** is a repository of pre-trained machine learning models. These models are categorised in three broad problem domains -\n+ Image\n+ Text\n+ Video\n\nThere are several models, which you can quickly start using without much hassle. Visit [TensorFlow Hub](https:\/\/tfhub.dev\/) for more details.","92266c92":"### What is Transfer Learning?\n**Transfer Learning** is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem (Source: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Transfer_learning)). \n\nTransfer Learning overcomes the problem of isolated learning by applying the knowledge gained during learning one task to another different but similar task. For example, knowledge gained while learning to recognize cars could be applied when trying to recognize trucks! The intution actually came from the humans. We humans always practice *Transfer Learning*. A person who knows how to drive a bike, finds it easy to learn how to drive a car. Similarly *Transfer Learning* helps in learning difficult tasks with less effort.","98772679":"#### *location*","28c24fd3":"### Lemmatization\n*Lemmatization* is a technique where a word is reduced to its base or dictionary form. Like - *Go*, *Going* & *Gone* will all be replaced by *Go*. Let's create a custom function to lemmatize our tweets. ","a999d161":"### The Dependent Variable - *target*","e11900e5":"I will be using [Universal Sentence Encoder](https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4) for modeling. It encodes text into high-dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks.\n\nThe model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. There are two variations of this model on *TensorFlow Hub* -\n+ Deep Averaging Network (DAN) Encoder.\n+ Transformer Encoder.\n\nI will be using the one with DAN architecture, since it is computationally less intensive. We will download this model as a *KearsLayer* and use it as input layer for our Keras Sequential Model.","143235bc":"# **Getting Started with Transfer Learning Using Tensorflow Hub**","2e10bf06":"There are *221* unique words present in training set. Some text cleaning is required here to extract some information from this variable.","1a97c78b":"Welcome to the introductory notebook on using **TensorFlow Hub** for building transfer learning models. This notebook is focused on quickly get you started with building sophisticated models with very little knowledge of modeling with **TensorFlow**. Let's get started!","707602dd":"There are many repetitions of location in the dataset with multiple names. For example - USA, United States & New York are separate entries. Also there are some numbers in this variable. We need to address these issues before making use to this variable in our model.","99f873ff":"## Prepare for Submission","36e3ebc1":"As one can expect from any textual data, there are all sorts of messyness going on here. There are numbers, special characters, links, punctuation marks etc. present in the tweets. We need to clean these before we proceed with modeling, in order to get good results.\n\nNow, let's see how a typical \"Real\" & \"Not Real\" tweet looks like.","f65a7464":"I have added three new variables to our training and testing set so that we can test which strategy is producing best results. ","7270651f":"### The Problem\nFor the demonstration of the transfer learning technique, I will be using the case of [](http:\/\/)[this](https:\/\/www.kaggle.com\/c\/nlp-getting-started) competition. The goal here is to build a machine learning model which is able to predict whether a tweet belongs to a real disaster or not!","4e14aec3":"## Modeling Using Pretrained Model","06200d4d":"### Text Cleaning\nLets begin our text preprocessing by creating a custom function to remove numbers, links, punctuations etc. The following function has been taken from [Parul Pandey](https:\/\/www.kaggle.com\/parulpandey\/getting-started-with-nlp-a-general-intro)'s notebook. ","8e3ec708":"## Basic EDA\nLet's do some basic EDA to better understand the dataset provided.","40981285":"Hi Kagglers! I have been a *R* lover for long now (and I still am!), but recently I have started developing interest for **Python**. Here is my first public **Python** notebook. **Please do read it and hit upvote, if you like it**. If you have any ideas to improve the analysis, please post it in the comments.","9a090803":"## Text Preprocessing","feb786c6":"**Thanks for taking out time to go through my notebook! Go ahead and experiment with other *TensorFlow Hub* models and try to improve the results.**","2c7e053c":"Some additional efforts required in cleaning the tweets to achieve better accuracy. Nonetheless, let's move to medeling and see how good our model is.","e4772f29":"## Prediction","07da5e2a":"+ There are *7613* rows and *5* columns in the training set.\n+ There are *3263* rows and *4* columns in the test set.\n+ *target* is the dependent variable, with values *0* and *1* indicating a fake and real disaster tweet respectively.\n+ *id* is just an unique identifier of the tweet. Training and Test set are subsets of a sample of tweets collected.\n+ There are many missing values in *location* variable.","575f38d3":"*location* has many missing values. Are those missing values indicate non-legitimate tweets? There might be some information here. We will look at it at a later stage.","4852071e":"### Creating Training & Testing Set\nI will be using **stopwords_cleaned** variable for training our classification model, as it produced better results than the other text variables.\n","09537740":"### Independent Variables\n#### *keyword*","816b24f2":"### Removing Stopwords\nSimply removing all stopwords from the text might lead to loss of semantic meaning of the sentences. Since, we intend to use models which capture the semantic meaning its important that we take care of this problem. We will be using *nlppreprocess* library from PyPI to remove only unwanted stopwords. To know more about why removing stopwords blindly might not be a good idea here, read this nice article [here](https:\/\/towardsdatascience.com\/why-you-should-avoid-removing-stopwords-aa7a353d2a52). ","b1c62dfe":"### Dataset\nDataset given here is tweets from different users split into training and test set. **target** is the dependent varible, which we are interested in predicting for the test set. Let's begin with our analysis.","3b9e16c4":"### Importing Libraries"}}