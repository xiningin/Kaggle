{"cell_type":{"f414f8e5":"code","608c95e0":"code","1d235e79":"code","fde12fd1":"code","2d762973":"code","b36f6614":"code","53a395bd":"code","c9a8bb01":"code","e6341079":"code","b1db5d6c":"code","d6e97f8b":"code","843b9fc9":"code","07d66689":"code","93a59d5f":"code","f3b4631c":"code","5109517c":"code","1ce28108":"code","323bdb95":"code","8dd34ca3":"code","3e8f000e":"markdown","b3ef8c39":"markdown","73529dac":"markdown","a0b14fe2":"markdown","5da8b239":"markdown","b8fdb5b3":"markdown"},"source":{"f414f8e5":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\ndata = pd.read_csv(\"..\/input\/winequality-red.csv\")\n","608c95e0":"data.describe()","1d235e79":"# Checking all Attributes against the QUALITY attribute.","fde12fd1":"sns.boxplot('quality', 'alcohol', data = data)","2d762973":"## Observations from Bivariate analysis using BOXPLOT\n\n1. fixed acidity has outliers at 5 and 6\n2. volatile acidity has outliers at 5 and 6 and 7\n3. citric acid looks OK\n4. residual sugar has outliers at 5 and 6 and 7\n5. chlorides has outliers at 4 and 5 and 6\n6. free sulfur dioxide has outliers at 5 and 6\n7. total sulfur dioxide has outliers at 6 and 7\n8. density has outliers at 5 and 6\n9. pH has outliers at 6 and 7\n10. sulphates has outliers at 5 and 6\n11. alcohol has outliers at 5 \n\nOverall the Quality 5, 6 and 7 are mostly having Outliers","b36f6614":"#count of the target variable\nsns.countplot(x='quality', data=data)","53a395bd":"Observations:\n    We can see that Quality 5, 6 and 7 are Contributing more..\n    and Outliers are in these 3 Categories..","c9a8bb01":"corr = data.corr()\nsns.heatmap(data=corr,annot=True,cmap='RdYlGn',linewidths=0.2)\nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.show()","e6341079":"density and fixed acidity are correlated @ 0.67\ncitric acid and fixed acidity are correlated @ 0.67\ntotal sulfur dioxide and free sulfur dioxide are correlated @ 0.67","b1db5d6c":"# \"total sulfur dioxide\" Analysis Before outliers deletion (before LOG)\n\n#IQR = 1.5 * (np.percentile(data['total sulfur dioxide'], 75) - np.percentile(data['total sulfur dioxide'], 25))\n#print(\"IQR of ApplicantIncome is: \", IQR)\n#print(\"\\n DataDescriptions of total sulfur dioxide BEFORE LOG: \\n\",data['total sulfur dioxide'].describe())\n#LA_more_IQR = data[data['total sulfur dioxide']>IQR]\n#print(\"Records above the IQR: \", LA_more_IQR.shape)\n#print(\"There are \",\n#      (LA_more_IQR['total sulfur dioxide'].count()\/data['total sulfur dioxide'].count())*100 , \"% of total sulfur dioxide are Outliers\")","d6e97f8b":"# Removing OutLiners of total sulfur dioxide using LOG\n\n#out_log_LA = np.log(data['total sulfur dioxide'])\n#print(\"\\n DataDescriptions of total sulfur dioxide AFTER LOG: \\n\",out_log_LA.describe())\n\n#data['total sulfur dioxide'] = out_log_LA\n#data.boxplot(column = ('total sulfur dioxide'))","843b9fc9":"# 3 and 4 = LOW      #1\n# 5 and 6 = Average  #2\n# 7 and 8 = High     #3  \nReview = []\nfor i in data['quality']:\n    if i >= 3 and i<=4:\n        Review.append('1')\n    if i >= 5 and i<=6:\n        Review.append('2')\n    if i >= 7 and i<=8:\n        Review.append('3')\n\ndata['Review'] = Review","07d66689":"#next we shall create a new column called Review. This column will contain the values of 1,2, and 3. \n#1 - Bad\n#2 - Average\n#3 - Excellent\n#This will be split in the following way. \n#1,2,3 --> Bad\n#4,5,6,7 --> Average\n#8,9,10 --> Excellent\n#Create an empty list called Reviews\nReview = []\nfor i in data['quality']:\n    if i >= 1 and i <= 3:\n        Review.append('1')\n    elif i >= 4 and i <= 7:\n        Review.append('2')\n    elif i >= 8 and i <= 10:\n        Review.append('3')\ndata['Review'] = Review","93a59d5f":"print(data.columns)\nprint(data['Review'].value_counts())\n#print(data.iloc[:,1:len(data.columns)-2])\n","f3b4631c":"# Start Modeling\n# Split Training and Testing data (within training data file only)\n# Hold out method validation\n\nfrom sklearn.model_selection import train_test_split\n\n#X_train = train.loc[:,('LoanAmount','Family_Income','Loan_Amount_Term', 'Gender_Married',  'Education_SelfEmployed', 'Credit_History')]\n#Y_train = train.iloc[:,len(train.columns)-1:]  # this represnts the (output) Label LOAN_STATUS\n\nX_train = data.iloc[:,1:len(data.columns)-2] # this represents the input Features\nY_train = data.loc[:,'Review']\n\n\n# Scaling features (only feature NOT observation)\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n# Scaling - brings to range (0 to 1)\nScaleFn = MinMaxScaler()\nX_Scale = ScaleFn.fit_transform(X_train)\n# Standardise - brings to Zero mean, Unit variance\nScaleFn = StandardScaler()\nX_Strd = ScaleFn.fit_transform(X_train)","5109517c":"from sklearn.decomposition import PCA\npca = PCA()\nx_pca = pca.fit_transform(X_Strd)","1ce28108":"#plot the graph to find the principal components\nplt.figure(figsize=(10,10))\nplt.plot(np.cumsum(pca.explained_variance_ratio_), 'ro-')\nplt.grid()","323bdb95":"#AS per the graph, we can see that 8 principal components attribute for 90% of variation in the data. \n#we shall pick the first 8 components for our prediction.\npca_new = PCA(n_components=8)\nx_new = pca_new.fit_transform(X_Strd)\nprint(x_new)","8dd34ca3":"test_size = .30\nseedNo = 11\n\n\nX_train,X_test,Y_train,Y_test = train_test_split(x_new,Y_train,test_size = test_size, random_state = seedNo)\n\nprint(\"train X\", X_train.shape)\nprint(\"train Y\", Y_train.shape)\nprint(\"test X\", X_test.shape)\nprint(\"test Y\", Y_test.shape)\n\n# Choose algorithm and train\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nmymodel = []\nmymodel.append(('LogReg', LogisticRegression()))\nmymodel.append(('KNN', KNeighborsClassifier()))\nmymodel.append(('DeciTree', DecisionTreeClassifier()))\nmymodel.append(('RandForest', RandomForestClassifier()))\nmymodel.append(('SVM', SVC()))\nmymodel.append(('XGBoost', XGBClassifier()))\n\n\n\nAll_model_result = []\nAll_model_name = []\nfor algoname, algorithm in mymodel:    \n    kfoldFn = KFold(n_splits = 11, random_state = seedNo)\n    Eval_result = cross_val_score(algorithm, X_train, Y_train, cv = kfoldFn, scoring = 'accuracy')\n    \n    All_model_result.append(Eval_result)\n    All_model_name.append(algoname)\n    print(\"Modelname and Model accuracy:\", algoname, 100*Eval_result.mean(),\"%\")","3e8f000e":"## Preprocessing","b3ef8c39":"### Proceed to perform PCA","73529dac":"## Approach 2 for feature Engneering, using this getting the 98% Model Accuaracy","a0b14fe2":"## Approach 1 for feature Engneering, using this getting the 83% Model Accuaracy ","5da8b239":"## Prepare Data Model","b8fdb5b3":"### Bivariate Analysis"}}