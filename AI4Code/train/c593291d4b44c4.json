{"cell_type":{"69055f64":"code","f9658bfc":"code","2b8516d2":"code","7b3bc5fc":"code","5e5e4b9e":"code","dc682127":"code","9b99a7d8":"code","db7ca183":"code","1eb96bfc":"code","854e9d3b":"code","45a0d3dc":"code","02e9d1d9":"code","1747e73e":"code","035cdf88":"code","8aa63379":"code","a26f3934":"code","d1ab4f1c":"code","d5bdfa7c":"code","b5bce8f5":"code","adcc819d":"code","66335f9a":"code","ab6cdaa3":"code","fc37a9a7":"code","401dc7ac":"code","e005f72a":"code","c6c6156b":"code","3e499f1f":"code","6be80173":"code","584cd1da":"markdown","d72dc379":"markdown","e0f825f2":"markdown","47a2d6a1":"markdown","3f4f67c8":"markdown","ea49c911":"markdown","90df9834":"markdown","9330dcce":"markdown","afb53d61":"markdown","9bc62887":"markdown","19ea0b11":"markdown","b5625d79":"markdown","15c46290":"markdown","c0326603":"markdown","33e81934":"markdown","906e8550":"markdown","be16238c":"markdown","f210bcbe":"markdown","b150d39e":"markdown","d7f22f92":"markdown","7b4bbe8e":"markdown","1179c1ee":"markdown","1a2174c8":"markdown","25be24ec":"markdown","99fa0e53":"markdown","e4c869e2":"markdown","1f98d886":"markdown","edb6d2c2":"markdown","db2ef4c3":"markdown","04ef5a95":"markdown","f5f58ab4":"markdown"},"source":{"69055f64":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n#\nfrom xgboost.sklearn import XGBClassifier\nimport xgboost as xgb\nimport seaborn as sns\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\n#\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import linear_model\nfrom sklearn import preprocessing\n#\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.preprocessing import scale, StandardScaler\n#\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import roc_auc_score, make_scorer\nfrom sklearn.metrics import r2_score\n\nfrom sklearn.metrics import log_loss\nfrom collections import defaultdict\nimport warnings\nwarnings.filterwarnings('always') \n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f9658bfc":"df = pd.read_csv('..\/input\/ab-testing\/AdSmartABdata.csv')","2b8516d2":" p= df[~((df['yes']== 0) & (df['no']== 0))]\nnp =  df[(df['yes']== 0) & (df['no']== 0)]","7b3bc5fc":"print('Number of yes for P would be distributed among 1 and 0')\nprint(p['yes'].value_counts())\nprint('    Number of yes and no for np would just be zero')\nprint(np['yes'].value_counts())\nprint(np['no'].value_counts())","5e5e4b9e":"np =np.rename(columns={'yes':'aware', 'no': 'participate'})\np =p.rename(columns={'yes':'aware', 'no': 'participate'})","dc682127":"p['participate'] = 1","9b99a7d8":"p.head()","db7ca183":"t = pd.concat([p,np])\nt = t.sort_index()\nt.head()","1eb96bfc":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = \"Smart Ad\"\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()","854e9d3b":"plotCorrelationMatrix(t,8)","45a0d3dc":"label_encoder = preprocessing.LabelEncoder()","02e9d1d9":"t['browser'] = label_encoder.fit_transform(t[\"browser\"])\nt['experiment'] = label_encoder.fit_transform(t[\"experiment\"])\nt['date'] = label_encoder.fit_transform(t[\"date\"])\nt['device_make'] = label_encoder.fit_transform(t[\"device_make\"])","1747e73e":"X = t.drop(columns={'auction_id', 'aware'}).values\nY = t['aware'].values","035cdf88":"# performing feature scaling on our training data\nfrom sklearn.preprocessing import StandardScaler\nscaler = MinMaxScaler()\n\n# fitting and transforming X_train while transforming X_test\nX = scaler.fit_transform(X)","8aa63379":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.1, random_state=6)","a26f3934":"from sklearn.linear_model import LogisticRegressionCV\n\nclf = LogisticRegressionCV(cv=5, random_state=6, scoring= 'accuracy')\nclf.fit(X_train, y_train)\ny_pred_log = clf.predict(X_test)\ny_predpr_log =clf.predict_proba(X_test)\nclf.score(X_train, y_train)","d1ab4f1c":"rocauc_scorer = metrics.make_scorer(metrics.accuracy_score)\n\nrfc = DecisionTreeClassifier(random_state=5)\n#                              , oob_score = True) \nparam_grid = { \n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth':[50,150],\n    'min_samples_leaf':[1,10]\n}\n\nCV_rfc = GridSearchCV(estimator=rfc, \n                      param_grid=param_grid,\n                      scoring = rocauc_scorer,\n                      cv= 5)\n\nCV_rfc.fit(X_train, y_train)\n","d5bdfa7c":"CV_rfc.score(X_train, y_train)","b5bce8f5":"y_pred_dec= CV_rfc.predict(X_test)\ny_predpr_dec= CV_rfc.predict_proba(X_test)\n","adcc819d":"features = t.columns\nimportances = CV_rfc.best_estimator_.feature_importances_\nindices = importances.argsort()\n\nf, ax = plt.subplots(figsize=(15, 8))\nplt.title('Feature Importance')\nplt.barh(range(len(indices)), importances[indices], color='orange', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')","66335f9a":"xgb1 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)","ab6cdaa3":"def modelfit(alg, useTrainCV=True, cv_folds=5, early_stopping_rounds=200):\n    \n    if useTrainCV:\n        xgb_param = alg.get_xgb_params()\n        xgtrain = xgb.DMatrix(X_train, label=y_train)\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n        alg.set_params(n_estimators=cvresult.shape[0])\n         \n            \n    #Fit the algorithm on the data\n    alg.fit(X_train, y_train,eval_metric='auc')\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(X_train)\n    dtrain_predprob = alg.predict_proba(X_train)[:,1]\n        \n    #Print model report:\n    print (\"\\nModel Report\")\n    print (\"Accuracy : %.4g\" % metrics.accuracy_score(y_train, dtrain_predictions))\n    print(\"AUC Score : %f\" % metrics.roc_auc_score(y_train, dtrain_predprob))\n                 \n   \n    \n    return alg","fc37a9a7":"mod =modelfit(xgb1)\n","401dc7ac":"y_pred_xgb =mod.predict(X_test)\ny_predpr_xgb =mod.predict_proba(X_test)","e005f72a":"features = t.columns\nimportances = mod.feature_importances_\nindices = importances.argsort()\n\nf, ax = plt.subplots(figsize=(15, 8))\nplt.title('Feature Importance')\nplt.barh(range(len(indices)), importances[indices], color='orange', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')","c6c6156b":"# printing the classification report for each classifier to assess performance\nfrom sklearn.metrics import classification_report\n\n# classification report for Logistic Regression\nprint(\"Logistic Regression classification report:\")\nprint(classification_report(y_test, y_pred_log))\n\n\n# classification report for Decision Tree Classifier\nprint(\"Decision Tree classification report:\")\nprint(classification_report(y_test, y_pred_dec))\n\n# classification report for Decision Tree Classifier\nprint(\"XGBoost report:\")\nprint(classification_report(y_test, y_pred_xgb))\n","3e499f1f":"\nprint('Logistic Regression classifier: Confusion Matrix')\nprint(confusion_matrix(y_pred_log, y_test))\n\nprint('Decision Tree classifier: Confusion Matrix')\nprint(confusion_matrix(y_pred_dec, y_test))\n\nprint('XGBoost Classifier : Confusion Matrix')\nprint(confusion_matrix(y_pred_xgb, y_test))\n","6be80173":"\nprint('Logistic Regression classifier: Log Loss')\nprint(log_loss(y_test,y_predpr_log))\n\nprint('Decision Tree classifier: Log Loss')\nprint(log_loss( y_test,y_predpr_dec))\n\nprint('XGBoost Classifier : Log Loss')\nprint(log_loss( y_test, y_predpr_xgb))\n","584cd1da":"We try to now rename the columns yes would represent people who are participated in the survey and are either aware or not and while no would represent those who didn't participate in the survey but where targetted by our Ad, that is they saw the Advert","d72dc379":"The different score above help us understand the Accuracy for the for each labels in our Data,For the Support is the total number of rows which is equal around our dataset.\n","e0f825f2":"The reason we used K-Fold Stratified Cross Validation is to maintain a Balanced data set that contains but Variable represented equal, so that our machine learning algorithm can learn properly","47a2d6a1":"In this Calculation we are assuming that the phone people use are not Factors that determine if they would click an Ad in the first place, so we focused on the time at which they are seeing the Ad, the day they see the Ads and the different platform which they see the ads, since the ad would be rendered exactly the same way for the same OS type, we assume that the ad might not be very appealling in some Browsers ","3f4f67c8":"> check if this worked","ea49c911":"I would append the people that didn't participate to the Data of people who participated, they would be represented as 0 which also means they didn't participate.","90df9834":"The difference with using Machine Learning to test Hypothesis here is explainability of your result and understanding ","9330dcce":"#### Predicting using Decision Trees ","afb53d61":"### Importing Libriaries for Analysis","9bc62887":"### Predictions and Accuracy of our Test data","19ea0b11":"#### Encoding our Data for our Machine Learning Model ","b5625d79":"The best Accuracy was gotten from the XG boost  Algorithm","15c46290":"### Plotting a Correlation Matrix","c0326603":"#### Confusion Matrix for our Data","33e81934":"> > confirm our changes\n   ","906e8550":"I would print a Classification report that would show","be16238c":"### Data Cleaning and Manipulation\n","f210bcbe":"### Explain what the difference is between using A\/B testing to test a hypothesis vs using Machine learning to learn the viability of the same effect?\n","b150d39e":"The data was splitted between those who participated and those that didn't","d7f22f92":"### What information do you gain using the Machine Learning approach that you couldn\u2019t obtain using A\/B testing?\n","7b4bbe8e":"I would base this analysis only on people who actually participated in the survey, since those are the most important people in this experiment.","1179c1ee":"### Explain the purpose of training using k-fold cross validation instead of using the whole data to train the ML models?\n","1a2174c8":"#### In this Notebook we would be analyzing the importance of different columns or features to the prediction of our Analysis, then try to understand what is the main determinant in the creating awareness for our brand, which is equal the number of yes, gotten. And if the experiment was also a determining factor in this experiment.\n","25be24ec":"### Columns Description\n\n* **auction_id:** the unique id of the online user who has been presented the BIO. In standard terminologies this is called an impression id. The user may see the BIO questionnaire but choose not to respond. In that case both the yes and no columns are zero. \n\n\n* **experiment:** which group the user belongs to - control or exposed.\n    * **control:** users who have been shown a dummy ad\n    * **exposed:** users who have been shown a creative, an online interactive ad, with the SmartAd brand. \n    \n    \n* **date:** the date in YYYY-MM-DD format\n\n\n* **hour:** the hour of the day in HH format.\n\n\n* **device_make:** the name of the type of device the user has e.g. Samsung\n\n\n* **platform_os:** the id of the OS the user has.\n\n\n* **browser:** the name of the browser the user uses to see the BIO questionnaire.\n\n\n* **yes:** 1 if the user chooses the \u201cYes\u201d radio button for the BIO questionnaire.\n\n\n* **no:** 1 if the user chooses the \u201cNo\u201d radio button for the BIO questionnaire.","99fa0e53":"That worked","e4c869e2":"I would use aware which signifies Yes to represent people who participated in the survey and know about our brand, i will fill the column of participate for all those who are participated by filling yes or no with one, for the non participant it is already zero","1f98d886":"### Apply ML and train using 5-fold CV\n\nTrain a machine learning model using 5-fold cross validation the following 3 different algorithms:     \n           Logistic Regression    \n           Decision Trees   \n           XGBoost    \n","edb6d2c2":"The information i couldn't get from the A\/B testing approach is what really drive my model and to what extent it does for each of the Features in our Data set, what can help us get what we are looking for more accurately basically","db2ef4c3":"#### Training and Fitting using Logistic Regression","04ef5a95":"#### Train, Test split for Prediction","f5f58ab4":"#### Training and Fitting using Xgboost"}}