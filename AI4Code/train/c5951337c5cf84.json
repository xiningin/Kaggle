{"cell_type":{"9d46ddcf":"code","ae296d52":"code","bb9aca62":"code","e358c76f":"code","cc38eea2":"code","ebf5e621":"code","1b596698":"code","eb402f52":"code","7d834f37":"code","5a26781a":"code","aebd4f49":"code","332e29ce":"code","eaf7793c":"code","6b5f9eaf":"code","769de6c4":"code","c8db5155":"code","24178f81":"code","17cbf06d":"code","93736f3c":"code","0d8dd47f":"code","9a73c8e3":"code","09dc6101":"code","a0f0941a":"code","39c6f9b5":"code","dd6d3c8f":"code","1ed3393e":"code","461a77c2":"code","6aa61c14":"code","5095d6f7":"code","1f52ddcb":"code","707a3779":"code","80e72f29":"code","b7aa4c85":"code","f56c8a8b":"code","2b27d1c5":"code","b758b082":"code","2001cfb4":"code","6464eeb6":"code","807e317f":"code","e7a4b8d3":"code","39a71893":"code","e82cff53":"code","64809719":"code","9b3742b6":"code","3b408789":"code","3a025b47":"code","ac07b02d":"code","af479410":"code","01fcf7b3":"code","b093e5e6":"code","3313a8e8":"code","7c8540c0":"code","edbd5be5":"code","5613d6d8":"code","f29193d7":"code","e07a0885":"markdown","e8a05ab0":"markdown","62fadd56":"markdown","ed9823b4":"markdown","08f7582a":"markdown","3968a4ac":"markdown","44cf9354":"markdown","dc1ba9af":"markdown","3bd4858e":"markdown","9ee0fe27":"markdown","1c5add7a":"markdown","ebeff06d":"markdown","1fadd284":"markdown","a699338a":"markdown","296085a3":"markdown","2d1f0745":"markdown","8252848b":"markdown"},"source":{"9d46ddcf":"#importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import xticks\nimport seaborn as sns\n\nfrom sklearn import metrics\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\n\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')","ae296d52":"#Importing train dataset\nhouse = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n\nprint(house.shape)","bb9aca62":"pd.set_option(\"display.max_columns\", 500)\nhouse.head()","e358c76f":"house.info()","cc38eea2":"# divide all variables to numerical and categorical\nhouse_cols = house.columns\nhouse_num= [x for x in house_cols if house[x].dtype in ['float64','int64']]\nhouse_cat= [x for x in house_cols if house[x].dtype=='object']\n\n# 'MSSubClass','OverallQual','OverallCond' are categorical variable but values are numeric so move to categorical list\nhouse_num.remove('MSSubClass')\nhouse_cat.append('MSSubClass')\n\nhouse_num.remove('OverallQual')\nhouse_cat.append('OverallQual')\n\nhouse_num.remove('OverallCond')\nhouse_cat.append('OverallCond')\n\nprint(house_num)\nprint(house_cat)","ebf5e621":"# check missing values in Numerical columns\nNA_house = house[house_num].isnull().sum()\nNA_house = NA_house[NA_house.values >0]\nNA_house = NA_house.sort_values(ascending=False)\nplt.figure(figsize=(20,4))\nNA_house.plot(kind='bar')\nplt.title('List of Columns & NA counts')\nplt.show()","1b596698":"# check missing values in Categorical columns\nNA_house = house[house_cat].isnull().sum()\nNA_house = NA_house[NA_house.values >0]\nNA_house = NA_house.sort_values(ascending=False)\nplt.figure(figsize=(20,4))\nNA_house.plot(kind='bar')\nplt.title('List of Columns & NA counts')\nplt.show()","eb402f52":"def missing_value_correction(df,df_cat,df_num):\n    for col in df_cat:\n        df[col]=df[col].fillna('unknown')\n    for col in df_num:\n        df[col]=df[col].fillna(0)\n    return df","7d834f37":"# apply missing value correction on the dataset\nhouse = missing_value_correction(house, house_cat, house_num)\nhouse.head()","5a26781a":"### GarageFinish GarageQual GarageCond GarageType if NA then no Garage\n\nfig, ax=plt.subplots(nrows =1,ncols=4,figsize=(20,8))\nsns.boxplot(y=house['GarageArea'], x=house['GarageFinish'],ax=ax[0])\nsns.boxplot(y=house['GarageArea'], x=house['GarageQual'],ax=ax[1])\nsns.boxplot(y=house['GarageArea'], x=house['GarageCond'],ax=ax[2])\nsns.boxplot(y=house['GarageArea'], x=house['GarageType'],ax=ax[3])\nxticks(rotation = 90)\nplt.show","aebd4f49":"### BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinSF1, BsmtFinType2 and BsmtFinSF2\n\nfig, ax=plt.subplots(nrows =2,ncols=3,figsize=(20,8))\nsns.boxplot(y=house['TotalBsmtSF'], x=house['BsmtQual'],ax=ax[0][0])\nsns.boxplot(y=house['TotalBsmtSF'], x=house['BsmtCond'],ax=ax[0][1])\nsns.boxplot(y=house['TotalBsmtSF'], x=house['BsmtExposure'],ax=ax[0][2])\nsns.boxplot(y=house['BsmtFinSF1'], x=house['BsmtFinType1'],ax=ax[1][0])\nsns.boxplot(y=house['BsmtFinSF2'], x=house['BsmtFinType2'],ax=ax[1][1])","332e29ce":"house[(house['BsmtExposure']=='unknown')& (house['TotalBsmtSF']>0)]","eaf7793c":"house[(house['BsmtFinType2']=='unknown')& (house['BsmtFinSF2']>0)]","6b5f9eaf":"## MasVnrArea MasVnrType\n\nplt.figure(figsize=(20, 10))\nsns.boxplot(y=house['MasVnrArea'], x=house['MasVnrType'])","769de6c4":"# create new columns 'BuiltAge', 'RemodelAge','GarageAge'\ndef date_variable_toAge(house, house_cat, house_num):\n    import datetime\n    today = datetime.datetime.now()\n\n    house.loc[np.isnan(house['YearBuilt']), ['YearBuilt']] = today.year\n    house.loc[np.isnan(house['YearRemodAdd']), ['YearRemodAdd']] = today.year\n    house.loc[np.isnan(house['GarageYrBlt']), ['GarageYrBlt']] = today.year\n    house.loc[np.isnan(house['YrSold']), ['YrSold']] = today.year\n\n    house['BuiltAge'] = house['YearBuilt'].apply(lambda x : today.year-x )\n    house['RemodelAge'] = house['YearRemodAdd'].apply(lambda x: today.year-x )\n    house['GarageAge'] = house['GarageYrBlt'].apply(lambda x: today.year-x)\n    house['SoldAge'] = house['YrSold'].apply(lambda x: today.year-x)\n\n    house = house.drop(['YearBuilt','YearRemodAdd','GarageYrBlt','YrSold'],axis=1)\n    \n    house_num.remove('YearBuilt')\n    house_num.remove('YearRemodAdd')\n    house_num.remove('GarageYrBlt')\n    house_num.remove('YrSold')\n    \n    house_num.append('BuiltAge')\n    house_num.append('RemodelAge')\n    house_num.append('GarageAge')\n    house_num.append('SoldAge')\n    \n    return house, house_cat, house_num","c8db5155":"# change year values to Age of the field as of today\nhouse, house_cat, house_num = date_variable_toAge(house, house_cat, house_num)","24178f81":"house.head()","17cbf06d":"# Dummy variables for catrgorical variables \ndef create_dummy_for_cat(house,house_cat):\n    house_dummies=house['Id']\n\n    for col in house_cat:\n        house_dummy = pd.get_dummies(house[col], prefix=col ,drop_first=True) \n        house_dummies = pd.concat([house_dummies , house_dummy] ,axis=1) \n    \n    house_dummies.drop(columns='Id',inplace=True)\n\n    return house_dummies","93736f3c":"house_dummies = create_dummy_for_cat(house, house_cat)\nhouse = pd.concat([house , house_dummies] ,axis=1) \nhouse.drop(columns =house_cat, inplace = True)\nhouse.shape","0d8dd47f":"house.head()","9a73c8e3":"# Checking outliers at 25%,50%,75%,90%,95% and 99%\nhouse[house_num].describe(percentiles=[.25,.5,.75,.90,.95,.99])","09dc6101":"# box plot all numerical variable\nplt.figure(figsize=(20, 50))\nj=0\nfor i in ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1',  'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea']:\n    j=j+1\n    plt.subplot(7,4,j)\n    sns.boxplot(data=house, y=i)\n    \nplt.show()","a0f0941a":"house[(house['LotFrontage']>300)\n        |(house['LotArea']>200000)\n        |(house['MasVnrArea']>1200)\n        |(house['BsmtFinSF1']>4000)\n        |(house['TotalBsmtSF']>5000)\n        |(house['1stFlrSF']>4000)\n        |(house['2ndFlrSF']>2000)\n        |(house['GrLivArea']>5000)]","39c6f9b5":"house=house[~house['Id'].isin([298,314,1299])]\nhouse.shape","dd6d3c8f":"house=house.set_index('Id')\nhouse_num.remove('Id')","1ed3393e":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\n# Apply scaler() to all the columns except the categorical variables and target variable\nhouse_scale_columns = house_num\nhouse_scale_columns.remove('SalePrice')\nhouse[house_scale_columns] = scaler.fit_transform(house[house_scale_columns])","461a77c2":"house.head()","6aa61c14":"X= house.drop('SalePrice',axis=1)\ny= house['SalePrice']","5095d6f7":"# split into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    train_size=0.7,\n                                                    test_size = 0.3, random_state=100)","1f52ddcb":"# list of alphas to tune\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50 ]}\n\n\nridge = Ridge()\n\n# cross validation\nfolds = 5\nmodel_cv = GridSearchCV(estimator = ridge, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel_cv.fit(X_train, y_train)","707a3779":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results = cv_results[cv_results['param_alpha']>=1]\ncv_results.head()","80e72f29":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\n\n# plotting\nplt.figure(figsize=(20, 10))\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.vlines(x=2.5,ymax=-5000, ymin=-17500, colors=\"r\", linestyles=\"--\")\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","b7aa4c85":"alpha = 2.5\nridge = Ridge(alpha=alpha)\n\nridge.fit(X_train, y_train)","f56c8a8b":"y_pred = ridge.predict(X_test)","2b27d1c5":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_pred)\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)                          # Y-label","b758b082":"from sklearn.metrics import r2_score\nr2_score(y_test, y_pred)","2001cfb4":"lasso = Lasso()\n\n# list of alphas to tune\nparams = {'alpha': [1, 10, 25, 35, 40, 45, 50]}\n\n\n# cross validation\nfolds = 5\n\n# cross validation\nmodel_cv = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \n\nmodel_cv.fit(X_train, y_train) ","6464eeb6":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","807e317f":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n\n# plotting\nplt.figure(figsize=(20, 10))\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.vlines(x=25, ymax=-10000, ymin=-17500, colors=\"r\", linestyles=\"--\")\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","e7a4b8d3":"alpha =25\n\nlasso = Lasso(alpha=alpha)\n        \nlasso.fit(X_train, y_train) ","39a71893":"# feature selection using Lasso\nfeatures = pd.DataFrame(lasso.coef_,X_train.columns)\nfeatures = features.reset_index()\nfeatures.columns = ['feature','coeficients']\nselected_features = features[features['coeficients']>0]\nselected_features=selected_features.sort_values(by='coeficients', ascending =False)\nselected_features.columns = ['feature','coeficients']\nselected_features","e82cff53":"y_pred = lasso.predict(X_test)","64809719":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_pred)\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)                          # Y-label","9b3742b6":"from sklearn.metrics import r2_score\nr2_score(y_test, y_pred)","3b408789":"import xgboost as xgb\n\nmodel_xgb = xgb.XGBRegressor(n_estimators=200)\n\n# list of alphas to tune\nparams = {'learning_rate':[0.03,0.05,0.1],\n         'subsample':[0.2,0.5,0.6]}\n\n\n# cross validation\nfolds = 3\n\n# cross validation\nmodel_cv = GridSearchCV(estimator = model_xgb, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \n\nmodel_cv.fit(X_train, y_train) ","3a025b47":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results[cv_results['mean_test_score']== max(cv_results['mean_test_score'])]","ac07b02d":"model_xgb = xgb.XGBRegressor(n_estimators=200,learning_rate= 0.05, subsample= 0.6 )\nmodel_xgb.fit(X_train,y_train)\n\nxgb_pred = model_xgb.predict(X_test)\nr2_score(y_test, xgb_pred)","af479410":"model_ridge = Ridge(alpha = 2.5)\nmodel_ridge.fit(X_train, y_train)\n\nmodel_lasso = Lasso(alpha = 25)\nmodel_lasso.fit(X_train, y_train)\n\nmodel_xgb = xgb.XGBRegressor(n_estimators=200,learning_rate= 0.05, subsample= 0.6 )\nmodel_xgb.fit(X_train,y_train)\n\nr_pred = model_ridge.predict(X_test)\nl_pred = model_lasso.predict(X_test)\nx_pred = model_xgb.predict(X_test)\n\ny_pred= r_pred*0.7 + x_pred *0.25 + l_pred* 0.05\nr2_score(y_test, y_pred)","01fcf7b3":"test=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ntest_cols = test.columns\ntest_num= [x for x in test_cols if test[x].dtype in ['float64','int64']]\ntest_cat= [x for x in test_cols if test[x].dtype=='object']\n\n# 'MSSubClass','OverallQual','OverallCond' are categorical variable but values are numeric so move to categorical list\ntest_num.remove('MSSubClass')\ntest_cat.append('MSSubClass')\n\ntest_num.remove('OverallQual')\ntest_cat.append('OverallQual')\n\ntest_num.remove('OverallCond')\ntest_cat.append('OverallCond')\n\nprint(test_num)\nprint(test_cat)","b093e5e6":"# Pre-Processing test set similarily to training\n\ntest = missing_value_correction(test,test_cat,test_num)\ntest,test_cat,test_num=  date_variable_toAge(test,test_cat,test_num)","3313a8e8":"# Create Dummies from cateogrical features in test set\ntest_dummies = create_dummy_for_cat(test,test_cat)\ntest = pd.concat([test , test_dummies] ,axis=1) \ntest.drop(columns =test_cat, inplace = True)\ntest.shape","7c8540c0":"test=test.set_index('Id')\ntest_num.remove('Id')\n\n# Apply scaler() as in training data to all the columns except the categorical variables and target variable\ntest_scale_columns = test_num\ntest[test_scale_columns] = scaler.fit_transform(test[test_scale_columns])","edbd5be5":"# compare and print features of test Not in the Lasso set of selected features\n# Add a Dummy column with all zero for such features\n\nfor col in list(X_train.columns):\n    if col not in list(test.columns):\n        print(col)\n        test[col]=0\ntest = test[X_train.columns]","5613d6d8":"r_pred = model_ridge.predict(test)\nl_pred = model_lasso.predict(test)\nx_pred = model_xgb.predict(test)\n\ny_test_pred= r_pred*0.7 + x_pred *0.25 + l_pred* 0.05\n\nsubmission= pd.DataFrame({'Id': test.index, 'SalePrice': y_test_pred})\nsubmission.head()","f29193d7":"submission.to_csv('HPP_submission.csv', index=False)","e07a0885":"### Create Dummy Variables for Categorical variables","e8a05ab0":"###  Missinng Value treatment \nDefine a function to treat any missing values in both categorical and numerical columns","62fadd56":"# Problem Statement\n\nA US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them on at a higher price.\n\n1. Build a regression model using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.\n2. Using lasso identify variables are significant in predicting the price of a house.\n3. Determine the optimal value of lambda for ridge and lasso regression","ed9823b4":"### feature selection using Lasso","08f7582a":"### Check for outliers","3968a4ac":"predict on given test to generate submission file","44cf9354":"## Ridge Regression","dc1ba9af":"Ensemble of Lasso, Ridge and XGBoost","3bd4858e":"We suspect possibility of outlier in ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'SalePrice']","9ee0fe27":"drop id = 298,314,935,1299 which are clear outliers","1c5add7a":"# Final Model","ebeff06d":"## Rescaling the Features","1fadd284":"## Lasso Regresssion","a699338a":"### Define a function to Derive Age columns for different date","296085a3":"## Top 5 Predictors\n1. GrLivArea: Above grade (ground) living area square feet\n2. TotalBsmtSF: Total square feet of basement area\n3. OverallQual: Rates the overall material and finish of the house = 9 or 10\n4. 2ndFlrSF: Second floor square feet\n5. 1stFlrSF: Second floor square feet\n","2d1f0745":"## EDA\n\nLets plot categoical and numerical columns in order of most missing values.","8252848b":"## Model Building\n\n### Test Train split"}}