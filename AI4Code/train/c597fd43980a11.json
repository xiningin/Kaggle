{"cell_type":{"f8dd6b87":"code","282d51d9":"code","2c7fbe69":"code","9aeb84b7":"code","141b0a54":"code","30ce810b":"code","33a6978d":"code","c037762b":"code","e92801f7":"code","81207b3c":"code","fb5baa64":"code","e507502d":"code","b9e0a333":"code","16b6dc22":"code","52758936":"code","d94e8f6f":"code","bc3cbd83":"code","992773ce":"code","ec5b6873":"code","3a10121d":"code","dbc97104":"code","aa3e2c91":"code","21055873":"code","1ece9038":"code","eaf0ff91":"code","eb011c69":"code","f8802eca":"code","c9efbe1f":"code","aa88d58a":"code","729870a7":"code","900a3124":"code","944d60ca":"code","0a7531ee":"code","e250ec6a":"code","fa0e2bc6":"code","d10b0410":"code","08cfad97":"code","6021a11f":"code","14e50df1":"code","017453da":"code","5df33ce0":"markdown","14e416a3":"markdown","709011ea":"markdown","a397df24":"markdown","afa411d8":"markdown","e9d98575":"markdown","8847e909":"markdown","a1aa824d":"markdown","c8d80244":"markdown","058c3bbb":"markdown","91788193":"markdown","1e8ccdba":"markdown","9efdc4f8":"markdown","c77a73ed":"markdown","668efc7d":"markdown","5374accb":"markdown","d4a4ca02":"markdown","ff7e79bd":"markdown","05a7babf":"markdown","fef0d047":"markdown","b0476f9e":"markdown","aef6e7f1":"markdown","89b1cdff":"markdown","7df91605":"markdown","2c943dcd":"markdown","7aa2aca0":"markdown","fbb53107":"markdown","0da4f6f6":"markdown","0fad5674":"markdown","3ff867e7":"markdown","0709ba1f":"markdown","9e30316c":"markdown","3054ad73":"markdown","a9834b9a":"markdown","5c393fb1":"markdown","6e862cd7":"markdown","5354b982":"markdown","cbd26916":"markdown","9e594181":"markdown","4a8f7e3c":"markdown","15c7b207":"markdown","31539b64":"markdown","c94d04fb":"markdown","c9598ef7":"markdown","be3dba0e":"markdown","6cdd1da0":"markdown"},"source":{"f8dd6b87":"import matplotlib.pyplot as plt\nfrom keras.models import Model\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom numpy import expand_dims\nimport numpy as np\nfrom tensorflow.keras.utils import plot_model\nimport cv2","282d51d9":"from tensorflow import keras\nbase_model = keras.models.load_model('..\/input\/intel-image-classifier-model\/intel_image_classifier.h5')\nbase_model.summary()","2c7fbe69":"plot_model(base_model, show_shapes=True)","9aeb84b7":"# summarize filter shapes\nfor layer in base_model.layers:\n    print(layer.name)","141b0a54":"# Summarize filter shapes\nfor layer in base_model.layers:\n    # Check for convolutional layer\n    if 'conv' not in layer.name:\n        continue\n    # Get filter weights\n    filters, biases = layer.get_weights()\n    print(layer.name, filters.shape)","30ce810b":"base_model.layers[0]","33a6978d":"# get filter weights\nfilters, biases = base_model.layers[0].get_weights()\nprint(base_model.layers[0].name, filters.shape)","c037762b":"filters.shape","e92801f7":"biases","81207b3c":"# Scale \nf_min, f_max = filters.min(), filters.max()\nfilters = (filters - f_min) \/ (f_max - f_min)\n\n# Plot one filter of the first convolutionnal layer\nn_filters, ix = 1, 1\nf = filters[:, :, :, 0]\nplt.imshow(f[:, :, 0], cmap='gray') # first filter, index 0\n\nplt.show()","fb5baa64":"# Get conv layers\nconv_layers = [layer for layer in base_model.layers if 'conv' in layer.name]\nconv_layers","e507502d":"len(conv_layers)","b9e0a333":"# Figure size\nfig = plt.figure(figsize=(10, 30))\n\n# plot first few filters\nn_filters, ix = len(conv_layers), 1\nn_per_line = 3\nfor i in range(n_filters):\n    # Retrive weights from layer i\n    filters, biases = conv_layers[i].get_weights()\n    # normalize filter values to 0-1 so we can visualize them\n    f_min, f_max = filters.min(), filters.max()\n    filters = (filters - f_min) \/ (f_max - f_min)\n    # get the filter\n    f = filters[:, :, :, 1]\n\n    # Plot\n    # specify subplot and turn of axis\n    ax = plt.subplot(n_filters, n_per_line, ix)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(conv_layers[i].name)\n    # plot filter channel in grayscale\n    plt.imshow(f[:, :, 0], cmap='gray')\n    ix += 1\n# show the figure\nplt.show()","16b6dc22":"# To store indexes of convolutionnal layers (will be used later)\nconv_indexes = []\n\n\nfor i in range(len(base_model.layers)):\n    layer = base_model.layers[i]\n    # Check for convolutional layer\n    if 'conv' not in layer.name:\n        continue\n    \n    # Summarize output shape\n    print(i, layer.name, layer.output.shape)\n    conv_indexes.append(i)","52758936":"# redefine model to output right after the first hidden laye\nfirst_conv_model = Model(inputs=base_model.inputs, outputs=base_model.layers[0].output)\nfirst_conv_model.summary()","d94e8f6f":"plot_model(first_conv_model)","bc3cbd83":"img_path = '..\/input\/intel-image-classification\/seg_train\/seg_train\/street\/10070.jpg'\nimg_size = (128,128)","992773ce":"img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)    \nimg = cv2.resize(img, img_size) # Resize the images\n\nimg = np.array(img)\nimg = img\/255\n\nprint(img.shape)","ec5b6873":"# Just to plot a colored version\nimg_color = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)    \nplt.imshow(img_color[...,::-1])","3a10121d":"# Expand dimensions so that it represents a single 'sample'\nimg = expand_dims(img, axis=0)\n\n# Get prediction (feature map) for first hidden layer\nfeature_maps = first_conv_model.predict(img)\nprint(feature_maps.shape)","dbc97104":"n_features_maps = 32","aa3e2c91":"def plot_feature_maps(feature_maps, cmap=None, square_size=8):\n    fig = plt.figure(figsize=(20, 20))\n\n    n_feature_maps = np.array(feature_maps).shape[-1]\n    print(\"There are {} feature maps\".format(n_feature_maps))\n    ix = 1\n    for _ in range(square_size):\n        for _ in range(square_size):\n            # Specify subplot and turn of axis\n            ax = plt.subplot(square_size, square_size, ix)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_title(\"Feature map {}\".format(ix))\n            # Plot \n            plt.imshow(feature_maps[0, :, :, ix-1], cmap=cmap)\n            ix += 1\n\n        if(ix >= n_feature_maps):\n            break\n\n    # Show the figure\n    plt.show()","21055873":"# Plot all 32 maps\nplot_feature_maps(feature_maps)","1ece9038":"base_model.layers[4]","eaf0ff91":"# redefine model to output right after the first hidden layer\nsecond_conv_model = Model(inputs=base_model.inputs, outputs=base_model.layers[4].output)\nsecond_conv_model.summary()","eb011c69":"plot_model(second_conv_model)","f8802eca":"feature_maps = second_conv_model.predict(img)\nprint(\"Shape of feature maps: {}\".format(feature_maps.shape))\n\nplot_feature_maps(feature_maps)","c9efbe1f":"# redefine model to output right after the first hidden layer\nlayer_idx = 7\nconv_model = Model(inputs=base_model.inputs, outputs=base_model.layers[layer_idx].output)\nconv_model.summary()","aa88d58a":"plot_model(conv_model)","729870a7":"feature_maps = conv_model.predict(img)\nprint(\"Shape of feature maps: {}\".format(feature_maps.shape))\n\nplot_feature_maps(feature_maps)","900a3124":"conv_indexes","944d60ca":"\noutputs = [base_model.layers[i].output for i in conv_indexes] # now our conv_indexes variable is useful :)\nconv_layers_model = Model(inputs=base_model.inputs, outputs=outputs)\nconv_layers_model.summary()","0a7531ee":"plot_model(conv_layers_model)","e250ec6a":"from IPython.core.display import HTML, display\ndef render_html(text):\n    raw_html = \"<h1>\"+text+\"<\/h1>\"\n    return HTML(raw_html)","fa0e2bc6":"# Get the feature maps\nfeature_maps = conv_layers_model.predict(img)\n\nconv_idx = 0\nfor fmap in feature_maps:\n    display(render_html(\"Feature maps extracted from {}\".format(conv_layers[conv_idx].name)))\n    conv_idx += 1\n    plot_feature_maps(fmap)","d10b0410":"def get_model_with_conv_layers_only(base_model):\n    \"\"\"\n    Build a model from the conv. layers in the base_model\n    \"\"\"\n    \n    # Get convolutionnal layers from model\n    conv_layers = [layer for layer in base_model.layers if 'conv' in layer.name]\n    print(\"There are {} conv. layers in the model.\".format(len(conv_layers)))\n\n    # Create new model from convolutionnal layers\n    outputs = [conv_layer.output for conv_layer in conv_layers]\n    model = Model(inputs=base_model.inputs, outputs=outputs)\n    \n    return model\n    ","08cfad97":"def load_img(img_path, img_size=(128,128), color_flag=cv2.IMREAD_UNCHANGED):\n    img = cv2.imread(img_path, color_flag)    \n    img = cv2.resize(img, img_size) # Resize the images\n    img = np.array(img)\n    \n    # Gray version used by our model\n    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    img_gray = img_gray\/255\n\n    print(\"Img shape: {}\".format(img.shape))\n    plt.imshow(img[...,::-1]) #cmap=plt.cm.binary\n    \n    \n    \n    return expand_dims(img_gray, axis=0) # ready to be fed to a model\n    ","6021a11f":"# Load image to test\npath = '..\/input\/intel-image-classification\/seg_train\/seg_train\/street\/10131.jpg'\nimg = load_img(path)\n\nmodel = get_model_with_conv_layers_only(base_model)\n\nfeature_maps = model.predict(img)\nconv_idx = 0\nfor fmap in feature_maps:\n    display(render_html(\"Feature maps extracted from {}\".format(conv_layers[conv_idx].name)))\n    conv_idx += 1\n    plot_feature_maps(fmap)","14e50df1":"# Load image to test\npath = '..\/input\/intel-image-classification\/seg_train\/seg_train\/forest\/10086.jpg'\nimg = load_img(path)\n\nmodel = get_model_with_conv_layers_only(base_model)\n\nfeature_maps = model.predict(img)\nconv_idx = 0\nfor fmap in feature_maps:\n    display(render_html(\"Feature maps extracted from {}\".format(conv_layers[conv_idx].name)))\n    conv_idx += 1\n    plot_feature_maps(fmap)","017453da":"# Load image to test\npath = '..\/input\/intel-image-classification\/seg_train\/seg_train\/sea\/10041.jpg'\nimg = load_img(path)\n\nmodel = get_model_with_conv_layers_only(base_model)\n\nfeature_maps = model.predict(img)\nconv_idx = 0\nfor fmap in feature_maps:\n    display(render_html(\"Feature maps extracted from {}\".format(conv_layers[conv_idx].name)))\n    conv_idx += 1\n    plot_feature_maps(fmap)","5df33ce0":"We will create a model containing only the convolution layers. We will then be able to retrieve the feature maps obtained for each of the successive layers and compare them.","14e416a3":"# With another image","709011ea":"There is also the conv2d feature map 21 (first layer) which is starting to look familiar with the way it transforms the base image :D. It stands out from the rest (both it and feature map 7)","a397df24":"## A short trip to the sea","afa411d8":"We understand globally how a convolutional neural network works, we know how to explain the mathematical equations that are used; but in the end we don't see what happens inside the network. We pass an image as input, it goes through the network and the model outputs a prediction. The goal here is to look inside the model to understand a little better how it \"sees\". What it relies on to make the classification.","e9d98575":"# What does my model see?  \nDiving into the heart of a deep learning model","8847e909":"# Load pretrained model","a1aa824d":"This image is part of the class \"**street**\".","c8d80244":"The feature map is the result obtained after applying a filter on an image. The visualization of the feature maps allows us to detect which features are detected by the filters (lines, shapes, etc.). By observing what is preserved (or ignored) from the original image we can try to understand the features that are analyzed by the model.","058c3bbb":"# Feature maps","91788193":"The model used here is from the [notebook](https:\/\/www.kaggle.com\/kelvar\/intel-image-classification-with-cnn) where I did the training. ","1e8ccdba":"#### Let's display some filters from all the convolution layers:","9efdc4f8":"### Plot the result","c77a73ed":"The deeper layers of the network contain many more feature maps (128, 256 etc.). For each layer, we limit ourselves to 64 feature maps to display. ","668efc7d":"# Feature maps from all conv. layers","5374accb":"On the first convolution layer we had 32 filters. Each of these filters applied to the original image gives us these 32 feature maps. #captainobvious","d4a4ca02":"#### A few thoughts","ff7e79bd":"# Filters","05a7babf":"The more we go through the layers, the less the results we get are humanly interpretable (just by visualizing them)","fef0d047":"Here when we look from layer to layer, we see that the sky with the sun comes out from one layer to the other. See for example :\n* conv2d : 7, 21\n* conv2d_1: 14, 34\n* conv2d_2: 1, 6, 22, 45 \n...\n\nWe also see the water that can be clearly distinguished from layer to layer. And even in the layers where you can't see anything concrete, you can see shapes that remind you of the results of previous layers. The observed shapes tend to resemble each other from layer to layer and we can see how we could go from one to the other.","b0476f9e":"We will first retrieve the indexes of the convolution layers and the output format (the shape of the feature map). ","aef6e7f1":"So here is what one of the filters applied to the first convolution layer of the network looks like. ","89b1cdff":"First layer","7df91605":"Let's do some tests on a dataset image.","2c943dcd":"![conor-luddy-IGa3Md8wP6g-unsplash.jpg](attachment:06785c57-24af-4822-85be-41ed8b2fe14c.jpg)","7aa2aca0":"### Load image data","fbb53107":"Great.","0da4f6f6":"Photo by @opticonor from Unsplash","0fad5674":"## Feature maps from the second conv layer (at index 4)","3ff867e7":"First, let's go around all the layers of the network. We will display the name of each layer.","0709ba1f":"We will start by looking at some filters used by the neural network. Filters are used to extract features from input images. A filter is ultimately a matrix that is applied to the image.","9e30316c":"What we are interested in here are the convolution steps. For each convolution layer, we will retrieve the shape of the associated filter. ","3054ad73":"On some feature maps you can see that the buildings are highlighted (feature maps 3, 13, 20 for eg.). When we look a little more in detail we can see that on these feature maps are not identical. On some of them, the foreground elements stand out and are more visible (at least for the human that I am) - like for example between feature map 20 and 28. \n  \nOn other pictures, you can't see much. A few lines of buildings and a few points (2, 6, 24, 26).\n  \nLet's see what it looks like after going through the second convolution layer.","a9834b9a":"As before, we create a model that gives us its output at the desired convolution layer.","5c393fb1":"## Here we go again","6e862cd7":"To do this, we build a new model that has the first convolution layer as its final layer. Our new model therefore renders its output directly after the input image has passed through this convolution layer.","5354b982":"## Feature maps a random conv layer","cbd26916":"We can see that there are the shapes of the buildings that are detected in the first convolution layers. For example, in the conv2d layer, similar results to the previous image are observed. The buildings stand out in the image. On some feature maps you can see the road features next to them which also stand out.   \n  \nOn other feature maps, it looks like windows are coming out of the building. For example on conv2d_2 (the third convolution layer) with feature maps 41, 47, 60 or 63. \n\nOn the following layers on the other hand it starts to be difficult to distinguish elements. And the more we go on, the less we have things that are humanly understandable.  \n\nWe can still see globally that there are the contours of objects that have emerged in the first layers. Objects in the foreground are also more or less visible on some feature maps.","9e594181":"##### References","4a8f7e3c":"https:\/\/machinelearningmastery.com\/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks\/  \nhttps:\/\/www.youtube.com\/watch?v=ho6JXE3EbZ8  \nhttps:\/\/towardsdatascience.com\/convolutional-neural-network-feature-map-and-filter-visualization-f75012a5a49c","15c7b207":"# Conclusion","31539b64":"We can see more elements than on the first convolution layer. On the feature maps 14 for example we have the feeling that the sky is highlighted. The same observation is made on other results.","c94d04fb":"#### A few thoughts","c9598ef7":"## Feature maps from the 1st conv. layer","be3dba0e":"32 filters of shape 3,3 and depth 1 (channel - grayscale)","6cdd1da0":"The final word goes to Jason Brownlee from https:\/\/machinelearningmastery.com:  \n\n\"*We can see that the feature maps closer to the input of the model capture a lot of fine detail in the image and that as we progress deeper into the model, the feature maps show less and less detail.*  \n  \n*This pattern was to be expected, as the model abstracts the features from the image into more general concepts that can be used to make a classification. Although it is not clear from the final image that the model saw a bird, we generally lose the ability to interpret these deeper feature maps.* \""}}