{"cell_type":{"a239fd80":"code","596b77eb":"code","6c71704a":"code","a98f3e49":"code","9ad664c6":"code","3e1c81ee":"code","1dc56156":"code","3fa02e8a":"code","3db40925":"code","2d29e8fe":"code","fac24427":"code","cd3f4af3":"code","df3b22ca":"code","98f754d1":"code","1db09d1d":"code","a8c46f6e":"code","2bafe3b3":"code","742c821c":"code","98817d5c":"code","a8604a80":"code","baecfb90":"code","4bad2091":"code","c7472283":"code","6be8b474":"code","753c13b7":"code","f17c4a30":"code","7592ae6e":"code","00bd942b":"code","0ba9ae55":"code","6dec4572":"code","4907363a":"code","13d59b24":"code","88b2220a":"code","941a36d1":"code","c92097e5":"code","688adfe8":"code","72e5be55":"code","7d2f956a":"code","5cbd5011":"code","95891ccc":"code","17a15eba":"code","d5e68ca6":"code","8812144a":"code","4e06d0f9":"code","4a56d5c1":"code","d2261741":"code","5c4f99ca":"code","24a408a3":"code","43be6bba":"code","177a8ce7":"code","4e54944b":"markdown","4e642058":"markdown","7b4990fc":"markdown","bf54d0fd":"markdown","2de93281":"markdown","14d64e26":"markdown","c08eef50":"markdown","a2102783":"markdown","5887db1f":"markdown","c1dec53d":"markdown","2f9cc706":"markdown","bee552d4":"markdown","d0ffd344":"markdown","787dd6d3":"markdown","bcc69feb":"markdown","91a5fb87":"markdown","83a60bb4":"markdown","5b4c9331":"markdown","251f917b":"markdown","af96dc85":"markdown","c2a39728":"markdown","6b1b05da":"markdown","0e54634a":"markdown","05909de0":"markdown"},"source":{"a239fd80":"from __future__ import print_function, division\n\nimport time\nimport os\nimport copy\nfrom tqdm import tqdm\nimport random\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport cv2\n\nfrom sklearn.model_selection import StratifiedKFold, KFold, ShuffleSplit, StratifiedShuffleSplit\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision import datasets, models, transforms\n\nplt.ion()","596b77eb":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \n    \ndef get_img(path):\n    im_bgr = cv2.imread(path)\n    im_rgb = im_bgr[:, :, ::-1]\n    return im_rgb","6c71704a":"seed_everything(42)","a98f3e49":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","9ad664c6":"train_data = pd.read_csv('..\/input\/greenhouse-plant-classification\/train.csv')\ntest_data = pd.read_csv('..\/input\/greenhouse-plant-classification\/test.csv')","3e1c81ee":"train_data['label'].nunique()","1dc56156":"num_classes = len(train_data['label'].unique())\nnum_classes","3fa02e8a":"train_set, validation_set = train_data.iloc[:int(len(train_data) * 0.7)], train_data.iloc[int(len(train_data) * 0.7):]","3db40925":"train_set.shape, validation_set.shape","2d29e8fe":"data_transforms = {\n    'train': transforms.Compose([\n      transforms.ToPILImage(),\n      transforms.Resize(224),\n      transforms.ToTensor(),\n      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    \n    'val': transforms.Compose([\n      transforms.ToPILImage(),\n      transforms.Resize(224),\n      transforms.ToTensor(),\n      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n}","fac24427":"class DLRDataset(Dataset):\n    def __init__(self, df, data_root, transforms=None, is_train=True):\n        \n        super().__init__()\n        self.df = df.reset_index(drop=True).copy()\n        self.transforms = transforms\n        self.data_root = data_root\n        self.is_train = is_train\n    \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, index: int):\n        \n        if self.is_train:\n            target = self.df.iloc[index]['label']\n          \n        path = \"{}\/{}\".format(self.data_root, self.df.iloc[index]['id'])        \n        img  = get_img(path)\n        \n        if self.transforms:\n            img = self.transforms(img)\n            \n        if self.is_train:\n            return img, target\n        else:\n            return img","cd3f4af3":"train_dataset = DLRDataset(df=train_set,\n                           data_root='..\/input\/greenhouse-plant-classification\/train\/train',\n                           transforms=data_transforms['train'],\n                           is_train=True)\n\nvalid_dataset = DLRDataset(df=validation_set,\n                           data_root='..\/input\/greenhouse-plant-classification\/train\/train',\n                           transforms=data_transforms['train'],\n                           is_train=True)\n\ntest_dataset = DLRDataset(df=test_data,\n                           data_root='..\/input\/greenhouse-plant-classification\/test\/test',\n                           transforms=data_transforms['val'],\n                           is_train=False)","df3b22ca":"train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=8, shuffle=True, num_workers=4)\nvalid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=8, shuffle=False, num_workers=4)\ntest_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=8, shuffle=False, num_workers=4)","98f754d1":"from PIL import Image\n\nimage = Image.open('..\/input\/greenhouse-plant-classification\/test\/test\/1598.jpg')\nimage.show()","1db09d1d":"###\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n\n        # (batch size ommited) [channels, H, W]\n\n        self.cnn_layers = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=0),  # [3, 224, 224] -> [16, 220, 220]\n            nn.BatchNorm2d(16),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0),  # [16, 220, 220] -> [32, 216, 216]\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2),  # [32, 216, 216] -> [32, 108, 108]\n\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=0),  # [32, 108, 108] -> [64, 104, 104]\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2),  # [64, 104, 104] -> [64, 52, 52]\n            #nn.Dropout(0.3),\n\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=0),  # [64, 52, 52] -> [64, 50, 50]\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2),  # [64, 50, 50] -> [64, 25, 25]\n            #nn.Dropout(0.3),\n\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),  # [64, 25, 25] -> [128, 24, 24]\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2),  # [128, 24, 24] -> [128, 12, 12]\n            #nn.Dropout(0.3),\n\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=0),  # [128, 12, 12] -> [256, 10, 10]\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2), # [256, 10, 10] -> [256, 5, 5]\n            #nn.Dropout(0.3)\n            \n        )\n\n        self.linear_layers = nn.Sequential(\n            nn.Linear(in_features=256*5*5, out_features=128),  # [1, 6400] -> [128]\n            nn.ReLU(inplace=True),\n            nn.Linear(in_features=128, out_features=20)  # [128] -> [20]\n        )\n\n\n    def forward(self, x):\n\n        #x = nn.functional.interpolate(x, 56)  # [3, 224, 224] -> [3, 56, 56]\n\n        x = self.cnn_layers(x)  # [3, 56, 56] -> [256, 3, 3]\n        x = x.view(x.size(0), -1)  # [256, 3, 3] -> [2304]\n        x = self.linear_layers(x)  # [2304] -> [3]\n        return x\n###","a8c46f6e":"model = SimpleCNN()\nmodel.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n#optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n\ncriterion = torch.nn.CrossEntropyLoss()\ncriterion.to(device);","2bafe3b3":"def train_epoch(dloader, model):\n\n    model.train()\n\n    train_epoch_loss = .0\n    train_epoch_corrects = 0\n\n    for img, label in dloader:\n\n        img, label = img.to(device), label.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(img)\n        pred = torch.max(output, 1)[1]\n\n        loss = criterion(output, label)\n\n        loss.backward()\n        optimizer.step()\n\n        train_epoch_loss += loss.item()\n        train_epoch_corrects += torch.sum(pred == label.data)\n\n    return train_epoch_loss, (train_epoch_corrects \/ len(train_dataset))","742c821c":"def valid_epoch(dloader, model):\n\n    model.eval()\n\n    valid_epoch_loss = .0\n    valid_epoch_corrects = 0\n    total = 0\n\n    for img, label in dloader:\n\n        img, label = img.to(device), label.to(device)\n\n        with torch.no_grad():\n\n            output = model(img)\n            pred = torch.max(output, 1)[1]\n\n            loss = criterion(output, label)\n\n            valid_epoch_loss += loss.item()\n            valid_epoch_corrects += torch.sum(pred == label.data)\n            total += len(label.data)\n\n    return valid_epoch_loss, (valid_epoch_corrects \/ total)","98817d5c":"def train(model, criterion, optimizer, num_epochs=3):\n\n    hist = {\n        'train_loss_hist': [],\n        'train_acc_hist': [],\n        'valid_loss_hist': [],\n        'valid_acc_hist': []\n    }\n\n    for epoch in tqdm(range(num_epochs)):\n\n        train_epoch_loss, train_epoch_acc = train_epoch(train_dataloader, model)\n        hist['train_loss_hist'].append(train_epoch_loss)\n        hist['train_acc_hist'].append(train_epoch_acc)\n\n        valid_epoch_loss, valid_epoch_acc = valid_epoch(train_dataloader, model)\n        hist['valid_loss_hist'].append(valid_epoch_loss)\n        hist['valid_acc_hist'].append(valid_epoch_acc)\n\n    return hist","a8604a80":"hist = train(model, criterion, optimizer, num_epochs=10)","baecfb90":"fig = plt.figure(figsize=(12, 8))\nplt.title('Loss')\nplt.plot(list(range(len(hist['train_loss_hist']))), hist['train_loss_hist'], label='Train')\nplt.plot(list(range(len(hist['valid_loss_hist']))), hist['valid_loss_hist'], label='Test')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()","4bad2091":"fig = plt.figure(figsize=(12, 8))\nplt.title('Accuracy')\nplt.plot(list(range(len(hist['train_acc_hist']))), hist['train_acc_hist'], label='Train')\nplt.plot(list(range(len(hist['valid_acc_hist']))), hist['valid_acc_hist'], label='Test')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()","c7472283":"predictions = []\nfor img in test_dataloader:\n    predictions.extend(list(model(img.to(device)).argmax(axis=1).cpu().numpy()))","6be8b474":"test_data['label'] = predictions\ntest_data.to_csv('sumbission_CNN.csv', index=False)","753c13b7":"resnet18 = models.resnet18(pretrained=True, progress=True)\nnum_linear_features = resnet18.fc.in_features\nresnet18.fc = nn.Linear(num_linear_features, 20)\n\nresnet18.to(device)\n\noptimizer = torch.optim.SGD(resnet18.parameters(), lr=1e-4, momentum=0.9)\n\ncriterion = torch.nn.CrossEntropyLoss()\ncriterion.to(device);","f17c4a30":"class ResNet34(nn.Module):\n    def __init__(self):\n        super(ResNet34, self).__init__()\n        self.model = models.resnet34(pretrained=True)\n        self.model.fc = nn.Linear(in_features=512, out_features=20, bias=True)\n\n    def forward(self, image):\n        features = self.model(image)\n        return features\n\nresnet34 = ResNet34().cuda()","7592ae6e":"resnet18.to(device)\n\noptimizer = torch.optim.SGD(resnet18.parameters(), lr=1e-4, momentum=0.9)\n\ncriterion = torch.nn.CrossEntropyLoss()\ncriterion.to(device);","00bd942b":"resnet18_hist = train(resnet18, criterion, optimizer, num_epochs=10)","0ba9ae55":"fig = plt.figure(figsize=(12, 8))\nplt.title('Loss')\nplt.plot(list(range(len(resnet18_hist['train_loss_hist']))), resnet18_hist['train_loss_hist'], label='Train')\nplt.plot(list(range(len(resnet18_hist['valid_loss_hist']))), resnet18_hist['valid_loss_hist'], label='Test')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()","6dec4572":"fig = plt.figure(figsize=(12, 8))\nplt.title('Accuracy')\nplt.plot(list(range(len(resnet18_hist['train_acc_hist']))), resnet18_hist['train_acc_hist'], label='Train')\nplt.plot(list(range(len(resnet18_hist['valid_acc_hist']))), resnet18_hist['valid_acc_hist'], label='Test')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()","4907363a":"predictions_resnet = []\nfor img in test_dataloader:\n    predictions_resnet.extend(list(resnet18(img.to(device)).argmax(axis=1).cpu().numpy()))","13d59b24":"test_data['label'] = predictions_resnet\ntest_data.to_csv('sumbission_resnet.csv', index=False)","88b2220a":"test_data['label'].value_counts()","941a36d1":"class Ynet(nn.Module):\n    def __init__(self):\n        super(Ynet, self).__init__()\n        \n        self.cnn_layers = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=0),  # [3, 224, 224] -> [16, 220, 220]\n            nn.BatchNorm2d(16),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2),  # [16, 220, 220] -> [16, 110, 110]\n\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0),  # [16, 110, 110] -> [32, 106, 106]\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2),  # [32, 106, 106] -> [32, 53, 53]\n\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=1, padding=0),  # [32, 53, 53] -> [64, 50, 50]\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2),  # [64, 50, 50] -> [64, 25, 25]\n            \n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2, stride=1, padding=0),  # [64, 25, 25] -> [128, 24, 24]\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2),  # [128, 24, 24] -> [128, 12, 12]\n            )\n        \n        self.linear_layers = nn.Sequential(\n            nn.Linear(in_features=128*12*12, out_features=1024),  # [1, 18432] -> [1024]\n            nn.ReLU(inplace=True),\n            nn.Linear(in_features=1024, out_features=128),  # [1, 18432] -> [128]\n            nn.ReLU(inplace=True),\n            nn.Linear(in_features=128, out_features=20)  # [128] -> [20]\n        )\n        \n    def forward(self, x):\n\n        x = self.cnn_layers(x)  # [3, 224, 224] -> [128, 12, 12]\n        x = x.view(x.size(0), -1)  # [128, 12, 12] -> [18432]\n        x = self.linear_layers(x)  # [18432] -> [20]\n        return x\n","c92097e5":"model_Ynet = Ynet()\nmodel_Ynet.to(device)\n\noptimizer = torch.optim.Adam(model_Ynet.parameters(), lr=1e-4)\n\ncriterion = torch.nn.CrossEntropyLoss()\ncriterion.to(device);","688adfe8":"hist_Ynet = train(model_Ynet, criterion, optimizer, num_epochs=10)","72e5be55":"fig = plt.figure(figsize=(12, 8))\nplt.title('Loss')\nplt.plot(list(range(len(hist_Ynet['train_loss_hist']))), hist_Ynet['train_loss_hist'], label='Train')\nplt.plot(list(range(len(hist_Ynet['valid_loss_hist']))), hist_Ynet['valid_loss_hist'], label='Test')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()","7d2f956a":"fig = plt.figure(figsize=(12, 8))\nplt.title('Accuracy')\nplt.plot(list(range(len(hist_Ynet['train_acc_hist']))), hist_Ynet['train_acc_hist'], label='Train')\nplt.plot(list(range(len(hist_Ynet['valid_acc_hist']))), hist_Ynet['valid_acc_hist'], label='Test')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()","5cbd5011":"predictions_Ynet = []\nfor img in test_dataloader:\n    predictions_Ynet.extend(list(model_Ynet(img.to(device)).argmax(axis=1).cpu().numpy()))","95891ccc":"test_data['label'] = predictions_Ynet\ntest_data.to_csv('sumbission_Ynet.csv', index=False)","17a15eba":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\n\ntest_data['label'].value_counts()","d5e68ca6":"len(predictions_Ynet_small)","8812144a":"test_data['label'] = [2]*1592\ntest_data.to_csv('sumbission_part3.csv', index=False)","4e06d0f9":"class Ynet_small(nn.Module):\n    def __init__(self):\n        super(Ynet_small, self).__init__()\n\n    # (batch size ommited) [channels, H, W]\n\n        self.cnn_layers = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=0),  # [3, 56, 56] -> [16, 52, 52]\n            nn.BatchNorm2d(16),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2),  # [16, 52, 52] -> [16, 26, 26]\n\n            nn.Conv2d(in_channels=16, out_channels=64, kernel_size=3, stride=1, padding=0),  # [16, 26, 26] -> [64, 24, 24]\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2),  # [64, 24, 24] -> [64, 12, 12]\n        )\n\n        self.linear_layers = nn.Sequential(\n            nn.Linear(in_features=64*12*12, out_features=128),  # [1, 9216] -> [128]\n            nn.ReLU(inplace=True),\n            nn.Linear(in_features=128, out_features=20)  # [128] -> [20]\n        )\n\n    def forward(self, x):\n        x = nn.functional.interpolate(x, 56)  # [3, 224, 224] -> [3, 56, 56]\n        x = self.cnn_layers(x)  # [3, 56, 56] -> [64, 12, 12]\n        x = x.view(x.size(0), -1)  # [256, 3, 3] -> [9216]\n        x = self.linear_layers(x)  # [9216] -> [20]\n        return x","4a56d5c1":"model_Ynet_small = Ynet_small()\nmodel_Ynet_small.to(device)\n\noptimizer = torch.optim.Adam(model_Ynet_small.parameters(), lr=1e-4)\n\ncriterion = torch.nn.CrossEntropyLoss()\ncriterion.to(device);","d2261741":"hist_Ynet_small = train(model_Ynet_small, criterion, optimizer, num_epochs=10)","5c4f99ca":"fig = plt.figure(figsize=(12, 8))\nplt.title('Loss')\nplt.plot(list(range(len(hist_Ynet_small['train_loss_hist']))), hist_Ynet_small['train_loss_hist'], label='Train')\nplt.plot(list(range(len(hist_Ynet_small['valid_loss_hist']))), hist_Ynet_small['valid_loss_hist'], label='Test')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()","24a408a3":"fig = plt.figure(figsize=(12, 8))\nplt.title('Accuracy')\nplt.plot(list(range(len(hist_Ynet_small['train_acc_hist']))), hist_Ynet['train_acc_hist'], label='Train')\nplt.plot(list(range(len(hist_Ynet_small['valid_acc_hist']))), hist_Ynet['valid_acc_hist'], label='Test')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()","43be6bba":"predictions_Ynet_small = []\nfor img in test_dataloader:\n    predictions_Ynet_small.extend(list(model_Ynet_small(img.to(device)).argmax(axis=1).cpu().numpy()))","177a8ce7":"test_data['label'] = predictions_Ynet_small\ntest_data.to_csv('sumbission_Ynet_small.csv', index=False)","4e54944b":"## Predictions","4e642058":"This submission achieves 0.5 accuracy, while it is clearly a bad prediction. This shows that accuracy is not always a good metric to measure model performance.\nThat's why roc auc and f-1 scores need to be used as well","7b4990fc":"## Why the value of loss function on the validation set can be lower than on the training set?\n\n1.\tThe training loss might be higher because we use dropout technique. We artificially make it harder for the network to work with the train data. During validation, however, all the units are available. Thus, the prediction is more accurate.\n2.\tData in the validation set may be just simpler by a coincidence\n\n","bf54d0fd":"# Part1","2de93281":"![%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.png](attachment:%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.png)","14d64e26":"# Part 4","c08eef50":"As one can see from the table above, there is no need in using dropout technique. However, still validation loss < train loss.   \nAs one can also see from the plots, there is no need to increase the number of epochs as loss curves are more or less linear at the end of the plot.\n","a2102783":"# Part 5*","5887db1f":"Min train loss = 2.23  \nMin valid loss = 0.94  \nLoss score is even better now, but still validation loss < train loss\n","c1dec53d":"### Part1 results discussion\n\nWe have tried changing the model from SimpleCNN in the seminar and achieved 0.9993 accuracy on public test set\nChanges made:\n\n- Use whole images for training 3x224x224 without interpolation\n- Dropout and Maxpooling layers added\n\nThe best performance was shown by pretrained resnet18 model that achieved 1.0 accuracy on public test set\n\n\nParameters that we tried to tune:\n- Changing optimizers: SGD, Adam\n- Changing\/adding Maxpooling and Dropout\n\n","2f9cc706":"### 1 Let us vary droput rate:\n![%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.png](attachment:%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.png)","bee552d4":"## Using changed version of SimpleCNN","d0ffd344":"# Group 1\n\nJerry Atorigo, Ilya Veretennikov, Yerzhan Imanmalik","787dd6d3":"### Part4 discussion\n\nSo we managed to decrease the number of layers in our Ynet. And the final version consist of:\n\n- Interpolation from 3x224x224 to 3x56x56\n- Two conv2d layers\n- Two linear layers\n\nAnd is able to achieve 1.0 accuracy on the public test set if all the calculations were correct.\n\nThat is, the task is fully solved","bcc69feb":"# Part 2","91a5fb87":"### Let us try other hyperparameters:\nLr = 1e-2 instead of 1e-4:  \nmin train loss: 196.46  \nmin valid loss: 182.97  \nvery-very bad\u2026  \n\n","83a60bb4":"# EDA","5b4c9331":"### Ynet results discussion\n\nYnet achieved 1.0 accuracy on public test set. It is the model with 4 Convolutional and 3 Linear layers.\nImplemented with:\n- Batchnorm on each layer\n- Maxpooling on each layer\n- No dropout\n\nSo we observe that the model without dropout (in Part2) performs better than CNN with dropout (in Part1). It might be that we are largely overfitting to the train data, but the public test set comes from the same distribution and that's why the score remains so high","251f917b":"## Preprocessing","af96dc85":"From the predictions that received close to 1.0 accuracy on public test set we can see that the most occuring label is '2'. Thus, let us try to make a submission with all of the labels equal 2:","c2a39728":"# Part3\n","6b1b05da":"### 2.\tTo check if (2) is true and validation set is just simpler, let\u2019s shuffle the data\n![%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.png](attachment:%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5.png) \n^train_data here is train and validation data  ","0e54634a":"Building our own model. Let's call it Ynet","05909de0":"## Pre-trained models"}}