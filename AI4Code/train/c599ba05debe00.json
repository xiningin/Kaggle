{"cell_type":{"e5669611":"code","5db28038":"code","6cc748b4":"code","a97d5642":"code","109068af":"code","69f16ad1":"code","1e9de3f5":"code","d4ea9b8c":"code","2b0c10d7":"code","7d3402d7":"code","463de660":"code","b346c057":"code","33d16b5e":"code","ea67f534":"code","9489924f":"code","77178f04":"code","0c109ee2":"code","c68cc957":"code","089b41ab":"code","16c0f826":"code","b188cb46":"code","9a126237":"code","d0af5b98":"code","7a1691a3":"code","da3b03f7":"code","eeb710d2":"code","fc56799c":"code","88e08352":"code","58c8b348":"code","8489fafa":"code","92666f3e":"code","ceed6008":"code","cfba5800":"code","e219504e":"code","e773d3a9":"code","49dad89e":"code","87669963":"code","fe9f009d":"code","55ed8559":"code","36890cd8":"code","111ebe8e":"code","17025094":"code","d587877e":"code","74b15501":"code","607fbe73":"code","fafe175e":"code","0216661f":"code","2dabc4ab":"code","476b622b":"code","dca31574":"code","1e7d1ec3":"code","558d8ffe":"code","0588b138":"code","356cec97":"code","2ee4b21c":"code","672437ee":"code","4a8ee70b":"markdown","3133cdc1":"markdown","4491ee20":"markdown","041a98c0":"markdown","165c9f3e":"markdown","23f9f43d":"markdown","b9cfa385":"markdown","66fd5bed":"markdown","e88e588f":"markdown","9310d564":"markdown","c2d47d55":"markdown","a656c086":"markdown","295d881e":"markdown","db7591e6":"markdown","a22461c0":"markdown","809ead3b":"markdown","64faa36e":"markdown","3c40fe00":"markdown","c2778c71":"markdown","399803db":"markdown","2af43f38":"markdown","12b53a5b":"markdown"},"source":{"e5669611":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom fastai.vision import *","5db28038":"TRAIN = Path('..\/input\/the-nature-conservancy-fisheries-monitoring\/train\/')\nTEST = Path('..\/input\/test-stg2\/test_stg2\/')\nPATH = Path('..\/input\/the-nature-conservancy-fisheries-monitoring\/')\n\nPATH_STR = '..\/input\/the-nature-conservancy-fisheries-monitoring\/train\/'\nPATH_WORKING = Path('\/kaggle\/working\/')","6cc748b4":"os.listdir('..\/input\/samplestg2\/')","a97d5642":"TRAIN.ls()","109068af":"#TEST.ls()","69f16ad1":"sample_sub1 = pd.read_csv(PATH\/'sample_submission_stg1.csv')\nsample_sub2 = pd.read_csv('..\/input\/samplestg2\/sample_submission_stg2.csv')","1e9de3f5":"for idx,name in enumerate(os.listdir(TRAIN)):\n    print(idx,name)","d4ea9b8c":"num_files = [(name,len(os.listdir(TRAIN\/name))) for idx,name in enumerate(os.listdir(TRAIN)) if name != \".DS_Store\"]","2b0c10d7":"files_df = pd.DataFrame(num_files,columns=['folder','count'])","7d3402d7":"files_df","463de660":"files_df['count'].sum()","b346c057":"sns.barplot(x='folder',y='count',data=files_df)","33d16b5e":"os.listdir(TRAIN\/'SHARK')[0]","ea67f534":"from PIL import Image\nfrom glob import glob\ntrain_files = [(name,os.listdir(TRAIN\/'{}'.format(name))) for idx,name in enumerate(os.listdir(TRAIN)) if name != \".DS_Store\"]","9489924f":"# Define function to get image size\ndef get_sizes(files):\n    '''Pass a list of file names and return image sizes'''\n    sizes = []\n    for folder in files:\n        n = folder[0]\n        f = folder[1]\n        for files in f:\n            image_file = PATH_STR+n+'\/'+files\n            with Image.open(image_file) as im:                \n                sizes.append(im.size) # return value is a tuple, ex.: (1200, 800)\n    return sizes","77178f04":"img_sizes = ['_'.join(map(str, s)) for s in get_sizes(train_files)]","0c109ee2":"sizes_df = pd.DataFrame(img_sizes,columns=['sizes'])\nsizes_df.head()","c68cc957":"c = sizes_df.reset_index().groupby('sizes')['index'].count().reset_index()\nc","089b41ab":"fig = plt.figure(figsize=(12,12))\nax = sns.barplot(x='sizes',y='index',data=c)\nax.set(xlabel='Sizes', ylabel='Count')","16c0f826":"sample_sharks = [PATH_STR+'SHARK'+'\/'+files for files in train_files[0][1][0:10]]","b188cb46":"src = ImageList.from_folder(path=TRAIN).split_by_rand_pct(0.2).label_from_folder().add_test_folder(TEST)","9a126237":"tfms = get_transforms()","d0af5b98":"data = src.transform(tfms,size=(670,1192)).databunch(bs=8).normalize(imagenet_stats)","7a1691a3":"print(data.classes,data.c)","da3b03f7":"data.show_batch(rows=3)","eeb710d2":"learn = cnn_learner(data, models.resnet50, metrics=[accuracy],model_dir='\/kaggle\/working\/')","fc56799c":"learn.lr_find()","88e08352":"learn.recorder.plot(suggestion=True)","58c8b348":"learn.fit_one_cycle(5,3e-3)","8489fafa":"learn.recorder.plot_losses()","92666f3e":"learn.save('stage1-fisheries')","ceed6008":"learn.unfreeze()","cfba5800":"learn.lr_find()","e219504e":"learn.recorder.plot()","e773d3a9":"learn.fit_one_cycle(5,slice(3e-5,3e-4))","49dad89e":"learn.save('stage2-fisheries')","87669963":"learn.export('\/kaggle\/working\/export.pkl')","fe9f009d":"sample_sub1.shape","55ed8559":"interp = ClassificationInterpretation.from_learner(learn)","36890cd8":"interp.plot_confusion_matrix()","111ebe8e":"interp.plot_top_losses(9, figsize=(7, 7))\n# Maybe feed the network with top losses to improve score?","17025094":"WORKING_PATH = Path('\/kaggle\/working')","d587877e":"testset = ImageList.from_folder(TEST).add(ImageList.from_folder(PATH\/'test_stg1'))","74b15501":"learn_preds = load_learner(WORKING_PATH, test=testset)","607fbe73":"preds, _ = learn_preds.get_preds(ds_type=DatasetType.Test)","fafe175e":"fish_classes = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']","0216661f":"probabilities = preds[0].tolist()\n[f\"{index}: {probabilities[index]}\" for index in range(len(probabilities))]\nfor idx,name in enumerate(fish_classes):\n    print('{} : {}'.format(name,probabilities[idx]))\n    ","2dabc4ab":"preds_df = pd.DataFrame(preds.numpy(),columns=fish_classes)","476b622b":"preds_df.head()\npreds_df.shape","dca31574":"sample_sub2.head()","1e7d1ec3":"# remove file extension from filename\nImageId = list(sample_sub2['image'])","558d8ffe":"preds_df['image'] = ImageId","0588b138":"preds_df.head()","356cec97":"cols = list(preds_df.columns.values)\ncols","2ee4b21c":"preds_df = preds_df[['image','ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']]\npreds_df.head()","672437ee":"preds_df.to_csv(\"\/kaggle\/working\/submission.csv\", index=False)","4a8ee70b":"### Check number of files per folder","3133cdc1":"We also have very different sizes of images, with the majority being 1280x720. What should we do? Well, on this first try we are going to use a smaller size of 512x512 just to see what happens.","4491ee20":"### Create databunch","041a98c0":"### Fine tunning","165c9f3e":"### Barplot","23f9f43d":"### Take a look at some pictures - is everything alright?","b9cfa385":"So we have eight folders with images. There folders are the classes we want to predict:\n\n**1. Other (meaning that there are fish present but not in the above categories)**\n\n**2. SHARK (Shark, duh)**\n\n**3. ALB (Albacore tuna)**\n\n**4. LAG (Opah)**\n\n**5. NoF (No Fish)**\n\n**6. DOL (Mahi Mahi)**\n\n**7. YFT (Yellowfin tuna)**\n\n**8. BET (Bigeye tuna)**","66fd5bed":"# Train model\n\nOur files are in different folders so we need to access them. Luckly, fastai has a perfect API to do that called DataBlock. We can scrap each folder on the train set and get it's images, split into validation set and label them using just a few lines of code.","e88e588f":"### Base model","9310d564":"### Load data","c2d47d55":"Ok, so we do have a **HUGE** class imbalance here, with most of the files consisting of Albacore tuna images. This might not be a problem in our first try, but we on the next updates we might want to take a deeper look.","a656c086":"All files are in folders. So let's do some analysis of them\n\n* **How many files per folder? Gives and idea of how long it will take to train**\n\n\n* **How many classes? Maybe there is some class imbalance and we need to train a model robust enough to deal with it with good performance.**\n\n\n* **File sizes? I'm still new to CNN so I can't tell right now how to deal with variable size input. Maybe on the next update.**","295d881e":"For some reason uusin Matthews Correlation ","db7591e6":"### Define transforms","a22461c0":"Confusion matrix doesn't look so bad. We can see that even with huge class imbalance the model did quite a decent job. But we probably are going to do something about it. One strategy could be to randomly change some of the images of less frequent classes, save them and train the model using the modified images. \n\nEx:\n\nimage_mod = transformation(image_original)\n","809ead3b":"# Make predictions and submit","64faa36e":"# Load data","3c40fe00":"# Evaluation","c2778c71":"### Check image size","399803db":"# Exploratory data analysis","2af43f38":"# Define paths","12b53a5b":"The images have a lot of noise, such as water droplets on the camera lens, some were taken at night, fishes inside boxes etc. How can we tackle this? I don't know yet :) but maybe we could tweak the channels to highlight the most important thing: Fishes.\n\nWe could also do some image augmentation to zoom in and crop."}}