{"cell_type":{"96eaea50":"code","689cb763":"code","7a9c2c9a":"code","3bbc2ea7":"code","8d4c0b7f":"code","8e41486d":"code","45426c75":"code","dff739f2":"code","dd9b7ab4":"code","72b9eaeb":"code","84f31251":"code","150c6438":"code","4c26f4f5":"code","1e6bc886":"code","35a7654e":"code","7b361036":"code","85a3d05a":"code","adfbd094":"code","5c9e3e24":"code","a571191a":"code","75722c9e":"code","65d20226":"code","5277c2f2":"code","b558589c":"code","4b304b77":"code","da68009a":"code","b2deaa40":"code","b2c916fa":"code","e719537a":"code","f62a954d":"markdown","81f55a87":"markdown","12efa64c":"markdown","c4f2cc57":"markdown","5bf22735":"markdown","cac877ad":"markdown","b04427d3":"markdown","ec2c5157":"markdown","74934eae":"markdown","3a3b2117":"markdown","6037a351":"markdown","747051da":"markdown","0bb5440a":"markdown","a1e8596c":"markdown","56486db6":"markdown","cbd79f2c":"markdown","c9297ca0":"markdown","87f3f942":"markdown","eac2c030":"markdown","2bc49910":"markdown","0c1bd5a0":"markdown","a7e94a51":"markdown","03a694fc":"markdown","21fd88be":"markdown","f23f1dcd":"markdown","6212dc68":"markdown","20cb816c":"markdown","49004253":"markdown"},"source":{"96eaea50":"!pip install nb_black -q","689cb763":"%load_ext nb_black","7a9c2c9a":"import numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport os\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\n\ndata = pd.read_csv(\"\/kaggle\/input\/churn-modeling-dataset\/Churn_Modelling.csv\").drop(\n    [\"RowNumber\", \"CustomerId\", \"Surname\"], axis=1\n)\ndata.head()","3bbc2ea7":"# To plot numerical column\ndef plot_hist(data, column):\n    fig = px.histogram(data, x=column, color=\"Exited\")\n    fig.show()\n    fig = ff.create_table(pd.DataFrame(data[column].describe()).T)\n    fig.show()\n\n\n# To plot categorical column\ndef plot_count(data, column):\n    df = data.groupby(column)[\"Exited\"].value_counts()\n    df = pd.DataFrame(df)\n    df.columns = [\"Count\"]\n    df.reset_index(inplace=True)\n    fig = px.bar(df, x=column, y=\"Count\", color=\"Exited\", text=\"Count\", barmode=\"group\")\n    fig.show()","8d4c0b7f":"data.info()","8e41486d":"plot_hist(data, \"CreditScore\")","45426c75":"plot_count(data, \"Geography\")","dff739f2":"plot_count(data, \"Gender\")","dd9b7ab4":"plot_hist(data, \"Age\")","72b9eaeb":"plot_hist(data, \"Tenure\")","84f31251":"plot_hist(data, \"Balance\")","150c6438":"plot_hist(data, \"NumOfProducts\")","4c26f4f5":"plot_hist(data, \"HasCrCard\")","1e6bc886":"plot_hist(data, \"IsActiveMember\")","35a7654e":"plot_hist(data, \"EstimatedSalary\")","7b361036":"from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n\nenc = OneHotEncoder(handle_unknown=\"ignore\")\nstander_scaler = StandardScaler()\nlabel_encoder = LabelEncoder()\n\nX = np.concatenate(\n    (\n        ## OneHotEncoder\n        enc.fit_transform(data[[\"Geography\"]]).toarray(),\n        ## Stander Scaler\n        stander_scaler.fit_transform(\n            data[\n                [\n                    \"CreditScore\",\n                    \"Age\",\n                    \"Tenure\",\n                    \"Balance\",\n                    \"NumOfProducts\",\n                    \"EstimatedSalary\",\n                ]\n            ]\n        ),\n        ## LabelEncoder\n        label_encoder.fit_transform(data[[\"Gender\"]]).reshape(-1, 1),\n        ## No formatation\n        data[[\"HasCrCard\", \"IsActiveMember\"]].values,\n    ),\n    axis=1,\n)\n\ny = data.Exited.values\nX.shape","85a3d05a":"columns = (\n    [el for el in enc.categories_[0]]\n    + [\"CreditScore\", \"Age\", \"Tenure\", \"Balance\", \"NumOfProducts\", \"EstimatedSalary\",]\n    + [\"Gender\"]\n    + [\"HasCrCard\", \"IsActiveMember\"]\n    + [\"Exited\"]\n)","adfbd094":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ntable = pd.DataFrame(np.concatenate([X, y.reshape(-1, 1)], axis=1))\ntable.columns = columns\ntable = table.corr()\nwith sns.axes_style(\"white\"):\n    mask = np.zeros_like(table)\n    mask[np.triu_indices_from(mask)] = True\n    plt.figure(figsize=(10, 10))\n    sns.heatmap(\n        round(table, 2),\n        cmap=\"Reds\",\n        mask=mask,\n        vmax=table.max().max(),\n        vmin=table.min().min(),\n        linewidths=0.5,\n        annot=True,\n        annot_kws={\"size\": 12},\n    ).set_title(\"Correlation Matrix App behavior dataset\")","5c9e3e24":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Conv2D\nimport keras\n\n\ndef get_model():\n    return Sequential(\n        [\n            Dense(units=200, input_dim=12, activation=\"relu\"),\n            Dense(150, activation=\"relu\"),\n            Dropout(0.2),\n            Dense(100, activation=\"relu\"),\n            Dense(100, activation=\"relu\"),\n            Dropout(0.2),\n            Dense(100, activation=\"relu\"),\n            Dense(100, activation=\"relu\"),\n            Dense(100, activation=\"relu\"),\n            Dropout(0.2),\n            Dense(100, activation=\"relu\"),\n            Dense(1, activation=\"sigmoid\"),\n        ]\n    )\n\n\ndef train_ann(X, y):\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.1, random_state=42\n    )\n\n    model = get_model()\n\n    model.compile(\n        optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"mse\", \"accuracy\"],\n    )\n\n    # Trainig and returning back the results.\n    history = model.fit(\n        X_train,\n        y_train,\n        batch_size=10,\n        epochs=50,\n        verbose=0,\n        validation_data=(X_test, y_test),\n    )\n    loss, mse, acc = model.evaluate(X_test, y_test, verbose=0)\n    fig = ff.create_table(\n        pd.DataFrame([(loss, mse, acc)], columns=[\"Loss\", \"MSE\", \"Accuracy\"]),\n    )\n    fig.show()","a571191a":"from imblearn.over_sampling import SMOTE\n\nX, y = SMOTE(random_state=42).fit_resample(X, y)\n\ntrain_ann(X, y)","75722c9e":"from imblearn.over_sampling import RandomOverSampler\n\nX, y = RandomOverSampler(random_state=42).fit_resample(X, y)\n\ntrain_ann(X, y)","65d20226":"from imblearn.over_sampling import BorderlineSMOTE\n\nX, y = BorderlineSMOTE(random_state=42).fit_resample(X, y)\n\ntrain_ann(X, y)","5277c2f2":"from imblearn.over_sampling import ADASYN\n\nX, y = ADASYN(random_state=42).fit_resample(X, y)\n\ntrain_ann(X, y)","b558589c":"from imblearn.over_sampling import KMeansSMOTE\n\nX, y = KMeansSMOTE(random_state=42).fit_resample(X, y)\n\ntrain_ann(X, y)","4b304b77":"from imblearn.over_sampling import SVMSMOTE\n\nX, y = SVMSMOTE(random_state=42).fit_resample(X, y)\n\ntrain_ann(X, y)","da68009a":"from imblearn.under_sampling import ClusterCentroids\n\nX, y = ClusterCentroids(random_state=42).fit_resample(X, y)\n\ntrain_ann(X, y)","b2deaa40":"from imblearn.under_sampling import AllKNN\n\nX, y = AllKNN().fit_resample(X, y)\n\ntrain_ann(X, y)","b2c916fa":"from imblearn.under_sampling import NeighbourhoodCleaningRule\n\nX, y = NeighbourhoodCleaningRule().fit_resample(X, y)\n\ntrain_ann(X, y)","e719537a":"from imblearn.under_sampling import RandomUnderSampler\n\nX, y = RandomUnderSampler().fit_resample(X, y)\n\ntrain_ann(X, y)","f62a954d":"# EAD\n\n### CreditScore","81f55a87":"# Data porfile\n\n- There is 10 columns;\n- No missing values;\n- Exited column is the target;\n\n### Columns meaning\n- CreditScore: Customer score in financial context;\n- Geography: Represets the customer contry;\n- Gender: Just customer's sex;\n- Age: Just Age;\n- Tenure: How much time as customer;\n- Balance: How much money in the bank;\n- NumOfProducts: How much products the customer uses;\n- HasCrCard: Does have the customer a credit card?\n- IsActiveMember: Is the customer an active member?\n- EstimetedSalary: How much is the customer salary?\n- Exited: Client churn flag\n","12efa64c":"## Tenure","c4f2cc57":"## Balance          ","5bf22735":"## Gender           ","cac877ad":"# Over-sampling\n\n## SMOTE\nClass to perform over-sampling using SMOTE.\n\nThis object is an implementation of SMOTE - Synthetic Minority Over-sampling Technique as presented in [R001eabbe5dd7-1](https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.over_sampling.SMOTE.html#r001eabbe5dd7-1).\n\nRead more in the User Guide.\n\n","b04427d3":"## KMeansSMOTE\nApply a KMeans clustering before to over-sample using SMOTE.\n\nThis is an implementation of the algorithm described in [Rea5937a049dc-1](https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.over_sampling.KMeansSMOTE.html#rea5937a049dc-1).\n\nRead more in the User Guide.","ec2c5157":"# Importing dataset and mini-EAD","74934eae":"## NeighbourhoodCleaningRule\n\nClass performing under-sampling based on the neighbourhood cleaning rule.\n\nRead more in the [User Guide](https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/under_sampling.html#condensed-nearest-neighbors).\n","3a3b2117":"## RandomOverSampler\n\nClass to perform random over-sampling.\n\nObject to over-sample the minority class(es) by picking samples at random with replacement.\n\nRead more in the [User Guide](https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/over_sampling.html#random-over-sampler).","6037a351":"# Data formatation\n\n- StandardScaler -> Standardize features by removing the mean and scaling to unit variance The standard score of a sample x is calculated as: z = (x - u) \/ s.\n\n- LabelEncoder -> Encode target labels with value between 0 and n_classes-1.\n\n- OneHotEncoder -> Encode categorical features as a one-hot numeric array.\n\n","747051da":"## BorderlineSMOTE\n\nOver-sampling using Borderline SMOTE.\n\nThis algorithm is a variant of the original SMOTE algorithm proposed in [R63962efaf197-2](https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.over_sampling.BorderlineSMOTE.html#r63962efaf197-2). Borderline samples will be detected and used to generate new synthetic samples.","0bb5440a":"## Correlation Matrix","a1e8596c":"# Train ANN","56486db6":"Using the nb_black formatter.","cbd79f2c":"## SVMSMOTE\nOver-sampling using SVM-SMOTE.\n\nVariant of SMOTE algorithm which use an SVM algorithm to detect sample to use for generating new synthetic samples as proposed in [R88acb9955f91-2](https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.over_sampling.SVMSMOTE.html#r88acb9955f91-2).\n\nRead more in the [User Guide](https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/over_sampling.html#smote-adasyn).","c9297ca0":"## EstimatedSalary","87f3f942":"## HasCrCard","eac2c030":"## NumOfProducts","2bc49910":"## Geography","0c1bd5a0":"## IsActiveMember","a7e94a51":"Using the [dataset](https:\/\/www.kaggle.com\/shivan118\/churn-modeling-dataset) I'm going to use approaches to predict data that come from a unbalance dataset.","03a694fc":"## ADASYN\nPerform over-sampling using Adaptive Synthetic (ADASYN) sampling approach for imbalanced datasets.\n\nRead more in the [User Guide](https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/over_sampling.html#smote-adasyn).","21fd88be":"## RandomUnderSampler\nClass to perform random under-sampling.\n\nUnder-sample the majority class(es) by randomly picking samples with or without replacement.\n\nRead more in the [User Guide](https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/under_sampling.html#controlled-under-sampling).","f23f1dcd":"## AllKNN\n\nClass to perform under-sampling based on the AllKNN method.\n","6212dc68":"## Age","20cb816c":"Geting the name of our new columns after transformed...","49004253":"# Under-Sampling\n\n## ClusterCentroids\nPerform under-sampling by generating centroids based on clustering methods.\n\nMethod that under samples the majority class by replacing a cluster of majority samples by the cluster centroid of a KMeans algorithm. This algorithm keeps N majority samples by fitting the KMeans algorithm with N cluster to the majority class and using the coordinates of the N cluster centroids as the new majority samples."}}