{"cell_type":{"d220832f":"code","e9a9183a":"code","b9909242":"code","ac9fd7a5":"code","32420b54":"code","de87cbe2":"code","56893431":"code","109ce4c7":"code","349dd591":"code","baa308aa":"code","bf22415b":"code","0d746ea8":"code","9d7428e3":"code","8f5b9c4d":"code","41823fe4":"code","102ad436":"code","26f4e57b":"code","933b630d":"code","93e63bb5":"code","821bf60e":"code","3c82fda2":"code","d2752fc6":"code","6f998398":"code","6f816929":"code","c3a59809":"code","5f71b70b":"code","73581a34":"code","6bd0213b":"code","6f9be64e":"code","57430cdd":"code","7d840dc6":"code","63648e6a":"code","be78e01f":"code","6dcc8d9c":"code","822cdb0f":"code","3b90ef7d":"code","3c10ee34":"code","0d133d8d":"code","5872fc35":"code","0d66223c":"code","73415c64":"code","0a4faee8":"code","02d30b54":"code","d7c0e43c":"code","a5c34998":"code","3799525e":"code","d993106e":"code","1aa971c9":"code","2b4e2b46":"code","94228b2f":"code","bdadb293":"code","55db038b":"code","38fcfce5":"code","69240b57":"code","82e6a47c":"code","0671b53e":"code","c38ac856":"code","d0a80ee0":"code","d77e181c":"code","cbb6e615":"code","7786c4e1":"code","289213d2":"code","ecf4d798":"code","2da25867":"code","c4a70ce8":"code","c725ff5e":"code","c6f6a82d":"code","7cbb18b6":"code","f6bb0351":"code","ef1d7c68":"code","9c18071c":"code","2ec75f8c":"code","c153a98e":"code","b89d28d1":"code","a9889706":"code","cca5edc3":"code","3ab625b9":"code","48f0cae4":"code","862ecc04":"code","cdf209b7":"code","d3ce2f48":"code","5e6f6049":"code","d149dd65":"code","46430977":"code","4428266f":"code","2f87b848":"code","552cd686":"code","f1c2dc64":"code","20c3b43f":"code","bf8e18c1":"code","3a3ca636":"code","c428c63b":"code","43203dea":"code","c7568a89":"code","3cc2b8d5":"code","e07a13d0":"code","1cc92f35":"code","d88cb71d":"code","12459ffd":"markdown","1e19b21d":"markdown","608ed0fc":"markdown"},"source":{"d220832f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\nfrom sklearn import preprocessing","e9a9183a":"pd.set_option('display.max_columns',500)","b9909242":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix","ac9fd7a5":"\ndata1 = pd.read_csv(\"..\/input\/adult-census-income\/adult.csv\")\nx=data1.drop('income',axis=1)\ny=data1['income']","32420b54":"x,y = train_test_split(data1,random_state=7)","de87cbe2":"x.head()","56893431":"x.groupby('income').size()","109ce4c7":"x.info()","349dd591":"x.describe()","baa308aa":"x.shape","bf22415b":"(x=='?').sum()","0d746ea8":"((x=='?').sum()*100\/32561).round(2)","9d7428e3":"((y=='?').sum()*100\/32561).round(2)","8f5b9c4d":"#data[data[::] != '?']\nx = x[(x['workclass']!='?')& (x['occupation']!='?') & (x['native.country']!='?')]","41823fe4":"#data[data[::] != '?']\ny = y[(y['workclass']!='?')& (y['occupation']!='?') & (y['native.country']!='?')]","102ad436":"(x=='?').sum()","26f4e57b":"(y=='?').sum()","933b630d":"x.info()","93e63bb5":"sns.pairplot(x)","821bf60e":"correlation = x.corr()\n# plot correlation matrix\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(correlation, vmin=-1, vmax=1)\nfig.colorbar(cax)\n#sns.heatmap(x.select_dtypes([object]), annot=True, annot_kws={\"size\": 7})","3c82fda2":"\nname = ['age','fnlwgt','education.num','capital.gain','capital.loss','hours.per.week']\n\nfor c in name:\n    sns.boxplot(x=x[c],data=x)\n\n    plt.show()\n","d2752fc6":"x.select_dtypes(['object']).head()","6f998398":"x['income'].unique()","6f816929":"x['workclass'].unique()","c3a59809":"x['education'].unique()","5f71b70b":"x['occupation'].unique()","73581a34":"x['sex'].unique()","6bd0213b":"x['workclass'].unique()","6f9be64e":"x['native.country'].unique()","57430cdd":"y['native.country'].unique()","7d840dc6":"y.replace(['South','Hong'],['South korea','Hong kong'],inplace=True)","63648e6a":"x.replace(['South','Hong'],['South korea','Hong kong'],inplace=True)","be78e01f":"x['native.country'].unique()","6dcc8d9c":"x['net_capital']=x['capital.gain']-x['capital.loss']\nx.drop(['capital.gain','capital.loss'],1,inplace=True)","822cdb0f":"y['net_capital']=y['capital.gain']-y['capital.loss']\ny.drop(['capital.gain','capital.loss'],1,inplace=True)","3b90ef7d":"y.head()","3c10ee34":"x.head()","0d133d8d":"\nname = ['age','fnlwgt','education.num','net_capital','hours.per.week']\nfor c in name:\n    sns.distplot(x[c], hist=True, kde=True, \n             bins=int(180\/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\n    plt.show()","5872fc35":"\nname = ['age','fnlwgt','education.num','net_capital','hours.per.week']\nfor c in name:\n    sns.distplot(y[c], hist=True, kde=True, \n             bins=int(180\/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\n    plt.show()","0d66223c":"d = x.loc[:,['age','fnlwgt','education.num','net_capital','hours.per.week']]","73415c64":"d1 = y.loc[:,['age','fnlwgt','education.num','net_capital','hours.per.week']]","0a4faee8":"d.head()","02d30b54":"d1.head()","d7c0e43c":"from sklearn.preprocessing import Normalizer","a5c34998":"pt = preprocessing.QuantileTransformer(output_distribution='normal')\nd=pd.DataFrame(pt.fit_transform(d),columns=['age','fnlwgt','education.num','net_capital','hours.per.week'])\n","3799525e":"pt = preprocessing.QuantileTransformer(output_distribution='normal')\nd1=pd.DataFrame(pt.fit_transform(d1),columns=['age','fnlwgt','education.num','net_capital','hours.per.week'])\n","d993106e":"\nd=pd.DataFrame(Normalizer().fit_transform(d),columns=['age','fnlwgt','education.num','net_capital','hours.per.week'])\n","1aa971c9":"\nd1=pd.DataFrame(Normalizer().fit_transform(d1),columns=['age','fnlwgt','education.num','net_capital','hours.per.week'])\n","2b4e2b46":"pt = preprocessing.QuantileTransformer(output_distribution='normal')\nd=pd.DataFrame(pt.fit_transform(d),columns=['age','fnlwgt','education.num','net_capital','hours.per.week'])\n","94228b2f":"pt = preprocessing.QuantileTransformer(output_distribution='normal')\nd1=pd.DataFrame(pt.fit_transform(d1),columns=['age','fnlwgt','education.num','net_capital','hours.per.week'])\n","bdadb293":"name = ['age','fnlwgt','education.num','net_capital','hours.per.week']\n\nfor c in name:\n    sns.distplot(d[c], hist=True, kde=True, \n             bins=int(180\/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\n    plt.show()","55db038b":"name = ['age','fnlwgt','education.num','net_capital','hours.per.week']\n\nfor c in name:\n    sns.distplot(d1[c], hist=True, kde=True, \n             bins=int(180\/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\n    plt.show()","38fcfce5":"sns.heatmap(x.corr(),annot = True)","69240b57":"sns.heatmap(y.corr(),annot = True)","82e6a47c":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nfor c in x.select_dtypes(['object']).columns:\n    \n        x[c]=le.fit_transform(x[c])\n        ","0671b53e":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nfor c in y.select_dtypes(['object']).columns:\n    \n        y[c]=le.fit_transform(y[c])\n        ","c38ac856":"d.head()","d0a80ee0":"d1.head()","d77e181c":"x.drop(['age','fnlwgt','education.num','net_capital','hours.per.week'],1,inplace=True)","cbb6e615":"y.drop(['age','fnlwgt','education.num','net_capital','hours.per.week'],1,inplace=True)","7786c4e1":"x=pd.merge(x,d,left_index=True,right_index=True)","289213d2":"y=pd.merge(y,d,left_index=True,right_index=True)","ecf4d798":"x.head()","2da25867":"x.shape","c4a70ce8":"y.head()","c725ff5e":"#pca\n#treebaseapproach\n#rfe","c6f6a82d":"plt.figure(figsize=(20,10))\nsns.heatmap(x.corr(),annot = True)","7cbb18b6":"plt.figure(figsize=(20,10))\nsns.heatmap(y.corr(),annot = True)","f6bb0351":"x_train = x.drop('income',1)\ny_train = x['income']","ef1d7c68":"x_test = x.drop('income',1)\ny_test = x['income']","9c18071c":"from sklearn.feature_selection import RFECV","2ec75f8c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport xgboost as xgb","c153a98e":"from sklearn.metrics import confusion_matrix,accuracy_score","b89d28d1":"import warnings\nwarnings.filterwarnings('ignore')","a9889706":"rfe = RFECV(estimator = DecisionTreeClassifier(random_state=1) , cv=4, scoring = 'accuracy')\nrfe = rfe.fit(x_train,y_train)\n\ncol = x_train.columns[rfe.support_]\n\nacc = accuracy_score(y_test,rfe.estimator_.predict(x_test[col]))\n\nprint('Number of features selected: {}'.format(rfe.n_features_))\nprint('Test Accuracy {}'.format(acc))\nprint(confusion_matrix(y_test,rfe.estimator_.predict(x_test[col])))\n\n\nplt.figure()\nplt.xlabel('k')\nplt.ylabel('CV accuracy')\nplt.plot(np.arange(1, rfe.grid_scores_.size+1), rfe.grid_scores_)\nplt.show()","cca5edc3":"rfe = RFECV(estimator = RandomForestClassifier(random_state=1) , cv=4, scoring = 'accuracy')\nrfe = rfe.fit(x_train,y_train)\n\ncol = x_train.columns[rfe.support_]\n\nacc = accuracy_score(y_test,rfe.estimator_.predict(x_test[col]))\n\nprint('Number of features selected: {}'.format(rfe.n_features_))\nprint('Test Accuracy {}'.format(acc))\nprint(confusion_matrix(y_test,rfe.estimator_.predict(x_test[col])))\n\nplt.figure()\nplt.xlabel('k')\nplt.ylabel('CV accuracy')\nplt.plot(np.arange(1, rfe.grid_scores_.size+1), rfe.grid_scores_)\nplt.show()","3ab625b9":"rfe = RFECV(estimator = AdaBoostClassifier(random_state=1) , cv=4, scoring = 'accuracy')\nrfe = rfe.fit(x_train,y_train)\n\ncol = x_train.columns[rfe.support_]\n\nacc = accuracy_score(y_test,rfe.estimator_.predict(x_test[col]))\n\nprint('Number of features selected: {}'.format(rfe.n_features_))\nprint('Test Accuracy {}'.format(acc))\nprint(confusion_matrix(y_test,rfe.estimator_.predict(x_test[col])))\n\nplt.figure()\nplt.xlabel('k')\nplt.ylabel('CV accuracy')\nplt.plot(np.arange(1, rfe.grid_scores_.size+1), rfe.grid_scores_)\nplt.show()","48f0cae4":"rfe = RFECV(estimator = GradientBoostingClassifier(random_state=1) , cv=4, scoring = 'accuracy')\nrfe = rfe.fit(x_train,y_train)\n\ncol = x_train.columns[rfe.support_]\n\nacc = accuracy_score(y_test,rfe.estimator_.predict(x_test[col]))\n\nprint('Number of features selected: {}'.format(rfe.n_features_))\nprint('Test Accuracy {}'.format(acc))\nprint(confusion_matrix(y_test,rfe.estimator_.predict(x_test[col])))\n\nplt.figure()\nplt.xlabel('k')\nplt.ylabel('CV accuracy')\nplt.plot(np.arange(1, rfe.grid_scores_.size+1), rfe.grid_scores_)\nplt.show()","862ecc04":"from sklearn.ensemble import RandomForestClassifier\n \n# Feature importance values from Random Forests\nrf = RandomForestClassifier(n_jobs=-1, random_state=1)\nrf.fit(x_train, y_train)\nfeat_imp = rf.feature_importances_\n\ncols = x_train.columns[feat_imp >= 0.01]\nest_imp = RandomForestClassifier(random_state=1)\nest_imp.fit(x_train[cols], y_train)\n \n# Test accuracy\nacc = accuracy_score(y_test, est_imp.predict(x_test[cols]))\nprint('Number of features selected: {}'.format(len(cols)))\nprint('Test Accuracy {}'.format(acc))\nprint(confusion_matrix(y_test,est_imp.predict(x_test[cols])))","cdf209b7":"\n\nrf = AdaBoostClassifier( random_state=1)\nrf.fit(x_train, y_train)\nfeat_imp = rf.feature_importances_\n\ncols = x_train.columns[feat_imp >= 0.01]\nest_imp = AdaBoostClassifier( random_state=1)\nest_imp.fit(x_train[cols], y_train)\n \n# Test accuracy\nacc = accuracy_score(y_test, est_imp.predict(x_test[cols]))\nprint('Number of features selected: {}'.format(len(cols)))\nprint('Test Accuracy {}'.format(acc))\nprint(confusion_matrix(y_test,est_imp.predict(x_test[cols])))","d3ce2f48":"rf = GradientBoostingClassifier( random_state=1)\nrf.fit(x_train, y_train)\nfeat_imp = rf.feature_importances_\n\ncols = x_train.columns[feat_imp >= 0.01]\nest_imp = GradientBoostingClassifier( random_state=1)\nest_imp.fit(x_train[cols], y_train)\n \n# Test accuracy\nacc = accuracy_score(y_test, est_imp.predict(x_test[cols]))\nprint('Number of features selected: {}'.format(len(cols)))\nprint('Test Accuracy {}'.format(acc))\nprint(confusion_matrix(y_test,est_imp.predict(x_test[cols])))","5e6f6049":"rf = xgb.XGBClassifier(random_state=1)\nrf.fit(x_train, y_train)\nfeat_imp = rf.feature_importances_\n\ncols = x_train.columns[feat_imp >= 0.01]\nest_imp = xgb.XGBClassifier(random_state=1)\nest_imp.fit(x_train[cols], y_train)\n \n# Test accuracy\nacc = accuracy_score(y_test, est_imp.predict(x_test[cols]))\nprint('Number of features selected: {}'.format(len(cols)))\nprint('Test Accuracy {}'.format(acc))\nprint(confusion_matrix(y_test,est_imp.predict(x_test[cols])))","d149dd65":"x['income']=le.fit_transform(x['income'])","46430977":"x.head()","4428266f":"for c in x.select_dtypes(['object']).columns:\n    cont = pd.get_dummies(x[c],prefix='Contract')\n    x = pd.concat([x,cont],axis=1)\n    x.drop(c,1,inplace=True)\n    ","2f87b848":"for c in y.select_dtypes(['object']).columns:\n    cont = pd.get_dummies(y[c],prefix='Contract')\n    y = pd.concat([y,cont],axis=1)\n    y.drop(c,1,inplace=True)\n    ","552cd686":"x.head()","f1c2dc64":"x.shape","20c3b43f":"x_train = x.drop('income',1)\ny_train = x['income']","bf8e18c1":"x_test = x.drop('income',1)\ny_test = x['income']","3a3ca636":"rfe = RFECV(estimator = DecisionTreeClassifier(random_state=1) , cv=4, scoring = 'accuracy')\nrfe = rfe.fit(x_train,y_train)\n\ncol = x_train.columns[rfe.support_]\n\nacc = accuracy_score(y_test,rfe.estimator_.predict(x_test[col]))\n\nprint('Number of features selected: {}'.format(rfe.n_features_))\nprint('Test Accuracy {}'.format(acc))\nprint(confusion_matrix(y_test,rfe.estimator_.predict(x_test[col])))\n\n\nplt.figure()\nplt.xlabel('k')\nplt.ylabel('CV accuracy')\nplt.plot(np.arange(1, rfe.grid_scores_.size+1), rfe.grid_scores_)\nplt.show()","c428c63b":"rfe = RFECV(estimator = RandomForestClassifier(random_state=1) , cv=4, scoring = 'accuracy')\nrfe = rfe.fit(x_train,y_train)\n\ncol = x_train.columns[rfe.support_]\n\nacc = accuracy_score(y_test,rfe.estimator_.predict(x_test[col]))\n\nprint('Number of features selected: {}'.format(rfe.n_features_))\nprint('Test Accuracy {}'.format(acc))\nprint(confusion_matrix(y_test,rfe.estimator_.predict(x_test[col])))\n\nplt.figure()\nplt.xlabel('k')\nplt.ylabel('CV accuracy')\nplt.plot(np.arange(1, rfe.grid_scores_.size+1), rfe.grid_scores_)\nplt.show()","43203dea":"rfe = RFECV(estimator = AdaBoostClassifier(random_state=1) , cv=4, scoring = 'accuracy')\nrfe = rfe.fit(x_train,y_train)\n\ncol = x_train.columns[rfe.support_]\n\nacc = accuracy_score(y_test,rfe.estimator_.predict(x_test[col]))\n\nprint('Number of features selected: {}'.format(rfe.n_features_))\nprint('Test Accuracy {}'.format(acc))\nprint(confusion_matrix(y_test,rfe.estimator_.predict(x_test[col])))\n\nplt.figure()\nplt.xlabel('k')\nplt.ylabel('CV accuracy')\nplt.plot(np.arange(1, rfe.grid_scores_.size+1), rfe.grid_scores_)\nplt.show()","c7568a89":"rfe = RFECV(estimator = GradientBoostingClassifier(random_state=1) , cv=4, scoring = 'accuracy')\nrfe = rfe.fit(x_train,y_train)\n\ncol = x_train.columns[rfe.support_]\n\nacc = accuracy_score(y_test,rfe.estimator_.predict(x_test[col]))\n\nprint('Number of features selected: {}'.format(rfe.n_features_))\nprint('Test Accuracy {}'.format(acc))\nprint(confusion_matrix(y_test,rfe.estimator_.predict(x_test[col])))\n\nplt.figure()\nplt.xlabel('k')\nplt.ylabel('CV accuracy')\nplt.plot(np.arange(1, rfe.grid_scores_.size+1), rfe.grid_scores_)\nplt.show()","3cc2b8d5":"from sklearn.ensemble import RandomForestClassifier\n \n# Feature importance values from Random Forests\nrf = RandomForestClassifier(n_jobs=-1, random_state=1)\nrf.fit(x_train, y_train)\nfeat_imp = rf.feature_importances_\n\ncols = x_train.columns[feat_imp >= 0.01]\nest_imp = RandomForestClassifier(random_state=1)\nest_imp.fit(x_train[cols], y_train)\n \n# Test accuracy\nacc = accuracy_score(y_test, est_imp.predict(x_test[cols]))\nprint('Number of features selected: {}'.format(len(cols)))\nprint('Test Accuracy {}'.format(acc))\nprint(confusion_matrix(y_test,est_imp.predict(x_test[cols])))","e07a13d0":"\n\nrf = AdaBoostClassifier( random_state=1)\nrf.fit(x_train, y_train)\nfeat_imp = rf.feature_importances_\n\ncols = x_train.columns[feat_imp >= 0.01]\nest_imp = AdaBoostClassifier( random_state=1)\nest_imp.fit(x_train[cols], y_train)\n \n# Test accuracy\nacc = accuracy_score(y_test, est_imp.predict(x_test[cols]))\nprint('Number of features selected: {}'.format(len(cols)))\nprint('Test Accuracy {}'.format(acc))\nprint(confusion_matrix(y_test,est_imp.predict(x_test[cols])))","1cc92f35":"rf = GradientBoostingClassifier( random_state=1)\nrf.fit(x_train, y_train)\nfeat_imp = rf.feature_importances_\n\ncols = x_train.columns[feat_imp >= 0.01]\nest_imp = GradientBoostingClassifier( random_state=1)\nest_imp.fit(x_train[cols], y_train)\n \n# Test accuracy\nacc = accuracy_score(y_test, est_imp.predict(x_test[cols]))\nprint('Number of features selected: {}'.format(len(cols)))\nprint('Test Accuracy {}'.format(acc))\nprint(confusion_matrix(y_test,est_imp.predict(x_test[cols])))","d88cb71d":"rf = xgb.XGBClassifier(random_state=1)\nrf.fit(x_train, y_train)\nfeat_imp = rf.feature_importances_\n\ncols = x_train.columns[feat_imp >= 0.01]\nest_imp = xgb.XGBClassifier(random_state=1)\nest_imp.fit(x_train[cols], y_train)\n \n# Test accuracy\nacc = accuracy_score(y_test, est_imp.predict(x_test[cols]))\nprint('Number of features selected: {}'.format(len(cols)))\nprint('Test Accuracy {}'.format(acc))\nprint(confusion_matrix(y_test,est_imp.predict(x_test[cols])))","12459ffd":"quantile\nnormalizer\nquantile","1e19b21d":"# Adult census income","608ed0fc":"# one hot"}}