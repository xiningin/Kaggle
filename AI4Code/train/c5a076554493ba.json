{"cell_type":{"4c3e5fa9":"code","806f80c1":"code","f0bfe783":"code","d8e22734":"code","1105de52":"code","0c7101ca":"code","7916596d":"code","4ffa5d22":"code","927c5242":"code","6f487a54":"markdown","f31330c7":"markdown","686e0ad0":"markdown"},"source":{"4c3e5fa9":"%%writefile random_agent.py\n\nimport random\n\ndef random_agent(observation, configuration):\n    return random.randrange(configuration.banditCount)","806f80c1":"%%writefile ucb_agent.py\n\nimport math\n\nlast_bandit = -1\ntotal_reward = 0\n\nsums_of_reward = None\nnumbers_of_selections = None\nreward_hist = []\n\ndef ucb_agent(observation, configuration):    \n    global sums_of_reward, numbers_of_selections, last_bandit, total_reward\n    global reward_hist \n\n    if observation.step == 0:\n        numbers_of_selections = [0] * configuration[\"banditCount\"]\n        sums_of_reward = [0] * configuration[\"banditCount\"]\n\n    if last_bandit > -1:\n        reward = observation.reward - total_reward\n        sums_of_reward[last_bandit] += reward\n        total_reward += reward\n        \n        reward_hist.append(reward)\n\n    bandit = 0\n    max_upper_bound = 0\n    for i in range(0, configuration.banditCount):\n        if (numbers_of_selections[i] > 0):\n            average_reward = sums_of_reward[i] \/ numbers_of_selections[i]\n            delta_i = math.sqrt(2 * math.log(observation.step+1) \/ numbers_of_selections[i])\n            upper_bound = average_reward + delta_i\n        else:\n            upper_bound = 1e400\n        if upper_bound > max_upper_bound and last_bandit != i:\n            max_upper_bound = upper_bound\n            bandit = i\n            last_bandit = bandit\n\n    numbers_of_selections[bandit] += 1\n\n    if bandit is None:\n        bandit = 0\n\n    return bandit","f0bfe783":"!pip install kaggle-environments --upgrade","d8e22734":"import matplotlib.pyplot as plt\ncolors_db = ['g','b']\ndef plot_final_rewards(hist):\n    num_episodes = 0\n    \n    plt.figure(figsize=(12,8))\n    for i,agent in enumerate(hist.keys()):\n        plt.plot(hist[agent], label=agent, color=colors_db[i])\n        num_episodes = len(hist[agent])\n        avg_final_reward = np.array(hist[agent]).mean()\n        plt.plot([0, num_episodes-1],[avg_final_reward, avg_final_reward], label=agent+' avg.', color=colors_db[i],linestyle='dashed')\n        \n    plt.legend(bbox_to_anchor=(1.2, 0.5))\n    plt.xlabel(\"Iterations\")\n    plt.ylabel(\"Final Reward\")\n    plt.title(\"Final Agent Rewards for \" \n              + str(num_episodes) + \" Episodes\")\n    plt.show()","1105de52":"from kaggle_environments import make\nfrom collections import defaultdict\nfrom tqdm import tqdm\nimport numpy as np","0c7101ca":"hist = defaultdict(list)\nnum_trails = 10\n\nfor i in tqdm(range(num_trails)):\n    env = make(\"mab\")\n    env.run([\"random_agent.py\", \"ucb_agent.py\"])\n    hist['random_agent'].append(env.state[0]['reward'])\n    hist['ucb_agent'].append(env.state[1]['reward'])\n    \n\nplot_final_rewards(hist)","7916596d":"%%writefile bayesian_sub.py\n\nimport numpy as np\nfrom scipy.stats import beta\n\nps_a = None\npost_b = None\nbandit = None\ntotal_reward = 0\n\n\ndef agent(observation, configuration):\n    global reward_sums, total_reward, bandit, post_a, post_b\n    \n    n_bandits = configuration.banditCount\n\n    if observation.step == 0:\n        post_a = np.ones(n_bandits)\n        post_b = np.ones(n_bandits)\n    else:\n        r = observation.reward - total_reward\n        total_reward = observation.reward\n\n        post_a[bandit] += r\n        post_b[bandit] += (1 - r)\n\n    \n    bound = post_a \/ (post_a + post_b).astype(float) + beta.std(post_a, post_b) * 21\n    bandit = int(np.argmax(bound))\n    \n    return bandit","4ffa5d22":"hist = defaultdict(list)\nnum_trails = 10\n\nfor i in tqdm(range(num_trails)):\n    env = make(\"mab\")\n    env.run([\"bayesian_sub.py\", \"ucb_agent.py\"])\n    hist['bayesian_sub'].append(env.state[0]['reward'])\n    hist['ucb_agent'].append(env.state[1]['reward'])\n    \n\nplot_final_rewards(hist)","927c5242":"hist = defaultdict(list)\nnum_trails = 100\nenv = make(\"mab\")\nfor i in tqdm(range(num_trails)):\n    env.reset()\n    env.run([\"bayesian_sub.py\", \"ucb_agent.py\"])\n    hist['bayesian_sub'].append(env.state[0]['reward'])\n    hist['ucb_agent'].append(env.state[1]['reward'])\n    \n\nplot_final_rewards(hist)","6f487a54":"Initial codes are from isaienkov","f31330c7":"## Performance of both UCB & Bayesian are almost similar","686e0ad0":"# UCB vs Bayesian"}}