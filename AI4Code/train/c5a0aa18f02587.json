{"cell_type":{"18a57a3c":"code","6f99145f":"code","2dd5714f":"code","0d2803b1":"code","2b58bfb5":"code","f6860f9b":"code","0c4c8c6a":"code","711396b3":"code","f03d29c3":"code","94afd62b":"code","a52b3e03":"code","530e10c6":"code","cec2be47":"code","65581acf":"code","9896ae19":"code","d0221164":"code","6844d91e":"markdown","a48c1ee1":"markdown","a336b7f6":"markdown","f26c7906":"markdown","4f026e42":"markdown","e7353640":"markdown"},"source":{"18a57a3c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","6f99145f":"import pandas as pd\n\ndata = pd.read_csv('..\/input\/train.csv')\ndata.head()\n","2dd5714f":"import torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(28 * 28, 400)\n        self.fc2 = nn.Linear(400, 400)\n        self.fc3 = nn.Linear(400, 10)\n    \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return F.log_softmax(x, dim=1)\n\n\nnet = Net()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnet.cuda(device=device)","0d2803b1":"import pycuda.driver as cuda\ncuda.init()","2b58bfb5":"import torch.optim as optim\n\noptimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n# create a loss function\ncriterion = nn.NLLLoss()","f6860f9b":"data_train = data.head(40000)\ndata_test = data.tail(4000)\n\nX_train = data_train.drop('label', axis=1)\nX_test = data_test.drop('label', axis=1)\n\ny_train = data_train['label']\ny_test = data_test['label']","0c4c8c6a":"import torch\nepochs = 1000\n\n\ndata, target = torch.tensor(X_train.values, dtype=torch.float).cuda()\/255, torch.tensor(y_train.values).cuda()\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    net_out = net(data)\n    loss = criterion(net_out, target)\n    loss.backward()\n    optimizer.step()","711396b3":"net_out = net(data)\nloss = criterion(net_out, target)\nloss.data","f03d29c3":"data, target = torch.tensor(X_test.values, dtype=torch.float).cuda()\/255, torch.tensor(y_test.values).cuda()\nnet_out = net(data)\ntest_loss = criterion(net_out, target).data\npred = net_out.data.max(1)[1]  # get the index of the max log-probability\ncorrect = pred.eq(target.data).sum()\n\nprint('\\nTest set: Average loss: {:.4f}, Accuracy: {}\/{} ({:.0f}%)\\n'.format(\n    test_loss, correct, len(data),\n    100. * correct \/ len(data)))","94afd62b":"test = pd.read_csv('..\/input\/test.csv')\ntest.head()","a52b3e03":"test_tensor = torch.tensor(test.values, dtype=torch.float).cuda()\/255\nnet_out = net(test_tensor)\nnet_out","530e10c6":"labels = net_out.data.max(1)[1]","cec2be47":"labels = labels.cpu().numpy()","65581acf":"test.index.values","9896ae19":"submission = pd.DataFrame({'ImageId': pd.Series(range(1,28001)), 'Label': labels})\nsubmission.head()","d0221164":"submission.to_csv('submission.csv', index=False)","6844d91e":"# \u041e\u0446\u0435\u043d\u043a\u0430 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438","a48c1ee1":"# \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438\u0437 train.csv \u0432 data \u0438 \u0432\u044b\u0432\u043e\u0434 \u043f\u0435\u0440\u0432\u044b\u0445 5 \u0437\u0430\u0433\u0440\u0443\u0436\u0435\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445","a336b7f6":"# \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438\u0437 test.csv \u0432 test \u0438 \u0432\u044b\u0432\u043e\u0434 \u043f\u0435\u0440\u0432\u044b\u0445 5 \u0434\u0430\u043d\u043d\u044b\u0445","f26c7906":"# \u041f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438 \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438\u0437 test","4f026e42":"# \u0418\u043c\u043f\u043e\u0440\u0442 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0445 \u0434\u043b\u044f \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a \u0438 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043f\u0443\u0441\u0442\u043e\u0439 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438","e7353640":"# \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0441\u043e\u0437\u0434\u0430\u043d\u043d\u043e\u0439 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438 \u043d\u0430 \u0437\u0430\u0433\u0440\u0443\u0436\u0435\u043d\u043d\u044b\u0445 \u0438\u0437 train \u0434\u0430\u043d\u043d\u044b\u0445; 1000 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0439"}}