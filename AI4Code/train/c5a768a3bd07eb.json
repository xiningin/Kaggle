{"cell_type":{"92196072":"code","81a93757":"code","13fa9658":"code","a1aaaaa2":"code","9004df6a":"code","d8596eb5":"code","7e566e38":"code","3cd766c8":"code","451b5951":"code","1b423103":"code","f86f34a4":"code","0f3da9b0":"code","18a7328d":"code","414334c1":"code","841216d3":"code","5721d3c3":"code","3b632d46":"code","97f1121c":"code","fea9655c":"code","2818ad62":"code","8757aa59":"code","1b8159cb":"code","f20dec6e":"code","7043d7a3":"code","7c12ed96":"code","0de338e4":"code","0c06c7d2":"code","653cf781":"code","b94a7114":"code","7d0df046":"code","1ca76aa1":"code","fb7271af":"code","7166b505":"code","2d20edba":"code","054e4d3f":"code","1c62007c":"code","10355191":"code","a6bc2f1c":"code","7d7291eb":"code","f97d2b6e":"code","c2538433":"code","8fbe7c95":"code","96a5fdea":"code","49625a1f":"code","fbdc5829":"code","da05d2bf":"code","ccb898ab":"code","71a665e5":"code","e7f00544":"code","f72ec00e":"code","95d6945b":"markdown","534fa672":"markdown","21319540":"markdown","47f6db3e":"markdown","3d127626":"markdown","107a75e9":"markdown","eeb5c72f":"markdown","4b785b15":"markdown","29c24898":"markdown","6cf66c3d":"markdown","745b5ca0":"markdown","1326132d":"markdown","73019091":"markdown","e79354df":"markdown","977c5e98":"markdown","a2cb8a6f":"markdown"},"source":{"92196072":"import numpy as np, pandas as pd, xgb_wrapper as xgbw, lgbm_wrapper as lgbmw, project_tools as project, seaborn as sns, matplotlib.pyplot as plt, xgboost as xgb\nimport sklearn, imblearn\nimport sklearn.compose, sklearn.naive_bayes\nfrom pprint import pprint as display\nfrom tqdm.notebook import tqdm\nimport warnings\nsns.set_style('dark')","81a93757":"warnings.filterwarnings('ignore')","13fa9658":"# reading the csv data\ndata = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv', index_col='id')\n\n# splitting into train and test sets\ntrain_full, test_full = sklearn.model_selection.train_test_split(data, test_size=0.5, random_state=1, stratify=data['stroke'])","a1aaaaa2":"test, test_y = test_full.drop('stroke', axis=1), test_full['stroke']","9004df6a":"train = train_full.copy()","d8596eb5":"train.info()","7e566e38":"train.describe()","3cd766c8":"numerical = ['age', 'bmi', 'avg_glucose_level']\nbinary = ['hypertension', 'heart_disease']\ncategorical = ['ever_married', 'work_type', 'Residence_type', 'smoking_status', 'gender']","451b5951":"fig, axs = plt.subplots(4, 2, figsize=(16*2,9*4))\nfor ax, col in zip(np.nditer(axs, flags=['refs_ok']), categorical+binary+['stroke']):\n    ax = ax.item()\n    sns.countplot(x=train[col], ax=ax)\nplt.show()","1b423103":"g = sns.pairplot(data=data[numerical], diag_kind='hist', kind='scatter', diag_kws={'kde': True}, aspect=16\/9, height=18)","f86f34a4":"fig, axs = plt.subplots(4, 2, figsize=(16*2,9*4))\nfor ax, col in zip(np.nditer(axs, flags=['refs_ok']), categorical+binary):\n    ax = ax.item()\n    sns.barplot(x=train[col], y=train['stroke'], ax=ax)\nplt.show()","0f3da9b0":"fig, axs = plt.subplots(2, 2, figsize=(16*2,9*2))\nfor ax, col in zip(np.nditer(axs, flags=['refs_ok']), numerical):\n    ax = ax.item()\n    sns.regplot(x=train[col], y=train['stroke'], ax=ax, logistic=True, ci=False)\nplt.show()","18a7328d":"from sklearn.experimental import enable_iterative_imputer","414334c1":"ctrans = sklearn.pipeline.Pipeline([\n    ('encode', sklearn.preprocessing.OneHotEncoder(handle_unknown='ignore'))\n])","841216d3":"ntrans = sklearn.pipeline.Pipeline([\n    ('impute', sklearn.impute.IterativeImputer()),\n    ('transfom', sklearn.preprocessing.PowerTransformer())\n])","5721d3c3":"preprocess = sklearn.compose.ColumnTransformer([\n    ('categorical', ctrans, categorical),\n    ('binary', 'passthrough', binary),\n    ('numerical', ntrans, numerical)\n], remainder='passthrough', n_jobs=-1)","3b632d46":"resampler = imblearn.over_sampling.SMOTE(sampling_strategy='all', n_jobs=4, random_state=1)","97f1121c":"df_X, df_y = train.drop('stroke', axis=1), train.stroke","fea9655c":"df_X[categorical+binary] = df_X[categorical+binary].apply(lambda x: x.factorize()[0])","2818ad62":"df_X = df_X.fillna(0)","8757aa59":"train_balanced = pd.concat(resampler.fit_resample(df_X, df_y), axis=1)","1b8159cb":"train_balanced","f20dec6e":"arr = train[categorical].values","7043d7a3":"arr[:, 0]","7c12ed96":"index = np.where(arr[:, 0] == 'No')","0de338e4":"arry = train['stroke'].values","0c06c7d2":"train[['stroke', 'ever_married']].groupby('ever_married').mean()","653cf781":"s = pd.DataFrame(np.concatenate([arr, arry.reshape(-1, 1)], axis=1))[[0, 5]]","b94a7114":"s[5] = s[5].astype(int)","7d0df046":"s.groupby(0).mean()","1ca76aa1":"np.mean(arry[index])","fb7271af":"X, y = train_balanced.drop('stroke', axis=1), train_balanced['stroke']\n\npca = sklearn.pipeline.make_pipeline(preprocess, sklearn.decomposition.PCA(n_components=2, random_state=1))\n\nX_red = pca.fit_transform(X)\n\nreduced_X = pd.DataFrame(X_red, index=X.index)\n\nreduced_train = pd.concat([reduced_X, y], axis=1)\n\nplt.figure(figsize=(32,18))\nsns.scatterplot(data=reduced_train, x=1, y=0, hue='stroke')\nplt.show()","7166b505":"tune_train = train_balanced.sample(n=1000)","2d20edba":"X, y = tune_train.drop('stroke', axis=1), tune_train.stroke","054e4d3f":"np.random.seed(0)\ndist = np.random.uniform(low=0, high=1, size=(100))","1c62007c":"models = {\n    'logistic': {\n        'model': sklearn.linear_model.LogisticRegression(penalty='l2', solver='liblinear', class_weight='balanced', random_state=1),\n        'params': {\n            'C': dist}},\n    'random-forest': {\n        'model': sklearn.ensemble.RandomForestClassifier(random_state=1),\n        'params': {'bootstrap': [True, False],\n                   'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n                   'max_features': ['auto', 'sqrt'],\n                   'min_samples_leaf': [1, 2, 4],\n                   'min_samples_split': [2, 5, 10],\n                   'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]}},\n    'xgb': {\n        'model': xgbw.get_wrapper('XGBClassifier', random_state=1, booster='gbtree', n_estimators=1000, learning_rate=0.01, use_label_encoder=False),\n        'params': {\n            'reg_alpha': dist,\n            'reg_lambda': dist,\n            'gamma': dist,\n            'min_child_weight': dist,\n            'max_depth': [3, 4, 5, 6, 7]}},\n    'lgbm': {\n        'model': lgbmw.get_wrapper('LGBMClassifier', random_state=1, n_estimators=1000, learning_rate=0.01),\n        'params': {\n            'reg_alpha': dist,\n            'reg_lambda': dist,\n            'min_split_gain': dist,\n            'min_child_weight': dist,\n            'max_depth': [3, 4, 5, 6, 7]}}}","10355191":"model_best = {}\nfor name, model in tqdm(models.items()):\n    clf = model['model']\n    pdist = model['params']\n    pipeline = sklearn.pipeline.make_pipeline(preprocess, sklearn.model_selection.RandomizedSearchCV(estimator=clf, param_distributions=pdist, cv=10, n_iter=30, scoring='f1_weighted', n_jobs=-1))\n    pipeline.fit(X, y)\n    model_best[name] = sklearn.pipeline.make_pipeline(preprocess, pipeline['randomizedsearchcv'].best_estimator_)","a6bc2f1c":"X, y = train_balanced.drop('stroke', axis=1), train_balacned.stroke","7d7291eb":"scores = {}\nfnscores = {}\n\nfor name, pipeline in tqdm(model_best.items()):\n    results = sklearn.model_selection.cross_validate(pipeline, X, y, cv=10, scoring={'f1': sklearn.metrics.get_scorer('f1_weighted'), 'fnr': project.neg_false_negative_rate}, n_jobs=-1)\n    scores[name] = results['test_f1']\n    fnscores[name] = results['test_fnr']","f97d2b6e":"scores = pd.DataFrame(scores)\nfnscores = pd.DataFrame(fnscores)","c2538433":"scores.agg(['mean', 'std'])","8fbe7c95":"fnscores.agg(['mean', 'std'])","96a5fdea":"fig, axs = plt.subplots(2, 2, figsize=(32,18))\nfor ax, model in zip(np.nditer(axs, flags=['refs_ok']), scores.columns):\n    ax = ax.item()\n    sns.histplot(data=scores, x=model, kde=True, ax=ax)\nplt.show()","49625a1f":"fig, axs = plt.subplots(2, 2, figsize=(32,18))\nfor ax, model in zip(np.nditer(axs, flags=['refs_ok']), fnscores.columns):\n    ax = ax.item()\n    sns.histplot(data=fnscores, x=model, kde=True, ax=ax)\nplt.show()","fbdc5829":"plt.figure(figsize=(32,18))\nsns.barplot(data=scores)\nplt.show()","da05d2bf":"plt.figure(figsize=(32,18))\nsns.barplot(data=fnscores * -1)\nplt.show()","ccb898ab":"for name, pipeline in tqdm(model_best.items()):\n    model_best[name] = pipeline.fit(X, y)","71a665e5":"final_model_scores = pd.DataFrame(index=['accuracy', 'f1', 'auc', 'fnr'])","e7f00544":"with tqdm(model_best.keys()) as bar:\n    for model_name in bar:\n        bar.set_description(model_name)\n        clf = model_best[model_name]\n        preds = clf.predict(test)\n        final_model_scores[model_name] = (sklearn.metrics.get_scorer('balanced_accuracy')(clf, test, test_y),\n                                          sklearn.metrics.get_scorer('f1_weighted')(clf, test, test_y), \n                                          sklearn.metrics.get_scorer('roc_auc')(clf, test, test_y), \n                                          project.neg_false_negative_rate(clf, test, test_y) * -1)\n        output = pd.DataFrame({\n            'PassengerId': test.index,\n            'Survived': preds\n        })\n        file_name = '{}-preds.csv'.format(model_name)\n        output.to_csv(file_name, index=False)","f72ec00e":"final_model_scores","95d6945b":"The distributions of the `bmi` and `avg_glucose_level` features are right-tailed, whereas the distribution of the `age` feature is left-tailed.","534fa672":"The logistic regression plots show that the chances of having a stroke increase with the increase of `avg_glucose_level`, `age` and `bmi`.","21319540":"According to the data, a person's chances of having a stroke are higher if:\n- they have heart disease\n- they live in an urban area\n- they are male\n- they have high blood pressure\n- they have formally smoked or are presently smoking\n- they have been married\n- they are self employed","47f6db3e":"<a name='start'><\/a>\n# Stroke Prediction","3d127626":"<a name='obs'><\/a>\n## Data Observation","107a75e9":"<a name='vis'><\/a>\n## Data Visualisation with Seaborn and Matplotlib","eeb5c72f":"Before exploring any of the data, I have decided to conduct some brief reaserch into the medical condition and its risk factors.\n\n*A stroke is a serious medical condition where the blood flow to part of the brain is cut off.*\n\nSome of the lifestyle-related risk factors for stroke are:\n- smoking\n- high blood pressure\n- high blood cholesterol levels\n- obesity\n- a diet high in saturated fats and salt...\n    - ... and low in fruit, fibre and vegetables\n- diabetes\n- heavy alcohol intake\n- insufficent regular exercise\n- heart disorders like coronary heart disease\n\nThis will tell us what to expect in our data, for example people who smoke and drink alcohol heavily will have a higher chance of having a stroke.\n\nWith this classification problem, a false negative is far more dangerous than a false positive.","4b785b15":"The highest weighted f1 score belongs to the random forest model, followed by the lightgbm model, however the lightgbm model's scores has a lower standard deviation than the scores of the random forst model, suggesting that the performance of the model is more consistent across folds. ","29c24898":"<a name='tune'><\/a>\n## Hyperparameter Tuning ","6cf66c3d":"More people have been married than not, more people live in an urban area than a rural area, more people do not have hypertension or heart disease, and most of the people have never smoked and have private jobs. Also, more people in the dataset are female than male.\n\nThe binary target is extremely skewed, so stratified cross validation would be the best choice - oversampling could be used.","745b5ca0":"There are some missing values in the `bmi` column.","1326132d":"There are 10 features in the dataset, and the features relevant to the risk factors mentioned earlier are: \n- `bmi`\n- `avg_glucose_level`\n- `heart_disease`\n- `smoking_status`\n- `hypertension` (aka high blood pressure)","73019091":"### Bivariate","e79354df":"<a name='prep'><\/a>\n## Data Preprocessing with Scikit-Learn","977c5e98":"### Univariate","a2cb8a6f":"## Dimensionality Reduction - PCA"}}