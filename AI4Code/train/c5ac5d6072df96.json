{"cell_type":{"fa77f468":"code","7a6ba974":"code","9964e357":"code","8550499b":"code","917edb19":"code","8e168076":"code","aa56b41c":"code","ab28c581":"code","e3ae0228":"code","a8223612":"code","a06fc461":"code","156c8224":"code","7106b698":"code","90ee0ba0":"code","2cbbb632":"code","fb836ce0":"code","54a5bf81":"code","555195d5":"code","6fed8c4e":"code","501a4884":"code","b17becd1":"code","0f596dd9":"markdown","70c56cdb":"markdown","dd2aa0c9":"markdown","d5432e1c":"markdown","79095cd9":"markdown","1862b762":"markdown","a44fac19":"markdown","d5af7661":"markdown","fc9804f1":"markdown"},"source":{"fa77f468":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder,RobustScaler, PowerTransformer, PolynomialFeatures\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LinearRegression, ElasticNet, LassoLars, Lasso, RidgeCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.feature_selection import SelectFromModel\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew, boxcox_normmax\nfrom scipy.special import boxcox1p\n\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt  \n\nimport warnings\nwarnings.filterwarnings('ignore')","7a6ba974":"train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nsub = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")","9964e357":"train.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","8550499b":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","917edb19":"train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)|(train['SalePrice']<36000)].index)\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","8e168076":"def plot_dist(var):\n    sns.distplot(var, fit=norm);\n    (mu, sigma) = norm.fit(var)\n\n    #plot the distribution\n    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n                loc='best')\n    plt.ylabel('Frequency')\n    plt.title('Distribution')\n\n    #QQ-plot\n    plt.figure()\n    stats.probplot(var, plot=plt)\n    plt.show()\n\nplot_dist(train['SalePrice'])","aa56b41c":"train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\nplot_dist(train['SalePrice'])","ab28c581":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","e3ae0228":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","a8223612":"for col in (\"FireplaceQu\", \"Fence\",\"Alley\", \"MiscFeature\", \"PoolQC\", 'GarageType',\\\n            'GarageFinish', 'GarageQual', 'GarageCond',\"MasVnrType\",'MSSubClass',\\\n           'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\n\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2',\\\n            'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath',\\\n           \"MasVnrArea\"):\n    all_data[col] = all_data[col].fillna(0)\n\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\nall_data = all_data.drop(['Utilities'], axis=1)\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data.isnull().sum().sort_values(ascending=False)","a06fc461":"all_data['exists_garage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nall_data['exists_bsmt'] = all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nall_data[\"OverallGrade\"] = all_data[\"OverallQual\"] * all_data[\"OverallCond\"]\nall_data['Total_Bath'] = all_data['FullBath'] + (0.5 * all_data['HalfBath']) + all_data['BsmtFullBath'] + (0.5 * all_data['BsmtHalfBath'])\nall_data[\"SimplOverallCond\"] = all_data.OverallCond.replace({1 : 1, 2 : 1, 3 : 1, # bad\n                                                       4 : 2, 5 : 2, 6 : 2, # average\n                                                       7 : 3, 8 : 3, 9 : 3, 10 : 3 # good\n                                                      })\nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)\nall_data['OverallQual']=all_data['OverallQual'].astype(str)\nall_data['GarageYrBlt']=all_data['GarageYrBlt'].astype(str)\nall_data['GarageCars']=all_data['GarageCars'].astype(str)\nall_data['BedroomAbvGr']=all_data['BedroomAbvGr'].astype(str)\nall_data['HalfBath']=all_data['HalfBath'].astype(str)\n\n\nobject_feats = all_data.dtypes[all_data.dtypes == \"object\"].index.tolist()\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index.tolist()","156c8224":"skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna()))\nsk = skewed_feats[abs(skewed_feats)>0.5].index.to_list()\nall_data[sk] = np.log1p(all_data[sk])","7106b698":"train = all_data[:ntrain]\ntest  = all_data[ntrain:]","90ee0ba0":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(handle_unknown='ignore'), object_feats),\n        ('num', RobustScaler() , numeric_feats)\n    ])","2cbbb632":"xg = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nclf_xg = Pipeline(steps=[\n                    ('pre', preprocessor),\n                    ('poly', PolynomialFeatures(2)),\n                    ('selection', SelectFromModel(estimator=RandomForestRegressor(n_estimators=300, random_state=1))),\n                    ('a', xg),\n                    ])\n\nclf_xg.fit(train, y_train)","fb836ce0":"lg = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf=6, min_sum_hessian_in_leaf = 11)\nclf_lg = Pipeline(steps=[\n                    ('pre', preprocessor),\n                    ('a', lg),\n                    ])\n\nclf_lg.fit(train, y_train)","54a5bf81":"estimators = [\n('', RandomForestRegressor(n_estimators=300,random_state=1)),\n('kernel_ridge', KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)),\n('Boosting', GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', \\\n                                       min_samples_leaf=15, min_samples_split=10, loss='huber',random_state=1)),\n('elasticnet', ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3)),\n('lasso', Lasso(alpha = 0.0005, random_state=1)),\n             ]\nstack_reg = StackingRegressor(estimators = estimators, final_estimator = RandomForestRegressor(n_estimators = 500 ,random_state=1), n_jobs=-1)\n\nclf = Pipeline(steps=[\n                    ('pre', preprocessor),\n                    ('a', stack_reg),\n                    ])\nclf.fit(train, y_train)","555195d5":"#clf.get_params()","6fed8c4e":"param = {\n    'a__final_estimator__n_estimators': [200, 300, 400, 500, 600],\n}\nStack = GridSearchCV(clf, param, scoring='accuracy', cv=10).fit(train, y_train)\nprint(Stack.best_estimator_)\nprint('best score:')\nprint(Stack.best_score_)","501a4884":"predictions = 0.5*np.expm1(Stack.best_estimator_.predict(test)) + 0.25*np.expm1(clf_xg.predict(test))+ 0.25*np.expm1(clf_lg.predict(test))","b17becd1":"sub['SalePrice'] = predictions\nsub.to_csv('submission.csv',index=False)","0f596dd9":"# Training and Predicting","70c56cdb":"# Outliers","dd2aa0c9":"# Target value transformation","d5432e1c":"**Non-normality disappears after log(1+x) transformation**","79095cd9":"# Eliminating Skewness of features","1862b762":"# Independent variables Imputing","a44fac19":"# **Importing libraries**","d5af7661":"# Feature engineering","fc9804f1":"# Importing and Saving Data"}}