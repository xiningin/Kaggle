{"cell_type":{"f37acbf9":"code","3351b006":"code","36837d11":"code","56a27346":"code","52db9692":"code","873af5c3":"code","c963ba2d":"code","589cc883":"code","565e38b2":"code","8c22a6ba":"code","7b7e14f0":"code","2c1e40ed":"code","cfb8d38c":"code","393e98f7":"code","dffae127":"code","0c744dbc":"code","79ba42b2":"code","feecd1a3":"code","244f552c":"code","6742cc6b":"code","e6facfd3":"code","b57677c5":"code","78e9a333":"code","50ea2aab":"code","52fde658":"code","522706b7":"code","1c1d69ca":"code","fcb1e171":"code","d3d35d60":"code","5ffebdb2":"code","eea1fe57":"code","b7b94e61":"code","dce0834d":"code","5d086571":"code","ddf1f5bf":"code","ec7bbe9f":"code","b462a80f":"code","738ee6f8":"code","b16238aa":"code","f68b7df6":"code","ed714159":"code","e8f961da":"code","0785b602":"code","fda5a1bf":"code","5221b957":"code","37949f6d":"code","74681662":"code","0063b5c6":"markdown","f3b0fc7f":"markdown","8343038e":"markdown","9cea0395":"markdown","4d707828":"markdown","7cee3198":"markdown","a3bc05fc":"markdown","6bf944bd":"markdown","4cd1a094":"markdown","55be7018":"markdown","5df3b25f":"markdown","9f7538d1":"markdown","22dc508a":"markdown","574bf074":"markdown","c099a537":"markdown","fd8e3756":"markdown","903358f3":"markdown","0167d82c":"markdown","0e58b028":"markdown","7eceddaa":"markdown","af67454e":"markdown","683d65bb":"markdown","eb06f12d":"markdown","3d76f80b":"markdown","6e3f306e":"markdown","2903771a":"markdown","9218b5e0":"markdown","8f750ae3":"markdown","814b0d00":"markdown","eec2b564":"markdown","39cf9318":"markdown"},"source":{"f37acbf9":"!pip install textstat\n!pip install rich\n\nimport os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport re\nimport nltk\nimport textstat\nimport time\nimport wandb\nimport rich\nimport spacy\n\nfrom pandas import DataFrame\nfrom matplotlib.lines import Line2D\nfrom rich.console import Console\nfrom rich import print\nfrom rich.theme import Theme\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag\nfrom collections import Counter\nfrom wordcloud import WordCloud,STOPWORDS\nfrom spacy import displacy\nfrom nltk.tokenize import sent_tokenize, word_tokenize \nfrom sklearn.feature_extraction.text import CountVectorizer as CV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error as mse\n\nnltk.download('stopwords')","3351b006":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"api_key\")\n\nos.environ[\"WANDB_SILENT\"] = \"true\"","36837d11":"! wandb login $api_key","56a27346":"sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\ndef custom_palette(custom_colors):\n    customPalette = sns.set_palette(sns.color_palette(custom_colors))\n    sns.palplot(sns.color_palette(custom_colors),size=0.8)\n    plt.tick_params(axis='both', labelsize=0, length = 0)\n\npalette = [\"#7209B7\",\"#3F88C5\",\"#136F63\",\"#F72585\",\"#FFBA08\"]\npalette2 = sns.diverging_palette(120, 220, n=20)\ncustom_palette(palette)\n\ncustom_theme = Theme({\n    \"info\" : \"italic bold cyan\",\n    \"warning\": \"italic bold magenta\",\n    \"danger\": \"bold blue\"\n})\n\nconsole = Console(theme=custom_theme)","52db9692":"train_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")","873af5c3":"train_df.head()","c963ba2d":"test_df.head()","589cc883":"train_df.nunique()","565e38b2":"msno.bar(train_df,color=palette[2], sort=\"ascending\", figsize=(10,5), fontsize=12)\nplt.show()","8c22a6ba":"excerpt1 = train_df['excerpt'].min()\nconsole.print(\"Before preprocessing: \",style=\"info\")\nconsole.print(excerpt1,style='warning')\n\ne = re.sub(\"[^a-zA-Z]\", \" \", excerpt1)\ne = e.lower()\n        \ne = nltk.word_tokenize(e)\n        \ne = [word for word in e if not word in set(stopwords.words(\"english\"))]\n        \nlemma = nltk.WordNetLemmatizer()\ne = [lemma.lemmatize(word) for word in e]\ne=\" \".join(e)\nconsole.print(\"After preprocessing: \",style=\"info\")\nconsole.print(e,style='warning')","7b7e14f0":"#====== Preprocessing function ======\ndef preprocess(data):\n    excerpt_processed=[]\n    for e in data['excerpt']:\n        \n        # find alphabets\n        e = re.sub(\"[^a-zA-Z]\", \" \", e)\n        \n        # convert to lower case\n        e = e.lower()\n        \n        # tokenize words\n        e = nltk.word_tokenize(e)\n        \n        # remove stopwords\n        e = [word for word in e if not word in set(stopwords.words(\"english\"))]\n        \n        # lemmatization\n        lemma = nltk.WordNetLemmatizer()\n        e = [lemma.lemmatize(word) for word in e]\n        e=\" \".join(e)\n        \n        excerpt_processed.append(e)\n        \n    return excerpt_processed ","2c1e40ed":"# train_df[\"excerpt_preprocessed\"] = preprocess(train_df)\n# test_df[\"excerpt_preprocessed\"] = preprocess(test_df)\n\n# #====== Saving to csv files and creating artifacts ======\n# train_df.to_csv(\"train_excerpt_preprocessed.csv\")\n\n# run = wandb.init(project='commonlit', name='excerpt_preprocessed')\n\n# artifact = wandb.Artifact('train_excerpt_preprocessed', type='dataset')\n\n# #====== Add a file to the artifact's contents ======\n# artifact.add_file(\"train_excerpt_preprocessed.csv\")\n\n# #====== Save the artifact version to W&B and mark it as the output of this run ====== \n# run.log_artifact(artifact)\n\n# run.finish()\n\n# #====== Saving to csv files and creating artifacts ======\n# test_df.to_csv(\"test_excerpt_preprocessed.csv\")\n\n# run = wandb.init(project='commonlit', name='excerpt_preprocessed')\n\n# artifact = wandb.Artifact('test_excerpt_preprocessed', type='dataset')\n\n# #====== Add a file to the artifact's contents ======\n# artifact.add_file(\"test_excerpt_preprocessed.csv\")\n\n# #====== Save the artifact version to W&B and mark it as the output of this run ====== \n# run.log_artifact(artifact)\n\n# run.finish()","cfb8d38c":"run = wandb.init(project='commonlit')\nartifact = run.use_artifact('ruchi798\/commonlit\/train_excerpt_preprocessed:v0', type='dataset')\nartifact_dir = artifact.download()\nrun.finish()\n\npath = os.path.join(artifact_dir,\"train_excerpt_preprocessed.csv\")\ntrain_df = pd.read_csv(path)\ntrain_df = train_df.drop(columns=[\"Unnamed: 0\"])\n\nrun = wandb.init(project='commonlit')\nartifact = run.use_artifact('ruchi798\/commonlit\/test_excerpt_preprocessed:v0', type='dataset')\nartifact_dir = artifact.download()\nrun.finish()\n\npath = os.path.join(artifact_dir,\"test_excerpt_preprocessed.csv\")\ntest_df = pd.read_csv(path)\ntest_df = test_df.drop(columns=[\"Unnamed: 0\"])","393e98f7":"#====== Function to plot wandb bar chart ======\ndef plot_wb_bar(df,col1,col2,name,title): \n    run = wandb.init(project='commonlit', job_type='image-visualization',name=name)\n    \n    dt = [[label, val] for (label, val) in zip(df[col1], df[col2])]\n    table = wandb.Table(data=dt, columns = [col1,col2])\n    wandb.log({name : wandb.plot.bar(table, col1,col2,title=title)})\n\n    run.finish()\n    \n#====== Function to plot wandb histogram ======\ndef plot_wb_hist(df,name,title):\n    run = wandb.init(project='commonlit', job_type='image-visualization',name=name)\n\n    dt = [[x] for x in df[name]]\n    table = wandb.Table(data=dt, columns=[name])\n    wandb.log({name : wandb.plot.histogram(table, name, title=title)})\n\n    run.finish()","dffae127":"fig, ax = plt.subplots(1,2,figsize=(20,10))\nsns.kdeplot(train_df['target'], color=palette[0], shade=True,ax=ax[0])\nsns.kdeplot(train_df['standard_error'], color=palette[1], shade=True,ax=ax[1])\nax[0].axvline(train_df['target'].mean(), color=palette[0],linestyle=':', linewidth=2)\nax[1].axvline(train_df['standard_error'].mean(), color=palette[1],linestyle=':', linewidth=2)\nax[0].set_title(\"Target Distribution\",font=\"Serif\")\nax[1].set_title(\"Standard Error Distribution\",font=\"Serif\")\nax[0].annotate('mean', xy=(-0.3* np.pi, 0.2), xytext=(1, 0.2), font='Serif',\n            arrowprops=dict(arrowstyle=\"->\",\n                            connectionstyle=\"angle3,angleA=0,angleB=-90\"));\nax[1].annotate('mean', xy=(0.49, 6), xytext=(0.57, 6), font='Serif',\n            arrowprops=dict(arrowstyle=\"->\",\n                            connectionstyle=\"angle3,angleA=0,angleB=-90\"));\nplt.show()\n\nsns.jointplot(x=train_df['target'], y=train_df['standard_error'], kind='hex',height=10,edgecolor=palette[4])\nplt.suptitle(\"Target vs Standard error \",font=\"Serif\")\nplt.subplots_adjust(top=0.95)\nplt.show()","0c744dbc":"plot_wb_hist(train_df,\"target\",\"Target Distribution\")\nplot_wb_hist(train_df,\"standard_error\",\"Standard Error Distribution\")","79ba42b2":"run = wandb.init(project='commonlit', name='count')\n\n# maximum target\nm_t = train_df[\"target\"].max() \n\n# minimum target\nl_t = train_df[\"target\"].min() \n\n# maximum standard error\nm_se = train_df[\"standard_error\"].max()\n\n# minimum standard error\nl_se = train_df[\"standard_error\"].min() \n\nwandb.log({'Target (highest value)': m_t, \n           'Target (lowest value)': l_t,\n           'Standard error (highest value)': m_se, \n           'Standard error (lowest value)': l_se\n          })\n\nrun.finish()","feecd1a3":"plt.figure(figsize=(16, 8))\nsns.countplot(y=\"license\",data=train_df,palette=\"BrBG\",linewidth=3)\nplt.title(\"License Distribution\",font=\"Serif\")\nplt.show()","244f552c":"license_data = pd.DataFrame(train_df.license.value_counts().reset_index().values,columns=[\"license\", \"counts\"])\nplot_wb_bar(license_data,'license', 'counts',\"license\",\"License Distribution\")","6742cc6b":"def get_top_n_words(corpus, n=None):\n    vec = CV().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef get_top_n_bigram(corpus, n=None):\n    vec = CV(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\n\ndef get_top_n_trigram(corpus, n=None):\n    vec = CV(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","e6facfd3":"def plot_bt(x,w,p):\n    common_words = x(train_df['excerpt_preprocessed'], 20)\n    common_words_df = DataFrame (common_words,columns=['word','freq'])\n\n    plt.figure(figsize=(16,8))\n    sns.barplot(x='freq', y='word', data=common_words_df,facecolor=(0, 0, 0, 0),linewidth=3,edgecolor=sns.color_palette(p,20))\n    plt.title(\"Top 20 \"+ w,font='Serif')\n    plt.xlabel(\"Frequency\", fontsize=14)\n    plt.yticks(fontsize=13)\n    plt.xticks(rotation=45, fontsize=13)\n    plt.ylabel(\"\");\n    return common_words_df","b57677c5":"common_words = get_top_n_words(train_df['excerpt_preprocessed'], 20)\ncommon_words_df1 = DataFrame(common_words,columns=['word','freq'])\nplt.figure(figsize=(16, 8))\nax = sns.barplot(x='freq', y='word', data=common_words_df1,facecolor=(0, 0, 0, 0),linewidth=3,edgecolor=sns.color_palette(\"ch:start=3, rot=.1\",20))\n\nplt.title(\"Top 20 unigrams\",font='Serif')\nplt.xlabel(\"Frequency\", fontsize=14)\nplt.yticks(fontsize=13)\nplt.xticks(rotation=45, fontsize=13)\nplt.ylabel(\"\");\n\ncommon_words_df2 = plot_bt(get_top_n_bigram,\"bigrams\",\"ch:rot=-.5\")\ncommon_words_df3 = plot_bt(get_top_n_trigram,\"trigrams\",\"ch:start=-1, rot=-.6\")","78e9a333":"plot_wb_bar(common_words_df1,'word','freq',\"unigrams\",\"Top 20 unigrams\")\nplot_wb_bar(common_words_df2,'word','freq',\"bigrams\",\"Top 20 bigrams\")\nplot_wb_bar(common_words_df3,'word','freq',\"trigrams\",\"Top 20 trigrams\")","50ea2aab":"# color function for the wordcloud\ndef color_wc(word=None,font_size=None,position=None, orientation=None,font_path=None, random_state=None):\n    h = int(360.0 * 150.0 \/ 255.0)\n    s = int(100.0 * 255.0 \/ 255.0)\n    l = int(100.0 * float(random_state.randint(80, 120)) \/ 255.0)\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n\nplt.subplots(figsize=(16,16))\nwc = WordCloud(stopwords=STOPWORDS,background_color=\"white\", contour_width=2, contour_color='blue',width=1500, height=750,color_func=color_wc,max_words=150, max_font_size=256,random_state=42)\nwc.generate(' '.join(train_df['excerpt_preprocessed']))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","52fde658":"text_props = train_df.copy()\n\ndef avg_word_len(df):\n    df = df.str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))\n    return df\n\ntext_len = train_df['excerpt'].str.len()\ntext_len_pre = train_df['excerpt_preprocessed'].str.len()\navg_text = avg_word_len(train_df['excerpt'])\navg_text_pre = avg_word_len(train_df['excerpt_preprocessed'])\nlexicon_count = []\nlexicon_count_pre = []\nsentence_count = []\nfor i in range(len(train_df)):\n    lc = textstat.lexicon_count(train_df['excerpt'][i])\n    lcp = textstat.lexicon_count(train_df['excerpt_preprocessed'][i])\n    sc = textstat.sentence_count(train_df['excerpt'][i])\n    lexicon_count.append(lc)\n    lexicon_count_pre.append(lcp)\n    sentence_count.append(sc)\n    \ntext_props['text_len'] = text_len\ntext_props['text_len_pre'] = text_len_pre\ntext_props['lexicon_count'] = lexicon_count\ntext_props['lexicon_count_pre'] = lexicon_count_pre\ntext_props['avg_text'] = avg_text\ntext_props['avg_text_pre'] = avg_text_pre\ntext_props['sentence_count'] = sentence_count\n\ndef plot_distribution(col1,col2,title1,title2):\n    fig, ax = plt.subplots(1,2,figsize=(20,10))\n    sns.kdeplot(data=text_props, x=col1,color=palette[3],label=\"Excerpt\",ax=ax[0])\n    sns.kdeplot(data=text_props, x=col2,color=palette[4],label=\"Excerpt preprocessed\",ax=ax[0])\n    ax[0].set_title(title1,font=\"Serif\")\n\n    sns.scatterplot(data=text_props,x=col1,y='target',color= palette[3],ax=ax[1],markers='.')\n    sns.scatterplot(data=text_props,x=col2,y='target',color= palette[4],ax=ax[1],markers='.')\n    ax[1].set_title(title2,font=\"Serif\")\n\n    plt.show()\n\ncustom_lines = [Line2D([0], [0], color=palette[3], lw=4),\n                Line2D([0], [0], color=palette[4], lw=4)]\n\nplt.figure(figsize=(20, 1))\nlegend = plt.legend(custom_lines, ['Excerpt', 'Excerpt preprocessed'],loc=\"center\")\nplt.setp(legend.texts, family='Serif')\nplt.axis('off')\nplt.show()\n\nplot_distribution(\"text_len\",\"text_len_pre\",\"Character count distribution\",\"Character count vs Target\")\nplot_distribution(\"lexicon_count\",\"lexicon_count_pre\",\"Word count distribution\",\"Word count vs Target\")\nplot_distribution(\"avg_text\",\"avg_text_pre\", \"Average word length distribution\",\"Average word length vs Target\")\n\nfig, ax = plt.subplots(1,2,figsize=(20,10))\nsns.kdeplot(data=text_props, x=sentence_count,color=palette[3],label=\"Excerpt\",ax=ax[0])\nax[0].set_title(\"Sentence count distribution\",font=\"Serif\")\nax[0].set_xlabel(\"sentence_count\")\nsns.scatterplot(data=text_props,x='sentence_count',y='target',color= palette[3],ax=ax[1],markers='.')\nax[1].set_title(\"Sentence count vs Target\",font=\"Serif\")\nplt.show()\n\nnum_cols = ['text_len','text_len_pre','lexicon_count','lexicon_count_pre','avg_text','avg_text_pre','sentence_count','target']\ncorr = text_props[num_cols].corr()\n\nfig = plt.figure(figsize=(12,12),dpi=80)\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, cmap='BuPu', robust=True, center=0,\n            square=True, linewidths=.5)\nplt.title('Correlation of text properties', fontsize=15,font=\"Serif\")\nplt.show()","522706b7":"plot_wb_hist(text_props,\"text_len\",\"Character Count Distribution\")\nplot_wb_hist(text_props,\"lexicon_count\",\"Word Count Distribution\")\nplot_wb_hist(text_props,\"avg_text\",\"Average Word Length Distribution\")\nplot_wb_hist(text_props,\"sentence_count\",\"Sentence Count Distribution\")","1c1d69ca":"text_props['pos_tags'] = text_props['excerpt_preprocessed'].str.split().map(pos_tag)\n\ndef count_tags(pos_tags):\n    tag_count = {}\n    for word,tag in pos_tags:\n        if tag in tag_count:\n            tag_count[tag] += 1\n        else:\n            tag_count[tag] = 1\n    return tag_count\n\ntext_props['tag_counts'] = text_props['pos_tags'].map(count_tags)","fcb1e171":"set_pos = set([tag for tags in text_props['tag_counts'] for tag in tags])\ntag_cols = list(set_pos)\n\nfor tag in tag_cols:\n    text_props[tag] = text_props['tag_counts'].map(lambda x: x.get(tag, 0))","d3d35d60":"pos = text_props[tag_cols].sum().sort_values(ascending = False)\nplt.figure(figsize=(16,10))\nax = sns.barplot(x=pos.index, y=pos.values,palette=\"Wistia\")\nplt.xticks(rotation = 50)\nax.set_yscale('log')\nplt.title('POS tags frequency',fontsize=15,font=\"Serif\")\nplt.show()","5ffebdb2":"pos_data = pd.DataFrame({'part_of_speech':pos.index, 'freq':pos.values})\nplot_wb_bar(pos_data,'part_of_speech', 'freq',\"POS\",\"POS tags frequency\")","eea1fe57":"plt.figure(figsize=(10,8))\nsns.scatterplot(data=text_props,x='NN',y='target',color= palette[3],markers='.',label=\"noun, singular\")\nsns.scatterplot(data=text_props,x='JJ',y='target',color= palette[4],markers='.',label=\"adjective\",)\nsns.scatterplot(data=text_props,x='VBD',y='target',color= palette[0],markers='.',label=\"verb past tense\")\nsns.scatterplot(data=text_props,x='RB',y='target',color= palette[1],markers='.',label=\"adverb\")\nplt.legend(title=\"POS tag\",bbox_to_anchor=(1.4, 1))\nplt.xlabel(\"POS tags count\")\nplt.title(\"POS vs Target\")\nplt.show()","b7b94e61":"toughest_excerpt = text_props[text_props[\"target\"] == text_props[\"target\"].min()].excerpt.values[0]\nlowest_target = text_props[text_props[\"target\"] == text_props[\"target\"].min()].target.values[0]\nnlp = spacy.load(\"en_core_web_sm\")\nsentences = sent_tokenize(toughest_excerpt)\nword_count = lambda sentence: len(word_tokenize(sentence))\npos_text = max(sentences, key=word_count)  \n\nconsole.print(\"Target of the toughest excerpt: \",style=\"info\")\nconsole.print(lowest_target,style='warning')\n\nconsole.print(\"Longest sentence of the toughest excerpt: \",style=\"info\")","dce0834d":"doc = nlp(pos_text)\ndisplacy.render(doc, style=\"dep\")","5d086571":"# flesch_re, flesch_kg, fog_scale, automated_r,coleman, linsear, text_standard  = ([] for i in range(7))\n# for i in range(len(text_props)):\n#     flr = textstat.flesch_reading_ease(train_df['excerpt'][i])\n#     flkg = textstat.flesch_kincaid_grade(train_df['excerpt'][i])\n#     fs = textstat.gunning_fog(train_df['excerpt'][i])\n#     ar = textstat.automated_readability_index(train_df['excerpt'][i])\n#     cole = textstat.coleman_liau_index(train_df['excerpt'][i])\n#     lins = textstat.linsear_write_formula(train_df['excerpt'][i])\n#     ts = textstat.text_standard(train_df['excerpt'][i])\n    \n#     flesch_re.append(flr)\n#     flesch_kg.append(flkg)\n#     fog_scale.append(fs)\n#     automated_r.append(ar)\n#     coleman.append(cole)\n#     linsear.append(lins)\n#     text_standard.append(ts)\n    \n# text_props['flesch_re'] = flesch_re\n# text_props['flesch_kg'] = flesch_kg\n# text_props['fog_scale'] = fog_scale\n# text_props['automated_r'] = automated_r\n# text_props['coleman'] = coleman\n# text_props['linsear'] = linsear\n# text_props['text_standard'] = text_standard","ddf1f5bf":"# #====== Saving to csv files and creating artifacts ======\n# text_props.to_csv(\"text_props_readability.csv\")\n\n# run = wandb.init(project='commonlit', name='text_props_readability')\n\n# artifact = wandb.Artifact('text_props_readability', type='dataset')\n\n# #====== Add a file to the artifact's contents ======\n# artifact.add_file(\"text_props_readability.csv\")\n\n# #====== Save the artifact version to W&B and mark it as the output of this run ====== \n# run.log_artifact(artifact)\n\n# run.finish()","ec7bbe9f":"run = wandb.init(project='commonlit')\nartifact = run.use_artifact('ruchi798\/commonlit\/text_props_readability:v0', type='dataset')\nartifact_dir = artifact.download()\nrun.finish()\n\npath = os.path.join(artifact_dir,\"text_props_readability.csv\")\ntext_props = pd.read_csv(path)\ntext_props = text_props.drop(columns=[\"Unnamed: 0\"])","b462a80f":"readability_cols = ['flesch_re','flesch_kg','fog_scale','automated_r','coleman','linsear','text_standard','target']\n\ncorr = text_props[readability_cols].corr()\nfig = plt.figure(figsize=(12,12),dpi=80)\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, cmap='PuBuGn', robust=True, center=0,\n            square=True, linewidths=.5,annot=True)\nplt.title('Correlation of readability tests', fontsize=15,font=\"Serif\")\nplt.show()","738ee6f8":"plt.figure(figsize=(10,8))\nsns.kdeplot(text_props[\"flesch_re\"],color=palette[4],shade=True)\nplt.title(\"Distribution of Flesch Reading Ease test\")\nplt.show()","b16238aa":"plot_wb_hist(text_props,\"flesch_re\",\"Flesch Reading Ease Distribution\")","f68b7df6":"text_props.loc[text_props['flesch_re'] > 60]['flesch_re'].count() \/ len(text_props) *100","ed714159":"pd.set_option('display.max_colwidth', None)\nmax_text = text_props[text_props[\"target\"] == text_props[\"target\"].max()]['excerpt']\nmin_text = text_props[text_props[\"target\"] == text_props[\"target\"].min()]['excerpt']\n\nmax_text_f = text_props[text_props[\"flesch_re\"] == text_props[\"flesch_re\"].max()]['excerpt']\nmin_text_f = text_props[text_props[\"flesch_re\"] == text_props[\"flesch_re\"].min()]['excerpt']\n\nconsole.print(\"Highest Target\", style=\"danger\")\nconsole.print(max_text, style=\"info\")\ntext_props[text_props[\"target\"] == text_props[\"target\"].max()][['flesch_re','target','text_standard']]","e8f961da":"console.print(\"Highest Flesch Reading Ease Score\", style=\"danger\")\nconsole.print(max_text_f, style=\"info\")\ntext_props[text_props[\"flesch_re\"] == text_props[\"flesch_re\"].max()][['flesch_re','target','text_standard']]","0785b602":"console.print(\"Lowest Target\", style=\"danger\")\nconsole.print(min_text, style=\"warning\")\ntext_props[text_props[\"target\"] == text_props[\"target\"].min()][['flesch_re','target','text_standard']]","fda5a1bf":"console.print(\"Lowest Flesch Reading Ease Score\", style=\"danger\")\nconsole.print(min_text_f, style=\"warning\")\ntext_props[text_props[\"flesch_re\"] == text_props[\"flesch_re\"].min()][['flesch_re','target','text_standard']]","5221b957":"def training(model, X_train, y_train, X_test, y_test, model_name):\n    t1 = time.time()\n    \n    model = make_pipeline(\n        TfidfVectorizer(binary=True, ngram_range=(1,1)),\n        model,\n    )\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    MSE = mse(y_test, y_pred)\n    \n    t2 = time.time()\n    training_time = t2-t1 \n    \n    console.print(\"--- Model:\", model_name,\"---\",style='warning')\n    console.print(\"MSE: \",MSE,style='danger')\n    console.print(\"Training time:\",training_time,style='danger')\n\nridge = Ridge(fit_intercept = True, normalize = False)\nlr = LinearRegression()\nm = [ridge,lr]\nmn = [\"Ridge Regression\",\"Linear Regression\"]\n\nX = train_df[\"excerpt_preprocessed\"]\ny = train_df['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nfor i in range(0,len(m)):\n    training(model=m[i], X_train=X_train, y_train=y_train, X_test=X_test,y_test=y_test, model_name=mn[i])","37949f6d":"def training_all(model,X,y):\n    \n    model = make_pipeline(\n        TfidfVectorizer(binary=True, ngram_range=(1,1)),\n        model,\n    )\n    model.fit(X, y)\n    y_pred = model.predict(test_df[\"excerpt_preprocessed\"])\n    \n    return y_pred","74681662":"test_pred = training_all(lr,X,y)\npredictions = pd.DataFrame()\npredictions['id'] = test_df['id']\npredictions['target'] = test_pred\npredictions.to_csv(\"\/kaggle\/working\/submission.csv\", index=False)\npredictions","0063b5c6":"\u0420\u0435\u0433\u0438\u0441\u0442\u0440\u0430\u0446\u0438\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u0445 \u0433\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c \u0434\u043b\u044f \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0441\u0438\u043c\u0432\u043e\u043b\u043e\u0432, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0441\u043b\u043e\u0432, \u0441\u0440\u0435\u0434\u043d\u0435\u0439 \u0434\u043b\u0438\u043d\u044b \u0441\u043b\u043e\u0432\u0430 \u0438 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439 \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f","f3b0fc7f":"<img src=\"https:\/\/i.imgur.com\/mi9U6o5.png\">","8343038e":"\u0421\u043d\u0438\u043c\u043e\u043a \u0432\u043d\u043e\u0432\u044c \u0441\u043e\u0437\u0434\u0430\u043d\u043d\u044b\u0445 \u0430\u0440\u0442\u0435\u0444\u0430\u043a\u0442\u043e\u0432  \u2b07\ufe0f\n\n<img src=\"https:\/\/i.imgur.com\/WFeQRt7.png\">","9cea0395":"\u0418\u043b\u043b\u044e\u0441\u0442\u0440\u0430\u0446\u0438\u0438 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u044b \u26a1\n\n- [Canva](https:\/\/www.canva.com\/) \ud83d\udd8c\ufe0f\n\n<img src=\"https:\/\/i.imgur.com\/pl3FhXV.png\">","4d707828":"| Abbreviation    | Meaning                                              |\n|-----------------|------------------------------------------------------|\n| CC              | coordinating conjunction                             |\n| CD              | cardinal digit                                       |\n| DT              | determiner                                           |\n| EX              | existential there                                    |\n| FW              | foreign word                                         |\n| IN              | preposition\/subordinating conjunction                |\n| JJ              | adjective (large)                                    |\n| JJR             | adjective, comparative (larger)                      |\n| JJS             | adjective, superlative (largest)                     |\n| LS              | list item marker                                     |\n| MD              | modal (could, will)                                  |\n| NN              | noun, singular                                       |\n| NNS             | noun plural                                          |\n| NNP             | proper noun, singular                                |\n| NNPS            | proper noun, plural                                  |\n| PDT             | predeterminer                                        |\n| POS             | possessive ending (parent\\ 's)                       |\n| PRP             | personal pronoun (hers, herself, him,himself)        |\n| PRP dollar-sign | possessive pronoun (her, his, mine, my, our )        |\n| RB              | adverb (occasionally, swiftly)                       |\n| RBR             | adverb, comparative (greater)                        |\n| RBS             | adverb, superlative (biggest)                        |\n| RP              | particle (about)                                     |\n| SYM             | symbol                                               |\n| TO              | infinite marker (to)                                 |\n| UH              | interjection (goodbye)                               |\n| VB              | verb (ask)                                           |\n| VBG             | verb gerund (judging)                                |\n| VBD             | verb past tense (pleaded)                            |\n| VBN             | verb past participle (reunified)                     |\n| VBP             | verb, present tense not 3rd person singular(wrap)    |\n| VBZ             | verb, present tense with 3rd person singular (bases) |\n| WDT             | wh-determiner (that, what)                           |\n| WP              | wh- pronoun (who)                                    |\n| WP dollar-sign  | possessive wh-pronoun                                |\n| WRB             | wh- adverb (how)                                     |\n\n\ud83d\udccc \u0427\u0435\u043c \u0432\u044b\u0448\u0435 \u043e\u0446\u0435\u043d\u043a\u0430, \u0442\u0435\u043c \u0441\u043b\u043e\u0436\u043d\u0435\u0435 \u0433\u0440\u0430\u043c\u043c\u0430\u0442\u0438\u043a\u0430 (?)\n\n[Penn Part of Speech Tags](https:\/\/cs.nyu.edu\/~grishman\/jet\/guide\/PennPOS.html)","7cee3198":"<img src=\"https:\/\/camo.githubusercontent.com\/dd842f7b0be57140e68b2ab9cb007992acd131c48284eaf6b1aca758bfea358b\/68747470733a2f2f692e696d6775722e636f6d2f52557469567a482e706e67\">\n\n> \u042f \u0431\u0443\u0434\u0443 \u0438\u043d\u0442\u0435\u0433\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c W&B \u0434\u043b\u044f \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0438 \u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0430\u0446\u0438\u0438 \u0430\u0440\u0442\u0435\u0444\u0430\u043a\u0442\u043e\u0432!\n> \n> [\u041f\u0440\u043e\u0435\u043a\u0442 CommonLit \u043d\u0430 \u043f\u0430\u043d\u0435\u043b\u0438 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u043e\u0432 W&B](https:\/\/wandb.ai\/ruchi798\/commonlit?workspace=user-ruchi798) \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f\n> \n> - \u0427\u0442\u043e\u0431\u044b \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043a\u043b\u044e\u0447 API, \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u0443\u0447\u0435\u0442\u043d\u0443\u044e \u0437\u0430\u043f\u0438\u0441\u044c \u043d\u0430 [\u0432\u0435\u0431-\u0441\u0430\u0439\u0442\u0435](https:\/\/wandb.ai\/site).\n> - \u0417\u0430\u0442\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0439\u0442\u0435 \u0441\u0435\u043a\u0440\u0435\u0442\u044b \u0434\u043b\u044f \u0431\u043e\u043b\u0435\u0435 \u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0433\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u043a\u043b\u044e\u0447\u0435\u0439 API \ud83e\udd2b","a3bc05fc":"# \u0411\u0430\u0437\u043e\u0432\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c \u2699\ufe0f","6bf944bd":"# \u0412\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \ud83d\udcdd\n\ud83c\udfaf **\u0426\u0435\u043b\u044c:** \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u044b \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u0441\u043b\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u043e\u0442\u0440\u044b\u0432\u043a\u043e\u0432 \u0432\u043e \u0432\u0440\u0435\u043c\u044f \u0447\u0442\u0435\u043d\u0438\u044f, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c  \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0430  \u0441 3 \u043f\u043e 12-\u0445 \u043a\u043b\u0430\u0441\u0441\u0430\u0445. \n\n\ud83d\udcd6 **\u0414\u0430\u043d\u043d\u044b\u0435:** \n> **train.csv \/ test.csv** - \u043d\u0430\u0431\u043e\u0440 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f\n> - ```id``` - \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0439 \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u043e\u0442\u0440\u044b\u0432\u043a\u0430\n> - ```url_legal``` - URL-\u0430\u0434\u0440\u0435\u0441 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u0430\n> - ```license``` - \u043b\u0438\u0446\u0435\u043d\u0437\u0438\u044f \u043d\u0430 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0439 \u043c\u0430\u0442\u0435\u0440\u0438\u0430\u043b\n> - ```excerpt``` - \u0442\u0435\u043a\u0441\u0442, \u0447\u0442\u043e\u0431\u044b \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u043b\u0435\u0433\u043a\u043e\u0441\u0442\u044c \u0447\u0442\u0435\u043d\u0438\u044f\n> - ```target``` - \u043b\u0435\u0433\u043a\u043e\u0441\u0442\u044c \u0447\u0442\u0435\u043d\u0438\u044f\n> - ```standard_error``` -  \u043c\u0435\u0440\u0430 \u0440\u0430\u0437\u0431\u0440\u043e\u0441\u0430 \u043e\u0446\u0435\u043d\u043e\u043a \u043c\u0435\u0436\u0434\u0443 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u043c\u0438 \u043e\u0446\u0435\u043d\u0449\u0438\u043a\u0430\u043c\u0438 \u0437\u0430 \u043a\u0430\u0436\u0434\u044b\u0439 \u043e\u0442\u0440\u044b\u0432\u043e\u043a\n\n\ud83d\udccc **\u041f\u0440\u0438\u043c\u0435\u0447\u0430\u043d\u0438\u0435:** ```url_legal```, ```license``` \u0438 ```standard error``` \u043d\u0435 \u0443\u043a\u0430\u0437\u0430\u043d\u044b \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u043c \u043d\u0430\u0431\u043e\u0440\u0435.\n\n\ud83e\uddea **\u041f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u044c \u043e\u0446\u0435\u043d\u043a\u0438:** Root Mean Squared Error (RMSE)\n> $$RMSE = \\sqrt{\\frac{1}{n}\\Sigma_{i=1}^{n}{\\Big(\\frac{y_i - \\hat{y_i}}{\\sigma_i}\\Big)^2}}$$\n> \u0413\u0434\u0435\n> * $y_i$ : \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\n> * $\\hat{y_i}$ : \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u0443\u0435\u043c\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\n> * $n$ : \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u0442\u0440\u043e\u043a \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445","4cd1a094":"\u0412\u0435\u0434\u0435\u043d\u0438\u0435 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u043e\u0439 \u0433\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c\u044b \u0434\u043b\u044f \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043e\u0446\u0435\u043d\u043e\u043a Flesch Reading Ease \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f","55be7018":" \u0412\u0435\u0434\u0435\u043d\u0438\u0435 \u0436\u0443\u0440\u043d\u0430\u043b\u0430 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u0445 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f","5df3b25f":"\u0420\u0435\u0433\u0438\u0441\u0442\u0440\u0430\u0446\u0438\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u043e\u0439 \u0433\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c\u044b \u0434\u043b\u044f \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043b\u0438\u0446\u0435\u043d\u0437\u0438\u0439 \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f","9f7538d1":"# \u041e\u0442\u0440\u044b\u0432\u043e\u043a \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u2702\ufe0f","22dc508a":"# \u041e\u0442\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0439 \u0444\u0430\u0439\u043b \ud83d\udcdd","574bf074":"\u0421\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442 \u0441\u0438\u043b\u044c\u043d\u0430\u044f \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u044f \u043c\u0435\u0436\u0434\u0443 \u043d\u0430\u0448\u0435\u0439 ```target``` \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438 \u0442\u0435\u0441\u0442\u0430 ```Flesch Readability Ease```.\n\n\u0414\u0430\u0432\u0430\u0439\u0442\u0435 \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0434\u043b\u044f ```Flesch Readability Ease```.\n\n| Score          | Notes                                                                  |\n|----------------|------------------------------------------------------------------------|\n| 90-100         | very easy to read, easily understood by an average 11-year-old student |\n| 80-90          | easy to read                                                           |\n| 70-80          | fairly easy to read                                                    |\n| 60-70          | easily understood by 13- to 15-year-old students                       |\n| 50-60          | fairly difficult to read                                               |\n| 30-50          | difficult to read, best understood by college graduates                |\n| 0-30           | very difficult to read, best understood by university graduates        |","c099a537":"# \u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 \ud83d\udcda","fd8e3756":"# \u0422\u0435\u0441\u0442\u044b \u0447\u0430\u0441\u0442\u043e\u0442\u044b \u0447\u0442\u0435\u043d\u0438\u044f  \ud83e\uddea\n\n[textstat](https:\/\/pypi.org\/project\/textstat\/) \u044d\u0442\u043e \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0430, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0430\u044f \u0434\u043b\u044f \u0440\u0430\u0441\u0447\u0435\u0442\u0430 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0438 \u043f\u043e \u0442\u0435\u043a\u0441\u0442\u0443. \u041e\u043d \u043e\u0447\u0435\u043d\u044c \u043f\u0440\u0438\u0433\u043e\u0434\u0438\u043b\u0441\u044f \u0434\u043b\u044f \u043f\u043e\u0434\u0441\u0447\u0435\u0442\u0430 \u0431\u0430\u043b\u043b\u043e\u0432 \u043f\u043e \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u043c \u0442\u0435\u0441\u0442\u0430\u043c \u043d\u0430 \u0447\u0430\u0441\u0442\u043e\u0442\u0443 \u0447\u0442\u0435\u043d\u0438\u044f!\n\n- ```flesch_re:``` [The Flesch Reading Ease formula](https:\/\/en.wikipedia.org\/wiki\/Flesch%E2%80%93Kincaid_readability_tests#Flesch_reading_ease)\n- ```flesch_kg:``` [The Flesch-Kincaid Grade Level ](https:\/\/en.wikipedia.org\/wiki\/Flesch%E2%80%93Kincaid_readability_tests#Flesch_reading_ease)\n- ```fog_scale:``` [The Fog Scale (Gunning FOG Formula)](https:\/\/en.wikipedia.org\/wiki\/Gunning_fog_index)\n- ```automated_r:``` [Automated Readability Index](https:\/\/en.wikipedia.org\/wiki\/Automated_readability_index)\n- ```coleman:``` [The Coleman-Liau Index](https:\/\/en.wikipedia.org\/wiki\/Coleman%E2%80%93Liau_index)\n- ```linsear:``` [Linsear Write Formula](https:\/\/en.wikipedia.org\/wiki\/Linsear_Write)\n- ```text_standard:``` Readability Consensus based upon all the above tests","903358f3":"\u0412\u043e\u0442 \u0441\u043d\u0438\u043c\u043e\u043a \u043c\u043e\u0435\u0433\u043e [\u043f\u0440\u043e\u0435\u043a\u0442\u0430](https:\/\/wandb.ai\/ruchi798\/commonlit?workspace=user-ruchi798) \u2b07\ufe0f\n\n<img src=\"https:\/\/i.imgur.com\/vmxri2T.png\">","0167d82c":"\u0420\u0435\u0433\u0438\u0441\u0442\u0440\u0430\u0446\u0438\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u0445 \u0433\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c \u0434\u043b\u044f \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0441\u0438\u043c\u0432\u043e\u043b\u043e\u0432, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0441\u043b\u043e\u0432, \u0441\u0440\u0435\u0434\u043d\u0435\u0439 \u0434\u043b\u0438\u043d\u044b \u0441\u043b\u043e\u0432\u0430 \u0438 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439 \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f","0e58b028":"\u0417\u0434\u0435\u0441\u044c \u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b \u043c\u043e\u0434\u0443\u043b\u044c viz \u043f\u043e\u0434 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435\u043c [missingno](https:\/\/pypi.org\/project\/missingno\/) \u0434\u043b\u044f \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0432 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u043c \u043d\u0430\u0431\u043e\u0440\u0435.\n\n\u0423 \u043d\u0430\u0441 \u043d\u0435\u0442 \u043f\u0440\u043e\u043f\u0443\u0449\u0435\u043d\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0432 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u0443\u044e\u0449\u0438\u0445 \u0432\u0430\u0441 \u0441\u0442\u043e\u043b\u0431\u0446\u0430\u0445,\u0442\u043e \u0435\u0441\u0442\u044c ```excerpt```, ```target``` \u0438 ```standard_error```!","7eceddaa":"# EDA \ud83d\udcca","af67454e":"\ud83d\udccc \u0411\u043e\u043b\u0435\u0435 **70%** \u043e\u0442\u0440\u044b\u0432\u043a\u043e\u0432 \u043f\u043e\u043d\u044f\u0442\u043d\u044b \u0434\u0435\u0442\u044f\u043c **13-15 \u043b\u0435\u0442**.","683d65bb":"# \u041e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\ud83d\udd2e","eb06f12d":"\u041f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 \u044f \u0443\u0436\u0435 \u0437\u0430\u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u043b \u0430\u0440\u0442\u0435\u0444\u0430\u043a\u0442, \u044f \u043c\u043e\u0433\u0443 \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0435\u0433\u043e \u0442\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c \u2b07\ufe0f","3d76f80b":"\ud83d\udccc ```sentence_count``` \u0438```target``` \u043e\u0447\u0435\u043d\u044c \u0441\u0438\u043b\u044c\u043d\u043e \u043a\u043e\u0440\u0440\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u044b, \u043f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 \u043e\u0447\u0435\u043d\u044c \u0434\u043b\u0438\u043d\u043d\u044b\u0435 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u0441\u043b\u043e\u0436\u043d\u044b\u043c\u0438 \u0434\u043b\u044f \u0447\u0442\u0435\u043d\u0438\u044f \u0438 \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u044f.","6e3f306e":"\u0414\u0430\u0432\u0430\u0439\u0442\u0435 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c, \u043a\u0430\u043a\u0438\u0435 \u043e\u0442\u0440\u044b\u0432\u043a\u0438 \u0438\u043c\u0435\u044e\u0442 \u0441\u0430\u043c\u044b\u0439 \u0432\u044b\u0441\u043e\u043a\u0438\u0439 \u0438 \u0441\u0430\u043c\u044b\u0439 \u043d\u0438\u0437\u043a\u0438\u0439 \u0431\u0430\u043b\u043b ```Target``` \u0438 ```Flesch Reading Ease Score``` \u2b07\ufe0f","2903771a":"\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u043d\u043e\u0433\u043e \u0430\u0440\u0442\u0435\u0444\u0430\u043a\u0442\u0430 \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f","9218b5e0":"# \u0422\u0435\u0433\u0438 \u0447\u0430\u0441\u0442\u0438 \u0440\u0435\u0447\u0438  \ud83c\udff7\ufe0f","8f750ae3":"\u0420\u0435\u0433\u0438\u0441\u0442\u0440\u0430\u0446\u0438\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u043e\u0439 \u0433\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c\u044b \u0434\u043b\u044f \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f POS \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f","814b0d00":"\u041f\u0440\u0435\u0436\u0434\u0435 \u0447\u0435\u043c \u043c\u044b \u043f\u0440\u043e\u0434\u043e\u043b\u0436\u0438\u043c, \u0432\u0430\u0436\u043d\u043e \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u043e\u0442\u0440\u044b\u0432\u043e\u043a!","eec2b564":"\u0420\u0435\u0433\u0438\u0441\u0442\u0440\u0430\u0446\u0438\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u0445 \u0433\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c \u0434\u043b\u044f \u0446\u0435\u043b\u0435\u0432\u043e\u0433\u043e \u0438 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0433\u043e \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043e\u0448\u0438\u0431\u043e\u043a \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f","39cf9318":"\u0420\u0435\u0433\u0438\u0441\u0442\u0440\u0430\u0446\u0438\u044f \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441\u0432\u043e\u0439\u0441\u0442\u0432 \u0442\u0435\u043a\u0441\u0442\u0430 \u043a\u0430\u043a \u0430\u0440\u0442\u0435\u0444\u0430\u043a\u0442\u0430 \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f\n\n\u042d\u0442\u043e \u043f\u043e\u043c\u043e\u0433\u0430\u0435\u0442 \u043c\u043d\u0435 \u0441\u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0442\u044c \u0432\u0440\u0435\u043c\u044f, \u0442\u0430\u043a \u043a\u0430\u043a \u044f \u043c\u043e\u0433\u0443 \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u043d\u044b\u0439 \u0430\u0440\u0442\u0435\u0444\u0430\u043a\u0442 \u0432 \u0441\u0432\u043e\u0435\u043c \u0440\u0430\u0431\u043e\u0447\u0435\u043c \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0435  "}}