{"cell_type":{"f6070275":"code","1f6607c8":"code","3c4ce7c0":"code","da97f58a":"code","bd9ffecf":"code","cb9abe90":"code","1ced1d94":"code","e7e7d075":"code","ec28c038":"code","f79bbd8c":"code","ccfffe3e":"code","b6bf0a4a":"code","5a7a06e4":"code","8ba00d96":"code","00907bf4":"code","cb17d329":"code","61778e28":"code","8efaf4fd":"code","e03de38d":"code","95671767":"code","fe9ec84f":"code","e27f8c4d":"code","ae69a65f":"code","b7e107a6":"code","7cc30e52":"code","d07594e6":"code","e19b28eb":"code","e1f97576":"code","b9dfe961":"code","70289148":"code","47d0d117":"code","9a3b53ec":"code","f2408622":"code","f1f2b488":"markdown","76585166":"markdown","ba58b280":"markdown","8cb8cb7b":"markdown","35adc596":"markdown","3bf18d0e":"markdown","9ee91016":"markdown","64b0d8d5":"markdown","fe5f45de":"markdown","77fdc307":"markdown","4896bdc0":"markdown","0f4b7ed2":"markdown","91d2c400":"markdown","97143237":"markdown","36fbe7ee":"markdown","15beca47":"markdown","30015659":"markdown","10db25e3":"markdown","e60cbea2":"markdown","2b1e322a":"markdown"},"source":{"f6070275":"from IPython.display import YouTubeVideo","1f6607c8":"\nYouTubeVideo('Vk2jDXxZIhs', width=800, height=500)","3c4ce7c0":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n%matplotlib inline \n\n# Sklearn imports \nfrom sklearn.linear_model import Lasso,Ridge, LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn import neighbors","da97f58a":"data=pd.read_csv('\/kaggle\/input\/electric-motor-temperature\/pmsm_temperature_data.csv')","bd9ffecf":"data.head(20)\n","cb9abe90":"data.shape","1ced1d94":"data.isnull().sum()\n\n# No Nulls values are detected ","e7e7d075":"#data['profile_id'].value_counts().sort_index()\n\n#Un commnet to see the values under each profile_id ","ec28c038":"plt.figure(figsize=(15,8));\n\nsns.barplot(x=data.groupby('profile_id').agg('count').sort_values(by='pm').index,\n            y=data.groupby('profile_id').agg('count').sort_values(by='pm')['pm'],\n            order=data.groupby('profile_id').agg('count').sort_values(by='pm').index,\n           orient='v',color='orange');\n\nplt.title('Profile_id vs data points');","f79bbd8c":"X_test=data[(data['profile_id']==65) | (data['profile_id']==72)]\nX_train=data[(data['profile_id']!=65) | (data['profile_id']!=72)]","ccfffe3e":"# Let see how the other features are colrelated with each other \n\ndata.columns","b6bf0a4a":"columns=['ambient', 'coolant', 'u_d', 'u_q', 'motor_speed', 'i_d',\n       'i_q', 'pm', 'stator_yoke', 'stator_tooth', 'stator_winding','torque']\n\ncorr=data[columns].corr()\n\nplt.figure(figsize=(12,10))\nsns.heatmap(corr,annot=True);","5a7a06e4":"#Change the profile ID number \n\nplt.figure(figsize=(20,5))\ndata[data['profile_id'] == 80]['stator_yoke'].plot(label = 'stator yoke')\ndata[data['profile_id'] == 80]['stator_tooth'].plot(label = 'stator tooth')\ndata[data['profile_id'] == 80]['stator_winding'].plot(label = 'stator winding')\nplt.legend()","8ba00d96":"data.describe()","00907bf4":"X_train.head()","cb17d329":"X=X_train[['ambient','coolant','u_d','u_q','motor_speed','torque','i_d','i_q']]\ny=X_train['pm']\n\ny_test_rotor=X_test['pm']\nX_test_rotor=X_test[['ambient','coolant','u_d','u_q','motor_speed','torque','i_d','i_q']]\n\n\n#from sklearn.model_selection import train_test_split\n\n\n#X_train,X_val,y_train,y_val=train_test_split(X,y,shuffle=True,random_state=7)","61778e28":"\n\nlr=LinearRegression(normalize=False)\n\nkfold=model_selection.KFold(n_splits=10,shuffle=True,random_state=101)\n\nlr_scores=[]\n\nlr_scores=cross_val_score(lr,X,y,cv=kfold,n_jobs=-1,scoring='neg_root_mean_squared_error')\n\n    \nprint('RMSE for Linear Regression is {0:.3f} and variance of MSE is {1:.8f}'.format(-1*np.mean(lr_scores),np.var(lr_scores)))\n","8efaf4fd":"\nridge=Ridge()\nparam_grid={\n    'alpha':[0.001,0.01,1,5,10,20],\n    'normalize' : [True,False],\n    'fit_intercept':[True,False]\n}\n\nkfold=model_selection.KFold(n_splits=10,shuffle=True,random_state=101)\n\nmodel=model_selection.RandomizedSearchCV(\n                        estimator=ridge,\n                        param_distributions=param_grid,\n                        n_iter=20,\n                        scoring='neg_root_mean_squared_error',\n                        cv=kfold,\n                        refit=True,\n                        verbose=5,\n                        n_jobs=-1\n                        )\n\nmodel.fit(X,y)\n\nprint('Best Scorer{}'.format(model.best_score_))\n\nprint('\/n')\n\nprint('Best Parameters{}'.format(model.best_params_))\n\n","e03de38d":"# implementing Ridge Regression \n\nridge=Ridge(alpha=5,fit_intercept=True,normalize=False)\n\nkfold=model_selection.KFold(n_splits=10,shuffle=True,random_state=101)\n\nridge_scores=[]\n\nridge_scores=cross_val_score(ridge,X,y,cv=kfold,n_jobs=-1,scoring='neg_root_mean_squared_error')\n\n    \nprint('RMSE for Ridge Regression is {0:.5f} and variance of MSE is {1:.8f}'.format(-1*np.mean(ridge_scores),np.var(ridge_scores)))","95671767":"lasso=Lasso()\n\nparam_grid={\n    'alpha':[0.001,0.01,1,5,10,20],\n    'normalize' : [True,False],\n    'fit_intercept':[True,False]\n}\n\nkfold=model_selection.KFold(n_splits=10,shuffle=True,random_state=101)\n\nmodel=model_selection.RandomizedSearchCV(\n                        estimator=lasso,\n                        param_distributions=param_grid,\n                        n_iter=20,\n                        scoring='neg_mean_squared_error',\n                        cv=kfold,\n                        refit=True,\n                        verbose=5,\n                        n_jobs=-1\n                        )\n\nmodel.fit(X,y)\n\nprint('Best Scorer{}'.format(model.best_score_))\n\nprint('\/n')\n\nprint('Best Parameters{}'.format(model.best_params_))\n\n","fe9ec84f":" \n\nlasso=Lasso(alpha=0.001,fit_intercept=True,normalize=False)\n\nkfold=model_selection.KFold(n_splits=10,shuffle=True,random_state=101)\n\nlasso_scores=[]\n\nlasso_scores=cross_val_score(lasso,X,y,cv=kfold,n_jobs=-1,scoring='neg_root_mean_squared_error')\n\n    \nprint('RMSE for Lasso Regression is {0:.5f} and variance of MSE is {1:.8f}'.format(-1*np.mean(lasso_scores),np.var(lasso_scores)))","e27f8c4d":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nlasso_pipeline=Pipeline([\n    ('poly',PolynomialFeatures(2)),\n    ('scalar',StandardScaler())\n])\n\n\n\nX_poly=lasso_pipeline.fit_transform(X)\n\n# implementing Lasso Regression \n\nlasso=Lasso(alpha=0.001,fit_intercept=True,normalize=False)\n\nkfold=model_selection.KFold(n_splits=10,shuffle=True,random_state=101)\n\nlasso_scores=[]\n\nlasso_scores=cross_val_score(lasso,X_poly,y,cv=kfold,n_jobs=-1,scoring='neg_root_mean_squared_error')\n\n    \nprint('RMSE for Lasso Regression is {0:.5f} and variance of MSE is {1:.8f}'.format(-1*np.mean(lasso_scores),np.var(lasso_scores)))","ae69a65f":"from sklearn import neighbors\n\nknn=neighbors.KNeighborsRegressor()\n\nparam_grid={\n    'n_neighbors':np.arange(1,10),\n    'weights':['uniform', 'distance']   \n    \n}\n\nkfold=model_selection.KFold(n_splits=5,shuffle=True,random_state=101)\n\nmodel=model_selection.GridSearchCV(\n                        estimator=knn,\n                        param_grid=param_grid,\n                        scoring='neg_root_mean_squared_error',\n                        cv=kfold,\n                        refit=True,\n                        verbose=5,\n                        n_jobs=-1\n                        )\nmodel.fit(X,y)\n\nprint('Best Scorer{}'.format(model.best_score_))\n\nprint('\/n')\n\nprint('Best Parameters{}'.format(model.best_params_))\n","b7e107a6":"knn=neighbors.KNeighborsRegressor(n_neighbors=3,weights='distance')\n\nkfold=model_selection.KFold(n_splits=10,shuffle=True,random_state=101)\n\nknn_scores=[]\n\nknn_scores=cross_val_score(knn,X,y,cv=kfold,n_jobs=-1,scoring='neg_root_mean_squared_error')\n\n    \nprint('RMSE for KNN Regression is {0:.5f} and variance of MSE is {1:.8f}'.format(-1*np.mean(knn_scores),np.var(knn_scores)))","7cc30e52":"# As the KNN is so far the best one using KNN with sklearn BaggingClassifier class ","d07594e6":"#from sklearn.ensemble import BaggingRegressor\n#from sklearn import neighbors\n\n#knn_bag=BaggingRegressor(neighbors.KNeighborsRegressor(n_neighbors=3),\n                          #max_samples=1.0, \n                          #max_features=1.0,\n                         # oob_score=True,\n                          #bootstrap=True)\n\n#param_dist={\n#    'n_estimators':np.arange(10,120,10)\n#}\n\n#model=model_selection.GridSearchCV(\n#                        estimator=knn_bag,\n #                       param_grid=param_dist,\n#            \n #                       scoring='neg_mean_squared_error',\n#                        cv=5,\n  #                      refit=True,\n   #                     verbose=5,\n     #                   n_jobs=-1\n    #                    )\n\n\n#model.fit(X,y)\n\n#print('Best Scorer{}'.format(model.best_score_))\n\n#print('\/n')\n\n#print('Best Parameters{}'.format(model.best_params_))","e19b28eb":"# Random forest Regressor \n\n#from sklearn import ensemble\n\n#random_forest=ensemble.RandomForestRegressor(criterion='mse',random_state=101,n_jobs=-1)\n\n#param_distribution={\n#    'n_estimators':np.arange(100,500,50),\n#    'max_depth':[2,4,6,8,'None'], \n#    'min_samples_split':np.arange(2,20,2), \n#    'warm_start':[False,True]\n#}\n\n\n#kfold=model_selection.KFold(n_splits=10,shuffle=True,random_state=101)\n\n#model=model_selection.RandomizedSearchCV(\n #                       estimator=random_forest,\n #                       param_distributions=param_distribution,\n #                       n_iter=80,\n #                       scoring='neg_mean_squared_error',\n #                       cv=kfold,\n                       # refit=True,\n                       # verbose=5,\n                       # n_jobs=-1\n                       # )\n\n#model.fit(X,y)\n\n#print('Best Scorer{}'.format(model.best_score_))\n\n#print('\/n')\n\n#print('Best Parameters{}'.format(model.best_params_))","e1f97576":"# Working on XGboost \nimport xgboost as xgb \n\ndata_dmatrix = xgb.DMatrix(data=X,label=y)\n\nparams = {\"objective\":\"reg:squarederror\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n                'max_depth': 5, 'alpha': 10}\n\ncv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n                    num_boost_round=1500,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)\n\ncv_results.tail()","b9dfe961":"# Implementing pipeline \n#from sklearn.pipeline import Pipeline\n#from sklearn.ensemble import VotingRegressor\n#import xgboost as xgb \n\n\n#votingregressor = VotingRegressor([('lasso',Lasso(alpha=0.001,fit_intercept=True,normalize=False)), \n  #                    ('knn', neighbors.KNeighborsRegressor(n_neighbors=3,weights='distance')),\n  #                    ('xgb',xgb.XGBRegressor(objective='reg:squarederror',colsample_bytree=0.3,learning_rate=0.1,max_depth=5,alpha=10,n_rounds=1500))\n    #                  ])\n\n\n#vot_pipeline=Pipeline([\n  #  ('poly',PolynomialFeatures(2)),\n  #  ('scalar',StandardScaler()),\n  #  ('voting',votingregressor)\n#])\n\n#kfold=model_selection.KFold(n_splits=10,shuffle=True,random_state=101)\n\n#vot_scores=[]\n\n#vot_scores=cross_val_score(vot_pipeline,X,y,cv=kfold,n_jobs=-1,scoring='neg_root_mean_squared_error')\n\n#-1*np.mean(vot_scores)","70289148":"KNN_rotor=neighbors.KNeighborsRegressor(n_neighbors=3,weights='distance')\n\n\nKNN_rotor.fit(X,y)\n\ny_rotor_predict=KNN_rotor.predict(X_test_rotor)\n\nprint(\"The RMSE for rotor temperature is {}\".format(metrics.mean_squared_error(y_test_rotor,y_rotor_predict)))\n","47d0d117":"X_stator_train=X_train[['ambient','coolant','u_d','u_q','motor_speed','torque','i_d','i_q']]\ny_stator_train=X_train['stator_tooth']\n\ny_stator_test=X_test['stator_tooth']\nX_stator_test=X_test[['ambient','coolant','u_d','u_q','motor_speed','torque','i_d','i_q']]","9a3b53ec":"from sklearn import neighbors\n\nknn=neighbors.KNeighborsRegressor()\n\nparam_grid={\n    'n_neighbors':np.arange(1,15),\n    'weights':['uniform', 'distance']   \n    \n}\n\nkfold=model_selection.KFold(n_splits=5,shuffle=True,random_state=101)\n\nstator_model=model_selection.GridSearchCV(\n                        estimator=knn,\n                        param_grid=param_grid,\n                        scoring='neg_root_mean_squared_error',\n                        cv=kfold,\n                        refit=True,\n                        verbose=5,\n                        n_jobs=-1\n                        )\nstator_model.fit(X_stator_train,y_stator_train)\n\nprint('Best Scorer{}'.format(stator_model.best_score_))\n\nprint('\/n')\n\nprint('Best Parameters{}'.format(stator_model.best_params_))","f2408622":"KNN_stator=neighbors.KNeighborsRegressor(n_neighbors=3,weights='distance')\n\n\nKNN_stator.fit(X_stator_train,y_stator_train)\n\ny_stator_predict=KNN_stator.predict(X_stator_test)\n\nprint(\"The RMSE for rotor temperature is {}\".format(metrics.mean_squared_error(y_stator_test,y_stator_predict)))\n","f1f2b488":"# Final Working Model for Predicting for Predicting Rotor Temperature ","76585166":"## Implementing the best fit Ridge model from above ","ba58b280":"# Implementing Model for Stator tooth \nthe other stators temperature should be around this ballpark temperature\n","8cb8cb7b":"# How the data was collected \n\nAll recordings are sampled at 2 Hz. The data set consists of multiple measurement sessions, which can be distinguished from each other by column \"profile_id\". A measurement session can be between one and six hours long.\n\nThe motor is excited by hand-designed driving cycles denoting a reference motor speed and a reference torque.\nCurrents in d\/q-coordinates (columns \"id\" and iq\") and voltages in d\/q-coordinates (columns \"ud\" and \"uq\") are a result of a standard control strategy trying to follow the reference speed and torque.\nColumns \"motor_speed\" and \"torque\" are the resulting quantities achieved by that strategy, derived from set currents and voltages.\n\nMost driving cycles denote random walks in the speed-torque-plane in order to imitate real world driving cycles to a more accurate degree than constant excitations and ramp-ups and -downs would.","35adc596":"# Establishing a baseline using Linear Regression ","3bf18d0e":"## Most Linear Regression model gives us RMSE value close by so this can serve as good baseline model ","9ee91016":"# Aim of this notebook \n\nMy aim with this notebook was to predict the stator winding and rotor winding temperatures and select the best model out of various ones avaliable. \n","64b0d8d5":"## Implementing the Best Lasso Regression","fe5f45de":"## KNN Regressor ","77fdc307":"1. ## Optimizing Ridge Regression ","4896bdc0":"# Context\n\nThe data set comprises several sensor data collected from a permanent magnet synchronous motor (PMSM) deployed on a test bench. The PMSM represents a german OEM's prototype model. Test bench measurements were collected by the LEA department at Paderborn University.This data set is mildly anonymized.<br>\n\n# Working of PMSM \n\n\nThe below embedded vedio shows the working of the PMSM motor . Basically the motor consists of two parts the stator (meaning stationary) and rotor ( which rotates) . A alternating current is passed through the stator which generates a rotating magnetic field and a DC current is passed through the rotor which generates a stationary magnetic field . When the opposite poles of the two magnetic field unite the stator's rotating magnetic field drives the rotor's stationary magnetic field.<br> \n\n\n\n\n\n","0f4b7ed2":"# Spliting into Train and Test set \n\nGiven from desription we know that profile id 65 and 72 should be used in test set ","91d2c400":"## Points to note \n\nWe see that the rotor temperature(pm) is corelated well with other features . Rotor temperature is also well corelated with \nstator temperarture so we can use predict rotor temperature first and then use that as an feature to predict stator temps\n\nAlso we see that there is a great corelation among various starter temperature so we can also decided if we want to predict one of the startor components only and assume the other around same temperature range . See the graph below \n\nAlso we see that stator tooth is most of the time in between stator yoke and stator winding\n\n\nAlso torque is in well agreement with i_q","97143237":"In this dataset we need to predict the stator and rotor temperature given by \n\n__Target features__:<br>\n\n\nstator_yoke<br>\nstator_winding<br>\nstator_tooth<br>\npm<br>\n\n\nRest all feature exluding torque as it is difficult to measure and profile_id as it is a label can be us for prediction \n","36fbe7ee":"## Optimizing Lasso Regression ","15beca47":"As part of dependent variables we have 4 temperature to predict .But out of the 4 temperatures required to predict 3 of stator usually follow the same profile and also are in the close vicinity of each other. Thus I feel if a model which can accurately model one of the three stator temperature should work for us. \n","30015659":"# Implementing the best KNN model ","10db25e3":"From above we see that the min max and mean for all variables are with same value range hence I do not feel the need to \nnormalize the data \n","e60cbea2":"# Linear model with polynomial features \n\nI feel polynomial features should be good as this will create additional features . Also limitting the degree of polynomial to 2 as since usually Power generated within an electrical component has an effect on the temperature of the component and power is usually a product of 2 terms. \n","2b1e322a":"In the above graph we see that number of test points in each profile_id. This basically represents how long the test was run \n\nWe see that max number of points are there in 65 ,6 and 20 "}}