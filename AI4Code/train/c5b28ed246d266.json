{"cell_type":{"4cf4e33a":"code","c5d28651":"code","771a0b8c":"code","ffd4a3f1":"code","317cd745":"code","73e3db6c":"code","8f19a8ed":"code","c68fdb5c":"code","f63a9d0f":"code","9abc1895":"code","21e92303":"code","81751a21":"code","7b1e865b":"code","00a5fbee":"code","48acd7ae":"code","7192a6eb":"code","0c0912e6":"code","dac8d7d0":"code","5cbdf824":"code","74d983a1":"code","0c0d02cc":"code","17a41b7a":"code","8cb02172":"code","f9bd6b29":"code","e0b04e32":"code","1954070e":"code","cf2c7319":"code","9680c396":"markdown","52c09bd1":"markdown","cd31148c":"markdown","3b313e46":"markdown","08d1553a":"markdown","b5c15f42":"markdown","91d3478b":"markdown","e67136e8":"markdown","9e07adfe":"markdown","501c7a26":"markdown","8443c7c7":"markdown"},"source":{"4cf4e33a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c5d28651":"df=pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf.head(5)","771a0b8c":"df.describe()","ffd4a3f1":"# droppping 'id' as its of no use for predicting\ndf.drop('id',axis=1,inplace=True)","317cd745":"df.head()","73e3db6c":"df.isnull().sum()","8f19a8ed":"# dataframe of those having bmi NaN\nbmi_none=df[df['bmi'].isna()==True]\nbmi_none['stroke'].value_counts()","c68fdb5c":"# filling all the nan values using mean values of particular category(0,1)\ndf1=df[df['stroke']==1].fillna(df['bmi'][df['stroke']==1].mean())\ndf2=df[df['stroke']==0].fillna(df['bmi'][df['stroke']==0].mean())","f63a9d0f":"#concatenating 2 dataframes into a final one\nresult_df=pd.concat([df1,df2])","9abc1895":"result_df.isna().sum()","21e92303":"result_df['stroke'].value_counts()","81751a21":"import seaborn as sns\nsns.pairplot(df,hue=\"stroke\")","7b1e865b":"# doing one-hot encoding of categorical variables\nresult_df=pd.get_dummies(result_df,drop_first=True)","00a5fbee":"result_df.head()","48acd7ae":"result_df.columns","7192a6eb":"x=result_df[['age', 'hypertension', 'heart_disease', 'avg_glucose_level', 'bmi',\n       'gender_Male', 'gender_Other', 'ever_married_Yes',\n       'work_type_Never_worked', 'work_type_Private',\n       'work_type_Self-employed', 'work_type_children', 'Residence_type_Urban',\n       'smoking_status_formerly smoked', 'smoking_status_never smoked',\n       'smoking_status_smokes']]\ny=result_df['stroke']","0c0912e6":"from imblearn.over_sampling import SMOTE\nsampling=SMOTE()\nx,y=sampling.fit_resample(x,y)","dac8d7d0":"y.value_counts()","5cbdf824":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=0)","74d983a1":"import lightgbm as lgb","0c0d02cc":"train_data=lgb.Dataset(x_train,label=y_train)","17a41b7a":"param = {'num_leaves':100, 'objective':'binary','max_depth':5,'learning_rate':.05}\nparam['metric'] = ['auc']","8cb02172":"# Training of model\nlgbm=lgb.train(param,train_data,5)","f9bd6b29":"y_pred=lgbm.predict(x_test)","e0b04e32":"y_preds=[]\nfor i in y_pred:\n    if i>0.5:\n        y_preds.append(1)\n    else:\n        y_preds.append(0)","1954070e":"from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_preds,y_test))","cf2c7319":"sns.heatmap(confusion_matrix(y_preds,y_test),annot=True)","9680c396":"### Using SMOTE for oversampling","52c09bd1":"### Model Building using LightGBM","cd31148c":"Since we will use lightGBM which is decision tree based algorithm there is no need to scale values","3b313e46":"### Exploratory Data Analysis","08d1553a":"#### Setting Parameters","b5c15f42":"Since this dataset is imbalanced we have to balance it using sampling techniques","91d3478b":"Since we have 201 values of bmi missing we have to fill these before predicting as bmi can be an important index","e67136e8":"### Making Predictions","9e07adfe":"LightGBM is a gradient boosting model that uses tree-based algorithms. It is much faster than the usual tree-based algorithms like Decision Trees, Random Forests, etc. It has the following advantages over the traditional machine learning algorithms.\n\n* Faster training speed with better efficiency. \n* Lower memory usage.\n* Supports GPU processing. \n* Highly scalable and efficiently handles large datasets","501c7a26":"### Evaluating Results","8443c7c7":"### Importing the Dataset"}}