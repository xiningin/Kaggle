{"cell_type":{"4781a8c3":"code","e1e8100b":"code","1cd53df3":"code","156b754c":"code","db17c298":"code","c4616c24":"code","2be15b1a":"code","3b224066":"code","a83125a2":"code","b27c20c6":"code","e7277d07":"code","606dd09c":"code","c4b357a9":"code","fd9c1df6":"code","8fb4d13a":"code","f2e65d0a":"code","bdcb8ee1":"code","a3d5b218":"code","d967df19":"code","16e0634b":"code","fea6560d":"code","72a1282f":"code","299b4858":"code","49a74fba":"code","d9c7ca86":"code","9635965f":"code","7adcff51":"code","d3e7161b":"code","18d77e6a":"code","07d36e47":"code","cd187b5c":"code","311ad9a5":"code","f34b4aa7":"code","34fe2274":"markdown","8090a423":"markdown","cdefc8d5":"markdown","67d3ebe4":"markdown","27ccd055":"markdown","b2fcabf3":"markdown","48b3c83a":"markdown","d47be135":"markdown","dad95f9b":"markdown","aad08e34":"markdown","6479fb66":"markdown","8039eda4":"markdown","875a7e7f":"markdown","eb147dda":"markdown"},"source":{"4781a8c3":"# # For Data reading \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# For data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# For Feature Scaling & Feature Importance\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import ExtraTreesRegressor\n\n# For model building & scoreing\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\n\n# others\nimport warnings\nwarnings.filterwarnings('ignore')","e1e8100b":"train_df = pd.read_csv('..\/input\/av-healthcare-analytics-ii\/healthcare\/train_data.csv')\ntest_df = pd.read_csv('..\/input\/av-healthcare-analytics-ii\/healthcare\/test_data.csv')\ntrain_dict = pd.read_csv('..\/input\/av-healthcare-analytics-ii\/healthcare\/train_data_dictionary.csv')","1cd53df3":"# Let's find unique values of dependent feature\ntrain_df[\"Stay\"].unique()","156b754c":"# Let's create a dictionary for dependent feature\nencode = {\n    '0-10' : 1, '11-20' : 2, '21-30' : 3, '31-40' : 4, '41-50' : 5, '51-60' : 6, '61-70' : 7, '71-80' : 8,\n    '81-90' : 9, '91-100' : 10, 'More than 100 Days' : 11\n}\ntrain_df['Stay'] = train_df['Stay'].map(encode)","db17c298":"# Let's check missing values\nprint('Train Dataset:::::::::::::::')\nprint(train_df.isnull().sum())\nprint(\"=========================================\")\nprint('Test Dataset::::::::::::::::')\nprint(test_df.isnull().sum())","c4616c24":"# Find features of missing values \ndef NaNFeature(df):\n    nan_feature = [n for n in df.columns if df[n].isnull().sum()>=1]\n    return nan_feature","2be15b1a":"# Let's fill missing values of train detaset \nnan_features_train = NaNFeature(train_df)\nfor fillnan in nan_features_train:\n    train_df[fillnan].fillna(train_df[fillnan].mode()[0], inplace=True)","3b224066":"# Let's fill missing values of test detaset \nnan_features_test = NaNFeature(test_df)\nfor fillnan in nan_features_test:\n    test_df[fillnan].fillna(test_df[fillnan].mode()[0], inplace=True)","a83125a2":"# Lets check missing values percentage\nprint('Train Dataset:::::::::::::::')\nprint(np.round(train_df.isnull().sum() * 100 \/ len(train_df), 4))\nprint(\"=========================================\")\nprint('Test Dataset:::::::::::::::')\nprint(np.round(test_df.isnull().sum() * 100 \/ len(test_df), 4))","b27c20c6":"# Let's Find out categorical features through a function\ndef CatFeatures(df):\n    features = [feature for feature in df.columns if df[feature].dtypes == \"O\"]\n    return features","e7277d07":"# categorical features of train dataset\ncat_features_train = CatFeatures(train_df)\ncat_features_train","606dd09c":"# Let's check unique value of categorical features of train data\nfor i in cat_features_train:\n    print(train_df[i].unique())","c4b357a9":"# categorical features of test dataset\ncat_features_test = CatFeatures(test_df)\ncat_features_test","fd9c1df6":"for i in cat_features_test:\n    print(test_df[i].unique())","8fb4d13a":"# # Let's create a function to handle categorical features \ndef CatToNumaric():\n    # Handle categorical feature of train dataset\n    for n in cat_features_train:\n        num_data = dict(zip(train_df[n].unique(), range(len(train_df[n].unique()))))\n        train_df[n] = train_df[n].map(num_data) # or train_df[n].replace(num_data, inplace=True)\n        \n    # Handle categorical features of test dataset\n    for n in cat_features_test:\n        num_data = dict(zip(test_df[n].unique(), range(len(test_df[n].unique()))))\n        test_df[n] = test_df[n].map(num_data) # or test_df[n].replace(num_data, inplace=True)","f2e65d0a":"# Let's check features data types\nCatToNumaric()\nprint('Train Dataset:::::::::::::::')\nprint(train_df.dtypes)\nprint(\"=====================================\")\nprint('Test Dataset:::::::::::::::')\nprint(test_df.dtypes)","bdcb8ee1":"# Let's see the train dictionary data to drop un necessary features\ntrain_dict","a3d5b218":"# Lets drop features those are necessary so much\ndef DropFeatures(df):\n    drop_features = {'case_id', 'Hospital_code', 'Hospital_type_code', 'patientid'}\n    df.drop(drop_features, axis=1, inplace=True)\n    return df","d967df19":"# Show train dataset\ntrain_data = DropFeatures(train_df)\ntrain_data.head()","16e0634b":"# Show test dataset\ntest_data = DropFeatures(test_df)\ntest_data.head()","fea6560d":"# create X_train & X_test for feature scaling \nX_train = train_data.iloc[: , :-1]\nX_test = test_data\n\n# y_train (depended feature)\ny_train = train_data.iloc[: , -1]","72a1282f":"# create function for scaling X_ data \ndef FeatureScaler(df):\n    min_max = MinMaxScaler()\n    df = pd.DataFrame(min_max.fit_transform(df), columns=df.columns)\n    return df","299b4858":"# Let's show final train dataset\nX_train_final = FeatureScaler(X_train)\nX_train_final.head()","49a74fba":"# Let's show final test dataset\nX_test_final = FeatureScaler(X_test)\nX_test_final.head()","d9c7ca86":"# Let's call Extra Trees Regressor function\nfeature_imp = ExtraTreesRegressor()\nfeature_imp.fit(X_train_final, y_train)\n# Let's show the list of feature importance\nfeature_imp.feature_importances_","9635965f":"# Let's show a plot of ten (10) features\nfeature_importance = pd.Series(feature_imp.feature_importances_, index=X_train_final.columns)\nfeature_importance.nlargest(10).plot(kind='barh')\nplt.show()","7adcff51":"# Create model\nstay_predict = RandomForestClassifier()\nstay_predict.fit(X_train_final, y_train)","d3e7161b":"# Let's test the model\ny_test = stay_predict.predict(X_test_final)\ny_test","18d77e6a":"# For submission file we need 'case_id' so read sample_submission file\nsample_sub_df = test_df = pd.read_csv('..\/input\/av-healthcare-analytics-ii\/healthcare\/sample_sub.csv')","07d36e47":"predection_df = pd.DataFrame()\npredection_df['case_id'] = sample_sub_df['case_id'] \npredection_df['Stay'] = y_test\n\ndecode_prediction = { 1 : '0-10', 2 : '11-20', 3 : '21-30', 4 : '31-40', 5 : '41-50', 6 : '51-60', 7 : '61-70'\n            ,8 : '71-80', 9 : '81-90', 10 : '91-100', 11 : 'More than 100 Days'}\n\npredection_df['Stay'] = predection_df['Stay'].map(decode_prediction)\npredection_df.head()","cd187b5c":"# Model score\nstay_predict.score(X_train_final, y_train)","311ad9a5":"# Cross Validation \nscore = cross_val_score(stay_predict, X_train_final, y_train.ravel(), cv=10)\nscore.mean()","f34b4aa7":"submission = predection_df.copy()\nsubmission.head()","34fe2274":"### Create Model with Random Forest Classifier","8090a423":"Source Code on github-  https:\/\/github.com\/sheikhmasudrana\/ML_Practice\/tree\/master\/Healthcare%20Analytics(stay%20days%20prediction)","cdefc8d5":"### Feature Scaling (MinMax Scaler)","67d3ebe4":"#### Thanks\nWish to get comments from all.","27ccd055":"#### Let's Encoded dependent Feature (\"Stay\" column)","b2fcabf3":"#### Decode Prediction data","48b3c83a":"#### Create a submission File","d47be135":"### Handle Categorical Features","dad95f9b":"### Scoring & Validation","aad08e34":"#### Model Testing","6479fb66":"#### Clean unused features","8039eda4":"### Handle Missing value of features","875a7e7f":"#### Read Dataset","eb147dda":"### Find Feature Importance\n    Find best 10 features from datasets"}}