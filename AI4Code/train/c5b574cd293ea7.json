{"cell_type":{"77aa846f":"code","d7fe1797":"code","64311a6a":"code","74407226":"code","d88e3511":"code","ede3ad81":"code","112ceb39":"code","ddc8acc2":"code","4ffc639c":"code","af0f7b0b":"code","4d7c952a":"code","523f1906":"code","8d80f45a":"code","96197db7":"code","875ea63e":"code","e258228e":"code","bcf44210":"code","e9201b6c":"code","9f9dbef6":"code","60dc874a":"code","92d089f0":"code","eaf62576":"code","f789fd94":"code","6a61d785":"code","3fa5eb86":"code","a678576c":"code","ef6ca098":"code","b955cca6":"code","ce9c6743":"code","a0f8669e":"code","215ef4cb":"code","a505cf71":"code","bf141032":"code","5a20da7c":"code","16219fe0":"code","bf505ab7":"markdown","3a7882b9":"markdown","e2979083":"markdown","00614044":"markdown","ffa242c0":"markdown","549dc79e":"markdown","7bf8fd2f":"markdown","298b4611":"markdown","0cdc77c6":"markdown","81994d49":"markdown","4c836990":"markdown","e860a77a":"markdown","227184b3":"markdown","f8091c3d":"markdown","859486d2":"markdown","db5e4f0d":"markdown"},"source":{"77aa846f":"# linear algebra\nimport numpy as np \n\n# data processing\nimport pandas as pd \nfrom sklearn.preprocessing import LabelEncoder\n\n# data visualization\nimport matplotlib.pyplot as plt \nimport plotly.express as px \nimport seaborn as sns \nfrom scipy import stats  \n\n# Prediction models\nfrom sklearn.ensemble import RandomForestRegressor \nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport xgboost as xgb\n\n#Evaluation\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error \nfrom hyperopt import hp, tpe, fmin","d7fe1797":"train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","64311a6a":"train.head(10)","74407226":"train.info()","d88e3511":"#correlation map\nf,ax=plt.subplots(figsize=(25, 25))\nsns.heatmap(train.corr(), annot=True, linewidths=.4, fmt= '.1f',ax=ax)\nplt.show()","ede3ad81":"train.isnull().sum()[train.isnull().sum() > 100] # Features possess dramatically numerous null values that we cannot fill them properly. ","112ceb39":"train.drop([\"LotFrontage\",\"Alley\",\"FireplaceQu\",\"PoolQC\",\"Fence\",\"MiscFeature\"],axis=1,inplace = True) # Dropping them","ddc8acc2":"fig, ax =plt.subplots(1,2,figsize=(14, 6))\nsns.scatterplot(x=\"TotalBsmtSF\", y=\"SalePrice\", data=train,ax=ax[0])\nsns.scatterplot(x=\"GrLivArea\", y=\"SalePrice\", data=train,ax=ax[1])\nfig.show()","4ffc639c":"train = train[train.TotalBsmtSF < 5000].reset_index(drop = True)\ntrain= train[train[\"GrLivArea\"] < 4500].reset_index(drop = True)","af0f7b0b":"fig_neigh = train.groupby(\"Neighborhood\").median().sort_values(by=\"SalePrice\")\nfig = px.box(fig_neigh, x=fig_neigh.index, y=\"SalePrice\",)\nfig.show()","4d7c952a":"def neig_func(model):\n    dic = {'NridgHt': 25, 'NoRidge': 24, 'StoneBr': 23,\n           'Timber': 22, 'Somerst': 21, 'Veenker': 20, 'Crawfor': 19,\n           'ClearCr': 18, 'CollgCr': 17, 'Blmngtn': 16, 'NWAmes': 15,\n           'Gilbert': 14, 'SawyerW': 13, 'Mitchel': 12, 'NPkVill': 11,\n           'NAmes': 10, 'SWISU': 9, 'Blueste': 8, 'Sawyer': 7, 'BrkSide': 6,\n           'Edwards': 5, 'OldTown': 4, 'BrDale': 3, 'IDOTRR': 2, 'MeadowV': 1}\n    model[\"Neighborhood\"].replace(dic,inplace = True)","523f1906":"def land_slope(model):\n    # Dictionary values are created through the opposite of slope values. In other words, High value for sharp slope, Low value for flattened surface.\n    \n    #Gtl -> Gentle slope\n    #Mod -> Moderate Slope\n    #Sev -> Severe Slope\n    \n    dic = {\"Gtl\":3,\"Mod\":2,\"Sev\":1}\n    model[\"LandSlope\"].replace(dic,inplace = True)","8d80f45a":"train[\"Garage\"] = train[\"GarageCars\"] * train[\"GarageArea\"]\ncorr_matrix = train.corr()\nprint(corr_matrix[\"SalePrice\"].sort_values(ascending = False)[3:6])\n# By creating new feature we get better correlation.","96197db7":"# Function implemented to complete null values by most repeated value for each one of them.\ndef complete_null(model):\n    for i in model.isnull().sum()[model.isnull().sum() != 0].index:\n        model[i].fillna(model[i].mode()[0],inplace = True)","875ea63e":"from scipy.stats import norm\n# histogram and normal probability plot. Thanks to Pedro Marcelino for this insightful trick. I learned that the notebook created by him.\nsns.distplot(train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)","e258228e":"train[\"SalePrice\"] = np.log(train[\"SalePrice\"])","bcf44210":"def changes(model):\n    model.drop([\"GarageCars\",\"Utilities\",\"BsmtFinSF2\",\"BsmtUnfSF\"],axis=1,inplace = True) # No need them\n    \n    # Used the functions implemented above\n    land_slope(model) \n    neig_func(model)\n    \n    # To get better correlation, I segmented the data with slightly better values.\n    model[\"YearBuilt\"] = pd.cut(model[\"YearBuilt\"],bins=[1870,1900,1910,1920,1930,1940,1950,1960,1970,1980,1990,1995,2000,2003,2005,2007,2011],labels =[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]).astype(int)\n    \n    # Categorical features to numerical features\n    model[\"Foundation\"].replace({\"PConc\":6,\"Stone\":5,\"CBlock\":4,\"Wood\":3,\"BrkTil\":2,\"Slab\":1},inplace = True)\n    model.replace({\"Ex\":5,\"Gd\":4,\"TA\":3,\"Fa\":2,\"Po\":1},inplace = True)\n    model[\"CentralAir\"].replace({\"Y\":1,\"N\":0},inplace = True)\n    \n    # Some ineffable feature engineering \n    model[\"FirstandSecondSum\"] = model[\"2ndFlrSF\"] + model[\"1stFlrSF\"]\n    model[\"Qual\"] = model[\"KitchenQual\"] + model[\"ExterQual\"]\n    model[\"NeigYear\"] = model[\"Neighborhood\"] + model[\"YearBuilt\"]\n    model[\"AllQual\"] = model[\"OverallQual\"] * model[\"Qual\"]\n    model[\"hadi\"] = model[\"FirstandSecondSum\"] + model[\"TotalBsmtSF\"] # \"hadi\" means \"come on\" in Turkish. I created this feature hopefully when I was stucked in generating new feautes.\n    \n    # Log transformation same like we did for \"SalePrice\"\n    model[\"GrLivArea\"] = np.log(model[\"GrLivArea\"])\n    model[\"LotArea\"] = np.log(model[\"LotArea\"])\n    model['hadi'] = np.log(model['hadi'])\n    \n    # Another anomaly detection\n    model = model[model[\"LotArea\"] < 100000].reset_index(drop=True)\n    \n    # Some brand new features after I reach top 40%.\n    model = model.assign(HasFirePlace = (model[\"Fireplaces\"] != 0).astype(int))\n    model = model.assign(HasGarageArea = (model[\"GarageArea\"] != 0).astype(int))\n    model = model.assign(HasOpenPorchSF = (model[\"OpenPorchSF\"] != 0).astype(int))\n    model = model.assign(HasPoolArea = (model[\"PoolArea\"] != 0).astype(int))\n    model = model.assign(HasMasVnrArea = (model[\"MasVnrArea\"] != 0).astype(int))  \n    \n    model.drop([\"KitchenQual\",\"OverallQual\",\"Qual\",\"Fireplaces\",\"OpenPorchSF\",\"FirstandSecondSum\",\"TotalBsmtSF\",\"ExterQual\",\"GarageArea\",\"2ndFlrSF\",\"1stFlrSF\",\"YearRemodAdd\",\"PoolArea\"],axis=1,inplace = True)\n\n    return model\ntrain = changes(train)","e9201b6c":"train[\"BsmtQual\"].fillna(train.BsmtQual.median(),inplace = True)\ntrain[\"MasVnrArea\"].fillna(train.MasVnrArea.median(),inplace = True)\ntrain[\"BsmtCond\"].fillna(3,inplace=True)\ncomplete_null(train)\ntrain = pd.get_dummies(train)","9f9dbef6":"print(\"Train Data has {} rows\".format(train.shape[0]))\nprint(\"Train Data has {} columns\".format(train.shape[1]))\nprint(f\"Train Data memory usage is {train.memory_usage().sum() \/ 1024 ** 2:.3f} MB\")","60dc874a":"train.info()","92d089f0":"corr_matrix = train.corr()\ncorr_matrix[\"SalePrice\"].sort_values(ascending = False)[:20]","eaf62576":"# Take a look on how it works with some powerful models\ndef predict_func(df):\n    X = df.drop(\"SalePrice\",axis=1)\n    y = df.loc[:,\"SalePrice\"]\n    \n    ran_for_reg = RandomForestRegressor()\n    scores_rand = cross_val_score(ran_for_reg, X, y, cv=15,scoring=\"neg_root_mean_squared_error\")\n    print(\"Random Forest Regressor: \" + str(-scores_rand.mean()))\n    \n    grad_boost_reg = GradientBoostingRegressor()\n    scores_grad = cross_val_score(grad_boost_reg, X, y, cv=15,scoring=\"neg_root_mean_squared_error\")\n    print(\"Gradient Boost Regression: \" + str(-scores_grad.mean()))\n    \n    xg_boost_reg = xgb.XGBRegressor()\n    scores_xg = cross_val_score(xg_boost_reg, X, y, cv=15,scoring=\"neg_root_mean_squared_error\")\n    print(\"XG Boost Regression: \" + str(-scores_xg.mean()))","f789fd94":"predict_func(train.drop(\"Id\",axis=1)) # We do not need Id column deeply","6a61d785":"X_train = train.drop([\"SalePrice\",\"Id\"],axis=1)\ny_train = train.loc[:,\"SalePrice\"]","3fa5eb86":"def test_changes(model):\n    model[\"GarageCars\"].fillna(model.GarageCars.median(),inplace = True)\n    model[\"GarageArea\"].fillna(model.GarageArea.mean(),inplace = True)\n    model[\"Garage\"] = model[\"GarageCars\"] * model[\"GarageArea\"]\n    model.drop([\"GarageCars\",\"LotFrontage\",\"Alley\",\"FireplaceQu\",\"PoolQC\",\"Fence\",\"MiscFeature\",\"Utilities\",\"BsmtFinSF2\",\"BsmtUnfSF\"],axis=1,inplace = True)\n    land_slope(model)\n    neig_func(model)\n    model[\"YearBuilt\"] = pd.cut(model[\"YearBuilt\"],bins=[1870,1900,1910,1920,1930,1940,1950,1960,1970,1980,1990,1995,2000,2003,2005,2007,2011],labels =[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]).astype(int)\n    model[\"Foundation\"].replace({\"PConc\":6,\"Stone\":5,\"CBlock\":4,\"Wood\":3,\"BrkTil\":2,\"Slab\":1},inplace = True)\n    model.replace({\"Ex\":5,\"Gd\":4,\"TA\":3,\"Fa\":2,\"Po\":1},inplace = True)\n    model[\"CentralAir\"].replace({\"Y\":1,\"N\":0},inplace = True)\n    model[\"FirstandSecondSum\"] = model[\"2ndFlrSF\"] + model[\"1stFlrSF\"]\n    model[\"KitchenQual\"].fillna(model.KitchenQual.median(),inplace = True)\n    model[\"ExterQual\"].fillna(model.ExterQual.median(),inplace = True)\n    model[\"Qual\"] = model[\"KitchenQual\"] + model[\"ExterQual\"]\n    model[\"NeigYear\"] = model[\"Neighborhood\"] + model[\"YearBuilt\"]\n    \n    model[\"AllQual\"] = model[\"OverallQual\"] * model[\"Qual\"]\n    model[\"hadi\"] = model[\"FirstandSecondSum\"] + model[\"TotalBsmtSF\"]\n    model[\"GrLivArea\"] = np.log(model[\"GrLivArea\"])\n    model[\"LotArea\"] = np.log(model[\"LotArea\"])\n    model['hadi'] = np.log(model['hadi'])\n    model = model.assign(HasFirePlace = (model[\"Fireplaces\"] != 0).astype(int))\n    model = model.assign(HasGarageArea = (model[\"GarageArea\"] != 0).astype(int))\n    model = model.assign(HasOpenPorchSF = (model[\"OpenPorchSF\"] != 0).astype(int))\n    model = model.assign(HasPoolArea = (model[\"PoolArea\"] != 0).astype(int))\n    model = model.assign(HasMasVnrArea = (model[\"MasVnrArea\"] != 0).astype(int))\n    model.drop([\"KitchenQual\",\"OverallQual\",\"Qual\",\"Fireplaces\",\"OpenPorchSF\",\"FirstandSecondSum\",\"TotalBsmtSF\",\"ExterQual\",\"GarageArea\",\"2ndFlrSF\",\"1stFlrSF\",\"YearRemodAdd\",\"PoolArea\"],axis=1,inplace = True)\n    return model\ntest = test_changes(test)\ntest[\"BsmtQual\"].fillna(test.BsmtQual.median(),inplace = True)\ntest[\"MasVnrArea\"].fillna(test.MasVnrArea.median(),inplace = True)\ntest[\"hadi\"].fillna(test.hadi.median(),inplace = True)\ncomplete_null(test)\ntest = pd.get_dummies(test)\nX_test = test.drop(\"Id\",axis=1)","a678576c":"print(\"Test Data has {} rows\".format(test.shape[0]))\nprint(\"Test Data has {} columns\".format(test.shape[1]))\nprint(f\"Test Data memory usage is {test.memory_usage().sum() \/ 1024 ** 2:.3f} MB\")","ef6ca098":"for i in X_train.columns:   \n    if not i in X_test:\n        print(i)","b955cca6":"not_in = ['Condition2_RRAe',\n 'Condition2_RRAn',\n 'Condition2_RRNn',\n 'HouseStyle_2.5Fin',\n 'RoofMatl_Membran',\n 'RoofMatl_Metal',\n 'RoofMatl_Roll',\n 'Exterior1st_ImStucc',\n 'Exterior1st_Stone',\n 'Exterior2nd_Other',\n 'Heating_Floor',\n 'Heating_OthW',\n 'Electrical_Mix']\n\nX_train.drop(not_in,axis=1,inplace=True)","ce9c6743":"predict_func(train.drop(\"Id\",axis=1))","a0f8669e":"# Parameters we are about to use for trying to get best result for xgboosting. I decide to use xgboosting because it works well better than any other regression models. I try all of the models and conclude with xgboosting.\nspace = {'n_estimators':hp.quniform('n_estimators', 1000, 4000, 100),\n         'gamma':hp.uniform('gamma', 0.01, 0.05),\n         'learning_rate':hp.uniform('learning_rate', 0.00001, 0.025),\n         'max_depth':hp.quniform('max_depth', 3,7,1),\n         'subsample':hp.uniform('subsample', 0.60, 0.95),\n         'colsample_bytree':hp.uniform('colsample_bytree', 0.60, 0.98),\n         'colsample_bylevel':hp.uniform('colsample_bylevel', 0.60, 0.98),\n         'reg_lambda': hp.uniform('reg_lambda', 1, 20)\n        }\n\ndef objective(params):\n    params = {'n_estimators': int(params['n_estimators']),\n             'gamma': params['gamma'],\n             'learning_rate': params['learning_rate'],\n             'max_depth': int(params['max_depth']),\n             'subsample': params['subsample'],\n             'colsample_bytree': params['colsample_bytree'],\n             'colsample_bylevel': params['colsample_bylevel'],\n             'reg_lambda': params['reg_lambda']}\n    \n    xb_a= xgb.XGBRegressor(**params)\n    score = cross_val_score(xb_a, X_train, y_train, scoring='neg_mean_squared_error', cv=5, n_jobs=-1).mean()\n    return -score","215ef4cb":"best = fmin(fn= objective, space= space, max_evals=20, rstate=np.random.RandomState(1), algo=tpe.suggest)","a505cf71":"xb_b = xgb.XGBRegressor(random_state=0,\n                        n_estimators=int(best['n_estimators']), \n                        colsample_bytree= best['colsample_bytree'],\n                        gamma= best['gamma'],\n                        learning_rate= best['learning_rate'],\n                        max_depth= int(best['max_depth']),\n                        subsample= best['subsample'],\n                        colsample_bylevel= best['colsample_bylevel'],\n                        reg_lambda= best['reg_lambda']\n                       )\n\nxb_b.fit(X_train, y_train)","bf141032":"mean_squared_error(y_train,xb_b.predict(X_train))","5a20da7c":"FirstTest = xb_b.predict(X_test)\nFirstTest = FirstTest.reshape(-1,1)","16219fe0":"FirstTest = np.exp(FirstTest) \nresult = pd.DataFrame(data = test[\"Id\"])\nresult[\"SalePrice\"]= FirstTest\nresult.to_csv(r'xgboost_result.csv',index = False, header=True)","bf505ab7":"To get lower memory usage you can transform data types to int8 or somethings like that but in this notebook I don't do that because memory is not a big deal in this notebook.","3a7882b9":"# Quick Look","e2979083":"I am glad for any comment or suggestion also do not hesitate to ask any kind of questions.","00614044":"Let's apply the same changes to test data","ffa242c0":"# Feature Engineering","549dc79e":"# Prediction","7bf8fd2f":"In case of positive skewness, log transformations usually works well. It became like more the famous normal distrubition after transformation below.","298b4611":"Hi there! \n\nWelcome to my notebook. This is my second notebook ever in the Kaggle platform and I choose housing price competition which is a getting started competition for beginners.\nThe competition is about predicting houses sale prices in Ames, Iowa with the given tabular data.\n\nIn the notebook, I try to explain functions with commands and headers also in this notebook I get 0.12687 point which means top 25% in the leaderboard.","0cdc77c6":"Hey look at the number of columns ! \n\nUnfortunately, both of data do not contain exactly same column but we can get them like we want to use","81994d49":"It is kind of linear relation so we can use it by encoding them accoring to descending median values.","4c836990":"As you remember we applied log transformasion to SalePrice so the model did the job by looking at transformed data but kaggle does not accept results like that so we need to transform SalePrice back with np.exp because it is reverse of np.log","e860a77a":"I think we did not lose anything by deleting them because errors look almost same with the errors before deleting them","227184b3":"These are the columns in train but not in test. Therefore, if we get rid of them, then it will be just OK.","f8091c3d":"I think not bad at all so let's seperate our final data before using optimizer for xgboost","859486d2":"For \"TotalBsmtSF\", there is an anomaly at approx. 6000 on x-axis that can fail the model. Same approach for the \"GrLivArea\".\n\nFor \"GrLivArea\", there is an anomaly at right most point that can fail the model same as \"TotalBsmtSF\".\n\nSo we should ignore them.","db5e4f0d":"By deleting them from the data we get better correlation because the line is more linear now."}}