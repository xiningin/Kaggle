{"cell_type":{"b9816932":"code","07b0308e":"code","666160a0":"code","80a20d16":"code","1fac4e17":"code","d7055047":"code","6280b5b0":"code","aa4fbb8f":"code","8b5e4973":"code","947797b2":"code","fd20980c":"code","e0a48e94":"code","cf03ba56":"code","7acce8ef":"code","91d870a0":"code","d972eccb":"code","466c6315":"code","da8f581c":"code","a68bafa2":"code","0ee25d04":"code","463dbf4c":"code","808dcf12":"code","eda271eb":"code","96a36f04":"code","5d94a49c":"code","6d6f69c5":"code","def39b9c":"code","0bc146ea":"code","bb82797d":"code","56b2ad06":"code","024e7bc3":"markdown","b30e342b":"markdown","ce0aff5b":"markdown"},"source":{"b9816932":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns","07b0308e":"import warnings\nwarnings.filterwarnings('ignore')","666160a0":"data = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\nval_data = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\nsample = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/sample_submission.csv')\ndata","80a20d16":"val_data","1fac4e17":"val_data.tail()","d7055047":"sample","6280b5b0":"val_data.less_toxic","aa4fbb8f":"less_toxic_comments = val_data.less_toxic\nd_less = {\n    'comments':less_toxic_comments,\n}\nless_toxic= pd.DataFrame(d_less)\nless_toxic[\"toxic\"] = \"less_toxic\"\nless_toxic","8b5e4973":"more_toxic = val_data.more_toxic\nd_more = {\n    'comments':more_toxic,\n}\n\nmore_toxic= pd.DataFrame(d_more)\nmore_toxic[\"toxic\"] = \"more_toxic\"\nmore_toxic","947797b2":"train = pd.concat([less_toxic,more_toxic])","fd20980c":"train.toxic.value_counts()","e0a48e94":"train_d = train.sample(frac=1, random_state=42).reset_index()\ntrain_d","cf03ba56":"train_d['toxic'] = train_d['toxic'].apply(lambda x: 1 if x=='more_toxic' else 0)\ntrain_d","7acce8ef":"train_d = train_d[['comments','toxic']]\ncleaned_data = train_d.drop_duplicates()\ncleaned_data.tail()","91d870a0":"train_d.tail()","d972eccb":"from sklearn.model_selection import train_test_split\n\ntrain_data_m,val_data_m,train_labels_m,val_labels_m = train_test_split(train_d['comments'].to_numpy(),\n                                                               train_d['toxic'].to_numpy(),\n                                                               test_size=0.2,\n                                                               random_state=41)\n\ntrain_data_m.shape,val_data_m.shape,train_labels_m.shape,val_labels_m.shape","466c6315":"from sklearn.model_selection import train_test_split\n\ntrain_data,val_data,train_labels,val_labels = train_test_split(cleaned_data['comments'].to_numpy(),\n                                                               cleaned_data['toxic'].to_numpy(),\n                                                               test_size=0.2,\n                                                               random_state=41)\n\ntrain_data.shape,val_data.shape,train_labels.shape,val_labels.shape","da8f581c":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n# Setup text vectorization with custom variables\nmax_vocab_length = 50000 # max number of words to have in our vocabulary\nmax_length = 15 # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n\ntext_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n                                    output_mode=\"int\",\n                                    output_sequence_length=max_length)","a68bafa2":"train_vac = text_vectorizer.adapt(train_d.comments)\ntrain_vac","0ee25d04":"from tensorflow.keras.layers import Embedding\n\nembedding = Embedding(input_dim=max_vocab_length,\n                      output_dim=128,\n                       embeddings_initializer=\"uniform\", # default, intialize randomly\n                             input_length=max_length, # how long is each input\n                             name=\"embedding_1\") \n\nembedding","463dbf4c":"input = layers.Input(shape=(1,),dtype='string',name='input_layer')\nvectorized = text_vectorizer(input)\nembedded = embedding(vectorized)\n\nx = layers.GlobalAveragePooling1D()(embedded)\n\noutput = layers.Dense(1,activation='sigmoid',name=\"output_layer\")(x)\n\nmodel_1 = keras.Model(input,output,name='model_1')\n","808dcf12":"model_1.compile(loss=\"binary_crossentropy\",\n              optimizer= tf.keras.optimizers.Adam(),\n              metrics=['accuracy'])","eda271eb":"history_model_1_c = model_1.fit(train_data_m,train_labels_m,\n                              # validation_data=(val_data,val_labels),\n                              epochs=5)","96a36f04":"model_1.evaluate(val_data,val_labels)","5d94a49c":"val_pred_ = model_1.predict(val_data)\nval_pred_ = tf.squeeze(tf.round(val_pred_))\nval_pred_","6d6f69c5":"predicted = model_1.predict(data.text)\npredicted","def39b9c":"d = np.array(tf.round(predicted))","0bc146ea":"data1 = data.drop(columns=['text'])","bb82797d":"data1['score'] = d\ndata1.head()","56b2ad06":"data1.to_csv(\"submission.csv\",index = False)","024e7bc3":"By comparing both model we can say that tensorflow model_1 predicted well.","b30e342b":"by unclean data","ce0aff5b":"\n\n1.   by uncleaned max data (simple tensorflow) -> [0.6857806444168091, 0.641813337802887]\n2.   by cleand data (simple tensorflow) -> \n\n"}}