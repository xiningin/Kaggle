{"cell_type":{"da9342ce":"code","d2a8121a":"code","96b3227e":"code","4595c428":"code","f9e92215":"code","c24232f3":"code","0b7d725d":"code","a2f09295":"code","e50b3671":"markdown","26da55d6":"markdown","a128993c":"markdown","2a850fb0":"markdown","c20037ea":"markdown","e63f57ad":"markdown","2594767f":"markdown"},"source":{"da9342ce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d2a8121a":"import pandas as pd\nimport numpy as np\n\nX_train_csv = pd.read_csv(\"..\/input\/student-performance-data-set-competition-form\/X_train.csv\")\ny_train_csv = pd.read_csv(\"..\/input\/student-performance-data-set-competition-form\/y_train.csv\")\nX_test_csv  = pd.read_csv(\"..\/input\/student-performance-data-set-competition-form\/X_test.csv\")\ny_test_csv  = pd.read_csv(\"..\/input\/student-performance-data-set-competition-form\/test_label\/y_test.csv\")\n\ndef EDA(name, df) :\n    print(\"== ============================================================\")\n    print(\"== EDA : \", name)\n    print(\"== ============================================================\")\n    \n    print(\">>> all column info\")\n    print(df.columns)\n    \n    print(\">>> inlude object column info\")\n    print(df.select_dtypes(include=object).columns)\n    for col in df.select_dtypes(include=object).columns :\n        print(\"-- ------------------------------------------------------------\")\n        print(col, \"-\", df[col].nunique(), \" : \", df[col].unique())\n        print(df[col].value_counts())\n        print(\"null check : \", df[col].isnull().sum())\n        \n    print(\">>> exclude object column info\")\n    print(df.select_dtypes(exclude=object).columns)\n    for col in df.select_dtypes(exclude=object).columns :\n        print(\"-- ------------------------------------------------------------\")\n        print(col, \"-\", df[col].unique()[:10])\n        print(df[col].describe())\n        print(\"null check : \", df[col].isnull().sum())\n    \n    print(\"== ============================================================\")\n    print(df.isnull().sum())\n    print(df.info())\n    print(df.describe())\n    print(\"== ============================================================\\n\\n\")\n\nEDA(\"X_train_csv\", X_train_csv)\nEDA(\"y_train_csv\", y_train_csv)\nEDA(\"X_test_csv\", X_test_csv)","96b3227e":"## User Defined Variable\ntarget_col = \"G3\"\ndrop_col = [\"StudentID\"]\nis_scaled = True\n\n### strategy\n\"\"\"\nIndex(['StudentID', 'school', 'sex', 'age', 'address', 'famsize', 'Pstatus',\n       'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime',\n       'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities',\n       'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime',\n       'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2'],\n      dtype='object')\n\"\"\"\n\n## Data Preprocessing\ndef data_preprocessing(df_train, df_test) :\n    split_len = df_train.shape[0]\n    \n    print(\" > One-Hot encoding pre count check ---------------------------\")\n    print(df_train.shape)\n    print(df_test.shape)\n    \n    df_dum = pd.get_dummies(pd.concat([df_train, df_test], axis=0))\n    df_train = df_dum[:split_len]\n    df_test  = df_dum[split_len:]\n    \n    print(\" > One-Hot encoding shape check -------------------------------\")\n    print(df_dum.shape)\n    \n    print(\" > One-Hot encoding split count check -------------------------\")\n    print(df_train.shape)\n    print(df_test.shape)\n    \n    print(\" > One-Hot encoding type check --------------------------------\")\n    print(df_train.info())\n    print(df_test.info())\n    \n    return df_train, df_test\n\nX = X_train_csv.drop(drop_col, axis=1)\ny = y_train_csv[target_col]\nX_sub = X_test_csv.drop(drop_col, axis=1)\n\nX, X_sub = data_preprocessing(X, X_sub)","4595c428":"## Holdout\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n## Regulation\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(X_train)\n\nX_scaled_train = X_train\nX_scaled_test = X_test\nX_scaled_sub = X_sub\n\nif is_scaled :\n    X_scaled_train = scaler.transform(X_train)\n    X_scaled_test = scaler.transform(X_test)\n    X_scaled_sub = scaler.transform(X_sub)","f9e92215":"## model - RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(random_state=42)\nmodel.fit(X_scaled_train, y_train)\n\n## predict\npred_train = model.predict(X_scaled_train)\npred_test = model.predict(X_scaled_test)\npred_sub = model.predict(X_scaled_sub)\n\n## Evaluation\nfrom sklearn.metrics import r2_score\nprint(\"\\n\\n>>> train -----------------------------------------------\")\nprint(\"score    : \", model.score(X_scaled_train, y_train))\nprint(\"r2_score : \", r2_score(y_train, pred_train))\n\nprint(\"\\n\\n>>> test ------------------------------------------------\")\nprint(\"score    : \", model.score(X_scaled_test, y_test))\nprint(\"r2_score : \", r2_score(y_test, pred_test))\n\nprint(\"\\n\\n>>> submission creation ---------------------------------\")\ndf_sub = pd.DataFrame({\"StudentID\":X_test_csv[\"StudentID\"], \"G3\":pred_sub})\ndf_sub.to_csv(\"y_submission.csv\", index=False)\n\ndf_subr = pd.read_csv(\".\/y_submission.csv\")\ndisplay(df_subr.head())\n\ny_sub     = y_test_csv[\"G3\"]\npred_subr = df_subr[\"G3\"]\n\nprint(\"\\n\\n>>> submission ------------------------------------------\")\nprint(\"score    : \", model.score(X_scaled_sub, y_sub))\nprint(\"r2_score : \", r2_score(y_sub, pred_subr))","c24232f3":"## Enclosure.01 Cross_val\n## cross valid \nfrom sklearn.model_selection import GridSearchCV, cross_val_score, KFold\nkfold = KFold(n_splits=5)\nscore = cross_val_score(RandomForestRegressor(random_state=42), X_scaled_train, y_train, cv=kfold, scoring=\"r2\")\nprint(score.mean())\nprint(score)\n# help(GridSearchCV)","0b7d725d":"## Enclosure.02 GridSearchCV\n## GridSearchCV\n\"\"\"\ngrid_param = {\"n_estimators\":range(100, 1000, 100), \"min_samples_split\":range(1, 10, 1), \"min_samples_leaf\":range(1,5,1), \"max_features\":[\"log2\", \"sqrt\", \"auto\"]}\ngrid_search = GridSearchCV(model, grid_param)\ngrid_search.fit(X_scaled_train, y_train)\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)\nprint(grid_search.score(X_scaled_test, y_test))\n\"\"\"","a2f09295":"## Enclosure.03 FeatureImportances\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nftr_importances_values = model.feature_importances_\nftr_importances = pd.Series(ftr_importances_values, index = X_train.columns)\nftr_top20 = ftr_importances.sort_values(ascending=False)[:20]\n\nplt.figure(figsize=(8,6))\nplt.title(\"Top 20 Feature Importances\")\nsns.barplot(x=ftr_top20, y=ftr_top20.index)\nplt.show()","e50b3671":"## Enclosure.02 GridSearchCV","26da55d6":"## Part.01 Data Load & EDA","a128993c":"## Part.02 Data Preprocessing","2a850fb0":"## Part.04 Data Modeling & Evaluation","c20037ea":"## Part.03 Data Holdout & Data Normalization","e63f57ad":"## Enclosure.03 FeatureImportances","2594767f":"## Enclosure.01 Cross_val"}}