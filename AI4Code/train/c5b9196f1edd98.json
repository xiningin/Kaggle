{"cell_type":{"eb5b0045":"code","1dae7015":"code","02a77fff":"code","4a56399c":"code","1c013ca1":"code","7f18cf6b":"code","64e30f97":"code","dd40cc44":"code","0bb644a1":"code","cc3da4a7":"code","9c85871b":"code","7abbe735":"code","39c76830":"code","76568be7":"code","9d9b8b88":"code","72f394da":"markdown","f62b9465":"markdown","b22b4bf4":"markdown","dc235062":"markdown","b1661a69":"markdown","0eafec7e":"markdown","8a1478e4":"markdown","e63599bf":"markdown","a7e4ad4a":"markdown"},"source":{"eb5b0045":"from transformers import DistilBertTokenizer, DistilBertModel\nimport torch\nfrom torch import nn\nimport json\nimport glob\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm","1dae7015":"tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n# model = DistilBertModel.from_pretrained('distilbert-base-uncased')\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","02a77fff":"# root = \"..\/input\/meme-project-raw\"\nroot = \"..\/input\/meme-project-clean-json\"\njson_list = glob.glob(os.path.join(root, \"*.json\"))\ndata = []\nfor path in json_list:\n    with open(path, \"r\") as f:\n        data += json.load(f)\ndata[4], len(data)","4a56399c":"data_train, data_test = train_test_split(data, test_size=0.2, random_state=42)\nlen(data_train), len(data_test)","1c013ca1":"class SplitterDataset(Dataset):\n\n    def __init__(self, data, tokenizer, max_seq_len):\n        self.tokenizer = tokenizer\n        self.data = data\n        self.max_seq_len = max_seq_len\n        trunc_data = []\n        for lst in self.data:\n            senlen = len(lst[0]) + len(lst[1])\n            if senlen < 500 and senlen > 1:\n                trunc_data.append(lst)\n        self.data = trunc_data\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sen = self.data[idx]\n        tok0 = tokenizer(sen[0])\n        tok1 = tokenizer(sen[1])\n        ids = tok0['input_ids'][:-1] + tok1['input_ids'][1:-1]\n        msk = [1] * (len(tok0['input_ids']) + len(tok1['input_ids']) - 3)\n        tar = len(tok0['input_ids'])-2\n        return {'input_ids': ids, 'attention_mask': msk}, tar\n    def collate_fn(self, batch):\n        maxl = max([len(item[0]['input_ids']) for item in batch])\n        input_ids = []\n        attention_mask = []\n        labels = []\n        for i, item in enumerate(batch):\n            x = item[0]\n            tar = item[1]\n            l = len(x['input_ids'])\n            this_input_ids = x['input_ids'][:] + [self.tokenizer.pad_token_id] * (maxl-l)\n            this_input_ids = this_input_ids[:self.max_seq_len]\n            input_ids.append(this_input_ids)\n            this_attention_mask = x['attention_mask'][:] + [0] * (maxl-l)\n            this_attention_mask = this_attention_mask[:self.max_seq_len]\n            attention_mask.append(this_attention_mask)\n            labels.append(min(tar, self.max_seq_len-1))\n        return torch.tensor(input_ids).to(device), torch.tensor(attention_mask).to(device), torch.tensor(labels).to(device)","7f18cf6b":"train_dataset = SplitterDataset(data_train, tokenizer, 256)\ntest_dataset = SplitterDataset(data_test, tokenizer, 256)\ntrain_dataset[0], len(train_dataset), len(test_dataset)","64e30f97":"train_dataloader = DataLoader(train_dataset, batch_size=64, collate_fn=train_dataset.collate_fn, shuffle=True, num_workers=0)\ntest_dataloader = DataLoader(test_dataset, batch_size=64, collate_fn=test_dataset.collate_fn, shuffle=False, num_workers=0)","dd40cc44":"for i, batch in enumerate(train_dataloader):\n    if i == 1:\n        break\n    x, m, y = batch[0], batch[1], batch[2]\nx[0], m[0], y[0]","0bb644a1":"import torch\nclass SplitterModel(torch.nn.Module):\n    def __init__(self):\n        super(SplitterModel, self).__init__()\n        self.backbone = DistilBertModel.from_pretrained('distilbert-base-uncased')\n        self.head = nn.Linear(768, 1)\n\n    def forward(self, input_ids, attention_mask):\n        h = self.backbone(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n        logit = self.head(h)\n        return logit","cc3da4a7":"model = SplitterModel().to(device)","9c85871b":"n_epoch = 3\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epoch*len(train_dataloader))","7abbe735":"best_te_loss = 1e5\nbest_ep = -1\n\nfor epoch in range(n_epoch):\n    print(f\"running epoch {epoch}, best test loss {best_te_loss} after epoch {best_ep}\")\n    #train\n    model.train()\n    step = 0\n    tr_loss = 0.0\n    pbar = tqdm(train_dataloader, leave=False)\n    for i, batch in enumerate(pbar):\n        step += 1\n        input_ids, attention_mask, labels = batch\n\n        optimizer.zero_grad()\n\n        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = logits.squeeze(-1)\n        B, N = logits.size()\n\n        loss = criterion(logits, labels)\n\n        lossitem = loss.item()\n        tr_loss += lossitem\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n        pbar.set_description(f\"tr batch loss: {lossitem}\", refresh=True)\n    tr_loss \/= step\n    # test\n    model.eval()\n    step = 0\n    te_loss = 0.0\n    test_pbar = tqdm(test_dataloader, leave=False)\n    with torch.no_grad():\n        for i, batch in enumerate(test_pbar):\n            step += 1\n            input_ids, attention_mask, labels = batch\n            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = logits.squeeze(-1)\n            B, N = logits.size()\n            loss = criterion(logits, labels)\n            lossitem = loss.item()\n            te_loss += lossitem\n\n            test_pbar.set_description(f\"te batch loss: {lossitem}\", refresh=True)\n        te_loss \/= step\n#     epoch_pbar.set_description(f\"epoch tr loss: {tr_loss}, te loss: {te_loss}\", refresh=True)\n    print(f\"epoch {epoch}, tr_loss {tr_loss}, te loss: {te_loss}\")\n    if te_loss < best_te_loss:\n        best_ep = epoch\n        best_te_loss = te_loss\n        torch.save(model.state_dict(), \"best_model.pt\")","39c76830":"model.load_state_dict(torch.load('..\/input\/bert-splitter-dirty\/best_model.pt'))\n# model.load_state_dict(torch.load('.\/best_model.pt'))","76568be7":"model.eval()\ntotal = 0\ncorrect = 0\nmidcut_correct = 0\ntest_pbar = tqdm(test_dataloader)\n\nwith torch.no_grad():\n    for i, batch in enumerate(test_pbar):\n        input_ids, attention_mask, labels = batch\n        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = logits.squeeze(-1)\n        B, N = logits.size()\n        total += B\n        correct += torch.sum(torch.argmax(logits, dim=1)==labels).item()\n        for j in range(B):\n            sample = input_ids[j]\n            sample_len = torch.sum(sample!=0).item()\n            cut = sample_len \/\/ 2\n            if cut == labels[j].item():\n                midcut_correct += 1\n\nprint(\"test precision: {}\".format(correct\/total))\nprint(\"middle cut precision: {}\".format(midcut_correct\/total))","9d9b8b88":"sidx = 11\nidx = 51\nwith torch.no_grad():\n    for i, batch in enumerate(test_dataloader):\n        if i != sidx:\n            continue\n        else:\n            input_ids, attention_mask, labels = batch\n            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = logits.squeeze(-1)\n            B, N = logits.size()\n            pred = torch.argmax(logits, dim=1)[idx].item()\n            gt = labels[idx].item()\n            print(\"predicted upper: \" + tokenizer.decode([x for x in input_ids[0][1:pred+1].tolist() if x != 0]))\n            print(\"predicted lower: \" + tokenizer.decode([x for x in input_ids[0][pred+1:].tolist() if x != 0]))\n            print()\n            print(\"gt upper: \" + tokenizer.decode([x for x in input_ids[0][1:gt+1].tolist() if x != 0]))\n            print(\"gt lower: \" + tokenizer.decode([x for x in input_ids[0][gt+1:].tolist() if x != 0]))\n            break","72f394da":"# Training","f62b9465":"## Validation Precision","b22b4bf4":"# Preparing Data and Model","dc235062":"# Evaluating","b1661a69":"## SplitterModel","0eafec7e":"## Loading Pre-trained BERT Model","8a1478e4":"## 20% Data for Validation","e63599bf":"## Case Analysis","a7e4ad4a":"## SplitterDataset"}}