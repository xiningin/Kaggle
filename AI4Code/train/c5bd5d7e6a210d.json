{"cell_type":{"e2b5bedb":"code","5dea03b2":"code","e5a860a5":"code","4bb446f8":"code","36040d1c":"code","4c762fe7":"code","df4bf11e":"code","9a256587":"code","cc48f8a6":"code","2a5a1a28":"code","b05f72a7":"code","dd399d76":"code","90cf0987":"code","0d5672b5":"code","8fa0a92a":"code","0f404b08":"code","6e6802a8":"code","12d24078":"code","d4421ae7":"code","32f6a565":"code","697f2f02":"code","24a6e276":"code","8e46da60":"code","798945cc":"code","ecadaf40":"code","e05b1125":"code","a1afb55c":"code","ad6120a0":"code","d96264a2":"code","98dd30a0":"code","c87841c6":"code","f2cc6032":"code","84b04719":"code","9b3567f0":"code","f2836762":"code","d651e1e6":"code","a90ebd2e":"markdown","64e147f0":"markdown","80dbdda7":"markdown","d0eaed3f":"markdown","7fed3adc":"markdown","d1e48ee4":"markdown","ec4f4a09":"markdown","464a698c":"markdown","f8d404fa":"markdown","273e6299":"markdown","53b96d46":"markdown","117be8d6":"markdown","e8f6169b":"markdown","5d6a70ed":"markdown","5bf9271d":"markdown","2d4d4e3a":"markdown","687ef33c":"markdown","9835b61f":"markdown","6f1b4fd6":"markdown","74f0acab":"markdown","eafb711c":"markdown","e166480e":"markdown","975d2f30":"markdown","3c9376b4":"markdown","efe07544":"markdown"},"source":{"e2b5bedb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5dea03b2":"# Load libraries for analysis and visualization\nimport matplotlib.pyplot as plt \n\n# Machine learning libraries\nimport seaborn as sns  \n\n# https:\/\/github.com\/ResidentMario\/missingno -> using missingno functionality\n# Bilogur, (2018). Missingno: a missing data visualization suite. Journal of Open Source Software, 3(22), 547, \n# https:\/\/doi.org\/10.21105\/joss.00547\nimport missingno as msno","e5a860a5":"# to read those 2 files, and to assign them to the 2 variables\nTitanic_train_data = pd.read_csv('..\/input\/titanic\/train.csv')\nTitanic_test_data = pd.read_csv('..\/input\/titanic\/test.csv')","4bb446f8":"# provide the first 5 rows of Titanic_train_data (This variable is a DataFrame)\nTitanic_train_data.head()","36040d1c":"print(Titanic_train_data.shape, \"----> shows the Row x Column of the DataFrame \")","4c762fe7":"# this provides quick description of the data inside the DataFrame\n# it shows the details of each column in terms of how many records\/rows are NOT null, and type of each column\nprint(Titanic_train_data.info())","df4bf11e":"# The white lines are Null values (NaN)\nmsno.matrix(Titanic_train_data, color = (0.30, 0.60, 1.00) )","9a256587":"Titanic_test_data.head()","cc48f8a6":"# shows Columns with NaN records\ntrainData_cols_with_missing = [colu for colu in Titanic_train_data\n                     if Titanic_train_data[colu].isnull().any()]\n\ntrainData_cols_with_missing","2a5a1a28":"# shows Columns with NaN records, and the number of the records\nprint('-'*30 + 'training data' + '+'*30)\nfor i in np.arange(Titanic_train_data.shape[1]): # [1] = grabs all Columns -> then go through each Column\n    n = Titanic_train_data.iloc[:,i].isnull().sum() # sums NaN records in each Column\n    if n > 0:\n        print(list(Titanic_train_data.columns.values)[i] + ': ' + str(n) + ' NaNs')","b05f72a7":"# plots a histogram for each numerical attribute, i.e. a column contains integer type\nTitanic_train_data.hist(bins=20, figsize=(15,10)) \n\n# figsize(r,c) ===> r x c: r = number of rows from 1, c = # of columns from 1\n# https:\/\/www.pythonpool.com\/matplotlib-figsize\/ \n# --> Width \u2013 Here, we have to input the width in inches. The default width is 6.\n# --> Height \u2013 Here, we have to input the height of the graph. The default value is 4. \n\nplt.show()","dd399d76":"# filters Titanic_train_data by Survived = 1 (i.e. alive), outputs Sex and Survived columns, where grouping the outputs by Sex\n# decomposes \"Survived\" feature \/ column\nTitanic_train_data.loc[Titanic_train_data.Survived == 1][['Sex','Survived']].groupby(['Sex'], as_index=False).count()","90cf0987":"# filters Titanic_train_data by Survived = 0 (i.e. dead), outputs Sex and Survived columns, where grouping the outputs by Sex\n# decomposes \"Survived\" feature \/ column\nTitanic_train_data.loc[Titanic_train_data.Survived == 0][['Sex','Survived']].groupby(['Sex'], as_index=False).count()","0d5672b5":"Surv = Titanic_train_data.loc[Titanic_train_data.Survived == 1]\nNoSurv = Titanic_train_data.loc[Titanic_train_data.Survived == 0]\n\nfigure, axs = plt.subplots(1, 2, figsize=(20,10)) \n\nsns.histplot(x='Age',hue='Survived',data=Surv, ax=axs[0]) # axis starts from 0\nsns.histplot(x='Age',hue='Survived',data=NoSurv, ax=axs[1]) \n\naxs[0].set_title('Total passengers grouping by Age that survived')\naxs[1].set_title('Total passengers grouping by Age that died')","8fa0a92a":"plt.figure(figsize=(20,15))\n\nSeabornSwarmPlot = sns.swarmplot(x='Pclass',y='Fare',hue='Survived',data=Titanic_train_data, palette='colorblind', dodge=True)\n# for values of palette = https:\/\/seaborn.pydata.org\/generated\/seaborn.color_palette.html#seaborn.color_palette\n# https:\/\/seaborn.pydata.org\/examples\/palette_choices.html?highlight=palette\n\nSeabornSwarmPlot.set_title('Plot of Fare between Pclass with regards to Survival')\n\nplt.show()","0f404b08":"# there are 177 NaNs in Age column. \n#\u00a0Model in Machine Learning cannot accept Null. So either remove the Age column OR add a \"value\" in those 177 records.\n\ntrainDataImputed = Titanic_train_data.copy()\nprint(trainDataImputed['Age'].mean(), \"mean before Imputation\") # 29.69911764705882\nprint(trainDataImputed['Age'].sum(),\"sum before Imputation\") # 21205.17\n\ntrainDataImputed['Age'] = trainDataImputed['Age'].fillna(trainDataImputed['Age'].mean())\n\nprint(trainDataImputed['Age'].mean(),\"mean after Imputation\") # 29.699117647058763 \nprint(trainDataImputed['Age'].sum(),\"sum after Imputation\") # 26461.91382352941\n\nprint(trainDataImputed.info()) ","6e6802a8":"# Interesting observation\n\n# to find people who have Ticket = 113803\n# So far I know Index = 3, PassengerId = 4, Female, SibSp = 1, Survived = 1, Pclass = 1, owned that Ticket\ntrainDataImputed.loc[trainDataImputed.Ticket == '113803']","12d24078":"print(\"This shows an example of SibSp= 1 and same Ticket\")\nprint(\"They were a couple. And also, of course, shared the same Cabin = C123\")\nprint(\"Pclass =1, so Upper class\")\nprint(\"The wife Survived but not the husband. Shows that Male did NOT survive eventhough on Upper class\")","d4421ae7":"# Interesting observation\n\n# to find people who have Ticket = W.\/C. 6607\n# So far I know index = 888, PassengerId = 889, Female, SibSp = 1, Parch= 2, Survived = 0, Pclass = 3, owned that Ticket\ntrainDataImputed.loc[trainDataImputed.Ticket == 'W.\/C. 6607']","32f6a565":"print(\"This shows an example of SibSp= 1, Parch = 2 and same Ticket\")\nprint(\"Note that both were missing Age. So now it uses Mean age of the column\")\nprint(\"I think the guy is the father, and the woman is the daughter, hence Parch = 2\")\nprint(\"Both were in Pclass = 3, lower class, and both were dead\")\nprint(\"Here irregardless she was a Female, she did not surive\")","697f2f02":"trainDataImputed['Sex']=trainDataImputed['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n\nprint(trainDataImputed['Sex'], \"----> after remapping the String to Integer\")\n# it works here regarding Map(). I did this directly onto the Series \"Sex\" inside the trainDataImputed DataFrame","24a6e276":"# so based on above, drops the relevant columns\nnewTrainDataImputed = trainDataImputed.drop(['PassengerId','Survived','Name','SibSp','Parch','Ticket','Cabin','Embarked'], axis=1)\n\nnewTrainDataImputed","8e46da60":"# check if there is a missing value\/Null\/NaN in the above columns\n# copied from https:\/\/www.kaggle.com\/alexisbcook\/missing-values\n\ncols_with_missing = [col for col in newTrainDataImputed\n                     if newTrainDataImputed[col].isnull().any()]\n\ncols_with_missing","798945cc":"# set the relevant variables for the Training Data\n\ny_train = Titanic_train_data['Survived']\nX_train = newTrainDataImputed","ecadaf40":"# I must drop columns in the Test data.\n# These columns come from the analysis I did from above. \n\nX_test = Titanic_test_data.copy()\nX_test = X_test.drop(['PassengerId','Name','SibSp','Parch','Ticket','Cabin','Embarked'], axis=1)\n\n# I must change the Sex to 0 and 1\nX_test['Sex']=X_test['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n\nX_test.describe()","e05b1125":"# I must replace NaN (Null value) to Mean value of the Age column\n\nprint(X_test['Age'].mean(), \"mean before Imputation\") # 30.272590361445783\nprint(X_test['Age'].sum(),\"sum before Imputation\") # 10050.5\nX_test['Age'] = X_test['Age'].fillna(X_test['Age'].mean())\nprint(X_test['Age'].mean(),\"mean after Imputation\") # 30.272590361445815 \nprint(X_test['Age'].sum(),\"sum after Imputation\") # 12653.942771084337\n\nprint(\"------\")\n\n# I must replace 1 NaN value in Fare, from the initial analysis\n# Use Mean value\n\nprint(X_test['Fare'].mean(), \"mean Fare before Imputation\") # 35.6271884892086\nprint(X_test['Fare'].sum(),\"sum Fare before Imputation\") # 14856.5376\nX_test['Fare'] = X_test['Fare'].fillna(X_test['Fare'].mean())\nprint(X_test['Fare'].mean(),\"mean Fare after Imputation\") # 35.6271884892086\nprint(X_test['Fare'].sum(),\"sum Fare after Imputation\") # 14892.164788489208\n\nX_test.describe()","a1afb55c":"# to double check if there is still a missing value\/Null\/NaN in the above columns\n\nX_test_cols_with_missing = [COLUMN for COLUMN in X_test\n                     if X_test[COLUMN].isnull().any()]\n\nX_test_cols_with_missing","ad6120a0":"#import models\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.linear_model import LogisticRegression","d96264a2":"model_RFR = RandomForestRegressor(n_estimators = 100, random_state = 0)\nmodel_RFR.fit(X_train,y_train)\nprint(model_RFR.fit(X_train,y_train), \"---> model_RFR.fit(X_train,y_train)\")\n\npreds_RFR = model_RFR.predict(X_test)\nprint(preds_RFR, \"---> preds_RFR\")","98dd30a0":"model_RFC = RandomForestClassifier(n_estimators = 100, random_state = 0)\nmodel_RFC.fit(X_train,y_train)\nprint(model_RFC.fit(X_train,y_train), \"---> model_RFC.fit(X_train,y_train)\")\npreds_RFC = model_RFC.predict(X_test)\nprint(preds_RFC, \"---> preds_RFC\")","c87841c6":"model_LogReg = LogisticRegression(random_state=0)\nmodel_LogReg.fit(X_train,y_train)\nprint(model_LogReg.fit(X_train,y_train), \"---> model_LogReg.fit(X_train,y_train)\")\npreds_LogReg = model_LogReg.predict(X_test)\nprint(preds_LogReg, \"---> preds_LogReg\")","f2cc6032":"output = pd.DataFrame({'PassengerId': Titanic_test_data.PassengerId, 'Survived': preds_RFR})\noutput.to_csv('trial01_RFR_submission.csv', index=False) #trial 01 = RandomForestRegressor\nprint(\"Your submission was successfully saved!\")\n# Public score = 0","84b04719":"output = pd.DataFrame({'PassengerId': Titanic_test_data.PassengerId, 'Survived': preds_RFC})\noutput.to_csv('trial02_RFC_submission.csv', index=False) #trial 02 = RandomForestClassifier\nprint(\"Your submission was successfully saved!\")\n# Public score = 0.76315","9b3567f0":"output = pd.DataFrame({'PassengerId': Titanic_test_data.PassengerId, 'Survived': preds_LogReg})\noutput.to_csv('trial03_LogReg_submission.csv', index=False) #trial 03 = LogisticRegression\nprint(\"Your submission was successfully saved!\")\n# Public score = 0.75598","f2836762":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline \n\ntest_pipeline = Pipeline([(\"std_scaler\",StandardScaler()),(\"RFC\",RandomForestClassifier(n_estimators = 100, random_state = 0))])\ntest_pipeline.fit(X_train,y_train)\nprint(test_pipeline.fit(X_train,y_train), \"---> test_pipeline.fit(X_train,y_train)\")\ny_preds_RFC = test_pipeline.predict(X_test)\nprint(y_preds_RFC, \"---> y_preds_RFC\")","d651e1e6":"output = pd.DataFrame({'PassengerId': Titanic_test_data.PassengerId, 'Survived': y_preds_RFC})\noutput.to_csv('trial04_PipelineRFC_submission.csv', index=False) #trial 04 = Pipeline containing RandomForestClassifier\nprint(\"Your submission was successfully saved!\")\n# Public score = ??","a90ebd2e":"**Note**\n\nBelow codes are by default added when you created a new Notebook","64e147f0":"**Modify Sex column in the Training data**\n\nTo change values in Sex column to Integer from String\n\ne.g Female = 0, Male = 1","80dbdda7":"## **Step C**\n\n--> *Step 3: Cleansing, integrating, and transforming data*\n\n==> *Discover and Visualize the Data to Gain Insights*","d0eaed3f":"# Inspirations:\n\n* Professor Andrew Ng\u00b4s Machine Learning course in Coursera; https:\/\/www.coursera.org\/learn\/machine-learning\n* Courses from Kaggle.com; https:\/\/www.kaggle.com\/learn: \n    * Python, \n    * Intro to Machine Learning, \n    * Intermediate Machine Learning, \n    * Pandas, \n    * and Microchallenges\n* https:\/\/www.kaggle.com\/ydalat\/titanic-a-step-by-step-intro-to-machine-learning\n* https:\/\/www.kaggle.com\/alexisbcook\/missing-values\n* https:\/\/www.kaggle.com\/code1110\/houseprice-data-cleaning-visualization\/comments#262739\n* https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n* https:\/\/www.kaggle.com\/dansbecker\/cross-validation\n* https:\/\/medium.com\/kaggle-blog\/i-trained-a-model-what-is-next-d1ba1c560e26?source=collection_home---4------0-----------------------\n* Book: Introducing Data Science, Big Data, Machine Learning, and more, using Python tools\n* Book: Hands-On Machine Learning with Scikit-Learn and TensorFlow 2nd edition\n* https:\/\/www.kaggle.com\/questions-and-answers\/161303\n* https:\/\/www.theodinproject.com\/paths\/foundations\/courses\/foundations\/lessons\/git-basics\n* https:\/\/stackoverflow.com\/questions\/49310470\/using-kaggle-datasets-in-google-colab\n* https:\/\/colab.research.google.com\/drive\/1DofKEdQYaXmDWBzuResXWWvxhLgDeVyl\n* https:\/\/github.com\/Kaggle\/kaggle-api\n* https:\/\/github.com\/googlecolab\/colabtools\/issues\/17\n* https:\/\/stackoverflow.com\/questions\/48420759\/upload-local-files-using-google-colab\n* https:\/\/www.youtube.com\/watch?v=eEgZtNOCJhk\n* https:\/\/contentsimplicity.com\/how-to-set-up-kaggle-in-google-colab\/\n* https:\/\/www.machinelearningmindset.com\/kaggle-dataset-in-google-colab\/\n* https:\/\/colab.research.google.com\/drive\/1DofKEdQYaXmDWBzuResXWWvxhLgDeVyl#scrollTo=rPZaJ7oOGuyh\n* http:\/\/www.dataperspective.info\/2019\/02\/how-to-import-data-into-google-colab.html\n* https:\/\/www.kdnuggets.com\/2019\/01\/more-google-colab-environment-management-tips.html\/\n* https:\/\/www.kdnuggets.com\/2018\/02\/essential-google-colaboratory-tips-tricks.html\n* https:\/\/stackoverflow.com\/questions\/47320052\/load-local-data-files-to-colaboratory\n* https:\/\/github.com\/googlecolab\/colabtools\n* https:\/\/research.google.com\/colaboratory\/faq.html?hl=zh-tw\n* https:\/\/colab.research.google.com\/notebooks\/io.ipynb\n* https:\/\/cloud.google.com\/automl-tables\/docs\/notebooks\n* https:\/\/github.com\/GoogleCloudPlatform\/ai-platform-samples\/blob\/master\/notebooks\/samples\/tables\/census_income_prediction\/getting_started_notebook.ipynb\n* https:\/\/research.google.com\/colaboratory\/local-runtimes.html\n* https:\/\/cloud.google.com\/automl-tables\/docs\/beginners-guide","7fed3adc":"Quick observations\n\n* At this stage, to remove Cabin column.\n* Reason is it is 204 filled with values (204\/891 = 22.90%)\n* To think about. How does the Cabin column contribute to Survival, if any?\n\n\n* Also, I remove Name column, because I do not know how to manipulate it at this stage.\n* My feeling is there could be racist bias towards Non \"American\" at that time.\n* E.g. anyone with Irish name, or maybe Italian name?","d1e48ee4":"The output is WRONG! It does not provide 1 or 0 for survival.\n\nWhy this happened?\n\nError 1:\n* Error when want to use Random Forest Regressor.\n* You have categorical data, but your model needs something numerical. \n* This happened when I did not change the values \"Female\" and \"Male\" in the \"Sex\" column to numbers such as \"10\" and \"500\".\n* From a Kaggle's explanation \"See our one hot encoding tutorial for a solution\": https:\/\/www.kaggle.com\/dansbecker\/using-categorical-data-with-one-hot-encoding\n\nThis fixed only symptom instead of the root cause of the error.\n\nError 2:\nThis is the root cause of the error.\n\n* The issue is the Titanic problem is a CLASSIFICATION instead of REGRESSION topic\n* Here the Regression model is predicting value for Survival for Person ABC.\n* I want to classify if Person ABC died or alive.\n\nWhat to do next time?\n\n**Ensure I understand what the issue\/objective is.**\n","ec4f4a09":"**Notes**\n\nSince I created this notebook from \"Code\" -> \"New Notebook\", I MUST MANUALLY add the Titanic train.csv and test.csv.\n\nHow to add MANUALLY those 2 files:\n\n* Go to upper right hand corner of the notebook\n* Go to \"+ Add Data\" -> click on it\n* Go to \"Competition Data\" \n* Seek Titanic\n* Click \"Add\" button\n\net voil\u00e0!\n\nIf I created the notebook out of the Titanic Competition, then the Kaggle system would have attached those 2 csv files AUTOMATICALLY.","464a698c":"Based on what I investigated so far, in the Training Data (Titanic_train_data)\n\n**I want to retain:**\n* Pclass -> Upper class seem to likely survive\n* Sex -> More Females survived than Males\n    * Surived Female 233 (68,13%)\n    * Survived Male 109 (31,87%)\n    * Percentage survived Female in relative to total passengers (= 233 \/ 891) = 26.15%\n    * Percentage survived Male in relative to total passengers (= 109 \/ 891) = 12.23%\n\n* Age -> Needs further refinement in analysis. So far I have weak reasoning. I need to break this down more in terms of Pclass.\n    * 30+ kids aged 0 to 5 survived in comparison to 12+ who did not.  \n    * Another outlier aged 75 to 80 who survived.\n* Fare -> It seems if you paid more and in Pclass 1, you were likely to survive.\n\n**To remove:**\n* PassengerId -> No info\n* Survived -> this is what we WANT to PREDICT\n\n**Neutral**\n* Cabin -> shows additional info, but I do not know how to proceed. Put as To remove at the moment.\n* Embarked -> i do not know how this contributes or to analyse to survival. Put as To remove at the moment.\n* SibSp -> shows additional info, but I do not know how to proceed. Put as To remove at the moment.\n* Parch -> shows additional info, but I do not know how to proceed. Put as To remove at the moment.\n* Ticket -> shows additional info, but I do not know how to proceed. Put as To remove at the moment.\n* Name -> shows additional info, but I do not know how this contributes or to analyse to survival. Put as To remove at the moment.\n","f8d404fa":"We still have missing values in Age and Fare columns, 86 and 1 respectively.","273e6299":"# **Get your hands dirty:**","53b96d46":"## **Beginning of the process, Step A**\n\nWhat I understand so far, before starting work on Machine Learning \/ Data Science assignment, is to know what is your objective of the work. \n\n**Excerpt from the book: Introducing Data Science, Big Data, Machine Learning, and more, using Python tools. **\n\n--> *Step 1: Defining reseach goals and creating a project charter*\n\n**Excerpt from the book: Hands-On Machine Learning with Scikit-Learn and TensorFlow book 2nd edition. **\n\n==> *Look at the Big Picture -> Frame the Problem*\n\n***My answer***\n\nHere, basically I want to find out from the test.csv, who died and who survived.","117be8d6":"This produces an error\n\nfrom sklearn.pipeline import Pipeline \n\ntest_pipeline = Pipeline([\"RFC\",RandomForestClassifier(n_estimators = 100, random_state = 0)])\ntest_pipeline.fit(X_train,y_train)\nprint(test_pipeline.fit(X_train,y_train), \"---> test_pipeline.fit(X_train,y_train)\")\n\nError message\n\nAttributeError: 'RandomForestClassifier' object has no attribute 'estimators_'","e8f6169b":"**Repeat the codes for test_data:**\n\n\n>      Titanic_test_data.head()\n\n>      print(Titanic_test_data.shape, \"--> shows the Row x Column of the DataFrame \")\n\n>      print(Titanic_test_data.info())\n\n>      msno.matrix(Titanic_test_data)\n","5d6a70ed":"**Note**\n\nFind out why the error warning pops up, below is an example:\n\n\" \/opt\/conda\/lib\/python3.7\/site-packages\/seaborn\/categorical.py:1296: UserWarning: 22.7% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning) \"\n\nIt seems that either I am using the wrong plot function, or the current size of the swarmplot is small that it does not accommodate all the data. ","5bf9271d":"All good for the Test data. ","2d4d4e3a":"## **Step D**\n\n--> *Step 4: Exploratory data analysis*\n\n==> *Prepare the Data for Machine Learning Algorithms*","687ef33c":"I am a beginner to Data Science field and Machine Learning models. Below is the first round of my work, after many stumbling and iterations, on the Titanic dataset to understand the breadth and width of both topics.","9835b61f":"## **Step B**\n\n--> *Step 2: Retrieving data*\n\n==> *Get the Data*","6f1b4fd6":"# Relevant links for this analysis\n\n* https:\/\/github.com\/shafier\/Kaggle_Titanic\n    * Find my codes here in the GitHub.\n\n\n* https:\/\/colab.research.google.com\/github\/shafier\/Kaggle_Titanic\/blob\/main\/understanding-data-science-on-titanic-01.ipynb#scrollTo=r7EW1VQ8MMLD\n    * Open the analysis in Google Colaboratory.\n\n\n* https:\/\/workingaroundtheworld.wordpress.com\/2021\/05\/08\/understanding-data-science-part-01\/\n    * My blog on the analysis. ","74f0acab":"## **Step E**\n\n--> *Step 5: Build the models*\n\n==> *Select and Train a Model*","eafb711c":"**Important note**\n\nhttps:\/\/www.kaggle.com\/dansbecker\/handling-missing-values\n\nMost libraries (including scikit-learn) will give you an error if you try to build a model using data with missing values. \n\nThus, ensure there is NO missing value \/ Null value \/ NaN in any of the columns that you want to use for prediction. \n\n* I.e. you select Columns A, B, and K in the test DataFrame. \n* These are used for the prediction.\n* You have selected a Model PQR to do the prediction.\n* If 1 record in Column K contains no value, then Model PQR will show error message.","e166480e":"**Repeat the codes for test_data:**\n\n>       print('-'*30 + 'test data' + '+'*30)\n\n>       for i in np.arange(Titanic_test_data.shape[1]):\n\n>       n = Titanic_test_data.iloc[:,i].isnull().sum()\n\n>       if n > 0:\n\n>       print(list(Titanic_test_data.columns.values)[i] + ': ' + str(n) + ' NaNs')","975d2f30":"# Prepare the required variables to FIT a model and PREDICT \"Survived\" in the Test data","3c9376b4":"# Next actions\n\nFor my next iterations of the analysis, I want and would like to do the following: \n\n* Add \"Pipeline\"\n* Add \"Feature Engineering\"\n    * For example\n        * Add new \"Title\" column\n        > trainDataImputed['Title'] = trainDataImputed['Name'].str.extract(' ([A-Za-z]+)\\.', expand=True)\n        \n        > pd.crosstab(trainDataImputed['Title'], trainDataImputed['Sex'])\n* Create a function that runs the FIT, and PREDICT automatically\n* Add \"Correlations\"\n* Add \"Scatter Matrix\"\n* Add \"SimpleImputer\"\n* Add shortcut to have multiple Models https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python\n    * For example \n        > from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier)\n* Add \"Confusion matrix\"\n* Play with \"mean_absolute_error\"\n   > from sklearn.metrics import mean_absolute_error\n   \n   > print(mean_absolute_error(y_true_value, y_prediction_value))\n* Add \"Cross Validation\"\n    > from sklearn.model_selection import cross_val_score\n        \n    > scores = cross_val_score(my_pipeline, X, y, scoring='neg_mean_absolute_error')","efe07544":"# Aims\n\nStep 1 Get a simple Machine Learning model to work -> DONE\n\nStep 2 Work on improving the Training Data, e.g. from the view of creating a new column etc. \n\nStep 3 Work on adding complex models\n\nStep 4 Do Cross Validation\n\nI wrote these aims as reference points before getting started with the work. The aims are not fixed in stone. They can be changed depending on my level of understanding of Data Science. "}}