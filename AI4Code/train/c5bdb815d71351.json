{"cell_type":{"fe531412":"code","50092056":"code","99210c2a":"code","c663bbf5":"code","55a68a17":"code","9b048fd6":"code","47158d1e":"code","bc75e9b9":"code","6849608d":"code","b13338cd":"code","e82cd2be":"code","d21309f0":"code","d5350ee9":"code","b3a06bd8":"code","720dabb2":"code","2504cee9":"code","28e9641a":"code","a32368f8":"code","17322a95":"code","2d8866aa":"code","aeb13182":"code","caba8d44":"code","899730f3":"code","48371047":"code","beb9bb34":"code","d105ee09":"code","c687e148":"code","c726f338":"code","b0b4dfb2":"code","256480fb":"code","51bda544":"code","15c1bc96":"code","a0e27500":"code","7722b19e":"markdown","4c7995de":"markdown","048843a1":"markdown","9b53d6dd":"markdown","df764faf":"markdown","9d92e7a2":"markdown","9805ad9f":"markdown","bc46f966":"markdown","a94a5eec":"markdown","05f90655":"markdown","f11e6524":"markdown","ae2e6be6":"markdown","00bdc6df":"markdown"},"source":{"fe531412":"# import packages\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport warnings\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\n#from imblearn.over_sampling import SMOTE\n#from imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\nfrom collections import Counter\nfrom sklearn.decomposition import PCA\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","50092056":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","99210c2a":"# import data\ndfclass = pd.read_csv('\/kaggle\/input\/classification\/exercise_05_train.csv')\nX = dfclass.drop(columns = ['y'])\ny = dfclass['y'] ","c663bbf5":"# check if duplication\nprint('Dataset has', X.duplicated().sum(), ' duplications') #0\n# check data shape\nprint('Datashape as', X.shape) # 100 attributes, may need dimensional reduction\n# check response balancy\nfrom collections import Counter\nprint('Response counts as ', Counter(y)) # Counter({0: 31853, 1: 8147}) Not severely imbalanced\n# check missing value\nmissing_data = [x for x in X.columns if sum(X[x].isnull()) > 0]\nmis = pd.DataFrame(X[missing_data].dtypes)\nmis['prec'] = [100*(sum(X[i].isnull())\/X.shape[0]) for i in missing_data]\nprint('Dataset has ', len(missing_data), ' columns of missing value no more than', mis.T.iloc[1].max(), '%' )\nmis.T","55a68a17":"X.x41 = X.x41.astype(str).str.strip('$').astype(float)\nX.x45 = X.x45.astype(str).str.strip('%').astype(float)","9b048fd6":"# check summary of numerical data\nprint('Number of numerical cols ', len(X.select_dtypes(np.number).columns))\npd.DataFrame(X.describe(include = [np.number]))","47158d1e":"# impute with 0\nprint('column ', X.columns[(X.T==0).any(1)].tolist(), 'contains ', 100*(sum(X.x45==0)\/X.x45.shape[0]), '% of 0')\n# column 45 contains 39% of 0, impute na with 0\nX.x45.fillna(0, inplace = True)\nprint('x45 still has missing?', X.x45.isna().any())","bc75e9b9":"# impute with median\n# assumption: CV = std\/mean >1 then high (if mean is too small then only look at std value), impute median\nspread_x_median = pd.DataFrame(X.std().sort_values(ascending=False)[((X.std()\/X.mean()>1) & (X.mean()>1))|(X.std()>=50)])\nspread = spread_x_median.T.columns\nX[spread] = X[spread].fillna(X[spread].median())\nspread_x_median.T","6849608d":"# impute rest with median\nX = X.fillna(X.mean())","b13338cd":"num_col = X.select_dtypes(np.number).columns\n# identify outliers with upper and lower whisker \nq1 = X[num_col].quantile(0.25)\nq3 = X[num_col].quantile(0.75)\niqr = q3 - q1\nupw = q3+1.5*iqr\nlow = q1-1.5*iqr\n# find attributes with most outliers\nnum_col = X.select_dtypes(np.number).columns\ndfa = X[num_col][(X[num_col]<(low))|(X[num_col]>upw)]\ndfa.head(5) # outliers are spreaded out among different rows\nmd = [x for x in dfa.columns if sum(dfa[x].isnull()) > 0]\nms = pd.DataFrame(dfa[md].dtypes)\nms['prec'] = [100*(1- sum(dfa[i].isnull())\/dfa.shape[0]) for i in md]\nms['prec'].sort_values(ascending = False).head(3)\n# attribute with max outliers has <1.5% outliers","e82cd2be":"# outliers distribution with boxplot\n#import warnings\nwarnings.filterwarnings('ignore')\nboxp = X[['x44' ,'x75', 'x69']]\nboxp['y'] = y\nboxplot = boxp.boxplot(by='y', layout = (1,6), figsize = (30 , 10), fontsize = 20)","d21309f0":"import warnings\nprint(dfclass.corr()['y'].sort_values(ascending =False).head(4))\nwarnings.filterwarnings('ignore')\nboxp = X[['x97', 'x58', 'x1']]\nboxp['y'] = y\nboxplot = boxp.boxplot(by='y', layout = (1,6), figsize = (30 , 10), fontsize = 20)","d5350ee9":"cat_col = X.select_dtypes(np.object).columns\npd.DataFrame(X.describe(include = [np.object]))\n# impute with mode\nX.x34.fillna('volkswagon', inplace = True)\nX.x35.fillna('wed', inplace = True)\nX.x68.fillna('July', inplace = True)\nX.x93.fillna('asia', inplace = True)","b3a06bd8":"# clean typos\nprint('original set: ', set(X.x35))\nX.x35.replace({'fri':'friday', 'thur': 'thurday', 'wed':'wednesday' }, inplace = True)\nset(X.x35)","720dabb2":"dx34 = pd.get_dummies(X.x34)\nX = X.merge(dx34, left_index=True, right_index=True)\ndx93 = pd.get_dummies(X.x93) \nX = X.merge(dx93, left_index=True, right_index=True)\nX.head(5)","2504cee9":"print('original x35 weekdays: ', X.x35.value_counts())\nX.x35 =X.x35.replace({'monday':1, 'tuesday':2, 'wednesday':3, 'thurday':4, 'friday':5})\nprint('encode x35 weekdays: ', X.x35.value_counts())","28e9641a":"print('original x68 weekdays: ', X.x68.value_counts())\nX.x68 =X.x68.replace({'January':1, 'Feb':2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6 ,  'July':7, 'Aug':8, 'sept.':9, 'Oct':10, 'Nov':11, 'Dev':12})\nprint('encode x68 weekdays: ', X.x68.value_counts())","a32368f8":"print(X.select_dtypes(np.object).columns)\nX.drop(columns = ['x34', 'x93'], inplace = True)\nprint(X.shape) #100+10+3-2 = 111+y\n#X.to_csv('exercise_05_train_prep.csv')","17322a95":"# import data\ndfclass = pd.read_csv('\/kaggle\/input\/classification-dataset\/exercise_05_train_prepy.csv')\nX = dfclass.drop(columns = ['y'])\ny = dfclass['y'] ","2d8866aa":"# test among multiple models\n# split train and test set\nfrom sklearn.model_selection import train_test_split\nfrom lazypredict.Supervised import LazyClassifier\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=.5,random_state =10)\nclf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\nmodels,predictions = clf.fit(X_train, X_test, y_train, y_test)\nprint(models)","aeb13182":"# system time for kaggle notebook was way too long to run, import results from local\nfrom IPython import display \ndisplay.Image(\"\/kaggle\/input\/modelimage\/modeltest.PNG\")","caba8d44":"# Scaler : scale to [0,1] as dataset was NOT centroalized to 0\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nmscaler = MinMaxScaler() \nX_ms=mscaler.fit_transform(X)\n\n# PCA dimension reduction\nfrom sklearn.decomposition import PCA\npca = PCA(73)\npca.fit(X_ms)\nnp.cumsum(pca.explained_variance_ratio_) \n# require 73 pcas to retain 90% of total information and top 2 pcas each covers limited amount of information - up to 0.21\n\n# LDA dimension reduction\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda_u = LinearDiscriminantAnalysis() # n_components cannot be larger than min(n_features, n_classes - 1)\nprojectedl_u=lda_u.fit(X, y).transform(X)\nlda_1 =pd.Series(np.transpose(projectedl_u)[0], name ='LDA1')\nrefu = y.replace({0:'N', 1:'Y'})\ndftul = pd.concat([lda_1 , refu], axis=1)\nsns.pairplot(dftul,hue='y', plot_kws={'alpha': 0.5})","899730f3":"# Prepare validation set\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.pipeline import Pipeline\nfrom collections import Counter\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\nprint(Counter(y_train))\n\n# Balancing data REDUCE performance !!DO NOT Balance data\n#from imblearn.over_sampling import SMOTE\n#from imblearn.under_sampling import RandomUnderSampler\n#over = SMOTE(sampling_strategy=0.5)\n#under = RandomUnderSampler(sampling_strategy=0.5)\n#steps = [('o', over), ('u', under)]\n\n# scale data\npipeline2 = Pipeline(\n[('mmscale', MinMaxScaler() )])\n# PCA 73 lost 10% information REDUCE performance !!DO NOT reduce dimension for SVM\n# , ('pca', PCA(73))])\nX_train = pipeline2.fit_transform(X_train)\nX_test = pipeline2.transform(X_test)","48371047":"# Mannual grid search for best parameter\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import  SVC\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, f1_score, roc_auc_score, precision_score, recall_score, precision_recall_curve, average_precision_score\n  \n# defining parameter range \nparam_grid = {'C': [1, 10, 100, 1000],  # 1000 takes significant computing time\n              'gamma': [1, 0.1, 0.01, 'scale'], \n              'kernel': ['rbf', 'sigmoid'],\n              'degree': [1,2,3]\n             }   \ngrid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3, scoring='roc_auc') \n# fitting the model for grid search \ngrid.fit(X_train, y_train)\n\n# print best parameter after tuning \nprint(grid.best_params_) \n# print how our model looks after hyper-parameter tuning \nprint(grid.best_estimator_) \n\ny_predg = grid.predict(X_test)\nprint(f'Area Under Curve: {roc_auc_score(y_test, y_predg)}')\nprint(classification_report(y_test, y_predg))\n\n#SVC(default)\n#{'C': 1.0, 'class_weight': None,  # 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'random_state': None}\n#AUC: 0.8081950106371\n#weighted accuracy: 0.90\n\n#SVC(C=1000, gamma=0.01, degree=3, kernel='rbf')\n#AUC: 0.8564275553656884\n#weighted accuracy: 0.92\n\n#best parameter\n#SVC(C=100, gamma=0.1, degree=3, kernel='rbf')\n#AUC: 0.8851907734709265\n#weighted accuracy: 0.93","beb9bb34":"# Further automated grid search for parameter\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.svm import  SVC\n#!pip install skopt\nimport skopt\nfrom skopt import BayesSearchCV\nparams = dict()\nparams['C'] = (50.0, 150.0, 'log-uniform')\nparams['gamma'] = (0.005, 0.15, 'log-uniform')\n#params['degree'] = (1,5)\n#params['kernel'] = ['rbf', 'sigmoid']\n# define evaluation\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=10)\n# define the search\nsearch = BayesSearchCV(estimator=SVC(degree=3, kernel='rbf', probability=True), search_spaces=params, n_jobs=-1, cv=cv, scoring='roc_auc')\n# perform the search\nsearch.fit(X_train, y_train)\n# report the best result\nprint(search.best_score_)\nprint(search.best_params_)","d105ee09":"#chose model based on roc_auc score\n# no scale no pca 0.5\n# scale and pca 0.89\n\n# BEST scale without pca ROC AUC 0.97\nfrom sklearn.svm import  SVC\nmodel = SVC(C=100, gamma=0.1, degree=3, kernel='rbf', probability=True)\nmodel.fit(X_train, y_train)\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, f1_score, roc_auc_score, precision_score, recall_score, precision_recall_curve, average_precision_score\ny_pred = model.predict(X_test)\n#print(f'Area Under Curve: {roc_auc_score(y_test, y_pred)}')\nprint(classification_report(y_test, y_pred))\ny_pred_prob = model.predict_proba(X_test)\n\nprint(f'Area Under Curve based on predicted results: {roc_auc_score(y_test, y_pred)}')\nprint(f'Area Under Curve based on SVM estimated probability: {roc_auc_score(y_test,pd.DataFrame(y_pred_prob).reset_index()[1])}')\n\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score,confusion_matrix,roc_curve, roc_auc_score, precision_score, recall_score, precision_recall_curve, average_precision_score\n\nprint(f'Accuracy Score: {accuracy_score(y_test,y_pred)}')\nprint(f'Balanced Accuracy Score: {balanced_accuracy_score(y_test,y_pred)}')\nprint(f'F1 score: {f1_score(y_test,y_pred)}')\nprint(f'F1 Weighted score: {f1_score(y_test,y_pred, average=\"weighted\" )}')\nprint(f'Recall score: {recall_score(y_test,y_pred)}')\nprint(f'Precision: {precision_score(y_test, y_pred)}')\nprint(f'Confusion Matrix: \\n{confusion_matrix(y_test, y_pred)}')\nprint(f'Area Under Curve: {roc_auc_score(y_test, y_pred)}')\nprint(f'PR AUC: {average_precision_score(y_test, y_pred)}')","c687e148":"# lift chart\ndf=pd.DataFrame(y_pred_prob)\ndf['predicted']=df[1]\ndf['response']=pd.DataFrame(y_test).reset_index()['y']\n# rank observations by predicted probability\ndf['group'] = pd.qcut(df['predicted'], 20, labels=False, duplicates='drop')\ndf['group'] = df['group'].max() - df['group'] + 1\n# lift table \nlift_table = df.groupby('group').agg({'response':['count','sum'], 'predicted':['min','max']})\nlift_table.columns = ['count','response','min_score','max_score']\n# cumulative counts\ncumulative = {}\nfor i in lift_table.index:\n    cumulative[i] = {'cumulative_response': lift_table['response'][lift_table.index <= i].sum(),\n                     'cumulative_count': lift_table['count'][lift_table.index <= i].sum()}\ncumulative = pd.DataFrame(cumulative).transpose()\nlift_table = pd.merge(lift_table, cumulative, left_index=True, right_index=True)\n# cumulative percent, recall, accuracy, lift\ntotal_count = len(df)\ntotal_response = lift_table['response'].sum()\nlift_table['cumulative_percent'] = lift_table['cumulative_count'] \/ total_count\nlift_table['cumulative_recall'] = lift_table['cumulative_response'] \/ total_response\nlift_table['cumulative_accuracy'] = lift_table['cumulative_response'] \/ lift_table['cumulative_count']\nlift_table['cumulative_lift'] = lift_table['cumulative_recall'] \/ lift_table['cumulative_percent']\nlift_table","c726f338":"# load data and prepare array\nimport pandas as pd\nimport numpy as np\n# import as array\nfname ='\/kaggle\/input\/classification-dataset\/exercise_05_train_prepy.csv'\nall_features = []\nall_targets = []\nwith open(fname) as f:\n    for i, line in enumerate(f):\n        if i == 0:\n            #print(\"HEADER:\", line.strip())\n            continue  # Skip header\n        fields = line.strip().split(\",\")\n        all_features.append([float(v.replace('\"', \"\")) for v in fields[:-1]])\n        all_targets.append([int(fields[-1].replace('\"', \"\"))])\n        #if i == 1:\n            #print(\"EXAMPLE FEATURES:\", all_features[-1])\n\nfeatures = np.array(all_features, dtype=\"float32\")\ntargets = np.array(all_targets, dtype=\"uint8\")\nprint(\"features.shape:\", features.shape)\nprint(\"targets.shape:\", targets.shape)\n\n# validation set\nnum_val_samples = int(len(features) * 0.2)\ntrain_features = features[:-num_val_samples]\ntrain_targets = targets[:-num_val_samples]\nval_features = features[-num_val_samples:]\nval_targets = targets[-num_val_samples:]\n\nprint(\"Number of training samples:\", len(train_features))\nprint(\"Number of validation samples:\", len(val_features))\n\n#data balancy\ncounts = np.bincount(train_targets[:, 0])\nprint(\n    \"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n        counts[1], 100 * float(counts[1]) \/ len(train_targets)))\n\nweight_for_0 = 1.0 \/ counts[0]\nweight_for_1 = 1.0 \/ counts[1]","b0b4dfb2":"# normalize data\nmean = np.mean(train_features, axis=0)\nmeant=mean.copy()\nprint(mean[:5])\ntrain_features -= mean\nval_features -= mean\nstd = np.std(train_features, axis=0)\nstdt=std.copy()\nprint(std[:5])\ntrain_features \/= std\nval_features \/= std","256480fb":"#build model and tune hidden-layer\nfrom tensorflow import keras\n\nmodel = keras.Sequential(\n    [\n        keras.layers.Dense(256, activation=\"relu\", input_shape=(train_features.shape[-1],)),\n        keras.layers.Dense(256, activation=\"relu\"),\n        keras.layers.Dropout(0.3),\n        keras.layers.Dense(256, activation=\"relu\"),\n        keras.layers.Dropout(0.3),\n        \n        # tested, dropout rate shows best at 0.3 among [0.2-0.5]\n        \n        # tested, do not neet additional hidden layer\n        ##keras.layers.Dense(256, activation=\"relu\"),\n        ##keras.layers.Dropout(0.1),\n        \n        keras.layers.Dense(1, activation=\"sigmoid\"), #'sigmoid', 'softmax'\n    ]\n)\nmodel.summary()","51bda544":"#compile model and tune parameters\nmetrics = [\n    keras.metrics.FalseNegatives(name=\"fn\"),\n    keras.metrics.FalsePositives(name=\"fp\"),\n    keras.metrics.TrueNegatives(name=\"tn\"),\n    keras.metrics.TruePositives(name=\"tp\"),\n    keras.metrics.Precision(name=\"precision\"),\n    keras.metrics.Recall(name=\"recall\")\n    #keras.metrics.AUC(name ='AUC') # keras seemed to have issue with this metric\n]\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(1e-2), loss=\"binary_crossentropy\", metrics=metrics\n)\n\ncallbacks = [keras.callbacks.ModelCheckpoint(\"fraud_model_at_epoch_{epoch}.h5\")]\nclass_weight = {0: weight_for_0, 1: weight_for_1}\n\nmodel.fit(\n    train_features,\n    train_targets,\n    batch_size=2048, #500, 2000\n    epochs=30, #20, 30, 100\n    verbose=2,\n    callbacks=callbacks,\n    validation_data=(val_features, val_targets),\n    class_weight=class_weight\n)\n\n#alternative AUC_ROC score\nfrom sklearn.metrics import roc_auc_score\ny_pred = model.predict_proba(val_features)\nprint(roc_auc_score(val_targets, y_pred)) #0.9843862664204388","15c1bc96":"df['predicted']=pd.DataFrame(y_pred)\ndf['response']=val_targets\n# rank observations by predicted probability\ndf['group'] = pd.qcut(df['predicted'], 20, labels=False, duplicates='drop')\ndf['group'] = df['group'].max() - df['group'] + 1\n# lift table \nlift_table = df.groupby('group').agg({'response':['count','sum'], 'predicted':['min','max']})\nlift_table.columns = ['count','response','min_score','max_score']\n# cumulative counts\ncumulative = {}\nfor i in lift_table.index:\n    cumulative[i] = {'cumulative_response': lift_table['response'][lift_table.index <= i].sum(),\n                     'cumulative_count': lift_table['count'][lift_table.index <= i].sum()}\ncumulative = pd.DataFrame(cumulative).transpose()\nlift_table = pd.merge(lift_table, cumulative, left_index=True, right_index=True)\n# cumulative percent, recall, accuracy, lift\ntotal_count = len(df)\ntotal_response = lift_table['response'].sum()\nlift_table['cumulative_percent'] = lift_table['cumulative_count'] \/ total_count\nlift_table['cumulative_recall'] = lift_table['cumulative_response'] \/ total_response\nlift_table['cumulative_accuracy'] = lift_table['cumulative_response'] \/ lift_table['cumulative_count']\nlift_table['cumulative_lift'] = lift_table['cumulative_recall'] \/ lift_table['cumulative_percent']\nlift_table","a0e27500":"# svm m1\n# predict test set\nX_test_p = pipeline2.transform(Xt)\nyt_pred_prob = model.predict_proba(X_test_p)\n# ann m2\n# predict test set\nfname ='exercise_05_test_prepy.csv'\nt_features = []\nwith open(fname) as f:\n    for i, line in enumerate(f):\n        if i == 0:\n            #print(\"HEADER:\", line.strip())\n            continue  # Skip header\n        fields = line.strip().split(\",\")\n        t_features.append([float(v.replace('\"', \"\")) for v in fields[:]])\n\ntest_features = np.array(t_features, dtype=\"float32\")\n\nprint(\"features.shape:\", test_features.shape)\n\n#normalize data\ntest_features -= meant\ntest_features \/= stdt\n#predict results\nyt_pred = model.predict_proba(test_features)","7722b19e":"#### Final SVM Model 1\n- Transformation: \n\nmin-max scale without dimensional reduction\n- Final Model and parameter: \nSupport Vector Machine with Radial Basis Function kernel, C regularsation as 100, hyperplane (degree) has less impact in this case\n\n- Select reasons: \nModel was selected based on best ROC_AUC score (depend on calculation method range from 0.97-0.99), and balanced accuracy (0.98). As shown in lift chart, top 20% of test population with highest probability could capture almost 95% actual true response. As we do not have further information about the predict response, if focus on capturing actual predicted true (eg, identify covid paitent), chose high Recall.","4c7995de":"## Step 1 - Clean and prepare data","048843a1":"#### impute missing value\nWhat value to impute depends on data, impute mean\/median for age or weight which could not be 0, while absense for certain attributes should be treat as 0 \n\nSince there is no further information about the data, assume :\n- if original conatins 0, impute 0\n- else if attributes standard deviation is large (could potentialy contain outliers), impute median\n- else impute mean","9b53d6dd":"Encode:\n- Nominal Variables (no relationship between values), one-hot or dummy encode;\n- Ordinal Variables (sequential or hierarchy), ordinal encode or label encode","df764faf":"# Step 2 - Build Models\n\n\n- scale\n- dimensional reduction (+check correlation and interaction among attributes)\n- train test model (+balance data, GridSearchCV)\n- evaluate with ROC and lift table","9d92e7a2":"- do not see clear margin of separation based on LDA, PCA(3d)\n- high dimensional space, 70+ after PCA\n#### SVM had best performance from the test run and good at hgh dimensional space and seperating margine\n## Model 1 SVM","9805ad9f":"# Step 3 - Generate predictions\nprepare test set\npredict with SVM\npredict with ANN MLP","bc46f966":"### Model 2 ANN-MLP\nas PCA could not significantly reduce dimension while keep enough information test deep learning artificial neural network multilayer perceptron (ANN MLP) as alternative high dimensional efficiency model","a94a5eec":"### Numerical data EDAs\n- summary\n- missing value\n- outliers\n\n\n\nx41 and x45 are numerical data with $sign or %, convert to numerical then treat as numerical","05f90655":"# Step 4 - Compare models\n\nMultiple models were tested and SVM(support-vector machine) and ANN MLP*(artificial neural network multilayer perceptron) were selected for their best predicting performance, scored by ROC_AUC and balanced accuracy. Both SVM and ANN MLP have similar Accuracy and F1 score, ANN MLP has higher recall. However, which score should be used to evaluate model performance depends on business requirements. If the analysis meant to identify patient, accident or fraud, high recall would be preferred. Otherwise, if the analysis meant for promotion and low-cost advertising, high precision would be better.\nThere are pros and cons for each model\n##### SVM Pros\nSVM works more effective in high dimensional space (large number of attributes).\nSVM has low bias while provides regularisation parameter C*** to prevent over-fitting and improve generalization capability.\nSVM has strong stability and good performance in this analysis.\nCompared to ANN, no local minimum.\n##### SVM Cons\nTime: To achieve accurate prediction, C needs to be high and will require long computing time to fine tune.\nProbability Estimation: SVM does not generate probability, probability has to be estimated with Platt scaling.\nScaling: MUST scale data before applying SVM.\nInterpretation: Not easy to visualize and interpretate as other algorithms.\n\n##### ANN MLP Pros\nCapability of handle massive amount of data with parallel process and distributed memory.\nCapture complex relationship among data and generalize it to unseen data (high predicting performance).\nEfficient learning with only passing important information to next layer. -- Compared to SVM, capability of on-line learning\n##### ANN MLP Cons\nRequire hardware for batching process\nExists local minimum caused by different random weight initializations\nTime: To achieve accurate prediction, require long computing time to fine tune.\nScaling: MUST scale data\nInterpretation: Not easy to visualize and interpretate as other algorithms.\n\n\nTo achieve the best performance, ensemble method could be applied:\nWhen SVM and ANN MLP has different results, depend on the business requirement, a. if objective is to identify accident, fraud or not capturing actual true event could be costly, use ANN MLP for its high performance in capturing most actual true event (higher recall score); b. if objective is to promote, recruit or target actual true even with fixed sample, use SVM for its higher precision score.","f11e6524":"#### outliers\nWether delete or not outliers depending on assumption and source of outlier\n\nSince there is no further information about the data, assume :\n- if outliers are for both sides and do not impact response y, keep them\n- else if outliers all in the same row, data quality delete row or replace with mean","ae2e6be6":"### Categorical data EDAs\n- summary\n- missing value\n- enode\n\n\nx41 and x45 are numerical data with $sign or %, convert to numerical then treat as numerical","00bdc6df":"Outliers seemed to be distributed on both side, even for highly correlated attributes, outliers seemed not impacting the response. Not to delete outliers"}}