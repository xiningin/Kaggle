{"cell_type":{"f44321c7":"code","3dc649bb":"code","af78b012":"code","e017c71c":"code","d34842e8":"code","cbaa1178":"code","b0c2cf0d":"code","3f569d0e":"code","eb272ff3":"code","049d1aa2":"code","f6757988":"code","29c5938b":"code","5dd0d366":"markdown","ad753ad0":"markdown","daf23b3e":"markdown"},"source":{"f44321c7":"!pip install tensorflow_addons\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nimport pandas as pd\nimport numpy as np\nimport random\nfrom tensorflow.keras.callbacks import Callback, LearningRateScheduler\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import losses, models, optimizers\nimport tensorflow_addons as tfa\nimport gc\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import f1_score\n\nimport warnings\nwarnings.simplefilter('ignore')\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows', 500)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3dc649bb":"# configurations and main hyperparammeters\nEPOCHS = 180\nNNBATCHSIZE = 16\nGROUP_BATCH_SIZE = 4000\nSEED = 321\nLR = 0.0015\nSPLITS = 6\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)","af78b012":"# read data\ndef read_data():\n    train = pd.read_csv('\/kaggle\/input\/data-without-drift\/train_clean.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n    test  = pd.read_csv('\/kaggle\/input\/data-without-drift\/test_clean.csv', dtype={'time': np.float32, 'signal': np.float32})\n    sub  = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/sample_submission.csv', dtype={'time': np.float32})\n    \n    Y_train_proba = np.load(\"\/kaggle\/input\/ion-shifted-rfc-proba\/Y_train_proba.npy\")\n    Y_test_proba = np.load(\"\/kaggle\/input\/ion-shifted-rfc-proba\/Y_test_proba.npy\")\n    \n    for i in range(11):\n        train[f\"proba_{i}\"] = Y_train_proba[:, i]\n        test[f\"proba_{i}\"] = Y_test_proba[:, i]\n\n    return train, test, sub\n\n# create batches of 4000 observations\ndef batching(df, batch_size):\n    df['group'] = df.groupby(df.index\/\/batch_size, sort=False)['signal'].agg(['ngroup']).values\n    df['group'] = df['group'].astype(np.uint16)\n    return df\n\n# normalize the data (standard scaler). We can also try other scalers for a better score!\ndef normalize(train, test):\n    train_input_mean = train.signal.mean()\n    train_input_sigma = train.signal.std()\n    train['signal'] = (train.signal - train_input_mean) \/ train_input_sigma\n    test['signal'] = (test.signal - train_input_mean) \/ train_input_sigma\n    return train, test\n\n# get lead and lags features\ndef lag_with_pct_change(df, windows):\n    for window in windows:    \n        df['signal_shift_pos_' + str(window)] = df.groupby('group')['signal'].shift(window).fillna(0)\n        df['signal_shift_neg_' + str(window)] = df.groupby('group')['signal'].shift(-1 * window).fillna(0)\n    return df\n\n# main module to run feature engineering. Here you may want to try and add other features and check if your score imporves :).\ndef run_feat_engineering(df, batch_size):\n    # create batches\n    df = batching(df, batch_size = batch_size)\n    # create leads and lags (1, 2, 3 making them 6 features)\n    df = lag_with_pct_change(df, [1, 2, 3])\n    # create signal ** 2 (this is the new feature)\n    df['signal_2'] = df['signal'] ** 2\n    return df\n\n# fillna with the mean and select features for training\ndef feature_selection(train, test):\n    features = [col for col in train.columns if col not in ['index', 'group', 'open_channels', 'time']]\n    train = train.replace([np.inf, -np.inf], np.nan)\n    test = test.replace([np.inf, -np.inf], np.nan)\n    for feature in features:\n        feature_mean = pd.concat([train[feature], test[feature]], axis = 0).mean()\n        train[feature] = train[feature].fillna(feature_mean)\n        test[feature] = test[feature].fillna(feature_mean)\n    return train, test, features","e017c71c":"train, test, sample_submission = read_data()\ntrain, test = normalize(train, test)","d34842e8":"train = run_feat_engineering(train, batch_size = GROUP_BATCH_SIZE)\ntest = run_feat_engineering(test, batch_size = GROUP_BATCH_SIZE)\ntrain, test, features = feature_selection(train, test)","cbaa1178":"train['signal_rolling_mean_1h'] = train['signal'].rolling(window = 100).mean().fillna(0)\ntest['signal_rolling_mean_1h'] = test['signal'].rolling(window = 100).mean().fillna(0)\n\ntrain['signal_rolling_std_1h'] = train['signal'].rolling(window = 100).std().fillna(0)\ntest['signal_rolling_std_1h'] = test['signal'].rolling(window = 100).std().fillna(0)\n\ntrain['signal_rolling_mean_1t'] = train['signal'].rolling(window = 1000).mean().fillna(0)\ntest['signal_rolling_mean_1t'] = test['signal'].rolling(window = 1000).mean().fillna(0)\n\ntrain['signal_rolling_std_1t'] = train['signal'].rolling(window = 1000).std().fillna(0)\ntest['signal_rolling_std_1t'] = test['signal'].rolling(window = 1000).std().fillna(0)","b0c2cf0d":"# train.head()\n# test.head()\n# features","3f569d0e":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n# import lightgbm as lgb\nimport optuna.integration.lightgbm as lgb\nimport optuna, os, uuid, pickle","eb272ff3":"X = train.drop(['time', 'open_channels'], axis = 1)\ny = train['open_channels']\nX_test = test.drop(['time'], axis = 1)\nX_train, X_val, y_train, y_val = train_test_split(X, y)\n\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)\n\nparams = {\n    \"objective\": \"multiclass\",\n    \"num_class\": 11,\n    \"metric\": \"multi_logloss\",\n    \"verbosity\": -1,\n    \"boosting_type\": \"dart\",\n#     \"boosting_type\": \"goss\",\n    'num_boost_round': 3,\n}\n\nbest_params, tuning_history = dict(), list()\nmodel = lgb.train(\n    params,\n    lgb_train,\n    valid_sets=[lgb_train, lgb_eval],\n#     verbose_eval=1,\n    verbose_eval=100,\n    early_stopping_rounds=100,\n#     early_stopping_rounds=1,\n    best_params=best_params,\n    tuning_history=tuning_history,\n)\n\nprint(\"Best Params:\", best_params)\nprint(\"Tuning history:\", tuning_history)","049d1aa2":"best_params = model.params\nbest_params","f6757988":"model = lgb.train(best_params, lgb_train, valid_sets=lgb_eval, num_boost_round=3)\ny_pred = model.predict(X_test, num_iteration=model.best_iteration)\ny_pred_max = np.argmax(y_pred, axis=1)","29c5938b":"sample_submission['open_channels'] = y_pred_max\n# sample_submission['open_channels'].value_counts()\n\nsample_submission.to_csv('submission_wavenet_lgbm.csv', index=False, float_format='%.4f')","5dd0d366":"thanks to https:\/\/www.kaggle.com\/nxrprime\/wavenet-with-shifted-rfc-proba-and-cbr\nthanks to https:\/\/www.kaggle.com\/keitarokonishi\/lgbm-version-wavenet-with-shifted-rfc-proba-and-cb","ad753ad0":"## optuna","daf23b3e":"## check data"}}