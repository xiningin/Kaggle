{"cell_type":{"45a381f7":"code","34dfa040":"code","0af2d8cb":"code","0f99ded8":"code","37204d33":"markdown"},"source":{"45a381f7":"# importing the required libraries\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n#Apply the default default seaborn theme, scaling, and color palette\nsns.set()\n# Sklearn related imports\nfrom sklearn import datasets\nfrom sklearn import metrics\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix","34dfa040":"#load data\n#We have 30 columns of data on which a decision of whether breast cancer cells are malignant or benign is decided.\nfrom sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\n#split data into train and test\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=66)\n#instantiate KNN classifier n_neighbours=3\n#KNN only has 1 parameter to adjust which is n_neighbours\nclf = KNeighborsClassifier(n_neighbors=3)\n#fit classifier\nclf.fit(X_train, y_train)\n#evaluation\nprint(\"Running the model with n_neighbours=3.......\")\nprint(\"Training set score for n_neighbours=3: {:.2f}\".format(clf.score(X_train, y_train)))\nprint(\"Test set accuracy for n_neighbours=3: {:.2f}\".format(clf.score(X_test, y_test)))\n\n#we see that our model is about 86% accurate for n_neighbours=3, meaning the model predicted the class\n#correctly for 86% of the samples in the test dataset.\n\n#view the optimum n_neighbours by plotting n_neighbours vs Accuracy scove and test score \ntraining_accuracy = []\ntest_accuracy = []\n# try n_neighbors from 1 to 10\nneighbors_settings = range(1, 11)\nfor n_neighbors in neighbors_settings:\n    #instantiate KNN classifier\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n    #fit classifier\n    clf.fit(X_train, y_train)\n    #record training set accuracy\n    training_accuracy.append(clf.score(X_train, y_train))\n    # record generalization accuracy or test accuracy\n    test_accuracy.append(clf.score(X_test, y_test))\n    \n# basic chart plot\nplt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\nplt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\nprint()\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"n_neighbors\")\nplt.legend()\nprint(\"Best value for n_neighbours seems to peak at n_neighbours=6 where test accuracy is highest\")\nprint()\n\n#lets re-try the model with n_neighbours=6\n#instantiate KNN classifier\nclf = KNeighborsClassifier(n_neighbors=6)\n#fit classifier\nclf.fit(X_train, y_train)\n#evaluation\nprint(\"Re-running the model again with n_neighbours=6....\")\nprint(\"Training set score with n_neighbours=6: {:.2f}\".format(clf.score(X_train, y_train)))\nprint(\"Test set accuracy with n_neighbours=6: {:.2f}\".format(clf.score(X_test, y_test)))\n#we see that our model is about 94% accurate for n_neighbours=3, meaning the model predicted the class\n#correctly for 94% of the samples in the test dataset.\n","0af2d8cb":"# Classification Report\nprint(\"Classification report for KNN Model n_neighbours=6:\")\nprint()\nprint(classification_report(y_test, clf.predict(X_test)))\nprint()","0f99ded8":"# Plot the confusion matrix using Seaborn library\nprint(\"Correlation Matrix for KNN Model n_neighbours=6:\")\nplt.figure(figsize=(5,5))\n_ = sns.heatmap(confusion_matrix(y_test, clf.predict(X_test)), \n                annot=True,fmt='', annot_kws={\"size\": 18},cmap=plt.cm.winter_r) \n_ = plt.ylabel('Actual', fontweight='bold')\n_ = plt.xlabel('Predicted', fontweight='bold')","37204d33":"# K-Nearest Neighbour\n\n## Objective: Using the breast cancer dataset from the sklearn library, predict whether a cancer case is malignant or benign"}}