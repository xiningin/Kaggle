{"cell_type":{"33965d65":"code","56c2c1d4":"code","3e1c9dee":"code","390ea2a0":"code","72ac59c5":"code","0288fe85":"code","baf526ec":"code","e6c00733":"code","2a470eca":"code","d5f6c0a1":"code","c744a3ec":"code","1acd6fee":"code","db34f816":"code","f641a04d":"code","89e90d65":"code","440741f8":"code","26e8a769":"code","f4a24811":"code","6e724521":"code","84139336":"code","9258d1e9":"code","22cbe1bf":"code","e17ea5cc":"code","cbba3ec1":"markdown","c1cc9f1c":"markdown","c585be18":"markdown","40806c22":"markdown","a49a950b":"markdown","66379bf8":"markdown","1f48aa9f":"markdown","299835c0":"markdown","6019d27b":"markdown","9909b415":"markdown","6ab0746c":"markdown","103bc280":"markdown","a1b8bb1c":"markdown","2e31cb0a":"markdown","59ac1ee2":"markdown","12a4e604":"markdown","187a5fc1":"markdown","e63a76c1":"markdown","4b271a87":"markdown","7dcb2859":"markdown","ac22d00a":"markdown","e206c4de":"markdown","31a18af4":"markdown"},"source":{"33965d65":"import pandas as pd\nimport numpy as np\n\ndf_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","56c2c1d4":"print(df_train.shape)\nprint(df_test.shape)","3e1c9dee":"df_train.head()","390ea2a0":"df_train.info()","72ac59c5":"df_train_labels = df_train[\"Survived\"].copy() \ndf_train = df_train.drop([\"Survived\"], axis=1)","0288fe85":"df_train[\"Ticket\"].nunique()","baf526ec":"df_train = df_train.drop([\"PassengerId\", \"Name\", \"Ticket\"], axis=1)","e6c00733":"from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy=\"median\")\ndf_train_num = df_train[[\"Age\",\"SibSp\",\"Parch\",\"Fare\"]]\nimputer.fit(df_train_num)\ndf_train_num_numpy = imputer.transform(df_train_num)\ndf_train_num = pd.DataFrame(df_train_num_numpy, \n                            columns=df_train_num.columns, \n                            index=df_train_num.index)","2a470eca":"df_train_num.head()","d5f6c0a1":"df_train = df_train.drop([\"Cabin\"], axis=1)","c744a3ec":"imputer_cat = SimpleImputer(strategy=\"most_frequent\")\ndf_train_cat = df_train[[\"Pclass\",\"Sex\",\"Embarked\"]]\nimputer_cat.fit(df_train_cat)\ndf_train_cat_numpy = imputer_cat.transform(df_train_cat)\ndf_train_cat = pd.DataFrame(df_train_cat_numpy, \n                            columns=df_train_cat.columns, \n                            index=df_train_cat.index)","1acd6fee":"df_train_cat.head()","db34f816":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ndf_train_num_numpy = scaler.fit_transform(df_train_num)\ndf_train_num = pd.DataFrame(df_train_num_numpy, \n                            columns=df_train_num.columns, \n                            index=df_train_num.index)","f641a04d":"df_train_num.head()","89e90d65":"from sklearn.preprocessing import OneHotEncoder\n\nohencoder = OneHotEncoder()\ndf_train_cat = ohencoder.fit_transform(df_train_cat)","440741f8":"df_train_cat = df_train_cat.toarray()","26e8a769":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass DeleteNotUsefulFeatures(BaseEstimator, TransformerMixin):\n    def __init__(self, delembarked = False):\n        self.delembarked = delembarked\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X = X.drop([\"PassengerId\",\"Name\",\"Ticket\",\"Cabin\"], axis=1)\n        if self.delembarked:\n            X = X.drop([\"Embarked\"], axis=1)\n        return np.c_[X]","f4a24811":"from sklearn.pipeline import Pipeline\n\npipeline_num = Pipeline([\n    ('imputer', SimpleImputer(strategy=\"median\")),\n    ('scaler', StandardScaler())\n])","6e724521":"pipeline_cat = Pipeline([\n    ('del_features', DeleteNotUsefulFeatures(delembarked=False)),\n    ('imputer_cat', SimpleImputer(strategy=\"most_frequent\")),\n    ('ohencoder', OneHotEncoder())\n])","84139336":"from sklearn.compose import ColumnTransformer\n\nnum_features = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]\ncat_features = [\"PassengerId\", \"Pclass\", \"Name\", \"Sex\", \"Ticket\", \"Cabin\", \"Embarked\"]\nall_features = num_features + cat_features\n\nfull_pipeline = ColumnTransformer([\n    ('num_transform', pipeline_num, num_features),\n    ('cat_transform', pipeline_cat, cat_features)\n])","9258d1e9":"training_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")","22cbe1bf":"training_data.head()","e17ea5cc":"train_transformed = full_pipeline.fit_transform(training_data)\ntrain_transformed","cbba3ec1":"#### Filling missing _Age_ values with Median with _SimpleImputer_\nScikit provides a great class to take care of missing values: _SimpleImputer_\n\nWe will apply this class on all numerical features so as to deal with missing values, if any, on unseen data.","c1cc9f1c":"It turns out that Ticket also has alot of unique values (80% approx of the entire set). So we should probably discard it for this tutorial.","c585be18":"#### Deleting _Cabin_ feature","40806c22":"# Introduction to Scikit-Learn Pipelines\n\nWhile working on a Machine Learning project, a vital step is to transform data before feeding it to heavy algorithms. Transformations can be done on all kinds of data types and helps the algoritm to learn from data quickly and accurately. Following are few examples of data transformations :-\n1. On numerical data :-<br>\n1.1. Min-max scaling<br>\n1.2. Standardisation, etc.<br>\n2. On categorical data :-<br>\n2.1. Ordinal Encoding<br>\n2.2. One-hot encoding, etc.<br>\n\nBesides these popular engineering functions, you can create your custom transformations in Scikit-Learn which are useful to merge, extract, and modify your data features. \n\nSo you did some EDA and brainstormed some feature engineering techniques for your dataset, great! But while handling a large dataset with 100s of attributes, keeping track of all these steps can get tedious. Also, after successfully applying all these functions on your training set and training your model, you need to go through the same process for your test set. As the task list gets longer, its easier to use the power of Scikit-Learn pipelines. Not only it keeps track of your transformations but also keeps your code clean, makes it easy to apply same changes on test set, and helps in de-bugging.\n\nHere, we will use the famous Titanic survival dataset and see how Pipelines can help us make our job easier.","a49a950b":"### 5.3. Builing Categorical Pipeline\nWe are adding three transformations in the Ctaegorical data pipeline. These includes, custom transformation class to delete features, to deal with missing values and One-hot encoder.","66379bf8":"### 5.2. Building Numerical Pipeline\nWe are adding two transformations in the Numerical data pipeline which we implemented earlier, i.e, to deal with missing values and Standardisation.","1f48aa9f":"df_train_cat is a sparse matrix, to convert it to dense matric use, toarray()","299835c0":"### Dealing with missing values \n\nIdeally we have 3 options to deal with missing values in a dataset :- \n1. Delete the attribute which contains missing values\n2. Delete rows with missing values \n3. Fill missing values with something (0, mean, median, etc.)\n\nIn the training set, we have 3 columns with missing data :- \n1. <b>Age<\/b> - It contains 177(891-714) missing values. While its closer to 20% of the entire training set, discarding it can be costly to the model as Age might be an important factor for survival. Also, filling it with 0 is also not suitable. So, its better to fill missing values with mean or median.\n2. <b>Cabin<\/b> - It contains 687(891-204) missing values. Choosing option 1 will be ideal in this situation as 204 datapoints might not add much value to our model(but it should be checked before being sure).\n3. <b>Embarked<\/b> - It contains 2 missing values. Choosing option 3 will be ideal. We can replace missing values with most frequent values.\n\nWhile we only have 3 features with missing values in our training data, we need to account for more features with missing value that might be in our test set.","6019d27b":"Here, we have imported SimpleImputer, created an imputer with strategy=\"median\", created a dataframe of just numerical features, applied fit() on the numerical dataset, and transformed the dataset. The output of transformed dataset is a NumPy array so we later convereted it to a Pandas Dataframe.","9909b415":"## 3. Standardising numerical features\nHere we will standardise basic numerical features using Scikit-Learn _StandardScaler_","6ab0746c":"### 5.1. Creating custom transformers \nCustom transformation is nothing but changing data and features according to insights and observations rather than using traditional transformations. For eg:- Creating a new column from product of 2 columns. In case of this dataset, we could create a new column with sum of SibSp and Parch and discard them if we want to reduce our features. Another can be made with combination of age and fare.\n\nPossibilities are endless, also, looking back at the current notebook, we have already implemented a transformation, we have deleted few attributes  which were not useful. \n\nHere is an example to create a custom data transformation class which work seemlessly with Scikit-Learn's other classes.","103bc280":"### 5.4. Getting everything together using _ColumnTransformer_\n\nNow we will merge the two pipelines to create one full pipeline and transform our complete dataset in a single go.","a1b8bb1c":"## 5. Creating Pipeline\nFinally, we move to the main step, creating a Scikit-Learn pipeline. All the above data transformations that we have applied can be implemented using a simple _Pipeline_ function very easily. \nFirst, we will learn to build a custom transformation class.","2e31cb0a":"### Observations\n\n1. <b>PassengerId<\/b> is of type int and indetifies each passenger uniquely but won't be useful for our model.\n\n2. <b>Survived<\/b> is the target attribute of type Bool.\n\n3. <b>Pclass<\/b> diversify passengers into classes. Although the value is of type integer, this feature is of categorical nature.\n\n4. <b>Name<\/b> is of type object(string). Most likely it will be unique to each passenger and can be discarded before training.\n\n5. <b>Sex<\/b> is of type object(string). Its of categorical nature.\n\n6. <b>Age<\/b> is of type float. Looking at the dataset summary, we can observe that it contains some missing values.\n\n7. <b>SibSp<\/b> is of type int and describes the number of siblings\/spouses of each passenger on the ship.\n\n8. <b>Parch<\/b> is of type int and describes the number of parents\/children of each passenger on the ship.\n\n9. <b>Ticket<\/b> is of type object(string). It might have many(or all) unique values.\n\n10. <b>Fare<\/b> is of type float.\n\n11. <b>Cabin<\/b> is of type object(string). Its of categorical nature but contains many missing values.\n\n12. <b>Embarked<\/b> is of type object(string). Its of categorical nature and contains few missing values.","59ac1ee2":"To get a quick summary of the dataset use info()","12a4e604":"### Deleting not useful features\nBy our observating above, we should discard PassengerId and Name while Ticket feature is still to be explored","187a5fc1":"### Separating target feature","e63a76c1":"## 4. Dealing with Categorical features\nWe are remaining with only 3 categorical features, which are, Pclass, Sex, and Embarked.\n\nWe will use Sci-kit learn's OneHotEncder class to transform these into one-hot vectors.","4b271a87":"<b> So we have successfully transformed our entire dataset using Scikit-Learn's Pipeline. This can be very useful in projects and is a great tool to power through the code.\n    \nIf you enjoyed the tutorial, please do give it an upvote!\nComment below for feedback and suggestions. Thank you!","7dcb2859":"#### Filling missing _Embarked_ values with Most Frequent with _SimpleImputer_\nWe will apply this class on all categorical features so as to deal with missing values, if any, on unseen data.","ac22d00a":"## 2. Feature analysis","e206c4de":"## 1. Importing Titanic survival dataset\nDataset available on Kaggle <a href=\"https:\/\/www.kaggle.com\/c\/titanic\/data?select=test.csv\">here<\/a>.","31a18af4":"We created a new class to delete unwanted features and which will run smoothly in our pipeline to transform data. \nHere, BaseEstimator is used to avail two extra methods (get_params() and set_params()) and TransformerMixin is used to avail fit_transform() without explicitly writing a function for it."}}