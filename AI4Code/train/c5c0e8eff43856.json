{"cell_type":{"b1530a30":"code","ce4d7cda":"code","158a4bb2":"code","e7ab601c":"code","1b8d519d":"code","5b2fde38":"code","184f962d":"code","4f8bf0c8":"code","7e3c106f":"code","f213aa09":"code","3ec0e90c":"code","6e6aeb2c":"code","2ef8e43d":"code","0d7b1b5f":"code","080f84fa":"code","ff95bdd3":"code","c5d574ac":"code","82267bd2":"code","2b8d5f23":"code","968f31c8":"code","4746f5c8":"code","c367f602":"code","42ecc56c":"markdown","c417af13":"markdown","cda5fb07":"markdown","a49ad394":"markdown","e601e887":"markdown","3737b677":"markdown","5f94b204":"markdown","dd52d02c":"markdown","01d7d211":"markdown","096a18fa":"markdown","84628d2e":"markdown","e2d75433":"markdown","837b57bd":"markdown","1f295344":"markdown","6726a475":"markdown","e95508ca":"markdown","caf80799":"markdown","3544ccf1":"markdown","9133975a":"markdown"},"source":{"b1530a30":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ce4d7cda":"data1=pd.read_csv(\"..\/input\/resumes_development.csv\")\ndata2=pd.read_csv(\"..\/input\/resumes_pilot.csv\")","158a4bb2":"data=pd.concat([data1,data2])\ndata.head(10)","e7ab601c":"data.info()","1b8d519d":"data[data.isnull().any(axis=1)].count()","5b2fde38":"data.drop([\"Female\", \"URM\", \"Disability\", \"Unnamed: 0\"], axis=1, inplace=True)","184f962d":"plt.figure(figsize=[5,5])\nsns.set(style='darkgrid')\nsns.countplot(x=\"Veteran\", data=data, palette='RdYlBu')\ndata.loc[:,'Veteran'].value_counts()","4f8bf0c8":"#train test split\ny=data.Veteran.values\nx=data.drop([\"Veteran\"], axis=1)\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3, random_state=1)","7e3c106f":"#Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(solver='lbfgs')\nlr.fit(x_train, y_train)\nlr_prediction = lr.predict(x_test)\nlr_score = lr.score(x_test,y_test)\nprint(\"Logistic Regression Test Accuracy: {}%\".format(round(lr.score(x_test,y_test)*100,2)))\n\n#Confusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\nlr_cm = confusion_matrix(y_test, lr_prediction)\n\n#Mean Squared Error\n\nfrom sklearn.metrics import mean_squared_error\nlr_mse = mean_squared_error(y_test, lr_prediction)","f213aa09":"#K_Nearest Neighbour (KNN) Classification\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=14)\nknn.fit(x_train, y_train)\nknn_prediction = knn.predict(x_test)\nknn_score = knn.score(x_test, y_test)\nprint(\"KNN Classification Test Accuracy: {}%\".format(round(knn.score(x_test,y_test)*100,2)))\n\n#Confusion Matrix\n\nknn_cm = confusion_matrix(y_test, knn_prediction)\n\n#Mean Squared Error\n\nknn_mse = mean_squared_error(y_test, knn_prediction)","3ec0e90c":"#Find Best K Value\n\nscore_list = []\nfor each in range(1,30):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train, y_train)\n    score_list.append(knn2.score(x_test, y_test))\nplt.plot(range(1,30), score_list)\nplt.xlabel(\"K Values\")\nplt.ylabel(\"Accuracy\")\nplt.show()","6e6aeb2c":"# Support Vector Machine (SVM)\n\nfrom sklearn.svm import SVC\nsvm = SVC(random_state=1, gamma='auto')\nsvm.fit(x_train, y_train)\nsvm_prediction = svm.predict(x_test)\nsvm_score = svm.score(x_test, y_test)\nprint(\"SVM Classification Test Accuracy: {}%\".format(round(svm.score(x_test,y_test)*100,2)))\n\n#Confusion Matrix\n\nsvm_cm = confusion_matrix(y_test, svm_prediction)\n\n#Mean Squared Error\n\nsvm_mse = mean_squared_error(y_test, svm_prediction)","2ef8e43d":"#Naive Bayes Classification\n\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train, y_train)\nnb_prediction = nb.predict(x_test)\nnb_score = nb.score(x_test, y_test)\nprint(\"Naive Bayes Classification Test Accuracy: {}%\".format(round(nb.score(x_test,y_test)*100,2)))\n\n#Confusion Matrix\n\nnb_cm = confusion_matrix(y_test, nb_prediction)\n\n#Mean Squared Error\n\nnb_mse = mean_squared_error(y_test, nb_prediction)","0d7b1b5f":"#Decision Tree Classification\n\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train, y_train)\ndt_prediction = dt.predict(x_test)\ndt_score = dt.score(x_test, y_test)\nprint(\"Decision Tree Classification Test Accuracy: {}%\".format(round(dt.score(x_test,y_test)*100,2)))\n\n#Confusion Matrix\n\ndt_cm = confusion_matrix(y_test, dt_prediction)\n\n#Mean Squared Error\n\ndt_mse = mean_squared_error(y_test, dt_prediction)","080f84fa":"#Random Forest Classification\n\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, random_state=1)\nrf.fit(x_train, y_train)\nrf_prediction = rf.predict(x_test)\nrf_score = rf.score(x_test, y_test)\nprint(\"Random Forest Classification Test Accuracy: {}%\".format(round(rf.score(x_test,y_test)*100,2)))\n\n#Confusion Matrix\n\nrf_cm = confusion_matrix(y_test, rf_prediction)\n\n#Mean Squared Error\n\nrf_mse = mean_squared_error(y_test, rf_prediction)","ff95bdd3":"#Visualization of Confusion Matrix\n\nplt.figure(figsize=(20,15))\n\nplt.suptitle(\"Confusion Matrixes\", fontsize=18)\n\nplt.subplot(2,3,1)\nplt.title(\"Logistic Regression Confusion Matrix\")\nsns.heatmap(lr_cm, cbar=False, annot=True, cmap=\"PuBuGn\", fmt=\"d\")\n\nplt.subplot(2,3,2)\nplt.title(\"KNN Classification Confusion Matrix\")\nsns.heatmap(knn_cm, cbar=False, annot=True, cmap=\"PuBuGn\", fmt=\"d\")\n\nplt.subplot(2,3,3)\nplt.title(\"SVM Classification Confusion Matrix\")\nsns.heatmap(svm_cm, cbar=False, annot=True, cmap=\"PuBuGn\", fmt=\"d\")\n\nplt.subplot(2,3,4)\nplt.title(\"Naive Bayes Classification Confusion Matrix\")\nsns.heatmap(nb_cm, cbar=False, annot=True, cmap=\"PuBuGn\", fmt=\"d\")\n\nplt.subplot(2,3,5)\nplt.title(\"Decision Tree Classification Confusion Matrix\")\nsns.heatmap(dt_cm, cbar=False, annot=True, cmap=\"PuBuGn\", fmt=\"d\")\n\nplt.subplot(2,3,6)\nplt.title(\"Random Forest Classification Confusion Matrix\")\nsns.heatmap(rf_cm, cbar=False, annot=True, cmap=\"PuBuGn\", fmt=\"d\")\n\nplt.show()","c5d574ac":"TN = [lr_cm[0,0], knn_cm[0,0], svm_cm[0,0], nb_cm[0,0], dt_cm[0,0], rf_cm[0,0]]\nFP = [lr_cm[0,1], knn_cm[0,1], svm_cm[0,1], nb_cm[0,1], dt_cm[0,1], rf_cm[0,1]]\nFN = [lr_cm[1,0], knn_cm[1,0], svm_cm[1,0], nb_cm[1,0], dt_cm[1,0], rf_cm[1,0]]\nTP = [lr_cm[1,1], knn_cm[1,1], svm_cm[1,1], nb_cm[1,1], dt_cm[1,1], rf_cm[1,1]]\nAccuracy = [lr_score, knn_score, svm_score, nb_score, dt_score, rf_score]\nMSE = [lr_mse, knn_mse, svm_mse, nb_mse, dt_mse, rf_mse]\nClassification = [\"Logistic Regression\", \"KNN Classification\", \"SVM Classification\", \"Naive Bayes Classification\", \n                  \"Decision Tree Classification\", \"Random Forest Classification\"]\nlist_matrix = [Classification, TN, FP, FN, TP, Accuracy, MSE]\nlist_headers = [\"Model\", \"TN\", \"FP\", \"FN\", \"TP\", \"Accuracy\", \"MSE\"]\nzipped = list(zip(list_headers, list_matrix))\ndata_dict = dict(zipped)\ndf=pd.DataFrame(data_dict)","82267bd2":"df","2b8d5f23":"trace1 = {\n    'x':df.Model,\n    'y':df.TN,\n    'name':'True Negative',\n    'type':'bar'}\n\ntrace2 = {\n    'x':df.Model,\n    'y':df.FP,\n    'name':'False Positive',\n    'type':'bar'}\n\ntrace3 = {\n    'x':df.Model,\n    'y':df.FN,\n    'name':'False Negative',\n    'type':'bar'}\n\ntrace4 = {\n    'x':df.Model,\n    'y':df.TP,\n    'name':'True Positive',\n    'type':'bar'}\n\ngraph = [trace1, trace2, trace3, trace4];\nlayout = {\n  'xaxis': {'title': 'Classification Models'},\n  'barmode': 'relative',\n  'title': 'Confusion Matrix Values of Classification Models'\n};\nfig = go.Figure(data = graph, layout = layout)\niplot(fig)","968f31c8":"#Accuracy\nplt.figure(figsize=(15,10))\nax= sns.barplot(x=df.Model, y=df.Accuracy, palette = sns.cubehelix_palette(len(df.Model)))\nax.set_xticklabels(ax.get_xticklabels(),rotation=30)\nplt.xlabel('Classification Models')\nplt.ylabel('Accuracy')\nplt.title('Accuracy Scores of Classification Models')\nfor i in ax.patches:\n    ax.text(i.get_x()+.19, i.get_height()-0.3, \\\n            str(round((i.get_height()), 4)), fontsize=15, color='white')\nplt.show()","4746f5c8":"#MSE\nplt.figure(figsize=(15,10))\nax= sns.barplot(x=df.Model, y=df.MSE, palette = sns.cubehelix_palette(len(df.Model)))\nax.set_xticklabels(ax.get_xticklabels(),rotation=30)\nplt.xlabel('Classification Models')\nplt.ylabel('Mean Squared Error')\nplt.title('MSE Scores of Classification Models')\nfor i in ax.patches:\n    ax.text(i.get_x()+.19, i.get_height()-0.1, \\\n            str(round((i.get_height()), 5)), fontsize=15, color='white')\nplt.show()","c367f602":"d = {'y_test': y_test, 'Logistic_Regression_prediction': lr_prediction, 'KNN_prediction': knn_prediction, \n     'SVM_prediction': svm_prediction, 'Naive_Bayes_prediction': nb_prediction, 'Decision_Tree_prediction': dt_prediction, \n     'Random_Forest_prediction': rf_prediction}\ndata1=pd.DataFrame(data=d)\ndata1.T","42ecc56c":"SUPPORT VECTOR MACHINE (SVM) CLASSIFICATION MODEL","c417af13":"DECISION TREE CLASSIFICATION MODEL","cda5fb07":"K-NEAREST NEIGHBOUR (KNN) CLASSIFICATION MODEL","a49ad394":"As as result **KNN Classification** and **SVM Classification** models have the best performance. In addition, **Decision Tree Classification** model has the worst performance for this study.","e601e887":"EXPLORATORY DATA ANALYSIS (EDA)\n\nThe data is split into two files. For using model selection, I concatenate two files. Then, I try to understand and prepare my dataset.","3737b677":"As we can see, there is no NaN value in the dataset. So, we can take a step for machine learning algorithms. I want to predict the **veteran status** of the applicants by other skills. But I want to analyze *only skill features*. So, I drop the **demographic informations **(Female, URM, Disability) and \"Unnamed: 0\" column.","5f94b204":"We can see the details of **veteran** feature:","dd52d02c":"CONFUSION MATRIX\n\nA confusion matrix is a summary of prediction results on a classification problem.\n\nPositive (P) : Observation is positive (for example: is a Veteran).\n\nNegative (N) : Observation is not positive (for example: is not a Veteran).\n\nTrue Positive (TP) : Observation is positive, and is predicted to be positive.\n\nFalse Negative (FN) : Observation is positive, but is predicted negative.\n\nTrue Negative (TN) : Observation is negative, and is predicted to be negative.\n\nFalse Positive (FP) : Observation is negative, but is predicted positive.","01d7d211":"For a better analysis, I summarize the confusion matrix values, accuracy and mean squared error scores of models:","096a18fa":"In addition, we can see y_test and prediction values on dataframe:","84628d2e":"COMPARISON OF ACCURACY AND MEAN SQUARED ERROR SCORES","e2d75433":"SPLITTING DATA FOR TRAINING AND TESTING\n\nI am going to split my data set into as train (x_train, y_train) and test (x_test, y_test) datas.\nThen I am going to teach my machine learning algorithms by using trainig data set.\nLater I will use my trained model to predict my test data (y_pred).\nFinally I will compare my predictions (y_pred) with my test data (y_test).","837b57bd":"RANDOM FOREST CLASSIFICATION MODEL","1f295344":"We can see TN, FP, FN, TP values of classification models on the stacked bar plot:","6726a475":"NAIVE BAYES CLASSIFICATION MODEL","e95508ca":"**SKLEARN LIBRARY CLASSIFICATION ALGORITHMS COMPARISON WITH STRATEGEION RESUME SKILLS**\n\nIn this study, I will compare the supervised machine learning algorithms by strategeion resume skills. The dataset is related to human resurces area and has binary features:\n* 218 skill features: whether or not the corresponding skill is on the applicant's resume.\n* 4 protected features: demographic information about the applicant.\n\nI compare these models in study:\n* Logistic Regression\n* KNN Classification\n* Support Vector Machine Classification\n* Naive Bayes Classification\n* Decision Tree Classification\n* Random Forest Classification","caf80799":"LOGISTIC REGRESSION CLASSIFICATION MODEL","3544ccf1":"**Accuracy:**\n\nIt is the ratio of number of correct predictions to the total number of input samples.\nAccuracy= Number of Correct Predictions\/ Total Number of Predictions Made","9133975a":"**Mean Squared Error(MSE):**\n\nMean Squared Error(MSE) takes the average of the square of the difference between the original values and the predicted values."}}