{"cell_type":{"8cb5f1e7":"code","7aac2c96":"code","dcc6961f":"code","4ea2a503":"code","ad81cad9":"code","46e20cb1":"code","3b3bcf49":"code","dd865137":"code","9745ebff":"code","0baa0e9e":"code","dc43256d":"code","8d99b8c2":"markdown","f1fef632":"markdown","a1f6721d":"markdown","ff65b3a3":"markdown","033b9767":"markdown","7126df74":"markdown","6cbadcad":"markdown","2f026a88":"markdown","1ecdf91a":"markdown"},"source":{"8cb5f1e7":"import numpy as np\n\nimport torch\nfrom torch import nn, optim\n\nfrom kaggle_environments import evaluate, make, utils\nfrom kaggle_environments.envs.rps.utils import get_score\nfrom kaggle_environments.envs.rps.agents import *","7aac2c96":"class RPS(nn.Module):\n    \"\"\"\n    Class that predict logits of action probabilities given game history.\n        Inputs: game history [bs, 2, 10].\n        Outputs: logits of action probabilities [bs, 3].\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.conv_layers = nn.Sequential(\n            nn.Conv1d(2, 4, 3, 1, 1, bias=False),\n            nn.ReLU(),\n            nn.AvgPool1d(2),\n            nn.Conv1d(4, 8, 3, 1, 1, bias=False),\n            nn.ReLU(),\n            nn.AvgPool1d(2),\n            nn.Conv1d(8, 16, 2, 1, 1, bias=False),\n            nn.ReLU(),\n            nn.AvgPool1d(2)\n        )\n        self.head = nn.Sequential(\n            nn.Linear(16, 6),\n            nn.ReLU(),\n            nn.Linear(6, 3)\n        )\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = torch.flatten(x, 1)\n        x = self.head(x)\n        return x","dcc6961f":"def soft_cross_entropy(target, prediciton):\n    log_probs = nn.functional.log_softmax(prediciton, dim=1)\n    sce = -(target * log_probs).sum() \/ target.shape[0]\n    return sce","4ea2a503":"def train_step(model, data, optimizer):\n    model.train()\n    torch.set_grad_enabled(True)\n\n    X = data['X'].view(-1, 2, 10)\n    y = data['y'].view(-1, 3)\n    prd = model(X)\n    loss = soft_cross_entropy(y, prd)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()","ad81cad9":"bs = 6 # batch size  \n\nopponent_actions = []\nagent_actions = []\nactions = []\nbatch_x = []\nbatch_y = []\n\n\ndef agent(observation, configuration):\n    \n    global actions, agent_actions, opponent_actions\n    global model, optimizer\n    global batch_x, batch_y\n    global bs\n    \n    # first step\n    if observation.step == 0:\n        hand = np.random.randint(2)\n        actions.append(hand)\n        return hand\n    \n    # first warm up rounds\n    if 0 < observation.step < 12:\n        opponent_actions.append(observation.lastOpponentAction)\n        agent_actions.append(actions[-1])\n        hand = np.random.randint(2)\n        actions.append(hand)\n        return hand\n    \n    # start to train CNN\n    elif observation.step >= 12:\n        opponent_actions.append(observation.lastOpponentAction)\n        agent_actions.append(actions[-1])\n        \n        wining_action = (opponent_actions[-1] + 1) % 3 \n        fair_action = opponent_actions[-1]\n        lose_action = (opponent_actions[-1] - 1) % 3 \n\n        # soft labels for target    \n        y = [0, 0, 0]\n        y[wining_action] = 0.7\n        y[fair_action] = 0.2\n        y[lose_action] = 0.1 \n        \n        # add data for history\n        batch_x.append([opponent_actions[-2:-12:-1],\n                        agent_actions[-2:-12:-1]])\n        batch_y.append(y)\n        \n        # data for single CNN update \n        data = {'X': torch.Tensor([opponent_actions[-2:-12:-1],\n                                   agent_actions[-2:-12:-1]]),\n                'y': torch.Tensor(y)} \n        \n        # evaluate single training step\n        train_step(model, data, optimizer)\n        \n        # evaluate mini-batch training steps\n        if observation.step % 10 == 0:\n            k = 1 if observation.step < 100 else 3\n            for _ in range(k):\n                idxs = np.random.choice(list(range(len(batch_y))), bs)\n                data = {'X': torch.Tensor(np.array(batch_x)[idxs]),\n                        'y': torch.Tensor(np.array(batch_y)[idxs])}\n                train_step(model, data, optimizer)\n        \n        # data for current action prediction\n        X_prd = torch.Tensor([opponent_actions[-1:-11:-1],\n                              agent_actions[-1:-11:-1]]).view(1, 2, -1)\n        \n        # predict logits\n        probs = model(X_prd).view(3)\n        # calculate probabilities\n        probs = nn.functional.softmax(probs, dim=0).detach().cpu().numpy()\n        \n        # choose action\n        hand = np.random.choice([0, 1, 2], p=probs)\n        actions.append(hand)\n        \n        return int(hand)","46e20cb1":"env = make('rps', configuration={\"episodeSteps\": 1000}, debug=True)\n\nmodel = RPS()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\nenv.reset()\nenv.run([agent, rock])\nenv.render(mode='ipython', width=500, height=500)","3b3bcf49":"def random_agent(observation, configuration):\n    hand = np.random.randint(3)\n    return hand","dd865137":"my_last_action = 0\n\ndef hit_the_last_own_action(observation, configuration):\n    global my_last_action\n    my_last_action = (my_last_action + 1) % 3\n    return my_last_action","9745ebff":"def fight(agent, opponents, n_fights=3):\n    for opponent_name in opponents.keys():\n        scores = []\n        wins = 0\n        for _ in range(n_fights):\n        # reinitialize agent\n            model = RPS()\n            optimizer = optim.Adam(model.parameters(), lr=1e-3) \n\n            opponent_agent = opponents[opponent_name]\n            score = evaluate('rps', [agent, opponent_agent], configuration={'episodeSteps': 1000})\n            agent_score = score[0][0]\n            scores.append(agent_score)\n            if agent_score > 0:\n                wins += 1\n        \n        print('='*20)\n        print(f'Opponent: {opponent_name}')\n        print(f'Wins rate: {wins\/n_fights*100:.0f}% mean score: {np.mean(scores):.0f}')","0baa0e9e":"opponents = {\n    \"rock\": rock,\n    \"paper\": paper,\n    \"scissors\": scissors,\n    \"copy_opponent\": copy_opponent,\n    \"reactionary\": reactionary,\n    \"counter_reactionary\": counter_reactionary,\n    \"statistical\": statistical,\n    \"random\": random_agent,\n    \"hit_the_last_own_action\": hit_the_last_own_action\n}","dc43256d":"fight(agent, opponents, n_fights=3)","8d99b8c2":"## Fight club:\nTest CNN bot on different opponents.","f1fef632":"![image.png](attachment:image.png)","a1f6721d":"## Bot:","ff65b3a3":"## CNN model:","033b9767":"# CNN bot for Rock, Paper, Scissors\n\nIn this notebook I'm describing a CNN based bot for RPS. <br>\nAs of right now, the best version of the bot steadily scores >850 on a lb and seems it still has some room for improvement. <br>\n\n## Main ideas:\n1. History of bot and opponent actions can be represented as [2, N] tensor.\n2. CNN can be trained to predict probabilities of action given game history.\n\n## Algorithm steps:\n\n1. First 11 rounds bot plays randomly as a warm up.\n2. Bot keeps track of its own and opponent actions.\n3. 10 latest actions starting from one but last are stacked in [2, 10] tensor and are used as an input for CNN (shaded on a figure below). \n4. Winning action in the last round is used as the target.\n5. Cross-entropy with soft labels is used to calculate loss. \n6. CNN is updated with single training sample every step.\n7. CNN is updated with mini-batch every 10 steps, mini-batch is sampled randomly from the historical data.","7126df74":"## Test fight:","6cbadcad":"### Few additional bots for the test fight:","2f026a88":"It seems to me that soft labels perform better than hard labels. <br>\nI suppose the reason is that model is less penalized for not winning action, as a result it performs not optimal actions more often, which adds stochastisity to the model and it becomes less predictable for the opponent. <br>\nThus the model balances between winning strategy and stochasticity.","1ecdf91a":"CNN bot is steadily beats some of the basic agents, slightly tuned version performs well on a lb. <br>\nSuch a model is sensetive to a hyperparameters. <br>\nThere are a lot of options to experiment with, like CNN arcitecture, optimizer, training cheme, etc."}}