{"cell_type":{"b15455d3":"code","ba950872":"code","ae4af28e":"code","6a24a676":"code","22e2c2fa":"code","62a3d3f2":"code","32cc5796":"code","19342af9":"code","b372f0c2":"code","0a984fbb":"code","954a09d4":"code","fc78858b":"code","6b4e3742":"code","144eb36f":"code","1636c595":"code","3482ff8d":"code","c5e2f3d1":"code","674630f8":"code","d0ad37f3":"code","97cbeda2":"code","e95fe849":"code","e9b1ca49":"code","dd5d80a4":"code","44a19103":"code","f98793b8":"code","9441fa3b":"code","0f35c085":"code","7b7fefa3":"code","7e8d2526":"code","07f18711":"code","b4cf6a28":"code","480ee41b":"code","5dca0617":"code","1eaa4430":"code","517935a7":"code","bc0d1a35":"code","36e2b62c":"code","1941dfcf":"code","5bb03796":"code","d6951138":"code","586846f8":"code","2526d185":"code","889de2a2":"code","2ddd2aa0":"markdown"},"source":{"b15455d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\nfrom tqdm import tqdm, trange\n","ba950872":"input_data = pd.read_csv(\"..\/input\/ner_dataset.csv\", encoding=\"latin1\")\n#input_data","ae4af28e":"input_data = input_data.fillna(method=\"ffill\")\ninput_data.tail(10)","6a24a676":"words_list = list(set(input_data[\"Word\"].values))\nwords_list[:10]","22e2c2fa":"number_words = len(words_list); number_words # number of unique words in the corpus","62a3d3f2":"class RetrieveSentance(object):\n    \n    def __init__(self, data):\n        self.n_sent = 1\n        self.data = data\n        self.empty = False\n        function = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n                                                           s[\"POS\"].values.tolist(),\n                                                           s[\"Tag\"].values.tolist())]\n        self.grouped = self.data.groupby(\"Sentence #\").apply(function)\n        self.sentences = [s for s in self.grouped]\n    \n    def retrieve(self):\n        try:\n            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n            self.n_sent += 1\n            return s\n        except:\n            return None","32cc5796":"Sentences = RetrieveSentance(input_data)\nSentences","19342af9":"Sentences_list = [\" \".join([s[0] for s in sent]) for sent in Sentences.sentences]\nSentences_list[0]","b372f0c2":"len(Sentences_list) #number of sentences ","0a984fbb":"labels = [[s[2] for s in sent] for sent in Sentences.sentences]\nprint(labels[0])","954a09d4":"#labels [0] # list of lists of dimension (sentences,labels)","fc78858b":"tags2vals = list(set(input_data[\"Tag\"].values))\ntag2idx = {t: i for i, t in enumerate(tags2vals)}","6b4e3742":"tags2vals # 17 kinds of tags ","144eb36f":"tag2idx # indexing the tag ","1636c595":"import torch\nfrom torch.optim import Adam\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom pytorch_pretrained_bert import BertTokenizer, BertConfig\nfrom pytorch_pretrained_bert import BertForTokenClassification, BertAdam","3482ff8d":"max_seq_len = 75 # tokens\nbatch_s = 32 # batch size","c5e2f3d1":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()","674630f8":"torch.cuda.get_device_name(0) ","d0ad37f3":"tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=True)\n","97cbeda2":"tokenized_texts = [tokenizer.tokenize(sent) for sent in Sentences_list]\nprint(tokenized_texts[0])","e95fe849":"len(tokenized_texts)","e9b1ca49":"print(tokenized_texts[1])","dd5d80a4":"X = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n                          maxlen=max_seq_len, dtype=\"long\", truncating=\"post\", padding=\"post\")","44a19103":"Y = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n                     maxlen=max_seq_len, value=tag2idx[\"O\"], padding=\"post\",\n                     dtype=\"long\", truncating=\"post\")","f98793b8":"X.shape # (sentences, maximum sequence length)","9441fa3b":"Y.shape","0f35c085":"X","7b7fefa3":"Y","7e8d2526":"attention_masks = [[float(i>0) for i in ii] for ii in X]","07f18711":"len(attention_masks) # list of lists of shape (sentences, labels )","b4cf6a28":"#attention_masks[0]","480ee41b":"X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, \n                                                            random_state=20, test_size=0.1)\nMask_train, Mask_valid, _, _ = train_test_split(attention_masks, X,\n                                             random_state=20, test_size=0.1)","5dca0617":"X_train = torch.tensor(X_train)\nX_valid = torch.tensor(X_valid)\nY_train = torch.tensor(Y_train)\nY_valid = torch.tensor(Y_valid)\nMask_train = torch.tensor(Mask_train)\nMask_valid = torch.tensor(Mask_valid)","1eaa4430":"data_train = TensorDataset(X_train, Mask_train, Y_train)\ndata_train_sampler = RandomSampler(data_train)\nDL_train = DataLoader(data_train, sampler=data_train_sampler, batch_size=batch_s)\n\ndata_valid = TensorDataset(X_valid, Mask_valid, Y_valid)\ndata_valid_sampler = SequentialSampler(data_valid)\nDL_valid = DataLoader(data_valid, sampler=data_valid_sampler, batch_size=batch_s)","517935a7":"model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(tag2idx))","bc0d1a35":"model.cuda();","36e2b62c":"FULL_FINETUNING = True\nif FULL_FINETUNING:\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'gamma', 'beta']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n         'weight_decay_rate': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n         'weight_decay_rate': 0.0}\n    ]\nelse:\n    param_optimizer = list(model.classifier.named_parameters()) \n    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\noptimizer = Adam(optimizer_grouped_parameters, lr=3e-5)","1941dfcf":"#from seqeval.metrics import f1_score\nfrom sklearn.metrics import f1_score\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=2).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) \/ len(labels_flat)","5bb03796":"epochs = 5\nmax_grad_norm = 1.0\n\nfor _ in trange(epochs, desc=\"Epoch\"):\n    # TRAIN loop\n    model.train()\n    tr_loss = 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    for step, batch in enumerate(DL_train):\n        # add batch to gpu\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n        # forward pass\n        loss = model(b_input_ids, token_type_ids=None,\n                     attention_mask=b_input_mask, labels=b_labels)\n        # backward pass\n        loss.backward()\n        # track train loss\n        tr_loss += loss.item()\n        nb_tr_examples += b_input_ids.size(0)\n        nb_tr_steps += 1\n        # gradient clipping\n        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n        # update parameters\n        optimizer.step()\n        model.zero_grad()\n    # print train loss per epoch\n    print(\"Train loss: {}\".format(tr_loss\/nb_tr_steps))\n    # VALIDATION on validation set\n    model.eval()\n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n    predictions , true_labels = [], []\n    for batch in DL_valid:\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n        \n        with torch.no_grad():\n            tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n                                  attention_mask=b_input_mask, labels=b_labels)\n            logits = model(b_input_ids, token_type_ids=None,\n                           attention_mask=b_input_mask)\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n        true_labels.append(label_ids)\n        \n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n        \n        eval_loss += tmp_eval_loss.mean().item()\n        eval_accuracy += tmp_eval_accuracy\n        \n        nb_eval_examples += b_input_ids.size(0)\n        nb_eval_steps += 1\n    eval_loss = eval_loss\/nb_eval_steps\n    print(\"Validation loss: {}\".format(eval_loss))\n    print(\"Validation Accuracy: {}\".format(eval_accuracy\/nb_eval_steps))\n    pred_tags = [tags2vals[p_i] for p in predictions for p_i in p]\n    valid_tags = [tags2vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n    #print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))","d6951138":"model.eval()\npredictions = []\ntrue_labels = []\neval_loss, eval_accuracy = 0, 0\nnb_eval_steps, nb_eval_examples = 0, 0\nfor batch in DL_valid:\n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask, b_labels = batch\n\n    with torch.no_grad():\n        tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n                              attention_mask=b_input_mask, labels=b_labels)\n        logits = model(b_input_ids, token_type_ids=None,\n                       attention_mask=b_input_mask)\n        \n    logits = logits.detach().cpu().numpy()\n    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n    label_ids = b_labels.to('cpu').numpy()\n    true_labels.append(label_ids)\n    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n\n    eval_loss += tmp_eval_loss.mean().item()\n    eval_accuracy += tmp_eval_accuracy\n\n    nb_eval_examples += b_input_ids.size(0)\n    nb_eval_steps += 1\n\npred_tags = [[tags2vals[p_i] for p_i in p] for p in predictions]\nvalid_tags = [[tags2vals[l_ii] for l_ii in l_i] for l in true_labels for l_i in l ]\nprint(\"Validation loss: {}\".format(eval_loss\/nb_eval_steps))\nprint(\"Validation Accuracy: {}\".format(eval_accuracy\/nb_eval_steps))\n#print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))","586846f8":"valid_data = [tokenizer.convert_ids_to_tokens(id.numpy()) for id in X_valid]","2526d185":"submission = pd.DataFrame.from_dict({\n    'text': valid_data,\n    'valid_tags': valid_tags,\n    'prediction': pred_tags\n})\nsubmission.to_csv('predicated_data.csv', index=False)","889de2a2":"model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n\n# If we save using the predefined names, we can load using `from_pretrained`\n#output_model_file = os.path.join(OUTPUT_DIR, WEIGHTS_NAME)\n#output_config_file = os.path.join(OUTPUT_DIR, CONFIG_NAME)\n\ntorch.save(model_to_save.state_dict(), \"pytorch_model.bin\")\nmodel_to_save.config.to_json_file(\"config.json\")\n#tokenizer.save_vocabulary(\"\/vocab1.txt\")","2ddd2aa0":"Using pytorch_pretrained_bert for token level classification - Followed the tutorial at  https:\/\/www.depends-on-the-definition.com\/named-entity-recognition-with-bert\/"}}