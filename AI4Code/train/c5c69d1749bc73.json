{"cell_type":{"ce4fa628":"code","8e676b81":"code","e515775e":"code","da142a91":"code","322d8ff7":"code","b514cae6":"code","7aee1f4b":"code","b11dc0e0":"code","8121d223":"code","a6476be5":"code","f736831e":"code","c92c42e2":"code","6217bfe6":"markdown","490158eb":"markdown","654ff7f7":"markdown","8771dea9":"markdown","24bb3967":"markdown"},"source":{"ce4fa628":"#Importing stuff\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier","8e676b81":"#Reading the data\ntrain=pd.read_csv('..\/input\/titanic\/train.csv',index_col='PassengerId')\ntest=pd.read_csv('..\/input\/titanic\/test.csv',index_col='PassengerId')\ny=train['Survived']\nX=train.append(test)","e515775e":"#Extracting Titles from Nameand mapping them to new values\nX['Title']=X['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\nX['Title'].replace(['Miss', 'Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss\/Ms', inplace=True)\nX['Title'].replace(['Major', 'Col', 'Capt', 'Don', 'Sir', 'Jonkheer'], 'Mr', inplace=True)\nsns.catplot(x='Title',y='Survived',data=X,kind='bar')","da142a91":"#Embarked\nX['Embarked']=X['Embarked'].fillna('S')\n\n#Fare\ndef rem_fare(r):\n    if r.Fare==0:\n        r.Fare=np.NaN\n    return r    \nX=X.apply(rem_fare,axis=1)        \nX['Ticket_Freq'] = X.groupby('Ticket')['Ticket'].transform('count')\ndef fare(r):\n    return r.Fare\/\/r.Ticket_Freq\nX['Fare_bin']=X.apply(fare,axis=1)\nX['Fare_bin']=X['Fare_bin'].fillna(X['Fare_bin'].median())\nX['Fare_bin']=pd.cut(X['Fare_bin'],13)\n\nsns.catplot(x='Fare_bin',y='Survived',data=X,kind='bar',aspect=4)","322d8ff7":"#Family Size\nX['Fam_Size'] = X['SibSp'] + X['Parch'] + 1\nX['Fam_Size']=pd.cut(X['Fam_Size'],[0,1,4,7,11])\nsns.catplot(x='Fam_Size',y='Survived',data=X,kind='bar')","b514cae6":"#Encoding \nX['Fam_Size']=LabelEncoder().fit_transform(X['Fam_Size'])\nX['Fare_bin']=LabelEncoder().fit_transform(X['Fare_bin'])\n\nencoded_features = []\ncat_features = ['Pclass','Embarked', 'Title', 'Fam_Size','Fare_bin']\nfor feature in cat_features:\n        encoded_feat = OneHotEncoder().fit_transform(X[feature].values.reshape(-1, 1)).toarray()\n        n = X[feature].nunique()\n        cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n        encoded_df.index = X.index\n        encoded_features.append(encoded_df)\n    \n\nX=pd.concat([X,*encoded_features[:]], axis=1)","7aee1f4b":"#Dropping useless values\nX=X.drop(['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket',\n       'Fare', 'Cabin', 'Embarked', 'Title', 'Ticket_Freq', 'Fare_bin',\n       'Fam_Size'],axis=1)\nX.columns","b11dc0e0":"train=X[:891]\ntest=X[891:]\nX_train, X_test, y_train, y_test = train_test_split(train, y, test_size=.2, random_state=42)","8121d223":"#Using multiple models to create a Voting Classifier\nmodel1 = LogisticRegression(random_state=1)\nmodel2 = KNeighborsClassifier(3)\nmodel3 = GaussianProcessClassifier(1.0 * RBF(1.0))\nmodel4 = RandomForestClassifier(n_estimators = 100, max_depth=5, random_state=1)\nmodel5 = AdaBoostClassifier(n_estimators = 100, random_state=1)\n\nclf=VotingClassifier(estimators=[('lr', model1), ('knc', model2), ('gpc', model3),('rfc',model4),('abc',model5)], \n                     voting='hard')\n\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_test)\naccuracy_score(y_pred,y_test) #Testing the model on unseen data","a6476be5":"cross_val_score(clf,train,y,cv=6) #Cross Validation Scores","f736831e":"#Training the model on the entire data to make final predictions\nclf = VotingClassifier(estimators=[('lr', model1), ('knc', model2), ('gpc', model3),('rfc',model4),('abc',model5)], voting='hard')\nclf.fit(train,y)\nresult=clf.predict(test)","c92c42e2":"res=pd.read_csv('..\/input\/titanic\/gender_submission.csv',index_col=None)\nres['Survived']=result\nres.to_csv('submission.csv',index=False)","6217bfe6":"# Building a Classifier\n\nWe will use 4 seperate models to build the final classifer. You can play around the individual classifiers to see how the ensemble is more effective than individual ones. Since the Titanic dataset is relatively small and has many outliers it will tough to see major differences but a well made ensemble should outperform single models.","490158eb":"# Titanic Simple Model\nFirst we will import the basic stuff that we will need throughout this notebook.","654ff7f7":"# Reading the Data","8771dea9":"# Encoding\n\nBelow we will first label encode categories like family size and fare bin so that they can then be one hot encoded.\nBy one hot encoding all the categorical columns we will improve our models peformance.","24bb3967":"# Feature Extraction\nBelow we will do three major things\n1. Create a title column after extracting titles from names\n2. Bin the continuos fare ranges\n3. Join SibSp and Parch to create family size\n"}}