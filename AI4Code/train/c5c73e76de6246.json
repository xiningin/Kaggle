{"cell_type":{"d0384896":"code","efdb057e":"code","c9b30835":"code","1ec976a4":"code","7f63767b":"code","677aa316":"code","7d5e9db1":"code","cbf810a8":"code","feea8517":"code","1c1895ac":"code","faad47a8":"code","943f3b72":"code","58dd69fc":"markdown","c8cb17aa":"markdown","ffd42dcf":"markdown","9facd7ad":"markdown","93551c88":"markdown","2cca5f6a":"markdown","b4c90bd4":"markdown","0423ece1":"markdown","406f93cb":"markdown","9567f57d":"markdown","e4d7b7ed":"markdown","c6354be4":"markdown","59940bd6":"markdown","0cb0c8fb":"markdown","d53a89e7":"markdown","c4a20448":"markdown","75b1c308":"markdown","4a59b93d":"markdown","e7d3eda8":"markdown","1de55204":"markdown"},"source":{"d0384896":"import subprocess\nimport sys\ndef install(package):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\",\"install\",package])\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nfrom sklearn.svm import SVC\nfrom IPython.display import Image\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nimport scipy.stats as ss\nimport math\nfrom sklearn.feature_selection import SelectKBest, chi2\nimport plotly.figure_factory as ff\nfrom sklearn.feature_selection import SequentialFeatureSelector\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly import tools\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nclass data_transf:\n    def __init__(self,data,selection=0,split=True):\n        self.data = data\n        self.split = split\n    def bina(self,x):\n        if x=='e':\n            return 0\n        else:\n            return 1\n    \n    def modify(self):\n\n        if self.split:\n            split = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=10)\n            for train_index, test_index in split.split(self.data, self.data[[\"class\",\"odor\",'gill-attachment','stalk-surface-below-ring','stalk-surface-below-ring','gill-color']]):\n                train_set = self.data.loc[train_index]\n                test_set = self.data.loc[test_index]\n            \n\n            binary_test=test_set['class'].apply(self.bina)\n            test_set=pd.concat([test_set.drop('class',axis=1),binary_test],axis=1)\n            test_set=pd.get_dummies(test_set)\n             \n            binary_train=train_set['class'].apply(self.bina)\n            train_set=pd.concat([train_set.drop('class',axis=1),binary_train],axis=1)\n            train_set=pd.get_dummies(train_set)\n            \n            a=[]\n            for col in train_set.columns:\n                for x in train_set[col].value_counts():\n                    if col not in test_set.columns:\n                        train_set.drop(col,axis=1,inplace=True)\n                        break\n                    elif x<5:\n                        train_set.drop(col,axis=1,inplace=True)\n                        test_set.drop(col,axis=1,inplace=True)\n                        \n\n            return train_set.drop('class',axis=1),test_set.drop('class',axis=1),train_set['class'],test_set['class']\n\n        return self.data\ndef bina(x):\n    if x=='e':\n        return 0\n    else:\n        return 1\n\nmush_data=pd.read_csv('\/kaggle\/input\/mushroom-classification\/mushrooms.csv')\nmush_data","efdb057e":"obj=data_transf(mush_data)\nX_train,X_test,y_train,y_test=obj.modify()\nclf = DecisionTreeClassifier(random_state=0,max_depth=5)\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_test)\nprint('Accuracy_score={A}'.format(A=accuracy_score(y_test, y_pred)))\nprint('f1_score={f1}'.format(f1=f1_score(y_test, y_pred)))","c9b30835":"colorscale = [[0, '#4d004c'],[.5, '#f2e5ff'],[1, '#ffffff']]\nfig =  ff.create_table(pd.DataFrame({'col':mush_data.columns,'types':mush_data.dtypes}).reset_index(drop=True),height_constant=16, colorscale=colorscale,index=True)\nfor i in range(len(fig.layout.annotations)):\n    fig.layout.annotations[i].font.size = 13\nfig.layout.width=500\nfig","1ec976a4":"specs = [[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}],[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}],[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}],[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}],[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}]]\nfig = make_subplots(rows=5, cols=5, specs=specs)\na=1\nb=1\nxx=-0.172\nyy=1.267\nl=[]\nfor col in mush_data.columns:\n    fig.add_trace(go.Pie(labels=[n for n in mush_data[col].value_counts().index], values=[v for v in mush_data[col].value_counts()*100\/sum(mush_data[col].value_counts())], name=col),\n              a, b)\n    l.append(dict(text=col, x=xx+(0.225*b), y=yy-(0.222*a), font_size=10, showarrow=False))\n    a+=1\n    if a>5:\n        a=1\n        b+=1\n\n\n\nfig.update(layout_title_text='Features',\n           layout_showlegend=False)\n  \nfig.update_layout(\n    title_font_family=\"Arial\",\n    title_font_size=25,\n    annotations=l)\n\nfig.update_traces(hole=.4, hoverinfo=\"label+percent+name\",textinfo='none')\n\nfig.show()","7f63767b":"mush_data=mush_data.drop('veil-type',axis=1)\ndef cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n    rcorr = r-((r-1)**2)\/(n-1)\n    kcorr = k-((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr\/min((kcorr-1),(rcorr-1)))\n\nl1=[]\nl2=[]\nfor col in mush_data:\n    for col2 in mush_data:\n        l1.append(round(cramers_v(mush_data[col],mush_data[col2])*100)\/100)\n    l2.append(l1)\n    l1=[]\ndf = pd.DataFrame(np.array(l2),columns=[mush_data.columns])\ndf.index=df.columns\nplt.figure(figsize=(13,13)) \nmask = np.triu(np.ones_like(df, dtype=bool))\nsns.heatmap(df,mask=mask, annot=True)","677aa316":"np.random.seed(100)\n\n\nX_new = SelectKBest(chi2, k='all')\nX_new.fit(X_train, y_train)\n\nfs = SelectKBest(score_func=mutual_info_classif, k='all')\nfs.fit(X_train, y_train)\n\n\ncoef = pd.DataFrame({'col':list(X_train.columns),'coef':[v for v in X_new.scores_]}).sort_values(by='coef')\ncoef['cumul']=[i for i in range(0,115)]\n\n\n\n\nfig = make_subplots(rows=3, cols=1, subplot_titles=(\"Chi2\",\"\", \"Mutual_info\"))\n\nfig.add_trace(\n    go.Bar(x=coef['col'], y=coef['coef'], hovertext=[('n:'+ str(n)) for n in coef['cumul']]),\n    row=1, col=1\n)\n\ncoef = pd.DataFrame({'col':list(X_train.columns),'coef':[v for v in fs.scores_]}).sort_values(by='coef')\ncoef['cumul']=[i for i in range(0,115)]\n\nfig.add_trace(\n    go.Bar(x=coef['col'], y=coef['coef'], hovertext=[('n:'+ str(n)) for n in coef['cumul']]),\n    row=3, col=1\n)\n\n\n\nfig.update_layout(height=800,width=600, title_text=\"Selection\",showlegend=False)\n\nfig.show()","7d5e9db1":"np.random.seed(1111)\nX_new = SelectKBest(chi2, k=94,)\nX_new.fit(X_train, y_train)\n\nfs = SelectKBest(score_func=mutual_info_classif, k=94)\nfs.fit(X_train, y_train)\n\nheaderColor = 'grey'\ncolors = []\ni=0\nfor x in range(0,114):\n    if list(X_new.get_support())[x] or list(fs.get_support())[x]:\n        colors.append('lightgrey')\n    else:\n        colors.append('darksalmon')\n        i-=1\n    i+=1\nResults=[['discarded','kept'][list(X_new.get_support())[x] or list(fs.get_support())[x]] for x in range(0,114)]\n\n\nfig = go.Figure(data=[go.Table(\n  header=dict(\n    values=['<b>Columns<\/b>','<b>Chi2<\/b>','<b>Mutual-info<\/b>','<b>Result<\/b>'],\n    line_color='darkslategray',\n    fill_color=headerColor,\n    align=['left','center'],\n    font=dict(color='white', size=12)\n  ),\n  cells=dict(\n      values=[\n        list(X_train.columns),\n        list(X_new.get_support()),\n        list(fs.get_support()),\n        Results],\n    line_color='darkslategray',\n    fill_color = [colors],\n    align = ['left', 'center'],\n    font = dict(color = 'darkslategray', size = 11)\n        ))\n])\n\n\nfig.show()","cbf810a8":"\nnp.random.seed(3)\n\n\nl=[X_train,X_train.iloc[:,X_new.get_support(indices=True)],X_train.iloc[:,fs.get_support(indices=True)],X_train.iloc[:,list(set(X_new.get_support(indices=True))|set(fs.get_support(indices=True)))]]\nl2=['chi','mutual','mix']\n\nscorer={\n    'Normal':[],\n    'Chi':[],\n    'mutual':[],\n    'mix':[]\n}\n\nx=-1\nfor p in scorer:\n    x+=1\n    for y in range(2,15):\n        a=DecisionTreeClassifier(max_depth=y)\n        a.fit(X_train,y_train)\n        y_pred = a.predict(X_train)\n        scorer[p].append(accuracy_score(y_train, y_pred))\n\npt=pd.DataFrame(\n    scorer\n)\npt.reset_index(inplace=True)\n\n\n\nscorer2={\n    'Normal':[],\n    'Chi':[],\n    'mutual':[],\n    'mix':[]\n}\n\nx=-1\nfor p in scorer2:\n    x+=1\n    for y in range(2,15):\n        a=DecisionTreeClassifier(max_leaf_nodes=y)\n        a.fit(l[x],y_train)\n        y_pred = a.predict(l[x])\n        scorer2[p].append(accuracy_score(y_train, y_pred))\n        \n        \n\npt2=pd.DataFrame(\n    scorer2\n)\npt2.reset_index(inplace=True)\n\n\n\n\n\nfig = make_subplots(rows=1, cols=2,subplot_titles=(\"Increasing Max_depth\", \"Increasing Max_leaf_nodes\"))\n\nfig.add_trace(go.Scatter(x=pt['index'], y=pt['Normal'],\n                    mode='lines+markers',\n                    name='Max_Depth_Normal'),row=1,col=1)\nfig.add_trace(go.Scatter(x=pt['index'], y=pt['Chi'],\n                    mode='lines+markers',\n                    name='Max_Depth_Chi'),row=1,col=1)\nfig.add_trace(go.Scatter(x=pt['index'], y=pt['mutual'],\n                    mode='lines+markers',\n                    name='Max_Depth_mutual'),row=1,col=1)\nfig.add_trace(go.Scatter(x=pt['index'], y=pt['mix'],\n                    mode='lines+markers', name='Max_Depth_mix'),row=1,col=1)\n\nfig.add_trace(go.Scatter(x=pt2['index'], y=pt2['Normal'],\n                    mode='lines+markers',\n                    name='Max_leaf_nodes_Normal'),row=1,col=2)\nfig.add_trace(go.Scatter(x=pt2['index'], y=pt2['Chi'],\n                    mode='lines+markers',\n                    name='Max_leaf_nodes_Chi'),row=1,col=2)\nfig.add_trace(go.Scatter(x=pt2['index'], y=pt2['mutual'],\n                    mode='lines+markers',\n                    name='Max_leaf_nodes_mutual'),row=1,col=2)\nfig.add_trace(go.Scatter(x=pt2['index'], y=pt2['mix'],\n                    mode='lines+markers', name='Max_leaf_nodes_mix'),row=1,col=2)\n\n\n\n\n\n\nfig.update_xaxes(title_text=\"Max_depth\", row=1, col=1)\nfig.update_xaxes(title_text=\"Max_leaf_nodes\", row=1, col=2)\n\n\n\n\n\nfig.show()","feea8517":"\nfig = make_subplots(rows=1, cols=2,subplot_titles=(\"Decision Tree\",'Support-vector machine'))\n\nscorer3={\n    'Max_depth':[],\n    'Accuracy':[]\n}\n\nx=-1\n\nfor y in range(2,20):\n    a=DecisionTreeClassifier(max_depth=y)\n    a.fit(X_train,y_train)\n    y_pred = a.predict(X_test)\n    scorer3['Accuracy'].append(f1_score(y_test, y_pred))\n    scorer3['Max_depth'].append(y) \n        \n\npt3=pd.DataFrame(\n    scorer3\n)\n\n\n\nscorer4={\n    'gamma':[],\n    'f1_score':[]\n}\n\n\nx=-1\nd=np.arange(0.01,0.1,0.01)\nfor y in d:\n    clf = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"svm_clf\", SVC(kernel=\"rbf\", gamma=y, C=0.001))\n    ])\n    clf.fit(X_train, y_train)\n    clf_pred = clf.predict(X_train)\n    scorer4['f1_score'].append(f1_score(y_test, y_pred))\n    scorer4['gamma'].append(y) \n        \n\npt4=pd.DataFrame(\n    scorer4\n)\n\n\n\n\nfig.add_trace(go.Scatter(x=pt3['Max_depth'], y=pt3['Accuracy'],name='Increasing Max_depth',\n                    mode='lines+markers'),row=1,col=1)\nfig.update_yaxes(title_text=\"f1_score\", row=1, col=1)\nfig.update_xaxes(title_text=\"Max_depth\",row=1, col=1)\n\n\nfig.add_trace(go.Scatter(x=pt4['gamma'], y=pt4['f1_score'],name='Increasing gamma',\n                    mode='lines+markers'),row=1,col=2)\nfig.update_yaxes(title_text=\"f1_score\", row=1, col=2)\nfig.update_xaxes(title_text=\"Increasing gamma\",row=1, col=2)","1c1895ac":"install(\"ivis[cpu]\")\nfrom ivis import Ivis\nnp.random.seed(3)\nmush_y=mush_data['class'].apply(bina)\nmush_X=pd.get_dummies(mush_data.drop('class',axis=1))\nmodel = Ivis(embedding_dims=2,k=18,model='maaten',n_epochs_without_progress=18)\n\nembeddings = model.fit_transform(mush_X.to_numpy())","faad47a8":"fig, ax = plt.subplots(figsize=(6, 4), dpi=150)\nsc = plt.scatter(x=embeddings[:, 0], y=embeddings[:, 1],c=mush_y, s=3)\nplt.xlabel('ivis 1')\nplt.ylabel('ivis 2')\nplt.title('data')\nax.grid(True)\nlegend1 = ax.legend(*sc.legend_elements(),loc=\"lower left\", title=\"Classes\")","943f3b72":"clf = DecisionTreeClassifier(random_state=0,max_depth=15)\nclf.fit(X_train,y_train)\ntarget_names=list(mush_data['class'].value_counts().index)[::-1]\nexport_graphviz(clf, out_file='tree_limited.dot', feature_names = X_train.columns,\n                class_names = target_names,\n                rounded = True, proportion = False, precision = 2, filled = True)\n!dot -Tpng tree_limited.dot -o tree_limited.png -Gdpi=600\n\nImage(filename = 'tree_limited.png')","58dd69fc":"**To explain what I meant when I said both I made this plot. When both methods discard a column we delete it, otherwise we keep it.**","c8cb17aa":"**Here we have the accuracy of a decision tree and of a svm trained on the train set when used on the test set. This isn't something that would be done during a machine learning project normally but it allowed me to see what i explained in the introduction. Both these simple models are able to do this classification perfectly and easily.**","ffd42dcf":"**There can only be two reasons for such a thing:** <br>\n**1. The dataset isn't good(I doubt this is the case)** <br>\n**2. There are well defined multi-dimensional clusters that allows our algorithm to divide poisonous and edible mushrooms easily**","9facd7ad":"**The First thing we want to do is understand our data. Let's start by looking at our features' types** ","93551c88":"**Here I used two of the methods sklearn offers us to select variables. Chi2 and Mutual_info selections. The problem now is 'which one should we choose?'. I decided to test how the algorithm would work with one of them,none or both.**","2cca5f6a":"**This is the last part of the notebook. Here we have a representation of a decision tree trained on this dataset.** <br>\n**As we can see, by the time we reach depth=4 the gini impurity becomes 0 for all the leaves, this further confirms what we've seen earlier, that the decision tree is able to divide both classes perfectly, easily and quickly.**","b4c90bd4":"**Here I used Ivis a dimensionality reduction specialized in finding clusters.**<br> \n**As we can see we have really well defined groups for the two classes of mushrooms (0=edible,1=poisonous)** <br>\n**This supports the second hypothesis in my previous comment**","0423ece1":"**This is a classical dataset that everyone that decides to study machine learning knows, the mushroom dataset.** <br>\n**But for those who don't: The purpose of the study is to produce a classification algorithm capable of differentiating between poisonous and edible mushrooms.** <br>\n**This is the first time I try to tackle this project and I decided to proceed in an ordered manner following each classical step that needs to be made for a machine learning project.**<br>\n**Sadly this turned out to be useless in this specific case:**","406f93cb":"**Hope you enjoyed my project, thanks for the attention.**","9567f57d":"<blockquote><b><i><h1 style=\"font-family: Arial ; text-align:center;\">Studying and explaining the mushroom dataset<\/h1><\/i><\/b><\/blockquote>","e4d7b7ed":"# Feature Selection","c6354be4":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5642C5;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:white;\">\n\n**WARNING:** If you are interested in the steps that It would be normal to take during a classification problem go on, if you are only interested in some insights on why this is such un easy problem you can go directly to the explanation paragraph   \n<\/p>\n<\/div>","59940bd6":"**As we can see some features have a variable with few istances and there's even one that has only one type of entry veil-type. This one needs to be dropped immediatly. As for the others, I dealt with the problem in a class I put in class in the first chunk of code to produce one-hot-encoded train and test subsets.**","0cb0c8fb":"**As we can see if we randomly divide our dataset in a test and training subsets, do the needed transformations and run a DecisionTreeClassifier both the accuracy and the f1 score of the model on the test are 1.** <br> \n**This doesn't mean that the model is overfitting (since I'm testing it on the test set), but that this is an extremely easy classification problem for the algorithms to solve.**<br>\n**I tried to explore what is happening later on in the notebook.**","d53a89e7":"# Data Exploration ","c4a20448":"**I made this plot to see the accuracy of a decision tree trained and tested on the train set with increasing max_leaf_nodes and max_depth. I wanted to see which one of the methods would perform the best out of all of them. All of them, aside from mutual-info, were on the same level. But one thing about these plot intrigued me, both of them overfit too quickly. This is why I made the following plot.**","75b1c308":"# Explanation","4a59b93d":"<img src=\"http:\/\/cdn6.bigcommerce.com\/s-tu5jcd\/product_images\/uploaded_images\/mushroom-parts-labeled-small.jpg?t=1456508398\" style=\"width:350px;height:350px;\">","e7d3eda8":"**Ok, now that we know we are dealing only with categorical variables we can estabilish what to do next. There are 22 features in total and for each of them we need to see the proportions of the various entries inside. The reason for doing this is that there might be some features composed mostly or even totally by just one variable making them useless, or\/and we might have some features with one variable present in only a few instances and that might force me to stratify the splitting in train\/test to be sure to have the same variables in both samples.** <br>\n**To do this we can use a visualization.**","1de55204":"**Here I made a correlation Matrix. To do this I defined a personalized formula Cram\u00e9r's V, this formula measures association between two categorical variables, and I can use it for this dataset since all of my variables are like that. With thi piece of code I confronted all of the features 1 vs 1. It's interesting to see some really high correlations here like class and odor. The problem is that we cannot exclude some multicollinearity between our features seeing this plot. This added with the fact that some variables appear only few times in some columns we need to do some feature selection.**"}}