{"cell_type":{"baf80737":"code","99c29ce0":"code","1110c596":"code","8f38d44c":"code","01fa8d63":"code","a1964059":"code","cb01dbd0":"code","307503b2":"code","84631b51":"code","d4229688":"code","332af0b0":"code","ef10febb":"code","a4a66f55":"code","10234a2c":"code","e72325ae":"code","fe9593fc":"code","093da127":"code","b61111b8":"code","03a1c224":"code","d1a528b4":"code","d8198bfe":"code","45402117":"code","619967a5":"code","1e959da1":"code","3072f3fd":"code","3bad2fdb":"code","6ce93047":"code","b2cd62e6":"code","2e3d87cd":"code","4abcc0fe":"code","6bea497c":"code","83b31ab2":"code","d54a0c38":"code","2f808032":"code","6071c10e":"code","429f47cf":"code","171275f3":"code","aae9e5f0":"code","5aba79f0":"code","a87b696c":"code","fddf9990":"code","22f8b1ff":"code","d5221b00":"code","0f8f4ada":"code","9c272724":"code","ebf4329f":"code","fbdf00fa":"code","f85c5b1a":"code","f5b7af9b":"code","f67c74e9":"code","f319245e":"code","69843c27":"code","502ce3b6":"code","3616a043":"code","3b92ad53":"code","10b540ab":"code","f06161ed":"code","bda5196b":"code","6a450e38":"code","1141dc8e":"code","8507de19":"code","9d7197d1":"code","130569a8":"code","564a92d0":"code","3bbab4ae":"code","b933c058":"code","a866cb35":"code","847188e6":"code","719c0497":"code","cb5c0354":"code","f1dca419":"code","7aee002b":"code","82d4d992":"code","394447cc":"code","6a82a34e":"markdown","b04c1699":"markdown","93a98954":"markdown","ab4d8f71":"markdown","0ca05353":"markdown","ea4943b1":"markdown","49718242":"markdown","b84f0a3b":"markdown","a323549a":"markdown","91102e9e":"markdown","7833e21a":"markdown","338217f0":"markdown","b90b79ad":"markdown","736936c8":"markdown","f87bc812":"markdown","9024a471":"markdown","5fb86999":"markdown","2d125f7a":"markdown","fea6612d":"markdown","592b2e02":"markdown","acf68b76":"markdown","99a6e971":"markdown","68b2e62f":"markdown","3e5b473b":"markdown","df860275":"markdown","2ebecda5":"markdown","19044bf3":"markdown","94cb9ac3":"markdown","ccab98ed":"markdown","5f930a9a":"markdown","50323486":"markdown","785cb640":"markdown"},"source":{"baf80737":"pip install eli5","99c29ce0":"#Importing required libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib as mpl\nfrom datetime import datetime\nfrom sklearn import preprocessing, model_selection, metrics, ensemble\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom eli5.sklearn import PermutationImportance","1110c596":"#Reading train and test data\ntrain = pd.read_csv('..\/input\/Train.csv')\ntest = pd.read_csv('..\/input\/Test.csv')\n","8f38d44c":"#Let us have a look at the structure of data in each file\nprint(\"Train data structure -> \",\"Rows: \",train.shape[0],'\\t',\"Cols: \",train.shape[1])\nprint(\"Test data structure ->\", \"Rows: \",test.shape[0],'\\t',\"Cols: \",test.shape[1])\n","01fa8d63":"train.head(10).append(train.tail(10))","a1964059":"train.info()","cb01dbd0":"#Let us find out the unique number of patients in train set\ntrain.Patient_ID.nunique()","307503b2":"train.groupby('Patient_ID')['Health_Camp_ID'].count().unique()","84631b51":"train_grouped = train.groupby('Patient_ID').agg('size').reset_index()\ntrain_grouped.columns = [\"Patient_ID\", \"Number_of_registrations_per_person\"]\ntrain_grouped = train_grouped.groupby('Number_of_registrations_per_person').agg('size').reset_index()\ntrain_grouped.columns = [\"Number_of_registrations_per_person\", \"Count\"]\nmpl.pyplot.bar(train_grouped.Number_of_registrations_per_person, train_grouped.Count, color='g')\nmpl.pyplot.show()\ntrain_grouped","d4229688":"#Handling missing values in registration date and converting to numeric type.\n# There are two methods either to drop the missing dates or impute them with some old date value like '18-Jan-1900' so that model understands it is an outlier.\n# The former gave better results so I will be dropping the missing dates. Also to maintain continuity I have decided to take data only after 2005 since all missing dates are before 2005.\ntrain['Registration_Date'] = pd.to_datetime(train['Registration_Date'], format=\"%d-%b-%y\")\n\n","332af0b0":"train_indexed = train.set_index('Registration_Date')\ntrain_indexed","ef10febb":"train = train_indexed['2005':]\ntrain = train.reset_index()\ntrain","a4a66f55":"train['Registration_Date'] = train['Registration_Date'].apply(lambda x: x.toordinal())","10234a2c":"test.head(10).append(test.tail(10))","e72325ae":"test.info()","fe9593fc":"#Let us find out the unique number of patients in test set\ntest.Patient_ID.nunique()","093da127":"test.groupby('Patient_ID')['Health_Camp_ID'].count().unique()","b61111b8":"test_grouped = test.groupby('Patient_ID').agg('size').reset_index()\ntest_grouped.columns = [\"Patient_ID\", \"Number_of_registrations_per_person\"]\ntest_grouped = test_grouped.groupby('Number_of_registrations_per_person').agg('size').reset_index()\ntest_grouped.columns = [\"Number_of_registrations_per_person\", \"Count\"]\nmpl.pyplot.bar(test_grouped.Number_of_registrations_per_person, test_grouped.Count, color='g')\nmpl.pyplot.show()\ntest_grouped","03a1c224":"#Converting date to numeric\ntest['Registration_Date'] = pd.to_datetime(test['Registration_Date'], format=\"%d-%b-%y\")\ntest['Registration_Date'] = test['Registration_Date'].apply(lambda x: x.toordinal())","d1a528b4":"# Let us now input other supporting data and have a quick analysis \n\npatient_data = pd.read_csv('..\/input\/Patient_Profile.csv')\ncamp_data = pd.read_csv('..\/input\/Health_Camp_Detail.csv')\nFHC_attended = pd.read_csv('..\/input\/First_Health_Camp_Attended.csv')\nSHC_attended = pd.read_csv('..\/input\/Second_Health_Camp_Attended.csv')\nTHC_attended = pd.read_csv('..\/input\/Third_Health_Camp_Attended.csv')","d8198bfe":"print(\"Patient data structure -> \",\"Rows: \", patient_data.shape[0],'\\t',\"Cols: \",patient_data.shape[1])\nprint(\"Camps data structure -> \",\"Rows: \", camp_data.shape[0],'\\t',\"Cols: \",camp_data.shape[1])\nprint(\"First Format Health Camp Attended data structure ->  \",\"Rows: \", FHC_attended.shape[0],'\\t',\"Cols: \",FHC_attended.shape[1])\nprint(\"Second Format Health Camp Attended data structure ->  \",\"Rows: \", SHC_attended.shape[0],'\\t',\"Cols: \",SHC_attended.shape[1])\nprint(\"Third Format Health Camp Attended data structure ->  \",\"Rows: \", THC_attended.shape[0],'\\t',\"Cols: \",THC_attended.shape[1])","45402117":"#Let us find out what all data contains null values\ndataframes = [patient_data,camp_data,FHC_attended,SHC_attended,THC_attended]\nfor i in dataframes:\n  print(i.isnull().sum())","619967a5":"FHC_attended.head()","1e959da1":"FHC_attended.drop('Unnamed: 4',axis=1,inplace = True)","3072f3fd":"FHC_attended.head()","3bad2fdb":"patient_data.head()","6ce93047":"patient_data.info()","b2cd62e6":"# City_Type and Employer_Category have lots of NULL values. We will not be using them.\n# Let us now look at the different categorical variables\n\ncat_cols = ['Income','Education_Score','Age','First_Interaction']\nfor i in cat_cols:\n  print(\"Different categories in \",i,\" : \",'\\n',patient_data[i].value_counts())\n\n\n","2e3d87cd":"#Replacing None with -999\npatient_data.replace('None',-999,inplace=True)\n\n# Converting categorical columns to numeric\npatient_data['Income'] = patient_data['Income'].astype('int64')\npatient_data['Age'] = patient_data['Age'].astype('int64')\npatient_data['Education_Score'] = patient_data['Education_Score'].astype('float64')\npatient_data['First_Interaction'] = pd.to_datetime(patient_data['First_Interaction'], format=\"%d-%b-%y\")\npatient_data['First_Interaction'] = patient_data['First_Interaction'].apply(lambda x: x.toordinal())","4abcc0fe":"patient_data.info()","6bea497c":"camp_data.head()","83b31ab2":"camp_data.info()","d54a0c38":"cat_cols = ['Camp_Start_Date','Camp_End_Date','Category1','Category2']\nfor i in cat_cols:\n  print(\"Different categories in \",i,\" : \",'\\n',camp_data[i].value_counts())","2f808032":"#Converting date time to int\ncamp_data['Camp_Start_Date'] = pd.to_datetime(camp_data['Camp_Start_Date'], format=\"%d-%b-%y\")\ncamp_data['Camp_Start_Date'] = camp_data['Camp_Start_Date'].apply(lambda x: x.toordinal())\n\ncamp_data['Camp_End_Date'] = pd.to_datetime(camp_data['Camp_End_Date'],format=\"%d-%b-%y\")\ncamp_data['Camp_End_Date'] = camp_data['Camp_End_Date'].apply(lambda x: x.toordinal())","6071c10e":"#Creating an additional feature which will tell us the duration of the camp. Higher duration means chances of higher attendance\ncamp_data['Camp_Duration'] = camp_data['Camp_End_Date'] - camp_data['Camp_Start_Date']\n","429f47cf":"camp_data.head()","171275f3":"print(FHC_attended.head())\nFHC_attended.info()","aae9e5f0":"print(SHC_attended.head())\nTHC_attended.info()","5aba79f0":"print(THC_attended.head())\nTHC_attended.info()","a87b696c":"#Renaming healthscore columns in FHC_attended and SHC_attended\nFHC_attended.rename(columns = {'Health_Score':'Health_Score_Camp1'},inplace = True)\nSHC_attended.rename(columns = {'Health Score':'Health_Score_Camp2'},inplace = True)\nprint(FHC_attended.head())\nprint(SHC_attended.head())","fddf9990":"FHC_attended['Outcome_1'] = 1\nSHC_attended['Outcome_2'] = 1\nTHC_attended['Outcome_3'] = np.where(THC_attended['Number_of_stall_visited']>0,1,0)","22f8b1ff":"print(FHC_attended.head(),'\\n',SHC_attended.head(),'\\n',THC_attended.head())","d5221b00":"#Merging metadata with train \ntrain = pd.merge(train,patient_data,on='Patient_ID',how = 'left')\ntrain = pd.merge(train,camp_data,on='Health_Camp_ID',how = 'left')\ntrain = pd.merge(train,FHC_attended,on=['Patient_ID','Health_Camp_ID'],how = 'left')\ntrain = pd.merge(train,SHC_attended,on=['Patient_ID','Health_Camp_ID'],how = 'left')\ntrain = pd.merge(train,THC_attended,on=['Patient_ID','Health_Camp_ID'],how = 'left')\ntrain.head()","0f8f4ada":"#Creating Final Outcome column\ntrain['Final_Outcome'] = np.where((train['Outcome_1'] == 1) | (train['Outcome_2'] == 1) | (train['Outcome_3'] == 1),1,0)","9c272724":"train.isnull().sum()","ebf4329f":"train['Has_Donated'] = np.where(train['Donation']>0,1,0)","fbdf00fa":"list_donated = []\nfor i,row in train.iterrows():\n  if row['Has_Donated'] == 1:\n    list_donated.append(row['Patient_ID'])\n\nlist_donated","f85c5b1a":"#Creating additional features\ntrain['Camps_Registered_per_patient'] = train.groupby('Patient_ID')['Health_Camp_ID'].transform('count')\ntrain['Patients_Registered_per_camp'] = train.groupby('Health_Camp_ID')['Patient_ID'].transform('count')\ntrain['CampStart_Registration_Duration'] = train['Camp_Start_Date'] - train['Registration_Date']\ntrain['CampEnd_Registration_Duration'] = train['Camp_End_Date'] - train['Registration_Date']\ntrain['Registration_FirstInteraction_Duration'] = train['Registration_Date'] - train['First_Interaction']\ntrain['CampStart_FirstInteraction_Duration'] = train['Camp_Start_Date'] - train['First_Interaction']","f5b7af9b":"train.isnull().sum()","f67c74e9":"#Merging metadata with test\ntest = pd.merge(test,patient_data,on='Patient_ID',how = 'left')\ntest = pd.merge(test,camp_data,on='Health_Camp_ID',how = 'left')\ntest.head()","f319245e":"#Creating extra features\ntest['Camps_Registered_per_patient'] = test.groupby('Patient_ID')['Health_Camp_ID'].transform('count')\ntest['Patients_Registered_per_camp'] = test.groupby('Health_Camp_ID')['Patient_ID'].transform('count')\ntest[\"CampStart_Registration_Duration\"] = test[\"Camp_Start_Date\"] - test[\"Registration_Date\"]\ntest[\"CampEnd_Registration_Duration\"] = test[\"Camp_End_Date\"] - test[\"Registration_Date\"]\ntest[\"Registration_FirstInteraction_Duration\"] = test[\"Registration_Date\"] - test[\"First_Interaction\"]\ntest[\"CampStart_FirstInteraction_Duration\"] = test[\"Camp_Start_Date\"] - test[\"First_Interaction\"]\ntest['Has_Donated'] = np.where(test['Patient_ID'].isin(list_donated),1,0)","69843c27":"from sklearn.preprocessing import LabelEncoder\ncat_columns = []\nfor col in train.columns:\n  if train[col].dtype == 'object':\n    print(col)\n    cat_columns.append(col)\n    enc = preprocessing.LabelEncoder()\n    full_list = list(train[col].values) + list(test[col].values)\n    enc.fit(full_list)\n    train[col] = enc.transform(list(train[col].values))\n    test[col]  = enc.transform(list(test[col].values))","502ce3b6":"#Finally sorting the train dataframe ased on Registration Date and patient ID to capture any time related trand\ntrain = train.sort_values(['Registration_Date','Patient_ID'])\n","3616a043":"train.head()","3b92ad53":"test.head()","10b540ab":"# These columns were chosen after looking at feature importances and vrious tral\/error as they gave the best results.\ncols_to_use = ['Income','Age','Category1','Category2','Camp_Duration','Patients_Registered_per_camp','Camps_Registered_per_patient','CampStart_Registration_Duration','CampEnd_Registration_Duration','Registration_FirstInteraction_Duration','CampStart_FirstInteraction_Duration','Has_Donated']\nX = train[cols_to_use]\nY = train['Final_Outcome']","f06161ed":"test_final = test[X.columns]","bda5196b":"X.shape","6a450e38":"Y.shape","1141dc8e":"X.isnull().sum()","8507de19":"test_final.head()","9d7197d1":"#function for creating k-fold like validation sets for TS data. Since there is autocorrelation among datapoints we cannot use classical K-Fold approach\ndef train_test_split(k):\n   train_size = int(train.shape[0] * k)\n   x_tr, y_tr = X[0:train_size], Y[0:train_size]\n   x_v,y_v = X[train_size:X.shape[0]], Y[train_size:X.shape[0]]\n   return x_tr,y_tr,x_v,y_v\n","130569a8":"#ratios in which train data will be split\nratio = [0.88,0.9,0.92,0.94,0.96]","564a92d0":"#Applying Time Based validation(Results averaged over 5 subsets)\nroc_auc = 0\nfor i in ratio:\n  x_train,y_train,x_val,y_val = train_test_split(i)\n  rf=RandomForestClassifier(n_estimators=1000,max_depth=8,min_samples_split=4,max_features='sqrt')\n  model=rf.fit(x_train,y_train)\n  pred_val = model.predict(x_val)\n   \n  print(metrics.roc_auc_score(pred_val, y_val))\n  roc_auc+=metrics.roc_auc_score(pred_val, y_val)\n    \n\n\n\nprint(roc_auc\/5)","3bbab4ae":"#Fitting data to entire dataset\nrf.fit(X,Y)","b933c058":"pred_rf = rf.predict_proba(test_final)\npred_rf","a866cb35":"#storing probabilities of only success case\npred_prob_rf = []\nfor i in range(0,pred_rf.shape[0]):\n  pred_prob_rf.append(pred_rf[i][1])\n\npred_prob_rf","847188e6":"xgb = XGBClassifier(objective = \"binary:logistic\",eval_metric= 'auc',eta = 0.02,subsample = 0.8,min_child_weight = 5,colsample_bytree = 0.7,max_depth = 6)\n","719c0497":"#Applying Time Based validation(Results averaged over 5 subsets)\nroc_auc = 0\nfor i in ratio:\n  x_train,y_train,x_val,y_val = train_test_split(i)\n  model=xgb.fit(x_train,y_train)\n  pred_val = model.predict(x_val)\n  \n  print(metrics.roc_auc_score(pred_val, y_val))\n  roc_auc+=metrics.roc_auc_score(pred_val, y_val)\n    \n\n\n\nprint(roc_auc\/5)","cb5c0354":"xgb.fit(X,Y)","f1dca419":"pred_xgb = xgb.predict_proba(test_final)\npred_xgb","7aee002b":"pred_prob_xgb = []\nfor i in range(0,pred_xgb.shape[0]):\n  pred_prob_xgb.append(pred_xgb[i][1])\n\npred_prob_xgb","82d4d992":"#Averaging prediction of both the models\ndf_solution = pd.DataFrame()\ndf_solution['Patient_ID'] = test.Patient_ID\ndf_solution['Health_Camp_ID'] = test.Health_Camp_ID\ndf_solution['Outcome'] = pred_prob_rf\ndf_solution ","394447cc":"df_solution.to_csv('rf.csv',index=False)","6a82a34e":"\n\n> Some of the 'Registration_Dates' are missing in the dataset , we will either need to impute these values or drop them.\n\n","b04c1699":"\n\n> In almost all the variables large chunk of data is unknown\/missing.\nSince we cannot drop all the data so we will impute all of it with some large random number.\n\n","93a98954":"# Merging Data and Feature Engineering","ab4d8f71":"**Thought process behind chosing a time based validation set**\n\n\n\n> Since all the data in test set were in future dates as compared to train set so creating a similar validation set seemed intuitive\n\n","0ca05353":"\n\n> No null values in the test set\n\n","ea4943b1":"**Preparing Metadata for concatenating to train and test sets**","49718242":"\n\n> 'City_Type' and 'Employer_Category' contains a large number of NULL values(More than 60%).It seems only logical to drop these columns.\n\n\n> One more observation is the presence of column  'Unnamed:4' in First_Health_Camp.csv ...this maybe due to improper formatting of the file , so we will simply drop the columns\n\n\n\n","b84f0a3b":"\n\n> We have total of 35249 records in the test set with 5 anonymized features","a323549a":"## Creating Target Variable","91102e9e":"\n\n> Test set follows almost similar distribution as train set as most number of patients atleast for the number of registrations made per patient\n\n","7833e21a":"\n\n> Number of registrations vary from 1-32 and most of the patients have visited to the camps between 1-5 times.\n\n**This is a good indicator that people having repeating behaviour of visiting camps will have a higher probability of success. We will use this to create some additional features**\n\n","338217f0":"### 3.First,Second and Third Camp Format Data","b90b79ad":"### 1.   Patient Data\n\n\n","736936c8":"## Test Data Analysis","f87bc812":"\n\n> No column has any missing category. We will Label encode the two object types categories after joining with train and test data\n\n","9024a471":"### 2.Camps Data","5fb86999":"## Train Data Analysis","2d125f7a":"**There are 29898 unique patient ID's in a dataset of 75278 records which means many patients have registered for the camps more than 1 time. Let us look at the unique number of registrations a patient has made to the camps and its distribution**\n\n\n\n> Note : Here we are only trying to look at the number of registrations done by a patient(This does not mean the patient also attended the camp)\n\n","fea6612d":"**Just like train set , test set has 15324 unique patient ID's in a dataset of 35248 records. Let us look at the unique number of registrations a patient has made to the camps**","592b2e02":"**List of the features we will be creating**\n\n\n1.   *Camps_Registered_per_patient*: Number of camps a patient   has  registered for.\n2.   *Patients_Registered_per_camp* :Number of patients registered per camp.\n3.   *CampStart_Registration_Duration*: Duration between Camp start date and Registration date.\n4.   *CampEnd_Registration_Duration*: Duration between Camp end date and Registration date.\n5.   *Registration_FirstInteraction_Duration*: Duration between First Interaction and Registration date.\n6.   *CampStart_FirstInteraction_Duration*: Duration between First Interaction and Camp Start date.\n7.   *Has_Donated*: Whether a patient has denoted before or not.\n\n\n\n\n\n\n\n\n","acf68b76":"## RandomForestClassifier","99a6e971":"\n\n*   Our initial train and test data have 8 columns with test data almost half the size of test data.\n*   The interesting thing is we do not have a pre-defined target variable . We will be creating the target variable using the metadata provided later.\n\n","68b2e62f":"\n\n> All 3 files data looks good. The only thing we will be doing is giving 'Health_Score' column a new name in FHC_attended and SHC_attended to make it more interpretable\n\n","3e5b473b":"**One interesting thing about this dataset is that there is no pre-defined target variable,instead there are pre-defined rules from which we will be creating our own target variable.**\n\nApproach:\n*   Using FHC_attended,SHC_attended,THC_attended datasets create an additional feature as 'Outcome' with each row getting the value as 1. This is because all these patients have attended atleast 1 camp which means 'Success' in our case.\n\n*   Join the three datasets with train set and after joining fill the records having 'NaNs' in Outcome column with 0. This is because NaNs indicate that the patient registered for the camps but did not visit the camp which is 'Failure' in our case.\n\n","df860275":"# Data Analysis","2ebecda5":"# Model Building","19044bf3":"Now that we have our train and test set prepared lets have alook at them and then prepare our model","94cb9ac3":"# Prepare Submission File","ccab98ed":"\n\n> We have total of 75278 records in the training set with 5 anonymized features","5f930a9a":"**Hackathon 8:** HealthCare Analytics\n\n**Platform:** Analytics Vidhya\n\n# Problem Statement\n\n**The aim of this task was to predict the probability of 'Success' scenareo where Success was defined as the case where a registered patient ateended any one of the 3 medical camps**\n\nFor further details refer to the [problem statement](https:\/\/https:\/\/datahack.analyticsvidhya.com\/contest\/janatahack-healthcare-analytics\/#ProblemStatement)\n\n\n\n**Personal Remark:** Most interesting problem statement I have come across till now. If you are a beginner I urge you to dirty your hands on it. There is a lot to learn !!!! \nHope this notebook will be helpful.","50323486":"## Metadata Analysis","785cb640":"## XGBoostClassifier"}}