{"cell_type":{"5ab66458":"code","26674772":"code","200297e3":"code","e3d0260f":"code","6ed15009":"code","b63ef0cc":"code","f0cabf0f":"code","bd2800d6":"code","3e2ae7eb":"code","090f2a99":"code","c43d9e02":"code","19e3efc6":"code","dec8831a":"code","cbe0c0a8":"code","e47709a1":"code","0de50543":"code","47680e61":"code","a9346b18":"code","69b52c6e":"code","9df03be7":"code","bcb8009f":"code","e24772dd":"code","7d194fc2":"code","2d5aeee8":"code","505e540e":"code","e0761114":"code","0c17b204":"code","a25791ee":"code","d2ffa957":"code","6006eb5f":"code","d82c698d":"code","3222371e":"code","ed65dde2":"code","6bf70326":"code","6c44797c":"code","def0e483":"code","e92bc01b":"code","1988e228":"code","9092299e":"code","9e6b6f6e":"code","b42b4f39":"code","ce619f16":"code","e82d2011":"code","9bb0875c":"code","c79ed98c":"code","6112137c":"code","00a3afe6":"code","19636895":"code","353de049":"code","7b7c065b":"code","1aec71ae":"code","599423cc":"code","7afb8a52":"code","c8779c1e":"code","c3a30511":"code","f5d219c5":"code","63a4f9b2":"code","2dd196ca":"code","8d5a340a":"code","f60e4444":"code","c8d57bbb":"code","aa65382d":"code","7810e506":"code","f4ddadce":"code","0478391f":"code","9d97f9a2":"code","1e4b3b5d":"code","7c6cafbf":"code","7ffaa9f6":"code","d84860b3":"code","89beadf4":"code","ebd36898":"code","835aaff8":"code","9adff28b":"code","afd742c0":"code","238d8bc1":"code","2d52b878":"code","96d8a61b":"code","f4feb2d1":"code","1222e479":"code","27653e76":"code","ded33783":"code","499eea53":"code","1c382cb6":"code","49adcf97":"code","55dff942":"code","e9453aa3":"code","4091940c":"code","83c2e442":"code","16d578a1":"code","570419e3":"code","45ef40fc":"code","ef816db8":"code","0b4d5503":"code","a5345049":"code","4d69fc66":"code","3abd4c07":"code","87802ba6":"code","9e23fa86":"code","1bbc64f8":"code","52b30a40":"code","983b22cb":"code","7528ef77":"code","1c6394b2":"code","3abb6601":"code","2f3e4843":"code","52175299":"code","f253417e":"code","93dc3a72":"code","10a7cc77":"code","46038e52":"markdown","194649d1":"markdown","e6a210c8":"markdown","8147e88f":"markdown","b1da23bd":"markdown","cd663e24":"markdown","9f524b36":"markdown","ca27f860":"markdown","575d7afe":"markdown","cd574808":"markdown","2d5396fe":"markdown","5049051c":"markdown","5f022657":"markdown","b193a6c7":"markdown","4f207e82":"markdown","21d2191a":"markdown","553d67a8":"markdown","cedcf977":"markdown","5d6ab20d":"markdown","32e4fec6":"markdown","d796c2d0":"markdown","2b243a91":"markdown","8505316b":"markdown","698a2d32":"markdown","d8d498c4":"markdown","0653e522":"markdown","911a3508":"markdown","337a312f":"markdown","5652bd7a":"markdown","b774cb52":"markdown","8d02d549":"markdown","3c91bb79":"markdown","a2699856":"markdown","d0702866":"markdown","d7a1d9e2":"markdown","09925179":"markdown","78648d0f":"markdown","651e61d0":"markdown","3d209af2":"markdown","5328d17b":"markdown","202751f5":"markdown","d53df3ef":"markdown","f2b0ec6a":"markdown","83111294":"markdown","7abba1df":"markdown","0fc92f73":"markdown","877a1e0a":"markdown","62a3da18":"markdown","943d7df9":"markdown","92d2172e":"markdown","56c54964":"markdown"},"source":{"5ab66458":"import pandas as pd\nimport numpy as np\nimport os\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\n\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nfrom collections import defaultdict\nfrom collections import Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\n\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\n\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam","26674772":"train = pd.read_csv('\/kaggle\/input\/liar-dataset-factchecking\/train2.tsv',delimiter='\\t',encoding='utf-8', header=None)\ntest = pd.read_csv('\/kaggle\/input\/liar-dataset-factchecking\/test2.tsv',delimiter='\\t',encoding='utf-8', header=None)\nvalid = pd.read_csv('\/kaggle\/input\/liar-dataset-factchecking\/val2.tsv',delimiter='\\t',encoding='utf-8', header=None)","200297e3":"train.columns = ['values','id','label','statement','subject','speaker', 'job', 'state','party','barely_true_c','false_c','half_true_c','mostly_true_c','pants_on_fire_c','venue','extracted_justification']\ntest.columns = ['values','id','label','statement','subject','speaker', 'job', 'state','party','barely_true_c','false_c','half_true_c','mostly_true_c','pants_on_fire_c','venue','extracted_justification']\nvalid.columns = ['values','id','label','statement','subject','speaker', 'job', 'state','party','barely_true_c','false_c','half_true_c','mostly_true_c','pants_on_fire_c','venue','extracted_justification']","e3d0260f":"# reserve training and validation files for Training Set to provide enough examples for training\ntrain = pd.concat([train, valid])\nprint('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))\nprint('There are {} rows and {} columns in test'.format(test.shape[0],test.shape[1]))","6ed15009":"train.head(10)","b63ef0cc":"set(train['label'].tolist())","f0cabf0f":"true = ['true', 'mostly-true', 'barely-true', 'half-true']\nfalse = ['pants-fire', 'false']","bd2800d6":"train.shape","3e2ae7eb":"train_true = train[train['label'].isin(true)]\ntrain_false = train[train['label'].isin(false)]\n# Copy the contents of two columns \"statement\" and \"extracted_justification\" to new column \"text\"\ntrain_true['text'] = train_true['statement']\ntrain_false['text'] = train_false['statement']\ntrain = pd.concat([train_true, train_false]) # Final training data\ntrain.head()","090f2a99":"test_true = test[test['label'].isin(true)]\ntest_false = test[test['label'].isin(false)]\n# Copy the contents of two columns \"statement\" and \"extracted_justification\" to new column \"text\"\ntest_true['text'] = test_true['statement']\ntest_false['text'] = test_false['statement']\ntest = pd.concat([test_true, test_false]) # Final test data\ntest1 = pd.concat([test_true, test_false])\ntest.head()","c43d9e02":"true = ['true', 'mostly-true', 'barely-true', 'half-true']\nfalse = ['pants-fire', 'false']","19e3efc6":"# Values of label\ntruth_ = {'true':1, 'mostly-true':1, 'barely-true':1, 'half-true':1, 'pants-fire':0, 'false':0} \n#train['binary_label'] = train['label'].apply(lambda x: truth_[x])\ntrain['binary_label'] = train['label'].apply(lambda x: truth_[x])\ntest['binary_label'] = test['label'].apply(lambda x: truth_[x])\n\ntrain.head() # Check the new columns just created","dec8831a":"# extracting the number of examples of each class\nReal_len = train[train['binary_label'] == 1].shape[0]\nNot_len = train[train['binary_label'] == 0].shape[0]","cbe0c0a8":"# bar plot of the 3 classes\nplt.rcParams['figure.figsize'] = (7, 5)\nplt.bar(10,Real_len,3, label=\"Real\", color='blue')\nplt.bar(15,Not_len,3, label=\"Not\", color='red')\nplt.legend()\nplt.ylabel('Number of examples')\nplt.title('Propertion of examples')\nplt.show()","e47709a1":"def length(text):    \n    '''a function which returns the length of text'''\n    return len(str(text))","0de50543":"train['statement_length'] = train['statement'].apply(length)","47680e61":"plt.rcParams['figure.figsize'] = (18.0, 6.0)\nbins = 150\nplt.hist(train[train['binary_label'] == 0]['statement_length'], alpha = 0.6, bins=bins, label='Not')\nplt.hist(train[train['binary_label'] == 1]['statement_length'], alpha = 0.8, bins=bins, label='Real')\nplt.xlabel('length')\nplt.ylabel('numbers')\nplt.legend(loc='upper right')\nplt.xlim(0,150)\nplt.grid()\nplt.show()","a9346b18":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntext_len=train[train['binary_label']==1]['statement'].str.len()\nax1.hist(text_len,color='blue')\nax1.set_title('Fake')\ntext_len=train[train['binary_label']==0]['statement'].str.len()\nax2.hist(text_len,color='red')\nax2.set_title('Not Fake')\nfig.suptitle('Characters in text')\nplt.show()","69b52c6e":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntext_len=train[train['binary_label']==1]['statement'].str.split().map(lambda x: len(str(x)))\nax1.hist(text_len,color='blue')\nax1.set_title('Fake')\ntext_len=train[train['binary_label']==0]['statement'].str.split().map(lambda x: len(str(x)))\nax2.hist(text_len,color='red')\nax2.set_title('Not Fake')\nfig.suptitle('Words in text')\nplt.show()","9df03be7":"train['extracted_justification_length'] = train['extracted_justification'].apply(length)","bcb8009f":"plt.rcParams['figure.figsize'] = (18.0, 6.0)\nbins = 150\nplt.hist(train[train['binary_label'] == 0]['extracted_justification_length'], alpha = 0.6, bins=bins, label='Not')\nplt.hist(train[train['binary_label'] == 1]['extracted_justification_length'], alpha = 0.8, bins=bins, label='Real')\nplt.xlabel('length')\nplt.ylabel('numbers')\nplt.legend(loc='upper right')\nplt.xlim(0,150)\nplt.grid()\nplt.show()","e24772dd":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntext_len=train[train['binary_label']==1]['extracted_justification'].str.len()\nax1.hist(text_len,color='blue')\nax1.set_title('Fake')\ntext_len=train[train['binary_label']==0]['extracted_justification'].str.len()\nax2.hist(text_len,color='red')\nax2.set_title('Not Fake')\nfig.suptitle('Characters in text')\nplt.show()","7d194fc2":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntext_len=train[train['binary_label']==1]['extracted_justification'].str.split().map(lambda x: len(str(x)))\nax1.hist(text_len,color='blue')\nax1.set_title('Fake')\ntext_len=train[train['binary_label']==0]['extracted_justification'].str.split().map(lambda x: len(str(x)))\nax2.hist(text_len,color='red')\nax2.set_title('Not Fake')\nfig.suptitle('Words in text')\nplt.show()","2d5aeee8":"train['text_length'] = train['text'].apply(length)","505e540e":"plt.rcParams['figure.figsize'] = (18.0, 6.0)\nbins = 150\nplt.hist(train[train['binary_label'] == 0]['text_length'], alpha = 0.6, bins=bins, label='Not')\nplt.hist(train[train['binary_label'] == 1]['text_length'], alpha = 0.8, bins=bins, label='Real')\nplt.xlabel('length')\nplt.ylabel('numbers')\nplt.legend(loc='upper right')\nplt.xlim(0,150)\nplt.grid()\nplt.show()","e0761114":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntext_len=train[train['binary_label']==1]['text'].str.len()\nax1.hist(text_len,color='blue')\nax1.set_title('Fake')\ntext_len=train[train['binary_label']==0]['text'].str.len()\nax2.hist(text_len,color='red')\nax2.set_title('Not Fake')\nfig.suptitle('Characters in text')\nplt.show()","0c17b204":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntext_len=train[train['binary_label']==1]['text'].str.split().map(lambda x: len(str(x)))\nax1.hist(text_len,color='blue')\nax1.set_title('Fake')\ntext_len=train[train['binary_label']==0]['text'].str.split().map(lambda x: len(str(x)))\nax2.hist(text_len,color='red')\nax2.set_title('Not Fake')\nfig.suptitle('Words in text')\nplt.show()","a25791ee":"def create_corpus(target):\n    corpus=[]\n    val=[]\n    val = train[train['binary_label']==target]['text'].to_string()\n    corpus = val.split()\n    return corpus","d2ffa957":"def create_corpus_df(data, target):\n    corpus=[]\n    val=[]\n    #train[train['binary_label']==target]['text'].str.split()\n    val = data[data['binary_label']==target]['text'].to_string()\n    corpus = val.split()\n    return corpus","6006eb5f":"corpus=create_corpus(0)\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]","d82c698d":"# displaying the stopwords\nnp.array(stop)","3222371e":"plt.rcParams['figure.figsize'] = (18.0, 6.0)\nx,y=zip(*top)\nplt.bar(x,y)","ed65dde2":"corpus=create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    \n\nplt.rcParams['figure.figsize'] = (18.0, 6.0)\nx,y=zip(*top)\nplt.bar(x,y)","6bf70326":"plt.figure(figsize=(16,5))\ncorpus=create_corpus(1)\n\ndic=defaultdict(int)\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y)","6c44797c":"plt.figure(figsize=(16,5))\ncorpus=create_corpus(0)\ndic=defaultdict(int)\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y,color='green')","def0e483":"plt.figure(figsize=(16,5))\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)","e92bc01b":"sns.barplot(x=y,y=x)","1988e228":"def get_top_text_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","9092299e":"plt.figure(figsize=(16,5))\ntop_text_bigrams=get_top_text_bigrams(train['text'].values.astype('U'))[:10]\nx,y=map(list,zip(*top_text_bigrams))\nsns.barplot(x=y,y=x)","9e6b6f6e":"df=pd.concat([train,test])\ndf.shape","b42b4f39":"df['text']=df['text'].values.astype('U')","ce619f16":"example = 'We are Logically https:\/\/www.logically.co.uk\/about'","e82d2011":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\nremove_URL(example)","9bb0875c":"df['text']=df['text'].apply(lambda x : remove_URL(x))","c79ed98c":"example = \"\"\"<div>\n<h1>Real or Fake<\/h1>\n<p>Logically <\/p>\n<a href=\"https:\/\/www.logically.co.uk\/about\">We are Logically<\/a>\n<\/div>\"\"\"","6112137c":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\nprint(remove_html(example))","00a3afe6":"df['text']=df['text'].apply(lambda x : remove_html(x))","19636895":"# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","353de049":"remove_emoji(\"Omg another Earthquake \ud83d\ude14\ud83d\ude14\")","7b7c065b":"df['text']=df['text'].apply(lambda x: remove_emoji(x))","1aec71ae":"example=\"We are #Logically!!\"","599423cc":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)","7afb8a52":"print(remove_punct(example))","c8779c1e":"df['text']=df['text'].apply(lambda x : remove_punct(x))","c3a30511":"corpus_new1=create_corpus_df(df,1)\nlen(corpus_new1)","f5d219c5":"corpus_new1[:10]","63a4f9b2":"# Generating the wordcloud with the values under the category dataframe\nplt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n                          background_color='black',\n                          max_font_size = 80\n                         ).generate(\" \".join(corpus_new1[:50]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","2dd196ca":"corpus_new0=create_corpus_df(df,0)\nlen(corpus_new0)","8d5a340a":"corpus_new0[:10]","f60e4444":"# Generating the wordcloud with the values under the category dataframe\nplt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n                          background_color='black',\n                          max_font_size = 80\n                         ).generate(\" \".join(corpus_new0[:50]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","c8d57bbb":"df.head(10)","aa65382d":"random_state_split = 10\nDropout_num = 0\nlearning_rate = 6e-6\nvalid = 0.2\nepochs_num = 3\nbatch_size_num = 16\ntarget_corrected = False\ntarget_big_corrected = False","7810e506":"def cv(data):\n    count_vectorizer = CountVectorizer()\n\n    emb = count_vectorizer.fit_transform(data)\n\n    return emb, count_vectorizer\n\nlist_corpus = df[\"text\"].tolist()\nlist_labels = df[\"binary_label\"].tolist()\n\nX_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, \n                                                                                random_state=random_state_split)\n\nX_train_counts, count_vectorizer = cv(X_train)\nX_test_counts = count_vectorizer.transform(X_test)","f4ddadce":"def plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n        lsa = TruncatedSVD(n_components=2)\n        lsa.fit(test_data)\n        lsa_scores = lsa.transform(test_data)\n        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n        color_column = [color_mapper[label] for label in test_labels]\n        colors = ['orange','blue']\n        if plot:\n            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n            orange_patch = mpatches.Patch(color='orange', label='Not')\n            blue_patch = mpatches.Patch(color='blue', label='Real')\n            plt.legend(handles=[orange_patch, blue_patch], prop={'size': 30})\n\nfig = plt.figure(figsize=(16, 16))          \nplot_LSA(X_train_counts, y_train)\nplt.show()","0478391f":"def tfidf(data):\n    tfidf_vectorizer = TfidfVectorizer()\n\n    train = tfidf_vectorizer.fit_transform(data)\n\n    return train, tfidf_vectorizer\n\nX_train_tfidf, tfidf_vectorizer = tfidf(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)","9d97f9a2":"fig = plt.figure(figsize=(16, 16))          \nplot_LSA(X_train_tfidf, y_train)\nplt.show()","1e4b3b5d":"def create_corpus_new(df):\n    corpus=[]\n    for text in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(text)]\n        corpus.append(words)\n    return corpus   ","7c6cafbf":"corpus=create_corpus_new(df)","7ffaa9f6":"embedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word = values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","d84860b3":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntext_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","89beadf4":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","ebd36898":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i < num_words:\n        emb_vec=embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i]=emb_vec           ","835aaff8":"text_pad[0][0:]","9adff28b":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=3e-4)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","afd742c0":"model.summary()","238d8bc1":"train1=text_pad[:train.shape[0]]\ntest1=text_pad[train.shape[0]:]","2d52b878":"X_train,X_test,y_train,y_test=train_test_split(train1,train['binary_label'].values,test_size=0.2)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","96d8a61b":"fig = plt.figure(figsize=(16, 16))          \nplot_LSA(train1,train['binary_label'])\nplt.show()","f4feb2d1":"# Recomended 10-20 epochs\nhistory=model.fit(X_train,y_train,batch_size=4,epochs=10,validation_data=(X_test,y_test),verbose=2)","1222e479":"train_pred_GloVe = model.predict(test1)\ntrain_pred_GloVe_int = train_pred_GloVe.round().astype('int')","27653e76":"# We will use the official tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","ded33783":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport tokenization","499eea53":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","1c382cb6":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    \n    if Dropout_num == 0:\n        # Without Dropout\n        out = Dense(1, activation='sigmoid')(clf_output)\n    else:\n        # With Dropout(Dropout_num), Dropout_num > 0\n        x = Dropout(Dropout_num)(clf_output)\n        out = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","49adcf97":"def clean_text(text):\n    \"\"\"Removes links and non-ASCII characters\"\"\"\n    \n    text = ''.join([x for x in tweet if x in string.printable])\n    \n    # Removing URLs\n    text = re.sub(r\"http\\S+\", \"\", text)\n    \n    return text","55dff942":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","e9453aa3":"def remove_punctuations(text):\n    punctuations = '@#!?+&*[]-%.:\/();$=><|{}^' + \"'`\"\n    \n    for p in punctuations:\n        text = text.replace(p, f' {p} ')\n\n    text = text.replace('...', ' ... ')\n    \n    if '...' not in text:\n        text = text.replace('..', ' ... ')\n    \n    return text","4091940c":"abbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}","83c2e442":"def convert_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word","16d578a1":"def convert_abbrev_in_text(text):\n    tokens = word_tokenize(text)\n    tokens = [convert_abbrev(word) for word in tokens]\n    text = ' '.join(tokens)\n    return text","570419e3":"# Load BERT from the Tensorflow Hub\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","45ef40fc":"# Load CSV files containing training data\ntrain = train.copy()\ntest = test.copy()","ef816db8":"if target_big_corrected:\n    train[\"text\"] = train[\"text\"].apply(lambda x: clean_text(x))\n    test[\"text\"] = test[\"text\"].apply(lambda x: clean_text(x))\n    \n    train[\"text\"] = train[\"text\"].apply(lambda x: remove_emoji(x))\n    test[\"text\"] = test[\"text\"].apply(lambda x: remove_emoji(x))\n    \n    train[\"text\"] = train[\"text\"].apply(lambda x: remove_punctuations(x))\n    test[\"text\"] = test[\"text\"].apply(lambda x: remove_punctuations(x))\n    \n    train[\"text\"] = train[\"text\"].apply(lambda x: convert_abbrev_in_text(x))\n    test[\"text\"] = test[\"text\"].apply(lambda x: convert_abbrev_in_text(x))","0b4d5503":"train['text'] = train['text'].astype('U')\ntest['text'] = test['text'].astype('U')","a5345049":"# Load tokenizer from the bert layer\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","4d69fc66":"# Encode the text into tokens, masks, and segment flags\ntrain_input = bert_encode(train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train.binary_label.values","3abd4c07":"# Build BERT model with my tuning\nmodel_BERT = build_model(bert_layer, max_len=160)\nmodel_BERT.summary()","87802ba6":"# Train BERT model with my tuning\ncheckpoint = ModelCheckpoint('model_BERT.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model_BERT.fit(\n    train_input, train_labels,\n    validation_split = valid,\n    epochs = epochs_num, # recomended 3-5 epochs\n    callbacks=[checkpoint],\n    batch_size = batch_size_num\n)","9e23fa86":"# Prediction by BERT model with my tuning\nmodel_BERT.load_weights('model_BERT.h5')\ntest_pred_BERT = model_BERT.predict(test_input)\ntest_pred_BERT_int = test_pred_BERT.round().astype('int')","1bbc64f8":"# Prediction by BERT model with my tuning for the training data - for the Confusion Matrix\ntrain_pred_BERT = model_BERT.predict(train_input)\ntrain_pred_BERT_int = train_pred_BERT.round().astype('int')","52b30a40":"pred = pd.DataFrame(test_pred_BERT, columns=['preds'])\npred.plot.hist()","983b22cb":"evaluation = pd.DataFrame(test_pred_BERT_int, columns=['preds'])\nevaluation['real'] = test['binary_label']","7528ef77":"# Showing Confusion Matrix\ndef plot_cm(y_true, y_pred, title, figsize=(5,5)):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","1c6394b2":"# Showing Confusion Matrix for GloVe model\nplot_cm(train_pred_GloVe_int, test['binary_label'].values, 'Confusion matrix for GloVe model', figsize=(7,7))","3abb6601":"# Showing Confusion Matrix for BERT model\nplot_cm(test_pred_BERT_int, test['binary_label'].values, 'Confusion matrix for BERT model', figsize=(7,7))","2f3e4843":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix","52175299":"testy = test['binary_label'].values\npreds = train_pred_GloVe_int","f253417e":"# accuracy: (tp + tn) \/ (p + n)\naccuracy = accuracy_score(testy, preds)\nprint('Accuracy: %f' % accuracy)\n# precision tp \/ (tp + fp)\nprecision = precision_score(testy, preds)\nprint('Precision: %f' % precision)\n# recall: tp \/ (tp + fn)\nrecall = recall_score(testy, preds)\nprint('Recall: %f' % recall)\n# f1: 2 tp \/ (2 tp + fp + fn)\nf1 = f1_score(testy, preds)\nprint('F1 score: %f' % f1)\n \n# kappa\nkappa = cohen_kappa_score(testy, preds)\nprint('Cohens kappa: %f' % kappa)\n# ROC AUC\nauc = roc_auc_score(testy, preds)\nprint('ROC AUC: %f' % auc)","93dc3a72":"testy = test['binary_label'].values\npreds = test_pred_BERT_int","10a7cc77":"# accuracy: (tp + tn) \/ (p + n)\naccuracy = accuracy_score(testy, preds)\nprint('Accuracy: %f' % accuracy)\n# precision tp \/ (tp + fp)\nprecision = precision_score(testy, preds)\nprint('Precision: %f' % precision)\n# recall: tp \/ (tp + fn)\nrecall = recall_score(testy, preds)\nprint('Recall: %f' % recall)\n# f1: 2 tp \/ (2 tp + fp + fn)\nf1 = f1_score(testy, preds)\nprint('F1 score: %f' % f1)\n \n# kappa\nkappa = cohen_kappa_score(testy, preds)\nprint('Cohens kappa: %f' % kappa)\n# ROC AUC\nauc = roc_auc_score(testy, preds)\nprint('ROC AUC: %f' % auc)","46038e52":"## N-gram analysis","194649d1":"# Why do we factcheck?\n#### We rely on information to make meaningful decisions that affect our lives, but the nature of the internet means that misinformationreaches more people faster than ever before.\n\n#### The human brain is lazy. People tend to believe things that put less stress on their brains or that are coherent with what they already know. This can cause us to overlook relevant facts, even when they are clearly presented.\n\n#### It\u2019s not often that we look for what we don\u2019t see, but today it\u2019s easier than ever to control what people do see. The evolution of mobile internet technologies over last two decades, has fundamentally changed our relationship with information and facts, but information in many of these digital spaces isn\u2019t always true. We already know that falsehoods travel faster and farther than truth online\u2014with the potential to incite violence, wreak havoc on financial markets and disrupt democracies.\n\n## That\u2019s why we\u2019ve created logically.\n\n![](https:\/\/www.logically.co.uk\/static\/images\/fact-check\/fc-illustration.png)\n\n## Fact Check with Logically\n\n#### Our mobile app gives you access to the largest team of dedicated fact checkers on the planet. If you spot something online that you\u2019re not sure about\u2014whether it\u2019s a fact, assertion or claim\u2014send it to us and we\u2019ll investigate it for you. Our fact checking team will do our best to determine whether or not this particular bit of information is reliable, and send you a short report that you can share with your friends, to make sure that no one falls victim to false information.","e6a210c8":"### For the binary runs we grouped pants on fire, false and mostly false as FALSE and true, mostly true and half true as TRUE. As reference, Wang (2017 best models (text and metadata)","8147e88f":"# NLP:\n* EDA (with WordCloud) \n* Bag of Words \n* TF IDF\n* GloVe\n* BERT with TFHub and with Submission\n* PCA visualization for the main models\n* Showing Confusion Matrices for BERT and GloVe","b1da23bd":"![](http:\/\/2.bp.blogspot.com\/-5qilZgosGX0\/Xoq9WxMOmeI\/AAAAAAAAG34\/miDkcnu2NLkY99tmflWrC1nCPKy-IzGFQCK4BGAYYCw\/s1600\/LogicallyLogoV2.png)","cd663e24":"#### As we are done with preparartion of training and test data. Now, create a new column for each label and assign binary values (0 or 1) to the corresponding labels in both training and test dataset.","9f524b36":"## Download data","ca27f860":"# Create table headers    \n","575d7afe":"## Build and train BERT model","cd574808":"## 1. Import libraries <a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","2d5396fe":"### Analyzing punctuations","5049051c":"### Number of characters in statement","5f022657":"In both of them,\"the\" dominates which is followed by \"of\".","b193a6c7":"## 2. Download data <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","4f207e82":"Now,we will analyze tweets with class 1.","21d2191a":"## 4. Data Cleaning <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","553d67a8":"Before we begin with anything else, let's check the class distribution.","cedcf977":"### Removing punctuations","5d6ab20d":"### Here we will use GloVe pretrained corpus model to represent our words. It is available in 3 varieties : 50D, 100D and 200 Dimentional. We will try 100D here.","32e4fec6":"## 9. BERT using TFHub <a class=\"anchor\" id=\"9\"><\/a>\n\n[Back to Table of Contents](#0.1)","d796c2d0":"### Removing HTML tags","2b243a91":"## 6. Bag of Words Counts <a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","8505316b":"### Removing Emojis","698a2d32":"### Common words","d8d498c4":"### Number of characters in text = statement_+_extracted_justification","0653e522":"### Real news","911a3508":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents  \n1. [Import libraries](#1)\n1. [Download data](#2)\n1. [EDA](#3)\n1. [Data Cleaning](#4)\n1. [WordCloud](#5)\n1. [Bag of Words Counts](#6)\n1. [TF IDF](#7)\n1. [GloVe](#8)\n1. [BERT using TFHub](#9)\n   - [Submission by BERT](#9.1)\n1. [Showing Confusion Matrices](#10)","337a312f":"### Visualizing the embeddings","5652bd7a":"## 10. Showing Confusion Matrices<a class=\"anchor\" id=\"10\"><\/a>\n\n[Back to Table of Contents](#0.1)","b774cb52":"## 8. GloVe <a class=\"anchor\" id=\"8\"><\/a>\n\n[Back to Table of Contents](#0.1)","8d02d549":"### The training data contains a row per statement, with an id, the text of the statement, and 13 different features. A label that we'll try to predict.","3c91bb79":"## Prediction","a2699856":"## Big target correction","d0702866":"we will do a bigram (n=2) analysis over the texts. Let's check the most common bigrams in texts.","d7a1d9e2":"## 3. EDA <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","09925179":"# Class distribution","78648d0f":"### Common stopwords in texts","651e61d0":"[Go to Top](#0)","3d209af2":"### 9.1. Submission by BERT<a class=\"anchor\" id=\"9.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","5328d17b":"Now,we will move on to class 0.","202751f5":"## 5. WordCloud <a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","d53df3ef":"### Number of characters in extracted_justification","f2b0ec6a":"First let's check texts indicating real news.","83111294":"## 7. TF IDF <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","7abba1df":"#### These embeddings don't look very cleanly separated. Let's see if we can still fit a useful model on them.","0fc92f73":"### Lot of cleaning needed !","877a1e0a":"### Fake news","62a3da18":"## Target correction","943d7df9":"### Removing urls","92d2172e":"## I hope you find this kernel useful and enjoyable.","56c54964":"## Baseline Model with GloVe results"}}