{"cell_type":{"70433125":"code","d4541da9":"code","1b49055d":"code","aec5e28b":"code","be158ad8":"code","7d39e078":"code","a9a764df":"code","1523e153":"code","6a54e257":"code","b2d045ef":"code","f0967c14":"code","4d38c159":"code","69332394":"code","935c95be":"code","2646199f":"code","f72387eb":"code","72ee286a":"code","ada94ec9":"code","b6637451":"code","329b9565":"code","6742f876":"code","71d23163":"code","371a9200":"code","bc4cdeeb":"code","2f4adc5c":"code","5ab471b9":"code","3c6041e8":"code","8b724c13":"code","8b332353":"code","78246cd9":"code","fda73e73":"code","6f2b7e66":"code","ed5ec132":"code","3e2eaeb8":"code","084f5c70":"code","3d7edd7e":"code","ef7f3905":"code","918ad429":"code","1a6f6f40":"code","d90436bb":"code","d68f06c2":"code","3483bd3d":"code","0dc0f138":"code","a44488a8":"code","46da8a9b":"code","4f260ff9":"code","a4b2ef69":"code","517b31cc":"code","c16aafce":"code","029a9f41":"code","db9d1e42":"code","27783881":"code","f06bd424":"code","947d7b2e":"code","581d35e3":"code","a1959520":"markdown","b9a31194":"markdown","efca3e50":"markdown","06fecdc7":"markdown","5a0e7eb2":"markdown","4a700d74":"markdown","5888954f":"markdown","b4b88bcf":"markdown","38034c33":"markdown","f4a8952d":"markdown","0bf6b66c":"markdown","834f3889":"markdown","a97e33e6":"markdown","63369556":"markdown","5bfde36c":"markdown","e09e0112":"markdown","6240dbe1":"markdown","a3d07d0a":"markdown","5c76fb77":"markdown","44eedd13":"markdown","8d1cb4b2":"markdown","118aac51":"markdown","7eab5e2e":"markdown","f9410b41":"markdown","b6cc7470":"markdown","22bba223":"markdown","06d4e0e4":"markdown","7a91e06a":"markdown","cff61bdc":"markdown","1afda0f8":"markdown","01c926e0":"markdown","218b9aad":"markdown","5661f059":"markdown","287f1aaa":"markdown","c29cda0f":"markdown","d5f3885d":"markdown"},"source":{"70433125":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d4541da9":"import plotly.graph_objs as go\nfrom plotly.offline import  init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud,STOPWORDS\nimport plotly_express as px\n%matplotlib inline","1b49055d":"import re\nimport string\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel","aec5e28b":"# goodreads data\n\nbooks_data = pd.read_csv('\/kaggle\/input\/goodbooks-10k\/books.csv',error_bad_lines = False)\ntags_data = pd.read_csv('\/kaggle\/input\/goodbooks-10k\/book_tags.csv')\nratings_data = pd.read_csv('\/kaggle\/input\/goodbooks-10k\/ratings.csv')\nbook_tags = pd.read_csv('\/kaggle\/input\/goodbooks-10k\/tags.csv')\n\n# book crossing data\n\nuser_cols = ['user_id', 'location', 'age']\ncross_users_data = pd.read_csv('..\/input\/bookcrossing-dataset\/Book reviews\/BX-Users.csv', sep=';', names=user_cols, encoding='latin-1', low_memory=False, skiprows=1)\nbook_cols = ['isbn', 'book_title' ,'book_author','year_of_publication', 'publisher', 'img_s', 'img_m', 'img_l']\ncross_books_data = pd.read_csv('..\/input\/bookcrossing-dataset\/Book reviews\/BX-Books.csv', sep=';', names=book_cols, encoding='latin-1', low_memory=False, skiprows=1)\nrating_cols = ['user_id', 'isbn', 'rating']\ncross_ratings_data = pd.read_csv('..\/input\/bookcrossing-dataset\/Book reviews\/BX-Book-Ratings.csv', sep=';', names=rating_cols, encoding='latin-1', low_memory=False, skiprows=1)","be158ad8":"books_data.head()","7d39e078":"cross_books_data.head()","a9a764df":"books_data = books_data.drop(columns=['id', 'best_book_id', 'work_id', 'isbn', 'isbn13', 'title','work_ratings_count',\n                                   'work_text_reviews_count', 'ratings_1', 'ratings_2', 'ratings_3', 'ratings_4', 'ratings_5', \n                                    'image_url','small_image_url'])","1523e153":"books_data = books_data.dropna()\ncross_books_data = cross_books_data.drop(columns=['img_s', 'img_m', 'img_l'])","6a54e257":"ratings_data = ratings_data.sort_values(\"user_id\")\nratings_data.drop_duplicates(subset =[\"user_id\",\"book_id\"], keep = False, inplace = True) \nbooks_data.drop_duplicates(subset='original_title',keep=False,inplace=True)\nbook_tags.drop_duplicates(subset='tag_id',keep=False,inplace=True)\ntags_data.drop_duplicates(subset=['tag_id','goodreads_book_id'],keep=False,inplace=True)\ncross_ratings_data.drop_duplicates(subset =[\"user_id\",\"isbn\"], keep = False, inplace = True) \ncross_books_data.drop_duplicates(subset='book_title',keep=False,inplace=True)","b2d045ef":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","f0967c14":"cross_books_data['book_title'] = cross_books_data['book_title'].apply(lambda x:clean_text(x))","4d38c159":"merge_data = pd.merge(cross_books_data, cross_ratings_data, on='isbn')\nmerge_data =  merge_data.sort_values('isbn', ascending=True)\nmerge_data.head()","69332394":"top_rated = books_data.sort_values('average_rating', ascending=False)\ntf_top_rated = top_rated[:25]\nfig = px.bar(tf_top_rated, x=\"average_rating\", y=\"original_title\", title='Top Rated Books and Their Ratings',\n             orientation='h', color='original_title', width=1500, height=700)\nfig.show()","935c95be":"fig = px.treemap(tf_top_rated, path=['original_title'], values='average_rating',title='Top Rated Books', width=1000, height=700)\nfig.show()","2646199f":"top_popular = books_data.sort_values('ratings_count', ascending=False)\nfifty_top_popular = top_popular[:50]\nfig = px.bar(fifty_top_popular, x=\"ratings_count\", y=\"original_title\", title='Top Popular Books', orientation='h', color='original_title',\n             width=1500, height=700)\nfig.show()","f72387eb":"fig = px.treemap(fifty_top_popular, path=['original_title'], values='ratings_count',title='Popular Books', width=1000, height=700)\nfig.show()","72ee286a":"fifty_top_authors = top_rated[:50]\nfig = px.treemap(fifty_top_authors, path=['authors'], values='average_rating',title='Popular Authors', width=1000, height=700)\nfig.show()","ada94ec9":"top_author_counts = books_data['authors'].value_counts().reset_index()\ntop_author_counts.columns = ['value', 'count']\ntop_author_counts['value'] = top_author_counts['value']\ntop_author_counts = top_author_counts.sort_values('count')\nfig = px.bar(top_author_counts.tail(50), x=\"count\", y=\"value\", title='Top Authors', orientation='h', color='value',\n             width=1000, height=700)\nfig.show()","b6637451":"cross_typ = merge_data['year_of_publication'].value_counts().reset_index()\ncross_typ.columns = ['value', 'count']\ncross_typ['value'] = cross_typ['value'] + ' year'\ncross_typ = cross_typ.sort_values('count')\nfig = px.bar(cross_typ.tail(50), x=\"count\", y=\"value\", title='Top Years of Publishing', orientation='h', color='value',\n             width=1000, height=700)\nfig.show()","329b9565":"cross_author_counts = merge_data['book_author'].value_counts().reset_index()\ncross_author_counts.columns = ['value', 'count']\ncross_author_counts['value'] = cross_author_counts['value']\ncross_author_counts = cross_author_counts.sort_values('count')\nfig = px.bar(cross_author_counts.tail(50), x=\"count\", y=\"value\", title='Top Authors', orientation='h', color='value',\n             width=1000, height=700)\nfig.show()","6742f876":"top_book_counts = merge_data['book_title'].value_counts().reset_index()\ntop_book_counts.columns = ['value', 'count']\ntop_book_counts['value'] = top_book_counts['value']\ntop_book_counts = top_book_counts.sort_values('count')\nfig = px.bar(top_book_counts.tail(20), x=\"count\", y=\"value\", title='Top Books', orientation='h', color='value',\n             width=1000, height=700)\nfig.show()","71d23163":"merge_data['rating'].value_counts().iplot(kind='bar',\n                                         xTitle='Rating',\n                                         yTitle='Counts',\n                                         title='Rating Distribution',\n                                         color='blue')","371a9200":"stop_words=set(STOPWORDS)\nauthor_string = \" \".join(books_data['authors'])\ntitle_string = \" \".join(books_data['original_title'])\ncross_author_string = \" \".join(merge_data['book_author'].astype(str))\ncross_title_string = \" \".join(merge_data['book_title'].astype(str))\ncross_publisher_string = \" \".join(merge_data['publisher'].astype(str))","bc4cdeeb":"def wordcloud(string):\n    wc = WordCloud(width=800,height=500,mask=None,random_state=21, max_font_size=110,stopwords=stop_words).generate(string)\n    fig=plt.figure(figsize=(16,8))\n    plt.axis('off')\n    plt.imshow(wc)","2f4adc5c":"wordcloud(author_string)","5ab471b9":"wordcloud(title_string)","3c6041e8":"wordcloud(cross_author_string)","8b724c13":"wordcloud(cross_title_string)","8b332353":"wordcloud(cross_publisher_string)","78246cd9":"content_data = books_data[['original_title','authors','average_rating']]\ncontent_data = content_data.astype(str)","fda73e73":"content_data['content'] = content_data['original_title'] + ' ' + content_data['authors'] + ' ' + content_data['average_rating']","6f2b7e66":"content_data = content_data.reset_index()\nindices = pd.Series(content_data.index, index=content_data['original_title'])","ed5ec132":"#removing stopwords\ntfidf = TfidfVectorizer(stop_words='english')\n\n#Construct the required TF-IDF matrix by fitting and transforming the data\ntfidf_matrix = tfidf.fit_transform(content_data['authors'])\n\n#Output the shape of tfidf_matrix\ntfidf_matrix.shape","3e2eaeb8":"cosine_sim_author = linear_kernel(tfidf_matrix, tfidf_matrix)","084f5c70":"def get_recommendations_books(title, cosine_sim=cosine_sim_author):\n    idx = indices[title]\n\n    # Get the pairwsie similarity scores of all books with that book\n    sim_scores = list(enumerate(cosine_sim_author[idx]))\n\n    # Sort the books based on the similarity scores\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n\n    # Get the scores of the 10 most similar books\n    sim_scores = sim_scores[1:11]\n\n    # Get the book indices\n    book_indices = [i[0] for i in sim_scores]\n\n    # Return the top 10 most similar books\n    return list(content_data['original_title'].iloc[book_indices])","3d7edd7e":"def author_book_shows(book):\n    for book in book:\n        print(book)","ef7f3905":"books1 = get_recommendations_books('The Hobbit', cosine_sim_author)\nauthor_book_shows(books1)","918ad429":"books2 =get_recommendations_books('Shadow Kiss', cosine_sim_author)\nauthor_book_shows(books2)","1a6f6f40":"books3 = get_recommendations_books('Harry Potter and the Goblet of Fire', cosine_sim_author)\nauthor_book_shows(books3)","d90436bb":"count = CountVectorizer(stop_words='english')\ncount_matrix = count.fit_transform(content_data['content'])\n\ncosine_sim_content = cosine_similarity(count_matrix, count_matrix)","d68f06c2":"def get_recommendations(title, cosine_sim=cosine_sim_content):\n    idx = indices[title]\n\n    # Get the pairwsie similarity scores of all books with that book\n    sim_scores = list(enumerate(cosine_sim_content[idx]))\n\n    # Sort the books based on the similarity scores\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n\n    # Get the scores of the 10 most similar books\n    sim_scores = sim_scores[1:11]\n\n    # Get the book indices\n    book_indices = [i[0] for i in sim_scores]\n\n    # Return the top 10 most similar books\n    return list(content_data['original_title'].iloc[book_indices])","3483bd3d":"def book_shows(book):\n    for book in book:\n        print(book)","0dc0f138":"books4 = get_recommendations('The Hobbit', cosine_sim_content)\nbook_shows(books4)","a44488a8":"books5 =get_recommendations('Shadow Kiss', cosine_sim_content)\nbook_shows(books5)","46da8a9b":"books6 =get_recommendations('The Two Towers', cosine_sim_content)\nbook_shows(books6)","4f260ff9":"books7 = get_recommendations('Harry Potter and the Goblet of Fire', cosine_sim_content)\nbook_shows(books7)","a4b2ef69":"merge_data = merge_data[:40000]","517b31cc":"book_rating = pd.pivot_table(merge_data, index='user_id', values='rating', columns='book_title', fill_value=0)\nbook_rating","c16aafce":"book_corr = np.corrcoef(book_rating.T)","029a9f41":"book_corr.shape","db9d1e42":"book_list=  list(book_rating)\nbook_titles =[] \nfor i in range(len(book_list)):\n    book_titles.append(book_list[i])","27783881":"def get_recommendation_collabarative(books_list):\n    similar_books = np.zeros(book_corr.shape[0])\n    \n    for book in books_list:    \n        book_index = book_titles.index(book)\n        similar_books += book_corr[book_index] \n    book_preferences = []\n    for i in range(len(book_titles)):\n        book_preferences.append((book_titles[i],similar_books[i]))\n        \n    return sorted(book_preferences, key= lambda x: x[1], reverse=True)","f06bd424":"list_of_books = ['one hundred years of solitude',\n                 'stardust',\n                 'mogs christmas',\n                 'dragonmede',\n                 'twopence to cross the mersey',\n                 'the candywine development']","947d7b2e":"books8 = get_recommendation_collabarative(list_of_books)","581d35e3":"i=0\nn =0\nwhile n < 9:\n    similar_books_to_read= books8[i][0]\n    i += 1\n    if similar_books_to_read in list_of_books:\n        continue\n    else:\n        print(similar_books_to_read)\n        n += 1","a1959520":"2.1.2 Popular Book","b9a31194":"2.2.4 Let's see Rating Distribution","efca3e50":"**make a book list**","06fecdc7":"**This method uses attributes of the content to recommend similar content. It doesn\u2019t have a cold-start problem because it works through attributes or tags of the content, such as book title, authors or rating, so that new book can be recommended right away.**","5a0e7eb2":"# 4. colloaborative Recommendation","4a700d74":"**top similar books collabarative**","5888954f":"**2.1 Good Reads Visualisation**","b4b88bcf":"1.3 **drop unnecessary data**","38034c33":"**Define Recommendation function**","f4a8952d":"**The dataset contains information about book title, authors, publisher, user and their ratings.we have to find out what book you should be reading next ( there are very few free content recommendation systems that suggest books last I checked ), what are the details of every book you have read, create a word cloud from the books you want to read - all possible approaches to explore the dataset.**","0bf6b66c":"# If you found this kernel helpful, please upvote it.","834f3889":"2.2.2 top authors(frequency of book)","a97e33e6":"2.1.1 Top Rated","63369556":"**1.2 Read CSV Data**","5bfde36c":"2.2.1 Top Years of Publishing","e09e0112":"**find correlation b\/w books**","6240dbe1":"**2.3 Wordclouds**","a3d07d0a":"**1.4 Drop Duplicates from all the dataset**","5c76fb77":"**Compute the cosine similarity matrix**","44eedd13":"*We are going to use a simple similarity-based method called cosine similarity*","8d1cb4b2":"2.2.3 Top Books","118aac51":"*The advantage of TF-IDF encoding is that it will weigh a term (a tag for a book in our example) according to the importance of the term within the document: The more frequently the term appears, the larger its weight will be. At the same time, it weighs the item inversely to the frequency of this term across the entire dataset: It will emphasise terms that are relatively rare occurrences in the general dataset but of importance to the specific content at hand. That means that words such as \u2018is\u2019, \u2018are\u2019, \u2018by\u2019 or \u2018a\u2019 which are likely to show up in every book content but aren\u2019t useful for our user-recommendation, will be weighed less than words that are more unique to the content that we are recommending.*","7eab5e2e":"# 3. content based recommondation","f9410b41":"# 1. About Dataset","b6cc7470":"**2.2 Cross Book Visualisation**","22bba223":"2.1.3 Top Popular Authors","06d4e0e4":"2.1.4 Top author(frequencies of books)","7a91e06a":"**In collaborative-filtering items are recommended, for example books, based on how similar your user profile is to other users\u2019, finds the users that are most similar to you and then recommends items that they have shown a preference for. This method suffers from the so-called cold-start problem: If there is a new book, no-one else would\u2019ve yet liked or watched it, so you\u2019re not going to have this in your list of recommended books, even if you\u2019d love it.**","cff61bdc":"**3.2 content based filtering on multiple matrix**","1afda0f8":"**1.1 import Libraries**","01c926e0":"**author wise recommodation**","218b9aad":"![](https:\/\/images.unsplash.com\/photo-1507842217343-583bb7270b66?ixlib=rb-1.2.1&w=1000&q=80)","5661f059":"**3.1 content based recommodation author**","287f1aaa":"**pivot table**","c29cda0f":"# 2. Data Visualisation","d5f3885d":"**1.5 clean the text**"}}