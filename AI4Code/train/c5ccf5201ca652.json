{"cell_type":{"23960e84":"code","42d7faed":"code","85194b06":"code","31d1adb1":"code","7b53d7e5":"code","bc9765e7":"code","016f4d92":"code","59eae945":"code","9d367b69":"code","34fc61d4":"code","e4521cbf":"code","c37506b1":"code","fb3003ae":"code","1240da1d":"code","b4d49aae":"code","5e4fcbbd":"code","d1594b6f":"code","21cac829":"code","c7acb55b":"code","989946a1":"code","b887f747":"code","24d219ce":"code","b42eb5ea":"code","3d335b25":"code","8e8755e5":"code","c1bc8e71":"code","77dc9c47":"code","13dd95a5":"code","74b80b09":"markdown","347562e7":"markdown","5ab9a272":"markdown","a5b22905":"markdown","e95cc0b0":"markdown","3f6245bc":"markdown","ec244c65":"markdown","6ed1b10f":"markdown","0867146b":"markdown","837162f4":"markdown","94512888":"markdown","e6761201":"markdown"},"source":{"23960e84":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","42d7faed":"data = pd.read_csv('\/kaggle\/input\/churn-modelling\/Churn_Modelling.csv')","85194b06":"data.head(2)","31d1adb1":"data.info()","7b53d7e5":"data = data.drop(['CustomerId','Surname','RowNumber'],axis=1)","bc9765e7":"fig,ax = plt.subplots(1,2,figsize=(16,6))\nsns.set(font_scale=1.5)\nsns.countplot(data=data,x='Geography',hue='Exited',ax=ax[0])\nsns.countplot(data=data,x='Gender',hue='Exited',ax=ax[1])\nplt.show()","016f4d92":"data = pd.concat([data,pd.get_dummies(data[['Geography','Gender']])],axis=1).drop(['Gender','Geography'],axis=1)","59eae945":"plt.figure(figsize=(8,6))\nsns.set(font_scale=1)\nsns.heatmap(data.corr(),annot=True,fmt='0.1f',cmap='icefire')\nplt.title('Correlation Matrix - Finding any Collinearity')\nplt.show()","9d367b69":"sns.set(font_scale=1.5)\nsns.countplot(x=data['Exited'])\nplt.show()","34fc61d4":"fig,ax = plt.subplots(1,2,figsize=(16,6))\nsns.set(font_scale=1.5)\nsns.histplot(data=data,x='Age',hue='Exited',ax=ax[0])\nsns.boxplot(data=data,x='Exited',y='Age',ax=ax[1])\nplt.show()","e4521cbf":"sns.set(font_scale=1.5)\nsns.countplot(x=data['NumOfProducts'],hue=data['Exited'])\nplt.show()","c37506b1":"sns.set(font_scale=1.5)\nsns.boxplot(data=data,y='Balance',x='Exited')\nplt.show()","fb3003ae":"sns.set(font_scale=1)\nfig,ax = plt.subplots(1,3,figsize=(20,5))\nsns.kdeplot(data=data,x='Tenure',hue='Exited',ax=ax[0])\nsns.kdeplot(data=data,x='EstimatedSalary',hue='Exited',ax=ax[1])\nsns.kdeplot(data=data,x='CreditScore',hue='Exited',ax=ax[2])\nax[0].title.set_text('Tenure')\nax[1].title.set_text('Estimated Salary')\nax[2].title.set_text('Credit Score')","1240da1d":"from sklearn.model_selection import train_test_split\n\nxtrain,xtest,ytrain,ytest = train_test_split(data.drop('Exited',axis=1),data['Exited'],test_size=0.2,random_state=1)","b4d49aae":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\n\nfeatures = xtrain\ntarget = ytrain\n\nlr = LogisticRegression(random_state=1,class_weight='balanced')\nrfc = RandomForestClassifier(random_state=1,class_weight='balanced')\nxgbc = XGBClassifier(random_state=1,use_label_encoder=False,verbosity=0,tree_method='hist')\n\nlrscores = cross_val_score(lr,features,target,cv=5,scoring='f1')\nrfcscores = cross_val_score(rfc,features,target,cv=5,scoring='f1')\nxgbcscores = cross_val_score(xgbc,features,target,cv=5,scoring='f1')\n\nprint('Logistic Regression f1 Score: {}\\nRandom Forest f1 Score:{}\\nXGBoost Classifier f1 Score:{}'.format(round(np.mean(lrscores),2),round(np.mean(rfcscores),2),round(np.mean(xgbcscores),2)))","5e4fcbbd":"featurecolumns = ['CreditScore', 'Age', 'Balance', 'NumOfProducts',\n       'IsActiveMember', 'EstimatedSalary', 'Geography_Spain',\n       'Geography_Germany', 'Gender_Female']\nfeatures = features[featurecolumns]","d1594b6f":"from sklearn.model_selection import RandomizedSearchCV\nimport timeit\n\n# Random Forest Tuning\ntic=timeit.default_timer()\nparamsrfc = { \n    'n_estimators': [25,50,100,150],\n    'max_features': ['sqrt'],\n    'max_depth' : [4,6,8,10],\n    'criterion' :['gini'],\n    'class_weight' :['balanced']\n}\n\nrsearchcv_rfc = RandomizedSearchCV(rfc,param_distributions=paramsrfc,cv=5,scoring='f1',random_state=1)\nrsearchcv_rfc.fit(features,target)\ntoc=timeit.default_timer()\nprint('Time taken to tune RFC: {}'.format(toc-tic))\n\nbest_paramsrfc = rsearchcv_rfc.best_params_\nbest_scorerfc = rsearchcv_rfc.best_score_\nmodelrfc = rsearchcv_rfc.best_estimator_\n\n# XGBoost Classifier Tuning\ntic=timeit.default_timer()\nparamsxgbc = {\n    \"learning_rate\": [0.001,0.01,0.1,0.3],\n    \"max_depth\":[5,8,10],\n    \"subsample\":[0.5, 0.75, 1.0],\n    \"n_estimators\":[50,150,500],\n    'scale_pos_weight':[1,2,3,4],\n    'objective': ['binary:logistic','reg:logistic']\n    }\n\nrandomsearchcv_xgbc = RandomizedSearchCV(xgbc,param_distributions=paramsxgbc,cv=5,scoring='f1',random_state=1)\nrandomsearchcv_xgbc.fit(features,target)\ntoc=timeit.default_timer()\nprint('Time taken to tune XGBoost: {}'.format(toc-tic))\n\nbest_paramsxgbc = randomsearchcv_xgbc.best_params_\nbest_scorexgbc = randomsearchcv_xgbc.best_score_\nmodelxgbc = randomsearchcv_xgbc.best_estimator_","21cac829":"print('Best rfc model f1 score:',round(best_scorerfc,3),\"\\n\")\nprint('Best rfc parameters')\nfor k,v in best_paramsrfc.items():\n    print(k,\":\",v)\n    \nprint('\\n')   \nprint('Best xgbc model f1 score:',round(best_scorexgbc,3),\"\\n\")\nprint('Best xgbc parameters')\nfor k,v in best_paramsxgbc.items():\n    print(k,\":\",v)","c7acb55b":"rfcimportances = {'Features':features.columns,'Importances':modelrfc.feature_importances_}\nrfcimportances = pd.DataFrame(rfcimportances).sort_values('Importances',ascending=False)\n\nxgbcimportances = {'Features':features.columns,'Importances':modelxgbc.feature_importances_}\nxgbcimportances = pd.DataFrame(xgbcimportances).sort_values('Importances',ascending=False)\n\nfig,ax = plt.subplots(1,2,figsize=(16,6))\nsns.barplot(y=rfcimportances['Features'],x=rfcimportances['Importances'],ax=ax[0])\nsns.barplot(y=xgbcimportances['Features'],x=xgbcimportances['Importances'],ax=ax[1])\nax[0].title.set_text('Random Forest Feature Importances')\nax[1].title.set_text('XGBoost Feature Importances')\nsns.set(font_scale=1)\nplt.tight_layout()\nplt.show()","989946a1":"from sklearn.metrics import precision_recall_curve\n\nsns.set(font_scale=1)\npredictionsproba = modelrfc.predict_proba(xtest[featurecolumns])\npredictionsproba_2 = modelxgbc.predict_proba(xtest[featurecolumns])\nlr.fit(features[featurecolumns],target)\nrfc.fit(features[featurecolumns],target)\nxgbc.fit(features[featurecolumns],target)\npredictionsproba_lr = lr.predict_proba(xtest[featurecolumns])\npredictionsproba_rfc = rfc.predict_proba(xtest[featurecolumns])\npredictionsproba_xgbc = xgbc.predict_proba(xtest[featurecolumns])\nprecision,recall,thresholds = precision_recall_curve(ytest,predictionsproba[:,1])\nprecision_lr,recall_lr,thresholds_lr = precision_recall_curve(ytest,predictionsproba_lr[:,1])\nprecision_rfc,recall_rfc,thresholds_rfc = precision_recall_curve(ytest,predictionsproba_rfc[:,1])\nprecision_xgbc,recall_xgbc,thresholds_xgbc = precision_recall_curve(ytest,predictionsproba_xgbc[:,1])\nprecision_2,recall_2,thresholds_2 = precision_recall_curve(ytest,predictionsproba_2[:,1])\n\nplt.figure(figsize=(12,6))\n#Tuned Models\nsns.lineplot(x=recall,y=precision,label='Random Forest - tuned')\nsns.lineplot(x=recall_2,y=precision_2,label='XGBoostClassifier - tuned')\n#Untuned Models\nsns.lineplot(x=recall_lr,y=precision_lr,label='Logistic Regression - untuned')\nsns.lineplot(x=recall_rfc,y=precision_rfc,label='Random Forest - untuned')\nsns.lineplot(x=recall_xgbc,y=precision_xgbc,label='XGBoostClassifier - untuned')\n\nplt.xlabel('Recall - Of actual posi, how many were correctly predicted')\nplt.ylabel('Precision - Of predicted posi, how many were actual?')\nplt.title('Precision Recall Curve')\nplt.show()","b887f747":"from sklearn.model_selection import learning_curve\n\nfig,ax = plt.subplots(1,2,figsize=(16,6))\nsns.set(font_scale=1.5)\nlearningcurverfc = learning_curve(modelrfc, xtrain, ytrain, cv=5, scoring='f1')\nlearningcurvexgbc = learning_curve(modelxgbc,xtrain,ytrain,cv=5,scoring='f1')\ntrainsizerfc = learningcurverfc[0]\ntrainscorerfc = []\ntestscorerfc = []\ntrainstdrfc = []\nteststdrfc = []\ntrainsizexgbc = learningcurvexgbc[0]\ntrainscorexgbc = []\ntestscorexgbc = []\ntrainstdxgbc = []\nteststdxgbc = []\n\nfor i in range(len(trainsizerfc)):\n    trainscorerfc.append(np.mean(learningcurverfc[1][i]))\n    testscorerfc.append(np.mean(learningcurverfc[2][i]))\n    trainstdrfc.append(np.std(learningcurverfc[1][i]))\n    teststdrfc.append(np.std(learningcurverfc[2][i]))\n    \nfor i in range(len(trainsizexgbc)):\n    trainscorexgbc.append(np.mean(learningcurvexgbc[1][i]))\n    testscorexgbc.append(np.mean(learningcurvexgbc[2][i]))\n    trainstdxgbc.append(np.std(learningcurvexgbc[1][i]))\n    teststdxgbc.append(np.std(learningcurvexgbc[2][i]))\n\nax[0].title.set_text('Learning Curve - Tuned Random Forest')\nax[1].title.set_text('Learning Curve - Tuned XGBoost')\nsns.lineplot(x=trainsizerfc,y=trainscorerfc,ax=ax[0],label='Training Score')\nax[0].fill_between(trainsizerfc,np.array(trainscorerfc)-np.array(trainstdrfc),np.array(trainscorerfc)+np.array(trainstdrfc),alpha=0.2,color='b')\nsns.lineplot(x=trainsizerfc,y=testscorerfc,ax=ax[0],color='g',label='Testing Score')\nax[0].fill_between(trainsizerfc,np.array(testscorerfc)-np.array(teststdrfc),np.array(testscorerfc)+np.array(teststdrfc),alpha=0.2,color='g')\n\nsns.lineplot(x=trainsizexgbc,y=trainscorexgbc,ax=ax[1],label='Training Score')\nax[1].fill_between(trainsizexgbc,np.array(trainscorexgbc)-np.array(trainstdxgbc),np.array(trainscorexgbc)+np.array(trainstdxgbc),alpha=0.2,color='b')\nsns.lineplot(x=trainsizexgbc,y=testscorexgbc,ax=ax[1],color='g',label='Testing Score')\nax[1].fill_between(trainsizexgbc,np.array(testscorexgbc)-np.array(teststdxgbc),np.array(testscorexgbc)+np.array(teststdxgbc),alpha=0.2,color='g')\nfor i in range(2):\n    ax[i].set_xlabel('Training Size')\n    ax[i].set_ylabel('F1 Score')\nplt.tight_layout()\nplt.show()","24d219ce":"predictions = modelrfc.predict(xtest[featurecolumns])\npredictions_2 = modelxgbc.predict(xtest[featurecolumns])\n\nfrom sklearn.metrics import confusion_matrix\nconfusionmatrix = pd.crosstab(ytest,predictions,rownames=['Actual'],colnames=['Predicted'])\nconfusionmatrix2 = pd.crosstab(ytest,predictions_2,rownames=['Actual'],colnames=['Predicted'])\n\nfig,ax = plt.subplots(1,2,figsize=(16,6))\nsns.set(font_scale=1.5)\nsns.heatmap(confusionmatrix,annot=True,fmt='g',cmap='magma',cbar=False,ax=ax[0])\nsns.heatmap(confusionmatrix2,annot=True,fmt='g',cmap='magma',cbar=False,ax=ax[1])\nax[0].title.set_text('Confusion Matrix - Tuned RFC')\nax[1].title.set_text('Confusion Matrix - Tuned XGBoost')\nplt.show()","b42eb5ea":"truepositive = confusionmatrix[1][1]\ntruenegative = confusionmatrix[0][0]\nfalsepositive = confusionmatrix[1][0]\nfalsenegative = confusionmatrix[0][1]\n\n# precision = tp\/(tp+fp)   <-- Of those predicted positive, how many were actually positive?\n# recall = tp\/(tp+fn)      <-- Of those actual positives, how many were correctly predicted positive?\n\nprecision = truepositive\/(truepositive+falsepositive)\nrecall = truepositive\/(truepositive+falsenegative)\n\nprint('Precision:{} Recall:{}'.format(round(precision,3),round(recall,3)))","3d335b25":"from sklearn.metrics import classification_report\nprint(classification_report(ytest,predictions,digits=3))","8e8755e5":"# SHAP summary of Random Forest\nimport shap\nexplainerrfc = shap.TreeExplainer(modelrfc,model_output='raw')\nshap_valuesrfc = explainerrfc.shap_values(xtest[featurecolumns])\nshap.summary_plot(shap_valuesrfc[1], xtest[featurecolumns])","c1bc8e71":"# SHAP summary of XGBoost\nimport shap\nexplainerxgbc = shap.TreeExplainer(modelxgbc)\nshap_valuesxgbc = explainerxgbc.shap_values(xtest[featurecolumns])\nshap.summary_plot(shap_valuesxgbc, xtest[featurecolumns])","77dc9c47":"for i in featurecolumns:\n    fig,ax = plt.subplots(1,2,figsize=(16,6))\n    shap.dependence_plot(i,shap_valuesrfc[1],xtest[featurecolumns],ax=ax[0],show=False)\n    shap.dependence_plot(i,shap_valuesxgbc,xtest[featurecolumns],ax=ax[1],show=False)\n    ax[0].set_title('Random Forest SHAP Dependence - Proba')\n    ax[1].set_title('XGBoost SHAP Dependence - Log Odds')\n    plt.tight_layout()\n    plt.show()","13dd95a5":"# Prediction for a specific customer and the local feature importances (how each feature impacts prediction away from baseline 0.5)\nprediction = modelrfc.predict_proba(xtest[featurecolumns].iloc[199].values.reshape(1,-1))\nprint('Probability of Churn for 200th customer: {}'.format(prediction[0][1]))\nshap.initjs()\nshap_values = explainerrfc.shap_values(xtest[featurecolumns].iloc[199])\nshap.force_plot(explainerrfc.expected_value[1],shap_values[1],xtest[featurecolumns].iloc[199])","74b80b09":"#### Because the targets are imbalanced (exited=0 occurs 4 times more than exited=1), I will be balancing the classes in the model instantiations","347562e7":"### Modelling - Will compare the performance of Logistic Regression and Random Forest, once I've studied and understood XGBoost I will include this also","5ab9a272":"#### From looking at the Random Forest learning curve of the model it would beneficial to obtain more data to improve model performance","a5b22905":"#### There is some collinearity here, we'll note to remove one of the genders as this is a binary feature in the dataset anyway","e95cc0b0":"### Interpreting the confusion matrix\n#### **The model predicted 430 customers churning, of those, 265 did and 165 stayed.** Thought experiment : if we used this model to predict churn and entice customers before they leave, what would be the cost of applying product discounts, offers etc on those 165 customers who didn't churn.\n#### **Conversely, of the 415 customers that actually churned, 150 were predicted to stay.** What would be the cost of losing these customers?","3f6245bc":"#### By tuning the model, the f1 score has improved from 0.55 to 0.62!","ec244c65":"### Onto model tuning, I will use gridsearchCV to find the best combination of hyperparameters from a selection provided and use the f1 score as a measure of accuracy. The GridSearchCV function in Sklearn will return the best set of hyperparameters, the best score and the corresponding estimator(model) with these hyperparameters","6ed1b10f":"### Lets first remove features that will provide no predictive power, e.g names, Ids, unqiue numbers etc","0867146b":"### Looks like RandomForest performs better than Logistic Regression on this data, before moving onto tuning our Random Forest lets first choose the best features in predicting whether a customer will exit(churn)","837162f4":"### Customers who end up exiting(churning) tend to be older","94512888":"#### There is an imbalance in the dataset where customers are 4 times more likely to not exit (churn), we will need to account for this when building models especially if we want to prioritise capturing those customers who will churn. However, in doing so we will also create a higher number of false positives. This would usually be a decision made by the business\/user in weighing the cost of running promos etc to retain a false positive (ie. a customer who wasn't going to exit\/churn) vs the cost of lost revenue if that customer does leave","e6761201":"#### Exploring SHAP as a method of understanding the model and local feature importance"}}