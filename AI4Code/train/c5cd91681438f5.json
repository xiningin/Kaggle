{"cell_type":{"998d38b7":"code","fc4a513f":"code","e8e1aa21":"code","64854bc6":"code","45266c31":"code","a768e616":"code","b542776b":"code","77e31a60":"code","ab53cc83":"code","68ff0f52":"code","3fd1a4e6":"code","7fe7a66c":"code","ea8d6cd6":"code","ec106292":"code","24c59804":"markdown","342eeb7a":"markdown","8aaad727":"markdown","590916bf":"markdown","9fa65c05":"markdown","04add7e3":"markdown"},"source":{"998d38b7":"# Quick load dataset and check\nimport pandas as pd\nimport sklearn\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas.testing as tm\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nimport sklearn.naive_bayes as nb\nfrom sklearn.neural_network import MLPClassifier\nfrom imblearn.over_sampling import SMOTE\nimport math\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_regression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree","fc4a513f":"filename = \"train_set.csv\"\ndata_train = pd.read_csv(filename)\nfilename = \"test_set.csv\"\ndata_test = pd.read_csv(filename)\n\ndata_train.describe()","e8e1aa21":"## Column\ndef get_all_elements(data, noprint=False):\n    cols = data.columns[1:]\n    thisdict = dict(zip(cols,list(map(lambda x: list(set(x)),\\\n                                      data.values[:,1:].T))))\n    \n    if noprint:\n        return thisdict\n    np.set_printoptions(threshold=20)\n    np.set_printoptions(threshold=None)\n    return thisdict","64854bc6":"def find_missing(data, dict_):\n    missing_features = [ele[0] for ele in list(dict_.items()) if -1 in ele[1]]\n    [missing_features.append(ele[0]) for ele in list(dict_.items()) if np.nan in ele[1]]\n    return missing_features","45266c31":"def replace_missing(bad_features, data):\n    feature_dict = {}\n    for feature in bad_features:\n        data, most_related = replace_missing_correlation(feature, data)\n        feature_dict[feature] = most_related\n        \n    dict_train = get_all_elements(data)\n    bad_features = find_missing(data, dict_train)\n    imputer = SimpleImputer(missing_values=-1,strategy='median')\n    data = pd.DataFrame(imputer.fit_transform(data),\n                        index = data.index,\n                        columns = data.columns)\n    return data, feature_dict\n\ndef replace_missing_correlation(feature, data, related_feature=None):\n    if related_feature == None:\n        df_all_corr = data.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\n        df_all_corr.rename(columns={\"level_0\": \"Feature 1\", \n                                    \"level_1\": \"Feature 2\", \n                                    0: 'Correlation Coefficient'}, \n                           inplace=True)\n        most_related = df_all_corr[df_all_corr['Feature 1'] == feature]['Feature 2'].iloc[1]\n        pd.set_option('display.max_rows', 10)\n        df_all_corr[df_all_corr['Feature 1'] == feature]\n    else:\n        most_related = related_feature\n    data.loc[:, feature] = data[feature].replace(-1, np.nan)\n    data.loc[:, feature] = data.groupby([most_related])[feature].apply(lambda x: x.fillna(x.mean()))\n    data.loc[:, feature] = data[feature].replace(np.nan, -1)\n    return data, most_related","a768e616":"def hot_encoding(data, feature):\n    print(data.shape)\n    data = pd.get_dummies(data, columns=[feature], prefix=[feature])\n    print(data.shape)\n    return data","b542776b":"def extrac_one_label(x_val, y_val, label):\n    X_pos = x_val[y_val == label]\n    y_pos = y_val[y_val == label]\n    return X_pos, y_pos\n\ndef test_results(clf, x_val, y_val):\n    X_pos, y_pos = extrac_one_label(x_val, y_val, 0)\n    print(\"Score 0:\", clf.score(X_pos, y_pos))\n    X_pos, y_pos = extrac_one_label(x_val, y_val, 1)\n    print(\"Score 1:\", clf.score(X_pos, y_pos))\n    return 0","77e31a60":"def pca_transform(data, n=40):\n    pca = PCA(n_components=n)\n    pca.fit(data)\n    data_new = pca.transform(data)\n    print(data_new.shape)\n    return data_new, pca\n\ndef feature_target_corr(x_data, y_data):\n    df_correllations = x_data.corrwith(y_data).abs()\n    print(df_correllations.sort_values()[0:20])\n    return df_correllations.sort_values()\n\ndef cat_feature_encoding(data, dict_):\n    cat_features = [x for x in list(dict_.keys()) if x[-3:]==\"cat\"]\n    for feature in cat_features:\n        print(feature)\n        data = hot_encoding(data, feature)\n    return data\n\ndef over_sampling(data):\n    shuffled_df = data.sample(frac=1, random_state=42)\n    pos_df = shuffled_df.loc[shuffled_df['target'] == 1]\n    neg_df = shuffled_df.loc[shuffled_df['target'] == 0]\n    multi = math.ceil(len(neg_df)\/len(pos_df))\n    pos_df_over = pd.concat([pos_df]*multi).sample(n=len(neg_df))\n    over_df = pd.concat([pos_df_over, neg_df])\n    return over_df","ab53cc83":"data_train = pd.read_csv(\"train_set.csv\")\n\n# replace missing features\ndict_train = get_all_elements(data_train)\nbad_features = find_missing(data_train, dict_train)\ndata_train, corr_features = replace_missing(bad_features, data_train)\n\n# oversampling\ndata_train = over_sampling(data_train)\n\n# split into x- and y-data\ndata_X = data_train[data_train.columns[2:]]\ndata_Y = data_train[\"target\"]\n\n# encode categorical features\n#data_X = cat_feature_encoding(data_X, dict_train)\n\n# split data into training- and test-data\nx_train, x_val, y_train, y_val = train_test_split(data_X, data_Y,\n                                                  test_size = 0.2, shuffle = True)\n\n# encode features to have mean 0 and standard deviation of 1\nscaler = StandardScaler()\nscaler.fit(x_train)\nx_train = pd.DataFrame(scaler.transform(x_train),\n                       index = x_train.index,\n                       columns = x_train.columns)\nx_val = pd.DataFrame(scaler.transform(x_val),\n                     index = x_val.index,\n                     columns = x_val.columns)\n\n# select n features using KBest\n#selector = SelectKBest(f_classif, k=40)\n#selector.fit(data_X, data_Y)\n#data_X_new = selector.transform(data_X)\n\n#for corr_len in [10, 12, 14, 16, 18, 20]:\n#for alpha in [10**-2, 10**-3, 10**-4, 10**-5, 10**-6, 10**-7]:\ncorr_cols = feature_target_corr(x_train, y_train)\ndata_X_new = x_train.drop(corr_cols.index.values[0:10], axis=1)\nx_val_new = x_val.drop(corr_cols.index.values[0:10], axis=1)\n\n#data_X_new, pca = pca_transform(x_train, 40)\n#x_val_new = pca.transform(x_val)\n\n\nclf = MLPClassifier(hidden_layer_sizes=(120), solver=\"adam\", alpha=10**-6, random_state=1, activation=\"logistic\",\n                    verbose=False, max_iter=500)\n\n#clf = SGDClassifier(tol=1e-6, loss=\"log\")\n#clf = DecisionTreeClassifier( )\n#clf = CategoricalNB()\n#clf = GaussianNB()\n\n\nclf.fit(data_X_new, y_train)\n\nprint(\"Accuracy: \",clf.score(x_val_new, y_val))\ntest_results(clf, x_val_new, y_val)\nprint(\"F-score: \",f1_score(y_val, clf.predict(x_val_new), average=\"macro\"))\n","68ff0f52":"# read test_data\ndata_test = pd.read_csv(\"test_set.csv\")\nids = data_test[\"id\"]\ndata_test = data_test.drop(columns=['id'])\n\n# calculate and replace missing features\ndict_test = get_all_elements(data_test)\nbad_features = find_missing(data_test, dict_test)\nfor feature in bad_features:\n    data_test, missing_feature = replace_missing_correlation(feature, data_test, corr_features[feature])\n\n# apply standard scaling\ndata_test_X = pd.DataFrame(scaler.transform(data_test),\n                           index = data_test.index,\n                           columns = data_test.columns)\n\n#data_test_X_new = pca.transform(data_test_X)\n\n# remove features with low correlation\ndata_test_X_new = data_test_X.drop(corr_cols.index.values[0:10], axis=1)\n\nprint(data_test_X_new.shape)\n\ny_target = clf.predict(data_test_X_new)\nsum(y_target==0)","3fd1a4e6":"data_test.columns","7fe7a66c":"print(data_test_X_new.shape)\n\ny_target = clf.predict(data_test_X_new)\nsum(y_target==0)","ea8d6cd6":"data_test = pd.read_csv(\"test_set.csv\")\n\ndata_out = pd.DataFrame(data_test['id'].copy())\ndata_out.insert(1, \"target\", y_target, True)\ndata_out = data_out.astype({\"target\": int})\ndata_out.to_csv('submission.csv',index=False)","ec106292":"data_out","24c59804":"### Hot Encoding","342eeb7a":"### Neural Network","8aaad727":"### Features and unique values\n\nPrint out features and unique values","590916bf":"### Output generation","9fa65c05":"### Score 0 and Score 1","04add7e3":"### Missing values"}}