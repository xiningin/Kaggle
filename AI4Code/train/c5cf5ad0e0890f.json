{"cell_type":{"afeec549":"code","734bd9bf":"code","3eb876c6":"code","f4adde17":"code","89e8f26e":"code","886ee168":"code","8e0682c8":"code","f6064815":"code","8b2313f0":"code","c43e1574":"code","c877aa0a":"code","ae22fe0c":"code","b61a3f74":"code","44cc2e1f":"code","44a55e59":"code","533524bf":"code","b920ab1a":"code","e49564f1":"markdown","c6407afc":"markdown","c1a28c60":"markdown","f4e6cea7":"markdown","27082607":"markdown","1efd3af8":"markdown","902dfa28":"markdown","9730ae6f":"markdown","14827763":"markdown","25d65e4c":"markdown","646e4af9":"markdown","9bb665f9":"markdown","9fcb440a":"markdown","02b705e0":"markdown","14e15b86":"markdown","ccd19fe9":"markdown"},"source":{"afeec549":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns\n\nfrom sklearn.model_selection import *\nfrom sklearn.feature_selection import *\nfrom sklearn.metrics import *\nfrom sklearn.preprocessing import *\nfrom sklearn.ensemble import *\nfrom sklearn.decomposition import PCA\nfrom sklearn.naive_bayes import BernoulliNB\n\n# this code help in displaying complete block of data rather than ... in the columns\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\nFOLDS = 5","734bd9bf":"data = pd.read_csv('..\/input\/company-bankruptcy-prediction\/data.csv')\ndata.head()","3eb876c6":"data.describe()","f4adde17":"data.hist(figsize=(40,60))\nplt.show()","89e8f26e":"data.corrwith(data['Bankrupt?'])","886ee168":"data.isna().sum()","8e0682c8":"def corr_skew(X):\n    s = X.skew().reset_index().rename(columns = {0:'skew'})\n\n    pos = list(s[s['skew']>=1]['index'].values)\n    neg = list(s[s['skew']<=-1]['index'].values)\n\n    X[pos] = (X[pos]+1).apply(np.log)\n    X[neg] = (X[neg])**3\n    return X","f6064815":"y  = data['Bankrupt?']\nX  = data.drop(['Bankrupt?'],axis=1)\nX = corr_skew(X)","8b2313f0":"def plot_PCA(X,Y):\n    \n    pca = PCA()\n    X = pca.fit_transform(X)\n\n    # 2D plot\n    plt.scatter(X[:,0],X[:,1],c=y,cmap=ListedColormap(['b','r']))\n    plt.show()    \n\n    # 3D plot\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection = '3d')\n    ax.scatter(X[:,0],X[:,1],X[:,2],c=y,cmap=ListedColormap(['b','r']))\n    plt.show()","c43e1574":"plot_PCA(X,y)","c877aa0a":"def pred_stratified(X,y):\n    X = X.values\n    y = y.values\n\n    skf = StratifiedKFold(n_splits=FOLDS)\n    aucs = []\n    fig, ax = plt.subplots()\n\n    for train_index, test_index in skf.split(X, y):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n        scaler = StandardScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_test = scaler.transform(X_test)\n\n        model = BernoulliNB(alpha = 10)\n        model.fit(X_train,y_train)\n\n        yxgb = model.predict(X_test)\n        plot_roc_curve(model, X_test, y_test, ax=ax)\n        aucs.append(roc_auc_score(y_true=y_test,y_score=yxgb))\n    plt.show()      \n    return sum(aucs)\/5","ae22fe0c":"pred_stratified(X,y)","b61a3f74":"sel = SelectFromModel(RandomForestClassifier(random_state=42))\n\ny  = data['Bankrupt?']\nX  = data.drop(['Bankrupt?'],axis=1)\nX = corr_skew(X)\nsel.fit(X,y)","44cc2e1f":"features = X.columns[(sel.get_support())]\nprint(len(features))\nfeatures","44a55e59":"X = X.filter(items=features)","533524bf":"plot_PCA(X,y)","b920ab1a":"pred_stratified(X,y)","e49564f1":"If you observe carefully most of the columns are skewed.  \nThis means many of the data points lie towards it's minimum or maximum.  \nFor Example, observe **Operating Expense Rate** and **Research and development expense rate**.  \nThis means model will not be able distuingish well between yes and no as most value lie towards one end.  \nHence we'll learn about how to reduce skewness.  ","c6407afc":"# Data Plots\nPandas itself support many of the matplotlib functionalities, and we'll use the same.","c1a28c60":"I've used Random forest to select columns based on it's importance. You can try with other classifeir as well.  ","f4e6cea7":"Correlation means how directly or indirectly our column effects another column.  \n`+1` means direct, i.e increase in column will lead increase in label value.  \n`-1` for just the opposite.","27082607":"As we observe, there are no missing values in the dataset.","1efd3af8":"The above function finds the skewness of each column.  \nSkewness greater than 1 is corrected ny using `log`, where as less than -1 is corrected using `cube`.  \n**Note in log I've used +1 to prevent log of 0 if any**","902dfa28":"The above function reduces are dataset to help us plot.  \nThis gives us a fairly good idea how well our model be able to fit it.  ","9730ae6f":"# Bankruptcy Prediction\nWhy is it necessary?  \nPredicting certain public domain information will help you invest wisely, or help you decide weather to trust company or not.  \nAlso this is advantageous to Banks who can prevent losses by avoinding loan to these companies.  \nThis data however is limited to certain companies and doesnot represent complete dataset.  0\n\n**In this notebook**, I'll demonstrate a rather simple model based on Naive Bayes.  \nFor those of who unaware of tha concept, do read about it as it is very simple and is based on high school probabilty concepts.","14827763":"# Understanding Data","25d65e4c":"# Importing Data","646e4af9":"Let's break the function above.  \n1. Stratified K fold:  \nYou might have use train test split previously. It randomly splits the data into two parts.  \nHowever Stratified Sampling helps in our test set being similar to train, i.e ratio of each target label is same in train in test.  \nThis method is an important sampling technique and a good practice to implement.\n2. Folds:\nTo observe performance we sample using stratified method 5 times and average it get an average result. \n3. Scaling:\nThis is done to normalise the dast","9bb665f9":"As observed we've obtained similar result on reduced dataset as well with only 1\/3 the columns.  \nThis increases our effeciency and reduces time on training and predicting.  \nI hope you learned from this notebook.  \n### Happy Learning","9fcb440a":"# Visualise data and labels","02b705e0":"# Model Training","14e15b86":"# Feature Reduction\nAs we see, the train data has 90+ features. This makes training costly.  \nMoreover many of these columns have same correlation that points towards multi collinearity.  \nA simple way to understand is, two columns have exatly same influence on target, so why not drop 1 and multiply other by 2??.  \n**Multiplication by 2 is not actually done, however is reflected automatically in the equation my the co-effecients**.  \nThis means we can achieve similar score even with a reduced number of columns. ","ccd19fe9":"# Visualise Data and Labels on reduced data"}}