{"cell_type":{"59ccab73":"code","9fbe7541":"code","5a9633b0":"code","c2aa597b":"code","0811ee2d":"code","8d77ab5d":"code","a0dace1b":"code","f1605318":"code","92a31f85":"code","5e899365":"code","641b7a6d":"code","c7ecc08f":"code","13841104":"code","4d312c45":"code","fd6bd996":"code","5a69dbe7":"code","eb0ac31c":"code","3fa8daa0":"code","5476c560":"code","cf96067c":"code","ede8442d":"code","4a15c2d4":"code","999d5031":"code","553d735d":"code","118c6d4c":"code","4fe7057b":"code","f0204a4f":"code","e971243c":"code","c9aa5f52":"code","1d6f7bf2":"code","35228236":"code","5804631f":"code","8b855191":"code","2b236419":"code","eb66b915":"code","fb04a058":"code","732178a9":"code","f1415ff2":"code","959f46dc":"code","4ef5f023":"code","d890220b":"code","9dcef32e":"code","809a7994":"code","869dea0b":"code","2f0fad1a":"code","a93a8d8d":"code","24f524de":"code","e887ca12":"code","19349827":"code","56f57dde":"code","3e6623e1":"code","781306a8":"code","be4dd6db":"code","e9e5a1b8":"code","97f96959":"code","1e8645f7":"code","bcbd2456":"code","d4488801":"code","7c45b8be":"code","2bf7cceb":"code","b906069a":"code","47e1ce4f":"code","89bdd6c4":"code","192c12cd":"code","e1e6497b":"code","ecc90132":"code","1d545223":"code","4c540c5c":"code","a5a34bfe":"code","856f58e6":"code","9b933de9":"code","fdced544":"code","407b990c":"code","67354a83":"code","a8926281":"code","4098d77d":"code","be6510ea":"markdown","a6fec526":"markdown","1be3631d":"markdown","095f556d":"markdown","898f380d":"markdown","62ea8152":"markdown","d303c6aa":"markdown","04f7cf7d":"markdown","12ec6222":"markdown","620aa41d":"markdown","f426e53b":"markdown","a16e4daa":"markdown","e28648b9":"markdown","5ad82a50":"markdown","eac5edb5":"markdown","26c168da":"markdown","f4936ee1":"markdown","d74f9028":"markdown","353029f5":"markdown","44ad9375":"markdown","49bd1ad5":"markdown","7540447d":"markdown","e169d23a":"markdown","29ba683f":"markdown","273deb05":"markdown","1b427b7b":"markdown","5452d112":"markdown"},"source":{"59ccab73":"import pandas as pd\nimport numpy as np\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns","9fbe7541":"# data = pd.read_csv('Data\/train.csv',index_col='ID_code')\ndata = pd.read_csv('\/kaggle\/input\/santander-customer-transaction-prediction\/train.csv')\n\n# data.set_index('ID_code',inplace=True)\ndata.drop('ID_code',axis=1,inplace=True)","5a9633b0":"data = data.sample(100)","c2aa597b":"fig, axs = plt.subplots(1,10,figsize=(30,5))\nfig.subplots_adjust(hspace = .5, wspace=.5)\naxs = axs.ravel()\n\ncounter=0\nfor i in data.columns[1:10]:\n    \n    axs[counter].set_title(i)\n#     axs[counter].hist(data[i])\n    sns.distplot(data[i],ax=axs[counter])\n    axs[counter].set_ylabel(\"Distribution\")\n#     plt.xticks(rotation=90)\n    axs[counter].set_xticklabels(data[i].unique(),rotation=90)\n    counter+=1  ","0811ee2d":"fig, axs = plt.subplots(1,10,figsize=(30,5))\nfig.subplots_adjust(hspace = .5, wspace=.5)\naxs = axs.ravel()\ncounter=0\n\nfor i in data.columns[1:10]:\n\n    \n    data_pivot = data[['target']]\n    data_pivot[i] = pd.qcut(data[i],q=10,duplicates='drop')\n    data_plot = pd.pivot_table(data_pivot,'target',i)\n\n    plt.title(i)\n    sns.barplot(data_plot.index,'target',data=data_plot,ax=axs[counter])\n    plt.ylabel(\"Percentage of Approval\")\n    plt.xticks(rotation=90)\n\n    counter+=1   ","8d77ab5d":"from sklearn.preprocessing import MinMaxScaler\n\nX = data.drop('target',axis=1)\ny = data.target\n\nmms = MinMaxScaler()\nX = pd.DataFrame(mms.fit_transform(X),columns=X.columns)\n\ndata_plot = pd.concat([X,y],axis=1)","a0dace1b":"fig, axs = plt.subplots(1,10,figsize=(30,5))\nfig.subplots_adjust(hspace = .5, wspace=.5)\naxs = axs.ravel()\n\ncounter=0\nfor i in data.columns[1:10]:\n    \n    axs[counter].set_title(i)\n#     axs[counter].hist(data[i])\n    sns.distplot(data_plot[i],ax=axs[counter])\n    axs[counter].set_ylabel(\"Distribution\")\n#     plt.xticks(rotation=90)\n    axs[counter].set_xticklabels(data_plot[i].unique(),rotation=90)\n    counter+=1  ","f1605318":"# data = pd.read_csv('Data\/train.csv',index_col='ID_code')\ndata = pd.read_csv('\/kaggle\/input\/santander-customer-transaction-prediction\/train.csv')\n\n# data.set_index('ID_code',inplace=True)\ndata.drop('ID_code',axis=1,inplace=True)","92a31f85":"X = data.drop('target',axis=1)\ny = data.target","5e899365":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.33, random_state=42)","641b7a6d":"from sklearn.linear_model import LogisticRegression\n\nlogreg=LogisticRegression(max_iter=300)\nlogreg.fit(X_train,y_train.values.ravel())\ny_pred = logreg.predict(X_test)\n\nprint('Accuracy of logistic regression classifier on test set: {:.5f}'.format(logreg.score(X_test, y_test)))","c7ecc08f":"from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n\ncm = confusion_matrix(y_test, y_pred) # rows = truth, cols = prediction\ndf_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\n\nplt.figure(figsize = (5,4))\nsns.set(font_scale=1.4)\n\nsns.heatmap(df_cm, annot=True, fmt='g', cmap=\"Blues\")\nprint(\"Test Data Accuracy: %0.4f\" % accuracy_score(y_test, y_pred))","13841104":"print(classification_report(y_test, y_pred))","4d312c45":"from sklearn.preprocessing import MinMaxScaler\n\nX = data.drop('target',axis=1)\ny = data.target\n\nmms = MinMaxScaler()\nX = pd.DataFrame(mms.fit_transform(X),columns=X.columns)","fd6bd996":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.33, random_state=42)","5a69dbe7":"from sklearn.linear_model import LogisticRegression\n\nlogreg=LogisticRegression(max_iter=300)\nlogreg.fit(X_train,y_train.values.ravel())\ny_pred = logreg.predict(X_test)\n\nprint('Accuracy of logistic regression classifier on test set: {:.5f}'.format(logreg.score(X_test, y_test)))","eb0ac31c":"from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n\ncm = confusion_matrix(y_test, y_pred) # rows = truth, cols = prediction\ndf_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\n\nplt.figure(figsize = (5,4))\nsns.set(font_scale=1.4)\n\nsns.heatmap(df_cm, annot=True, fmt='g', cmap=\"Blues\")\nprint(\"Test Data Accuracy: %0.4f\" % accuracy_score(y_test, y_pred))","3fa8daa0":"print(classification_report(y_test, y_pred))","5476c560":"data.target.value_counts()","cf96067c":"from imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\nfrom collections import Counter\n\n\nunder = RandomUnderSampler(sampling_strategy=0.25)\nover = SMOTE(sampling_strategy=0.5)\n\nX = data.drop('target',axis=1)\ny = data.target\n","ede8442d":"X,y = under.fit_resample(X,y)","4a15c2d4":"y.value_counts()","999d5031":"X,y = over.fit_resample(X,y)","553d735d":"y.value_counts()","118c6d4c":"# summarize the new class distribution\ncounter = Counter(y)\nprint(counter)\n","4fe7057b":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.33, random_state=42)","f0204a4f":"from sklearn.linear_model import LogisticRegression\n\nlogreg2=LogisticRegression(max_iter=300)\nlogreg2.fit(X_train,y_train.values.ravel())\ny_pred = logreg2.predict(X_test)\n\nprint('Accuracy of logistic regression classifier on test set: {:.5f}'.format(logreg2.score(X_test, y_test)))","e971243c":"from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n\ncm = confusion_matrix(y_test, y_pred) # rows = truth, cols = prediction\ndf_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\n\nplt.figure(figsize = (5,4))\nsns.set(font_scale=1.4)\n\nsns.heatmap(df_cm, annot=True, fmt='g', cmap=\"Blues\")\nprint(\"Test Data Accuracy: %0.4f\" % accuracy_score(y_test, y_pred))","c9aa5f52":"print(classification_report(y_test, y_pred))","1d6f7bf2":"from sklearn.preprocessing import MinMaxScaler\n\nX = data.drop('target',axis=1)\ny = data.target\n\nmms = MinMaxScaler()\nX = pd.DataFrame(mms.fit_transform(X),columns=X.columns)","35228236":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.33, random_state=42)","5804631f":"import statsmodels.api  as sm\n\nlogit_model2=sm.Logit(y_train,X_train)\nresult=logit_model2.fit()\nprint(result.summary2())","8b855191":"X_train_negd = X_train.drop(['var_7','var_10','var_17','var_29','var_30','var_27',\n                             'var_38','var_39','var_41','var_73','var_98','var_61',\n                             'var_100','var_103','var_136','var_153','var_158',\n                             'var_161',\n                             ],axis=1)","2b236419":"X_test_negd = X_test.drop(['var_7','var_10','var_17','var_29','var_30','var_27',\n                            'var_38','var_39','var_41','var_73','var_98','var_61',\n                            'var_100','var_103','var_136','var_153','var_158',\n                            'var_161',\n                             ],axis=1)","eb66b915":"from sklearn.linear_model import LogisticRegression\n\nlogreg=LogisticRegression(max_iter=300)\nlogreg.fit(X_train_negd,y_train.values.ravel())\ny_pred = logreg.predict(X_test_negd)\n\nprint('Accuracy of logistic regression classifier on test set: {:.5f}'.format(logreg.score(X_test_negd, y_test)))","fb04a058":"from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n\ncm = confusion_matrix(y_test, y_pred) # rows = truth, cols = prediction\ndf_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\n\nplt.figure(figsize = (5,4))\nsns.set(font_scale=1.4)\n\nsns.heatmap(df_cm, annot=True, fmt='g', cmap=\"Blues\")\nprint(\"Test Data Accuracy: %0.4f\" % accuracy_score(y_test, y_pred))","732178a9":"print(classification_report(y_test, y_pred))","f1415ff2":"X_train_negd = X_train.drop(['var_7','var_10','var_17','var_29','var_30','var_27',\n                             'var_38','var_39','var_41','var_73','var_98','var_61',\n                             'var_100','var_103','var_136','var_153','var_158',\n                             'var_161','var_183',\n                             'var_16','var_37','var_46','var_65','var_69','var_79',\n                             'var_96','var_117','var_124','var_126','var_185','var_189',\n                             'var_61','var_60','var_47'                             \n                             ],axis=1)","959f46dc":"X_test_negd = X_test.drop(['var_7','var_10','var_17','var_29','var_30','var_27',\n                            'var_38','var_39','var_41','var_73','var_98','var_61',\n                            'var_100','var_103','var_136','var_153','var_158',\n                            'var_161','var_183',\n                           'var_16','var_37','var_46','var_65','var_69','var_79',\n                             'var_96','var_117','var_124','var_126','var_185','var_189',\n                           'var_61','var_60','var_47'\n                             ],axis=1)","4ef5f023":"from sklearn.linear_model import LogisticRegression\n\nlogreg=LogisticRegression(max_iter=300)\nlogreg.fit(X_train_negd,y_train.values.ravel())\ny_pred = logreg.predict(X_test_negd)\n\nprint('Accuracy of logistic regression classifier on test set: {:.5f}'.format(logreg.score(X_test_negd, y_test)))","d890220b":"from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n\ncm = confusion_matrix(y_test, y_pred) # rows = truth, cols = prediction\ndf_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\n\nplt.figure(figsize = (5,4))\nsns.set(font_scale=1.4)\n\nsns.heatmap(df_cm, annot=True, fmt='g', cmap=\"Blues\")\nprint(\"Test Data Accuracy: %0.4f\" % accuracy_score(y_test, y_pred))","9dcef32e":"print(classification_report(y_test, y_pred))","809a7994":"# Recursive Feature Elimination\nfrom sklearn.feature_selection import RFE\n\nlogreg = LogisticRegression(max_iter=4000)\n\n# Select Best X Features\nrfe = RFE(logreg, n_features_to_select=None)\nrfe = rfe.fit(X_train, y_train.values.ravel())","869dea0b":"print(rfe.support_)\nprint(rfe.ranking_)","2f0fad1a":"X_train_rfe = X_train[X_train.columns[rfe.support_]]\nX_test_rfe = X_test[X_train.columns[rfe.support_]]","a93a8d8d":"from sklearn.linear_model import LogisticRegression\n\nlogreg=LogisticRegression(max_iter=300)\nlogreg.fit(X_train_rfe,y_train.values.ravel())\ny_pred = logreg.predict(X_test_rfe)\n\nprint('Accuracy of logistic regression classifier on test set: {:.5f}'.format(logreg.score(X_test_rfe, y_test)))","24f524de":"from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n\ncm = confusion_matrix(y_test, y_pred) # rows = truth, cols = prediction\ndf_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\n\nplt.figure(figsize = (5,4))\nsns.set(font_scale=1.4)\n\nsns.heatmap(df_cm, annot=True, fmt='g', cmap=\"Blues\")\nprint(\"Test Data Accuracy: %0.4f\" % accuracy_score(y_test, y_pred))","e887ca12":"print(classification_report(y_test, y_pred))","19349827":"ratio = y[y==1].shape[0]\/y[y==0].shape[0]\nindex_1 = y[y==1].index\nindex_0 = y[y==0].index","56f57dde":"from numpy import random\n\nindex_0_sel = random.choice(index_1, size = 20000)\nindex_1_sel = random.choice(index_0, size = int(20000*ratio))\nindex_sel = np.concatenate((index_0_sel,index_1_sel))\n\n\nX_sel = X.iloc[index_sel]\ny_sel = y.iloc[index_sel]","3e6623e1":"X_train,X_test,y_train,y_test = train_test_split(X_sel, y_sel, test_size=0.33, random_state=42)","781306a8":"X_train = X_train.drop(['var_7','var_10','var_17','var_29','var_30','var_27',\n                             'var_38','var_39','var_41','var_73','var_98','var_61',\n                             'var_100','var_103','var_136','var_153','var_158',\n                             'var_161','var_183',\n                             'var_16','var_37','var_46','var_65','var_69','var_79',\n                             'var_96','var_117','var_124','var_126','var_185','var_189',\n                             'var_61','var_60','var_47'                             \n                             ],axis=1)","be4dd6db":"X_test = X_test.drop(['var_7','var_10','var_17','var_29','var_30','var_27',\n                            'var_38','var_39','var_41','var_73','var_98','var_61',\n                            'var_100','var_103','var_136','var_153','var_158',\n                            'var_161','var_183',\n                           'var_16','var_37','var_46','var_65','var_69','var_79',\n                             'var_96','var_117','var_124','var_126','var_185','var_189',\n                           'var_61','var_60','var_47'\n                             ],axis=1)","e9e5a1b8":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","97f96959":"## Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0, penalty = 'l2')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nresults = pd.DataFrame([['Logistic Regression (Lasso)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])","1e8645f7":"## SVM (Linear)\nfrom sklearn.svm import SVC\nclassifier = SVC(random_state = 0, kernel = 'linear', probability= True)\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['SVM (Linear)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","bcbd2456":"## SVM (rbf)\nfrom sklearn.svm import SVC\nclassifier = SVC(random_state = 0, kernel = 'rbf', probability= True)\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['SVM (RBF)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","d4488801":"## Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Naive Bayes (Gaussian)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","7c45b8be":"## Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion='entropy', random_state=0)\nclassifier.fit(X_train, y_train)\n\n#Predicting the best set result\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Decision Tree', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","2bf7cceb":"## Random Forest Gini (n=100)\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(random_state = 0, n_estimators = 100,\n                                    criterion = 'gini')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Random Forest Gini (n=100)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","b906069a":"## Random Forest Gini (n=200)\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(random_state = 0, n_estimators = 200,\n                                    criterion = 'gini')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Random Forest Gini (n=200)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","47e1ce4f":"## Random Forest Gini (n=300)\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(random_state = 0, n_estimators = 300,\n                                    criterion = 'gini')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Random Forest Gini (n=300)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","89bdd6c4":"## Random Forest Entropy (n=100)\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(random_state = 0, n_estimators = 100,\n                                    criterion = 'entropy')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Random Forest Entropy (n=100)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","192c12cd":"## Random Forest Entropy (n=200)\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(random_state = 0, n_estimators = 200,\n                                    criterion = 'entropy')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Random Forest Entropy (n=200)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","e1e6497b":"## Random Forest Entropy (n=300)\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(random_state = 0, n_estimators = 300,\n                                    criterion = 'entropy')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Random Forest Entropy (n=300)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","ecc90132":"## K-Nearest Neighbors (K-NN)\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors=15, metric='minkowski', p= 2)\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['K-Nearest Neighbors (minkowski)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","1d545223":"results","4c540c5c":"X = data.drop('target',axis=1)\ny = data.target","a5a34bfe":"mms = MinMaxScaler()\nX = pd.DataFrame(mms.fit_transform(X),columns=X.columns)","856f58e6":"X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.33, random_state=42)","9b933de9":"X_train = X_train.drop(['var_7','var_10','var_17','var_29','var_30','var_27',\n                             'var_38','var_39','var_41','var_73','var_98','var_61',\n                             'var_100','var_103','var_136','var_153','var_158',\n                             'var_161','var_183',\n                             'var_16','var_37','var_46','var_65','var_69','var_79',\n                             'var_96','var_117','var_124','var_126','var_185','var_189',\n                             'var_61','var_60','var_47'                             \n                             ],axis=1)","fdced544":"X_test = X_test.drop(['var_7','var_10','var_17','var_29','var_30','var_27',\n                            'var_38','var_39','var_41','var_73','var_98','var_61',\n                            'var_100','var_103','var_136','var_153','var_158',\n                            'var_161','var_183',\n                           'var_16','var_37','var_46','var_65','var_69','var_79',\n                             'var_96','var_117','var_124','var_126','var_185','var_189',\n                           'var_61','var_60','var_47'\n                             ],axis=1)","407b990c":"## Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\n\nprint('Accuracy of Naive bayes classifier on test set: {:.5f}'.format(accuracy_score(y_test,y_pred)))","67354a83":"from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n\ncm = confusion_matrix(y_test, y_pred) # rows = truth, cols = prediction\ndf_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\n\nplt.figure(figsize = (5,4))\nsns.set(font_scale=1.4)\n\nsns.heatmap(df_cm, annot=True, fmt='g', cmap=\"Blues\")\nprint(\"Test Data Accuracy: %0.4f\" % accuracy_score(y_test, y_pred))","a8926281":"print(classification_report(y_test, y_pred))","4098d77d":"test = pd.read_csv(\"\/kaggle\/input\/santander-customer-transaction-prediction\/test.csv\",index_col='ID_code')\n\ntest = test.drop(['var_7','var_10','var_17','var_29','var_30','var_27',\n                             'var_38','var_39','var_41','var_73','var_98','var_61',\n                             'var_100','var_103','var_136','var_153','var_158',\n                             'var_161','var_183',\n                             'var_16','var_37','var_46','var_65','var_69','var_79',\n                             'var_96','var_117','var_124','var_126','var_185','var_189',\n                             'var_61','var_60','var_47'                             \n                             ],axis=1)\n\nsol = pd.DataFrame(test.index)\n\nmms = MinMaxScaler()\ntest = pd.DataFrame(mms.fit_transform(test),columns=test.columns)\n\nsol['target']=classifier.predict(test)\n\nsol.set_index('ID_code',inplace=True)\n\nsol.to_csv('submission_NB')","be6510ea":"I am using this notebook to learn new and advance techniques, before I can begin reading others works, I want to work on the data myself and get a feel of the problem.\n\n* I am looking to understand the limitation this large amount of data\n* I am seeing what a base accuracy any novice can get\n* I recently learnt upscaling, RFE so I want to test it out\n* I am pretty sure, a lot of the improvement will be done based on how the data is preprocessed! So I want to use my own ideas before Begin to read others\n\nReaders, please feel free to comment\/ crticize\/ advice ","a6fec526":"##### Not all models are running with so many data, So I will select a few on my on. As we have observed, that adjusting the ratios of sample sets are imoacting the accuracy of the model, I have decieded to maintain the same ratios. \n\nMore enlightened folks please, I implore you to give me your sugesstion and valueble feedback on the limitations of my actions!!\n\nAnd also feel free to critize the hell out of this notebook and help me out :P\n\nPlease also let me know if there is a package that would have helped me select a few data points out of these ","1be3631d":"#### Observations\n\n* We can observe higher transactions (1s) for 10% of the people in most of the graphs\n \n* Either they are in the begining, ie, 10th percecntile or in the end, ie, 90th percentile\n\n* Such distinctions across all variables will yield a good accuracy","095f556d":"### Trying Other Models","898f380d":"#### Conclusion\n\n* Some of the features show bimodal, it would be lovely to learn how to treat that?\n\nPS: I did read online, but I would prefer the kaggle community solutions. It tends to be more pragmatic and industrial, compared to some of the material I have read, which seems research-ish \n\n\nAs suggested by these thereads, that the best solutions would be to use \"gausian mixture\" and \"logistic Regression\"\nhttps:\/\/www.researchgate.net\/post\/How_to_analyse_a_Bimodal_response_variable\nhttps:\/\/www.researchgate.net\/post\/How_can_we_deal_with_bimodal_variables\n\nwhich you will see later do yield the best results. The two highest accuracy models are the Naive Bayes and logistic Regression!\n\nBut is there any solution to treat the data, logiacally I dont think so, Help Kagglers?","62ea8152":"#### Using RFE : For Feature Selection","d303c6aa":"## Changing sample ratios by using upscaling and down scaling ","04f7cf7d":"Since I am using Logistic Regression I want to try using MinMaxScaler() once : It might yield better result. \n\nThe data if it is in 0-1 range it improves logistic regression results","12ec6222":"#### Conclusion of Udersampling and over Sampling\n\nWe can see a reduction in the prediction ratios, that is we are overpredicting the amount of 1s, after sample manuplation, thus we shall not use it!","620aa41d":"##### Slight drop in recall for 1, mean a value that was 1 got predicted as 0","f426e53b":"#### Second, I will eleminate both variables only with -ve & +ve coeff. of variables : ","a16e4daa":"### Min Max Scaler Transformation","e28648b9":"### Using Logistic Regression ","5ad82a50":"#### Conclusion\nSince all the data were a continuous type using logistic regression was obious","eac5edb5":"# Conslusion\n\nSome Advice on how should I proceed to  improve my accuracy?","26c168da":"### Naive Bayes (Gaussian)\n\nUsing the full data","f4936ee1":"### Test Data","d74f9028":"How to improve the overall accuracy of the model, \n\nLet us first understand : We have high recall for 0 and slightly lower precision for 0, that means we need to reduce the number of 0 predictions. \n\nThat inturn will drive my model prediction of 1s to increase, \n\n* Genrally \n> * I want to reduce the elements with the lower coeff magnituge  \n> * Reduce elements with high p-val (>0.02) (I am chosing a confidence level of 98%) \n* For this model\n> * But I shall keep the values with high coeff magnitude if it is +ve, and Pval <0.05 -> coz the model has low \n> * I shall remove the values with coeff of magnitude -ve and pval>0.02","353029f5":"##### Slight drop in recall for 1, mean a value that was 1 got predicted as 0","44ad9375":"### Exploratoty Data Analysis - Basic","49bd1ad5":"### Min Max Scaler Transformation + Data Selection","7540447d":"How much of a -ve impact will it make if I run the EDA on \n\n* first 500 or 'x' data\n* random 500 or 'x' data\n\nHow can I determine 'x'","e169d23a":"### Results","29ba683f":"# Santanders Customer Transaction - Competition","273deb05":"We can see an increase in overall accuracy by 0.01.","1b427b7b":"#### First I will eleminate variables only with -ve coeff. of variables : ","5452d112":"#### Ploting afer using minmax just to check what happens to the bimodals"}}