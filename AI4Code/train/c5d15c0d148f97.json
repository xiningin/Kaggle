{"cell_type":{"32122f21":"code","74b9f508":"code","094ba43c":"code","cfef7028":"code","91079330":"code","ce61bd71":"code","2f92efc2":"code","1d3258ad":"code","e467dd3a":"code","64848764":"code","1c7cda36":"code","2c16862a":"code","85d2c108":"code","fc2ef57b":"code","a10d74a4":"code","1ba442d1":"code","6c297b33":"code","0c241aae":"code","8ca31e15":"code","1a72d2da":"code","14c1da56":"code","a8c32fcd":"code","7da26f85":"code","d72e7e26":"code","0f0ce35e":"code","ac81129c":"code","739c2290":"code","30e1d503":"code","669324ff":"code","e800429c":"code","5777f7ec":"code","cafdd99f":"code","bce48f30":"code","d00e1ae3":"code","7b3a92aa":"code","15200c05":"code","068dca1e":"code","9b28575e":"code","1c76d11e":"code","1411dfcf":"code","38f833ce":"markdown","eb57ffcc":"markdown","6c248ebb":"markdown","7ddb4328":"markdown","5e6a7451":"markdown","0b82b070":"markdown","bbbeb4fe":"markdown","83bcf15a":"markdown","22e32039":"markdown","b632ef34":"markdown","ed018559":"markdown","adac0952":"markdown","3409197e":"markdown","26cf3b53":"markdown","97629c0c":"markdown","0a96aac0":"markdown","7585e7b8":"markdown","b961e8ac":"markdown","f59984ef":"markdown","cc53dbef":"markdown","d9c6a15e":"markdown","32c21441":"markdown","4a32a760":"markdown","a8a2ddff":"markdown","20d508e1":"markdown","931cf4c3":"markdown","267c9996":"markdown","b4e4e086":"markdown"},"source":{"32122f21":"import numpy as np \nimport pandas as pd \nfrom google.cloud import bigquery\nfrom bq_helper import BigQueryHelper\nfrom time import time\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\nimport folium\nimport json\nimport requests\nfrom folium import Choropleth, Circle, Marker\nfrom folium.plugins import HeatMap, MarkerCluster\nfrom folium import GeoJson\nfrom folium.features import GeoJsonPopup, GeoJsonTooltip\nfrom folium.plugins import HeatMap, HeatMapWithTime\n\npd.set_option('display.max_columns', None) # tells Pandas to display all columns, instead of hiding middle columns to fit into the screen\npd.set_option('display.max_rows', 20)\n\n# Create a \"Client\" object\nclient = bigquery.Client()\n\n# Construct a reference to the \"san_francisco\" dataset\ndataset_ref = client.dataset(\"san_francisco\", project=\"bigquery-public-data\")","74b9f508":"# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n# List all the tables in the dataset\ntables = list(client.list_tables(dataset))\n# Print names of all tables in the dataset \nfor table in tables:  \n    print(table.table_id)","094ba43c":"# Construct a reference to the \"311_service_requests\" table\ntable_ref = dataset_ref.table(\"311_service_requests\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\ntable.schema","cfef7028":"# Preview the first five lines of the \"full\" table\nclient.list_rows(table, max_results=5).to_dataframe()","91079330":"query = \"\"\"\n        SELECT DISTINCT EXTRACT(DATE FROM created_date) dates\n        FROM `bigquery-public-data.san_francisco.311_service_requests`\n        ORDER BY dates DESC\n        \"\"\"\nquery_job = client.query(query)\ndates_list = query_job.to_dataframe()\n\nprint('First date is', dates_list.iloc[-1].dates)\nprint('Last date is', dates_list.iloc[0].dates)","ce61bd71":"%%time\n# Estimate the size of the table, possible max size of a query\nquery = \"\"\"\n        SELECT *\n        FROM `bigquery-public-data.san_francisco.311_service_requests`\n        \"\"\"\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job = client.query(query, job_config=dry_run_config)\n\nprint(\"This query will process %i Mbytes.\"%(int(dry_run_query_job.total_bytes_processed) \/ (1024*1024)))","2f92efc2":"# Determine number of calls for each category, keep only those with significant number of calls, overall count for the whole timeperiod\nquery = \"\"\"\n        WITH limited as (\n        SELECT * \n        FROM `bigquery-public-data.san_francisco.311_service_requests`\n        WHERE (DATE(created_date) > '2008-12-31') AND (DATE(created_date) < '2018-01-01')\n        ),\n        cats as (\n        SELECT category, COUNT(1) as num_category_calls\n        FROM limited\n        GROUP BY category\n        ORDER BY num_category_calls DESC\n        )\n        SELECT *\n        FROM cats\n        WHERE num_category_calls > 1000\n        \"\"\"\n\n# Set up the query\nquery_job = client.query(query)\n# API request - run the query, and return a pandas DataFrame\ncategories = query_job.to_dataframe()","1d3258ad":"# Determine number of calls for each category for each year\nquery = \"\"\"\n        WITH limited as (\n        SELECT * \n        FROM `bigquery-public-data.san_francisco.311_service_requests`\n        WHERE (DATE(created_date) > '2008-12-31') AND (DATE(created_date) < '2018-01-01') \n        ),\n        cats as (\n        SELECT category, COUNT(1) as num_category_calls, EXTRACT(YEAR FROM created_date) year\n        FROM limited\n        GROUP BY category, year\n        ORDER BY year DESC\n        )\n        SELECT *\n        FROM cats\n        \"\"\"\nquery_job = client.query(query)\ncats_years = query_job.to_dataframe()","e467dd3a":"categories.T","64848764":"# top 10 categories\ntop10 = list(categories.category[:10])\n\ncats_years100 = cats_years.loc[cats_years.num_category_calls > 100]\n# cat_U = list(cats_years100.category.unique())\n\nplt.figure(figsize=(20,10))\nfor cat in top10:\n    cur_cat = cats_years100.loc[cats_years100.category == cat,['year','num_category_calls']]\n    plt.plot(cur_cat.year, cur_cat.num_category_calls, label = cat )\n\nplt.title('Categories over the years')\nplt.legend()\nplt.show()\n","1c7cda36":"# Lets have a closer look at the homeless concerns calls\nquery = \"\"\"\n        SELECT *\n        FROM `bigquery-public-data.san_francisco.311_service_requests`\n        WHERE category = 'Homeless Concerns'\n        ORDER BY created_date\n        \"\"\"\nquery_job = client.query(query)\nhomeless = query_job.to_dataframe()\nhomeless.head(5)","2c16862a":"# Determine number of calls for each category, overall count for the whole timeperiod\nquery = \"\"\"\n        WITH limited as (\n        SELECT * \n        FROM `bigquery-public-data.san_francisco.311_service_requests`\n        WHERE category = 'Homeless Concerns'\n        ),\n        cats as (\n        SELECT complaint_type, COUNT(1) as num_complaint_type\n        FROM limited\n        GROUP BY complaint_type\n        ORDER BY num_complaint_type DESC\n        )\n        SELECT *\n        FROM cats\n        \"\"\"\nquery_job = client.query(query)\ncomplaint_type = query_job.to_dataframe()\nprint(complaint_type.head(10))\nprint('')\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.bar(complaint_type.complaint_type,complaint_type.num_complaint_type)\nplt.show()","85d2c108":"query = \"\"\"\n        SELECT neighborhood, COUNT(1) num_neighb\n        FROM `bigquery-public-data.san_francisco.311_service_requests`\n        WHERE category = 'Homeless Concerns'\n        GROUP BY neighborhood\n        ORDER BY num_neighb DESC\n        \"\"\"\nquery_job = client.query(query)\nneighb_incidents = query_job.to_dataframe()","fc2ef57b":"print('Total number of neighborhoods : %i'%neighb_incidents.shape[0])\nneighb_incidents.T","a10d74a4":"fig = plt.figure(figsize=(15, 8))\nax = fig.add_axes([0,0,1,1])\nax.bar(neighb_incidents.neighborhood[:20],neighb_incidents.num_neighb[:20])\nplt.xticks(rotation='vertical')\nplt.show()","1ba442d1":"query = \"\"\"\n        SELECT h.neighborhood, h.num_homeless, sc.num_clean, g.num_graffiti, e.num_encampments\n        FROM (SELECT neighborhood, COUNT(1) num_homeless FROM `bigquery-public-data.san_francisco.311_service_requests` \n              WHERE category = 'Homeless Concerns' GROUP BY neighborhood) as h\n        FULL JOIN (SELECT neighborhood, COUNT(1) num_clean FROM `bigquery-public-data.san_francisco.311_service_requests` \n                  WHERE category = 'Street and Sidewalk Cleaning' GROUP BY neighborhood) as sc\n                  ON h.neighborhood = sc.neighborhood\n        FULL JOIN (SELECT neighborhood, COUNT(1) num_graffiti FROM `bigquery-public-data.san_francisco.311_service_requests` \n                  WHERE category = 'Graffiti' GROUP BY neighborhood) as g\n                  ON h.neighborhood = g.neighborhood\n        FULL JOIN (SELECT neighborhood, COUNT(1) num_encampments FROM `bigquery-public-data.san_francisco.311_service_requests` \n                  WHERE category = 'Encampments' GROUP BY neighborhood) as e\n                  ON h.neighborhood = e.neighborhood\n        ORDER BY h.num_homeless DESC\n        \"\"\"\n\nquery_job = client.query(query)\nneighb_incidents = query_job.to_dataframe()","6c297b33":"# create columns with percentage of each issue for each neighborhood\nneighb_incidents['perc_homeless'] = neighb_incidents['num_homeless'] \/ neighb_incidents['num_homeless'].sum() * 100\nneighb_incidents['perc_clean'] = neighb_incidents['num_clean'] \/ neighb_incidents['num_clean'].sum() * 100\nneighb_incidents['perc_graffiti'] = neighb_incidents['num_graffiti'] \/ neighb_incidents['num_graffiti'].sum() * 100\nneighb_incidents['perc_encampments'] = neighb_incidents['num_encampments'] \/ neighb_incidents['num_encampments'].sum() * 100\n# total number of all 4 types of issues reported for each neighborhood\nneighb_incidents['all'] = neighb_incidents['num_homeless'] + neighb_incidents['num_graffiti'] + neighb_incidents['num_clean']  + neighb_incidents['num_encampments']\n\n\n# need to rearange columns for Folium to be able to pick the index and the value column\ncols = neighb_incidents.columns.tolist()\nnew_cols = [cols[0]]+[cols[-1]]+cols[1:-1]\nnew_cols","0c241aae":"# lets have a look at the first 10 rows\nprint(neighb_incidents.shape)\nneighb_incidents = neighb_incidents[new_cols]\nneighb_incidents.head(10)","8ca31e15":"# setup for the Folium map\nSF_COORDINATES = (37.76, -122.45)\n# loading neighborhoods definition and metadata\nsf_zones = '..\/input\/sf-neighborhoods\/SF_Find_Neighborhoods.geojson'\nwith open(sf_zones) as response:\n    geo_json_data = json.load(response)","1a72d2da":"j_names = []\nfor i in range(len(geo_json_data['features'])):\n    j_names.append(geo_json_data['features'][i]['properties']['name'])\n    \nprint(set(list(neighb_incidents.neighborhood)) - set(j_names))\nprint(set(j_names) - set(list(neighb_incidents.neighborhood)))","14c1da56":"neighb_incidents.loc[neighb_incidents.neighborhood == \"Fisherman's Wharf\",'neighborhood'] = \"Fishermans Wharf\"\nneighb_incidents.loc[neighb_incidents.neighborhood == \"St. Mary's Park\",'neighborhood'] = \"St. Marys Park\"\nneighb_incidents.drop(neighb_incidents.loc[neighb_incidents['neighborhood'] == ''].index, inplace = True)","a8c32fcd":"# have to implement a fail safe approach since there is a big chance of missing data for some of the issues, when they were not reported in certain neighborhoods\nfor i in range(len(geo_json_data['features'])):\n    ph,nh, pe,ne, pg,ng, pc,nc = 0,0,0,0,0,0,0,0\n    try:\n        ph = neighb_incidents.loc[neighb_incidents.neighborhood == geo_json_data['features'][i]['properties']['name'],'perc_homeless'].values[0]\n        geo_json_data['features'][i]['properties']['perc_homeless'] = ph\n        nh = neighb_incidents.loc[neighb_incidents.neighborhood == geo_json_data['features'][i]['properties']['name'],'num_homeless'].values[0]\n        geo_json_data['features'][i]['properties']['num_homeless'] = int(nh)\n    except:\n        print('Didn`t find neighborhood %s'%geo_json_data['features'][i]['properties']['name'])\n        geo_json_data['features'][i]['properties']['perc_homeless'] = 0\n        geo_json_data['features'][i]['properties']['num_homeless'] = 0\n    try:\n        pc = neighb_incidents.loc[neighb_incidents.neighborhood == geo_json_data['features'][i]['properties']['name'],'perc_clean'].values[0]\n        geo_json_data['features'][i]['properties']['perc_clean'] = pc\n        nc = neighb_incidents.loc[neighb_incidents.neighborhood == geo_json_data['features'][i]['properties']['name'],'num_clean'].values[0]\n        geo_json_data['features'][i]['properties']['num_clean'] = int(nc)\n    except:\n        print('Didn`t find neighborhood %s'%geo_json_data['features'][i]['properties']['name'])\n        geo_json_data['features'][i]['properties']['perc_clean'] = 0\n        geo_json_data['features'][i]['properties']['num_clean'] = 0\n    try:\n        pg = neighb_incidents.loc[neighb_incidents.neighborhood == geo_json_data['features'][i]['properties']['name'],'perc_graffiti'].values[0]\n        geo_json_data['features'][i]['properties']['perc_graffiti'] = pg\n        ng = neighb_incidents.loc[neighb_incidents.neighborhood == geo_json_data['features'][i]['properties']['name'],'num_graffiti'].values[0]\n        geo_json_data['features'][i]['properties']['num_graffiti'] = int(ng)\n    except:\n        print('Didn`t find neighborhood %s'%geo_json_data['features'][i]['properties']['name'])\n        geo_json_data['features'][i]['properties']['perc_graffiti'] = 0\n        geo_json_data['features'][i]['properties']['num_graffiti'] = 0\n    try:\n        pe = neighb_incidents.loc[neighb_incidents.neighborhood == geo_json_data['features'][i]['properties']['name'],'perc_encampments'].values[0]\n        geo_json_data['features'][i]['properties']['perc_encampments'] = pe\n        ne = neighb_incidents.loc[neighb_incidents.neighborhood == geo_json_data['features'][i]['properties']['name'],'num_encampments'].values[0]        \n        geo_json_data['features'][i]['properties']['num_encampments'] = int(ne)\n    except:\n        print('Didn`t find neighborhood %s'%geo_json_data['features'][i]['properties']['name'])\n        geo_json_data['features'][i]['properties']['perc_encampments'] = 0      \n        geo_json_data['features'][i]['properties']['num_encampments'] = 0\n\n# replace missing values with zeros\nneighb_incidents.fillna(0, inplace = True)        ","7da26f85":"# a litle sanity check\ngeo_json_data['features'][20]\n# it seems that everything worked","d72e7e26":"# dynamic tooltip reflecting percentages\ntooltip = GeoJsonTooltip(\n    fields=['name', 'perc_homeless', 'perc_clean', 'perc_graffiti', 'perc_encampments'],\n    aliases=[\"Neighborhood:\", \"% of homeless issues:\", \"% of cleanliness issues:\", \"% of graffiti issues:\", \"% of encampments issues:\"],\n    localize=True,\n    sticky=False,\n    labels=True,\n    style=\"\"\"\n        background-color: #F0EFEF;\n        border: 2px solid black;\n        border-radius: 3px;\n        box-shadow: 3px;\n    \"\"\",\n    max_width=800,\n)","0f0ce35e":"# Create an empty map zoomed in on San Francisco\nsf_311_map = folium.Map(location = SF_COORDINATES, zoom_start=12)\n\nfolium.Choropleth(\n    geo_data = geo_json_data,\n    data = neighb_incidents,\n    columns = ['neighborhood', 'all'],\n    key_on = 'feature.properties.name',\n    fill_color = 'YlOrRd', \n    fill_opacity = 0.3, \n    line_opacity = 0.5,\n    line_weight=2,\n    highlight=True,\n    control_scale=True,\n    legend_name = 'San Fransisco 311 calls by Neighborhood'\n).geojson.add_child(tooltip).add_to(sf_311_map)\n\nsf_311_map","ac81129c":"corr = neighb_incidents[['num_homeless', 'num_clean', 'num_graffiti', 'num_encampments']].corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(3)","739c2290":"# Construct a reference to the \"sfpd_incidents\" table\ntable_ref = dataset_ref.table(\"sfpd_incidents\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\ntable.schema","30e1d503":"%%time\n# Estimate the size of the table, possible max size of a query\nquery = \"\"\"\n        SELECT *\n        FROM `bigquery-public-data.san_francisco.sfpd_incidents`\n        \"\"\"\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job = client.query(query, job_config=dry_run_config)\n\nprint(\"This query will process %i Mbytes.\"%(int(dry_run_query_job.total_bytes_processed) \/ (1024*1024)))","669324ff":"# Preview the first five lines of the \"full\" table\nclient.list_rows(table, max_results=5).to_dataframe()","e800429c":"query = \"\"\"\n        SELECT DISTINCT EXTRACT(DATE FROM timestamp) dates\n        FROM `bigquery-public-data.san_francisco.sfpd_incidents`\n        ORDER BY dates DESC\n        \"\"\"\nquery_job = client.query(query)\ndates_list = query_job.to_dataframe()\n\nprint('First date is', dates_list.iloc[-1].dates)\nprint('Last date is', dates_list.iloc[0].dates)","5777f7ec":"# Determine number of events for each category\nquery = \"\"\"\n        WITH limited as (\n        SELECT * \n        FROM `bigquery-public-data.san_francisco.sfpd_incidents`\n        WHERE (DATE(timestamp) < '2018-01-01')\n        ),\n        cats as (\n        SELECT category, COUNT(1) as num_category_calls\n        FROM limited\n        GROUP BY category\n        ORDER BY num_category_calls DESC\n        )\n        SELECT *\n        FROM cats\n        \"\"\"\n#         WHERE num_category_calls > 1000\n\nquery_job = client.query(query)\ncategories = query_job.to_dataframe()","cafdd99f":"categories.T","bce48f30":"# Determine number of events for each category for each year\nquery = \"\"\"\n        WITH limited as (\n        SELECT * \n        FROM `bigquery-public-data.san_francisco.sfpd_incidents`\n        WHERE (DATE(timestamp) < '2018-01-01') \n        ),\n        cats as (\n        SELECT category, COUNT(1) as num_category_calls, EXTRACT(YEAR FROM timestamp) year\n        FROM limited\n        GROUP BY category, year\n        ORDER BY year DESC\n        )\n        SELECT *\n        FROM cats\n        \"\"\"\nquery_job = client.query(query)\ncats_years = query_job.to_dataframe()","d00e1ae3":"# top 20 categories\ntop20 = list(categories.category[:20])\n\nplt.figure(figsize=(20,10))\nfor cat in top20:\n    cur_cat = cats_years.loc[cats_years.category == cat,['year','num_category_calls']]\n    plt.plot(cur_cat.year, cur_cat.num_category_calls, label = cat )\n\nplt.title('Categories over the years')\nplt.legend()\nplt.show()","7b3a92aa":"query = \"\"\"\n        SELECT category, longitude, latitude, DATE(timestamp) as ev_date\n        FROM `bigquery-public-data.san_francisco.sfpd_incidents`\n        WHERE DATE(timestamp)  > '2008-12-31' AND DATE(timestamp)  < '2018-01-01'\n        ORDER BY ev_date\n        \"\"\"\nquery_job = client.query(query)\nall_incidents = query_job.to_dataframe()\nprint(all_incidents.shape)","15200c05":"larceny_theft = all_incidents.loc[all_incidents.category == 'LARCENY\/THEFT']\n# check basic requirements\nprint(larceny_theft.isna().sum())\nprint(larceny_theft.info())","068dca1e":"larceny_theft2017 = larceny_theft.loc[larceny_theft.ev_date > datetime.strptime('2017-01-01', '%Y-%m-%d').date()]\n\n# List comprehension to make list of lists required for the heatmap plugin\nheat_data = [[row['latitude'],row['longitude']] for index, row in larceny_theft2017.iterrows()]\n\n# Create an empty map zoomed in on San Francisco\nsf_larceny = folium.Map(location = SF_COORDINATES, zoom_start=13)\n\nfolium.Choropleth(\n    geo_data = geo_json_data,\n    data = neighb_incidents,\n    columns = ['neighborhood', 'all'],\n    key_on = 'feature.properties.name',\n    fill_color = 'YlOrRd', \n    fill_opacity = 0.3, \n    line_opacity = 0.5,\n    line_weight=2,\n    highlight=True,\n    control_scale=True,\n    legend_name = 'San Fransisco 311 calls by Neighborhood'\n).geojson.add_child(tooltip).add_to(sf_larceny)\n\n# add heatmap to the choropleth map\nHeatMap(heat_data,radius=15).add_to(sf_larceny)\nsf_larceny","9b28575e":"sex_offences = all_incidents.loc[all_incidents.category == 'SEX OFFENSES, FORCIBLE']\n\n#  lets focus on the last year of the crime data alone, \n#  otherwise there will be too many datapoint on the map and it'll be absolutely overloaded\nsex_offences2017 = sex_offences.loc[sex_offences.ev_date > datetime.strptime('2017-01-01', '%Y-%m-%d').date()]\n\n# List comprehension to make list of lists required for the heatmap plugin\nheat_data_sex = [[row['latitude'],row['longitude']] for index, row in sex_offences2017.iterrows()]\n\n# Create an empty map zoomed in on San Francisco\nsf_sex = folium.Map(location = SF_COORDINATES, zoom_start=12)\n\nfolium.Choropleth(\n    geo_data = geo_json_data,\n    data = neighb_incidents,\n    columns = ['neighborhood', 'all'],\n    key_on = 'feature.properties.name',\n    fill_color = 'YlOrRd', \n    fill_opacity = 0.3, \n    line_opacity = 0.5,\n    line_weight=2,\n    highlight=True,\n    control_scale=True,\n    legend_name = 'San Fransisco 311 calls by Neighborhood'\n).geojson.add_child(tooltip).add_to(sf_sex)\n\n# add heatmap of 'SEX OFFENSES, FORCIBLE' to the choropleth map\nHeatMap(heat_data_sex,radius=15).add_to(sf_sex)\nsf_sex","1c76d11e":"query = \"\"\"\n        SELECT longitude, latitude, DATE(timestamp) as ev_date, EXTRACT(YEAR FROM timestamp) year\n        FROM `bigquery-public-data.san_francisco.sfpd_incidents`\n        WHERE DATE(timestamp)  < '2018-01-01' AND category = 'LARCENY\/THEFT'\n        ORDER BY ev_date\n        \"\"\"\nquery_job = client.query(query)\nall_incidents = query_job.to_dataframe()\nprint(all_incidents.shape)\nall_incidents.head(5)","1411dfcf":"# create a list of list of lists [year1 of [coords [ [long, lat] ... [long, lat] ]], ... [yearN] ]\nheat_data = []\nyears = list(all_incidents.year.unique())\nfor y in years:\n    cur_year = all_incidents.loc[all_incidents.year == y]\n    heat_data += [[list(a) for a in zip(list(cur_year.latitude),list(cur_year.longitude))]]\n    \n# Create an empty map zoomed in on San Francisco\nsf_time = folium.Map(location = SF_COORDINATES, zoom_start=13)\n\nfolium.Choropleth(\n    geo_data = geo_json_data,\n    fill_color = 'YlOrRd', \n    fill_opacity = 0.3, \n    line_opacity = 0.5,\n    line_weight=2,\n    highlight=True,\n    control_scale=True,\n    legend_name = 'San Fransisco 311 calls by Neighborhood'\n).geojson.add_child(tooltip).add_to(sf_time)\n\nHeatMapWithTime(heat_data,radius=10, index = years, auto_play=True, max_opacity=0.3).add_to(sf_time)\nsf_time","38f833ce":"As we can see the geographical part of the data is significantly different - it is segmented by police districts instead of the neighborhoods. In theory we could remap it using the neighborhoods boundaries from the geojson file and coords from the police data. However I want to try a different approach - combine heatmap of crimes over our choropleth map with 311 data.\n\nBut first lets see what time period we have the data for, and what types of crimes registered there with basic stats again.","eb57ffcc":"There are too many categories so lets focus on the top 10 issues reported to the system and plot them side by side.","6c248ebb":"A brief look at the table's data, to get a sense of what kind of information we have in there.","7ddb4328":"And it seems that human waste was the most 'popular' complaint.\n\nWhich neighborhoods are affected by the issue more? Lets rank them by the count of all types of complaints in this category.","5e6a7451":"And again lets estimate quickly the size of the dataset and the posible size of queries to avoid reaching the data cap:","0b82b070":"We must check data integrity: if the sets of the neighborhoods in the created dataframe and json file are the same, the names are matching, no empty neighborhoods etc.","bbbeb4fe":"List all tables in the dataset.","83bcf15a":"## Sex offenses, forcible \nNow lets have a look at another category of the crimes, last one from our Top 20 - 'SEX OFFENSES, FORCIBLE' for the last year available:","22e32039":"And it looks like there is indeed rather weak correlation between homeless concerns reports and all 3 other categories. However encampments reports significantly correlated with both cleanliness and graffiti problems.","b632ef34":"Interestingly enough the picture is clear: most problematic neighborhoods in terms of the 311 reported issues also have the biggest numbers of both larceny and forcible sex crimes.","ed018559":"There are too many categories so lets focus on the top 20 issues reported to the system and plot them side by side.","adac0952":"Based on the fact a full query will take less than 0.8Gb we can safely make almost any types of queries and not to worry about the exceeding the data cap. <br>\n\n\nLets find out what kind of requests residents made in the period 2009-2017 overall, and rank them by the number of calls.","3409197e":"So we have information from 2008 till 2018. However since there is only about a month of the data for the year 2018, and just half a year of the data for 2008 I'll drop this partial data for all future analysis, when comparing the data year by year.\n\nLets estimate quickly the size of the dataset and the posible size of queries to avoid reaching the data cap (Kaggle limits us at 5Tb per month):","26cf3b53":"Few problems here: we need to rename a couple of the neighborhoods, and to drop one row without the neighborhood name from the dataframe.","97629c0c":"It looks like most of the crimes from the Top 20 remained on approximately same level, except Larceny\/Theft which skyrocketed starting in about 2010. So I'll be interested in this crime in particular, combining all events from all categories into a single map will not make much sense and will simply overload it visually. \n\nAnd lets use the same time period of 2009-2017 as we used for 311 data.\n\n## Larceny\/Theft mapping","0a96aac0":"Some of the issues are quite persistent\/stable throughout this whole period of time, however a few issues show significant upward trend. There are certainly growing problems with streets' cleanliness, graffiti and homeless concerns, as well as encampments.\n\nI will start with analyzing the homeless concerns category, and then we will try to decide if there is a relation with other issues.","7585e7b8":"Now we have to modify the geojson file by adding all the numbers from the dataframe - this is a workaround needed for us to be able to use dynamic tooltips displaying the statistics for each neighborhood, since unfortunately Choropleth class is not capable of doing it yet via built-in tooltips.","b961e8ac":"Lets plot top 20 most problematic neighborhoods.","f59984ef":"# Observations\nClearly there are a few quite problematic neighborhoods: 'Mission', 'South of Market', 'Tenderloin', 'Civic Center' etc, with 'Mission' being the worst in the city. Some other neighboring areas suffer from these issues too, but significantly less so. Interestingly enough there are other neighborhoods that report cleanliness and graffiti issues while not complaining about other issues, for example both Inner and Outer Richmonds, or Portola.\n\nMoreover percentage of complains regarding cleanliness and graffity are on par for such neighborhoods as Portola, Inner Richmond and Tenderloin. Which fact suggests that these issues are not directly correlated.\n\nHowever lets analyze the correlations further:","cc53dbef":"# San Francisco Police data\nLets have a look at  the police data and find out if we can combine these 2 databases into a compelling map.","d9c6a15e":"Lets determine what time period is covered by the data:","32c21441":"## Larceny dynamics in time\nAnd finally lets create a map reflecting dynamics of the larceny crimes in time for the available data: 2003 through 2017.","4a32a760":"Lets focus on the last year of the crime data alone, otherwise there will be too many datapoint on the map and it'll be absolutely overloaded.","a8a2ddff":"Lets put together information on 4 issues with the upward trend.","20d508e1":"Now I'd like to see what complaints people made in regards to homeless concerns:","931cf4c3":"The data starts with full 2003 and we have only 10 days of 2018, so we will drop 2018 again from the consideration.","267c9996":"# 311 data analysis\nFirst part of this notebook explores data from the BigQuery dataset which\n includes all San Francisco 311 service requests from July\n2008 though 2018, and was updated daily. 311 is a non-emergency\nnumber that provides access to non-emergency municipal services.\n\nSecond part of the notebook will plug in the data from SFPD as well, and I'll try to explore some correlation between these 2 databases.\n\nI'll use SQL for data extraction and brief EDA.","b4e4e086":"At first I will work with 311 requests data alone.<br>\nWhat's in the table, lets have a look at the schema?"}}