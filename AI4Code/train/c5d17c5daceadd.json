{"cell_type":{"ee9e5672":"code","56e3e915":"code","b5945513":"code","f0d040a8":"code","0a6b7274":"code","3e5524ea":"code","170d8b29":"code","fe974530":"code","562a35d9":"code","62951799":"code","97174ca7":"code","b0cd8a29":"code","9353e2e7":"code","6a164610":"code","940fcab5":"code","24febc94":"code","23135f76":"code","20ec189a":"code","c504dc91":"code","1283f774":"code","eb54ef02":"code","0c82e533":"code","b466f12e":"markdown","568b0ae7":"markdown","76da26c9":"markdown","26c492b9":"markdown","8dce9056":"markdown","4d529c4c":"markdown","4db1a43f":"markdown","81aaac75":"markdown","ae072959":"markdown","2581828c":"markdown","22d18374":"markdown","ce2a90ef":"markdown","6d67cb56":"markdown","525e269d":"markdown","7720397d":"markdown","8e753467":"markdown","82923432":"markdown","3dc9b81c":"markdown","5c48cd10":"markdown","a9c533fa":"markdown","30f61a16":"markdown","4386f660":"markdown","9c26dbae":"markdown","fc7c0aed":"markdown","d0d0cef0":"markdown","81ff5276":"markdown","dd91892f":"markdown","14871f6b":"markdown","5aa65ef4":"markdown","0c43e48f":"markdown","72194750":"markdown","3c49a388":"markdown"},"source":{"ee9e5672":"import numpy as np\nimport h5py\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (5.0, 4.0) # se fija el tama\u00f1o de los gr\u00e1ficos\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n%load_ext autoreload\n%autoreload 2\n\nnp.random.seed(1)     # se utiliza para replicar las funciones aleatorias ","56e3e915":"from shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"..\/input\/dnn_utils_v2.py\", dst = \"..\/working\/dnn_utils_v2.py\")\ncopyfile(src = \"..\/input\/testCases_v4.py\", dst = \"..\/working\/testCases_v4.py\")\n\n# import all our functions\n\nfrom testCases_v4 import *\nfrom dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward","b5945513":"# FUNCI\u00d3N A CALIFICAR: initialize_parameters\n\ndef initialize_parameters(n_x, n_h, n_y):\n    \"\"\"\n    Input:\n    n_x: tama\u00f1o de la capa de entrada\n    n_h: tama\u00f1o de la capa escondida\n    n_y: tama\u00f1o de la capa de salida\n    Output:\n    parameters: diccionario python con los parametros parameters:\n                    W1: matriz de pesos con dimensiones (n_h, n_x)\n                    b1: matriz de sesgos con dimensiones (n_h, 1)\n                    W2: matriz de pesos con dimensiones (n_y, n_h)\n                    b2: matriz de sesgos con dimensiones (n_y, 1)\n    \"\"\"\n    \n    np.random.seed(1)\n    \n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 4 l\u00edneas de c\u00f3digo)\n    W1 = \n    b1 = \n    W2 = \n    b2 =\n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    assert(W1.shape == (n_h, n_x))\n    assert(b1.shape == (n_h, 1))\n    assert(W2.shape == (n_y, n_h))\n    assert(b2.shape == (n_y, 1))\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters    ","f0d040a8":"parameters = initialize_parameters(3,2,1)\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))","0a6b7274":"# FUNCI\u00d3N A CALIFICAR: initialize_parameters_deep\n\ndef initialize_parameters_deep(layer_dims):\n    \"\"\"\n    Input:\n    layer_dims: arreglo (lista) de python con las dimensiones de cada capa de la red\n    Output:\n    parameters: diccionario python con los parametros \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    Wl: matriz de pesos (layer_dims[l], layer_dims[l-1])\n                    bl: vector de sesgo (layer_dims[l], 1)\n    \"\"\"\n    \n    np.random.seed(3)\n    parameters = {}\n    L = len(layer_dims)            # n\u00famero de capas de la red\n\n    for l in range(1, L):\n        ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 2 l\u00edneas de c\u00f3digo)\n        parameters['W' + str(l)] = \n        parameters['b' + str(l)] = \n        ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n        \n        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n\n        \n    return parameters","3e5524ea":"parameters = initialize_parameters_deep([5,4,3])\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))","170d8b29":"# FUNCI\u00d3N A CALIFICAR: linear_forward\n\ndef linear_forward(A, W, b):\n    \"\"\"\n    Implemente la parte lineal para la propagaci\u00f3n hacia delante de una capa.\n    Input:\n    A: las activaciones de la capa previa (o los datos de entrada): (tama\u00f1o de la capa previa, n\u00famero de ejemplos)\n    W: matriz de pesos, un arreglo numpy de dimensiones (tama\u00f1o de la capa actual, tama\u00f1o de la capa previa)\n    b: vector de sesgo, un arreglo numpy de dimensiones (tama\u00f1o de la capa actual, 1)\n    Output:\n    Z: la entrada para la funci\u00f3n de activaci\u00f3n, tambi\u00e9n llamado par\u00e1metro de pre-activaci\u00f3n \n    cache: diccionario python con \"A\", \"W\" y \"b\", almacenados para computar los pasos hacia atr\u00e1s de manera eficiente\n    \"\"\"\n    \n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 1 l\u00ednea de c\u00f3digo)\n    Z = \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    assert(Z.shape == (W.shape[0], A.shape[1]))\n    cache = (A, W, b)\n    \n    return Z, cache","fe974530":"A, W, b = linear_forward_test_case()\n\nZ, linear_cache = linear_forward(A, W, b)\nprint(\"Z = \" + str(Z))","562a35d9":"# FUNCI\u00d3N A CALIFICAR: linear_activation_forward\n\ndef linear_activation_forward(A_prev, W, b, activation):\n    \"\"\"\n    Implemente la propagaci\u00f3n hacia delante para la capa LINEAL->ACTIVACION\n    Input:\n    A_prev: activaciones de la capa previa (o de los datos de entrada): (tama\u00f1o de la capa previa, n\u00famero de ejemplos)\n    W: matriz de pesos, un arreglo numpy de dimensiones (tama\u00f1o de la capa actual, tama\u00f1o de la capa previa)\n    b: vector de sesgo, un arreglo numpy de dimensiones (tama\u00f1o de la capa actual, 1)\n    activation: la activaci\u00f3n a ser usada en la capa, guardada como una cadena de texto: \"sigmoid\" or \"relu\"\n    Output:\n    A: la salida de la funci\u00f3n de activaci\u00f3n, tambi\u00e9n llamada valor de post-activacion \n    cache: dicionario python con la \"cache_lineal\" y la \"cache_activacion\"\n    \"\"\"\n    \n    if activation == \"sigmoid\":\n        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n        ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 2 l\u00edneas de c\u00f3digo)\n        Z, linear_cache = \n        A, activation_cache = \n        ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    elif activation == \"relu\":\n        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n        ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 2 l\u00edneas de c\u00f3digo)\n        Z, linear_cache = \n        A, activation_cache = \n        ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n    cache = (linear_cache, activation_cache)\n\n    return A, cache","62951799":"A_prev, W, b = linear_activation_forward_test_case()\n\nA, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\nprint(\"Con sigmoide: A = \" + str(A))\n\nA, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\nprint(\"Con ReLU: A = \" + str(A))","97174ca7":"# FUNCI\u00d3N A CALIFICAR: L_model_forward\n\ndef L_model_forward(X, parameters):\n    \"\"\"\n    Implemente la propagaci\u00f3n hacia delante para calcular [LINEAL->RELU]*(L-1)->LINEAL->SIGMOIDE\n    Input:\n    X: datos de entrada, arreglo de tama\u00f1o (tama\u00f1o del input, n\u00famero de ejemplos)\n    parameters: salida de initialize_parameters_deep()\n    Output:\n    AL: \u00faltimo valor de post-activaci\u00f3n\n    caches: lista de caches con cada cach\u00e9 de linear_activation_forward() (hay L-1 cach\u00e9s, indexadas de 0 a L-1)\n    \"\"\"\n\n    caches = []\n    A = X\n    L = len(parameters) \/\/ 2                  # n\u00famero de capas en la red neuronal\n    \n    # Implemente [LINEAL -> RELU]*(L-1). A\u00f1ada \"cache\" a la lista de \"caches\".\n    for l in range(1, L):\n        A_prev = A \n        ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 2 l\u00edneas de c\u00f3digo)\n        A, cache = \n                        \n        ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    # Implemente LINEAL -> SIGMOIDE. A\u00f1ada \"cache\" a la lista de \"caches\".\n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 2 l\u00edneas de c\u00f3digo)\n    AL, cache =\n    \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    assert(AL.shape == (1,X.shape[1]))\n            \n    return AL, caches","b0cd8a29":"X, parameters = L_model_forward_test_case_2hidden()\nAL, caches = L_model_forward(X, parameters)\nprint(\"AL = \" + str(AL))\nprint(\"Longitud de la lista de caches = \" + str(len(caches)))","9353e2e7":"# FUNCI\u00d3N A CALIFICAR: compute_cost\n\ndef compute_cost(AL, Y):\n    \"\"\"\n    Implemente la funci\u00f3n de coste por entrp\u00eda cruzada.\n    Input:\n    AL: vector con las probabilidades para las etiquetas de predicci\u00f3n, dimensiones (1, n\u00famero de ejemplos)\n    Y: vector de etiquetas observadas, de dimensi\u00f3n (1, n\u00famero de ejemplos)\n    Output:\n    coste: coste de entrop\u00eda cruzada\n    \"\"\"\n    \n    m = Y.shape[1]\n\n    # Compute la p\u00e9rdida de AL e Y.\n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 1 l\u00ednea de c\u00f3digo)\n    cost = \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    cost = np.squeeze(cost)      # Para asegurar que la dimensi\u00f3n de se coste es correcta (e.g. [[17]] se torna en 17).\n    assert(cost.shape == ())\n    \n    return cost","6a164610":"Y, AL = compute_cost_test_case()\n\nprint(\"coste = \" + str(compute_cost(AL, Y)))","940fcab5":"# FUNCI\u00d3N A CALIFICAR: linear_backward\n\ndef linear_backward(dZ, cache):\n    \"\"\"\n    Implemente la parte lineal de la retro-propagaci\u00f3n para una sola capa [l]\n    Input:\n    dZ: Gradiente del coste con respecto al output lineal de la capa actual\n    cache: conjunto de velores (A_prev, W, b) provenientes de la propagaci\u00f3n hacia delante en la capa actual\n    Output:\n    dA_prev: Gradiente del coste con respecto a la activaci\u00f3n (de la capa previa: l-1), del mismo tama\u00f1o como A_prev\n    dW: Gradiente del coste con respecto a W (de la capa actual: l), del mismo tama\u00f1o que W\n    db: Gradiente del coste con respecto a b (de la capa actual: l), del mismo tama\u00f1o que b\n    \"\"\"\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n\n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 3 l\u00edneas de c\u00f3digo)\n    dW = \n    db = \n    dA_prev = \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    assert (dA_prev.shape == A_prev.shape)\n    assert (dW.shape == W.shape)\n    assert (db.shape == b.shape)\n    \n    return dA_prev, dW, db","24febc94":"dZ, linear_cache = linear_backward_test_case()\n\ndA_prev, dW, db = linear_backward(dZ, linear_cache)\nprint (\"dA_prev = \"+ str(dA_prev))\nprint (\"dW = \" + str(dW))\nprint (\"db = \" + str(db))","23135f76":"# FUNCI\u00d3N A CALIFICAR: linear_activation_backward\n\ndef linear_activation_backward(dA, cache, activation):\n    \"\"\"\n    Implemente la retro-propagaci\u00f3n par la capa LINEAL->ACTIVACION .\n    Input:\n    dA: gradiente post-activacion para la capa actual l \n    cache: conjunto de valores (linear_cache, activation_cache) que se guardan para calcular la retro-propagaci\u00f3n de manera eficiente\n    activacion: la activaci\u00f3n a ser usada en esta capa, guardada como un arreglo de texto: \"sigmoid\" o \"relu\"\n    Output:\n    dA_prev: gradiente del coste con respecto a la activaci\u00f3n (de la capa previa l-1), de las mismas dimensiones que A_prev\n    dW: gradiente del coste con respecto a W (capa actual l), mismas dimensiones que W\n    db: gradiente del coste con respecto a b (capa actual l), mismas dimensiones que b\n    \"\"\"\n    linear_cache, activation_cache = cache\n    \n    if activation == \"relu\":\n        ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 2 l\u00edneas de c\u00f3digo)\n        dZ = \n        dA_prev, dW, db = \n        ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n        \n    elif activation == \"sigmoid\":\n        ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 2 l\u00edneas de c\u00f3digo)\n        dZ = \n        dA_prev, dW, db = \n        ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    return dA_prev, dW, db","20ec189a":"dAL, linear_activation_cache = linear_activation_backward_test_case()\n\ndA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\nprint (\"sigmoid:\")\nprint (\"dA_prev = \"+ str(dA_prev))\nprint (\"dW = \" + str(dW))\nprint (\"db = \" + str(db) + \"\\n\")\n\ndA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\nprint (\"relu:\")\nprint (\"dA_prev = \"+ str(dA_prev))\nprint (\"dW = \" + str(dW))\nprint (\"db = \" + str(db))","c504dc91":"# FUNCI\u00d3N A CALIFICAR: L_model_backward\n\ndef L_model_backward(AL, Y, caches):\n    \"\"\"\n    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n    Input:\n    AL: vector con las probabilidades, salida para propagaci\u00f3n hacia delante L_model_forward()\n    Y: vector de clases\/etiquetas observadas, de dimensi\u00f3n (1, n\u00famero de ejemplos)\n    caches: lista de caches, donde se tiene\n                - cada cache de linear_activation_forward() con \"relu\" (i.e., caches[l]; l = 0...L-2)\n                - el cache de linear_activation_forward() con \"sigmoid\" (i.e, [L-1])\n    Output:\n    grads: Un diccionario con los gradientes\n             grads[\"dA\" + str(l)] = ... \n             grads[\"dW\" + str(l)] = ...\n             grads[\"db\" + str(l)] = ... \n    \"\"\"\n    grads = {}\n    L = len(caches) # n\u00famero de capas\n    m = AL.shape[1]\n    Y = Y.reshape(AL.shape) # Y es del mismo tama\u00f1o que AL\n    \n    # Initializacion de la retro-propagaci\u00f3n\n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (1 l\u00ednea de c\u00f3digo)\n    dAL =  # derivada del coste con respecto a AL\n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    # Gradientes para la ultima capa L (SIGMOIDE -> LINEAL). Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (2 l\u00edneas de c\u00f3digo)\n    current_cache = \n    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    #Loop from l=L-2 to l=0\n    for l in reversed(range(L-1)):\n        # Gradientes para la l-\u00e9sima capa: gradientes (RELU -> LINEAL).\n        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n        ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (5 l\u00edneas de c\u00f3digo)\n        current_cache = \n        dA_prev_temp, dW_temp, db_temp = \n        grads[\"dA\" + str(l)] = \n        grads[\"dW\" + str(l + 1)] = \n        grads[\"db\" + str(l + 1)] = \n        ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n\n    return grads","1283f774":"AL, Y_assess, caches = L_model_backward_test_case()\ngrads = L_model_backward(AL, Y_assess, caches)\nprint_grads(grads)","eb54ef02":"# FUNCI\u00d3N A CALIFICAR: update_parameters\n\ndef update_parameters(parameters, grads, learning_rate):\n    \"\"\"\n    Actualice los parametros utilizando GD\n    Input: \n    parameters: diccionario python con los parametros \n    grads: diccionario python con los gradientes, resultado de L_model_backward\n    Output:\n    parameters: diccionario python con los parametros actualizados \n                  parameters[\"W\" + str(l)] = ... \n                  parameters[\"b\" + str(l)] = ...\n    \"\"\"\n    \n    L = len(parameters) \/\/ 2 # numero de capas en la red neuronal\n\n    # Regla de actualizaci\u00f3n para cada par\u00e1metro (utilice on bucle for).\n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 3 l\u00edneas de c\u00f3digo)\n    for l in range(L):\n        parameters[\"W\" + str(l+1)] = \n        parameters[\"b\" + str(l+1)] = \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    return parameters","0c82e533":"parameters, grads = update_parameters_test_case()\nparameters = update_parameters(parameters, grads, 0.1)\n\nprint (\"W1 = \"+ str(parameters[\"W1\"]))\nprint (\"b1 = \"+ str(parameters[\"b1\"]))\nprint (\"W2 = \"+ str(parameters[\"W2\"]))\nprint (\"b2 = \"+ str(parameters[\"b2\"]))","b466f12e":"**Ejercicio**: Utilice las tres f\u00f3rmulas (arriba) para implementar linear_backward().","568b0ae7":"**Salida esperada**\n\n<table style=\"width:60%\">\n  \n  <tr>\n    <td > dW1 <\/td> \n           <td > [[ 0.41010002  0.07807203  0.13798444  0.10502167]\n [ 0.          0.          0.          0.        ]\n [ 0.05283652  0.01005865  0.01777766  0.0135308 ]] <\/td> \n  <\/tr> \n  \n    <tr>\n    <td > db1 <\/td> \n           <td > [[-0.22007063]\n [ 0.        ]\n [-0.02835349]] <\/td> \n  <\/tr> \n  \n  <tr>\n  <td > dA1 <\/td> \n           <td > [[ 0.12913162 -0.44014127]\n [-0.14175655  0.48317296]\n [ 0.01663708 -0.05670698]] <\/td> \n\n  <\/tr> \n<\/table>\n\n","76da26c9":"## 4 - Propagaci\u00f3n hacia delante\n\n### 4.1 - Lineal hacia delante \nUna vez inicializados los par\u00e1metros, debe implementar la propagaci\u00f3n hacia delante. Va a empezar por implementar algunas funciones b\u00e1sicas para ser utilizadas m\u00e1s adelante en la implementaci\u00f3n del modelo. Va a implementar 3 funciones:\n\n- LINEAL\n- LINEAL -> ACTIVACION donde la activaci\u00f3n ser\u00e1 ReLU o Sigmoide. \n- [LINEAL -> RELU] $\\times$ (L-1) -> LINEAL -> SIGMOIDE (modelo completo)\n\nEsta implementaci\u00f3n (vectorizada) de la propagaci\u00f3n hacia delante calcula las siguientes ecuaciones:\n\n$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$$\n\ndonde $A^{[0]} = X$. \n\n**Ejercicio**: Construya la parte LINEAL de la propagaci\u00f3n hacia delante.\n\n**Ayuda**:\nLa representaci\u00f3n matem\u00e1tica de esta implementaci\u00f3n para una capa es $Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$. Puede ser \u00fatil la funci\u00f3n`np.dot()`. Tambi\u00e9n, si las dimensiones no casan, puede investigar lo que ocurre llamando a `W.shape`.","26c492b9":"**Ejercicio**: Implemente `update_parameters()` para actualizar los par\u00e1mtros usando GD.\n\n**Instrucciones**:\nActualice los par\u00e1metros utilizando GD en cada $W^{[l]}$ y $b^{[l]}$; $l = 1, 2, ..., L$. \n","8dce9056":"### 6.3 - Retro-propagaci\u00f3n en L capas \n\nAhora va a implementar la funci\u00f3n de retro-propagaci\u00f3n para toda la red neuronal. Recuerde que cuando implement\u00f3 la funci\u00f3n `L_model_forward`, en cada iteraci\u00f3n, guard\u00f3 una cach\u00e9 que conten\u00eda (X,W,b, z). En el paso e la retro-propagaci\u00f3n, esas variables est\u00e1n a disposici\u00f3n para calcular los gradientes. Por lo tanto, en la funci\u00f3n `L_model_backward`, se puede iterar sobre todas las capas escondidas hacia atr\u00e1s, empezando de la \u00faltima capa $L$. \n\nEn cada paso hacia atr\u00e1s, se utilizan los valores de la cach\u00e9 en la capa $l$, para retro-propagar sobre la capa $l$. \n\n** Inicializando la retro-propagaci\u00f3n**:\nPara retro-propagar sobre esta red, sabemos que la salida es, \n$A^{[L]} = \\sigma(Z^{[L]})$. Por lo tanto, necesita calcular `dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$.\nPara hacerlo, utilice la f\u00f3rmula:\n```python\ndAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivada del coste con respecto a AL\n```\n\nPuede utilizar el gradiente de post-activacion `dAL` para seguir yendo hacia atr\u00e1s. Este gradiente se le puede pasar la funci\u00f3n hacia atr\u00e1s LINEAL->SIGMOIDE implementada antes (que utilizar\u00e1n los valores guaradados por la funci\u00f3n L_model_forward). Luego se debe utilizar un bucle `for` para iterar sobre todas las otras capas utilizando la funci\u00f3n hacia atr\u00e1s LINEAL->RELU. \n\nSe debe guardar cada dA, dW, y db en el diccionario grads. Para hacerlo, utilice la f\u00f3rmula : \n\n$$grads[\"dW\" + str(l)] = dW^{[l]}$$\n\nPor ejemplo, para $l=3$ se guardar\u00eda $dW^{[l]}$ en `grads[\"dW3\"]`.\n\n**Ejercicio**: Implemente la retro-propagaci\u00f3n para el modelo *[LINEAL->RELU] $\\times$ (L-1) -> LINEAL -> SIGMOIDE*.","4d529c4c":"## 6 - Retro-propagaci\u00f3n\n\nComo en el caso de la propagaci\u00f3n hacia delante, va a implementar funciones auxiliares para la retro-propagaci\u00f3n. Recuerde que la retro-propagaci\u00f3n permite calcular el gradiente de la funci\u00f3n de coste con respecto a los par\u00e1metros. \n\nAn\u00e1logamente a la propgaci\u00f3n hacia delante, la retro-propagaci\u00f3n se va a construir en tres pasos:\n- LINEAL hacia atr\u00e1s\n- LINEAL -> ACTIVACION hacia atr\u00e1s, donde ACTIVACION calcula la derivada de la funci\u00f3n de activaci\u00f3n (ReLU o sigmoide)\n- [LINEAL -> RELU] $\\times$ (L-1) -> LINEAL -> SIGMOIDE hacia atr\u00e1s (modelo completo)\n\n","4db1a43f":"### 6.1 - Linear hacia atr\u00e1s\n\nPara la capa $l$, la parte lineal es: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (seguida por una activaci\u00f3n).\n\nSuponga que ya ha calculado la derivada $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. Ahora quiere obtener $(dW^{[l]}, db^{[l]} dA^{[l-1]})$.\n\n\nLos tres outputs $(dW^{[l]}, db^{[l]}, dA^{[l]})$ son computados utilizando el input $dZ^{[l]}$. \n\nEstas son la f\u00f3rmulas que necesita:\n$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$\n$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$\n","81aaac75":"# Construcci\u00f3n de una red profunda\n\nPreviamente hemos entrenado una red neuronal de dos capas (con una capa escondida), Ahora vamos a cosntruir una red profunda, con m\u00faltiples capas escondidas.\n\n- En este taller, se van a implementar las funciones requeridas para construir una red neuronal profunda.\n- Luego se podr\u00e1n utilizar estas funciones para construir una red neuronal profunda para clasificaci\u00f3n de imagenes.\n\n**Luego de este taller usted va a haber aprendido a:**\n- Utilizar unidades no-lineales mediante una funci\u00f3n como ReLU para mejorar el modelo\n- Construir una red neuronal profunda (con m\u00e1s de una capa escondida)\n- Implementar de manera pr\u00e1ctica una red neuronal\n\n**Notaci\u00f3n**:\n- Super\u00edndice $[l]$ denota una cantidad asociada con la $l-\u00e9sima$ capa. \n    - Ejemplo: $a^{[l]}$ es la activaci\u00f3n de la $l-\u00e9sima$ capa. $W^{[l]}$ y $b^{[l]}$ son los par\u00e1metros de la $l-\u00e9sima$ capa.\n- Super\u00edndice $(i)$ denota una cantidad asociada con el $i-\u00e9simo$ ejemplo. \n    - Ejemplo: $x^{(i)}$ es el $i-\u00e9simo$ ejemplo de entrenamiento.\n- Sub\u00edndice $i$ denota la $i-\u00e9sima$ entrada de un vector.\n    - Ejemplo: $a^{[l]}_i$ denota la $i-\u00e9sima$ entrada de las activaciones de la $l-\u00e9sima$ capa.","ae072959":"### 6.2 - Activaci\u00f3n-lineal hacia atr\u00e1s\n\nA continuaci\u00f3n, va a crear una funci\u00f3n que combine la dos funciones auxiliares: **`linear_backward`** y el paso hacia atr\u00e1s de la activaci\u00f3n **`linear_activation_backward`**. \n\nPara implementar `linear_activation_backward`, se provee de dos funciones hacia atr\u00e1s:\n- **`sigmoid_backward`**: Implementa retro-propagaci\u00f3n para la unidad SIGMOIDE, tal que \n\n```python\ndZ = sigmoid_backward(dA, activation_cache)\n```\n\n- **`relu_backward`**: Implementa retro-propagaci\u00f3n para la unidad RELU, de forma que \n\n```python\ndZ = relu_backward(dA, activation_cache)\n```\n\nSi $g(.)$ es la funci\u00f3n de activaci\u00f3n, entonces \n`sigmoid_backward` y `relu_backward` calculan $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$.  \n\n**Ejercicio**: Implemente la retro-propagaci\u00f3n para la capa *LINEAL->ACTIVACION*.","2581828c":"**Ejercicio**: Implemente la inicializaci\u00f3n de una red neuronal con L capas. \n\n**Instrucciones**:\n- La estructura del modelo es *[LINEAL -> RELU] $ \\times$ (L-1) -> LINEAL -> SIGMOIDE*. Esto es, la red tiene $L-1$ capas utilizando una funci\u00f3n de activaci\u00f3n ReLU, seguido de una capa de salida con la funci\u00f3n de activaci\u00f3n Sigmoide.\n- Use una inicializaci\u00f3n aleatoria para las matrices de pesos. Utilice `np.random.randn(shape) * 0.01`.\n- Use una inicializaci\u00f3n de ceros para los sesgos. Utilice `np.zeros(shape)`.\n- El n\u00famero de unidades en cada capa $n^{[l]}$, se guarda en la variable `layer_dims`. De esta manera, por ejemplo, las `layer_dims` para el taller de la semana pasada con \"Un red neuronal sencilla\" ser\u00eda [2,4,1]: donde hay 2 entradas, una capa escondida con 4 unidades, y una capa de salida con una unidad. Por lo tanto, la forma de `W1` es de (4,2), la de `b1` (4,1), `W2` (1,4) y `b2` (1,1). Ahora se puede generalizar para $L$ capas! \n\n\n**Ayuda**:\nLa implementaci\u00f3n para $L=1$ ser\u00eda de la siguiente manera:\n```python\n    if L == 1:\n        parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n        parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))\n```","22d18374":"**Salida esperada**:\n\n<table style=\"width:35%\">\n  \n  <tr>\n    <td> **Z** <\/td>\n    <td> [[ 3.26295337 -1.23429987]] <\/td> \n  <\/tr>\n  \n<\/table>","ce2a90ef":"**Salida esperada**:\n\n<table style=\"width:50%\">\n  <tr>\n    <td> **AL** <\/td>\n    <td > [[ 0.03921668  0.70498921  0.19734387  0.04728177]]<\/td> \n  <\/tr>\n  <tr>\n    <td> **Longitud de la lista de caches ** <\/td>\n    <td > 3 <\/td> \n  <\/tr>\n<\/table>","6d67cb56":"\n## 7 - Conclusi\u00f3n\n\nCon este taller ya ha consturido todas las funciones requeridas para construir una red neuronal profunda! \n\nUna vez logrado esto, el siguiente taller ser\u00e1 m\u00e1s directo. En el pr\u00f3ximo taller va a construir dos modelos:\n- Una red nueronal de dos capas\n- Una red neuronal de L capas\n\nEspero te haya gustado este Notebook. Por favor compartelo para que entre todos aprendamos juntos.\n\nPuedes Seguirme a mi cuenta en Twitter **[@andres_jejen](https:\/\/twitter.com\/andres_jejen)** Constantemente comparto noticias y contenido educativo sibre Machine Learning, Big Data y Data Science.","525e269d":"## 2 - Resumen del taller\n\nPara construir la red neuronal, primero se construir\u00e1n funciones auxiliares que permitir\u00e1n implementar una red neuronal de dos y de L capas. Cada funci\u00f3n auxiliar se puede construir siguiendo las instrucciones: \n\n- Inicializar los par\u00e1metros para una red de 2 capas y para una red de $L$ capas.\n- Implementar la propagaci\u00f3n hacia delante.\n     - Desarrolle la propagaci\u00f3n LINEAL hacia delante de una capa, obteniendo $Z^{[l]}$, y luego la funci\u00f3n de ACTIVACION, RELU o SIGMOIDE.\n     - Combine los pasos [LINEAL->ACTIVACION] en una sola funci\u00f3n (hacia delante).\n     - Agrupe las funciones de propagaci\u00f3n hacia delante [LINEAL->RELU] L-1 veces (para las capas 1 hasta L-1) y a\u00f1ada una [LINEAL->SIGMOIDE] al final (para la \u00faltima capa $L$). De esta manera obtendr\u00e1 la funci\u00f3n L_model_forward.\n- Compute la p\u00e9rdida.\n- Implemente la retro-propagaci\u00f3n.\n    - Complete la parte LINEAL de la retro-propagaci\u00f3n de una capa (el gradiente de la funci\u00f3n de activaci\u00f3n le va a ser proporcionado). \n    - Combine los pasos en una nueva funci\u00f3n de retro-propagaci\u00f3n [LINEAL->ACTIVACION].\n    - Agrupe la funciones de retro-propagaci\u00f3n [LINEAL->RELU] L-1 veces y a\u00f1ada la correspondiente [LINEAL->SIGMOIDE] en una nueva funci\u00f3n L_model_backward.\n- Por \u00faltimo, actualice los par\u00e1metros.\n\n**Anotaci\u00f3n:** cada funci\u00f3n de propagaci\u00f3n hacia delante tiene su correspondiente funci\u00f3n hacia atr\u00e1s. Por ello, a cada paso hacia delante, se guardan en la cach\u00e9 algunos valores necesarios para calcular los gradientes.","7720397d":"**Salida esperada**:\n       \n<table style=\"width:35%\">\n  <tr>\n    <td> **With sigmoid: A ** <\/td>\n    <td > [[ 0.96890023  0.11013289]]<\/td> \n  <\/tr>\n  <tr>\n    <td> **With ReLU: A ** <\/td>\n    <td > [[ 3.43896131  0.        ]]<\/td> \n  <\/tr>\n<\/table>\n","8e753467":"**Salida esperada con el sigmoide:**\n\n<table style=\"width:100%\">\n  <tr>\n    <td > dA_prev <\/td> \n           <td >[[ 0.11017994  0.01105339]\n [ 0.09466817  0.00949723]\n [-0.05743092 -0.00576154]] <\/td> \n\n  <\/tr> \n  \n    <tr>\n    <td > dW <\/td> \n           <td > [[ 0.10266786  0.09778551 -0.01968084]] <\/td> \n  <\/tr> \n  \n    <tr>\n    <td > db <\/td> \n           <td > [[-0.05729622]] <\/td> \n  <\/tr> \n<\/table>\n\n","82923432":"**Salida esperada**:\n       \n<table style=\"width:80%\">\n  <tr>\n    <td> **W1** <\/td>\n    <td>[[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]<\/td> \n  <\/tr>\n  \n  <tr>\n    <td>**b1** <\/td>\n    <td>[[ 0.]\n [ 0.]\n [ 0.]\n [ 0.]]<\/td> \n  <\/tr>\n  \n  <tr>\n    <td>**W2** <\/td>\n    <td>[[-0.01185047 -0.0020565   0.01486148  0.00236716]\n [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n [-0.00768836 -0.00230031  0.00745056  0.01976111]]<\/td> \n  <\/tr>\n  \n  <tr>\n    <td>**b2** <\/td>\n    <td>[[ 0.]\n [ 0.]\n [ 0.]]<\/td> \n  <\/tr>\n  \n<\/table>","3dc9b81c":"## 3 - Inicializaci\u00f3n\n\nDesarrolle dos funciones auxiliares para inicializar los par\u00e1metros de su modelo. La primera funci\u00f3n permitir\u00e1 inicializar los par\u00e1metros para un modelo con dos capas. La segunda permitir\u00e1 generalizar el proceso de inicializaci\u00f3n para $L$ capas.\n\n### 3.1 - Red neuronal con 2 capas\n\n**Ejercicio**: Crear e inicializar los par\u00e1metros para una red de 2 capas.\n\n**Instrucciones**:\n- La estructura del modelo es *LINEAL -> RELU -> LINEAL -> SIGMOIDE*. \n- Inicializar aleatoriamente los pesos. Puede utilizar la funci\u00f3n `np.random.randn(dimensiones)*0.01` con las dimensiones correctas.\n- Inicialice los sesgos a cero. Puede utilizar la funci\u00f3n `np.zeros(dimensiones)`.","5c48cd10":"**Salida esperada**\n\n<table style=\"width:100%\"> \n    <tr>\n    <td > W1 <\/td> \n           <td > [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n [-1.0535704  -0.86128581  0.68284052  2.20374577]] <\/td> \n  <\/tr> \n  \n    <tr>\n    <td > b1 <\/td> \n           <td > [[-0.04659241]\n [-1.28888275]\n [ 0.53405496]] <\/td> \n  <\/tr> \n  <tr>\n    <td > W2 <\/td> \n           <td > [[-0.55569196  0.0354055   1.32964895]]<\/td> \n  <\/tr> \n  \n    <tr>\n    <td > b2 <\/td> \n           <td > [[-0.84610769]] <\/td> \n  <\/tr> \n<\/table>\n","a9c533fa":"### d) Modelo con L capas \n\nCon el fin de facilitar la implementaci\u00f3n de la red neuronal de $L$ capas que queremos implementar, necesitamos una funci\u00f3n que replique la propagaci\u00f3n hacia delante (`linear_activation_forward`) con RELU, $L-1$ veces, seguida por la funci\u00f3n (`linear_activation_forward`) SIGMOIDE.\n\n\n**Ejercicio**: Implemente la propagaci\u00f3n hacia delante del modelo descrito anteriormente.\n\n**Instrucciones**: En el c\u00f3digo abajo, la variable `AL` denota $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$ (Esta es la estimaci\u00f3n $\\hat{Y}$.) \n\n**Ayuda**:\n- Use las funciones que ha programada arriba \n- Use un bucle for para replicar la [LINEAL->RELU] (L-1) veces\n- No olvide ir guardando las caches en la lista \"caches\". Para a\u00f1adir un nuevo valor use `list.append(c)`.","30f61a16":"**Salida esperada con RELU:**\n\n<table style=\"width:100%\">\n  <tr>\n    <td > dA_prev <\/td> \n           <td > [[ 0.44090989  0.        ]\n [ 0.37883606  0.        ]\n [-0.2298228   0.        ]] <\/td> \n\n  <\/tr> \n  \n    <tr>\n    <td > dW <\/td> \n           <td > [[ 0.44513824  0.37371418 -0.10478989]] <\/td> \n  <\/tr> \n  \n    <tr>\n    <td > db <\/td> \n           <td > [[-0.20837892]] <\/td> \n  <\/tr> \n<\/table>\n\n","4386f660":"Muy bien, llegado a este punto ya tiene todo el proceso de propagaci\u00f3n hacia delente completo, tomando el input X y obteniendo outputs del vector-fila $A^{[L]}$ con sus predicciones (a partir de lo cual puede calcular el coste o p\u00e9rdida de sus predicciones). Tambi\u00e9n se ha quedado con los valores intermedios en \"caches\". ","9c26dbae":"## 5 - Funci\u00f3n de p\u00e9rdida o coste\n\nAhora va a implementar la propagaci\u00f3n hacia delate y hacia atr\u00e1s. Debe computar el coste con el fin de verificar si su modelo en verdad est\u00e1 aprendiendo.\n\n**Ejercicio**: Calcule el coste por entrop\u00eda-cruzada $J$, en base a la siguiente f\u00f3rmula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))\u00a0$$\n","fc7c0aed":"### 3.2 - Red Neuronal con L capas\n\nLa inicializaci\u00f3n de una red neuronal profunda es m\u00e1s compleja al haber m\u00e1s matrices de pesos y vectores de sesgo. Al completar `initialize_parameters_deep`, debe asegurarse que sus dimensiones sean coherentes al pasar de capa en capa. Recuerde que  $n^{[l]}$ es el n\u00famero de unidades en la capa $l$. Entonces, e.g., si el tama\u00f1o de la entrada $X$ es $(12288, 209)$ (con $m=209$ ejemplos), se tiene que:\n\n<table style=\"width:100%\">\n\n\n    <tr>\n        <td>  <\/td> \n        <td> **Dimensi\u00f3n de W** <\/td> \n        <td> **Dimensi\u00f3n de b**  <\/td> \n        <td> **Activaci\u00f3n** <\/td>\n        <td> **Dimensi\u00f3n de la activaci\u00f3n** <\/td> \n    <tr>\n    \n    <tr>\n        <td> **Capa 1** <\/td> \n        <td> $(n^{[1]},12288)$ <\/td> \n        <td> $(n^{[1]},1)$ <\/td> \n        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ <\/td> \n        \n        <td> $(n^{[1]},209)$ <\/td> \n    <tr>\n    \n    <tr>\n        <td> **Capa 2** <\/td> \n        <td> $(n^{[2]}, n^{[1]})$  <\/td> \n        <td> $(n^{[2]},1)$ <\/td> \n        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ <\/td> \n        <td> $(n^{[2]}, 209)$ <\/td> \n    <tr>\n   \n       <tr>\n        <td> $\\vdots$ <\/td> \n        <td> $\\vdots$  <\/td> \n        <td> $\\vdots$  <\/td> \n        <td> $\\vdots$<\/td> \n        <td> $\\vdots$  <\/td> \n    <tr>\n    \n   <tr>\n        <td> **Capa L-1** <\/td> \n        <td> $(n^{[L-1]}, n^{[L-2]})$ <\/td> \n        <td> $(n^{[L-1]}, 1)$  <\/td> \n        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ <\/td> \n        <td> $(n^{[L-1]}, 209)$ <\/td> \n    <tr>\n    \n    \n   <tr>\n        <td> **Capa L** <\/td> \n        <td> $(n^{[L]}, n^{[L-1]})$ <\/td> \n        <td> $(n^{[L]}, 1)$ <\/td>\n        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$<\/td>\n        <td> $(n^{[L]}, 209)$  <\/td> \n    <tr>\n\n<\/table>\n\nRecuerde que en python, el c\u00e1lculo de $W X + b$ lleva a cabo la operaci\u00f3n de broadcasting.\n\nEntonces, si:\n\n$$ W = \\begin{bmatrix}\n    j  & k  & l\\\\\n    m  & n & o \\\\\n    p  & q & r \n\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n    a  & b  & c\\\\\n    d  & e & f \\\\\n    g  & h & i \n\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n    s  \\\\\n    t  \\\\\n    u\n\\end{bmatrix}$$\n\nel resultado de $WX + b$ ser\u00e1:\n\n$$ WX + b = \\begin{bmatrix}\n    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n\\end{bmatrix} $$","d0d0cef0":"**Nota**: En deep learning, la computaci\u00f3n de \"[LINEAL->ACTIVACION]\" se cuenta como una sola capa de la red neuronal.","81ff5276":"### 4.2 - Activaci\u00f3n-lineal hacia delante\n\nEn este taller, vamos a utilizar dos funciones de activaci\u00f3n:\n\n- **Sigmoide**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. Esta funci\u00f3n `sigmoid` est\u00e1 dada, y devuelve 2 objetos: el valor de activaci\u00f3n \"`a`\" y una \"`cache`\" que contiene \"`Z`\" (la cual se le pasa a la correspondiente funci\u00f3n de retro-propagaci\u00f3n). \n\nPara usarla basta con este comando: \n``` python\nA, activation_cache = sigmoid(Z)\n```\n\n- **ReLU**: La f\u00f3rmula matem\u00e1tica para ReLU es $A = RELU(Z) = max(0, Z)$. Esta funci\u00f3n `relu` tambi\u00e9n est\u00e1 dada, y devuelve 2 objetos: el valor de activaci\u00f3n \"`a`\" y una \"`cache`\" que contiene \"`Z`\" (la cual se le pasa a la correspondiente funci\u00f3n de retro-propagaci\u00f3n). \n\nPara usarla basta con este comando: \n``` python\nA, activation_cache = relu(Z)\n```","dd91892f":"**Salida esperada**:\n\n<table>\n\n    <tr>\n    <td>**coste** <\/td>\n    <td> 0.41493159961539694<\/td> \n    <\/tr>\n<\/table>","14871f6b":"**Salida esperada**:\n\n<table style=\"width:90%\">\n  <tr>\n    <td> **dA_prev** <\/td>\n    <td > [[ 0.51822968 -0.19517421]\n [-0.40506361  0.15255393]\n [ 2.37496825 -0.89445391]] <\/td> \n  <\/tr> \n  \n    <tr>\n        <td> **dW** <\/td>\n        <td > [[-0.10076895  1.40685096  1.64992505]] <\/td> \n    <\/tr> \n  \n    <tr>\n        <td> **db** <\/td>\n        <td> [[ 0.50629448]] <\/td> \n    <\/tr> \n    \n<\/table>\n\n","5aa65ef4":"### 6.4 - Actualizaci\u00f3n de par\u00e1metros\n\nEn esta secci\u00f3n, se actualizan los par\u00e1metros del modelo, utilizando el m\u00e9todo de Descenso en la Direcci\u00f3n del Gradiente (GD): \n\n$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n\ndonde $\\alpha$ es la tasa de aprendizaje. Tras la actualizaci\u00f3n de los parametros, se deben guardar en los parametros del diccionario. ","0c43e48f":"## 1. Paquetes\n\nPrimero se deben importar todos los paquetes que se van a necesitar durante este taller.\n- [numpy](www.numpy.org) paquete b\u00e1sico para ciencias computacionales con Python.\n- [matplotlib](http:\/\/matplotlib.org) librer\u00eda para graficar en Python.\n- dnn_utils provee distintas funciones que se van a usar durante el taller\n- testCases tiene los ejemplos de prueba para evaluar la implementacion de las funciones\n","72194750":"Para mayor conveniencia, vamos a agrupar dos funciones (Lineal y Activacion) en una sola (LINEAL->ACTIVACION). Por lo tanto, va a implementar una funci\u00f3n que da el paso LINEAL hacia delante seguido del paso de ACTIVACION hacia delante.\n\n**Ejercicio**: Implemente la propagaci\u00f3n hacia delante de la capa *LINEAL->ACTIVACION*. La ecuaci\u00f3n matem\u00e1tica es: $$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$$ donde la activaci\u00f3n \"g\" puede ser sigmoide() o relu(). Utilice linear_forward() y la funci\u00f3n de activaci\u00f3n correcta.","3c49a388":"**Salida esperada**:\n       \n<table style=\"width:80%\">\n  <tr>\n    <td> **W1** <\/td>\n    <td> [[ 0.01624345 -0.00611756 -0.00528172]\n [-0.01072969  0.00865408 -0.02301539]] <\/td> \n  <\/tr>\n\n  <tr>\n    <td> **b1**<\/td>\n    <td>[[ 0.]\n [ 0.]]<\/td> \n  <\/tr>\n  \n  <tr>\n    <td>**W2**<\/td>\n    <td> [[ 0.01744812 -0.00761207]]<\/td>\n  <\/tr>\n  \n  <tr>\n    <td> **b2** <\/td>\n    <td> [[ 0.]] <\/td> \n  <\/tr>\n  \n<\/table>"}}