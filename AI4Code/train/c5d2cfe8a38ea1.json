{"cell_type":{"5f2655d5":"code","51e19b35":"code","62085dce":"code","ceae4491":"code","542b19bd":"code","7cbb5efd":"code","6814f25b":"code","a11d7e59":"code","f84f08db":"code","0bd433c5":"code","b3fa0216":"code","d784c055":"code","8c74949d":"code","b301c770":"code","4ea19da8":"code","e27d1192":"code","4ae9f3d5":"code","08776416":"code","b3f8ded6":"code","39b149dd":"code","5d02decb":"code","98defc25":"code","b06b1ee1":"code","e6239d4d":"code","1e20c196":"code","9b5a437b":"code","fba18849":"code","87b7a9a7":"code","752bd1ab":"code","f1a047cf":"code","3d0a1c89":"code","d5ee4758":"code","f4bdc41b":"code","f1bdac3a":"code","e6ab0927":"code","d6f7c027":"code","b4cae817":"code","ab721da5":"code","7a5e53d7":"code","c4b3d3f5":"code","20531cc0":"code","ea40e5af":"code","948076a1":"code","058dd9eb":"code","f958e855":"code","f8137dd0":"code","25354ec1":"code","451696fb":"code","c063265d":"code","7db74445":"code","c3b07dc0":"code","e213b1ff":"code","60777df8":"markdown","71fe7765":"markdown","7dbc3bc1":"markdown","829d125e":"markdown","cbfd45d4":"markdown","f8131ade":"markdown","d0557ad9":"markdown","7cd37ce4":"markdown","bdbe0da7":"markdown","cda1f415":"markdown","dd9d56b1":"markdown","33015993":"markdown","2c2e85de":"markdown","bf05d719":"markdown","f73e7689":"markdown","d42dc157":"markdown","bc89178d":"markdown","800ce795":"markdown","580f62f1":"markdown","2da58275":"markdown","23bf5631":"markdown","7b105d72":"markdown","b06e6398":"markdown","adfe79d3":"markdown","c37b95c2":"markdown","e8d3d0ba":"markdown","6fab3043":"markdown","32e67de5":"markdown","269ca1dc":"markdown"},"source":{"5f2655d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm, skew \nfrom scipy import stats\nimport datetime\nimport ast\nimport json\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\n#Standardization\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir(\".\"))\n\n# Any results you write to the current directory are saved as output.","51e19b35":"df_train = pd.read_csv(\"..\/input\/tmdb-box-office-prediction\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/tmdb-box-office-prediction\/test.csv\")\n\nprint(df_train.index)\nprint(df_test.index)\n","62085dce":"df_all = pd.concat([df_train, df_test], sort=False).reset_index()\n\n\nprint(df_all.isnull().sum())","ceae4491":"import cv2\n\ndef loadPosterImages(df, _path_to_base):\n    \n    images = []\n    \n    for i in df.id:\n        path_to_img = _path_to_base + str(i) + \".jpeg\"\n        if os.path.exists(path_to_img) == False:\n            print(path_to_img + \" is null\")\n            image = np.zeros([64,64,3],dtype=np.uint8)\n        else:\n            image = cv2.imread(path_to_img)\n            image = cv2.resize(image, (64, 64))\n        \n        images.append(image)\n    \n    \n    return np.array(images)","542b19bd":"use_npy=0\n\nif use_npy != 1:\n    poster_train_img = loadPosterImages(df_train, \"..\/input\/tmdb-box-office-prediction-posters\/tmdb_box_office_prediction_posters\/tmdb_box_office_prediction_posters\/train\/\")\n    poster_test_img = loadPosterImages(df_test, \"..\/input\/tmdb-box-office-prediction-posters\/tmdb_box_office_prediction_posters\/tmdb_box_office_prediction_posters\/test\/\")\n\n    np.save('poster_train_img.npy', poster_train_img)\n    np.save('poster_test_img.npy', poster_test_img)\nelse:\n    #Loading images needs long time, so I save them as numpy binary and load from the npy files instead.\n    poster_train_img = np.load('poster_train_img.npy')\n    poster_test_img = np.load('poster_test_img.npy')\n\n\nposter_train_img = poster_train_img \/ 255.0\nposter_test_img = poster_test_img \/ 255.0","7cbb5efd":"print(df_train.loc[df_train[\"id\"] == 2303, \"title\"])\nprint(df_test.loc[df_test[\"id\"] == 3829, \"title\"])\nprint(df_test.loc[df_test[\"id\"] == 4925, \"title\"])\n\ndf_train.drop(df_train.loc[df_train[\"id\"] == 2303].index, inplace=True)\nprint(df_train.loc[df_train[\"id\"] == 2303, \"title\"])","6814f25b":"target_col_name = \"revenue\"\nsc = StandardScaler()","a11d7e59":"def visualize_distribution(y):\n    \n    plt.figure(figsize=(15, 8))\n    sns.distplot(y,fit=norm)\n    mu,sigma=norm.fit(y)\n    plt.legend([\"Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f})\".format(mu,sigma)])\n    plt.title(\"Distribution\")\n    plt.ylabel(\"Frequency\")\n    plt.show()\n    \n    \ndef visualize_probplot(y):\n    plt.figure(figsize=(15, 8))\n    stats.probplot(y,plot=plt)\n    plt.show()","f84f08db":"def showJointPlot(df, col_name):\n    \n\n    if not col_name in df.columns:\n        print(col_name + \" is not inside columns\")\n        return\n\n    print(\"***[\" + col_name + \"]***\")\n    print(\"describe : \")\n    print(df[col_name].describe())\n    print(\"skew : \")\n    print(df[col_name].skew())\n\n    #correlation\n    corrmat = df.corr()\n    num_of_col = len(corrmat.columns)\n    cols = corrmat.nlargest(num_of_col, col_name)[col_name]\n    print(\"*****[ corr : \" + col_name + \" ]*****\")\n    print(cols)\n    print(\"*****[\" + col_name + \"]*****\")\n    print(\"\\n\")\n\n\n    visualize_distribution(df[col_name].dropna())\n    visualize_probplot(df[col_name].dropna())\n\n\n    if col_name != target_col_name and target_col_name in df.columns:\n        plt.figure(figsize=(15, 8))\n        sns.jointplot(col_name, target_col_name, df)\n\n    print(\"******\\n\")\n    return ","0bd433c5":"def showValueCount(df, col_name):\n\n    if not col_name in df.columns:\n        print(col_name, \" is not inside columns\")\n        return\n\n    print(\"***[\" + col_name + \"]***\")\n    print(\"describe :\")\n    print(df[col_name].describe())\n\n    df_value = df[col_name].value_counts(dropna=False)\n    print(\"value_counts :\")\n    print(df_value)\n\n    plt.figure(figsize=(15,8))\n    sns.barplot(df_value.index, df_value.values, alpha=0.8)\n    plt.ylabel('Number of each element', fontsize=12)\n    plt.xlabel(col_name, fontsize=12)\n    plt.xticks(rotation=90, size='small')\n    plt.show()\n\n\n    if col_name != target_col_name and target_col_name in df.columns:\n        plt.figure(figsize=(15, 8))\n        plt.xticks(rotation=90, size='small')\n        sns.boxplot(x=df[col_name], y =df[target_col_name])\n        plt.show()\n\n    print(\"******\\n\")\n    return ","b3fa0216":"def countAndExpandFromDictToColumns(df, expanded_col_name, _val_name):\n    \n    df[expanded_col_name] = df[expanded_col_name].dropna().map(lambda x : ast.literal_eval(x))\n    \n    def connectToString(x, prefix, val_name):\n\n        str_names = []\n        for val in x:\n\n            str_names.append(prefix + \"_\" + val[val_name])\n\n        return \",\".join(str_names)\n\n\n    df[expanded_col_name] = df[expanded_col_name].dropna().map(lambda x : connectToString(x, prefix=expanded_col_name, val_name=_val_name))\n    #print(df[expanded_col_name].head())\n    df_tmp = df[expanded_col_name].dropna().str.get_dummies(sep=',')\n    #print(df_tmp.info())\n    df = pd.concat([df, df_tmp], axis=1, sort=False)\n\n    df.drop(expanded_col_name, axis=1, inplace=True)\n\n\n    return df","d784c055":"def countFromDict(df, count_col_name, _val_name=None):\n    \n    count_dic = {}\n\n    def countFromDf(x):\n        for val in ast.literal_eval(x):\n\n            if _val_name != None:\n                element_val = val[_val_name]\n            else:\n                element_val = json.dumps(val)\n            if count_dic.get(element_val, 0) == 0:\n                count_dic[element_val] = 1\n            else:\n                count_dic[element_val] = count_dic[element_val] + 1\n        \n        return x\n\n    _ = df[count_col_name].dropna().map(lambda x : countFromDf(x))\n    \n    df_count = pd.DataFrame(list(count_dic.items()),columns=['key_name','num'])\n    df_count.sort_values(\"num\", ascending=False, inplace=True)\n\n    return df_count","8c74949d":"col_name = \"revenue\"\n\nshowJointPlot(df_all, col_name)","b301c770":"df_million_index = df_all.loc[df_all[col_name] < 100].index\ndf_thousand_index = df_all.loc[df_all[col_name] < 1000].index\n\ndf_all.loc[df_million_index, col_name] = df_all.loc[df_million_index, col_name].map(lambda x: x * 1000000)\ndf_all.loc[df_thousand_index, col_name] = df_all.loc[df_thousand_index, col_name].map(lambda x: x * 1000)\n\ndf_all[col_name] = np.log(df_all[col_name])\nshowJointPlot(df_all, col_name)\n","4ea19da8":"col_name = \"budget\"\n\nshowJointPlot(df_all, col_name)","e27d1192":"df_million_budget_index = df_all.loc[df_all[col_name] < 100].index\ndf_thousand_budget_index = df_all.loc[df_all[col_name] < 1000].index\n\ndf_all.loc[df_million_budget_index, col_name] = df_all.loc[df_million_budget_index, col_name].map(lambda x: x * 1000000)\ndf_all.loc[df_thousand_budget_index, col_name] = df_all.loc[df_thousand_budget_index, col_name].map(lambda x: x * 1000)\n\n\n\n\n#df_all[col_name] = pd.Series(sc.fit_transform(df_all[col_name].values.reshape(-1, 1)).flatten())\ndf_all[col_name] = np.log1p(df_all[col_name])\nshowJointPlot(df_all, col_name)","4ae9f3d5":"col_name = \"release_date\"\n\n            \nprint(\"\\n**fillNanValues : \" + col_name +  \"***\")\nprint(df_all.loc[df_all[\"release_date\"].isnull() == True, [\"title\", \"imdb_id\", col_name]])\n\n#fill from IMDB\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0210130\", col_name] = \"3\/20\/01\"\n\n","08776416":"df_all[col_name + \"_year\"] = df_all[col_name].map(lambda x: int(x.split(\"\/\")[2]))\ndf_all[col_name + \"_month\"] = df_all[col_name].map(lambda x: int(x.split(\"\/\")[0]))\ndf_all[col_name + \"_day\"] = df_all[col_name].map(lambda x: int(x.split(\"\/\")[1]))\n\n\n#we assume that release years are between 1900 and 2017\ndf_all.loc[(df_all[col_name + \"_year\"] < 18), col_name + \"_year\"] += 2000\ndf_all.loc[(df_all[col_name + \"_year\"] >= 18) & (df_all[col_name + \"_year\"] < 100), col_name + \"_year\"] += 1900\n\ndf_all[col_name] = df_all.apply(lambda x: datetime.datetime(x[col_name + \"_year\"], x[col_name + \"_month\"], x[col_name + \"_day\"]), axis=1)\n\ndf_all[col_name + \"_month\"] = df_all[col_name + \"_month\"].replace({1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", 6 : \"Jun\", 7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"})\n\ndow = [\"Mon\",\"Tue\",\"Wed\",\"Thr\",\"Fri\",\"Sat\",\"Sun\"]\ndf_all[col_name + \"_dayofweek\"] = df_all[col_name].map(lambda x: dow[x.weekday()])\n\ndf_all[col_name + \"_week\"]= pd.Series(len(df_all[col_name]), index=df_all.index)\ndf_all.loc[df_all[col_name + \"_day\"] <= 7, col_name + \"_week\"] = \"w1\"\ndf_all.loc[(df_all[col_name + \"_day\"] > 7) & (df_all[col_name + \"_day\"] <= 14), col_name + \"_week\"] = \"w2\"\ndf_all.loc[(df_all[col_name + \"_day\"] > 14) & (df_all[col_name + \"_day\"] <= 21), col_name + \"_week\"] = \"w3\"\ndf_all.loc[(df_all[col_name + \"_day\"] > 21) & (df_all[col_name + \"_day\"] <= 28), col_name + \"_week\"] = \"w4\"\ndf_all.loc[(df_all[col_name + \"_day\"] > 28), col_name + \"_week\"] = \"w5\"\n\ndf_all[col_name + \"_MonthWeek\"] = df_all[col_name + \"_month\"] + df_all[col_name + \"_week\"]\n\n\ndf_all.drop(col_name + \"_month\", inplace=True, axis=1)\ndf_all.drop(col_name + \"_week\", inplace=True, axis=1)\ndf_all.drop(col_name + \"_day\", inplace=True, axis=1)\ndf_all.drop(col_name, inplace=True, axis=1)","b3f8ded6":"showValueCount(df_all, col_name + \"_year\")\nshowValueCount(df_all, col_name + \"_dayofweek\")\nshowValueCount(df_all, col_name + \"_MonthWeek\")","39b149dd":"col_name = \"belongs_to_collection\"\n\ndf_all[\"inCollection\"] = 0\ndf_all.loc[df_all[col_name].isnull() == False, \"inCollection\"] = 1\n\nshowValueCount(df_all, \"inCollection\")\ndf_all.drop(col_name, inplace=True, axis=1)","5d02decb":"col_name = \"genres\"\n\nprint(df_all.loc[df_all[col_name].isnull(), [\"imdb_id\", \"title\", col_name]])\nprint(\"\\n**fillNanValues : \" + col_name +  \"***\")\n\n#fill from IMDB\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0349159\", col_name] = \"[{'id': 12, 'name': 'Adventure'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0261755\", col_name] = \"[{'id': 18, 'name': 'Drama'}, {'id': 35, 'name': 'Comedy'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0110289\", col_name] = \"[{'id': 35, 'name': 'Comedy'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0352622\", col_name] = \"[{'id': 10749, 'name': 'Romance'}, {'id': 18, 'name': 'Drama'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0984177\", col_name] = \"[{'id': 28, 'name': 'Action'}, {'id': 18, 'name': 'Drama'}, {'id': 10749, 'name': 'Romance'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0833448\", col_name] = \"[{'id': 53, 'name': 'Thriller'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt1766044\", col_name] = \"[{'id': 18, 'name': 'Drama'}, {'id': 14, 'name': 'Fantasy'}, {'id': 9648, 'name': 'Mystery'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0090904\", col_name] = \"[{'id': 28, 'name': 'Action'}, {'id': 80, 'name': 'Crime'}, {'id': 53, 'name': 'Thriller'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0086405\", col_name] = \"[{'id': 18, 'name': 'Drama'}, {'id': 10749, 'name': 'Romance'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0044177\", col_name] = \"[{'id': 12, 'name': 'Adventure'}, {'id': 99999999, 'name': 'Biography'}, {'id': 18, 'name': 'Drama'}, {'id': 10749, 'name': 'Romance'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0108234\", col_name] = \"[{'id': 28, 'name': 'Action'}, {'id': 80, 'name': 'Crime'}, {'id': 18, 'name': 'Drama'}, {'id': 53, 'name': 'Thriller'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt1572916\", col_name] = \"[{'id': 18, 'name': 'Drama'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt1569465\", col_name] = \"[{'id': 35, 'name': 'Comedy'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0405699\", col_name] = \"[{'id': 28, 'name': 'Action'}, {'id': 80, 'name': 'Crime'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0461892\", col_name] = \"[{'id': 28, 'name': 'Action'}, {'id': 18, 'name': 'Drama'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt3121604\", col_name] = \"[{'id': 18, 'name': 'Drama'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt1164092\", col_name] = \"[{'id': 99, 'name': 'Documentary'}, {'id': 99999999, 'name': 'Biography'}, {'id': 10751, 'name': 'Family'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0250282\", col_name] = \"[{'id': 35, 'name': 'Comedy'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt1620464\", col_name] = \"[{'id': 35, 'name': 'Comedy'}, {'id': 80, 'name': 'Crime'}, {'id': 9648, 'name': 'Mystery'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0073317\", col_name] = \"[{'id': 35, 'name': 'Comedy'}, {'id': 80, 'name': 'Crime'}, {'id': 18, 'name': 'Drama'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0361498\", col_name] = \"[{'id': 35, 'name': 'Comedy'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt0361596\", col_name] = \"[{'id': 99, 'name': 'Documentary'}, {'id': 18, 'name': 'Drama'}, {'id': 10752, 'name': 'War'}]\"\ndf_all.loc[df_all[\"imdb_id\"] == \"tt2192844\", col_name] = \"[{'id': 18, 'name': 'Drama'}]\"","98defc25":"df_all[col_name + \"_len\"] = df_all[col_name].map(lambda x: len(ast.literal_eval(x)) if pd.isnull(x) == False else 0)\ndf_all.loc[df_all[col_name + \"_len\"] > 5, col_name + \"_len\"] = 5\nshowValueCount(df_all, col_name + \"_len\")","b06b1ee1":"df_all = countAndExpandFromDictToColumns(df_all, col_name, \"name\")\nprint(df_all.shape)","e6239d4d":"col_name = \"runtime\"\n\nprint(\"\\n**fillNanValues : \" + col_name +  \"***\")\nprint(df_all.loc[df_all[\"runtime\"].isnull() == True, [\"title\", \"imdb_id\", \"release_date\", col_name]])\n\n\nshowJointPlot(df_all, col_name)","1e20c196":"df_all.sort_values(col_name, ascending=False)[[\"title\", col_name]]","9b5a437b":"df_all.loc[df_all[col_name] == 0, col_name] = np.nan\ndf_all[col_name].fillna(df_all[col_name].median(), inplace=True)\n\n#df_all[col_name] = pd.Series(sc.fit_transform(df_all[col_name].values.reshape(-1, 1)).flatten())\ndf_all[col_name] = np.log(df_all[col_name])\n#df_all.drop(index=df_all.loc[df_all[col_name] < 2.4, [\"title\", col_name]].index, inplace=True)\n#print(df_all.loc[df_all[col_name] < 2.4, [\"title\", col_name]])\nshowJointPlot(df_all, col_name)","fba18849":"col_name = \"spoken_languages\"\n\nprint(\"\\n**fillNanValues : \" + col_name +  \"***\")\n\ndf_count_sLangugaes = countFromDict(df_all, col_name)\ndf_count_sLangugaes[\"iso\"] = df_count_sLangugaes[\"key_name\"].map(lambda x: ast.literal_eval(x)[\"iso_639_1\"])\n#print(df_count_sLangugaes.head(len(df_count_sLangugaes)))\n\n\ndef returnStr(x):\n    \n    lis = df_count_sLangugaes.loc[df_count_sLangugaes[\"iso\"] == x.original_language, \"key_name\"].values\n    if len(lis) == 0:\n        #print(x, lis)\n        x.spoken_languages = np.nan\n    else:\n        str_dic = lis[0]\n        x.spoken_languages =  str(\"[\" + str_dic + \"]\")\n    \n    return x\n\ndf_all.loc[(df_all[col_name].isnull() == True), [col_name, \"original_language\"]] = df_all.loc[(df_all[col_name].isnull() == True), [col_name, \"original_language\"]].apply(lambda x: returnStr(x), axis=1)\ndf_all[\"lang_len\"] = df_all[col_name].map(lambda x: len(ast.literal_eval(x)))\n\ndf_all = countAndExpandFromDictToColumns(df_all, col_name, \"iso_639_1\")\ndf_all.shape","87b7a9a7":"col_name = \"original_language\"\n\n#print(df_all[col_name].isnull().sum())\n\n#df_all[\"isEnglish\"] = 0\n#df_all.loc[(df_all[col_name] == \"en\") | (df_all[\"spoken_languages_en\"] == 1), \"isEnglish\"] = 1\n\nshowValueCount(df_all, col_name)","752bd1ab":"col_name = \"overview\"\ndf_all[col_name].fillna(0, inplace=True)","f1a047cf":"col_name = \"popularity\"\n\nshowJointPlot(df_all, col_name)\ndf_all[col_name].sort_values(ascending=False)","3d0a1c89":"print(df_all.shape)\ndf_all[col_name + \"_RANK\"] = 2\ndf_all.loc[df_all[col_name] > 50, col_name + \"_RANK\"]= 3\ndf_all.loc[df_all[col_name] <= 3, col_name + \"_RANK\"] = 1\n\n#df_all.drop(df_all.loc[(df_all[\"revenue\"] < 10) & (df_all[col_name] > 25)].index, inplace=True)\ndf_all.loc[(df_all[col_name] < 0.0003), col_name] = 0.0003\n\n#df_all[col_name] = pd.Series(sc.fit_transform(df_all[col_name].values.reshape(-1, 1)).flatten())\ndf_all[col_name] = np.log(df_all[col_name])\n\nprint(df_all.shape)\n#showJointPlot(df_all, col_name)\n\n","d5ee4758":"col_name = \"crew\"\n\ndf_all[col_name + \"_num\"] = df_all[col_name].map(lambda x: len(ast.literal_eval(x)) if pd.isnull(x) == False else np.nan)\n","f4bdc41b":"#fill na and 0 \ndf_all[col_name + \"_num\"].fillna(df_all[col_name + \"_num\"].median(), inplace=True)\n\n#df_all[col_name + \"_num\"] = pd.Series(sc.fit_transform(df_all[col_name + \"_num\"].values.reshape(-1, 1)).flatten())\ndf_all[col_name + \"_num\"] = np.log(df_all[col_name + \"_num\"])\n\nshowJointPlot(df_all, col_name + \"_num\")\n\n\ndf_all.drop(col_name, axis=1, inplace=True)","f1bdac3a":"col_name = \"cast\"\n        \ndf_all[col_name + \"_num\"] = df_all[col_name].map(lambda x: len(ast.literal_eval(x)) if pd.isnull(x) == False else np.nan)\ndf_all.loc[df_all[col_name + \"_num\"] == 0, col_name + \"_num\"] = np.nan\n","e6ab0927":"\ncast_dict = {}\n\ndef countName(x):\n    cast_list = ast.literal_eval(x.cast)\n    \n    for each_cast in cast_list:\n        cast_name = each_cast[\"name\"]\n        \n        \n        if cast_name not in cast_dict.keys():\n            cast_dict[cast_name] = {\"train\":0, \"test\":0}\n        \n        which_data = \"train\"\n        if x.id > 3000:\n            which_data = \"test\"\n        cast_dict[cast_name][which_data] += 1\n    \n    return x\n\ndf_all = df_all.apply(lambda x: countName(x) if pd.isnull(x.cast) == False else x, axis=1)\n\ndf_cast_count = pd.DataFrame(cast_dict).T\ndf_cast_count.reset_index(inplace=True)\ndf_cast_count.rename(columns={'index': 'name'}, inplace=True)\ndf_cast_count[\"total\"] = df_cast_count[\"train\"] + df_cast_count[\"test\"]\ndf_cast_count.sort_values(\"total\", inplace=True)\nprint(df_cast_count.shape)\nprint(df_cast_count.columns)\nprint(df_cast_count.index)\ndf_cast_count\n    ","d6f7c027":"#fill na and 0 \ndf_all[col_name + \"_num\"].fillna( df_all[col_name + \"_num\"].median(), inplace=True)\n\n\n#df_all[col_name + \"_num\"] = pd.Series(sc.fit_transform(df_all[col_name + \"_num\"].values.reshape(-1, 1)).flatten())\ndf_all[col_name + \"_num\"] = np.log(df_all[col_name + \"_num\"])\n\nshowJointPlot(df_all, col_name + \"_num\")\n\ndf_all.drop(col_name, axis=1, inplace=True)","b4cae817":"col_name = \"Keywords\"\n\n#df_all[col_name + \"_len\"] = 0\n#df_count_keywords = countFromDict(df_all, col_name, \"name\")\n\n            \ndef serachOverview(x):\n\n    val = 0\n    freq_keywords = []\n    keywords_len = 0\n    if pd.isna(x[col_name]) == False:\n        overview_text = x[\"overview\"]\n        keyword_list = ast.literal_eval(x[col_name])\n        keywords_len = len(keyword_list)\n\n\n        for keyword in keyword_list:\n            word = str(keyword[\"name\"])\n            #print(word)\n            freq_num = df_count_keywords.loc[df_count_keywords[\"key_name\"] == word, \"num\"].values[0]\n            #print(freq_num)\n            if freq_num > 100:\n                freq_keywords.append(col_name + \"_\" + word)\n\n            if overview_text != 0 & str(overview_text).find(word) != -1:\n                val = val + 1\n\n    x[\"overview\"] = val\n    x[col_name] = \",\".join(freq_keywords)\n    x[col_name + \"_len\"] = keywords_len\n    return x\n\n\n\n#df_all = df_all.apply(lambda x: serachOverview(x), axis=1)\n\n#df_tmp = df_all[col_name].str.get_dummies(sep=',')\n#print(df_tmp.info())\n#df_all = pd.concat([df_all, df_tmp], axis=1, sort=False)\ndf_all.drop([\"Keywords\", \"overview\"], axis=1, inplace=True)","ab721da5":"col_name = \"production_countries\"\n        \nprint(\"\\n**fillNanValues : \" + col_name +  \"***\")\n\n#df = fillNAN_productionC(df)\ndf_all[\"production_countries\"].fillna(\"[{'iso_3166_1': 'US', 'name': 'United States of America'}]\", inplace=True)\ndf_all = countAndExpandFromDictToColumns(df_all, col_name, \"name\")\ndf_all.shape","7a5e53d7":"col_name = \"homepage\"\n\ndf_all[\"hasHomepage\"] = 1\ndf_all.loc[df_all[col_name].isnull() == True, \"hasHomepage\"] = 0\n\ndf_all.drop(col_name, axis=1, inplace=True)","c4b3d3f5":"df_all[\"budget_year_ratio\"] = df_all[\"budget\"] \/ df_all[\"release_date_year\"]\n\ndrop_cols = [\"imdb_id\", \"poster_path\", \"original_title\", \"tagline\", \"title\", \"status\", \"production_companies\"]\ndf_all.drop(drop_cols, axis=1, inplace=True)\n\ndf_all = pd.get_dummies(df_all, drop_first=True, columns=['release_date_dayofweek'])\n\ndf_all = pd.get_dummies(df_all)\ndf_all.shape\n\n","20531cc0":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import fbeta_score, make_scorer\nimport keras.backend as K\nfrom keras.layers import Input, Dense, Activation, BatchNormalization, MaxPooling2D\nfrom keras.models import Model\nfrom keras.layers import concatenate\nfrom keras.optimizers import RMSprop, Adam\nfrom keras.callbacks import EarlyStopping \nfrom keras.callbacks import LearningRateScheduler","ea40e5af":"train_idx = df_all[~df_all[target_col_name].isnull()].index.values\ndf_train = df_all.loc[train_idx].set_index('id')\ndf_train.drop([\"index\"], axis=1, inplace=True)\n\nmax_price = df_train[\"revenue\"].max()\ndf_train[\"revenue\"] = df_train[\"revenue\"] \/ max_price\n            \ntest_idx = df_all[df_all[target_col_name].isnull()].index.values\ndf_test = df_all.loc[test_idx].set_index('id')\ndf_test.drop([target_col_name, \"index\"], axis=1, inplace=True)\n\ndf_train_X = df_train.drop(target_col_name, axis=1)\ndf_train_y = df_train[target_col_name]\nprint(df_train.shape)\nprint(df_test.shape)\nprint(df_train.columns)\nprint(df_test.columns)\n\n\nscorer = make_scorer(mean_squared_error, greater_is_better = False)\n","948076a1":"\nindices = np.array(range(df_train.shape[0]))\nvalid_train_X, valid_test_X, valid_train_y, valid_test_y, indices_valid_train, indices_valid_test  = train_test_split(df_train_X, df_train_y, indices, test_size=0.2, shuffle=True, random_state=64)\n\nprint(\"valid_train_X\", valid_train_X.shape)\nprint(\"valid_test_X\", valid_test_X.shape)\nprint(\"valid_train_y\", valid_train_y.shape)\nprint(\"valid_test_y\", valid_test_y.shape)\n\n \nvalid_train_img_X = poster_train_img[indices_valid_train]\nvalid_test_img_X = poster_train_img[indices_valid_test]\ntest_img_X = poster_test_img\n\nprint(\"valid_train_img_X\", valid_train_img_X.shape)\nprint(\"valid_test_img_X\", valid_test_img_X.shape)\nprint(\"test_img_X\", test_img_X.shape)\n","058dd9eb":"def createNNmodel(_input_dim, regress=False):\n    \n    model = Sequential()\n    model.add(Dense(5, input_dim=_input_dim, activation=\"relu\"))\n    model.add(Dense(4, activation=\"relu\"))\n    model.add(Dense(3, activation=\"relu\"))\n    model.add(Dense(2, activation=\"relu\"))\n    \n    if regress:\n        model.add(Dense(1, activation=\"linear\"))\n    \n    return model\n\n\n\n","f958e855":"def createCNNmodel(width, height, depth, filters, regress=False):\n\n    inputShape = (height, width, depth)\n    chanDim = -1\n\n    inputs = Input(shape=inputShape)\n    \n    for (i, f) in enumerate(filters):\n\n        if i == 0:\n            x = inputs\n \n        # CONV => RELU => BN => POOL\n        x = Conv2D(f, (3, 3), padding=\"same\")(x)\n        x = Activation(\"relu\")(x)\n        x = BatchNormalization(axis=chanDim)(x)\n        x = MaxPooling2D(data_format='channels_last', pool_size=(2, 2))(x)\n        x = Dropout(0.25)(x)\n    \n    # flatten the volume, then FC => RELU => BN => DROPOUT\n    x = Flatten()(x)\n    x = Dense(16)(x)\n    x = Activation(\"relu\")(x)\n    x = BatchNormalization(axis=chanDim)(x)\n    x = Dropout(0.5)(x)\n\n    x = Dense(4)(x)\n    x = Activation(\"relu\")(x)\n\n    if regress:\n        x = Dense(1, activation=\"linear\")(x)\n        \n\n    model = Model(inputs, x)\n    \n    return model  ","f8137dd0":"def createBoxOfficeNetModel(_input_dim_NN, width_img, height_img, depth_img, filters_CNN):\n    \n    model_NN = createNNmodel(_input_dim_NN)\n    model_CNN = createCNNmodel(width_img, height_img, depth_img, filters=filters_CNN)\n\n    combinedInput = concatenate([model_NN.output, model_CNN.output])\n\n    x = Dense(4, activation=\"relu\")(combinedInput)\n    x = Dense(1, activation=\"linear\")(x)\n    \n    \n    model = Model(inputs=[model_NN.input, model_CNN.input], outputs=x)\n    \n    return model","25354ec1":"# my loss function for RMSE\ndef root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true))) ","451696fb":"def drawResultCurves(_history):\n    # Plot the loss and accuracy curves for training and validation \n\n    fig, ax = plt.subplots(2,1) \n    ax[0].plot(_history.history['loss'], color='b', label=\"Training loss\")\n    ax[0].plot(_history.history['val_loss'], color='r', label=\"Validation loss\",axes =ax[0])\n    legend = ax[0].legend(loc='best', shadow=True)\n\n    ax[1].plot(_history.history['mean_squared_logarithmic_error'], color='b', label=\"Training accuracy\")\n    ax[1].plot(_history.history['val_mean_squared_logarithmic_error'], color='r',label=\"Validation accuracy\")\n    legend = ax[1].legend(loc='best', shadow=True)\n\n    plt.show()","c063265d":"my_filters_CNN=[8]\nmodel = createBoxOfficeNetModel(valid_train_X.shape[1], 64, 64, 3, filters_CNN=my_filters_CNN)\n\n\nopt = Adam(lr=1e-5, decay=1e-4)\nmodel.compile(loss=root_mean_squared_error, optimizer=opt, metrics=['msle'])\n\n#model.summary()","7db74445":"_epochs=100\n\ndef step_decay(epoch):\n    x = 1e-3\n    if epoch >= 30: x = 1e-4\n    return x\nlr_decay = LearningRateScheduler(step_decay)\n\nearly_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=20, verbose=0, mode='auto')\n# train the model\nhistory = model.fit([valid_train_X, valid_train_img_X], valid_train_y, \n                    validation_data=([valid_test_X, valid_test_img_X], valid_test_y),\n                    #callbacks=[early_stopping],\n                    callbacks=[early_stopping, lr_decay],\n                    epochs=_epochs, batch_size=8, verbose = 2)\n\n              \ndrawResultCurves(history)","c3b07dc0":"# predict results with trained parameters\n         \npreds = model.predict([df_test.values, test_img_X])\n#pred_test_all_y = np.exp(preds)\npred_test_all_y = np.exp(preds * max_price)\n\ndf_test.loc[:, \"revenue\"] = pred_test_all_y","e213b1ff":"submission_Price = pd.DataFrame({\n            \"id\": df_test.index,\n            \"revenue\": np.nan\n        }, index=df_test.index)\n        \n               \nsubmission_Price.loc[df_test.index, \"revenue\"] = df_test[\"revenue\"]\n\nsubmission_Price.to_csv('submission.csv', index=False)\n\n\nsubmission_Price\n#for i in range(len(submission_Price)):\n    #print(submission_Price.iloc[i])","60777df8":"**production_countries**","71fe7765":"**Keywords**","7dbc3bc1":"the longest and shortest movies are ...","829d125e":"Neural Network model creation","cbfd45d4":"**Budget**","f8131ade":"revenue's minimum is 1 dollar...?","d0557ad9":"I add \"crew_num\"","7cd37ce4":"**runtime**","bdbe0da7":"budget contains many 0...","cda1f415":"I add new feature \"isEnglish\"","dd9d56b1":"**release_date**","33015993":"**overview**","2c2e85de":"**popularity**","bf05d719":"**cast**","f73e7689":"this feature is used in \"Keywords\"","d42dc157":"preparing for loading posters","bc89178d":"**crew**","800ce795":"**spoken_languages**","580f62f1":"I add the feature \"inCollection\" which means each movie belongs its collection or not.","2da58275":"**Preprocessing**","23bf5631":"**revenue**","7b105d72":"**others**","b06e6398":"**homepage**","adfe79d3":"**original_language**","c37b95c2":"**Submission**","e8d3d0ba":"**model**","6fab3043":"There are missing posters in the dataset. <br>\nTrain : 2303 <br>\nTest : 3829, 4925 <br>\nTheir titles are...\n","32e67de5":"**genres**","269ca1dc":"**belongs_to_collection**"}}