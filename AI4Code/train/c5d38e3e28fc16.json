{"cell_type":{"e65349a2":"code","40c2d46e":"code","581eb874":"code","355b0bce":"code","3a8e759a":"code","c2be7b8d":"code","db5f5ee1":"code","6965fb2e":"code","79b6eed2":"markdown","be848916":"markdown","454839ac":"markdown","1c7b73f2":"markdown","3d827604":"markdown","6184d2cd":"markdown","1810394d":"markdown"},"source":{"e65349a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","40c2d46e":"import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nplt.style.use('classic')","581eb874":"train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\",dtype = np.float32)\n\ntargets = train.label.values\nfeatures = train.loc[:,train.columns != \"label\"].values\/255","355b0bce":"features_train, features_test, targets_train, targets_test = train_test_split(features,\n                                                                             targets,\n                                                                             test_size=0.3,\n                                                                             random_state=123) \n\nfeatures_train_t = torch.from_numpy(features_train)\ntargets_train_t = torch.from_numpy(targets_train).type(torch.LongTensor)\nfeatures_test_t = torch.from_numpy(features_test)\ntargets_test_t = torch.from_numpy(targets_test).type(torch.LongTensor)","3a8e759a":"fig, axs = plt.subplots(2,4)\nfig.tight_layout()\naxs = axs.ravel()\n\nfor i in range(8):\n    axs[i].imshow(features[i].reshape(28,28))\n    axs[i].set_title(str(str(targets[i])))","c2be7b8d":"#Feed forward CNN\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        \n        # Conv layer #1\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)\n        self.relu1 = nn.ReLU()\n        \n        # Max pool layer #1\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n     \n        # Conv layer #2\n        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)\n        self.relu2 = nn.ReLU()\n        \n        # Max pool layer #2\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n        \n        # Fully connected layer #1\n        self.fc1 = nn.Linear(32 * 4 * 4, 10)\n        \n        # Dropout layer\n        self.dropout = nn.Dropout(p=0.2)\n    \n    def forward(self, in_):\n        # Conv layer #1\n        out_ = self.cnn1(in_)\n        out_ = self.relu1(out_)\n        \n        # Max pool layer #1\n        out_ = self.maxpool1(out_)\n        \n        # Conv layer #2\n        out_ = self.cnn2(out_)\n        out_ = self.relu2(out_)\n        \n        # Max pool layer #2\n        out_ = self.maxpool2(out_)\n        \n        # Flattening\n        out_ = out_.view(out_.size(0), -1)\n\n        # Linear function\n        out_ = self.fc1(out_)\n        \n        return out_\n\nbatch_size = 100\nn_iters = 2000\nn_epochs = n_iters \/ (len(features_train) \/ batch_size)\nn_epochs = int(n_epochs)\n\ntrain = torch.utils.data.TensorDataset(features_train_t,targets_train_t)\ntest = torch.utils.data.TensorDataset(features_test_t,targets_test_t)\n\ntrain_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle=False)\ntest_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle=False)\n\nmodel = CNN()\n\n# Cross Entropy Loss \nerror = nn.CrossEntropyLoss()\n\n# Adam Optimizer\nlearning_rate = 0.2\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","db5f5ee1":"count = 0\nloss_output = []\nn_iter = []\naccuracy_output = []\n\nfor epoch in range(n_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        \n        train = Variable(images.view(100,1,28,28))\n        labels = Variable(labels)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and cross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculating gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        if count % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                \n                test = Variable(images.view(100,1,28,28)) # 100 images (batch), each of size 28x28, 1 channel\n                \n                # Forward propagation\n                outputs = model(test)\n                \n                # Get predictions from the maximum value\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += len(labels)\n                \n                correct += (predicted == labels).sum() # if predicted value is correct, then increase the counter of total correct\n            \n            accuracy = 100 * correct \/ float(total)\n            \n            # store loss and iteration\n            loss_output.append(loss.data)\n            n_iter.append(count)\n            accuracy_output.append(accuracy)\n            \n        if count % 500 == 0:\n            # Print Loss\n            print('No. of iteration: {},  Loss: {},  Accuracy: {:.02f}%'.format(count, loss.data, accuracy))","6965fb2e":"# visualization of the loss function\nplt.plot(n_iter,loss_output, color=\"blue\")\nplt.xlabel(\"No. of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss vs. No. of iteration\")\nplt.show()\n\n# visualization of accuracy\nplt.plot(n_iter,accuracy_output,color=\"green\")\nplt.xlabel(\"No. of iteration\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy vs. No. of iteration\")\nplt.show()","79b6eed2":"<img src=\"https:\/\/aigeekprogrammer.com\/wp-content\/uploads\/2019\/08\/Handwriting-digit-recognition-Keras-MNIST.jpg\" width=500 height=300>","be848916":"# 4. Visualization of results","454839ac":"# 1. Libraries & Load data","1c7b73f2":"**Reference:**\n1. https:\/\/www.kaggle.com\/raghava95\/digit-recognition-with-pytorch \n2. https:\/\/github.com\/keineahnung2345\/pytorch-tutorial-jupyter-notebooks\/tree\/master\/tutorials ","3d827604":"# MNIST baseline with PyTorch and CNN","6184d2cd":"# 3. Neural network","1810394d":"# 2. Preparing data for train & test"}}