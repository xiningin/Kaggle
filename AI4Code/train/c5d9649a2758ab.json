{"cell_type":{"2590c0ac":"code","a5139d5a":"code","acdbb62f":"code","47c73158":"code","193dd285":"code","8ad24c76":"code","2bad79c5":"code","dc9962c7":"code","bf5a6452":"code","19597cc1":"code","0706df82":"code","55d2edd7":"code","ca785f71":"code","8ec9f864":"code","c8e3cc3e":"code","95984443":"code","b1f4314d":"code","cf8e8aec":"code","f4a5a07a":"markdown","99c22535":"markdown","95579559":"markdown","727fa875":"markdown","cc5e0ab0":"markdown","8b8aba78":"markdown","015321c0":"markdown"},"source":{"2590c0ac":"# !pip install transformers -U","a5139d5a":"import os\nimport gc\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tqdm.notebook import tqdm\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors","acdbb62f":"def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_mask=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen,\n        truncation=True\n    )\n    \n    return np.array(enc_di['input_ids'])","47c73158":"def dict_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_mask=True, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen,\n        truncation=True\n    )\n    \n    return {\n        \"input_ids\": np.array(enc_di['input_ids']),\n        \"attention_mask\": np.array(enc_di['attention_mask'])\n    }","193dd285":"from keras import backend as K\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","8ad24c76":"def build_model(transformer, max_len=512):\n    \"\"\"\n    https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \"\"\"\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n    sequence_output = transformer({\"input_ids\": input_word_ids, \"attention_mask\": attention_mask})[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs={\n        \"input_ids\": input_word_ids,\n        \"attention_mask\": attention_mask\n    }, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=[f1_m])\n    \n    return model","2bad79c5":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","dc9962c7":"MAX_LEN = 600\nMODEL = 'albert-xxlarge-v2'\nAUTO = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 2 * strategy.num_replicas_in_sync\ntokenizer = AutoTokenizer.from_pretrained(MODEL)","bf5a6452":"train = pd.read_csv(\"\/kaggle\/input\/text-classification-int20h\/train.csv\")\n\nmsk = np.random.rand(len(train)) < 0.92\nvalid = train[~msk]\ntrain = train[msk]\n\n\ntest = pd.read_csv(\"\/kaggle\/input\/text-classification-int20h\/test.csv\")\nsub = pd.read_csv(\"\/kaggle\/input\/text-classification-int20h\/submission.csv\")\n\nn_valid_steps = valid.shape[0] \/\/ BATCH_SIZE","19597cc1":"n_train_steps = train.shape[0] \/\/ BATCH_SIZE","0706df82":"%%time \nx_train = dict_encode(train.review.values, tokenizer, maxlen=MAX_LEN)\ny_train = train.sentiment.values\n\ndel train\ngc.collect()\n\nx_valid = dict_encode(valid.review.values, tokenizer, maxlen=MAX_LEN)\ny_valid = valid.sentiment.values\ndel valid\ngc.collect()\nx_test = dict_encode(test.review.values, tokenizer, maxlen=MAX_LEN)","55d2edd7":"!free -h","ca785f71":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(4096)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(BATCH_SIZE)\n)\ndel x_train, x_valid, y_train, y_valid\ngc.collect()","8ec9f864":"%%time\nwith strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","c8e3cc3e":"# Configuration\nEPOCHS = 20\n\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_train_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)","95984443":"# train_history_2 = model.fit(\n#     valid_dataset.repeat(),\n#     steps_per_epoch=n_valid_steps,\n#     epochs=2\n# )","b1f4314d":"model.save_weights(\"final_weights.h5\")","cf8e8aec":"sub['sentiment'] = model.predict(test_dataset, verbose=1)\nsub.sentiment = (sub.sentiment > 0.5).astype(int)\nsub.to_csv('submission7.csv', index=False)","f4a5a07a":"## Load model into the TPU","99c22535":"First, we train on the subset of the training set, which is completely in English.","95579559":"## Train Model","727fa875":"## Submission","cc5e0ab0":"Now that we have pretty much saturated the learning potential of the model on english only data, we train it for one more epoch on the `validation` set, which is significantly smaller but contains a mixture of different languages.","8b8aba78":"## Helper Functions","015321c0":"## TPU Configs"}}