{"cell_type":{"4b5545a6":"code","df7dc69b":"code","8307010f":"code","5f447741":"code","c652ba1f":"code","2d2cea4d":"code","fab8da79":"code","89f155af":"code","666e55cd":"code","488ffc6f":"code","45451475":"code","33fc324d":"code","4645fec5":"code","a8b04769":"code","86d7773a":"code","0e47709a":"code","ea69b5b9":"code","4e02b4d5":"code","c8672cb7":"code","d40fee9d":"code","c396138f":"code","9f84fb8a":"code","69820ba0":"code","7a1f1147":"code","705db862":"code","e1e74eb5":"code","fb239a2a":"code","51215d35":"code","3dceb279":"code","ffd66e51":"code","97f387e8":"code","52ae4334":"code","3204bd4a":"code","9fac866d":"code","40660e95":"code","36d98e24":"code","ca74780f":"code","2e210f0e":"code","bef76144":"markdown","604418cd":"markdown","6fbc13ab":"markdown","9542e5ff":"markdown","a8f6e917":"markdown","977dfd8a":"markdown","8acfb3b6":"markdown","ad2a138e":"markdown","a9e57af2":"markdown"},"source":{"4b5545a6":"# Import packages\n# Basic packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport pickle\nfrom math import floor\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\n# Evaluation and bayesian optimization\nfrom sklearn.metrics import make_scorer, mean_absolute_error\nfrom sklearn.metrics import mean_squared_error as MSE\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option(\"display.max_columns\", None)","df7dc69b":"# Load dataset\ntrainSet = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv').drop(columns=['Id'])\ntestSet = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv').drop(columns=['Id'])\nsubmitSet = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\n\ntrainSet.head()","8307010f":"# Drop columns with too much NA\ntrain = trainSet.drop(columns=['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis=0)\n\n# Fill missing value with median or most occurance value\ntrain['LotFrontage'] = train['LotFrontage'].fillna(train['LotFrontage'].median())\ntrain['MasVnrType'] = train['MasVnrType'].fillna(train['MasVnrType'].value_counts().idxmax())\ntrain['MasVnrArea'] = train['MasVnrArea'].fillna(train['MasVnrArea'].median())\ntrain['MasVnrType'] = train['MasVnrType'].fillna(train['MasVnrType'].value_counts().idxmax())\ntrain['BsmtQual'] = train['BsmtQual'].fillna(train['BsmtQual'].value_counts().idxmax())\ntrain['BsmtCond'] = train['BsmtCond'].fillna(train['BsmtCond'].value_counts().idxmax())\ntrain['BsmtExposure'] = train['BsmtExposure'].fillna(train['BsmtExposure'].value_counts().idxmax())\ntrain['BsmtFinType1'] = train['BsmtFinType1'].fillna(train['BsmtFinType1'].value_counts().idxmax())\ntrain['BsmtFinType2'] = train['BsmtFinType2'].fillna(train['BsmtFinType2'].value_counts().idxmax())\ntrain['Electrical'] = train['Electrical'].fillna(train['Electrical'].value_counts().idxmax())\ntrain['GarageType'] = train['GarageType'].fillna(train['GarageType'].value_counts().idxmax())\ntrain['GarageYrBlt'] = train['GarageYrBlt'].fillna(train['GarageYrBlt'].median())\ntrain['GarageFinish'] = train['GarageFinish'].fillna(train['GarageFinish'].value_counts().idxmax())\ntrain['GarageQual'] = train['GarageQual'].fillna(train['GarageQual'].value_counts().idxmax())\ntrain['GarageCond'] = train['GarageCond'].fillna(train['GarageCond'].value_counts().idxmax())\n\ntrain.info()","5f447741":"# Encode categorical val\ntrain = pd.get_dummies(train)\ntrain.head()","c652ba1f":"selected = ['BsmtQual_Gd', 'SaleType_New', 'Fireplaces', 'BsmtExposure_Gd', '2ndFlrSF',\n            'BsmtFinType1_GLQ', 'BsmtFinSF1', 'LowQualFinSF', 'GarageType_Detchd',\n            'KitchenAbvGr', 'MSZoning_RM', 'TotalBsmtSF', 'CentralAir_N', 'GrLivArea',\n            'KitchenQual_Ex', 'Exterior2nd_CmentBd', 'GarageCars', 'LandSlope_Gtl',\n            'BsmtQual_Ex', 'OverallQual', 'SalePrice']","2d2cea4d":"train_sel = train.loc[:,selected]\n\n# train validation split\nX_train, X_val, y_train, y_val = train_test_split(train_sel.drop(columns=['SalePrice']),\n                                                  train_sel['SalePrice'],\n                                                  test_size=0.2, random_state=123)","fab8da79":"!apt install -y build-essential swig curl\n!curl https:\/\/raw.githubusercontent.com\/automl\/auto-sklearn\/master\/requirements.txt | xargs -n 1 -L 1 pip install\n!pip install auto-sklearn\n!pip install scipy==1.7.0\n!pip install -U scikit-learn","89f155af":"from autosklearn.regression import AutoSklearnRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error","666e55cd":"# Create the model\nsklearn = AutoSklearnRegressor(time_left_for_this_task=3*60, per_run_time_limit=30, n_jobs=-1)\n\n# Fit the training data\nsklearn.fit(X_train, y_train)\n\n# Sprint Statistics\nprint(sklearn.sprint_statistics())\n\n# Predict the validation data\npred_sklearn = sklearn.predict(X_val)\n\n# Compute the RMSE\nrmse_sklearn=MSE(y_val, pred_sklearn)**0.5\nprint('RMSE: ' + str(rmse_sklearn))","488ffc6f":"# Scatter plot true and predicted values\nplt.scatter(pred_sklearn, y_val, alpha=0.2)\nplt.xlabel('predicted')\nplt.ylabel('true value')\nplt.text(100000, 400000, 'RMSE: ' + str(round(rmse_sklearn)))\nplt.text(100000, 350000, 'MAE: ' + str(round(mean_absolute_error(y_val, pred_sklearn))))\nplt.text(100000, 300000, 'R: ' + str(round(np.corrcoef(y_val, pred_sklearn)[0,1],4)))\nplt.show()","45451475":"# Show the models\nprint(sklearn.show_models())","33fc324d":"from tpot import TPOTRegressor","4645fec5":"# Create model\ncv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=123) \ntpot = TPOTRegressor(generations=5, population_size=5, cv=cv, scoring='neg_mean_absolute_error', verbosity=2, random_state=123, n_jobs=-1)\n\n# Fit the training data\ntpot.fit(X_train, y_train)\n\n# Export the result\ntpot.export('tpot_model.py')","a8b04769":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\n\n# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n# This part is replaced with the below part\n#tpot_data = pd.read_csv('PATH\/TO\/DATA\/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n#features = tpot_data.drop('target', axis=1)\n#training_features, testing_features, training_target, testing_target = \\\n#            train_test_split(features, tpot_data['target'], random_state=123)\n\n# This part replaces the previous part\ntraining_features = X_train\ntesting_features = X_val\ntraining_target= y_train\ntesting_target = y_val\n\n# Average CV score on the training set was: -18752.921100941825\nexported_pipeline = RandomForestRegressor(bootstrap=True, max_features=0.25, min_samples_leaf=3, min_samples_split=2, n_estimators=100)\n# Fix random state in exported estimator\nif hasattr(exported_pipeline, 'random_state'):\n    setattr(exported_pipeline, 'random_state', 123)\n\nexported_pipeline.fit(training_features, training_target)\nresults = exported_pipeline.predict(testing_features)","86d7773a":"pred_tpot = results\n\n# Scatter plot true and predicted values\nplt.scatter(pred_tpot, y_val, alpha=0.2)\nplt.xlabel('predicted')\nplt.ylabel('true value')\nplt.text(100000, 400000, 'RMSE: ' + str(round(MSE(y_val, pred_tpot)**0.5)))\nplt.text(100000, 350000, 'MAE: ' + str(round(mean_absolute_error(y_val, pred_tpot))))\nplt.text(100000, 300000, 'R: ' + str(round(np.corrcoef(y_val, pred_tpot)[0,1],4)))\nplt.show()","0e47709a":"!pip install git+https:\/\/github.com\/hyperopt\/hyperopt-sklearn.git","ea69b5b9":"from hpsklearn import HyperoptEstimator\nfrom hpsklearn import any_regressor\nfrom hpsklearn import any_preprocessing\nfrom hyperopt import tpe\nfrom sklearn.metrics import mean_squared_error","4e02b4d5":"# Convert data into array\nX_train_ar = np.array(X_train)\nX_val_ar = np.array(X_val)\ny_train_ar = np.array(y_train)\ny_val_ar = np.array(y_val)","c8672cb7":"# Create the model\nhyperopt = HyperoptEstimator(regressor=any_regressor('reg'), preprocessing=any_preprocessing('pre'),\n                             loss_fn=mean_squared_error, algo=tpe.suggest, max_evals=50,\n                             trial_timeout=20)\n\n# Fit the data\nhyperopt.fit(X_train, y_train)","d40fee9d":"# Predict the validation data\npred_hyperopt = hyperopt.predict(X_val)\n\n# Scatter plot true and predicted values\nplt.scatter(pred_hyperopt, y_val, alpha=0.2)\nplt.xlabel('predicted')\nplt.ylabel('true value')\nplt.text(100000, 400000, 'RMSE: ' + str(round(MSE(y_val_ar, pred_hyperopt)**0.5)))\nplt.text(100000, 350000, 'MAE: ' + str(round(mean_absolute_error(y_val_ar, pred_hyperopt))))\nplt.text(100000, 300000, 'R: ' + str(round(np.corrcoef(y_val_ar, pred_hyperopt)[0,1],4)))\nplt.show()","c396138f":"# Show the models\nprint(hyperopt.best_model())","9f84fb8a":"!pip install autokeras","69820ba0":"import autokeras","7a1f1147":"# Create the model\nkeras = autokeras.StructuredDataRegressor(max_trials=8)\n\n# Fit the training dataset\nkeras.fit(X_train, y_train, epochs=100)\n\n# Predict the validation data\npred_keras = keras.predict(X_val)","705db862":"# Convert predicted result into pandas series with numeric type\npred_keras_ = pd.DataFrame(pred_keras)\npred_keras_ = pred_keras_[0]\npred_keras_ = pd.to_numeric(pred_keras_)\n\n# Scatter plot true and predicted values\nplt.scatter(pred_keras_, y_val, alpha=0.2)\nplt.xlabel('predicted')\nplt.ylabel('true value')\nplt.text(100000, 400000, 'RMSE: ' + str(round(MSE(y_val_ar, pred_keras_)**0.5)))\nplt.text(100000, 350000, 'MAE: ' + str(round(mean_absolute_error(y_val_ar, pred_keras_))))\nplt.text(100000, 300000, 'R: ' + str(round(np.corrcoef(y_val_ar, pred_keras_)[0,1],4)))\nplt.show()","e1e74eb5":"# Show the built models\nkeras_export = keras.export_model()\nkeras_export.summary() # Scroll to the end of the warnings to find the neural network summary","fb239a2a":"!pip install -q -U git+https:\/\/github.com\/mljar\/mljar-supervised.git@master","51215d35":"from supervised.automl import AutoML","3dceb279":"# Create the model\nmljar = AutoML(mode=\"Compete\",  eval_metric=\"rmse\", total_time_limit=300,\n               features_selection=True)\n\n# Fit the training data\nmljar.fit(X_train, y_train)","ffd66e51":"# Predict the training data\nmljar_pred = mljar.predict(X_val)","97f387e8":"# Scatter plot true and predicted values\nplt.scatter(mljar_pred, y_val, alpha=0.2)\nplt.xlabel('predicted')\nplt.ylabel('true value')\nplt.text(100000, 400000, 'RMSE: ' + str(round(MSE(y_val, mljar_pred)**0.5)))\nplt.text(100000, 350000, 'MAE: ' + str(round(mean_absolute_error(y_val, mljar_pred))))\nplt.text(100000, 300000, 'R: ' + str(round(np.corrcoef(y_val, mljar_pred)[0,1],4)))\nplt.show()","52ae4334":"# Show the model results\nmljar.report()","3204bd4a":"!pip install -U pip\n!pip install -U setuptools wheel\n!pip install -U \"mxnet<2.0.0\"\n!pip install autogluon  ","9fac866d":"from autogluon.tabular import TabularDataset, TabularPredictor","40660e95":"# Prepare the data\nXy_train = X_train.reset_index(drop=True)\nXy_train['Target'] = y_train\n\nXy_val = X_val.reset_index(drop=True)\nXy_val['Target'] = y_val\n\nX_train_gluon = TabularDataset(Xy_train)\nX_val_gluon = TabularDataset(Xy_val)\n\n# Fit the training data\ngluon = TabularPredictor(label='Target').fit(X_train_gluon, time_limit=120)","36d98e24":"# Predict the training data\ngluon_pred = gluon.predict(X_val)\n\n# Scatter plot true and predicted values\nplt.scatter(gluon_pred, y_val, alpha=0.2)\nplt.xlabel('predicted')\nplt.ylabel('true value')\nplt.text(100000, 400000, 'RMSE: ' + str(round(MSE(y_val_ar, gluon_pred)**0.5)))\nplt.text(100000, 350000, 'MAE: ' + str(round(mean_absolute_error(y_val_ar, gluon_pred))))\nplt.text(100000, 300000, 'R: ' + str(round(np.corrcoef(y_val_ar, gluon_pred)[0,1],4)))\nplt.show()","ca74780f":"# Show the models\nleaderboard = gluon.leaderboard(X_train_gluon)","2e210f0e":"# Show the models\nleaderboard","bef76144":"# 2. Tree-based Pipeline Optimization Tool (TPOT)","604418cd":"# \ud83d\udcda Automated Machine Learning-Regression\n\nThis notebook provides Automated Machine Learning (AutoML) algorithms for a regression task. Data preparation is just simply performed as the pre-processing will be automatically done, followed by building Machine Learning algorithms and tuning the hyperparameters. The objective of this notebook is to serve as a cheat sheet.","6fbc13ab":"# 3. Hyperopt","9542e5ff":"# 1. Auto-Sklearn","a8f6e917":"Below is the tpot_model.py with a little adjustment.","977dfd8a":"# 5. MLJAR","8acfb3b6":"# 6. AutoGluon","ad2a138e":"# 4. AutoKeras","a9e57af2":"To find the process of feature selection, please visit this notebook https:\/\/www.kaggle.com\/rendyk\/regression-rmse-house-prices\n\nThat notebook demonstrates regression using conventional Machine Learning algorithms for learning the same dataset."}}