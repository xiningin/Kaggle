{"cell_type":{"9dfe49e7":"code","27c493f8":"code","5f78d37a":"code","927ac258":"code","e3a50122":"code","e794cfd9":"code","20e98c9b":"code","2496430e":"code","cc319f5e":"code","2c26fa83":"code","68178f7f":"code","baa3abcf":"markdown","5cd13e1f":"markdown","be93ff17":"markdown","e484619d":"markdown","73f9b212":"markdown","89712b79":"markdown","72dd2bea":"markdown","00ba670e":"markdown","bca9cd4b":"markdown","a8c45221":"markdown","98610c4b":"markdown"},"source":{"9dfe49e7":"from sklearn.datasets import make_classification, make_regression\nfrom sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold, RepeatedKFold\nfrom numpy import std, mean\nimport numpy as np\n\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor\n\nfrom xgboost import XGBClassifier, XGBRegressor\n\nfrom lightgbm import LGBMClassifier, LGBMRegressor\n\nfrom catboost import CatBoostClassifier, CatBoostRegressor","27c493f8":"# define dataset\nX, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n# summarize the dataset\nprint(X.shape, y.shape)\nmodel = GradientBoostingClassifier()\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\nprint('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n# fit the model on the whole dataset\nmodel = GradientBoostingClassifier()\nmodel.fit(X, y)\n# make a single prediction\nrow = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]]\nyhat = model.predict(row)\nprint('Prediction: %d' % yhat[0])","5f78d37a":"# define dataset\nX, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n# evaluate the model\nmodel = GradientBoostingRegressor()\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\nprint('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n# fit the model on the whole dataset\nmodel = GradientBoostingRegressor()\nmodel.fit(X, y)\n# make a single prediction\nrow = [[2.02220122, 0.31563495, 0.82797464, -0.30620401, 0.16003707, -1.44411381, 0.87616892, -0.50446586, 0.23009474, 0.76201118]]\nyhat = model.predict(row)\nprint('Prediction: %.3f' % yhat[0])","927ac258":"# define dataset\nX, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n# evaluate the model\nmodel = HistGradientBoostingClassifier()\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\nprint('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n# fit the model on the whole dataset\nmodel = HistGradientBoostingClassifier()\nmodel.fit(X, y)\n# make a single prediction\nrow = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]]\nyhat = model.predict(row)\nprint('Prediction: %d' % yhat[0])","e3a50122":"# define dataset\nX, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n# evaluate the model\nmodel = HistGradientBoostingRegressor()\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\nprint('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n# fit the model on the whole dataset\nmodel = HistGradientBoostingRegressor()\nmodel.fit(X, y)\n# make a single prediction\nrow = [[2.02220122, 0.31563495, 0.82797464, -0.30620401, 0.16003707, -1.44411381, 0.87616892, -0.50446586, 0.23009474, 0.76201118]]\nyhat = model.predict(row)\nprint('Prediction: %.3f' % yhat[0])","e794cfd9":"# define dataset\nX, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n# evaluate the model\nmodel = XGBClassifier(n_jobs=-1, use_label_encoder=False)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\nprint('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n# fit the model on the whole dataset\nmodel = XGBClassifier(n_jobs=-1, use_label_encoder=False)\nmodel.fit(X, y)\n# make a single prediction\nrow = [2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]\nrow = np.array(row).reshape((1, len(row)))\nyhat = model.predict(row)\nprint('Prediction: %d' % yhat[0])","20e98c9b":"# define dataset\nX, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n# evaluate the model\nmodel = XGBRegressor(objective='reg:squarederror')\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\nprint('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n# fit the model on the whole dataset\nmodel = XGBRegressor(objective='reg:squarederror')\nmodel.fit(X, y)\n# make a single prediction\nrow = [2.02220122, 0.31563495, 0.82797464, -0.30620401, 0.16003707, -1.44411381, 0.87616892, -0.50446586, 0.23009474, 0.76201118]\nrow = np.array(row).reshape((1, len(row)))\nyhat = model.predict(row)\nprint('Prediction: %.3f' % yhat[0])","2496430e":"# define dataset\nX, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n# evaluate the model\nmodel = LGBMClassifier()\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\nprint('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n# fit the model on the whole dataset\nmodel = LGBMClassifier()\nmodel.fit(X, y)\n# make a single prediction\nrow = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]]\nyhat = model.predict(row)\nprint('Prediction: %d' % yhat[0])","cc319f5e":"# define dataset\nX, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n# evaluate the model\nmodel = LGBMRegressor()\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\nprint('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n# fit the model on the whole dataset\nmodel = LGBMRegressor()\nmodel.fit(X, y)\n# make a single prediction\nrow = [[2.02220122, 0.31563495, 0.82797464, -0.30620401, 0.16003707, -1.44411381, 0.87616892, -0.50446586, 0.23009474, 0.76201118]]\nyhat = model.predict(row)\nprint('Prediction: %.3f' % yhat[0])","2c26fa83":"# define dataset\nX, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n# evaluate the model\nmodel = CatBoostClassifier(verbose=0, n_estimators=100)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\nprint('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n# fit the model on the whole dataset\nmodel = CatBoostClassifier(verbose=0, n_estimators=100)\nmodel.fit(X, y)\n# make a single prediction\nrow = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]]\nyhat = model.predict(row)\nprint('Prediction: %d' % yhat[0])","68178f7f":"# define dataset\nX, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n# evaluate the model\nmodel = CatBoostRegressor(verbose=0, n_estimators=100)\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\nprint('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n# fit the model on the whole dataset\nmodel = CatBoostRegressor(verbose=0, n_estimators=100)\nmodel.fit(X, y)\n# make a single prediction\nrow = [[2.02220122, 0.31563495, 0.82797464, -0.30620401, 0.16003707, -1.44411381, 0.87616892, -0.50446586, 0.23009474, 0.76201118]]\nyhat = model.predict(row)\nprint('Prediction: %.3f' % yhat[0])","baa3abcf":"# CatBoost Regression","5cd13e1f":"Thanks to a wonderful site,\nhttps:\/\/machinelearningmastery.com\/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost\/\nThe code belongs to the above site. There might be some minor changes.","be93ff17":"# LightGBM for Classification","e484619d":"# CatBoost Classifier","73f9b212":"# XGBoost for Classification","89712b79":"# Gradient Boosting with Scikit-Learn","72dd2bea":"# LightGBM for Regression","00ba670e":"# Histogram-Based Gradient Boosting Machine for Regression","bca9cd4b":"# Histogram-Based Gradient Boosting\n\nThe scikit-learn library provides an alternate implementation of the gradient boosting algorithm, referred to as histogram-based gradient boosting.\n\nThis is an alternate approach to implement gradient tree boosting inspired by the LightGBM library (described more later). This implementation is provided via the HistGradientBoostingClassifier and HistGradientBoostingRegressor classes.\n\nThe primary benefit of the histogram-based approach to gradient boosting is speed. These implementations are designed to be much faster to fit on training data.\n\nAt the time of writing, this is an experimental implementation and requires that you add the following line to your code to enable access to these classes.","a8c45221":"# XGBoost for Regression","98610c4b":"# Gradient Boosting Machine for Regression"}}