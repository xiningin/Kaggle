{"cell_type":{"9393cb81":"code","ca022f6c":"code","dc13013c":"code","0696da54":"code","abddc4e1":"code","c36a0868":"code","e62b4a8b":"code","079deb82":"code","abfaa904":"code","23822ea2":"code","1b8d1f92":"code","6b7b28c5":"code","a044daa3":"code","423a0952":"code","3c94fe32":"code","0812de66":"code","eef3a331":"code","0a4a2901":"markdown","6ffacfa4":"markdown","77fe1e3d":"markdown"},"source":{"9393cb81":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ca022f6c":"from sklearn.preprocessing import RobustScaler\nimport pandas as pd\nimport torch\nimport torch.optim as optim\nimport numpy as np\nimport random\ntorch.manual_seed(1)","dc13013c":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nrandom.seed(777)\ntorch.manual_seed(777)\n\nif device == 'cuda':\n  torch.cuda.manual_seed_all(777)","0696da54":"seaice_data = pd.read_csv('\/content\/seaice_train.csv')","abddc4e1":"from datetime import datetime as dt\nfrom dateutil.parser import parse\nimport time","c36a0868":"def toYearFraction(date):\n    def sinceEpoch(date): # returns seconds since epoch\n        return time.mktime(date.timetuple())\n    s = sinceEpoch\n\n    year = date.year\n    startOfThisYear = dt(year=year, month=1, day=1)\n    startOfNextYear = dt(year=year+1, month=1, day=1)\n    yearElapsed = s(date) - s(startOfThisYear)\n    yearDuration = s(startOfNextYear) - s(startOfThisYear)\n    # \ucd08\ub2e8\uc704\ub85c 1\ub144 \/ \ud604\uc7ac\ub0a0\uc9dc \uc758 \ube44\uc728 \n    fraction = yearElapsed\/yearDuration\n    return date.year + fraction","e62b4a8b":"train_decimal_year = []\n\ntrain_decimal_year.append(toYearFraction(parse(\"1978-11-15\")))\ntrain_decimal_year.append(toYearFraction(parse(\"1978-12-15\")))\n\nfor i in range(1979, 2019):\n  if i == 2016:\n    continue\n  for m in range(1 , 13):\n    flag = int(m \/ 10)\n    if flag == 0:\n      date = (\"{}-0{}-15\".format(i,m))\n    else:\n      date = (\"{}-{}-15\".format(i,m))\n    train_decimal_year.append(toYearFraction(parse(date)))\n\ntrain_decimal_year.append(toYearFraction(parse(\"2019-01-15\")))\ntrain_decimal_year.append(toYearFraction(parse(\"2019-02-15\")))\ntrain_decimal_year.append(toYearFraction(parse(\"2019-03-15\")))\ntrain_decimal_year.append(toYearFraction(parse(\"2019-04-15\")))\ntrain_decimal_year.append(toYearFraction(parse(\"2019-05-15\")))\n\ntrain_decimal_year = np.array(train_decimal_year)\ntrain_decimal_year.shape\nseaice_data = seaice_data.drop(['month'],axis = 1)\n\nfor i in range(475):\n  seaice_data.iloc[i,0] = train_decimal_year[i]\n\nseaice_data.tail(30)\nseaice_data.iloc[447:475]\ntrain_csv = seaice_data","079deb82":"x_carbon = np.array(seaice_data.iloc[0:447,0:4])\nx_seaice = np.array(seaice_data.iloc[0:447,5])\ny_carbon = np.array(seaice_data.iloc[0:447,4])\nscaler = RobustScaler()\nx_carbon_s = scaler.fit_transform(x_carbon)","abfaa904":"learning_rate = 0.01\nbatch_size = 10\ndrop_prob = 0.1","23822ea2":"x_ctrain = torch.FloatTensor(x_carbon_s).to(device)\ny_ctrain = torch.FloatTensor(np.transpose(y_carbon[np.newaxis])).to(device)\n\nc_dataset = torch.utils.data.TensorDataset(x_ctrain, y_ctrain)\nc_data_loader = torch.utils.data.DataLoader(dataset = c_dataset,batch_size = batch_size, shuffle= True, drop_last = True)\n\nclinear1 = torch.nn.Linear(4,4,bias=True)\nclinear2 = torch.nn.Linear(4,1,bias=True)\n\nrelu = torch.nn.ReLU()\ndropout = torch.nn.Dropout(p=drop_prob)\n\ntorch.nn.init.xavier_normal_(clinear1.weight)\ntorch.nn.init.xavier_normal_(clinear2.weight)\ntorch.nn.init.xavier_normal_(clinear3.weight)\ntorch.nn.init.xavier_normal_(clinear4.weight)\n\ncmodel = torch.nn.Sequential(clinear1,relu,clinear2).to(device)","1b8d1f92":"import torch.nn.functional as F\ntotal_batch = len(c_data_loader)\noptimizer = optim.Adam(cmodel.parameters(), lr = 1)\nloss = torch.nn.MSELoss().to(device)\n\nnb_epochs = 500\nfor epochs in range(nb_epochs + 1):\n  avg_cost = 0\n  for cX,cY in c_data_loader:\n    cX = cX.view(-1, 4).to(device)\n    cY = cY.to(device)\n\n    optimizer.zero_grad()\n    hypothesis = cmodel(cX)\n    cost = loss(hypothesis,cY)\n    cost.backward()\n    optimizer.step()\n\n    avg_cost += cost \/ total_batch\n  if epochs % 10 == 0:\n    print(\"epochs {}\/{:4d}, cost {:.6f}\".format(epochs, nb_epochs, avg_cost))","6b7b28c5":"carbon_x_test = np.array(seaice_data.iloc[447:475,0:4])\ncarbon_xs_test = scaler.fit_transform(carbon_x_test)\nx_ctest = torch.FloatTensor(carbon_xs_test).to(device)\nwith torch.no_grad():\n  cmodel.eval()\n  carbon_pred = cmodel(x_ctest)\ncarbon_pred","a044daa3":"carbon_pred = carbon_pred.detach().cpu()\ncarbon_pred = carbon_pred.detach().numpy()\nindex = 0\nfor i in range(447,475):\n   seaice_data.iloc[i,4] = carbon_pred[index]\n   index = index + 1","423a0952":"train_csv = seaice_data\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nrandom.seed(1)\ntorch.manual_seed(1)\nif device == 'cuda':\n  torch.cuda.manual_seed_all(1)\n\ntrain_D = np.array(train_csv.iloc[:, 0:5])\ntrain_D = scaler.fit_transform(train_D)\ntrain_L = np.transpose(np.array(train_csv.iloc[:,5])[np.newaxis])\ntrain_D = torch.FloatTensor(train_D)\ntrain_L = torch.FloatTensor(train_L)\n\ndataset = torch.utils.data.TensorDataset(train_D, train_L)\ndata_loader = torch.utils.data.DataLoader(dataset = dataset,batch_size = batch_size, shuffle= True, drop_last = True)\n\nlinear1 = torch.nn.Linear(train_D.shape[1],32,bias=True)\n#linear_ = torch.nn.Linear(32,32,bias=True)\nlinear2 = torch.nn.Linear(32,128,bias=True)\nlinear3 = torch.nn.Linear(128,64,bias=True)\nlinear4 = torch.nn.Linear(64,64,bias=True) \nlinear5 = torch.nn.Linear(64,1,bias=True)\nrelu = torch.nn.ReLU()\ndropout = torch.nn.Dropout(p=drop_prob)\n\nmodel = torch.nn.Sequential(linear1,relu,\n                            linear2,relu,\n                            linear3,relu,\n                            linear4,relu,\n                            linear5).to(device)\n\nloss = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n\ntorch.nn.init.kaiming_uniform_(linear1.weight)\n#torch.nn.init.xavier_uniform_(linear_.weight)\ntorch.nn.init.kaiming_uniform_(linear2.weight)\ntorch.nn.init.kaiming_uniform_(linear3.weight)\ntorch.nn.init.kaiming_uniform_(linear4.weight)\ntorch.nn.init.kaiming_uniform_(linear5.weight)","3c94fe32":"total_batch = len(data_loader)\nfor epoch in range(1000):\n    avg_cost = 0\n\n    for X, Y in data_loader:\n\n        X = X.view(-1, train_D.shape[1]).to(device)      \n        Y = Y.to(device)\n\n        # \uadf8\ub798\ub514\uc5b8\ud2b8 \ucd08\uae30\ud654\n        optimizer.zero_grad()\n        # Forward \uacc4\uc0b0\n        hypothesis = model(X)\n        # Error \uacc4\uc0b0\n        cost = loss(hypothesis, Y)\n        # Backparopagation\n        cost.backward()\n        # \uac00\uc911\uce58 \uac31\uc2e0\n        optimizer.step()\n\n        # \ud3c9\uade0 Error \uacc4\uc0b0\n        avg_cost += cost \/ total_batch\n    if epoch % 10 == 0:\n      print('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))\n\nprint('Learning finished')","0812de66":"test_csv = pd.read_csv('\/content\/seaice_test.csv')\n\ntest_decimal_year = []\n\nfor m in range(1 , 13):\n  flag = int(m \/ 10)\n  if flag == 0:\n    date = (\"2016-0{}-15\".format(m))\n  else:\n    date = (\"2016-{}-15\".format(m))\n  test_decimal_year.append(toYearFraction(parse(date)))\n\ntest_decimal_year = np.array(test_decimal_year )\ntest_decimal_year\n\ntest_csv = test_csv.drop(['month'],axis = 1)\n\nfor i in range(12):\n  test_csv.iloc[i,0] = test_decimal_year[i]\n\ntest_D = scaler.transform(np.array(test_csv))\ntest_D = torch.FloatTensor(test_D).to(device)\n\nwith torch.no_grad():\n  model.eval()\n  pred = model(test_D).to(device)","eef3a331":"sample = pd.read_csv('.\/seaice_sample.csv')\nsample['seaice_extent'] = pred.cpu().detach().numpy()\nsample.to_csv('.\/defense4.csv',index = False)","0a4a2901":"**<h1 id=\"two\" style=\"color:purple;\">- \ud574\ube59 \uc608\uce21 NN \uc7ac\uc124\uacc4, \ucd08\uae30\ud654 kaiming \uc73c\ub85c \ubcc0\uacbd <\/h1>**","6ffacfa4":"# \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc704\ud574 \ucd94\uac00\ud55c \ubd80\ubd84\n\n**<h1 id=\"one_one\" style=\"color:purple;\">- Carbon \uc608\uce21 NN \uc124\uacc4 <\/h1>**","77fe1e3d":"<h1 style=\"font-family: 'Garamond';\"> <b><i>Prediction Of Sea Ice - defense score (2.08608)<\/i><\/b><\/h1>\n\n--- \n\n<b><a>* \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc704\ud574 \ucd94\uac00\ub41c \ubd80\ubd84<\/a><br><\/b>\n&emsp;&emsp;<a href=\"#one_one\">- Carbon \uc608\uce21 NN \uc124\uacc4 <\/a><br>\n&emsp;&emsp;<a href=\"#one_two\">- \ud574\ube59 \uc608\uce21 NN \uc7ac\uc124\uacc4, \ucd08\uae30\ud654 kaiming \uc73c\ub85c \ubcc0\uacbd <\/a><br>\n\n---"}}