{"cell_type":{"58bcbf8a":"code","036e803b":"code","8a27d680":"code","4edc1775":"code","c31a6056":"code","0edbc896":"code","a723ddb9":"code","272f685d":"code","626f4a13":"code","e4d722b6":"code","6987638e":"code","a703226f":"code","73cc8d25":"code","dd029b2d":"code","7492671e":"code","01b39dbd":"code","d3fca786":"code","c7607eaa":"code","37f63cfa":"code","69139a1a":"code","c793d645":"code","5a257896":"code","d29d44c5":"code","caaf2d78":"code","ab18701f":"code","d860c054":"code","2573d802":"code","25fbe96f":"code","992a9f8a":"code","0c63ce5b":"code","9e6751ba":"code","18f2fb7a":"code","4b91047e":"code","58585812":"code","3cf40eb5":"code","fa585e0c":"markdown","3968ae06":"markdown","0ee0cecf":"markdown","7f5decfb":"markdown","49431506":"markdown","480f7d5d":"markdown","faa7ccdc":"markdown","d3c15aef":"markdown","e6e156f7":"markdown","1356bd03":"markdown","49a29dd5":"markdown","9e8f735d":"markdown","8b2bb4f3":"markdown"},"source":{"58bcbf8a":"! pip install -q \/kaggle\/input\/readability\/readability-0.3.1-py3-none-any.whl\n! pip install -q \/kaggle\/input\/syntok\/syntok-1.3.1-py3-none-any.whl\nfrom textblob import TextBlob\nimport readability\nimport syntok.segmenter as segmenter\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","036e803b":"train_data = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv')","8a27d680":"train_data.info()\ntrain_data.head()","4edc1775":"test_data.info()\ntest_data.head()","c31a6056":"pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv')","0edbc896":"def sentiment_analysis(text):\n    return TextBlob(text).sentiment.polarity","a723ddb9":"def tokenize(text):\n    \"\"\"Tokenizing and creating excerpts in the format suggested in the README of readability project.\"\"\"\n    return '\\n\\n'.join(\n        '\\n'.join(\n            ' '.join(token.value for token in sentence)\n            for sentence in paragraph)\n        for paragraph in segmenter.analyze(text))","272f685d":"train_data.loc[:,'readability_object'] = train_data.apply(lambda row: readability.getmeasures(tokenize(row.excerpt), lang='en'), axis=1)","626f4a13":"train_data.info()\ntrain_data.head()","e4d722b6":"X = pd.DataFrame(train_data['id'])\nX.loc[:,'readability'] = train_data.apply(lambda row: row.readability_object['readability grades']['SMOGIndex'], axis=1)\nX.loc[:,'sentiment'] = train_data.apply(lambda row: sentiment_analysis(row.excerpt), axis=1)\nX.loc[:,'characters_per_word'] = train_data.apply(lambda row: row.readability_object['sentence info']['characters_per_word'], axis=1)\nX.loc[:,'syll_per_word'] = train_data.apply(lambda row: row.readability_object['sentence info']['syll_per_word'], axis=1)\nX.loc[:,'words_per_sentence'] = train_data.apply(lambda row: row.readability_object['sentence info']['words_per_sentence'], axis=1)\nX.loc[:,'sentences_per_paragraph'] = train_data.apply(lambda row: row.readability_object['sentence info']['sentences_per_paragraph'], axis=1)\nX.loc[:,'type_token_ratio'] = train_data.apply(lambda row: row.readability_object['sentence info']['type_token_ratio'], axis=1)\nX.loc[:,'characters'] = train_data.apply(lambda row: row.readability_object['sentence info']['characters'], axis=1)\nX.loc[:,'syllables'] = train_data.apply(lambda row: row.readability_object['sentence info']['syllables'], axis=1)\nX.loc[:,'words'] = train_data.apply(lambda row: row.readability_object['sentence info']['words'], axis=1)\nX.loc[:,'wordtypes'] = train_data.apply(lambda row: row.readability_object['sentence info']['wordtypes'], axis=1)\nX.loc[:,'sentences'] = train_data.apply(lambda row: row.readability_object['sentence info']['sentences'], axis=1)\nX.loc[:,'long_words'] = train_data.apply(lambda row: row.readability_object['sentence info']['long_words'], axis=1)\nX.loc[:,'complex_words'] = train_data.apply(lambda row: row.readability_object['sentence info']['complex_words'], axis=1)\nX.loc[:,'complex_words_dc'] = train_data.apply(lambda row: row.readability_object['sentence info']['complex_words_dc'], axis=1)\nX.loc[:,'tobeverb'] = train_data.apply(lambda row: row.readability_object['word usage']['tobeverb'], axis=1)\nX.loc[:,'auxverb'] = train_data.apply(lambda row: row.readability_object['word usage']['auxverb'], axis=1)\nX.loc[:,'conjunction'] = train_data.apply(lambda row: row.readability_object['word usage']['conjunction'], axis=1)\nX.loc[:,'pronoun'] = train_data.apply(lambda row: row.readability_object['word usage']['pronoun'], axis=1)\nX.loc[:,'preposition'] = train_data.apply(lambda row: row.readability_object['word usage']['preposition'], axis=1)\nX.loc[:,'nominalization'] = train_data.apply(lambda row: row.readability_object['word usage']['nominalization'], axis=1)","6987638e":"X.info()\nX.head()","a703226f":"corr = X.corr()\ncorr.info()\ncorr","73cc8d25":"fig, ax =plt.subplots(figsize=(8, 6))\nplt.title(\"Correlation Plot\")\nsns.heatmap(corr,\n            mask=np.zeros_like(corr, dtype=np.bool),\n            cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax)\nplt.show()","dd029b2d":"tar_corr = pd.merge(X, train_data['target'], left_index=True, right_index=True).corr().loc['target']\ntar_corr","7492671e":"to_remove = {'id', 'characters_per_word', 'characters', 'long_words', 'complex_words', 'sentences_per_paragraph'}\nfor val in tar_corr.index:\n    if tar_corr[val] > -0.1 and tar_corr[val] < 0.1:\n        to_remove.add(val)\n\nlist(to_remove)","01b39dbd":"X = X.drop(to_remove, axis=1)","d3fca786":"X.info()\nX.head()","c7607eaa":"corr = X.corr()\nfig, ax =plt.subplots(figsize=(8, 6))\nplt.title(\"Correlation Plot\")\nsns.heatmap(corr,\n            mask=np.zeros_like(corr, dtype=np.bool),\n            cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax)\nplt.show()","37f63cfa":"y = train_data['target']","69139a1a":"y.describe()","c793d645":"train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=0)","5a257896":"model = RandomForestRegressor(random_state=0)\ncv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=0)\nspace = {'n_estimators': [1000, 3000, 10000],\n         'max_depth': [3, 10, 30, 100]}\nsearch = GridSearchCV(model, space, cv=cv, scoring='neg_root_mean_squared_error', n_jobs=-1)\nresult = search.fit(train_X, train_y)","d29d44c5":"print(result.best_score_)\nprint(result.best_params_)","caaf2d78":"model = RandomForestRegressor(random_state=0, n_estimators=result.best_params_['n_estimators'], max_depth=result.best_params_['max_depth'])\n\nmodel.fit(train_X, train_y)","ab18701f":"train_preds = model.predict(train_X)\nmean_squared_error(train_y, train_preds)","d860c054":"val_preds = model.predict(val_X)\nmean_squared_error(val_y, val_preds)","2573d802":"test_data.loc[:,'readability_object'] = test_data.apply(lambda row: readability.getmeasures(tokenize(row.excerpt), lang='en'), axis=1)","25fbe96f":"test_data.info()\ntest_data.head()","992a9f8a":"X_test = pd.DataFrame(test_data['id'])\nX_test.loc[:,'readability'] = test_data.apply(lambda row: row.readability_object['readability grades']['SMOGIndex'], axis=1)\nX_test.loc[:,'sentiment'] = test_data.apply(lambda row: sentiment_analysis(row.excerpt), axis=1)\nX_test.loc[:,'characters_per_word'] = test_data.apply(lambda row: row.readability_object['sentence info']['characters_per_word'], axis=1)\nX_test.loc[:,'syll_per_word'] = test_data.apply(lambda row: row.readability_object['sentence info']['syll_per_word'], axis=1)\nX_test.loc[:,'words_per_sentence'] = test_data.apply(lambda row: row.readability_object['sentence info']['words_per_sentence'], axis=1)\nX_test.loc[:,'sentences_per_paragraph'] = test_data.apply(lambda row: row.readability_object['sentence info']['sentences_per_paragraph'], axis=1)\nX_test.loc[:,'type_token_ratio'] = test_data.apply(lambda row: row.readability_object['sentence info']['type_token_ratio'], axis=1)\nX_test.loc[:,'characters'] = test_data.apply(lambda row: row.readability_object['sentence info']['characters'], axis=1)\nX_test.loc[:,'syllables'] = test_data.apply(lambda row: row.readability_object['sentence info']['syllables'], axis=1)\nX_test.loc[:,'words'] = test_data.apply(lambda row: row.readability_object['sentence info']['words'], axis=1)\nX_test.loc[:,'wordtypes'] = test_data.apply(lambda row: row.readability_object['sentence info']['wordtypes'], axis=1)\nX_test.loc[:,'sentences'] = test_data.apply(lambda row: row.readability_object['sentence info']['sentences'], axis=1)\nX_test.loc[:,'long_words'] = test_data.apply(lambda row: row.readability_object['sentence info']['long_words'], axis=1)\nX_test.loc[:,'complex_words'] = test_data.apply(lambda row: row.readability_object['sentence info']['complex_words'], axis=1)\nX_test.loc[:,'complex_words_dc'] = test_data.apply(lambda row: row.readability_object['sentence info']['complex_words_dc'], axis=1)\nX_test.loc[:,'tobeverb'] = test_data.apply(lambda row: row.readability_object['word usage']['tobeverb'], axis=1)\nX_test.loc[:,'auxverb'] = test_data.apply(lambda row: row.readability_object['word usage']['auxverb'], axis=1)\nX_test.loc[:,'conjunction'] = test_data.apply(lambda row: row.readability_object['word usage']['conjunction'], axis=1)\nX_test.loc[:,'pronoun'] = test_data.apply(lambda row: row.readability_object['word usage']['pronoun'], axis=1)\nX_test.loc[:,'preposition'] = test_data.apply(lambda row: row.readability_object['word usage']['preposition'], axis=1)\nX_test.loc[:,'nominalization'] = test_data.apply(lambda row: row.readability_object['word usage']['nominalization'], axis=1)","0c63ce5b":"X_test = X_test.drop(to_remove, axis=1)","9e6751ba":"X_test.info()\nX_test.head()","18f2fb7a":"test_preds = model.predict(X_test)","4b91047e":"solution = pd.DataFrame(test_data['id'])\nsolution.loc[:, 'target'] = test_preds","58585812":"solution.info()","3cf40eb5":"solution.to_csv('submission.csv', index=False)","fa585e0c":"# Creating features for test set and predicting results","3968ae06":"We can see that following groups of features has high correlation\n - characters_per_word and syll_per_word\n - characters and syllables\n - long_words, complex_words and complex_words_dc\n - sentences and sentences_per_paragraph\n \nSo, for all these three groups, we will only take one features.","0ee0cecf":"In this notebook, I try to solve the [CommonLit Readability Prize](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/overview) competition using [Random Forest](https:\/\/en.wikipedia.org\/wiki\/Random_forest).\n\nI have created a similar model using [Decision Tree](https:\/\/en.wikipedia.org\/wiki\/Decision_tree) which got a score of 0.941 and using Random Forest which got a score of 0.923. Both of these models were trained using fewer features (Readability, Length and Sentiment). The notebook for the models are:\n - [Decision Tree with score 0.941](https:\/\/www.kaggle.com\/aniketsharma00411\/commonlit-readability-decision-tree)\n - [Random Forest with score 0.923](https:\/\/www.kaggle.com\/aniketsharma00411\/random-forest)\n\nIn this model I will first find all the information I can gather from excerpts using the [readability](https:\/\/pypi.org\/project\/readability\/) Python package. Then, I will find and remove features which are either redundant or does not provide insights for the target. After Feature Engineering, I will train a Random Forest regressor, optimal hyperparameter for which would be found using Grid Search.","7f5decfb":"I will be using the **SMOGIndex** readability grade as it was found to be best in [this notebook](https:\/\/www.kaggle.com\/aniketsharma00411\/commonlit-readability-data-observations).","49431506":"\n\nI am using the [readability](https:\/\/pypi.org\/project\/readability\/) and [syntok](https:\/\/pypi.org\/project\/syntok\/) to gather features from excerpts and [textblob](https:\/\/pypi.org\/project\/textblob\/) for sentiment analysis.\n","480f7d5d":"# Creating Features","faa7ccdc":"We will remove every feature with correlation value between -0.1 and 0.1.","d3c15aef":"Now, the correlation matrix looks much better.","e6e156f7":"# Functions","1356bd03":"# Training","49a29dd5":"# Evaluating the result","9e8f735d":"# Initialization","8b2bb4f3":"Using Grid Search to find the optimal values of hyperparameters."}}