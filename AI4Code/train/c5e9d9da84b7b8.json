{"cell_type":{"4a42ff0e":"code","f6c0a1c3":"code","8bc0eea2":"code","3ffdef76":"code","00b192fe":"code","d54d109a":"code","504f990a":"code","d22286e4":"code","8243f52c":"code","36e81bbe":"code","5bdde48c":"code","e03a7bba":"code","998f1733":"code","765c2589":"code","99f9f753":"code","23715c42":"code","edb162f7":"code","81ebcabc":"markdown","96ab6a2e":"markdown","0194619c":"markdown","df13d636":"markdown","21beb684":"markdown","78d827f5":"markdown"},"source":{"4a42ff0e":"# Import datasets & libraries\nfrom keras.datasets import cifar100\nimport matplotlib.pyplot as plt\nimport numpy as np","f6c0a1c3":"# Download dataset of CIFAR-100\n(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n\n# Check the shape of the array\nprint(f\"x_train shape: {x_train.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"x_test shape: {x_test.shape}\")\nprint(f\"y_test shape: {y_test.shape}\")\n\n# Number of samples in dataset\nprint(f\"Train: {x_train.shape[0]}\")\nprint(f\"Test: {x_test.shape[0]}\")\n\n# Data format\nprint(type(x_train))\nprint(type(y_train))","8bc0eea2":"# Show randomly 16 images in the CIFAR-100\nplt.figure(figsize=(10, 10))\nfor i in range (16):\n  rand_num = np.random.randint(0, 50000)\n  cifar_img = plt.subplot(4,4, i+1)\n  plt.imshow(x_train[rand_num])","3ffdef76":"# Import libraries for preprocessing images\nfrom tensorflow.keras.utils import to_categorical\n\n# Normalize images\ntrain_images = x_train.astype('float32')\/255\ntest_images = x_test.astype('float32')\/255\n\n# Transform labels to one hot encoding\ntrain_labels = to_categorical(y_train)\ntest_labels = to_categorical(y_test)","00b192fe":"# Import Libraries for CNN\nfrom keras.models import Sequential\nfrom keras.layers import MaxPooling2D, Conv2D, Flatten, Dense, Activation, Dropout","d54d109a":"# Plot function for visualisation training process\nimport matplotlib.pyplot as plt\n\ndef training_plot(history):\n  acc = history.history['acc']\n  val_acc = history.history['val_acc']\n  loss = history.history['loss']\n  val_loss = history.history['val_loss']\n  epochs = range(1, len(acc) + 1)\n  plt.plot(epochs, acc, 'bo', label='Training acc')\n  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n  plt.title('Training and validation accuracy')\n  plt.legend()\n  plt.figure()\n  plt.plot(epochs, loss, 'bo', label='Training loss')\n  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n  plt.title('Training and validation loss')\n  plt.legend()\n  plt.show()","504f990a":"# Batch norm model 4\nfrom keras.models import Sequential\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.pooling import MaxPool2D\nfrom keras.layers.core import Dense,Activation,Dropout,Flatten\n\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.initializers import RandomNormal, Constant\nmodel = Sequential()\n \nmodel.add(Conv2D(256,(3,3),padding='same',input_shape=(32,32,3)))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(256,(3,3),padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n \nmodel.add(Conv2D(512,(3,3),padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(512,(3,3),padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(512,(3,3),padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(512,(3,3),padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(512,(3,3),padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(512,(3,3),padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Flatten())\nmodel.add(Dense(1024))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization(momentum=0.95, \n        epsilon=0.005,\n        beta_initializer=RandomNormal(mean=0.0, stddev=0.05), \n        gamma_initializer=Constant(value=0.9)))\nmodel.add(Dense(100,activation='softmax'))\nmodel.summary()","d22286e4":"# Data Augmentation\n# Adding data augmentation for creating more images\n# Divide train and validation set \nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Configuration for creating new images\ntrain_datagen = ImageDataGenerator(\n    rotation_range=20,\n    horizontal_flip=True,\n)\n\nX_train, X_validation, y_train, y_validation = train_test_split(train_images, train_labels, test_size=0.2, random_state=93)\ntrain_datagen.fit(X_train)","8243f52c":"# Configure the model for training\nfrom tensorflow.keras import optimizers\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.RMSprop(learning_rate=1e-4),\n              metrics=['acc'])","36e81bbe":"# Training model\nimport time\ntraining_start = time.time()\nhistory = model.fit(train_datagen.flow(X_train, y_train, batch_size=64),\n          steps_per_epoch=100, \n          epochs=350,\n          validation_data=(X_validation, y_validation),\n          verbose=1)\ntraining_stop = time.time()\ntraining_time = training_stop - training_start\nprint(f\"Training time: {training_time}\")","5bdde48c":"# Visualize training process\ntraining_plot(history)","e03a7bba":"scores = model.evaluate(test_images, test_labels)\nprint(f'accuracy on test set: {model.metrics_names[1]} of {scores[1]*100}')","998f1733":"# Translate categorial to array for drawing confusion matrix\nfrom sklearn.metrics import confusion_matrix\nfrom numpy import argmax\nprediction = []\ntrue_labels = []\n\npred = model.predict(test_images)\nprint(test_labels.shape[0])\nfor i in range(test_labels.shape[0]):\n  prediction.append(argmax(pred[i]))\n  true_labels.append(argmax(test_labels[i]))\n\ncm = confusion_matrix(prediction, true_labels)","765c2589":"# Name of all classes in CIFAR-100\nclasses = ['beaver', 'dolphin', 'otter', 'seal', 'whale', \n'aquarium' ,'fish', 'ray', 'shark', 'trout', \n'orchids', 'poppies', 'roses', 'sunflowers', 'tulips', \n'bottles', 'bowls', 'cans', 'cups', 'plates', \n'apples', 'mushrooms', 'oranges', 'pears', 'sweet peppers', \n'clock', 'computer keyboard', 'lamp', 'telephone', 'television', 'bed', 'chair', 'couch', 'table', 'wardrobe', \n'bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach', \n'bear', 'leopard', 'lion', 'tiger', 'wolf', \n'bridge', 'castle', 'house', 'road', 'skyscraper', \n'cloud', 'forest', 'mountain', 'plain', 'sea', \n'camel', 'cattle', 'chimpanzee', 'elephant', 'kangaroo', \n'fox', 'porcupine', 'possum', 'raccoon', 'skunk', \n'crab', 'lobster', 'snail', 'spider', 'worm', \n'baby', 'boy', 'girl', 'man', 'woman', \n'crocodile', 'dinosaur', 'lizard', 'snake', 'turtle', \n'hamster', 'mouse', 'rabbit', 'shrew', 'squirrel', \n'maple', 'oak', 'palm', 'pine', 'willow', \n'bicycle', 'bus', 'motorcycle', 'pickup truck', 'train', \n'lawn-mower', 'rocket', 'streetcar', 'tank', 'tractor']","99f9f753":"# Plot the confusion matrix\nimport matplotlib.pyplot as plt\nprint(cm)\nfig = plt.figure(figsize=(24,24))\nax = fig.add_subplot(211)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix of the classifier')\nfig.colorbar(cax)\nax.set_xticklabels([''] + classes)\nax.set_yticklabels([''] + classes)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()","23715c42":"# Calculating f1 score\nfrom sklearn.metrics import f1_score\nprint(f\"f1 score: {f1_score(true_labels, prediction, average='weighted')}\")","edb162f7":"from sklearn.metrics import classification_report\nprint(classification_report(true_labels, prediction, target_names=classes, digits=5))","81ebcabc":"# 4. Training Configurations","96ab6a2e":"# 5. Experiment results","0194619c":"# 2. Preprocessing for CIFAR-100 dataset","df13d636":"# 1. Exploring the CIFAR-100 Dataset","21beb684":"## CNN with Batch norm model","78d827f5":"# 3. Build CNN models for classification task"}}