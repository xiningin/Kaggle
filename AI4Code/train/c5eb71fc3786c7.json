{"cell_type":{"95f9ca53":"code","362fa215":"code","4f61ecc9":"code","c96d853d":"code","ee419e51":"code","37a9504e":"code","0e20be69":"code","8d83208e":"markdown","269e1579":"markdown","cb43ad1a":"markdown","227d9b0d":"markdown"},"source":{"95f9ca53":"import numpy as np\nimport pandas as pd\nimport os\nimport glob\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","362fa215":"train_dir = \"\/kaggle\/input\/deepfake-detection-challenge\/train_sample_videos\/\"\ntrain_video_files = glob.glob(train_dir+\"*.mp4\")\ntrain_metadata = pd.read_json('..\/input\/deepfake-detection-challenge\/train_sample_videos\/metadata.json').T\ntrain_metadata.head()","4f61ecc9":"print(train_metadata[\"original\"].value_counts()[0:12])","c96d853d":"train_frequent=train_metadata[train_metadata[\"original\"]==\"qtnjyomzwo.mp4\"]\n\nframe_num=0\nvideo_1=list(train_frequent.index)[0]\nvideo_2=list(train_frequent.index)[1]\n\nfig, axes = plt.subplots(1,3, figsize=(30,10))\n\ncap = cv2.VideoCapture(train_dir+video_1)\ncap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n_, image = cap.read()\nimage_1 = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\ncap.release()\naxes[0].imshow(image_1[0:500,850:1350,:])\naxes[0].title.set_text(f\"{video_1}\")\n\ncap = cv2.VideoCapture(train_dir+video_2)\ncap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n_, image = cap.read()\nimage_2 = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\ncap.release()\naxes[1].imshow(image_2[0:500,850:1350,:])\naxes[1].title.set_text(f\"{video_2}\")\n\nimage_3=np.sum((image_1-image_2)**2,axis=2)\naxes[2].imshow(image_3[0:500,850:1350])\naxes[2].title.set_text(\"Difference(MSE)\")\n\nplt.show()","ee419e51":"train_frequent=train_metadata[train_metadata[\"original\"]==\"xngpzquyhs.mp4\"]\n\nframe_num=0\nvideo_1=list(train_frequent.index)[0]\nvideo_2=list(train_frequent.index)[1]\n\nfig, axes = plt.subplots(1,3, figsize=(30,10))\n\ncap = cv2.VideoCapture(train_dir+video_1)\ncap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n_, image = cap.read()\nimage_1 = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\ncap.release()\naxes[0].imshow(image_1[0:400,800:1250,:])\naxes[0].title.set_text(f\"{video_1}\")\n\ncap = cv2.VideoCapture(train_dir+video_2)\ncap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n_, image = cap.read()\nimage_2 = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\ncap.release()\naxes[1].imshow(image_2[0:400,800:1250,:])\naxes[1].title.set_text(f\"{video_2}\")\n\nimage_3=np.sum((image_1-image_2)**2,axis=2)\naxes[2].imshow(image_3[0:400,800:1250])\naxes[2].title.set_text(\"Difference(MSE>{})\")\n\nplt.show()","37a9504e":"frame_num=0\nall_first_image=[]\nfor i, file_name in tqdm(enumerate(list(train_metadata.index))):\n    cap = cv2.VideoCapture(train_dir+file_name)\n    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n    _, image = cap.read()\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    cap.release()\n    image=cv2.resize(image,(96,128))\n    all_first_image.append(image)\nall_first_image=np.array(all_first_image)\nthresh=27#if MSE<thresh, regard pixels as the same\nsimilarity_matrix=np.sum(((all_first_image[:,np.newaxis,:,:]-all_first_image[np.newaxis,:,:,:])**2).reshape(400,400,-1)<thresh,axis=2)","0e20be69":"print(similarity_matrix.shape)\nplt.pcolor(similarity_matrix[:20,:20])","8d83208e":"This shows the similarity among first 20 videos.\nFor example, you can see video no.3 and no.10 have the same source. (As you know, this is already provided information.)","269e1579":"### I just introduce the way to find the face by similarity approach.\n\nFaces can be detected easily \nif there are more than 2 images whose sources are the same.","cb43ad1a":"First two images are two fake images and the right one is the RMS between them. You can see the large error appears in the face. Face area can be cropped with filter and\/or clustering.(I don't show here)\nThis can be used to check if your detector is working well.\n\nThe following is other example.","227d9b0d":"I've also checked the similarity among all images."}}