{"cell_type":{"390c9239":"code","a3ffac6d":"code","ec447c71":"code","31d0745c":"code","b8918801":"code","f00bbb83":"code","1f33e4fc":"code","e6906171":"code","1fad526c":"code","619fc366":"code","465ffca4":"code","0ff3a204":"code","2204d231":"code","4fed2a91":"code","e9d9b993":"code","8da76f93":"code","c4fb4290":"code","8e1ab7e4":"code","00219c27":"code","279d6903":"code","119e649d":"code","19835700":"code","3c7dd136":"code","be969442":"code","c66910b5":"code","b3196989":"code","47c8a280":"code","70158ca3":"code","d9c522d1":"code","24a39637":"code","56ef8cc5":"code","1e381c59":"code","d20fd1bc":"markdown"},"source":{"390c9239":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('seaborn')\nsns.set(font_scale=2.5)\n\nimport missingno as msno\nfrom sklearn.preprocessing import LabelEncoder\n\n#ignore warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inlinethe current session","a3ffac6d":"df = pd.read_csv('..\/input\/bank-marketing\/bank-additional-full.csv', sep=';')\ndf.head()","ec447c71":"df.columns","31d0745c":"for col in df.columns : \n    msg = 'columnn : {:>10}\\t count of NaN value : {:.0f}'.format(col, 100 * (df[col].isnull().sum() ))\n    print(msg)","b8918801":"df.info()","f00bbb83":"df.shape","1f33e4fc":"int_column = df.dtypes[df.dtypes =='int64'].index |  df.dtypes[df.dtypes =='float64'].index","e6906171":"for col in int_column : \n    plt.figure(figsize=(12,4))\n    \n    plt.subplot(1,2,1)\n    sns.distplot(df[col])\n    plt.xlabel(col)\n    plt.ylabel('Density')\n    \n    plt.subplot(1,2,2)\n    sns.boxplot(x='y', y = col, data =df, showmeans = True)\n    plt.xlabel('Target')\n    plt.ylabel(col)\n    \n    plt.show()","1fad526c":"obj_column = df.dtypes[df.dtypes == 'object'].index\ndf[obj_column[2]].unique()","619fc366":"for i in range(0, len(obj_column)) :\n    print(obj_column[i])\n    print(df[obj_column[i]].unique())\n    print()","465ffca4":"for i in range(0, len(obj_column)) :\n    fig, ax = plt.subplots(figsize=(15,4))\n\n    sns.countplot(x = obj_column[i], data = df)\n    sns.set(font_scale=1)\n\n    ax.set_title('{} Count Distribution'.format(obj_column[i]))","0ff3a204":"labelencoder_X = LabelEncoder() ","2204d231":"df[\"job\"] = labelencoder_X.fit_transform(df[\"job\"])\ndf[\"marital\"] = labelencoder_X.fit_transform(df[\"marital\"])\ndf[\"education\"] = labelencoder_X.fit_transform(df[\"education\"])\ndf[\"default\"] = labelencoder_X.fit_transform(df[\"default\"])\ndf[\"housing\"] = labelencoder_X.fit_transform(df[\"housing\"])\ndf[\"loan\"] = labelencoder_X.fit_transform(df[\"loan\"])\ndf[\"contact\"] = labelencoder_X.fit_transform(df[\"contact\"])\ndf[\"month\"] = labelencoder_X.fit_transform(df[\"month\"])\ndf[\"day_of_week\"] = labelencoder_X.fit_transform(df[\"day_of_week\"])\ndf[\"poutcome\"] = labelencoder_X.fit_transform(df[\"poutcome\"])\ndf[\"y\"] = labelencoder_X.fit_transform(df[\"y\"])","4fed2a91":"pd.set_option('max_columns', None)\ndf.tail()","e9d9b993":"df.shape","8da76f93":"heatmap_data = df\n\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(20,20))\nplt.title('Pearson Correlation of Features', y = 1.05, size=15)\nsns.heatmap(heatmap_data.astype(float).corr(), linewidths=0.1, vmax=1.0, square=True, \n            cmap=colormap, linecolor='white', annot=True, annot_kws={'size':16})\n\ndel heatmap_data","c4fb4290":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import confusion_matrix\nfrom xgboost import XGBClassifier","8e1ab7e4":"X_train, X_test, y_train, y_test = train_test_split(df.drop('y',axis=1),\n                                                    df['y'],\n                                                    test_size=.3, random_state = 42,\n                                                    stratify= df['y'])","00219c27":"scaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nX_train = pd.DataFrame(X_train, columns=df.drop('y',axis=1).columns)\nX_test = pd.DataFrame(X_test, columns=df.drop('y',axis=1).columns)","279d6903":"models = [LogisticRegression(),\n          DecisionTreeClassifier(),\n          RandomForestClassifier(),\n          XGBClassifier()]\n\nnames = [ 'LogisticRegression',\n         'DecisionTreeClassifier',\n          'RandomForestClassifier',\n          'XGBClassifier']\n\nfor model,name in zip(models,names):\n    m = model.fit(X_train,y_train)\n    print(name, 'report:')\n    print('Train score',model.score(X_train,y_train))\n    print('Test score',model.score(X_test,y_test))\n    print()\n    print(\"Train confusion matrix:\\n\",confusion_matrix(y_train, model.predict(X_train)),'\\n')\n    print(\"Test confusion matrix:\\n\",confusion_matrix(y_test, model.predict(X_test)))\n    print('*'*50)","119e649d":"model = DecisionTreeClassifier(max_depth=3)\nmodel.fit(X_train, y_train)\n\nfrom sklearn.tree import plot_tree\nplt.figure(figsize=(20,15))\nplot_tree(model,\n          feature_names= df.drop('y', axis=1).columns,  \n          class_names= ['yes','no'],\n          filled=True)\nplt.show()\n","19835700":"from sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import accuracy_score","3c7dd136":"m = RandomForestClassifier().fit(X_train,y_train)\npred_y = m.predict(X_test)\nprint('*'*50)\nprint('Report')\nprint('model : RandomForestClassifier')\nprint('Train score',model.score(X_train,y_train))\nprint('Test score',model.score(X_test,y_test))\nprint()\nprint(\"accuracy: %.2f\" %accuracy_score(y_test, pred_y))\nprint(\"Precision : %.3f\" % precision_score(y_test, pred_y))\nprint(\"Recall : %.3f\" % recall_score(y_test, pred_y))\nprint(\"F1 : %.3f\" % f1_score(y_test, pred_y))\nprint()\nprint(\"Train confusion matrix:\\n\",confusion_matrix(y_train, model.predict(X_train)),'\\n')\nprint(\"Test confusion matrix:\\n\",confusion_matrix(y_test, model.predict(X_test)))\nprint('*'*50)","be969442":"from sklearn.linear_model import LogisticRegression\n\nlr_clf = LogisticRegression(solver='liblinear')\nlr_clf.fit(X_train, y_train)\n\nprint('Train score',model.score(X_train,y_train))\nprint('Test score',model.score(X_test,y_test))\n\nlrCoef = LogisticRegression().fit(X_train,y_train).coef_\nprint(lrCoef)","c66910b5":"print(\"Coefficient of Logistic Regression\")\nfor i in range(0, len(lrCoef[0])) :\n    print('{} : {}'.format(X_train.columns[i], lrCoef[0][i]))","b3196989":"coefdf = pd.DataFrame(data=X_train.columns, index=range(0, len(lrCoef[0])), columns=['Feature'])\ncoefdf['Coef'] = lrCoef[0]\ncoefdf['Absuolute num of Coef'] = abs(lrCoef[0])\ncoefdf = coefdf.sort_values(by='Absuolute num of Coef', ascending=False).reset_index(drop=True)\ncoefdf","47c8a280":"bcd = {'age', 'job', 'marital', 'education', 'default', 'housing', 'loan'}\nlc = {'contact', 'month', 'day_of_week', 'duration'}\noth = {'campaign', 'pdays', 'previous', 'poutcome'}\nsec = {'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed'}\n\ncoefdf['Category'] = 0\nfor i in range(0,coefdf.shape[0]) : \n    if coefdf['Feature'][i] in bcd : \n        coefdf['Category'][i] = 'Bank Client'\n    elif coefdf['Feature'][i] in lc : \n        coefdf['Category'][i] = 'Last Contact'\n    elif coefdf['Feature'][i] in oth : \n        coefdf['Category'][i] = 'Other'\n    else : \n        coefdf['Category'][i] = 'Social Economic'    \ncoefdf.sort_values(by='Absuolute num of Coef', ascending=False)","70158ca3":"fig, ax = plt.subplots(figsize=(10,5))\nsns.barplot(data=coefdf, y=coefdf['Feature'], x=coefdf['Coef'])\n\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\n\nplt.title('Coefficient of Logistic Regression\\n(score : 91%)', fontsize=20)\nplt.xlabel('Coefficient')\n\nplt.savefig('Coefficient of Logistic Regression.png')\nplt.show()","d9c522d1":"import shap","24a39637":"RFmodel = RandomForestClassifier().fit(X_train, y_train)\n\nexplainer = shap.TreeExplainer(RFmodel)\nshap_values = explainer.shap_values(X_test)\n\n# fig, ax = plt.subplots(figsize=(10,5))\nshap.summary_plot(shap_values, X_test, plot_size=(10,5), show=False)\nplt.title('SHAP of Random Forest Classifier\\n(score : 92%)')\nplt.show()","56ef8cc5":"# from sklearn.inspection import permutation_importance\n\n# result = permutation_importance(RFmodel, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2)\n# sorted_idx = result.importances_mean.argsort()\n\n# plt.figure(figsize=(10,5))\n# plt.title('Permutation Importance of Random Forest Classifier\\n(score : 92%)')\n\n# plt.xticks(fontsize=12)\n# plt.yticks(fontsize=12)\n\n# plt.boxplot(result.importances[sorted_idx].T,\n#             vert=False, labels=X.columns[sorted_idx]);","1e381c59":"XGBmodel = XGBClassifier().fit(X_train, y_train)\n\nexplainer = shap.TreeExplainer(XGBmodel)\nshap_values = explainer.shap_values(X_test)\n\nshap.summary_plot(shap_values, X_test, plot_size=(10,5), show=False)\nplt.title('SHAP of XGBoost Classifier\\n(score : 88%)')\nplt.figsize=(10,5)\nplt.show()","d20fd1bc":"**Duration** is the most important feature which should be discarded because it is not known before a call is performed. Still, it indicates that making a call longer could help a lot to increase the subscription. \n\nThe higher cons.price.idx and euribor3m, the lower emp.var.rate and nr.employed, which are all social and economic context attributes, the more likely to subscribe the term deposit.\nThe social and economic context attributes were the most helpful features to predict. \n\nAmong Bank Clients Data, default was a significant factor but not that much as Social Economic was. Other Bank Clients such as Job, Martial, Education were not an influential factor.   \n\nWhen it comes to attributes which are related with the last contact of the current campaign, coefficient of Contact and Month were higher than any other Bank Clients Data attributes.\n\nIn conclusion, subscription mostly depends on social and economic situation. Thus, to make more cost efficient to increase subcribition to a term deposit, it is needed to concentrate marketing budget on certain time when euribor rate, comsumer price rate are high, employment variation rate is low. "}}