{"cell_type":{"d9cc8dd3":"code","79acc18e":"code","e05439cf":"code","f9425e65":"code","e8c39f34":"code","ee613770":"code","5bf0f6f5":"code","27927f9f":"code","e7b67e4b":"code","2fc767d7":"code","599aea82":"code","733d67e6":"code","c2aab3a1":"code","186a7f2b":"code","e1b3bdf0":"code","fe576f2b":"code","b8056643":"code","37db3fed":"code","88b8853a":"code","29196874":"code","abaad2cf":"code","378c3086":"code","60b1e9ae":"code","6aef0eac":"code","5b080c1d":"code","471f13ad":"code","e6ae958a":"code","710f35ae":"code","d67ca6ae":"code","469e1ace":"markdown","11e4e3cc":"markdown","443958e3":"markdown","7408627d":"markdown","3b632f5c":"markdown","dd6d80f0":"markdown","41249317":"markdown","2f3673e4":"markdown","ed8757b3":"markdown","4bf14528":"markdown","762305dc":"markdown","127458b0":"markdown","b81ebae6":"markdown","64f80268":"markdown","89e8df0e":"markdown","b4e78347":"markdown"},"source":{"d9cc8dd3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport missingno as msno\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"whitegrid\")\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import GradientBoostingClassifier,AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV,RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score,plot_roc_curve\nimport warnings\nwarnings.filterwarnings('ignore')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","79acc18e":"df = pd.read_csv('..\/input\/water-potability\/water_potability.csv')\ndf.shape","e05439cf":"df.head()","f9425e65":"df.info()","e8c39f34":"df.describe()","ee613770":"df.isnull().sum()","5bf0f6f5":"plt.figure(figsize=(10,5))\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False)","27927f9f":"ndf = df\nndf['ph']=df['ph'].fillna(df['ph'].mean())\nndf['Sulfate']=df['Sulfate'].fillna(df['Sulfate'].mean())\nndf['Trihalomethanes']=df['Trihalomethanes'].fillna(df['Trihalomethanes'].mean())","e7b67e4b":"ndf.isnull().sum()","2fc767d7":"sns.countplot(x=\"Potability\", data=ndf,palette=\"Set3\")","599aea82":"plt.figure(figsize=(16,12))\ncdf = ndf.drop('Potability',axis=1)\nfor i, column in enumerate(cdf.columns, 1):\n    plt.subplot(3,3,i)\n    sns.distplot(df[column])","733d67e6":"sns.displot(ndf, x=\"ph\", hue=\"Potability\", kind=\"kde\", multiple=\"stack\")","c2aab3a1":"sns.displot(ndf, x=\"Hardness\", hue=\"Potability\", kind=\"kde\", multiple=\"stack\")","186a7f2b":"sns.displot(ndf, x=\"Solids\", hue=\"Potability\", kind=\"kde\", multiple=\"stack\")","e1b3bdf0":"sns.displot(ndf, x=\"Chloramines\", hue=\"Potability\", kind=\"kde\", multiple=\"stack\")","fe576f2b":"sns.displot(ndf, x=\"Sulfate\", hue=\"Potability\", kind=\"kde\", multiple=\"stack\")","b8056643":"sns.displot(ndf, x=\"Conductivity\", hue=\"Potability\", kind=\"kde\", multiple=\"stack\")","37db3fed":"plt.figure(figsize=(10,8), dpi= 80)\nsns.pairplot(ndf, hue=\"Potability\",diag_kind=\"hist\")\nplt.show()","88b8853a":"sns.jointplot(data=ndf, x=\"Hardness\", y=\"ph\",hue='Potability')","29196874":"sns.displot(ndf, x=\"ph\", y=\"Hardness\", hue=\"Potability\", kind=\"kde\")","abaad2cf":"X = ndf.drop('Potability',axis=1)\ny=ndf['Potability'].copy()","378c3086":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size = 0.2,random_state=42,shuffle = True)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","60b1e9ae":"models ={'RandomForestClassifier':RandomForestClassifier(),'GradientBoostingClassifier':GradientBoostingClassifier(),\n        'AdaBoostClassifier':AdaBoostClassifier(),'LGBMClassifier':LGBMClassifier(),\n         \n        }","6aef0eac":"training_scores= []\ntesting_scores=[]\n\nfor key, value in models.items():\n    value.fit(X_train_scaled, y_train)\n    train_score= value.score(X_train_scaled,  y_train)\n    test_score= value.score(X_test_scaled, y_test)\n    training_scores.append(train_score)\n    testing_scores.append(test_score)\n    \n    print(f\"{key}\\n\")\n    print(\"Training Accuracy: {0:.3f}\".format(train_score*100))\n    print(\"Training Accuracy: {0:.3f} \\n\".format(test_score*100))","5b080c1d":"model = RandomForestClassifier()\nn_estimators = [10, 100, 500]\nmax_features = ['sqrt', 'log2']\n\n# Grid search\ngrid = dict(n_estimators=n_estimators,max_features=max_features)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train_scaled, y_train)\n\n#Results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","471f13ad":"rclf = RandomForestClassifier(max_features='log2',n_estimators= 500)\nrclf.fit(X_train_scaled,y_train)\ny_pred = rclf.predict(X_test_scaled)\nprint(\"Training Accuracy: {0:.3f}\".format(rclf.score(X_train_scaled, y_train)*100))\nprint(\"Testing Accuracy: {0:.3f}\".format(accuracy_score(y_test,y_pred)*100))\n","e6ae958a":"model = LGBMClassifier(boosting_type='dart')\nmax_bin=[255,300,350,450]\nlr=[0.01,0.001,0.0001,0.11]\nnum_leaves=[31,100,250]\n\ngrid = dict(max_bin=max_bin,learning_rate =lr,num_leaves=num_leaves)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train_scaled, y_train)\n\n#Results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","710f35ae":"lgb = LGBMClassifier(learning_rate= 0.11, max_bin= 255, num_leaves= 31,boosting_type='dart')\nlgb.fit(X_train_scaled,y_train)\ny_pred = lgb.predict(X_test_scaled)\nprint(\"Training Accuracy: {0:.3f}\".format(lgb.score(X_train_scaled, y_train)*100))\nprint(\"Testing Accuracy: {0:.3f}\".format(accuracy_score(y_test,y_pred)*100))","d67ca6ae":"models = [lgb, rclf]\nax = plt.gca()\nfor i in models:\n    plot_roc_curve(i, X_test_scaled, y_test, ax=ax)\n    \n","469e1ace":"Replacing Null values with mean","11e4e3cc":"## **Data Preprocessing**","443958e3":"Last five rows of dataset","7408627d":"#### Hyperparameter Tuning of **Random Forest Classifier** ","3b632f5c":"####  **Visualization**","dd6d80f0":"First Five rows of dataset","41249317":"Distribution of each feature column over taget variable","2f3673e4":"Pplot of two variables with bivariate and univariate graphsairwise relationships","ed8757b3":"**RandomForestClassifier** and **LGBMClassifier** performing well.","4bf14528":"Spliting Feature and Target variable","762305dc":"Count of Target variable","127458b0":"Distribution of feature variable","b81ebae6":"**Null values in dataset**","64f80268":"**Data Splitting and Scaling**\n\nsplit percentages include: **Train: 80%, Test: 20%**\n","89e8df0e":"## **Modeling**","b4e78347":"ph and Hardness with distribution over Potability"}}