{"cell_type":{"072296fb":"code","fe7382d7":"code","e2deb0a7":"code","fc4b87e3":"code","18494303":"code","e6d470db":"code","4b1a6d45":"code","ce38e170":"code","dd84f137":"code","1c7a895b":"code","c5adb984":"code","7ddc4ddd":"code","09e6b95c":"code","221a5ff1":"code","fdc97062":"code","cc48a1ca":"code","45b6fcad":"code","eb2afe85":"code","c987c838":"code","b9409241":"code","9d120597":"code","a8d02e1c":"code","5687ca7f":"code","4d8f7289":"code","75668591":"code","b11d6e6d":"code","db955b0a":"code","b7d41224":"code","dcbe56df":"code","0c49314f":"code","88bbe753":"code","f551640f":"code","67b8a3a6":"code","d1302548":"code","e990fea8":"code","b05eeab0":"code","f91a9479":"markdown","ec7b5e38":"markdown","1555defa":"markdown","4b3f54b1":"markdown","bfdc96ce":"markdown","9635306e":"markdown","2d422465":"markdown","1345777f":"markdown","640f7a51":"markdown","7e589642":"markdown","1ba3c507":"markdown","609a086a":"markdown","69f008d5":"markdown","f34be934":"markdown","ecd7c009":"markdown","c9ea18ec":"markdown","b1376752":"markdown","a45c51dd":"markdown","263d28e1":"markdown","2e8f1b08":"markdown","dd11a033":"markdown","69178902":"markdown","b4567fd3":"markdown","303ec50a":"markdown","aefe7e67":"markdown","7dddf9b8":"markdown","ffc31cff":"markdown","ac180889":"markdown","2ae6b9a4":"markdown","c82e7e38":"markdown","452e1411":"markdown","d6450e21":"markdown","eaae75c2":"markdown"},"source":{"072296fb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fe7382d7":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import GridSearchCV,train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline as pl2\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import f1_score,classification_report,roc_auc_score,precision_score,recall_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Input,BatchNormalization\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import Adam\nfrom keras.wrappers.scikit_learn import KerasClassifier","e2deb0a7":"df = pd.read_csv(\"\/kaggle\/input\/paysim1\/PS_20174392719_1491204439457_log.csv\")\ndf.head()","fc4b87e3":"df.dtypes","18494303":"df.shape","e6d470db":"df.isnull().sum()","4b1a6d45":"df.describe()","ce38e170":"#Total number of unique customers\nprint(f\"Total number of unique customers are {df.nameOrig.nunique()}\")\nprint(f\"Total number of unique customers are {df.nameDest.nunique()}\")\nprint(f\"Average no. of transactions per customer are {df.shape[0]\/df.nameOrig.nunique()}\")\nprint(f\"Average no. of transactions per recipient are {df.shape[0]\/df.nameDest.nunique()}\")","dd84f137":"print(df.isFraud.value_counts())\nsns.countplot(y=\"isFraud\",data=df)\nplt.show()","1c7a895b":"fig,axs = plt.subplots(2,2,figsize=(12,6))\naxs[0][0].title.set_text('Histogram of transaction amount')\naxs[0][0].hist(df[\"amount\"])\naxs[0][1].title.set_text(\"Histogram of opening customer balance\")\naxs[0][1].hist(df[\"oldbalanceOrg\"])\naxs[1][0].title.set_text(\"Histogram of closing customer balance\")\naxs[1][0].hist(df[\"newbalanceOrig\"])\naxs[1][1].title.set_text(\"Histogram of clsoing recipient balance\")\naxs[1][1].hist(df[\"newbalanceDest\"])\nplt.show()","c5adb984":"sns.barplot(x=df.type.unique(),y=df.groupby(\"type\")[\"isFraud\"].sum())","7ddc4ddd":"print(f\"% of transactions where difference between opening and closing balance of customer is not equal to transaction amount is {(1-len(df[np.abs(df.oldbalanceOrg-df.newbalanceOrig) == (df.amount)])\/len(df))*100}\")\nprint(f\"% of transactions where difference between opening and closing balance of recipient is not equal to transaction amount is {(1-len(df[np.abs(df.oldbalanceDest-df.newbalanceDest) == (df.amount)])\/len(df))*100}\")\nprint(f\"% of transactions where opening and closing balance of customer is equal to 0 but transaction amount is not equal to 0 is {(1-len(df[(df.oldbalanceOrg==0)&(df.newbalanceOrig==0)&(df.amount!=0)])\/len(df))*100}\")\nprint(f\"% of transactions where opening and closing balance of recipient is equal to 0 but transaction amount is not equal to 0 is {(1-len(df[(df.oldbalanceDest==0)&(df.newbalanceDest==0)&(df.amount!=0)])\/len(df))*100}\")\nprint(\"% of transfer transactions where the opening and closing balance of the customer remained the same is \" + str((len(df[(df.type==\"CASH_OUT\")&(df.oldbalanceOrg==df.newbalanceOrig)&(df.amount != 0)])\/len(df))*100))","09e6b95c":"sns.lineplot(x=list(range(1,744)),y=df.groupby(\"step\")[\"isFraud\"].sum())\nplt.xlabel(\"Hour of the month\")\nplt.ylabel(\"Number of transactions per hour\")\nplt.show()","221a5ff1":"df[\"hour\"] = df.step % 24\nfrauds_hour = pd.concat([df.groupby(\"hour\")[\"isFraud\"].sum(),df.groupby(\"hour\")[\"isFraud\"].count()],axis=1)\nfrauds_hour.columns = [\"Frauds\",\"Transactions\"]\nfrauds_hour[\"fraud_rate\"] = frauds_hour.Frauds\/frauds_hour.Transactions\nsns.barplot(x=frauds_hour.index,y=frauds_hour.fraud_rate)\nplt.show()","fdc97062":"fig,ax=plt.subplots(2,2,figsize=(14,6))\nax[0][0].title.set_text(\"Transaction amount - CASH OUT transactions\")\nax[0][0].bar(x=[\"Non-Fraud\",\"Fraud\"],height=df[df.type==\"CASH_OUT\"].groupby(\"isFraud\")[\"amount\"].median())\nax[0][1].title.set_text(\"Opening Customer balance - CASH OUT transactions\")\nax[0][1].bar(x=[\"Non-Fraud\",\"Fraud\"],height=df[df.type==\"CASH_OUT\"].groupby(\"isFraud\")[\"oldbalanceOrg\"].median())\nax[1][0].title.set_text(\"Transaction amount - TRANSFER transactions\")\nax[1][0].bar(x=[\"Non-Fraud\",\"Fraud\"],height=df[df.type==\"TRANSFER\"].groupby(\"isFraud\")[\"amount\"].median())\nax[1][1].title.set_text(\"Opening Customer balance - TRANSFER transactions\")\nax[1][1].bar(x=[\"Non-Fraud\",\"Fraud\"],height=df[df.type==\"TRANSFER\"].groupby(\"isFraud\")[\"oldbalanceOrg\"].median())\nfig.show()","cc48a1ca":"fig,ax=plt.subplots(1,2,figsize=(14,6))\nax[0].title.set_text(\"Opening Recipient Balance - CASH OUT transactions\")\nax[0].bar(x=[\"Non-Fraud\",\"Fraud\"],height=df[df.type==\"CASH_OUT\"].groupby(\"isFraud\")[\"oldbalanceDest\"].median())\nax[1].title.set_text(\"Opening Recipient Balance - TRANSFER transactions\")\nax[1].bar(x=[\"Non-Fraud\",\"Fraud\"],height=df[df.type==\"TRANSFER\"].groupby(\"isFraud\")[\"oldbalanceDest\"].median())\nfig.show()","45b6fcad":"fig,ax = plt.subplots(1,3,figsize=(18,6))\nax[0].title.set_text(\"Distribution of transaction amount pre transformations\")\nax[1].title.set_text(\"Distribution of transaction amount post log transformation\")\nax[2].title.set_text(\"Distribution of transaction amount post capping\")\nsns.boxplot(x=df.isFraud, y=df.amount,ax=ax[0])\nsns.boxplot(x=df.isFraud,y=np.log1p(df.amount),ax=ax[1])\ndf1 = df.copy()\ndf1[df1.amount > df1.amount.quantile(0.75)*3][\"amount\"] = df1.amount.quantile(0.75)*3\nsns.boxplot(x=df1.isFraud,y=(df1.amount),ax=ax[2])\nplt.show()","eb2afe85":"df[\"day\"] = round(df.step\/24)\nsns.barplot(x=list(range(1,33)),y=df[df.isFraud==1].groupby(\"day\")[\"amount\"].sum())\nplt.xlabel(\"Day\")\nplt.ylabel(\"Total Fraud transaction amount\")\nplt.show()","c987c838":"df[\"dayweek\"] = df.day % 7\nsns.barplot(x=list(range(1,8)),y=df[df.isFraud==1].groupby(\"dayweek\")[\"amount\"].mean())\nplt.xlabel(\"Day of the week\")\nplt.ylabel(\"Average Fraud transaction amount\")\nplt.show()","b9409241":"df1 = df.copy()","9d120597":"df1[\"hourday\"] = df1.step % 24\ndf1[\"hourweek\"] = df1.step % (24*7)\ndf1[\"day\"] = round(df1.step\/24)\ndf1[\"dayweek\"] = df1.day % 7\ndf1[\"daymonth\"] = df1.day % 30","a8d02e1c":"df1 = pd.get_dummies(df1,columns=[\"type\"])\ndf1.head()","5687ca7f":"df1[\"logamount\"] = np.log1p(df1[\"amount\"])\ndf1[\"logoldbalanceOrg\"] = np.log1p(df1[\"oldbalanceOrg\"])\ndf1[\"lognewbalanceOrig\"] = np.log1p(df1[\"newbalanceOrig\"])\ndf1[\"logoldbalanceDest\"] = np.log1p(df1[\"oldbalanceDest\"])\ndf1[\"lognewbalanceDest\"] = np.log1p(df1[\"newbalanceDest\"])\ndf1[\"custdiff\"] = df1[\"oldbalanceOrg\"] - df1[\"newbalanceOrig\"]\ndf1[\"destdiff\"] = df1[\"oldbalanceDest\"] - df1[\"newbalanceDest\"]\ndf1[\"custind\"] = np.where(df1[\"oldbalanceOrg\"] - df1[\"newbalanceOrig\"] == df1.amount,1,0)\ndf1[\"destind\"] = np.where(df1[\"oldbalanceDest\"] - df1[\"newbalanceDest\"] == df1.amount,1,0)\ndf1[\"custrto\"] = df1.oldbalanceOrg\/(df1.newbalanceOrig+1)\ndf1[\"destrto\"] = df1.oldbalanceDest\/(df1.newbalanceDest+1)\ndf1[\"custdestrto1\"] = df1.oldbalanceOrg\/(df1.oldbalanceDest+1)\ndf1[\"custdestrto2\"] = df1.newbalanceOrig\/(df1.newbalanceDest+1)\ndf1[\"custamountrto\"] = df1.oldbalanceOrg\/(df1.amount+1)\ndf1[\"destamountrto\"] = df1.oldbalanceDest\/(df1.amount+1)","4d8f7289":"df1 = df1.drop([\"isFlaggedFraud\",\"nameOrig\",\"nameDest\"],axis=1)","75668591":"X1,X2,y1,y2 = train_test_split(df1.drop(\"isFraud\",axis=1),df1[\"isFraud\"],test_size=0.75,random_state=1234,stratify = df1[\"isFraud\"])\nX_train,X_test,y_train,y_test = train_test_split(X1,y1,test_size=0.75,random_state=1234,stratify = y1)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","b11d6e6d":"from sklearn.tree import DecisionTreeClassifier,plot_tree\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline as pl2\n\ndt = pl2([\n    ('sampler',RandomUnderSampler(random_state=1234,sampling_strategy='majority')),\n    ('clf',DecisionTreeClassifier(max_depth=6))\n    ])\n\ndt.fit(X_train,y_train)","db955b0a":"plt.subplots(figsize=(18,10))\nplot_tree(dt['clf'],feature_names = X_train.columns)","b7d41224":"logreg = pl2([\n    ('stdize',StandardScaler()),\n    ('sampler',RandomUnderSampler(random_state=1234,sampling_strategy='majority')),\n    ('clf',LogisticRegression(max_iter=1000000))\n    ])\nlogreg.fit(X_train,y_train)","dcbe56df":"pd.DataFrame({\"Variable\":list(X_train.columns),\"Coefficient\":logreg['clf'].coef_[0]})","0c49314f":"pl = pl2([\n    ('sampler',RandomUnderSampler(random_state=1234,sampling_strategy='majority')),\n    ('clf',GradientBoostingClassifier(max_features='sqrt',subsample=0.7))\n    ])\nparameters = {'clf__learning_rate':[0.07,0.1],\n              'clf__n_estimators':[300,500],\n              'clf__max_depth':[5]\n    }\n\ncv = GridSearchCV(pl,parameters,scoring=\"roc_auc\",verbose=True,n_jobs=6)\ncv.fit(X_train,y_train)\nprint(cv.best_params_)\nprint(cv.best_score_)","88bbe753":"imp = pd.DataFrame({\"Variable\":list(X_train.columns),\"Importance\":cv.best_estimator_['clf'].feature_importances_})\nimp = imp.sort_values(\"Importance\",ascending=False)\nsns.barplot(x=imp.Importance.head(10),y=imp.Variable.head(10))","f551640f":"from sklearn.metrics import f1_score,classification_report,roc_auc_score\nprint(f\"F1 score of decision tree classifier is {f1_score(dt.predict(X_test),y_test)}\")\nprint(f\"F1 score of GBM classifier is {f1_score(cv.best_estimator_.predict(X_test),y_test)}\")\nprint(f\"F1 score of logistic regression classifier is {f1_score(logreg.predict(X_test),y_test)}\")\nprint(f\"AUC of decision tree classifier is {roc_auc_score(dt.predict(X_test),y_test)}\")\nprint(f\"AUC of GBM classifier is {roc_auc_score(cv.best_estimator_.predict(X_test),y_test)}\")\nprint(f\"AUC of logistic regression classifier is {roc_auc_score(logreg.predict(X_test),y_test)}\")","67b8a3a6":"sns.barplot(x=[\"Decision Tree\",\"Logistic Regression\",\"GBM\"],\n            y=[roc_auc_score(dt.predict(X_test),y_test),roc_auc_score(logreg.predict(X_test),y_test),\n               roc_auc_score(cv.best_estimator_.predict(X_test),y_test)])\nplt.ylabel(\"AUC\")\nplt.show()","d1302548":"dt_probs = pd.DataFrame({\"Actual\":y_test,\"Predicted\":cv.best_estimator_.predict_proba(X_test)[:,1],\"Amount\":X_test.amount})\ndt_probs[\"Predicted_95\"] = np.where(dt_probs.Predicted > 0.95,1,0)\ndt_probs[\"Predicted_50\"] = np.where(dt_probs.Predicted > 0.5,1,0)\n#cut - probability greater than 0.5\nprint(\"Precision for threshold = 0.5 is \"+ str(precision_score(dt_probs[\"Predicted_50\"],y_test)))\nprint(\"Recall for threshold = 0.5 is \"+ str(recall_score(dt_probs[\"Predicted_50\"],y_test)))\n#cut - probability greater than 0.95\nprint(\"Precision for threshold = 0.95 is \"+ str(precision_score(dt_probs[\"Predicted_95\"],y_test)))\nprint(\"Recall for threshold = 0.95 is \"+ str(recall_score(dt_probs[\"Predicted_95\"],y_test)))","e990fea8":"#Frauds captured by \"Amount > 200,000\" strategy\nprint(\"% Frauds captured by 'Amount > 200,000' strategy in number and amount\")\nprint(len(dt_probs[(dt_probs.Amount > 200000) & (dt_probs.Actual==1)])\/len(dt_probs[dt_probs.Actual==1]))\nprint(sum(dt_probs[(dt_probs.Amount > 200000) & (dt_probs.Actual==1)][\"Amount\"])\/sum(dt_probs[dt_probs.Actual==1][\"Amount\"]))\nprint(\"% Frauds captured by GBM model in number and amount\")\nprint(len(dt_probs[(dt_probs.Predicted_50 == 1) & (dt_probs.Actual==1)])\/len(dt_probs[dt_probs.Actual==1]))\nprint(sum(dt_probs[(dt_probs.Predicted_50 == 1) & (dt_probs.Actual==1)][\"Amount\"])\/sum(dt_probs[dt_probs.Actual==1][\"Amount\"]))","b05eeab0":"print(\"The fraud losses saved due to using GBM model is\")\nprint(sum(dt_probs[(dt_probs.Predicted_50 == 1) & (dt_probs.Actual==1)][\"Amount\"]) - sum(dt_probs[(dt_probs.Amount > 200000) & (dt_probs.Actual==1)][\"Amount\"]))\nprint(\"Incremental beneift in number of frauds captured\")\nprint(len(dt_probs[(dt_probs.Predicted_50 == 1) & (dt_probs.Actual==1)][\"Amount\"]) - len(dt_probs[(dt_probs.Amount > 200000) & (dt_probs.Actual==1)][\"Amount\"]))","f91a9479":"We can also calculate the extra frauds captured and hence money saved compared to the default strategy - amount greater than 200,000 (mentioned in the data problem statement).","ec7b5e38":"## Data Cleaning\n\nBefore model development, we need to clean the data i.e. deal with missing values and outliers. Since the missing values are 0 in all the columns, there is no need to handle them.\n\nFor outlier treatment, capping will not be done on the quantitative variables as we can lose some information because frauds happen at high transactions. Log transformed variables and the original variables both will also be used in the model and the algorithm decides which variable is better.\n\nOther issues with the data (variables not talking with each other) can also be dealt with if more information is present. The less event rate is handled during model development using sampling techniques.","1555defa":"Importing necessary packages to visualiza data and build models","4b3f54b1":"## Feature Generation\n\nThere are 5 numeric features, 1 categorical feature and 1 datetime feature that we can use in the model. In this section, the three different feature types would be explored.","bfdc96ce":"Anyone with a bank account currently living must have encountered the words 'Transaction Fraud'. It might be their personal experience or a third-party experience heard from someone else or from agents selling insurance. As the world is becoming more and more digital, the transaction frauds are also increasing at a rapid pace. In a study, it is found out that 24 bilion USD is lost due to credit card fraud transactions in one year.Banking and financial institutions are facing severe challenges due to fraudulent transactions. \n\nTo tackle these frauds, there is a need for banks to put strategies in place so that customers' money is not lost. The most common strategy used by banks is put a threshold on transaction amount and any transaction above that amount requires multiple-authentications from customer. This method works well in most of the cases but it is a crude way to make a fraud strategy as it spoils the customer experience. Also having a dumb strategy like this would not work because of the fraudsters' scary ability of adapting to different environments. An evolving and intelligent fraud policy is the best solution to this problem. Machine learning is the perfect answer and algorithms like Boosting and Regression can be used to classify frauds.\n\nPost building a model, we can use this to predict whether a fraud can happen or not and only authenticate a transaction if the model says so. There is another problem that many banks encounter while model implementation that is to run the model on real-time basis and give results instantly. Unlike models to optimize marketing campaigns or determine customer's risk score which can be done in batch, transaction models need to be run on real-time. Hence there is lot of emphasis on big data cloud platforms like AWS, Azure and Hadoop that stores data and implement machine learning models on scale when considering transaction frauds.\n\nThis case study aims to answer some of the questions related to transaction frauds using visualizations. Couple of classification models would also be built at the end and the features that drive these models' performance would be explained.","9635306e":"### Categorical Features\n\nThere are three categorical features present in the data - 'type', 'nameOrig' and 'nameDest'. 'nameOrig' and 'nameDest' are ignored during this exercise because it is very memory and time-consuming to work with millions of unique categories. Only 'type' is handled using get_dummies() function of pandas. Code and output is given below. Provided there are no space and processing power issues, variables like number of cash-in\/cash-out\/etc. transactions made by a customer during last hour\/3 hours\/6 hours\/1 day can be calculated.","2d422465":"We can also drop some the ID Variables from the dataset","1345777f":"As we can see, GBM model is performing the best out of the three models in terms of F1 score and AUC. Precision and recall can also be calculated and compared depending on the business requirements. If the business values loss remedy more than anything else, recall is the metric to go to becasue we can get how many actual frauds are captured. If customer experience matters, we can check precision. Depending on what we need to maximize - precision or recall, the right cut-off for probabilities can be selected.\n\nIn the below case, for GBM model precision and recall of thresholds 0.5 and 0.95 are compared.","640f7a51":"Since we are dealing with skewed data, accuracy is not a suitable metric to compare model performances. F1 score and AUC are better metrics in this case.","7e589642":"### Datetime features","1ba3c507":"Secondly, there are very big outliers in the quantitative variables. Generally, very very big outliers are removed because there is a chance that it because of wrong recording. But in this case, wrong recording is not the case because very large frauds transactions tend to happen. So we cannot remove the outliers and there is a need of treating them before analysis or modelling.","609a086a":"### Gradient Boosting Classifier\n\nA cross validation GBM model is built with little hyperparameter tuning. AUC is used to select the best model from the different cross-validation folds. Top 10 most important variables are also plotted.","69f008d5":"The below plot shows the total fraud transaction amount on a daily basis","f34be934":"Three different algorithms are used to build a model during this case study - Decision Tree, Logistic Regression, and GBM. To deal with skewed data, the most popular methods like Undersampling, oversampling and SMOTE can be used. Undersampling removes records of the majority class randomly and oversampling duplicates the records of minority class. SMOTE is a modelling technique that generates new minority class records and the most accurate. Since SMOTE is a modelling technique that takes lot of time and oversampling increases the already large sample size, undersampling is used in this exercise. Algorithms like Random Forest and LightGBM have inbuilt weight adjusting parameters to deal with skewed data.","ecd7c009":"## Exploratory Data Analysis\n\nThrough the exploratory data analysis, we can prove some hypothesis about fraud attacks and get some visual interpretations from data.","c9ea18ec":"'Step' is the only datetime feature. Hour of the day, Day of the week, Day of the month, Week of the month variables are created in this exercise. Codes are written below. Some other variables that can be created are day to hour transactions, day to hour fraud transactions and hours since last fraud transaction.","b1376752":"## Issues with the dataset\n\nSimple univariate and bivariate analysis is done on all the variables to diagnose the data. There are primarily four issues with the data.\n\nFirst, the number of frauds are in avery minute amount in the data. From a logical standpoint, it makes sense and also it is good for the society that there are very less number of frauds happening. But from a modelling and data point of view, less number of events is a troublesome issue and there is a need of solving it before developing the model.","a45c51dd":"As mentioned in the issues sections, there is a need of outlier treatment in the quantitative variables. Two popular ways to treat the outliers are transformations and capping. For transformation, Log(1+x) is a decent one when there are zeroes present in the variable and 3xp75 is a good cut-off for capping. The box plots for one variable 'amount' are shown below pre and post transformations.","263d28e1":"We can look at the transaction amount and customer's opening balance for fraud and non-fraud cash-out and tranfer transactions seperately. Median for these variables are compared because mean is biased because of outliers. From the plots, we can see that these variables are abnormally high for fraud cash-out transactions compared to non-fraud cash-out transactions.","2e8f1b08":"Incremental benefit in both number are amount for test population is given below","dd11a033":"Another analysis that can be interesting is at which hour of a day, the fraud attacks generally happen. From the step variable, we can get the hour of day. The below plot shows the frauds at different hours of day. It tells that frauds happen during sleeping hours the most. Close to 20% of transactions that happen during 4 AM and 5 AM are fraud transactions.","69178902":"### Decision Tree Classifier\n\nA simple decision tree classifier with a depth of 6 is built. The code and decision tree is shown below.","b4567fd3":"Though there are five transaction types, frauds happened only when the transaction type is TRANSFER or CASH_IN. Frauds happen the most during Payments, Transfers, Cash-outs from customers and Cash-ins to recipients. This trend is not reflected in the data.","303ec50a":"## Proposals to improve the model\n\u2022 Run the entire process above on PySpark instead of Python which saves lot of time and memory.\n\n\u2022 Explore other outlier treatment techniques to handle the quantitative variables in a better manner.\n\n\u2022 Get more data and information to solve the problem of '2 variables not talking with each other' issue.\n\n\u2022 More variables can be created as mentioned in the feature generation section.\n\n\u2022 Sampling techniques other than Undersampling can be used and the best model can be taken.\n\n\u2022 Hyperparameter tuning is not done on any model. There is still lot of scope of improvement by tuning the models.\n\n\u2022 Selecting the best model based out of precision and recall rather than AUC and F-1 score","aefe7e67":"We can look at the recipient's opening balance for fraud and non-fraud cash-out and transfer transactions seperately. From the plots, we can see that this variable is very less for transfer fraud transactions compared to transfer non-fraud transactions which is reverse of what we observed in customer balance.","7dddf9b8":"### Logistic Regression Classifier\n\nLogistic regression model without regularization is built with all the variables inputted. Multiple iterations to remove multi-collinearity and variables with high p-values need to be done on logistic regression equation to get the best model. In this case, only iteration is done","ffc31cff":"We can also see the fraud transactions per day of the week.","ac180889":" Last but not the least, many variables are not talking with each other. For example, take a transaction from a customer \"C1900366749\" to recipient \"C997608398\". The data states that opening customer balance is 4465, closing customer balance is 0, opening recipient balance is 10845, closing recipient balance is 157982.12 and transaction amount is 9644.94. The difference between opening and closing balance of customer, difference between opening and closing balance of recipient and transaction amount must be equal which is not happening for most of the observations. (>85%) Also, the customer's account balance remained same when TRANSFER transactions happened more than 15% of the times. There are multiple reasons this can happen - Different currencies or multiple transactions at the same time or wrong recording. We don't have solution for any of these problems.","2ae6b9a4":"One popular trend is fraud attacks by big criminals happen in a short period of time. To check this, we can plot the fraud attacks with time. In our data, we have data every hour. The below graph shows the fraud attack every hour in the 744 hours. As expected there are peaks and troughs and also a very big peak. This suggests that frauds happen in short period of time.","c82e7e38":"## Model Development\n\nPrior to model development the dataset need to be divided into train and test samples. In this case, since the modelling dataset is huge, model will be built on a 25% sample only. Post sampling, ~1.1M samples are present in train dataset. Stratify option in train_test_split is used to make sure that the fraud cases are also divided randomly. Code to create train and test samples is given below.","452e1411":"## Model Performance","d6450e21":"### Quantitative Features\n\nAs seen in the issues section, difference between opening and closing balance should give the transaction amount. As fraud also can be a reason why there is such a differnce, creating such variables would be useful. Also log transformations as explained in the previous section also would be created. Ratio between different balances can also turn out to be useful. One is added to denominator to handle the missing values Interaction variables with 'type' can be created. Binned quantitative variables also sometimes give a good result in case of regression modelling.","eaae75c2":"## Introduction\nReading the transaction data into a dataframe and checking shape, data types and the summary."}}