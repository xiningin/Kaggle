{"cell_type":{"95cb493c":"code","19371e6f":"code","46078e0e":"code","fa8f3d26":"code","4cbf3511":"code","b830a825":"code","1f23c6c1":"code","aee3a081":"code","096a4f88":"code","0cb3cdd7":"code","f75f1d79":"code","c7a85e12":"code","0664900d":"code","a685b67b":"code","2c5405e3":"code","c364c859":"code","351f437d":"code","1b46d74a":"code","464dd9ee":"code","12b097bd":"code","b0ad50d0":"code","02708a13":"code","eb2ac57e":"code","61a6c00d":"code","ba637b2e":"code","5834a5b7":"code","2562658c":"code","a1589673":"code","440b30f5":"code","afe2edfd":"code","e42967a0":"code","8bfbd3ce":"markdown","d7a2b9ca":"markdown","e55eaa1e":"markdown","f32a99e0":"markdown","9b110b10":"markdown","9c7dc2a9":"markdown","a38b95b3":"markdown","60d2da52":"markdown","3c1e5b80":"markdown","0a97ab63":"markdown","9a7e4d32":"markdown","625e63e4":"markdown","afd53514":"markdown","b80e759a":"markdown","14d1398b":"markdown","8a0f985d":"markdown","9037b715":"markdown","f39da635":"markdown","ab0312ca":"markdown","fb9328bf":"markdown","381a6738":"markdown","88d39498":"markdown"},"source":{"95cb493c":"# Python \u22653.5 is required\nimport sys\nassert sys.version_info >= (3, 5)\n\n# Scikit-Learn \u22650.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\n\n# Common imports\nimport numpy as np\nimport os\n\n# To plot pretty figures\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\nimport pandas as pd\nimport seaborn as sns #for better and easier plots\n\n%matplotlib inline\n\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")","19371e6f":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","46078e0e":"train.head(3) #looking at the first 3 entries, just to have an overall picture of what we have at hand","fa8f3d26":"train_labels = train['label'].copy()\ntrain.drop('label', axis=1, inplace=True)","4cbf3511":"print(train.shape, test.shape) #checking the shape","b830a825":"train \/= 255\ntest \/= 255","1f23c6c1":"fig, ax = plt.subplots(figsize=(8,5))\nsns.countplot(train_labels)","aee3a081":"list_indx = [] #creating a list of the indexes for each digit\nfor i in range(10):\n    for nr in range(10):\n        indx = train_labels[train_labels==nr].index[i]\n        list_indx.append(indx) ","096a4f88":"fig, axs = plt.subplots(10, 10, sharex=True, sharey=True, figsize=(10,12))\naxs = axs.flatten() \nfor n, i in enumerate(list_indx): #n for each different plot and i for each different index stored in list_index\n    im = train.iloc[i]\n    im = im.values.reshape(-1,28,28,1) #reshaping it to 28x28 pixels\n    axs[n].imshow(im[0,:,:,0], cmap=plt.get_cmap('gray')) #making the background dark\n    axs[n].set_title(train_labels[i])\nplt.tight_layout()    ","0cb3cdd7":"from sklearn.model_selection import train_test_split\n\ntrain, train_val, labels_train, labels_val = train_test_split(train, train_labels, test_size=0.1, random_state=42)","f75f1d79":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\ndef report_and_confusion_matrix(label, prediction):\n    print(\"Model Report\")\n    print(classification_report(label, prediction))\n    score = accuracy_score(label, prediction)\n    print(\"Accuracy : \"+ str(score))\n    \n    ####################\n    fig, ax = plt.subplots(figsize=(8,8)) #setting the figure size and ax\n    mtx = confusion_matrix(label, prediction)\n    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5,  cbar=True, ax=ax) #create a heatmap with the values of our confusion matrix\n    plt.ylabel('true label')\n    plt.xlabel('predicted label')","c7a85e12":"from sklearn.linear_model import LogisticRegression\nmodel_lgre = LogisticRegression(random_state=0)\nparam_grid = {'C': [0.014,0.012], 'multi_class': ['multinomial'],  \n              'penalty': ['l1'],'solver': ['saga'], 'tol': [0.1] }\nGridCV_LR = GridSearchCV(model_lgre, param_grid, verbose=1, cv=5)\nGridCV_LR.fit(train,labels_train)\nscore_grid_LR = GridCV_LR.best_score_","0664900d":"print(score_grid_LR)","a685b67b":"##using the logist regression parameters found in our grid search, let's predict using our validation set and see how well the model is doing\npredict_lgr = GridCV_LR.predict(train_val)\nreport_and_confusion_matrix(labels_val, predict_lgr)","2c5405e3":"from sklearn.neighbors import KNeighborsClassifier\nknn_clf = KNeighborsClassifier(weights='distance', n_neighbors=4)\nknn_clf.fit(train, labels_train)","c364c859":"predict_knn = knn_clf.predict(train_val)\nreport_and_confusion_matrix(labels_val, predict_knn)","351f437d":"from sklearn.ensemble import RandomForestClassifier\n\nrand_forest_clf = RandomForestClassifier(random_state=42)\nparam_grid = {'max_depth': [15], 'max_features': [100],  \n              'min_samples_split': [5],'n_estimators' : [50] }\nGridCV_rd_clf = GridSearchCV(rand_forest_clf, param_grid, verbose=1, cv=5)\nGridCV_rd_clf.fit(train, labels_train)\nscore_grid_rd = GridCV_rd_clf.best_score_","1b46d74a":"print(score_grid_rd)","464dd9ee":"predict_rand_forest = GridCV_rd_clf.predict(train_val)\nreport_and_confusion_matrix(labels_val, predict_rand_forest)","12b097bd":"from sklearn.neural_network import MLPClassifier\n\nmlp_clf = MLPClassifier(activation = \"logistic\", hidden_layer_sizes=(200,), random_state=42,batch_size=2000)\nparam_grid = { 'max_iter': [600], 'alpha': [1e-4], \n               'solver': ['sgd'], 'learning_rate_init': [0.05,0.06],'tol': [1e-4] }\n    \nGridCV_MLP = GridSearchCV(mlp_clf, param_grid, verbose=1, cv=3)\nGridCV_MLP.fit(train,labels_train)\nscore_grid_MLP = GridCV_MLP.best_score_","b0ad50d0":"print(score_grid_MLP)","02708a13":"predict_MLP = GridCV_MLP.predict(train_val)\nreport_and_confusion_matrix(labels_val, predict_MLP)","eb2ac57e":"predict_test = GridCV_MLP.predict(test)","61a6c00d":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['Label'] = predict_test.astype('int64')\nsub.to_csv(\"mlp.csv\", index=False)","ba637b2e":"## loading relevant models for this part of the notebook\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam, SGD\nfrom keras.layers import Dense, Activation, Dropout, Input, concatenate\nfrom keras.utils.np_utils import to_categorical\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.callbacks import EarlyStopping","5834a5b7":"def build_model(input_shape, n_hidden=1, n_neurons=30,optimizer = SGD(3e-3)):\n    model = Sequential()\n    options = {\"input_shape\": input_shape}\n    for layer in range(n_hidden):\n        model.add(Dense(n_neurons, activation=\"elu\", kernel_initializer=\"he_normal\", **options))\n        model.add(Dropout(0.25))\n        options = {}\n    model.add(Dense(10, activation='softmax',**options))\n    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer,metrics=['accuracy'])\n    return model","2562658c":"y_train = to_categorical(labels_train, 10)\ny_val = to_categorical(labels_val, 10)","a1589673":"model = build_model(input_shape=train.shape[1:], n_hidden=4, n_neurons=60, optimizer='Adam')\nhistory = model.fit(x=train, y=y_train, epochs=60, validation_data=(train_val, y_val))","440b30f5":"pd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1) # set the vertical range to [0-1]","afe2edfd":"prediction_keras_nn = model.predict_classes(train_val)\nreport_and_confusion_matrix(labels_val, prediction_keras_nn)","e42967a0":"prediction_keras_nn_test_sub = model.predict_classes(test)\nsub['Label'] = prediction_keras_nn_test_sub.astype('int64')\nsub.to_csv(\"Keras_NN.csv\", index=False)","8bfbd3ce":"## Separating a part of our training set to validate our models, normally we have 20% for evaluation and testing set, as the testing set was given, I am gonna separate 10% for the validation set","d7a2b9ca":"### A significant improvement compared to logistic regression...\n#### Now, random forest? \n\n3. The random forest model","e55eaa1e":"4. Multi Layer Perceptron","f32a99e0":"* The random forest model is generalizing a bit better than the KNN","9b110b10":"***","9c7dc2a9":"#### plotting the accuracy, loss in both training set and validation set","a38b95b3":"#### loading our training and test set","60d2da52":"* Our multi layer perceptron gave us the best result so far...","3c1e5b80":"#### As can be seen, the model is generalizig fairly well, the confusion matrix is showing that some digits have like 1s, are easier to predict and 5s not quite so.(this is due to the shapes of some digits)","0a97ab63":"2. K NeighborsClassifier","9a7e4d32":"#### As can be seen, we have a varied of different digits for each digit\n##### most of them are easily identifiable, whereas some are not so, namely, 4s, 2s? 5s?","625e63e4":"***\n### Models \n1. Logistic Regression","afd53514":"### the dataset is composed of 784 columns, each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. ","b80e759a":"* Normalization \n* dividing each pixel and label of our dataset by 255","14d1398b":"## Loading relevant models in the beggining of the code\n#### I am also asserting that we re using python 3.5 and scikit-learn version 0.20 ","8a0f985d":"#### Plotting the distribution of digits in our dataset","9037b715":"***\n\n## creating a function that evaluates the prediction that has been made by different models and plot the confusion matrix","f39da635":"* we don't assymetries in our distribution, all the numbers are equally represented\n\n#### getting 100 digits and plotting then to have an idea of the data we have, how the image looks like","ab0312ca":"* our target is the label column, so I am gonna separate it and create our Y_value, which is our ground truth, the value that we want to predict","fb9328bf":"***","381a6738":"** it can be seen that the model is overfitting, the validation loss is telling us that, in order to get a better score I would need to use some regularization, or early stopping**","88d39498":"# This is my first kernel for the MNIST Digit Recognizer Competition\n## All help and feedback is welcome.\n\nCredits to this kernel [here](https:\/\/www.kaggle.com\/dejavu23\/mnist-sklearn-and-keras)"}}