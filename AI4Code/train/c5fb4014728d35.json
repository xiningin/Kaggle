{"cell_type":{"8e98cbe4":"code","f2f9fc32":"code","633a9a45":"code","f384385d":"code","62094f30":"code","3cc2f791":"code","ccfcad41":"code","68711876":"code","db5e13f6":"code","ce615abe":"code","dd69482f":"code","808b89fb":"code","9fee0bf1":"code","f571186a":"code","d3b81d1d":"code","3f156654":"code","b40ccd36":"code","94caaea1":"code","ca771b46":"code","2fe3ea65":"code","48878c4a":"code","4f969fa8":"code","88454839":"code","257257b7":"code","d287cf10":"code","30b6e431":"code","181ffb34":"code","24b6c68b":"code","9d338a41":"code","98f580f3":"code","51d0b344":"code","5b95cfff":"code","ce405c04":"code","70d7c45d":"code","906464ce":"code","59b69695":"code","10fae702":"code","d47fa9f5":"code","74f688c5":"code","0a9eeaf4":"code","d92b3288":"code","379841d7":"code","c1345461":"code","bf1e0251":"code","cddcd5d2":"code","c0fee8ae":"markdown","9984125a":"markdown","98e8b203":"markdown","66c4f6f2":"markdown","5a83697e":"markdown","5953f724":"markdown","1e4c4355":"markdown","087ab671":"markdown","813282f0":"markdown","78d31066":"markdown","0cb27dec":"markdown","ee9991b9":"markdown","9d071eea":"markdown","f00be967":"markdown"},"source":{"8e98cbe4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f2f9fc32":"import numpy as np \nimport pandas as pd","633a9a45":"train_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","f384385d":"train_df['target'].value_counts()","62094f30":"train_df.isna().sum()","3cc2f791":"train_df.fillna('unknown',inplace=True)","ccfcad41":"train_df.isna().sum()","68711876":"#library that contains punctuation\nimport string\nstring.punctuation","db5e13f6":"def remove_punctuation(text):\n    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n    return punctuationfree","ce615abe":"#REMOVE PUNCTUATION TO CLEAN THE TEXT\ntrain_df['text']= train_df['text'].apply(lambda x:remove_punctuation(x))","dd69482f":"# APPLYING LOWERCASE\ntrain_df['text']= train_df['text'].apply(lambda x: x.lower())","808b89fb":"def tokenize(string):\n    '''\n    Tokenizes the string to a list of words\n    '''\n    tokens = string.split()\n    return tokens","9fee0bf1":"# TOKENIZING THE CLEANSED TEXT \ntrain_df['text']= train_df['text'].apply(lambda x: tokenize(x))","f571186a":"train_df.head()","d3b81d1d":"train_df['keyword']= train_df['keyword'].apply(lambda x: tokenize(x))","3f156654":"# DROPPING THE ID FROM THE DATASET AS IT DOESNOT GIVE ANY RELEVANT INFORMATION.\ntrain_df.drop(columns=['id'],inplace=True)","b40ccd36":"#importing nlp library\nimport nltk\n#Stop words present in the library\nstopwords = nltk.corpus.stopwords.words('english')","94caaea1":"def remove_stopwords(text):\n    output= [i for i in text if i not in stopwords]\n    return output","ca771b46":"# REMOVING STOPWORDS\ntrain_df['text']= train_df['text'].apply(lambda x:remove_stopwords(x))","2fe3ea65":"from nltk.stem.porter import PorterStemmer\nporter_stemmer = PorterStemmer()","48878c4a":"# STEMMING\ndef stemming(text):\n    stem_text = [porter_stemmer.stem(word) for word in text]\n    return stem_text\ntrain_df['text']=train_df['text'].apply(lambda x: stemming(x))","4f969fa8":"from nltk.stem import WordNetLemmatizer\n#defining the object for Lemmatization\nwordnet_lemmatizer = WordNetLemmatizer()","88454839":"#defining the function for lemmatization\ndef lemmatizer(text):\n    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n    return lemm_text","257257b7":"nltk.download('wordnet')","d287cf10":"vocab = []\n\n'''\nWe add all the lists of tokenized strings to make one large list of words\n\nNote ['a','b'] + ['c'] = ['a','b','c']\n\n'''\n\nfor i in train_df['text'].values:\n    vocab = vocab + i\n\nprint(len(vocab))","30b6e431":"# We make a set of the vocab words to remove multiple occurences of a same word, implying only unique words stay in set.\n\nset_vocab = set(vocab)\nvocab = list(set_vocab)\n# we convert that set back to a list\nprint(len(vocab),type(vocab))","181ffb34":"## Converting the tokens back to strings to feed it into Count Vectorizer\n\ntrain_df['text_strings'] = train_df['text'].apply(lambda x: ' '.join([str(elem) for elem in x]))","24b6c68b":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(train_df['text_strings'])","9d338a41":"x_train = X.toarray()","98f580f3":"import numpy as nper\nx_train = np.array(x_train)\ny_train = train_df['target']","51d0b344":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=0)\nclf.fit(x_train,y_train)","5b95cfff":"pred = clf.predict(x_train)\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_train, pred)","ce405c04":"from sklearn.tree import DecisionTreeClassifier\nclassifier2 = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier2.fit(x_train, y_train)","70d7c45d":"y_pred2 = classifier2.predict(x_train)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_train, y_pred2)\nprint(cm)\naccuracy_score(y_train, y_pred2)","906464ce":"test_df.fillna('',inplace=True)","59b69695":"test_df.drop(columns=['id','keyword','location'],inplace=True)\n","10fae702":"test_df['text']= test_df['text'].apply(lambda x:remove_punctuation(x))\ntest_df['text']= test_df['text'].apply(lambda x: tokenize(x))\ntest_df['text']= test_df['text'].apply(lambda x:remove_stopwords(x))\ntest_df['text']= test_df['text'].apply(lambda x: stemming(x))","d47fa9f5":"test_df['text_strings'] = test_df['text'].apply(lambda x: ' '.join([str(elem) for elem in x]))","74f688c5":"x_test = vectorizer.transform(test_df['text_strings'])","0a9eeaf4":"x_test = x_test.toarray()\nx_test = np.array(x_test)","d92b3288":"y_test_pred = classifier2.predict(x_test)","379841d7":"submission = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsubmission['target'] = y_test_pred","c1345461":"submission.head()","bf1e0251":"final_submission = submission[['id','target']]","cddcd5d2":"final_submission.to_csv('final_submission.csv')","c0fee8ae":"### FILLING THE MISSING VALUES WITH AN ARBITARY VALUE","9984125a":"WE SEE THAT NO NULL VALUES ARE PRESENT IN THE DATASET. HENCE, WE CAN PROCEED TO DATA PREPROCESSING.","98e8b203":"WE FIND THAT ACCURACY OF DECISION TREE MODEL IS FAR BETTER THAN LOGISTIC REGRESSION. THEREFORE, WE WILL USE THE LATTER FOR PREDICTING OUR TEST SET RESULTS.","66c4f6f2":"## IMPORTING THE TRAIN AND TEST DATASET","5a83697e":"## LOGISTIC REFRESSION MODEL","5953f724":"## DATA PREPROCESSING FOR THE TRAIN SET","1e4c4355":"### CONFUSION MATRIX AND ACCURACY SCORE","087ab671":"COUNTING THE 0s and 1s IN THE DATASET","813282f0":"### Preprocessing the test set","78d31066":"### SAVING THE RESULTS","0cb27dec":"### CHECKING FOR THE MISSING VALUES","ee9991b9":"## DECISION TREE MODEL","9d071eea":"## IMPORTING THE LIBRARIES","f00be967":"### CALCULATING THE ACCURACY"}}