{"cell_type":{"4ee41e8a":"code","3e48b4dc":"code","4829453c":"code","dfbfcef0":"code","01600e5c":"code","cf01ba36":"code","1392704d":"code","11bb7679":"code","ec1f9958":"code","def2e410":"code","7b781ec9":"code","33ae4c70":"code","40ce5fb3":"code","45b4265b":"code","c4455aae":"code","bd044b1f":"code","19fe0e61":"markdown","2c98c181":"markdown","ef889349":"markdown","fe52e0ad":"markdown","4cc5129b":"markdown","d7564aad":"markdown","4eb9283e":"markdown","7b004302":"markdown"},"source":{"4ee41e8a":"#Load packages\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold, KFold","3e48b4dc":"#Load data; drop target and ID's\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\ntrain.drop(train[['ID_code', 'target']], axis=1, inplace=True)\ntest.drop(test[['ID_code']], axis=1, inplace=True)","4829453c":"#https:\/\/www.kaggle.com\/yag320\/list-of-fake-samples-and-public-private-lb-split\/\nfrom tqdm import tqdm_notebook as tqdm\n\n#df_test = pd.read_csv(test_path)\n#df_test.drop(['ID_code'], axis=1, inplace=True)\ndf_test = test.values\n\nunique_samples = []\nunique_count = np.zeros_like(df_test)\nfor feature in tqdm(range(df_test.shape[1])):\n    _, index_, count_ = np.unique(df_test[:, feature], return_counts=True, return_index=True)\n    unique_count[index_[count_ == 1], feature] += 1\n\n# Samples which have unique values are real the others are fake\nreal_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\nsynthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n\nprint(len(real_samples_indexes))\nprint(len(synthetic_samples_indexes))","dfbfcef0":"#df_test_real = df_test[real_samples_indexes].copy()\ntest = test.iloc[real_samples_indexes].copy()","01600e5c":"#Create label array and complete dataset\ny1 = np.array([0]*train.shape[0])\ny2 = np.array([1]*test.shape[0])\ny = np.concatenate((y1, y2))\n\nX_data = pd.concat([train, test])\nX_data.reset_index(drop=True, inplace=True)","cf01ba36":"print(X_data.shape, train.shape, test.shape)","1392704d":"#Initialize splits&LGBM\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=13)\n\nlgb_model = lgb.LGBMClassifier(max_depth=-1,\n                                   n_estimators=500,\n                                   learning_rate=0.01,\n                                   objective='binary', \n                                   n_jobs=-1)\n                                   \ncounter = 1","11bb7679":"#Train 5-fold adversarial validation classifier\nfor train_index, test_index in skf.split(X_data, y):\n    print('\\nFold {}'.format(counter))\n    X_fit, X_val = X_data.loc[train_index], X_data.loc[test_index]\n    y_fit, y_val = y[train_index], y[test_index]\n    \n    lgb_model.fit(X_fit, y_fit, eval_metric='auc', \n              eval_set=[(X_val, y_val)], \n              verbose=100, early_stopping_rounds=10)\n    counter+=1","ec1f9958":"param = {'num_leaves': 50,\n         'min_data_in_leaf': 30, \n         'objective':'binary',\n         'max_depth': 5,\n         'learning_rate': 0.006,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 27,\n         \"metric\": 'auc',\n         \"verbosity\": -1}\n\nrandom_state = 42\nparam = {\n    \"objective\" : \"binary\", \"metric\" : \"auc\", \"boosting\": 'gbdt', \"max_depth\" : -1, \"num_leaves\" : 13,\n    \"learning_rate\" : 0.01, \"bagging_freq\": 5, \"bagging_fraction\" : 0.4, \"feature_fraction\" : 0.05,\n    \"min_data_in_leaf\": 80, \"min_sum_heassian_in_leaf\": 10, \"tree_learner\": \"serial\", \"boost_from_average\": \"false\",\n    \"bagging_seed\" : random_state, \"verbosity\" : 1, \"seed\": random_state\n}\n","def2e410":"train_test = X_data\ntarget=y\nfeatures = [c for c in train_test.columns if c not in ['ID_code', 'target']]","7b781ec9":"folds = KFold(n_splits=5, shuffle=True, random_state=15)\noof = np.zeros(len(train_test))\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_test.values, target)):\n    print(\"fold n\u00b0{}\".format(fold_))\n    trn_data = lgb.Dataset(train_test.iloc[trn_idx][features], label=target[trn_idx])\n    val_data = lgb.Dataset(train_test.iloc[val_idx][features], label=target[val_idx])\n\n    num_round = 30000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000,\\\n                    early_stopping_rounds = 1400)\n    oof[val_idx] = clf.predict(train_test.iloc[val_idx][features], num_iteration=clf.best_iteration)","33ae4c70":"from sklearn import model_selection, preprocessing, metrics\nmetrics.roc_auc_score(target, oof)\n\n# Initial Script : AUC 0.534110539775; \n# Original params - AUC 0.503084033525\n\n#=> deleting the \"fake\" test data showed a clearer picture, train & test are similar","40ce5fb3":"#Load more packages\nfrom scipy.stats import ks_2samp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore')","45b4265b":"#Perform KS-Test for each feature from train\/test. Draw its distribution. Count features based on statistics.\n#Plots are hidden. If you'd like to look at them - press \"Output\" button.\nhypothesisnotrejected = []\nhypothesisrejected = []\n\nfor col in train.columns:\n    statistic, pvalue = ks_2samp(train[col], test[col])\n    if pvalue>=statistic:\n        hypothesisnotrejected.append(col)\n    if pvalue<statistic:\n        hypothesisrejected.append(col)\n        \n    plt.figure(figsize=(8,4))\n    plt.title(\"Kolmogorov-Smirnov test for train\/test\\n\"\n              \"feature: {}, statistics: {:.5f}, pvalue: {:5f}\".format(col, statistic, pvalue))\n    sns.kdeplot(train[col], color='blue', shade=True, label='Train')\n    sns.kdeplot(test[col], color='green', shade=True, label='Test')\n\n    plt.show()","c4455aae":"len(hypothesisnotrejected), len(hypothesisrejected)","bd044b1f":"print(hypothesisrejected)","19fe0e61":"Average AUC across folds is stable and concentrates around 0.5. It means that we can hardly distinguish train set from test set using adversarial validation.\n\nNow let's expand our investigation of dataset and look at distribution of features in train and test sets with respect to [Kolmogorov-Smirnov test](https:\/\/en.wikipedia.org\/wiki\/Kolmogorov%E2%80%93Smirnov_test).","2c98c181":"## Abstract\n\nThe aim of this notebook is to check whether train and test sets are significantly different. Can we trust our local validation schemas and public LB? I'll use adversarial validation and Kolmogorov-Smirnov Test for these purposes.\n\nBased on https:\/\/www.kaggle.com\/bearstrikesback\/adversarial-validation-plus-ks-test\nand the  split-script https:\/\/www.kaggle.com\/yag320\/list-of-fake-samples-and-public-private-lb-split\/\nand also on based on https:\/\/www.kaggle.com\/tunguz\/adversarial-santander\n","ef889349":"****## Conclusion:\n\nFrom adversarial validation we have no evidence that train and test sets come from different distributions. AUC around 0.50 states that LGBM can hardly distinguish train observations from test. These datasets are quite similar. Local validation schemas and public LB track should correctly reflect your efforts in this competition.\n\nFrom Kolmogorov-Smirnov Test we can also state that both sets are quite similar. Hypothesis that samples are drawn from the same distribution can be rejected only for 1 out of 200 features based on KS-Test. Probably, we should pay more attention to those 1 feature.","fe52e0ad":"### Kolmogorov-Smirnov Test","4cc5129b":"Let's take a look at the overall oof CV AUC:","d7564aad":"### Adversarial Validation","4eb9283e":"## Another version\nBased on https:\/\/www.kaggle.com\/tunguz\/adversarial-santander","7b004302":"As we can see, 199 features successfully passed Kolmogorov-Smirnov test. We cannot reject null hypothesis that those features in train and test sets came from the same distribution. 1 feature hasn't passed this test and probably require our attention."}}