{"cell_type":{"3368b20b":"code","4d587bb5":"code","2965f2c7":"code","70820ce8":"code","f53bce26":"code","416b0d9f":"code","39745da6":"code","f0eea358":"code","74e30e65":"markdown","49ca55cb":"markdown","8f5e7ab6":"markdown","28efe0c1":"markdown","df0ce86a":"markdown","9c0aa5c7":"markdown","6f1c6ba6":"markdown","76c95cde":"markdown","d1bbd450":"markdown"},"source":{"3368b20b":"# IMPORTS\n\n# basic utility libraries\nimport numpy as np\nimport pandas as pd\nimport time\nimport datetime\nimport pickle\nfrom matplotlib import pyplot as plt\n%matplotlib notebook\n\n# keras\nfrom keras.datasets import mnist\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, BatchNormalization\nfrom keras.optimizers import SGD\nfrom keras.callbacks import Callback\nfrom keras.models import load_model\n\n# learning and optimisation helper libraries\nfrom sklearn.model_selection import KFold\nfrom hyperopt import fmin, tpe, Trials, hp, rand\nfrom hyperopt.pyll.stochastic import sample\n\n\n# LOAD DATA\n\ndef load_dataset():\n    train_set = pd.read_csv('..\/input\/mnist-in-csv\/mnist_train.csv')\n    test_set = pd.read_csv('..\/input\/mnist-in-csv\/mnist_test.csv')\n    # training features, normalized and reshaped\n    trainX = np.array(train_set.iloc[:, 1:])\n    trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n    # testing features, normalized and reshaped\n    testX = np.array(test_set.iloc[:, 1:])\n    testX = testX.reshape((testX.shape[0], 28, 28, 1))\n    # training features: extract the labels, reshape, and one hot encode\n    trainY = train_set.label\n    trainY = np.array(trainY).reshape(-1, 1)\n    trainY = to_categorical(trainY)\n    # test features: extract the labels, reshape, and one hot encode\n    testY = test_set.label\n    testY = np.array(testY).reshape(-1, 1)\n    testY = to_categorical(testY)\n    return trainX, trainY, testX, testY\n\n# load dataset (60k training samples, and 10k test samples)\nload_trainX, load_trainY, load_testX, load_testY = load_dataset()","4d587bb5":"#### CUSTOM CALLBACK\n\nclass PlotProgress(Callback):\n    def on_train_begin(self, logs={}):\n        self.fig, self.ax1, self.ax2 = self.prepare_plot()\n        self.loss_history = list()\n        self.val_loss_history = list()\n        self.accuracy_history = list()\n        self.val_accuracy_history = list()\n        \n    # plot diagnostic learning curve\n    def prepare_plot(self):\n        fig, (ax1, ax2) = plt.subplots(2,1)\n        ax1.set_title('Cross Entropy Loss')\n        ax2.set_title('Accuracy')\n        plt.tight_layout()\n        return fig, ax1, ax2\n\n    def update_plot(self):\n        # plot loss\n        self.ax1.plot(self.loss_history, color='blue', label='train_acc')\n        self.ax1.plot(self.val_loss_history, color='orange', label='val_acc')\n        # plot accuracy\n        self.ax2.plot(self.accuracy_history, color='blue', label='train_acc')\n        self.ax2.plot(self.val_accuracy_history, color='orange', label='val_acc')\n        self.fig.canvas.draw()\n\n    def on_epoch_end(self, epoch, logs=None):\n        self.loss_history.append(logs.get('loss'))\n        self.val_loss_history.append(logs.get('val_loss'))\n        self.accuracy_history.append(logs.get('accuracy'))\n        self.val_accuracy_history.append(logs.get('val_accuracy'))\n        self.update_plot()\n\n        \n        \n#### PREPROCESSING\n            \n# this is actually meant to be called by prep_data\ndef prep_pixels(dataset):\n    dataset_norm = dataset.astype('float32')\n    dataset_norm = dataset_norm \/ 255.0\n    return dataset_norm\n\n\ndef prep_data(training_size=0):\n    global load_trainX, load_trainY, load_testX, load_testY\n    trainX, trainY, testX, testY = np.copy(load_trainX), np.copy(load_trainY), np.copy(load_testX), np.copy(load_testY)\n    trainX, testX = prep_pixels(trainX), prep_pixels(testX)\n    if training_size == 0:\n        training_size = trainX.shape[0]\n    trainX = trainX[0:training_size]\n    trainY = trainY[0:training_size]\n    return trainX, trainY, testX, testY\n\n\n\n#### DEFINE MODEL AND TRAINING\/EVALUTION METHODS\n\ndef define_model(learning_rate, momentum):\n    model = Sequential()\n    model.add(Conv2D(32, (3,3), activation = 'relu', kernel_initializer = 'he_uniform', input_shape=(28,28,1)))\n    model.add(MaxPooling2D((2,2)))\n    model.add(Flatten())\n    model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(10, activation='softmax'))\n    opt = SGD(lr=learning_rate, momentum=momentum)\n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\ndef evaluate_model(trainX, trainY, testX, testY, max_epochs, learning_rate, momentum, batch_size, model=None, callbacks=[]):\n    if model == None:\n        model = define_model(learning_rate, momentum)\n    history = model.fit(trainX, trainY, epochs=max_epochs, batch_size=batch_size, validation_data=(testX, testY), verbose=0, callbacks = callbacks)\n    return model, history\n\n\ndef evaluate_model_cross_validation(trainX, trainY, max_epochs, learning_rate, momentum, batch_size, n_folds=5):\n    scores, histories = list(), list()\n    # prepare cross validation\n    kfold = KFold(n_folds, shuffle=True, random_state=1)\n    # enumerate splits\n    for trainFold_ix, testFold_ix in kfold.split(trainX):\n        # select rows for train and test\n        trainFoldsX, trainFoldsY, testFoldX, testFoldY = trainX[trainFold_ix], trainY[trainFold_ix], trainX[testFold_ix], trainY[testFold_ix]\n        # fit model\n        model = define_model(learning_rate, momentum)\n        history = model.fit(trainFoldsX, trainFoldsY, epochs=max_epochs, batch_size=batch_size, validation_data=(testFoldX, testFoldY), verbose=0)\n        # evaluate model\n        _, acc = model.evaluate(testFoldX, testFoldY, verbose=0)\n        # stores scores\n        scores.append(acc)\n        histories.append(history)\n    return scores, histories\n\n\n\n#### VISUALISATION OF TRAINING OUTCOME\n\ndef summarize_diagnostics(histories):\n    fig, (ax1, ax2) = plt.subplots(2,1)\n    for i in range(len(histories)):\n        # plot loss\n        ax1.set_title('Cross Entropy Loss')\n        ax1.plot(histories[i]['loss'], color='blue', label='train_loss')\n        ax1.plot(histories[i]['val_loss'], color='orange', label='val_loss')\n        # plot accuracy\n        ax2.set_title('Accuracy')\n        ax2.plot(histories[i]['accuracy'], color='blue', label='train_acc')\n        ax2.plot(histories[i]['val_accuracy'], color='orange', label='val_acc')\n    fig.canvas.draw()\n    \n    \n    \n#### HYPERPARAMETER OPTIMIZATION\n\ndef grid_search(trainX, trainY, testX, testY, max_epochs, learning_rates, momentums, batch_sizes):\n    hyperparameter_sets, scores = list(), list()\n    callbacks = [PlotProgress()]\n    i = 1\n    total_runs = len(learning_rates) * len(momentums) *  len(batch_sizes)\n    running_time = 0\n    start = time.time()\n    for lr in learning_rates:\n        for momentum in momentums:\n            for bs in batch_sizes:\n                if i > 1:\n                    time_remaining = running_time\/(i-1)*(total_runs-(i-1))\n                    print('{} of {} done so far so far in {}s. Estimated time remaining: {}s'.format(i-1, total_runs, running_time, time_remaining))\n                print('Starting run {} of {}'.format(i, total_runs))\n                print('Evaluating Hyperparameters: learning_rate: {}, momentum: {}, batch_size: {}'.format(lr, momentum, bs))\n                accuracies, _ = evaluate_model_cross_validation(trainX, trainY, max_epochs=max_epochs, learning_rate=lr, momentum=momentum, batch_size=bs, n_folds=5)\n                score = np.log10(1 - np.mean(accuracies))\n                scores.append(score)\n                with open('grid_scores.pickle', 'wb') as file:\n                    pickle.dump(scores, file)\n                hyperparameter_sets.append({'learning_rate': lr, 'momentum': momentum, 'batch_size': bs})\n                with open('grid_hpsets.pickle', 'wb') as file:\n                    pickle.dump(hyperparameter_sets, file)\n                running_time = time.time() - start\n                i+=1\n    return hyperparameter_sets, scores\n\n\ndef selective_search(kind, space, max_evals, batch_size=32, plot=False):\n    \n    trainX, trainY, testX, testY = prep_data()\n    \n    try:\n        hyperparameter_sets = pickle.load(open('{}_hpsets.pickle'.format(kind), 'rb'))\n    except:\n        hyperparameter_sets = list()\n    try:\n        scores = pickle.load(open('{}_scores.pickle'.format(kind), 'rb'))\n    except:\n        scores = list()\n\n    if plot:\n        fig = plt.figure(figsize=(8, 5))\n        ax = fig.add_subplot(1,1,1)\n        ax.set_title('{} search'.format(kind))\n        ax.set_xlabel('# Iteration')\n        ax.set_ylabel('log(1 - accuracy)')\n        \n    def objective(params):\n        lr, momentum = params['lr'], params['momentum']\n        if plot:\n            ax.set_title('Evaluating: lr: {:0.4f}, momentum: {:0.4f}, bs: {}'.format(lr, momentum, batch_size))\n        accuracies, _ = evaluate_model_cross_validation(trainX, trainY, max_epochs=1, learning_rate=lr, momentum=momentum, batch_size=batch_size, n_folds=5)\n        score = np.log10(1 - np.mean(accuracies))\n        scores.append(score)\n        with open('{}_scores.pickle'.format(kind), 'wb') as file:\n            pickle.dump(scores, file)\n        hyperparameter_sets.append({'learning_rate': lr, 'momentum': momentum, 'batch_size': batch_size})\n        with open('{}_hpsets.pickle'.format(kind), 'wb') as file:\n            pickle.dump(hyperparameter_sets, file)\n        if plot:\n            x = range(len(scores))\n            ax.plot(x, scores, 'bo--')\n            fig.canvas.draw()\n        return score\n    \n    def run_trials(kind, objective, space, max_evals):\n        evals_step = 20\n\n        try:\n            trials = pickle.load(open('{}_trials.pickle'.format(kind), 'rb'))\n            max_evals = len(trials.trials) + evals_step\n            print('Starting new set of trials')\n        except:\n            trials = Trials()\n\n        if kind == 'bayesian':\n            best = fmin(fn=objective, space=space, algo=tpe.suggest, trials=trials, max_evals=max_evals)\n        elif kind == 'random':\n            best = fmin(fn=objective, space=space, algo=rand.suggest, trials=trials, max_evals=max_evals)\n        else:\n            raise BaseError('First parameter \"kind\" must be either \"bayesian\" or \"random\"')\n\n        with open('{}_trials.pickle'.format(kind), 'wb') as file:\n            pickle.dump(trials, file)\n\n    while True:\n        run_trials(kind, objective, space, max_evals)\n        \n    return histories, hyperparameter_sets, scores","2965f2c7":"trainX, trainY, testX, testY = prep_data()\n_, history = evaluate_model(trainX, trainY, testX, testY, 10, learning_rate = 1e-2, momentum=0.9, batch_size = 32, callbacks = [PlotProgress()])\nprint('Final validation accuracy is: {:0.2f}%'.format(100 * history.history['val_accuracy'][-1]))","70820ce8":"trainX, trainY, testX, testY = prep_data()\nscores_1ep, _ = evaluate_model_cross_validation(trainX, trainY, max_epochs=1, learning_rate=1e-2, momentum=0.9, batch_size=32, n_folds=5)\nscores_10ep, _ = evaluate_model_cross_validation(trainX, trainY, max_epochs=10, learning_rate=1e-2, momentum=0.9, batch_size=32, n_folds=5)\nplt.boxplot([scores_1ep, scores_10ep], labels=['1 epoch','10 epochs'])\nplt.title('Box whisker plot of accuracies')\nplt.ylabel('Accuracy')\nplt.show()","f53bce26":"with open('..\/input\/hyperparameter-optimisation-on-a-cnn\/grid_scores.pickle', 'rb') as file:\n    scores = pickle.load(file)\n    \nwith open('..\/input\/hyperparameter-optimisation-on-a-cnn\/grid_hpsets.pickle', 'rb') as file:\n    hp_sets = pickle.load(file)\n\nresults_df = pd.DataFrame(hp_sets)\nresults_df['score'] = scores\n\nlearning_rates = [np.sqrt(10)*1e-4, 1e-3, np.sqrt(10)*1e-3, 1e-2, np.sqrt(10)*1e-2, 1e-1]\nmomentums = [1-np.sqrt(10)*1e-1, 1-1e-1, 1-np.sqrt(10)*1e-2, 1-1e-2, 1-np.sqrt(10)*1e-3]\nbatch_sizes = [32]\n\ndef plot2D_lr(batch_size=32):\n    nplts = len(momentums)\n    nrows = np.ceil(nplts\/2)\n    fig = plt.figure(figsize=(8, 3*nrows))\n    i = 1\n    for momentum in momentums:\n        plot_df = results_df[(results_df['batch_size'] == batch_size) & (results_df['momentum'] == momentum)]\n        ax = fig.add_subplot(nrows, 2, i)\n        x = {'series': np.log10(plot_df['learning_rate']), 'label': 'log(learning_rate)'}\n        y = {'series': plot_df['score'], 'label': 'log(1 - acc)'}\n        ax.scatter(x['series'], y['series'])\n        ax.set_xlabel(x['label'])\n        ax.set_ylabel(y['label'])\n        ax.set_title('bs = {}, momentum = {}'.format(batch_size, momentum))\n        i += 1\n    plt.tight_layout()\n    plt.show()\n\ndef plot2D_momentum(batch_size=32):\n    nplts = len(learning_rates)\n    nrows = np.ceil(nplts\/2)\n    fig = plt.figure(figsize=(8, 3*nrows))\n    i = 1\n    for learning_rate in learning_rates:\n        plot_df = results_df[(results_df['batch_size'] == batch_size) & (results_df['learning_rate'] == learning_rate)]\n        ax = fig.add_subplot(nrows, 2, i)\n        x = {'series': np.log10(1 - plot_df['momentum']), 'label': 'log(1 - momentum)'}\n        y = {'series': plot_df['score'], 'label': 'log(1 - acc)'}\n        ax.scatter(x['series'], y['series'])\n        ax.set_xlabel(x['label'])\n        ax.set_ylabel(y['label'])\n        ax.set_title('bs = {}, learning_rate = {}'.format(batch_size, learning_rate))\n        i += 1\n    plt.tight_layout()\n    plt.show()\n    \nplot2D_lr()","416b0d9f":"# this is the function we'll use to plot the results of the random search as well as the bayesian optimization\ndef plot_selective_search_results(scores, hpsets):\n    scores = scores[:1000]\n    hpsets = hpsets[:1000]\n    fig = plt.figure(figsize=(18, 25))\n    x = range(len(scores))\n    ax = fig.add_subplot(6,1,1)\n    ax.plot(x, scores, 'b.--')\n    poly = np.poly1d(np.polyfit(x, scores, 1))\n    ax.plot(x, poly(x), 'r-')\n    ax.set_title('1. Score over iterations')\n    ax.set_xlabel('# iterations')\n    ax.set_ylabel('log10(1 - validation_accuracy)')\n    ax.legend(labels=['signal', 'fit: {:0.1E}*x {:0.1f}'.format(poly[1], poly[0])], loc='upper right')\n    \n    ax = fig.add_subplot(6,1,2)\n    ax.plot(x, np.minimum.accumulate(scores), 'b.')\n    ax.set_title('2. Running best score over iterations (best of all was {})'.format(np.min(scores)))\n    ax.set_xlabel('# iterations')\n    ax.set_ylabel('log10(1 - validation_accuracy)')\n\n    learning_rates = np.log10([hpsets[i]['learning_rate'] for i in x])\n    ax = fig.add_subplot(6,1,3)\n    ax.plot(x, learning_rates, 'bo')\n    ax.set_title('3. Learning rate over iterations')\n    ax.set_xlabel('# iterations')\n    ax.set_ylabel('log10(learning_rate)')\n\n    bs = 100\n    len_series = int(np.ceil(len(scores)\/bs))\n    batched_learning_rates = [learning_rates[i*bs:i*bs+bs] for i in range(len_series)]\n    ax = fig.add_subplot(6,1,4)\n    ax.boxplot(batched_learning_rates, positions=np.arange(len_series))\n    ax.set_title('4. Box and whisker plots of learning rates for batched iterations')\n    ax.set_xlabel('# batch (each batch contains {} iterations)'.format(bs))\n    ax.set_ylabel('log10(learning_rate)')\n\n    momentums = np.log10([1 - hpsets[i]['momentum'] for i in x])\n    ax = fig.add_subplot(6,1,5)\n    ax.plot(x, momentums, 'bo')\n    ax.set_title('5. Momentum over iterations')\n    ax.set_xlabel('# iterations')\n    ax.set_ylabel('log10(1 - momentum)')\n\n    batched_momentums = [momentums[i*bs:i*bs+bs] for i in range(len_series)]\n    ax = fig.add_subplot(6,1,6)\n    ax.boxplot(batched_momentums, positions=np.arange(len_series))\n    ax.set_title('6. Box and whisker plots momentums for batched iterations')\n    ax.set_xlabel('# batch (each batch contains {} iterations)'.format(bs))\n    ax.set_ylabel('log10(1 - momentum)')\n\n    plt.tight_layout()\n    \nwith open('..\/input\/hyperparameter-optimisation-on-a-cnn\/random_scores.pickle', 'rb') as file:\n    rand_scores = pickle.load(file) \nwith open('..\/input\/hyperparameter-optimisation-on-a-cnn\/random_hpsets.pickle', 'rb') as file:\n    rand_hpsets = pickle.load(file)\n\nplot_selective_search_results(rand_scores, rand_hpsets)","39745da6":"with open('..\/input\/hyperparameter-optimisation-on-a-cnn\/bayesian_scores.pickle', 'rb') as file:\n    bo_histories = pickle.load(file) \nwith open('..\/input\/hyperparameter-optimisation-on-a-cnn\/bayesian_hpsets.pickle', 'rb') as file:\n    bo_hpsets = pickle.load(file)\n\nplot_selective_search_results(bo_histories, bo_hpsets)","f0eea358":"with open('..\/input\/hyperparameter-optimisation-on-a-cnn\/grid_scores.pickle', 'rb') as file:\n    grid_scores = pickle.load(file)\nwith open('..\/input\/hyperparameter-optimisation-on-a-cnn\/grid_hpsets.pickle', 'rb') as file:\n    grid_hpsets = pickle.load(file)  \nwith open('..\/input\/hyperparameter-optimisation-on-a-cnn\/random_scores.pickle', 'rb') as file:\n    rand_scores = pickle.load(file)\nwith open('..\/input\/hyperparameter-optimisation-on-a-cnn\/random_hpsets.pickle', 'rb') as file:\n    rand_hpsets = pickle.load(file)\nwith open('..\/input\/hyperparameter-optimisation-on-a-cnn\/bayesian_scores.pickle', 'rb') as file:\n    bayesian_scores = pickle.load(file) \nwith open('..\/input\/hyperparameter-optimisation-on-a-cnn\/bayesian_hpsets.pickle', 'rb') as file:\n    bayesian_hpsets = pickle.load(file) \n\nprint('Grid best', grid_hpsets[np.argsort(grid_scores)[0]])\nprint('Random best', rand_hpsets[np.argsort(rand_scores)[0]])\nprint('Bayesian best', bayesian_hpsets[np.argsort(bayesian_scores)[0]])\n    \nfig = plt.figure(figsize=(9.5,4))\nax = fig.add_subplot(1,1,1)\nax.plot(range(len(rand_scores)), np.minimum.accumulate(rand_scores), 'r.')\nax.plot(range(len(bayesian_scores)), np.minimum.accumulate(bayesian_scores), 'b.')\nax.plot(range(len(grid_scores)), np.minimum.accumulate(grid_scores), 'g.')\nax.set_title('Running best scores of each method of hyperparameter optimisation')\nax.set_ylabel('log10(1 - val_accuracy)')\nax.set_xlabel('# iteration')\nax.legend(['random search', 'bayesian optimization', 'grid_search'])\nax.set_ylim(-1.7, -1.6)","74e30e65":"# 7. Bayesian optimisation\n\nNow let's try running Bayesian optimisation . We can make observations about each chart produced below:\n\n1. **Score over iterations** and 2. **Running best score over iterations**\n\n  - We do observe a trend towards better accuracy as seen by the fitted line. This is probably less because the minimum is improving, and more because the algorithm spends less time evaluating hyperparameters which are clearly not candidates for optimal performance.\n  \n  \n3. **Learning rate over iterations** and 4. **the corresponding box and whisker plots**\n  - A strange thing we see here is convergence and divergence of the trialled hyperparameters. My guess is because the noise from the statistical deviations are not allowing the algorithm to map out the terrain reliably. It can't settle on a minimum because each time it tests a certain set of hyperparameters it gets a slightly different answer.\n  - Nevertheless we do see hints that the algorithm restricts its search space to a narrower neighborhood than the full range, as expected.\n  \n  \n5. **Momentum over iterations** and 6. **the corresponding box and whisker plots**\n  - Here we make similar observations to the learning rates. What's interesting, is the way the average tends to converge and diverge with the learning rate. Remember I mentioned earlier that as momentum increases, we need to decrease learning rate to maintain good model training performance. So there is somehow a coupling between momentum and learning rate if we try to maintain good performance. This is what the optimisation algorithm is demonstrating for us here!","49ca55cb":"# 6. Random search\n\nWe run random search for 1000 iterations over the same ranges in the grid search (but now the ranges are continuous instead of discrete). This is a control experiment for the subsequent Bayesian Optimization experiment. Notice that the scores tend to vary evenly throughout the runs, and that the spread of trialled hyperparameters stays even throughout the experiment.","8f5e7ab6":"# 3. Do a one time training run\n\nLet's just pick some hyperparameters and try to do a training run. In this example we'll use:\n\n- A batch size of 32\n- Learning rate of 1e-2\n- Momentum of 0.9\n- 10 epochs\n\n(Bottom axis on the charts is for epoch #)","28efe0c1":"# 2. Prepare functions for training and optimisation\n\nSome functions worth calling out:\n\n- **define_model** - This is where we define the architecture of our CNN. I've decided to stick to a fairly simple one in order to limit computational resources required.\n\n- **evaluate_model_cross_validation** - This is for running k-fold cross validation. We take the training data, shuffle it, and split it into k groups. We train a new model on k-1 groups, and validate it on the remaining group. We do this k times (starting with freshly initialized model weights each time), giving each group a chance to be the validation group. Then we take an average over the validation accuracies of each trial to get an estimate for the performance of the model\/training parameters.\n\n- **grid_search** - We trial a preset selection of hyperparameter combinations, evaluating the performance of the model with each run. This lets us get a coarse map of model performance over hyperparameter space. In the interest of saving time and resources we only do single evaluations of the model at each selected point in hyperparameter space. Although given more time and resources, we could do k-fold cross validation.\n\n- **selecive_search** - We run either Random Search, or Bayesian Optimisation over a selected range of hyperparameters. \n  - Random search involves randomly selecting hyperparameters in the given range and trialling them. \n  - Bayesian Optimisation on the other hand, uses knowledge of the performance of previous trials to construct probabilisitic surrogate functions for the real objective function (or put simply - it guesses the relationship between hyperparameter choice and model perormance). It then uses this knowledge to make a best guess for which hyperparameters to choose next.","df0ce86a":"# 1. Import libraries and load data\n\n- I use Keras on Tensorflow for building and training the model.\n\n- I use Hyperopt for hyperparameter optimisation\n\n- The dataset used here can be found at https:\/\/www.kaggle.com\/oddrationale\/mnist-in-csv","9c0aa5c7":"In this notebook I'll run through the following topics:\n- Loading and preparing MNIST handwritten digits data.\n- Building and training a simple CNN to recognise the digits.\n- Checking the model\/training performance with 5-fold cross validation.\n- Implementing and comparing 3 methods of hyperparameter optimisation: grid search, random search, Bayesian Optimisation\n\nI haven't optimised this notebook for easy reading or a wide audience. You might like to use this notebook if:\n- You are hunting for code snippets in general\n- You want to see the comparison between the three hyperparameter optimisation methods with painstakingly running it yourself.\n\n#### **Full disclosure:**\n_Before doing this exercise I had never applied deep learning in any practical sense, and only had a surface level understanding of what it is. This notebook could be useful for those of you just starting out. At the same time, take everything you see with a pinch of salt. The starter code for this project was borrowed from https:\/\/machinelearningmastery.com\/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification\/_","6f1c6ba6":"# 4. Try K-Fold cross validation\n\nLet's try 5-fold cross validation with the same hyperparameters as above to get a sense for the standard deviation and average. We'll do it for 1 epoch (we'll end up using this later) and 10 epochs.","76c95cde":"# 8. Final remarks about hyperoptimisation (in this specific use case)\n\nAlthough we were able to demonstrate Bayesain Optimisation in action, the results are far from inspiring. The chart below shows us the running best scores from each of the methods. Bayesian optimisation gets the best results but only beats random search by 0.04% in accuracy. Our course grid seach got 97.8% accuracy in 30 iterations whereas it took Bayesian Optmisation 500 trials to get 97.9% accuracy.\n\nIt's definitely worth noting that the statistical noise may have spoiled the ability of Bayesian Optimisation to do its job.\n\nIn any case, for the practical purposes of this example, it would have been suffiecient to run grid search and Bayesian Optimisation for say 100 iterations each to get a decent idea of how the model might peform.","d1bbd450":"# 5. Grid search to map the terrain\n\nNow we run a coarse grid search over hyperparameters to get a rough idea of where our good values might be. Later on we will be running lengthy optimisation algorithms, so in the interest of saving time we need to limit ourselves to 1 epoch which as we will see, still provides meaningful results. We'll stick with a batch size of 32 while searching over learning rate and momentum. \n\nWe'll also need to apply 5-fold cross-validation to drown out the noise. It's not entirely necessary for this grid search as the spread is small enough such that the results would still be informative, but it will be needed later for the Bayesian Optimization run.\n\nI will not actually run the code in this notebook, but instead I'll show the results from what I ran on a Kaggle kernel using a GPU for the heavy lifting. Note that even though in some regimes it looks like our minimum might fall outside of the grid search range, I've checked those areas offline and found that the score got worse.\n\nWhat explains the shifting of the minimum as momentum changes you might ask? In a stable configuration, momentum tends to accelerate the convergence towards a minimum, therefore allowing for smaller learning rates to be used. On the other hand, if learning rate is maintained as momentum increases, the system tends to become unstable. Try this interactive simulation to get a feeling for it https:\/\/distill.pub\/2017\/momentum\/"}}