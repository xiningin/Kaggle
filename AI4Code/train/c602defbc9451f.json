{"cell_type":{"fe75d491":"code","b62595d1":"code","8ddffc80":"code","43c8bcca":"code","ffd25a4b":"code","db88bc7a":"code","dbf5ae48":"code","6930bd2d":"code","b91b4709":"code","e4f36427":"code","c07fdc21":"code","26d6292b":"code","62e2db29":"code","cdcf03b6":"code","36057ed3":"code","7e65b07a":"code","862ea297":"code","6e73eeab":"code","7ff2bfb4":"code","3cb262a2":"code","cfd973b0":"code","22a93af0":"code","ed32e27a":"code","27681025":"code","c070d042":"code","bd8069fa":"code","b2753801":"code","8d94ad99":"code","8412ea9b":"code","56cb902b":"code","53ede893":"code","ed3c8313":"code","828fad78":"code","67a229e8":"code","97153c67":"code","39335619":"code","0056f1e9":"markdown","bd30e10f":"markdown","22914cbe":"markdown","375a6563":"markdown","77890e64":"markdown","9db0ccb0":"markdown","3c2c4db0":"markdown","aded39b1":"markdown","643064cc":"markdown","4d7f8520":"markdown","ec62708f":"markdown","c392d8ff":"markdown","1085ac41":"markdown","617cde1d":"markdown","4f55333c":"markdown","5aabea59":"markdown","f55231eb":"markdown","877eb983":"markdown","a74cbfd9":"markdown","a5f20930":"markdown","08d2daea":"markdown","b71514cd":"markdown","29b106b6":"markdown","dd6130ba":"markdown","603ca515":"markdown","5dd76e68":"markdown","531652cb":"markdown","2be69140":"markdown","9005411c":"markdown","bc71f6bb":"markdown","770a5d92":"markdown","f096d842":"markdown"},"source":{"fe75d491":"import numpy as np \nimport pandas as pd \n\n# Input data files are available in the read-only \"..\/input\/\" directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b62595d1":"# import libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\nimport IPython.display as display\n\n# Graphics libraries\n#!pip install pywaffle\n#!pip install pycomp\n\nimport shap\n\n#from pywaffle import Waffle\nimport xgboost\nimport pickle\n\nimport warnings\n#warnings.filterwarnings(\"ignore\")","8ddffc80":"def missing_count(df):\n    missing_count = df.isna().sum()\n    missing_df = (pd.concat([missing_count.rename('Missing count'),\n                     missing_count.div(len(df))\n                          .rename('Missing ratio')],axis = 1)\n             .loc[missing_count.ne(0)])\n    return missing_df.sort_values(by=\"Missing ratio\")","43c8bcca":"SampleSubmission = \"..\/input\/tabular-playground-series-jan-2021\/sample_submission.csv\"\ntraintablepath = \"..\/input\/tabular-playground-series-jan-2021\/train.csv\"\ntesttablepath = \"..\/input\/tabular-playground-series-jan-2021\/test.csv\"","ffd25a4b":"table_train = pd.read_csv(traintablepath)\ntable_test =  pd.read_csv(testtablepath)\nsampSum = pd.read_csv(SampleSubmission)","db88bc7a":"missing_df = missing_count(table_train)\n\ndisplay.display(table_train.describe().T)\ndisplay.display(missing_df)\ntable_train.head()","dbf5ae48":"missing_df = missing_count(table_test)\n\ndisplay.display(table_test.describe().T)\ndisplay.display(missing_df)\ntable_test.head()","6930bd2d":"columns = [\"id\"]\ntable_train = table_train.drop(columns=columns)\ntable_test = table_test.drop(columns=columns)","b91b4709":"table_train.dtypes","e4f36427":"def plot_whole_features(df):\n    columns_num = df.columns\n    fig, axes = plt.subplots(nrows=round(len(columns_num)\/3), ncols=3, figsize=(40, 40))\n    for ax, column in zip(axes.flatten(),columns_num):\n        sns.histplot(df[column], ax=ax )\n        sns.set(font_scale=1)","c07fdc21":"# Create distplot with curve_type set to 'normal'\ndef hist_plotly(df, col,bin_size=5,color = \"slategray\"):\n    colors = [color]\n    fig = ff.create_distplot([df[col].dropna()], [col],bin_size=bin_size,\n                             curve_type='kde', # override default 'kde'\n                             show_rug=False,\n                             colors=colors)\n\n    # Add title\n    fig.update_layout(title_text='{} - Distplot with Normal Distribution'.format(col.upper()) ,\n                  template= \"plotly_dark\" , \n                  xaxis = dict(title = col.upper()))\n    fig.show()","26d6292b":"hist_plotly(table_train, \"target\", bin_size = .01)","62e2db29":"plot_whole_features(table_train)","cdcf03b6":"table_train.corr(method = \"pearson\").style.background_gradient(cmap='Reds')","36057ed3":"# Compute the correlation matrix\ncorr = table_train.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(15, 12))\nax.title.set_text('Corr Matrix - Whole Features')\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\n_ = sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1.,vmin=-1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n","7e65b07a":"corr_featu=[\"cont1\", \"cont2\", \"cont3\", \"cont4\", \"cont14\" , \"cont8\", \"cont9\", \"cont12\" ]","862ea297":"from sklearn.preprocessing import PowerTransformer, PolynomialFeatures\nfrom sklearn.preprocessing import QuantileTransformer\n\ndef poly_augm(df, columns, encoder, suff= \"_poly\"):\n    df = df.join(pd.DataFrame(encoder.transform(df[columns]), columns = encoder.get_feature_names(columns) ).add_suffix(suff))\n    return df\n","6e73eeab":"table_train = table_train[table_train[\"target\"]>1]","7ff2bfb4":"poly = PolynomialFeatures(3,interaction_only = True)\npoly.fit(table_train[corr_featu])\n\ntable_train_agg = poly_augm(table_train, corr_featu, poly)\ntable_test_agg = poly_augm(table_test, corr_featu, poly)\n\ntable_train_agg.head()","3cb262a2":"# Train\ntrain_model_sc = table_train_agg.copy()\ntrain_label = train_model_sc[\"target\"]\ntrain_model_sc = train_model_sc.drop(columns = [\"target\"])\ncolumns = train_model_sc.columns\n# Test\ntest_model_sc = table_test_agg.copy()\n#Scaler\nqt = QuantileTransformer(n_quantiles=1000, random_state=0,output_distribution = \"normal\")\n# Scaling\ntrain_model_sc[columns] = qt.fit_transform(train_model_sc)\ntest_model_sc[columns]  = qt.transform(test_model_sc)\n\ntrain_model_sc.head()","cfd973b0":"from xgboost.sklearn import XGBRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV , RandomizedSearchCV\nfrom sklearn.metrics import roc_auc_score, classification_report\nfrom sklearn.metrics import r2_score , explained_variance_score , mean_absolute_error , mean_squared_error\nfrom IPython.display import display\nfrom lightgbm import LGBMRegressor\n\ndef regression_error(y_true, y_pred):\n    r2 = r2_score(y_true, y_pred )\n    evs = explained_variance_score(y_true, y_pred )\n    mae = mean_absolute_error(y_true, y_pred )\n    mse = mean_squared_error(y_true, y_pred )\n    data = [[r2, \"\"],[evs,\"\"] ,[ \"\", mae],[ \"\", mse]]\n    info = pd.DataFrame(data =data, index = ['R2','ExplaVarScore', 'MeanAbsErr', 'MeanSquaredErr'], columns =['Score', 'Error'])\n    display(info)\n    \n\nX_train, X_test, y_train, y_test = train_test_split(train_model_sc, train_label, train_size=0.8)","22a93af0":"tuned_parameters_xgb = [{'tree_method': ['gpu_hist'],'booster':['gbtree'], 'predictor':['gpu_predictor'], 'learning_rate':np.linspace(.05, 1, 20),'min_child_weight':np.linspace(0, 5, 50), 'n_estimators':list(range(50, 300, 20)), # 'subsample':[0.1, 0.5, 0.8,], 'n_estimators':[100,150], 'min_child_weight':[0.1, 0.5, 1.0,],\n                    'max_depth': list(range(3,15)), 'gamma': np.linspace(0,1.,20) }]\n\nscores = {'r2': 'r2', 'nmae':'neg_median_absolute_error' , 'nrmse': 'neg_root_mean_squared_error'}     \n# GridSearchCV\nclf = RandomizedSearchCV(XGBRegressor(), tuned_parameters_xgb,\n                   scoring=scores, \n                   refit= \"nrmse\",\n                   cv=7,\n                   verbose=2,\n                   n_jobs=2,\n                   n_iter= 40,)\n\nclf.fit(X_train, y_train)\n# Validation\npred = clf.best_estimator_.predict(X_test)\nprint(regression_error(y_test, pred ))\nprint(clf.best_params_)","ed32e27a":"tuned_parameters_lgb = [{'metric':['rmse'],'objective': ['regression'], 'learning_rate':np.linspace(.01, 1, 20),'min_gain_to_split':np.linspace(0, 5, 50), 'num_leaves':list(range(20, 200, 20)), # 'subsample':[0.1, 0.5, 0.8,], 'n_estimators':[100,150], 'min_child_weight':[0.1, 0.5, 1.0,],\n                    'max_depth': list(range(3,15)), 'max_bin': list(range(50,1000, 50)) }]\n\nscores_lgb = {'r2': 'r2', 'nmae':'neg_median_absolute_error' , 'nrmse': 'neg_root_mean_squared_error'}     \n# GridSearchCV\nclf_lgb = RandomizedSearchCV(LGBMRegressor(), tuned_parameters_lgb,\n                   scoring=scores, \n                   refit= \"nrmse\",\n                   cv=7,\n                   verbose=2,\n                   n_jobs=2,\n                   n_iter= 40,)\n\nclf_lgb.fit(X_train, y_train)\n# Validation\npred = clf_lgb.best_estimator_.predict(X_test)\nprint(regression_error(y_test, pred ))\nprint(clf_lgb.best_params_)","27681025":"clf_lgb.cv_results_","c070d042":"params_lgb = clf_lgb.best_params_ ","bd8069fa":"clf.cv_results_","b2753801":"params= clf.best_params_ ","8d94ad99":"model_lgb = LGBMRegressor(**params_lgb)\nmodel_lgb.fit(train_model_sc, train_label)","8412ea9b":"model = XGBRegressor(**params)\nmodel.fit(train_model_sc, train_label)","56cb902b":"# LGB\ny_pred     = model_lgb.predict(test_model_sc)\n\nencounter_IDs = pd.read_csv(testtablepath)[[\"id\"]].values\ndf_sub = {'id': encounter_IDs[:,0], 'target': y_pred}\ndf_predictions = pd.DataFrame.from_dict(df_sub).set_index(['id'])\ndf_predictions.to_csv('Predictions_final_lgb.csv')\n\ndf_predictions.head(10)","53ede893":"# XGB\ny_pred     = model.predict(test_model_sc)\n\nencounter_IDs = pd.read_csv(testtablepath)[[\"id\"]].values\ndf_sub = {'id': encounter_IDs[:,0], 'target': y_pred}\ndf_predictions = pd.DataFrame.from_dict(df_sub).set_index(['id'])\ndf_predictions.to_csv('Predictions_final.csv')\n\ndf_predictions.head(10)","ed3c8313":"from sklearn.tree import DecisionTreeRegressor   as DTR\nfrom sklearn.ensemble import StackingRegressor as SR\nmodel_sc = SR(estimators = [('xgb' ,model), ('lgb' ,model_lgb)],final_estimator= DTR(random_state=17,max_depth=4,criterion= \"mse\"),cv = 8 )\nmodel_sc.fit(X_train, y_train)\npred = model_sc.predict(X_test)\nprint(regression_error(y_test, pred ))","828fad78":"model_sc.fit(train_model_sc, train_label)","67a229e8":"# Staking Regressor\ny_pred     = model_sc.predict(test_model_sc)\n\nencounter_IDs = pd.read_csv(testtablepath)[[\"id\"]].values\ndf_sub = {'id': encounter_IDs[:,0], 'target': y_pred}\ndf_predictions = pd.DataFrame.from_dict(df_sub).set_index(['id'])\ndf_predictions.to_csv('Predictions_final_sc.csv')\n\ndf_predictions.head(10)","97153c67":"import shap\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test)\nshap.summary_plot(shap_values, X_test, plot_type=\"bar\",\n                  title = \"\")","39335619":"shap.summary_plot(shap_values, X_test)","0056f1e9":"<a id=\"ch99\"><\/a>\n\n## Submission File\n\nWell folks, this is it. Is this was useful for you, plesase give an upvote and let me a comment about how to improve this notebook. Thanks","bd30e10f":"## Test file overview","22914cbe":"### Feature Generation\n\nOf course, i've created  more features, i'd considered the most inner interactive features.\n\n[\"cont1\", \"cont6\", \"cont7\", \"cont8\", \"cont9\", \"cont10\", \"cont11\", \"cont12\" ]\n","375a6563":"### Open files","77890e64":"## Exploring Correlation Matrix\n\nFor a fast clean visualization about the relation between columns we can use our best friend, the correlation matrix.\n","9db0ccb0":"### Differentiate between Numerical and Categorical Data.\n\nWe see that that this data set only contain real float64 columns. So let's work with that.\n","3c2c4db0":"<a id=\"ch6\"><\/a>\n# CHAPTER 6. Model Data","aded39b1":"### Paths \ud83d\udcda","643064cc":"## Output Target Distribution\n1. we see that this output is very tricky, the behavior is not normalized and so many outliers.\n1. A bimodal behavior could be a problem for a good performance.","4d7f8520":"## Delete the outlier value in the dataset","ec62708f":"### Droping useless column\n","c392d8ff":"### XGB model Randomized Search","1085ac41":"### Scaling Data\n1. QuantileTransformer\n\nTransform features using quantiles information.\n\nThis method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme.\n\nThe transformation is applied on each feature independently. First an estimate of the cumulative distribution function of a feature is used to map the original values to a uniform distribution. The obtained values are then mapped to the desired output distribution using the associated quantile function. Features values of new\/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.","617cde1d":"## Stacking Regressor\nNow i going to stack bouth regressors using staking regressor","4f55333c":"For this competition, you will be predicting a continuous target based on a number of feature columns given in the data. All of the feature columns, cont1 - cont14 are continuous.\n\n## Files\n* **train.csv** - the training data with the target column\n* **test.csv** - the test set; you will be predicting the target for each row in this file\n* **sample_submission.csv** - a sample submission file in the correct format","5aabea59":"## Model Time","f55231eb":"<a id=\"ch4\"><\/a>\n# CHAPTER 4. Prepare data for consumption\n1. If we review the chapter before, we can't find any nan value, is a very idealistic situation, but is good for us.\n1. There is a feature which is useless, \"id\", so we need to drop it. The unique identifiers are just noise.\n1. We need to see what type of data there in our dataset","877eb983":"### Training - Model Optimization\n\nThis step will help us to chose the best hyperparams for our model.\nBe awere that this process is time computing expensive. so, take a rest and do anything else.\n\n#### **RandomizedSearchCV**\n\nRandomized search on hyper parameters.\n\nRandomizedSearchCV implements a \u201cfit\u201d and a \u201cscore\u201d method. It also implements \u201cscore_samples\u201d, \u201cpredict\u201d, \u201cpredict_proba\u201d, \u201cdecision_function\u201d, \u201ctransform\u201d and \u201cinverse_transform\u201d if they are implemented in the estimator used.\n\nThe parameters of the estimator used to apply these methods are optimized by cross-validated search over parameter settings.\n\nIn contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter.\n\nLet's look [xgboost classifier](https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#module-xgboost.sklearn)","a74cbfd9":"# Table of Contents\n1. [Chapter 1 - What is the problem?](#ch1)\n1. [Chapter 2 - Data Science Pipeline](#ch2)\n1. [Chapter 3 - Overview data](#ch3)\n1. [Chapter 4 - Prepare data for consumption](#ch4)\n1. [Chapter 5 - Exploratory Data Analysis](#ch5)\n1. [Chapter 6 - Model Data](#ch6)\n1. [Chapter 7 - What did we find about diabetes?](#ch7)\n1. [Submission File](#ch99)","a5f20930":"<center>\n    <img src= \"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/24673\/logos\/header.png\" width=\"600\">\n<\/center>\n<center>\n    <img src= \"https:\/\/storage.googleapis.com\/molten\/lava\/2019\/05\/48c835e7-machine-learning-gif.gif\" width=\"600\">\n<\/center>","08d2daea":"### LGBM model Randomized Search","b71514cd":"<a id=\"ch3\"><\/a>\n# CHAPTER 3. Overview Data","29b106b6":"\n<body>\n    \n<h1 style=\"color:Blue; font-family: Roboto, sans-serif; text-align:center; font-size:30px\">Practice your ML regression skills on this approachable dataset! \ud83d\udcda\ud83d\udcac<\/h1>\n    \n<\/body>\n","dd6130ba":"## Feature importance\n\nLet's see what can tell us our model. What features are relevant for the task.\n\nThis step will be implemented using SHAP, who use game theory for extract information about our model, we could\u2019ve used the feature importance calculations that come with XGBoost, but it\u2019s not accurate at all or even contradictory.\n\nIf you want to see more about SHAP, please [check the link. ](https:\/\/shap.readthedocs.io\/en\/latest\/example_notebooks\/tabular_examples\/tree_based_models\/Census%20income%20classification%20with%20XGBoost.html)","603ca515":"<a id=\"ch8\"><\/a>\n\n# Chapter 7 - What did we find about diabetes","5dd76e68":"<a id=\"ch2\"><\/a>\n# CHAPTER 2. Data Science Pipeline\n\n1. **Define the Problem:** We face a blind columns data, for a regression problem.\n\n2. **Gather the Data:** The challenge provided us with structured data, hence we can accomplish the problem like a [supervised learning](https:\/\/machinelearningmastery.com\/supervised-and-unsupervised-machine-learning-algorithms\/).\n\n3. **Prepare Data for Consumption:** This step could overlap exploratory data analysis EDA, while our exploration we'll need to clean, transform, reformat or scale our dataset, all this for extract useful information to clarify our problem and have healthy data, to avoid missing or outlier data points.\n\n4. **Perform Exploratory Analysis:** Without analysis our work could be useless, because \"garbage-in, garbage-out (GIGO)\". This is the step when we going to create our personal perspective of the problem but trying really hard to be objective, sometimes we have to use statistics tools to validate some hypothesis. All this analysis will be support by graphics aims to looking patterns, comparations and correlations. Also, we going to need to differentiate categorical vs numerical features, and balance of those. These aspects of the data are important to create the best hypothesis for our data model.\n\n5. **Model Data:** The data model that we'll chose determine the algorithms available for use. The algorithm is just a fancy tool completely useless in the wrong context. That\u2019s because data modeling came after EDA, first you understand the data, then you model it.\n\n6. **Validate and Implement Data Model:** After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](https:\/\/machinelearningmastery.com\/overfitting-and-underfitting-with-machine-learning-algorithms\/).\n\n7. **Optimize and Strategize:** After all these steps, we'll need to repeat it, creating different hypothesis and models, optimize our process aims to make better predictions.","531652cb":"## Final Training\nAfter run and succesfuly finished the RandomSearch, we need to make the last training with the whole data set, using the best hyperparameters.","2be69140":"## Train file overview\n\n1. Description table shows us a simple distribution behavior for every feature of the data.\n2. Missing rate values present on our dataset.\n3. Sample train table, only five rows.\n","9005411c":"<a id=\"ch1\"><\/a>\n# CHAPTER 1. What is the problem?\n\nTabular Playground Series - Jan 2021, is an entry project to show our data science skills to progress and learn about the field. The project demands us a regression approach to predict the label data, and yes, this is another supervised learning competition.","bc71f6bb":"<a id=\"ch5\"><\/a>\n# CHAPTER 5. Exploratory Data Analysis\n1. We can see that our data need to be normalize\n1. There are too many irregular behaviors that we need to correct on the preprocessing steps\n","770a5d92":"<h1 style=\"color:RED; text-align:left; font-family: 'Oswald', sans-serif \"> ABOUT: <\/h1>\n\n<h3 style=\"text-align:Left\">Hi everyone, I want to share with you all my data exploration, analysis and model selection about this topic, I've been scraching this dataset for a while, and result very interesting for start into data science field. I'm really looking forwards about your comments and recomendation for this project.\n<\/h3>\n\n\n<h2 style=\" color:Blue; text-align:center; font-family:'Lobster',\">Let's do it!<\/h2>","f096d842":"## Features"}}