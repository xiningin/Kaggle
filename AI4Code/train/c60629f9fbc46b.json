{"cell_type":{"fef95106":"code","7e5d7f32":"code","54f8409a":"code","6b149fb2":"code","09c1ff02":"code","10fac16e":"code","7cbfcb60":"code","d1056423":"code","b6e74638":"code","a37d0272":"code","72058127":"code","d29f12a8":"code","732793ad":"code","8b91903c":"code","53b215b0":"code","89525862":"code","1b985cc3":"code","8e570e74":"code","bef48ada":"code","875f8f1e":"code","92ebebd9":"code","c382e018":"code","6c67a799":"code","7aefbdd8":"code","faacc3bf":"code","e3b8f2cc":"code","d146d947":"code","9a711db7":"code","2eb34a2b":"code","5f6f7f6d":"code","0e257412":"code","7c9b7876":"code","18fe4241":"code","a8d2a7dc":"code","4cf24f1a":"code","9b8f69f0":"code","105320dc":"code","43aef84d":"code","dc604d98":"code","5cd6a217":"code","734436e9":"code","2f27570c":"code","19a19769":"code","3473dade":"code","a89ac5ec":"code","fd3cc51a":"code","e77bd556":"code","75446ebb":"code","ee212d2d":"code","5007dc76":"code","2c419fd5":"code","59981654":"code","f2e59346":"code","00d413b6":"code","234c2128":"code","a6440a21":"code","515980b7":"code","cb09bbba":"code","765a01c5":"code","61454347":"code","6ac22681":"code","4ec7f331":"code","532539db":"code","6d912434":"code","c0050560":"code","57951cc4":"code","44056689":"code","ad0e7c8e":"code","81719244":"code","759e04fc":"code","3cdfb5e6":"code","f2b98b34":"code","226c8647":"code","32eaa905":"code","b9863a99":"code","3995789b":"markdown","37ca6f7e":"markdown","c99327cf":"markdown","0235b980":"markdown","f13dcd8e":"markdown","ba907d16":"markdown","409867a7":"markdown","a0f93c73":"markdown","f9ae746e":"markdown","e65d86f9":"markdown","80e2f498":"markdown","d192eb7c":"markdown","be0f3227":"markdown","47515718":"markdown","5277e004":"markdown","93cd3dfc":"markdown","5e9d4bb3":"markdown","ccce9633":"markdown","fdc932bd":"markdown","b3b12be7":"markdown","0fb310f1":"markdown","9b037ca7":"markdown","d8e844bd":"markdown","35b39561":"markdown","42959ea4":"markdown","e6be97fb":"markdown","d06d4a7f":"markdown","d7e6a1ea":"markdown","e139c58f":"markdown","4201f5e5":"markdown"},"source":{"fef95106":"!pip install git+https:\/\/github.com\/scikit-optimize\/scikit-optimize.git\n!pip install -U skorch","7e5d7f32":"from copy import deepcopy\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom collections import OrderedDict\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import TransformedTargetRegressor\n\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\nimport torch\nfrom torch import nn\nimport skorch\nfrom skorch import NeuralNetRegressor\n\nfrom skopt import BayesSearchCV, gp_minimize\nfrom skopt.callbacks import DeltaYStopper, CheckpointSaver\nfrom skopt.space import Real, Categorical, Integer\n\nplt.rcParams['figure.constrained_layout.use'] = True\n","54f8409a":"from sklearn.datasets import load_boston\n\nboston_dataset = load_boston()\ndataset = pd.DataFrame(boston_dataset.data, columns = boston_dataset.feature_names)\ndataset['MEDV'] = boston_dataset.target","6b149fb2":"dataset.head(5)","09c1ff02":"dataset.shape","10fac16e":"dataset.isnull().sum()","7cbfcb60":"fig = plt.figure(figsize = (12, 8))\nhist_ranges = []\nfor i in range(dataset.shape[1]):\n    plt.subplot(3, 5, i + 1)\n    hist_ranges.append((dataset.iloc[:,i].min(), dataset.iloc[:,i].max()))\n    sns.histplot(dataset.iloc[:,i], binrange=hist_ranges[i], bins=30, kde=True)","d1056423":"sns.pairplot(dataset)","b6e74638":"corr = dataset.corr()","a37d0272":"fig, ax = plt.subplots(figsize=(13, 13))\nsns.heatmap(corr, cmap='RdBu', annot=True, fmt=\".2f\")\nplt.xticks(np.arange(len(corr.columns)) + 0.5, corr.columns);\n\nplt.yticks(np.arange(len(corr.columns))+0.5, corr.columns, rotation='horizontal');\nax.set_ylim(ax.get_ylim()[0]+ 0.5, ax.get_ylim()[1] - 0.5);  # \u0432\u043e\u0442 \u0442\u0430\u043a\u043e\u0439 \u0432\u043e\u0442 \u043a\u043e\u0441\u0442\u044b\u043b\u044c \u0434\u043b\u044f \u0442\u0435\u043a\u0443\u0449\u0435\u0439 matplotlib","72058127":"X = dataset.drop([\"MEDV\"], axis=1)\ny = dataset[\"MEDV\"]\nX[\"CHAS\"] = X[\"CHAS\"].astype(int)\n\nX_train, X_rem, y_train, y_rem = train_test_split(X, y, test_size = 0.2, random_state = 53042, \n                                                    stratify=pd.cut(X[\"LSTAT\"], labels=False, bins=25))\nX_val, X_test, y_val, y_test = train_test_split(X_rem, y_rem, test_size = 0.5, random_state = 42, \n                                                    stratify=pd.cut(X_rem[\"LSTAT\"], labels=False, bins=5))\n\ndel X, y, X_rem, y_rem\n\nprint(f\"Shape of X_train: {X_train.shape}\")\nprint(f\"Shape of X_test: {X_test.shape}\")\nprint(f\"Shape of y_train: {y_train.shape}\")\nprint(f\"Shape of y_test: {y_test.shape}\")","d29f12a8":"sns.histplot(y_train, binrange=hist_ranges[-1], bins=15, kde=True, stat=\"probability\")\nsns.histplot(y_val, binrange=hist_ranges[-1], bins=15, kde=True, stat=\"probability\", color=\"r\")\nsns.histplot(y_test, binrange=hist_ranges[-1], bins=15, kde=True, stat=\"probability\", color=\"g\")","732793ad":"fig = plt.figure(figsize = (12, 8))\nfor i in range(X_train.shape[1]):\n    plt.subplot(3, 5, i + 1)\n    sns.histplot(X_train.iloc[:,i], binrange=hist_ranges[i], bins=30, kde=True, stat='density', color='b')\n    sns.histplot(X_val.iloc[:,i], binrange=hist_ranges[i], bins=30, kde=True, stat='density', color='r')\n    sns.histplot(X_test.iloc[:,i], binrange=hist_ranges[i], bins=30, kde=True, stat='density', color='g')","8b91903c":"def optimizer(model_name, \n              model, \n              paramdict, \n              scoring='r2',  # 'neg_root_mean_squared_error', \n              n_iter=50, \n              fit_params={}, \n              X_train=X_train, y_train=y_train, \n              silent=False):\n    \n    checkpoint_saver = CheckpointSaver(f\".\/{model_name}_optimizer_checkpoint.pkl\", compress=9)\n\n    def status_print(arg):\n        print(f\"# {len(arg.func_vals)} : BEST R2 = {-arg.fun}. CUR R2 = {-arg.func_vals[-1]}\")\n        if arg.func_vals[-1] == arg.fun:\n            print(f\"best params = {list(zip(sorted(paramdict.keys()), arg.x))}\")\n    \n    search = BayesSearchCV(\n                    model,\n                    paramdict,\n                    cv=10,\n                    scoring=scoring,\n                    n_iter=n_iter\n                )\n    \n    search.fit(X_train, y_train, callback=[checkpoint_saver, status_print if not silent else None], **fit_params)\n    \n    if not silent:\n        print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n        print(search.best_params_, end=\"\\n\\n\")\n        \n    return search","53b215b0":"def train_baseline(model,\n                   X_train=X_train, y_train=y_train,\n                   silent=False,\n                   n_splits=5,\n                   random_state=42):\n    \n    cv_scores = {\"train_RMSE\": [],\n                 \"val_RMSE\": [],\n                 \"train_R2\": [],\n                 \"val_R2\": []}\n    cv_models = []\n    scores = {}\n\n    meta_feature = np.zeros(X_train.shape[0])  # oof cv pred (for further stacking)\n    y_train_pred_A = np.zeros((n_splits, X_train.shape[0]))\n    \n    cv = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n    \n    for i, (train_idx, val_idx) in enumerate(cv.split(X_train)):\n        if type(X_train) == np.ndarray:  # for pytorch nn \n            train, val = X_train[train_idx], X_train[val_idx]\n            train_tgt, val_tgt = y_train[train_idx], y_train[val_idx]\n        else:\n            train, val = X_train.iloc[train_idx,:], X_train.iloc[val_idx,:]\n            train_tgt, val_tgt = y_train.values[train_idx], y_train.values[val_idx]\n        \n        m = deepcopy(model)\n        m.fit(train, train_tgt)\n        cv_models.append(m)\n        \n        train_predict = m.predict(train)\n        val_predict = m.predict(val)\n        \n        y_train_pred_A[i][train_idx] = train_predict.reshape(-1)\n        meta_feature[val_idx] = val_predict.reshape(-1)\n        \n        cv_scores[\"train_R2\"].append(r2_score(train_tgt, train_predict))\n        cv_scores[\"val_R2\"].append(r2_score(val_tgt, val_predict))\n        cv_scores[\"train_RMSE\"].append(np.sqrt(mean_squared_error(train_tgt, train_predict)))\n        cv_scores[\"val_RMSE\"].append(np.sqrt(mean_squared_error(val_tgt, val_predict)))\n        \n    y_train_pred_A = y_train_pred_A.sum(0)\/(n_splits-1)\n    \n    model_ = deepcopy(model)\n    model_.fit(X_train, y_train)\n    y_train_pred_B = model_.predict(X_train).reshape(-1)\n    \n    for m in [\"R2\", \"RMSE\"]:\n        for ds in [\"train\", \"val\"]:\n            scores[f\"CV_{m}_{ds}_mean\"] = np.mean(cv_scores[f\"{ds}_{m}\"])\n            scores[f\"CV_{m}_{ds}_std\"] = np.std(cv_scores[f\"{ds}_{m}\"])\n    \n    scores[\"R2_train (A)\"] = r2_score(y_train, y_train_pred_A)\n    scores[\"RMSE_train (A)\"] = np.sqrt(mean_squared_error(y_train, y_train_pred_A))\n    scores[\"R2_train (B)\"] = r2_score(y_train, y_train_pred_B)\n    scores[\"RMSE_train (B)\"] = np.sqrt(mean_squared_error(y_train, y_train_pred_B))\n    \n    if not silent:\n        for m in [\"R2\", \"RMSE\"]:\n            for ds in [\"train\", \"val\"]:\n                print(f'CV_{m}_{ds}: mean - {scores[f\"CV_{m}_{ds}_mean\"]}, sd - {scores[f\"CV_{m}_{ds}_std\"]}')\n            \n        for k in scores:\n            if k[:3] != \"CV_\":\n                print(f\"{k}: {scores[k]}\")\n            \n    return {\n            \"model_A\": model_, \n            \"models_B\": cv_models, \n            \"preds\": {\n                \"y_train_pred_A\": y_train_pred_A,\n                \"y_train_pred_B\": y_train_pred_B,\n                \"meta_feature\": meta_feature\n            },\n            \"cv_scores\": cv_scores,\n            \"scores\": scores\n           }\n\ndef inference_baseline(model, X_val=X_val):\n    if isinstance(model, list):\n        y_val_pred = np.zeros((len(model), X_val.shape[0]))\n        for i, m in enumerate(model):\n            y_val_pred[i] = m.predict(X_val).reshape(-1)\n        y_val_pred = y_val_pred.mean(0)\n    else:\n        y_val_pred = model.predict(X_val).reshape(-1)\n    \n    return y_val_pred\n\ndef eval_baseline(model, \n                  X_val=X_val, y_val=y_val):\n    y_val_pred = inference_baseline(model, X_val)\n    \n    r2 = r2_score(y_val, y_val_pred)\n    rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n    \n    return {\"pred\": y_val_pred,\n            \"R2\": r2,\n            \"RMSE\": rmse}","89525862":"def plot(estimator_name):\n    f, axes = plt.subplots(2, 2, figsize=(10, 10))\n    \n    sns.scatterplot(x=baselines[estimator_name]['preds']['y_train_pred_A'],\n                    y=y_train,\n                    color='r', alpha=0.5,\n                    ax=axes[0, 0])\n    sns.scatterplot(x=baselines[estimator_name]['preds']['y_train_pred_B'],\n                    y=y_train,\n                    color='b', alpha=0.5,\n                    ax=axes[0, 0])\n    sns.scatterplot(x=baselines[estimator_name]['preds']['meta_feature'],\n                    y=y_train,\n                    ax=axes[0, 1])\n    sns.scatterplot(x=baselines[estimator_name]['preds']['y_train_pred_A'],\n                    y=baselines[estimator_name]['preds']['y_train_pred_B'],\n                    ax=axes[1, 0])\n    \n    axes[0, 0].set_title(\"y_train_pred_A, y_train_pred_B\")\n    axes[0, 1].set_title(\"meta_feature\")\n    axes[1, 0].set_title(\"y_train_pred_B \/ y_train_pred_A\")\n    \n    if 'inference' in baselines[estimator_name]:\n        sns.scatterplot(x=baselines[estimator_name]['inference']['y_val_A']['pred'],\n                        y=y_val,\n                        color='r', alpha=0.5,\n                        ax=axes[1, 1])\n        sns.scatterplot(x=baselines[estimator_name]['inference']['y_val_B']['pred'],\n                        y=y_val,\n                        color='b', alpha=0.5,\n                        ax=axes[1, 1])\n        axes[1, 1].set_title(\"y_val_A, y_val_B\")\n    else:\n        axes[1, 1].axis('off')","1b985cc3":"baselines = {}","8e570e74":"estimator_name = \"KNN\"\n\nestimator = Pipeline([\n    (\"scaler1\", StandardScaler()),\n    (\"model\", KNeighborsRegressor())\n])\n\n# optim = optimizer(estimator_name,\n#                   estimator, \n#                   {\n#                       \"model__n_neighbors\": Integer(1, 20),\n#                       \"model__weights\": Categorical([\"uniform\", \"distance\"]),\n#                       \"model__p\": Categorical([1,2])\n#                   })\n\n# best_params = optim.best_params_\n\nbest_params = OrderedDict([('model__n_neighbors', 3), ('model__p', 2), ('model__weights', 'distance')])\n\nestimator.set_params(**best_params);","bef48ada":"baselines[estimator_name] = train_baseline(estimator)\nbaselines[estimator_name][\"inference\"] = {\"y_val_A\": eval_baseline(baselines[estimator_name]['model_A']),\n                                          \"y_val_B\": eval_baseline(baselines[estimator_name]['models_B'])}\n\nfor x in \"AB\":\n    print(f\"R2 val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['R2']}\")\n    print(f\"RMSE val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['RMSE']}\")\n    \nplot(estimator_name)","875f8f1e":"estimator_name = \"LR\"\n\nestimator = Pipeline([\n    (\"scaler1\", StandardScaler()),\n    (\"scaler2\", PowerTransformer()),\n    (\"model\", TransformedTargetRegressor(\n        regressor=LinearRegression(),\n        transformer=Pipeline([\n                (\"ss\", StandardScaler()),\n                (\"pt\", PowerTransformer())\n            ]))\n    )\n])","92ebebd9":"baselines[estimator_name] = train_baseline(estimator)\nbaselines[estimator_name][\"inference\"] = {\"y_val_A\": eval_baseline(baselines[estimator_name]['model_A']),\n                                          \"y_val_B\": eval_baseline(baselines[estimator_name]['models_B'])}\n\nfor x in \"AB\":\n    print(f\"R2 val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['R2']}\")\n    print(f\"RMSE val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['RMSE']}\")\n    \nplot(estimator_name)","c382e018":"estimator_name = \"PolyR\"\n\nestimator = Pipeline([\n    (\"scaler1\", StandardScaler()),\n    (\"scaler2\", PowerTransformer()),\n    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n    (\"model\", TransformedTargetRegressor(\n        regressor=LinearRegression(), \n        transformer=PowerTransformer())\n    )\n])","6c67a799":"baselines[estimator_name] = train_baseline(estimator)\nbaselines[estimator_name][\"inference\"] = {\"y_val_A\": eval_baseline(baselines[estimator_name]['model_A']),\n                                          \"y_val_B\": eval_baseline(baselines[estimator_name]['models_B'])}\n\nfor x in \"AB\":\n    print(f\"R2 val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['R2']}\")\n    print(f\"RMSE val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['RMSE']}\")\n    \nplot(estimator_name)","7aefbdd8":"estimator_name = \"SGDRegressor\"\n\nestimator = Pipeline([\n    (\"scaler1\", StandardScaler()),\n    (\"scaler2\", PowerTransformer()),\n    (\"model\", TransformedTargetRegressor(\n        regressor=SGDRegressor(penalty='elasticnet'), \n        transformer=Pipeline([\n                (\"ss\", StandardScaler()),\n                (\"pt\", PowerTransformer())\n            ]))\n    )\n])\n\n# optim = optimizer(estimator_name,\n#                   estimator,\n#                   {\n#                       \"model__regressor__l1_ratio\": Real(1e-12, 0.999, \"log-uniform\"),\n#                       #\"model__regressor__alpha\": Real(1e-12, 0.999, \"log-uniform\")\n#                   })\n\n# best_params = optim.best_params_\n\nbest_params = OrderedDict([('model__regressor__l1_ratio', 1.2373093839020889e-05)])\n\nestimator.set_params(**best_params);","faacc3bf":"baselines[estimator_name] = train_baseline(estimator)\nbaselines[estimator_name][\"inference\"] = {\"y_val_A\": eval_baseline(baselines[estimator_name]['model_A']),\n                                          \"y_val_B\": eval_baseline(baselines[estimator_name]['models_B'])}\n\nfor x in \"AB\":\n    print(f\"R2 val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['R2']}\")\n    print(f\"RMSE val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['RMSE']}\")\n    \nplot(estimator_name)","e3b8f2cc":"estimator_name = \"Ridge\"\n\nestimator = Pipeline([\n    (\"scaler1\", StandardScaler()),\n    (\"scaler2\", PowerTransformer()),\n    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n    (\"model\", TransformedTargetRegressor(\n        regressor=Ridge(fit_intercept=True), \n        transformer=Pipeline([\n                (\"ss\", StandardScaler()),\n                (\"pt\", PowerTransformer())\n            ]))\n    )\n])\n\n# optim = optimizer(estimator_name,\n#                   estimator,\n#                   {\n#                       \"model__regressor__alpha\": Real(0.01, 100)\n#                   })\n\n# best_params = optim.best_params_\n\nbest_params = OrderedDict([('model__regressor__alpha', 4.012431322066786)])\n\nestimator.set_params(**best_params);","d146d947":"baselines[estimator_name] = train_baseline(estimator)\nbaselines[estimator_name][\"inference\"] = {\"y_val_A\": eval_baseline(baselines[estimator_name]['model_A']),\n                                          \"y_val_B\": eval_baseline(baselines[estimator_name]['models_B'])}\n\nfor x in \"AB\":\n    print(f\"R2 val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['R2']}\")\n    print(f\"RMSE val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['RMSE']}\")\n    \nplot(estimator_name)","9a711db7":"estimator_name = \"Lasso\"\n\nestimator = Pipeline([\n    (\"scaler1\", StandardScaler()),\n    (\"scaler2\", PowerTransformer()),\n    (\"poly\", PolynomialFeatures(degree=2)),\n    (\"model\", TransformedTargetRegressor(\n        regressor=Lasso(fit_intercept=True, max_iter=50000), \n        transformer=Pipeline([\n                (\"ss\", StandardScaler()),\n                (\"pt\", PowerTransformer())\n            ]))\n    )\n])\n\n# optim = optimizer(estimator_name,\n#                   estimator, \n#                   {\n#                       \"model__regressor__alpha\": Real(1e-9, 0.01)\n#                   }\n#                  )\n\n# best_params = optim.best_params_\n\nbest_params = OrderedDict([('model__regressor__alpha', 0.002493152301592987)])\n\nestimator.set_params(**best_params);","2eb34a2b":"baselines[estimator_name] = train_baseline(estimator)\nbaselines[estimator_name][\"inference\"] = {\"y_val_A\": eval_baseline(baselines[estimator_name]['model_A']),\n                                          \"y_val_B\": eval_baseline(baselines[estimator_name]['models_B'])}\n\nfor x in \"AB\":\n    print(f\"R2 val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['R2']}\")\n    print(f\"RMSE val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['RMSE']}\")\n    \nplot(estimator_name)","5f6f7f6d":"estimator_name = \"ElasticNet\"\n\nestimator = Pipeline([\n    (\"scaler1\", StandardScaler()),\n    (\"scaler2\", PowerTransformer()),\n    (\"poly\", PolynomialFeatures(degree=2)),\n    (\"model\", TransformedTargetRegressor(\n        regressor=ElasticNet(fit_intercept=True, max_iter=10000), \n        transformer=Pipeline([\n                (\"ss\", StandardScaler()),\n                (\"pt\", PowerTransformer())\n            ]))\n    )\n])\n\n# optim = optimizer(estimator_name,\n#                   estimator,\n#                   {\n#                       \"model__regressor__alpha\": Real(0.001, 0.01),\n#                       \"model__regressor__l1_ratio\" : Real(0.3, 1.)\n#                   })\n\n# best_params = optim.best_params_\n\nbest_params = OrderedDict([('model__regressor__alpha', 0.002709773209169479), \n                           ('model__regressor__l1_ratio', 0.86828906063699)])\n\nestimator.set_params(**best_params);","0e257412":"baselines[estimator_name] = train_baseline(estimator)\nbaselines[estimator_name][\"inference\"] = {\"y_val_A\": eval_baseline(baselines[estimator_name]['model_A']),\n                                          \"y_val_B\": eval_baseline(baselines[estimator_name]['models_B'])}\n\nfor x in \"AB\":\n    print(f\"R2 val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['R2']}\")\n    print(f\"RMSE val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['RMSE']}\")\n    \nplot(estimator_name)","7c9b7876":"estimator_name = \"SVR\"\n\nestimator = Pipeline([\n    (\"scaler1\", StandardScaler()),\n    (\"scaler2\", PowerTransformer()),\n    (\"model\", TransformedTargetRegressor(\n        regressor=SVR(kernel = 'rbf', gamma = 'scale'),\n        transformer=Pipeline([\n                (\"ss\", StandardScaler()),\n                (\"pt\", PowerTransformer())\n            ])))\n])","18fe4241":"baselines[estimator_name] = train_baseline(estimator)\nbaselines[estimator_name][\"inference\"] = {\"y_val_A\": eval_baseline(baselines[estimator_name]['model_A']),\n                                          \"y_val_B\": eval_baseline(baselines[estimator_name]['models_B'])}\n\nfor x in \"AB\":\n    print(f\"R2 val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['R2']}\")\n    print(f\"RMSE val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['RMSE']}\")\n    \nplot(estimator_name)","a8d2a7dc":"estimator_name = \"DTR\"\n\nestimator = Pipeline([\n    (\"model\", DecisionTreeRegressor(random_state = 0))\n])\n\n# optim = optimizer(estimator_name,\n#                   estimator,\n#                   {\n#                       \"model__max_depth\": Integer(1,10),\n#                       \"model__splitter\": Categorical([\"best\", \"random\"]),\n#                       \"model__min_samples_split\": Integer(2, 50)\n#                   })\n\n# best_params = optim.best_params_\n\nbest_params = OrderedDict([('model__max_depth', 9), \n                           ('model__min_samples_split', 26), \n                           ('model__splitter', 'best')])\n\nestimator.set_params(**best_params);","4cf24f1a":"baselines[estimator_name] = train_baseline(estimator)\nbaselines[estimator_name][\"inference\"] = {\"y_val_A\": eval_baseline(baselines[estimator_name]['model_A']),\n                                          \"y_val_B\": eval_baseline(baselines[estimator_name]['models_B'])}\n\nfor x in \"AB\":\n    print(f\"R2 val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['R2']}\")\n    print(f\"RMSE val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['RMSE']}\")\n    \nplot(estimator_name)","9b8f69f0":"estimator_name = \"RFR\"\n\nestimator = RandomForestRegressor(n_estimators=450, random_state=42)\n\n# optim = optimizer(estimator_name,\n#                   estimator,\n#                   {\n#                       \"max_depth\" : Integer(5, 25),\n#                       \"min_samples_split\": Integer(2, 50),\n#                       \"max_features\": Categorical([\"auto\", \"sqrt\", \"log2\"])\n#                   })\n\n# best_params = optim.best_params_\n\nbest_params = OrderedDict([('max_depth', 15), \n                           ('max_features', 'auto'), \n                           ('min_samples_split', 2)])\n\nestimator.set_params(**best_params);","105320dc":"baselines[estimator_name] = train_baseline(estimator)\nbaselines[estimator_name][\"inference\"] = {\"y_val_A\": eval_baseline(baselines[estimator_name]['model_A']),\n                                          \"y_val_B\": eval_baseline(baselines[estimator_name]['models_B'])}\n\nfor x in \"AB\":\n    print(f\"R2 val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['R2']}\")\n    print(f\"RMSE val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['RMSE']}\")\n    \nplot(estimator_name)","43aef84d":"estimator_name = \"CatBoost\"\n\nestimator = CatBoostRegressor(verbose=0, cat_features=[\"CHAS\"])\n\n# optim = optimizer(estimator_name,\n#                   estimator,\n#                   {\n#                       \"learning_rate\" : Real(0.01, 0.1),\n#                       \"l2_leaf_reg\": Real(1e-9, 100),\n#                       \"depth\": Integer(1,10)\n#                   },\n#                   fit_params={\"early_stopping_rounds\": 5}\n#                  )\n\n# best_params = optim.best_params_\n\nbest_params = OrderedDict([('depth', 6), \n                           ('l2_leaf_reg', 1e-09), \n                           ('learning_rate', 0.05899042070679496)])\n\nestimator.set_params(**best_params);","dc604d98":"baselines[estimator_name] = train_baseline(estimator)\nbaselines[estimator_name][\"inference\"] = {\"y_val_A\": eval_baseline(baselines[estimator_name]['model_A']),\n                                          \"y_val_B\": eval_baseline(baselines[estimator_name]['models_B'])}\n\nfor x in \"AB\":\n    print(f\"R2 val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['R2']}\")\n    print(f\"RMSE val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['RMSE']}\")\n    \nplot(estimator_name)","5cd6a217":"estimator_name = \"LGBMRegressor\"\n\nestimator = LGBMRegressor(verbose=-1)\n\n# optim = optimizer(estimator_name,\n#                   estimator,\n#                   {\n#                       \"max_depth\": Integer(1, 20),\n#                       \"learning_rate\": Real(0.0001, 0.5),\n#                       \"reg_lambda\": Real(1e-9, 100, \"log-uniform\")\n#                   })\n\n# best_params = optim.best_params_\n\nbest_params = OrderedDict([('learning_rate', 0.20239583541171516), \n                          ('max_depth', 17), \n                          ('reg_lambda', 4.209489608258535)])\n\nestimator.set_params(**best_params);","734436e9":"baselines[estimator_name] = train_baseline(estimator)\nbaselines[estimator_name][\"inference\"] = {\"y_val_A\": eval_baseline(baselines[estimator_name]['model_A']),\n                                          \"y_val_B\": eval_baseline(baselines[estimator_name]['models_B'])}\n\nfor x in \"AB\":\n    print(f\"R2 val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['R2']}\")\n    print(f\"RMSE val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['RMSE']}\")\n    \nplot(estimator_name)","2f27570c":"estimator_name = \"XGBRegressor\"\n\nestimator = XGBRegressor()\n\n# optim = optimizer(estimator_name,\n#                   estimator,\n#                   {\n#                       \"learning_rate\": Real(0.001, 0.3)\n#                   })\n\n# best_params = optim.best_params_\n\nbest_params = OrderedDict([('learning_rate', 0.0665169095826227)])\n\nestimator.set_params(**best_params);","19a19769":"baselines[estimator_name] = train_baseline(estimator)\nbaselines[estimator_name][\"inference\"] = {\"y_val_A\": eval_baseline(baselines[estimator_name]['model_A']),\n                                          \"y_val_B\": eval_baseline(baselines[estimator_name]['models_B'])}\n\nfor x in \"AB\":\n    print(f\"R2 val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['R2']}\")\n    print(f\"RMSE val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['RMSE']}\")\n    \nplot(estimator_name)","3473dade":"class BostonNet(nn.Module):\n    def __init__(self,\n                 num_units_1=13, drop_1=0.1,\n                 num_units_2=13, drop_2=0.1):\n        super(BostonNet,self).__init__()\n        \n        self.module = nn.Sequential(\n            nn.Linear(13, num_units_1),\n            nn.PReLU(),\n            nn.BatchNorm1d(num_units_1),\n            nn.Dropout(p=drop_1),\n            nn.Linear(num_units_1, num_units_2),\n            nn.PReLU(),\n            nn.BatchNorm1d(num_units_2),\n            nn.Dropout(p=drop_2),\n            nn.Linear(num_units_2, 1),\n        )\n        \n    def forward(self,X):\n        X = self.module(X)\n        return X","a89ac5ec":"ds_train = {\"X_train\": X_train.values.astype(np.float32), \"y_train\": y_train.values.reshape(-1,1).astype(np.float32)}\nds_val = {\"X_val\": X_val.values.astype(np.float32), \"y_val\": y_val.values.reshape(-1,1).astype(np.float32)}","fd3cc51a":"estimator_name = \"NNet\"\n\nestimator = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"model\", NeuralNetRegressor(\n            BostonNet, \n            criterion=nn.MSELoss,\n            max_epochs=50,\n            optimizer=torch.optim.Adam,\n            verbose=0,\n            #iterator_train__shuffle=True,\n            iterator_train__drop_last=True\n        )\n    )\n])\n\n# optim = optimizer(estimator_name,\n#                   estimator,\n#                   {\n#                       \"model__optimizer__lr\": Real(0.001, 0.1, \"log-uniform\"),\n#                       \"model__optimizer__weight_decay\": Real(0., 0.25),\n#                       \"model__batch_size\": Integer(2, 500),\n#                       \"model__module__num_units_1\": Integer(5,2000),\n#                       \"model__module__drop_1\": Real(0., 0.75),\n#                       \"model__module__num_units_2\": Integer(5,2000),\n#                       \"model__module__drop_2\": Real(0., 0.75)\n#                   },\n#                   **ds_train,\n#                   n_iter=250)\n\n# best_params = optim.best_params_\n\nbest_params = OrderedDict([(\"model__batch_size\", 75), \n                           (\"model__module__drop_1\", 0.2829704436470256), \n                           (\"model__module__drop_2\", 0.0), \n                           (\"model__module__num_units_1\", 1000), \n                           (\"model__module__num_units_2\", 103), \n                           (\"model__optimizer__lr\", 0.08153514486408545), \n                           (\"model__optimizer__weight_decay\", 0.0)])\n\nestimator.set_params(**best_params);\nestimator.set_params(**{\"model__verbose\": 0, \"model__max_epochs\": 50});","e77bd556":"baselines[estimator_name] = train_baseline(estimator, **ds_train)\nbaselines[estimator_name][\"inference\"] = {\"y_val_A\": eval_baseline(baselines[estimator_name]['model_A'], **ds_val),\n                                          \"y_val_B\": eval_baseline(baselines[estimator_name]['models_B'], **ds_val)}\n\nfor x in \"AB\":\n    print(f\"R2 val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['R2']}\")\n    print(f\"RMSE val {x}: {baselines[estimator_name]['inference'][f'y_val_{x}']['RMSE']}\")\n    \nplot(estimator_name)","75446ebb":"baselines_scores = {}\nfor est in baselines:\n    baselines_scores[est] = baselines[est][\"scores\"]\n    \n    baselines_scores[est][\"R2_val (A)\"] = baselines[est][\"inference\"][\"y_val_A\"][\"R2\"]\n    baselines_scores[est][\"RMSE_val (A)\"] = baselines[est][\"inference\"][\"y_val_A\"][\"RMSE\"]\n    baselines_scores[est][\"R2_val (B)\"] = baselines[est][\"inference\"][\"y_val_B\"][\"R2\"]\n    baselines_scores[est][\"RMSE_val (B)\"] = baselines[est][\"inference\"][\"y_val_B\"][\"RMSE\"]","ee212d2d":"results_df = pd.DataFrame(baselines_scores).T\nresults_df.reset_index(inplace=True)\nresults_df.rename(columns={\"index\": \"model\"}, inplace=True)\n\n\ncolumns = ['model', \n           'CV_R2_train_mean', 'CV_R2_val_mean', \n           'CV_RMSE_train_mean', 'CV_RMSE_val_mean', \n           'R2_train (A)', 'RMSE_train (A)', \n           'R2_train (B)', 'RMSE_train (B)',\n           'R2_val (A)', 'RMSE_val (A)',\n           'R2_val (B)', 'RMSE_val (B)']\nresults_df[columns].style.background_gradient(subset=[\"CV_R2_val_mean\", \"R2_val (A)\", \"R2_val (B)\"])\\\n    .background_gradient(subset=[\"RMSE_val (A)\", \"RMSE_val (B)\", \"CV_RMSE_val_mean\"], cmap=\"PuBu_r\",)  # (subset=results_df.columns[1:], color=\"green\")","5007dc76":"f, axe = plt.subplots(1,1, figsize=(12,4))\n\nvalues = results_df[\"CV_R2_val_mean\"].values\nsns.barplot(x='CV_R2_val_mean', \n            y='model', \n            data = results_df, \n            palette=['grey' if x < max(values) else \"red\" for x in values], ax = axe)\n\naxe.set_xlabel('CV_R2_val_mean', size=16)\naxe.set_ylabel('Model', size=16)\naxe.set_xlim(0.6, 1.0)\nplt.show()","2c419fd5":"f, axes = plt.subplots(1, 4, figsize=(18,4))\n\nfor i, r2 in enumerate([\"R2_train (A)\", \"R2_train (B)\", \"R2_val (A)\", \"R2_val (B)\"]):\n    values = results_df[r2].values\n    sns.barplot(x=r2, y='model', data = results_df, palette=['grey' if x < max(values) else \"red\" for x in values], ax = axes[i])\n    axes[i].set_xlabel(r2, size=16)\n    axes[i].set_ylabel('Model')\n    axes[i].set_xlim(0.55,1.01)\n\nplt.show()","59981654":"f, axe = plt.subplots(1,1, figsize=(12,8))\nsns.barplot(x='value', \n            y='model', \n            hue='variable', \n            data=results_df[[\"model\", \"R2_train (A)\", \"R2_train (B)\", \"R2_val (A)\", \"R2_val (B)\"]].melt(id_vars=\"model\"), \n            palette=['c', 'b','r','g'], alpha=0.75,\n            ax=axe)\naxe.set_xlabel(\"R2 Score\", size=16)\naxe.set_ylabel('Model')\naxe.set_xlim(0.55,1.01);","f2e59346":"f, axe = plt.subplots(1,2, figsize=(14, 7))\n\ndef label_point(x, y, val, ax):\n    a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)\n    for i, point in a.iterrows():\n        ax.text(point['x'], point['y'], str(point['val']))\n        \nfor i, x in enumerate('AB'):\n    sns.scatterplot(x=f'R2_train ({x})', y=f'R2_val ({x})', data=results_df, size = 16, ax=axe[i])\n    sns.lineplot(x=[0.7,1], y=[0.7,1], marker='', size=0.1, ax=axe[i])\n\n\n    label_point(results_df[f\"R2_train ({x})\"], results_df[f'R2_val ({x})'], results_df.model, axe[i]);\n    #axe[i].legend([],[], frameon=False);\n    axe[i].set_xlim(0.7, 1)\n    axe[i].set_ylim(0.7, 1)","00d413b6":"f, axe = plt.subplots(1,1, figsize=(12,8))\n\nresults_df.sort_values(by=['RMSE_val (A)'], ascending=False, inplace=True)\nsns.barplot(y='RMSE_val (A)', x='model', data = results_df, palette='Reds_d', ax = axe)\n\naxe.set_xlabel('RMSE_val (A)', size=16)\naxe.set_ylabel('Model', size=16)\n#axe.set_xlim(0.6, 1.0)\nplt.show()","234c2128":"f, axe = plt.subplots(1,1, figsize=(12,8))\n\nresults_df.sort_values(by=['RMSE_val (B)'], ascending=False, inplace=True)\nsns.barplot(y='RMSE_val (B)', x='model', data = results_df, palette='Reds_d', ax = axe)\n\naxe.set_xlabel('RMSE_val (B)', size=16)\naxe.set_ylabel('Model', size=16)\n#axe.set_xlim(0.6, 1.0)\nplt.show()","a6440a21":"models_ranged_A = results_df.sort_values(by=['RMSE_val (A)'])[\"model\"].to_list()\nmodels_ranged_B = results_df.sort_values(by=['RMSE_val (B)'])[\"model\"].to_list()\nsingle_mpdels = results_df['model'].to_list()","515980b7":"df_val_A, df_val_B, df_meta, df_test_A, df_test_B = (pd.DataFrame() for _ in range(5))\n\nfor k in baselines:\n    df_val_A[k] = baselines[k][\"inference\"][\"y_val_A\"][\"pred\"]\n    df_val_B[k] = baselines[k][\"inference\"][\"y_val_B\"][\"pred\"]\n    df_meta[k] = baselines[k][\"preds\"][\"meta_feature\"]\n    \n    if k == \"NNet\":\n        df_test_A[k] = inference_baseline(baselines[k]['model_A'], X_test.values.astype(np.float32))\n        df_test_B[k] = inference_baseline(baselines[k]['models_B'], X_test.values.astype(np.float32))\n    else:\n        df_test_A[k] = inference_baseline(baselines[k]['model_A'], X_test)\n        df_test_B[k] = inference_baseline(baselines[k]['models_B'], X_test)","cb09bbba":"rmse_scores = results_df[['model', 'RMSE_val (A)', 'RMSE_val (B)']]","765a01c5":"pd.options.mode.chained_assignment = None  # default='warn'\n\nrmse_scores['RMSE_test (A)'] = rmse_scores['model'].apply(lambda x: np.sqrt(mean_squared_error(y_test, df_test_A[x])))\nrmse_scores['RMSE_test (B)'] = rmse_scores['model'].apply(lambda x: np.sqrt(mean_squared_error(y_test, df_test_B[x])))","61454347":"rmse_scores","6ac22681":"mins = rmse_scores.min(0).to_dict()\nmins","4ec7f331":"f, axe = plt.subplots(1,1, figsize=(12,8))\n\ndf = pd.melt(rmse_scores[['model', 'RMSE_val (A)', 'RMSE_test (A)']], id_vars=['model'])\n\nsns.barplot(y='value', x='model', hue='variable', data=df, palette='magma', ax = axe)\nsns.lineplot(x=[-1, rmse_scores.shape[0] + 1], y=[mins[\"RMSE_val (A)\"]]*2, linestyle='--')\nsns.lineplot(x=[-1, rmse_scores.shape[0] + 1], y=[mins[\"RMSE_test (A)\"]]*2, linestyle='--')","532539db":"f, axe = plt.subplots(1,1, figsize=(12,8))\n\ndf = pd.melt(rmse_scores[['model', 'RMSE_val (B)', 'RMSE_test (B)']], id_vars=['model'])\n\nsns.barplot(y='value', x='model', hue='variable', data=df, palette='magma', ax = axe)\nsns.lineplot(x=[-1, rmse_scores.shape[0] + 1], y=[mins[\"RMSE_val (B)\"]]*2, linestyle='--')\nsns.lineplot(x=[-1, rmse_scores.shape[0] + 1], y=[mins[\"RMSE_test (B)\"]]*2, linestyle='--')","6d912434":"rmse_scores = rmse_scores.append({'model': 's_aver',\n                                  'RMSE_val (A)': np.sqrt(mean_squared_error(y_val, df_val_A.mean(1))),\n                                  'RMSE_val (B)': np.sqrt(mean_squared_error(y_val, df_val_B.mean(1))),\n                                  'RMSE_test (A)': np.sqrt(mean_squared_error(y_test, df_test_A.mean(1))),\n                                  'RMSE_test (B)': np.sqrt(mean_squared_error(y_test, df_test_B.mean(1)))}, ignore_index=True)\nrmse_scores[rmse_scores['model'] == 's_aver']","c0050560":"greedy_aver_result = {'model': 'greedy_aver'}\n\nfor x in {'A': (df_val_A, df_test_A), 'B': (df_val_B, df_test_B)}.items():\n    models_ranged = results_df.sort_values(by=[f\"RMSE_val ({x[0]})\"], ascending=False)[\"model\"].to_list()\n\n    best_ensemble = [models_ranged.pop()]\n    best_val_score = baselines[best_ensemble[0]][\"inference\"][f\"y_val_{x[0]}\"][\"RMSE\"]\n\n    while len(models_ranged) > 0:\n        candidate = None\n        for i in range(len(models_ranged)):\n            rmse = np.sqrt(mean_squared_error(y_val, x[1][0][best_ensemble + [models_ranged[i]]].mean(1)))\n            if rmse < best_val_score:\n                candidate = i\n                best_val_score = rmse\n        if candidate is None:\n            break\n        best_ensemble += [models_ranged.pop(candidate)]\n    \n    greedy_aver_result[f\"RMSE_val ({x[0]})\"] = np.sqrt(mean_squared_error(y_val, x[1][0][best_ensemble].mean(1)))\n    greedy_aver_result[f\"RMSE_test ({x[0]})\"] = np.sqrt(mean_squared_error(y_test, x[1][1][best_ensemble].mean(1)))\n    \n    print(x[0], best_ensemble, best_val_score)","57951cc4":"rmse_scores = rmse_scores.append(greedy_aver_result, ignore_index=True)\n\nrmse_scores[rmse_scores['model'] == 'greedy_aver']","44056689":"def f(x):\n    return -r2_score(y_val, (df_val_A * (x + [1. - sum(x)])).sum(1))\n\nres = gp_minimize(f,  [(0., 1.)] * (df_val_A.shape[1]-1), n_calls=50, n_random_starts=5, random_state=1234,\n                 callback=None  # lambda x: print(-x.fun)\n                 )\n\nw_A = res.x + [1. - sum(res.x)]\n\ndef f(x):\n    return -r2_score(y_val, (df_val_B * (x + [1. - sum(x)])).sum(1))\n\nres = gp_minimize(f,  [(0., 1.)] * (df_val_B.shape[1]-1), n_calls=50, n_random_starts=5, random_state=1234,\n                 callback=None  # lambda x: print(-x.fun)\n                 )\nw_B = res.x + [1. - sum(res.x)]\n\nw_A, w_B","ad0e7c8e":"# w_A = [0.11752863701239541, 0.0, 0.0, 0.0, 0.0,\n#        0.00228299168724918, 0.0, 0.0, 0.0, 0.0886471038536811,\n#        0.196736870858699, 0.0, 0.3555913645053595, 0.2392130320826158]\n\n# w_B = [0.0, 0.0, 0.08154069202274569, 0.0, 0.0,\n#        0.0, 0.0, 0.0, 0.0, 0.0,\n#        0.0, 0.0, 0.5139895337390048, 0.4044697742382495]","81719244":"bayes_aver_result = {'model': 'bayes_aver',\n                     'RMSE_val (A)': np.sqrt(mean_squared_error(y_val, (df_val_A * w_A).sum(1))),\n                     'RMSE_val (B)': np.sqrt(mean_squared_error(y_val, (df_val_B * w_B).sum(1))),\n                     'RMSE_test (A)': np.sqrt(mean_squared_error(y_test, (df_test_A * w_A).sum(1))),\n                     'RMSE_test (B)': np.sqrt(mean_squared_error(y_test, (df_test_B * w_B).sum(1)))}\n\nrmse_scores = rmse_scores.append(bayes_aver_result, ignore_index=True)\n\nrmse_scores[rmse_scores['model'] == 'bayes_aver']","759e04fc":"# significant weights\n\nw_A_ = [w if w > 0.05 else 0 for w in w_A]\nw_B_ = [w if w > 0.05 else 0 for w in w_B]\n\nbayes_aver_result = {'model': 'bayes_aver_sw',\n                     'RMSE_val (A)': np.sqrt(mean_squared_error(y_val, (df_val_A * w_A_ \/ sum(w_A_)).sum(1))),\n                     'RMSE_val (B)': np.sqrt(mean_squared_error(y_val, (df_val_B * w_B_ \/ sum(w_B_)).sum(1))),\n                     'RMSE_test (A)': np.sqrt(mean_squared_error(y_test, (df_test_A * w_A_ \/ sum(w_A_)).sum(1))),\n                     'RMSE_test (B)': np.sqrt(mean_squared_error(y_test, (df_test_B * w_B_ \/ sum(w_B_)).sum(1)))}\n\nrmse_scores = rmse_scores.append(bayes_aver_result, ignore_index=True)\n\nrmse_scores[rmse_scores['model'] == 'bayes_aver_sw']","3cdfb5e6":"stacking_estimator = ElasticNet()\n\nstacking_estimator.fit(df_meta, y_train)\n\nrmse_scores = rmse_scores.append({'model': 'stacking_all',\n                                  'RMSE_val (A)': np.sqrt(mean_squared_error(y_val, stacking_estimator.predict(df_val_A))),\n                                  'RMSE_val (B)': np.sqrt(mean_squared_error(y_val, stacking_estimator.predict(df_val_B))), \n                                  'RMSE_test (A)': np.sqrt(mean_squared_error(y_test, stacking_estimator.predict(df_test_A))), \n                                  'RMSE_test (B)': np.sqrt(mean_squared_error(y_test, stacking_estimator.predict(df_test_B)))}, \n                                 ignore_index=True)\nrmse_scores[rmse_scores['model'] == 'stacking_all']","f2b98b34":"stacking_estimator = ElasticNet()\n\ngreedy_stacking_result = {'model': 'greedy_stacking'}\n\nfor x in {'A': (df_val_A, df_test_A), 'B': (df_val_B, df_test_B)}.items():\n    models_ranged = results_df.sort_values(by=[f\"RMSE_val ({x[0]})\"], ascending=False)[\"model\"].to_list()\n\n    best_ensemble = [models_ranged.pop()]\n    stacking_estimator.fit(df_meta[best_ensemble], y_train)\n    best_val_score = np.sqrt(mean_squared_error(y_val, stacking_estimator.predict(x[1][0][best_ensemble])))\n\n    while len(models_ranged) > 0:\n        candidate = None\n        for i in range(len(models_ranged)):\n            stacking_estimator.fit(df_meta[best_ensemble + [models_ranged[i]]], y_train)\n            rmse = np.sqrt(mean_squared_error(y_val, stacking_estimator.predict(x[1][0][best_ensemble + [models_ranged[i]]])))\n            if rmse < best_val_score:\n                candidate = i\n                best_val_score = rmse\n        if candidate is None:\n            break\n        best_ensemble += [models_ranged.pop(candidate)]\n\n    stacking_estimator.fit(df_meta[best_ensemble], y_train)\n    \n    greedy_stacking_result[f\"RMSE_val ({x[0]})\"] = np.sqrt(mean_squared_error(y_val, stacking_estimator.predict(x[1][0][best_ensemble])))\n    greedy_stacking_result[f\"RMSE_test ({x[0]})\"] = np.sqrt(mean_squared_error(y_test, stacking_estimator.predict(x[1][1][best_ensemble])))\n    \n    print(x[0], best_ensemble, best_val_score)","226c8647":"rmse_scores = rmse_scores.append(greedy_stacking_result, ignore_index=True)\n\nrmse_scores[rmse_scores['model'] == 'greedy_stacking']","32eaa905":"f, axe = plt.subplots(1,1, figsize=(12,8))\n\ndf = pd.melt(rmse_scores[len(single_mpdels):][['model', 'RMSE_val (A)', 'RMSE_test (A)']], id_vars=['model'])\n\nsns.barplot(y='value', x='model', hue='variable', data=df, palette='magma', ax = axe)\nsns.lineplot(x=[-1, rmse_scores.shape[0] + 1 - len(single_mpdels)], y=[mins[\"RMSE_val (A)\"]]*2, linestyle='--')\nsns.lineplot(x=[-1, rmse_scores.shape[0] + 1 - len(single_mpdels)], y=[mins[\"RMSE_test (A)\"]]*2, linestyle='--')","b9863a99":"f, axe = plt.subplots(1,1, figsize=(12,8))\n\ndf = pd.melt(rmse_scores[len(single_mpdels):][['model', 'RMSE_val (B)', 'RMSE_test (B)']], id_vars=['model'])\n\nsns.barplot(y='value', x='model', hue='variable', data=df, palette='magma', ax = axe)\nsns.lineplot(x=[-1, rmse_scores.shape[0] + 1 - len(single_mpdels)], y=[mins[\"RMSE_val (B)\"]]*2, linestyle='--')\nsns.lineplot(x=[-1, rmse_scores.shape[0] + 1 - len(single_mpdels)], y=[mins[\"RMSE_test (B)\"]]*2, linestyle='--')","3995789b":"# <FONT color='red'>Support Vector Regression<\/FONT> ","37ca6f7e":"# <FONT color='red'>NNET<\/FONT>","c99327cf":"## Compare ensambles","0235b980":"## 7. Stacking and blending\n\nThis dataset is too small to get a stable solution. Thus, Stacking and blending are given as an simple example.","f13dcd8e":"# <FONT color='red'>Linear Regression<\/FONT>","ba907d16":"### simple averaging","409867a7":"## 3. Load Dataset","a0f93c73":"# <FONT color='red'>XGboost<\/FONT>\nfrom box","f9ae746e":"# <FONT color='red'>Decision Tree Regression<\/FONT>","e65d86f9":"## 6. Compare single models\n","80e2f498":"# <FONT color='red'>LGBMRegressor<\/FONT>\nfrom box","d192eb7c":"# <FONT color='red'>CatBoost<\/FONT> \nfrom box","be0f3227":"A - one model trained on the entire training dataset  \nB - several models trained in the cross validation process","47515718":"### 'Greedy' Stacking (ElasticNet) ","5277e004":"# <FONT color='red'>KNN<\/FONT>","93cd3dfc":"### Stacking (ElasticNet)","5e9d4bb3":"# <FONT color='red'>Ridge Regression<\/FONT>","ccce9633":"# <FONT color='red'>Lasso Regression<\/FONT>","fdc932bd":"# Boston house prices dataset  \n## 1. Overview \n***Number of Instances***: 506  \n***Number of Attributes***: 13 numeric\/categorical predictive. Median Value (attribute 14) is usually the target.  \n\n<style>\ntable th:first-of-type {\n    width: 20%;\n}\ntable th:nth-of-type(2) {\n    width: 40%;\n}\ntable th:nth-of-type(3) {\n    width: 40%;\n}\n<\/style>\n\n|column|description|rus|\n|-|-|-|\n|CRIM|Per capita crime rate by town|\u0423\u0440\u043e\u0432\u0435\u043d\u044c \u043f\u0440\u0435\u0441\u0442\u0443\u043f\u043d\u043e\u0441\u0442\u0438 \u043d\u0430 \u0434\u0443\u0448\u0443 \u043d\u0430\u0441\u0435\u043b\u0435\u043d\u0438\u044f \u043f\u043e \u0433\u043e\u0440\u043e\u0434\u0430\u043c|\n|ZN|Proportion of residential land zoned for lots over 25,000 sq. ft|\u0414\u043e\u043b\u044f \u0437\u0435\u043c\u0435\u043b\u044c \u043f\u043e\u0434 \u0436\u0438\u043b\u0443\u044e \u0437\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0443, \u0437\u043e\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u043f\u043e\u0434 \u0443\u0447\u0430\u0441\u0442\u043a\u0438 \u043f\u043b\u043e\u0449\u0430\u0434\u044c\u044e \u0431\u043e\u043b\u0435\u0435 25 000 \u043a\u0432. \u0424\u0443\u0442\u043e\u0432|\n|INDUS|Proportion of non-retail business acres per town|\u0414\u043e\u043b\u044f \u0430\u043a\u0440\u043e\u0432, \u043d\u0435 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u0441 \u0440\u043e\u0437\u043d\u0438\u0447\u043d\u043e\u0439 \u0442\u043e\u0440\u0433\u043e\u0432\u043b\u0435\u0439, \u043d\u0430 \u0433\u043e\u0440\u043e\u0434|\n|CHAS|Charles River dummy variable (1 if tract bounds river; 0 otherwise)|\u0424\u0438\u043a\u0442\u0438\u0432\u043d\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f (1, \u0435\u0441\u043b\u0438 \u0443\u0447\u0430\u0441\u0442\u043e\u043a \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0438\u0432\u0430\u0435\u0442 \u0440\u0435\u043a\u0443; 0 \u0432 \u043f\u0440\u043e\u0442\u0438\u0432\u043d\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435)|\n|NOX|Nitric oxide concentration (parts per 10 million)|\u041a\u043e\u043d\u0446\u0435\u043d\u0442\u0440\u0430\u0446\u0438\u044f \u043e\u043a\u0441\u0438\u0434\u0430 \u0430\u0437\u043e\u0442\u0430 (\u0447\u0430\u0441\u0442\u0435\u0439 \u043d\u0430 10 \u043c\u0438\u043b\u043b\u0438\u043e\u043d\u043e\u0432)|\n|RM|Average number of rooms per dwelling|\u0421\u0440\u0435\u0434\u043d\u0435\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043e\u043c\u043d\u0430\u0442 \u0432 \u0434\u043e\u043c\u0435|\n|AGE|Proportion of owner-occupied units built prior to 1940|\u0414\u043e\u043b\u044f \u0436\u0438\u043b\u044b\u0445 \u0434\u043e\u043c\u043e\u0432, \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u044b\u0445 \u0434\u043e 1940 \u0433.|\n|DIS|Weighted distances to five Boston employment centers|\u0412\u0437\u0432\u0435\u0448\u0435\u043d\u043d\u044b\u0435 \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u044f \u0434\u043e \u043f\u044f\u0442\u0438 \u0431\u043e\u0441\u0442\u043e\u043d\u0441\u043a\u0438\u0445 \u0446\u0435\u043d\u0442\u0440\u043e\u0432 \u0437\u0430\u043d\u044f\u0442\u043e\u0441\u0442\u0438|\n|RAD|Index of accessibility to radial highways|\u0418\u043d\u0434\u0435\u043a\u0441 \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u043e\u0441\u0442\u0438 \u0440\u0430\u0434\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u043c\u0430\u0433\u0438\u0441\u0442\u0440\u0430\u043b\u0435\u0439|\n|TAX|full-value property-tax rate per \\10,000|\u041f\u043e\u043b\u043d\u0430\u044f \u0441\u0442\u0430\u0432\u043a\u0430 \u043d\u0430\u043b\u043e\u0433\u0430 \u043d\u0430 \u0438\u043c\u0443\u0449\u0435\u0441\u0442\u0432\u043e (\u0432 10 000 \u0434\u043e\u043b\u043b\u0430\u0440\u043e\u0432 \u0421\u0428\u0410)|\n|PTRATIO|Pupil-teacher ratio by town|\u0421\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u0443\u0447\u0435\u043d\u0438\u043a\u043e\u0432 \u0438 \u0443\u0447\u0438\u0442\u0435\u043b\u0435\u0439 \u043f\u043e \u0433\u043e\u0440\u043e\u0434\u0430\u043c|\n|B|1000(Bk \u2014 0.63)\u00b2, where Bk is the proportion of [people of African American descent] by town|1000 (Bk - 0,63) \u00b2, \u0433\u0434\u0435 Bk - \u0434\u043e\u043b\u044f [\u043b\u0438\u0446 \u0430\u0444\u0440\u043e\u0430\u043c\u0435\u0440\u0438\u043a\u0430\u043d\u0441\u043a\u043e\u0433\u043e \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u044f] \u043f\u043e \u0433\u043e\u0440\u043e\u0434\u0430\u043c.|\n|LSTAT|Percentage of lower status of the population|\u041f\u0440\u043e\u0446\u0435\u043d\u0442 \u043d\u0438\u0437\u0448\u0435\u0433\u043e \u0441\u0442\u0430\u0442\u0443\u0441\u0430 \u043d\u0430\u0441\u0435\u043b\u0435\u043d\u0438\u044f|\n|**MEDV**|Median value of owner-occupied homes in $1000s|\u0421\u0440\u0435\u0434\u043d\u044f\u044f \u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u044c \u0434\u043e\u043c\u043e\u0432, \u0437\u0430\u043d\u0438\u043c\u0430\u0435\u043c\u044b\u0445 \u0432\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u0430\u043c\u0438 (\u0432 1000 \u0434\u043e\u043b\u043b\u0430\u0440\u043e\u0432 \u0421\u0428\u0410).|\n\n<FONT color='red'>MEDV<\/FONT> - target feature\n\n***Missing Attribute Values***: None  \n***Creator***: Harrison, D. and Rubinfeld, D.L.\n\nThis is a copy of UCI ML housing dataset.  \n[https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/housing\/](https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/housing\/)\n\n\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\nprices and the demand for clean air', J. Environ. Economics & Management,\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\npages 244-261 of the latter.\n\nThe Boston house-price data has been used in many machine learning papers that address regression\nproblems.   \n\n***References***\n\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n\n","b3b12be7":"## 5. Weak baselines","0fb310f1":"## 4. Analysis","9b037ca7":"\u041d\u0430\u0438\u0431\u043e\u043b\u044c\u0448\u0430\u044f \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u044f \u043c\u0435\u0436\u0434\u0443 \u0444\u0438\u0447\u0430\u043c\u0438: RAD-TAX, DIS - (NOX, AGE, INDUS), NOX - INDUS, LSTAT - MEDV, RM - MEDV\n","d8e844bd":"# <FONT color='red'>Random Forest Regression<\/FONT>","35b39561":"## 2. Imports","42959ea4":"# Conclusion\n\nThe greedy averaging algorithm looks the most preferable in this case.","e6be97fb":"# <FONT color='red'>SGDRegressor<\/FONT>","d06d4a7f":"### 'greedy' averaging","d7e6a1ea":"# <FONT color='red'>Polynomial Regression<\/FONT>\n\n\u0434\u043e 2\u0439 \u0441\u0442\u0435\u043f\u0435\u043d\u0438","e139c58f":"# <FONT color='red'>ElasticNet<\/FONT>","4201f5e5":"### Bayesian optimization of weights for averaging"}}