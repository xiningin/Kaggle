{"cell_type":{"6e366e42":"code","01fc0625":"code","5530b8f0":"code","10ec03db":"code","b6d38131":"code","f133f339":"code","6805923d":"code","ede41dee":"code","a279d9a3":"code","6482de04":"code","f59f77b2":"code","8972956e":"code","57cd48fb":"code","26a15647":"code","6d5855ce":"code","565f076a":"code","b979a2d8":"code","d42de7c2":"code","ce8d121c":"code","5fba83b8":"code","4d4411dd":"code","a58d005e":"code","140f41c7":"code","beb92fdd":"code","a713652d":"code","e4529ff8":"code","ac58d7ca":"code","fc57fe51":"code","84b8550e":"code","89de3ebd":"code","8b8b391f":"code","997d6781":"code","1274508e":"code","a1d9a626":"code","3185fde4":"code","13581b6f":"code","adc34f1e":"code","82a24a6d":"code","32169364":"code","b5352d3e":"code","8bbb44d0":"code","d5ff59e1":"code","6af4b8b3":"markdown","b83b91c2":"markdown","d4c65a99":"markdown","b9399945":"markdown","ce636124":"markdown","46c6eb90":"markdown","e7b2182c":"markdown","11388891":"markdown","84d2e45e":"markdown"},"source":{"6e366e42":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchtext.data import Field, BucketIterator\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nimport spacy\n\nimport random\nimport math\nimport time","01fc0625":"SEED = 1234\n\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","5530b8f0":"spacy_en = spacy.load('en')","10ec03db":"def tokenize(text):\n    \"\"\"\n    Tokenizes English text from a string into a list of strings (tokens)\n    \"\"\"\n    return [tok.text for tok in spacy_en.tokenizer(text) if not tok.text.isspace()]","b6d38131":"from torchtext import data, vocab\n\ntokenizer = data.get_tokenizer('spacy')\nTEXT = Field(tokenize=tokenize,\n            init_token = '<sos>', \n            eos_token = '<eos>', \n            include_lengths = True,\n            lower = True)\n\n","f133f339":"# let's train model on a part of train.csv to speed up computations\n\n# import pandas as pd\n\n# train_df = pd.read_csv('\/kaggle\/input\/title-generation\/train.csv').head(30001)\n# train_df.to_csv('\/kaggle\/working\/train_small.csv', index=False)","6805923d":"%%time\ntrn_data_fields = [(\"src\", TEXT),\n                   (\"trg\", TEXT)]\n\ndataset = data.TabularDataset(\n    path='\/kaggle\/input\/title-generation\/train.csv',\n    format='csv',\n    skip_header=True,\n    fields=trn_data_fields\n)\n\ntrain_data, valid_data, test_data = dataset.split(split_ratio=[0.98, 0.01, 0.01])","ede41dee":"TEXT.build_vocab(train_data, min_freq = 7)\nprint(f\"Unique tokens in vocabulary: {len(TEXT.vocab)}\")","a279d9a3":"BATCH_SIZE = 32\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data), \n     batch_size = BATCH_SIZE,\n     sort_within_batch = True,\n     sort_key = lambda x : len(x.src),\n     device = device)","6482de04":"class Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n        super().__init__()\n        \n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        \n        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n        \n        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, src, src_len):\n        \n        #src = [src sent len, batch size]\n        #src_len = [src sent len]\n        \n        embedded = self.dropout(self.embedding(src))\n        \n        #embedded = [src sent len, batch size, emb dim]\n                \n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len)\n                \n        packed_outputs, hidden = self.rnn(packed_embedded)\n                                 \n        #packed_outputs is a packed sequence containing all hidden states\n        #hidden is now from the final non-padded element in the batch\n            \n        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n            \n        #outputs is now a non-packed sequence, all hidden states obtained\n        #  when the input is a pad token are all zeros\n            \n        #outputs = [sent len, batch size, hid dim * num directions]\n        #hidden = [n layers * num directions, batch size, hid dim]\n        \n        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n        #outputs are always from the last layer\n        \n        #hidden [-2, :, : ] is the last of the forwards RNN \n        #hidden [-1, :, : ] is the last of the backwards RNN\n        \n        #initial decoder hidden is final hidden state of the forwards and backwards \n        #  encoder RNNs fed through a linear layer\n        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n        \n        #outputs = [sent len, batch size, enc hid dim * 2]\n        #hidden = [batch size, dec hid dim]\n        \n        return outputs, hidden","f59f77b2":"class Attention(nn.Module):\n    def __init__(self, enc_hid_dim, dec_hid_dim):\n        super().__init__()\n        \n        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n        self.v = nn.Parameter(torch.rand(dec_hid_dim))\n        \n    def forward(self, hidden, encoder_outputs, mask):\n        \n        #hidden = [batch size, dec hid dim]\n        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n        #mask = [batch size, src sent len]\n        \n        batch_size = encoder_outputs.shape[1]\n        src_len = encoder_outputs.shape[0]\n        \n        #repeat encoder hidden state src_len times\n        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n        \n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n        \n        #hidden = [batch size, src sent len, dec hid dim]\n        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n        \n        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n        \n        #energy = [batch size, src sent len, dec hid dim]\n                \n        energy = energy.permute(0, 2, 1)\n        \n        #energy = [batch size, dec hid dim, src sent len]\n        \n        #v = [dec hid dim]\n        \n        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n        \n        #v = [batch size, 1, dec hid dim]\n            \n        attention = torch.bmm(v, energy).squeeze(1)\n        \n        #attention = [batch size, src sent len]\n        \n        attention = attention.masked_fill(mask == 0, -1e10)\n        \n        return F.softmax(attention, dim = 1)","8972956e":"class Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n        super().__init__()\n\n        self.output_dim = output_dim\n        self.attention = attention\n        \n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        \n        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n        \n        self.out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, input, hidden, encoder_outputs, mask):\n             \n        #input = [batch size]\n        #hidden = [batch size, dec hid dim]\n        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n        #mask = [batch size, src sent len]\n        \n        input = input.unsqueeze(0)\n        \n        #input = [1, batch size]\n        \n        embedded = self.dropout(self.embedding(input))\n        \n        #embedded = [1, batch size, emb dim]\n        \n        a = self.attention(hidden, encoder_outputs, mask)\n                \n        #a = [batch size, src sent len]\n        \n        a = a.unsqueeze(1)\n        \n        #a = [batch size, 1, src sent len]\n        \n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n        \n        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n        \n        weighted = torch.bmm(a, encoder_outputs)\n        \n        #weighted = [batch size, 1, enc hid dim * 2]\n        \n        weighted = weighted.permute(1, 0, 2)\n        \n        #weighted = [1, batch size, enc hid dim * 2]\n        \n        rnn_input = torch.cat((embedded, weighted), dim = 2)\n        \n        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n            \n        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n        \n        #output = [sent len, batch size, dec hid dim * n directions]\n        #hidden = [n layers * n directions, batch size, dec hid dim]\n        \n        #sent len, n layers and n directions will always be 1 in this decoder, therefore:\n        #output = [1, batch size, dec hid dim]\n        #hidden = [1, batch size, dec hid dim]\n        #this also means that output == hidden\n        assert (output == hidden).all()\n        \n        embedded = embedded.squeeze(0)\n        output = output.squeeze(0)\n        weighted = weighted.squeeze(0)\n        \n        output = self.out(torch.cat((output, weighted, embedded), dim = 1))\n        \n        #output = [bsz, output dim]\n        \n        return output, hidden.squeeze(0), a.squeeze(1)","57cd48fb":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, pad_idx, sos_idx, eos_idx, device):\n        super().__init__()\n        \n        self.encoder = encoder\n        self.decoder = decoder\n        self.pad_idx = pad_idx\n        self.sos_idx = sos_idx\n        self.eos_idx = eos_idx\n        self.device = device\n        \n    def create_mask(self, src):\n        mask = (src != self.pad_idx).permute(1, 0)\n        return mask\n        \n    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n        \n        #src = [src sent len, batch size]\n        #src_len = [batch size]\n        #trg = [trg sent len, batch size]\n        #teacher_forcing_ratio is probability to use teacher forcing\n        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n        \n        if trg is None:\n            assert teacher_forcing_ratio == 0, \"Must be zero during inference\"\n            inference = True\n            trg = torch.zeros((100, src.shape[1])).long().fill_(self.sos_idx).to(src.device)\n        else:\n            inference = False\n            \n        batch_size = src.shape[1]\n        max_len = trg.shape[0]\n        trg_vocab_size = self.decoder.output_dim\n        \n        #tensor to store decoder outputs\n        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n        \n        #tensor to store attention\n        attentions = torch.zeros(max_len, batch_size, src.shape[0]).to(self.device)\n        \n        #encoder_outputs is all hidden states of the input sequence, back and forwards\n        #hidden is the final forward and backward hidden states, passed through a linear layer\n        encoder_outputs, hidden = self.encoder(src, src_len)\n                \n        #first input to the decoder is the <sos> tokens\n        input = trg[0,:]\n        \n        mask = self.create_mask(src)\n                \n        #mask = [batch size, src sent len]\n                \n        for t in range(1, max_len):\n            \n            #insert input token embedding, previous hidden state, all encoder hidden states \n            # and mask\n            #receive output tensor (predictions), new hidden state and attention tensor\n            output, hidden, attention = self.decoder(input, hidden, encoder_outputs, mask)\n            \n            #place predictions in a tensor holding predictions for each token\n            outputs[t] = output\n            \n            #place attentions in a tensor holding attention value for each input token\n            attentions[t] = attention\n            \n            #decide if we are going to use teacher forcing or not\n            teacher_force = random.random() < teacher_forcing_ratio\n            \n            #get the highest predicted token from our predictions\n            top1 = output.argmax(1) \n            \n            #if teacher forcing, use actual next token as next input\n            #if not, use predicted token\n            input = trg[t] if teacher_force else top1\n            \n            #if doing inference and next token\/prediction is an eos token then stop\n            if inference and input.item() == self.eos_idx:\n                return outputs[:t], attentions[:t]\n            \n        return outputs, attentions","26a15647":"INPUT_DIM = len(TEXT.vocab)\nOUTPUT_DIM = len(TEXT.vocab)\nENC_EMB_DIM = 128\nDEC_EMB_DIM = 128\nENC_HID_DIM = 64\nDEC_HID_DIM = 64\nENC_DROPOUT = 0.8\nDEC_DROPOUT = 0.8\nPAD_IDX = TEXT.vocab.stoi['<pad>']\nSOS_IDX = TEXT.vocab.stoi['<sos>']\nEOS_IDX = TEXT.vocab.stoi['<eos>']\n\nattn = Attention(ENC_HID_DIM, DEC_HID_DIM)\nenc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\ndec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n\nmodel = Seq2Seq(enc, dec, PAD_IDX, SOS_IDX, EOS_IDX, device).to(device)","6d5855ce":"def init_weights(m):\n    for name, param in m.named_parameters():\n        if 'weight' in name:\n            nn.init.normal_(param.data, mean=0, std=0.01)\n        else:\n            nn.init.constant_(param.data, 0)\n            \nmodel.apply(init_weights)","565f076a":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","b979a2d8":"optimizer = optim.Adam(model.parameters())","d42de7c2":"criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)","ce8d121c":"import matplotlib\nmatplotlib.rcParams.update({'figure.figsize': (16, 12), 'font.size': 14})\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython.display import clear_output\n\n\ndef train(model, iterator, optimizer, criterion, clip, train_history=None, valid_history=None):\n    \n    model.train()\n    \n    epoch_loss = 0\n    history = []\n    for i, batch in enumerate(iterator):\n        \n        src, src_len = batch.src\n        trg, trg_len = batch.trg\n        \n        optimizer.zero_grad()\n        \n        output, attetion = model(src, src_len, trg)\n        \n        #trg = [trg sent len, batch size]\n        #output = [trg sent len, batch size, output dim]\n        \n        output = output[1:].view(-1, output.shape[-1])\n        trg = trg[1:].view(-1)\n        \n        #trg = [(trg sent len - 1) * batch size]\n        #output = [(trg sent len - 1) * batch size, output dim]\n        \n        loss = criterion(output, trg)\n        \n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        \n        history.append(loss.cpu().data.numpy())\n        if (i+1)%10==0:\n            fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 8))\n\n            clear_output(True)\n            ax[0].plot(history, label='train loss')\n            ax[0].set_xlabel('Batch')\n            ax[0].set_title('Train loss')\n            if train_history is not None:\n                ax[1].plot(train_history, label='general train history')\n                ax[1].set_xlabel('Epoch')\n            if valid_history is not None:\n                ax[1].plot(valid_history, label='general valid history')\n            plt.legend()\n            \n            plt.show()\n        \n    return epoch_loss \/ len(iterator)","5fba83b8":"def evaluate(model, iterator, criterion):\n    \n    model.eval()\n    \n    epoch_loss = 0\n    \n    with torch.no_grad():\n    \n        for i, batch in enumerate(iterator):\n\n            src, src_len = batch.src\n            trg, trg_len = batch.trg\n\n            output, attention = model(src, src_len, trg, 0) #turn off teacher forcing\n\n            #trg = [trg sent len, batch size]\n            #output = [trg sent len, batch size, output dim]\n\n            output = output[1:].view(-1, output.shape[-1])\n            trg = trg[1:].view(-1)\n\n            #trg = [(trg sent len - 1) * batch size]\n            #output = [(trg sent len - 1) * batch size, output dim]\n\n            loss = criterion(output, trg)\n\n            epoch_loss += loss.item()\n        \n    return epoch_loss \/ len(iterator)","4d4411dd":"def epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time \/ 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","a58d005e":"MODEL_NAME = '\/kaggle\/working\/baseline2.pt'","140f41c7":"# to get results like in leaderboard train the model\n# for 10 epochs on the whole dataset (135000 samples instead of 30000)\n\nN_EPOCHS = 7\nCLIP = 1\n\ntrain_history = []\nvalid_history = []\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n    \n    start_time = time.time()\n    \n    train_loss = train(model, train_iterator, optimizer, criterion, CLIP, train_history, valid_history)\n    valid_loss = evaluate(model, valid_iterator, criterion)\n    \n    end_time = time.time()\n    \n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), MODEL_NAME)\n        \n        \n    train_history.append(train_loss)\n    valid_history.append(valid_loss)\n    \n    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')","beb92fdd":"# for cpu usage\n# model.load_state_dict(torch.load(MODEL_NAME, map_location=torch.device('cpu')))\n\n# for gpu usage\n# model.load_state_dict(torch.load(MODEL_NAME), map_location=torch.device('cpu'))\n\n\ntest_loss = evaluate(model, test_iterator, criterion)\n\nprint(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')","a713652d":"def translate_sentence(model, tokenized_sentence):\n    model.eval()\n    tokenized_sentence = ['<sos>'] + [t.lower() for t in tokenized_sentence] + ['<eos>']\n    numericalized = [TEXT.vocab.stoi[t] for t in tokenized_sentence] \n    sentence_length = torch.LongTensor([len(numericalized)]).to(device) \n    tensor = torch.LongTensor(numericalized).unsqueeze(1).to(device) \n    translation_tensor_logits, attention = model(tensor, sentence_length, None, 0) \n    translation_tensor = torch.argmax(translation_tensor_logits.squeeze(1), 1)\n    translation = [TEXT.vocab.itos[t] for t in translation_tensor]\n    translation, attention = translation[1:], attention[1:]\n    return translation, attention","e4529ff8":"def display_attention(sentence, translation, attention):\n    \n    fig = plt.figure(figsize=(30,50))\n    ax = fig.add_subplot(111)\n    \n    attention = attention.squeeze(1).cpu().detach().numpy().T\n    \n    cax = ax.matshow(attention, cmap='bone')\n   \n    ax.tick_params(labelsize=12)\n    ax.set_yticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'])\n    ax.set_xticklabels(['']+translation, rotation=80)\n\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n    plt.show()\n    plt.close()","ac58d7ca":"example_idx = 100\n\nsrc = vars(train_data.examples[example_idx])['src']\ntrg = vars(train_data.examples[example_idx])['trg']\n\nprint(f'src = {src}')\nprint(f'trg = {trg}')","fc57fe51":"translation, attention = translate_sentence(model, src)\n\nprint(f'predicted trg = {translation}')","84b8550e":"display_attention(src, translation, attention)","89de3ebd":"for example_idx in range(100):\n    src = vars(test_data.examples[example_idx])['src']\n    trg = vars(test_data.examples[example_idx])['trg']\n    translation, attention = translate_sentence(model, src)\n\n    print('\u041e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u044b\u0439 \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043e\u043a: ', ' '.join(trg))\n    print('\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043e\u043a: ', ' '.join(translation))\n    print('-----------------------------------')","8b8b391f":"example_idx = 0\n\nsrc = vars(valid_data.examples[example_idx])['src']\ntrg = vars(valid_data.examples[example_idx])['trg']\n\nprint(f'src = {src}')\nprint(f'trg = {trg}')","997d6781":"translation, attention = translate_sentence(model, src)\n\nprint(f'predicted trg = {translation}')\n\ndisplay_attention(src, translation, attention)","1274508e":"example_idx = 10\n\nsrc = vars(test_data.examples[example_idx])['src']\ntrg = vars(test_data.examples[example_idx])['trg']\n\nprint(f'src = {src}')\nprint(f'trg = {trg}')","a1d9a626":"translation, attention = translate_sentence(model, src)\n\nprint(f'predicted trg = {translation}')\n\ndisplay_attention(src, translation, attention)","3185fde4":"import nltk\n\nn_gram_weights = [0.3334, 0.3333, 0.3333]","13581b6f":"test_len = len(test_data)","adc34f1e":"original_texts = []\ngenerated_texts = []\nmacro_bleu = 0\n\nfor example_idx in range(test_len):\n    src = vars(test_data.examples[example_idx])['src']\n    trg = vars(test_data.examples[example_idx])['trg']\n    translation, _ = translate_sentence(model, src)\n\n    original_texts.append(trg)\n    generated_texts.append(translation)\n\n    bleu_score = nltk.translate.bleu_score.sentence_bleu(\n        [trg],\n        translation,\n        weights = n_gram_weights\n    )    \n    macro_bleu += bleu_score\n\nmacro_bleu \/= test_len","82a24a6d":"# averaging sentence-level BLEU (i.e. macro-average precision)\nprint('Macro-average BLEU (LSTM): {0:.5f}'.format(macro_bleu))","32169364":"import pandas as pd\n\nsubmission_data = pd.read_csv('\/kaggle\/input\/title-generation\/test.csv')\nabstracts = submission_data['abstract'].values","b5352d3e":"titles = []\nfor abstract in abstracts:\n    title, _ = translate_sentence(model, abstract.split())\n    titles.append(' '.join(title).replace('<unk>', ''))","8bbb44d0":"submission_df = pd.DataFrame({'abstract': abstracts, 'title': titles})\nsubmission_df.to_csv('predicted_titles.csv', index=False)","d5ff59e1":"import string\nfrom nltk.util import ngrams\nimport numpy as np\nimport pandas as pd\nimport pickle\n\n\ndef generate_csv(input_file='predicted_titles.csv',\n                 output_file='submission.csv',\n                 voc_file='\/kaggle\/input\/title-generation\/vocs.pkl'):\n    '''\n    Generates file in format required for submitting result to Kaggle\n    \n    Parameters:\n        input_file (str) : path to csv file with your predicted titles.\n                           Should have two fields: abstract and title\n        output_file (str) : path to output submission file\n        voc_file (str) : path to voc.pkl file\n    '''\n    data = pd.read_csv(input_file)\n    with open(voc_file, 'rb') as voc_file:\n        vocs = pickle.load(voc_file)\n\n    with open(output_file, 'w') as res_file:\n        res_file.write('Id,Predict\\n')\n        \n    output_idx = 0\n    for row_idx, row in data.iterrows():\n        trg = row['title']\n        trg = trg.translate(str.maketrans('', '', string.punctuation)).lower().split()\n        trg.extend(['_'.join(ngram) for ngram in list(ngrams(trg, 2)) + list(ngrams(trg, 3))])\n        \n        VOCAB_stoi = vocs[row_idx]\n        trg_intersection = set(VOCAB_stoi.keys()).intersection(set(trg))\n        trg_vec = np.zeros(len(VOCAB_stoi))    \n\n        for word in trg_intersection:\n            trg_vec[VOCAB_stoi[word]] = 1\n\n        with open(output_file, 'a') as res_file:\n            for is_word in trg_vec:\n                res_file.write('{0},{1}\\n'.format(output_idx, int(is_word)))\n                output_idx += 1\n\n\ngenerate_csv()","6af4b8b3":"### \u0421\u0447\u0438\u0442\u0430\u0435\u043c BLEU","b83b91c2":"# \u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u043e\u0432 \u043d\u0430\u0443\u0447\u043d\u044b\u0445 \u0441\u0442\u0430\u0442\u0435\u0439: \u0441\u043b\u0430\u0431\u044b\u0439 baseline","d4c65a99":"\u0421 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0444\u0443\u043d\u043a\u0446\u0438\u0438 `generate_csv` \u043f\u0440\u0438\u0432\u043e\u0434\u0438\u043c \u0444\u0430\u0439\u043b `submission_prediction.csv` \u0432 \u0444\u043e\u0440\u043c\u0430\u0442, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0439 \u0434\u043b\u044f \u043f\u043e\u0441\u044b\u043b\u043a\u0438 \u0432 \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u0435 \u043d\u0430 Kaggle:","b9399945":"### \u0414\u0435\u043b\u0430\u0435\u043c submission \u0432 Kaggle","ce636124":"### \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438","46c6eb90":"\u0418\u0441\u0442\u043e\u0447\u043d\u0438\u043a: https:\/\/github.com\/bentrevett\/pytorch-seq2seq","e7b2182c":"\u0417\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u0438 \u0432 \u0444\u0430\u0439\u043b \u0444\u043e\u0440\u043c\u0430\u0442\u0430 `<abstract>,<title>`:","11388891":"\u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u043e\u0432 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445:","84d2e45e":"### \u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u043e\u0432"}}