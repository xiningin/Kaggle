{"cell_type":{"aef0bddb":"code","3b1ca147":"code","a9b65f83":"code","f9a07dad":"code","7b4878ba":"code","c97a9194":"code","29c649a0":"code","c3e63dc8":"code","336bef08":"code","ea0b0138":"code","8837a13e":"code","fc83c453":"code","a0b6b2af":"code","dce975e8":"code","de60c427":"code","be898003":"code","d65361b5":"code","6fd5c232":"code","cac56ab0":"code","cb19e812":"code","18863c10":"code","bd04ce2b":"code","b77b896c":"code","392a8d9a":"code","0807ef25":"code","d84fc08a":"code","b8919eb8":"markdown"},"source":{"aef0bddb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3b1ca147":"# Import required librarues\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import QuantileTransformer\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\n\n\nfrom mlxtend.classifier import StackingCVClassifier\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.metrics import accuracy_score\n\nfrom mlxtend.feature_selection import ColumnSelector\nfrom sklearn.pipeline import make_pipeline","a9b65f83":"train = pd.read_csv(\"\/kaggle\/input\/learn-together\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/learn-together\/test.csv\")","f9a07dad":"# Remove the Labels and make them y\ny = train['Cover_Type']\n\n# Remove label from Train set\nX = train.drop(['Cover_Type'],axis=1)\n\n# Rename test to text_X\ntest_X = test\n\n\n\n# split data into training and validation data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX = X.drop(['Id'], axis = 1)\ntrain_X = train_X.drop(['Id'], axis = 1)\nval_X = val_X.drop(['Id'], axis = 1)\ntest_X = test_X.drop(['Id'], axis = 1)","7b4878ba":"train_X.describe()","c97a9194":"val_X.describe()","29c649a0":"sns.distplot(train_X['Elevation'], label = 'train_X')\nsns.distplot(val_X['Elevation'], label = 'val_X')\nsns.distplot(test_X['Elevation'], label = 'test_X')\nplt.legend()\nplt.title('Elevation')\nplt.show()","c3e63dc8":"sns.distplot(train_X['Aspect'], label = 'train_X')\nsns.distplot(val_X['Aspect'], label = 'val_X')\nsns.distplot(test_X['Aspect'], label = 'test_X')\nplt.title('Aspect')\nplt.legend()\nplt.show()","336bef08":"sns.distplot(train_X['Horizontal_Distance_To_Hydrology'], label = 'train_X')\nsns.distplot(val_X['Horizontal_Distance_To_Hydrology'], label = 'val_X')\nsns.distplot(test_X['Horizontal_Distance_To_Hydrology'], label = 'test_X')\nplt.title('Horizontal_Distance_To_Hydrology')\nplt.legend()\nplt.show()","ea0b0138":"sns.distplot(train_X['Vertical_Distance_To_Hydrology'], label = 'train_X')\nsns.distplot(val_X['Vertical_Distance_To_Hydrology'], label = 'val_X')\nsns.distplot(test_X['Vertical_Distance_To_Hydrology'], label = 'test_X')\nplt.title('Vertical_Distance_To_Hydrology')\nplt.legend()\nplt.show()","8837a13e":"sns.distplot(train_X['Horizontal_Distance_To_Roadways'], label = 'train_X')\nsns.distplot(val_X['Horizontal_Distance_To_Roadways'], label = 'val_X')\nsns.distplot(test_X['Horizontal_Distance_To_Roadways'], label = 'test_X')\nplt.title('Horizontal_Distance_To_Roadways')\nplt.legend()\nplt.show()","fc83c453":"sns.distplot(train_X['Hillshade_9am'], label = 'train_X')\nsns.distplot(val_X['Hillshade_9am'], label = 'val_X')\nsns.distplot(test_X['Hillshade_9am'], label = 'test_X')\nplt.title('Hillshade_9am')\nplt.legend()\nplt.show()","a0b6b2af":"### define the classifiers\n### Parameters from :https:\/\/www.kaggle.com\/joshofg\/pure-random-forest-hyperparameter-tuning\n\nclassifier_rf = RandomForestClassifier(n_estimators = 719,\n                                       max_features = 0.3,\n                                       max_depth = 464,\n                                       min_samples_split = 2,\n                                       min_samples_leaf = 1,\n                                       bootstrap = False,\n                                       random_state=42)\nclassifier_xgb = OneVsRestClassifier(XGBClassifier(n_estimators = 719,\n                                                   max_depth = 464,\n                                                   random_state=42))\nclassifier_et = ExtraTreesClassifier(random_state=42)\n\nclassifier_adb = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(random_state=42), random_state=42)\nclassifier_bg = BaggingClassifier(random_state=42)","dce975e8":"rf_fet = np.array([0, 3, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53])","de60c427":"xgb_fet = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 48, 49, 51, 52])","be898003":"et_fet = np.array([0, 1, 3, 5, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53])","d65361b5":"adb_fet = np.array([0, 1, 3, 5, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 33, 34, 36, 37, 38, 39, 41, 42, 44, 45, 46, 47, 48, 50, 51, 52, 53])","6fd5c232":"pipe_rf = make_pipeline(ColumnSelector(cols=(0, 3, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53)),\n                        classifier_rf)\npipe_xgb = make_pipeline(ColumnSelector(cols=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 48, 49, 51, 52)),\n                         classifier_xgb)\npipe_et = make_pipeline(ColumnSelector(cols=(0, 1, 3, 5, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53)),\n                        classifier_et)\npipe_adb = make_pipeline(ColumnSelector(cols=(0, 1, 3, 5, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 33, 34, 36, 37, 38, 39, 41, 42, 44, 45, 46, 47, 48, 50, 51, 52, 53)),\n                        classifier_adb)\npipe_bg = make_pipeline(ColumnSelector(cols=(0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53)),\n                        classifier_bg)","cac56ab0":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nsclf = StackingCVClassifier(classifiers=[pipe_rf,\n                                         pipe_xgb,\n                                         pipe_et,\n                                         pipe_adb,\n                                         pipe_bg],\n                            use_probas=True,\n                            meta_classifier=classifier_rf)\n\n\n\nlabels = ['Random Forest', 'XGBoost', 'ExtraTrees', 'AdaBoost', 'Bagging', 'MetaClassifier']\n\n\n\n\nfor clf, label in zip([classifier_rf, classifier_xgb, classifier_et, classifier_adb, classifier_bg, sclf], labels):\n    scores = cross_val_score(clf, train_X.values, train_y.values.ravel(),\n                             cv=5,\n                             scoring='accuracy')\n    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))","cb19e812":"sclf.fit(train_X.values, train_y.values.ravel())","18863c10":"\nval_pred = sclf.predict(val_X.values)","bd04ce2b":"acc = accuracy_score(val_y, val_pred)\nprint(acc)","b77b896c":"sclffin = StackingCVClassifier(classifiers=[pipe_rf,\n                                            pipe_xgb,\n                                            pipe_et,\n                                            pipe_adb,\n                                            pipe_bg],\n                               use_probas=True,\n                               meta_classifier=classifier_rf)","392a8d9a":"sclffin.fit(X.values, y.values.ravel())","0807ef25":"test_ids = test[\"Id\"]\ntest_pred = sclffin.predict(test_X.values)","d84fc08a":"# Save test predictions to file\noutput = pd.DataFrame({'Id': test_ids,\n                       'Cover_Type': test_pred})\noutput.to_csv('submission.csv', index=False)","b8919eb8":"### Import the Raw Data"}}