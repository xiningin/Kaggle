{"cell_type":{"e77b1a6a":"code","db88b891":"code","db516381":"code","81064743":"code","34327981":"code","29ab5a30":"code","333ca850":"code","127540f5":"code","11138c68":"code","1fb5d3fc":"code","18d1a0cf":"code","66ada03c":"code","c93ebcf1":"code","bc44e59c":"code","60ebf604":"code","0f62363c":"code","0104f20b":"code","e5f42dbf":"code","0231f486":"code","81bd7754":"code","62f19315":"code","b31f450a":"code","dda548b1":"code","975f87fb":"code","1365320a":"code","2908c1ba":"code","bf0b7298":"code","da502647":"code","39416639":"code","59a88b39":"code","77d1815a":"code","b549557b":"code","e77c4330":"code","4235ae8a":"code","8cf06490":"code","d31c23c7":"code","2d9c1da4":"code","519bd377":"code","80bd31e3":"code","b23f7ea5":"code","bd158cad":"code","a1aea1d0":"code","0faf78f6":"code","41c7345c":"code","2933063f":"code","56c6baec":"code","4204d949":"markdown"},"source":{"e77b1a6a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","db88b891":"!tar -zxvf ..\/input\/cifar-10-python.tar.gz","db516381":"import torch\nimport torchvision\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt","81064743":"means = (0.491, 0.482, 0.447)\nstds = (0.247, 0.243, 0.262)","34327981":"transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(means,stds)])\ntrainset = torchvision.datasets.CIFAR10(root='.', train=True,\n                                        download=False, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=100,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='.', train=False,\n                                       download=False, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=128,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')","29ab5a30":"def imshow(img, figsize=None, denormalize=True):\n    npimg = img.numpy()\n    npimg = np.transpose(npimg, (1, 2, 0))\n    if denormalize:\n        npimg = npimg * stds + means\n    npimg = np.clip(npimg, 0, 1)\n    fig = plt.figure(figsize=figsize)\n    ax = fig.add_subplot('111')\n    ax.set_aspect('equal')\n    ax.imshow(npimg)","333ca850":"images, labels = iter(trainloader).next()\nprint(type(images))\nprint(images.shape)","127540f5":"grid = torchvision.utils.make_grid(images[:32])\nimshow(grid)\nfor i in range(32):\n    if i % 8 == 0:\n        print('\\n')\n    print(classes[labels[i]],end=' ')","11138c68":"from tqdm import tqdm_notebook\nimport tqdm\nimport time","1fb5d3fc":"from torch import nn\nimport torch.nn.functional as F","18d1a0cf":"class ConvBlock(nn.Module):\n    def __init__(self, input_channels, output_channels, kernel_size, batch_norm=True, pooling=None, pooling_kernel_size=None, dropout=None):\n        super().__init__()\n        layers = []\n        layers.append(nn.Conv2d(input_channels, output_channels, kernel_size=kernel_size))\n        if batch_norm:\n            layers.append(nn.BatchNorm2d(output_channels))\n        if pooling == 'max':\n            layers.append(nn.MaxPool2d(pooling_kernel_size))\n        elif pooling == 'avg':\n            layers.append(nn.AvgPool2d(pooling_kernel_size))\n            \n        layers.append(nn.ReLU())\n        if dropout is not None:\n            layers.append(nn.Dropout2d(p=dropout))\n        self.block = nn.Sequential(*layers)\n        \n    def forward(self, x):\n        return self.block(x)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv_part = nn.Sequential(\n            ConvBlock(3, 96, kernel_size=5), # 96 x 28 x 28\n            ConvBlock(96, 128, kernel_size=5, pooling='max', pooling_kernel_size=2, batch_norm=False, dropout=0.2), # 128 x 12 x 12\n            ConvBlock(128, 256, kernel_size=5, pooling='max', pooling_kernel_size=2,  batch_norm=False, dropout=0.2), # 128 x 4 x 4\n            ConvBlock(256, 256, kernel_size=3,  batch_norm=False, dropout=0.2),\n            nn.AdaptiveAvgPool2d((1,1))\n        )\n        \n        self.dense1 = nn.Sequential(\n#             nn.BatchNorm1d(256),\n            nn.Dropout(p=0.2),\n            nn.Linear(256, 10)\n        )\n        \n        \n    def linear_size(self):\n        x = torch.empty(1, 3, 32, 32, dtype=torch.float32).to(next(iter(self.parameters())).device)\n        x = self.conv_part(x)\n        print(x.size())\n        \n    def forward(self,x):\n        x = self.conv_part(x)\n        x = x.view(x.size(0), -1)\n        x = self.dense1(x)\n        return x\n        ","66ada03c":"def foo(first, second,**kws):\n    print(first, second, kws)\n    \nfoo(**{'first':1, 'second': 2, 'third': 3})","c93ebcf1":"class HyperSampler:\n    def sample(self, size):\n        raise NotImplementedError","bc44e59c":"class LambdaSampler(HyperSampler):\n    def __init__(self, fn):\n        super().__init__()\n        self._fn = fn\n        \n    def sample(self, size):\n        return self._fn(size)","60ebf604":"class FixedSampler(HyperSampler):\n    def __init__(self, array):\n        super().__init__()\n        self._arr = np.asarray(array)\n    \n    def sample(self, size):\n        if len(self._arr) < size:\n            raise ValueError(\"len(self._arr) < size\")\n        return self._arr[:size]","0f62363c":"class ArraySampler(HyperSampler):\n    def __init__(self, array):\n        super().__init__()\n        self._arr = np.asarray(array)\n        \n    def sample(self, size):\n        idx = np.random.choice(len(self._arr), size=size)\n        return self._arr[idx]","0104f20b":"def h_enum(*values):\n    return ArraySampler(values)\n\ndef h_set(values):\n    return ArraySampler(values)\n\ndef h_fixed_enum(*values):\n    return FixedSampler(values)\n\ndef h_fixed_set(value):\n    return FixedSampler(values)","e5f42dbf":"def split_dictionary(d, criterion):\n    a, b = {}, {}\n    for k, v in d.items():\n        if criterion(k, v):\n            a[k] = v\n        else:\n            b[k] = v\n    return a, b","0231f486":"print(dict({'a': 5} ,**{'b': 6}))","81bd7754":"class BestModel:\n    def __init__(self, path, initial_criterion):\n        self.path = path\n        self.criterion = initial_criterion\n        \n    def update(self, model, optimizer, criterion):\n        self.criterion = criterion\n        torch.save({'model_state': model.state_dict(), 'optimizer_state': optimizer.state_dict(), 'criterion': criterion}, self.path)\n        \n    def load_model_data(self):\n        return torch.load(self.path)\n    \n    def restore(self, model, optimizer):\n        model_data = self.load_model_data()\n        model.load_state_dict(model_data['model_state'])\n        optimizer.load_state_dict(model_data['optimizer_state'])","62f19315":"from collections import defaultdict\nimport itertools\n\ndef hyper_search(num_trials,\n                 model_factory, optimizer_factory,\n                 training_set, validation_set,\n                 training_function, best_model,\n                 parameters, device, repeats=1, random_seed=438):\n    param_groups = ('model', 'optimizer', 'train_loader', 'val_loader', 'training')\n    fixed_params, sampled_params = {}, {}\n    for param_group in param_groups:\n        sampled, fixed = split_dictionary(parameters.get(param_group, {}), lambda _,v: isinstance(v, HyperSampler))\n        fixed_params[param_group] = fixed\n        sampled_params[param_group] = sampled\n        \n    def flatten(dict_of_dicts):\n        result = {}\n        for pg, params in dict_of_dicts.items():\n            for k,v in params.items():\n                yield pg + '_' + k, v\n                \n    def merge(frame_dict, flattened):\n        for k, v in flattened:\n            frame_dict[k].append(v)\n        \n    random_queue = defaultdict(dict)\n    for p in param_groups:\n        for k, population in sampled_params[p].items():\n            sample = population.sample(num_trials)\n            random_queue[p][k] = sample\n            \n#     print(random_queue)\n#     print(fixed_params)\n    \n    best_setting = None\n    \n    result_table = defaultdict(list)\n    \n    def decay(x):\n#         print(x, type(x))\n        if isinstance(x, np.generic):\n            return x.item()\n        return x\n    \n    for trial in tqdm_notebook(range(num_trials), total=num_trials):\n        np.random.seed(random_seed)\n        torch.manual_seed(random_seed)\n        current_trial_random = {pg: {k: decay(sample[trial]) for k, sample in random_queue[pg].items()} for pg in param_groups}\n        iteration_setting = {pg:dict(fixed_params[pg],**current_trial_random[pg]) for pg in param_groups}\n        \n        print('Trial', trial + 1)\n        print('Setting', dict(flatten(current_trial_random)))\n        model = model_factory(**iteration_setting['model']).to(device)\n        optimizer = optimizer_factory(model.parameters(), **iteration_setting['optimizer'])\n#         print(type(iteration_setting['train_loader']['batch_size']))\n        train_loader = torch.utils.data.DataLoader(training_set,**iteration_setting['train_loader'])\n        val_loader = torch.utils.data.DataLoader(validation_set, **iteration_setting['val_loader'])\n        \n        try:\n            setting_best_model:BestModel = training_function( model, optimizer, train_loader, val_loader, **iteration_setting['training'])\n        except Exception as ex:\n            print(ex)\n        else:\n            if setting_best_model.criterion is not None:\n                if setting_best_model.criterion < best_model.criterion:\n                    best_model, best_setting = setting_best_model, iteration_setting\n\n            print('Trial', trial + 1)\n            print('Setting', dict(flatten(current_trial_random)))\n            print('criterion', setting_best_model.criterion)\n            print('=============')\n\n            merge(result_table, flatten(iteration_setting))\n            result_table['criterion'].append(setting_best_model.criterion)\n                \n    return best_model, best_setting, pd.DataFrame(result_table)\n    ","b31f450a":"# hyper_search(10,None,None,None,None, {\n#     'model': {'n_layers' : h_enum(1,2,3), 'dropout': h_enum(0.1,0.2,0.3,0.4,0.5)},\n#     'optimizer': {'learning_rate': h_set(np.logspace(-5,0,num=6)), 'class': 'Adam'},\n#     'train_loader': {'batch_size': h_enum(64,128,256)},\n#     'val_loader': {'batch_size': 512}\n# })","dda548b1":"from collections import namedtuple","975f87fb":"def train_model(net, optimizer, trainloader, testloader, criterion, n_epochs, best_model:BestModel=None, patience=5, n_prints=5):\n    attempts_left = patience\n    \n    print_every = len(trainloader) \/\/ n_prints\n#     print('Printing every', print_every, 'batches')\n    \n    if best_model is None:\n        best_model = BestModel('temp', 10000)\n    \n    for epoch in tqdm_notebook(range( n_epochs)):  \n        if attempts_left < 0:\n            break\n            \n        running_loss = 0.0\n        net.train()\n        for i, data in tqdm_notebook(enumerate(trainloader, 0), total = len(trainloader)):\n            # get the inputs\n            inputs, labels = data\n\n            inputs, labels = inputs.cuda(), labels.cuda()\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            \n            # print statistics\n            running_loss += loss.item()\n            if i % print_every == print_every - 1:    # print every 2000 mini-batches\n#                 print( labels.tolist())\n                grad_norm = 0.0\n                for p in net.parameters():\n                    grad_norm += p.grad.data.norm().item()\n                    \n                running_loss \/= print_every\n                print('[%d, %5d] loss: %.3f grad_norm: %.3f' %\n                      (epoch + 1, i + 1, loss, grad_norm))\n\n                running_loss = 0.0\n        \n\n        net.eval()\n\n        with torch.no_grad():\n            running_val_loss = 0.0\n            for i, data in tqdm_notebook(enumerate(testloader, 0), total = len(testloader)):\n                inputs, labels = data\n                inputs, labels = inputs.cuda(), labels.cuda()\n\n                outputs = net(inputs)\n                loss = criterion(outputs, labels)\n#                 print(loss.item())\n\n                running_val_loss += loss.item()\n                \n            val_loss = running_val_loss \/ len(testloader)\n            print('[%d] val_loss: %.3f' % (i+1, val_loss))\n            \n            if val_loss < best_model.criterion:\n                print('Val_loss improved from {} to {}'.format(best_model.criterion, val_loss))\n                best_model.update(net,optimizer,val_loss)\n                attempts_left = patience\n            else:\n                print('No improvement attempts left {}'.format(attempts_left))\n                attempts_left -= 1\n                \n\n    print('Finished Training')\n    return best_model\n#     best_model.restore(net, optimizer)","1365320a":"net = Net().cuda()\nprint(net.linear_size())\noptimizer = torch.optim.Adam(net.parameters(), lr=3e-4)\ncriterion = nn.CrossEntropyLoss()","2908c1ba":"best_model = BestModel('conv_hs', 10000)","bf0b7298":"del optimizer\ndel net\n","da502647":"loader = torch.utils.data.DataLoader(trainset, **dict(batch_size=64))\ndel loader","39416639":"saved_model, best_setting, results = hyper_search(1, Net, torch.optim.Adam, training_set=trainset, validation_set=testset, training_function=train_model, best_model=best_model,\n             parameters={\n                 'model': {},\n                 'optimizer': {'lr': 3e-4},\n                 'training': {'criterion': nn.CrossEntropyLoss(),\n                                      'n_epochs': h_fixed_enum(2),\n                                      'patience': 4,\n                                      'n_prints': 1},\n                 'train_loader': {'batch_size': h_fixed_enum(512), 'shuffle':True},\n                 'val_loader': {'batch_size': 256}\n             }, device=torch.device('cuda'), random_seed=531)\n\nprint(best_setting)","59a88b39":"results","77d1815a":"best_model = BestModel('conv_v1', 1000)","b549557b":"train_model(net, optimizer, criterion, 40, best_model, patience=6, n_prints=5)","e77c4330":"best_model.restore(net, optimizer)","4235ae8a":"dataiter = iter(testloader)\nimages, labels = dataiter.next()\n\nnet.eval()\nwith torch.no_grad():\n    outputs = net(images[:32].cuda())\n    _, predicted = torch.max(outputs.data, 1)\n    print(predicted)\n\n\n# print images\nimshow(torchvision.utils.make_grid(images[:32]))\nprint('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(32)))\nprint('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(32)))\n","8cf06490":"from sklearn import metrics","d31c23c7":"def evaluate(net, loader):\n    net.eval()\n    all_pred = []\n    correct_pred = []\n    loss = nn.CrossEntropyLoss()\n    with torch.no_grad():\n        running_loss = 0.0\n        for data in tqdm_notebook(loader):\n            images, labels = data \n            output = net(images.cuda())\n            running_loss += loss(output, labels.cuda()).item()\n            preds = output.max(1)[1].tolist()\n            all_pred.extend(preds)\n            correct_pred.extend(labels.tolist())\n    \n    running_loss \/= len(loader)\n    \n    print(metrics.classification_report(correct_pred, all_pred,target_names=classes))\n    cm = metrics.confusion_matrix(correct_pred, all_pred)\n    cm = pd.DataFrame(cm, index=classes, columns=classes)\n    print(cm)\n    print('Accuracy:', metrics.accuracy_score(correct_pred, all_pred))\n    print('Loss: ', running_loss)","2d9c1da4":"evaluate(net,testloader)","519bd377":"kernels = net.conv_part[0].block[0].weight.data.cpu()\n# mm_kernels = (mm_kernels - min_values) \/ (max_values - min_values)\n# print(mm_kernels)\ngrid_of_kernels = torchvision.utils.make_grid(kernels)\nimshow(grid_of_kernels.cpu(), denormalize=True, figsize=(6,6))\nplt.show()\n# imshow(torchvision.utils.make_grid(kernels.cpu(), normalize=True), denormalize=False)","80bd31e3":"testimages, _ = next(iter(testloader))","b23f7ea5":"grid = torchvision.utils.make_grid(testimages,nrow=16)\nimshow(grid,(16,16))","bd158cad":"with torch.no_grad():\n    transformed = net.conv_part[:4](testimages.cuda()).cpu()\n#     transformed = testimages","a1aea1d0":"transformed.shape","0faf78f6":"def normalize_image(img):\n    tr_img = img.transpose(0,1).reshape(img.size(1), -1)\n    channel_min = tr_img.min(dim=1)[0].view(1,-1,1,1)\n    channel_max = tr_img.max(dim=1)[0].view(1,-1,1,1)\n#     print(channel_max - channel_min)\n    return (img - channel_min) \/ (channel_max - channel_min + 0.01)\n    ","41c7345c":"transformed = normalize_image(transformed)","2933063f":"def show_image_number(num):\n    fig = plt.figure(figsize=(12,12))\n    total_channels = min(transformed[num].size(0), 96)\n    print(total_channels)\n    for i in range(total_channels):\n        ax = fig.add_subplot(12,8,i+1)\n        ax.imshow(transformed[num,i].numpy(), cmap='gray')\n        ax.axis('off')\n    plt.show()\n    imshow(testimages[num])\n#     ax.imshow(np.transpose(testimages[num].numpy(),(1,2,0)))\n    plt.show()","56c6baec":"show_image_number(56)","4204d949":"39.4% Multinomial logistic regression"}}