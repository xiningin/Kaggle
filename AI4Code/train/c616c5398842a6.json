{"cell_type":{"d6e1b93c":"code","a8fcf918":"code","9ac4dfed":"code","2b09966a":"code","95585a90":"code","a6d25337":"code","fe35939c":"code","a367c49d":"code","427a65d0":"code","3205a8e2":"code","584445bd":"code","e16538ef":"code","559bd4af":"code","90b09ceb":"code","e0ee5f22":"code","44ff41bc":"code","712b8f63":"code","d3d0ab18":"code","29a135c4":"code","4055e0b9":"code","1f1e8b9b":"code","dc003d61":"code","05c4aec4":"code","b4b1a1b6":"code","1b27150e":"code","af17b323":"code","4d43b30a":"code","10a1882f":"code","67faca20":"code","88ebf3ca":"code","6f198482":"code","b248803a":"code","6182e28b":"code","b7d058d7":"code","1bd6e631":"code","636e2e49":"code","6c08ab4b":"code","8b8811d8":"code","a8cb345b":"code","99797dcd":"code","77540868":"code","7066973a":"code","a9de71b9":"code","dc0685d2":"code","f3ea38b8":"code","f59a28fd":"code","a7226544":"markdown","3d7b0ec9":"markdown","269bd465":"markdown","c2b62561":"markdown","c2548e88":"markdown","0660fe5c":"markdown","07775747":"markdown","d2a95bcf":"markdown","7576751f":"markdown","a8386d18":"markdown","466c2ac2":"markdown","f058a583":"markdown","a5a36060":"markdown","1515df7d":"markdown","b2bf73f3":"markdown","c0fba04c":"markdown","85b6a690":"markdown","884168ac":"markdown","02343f49":"markdown","107cec88":"markdown","7842de1c":"markdown"},"source":{"d6e1b93c":"import numpy as np\nimport pandas as pd\n","a8fcf918":"Eng = pd.read_csv('..\/input\/english-to-french\/small_vocab_en.csv',error_bad_lines=False,header=None, names=['Line 1','Line 2'])\nEng.head()","9ac4dfed":"Fr = pd.read_csv('..\/input\/english-to-french\/small_vocab_fr.csv',error_bad_lines=False,header=None, names=['Line 1','Line 2'])\nFr.head()","2b09966a":"Fr.shape , Eng.shape","95585a90":"def findlist(df):\n    rows_with_nan = []\n    for index, row in df.iterrows():\n        is_nan_series = row.isnull()\n        if is_nan_series.any():\n            rows_with_nan.append(index)\n    return rows_with_nan\n","a6d25337":"def Union(lst1, lst2): \n    final_list = list(set(lst1) | set(lst2)) \n    return final_list","fe35939c":"E = findlist(Eng)\nF = findlist(Fr)","a367c49d":"del_ind = Union(E,F)\nprint(len(E),len(F),len(del_ind))","427a65d0":"del_ind =  sorted(del_ind)","3205a8e2":"Eng = Eng.drop(del_ind)\nFr = Fr.drop(del_ind)","584445bd":"Fr.shape , Eng.shape","e16538ef":"from keras.preprocessing.text import Tokenizer\nimport tensorflow as tf\ndef tokenize(x):\n    \"\"\"\n    Tokenize x\n    :param x: List of sentences\/strings to be tokenized\n    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n    \"\"\"\n    tokenizer=Tokenizer()\n    tokenizer.fit_on_texts(x)\n    t=tokenizer.texts_to_sequences(x)\n    # TODO: Implement\n    return t, tokenizer\n\n# Tokenize Example output\ntext_sentences = [\n    'The quick brown fox jumps over the lazy dog .',\n    'By Jove , my quick study of lexicography won a prize .',\n    'This is a short sentence .']\ntext_tokenized, text_tokenizer = tokenize(text_sentences)\nprint(text_tokenizer.word_index)\nprint()\nfor sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n    print('Sequence {} in x'.format(sample_i + 1))\n    print('  Input:  {}'.format(sent))\n    print('  Output: {}'.format(token_sent))","559bd4af":"from keras.preprocessing.sequence import pad_sequences\ndef pad(x, length=None):\n    \"\"\"\n    Pad x\n    :param x: List of sequences.\n    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n    :return: Padded numpy array of sequences\n    \"\"\"\n    # TODO: Implement\n    padding=pad_sequences(x,padding='post',maxlen=length)\n    return padding\n\n# Pad Tokenized output\ntest_pad = pad(text_tokenized)\nfor sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n    print('Sequence {} in x'.format(sample_i + 1))\n    print('  Input:  {}'.format(np.array(token_sent)))\n    print('  Output: {}'.format(pad_sent))","90b09ceb":"def preprocess_input(df):\n    sentences = []\n    for i in df.index:\n        sentences.append(\"<SOS> \"+ str(df.loc[i,:]['Line 1']) + str(df.loc[i,:]['Line 2']+\" <EOS>\" ))\n    text_tokenized, text_tokenizer = tokenize(sentences)\n    text_pad = pad(text_tokenized)\n    return text_pad, text_tokenizer, sentences\n\n\nEnglish , token_English , sent_English = preprocess_input(Eng)\nFrench , token_French , sent_French = preprocess_input(Fr)","e0ee5f22":"print(\"English vocabulary size:\", len(token_English.word_index))\nprint(\"French vocabulary size:\", len(token_French.word_index))\nprint(\"English Longest sentence size:\", len(English[0]))\nprint(\"French Longest sentence size:\", len(French[0]))","44ff41bc":"vocab_eng = len(token_English.word_index)\nvocab_fr = len(token_French.word_index)\nmaxlen_eng = len(English[0])\nmaxlen_fr = len(French[0])","712b8f63":"def get_angles(pos, i, d_model):\n  angle_rates = 1 \/ np.power(10000, (2 * (i\/\/2)) \/ np.float32(d_model))\n  return pos * angle_rates\n\ndef positional_encoding(position, d_model):\n  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                          np.arange(d_model)[np.newaxis, :],\n                          d_model)\n  # apply sin to even indices in the array; 2i\n  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n  # apply cos to odd indices in the array; 2i+1\n  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n  pos_encoding = angle_rads[np.newaxis, ...]\n  return tf.cast(pos_encoding, dtype=tf.float32)\n","d3d0ab18":"import matplotlib.pyplot as plt\npos_encoding = positional_encoding(50, 512)\nprint (pos_encoding.shape)\n","29a135c4":"plt.figure(figsize=(20,6))\nplt.pcolormesh(pos_encoding[0], cmap='RdBu')\nplt.xlabel('Depth')\nplt.xlim((0, 512))\nplt.ylabel('Position')\nplt.colorbar()\nplt.show()","4055e0b9":"def create_padding_mask(seq):\n  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n  \n  # add extra dimensions to add the padding\n  # to the attention logits.\n  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\nx = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\ncreate_padding_mask(x)\n","1f1e8b9b":"def create_look_ahead_mask(size):\n  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n  return mask  # (seq_len, seq_len)\nx = tf.random.uniform((1, 3))\ntemp = create_look_ahead_mask(x.shape[1])\nprint(x)\nprint(temp)","dc003d61":"def scaled_dot_product_attention(q, k, v, mask):\n  \"\"\"Calculate the attention weights.\n  q, k, v must have matching leading dimensions.\n  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n  The mask has different shapes depending on its type(padding or look ahead) \n  but it must be broadcastable for addition.\n  \n  Args:\n    q: query shape == (..., seq_len_q, depth)\n    k: key shape == (..., seq_len_k, depth)\n    v: value shape == (..., seq_len_v, depth_v)\n    mask: Float tensor with shape broadcastable \n          to (..., seq_len_q, seq_len_k). Defaults to None.\n    \n  Returns:\n    output, attention_weights\n  \"\"\"\n\n  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n  \n  # scale matmul_qk\n  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n  scaled_attention_logits = matmul_qk \/ tf.math.sqrt(dk)\n\n  # add the mask to the scaled tensor.\n  if mask is not None:\n    scaled_attention_logits += (mask * -1e9)  \n\n  # softmax is normalized on the last axis (seq_len_k) so that the scores\n  # add up to 1.\n  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n  return output, attention_weights","05c4aec4":"class MultiHeadAttention(tf.keras.layers.Layer):\n  def __init__(self, d_model, num_heads):\n    super(MultiHeadAttention, self).__init__()\n    self.num_heads = num_heads\n    self.d_model = d_model\n    \n    assert d_model % self.num_heads == 0\n    \n    self.depth = d_model \/\/ self.num_heads\n    \n    self.wq = tf.keras.layers.Dense(d_model)\n    self.wk = tf.keras.layers.Dense(d_model)\n    self.wv = tf.keras.layers.Dense(d_model)\n    \n    self.dense = tf.keras.layers.Dense(d_model)\n        \n  def split_heads(self, x, batch_size):\n    \"\"\"Split the last dimension into (num_heads, depth).\n    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n    \"\"\"\n    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n    return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n  def call(self, v, k, q, mask):\n    batch_size = tf.shape(q)[0]\n    \n    q = self.wq(q)  # (batch_size, seq_len, d_model)\n    k = self.wk(k)  # (batch_size, seq_len, d_model)\n    v = self.wv(v)  # (batch_size, seq_len, d_model)\n    \n    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n    \n    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n    scaled_attention, attention_weights = scaled_dot_product_attention(\n        q, k, v, mask)\n    \n    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n    concat_attention = tf.reshape(scaled_attention, \n                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n        \n    return output, attention_weights","b4b1a1b6":"# Create a MultiHeadAttention layer to try out. At each location in the sequence, y, the MultiHeadAttention runs \n# all 8 attention heads across all other locations in the sequence, returning a new vector of the same length \n# at each location.\ntemp_mha = MultiHeadAttention(d_model=512, num_heads=8)\ny = tf.random.uniform((10, 60, 512))  # (batch_size, encoder_sequence, d_model)\nout, attn = temp_mha(y, k=y, q=y, mask=None)\ny.shape,out.shape, attn.shape","1b27150e":"def point_wise_feed_forward_network(d_model, dff):\n  return tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n  ])\nsample_ffn = point_wise_feed_forward_network(512, 2048)\nsample_ffn(tf.random.uniform((64, 50, 512))).shape","af17b323":"class EncoderLayer(tf.keras.layers.Layer):\n  def __init__(self, d_model, num_heads, dff, rate=0.1):\n    super(EncoderLayer, self).__init__()\n\n    self.mha = MultiHeadAttention(d_model, num_heads)\n    self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    \n    self.dropout1 = tf.keras.layers.Dropout(rate)\n    self.dropout2 = tf.keras.layers.Dropout(rate)\n    \n  def call(self, x, training, mask):\n\n    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n    attn_output = self.dropout1(attn_output, training=training)\n    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model) , residual conection\n    \n    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n    ffn_output = self.dropout2(ffn_output, training=training)\n    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n    \n    return out2\nsample_encoder_layer = EncoderLayer(512, 8, 2048)\n\nsample_encoder_layer_output = sample_encoder_layer(\n    tf.random.uniform((64, 43, 512)), False, None)\n\nsample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)","4d43b30a":"class DecoderLayer(tf.keras.layers.Layer):\n  def __init__(self, d_model, num_heads, dff, rate=0.1):\n    super(DecoderLayer, self).__init__()\n\n    self.mha1 = MultiHeadAttention(d_model, num_heads)\n    self.mha2 = MultiHeadAttention(d_model, num_heads)\n\n    self.ffn = point_wise_feed_forward_network(d_model, dff)\n \n    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    \n    self.dropout1 = tf.keras.layers.Dropout(rate)\n    self.dropout2 = tf.keras.layers.Dropout(rate)\n    self.dropout3 = tf.keras.layers.Dropout(rate)\n    \n    \n  def call(self, x, enc_output, training, \n           look_ahead_mask, padding_mask):\n    # enc_output.shape == (batch_size, input_seq_len, d_model)\n\n    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n    attn1 = self.dropout1(attn1, training=training)\n    out1 = self.layernorm1(attn1 + x)\n    \n    attn2, attn_weights_block2 = self.mha2(\n        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n    attn2 = self.dropout2(attn2, training=training)\n    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n    \n    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n    ffn_output = self.dropout3(ffn_output, training=training)\n    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n    \n    return out3, attn_weights_block1, attn_weights_block2\n\nsample_decoder_layer = DecoderLayer(512, 8, 2048)\n\nsample_decoder_layer_output, _, _ = sample_decoder_layer(\n    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n    False, None, None)\n\nsample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)","10a1882f":"class Encoder(tf.keras.layers.Layer):\n  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n               maximum_position_encoding, rate=0.1):\n    super(Encoder, self).__init__()\n\n    self.d_model = d_model\n    self.num_layers = num_layers\n    \n    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n    self.pos_encoding = positional_encoding(maximum_position_encoding, \n                                            self.d_model)\n    \n    \n    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n                       for _ in range(num_layers)]\n  \n    self.dropout = tf.keras.layers.Dropout(rate)\n        \n  def call(self, x, training, mask):\n\n    seq_len = tf.shape(x)[1]\n    \n    # adding embedding and position encoding.\n    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n    x += self.pos_encoding[:, :seq_len, :]\n\n    x = self.dropout(x, training=training)\n    \n    for i in range(self.num_layers):\n      x = self.enc_layers[i](x, training, mask)\n    \n    return x  # (batch_size, input_seq_len, d_model)\nsample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n                         dff=2048, input_vocab_size=8500,\n                         maximum_position_encoding=10000)\ntemp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n\nsample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n\nprint (temp_input.shape , sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)","67faca20":"class Decoder(tf.keras.layers.Layer):\n  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n               maximum_position_encoding, rate=0.1):\n    super(Decoder, self).__init__()\n\n    self.d_model = d_model\n    self.num_layers = num_layers\n    \n    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n    \n    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n                       for _ in range(num_layers)]\n    self.dropout = tf.keras.layers.Dropout(rate)\n    \n  def call(self, x, enc_output, training, \n           look_ahead_mask, padding_mask):\n\n    seq_len = tf.shape(x)[1]\n    attention_weights = {}\n    \n    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n    x += self.pos_encoding[:, :seq_len, :]\n    \n    x = self.dropout(x, training=training)\n\n    for i in range(self.num_layers):\n      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n                                             look_ahead_mask, padding_mask)\n      \n      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n    \n    # x.shape == (batch_size, target_seq_len, d_model)\n    return x, attention_weights\n\nsample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n                         dff=2048, target_vocab_size=8000,\n                         maximum_position_encoding=5000)\ntemp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n\noutput, attn = sample_decoder(temp_input, \n                              enc_output=sample_encoder_output, \n                              training=False,\n                              look_ahead_mask=None, \n                              padding_mask=None)\n\noutput.shape, attn['decoder_layer2_block2'].shape","88ebf3ca":"class Transformer(tf.keras.Model):\n  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n               target_vocab_size, pe_input, pe_target, rate=0.1):\n    super(Transformer, self).__init__()\n\n    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n                           input_vocab_size, pe_input, rate)\n\n    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n                           target_vocab_size, pe_target, rate)\n\n    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n    \n  def call(self, inp, tar, training, enc_padding_mask, \n           look_ahead_mask, dec_padding_mask):\n\n    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n    \n    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n    dec_output, attention_weights = self.decoder(\n        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n    \n    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n    \n    return final_output, attention_weights\n\nsample_transformer = Transformer(\n    num_layers=2, d_model=512, num_heads=8, dff=2048, \n    input_vocab_size=8500, target_vocab_size=8000, \n    pe_input=10000, pe_target=6000)\n\ntemp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\ntemp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n\nfn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n                               enc_padding_mask=None, \n                               look_ahead_mask=None,\n                               dec_padding_mask=None)\n\nfn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)","6f198482":"num_layers = 4\nd_model = 128\ndff = 512\nnum_heads = 8\n\ninput_vocab_size = len(token_English.word_index) + 2\ntarget_vocab_size = len(token_French.word_index) + 2\ndropout_rate = 0.1","b248803a":"\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n  def __init__(self, d_model, warmup_steps=4000):\n    super(CustomSchedule, self).__init__()\n    \n    self.d_model = d_model\n    self.d_model = tf.cast(self.d_model, tf.float32)\n\n    self.warmup_steps = warmup_steps\n    \n  def __call__(self, step):\n    arg1 = tf.math.rsqrt(step)\n    arg2 = step * (self.warmup_steps ** -1.5)\n    \n    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\nlearning_rate = CustomSchedule(d_model)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n                                     epsilon=1e-9)","6182e28b":"loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\ndef loss_function(real, pred):\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n  loss_ = loss_object(real, pred)\n\n  mask = tf.cast(mask, dtype=loss_.dtype)\n  loss_ *= mask\n  \n  return tf.reduce_sum(loss_)\/tf.reduce_sum(mask)\n\n\ndef accuracy_function(real, pred):\n  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n  \n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n  accuracies = tf.math.logical_and(mask, accuracies)\n\n  accuracies = tf.cast(accuracies, dtype=tf.float32)\n  mask = tf.cast(mask, dtype=tf.float32)\n  return tf.reduce_sum(accuracies)\/tf.reduce_sum(mask)\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.Mean(name='train_accuracy')","b7d058d7":"transformer = Transformer(num_layers, d_model, num_heads, dff,\n                          input_vocab_size, target_vocab_size, \n                          pe_input=input_vocab_size, \n                          pe_target=target_vocab_size,\n                          rate=dropout_rate)\n","1bd6e631":"def create_masks(inp, tar):\n  # Encoder padding mask\n  enc_padding_mask = create_padding_mask(inp)\n  \n  # Used in the 2nd attention block in the decoder.\n  # This padding mask is used to mask the encoder outputs.\n  dec_padding_mask = create_padding_mask(inp)\n  \n  # Used in the 1st attention block in the decoder.\n  # It is used to pad and mask future tokens in the input received by \n  # the decoder.\n  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n  dec_target_padding_mask = create_padding_mask(tar)\n  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n  \n  return enc_padding_mask, combined_mask, dec_padding_mask","636e2e49":"# chk points\ncheckpoint_path = \".\/train\"\n\nckpt = tf.train.Checkpoint(transformer=transformer,\n                           optimizer=optimizer)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n\n# if a checkpoint exists, restore the latest checkpoint.\nif ckpt_manager.latest_checkpoint:\n  ckpt.restore(ckpt_manager.latest_checkpoint)\n  print ('Latest checkpoint restored!!')","6c08ab4b":"EPOCHS = 5","8b8811d8":"# The @tf.function trace-compiles train_step into a TF graph for faster\n# execution. The function specializes to the precise shape of the argument\n# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n# batch sizes (the last batch is smaller), use input_signature to specify\n# more generic shapes.\n\n\ntrain_step_signature = [\n    tf.TensorSpec(shape=(61, 17), dtype=tf.int64),\n    tf.TensorSpec(shape=(61,23), dtype=tf.int64),\n]\n@tf.function(input_signature=train_step_signature)\ndef train_step(inp, tar):\n  tar_inp = tar[:, :-1]\n  tar_real = tar[:, 1:]\n  \n  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n  #tf.print(enc_padding_mask.shape, combined_mask.shape, dec_padding_mask.shape)\n  with tf.GradientTape() as tape:\n        #tf.print(inp.shape , tar_inp.shape, tar_real.shape )\n        predictions, _ = transformer(inp, tar_inp, \n                                     True, \n                                     enc_padding_mask, \n                                     combined_mask, \n                                     dec_padding_mask)\n        \n        #tf.print(predictions.shape)\n        loss = loss_function(tar_real, predictions)\n        #tf.print(loss)\n\n  gradients = tape.gradient(loss, transformer.trainable_variables)    \n  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n  \n  train_loss(loss)\n  train_accuracy(accuracy_function(tar_real, predictions))","a8cb345b":"start_pt = 0\nbatch_size = 61\ninp = tf.convert_to_tensor(np.array(English[start_pt:start_pt+batch_size]),dtype=tf.int64)\ntar = tf.convert_to_tensor(np.array(French[start_pt:start_pt+batch_size]),dtype=tf.int64)\ntrain_step(inp, tar)","99797dcd":"from tensorflow import keras\nimport time\nbatch_size = 61\nfor epoch in range(EPOCHS):\n  start = time.time()\n  \n  train_loss.reset_states()\n  train_accuracy.reset_states()\n  \n  start_pt = 0\n  # inp -> portuguese, tar -> english\n  for i in range(int(len(English)\/batch_size)):\n    inp = tf.convert_to_tensor(np.array(English[start_pt:start_pt+batch_size]),dtype=tf.int64)\n    tar = tf.convert_to_tensor(np.array(French[start_pt:start_pt+batch_size]),dtype=tf.int64)\n    start_pt = start_pt + batch_size\n    train_step(inp, tar)\n    \n    if i % 100 == 0:\n      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n          epoch + 1, i, train_loss.result(), train_accuracy.result()))\n      \n  if (epoch + 1) % 5 == 0:\n    ckpt_save_path = ckpt_manager.save()\n    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n                                                         ckpt_save_path))\n    \n  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n                                                train_loss.result(), \n                                                train_accuracy.result()))\n\n  print ('Total time taken for that epoch: {} secs\\n'.format(time.time() - start))","77540868":"def evaluate(sentence):\n  sentence[0] = '<SOS> '+sentence[0]+'<EOS>'\n  sentence = pad(token_English.texts_to_sequences(sentence) , length = maxlen_eng)\n  \n  sentence = tf.convert_to_tensor(np.array(sentence),dtype=tf.int64)\n  \n  \n  \n  decoder_input = tf.convert_to_tensor(np.array(token_French.texts_to_sequences(['SOS'])),dtype=tf.int64)\n                                       \n  \n  \n  for i in range(40):\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n        sentence, decoder_input)\n    #tf.print(enc_padding_mask.shape, combined_mask.shape, dec_padding_mask.shape)\n    # predictions.shape == (batch_size, seq_len, vocab_size)\n    predictions, attention_weights = transformer(sentence, \n                                                 decoder_input,\n                                                 False,\n                                                 enc_padding_mask,\n                                                 combined_mask,\n                                                 dec_padding_mask)\n    \n    # select the last word from the seq_len dimension\n    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n\n    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int64)\n    \n    #tf.print(predicted_id,decoder_input)\n    # return the result if the predicted_id is equal to the end token\n    if predicted_id == token_French.texts_to_sequences(['EOS']):\n      return tf.squeeze(decoder_input, axis=0), attention_weights , sentence\n    \n    # concatentate the predicted_id to the output which is given to the decoder\n    # as its input.\n    decoder_input = tf.concat([decoder_input, predicted_id], axis=1)\n\n  return tf.squeeze(decoder_input, axis=0), attention_weights , sentence  ","7066973a":"def plot_attention_weights(attention, sentence_vec, result_vec, sentence , result, layer):\n  fig = plt.figure(figsize=(16, 8))\n  \n  \n  attention = tf.squeeze(attention[layer], axis=0)\n  sent = sentence.split(\" \")\n  res = result.split(\" \")\n  slt = []\n  \n  for s in sent:\n    slt.append(s)\n  rlt = []\n  \n  for s in res:\n    rlt.append(s)\n  res = result.split(\" \")\n  for head in range(attention.shape[0]):\n    ax = fig.add_subplot(2, 4, head+1)\n    \n    # plot the attention weights\n    ax.matshow(attention[head][:-1, :], cmap='viridis')\n\n    fontdict = {'fontsize': 10}\n    \n    ax.set_xticks(range(len(slt)))\n    ax.set_yticks(range(len(rlt)))\n    \n    ax.set_ylim(len(result_vec)-1.5, -0.5)\n    \n    ax.set_xticklabels(\n        slt, \n        fontdict=fontdict, rotation=90)\n    \n    ax.set_yticklabels(rlt, \n                       fontdict=fontdict)\n    \n    ax.set_xlabel('Head {}'.format(head+1))\n  \n  plt.tight_layout()\n  plt.show()\n","a9de71b9":"def French(word):\n    sent = \"\"\n    for i in word:\n        if i != 2:\n            sent = sent + [key for key, value in token_French.word_index.items() if value == i][0]+\" \"\n    return sent","dc0685d2":"from random import randint\n\ndef result(samples, plot=False):\n for i in range(samples):\n    value = -1\n    while value not in Eng.index:\n        value = randint(0, Eng.shape[0]\/2)\n    sent = [Eng.loc[value,:]['Line 1']+\" \"+Eng.loc[value,:]['Line 2']]\n    real = [Fr.loc[value,:]['Line 1']+\" \"+Fr.loc[value,:]['Line 2']]\n    print('En          :  '+sent[0])\n    print('Fr Actual :    '+real[0])\n    decoded , attn_wt, sentence = evaluate(sent)\n    print('Fr Predicted : '+French(decoded))\n    print()\n    print()\n    if plot:\n        plot_attention_weights(attn_wt,sentence,decoded,sent[0],French(decoded),'decoder_layer4_block2')","f3ea38b8":"result(25)","f59a28fd":"result(5,True)","a7226544":"## Tokenize ","3d7b0ec9":"# Transformer \n\n","269bd465":"\n# Evaluate","c2b62561":"### Encoder layer","c2548e88":"## Decoder","0660fe5c":"# Compile Model","07775747":"## Multi-head attention:\n","d2a95bcf":"### Decoder layer","7576751f":"## Encoder and decoder","a8386d18":"# Data Reading","466c2ac2":"# Set hyperparameters","f058a583":"# Training","a5a36060":"# Optimizer","1515df7d":"## Look Ahead Mask","b2bf73f3":"# Loss and metrics","c0fba04c":"## Scaled Product","85b6a690":"# Masking","884168ac":"## Encoder","02343f49":"## Point wise feed forward network","107cec88":"## Padding","7842de1c":"## Positional encoding"}}