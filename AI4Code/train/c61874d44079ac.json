{"cell_type":{"cc08322b":"code","abcc128d":"code","b9f41fcf":"code","d8e64762":"code","32ad16ad":"code","b979ccde":"code","cfd198c3":"code","3de1118d":"code","c69a35bb":"code","93a2df95":"code","ef41b401":"code","bb2672ad":"code","65f559f2":"code","49be573b":"code","20151f72":"code","39f40e4c":"code","dfff1b51":"code","6e06e9bc":"code","a02f21d8":"code","bed532ac":"code","6bea3423":"code","a5954b28":"code","9857fa82":"code","a013425b":"code","f07b9dd0":"code","62d63f4d":"code","d69510ff":"code","b6896200":"markdown","785ce181":"markdown","e17ef528":"markdown","8d10222f":"markdown","41594954":"markdown"},"source":{"cc08322b":"#Import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","abcc128d":"#Reading the dataset\ndf=pd.read_csv('..\/input\/diabetes2.csv')","b9f41fcf":"#Check the first five values\ndf.head()","d8e64762":"#Checking the null values..\ndf.isnull().sum()","32ad16ad":"#Checking the meta data\ndf.info()","b979ccde":"#Doing some basic statistic\ndf.describe()","cfd198c3":"#Checking the unique values of dependent variable\ndf[\"Outcome\"].unique()","3de1118d":"#Check the dimensions\ndf.shape","c69a35bb":"#Ploting the headmap to check the correlation between all variables\nimport seaborn as sns\nsns.heatmap(df)","93a2df95":"#Ploting the histogram to check the distribution of each  columns \np = df.hist(figsize = (20,20))","ef41b401":"#Pairplot to visualize the correlation with one variable and all other variables respectively.\np=sns.pairplot(df, hue = 'Outcome')","bb2672ad":"#Checking the correlation using heatmap\nplt.figure(figsize=(12,10))  # on this line I just set the size of figure to 12 by 10.\np=sns.heatmap(df.corr(), annot=True,cmap ='RdYlGn')  # seaborn has very simple solution for heatmap","65f559f2":"#Shortcut to scale all independent variable in one go.... \nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX =  pd.DataFrame(sc_X.fit_transform(df.drop([\"Outcome\"],axis = 1),),\n        columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age'])","49be573b":"X.head()","20151f72":"\n#Separating dataset into independent and dependent variables\nX = df.iloc[:, 0:8].values\ny = df.iloc[:, -1].values\n\n\n","39f40e4c":"#Splitting into training and testing dataset....\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.3,random_state=0)","dfff1b51":"#Fitting Decisiontree into dataset\nfrom sklearn.tree import DecisionTreeClassifier\n#Creating a confusion matrix\nfrom sklearn.metrics import confusion_matrix\n#Check the accuracy\nfrom sklearn.metrics import accuracy_score\n\n\ndtree_c=DecisionTreeClassifier(criterion='entropy',random_state=0)\ndtree_c.fit(X_train,y_train)\ndtree_pred=dtree_c.predict(X_test)\ndtree_cm=confusion_matrix(y_test,dtree_pred)\nprint(\"The accuracy of DecisionTreeClassifier is:\",accuracy_score(dtree_pred,y_test))\n\n\n\n\n","6e06e9bc":"print(dtree_cm)","a02f21d8":"#Fitting Randomforest into dataset\nfrom sklearn.ensemble import RandomForestClassifier\nrdf_c=RandomForestClassifier(n_estimators=10,criterion='entropy',random_state=0)\nrdf_c.fit(X_train,y_train)\nrdf_pred=rdf_c.predict(X_test)\nrdf_cm=confusion_matrix(y_test,rdf_pred)\nprint(\"The accuracy of RandomForestClassifier is:\",accuracy_score(rdf_pred,y_test))\n","bed532ac":"print(rdf_cm)","6bea3423":"#Fitting Logistic regression into dataset\nfrom sklearn.linear_model import LogisticRegression\nlr_c=LogisticRegression(random_state=0)\nlr_c.fit(X_train,y_train)\nlr_pred=lr_c.predict(X_test)\nlr_cm=confusion_matrix(y_test,lr_pred)\nprint(\"The accuracy of  LogisticRegression is:\",accuracy_score(y_test, lr_pred))\n","a5954b28":"print(lr_cm)","9857fa82":"#Fitting KNN into dataset\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn=KNeighborsClassifier(n_neighbors=2)\nknn.fit(X_train,y_train)\nknn_pred=knn.predict(X_test)\nknn_cm=confusion_matrix(y_test,knn_pred)\nprint(\"The accuracy of KNeighborsClassifier is:\",accuracy_score(knn_pred,y_test))\n\n\n","a013425b":"print(knn_cm)","f07b9dd0":"#Fitting Naive bayes into dataset\nfrom sklearn.naive_bayes import GaussianNB\n\n\n\ngaussian=GaussianNB()\ngaussian.fit(X_train,y_train)\nbayes_pred=gaussian.predict(X_test)\nbayes_cm=confusion_matrix(y_test,bayes_pred)\nprint(\"The accuracy of naives bayes is:\",accuracy_score(bayes_pred,y_test))\n\n","62d63f4d":"print(bayes_cm)","d69510ff":"\n\n#confusion matrix.....\nplt.figure(figsize=(20,10))\nplt.subplot(2,4,3)\nplt.title(\"LogisticRegression_cm\")\nsns.heatmap(lr_cm,annot=True,cmap=\"prism\",fmt=\"d\",cbar=False)\n\nplt.subplot(2,4,5)\nplt.title(\"bayes_cm\")\nsns.heatmap(bayes_cm,annot=True,cmap=\"binary_r\",fmt=\"d\",cbar=False)\nplt.subplot(2,4,2)\nplt.title(\"RandomForest\")\nsns.heatmap(rdf_cm,annot=True,cmap=\"ocean_r\",fmt=\"d\",cbar=False)\n\nplt.subplot(2,4,1)\nplt.title(\"DecisionTree_cm\")\nsns.heatmap(dtree_cm,annot=True,cmap=\"twilight_shifted_r\",fmt=\"d\",cbar=False)\nplt.subplot(2,4,4)\nplt.title(\"kNN_cm\")\nsns.heatmap(knn_cm,annot=True,cmap=\"Wistia\",fmt=\"d\",cbar=False)\nplt.show()\n","b6896200":"# ||About the dataset||\n\nThe dataset contains.....\n\n1. Pregnancies: Number of times pregnant\n2. Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n3. BloodPressure: Diastolic blood pressure (mm Hg)\n4. SkinThickness: Triceps skin fold thickness (mm)\n5. Insulin: 2-Hour serum insulin (mu U\/ml)\n6. BMI: Body mass index (weight in kg\/(height in m)^2)\n7. DiabetesPedigreeFunction: Diabetes pedigree function\n8. Age: Age (years)\n9. Outcome: Class variable (0 or 1)","785ce181":"# ||Exercise||","e17ef528":"# ||Goal--||\n\n  We will try to build a machine learning model to accurately predict whether or not the patients in the dataset have diabetes or not?\n  ","8d10222f":"# ||Conclusion||\n\n    Looks like from the above model Logistic regression is the best fit model for diabetes dataset.","41594954":"Hello everyone....\n\nHope you all are doing well.... Today i am working on diabetes dataset but before we start i just share some tips and tricks based on my Experience...\n\nTry to be Data player before Data scientist because if you are habituated of handling the data,you can save your time when you are handling real world projects.\n \n Tricks and Tips:-\n 1. Try to spend your 80% of the time in data wrangling ,data cleaning or data preprocessing.\n 2. Daily try practice different kind of dataset everytime (Ex:-csv,tsv,image file,text file etc)\n 3. It's okay if you do mistakes ,just focus on your practice...However you will by doing mistakes.\n 4. Try to apply new shortcuts or new technique to solving the same issue.\n 5. Add a comment and write a short note if you are working on any new topic,such that if you visit the same code         once again then it would be clear for you.\n 6. Not required to work only on complex code, you can even work on simple code using the same technique.\n 7. Code should be polished such that if a new user try to understand the code,he\/she should understand it easily.\n \n \n \n These are some tricks and tips,Hope this is helpful for you!"}}