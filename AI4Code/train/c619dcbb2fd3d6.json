{"cell_type":{"bd544881":"code","7cc844e2":"code","00f64a53":"code","f694c718":"code","8647b6e2":"code","82971786":"code","ec2e74b0":"code","de4acb3a":"code","68ff13db":"code","af22b31b":"code","3a3f7ee4":"code","f9ae9a63":"code","1f75fa7b":"code","7ee5b0f4":"code","51af20a6":"code","d87e0a33":"code","df4e9fe7":"code","efe36cf1":"code","627d7bc2":"code","6a5425df":"code","8617c3c7":"code","f9e183c8":"code","fc0187a8":"code","f2f144b1":"code","295d2f06":"code","84cfbf8e":"code","de2b586f":"code","2cecb251":"code","6765ed6f":"code","1bd03a16":"code","7f25ec4d":"code","9e9f8136":"markdown","26a52451":"markdown","4730d5bb":"markdown","25ab144c":"markdown","dcd03f3c":"markdown","1e9b0d63":"markdown"},"source":{"bd544881":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7cc844e2":"import pandas as pd \nimport numpy as np \nimport pickle \nimport matplotlib.pyplot as plt \nfrom scipy import stats \nimport tensorflow as tf \nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn import metrics \nfrom sklearn.model_selection import train_test_split \nfrom collections import Counter\nfrom cycler import cycler\nfrom scipy.stats import norm, skew, probplot\nfrom scipy.optimize import curve_fit\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import LSTM,BatchNormalization\nfrom keras.layers.core import Dense, Dropout\nfrom keras.utils import np_utils\nfrom keras.optimizers import RMSprop, Adam\nfrom keras.models import Model\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.layers import Input, Activation, Flatten, Dense, Dropout\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.advanced_activations import ELU\nfrom keras.optimizers import Adam\nfrom keras.utils import to_categorical, np_utils\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom keras.preprocessing.image import ImageDataGenerator\nimport keras.backend as K\nfrom keras.layers import Input, Embedding, LSTM, Dense, GRU\nfrom keras.layers import Concatenate, Dense, LSTM, Input, concatenate","00f64a53":"# Load datasets\ntrain_df = pd.read_csv('\/kaggle\/input\/human-activity-recognition-with-rnn\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/human-activity-recognition-with-rnn\/test.csv')\ntrain_df.head()","f694c718":"for col in train_df.columns:\n    print(col)\n\n","8647b6e2":"# Group and count main names of columns\npd.DataFrame.from_dict(Counter([col.split('-')[0].split('(')[0] for col in train_df.columns]), orient='index').rename(columns={0:'count'}).sort_values('count', ascending=False)","82971786":"# check the null values present in the dataframe\nprint('Null Values In DataFrame: {}\\n'.format(train_df.isna().sum().sum()))\ntrain_df.info()","ec2e74b0":"Y_train =  train_df.pop('activity')","de4acb3a":"Y_train_labels = Y_train.map({1: 'WALKING', 2:'WALKING_UPSTAIRS',3:'WALKING_DOWNSTAIRS',\\\n                       4:'SITTING', 5:'STANDING',6:'LAYING'})","68ff13db":"print(Y_train_labels)","af22b31b":"new_df = train_df\nnew_df['activity'] = Y_train\nnew_df['activity_labels'] = Y_train_labels\nnew_df.head()","3a3f7ee4":"\n\n# Check subject wise\n\nsns.set_style('whitegrid')\nplt.rcParams['font.family'] = 'Dejavu Sans'\n\nplt.figure(figsize=(16,8))\nplt.title('Data provided by each user', fontsize=20)\nsns.countplot(x='subject',hue='activity_labels', data = new_df)\nplt.show()","f9ae9a63":"# Check the labels distribution for the whole data\nplt.title('No of Datapoints per Activity', fontsize=15)\nsns.countplot(new_df.activity_labels)\nplt.xticks(rotation=90)\nplt.show()\n","1f75fa7b":"columns = new_df.columns\n\n# Removing '()' from column names\ncolumns = columns.str.replace('[()]','')\ncolumns = columns.str.replace('[-]', '')\ncolumns = columns.str.replace('[,]','')\n\nnew_df.columns = columns\n","7ee5b0f4":"new_df.columns","51af20a6":"X_train, X_val, y_train, y_val = train_test_split(new_df,Y_train,test_size = 0.2,shuffle = True)","d87e0a33":"x_train = X_train.drop(['subject', 'activity', 'activity_labels', 'id'], axis=1)\ny_train_name = X_train.activity_labels","df4e9fe7":"x_val = X_val.drop(['subject', 'activity', 'activity_labels','id'], axis=1)\ny_val_name = X_val.activity_labels","efe36cf1":"print('X_train and y_train : ({},{})'.format(x_train.shape, y_train.shape))\nprint('X_test  and y_test  : ({},{})'.format(x_val.shape, y_val.shape))","627d7bc2":"# Initializing parameters\nepochs = 30\nbatch_size = 32\nn_hidden = 32\nnum_classes = 6","6a5425df":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nx_train=sc.fit_transform(x_train)\nx_val=sc.transform(x_val)","8617c3c7":"from sklearn.preprocessing import LabelEncoder\nencoder=LabelEncoder()\ny_train=encoder.fit_transform(y_train)\ny_train=pd.get_dummies(y_train).values\n\nencoder=LabelEncoder()\ny_val=encoder.fit_transform(y_val)\ny_val=pd.get_dummies(y_val).values\nprint(y_train.shape, y_val.shape)","f9e183c8":"# Reshaping inputs\nfrom numpy import zeros, newaxis\nxx_train =  x_train[:, :, newaxis]\nxx_train.shape\nxx_val = x_val[:, :, newaxis]\nxx_val.shape","fc0187a8":"test_df_2 = test_df.drop(columns = ['id', 'subject'])\nx_test=test_df_2.to_numpy()\nfrom sklearn.preprocessing import StandardScaler\nX_test = sc.transform(x_test)\nprint(X_test.shape)\nX_test=X_test.reshape((2947,561,1))\nprint(X_test.shape)","f2f144b1":"'''\ndef model1(input_dim, num_classes):\n    inp = Input(shape = (input_dim,1))\n    x = keras.layers.LSTM(32,return_sequences=True)(inp)\n    x = Dropout(0.4)(x)\n    x = keras.layers.LSTM(64, return_sequences = True)(x)\n    x = Dropout(0.4)(x)\n    x = Dense(128,activation = 'relu')(x)\n    x = Dropout(0.4)(x)\n    x = Dense(64, activation = 'relu')(x)\n    x = Dropout(0.4)(x)\n    layer1 = x\n    y = GRU(32,return_sequences=True)(inp)\n    y = Dropout(0.4)(y)\n    y = GRU(64, return_sequences = True)(y)\n    y = Dropout(0.4)(y)\n    y = Dense(128,activation = 'relu')(y)\n    y = Dropout(0.4)(y)\n    y = Dense(64, activation = 'relu')(y)\n    y = Dropout(0.4)(y)\n    layer2 = y\n    concat = concatenate([layer1,layer2])\n    z = Flatten()(concat)\n    z = Dense(32, activation = 'relu')(z)\n    z = Dropout(0.4)(z)\n    z = BatchNormalization()(z)\n    z = Dense(num_classes, activation='softmax')(z)\n    model = Model(inputs = inp, outputs = z, name='Ensemble')\n    return model\n    \n    \ncp_filepath = \"Best_ensemble1_trial1_best_acc_model1.h5\"\nes_callback = keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", min_delta=0, patience=7,restore_best_weights= True)\nLRPlateu = ReduceLROnPlateau(monitor='val_accuracy',mode='max',factor=0.5, patience=7, min_lr=0.00001, verbose=1)\ncheckpoint = ModelCheckpoint(cp_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\nCALLBACKS = [LRPlateu, checkpoint, es_callback]\nmodel_1 = model1(x_train.shape[1],num_classes)\nmodel_1.summary()\nimport keras\n# Compiling the model\nmodel_1.compile(optimizer = 'adam',loss='categorical_crossentropy', metrics=['accuracy', keras.metrics.Precision(name='precision'), keras.metrics.Recall(name='recall')])\nmodel_1.fit(xx_train,\n          y_train,\n          batch_size=batch_size,\n          validation_data=(xx_val, y_val),\n          epochs=epochs,\n          callbacks = CALLBACKS)\nscore = model_1.evaluate(xx_val, y_val)\nprint(score)\npreds = model_1.predict(X_test,batch_size=1).argmax(axis=1)+1\nprint(preds)\nmodel = model1(x_train.shape[1],num_classes)\nmodel.compile(optimizer = 'adam',loss='categorical_crossentropy', metrics=['accuracy', keras.metrics.Precision(name='precision'), keras.metrics.Recall(name='recall')])\nmodel.load_weights('.\/Best_ensemble1_trial1_best_acc_model1.h5')\nmodel.summary()\nnp.savetxt(\"submissions_trial_1_ensemble_mddel1.csv\", np.dstack((np.arange(0, preds.size),preds))[0],\"%d,%d\",header=\"id, activity\")\nprint(preds[:10])","295d2f06":"def model2(input_dim, num_classes):\n    inp = Input(shape = (input_dim,1))\n    x = keras.layers.LSTM(32,return_sequences=True)(inp)\n    x = Dropout(0.4)(x)\n    x = keras.layers.LSTM(64, return_sequences = True)(x)\n    x = Dropout(0.4)(x)\n    layer1 = x\n    y = GRU(32,return_sequences=True)(inp)\n    y = Dropout(0.4)(y)\n    y = GRU(64, return_sequences = True)(y)\n    y = Dropout(0.4)(y)\n    layer2 = y\n    concat = concatenate([layer1,layer2])\n    z = Flatten()(concat)\n    z = Dense(64, activation = 'relu')(z)\n    z = Dropout(0.4)(z)\n    z = Dense(32, activation = 'relu')(z)\n    z = Dropout(0.4)(z)\n    z = BatchNormalization()(z)\n    z = Dense(num_classes, activation='softmax')(z)\n    model = Model(inputs = inp, outputs = z, name='Ensemble')\n    return model","84cfbf8e":"model_2 = model2(x_train.shape[1],num_classes)\nmodel_2.summary()","de2b586f":"cp_filepath = \"Best_ensemble2_trial4_best_acc_model2_val0.2.h5\"\nes_callback = keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", min_delta=0, patience=7,restore_best_weights= True)\nLRPlateu = ReduceLROnPlateau(monitor='val_accuracy',mode='max',factor=0.5, patience=7, min_lr=0.00001, verbose=1)\ncheckpoint = ModelCheckpoint(cp_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\nCALLBACKS = [LRPlateu, checkpoint, es_callback]","2cecb251":"import keras\n# Compiling the model\nmodel_2 = model2(x_train.shape[1],num_classes)\nmodel_2.summary()\nmodel_2.compile(optimizer = 'adam',loss='categorical_crossentropy', metrics=['accuracy', keras.metrics.Precision(name='precision'), keras.metrics.Recall(name='recall')])\nmodel_2.fit(xx_train,\n          y_train,\n          batch_size=batch_size,\n          validation_data=(xx_val, y_val),\n          epochs=epochs,\n          callbacks=CALLBACKS)\nscore = model_2.evaluate(xx_val, y_val)\nprint(score)\npreds = model_2.predict(X_test,batch_size=1).argmax(axis=1)+1\nprint(preds)","6765ed6f":"model = model2(x_train.shape[1],num_classes)\nmodel.compile(optimizer = 'rmsprop',loss='categorical_crossentropy', metrics=['accuracy', keras.metrics.Precision(name='precision'), keras.metrics.Recall(name='recall')])\nmodel.load_weights('..\/input\/best-weights\/Best_ensemble_w__best_acc.h5')\nmodel.summary()","1bd03a16":"score = model.evaluate(xx_val, y_val)\nprint(score)\npreds = model.predict(X_test,batch_size=1).argmax(axis=1)+1\nprint(preds)","7f25ec4d":"np.savetxt(\"submissions_final.csv\", np.dstack((np.arange(0, preds.size),preds))[0],\"%d,%d\",header=\"id, activity\")\nprint(preds[:10])","9e9f8136":"# Architecture","26a52451":"## **Create  Test Set**","4730d5bb":"# Evaluation","25ab144c":"# **Model**","dcd03f3c":"# **Check for Data imbalance**","1e9b0d63":"# Renaming column names for easy usage"}}