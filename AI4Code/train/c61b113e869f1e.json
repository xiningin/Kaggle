{"cell_type":{"36dd8bc2":"code","97eaa3c2":"code","455d2d67":"code","42abd147":"code","2a9f9ed4":"code","2ff3daa9":"code","77beb894":"code","20f79f77":"code","49bda3e8":"code","9a972384":"code","3ca52bcf":"code","dd648ddd":"markdown","89848ac9":"markdown","3bd6210c":"markdown","fb633ff5":"markdown","81dc314a":"markdown","e8e773b1":"markdown","4a2d9816":"markdown","48bf028c":"markdown","b772f7d7":"markdown","bcef87b4":"markdown","5d9ce98b":"markdown","9c2800c6":"markdown"},"source":{"36dd8bc2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","97eaa3c2":"import pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nimport string\n\nurl = \"https:\/\/iaa.ewr.govt.nz\/PublicRegister\/Lastname.aspx?FirstLetter=a\"\nrequest = requests.get(url)\nsoup = BeautifulSoup(request.content, \"lxml\")\nadvisorlist = soup.find_all('a')","455d2d67":"advisorlist2 = soup.find_all(\"td\")","42abd147":"advisorlinks = []\n\nfor letter in string.ascii_lowercase:\n    url = \"https:\/\/iaa.ewr.govt.nz\/PublicRegister\/Lastname.aspx?FirstLetter=\" + letter\n    r = requests.get(url)\n    soup = BeautifulSoup(r.content, \"lxml\")\n    advisorlist = soup.find_all(\"td\")\n    for bin in advisorlist:\n        for link in bin.find_all('a', href = True):\n            advisorlinks.append(\"https:\/\/iaa.ewr.govt.nz\"+ link['href'])\n            \nprint(advisorlinks[0])\nprint(len(advisorlinks))","2a9f9ed4":"try: \n    testlink = \"https:\/\/iaa.ewr.govt.nz\/PublicRegister\/View.aspx?search=1&p=1&adviserNumber=200901358\"\n    rtest = requests.get(testlink)\n    soup = BeautifulSoup(rtest.content, \"lxml\")\n\n    name = soup.find('h3', class_ =\"DisplayName\").text.strip()\n    phone = soup.find('span', id=\"ctl00_maincontent_ucIAADetails_PhoneNumber\").text.strip()\n    mobile = soup.find('span', id=\"ctl00_maincontent_ucIAADetails_Mobile\").text.strip()\n    email = soup.find('a', id=\"ctl00_maincontent_ucIAADetails_EmailLink\").text.strip()\n    website = soup.find('a', id=\"ctl00_maincontent_ucIAADetails_WebUrlLink\").text.strip()\n    licencestatus = soup.find('span', id=\"ctl00_maincontent_ucIAADetails_LicenceStatus\").text.strip()\n    licenceclass = soup.find('span', id=\"ctl00_maincontent_ucIAADetails_fkLicence\").text.strip()\n    licenceexpirydate = soup.find('span', id=\"ctl00_maincontent_ucIAADetails_ExpiryDate\").text.strip()\n    businessaddress = soup.find('span', id=\"ctl00_maincontent_ucIAADetails_BusinessAddress\").get_text(separator=\", \").strip()\n    serviceaddress = soup.find('span', id=\"ctl00_maincontent_ucIAADetails_ServiceAddress\").get_text(separator=\", \").strip()\n\n    advisordata = {\n        'name': name,\n        'phone': phone, \n        'mobile': mobile,\n        'email': email,\n        'website': website,\n        'licencestatus': licencestatus,\n        'licenceclass': licenceclass, \n        'licenceexpirydate': licenceexpirydate,\n        'businessaddress': businessaddress,\n        'serviceaddress': serviceaddress  \n    }\nexcept: \n    print(\"Error occured\")","2ff3daa9":"testlink = \"https:\/\/iaa.ewr.govt.nz\/PublicRegister\/View.aspx?search=1&p=1&adviserNumber=200901358\"\nrtest = requests.get(testlink)\nsoup = BeautifulSoup(rtest.content, \"lxml\")\n\nname = soup.find('h3', class_ =\"DisplayName\").text.strip()\n\ntry:\n    phone = soup.find('span', id=\"ctl00_maincontent_ucIAADetails_PhoneNumber\").text.strip()\nexcept: \n    phone = \"NA\"\n        \ntry:\n    mobile = soup.find('span', id=\"ctl00_maincontent_ucIAADetails_Mobile\").text.strip()\nexcept:\n    mobile = \"NA\"\n\ntry:\n    email = soup.find('a', id=\"ctl00_maincontent_ucIAADetails_EmailLink\").text.strip()\nexcept: \n    email = \"NA\"\n        \ntry:\n    website = soup.find('a', id=\"ctl00_maincontent_ucIAADetails_WebUrlLink\").text.strip()\nexcept: \n    website = \"NA\"\n        \nlicencestatus = soup.find('span', id=\"ctl00_maincontent_ucIAADetails_LicenceStatus\").text.strip()\n    \nlicenceclass = soup.find('span', id=\"ctl00_maincontent_ucIAADetails_fkLicence\").text.strip()\n    \nlicenceexpirydate = soup.find('span', id=\"ctl00_maincontent_ucIAADetails_ExpiryDate\").text.strip()\n    \ntry:\n    businessaddress = soup.find('span', id=\"ctl00_maincontent_ucIAADetails_BusinessAddress\").get_text(separator=\", \").strip()\nexcept:\n    businessaddress = \"NA\"\n        \ntry:\n    serviceaddress = soup.find('span', id=\"ctl00_maincontent_ucIAADetails_ServiceAddress\").get_text(separator=\", \").strip()\nexcept: \n    serviceaddress = \"NA\"\n        \n        \nadvisordata = {\n    'name': name,\n    'phone': phone, \n    'mobile': mobile,\n    'email': email,\n    'website': website,\n    'licencestatus': licencestatus,\n    'licenceclass': licenceclass, \n    'licenceexpirydate': licenceexpirydate,\n    'businessaddress': businessaddress,\n    'serviceaddress': serviceaddress  \n    }\n    \nprint(advisordata)","77beb894":"from datetime import datetime\n\nstart_time = datetime.now()\n\nadvisorlist = []\n\nfor link in advisorlinks: \n    r = requests.get(link)\n    soup = BeautifulSoup(r.content, \"lxml\")\n\n    name = soup.find('h3', class_ =\"DisplayName\").text.strip()\n\n    try:\n        phone = soup.find('span', id=\"ctl00_maincontent_ucIAADetails_PhoneNumber\").text.strip()\n    except: \n        phone = \"NA\"\n        \n    try:\n        mobile = soup.find('span', id=\"ctl00_maincontent_ucIAADetails_Mobile\").text.strip()\n    except:\n        mobile = \"NA\"\n\n    try:\n        email = soup.find('a', id=\"ctl00_maincontent_ucIAADetails_EmailLink\").text.strip()\n    except: \n        email = \"NA\"\n        \n    try:\n        website = soup.find('a', id=\"ctl00_maincontent_ucIAADetails_WebUrlLink\").text.strip()\n    except: \n        website = \"NA\"\n        \n    licencestatus = soup.find('span', id=\"ctl00_maincontent_ucIAADetails_LicenceStatus\").text.strip()\n    \n    licenceclass = soup.find('span', id=\"ctl00_maincontent_ucIAADetails_fkLicence\").text.strip()\n    \n    licenceexpirydate = soup.find('span', id=\"ctl00_maincontent_ucIAADetails_ExpiryDate\").text.strip()\n    \n    try:\n        businessaddress = soup.find('span', id=\"ctl00_maincontent_ucIAADetails_BusinessAddress\").get_text(separator=\", \").strip()\n    except:\n        businessaddress = \"NA\"\n        \n    try:\n        serviceaddress = soup.find('span', id=\"ctl00_maincontent_ucIAADetails_ServiceAddress\").get_text(separator=\", \").strip()\n    except: \n        serviceaddress = \"NA\"\n        \n        \n    advisordata = {\n    'name': name,\n    'phone': phone, \n    'mobile': mobile,\n    'email': email,\n    'website': website,\n    'licencestatus': licencestatus,\n    'licenceclass': licenceclass, \n    'licenceexpirydate': licenceexpirydate,\n    'businessaddress': businessaddress,\n    'serviceaddress': serviceaddress  \n    }\n    \n    advisorlist.append(advisordata)\n\nend_time = datetime.now()\nprint('Duration: {}'.format(end_time - start_time))","20f79f77":"mobileif = soup.find('span', id=\"ctl00_maincontent_ucIAADetails_Mobile\")\nif mobileif is None:\n    mobileif = 'N\/A'\nelse: mobileif = soup.find('span', id=\"ctl00_maincontent_ucIAADetails_Mobile\").text.strip()","49bda3e8":"df = pd.DataFrame.from_dict(advisorlist)\ndf['country'] = df['serviceaddress'].str.rsplit(',').str[-1]\ndf['Business Name'] = df['businessaddress'].str.rsplit(',').str[0]","9a972384":"# getting second and third - to - last positions: \na = df['serviceaddress'].str.rsplit(',').str[-2]\nb = df['serviceaddress'].str.rsplit(',').str[-3]\n\n# creating function to check whether record has any numerical characters: \ndef has_numbers(inputString):\n    return any(char.isdigit() for char in inputString)\n\n# Comparing every single line, selecting only those with numerical characters: \ncityzip = []\n\nfor number in range (0,1276):\n    if has_numbers(a[number]) is True:\n        cityzip.append(a[number])\n    else: cityzip.append(b[number])\n\ndf['cityzip'] = cityzip","3ca52bcf":"region = []\n\nfor number in range (0,1276):\n    if has_numbers(a[number]) is False:\n        region.append(a[number])\n    else: region.append(b[number])\n\ndf['region'] = region","dd648ddd":"On top of this, I will create autorefresh and Python will update this infor once every fortnight - but this will be a different story. \n\nThank you for being with me. Feel free to leave your comments and share better practices. As you remember, I am only in the beginning of this run. \n\nThank you","89848ac9":"Great. This is what we were looking for. Now we will create a loop that will check every single page. It will take time and I will measure this time - so we can compare try\/except and if\/else afterwards: ","3bd6210c":"Great, we now have list of all 1276 page links where we can obtain their contact details. Now let's inspect the page and figure out where we can obtain contact information:\n\n![](https:\/\/i.ibb.co\/3FXgvV3\/4.png)\n\nThe process should be pretty straightforward:\n+ Name - tag 'h3' and class_ is ''DisplayName;\n+ Phone number - tag 'p' with respective id \"ctl00_maincontent_ucIAADetails_PhoneNumber\";\n+ Mobile number - tag 'p' with id \"ctl00_maincontent_ucIAADetails_Mobile\"; \n+ Email - tag 'p' with id \"ctl00_maincontent_ucIAADetails_EmailLink\";\n+ Web site - tag 'p' with id \"ctl00_maincontent_ucIAADetails_WebUrlLink\";\n+ Licence Status, Licence Class, Licence Expiry Date, Business Address and Service Address we will obtain using the same algorithm. \n\nWe will store this data in the 'advisordata' dictionary. We will try with the random advisor - just to see if bs4 will work: ","fb633ff5":"I also wanted to deduct City and ZIP code which are from record to record either on the second-to-last or third-to-last positions (I mean position after second-to-last comma, etc). Thus, I had to do a bit of magic here. I splitted both second- and thrid-comma data, then compared if any of these two had numerical characters inside (ZIP) - and if yes, I recorded this into cityzip field: ","81dc314a":"At the end of the day, we have raw (still very raw) dataset containing all information about registered NZ immigration advisors. Unfortunately, IAA has very messy data, and for the rest of the day I will be cleaning and standardising this dataset to make it perfect and workable for our web site visitors.\n\n![](https:\/\/i.ibb.co\/f8wFWKg\/5.png)","e8e773b1":"![](https:\/\/i.ibb.co\/Dr3G6R2\/6.png)\n\nDespite the output has all the information we need - it also has lots of redundant 'a' tags that we don't need. Thus, we will try another approach. This time we need to find unique tags that go together with these links: and I believe \"td\" will do its job:","4a2d9816":"Finally, I did the same 'magic' with region:","48bf028c":"It didn't work. Only because some advisors don't have all required information. Thus, we need to create a tool that will skip the field and leave 'NA' or \"N\/A\" if information is omitted. \n\nGenerally, there are multiple way to do it: try\/except of good old if\/else. I will go with the first options - just because it was new for me. Some people say that try\/except should work faster (we will check this later). Considering that fact that I am developing a tool that will go though 1276 pages and scrape info from every single one, I want it to be as fast as possible. Thus, we redesign our code. Fields 'Name', 'Licence Status', 'Licence Class', 'Licence Expiry Date' are mandatory - thus, we shouldn't worry about these. ","b772f7d7":"Here we are. We now have raw dataset with details of all 1276 advisors - exactly what we were looking for. It took 3 minutes and 05 seconds (on my machine) for the try\/except method and 3 minutes 06 seconds for if\/else method. Thus, statistically, I am assuming these two have identical efficiency. But if we had to scrape larger datasets - we could see different results. We will see in future. \n\nJust in case, the code for if\/else method is below: ","bcef87b4":"Hi everyone! This is my first experience working with Kaggle Code (Notebooks) - so might not be the perfect one. But it will get better. All sugegestions are welcomed.\n\nI started using Python and R quite extensively last year - the reason being is my Uni (Master of Analytics) as well as my job (Data Analyst). Since then I've been following Kaggle and was quite impressed with the community. I hope I also have something to contribute. I will be sharing many interesting projects that we are doing at Uni as well as work. My ultimate goal is to become a Data Scientist (maybe once I graduate).\n\nSo, the purpose of this notebook is to share my web-scraping experience with Python and famous Beautiful Soup. I had to do this as part of my side project - community web-site - that will combine all information about immigration to New Zealand. We are only at the planning stage, but we need databases that will contain information on official immigration advisors, translation services as well as recongized clinics where migrants can do their health checks. \n\nThis time we start with the list of **all registered New Zealand immigration advisers**.\n\nThis information is already available on the web, in particular [Immigration Advisers Authority](https:\/\/iaa.ewr.govt.nz\/PublicRegister\/Lastname.aspx) (IAA) has the list of all current advisers as well as look up tools. But the information seems a bit messy, drop down list has at least 8 Aucklands which makes search for an advisor very inconvenient. Our database should be cleaner and clearer. \n\n![](https:\/\/i.ibb.co\/g7jhf22\/1.png)\n\n\nSo, we start our journey with inspecting IAA web site. They don't have direct link where you can get all information at once, instead every advisor gets their personal page with all contact information, as well as IAA Alphabetical Search page where all current names listed: \n\n![](https:\/\/i.ibb.co\/NrwTjj0\/2.png)\n\nSo, in theory - using bs4 we should be able to loop through every letter of the alphabet, get page links for every single advisor - and then loop through these links and obtain required contact information. Let's start\n\nFirst, I will try on a single letter 'a' and see how this goes. Remember, here we are trying to get pagelinks for every single advisor. By inspecting the [page](https:\/\/iaa.ewr.govt.nz\/PublicRegister\/Lastname.aspx), I see that the links are hidden within the \"a\" tags with 'href' in it:\n\n![](https:\/\/i.ibb.co\/SNwSpJS\/3.png)\n\nThus, we are getting our bs4 to find all \"a\" tags: ","5d9ce98b":"![](https:\/\/i.ibb.co\/kXhk3kp\/7.png)\n\nPerfect! This is what we need. Now within every \"td\" tag we can find every \"a\" tag and get the link from there. We will be looping though the every leet of the alphabet: ","9c2800c6":"Now, we will carry on working with raw data. First, we convert our dictionary to Data Frame and by splitting respective fields obtain more useful information:\n+ Country = split data after the last comma in serviceaddress field;\n+ Businesss Name - split data before the first comma in businessaddress field;"}}