{"cell_type":{"7db11d79":"code","bfd94d06":"code","9ec85fa3":"code","6f56d8f2":"code","5bff0af0":"code","e6d68f5e":"code","7a098f92":"code","7e557f2e":"code","a091e512":"code","058daf0d":"code","312b0741":"code","66a35008":"code","92a41edd":"code","67f01d77":"code","83dd48c7":"code","16c4f437":"code","c341cd24":"code","8615be35":"code","72c177ff":"code","e790003d":"code","320c1cbf":"code","337a729d":"code","e038dde3":"code","2369cc26":"code","c91944f7":"code","6286e895":"code","602d97d4":"code","27eab16d":"code","0848fe01":"code","48438843":"code","176c6572":"code","f01dab4d":"code","8570c133":"markdown","abbeda52":"markdown","2f6bafc7":"markdown","9a41e312":"markdown","2e129cb5":"markdown","f08b1d53":"markdown","9277377d":"markdown","62aafe99":"markdown","82624211":"markdown","a988f766":"markdown","d867503d":"markdown","07f09163":"markdown","7e9b1715":"markdown","8dbbe6a3":"markdown","4afb9517":"markdown","088822e4":"markdown","be542afc":"markdown","4704ef62":"markdown","04cddd27":"markdown","498b5e45":"markdown","bedb1589":"markdown","8b18d212":"markdown","628153d3":"markdown","a7a41fb2":"markdown","35e00c8b":"markdown"},"source":{"7db11d79":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntrain = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')","bfd94d06":"train","9ec85fa3":"# data segmentation\nX = train.drop('HeartDisease', axis=1)\ny = train['HeartDisease']\n\ntrain_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=0) # train, valid 8:2 \ubd84\ud560","6f56d8f2":"train_x","5bff0af0":"# We need to duplicate the original state of our training data and test data.\ntrain_x_saved = train_x.copy()\ntest_x_saved = test_x.copy()\n\n# Functions that return training data and test data\ndef load_data():\n    train_x, test_x = train_x_saved.copy(), test_x_saved.copy()\n    return train_x, test_x","e6d68f5e":"# Store the category type to be converted into a list\ncat_cols = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']","7a098f92":"train_x, test_x = load_data()","7e557f2e":"train_x.head()","a091e512":"# Combining training data and test data to perform one-hot encoding through get_dummies\nall_x = pd.concat([train_x, test_x])\nall_x = pd.get_dummies(all_x, columns=cat_cols)\n\n# Repartition of training data and test data\ntrain_x = all_x.iloc[:train_x.shape[0], :].reset_index(drop=True)\ntest_x = all_x.iloc[train_x.shape[0]:, :].reset_index(drop=True)","058daf0d":"train_x.head()","312b0741":"from sklearn.preprocessing import LabelEncoder","66a35008":"train_x, test_x = load_data()","92a41edd":"for c in cat_cols:\n    # Transform data after defining based on training data\n    le = LabelEncoder()\n    le.fit(train_x[c])\n    train_x[c] = le.transform(train_x[c])\n    test_x[c] = le.transform(test_x[c])","67f01d77":"train_x","83dd48c7":"from sklearn.feature_extraction import FeatureHasher","16c4f437":"train_x, test_x = load_data()","c341cd24":"for c in cat_cols:\n    # The usage of FeatureHasher is slightly different from other encoders.\n    fh = FeatureHasher(n_features=5, input_type='string')\n\n    # After converting the variable to a string, apply FeatureHasher.\n    hash_train = fh.transform(train_x[[c]].astype(str).values)\n    hash_test = fh.transform(test_x[[c]].astype(str).values)\n\n    # Change the DataFrame\n    hash_train = pd.DataFrame(hash_train.todense(), columns=[f'{c}_{i}' for i in range(5)])\n    hash_test = pd.DataFrame(hash_test.todense(), columns=[f'{c}_{i}' for i in range(5)])\n\n    # Combine with original dataframe\n    train_x = pd.concat([train_x, hash_train], axis=1)\n    test_x = pd.concat([test_x, hash_test], axis=1)","8615be35":"# Delete the original categorical variable\ntrain_x.drop(cat_cols, axis=1, inplace=True)\ntest_x.drop(cat_cols, axis=1, inplace=True)","72c177ff":"train_x\n\n#### This method is not often used because feature hahing frequently generates null values \u200b\u200band can respond with GBDT after label encoding","e790003d":"train_x, test_x = load_data()","320c1cbf":"for c in cat_cols:\n    freq = train_x[c].value_counts()\n    # Replaced by the number of occurrences of a category\n    train_x[c] = train_x[c].map(freq)\n    test_x[c] = test_x[c].map(freq)","337a729d":"train_x","e038dde3":"from sklearn.model_selection import KFold","2369cc26":"train_x, test_x = load_data()","c91944f7":"for c in cat_cols:\n    # Calculate the target mean for each category across the training data\n    data_tmp = pd.DataFrame({c: train_x[c], 'target': train_y})\n    target_mean = data_tmp.groupby(c)['target'].mean()\n\n    # Change the category of test data\n    test_x[c] = test_x[c].map(target_mean)\n\n    # Prepare an array to store values \u200b\u200bafter transforming the training data\n    tmp = np.repeat(np.nan, train_x.shape[0])\n\n    kf = KFold(n_splits=4, shuffle=True, random_state=72)\n    for idx_1, idx_2 in kf.split(train_x):\n        # Calculate the mean of each categorical objective variable out of fold\n        target_mean = data_tmp.iloc[idx_1].groupby(c)['target'].mean()\n        # Store the converted value in a date array\n        tmp[idx_2] = train_x[c].iloc[idx_2].map(target_mean)\n\n    train_x[c] = tmp","6286e895":"train_x","602d97d4":"from sklearn.model_selection import KFold","27eab16d":"train_x, test_x = load_data()","0848fe01":"kf = KFold(n_splits=4, shuffle=True, random_state=71)\nfor i, (tr_idx, va_idx) in enumerate(kf.split(train_x)):\n\n    # Distinguish between training data and validation data in training data\n    tr_x, va_x = train_x.iloc[tr_idx].copy(), train_x.iloc[va_idx].copy()\n    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n\n    # Iterate over variables to perform target encoding\n    for c in cat_cols:\n        # Calculate the target mean for each category across the training data\n        data_tmp = pd.DataFrame({c: tr_x[c], 'target': tr_y})\n        target_mean = data_tmp.groupby(c)['target'].mean()\n        # Category substitution of validation data\n        va_x.loc[:, c] = va_x[c].map(target_mean)\n\n        # Prepare an array to store values \u200b\u200bafter transforming the training data\n        tmp = np.repeat(np.nan, tr_x.shape[0])\n        kf_encoding = KFold(n_splits=4, shuffle=True, random_state=72)\n        for idx_1, idx_2 in kf_encoding.split(tr_x):\n            # Calculating the average of the objective variable for each category in out of fold\n            target_mean = data_tmp.iloc[idx_1].groupby(c)['target'].mean()\n            # Storing values \u200b\u200bin date array after conversion\n            tmp[idx_2] = tr_x[c].iloc[idx_2].map(target_mean)\n\n        tr_x.loc[:, c] = tmp\n\n    # Storing encoded features as needed and allowing them to be read later.","48438843":"train_x = pd.concat([tr_x,va_x])\ntrain_y = pd.concat([tr_y,va_y])\ntrain_x","176c6572":"train_x, test_x = load_data()\n\nfrom sklearn.model_selection import KFold\n\nkf = KFold(n_splits=4, shuffle=True, random_state=71)\n\n# Looping a variable to perform target encoding\nfor c in cat_cols:\n\n    data_tmp = pd.DataFrame({c: train_x[c], 'target': train_y})\n    \n    tmp = np.repeat(np.nan, train_x.shape[0])\n\n    # Divide the validation data from the training data\n    for i, (tr_idx, va_idx) in enumerate(kf.split(train_x)):\n        # Calculating the average of the objective variable for each category on the training data\n        target_mean = data_tmp.iloc[tr_idx].groupby(c)['target'].mean()\n        # \uac80\uc99d \ub370\uc774\ud130\uc5d0 \ub300\ud574 \ubcc0\ud658 \ud6c4 \uac12\uc744 \ub0a0\uc9dc \ubc30\uc5f4\uc5d0 \uc800\uc7a5\n        tmp[va_idx] = train_x[c].iloc[va_idx].map(target_mean)\n\n    train_x[c] = tmp","f01dab4d":"train_x","8570c133":"![image.png](attachment:b36e0d64-4452-47a5-97c4-b5367f6122a5.png)\n![image.png](attachment:214f44d9-f182-47b9-941c-3ca097f73453.png)\n![image.png](attachment:bf4c3f1f-489a-4c33-9cf4-919dd93c94c8.png)\n![image.png](attachment:cc1868d1-e5db-4f68-8276-51ee49bddefd.png)","abbeda52":"**Save it as a function because you need to apply different changes over and over again.**","2f6bafc7":"<div style=\"background-color:#F15F5F;border-radius:5px;display:fill;\">\n    <h1><center style =\"margin-left : 20px;\">Target Encoding - When matching the fold split of the cross-validation and the fold of the target encoding<\/center><\/h1>\n<\/div>","9a41e312":"<div style=\"background-color:rgba(0, 255, 255, 0.6);border-radius:5px;display:fill;\">\n    <h1><center style =\"margin-left : 20px;\">One-Hot Encoding<\/center><\/h1>\n<\/div>","2e129cb5":"<div style=\"background-color:#FA2D7D;border-radius:5px;display:fill;\">\n    <h1><center style =\"margin-left : 20px;\">Target Encoding<\/center><\/h1>\n<\/div>","f08b1d53":"![label.PNG](attachment:8582704b-835d-4df8-b1eb-5186fd22f979.PNG)","9277377d":"#### One-Hot encoding is the most representative treatment for categorical variables. \n#### For each level of a categorical variable, create a variable with two values, 0 and 1, \n#### indicating whether it is at that level or not. Therefore, applying One-Hot encoding to a categorical \n#### variable with n levels produces n features of the variable with two values \u200b\u200b(0,1). This is called a dummy variable.","62aafe99":"<div style=\"background-color:#F261AA;border-radius:5px;display:fill;\">\n    <h1><center style =\"margin-left : 20px;\">Feature Hashing<\/center><\/h1>\n<\/div>","82624211":"#### After conversion to One-Hot encoding, the number of features equals the number of levels in the category. \n#### Feature Hashing is a transformation that reduces the number. The number of features after conversion is determined first, \n#### and the position to display the flag is determined for each level using a hash function.\n#### One-hot encoding displays flags in different positions for each level, but in Feature Hashin, since the number of features determined \n#### after transformation is less than the number of levels in a category, it is possible to display flags in the same position at different levels by calculation according to the hash function.","a988f766":"#### Frequency encoding is a method of replacing categorical variables with the number or frequency of occurrences of each level. \n#### It is valid when there is a relationship between the frequency of appearance of each level and the objective variable. (To check the relationship with the target variable, there is a way to check the Correlation value.)\n#### It can also be used as a variant of label encoding to create an index that lists by frequency rather than alphabetically. However, be careful as there are times when the value of the tie rate occurs.","d867503d":"#### Hi all. \ud83d\ude4b\n\n#### Nice to meet you!\n#### It describes a specific type, and does not explain which type it is. \n#### If you use the LightGBM algorithm, you can apply the category type as it is, \n#### but if you want to use the NN model, the linear model, and the decision model, \n#### this process is essential. Let's learn how to switch to fit the data you're using.","07f09163":"<div style=\"background-color: #FFBB00; border-radius:5px;display:fill;\">\n    <h1><center style =\"margin-left : 20px;\">Label Encoding<\/center><\/h1>\n<\/div>","7e9b1715":"![onehot.PNG](attachment:81328835-bb90-4e5f-94e0-47539eb8c144.PNG)","8dbbe6a3":"#### Label encoding simply converts each level to an integer. \n#### For example, label-encoding a categorical variable with 5 levels turns each level into a number from 0 to 4.\n#### I would normally see the level as a string and change it to an index listed alphabetically.\n#### Most of the index numbers in alphabetical order have no intrinsic meaning. \n#### Therefore, unless it is based on a decision tree model, it is not very appropriate to directly use the \n#### features converted by label encoding for training.","4afb9517":"<div style=\"background-color:rgba(0, 255, 255, 0.6);border-radius:5px;display:fill;\">\n    <h1><center style =\"margin-left : 20px;\">Import Data<\/center><\/h1>\n<\/div>","088822e4":"#### Target encoding is a method of converting a categorical variable into a numeric variable using an objective variable. \n#### The basic concept is that the average value of the target variable in each level group of the categorical variable is aggregated as training data and replaced with the value.","be542afc":"#### As mentioned earlier when cross-validating, target encoding requires retransformation for each fold of cross-validation. \n#### This is because conversion is required again to prevent the target variable of the verification data from being included in the variable. \n#### That is, in each fold of cross-validation, training data excluding validation data is divided into folds for target encoding and converted.","4704ef62":"#### Category embedding is a method of using a continuous value or a continuous value vector representing the characteristics of a category value instead of a category value.\n\n#### For example, in the case of a categorical value indicating the name of an athlete, the athlete's age, annual salary, physical ability, etc. are used instead. As another example, \n#### in the case of a category value indicating a region name, the area, population, etc. of the corresponding region may be used.\n\n#### However, when embedding is used, there is a burden of selecting features according to the purpose of data analysis and investigating additional data outside the current data.","04cddd27":"<div style=\"background-color:#D5D5D5;border-radius:5px;display:fill;\">\n    <h1><center style =\"margin-left : 20px;\">Target Encoding - For each fold in cross-validation<\/center><\/h1>\n<\/div>","498b5e45":"<div style=\"background-color:#47C832;border-radius:5px;display:fill;\">\n    <h1><center style =\"margin-left : 20px;\">Frequency Encoding<\/center><\/h1>\n<\/div>","bedb1589":"<div style=\"background-color:rgba(0, 255, 255, 0.6);border-radius:5px;display:fill;\">\n    <h1><center style =\"margin-left : 20px;\">Category Embedding<\/center><\/h1>\n<\/div>","8b18d212":"![freqency encoding.PNG](attachment:44f68f4c-628d-45aa-933e-6438446f5137.PNG)","628153d3":"![feature hashing.PNG](attachment:33b5e0d2-29b4-401d-b813-1715b7dd90e0.PNG)","a7a41fb2":"#### If the average is simply obtained from all the data, it is converted into a categorical variable with the target variable value of the row data to be converted, resulting in information leakage. \n#### Therefore, it must be converted so that the target variable of the row data to be converted is not used.\n#### If the average value of the target variable is calculated by the out-of-fold method in which the training data is divided into folds for target encoding, and each fold is calculated with data other than its own fold, \n#### the value of the target variable of the row data to be converted is not included in the calculation. can be converted The number of folds for coding is 4-10. On the other hand,\n#### in the test data, the average value of the target variables of the entire training data is calculated and transformed.","35e00c8b":"## Tip\n#### Target encoding can be a very effective feature, but it may not have much effect depending on the data. \n#### In particular, in data with a strong time series tendency, the frequency of occurrence of categories may change over time, \n#### so simple aggregation by category does not reflect temporal changes in many cases, so it is often not a good feature. On the other hand, there is a risk of data information of the target variable being leaked."}}