{"cell_type":{"62fbd50d":"code","1d9df8e4":"code","8d2f4ec3":"code","ed566a61":"code","efe2a66a":"code","a86416a1":"code","b759ba33":"code","37fa34b8":"code","61ccd739":"code","f222312a":"code","cb84f701":"code","04618583":"code","f5766bbb":"code","43076a26":"code","0c54531f":"code","7b13e0af":"code","b89477a0":"code","244b25cf":"code","0046f335":"code","c328d327":"code","c17dbc00":"code","1e765cac":"code","09d0957c":"markdown","247234c5":"markdown","fc18cf44":"markdown","470e8c02":"markdown","22b21872":"markdown","d810d473":"markdown","3d1b6f82":"markdown","3bff1461":"markdown","c8afc2df":"markdown","7fe0bf67":"markdown","dcb2c835":"markdown","8ba3fb7a":"markdown","da030bb8":"markdown","2e92a78c":"markdown","a7060271":"markdown","3a93f818":"markdown","489c5c20":"markdown","da8aad4d":"markdown","ddb7fd7c":"markdown","ade01dfe":"markdown","34596297":"markdown","788c90b4":"markdown","a637699d":"markdown","89e08b01":"markdown","e3e685f3":"markdown","143f7ff6":"markdown","054b26b6":"markdown","ad8ab7f4":"markdown","89659b8c":"markdown","02984615":"markdown","6d5893ff":"markdown","eadb3101":"markdown","d1177517":"markdown","e669b1aa":"markdown","074aaafa":"markdown","223bd735":"markdown","06c401c9":"markdown","ee846c0f":"markdown","2e2f5b83":"markdown","f5072007":"markdown"},"source":{"62fbd50d":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import cross_val_score","1d9df8e4":"run_in_kaggle = True\n\ninput_dir_local = 'input'\ninput_dir_kaggle = '\/kaggle\/input\/titanic'\n\nif run_in_kaggle:\n    input_dir = input_dir_kaggle\nelse:\n    input_dir = input_dir_local","8d2f4ec3":"train = pd.read_csv(input_dir + '\/train.csv', index_col='PassengerId')\ntest = pd.read_csv(input_dir + '\/test.csv', index_col='PassengerId')","ed566a61":"main_title_map = {'Lady': 'Mrs', 'Mme': 'Mrs', 'Dona': 'Mrs', 'the Countess': 'Mrs',\n         'Ms': 'Miss', 'Mlle': 'Miss',\n         'Sir': 'Mr', 'Major': 'Mr', 'Capt': 'Mr', 'Jonkheer': 'Mr', 'Don': 'Mr', 'Col': 'Mr', 'Rev': 'Mr', 'Dr': 'Mr'}\n\ndef get_title(full_name):\n    return full_name.split(',')[1].split('.')[0].strip()\n\ndef set_title_mr(data):\n    titles = data['Name'].apply(get_title).replace(main_title_map)\n    data['Title_Mr'] = titles.apply(lambda title: 1 if title == 'Mr' else 0)\n\nset_title_mr(train)\nset_title_mr(test)","efe2a66a":"male_map = {'male': 1, 'female': 0}\ntrain['Male'] = train['Sex'].map(male_map)\ntest['Male'] = test['Sex'].map(male_map)","a86416a1":"class3_map = {1: 0, 2: 0, 3: 1}\ntrain['Pclass_3'] = train['Pclass'].map(class3_map)\ntest['Pclass_3'] = test['Pclass'].map(class3_map)","b759ba33":"def extract_lastname(full_name):\n    return full_name.split(',')[0]\n\ntrain['Last name'] = train['Name'].apply(extract_lastname)\ntest['Last name'] = test['Name'].apply(extract_lastname)","37fa34b8":"def prepare_family_ticket_frequencies_actual(data, is_train, train, last_names_survival, tickets_survival):\n    data['Known family\/ticket survived %'] = np.NaN\n\n    mean_train_survive = train['Survived'].mean()\n\n    # go over all test passengers, and fill in the survival information\n    for i in data.index:\n        did_survive = 1 if (is_train == 1) and (train.loc[i, 'Survived'] == 1) else 0\n        last_name = data.loc[i, 'Last name']\n        ticket = data.loc[i, 'Ticket']\n        family_survived = np.NaN\n        ticket_survived = np.NaN\n\n        # if have other passengers in training set of same family whose survival information is known, copy average here\n        if last_name in last_names_survival:\n            last_name_count, last_name_sum = last_names_survival[last_name]\n            # if from test set, take family survival information as is. \n            # If from train set, need to remove the specific passenger from the average, since the information should be about rest of the family\n            if last_name_count > is_train:\n                family_survived = (last_name_sum - did_survive) \/ (last_name_count - is_train)\n\n        # if have other passengers in training set of same family whose survival information is known, copy average here\n        # add information for training only of how many of known survived in the same ticket\n        if ticket in tickets_survival:\n            ticket_count, ticket_sum = tickets_survival[ticket]\n            # if from test set, take ticket survival information as is. \n            # If from train set, need to remove the specific passenger from the average, since the information should be about rest of the party\n            if ticket_count > is_train:\n                ticket_survived = (ticket_sum - did_survive) \/ (ticket_count - is_train)\n\n        # For final value - take average if both known, or take whatever is known.  If neither family, nor ticket survival known, take mean survival rate\n        if np.isnan(family_survived) == False:\n            if np.isnan(ticket_survived) == False:\n                # both family and ticket survival rates known, take average\n                data.loc[i, 'Known family\/ticket survived %'] = (family_survived + ticket_survived) \/ 2\n            else:\n                # only family survival known, take it\n                data.loc[i, 'Known family\/ticket survived %'] = family_survived\n        elif np.isnan(ticket_survived) == False:\n            # only ticket is known - take value from ticket\n            data.loc[i, 'Known family\/ticket survived %'] = ticket_survived\n        else:\n            # none known, set mean survival value\n            data.loc[i, 'Known family\/ticket survived %'] = mean_train_survive\n            \n    \ndef prepare_family_ticket_frequencies(train, test):\n\n    # Prepare survival information for families based on last name\n    last_names_survival = {}\n\n    for last_name in (set(train['Last name'].unique()) | set(test['Last name'].unique())):\n        last_name_survived = train[train['Last name'] == last_name]['Survived']\n        if last_name_survived.shape[0] > 0:\n            last_names_survival[last_name] = (last_name_survived.count(), last_name_survived.sum())\n\n    # Prepare survival information for parties based on same ticket\n    tickets_survival = {}\n\n    for ticket in (set(train['Ticket'].unique()) | set(test['Ticket'].unique())):\n        ticket_survived = train[train['Ticket'] == ticket]['Survived']\n        if ticket_survived.shape[0] > 0:\n            tickets_survival[ticket] = (ticket_survived.count(), ticket_survived.sum())\n\n    prepare_family_ticket_frequencies_actual(train, True, train, last_names_survival, tickets_survival)\n    prepare_family_ticket_frequencies_actual(test, False, train, last_names_survival, tickets_survival)","61ccd739":"prepare_family_ticket_frequencies(train, test)","f222312a":"train.head()","cb84f701":"train.describe()","04618583":"test.head()","f5766bbb":"test.describe()","43076a26":"feat_to_train_on = ['Title_Mr', 'Male', 'Pclass_3', 'Known family\/ticket survived %']\nx_train = train[feat_to_train_on]\ny_train = train['Survived']\nx_test = test[feat_to_train_on]","0c54531f":"x_train.head()","7b13e0af":"x_test.head()","b89477a0":"model = ExtraTreesClassifier(max_depth=5, n_estimators=150)\nmodel.fit(x_train, y_train)","244b25cf":"feature_importances = pd.DataFrame({'Feature Importance': model.feature_importances_}, index=x_train.columns).\\\n            sort_values(by='Feature Importance', ascending=False)\nfeature_importances","0046f335":"model.score(x_train, y_train)","c328d327":"def cross_val(num_folds):\n    accuracies = cross_val_score(model, x_train, y_train, cv=num_folds)\n    print(f'Cross validation accuracy with {num_folds} folds: ' \n          f'Mean: {accuracies.mean().round(3)}, STD: {accuracies.std().round(3)}, '\n          f'Mean-3*STD (lowest accuracy with 99.7% certainty): {(accuracies.mean() - 3*accuracies.std()).round(3)}')","c17dbc00":"cross_val(3)\ncross_val(5)\ncross_val(10)","1e765cac":"preds = model.predict(x_test)\nx_test_with_preds = x_test.copy()\nx_test_with_preds['Survived'] = preds\nx_test_with_preds[['Survived']].to_csv('preds.csv')","09d0957c":"Full results file can be found [here](https:\/\/github.com\/yoni2k\/ml-kaggle-challenges\/blob\/master\/titanic\/output\/best\/2019_12_30_11_23_12\/results%20with%20colors.xlsx) ","247234c5":"## Checking accuracy metric","fc18cf44":"Seems that most predictive power is whether the person is `Title_Mr` (low survival).\n\nA bit lower predictive power is with features \n- `Male` - *low survival* - based on EDA (see links above), little boys were mostly saved, but male teenagers had lower chance of survival than female teenagers)\n- `Pclass_3` - *low survival* for low class travelers\n- `Known family\/ticket survived %\t` - higher party traveling together survival - higher survival for individual in the party","470e8c02":"Insights:\n- `LogisticRegression` \n  - takes a lot of features (and only the binned variation of features and not continuous values for features like `Age` which makes sense since some of the relationships are not linear)\n  - gives best *cross validation accuracy - 3 STD* with lowest variance\n- `XGBoostClassifier` and `RandomForestClassifier` clearly overfits (train accuracy ~90%) - not useful.  Indeed shown bad results on the test set\n- `KNeighborsClassifier(n_neighbors=8)` gives pretty good accuracies and pretty low variance.\n- `ExtraTreesClassifier(max_depth=5)` gives cross validated **best accuracy** and 2nd best after `LogisticRegression` *cross validation accuracy - 3 * STD*","22b21872":"## Details of how arrived at the conclusions above\nIn the final runs, the following were used:\n- Classifiers: \n  - LogisticRegression(solver='lbfgs')\n  - KNeighborsClassifier(n_neighbors=8)\n  - SVC(gamma='auto', kernel='rbf', probability=True)\n  - RandomForestClassifier(n_estimators=250, max_depth=7)\n  - ExtraTreesClassifier(max_depth=5, n_estimators=150)\n  - XGBClassifier(objective='binary:logistic', n_estimators=250)\n- Features\n  - All possible features were included (~40-50 depending if the features were binned or not)\n  - Features representations \/ views combinations: \n    - With \/ without binning of `Family size`\n    - With \/ without binning of `Age`\n  - Example of features used: `Male`, `SibSp`, `Parch`, `Fare 13.5+`, `Fare log`, `Title_Master`, `Title_Miss`, `Title_Mr`, `Title_Mrs`, `SibSpBin_0`, `SibSpBin_1`, `SibSpBin_2`, `SibSpBin_3`, `SibSpBin_4`, `SibSpBin_5+`, `ParchBin_0`, `ParchBin_1`, `ParchBin_2`, `ParchBin_3`, `ParchBin_4+`, `Family size`, `Family size bin_1`, `Family size bin_23`, `Family size bin_4`, `Family size bin_567`, `Family size bin_8+`, `DeckBin_AG`, `DeckBin_B`, `DeckBin_CF`, `DeckBin_DE`, `DeckBin_unknown_T`, `Pclass_1`, `Pclass_2`, `Pclass_3`, `Embarked_C`, `Embarked_Q`, `Embarked_S`, `Known family\/ticket survived %`, `Age`, `Age Bin_-4`, `Age Bin_11-24`, `Age Bin_24-32`, `Age Bin_32-42`, `Age Bin_4-11`, `Age Bin_42+`","d810d473":"### Prepare feature `Male`","3d1b6f82":"All models:\n![image.png](attachment:image.png)","3bff1461":"Indeed there were around ~65% males on Titanic, out of which most were not kids (hence 60% of people were grouped to be `Mr`) and slightly > 50% of passenger class 3.  See some initial EDA (Explority Data Analysis) and descriptive statistics of Titanic dataset [here](https:\/\/github.com\/yoni2k\/ml-kaggle-challenges\/blob\/master\/titanic\/initial-titanic-logistic-regression.ipynb)","c8afc2df":"## Checking feature importances","7fe0bf67":"**Step 2 survival rate:** do the actual calculation of `Known family\/ticket survived %`","dcb2c835":"## Before we start:\nIf you haven't done or seen some EDA, statistics of the Titanic dataset, suggest to stop and do so or at least read someone else's.  You can find some on mine here:\n- [Initial analysis + Logistic regression](https:\/\/github.com\/yoni2k\/ml-kaggle-challenges\/blob\/master\/titanic\/initial-titanic-logistic-regression.ipynb)\n- [Fare feature investigations](https:\/\/github.com\/yoni2k\/ml-kaggle-challenges\/blob\/master\/titanic\/Fare_investigation.ipynb)\n- [Age feature investigations](https:\/\/github.com\/yoni2k\/ml-kaggle-challenges\/blob\/master\/titanic\/age_investigation.ipynb)\n- [More feature engineering](https:\/\/github.com\/yoni2k\/ml-kaggle-challenges\/blob\/master\/titanic\/more_feature_engineering.ipynb)\n- [Advanced feature engineering](https:\/\/github.com\/yoni2k\/ml-kaggle-challenges\/blob\/master\/titanic\/Advanced%20feature%20engineering.ipynb)","8ba3fb7a":"## Bottom line kernel code:","da030bb8":"## Scaling","2e92a78c":"### Prepare feature `Title_Mr`","a7060271":"Accuracy with cross validation with different number of folds:","3a93f818":"## Preparing predictions","489c5c20":"**Explanation**:\n- In general, it seems that families\/parties traveling together survived mostly together, meaning that many families survived fully, or perished fully.  Although there are families where some members survived, and some didn't, survival rate of the rest of the family members is a good predictor of survival of the rest of the family.\n- There are 2 ways to recognize family:\n  - By ticket number.  This is a very good way to recognize a party traveling together, even if they don't have the same last name, or are not related.\n  - By last name.  This is not perfect. There are 2 issues: \n      - There is a party traveling together, possibly related but not with the same last name, like a married daughter etc.  This is resolved partially by splitting into parties based on ticket number.\n      - There are some common last names.  See my github notebook [here](https:\/\/github.com\/yoni2k\/ml-kaggle-challenges\/blob\/master\/titanic\/Same%20last%20name%20different%20deck%2C%20embarked%2C%20pclass.ipynb) where I suggest another step of recognizing people with same last name, but that are probably not the same family since they didn't embark at the same place, are not traveling in the same class or deck, and not considering them the same family.\n- We calculate separately known ticket survival rate based on training set only, and \"family\" survival rate.\n- If only one of them know for a specific passenger, take it as the final `Known family\/ticket survived %`\n- If both ticket and family survival rates are known, average them for the final value of `Known family\/ticket survived %`","da8aad4d":"### Prepare feature `Known family\/ticket survived %`","ddb7fd7c":"That's it, we are done!","ade01dfe":"Accuracy score of training score without cross-validation:","34596297":"## Who is this Kernel for:\n- If after getting your hands dirty, doing EDA, building your own models, and trying to improve on it, reading and seeing many different other Titanic kernels, you want to see another **simple elegant solution bottom line** that **follows all the best practices of good Data Science \/ Machine Learning process**, then this Kernel is for you!  I don't advise it being your first kernel of Titanic.\n- The goal is not to show the process in great detail, but the high level process and it's assumptions, and bottom line model. I will give many links to details of the process.\n\nThis kernel could be found [here](https:\/\/github.com\/yoni2k\/ml-kaggle-challenges\/blob\/master\/titanic\/Simplest%20bottom%20line%20Kernel%20for%20Kaggle.ipynb) on Github","788c90b4":"## What was tried and didn't work\n- Replacing `KFold` with `StratifiedKFold` gave higher variance results instead of low variance results as expected. Therefore, continued using `KFold`\n- Bagging using `BaggingClassifier` didn't improve the accuracy or variance.  Tried on various `max_samples`, `max_features`, `bootstrap` values\n- Grid searches with `GridSearchCV` on hyperparameters were usually not helpful, since they always gave models overfitting on training set.  It was necessary to try manually different parameters with cross-validatoin.  In the future, plan on using an AutoML tool that would do that.\n- Binning values myself based on survival % for `Age` and other numeric features - better to choose Classifiers that can handle doing it on their own (since we can miss connections between numerous features together if doing it manually on our own).\n- Choosing relevant features on my own based on correlation between features and feature importances.  Doesn't work since any change in other features, hyperparameter can change the list of best features.  Also, the list is per model.\n- Soft and hard voting of different classifiers - improved results slightly, but since I made the change that every classifier chooses it's own features, became not relevant since voting assumes same list of features","a637699d":"## Credits:\n- In addition to trying many different things myself, I read and tried numerous kernels out there.  One specific one worth mentioning is [this kernel](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial) by Gunes Evitan [@gunesevitan](https:\/\/www.kaggle.com\/gunesevitan)\n- Jason Brownlee from [machinelearningmastery.com](https:\/\/machinelearningmastery.com\/) for giving me the conviction that Machine Learning can be done as a ordered process","89e08b01":"## Training","e3e685f3":"## Process followed:\nIf some of the below doesn't sound familiar, I suggest to read up on it online, or check out my [Github project](https:\/\/github.com\/yoni2k\/ml-kaggle-challenges\/blob\/master\/titanic)\n- Use **numerous classifiers**. Main ones I used are: `LogisticRegression`, `KNeighborsClassifier`, `SVC`, `GaussianNB`, `RandomForestClassifier`, `ExtraTreesClassifier`, `xgboost.XGBClassifier`\n- Use **feature engineering** heavily - adding new features that can help (like `Family size`, `Title`, `Deck`, `Known family\/ticket survived %`, `Fare per person` and others) - most learned from other great kernels on Kaggle \n- Advanced **Imputing** techniques for features with missing values, especially for `Age` feature using `RandomForestRegressor` using other known features (`Pclass`, `Title`, `Parch`, `SibSp`, `Embarked`)\n- Try **variations of representation of the same features** like `Age`, `Fare`, `SibSp`, `Parch`, `Family size` - using the numeric feature as is, bin by survival rate, bin automatically into a certain number of bins, perform transformations like `Log transformation` on `Fare` feature etc.\n- Use `RFECV` for **Feature Selection** - instead of manually choosing relevant features, let each model choose it's most helpful features using `RFECV`\n- Use **cross validation** heavily on the whole process, with 5 folds and numerous `random_state` values.  What I realized early on, is that test set accuracy results are often unexpected without using cross validation.  Test set accuracy results are even unexpected following using cross validation on the training set.  Therefore, I used cross validation for the whole flow - taking a fold, and doing all the steps above including scaling, automatic feature selection etc. in the fold.  I aslo looked for low variance models, since higher accuracy models usually overfitted and didn't do well on test data, so I prefered lower variance models even if the accuracy was lower.\n- Use more than one **metric**. In addition to `accuracy` (which is the metric Kaggle scores the specific challenge by), use also `ROC AUC score`, `F1 score`\/`Precision`\/`Recall`.  However, because of a very large difference between training and test scores, I found the metric of *Cross Validation Accuracy on the whole process - 3 * Standard Deviation* to be the most helpful, since it looks both at the accuracy, but also gives a lot of weight to variance \/ Standard Deviation.   \n\nSee later on in the kernel for other things I tried that didn't help \/ didn't work.","143f7ff6":"### Checking that feature engineering was done properly","054b26b6":"## Data Leakage \/ contamination\nDefinition: **Data Leakage** or otherwise known as **Data Contamination** is when knowledge about expected results (**target leakage**) or the test set (**train-test contamination**) that the model will run on somehow finds it's way into the model or the data of the model.\n\nData leakage allows to get amazing results, but doesn't help in real world problems since there this knowledge will not be available (usually we don't know what predictions we'll need to make).\n\nMany of Kaggle competitions allow for data leakage, and best scores are acquired by creating on purpose features that are contaminated.\n\n## Examples of Data Leakage in Titanic dataset:\n- `Ticket_Frequency` that looks at test data, and groups by ticket between training and test set\n- Scaling based on test and train test together.  \n- Looking at distributions of test set, and steering the solution in the direction of better solving the types of observations in test set.\n- Extreme case is getting the solutions of the test set from the Internet, and running our models, grid searches and hyperparameter tuning by checking how well we did on the test set.  This will get us a great score, but this process will not be transferable to real world where the real value of prediction is not known, otherwise we wouldn't need a model to predict it. \n\n### More links about Data Leakage:\n- [Data Leakage explanation on Kaggle](https:\/\/www.kaggle.com\/alexisbcook\/data-leakage)\n- [Data Leakage in Machine Larning](https:\/\/machinelearningmastery.com\/data-leakage-machine-learning\/) on [machinelearning.com](https:\/\/machinelearningmastery.com)","ad8ab7f4":"**Step 1 survival rate:** Prepare last name that will be needed for a decision if passengers belong to the same family","89659b8c":"Choosing 1\/4 of the models with best *cross validation accuracy - 3 STD*:\n![image.png](attachment:image.png)","02984615":"Based on these scores, we can be relatively confident to get at least 75.5% accuracy, and most probably above 80% on test score","6d5893ff":"Indeed:\n- taking only `ExtraTreesClassifier` gives accuracy of **81.8%** (top 3% of results) on the test set on Kaggel! This is with only 4 features: `Male`, `Title_Mr`, `Pclass_3`, `Known family\/ticket survived %`\n- Taking 3 best classifiers above (`LogisticRegression`, `KNeighborsClassifier(n_neighbors=8)`, `ExtraTreesClassifier(max_depth=5)`) and doing some ensembling makes the results even slightly better: **82.8%** on Kaggle test set and puts us in **top 2% of the competition** without any data contamination!  See ensembling done on my Github [here](https:\/\/github.com\/yoni2k\/ml-kaggle-challenges\/blob\/master\/titanic\/output.py) by using `ExtraTreesClassifier(max_depth=2)` done on the `predict_proba_` results of the 3 classifiers above","eadb3101":"## Kernel without Data Leakage, following best practices of Machine Learning, that gets you in the top 3% with only 4 features (and into top 2% with some ensembling)!","d1177517":"Since all the values are already in \\[0,1\\] range, and ExtraTreesClassifier doesn't need scaling, no scaling was done.  In my full example [here](https:\/\/github.com\/yoni2k\/ml-kaggle-challenges\/blob\/master\/titanic), `MinMaxScaler` was done on all features","e669b1aa":"## Assumptions:\n- Simplicity over slightly better accuracy.  I was actually able to get a slightly better accuracy (82.3%) which **puts you in top 2% on Kaggle** with a slightly more complicated model, using numerous classifiers and ensembling, but **also without data contamination**.  See details in my [Github project](https:\/\/github.com\/yoni2k\/ml-kaggle-challenges\/blob\/master\/titanic)\n- Never use test data in any way\n- Choose models based on results on cross validation of train data, and not based on trying combinations on test data","074aaafa":"## Solution conclusions:\n`ExtraTreesClassifier` with `max_depth=5` did the best as far as cross validated accuracy with only 4 features: `Male`, `Title_Mr`, `Pclass_3`, `Known family\/ticket survived %`.  \n\nIt knew to generalize well, and didn't overfit, like some of the other classifiers (like `RandomForestClassifier` and `xgboost.XGBClassifier` that went very quickly from not generalizing well to overfitting)\n\nSee details in \"Details of how arrived at the conclusions\" below\n\n## About Extra Trees Classifier:\nIf you are not familiar with Extra Trees, it is a classifier that is in some way similar to and some ways different from Random Forest.  It is also an ensemble classifier, and a forest of trees, but the trees are even more random than in Random Forest, hense it's also sometimes called \"Extremely Randomized Trees\".  While Random Forest takes a random subset of features, and then looks for the best split, Extra Trees takes a random subset of features and makes a **random** split and can create many more trees faster (since it doesn't need to calculate the best split), and then check which trees give good scores.  The reasoning why this prevents overfitting for small set of data like the Titanic is because the range of types of trees created by Extra Trees is much larger, and it really finds the generalization with lower variance, while Random Forest will often times overfit on the train data, like in the Titanic case.  It is not the absolute best Classifier, since such a thing doesn't exist, but it works very well for the specific problem of the Titanic, and a nice tool to have in your toolbox when you try numerous classifiers.\n\nYou can read more about `ExtraTreesClassifier` here:\n- [sklearn help](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html)\n- Great summary on \"Towards Data Science\" on Medium on differences between Random Forest and Extra Trees, [\n\"An Intuitive Explanation of Random Forest and Extra Trees Classifiers\"](https:\/\/towardsdatascience.com\/an-intuitive-explanation-of-random-forest-and-extra-trees-classifiers-8507ac21d54b) by Frank Ceballos","223bd735":"**Credit:** the idea taken from [this kernel](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial) by Gunes Evitan [@gunesevitan](https:\/\/www.kaggle.com\/gunesevitan)","06c401c9":"### Prepare feature `Pclass_3`","ee846c0f":"#### Thank you! Would love comments and suggestions! ","2e2f5b83":"## My goal:\nI created this kernel after an extensive time that I got my hands dirty and tried many different methods, and variations of the models, features, ensembles, metrics etc.\n\nMy goal was not to reach the absolutely best accuracy, but to: **learn and try in practice as much as possible different methods and tools of Data Science \/ Machine Learning**.\n\nI followed the best practices of Machine Learning, even if it means getting lower accuracy.  Most importantly, there is **no data leakage \/ data contamination**.  There is an assumption in my work, that test data is not available at the time of training.  This is the proper assumption in most real world problems.\n- no features look at the test data in general, and correct results that are available online in particular.  One such example I saw in many kernels is `Ticket_Frequency` feature that takes a party size both from the training and test data together.\n- Scaling done only based on train data, and specifically the relevant fold of the train data\n- I personally never looked at the correct results available online\n- I chose models based on accuracy scores (with cross validation) acquired on **training** data, and didn't just try various combinations and chose what worked best on the test data\n\n**I presume that the score I got of ~82% accuracy is close to the highest score one could get without overfitting on test data.**  \n\n**I would love to hear if someone was able to get significantly better accuracy without using test data, overfitting on test data, or trying directly on test data many possible variations.**","f5072007":"### Prepare data for training"}}