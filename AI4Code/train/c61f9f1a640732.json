{"cell_type":{"c336f318":"code","0e23ab63":"code","ba8e4e03":"code","8e004302":"code","024fb575":"code","1de837a7":"code","935a5070":"code","a50198a4":"code","20363868":"code","f8c85a5b":"code","bc542938":"code","7b2442c8":"code","a528cc8d":"code","cec8e8dd":"code","f799c7b0":"code","2eb8b2a9":"code","0c388773":"code","a467d582":"markdown","1d447a30":"markdown","4257480b":"markdown","ff860d6f":"markdown","cee5d619":"markdown","6ac7908b":"markdown","4b63a258":"markdown","f7978b8c":"markdown","573169e9":"markdown","740de778":"markdown","cd9b0f61":"markdown","c6f85e73":"markdown","36603ea1":"markdown","0972b8ae":"markdown","93f47538":"markdown"},"source":{"c336f318":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","0e23ab63":"def submit(model, test_features, test_ids, filename):\n    loss_pred = model.predict(test_features)\n    submission = pd.DataFrame({\"id\": test_ids, \"loss\": loss_pred.reshape(-1)})\n    submission.to_csv(filename, index=False)","ba8e4e03":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv\")","8e004302":"train.head()","024fb575":"train.shape","1de837a7":"train.describe().transpose()","935a5070":"corr_score = train.corr()","a50198a4":"corr_score[\"loss\"].sort_values(ascending=False)","20363868":"train.pop(\"id\")\ntest_ids = test.pop(\"id\")","f8c85a5b":"train_mean = train.mean()\ntrain_std = train.std()","bc542938":"train_targets_mean = train_mean.pop(\"loss\")\ntrain_targets_std = train_std.pop(\"loss\")","7b2442c8":"validation_split = 0.2","a528cc8d":"train_features, validation_features = train_test_split(train, test_size=validation_split)","cec8e8dd":"train_targets, validation_targets = train_features.pop(\"loss\"),  validation_features.pop(\"loss\")","f799c7b0":"should_scale = False\nif should_scale == True:\n    train_features = (train_features - train_mean) \/ train_std\n    validation_features = (validation_features - train_mean) \/ train_std\n    test_features = (test - train_mean) \/ train_std\n    print(test_features.head())\n    print(train_features.head())\n    print(validation_features.head())\nelse:\n     test_features = test","2eb8b2a9":"import catboost\nimport time\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nbegin = time.time()\nparameters = {\n    \"depth\": [6, 7, 8],\n    \"learning_rate\": [0.08, 0.1],\n    \"iterations\": [300, 350], \n}\ndef train_catboost(hyperparameters, X_train, X_val, y_train, y_val):\n    keys = hyperparameters.keys()\n    best_index = {key:0 for key in keys}\n    best_cat = None\n    best_score = 10e8\n    for (index, key) in enumerate(keys):\n        print(\"Find best parameter for %s\" %(key))\n        items = hyperparameters[key]\n        best_parameter = None\n        temp_best = 10e8\n        for (key_index, item) in enumerate(items):\n            iterations = hyperparameters[\"iterations\"][best_index[\"iterations\"]] if key != \"iterations\" else item\n            learning_rate = hyperparameters[\"learning_rate\"][best_index[\"learning_rate\"]] if key != \"learning_rate\" else item\n            depth = hyperparameters[\"depth\"][best_index[\"depth\"]] if key != \"depth\" else item\n            print(\"Train with iterations: %d learning_rate: %.2f depth:%d\"%(iterations, learning_rate, depth))\n            cat = catboost.CatBoostRegressor(\n                iterations = iterations, \n                learning_rate = learning_rate,\n                depth = depth\n            )\n            cat.fit(X_train, y_train, verbose=False)\n            y_pred = cat.predict(X_val)\n            score = np.sqrt(mean_squared_error(y_val, y_pred))\n            print(\"RMSE: %.2f\"%(score))\n            if score < temp_best:\n                temp_best = score\n                best_index[key] = key_index\n                best_parameter = item\n            if score < best_score:\n                best_score = score\n                best_cat = cat\n        print(\"Best Parameter for %s: \"%(key), best_parameter)\n    best_parameters = {\n        \"iterations\": hyperparameters[\"iterations\"][best_index[\"iterations\"]],\n        \"learning_rate\": hyperparameters[\"learning_rate\"][best_index[\"learning_rate\"]],\n        \"depth\": hyperparameters[\"depth\"][best_index[\"depth\"]]\n    }\n    return best_cat, best_score, best_parameters\nbest_cat, best_score, best_parameters = train_catboost(parameters, train_features, validation_features, train_targets, validation_targets)\nprint(\"Best CatBoost Model: \", best_cat)\nprint(\"Best MAE: \", best_score)\nelapsed = time.time() - begin \nprint(\"Elapsed time: \", elapsed)\nsubmit(best_cat, test_features, test_ids, \"submission.csv\")","0c388773":"from sklearn.model_selection import KFold\nfold = 1\nfor train_indices, val_indices in KFold(n_splits=5, shuffle=True).split(train):\n    print(\"Training with Fold %d\"%(fold))\n    X_train = train.iloc[train_indices]\n    X_val = train.iloc[val_indices]\n    y_train = X_train.pop(\"loss\")\n    y_val = X_val.pop(\"loss\")\n    if should_scale:\n        X_train = (X_train - train_mean) \/ train_std\n        X_val = (X_val - train_mean) \/ train_std\n    cat = catboost.CatBoostRegressor(\n        iterations = best_parameters[\"iterations\"], \n        learning_rate = best_parameters[\"learning_rate\"],\n        depth = best_parameters[\"depth\"]\n    )\n    cat.fit(X_train, y_train, verbose=False)\n    y_pred = cat.predict(X_val)\n    score = np.sqrt(mean_squared_error(y_val, y_pred))\n    print(\"RMSE: %.2f\"%(score))\n    submit(cat, test_features, test_ids, \"submission_fold%d.csv\"%(fold))\n    fold += 1","a467d582":"# CatBoost Tabular Playground Series Prediction","1d447a30":"### Data Scaling","4257480b":"## Common Functions","ff860d6f":"## Load datasets","cee5d619":"## Conclusion\nHyper Parameter Searching don't affect the result too much. K-Fold Alogrithm has obvious impact on validation dataset, but doesn't have an obvious impact on test result.","6ac7908b":"## Drop id column","4b63a258":"## Model Development\n### Using Catboost","f7978b8c":"## Import Packages","573169e9":"I will apply K-Fold alogorithm to best Model for training. The result looks good, sometime I can get 7.82, howerver when I submit the results, the scores are about 7.9, and just litte difference between different fold of data.","740de778":"## Your upvote can encourage me updating notebooks on Kaggle, if you like my work, give me an upvote.","cd9b0f61":"## EDA","c6f85e73":"### Train Validation Split","36603ea1":"## Summary\nIn this notebook, I will use CatBoost Regressor to solve Tabular Playground Series Prediction. I will try hyperparameter searching and K-Fold Alogorithm to see if this can have an impact on test results.","0972b8ae":"There isn't an obvious correlation between features and target values.","93f47538":"## Data Preprocessing"}}