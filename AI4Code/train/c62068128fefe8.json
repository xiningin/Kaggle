{"cell_type":{"2b363cb7":"code","d8148982":"code","73afaba1":"code","575c8b35":"code","79fcc213":"code","c253b431":"code","fd6c3e56":"code","5434a0f6":"code","0d2de892":"code","b4fd1b64":"markdown","eb4e9f72":"markdown"},"source":{"2b363cb7":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss, accuracy_score\nfrom xgboost import XGBClassifier\n\nimport optuna","d8148982":"df = pd.read_csv(\"..\/input\/d\/pasadenian\/titanic\/train_5folds.csv\") # use your own splitted data (5 folds)\ndf_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in ('PassengerId', 'Survived', 'Name', 'Ticket', 'Cabin', 'kfold')]\nobject_cols = ['Pclass', 'Sex', 'Embarked']\nobject_cols_to_impute = ['Embarked']\nnumerical_cols_to_impute = ['Age', 'Fare']\ndf_test = df_test[useful_features]","73afaba1":"def run(trial):\n    fold = 0\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n    reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n    reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n    subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n    max_depth = trial.suggest_int(\"max_depth\", 1, 7)\n\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n\n    imputer1 = SimpleImputer(strategy='median')\n    xtrain[numerical_cols_to_impute] = imputer1.fit_transform(xtrain[numerical_cols_to_impute])\n    xvalid[numerical_cols_to_impute] = imputer1.transform(xvalid[numerical_cols_to_impute])\n    \n    imputer2 = SimpleImputer(strategy='most_frequent')\n    xtrain[object_cols_to_impute] = imputer2.fit_transform(xtrain[object_cols_to_impute])\n    xvalid[object_cols_to_impute] = imputer2.transform(xvalid[object_cols_to_impute])\n\n    ytrain = xtrain.Survived\n    yvalid = xvalid.Survived\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ohe = preprocessing.OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n    xtrain_ohe = ohe.fit_transform(xtrain[object_cols])\n    xvalid_ohe = ohe.transform(xvalid[object_cols])\n    \n    xtrain_ohe = pd.DataFrame(xtrain_ohe, columns=[f\"ohe_{i}\" for i in range(xtrain_ohe.shape[1])])\n    xvalid_ohe = pd.DataFrame(xvalid_ohe, columns=[f\"ohe_{i}\" for i in range(xvalid_ohe.shape[1])])\n\n    xtrain = pd.concat([xtrain, xtrain_ohe], axis=1)\n    xvalid = pd.concat([xvalid, xvalid_ohe], axis=1)\n    \n    xtrain = xtrain.drop(object_cols, axis=1)\n    xvalid = xvalid.drop(object_cols, axis=1)\n    \n    model = XGBClassifier(\n        random_state=42,\n        tree_method=\"gpu_hist\",\n        gpu_id=1,\n        predictor=\"gpu_predictor\",\n        n_estimators=7000,\n        learning_rate=learning_rate,\n        reg_lambda=reg_lambda,\n        reg_alpha=reg_alpha,\n        subsample=subsample,\n        colsample_bytree=colsample_bytree,\n        max_depth=max_depth,\n    )\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    preds_valid = model.predict(xvalid)\n    loss = log_loss(yvalid, preds_valid)\n    return loss","575c8b35":"study = optuna.create_study(direction=\"minimize\")\nstudy.optimize(run, n_trials=5)","79fcc213":"study.best_params","c253b431":"final_predictions = []; scores=[]; acc_scores=[]\nfor fold in range(5):\n\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    imputer1 = SimpleImputer(strategy='median')\n    xtrain[numerical_cols_to_impute] = imputer1.fit_transform(xtrain[numerical_cols_to_impute])\n    xvalid[numerical_cols_to_impute] = imputer1.transform(xvalid[numerical_cols_to_impute])\n    xtest[numerical_cols_to_impute] = imputer1.transform(xtest[numerical_cols_to_impute])\n    \n    imputer2 = SimpleImputer(strategy='most_frequent')\n    xtrain[object_cols_to_impute] = imputer2.fit_transform(xtrain[object_cols_to_impute])\n    xvalid[object_cols_to_impute] = imputer2.transform(xvalid[object_cols_to_impute])\n    xtest[object_cols_to_impute] = imputer2.transform(xtest[object_cols_to_impute])\n\n    ytrain = xtrain.Survived\n    yvalid = xvalid.Survived\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n  \n    params = {'learning_rate': 0.08361505685377674, \n              'reg_lambda': 0.9777467403058739, \n              'reg_alpha': 1.1310281041673675e-06, \n              'subsample': 0.6746707311826466, \n              'colsample_bytree': 0.6090312940898472, \n              'max_depth': 4\n    }\n    \n    model = XGBClassifier(\n        random_state=0, \n        objective='binary:logistic', \n        eval_metric='logloss', \n        use_label_encoder=False,\n        #tree_method='gpu_hist',\n        #gpu_id=0,\n        #predictor=\"gpu_predictor\",\n        n_estimators=7000,\n        **params\n    )\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    loss = log_loss(yvalid, preds_valid)\n    accuracy = accuracy_score(yvalid, preds_valid)\n    print(fold, loss)\n    scores.append(loss)\n    acc_scores.append(accuracy)\n    \nprint(np.mean(scores), np.std(scores))\nprint(f\"CV score (accuracy): {np.mean(acc_scores):.3f}\")","fd6c3e56":"preds = np.column_stack(final_predictions)\npreds = [*map(lambda x:np.argmax(np.bincount(x)), preds)]","5434a0f6":"sample_submission.Survived = preds\nsample_submission.to_csv(\"submission.csv\", index=False)","0d2de892":"sample_submission","b4fd1b64":"Inherited from https:\/\/www.kaggle.com\/abhishek\/competition-part-4-hyperparameter-tuning-optuna\n\nBaseline prediction (https:\/\/www.kaggle.com\/pasadenian\/titanic-baseline\/edit\/run\/75681324) + optuna hyperparameter tuning","eb4e9f72":"Run the model with the best set of params"}}