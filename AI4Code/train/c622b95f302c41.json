{"cell_type":{"8cdbcfa9":"code","063fc6fe":"code","f4b473a2":"code","e8062c21":"code","79b24605":"code","d5bc00c3":"code","0dfdfa9e":"code","181bb1a8":"code","cf0a3b55":"code","38618329":"code","7a7c6ad1":"code","b13451e3":"code","6d10244a":"code","9f24dfd4":"code","b60009c0":"code","44295a29":"code","007ae7bd":"code","faa8c7f3":"code","0dd0ed8b":"code","65506281":"code","00d6ee7b":"code","0fd8bbed":"code","3f09ca08":"code","03801dad":"code","2a40cf44":"code","5fb43024":"code","c32e5fc1":"code","210073c1":"code","173d6f9c":"code","9ad02dee":"markdown","84020d07":"markdown","670994dc":"markdown","0f71a8cb":"markdown","0622cde0":"markdown","02a4b77b":"markdown","63012dbb":"markdown","bab61e0a":"markdown","a8b69a62":"markdown","b6fca591":"markdown","d9e88937":"markdown","e0381bb8":"markdown","e06481a9":"markdown","876b3bc8":"markdown","542cab27":"markdown","63f5093f":"markdown","cb1543eb":"markdown","89e5fbe7":"markdown","f96168de":"markdown","ec71cfd0":"markdown","972bf4b4":"markdown","b5a1f6c2":"markdown","41889743":"markdown","336b44d7":"markdown","6d03ca2c":"markdown","c5addc13":"markdown","b96eaff1":"markdown"},"source":{"8cdbcfa9":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict, Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords', quiet=True)\nstopwords = stopwords.words('english')\n\nsns.set(style=\"white\", font_scale=1.2)\nplt.rcParams[\"figure.figsize\"] = [10,8]\npd.set_option.display_max_columns = 0\npd.set_option.display_max_rows = 0","063fc6fe":"train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","f4b473a2":"train.head()","e8062c21":"train.shape, test.shape, test.shape[0]\/train.shape[0]","79b24605":"train.info()","d5bc00c3":"null_counts = pd.DataFrame({\"Num_Null\": train.isnull().sum()})\nnull_counts[\"Pct_Null\"] = null_counts[\"Num_Null\"] \/ train.count() * 100\nnull_counts","0dfdfa9e":"keywords_vc = pd.DataFrame({\"Count\": train[\"keyword\"].value_counts()})\nsns.barplot(y=keywords_vc[0:30].index, x=keywords_vc[0:30][\"Count\"], orient='h')\nplt.title(\"Top 30 Keywords\")\nplt.show()","181bb1a8":"len(train[\"keyword\"].value_counts())","cf0a3b55":"disaster_keywords = train.loc[train[\"target\"] == 1][\"keyword\"].value_counts()\nnondisaster_keywords = train.loc[train[\"target\"] == 0][\"keyword\"].value_counts()\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(y=disaster_keywords[0:30].index, x=disaster_keywords[0:30], orient='h', ax=ax[0], palette=\"Reds_d\")\nsns.barplot(y=nondisaster_keywords[0:30].index, x=nondisaster_keywords[0:30], orient='h', ax=ax[1], palette=\"Blues_d\")\nax[0].set_title(\"Top 30 Keywords - Disaster Tweets\")\nax[0].set_xlabel(\"Keyword Frequency\")\nax[1].set_title(\"Top 30 Keywords - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Keyword Frequency\")\nplt.tight_layout()\nplt.show()","38618329":"armageddon_tweets = train[(train[\"keyword\"].fillna(\"\").str.contains(\"armageddon\")) & (train[\"target\"] == 0)]\nprint(\"An example tweet:\\n\", armageddon_tweets.iloc[10, 3])\narmageddon_tweets.head()","7a7c6ad1":"def keyword_disaster_probabilities(x):\n    tweets_w_keyword = np.sum(train[\"keyword\"].fillna(\"\").str.contains(x))\n    tweets_w_keyword_disaster = np.sum(train[\"keyword\"].fillna(\"\").str.contains(x) & train[\"target\"] == 1)\n    return tweets_w_keyword_disaster \/ tweets_w_keyword\n\nkeywords_vc[\"Disaster_Probability\"] = keywords_vc.index.map(keyword_disaster_probabilities)\nkeywords_vc.head()","b13451e3":"keywords_vc.sort_values(by=\"Disaster_Probability\", ascending=False).head(10)","6d10244a":"keywords_vc.sort_values(by=\"Disaster_Probability\").head(10)","9f24dfd4":"locations_vc = train[\"location\"].value_counts()\nsns.barplot(y=locations_vc[0:30].index, x=locations_vc[0:30], orient='h')\nplt.title(\"Top 30 Locations\")\nplt.show()","b60009c0":"len(train[\"location\"].value_counts())","44295a29":"disaster_locations = train.loc[train[\"target\"] == 1][\"location\"].value_counts()\nnondisaster_locations = train.loc[train[\"target\"] == 0][\"location\"].value_counts()\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(y=disaster_locations[0:30].index, x=disaster_locations[0:30], orient='h', ax=ax[0], palette=\"Reds_d\")\nsns.barplot(y=nondisaster_locations[0:30].index, x=nondisaster_locations[0:30], orient='h', ax=ax[1], palette=\"Blues_d\")\nax[0].set_title(\"Top 30 Locations - Disaster Tweets\")\nax[0].set_xlabel(\"Keyword Frequency\")\nax[1].set_title(\"Top 30 Locations - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Keyword Frequency\")\nplt.tight_layout()\nplt.show()","007ae7bd":"train[\"tweet_length\"] = train[\"text\"].apply(len)\nsns.distplot(train[\"tweet_length\"])\nplt.title(\"Histogram of Tweet Length\")\nplt.xlabel(\"Number of Characters\")\nplt.ylabel(\"Density\")\nplt.show()","faa8c7f3":"min(train[\"tweet_length\"]), max(train[\"tweet_length\"])","0dd0ed8b":"g = sns.FacetGrid(train, col=\"target\", height=5)\ng = g.map(sns.distplot, \"tweet_length\")\nplt.suptitle(\"Distribution Tweet Length\")\nplt.show()","65506281":"def count_words(x):\n    return len(x.split())\n\ntrain[\"num_words\"] = train[\"text\"].apply(count_words)\nsns.distplot(train[\"num_words\"], bins=10)\nplt.title(\"Histogram of Number of Words per Tweet\")\nplt.xlabel(\"Number of Words\")\nplt.ylabel(\"Density\")\nplt.show()","00d6ee7b":"g = sns.FacetGrid(train, col=\"target\", height=5)\ng = g.map(sns.distplot, \"num_words\")\nplt.suptitle(\"Distribution Number of Words\")\nplt.show()","0fd8bbed":"def avg_word_length(x):\n    return np.sum([len(w) for w in x.split()]) \/ len(x.split())\n\ntrain[\"avg_word_length\"] = train[\"text\"].apply(avg_word_length)\nsns.distplot(train[\"avg_word_length\"])\nplt.title(\"Histogram of Average Word Length\")\nplt.xlabel(\"Average Word Length\")\nplt.ylabel(\"Density\")\nplt.show()","3f09ca08":"g = sns.FacetGrid(train, col=\"target\", height=5)\ng = g.map(sns.distplot, \"avg_word_length\")","03801dad":"def create_corpus(target):\n    corpus = []\n\n    for w in train.loc[train[\"target\"] == target][\"text\"].str.split():\n        for i in w:\n            corpus.append(i)\n            \n    return corpus\n\ndef create_corpus_dict(target):\n    corpus = create_corpus(target)\n            \n    stop_dict = defaultdict(int)\n    for word in corpus:\n        if word in stopwords:\n            stop_dict[word] += 1\n    return sorted(stop_dict.items(), key=lambda x:x[1], reverse=True)","2a40cf44":"corpus_disaster_dict = create_corpus_dict(0)\ncorpus_non_disaster_dict = create_corpus_dict(1)\n\ndisaster_x, disaster_y = zip(*corpus_disaster_dict)\nnon_disaster_x, non_disaster_y = zip(*corpus_non_disaster_dict)\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(y=list(disaster_x)[0:30], x=list(disaster_y)[0:30], orient='h', palette=\"Reds_d\", ax=ax[0])\nsns.barplot(y=list(non_disaster_x)[0:30], x=list(non_disaster_y)[0:30], orient='h', palette=\"Blues_d\", ax=ax[1]) \nax[0].set_title(\"Top 30 Stop Words - Disaster Tweets\")\nax[0].set_xlabel(\"Stop Word Frequency\")\nax[1].set_title(\"Top 30 Stop Words - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Stop Word Frequency\")\nplt.tight_layout()\nplt.show()","5fb43024":"corpus_disaster, corpus_non_disaster = create_corpus(1), create_corpus(0)\ncounter_disaster, counter_non_disaster = Counter(corpus_disaster), Counter(corpus_non_disaster)\nx_disaster, y_disaster, x_non_disaster, y_non_disaster = [], [], [], []\n\ncounter = 0\nfor word, count in counter_disaster.most_common()[0:100]:\n    if (word not in stopwords and counter < 15):\n        counter += 1\n        x_disaster.append(word)\n        y_disaster.append(count)\n\ncounter = 0\nfor word, count in counter_non_disaster.most_common()[0:100]:\n    if (word not in stopwords and counter < 15):\n        counter += 1\n        x_non_disaster.append(word)\n        y_non_disaster.append(count)\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(x=y_disaster, y=x_disaster, orient='h', palette=\"Reds_d\", ax=ax[0])\nsns.barplot(x=y_non_disaster, y=x_non_disaster, orient='h', palette=\"Blues_d\", ax=ax[1])\nax[0].set_title(\"Top 15 Non-Stopwords - Disaster Tweets\")\nax[0].set_xlabel(\"Word Frequency\")\nax[1].set_title(\"Top 15 Non-Stopwords - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Word Frequency\")\nplt.tight_layout()\nplt.show()","c32e5fc1":"def bigrams(target):\n    corpus = train[train[\"target\"] == target][\"text\"]\n    count_vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = count_vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in count_vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq","210073c1":"bigrams_disaster = bigrams(1)[:15]\nbigrams_non_disaster = bigrams(0)[:15]\n\nx_disaster, y_disaster = map(list, zip(*bigrams_disaster))\nx_non_disaster, y_non_disaster = map(list, zip(*bigrams_non_disaster))\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(x=y_disaster, y=x_disaster, orient='h', palette=\"Reds_d\", ax=ax[0])\nsns.barplot(x=y_non_disaster, y=x_non_disaster, orient='h', palette=\"Blues_d\", ax=ax[1])\n\nax[0].set_title(\"Top 15 Bigrams - Disaster Tweets\")\nax[0].set_xlabel(\"Word Frequency\")\nax[1].set_title(\"Top 15 Bigrams - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Word Frequency\")\nplt.tight_layout()\nplt.show()","173d6f9c":"target_vc = train[\"target\"].value_counts(normalize=True)\nprint(\"Not Disaster: {:.2%}, Disaster: {:.2%}\".format(target_vc[0], target_vc[1]))\nsns.barplot(x=target_vc.index, y=target_vc)\nplt.title(\"Histogram of Disaster vs. Non-Disaster\")\nplt.xlabel(\"0 = Non-Disaster, 1 = Disaster\")\nplt.show()","9ad02dee":"We can pull out non-disaster tweets containing words that we would expect to be associated with disasters. Clearly, an accurate model will need to take context into account.","84020d07":"**Common Bigrams**\n\nFinally, let's look at the most common bigrams (word pairs).","670994dc":"Let's sort our dataframe to see which keywords are most often correlated with disaster.","0f71a8cb":"The shortest tweet is 7 characters long; the longest tweet is 157 characters long.","0622cde0":"Likewise, we can reverse the sort order to see the least \"disastrous\" words. Some of these are surprising!","02a4b77b":"Interestingly, observe that the location histogram does vary based on disaster vs. non-disaster content. For example, Nigeria is the top 3rd location for disaster tweets yet does not appear in the top 30 locations for non-disaster tweets. Similar observations can be made for India and Mumbai.","63012dbb":"As we can see when we pull out the top 30 keywords for disaster vs non-disaster tweets, word appearance alone is not sufficient to classify content. For example, \"body%20bags\" and \"armageddon\" are the top 1st and 2nd keyword for non-disaster tweets!","bab61e0a":"### Text aka Tweet Content\n\n**Tweet Length**\n\nWhat is the distribution of tweet length?","a8b69a62":"### Location\n\nWhere do most tweet originate? As we can see in the histogram below, USA\/United States is the most common tweet origin. Locations range from specific cities, such as Chicago, IL, to more general (and perhaps less helpful) regions, such as Earth and Everywhere.","b6fca591":"**Common Words**\n\nWith that out of the way, let's examine the most common non-stopwords.","d9e88937":"**Common Stopwords**\n\nNow, let's see the most common stopwords, i.e. the words that should be ignored. This is not a particularly illuminating exercise, however it is a good refresher for using the NLTK package.","e0381bb8":"### Conclusion\n\nThanks very much for reading; I hope you enjoyed this exploratory data analysis. If you did, be sure to upvote so you can find this notebook again easily in your Favorites tab.\n\nSuggestions for additional EDA? Please leave a comment below.\n\nUntil next time, happy coding :)","e06481a9":"Again, we see no striking difference between histograms of average word length for disaster vs. non-disaster tweets.","876b3bc8":"However, no strong pattern appears when we create separate histograms for disaster vs. non-disaster tweets.","542cab27":"If we look at the distribution of tweet length for disaster and non-disaster tweets separately, we see that disaster tweets tend to be lengthier.","63f5093f":"### Explore the Data","cb1543eb":"**Average Word Length**\n\nWhat about average word length?","89e5fbe7":"Overall, there are 221 different keywords associated with tweets.","f96168de":"There are a significant number of missing values in the keyword and location columns.","ec71cfd0":"### Target\n\nDo we have approximately equal numbers of true\/false targets? More or less. Based on the histogram below, we see that 43% of tweets correspond to real disasters.","972bf4b4":"We can also calculate the probability of disaster given the appearance of a keyword.","b5a1f6c2":"# Disaster Tweets: Exploratory Data Analysis\n\nGreetings! Thanks for checking out my code. \n\nIn this notebook, we will conduct basic EDA to get a better understanding of our dataset and how features are distributed. We will also utilize the NLTK package to parse common words and stopwords.","41889743":"There are 7613 observations in the training data. The test dataset is less than half this size, totalling 3263 rows.","336b44d7":"**Number of Words**\n\nSimilarly, we can count the number of words in each tweet and examine the distribution.","6d03ca2c":"### Keyword\n\nWhat are some of the most commonly used keywords?","c5addc13":"**Data Dictionary**\n\n* id - unique identifier\n* keyword - a specific keyword from the tweet\n* location - origin of tweet\n* text - content of tweet\n* target - binary indicator denoting whether the tweet is about a disaster, our value to predict","b96eaff1":"Overall, there are 3341 unique locations."}}