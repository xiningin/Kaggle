{"cell_type":{"a4b95972":"code","47473772":"code","e151503e":"code","a52e2093":"code","3a156b27":"code","2cc0b936":"code","d67ad33a":"code","991022d2":"code","4951a8b6":"code","66caf83a":"code","f0b2d6f9":"code","930031a7":"code","873c48be":"code","e59c9d44":"code","6570a8d5":"code","cdbfcb5b":"code","3fa77a72":"code","f45cba80":"code","4612049b":"code","0a60fe87":"code","3cfdf13e":"code","f4b4d942":"code","a8e28d46":"code","86467947":"code","ae96903f":"code","934ebbf5":"code","62f3c0b9":"code","62d6e1c0":"code","6816811f":"code","b776bf63":"code","474d45d8":"code","103c434c":"code","08e2b21f":"code","f85a3f3c":"code","6fa1e4cb":"code","622a4744":"code","54c7f85c":"code","c05e282a":"code","e36884d1":"code","b2260fdc":"code","49c0b4fe":"code","ae7296d4":"code","cdd97537":"code","c63e3dac":"code","e8c5e747":"code","987fb3c1":"code","5deb1be8":"code","9248d911":"code","586a5b2b":"code","f06a929a":"code","f7b923b3":"code","48fa160f":"code","a43afa26":"code","774825a9":"code","8c122c14":"code","ebea5ada":"code","3af6a209":"code","54a51ceb":"code","632fe799":"code","e10ac8c6":"markdown","d32503c6":"markdown","fe40c3ad":"markdown","61acbf37":"markdown","a972376b":"markdown","d749faf7":"markdown","00b9c5a4":"markdown","f6e09f6a":"markdown","e86c5fb8":"markdown","f35de343":"markdown","fc764509":"markdown","3c7bfad0":"markdown","a282b799":"markdown","6ef996c9":"markdown","a5a91f5f":"markdown","71e00a04":"markdown","ddadb09a":"markdown","4c74cda8":"markdown","aa244fd7":"markdown","ae3f5cfa":"markdown","b91d003e":"markdown","5550542a":"markdown","55b48626":"markdown","ada0df24":"markdown","9e1610dd":"markdown","17c83caa":"markdown","5d447948":"markdown","000cc4c7":"markdown","45fa5282":"markdown","a20dda86":"markdown","c2835d63":"markdown","c7fe2aea":"markdown","133ee0c4":"markdown","8f82dd8e":"markdown","b590db0c":"markdown","37ab9237":"markdown","8709234c":"markdown","848715dc":"markdown","844bbb55":"markdown","8fd932e9":"markdown","00c6a1b4":"markdown","a78db2b0":"markdown","5f7a8a1d":"markdown","31e5810a":"markdown","5450228c":"markdown","18daa0e6":"markdown","7aa35087":"markdown","b96d94f3":"markdown","6e173610":"markdown","75f991cb":"markdown","49904817":"markdown","46fc7010":"markdown","2f15f23c":"markdown","3afa25f6":"markdown","06704e89":"markdown","34e3b21c":"markdown","5d79841c":"markdown","2a866759":"markdown","84a497a3":"markdown","9ee2d618":"markdown","5bf380d3":"markdown","6a4882bf":"markdown","afb8ba07":"markdown","f206c68e":"markdown","3e4a9426":"markdown","88610c05":"markdown"},"source":{"a4b95972":"class KernelSettings:\n    \n    def __init__(self, fit_baseline=False, fit_improved_baseline=False):\n        self.fit_baseline = fit_baseline\n        self.fit_improved_baseline = fit_improved_baseline","47473772":"kernelsettings = KernelSettings(fit_baseline=False, fit_improved_baseline=True)","e151503e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom PIL import Image\nfrom scipy.misc import imread\n\nimport tensorflow as tf\nsns.set()\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# Any results you write to the current directory are saved as output.","a52e2093":"train_labels = pd.read_csv(\"..\/input\/human-protein-atlas-image-classification\/train.csv\")\ntrain_labels.head()","3a156b27":"train_labels.shape[0]","2cc0b936":"label_names = {\n    0:  \"Nucleoplasm\",  \n    1:  \"Nuclear membrane\",   \n    2:  \"Nucleoli\",   \n    3:  \"Nucleoli fibrillar center\",   \n    4:  \"Nuclear speckles\",\n    5:  \"Nuclear bodies\",   \n    6:  \"Endoplasmic reticulum\",   \n    7:  \"Golgi apparatus\",   \n    8:  \"Peroxisomes\",   \n    9:  \"Endosomes\",   \n    10:  \"Lysosomes\",   \n    11:  \"Intermediate filaments\",   \n    12:  \"Actin filaments\",   \n    13:  \"Focal adhesion sites\",   \n    14:  \"Microtubules\",   \n    15:  \"Microtubule ends\",   \n    16:  \"Cytokinetic bridge\",   \n    17:  \"Mitotic spindle\",   \n    18:  \"Microtubule organizing center\",   \n    19:  \"Centrosome\",   \n    20:  \"Lipid droplets\",   \n    21:  \"Plasma membrane\",   \n    22:  \"Cell junctions\",   \n    23:  \"Mitochondria\",   \n    24:  \"Aggresome\",   \n    25:  \"Cytosol\",   \n    26:  \"Cytoplasmic bodies\",   \n    27:  \"Rods & rings\"\n}\n\nreverse_train_labels = dict((v,k) for k,v in label_names.items())\n\ndef fill_targets(row):\n    row.Target = np.array(row.Target.split(\" \")).astype(np.int)\n    for num in row.Target:\n        name = label_names[int(num)]\n        row.loc[name] = 1\n    return row","d67ad33a":"for key in label_names.keys():\n    train_labels[label_names[key]] = 0","991022d2":"train_labels = train_labels.apply(fill_targets, axis=1)\ntrain_labels.head()","4951a8b6":"target_counts = train_labels.drop([\"Id\", \"Target\"],axis=1).sum(axis=0).sort_values(ascending=False)\nplt.figure(figsize=(15,15))\nsns.barplot(y=target_counts.index.values, x=target_counts.values, order=target_counts.index)","66caf83a":"train_labels[\"number_of_targets\"] = train_labels.drop([\"Id\", \"Target\"],axis=1).sum(axis=1)\ncount_perc = np.round(100 * train_labels[\"number_of_targets\"].value_counts() \/ train_labels.shape[0], 2)\nplt.figure(figsize=(20,5))\nsns.barplot(x=count_perc.index.values, y=count_perc.values, palette=\"Reds\")\nplt.xlabel(\"Number of targets per image\")\nplt.ylabel(\"% of data\")","f0b2d6f9":"plt.figure(figsize=(15,15))\nsns.heatmap(train_labels[train_labels.number_of_targets>1].drop(\n    [\"Id\", \"Target\", \"number_of_targets\"],axis=1\n).corr(), cmap=\"RdYlBu\", vmin=-1, vmax=1)","930031a7":"def find_counts(special_target, labels):\n    counts = labels[labels[special_target] == 1].drop(\n        [\"Id\", \"Target\", \"number_of_targets\"],axis=1\n    ).sum(axis=0)\n    counts = counts[counts > 0]\n    counts = counts.sort_values()\n    return counts","873c48be":"lyso_endo_counts = find_counts(\"Lysosomes\", train_labels)\n\nplt.figure(figsize=(10,3))\nsns.barplot(x=lyso_endo_counts.index.values, y=lyso_endo_counts.values, palette=\"Blues\")","e59c9d44":"rod_rings_counts = find_counts(\"Rods & rings\", train_labels)\nplt.figure(figsize=(15,3))\nsns.barplot(x=rod_rings_counts.index.values, y=rod_rings_counts.values, palette=\"Greens\")","6570a8d5":"peroxi_counts = find_counts(\"Peroxisomes\", train_labels)\n\nplt.figure(figsize=(15,3))\nsns.barplot(x=peroxi_counts.index.values, y=peroxi_counts.values, palette=\"Reds\")","cdbfcb5b":"tubeends_counts = find_counts(\"Microtubule ends\", train_labels)\n\nplt.figure(figsize=(15,3))\nsns.barplot(x=tubeends_counts.index.values, y=tubeends_counts.values, palette=\"Purples\")","3fa77a72":"nuclear_speckles_counts = find_counts(\"Nuclear speckles\", train_labels)\n\nplt.figure(figsize=(15,3))\nsns.barplot(x=nuclear_speckles_counts.index.values, y=nuclear_speckles_counts.values, palette=\"Oranges\")\nplt.xticks(rotation=\"70\")","f45cba80":"from os import listdir\n\nfiles = listdir(\"..\/input\/human-protein-atlas-image-classification\/train\")\nfor n in range(10):\n    print(files[n])","4612049b":"len(files) \/ 4 == train_labels.shape[0]","0a60fe87":"train_path = \"..\/input\/human-protein-atlas-image-classification\/train\/\"","3cfdf13e":"def load_image(basepath, image_id):\n    images = np.zeros(shape=(4,512,512))\n    images[0,:,:] = imread(basepath + image_id + \"_green\" + \".png\")\n    images[1,:,:] = imread(basepath + image_id + \"_red\" + \".png\")\n    images[2,:,:] = imread(basepath + image_id + \"_blue\" + \".png\")\n    images[3,:,:] = imread(basepath + image_id + \"_yellow\" + \".png\")\n    return images\n\ndef make_image_row(image, subax, title):\n    subax[0].imshow(image[0], cmap=\"Greens\")\n    subax[1].imshow(image[1], cmap=\"Reds\")\n    subax[1].set_title(\"stained microtubules\")\n    subax[2].imshow(image[2], cmap=\"Blues\")\n    subax[2].set_title(\"stained nucleus\")\n    subax[3].imshow(image[3], cmap=\"Oranges\")\n    subax[3].set_title(\"stained endoplasmatic reticulum\")\n    subax[0].set_title(title)\n    return subax\n\ndef make_title(file_id):\n    file_targets = train_labels.loc[train_labels.Id==file_id, \"Target\"].values[0]\n    title = \" - \"\n    for n in file_targets:\n        title += label_names[n] + \" - \"\n    return title","f4b4d942":"class TargetGroupIterator:\n    \n    def __init__(self, target_names, batch_size, basepath):\n        self.target_names = target_names\n        self.target_list = [reverse_train_labels[key] for key in target_names]\n        self.batch_shape = (batch_size, 4, 512, 512)\n        self.basepath = basepath\n    \n    def find_matching_data_entries(self):\n        train_labels[\"check_col\"] = train_labels.Target.apply(\n            lambda l: self.check_subset(l)\n        )\n        self.images_identifier = train_labels[train_labels.check_col==1].Id.values\n        train_labels.drop(\"check_col\", axis=1, inplace=True)\n    \n    def check_subset(self, targets):\n        return np.where(set(self.target_list).issuperset(set(targets)), 1, 0)\n    \n    def get_loader(self):\n        filenames = []\n        idx = 0\n        images = np.zeros(self.batch_shape)\n        for image_id in self.images_identifier:\n            images[idx,:,:,:] = load_image(self.basepath, image_id)\n            filenames.append(image_id)\n            idx += 1\n            if idx == self.batch_shape[0]:\n                yield filenames, images\n                filenames = []\n                images = np.zeros(self.batch_shape)\n                idx = 0\n        if idx > 0:\n            yield filenames, images\n            ","a8e28d46":"your_choice = [\"Lysosomes\", \"Endosomes\"]\nyour_batch_size = 3","86467947":"imageloader = TargetGroupIterator(your_choice, your_batch_size, train_path)\nimageloader.find_matching_data_entries()\niterator = imageloader.get_loader()","ae96903f":"file_ids, images = next(iterator)\n\nfig, ax = plt.subplots(len(file_ids),4,figsize=(20,5*len(file_ids)))\nif ax.shape == (4,):\n    ax = ax.reshape(1,-1)\nfor n in range(len(file_ids)):\n    make_image_row(images[n], ax[n], make_title(file_ids[n]))","934ebbf5":"train_files = listdir(\"..\/input\/human-protein-atlas-image-classification\/train\")\ntest_files = listdir(\"..\/input\/human-protein-atlas-image-classification\/test\")\npercentage = np.round(len(test_files) \/ len(train_files) * 100)\n\nprint(\"The test set size turns out to be {} % compared to the train set.\".format(percentage))","62f3c0b9":"from sklearn.model_selection import RepeatedKFold\n\nsplitter = RepeatedKFold(n_splits=3, n_repeats=1, random_state=0)","62d6e1c0":"partitions = []\n\nfor train_idx, test_idx in splitter.split(train_labels.index.values):\n    partition = {}\n    partition[\"train\"] = train_labels.Id.values[train_idx]\n    partition[\"validation\"] = train_labels.Id.values[test_idx]\n    partitions.append(partition)\n    print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n    print(\"TRAIN:\", len(train_idx), \"TEST:\", len(test_idx))","6816811f":"partitions[0][\"train\"][0:5]","b776bf63":"class ModelParameter:\n    \n    def __init__(self, basepath,\n                 num_classes=28,\n                 image_rows=512,\n                 image_cols=512,\n                 batch_size=200,\n                 n_channels=1,\n                 row_scale_factor=4,\n                 col_scale_factor=4,\n                 shuffle=False,\n                 n_epochs=1):\n        self.basepath = basepath\n        self.num_classes = num_classes\n        self.image_rows = image_rows\n        self.image_cols = image_cols\n        self.batch_size = batch_size\n        self.n_channels = n_channels\n        self.shuffle = shuffle\n        self.row_scale_factor = row_scale_factor\n        self.col_scale_factor = col_scale_factor\n        self.scaled_row_dim = np.int(self.image_rows \/ self.row_scale_factor)\n        self.scaled_col_dim = np.int(self.image_cols \/ self.col_scale_factor)\n        self.n_epochs = n_epochs","474d45d8":"parameter = ModelParameter(train_path)","103c434c":"from skimage.transform import resize\n\nclass ImagePreprocessor:\n    \n    def __init__(self, modelparameter):\n        self.parameter = modelparameter\n        self.basepath = self.parameter.basepath\n        self.scaled_row_dim = self.parameter.scaled_row_dim\n        self.scaled_col_dim = self.parameter.scaled_col_dim\n        self.n_channels = self.parameter.n_channels\n    \n    def preprocess(self, image):\n        image = self.resize(image)\n        image = self.reshape(image)\n        image = self.normalize(image)\n        return image\n    \n    def resize(self, image):\n        image = resize(image, (self.scaled_row_dim, self.scaled_col_dim))\n        return image\n    \n    def reshape(self, image):\n        image = np.reshape(image, (image.shape[0], image.shape[1], self.n_channels))\n        return image\n    \n    def normalize(self, image):\n        image \/= 255 \n        return image\n    \n    def load_image(self, image_id):\n        image = np.zeros(shape=(512,512,4))\n        image[:,:,0] = imread(self.basepath + image_id + \"_green\" + \".png\")\n        image[:,:,1] = imread(self.basepath + image_id + \"_blue\" + \".png\")\n        image[:,:,2] = imread(self.basepath + image_id + \"_red\" + \".png\")\n        image[:,:,3] = imread(self.basepath + image_id + \"_yellow\" + \".png\")\n        return image[:,:,0:self.parameter.n_channels]\n        ","08e2b21f":"preprocessor = ImagePreprocessor(parameter)","f85a3f3c":"example = images[0,0]\npreprocessed = preprocessor.preprocess(example)\nprint(example.shape)\nprint(preprocessed.shape)\n\nfig, ax = plt.subplots(1,2,figsize=(20,10))\nax[0].imshow(example, cmap=\"Greens\")\nax[1].imshow(preprocessed.reshape(parameter.scaled_row_dim,parameter.scaled_col_dim), cmap=\"Greens\")","6fa1e4cb":"import keras\n\nclass DataGenerator(keras.utils.Sequence):\n    \n    def __init__(self, list_IDs, labels, modelparameter, imagepreprocessor):\n        self.params = modelparameter\n        self.labels = labels\n        self.list_IDs = list_IDs\n        self.dim = (self.params.scaled_row_dim, self.params.scaled_col_dim)\n        self.batch_size = self.params.batch_size\n        self.n_channels = self.params.n_channels\n        self.num_classes = self.params.num_classes\n        self.shuffle = self.params.shuffle\n        self.preprocessor = imagepreprocessor\n        self.on_epoch_end()\n    \n    def on_epoch_end(self):\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n    \n    def get_targets_per_image(self, identifier):\n        return self.labels.loc[self.labels.Id==identifier].drop(\n                [\"Id\", \"Target\", \"number_of_targets\"], axis=1).values\n            \n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        y = np.empty((self.batch_size, self.num_classes), dtype=int)\n        # Generate data\n        for i, identifier in enumerate(list_IDs_temp):\n            # Store sample\n            image = self.preprocessor.load_image(identifier)\n            image = self.preprocessor.preprocess(image)\n            X[i] = image\n            # Store class\n            y[i] = self.get_targets_per_image(identifier)\n        return X, y\n    \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) \/ self.batch_size))\n    \n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp)\n        return X, y","622a4744":"class PredictGenerator:\n    \n    def __init__(self, predict_Ids, imagepreprocessor, predict_path):\n        self.preprocessor = imagepreprocessor\n        self.preprocessor.basepath = predict_path\n        self.identifiers = predict_Ids\n    \n    def predict(self, model):\n        y = np.empty(shape=(len(self.identifiers), self.preprocessor.parameter.num_classes))\n        for n in range(len(self.identifiers)):\n            image = self.preprocessor.load_image(self.identifiers[n])\n            image = self.preprocessor.preprocess(image)\n            image = image.reshape((1, *image.shape))\n            y[n] = model.predict(image)\n        return y","54c7f85c":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.losses import binary_crossentropy\nfrom keras.optimizers import Adadelta\nfrom keras.models import load_model\n\n\nclass BaseLineModel:\n    \n    def __init__(self, modelparameter):\n        self.params = modelparameter\n        self.num_classes = self.params.num_classes\n        self.img_rows = self.params.scaled_row_dim\n        self.img_cols = self.params.scaled_col_dim\n        self.n_channels = self.params.n_channels\n        self.input_shape = (self.img_rows, self.img_cols, self.n_channels)\n        self.my_metrics = ['accuracy']\n    \n    def build_model(self):\n        self.model = Sequential()\n        self.model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=self.input_shape))\n        self.model.add(Conv2D(32, (3, 3), activation='relu'))\n        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n        self.model.add(Dropout(0.25))\n        self.model.add(Flatten())\n        self.model.add(Dense(64, activation='relu'))\n        self.model.add(Dropout(0.5))\n        self.model.add(Dense(self.num_classes, activation='sigmoid'))\n    \n    def compile_model(self):\n        self.model.compile(loss=keras.losses.binary_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=self.my_metrics)\n    \n    def set_generators(self, train_generator, validation_generator):\n        self.training_generator = train_generator\n        self.validation_generator = validation_generator\n    \n    def learn(self):\n        return self.model.fit_generator(generator=self.training_generator,\n                    validation_data=self.validation_generator,\n                    epochs=self.params.n_epochs, \n                    use_multiprocessing=True,\n                    workers=8)\n    \n    def score(self):\n        return self.model.evaluate_generator(generator=self.validation_generator,\n                                      use_multiprocessing=True, \n                                      workers=8)\n    \n    def predict(self, predict_generator):\n        y = predict_generator.predict(self.model)\n        return y\n    \n    def save(self, modeloutputpath):\n        self.model.save(modeloutputpath)\n    \n    def load(self, modelinputpath):\n        self.model = load_model(modelinputpath)","c05e282a":"# Datasets\npartition = partitions[0]\nlabels = train_labels\n\nprint(\"Number of samples in train: {}\".format(len(partition[\"train\"])))\nprint(\"Number of samples in validation: {}\".format(len(partition[\"validation\"])))","e36884d1":"training_generator = DataGenerator(partition['train'], labels, parameter, preprocessor)\nvalidation_generator = DataGenerator(partition['validation'], labels, parameter, preprocessor)\npredict_generator = PredictGenerator(partition['validation'], preprocessor, train_path)","b2260fdc":"# Run computation and store results as csv\nif kernelsettings.fit_baseline == True:\n    model = BaseLineModel(parameter)\n    model.build_model()\n    model.compile_model()\n    model.set_generators(training_generator, validation_generator)\n    history = model.learn()\n    #model.save(\"baseline_model.h5\")\n    proba_predictions = model.predict(predict_generator)\n    baseline_proba_predictions = pd.DataFrame(proba_predictions, columns=train_labels.drop(\n        [\"Target\", \"number_of_targets\", \"Id\"], axis=1).columns)\n    baseline_proba_predictions.to_csv(\"baseline_predictions.csv\")\n# If you already have done a baseline fit once, \n# you can load predictions as csv and further fitting is not neccessary:\nelse:\n    baseline_proba_predictions = pd.read_csv(\"..\/input\/protein-atlas-eab-predictions\/baseline_predictions.csv\", index_col=0)","49c0b4fe":"validation_labels = train_labels.loc[train_labels.Id.isin(partition[\"validation\"])]","ae7296d4":"print(validation_labels.shape)\nprint(baseline_proba_predictions.shape)","cdd97537":"baseline_proba_predictions.tail()","c63e3dac":"proba_predictions = baseline_proba_predictions.values","e8c5e747":"hot_values = validation_labels.drop([\"Id\", \"Target\", \"number_of_targets\"], axis=1).values.flatten()\none_hot = (hot_values.sum()) \/ hot_values.shape[0] * 100\nzero_hot = (hot_values.shape[0] - hot_values.sum()) \/ hot_values.shape[0] * 100\n\nfig, ax = plt.subplots(1,2, figsize=(20,5))\nsns.distplot(proba_predictions.flatten() * 100, color=\"DodgerBlue\", ax=ax[0])\nax[0].set_xlabel(\"Probability in %\")\nax[0].set_ylabel(\"Density\")\nax[0].set_title(\"Predicted probabilities\")\nsns.barplot(x=[\"label = 0\", \"label = 1\"], y=[zero_hot, one_hot], ax=ax[1])\nax[1].set_ylim([0,100])\nax[1].set_title(\"True target label count\")\nax[1].set_ylabel(\"Percentage\")","987fb3c1":"mean_predictions = np.mean(proba_predictions, axis=0)\nstd_predictions = np.std(proba_predictions, axis=0)\nmean_targets = validation_labels.drop([\"Id\", \"Target\", \"number_of_targets\"], axis=1).mean()\n\nlabels = validation_labels.drop([\"Id\", \"Target\", \"number_of_targets\"], axis=1).columns.values\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.barplot(x=labels,\n            y=mean_predictions,\n            ax=ax[0])\nax[0].set_xticklabels(labels=labels,\n                      rotation=90)\nax[0].set_ylabel(\"Mean predicted probability\")\nax[0].set_title(\"Mean predicted probability per class over all samples\")\nsns.barplot(x=labels,\n           y=std_predictions,\n           ax=ax[1])\nax[1].set_xticklabels(labels=labels,\n                      rotation=90)\nax[1].set_ylabel(\"Standard deviation\")\nax[1].set_title(\"Standard deviation of predicted probability per class over all samples\")","5deb1be8":"fig, ax = plt.subplots(1,1,figsize=(20,5))\nsns.barplot(x=labels, y=mean_targets.values, ax=ax)\nax.set_xticklabels(labels=labels,\n                      rotation=90)\nax.set_ylabel(\"Percentage of hot (1)\")\nax.set_title(\"Percentage of hot counts (ones) per target class\")","9248d911":"feature = \"Cytosol\"","586a5b2b":"plt.figure(figsize=(20,5))\nsns.distplot(baseline_proba_predictions[feature].values[0:-10], color=\"Purple\")\nplt.xlabel(\"Predicted probabilites of {}\".format(feature))\nplt.ylabel(\"Density\")\nplt.xlim([0,1])","f06a929a":"wishlist = [\"Nucleoplasm\", \"Cytosol\", \"Plasma membrane\"]","f7b923b3":"class ImprovedDataGenerator(DataGenerator):\n    \n    # in contrast to the base DataGenerator we add a target wishlist to init\n    def __init__(self, list_IDs, labels, modelparameter, imagepreprocessor, target_wishlist):\n        super().__init__(list_IDs, labels, modelparameter, imagepreprocessor)\n        self.target_wishlist = target_wishlist\n    \n    def get_targets_per_image(self, identifier):\n        return self.labels.loc[self.labels.Id==identifier][self.target_wishlist].values","48fa160f":"import keras.backend as K\n\ndef base_f1(y_true, y_pred):\n    y_pred = K.round(y_pred)\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + K.epsilon())\n    r = tp \/ (tp + fn + K.epsilon())\n\n    f1 = 2*p*r \/ (p+r+K.epsilon())\n    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n    return f1\n\ndef f1_min(y_true, y_pred):\n    f1 = base_f1(y_true, y_pred)\n    return K.min(f1)\n\ndef f1_max(y_true, y_pred):\n    f1 = base_f1(y_true, y_pred)\n    return K.max(f1)\n\ndef f1_mean(y_true, y_pred):\n    f1 = base_f1(y_true, y_pred)\n    return K.mean(f1)\n\ndef f1_std(y_true, y_pred):\n    f1 = base_f1(y_true, y_pred)\n    return K.std(f1)","a43afa26":"class TrackHistory(keras.callbacks.Callback):\n    \n    def on_train_begin(self, logs={}):\n        self.losses = []\n\n    def on_batch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))","774825a9":"class ImprovedModel(BaseLineModel):\n    \n    def __init__(self, modelparameter, my_metrics=[f1_mean, f1_std, f1_min, f1_max]):\n        super().__init__(modelparameter)\n        self.my_metrics = my_metrics\n    \n    def learn(self):\n        self.history = TrackHistory()\n        return self.model.fit_generator(generator=self.training_generator,\n                    validation_data=self.validation_generator,\n                    epochs=self.params.n_epochs, \n                    use_multiprocessing=True,\n                    workers=8,\n                    callbacks = [self.history])","8c122c14":"parameter = ModelParameter(train_path, num_classes=len(wishlist), n_epochs=5, batch_size=64)\npreprocessor = ImagePreprocessor(parameter)\nlabels = train_labels","ebea5ada":"training_generator = ImprovedDataGenerator(partition['train'], labels,\n                                           parameter, preprocessor, wishlist)\nvalidation_generator = ImprovedDataGenerator(partition['validation'], labels,\n                                             parameter, preprocessor, wishlist)\npredict_generator = PredictGenerator(partition['validation'], preprocessor, train_path)","3af6a209":"# Run computation and store results as csv\nif kernelsettings.fit_improved_baseline == True:\n    model = ImprovedModel(parameter)\n    model.build_model()\n    model.compile_model()\n    model.set_generators(training_generator, validation_generator)\n    epoch_history = model.learn()\n    proba_predictions = model.predict(predict_generator)\n    #model.save(\"improved_model.h5\")\n    improved_proba_predictions = pd.DataFrame(proba_predictions, columns=wishlist)\n    improved_proba_predictions.to_csv(\"improved_predictions.csv\")\n# If you already have done a baseline fit once, \n# you can load predictions as csv and further fitting is not neccessary:\nelse:\n    improved_proba_predictions = pd.read_csv(\"..\/input\/protein-atlas-eab-predictions\/improved_predictions.csv\", index_col=0)","54a51ceb":"if kernelsettings.fit_improved_baseline == True:\n    print(epoch_history.history.keys())\n    fig, ax = plt.subplots(2,1,figsize=(20,5))\n    ax[0].plot(model.history.losses, color=\"Red\")\n    ax[0].set_xlabel(\"Update step\")\n    ax[0].set_ylabel(\"Train loss\")\n    ax[0].savefig(\"loss_of_improved_model\", format=\"eps\")\n    ax[1].plot(epoch_history.history[\"val_loss\"], color=\"Green\")\n    ax[1].plot(epoch_history.history[\"loss\"])","632fe799":"fig, ax = plt.subplots(3,1,figsize=(25,15))\nsns.distplot(improved_proba_predictions.values[:,0], color=\"Orange\", ax=ax[0])\nax[0].set_xlabel(\"Predicted probabilites of {}\".format(improved_proba_predictions.columns.values[0]))\nax[0].set_xlim([0,1])\nsns.distplot(improved_proba_predictions.values[:,1], color=\"Purple\", ax=ax[1])\nax[1].set_xlabel(\"Predicted probabilites of {}\".format(improved_proba_predictions.columns.values[1]))\nax[1].set_xlim([0,1])\nsns.distplot(improved_proba_predictions.values[:,2], color=\"Limegreen\", ax=ax[2])\nax[2].set_xlabel(\"Predicted probabilites of {}\".format(improved_proba_predictions.columns.values[2]))\nax[2].set_xlim([0,1])","e10ac8c6":"Even though this accuracy looks nice it's an illusion! We are far away from a good model. Let's try to understand why...","d32503c6":"# Welcome to the Human Protein Atlas Competition!\n\nI started this kernel notebook to explore the data and build a simple baseline model to play with. I have never mind that the results are useful for many kagglers that like to start with the competition but need some starter code or some inspiration. Thank you for pushing this kernel that far! :-)\n\nIf you have just found this kernel, here is a **short summary of what you can find**:\n\n1. Encoding of binary target labels out of the given multilabel list per image,\n2. Visual analysis of target protein distribution in the train set,\n3. A simple image generator that yields images of a target-protein-wishlist. Each sample that has at least one match with this list is returned.\n4. Some ideas on validation.\n5. A baseline model build with keras that is supported by:\n    * A modelparameter class that holds all parameters that are necessary to build the model, to load the data and to preprocess the images.\n    * A data generator that can be used with CPU\/GPU computing to perform training and validation.\n    * An image preprocessor that rescales, reshapes and normalizes the images for feeding into the model.\n6. Ideas on how to improve the baseline model by tracking loss with a keras callback. \n7. Some ideas on how to proceed. (coming soon)","fe40c3ad":"### Data Generator\n\nI highly build upon the [nice data generator presented by Shervine Amidi.](https:\/\/stanford.edu\/~shervine\/blog\/keras-how-to-generate-data-on-the-fly) Thank you! :-) \n","61acbf37":"### The target wish list\n\nTo introduce a target wishlist that we can change whenever we want we need to improve the data generator. For this purpose we're going to extend the class we have already written. Taking a closer look at the base generator you can see that there is just one line code in def data_generation(self, list_IDs_temp) we have to change, namely the part with y[i] = ... inside the for loop over temp list ids (image identifiers of the batch). To make things easier, I added a small method to the DataGenerator we already had:\n\n```\ndef get_targets_per_image(self, identifier):\n        return self.labels.loc[self.labels.Id==identifier].drop(\n                [\"Id\", \"Target\", \"number_of_targets\"], axis=1).values\n```\n\nThis method just avoids the direct pass to y[i], the targets per image in a batch. Now, we can overwrite this method in our ImprovedDataGenerator without loosing functionality:","a972376b":"## How do the images look like?\n\n","d749faf7":"### Plug and Play\n\n:-) \n\nThis part makes fun! Actually I don't know if our model learns something meaningful. But we can try to find out by improving our model adding **new features** and playing with **different parameter settings**. The latter I like to do in a **plug-and-play style**: Setting the flag *improve=True* adds a change to our model whereas *improve=False* uses the old concept we already used in the baseline. \n\n","00b9c5a4":"### Lysosomes and endosomes\n\nLet's start with these high correlated features!","f6e09f6a":"As our test data is 38 % of size compared to the train set it makes sense to use 3-Fold cross validation where the test set is 33 % of size compared to the train set. As we are working with neural networks that can be demanding in computational resources, let's only use 2 repetitions. ","e86c5fb8":"## How many targets are most common?","f35de343":"## One-Step-Improvement\n\n\nOk, again let's go one step back and choose the most common target proteins that are present in our data: nucleoplasm, cytosol and plasma membrane. If we are not able to predict them we can go home and stay in bed ;-) .","fc764509":"Improve! :-) We can already see that we might need **more than 1 epoch to learn the pattern in the data**. Computing the gradient with batches is a stochastic process: Depending on the batch samples the gradients may sometimes lead to some good update directions in weight space that points to some minimum of loss.... and sometimes they could lead to the opposite. Well this really depends on the samples within one batch. This becomes more robust with **more samples per batch.** But then we will **need even more epochs** as we make less weight updata aka learning steps! But even if this stochastic gradient descent looks wiggly it has an advantage too: It can escape from local minima of the loss function. That's nice. \n\nBefore we improve the model using more epochs and smaller batches, let's **implement a new feature: a loss callback**. This way we can see if the loss decreases during weight update steps! And we can see if we need more epochs or if the loss has already converged and settled down. ","3c7bfad0":"### Take-Away\n\n* We can see that most common protein structures belong to coarse grained cellular components like the plasma membrane, the cytosol and the nucleus. \n* In contrast small components like the lipid droplets, peroxisomes, endosomes, lysosomes, microtubule ends, rods and rings are very seldom in our train data. For these classes the prediction will be very difficult as we have only a few examples that may not cover all variabilities and as our model probably will be confused during ins learning process by the major classes. Due to this confusion we will make less accurate predictions on the minor classes.\n* Consequently accuracy is not the right score here to measure your performance and validation strategy should be very fine. ","a282b799":"## Which proteins occur most often in images?","6ef996c9":"Jeahy! This looks far better than the distributions we obtained with the baseline model! :-) **Thus increasing epochs while decreasing the batch size helped our model as we made more learning steps.** But... as we have seen by the noise of our losses as well we make large jumps in weight space with each update step. That's not nice! Sometimes these jumps could even lead to exploding losses as well. Then the jump was so big that we escaped from nice regions that lead to a local minimum of loss.","a5a91f5f":"### To which targets do the high and small predicted probabilities belong to?","71e00a04":"## Latest important updates\n\nBeside building competition code I'm still updating this kernel with ideas and code. To make it easier for you to checkout new content, here is an **update summary** of the latest changes:\n\n* DataGenerator: The **multithreaded version to perform predictions with DataGenerator was dropped**. I've tried out various different methods the last days (using ImageGenerator, flow_form_dataframe etc.) but every time there were some annoying problems. As training is the most expensive part, I decided to keep multithreading for fitting but to drop if for making predictions. \n* **Bug-fix** in modelparameter class. **Shuffle was always set to True**. Now default is False and you can change it interactively. \n* **Modelparameter class** has now an **attribute basepath** that points to your desired image directory. If you perform a prediction with same modelparameters, the path is overwritten during prediction with predict_path. Feels dirty, this will be changed soon. \n* The **image preprocessor is now responsible for loading the images**. With default model parameter n_channels=1 it only returns the green image. If you like to change that increase n_channels as you like or do it by hand but make sure than modelparameters has a suitable n_channels. \n* The **f1 score** of the improved metric is extended by **min, max, std** besides the mean to gain more insights of the f1 score distribution between different target classes. \n* Some ideas what causes the gradient noise and how to set up solutions. ","ddadb09a":"Let's try to visualize specific target groups. **In this example we will see images that contain the protein structures lysosomes or endosomes**. Set target values of your choice and the target group iterator will collect all images that are subset of your choice:","4c74cda8":"### Image Preprocessor\n\nLet's write a simple image preprocessor that handles for example the rescaling of the images. Perhaps we can expand its functionality during improvement of the baseline model. ","aa244fd7":"### Take-Away\n\n* Looking at this few examples we can already obtain some insights:\n    * The staining of target proteins in the green channel was not equally successful. The **images differ in their intensities and the target proteins are not always located the same way**. The first image you can get by the loader shows endosomes that are spread all over the cells and in the second and third you can find endosomes and lysosomes more concetrated around the nucleus. \n    * Especially **in the red channel we can see morphological differences**. It looks like if the cells are of different types. This is just an assumption but perhaps one could use the red channel information to reveal cell types. ","ae3f5cfa":"### Training the baseline on the first cv-fold","b91d003e":"## Where to go next?","5550542a":"Ah, ok, great! It seems that for one image id, there are different color channels present. Looking into the data description of this competition we can find that:\n\n* Each image is actually splitted into 4 different image files. \n* These 4 files correspond to 4 different filter:\n    * a **green** filter for the **target protein structure** of interest\n    * **blue** landmark filter for the **nucleus**\n    * **red** landmark filter for **microtubules**\n    * **yellow** landmark filter for the **endoplasmatic reticulum**\n* Each image is of size 512 x 512","55b48626":"### CNN Baseline model using keras","ada0df24":"#### Looking at a preprocessed example image","9e1610dd":"## Helper code","17c83caa":"### Take-Away\n\n* We can see that our model was always very uncertain to predict the presence of a target protein. All probabilities are close to zero and there are only a few with targets where our model predicted a protein structure with higher than 10 %.\n* If we take a look at the true target label count we can see that most of our targets are filled with zero. This corresponds to an absence of corresponding target proteins. This makes sense: For each image we have a high probability to contain either 1 or 2 target protein structures. Their label values are one whereas all others are zero. \n* Consequently our high accuracy belongs to the high correct prediction of the absence of target proteins. In contrast we weren't able to predict the presence of a target protein which is the most relevant part! \n* Now a bell should ring :-) Have you ever heard about imbalanced classes and model confusion? ","5d447948":"### Take-away\n\n* We can see that many targets only have very slight correlations. \n* In contrast, endosomes and lysosomes often occur together and sometimes seem to be located at the endoplasmatic reticulum. \n* In addition we find that the mitotic spindle often comes together with the cytokinetic bridge. This makes sense as both are participants for cellular division. And in this process microtubules and thier ends are active and participate as well. Consequently we find a positive correlation between these targets.","000cc4c7":"To understand the performance of our model we will use **k-fold cross validation**. The train data is splitted into k chunks and each chunk is used once for testing the prediction performance whereas the others are used for training. As our targets show relationships seemed to be grouped somehow the performance per test chunk probably highly depends on the target distribution per test chunk. For example there could be chunks with very seldom targets that may obtain a bad score and some chunks with very common targets and a very good score. To reduce this effect, we will **repeat the K-Fold several times** and look at scoing distributions in the end.","45fa5282":"### Microtubule ends","a20dda86":"Hmmmm this still looks not good :-( Have to search even further. ","c2835d63":"## How do images of specific targets look like?\n\nWhile looking at examples, we can build an batch loader:","c7fe2aea":"### Rods and rings","133ee0c4":"To keep the kernel dense, the target group iterator has a batch size which stands for the number of examples you like to look at once. In this example you can see a maximum amount of 3 images at one iteration.  **To observe the next 3 examples of your target group, just run the cell below again.** This way you can run the cell until you have seen all images of your group without polluting the kernel:","8f82dd8e":"## Kernel settings\n\nThis notebook contains model fitting that may take some time. **If you don't like to wait for compution, you can set fit_baseline and\/or fit_improved_baseline of the KernelSettings class to False**:","b590db0c":"This splitter is now a generator. Hence if you call splitters split method it will yield one Fold of the repeated K-Folds. Consequently if we choose n_repeats=2 we will end up with 6 Folds in total: 3 Folds for the first cross validation and again 3 Folds for the repeated cross validation. We will perform the splitting on the image ids. This way we can easily load images and targets given the chunk ids. **Due to performance reasons I will only use one cv-fold to explore results and one repeat!**","37ab9237":"### Does our model try to classify?\n\nIf this is the case and our model starts learning we should see more bimodal distributions of the predicted probability per target label:","8709234c":"## What do the results tell us?\n\nLet's have a look at predicted probabilites per target class:","848715dc":"This way the prediction probabilities of the corresponding model are loaded as csv from added data source. ","844bbb55":"We are computing the change of the loss with respect to a change in the weights for each sample one after another. Consequently in original gradient descent we need to pass the whole dataset once for just one single update step of gradient descent. As our initial weights are not sufficent to solve the classification task we need many such update steps. **But what if the dataset it too large to wait a long time just for one of these steps?**\n\nPerhaps it would be sufficient and already good to use only some $M < N$ of the $N$ samples to compute the gradients $\\partial_{w_{i,j}} E$:\n\n$$ \\partial_{w_{i,j}} E = \\sum_{m=1}^{M} \\partial_{w_{i,j}} E_{m} $$\n\nThis way **we could use only a batch of samples, compute the gradients and perform the update of weights**. After that we continue by computing the gradients with the next batch. This could be done in sequence of samples or random with replacement. This means that we already start learning without passing the whole dataset. In my case I chose a batch_size of 200. With a total number of samples of around 20000 in my training set this means that I have already done 200 weight updates. **After doing so I have used each sample once and this means the whole dataset was passed through gradient descent**. As far as I know this means, **one epoch**. In addition we can now say that we want to shuffle the samples before doing weight updates after each batch again in the next epoch. This way our model sees a different set, computes some other gradients and hence the loss minimziation could be more robust.   ","8fd932e9":"Ok, now let's increase the number of epochs and decrease the batch_size. This way we use more weight update steps and hopefully makes our model learn more than before: ","00c6a1b4":"### K-Fold Cross-Validation","a78db2b0":"### Take-away\n\n* Most train images only have 1 or two target labels.\n* More than 3 targets are very seldom!","5f7a8a1d":"Let's see how many test and train samples we have in this competition:","31e5810a":"How many samples do we have?","5450228c":"### Add scoring metrics\n\nWe have already seen that the accuracy score is an illusion and does not help to figure out how good our predictions are. Let's take a closer look to the competition scoring and alternatives:\n\n* **F1 macro score**: Check out this [nice implementation of Guglielmo Camporese](https:\/\/www.kaggle.com\/guglielmocamporese\/macro-f1-score-keras). Thank you very much! We can easily add it to our model.\n* But even with that score we should be careful! We have 28 different classes that are **very different in their frequency of being present**.  In addition we have to deal with **highly imbalanced classes per single target**. Even for the most common target nucleoplasm there are only 40 % of samples that show it and 60 % not. This imbalance becomes even more dramatic for seldom targets like rods and rings. We should **attach more importance to true positives**. \n* Well, there is **one problem with the competition score** that we can use to measure the performance of our model: **The f1 mean**. The mean is not robust towards outliers and consequently not very informative to understand the distribution of f1 scores for each target class. Perhaps we will make nice predictions for Cytosol but bad ones for Nucleoplasmn... who knows? Consequently it could be nice to introduce some further statistical quantities like **min** as well as **max** and the **standard deviation** . This way we can see the worst and gain some insights how the scores are spread over the classes. ","18daa0e6":"## Loading packages and data","7aa35087":"### Collecting ideas\n\nNext we need to setup a simple baseline model. This need not be very complex or very good. Its our first attempt to play with and to figure out how to improve. For this purpose let's use the deep learning library [keras](https:\/\/keras.io\/). This tools makes it easy for us to build and train neural networks. First of all, we should collect some ideas:\n\n* To **stay simple let's use only the green channel image of our images per id**. The competition says that it shows the stained target proteins and consequently it's hopefully the most informative one. The other images are like references showing microtubules, nucleus and endoplasmatic reticulum. We don't acutally now how informative they are and in our current state they would blow up our neural network with a huge amount of network weigths that we might not need.\n* Let's use **generators to only load data images of our batch and not all in once**. Using keras fit_generator, evaluate_generator and predict_generator we can directly connect them to keras without worrying much about how keras does its job. For this purpose I highly follow a descprition of a post in the www for which you will find the link below.\n* It could be advantegous to write a **small class that does simple preprocessing per image.** This way we can easily change something of this phase without producing chaos in the model itself or during data loading.   \n* I'm going to use a **small class that hold parameters that are used or shared between the data loader, the image preprocessor and the baseline model**. Passing an instance of this class to them reduced the risk of setting different parameters and obtaining mismatch errors for example during build & compile of the network layers. \n","b96d94f3":"## Building a baseline model","6e173610":"No, it does not seem that our model starts to separate well. The mode is close to the fraction of one-hot-counts over all samples. At least the flat tail gives hope that learning could be in progress.  But even though our next goal should be to find out what to tune in such a way that our model really starts learning! ","75f991cb":"With our new strategy to perform predictions we should obtain the same shape[0] of the true targets and the predicted ones:","49904817":"Ok, now we will create an instance of this class and pass it to the DataGenerator, the BaseLineModel and the ImagePreprocessor.","46fc7010":"## How are special and seldom targets grouped?","2f15f23c":"### Peek into the directory\n\nBefore we start loading images, let's have a look into the train directory to get an impression of what we can find there:","3afa25f6":"You can see that we have lost a lot of information by downscaling the image!","06704e89":"### Take-Away\n\n* Our baseline model seemed to learn something even if this something does not look very nice. \n* Taking a look at the standard deviation we can see that all samples have nearly the same predicted values. There is no deviation, no difference between them. This is of course very bad! :-(\n\nLet's go one step deeper and take a look at the Cytosol (choose another feature if you like ;-)). Here we can see a higher standard deviation than for all other samples and perhaps its corresponding distribution starts to diverge, trying to get bimodal. This would be great at it indicates that the model starts solving the problem of binary classification for this target:","34e3b21c":"### Take-away\n\n* We can see that even with very seldom targets we find some kind of grouping with other targets that reveal where the protein structure seems to be located. \n* For example, we can see that rods and rings have something to do with the nucleus whereas peroxisomes may be located in the nucleus as well as in the cytosol.\n* Perhaps this patterns might help to build a more robust model!  ","5d79841c":"### What does the loss tell us?\n\nThe loss is very noisy! While decreasing the batch size we increased the number of learning steps. Hence our model learns faster. But... with smaller batch size there **are fewer samples to learn from, to compute gradients from**! The gradients we obtain may be very specific to the images and class labels that are covered by the batch of the current learning step. **There was a tradeoff we made**. We gained more learning speed but payed with a reduced gradient quality. Before increasing the batch size again and waiting too long for predictions we might improve by choosing another way:\n\n1. Weight regularization\n2. Gradient clipping\n\nThese two will be the next improvement steps. Nonetheless, one question remains: Has our model started learning? Can we see a separating force that tries to split zero and one predictions?","2a866759":"## Which targets are correlated?\n\nLet's see if we find some correlations between our targets. This way we may already see that some proteins often come together.","84a497a3":"## How can we tackle gradient jiggles?\n\nLet's try to dive deeper into the problem. We compute the gradients with respect to the weights after processing each batch this way:\n\n$$ \\partial_{w_{i,j}} E = \\sum_{m=1}^{M} \\partial_{w_{i,j}} E_{m} $$\n\nLooking at the sum we can see one disadvantage... it's mainly driven by high contributions. **An image in the batch that causes very high positive or negative gradients for the weight $w_{i,j}$ have more impact on the overall gradient than images with low absolute values**. This can be bad especially in the case of outlier images that are not representative to explain the pattern in the data. Consequently our model may try to learn from exotics. In addition we have to be very **careful with small batches as its target distribution might not reflect the overall pattern**. Imagine we would try to distinguish dogs from cats. With a batch size of 10 we are likely to fill up these places with imbalanced targets. For example it could be occupied with cats only. This would yield gradients that try to improve the detection of cats thereby changing the weights we might need to identify dogs. Hence beside image outliers the target distribution itself influences the learning as well. This can cause jiggles as well. One step we try to improve nucleoplasmn and the next perhaps cytosol but with a downgrade of the nucleoplasmn predictions and the next steps it could be the other way round. ","9ee2d618":"### Our goal\n\n* Predict various protein structures in cellular images\n* there are 28 different target proteins\n* multiple proteins can be present in one image (multilabel classification)\n* 27 different cell types of highly different morphology","5bf380d3":"Let's create an instance of this preprocessor and pass it to the data generator.","6a4882bf":"### Track losses and scores\n\nOk, after adding the metrics we like to observe we should try to obtain more insights into the learning process of our model. One question on my mind draws circles: **What happened to the loss after each batch during one epoch?** Does it converge? Has our model started to learn or does nothing happen? Currently we obtain a history after calling fit_generator, but this history only contains the loss of train and validation data after one epoch. It does not contain losses that are obtained after computing each batch. But wait a minute... **How does our model update gradients? After each batch? After one epoch?** \n\nIf we take a look at simple feedforward networks that are close related to CNNs add gradient descent, we can see that learning means to compute the derivatives of the loss with respect to the weights over all samples:\n\n$$ w_{i,j}^{new} = w_{i,j}^{old} - \\eta \\cdot \\partial_{w_{i,j}} E$$\n\nWith a set of independent observation samples, we can obtain the gradients this way:\n\n$$ \\partial_{w_{i,j}} E = \\sum_{n=1}^{N} \\partial_{w_{i,j}} E_{n}$$\n","afb8ba07":"### Nuclear speckles","f206c68e":"Let's check if the number of files divided by 4 yields the number of target samples:","3e4a9426":"### Shared Parameter class","88610c05":"### Peroxisomes"}}