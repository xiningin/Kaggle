{"cell_type":{"38c3a5eb":"code","5a8571b2":"code","9bd470ca":"code","fa98fa67":"code","1de1b468":"code","be515896":"code","ced888ff":"code","e1a3faaa":"code","4d583378":"code","d194ff8c":"code","95afcf0e":"code","053b08dc":"code","ae201cba":"code","9cbe8500":"code","56ad375a":"code","7a4e9b6a":"code","a18f3dec":"code","516763c5":"code","d3b17b54":"code","76537a3b":"markdown","ed16b765":"markdown","ba1199f0":"markdown","d396037a":"markdown","71e60a06":"markdown","59dc5748":"markdown","d1eb33ef":"markdown","62026a9e":"markdown"},"source":{"38c3a5eb":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2, matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.applications import EfficientNetB0\nprint('TF',tf.__version__)\n#Text Color\nfrom termcolor import colored\nfrom wordcloud import WordCloud, STOPWORDS","5a8571b2":"# RESTRICT TENSORFLOW TO 12GB OF GPU RAM\n# SO THAT WE HAVE GPU RAM FOR RAPIDS CUML KNN\nLIMIT = 1\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    tf.config.experimental.set_virtual_device_configuration(\n        gpus[0],\n        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    print(e)\nprint('Restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\nprint('so RAPIDS can use %iGB GPU RAM'%(16-LIMIT))","9bd470ca":"def getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target, row[col]) )\n        return 2*n \/ (len(row.target) + len(row[col]))\n    return f1score","fa98fa67":"COMPUTE_CV = True\ntrain = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\ntest = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\nif len(test)>3: COMPUTE_CV = False\nelse: print('this submission notebook will compute CV score but commit notebook will not')","1de1b468":"import cudf\n\nif COMPUTE_CV:\n    test     = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\n    new_test = [test, test[:1500]]\n    test_pd  = pd.concat(new_test)\n    test     = test_pd\n    test_gf  = cudf.DataFrame(test)\n    print('Test shape is', test_gf.shape)\n    print('Using train as test to compute CV (since commit notebook). Shape is', test_gf.shape)\nelse:\n    test    = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\n    test_gf = cudf.DataFrame(test)\n    print('Test shape is', test_gf.shape)\ntest_gf.head()","be515896":"class DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, df, img_size=256, batch_size=32, path=''): \n        self.df = df\n        self.img_size = img_size\n        self.batch_size = batch_size\n        self.path = path\n        self.indexes = np.arange( len(self.df) )\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        ct = len(self.df) \/\/ self.batch_size\n        ct += int(( (len(self.df)) % self.batch_size)!=0)\n        return ct\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X = self.__data_generation(indexes)\n        return X\n            \n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' \n        X = np.zeros((len(indexes),self.img_size,self.img_size,3),dtype='float32')\n        df = self.df.iloc[indexes]\n        for i,(index,row) in enumerate(df.iterrows()):\n            img = cv2.imread(self.path+row.image)\n            X[i,] = cv2.resize(img,(self.img_size,self.img_size)) #\/128.0 - 1.0\n        return X","ced888ff":"from tensorflow.keras.applications import EfficientNetB0, EfficientNetB3, EfficientNetB5,EfficientNetB7\nimport gc\nTRAIN_IMGS = '..\/input\/shopee-product-matching\/train_images\/'\nTEST_IMGS  = '..\/input\/shopee-product-matching\/test_images\/'\n\nMODEL = EfficientNetB0\nif COMPUTE_CV:\n    BASE = TRAIN_IMGS\nelse:\n    BASE = TEST_IMGS\n    \nif MODEL == EfficientNetB0:\n    WGT = '..\/input\/tfkerasefficientnetimagenetnotop\/efficientnetb0_notop.h5'\n    model = EfficientNetB0(weights=WGT, include_top=False, pooling='avg', input_shape=None)\nelif MODEL == EfficientNetB3:\n    WGT = '..\/input\/tfkerasefficientnetimagenetnotop\/efficientnetb3_notop.h5'\n    model = EfficientNetB3(weights=WGT, include_top=False, pooling='avg', input_shape=None)\nelif MODEL == EfficientNetB5:\n    WGT = '..\/input\/tfkerasefficientnetimagenetnotop\/efficientnetb5_notop.h5'\n    model = EfficientNetB5(weights=WGT, include_top=False, pooling='avg', input_shape=None)\nelif MODEL == EfficientNetB7:\n    WGT = '..\/input\/tfkerasefficientnetimagenetnotop\/efficientnetb7_notop.h5'\n    model = EfficientNetB7(weights=WGT, include_top=False, pooling='avg', input_shape=None)\n       ","e1a3faaa":"embeds = []\nCHUNK = 1024 * 4\n\nprint('Computing image embeddings...')\nCTS = len(test) \/\/ CHUNK\n\nif len(test) % CHUNK != 0: CTS += 1\nprint(CTS)\nfor i, j in enumerate(range(CTS)):\n    \n    a = j * CHUNK\n    b = (j+1) * CHUNK\n    b = min(b, len(test))\n    print('chunk', a, 'to', b)\n    \n    test_gen = DataGenerator(test.iloc[a:b], batch_size=32, path=BASE)\n    image_embeddings = model.predict(test_gen, verbose=1, use_multiprocessing=True, workers=4)\n    embeds.append(image_embeddings)","4d583378":"del model\n_ = gc.collect()\nimage_embeddings = np.concatenate(embeds)\nprint('image embeddings shape',image_embeddings.shape)\n","d194ff8c":"from cuml.neighbors import NearestNeighbors\n\nKNN           = 50 #50\nif len(test) == 3: KNN = 2\nmodel = NearestNeighbors(n_neighbors=KNN)\nmodel.fit(image_embeddings)","95afcf0e":"preds = []\nCHUNK = 1024*4\n\nprint('Finding similar images...')\nCTS = len(image_embeddings) \/\/ CHUNK\nif len(image_embeddings) % CHUNK != 0: CTS += 1\nfor j in range(CTS):\n    \n    a = j * CHUNK\n    b = (j+1) * CHUNK\n    b = min(b, len(image_embeddings))\n    print('chunk', a, 'to', b)\n    distances, indices = model.kneighbors(image_embeddings[a:b, ])\n    for k in range(b-a):\n        IDX = np.where(distances[k, ] < 6.0)[0] # 6.0, 3.6\n        IDS = indices[k, IDX]\n        o = test.iloc[IDS].posting_id.values\n        preds.append(o)\n        \ndel model, distances, indices, image_embeddings, embeds\n_ = gc.collect()","053b08dc":"test['preds2'] = preds\ntest.head()","ae201cba":"from cuml.feature_extraction.text import TfidfVectorizer\n\nprint('Computing text embeddings...')\nmodel = TfidfVectorizer(stop_words='english', binary=True, max_features=25_000)\ntext_embeddings = model.fit_transform(test_gf.title).toarray()\nprint('text embeddings shape',text_embeddings.shape)","9cbe8500":"import cupy\npreds = []\nCHUNK = 1024*4\n\nprint('Finding similar titles...')\nCTS = len(test)\/\/CHUNK\nif len(test)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(test))\n    print('chunk',a,'to',b)\n    \n    # COSINE SIMILARITY DISTANCE\n    cts = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n    \n    for k in range(b-a):\n        \n        IDX = cupy.where(cts[k,]>0.7)[0]\n        o = test.iloc[cupy.asnumpy(IDX)].posting_id.values\n        preds.append(o)\n       \n        \ndel model, text_embeddings\n_ = gc.collect()","56ad375a":"test['preds'] = preds\ntest.head()","7a4e9b6a":"tmp = test.groupby('image_phash').posting_id.agg('unique').to_dict()\ntest['preds3'] = test.image_phash.map(tmp)\ntest.head()","a18f3dec":"def combine_for_sub(row):\n    x = np.concatenate([row.preds, row.preds2, row.preds3])\n    return ' '.join( np.unique(x) )\n\ndef combine_for_cv(row):\n    x = np.concatenate([row.preds, row.preds2, row.preds3])\n    return np.unique(x)","516763c5":"if COMPUTE_CV:\n    tmp = test.groupby('label_group').posting_id.agg('unique').to_dict()\n    test['target'] = test.label_group.map(tmp)\n    test['oof'] = test.apply(combine_for_cv,axis=1)\n    test['f1'] = test.apply(getMetric('oof'),axis=1)\n    print('CV Score =', test.f1.mean() )\n    print(\"CV for image :\", round(test.apply(getMetric('preds2'),axis=1).mean(), 3))\n    print(\"CV for text  :\", round(test.apply(getMetric('preds'),axis=1).mean(), 3))\n    print(\"CV for phash :\", round(test.apply(getMetric('preds3'),axis=1).mean(), 3))\n\ntest['matches'] = test.apply(combine_for_sub,axis=1)","d3b17b54":"test[['posting_id','matches']].to_csv('submission.csv',index=False)\nsub = pd.read_csv('submission.csv')\nsub.head()","76537a3b":"### Compute CV Score","ed16b765":"#### Retrieval method1: use KNN","ba1199f0":"### Use  Phash Feature","d396037a":"## submission","71e60a06":"## Compute RAPIDS Model CV and Infer Submission","59dc5748":"# Import lib","d1eb33ef":"### Use Text Embeddings","62026a9e":"### Use Image Embeddings"}}