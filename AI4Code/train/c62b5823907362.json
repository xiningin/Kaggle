{"cell_type":{"43a4c568":"code","7d50bb9c":"code","bd6f138a":"code","7fa65e91":"code","624bee8d":"code","5c8f0624":"code","5e9c8f82":"code","fac5cb38":"code","740c8b9d":"code","33a7a7c3":"code","568451ff":"code","11792770":"code","6dcbaaf0":"code","91e836f2":"code","28773aa3":"code","c6d4c8a7":"code","7e952859":"code","efb347c4":"code","3ebc4843":"code","afc97ceb":"code","7c2db313":"code","818f94e5":"code","3c07d4cb":"code","f8c0784f":"markdown"},"source":{"43a4c568":"import pickle\nimport pandas as pd\nhpa_df = pd.read_pickle(\"..\/input\/partial-dataframes-with-paths-for-training\/dataset_partial_224\")","7d50bb9c":"hpa_df","bd6f138a":"LABELS= {\n0: \"Nucleoplasm\",\n1: \"Nuclear membrane\",\n2: \"Nucleoli\",\n3: \"Nucleoli fibrillar center\",\n4: \"Nuclear speckles\",\n5: \"Nuclear bodies\",\n6: \"Endoplasmic reticulum\",\n7: \"Golgi apparatus\",\n8: \"Intermediate filaments\",\n9: \"Actin filaments\",\n10: \"Microtubules\",\n11: \"Mitotic spindle\",\n12: \"Centrosome\",\n13: \"Plasma membrane\",\n14: \"Mitochondria\",\n15: \"Aggresome\",\n16: \"Cytosol\",\n17: \"Vesicles and punctate cytosolic patterns\",\n18: \"Negative\"\n}","7fa65e91":"import tensorflow as tf\nprint(tf.__version__)\n\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nimport tensorflow_addons as tfa\n\nimport os\nimport re\nimport cv2\nimport glob\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom functools import partial\nimport matplotlib.pyplot as plt","624bee8d":"IMG_WIDTH = 224\nIMG_HEIGHT = 224\nBATCH_SIZE = 32\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE","5c8f0624":"#%%script echo skipping\n#set an amount of folds to split dataframe into --> k-fold cross validation\n# explanation: https:\/\/towardsdatascience.com\/cross-validation-explained-evaluating-estimator-performance-e51e5430ff85\nN_FOLDS=5\n#choose which one of the 5 folds will be used as validation set this time\ni_VAL_FOLD=1\nhpa_df=np.array_split(hpa_df, N_FOLDS+1) #add one extra part for testing set\ndf_test_split=hpa_df[-1]\nhpa_df=hpa_df[:-1]\ni_training = [i for i in range(N_FOLDS)]\ni_training.pop(i_VAL_FOLD-1)\ni_validation=i_VAL_FOLD-1\ndf_train_split=list()\nfor i in i_training:\n    df_train_split.append(hpa_df[i])\ndf_train_split=pd.concat(df_train_split)\ndf_val_split=hpa_df[i_validation]","5e9c8f82":"print(len(df_train_split))\nprint(len(df_val_split))\nprint(len(df_test_split))","fac5cb38":"#%%script echo skipping\n#analyze class imbalance and set up class weights here\n#https:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/improve-class-imbalance-class-weights\/\ny_train=df_train_split[\"Label\"].apply(lambda x:list(map(int, x.split(\"|\"))))\ny_train=y_train.values\ny_train=np.concatenate(y_train)\nfrom sklearn.utils import class_weight\nclass_weights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(y_train),\n                                                 y_train)","740c8b9d":"#%%script echo skipping\ntmp_dict={}\nfor i in range(len(LABELS)):\n    tmp_dict[i]=class_weights[i]\nclass_weights=tmp_dict\nclass_weights","33a7a7c3":"#%%script echo skipping\n@tf.function\ndef multiple_one_hot(cat_tensor, depth_list):\n    \"\"\"Creates one-hot-encodings for multiple categorical attributes and\n    concatenates the resulting encodings\n\n    Args:\n        cat_tensor (tf.Tensor): tensor with mutiple columns containing categorical features\n        depth_list (list): list of the no. of values (depth) for each categorical\n\n    Returns:\n        one_hot_enc_tensor (tf.Tensor): concatenated one-hot-encodings of cat_tensor\n    \"\"\"\n    one_hot_enc_tensor = tf.one_hot(cat_int_tensor[:,0], depth_list[0], axis=1)\n    for col in range(1, len(depth_list)):\n        add = tf.one_hot(cat_int_tensor[:,col], depth_list[col], axis=1)\n        one_hot_enc_tensor = tf.concat([one_hot_enc_tensor, add], axis=1)\n    return one_hot_enc_tensor\n\n@tf.function\ndef load_image(df_dict):\n    # Load image\n    rgb = tf.io.read_file(df_dict['path'])\n    image = tf.image.decode_png(rgb, channels=3)\n    #https:\/\/medium.com\/@kyawsawhtoon\/a-tutorial-to-histogram-equalization-497600f270e2\n    image=tf.image.per_image_standardization(image)\n    \n    # Parse label\n    label = tf.strings.split(df_dict['Label'], sep='|')\n    label = tf.strings.to_number(label, out_type=tf.int32)\n    label = tf.reduce_sum(tf.one_hot(indices=label, depth=19), axis=0)\n    \n    return image, label","568451ff":"#%%script echo skipping\ntrain_ds = tf.data.Dataset.from_tensor_slices(dict(df_train_split))\nval_ds = tf.data.Dataset.from_tensor_slices(dict(df_val_split))\n\n# Training Dataset\ntrain_ds = (\n    train_ds\n    .shuffle(1024)\n    .map(load_image, num_parallel_calls=AUTOTUNE)\n    .batch(BATCH_SIZE)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\n# Validation Dataset\nval_ds = (\n    val_ds\n    .shuffle(1024)\n    .map(load_image, num_parallel_calls=AUTOTUNE)\n    .batch(BATCH_SIZE)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)","11792770":"#%%script echo skipping\ndef get_label_name(labels):\n    l = np.where(labels == 1.)[0]\n    label_names = []\n    for label in l:\n        label_names.append(LABELS[label])\n        \n    return '-'.join(str(label_name) for label_name in label_names)\n\ndef show_batch(image_batch, label_batch):\n  plt.figure(figsize=(20,20))\n  for n in range(10):\n      ax = plt.subplot(5,5,n+1)\n      plt.imshow(image_batch[n])\n      plt.title(get_label_name(label_batch[n].numpy()))\n      plt.axis('off')","6dcbaaf0":"#%%script echo skipping\n# Training batch\nimage_batch, label_batch = next(iter(train_ds))\nshow_batch(image_batch, label_batch)\n#print(label_batch)","91e836f2":"#%%script echo skipping\n\ndef get_model():\n    base_model = tf.keras.applications.EfficientNetB0(include_top=False, weights='imagenet')\n    base_model.trainable = True\n\n    inputs = Input((IMG_HEIGHT, IMG_WIDTH, 3))\n    x = base_model(inputs, training=True)\n    x = GlobalAveragePooling2D()(x)\n    x = Dropout(0.3)(x)\n    outputs = Dense(len(LABELS), activation='sigmoid')(x)\n    \n    return Model(inputs, outputs)\n\ntf.keras.backend.clear_session()\nmodel = get_model()\nmodel.summary()","28773aa3":"#%%script echo skipping\ntime_stopping_callback = tfa.callbacks.TimeStopping(seconds=int(round(60*60*8)), verbose=1) #8h\n\nearlystopper = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=10, verbose=0, mode='min',\n    restore_best_weights=True\n)","c6d4c8a7":"#%%script echo skipping\n#set up checkpoint save\n#source:https:\/\/www.tensorflow.org\/tutorials\/keras\/save_and_load\n!pip install -q pyyaml h5py\nimport os\ncheckpoint_path = \".\/cp.ckpt\"\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\n# Create a callback that saves the model's weights\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                 save_weights_only=True,\n                                                 verbose=1)","7e952859":"#%%script echo skipping\nimport keras.backend as K\nK_epsilon = K.epsilon()\ndef f1(y_true, y_pred):\n    y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), 0.5), K.floatx())\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + K_epsilon)\n    r = tp \/ (tp + fn + K_epsilon)\n\n    f1 = 2*p*r \/ (p+r+K_epsilon)\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)\ndef f1_loss(y_true, y_pred):\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + K_epsilon)\n    r = tp \/ (tp + fn + K_epsilon)\n\n    f1 = 2*p*r \/ (p+r+K_epsilon)\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return 1-K.mean(f1)","efb347c4":"#%%script echo skipping\nimport tensorflow as tf\nimport timeit\n\ndevice_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","3ebc4843":"#%%script echo skipping\n# Initialize model\ntf.keras.backend.clear_session()\nmodel = get_model()\n\n#model.load_weights(checkpoint_path)\n\n#https:\/\/stackoverflow.com\/questions\/47490834\/how-can-i-print-the-learning-rate-at-each-epoch-with-adam-optimizer-in-keras\nimport keras\ndef get_lr_metric(optimizer):\n    def lr(y_true, y_pred):\n        return optimizer._decayed_lr(tf.float32)\n    return lr\noptimizer = keras.optimizers.Adam()\nlr_metric = get_lr_metric(optimizer)\n\n#https:\/\/keras.io\/api\/optimizers\/learning_rate_schedules\/exponential_decay\/\ninitial_learning_rate = 0.1\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate,\n    decay_steps=1000,\n    decay_rate=0.96,\n    staircase=True)\n\noptimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n\n# Compile model\nmodel.compile(optimizer=optimizer,\n              loss=f1_loss,metrics=['accuracy',f1,tf.keras.metrics.AUC(multi_label=True),'binary_crossentropy',lr_metric])\n\n# Train\nhistory=model.fit(train_ds,\n                  epochs=1000,\n                  validation_data=val_ds,\n                  class_weight=class_weights,\n                  callbacks=[cp_callback,time_stopping_callback])","afc97ceb":"#%%script echo skipping\nhistory.history","7c2db313":"#%%script echo skipping\n#source: https:\/\/machinelearningmastery.com\/display-deep-learning-model-training-history-in-keras\/\n# list all data in history\nprint(history.history.keys())\n\ndef plot_metric(name):\n    val_name='val_'+name\n    plt.plot(history.history[name])\n    plt.plot(history.history[val_name])\n    plt.title(name)\n    plt.ylabel(name)\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.show()\n    \nplot_metric('loss')\nplot_metric('accuracy')\nplot_metric('f1')\nplot_metric('auc')\nplot_metric('binary_crossentropy')\nplot_metric('lr')","818f94e5":"with open('.\/historyhistory.pkl', 'wb') as handle:\n    pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)","3c07d4cb":"with open('.\/history.pkl', 'wb') as handle:\n    pickle.dump(history, handle, protocol=pickle.HIGHEST_PROTOCOL)","f8c0784f":"#### this notebook is part of the documentation on my HPA approach  \n    -> main notebook: https:\/\/www.kaggle.com\/philipjamessullivan\/0-hpa-approach-summary"}}