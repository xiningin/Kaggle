{"cell_type":{"9bf56796":"code","2ca2f2ac":"code","03720d1c":"code","0f55e08c":"code","577951e5":"code","93a0deab":"code","5e27b67e":"code","d76233a9":"code","7ad47944":"code","6d94fe5c":"code","f5ed356e":"code","259aac56":"code","b2e5d5aa":"code","7e9d1d0a":"code","ab3ecb39":"code","838d43a9":"code","4fa90b25":"code","fb7dbcb5":"code","e98e12dc":"code","853ee4d5":"code","5bc140fe":"code","adeabf4e":"code","ca36c0bc":"code","e3483082":"code","3ec7a0ba":"code","e2507cc3":"code","04489182":"markdown","03fa05bc":"markdown","b851d2a6":"markdown","5a3cfdc5":"markdown","13ab9404":"markdown","7520bc21":"markdown","87aa44d0":"markdown","607bb6f5":"markdown","bca7c76e":"markdown","63e2e743":"markdown","5e853c87":"markdown","08d4e1af":"markdown","7797b4c8":"markdown","694922fe":"markdown","6ff3ac57":"markdown","51363d0a":"markdown","85026012":"markdown","c89d8756":"markdown","1a5c4c5c":"markdown","67dd0831":"markdown","c1c3a9e5":"markdown","c915b89b":"markdown","4b510303":"markdown","7f7c1529":"markdown","aba887d1":"markdown","177c0caf":"markdown","b7c0de46":"markdown","340e3f0e":"markdown","bf307cea":"markdown","438ac63a":"markdown","81be5f33":"markdown","7b143932":"markdown","d5a208ab":"markdown","5ab33e5b":"markdown","5c3cd248":"markdown","d7faccb7":"markdown","08d0f1c0":"markdown","eb8e29b1":"markdown","84703172":"markdown","f9fc8f1a":"markdown","0265e69a":"markdown"},"source":{"9bf56796":"%%javascript\n$.getScript('https:\/\/kmahelona.github.io\/ipython_notebook_goodies\/ipython_notebook_toc.js')","2ca2f2ac":"import pathlib\nimport sys\n\nimport pandas\n\n%matplotlib inline\n\n\n# import package\nROOT_DIR = '..\/input\/kaggle-cpe\/cpe\/cpe\/'\nROOT_DIR = str(pathlib.Path(ROOT_DIR).resolve())\nsys.path.append(ROOT_DIR)\n\nfrom cpe_help import Department, util\n\n\ndept = Department.sample()","03720d1c":"path = dept.output_dir \/ 'acs' \/ 'city.geojson'\ndf = util.io.load_geojson(path)\ndf","0f55e08c":"df.plot();","577951e5":"path = dept.output_dir \/ 'acs' \/ 'census_tracts.geojson'\ndf = util.io.load_geojson(path)\ndf.head()","93a0deab":"df.plot();","5e27b67e":"path = dept.output_dir \/ 'acs' \/ 'block_groups.geojson'\ndf = util.io.load_geojson(path)\ndf.head()","d76233a9":"df.plot();","7ad47944":"path = dept.output_dir \/ 'acs' \/ 'police_precincts.geojson'\ndf = util.io.load_geojson(path)\ndf.head()","6d94fe5c":"df.plot();","f5ed356e":"dept = Department('37-00027')","259aac56":"df = dept.files['uof'].load_raw()\ndf.head()","b2e5d5aa":"df = dept.files['uof'].load_processed()\ndf.head()","7e9d1d0a":"df = df[df['LOCATION_GEOCODED'].astype(bool)]\ndf.plot(markersize=5);","ab3ecb39":"from cpe_help import Department\n\ndept = Department.sample()\ndept","838d43a9":"dept.full_name","4fa90b25":"df = dept.load_police_precincts()\ndf.head()","fb7dbcb5":"df.plot();","e98e12dc":"for file in util.file.list_files(dept.output_dir):\n    print(file.relative_to(util.path.BASE_DIR))","853ee4d5":"from cpe_help import Department\n\nclass Department3700027(Department):\n    # code for the department goes here\n    # ...\n    pass","5bc140fe":"from cpe_help import Department, util\n\nclass Department3700027(Department):\n\n    # NAD 1983 StatePlane Texas Central FIPS 4203 Feet\n    CRS = util.crs.from_esri(102739)\n\n    def load_external_shapefile(self):\n        # set up CRS when loading police boundaries\n        df = super().load_external_shapefile()\n        df.crs = self.CRS\n        return df","adeabf4e":"dept = Department('11-00091')\ndept","ca36c0bc":"dept = Department('37-00027')\ndept","e3483082":"import pandas\n\nfrom cpe_help import DepartmentFile, util\n\n\n# real UOF example for department 37-00027 (Austin, Texas)\n\nclass UOF(DepartmentFile):\n\n    def __init__(self, department):\n        self.department = department\n\n    @property\n    def raw_path(self):\n        directory = self.department.tabular_input_dir\n        return directory \/ '37-00027_UOF-P_2014-2016_prepped.csv'\n\n    @property\n    def processed_path(self):\n        directory = self.department.other_output_dir\n        return directory \/ 'uof.geojson'\n\n    def load_raw(self):\n        return pandas.read_csv(\n            self.raw_path,\n            low_memory=False,\n            skiprows=[1],\n        )\n\n    def load_processed(self):\n        return util.io.load_geojson(self.processed_path)\n\n    def process(self):\n        # loads raw file, processes it and saves output to self.processed_path\n        pass","3ec7a0ba":"import collections\n\n\nclass Department3700027(Department):\n\n    @property\n    def files(self):\n        return collections.OrderedDict([\n            ('uof', UOF(self)),\n        ])","e2507cc3":"dept = Department('37-00027')\ndf = dept.files['uof'].load_processed()\ndf.head()","04489182":"# How does it work?\n\nIn this section, I will get a little more technical and show the thing works.\n\nI will also try to provide some examples of how the pieces are assembled together.","03fa05bc":"# CPE helper\n\nHello folks! This is the main kernel and where I will present my answer. I've been working a lot the last two months to provide an answer that would be really good for CPE and the things they do.","b851d2a6":"3. Override\/create any of the methods you want\n\n   For example, if you want to set a specific CRS when loading the input shapefile, you can override the `load_external_shapefile` method:","5a3cfdc5":"### UOF files\n\nI didn't have enough time to actually deal with the preprocessing of the provided UOF files (and other files also, like OIS, vehicle stops, etc). This is something that I would visit with much care, thinking, how much standardization can we do before we lose the original shape of the data.\n\n### Geocoding\n\nThis is linked with the above. Lots of department files had spatial data encoded in the form of an address. They can be transformed into geographic coordinates by [geocoding](https:\/\/en.wikipedia.org\/wiki\/Geocoding), but this was left out of scope for my solution.\n\nAs you have already worked with Google, you may try to cooperate with them on this, since Google has one of the best geocoding services out there. You may also roll your own solution.\n\n### Areal interpolation\n\nTo transform the statistics retrieved from the Census API into statistics for police departments, one would need to use [areal interpolation][1].\n\nThe method used here is *weighted* areal interpolation. It is one of the simplest methods available, so, it may not come with the biggest accuracy.\n\nThere has been lots of research on this area... Lam provides a [review][2] of the areal interpolation methods that were available by 1983.\n\nThe method that I would use if resources were available, is dasymetric mapping. It makes use of ancillary variables to improve the accuracy of predictions over the target regions.\n\nMennis (2015) provides an [excellent example][3] of using zoning data to increase the accuracy of urban population analysis.\n\nOne problem with the areal interpolation thing is that there are not enough tools available, and the ones that are available are coupled into ArcGIS. There is, however python package available to interact with ArcGIS ([arcpy][4]). So, maybe that's the way to go.\n\n\n[1]: http:\/\/desktop.arcgis.com\/en\/arcmap\/latest\/extensions\/geostatistical-analyst\/what-is-areal-interpolation.htm\n[2]: https:\/\/pdfs.semanticscholar.org\/1d0e\/c81b45f4cef124d9369cc8d8f4883f6a8c22.pdf\n[3]: https:\/\/www.huduser.gov\/portal\/periodicals\/cityscpe\/vol17num1\/ch9.pdf\n[4]: http:\/\/desktop.arcgis.com\/en\/arcmap\/latest\/analyze\/arcpy\/what-is-arcpy-.htm\n\n### Margin of Error\n\nAnother limitation of the machine is the lack of margin of error for predicted variables. I believe this margin of error could be calculated using [Variance Replicate Tables](https:\/\/www.census.gov\/programs-surveys\/acs\/data\/variance-tables.html), but, to be frank, my knowledge in the area is not enough. Also, there's the issue of integrating this with areal interpolation (that may generate an error of its own kind) and the large amount of complexity that would need to be added into the codebase.\n\n### Year\n\nI don't believe this is a serious impedance, but, it's more of something to be considered. Currently, you can only specify one year in the configuration file and then:\n\n- TIGER will retrieve boundaries for that year\n- ACS will retrieve 5-years estimates ending in that year","13ab9404":"# How to use it?","7520bc21":"## Python package\n\nThe answer in itself is a python package. So, if you are using python, you can access department methods directly.","87aa44d0":"# Last considerations","607bb6f5":"## Output directory\n\nIf you want to retrieve the outputs manually, they are stored in a specific directory for each department.\n\nFor example:","bca7c76e":"## Individual department files\n\nIndividual department files can also be processed. For example, the use of force file for the Austin Police Department had coordinate variables in a non-usual CRS (coordinate reference system). The processing step transformed those coordinates into the latitude and longitude values.","63e2e743":"## Limitations","5e853c87":"### Block group level\n\n<https:\/\/en.wikipedia.org\/wiki\/Census_block_group>","08d4e1af":"This represents a specific department file for a specific department. We can plug it in the department simply by modifying its `files` attribute of the department. Iike so:","7797b4c8":"## Configuration\n\nIt is also possible to configure some things.\n\nMost of the configuration is present in the `cpe_help.conf` file. It is in a simple format that resembles windows INI files.\n\nSome parameters:\n\n\n### Census year\n\nThis is the year Census data will be retrieved from. For example, if it is configured to 2015, the machine will retrieve ACS 5-year estimates from 2011 to 2015 and TIGER shapefiles from 2015. As I'm writing, there's data available up to 2017 ([release](https:\/\/www.census.gov\/programs-surveys\/acs\/news\/data-releases\/2017\/release.html)).\n  \n*Do not change the year without checking ACS variables first*. The name of the variable on the ACS endpoint [may vary from year to year](https:\/\/www.census.gov\/programs-surveys\/acs\/technical-documentation\/table-and-geography-changes.html). This can lead the machine to download erroneous values.\n\n### Census key\n\nThis is the key that will be used in the requests to the Census API. Make sure to **[request a new key](https:\/\/api.census.gov\/data\/key_signup.html)** and set it up in the configuration file.\n\n### ACS Variables\n\nRepresents variables that will be requested from the Census API (ACS 5-year esimates).\n  \nThe values to left of ` = ` correspond to the variable names in the Census Data API. The values to the right correspond to how we want to save this variable locally.\n  \nYou can look at this [little guide](https:\/\/www.kaggle.com\/center-for-policing-equity\/data-science-for-good\/discussion\/70489) if you think of adding new variables.\n  \nAlso, make sure that the variables are available at the block group level (some aren't, even if they are part of a detailed table).\n\n### Date format\n\nSets the standard date and time format for output ([reference](http:\/\/strftime.org\/)).\n\n### User-Agent\n\nDetermines the [User-Agent string](https:\/\/developer.mozilla.org\/en-US\/docs\/Web\/HTTP\/Headers\/User-Agent) sent alongside requests. It should contain identification and possibly a way to contact CPE.\n\nSome pointers on choosing the User-Agent string: <https:\/\/webmasters.stackexchange.com\/a\/6305>","694922fe":"Original file:","6ff3ac57":"## Main Classes","51363d0a":"### Create the conda environment\n\nTo run the project, you will need a custom conda environment. You can create one by:\n\n1. Start the Anaconda prompt (Windows) or terminal (Linux\/MacOS)\n2. Move into the project root directory (`cd` into it)\n3. Update conda using the following command\n\n   ```\n   conda update conda\n   ```\n\n4. Create the environment\n\n   ```\n   conda env create -f environment.yml\n   ```\n\nThe last step may take a while (don't worry if it looks stuck at the `Solving...` phase, it is just calculating things). Once it finishes, take a look at the final lines of output and, it's all done!","85026012":"### Areal interpolation\n\nAreal interpolation is the name of the method for using information from a set of polygons into a set of another polygons. This is what Chris provided in an initial example.\n\nThe type of areal interpolation used here is *weighted areal interpolation*. There's more information about it and other methods in the [Limitations](#Limitations) section.","c89d8756":"This will return the generic class department. However, if a specific class has been added for the department in question, it will be instantiated instead:","1a5c4c5c":"DepartmentFile's will be automatically retrieved and processed when you run the pipeline. Also, you can access their contents by calling one of their methods:","67dd0831":"### Census tract level","c1c3a9e5":"# Demonstration\n\n*results here came from a local run (not on the Kaggle kernel)*","c915b89b":"### Department\n\nIn the hope of allowing ease customization, I took an object-oriented approach, setting a department as a class. Each class has methods for load, saving and processing files of the department.\n\nIf we want to load department 11-00091 class, just call the Department constructor:","4b510303":"### Clone the repository\n\nThe project itself is being hosted at github. If you have git, you can clone it by running the command:\n\n```bash\ngit clone https:\/\/github.com\/araraonline\/kag-cpe\n```\n\nIf you do not have git, you can download the repository directly from the project page:\n\n1. Go to <https:\/\/github.com\/araraonline\/kag-cpe>\n2. Click \"Clone or download\" and then \"\"Download ZIP\"\n3. Once you download the zip file, extract it to a directory of your choice","7f7c1529":"# Summary\n\nTL;DR: I've created a file processor that has two main functions: generate population statistics for each police precinct and assist with the standardization of the police data.\n\n*Take a look at the code if possible. I believe it is well documented and one of the best aspects of the solution:*\n\n- [example function (areal interpolation)](https:\/\/github.com\/araraonline\/kag-cpe\/blob\/master\/cpe_help\/util\/interpolation.py)\n- [project on github](https:\/\/github.com\/araraonline\/kag-cpe)\n\n![](http:\/\/policingequity.org\/wp-content\/themes\/bkt\/images\/center-for-policing-equity.png)\n\nCenter for Policing Equity (CPE) is a research center that is focused on justice and racial equity. To put in a another light, they produce important research that helps law enforcement agencies and communities to forge a way towards mutual trust and public safety.\n\nNow, CPE faces a sort of trouble, and that is the huge amount and huge variety in the data that they receive. Basically, they have information on:\n\n- Location of police precincts\n- Use of force incidents\n- Officer-involved shootings\n- Vehicle stops\n- Etc\n\nAnother issue is that they need to aggregate data found at the police level and data found at the census level. I believe this was the main thing that was asked in this challenge and it is where my answer comes in...\n\nI came up with a tool that automates a big part of the process. First, it retrieves demographic characteristics that are to be used in the analysis. Second, it makes those characteristics available at the police precinct level.\n\nThese are the main functions. But also, there is a full python package that gives support to it, providing a nice framework for CPE, be it if they decide to automate more of the process or if they just want a quick set of tools to assist with the daily work.","aba887d1":"## Running\n\nThis is the easiest step, but also takes a long time on the first run.\n\nYou must first activate your conda environment:\n\n- Windows: `activate cpe-kaggle`\n- Linux\/MacOS: `source activate cpe-kaggle`\n\nThen, make sure you are in the package main directory and run the preparation script (it will create the necessary directories for departments\/etc):\n\n```\ndoit -f prepare_kaggle.py\n```\n\nAfter that, you can start the script for the main pipeline:\n\n```\ndoit\n```\n\n*(when no file is specified, the default file executed is the dodo.py)*\n\nAfter this, you will see a long list of tasks being executed. (message me if you've got any errors). Their names are pretty indicative. You can see what they are actually doing by reading the dodo.py source code or get a simple description by running the command `doit list`.\n\nTake note that some tasks will take a long time to run. This is due to them being CPU-intensive or downloading a big amount of data.","177c0caf":"## ACS Data","b7c0de46":"### Install conda\n\nConda is a package and environment manager that is used to install the dependencies for the project. You can retrieve it by installing Anaconda or Miniconda. Miniconda is a lightweight version of Anaconda, I usually opt for it!\n\n- [Download Anaconda](https:\/\/www.anaconda.com\/download\/)\n- [Download Miniconda](https:\/\/conda.io\/miniconda.html)","340e3f0e":"### Census API\n\nOne of the requests of this challenge involved using data from the ACS to generate statistics for each police precincts. I noticed that this data was being retrieved manually, and at census tract level. So, I went one step ahead and created an automation tools that would retrieve the variables you want and interpolate those values into the police precinct areas.\n\nThe variables themselves can be changed using the configuration files and I think this shall be a very useful tool for CPE.","bf307cea":"### TIGER shapefiles\n\nWe are initially provided boundaries for each police district in a department. With these precincts, we can localize where the department is (city\/state\/county).\n\nThis localization is essential because we want to retrieve demographic values only for the needed areas.\n\nTo do so, we use a Census service called TIGER (Topologically Integrated Geographic Encoding and Referencing).\n\nYou can read more about TIGER [here](https:\/\/www.census.gov\/geo\/maps-data\/data\/tiger.html).","438ac63a":"### DepartmentFile\n\nDepartments may have some files that are specific to them. Each department file must be processed in a specific manner, and you can do so with a DepartmentFile.\n\nFor example, to create a new DepartmentFile:","81be5f33":"### City level","7b143932":"<h1 id=\"tocheading\">Table of Contents<\/h1>\n<div id=\"toc\"><\/div>","d5a208ab":"## Other things\n\nHere are some aspects that are not crucial to the understanding of the program itself, but may be useful if you want to modify something some of the code later on.","5ab33e5b":"Processed file (note that this table has a `geometry` column with points in it and that the `INCIDENT_DATE` was standardized:","5c3cd248":"## Some suggestions\n\n### Call Rscript from python\n\nI've noticed you using R in your work. While this answer is presented as python, it may be actually easy to integrate R into it, for example, if you wanna generate some plots with ggplot.\n\nThe way to do it would be to create a R script that does the function you want, and then call this R script from python. Like so:\n\n```\nimport subprocess\n\nsubprocess.run([\n\t'Rscript',\n\t'foo.R',\n\t'arg1',\n\t'arg2',\n])\n```\n\nYou can also try the [rpy2](https:\/\/rpy2.readthedocs.io\/) package.\n\nRef: [Rscript function | R Documentation](https:\/\/www.rdocumentation.org\/packages\/utils\/versions\/3.5.1\/topics\/Rscript)\n\n### Ask for missing data\n\nWhen there's missing data, like in the shapefile where there were missing points, I think the best thing you can do is ask the police for the missing data. We can try anything in the code, but, anything we try, it won't be better than the actual data.\n\n### Date and time format\n\nThe date and time columns in the data are present in the most varied formats. I recommend using a standard, like [ISO 8601](https:\/\/en.wikipedia.org\/wiki\/ISO_8601).\n\n## A little note about levels of automation\n\nThere's always a doubt of how deep this processing can go. In an ideal place, all departments would give CPE all the data theiy need and this data would come clean and standardized... In a situation like this, it would be pretty simple to generate an script to, say, for each department, generate some analysis and reports...\n\nThe data that is coming from departments, however, is not a sea of roses. It comes in the most varied formats with various levels of detail. This makes it hard to create an automation method that is valid for all files. Essentially, we'd be method for each one of them. In this same light, I believe CPE did an awesome job at coming up with hand-tailored analysis for each department.\n\nBut also, it makes me question where could automation be useful? I believe that, in this case, it could be useful to keep two versions of each file. One version, the raw, would be manually used by CPE to provide customized analysis that can be so very flexible and beautiful. The other version, standardized, could be used to feed an automation pipeline and then generate figures, reports, maps, etc.\n\nThe `DepartmentFile` class was based on this line of thought.\n\n## Trouble?\n\nIf you have any trouble for the installation or running steps, please, leave a comment below! I will try to help the best I can.\n\n## Thanks\n\nI would like to thank everyone involved in this challenge. Two people that were actually special are Kat and Chris. Kat is the representative for CPE and I think she is helping organize this competition. Chris is on the Kaggle side and he's the main organizer here. Both were really helpful whenever needed, and provided crucial information for the challenge! Also, they are really nice :)\n\nAlso sending my regards to Shivam Bansal and Jose Berengueres that are being great companions through the end of this competition.\n\nBesides people here, I want to thank my friend Zin that's helping me with internet issues and Windows issues right now...\n\nNow I gotta go and upload my code!\n\nBye bye!","d7faccb7":"### Police precincts level","08d0f1c0":"This contain exclusive handling that involves the loading of shapefiles and the processing of a UOF file.","eb8e29b1":"## Pipeline\n\nFirst of all, this machine is a pipeline. That is, a series of steps that can be taken in a determined order. To assemble these steps together, I used the [doit](http:\/\/pydoit.org\/) package. This package  is just like Make, but a bit more tasty and with python bindings, easing the job a lot.\n\nSo, doit abstracts pipelines as a series of tasks,... Each task has some dependencies and some targets. For example, our dependencies are the inputs files and our targets are the outputs. There's also a recipe for each step, and, amazingly, each step will only be calculated if it needs to be.","84703172":"## Customizing departments\n\nEach department can have a specific class linked to it (which defaults to the Department class, if not specified). To create custom behavior for a specific department (say, department 37-00027):\n\n1. Create a `department3700027.py` file at the `cpe_help\/departments` directory.\n2. Inside the file, declare a subclass of Department named Department3700027. For example:","f9fc8f1a":"And it's done!\n\n*you can also add department-specific files for precessing, check the [DepartmentFile](#DepartmentFile) section below*","0265e69a":"## Preparation\n\nBefore running the project, you will first need to do some preparation steps..."}}