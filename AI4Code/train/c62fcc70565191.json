{"cell_type":{"35db511d":"code","911d030d":"code","df7c6098":"code","b8bb3852":"code","dd0e3a9d":"code","9beb9951":"code","ceb7b16c":"code","75e858de":"code","16da3e6c":"code","1a6c7a6f":"code","4e10c001":"code","40a6e990":"code","e19f84ca":"code","354fdb09":"code","b4bdcef6":"code","11c29389":"code","0cb5783f":"code","798bc6e9":"code","54720f55":"code","da0e93b2":"code","3cdf0438":"code","3d2b21f9":"code","148328c1":"code","5d7e0230":"code","23d77c16":"code","3360a244":"code","bceb77e9":"code","fe0e0600":"code","58b41a41":"code","a1989508":"code","6633785c":"code","a81a1e73":"code","34c7e4ed":"code","c5e3a232":"code","1e7399ac":"code","69543554":"code","7366b3d2":"code","be0a9e55":"code","55067fc6":"code","74c549ef":"code","85f40173":"markdown","8c31206b":"markdown","d2bc088f":"markdown","ab6e7734":"markdown","7a6dcfce":"markdown","10c4abad":"markdown"},"source":{"35db511d":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#plot \nplt.rcParams['figure.figsize'] = (15,9)\nsns.set_palette('gist_earth')","911d030d":"# Read datasets from csv\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# Merge the 2 dataframes for EDA and feature engineeraing\nfull_dataset = pd.concat([train, test], axis = 0, sort=True)\n\n# Set PassengerId as Index\nfull_dataset.set_index('PassengerId', drop = False, inplace=True)\ntrain = full_dataset[:891]\n\n ","df7c6098":"# Identify Missing Values\nnan = full_dataset.isnull().sum()\nidx_nan = nan.mask(nan==0).dropna().index\n\nsns.heatmap(full_dataset[idx_nan].transpose().isnull(), cmap = 'Greens', cbar = False)\nnan[idx_nan].drop('Survived').sort_values()","b8bb3852":"\nprint(np.count_nonzero(full_dataset['Ticket'].unique()))","dd0e3a9d":"def parse_ticket(str1):\n    \"\"\"\n    Function to parse the Letter part of the Ticket code\n    \"\"\"\n    m = re.search(r'(.*)(\\s\\d|\\s\\d{4,7}$)',str1)\n    s = re.search(r'[A-Z]+',str1)\n    if m: # removing non alphanumeric characters and binding the numbers and letters before the space\n        str2 = m.group(1)\n        n =re.search(r'([A-Z]+)[^A-Z0-9]*([A-Z]+)*[^A-Z0-9]*([A-Z0-9]*)[^A-Z]*([A-Z]*)*',str2)\n        new_str = ''\n        if n:    \n            if n.group(1):\n                new_str+=n.group(1)\n                if n.group(2) or n.group(3):\n                    if n.group(2):\n                        new_str+=n.group(2)\n                    if n.group(3):\n                        new_str+=n.group(3)\n                        if n.group(4):\n                            new_str+=n.group(4)\n                            if n.group(5):\n                                new_str+=m.group(5)\n    elif s:\n        new_str = s.group(0) # Ticket with letters only\n    else:\n        new_str = 'XXX' #Ticket with only numercial values\n    return new_str\n\n\nfull_dataset['Ticket_short'] = full_dataset.Ticket.map(parse_ticket)","9beb9951":"full_dataset['Ticket_short'] ","ceb7b16c":"# Cabin\ndef parse_Cabin (cabin):\n    if type(cabin) == str:\n        m = re.search(r'([A-Z])+', cabin)\n        return m.group(1)\n    else:\n        return 'X'\n        \nfull_dataset['Cabin_short'] = full_dataset['Cabin'].map(parse_Cabin)","75e858de":"# Fare\n# Fare Adjustment\nfare_original = full_dataset['Fare'].copy()\n\ndict_ticket_size = dict(full_dataset.groupby('Ticket').Fare.count())\nticket_size = full_dataset['Ticket'].map(dict_ticket_size)\nfull_dataset['Fare'] = full_dataset.Fare\/ticket_size\n\n\n# Plot Fare Adjustment\nfig, (ax0, ax1) = plt.subplots(2)\nax0.hist(fare_original.dropna(), bins=80 ,color='green');\nax0.set_xlabel('Fare(Original)')\n\nax1.hist(full_dataset['Fare'].dropna(), bins=80 ,color='green');\nax1.set_xlabel('Fare (Adjusted)');","16da3e6c":"# Calculate mean fare cost for each Passenger Class\ndict_fare_by_Pclass = dict(full_dataset.groupby('Pclass').Fare.mean())\n\n# fill value according to Passenger Class\nmissing_fare = full_dataset.loc[full_dataset.Fare.isnull(),'Pclass'].map(dict_fare_by_Pclass)\nfull_dataset.loc[full_dataset.Fare.isnull(),'Fare'] = missing_fare","1a6c7a6f":"# Descriptive Statistics of the full dataset\ndisplay(full_dataset.describe())\nprint(f\"survived: {full_dataset.Survived.mean()*100:.2f}%\")","4e10c001":"# EDA - Distributions\nvar_to_plot = ['Pclass','Sex','SibSp','Parch','Embarked','Survived']\n\n# Plot Categorical Var\nfig, axs = plt.subplots(4,3, figsize=(15,12))\nfor i,key in enumerate(var_to_plot):\n    sns.countplot(key, data=full_dataset, ax=axs[i\/\/3,i%3], palette='Set2')\n     \n# Plot Age\nplt.subplot2grid((4,3),(2,0),rowspan=1,colspan=3);\nsns.distplot(full_dataset.Age.dropna(), bins=range(0,80,2), kde=False,color='darkblue' )\nplt.xlabel('Age');\n\n# Plot Fare\nplt.subplot2grid((4,3),(3,0),rowspan=1,colspan=3);\nsns.distplot(full_dataset.Fare.dropna(), bins=100, kde=False,color='darkblue')\nplt.xlabel('Fare');\nplt.tight_layout()","40a6e990":"# Plot all categorical features with Survival rate\nvar_to_plot = ['Pclass','Sex','SibSp','Parch','Embarked','Cabin_short']\n\nf, axs = plt.subplots(3,5, sharey=True)\ncoord = [(0,0),(0,2),(1,0),(1,2),(2,0),(2,2)]\nfor i,key in enumerate(var_to_plot): # except feature Survived\n    plt.subplot2grid((3,5),(coord[i]),rowspan=1,colspan=2);\n    sns.barplot(data = full_dataset, x= key, y='Survived', color='red');\n    plt.axhline(y=0.3838, color='k', linestyle='--')\n\n# Plot Correlation\ncorr = pd.DataFrame(full_dataset.corr()['Survived'][:-1])\nplt.subplot2grid((3,5),(0,4),rowspan=3,colspan=1);\nsns.heatmap(corr, cmap = \"BrBG\", annot = True, annot_kws = {'fontsize': 12 });\nplt.tight_layout()","e19f84ca":"# Create DataFrame Features to record potential predictors for later model training\nfeatures = pd.DataFrame()\nfeatures['Pclass'] = full_dataset['Pclass']\nfeatures['Fare'] = full_dataset['Fare']\nfeatures['Sex'] = full_dataset['Sex']","354fdb09":"d = dict(full_dataset['Ticket_short'].value_counts())\nticket_count = full_dataset['Ticket_short'].map(d)\n# Show % survived by Ticket\ndisplay(full_dataset.groupby('Ticket_short').Survived.aggregate(['mean','count']).dropna().sort_values('count').transpose())\n# Plot % survived by Ticket, droping those tickets with <10 count\nsns.barplot(data = full_dataset[ticket_count > 10], x = 'Ticket_short', y = 'Survived')\nplt.axhline(y=0.3838, color='k', linestyle='--');","b4bdcef6":"features['A5'] = (full_dataset['Ticket_short'] == 'A5').astype(int)\nfeatures['PC'] = (full_dataset['Ticket_short'] == 'PC').astype(int)","11c29389":"# Plot number of survived passengers by PClass, Sex and Age\nfacet = sns.FacetGrid(full_dataset, row = 'Pclass',col='Sex', hue = 'Survived', aspect=2, palette = 'Set1')\nfacet.map(plt.hist, 'Age', histtype='step', bins = np.arange(0,80,4))\n\nfacet.add_legend();","0cb5783f":"# Create Age Quartiles\nAge_quartile = pd.qcut(full_dataset.Age,10)\n\n# Plot age quartiles by sex with survival rate\nsns.barplot(data = full_dataset, x= Age_quartile, y='Survived', hue = 'Sex');\nplt.axhline(y=0.3838, color='k', linestyle='--')\nplt.xticks(rotation = 30)\nplt.title('Across All Classes');","798bc6e9":"# Parse Titles from Names\ndef parse_title(str):\n    m = re.search(', (\\w+ *\\w*)\\.',str)\n    return m.group(1)\n    \ntitle = full_dataset.Name.map(parse_title)\ntitle.unique()","54720f55":"# Simplify title groups\ndict_Title = {\"Capt\":       \"Officer\",\n                    \"Col\":        \"Officer\",\n                    \"Major\":      \"Officer\",\n                    \"Jonkheer\":   \"Royalty\",\n                    \"Don\":        \"Royalty\",\n                    \"Sir\" :       \"Royalty\",\n                    \"Dr\":         \"Officer\",\n                    \"Rev\":        \"Officer\",\n                    \"the Countess\":\"Royalty\",\n                    \"Dona\":       \"Royalty\",\n                    \"Mme\":        \"Mrs\",\n                    \"Mlle\":       \"Miss\",\n                    \"Ms\":         \"Mrs\",\n                    \"Mr\" :        \"Mr\",\n                    \"Mrs\" :       \"Mrs\",\n                    \"Miss\" :      \"Miss\",\n                    \"Master\" :    \"Master\",\n                    \"Lady\" :      \"Royalty\"\n                    }\n\ntitle = title.map(dict_Title)\n\n# Plot the distribution of Age by Title\nplt.figure(figsize = (14,6))\nsns.violinplot(x = title, y = full_dataset['Age']);","da0e93b2":"# Calculate mean age of each title group\ndf_title = pd.DataFrame(title).join(full_dataset[['Age','Survived']])\ndict_age = df_title.groupby('Name').Age.mean()\n\n# Fill in Age according to passenger's title\nidx = full_dataset.Age.isnull()\nfull_dataset.loc[idx,'Age'] = df_title.loc[idx, 'Name'].map(dict_age)","3cdf0438":"# Plot title with Survived\nsns.barplot(data = df_title, x= 'Name', y='Survived');\nplt.axhline(y=0.3838, color='k', linestyle='--');","3d2b21f9":"# Record useful features in features dataframe\nfeatures['Title'] = df_title['Name']\nfeatures['Child'] = (full_dataset['Age'] <= 14).astype(int)","148328c1":"# function to parse surname of the passengers\ndef parse_surname(name):\n    return name.split(',')[0]\n# Calculate Family Size\nfamily = pd.DataFrame(full_dataset[['Parch','SibSp','Ticket']])\nfamily['Family_size'] = 1 + family.Parch + family.SibSp\n\n# Parse Surname from Name\nfamily['Surname'] = full_dataset.Name.map(parse_surname)\n\n# Surname Code and Surname Size\ndict_scount = dict(family.groupby('Surname').Family_size.count())\ndict_scode = dict(zip(dict_scount.keys(), range(len(dict_scount))))\n\nfamily['Surname_code'] = family['Surname'].map(dict_scode)\nfamily['Surname_count'] = family['Surname'].map(dict_scount)\n\n# Examples with common surname\ndisplay(full_dataset[family.Surname == 'Smith'])","5d7e0230":"def tick2fam_gen(df):\n    # initialize ticket dict\n    dict_tick2fam = {'000000': 0}\n    fam_counter = 0\n        \n    for i in df.index:    \n        keys = list(dict_tick2fam.keys())\n        chk_key = df.loc[i, 'Ticket']\n        for key in keys:\n            if len(chk_key) == len(key): #if their tickets have high similarity\n                if (chk_key[-4:].isdigit()) & (key[-4:].isdigit()): \n                    if (chk_key[:-2] == key[:-2]) & (np.abs(int(chk_key[-2:]) - int(key[-2:])) <= 10):\n                        dict_tick2fam[chk_key] = dict_tick2fam[key]\n                        break\n                    \n            if key == keys[-1]: # no match, assign a new code to the passenger\n                fam_counter += 1\n                dict_tick2fam[chk_key] = str(fam_counter)  \n                \n    return dict_tick2fam","23d77c16":"# Single out Surnames with size > true family size (may have more than 1 family involved)\nsurname2chk = family[family['Family_size'] < family['Surname_count']].Surname.unique() \n# chk_surname2 = family_infer[family['FamilySize'] > family['SurnameSize']].Surname.unique() # unidentified fam\n\n# Regrouping Families according to Family Size and Ticket.\nfamily['Surname_adj'] = family['Surname'] #new column for corrected family_group\n\nfor s in surname2chk:\n    family_regroup = family[family['Surname'] == s] #get family with specific surname\n    fam_code_dict = tick2fam_gen(family_regroup) #pass in df to get family codes within the same surname\n\n    for idx in family_regroup.index: #assign family code 1by1\n        curr_ticket = full_dataset.loc[idx].Ticket\n        fam_code = fam_code_dict[curr_ticket]\n\n        if family_regroup.loc[idx, 'Family_size'] == 1: #for passengers traveling alone\n            #relatives that shares surname and ticket, which Parch and SibSp failed to record\n            if family_regroup.Ticket.value_counts()[curr_ticket] > 1: \n                family.loc[idx, 'Surname_adj'] =  s + '-hidfam' + fam_code\n            #single traveler\n            else: \n                family.loc[idx, 'Surname_adj'] =  s + '-single' + fam_code\n        #different families\n        else: \n            family.loc[idx, 'Surname_adj'] =  s + '-fam' + fam_code\n\ndisplay(family[family.Surname == 'Smith'])","3360a244":"# Assign codes to families\ndict_fcount = dict(family.groupby('Surname_adj').Family_size.count())\ndict_fcode = dict(zip(dict_fcount.keys(), range(len(dict_fcount))))\n\nfamily['Family_code'] = family['Surname_adj'].map(dict_fcode)\nfamily['Family_count'] = family['Surname_adj'].map(dict_fcount)\n\nprint(f\"No. of Family Before Regrouping: {len(family.Surname_code.unique())}\")\nprint(f\"No. of Family After Regrouping: {len(family.Family_code.unique())}\")","bceb77e9":"# Identify Groups (Those holding the same ticket code, could be friends\/family)\ngroup = pd.DataFrame(family[['Surname_code','Surname_count','Family_code','Family_count']])\n\ndict_tcount = dict(full_dataset.groupby('Ticket').PassengerId.count())\ndict_tcode = dict(zip(dict_tcount.keys(),range(len(dict_tcount))))\n\ngroup['Ticket_code'] = full_dataset.Ticket.map(dict_tcode)\ngroup['Ticket_count'] = full_dataset.Ticket.map(dict_tcount)\n\nprint(f\"No. of Tickets Identified: {len(group['Ticket_code'].unique())}\")\ndisplay(full_dataset[(full_dataset.Ticket == 'A\/4 48871') |(full_dataset.Ticket == 'A\/4 48873')])","fe0e0600":"def ChainCombineGroups(df, colA, colB):\n\n    # make a copy of DFs for iteration\n    data = df.copy()\n    search_df = data.copy()\n    \n    group_count = 0\n\n    while not search_df.empty:\n\n        # Initiate pool and Select Reference item\n        pool = search_df.iloc[:1]\n        idx = pool.index\n\n        # Remove 1st item from searching df\n        search_df.drop(index = idx, inplace = True)\n\n        # Initialize Search\n        flag_init = 1\n        update = pd.DataFrame()\n\n        # While loop to exhausively search for commonalities, pool is updated until no more common features are found\n        while (flag_init or not update.empty):\n\n            flag_init = 0\n\n            # target labels to look for\n            pool_A_uniq = np.unique(pool[colA])\n            pool_B_uniq = np.unique(pool[colB])\n\n            for col in [colA,colB]:\n                idx = []\n\n                # get all indexs of items with the same label\n                for num in np.unique(pool[col]):\n                    idx.extend(search_df[search_df[col] == num].index)\n\n                # update pool\n                update = search_df.loc[idx]\n                pool = pd.concat([pool, update], axis = 0)\n\n                # remove item from searching df\n                search_df = search_df.drop(index = idx)\n\n            # assign group num\n            data.loc[pool.index, 'Group_'] = group_count\n\n        group_count += 1\n        \n    return np.array(data['Group_'].astype(int))","58b41a41":"# Assign Final group no.\ngroup['Group_code'] = ChainCombineGroups(group, 'Family_code', 'Ticket_code')\n\n# Calculate group sizes\ndict_gcount = dict(group.groupby('Group_code').Family_code.count())\ngroup['Group_count'] = group.Group_code.map(dict_gcount)\n         \nprint(f\"Family: {len(family['Family_code'].unique())}\")\nprint(f\"Group: {len(group['Ticket_code'].unique())}\")\nprint(f\"Combined: {len(group['Group_code'].unique())}\\n\")\nprint('An example of grouping the both friends and family under a same group:')\ndisplay(pd.concat([full_dataset['Ticket'],family[['Surname','Family_code']],group[['Ticket_code','Group_code']]], axis = 1)[group['Group_code'] == 458])","a1989508":"# Prepare the df by adding the Survived features\ngroup_final = pd.concat([family[['Surname_code','Surname_count','Family_code','Family_count']],\n                       group[['Ticket_code','Ticket_count','Group_code','Group_count']],\n                        full_dataset['Survived']], axis = 1)","6633785c":"for param in [('Surname_code','Surname_count'),\n              ('Family_code','Family_count'),\n              ('Ticket_code','Ticket_count'),\n              ('Group_code','Group_count')]: # keep group at last\n    \n    # No. of member survived in each group\n    n_member_survived_by_gp = group_final.groupby(param[0]).Survived.sum()\n    \n    # No. of member survived in a particular group, discounting the passenger concerned\n    n_mem_survived = group_final[param[0]].map(n_member_survived_by_gp)\n    n_mem_survived_adj = n_mem_survived - group_final.Survived.apply(lambda x: 1 if x == 1 else 0)\n\n    # Same for the dead\n    n_member_dead_by_gp = group_final.groupby(param[0]).Survived.count() - group_final.groupby(param[0]).Survived.sum()\n    n_mem_dead  = group_final[param[0]].map(n_member_dead_by_gp)\n    n_mem_dead_adj = n_mem_dead - group_final.Survived.apply(lambda x: 1 if x == 0 else 0)\n\n    # How many people from that group that we do not have data on.\n    unknown_factor = (group_final[param[1]] - n_mem_survived_adj - n_mem_dead_adj)\/group_final[param[1]]\n    confidence = 1 - unknown_factor\n\n    # Ratio of members survived in that group, ranging from -1 to 1, adjusted by the confidence weight\n    key = 'Confidence_member_survived'+'_'+param[0]\n    ratio = (1\/group_final[param[1]]) * (n_mem_survived_adj - n_mem_dead_adj)\n    group_final[key] = confidence * ratio\n\n# Display Correlation\nplt.barh(group_final.corr().Survived[-4:].index, group_final.corr().Survived[-4:])\nplt.xlabel('Correlation with Survived');\n\nfeatures['Cf_mem_survived'] = group_final['Confidence_member_survived_Group_code']","a81a1e73":"features['Parch'] = full_dataset['Parch']\nfeatures['SibSp'] = full_dataset['SibSp']\nfeatures['Group_size'] = group['Group_count']\n\nfeatures.head()","34c7e4ed":"from sklearn.preprocessing import StandardScaler\n\n# Standardize the continuous variables\nscalar = StandardScaler()\nfeatures_z_transformed = features.copy()\ncontinuous = ['Fare'] \nfeatures_z_transformed[continuous] = scalar.fit_transform(features_z_transformed[continuous])\n\n# Transform Sex labels into binary code\nfeatures_z_transformed.Sex = features_z_transformed.Sex.apply(lambda x: 1 if x == 'male' else 0)\n\n# One-hot Encoding\nfeatures_final = pd.get_dummies(features_z_transformed)\n\nencoded = list(features_final.columns)\nprint(\"{} total features after one-hot encoding.\".format(len(encoded)))\n\n# Seperate Train Data and Test Data\nfeatures_final_train = features_final[:891]\nfeatures_final_test = features_final[891:]","c5e3a232":"# Spliting Training Sets into Train and Cross-validation sets\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n\nX_train, X_test, y_train, y_test = train_test_split(features_final_train, \n                                                    train.Survived, \n                                                    test_size = 0.2, \n                                                    random_state = 0)","1e7399ac":"# Create Model Training Pipeline\nfrom sklearn.metrics import accuracy_score\n\ndef train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n    \n    results = {}\n    \n    # Fit the learner to the training data using slicing with 'sample_size' using .fit(training_features[:], training_labels[:])\n    learner = learner.fit(X_train[:sample_size], y_train[:sample_size])\n    \n    # Get the predictions on the test set(X_test),\n    predictions_test = learner.predict(X_test)\n    \n    # then get predictions on the training samples(X_train)\n    predictions_train = learner.predict(X_train)\n            \n    # Compute accuracy on the training samples\n    results['acc_train'] = accuracy_score(y_train, predictions_train)\n        \n    # Compute accuracy on test set using accuracy_score()\n    results['acc_test'] = accuracy_score(y_test, predictions_test)\n       \n    # Success\n    print(\"{} trained on {} samples. Acc: {:.4f}\".format(learner.__class__.__name__, sample_size, results['acc_test']))\n        \n    # Return the results\n    return results","69543554":"from sklearn.ensemble import RandomForestClassifier\n\n# Initialize the three models\nclf_C = RandomForestClassifier(random_state= 0)\n\n# Calculate the number of samples for 10%, 50%, and 100% of the training data\nsamples_100 = len(y_train)\nsamples_10 = int(len(y_train)\/2)\nsamples_1 = int(len(y_train)\/10)\n\n# Collect results on the learners\nresults = {}\nfor clf in [ clf_C]:\n    clf_name = clf.__class__.__name__\n    results[clf_name] = {}\n    for i, samples in enumerate([samples_1, samples_10, samples_100]):\n        results[clf_name][i] = \\\n        train_predict(clf, samples, X_train, y_train, X_test, y_test)","7366b3d2":"# Reshaping the Results for plotting\ndf = pd.DataFrame()\n\nfor i in results.items():\n    temp = pd.DataFrame(i[1]).rename(columns={0:'1% of train', 1:'10% of train', 2:'100% of train'})\n    temp['model'] = i[0]\n    df = pd.concat([df, temp], axis = 0)\ndf_plot = df.reset_index().melt(id_vars=['index','model'])\n\n# Ploting the results\nfig, axs = plt.subplots(1,2,figsize = (16,5))\nfor i,key in enumerate(df_plot['index'].unique()[:2]):\n    ax = axs[i%2]\n    sns.barplot(data = df_plot[df_plot['index'] == key], x = 'model', y = 'value',\n                hue = 'variable', ax = ax)\n    ax.set_ylim([0.6,1])\n    ax.set_title(key)\n    ax.legend(loc=\"lower right\")","be0a9e55":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nwarnings.filterwarnings('ignore')\n\nclf = RandomForestClassifier(random_state = 0, oob_score = True)\n\nparameters = {'criterion' :['gini'],\n             'n_estimators' : [350], \n             'max_depth':[5], \n             'min_samples_leaf': [4], \n              'max_leaf_nodes': [10], \n              'min_impurity_decrease': [0], \n              'max_features' : [1] \n             }\n\nscorer = make_scorer(accuracy_score)\n\ngrid_obj = GridSearchCV(clf, parameters, scoring = scorer, cv = 10)\n\ngrid_fit = grid_obj.fit(X_train,y_train)\n\nbest_clf = grid_fit.best_estimator_\n\npredictions = (clf.fit(X_train, y_train)).predict(X_test)\nbest_predictions = best_clf.predict(X_test)\n\nprint(\"Unoptimized model\\n\")\nprint(\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\nprint(\"Oob score on testing data: {:.4f}\".format(clf.oob_score_))\nprint(\"\\nOptimized Model\\n\")\nprint(\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\nprint(\"Final oob score on the testing data: {:.4f}\".format(best_clf.oob_score_))\nprint(\"\\nBest Parameters\\n\")\nbest_clf","55067fc6":"# Plot Feature Importnace\nidx = np.argsort(best_clf.feature_importances_)\nplt.figure(figsize = (12,8))\nplt.barh(range(len(best_clf.feature_importances_)),best_clf.feature_importances_[idx])\nplt.yticks(range(len(best_clf.feature_importances_)),features_final_train.columns[idx]);\nplt.title('Feature Importance in the data');","74c549ef":"# Output for Kaggle competition\nfinal_predict = best_clf.predict(features_final_test)\n\nprediction = pd.DataFrame(full_dataset[891:].PassengerId)\nprediction['Survived'] = final_predict.astype('int')\n\nprediction.to_csv('prediction_final_sub.csv',index = False)\nprint(\"Final prediction for test data is in prediction_final_sub.csv file!\")","85f40173":"**Results form the above analysis in the graphs:**\n\n* Sex seems to have a strong predictive power, which makes sense due to the \"Women and Children First\" preference instructions for deciding who can get on the lifeboats.\n\n* Pclass(passenger class) and Fare also showed a moderate correalation with Survival. These higher class passengers lives and have most of their activities near the deck, thus, closer to the lifeboats.\n\n* It is surprising to find no significant correlation between Age and Survived. Their relationship may not be linear.\n\n* Cabin seem to have some relationships with survival, although we have lots of Non values in this feature. Perhaps it's possible to guess those Nan values after looking into its relationships with Ticket no., Embark and PClass.\n\n* Embark C seem have significantly higher survival rate compared to Embark S, which also have a relatively low variance, There may be a relationship of where they board the Titanic and where they stay on boat.","8c31206b":"**Feature Engineering part 1**","d2bc088f":"**Below Function Parameters**\n- learner: the learning algorithm to be trained and predicted on\n- sample_size: the size of samples to be drawn from training set\n- X_train: features training set\n- y_train: income training set\n- X_test: features testing set\n- y_test: income testing set\n","ab6e7734":"**Below Function Parameters**\n\nThis function takes in 2 columns of labels and chain all items which share\nthe same labels within each of the 2 columns\n    \n    input:\n*     df - DataFrame\n*     colA - Key for Col\n*     colB - Key for Col \n\n    output:\n*     array of numeric grouping labels\n    ","7a6dcfce":"   \n    Function to judge if passengers are probably to be in the same family.\n    Input: DataFrame with Passenger surname and ticket\n    Return: Code generated to specify different families\n    ","10c4abad":"**Feature Engineering Part 2**"}}