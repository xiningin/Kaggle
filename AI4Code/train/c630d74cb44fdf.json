{"cell_type":{"c4ee56d1":"code","b15ce477":"code","dabb62c1":"code","30a5e9dd":"code","05b232c8":"code","e2308ff6":"code","c7e0c66e":"code","d1f94a89":"code","690e55a0":"code","3886e974":"code","66ce0ee9":"code","6a2277d9":"code","e9e40c0b":"code","45955733":"code","bcbd3638":"code","49624bad":"code","d3bedc2d":"code","95cb7300":"code","43e4d0a5":"code","e59d7dbf":"code","7843719d":"code","8fc963ab":"code","02ab51bd":"code","86dc8f86":"code","b05f74fe":"code","b8455d84":"code","4fe0f7a4":"code","76f45e56":"code","92b46433":"code","351a2d1a":"code","1ace46dc":"code","6dc1da9c":"code","2915880c":"code","9ac1d0f5":"code","39a32dce":"code","13c7ab1a":"code","f3da19da":"code","4cf2c9fe":"code","643f1e39":"code","76dd5733":"code","5be28a46":"code","4854fe54":"code","cc92a0f0":"code","5f3f0e59":"code","c3005af6":"code","308484ea":"code","73959704":"code","84bfe456":"code","76c72ea9":"code","7c3d99ae":"code","7a29acc5":"code","2f7dd0f6":"code","67145ab7":"code","1c2ec22f":"code","fa764b86":"code","10af9e8c":"code","0ab34e8f":"code","4d44d414":"code","c6fadaf0":"code","25ad0fcf":"code","eb269ee1":"code","f6ad218d":"code","09ba328b":"code","ac44647e":"code","a0869567":"code","56e9a3f8":"code","df24d8dd":"code","5ddcba65":"code","d6c6a22f":"code","2bf0127c":"code","551e1b7c":"code","ee53f4bf":"code","ff66dcf0":"code","3921035d":"code","c37ecc2f":"code","7e5bb405":"code","d486177b":"code","f1512225":"code","a18317ef":"code","21f800c6":"code","6393f229":"code","ea71ff94":"code","b09486e5":"code","94d70ea7":"code","0a382023":"code","b0682d67":"code","f54b5abd":"code","30386ba1":"code","ee8132a8":"code","7dce605e":"code","042058bb":"code","197856cf":"code","31c78950":"code","83823994":"code","50ee8142":"code","76196f54":"code","8d5bf34f":"code","a6582199":"code","df52af52":"code","5fc9364b":"code","4695e910":"code","7b97af94":"code","1fd316e9":"code","d79d5e88":"code","f2642e81":"code","c45d0782":"code","b6bd4d4b":"code","7364a905":"code","b2a2449f":"code","f587f4d8":"code","842255ae":"code","de311370":"code","65597ccf":"code","434a3b32":"code","a11a8ab5":"code","0140d58a":"code","bcd76df2":"code","48134944":"code","a056495e":"code","a741be12":"code","e487f9fe":"code","6c942935":"code","3deadc0e":"code","a4c3b039":"code","c1806f43":"code","17ec008e":"code","7cd8f56a":"code","0791e9a8":"code","ee65129a":"code","62fc673d":"code","aaf3d5d0":"code","7143d459":"code","8b637843":"code","bdccaaa6":"code","02361059":"code","f4034f48":"code","326b2b5e":"code","968f7fe9":"code","97d8d334":"code","66b2e9ad":"code","6e631802":"code","a8e2faa2":"code","ee576067":"code","01a82060":"code","b10ee35f":"code","5ce331f1":"code","0c605764":"code","fdcab41a":"code","3257de77":"code","2af47116":"code","28520791":"code","44d45f6a":"code","64c41089":"code","6f5d634b":"code","9d408611":"code","9ba8039a":"code","bc2dfd3f":"code","ebdefb40":"code","df584285":"code","8789023c":"code","46e4c36f":"code","a4f1ca48":"code","8f6622e0":"code","853b1587":"code","12695660":"code","d4323546":"code","c850060f":"code","f17289b6":"code","4bab9afb":"code","f8b87325":"code","530c0470":"code","e07dbf96":"code","99b4d779":"code","052c7d4e":"code","43861af1":"code","0526c0c3":"code","80ccb88c":"code","867daf24":"code","2f2cb890":"code","a7f55b5d":"code","b31c28a6":"code","4f43add3":"code","d0583a4e":"code","86a57c1f":"code","f3b67d2d":"code","b6126565":"code","f7c2d568":"code","dd551fb7":"code","5e33f71a":"code","01625247":"code","8dc36916":"code","5c36275a":"code","2d0a87cc":"code","e53da78a":"code","46e045f4":"code","fd53c858":"code","cc4e67ea":"code","56123c77":"code","a7811009":"code","efb540b2":"code","c37420eb":"code","2039c2bd":"code","0abaa4ed":"code","a1bdec8f":"code","b6105ee4":"code","906f6d6b":"code","30a72cb3":"code","d58ff0f3":"code","6694c251":"code","f6d93b87":"code","663ccce5":"code","0a3ce1ec":"code","6c44b9e4":"code","95b7b0d8":"code","ab3d4cbb":"code","ecd54734":"code","ab271630":"code","219f870d":"code","294016c6":"code","f92003e3":"code","013a4d1f":"code","d24dd8ab":"code","4dc764b5":"code","fddc5d45":"code","904eb011":"code","14a5ca04":"code","54845417":"code","63075454":"code","a70c0e9b":"code","bfec2668":"code","aa4d4132":"code","61115584":"code","d4a3121d":"code","68ec010f":"code","f2b92e2d":"code","24024506":"code","56c9b33f":"code","51d18cfa":"code","ead70015":"code","797fb8cf":"markdown","37d2394b":"markdown","f0020de8":"markdown","7b282440":"markdown","683cefab":"markdown","6a909357":"markdown","4d07e693":"markdown","3085fab9":"markdown","a4c99557":"markdown","76c7bb64":"markdown","f49df6e9":"markdown","3d0d7346":"markdown","69b8efa0":"markdown","22ddc809":"markdown","480396d7":"markdown","f2a3fa8a":"markdown","a65f9eea":"markdown","c49cca39":"markdown","be955be8":"markdown","ea8cff2c":"markdown","d21e221c":"markdown","f02ec5bf":"markdown","57cc6bfc":"markdown","59849484":"markdown","c3d2ee3f":"markdown","74be550e":"markdown","bde095d0":"markdown","61f65156":"markdown","a1088aa6":"markdown","0760b2df":"markdown","e92dcf0e":"markdown","edc4adf7":"markdown","93697d24":"markdown","795e71be":"markdown","45e69fad":"markdown","697baf37":"markdown","bf5237b3":"markdown","a9ffc851":"markdown","6ff13dff":"markdown","fdce9ecc":"markdown","6a654122":"markdown","6d4cf359":"markdown","93cf55d6":"markdown","6d2aec46":"markdown","01e412b5":"markdown","9d363375":"markdown","d446fca0":"markdown","4dbbd7e9":"markdown","e2e76d2a":"markdown","3aaac471":"markdown","e75e59d2":"markdown","cf34035d":"markdown","b322f2c3":"markdown","45c01675":"markdown","56935182":"markdown","aee600f3":"markdown","b830c886":"markdown","e2ed5faf":"markdown","f2fedd31":"markdown","562c912b":"markdown","c106f87e":"markdown","79190cd9":"markdown","6739ad84":"markdown","1a5f79a2":"markdown","57b8eafd":"markdown","44494ad2":"markdown","6b0a3d44":"markdown","1f997960":"markdown","fffe6a17":"markdown","a95078d7":"markdown","de734b66":"markdown","13e8c9ad":"markdown","8d2e3536":"markdown","1a2cbf11":"markdown","55fe3e43":"markdown","5b4ff25f":"markdown","0a8efe8a":"markdown","e4de3bd0":"markdown","4d783ab3":"markdown","6ba9f1ec":"markdown","a1846360":"markdown","4e44edd3":"markdown","167c1c05":"markdown","7d308a7f":"markdown","32d736b6":"markdown","825bead9":"markdown","886c1df0":"markdown","92c52d2f":"markdown","dce06e04":"markdown","4a9c1428":"markdown","79ad8f2c":"markdown","1b02e58e":"markdown","631e3c8d":"markdown","f010f749":"markdown","134e9446":"markdown","1b1b2383":"markdown","858189d2":"markdown","68b92584":"markdown","8dba7e6a":"markdown","09f2f64c":"markdown","2dec1531":"markdown","e26b21f0":"markdown","20d7122e":"markdown","c4026e76":"markdown","22b666b4":"markdown","3541d793":"markdown"},"source":{"c4ee56d1":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.svm import SVC\nimport warnings\nwarnings.filterwarnings('ignore')","b15ce477":"#Load the Train set\ntrain_set = pd.read_csv('..\/input\/titanic\/train.csv')\ntrain_set.tail(10)","dabb62c1":"#Load the Test set\ntest_set = pd.read_csv('..\/input\/titanic\/test.csv')\ntest_set.head(10)","30a5e9dd":"# inspect the structure etc.\nprint(train_set.info(), \"\\n\")\nprint(train_set.shape)","05b232c8":"print(test_set.info(), \"\\n\")\nprint(test_set.shape)","e2308ff6":"train_set['Survived'].value_counts()","c7e0c66e":"# missing values in Train set df\ntrain_set.isnull().sum()","d1f94a89":"round(100*(test_set.isnull().sum().sort_values(ascending=False)\/len(test_set.index)), 2)","690e55a0":"# summing up the missing values (column-wise) and displaying fraction of NaNs\nround(100*(train_set.isnull().sum().sort_values(ascending=False)\/len(train_set.index)), 2)","3886e974":"train_set.Age.describe()","66ce0ee9":"train_set['Title']=train_set['Name'].map(lambda x: x.split(',')[1].split('.')[0].lstrip())\ntrain_set.head()","6a2277d9":"train_set['Title'].value_counts()","e9e40c0b":"print(train_set.info())","45955733":"#Check the list of values in title column\ntrain_set.Title.unique()","bcbd3638":"# lets sort the remaining other categories in title to various sub category of Mr, Miss, mrs, train_set\ntitle_list=['Mrs', 'Mr', 'Master', 'Miss']\ntrain_set.loc[~train_set['Title'].isin(title_list),['Age','Sex','Title']]","49624bad":"# function to bucket other titles into major 4\ndef fix_title(x):\n    title=x['Title']\n    if title in ['Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col','Sir']:\n        return 'Mr'\n    elif title in ['the Countess', 'Mme','Lady','Dona']:\n        return 'Mrs'\n    elif title in ['Mlle', 'Ms']:\n        return 'Miss'\n    elif title =='Dr':\n        if x['Sex']=='Male':\n            return 'Mr'\n        else:\n            return 'Mrs'\n    else:\n        return title","d3bedc2d":"train_set['Title']=train_set.apply(fix_title, axis=1)\ntrain_set['Title'].value_counts()","95cb7300":"train_set.Age.isnull().sum()","43e4d0a5":"#Check mean  on title Subclass w.r.t Age\ntrain_set.groupby(['Title'])['Age'].describe()","e59d7dbf":"train_set.groupby(['Title'])['Age'].median()","7843719d":"round(100*(train_set.isnull().sum().sort_values(ascending=False)\/len(train_set.index)), 2)","8fc963ab":"# Total Nullvalues in Age Column\ntrain_set.Age.isnull().sum()","02ab51bd":"print('Value of Master Class with Null Values {other}'.format(other=train_set.loc[(train_set.Title=='Master'),['Age']].isnull().sum()))\nprint('Value of Miss Class with Null Values {other}'.format(other=train_set.loc[(train_set.Title=='Miss'),['Age']].isnull().sum()))\nprint('Value of Mrs Class with Null Values {other}'.format(other=train_set.loc[(train_set.Title=='Mrs'),['Age']].isnull().sum()))\nprint('Value of Mr Class with Null Values {other}'.format(other=train_set.loc[(train_set.Title=='Mr'),['Age']].isnull().sum()))","86dc8f86":"train_set.loc[(train_set.Title=='Master'),['Age']].isnull().sum()","b05f74fe":"#Impute Missing values in Age Column\nmaster_median=train_set.loc[(train_set.Title=='Master') & ~(train_set.Age.isnull()),['Age']].median(axis=0, skipna=True).astype('float')\nmr_median=train_set.loc[(train_set.Title=='Mr') & ~(train_set.Age.isnull()),['Age']].median(axis=0, skipna=True).astype('float')\nmiss_median=train_set.loc[(train_set.Title=='Miss') & ~(train_set.Age.isnull()),['Age']].median(axis=0, skipna=True).astype('float')\nmrs_median=train_set.loc[(train_set.Title=='Mrs') & ~(train_set.Age.isnull()),['Age']].median(axis=0, skipna=True).astype('float')","b8455d84":"train_set.loc[(train_set.Title=='Master') & (train_set.Age.isnull()),'Age']=train_set.loc[(train_set.Title=='Master') & (train_set.Age.isnull()),'Age'].replace(np.nan,master_median.median())\ntrain_set.loc[(train_set.Title=='Miss') & (train_set.Age.isnull()),'Age']=train_set.loc[(train_set.Title=='Miss') & (train_set.Age.isnull()),'Age'].replace(np.nan,miss_median.median())\ntrain_set.loc[(train_set.Title=='Mrs') & (train_set.Age.isnull()),'Age']=train_set.loc[(train_set.Title=='Mrs') & (train_set.Age.isnull()),'Age'].replace(np.nan,mrs_median.median())\ntrain_set.loc[(train_set.Title=='Mr') & (train_set.Age.isnull()),'Age']=train_set.loc[(train_set.Title=='Mr') & (train_set.Age.isnull()),'Age'].replace(np.nan,mr_median.median())","4fe0f7a4":"# After Imputation on Age Colums verify the null values\ntrain_set.Age.isnull().sum()","76f45e56":"# Again summing up the missing values (column-wise) and displaying fraction of NaNs\nround(100*(train_set.isnull().sum().sort_values(ascending=False)\/len(train_set.index)), 2)","92b46433":"# GEt the unique set of Value of Cabin\ntrain_set.Cabin.unique()","351a2d1a":"train_set.Cabin.isnull().sum()","1ace46dc":"# Lets see the cabin with passenger class\nclass_cabin=train_set.groupby(['Pclass'])['Cabin'].count()\nclass_cabin","6dc1da9c":"# No Pclass\ntrain_set.Pclass.value_counts()","2915880c":"print('Value of 1st passenger Class with Null Values {other}'.format(other=train_set.loc[(train_set.Pclass==1),['Cabin']].isnull().sum()))\nprint('Value of 2nd passenger Class with Null Values {other}'.format(other=train_set.loc[(train_set.Pclass==2),['Cabin']].isnull().sum()))\nprint('Value of 3rd passenger Class with Null Values {other}'.format(other=train_set.loc[(train_set.Pclass==3),['Cabin']].isnull().sum()))\n","9ac1d0f5":"train_set.loc[(train_set.Pclass==1) & ~(train_set.Cabin.isnull()),['Cabin']]","39a32dce":"# Lets have Deck # as separte Columns and null as GNR\npd.Series(train_set.loc[(train_set.Pclass==1) & ~(train_set.Cabin.isnull()),['Cabin']].values.flatten()).astype('str').str[0]","13c7ab1a":"# Lets have Deck # as separte Columns and null as GNR\ntrain_set['Deck']=pd.Series(train_set.loc[~(train_set.Cabin.isnull()),['Cabin']].values.flatten()).astype('str').str[0]","f3da19da":"train_set.head()","4cf2c9fe":"# Lets see the unique value and count of Deck Column\ntrain_set['Deck'].value_counts()","643f1e39":"train_set.Deck.unique()","76dd5733":"train_set.Deck.isnull().sum()","5be28a46":"# REplace Nan in Decek to GNR\ntrain_set.loc[(train_set.Deck.isnull()),'Deck']=train_set.loc[ (train_set.Deck.isnull()),'Deck'].replace(np.nan,'GNR')\ntrain_set.Deck.isnull().sum()","4854fe54":"# Remove Cabin Column\ntrain_set.drop('Cabin',axis=1,inplace=True)","cc92a0f0":"train_set.head()","5f3f0e59":"# Now lets check the column with null values\ntrain_set.isnull().sum()","c3005af6":"# Value of Embarked on various categories\ntrain_set.Embarked.value_counts()","308484ea":"train_set.Embarked.isnull().sum()","73959704":"#Lets impute 2 null records of Embarked with value 'S' as it have max occurance\ntrain_set.loc[(train_set.Embarked.isnull()),'Embarked']=train_set.loc[ (train_set.Embarked.isnull()),'Embarked'].replace(np.nan,'S')\ntrain_set.Embarked.isnull().sum()","84bfe456":"# Check if any null columns are present\ntrain_set.isnull().sum()","76c72ea9":"sns.distplot(train_set['Age'])","7c3d99ae":"# pairplot\nsns.pairplot(train_set)\nplt.show()","7a29acc5":"sns.countplot(x=\"Pclass\", data=train_set)","2f7dd0f6":"sns.distplot(train_set['Fare'])","67145ab7":"sns.boxplot(y=train_set['Fare'])","1c2ec22f":"# Checking for Outlier\ntrain_set.Fare.describe(percentiles=[.25, .5, .75, .90, .95, .99])","fa764b86":"sns.countplot(x=\"Survived\", data=train_set)","10af9e8c":"### Checking the Survival Rate Rate\nsurvival = (sum(train_set['Survived'])\/len(train_set['Survived'].index))*100\nsurvival","0ab34e8f":"# Remove name and Passenger Id Column\ntrain_set.drop(['PassengerId', 'Name'],axis=1,inplace=True)","4d44d414":"#Checking the Correlation Matrix\nplt.figure(figsize = (15,10))\nsns.heatmap(train_set.corr(),annot = True)\nplt.show()","c6fadaf0":"#Check the Survival rate by Paasaenger Class\nprint(train_set [['Pclass','Survived']].groupby('Pclass').mean())","25ad0fcf":"sns.countplot(x=\"Pclass\", hue=\"Survived\", data=train_set)","eb269ee1":"#Check the Survival rate by Paasaenger Class\nsep=\"---------------------------------------------------------------\"\nprint( round(train_set [['Sex','Survived']].groupby(['Sex']).mean()*100,1),'\\n',sep)\nprint(train_set [['Pclass','Sex','Survived']].groupby(['Pclass','Sex']).agg(['count','mean']))","f6ad218d":"#tracking th Survival on the basis of Sex and PClass\ng = sns.catplot(x=\"Pclass\", hue=\"Sex\", col=\"Survived\",\n                data=train_set, kind=\"count\",\n                height=4, aspect=.7);","09ba328b":"# check the impact of Embarked Colum on Survival\nprint( round(train_set [['Embarked','Survived']].groupby(['Embarked']).mean()*100,1),'\\n',sep)\nprint(train_set [['Pclass','Embarked','Survived']].groupby(['Embarked','Pclass']).agg(['count','mean']),'\\n',sep)\nprint(train_set [['Pclass','Embarked','Survived','Sex']].groupby(['Embarked','Pclass','Sex']).agg(['count','mean']))","ac44647e":"sns.countplot(x=\"Parch\", hue=\"Survived\", data=train_set)","a0869567":"sns.countplot(x=\"SibSp\", hue=\"Survived\", data=train_set)","56e9a3f8":"train_set['Family']=train_set['SibSp']+train_set['Parch']+1\ntrain_set.head()","df24d8dd":"df = train_set.groupby(['Ticket']).size().reset_index(name='count')\nprint( df)","5ddcba65":"master=pd.concat([train_set, test_set])\nmaster.head()","d6c6a22f":"# New column for Ticket Head Count on teh complete data\ntrain_set['TicketHeadCount']=train_set['Ticket'].map(master['Ticket'].value_counts())\ntrain_set.head()","2bf0127c":"#Let take fair per Person as per Ticket head Count\ntrain_set['FairPerPerson']=train_set['Fare']\/train_set['TicketHeadCount']\ntrain_set[['FairPerPerson']].describe(percentiles=[.25, .5, .75, .90, .95, .99])","551e1b7c":"# Lets check the distribution\nsns.distplot(train_set['FairPerPerson'])","ee53f4bf":"#Check the impact of Fair on chances of Survival\nplt.figure(figsize = (15,10))\nsns.violinplot(x=\"Family\", y=\"FairPerPerson\", hue=\"Survived\",\n                    data=train_set, palette=\"muted\")\nplt.show()","ff66dcf0":"sns.violinplot(x=\"Pclass\", y=\"FairPerPerson\", hue=\"Sex\",\n                    data=train_set, palette=\"muted\")","3921035d":"sns.violinplot(x=\"Survived\", y=\"Age\", hue=\"Sex\",\n                    data=train_set, palette=\"muted\")","c37ecc2f":"plt.figure(figsize=(20, 12))\nplt.subplot(2,3,1)\nsns.stripplot(x=\"Survived\",y=\"Age\",data=train_set.loc[(train_set['Age']>0.0) & (train_set.Age<15.0)],jitter=True,palette='Set1')\nplt.subplot(2,3,2)\nsns.stripplot(x=\"Survived\",y=\"Age\",data=train_set.loc[(train_set['Age']>15.0) & (train_set.Age<40.0)],jitter=True,palette='Set1')\nplt.subplot(2,3,3)\nsns.stripplot(x=\"Survived\",y=\"Age\",data=train_set.loc[(train_set['Age']>40.0) & (train_set.Age<60.0)],jitter=True,palette='Set1')\nplt.subplot(2,3,4)\nsns.stripplot(x=\"Survived\",y=\"Age\",data=train_set.loc[(train_set['Age']>60.0) & (train_set.Age<80.0)],jitter=True,palette='Set1')\n\nplt.show()","7e5bb405":"# Lets check on graph the survival of male aginst Female within age 15-40 years\nsns.stripplot(x=\"Survived\",y=\"Age\",data=train_set.loc[(train_set['Age']>15.0) & (train_set.Age<40.0)],jitter=True,hue='Sex',palette='Set1')","d486177b":"#tracking th Survival on the basis of Family Size and Sex\ng = sns.catplot(x=\"Family\", hue=\"Sex\", col=\"Survived\",\n                data=train_set, kind=\"count\",\n                height=4, aspect=.7);","f1512225":"plt.figure(figsize=(16, 6))\nsns.catplot(x=\"Family\", col=\"Survived\",data=train_set.loc[train_set.Family==1], kind=\"count\",height=4, aspect=.7)\nplt.figure(figsize=(16, 6))\nsns.catplot(x=\"Family\", col=\"Survived\",\n                data=train_set.loc[train_set.Family.between(2,4)], kind=\"count\",\n                height=4, aspect=.7)\nplt.figure(figsize=(16, 6))\nsns.catplot(x=\"Family\", col=\"Survived\",data=train_set.loc[train_set.Family>4], kind=\"count\",height=4, aspect=.7)","a18317ef":"print(\"Casuality Rate on Solo Travellers on 1st Class\", round(len(train_set.loc[(train_set['Pclass']==1) & (train_set['Family']==1) & (train_set['Survived']==0)])\/len(train_set.loc[(train_set['Pclass']==1) & (train_set['Family']==1)])*100,2),'%')\nprint(\"Casuality Rate on family Group between 2 to 4 on 1st Class\", round(len(train_set.loc[(train_set['Pclass']==1) & (train_set['Family'].between(2,4)) & (train_set['Survived']==0)])\/len(train_set.loc[(train_set['Pclass']==1) & (train_set['Family'].between(2,4))])*100,2),'%')\nprint(\"Casuality Rate on Group of 5 and above on 1st Class\", round(len(train_set.loc[(train_set['Pclass']==1) & (train_set['Family']>4) & (train_set['Survived']==0)])\/len(train_set.loc[(train_set['Pclass']==1) & (train_set['Family']>4)])*100,2),'%')\n\n","21f800c6":"# Same as we can see in Graph too the Rate of Death on 1st class with repect to family Size\nsns.catplot(x=\"Family\", col=\"Survived\",data=train_set.loc[(train_set.Family==1) & (train_set.Pclass==1)], kind=\"count\",height=4, aspect=.7)\nsns.catplot(x=\"Family\", col=\"Survived\",data=train_set.loc[(train_set['Pclass']==1) & (train_set['Family'].between(2,4))], kind=\"count\",height=4, aspect=.7)\nsns.catplot(x=\"Family\", col=\"Survived\",data=train_set.loc[(train_set.Family>4) & (train_set.Pclass==1)], kind=\"count\",height=4, aspect=.7)","6393f229":"print(\"Casuality Rate on Solo Travellers on 2nd Class\", round(len(train_set.loc[(train_set['Pclass']==2) & (train_set['Family']==1) & (train_set['Survived']==0)])\/len(train_set.loc[(train_set['Pclass']==2) & (train_set['Family']==1)])*100,2),'%')\nprint(\"Casuality Rate on family Group between 2 to 4 on 2nd Class\", round(len(train_set.loc[(train_set['Pclass']==2) & (train_set['Family'].between(2,4)) & (train_set['Survived']==0)])\/len(train_set.loc[(train_set['Pclass']==2) & (train_set['Family'].between(2,4))])*100,2),'%')\nprint(\"Casuality Rate on Group of 5 and above on 2nd Class\", round(len(train_set.loc[(train_set['Pclass']==2) & (train_set['Family']>4) & (train_set['Survived']==0)])\/len(train_set.loc[(train_set['Pclass']==2) & (train_set['Family']>4)])*100,2),'%')","ea71ff94":"# Same as we can see in Graph too the Rate of Death on 2nd class with repect to family Size\nsns.catplot(x=\"Family\", col=\"Survived\",data=train_set.loc[(train_set.Family==1) & (train_set.Pclass==2)], kind=\"count\",height=4, aspect=.7)\nsns.catplot(x=\"Family\", col=\"Survived\",data=train_set.loc[(train_set['Pclass']==2) & (train_set['Family'].between(2,4))], kind=\"count\",height=4, aspect=.7)\nsns.catplot(x=\"Family\", col=\"Survived\",data=train_set.loc[(train_set.Family>4) & (train_set.Pclass==2)], kind=\"count\",height=4, aspect=.7)","b09486e5":"print(\"Casuality Rate on Solo Travellers on 3rd Class\", round(len(train_set.loc[(train_set['Pclass']==3) & (train_set['Family']==1) & (train_set['Survived']==0)])\/len(train_set.loc[(train_set['Pclass']==3) & (train_set['Family']==1)])*100,2),'%')\nprint(\"Casuality Rate on family Group between 2 to 4 on 3rd Class\", round(len(train_set.loc[(train_set['Pclass']==3) & (train_set['Family'].between(2,4)) & (train_set['Survived']==0)])\/len(train_set.loc[(train_set['Pclass']==3) & (train_set['Family'].between(2,4))])*100,2),'%')\nprint(\"Casuality Rate on Group of 5 and above on 3rd Class\", round(len(train_set.loc[(train_set['Pclass']==3) & (train_set['Family']>4) & (train_set['Survived']==0)])\/len(train_set.loc[(train_set['Pclass']==3) & (train_set['Family']>4)])*100,2),'%')","94d70ea7":"# Same as we can see in Graph too the Rate of Death on 3rd class with repect to family Size\nsns.catplot(x=\"Family\", col=\"Survived\",data=train_set.loc[(train_set.Family==1) & (train_set.Pclass==3)], kind=\"count\",height=4, aspect=.7)\nsns.catplot(x=\"Family\", col=\"Survived\",data=train_set.loc[(train_set['Pclass']==3) & (train_set['Family'].between(2,4))], kind=\"count\",height=4, aspect=.7)\nsns.catplot(x=\"Family\", col=\"Survived\",data=train_set.loc[(train_set.Family>4) & (train_set.Pclass==3)], kind=\"count\",height=4, aspect=.7)","0a382023":"print(\"Casuality Rate on Age Group 0-15 on 1st Class\", round(len(train_set.loc[(train_set['Pclass']==1) & (train_set['Age'].between(0,15)) & (train_set['Survived']==0)])\/len(train_set.loc[(train_set['Pclass']==1) & (train_set['Age'].between(0,15))])*100,2),'%')\nprint(\"Casuality Rate on Age Group between 15 to 40 on 1st Class\", round(len(train_set.loc[(train_set['Pclass']==1) & (train_set['Age'].between(15,40)) & (train_set['Survived']==0)])\/len(train_set.loc[(train_set['Pclass']==1) & (train_set['Age'].between(15,40))])*100,2),'%')\nprint(\"Casuality Rate on Group of 40-60 on 1st Class\", round(len(train_set.loc[(train_set['Pclass']==1) & (train_set['Age'].between(40,60)) & (train_set['Survived']==0)])\/len(train_set.loc[(train_set['Pclass']==1) & (train_set['Age'].between(40,60))])*100,2),'%')\nprint(\"Casuality Rate on Group of 60-80 on 1st Class\", round(len(train_set.loc[(train_set['Pclass']==1) & (train_set['Age'].between(60,80)) & (train_set['Survived']==0)])\/len(train_set.loc[(train_set['Pclass']==1) & (train_set['Age'].between(60,80))])*100,2),'%')","b0682d67":"# Same as we can see in Graph too the Rate of Death on 1st class with repect to Age Group\nsns.catplot(x=\"Family\", col=\"Survived\",data=train_set.loc[(train_set['Age'].between(0,15)) & (train_set.Pclass==1)], kind=\"count\",height=4, aspect=.7, hue='Sex')\nsns.catplot(x=\"Family\", col=\"Survived\",data=train_set.loc[(train_set['Pclass']==1) & (train_set['Age'].between(15,40))], kind=\"count\",height=4, aspect=.7,hue='Sex')\nsns.catplot(x=\"Family\", col=\"Survived\",data=train_set.loc[(train_set['Age'].between(40,60)) & (train_set.Pclass==1)], kind=\"count\",height=4, aspect=.7,hue='Sex')\nsns.catplot(x=\"Family\", col=\"Survived\",data=train_set.loc[(train_set['Age'].between(60,80)) & (train_set.Pclass==1)], kind=\"count\",height=4, aspect=.7,hue='Sex')","f54b5abd":"print(\"Casuality Rate on Age Group 0-15 on 2nd Class\", round(len(train_set.loc[(train_set['Pclass']==2) & (train_set['Age'].between(0,15)) & (train_set['Survived']==0)])\/len(train_set.loc[(train_set['Pclass']==2) & (train_set['Age'].between(0,15))])*100,2),'%')\nprint(\"Casuality Rate on Age Group between 15 to 40 on 2nd Class\", round(len(train_set.loc[(train_set['Pclass']==2) & (train_set['Age'].between(15,40)) & (train_set['Survived']==0)])\/len(train_set.loc[(train_set['Pclass']==2) & (train_set['Age'].between(15,40))])*100,2),'%')\nprint(\"Casuality Rate on Group of 40-60 on 2nd Class\", round(len(train_set.loc[(train_set['Pclass']==2) & (train_set['Age'].between(40,60)) & (train_set['Survived']==0)])\/len(train_set.loc[(train_set['Pclass']==2) & (train_set['Age'].between(40,60))])*100,2),'%')\nprint(\"Casuality Rate on Group of 60-80 on 2nd Class\", round(len(train_set.loc[(train_set['Pclass']==2) & (train_set['Age'].between(60,80)) & (train_set['Survived']==0)])\/len(train_set.loc[(train_set['Pclass']==2) & (train_set['Age'].between(60,80))])*100,2),'%')","30386ba1":"# Same as we can see in Graph too the Rate of Death on 2nd class with repect to Age Group\nsns.catplot(x=\"Family\", col=\"Survived\",data=train_set.loc[(train_set['Age'].between(0,15)) & (train_set.Pclass==2)], kind=\"count\",height=4, aspect=.7, hue='Sex')\nsns.catplot(x=\"Family\", col=\"Survived\",data=train_set.loc[(train_set['Pclass']==2) & (train_set['Age'].between(15,40))], kind=\"count\",height=4, aspect=.7,hue='Sex')\nsns.catplot(x=\"Family\", col=\"Survived\",data=train_set.loc[(train_set['Age'].between(40,60)) & (train_set.Pclass==2)], kind=\"count\",height=4, aspect=.7,hue='Sex')\nsns.catplot(x=\"Family\", col=\"Survived\",data=train_set.loc[(train_set['Age'].between(60,80)) & (train_set.Pclass==2)], kind=\"count\",height=4, aspect=.7,hue='Sex')","ee8132a8":"print(\"Casuality Rate on Age Group 0-15 on 3rd Class\", round(len(train_set.loc[(train_set['Pclass']==3) & (train_set['Age'].between(0,15)) & (train_set['Survived']==0)])\/len(train_set.loc[(train_set['Pclass']==3) & (train_set['Age'].between(0,15))])*100,2),'%')\nprint(\"Casuality Rate on Age Group between 15 to 40 on 3rd Class\", round(len(train_set.loc[(train_set['Pclass']==3) & (train_set['Age'].between(15,40)) & (train_set['Survived']==0)])\/len(train_set.loc[(train_set['Pclass']==3) & (train_set['Age'].between(15,40))])*100,2),'%')\nprint(\"Casuality Rate on Group of 40-60 on 3rd Class\", round(len(train_set.loc[(train_set['Pclass']==3) & (train_set['Age'].between(40,60)) & (train_set['Survived']==0)])\/len(train_set.loc[(train_set['Pclass']==3) & (train_set['Age'].between(40,60))])*100,2),'%')\nprint(\"Casuality Rate on Group of 60-80 on 3rd Class\", round(len(train_set.loc[(train_set['Pclass']==3) & (train_set['Age'].between(60,80)) & (train_set['Survived']==0)])\/len(train_set.loc[(train_set['Pclass']==3) & (train_set['Age'].between(60,80))])*100,2),'%')","7dce605e":"# Same as we can see in Graph too the Rate of Death on 3rd class with repect to Age Group\nsns.catplot(x=\"Family\", col=\"Survived\",data=train_set.loc[(train_set['Age'].between(0,15)) & (train_set.Pclass==3)], kind=\"count\",height=4, aspect=.7, hue='Sex')\nsns.catplot(x=\"Family\", col=\"Survived\",data=train_set.loc[(train_set['Pclass']==3) & (train_set['Age'].between(15,40))], kind=\"count\",height=4, aspect=.7,hue='Sex')\nsns.catplot(x=\"Family\", col=\"Survived\",data=train_set.loc[(train_set['Age'].between(40,60)) & (train_set.Pclass==3)], kind=\"count\",height=4, aspect=.7,hue='Sex')\nsns.catplot(x=\"Family\", col=\"Survived\",data=train_set.loc[(train_set['Age'].between(60,80)) & (train_set.Pclass==3)], kind=\"count\",height=4, aspect=.7,hue='Sex')","042058bb":"sns.boxplot(x = 'Pclass', y = 'FairPerPerson',hue='Survived', data = train_set)","197856cf":"# Group the Deck by Class\nprint(train_set.groupby([ 'Pclass','Deck'])['Survived'].agg(['count','mean']))","31c78950":"# Lets Check the pattern of Deck on Age\nsns.swarmplot(x=\"Deck\",y=\"Age\",hue='Sex',data=train_set,palette=\"Set1\", split=True)","83823994":"sns.swarmplot(x=\"Deck\",y=\"FairPerPerson\",hue='Pclass',data=train_set,palette=\"Set1\", split=True)","50ee8142":"plt.figure(figsize=(25, 14))\nsns.catplot(x=\"Deck\", col=\"Survived\",data=train_set, kind=\"count\",height=4, aspect=.7, hue='Pclass')\nplt.show()","76196f54":"# Lets now check the records where we have Fare=0\nprint(train_set[(train_set['Fare']==0)].groupby(['Pclass', 'Ticket']).agg(['count']))","8d5bf34f":"print(train_set.loc[(train_set['FairPerPerson']==0),['Embarked','Ticket','Age','Sex','Family','TicketHeadCount']])","a6582199":"train_set['FairPerPerson'].describe(percentiles=[.25, .5, .75, .90, .95, .99])","df52af52":"train_set.info()","5fc9364b":"train_set.loc[(train_set.Pclass==1) & (train_set.FairPerPerson>=60)]['FairPerPerson'].describe(percentiles=[.25, .5, .75, .90, .95, .99])","4695e910":"quantile_1, quantile_3 = np.percentile(train_set.FairPerPerson, [25, 75])","7b97af94":"print(quantile_1, quantile_3)","1fd316e9":"iqr_value = quantile_3 - quantile_1\niqr_value","d79d5e88":"lower_bound_val = quantile_1 - (1.5 * iqr_value)\nupper_bound_val = quantile_3 + (1.5 * iqr_value)\nprint(lower_bound_val, upper_bound_val)","f2642e81":"plt.figure(figsize = (10, 5))\nsns.kdeplot(train_set.FairPerPerson)\nplt.axvline(x=-2.0, color = 'red')\nplt.axvline(x=23.32, color = 'red')","c45d0782":"train_set[(train_set.FairPerPerson >= lower_bound_val) & (train_set.FairPerPerson <= upper_bound_val)].info()","b6bd4d4b":"round(100*(train_set[(train_set.FairPerPerson >= lower_bound_val) & (train_set.FairPerPerson <= upper_bound_val)].count()\/len(train_set.index)), 2)","7364a905":"round(100*(train_set[(train_set.FairPerPerson >= 0) & (train_set.FairPerPerson <= 60)].count()\/len(train_set.index)), 2)","b2a2449f":"train_set_copy=train_set.loc[(train_set.FairPerPerson>0) & (train_set.FairPerPerson<=60)]\ntrain_set_copy.head()","f587f4d8":"train_set_copy.info()","842255ae":"sns.boxplot(x = 'Pclass', y = 'FairPerPerson',hue='Survived', data = train_set_copy)","de311370":"#train_set.drop(['Parch', 'Fare','Title','Deck','SibSp','TicketHeadCount'],axis=1,inplace=True)\ntrain_set_copy.drop(['Parch','Ticket','Fare','Title','Deck','SibSp','TicketHeadCount'],axis=1,inplace=True)\ntrain_set_copy.head()","65597ccf":"#convert Pclass as category type\ntrain_set_copy['Pclass'] = train_set_copy['Pclass'].map( {1: 'FirstClass', 2: 'SecondClass', 3:'ThirdClass'} ).astype('category')\ntrain_set_copy.head()    ","434a3b32":"# Creating a dummy variable for some of the categorical variables and dropping the first one.\ndummy1 = pd.get_dummies(train_set_copy[['Sex','Pclass','Embarked']], drop_first=True)\n\n# Adding the results to the master dataframe\ntrain_set_copy = pd.concat([train_set_copy, dummy1], axis=1)\ntrain_set_copy.head()","a11a8ab5":"#Drop Original Columns\ntrain_set_copy.drop(['Pclass','Sex', 'Embarked'],axis=1,inplace=True)","0140d58a":"train_set_copy.info()","bcd76df2":"from sklearn.model_selection import train_test_split","48134944":"# Putting feature variable to X\nX = train_set_copy.drop(['Survived'], axis=1)\n\nX.head()","a056495e":"# Putting response variable to y\ny = train_set_copy['Survived']\n\ny.head()","a741be12":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","e487f9fe":"from sklearn.preprocessing import StandardScaler","6c942935":"scaler = StandardScaler()\n\nX_train[['Age','FairPerPerson','Family']] = scaler.fit_transform(X_train[['Age','FairPerPerson','Family']])\n\nX_train.head()","3deadc0e":"### Checking the Survival Rate\nSurvival = (sum(train_set_copy['Survived'])\/len(train_set_copy['Survived'].index))*100\nSurvival","a4c3b039":"plt.figure(figsize = (20,10))\nsns.heatmap(X_train.corr(),annot = True)\nplt.show()","c1806f43":"import statsmodels.api as sm","17ec008e":"# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","7cd8f56a":"col = X_train.columns","0791e9a8":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","ee65129a":"col = col.drop('Embarked_Q', 1)\ncol","62fc673d":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","aaf3d5d0":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","7143d459":"col = col.drop('FairPerPerson', 1)\ncol","8b637843":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","bdccaaa6":"col = col.drop('Embarked_S', 1)\ncol","02361059":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","f4034f48":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","326b2b5e":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred[:10]","968f7fe9":"y_train_pred_final = pd.DataFrame({'Survived':y_train.values, 'Survived_Prob':y_train_pred})\n\ny_train_pred_final.head()","97d8d334":"y_train_pred_final['predicted'] = y_train_pred_final.Survived_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","66b2e9ad":"from sklearn.metrics import confusion_matrix,roc_auc_score,f1_score\n\n# define function to calculate and print model metrics.\ndef printMetrics(y_test,y_pred):\n    cp = confusion_matrix(y_test,y_pred)\n    sensitivity = cp[1,1]\/(cp[1,0]+cp[1,1])\n    specificity =  cp[0,0]\/(cp[0,1]+cp[0,0])\n    precision = cp[1,1]\/(cp[0,1]+cp[1,1])\n    print('Confusion Matrix: \\n',cp)\n    print(\"Sensitivity: \", sensitivity)\n    print(\"Specificity: \",specificity)\n    print(\"AUC Score: \", roc_auc_score(y_test,y_pred)) \n    print(\"Precision: \",precision)\n    print(\"f1 Score: \",f1_score(y_test,y_pred))","6e631802":"printMetrics(y_train_pred_final.Survived, y_train_pred_final.predicted)","a8e2faa2":"from sklearn import metrics\n# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Survived, y_train_pred_final.predicted))","ee576067":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","01a82060":"draw_roc(y_train_pred_final.Survived, y_train_pred_final.Survived_Prob)","b10ee35f":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Survived_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","5ce331f1":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Survived, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","0c605764":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","fdcab41a":"y_train_pred_final['final_predicted'] = y_train_pred_final.Survived_Prob.map( lambda x: 1 if x > 0.37 else 0)\n\ny_train_pred_final.head()","3257de77":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Survived, y_train_pred_final.final_predicted)","2af47116":"printMetrics(y_train_pred_final.Survived, y_train_pred_final.final_predicted)","28520791":"X_test[['Age','FairPerPerson','Family']] = scaler.transform(X_test[['Age','FairPerPerson','Family']])","44d45f6a":"X_test = X_test[col]\nX_test.head()","64c41089":"X_test_sm = sm.add_constant(X_test)","6f5d634b":"y_test_pred = res.predict(X_test_sm)\ny_test_pred[:10]","9d408611":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)\ny_pred_1.head()","9ba8039a":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)\ny_test_df.head()","bc2dfd3f":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)\ny_pred_final","ebdefb40":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Survived_Prob'})\ny_pred_final","df584285":"y_pred_final['final_predicted'] = y_pred_final.Survived_Prob.map(lambda x: 1 if x > 0.37 else 0)\ny_pred_final.head()","8789023c":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.Survived, y_pred_final.final_predicted)","46e4c36f":"printMetrics(y_pred_final.Survived, y_pred_final.final_predicted)","a4f1ca48":"# Importing random forest classifier from sklearn library\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Running the random forest with default parameters.\nrfc = RandomForestClassifier()","8f6622e0":"# Splitting the data into train and test, \n#we are doing a split again beacuse on top for Logistic we have done feature scalling. Here we will observe without Feature Scalling.\nX_random_train, X_random_test, y__random_train, y_random_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","853b1587":"rfc.fit(X_random_train,y__random_train)","12695660":"# Making predictions\npredictions = rfc.predict(X_random_test)","d4323546":"# Importing classification report and confusion matrix from sklearn metrics\nfrom sklearn.metrics import classification_report,accuracy_score","c850060f":"# Let's check the report of our default model\nprint(classification_report(y_test,predictions))","f17289b6":"# Printing confusion matrix\nprint(confusion_matrix(y_test,predictions))","4bab9afb":"print(accuracy_score(y_test,predictions))","f8b87325":"# GridSearchCV to find optimal n_estimators\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_depth': range(2, 20, 5)}\n\n# instantiate the model\nrf = RandomForestClassifier()\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_random_train, y__random_train)","530c0470":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","e07dbf96":"# plotting accuracies with max_depth\nplt.figure()\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n","99b4d779":"# GridSearchCV to find optimal n_estimators\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'n_estimators': range(100, 1500, 200)}\n\n# instantiate the model (note we are specifying a max_depth)\nrf = RandomForestClassifier(max_depth=3)\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_random_train, y__random_train)","052c7d4e":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","43861af1":"# plotting accuracies with n_estimators\nplt.figure()\nplt.plot(scores[\"param_n_estimators\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_n_estimators\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"n_estimators\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n","0526c0c3":"# GridSearchCV to find optimal max_features\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_features': [2, 4, 5, 7]}\n\n# instantiate the model\nrf = RandomForestClassifier(max_depth=3)\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_random_train, y__random_train)","80ccb88c":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","867daf24":"# plotting accuracies with max_features\nplt.figure()\nplt.plot(scores[\"param_max_features\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_features\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_features\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n","2f2cb890":"# GridSearchCV to find optimal min_samples_leaf\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_leaf': range(30, 100, 20)}\n\n# instantiate the model\nrf = RandomForestClassifier()\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_random_train, y__random_train)","a7f55b5d":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","b31c28a6":"# plotting accuracies with min_samples_leaf\nplt.figure()\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_leaf\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","4f43add3":"# GridSearchCV to find optimal min_samples_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_split': range(50, 300, 50)}\n\n# instantiate the model\nrf = RandomForestClassifier()\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_random_train, y__random_train)","d0583a4e":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","86a57c1f":"# plotting accuracies with min_samples_split\nplt.figure()\nplt.plot(scores[\"param_min_samples_split\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_split\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_split\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","f3b67d2d":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [2,3,4],\n    'min_samples_leaf': range(30, 100, 20),\n    'min_samples_split': range(50, 300, 50),\n    'n_estimators': [200,400,600,900], \n    'max_features': [3,4,5]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1,verbose = 1)","b6126565":"grid_search.fit(X_random_train, y__random_train)","f7c2d568":"# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","dd551fb7":"# model with the best hyperparameters\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(bootstrap=True,\n                             max_depth=4,\n                             min_samples_leaf=30, \n                             min_samples_split=150,\n                             max_features=3,\n                             n_estimators=900)","5e33f71a":"rfc.fit(X_random_train, y__random_train)","01625247":"# predict\npredictions = rfc.predict(X_random_test)","8dc36916":"print(classification_report(y_random_test,predictions))","5c36275a":"print(confusion_matrix(y_random_test,predictions))","2d0a87cc":"print(metrics.accuracy_score(y_random_test, predictions))","e53da78a":"X_svc_train, X_svc_test, y_svc_train, y_svc_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","46e045f4":"scaler = StandardScaler()\n\nX_svc_train[['Age','FairPerPerson','Family']] = scaler.fit_transform(X_svc_train[['Age','FairPerPerson','Family']])","fd53c858":"X_svc_test[['Age','FairPerPerson','Family']] = scaler.transform(X_svc_test[['Age','FairPerPerson','Family']])","cc4e67ea":"# Model building\nfrom sklearn.svm import SVC\n# instantiate an object of class SVC()\n# note that we are using cost C=1\nmodel = SVC(C = 1)\n\n# fit\nmodel.fit(X_svc_train, y_svc_train)\n\n# predict\ny_pred = model.predict(X_svc_test)","56123c77":"metrics.confusion_matrix(y_true=y_svc_test, y_pred=y_pred)","a7811009":"# print other metrics\n\n# accuracy\nprint(\"accuracy\", metrics.accuracy_score(y_svc_test, y_pred))\n\n# precision\nprint(\"precision\", metrics.precision_score(y_svc_test, y_pred))\n\n# recall\/sensitivity\nprint(\"recall\", metrics.recall_score(y_svc_test, y_pred))\n","efb540b2":"printMetrics(y_svc_test,y_pred)","c37420eb":"# creating a KFold object with 5 splits \nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 4)\n\n# instantiating a model with cost=1\nmodel = SVC(C = 1)\n\n# computing the cross-validation scores \n# note that the argument cv takes the 'folds' object, and\n# we have specified 'accuracy' as the metric\n\ncv_results = cross_val_score(model, X_svc_train, y_svc_train, cv = folds, scoring = 'accuracy') ","2039c2bd":"# print 5 accuracies obtained from the 5 folds\nprint(cv_results)\nprint(\"mean accuracy = {}\".format(cv_results.mean()))","0abaa4ed":"# specify range of parameters (C) as a list\nparams = {\"C\": [0.1, 1, 10, 100, 1000]}\n\nmodel = SVC()\n\n# set up grid search scheme\n# note that we are still using the 5 fold CV scheme we set up earlier\nmodel_cv = GridSearchCV(estimator = model, param_grid = params, \n                        scoring= 'accuracy', \n                        cv = folds, \n                        verbose = 1,\n                       return_train_score=True)      ","a1bdec8f":"model_cv.fit(X_svc_train, y_svc_train)","b6105ee4":"# results of grid search CV\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","906f6d6b":"# plot of C versus train and test scores\n\nplt.figure(figsize=(8, 6))\nplt.plot(cv_results['param_C'], cv_results['mean_test_score'])\nplt.plot(cv_results['param_C'], cv_results['mean_train_score'])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\nplt.xscale('log')","30a72cb3":"best_score = model_cv.best_score_\nbest_C = model_cv.best_params_['C']\n\nprint(\" The highest test accuracy is {0} at C = {1}\".format(best_score, best_C))","d58ff0f3":"# model with the best value of C\nmodel = SVC(C=1)\n\n# fit\nmodel.fit(X_svc_train, y_svc_train)\n\n# predict\n#y_pred = model.predict(X_svc_test)","6694c251":"# print other metrics\nfrom sklearn import metrics\n# accuracy\nprint(\"accuracy\", metrics.accuracy_score(y_svc_test, y_pred))\n\n# precision\nprint(\"precision\", metrics.precision_score(y_svc_test, y_pred))\n\n# recall\/sensitivity\nprint(\"recall\", metrics.recall_score(y_svc_test, y_pred))\n\n","f6d93b87":"printMetrics(y_svc_test,y_pred)","663ccce5":"\n\ndataModel = {'Model': ['Linear SVC','Random Forest','Logistic Regression'],\n        'Accuracy': [0.8084,0.777,0.794],\n        'Precision':[0.8118,0.78,0.691],\n        'Recall':[0.725,0.75,0.79],\n        'F1 Score':[0.766,0.73,0.736]\n        }\n\ndfm = pd.DataFrame(dataModel, columns = ['Model', 'Accuracy','Precision','Recall', 'F1 Score'])\n\ndfm.sort_values(by = ['Accuracy'], ascending = False, inplace = True)    \ndfm","0a3ce1ec":"plt.figure(figsize=(20, 10))\nplt.subplot(2,4,1)\nsns.barplot(x=\"Model\", y=\"Precision\",data=dfm,palette='hot',edgecolor=sns.color_palette('dark',7))\nplt.xticks(rotation=90)\nplt.title('MLA Precision Comparison')\nplt.subplot(2,4,2)\nsns.barplot(x=\"Model\", y=\"Accuracy\",data=dfm,palette='hot',edgecolor=sns.color_palette('dark',7))\nplt.xticks(rotation=90)\nplt.title('MLA Test Accuracy Comparison')\nplt.subplot(2,4,3)\nsns.barplot(x=\"Model\", y=\"Recall\",data=dfm,palette='hot',edgecolor=sns.color_palette('dark',7))\nplt.xticks(rotation=90)\nplt.title('MLA Recall Comparison')\nplt.subplot(2,4,4)\nsns.barplot(x=\"Model\", y=\"F1 Score\",data=dfm,palette='hot',edgecolor=sns.color_palette('dark',7))\nplt.xticks(rotation=90)\nplt.title('MLA F1 Score Comparison')\nplt.show()","6c44b9e4":"test_set.Age.describe()","95b7b0d8":"# summing up the missing values (column-wise) and displaying fraction of NaNs\nround(100*(test_set.isnull().sum().sort_values(ascending=False)\/len(test_set.index)), 2)","ab3d4cbb":"test_set['Title']=test_set['Name'].map(lambda x: x.split(',')[1].split('.')[0].lstrip())\ntest_set.head()","ecd54734":"test_set['Title'].value_counts()","ab271630":"test_set['Title']=test_set.apply(fix_title, axis=1)\ntest_set['Title'].value_counts()","219f870d":"test_set.groupby(['Title'])['Age'].median()","294016c6":"test_set.groupby(['Title'])['Age'].describe()","f92003e3":"#Impute Missing values in Age Column\nmaster_median=test_set.loc[(test_set.Title=='Master') & ~(test_set.Age.isnull()),['Age']].median(axis=0, skipna=True).astype('float')\nmr_median=test_set.loc[(test_set.Title=='Mr') & ~(test_set.Age.isnull()),['Age']].median(axis=0, skipna=True).astype('float')\nmiss_median=test_set.loc[(test_set.Title=='Miss') & ~(test_set.Age.isnull()),['Age']].median(axis=0, skipna=True).astype('float')\nmrs_median=test_set.loc[(test_set.Title=='Mrs') & ~(test_set.Age.isnull()),['Age']].median(axis=0, skipna=True).astype('float')","013a4d1f":"test_set.loc[(test_set.Title=='Master') & (test_set.Age.isnull()),'Age']=test_set.loc[(test_set.Title=='Master') & (test_set.Age.isnull()),'Age'].replace(np.nan,master_median.median())\ntest_set.loc[(test_set.Title=='Miss') & (test_set.Age.isnull()),'Age']=test_set.loc[(test_set.Title=='Miss') & (test_set.Age.isnull()),'Age'].replace(np.nan,miss_median.median())\ntest_set.loc[(test_set.Title=='Mrs') & (test_set.Age.isnull()),'Age']=test_set.loc[(test_set.Title=='Mrs') & (test_set.Age.isnull()),'Age'].replace(np.nan,mrs_median.median())\ntest_set.loc[(test_set.Title=='Mr') & (test_set.Age.isnull()),'Age']=test_set.loc[(test_set.Title=='Mr') & (test_set.Age.isnull()),'Age'].replace(np.nan,mr_median.median())","d24dd8ab":"test_set.Age.isnull().sum()","4dc764b5":"# Again summing up the missing values (column-wise) and displaying fraction of NaNs\nround(100*(test_set.isnull().sum().sort_values(ascending=False)\/len(test_set.index)), 2)","fddc5d45":"test_set['Fare'].median()","904eb011":"test_set['Fare'].fillna(test_set['Fare'].median(), inplace=True)","14a5ca04":"# Again summing up the missing values (column-wise) and displaying fraction of NaNs\nround(100*(test_set.isnull().sum().sort_values(ascending=False)\/len(test_set.index)), 2)","54845417":"test_set['Family']=test_set['SibSp']+test_set['Parch']\ntest_set.head()","63075454":"# New column for Ticket Head Count on teh complete data\ntest_set['TicketHeadCount']=test_set['Ticket'].map(master['Ticket'].value_counts())\ntest_set.head()","a70c0e9b":"#Let take fair per Person as per Ticket head Count\ntest_set['FairPerPerson']=test_set['Fare']\/test_set['TicketHeadCount']\ntest_set[['FairPerPerson']].describe(percentiles=[.25, .5, .75, .90, .95, .99])","bfec2668":"test_set_copy=test_set.loc[(test_set.FairPerPerson>0) & (test_set.FairPerPerson<=60)]\ntest_set_copy.head()","aa4d4132":"#train_set.drop(['Parch', 'Fare','Title','Deck','SibSp','TicketHeadCount'],axis=1,inplace=True)\ntest_set_copy.drop(['Parch','Ticket','Fare','Title','Cabin','SibSp','TicketHeadCount'],axis=1,inplace=True)\ntest_set_copy.head()","61115584":"test_set_copy.drop(['PassengerId','Name'],axis=1,inplace=True)\ntest_set_copy.head()","d4a3121d":"#convert Pclass as category type\ntest_set_copy['Pclass'] = test_set_copy['Pclass'].map( {1: 'FirstClass', 2: 'SecondClass', 3:'ThirdClass'} ).astype('category')\ntest_set_copy.head()    ","68ec010f":"# Creating a dummy variable for some of the categorical variables and dropping the first one.\ntestdummy1 = pd.get_dummies(test_set_copy[['Sex','Pclass','Embarked']], drop_first=True)\n\n# Adding the results to the master dataframe\ntest_set_copy = pd.concat([test_set_copy, testdummy1], axis=1)\ntest_set_copy.head()","f2b92e2d":"#Drop Original Columns\ntest_set_copy.drop(['Pclass','Sex', 'Embarked'],axis=1,inplace=True)","24024506":"test_set_copy[['Age','FairPerPerson','Family']] = scaler.transform(test_set_copy[['Age','FairPerPerson','Family']])","56c9b33f":"\n# predict\ny_test_pred = model.predict(test_set_copy)\ny_test_pred","51d18cfa":"test_set_copy_dummy=test_set.loc[(test_set.FairPerPerson>0) & (test_set.FairPerPerson<=60)]\npredicted_df = pd.DataFrame({'PassengerId': test_set_copy_dummy['PassengerId'], 'Survived': y_test_pred})\npredicted_df.head()","ead70015":"predicted_df.to_csv('final_kaggle_submission.csv', index=False)","797fb8cf":"###### Defualt Hyperparameters\nLet's first fit a random forest model with default hyperparameters.","37d2394b":"##### Inferential Stats","f0020de8":"Apparently, the training and test scores *both* seem to increase as we increase max_features till 4 and then testaccuracy drops by 10%, and the model doesn't seem to overfit more with increasing max_features. Think about why that might be the case.","7b282440":"Now it looks convincing ","683cefab":"There is mostly negative correlation between Pclass, Age, Fare rest there is no big Correlation","6a909357":"Now only 2 records in Embarked have null values. We can either remove them as very less in no or we can replace with value of Max counts of category ","4d07e693":"###### Let's see the dividend on Age wrt to Pclass and impact of this on Gender","3085fab9":"We have 23% loss of data if we do outlier filteration on Fare using Inter-Quartile Range. We will choose some Fare range which we set max to leave out after that.\n1. People with Fare 0  we will exclude them.\n2. People with fair greter than 60 we will exclude them\n\nBy doing this we will only loose approve 2% of data and that is on which we can still live with it.","a4c99557":"Highest Survival in Class 1 and lowest in Class 3","76c7bb64":"From the Above represtation we can clearly see the Linear SVC Model Performace is better among all.","f49df6e9":"1. Master Class have Age mean 4.57 and median 3.5\n2. Miss Class have Age mean and median approx 21.0\n3. Mr. Class have Age mean 32 and median 30\n4. Mrs class have Age mean 36 and median 35. \n\nSo Mean and Median are very close by on all the values so lets impute the missing values in Age with Median","3d0d7346":"An ROC curve demonstrates several things:\n\n- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","69b8efa0":"### Tuning min_samples_leaf","22ddc809":"###### 2nd Class wrt to age group, family size, gender","480396d7":"As we can see and multiple people have same the same ticket #. No surprise here as people are travelling with Sibing and Parent. Lets create one more varibale for Ticket Head Count and then we can divide the Total fare per Person","f2a3fa8a":"Lets first use Name colums to drive the new Title columns which can be helpful in idea of age by grouping them","a65f9eea":"In my sense the desk or cabin information is missed out. And anyways Deck\/Cabinlist is not bringing any such hidden insights. Just Normal stuff.","c49cca39":"We can see in Fair column we have Outlier, we will check on this using Scalling later","be955be8":"## Grid Search to Find Optimal Hyperparameters","ea8cff2c":"##### For categorical variables with multiple levels, create dummy features (one-hot encoded)\n","d21e221c":"Now we have FairPerPerson have the highest p value so lets drop it","f02ec5bf":"There is certainely outlier in Fair and same thing we can see in Fair Per Person","57cc6bfc":"Let's try to find the optimum values for ```max_depth``` and understand how the value of max_depth impacts the overall accuracy of the ensemble.\n","59849484":"We can now find the optimal hyperparameters using GridSearchCV.","c3d2ee3f":"### Tuning n_estimators","74be550e":"Let's try to find the optimum values for n_estimators and understand how the value of n_estimators impacts the overall accuracy. Notice that we'll specify an appropriately low value of max_depth, so that the trees do not overfit.\n<br>\n","bde095d0":"#### Test Set Submissions","61f65156":"We can Removed Passanger Id and name as they will not of any use in this case","a1088aa6":" We Can see the Cabin list is not giving much information and every Class of Passanger have some missing values. We can see from the above few Steps that PClass 3 is lower class have highest missing values.\n 1. 1st Class have most popluated have given the Cabin # and can say the First letter is the kind of Deck# as we have in Ships for ex A, C, B, T, D, E\n 2. Same as in rest of the class we can relate that if there is Cabin List no is there so particulary assigned then is attached to some deck and for other cases where cabin # is Null we can say it as General\/ unkwnown. Let say it GNR(general).","0760b2df":"From the above we can categorize as below\n1. Don, major, Capt,Jonkheer, Rev, Col as MR as Sex=Male and age is also greater that 30.\n2. Countess, MMe can be categorized as Mrs as sex = Female and age is greater than 30\n3. Mlle, Ms as Miss because sex=Female, age<30\n4. Dr will be categorized into Mr and Mrs on the basis of Sex\n","e92dcf0e":"Few Points:\n1. From the graph the Single Member or solo passanger survival rate is very poor.\n2. Male Solo Passanger number is quite high and mortality rate too in comparision too Female.\n3. Survival Chances become low after family size>4","edc4adf7":"#### Plotting the ROC Curve","93697d24":"1. There are no solo passengers in group of 0-15 Years.\n2. There is no casuality in 2nd class for 0-15years.\n3. In 2nd Class age group of people 15-40 Years the male survival chances are very poor which means they will more prune towards death.\n4. 15-40 years Female travelling either solo or in group habe better chance for getting survived.\n5. in AGe group of 40-60 years in Men leaving solo travellers nobody survived who had 2nd Class reservation. However the there are no male or females travelled in thise with group of people more than 5.\n6. The no of people in 40-60 years also less in comparision with other classes.\n7. In 2nd Class 60-80 years traveller are very few and more prone towards meeting casuality.","795e71be":"You can see that as we increase the value of max_depth, both train and test scores increase till a point, but after that test score starts to decrease. The ensemble tries to overfit as we increase the max_depth.\n\nThus, controlling the depth of the constituent trees will help reduce overfitting in the forest.","45e69fad":"Though the training accuracy monotonically increases with C, the test accuracy gradually reduces. Thus, we can conclude that higher values of C tend to **overfit** the model. This is because a high C value aims to classify all training examples correctly (since C is the *cost of misclassification* - if you impose a high cost on the model, it will avoid misclassifying any points by overfitting the data). ","697baf37":"##### 1st Class","bf5237b3":"Lets make the DECK Nan values as GNR. The null values count is same as Cabin","a9ffc851":"## Random Forest","6ff13dff":"Let's now check the optimum value for min samples leaf in our case.","fdce9ecc":"Now there are no missing values present in the Data. Now Lets do plot some graphs to see distribution of columns and relationship between target variable","6a654122":"Parent and Sibiling are 2 information displayed separetely and it will be godd if we make them as as kind of Family Size= Sum of Parent+sibiling","6d4cf359":"But before the Ticket head Count we need to take test data file for calculating actual no, like how many totla passanger were travelling on the same ticket #. Then only Fair per person will be the right amount. So for this only combine the test and train data","93cf55d6":"##### Test-Train Split","6d2aec46":"This excerice of checking mean, median into subclass is important as it will help in decision on imputing Null values on Age as title give relative information on Age","01e412b5":"##### Feature Scaling","9d363375":"As % wise the trend is common it is not drawing any impact on Target variable. Less fair or more fair will not impact the cause of Survival","d446fca0":"1. Out of approx500+ Solo Travellers the casuality is around 370. Which is higher in male as we have already seen too\n2. Family Size between 2-4 have more survival chance\n3. From Family 5 or greater the chances of casuality is high.\n4. Family 8 and above group there is no chance of Survival as per the data\n\nThe group of 5 and above have more casuality  might be due to belonging of Class 3. Let's check that too","4dbbd7e9":"Making Predictions on Test Set","e2e76d2a":"Now every feature is within the limit of p test.Now we can check on VIF if any variable is out of the range of 5.0","3aaac471":"Class 3 Passenger are poorly affected with the sink","e75e59d2":"##### As we can see above Cabin and Age have maximum chunk of missing values. So we need to check or get inference for them to imput with some values","cf34035d":"##### 38% of the total have survived which  is good amout of no in target variable.So there is no class imbalance","b322f2c3":"VIF is also in range and controlled","45c01675":"Now Only Cabin have massive chunk \nand Very few in Embarked have the missing values. We need to find pattern in Cabin and for Embarked we can remove and impute from Median","56935182":"#### Checking VIFs","aee600f3":"#### From the curve above, 0.37 is the optimum point to take it as a cutoff probability.","b830c886":"Now we have Embarked_S as the over-excedding p value. So we will drop it ","e2ed5faf":"##### Model Building","f2fedd31":"###### Relationship between Pclass , family Size, and impact on casuality rate","562c912b":"On Fare Per Person among Survival and non survivers are same . Only 1st class have some outliers","c106f87e":"Even over the class the Deck are present for each. So nothing surprises here.","79190cd9":"###### 1st Class wrt to age group","6739ad84":"Now lets clean our test file and do submissions","1a5f79a2":"1. There were no boy child in family group of 2.\n2. Very less casualities in age group of 0-15 Years. And the reason will be and small kid always been protected by mother. So Females survivals are more in any class so being in 1st class the % is high to be get safed, the Mother also survived and makes the kid also safe\n3. 15-40 years Age Group in an there is only 1 Female Casulaities. \n4. 15-40 years solo travellers in 1st class have been on sufferring side. However if you are with 2 or more people in group your chances of getting to be safed increases.\n5. 40-60 years Age Group Solo Travellers are wrost hit by this.\n6. 40-60 years female casulaites are near to zero and only belongs to family group 0f 2 or 3 people.\n7. 60-80 Years Age don't have Female travellers in abundance. There were few and gets evacuted being in Senior Citizen category.\n8. if you are in Age Group of 60-80 and male passanger in 1st class your chance to be survived will approx 8-10% only.\n9. in 1st Class irespective of any class being a female Passanger on any age group chances to be survived is equal to or greater than 90%","57b8eafd":"Now we can remove the Cabin columns as the information noew can be checked and inferred from Deck Column","44494ad2":"### Tuning min_samples_split\n\nLet's now look at the performance of the ensemble as we vary min_samples_split.","6b0a3d44":"### Hyperparameter Tuning","1f997960":"###### 3rd Class","fffe6a17":"## Grid Search to Find Optimal Hyperparameter C","a95078d7":"There are a few variables with high p value>.005. It's best to drop these variables as they aren't helping much with prediction and unnecessarily making the model complex. The variable 'Embarked_Q' has the highest Q Value. So let's start by dropping that.","de734b66":"So far so good, let's now look at the list of hyperparameters which we can tune to improve model performance.","13e8c9ad":"## Linear SVC Model Building\n\nLet's build a linear SVM mode now. The ```SVC()``` class does that in sklearn. ","8d2e3536":"The hyperparameter **min_samples_leaf** is the minimum number of samples required to be at a leaf node:\n- If int, then consider min_samples_leaf as the minimum number.\n- If float, then min_samples_leaf is a percentage and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.","1a2cbf11":"##### Dropping some columns which we will impact on Survival.\n1. Title is used for imputing Age missing values and things can be drawn out of Age column.\n2. Will drop Parch, Sibsp as we derived Family Column.\n3. Will drop Ticket, Deck, Ticket Head Count Column as it will give any impact.\n4. Will drop Fare columns as we derived FairPerPerson column to use","55fe3e43":"In all the clases the survival rate for Female is higher during the evacuation time Female Passenger were the first to be evcuated we can infer from the above.\n\n96% and 92% females were able to survive on the 1st and 2nd class respectively.\n\nThe Survival rate of Male is very less and as 1st is premium class near to the evacuation area o have seen max# of survival in male category. rest keep deminishing","5b4ff25f":"###### we can see there are unnecessary catogories for Dr, Major and and some title for male as sir and Don and etc. Lets keep four categories only Mr., Miss. , Mrs, train_set","0a8efe8a":"Max no of people have per per ticket cost between 0-$20.0 and then between $20-$40. As min Fair is zero that is not possible either there is a mistake or it can be the staff those will also be boarded for passanger service. We will check in sometime for this","e4de3bd0":" Few Points:\n 1. As we have seen Females are already have higher % of survival in any class, so grouping them in Emabarked will not see any change.\n 2. We can see In \"Q\" Embarked Station for 2nd Class only 1 Male have a entry and unfortounetly he expied as Survival is Zero\n 3. Huge Loss of life those have embarked At S.\n 4. 1st Class of Embarked station \"C\" and \"S\" have seen male survival between 35%-40%\n 5. Maximum Passenger boarded at Station \"S\" and have least survival than other and have maximum entry for 3rd Class Passanger\n 6. \"Q\" board station is very least preferred for 1st and 2nd Class Passanger","4d783ab3":"##### Creating new column 'predicted' with 1 if Survived_Prob > 0.5 else 0","6ba9f1ec":"Few Points:\n1. Age group between 0-10 have the good survival% in both gender.\n2. Female see the more survives than death in any Age Group\n3. After age 60 everybody tends to death.\n4. Male sees more death than survivors between 15-25 Age group","a1846360":"###### Lets Check on Family Size now","4e44edd3":"\n\n## **> TITANIC PROBLEM - KAGGLE**\n\nRMS Titanic was a British passenger liner operated by the White Star Line that sank in the North Atlantic Ocean in the early morning hours of 15 April 1912, after striking an iceberg during her maiden voyage from Southampton to New York City. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making the sinking one of modern history's deadliest peacetime commercial marine disasters.\n\nAlthough Titanic had advanced safety features, such as watertight compartments and remotely activated watertight doors, it only carried enough lifeboats for 1,178 people\u2014about half the number on board, and one third of her total capacity\u2014due to the maritime safety regulations of those days.\n\nThe disaster was met with worldwide shock and outrage at the huge loss of life, shortage of life boats , unequal treatment of pasengers among different classes as well as the regulatory and operational failures that led to it. \n\nKaggle has given a data set with information of passengers. Titanic problem in kaggle mainly ask us to find answers for these questions.\n\n1. Whether there were any bias in Saving the people?\n2. If yes, what are the features which helped a person to get survived ?\n3. Can you create an ML model which predict the the survival chance of a given test data ?\n4. How much accuracy can be claimed in prediction?\n","167c1c05":"##### Creating a dataframe with the actual Survived flag and the predicted probabilities","7d308a7f":"Clearly as said earlier the Age Group Section 15-40 have seen more deaths and also major % of travellers\n\nAbove 60+ Age the % of death is very high","32d736b6":"###### 2nd Class","825bead9":"1. 3rd class passenger are worst hit migt be far from life boat evacuation area casued this high rate of deaths\n2. Solo traveller have ~80% casulities\n3. Group of people 2-4 had few chances of survival.\n4. group of people more than 4 having highest caulity rate overall and in particular too 3rd class\n5. For sure there are no survivours for group of family greater than 7.","886c1df0":"1. In AGe Group 1-15 years with family group more than 4 very rare chances of Survival\n2. Girl or boy with Family Group of 5 or 6 people, nobody survived.\n3. Most of the crowd in class 3 travelled are of age between 15-40 years.\n4. Solo Male Travellers of 15-40 age majorly dead.\n5. Family group greater than 3 in age group of 15-40 years are mostly dead.Also there are no male survivours too above group of 3 in that age group\n6. 40-60 Age Group casuality Rate is too high. 95% of people are dead. In this Age bucket either it is male or Female the casulity rate is high. No Female has survived.\n7. Age Group 60-80 years are solo passenger and very few in no.\n8. Only 1 Female in Class 3 for 60-80Years travelled and lucky to be survived.\n9. 100% male casulity in Class 3for 60-80Age Group.","92c52d2f":"###### Now lets check the relationship between targert variable and other dependent variable","dce06e04":"We have recived good accuracy with Linear SVM too on test ~81%. Other metrics are also as good % between 70-80%","4a9c1428":"The male Deaths are more dense between 15-40 age group","79ad8f2c":"### Finding Optimal Cutoff Point","1b02e58e":"1. From this above data it looks like these are Line man and Crew member. \n2. All have the same boarding station and Ticket no on some says\"Line\". \n3. All are male and of approx average age group 25-45 years.\n4. These are solo  travellers no sibiling or parent","631e3c8d":"The Deck are randomly distributed over. GNR which we named as unknown have highest distribution.\n\n2 reasons->Either Deck no on others are missing or Deck space were small rooms","f010f749":"Few Points:\n1. the group of 2 people is very likely to get survived on Class 1.\n2. Nobody is the Class 1 had a family greater than 6.\n3. group of people between 2-4 on 1st Class are frequent survivours than other groups","134e9446":"1. Solo traveller on 2nd class are  worst hit by this casuality\n2. Same as like 1st class casulity rate is low in 2nd class for group of people 2-4\n3. Quite Interesting as per percentage the casulity rate for group of 5 or 6 is zero. But only 1 case of this type of family has been there for each group and they were good and lucky enought to be safe together. might we all were together and near to life boats.\n4. No groups of grater than 6 boared for 2nd Class","1b1b2383":"AS we have got the value of C=1 as earlier so the model metrics will same as we have already seen earlier.\n*81%accuracy with Linear SVC model.","858189d2":"###### 3rd Class wrt to age group, family size, gender","68b92584":"#### Cabin","8dba7e6a":"### Tuning max_depth\ninteger or None, optional (default=None)The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.","09f2f64c":"### Tuning max_features\n\nLet's see how the model performance varies with ```max_features```, which is the maximum numbre of features considered for splitting at a node.","2dec1531":"As expected the difference in Fair per class","e26b21f0":"## Hyperparameter Tuning","20d7122e":"### Making predictions on the test set","c4026e76":"### Missing Value treatment","22b666b4":"###### Age","3541d793":"#### EDA Discovery\n1. Female Gender is highly survived on the ship regardless of any class against male.\n2. People with 1st Class have low casulity rate.\n3. Pclass and Fair are co-linear and will explain the same impact on Targer variable. As Fair increase, class increase and with that your survival chances do increase.\n4. Travelling in a group of 2-4 will increase your chances of getting saved.\n5. Senior Citizen on lower class will not be able to make it out from the casulity.\n6. 25-40 age group will have more chances to get saved."}}