{"cell_type":{"9bceae3e":"code","bbc012dd":"code","858cbb39":"code","751bdfbb":"code","ec6d9601":"code","bd9c2ecf":"code","58cac520":"code","4d6d37a9":"code","00fc8815":"code","8afe90fe":"code","509f7fe9":"code","5e05a7ec":"code","f3df2d88":"code","fda15535":"code","72105d65":"code","9df890f1":"code","41d6c1b6":"code","83762f2d":"code","1ff63e55":"code","c01d2457":"code","2c13d93b":"code","21f5d4a5":"markdown","c07ab599":"markdown","37166431":"markdown","8a1c9c49":"markdown","0375bf51":"markdown","ba157586":"markdown","7bf0f16e":"markdown","8cb4e8f5":"markdown","0e36be6f":"markdown","b35790bf":"markdown","ff449cf1":"markdown","0b50a87e":"markdown","97876329":"markdown","d2968513":"markdown","00c58550":"markdown","3af132d1":"markdown","ae34bc27":"markdown","c955588f":"markdown"},"source":{"9bceae3e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas import Timestamp\nfrom datetime import date\nfrom dateutil.relativedelta import relativedelta\nfrom sklearn.linear_model import LinearRegression","bbc012dd":"df_2011 = pd.read_csv('..\/input\/oakland-crime-statistics-2011-to-2016\/records-for-2011.csv', parse_dates=['Create Time', 'Closed Time'])\ndf_2012 = pd.read_csv('..\/input\/oakland-crime-statistics-2011-to-2016\/records-for-2012.csv', parse_dates=['Create Time', 'Closed Time'])\ndf_2013 = pd.read_csv('..\/input\/oakland-crime-statistics-2011-to-2016\/records-for-2013.csv', parse_dates=['Create Time', 'Closed Time'])\ndf_2014 = pd.read_csv('..\/input\/oakland-crime-statistics-2011-to-2016\/records-for-2014.csv', parse_dates=['Create Time', 'Closed Time'])\ndf_2015 = pd.read_csv('..\/input\/oakland-crime-statistics-2011-to-2016\/records-for-2015.csv', parse_dates=['Create Time', 'Closed Time'])\ndf_2016 = pd.read_csv('..\/input\/oakland-crime-statistics-2011-to-2016\/records-for-2016.csv', parse_dates=['Create Time', 'Closed Time'])","858cbb39":"list_dfs = [df_2011, df_2012, df_2013, df_2014, df_2015, df_2016]","751bdfbb":"def shapes():\n    x = 0\n    for i in list_dfs:\n        print(f\"Shape of dataset for {x+2011} is {i.shape}\")\n        x+=1\nshapes()","ec6d9601":"df_2011.head()","bd9c2ecf":"df_2012.head()","58cac520":"df_2013.head()","4d6d37a9":"df_2014.head()","00fc8815":"df_2015.head()","8afe90fe":"df_2016.head()","509f7fe9":"# Code to show count of priority crimes per year.\na = 0\nfor i in list_dfs:\n    print(i[i['Priority']!=0].groupby(['Priority']).size().reset_index(name=str(f'Count in {a + 2011}')))\n    a += 1\n    print(' ')","5e05a7ec":"# Bar charts for comparing priority type crimes\ndf = pd.DataFrame([\n    [1, 36699, 41926, 43171, 42773, 42418, 24555],\n    [2, 143314, 145504, 144859, 144707, 150162, 86272]\n],\ncolumns=['Priority']+[f'Count in {x}' for x in range(2011,2017)]\n)\n\ndf.plot.bar(x='Priority', subplots=True, layout=(2,3), figsize=(15, 7))","f3df2d88":"pri1_2011 = 36699\npri2_2011 = 143314\ntotal_2011 = pri1_2011 + pri2_2011\nprint(f\"Priority 1 crimes amounted to {round((pri1_2011\/total_2011)*100, 3)}%, priority 2 crimes amounted to {round((pri2_2011\/total_2011)*100, 3)}% in 2011.\")\nprint(\"-----------------------------------------------------------------------------------------------------------------------------------------\")\npri1_2012 = 41926\npri2_2012 = 145504\ntotal_2012 = pri1_2012 + pri2_2012\nprint(f\"Priority 1 crimes amounted to {round((pri1_2012\/total_2012)*100, 3)}%, priority 2 crimes amounted to {round((pri2_2012\/total_2012)*100, 3)}% in 2012.\")\nprint(\"-----------------------------------------------------------------------------------------------------------------------------------------\")\npri1_2013 = 43171\npri2_2013 = 144859\ntotal_2013 = pri1_2013 + pri2_2013\nprint(f\"Priority 1 crimes amounted to {round((pri1_2013\/total_2013)*100, 3)}%, priority 2 crimes amounted to {round((pri2_2013\/total_2013)*100, 3)}% in 2013.\")\nprint(\"-----------------------------------------------------------------------------------------------------------------------------------------\")\npri1_2014 = 42773\npri2_2014 = 144707\ntotal_2014 = pri1_2014 + pri2_2014\nprint(f\"Priority 1 crimes amounted to {round((pri1_2014\/total_2014)*100, 3)}% priority 2 crimes amounted to {round((pri2_2014\/total_2014)*100, 3)}% in 2014.\")\nprint(\"-----------------------------------------------------------------------------------------------------------------------------------------\")\npri1_2015 = 42418\npri2_2015 = 150162\ntotal_2015 = pri1_2015 + pri2_2015\nprint(f\"Priority 1 crimes amounted to {round((pri1_2015\/total_2015)*100, 3)}%, priority 2 crimes amounted to {round((pri2_2015\/total_2015)*100, 3)}% in 2015.\")\nprint(\"-----------------------------------------------------------------------------------------------------------------------------------------\")\npri1_2016 = 24555\npri2_2016 = 86272\ntotal_2016 = pri1_2016 + pri2_2016\nprint(f\"Priority 1 crimes amounted to {round((pri1_2016\/total_2016)*100, 3)}% and priority 2 crimes amounted to {round((pri2_2016\/total_2016)*100, 3)}%, for the first half of 2016.\")\nprint(\"-----------------------------------------------------------------------------------------------------------------------------------------\")","fda15535":"# Mean Priority count per Area\/Location\/Beat\ndef areaid_groupby():\n    for i in list_dfs:\n        print(i[i['Priority']!=0].groupby(['Area Id', 'Priority']).size())\n        print(' ')\nareaid_groupby()","72105d65":"fig, axes= plt.subplots(2, 3)\nfor i, d in enumerate(list_dfs):\n    ax = axes.flatten()[i]\n    dplot = d[['Area Id', 'Priority']].pivot_table(index='Area Id', columns=['Priority'], aggfunc=len)\n    dplot = (dplot.assign(total=lambda x: x.sum(axis=1))\n                  .sort_values('total', ascending=False)\n                  .head(10)\n                  .drop('total', axis=1))\n    dplot.plot.bar(ax=ax, figsize=(15, 7), stacked=True)\n    ax.set_title(f\"Plot of Priority 1 and 2 crimes within Area Id for {i+2011}\")\n    plt.tight_layout()","9df890f1":"# Value count for beats displayed by priority \nfor i in list_dfs:\n    print(i[i['Priority']!=0].groupby(['Beat', 'Priority']).size())\n    print(' ')","41d6c1b6":"fig, axes = plt.subplots(2, 3)\nfor i, d in enumerate(list_dfs):\n    ax = axes.flatten()[i]\n    dplot = d[['Beat', 'Priority']].pivot_table(index='Beat', columns=['Priority'], aggfunc=len)\n    dplot = (dplot.assign(total=lambda x: x.sum(axis=1))\n                  .sort_values('total', ascending=False)\n                  .head(10)\n                  .drop('total', axis=1))\n    dplot.plot.bar(ax=ax, figsize=(15, 7), stacked=True)\n    ax.set_title(f\"Top 10 Beats for {i+ 2011}\")\n    plt.tight_layout()","83762f2d":"# Top 20 most popular crimes across the data sets\ndf1 = df_2011['Incident Type Description'].value_counts()[:10]\ndf2 = df_2012['Incident Type Description'].value_counts()[:10]\ndf3 = df_2013['Incident Type Description'].value_counts()[:10]\ndf4 = df_2014['Incident Type Description'].value_counts()[:10]\ndf5 = df_2015['Incident Type Description'].value_counts()[:10]\ndf6 = df_2016['Incident Type Description'].value_counts()[:10]\nlist_df = [df1, df2, df3, df4, df5, df6]\nfig, axes = plt.subplots(2, 3)\nfor d, i in zip(list_df, range(6)):\n    ax=axes.ravel()[i];\n    ax.set_title(f\"Top 20 crimes in {i+2011}\")\n    d.plot.barh(ax=ax, figsize=(15, 7))\n    plt.tight_layout()","1ff63e55":"fig, axes = plt.subplots(2, 3)\nfor i, d in enumerate(list_dfs):\n    ax = axes.flatten()[i]\n    dplot = d[['Incident Type Id', 'Priority']].pivot_table(index='Incident Type Id', columns='Priority',aggfunc=len)\n    dplot = (dplot.assign(total=lambda x: x.sum(axis=1))\n                  .sort_values('total', ascending=False)\n                  .head(10)\n                  .drop('total', axis=1))\n    dplot.plot.barh(ax=ax, figsize=(15, 7), stacked=True)\n    ax.set_title(f\"Plot of Top 10 Incidents in {i+2011}\")\n    plt.tight_layout()","c01d2457":"# Total amount of pripority crimes per month\npri_count_list = [df_2011.groupby(['Priority', df_2011['Create Time'].dt.to_period('m')]).Priority.count(),\n                  df_2012.groupby(['Priority', df_2012['Create Time'].dt.to_period('m')]).Priority.count(),\n                  df_2013.groupby(['Priority', df_2013['Create Time'].dt.to_period('m')]).Priority.count(),\n                  df_2014.groupby(['Priority', df_2014['Create Time'].dt.to_period('m')]).Priority.count(),\n                  df_2015.groupby(['Priority', df_2015['Create Time'].dt.to_period('m')]).Priority.count(),\n                  df_2016.groupby(['Priority', df_2016['Create Time'].dt.to_period('m')]).Priority.count()]\nfig, axes = plt.subplots(2, 3)\nfor d, ax in zip(pri_count_list, axes.ravel()):\n    plot_df1 = d.unstack('Priority').loc[:, 1]\n    plot_df2 = d.unstack('Priority').loc[:, 2]\n    plot_df1.index = pd.PeriodIndex(plot_df1.index.tolist(), freq='m')\n    plot_df2.index = pd.PeriodIndex(plot_df2.index.tolist(), freq='m')\n    plt.suptitle('Visualisation of priorities by the year')\n    plot_df1.plot(ax=ax, legend=True, figsize=(15, 7))\n    plot_df2.plot(ax=ax, legend=True, figsize=(15, 7))","2c13d93b":"count = 2011\nx = []\nfor i in list_dfs:\n    i['Difference in hours'] = i['Closed Time'] - i['Create Time']\n    i['Difference in hours'] = i['Difference in hours']\/np.timedelta64(1, 'h')\n    mean_hours = round(i['Difference in hours'].mean(), 3)\n    x.append(mean_hours)\n    print(f\"Difference in hours for {count} is {mean_hours} with a reported {i.shape[0]} crimes.\")\n    count += 1\n","21f5d4a5":"### Time analysis","c07ab599":"Summing the amount of Priority 1 and 2 crimes per dataset we can see that there is an increase in both crimes.","37166431":"Two graphs to show the 'Indcident Type Decription' as well as it's Id. The first graph shows that 'Alarm Ringer' is by far the most reported crime, however in graph 2 we can see that only a small percentage of that is *priority 1*. All through the 6 datasets we can see that 'Battery\/242' is the highest reported *priority 1* crime.","8a1c9c49":"### Beat Analysis","0375bf51":"### Area ID analysis.","ba157586":"> ### Priority Analysis","7bf0f16e":"Crime seems to be at a stable rate throughout the datasets. The margin of difference in percentage is only slight throughout the 6 years observed.","8cb4e8f5":"# Analysis of Crime in Oakland of the datasets 2011-2016","0e36be6f":"### Importing libraries","b35790bf":"I have decided to focus on the Priority column within all datasets, and compare with other columns for analysis.","ff449cf1":"#### Import files","0b50a87e":"Amount of Priority crimes for all years observed:","97876329":"The Area Id's for each dataset have not been consistent with their category. To see the amount of crimes for each year split by priority, check below:","d2968513":"A small project to find details of crimes in Oakland, CA, over the datasets that span from 2011-2016. \n\nAs a bit of background information, there are some names of crimes within the dataset that may not be clear to people outside of the United States (such as myself), so it is useful to research the terminology within the datasets. For e.g. definitions for '*priority 1 & 2*' crimes are: \n\n*Priority 1 crime is said to be an urgent crime, for e.g. lights and sirens authorised, armed robbery, officer down etc.*\n\n*Priority 2 crime is said to be of less urgency, for e.g. lights and sirens authorised, but follow basic traffic rules.*\n\nWhilst evaluating the data I had also found points where I have diregarded for being unnecessary. These were things such as - while analysing priority crimes, a *prirority 0* had appeared in only 3 datasets of which had a count of less than 10 crimes. Where as *priority 1 & 2* had a count of a much larger number for e.g. 25000.\n\nA big point to make - the data set for 2016 is inconclusive and the last entry is in the middle of the year.","00c58550":"## Background information","3af132d1":"### First few rows of data for all data sets.","ae34bc27":"### Incident type description (Incident type id) analysis","c955588f":"The visualisation shows us that within each year, Priority 2 crimes seem to peak around July\/August time. Apart from in 2014 where there seemed to be a drop.The plot for 2016 shows an inconclusive graph since the dataset was only a 7 month long time span."}}