{"cell_type":{"5ec0e391":"code","68f1dd0f":"code","0b8cb4bd":"code","4ca69151":"code","d24ae7e8":"code","1c3dd827":"code","82bb3405":"code","f0b5db0b":"code","14cefded":"code","552ad717":"code","46f21b45":"code","68eccec0":"code","ca292879":"code","1f483441":"code","1fc32e15":"code","7907f1ad":"code","b5f5c2a6":"code","38ac27b1":"code","fb571c09":"markdown","dd7c2fb8":"markdown","58ff398b":"markdown","cd858c07":"markdown","987d8eb6":"markdown"},"source":{"5ec0e391":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","68f1dd0f":"from sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor","0b8cb4bd":"train = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_sub = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")","4ca69151":"categorical_cols = [col for col in train.columns if \"cat\" in col]\n\nnew_train = pd.get_dummies(train, columns=categorical_cols, prefix_sep=\"_\")\nnew_test = pd.get_dummies(test, columns=categorical_cols, prefix_sep=\"_\")\nnew_test = new_test.drop(\"id\", axis=1)","d24ae7e8":"new_train[\"kfold\"] = -1\n\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=47)\nfor k, (train_idx, valid_idx) in enumerate(kf.split(X=new_train)):\n    new_train.loc[valid_idx, \"kfold\"] = k","1c3dd827":"def train_test_data(df, fold):\n    x_train = df[df.kfold != fold].reset_index(drop=True)\n    x_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    y_train = x_train.target\n    y_valid = x_valid.target\n    \n    x_train = x_train.drop([\"id\", \"target\", \"kfold\"], axis=1)\n    x_valid = x_valid.drop([\"id\", \"target\", \"kfold\"], axis=1)\n    \n    return {\"x_train\": x_train,\n           \"y_train\": y_train,\n           \"x_valid\": x_valid,\n           \"y_valid\": y_valid}","82bb3405":"def n_trees_get_models():\n    models = dict()\n    n_trees = [10, 50, 100, 500, 1000]\n    for n in n_trees:\n        models[str(n)] = LGBMRegressor(n_estimators=n)\n    return models","f0b5db0b":"def n_depth_get_models():\n    models = dict()\n    for i in range(1,11):\n        models[str(i)] = LGBMRegressor(max_depth=i, num_leaves=2**i)\n    return models","14cefded":"def n_lr_get_models():\n    models = dict()\n    rates = [0.0001, 0.001, 0.01, 0.1, 1.0]\n    for r in rates:\n        key = '%.4f' % r\n        models[key] = LGBMRegressor(learning_rate=r)\n    return models","552ad717":"def n_boosting_types_get_models():\n    models = dict()\n    boosting_types = ['gbdt', 'dart', 'goss']\n    for t in boosting_types:\n        models[t] = LGBMRegressor(boosting_type=t)\n    return models","46f21b45":"for fold in range(5):\n    datasets = train_test_data(new_train, fold)\n    x_train = datasets[\"x_train\"]\n    y_train = datasets[\"y_train\"]\n    x_valid = datasets[\"x_valid\"]\n    y_valid = datasets[\"y_valid\"]\n    models = n_trees_get_models()\n    print(\"************ FOLD: \"+str(fold + 1)+\" ************\")\n    for name, model in models.items():\n        model.fit(x_train, y_train)\n        preds = model.predict(x_valid)\n        preds_test = model.predict(new_test)\n#         final_preds.append(preds_test)\n        rmse = mean_squared_error(y_valid, preds, squared=False)\n        print(\"For \"+str(name)+\" trees: \")\n        print(\"RMSE Error for fold\", fold + 1, \": \", rmse)","68eccec0":"for fold in range(5):\n    datasets = train_test_data(new_train, fold)\n    x_train = datasets[\"x_train\"]\n    y_train = datasets[\"y_train\"]\n    x_valid = datasets[\"x_valid\"]\n    y_valid = datasets[\"y_valid\"]\n    models = n_depth_get_models()\n    print(\"************ FOLD: \"+str(fold + 1)+\" ************\")\n    for name, model in models.items():\n        model.fit(x_train, y_train)\n        preds = model.predict(x_valid)\n        preds_test = model.predict(new_test)\n#         final_preds.append(preds_test)\n        rmse = mean_squared_error(y_valid, preds, squared=False)\n        print(\"For \"+str(name)+\" depth: \")\n        print(\"RMSE Error for fold\", fold + 1, \": \", rmse)","ca292879":"for fold in range(5):\n    datasets = train_test_data(new_train, fold)\n    x_train = datasets[\"x_train\"]\n    y_train = datasets[\"y_train\"]\n    x_valid = datasets[\"x_valid\"]\n    y_valid = datasets[\"y_valid\"]\n    models = n_lr_get_models()\n    print(\"************ FOLD: \"+str(fold + 1)+\" ************\")\n    for name, model in models.items():\n        model.fit(x_train, y_train)\n        preds = model.predict(x_valid)\n        preds_test = model.predict(new_test)\n#         final_preds.append(preds_test)\n        rmse = mean_squared_error(y_valid, preds, squared=False)\n        print(\"For learning rate: \"+str(name))\n        print(\"RMSE Error for fold\", fold + 1, \": \", rmse)","1f483441":"for fold in range(5):\n    datasets = train_test_data(new_train, fold)\n    x_train = datasets[\"x_train\"]\n    y_train = datasets[\"y_train\"]\n    x_valid = datasets[\"x_valid\"]\n    y_valid = datasets[\"y_valid\"]\n    models = n_boosting_types_get_models()\n    print(\"************ FOLD: \"+str(fold + 1)+\" ************\")\n    for name, model in models.items():\n        model.fit(x_train, y_train)\n        preds = model.predict(x_valid)\n        preds_test = model.predict(new_test)\n#         final_preds.append(preds_test)\n        rmse = mean_squared_error(y_valid, preds, squared=False)\n        print(\"For boosting type: \"+str(name))\n        print(\"RMSE Error for fold\", fold + 1, \": \", rmse)","1fc32e15":"new_train.head()","7907f1ad":"final_preds = []\nfor fold in range(5):\n    datasets = train_test_data(new_train, fold)\n    x_train = datasets[\"x_train\"]\n    y_train = datasets[\"y_train\"]\n    x_valid = datasets[\"x_valid\"]\n    y_valid = datasets[\"y_valid\"]\n    lgb = LGBMRegressor(n_estimators=500, \n                        max_depth=7, \n                        num_leaves=2**7, \n                        learning_rate=0.1, \n                        boosting_type='gbdt',\n                        random_state=47)\n    lgb.fit(x_train, y_train)\n    preds = lgb.predict(x_valid)\n    preds_test = lgb.predict(new_test)\n    final_preds.append(preds_test)\n    print(\"RMSE Error for fold\", fold + 1, \": \", mean_squared_error(y_valid, preds, squared=False))","b5f5c2a6":"predictions = np.mean(np.column_stack(final_preds), axis=1)","38ac27b1":"sample_sub.target = predictions\nsample_sub.to_csv(\"submission.csv\", index=False)","fb571c09":"## Checking for boosting type","dd7c2fb8":"## Checking for learning rate","58ff398b":"We can see that the optimized values for all the hyperparameters are:\n1. number of trees = 500\n2. depth of tree = 7\n3. learning rate = 0.1\n4. boosting type = gbdt\n\nSo, we will use these hyperparameters in our final training and predictions","cd858c07":"## Checking for number of trees","987d8eb6":"## Checking for depth of tree"}}