{"cell_type":{"a4fd4c0b":"code","f0a05cbd":"code","beec1f76":"code","e2ca21ca":"code","567c9fb3":"code","2f3f9d44":"code","90436d48":"code","de5342f8":"code","d7ade396":"code","b88cda45":"code","d2c94870":"code","a289a542":"code","537c6a45":"code","1d78de58":"code","0e917911":"code","05e8fb66":"code","660f7689":"code","0fa2dd0f":"code","6f0b681c":"code","28921822":"code","12637f1c":"code","421817fe":"code","b240d119":"code","9f6eab67":"code","5c83af80":"code","0de8c77d":"code","dc4f1f8f":"code","7119c629":"code","1374063e":"code","fb2de4f4":"code","aafd0729":"code","61999f00":"code","858cac28":"code","89ed297c":"code","7a7797f4":"code","0c010a0c":"code","90caf4fd":"code","d7082832":"code","411c58b2":"code","06d3f673":"markdown","a08ec77f":"markdown","afaeef41":"markdown","5205476f":"markdown","73e2aba2":"markdown","7d953045":"markdown","54a9633c":"markdown","565a9b4b":"markdown","8a0cbf00":"markdown","5787352d":"markdown","1d1d3393":"markdown","6805725d":"markdown","f0e139f4":"markdown","47874fff":"markdown","0e40b332":"markdown","61c7ea2c":"markdown","6c70782d":"markdown","7eb42bb6":"markdown","15b2b507":"markdown","34df683b":"markdown","beb292db":"markdown","1771f6bd":"markdown","7339eaad":"markdown","9b941a8c":"markdown","e48ced75":"markdown","37fafd6b":"markdown","f4898803":"markdown"},"source":{"a4fd4c0b":"import re, string \nimport pandas as pd \nimport numpy as np\nfrom time import time  \nimport re, itertools, random\nfrom collections import defaultdict\nimport spacy\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import TruncatedSVD\nfrom nltk.corpus import stopwords\nSTOPWORDS = set(stopwords.words('spanish'))\nfrom gensim.models import Word2Vec\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\nfrom scipy.spatial.distance import cdist\n# uncomment for first run, before put on Internet kernel (settings)\n!python -m spacy download es_core_news_md\n!python -m spacy link es_core_news_md es_md\nnlp = spacy.load('es_md', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed","f0a05cbd":"df = pd.read_csv(\"..\/input\/tweets-municipalidad-asuncion\/tweets_municipalidad.csv\")","beec1f76":"df.head(10)","e2ca21ca":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n    if text is None:\n        return ''\n    text = str(text).replace(\"nan\",'').lower()\n    text = re.sub(r'\\[.*?\\]', '', text)\n    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub(r'\\w*\\d\\w*', '', text)\n    # Remove a sentence if it is only one word long\n    if len(text) >= 2:\n        return ' '.join(word for word in text.split() if word not in STOPWORDS)\n\ndf_clean = pd.DataFrame(df.tweet.apply(lambda x: clean_text(x)))\ndf_clean = df_clean.dropna()\ndf_clean = df_clean.reset_index(drop=True)","567c9fb3":"df_clean.head(10)","2f3f9d44":"def lemmatizer(text):        \n    sent = []\n    doc = nlp(text)\n    for word in doc:\n        sent.append(word.lemma_)\n    return \" \".join(sent)\n\ndf_clean[\"text_lemmatize\"] = df_clean.apply(lambda x: lemmatizer(x['tweet']), axis=1)","90436d48":"df_clean.head(10)","de5342f8":"df_clean['text_lemmatize_clean'] = df_clean['text_lemmatize'].str.replace('-PRON-', '')","d7ade396":"sentences = [row.split() for row in df_clean['text_lemmatize_clean']]\nword_freq = defaultdict(int)\nfor sent in sentences:\n    for i in sent:\n        word_freq[i] += 1\nlen(word_freq)","b88cda45":"sorted(word_freq, key=word_freq.get, reverse=True)[:10]","d2c94870":"# min_count: minimum number of occurrences of a word in the corpus to be included in the model.\n# window: the maximum distance between the current and predicted word within a sentence.\n# size: the dimensionality of the feature vectors\n# workers: I know kaggle system is having 4 cores without gpu and 2 with gpu, \nw2v_model = Word2Vec(min_count=100,\n                     window=3,\n                     size=200,\n                     workers=4)","a289a542":"# this line of code to prepare the model vocabulary\nw2v_model.build_vocab(sentences)","537c6a45":"# train word vectors\nw2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=w2v_model.iter)","1d78de58":"len(w2v_model.wv.vocab)","0e917911":"# As we do not plan to train the model any further, \n# we are calling init_sims(), which will make the model much more memory-efficient\nw2v_model.init_sims(replace=True)","05e8fb66":"'dengue' in w2v_model.wv.vocab","660f7689":"w2v_model.wv.most_similar(positive=['dengue','mosquito','criadero'])\n#w2v_model.wv.most_similar(negative=['dengue','mosquito','criadero','minga'])","0fa2dd0f":"# how similar are these two words to each other \nw2v_model.wv.similarity('mosquito','dengue')","6f0b681c":"w2v_model.wv.doesnt_match(['dengue','mosquito','criadero','minga'])","28921822":"w2v_model.wv.most_similar(positive=['cateura','dengue'], negative=['mburicao'], topn=3)","12637f1c":"def tsne_plot(model, perplexity=10, n_iter=1000):\n    \"Create TSNE model and plot it\"\n    labels = []\n    tokens = []\n\n    i = 0\n    for word in sorted(model.wv.vocab.keys(), reverse=True):\n        tokens.append(model[word])\n        labels.append(word)\n        i+=1\n        if i >= 499:\n            break\n        \n    tsne_model = TSNE(n_components=2, init='pca', random_state=0, perplexity=perplexity, n_iter=n_iter)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n    \n    x_min, x_max = np.min(new_values, 0), np.max(new_values, 0)\n    X = (new_values - x_min) \/ (x_max - x_min)\n    shown_images = np.array([[1., 1.]])  # just something big\n    \n    plt.figure(figsize=(20, 20)) \n    for i in range(len(x)):\n        dist = np.sum((X[i] - shown_images) ** 2, 1)\n        '''if np.min(dist) < 1e-3:\n            # don't show points that are too close\n            continue'''\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(3, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()","421817fe":"# Use t-SNE to represent high-dimensional data \n# and the underlying relationships between vectors in a lower-dimensional space.\ntsne_plot(w2v_model,40,5000)","b240d119":"# First get the embeddings into a matrix\nembedding_size=200\nembeddings = np.zeros((len(w2v_model.wv.index2word), embedding_size))\nfor i in range(0, len(w2v_model.wv.index2word)):\n    w = w2v_model.wv.index2word[i]\n    embeddings[i] = w2v_model.wv[w]","9f6eab67":"svd = TruncatedSVD(n_components=2, algorithm='randomized', n_iter=500, random_state=101)\nembeddings_2d_projection = svd.fit_transform(embeddings)","5c83af80":"# Train a K-means cluster model with 6 clusters\nn_clusters = 6\nembedding_cluster_model = KMeans(n_clusters=n_clusters, random_state=0).fit(embeddings)","0de8c77d":"centroid_embedding_nearest_words = []\nfor centroid_embedding in embedding_cluster_model.cluster_centers_:\n    centroid_embedding_nearest_words.append(\n        np.argsort([i[0] for i in cdist(embeddings, np.array([centroid_embedding]), \"euclidean\")])[0:10]\n    )","dc4f1f8f":"plt.figure(figsize=(10,10))\ncolors = itertools.cycle([\"b\",\"g\",\"r\",\"c\",\"m\",\"y\",\"k\",\"w\"])\nc = 0\nfor word_indices in centroid_embedding_nearest_words:\n    clr = next(colors)\n    plt.scatter(\n        embeddings_2d_projection[word_indices,0],\n        embeddings_2d_projection[word_indices,1],\n        color=clr,\n        label=\"Cluster \" + str(c)\n    )\n    for ix in word_indices:\n        x, y = embeddings_2d_projection[ix,:]\n        plt.annotate(w2v_model.wv.index2word[ix], (x, y))\n    c+=1\nplt.legend(loc='lower left')\nplt.show()","7119c629":"def tsnescatterplot(model, word, list_names):\n    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n    its list of most similar words, and a list of words.\n    \"\"\"\n    arrays = np.empty((0, 200), dtype='f')\n    word_labels = [word]\n    color_list  = ['red']\n\n    # adds the vector of the query word\n    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n    \n    # gets list of most similar words\n    close_words = model.wv.most_similar([word])\n    \n    # adds the vector for each of the closest words to the array\n    for wrd_score in close_words:\n        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n        word_labels.append(wrd_score[0])\n        color_list.append('blue')\n        arrays = np.append(arrays, wrd_vector, axis=0)\n    \n    # adds the vector for each of the words from list_names to the array\n    for wrd in list_names:\n        wrd_vector = model.wv.__getitem__([wrd])\n        word_labels.append(wrd)\n        color_list.append('green')\n        arrays = np.append(arrays, wrd_vector, axis=0)\n        \n    # Reduces the dimensionality from 200 to 12 dimensions with PCA\n    reduc = PCA(n_components=12).fit_transform(arrays)\n    \n    # Finds t-SNE coordinates for 2 dimensions\n    np.set_printoptions(suppress=True)\n    \n    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n    \n    # Sets everything up to plot\n    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n                       'y': [y for y in Y[:, 1]],\n                       'words': word_labels,\n                       'color': color_list})\n    \n    fig, _ = plt.subplots()\n    fig.set_size_inches(9, 9)\n    \n    # Basic plot\n    p1 = sns.regplot(data=df,\n                     x=\"x\",\n                     y=\"y\",\n                     fit_reg=False,\n                     marker=\"o\",\n                     scatter_kws={'s': 40,\n                                  'facecolors': df['color']\n                                 }\n                    )\n    \n    # Adds annotations one by one with a loop\n    for line in range(0, df.shape[0]):\n         p1.text(df[\"x\"][line],\n                 df['y'][line],\n                 '  ' + df[\"words\"][line].title(),\n                 horizontalalignment='left',\n                 verticalalignment='bottom', size='medium',\n                 color=df['color'][line],\n                 weight='normal'\n                ).set_size(15)\n\n    \n    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n            \n    plt.title('t-SNE visualization for {}'.format(word.title()))","1374063e":"tsnescatterplot(w2v_model, 'dengue', sorted(word_freq, key=word_freq.get, reverse=True)[:10])","fb2de4f4":"tsnescatterplot(w2v_model, 'dengue', ['mburicao','cateura'])","aafd0729":"tsnescatterplot(w2v_model, 'dengue', [i[0] for i in w2v_model.wv.most_similar(negative=['dengue'])])","61999f00":"tsnescatterplot(w2v_model, \"dengue\", [t[0] for t in w2v_model.wv.most_similar(positive=[\"dengue\"], topn=20)][10:])","858cac28":"import fasttext\n!wget https:\/\/dl.fbaipublicfiles.com\/fasttext\/supervised-models\/lid.176.bin\nmodel = fasttext.load_model('lid.176.bin')\nimport langid","89ed297c":"def set_language(text):\n    lang = 'unknown'\n    try:\n        lang1 = model.predict(text, k=2) # against two languages\n        lang1 = lang1[0][0].replace('__label__','') if lang1[0][0].replace('__label__','') in ['en','es','gn'] else lang1[0][0].replace('__label__','') if lang1[1][0]>=0.7 else 'undefined' #fasttext            \n    except:\n        lang1 = lang \n        \n    # priority fasttext and es en gn\n    if text: #example -> lang1:es, lang2:pt\n        if (lang1=='gn' or lang1=='es' or lang1=='en'):\n            return lang1\n        else:\n            try:\n                lang2 = langid.classify(text)[0] \n            except:\n                lang2 = lang\n            if (lang2=='gn' or lang2=='es' or lang2=='en'):\n                return lang2\n            elif (lang1==lang2):\n                return lang1\n            else:\n                return lang1","7a7797f4":"set_language('Tapeuah\u1ebdpor\u00e3ite Vikipet\u00e3me') # -> https:\/\/gn.wikipedia.org\/wiki\/Ape","0c010a0c":"df_clean.loc[4,'language'] = \"\"\n\ndf_clean[\"language\"]=df_clean[\"tweet\"].apply(lambda text: set_language(text))","90caf4fd":"plt.figure(num=None, figsize=(20, 16), dpi=300, facecolor='w', edgecolor='k')\nax = sns.countplot(y=\"language\", data=df_clean)\nax = ax.set_title('Languages count')\nplt.show()","d7082832":"for index, row in df_clean.iterrows():\n    if(row['language']=='gn'):\n        print(row['tweet'], row['language'])","411c58b2":"for index, row in df_clean.iterrows():\n    if(row['language'] in ['unknown','undefined']):\n        if('__label__gn' in model.predict(row['tweet'], k=10)[0]):\n            print(row['tweet'], row['language'])","06d3f673":"## Lookup some words: *dengue*","a08ec77f":"Paraguay are an official bilingual (Spanish-Guaran\u00ed) country with some use of *spanglish* and many *anglicism*, I inspect the languages presented in this dataset by two popular language detectors.","afaeef41":"You can report any issues with spanish lemmatizer to [spaCy.io](https:\/\/spacy.io\/), e.g., this [post](https:\/\/github.com\/explosion\/spaCy\/issues\/2710).","5205476f":"Check interesting relations...","73e2aba2":"[Asunci\u00f3n](https:\/\/en.wikipedia.org\/wiki\/Asunci%C3%B3n) is located on the left bank of the Paraguay River, almost at the confluence of this river with the River Pilcomayo, on the South American continent ([Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Asunci%C3%B3n)).","7d953045":"**Top 10 most frequent words**","54a9633c":"> Languages in the corpus","565a9b4b":"> Dengue is a mosquito-borne viral disease that has rapidly spread in all regions of WHO in recent years. Dengue virus is transmitted by female mosquitoes mainly of the species Aedes aegypti and, to a lesser extent, Ae. albopictus. These mosquitoes are also vectors of chikungunya, yellow fever and Zika viruses. Dengue is widespread throughout the tropics, with local variations in risk influenced by rainfall, temperature, relative humidity and unplanned rapid urbanization. [For more info](https:\/\/www.who.int\/news-room\/fact-sheets\/detail\/dengue-and-severe-dengue)","8a0cbf00":"## Load data & packages","5787352d":"## Because it's fun: language detection","1d1d3393":"### References\n\n1. https:\/\/towardsdatascience.com\/understanding-word2vec-embedding-in-practice-3e9b8985953\n1. https:\/\/www.kaggle.com\/pierremegret\/gensim-word2vec-tutorial\n1. https:\/\/www.kaggle.com\/niclasko\/mercari-item-description-word2vec-embeddings-eda\n1. https:\/\/www.samyzaf.com\/ML\/nlp\/nlp.html\n1. https:\/\/radimrehurek.com\/gensim\/apiref.html\n1. https:\/\/fasttext.cc\/docs\/en\/language-identification.html","6805725d":"### Relations: *dengue*","f0e139f4":"*What does not match?*","47874fff":"## Text pre-processing","0e40b332":"Test language identification...","61c7ea2c":"# Word2vec Similarity, KMeans Clustering and Fasttext Language detection in tweets about Asunci\u00f3n (Paraguay) city council","6c70782d":"Which word is to *cateura* (garbage dump) as *dengue* is to *mburicao* (stream)?","7eb42bb6":"**Most similar words**","15b2b507":"Undefined are some spanish tweets and, of course, [**jopar\u00e1**](https:\/\/en.wikipedia.org\/wiki\/Jopara_language) (Guaran\u00ed-Spanish code-switching): \n> Jopar\u00e1 is a colloquial form of Guarani spoken in Paraguay which uses a number of Spanish loan words ([Lustig, Wolf](https:\/\/www.staff.uni-mainz.de\/lustig\/guarani\/art\/jopara.pdf)).","34df683b":"**10 Most similar words vs. 10 Most dissimilar**","beb292db":"What are people talking about?","1771f6bd":"### Clustering","7339eaad":"**10 Most similar words vs. 11th to 20th Most similar words**","9b941a8c":"## Data visualization","e48ced75":"## Modeling","37fafd6b":"**10 Most similar words vs. Top 10 Frequent words**","f4898803":"Another option is [tweet-preprocessor](https:\/\/pypi.org\/project\/tweet-preprocessor\/)..."}}