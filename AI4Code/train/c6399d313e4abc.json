{"cell_type":{"522d9893":"code","89b649c4":"code","25e1ec46":"code","a065254d":"code","52416393":"code","dba7f5fd":"code","0740c2fe":"code","a27fc647":"code","9a081919":"code","71cb15fc":"code","79d19594":"code","b053e77d":"code","47077cee":"code","da4ceae2":"code","c5ef0e58":"code","12c0cc94":"code","0a9bef5f":"code","eaf09813":"code","e077f7f6":"code","656dad6a":"code","1bc37b38":"code","5c4efe51":"code","c781486b":"code","ebab1822":"code","4ea69e80":"code","f33d44b0":"code","319378c1":"code","cd7acc8e":"code","8efbeac9":"code","e9fe4bda":"code","8f71cdc4":"code","368eb925":"code","774d310e":"code","025d8add":"code","eacd2d3d":"code","ee30edb1":"code","16187e8b":"code","95d4e10f":"code","455b5cc9":"code","fa171636":"code","7ee8c672":"code","49e541cc":"code","ec0c7fea":"code","c612c097":"code","1e3968a1":"code","0fb9dad3":"code","fca97d05":"code","d759ca47":"code","a20471b7":"code","471803d5":"code","8947d26f":"code","567e8846":"code","2ab01a59":"code","ee05fb41":"code","f5521900":"code","acbe5943":"code","8ad4c86e":"code","5e58303f":"code","e1a4b5b7":"code","331dc180":"code","b81f0d12":"code","f941e3de":"code","8e246736":"code","151d5eb4":"code","6d5847f5":"code","daf87ff8":"code","2a43bf81":"code","31209dcf":"code","d3fe5793":"code","35a3ed73":"code","c14fe94b":"code","ba06d603":"code","20edfe23":"code","520887f3":"code","0179ee73":"code","f08607bb":"code","39e107bf":"markdown","2d252d52":"markdown","2eff01ac":"markdown","36d0ba74":"markdown","76bdaae1":"markdown","f68f4e6f":"markdown","52bea244":"markdown","d930792d":"markdown","7e5204a3":"markdown","bf8c3341":"markdown","2bcec23e":"markdown","d4b0725d":"markdown","b8fc98a6":"markdown","ba1c10d2":"markdown","aa206e1c":"markdown","dd3ae516":"markdown","038c919a":"markdown","f781b917":"markdown","52a9dbd0":"markdown","ed7367bf":"markdown","e8f6236c":"markdown","7fb38fe5":"markdown","a3067b07":"markdown","e6825e9f":"markdown","db827e46":"markdown","d2a5211e":"markdown","8764b3c4":"markdown","3c0eb644":"markdown","0e1f6118":"markdown","c4d8aabe":"markdown","16e04c08":"markdown","e2f5dda4":"markdown","d7c6875a":"markdown","f0e5b13a":"markdown","b0cc90cf":"markdown","e845f36e":"markdown","e4a12653":"markdown","34313c01":"markdown","8519976a":"markdown","704d1c1b":"markdown","8c74734a":"markdown","c5a0a4d1":"markdown","22b1b860":"markdown","70aacf57":"markdown","77d991da":"markdown","d3ef7bdc":"markdown","686bd33a":"markdown","3ffa17ad":"markdown","22ba5cb6":"markdown","ad127d51":"markdown","11a08bd2":"markdown","4c625328":"markdown","c5c0413e":"markdown","9e1df01e":"markdown","80f8d6ea":"markdown","1d80447d":"markdown","a44aa83f":"markdown","e569e7e9":"markdown","61b3759b":"markdown","4d08f6ed":"markdown","971a3670":"markdown","2fdfd1ab":"markdown","02a8acbe":"markdown"},"source":{"522d9893":"debug = False","89b649c4":"debug2 = False","25e1ec46":"import numpy as np \nimport pandas as pd \nimport os\n       \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold","a065254d":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt \n\nimport transformers\nimport random\n\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nscaler = torch.cuda.amp.GradScaler() # GPU\u3067\u306e\u9ad8\u901f\u5316\u3002\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # cpu\u304cgpu\u304b\u3092\u81ea\u52d5\u5224\u65ad\ndevice","52416393":"SEED = 508\n\ndef random_seed(SEED):\n    \n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n\nrandom_seed(SEED)","dba7f5fd":"train = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntrain.head(3)","0740c2fe":"# excerpt\u306e0\u756a\u76ee\u306e\u6587\u7ae0\u4f8b\ntrain.excerpt[0]","a27fc647":"test = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\ntest","9a081919":"sample = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\nsample","71cb15fc":"# kaggle offline mode : submit\u306finternet\u304coffline\u306a\u306e\u3067\u3001offline\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\u516c\u958b\u3055\u308c\u3066\u3044\u308bdataset\u304b\u3089\u3082\u3063\u3066\u304d\u307e\u3059\u3002\ntokenizer = transformers.BertTokenizer.from_pretrained(\"..\/input\/bert-base-uncased\")\n\n# local PC\u306a\u3069online\u3067\u4f7f\u7528\u3059\u308b\u5834\u5408\u306f\u3053\u3061\u3089\u3092\u4f7f\u7528\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n# tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","79d19594":"test_s = train[\"excerpt\"].iloc[0]\ntest_s","b053e77d":"result1 = tokenizer.encode_plus(test_s)\nresult1","47077cee":"tokenizer.decode(result1[\"input_ids\"])","da4ceae2":"if debug2:\n    sen_length = []\n\n    for sentence in tqdm(train[\"excerpt\"]):\n\n        token_words = tokenizer.encode_plus(sentence)[\"input_ids\"]\n        sen_length.append(len(token_words))\n\n    print('maxlenth of all sentences are  ', max(sen_length))","c5ef0e58":"test_s","12c0cc94":"len(test_s.split(\" \"))","0a9bef5f":"result2 = tokenizer.encode_plus(\n    test_s,\n    add_special_tokens = True, # [CLS],[SEP]\u3092\u5165\u308c\u308b\u304b\n    max_length = 314, # padding\u3068trancation(\u5207\u308a\u51fa\u3057)\u3092\u4f7f\u3063\u3066\u3001\u5358\u8a9e\u6570\u3092\u305d\u308d\u3048\u308b\n    pad_to_max_length = True, # \u30d6\u30e9\u30f3\u30af\u7b87\u6240\u306b[PAD]\u3092\u5165\u308c\u308b\n    \n    truncation = True # \u5207\u308a\u51fa\u3057\u6a5f\u80fd\u3002\u4f8b\u3048\u3070max_length10\u3068\u304b\u306b\u3059\u308b\u3068\u3001\u6700\u521d\u306e10\u6587\u5b57\u3060\u3051\u306b\u3057\u3066\u304f\u308c\u308b\u6a5f\u80fd\u3002\u5165\u308c\u306a\u3044\u3068\u6012\u3089\u308c\u305f\u306e\u3067\u3001\u5165\u308c\u3066\u304a\u304f\n)","eaf09813":"result2","e077f7f6":"tokenizer.decode(result2[\"input_ids\"])","656dad6a":"result3 = tokenizer.encode_plus(\n    test_s,\n    add_special_tokens = True, # [CLS],[SEP]\u3092\u5165\u308c\u308b\u304b\n    max_length = 10, # padding\u3068trancation(\u5207\u308a\u51fa\u3057)\u3092\u4f7f\u3063\u3066\u3001\u5358\u8a9e\u6570\u3092\u305d\u308d\u3048\u308b\n    pad_to_max_length = True, # \u30d6\u30e9\u30f3\u30af\u7b87\u6240\u306b[PAD]\u3092\u5165\u308c\u308b\n    \n    truncation = True # \u5207\u308a\u51fa\u3057\u6a5f\u80fd\u3002\u4f8b\u3048\u3070max_length10\u3068\u304b\u306b\u3059\u308b\u3068\u3001\u6700\u521d\u306e10\u6587\u5b57\u3060\u3051\u306b\u3057\u3066\u304f\u308c\u308b\u6a5f\u80fd\u3002\u5165\u308c\u306a\u3044\u3068\u6012\u3089\u308c\u305f\u306e\u3067\u3001\u5165\u308c\u3066\u304a\u304f\n)","1bc37b38":"result3","5c4efe51":"max_sens = 314","c781486b":"train = train.sort_values(\"target\").reset_index(drop=True)\ntrain","ebab1822":"train[\"kfold\"] = train.index % 5","4ea69e80":"train","f33d44b0":"p_train = train[train[\"kfold\"]!=0].reset_index(drop=True)\np_valid = train[train[\"kfold\"]==0].reset_index(drop=True)","319378c1":"class BERTDataSet(Dataset):\n    \n    def __init__(self,sentences,targets):\n        \n        self.sentences = sentences\n        self.targets = targets\n        \n    def __len__(self):\n        \n        return len(self.sentences)\n    \n    def __getitem__(self,idx):\n        \n        sentence = self.sentences[idx]\n        \n        bert_sens = tokenizer.encode_plus(\n                                sentence,\n                                add_special_tokens = True, \n                                max_length = max_sens, # \u4e0a\u3067314\u306b\u8a2d\u5b9a\u3057\u3066\u3044\u307e\u3059\n                                pad_to_max_length = True, \n                                return_attention_mask = True)\n\n        ids = torch.tensor(bert_sens['input_ids'], dtype=torch.long)\n        mask = torch.tensor(bert_sens['attention_mask'], dtype=torch.long)\n        token_type_ids = torch.tensor(bert_sens['token_type_ids'], dtype=torch.long)\n     \n            \n        target = torch.tensor(self.targets[idx],dtype=torch.float)\n        \n        return {\n                'ids': ids,\n                'mask': mask,\n                'token_type_ids': token_type_ids,\n                'targets': target\n            }","cd7acc8e":"train_dataset = BERTDataSet(p_train[\"excerpt\"],p_train[\"target\"])\nvalid_dataset = BERTDataSet(p_valid[\"excerpt\"],p_valid[\"target\"])","8efbeac9":"train_dataset[0]","e9fe4bda":"train_batch = 16\nvalid_batch = 32","8f71cdc4":"train_dataloader = DataLoader(train_dataset,batch_size=train_batch,shuffle = True,num_workers=8,pin_memory=True)\nvalid_dataloader = DataLoader(valid_dataset,batch_size=valid_batch,shuffle = False,num_workers=8,pin_memory=True)","368eb925":"for a in train_dataloader:\n    print(a)\n    break","774d310e":"model = transformers.BertForSequenceClassification.from_pretrained(\"..\/input\/bert-base-uncased\",num_labels=1)","025d8add":"model.to(device)\nmodel.train()","eacd2d3d":"for a in train_dataloader:\n    ids = a[\"ids\"].to(device)\n    mask = a[\"mask\"].to(device)\n    #tokentype = a[\"token_type_ids\"].to(device)\n    \n    output = model(ids,mask)\n    break","ee30edb1":"output","16187e8b":"output[\"logits\"]","95d4e10f":"output[\"logits\"].shape","455b5cc9":"output[\"logits\"].squeeze(-1)","fa171636":"output[\"logits\"].squeeze(-1).shape","7ee8c672":"output = output[\"logits\"].squeeze(-1).shape","49e541cc":"from transformers import AdamW\nLR=2e-5\noptimizer = AdamW(model.parameters(), LR,betas=(0.9, 0.999), weight_decay=1e-2) ","ec0c7fea":"from transformers import get_linear_schedule_with_warmup\n\n\nepochs = 20\n\nif debug:\n    epochs = 1\n\ntrain_steps = int(len(p_train)\/train_batch*epochs)\nprint(train_steps)\n\nnum_steps = int(train_steps*0.1)\n\nscheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)","c612c097":"if debug2:\n\n\n\n    le=[]\n    train_dataloader = DataLoader(train_dataset,batch_size=train_batch,shuffle = True,num_workers=8,pin_memory=True)\n\n    for b in tqdm(range(epochs)):\n\n        for a in train_dataloader:\n            le.append(scheduler.get_last_lr())\n            scheduler.step()\n\n    x = np.arange(len(le))\n    plt.plot(x,le)","1e3968a1":"def loss_fn(output,target):\n    return torch.sqrt(nn.MSELoss()(output,target))","0fb9dad3":"def training(\n    train_dataloader,\n    model,\n    optimizer,\n    scheduler\n):\n    \n    model.train()\n    torch.backends.cudnn.benchmark = True\n\n    allpreds = []\n    alltargets = []\n\n    for a in train_dataloader:\n\n        losses = []\n\n        optimizer.zero_grad()\n\n        with torch.cuda.amp.autocast():\n\n            ids = a[\"ids\"].to(device,non_blocking=True) # non_blocking=True\u3067Pinned Memory\u304b\u3089GPU\u306b\u8ee2\u9001\u4e2d\u3082CPU\u304c\u52d5\u4f5c\u3067\u304d\u308b\u3089\u3057\u3044\u3002\n            mask = a[\"mask\"].to(device,non_blocking=True)\n            tokentype = a[\"token_type_ids\"].to(device,non_blocking=True)\n\n            output = model(ids,mask)\n            output = output[\"logits\"].squeeze(-1)\n\n            target = a[\"targets\"].to(device,non_blocking=True)\n\n            loss = loss_fn(output,target)\n\n\n            # \u30b9\u30b3\u30a2\u7fd2\u5f97\u7528\n            losses.append(loss.item())\n            allpreds.append(output.detach().cpu().numpy())\n            alltargets.append(target.detach().squeeze(-1).cpu().numpy())\n\n        scaler.scale(loss).backward() # \u30ed\u30b9\u306e\u30d0\u30c3\u30af\u30ef\u30fc\u30c9\n        scaler.step(optimizer) # \u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u30fc\u306e\u66f4\u65b0\n        scaler.update() # \u30b9\u30b1\u30fc\u30e9\u30fc\u306e\u66f4\u65b0\n        \n        del loss # \u3053\u3053\u3067loss\u3092\u6d88\u3057\u305f\u65b9\u304cGPU\u306e\u30e1\u30e2\u30ea\u306e\u7121\u99c4\u306a\u90e8\u5206\u3092\u6d88\u305b\u308b\u3089\u3057\u3044\u3002https:\/\/tma15.github.io\/blog\/2020\/08\/22\/pytorch%E4%B8%8D%E8%A6%81%E3%81%AB%E3%81%AA%E3%81%A3%E3%81%9F%E8%A8%88%E7%AE%97%E3%82%B0%E3%83%A9%E3%83%95%E3%82%92%E5%89%8A%E9%99%A4%E3%81%97%E3%81%A6%E3%83%A1%E3%83%A2%E3%83%AA%E3%82%92%E7%AF%80%E7%B4%84\/\n\n        scheduler.step() # \u5b66\u7fd2\u7387\u306e\u66f4\u65b0\n\n        # \u30c7\u30fc\u30bf\u30ed\u30fc\u30c0\u30fc\u306e\u5408\u4f75\n\n    allpreds = np.concatenate(allpreds)\n    alltargets = np.concatenate(alltargets)\n\n    # \u30ed\u30b9\u95a2\u6570\u306f\u30e2\u30cb\u30bf\u30fc\u7528\u3060\u3051\u3069\u3001\u4e00\u5fdc\u4fdd\u7ba1\n\n    losses = np.mean(losses)\n\n    # Score with rmse\n    train_rme_loss = np.sqrt(mean_squared_error(alltargets,allpreds))\n\n    return losses,train_rme_loss","fca97d05":"if debug2:\n    train_dataloader = DataLoader(train_dataset,batch_size=train_batch,shuffle = True,num_workers=4,pin_memory=True)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)\n\n    losses,train_rme_loss = training(train_dataloader,model,optimizer,scheduler)\n    \n    print(losses,train_rme_loss)","d759ca47":"def validating(\n    valid_dataloader,\n    model\n):\n    \n    model.eval()\n\n    allpreds = []\n    alltargets = []\n\n    for a in valid_dataloader:\n\n        losses = []\n\n        with torch.no_grad():\n\n            ids = a[\"ids\"].to(device)\n            mask = a[\"mask\"].to(device)\n            tokentype = a[\"token_type_ids\"].to(device)\n\n            output = model(ids,mask)\n            output = output[\"logits\"].squeeze(-1)\n\n            target = a[\"targets\"].to(device)\n\n            loss = loss_fn(output,target)\n\n\n            # \u63a1\u70b9\u7528\u306b\n            losses.append(loss.item())\n            allpreds.append(output.detach().cpu().numpy())\n            alltargets.append(target.detach().squeeze(-1).cpu().numpy())\n            \n            del loss\n\n\n    # dataloader\u5206\u3092\u7d50\u5408\n\n    allpreds = np.concatenate(allpreds)\n    alltargets = np.concatenate(alltargets)\n\n    # loss\u306f\u4f7f\u308f\u306a\u3044\u3051\u3069\u4e00\u5fdc\u56de\u53ce\n\n    losses = np.mean(losses)\n\n    # rmse\u3067\u30b9\u30b3\u30a2\u51fa\u3057\n    valid_rme_loss = np.sqrt(mean_squared_error(alltargets,allpreds))\n\n    return allpreds,losses,valid_rme_loss","a20471b7":"if debug2:\n    allpreds,losses,valid_rme_loss = validating(valid_dataloader,model)\n    print(allpreds[:3])\n    print(losses)\n    print(valid_rme_loss)","471803d5":"if debug2 == False:\n    for a in range(epochs):\n        for b in train_dataloader:\n            break\n\n    losses,train_rme_loss = training(train_dataloader,model,optimizer,scheduler)\n\n    for a in valid_dataloader:\n        break","8947d26f":"# initializing the data\n\np_train = train[train[\"kfold\"]!=0].reset_index(drop=True)\np_valid = train[train[\"kfold\"]==0].reset_index(drop=True)\n\n\ntrain_dataset = BERTDataSet(p_train[\"excerpt\"],p_train[\"target\"])\nvalid_dataset = BERTDataSet(p_valid[\"excerpt\"],p_valid[\"target\"])\n\ntrain_dataloader = DataLoader(train_dataset,batch_size=train_batch,shuffle = True,num_workers=4,pin_memory=True)\nvalid_dataloader = DataLoader(valid_dataset,batch_size=valid_batch,shuffle = False,num_workers=4,pin_memory=True)\n\nmodel = transformers.BertForSequenceClassification.from_pretrained(\"..\/input\/bert-base-uncased\",num_labels=1)\n\nmodel.to(device)\nLR=2e-5\noptimizer = AdamW(model.parameters(), LR,betas=(0.9, 0.999), weight_decay=1e-2) # AdamW optimizer\n\ntrain_steps = int(len(p_train)\/train_batch*epochs)\n\nnum_steps = int(train_steps*0.1)\n\nscheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)","567e8846":"trainlosses = []\nvallosses = []\nbestscore = None\n\ntrainscores = []\nvalidscores = []\n\nfor epoch in tqdm(range(epochs)):\n    \n    print(\"---------------\" + str(epoch) + \"start-------------\")\n    \n    trainloss,trainscore = training(train_dataloader,model,optimizer,scheduler)\n    \n    trainlosses.append(trainloss)\n    trainscores.append(trainscore)\n    \n    print(\"trainscore is \" + str(trainscore))\n    \n    preds,validloss,valscore=validating(valid_dataloader,model)\n    \n    vallosses.append(validloss)\n    validscores.append(valscore)\n\n    \n    print(\"valscore is \" + str(valscore))\n    \n    if bestscore is None:\n        bestscore = valscore\n        \n        print(\"Save first model\")\n        \n        state = {\n                        'state_dict': model.state_dict(),\n                        'optimizer_dict': optimizer.state_dict(),\n                        \"bestscore\":bestscore\n                    }\n            \n\n        torch.save(state, \"model0.pth\")\n        \n    elif bestscore > valscore:\n        \n        bestscore = valscore\n        \n        print(\"found better point\")\n        \n        state = {\n                        'state_dict': model.state_dict(),\n                        'optimizer_dict': optimizer.state_dict(),\n                        \"bestscore\":bestscore\n                    }\n            \n\n        torch.save(state, \"model0.pth\")\n        \n    else:\n        pass\n    ","2ab01a59":"plt.scatter(p_valid[\"target\"],preds)","ee05fb41":"x = np.arange(epochs)\nplt.plot(x,trainlosses)\nplt.plot(x,vallosses)","f5521900":"x = np.arange(epochs)\nplt.plot(x,trainscores)\nplt.plot(x,validscores)","acbe5943":"# \u4ed6\u306ek-fold\u7528\nbestscores = []\nbestscores.append(bestscore)","8ad4c86e":"for fold in range(1,5):\n    \n\n    # initializing the data\n\n    p_train = train[train[\"kfold\"]!=fold].reset_index(drop=True)\n    p_valid = train[train[\"kfold\"]==fold].reset_index(drop=True)\n\n\n    train_dataset = BERTDataSet(p_train[\"excerpt\"],p_train[\"target\"])\n    valid_dataset = BERTDataSet(p_valid[\"excerpt\"],p_valid[\"target\"])\n\n    train_dataloader = DataLoader(train_dataset,batch_size=train_batch,shuffle = True,num_workers=4,pin_memory=True)\n    valid_dataloader = DataLoader(valid_dataset,batch_size=valid_batch,shuffle = False,num_workers=4,pin_memory=True)\n\n    model = transformers.BertForSequenceClassification.from_pretrained(\"..\/input\/bert-base-uncased\",num_labels=1)\n\n    model.to(device)\n    LR=2e-5\n    optimizer = AdamW(model.parameters(), LR,betas=(0.9, 0.999), weight_decay=1e-2) # AdamW optimizer\n\n    train_steps = int(len(p_train)\/train_batch*epochs)\n\n    num_steps = int(train_steps*0.1)\n\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)\n\n\n    trainlosses = []\n    vallosses = []\n    bestscore = None\n\n    trainscores = []\n    validscores = []\n\n    for epoch in tqdm(range(epochs)):\n\n        print(\"---------------\" + str(epoch) + \"start-------------\")\n\n        trainloss,trainscore = training(train_dataloader,model,optimizer,scheduler)\n\n        trainlosses.append(trainloss)\n        trainscores.append(trainscore)\n\n        print(\"trainscore is \" + str(trainscore))\n\n        preds,validloss,valscore=validating(valid_dataloader,model)\n\n        vallosses.append(validloss)\n        validscores.append(valscore)\n\n\n        print(\"valscore is \" + str(valscore))\n\n        if bestscore is None:\n            bestscore = valscore\n\n            print(\"Save first model\")\n\n            state = {\n                            'state_dict': model.state_dict(),\n                            'optimizer_dict': optimizer.state_dict(),\n                            \"bestscore\":bestscore\n                        }\n\n\n            torch.save(state, \"model\" + str(fold) + \".pth\")\n\n        elif bestscore > valscore:\n\n            bestscore = valscore\n\n            print(\"found better point\")\n\n            state = {\n                            'state_dict': model.state_dict(),\n                            'optimizer_dict': optimizer.state_dict(),\n                            \"bestscore\":bestscore\n                        }\n\n\n            torch.save(state, \"model\"+ str(fold) + \".pth\")\n\n        else:\n            pass\n\n\n    bestscores.append(bestscore)","5e58303f":"bestscores","e1a4b5b7":"np.mean(bestscores)\nprint(\"my cv is \" + str(np.mean(bestscores)))","331dc180":"import gc\ndel train_dataset,valid_dataset,train_dataloader,valid_dataloader,model,optimizer,scheduler\n_ = gc.collect()","b81f0d12":"test","f941e3de":"class BERTinfDataSet(Dataset):\n    \n    def __init__(self,sentences):\n        \n        self.sentences = sentences\n       \n        \n    def __len__(self):\n        \n        return len(self.sentences)\n    \n    def __getitem__(self,idx):\n        \n        sentence = self.sentences[idx]\n        \n        \n        bert_sens = tokenizer.encode_plus(\n                                sentence,\n                                add_special_tokens = True, # [CLS],[SEP]\n                                max_length = 314,\n                                pad_to_max_length = True, # add padding to blank\n                                truncation=True)\n\n        ids = torch.tensor(bert_sens['input_ids'], dtype=torch.long)\n        mask = torch.tensor(bert_sens['attention_mask'], dtype=torch.long)\n        token_type_ids = torch.tensor(bert_sens['token_type_ids'], dtype=torch.long)\n     \n        \n    \n        \n        return {\n                'ids': ids,\n                'mask': mask,\n                'token_type_ids': token_type_ids,\n                \n            }","8e246736":"test_dataset = BERTinfDataSet(test[\"excerpt\"])","151d5eb4":"test_batch = 32","6d5847f5":"test_dataloader = DataLoader(test_dataset,batch_size=test_batch,shuffle = False,num_workers=4,pin_memory=True)","daf87ff8":"model = transformers.BertForSequenceClassification.from_pretrained('..\/input\/bert-base-uncased',num_labels=1)","2a43bf81":"pthes = [os.path.join(\".\/\",s) for s in os.listdir(\".\/\") if \".pth\" in s]\npthes","31209dcf":"def predicting(\n    test_dataloader,\n    model,\n    pthes\n    \n):\n\n    allpreds = []\n    \n    for pth in pthes:\n        \n        state = torch.load(pth)\n        \n        model.load_state_dict(state[\"state_dict\"])\n        model.to(device)\n        model.eval()\n    \n    \n        preds = []\n        allvalloss=0\n\n        with torch.no_grad():\n\n\n            for a in test_dataloader:\n\n\n\n                ids = a[\"ids\"].to(device)\n                mask = a[\"mask\"].to(device)\n                tokentype = a[\"token_type_ids\"].to(device)\n\n               # output = model(ids,mask,tokentype)\n                output = model(ids,mask)\n\n                output = output[\"logits\"].squeeze(-1)\n\n\n                preds.append(output.cpu().numpy())\n\n            preds = np.concatenate(preds)\n            \n            allpreds.append(preds)\n\n    return allpreds\n","d3fe5793":"allpreds = predicting(test_dataloader,model,pthes)","35a3ed73":"findf = pd.DataFrame(allpreds)\nfindf = findf.T","c14fe94b":"findf","ba06d603":"finpred = findf.mean(axis=1)\nfinpred","20edfe23":"sample","520887f3":"sample[\"target\"] = finpred","0179ee73":"sample","f08607bb":"sample.to_csv(\"submission.csv\",index = False)","39e107bf":"#### \u3053\u306e\u307e\u307e\u4f7f\u7528\u3057\u3066OK","2d252d52":"#### ver9\u4ee5\u964d\u3001inference only\u306enotebook\u3092\u4f7f\u7528\u305b\u305a\u306b\u3001\u3053\u306enotebook\u3060\u3051\u3067submit\u3059\u308b\u5834\u5408\u306f\u3001\u4ee5\u4e0b\u306edebug2\u3092false\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u4f55\u500b\u304b\u306e\u30b3\u30fc\u30c9\u3092\u7701\u3044\u3066\u3044\u307e\u3059\u3002","2eff01ac":"### model\u306e\u5165\u529b\u306ftokenizer\u3067\u51fa\u529b\u3055\u308c\u305f(\u5358\u8a9eid(input_ids)\u3001attention_mask(mask))\ntoken_type_id\u306f\u3053\u306e\u5834\u5408\u3001\u5165\u308c\u3066\u3082\u5165\u308c\u306a\u304f\u3066\u3082OK\u307f\u305f\u3044\u3067\u3059\u3002\n\noutput\u306e\u4e2d\u8eab\u3092\u898b\u3066\u307f\u308b","36d0ba74":"# \u3053\u3053\u307e\u3067\u898b\u3066\u9802\u304d\u3001\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\uff01\n\n# \u304a\u5f79\u306b\u7acb\u3061\u307e\u3057\u305f\u3089\u3001upvote\u3057\u3066\u3044\u305f\u3060\u3051\u308b\u3068\u5b09\u3057\u3044\u3067\u3059\u3002\n## \u307e\u305f\u3001\u5404\u53c2\u8003url\u306e\u65b9\u3005\u306b\u306f\u3001\u305f\u304f\u3055\u3093\u306e\u3053\u3068\u3092\u5b66\u3073\u307e\u3057\u305f\u3002\u5927\u5909\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3057\u305f\uff01","76bdaae1":"#### \u304d\u3061\u3093\u3068\u3067\u304d\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d","f68f4e6f":"# About this notebook (LB Score : 0.546 )\n ### pytorch BERT\u306e\u521d\u5fc3\u8005\u90e8\u5c4b\u3067\u3059\u3002English version is here. https:\/\/www.kaggle.com\/chumajin\/pytorch-bert-beginner-s-room\n \n \n ### \u30b9\u30b3\u30a2\u3092\u76ee\u6307\u3059\u3088\u308a\u3082\u3001\u4ee5\u4e0b\u306e\u3053\u3068\u3092\u76ee\u7684\u3068\u3057\u3066\u3044\u307e\u3059\u3002\n \n*   \u7406\u89e3\u3092\u6df1\u3081\u308b\u3002\n*   \u8a00\u8a9e\u51e6\u7406\u3067\u3082\u6570\u5b57\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3068\u540c\u3058\u3088\u3046\u306b\u51e6\u7406\u3059\u308b\u3002\n        \n       pytorch\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u7d44\u3093\u3060\u3053\u3068\u306a\u3044\u65b9\u306f\u307e\u305a\u306f\u3053\u3061\u3089\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u3002\n       \n       https:\/\/www.kaggle.com\/chumajin\/pytorch-neural-network-starter-detail\n       \n       \n\n      \n        \n\n\n*   BERT\u306e\u96db\u5f62\u3092\u4f5c\u6210\u3059\u308b\u3002\n  ","52bea244":"# validation\u3082\u4f5c\u6210\u3057\u307e\u3059\u3002","d930792d":"#### dataloader\u306e\u4f5c\u308a\u65b9(pin_memory\u3068\u304bnum_worker\u3001\u5f8c\u3067\u3067\u3066\u304f\u308bAMP\u3082)\u306f\u3053\u3053\u306e\u65b9\u304c\u308f\u304b\u308a\u3084\u3059\u304f\u89e3\u8aac\u3057\u3066\u304f\u308c\u3066\u3044\u308b\u3002\n\nhttps:\/\/qiita.com\/sugulu_Ogawa_ISID\/items\/62f5f7adee083d96a587\n","7e5204a3":"# 5. training\u3092\u3059\u308b\u95a2\u6570\u4f5c\u6210\n\n#### optimizer\u306e\u5b9a\u7fa9\n* \u4eca\u56de\u3001\u3044\u308d\u3044\u308d\u304a\u8a66\u3057\u3057\u3066\u3044\u3066\u3001\u5b66\u7fd2\u7387\u306e\u91cd\u8981\u3055\u3092\u5b66\u3073\u307e\u3057\u305f(\u6700\u521d\u3001\u5b66\u7fd2\u7387\u304c\u9ad8\u3059\u304e\u3066\u3001\u56de\u5e30\u3057\u3066\u4e88\u6e2c\u3057\u305f\u7d50\u679c\u304c\u5168\u90e8\u540c\u3058\u5024\u306b\u306a\u3063\u305f\u308a\u30011\u65e5\u4ee5\u4e0a\u306f\u307e\u308a\u307e\u3057\u305f\u30fb\u30fb\u30fb\u3002)\n\n* \u3068\u308a\u3042\u3048\u305a\u3001\u3053\u306enotebook\u306f\u3001Bert\u306efine-tuning\u306e\u4e0d\u5b89\u5b9a\u6027\u306b\u3064\u3044\u3066\u3001 https:\/\/ai-scholar.tech\/articles\/bert\/bert-fine-tuning \u3092\u53c2\u8003\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3042\u308b\u7a0b\u5ea6\u63a1\u7528\u3057\u307e\u3057\u305f\u3002\n\n* \u305f\u3060\u3057\u3001\u4e0a\u8a18\u6587\u732e\u3067\u306f\u3001bert-large-uncased\u3000\u3067\u306e\u8a71\u3067\u3059\u3002\u4eca\u56de\u306f\u30015 k-fold 20epoch\u56de\u3059\u306e\u306b\u3001\u6642\u9593\u304c\u304b\u304b\u308a\u3059\u304e\u3066\u3057\u307e\u3063\u305f\u305f\u3081\u3001bert-base-uncased\u306b\u3057\u3066\u3044\u307e\u3059\u3002\n\n* \u7d30\u304b\u3044\u306e\u3067\u3001\u3068\u308a\u3042\u3048\u305a\u4f7f\u3044\u305f\u3044\u3068\u3044\u3046\u4eba\u306f\u7121\u8996\u3057\u3066\u305d\u306e\u307e\u307e\u6d41\u3057\u3066OK\u3067\u3059\u3002","bf8c3341":"#### \u5c11\u3057\u6642\u9593\u304c\u304b\u304b\u308a\u307e\u3059\u304c\u3001\u5b66\u7fd2\u7387\u306e\u63a8\u79fb\u3092\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u3054\u3068\u306b\u898b\u3066\u3044\u304f\u3068\u3053\u3093\u306a\u611f\u3058\u3067\u3059\u3002\n#### ver9\u4ee5\u964d\u3001submit\u6642\u9593\u8d85\u904e\u5bfe\u7b56\u3067\u3001\u5b9f\u884c\u3057\u306a\u3044\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\u7d50\u679c\u306f\u30d7\u30ea\u30f3\u30c8\u30b9\u30af\u30ea\u30fc\u30f3\u3067\u8cbc\u3063\u3066\u3042\u308a\u307e\u3059\u3002","2bcec23e":"#### \u304d\u3061\u3093\u3068\u3067\u304d\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d","d4b0725d":"#### \u4f55\u3082\u8003\u3048\u305a\u306b\u3001tokenizer\u306eencode_plus\u306b\u98df\u308f\u305b\u305f\u5834\u5408","b8fc98a6":"(\u9014\u4e2d\u3067\u3082\u7d39\u4ecb\u3057\u3066\u3044\u307e\u3059\u304c) \n\n ### \u4ee5\u4e0b\u306einference only book\u306bver 7\u306e\u7d50\u679c\uff08\u3053\u308c\u3088\u308a\u5c11\u3057\u826f\u3044\u30b9\u30b3\u30a2)\u3092upload\u3057\u3066\u304a\u308a\u307e\u3059\u3002\n ### (ver8\u307e\u3067\u306f\u3053\u306enotebook\u3067\u306ftrain\u3082\u5165\u308b\u305f\u3081\u3001submit\u306b\u6642\u9593\u304c\u304b\u304b\u308a\u3059\u304e\u3066submit error\u304c\u8d77\u3053\u3063\u3066\u3044\u307e\u3057\u305f)\n\ninference only\u306enotebook\u306f\u3053\u3061\u3089\u3000\u3000https:\/\/www.kaggle.com\/chumajin\/inference-for-pytorch-bert-beginner-s-room?scriptVersionId=62477318\n\ninference only\u306b\u3059\u308b\u306e\u306f\u3001submit\u304c\u65e9\u3044\u306e\u3068\u3001internet on\u306b\u3057\u3066\u3057\u307e\u3063\u305f\u5834\u5408\u306bsubmit\u3067\u304d\u307e\u3059\u3002","ba1c10d2":"# 7. \u4ed6\u306eK-fold\u3092\u56de\u3057\u3066\u3044\u304d\u307e\u3059\u3002","aa206e1c":"#### \u6587\u7ae0\u3092\u77ed\u304f\u3059\u308b\u5834\u5408 (\u8a08\u7b97\u6642\u9593\u3068\u304b\u304b\u304b\u308b\u3068\u3053\u306e\u8fba\u3092\u8abf\u6574\u3059\u308b\u3053\u3068\u3082\u3042\u308b\u3002\u4f8b\u3048\u307010\u306b\u3057\u3066\u307f\u308b\u3002\u4ed6\u306enotebook\u3060\u3068256\u3068\u304b\u306b\u3057\u3066\u3044\u308b\u4eba\u3082\u3044\u307e\u3059\u3002)","dd3ae516":"#### \u4ee5\u4e0a\u306eoptimizer\u3068\u304bloss\u3092\u5165\u308c\u3066training\u3092\u95a2\u6570\u5316\u3002\n#### AMD\u306eautocast()\u3092\u4f7f\u7528\u3059\u308b\u306e\u3067\u3001\u305d\u306e\u66f8\u304d\u65b9\u306b\u3057\u3066\u3044\u307e\u3059\u3002\n\n\nhttps:\/\/qiita.com\/Sosuke115\/items\/40265e6aaf2e414e2fea\n\n\u30a4\u30e1\u30fc\u30b8\u306f\u3053\u3061\u3089\u306e\u65b9\u304c\u3057\u3084\u3059\u3044\u304b\n\nhttps:\/\/qiita.com\/sugulu_Ogawa_ISID\/items\/62f5f7adee083d96a587","038c919a":"#### \u5358\u8a9eid\u304b\u3089\u6587\u7ae0\u306b\u623b\u3057\u3066\u307f\u308b","f781b917":"#### inference\u306btarget\u306f\u306a\u3044\u306e\u3067\u3001\u7121\u3057\u7248\u3067Dataset\u3092\u4f5c\u308a\u307e\u3059\u3002","52a9dbd0":"# 8. inference\n\n#### \u4ee5\u4e0b\u306einference only book\u306bver 7\u306e\u7d50\u679c\u3092upload\u3057\u3066\u304a\u308a\u307e\u3059(ver8\u307e\u3067\u306f\u3053\u306enotebook\u3067\u306ftrain\u3082\u5165\u308b\u305f\u3081\u3001submit\u306b\u6642\u9593\u304c\u304b\u304b\u308a\u3059\u304e\u3066submit error\u304c\u8d77\u3053\u3063\u3066\u3044\u307e\u3057\u305f)\u3002\n\n#### inference\u306f\u3053\u3061\u3089\u3000https:\/\/www.kaggle.com\/chumajin\/inference-for-pytorch-bert-beginner-s-room?scriptVersionId=62477318\n\n#### inference\u7248\u306f\u3053\u308c\u3088\u308asubmit\u304c\u304b\u306a\u308a\u65e9\u3044\u3067\u3059\u3002","ed7367bf":"#### test\u30c7\u30fc\u30bf\u306etarget\u3092\u4e88\u6e2c\u3057\u3066submission\u30d5\u30a1\u30a4\u30eb\u3068\u3057\u3066\u63d0\u51fa\u3002","e8f6236c":"### \u89e3\u8aac\u7528\u306b\u3044\u308d\u3044\u308d\u3084\u3063\u3066\u304d\u305f\u306e\u3067\u3001parameter\u3092\u521d\u671f\u5316\u3057\u307e\u3059\u3002","7fb38fe5":"#### \u307e\u305a\u306f\u3001\u4eca\u56de\u306etrain\u30c7\u30fc\u30bf\u3067\u4e00\u756a\u9577\u3044\u6587\u7ae0\u306e\u5358\u8a9e\u6570\u3092\u6570\u3048\u3066\u307f\u307e\u3059\u3002\u5c11\u3057\u6642\u9593\u304b\u304b\u308a\u307e\u3059\u3002\n\n#### ver10\u4ee5\u964d\u3001\u3053\u306etrain & inference\u306enotebook\u3067submit\u3059\u308b\u5834\u5408\u3001\u7701\u7565\u3057\u3066\u3044\u307e\u3059\uff08submit\u306e\u6642\u9593\u8d85\u904e\u3067error\u51fa\u305f\u305f\u3081)\u3002\n\n#### \u6d41\u3057\u305f\u3044\u5834\u5408\u306fdebug2=True\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u7d50\u679c\u306f\u30d7\u30ea\u30f3\u30c8\u30b9\u30af\u30ea\u30fc\u30f3\u3057\u3066\u8cbc\u3063\u3066\u3042\u308a\u307e\u3059\u3002","a3067b07":"##### epoch\u6570\u3092debug = False\u3060\u306820 epoch, True\u3060\u30681\u306b\u3057\u3066\u3044\u307e\u3059\u3002\u6d41\u308c\u3060\u3051\u77e5\u308a\u305f\u3044\u65b9\u306fTrue\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002","e6825e9f":"# 0. \u4e0b\u6e96\u5099","db827e46":"#### id 0\u756a\u306e\u3082\u306e\u3092tokenizer\u3092\u4f7f\u3063\u3066\u3001\u3044\u308d\u3044\u308d\u3044\u3058\u3063\u3066\u307f\u307e\u3059\u3002","d2a5211e":"#### \u53c2\u8003\u307e\u3067\u306b\u3001\u4e0a\u8a18\u3092debug2=True\u306b\u3057\u3066\u5b9f\u884c\u3059\u308b\u3068\u4ee5\u4e0b\u306e\u30b0\u30e9\u30d5\u304c\u5f97\u3089\u308c\u307e\u3059\u3002\u6a2a\u8ef8\u304c\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u306e\u56de\u6570\u3001\u7e26\u8ef8\u304c\u5b66\u7fd2\u7387\u3067\u3059\u3002\n![image.png](attachment:4407ec9d-05cd-4ff7-8f93-0c13ffbd650b.png)","8764b3c4":"# 1. \u3053\u306e\u30b3\u30f3\u30da\u306eEDA","3c0eb644":"--------\u3068\u306f\u8a00\u3063\u3066\u3082\u3001\u3042\u308b\u7a0b\u5ea6\u30b9\u30b3\u30a2\u51fa\u306a\u3044\u3068\u516c\u958b\u3067\u304d\u306a\u304b\u3063\u305f\u306e\u3067\u3001\u4ee5\u4e0b\u5de5\u592b\u3057\u305f\u70b9\u306e\u30e1\u30e2\u3067\u3059 --------\n\n  (\u521d\u5fc3\u8005\u7528\u3068\u8a00\u3063\u3066\u3044\u308b\u304f\u305b\u306b\u30de\u30cb\u30a2\u30c3\u30af\u3002\u5f8c\u307b\u3069\u89e3\u8aac\u3057\u307e\u3059\u3002)\n* BertModel\u3092\u4f7f\u7528\u3059\u308b\u3088\u308a\u3082\u3001BertForSequenceClassification\u3092\u4f7f\u7528\u3057\u305f\u65b9\u304c\u79c1\u306f\u30b9\u30b3\u30a2\u304c\u51fa\u305f\u306e\u3067\u3001\u305d\u308c\u3092\u63a1\u7528\u3057\u307e\u3057\u305f\u3002\n* Bert\u306efine-tuning\u306e\u4e0d\u5b89\u5b9a\u6027\u306b\u3064\u3044\u3066\u3001 https:\/\/ai-scholar.tech\/articles\/bert\/bert-fine-tuning \u3092\u53c2\u8003\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u3042\u308b\u7a0b\u5ea6\u63a1\u7528\u3057\u307e\u3057\u305f\u3002\n     \n    \u203b\u3000Large-model\u306720 epoch,5 k-folds\u306f\u6642\u9593\u304b\u304b\u308b\u306e\u3067\u3001\u305d\u3053\u3060\u3051\u3001base uncased model\u63a1\u7528\u3057\u307e\u3057\u305f\u3002","0e1f6118":"#### \u521d\u3081\u3092\u8868\u3059[CLS]\u3068\u7d42\u308f\u308a\u3092\u8868\u3059[SEP]\u304c\u8ffd\u52a0\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308b\u3002\n#### \u3053\u308c\u30892\u3064\u3092Special tokens\u3068\u547c\u3093\u3067\u3044\u308b(tokenizer\u306e\u5f15\u6570\u306b\u5165\u308c\u308b\u304b\u5165\u308c\u306a\u3044\u304b\u306e\u9078\u629e\u80a2\u304c\u3042\u308bdefault\u306f\u5165\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b)","c4d8aabe":"#### 314\u500b\u306b\u6e80\u305f\u306a\u3044\u6587\u7ae0\u306f[PAD]\u3067\u57cb\u3081\u3066\u3042\u3052\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002","16e04c08":"#### \u30e9\u30f3\u30c0\u30e0\u30b7\u30fc\u30c9\u306e\u56fa\u5b9a","e2f5dda4":"  ### \u5c11\u3057\u3067\u3082\u304a\u5f79\u306b\u7acb\u3066\u308c\u3070\u5e78\u3044\u3067\u3059\u3002upvote\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u3044\u305f\u3057\u307e\u3059 !\n  ### \u3042\u3068\u3001\u3044\u3064\u3082upvote\u3057\u3066\u3044\u305f\u3060\u3051\u308b\u65b9\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\n  \n  \n  -----\u3054\u53c2\u8003--------------------------------------------------\n  ### inference only\u306enotebook\u306f\u3053\u3061\u3089\u3067\u3059\u3002ver 7\u306e\u7d50\u679c\uff08\u3053\u308c\u3088\u308a\u5c11\u3057\u826f\u3044\u30b9\u30b3\u30a2)\u3092upload\u3057\u3066\u304a\u308a\u307e\u3059\u3002\n  ### (ver8\u307e\u3067\u306f\u3053\u306enotebook\u3067\u306ftrain\u3082\u5165\u308b\u305f\u3081\u3001submit\u306b\u6642\u9593\u304c\u304b\u304b\u308a\u3059\u304e\u3066submit error\u304c\u8d77\u3053\u3063\u3066\u3044\u307e\u3057\u305f)\n\ninference only\u306enotebook\u306f\u3053\u3061\u3089\u3000\u3000https:\/\/www.kaggle.com\/chumajin\/inference-for-pytorch-bert-beginner-s-room?scriptVersionId=62477318\n\ninference only\u306b\u3059\u308b\u306e\u306f\u3001submit\u304c\u65e9\u3044\u306e\u3068\u3001internet on\u306b\u3057\u3066\u3057\u307e\u3063\u305f\u5834\u5408\u306bsubmit\u3067\u304d\u307e\u3059\u3002","d7c6875a":"#### input_ids\u306e102\u306e\u3042\u3068\u306b0\u304c\u5927\u91cf\u306b\u8ffd\u52a0\u3002attention_mask\u306b\u3082\u3001\u5148\u307b\u3069\u306f\u5168\u90e81\u3060\u3063\u305f\u304c\u30010\u304c\u8ffd\u52a0\u3055\u308c\u3066\u3044\u308b\u3002","f0e5b13a":"#### \u4ee5\u4e0b\u306f\u3001transformer\u306b\u5185\u8535\u3055\u308c\u305f\u5b66\u7fd2\u7387\u306e\u30b9\u30b1\u30b8\u30e5\u30fc\u30e9\u30fc(step\u3059\u308b\u3054\u3068\u306b\u5b66\u7fd2\u7387\u3092\u5909\u3048\u3066\u3044\u304f)\u3067\u3059\u3002\n#### \u2191\u306e\u30b5\u30a4\u30c8\u306e\u63a8\u5968\u3067\u306f\u300120 epoch\u3067\u3001\u6700\u521d\u306e10%\u3067\u3001\u76ee\u7684\u306e\u5b66\u7fd2\u7387\u306b\u5230\u9054\u3055\u305b\u305f\u5f8c\u30010\u306b\u306a\u308b\u3088\u3046\u306b\u6e1b\u8870\u3057\u3066\u3044\u304f\u30e2\u30c7\u30eb\u3092\u8a00\u3063\u3066\u3044\u305f\u306e\u3067\u3001\u305d\u308c\u3092\u4f5c\u308a\u307e\u3057\u305f\u3002","b0cc90cf":"#### input_ids\u306e\u5358\u8a9eid\u304b\u3089\u6587\u7ae0\u306b\u623b\u3057\u3066\u307f\u308b\u3002","e845f36e":"#### \u306a\u306e\u3067\u3001output\u306f\u3053\u306e\u5f62\u3067\u4f7f\u3044\u307e\u3059\u3002","e4a12653":"## \u3010\u91cd\u8981\u3011BERT\u3067\u306f\u3001\u305d\u308c\u305e\u308c\u306e\u6587\u7ae0\u3067\u306e\u5358\u8a9e\u6570\u3092\u305d\u308d\u3048\u3066\u3042\u3052\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n   ## (pretrained model\u306eMax\u306f512\u500b)","34313c01":"#### tokenizer\u306e\u5f15\u6570\u3092\u8abf\u6574\u3057\u3066\u3042\u3052\u308b\u3068\u81ea\u52d5\u306b\u8abf\u6574\u3057\u3066\u304f\u308c\u308b","8519976a":"#### \u307e\u305a\u306f\u3001\u304a\u8a66\u3057\u3067\u3001kfold = 0\u3092validation, \u305d\u308c\u4ee5\u5916\u3092train\u30c7\u30fc\u30bf\u3068\u3057\u307e\u3059\u3002reset_index()\u3057\u306a\u3044\u3068\u306f\u307e\u308a\u307e\u3059\u306e\u3067\u3001\u6ce8\u610f\u3002","704d1c1b":"# 6. traing\u5b9f\u65bd","8c74734a":"----version 10 \u8ffd\u52a0-----\n\n\nversion 7\u306ftrain & inference\u3067\u6642\u9593\u8d85\u904e\u3067submit\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u3057\u304b\u3057\u3001inference only\u3067submit\u3057\u305f\u3068\u3053\u308d\u30010.528\u3068\u30b9\u30b3\u30a2\u304c\u51fa\u307e\u3057\u305f\u3002\n\n\nversion 9\u306ftrain & inference\u306f\u6642\u9593\u8d85\u904e\u5bfe\u7b56\u3067\u4e00\u90e8\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3057\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u305d\u3046\u3059\u308b\u3068\u3001\u30b9\u30b3\u30a2\u304c0.546\u3068\u4e0b\u304c\u3063\u3066\u3057\u307e\u3044\u307e\u3057\u305f\u3002\n\n\n\u3053\u306e\u73fe\u8c61\u306b\u95a2\u3059\u308b\u8003\u5bdf\u306f\u3001\u65b0\u3057\u3044notebook\u3067\u884c\u3046\u4e88\u5b9a\u3067\u3059\u3002\uff08\u30ea\u30ea\u30fc\u30b9\u3057\u305f\u3089\u3001\u30b3\u30e1\u30f3\u30c8\u6b04\u306b\u66f8\u304d\u307e\u3059\uff09\n\n\n\u4ee5\u4e0b\u306f\u3001\u305d\u3053\u304b\u3089\u308f\u304b\u3063\u305f\u3053\u3068\u3067\u3059\u3002\u3044\u308d\u3044\u308d\u3068\u3064\u3058\u3064\u307e\u3092\u5408\u308f\u305b\u3001train & inference\u3067\u3082version7\u306e0.528\u30b9\u30b3\u30a2\u306b\u305d\u308d\u3048\u308b\u305f\u3081\u306e\u30b3\u30fc\u30c9\u3067\u3059\u3002\n\n\uff08\u30b3\u30fc\u30c9\u306e\u4e2d\u8eab\u3068\u304b\u3001\u7d30\u304b\u3044\u3053\u3068\u306f\u6c17\u306b\u3057\u306a\u304f\u3066\u3082\u826f\u3044\u3067\u3059\u304c\u3001\u30b9\u30b3\u30a2\u306e\u9055\u3044\u304c\u6c17\u306b\u306a\u308b\u65b9\u306f\u8003\u5bdf\u7528\u306e\u65b0\u3057\u3044notebook\u3092\u3054\u53c2\u7167\u304f\u3060\u3055\u3044)","c5a0a4d1":"#### loss\u95a2\u6570\u306f\u3001\u4eca\u56de\u306fRMSE\u306a\u306e\u3067\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u5b9a\u7fa9\u3057\u307e\u3057\u305f\u3002","22b1b860":"# 3. Pytorch neural network\u306e\u4e0b\u6e96\u5099\n## 3.1 k-fold","70aacf57":"#### \u63a8\u8ad6\u3059\u308b\u95a2\u6570\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002","77d991da":"#### shape\u898b\u308b\u3068\u30011\u304c\u4f59\u8a08\u306a\u306e\u3067\u3001squeeze\u3068\u3044\u3046\u306e\u3092\u4f7f\u3046\u3068\u30011\u306e\u3068\u3053\u308d\u3092cut\u3057\u3066\u304f\u308c\u307e\u3059","d3ef7bdc":"#### \u3082\u3057\u3001debug2\u3092True\u306b\u3057\u305f\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u7d50\u679c\u304c\u5f97\u3089\u308c\u307e\u3059\u3002\n\n![image.png](attachment:f76cf253-0513-49d9-939a-11c38ebb13a5.png)","686bd33a":"#### \u6587\u7ae0\u306e\u9577\u3055\u3092\u305d\u308d\u3048\u308b\u305f\u3081\u306b\u3001[PAD]\u304c\u8ffd\u52a0\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308b\u3002\n#### \u3053\u306e\u90e8\u5206\u306f\u3001Attention mask\u30670\u306b\u3057\u3066\u3001\u8a08\u7b97\u3055\u308c\u306a\u3044\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u3002","3ffa17ad":"#### bert\u306e\u30e2\u30c7\u30eb\u306f\u3044\u308d\u3044\u308d\u3042\u308b\u304c\u3001\u4eca\u56de\u306f\u3001bert-base-uncased\u3092\u4f7f\u7528\u3002\n#### bert-large-uncased\u3068\u304b\u3082\u3042\u308b\u304c\u3001BERT\u306e\u4e2d\u306e\u57cb\u3081\u8fbc\u307f\u30d9\u30af\u30c8\u30eb\u306e\u6b21\u5143\u6570\u306a\u3069\u304c\u9055\u3046\u3002\n#### https:\/\/www.kaggle.com\/xhlulu\/huggingface-bert \u306bbert-large-uncased\u306f\u843d\u3061\u3066\u3044\u308b\u306e\u3067\u3001input\u3092\u305d\u3053\u306b\u5909\u3048\u3066\u3082\u3089\u3048\u308c\u3070\u4f7f\u3048\u307e\u3059\u3002","22ba5cb6":"Tokenizer\u306eoutput\u3068\u3057\u3066\u306f\u8f9e\u66f8\u578b\u3067\u51fa\u3066\u304d\u3066\u3001\u4ee5\u4e0b\u306e3\u3064\u304c\u3042\u308b\n* input_ids : \u5358\u8a9eid (BERT\u306epretrained model\u306b\u5165\u3063\u3066\u3044\u308b\u3084\u3064)\u203b\u3000\u7d30\u304b\u3044\u3053\u3068\u3092\u8a00\u3046\u3068\u3001\u521d\u3081\u3068\u7d42\u308f\u308a\u306b\u3001101[CLS]\u3068102[SEP]\u304c\u8ffd\u52a0\u3055\u308c\u3066\u3044\u308b\n* token_type_ids : \u6587\u7ae0\u3092\u628a\u63e1\u3059\u308b\u30d0\u30a4\u30ca\u30ea\u30fc\u30de\u30b9\u30af : \u4eca\u56de\u306f\u56de\u5e30\u554f\u984c\u3067\u3001\u5168\u90e8 0\u3002\u6587\u7ae0\u9593\u306e\u7e4b\u304c\u308a\u3092\u898b\u308b\u3068\u304d\u306f\u9014\u4e2d\u306b[SEP]\u306a\u3069\u631f\u3093\u3067\u5909\u66f4\u3057\u305f\u308a\u3059\u308b\u3002\n* attention_mask : \u57cb\u3081\u8fbc\u307f\u3092\u5224\u5b9a\u3059\u308b\u30d0\u30a4\u30ca\u30ea\u30fc\u30de\u30b9\u30af : \u4f8b\u3048\u3070\u3001\u4eca\u56de\u306f\u3001\u5f8c\u307b\u3069\u6587\u5b57\u6570\u3092\u5408\u308f\u305b\u308b\u305f\u3081\u306b[PAD]\u3068\u3044\u3046\u306e\u3092\u4ee3\u5165\u3059\u308b\u304c\u3001\u305d\u308c\u3092\u5224\u5b9a\u3059\u308b\u3002\n\u3053\u306e\u65b9\u306a\u3069\u898b\u3066\u3044\u305f\u3060\u3051\u308b\u3068\u308f\u304b\u308b\u304b\u3082\u3067\u3059\u3002https:\/\/qiita.com\/omiita\/items\/72998858efc19a368e50","ad127d51":"https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds\n\n\n\u3053\u306e\u65b9\u304c\u4f5c\u308a\u65b9\u5171\u6709\u3057\u3066\u304f\u308c\u3066\u3044\u307e\u3059\u304c\u3001\u3053\u3053\u3067\u306f\u30b7\u30f3\u30d7\u30eb\u306btarget\u9806\u306b\u4e26\u3073\u66ff\u3048\u3066\u30015\u500b\u306efold\u306b\u9806\u756a\u3067\u308f\u3051\u307e\u3059\u3002","11a08bd2":"#### \u304d\u3061\u3093\u3068\u63a8\u6e2c\u5024\u304c\u51fa\u3066\u304d\u3066\u3044\u308b\u3002\u4f7f\u3044\u305f\u3044\u3068\u3053\u308d\u306flogits\u306e\u3068\u3053\u308d","4c625328":"#### excerpt\u306e\u6587\u7ae0\u306e\u8aad\u307f\u3084\u3059\u3055\u306e\u96e3\u6613\u5ea6\u307f\u305f\u3044\u306e\u304c\u3001target\u306b\u8a18\u5165\u3055\u308c\u3066\u3044\u3066\u3001\u305d\u308c\u3092\u4e88\u6e2c\u3059\u308b\u30b3\u30f3\u30da(\u3068\u3066\u3082\u30b7\u30f3\u30d7\u30eb)","c5c0413e":"# 2. BERT : Tokenizer \u306e\u7406\u89e3\u3092\u6df1\u3081\u308b\n##      \u82f1\u5358\u8a9e\u3092id\u5316\u3057\u305f\u308a\u3044\u308d\u3044\u308d\u3057\u3066\u304f\u308c\u308b\u3084\u3064","9e1df01e":"## 3.2 DataSet, DataLoader\u3092\u7d44\u3093\u3067\u3044\u304f(\u6570\u5b57\u306eNeural network\u3068\u540c\u3058)","80f8d6ea":"# 1epoch\u30c6\u30b9\u30c8(\u5b9f\u884c\u3057\u305f\u304b\u3063\u305f\u3089\u3001debug2=True\u306b\u3057\u3066\u304f\u3060\u3055\u3044)","1d80447d":"#### 314 words\u304c\u4eca\u56de\u306etrain\u30c7\u30fc\u30bf\u306emax\u5358\u8a9e\u6570\u3002\u305f\u3060\u3057\u3001[CLS]\u3068[SEP]\u3092\u542b\u3080\u3002","a44aa83f":"#### test\u306b\u306ftarget\u3068standard error\u304c\u306a\u3044","e569e7e9":"#### BERT\u306f\u7528\u9014\u306b\u5fdc\u3058\u3066\u69d8\u3005\u306apretrained\u30e2\u30c7\u30eb\u304c\u3042\u308b","61b3759b":"# 4. BERT\u306e\u30e2\u30c7\u30eb\u4f5c\u6210","4d08f6ed":"#### 5\u500b\u306e\u30e2\u30c7\u30eb\u306e\u7d50\u679c\u3092\u5e73\u5747\u3057\u307e\u3059\u3002","971a3670":"* BertModel\n* BertForPreTraining\n* BertForMaskedLM\n* BertForNextSentencePrediction\n* BertForSequenceClassification\n* BertForMultipleChoice\n* BertForTokenClassification\n* BertForQuestionAnswering\n\n\u3053\u306e\u30da\u30fc\u30b8\u306e\u65b9\u304c\u89e3\u8aac\u3057\u3066\u304f\u308c\u3066\u3044\u307e\u3059\u3002\n\nhttps:\/\/kento1109.hatenablog.com\/entry\/2019\/08\/20\/161936\n\n\u3088\u304f\u51fa\u3066\u304f\u308b\u306e\u306f\u3001\u6587\u7ae0\u306e\u4e00\u90e8\u3092\u96a0\u3057\u3066\u3001\u4e88\u6e2c\u3057\u305f\u308a\u3001\u6587\u7ae0\u9593\u306b\u7e4b\u304c\u308a\u304c\u3042\u308b\u304b\u306e\u5224\u5b9a\u306a\u3069\u3067\u3059\u3002\n\n\n\u4eca\u56de\u306f\u3001BertForSequenceClassification\u3000\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\n\n\u203b\u3000\u6700\u521dBertModel\u3092\u4f7f\u3063\u3066\u3044\u3058\u304f\u3063\u3066\u3044\u307e\u3057\u305f\u304c\u3001score\u304c\u79c1\u306f\u4e0a\u3052\u308c\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u3002\u3002\ndefault\u3067\u306f\u30010,1\u306e\u3088\u3046\u306b2\u5206\u985e\u3088\u3046\u306b\u51fa\u529b\u304c2\u3064\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u304c\u3001\u4eca\u56de\u306f\u56de\u5e30\u554f\u984c\u3067\u3059\u306e\u3067\u3001\u51fa\u529b(num_label)\u30921\u306b\u3057\u3066\u3042\u3052\u307e\u3059\u3002\n\u5206\u985e\u554f\u984c\u3092\u3057\u305f\u3044\u65b9\u306f\u3053\u3053\u3092\u8abf\u6574\u3059\u308c\u3070\u3088\u3044\u304b\u3068\u3002","2fdfd1ab":"#### \u4eca\u56de\u306f\u3001\u6700\u5927\u9577\u306e314\u3092\u4f7f\u7528\u3057\u3066\u3044\u304f\u3002(\u4ee5\u4e0b\u3001Dataset\u5185\u3067\u5909\u63db)","02a8acbe":"### \u4f5c\u6210\u3057\u305f\u30e2\u30c7\u30eb\u3092\u30ed\u30fc\u30c9\u3057\u307e\u3059\u3002"}}