{"cell_type":{"61ec8544":"code","fb250b14":"code","93e7b102":"code","1e397a24":"code","476659d0":"code","191f7016":"code","f4fdd75a":"code","ca07a382":"code","4a5af66f":"code","6261d33a":"code","e1d41cdb":"code","818890a1":"code","824f36de":"code","71abc0f0":"code","14e64086":"code","6c0e57cf":"code","c09b4563":"markdown","df65f5db":"markdown","9e9e8f1a":"markdown","65c7c451":"markdown","90c20b57":"markdown","20ae6cdb":"markdown","9e238709":"markdown","7962e3f4":"markdown","16009aa5":"markdown","a50ae452":"markdown","908a3ec3":"markdown","677133f0":"markdown","56639846":"markdown","eaadf568":"markdown","63a7d915":"markdown","3202c5f6":"markdown","2a5d315d":"markdown","d94e32ee":"markdown","a08b5dad":"markdown","2e7aaaa7":"markdown","30689448":"markdown","022b19c6":"markdown","e9b88466":"markdown"},"source":{"61ec8544":"#Import Necessary Modules\nimport pandas as pd #Table Manipulation\nimport numpy as np #Working with Numbers\nimport matplotlib.pyplot as plt #Plotting\nimport seaborn as sns #Plotting\nimport datetime #Working with Dates\nfrom bq_helper import BigQueryHelper #Accessing Google Big Query\nimport warnings #to ignore warnings (optional)\nwarnings.filterwarnings('ignore')\nimport plotly.express as px #Plotting\n\n#weather= pd.read_csv(\"..\/input\/weather-data\/weather_df (1).csv\")\n\n#Years under examination\nyears =range(1971, 2020)\n\n#Setting up our Big Query Helper Function\nhelper = BigQueryHelper('bigquery-public-data', 'noaa_gsod')\n\n#First Query to be processed --> Actual Weather Data\n#A string in python, will be parsed as SQL code\n#It will return all stations with \n#a latitude between 34 and 42 and\n#a longitude between 20 and 29\n\nsql = '''\nSELECT\n\n  year, mo, da, temp, min, max, prcp, stn, b.lat,b.lon, b.country\n  \nFROM\n    `bigquery-public-data.noaa_gsod.gsod{}` a\n\nINNER JOIN\n(SELECT usaf,country,lat,lon FROM `bigquery-public-data.noaa_gsod.stations`) b ON a.stn = b.usaf\n\nWHERE \n    b.lat < 42 AND b.lat > 34 AND b.lon > 20 AND b.lon < 29 \n    \n '''\n\n\n#Second Query to be processed --> Data regarding each station\n#We only get station name from it\n#Adds only informative value, we could also work without it\n\nstation_query = '''\n\nSELECT\n\n usaf, name\n \nFROM\n    `bigquery-public-data.noaa_gsod.stations`\n\nWHERE \n    lat < 42 AND lat > 34 AND lon > 20 AND lon < 29 \n    \n '''","fb250b14":"#Fetch Station Data\nstation_names = helper.query_to_pandas(station_query)\n#Rename\nstation_names.columns = [\"stn\", \"name\"]\n#Station Name as Integer in order to match with the basic dataframe\nstation_names = station_names.astype({'stn': 'int64'})","93e7b102":"to_download = True\n\nif to_download:\n    \n    #Fetch each year's distinctive dataset into a list\n    weather = [helper.query_to_pandas(sql.format(i)) for i in years ]\n    #Concatenate to create a DataFrame\n    weather = pd.concat(weather)\n    \n    # turning temp into celsius degrees\n    def far_to_cel(temp_f):\n        temp_c = (temp_f - 32) * 5\/9\n        return round(temp_c, 2)\n\n    for col in ['temp','min','max']:\n        \n        weather[col] = weather[col].replace({ 9999.9 : np.nan }).apply(far_to_cel)\n    \n    for col in ['year','mo','da']:\n        weather[col] = weather[col].astype(int)\n    \n    weather[\"Date\"] = pd.to_datetime(((weather.year)*10000+ (weather.mo)*100 + (weather.da)).apply(str),format='%Y%m%d' )\n    \n    weather[\"stn\"] = weather[\"stn\"].astype(int) \n    \nelse: #if not willing to download dataset, read it from working directory\n    \n    #Otherwise read from working directory\n    weather= pd.read_csv(\"..\/input\/weather-data\/weather_df (1).csv\")\n\n    #Keep only latest observations\n    weather = weather.query(\"year > 1970\")\n                            \n    #Date Column\n    weather[\"Date\"] = pd.to_datetime((weather.year*10000+weather.mo*100+weather.da).apply(str),format='%Y%m%d')","1e397a24":"#Bring each station's name along the lines of our dataframe\nweather = weather.merge(station_names, on = \"stn\", how = \"left\")\n\n#Get rid of the -repeated from merge- \"stn\" column \n#weather.drop(\"stn_1\",axis=1,inplace=True)\n#and take look at the resulting dataframe \nweather.head()","476659d0":"print(\"Our Final Dataset consists of:\")\nprint(weather.shape[0], \"Rows and\")\nprint(weather.shape[1], \"Columns\")","191f7016":"#First Create a custom function returning all wanted summary data \ndef station_info(x):\n    d = {}\n    d['Number_of_Observations'] = x['temp'].count()\n    d['Max_Temp'] = x['temp'].max()\n    d['Avg_Temp'] = x['temp'].mean()\n    d['Min_Temp'] = x['temp'].min()\n    d['Year_of_First_Observation'] = x['year'].min()\n    d['Year_of_Last_Observation'] = x['year'].max()\n    return pd.Series(d, index=['Number_of_Observations',\n                               'Max_Temp',\n                               'Avg_Temp', \n                               'Min_Temp', \n                               'Year_of_First_Observation',\n                              'Year_of_Last_Observation'])\n\n#Then use our custom function\nstations = weather.groupby(['lat','lon', \"country\", \"name\"]).apply(station_info).reset_index().rename(columns={0:'count'})\n#stations.head()\n\n#Which countries are included in our dataset (here, Greece's neighbors)\nstations.country.unique()\n\n#And represent each country with a distinct color\nstations[\"color\"] = np.select( \n    [stations.country == \"GR\",\n     stations.country == \"TU\",\n    stations.country == \"AL\",\n    stations.country == \"MK\",\n    stations.country == \"BU\"],\n    [\"blue\",\"red\", \"black\",\"orange\",\"darkgreen\"]\n) \n\n\n#Load Modules to depict maps\nimport folium\nimport folium\nfrom folium import Choropleth, Circle, Marker\nfrom folium.plugins import HeatMap, MarkerCluster\n\n#Create the basis of our map\nm_1 = folium.Map(location = [38,24.5], \n                 tiles='cartodbpositron', \n                 zoom_start=5.8)\n\n#Add markers for each observation of our Stations Dataframe\nfor idx, row in stations.iterrows():\n    \n    #Here we create a string containing html\n    #It will be parsed from folium.Popup function to create a beautiful popup containing information about each station    \n    popup_html = \"Name: \"+row[\"name\"]+\"<br>\"+\"Number_of_Observations: \"+str(round(row[\"Number_of_Observations\"]))+\"<br>\"+\"Average Temperature: \"+str(round(row[\"Avg_Temp\"],2))+\"<br>\"+\"Maximum Temperature: \"+str(round(row[\"Max_Temp\"],2))+\"<br>\"+\"Year of First Observation: \" + str(round(row[\"Year_of_First_Observation\"]))+\"<br>\"+\"Year of Last Observation: \" + str(round(row[\"Year_of_Last_Observation\"]))+\"<br>\"\n                    \n    Marker(radius = 70,location=[row['lat'], row['lon']],\n           \n\n           \n          popup=folium.Popup(popup_html),\n           \n           icon=folium.Icon(color=row[\"color\"])\n          ).add_to(m_1)\n\n    \n#Show the map    \nm_1","f4fdd75a":"grouped = weather.groupby('year').mean()\n#grouped.head()\n\ngrouped[\"year\"] = grouped.index\n\nimport plotly.io as pio\n\nfor i in range(0,grouped.shape[0]-4):\n    grouped.loc[grouped.index[i+4],'SMA_5'] = np.round((grouped.iloc[i,2]+ grouped.iloc[i+1,2] +grouped.iloc[i+2,2]+grouped.iloc[i+3,2]+grouped.iloc[i+4,2])\/5,1)","ca07a382":"from plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\ntrace1 = go.Scatter(\n    x = grouped.index,\n        y  = grouped[\"temp\"] ,\n        mode = \"lines+markers\",\n        name = \"Average Yearly Temperature\",\n    marker = dict(color = \"#3e4a5c\")\n)\n\ntrace2= go.Scatter(\n    x = grouped.index,\n        y  = grouped[\"SMA_5\"] ,\n        mode = \"lines+markers\",\nname = \"Moving Average of 5 years\",\n    marker = dict(color = \"#b1c3de\"))\n\n\ndata = [trace1, trace2]\n\nlayout = dict(title = 'Average Temperature per Year Rising',\n              xaxis= dict(title= 'Year', ticklen= 5,zeroline= False),\n              template = \"plotly_dark\"\n             )\n\n\nfig = dict(data = data,\n          layout = layout)\n\niplot(fig)","4a5af66f":"weather[\"season\"] = np.select( \n    [weather.mo == 1,weather.mo == 2,weather.mo == 3,weather.mo == 4,weather.mo == 5,weather.mo == 6,weather.mo == 7,weather.mo == 8,\n     weather.mo == 9,weather.mo == 10,weather.mo == 11,weather.mo == 12],\n    [\"winter\",\"winter\",\"spring\",\"spring\",\"spring\",\"summer\",\"summer\",\"summer\",\"fall\",\"fall\",\"fall\", \"winter\"]\n) \n\nseasonaly_grouped = weather.groupby([\"season\",\"year\"])[\"temp\",\"prcp\"].mean()\n\nseasonaly_grouped = seasonaly_grouped.reset_index()\n\nimport plotly.express as px\n\nfig = px.scatter(seasonaly_grouped, title = \"Average Temperature per Season - Still an Ascending Trend\",\n                 x='year', y='temp',color = \"season\",\n                facet_col='season', template=\"plotly_dark\")\n\nfor a in fig.layout.annotations:\n    a.text = a.text.split(\"=\")[1]\n\nfig.update_layout(showlegend=False)\n    \n    \nfig.show()","6261d33a":"weather[\"Month\"] = np.select( \n    [weather.mo == 1,weather.mo == 2,weather.mo == 3,weather.mo == 4,weather.mo == 5,weather.mo == 6,weather.mo == 7,weather.mo == 8,\n     weather.mo == 9,weather.mo == 10,weather.mo == 11,weather.mo == 12],\n    ['01.Jan', '02.Feb', '03.Mar', '04.Apr', '05.May', '06.Jun', '07.Jul', '08.Aug', '09.Sep', '10.Oct', '11.Nov', '12.Dec']\n) \n\nmonthly_grouped = weather.groupby([\"Month\",\"year\"])[\"temp\"].mean().reset_index()\n\n\n\n#monthly_grouped.columns = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n\n\nfig = px.line(monthly_grouped, title = \"Average Temperature per Month\",\n                 x='year', y='temp',color = \"Month\",\n                facet_col='Month', template=\"plotly_dark\")\n\nfor a in fig.layout.annotations:\n    a.text = a.text.split(\"=\")[1]\n\nfig.update_layout(showlegend=False)\n\nfig.show()","e1d41cdb":"#First we need to calculate the daily temperature that all our stations yield\ndaily_grouped = weather.groupby(\"Date\").mean().reset_index()\n\n#create a decade column\n\nfrom math import floor\n\nweather['decade'] = (((weather.year.apply(float))\/10).apply(floor))*10\n\nby_decade =weather.reset_index()[['temp','decade','mo']]\n\nby_decade['mo'] = by_decade['mo'].astype('int32')\n\nmonthly_means_by_decade = by_decade.groupby([\"decade\", \"mo\"]).apply(lambda x: x[\"temp\"].mean()).reset_index().rename(columns={0:'Mean Temp'})\n\nfig = px.line(monthly_means_by_decade, title = \"Temperatures by Month and Decade\",\n                 x='decade', y='Mean Temp',color = \"mo\",\n                facet_col='mo', template=\"plotly_dark\")\n\nfor a in fig.layout.annotations:\n    a.text = a.text.split(\"=\")[1]\n\nfig.update_layout(showlegend=False)\n\nfig.show()","818890a1":"\n#And then assign it to a new dataframe\ndf = daily_grouped[[\"Date\", \"temp\"]]\n\n#Set the Date as index\ndf.set_index(\"Date\", inplace=True)\n\n#Its frequency as daily\ndf = df.asfreq(\"d\")\n\n#And fill days without any observation with the most recent one\ndf = df.fillna(method = \"ffill\")","824f36de":"#Create a duplicate df for plotting\ndf_ = df.copy()\n#Date as Column\ndf_[\"Date\"] = df.index\n#Maximum Year existent in our dataset\nmax_year = df_[\"Date\"].apply(lambda x: x.year).max()\n#Train_Test Column\ndf_[\"Is_Test_Data\"] =  (df_[\"Date\"].apply(lambda x: x.year) == max_year)","71abc0f0":"#Plot\nfig = px.line(df_, title = \"Average Temperature per Day (Zoom in for Details)\",\n                 x= \"Date\", y='temp', color = \"Is_Test_Data\", template=\"plotly_dark\")\n\nfig.show()","14e64086":"#Then Split into train and test\ndf_train = df.iloc[:-365]\ndf_test = df.iloc[-365:]","6c0e57cf":"#Feed train to our model\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\nholt_model = ExponentialSmoothing(df_train,#data for training \n                                  seasonal_periods=365, #observations for each period - 365 days for each year \n                                  trend='additive', #we have negative values, only additive can work\n                                  seasonal='additive' ).fit()\n\n\n\n#After training, predict\nholt_pred = holt_model.forecast(365*2)\n\nplt.figure(figsize=(12, 8))\n\nplt.plot(df_test, marker='o', color='black', label= \"Actual\")\n\nplt.plot(holt_pred, marker='o', color='blue', label = \"Predicted\")\n\nplt.title(\"Predicted Temperatures - Holt Winters Model\")\n\nplt.legend(loc=\"upper right\")","c09b4563":"## Stations included in our Analysis","df65f5db":"While the model is **terribly wrong in its predictions**, it clearly predicts **ascending values of temperature** for the future, in accordance with what we observed in the EDA section.","9e9e8f1a":"# By Month","65c7c451":"## By Year","90c20b57":"On with the slightly less naive approach, contrasting comparable sizes (summer observations with summer observations, etc.):","20ae6cdb":"# Time Series Analysis","9e238709":"Next, we'll dive into our numbers and try to detect patterns.","7962e3f4":"Once again, the pattern seems to repeat. \n\n*Top right corners* contain the *highest* values, while *bottom left the lowest* for each month.\n\nBecause there are many years under examination, lines fluctuate a lot and it's hard for the human eye to detect a clear insight.\n\nWe will attempt to **merge** all the **yearly information into decades**. ","16009aa5":"Notice the **ascending trend**?\n\nLet's be more specific and compare temperatures of **each season**.\n\nWill we observe the same pattern repeating?","a50ae452":"# Data Preparation\n\nTo extract data from BigQuery, we will employ an SQL Query passed as a string to the BigQueryHelper function.\n\nWe will ask for all observations around the region of Greece from 1971 to 2020.\n\nAfter we successfully run the queries (and a little bit of a pre-process), we end up with a table like that:","908a3ec3":"# By Season","677133f0":"Still, the **later the year, the higher the temperature**.","56639846":"We will use all but the last year's data to train it, then make it predict last year's observations.\n\n**Train \/ test split is not optimized**, it's chosen at random.","eaadf568":"First, we will take a look at each station included in our dataset. \n\nAlong with it, we can access some basic info for each station.","63a7d915":"# Conclusion\n\nBoth the Exploratory Analysis and the Time Series Forecast indicate that we experience a **rise in temperatures** around the region of Greece.\n\nThe present kernel can be expanded with:\n\n* Examining **Different Regions**\n* Running the Analysis with **Maximum\/Minimum Temperature** in the epicentre instead of the Average Temperature  \n* Including **Extra Features** (precipitation, minimum \/ maximum temperatures, etc.) in the Analysis\n* More thorough Data **Pre-Processing** (for example excluding stations with extraordinary conditions such as mountain peaks)\n* More **Forecasting Techniques** (see [here](https:\/\/www.statsmodels.org\/stable\/user-guide.html#time-series-analysis) for a wide variety). Here, we only used one as an example.\n* More **Finetuning** of each Model","3202c5f6":"## By Decade and Month\n\nTo observe the above lines in a smoother form, we can group each year to its respective decade:","2a5d315d":"# Climate Change in Greece\n\nThis kernel aims to take advantage of the publicly available **Global Surface Summary of the Day** dataset released by the *National Oceanic and Atmospheric Administration* (NOAA) and, with the help of *Google BigQuery web service*, explore **temperature trends** in the wider **region of Greece**.\n\nIts main intend is to provide a **framework** for data practitioners to easily pivot the analysis towards the region of their preference, observe results and communicate them with each respective community.\n\nIt attempts to investigate temperature rises employing **Exploratory Data Analysis** and a very simplified **Time Series Forecasting**.\n\nNote that the key metric we examine here is the **average temperature of a whole day**, meaning that in order to calculate it we also consider measurements happened during night. Thus, hot summer days that reached temperatures of 40+ degrees Celcius in midday, will appear much lower due to the low temperatures of night that influence the average.","d94e32ee":"And sadly the trend becomes even more crystal clear.\n\n\nAn interesting thing to notice is that the **ascending trend is more prevalent in the summer months**.\n\n\nNow let's try to feed all our data in a time series model and observe whether the model will predict higher future temperatures.","a08b5dad":"The Yearly Average Temperature is kind of a naive approach, since **each year might contain different number of observations from different stations**.\nFor example, one year could contain many observations from northern\/mountainous stations or many winter observations (and vice versa) and as a result depict an incorrect situation.\nBut overall, it is a nice **stepping stone to start from**.","2e7aaaa7":"Will the trend that our eyes spot get **corroborated by statistics**?\n\nWe will deploy one basic time series prediction technique for the sole purpose of **observing the trends that arise from statistical calculations.** \n\nIn other words, the following model is not optimised to yield the most accurate predictions. We **won't fine-tune** it, we will just use its most default form.\n\nIts results will just act as an indicator of what our eyes have witnessed from the Exploratory Data Analysis section.\n\nBut first, our underlying data: ","30689448":"## Holt-Winters Exponential Smoothing\n\nThe Time Series Prediction technique for this specific purpose is [Holt Winters Exponential Smoothing](https:\/\/towardsdatascience.com\/holt-winters-exponential-smoothing-d703072c0572) because our Data are **definitely Seasonal** (no doubt about it - they are **temperatures**) and **most probably follow a (rising) Trend** (observed in the EDA section).\n\nLet's make it predict a year and observe whether prediction exhibits: \n* rising,\n* steady or\n* declining slope","022b19c6":"Diving deeper, we can compare month to month:","e9b88466":"# Data Exploration\n\nNow that we have a tidy table in our hands, it's time to explore it!"}}