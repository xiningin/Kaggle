{"cell_type":{"cf3260aa":"code","620ba6ad":"code","27b2f950":"code","f35d3dc5":"code","be5484eb":"code","dee2407d":"code","e86d94a9":"code","9b6f2705":"code","a3fbfc91":"code","1015e0bc":"code","b4147b37":"code","fbd1dcd7":"code","d9c02644":"code","8b6d5b82":"code","d75afaf4":"code","f09ffb94":"code","36ba15a4":"code","ffb45047":"code","62999463":"code","b4ce8504":"code","90bf2a35":"code","0b97aef8":"code","fcb90c1d":"code","86239406":"code","7adf32eb":"code","2ab7f1fa":"code","8549596c":"code","01a0cdea":"code","53df829b":"code","4128d0fa":"code","430cfbb7":"code","68b6a565":"code","68dfcc6e":"code","093fc841":"code","cccc6787":"code","32832aa3":"code","ef7fa9ac":"code","f5472c58":"code","a228acdb":"code","7959f218":"code","f63cd3f9":"code","62eab27a":"code","890ade9b":"code","6da598eb":"code","a16be33b":"code","bd8b0fd6":"code","6f0f5163":"code","48528497":"code","10146c20":"code","04b0de4f":"code","c12d4d0e":"code","ad943fca":"code","b6ab6098":"code","908f92ed":"code","882eca93":"code","c94453f7":"code","33449758":"code","4d114f18":"code","d8ae0c4c":"code","4ff07777":"code","f6bc36eb":"code","b2529a26":"code","038ff9fa":"markdown","8a901d81":"markdown","21308271":"markdown","f485cd68":"markdown","f592336c":"markdown","a00abfd0":"markdown","70915404":"markdown","a13cbba1":"markdown","dc3b0bf2":"markdown","5eb14116":"markdown","59fc8acf":"markdown","c78c7698":"markdown","3eb96c5e":"markdown","6a548830":"markdown","2512c3a5":"markdown","0df74f77":"markdown","a5d9879b":"markdown","df4eb492":"markdown","cbc82e4e":"markdown","cbd06980":"markdown","7e841ff3":"markdown","555483d2":"markdown","346aa186":"markdown","28d5a3fc":"markdown","f971abed":"markdown","58133b68":"markdown","bfc5e5f7":"markdown","b97eb344":"markdown"},"source":{"cf3260aa":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n%matplotlib inline","620ba6ad":"df = pd.read_csv(\"..\/input\/coffee-and-code-dataset\/CoffeeAndCodeLT2018 - CoffeeAndCodeLT2018.csv\")\ndf","27b2f950":"\n# Exploratory Data Analysis\ndef libraries():\n    global pd,np\n    import pandas as pd\n    import numpy as np\ndef load():\n    global df\n    df=pd.read_csv('..\/input\/coffee-and-code-dataset\/CoffeeAndCodeLT2018 - CoffeeAndCodeLT2018.csv')\n    \ndef top_rows(value):\n    print('\\033[1m'+ 'displaying the', value, 'rows from top'+'\\033[0m')\n    a=df.head(value)\n    print(a,'\\n')\n    \ndef bottom_rows(value):\n    print('\\033[1m'+'displaying the', value, 'rows from bottom'+'\\033[0m')\n    b=df.tail(value)\n    print(b,'\\n')\n    \ndef rows_columns():\n    print('\\033[1m'+'Shape of the Data set'+'\\033[0m')\n    c=df.shape\n    print(c,'\\n')\n    \ndef col_names():\n    print('\\033[1m'+'Column Names in the Data set'+'\\033[0m')\n    d=df.columns\n    print(d,'\\n')\n    \ndef information():\n    print('\\033[1m'+'Quick Overview of DataSet(info)'+'\\033[0m')\n    e = df.info()\n    print(e,'\\n')\n\ndef sizee():\n    print('\\033[1m'+'No.of Elements in the DataSet'+'\\033[0m')\n    f = df.size\n    print(f,'\\n')\n\ndef ndimension():\n    print('\\033[1m'+'Dimensions in your dataframe'+'\\033[0m')\n    g = df.ndim\n    print(g,'\\n')\n    \ndef stats_summary():\n    print('\\033[1m'+'Staistical Summary of DataSet'+'\\033[0m')\n    h = df.describe()\n    print(h,'\\n')\n    \ndef null_values():\n    print('\\033[1m'+'Number of Missing values in each column'+'\\033[0m')\n    i = df.isnull().sum()\n    print(i,'\\n')\n    \ndef n_unique():\n    print('\\033[1m'+'Number of unique elements'+'\\033[0m')\n    j = df.nunique()\n    print(j,'\\n')\n    \ndef memory_use():\n    print('\\033[1m'+'Memory used by all colomns in bytes'+'\\033[0m')\n    k = df.memory_usage()\n    print(k,'\\n')\n    \ndef is_na(value):\n    print('\\033[1m'+'Dataframe filled with boolean values with true indicating missing values'+'\\033[0m')\n    l = df.isna().head(value)\n    print(l,'\\n')\n    \ndef duplicate():\n    print('\\033[1m'+'Boolean Series denoting duplicate rows'+'\\033[0m')\n    m = df.duplicated().sum()\n    print(m,'\\n')\n    \ndef valuecounts():\n    print('\\033[1m'+'Series containing count of unique values'+'\\033[0m')\n    n = df.value_counts()\n    print(n,'\\n')\n\ndef datatypes():\n    print('\\033[1m'+'Datatype of each column'+'\\033[0m')\n    o = df.dtypes\n    print(o,'\\n')\n    \ndef correlation():\n    print('\\033[1m'+'Correalation between all columns in DataFrame'+'\\033[0m')\n    p = df.corr()\n    print(p,'\\n')\n    \ndef nonnull_count():\n    print('\\033[1m'+'Count of non-null values'+'\\033[0m')\n    q = df.count()\n    print(q,'\\n')\n    \ndef eda():\n    load()\n    value= 5 \n    datatypes()\n    top_rows(value)\n    bottom_rows(value)\n    rows_columns()\n    col_names()\n    information()\n    sizee()\n    ndimension()\n    stats_summary()\n    null_values()\n    n_unique()\n    memory_use()\n    is_na(value)\n    nonnull_count()\n    duplicate()\n    valuecounts()\n    correlation()\n    \n    \n    \n        \ndef stats_u(data,col):\n    if data[col].dtype == \"float64\":\n        print(col,\"has Quantitative data\")\n        mean_value=data[col].mean()\n        print('mean of',col,'column',mean_value)\n        max_value = data[col].max()\n        print('Maximum value of',col,'column',max_value)\n        min_value = data[col].min()\n        print('Minimum value of',col,'column',min_value)\n        median_value = data[col].median(skipna = True)\n        print('median of',col,'column',median_value)\n        std_value = data[col].std()\n        print('standard deviation of',col,'column',std_value)\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        print('quartile 1 of',col,'column is',q1)\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        print('quartile 2 of',col,'column is',q2)\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        print('quartile 3 of',col,'column is',q3)\n        q4 = data[col].quantile(1,interpolation='nearest')\n        print('quartile 4 of',col,'column is',q4)\n        IQR = q3 -q1\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        print('Lower Limit Point:',LLP)\n        print('Upper Limit Point:',ULP)\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers\")\n        else:\n            print(\"There are outliers\")\n            print(data[data[col]<LLP][col])\n            print(data[data[col]>ULP][col])\n            \n    elif data[col].dtype == \"int64\":\n        print(col,\"has Quantitative data\")\n        mean_value=data[col].mean()\n        print('mean of',col,'column',mean_value)\n        median_value = data[col].median(skipna = True)\n        print('median of',col,'column',median_value)\n        std_value = data[col].std()\n        print('standard deviation of',col,'column',std_value)\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        print('quartile 1 of',col,'column is',q1)\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        print('quartile 2 of',col,'column is',q2)\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        print('quartile 3 of',col,'column is',q3)\n        q4 = data[col].quantile(1,interpolation='nearest')\n        print('quartile 4 of',col,'column is',q4)\n        IQR = q3 -q1\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        print('Lower Limit Point:',LLP)\n        print('Upper Limit Point:',ULP)\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers\")\n        else:\n            print(\"There are outliers\")\n            print(\"Outliers are:\")\n            print(data[data[col]<LLP][col])\n            print(data[data[col]>ULP][col])\n    else:\n        print(col,'has Qualitative Data')\n        z = df[col].mode()\n        print('mode of',col,'column:\\n',z)\n        print('Count of mode is:\\n',df[col].value_counts())\n        print('Unique strings in',col,'are',data[col].nunique())\n        if(data[col].nunique() == 1):\n            print(col,'has same string')\n        elif(data[col].nunique() == 2):\n            print(col,'has binary strings')\n        else:\n            print(col,'has multi stings')\n\n\nlibraries()\neda()\n\nprint(\"----------------------------------------------------------------------------------------------------------------------\")\nprint('\\033[1m'+'Summary Of DataSet'+'\\033[0m')\nprint('\\033[1m'+'DataTypes in the DataSet:\\n'+'\\033[0m',df.dtypes)\nprint('\\033[1m'+'Columns in DataSet:'+'\\033[0m',df.columns)\nprint('\\033[1m'+'Shape of DataSet:'+'\\033[0m',df.shape)\nprint('\\033[1m'+'Size of DataSet:'+'\\033[0m',df.size)\nprint('\\033[1m'+'Dimension of DataSet:'+'\\033[0m',df.ndim)\nprint('\\033[1m'+'Total Memory used in DataSet:'+'\\033[0m',df.memory_usage().sum())\nprint('\\033[1m'+'Total Number of missing values in DataSet:'+'\\033[0m',df.isnull().sum().sum())\nprint('\\033[1m'+'Total Number of Unique values in DataSet:'+'\\033[0m',df.nunique().sum())\nprint('\\033[1m'+'Total Number of non null values in DataSet:'+'\\033[0m',df.count().sum())\nprint('\\033[1m'+'Total Number of duplicate rows in DataSet:'+'\\033[0m',df.duplicated().sum())\nprint(\"----------------------------------------------------------------------------------------------------------------------\")\nprint('\\033[1m'+'Summary Of Each Colomn'+'\\033[0m')\nprint(\"\\n\")\ncols=df.columns\ncols\nfor i in cols:\n    print('\\033[1m'+i+'\\033[0m')\n    stats_u(df,i)\n    print(\"\\n\")\n            ","f35d3dc5":"df.head()","be5484eb":"df.tail()","dee2407d":"df.dtypes","e86d94a9":"df.columns","9b6f2705":"df.size","a3fbfc91":"df.shape","1015e0bc":"df.info()","b4147b37":"df.describe()","fbd1dcd7":"df.isnull().sum()","d9c02644":"df.duplicated().sum()","8b6d5b82":"df.skew()","d75afaf4":"df.corr()","f09ffb94":"# Since there are less number of null values we can drop those rows\ndf.dropna(inplace =True)","36ba15a4":"df","ffb45047":"! pip install Autoviz","62999463":"! pip install xlrd","b4ce8504":"from autoviz.AutoViz_Class import AutoViz_Class\nAV = AutoViz_Class()\ndf_av = AV.AutoViz(\"..\/input\/coffee-and-code-dataset\/CoffeeAndCodeLT2018 - CoffeeAndCodeLT2018.csv\")","90bf2a35":"df['CodingHours'].value_counts()","0b97aef8":"sns.countplot(x = 'CodingHours',data = df)\nplt.show()","fcb90c1d":"df['CoffeeCupsPerDay'].value_counts()","86239406":"sns.countplot(x = 'CoffeeCupsPerDay',data = df)\nplt.show()","7adf32eb":"df['CoffeeTime'].value_counts()","2ab7f1fa":"sns.set(rc={'figure.figsize':(14,14)})\nsns.countplot(x = 'CoffeeTime',data = df)\nplt.show()","8549596c":"df['CodingWithoutCoffee'].value_counts()","01a0cdea":"sns.set(rc={'figure.figsize':(6,6)})\nsns.countplot(x = 'CodingWithoutCoffee',data = df)\nplt.show()","53df829b":"df['CoffeeType'].value_counts()","4128d0fa":"sns.set(rc={'figure.figsize':(15,15)})\nsns.countplot(x = 'CoffeeType',data = df)\nplt.show()","430cfbb7":"df['CoffeeSolveBugs'].value_counts()","68b6a565":"sns.set(rc={'figure.figsize':(6,6)})\nsns.countplot(x = 'CoffeeSolveBugs',data = df)\nplt.show()","68dfcc6e":"df['Gender'].value_counts()","093fc841":"sns.countplot(x = 'Gender',data = df)\nplt.show()","cccc6787":"df['Country'].value_counts()","32832aa3":"sns.countplot(x = 'Country',data = df)\nplt.show()","ef7fa9ac":"df['AgeRange'].value_counts()","f5472c58":"sns.countplot(x = 'AgeRange',data = df)\nplt.show()","a228acdb":"sns.set(rc={'figure.figsize':(6,6)})\ndata=df.copy()\ndata.groupby('CodingWithoutCoffee')['CodingHours'].mean().plot.bar()\nplt.xlabel('CodingWithoutCoffee')\nplt.ylabel('CodingHours')\nplt.title('CodingWithoutCoffee')\nplt.show()","7959f218":"sns.set(rc={'figure.figsize':(6,6)})\ndata=df.copy()\ndata.groupby('CoffeeType')['CodingHours'].mean().plot.bar()\nplt.xlabel('CoffeeType')\nplt.ylabel('CodingHours')\nplt.title('CoffeeType')\nplt.show()","f63cd3f9":"sns.set(rc={'figure.figsize':(6,6)})\ndata=df.copy()\ndata.groupby('CoffeeSolveBugs')['CodingHours'].mean().plot.bar()\nplt.xlabel('CoffeeSolveBugs')\nplt.ylabel('CodingHours')\nplt.title('CoffeeSolveBugs')\nplt.show()","62eab27a":"sns.set(rc={'figure.figsize':(6,6)})\ndata=df.copy()\ndata.groupby('Gender')['CodingHours'].mean().plot.bar()\nplt.xlabel('Gender')\nplt.ylabel('CodingHours')\nplt.title('Gender')\nplt.show()\n\n# Male code more than females","890ade9b":"sns.set(rc={'figure.figsize':(6,6)})\ndata=df.copy()\ndata.groupby('AgeRange')['CodingHours'].mean().plot.bar()\nplt.xlabel('AgeRange')\nplt.ylabel('CodingHours')\nplt.title('AgeRange')\nplt.show()\n\n# under 18 age people code more","6da598eb":"df1 = df.groupby('CoffeeTime').agg({'CoffeeCupsPerDay' :'mean'})\ndf1","a16be33b":"px.bar(data_frame=df1, barmode='group',\n       title = \"<b>Coffee Time wise Analyzing<\/b>\",template=\"plotly_dark\")","bd8b0fd6":"df2 = df.groupby('CodingWithoutCoffee').agg({'CodingHours':'mean','CoffeeCupsPerDay' :'mean'})\ndf2","6f0f5163":"px.bar(data_frame=df2, barmode='group',\n       title = \"<b>Coffee Time wise Analyzing<\/b>\",template=\"plotly_dark\")","48528497":"df3 = df.groupby('CoffeeType').agg({'CodingHours':'mean','CoffeeCupsPerDay' :'mean'})\ndf3","10146c20":"px.bar(data_frame=df3, barmode='group',\n       title = \"<b>Coffee Time wise Analyzing<\/b>\",template=\"plotly_dark\")","04b0de4f":"df4 = df.groupby('CoffeeSolveBugs').agg({'CodingHours':'mean','CoffeeCupsPerDay' :'mean'})\ndf4","c12d4d0e":"px.bar(data_frame=df4, barmode='group',\n       title = \"<b>Coffee Time wise Analyzing<\/b>\",template=\"plotly_dark\")","ad943fca":"df5 = df.groupby('Gender').agg({'CodingHours':'mean','CoffeeCupsPerDay' :'mean'})\ndf5","b6ab6098":"px.bar(data_frame=df5, barmode='group',\n       title = \"<b>Coffee Time wise Analyzing<\/b>\",template=\"plotly_dark\")","908f92ed":"df6 = df.groupby('AgeRange').agg({'CodingHours':'mean','CoffeeCupsPerDay' :'mean'})\ndf6","882eca93":"px.bar(data_frame=df6, barmode='group',\n       title = \"<b>Coffee Type wise Analyzing<\/b>\",template=\"plotly_dark\")","c94453f7":"px.bar(data_frame=df, x = 'CoffeeType' ,y = 'CoffeeCupsPerDay',color = 'Gender',\n       title = \"<b>Coffee Type wise Analyzing<\/b>\",template=\"plotly_dark\")","33449758":"px.bar(data_frame=df, x = 'CoffeeType' ,y = 'CoffeeCupsPerDay',color = 'AgeRange',\n       title = \"<b>Coffee Type wise Analyzing<\/b>\",template=\"plotly_dark\")","4d114f18":"px.bar(data_frame=df, x = 'CoffeeType' ,y = 'CoffeeCupsPerDay',color = 'CodingWithoutCoffee',\n       title = \"<b>Coffee Type wise Analyzing<\/b>\",template=\"plotly_dark\")","d8ae0c4c":"from IPython.core.display import HTML\n\ndef multi_table(table_list):\n    ''' Acceps a list of IpyTable objects and returns a table which contains each IpyTable in a cell\n    '''\n    return HTML(\n        '<table><tr style=\"background-color:white;\">' + \n        ''.join(['<td>' + table._repr_html_() + '<\/td>' for table in table_list]) +\n        '<\/tr><\/table>')","4ff07777":"df_nunique = {var: pd.DataFrame(df[var].value_counts()) \n              for var in {'CoffeeTime', 'CodingWithoutCoffee',\n       'CoffeeType', 'CoffeeSolveBugs', 'Gender', 'Country', 'AgeRange'}}\nmulti_table([df_nunique['CoffeeTime'], df_nunique['CodingWithoutCoffee'],df_nunique['CoffeeType'],df_nunique['CoffeeSolveBugs']\n            ,df_nunique['Gender'],df_nunique['Gender'],df_nunique['AgeRange']])","f6bc36eb":"df_groupby = {var: pd.DataFrame(df.groupby([var, 'CodingHours']).size()) \n              for var in {'CoffeeTime', 'CodingWithoutCoffee',\n       'CoffeeType', 'CoffeeSolveBugs', 'Gender', 'Country', 'AgeRange'}}\nmulti_table([df_groupby['CoffeeTime'], df_groupby['CodingWithoutCoffee'],df_groupby['CoffeeType'],df_groupby['CoffeeSolveBugs']\n            ,df_groupby['Gender'],df_groupby['Gender'],df_groupby['AgeRange']])","b2529a26":"df_groupby = {var: pd.DataFrame(df.groupby([var, 'CoffeeCupsPerDay']).size()) \n              for var in {'CoffeeTime', 'CodingWithoutCoffee',\n       'CoffeeType', 'CoffeeSolveBugs', 'Gender', 'Country', 'AgeRange'}}\nmulti_table([df_groupby['CoffeeTime'], df_groupby['CodingWithoutCoffee'],df_groupby['CoffeeType'],df_groupby['CoffeeSolveBugs']\n            ,df_groupby['Gender'],df_groupby['Gender'],df_groupby['AgeRange']])","038ff9fa":"#### Data consists most of males","8a901d81":"#### Most of the people code for 8 hrs","21308271":"#### Most of people drink coffee While coding","f485cd68":"#### This data is from only one country i.e., LEbanon","f592336c":"#### Most of the people sometimes solved bugs while drinking coffee","a00abfd0":"# Data Visualisation","70915404":"#### Most of the people drink Double Espresso (Doppio) about average of 5 cups a day while coding","a13cbba1":"# (IMPORTANT) Advanced Visualisation","dc3b0bf2":"#### Most of the people code with coffee","5eb14116":"#### Most of the times coffee solve bugs","59fc8acf":"# Importing Libraries","c78c7698":"#### Americano is only drink by people between age 30-39\n#### caffe latte is only drink by people between age 18-29\n#### people between age 18-29 drinks all types of coffee expect americano\n#### Under age 18 people drink only cappuccino\n#### Nescafe coffee is only drink by people between age 18-39","3eb96c5e":"# Exploratory Data Analysis Using User defined Function","6a548830":"#### From above multitables we can observe everything","2512c3a5":"#### Male code more and males drink more coffe cups per day","0df74f77":"# Exploratory Data Analysis","a5d9879b":"#### Americano is only drink by females\n#### Double Espresso, cappuccino is only drink by male","df4eb492":"#### Most of people sometimes code without coffee","cbc82e4e":"#### Under 18 age code more hours and between age 40-49 drink more cups of coffee per day","cbd06980":"#### In the morning and before coding people will not drink more cups of coffee","7e841ff3":"# Data Visualisation Using Autoviz","555483d2":"# Loading DataSet","346aa186":"#### Double expresso is the only coffee drink by people while coding","28d5a3fc":"#### Most of people who code are between age 18-29","f971abed":"## Getting unique values of each category","58133b68":"# Basic Data Cleaning","bfc5e5f7":"#### Most of the people drink 2 coffees a day","b97eb344":"#### Most of people drink NEscafe coffee"}}