{"cell_type":{"d420a58e":"code","1273f34f":"code","6bbd6564":"code","2f800ac3":"code","68d8b38c":"code","08015ae0":"code","3effca68":"code","4e88613e":"code","5ab730cc":"code","80be002c":"code","12c493f3":"code","940fde31":"code","4f052786":"code","3f5deee8":"code","02e6f356":"code","ee367269":"code","b76f0856":"code","fcd2bee9":"code","62009596":"code","e5d696c1":"code","cc87dc20":"code","3d6218a4":"code","a1ade180":"code","b3b03d29":"code","5e438700":"code","49a575c9":"code","221c3f47":"code","787a0b4e":"code","61269338":"code","538571f4":"code","db13ef0c":"code","74e8ce4e":"code","b2dc90da":"code","3feaa101":"code","271d296a":"code","fa58d04a":"code","b0d261fd":"code","4b3a005d":"code","c140559b":"code","e857fafd":"code","90e6b486":"code","28812d30":"code","8443b19a":"code","d6f79155":"markdown","97f9f06b":"markdown","6632b2ac":"markdown","a590bff5":"markdown","4be02e53":"markdown","dc13eb78":"markdown","45e03173":"markdown","7f19d2b7":"markdown","098dbfc9":"markdown","96231e75":"markdown","da193afa":"markdown","fe5fb392":"markdown","4e29f363":"markdown","2657950f":"markdown","90cbcc5e":"markdown","3436558e":"markdown"},"source":{"d420a58e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","1273f34f":"train=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest2=test.copy()\nfull = [train,test]\ntrain.shape","6bbd6564":"pd.set_option('display.max_columns', 500)\ntrain.head()\n","2f800ac3":"train_miss = train.isnull().sum(axis = 0).sort_values(ascending=False)\ntest_miss = test.isnull().sum(axis = 0).sort_values(ascending=False)","68d8b38c":"plt.figure(figsize=(15,4))\nsns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","08015ae0":"nanvals = pd.concat([train_miss,test_miss], axis=1).sort_values(by=0, ascending=False)","3effca68":"train = train.drop(['PoolQC','MiscFeature','Alley','Fence','FireplaceQu'], axis=1)\ntest = test.drop(['PoolQC','MiscFeature','Alley','Fence','FireplaceQu'], axis=1)","4e88613e":"train.head()","5ab730cc":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","80be002c":"train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n","12c493f3":"train_strings = train.select_dtypes(include ='object') \ntrain_numeric = train.drop(train_strings, axis=1)","940fde31":"nanvals = nanvals[(nanvals[0]!=0)|(nanvals[1]!=0)]","4f052786":"list(nanvals.index.unique())\ncols_with_miss = ['LotFrontage','GarageFinish','GarageQual','GarageType','GarageYrBlt','GarageCond',\\\n 'BsmtExposure','BsmtFinType2','BsmtQual','BsmtCond','BsmtFinType1','MasVnrType','MasVnrArea']","3f5deee8":"strings = []\nnumerics = []\nfor col in cols_with_miss:\n    if col in list(train_strings.columns):\n        strings.append(col)\n    else:\n        numerics.append(col)","02e6f356":"plt.figure(figsize=(15,4))\nsns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","ee367269":"for col in numerics:\n    train[col] = train[col].fillna(train[col].mean())\nfor col in strings:\n    train[col] = train[col].fillna(train[col].mode())","b76f0856":"for col in numerics:\n    test[col] = test[col].fillna(test[col].mean())\nfor col in strings:\n    test[col] = test[col].fillna(test[col].mode())\n    ","fcd2bee9":"train = train.fillna(method='ffill')\ntest = test.fillna(method='ffill')","62009596":"plt.figure(figsize=(15,4))\nsns.heatmap(test.isnull(),yticklabels=False,cbar=False,cmap='viridis')","e5d696c1":"plt.figure(figsize=(15,12))\nsns.heatmap(train_numeric.corr(),cmap='viridis')","cc87dc20":"full = [train, test]\nfor df in full:\n    df['MSSubClass'] = df['MSSubClass'].apply(str)\n    df['OverallCond'] = df['OverallCond'].astype(str)\n    df['YrSold'] = df['YrSold'].astype(str)\n    df['MoSold'] = df['MoSold'].astype(str)","3d6218a4":"numeric = list(train.drop(train_strings, axis=1).columns)\nnumeric = numeric[:-1]\nlist(train.select_dtypes(include ='object').columns)\nobjects = ['MSSubClass','MSZoning','Street','LotShape','LandContour','Utilities','LotConfig',\n           'LandSlope','Neighborhood','Condition1','Condition2','BldgType','HouseStyle','OverallCond',\n           'RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','ExterQual','ExterCond',\n           'Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','Heating',\n           'HeatingQC','CentralAir','Electrical','KitchenQual','Functional','GarageType','GarageFinish',\n           'GarageQual','GarageCond','PavedDrive','MoSold','YrSold','SaleType','SaleCondition'] ","a1ade180":"train[objects].describe()","b3b03d29":"list_of_huge_cats = ['Neighborhood', 'Exterior1st', 'Exterior2nd']","5e438700":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\nfull = [train, test]\nfor df in full:\n    for col in objects:\n        df[col] = label_encoder.fit_transform(df[col])","49a575c9":"train.head()","221c3f47":"for col in list_of_huge_cats:  \n    train[col+'_cut'] =pd.cut(train[col], 4)","787a0b4e":"for col in list_of_huge_cats:\n    print(train[col+'_cut'].value_counts())","61269338":"full = [train, test]\nfor df in full:    \n    df.loc[ df['Neighborhood'] <=6, 'Neighborhood'] = 0\n    df.loc[(df['Neighborhood'] > 6) & (df['Neighborhood'] <= 12), 'Neighborhood'] = 1\n    df.loc[(df['Neighborhood'] > 12) & (df['Neighborhood'] <= 18), 'Neighborhood'] = 2\n    df.loc[ df['Neighborhood'] > 18, 'Neighborhood'] = 3\n    \nfor df in full:    \n    df.loc[ df['Exterior1st'] <=3.5, 'Exterior1st'] = 0\n    df.loc[(df['Exterior1st'] > 3.5) & (df['Exterior1st'] <= 7), 'Exterior1st'] = 1\n    df.loc[(df['Exterior1st'] > 7) & (df['Exterior1st'] <= 10.5), 'Exterior1st'] = 2\n    df.loc[ df['Exterior1st'] > 10.5, 'Exterior1st'] = 3\n    \nfor df in full:    \n    df.loc[ df['Exterior2nd'] <=3.75, 'Exterior2nd'] = 0\n    df.loc[(df['Exterior2nd'] > 3.75) & (df['Exterior2nd'] <= 7.5), 'Exterior2nd'] = 1\n    df.loc[(df['Exterior2nd'] > 7.5) & (df['Exterior2nd'] <= 11.25), 'Exterior2nd'] = 2\n    df.loc[ df['Exterior2nd'] > 11.25, 'Exterior2nd'] = 3","538571f4":"train = train.drop(train[['Neighborhood_cut','Exterior1st_cut','Exterior2nd_cut']], axis=1)","db13ef0c":"train['TotalSF'] = train['TotalBsmtSF'] + train['1stFlrSF'] + train['2ndFlrSF']\ntest['TotalSF'] = test['TotalBsmtSF'] + test['1stFlrSF'] + test['2ndFlrSF']","74e8ce4e":"from sklearn.preprocessing import StandardScaler\nX = train.drop('SalePrice', axis=1)\ny = np.log1p(train[\"SalePrice\"])\nscaler = StandardScaler()\nscaler.fit(train[numeric])\nX[numeric] = scaler.transform(X[numeric])\ntest[numeric] = scaler.transform(test[numeric])","b2dc90da":"X.head()","3feaa101":"X = X.drop('Id',axis=1)\ntest = test.drop('Id', axis=1)","271d296a":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_log_error\nimport xgboost as xgb\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","fa58d04a":"from sklearn.model_selection import KFold, cross_val_score\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","b0d261fd":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n\nscore = rmsle_cv(model_xgb)\nprint(\"\\nXGboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","4b3a005d":"gboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nscore = rmsle_cv(gboost)\nprint(\"\\nGradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","c140559b":"import lightgbm as lgb\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nscore = rmsle_cv(model_lgb)\nprint(\"\\nLight GBM score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","e857fafd":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","90e6b486":"averaged_models = AveragingModels(models = (model_xgb, gboost, model_lgb))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","28812d30":"# averaged_models.fit(X_train,y_train)\n# preds = averaged_models.predict(X_test)\n# preds_test = averaged_models.predict(test)\n# print(mean_absolute_error(y_test,preds))\n\ngboost.fit(X_train,y_train)\npreds = gboost.predict(X_test)\npreds_test = gboost.predict(test)\nprint(mean_absolute_error(y_test,preds))","8443b19a":"submission = pd.DataFrame()\nsubmission['Id'] = test2.Id\nsubmission['SalePrice'] = np.expm1(preds_test)\nsubmission.to_csv('submission.csv',index=False)","d6f79155":"## Separating DF on numeric and categorical features","97f9f06b":"## Filling in the missing values","6632b2ac":"# Feature engineering","a590bff5":"## Stacked model 'Test' data predict","4be02e53":"## Add the summarizing squarefeets feature 'Total'","dc13eb78":"## Light GBM","45e03173":"## Averaging the models","7f19d2b7":"# Submission","098dbfc9":"## Standartizing numeric features","96231e75":"## XGboost\n","da193afa":"# Preprocessing data\n## Check for missing values","fe5fb392":"## Dividing categoricals on those, which fits label encoding, and those, which has many labels (I'll cut them into periods) \n","4e29f363":"## GradientBoosting","2657950f":"## Cutting huge categorical features into periods","90cbcc5e":"# Building prediction model\n## Cross-validation model","3436558e":"## Deleting outliers"}}