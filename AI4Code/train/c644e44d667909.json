{"cell_type":{"b152d7c7":"code","adbf785b":"code","4a2bc7bd":"code","91f9d5f0":"code","829230de":"code","f5ae4560":"code","6e613861":"code","54981546":"code","5d17a80c":"code","5bc06e84":"code","00191b8a":"code","21261449":"code","8c350830":"code","b9d9d89d":"code","bccf85e9":"code","88dbced6":"code","3bcce2da":"code","a6cdc391":"code","1c2521f1":"code","a8f3370e":"code","c9d10596":"code","97919fbb":"code","bebcfd04":"code","b49f07c9":"markdown","06c7f64f":"markdown","20ad9ff1":"markdown","379be773":"markdown","b0bdda8e":"markdown","0e497a32":"markdown","04c8b65f":"markdown","05301bea":"markdown","54869cc8":"markdown","5160eb98":"markdown","b47b3a99":"markdown","9be990a7":"markdown","e87d4696":"markdown"},"source":{"b152d7c7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","adbf785b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\npd.options.display.max_columns = 100\n\ntrain = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv\")\ntrain.head()","4a2bc7bd":"print(\"Columns in train data\")\nprint(\"-\"*23)\nprint(train.columns)\nprint('-'*100)\nprint(\"Columns in test data\")\nprint(\"-\"*23)\nprint(test.columns)","91f9d5f0":"print('Datatypes of Train dataset:\\n', train.info())\nprint('-'*50)\nprint('\\n\\nDatatypes of Train dataset:\\n', test.info())","829230de":"target_counts = train[\"target\"].value_counts(dropna=False).to_frame().reset_index(drop=False).sort_values(by=\"index\").reset_index(drop=True)\ntarget_counts.rename(columns = {'target' : 'count'}, inplace = True)\ntarget_counts.rename(columns = {'index' : 'target'}, inplace = True)\ntarget_counts[\"percentage\"] = target_counts[\"count\"] \/ target_counts[\"count\"].sum()\n\nsns.barplot(data=target_counts, x=\"target\", y=\"count\")\ndisplay(target_counts)","f5ae4560":"# Saving all features to all_features list.\nall_features=[]\nfor i in range(0,50):\n    all_features.append(\"feature_\" + str(i))","6e613861":"def feature_explorer(train,test):\n    \n    count_train = []\n    mean_train = []\n    std_train = []\n    min_train = []\n    max_train = []\n    sum_train = []\n    zero_train = []\n    nunique_train = []\n    count_test = []\n    mean_test = []\n    std_test = []\n    min_test = []\n    max_test = []\n    sum_test = []\n    zero_test = []\n    nunique_test = []\n\n\n    for feature in all_features:\n        count_train.append(train[feature].count())\n        nunique_train.append(train[feature].nunique())\n        mean_train.append(train[feature].mean())\n        std_train.append(train[feature].std())\n        min_train.append(train[feature].min())\n        max_train.append(train[feature].max())\n        sum_train.append(train[feature].sum())\n        zero_train.append(train[train[feature]==0][feature].count()\/len(train))\n\n        count_test.append(test[feature].count())\n        nunique_test.append(test[feature].nunique())\n        mean_test.append(test[feature].mean())\n        std_test.append(test[feature].std())\n        min_test.append(test[feature].min())\n        max_test.append(test[feature].max())\n        sum_test.append(test[feature].sum())\n        zero_test.append(test[test[feature]==0][feature].count()\/len(test))\n        \n    info_df = pd.DataFrame({\"count_train\":count_train,\"count_test\":count_test,\n                         \"nunique_train\":nunique_train,\"nunique_test\":nunique_test,\n                         \"mean_train\":mean_train,\"mean_test\":mean_test,\n                         \"std_train\":std_train,\"std_test\":std_test,\n                         \"min_train\":min_train,\"min_test\":min_test,\n                         \"max_train\":max_train,\"max_test\":max_test,\n                         \"zero_ratio_train\":zero_train,\"zero_ratio_test\":zero_test}, index=all_features)\n    display(info_df.style.bar(subset=['nunique_train',\"nunique_test\"], color=px.colors.qualitative.Pastel2[0]).bar(subset=['mean_train',\"mean_test\"], color=px.colors.qualitative.Safe[0]).background_gradient(subset=[\"std_train\",\"std_test\"], cmap='Greens').bar(subset=['zero_ratio_train',\"zero_ratio_test\"], color=px.colors.qualitative.Light24[0], vmin=0,vmax=1))","54981546":"feature_explorer(train,test)","5d17a80c":"def feature_explorer_2(train):\n    \n    count_1 = []\n    count_2 = []\n    count_3 = []\n    count_4 = []\n    nunique_1 = []\n    nunique_2 = []\n    nunique_3 = []\n    nunique_4 = []\n    mean_1 = []\n    mean_2 = []\n    mean_3 = []\n    mean_4 = []\n    std_1 = []\n    std_2 = []\n    std_3 = []\n    std_4 = []\n    zero_1 = []\n    zero_2 = []\n    zero_3 = []\n    zero_4 = []\n\n\n    for feature in all_features:\n        count_1.append(train[train[\"target\"]==\"Class_1\"][feature].count()\/len(train))\n        count_2.append(train[train[\"target\"]==\"Class_2\"][feature].count()\/len(train))\n        count_3.append(train[train[\"target\"]==\"Class_3\"][feature].count()\/len(train))\n        count_4.append(train[train[\"target\"]==\"Class_4\"][feature].count()\/len(train))\n        \n        nunique_1.append(train[train[\"target\"]==\"Class_1\"][feature].nunique())\n        nunique_2.append(train[train[\"target\"]==\"Class_2\"][feature].nunique())\n        nunique_3.append(train[train[\"target\"]==\"Class_3\"][feature].nunique())\n        nunique_4.append(train[train[\"target\"]==\"Class_4\"][feature].nunique())\n        \n        mean_1.append(train[train[\"target\"]==\"Class_1\"][feature].mean())\n        mean_2.append(train[train[\"target\"]==\"Class_2\"][feature].mean())\n        mean_3.append(train[train[\"target\"]==\"Class_3\"][feature].mean())\n        mean_4.append(train[train[\"target\"]==\"Class_4\"][feature].mean())\n\n        std_1.append(train[train[\"target\"]==\"Class_1\"][feature].std())\n        std_2.append(train[train[\"target\"]==\"Class_2\"][feature].std())\n        std_3.append(train[train[\"target\"]==\"Class_3\"][feature].std())\n        std_4.append(train[train[\"target\"]==\"Class_4\"][feature].std())\n        \n        zero_1.append(train[(train[\"target\"]==\"Class_1\")&(train[feature]==0)][feature].count()\/len(train[train[\"target\"]==\"Class_1\"]))\n        zero_2.append(train[(train[\"target\"]==\"Class_2\")&(train[feature]==0)][feature].count()\/len(train[train[\"target\"]==\"Class_2\"]))\n        zero_3.append(train[(train[\"target\"]==\"Class_3\")&(train[feature]==0)][feature].count()\/len(train[train[\"target\"]==\"Class_3\"]))\n        zero_4.append(train[(train[\"target\"]==\"Class_4\")&(train[feature]==0)][feature].count()\/len(train[train[\"target\"]==\"Class_4\"]))\n\n        \n    info_df = pd.DataFrame({\"count_1\":count_1,\"count_2\":count_2,\"count_3\":count_3,\"count_4\":count_4,\n                         \"nunique_1\":nunique_1,\"nunique_2\":nunique_2,\"nunique_3\":nunique_3,\"nunique_4\":nunique_4,\n                         \"mean_1\":mean_1,\"mean_2\":mean_2,\"mean_3\":mean_3,\"mean_4\":mean_4,\n                         \"std_1\":std_1,\"std_2\":std_2,\"std_3\":std_3,\"std_4\":std_4,\n                         \"zero_1\":zero_1,\"zero_2\":zero_2,\"zero_3\":zero_3,\"zero_4\":zero_4}, index=all_features)\n    display(info_df.style.background_gradient(subset=[\"count_1\",\"count_2\",\"count_3\",\"count_4\",], cmap='Greys', vmin=0, vmax=1).bar(subset=['nunique_1',\"nunique_2\",\"nunique_3\",\"nunique_4\"], color=px.colors.qualitative.Pastel2[0]).bar(subset=['mean_1','mean_2','mean_3','mean_4'], color=px.colors.qualitative.Safe[0]).background_gradient(subset=[\"std_1\",\"std_2\",\"std_3\",\"std_4\",], cmap='Greens').bar(subset=['zero_1',\"zero_2\",\"zero_3\",\"zero_4\"], color=px.colors.qualitative.Light24[0], vmin=0,vmax=1))","5bc06e84":"feature_explorer_2(train)","00191b8a":"plt.figure(figsize=(18,25))\nsns.boxplot(data=train[all_features], orient=\"h\")\nplt.title(\"Train Set\")","21261449":"plt.figure(figsize=(18,25))\nsns.boxplot(data=test[all_features], orient=\"h\")\nplt.title(\"Test Set\")","8c350830":"# Importing Catboost and sklearn libraries.\nfrom catboost import Pool, CatBoostClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\n\n# Reading datasets.\ntrain = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv\")\n\n# Saving all features to all_features list.\nall_features=[]\nfor i in range(0,50):\n    all_features.append(\"feature_\" + str(i))\n\n    \n# Converting all features to string.\nfor column in all_features:\n    train[column] = train[column].astype(str)\n    test[column] = test[column].astype(str)\n    \n    \n# Removing records which are not appear in test set from train set.\ntrain_have_test_dont_have = {}\nfor feature in all_features:\n    value = list(set(list(train[feature].unique())) - set(list(test[feature].unique())))\n    train_have_test_dont_have.update({feature:value})\n\nfor feature,value in train_have_test_dont_have.items():\n    train = train[~(train[feature].isin(value))]\ntrain.reset_index(inplace=True)\nprint(train.shape)","b9d9d89d":"# Defining a function that uses catboost with 10 fold and shows the validation scores for each fold and all folds.\ndef train_catboost_model(df, columns, categoric_columns, random_state, n_splits):\n\n    models = {}\n    logloss_all = []\n    folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)  \n    X_train = df[columns]\n    y_train = df['target']\n    feature_importance = pd.DataFrame(columns=[\"column\",\"importance\"])\n    validation_set = pd.DataFrame()\n    \n    # Building the models.\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X_train,  y_train)):\n        print(\"Fold --> \" + str(n_fold+1) + \"\/\" + str(n_splits))\n        train_X, train_y = X_train.iloc[train_idx].copy(), y_train.iloc[train_idx]\n        valid_X, valid_y = X_train.iloc[valid_idx].copy(), y_train.iloc[valid_idx]\n        dataset = Pool(train_X, train_y, categoric_columns)\n        evalset = Pool(valid_X, valid_y, categoric_columns)\n        model = CatBoostClassifier(\n            task_type=\"GPU\",\n            depth=4,\n            iterations=10000,\n            od_wait=1000,\n            od_type='Iter',\n            learning_rate=0.02,\n            use_best_model=True,\n            loss_function='MultiClass'\n            )\n\n        model.fit(dataset, plot=False, verbose=500, eval_set=evalset)\n        models.update({\"model\"+str(n_fold+1):model})\n        \n        \n        # Validation Prediction\n        validation_logloss=[]\n\n        _proba = model.predict_proba(valid_X[all_features])\n        valid_X.reset_index(inplace=True,drop=True)\n        valid_X[\"Class_1\"] = _proba[:,0]\n        valid_X[\"Class_2\"] = _proba[:,1]\n        valid_X[\"Class_3\"] = _proba[:,2]\n        valid_X[\"Class_4\"] = _proba[:,3]\n\n    \n        validation_predictions = []\n        for i in range(0,valid_X.shape[0]):\n            validation_predictions.append(list(valid_X.iloc[i][[\"Class_1\",\"Class_2\",\"Class_3\",\"Class_4\"]])) \n        \n        valid_y.reset_index(drop=True,inplace=True)\n        valid_X[\"target\"] = valid_y\n        validation_set = validation_set.append(valid_X,ignore_index=True)\n    \n        logloss_of_fold = log_loss(list(valid_y),validation_predictions)\n        logloss_all.append(logloss_of_fold)\n        print(\"logloss of validation for \" + \"Fold --> \" + str(n_fold+1) + \" --> \" + str(log_loss(list(valid_y),validation_predictions)))\n        print(\"-\"*120)\n    \n        # Feature Importance\n        feature_importance_fold = pd.DataFrame({'column': X_train.columns , 'importance': model.feature_importances_})\n        feature_importance = feature_importance.append(feature_importance_fold,ignore_index=True)\n        \n    feature_importance = feature_importance.groupby([\"column\"]).mean().reset_index(drop=False)\n    feature_importance.sort_values(['importance', 'column'], ascending=[False, True], inplace=True)\n    feature_importance.reset_index(inplace=True,drop=True)\n    \n    # Logloss Calculation of all models.\n    print(\"LOGLOSS OF VALIDATION OF ALL --> \" + str(sum(logloss_all)\/len(logloss_all)))\n    \n    \n    return validation_set,models,feature_importance","bccf85e9":"validation_set,models,feature_importance = train_catboost_model(df=train, columns=all_features, categoric_columns=all_features, random_state=33, n_splits=10)","88dbced6":"plt.figure(figsize=(15,30))\nax = sns.barplot(data=feature_importance, x=\"importance\", y=\"column\", color=\"b\")\ntotal = feature_importance[\"importance\"].sum()\nfor p in ax.patches:\n    percentage = '{:.2f}%'.format(100 * p.get_width()\/total)\n    x = p.get_x() + p.get_width() + 0.02\n    y = p.get_y() + p.get_height()\/2\n    ax.annotate(percentage, (x, y))\n    \nplt.title('Catboost Model Feature Importance')\nplt.ylabel('Features')\nplt.xlabel('Importance')","3bcce2da":"class_1_columns=[]\nclass_2_columns=[]\nclass_3_columns=[]\nclass_4_columns=[]\n    \nfor model in models.keys():\n    _proba = models[model].predict_proba(test[all_features])\n    \n    test[model+\"_class1\"] = _proba[:,0]\n    class_1_columns.append(model+\"_class1\")\n    \n    test[model+\"_class2\"] = _proba[:,1]\n    class_2_columns.append(model+\"_class2\")\n    \n    test[model+\"_class3\"] = _proba[:,2]\n    class_3_columns.append(model+\"_class3\")\n    \n    test[model+\"_class4\"] = _proba[:,3]\n    class_4_columns.append(model+\"_class4\")\n    \ntest[\"Class_1\"] = test[class_1_columns].apply(np.mean,axis=1)\ntest[\"Class_2\"] = test[class_2_columns].apply(np.mean,axis=1)\ntest[\"Class_3\"] = test[class_3_columns].apply(np.mean,axis=1)\ntest[\"Class_4\"] = test[class_4_columns].apply(np.mean,axis=1)\n    \n\nsubmission['Class_1'] = test['Class_1']\nsubmission['Class_2'] = test['Class_2']\nsubmission['Class_3'] = test['Class_3']\nsubmission['Class_4'] = test['Class_4']\ncatboost = submission.copy()\ncatboost.head()","a6cdc391":"pip install -U lightautoml","1c2521f1":"# Importing LightAutoML library and LabelEncoder from sklearn.\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.tasks import Task\nfrom sklearn.preprocessing import LabelEncoder\n\n# Reading datasets.\ntrain = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv\")\n\n\n# Label Encoding the target variable.\nle = LabelEncoder()\ntrain[\"target\"] = le.fit_transform(train[\"target\"])","a8f3370e":"# Running LightAutoML\n\nN_THREADS = 8 # threads cnt for lgbm and linear models\nN_FOLDS = 5 # folds cnt for AutoML\nRANDOM_STATE = 33 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTIMEOUT = 3600 * 4 # Time in seconds for automl run\n\n\ntask = Task('multiclass',)\n\nroles = {\n    'target': \"target\",\n    'drop': ['id'],\n}\n\n\nautoml = TabularUtilizedAutoML(task = task, \n                               timeout = TIMEOUT,\n                               cpu_limit = N_THREADS,\n                               reader_params = {'n_jobs': N_THREADS},\n)\n\noof_pred = automl.fit_predict(train, roles = roles)\nprint('oof_pred:\\n{}\\nShape = {}'.format(oof_pred[:10], oof_pred.shape))","c9d10596":"# Fast feature importances calculation\nfast_fi = automl.get_feature_scores('fast', silent = False)\nfast_fi.set_index('Feature')['Importance'].plot.bar(figsize = (20, 10), grid = True)","97919fbb":"# Prediction with LightAutoML\ntest_pred = automl.predict(test)\nprint('Prediction for test set:\\n{}\\nShape = {}'.format(test_pred[:10], test_pred.shape))\n\nprint('Check scores...')\nprint('OOF score: {}'.format(log_loss(train[\"target\"].values, oof_pred.data)))\n\n\nsubmission.iloc[:, 1:] = test_pred.data\nlightautoml = submission.copy()","bebcfd04":"catboost.rename(columns = {'Class_1' : 'catboost_Class_1'}, inplace = True)\ncatboost.rename(columns = {'Class_2' : 'catboost_Class_2'}, inplace = True)\ncatboost.rename(columns = {'Class_3' : 'catboost_Class_3'}, inplace = True)\ncatboost.rename(columns = {'Class_4' : 'catboost_Class_4'}, inplace = True)\nlightautoml.rename(columns = {'Class_1' : 'light_Class_1'}, inplace = True)\nlightautoml.rename(columns = {'Class_2' : 'light_Class_2'}, inplace = True)\nlightautoml.rename(columns = {'Class_3' : 'light_Class_3'}, inplace = True)\nlightautoml.rename(columns = {'Class_4' : 'light_Class_4'}, inplace = True)\n\nlightautoml_and_catboost = pd.merge(left=catboost, right=lightautoml, left_on=\"id\", right_on=\"id\", how=\"left\")\n\nlightautoml_and_catboost[\"Class_1\"] = (lightautoml_and_catboost[\"light_Class_1\"] * 0.80) + (lightautoml_and_catboost[\"catboost_Class_1\"] * 0.20)\nlightautoml_and_catboost[\"Class_2\"] = (lightautoml_and_catboost[\"light_Class_2\"] * 0.80) + (lightautoml_and_catboost[\"catboost_Class_2\"] * 0.20)\nlightautoml_and_catboost[\"Class_3\"] = (lightautoml_and_catboost[\"light_Class_3\"] * 0.80) + (lightautoml_and_catboost[\"catboost_Class_3\"] * 0.20)\nlightautoml_and_catboost[\"Class_4\"] = (lightautoml_and_catboost[\"light_Class_4\"] * 0.80) + (lightautoml_and_catboost[\"catboost_Class_4\"] * 0.20)\n\nlightautoml_and_catboost = lightautoml_and_catboost[[\"id\",\"Class_1\",\"Class_2\",\"Class_3\",\"Class_4\"]]\nlightautoml_and_catboost.to_csv(\"lightautoml_and_catboost.csv\", index=False)\n\nprint(lightautoml_and_catboost.shape)\nlightautoml_and_catboost","b49f07c9":"**1.4. Exploring Features (Train vs Test)**","06c7f64f":"**2.2. LightAutoML**","20ad9ff1":"**1.1. Read Data and First Look**","379be773":"![resim.png](attachment:4c7b5cae-19ea-467e-9f49-cf8bea90e344.png)","b0bdda8e":"# **2. Modeling**","0e497a32":"**2.1. Catboost**","04c8b65f":"![resim.png](attachment:77409394-37c5-49b6-b13d-cc78f30f4990.png)","05301bea":"# **3. Submission**","54869cc8":"**1.6. Outliers**","5160eb98":"![resim.png](attachment:e25169d4-3487-425d-b577-703d5528cfd4.png)","b47b3a99":"# **1. EDA**","9be990a7":"**1.5. Exploring Features by Each Class (Class_1 vs Class_2 vs Class_3 vs Class4)**","e87d4696":"**1.3. Target Distribution**"}}