{"cell_type":{"56cd4ec2":"code","6d945970":"code","b2faf678":"code","03fb9f62":"code","60ace61f":"code","79e922ae":"code","5c8c1e21":"code","afeb12fb":"code","ef72f288":"code","da4f1269":"code","7191a5ea":"code","2cce91ce":"code","e881f89b":"code","f8a01cf5":"code","cd772c47":"code","98ca8fdb":"code","967578bd":"code","5b6ced5e":"code","e6250f4f":"code","db799fd1":"code","60bb1a6a":"code","dc99aec3":"code","dbdb63b0":"code","9e731543":"code","f97888f1":"code","09a7256e":"code","d1e17b28":"code","18a95afb":"code","f0394e15":"code","1a24cc0c":"code","0909014a":"code","21b55e1b":"code","f7c3b3eb":"code","21170d36":"code","e0f0d6a0":"code","f7f910dd":"code","d0155e94":"code","3c3a1e3a":"code","33f19e73":"code","61939f37":"code","6e8a3379":"code","8a2bea97":"code","fe3ca60e":"code","22a786d5":"code","4603126c":"code","b8e6a8be":"code","64f98747":"markdown","7a9c483c":"markdown","8059a01d":"markdown","bdfd87cc":"markdown","8d5810a3":"markdown","af3c6581":"markdown","4b7d6e1b":"markdown","144d3418":"markdown","3e6fae80":"markdown","59dff84b":"markdown","6a5a78ad":"markdown","ec024729":"markdown","41f65831":"markdown","50daaf21":"markdown","31bf43b3":"markdown","9fef1450":"markdown","2ed63d43":"markdown","4db873a5":"markdown","6f7842ca":"markdown","d6a39dee":"markdown","d872e8d4":"markdown","e0a2ea3d":"markdown","751ce4b2":"markdown","dafd7590":"markdown","18af74f4":"markdown"},"source":{"56cd4ec2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Limit floats output to 3 decimal points\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n# Increase default figure and font sizes for easier viewing.\nplt.rcParams['figure.figsize'] = (8, 6)\nplt.rcParams['font.size'] = 14\n\n\"\"\"\nimport os\nprint(os.listdir(\"..\/input\"))\n\"\"\"\n","6d945970":"dataframe = pd.read_csv('..\/GeneticVariantClassification\/clinvar_conflicting.csv')","b2faf678":"dataframe.head()","03fb9f62":"fig = plt.figure(figsize = (20, 20))\nsns.countplot(x = 'CLASS', data = dataframe, hue = 'CHROM', palette='icefire')","60ace61f":"fig = plt.figure(figsize = (15,15))\nsns.heatmap(dataframe.isnull(), cmap = 'seismic', cbar = False)","79e922ae":"dataframe.head(2)","5c8c1e21":"considered = ['CHROM', 'POS', 'REF', 'ALT', 'AF_ESP', 'AF_EXAC', 'AF_TGP',\n       'CLNDISDB', 'CLNDN', 'CLNHGVS', 'CLNVC','MC', 'ORIGIN', 'CLASS',\n       'Allele', 'Consequence', 'IMPACT', 'SYMBOL', 'Feature_type',\n       'Feature', 'BIOTYPE', 'STRAND','CADD_PHRED', 'CADD_RAW']\n\ndataframe2 = dataframe[considered]\ndataframe2 = dataframe2.dropna()\ndataframe2['CHROM'] = dataframe2['CHROM'].astype(str)\n\nprint('Number of Null Values: ', dataframe2.isnull().sum().sum())","afeb12fb":"trimDown = []\nfor i in dataframe2.columns.values:\n    if dataframe2[i].nunique() < 1000:\n        trimDown.append(i)\n        \nprint(\"Columns selected for training are: \", trimDown)","ef72f288":"dataframe_final = dataframe2[trimDown]\ndataframe_final.info()","da4f1269":"dataframe_final['CHROM'] = dataframe_final['CHROM'].astype(str)\ndataframe_final.info()","7191a5ea":"dataframe_final","2cce91ce":"from sklearn.feature_extraction import FeatureHasher\nFH = FeatureHasher(n_features = 5, input_type = 'string')\n\nhash1 = FH.fit_transform(dataframe_final['REF'])\nhash1 = hash1.toarray()\nhashedFeatures1 = pd.DataFrame(hash1)","e881f89b":"nameList = {}\nfor i in hashedFeatures1.columns.values:\n    nameList[i] = \"REF\"+str(i+1)\n\nhashedFeatures1.rename(columns = nameList, inplace = True)\nprint(\"The Hashed REF table is something like this : \\n\",hashedFeatures1.head())","f8a01cf5":"hash2 = FH.fit_transform(dataframe_final['ALT'])\nhash2 = hash2.toarray()\nhashedFeatures2 = pd.DataFrame(hash2)\n\nnameList2 = {}\nfor i in hashedFeatures2.columns.values:\n    nameList[i] = \"ALT\"+str(i+1)\n\nhashedFeatures2.rename(columns = nameList, inplace = True)\nprint(\"The Hashed ALT table is something like this : \\n\",hashedFeatures2.head())","cd772c47":"binaryFeature1 = pd.get_dummies(dataframe_final['CLNVC'])\n\nprint(\"While the One hot encoded matrix of CLNVC Columns is like this : \\n\")\n\nbinaryFeature1.head()","98ca8fdb":"dataframe_final.columns","967578bd":"dataframe_final = dataframe_final.drop(columns = ['MC'], axis = 1)\ndataframe_final.columns","5b6ced5e":"hash0 = FH.fit_transform(dataframe_final['CHROM'])\nhash0 = hash0.toarray()\nhashedFeatures0 = pd.DataFrame(hash0)\n\nnameList0 = {}\nfor i in hashedFeatures0.columns.values:\n    nameList0[i] = \"CHROM\"+str(i+1)\n\n\nhashedFeatures0.rename(columns = nameList0, inplace = True)\nhashedFeatures0.head()","e6250f4f":"hash3 = FH.fit_transform(dataframe_final['Allele'])\nhash3 = hash3.toarray()\nhashedFeatures3 = pd.DataFrame(hash3)\n\nnameList3 = {}\nfor i in hashedFeatures3.columns.values:\n    nameList3[i] = \"Allele\"+str(i+1)\n\n\nhashedFeatures3.rename(columns = nameList3, inplace = True)\nhashedFeatures3.head()","db799fd1":"hash4 = FH.fit_transform(dataframe_final['Consequence'])\nhash4 = hash4.toarray()\nhashedFeatures4 = pd.DataFrame(hash4)\n\nnameList4 = {}\nfor i in hashedFeatures4.columns.values:\n    nameList4[i] = \"Consequence\"+str(i+1)\n\n\nhashedFeatures4.rename(columns = nameList4, inplace = True)\nhashedFeatures4.head()","60bb1a6a":"dataframe_final['IMPACT'].nunique()","dc99aec3":"dataframe_final = dataframe_final.drop(columns=['Feature_type'], axis = 1)","dbdb63b0":"binaryFeature3 = pd.get_dummies(dataframe_final['IMPACT'])\nbinaryFeature3.head()","9e731543":"binaryFeature4 = pd.get_dummies(dataframe_final['BIOTYPE'], drop_first=True)\nbinaryFeature4.head()","f97888f1":"binaryFeature5 = pd.get_dummies(dataframe_final['STRAND'], drop_first=True)\nbinaryFeature5.head()","09a7256e":"dataframe3 = pd.concat([binaryFeature1, binaryFeature3, binaryFeature4, binaryFeature5, hashedFeatures1 , hashedFeatures2, hashedFeatures3, hashedFeatures4,hashedFeatures0, dataframe_final['CLASS']], axis = 1)\ndataframe3 = dataframe3.dropna()\ndataframe3.rename(columns={1 : \"one\", 16 : \"sixteen\"}, inplace = True)\n\n\nprint(dataframe3.columns.values)\ndataframe3.head()","d1e17b28":"y = dataframe3['CLASS']\nX = dataframe3.drop(columns = ['CLASS'], axis = 1)","18a95afb":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n# from xgboost import XGBClassifier","f0394e15":"# XGBoost in different directory\n\nimport sys\nsys.path.append(\"\/usr\/local\/lib\/python3.7\/site-packages\")\n\nfrom xgboost import XGBClassifier","1a24cc0c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)","0909014a":"logReg = LogisticRegression()\nlogReg.fit(X_train, y_train)\npred_logReg = logReg.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_test, pred_logReg))","21b55e1b":"decisionTree = DecisionTreeClassifier(max_depth = 6)\ndecisionTree.fit(X_train, y_train)\npred_decisionTree = decisionTree.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_test, pred_decisionTree))","f7c3b3eb":"randomForest = RandomForestClassifier()\nrandomForest.fit(X_train, y_train)\npred_randomForest = randomForest.predict(X_test)\n\nprint( \"Classification Report :\\n \", classification_report(y_test, pred_randomForest))","21170d36":"gbc = GradientBoostingClassifier()\ngbc.fit(X_train, y_train)\npred_gbc = gbc.predict(X_test)\n\nprint('Classification Report :\\n ', classification_report(y_test, pred_gbc))","e0f0d6a0":"from collections import OrderedDict","f7f910dd":"important_features = {}\nfor i in zip(X.columns, logReg.coef_[0]):\n    important_features[i[0]] = i[1]\nimportant_final = OrderedDict(important_features)\n\ndataframe_features = pd.DataFrame(important_final, index = range(1)).T\ndataframe_features.rename(columns = {0: 'Importance_LogReg'}, inplace = True)\n\ndataframe_features.plot(kind = 'bar', figsize = (20,5))","d0155e94":"important_features2 = {}\nfor i in zip(X.columns, randomForest.feature_importances_):\n    important_features2[i[0]] = i[1]\n    \nimportant_final2 = OrderedDict(important_features2)\nprint(important_final2)\n\ndataframe_features2 = pd.DataFrame(important_final2, index = range(1)).T\ndataframe_features2.rename(columns = {0: 'Importance_RandomForest'}, inplace = True)\n\ndataframe_features2.plot(kind = 'bar', figsize = (15,5))","3c3a1e3a":"dataframe_compare = pd.concat([dataframe_features, dataframe_features2], axis = 1)\ndataframe_compare.plot(kind = 'bar', figsize = (20,5))","33f19e73":"# pip install keras","61939f37":"from keras.models import Sequential\nfrom keras.layers import (Dense, Flatten, Dropout, BatchNormalization)","6e8a3379":"model = Sequential()\n\nmodel.add(Dense(128 , input_dim = 38, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.33))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.33))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(1, activation = 'sigmoid'))","8a2bea97":"model.summary()","fe3ca60e":"model.fit(X, y, batch_size=64, epochs = 20, verbose=1)","22a786d5":"prediction = model.predict(X_test)","4603126c":"def finalPredictions(x):\n    if x<0.5 : \n        return 0\n    else:\n        return 1\npred_deep = []\nfor i in prediction:\n    pred_deep.append(finalPredictions(i))","b8e6a8be":"print(classification_report(y_test, pred_deep))","64f98747":"## Feature Engineering\n- Apply Feature Hashers on columns with over 10 unique values or one-hot encoding.\n    - Feature Hashers: turns seq of symbolic feature names (str) into scipy.sparse matrices\n    - One-hot encoding: sparse way of representing data in a binary string in which only a single bit can be 1, while others are 0","7a9c483c":"- Gradient Boost Classifier provides the best overall accuracy.\n- Poor results for Recall and f1-score","8059a01d":"## Random Forest","bdfd87cc":"## Comparison of Features Importance","8d5810a3":"#### Possible Approach (trim down dataset)\n- Consider the ones with mimimum losses and consider features with less than 1000 unique features:","af3c6581":"## Gradient Boost Classifier","4b7d6e1b":"## Comparing Feature Importance","144d3418":"#### Data:\n- ClinVar is a public resource containing annotations about human genetic variants. These variants are (usually manually) classified by clinical laboratories on a categorical spectrum ranging from benign, likely benign, uncertain significance, likely pathogenic, and pathogenic. Variants that have conflicting classifications (from laboratory to laboratory) can cause confusion when clinicians or researchers try to interpret whether the variant has an impact on the disease of a given patient.","3e6fae80":"- Only improvement in Recall and f1-score for conflicting genes.","59dff84b":"- Chromosomes (CHROM) are more of an important feature for Random Forest than it is for Logistical Regression.","6a5a78ad":"#### Objective:\n- Predict whether a variant will have conflicting clinical classifications.\n- Explore data in order to see what is expected from different classifiers and to determine what matters the most to each classifier.","ec024729":"### Machine Learning Libraries","41f65831":"## Deep Learning","50daaf21":"## Inference for Logistical Regression","31bf43b3":"#### Determine Null Values","9fef1450":"## Inference for Random Forest Classifier","2ed63d43":"- Null values are indicated by red coloration in the given graph above.\n- Visual inspection of the graph shows that the dataset appears incomplete or lacking massive amounts of information.\n","4db873a5":"#### Final Table","6f7842ca":"## Logistic Regression","d6a39dee":"# Genetic Variant Classification using Different Classifiers","d872e8d4":"## Exploratory Data Analysis","e0a2ea3d":"- Total precision of 65% demonstrates that this classifier performed better.\n- Improvement in Recall score of non-conflicting genes and Precision in conflicting genes.","751ce4b2":"## Decision Trees","dafd7590":"- Most Important Features:\n    - Insertions, Inversions, and Indels\n    - Microsatellite\n    - HIGH\n- Chromosomes display little importance.","18af74f4":"- 56% accuracy in terms of precision is poor.\n- Poor Recall score of non-conflicting genes."}}