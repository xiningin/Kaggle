{"cell_type":{"6e5f980b":"code","fb994e55":"code","ac0abaa0":"code","d16db74c":"code","f0aaa942":"code","779d2842":"code","10bc0dba":"code","6b5661c1":"markdown","ea6daeb8":"markdown","01bcd09c":"markdown","875101ba":"markdown","c50db261":"markdown","ae696d22":"markdown"},"source":{"6e5f980b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fb994e55":"train=pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\n\ntrain_labels=pd.get_dummies(train['label'])\ntrain_data=train.drop('label',axis=1)\ntest_data=test","ac0abaa0":"from tensorflow.keras.layers import Input\ninputLayer=Input(784)\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense\nmodels=[]\nn=10\nfor x in range(n):\n    hl1=Dense(20*(x+1),activation='sigmoid')(inputLayer)\n    hl2=Dense(20*(x+1),activation='relu')(hl1)+hl1\n    hl3=Dense(8*(x+1),activation='relu')(hl2)\n    outputLayer=Dense(10,activation=\"sigmoid\")(hl3)\n    models.append(Model(inputLayer,outputLayer))","d16db74c":"epochs_per_model=15\nhistories=[]\nfor x in range(n):\n    print(\"Model number \"+str(x+1))\n    models[x].compile(loss=\"MeanSquaredError\")\n    histories.append(models[x].fit(x=train_data,y=train_labels,batch_size=120,epochs=epochs_per_model,verbose=1))\n    ","f0aaa942":"import matplotlib.pyplot as plt\nfor x in range(n):\n    plt.plot(range(1,epochs_per_model+1),histories[x].history['loss'],label=\"Model\"+str(x+1))\nplt.title(\"Training loss as function of epoch number for all models\")\nplt.xlabel(\"Epoch number\")\nplt.ylabel(\"Loss\")\nplt.ylim(0,0.06)\nplt.legend()\nplt.grid()\nplt.show()","779d2842":"\nplt.plot(range(1,n+1),pd.Series(histories).apply(lambda x: x.history['loss'][epochs_per_model-1]))\nplt.title(\"Training loss as function of model size\")\nplt.xlabel(\"Epoch number\")\nplt.ylabel(\"Loss\")\nplt.grid()\nplt.show()","10bc0dba":"predictions=np.argmax(models[n-1].predict(test_data),axis=1)\nsubmitThis=[{'ImageId':x+1,'Label':predictions[x]} for x in range(len(test_data))]\npd.DataFrame(submitThis).to_csv(\"submission.csv\",index=False)","6b5661c1":"Plot training effectiveness of each model:","ea6daeb8":"Now we train them:","01bcd09c":"No real surprises here. What does the scaling curve look like?","875101ba":"Now define a list of dense networks of increasing size:","c50db261":"Let's submit the predictions of the beefiest model:","ae696d22":"Import and format data:"}}