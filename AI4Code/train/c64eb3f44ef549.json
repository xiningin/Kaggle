{"cell_type":{"0ad3ec05":"code","c03b83a0":"code","196e9b65":"code","62cf714d":"code","ffca103a":"code","058ea76f":"code","16cf4e74":"code","463a65c1":"code","0ad4027f":"code","36969ffb":"code","cf2881a2":"code","8f7e77e0":"code","68da87f9":"markdown","aea70d0e":"markdown","ec56f2ec":"markdown","ec7322da":"markdown","b9738cf1":"markdown","4fd520a4":"markdown","c39d3cff":"markdown","47ee9fb9":"markdown","26b01da2":"markdown","03b94aea":"markdown","834b90d2":"markdown","2c57e141":"markdown","4fefb700":"markdown"},"source":{"0ad3ec05":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c03b83a0":"import matplotlib.pyplot as plt\nfrom IPython.display import display\n","196e9b65":"file_path = '\/kaggle\/input\/opioid-overdose-deathse\/health-opioid-overdose-deaths\/Multiple Cause of Death, 1999-2014 v1.1.csv'\ndf = pd.read_csv(file_path, na_values=['Suppressed','Unreliable'])\ndf.info()\n","62cf714d":"# The structure of this function is based on \"Avocado | EDA\" done by \"ks_lar_wtf\"\n# I have merely modified it to suit my needs\n\n# https:\/\/www.kaggle.com\/kslarwtf\n# https:\/\/www.kaggle.com\/kslarwtf\/avocado-eda\/notebook\n\ndef clean_data(d, dname=None):\n    # Let's clean duplicates first\n    temp = d.duplicated().sum()\n    if temp:\n        print(F\"{temp} dupllicates found.\\n\")\n        print(\"*** Removing Duplicates ***\\n\")\n        d = d.drop_duplicates(ignore_index=True)\n    \n    # Let's check if we have any missing values\n    temp = df.isna().sum().any()\n    if temp:\n        print(\"*** There are missing values in the data. Let's remove those ***\\n\")\n        d = d.dropna()\n    \n    # Rename all Columns to lowercase \n    d.columns = d.columns.str.lower()\n    \n    # one column has name too long\n    old_name = 'prescriptions dispensed by us retailers in that year (millions)'\n    new_name = 'prescriptions dispensed'\n    d = d.rename(columns={old_name: new_name})\n\n    return d","ffca103a":"# The structure of this function is based on \"Avocado | EDA\" done by \"ks_lar_wtf\"\n# please refer to the links from the last cell\ndef examine_data(d, dname=None):\n    print(F\"*** Examining {dname} ***\\n\")\n    display(d.head())\n    display(d.info())\n    display(d.columns)","058ea76f":"df_cleaned = clean_data(df, \"Opioid Overdose Deaths\")\nexamine_data(df_cleaned, \"Opioid Overdose Deaths -- Cleaned\")","16cf4e74":"# California has highest death rate from opioid overdose\n# top 3 states are: California, Florida and New York\nstatewise_deaths = df_cleaned.groupby('state')['deaths'].sum().sort_values(ascending=False)\ndisplay(statewise_deaths)\nfig, ax = plt.subplots(figsize=(9,15))\nax.barh(statewise_deaths.index, statewise_deaths)\nax.invert_yaxis()\n","463a65c1":"df_cleaned.groupby('state')['prescriptions dispensed'].sum().sort_values(ascending=False)","0ad4027f":"yearly_deaths = df_cleaned.groupby('year')['deaths'].sum().sort_values(ascending=False)\ndisplay(yearly_deaths)\n\nfig, ax = plt.subplots(1,2, figsize=(15,8))\nax[0].plot(yearly_deaths.index, yearly_deaths)\nax[1].bar(yearly_deaths.index, yearly_deaths)","36969ffb":"yearly_population = df_cleaned.groupby('year')['population'].sum()\ndisplay(yearly_population)\n\nfig, ax = plt.subplots(1,2, figsize=(18,8))\nax[0].plot(yearly_population.index, yearly_population)\nax[1].bar(yearly_population.index, yearly_population)","cf2881a2":"fig3, ax3 = plt.subplots(1,2, figsize=(18,6), sharey=True)\nax3[0].plot(yearly_population.index, yearly_population)\nax3[0].set_xlabel('Years')\nax3[0].set_ylabel('Population Opioids Use')\n\nax3[1].bar(yearly_population.index, yearly_population)\nax3[1].set_xlabel('Years')\nax3[1].set_ylabel('Population Opioids Use')\n\nplt.ylim(2e8, 4e8)\n\n","8f7e77e0":"# Let's check the crude rate by state\ndf_cleaned.groupby('state')['crude rate'].mean().sort_values(ascending=False)","68da87f9":"# INTRO\n\nThis is my very first notebook on Kaggle. I started this only for one reason: to learn to explore datasets, to learn data science and to learn how to visualize using matplotlib. Also, the reason for chosing this specific dataset on opioid deaths: I like contributing to the planet and this world, so anything that helps the nature and the medicine, I am all in for it. Please forgive any mistakes for I am just a beginner. You are welcome to commment if you:\n\n* think any new explaoration or visualization that can give a better analysis of the dataset\n* find something that needs correction\n* can give ideas on how to make the notebook better\n\nCREDITS: <br\/>\nhttps:\/\/www.kaggle.com\/yamqwe\/opioid-overdose-deathse\/   <br\/>\nhttps:\/\/data.world\/health\/opioid-overdose-deaths","aea70d0e":"Year 2014 saw the largest number of deaths due to opioids overdose. In fact, death rate due to opioids overdose is increasing year by year. In 16 years, death rates have almost quadrupled. This is an alarming situation for a country. It should raise all the red flags within any organization working in the public interest.","ec56f2ec":"If you just look at the line chart, it looks like an explosive growth in population. But then if you draw a bar chart, you see the difference. The line chart gives us a a wrong overview of the population increase because the y-axis scale is different. In fact, y-axis scale of line chart is quite misleading (though both are using the default scale whatever matplotlib provides)\n\nTo have clean comparison, let's use similar (and not misleading) values for the y-axis for both kinds of charts. Since y-axis values range from **2.5e8 - 3.2e8**. We can use a range from **2e8 - 4e8**\n\nNow you will see that rise in population is there but it is not as explosive as line chart made us believe the first time. We will also reduce the height of the charts to make it look more appealing","ec7322da":"## FIRST IMPRESSIONS OF DATA\n\nAs you can see there are 8 columns but the **Non-Null Count** is not same for all of them. That means there are some values missing. So we will clean the dataset first. Instead of calling cleaning functions one by one, let's write a function to clean the dataset instead.  ","b9738cf1":"### After exploring the dataset I came to know that it has words like 'Suppressed' and 'Unreliable' instead of just NaN. So, we gonna treat those as NaN and read the dataset accordingly ","4fd520a4":"Our analysis is only as good as our data. In this specific case looks like our dataset does not have complete information regarding the prescription dispensed. It's not possible that 90% of the states are selling exactly the same number of opioids. Think about it: each state has a different population and hence number of prescriptions can't be exactly same. Second reason is exact same number of prescriptions can't cause 50 or 90% variation in overdoses across 52 states.","c39d3cff":"What we have done is taken the *mean* of all the crude rates for a particular state. We can see that even though California has the highest number of opioid overdose deaths, its crude rate is only 25% of the maximum value. New York and Florida are similar too in that aspect. I don't know what more I can make out of it for I have never studied Statistics. ","47ee9fb9":"Let's see the sale of the opioids per state if that matches the state with the highest overdose death rates ","26b01da2":"# More To Come\n\n## Once I learn more about how to analyze datasets and more visualization techniques, I will come back here and update this notebook.","03b94aea":"# BEGIN\n\nNext cell of code was automatically added by Kaggle. Kaggle notebook automatically imports Pandas, NumPy and the dataset file path. ","834b90d2":"Now the **Non-Null Count** is same for all the columns, which means we have effectively removed any rows with missing values. Now we can start doing some visualizations to see what's this dataset has. Let's see which state has the highest death rate from opioid overdose  ","2c57e141":"![Chemical structure of morphine, the prototypical opioid](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/3\/33\/Morphin_-_Morphine.svg\/282px-Morphin_-_Morphine.svg.png)\n\n\nChemical structure of morphine, the prototypical opioid, from [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/File:Morphin_-_Morphine.svg)","4fefb700":"## CRUDE RATE & CONFIDENCE INTERVAL\n\nCruade rate is the total number of events occurring in an entire population over a period of time, without reference to any of the individuals or subgroups within the population. \n\n\nWe indicate a confidence interval by its endpoints; for example, the 90% confidence interval for the number of people in poverty in the United States in 1995 is \"35,534,124 to 37,315,094.\" If we were to repeatedly make new estimates using exactly the same procedure (by drawing a new sample, conducting new interviews etc), 90% of the time the estimate will fall within the range given above. \n\n\nSOURCE: [The Free Dictionary](https:\/\/medical-dictionary.thefreedictionary.com\/crude+rate), [United States Census Bureau](https:\/\/www.census.gov\/programs-surveys\/saipe\/guidance\/confidence-intervals.html)\n\nYou can check out a small example of how to calculate Crude Rate at [Boston University School of Public Health](https:\/\/sphweb.bumc.bu.edu\/otlt\/MPH-Modules\/EP\/EP713_StandardizedRates\/EP713_StandardizedRates2.html) by *Wayne W. LaMorte*"}}