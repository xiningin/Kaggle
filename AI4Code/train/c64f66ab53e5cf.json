{"cell_type":{"0fd797db":"code","604392cf":"code","2ea1e7c4":"code","3315430a":"code","3de82af5":"code","a431568d":"code","a5204b9f":"code","e0c04002":"code","de3242d2":"code","bf34746e":"code","3589e321":"code","0f0c167a":"code","44b82662":"code","e9ec15dd":"code","9c17c8d4":"code","8c07c28e":"code","7b09f5bf":"code","5861ee55":"code","1d297848":"code","660cdd02":"code","76d9791b":"code","f921f238":"code","5e2e228c":"code","fd072cbf":"code","d1f6e2d5":"code","fbca2dfa":"code","705fbf95":"code","b23763d1":"code","49fa5840":"code","48615863":"code","fa0060bf":"code","81e2f3f8":"code","f7545ace":"code","c84540c4":"code","e6e97002":"code","58c0e8f6":"code","3d4825a8":"code","d8fe5850":"code","4bb483fc":"code","bccfc443":"markdown","ab90f54b":"markdown","e5eeee7f":"markdown","7e4abbea":"markdown","60a0b3fa":"markdown","f51200ce":"markdown","3123106f":"markdown","cff853f3":"markdown","46ee3091":"markdown","6ff48f5d":"markdown","db2883e5":"markdown","bf031c99":"markdown","aed81f1e":"markdown","9e308654":"markdown","5ea5e58b":"markdown","a27fd9fa":"markdown","1375b5da":"markdown","bd96e322":"markdown","4e986953":"markdown","9ee95492":"markdown"},"source":{"0fd797db":"!pip install iterative-stratification","604392cf":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nimport tensorflow_addons as tfa\n\nfrom sklearn.metrics import log_loss\nfrom tqdm.notebook import tqdm\n\nimport warnings\n\nwarnings.filterwarnings('ignore')","2ea1e7c4":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px","3315430a":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_non_targets = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","3de82af5":"train_features.head(1)","a431568d":"train_non_targets.head(1)","a5204b9f":"train_non_targets.head(1)","e0c04002":"values = np.sum(train_non_targets.iloc[:, 1:], axis = 0)\n\nsns.distplot(values)","de3242d2":"def create_fold(data):\n    \n    data['fold'] = -1\n    \n    data = data.sample(frac = 1).reset_index(drop = True)\n    \n    targets = data.drop('sig_id', axis=1).values\n    \n    splitter = MultilabelStratifiedKFold(n_splits=7, random_state=0)\n    \n    for fold, (train, valid) in enumerate(splitter.split(X=data, y=targets)):\n        \n        data.loc[valid, 'fold'] = fold\n        \n    return data","bf34746e":"combined = train_targets.merge(train_non_targets, on='sig_id', how='outer')\ncombined = create_fold(combined)\ntrain_targets['fold'] = combined['fold']\ntrain_non_targets['fold'] = combined['fold']\n\ndel(combined)","3589e321":"train_targets.to_csv('fold_data_targets.csv', index = False)","0f0c167a":"train_non_targets.to_csv('fold_data_non_targets.csv', index = False)","44b82662":"def preprocess(data):\n    \n    cp_time = pd.get_dummies(data['cp_time'])\n    cp_type = pd.get_dummies(data['cp_type'])\n    cp_dose = pd.get_dummies(data['cp_dose'])\n    \n    \n    data = data.join(cp_time)\n    data = data.join(cp_type)\n    data = data.join(cp_dose)\n    \n    data.drop(columns = ['cp_time', 'cp_dose', 'cp_type'], inplace=True)\n    \n    return data","e9ec15dd":"train_features = preprocess(train_features)","9c17c8d4":"def uni_non_targets():\n    train_df = train_features.merge(train_non_targets, on='sig_id', how='outer')   \n    return train_df","8c07c28e":"def create_model(num_inputs, num_outputs):\n    \n    model = tf.keras.Sequential([\n        \n        tf.keras.layers.Input(num_inputs),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.2),\n        \n        tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation=\"relu\")),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.5),\n        \n        tfa.layers.WeightNormalization(tf.keras.layers.Dense(1048, activation=\"relu\")),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.5),\n        \n        tfa.layers.WeightNormalization(tf.keras.layers.Dense(num_outputs, activation=\"sigmoid\"))\n        \n    ])\n    \n    \n    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(), sync_period=10),\n                  loss='binary_crossentropy', \n                  )\n    \n    return model","7b09f5bf":"def metric(y_true, y_pred):\n    \n    metrics = []\n    \n    for _target in train_targets.columns[1:-1]:\n        \n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels=[0,1]))\n        \n    return np.mean(metrics)","5861ee55":"train_df = train_features.merge(train_targets, on='sig_id', how='outer')","1d297848":"def run_model(fold):\n    \n    train_df = train_features.merge(train_targets, on='sig_id', how='outer')\n    \n    # defining the parameters\n    cols = train_df.columns\n    \n    ID = cols[0]\n    fold_col = cols[-1]\n    features = cols[1:880]\n    # we are skipping the mid (947th column) as it is the fold column of the previous part\n    targets = cols[880:-1]\n    \n    # loading the data\n    train = train_df[train_df['fold'] != fold]\n    valid = train_df[train_df['fold'] == fold]\n    \n    x_train = train.loc[:, features]\n    x_valid = valid.loc[:, features]\n    \n    y_train = train.loc[:, targets]\n    y_valid = valid.loc[:, targets]\n    \n    # creating the model\n    model = create_model(x_train.shape[1], y_train.shape[1])\n    \n    # Defining model callbacks\n    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n    \n    checkpoint_path = f'Fold_{fold}_basic.hdf5'\n    \n    cb_checkpt = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=0, save_best_only=True, \n                                save_weights_only=True, mode='min')\n    \n    # Fitting the model\n    model.fit(x_train, y_train, validation_data=(x_valid, y_valid), epochs=35, batch_size=128,\n             callbacks = [reduce_lr_loss, cb_checkpt], verbose=1)\n    \n    # Loading the best model\n    model.load_weights(checkpoint_path)\n    \n    # Making the predictions\n    y_valid_pred = model.predict(x_valid)\n    \n    # converting the predictions to dataframe\n    y_valid_pred = pd.DataFrame(y_valid_pred, columns=y_valid.columns)\n    \n    # Evaluating the final results\n    print('\\n\\n\\n')\n    print('OOF Metric: ', metric(y_valid, y_valid_pred))\n    \n    return","660cdd02":"run_model(0)","76d9791b":"def run_part1(fold):\n    \n    # loading the targets_non_scored concatinated data\n    train_df = uni_non_targets()\n    \n    # defining the parameters\n    cols = train_df.columns\n    \n    ID = cols[0]\n    fold_col = cols[-1]\n    features = cols[1:880]\n    targets = cols[880:-1]\n    \n    # loading the data\n    train = train_df[train_df['fold'] != fold]\n    valid = train_df[train_df['fold'] == fold]\n    \n    x_train = train.loc[:, features]\n    x_valid = valid.loc[:, features]\n    \n    y_train = train.loc[:, targets]\n    y_valid = valid.loc[:, targets]\n    \n    # Printing the shape\n    print(x_train.shape[1], y_train.shape[1])\n    \n    # creating the model\n    model = create_model(x_train.shape[1], y_train.shape[1])\n    \n    # Defining model callbacks\n    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n    \n    checkpoint_path = f'Fold_{fold}_part1.hdf5'\n    \n    cb_checkpt = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=0, save_best_only=True, \n                                save_weights_only=True, mode='min')\n    \n    # Fitting the model\n    model.fit(x_train, y_train, validation_data=(x_valid, y_valid), epochs=35, batch_size=128,\n             callbacks = [reduce_lr_loss, cb_checkpt], verbose=1)\n    \n    # Loading the best weights model\n    model.load_weights(checkpoint_path)\n    \n    # Making the predictions\n    y_valid_pred = model.predict(x_valid)\n    y_train_pred = model.predict(x_train)\n    \n    # converting the y_preds\n    y_valid_pred = pd.DataFrame(y_valid_pred, columns = y_valid.columns)\n    y_train_pred = pd.DataFrame(y_train_pred, columns = y_valid.columns)\n    \n    # replacing the train_df with the predicted data\n    train_df.loc[:, targets][train_df['fold'] != fold] = y_train_pred\n    train_df.loc[:, targets][train_df['fold'] == fold] = y_valid_pred\n    \n    # drop this fold\n    train_df.drop(columns='fold', inplace=True)\n    \n    return train_df","f921f238":"def run_part2(fold):\n    \n    # Prepairing final data\n    features = run_part1(fold)\n    targets = train_targets\n    \n    # Merging both\n    train_df = features.merge(targets, on='sig_id', how='outer')\n    \n    # defining the parameters\n    cols = train_df.columns\n    \n    ID = cols[0]\n    fold_col = cols[-1]\n    features = cols[1:1282]\n    targets = cols[1282:-1]\n    \n    # loading the data\n    train = train_df[train_df['fold'] != fold]\n    valid = train_df[train_df['fold'] == fold]\n    \n    x_train = train.loc[:, features]\n    x_valid = valid.loc[:, features]\n    \n    y_train = train.loc[:, targets]\n    y_valid = valid.loc[:, targets]\n    \n    # Some blank lines\n    print('\\n\\n\\n')\n    \n    # Printing the input shape\n    print(x_train.shape[1], y_train.shape[1])\n    \n    # creating the model\n    model = create_model(x_train.shape[1], y_train.shape[1])\n    \n    # Defining model callbacks\n    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n    \n    checkpoint_path = f'Fold_{fold}_part2.hdf5'\n    \n    cb_checkpt = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=0, save_best_only=True, \n                                save_weights_only=True, mode='min')\n    \n    \n    # Fitting the model\n    model.fit(x_train, y_train, validation_data=(x_valid, y_valid), epochs=35, batch_size=128,\n             callbacks = [reduce_lr_loss, cb_checkpt], verbose=1)\n    \n    # Loading the best model\n    model.load_weights(checkpoint_path)\n    \n    # Making the predictions\n    y_valid_pred = model.predict(x_valid)\n    \n    # converting the predictions to dataframe\n    y_valid_pred = pd.DataFrame(y_valid_pred, columns=y_valid.columns.values)\n    \n    # Evaluating the final results\n    print('\\n\\n\\n')\n    print('OOF Metric: ', metric(y_valid, y_valid_pred))\n    \n    return ","5e2e228c":"def run(fold):\n    \n    run_part2(fold)\n    \n    return","fd072cbf":"run(0)","d1f6e2d5":"def Seq_model(num_inputs, num_outputs):\n    \n    model = tf.keras.Sequential([\n        \n        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1024), input_shape=(1, num_inputs)),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.5),\n        \n        tfa.layers.WeightNormalization(tf.keras.layers.Dense(1048, activation=\"relu\")),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.5),\n        \n        tfa.layers.WeightNormalization(tf.keras.layers.Dense(num_outputs, activation=\"sigmoid\"))\n        \n    ])\n    \n    \n    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(), sync_period=10),\n                  loss='binary_crossentropy', \n                  )\n    \n    return model","fbca2dfa":"def run_seq_model(fold):\n    \n    train_df = train_features.merge(train_targets, on='sig_id', how='outer')\n    \n    # defining the parameters\n    cols = train_df.columns\n    \n    ID = cols[0]\n    fold_col = cols[-1]\n    features = cols[1:880]\n    # we are skipping the mid (947th column) as it is the fold column of the previous part\n    targets = cols[880:-1]\n    \n    # loading the data\n    train = train_df[train_df['fold'] != fold]\n    valid = train_df[train_df['fold'] == fold]\n    \n    x_train = train.loc[:, features]\n    x_valid = valid.loc[:, features]\n    \n    y_train = train.loc[:, targets]\n    y_valid = valid.loc[:, targets]\n    \n    \n    # reshaping the data for LSTM\n    x_train = np.array(x_train).reshape(-1, 1, 879)\n    x_valid = np.array(x_valid).reshape(-1, 1, 879)\n    \n    # Printing the input shape\n    print(x_train.shape[2], y_train.shape[1])\n    \n    # creating the model\n    model = Seq_model(x_train.shape[2], y_train.shape[1])\n    \n    # Defining model callbacks\n    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n    \n    checkpoint_path = f'Fold_{fold}_Seq.hdf5'\n    \n    cb_checkpt = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=0, save_best_only=True, \n                                save_weights_only=True, mode='min')\n    \n    # Fitting the model\n    model.fit(x_train, y_train, validation_data=(x_valid, y_valid), epochs=35, batch_size=128,\n             callbacks = [reduce_lr_loss, cb_checkpt], verbose=1)\n    \n    # Loading the best model\n    model.load_weights(checkpoint_path)\n    \n    # Making the predictions\n    y_valid_pred = model.predict(x_valid)\n    \n    # converting the predictions to dataframe\n    y_valid_pred = pd.DataFrame(y_valid_pred, columns=y_valid.columns)\n    \n    # Evaluating the final results\n    print('\\n\\n\\n')\n    print('OOF Metric: ', metric(y_valid, y_valid_pred))\n    \n    return","705fbf95":"run_seq_model(0)","b23763d1":"sample_submission.head(2)","49fa5840":"test_features.head(2)","48615863":"test_features = preprocess(test_features)","fa0060bf":"test_features.head(2)","81e2f3f8":"model = create_model(879, 206)\n\nmodel.load_weights('Fold_0_basic.hdf5')\n\npred = model.predict(test_features.iloc[:, 1:])","f7545ace":"pred = pd.DataFrame(pred, columns = train_targets.columns.values[1:-1])\n\nsub_file1 = sample_submission.copy()\nsub_file1.iloc[:, 1:] = pred","c84540c4":"sub_file1.to_csv('sub1.csv', index=False)","e6e97002":"model_part1 = create_model(879, 402)\n\nmodel_part1.load_weights('Fold_0_part1.hdf5')\n\npred_1 = model_part1.predict(test_features.iloc[:, 1:])\n\npred_1 = pd.DataFrame(pred_1, columns = train_non_targets.columns.values[1:-1])\npred_1['sig_id'] = test_features['sig_id']\n\nfeatures_test = test_features.copy()\n\nfeatures_test = features_test.merge(pred_1, on='sig_id', how='outer')","58c0e8f6":"model_part2 = create_model(1281, 206)\n\nmodel_part2.load_weights('.\/Fold_0_part2.hdf5')\n\npred_2 = model_part2.predict(features_test.iloc[:, 1:])\npred_2 = pd.DataFrame(pred_2, columns = train_targets.columns.values[1:-1])\n\nsub_file2 = sample_submission.copy()\nsub_file2.iloc[:, 1:] = pred_2","3d4825a8":"sub_file2.to_csv('sub2.csv', index=False)","d8fe5850":"model_seq = Seq_model(879, 206)\n\nmodel_seq.load_weights('.\/Fold_0_Seq.hdf5')\n\nto_pred = np.array(test_features.iloc[:, 1:]).reshape(-1, 1, 879)\n\npred_3 = model_seq.predict(to_pred)\n\npred_3 = pd.DataFrame(pred, columns = train_targets.columns.values[1:])\n\nsub_file3 = sample_submission.copy()\nsub_file3.iloc[:, 1:] = pred_3","4bb483fc":"sub_file3.to_csv('sub3.csv', index=False)","bccfc443":"## Loading the data","ab90f54b":"In this we will make a simple model and use it to predict the target labels.","e5eeee7f":"Note: I have run the notebook for only one fold. You should run it for others as well I ran the others on Colab and got similar result.","7e4abbea":"### Method-2","60a0b3fa":"## Create model and metric","f51200ce":"## Method-1","3123106f":"## Making Submissions","cff853f3":"In this we will make a model to predict the train_non_scored columns and then we will merge that with the data we have as features and then we will use that whole data as our feature vector.","46ee3091":"## HIT THE UPVOTE.... Thanks!!","6ff48f5d":"### Method-1","db2883e5":"## Unifying the data","bf031c99":"## Don't forget to hit the upvote later","aed81f1e":"## Preprocessing the data","9e308654":"### Method-3","5ea5e58b":"## Method-3","a27fd9fa":"## Method-2","1375b5da":"## EDA Non-targets","bd96e322":"### Although The OOF Metric didn't changed for the better but if you look at the validation_loss you will see that it is lower in Method2 in compare to Method1.","4e986953":"This is based upon the term \"Gene Sequence\". What if the Sequence could give us better results.","9ee95492":"## Creating folds"}}