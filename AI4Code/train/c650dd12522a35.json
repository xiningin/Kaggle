{"cell_type":{"a0234f5a":"code","b916578d":"code","74d8274f":"code","10b94354":"code","c5f9aea3":"code","463b1a7e":"code","64683465":"code","ccf2dbf0":"code","a638e15c":"code","ca0fc0a3":"code","a299ba33":"code","f3f3ccf6":"code","863efcc9":"code","5ee047a4":"code","2279b673":"code","efc83872":"code","471a535d":"code","f8596c0d":"code","688641ba":"code","2861ee4d":"code","c52d4bb2":"code","bbfcae84":"code","69beb5f7":"code","9a4fe2b1":"code","b9e4cfd6":"code","2c055651":"code","deeaab02":"markdown","7003d8e1":"markdown","0da41a6c":"markdown","69004155":"markdown","1a9ea583":"markdown","2c09c74b":"markdown","b8323338":"markdown","1d56597d":"markdown","69fff4e4":"markdown","ce5179aa":"markdown","f8d88cb6":"markdown","97d0ed67":"markdown","7fc9ab81":"markdown","d6c69021":"markdown","383890c1":"markdown","3269905f":"markdown","bd379a68":"markdown","141dc0df":"markdown","e90ca4d6":"markdown","9d3c53b4":"markdown","525e3be3":"markdown","90c31703":"markdown","8f8c6b0c":"markdown","23d2740f":"markdown","7311a607":"markdown","4e6b190a":"markdown","e06e6397":"markdown","92fd1312":"markdown","45e8d0f8":"markdown","026fee9b":"markdown","d86a2f6a":"markdown","d8d7a2f5":"markdown","1fd63684":"markdown","8c811f96":"markdown","c59afe8b":"markdown","7dc1c853":"markdown","74f49bdb":"markdown"},"source":{"a0234f5a":"import pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#from statistics import mean, stdev\n\n\n#from sklearn.tree import DecisionTreeClassifier\n#from sklearn.ensemble import RandomForestClassifier\n#from sklearn.ensemble import GradientBoostingClassifier\n#from lightgbm import LGBMClassifier\n","b916578d":"diabetes_df = pd.read_csv(\"..\/input\/diabetes-data-set\/diabetes.csv\")\ndiabetes_df","74d8274f":"diabetes_df.info()","10b94354":"diabetes_df.describe().T","c5f9aea3":"diabetes_df.hist(bins = 10, figsize = (10,10))\nplt.show()","463b1a7e":"sns.countplot(\"Outcome\", data = diabetes_df);","64683465":"corr_matrix = diabetes_df.corr()\nplt.figure(figsize = (20,15))\nsns.heatmap(corr_matrix,annot = True);","ccf2dbf0":"diabetes_df.info()","a638e15c":"sns.relplot(x = \"Glucose\", y = \"BloodPressure\", hue = \"Outcome\", data = diabetes_df);","ca0fc0a3":"sns.relplot(x = \"Glucose\", y = \"Insulin\", hue = \"Outcome\", data = diabetes_df);","a299ba33":"target_df = diabetes_df.loc[:,'Outcome']\ninput_df = diabetes_df.drop(['Outcome'],axis = 1).copy()","f3f3ccf6":"print(\"Shape of input dataframe \", input_df.shape)\nprint(\"Shape of target dataframe \", target_df.shape)","863efcc9":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaled_X = scaler.fit_transform(input_df)","5ee047a4":"scaled_X","2279b673":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(input_df, target_df, test_size = 0.2, random_state = 143, stratify = target_df) ","efc83872":"print(\"Shape of Training input features \", X_train.shape)\nprint(\"Shape of training output label \", y_train.shape)\n\nprint(\"Shape of test input features \", X_test.shape)\nprint(\"Shape of test output label \", y_test.shape)","471a535d":"X_train","f8596c0d":"from sklearn.linear_model import LogisticRegression\nlogistic_model = LogisticRegression(solver = 'liblinear', random_state = 143)\nlogistic_model.fit(X_train, y_train)","688641ba":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\n\n#Prediction on training data\npredicted_values = logistic_model.predict(X_train)\nprint(\"Accuracy score for Training data - \", accuracy_score(predicted_values, y_train))\n\n#Get the precision score \nprint(\"Precision score -\", precision_score(y_train, predicted_values))\n\n# Get the recall score \nprint(\"Recall score - \", recall_score(y_train, predicted_values))\n\n#Get the f1 score\nprint(\"F1 score - \", f1_score(y_train, predicted_values))\n\n#Get the confusion matrix\nprint(\"Confusion matrix for training set - \\n\", confusion_matrix(y_train, predicted_values))\n\n#Plotting confusion matrix\nprint(\"\\n\")\nplot_confusion_matrix(logistic_model, X_train, y_train)\nplt.show()","2861ee4d":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_confusion_matrix\n\n#Prediction on test data\npredicted_values = logistic_model.predict(X_test)\nprint(\"Accuracy score for Test data - \", accuracy_score(predicted_values, y_test))\n\n# Get the precision score \nprint(\"Precion score - \", precision_score(y_test, predicted_values))\n\n# Get the recall score \nprint(\"Recall score - \", recall_score(y_test, predicted_values))\n\n#Get the f1 score\nprint(\"F1 score - \", f1_score(y_test, predicted_values))\n\n#Get the confusion matrix \nprint(\"Confusion matrix for test set -\\n\", confusion_matrix(y_test, predicted_values))\n\n#Plot confusion matrix \nprint(\"\\n\")\nplot_confusion_matrix(logistic_model, X_test, y_test)\nplt.show()","c52d4bb2":"from sklearn.tree import DecisionTreeClassifier\ndecision_tree_model = DecisionTreeClassifier(random_state = 143)\ndecision_tree_model.fit(X_train, y_train)","bbfcae84":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\n\n#Prediction on training data\npredicted_values = decision_tree_model.predict(X_train)\nprint(\"Accuracy score for Training data - \", accuracy_score(predicted_values, y_train))\n\n#Get the precision score \nprint(\"Precision score -\", precision_score(y_train, predicted_values))\n\n# Get the recall score \nprint(\"Recall score - \", recall_score(y_train, predicted_values))\n\n#Get the f1 score\nprint(\"F1 score - \", f1_score(y_train, predicted_values))\n\n#Get the confusion matrix\nprint(\"Confusion matrix for training set - \\n\", confusion_matrix(y_train, predicted_values))\n\n#Plotting confusion matrix\nprint(\"\\n\")\nplot_confusion_matrix(decision_tree_model, X_train, y_train)\nplt.show()","69beb5f7":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_confusion_matrix\n\n#Prediction on test data\npredicted_values = decision_tree_model.predict(X_test)\nprint(\"Accuracy score for Test data - \", accuracy_score(predicted_values, y_test))\n\n# Get the precision score \nprint(\"Precion score - \", precision_score(y_test, predicted_values))\n\n# Get the recall score \nprint(\"Recall score - \", recall_score(y_test, predicted_values))\n\n#Get the f1 score\nprint(\"F1 score - \", f1_score(y_test, predicted_values))\n\n#Get the confusion matrix \nprint(\"Confusion matrix for test set -\\n\", confusion_matrix(y_test, predicted_values))\n\n#Plot confusion matrix \nprint(\"\\n\")\nplot_confusion_matrix(decision_tree_model, X_test, y_test)\nplt.show()","9a4fe2b1":"from sklearn.tree import DecisionTreeClassifier\ndecision_tree_model = DecisionTreeClassifier(random_state = 143, max_depth = 3, max_features = 4)\ndecision_tree_model.fit(X_train, y_train)","b9e4cfd6":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\n\n#Prediction on training data\npredicted_values = decision_tree_model.predict(X_train)\nprint(\"Accuracy score for Training data - \", accuracy_score(predicted_values, y_train))\n\n#Get the precision score \nprint(\"Precision score -\", precision_score(y_train, predicted_values))\n\n# Get the recall score \nprint(\"Recall score - \", recall_score(y_train, predicted_values))\n\n#Get the f1 score\nprint(\"F1 score - \", f1_score(y_train, predicted_values))\n\n#Get the confusion matrix\nprint(\"Confusion matrix for training set - \\n\", confusion_matrix(y_train, predicted_values))\n\n#Plotting confusion matrix\nprint(\"\\n\")\nplot_confusion_matrix(decision_tree_model, X_train, y_train)\nplt.show()","2c055651":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_confusion_matrix\n\n#Prediction on test data\npredicted_values = decision_tree_model.predict(X_test)\nprint(\"Accuracy score for Test data - \", accuracy_score(predicted_values, y_test))\n\n# Get the precision score \nprint(\"Precion score - \", precision_score(y_test, predicted_values))\n\n# Get the recall score \nprint(\"Recall score - \", recall_score(y_test, predicted_values))\n\n#Get the f1 score\nprint(\"F1 score - \", f1_score(y_test, predicted_values))\n\n#Get the confusion matrix \nprint(\"Confusion matrix for test set -\\n\", confusion_matrix(y_test, predicted_values))\n\n#Plot confusion matrix \nprint(\"\\n\")\nplot_confusion_matrix(decision_tree_model, X_test, y_test)\nplt.show()","deeaab02":"We can see that lesser the Glucose level, lesser the chances of being diabetic.","7003d8e1":"## Other relationships","0da41a6c":"**Note** *solver='liblinear'* is added due to an error which appearted while fitting the model on training set. Details of the error along with the resolution is given on [Stackoverflow](https:\/\/stackoverflow.com\/questions\/65682019\/attributeerror-str-object-has-no-attribute-decode-in-fitting-logistic-regre)","69004155":"## Preparing the Data for Model Building\n\n","1a9ea583":"# References \n- https:\/\/www.niddk.nih.gov\/health-information\/diabetes\/overview\/what-is-diabetes","2c09c74b":"## Splitting data into train and test dataset\n\nWe need to split our data into training and test set. Since our label is a binary classification label, and is imbalanced (more no than yes), we need to use Stratified split on Outcome column to ensure that our training and test data has an equal proportion of these classes for optimum results\n","b8323338":"**Logistic Regression - So we have an accuracy of 81% for test data**","1d56597d":"## Perform descriptive analytics on dataset","69fff4e4":"# Overview \nDiabetes is a disease that occurs when your blood glucose, also called blood sugar, is too high. Blood glucose is your main source of energy and comes from the food you eat. Insulin, a hormone made by the pancreas, helps glucose from food get into your cells to be used for energy. Sometimes your body doesn\u2019t make enough\u2014or any\u2014insulin or doesn\u2019t use insulin well. Glucose then stays in your blood and doesn\u2019t reach your cells.\n\nOver time, having too much glucose in your blood can cause health problems. Although diabetes has no cure, you can take steps to manage your diabetes and stay healthy.\n\nAs of 2015, 30.3 million people in the United States, or 9.4 percent of the population, had diabetes. More than 1 in 4 of them didn\u2019t know they had the disease. Diabetes affects 1 in 4 people over the age of 65. About 90-95 percent of cases in adults are type 2 diabetes.\n\n# Obective \nClassifying a patient on whether he\/she is diabetic or not based on the various input data points provided in the dataset. ","ce5179aa":"## Training model using Decision Tree Classifier with hyperparamters","f8d88cb6":"### Evaluating model performance after hyperparamter tuning \nLet us evaluate model performance on the training data","97d0ed67":"### Evaluating model performance\nLet us evaluate model performance on training data","7fc9ab81":"## Train the classification model - Logistic Regression","d6c69021":"We can see accuracy score of around 74 % with other values too reduced compared to the training outcome. This clearly signifies that **DecisionTreeClassifier has learnt the training data as it is and is a case of overfitting**. We need to tune the hyperparameters to improve this scenario","383890c1":"## Training model using Decision Tree Classifier","3269905f":"### Distribution of each feature\nDistribution  or Density of data is a critical check which helps us understand on how our data is distributed. Let us plot a historygram for all our features ","bd379a68":"We have total of 768 rows and 9 columns in the dataset","141dc0df":"### Scoring and evaluation\nLet us first test our model on the training data itself","e90ca4d6":"Let us now evaluate model performance on Test data","9d3c53b4":"## Import required libraries","525e3be3":"Our Outcome column is inbalanced. The number of people that don't have diabetes are in higher ratio than the number of people that have diabetes.","90c31703":"## Data \nThe data is openly available on [Kaggle](https:\/\/www.kaggle.com\/mathchi\/diabetes-data-set). \nBelow are the features which are provided in our input dataset. \n**Note-** Please note that all the records in the dataset are for females\n\n* **Pregnancies:** Number of times pregnant\n* **Glucose:** Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n* **BloodPressure:** Diastolic blood pressure (mm Hg)\n* **SkinThickness:** Triceps skin fold thickness (mm)\n* **Insulin:** 2-Hour serum insulin (mu U\/ml)\n* **BMI:** Body mass index (weight in kg\/(height in m)^2)\n* **DiabetesPedigreeFunction:** Diabetes pedigree function\n* **Age:** Age (years)\n* **Outcome:** Class variable (0 or 1)\n","8f8c6b0c":"# Steps\n- Setting up the development environment (Kaggle notebook in our case is already set)\n- Import required libraries \n- Import dataset in pandas \n- Perform descriptive analysis on data\n- Perform exploratory data analysis using statistics and visualisations \n- Prepare data for training - data cleaning, normalization, scaling, etc\n- Choose algorithms for training \n- Score and Evaluate \n- Try other algorithms if needed ","23d2740f":"## Normalizing the value \n\nWe can see that various features has input values ranging at different scale. To ensure that one feature value does not impact the model more than the other having lesser values, we need to normalize the values so that they have a proper scaling. Let us use StandardScaler for the same","7311a607":"### Correlation between the features\nA correlation signifies on how a feature is related to other. Does a feature impact the outcome our not. Let us do that.","4e6b190a":"### Separate target label and feature columns","e06e6397":"We can see that our dataset is pretty clean and has no missing values. Further all our inputs or features are integer or float format and hence is not categorical. Our Label, i.e. Outcome is categorical though","92fd1312":"## Load dataset in notebook","45e8d0f8":"### Count number instances for diabetes and non-diabetes","026fee9b":"From the heatmap we can see that there are a few features which are having a mild correlation with the Outcome. For example Glucose, BMI. Rest other features are in weak correlation. Let us get into more details of this coorelation using scatter plots","d86a2f6a":"# Conclusion\nWe have tried two models for classification and one can many more like SVM, KNN, RandomForestClassifier, etc. Developing a ML model is an iterative process until we receive the desired performance. For now, we will conlude that the model developed using Logistic Regression performed better","d8d7a2f5":"Evaluating performance on test data","1fd63684":"From above graph we get a mixed sesnse that higher insulin does results into being diabetic","8c811f96":"- From above graph we can see that Glucose, Blood Pressure, and BMI feaures have the normal distribution. This signifies that data is scattered around the mean value for all these features\n- In Outcome column we can see that people with no diabetes, i.e. 0 are almost double of that of people with diabetes. This means that our data has a **class imbalance **\n- Other columns have skewed data ","c59afe8b":"## Exploratory data analysis","7dc1c853":"See the interesting outcome we have above. DecisionTreeClassifier has accuracy, precision, recall of 1 and has correctly predicted all values on the training set. Let us try the performance on the test set","74f49bdb":"From the above outcome statistical insights on various features. For example, \n- we have mean pregnancies as 3.84 with a maximum of 17(that is too much and you might want to cross check the data source)\n- Other features like Glucose, Bloodpressure, etc too has mean, std. etc. revealed\n- In insulin we have a very high standard deviation which means data is distributed in a larger range"}}