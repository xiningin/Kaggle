{"cell_type":{"7c7fafd1":"code","e9f515a6":"code","cdeddcfd":"code","cc6e3d0d":"code","8a60106b":"code","8c29048e":"code","f1395a04":"code","ff675952":"code","fbf78f5a":"code","04a8fce0":"code","e0779a4e":"code","e33daedc":"code","d2ada61e":"code","53c30831":"code","73da0d2a":"markdown","ff1d27de":"markdown","788a8bbb":"markdown","699c45d8":"markdown","d39ecced":"markdown","146bea0b":"markdown","930e3ce3":"markdown","f7d472df":"markdown","427ebf41":"markdown","f67584a4":"markdown"},"source":{"7c7fafd1":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e9f515a6":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","cdeddcfd":"dataset_cols = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\ndataset = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', header=None, encoding='ISO-8859-1', names=dataset_cols)","cc6e3d0d":"import re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\ndef preprocess(text):\n  review = re.sub('[^a-zA-Z]',' ',text) \n  review = review.lower()\n  review = review.split()\n  ps = PorterStemmer()\n  all_stopwords = stopwords.words('english')\n  all_stopwords.remove('not')\n  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n  return ' '.join(review)","8a60106b":"dataset.text = dataset.text.apply(lambda x: preprocess(x))","8c29048e":"print(dataset.head())","f1395a04":"from sklearn.preprocessing import LabelEncoder\ny = dataset.target\nle = LabelEncoder()\ny = le.fit_transform(y)","ff675952":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(dataset.text, y, test_size = 0.20, random_state = 0)","fbf78f5a":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features=600)\nX_train_dtm = cv.fit_transform(X_train).toarray()","04a8fce0":"X_test_dtm = cv.transform(X_test).toarray()","e0779a4e":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train_dtm, y_train) ","e33daedc":"y_pred = classifier.predict(X_test_dtm)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","d2ada61e":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","53c30831":"import seaborn as sns\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))\n\n# confusion matrix\nfig, ax = plt.subplots()\nsns.heatmap(confusion_matrix(y_test, y_pred, normalize='true'), annot=True, ax=ax)\nax.set_title('Confusion Matrix')\nax.set_ylabel('Real Value')\nax.set_xlabel('Predicted Value')\n\nplt.show()","73da0d2a":"## Splitting the dataset into Training and Test set","ff1d27de":"## Making the Confusion Matrix","788a8bbb":"## Importing the Libraries","699c45d8":"# Twitter Sentiment Analysis","d39ecced":"## Predicting Test set results","146bea0b":"## Importing the Dataset","930e3ce3":"## Encoding categorical data   ","f7d472df":"## Cleaning the text","427ebf41":"## Training the Logistic Regression model on the Training set","f67584a4":"## Bag of Words Model"}}