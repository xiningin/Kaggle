{"cell_type":{"bc870f33":"code","0627b395":"code","15cb76f8":"code","d6bbe3f2":"code","571adc14":"code","ee0e448e":"code","f5a0c27e":"code","330d9520":"code","c19b8b5b":"code","0a9a02e7":"code","5225c169":"code","c9139ab4":"code","8af534f0":"code","d6438538":"markdown","4914d764":"markdown","32c34797":"markdown","27c76025":"markdown","375d5d16":"markdown","b9d05e96":"markdown","47172251":"markdown","e3715ca9":"markdown","a0ebc079":"markdown","1598e646":"markdown","2d1f854a":"markdown","2f5e4aaa":"markdown","f46f350b":"markdown","5590d2f1":"markdown","3e0591e1":"markdown"},"source":{"bc870f33":"import numpy as np \nimport pandas as pd \n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.svm import LinearSVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nBASE_PATH = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/\"","0627b395":"df = pd.read_csv(f\"{BASE_PATH}train.csv\")\nX = df.select_dtypes(\"number\").drop(\"SalePrice\", axis=1)\ny = df.SalePrice\npipe = make_pipeline(SimpleImputer(), RobustScaler(), LinearRegression())\nprint(f\"The R2 score is: {cross_val_score(pipe, X, y).mean():.4f}\")","15cb76f8":"num_cols = df.drop(\"SalePrice\", axis=1).select_dtypes(\"number\").columns\ncat_cols = df.select_dtypes(\"object\").columns\n\n# we instantiate a first Pipeline, that processes our numerical values\nnumeric_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer()),\n        ('scaler', RobustScaler())])\n\n# the same we do for categorical data\ncategorical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='most_frequent', fill_value='missing')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n    \n# a ColumnTransformer combines the two created pipelines\n# each tranformer gets the proper features according to \u00abnum_cols\u00bb and \u00abcat_cols\u00bb\npreprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numeric_transformer, num_cols),\n            ('cat', categorical_transformer, cat_cols)])\n\npipe = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', LinearRegression())])\n\nX = df.drop(\"SalePrice\", axis=1)\ny = df.SalePrice\nprint(f\"The R2 score is: {cross_val_score(pipe, X, y).mean():.4f}\")","d6bbe3f2":"# comment out all classifiers that you don't want to use\n# and do so for clf_names accordingly\nclassifiers = [\n               DummyRegressor(),\n               LinearRegression(n_jobs=-1), \n               Ridge(alpha=0.003, max_iter=30), \n               Lasso(alpha=.0005), \n               ElasticNet(alpha=0.0005, l1_ratio=.9),\n               KernelRidge(alpha=0.6, kernel=\"polynomial\", degree=2, coef0=2.5),\n               SGDRegressor(),\n               SVR(kernel=\"linear\"),\n               LinearSVR(),\n               RandomForestRegressor(n_jobs=-1, n_estimators=350, \n                                     max_depth=12, random_state=1),\n               GradientBoostingRegressor(n_estimators=500, max_depth=2),\n               lgb.LGBMRegressor(n_jobs=-1, max_depth=2, n_estimators=1000, \n                                 learning_rate=0.05),\n               xgb.XGBRegressor(objective=\"reg:squarederror\", n_jobs=-1, \n                                max_depth=2, n_estimators=1500, learning_rate=0.075),\n]\n\nclf_names = [\n            \"dummy\", \n            \"linear\", \n            \"ridge\",\n            \"lasso\",\n            \"elastic\",\n            \"kernlrdg\",\n            \"sgdreg\",\n            \"svr\",\n            \"linearsvr\",\n            \"randomforest\", \n            \"gbm\", \n            \"lgbm\", \n            \"xgboost\"\n]","571adc14":"def clean_data(data, is_train_data=True):\n    # add your code for data cleaning and feature engineering here\n    # e.g. create a new feature from existing ones\n    data['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\n\n    # add here the code that you only want to apply to your training data and not the test set\n    # e.g. removing outliers from the training data works... \n    # ...but you cannot remove samples from your test set.\n    if is_train_data == True:\n        data = data[data.GrLivArea < 4000]\n        \n    return data","ee0e448e":"def prepare_data(df, is_train_data=True):\n    \n    # split data into numerical & categorical in order to process seperately in the pipeline \n    numerical   = df.select_dtypes(\"number\").copy()\n    categorical = df.select_dtypes(\"object\").copy()\n    \n    # for training data only...\n    # ...convert SalePrice to log values and drop \"Id\" and \"SalePrice\" columns\n    if is_train_data == True :\n        SalePrice = numerical.SalePrice\n        y = np.log1p(SalePrice)\n        numerical.drop([\"Id\", \"SalePrice\"], axis=1, inplace=True)\n        \n    # for the test data: just drop \"Id\" and set \"y\" to None\n    else:\n        numerical.drop([\"Id\"], axis=1, inplace=True)\n        y = None\n    \n    # concatenate numerical and categorical data to X (our final training data)\n    X = pd.concat([numerical, categorical], axis=1)\n    \n    # in addition to X and y return the separated columns to use these separetely in our pipeline\n    return X, y, numerical.columns, categorical.columns","f5a0c27e":"def get_pipeline(classifier, num_cols, cat_cols):\n    # the numeric transformer gets the numerical data acording to num_cols\n    # the first step is the imputer which imputes all missing values to the mean\n    # in the second step all numerical data gets scaled by the StandardScaler()\n    numeric_transformer = Pipeline(steps=[\n        ('imputer', make_pipeline(SimpleImputer(strategy='mean'))),\n        ('scaler', StandardScaler())])\n    \n    # the categorical transformer gets all categorical data according to cat_cols\n    # again: first step is imputing missing values and one hot encoding the categoricals\n    categorical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='most_frequent', fill_value='missing')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n    \n    # the column transformer creates one Pipeline for categorical and numerical data each\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numeric_transformer, num_cols),\n            ('cat', categorical_transformer, cat_cols)])\n    \n    # return the whole pipeline with the classifier provided in the function call    \n    return Pipeline(steps=[('preprocessor', preprocessor), ('classifier', classifier)])","330d9520":"def score_models(df):\n    # retrieve X, y and the seperate columns names\n    X, y, num_cols, cat_cols = prepare_data(df)\n    \n    # since we converted SalePrice to log values, we use neg_mean_squared_error... \n    # ...rather than *neg_mean_squared_log_error* \n    scoring_metric = \"neg_mean_squared_error\"\n    scores = []\n    \n    for clf_name, classifier in zip(clf_names, classifiers):\n        # create a pipeline for each classifier\n        clf = get_pipeline(classifier, num_cols, cat_cols)\n        # set a kfold with 3 splits to get more robust scores. \n        # increase to 5 or 10 to get more precise estimations on models score\n        kfold = KFold(n_splits=3, shuffle=True, random_state=1)  \n        # crossvalidate and return the square root of the results\n        results = np.sqrt(-cross_val_score(clf, X, y, cv=kfold, scoring=scoring_metric))\n        scores.append([clf_name, results.mean()])\n\n    scores = pd.DataFrame(scores, columns=[\"classifier\", \"rmse\"]).sort_values(\"rmse\", ascending=False)\n    # just for good measure: add the mean of all scores to dataframe\n    scores.loc[len(scores) + 1, :] = [\"mean_all\", scores.rmse.mean()]\n    return scores.reset_index(drop=True)\n    ","c19b8b5b":"def train_models(df): \n    X, y, num_cols, cat_cols = prepare_data(df)\n    pipelines = []\n    \n    for clf_name, classifier in zip(clf_names, classifiers):\n        clf = get_pipeline(classifier, num_cols, cat_cols)\n        clf.fit(X, y)\n        pipelines.append(clf)\n    \n    return pipelines","0a9a02e7":"def predict_from_models(df_test, pipelines):\n    X_test, _ , _, _ = prepare_data(df_test, is_train_data=False)\n    predictions = []\n    \n    for pipeline in pipelines:\n        preds = pipeline.predict(X_test)\n        # we return the exponent of the predictions since we have log converted y for training\n        predictions.append(np.expm1(preds))\n    \n    return predictions","5225c169":"df = pd.read_csv(f\"{BASE_PATH}train.csv\")\ndf_test = pd.read_csv(f\"{BASE_PATH}test.csv\")\n\n# We clean the data\ndf = clean_data(df)\ndf_test = clean_data(df_test, is_train_data=False)","c9139ab4":"# We score the models on the preprocessed training data\nmy_scores = score_models(df)\ndisplay(my_scores)","8af534f0":"# We train the models on the whole training set and predict on the test data\nmodels = train_models(df)\npredictions = predict_from_models(df_test, models)\n# We average over the results of all 12 classifiers (simple ensembling)\n# we exclude the DummyRegressor and the SGDRegressor: they perform worst...\nprediction_final = pd.DataFrame(predictions[2:]).mean().T.values\n\nsubmission = pd.DataFrame({'Id': df_test.Id.values, 'SalePrice': prediction_final})\nsubmission.to_csv(f\"submission.csv\", index=False)","d6438538":"## <span style=\"color:darkgreen\">  \ud83c\udfcb\ufe0f\u200d\u2642\ufe0f Finally: Train the models\nFor each classifier we create and fit a pipeline.","4914d764":"## <font color=\"darkred\">How cool is that? \n> With **only 5 lines of code we imported our training data, separated describing features from the target variable, setup a pipeline with an Imputer (that fills in missing values), a Scaler and a LinearRegression classifier. We crossvalidated and printed out the result.** \n    \nIt can't be easier than that I think...  \n\nNow let's setup a pipeline that is able to **work on our categorical data as well.**","32c34797":"### <span style=\"color:darkgreen\">\ud83d\udc77\u200d\u2642\ufe0f Create the pipeline ","27c76025":"## <span style=\"color:darkgreen\">\ud83d\udd2e Make predictions with trained models  \nFor each fitted pipeline we retrieve predictions for SalePrice","375d5d16":"# Pipeline playground for 12+ classifiers (0.11860 LB)\nI want to **provide a clean, simple and beginner friendly template** that makes use of a flexible [scikit-learn pipeline](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html). I aim to **automate the datacleaning and feature engineering** as much as possible while **allowing for fast iteration**. \n\nThe **total training and processing time of the pipeline for 12 classifiers is around 20secs** (w\/o scoring the models) and gets you **0.11860 on the Leaderboard** with practically no fuss.","b9d05e96":"## Imports and globals \ud83e\udd16\nFirst we **import all the necessary libraries** and **set a base file path to the data sets**.","47172251":"## Choosing the estimators \ud83e\udd13\nSince this is meant as a sandbox for experimentation we set a list with 10 common classifiers (and their respective names) which we will use in our pipeline. \n\nThe initial hyperparameters I have [grid-searched](https:\/\/www.kaggle.com\/chmaxx\/extensive-data-exploration-modelling-python).","e3715ca9":"### <span style=\"color:darkgreen\">\ud83d\udc77\u200d\u2642\ufe0f Prepare our data for the pipeline ","a0ebc079":"### Have feedback? Found errors? Please let me know in the comments. \ud83d\udc4c\ud83d\ude0e","1598e646":"## <font color=\"darkred\">Even cooler... \n> With **only 9 lines of code** we processed all our features automagically. Our score improves accordingly.\n    \nNow we expand this to a playground for all the regression classifiers you can think of.","2d1f854a":"## \ud83d\ude80 And now: Let's use our pipeline... \n","2f5e4aaa":"## Setting up the Pipeline \ud83d\udca1\nWe will now setup simple functions to:\n*     **clean and prepare the data**\n*     **build the Pipeline**     \n*     **score models** \n*     **train models**\n*     **predict from models**","f46f350b":"## Superquick intro: What is a Pipeline?\nA pipeline is a [supercool class that scikit-learn provides](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html), which allows to chain so called transformers (that \u2013 uhmmm.... \u2013 transform your data) with a final estimator at the end. Let's look a a simple example.  ","5590d2f1":"## <span style=\"color:darkgreen\">\ud83c\udf21 Score the models with crossvalidation ","3e0591e1":"### <span style=\"color:darkgreen\">\ud83d\udc1a Encapsulate all our feature cleaning and engineering \nTo experiment, just add your code to this function.<\/span>"}}