{"cell_type":{"c80ddf8d":"code","8d37436b":"code","5310e135":"code","67bb02c3":"code","b6fedde8":"code","cb20c7c3":"code","17afc8bd":"code","2a59fee1":"code","a8825201":"code","d1ea0bc0":"code","e3c1dbea":"code","87205d77":"code","47b6c3f1":"code","d4e20569":"code","37f3a717":"code","c22b0603":"code","2c37509b":"code","7b017e4b":"code","ebf19c2f":"code","ecef002b":"code","772e2c4a":"code","423ce1fa":"code","94b84274":"code","87595610":"code","98bcc0a9":"code","b86b74ad":"code","8891bed1":"code","96418905":"code","d29b7e0b":"markdown","1f0a0877":"markdown","33fe1183":"markdown","0270ea6d":"markdown","35d8ce32":"markdown","e67135cc":"markdown","31500d45":"markdown","77c43c49":"markdown","8f9d4c2b":"markdown","88f56005":"markdown","827c57cb":"markdown","ece557cb":"markdown","58c95e8a":"markdown","fca78997":"markdown"},"source":{"c80ddf8d":"%%capture\n!pip install bs4 --quiet\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\n\n!pip install pandarallel\nfrom pandarallel import pandarallel\npandarallel.initialize(progress_bar=True)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\nfrom zipfile import ZipFile\nimport os\nimport re","8d37436b":"TEMP = '\/kaggle\/temp'\ntry:\n    os.mkdir(TEMP)\n    print(f'{TEMP} created')\nexcept:\n    print(f'{TEMP} already exists')","5310e135":"%%capture\n# install the latest VW version\n!git clone --recursive https:\/\/github.com\/VowpalWabbit\/vowpal_wabbit.git $TEMP\/vowpal_wabbit\n!cd $TEMP\/vowpal_wabbit\/; make \n!cd $TEMP\/vowpal_wabbit\/; make install","67bb02c3":"directory = '\/kaggle\/input\/word2vec-nlp-tutorial\/'\nfor file in os.listdir(directory):\n    if file.split('.')[-1] != 'zip':\n        continue\n    with ZipFile(directory+file, 'r') as archive:\n        archive.extractall()\n\nval_df = pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\ntest_df = pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\nval_df.shape, test_df.shape","b6fedde8":"test_df.head()","cb20c7c3":"stops = set(stopwords.words(\"english\"))\nstemmer = PorterStemmer()\n\n# the same as in preprocessing notebook\ndef preprocess_review(raw_review):\n    # Remove HTML\n    review_text = BeautifulSoup(raw_review,).get_text()\n    # Remove URLs\n    review_text = re.sub(\"https?:\\\/\\\/[\\w+.\\\/]+\", \" \", review_text)\n    # Remove non-letters\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n    # Convert to lower case, split into individual words\n    words = letters_only.lower().split()\n    # Remove stop words (and stem others if needed)\n    meaningful_words = [stemmer.stem(w) for w in words if not w in stops]\n        \n    return(\" \".join( meaningful_words))","17afc8bd":"%%time\nval_df['review'] = val_df['review'].parallel_apply(preprocess_review)\nval_df['sentiment'] = val_df['sentiment'].replace(0, -1)\ntest_df['review'] = test_df['review'].parallel_apply(preprocess_review)","2a59fee1":"np.savetxt(\"val.vw\", val_df[['sentiment', 'review']], delimiter=' |text ', fmt='%s')  # with labels\nnp.savetxt('test.vw', '|text ' + test_df['review'], fmt='%s')","a8825201":"# the real labels of training data are unknown, so lines with both possible labels\n# will be used for comparing, which seems much more faster than applying \"^-?1\" regex\nnp.savetxt(TEMP+'\/test_pattern0', '-1 |text ' + test_df['review'], fmt='%s')\nnp.savetxt(TEMP+'\/test_pattern1', '1 |text ' + test_df['review'], fmt='%s')\n!cat $TEMP\/test_pattern1 $TEMP\/test_pattern1 >$TEMP\/test_pattern","d1ea0bc0":"%%time\nINPUT_PATH = \"\/kaggle\/input\/preprocessing-for-vowpal-wabbit-sentiment-analysis\/train.vw\"\n!wc -l $INPUT_PATH\n# drop the train lines that appears in the test_pattern\n!grep -Fvxf $TEMP\/test_pattern $INPUT_PATH >\/kaggle\/temp\/temp.vw\n!wc -l $TEMP\/temp.vw\n!grep -Fvxf val.vw $TEMP\/temp.vw >train.vw \n!wc -l train.vw ","e3c1dbea":"%%time\n!vw --data=train.vw \\\n    --ngram=2 \\\n    --bit_precision=22 \\\n    --loss_function=hinge \\\n    --final_regressor=model.vw","87205d77":"%%time\n# predict\n!vw --initial_regressor=model.vw \\\n    --testonly \\\n    --data=val.vw \\\n    --ngram=2 \\\n    --binary \\\n    --predictions=pred.txt \\\n    --raw_predictions=pred_margins.txt","47b6c3f1":"from sklearn.metrics import classification_report\ny_pred = np.loadtxt('pred.txt', dtype='int')\ny_true = val_df['sentiment']\nprint(classification_report(y_true, y_pred, digits=4))","d4e20569":"from sklearn.metrics import hinge_loss, roc_auc_score\nraw = np.loadtxt('pred_margins.txt')\nprint(f'Hinge loss: {hinge_loss(y_true, raw)}')\nprint(f'ROC AUC: {roc_auc_score(y_true, raw)}')","37f3a717":"from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n\ndef vw_svm_gridsearch_scores(search_params, additional_params=\"\"):\n    '''\n    Fits VW SVM model (hinge loss) for each element of search space\n    and returns accuracy, f1 and hinge scores of each fit.\n    \n    search_params: 1d iterable\n        list of vw param strings to be tried\n        E.g. [\"--l2=0.1\", \"--l2=1\", \"--l2=10\"]\n    additional_params: string\n        additional parameters that to be applied to each fit\n        E.g. \"--bit_precision=26 --ngram=2\"\n    '''\n    acc_list, f1_list, loss_list, auc_list = [], [], [], []\n    \n    for param in tqdm(search_params):\n        # yeah, cursed\n        # fit\n        !vw --data=train.vw \\\n            $param $additional_params \\\n            --loss_function=hinge \\\n            --quiet \\\n            --final_regressor=model.vw\n\n        # predict\n        !vw --initial_regressor=model.vw \\\n            --testonly \\\n            $param $additional_params \\\n            --data=val.vw \\\n            --binary \\\n            --quiet \\\n            --predictions=val.pred \\\n            --raw_predictions=val_raw.pred\n\n        y_pred = np.loadtxt('val.pred', dtype='int')\n        raw = np.loadtxt('val_raw.pred')\n        acc_list.append(accuracy_score(y_true, y_pred))\n        f1_list.append(f1_score(y_true, y_pred))\n        loss_list.append(hinge_loss(y_true, raw))\n        auc_list.append(roc_auc_score(y_true, raw))\n    return {\n        'accuracy': acc_list,\n        'f1': f1_list,\n        'hinge': loss_list,\n        'roc_auc': auc_list\n    }","c22b0603":"def plot_scores(scores, ticks=None):\n    if ticks is None:\n        ticks = range(len(scores['roc_auc']))\n    fig, ax = plt.subplots(1, 3, figsize=(14, 5))\n    ax[0].plot(ticks, scores['roc_auc'], \"o-\")\n    ax[0].set_title('AUC')\n    ax[1].plot(ticks, scores['f1'], \"o-\")\n    ax[1].set_title('F1');\n    ax[2].plot(ticks, scores['hinge'], \"o-\")\n    ax[2].set_title('Hinge');","2c37509b":"hashdims = [f\"--bit_precision={i}\" for i in range(18, 30, 2)]\nscores = vw_svm_gridsearch_scores(hashdims, \"--ngram=2\")\nplot_scores(scores)","7b017e4b":"hashdims = [f\"--bit_precision={i}\" for i in range(18, 30, 2)]\nscores = vw_svm_gridsearch_scores(hashdims, \"--ngram=2 --ngram=3 --ngram=4\")\nplot_scores(scores, range(18, 30, 2))","ebf19c2f":"a = [f'--ngram={i}' for i in range(2, 7)]\nngrams = [\" \".join(a[:i]) for i in range(len(a)) ]\nscores = vw_svm_gridsearch_scores(ngrams, \"--bit_precision=26 --binary\")\nplot_scores(scores, range(1, 6))","ecef002b":"from itertools import product\na = [f'--ngram={i}' for i in range(2, 6)]\nngrams = [\" \".join(a[:i]) for i in range(len(a))]\nhashdims = [f\"--bit_precision={i}\" for i in range(24, 30, 2)]\nsearch_space = [\" \".join(i) for i in product(hashdims, ngrams)]\nscores = vw_svm_gridsearch_scores(search_space, \"--binary\")\nplot_scores(scores)","772e2c4a":"argmax = np.argmin(scores['hinge']) \nbest_params = search_space[argmax]\nbest_params","423ce1fa":"# !python $TEMP\/vowpal_wabbit\/utl\/vw-hyperopt.py \\\n#     --train=train.vw \\\n#     --holdout=val.vw \\\n#     --outer_loss_function=hinge \\\n#     --vw_space=\"--l2=1e-8..1e-2~LO --l1=1e-8..1e-2~LO\" \\\n#     --additional_cmd=\"--binary --bit_precision=28 --ngram=2 --ngram==3 --loss_function=hinge --quiet\" \\\n#     --max_eval=10","94b84274":"# !tail log.log -n 9","87595610":"%%time\n!vw --data=train.vw \\\n     $best_params \\\n    --loss_function=hinge \\\n    --final_regressor=model.vw\n\n# predict\n!vw --initial_regressor=model.vw \\\n    --testonly \\\n    --data=val.vw \\\n     $best_params \\\n    --binary \\\n    --predictions=val.pred \\\n    --raw_predictions=val_raw.pred","98bcc0a9":"y_true = val_df['sentiment']\ny_pred = np.loadtxt('val.pred', dtype='int')\nraw = np.loadtxt('val_raw.pred')\nprint(classification_report(y_true, y_pred, digits=4))\n\nprint(f'Hinge loss: {hinge_loss(y_true, raw)}')\nprint(f'ROC AUC: {roc_auc_score(y_true, raw)}')","b86b74ad":"!vw --initial_regressor=model.vw \\\n    --testonly \\\n    --data=test.vw \\\n     $best_params \\\n    --binary \\\n    --quiet \\\n    --raw_predictions=test.pred\n\na = np.loadtxt(\"test.pred\")\ntest_df['sentiment'] = a\ntest_df[['id','sentiment']].to_csv(\"submission.csv\", index=False, quoting=3) # 0.97240 accuracy!!","8891bed1":"test_df","96418905":"!rm *.log\n!rm *.pred\n!rm *.json\n!rm *.cache\n!rm *.txt\n!rm *.tsv","d29b7e0b":"Unzip and load test and original train data, it will be used for validation","1f0a0877":"# IMDB sentiment with Vowpal Wabbit\nAttempt to train model with the largest available dataset: [IMDb Largest Review Dataset by Enam Biswas](https:\/\/www.kaggle.com\/ebiswas\/imdb-review-dataset)\n\nMy preprocessing of the dataset can be found in [this kernel](https:\/\/www.kaggle.com\/andrii0yerko\/preprocessing-for-vowpal-wabbit-sentiment-analysis)\n\nThe model will be linear classifier on bag of words, that can be easily implemented with VW.\nThe main benefit of VW is that it works out-of-core, that means we don't need to load all the dataset into RAM and build a dictionary for bag of words, that can be to large in case of limited RAM, despite VW reads dataset line by line and create bag of words implicitly using hashing trick.","33fe1183":"Searching for the best hashdim and ngrams combination","0270ea6d":"Save in the Vowpal Wabbit format","35d8ce32":"## Environment preparation","e67135cc":"# Vowpal Wabbit\nLet's start with SVM model, bigrams and 22 bit hash","31500d45":"## Making a submission","77c43c49":"# Ways for improvement\n- Convergence hyperparameters tuning\n- Usually, set log loss as the objective is a good idea when roc auc is a target metric. Should be tried.\n- Better preprocessing. What about lemmatization instead of stemming? Maybe we should choose higher and lower rating thresholds for the positive and negative classes than ones used by original dataset creators.","8f9d4c2b":"# Final model","88f56005":"## Test & validation sets preprocessing\nValidation and test sets must be preprocessed in the same way as a training one.","827c57cb":"## Hyperparameter tuning\nLet's explore how the hash dimension and the ngrams affect the model quality","ece557cb":"At first, let's receive some intuition on how do the choosen hyperparameters impact the resulting score.","58c95e8a":"### Regularization\nRegularization doesn't help on such sparse data, so I wouldn't try it. If you want to be sure about this yourself, you can run the following code snippet based on `vw-hyperopt.py` (included in vw)","fca78997":"Let's respect the rules, and drop test lines from the training data.\n\nThe same will be done for the validation set to preserve data leakage"}}