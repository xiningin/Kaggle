{"cell_type":{"e1f0a138":"code","fac8833f":"code","e6717d2d":"code","2ffa2a9b":"code","7060b5eb":"code","1d485636":"code","6c778bb6":"code","2e16691e":"code","98b67526":"code","f4a67c2b":"code","8d7e7058":"code","34683314":"code","12eb9de0":"code","4862df72":"code","11c73423":"code","33d1934a":"code","a0791c72":"code","b50da945":"code","9f54b246":"code","413618de":"code","d7b8cfc1":"markdown","0cfc68cc":"markdown","f1b59aee":"markdown","9ca0ea2b":"markdown","05f7f008":"markdown","ea35ed2d":"markdown","6c57af84":"markdown","d1242184":"markdown","555995eb":"markdown","5abd3a09":"markdown","736a10fa":"markdown","92a245ed":"markdown","047657a2":"markdown","38143fb4":"markdown","76fee242":"markdown","3bc1c3da":"markdown","1741e8bc":"markdown","eea26046":"markdown","ced56a34":"markdown","1128d87a":"markdown","f7c4054e":"markdown","47595e57":"markdown","dbe64123":"markdown"},"source":{"e1f0a138":"from tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom keras.preprocessing.image import load_img\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import layers, callbacks\nimport cv2\nfrom IPython.display import Image\nfrom glob import glob\nimport random","fac8833f":"num_classes = 2\nresnet_weights_path = '..\/input\/resnet50\/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\nmy_new_model = Sequential()\nmy_new_model.add(ResNet50(include_top=False, pooling='avg', weights=resnet_weights_path))\nmy_new_model.add(Dense(num_classes, activation='softmax'))\n\n# Indicate whether the first layer should be trained\/changed or not.\nmy_new_model.layers[0].trainable = False","e6717d2d":"my_new_model.compile(optimizer='sgd', \n                     loss='categorical_crossentropy', \n                     metrics=['accuracy'])","2ffa2a9b":"images = '..\/input\/racoon-detection\/Images'","7060b5eb":"EPOCHS = 20\nimage_size = 224\ndata_generator = ImageDataGenerator(preprocess_input)\nImage_amount = 10\n\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=4, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)","1d485636":"train_generator = data_generator.flow_from_directory(\n                                        directory='..\/input\/pandas-vs-trash-pandas-raccoons\/RACCOOOON\/train',\n                                        target_size=(image_size, image_size),\n                                        batch_size=Image_amount,\n                                        class_mode='categorical')\n\nvalidation_generator = data_generator.flow_from_directory(\n                                        directory='..\/input\/pandas-vs-trash-pandas-raccoons\/RACCOOOON\/val',\n                                        target_size=(image_size, image_size),\n                                        class_mode='categorical')","6c778bb6":"history = my_new_model.fit(train_generator,\n                                steps_per_epoch=2,\n                                epochs=EPOCHS,\n                                callbacks=[early_stopping],\n                                validation_data=validation_generator,\n                                validation_steps=1)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\nresnet50acc = history_df['val_loss'].min()","2e16691e":"from keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.applications.vgg16 import decode_predictions\nfrom keras.applications.vgg16 import VGG16","98b67526":"num_classes = 2\nvgg16_weights_path = '..\/input\/vgg16\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\nmodel2 = Sequential()\nmodel2.add(VGG16(include_top=False, pooling='avg', weights=vgg16_weights_path))\nmodel2.add(Dense(num_classes, activation='softmax'))\n\n# Indicate whether the first layer should be trained\/changed or not.\nmodel2.layers[0].trainable = False","f4a67c2b":"model2.compile(optimizer='sgd', \n                     loss='categorical_crossentropy', \n                     metrics=['accuracy'])","8d7e7058":"EPOCHS = 20\nimage_size = 224\ndata_generator = ImageDataGenerator(preprocess_input)\nImage_amount = 10\n\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=4, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)","34683314":"train_generator = data_generator.flow_from_directory(\n                                        directory='..\/input\/pandas-vs-trash-pandas-raccoons\/RACCOOOON\/train',\n                                        target_size=(image_size, image_size),\n                                        batch_size=Image_amount,\n                                        class_mode='categorical')\n\nvalidation_generator = data_generator.flow_from_directory(\n                                        directory='..\/input\/pandas-vs-trash-pandas-raccoons\/RACCOOOON\/val',\n                                        target_size=(image_size, image_size),\n                                        class_mode='categorical')","12eb9de0":"history = model2.fit(train_generator,\n                                steps_per_epoch=2,\n                                epochs=EPOCHS,\n                                callbacks=[early_stopping],\n                                validation_data=validation_generator,\n                                validation_steps=1)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\nvgg16acc = history_df['val_loss'].min()","4862df72":"df = pd.DataFrame({'model':['VGG 16', 'Resnet 50'], 'Minimum Validation Loss':[vgg16acc, resnet50acc]})\ndf.describe","11c73423":"ax = df.plot.bar(x='model', y='Minimum Validation Loss', rot=0)","33d1934a":"image = load_img('..\/input\/pandas-vs-trash-pandas-raccoons\/RACCOOOON\/val\/upright\/raccoon-104.jpg', target_size=(224, 224))\n# convert the image pixels to a numpy array\nimage = img_to_array(image)\n# reshape data for the model\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n# prepare the image for the VGG model\nimage = preprocess_input(image)\nImage(\"..\/input\/pandas-vs-trash-pandas-raccoons\/RACCOOOON\/val\/upright\/raccoon-104.jpg\")","a0791c72":"pred = model2.predict(image)\nprint(pred)","b50da945":"if pred[0][0] < pred[0][1]:\n    print(\"it's a Trash Panda\")\nelse:\n    print(\"it's a Panda\")","9f54b246":"def prediction(imgsrc):\n    image = load_img(imgsrc, target_size=(224, 224))\n    image = img_to_array(image)\n    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n    image = preprocess_input(image)\n    pred = model2.predict(image)\n    if pred[0][0] < pred[0][1]:\n        return(\"it's a Trash Panda\")\n    else:\n        return(\"it's a Panda\")\n\nmultipleImages = glob('..\/input\/pandas-vs-trash-pandas-raccoons\/RACCOOOON\/val\/**\/**')\ndef plotImages2():\n    r = random.sample(multipleImages, 9)\n    plt.figure(figsize=(15,15))\n    img = 0\n    for i in range(9):\n        plt.subplot(331+img)\n        plt.imshow(cv2.imread(r[img])[...,::-1]);\n        plt.title(prediction(r[img]), fontsize='large')\n        img = img + 1","413618de":"plotImages2()","d7b8cfc1":"# Now i can try with a Vgg16 model to compare\nTo do so i will repeat the above steps.\n\n### Importing Anything new","0cfc68cc":"## importing moduals\n\nThis model requires different moduals for its various function to work, hence i have imported them all below.","f1b59aee":"# Yay!\n\nThe model works, we know that this image was of a raccoon and thats what it predicted. ","9ca0ea2b":"# Fitting","05f7f008":"# Fitting\nNow we have the data all that is left to do is train our model on the data as I have done below, to visualize its training I have plotted the training accuracy and validation data accuracy.","ea35ed2d":"# Comparison\n\nTo compare the two models i can put the min validation loss in a bar graph\n\n### making a table\nThis was done using the pandas dataframe function","6c57af84":"# Predicting if an image is of a Panda or Trash Panda\n\n<center><img src=\"https:\/\/ychef.files.bbci.co.uk\/976x549\/p089k4y6.jpg\" width=\"900\"><\/img><\/center>\n\n<br>\n\nIn this investigation, I aim to apply transfer learning to create and train a Neural Network to classify images from a labeled dataset of images and raccoons. To do this I will use the more accurate of two popular neural networks, Resnet50 and VGG16, to determine the most accurate I will have to train both of the models and compare them. The most accurate will then be used to make predictions on several images.\n\nBecause I am only classifying two things and I am using a pre-trained base my accuracy should be very good, in the high 90's. <br>\nHypothesis: If the VGG16 pre-trained base is used my model will have higher accuracy.<br>\nThis is based on the idea that more is better as VGG16 has 138 million parameters and ResNet has 25.5 million parameters.\n","d1242184":"### making a bar graph\nThis was once again done using pandas.","555995eb":"## Umm\nWell this was expected, it wasnt just gonna give a clean output, instead it would give the values from the last two neurones, so lets change that to show something easier to read! We know that if the 1st number is < than the second that means it's a racoon. ","5abd3a09":"# Define the Data\n\nto note, this may look like an unnesscary step but the VGG16 model and Resnet 50 use different preprocessing methods (i assume)  so the images mut go through this step again. oh both models use the term \"preprocess_input\" but they are different and the VGG16's was defined at the importing step, replacing resnet 50's. :)","736a10fa":"# Making Predictions\n\nTo make a prediction i will use the VGG16 model as it has the lower validation loss\n\n## Defining the image\n\nFirst i must choose an image to use, for this i will pick a random one and then go through and apply the filtering VGG16 uses so it can interpret the data.","92a245ed":"## Classification\nto enable the model to distinguish if an image is of a raccoon or panda it needs to be able to validate itself, using flow_from_directory the images have a class associated with them based on the file they are in. e.g. images of raccoons in the raccoon file will be labeled as 1 (that being a raccoon) so the model can validate accuracy and preform backpropagation.","047657a2":"## Setting Parameters","38143fb4":"# Specify the Model\n\nTo enable our model to achieve its goal I will be using a transfer learning method that takes the base from the Resnet 50 neural network and remove the head, then replaces it with a newly trained head to take the features from the base and make a prediction.\n","76fee242":"# Since I have more time let's make some more.\n\nHere what I have done is condensed the previous steps above into the function \"prediction\" that, when given an image src, will return text stating if the image is of a panda or raccoon. From there I used matplotlib to make a 3 * 3 grid. Then, using a loop, each sub-plot is filled with a randomly picked image using .imshow and openCV's .imread function. The same subplot is given a title matching the prediction function's output for the image in the subplot.","3bc1c3da":"### Spliting\nAnother important factor in accurately training a model is to use training and validation data, this was done in the data set itself with the training and test data being split evenly with both hiving 200 images of raccoons and 200 images of pandas\n\n### Defining some parameters:\nfor the following model I have chosen some parameters, 20 epochs were chosen as with the batch size of 20 all 400 images of pandas and raccoons are used. Another important factor is the ImageDataGenerator(preprocess_input) as this is the preprocessing that the rewsnet50 neural network used, so when we feed images to its base features can be accurately identified.\n<br>\n\n### Early stopping:\nTo prevent overfitting of our model I have implemented early stopping, this ends the model training if it no longer improves its accuracy.\n","1741e8bc":"## Well...\n\nIt would seem that Resnet is sooooo inaccurate (at least when I ran it), however, the difference is around .05 so not particularly big, nonetheless, for my final act I will predict an image using the VGG16 base as it has proved itself well ","eea26046":"# conclusion\nFrom the grid above and the other metrics, I can pretty confidently say that I have achieved the goal set out above. My model can certainly determine if an image is of a panda or a trash panda. As for the accuracy it was higher than I expected with the VGG16 model reporting a validation loss of 4.917345e-07 that equates to 99.99% accuracy. I must note that this is likely not the true accuracy as it is not based on all the images.\nIn addition to this, my hypothesis was reinforced with the VGG16 model outperforming the resnet50 model. This might be because of its larger amount of parameters or something else, I cannot say.\n## Problems\nMost of the code was simple to implement from the transfer learning exercise with most of the new things that were implemented being easy to do. Most. Making predictions was not. One of the main difficulties was preprocessing the data for the model that was just fit as I could not use the data_generator.flow_from_directory function. Instead, I needed the steps that it would normally do to be done separately. This hurdle however was overcome with some googling, the next step of predicting on multiple images also wasn't easy. Mainly because it used modules I wasn't familiar with such, as matplotlib or glob. With time this too was overcome. Another problem I encountered came with OpenCV's .imread for it uses BGR to read images whilst all my images are in RGB, luckily, by adding \"[...,::-1]\" after the image src this is somehow fixed. Truthfully I don't know how. The last but counterintuitively first problem was converting the raccoons in the jpg formal into png's, this was overcome by combining python converters that I found online into something that fit my use case.","ced56a34":"# Compile the Model","1128d87a":"# Compile the Model\n\nI will now compile the model with the following line. adding an optimizer, loss function, and metrics. Optimizers are algorithms or methods used to change the attributes of the neural network such as weights and learning rate to reduce the losses. In this case, the SDG or Stochastic gradient descent was chosen because it's preferred by the community. The loss function and metrics are added so we can validate the models' accuracy.","f7c4054e":"## Feeding this to the trained model","47595e57":"# Define the Data\n\nThe gathering of data is an important step in the creation of a neural network, in this case, there wasn't a dataset that I was satisfied with that could be used to achieve our goal. So I made one, the structure of this dataset follows the one in the transfer learning exercise having a train and test folder that is further divided into raccoon and panda folders where the images themselves have been placed. For the neural network to link to use the data set the variable \"images\" was made\n\n## Preparing data\n\none of the steps that I had to go through to create this dataset was to convert all the jpgs to pngs. To do so I made a python program that does it automatically, for reference I have listed it below:\n\nfrom PIL import Image\n\nfor i in range(196):\n    file = f\"raccoon{i}.png\"\n    file2 = f\"raccoon{i}.jpg\"\n    print(file)\n    im1 = Image.open(f\"C:\\\\Users\\\\1015599\\\\Downloads\\\\RaccoonPascalVOC\\\\images\\\\{file}\")\n    im1.save(f\"C:\\\\Users\\\\1015599\\\\Downloads\\\\RaccoonPascalVOC\\\\jpg\\\\{file2}\")\n    print(f\"done {i}.\")\n    \n\n    \n    ","dbe64123":"# Specify the Model"}}