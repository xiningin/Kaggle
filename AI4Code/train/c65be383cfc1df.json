{"cell_type":{"f2556181":"code","3d989f65":"code","b1e988a1":"code","1fea039f":"code","3f9c25f0":"code","2663364f":"code","2f4f1988":"code","e4039614":"code","11c997a4":"code","25bc7a32":"code","fb8512c0":"code","3cce70d7":"code","3c8d0093":"code","dd1a522e":"code","f64a18ea":"code","50016b5b":"code","c860ab1e":"code","0b749f20":"code","68fddaf8":"markdown","f3e379b3":"markdown","a1ea97ed":"markdown","14535944":"markdown","3aeaaf5c":"markdown","403b1467":"markdown","c4319e0d":"markdown","dc7ab8e7":"markdown","ac62b5e4":"markdown","7f7d4942":"markdown","0acbb39a":"markdown","52477d83":"markdown","791c95dd":"markdown","6dc32cd5":"markdown","5b5acef2":"markdown","a3b10232":"markdown","4c5d48e3":"markdown","aca3dd66":"markdown","e966bd87":"markdown","b7d71e32":"markdown","287c963d":"markdown","93f08bb5":"markdown","e5acf9d9":"markdown","c40eabf5":"markdown","cb157d9a":"markdown","bac22d9e":"markdown","3f4d7ad7":"markdown","93d3359c":"markdown","993b3139":"markdown","ffb26408":"markdown","9889ea03":"markdown","3cdebeab":"markdown","eaa14c79":"markdown","a0b888ed":"markdown","5f155572":"markdown","4e6e705e":"markdown","03376917":"markdown","b3270f1c":"markdown","277ec6d3":"markdown","67067e53":"markdown","c32b3ddb":"markdown","493ef987":"markdown","63ef3779":"markdown","120667f2":"markdown","c908e80e":"markdown","21c5b8d7":"markdown","8b4211ca":"markdown"},"source":{"f2556181":"# Importando as bibliotecas padr\u00e3o \n\n# Manipula\u00e7\u00e3o de dados\nimport pandas as pd\nimport numpy as np\n\n# Visualiza\u00e7\u00e3o de dados\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Machine Learning \nfrom sklearn import metrics # analisa a acur\u00e1cia de nossos modelos\n\n# Ocultando Warnings indesejados\nimport warnings\nwarnings.filterwarnings('ignore')","3d989f65":"# importando nossa base de dados\nfrom sklearn.datasets import load_breast_cancer #importando a base de dados nativas no sklearn\n\ndados=load_breast_cancer() # Carregando base de dados\n\n# vamos ver a descri\u00e7\u00e3o de nossa base de dados\nprint(dados.DESCR)","b1e988a1":"# Tranformando a base de dados em um DataFrame\n\ncancer=pd.DataFrame(data=dados.data, columns=dados.feature_names) # convertendo para dataframe com ajuda do Pandas\n\ncancer['Class']=dados.target # Adicionando a nossa Target","1fea039f":"# Um dataframe Pandas parece muito com uma tabela Excel\n\ncancer.head(3) # Visualizando as 3 primeiras linhas de nosso dataframe","3f9c25f0":"# Vamos come\u00e7ar descobrindo as dimens\u00f5es de nosso dataframe - Linhas X Colunas\ncancer.shape # nosso df tem 569 linhas distribuidas entre 31 colunas","2663364f":"# Distribui\u00e7\u00e3o de nossas classes\ncancer['Class'].value_counts() # 1- Benigno 0 = M\u00e1ligno","2f4f1988":"# Temos a informa\u00e7\u00e3o: 357 casos benignos e 212 casos malignos.\n\n# Vamos visualizar isso melhor\n# Criando um Gr\u00e1fico de Pizza - ou no ingl\u00eas um gr\u00e1fico de torta(PiePlot)\n\ncolors=['#35b2de','#ffcb5a'] # Apenas escolhando as cores\n\nlabels=cancer['Class'].value_counts().index\nplt.pie(cancer['Class'].value_counts(),autopct='%1.1f%%',colors=colors) # conta as ocorr\u00eancias de cada classe e exibe a porcentagem\nplt.legend(labels,bbox_to_anchor=(1.25,1),) # Nossas Legendas\nplt.title('Porcentagem: Benignos x Malignos ')\nplt.show()","e4039614":"# Misssing Values\ncancer.isnull().sum() ","11c997a4":"# Vamos criar nossas amostras para a constru\u00e7\u00e3o dos modelos\n# Vamos usar mais uma vez a biblioteca sklearn\nfrom sklearn.model_selection import train_test_split\n\n# primeiro vamos dividir nossa base de dados entre features e target\nX= cancer.iloc[:,0:-1]# Selecionando todas as linhas, da primeira coluna at\u00e9 a pen\u00faltima coluna.\nY=cancer.iloc[:,-1] # Selecionando todas as linhas da \u00faltima coluna ['Class'].\n\n\nx_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.30,random_state=42)\n# test-size: neste casos vamos dividir nosso dataset em 70% treino e 30% teste\n# random_state: vamos selecionar de forma aleat\u00f3ria","25bc7a32":"# Agora temos nossas bases de dados para treino e testes \nprint('X treino',x_train.shape)\nprint('X test',x_test.shape)\nprint('Y treino',y_train.shape)\nprint('Y test',y_test.shape)","fb8512c0":"# importando nosso modelo\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg=LogisticRegression() # Criando o modelo\nlogreg.fit(x_train,y_train) # Treinando o modelo\ny_pred=logreg.predict(x_test) # predizendo\nacc_logreg=round(metrics.accuracy_score(y_pred,y_test)*100,1) # avaliando a acur\u00e1cia. previs\u00f5es x resultados reais\nprint(\"{}% de acur\u00e1cia\".format(acc_logreg,))","3cce70d7":"# importando nosso modelo\nfrom sklearn.svm import SVC\n\nsvc=SVC() # Criando o modelo\nsvc.fit(x_train,y_train) # Treinando o modelo\ny_pred=svc.predict(x_test) # predizendo\nacc_svc=round(metrics.accuracy_score(y_pred,y_test)*100,1) # avaliando a acur\u00e1cia. previs\u00f5es x resultados reais\nprint(acc_svc,\"% de acur\u00e1cia\")","3c8d0093":"# importando nosso modelo\nfrom sklearn.naive_bayes import GaussianNB\n\ngaussian=GaussianNB() # Criando o modelo\ngaussian.fit(x_train,y_train) # Treinando o modelo\ny_pred=gaussian.predict(x_test) # predizendo\nacc_gaussian=round(metrics.accuracy_score(y_pred,y_test)*100,1) # avaliando a acur\u00e1cia. previs\u00f5es x resultados reais\nprint(acc_gaussian,\"% de acur\u00e1cia\")","dd1a522e":"# importando nosso modelo\nfrom sklearn.tree import DecisionTreeClassifier\n\ntree=DecisionTreeClassifier() # Criando o modelo\ntree.fit(x_train,y_train) # Treinando o modelo\ny_pred=tree.predict(x_test) # predizendo\nacc_tree=round(metrics.accuracy_score(y_pred,y_test)*100,1) # avaliando a acur\u00e1cia. previs\u00f5es x resultados reais\nprint(acc_tree,\"% de acur\u00e1cia\")","f64a18ea":"# importando nosso modelo\nfrom sklearn.ensemble import RandomForestClassifier\n\nforest=RandomForestClassifier(n_estimators=100) # Criando o modelo\nforest.fit(x_train,y_train) # Treinando o modelo\ny_pred=forest.predict(x_test) # predizendo\nacc_forest=round(metrics.accuracy_score(y_pred,y_test)*100,1) # avaliando a acur\u00e1cia. previs\u00f5es x resultados reais\nprint(acc_forest,\"% de acur\u00e1cia\")","50016b5b":"# importando nosso modelo\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn=KNeighborsClassifier(n_neighbors=1) # Criando o nosso classificador\nknn.fit(x_train,y_train) # Treinando o modelo\ny_pred=knn.predict(x_test) # Predizendo nossos dados de testet\nacc_knn=round(metrics.accuracy_score(y_pred,y_test)*100,1)\nprint(acc_knn,\"% de acur\u00e1cia\") # Exibindo resultado","c860ab1e":"# \u00c9 Simples, vamos construir modelos KNN dentro de um for, e testar qual s\u00e3o os melhores resultados.\n\nk_range=range(1,25) # vamos testar n_neighbors de 1 a 25\nscores=[] # vamos armazenar os resultados aqui\n\nfor k in k_range:\n    knn=KNeighborsClassifier(n_neighbors=k)\n    knn.fit(x_train,y_train)\n    y_pred=knn.predict(x_test)\n    scores.append(metrics.accuracy_score(y_pred,y_test))\n\n# Por \u00faltimo vamos gerar uma visualiza\u00e7\u00e3o para chegarmor ao veredito.\nplt.plot(k_range,scores)\nplt.xlabel(\"Valor de K para KNN\")\nplt.ylabel(\"Teste de Acur\u00e1cia\")\n","0b749f20":"modelos=pd.DataFrame({'Modelos':['Regress\u00e3o Log\u00edstica','Support Vector Machine',\\\n                    'Gaussian Naive Bayes','\u00c1rvore de Decis\u00e3o',\\\n                    'Random Forest','KNN'],\\\n         'Score':[acc_logreg,acc_svc,acc_gaussian,acc_tree,acc_forest,acc_knn]})\n\nmodelos.sort_values(by=\"Score\", ascending=False)","68fddaf8":"Bem, como podemos ver temos mais casos de benignos do que de m\u00e1lignos. Entretando \u00e9 importante que tenhamos uma quantidade balanceada de cada tipo para podermos construir nossos modelos.","f3e379b3":"## Machine Learning: Aprendendo os Principais algoritmos de Classifica\u00e7\u00e3o","a1ea97ed":"## Support Vector Machines","14535944":"### Objetivos:\n\nEste notebook <b>n\u00e3o<\/b> almeja cobrir todos os detalhes sobre modelos de classifica\u00e7\u00e3o em machine learning. Pelo contr\u00e1rio o objetivo \u00e9 ser pr\u00e1tico e direto, m\u00e3o na massa, ajudando voc\u00ea a implementar seus primeiros projetos de classifica\u00e7\u00e3o.","3aeaaf5c":"S\u00e3o 30 colunas + a nossa target na \u00faltima coluna ['Class'].","403b1467":"Este \u00e9 o primeiro notebook de uma s\u00e9rie sobre machine learning. Neste primeiro projeto vamos dar entrada no Aprendizado Supervisionado apredendendo sobre CLASSIFICA\u00c7\u00c3O e seus principais algoritmos. ","c4319e0d":"# Conclus\u00e3o","dc7ab8e7":"Entretanto, seria interessante dentre os 357 diagn\u00f3sticos benignos, escolher os melhores para compor a nova base de dados. Ou seja dentre os 357 casos vamos selecionar os melhores 212.","ac62b5e4":"Vimos aqui alguns dos principais algoritmos de classifica\u00e7\u00e3o, existem outros milhares. Qual \u00e9 o melhor entre eles? Isso depende do problema que voc\u00ea quer resolver, dos dados que tem a disposi\u00e7\u00e3o, enfim para se ter os melhores resultados voc\u00ea ter\u00e1 de testar, TESTE!","7f7d4942":"Aqui voc\u00ea vai:\n\n- Explorar uma base de dados\n- Visualizar informa\u00e7\u00f5es\n- Entender relacionamentos \n- Construir modelos de machine learning\n- Avaliar a efic\u00e1cia destes modelos","0acbb39a":"Manav Sehgal: Titanic Data Science Solution > [Kaggle Notebook](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)","52477d83":"Vamos ver na pr\u00e1tica alguns dos principais algoritmos de machine lerning para classifica\u00e7\u00e3o de dados.\nO objetivo deste notebook \u00e9 mostrar de forma b\u00e1sica, clara e simples como implementar estes algoritmos. ","791c95dd":"357 - 212 = 145, certo?","6dc32cd5":"## Regress\u00e3o Log\u00edstica","5b5acef2":"Leia mais artigos em: [Analiseosdados.com.br](https:\/\/analiseosdados.com.br)","a3b10232":"### A nossa Target","4c5d48e3":"Vamos come\u00e7ar olhando como est\u00e3o distribuidos estes diagn\u00f3sticos","aca3dd66":"### Refer\u00eancias:","e966bd87":"E como faremos isso?\nBem \u00e9 muito simples","b7d71e32":"## \u00c1rvore de Decis\u00e3o","287c963d":"Neste notebook pudemos ver de forma introdut\u00f3ria alguns conceitos em modelagem como, carregar e preparar os dados, dividir entre treino e teste, construir modelos e avaliar suas acur\u00e1cias.","93f08bb5":"Assumindo que queremos uma base de dados 50\/50, onde sabemos ter 357 diagn\u00f3sticos benignos e 212 m\u00e1lignos, logo conclu\u00edmos que vamos ter de excluir 145 casos benignos.","e5acf9d9":"## KNN","c40eabf5":"Espero que este projeto possa servir de base para voc\u00ea construir os seus!","cb157d9a":"## Gaussian Naive Bayes","bac22d9e":"# Modelos de Classifica\u00e7\u00e3o","3f4d7ad7":"Neste projetos iremos usar modelos de machine learning para classificar tipos de C\u00e2ncer. Existe v\u00e1rias bases de dados dispon\u00edveis de forma n\u00e1tiva no pacote SKlearn do python e vamos usar a load_breast_cancer.","93d3359c":"## Avaliando nossos  Classificadores","993b3139":"Existem v\u00e1rias crit\u00e9rios que poderiamos levar em considera\u00e7\u00e3o! Por exemplo:","ffb26408":"Janio Martinez Bachmann: Credit Fraud || Dealing with imbalanced dataset > [kaggle Notebook](https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets)","9889ea03":"Esta base de dados cont\u00e9m tipos de C\u00e2ncer entre Benignos e Malignos. De acordo com determinadas caracter\u00edsticas (Features), vamos prever nossas respostas (Targets). Vamos ensinar m\u00e1quinas a realizar diag\u00f3sticos (com a maior taxa de acerto poss\u00edvel) baseados em dados hist\u00f3ricos.","3cdebeab":"Nosso objetivo ser\u00e1, com base nas 30 colunas ( nossas features), prever a classifica\u00e7\u00e3o entre M\u00e1ligno=0 e Benigno=1 ( nossa target)","eaa14c79":"Entendeu? N\u00f3s vamos usar todos os casos m\u00e1lignos que temos a disposi\u00e7\u00e3o (212), mais a mesma quantidade de casos benignos.","a0b888ed":"Pedro Marcelino: Comprehensive data exploration with Python > [Kaggle Notebook](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python)","5f155572":"## Random Forest","4e6e705e":"   Ao transformar nossa base em dataframe poderemos trabalhar mais poderosamente nossos dados com \u00e0 ajuda do pacote Pandas.","03376917":"Poss\u00edvelmente temos em nossa base de dados, registros ou no caso diagn\u00f3sticos, onde nem todas as vari\u00e1veis est\u00e3o preenchidas, enquanto outros possuem todas as informa\u00e7\u00f5es. L\u00f3gicamente deveriamos dar prioridade para os registros completos certo?","b3270f1c":"Mas qual crit\u00e9rio usar para isso?","277ec6d3":"Em breve espero poder dar continuidade a este projeto de notebooks introdut\u00f3rios a machine learning, fique atento que o pr\u00f3ximo ser\u00e1 sobre Regress\u00e3o.","67067e53":"L\u00f3gicamente a efic\u00e1cia de cada algoritmo vai depender do problema que se quer resolver, aos dados que se possui (qu[](http:\/\/)alidade e quantidade) e da forma como tratamos e transformamos nossas vari\u00e1veis.","c32b3ddb":"Esta base de dados n\u00e3o \u00e9 muito desequilibrada, mesmo assim \u00e9 importante a balancearmos para que no futuro nosso modelo n\u00e3o fique bom apenas em prever casos benignos, ou apenas casos m\u00e1lignos, chamamos isso de <b>Overfitting<\/b>.","493ef987":"Data School: [Comparing Machine Learning Models in scikit-learn](https:\/\/www.youtube.com\/watch?v=0pP4EwWJgIU)\n\n\n","63ef3779":"1. Obtivemos 93.6% de acur\u00e1cia usando o KNN, entretanto, vemos que resultado deste modelo depende muito do n\u00famero de visizinhos determinado em n_neighbors. Ent\u00e3o qual \u00e9 a quantidade m\u00ednima de vizinhos (neighbors) para se obter o melhor modelo?","120667f2":"A target \u00e9 a nossa vari\u00e1vel de resposta, ela cont\u00eam dados reais sobre diagn\u00f3sticos. Como vimos, nossas base de dados tem 569 linhas, ou seja, temos dados sobre 569 diagn\u00f3sticos diferentes entre c\u00e2ncer benigno e maligno.","c908e80e":"### Introdu\u00e7\u00e3o","21c5b8d7":"Por\u00e9m neste projeto, para fins did\u00e1ticos, estamos tendo a sorte de trabalhar com uma base de dados j\u00e1 pr\u00e9 preparada, sem campos nulos, ou dados faltantes. ( Aproveite que isso n\u00e3o acontece no mundo real! )","8b4211ca":"Ent\u00e3o como estamos trabalhando com apenas dados completos e igualmente confi\u00e1veis, nosso trabalho ser\u00e1 apenas o de selecionar de forma aleat\u00f3ria, ou como chamamos, de forma r\u00e2ndomica os registros. "}}