{"cell_type":{"9c3fd5b5":"code","21eee143":"code","771ea1c7":"code","7a644f89":"code","c2223129":"code","788e2704":"code","1a7c67dc":"code","aa1def76":"code","fce608e8":"code","1f3dfcdd":"code","4b0aedfb":"code","223b96b0":"code","1595e0a1":"code","e164a53a":"code","f353205c":"code","23ab7199":"code","ae9921fd":"code","27fcfa9a":"code","e46ead49":"code","1ae18cf4":"code","6feb9522":"code","fe7ac1c4":"code","fe922c30":"code","361152cf":"code","ef5aa3bb":"code","467e3f3a":"code","b9c48a7b":"code","204c9bb3":"code","90e28f96":"code","12669b7b":"code","e1a702b6":"code","157745e4":"code","e88eb050":"code","f0d9e951":"code","913b8b55":"code","ef029a6f":"code","52eceec5":"code","a2271a8b":"code","c7001861":"code","303eb165":"code","8b96826b":"code","22ed3dfa":"code","70eca624":"code","f5d11934":"code","7f48bd07":"code","65d945fd":"code","7ae25602":"code","b71a9ec6":"code","ee6f4fec":"code","d487494f":"code","a983a689":"code","a1b9069f":"code","b2894e8c":"code","680fe761":"code","74418503":"code","cf2a96a6":"code","45ecaf28":"code","4c786248":"code","9b96e7f5":"code","d5a48c64":"code","648f817b":"code","c9fa1e02":"code","948c3cb6":"code","263c0924":"code","614c0aef":"code","355cb446":"code","8396bb82":"code","33e5295a":"code","588e67b8":"code","c5257117":"code","ae295bd9":"code","60445a17":"code","a81a736b":"code","4616a784":"code","9c361921":"code","847a7c6f":"code","141f9b40":"code","e42c1595":"code","3a6c3f3a":"code","4269eed2":"code","017292fe":"code","27412d5e":"code","407dbfc9":"code","ad8fcd41":"code","f2cc44cb":"code","4fd43dbd":"code","7c654cc6":"code","05a7e1d3":"code","c90afc6d":"code","717e7628":"code","7bcdc69d":"markdown","b4f47fcd":"markdown","e1c6865d":"markdown","747c2870":"markdown","73b1a38e":"markdown","cb4678e8":"markdown","2e37590d":"markdown","02800b0c":"markdown","315c6bbb":"markdown","c0f7b5af":"markdown","87f2de89":"markdown","b4ad94c9":"markdown","7d011b91":"markdown","d7d9d409":"markdown","d3119e70":"markdown","2a64bcec":"markdown","8fe4ac1b":"markdown","fbdf7548":"markdown"},"source":{"9c3fd5b5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","21eee143":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport sys\nimport pickle\nfrom datetime import datetime\n%matplotlib inline\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.utils import class_weight\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")","771ea1c7":"test_set = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n","7a644f89":"test_set.head()","c2223129":"test_set.shape","788e2704":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport sys\nimport pickle\nfrom datetime import datetime\n%matplotlib inline\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.utils import class_weight\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")","1a7c67dc":"df = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')","aa1def76":"df.info()","fce608e8":"df.head()","1f3dfcdd":"df[[\"excerpt\", \"target\"]].info()","4b0aedfb":"df[[\"excerpt\", \"target\"]].head()","223b96b0":"df['target'].hist(bins=40)\nplt.title('Distribuzione variabile target')\nplt.ylabel('Conteggio')\nplt.show()","1595e0a1":"df.isna().sum()","e164a53a":"df.isna().sum().sum()","f353205c":"df.corr()","23ab7199":"stde = df['standard_error']","ae9921fd":"df = df.drop(columns = ['url_legal','license', 'id', 'standard_error'])","27fcfa9a":"print(df['target'].min(),df['target'].max())\nnumberOfStars=6\nmybin=((df['target'].max())-(df['target'].min()-1))\/numberOfStars\nbins=[]\nfor i in range(numberOfStars+1):\n    bins.append((df['target'].min()-1)+i*mybin)\nprint (bins)\n\nstars = pd.cut(df.target,bins=6,labels=[0,1,2,3,4,5])#stelle mappate fra 0 e 4 perch\u00e8 il to_categorical() di keras parte da indice 0\ndf.insert(2,'stars',stars)\ndf[\"stars\"] = pd.to_numeric(df[\"stars\"])\n","e46ead49":"df[\"stars\"]","1ae18cf4":"df.stars.shape","6feb9522":"df.head()","fe7ac1c4":"df.shape","fe922c30":"df = df.drop(['target'], axis=1)","361152cf":"df.head()","ef5aa3bb":"df.stars.value_counts()","467e3f3a":"X= df['excerpt']\ny = df['stars']","b9c48a7b":"#splitto dataset in training e test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42,stratify=y)#stratifico rispetto y\n\n\nprint(type(X_train))\nprint (X_train[0])","204c9bb3":"from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\nfrom sklearn.feature_selection import SelectKBest, chi2# per features selection\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import Pipeline","90e28f96":"vect = CountVectorizer(min_df=1,ngram_range=(1,2))  # tokenization and feature extraction (fa tokenizzazione di default conterr\u00e0 contatori )\n                                  #min_df=5 elimino parole sulla coda lunga (numerose e poco frequenti count<5)\n                                  #ngram_range=(1,2) considero solo le singole parole e  bigrammi, non trigrammi \nvect.fit(X_train)\nX_train = vect.transform(X_train)\nX_test =vect.transform(X_test)","12669b7b":"sel = SelectKBest(chi2, k=5000)  # feature selection  faccio features selection in maniera supervisionata gli passo y_train\nsel.fit(X_train,y_train)\nX_train = sel.transform(X_train)\nX_test = sel.transform(X_test)","e1a702b6":"tfidf = TfidfTransformer()  # weighting\ntfidf.fit(X_train)\nX_train = tfidf.transform(X_train)\nX_test =tfidf.transform(X_test)\n","157745e4":"print(X_train)","e88eb050":"print(X_test)","f0d9e951":"# la classe 4 \u00e8 sbilanciata, decido quindi di bilanciarla computando la class weight che poi usero come parametro nel fitting della rete neurale\nmy_class_weights = class_weight.compute_class_weight('balanced', # per avere dataset perfettamente bilanciato (per il training) senza per\u00f2 toccare niente sul dataset\n                                                  np.unique(y_train),\n                                                  y_train)\n\nfor _class in [0 ,1,2,3,4]:\n    print(f\"Weight of instances in class {_class:d}: {my_class_weights[_class]:.2f}\")\ntype(my_class_weights)","913b8b55":"print('Before OverSampling, the shape of train_X: {}'.format(X_train.shape))\nprint('Before OverSampling, the shape of train_y: {} \\n'.format(y_train.shape))\n\n\n\nprint(\"Before OverSampling, counts of label '5': {}\".format(sum(y_train == 5)))\nprint(\"Before OverSampling, counts of label '4': {}\".format(sum(y_train == 4)))\nprint(\"Before OverSampling, counts of label '3': {}\".format(sum(y_train == 3)))\nprint(\"Before OverSampling, counts of label '2': {}\".format(sum(y_train == 2)))\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))\n  \n# import SMOTE module from imblearn library\n# pip install imblearn (if you don't have imblearn in your system)\nfrom imblearn.over_sampling import SMOTE\n\noversample = SMOTE(random_state = 2)\nX_train_res, y_train_res = oversample.fit_resample(X_train, y_train)\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n  \n\nprint(\"After OverSampling, counts of label '5': {}\".format(sum(y_train_res == 5)))\nprint(\"After OverSampling, counts of label '4': {}\".format(sum(y_train_res == 4)))    \nprint(\"After OverSampling, counts of label '3': {}\".format(sum(y_train_res == 3)))\nprint(\"After OverSampling, counts of label '2': {}\".format(sum(y_train_res == 2)))\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0)))","ef029a6f":"from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier()\n\n\nclf = DecisionTreeClassifier()\n\n\"\"\"param_list = {\n    'max_depth': [None, 2, 4, 6, 8, 16, 32],\n    'min_samples_leaf': [1, 5, 10, 20, 50, 100],\n    'min_samples_split': [2, 5, 10, 20, 50, 100],\n    'criterion': ['gini', 'entropy']\n}\n\n\ngrid_search = GridSearchCV(clf, param_grid=param_list, cv=5, scoring='accuracy') # ricerca esaustiva fra i parametri\ngrid_search.fit(X_train_res, y_train_res)# (NB: gli passo X_train,y_train ossia solo training perch\u00e8 poi utilizzo il modello ritornato)\nclf_best_grid = grid_search.best_estimator_ # il miglior calssificatore trovato rispetto all'accuratezza\"\"\"","52eceec5":"\"\"\"grid_search.best_params_\"\"\"","a2271a8b":"\"\"\"clf_best_grid\"\"\"","c7001861":"clf = DecisionTreeClassifier(criterion= 'gini', max_depth= 32, \n                             min_samples_leaf= 1, min_samples_split= 5) \n\nclf.fit(X_train_res, y_train_res)","303eb165":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom scikitplot.metrics import plot_roc","8b96826b":"y_pred = clf.predict(X_test)","22ed3dfa":"print('Accuracy %s' % accuracy_score(y_test, y_pred))","70eca624":"print(classification_report(y_test, y_pred))","f5d11934":"confusion_matrix(y_test, y_pred)","7f48bd07":"matrix = plot_confusion_matrix(clf, X_test, y_test,\n                            #display_labels=le.classes_,\n                            cmap=plt.cm.Blues,\n                            xticks_rotation=70)\nmatrix.ax_.set_title('Confusion Matrix')\n\nplt.show()","65d945fd":"plot_roc(y_test, clf.predict_proba(X_test))\nplt.show()","7ae25602":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import datasets, metrics, model_selection, svm\nfrom sklearn.metrics import confusion_matrix, accuracy_score \nfrom sklearn.metrics import f1_score, precision_score, recall_score, fbeta_score\nfrom sklearn.metrics import classification_report, precision_recall_curve\nfrom sklearn.metrics import auc, roc_auc_score, roc_curve\nfrom sklearn.model_selection import GridSearchCV # ricerca esaustiva fra i parametri\nfrom sklearn.model_selection import RandomizedSearchCV # ricerca randomica fra i parametri con numero prefissato di candidati\n\nclf = RandomForestClassifier()\n\n\"\"\"param_list = {\n    'max_depth': [None, 2, 4, 6, 8, 16, 32],\n    'min_samples_leaf': [1, 5, 10, 20, 50, 100],\n    'min_samples_split': [2, 5, 10, 20, 50, 100],\n    'criterion': ['gini', 'entropy'],\n    'n_estimators': [10,20,50,100,200]\n}\n\n\ngrid_search = GridSearchCV(clf, param_grid=param_list, cv=5, scoring='accuracy') # ricerca esaustiva fra i parametri\ngrid_search.fit(X_train_res, y_train_res)# (NB: gli passo X_train,y_train ossia solo training perch\u00e8 poi utilizzo il modello ritornato)\nclf_best_grid = grid_search.best_estimator_ # il miglior calssificatore trovato rispetto all'accuratezza\"\"\"","b71a9ec6":"\"\"\"grid_search.best_params_\"\"\"","ee6f4fec":"\"\"\"clf_best_grid\"\"\"","d487494f":"clf = RandomForestClassifier(criterion='gini',\n                       max_depth=None, min_samples_leaf=1,\n                       min_samples_split=10,\n                       n_estimators=200) \n\nclf.fit(X_train_res, y_train_res)","a983a689":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom scikitplot.metrics import plot_roc","a1b9069f":"y_pred = clf.predict(X_test)","b2894e8c":"print('Accuracy %s' % accuracy_score(y_test, y_pred))","680fe761":"print(classification_report(y_test, y_pred))","74418503":"confusion_matrix(y_test, y_pred)","cf2a96a6":"from sklearn.metrics import plot_confusion_matrix\n\nmatrix = plot_confusion_matrix(clf, X_test, y_test,\n                            #display_labels=le.classes_,\n                            cmap=plt.cm.Blues,\n                            xticks_rotation=70)\nmatrix.ax_.set_title('Confusion Matrix')\n\nplt.show()","45ecaf28":"plot_roc(y_test, clf.predict_proba(X_test))\nplt.show()","4c786248":"!pip install xgboost","9b96e7f5":"from numpy import loadtxt\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","d5a48c64":"from sklearn.model_selection import GridSearchCV # ricerca esaustiva fra i parametri\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import *\nclf = xgb.XGBClassifier()\n\n\nclf.fit(X_train_res,y_train_res)","648f817b":"\"\"\"clf_best_grid\"\"\"","c9fa1e02":"\"\"\"grid_search.best_params_\"\"\"","948c3cb6":"y_pred = clf.predict(X_test)","263c0924":"print('Accuracy %s' % accuracy_score(y_test, y_pred))","614c0aef":"print(classification_report(y_test, y_pred))","355cb446":"confusion_matrix(y_test, y_pred)","8396bb82":"from sklearn.metrics import plot_confusion_matrix\n\nmatrix = plot_confusion_matrix(clf, X_test, y_test,\n                            #display_labels=le.classes_,\n                            cmap=plt.cm.Blues,\n                            xticks_rotation=70)\nmatrix.ax_.set_title('Confusion Matrix')\n\nplt.show()","33e5295a":"plot_roc(y_test, clf.predict_proba(X_test))\nplt.show()","588e67b8":"!pip install mlxtend","c5257117":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport itertools\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics\n\n# Classifiers\nfrom sklearn.svm import NuSVC, SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import StackingCVClassifier # <- Here is our boy\n\n\n# Used to ignore warnings generated from StackingCVClassifier\nimport warnings\nwarnings.simplefilter('ignore')","ae295bd9":"# compare ensemble to each baseline classifier\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom matplotlib import pyplot","60445a17":"# get a stacking ensemble of models\ndef get_stacking():\n    # define the base models\n    level0 = list()\n    #level0.append(('lr', LogisticRegression()))\n    #level0.append(('knn', KNeighborsClassifier()))\n    level0.append(('rf', RandomForestClassifier()))\n    #level0.append(('xgb',  xgb.XGBClassifier()))\n    #level0.append(('gbc',  GradientBoostingClassifier()))\n    #level0.append(('cart', DecisionTreeClassifier()))\n    #level0.append(('linearsvm', LinearSVC()))\n    level0.append(('svm', SVC()))\n    #level0.append(('nusvm', NuSVC()))\n    level0.append(('mlp', MLPClassifier()))\n    #level0.append(('bayes', GaussianNB()))\n    # define meta learner model\n    level1 = LogisticRegression()\n    # define the stacking ensemble\n    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n    return model\n \n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    #models['lr'] = LogisticRegression()\n    #models['knn'] = KNeighborsClassifier()\n    models['rf'] = RandomForestClassifier()\n    #models['xgb'] = xgb.XGBClassifier()\n    #models['gbc'] = GradientBoostingClassifier()\n    #models['cart'] = DecisionTreeClassifier()\n    #models['linearsvm'] = LinearSVC()\n    models['svm'] = SVC()\n    #models['nusvm'] = NuSVC()\n    models['mlp'] = MLPClassifier()\n    #models['bayes'] = GaussianNB()\n    models['stacking'] = get_stacking()\n    return models\n \n# evaluate a give model using cross-validation\ndef evaluate_model(model, X_train_res, y_train_res):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = cross_val_score(model, X_train_res, y_train_res, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n    return scores\n\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model, X_train_res, y_train_res)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n\n    # plot model performance for comparison\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()\n","a81a736b":"# make a prediction with a stacking ensemble\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\n# define the base models\nlevel0 = list()\nlevel0.append(('rf', RandomForestClassifier()))\nlevel0.append(('svm', SVC()))\nlevel0.append(('mlp', MLPClassifier()))\n# define meta learner model\nlevel1 = LogisticRegression()\n\n# define the stacking ensemble\nmodel = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n\n\n# fit the model on training set\nmodel.fit(X_train_res, y_train_res)\n","4616a784":"y_pred = model.predict(X_test)","9c361921":"print('Accuracy %s' % accuracy_score(y_test, y_pred))","847a7c6f":"print(classification_report(y_test, y_pred))","141f9b40":"confusion_matrix(y_test, y_pred)","e42c1595":"from sklearn.metrics import plot_confusion_matrix\n\nmatrix = plot_confusion_matrix(model, X_test, y_test,\n                            #display_labels=le.classes_,\n                            cmap=plt.cm.Blues,\n                            xticks_rotation=70)\nmatrix.ax_.set_title('Confusion Matrix')\n\nplt.show()","3a6c3f3a":"plot_roc(y_test, model.predict_proba(X_test))\nplt.show()","4269eed2":"test_set.head()","017292fe":"dati = test_set['excerpt']","27412d5e":"dati","407dbfc9":"X= dati","ad8fcd41":"from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\nfrom sklearn.feature_selection import SelectKBest, chi2# per features selection\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import Pipeline","f2cc44cb":"vect = CountVectorizer(min_df=1,ngram_range=(1,2))  # tokenization and feature extraction (fa tokenizzazione di default conterr\u00e0 contatori )\n                                  #min_df=5 elimino parole sulla coda lunga (numerose e poco frequenti count<5)\n                                  #ngram_range=(1,2) considero solo le singole parole e  bigrammi, non trigrammi \nvect.fit(X)\nX = vect.transform(X)\n","4fd43dbd":"tfidf = TfidfTransformer()  # weighting\ntfidf.fit(X)\nX= tfidf.transform(X)\n","7c654cc6":"X","05a7e1d3":"print(X)","c90afc6d":"y_pred = model.predict(X)","717e7628":"print(y_pred)","7bcdc69d":"## **Decision Tree**","b4f47fcd":"**Dropping target**\n","e1c6865d":"**Unbalanced class! We are going to perform smote**","747c2870":"**Let's save standard error for later and drop url_legal and license columns**","73b1a38e":"**After running the previous cell to calculate the single models' performances, I received the follwing results.**\n\n* lr 0.708 (0.020)\n* knn 0.664 (0.018)\n* rf 0.773 (0.018)\n* xgb 0.727 (0.018)\n* gbc 0.713 (0.017)\n* cart 0.615 (0.021)\n* linearsvm 0.742 (0.020)\n* svm 0.783 (0.016)\n* mlp 0.779 (0.015)\n\n\n**I decided then to keep the best three models and to run again the cell for getting back the single ones' and the stacking one performances**","cb4678e8":"**Let's focus on \"excerpt\" and \"target\"**","2e37590d":"**Training the model on train_res with best params**","02800b0c":"**Let's transform continue target in categorical one**","315c6bbb":"## I focus in test set. Need first to convert excerpt with tf\/idf and the to get predictions ##","c0f7b5af":"**Correlation**","87f2de89":"## **Gradient Boosting Classifier**","b4ad94c9":"## **Random Forest**","7d011b91":"**Let's check the test set composition**","d7d9d409":"**Smote**","d3119e70":"**Features engineering**","2a64bcec":"## **Stacking Ensemble Classifier**","8fe4ac1b":"**Let's start working on the train set**","fbdf7548":"**Let's focus on null values**"}}