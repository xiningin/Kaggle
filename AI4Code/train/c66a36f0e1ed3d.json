{"cell_type":{"dce9a45f":"code","946b3700":"code","b157ea8b":"code","cea4390a":"code","c11f55da":"code","d4b9778e":"code","1b4c63b1":"code","fbe3beb6":"code","e7d6c4c9":"code","5c531131":"code","4fdf18fb":"code","d3681343":"markdown"},"source":{"dce9a45f":"import matplotlib.pylab as plt\nimport os\nimport math\nimport random\nimport time\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import get_cosine_schedule_with_warmup\nfrom transformers import get_linear_schedule_with_warmup\n\nfrom transformers import AdamW\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\nfrom transformers import get_cosine_schedule_with_warmup\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nimport gc\ngc.enable()","946b3700":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","b157ea8b":"def set_random_seed(random_seed):\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n\n    torch.manual_seed(random_seed)\n    torch.cuda.manual_seed(random_seed)\n    torch.cuda.manual_seed_all(random_seed)\n\n    torch.backends.cudnn.deterministic = True\n","cea4390a":"class NumDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length_c,  max_length_w, inference_only=False):\n        super().__init__()\n\n        self.df = df\n        self.inference_only = inference_only\n        self.text = df.clean_text.tolist()\n        self.mask_word = df.mask_word.tolist()\n\n        if not self.inference_only:\n            self.target = torch.tensor(df.synset_id.values, dtype=torch.int32)\n\n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding='max_length',\n            max_length=max_length_c,\n            truncation=True,\n            return_attention_mask=True\n        )\n\n        self.encoded_word = tokenizer.batch_encode_plus(\n            self.mask_word,\n            padding='max_length',\n            max_length=max_length_w,\n            truncation=True,\n            return_attention_mask=True\n        )\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n\n        input_ids_word = torch.tensor(self.encoded_word['input_ids'][index])\n        attention_mask_word = torch.tensor(self.encoded_word['attention_mask'][index])\n\n        if self.inference_only:\n            return (input_ids, attention_mask, input_ids_word, attention_mask_word)\n        else:\n            target = self.target[index]\n            return (input_ids, attention_mask, input_ids_word, attention_mask_word, target)","c11f55da":"def predict(models0, data_loader0, models1, data_loader1, models2, data_loader2, models3, data_loader3):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n\n    preds = torch.zeros((test.shape[0], 69)).to(DEVICE)\n    for model in models0:\n        model.eval()\n        res = []\n        with torch.no_grad():\n            for batch_num, (input_ids, attention_mask, input_ids_word,\n                            attention_mask_word) in enumerate(tqdm(data_loader0)):\n\n                input_ids = input_ids.to(DEVICE)\n                attention_mask = attention_mask.to(DEVICE)\n                input_ids_word = input_ids_word.to(DEVICE)\n                attention_mask_word = attention_mask_word.to(DEVICE)\n\n                pred = model(input_ids, attention_mask,\n                             input_ids_word,\n                             attention_mask_word\n                             )\n                res.append(pred)\n\n        preds = preds + torch.tensor(0.30) * torch.cat(res, dim=0).softmax(dim=1)\n        \n        \n    del models0, data_loader0\n    gc.collect()\n        \n    for model in models1:\n        model.eval()\n        res = []\n        with torch.no_grad():\n            for batch_num, (input_ids, attention_mask, input_ids_word,\n                            attention_mask_word) in enumerate(tqdm(data_loader1)):\n\n                input_ids = input_ids.to(DEVICE)\n                attention_mask = attention_mask.to(DEVICE)\n                input_ids_word = input_ids_word.to(DEVICE)\n                attention_mask_word = attention_mask_word.to(DEVICE)\n\n                pred = model(input_ids, attention_mask,\n                             input_ids_word,\n                             attention_mask_word\n                             )\n                res.append(pred)\n\n        preds = preds + torch.tensor(0.40) * torch.cat(res, dim=0).softmax(dim=1)\n    \n    del models1, data_loader1, res, pred\n    gc.collect()\n    \n   \n    for model in models2:\n        model.eval()\n        res = []\n        with torch.no_grad():\n            for batch_num, (input_ids, attention_mask, input_ids_word,\n                            attention_mask_word) in enumerate(tqdm(data_loader2)):\n\n                input_ids = input_ids.to(DEVICE)\n                attention_mask = attention_mask.to(DEVICE)\n                input_ids_word = input_ids_word.to(DEVICE)\n                attention_mask_word = attention_mask_word.to(DEVICE)\n\n                pred = model(input_ids, attention_mask,\n                             input_ids_word,\n                             attention_mask_word\n                             )\n                res.append(pred)\n\n        preds = preds + torch.tensor(0.25) * torch.cat(res, dim=0).softmax(dim=1)\n\n    del models2, data_loader2, res, pred\n    gc.collect()\n    \n        \n    for model in models3:\n        model.eval()\n        res = []\n        with torch.no_grad():\n            for batch_num, (input_ids, attention_mask, input_ids_word,\n                            attention_mask_word) in enumerate(tqdm(data_loader3)):\n\n                input_ids = input_ids.to(DEVICE)\n                attention_mask = attention_mask.to(DEVICE)\n                input_ids_word = input_ids_word.to(DEVICE)\n                attention_mask_word = attention_mask_word.to(DEVICE)\n\n                pred = model(input_ids, attention_mask,\n                             input_ids_word,\n                             attention_mask_word\n                             )\n                res.append(pred)\n\n        preds = preds + torch.tensor(0.25) * torch.cat(res, dim=0).softmax(dim=1)\n\n    del models3, data_loader3, res, pred\n    gc.collect()\n   \n    \n    _, predicted = torch.max(preds, 1)\n\n    return predicted.detach().cpu().numpy(), preds.detach().cpu().numpy()","d4b9778e":"class NumModel(nn.Module):\n    def __init__(self, PATH, hidden_size):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(PATH)\n        config.update({\"output_hidden_states\": True,\n                       \"hidden_dropout_prob\": 0.1,\n                       \"layer_norm_eps\": 1e-7})\n\n        self.roberta = AutoModel.from_pretrained(PATH, config=config)\n\n        self.attention_c = nn.Sequential(\n            nn.Linear(hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n\n        self.attention_w = nn.Sequential(\n            nn.Linear(hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n\n        self.layer_norm_c = nn.LayerNorm(hidden_size)\n        self.layer_norm_w = nn.LayerNorm(hidden_size)\n\n        #self.dropout = nn.Dropout(0.10)\n        self.regressor = nn.Sequential(\n            nn.Linear(hidden_size*2, 69),\n#             nn.ReLU(),\n#             nn.Linear(1024, 69)\n        )\n\n        #init_params([self.attention_w, self.attention_c, self.regressor])\n\n    def forward(self, input_ids, attention_mask, input_ids_word, attention_mask_word):\n\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)\n        last_layer_hidden_states = self.layer_norm_c(roberta_output.hidden_states[-1])\n        \n        roberta_output_word = self.roberta(input_ids=input_ids_word,\n                                      attention_mask=attention_mask_word)\n        last_layer_hidden_states_word = self.layer_norm_w(roberta_output_word.hidden_states[-1])\n        \n        weights = self.attention_c(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)\n    \n        \n        weights = self.attention_w(last_layer_hidden_states_word)\n        word_vector = torch.sum(weights * last_layer_hidden_states_word, dim=1)\n\n        out = torch.cat([context_vector, word_vector], dim=1)\n        out = self.regressor(out)\n\n        return out","1b4c63b1":"submission = pd.read_csv(\"..\/input\/muis-challenge\/submission.csv\")\n\nNUM_FOLDS = 5\nNUM_EPOCHS = 3\nBATCH_SIZE = 8\nMAX_LEN = 128\n\nset_random_seed(2021)\n\nROBERTA_BASE_PATH = \"..\/input\/num-challenge-xmlroberta-fine-tuned\/XLMRobertaModel\"\nTRANSFORMER_PATH = '..\/input\/num-challenge-l-fine-tuned-transformer-mode\/transformer'\nROBERTA_LARGE_PATH = \"..\/input\/num-challenge-xlmroberta-large-fine\/XLMRobertaLargeModel\"\nMULTI_BERT_PATH = 'bert-base-multilingual-cased'\n\nmodels0 = []\nmodels1 = []\nmodels2 = []\nmodels3 = []\n \nfor fold in [0]:\n    print(\"XLM Roberta base model\")\n    model_path = \"..\/input\/d\/lhagiimn\/num-challenge-xlm-roberta-base-v2\/model_3.pth\"\n    \n    model = NumModel(ROBERTA_BASE_PATH, 768).to(DEVICE)\n\n    model.load_state_dict(torch.load(model_path))\n    model.to(DEVICE)\n    model.eval()\n\n    models0.append(model)\n    \n    del model\n    gc.collect()\n    \n    \n    print(\"XLM Rober large model\")\n    model_path = \"..\/input\/d\/lhagiimn\/num-challenge-xlm-roberta-base-v2\/model_2.pth\"\n    model = NumModel(ROBERTA_LARGE_PATH, 1024).to(DEVICE)\n\n    model.load_state_dict(torch.load(model_path))\n    model.to(DEVICE)\n    model.eval()\n\n    models1.append(model)\n    \n    del model\n    gc.collect()\n\n\n    print(\"XLM Transformer model\")\n    model_path = \"..\/input\/d\/lhagiimn\/num-challenge-xlm-roberta-base-v2\/model_1.pth\"\n    \n    model = NumModel(TRANSFORMER_PATH, 1280).to(DEVICE)\n\n    model.load_state_dict(torch.load(model_path))\n    model.to(DEVICE)\n    model.eval()\n\n    models2.append(model)\n    \n    del model\n    gc.collect()\n\n\n    print(\"Multilingual BERT model\")\n    model_path = \"..\/input\/d\/lhagiimn\/num-challenge-xlm-roberta-base-v2\/model_4.pth\"\n    \n    model = NumModel(MULTI_BERT_PATH, 768).to(DEVICE)\n\n    model.load_state_dict(torch.load(model_path))\n    model.to(DEVICE)\n    model.eval()\n\n    models3.append(model)\n    \n    del model\n    gc.collect()\n    ","fbe3beb6":"ROBERTA_BASE_PATH = \"..\/input\/num-challenge-xmlroberta-fine-tuned\/XLMRobertaModel\"\nTRANSFORMER_PATH = '..\/input\/num-challenge-l-fine-tuned-transformer-mode\/transformer'\nROBERTA_LARGE_PATH = \"..\/input\/num-challenge-xlmroberta-large-fine\/XLMRobertaLargeModel\"\nMULTI_BERT_PATH = 'bert-base-multilingual-cased'\n\ntokenizer_large = AutoTokenizer.from_pretrained(ROBERTA_LARGE_PATH)\ntokenizer_base = AutoTokenizer.from_pretrained(ROBERTA_BASE_PATH)\ntokenizer_transformer = AutoTokenizer.from_pretrained(TRANSFORMER_PATH)\ntokenizer_bert = AutoTokenizer.from_pretrained(MULTI_BERT_PATH)\n\ntest = pd.read_csv('..\/input\/d\/lhagiimn\/num-challenge-xlm-roberta-base-v2\/test.csv')\ntest['mask_word'] = test['mask_word'].fillna(\"\u0443\u0442\u0433\u0430\u0433\u04af\u0439 \u0445\u043e\u043e\u0441\u043e\u043d\")\ntest_dataset = NumDataset(test, tokenizer_base, max_length_c=128,  max_length_w=96, inference_only=True)\ntest_loader0 = DataLoader(test_dataset, batch_size=16, drop_last=False, shuffle=False)\n\ntest_dataset = NumDataset(test, tokenizer_large, max_length_c=128,  max_length_w=96, inference_only=True)\ntest_loader1 = DataLoader(test_dataset, batch_size=16,drop_last=False, shuffle=False)\n\ntest_dataset = NumDataset(test, tokenizer_transformer, max_length_c=128,  max_length_w=96, inference_only=True)\ntest_loader2 = DataLoader(test_dataset, batch_size=16,drop_last=False, shuffle=False)\n\ntest_dataset = NumDataset(test, tokenizer_bert, max_length_c=128,  max_length_w=96, inference_only=True)\ntest_loader3 = DataLoader(test_dataset, batch_size=16,drop_last=False, shuffle=False)\n\n\npred, pred_prob = predict(models0, test_loader0, models1, test_loader1, models2, test_loader2, models3, test_loader3)","e7d6c4c9":"pred","5c531131":"np.save('logits.npy', pred_prob)","4fdf18fb":"test['pred'] = pred+1\n\nsubmission = submission.set_index(['text_id'])\ntest = test.set_index(['text_id'])\n\nsubmission['synset_id'] = test.loc[submission.index, 'pred'].values\nsubmission.to_csv('submission.csv')","d3681343":"# Pretrained Models\n\n1. **XLM Roberta-base Model:** https:\/\/www.kaggle.com\/lhagiimn\/num-challenge-xlmroberta-fine-tune \n2. **XLM Roberta-large Model:** https:\/\/www.kaggle.com\/lhagiimn\/num-challenge-xlmroberta-large-fine-tune\n3. **XLM Transformer Model:** https:\/\/www.kaggle.com\/lhagiimn\/num-challenge-transformer-fine-tune\n4. **Multilingual BERT Model:** 'bert-base-multilingual-cased'\n\n# Model Training Notebook\n\n**Notebook:** https:\/\/www.kaggle.com\/lhagiimn\/num-challenge-models-training\/notebook\n\n\n# Result\n\n1. XLM Roberta-base Model CV: 0.9595\n2. XLM Roberta-large Model CV: 0.9670\n3. XLM Transformer Model CV: 0.9580\n4. Multilingual BERT Model CV: 0.9587\n\nLB: 0.9635 {0.20, 0.40, 0.20, 0.20}\nLB: 0.9620 {0.10, 0.70, 0.10, 0.10}\nLB: ??? {0.20, 0.50, 0.25, 0.25}"}}