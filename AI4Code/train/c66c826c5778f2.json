{"cell_type":{"ed9b527d":"code","c6d067e7":"code","7dff0344":"code","ec3ac7fa":"code","2eef93c5":"code","0ef44236":"code","55f0aa3a":"code","facdf29e":"code","f479ae98":"code","06d05a92":"code","bfd162e4":"code","1ee44c28":"code","3a76b591":"code","94eeb188":"code","c265ae97":"code","60053683":"code","f7cc0b35":"code","5ee4ba81":"code","c7f5b3f7":"code","770ad063":"code","8677fd32":"code","bcea784b":"code","40c203d6":"code","7c226118":"code","99c59b3d":"markdown","17a87e89":"markdown","fca75fbc":"markdown","3979133c":"markdown","7fab7030":"markdown"},"source":{"ed9b527d":"!pip install --upgrade seaborn #Upgrade","c6d067e7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\nTrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nTrain = Train.set_index('Id')\nTest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nTest = Test.set_index('Id')\n\npreprocessing_path = '..\/input\/house-price-eda'\n\ntrain_scale = pd.read_csv(f'{preprocessing_path}\/train_scaled.csv')\ntrain_scale = train_scale.set_index('Id')\ntest_scale = pd.read_csv(f'{preprocessing_path}\/test_scaled.csv')\ntry:\n    test_scale = test_scale.rename(columns={'Unnamed: 0':'Id'})\nexcept:\n    pass\ntest_scale = test_scale.set_index('Id')\n\nTrain  = Train.loc[train_scale.index]\n\nimport glob\n\nlisting = glob.glob(f'{preprocessing_path}\/*_one_hot_pickle')\n\nimport pickle\n#Preprocess train data\nfor model_file in listing:\n    col = model_file.split('_one_')[0]\n\n    col = col.split('\/')[-1]\n    enc = pickle.load(open(model_file, 'rb'))\n    new_cols = pd.DataFrame(enc.transform(Train[[col]]).toarray(),\n                    columns=f'{col}_' + enc.categories_[0])\n    new_cols.index = Train.index\n    Train = pd.concat([Train, new_cols], axis = 1)\n    Train = Train.drop(col, axis = 1)\n    \nfor log_col in train_scale.columns[train_scale.columns.str.endswith('log')]:\n    col = log_col.split('_log')[0]\n    Train[log_col] = np.log(Train[col])\n    Train = Train.drop(col, axis = 1)\n    \nTrain = Train[train_scale.columns]\n#Train = Train.drop(1276) #Seem outlier\n#Preprocess test data\nfor model_file in listing:\n    col = model_file.split('_one_')[0]\n\n    col = col.split('\/')[-1]\n    enc = pickle.load(open(model_file, 'rb'))\n    new_cols = pd.DataFrame(enc.transform(Test[[col]]).toarray(),\n                    columns=f'{col}_' + enc.categories_[0])\n    \n    new_cols.index = Test.index\n    Test = pd.concat([Test, new_cols], axis = 1)\n    Test = Test.drop(col, axis = 1)\n    \nfor log_col in test_scale.columns[test_scale.columns.str.endswith('log')]:\n    col = log_col.split('_log')[0]\n    Test[log_col] = np.log(Test[col])\n    Test = Test.drop(col, axis = 1)\n    \nTest = Test[test_scale.columns]","7dff0344":"meanMSSubClass_Saleprice = Train[['MSSubClass', 'SalePrice_log']].groupby('MSSubClass').mean()","ec3ac7fa":"for col in Test.columns:\n    if len(set(Train[col])) > 2 and '_log' not in col:\n        Train[f'{col}_log'] = np.log(Train[col] + 1)\n        Train = Train.drop(col, axis = 1)\n        \n        Test[f'{col}_log'] = np.log(Test[col] + 1)\n        Test = Test.drop(col, axis = 1)","2eef93c5":"# Train['GrLivArea_log_square'] = Train['GrLivArea_log']**2\n# Test['GrLivArea_log_square'] = Test['GrLivArea_log']**2\n# for col in Train.columns:\n#     if '_log' in col and col != 'SalePrice_log':\n#         Train[col + '_square'] = Train[col] **2\n\n# for col in Test.columns:\n#     if '_log' in col and col != 'SalePrice_log':\n#         Test[col + '_square'] = Test[col] **2","0ef44236":"Train = Train.drop(1276)","55f0aa3a":"corr = Train.corr()['SalePrice_log']\nTrain_err = Train[corr[np.abs(corr) < 0.4].index]\nTest_err = Test[corr[np.abs(corr) < 0.4].index]\nTrain = Train[corr[np.abs(corr) > 0.4].index]\nTest = Test[corr[np.abs(corr) > 0.4].index.drop('SalePrice_log')]\n\n","facdf29e":"def getFoldValidation(length, K = 4):\n    diff = int(length\/K)\n    fraction = length % K\n    batchValidation = [[k*diff, (k + 1)*diff] for k in range(K)]\n    batchValidation[-1][1] = batchValidation[-1][1] + fraction - 1\n    return batchValidation\nbatchValidation = np.array(getFoldValidation(len(Train)))","f479ae98":"def getTrainTestBatch():\n    fold = 0\n    while fold < batchValidation.shape[0]:\n        train_batch = pd.concat([Train.iloc[batchValidation[i][0]:batchValidation[i][1]]\n             for i in range(batchValidation.shape[0]) if i !=  fold])\n        test_batch = Train.iloc[batchValidation[fold][0]:batchValidation[fold][1]]\n        yield train_batch, test_batch, fold\n        fold += 1\ntrain_batch, test_batch, fold = next(getTrainTestBatch())     \n                                ","06d05a92":"import keras.backend as K\n\ndef mse_loss(y_true, y_pred):\n    return K.sqrt(K.mean((y_true - y_pred)**2))\ndef e_loss(y_true, y_pred):\n    return K.sqrt(K.mean((np.e**y_true - np.e**y_pred)**2))\n","bfd162e4":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nclass linearAndErrorModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def getErrorModel(self, epochs=150):\n#         model = Sequential()\n#         model.add(Dense(12, input_dim=self.X_Train_err.shape[1], activation='sigmoid'))\n#         model.add(Dense(8, activation='sigmoid'))\n#         model.add(Dense(1, activation='linear'))\n#         # compile the keras model\n#         model.compile(loss='MSE', optimizer='adam', metrics=['MSE'])\n#         # fit the keras model on the dataset\n#         model.fit_ = model.fit\n#         model.fit = lambda X_Train, Y_Train:model.fit_(X_Train, Y_Train, epochs=epochs, batch_size=10)\n        \n        return xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                         learning_rate=0.05, max_depth=50, \n                         min_child_weight=1.7817, n_estimators=2200,\n                         reg_alpha=0.4640, reg_lambda=0.8571,\n                         subsample=0.5213,\n                         random_state =7, nthread = -1)\n\n    def __init__(self):\n        self.models = models\n    \n    # we define clones of the original models to fit the data in\n    def fit(self, X_Train, X_Train_err, y, epochs=150):\n        baseModel = LinearRegression()\n    \n        \n        \n        baseModel.fit(X_Train,Y_Train)\n        predict_Train = baseModel.predict(X_Train)\n        error_Train = Y_Train - predict_Train\n        \n        X_Train_err['predict'] = predict_Train\n        \n        self.X_Train_err = X_Train_err\n        self.errorModel = self.getErrorModel(epochs=epochs)\n        self.errorModel.fit(X_Train_err, error_Train)\n        \n        predict_Train = self.errorModel.predict(X_Train_err)\n        error_Train = error_Train.values.flatten() - predict_Train.flatten()\n        X_Train_err['predict_2'] = predict_Train\n        \n        self.errorModel_2 = self.getErrorModel(epochs=epochs)\n        self.errorModel_2.fit(X_Train_err, error_Train)\n        \n        \n        self.baseModel = baseModel\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X_val, X_val_err):\n        predict = self.baseModel.predict(X_val)\n        X_val_err['predict'] = predict\n        error = self.errorModel.predict(X_val_err)\n        \n        X_val_err['predict_2'] = error.flatten()\n        error_2 = self.errorModel_2.predict(X_val_err)\n        predict = predict.flatten() + error.flatten()\n        return predict","1ee44c28":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nimport xgboost as xgb\n#models = ['ridge', 'SVR']\n#models = ['linear', 'gradient_boost']\nmodels = ['linear']\n#models = 'NN'\ngenarator =  getTrainTestBatch()\n\nVal_Report = {key : [] for key in models}\n\nMSEs = np.array([])\nfor train_batch, test_batch, fold in genarator:\n    sampling = train_batch.sample(frac=1)\n    \n    \n#     model = LinearRegression()\n    \n#     dtr = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n#                              learning_rate=0.05, max_depth=3, \n#                              min_child_weight=1.7817, n_estimators=2200,\n#                              reg_alpha=0.4640, reg_lambda=0.8571,\n#                              subsample=0.5213,\n#                              random_state =7, nthread = -1)\n    \n    X_Train = sampling.drop('SalePrice_log', axis = 1)\n    X_Train_err = Train_err.loc[X_Train.index]\n    Y_Train = sampling[['SalePrice_log']]\n    \n   \n    \n    X_val = test_batch.drop('SalePrice_log', axis = 1)\n    X_val_err = Train_err.loc[X_val.index]\n    Y_val = test_batch[['SalePrice_log']]\n    \n    LAEM = linearAndErrorModels()\n    LAEM.fit(X_Train, X_Train_err, Y_Train, epochs = 1000)\n    predict = LAEM.predict(X_val, X_val_err)\n    \n    MSE = mse_loss(K.constant(Y_val), K.constant(predict)).numpy()\n    \n    MSEs = np.append(MSEs, MSE)\n","3a76b591":"MSEs","94eeb188":"X_Train = Train.drop('SalePrice_log', axis = 1)\nX_Train_err = Train_err.loc[X_Train.index]\nY_Train = Train[['SalePrice_log']]\n\nLAEM = linearAndErrorModels()\nLAEM.fit(X_Train, X_Train_err, Y_Train)","c265ae97":"predict = LAEM.predict(Test, Test_err)","60053683":"# model = LinearRegression()\n    \n# dtr = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n#                          learning_rate=0.05, max_depth=3, \n#                          min_child_weight=1.7817, n_estimators=2200,\n#                          reg_alpha=0.4640, reg_lambda=0.8571,\n#                          subsample=0.5213,\n#                          random_state =7, nthread = -1)\n\n# X_Train = Train.drop('SalePrice_log', axis = 1)\n# X_Train_err = Train_err.loc[X_Train.index]\n# Y_Train = Train[['SalePrice_log']]\n\n\n\n# X_val = Test\n# X_val_err = Test_err.loc[X_val.index]\n\n\n# model.fit(X_Train,Y_Train)\n\n# predict_Train = model.predict(X_Train)\n\n# error_Train = Y_Train - predict_Train\n\n# X_Train_err['predict'] = predict_Train\n\n# dtr.fit(X_Train_err, error_Train)\n\n# predict = model.predict(X_val)\n\n# X_val_err['predict'] = predict\n# err_predict = dtr.predict(X_val_err)\n\n# predict = predict.flatten() + err_predict","f7cc0b35":"# Y_Train.values.flatten() - (predict_Train.flatten() + dtr.predict(X_Train_err))","5ee4ba81":"# Y_Train.values.flatten()","c7f5b3f7":"# import seaborn as sns\n# error = pd.DataFrame(error)\n# sns.displot(error, x=\"SalePrice_log\")","770ad063":"# Train['error'] = error['SalePrice_log']","8677fd32":"# err_reg = LinearRegression().fit(Train.drop(['SalePrice_log','error'], axis =1), Train[['error']])\n# err_pred = err_reg.predict(Train.drop(['SalePrice_log','error'], axis =1))","bcea784b":"# mse_loss(K.constant(Train[['error']]), K.constant(err_pred)).numpy()","40c203d6":"predict = pd.DataFrame(predict)\npredict.index = Test.index\npredict.columns = ['SalePrice_log']\npredict['SalePrice'] = np.e**(predict['SalePrice_log'])\npredict = predict.drop('SalePrice_log', axis = 1)\npredict","7c226118":"predict.to_csv(\"submission.csv\")","99c59b3d":"# Data Preparation","17a87e89":"# Postprocessing","fca75fbc":"# Post-preprocessing","3979133c":"# Model","7fab7030":"# Batch fold spliter"}}