{"cell_type":{"7fd3aeba":"code","67296028":"code","8c42a6b8":"code","b00a0616":"code","7845bc87":"code","618e06b0":"code","41b1e595":"code","9f0724ad":"code","7d9e448c":"code","cd8ca8f0":"code","cf6fca91":"code","0d5e2f88":"code","25b50c87":"code","b9f3964d":"code","56a3469d":"code","5f081db2":"code","72425e0d":"code","40fc2b0e":"code","901aafa9":"code","78d18c64":"code","205b29d9":"code","31858c51":"code","35f992f9":"code","d87dc7d3":"code","f50d3a87":"code","7d9c5b0d":"code","e45d6c38":"code","50f4a3e0":"code","f2c59aef":"code","a6a53ba7":"code","a3690294":"code","fb71cf72":"code","076accc5":"code","fe454a4c":"code","7bfc0d38":"code","fbde657f":"code","be2868d3":"code","52d6986a":"code","52ed530f":"code","0b90a55f":"code","a8525354":"code","3ced4c78":"code","8f48ef05":"code","303bfaaa":"code","9c2b2d36":"code","adb03c9d":"code","3b899d54":"code","b1ce2184":"code","52df6f9d":"code","d042793a":"code","57dea774":"code","9e861b2a":"code","6d072298":"code","6b993d36":"code","a6ccf540":"code","198f9512":"code","a8b9ce41":"code","3dbdfc14":"code","d158356c":"code","e5c8c7d2":"code","34b449e2":"code","e4e7fbdf":"code","23c90d09":"code","9be1ee62":"code","c7c26908":"code","9bf99620":"code","a78c7687":"code","764a02d2":"code","834c0ac9":"code","f33139f4":"code","e20ebc98":"code","c9d30c86":"markdown","421c18c3":"markdown","8dd085c2":"markdown","b7f6bb93":"markdown","17488522":"markdown","7b4d6063":"markdown","54d32e4e":"markdown"},"source":{"7fd3aeba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas as pd\nimport numpy as np\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nimport spacy\nfrom bs4 import BeautifulSoup # Library beatifulsoup4 handles html\nfrom nltk.corpus import stopwords\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","67296028":"train_path = \"..\/input\/nlp-getting-started\/train.csv\"\ntest_path = \"..\/input\/nlp-getting-started\/test.csv\"\nsample_submission_path = \"..\/input\/nlp-getting-started\/sample_submission.csv\"","8c42a6b8":"df_train = pd.read_csv(train_path)\ndf_test = pd.read_csv(test_path)","b00a0616":"df_train.head()","7845bc87":"df_train.info()","618e06b0":"#We do not want to keep our meta data\ndf_train = df_train[['text','target']]\ndf_test = df_test[['text']]","41b1e595":"df_train.target.value_counts()","9f0724ad":"#Length of the tweets\ndf_train['text_len'] = df_train['text'].apply(len)\ndf_train['text_len'].describe()\n\ndf_test['text_len'] = df_test['text'].apply(len)","7d9e448c":"plt.hist(df_train.text_len)\nplt.show()","cd8ca8f0":"f, axes = plt.subplots(1, 2, figsize=(15, 5), sharex=True)\nf.suptitle(\"Histogram of char length of text\",fontsize=20)\nsns.distplot(df_train[df_train['target']==0].text_len.values,kde=False,bins=20,hist=True,ax=axes[0],label=\"Bins unreal disaster\",\n            kde_kws={\"color\": \"r\", \"lw\": 2, \"label\": \"KDE 0\"},\n                           hist_kws={ \"linewidth\": 2,\n                                     \"alpha\": 1, \"color\": \"orange\"})\naxes[0].legend(loc=\"best\")\naxes[0].set_ylabel(\"Rows Count\")\nsns.distplot(df_train[df_train['target']==1].text_len.values,kde=False,bins=20,hist=True,ax=axes[1],label=\"Bins real disaster\",\n            kde_kws={\"color\": \"g\", \"lw\": 2, \"label\": \"KDE 1\"},\n                           hist_kws={ \"linewidth\": 2,\n                                     \"alpha\": 1, \"color\": \"red\"})\naxes[1].legend(loc=\"best\")","cf6fca91":"#number of words\ndef word_count(txt):\n    return len(txt.split())\ndf_train['word_count'] = df_train.text.apply(word_count)\n\ndf_test['word_count'] = df_test.text.apply(word_count)","0d5e2f88":"#average number of characters per word\ndf_train['char_word_count']=df_train['text_len']\/df_train['word_count']\n\ndf_test['char_word_count']=df_test['text_len']\/df_test['word_count']","25b50c87":"df_train.head()","b9f3964d":"df_train.groupby(['target'])['char_word_count'].mean()","56a3469d":"def url_count(txt):\n    return len(re.findall('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',txt))","5f081db2":"df_train['url_count'] = df_train.text.apply(url_count)\n\ndf_test['url_count'] = df_test.text.apply(url_count)","72425e0d":"df_train['url_count'].value_counts()","40fc2b0e":"#create dummy for url instead of continuouse variable\ndf_train['url_d'] = (df_train['url_count'] > 0).astype(int)\n\ndf_test['url_d'] = (df_test['url_count'] > 0).astype(int)","901aafa9":"import emoji\ndef emoji_count(txt):\n    e_txt = emoji.demojize(txt)\n    return len(re.findall(':(.*?):',e_txt))","78d18c64":"df_train['emoji_count'] = df_train.text.apply(emoji_count)\n\ndf_test['emoji_count'] = df_test.text.apply(emoji_count)","205b29d9":"#create dummy for dummy instead of continuouse variable\ndf_train['emoji_d'] = (df_train['emoji_count'] > 0).astype(int)\n\ndf_test['emoji_d'] = (df_test['emoji_count'] > 0).astype(int)","31858c51":"def count_hashtags(text):\n    gethashtags = re.findall('#\\w*[a-zA-Z]\\w*',text)\n    return len(gethashtags)","35f992f9":"df_train['hash_count'] = df_train.text.apply(count_hashtags)\n\ndf_test['hash_count'] = df_test.text.apply(count_hashtags)","d87dc7d3":"#exclamation marks and question marks\ndef count_punctuations(text):\n    getpunctuation = re.findall('[?!]+?',text)\n    return len(getpunctuation)","f50d3a87":"df_train['punctuation_count'] = df_train.text.apply(count_punctuations)\n\ndf_test['punctuation_count'] = df_test.text.apply(count_punctuations)","7d9c5b0d":"\n\nf, axes = plt.subplots(1, 2, figsize=(15, 5), sharex=True)\nf.suptitle(\"Histogram of Excla\/Question marks\",fontsize=20)\nsns.distplot(df_train[df_train['target']==0].punctuation_count.values,kde=False,bins=20,hist=True,ax=axes[0],label=\"Bins unreal disaster\",\n            kde_kws={\"color\": \"r\", \"lw\": 2, \"label\": \"KDE 0\"},\n                           hist_kws={ \"linewidth\": 2,\n                                     \"alpha\": 1, \"color\": \"orange\"})\naxes[0].legend(loc=\"best\")\naxes[0].set_ylabel(\"Rows Count\")\nsns.distplot(df_train[df_train['target']==1].punctuation_count.values,kde=False,bins=20,hist=True,ax=axes[1],label=\"Bins real disaster\",\n            kde_kws={\"color\": \"g\", \"lw\": 2, \"label\": \"KDE 1\"},\n                           hist_kws={ \"linewidth\": 2,\n                                     \"alpha\": 1, \"color\": \"red\"})\naxes[1].legend(loc=\"best\")","e45d6c38":"df_train.head()","50f4a3e0":"# Lemmatize with POS Tag\ndef get_wordnet_pos(word):\n    \"\"\"Map POS tag to first character for lemmatization\"\"\"\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n\n    return tag_dict.get(tag, wordnet.NOUN)","f2c59aef":"def clean_text(df):\n    \n    tweets = []\n\n    lemmatizer = WordNetLemmatizer()\n    \n    \n    for tweet in df:\n        \n        # remove html content\n        tweet_text = BeautifulSoup(tweet).get_text()\n        \n        # remove non-alphabetic characters\n        tweet_text = re.sub(\"[^a-zA-Z]\",\" \", tweet_text)\n    \n        # tokenize the sentences\n        words = word_tokenize(tweet_text.lower())\n  \n        # filter stopwords\n        words = [w for w in words if w not in stopwords.words(\"english\")]\n        \n        # lemmatize each word to its lemma\n        lemma_words =[lemmatizer.lemmatize(i, get_wordnet_pos(i)) for i in words]\n    \n        tweets.append(lemma_words)\n       \n\n    return(tweets) ","a6a53ba7":"tweets = clean_text(df_train.text)\n\ntweets_test = clean_text(df_test.text)","a3690294":"# Undo the tokenization and put the data into a new column in the data frame.\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\n\ndf_train['text'] = [TreebankWordDetokenizer().detokenize(word) for word in tweets]\n\ndf_test['text'] = [TreebankWordDetokenizer().detokenize(word) for word in tweets_test]","fb71cf72":"# We conduct a simple dictionary based sentiment anlysis by using the the package Blob  \nfrom textblob import TextBlob\n# Defining a sentiment analyser function\ndef sentiment_analyser(text):\n    return text.apply(lambda Text: pd.Series(TextBlob(Text).sentiment.polarity))\n\n# Applying function to reviews\ndf_train['Polarity'] = sentiment_analyser(df_train['text'])\n\ndf_test['Polarity'] = sentiment_analyser(df_test['text'])","076accc5":"# This code is adapted from a kaggle post by Hsankesara (https:\/\/www.kaggle.com\/hsankesara\/understanding-medium)\ndef get_words_count(df, col):\n    words_count = {}\n    m = df.shape[0]\n    for i in range(m):\n        words = df[col].iat[i].split()\n        for word in words:\n            if word.lower() in words_count:\n                words_count[word.lower()] += 1\n            else:\n                words_count[word.lower()] = 1\n    return words_count","fe454a4c":"tweet_words = get_words_count(df_train[df_train.target == 1], 'text')","7bfc0d38":"tweet_words_df = pd.DataFrame(list(tweet_words.items()), columns=['words', 'count'])","fbde657f":"## List of 30 most frequent words occurred in tweet\ntweet_words_df.sort_values(by='count', ascending=False).head(30)","be2868d3":"#remove words less than 3 characters from dataframe\ntweet_words_df=tweet_words_df[tweet_words_df['words'].apply(len) > 2]","52d6986a":"#displayed in a WordCloud\nfrom wordcloud import WordCloud\nfig = plt.figure(dpi=100)\na4_dims = (6, 12)\nfig, ax = plt.subplots(figsize=a4_dims)\nwordcloud = WordCloud(background_color ='white', max_words=200,max_font_size=40,random_state=3).generate(str(tweet_words_df.sort_values(by='count', ascending=False)['words'].values[:15]))\nplt.imshow(wordcloud)\nplt.title = 'Top Word in the disaster tweets'\nplt.show()","52ed530f":"top_tweet_words = tweet_words_df.sort_values(by='count', ascending=False)['words'].values[:30]","0b90a55f":"df_train['top_tweet_count'] = df_train['text'].apply(lambda s: sum(s.count(top_tweet_words[i]) for  i in range(30)))\n\ndf_test['top_tweet_count'] = df_test['text'].apply(lambda s: sum(s.count(top_tweet_words[i]) for  i in range(30)))","a8525354":"from sklearn.model_selection import train_test_split\n#splitting into train and test data\nx_train, x_test, y_train, y_test = train_test_split(df_train, np.ravel(df_train.target), test_size=0.2, random_state=42)","3ced4c78":"x_train1=x_train[['word_count', 'char_word_count', 'url_count', 'emoji_count', 'hash_count', 'punctuation_count', 'Polarity', 'top_tweet_count']]\nx_test1=x_test[['word_count', 'char_word_count', 'url_count', 'emoji_count', 'hash_count', 'punctuation_count', 'Polarity', 'top_tweet_count']]","8f48ef05":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler().fit(x_train1)#mind that we use only x_train values not to leak the data to the test set\nx_train1= scaler.transform(x_train1)\nx_test1= scaler.transform(x_test1) ","303bfaaa":"# Build vocabulary using Keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom gensim.models import KeyedVectors\nfrom gensim.models.keyedvectors import Word2VecKeyedVectors","9c2b2d36":"NUM_WORDS = 2000\n\ntokenizer_obj = Tokenizer(NUM_WORDS, oov_token=1)  # We fit the tokenizer to the training set articles. The test set might include\ntokenizer_obj.fit_on_texts(x_train.text)  # words that are not part of the training data. The argument oov_token ensures that such new words are mapped to the specified index","adb03c9d":"# Convert training set articles to sequences of integer values\nX_tr_int = tokenizer_obj.texts_to_sequences(x_train.text)","3b899d54":"#* Determine the maximum article length in the training set\nmax_tweet_length = max([len(tweet) for tweet in X_tr_int])\nprint('The longest tweet of the training set has {} words.'.format(max_tweet_length))","b1ce2184":"X_tr_int_pad = pad_sequences(X_tr_int, max_tweet_length)","52df6f9d":"#apply to test dataset\n\n# Encode and pad the test data\nX_ts_int = tokenizer_obj.texts_to_sequences(x_test.text)  # Due to oov_token argument, new words will be mapped to 1\nX_ts_int_pad = pad_sequences(X_ts_int, max_tweet_length)","d042793a":"# Structure of the prepared training and test data\nX_tr_int_pad.shape, y_train.shape, X_ts_int_pad.shape, y_test.shape","57dea774":"# Load GloVe embeddings\nglove_index = {}\nwith open('..\/input\/glove\/glove.6B.50d.txt', 'r', encoding=\"utf8\") as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        glove_index[word] = coefs\n\nprint('Found %s word vectors.' % len(glove_index))","9e861b2a":"def get_embedding_matrix(tokenizer, pretrain, vocab_size):\n    '''\n        Helper function to construct an embedding matrix for \n        the focal corpus based on some pre-trained embeddings.\n    '''\n    \n    dim = 0\n    if isinstance(pretrain, KeyedVectors) or isinstance(pretrain, Word2VecKeyedVectors):\n        dim = pretrain.vector_size        \n    elif isinstance(pretrain, dict):\n        dim = next(iter(pretrain.values())).shape[0]  # get embedding of an arbitrary word\n    else:\n        raise Exception('{} is not supported'.format(type(pretrain)))\n    \n    \n    # Initialize embedding matrix\n    emb_mat = np.zeros((vocab_size, dim))\n\n    # There will be some words in our corpus for which we lack a pre-trained embedding.\n    # In this tutorial, we will simply use a vector of zeros for such words. We also keep\n    # track of the words to do some debugging if needed\n    oov_words = []\n    # Below we use the tokenizer object that created our task vocabulary. This is crucial to ensure\n    # that the position of a words in our embedding matrix corresponds to its index in our integer\n    # encoded input data\n    for word, i in tokenizer.word_index.items():  \n        # try-catch together with a zero-initilaized embedding matrix achieves our rough fix for oov words\n        try:\n            emb_mat[i] = pretrain[word]\n        except:\n            oov_words.append(word)\n    print('Created embedding matrix of shape {}'.format(emb_mat.shape))\n    print('Encountered {} out-of-vocabulary words.'.format(len(oov_words)))\n    return (emb_mat, oov_words)","6d072298":"# Create matrix with Glove embeddings\nglove_weights, _ = get_embedding_matrix(tokenizer_obj, glove_index, NUM_WORDS)","6b993d36":"from keras.models import Model\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding,GRU, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.initializers import Constant\nfrom keras.layers import Bidirectional\nfrom keras.layers import Input\nfrom keras.layers.merge import Concatenate ","a6ccf540":"EPOCH = 5\nEMBEDDING_DIM = 50\nBATCH_SIZE=32\nVAL_SPLIT = 0.25","198f9512":"#we have two inputs\ninput_1 = Input(shape=(max_tweet_length,))\n\ninput_2 = Input(shape=(8,))","a8b9ce41":"# submodel embedding layer\nembedding_layer = Embedding(NUM_WORDS, \n                         EMBEDDING_DIM,  \n                         embeddings_initializer=Constant(glove_weights), \n                         input_length=max_tweet_length, \n                         trainable=False\n                         ) (input_1)\nGRU_Layer_1 = Bidirectional(GRU(512))(embedding_layer)","3dbdfc14":"dense_layer_1 = Dense(10, activation='relu')(input_2)\ndense_layer_2 = Dense(10, activation='relu')(dense_layer_1)","d158356c":"concat_layer = Concatenate()([GRU_Layer_1, dense_layer_2])\ndense_layer_3 = Dense(10, activation='relu')(concat_layer)\noutput = Dense(1, activation='sigmoid')(dense_layer_3)\nmodel = Model(inputs=[input_1, input_2], outputs=output)","e5c8c7d2":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","34b449e2":"model_story = model.fit(x=[X_tr_int_pad, x_train1], y=y_train, validation_data=([X_ts_int_pad, x_test1], y_test), batch_size=BATCH_SIZE, epochs=EPOCH, verbose=1 )","e4e7fbdf":"df_test1=df_test[['word_count', 'char_word_count', 'url_count', 'emoji_count', 'hash_count', 'punctuation_count', 'Polarity', 'top_tweet_count']]","23c90d09":"df_test1= scaler.transform(df_test1)","9be1ee62":"# Encode and pad the test prediction data\ndf_ts_int = tokenizer_obj.texts_to_sequences(df_test.text) \ndf_ts_int_pad = pad_sequences(df_ts_int, max_tweet_length)","c7c26908":"#Prepare submission\n# make predictions on the testing data\npreds = model.predict([df_ts_int_pad , df_test1])","9bf99620":"preds","a78c7687":"preds=np.round(preds).astype(int)","764a02d2":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","834c0ac9":"sample_submission[\"target\"] = preds","f33139f4":"sample_submission.head()","e20ebc98":"sample_submission.to_csv(\"submission.csv\", index=False)","c9d30c86":"# Get data","421c18c3":"# Modeling","8dd085c2":"#Feature engineering part II","b7f6bb93":"Prepare datatset for predictions","17488522":"# EDA & Feature engineering\n\n*credits to https:\/\/www.kaggle.com\/aman2000jaiswal\/real-nlp-disaster-tweets for some feature engineering code","7b4d6063":"# Make predictions","54d32e4e":"# cleaning text"}}