{"cell_type":{"626a6221":"code","957c3fc7":"code","a291b161":"code","70ad9420":"code","80b04d4f":"code","c9b96e7b":"code","8d1e3382":"code","b6b7610e":"code","d68f2b11":"code","cf0aee5c":"code","0637ee4d":"code","4ccf25b3":"code","b01a02f6":"code","d424993c":"code","52b87300":"code","ff142ba3":"code","e285d292":"code","67d0b8c9":"code","f81e4872":"code","5eb5f335":"code","fbb08f64":"code","07e72526":"code","1918d0da":"code","41cd8404":"code","ce1a5ec6":"code","42c01fab":"code","030e7b15":"code","515bf8e1":"code","1c5b08f3":"code","1b30e30d":"code","09d24342":"code","0bade3ea":"code","4c1081d4":"code","d46b0da2":"code","86ce00ef":"code","a54c52de":"code","0f21de1e":"code","20e67c4c":"code","06a44014":"code","8308cb29":"code","53328170":"code","8c7d7973":"code","54fd0f07":"code","06bca9a1":"code","71fa20f5":"code","04cf0fb7":"code","e5aea496":"code","8765eeb7":"code","429df939":"code","f4905596":"code","7ee7bf9d":"code","c656646e":"code","ebdb23f7":"code","156e24c0":"code","0f28096d":"code","3acc9685":"code","a6b84022":"code","bb048811":"code","a453b7ef":"code","3890bf70":"code","c4618c78":"code","ebd282af":"code","c02b81d7":"code","aee4d359":"code","f8217450":"code","5e957d0b":"code","8567365e":"markdown","46ab9350":"markdown","adbc3833":"markdown","beb4321c":"markdown","1bbf9dd1":"markdown","11fbbdf6":"markdown","62208fab":"markdown","219c65ac":"markdown","9657e46c":"markdown","66bcdf57":"markdown","bc450d68":"markdown","74761e97":"markdown","19c60650":"markdown","b2047fa1":"markdown","8a35bf5a":"markdown","baa5f5cb":"markdown","10637a2b":"markdown","d666a170":"markdown","f45df655":"markdown","7650f526":"markdown","5878b041":"markdown","e4c8b403":"markdown","518c95a3":"markdown","fdc4ae7a":"markdown","0b50ab8b":"markdown"},"source":{"626a6221":"# linear algebra\nimport numpy as np\n\n# data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas as pd \n\n# working with datetime data\nfrom datetime import datetime, timedelta \n\n# data visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n%matplotlib inline\n\n# special module to split datset:\nfrom sklearn.model_selection import train_test_split\n\n# creating and training tools \nfrom sklearn.ensemble import RandomForestRegressor\n\n# model accuracy assessment tools\nfrom sklearn import metrics \n\n# import ast to convert string representation of list to list\nimport ast\n\n# Input data files are available in the \"..\/input\/\" directory.\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","957c3fc7":"# fix random seed trying to make experimental reproducibility\nRANDOM_SEED = 42","a291b161":"# fix modules version\n!pip freeze > requirements.txt","70ad9420":"# df_train = pd.read_csv('main_task.csv')\n# df_test = pd.read_csv('kaggle_task.csv')\n# sample_submission = pd.read_csv('sample_submission.csv')\n\ndf_train = pd.read_csv('\/kaggle\/input\/sf-dst-restaurant-rating\/main_task.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/sf-dst-restaurant-rating\/kaggle_task.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/sf-dst-restaurant-rating\/sample_submission.csv')","80b04d4f":"# combining the train and the test data in one dataset\ndf_train['sample'] = 1 # train mark\ndf_test['sample'] = 0 # test mark\ndf_test['Rating'] = 0 # initial value for target\ndata = df_test.append(df_train, sort=False).reset_index(drop=True)","c9b96e7b":"data.info()","8d1e3382":"data.sample(5)","b6b7610e":"# parsing data from TripAdviser\n# from bs4 import BeautifulSoup    \n# import requests  \n\n# using sleep and random avoiding ban from Tripadvisor\n# import time\n# import random\n\n# string processing\n# import re","d68f2b11":"# creating a list with Restaurant_id and URL\n\n# data_new = data.drop(['City', 'Name', 'Cuisine Style', 'Price Range','Reviews',\n#                   'ID_TA', 'Ranking', 'Number of Reviews', 'sample', 'Rating'], axis = 1)\n\n# data_list = data_new.reset_index().values.tolist()","cf0aee5c":"# def parsingTA(data_list):\n#     '''grabbing data about restaurant from Tripadvisor'''\n#     for restaurant in data_list:\n#         url = 'https:\/\/www.tripadvisor.ru' + restaurant[2]\n#         soup = BeautifulSoup(requests.get(url, headers={'User-Agent': 'Mozilla\/5.0'}).text, 'html.parser')\n        \n#         print(restaurant[0])\n#         pattern = re.compile('(\\d+)')\n        \n#         # finding photo amount\n#         photo_count = soup.find(class_='details')\n#         if photo_count == None: \n#             restaurant.append(0)\n#         else:\n            \n#             restaurant.append(int(''.join(pattern.findall(str(photo_count)))))\n        \n#         # finding review count\n#         review_count = soup.find(class_='reviewCount')\n#         if review_count == None:\n#             restaurant.append(0)\n#         else:\n#             restaurant.append(int(''.join(pattern.findall(str(review_count)))))\n        \n#         # finding verification\n#         verification_badge = soup.find(class_='ui_icon verified-checkmark-fill restaurants-claimed-badge-ClaimedBadge__icon--JSEju')\n#         if verification_badge != None:\n#             verification = 1\n#         else:\n#             verification = 0\n#         restaurant.append(verification)\n        \n#         # finding rank\n#         rank = soup.find(class_='header_popularity popIndexValidation')\n#         if rank == None:\n#             restaurant.append(0)\n#         else:\n#             rank1 = rank.find('span')\n#             restaurant.append(int(''.join(pattern.findall(str(rank1)))))\n        \n#         # finding restaurants amount in this city \n#         restaurants_amount = soup.find(class_='header_popularity popIndexValidation')\n#         if restaurants_amount == None:\n#             restaurant.append(0)\n#         else:\n#             restaurant.append(int(''.join(pattern.findall(str(restaurants_amount.contents[1])))))\n        \n#         # finding type rates\n#         rates1 = soup.find(class_='choices')\n#         if rates1 == None:\n#             for i in range(5):\n#                     restaurant.append(0)\n#         else:\n#             rates = rates1.find_all(class_='row_num is-shown-at-tablet')\n#             if rates == []:\n#                 for i in range(5):\n#                     restaurant.append(0)\n#             else:\n#                 for rate in rates:\n#                     restaurant.append(int(''.join(pattern.findall(str(rate)))))\n          \n          # avoiding ban from Tripadvisor\n#         time.sleep(random.randint(1,10))\n        \n# parsingTA(data_list[])\n","0637ee4d":"# creating new dataframe and saving to scv\n\n# new_df = pd.DataFrame(data_list[], columns = ['ID','Restaurant_id', 'URL_TA', 'photo', 'review_count_new',\n#                                               'verification', 'rank', 'restaurants_amount', 'rate_5',\n#                                               'rate_4', 'rate_3', 'rate_2', 'rate_1'])\n# new_df.to_csv('TripAdviser_add_info.csv', index = False)","4ccf25b3":"# additional_info = pd.read_csv('TripAdviser_add_info.csv')\nadditional_info = pd.read_csv(\"..\/input\/tripadviser-add-ifo\/TripAdviser_add_info.csv\")\nadditional_info = additional_info.drop(['ID', 'Restaurant_id', 'URL_TA'], axis = 1)\n\n# concatenation additional dataset with original dataset\ndata = pd.concat([data, additional_info], axis = 1)","b01a02f6":"data['mean_rank'] = data['rank']\/data['City'].map(data.groupby(['City'])['rank'].max())","d424993c":"# DEGRADE, doesn't work\n\n# data['mean_rate_5'] = data['rate_5']\/data['City'].map(data.groupby(['City'])['rate_5'].max())\n# data['mean_rate_4'] = data['rate_4']\/data['City'].map(data.groupby(['City'])['rate_4'].max())\n# data['mean_rate_3'] = data['rate_3']\/data['City'].map(data.groupby(['City'])['rate_3'].max())\n# data['mean_rate_2'] = data['rate_2']\/data['City'].map(data.groupby(['City'])['rate_2'].max())\n# data['mean_rate_1'] = data['rate_1']\/data['City'].map(data.groupby(['City'])['rate_1'].max())","52b87300":"# # creating new column characterizing nan values in Number of Reviews\n# # DEGRADE\n\ndata['Number_of_Reviews_isNAN'] = pd.isna(data['Number of Reviews']).astype('uint8')","ff142ba3":"# filling nan values, cause of clasterization and counting mean value\n\ndata['Number of Reviews'] = data['Number of Reviews'].fillna(0)","e285d292":"# trying to normalize ranking in each city\n# clastering restraunts in each city, finding restraunt with max number of reviews and \n# normalizing number regarding max value\n\ndata['mean_number_of_reviews'] = data['Number of Reviews']\/data['City'].map(data.groupby(['City'])['Number of Reviews'].max())","67d0b8c9":"data['Price Range'].value_counts()","f81e4872":"# creating dummies from price range values\n# Degrade MAE\n\n# price_dict = {'$':'low', '$$ - $$$':'average', '$$$$':'high'}\n# data['price_range'] = data['Price Range'].map(price_dict)\n\n# data = pd.get_dummies(data, columns=[ 'price_range',], dummy_na=True)","5eb5f335":"def new_price_range(row):\n#Function returns numbers 0-3 according values from Price Range\n\n    if str(row['Price Range']) == 'nan':\n        return 0\n    elif row['Price Range'] == '$':\n        return 1\n    elif row['Price Range'] == '$$ - $$$':\n        return 2\n    elif row['Price Range'] == '$$$$':\n        return 3\n\n# Creating new Series, values are filling with function new_price_range\nprice_range = data.apply(lambda row: new_price_range(row), axis=1)\n\n# Counting new values to test revisions\n#print(price_range.value_counts())\n\n# Adding new column\ndata['price_range'] = price_range","fbb08f64":"# adding column wich shows nan value in Price Range\n\ndata['price_na'] = pd.isna(data['Price Range']).astype('uint8')\ndata['price_na'].value_counts()","07e72526":"# trying to normalize value in each city\n\ndata['mean_price_in_city'] = data['City'].map(data.groupby(['City'])['price_range'].mean())","1918d0da":"city_list = data['City'].unique()\n\n# creating list of capital cities\nCapitals = ['Paris', 'Stockholm', 'London', 'Berlin', 'Bratislava', 'Vienna', 'Rome', 'Madrid',\n            'Dublin', 'Brussels', 'Warsaw', 'Budapest', 'Copenhagen','Amsterdam', 'Lisbon', 'Prague',\n            'Oslo','Helsinki', 'Edinburgh', 'Ljubljana', 'Athens', 'Luxembourg']","41cd8404":"def new_city(row):\n    '''Function returns 1 if restaurant located in capital city '''\n    if row['City'] in Capitals:\n        return 1\n    else:\n        return 0\n\n\n# Creating new Series, values are filling with function new_city\nis_in_capital = data.apply(lambda row: new_city(row), axis=1)\n\n# Counting new values to test revisions\n#print(is_in_capital.value_counts())\n\n# Adding new column if city is capital\ndata['is_in_capital'] = is_in_capital","ce1a5ec6":"# population, mln people\npopulation = {'Paris': 2.141, 'Stockholm': 0.973, 'London': 8.9, 'Berlin': 3.748, \n              'Munich': 1.456, 'Oporto': 0.214,'Milan': 1.352,'Bratislava': 0.424, \n              'Vienna': 1.889, 'Rome': 2.873, 'Barcelona': 5.515, 'Madrid': 6.55,\n              'Dublin': 1.361,'Brussels': 0.174, 'Zurich': 0.403, 'Warsaw': 1.708, \n              'Budapest': 1.75, 'Copenhagen': 0.602,'Amsterdam': 0.822,'Lyon': 0.513, \n              'Hamburg': 1.822,'Lisbon': 0.505, 'Prague': 1.319, 'Oslo': 0.673,\n              'Helsinki': 0.632,'Edinburgh': 0.482,'Geneva': 0.495, 'Ljubljana': 0.28,\n              'Athens': 0.664, 'Luxembourg': 0.602,'Krakow': 0.769}\n\n# Adding new column - city population\ndata['Population'] = data['City'].map(population)","42c01fab":"# adding new dummy columns with cities \n\ndef find_city(cell):\n    if city == cell:\n        return 1\n    return 0\n\nfor city in city_list:\n    data[city] = data['City'].apply(find_city)","030e7b15":"# creating new column characterizing nan values in Cuisine Style \ndata['Cuisine_Style_isNAN'] = pd.isna(data['Cuisine Style']).astype('uint8')","515bf8e1":"# finding mean value of cuisines amount in one restraunt\n\ndef fill_cuisine(s):\n    '''Function returns 1 if Cuisine Style is empty or number of cuisines'''\n    if str(s) == 'nan':\n        return 1\n    else:\n        return(len(ast.literal_eval(s)))\n\n# Addind count of cuisines\ndata['cuisine_amount'] = data['Cuisine Style'].apply(fill_cuisine)","1c5b08f3":"# analyzing distribution \n\ndata['cuisine_amount'].describe()","1b30e30d":"# clastering restraunts in each city, finding restraunt with max amount of cuisines and \n# normalizing cuisines amount regarding max value\n\ndata['norm_cuisine_amount'] = data['cuisine_amount']\/data['City'].map(data.groupby(['City'])['cuisine_amount'].max())","09d24342":"# counting amount of restraunts with each cuisine type\n\ndef counting_keys(dictionary, key_word):\n    '''if key_word is in dictionary, functions increment value by 1,\n    in other case function create new item in dictionary '''\n    if key_word in dictionary.keys():\n        dictionary[key_word] += 1\n    else:\n        dictionary[key_word] = 1\n\ncuisines_amount = {}\n\n# filling values\nfor row in data['Cuisine Style']:\n    if str(row) != 'nan':\n        cuisine_list = ast.literal_eval(row)\n        for cuisine in cuisine_list:\n            counting_keys(cuisines_amount,cuisine)\n","0bade3ea":"# analyzing popular and rare cuisines\n# creating dataframe from dictionary cuisines_amount\n\ndf_cuisines_amount = pd.DataFrame.from_dict(cuisines_amount, orient='index', columns = ['count'])\n\n#df_cuisines_amount.sort_values(by = 'count',ascending = False)","4c1081d4":"df_cuisines_amount.describe()","d46b0da2":"# list of rare cuisines\n\ndf_rare_cuisines = df_cuisines_amount[df_cuisines_amount['count']<10].sort_values(by = 'count',ascending = False)\n#df_rare_cuisines","86ce00ef":"# creating new columns wich characterized rare cuisins\ndef find_rare_cuisine(cell):\n    if str(cell) == 'nan':\n        return 0\n    if item in ast.literal_eval(cell):\n        return 1\n    return 0\n\nfor item in df_rare_cuisines.index:\n    data[item] = data['Cuisine Style'].apply(find_rare_cuisine)","a54c52de":"# list of popular cuisines\ndf_popular_cuisine = df_cuisines_amount[df_cuisines_amount['count']>1000].sort_values(by = 'count',ascending = True)\n#df_popular_cuisine","0f21de1e":"# creating new columns wich characterized poppular cuisins\n\ndef find_popular_cuisine(cell):\n    if str(cell) == 'nan':\n        return 0\n    if item in ast.literal_eval(cell):\n        return 1\n    return 0\n\nfor item in df_popular_cuisine.index:\n    data[item] = data['Cuisine Style'].apply(find_popular_cuisine)","20e67c4c":"# filling nan values \ndata['Reviews'] = data['Reviews'].fillna('[[], []]')","06a44014":"# finding the newest review of reviews in restraunts dataframe\ndef fill_last_review(row):\n    '''replacing nan values from text reviews cause of ast.literal_eval error\n       Function returns 01-01-1900 if reviews are empty\n       Function returns one date if it is only one review\n       Function returns the biggest date if there is two reviews'''\n    \n    str_review = ast.literal_eval(str(row['Reviews']).replace('nan','0'))\n    if str_review == [[], []]:\n        return 'NaN'\n    elif len(str_review[1]) == 1:\n        return(datetime.strptime(str_review[1][0],'%m\/%d\/%Y'))\n    else:\n        first_review_time = datetime.strptime(str_review[1][0],'%m\/%d\/%Y')\n        second_review_time = datetime.strptime(str_review[1][1],'%m\/%d\/%Y')\n        #print(row['Restaurant_id'])\n        if first_review_time < second_review_time:\n            return second_review_time\n        else:\n            return first_review_time\n\nlast_review = data.apply(lambda row:fill_last_review(row), axis=1)\n","8308cb29":"# finding the eldest review of reviews in restraunts dataframe\n\ndef fill_old_review(row):\n    '''replacing nan values from text reviews cause of ast.literal_eval error\n       Function returns 01-01-1900 if reviews are empty\n       Function returns one date if it is only one review\n       Function returns the earliest date if there is two reviews'''\n    \n    str_review = ast.literal_eval(str(row['Reviews'].replace('nan','0')))\n    if str_review == [[], []]:\n        return 'NaN'\n    elif len(str_review[1]) == 1:\n        return(datetime.strptime(str_review[1][0],'%m\/%d\/%Y'))\n    else:\n        first_review_time = datetime.strptime(str_review[1][0],'%m\/%d\/%Y')\n        second_review_time = datetime.strptime(str_review[1][1],'%m\/%d\/%Y')\n        #print(row['Restaurant_id'])\n        if first_review_time < second_review_time:\n            return first_review_time\n        else:\n            return second_review_time\n\n        \nold_review = data.apply(lambda row:fill_old_review(row), axis=1)","53328170":"# Adding difference between two reviews\ndata['difference'] = (last_review - old_review).dt.days","8c7d7973":"# adding difference between current day and last review\ndata['passed_time'] = (datetime.today()- last_review).dt.days","54fd0f07":"# adding columns with nan values in 'difference' and 'passed_time' columns\n\ndata['difference_isNAN'] = pd.isna(data['difference']).astype('uint8')\ndata['passed_time_isNAN'] = pd.isna(data['passed_time']).astype('uint8')","06bca9a1":"data['Ranking'].describe()","71fa20f5":"# this normalization degrade MAE\n# data['Ranking'] = data['Ranking']\/data['Ranking'].mean()","04cf0fb7":"# trying to normalize ranking in each city\n# clastering restraunts in each city, finding max rated restraunts and normalizing ranks regarding max ranking\n\ndata['mean_ranking'] = data['Ranking']\/data['City'].map(data.groupby(['City'])['Ranking'].max())\n\n#data['mean_ranking'].value_counts()","e5aea496":"# mining int numbers from 'ID_TA' and 'URL_TA', good correlation with ranking\n\ndata['ID_int'] = data['ID_TA'].apply(lambda s: int(s[1:]))\ndata['URL_int'] = data['URL_TA'].str.split('-').apply(lambda s: int(s[1][1:]))","8765eeb7":"# correlation heatmap\n# uninformative due to the large number of columns\n\nplt.rcParams['figure.figsize'] = (15,10)\nsns.heatmap(data.drop(['sample'], axis=1).corr(),)","429df939":"data.columns","f4905596":"# dropping object columns\n\ndata = data.drop(['City', 'Cuisine Style', 'Price Range','Reviews',\n                  'URL_TA', 'ID_TA', 'Restaurant_id'], axis = 1)","7ee7bf9d":"# filling nan values in 'difference' and 'passed_time'\n\ndata['difference'] = data['difference'].fillna(0)\ndata['passed_time'] = data['passed_time'].fillna(0)\n","c656646e":"# selecting the test part\ntrain_data = data.query('sample == 1').drop(['sample'], axis=1)\n\ny = train_data.Rating.values            # target\nX = train_data.drop(['Rating'], axis=1)\n\n# using train_test_split for splitting test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)","ebdb23f7":"# checking size\ntrain_data.shape, X.shape, X_train.shape, X_test.shape","156e24c0":"# creating model, setting are not changed \nmodel = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)","0f28096d":"# fitting the model on test dataset\nmodel.fit(X_train, y_train)\n\n# using the trained model to predict the rating of restaurants in a test dataset.\n# The predicted values are saved to the variable y_pred\ny_pred = model.predict(X_test)","3acc9685":"# analyzing the result of prediction\ny_pred","a6b84022":"# rounding predict with pitch 0.5\n\ny_pred_round = []\nfor item in y_pred:\n    y_pred_round.append(round(item\/0.5)*0.5)\ny_pred_round_array = np.asarray(y_pred_round)","bb048811":"y_pred_round_array","a453b7ef":"# Compare the predicted values (y_pred) with real (y_test), and see how much they differ on average\n# Mean Absolute Error (MAE) shows the average deviation of the predicted values from the real ones.\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred_round_array))","3890bf70":"# displaying the most important features for the model\nplt.rcParams['figure.figsize'] = (10,15)\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')","c4618c78":"# # creating submission file \n\n# test_data = data.query('sample == 0').drop(['sample'], axis=1)\n# test_data = test_data.drop(['Rating'], axis=1)\n\n# predict_submission = model.predict(test_data)\n\n# # rounding predict with pitch 0.5\n# predict_submission_round = []\n# for item in predict_submission:\n#     predict_submission_round.append(round(item\/0.5)*0.5)\n# predict_submission_round_array = np.asarray(predict_submission_round)\n\n# sample_submission['Rating'] = predict_submission_round_array\n# sample_submission.to_csv('submission.csv', index=False)\n# sample_submission.head()","ebd282af":"\n# review_good_words = ['good','unique','delicious','best','amazing','excellent','nice','clean','lovely','relaxed','great',\n#                      'heavenly','liked','fantastic','tasty','fresh','relaxing','perfect','hillarious','loved',\n#                      'outstanding','favourite','not bad']\n# review_bad_words = ['wasting','boring','avoid','overpriced','average','disappointing','standard','terrible','expensive',\n#                    'shame','rude','slow','horrible','catastrophy','worst','aggressive','dirty','very bad','nothing']\n\n# def fill_good_reviews(row):\n#     str_review = ast.literal_eval(row.replace('nan','0'))\n#     if len(str_review[0]) == 0:\n#         return 0\n#     elif len(str_review[0]) > 0:\n#         for word in review_good_words:\n#             if str(str_review[0][0]).find(word) != -1:\n#                 return 1\n#     elif len(str_review[0]) == 2:\n#         for word in review_good_words:\n#             if str(str_review[0][1]).find(word) != -1:\n#                 return 1\n#     return 0\n            \n# df['good_reviews'] = df['Reviews'].apply(fill_good_reviews)","c02b81d7":"# # empty set of unique cuisine\n# unique_cuisines = set()\n\n# # filling set of unique cuisine\n# for row in data['Cuisine Style']:\n#     if str(row) != 'nan':\n#         cuisine_list = ast.literal_eval(row)\n#         for cuisine in cuisine_list:\n#             unique_cuisines.add(cuisine)\n\n# # how many unique cuisines we have in dataset\n# print(len(unique_cuisines))\n\n\n# # counting amount of restraunts with each cuisine type\n# cuisines_amount = {}\n\n# # creating keys\n# for cuisine in unique_cuisines:\n#     cuisines_amount[cuisine] = 0\n\n# # filling values\n# for row in data['Cuisine Style']:\n#     if str(row) != 'nan':\n#         cuisine_list = ast.literal_eval(row)\n#         for cuisine in cuisine_list:\n#             cuisines_amount[cuisine] += 1\n\n# #cuisines_amount\n\n# # finding the most popular cuisine \n# max_count = 0\n# cuisine = ''\n\n# for key, value in cuisines_amount.items():\n#     if value > max_count:\n#         cuisine = key\n#         max_count = value\n\n# print(cuisine, max_count)\n\n# # finding the most popular cuisine another solution\n\n# cuisine = max(cuisines_amount, key = cuisines_amount.get)\n\n# print(cuisine,cuisines_amount[cuisine])\n\n# # finding rare cuisines\n\n# rare_cuisines = []\n\n# for key, value in cuisines_amount.items():\n#     if value == 1:\n#         rare_cuisines.append(key)\n\n# print(rare_cuisines)","aee4d359":"# creating columns reviews_na, difference bentween publication of to reviews and passed_time from latest review\n# creating new dataframe using only one reading cycle\n# DEGRADE\n\n\n\n# #new dataframe collecting info\n# df_reviews = pd.DataFrame(columns = ['reviews_na', 'difference', 'passed_time'])\n\n# i = 0 # index counter\n\n# for index, row in data.iterrows():\n#     # initializing values\n#     reviews_na = 0\n#     difference = np.nan\n#     passed_time = np.nan\n\n\n#     str_review = ast.literal_eval(str(row['Reviews']).replace('nan','0'))\n#     if str_review == [[], []]:\n#     # this means that review is empty\n#         reviews_na = 1\n\n#     elif len(str_review[1]) == 1:\n#     # there is only one review, we will calculate passed_time\n#         passed_time = (datetime.today() - datetime.strptime(str_review[1][0],'%m\/%d\/%Y')).days\n\n#     else:\n#     # there are two reviews, we will calculate passed_time and difference\n#         first_review_time = datetime.strptime(str_review[1][0],'%m\/%d\/%Y')\n#         second_review_time = datetime.strptime(str_review[1][1],'%m\/%d\/%Y')\n#         if first_review_time < second_review_time:\n#             difference = (second_review_time - first_review_time).days\n#             passed_time = (datetime.today() - second_review_time).days\n\n#         else:\n#             difference = (first_review_time - second_review_time).days\n#             passed_time = (datetime.today() - first_review_time).days\n\n#     df_reviews.loc[i] = [reviews_na, difference, passed_time]\n#     i += 1\n\n\n# data = pd.concat([data, df_reviews], axis=1)\n","f8217450":"# # searching the newest review date in dataset\n# newest_review = datetime.strptime('01\/01\/1900','%m\/%d\/%Y')\n# for item in last_review:\n#     if item > newest_review:\n#         newest_review = item\n# print(newest_review.strftime('%Y-%m-%d'))","5e957d0b":"# # searching the eldest review date in dataset\n# eldest_review = newest_review\n# for item in old_review:\n#     if item < eldest_review and item != datetime.strptime('01\/01\/1900','%m\/%d\/%Y'):\n#         eldest_review = item\n# print(eldest_review.strftime('%Y-%m-%d'))","8567365e":"# Parsing additional info from TripAdvisor","46ab9350":"# Submission\n","adbc3833":"# Processing additional info","beb4321c":"Finding newest_review in dataset","1bbf9dd1":"# Parts that doesn't work","11fbbdf6":"# Processing reviews","62208fab":"# EDA","219c65ac":"MAE: 0.13675","9657e46c":"Trying to normalize new columns in each city\n\nClastering restraunts in each city, finding max value in each column and normalizing regarding max value","66bcdf57":"# Processing Number of Reviews","bc450d68":"# Processing price range values","74761e97":"Finding eldest_review in dataset","19c60650":"# Analyzing ranking","b2047fa1":"# Deleting columns","8a35bf5a":"# Processing cuisine types","baa5f5cb":"# Importing modules","10637a2b":"# Processing city values","d666a170":"This part is commented cause of long processing time\nResults are downloaded to a separate dataset TripAdviser_add_info.csv","f45df655":"# Importing datasets","7650f526":"# Analyzing ID and URL","5878b041":"Calculations according to realDS tasks","e4c8b403":"Analizing words and add 2 new columns characterizing existing good or bad words in Reviews","518c95a3":"The initial version of the dataset consists of ten columns containing the following information:\n\n* Restaurant_id \u2014 restaurant identification number;\n* City \u2014 city where the restaurant is located;\n* Cuisine Style \u2014 style or styles that include dishes offered in the restaurant;\n* Ranking \u2014 the place that this restaurant occupies among all the restaurants in this city;\n* Rating \u2014 restaurant rating according to TripAdvisor (target value - this is the value that the model will have to predict);\n* Price Range \u2014 restaurant price range;\n* Number of Reviews \u2014 number of restaurant reviews;\n* Reviews \u2014 two reviews that are displayed on the restaurant website;\n* URL_TA \u2014 Restaurant page on TripAdvisor;\n* ID_TA \u2014 restaurant identifier in TripAdvisor database.\n\n","fdc4ae7a":"# Creating, fitting and testing","0b50ab8b":"Loading additional info from dataset. New columns:\n* photo - amount of loaded photo;\n* review_count_new - actual number of restaurant reviews;\n* verification - page verification badge;\n* rank - actual place that this restaurant occupies among all the restaurants in this city;\n* restaurants_amount - restaurants amount in this city;\n* rate_5 - number of travel rating \"Excellent\";\n* rate_4 - number of travel rating \"Very good\";\n* rate_3 - number of travel rating \"Average\";\n* rate_2 - number of travel rating \"Poor\";\n* rate_1 - number of travel rating \"Terrible\"."}}