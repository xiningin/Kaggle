{"cell_type":{"d847e6d6":"code","cd41bf20":"code","360920cf":"code","beeb9690":"code","bf54922b":"code","e134b1ed":"code","0e6dbef2":"code","c8dc1e95":"code","966f9a18":"code","a7ae75d0":"code","ef43fa81":"code","4c695ab7":"code","252d8dee":"code","c647c663":"code","55ab6535":"code","bd2a910c":"code","0e46d7ed":"code","daff45d8":"code","5397818f":"code","0e18f268":"code","6e58f1d1":"code","d2925415":"code","d99b98ef":"code","dae70f8c":"code","7fabd0be":"code","9e187c38":"code","9c2d38ca":"markdown","77c327a3":"markdown"},"source":{"d847e6d6":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport skimage.io\nimport os.path\nimport tqdm\nimport glob\nimport tensorflow as tf\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom sklearn.utils import shuffle\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.io import imread, imshow\nfrom skimage.transform import resize\nfrom skimage.color import grey2rgb\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import InputLayer, BatchNormalization, Dropout, Flatten, Dense, Activation, MaxPool2D, Conv2D\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.utils import to_categorical\nfrom keras import optimizers\nfrom tensorflow.keras.optimizers import Adam\n\nfrom keras.callbacks import Callback,ModelCheckpoint,ReduceLROnPlateau\nfrom keras.models import Sequential,load_model\nfrom keras.layers import Dense, Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\nimport keras.backend as K\n\n#import tensorflow_addons as tfa\n#from tensorflow.keras.metrics import Metric\n#from tensorflow_addons.utils.types import AcceptableDTypes, FloatTensorLike\nfrom typeguard import typechecked\nfrom typing import Optional","cd41bf20":"root_dir  =  Path('..\/input\/a-large-scale-fish-dataset\/Fish_Dataset\/Fish_Dataset')","360920cf":"root_dir","beeb9690":"# Get filepaths and Class\nfilepaths = list(root_dir.glob(r'**\/*.png'))\nlabels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n\nfilepaths = pd.Series(filepaths, name='Filepath').astype(str)\nlabels = pd.Series(labels, name='Class')","bf54922b":"filepaths","e134b1ed":"labels","0e6dbef2":"# Concatenate filepaths and labels\nimage_df = pd.concat([filepaths, labels], axis=1)\n\n# Drop GT images\nimage_df['Class'] = image_df['Class'].apply(lambda x: np.NaN if x[-2:] == 'GT' else x)\nimage_df = image_df.dropna(axis=0)","c8dc1e95":"# Sample 200 images from each class\nsamples = []\n\nfor category in image_df['Class'].unique():\n    category_slice = image_df.query(\"Class == @category\")\n    samples.append(category_slice.sample(200, random_state=1))\n\nimage_df = pd.concat(samples, axis=0).sample(frac=1.0, random_state=1).reset_index(drop=True)","966f9a18":"image_df","a7ae75d0":"x_train, x_test = train_test_split(image_df, test_size=0.2, shuffle=True, random_state=1)","ef43fa81":"train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n    validation_split=0.2\n)\n\ntest_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n)","4c695ab7":"train_images = train_generator.flow_from_dataframe(\n    dataframe=x_train,\n    x_col='Filepath',\n    y_col='Class',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=42,\n    subset='training'\n)\n","252d8dee":"val_images = train_generator.flow_from_dataframe(\n    dataframe=x_train,\n    x_col='Filepath',\n    y_col='Class',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=42,\n    subset='validation'\n)\n\n","c647c663":"test_images = test_generator.flow_from_dataframe(\n    dataframe=x_test,\n    x_col='Filepath',\n    y_col='Class',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=False\n)","55ab6535":"base_model = tf.keras.applications.NASNetMobile(input_shape=(224,224,3),include_top=False,weights=\"imagenet\")","bd2a910c":"# Freezing Layers\n\nfor layer in base_model.layers[:-4]:\n    layer.trainable=False","0e46d7ed":"# Building Model\n\nmodel=Sequential()\nmodel.add(base_model)\nmodel.add(Dropout(0.5))\nmodel.add(Flatten())\nmodel.add(BatchNormalization())\nmodel.add(Dense(32,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dense(9,activation='softmax'))","daff45d8":"# Model Summary\n\nmodel.summary()","5397818f":"from tensorflow.keras.utils import plot_model\nfrom IPython.display import Image\nplot_model(model, to_file='convnet.png', show_shapes=True,show_layer_names=True)\nImage(filename='convnet.png') ","0e18f268":"def f1_score(y_true, y_pred): #taken from old keras source code\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)\/(precision+recall+K.epsilon())\n    return f1_val","6e58f1d1":"METRICS = [\n      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n      tf.keras.metrics.Precision(name='precision'),\n      tf.keras.metrics.Recall(name='recall'),  \n      tf.keras.metrics.AUC(name='auc'),\n        f1_score,\n]","d2925415":"lrd = ReduceLROnPlateau(monitor = 'val_loss',patience = 2,verbose = 1,factor = 0.50, min_lr = 1e-10)\n\nmcp = ModelCheckpoint('model.h5')\n\nes = EarlyStopping(verbose=1, patience=2)","d99b98ef":"model.compile(optimizer='Adam', loss='categorical_crossentropy',metrics=METRICS)","dae70f8c":"%time\nhistory=model.fit(train_images,validation_data=val_images,epochs=15,verbose = 1,callbacks=[lrd,mcp,es])","7fabd0be":"#%% PLOTTING RESULTS (Train vs Validation FOLDER 1)\n\ndef Train_Val_Plot(acc,val_acc,loss,val_loss,auc,val_auc,precision,val_precision,f1,val_f1):\n    \n    fig, (ax1, ax2,ax3,ax4,ax5) = plt.subplots(1,5, figsize= (20,5))\n    fig.suptitle(\" MODEL'S METRICS VISUALIZATION \")\n\n    ax1.plot(range(1, len(acc) + 1), acc)\n    ax1.plot(range(1, len(val_acc) + 1), val_acc)\n    ax1.set_title('History of Accuracy')\n    ax1.set_xlabel('Epochs')\n    ax1.set_ylabel('Accuracy')\n    ax1.legend(['training', 'validation'])\n\n\n    ax2.plot(range(1, len(loss) + 1), loss)\n    ax2.plot(range(1, len(val_loss) + 1), val_loss)\n    ax2.set_title('History of Loss')\n    ax2.set_xlabel('Epochs')\n    ax2.set_ylabel('Loss')\n    ax2.legend(['training', 'validation'])\n    \n    ax3.plot(range(1, len(auc) + 1), auc)\n    ax3.plot(range(1, len(val_auc) + 1), val_auc)\n    ax3.set_title('History of AUC')\n    ax3.set_xlabel('Epochs')\n    ax3.set_ylabel('AUC')\n    ax3.legend(['training', 'validation'])\n    \n    ax4.plot(range(1, len(precision) + 1), precision)\n    ax4.plot(range(1, len(val_precision) + 1), val_precision)\n    ax4.set_title('History of Precision')\n    ax4.set_xlabel('Epochs')\n    ax4.set_ylabel('Precision')\n    ax4.legend(['training', 'validation'])\n    \n    ax5.plot(range(1, len(f1) + 1), f1)\n    ax5.plot(range(1, len(val_f1) + 1), val_f1)\n    ax5.set_title('History of F1-score')\n    ax5.set_xlabel('Epochs')\n    ax5.set_ylabel('F1 score')\n    ax5.legend(['training', 'validation'])\n\n\n    plt.show()\n    \n\nTrain_Val_Plot(history.history['accuracy'],history.history['val_accuracy'],\n               history.history['loss'],history.history['val_loss'],\n               history.history['auc'],history.history['val_auc'],\n               history.history['precision'],history.history['val_precision'],\n               history.history['f1_score'],history.history['val_f1_score']\n              )\n","9e187c38":"model.evaluate(test_images)","9c2d38ca":"### NASNet-Mobile \nNASNet-Mobile is a convolutional neural network that is trained on more than a million images from the ImageNet database . The network can classify images into 1000 object categories, such as keyboard, mouse, pencil, and many animals. ... The network has an image input size of 224-by-224\n\n![](https:\/\/i.stack.imgur.com\/h9TXi.jpg)\n\n\n\n\n\n### Description of the dataset\n\nThe dataset contains 9 different seafood types. For each class, there are 1000 augmented images and their pair-wise augmented ground truths.\nEach class can be found in the \"Fish_Dataset\" file with their ground truth labels. All images for each class are ordered from \"00000.png\" to \"01000.png\".\n\nFor example, if you want to access the ground truth images of the shrimp in the dataset, the order should be followed is \"Fish->Shrimp->Shrimp GT\".\n\n\n### Dataset \n\n[Link](https:\/\/www.kaggle.com\/crowww\/a-large-scale-fish-dataset\/code)\n","77c327a3":"### If you want help plz contact me\n\nYasserhesseinshakir@yahoo.com\n\nhttps:\/\/www.linkedin.com\/in\/yasir-hussein-314a65201\/\n\nhttps:\/\/www.kaggle.com\/yasserhessein\n\nRefeneces\n\nhttps:\/\/keras.io\/"}}