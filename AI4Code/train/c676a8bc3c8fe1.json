{"cell_type":{"2c5e2b9a":"code","f2e7a99d":"code","bf2e75a0":"code","d9bfb37d":"code","cd92ff18":"code","8b77ef72":"code","57d08381":"code","60aa37c0":"code","a0cfd58b":"code","d3b7d2a4":"code","866581b6":"code","76916aaf":"code","76bdb72e":"code","4dfc0db0":"code","a1e87bd7":"code","80f576e5":"code","bb71980c":"code","7056cea7":"code","c3e043d1":"code","eada7280":"code","26549ab3":"code","5a4a7f94":"code","879d5671":"code","73ea2513":"code","5e04ac79":"code","fa741f04":"code","3b55fef6":"code","33ad2fa8":"code","2484d1a3":"code","62f7e8e1":"code","f8bbbb28":"code","96834f20":"code","8853a508":"code","6b36c33f":"code","4ef940b9":"code","5fc371bc":"code","ebb767e2":"code","fc3dfb70":"code","781f7cb9":"code","636570ed":"code","be341ad9":"code","1bf387cf":"code","738c9c49":"code","d6db2a99":"code","ee684305":"code","98cee193":"code","e7f75fec":"code","ca923d72":"code","42ce9cce":"code","2324e9c6":"code","e0fab3fc":"code","03483960":"code","9d28d261":"code","bcef79fd":"code","b8a82c85":"code","97435d17":"code","e91a2954":"code","fecfdf10":"code","a1da53ce":"code","d82895a6":"code","92268b60":"code","85a98e0f":"code","1a70e73c":"code","788ea83c":"markdown","cebaf1cd":"markdown","894d1196":"markdown","5a746c8b":"markdown","009645d6":"markdown","9d51c030":"markdown","cbcd6f15":"markdown","e6018568":"markdown","925d158a":"markdown","e2e04a02":"markdown","aebab60e":"markdown","d409df0a":"markdown","136e32fd":"markdown","edc63467":"markdown","a1eae6ea":"markdown","189ccca7":"markdown","5405902e":"markdown","ad4071f6":"markdown","0e4f5ee1":"markdown","88421c87":"markdown","8a8f037f":"markdown","4c0cd770":"markdown","49e2c68f":"markdown","90d6a4db":"markdown","0e2ae2fd":"markdown","deabf53b":"markdown","2c07f739":"markdown","46f77fe5":"markdown","00c24ce3":"markdown","4f99c041":"markdown","7bcb159c":"markdown","a1511011":"markdown","28818c9c":"markdown","3a3701c4":"markdown","dd5301b0":"markdown","b3afea1b":"markdown","e123c315":"markdown","93ebc368":"markdown","074755c6":"markdown","412b0311":"markdown","cc7e2379":"markdown","a17b75cc":"markdown","ddb34dc7":"markdown"},"source":{"2c5e2b9a":"import os\nprint(os.listdir(\"..\/input\"))","f2e7a99d":"import sqlite3\nimport pandas as pd\n\n# Loading Data\n# using SQLite Table to read data.\ncon = sqlite3.connect('..\/input\/database.sqlite')\ndf = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score != 3\"\"\" , con) \n\n# printing head of the data\ndf.head(2)","bf2e75a0":"#shape of dataframe\ndf.shape","d9bfb37d":"#Name of the columns\ndf.columns","cd92ff18":"df.info()","8b77ef72":"# finding total number of unique product.\nprint(\"Number of unique products:\", df.ProductId.nunique())","57d08381":"#finding total number of unique users who provided review.\nprint(\"Number of unique users who provided reviews: \", df.UserId.nunique())","60aa37c0":"from datetime import datetime\n\n# Getting the first date for review in dataset.\nprint(\"First date of the review in the dataset: \", datetime.fromtimestamp(df.Time.min()))\n\n# Getting the last date for review in dataset.\nprint(\"Last date of the review in the dataset:\", datetime.fromtimestamp(df.Time.max()))\n\n#calculating time span for which review has been provided.\nprint(\"Timespan for which reviews has been provided:{} years \".format(int((datetime.fromtimestamp(df.Time.max())- datetime.fromtimestamp(df.Time.min())).days\/365)))\n","a0cfd58b":"#Distribution of rating\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"darkgrid\")\n\nx = (df['Score'].value_counts())*(100\/len(df))\nx = x.to_frame().reset_index()\nax = sns.barplot(x = 'index', y ='Score', data = x)\nax.set(xlabel='Score', ylabel='Percentage')\nplt.show()","d3b7d2a4":"print(\"Distribution found as below.\")\nfor i,perc in enumerate(x['Score']):\n    print(\"Score {} :{}%\".format(x['index'][i], round(perc,2)))","866581b6":"#Map review as positive or negative based on score.\ndef partition(score):\n    \"\"\"\n    This function is used to segregate reviews as positive and negative based on score.\n    Input: integer.\n    Output: string.\n    \n    \"\"\"\n    \n    if score<3:\n        return \"negative\"\n    return \"positive\"\n\nactualScore= df['Score']\nfinal = df\nfinal['Score']=actualScore.map(partition)\nfinal.head(2)","76916aaf":"#Converting reviews summary and text to lowercase.\nfinal['Summary'] = final['Summary'].map(str.lower)\nfinal['Text'] = final['Text'].map(str.lower)\nfinal.head(2)","76bdb72e":"import datetime\n\n#convering Time from unix timestamp format to YYYY-MM-DD HH:MM:SS format\nfinal[\"Time\"] = final[\"Time\"].map(lambda t: datetime.datetime.fromtimestamp(int(t)).strftime('%Y-%m-%d %H:%M:%S'))\nfinal.head(2)","4dfc0db0":"result= pd.read_sql_query(\"\"\"\nSELECT *\nFROM Reviews\nWHERE Score != 3 AND UserId=\"AR5J8UI46CURR\"\nORDER BY ProductID\n\"\"\", con)\nresult","a1e87bd7":"#Sorting data according to ProductId in ascending order\nfinal=final.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n\n#Deduplication of reviews\nfinal=final.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\nfinal.shape","80f576e5":"#Checking to see how much % of data still remains\n(final['Id'].size*1.0)\/(df['Id'].size*1.0)*100","bb71980c":"result= pd.read_sql_query(\"\"\"\nSELECT *\nFROM Reviews\nWHERE Score != 3 AND HelpfulnessNumerator > HelpfulnessDenominator\nORDER BY ProductID\n\"\"\", con)\nresult","7056cea7":"final=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]","c3e043d1":"#Checking to see how much % of data still remains\n(final['Id'].size*1.0)\/(df['Id'].size*1.0)*100","eada7280":"final_positive = final[final.Score == 'positive']\nfinal_positive = final_positive.sample(frac=0.035,random_state=1) #0.055","26549ab3":"final_negative = final[final.Score == 'negative']\nfinal_negative = final_negative.sample(frac=0.15,random_state=1) #0.25","5a4a7f94":"final_df = pd.concat([final_positive,final_negative],axis=0)","879d5671":"#Sorting data into ascending order of time\nfinal_df.sort_values('Time', axis=0, ascending=True, inplace=True, kind='quicksort', na_position='last')","73ea2513":"ax = sns.countplot(x = 'Score', data = final_df)\nax.set(xlabel='Polarity', ylabel='# of Reviews')\nplt.show()","5e04ac79":" final_df['Score'].value_counts()","fa741f04":"#Removing Stopwords, punctuations and Snowball Stemming","3b55fef6":"#importing library\nimport re\nfrom tqdm import tqdm\nfrom bs4 import BeautifulSoup","33ad2fa8":"#Creating set of stopwords\n\n# https:\/\/gist.github.com\/sebleier\/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\n\nstopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"])","2484d1a3":"# https:\/\/stackoverflow.com\/a\/47091490\/4084039\nimport re\n\ndef decontracted(phrase):\n    # fucntion to convert phrase into words\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","62f7e8e1":"#function to preprocess text\ndef preprocess_text(sentance):\n    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    sentance = decontracted(sentance)\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n    return sentance.strip()","f8bbbb28":"final_reviews = []\nfor sentance in final_df['Text'].values:\n    final_reviews.append(preprocess_text(sentance))","96834f20":"#checking random reviews.\nfinal_reviews[1290]","8853a508":"from sklearn.model_selection import cross_val_score\nimport numpy as np\n# 1: Hyper parameter Tuning (YOU CAN FOLLOW ANY ONE OF THESE)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\n#import pdb\n\ndef find_best_k(X_train, y_train,X_cv, max_k):\n    train_auc = []\n    cv_auc = []\n    k_neighbour = np.arange(1,max_k,2)\n    for i in k_neighbour:\n        neigh = KNeighborsClassifier(n_neighbors=i)\n        neigh.fit(X_train, y_train)\n        # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n        # not the predicted outputs\n        y_train_pred =  neigh.predict_proba(X_train)[:,1]\n        y_cv_pred =  neigh.predict_proba(X_cv)[:,1]\n\n        train_auc.append(roc_auc_score(y_train,y_train_pred))\n        cv_auc.append(roc_auc_score(y_cv, y_cv_pred))\n    \n    sns.set(style=\"darkgrid\")\n    plt.figure(figsize=(10,6))\n    plt.plot(k_neighbour, train_auc, label='Train AUC')\n    plt.scatter(k_neighbour, train_auc, label='Train AUC')\n    plt.plot(k_neighbour, cv_auc, label='CV AUC')\n    plt.scatter(k_neighbour, cv_auc, label='CV AUC')\n    plt.legend()\n    plt.xlabel(\"K: hyperparameter\")\n    plt.ylabel(\"AUC\")\n    plt.title(\"ERROR PLOTS\")\n    plt.show()\n","6b36c33f":"#seperating feature and out parameter\nX = np.array(final_reviews)\nY = final_df['Score']","4ef940b9":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, shuffle=False)# this is for time series split\nX_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.33, shuffle=False) # this is random splitting\n\nprint(\"Shape of train data set:\",X_train.shape, y_train.shape)\nprint(\"Shape of validation data set:\",X_cv.shape, y_cv.shape)\nprint(\"Shape of test data set:\", X_test.shape, y_test.shape)","5fc371bc":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\ncv.fit(X_train) # fit has to happen only on train data","ebb767e2":"# we use the fitted CountVectorizer to convert the text to vector\nX_train_bow = cv.transform(X_train)\nX_cv_bow = cv.transform(X_cv)\nX_test_bow = cv.transform(X_test)\n\nprint(\"After vectorizations\")\nprint(X_train_bow.shape, y_train.shape)\nprint(X_cv_bow.shape, y_cv.shape)\nprint(X_test_bow.shape, y_test.shape)","fc3dfb70":"#plot to find best k value\nfind_best_k(X_train_bow, y_train,X_cv_bow, 40)","781f7cb9":"import scikitplot.metrics as skplt\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n\n\ndef get_testing_result(X_train, X_test, best_k):\n    neigh = KNeighborsClassifier(n_neighbors=best_k)\n    neigh.fit(X_train, y_train)\n    \n    \n    skplt.plot_confusion_matrix(y_train ,neigh.predict(X_train))\n    plt.show()\n    print(\"=\"*50)\n    \n    skplt.plot_confusion_matrix(y_test ,neigh.predict(X_test))\n    plt.show()\n    print(\"=\"*50)\n    print(\"Classification report\")\n    print(classification_report(y_test ,neigh.predict(X_test)))\n    print(\"=\"*50)\n    \n    print(\"Accuracy:{}\".format(round(accuracy_score(y_test ,neigh.predict(X_test)),3)))","636570ed":"get_testing_result(X_train_bow, X_test_bow,15)","be341ad9":"cv = CountVectorizer(ngram_range=(1,2), min_df=15)\ncv.fit(X_train) # fit has to happen only on train data\n# we use the fitted CountVectorizer to convert the text to vector\nX_train_gram = cv.transform(X_train)\nX_cv_gram = cv.transform(X_cv)\nX_test_gram = cv.transform(X_test)\n\nprint(\"After vectorizations\")\nprint(X_train_gram.shape, y_train.shape)\nprint(X_cv_gram.shape, y_cv.shape)\nprint(X_test_gram.shape, y_test.shape)","1bf387cf":"find_best_k(X_train_gram, y_train,X_cv_gram, 40)","738c9c49":"get_testing_result(X_train_gram, X_test_gram,3)","d6db2a99":"from sklearn.feature_extraction.text import TfidfVectorizer\ncv = TfidfVectorizer(ngram_range=(1,2), min_df=15)\ncv.fit(X_train) # fit has to happen only on train data\n# we use the fitted CountVectorizer to convert the text to vector\nX_train_tfidf = cv.transform(X_train)\nX_cv_tfidf = cv.transform(X_cv)\nX_test_tfidf = cv.transform(X_test)\n\nprint(\"After vectorizations\")\nprint(X_train_tfidf.shape, y_train.shape)\nprint(X_cv_tfidf.shape, y_cv.shape)\nprint(X_test_tfidf.shape, y_test.shape)","ee684305":"find_best_k(X_train_tfidf, y_train,X_cv_tfidf, 40)","98cee193":"get_testing_result(X_train_tfidf, X_test_tfidf,5)","e7f75fec":"i=0\nlst_sentance_train=[]\nfor sentance in X_train:\n    lst_sentance_train.append(sentance.split())\nprint(lst_sentance_train[0])","ca923d72":"#Training W2V model\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\n\n\n# this line of code trains your w2v model on the give list of sentances\nw2v_model=Word2Vec(lst_sentance_train,min_count=5,size=50, workers=4)\n\n\nw2v_words = list(w2v_model.wv.vocab)\n","42ce9cce":"w2v_words = list(w2v_model.wv.vocab)\nprint(\"# of words that occured minimum 5 times \",len(w2v_words))","2324e9c6":"import numpy as np\ndef text_to_vec(X):\n    \n    lst_sentance=[]\n    for sentance in X:\n        lst_sentance.append(sentance.split())\n    print(lst_sentance[0])\n    \n    sent_vectors = []\n    for sent in tqdm(lst_sentance): # for each review\/sentence\n        sent_vec = np.zeros(50) # as word vectors are of zero length 50, you might need to change this to 300 if you use google's w2v\n        cnt_words =0; # num of words with a valid vector in the sentence\/review\n        for word in sent: # for each word in a review\/sentence\n            if word in w2v_words:\n                vec = w2v_model.wv[word]\n                sent_vec += vec\n                cnt_words += 1\n        if cnt_words != 0:\n            sent_vec \/= cnt_words\n        sent_vectors.append(sent_vec)\n    return sent_vectors","e0fab3fc":"#Convert train data into vector\nsent_vectors_train = text_to_vec(X_train)","03483960":"#Convert cv data into vector\nsent_vectors_cv = text_to_vec(X_cv)","9d28d261":"sent_vectors_test = text_to_vec(X_test)","bcef79fd":"import numpy as np\n\nsent_vectors_train = []; # the avg-w2v for each sentence\/review is stored in this list\nfor sent in tqdm(lst_sentance_train): # for each review\/sentence\n    sent_vec = np.zeros(50) # as word vectors are of zero length 50, you might need to change this to 300 if you use google's w2v\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        if word in w2v_words:\n            vec = w2v_model.wv[word]\n            sent_vec += vec\n            cnt_words += 1\n    if cnt_words != 0:\n        sent_vec \/= cnt_words\n    sent_vectors_train.append(sent_vec)\nprint(len(sent_vectors_train))\nprint(sent_vectors_train[0])","b8a82c85":"find_best_k(sent_vectors_train,y_train, sent_vectors_cv, 40)","97435d17":"get_testing_result(sent_vectors_train, sent_vectors_test,17)","e91a2954":"def text_to_vec_tf(X):\n    \n    \n    lst_sentance=[]\n    for sentance in X:\n        lst_sentance.append(sentance.split())\n    print(lst_sentance[0])\n    \n    model = TfidfVectorizer()\n    tf_idf_matrix = model.fit_transform(X)\n    tfidf_feat = model.get_feature_names() # tfidf words\/col-names\n    # we are converting a dictionary with word as a key, and the idf as a value\n    dictionary = dict(zip(model.get_feature_names(), list(model.idf_)))\n\n    tfidf_sent_vectors = []; # the tfidf-w2v for each sentence\/review is stored in this list\n    row=0;\n    for sent in tqdm(lst_sentance): # for each review\/sentence \n        sent_vec = np.zeros(50) # as word vectors are of zero length\n        weight_sum =0; # num of words with a valid vector in the sentence\/review\n        for word in sent: # for each word in a review\/sentence\n            if word in w2v_words and word in tfidf_feat:\n                vec = w2v_model.wv[word]\n                tf_idf = dictionary[word]*(sent.count(word)\/len(sent))\n                sent_vec += (vec * tf_idf)\n                weight_sum += tf_idf\n        if weight_sum != 0:\n            sent_vec \/= weight_sum\n        tfidf_sent_vectors.append(sent_vec)\n        row += 1\n    return tfidf_sent_vectors","fecfdf10":"sent_vector_train_avg_tfidf = text_to_vec_tf(X_train)","a1da53ce":"# S = [\"abc def pqr\", \"def def def abc\", \"pqr pqr def\"]\nmodel = TfidfVectorizer()\ntf_idf_matrix = model.fit_transform(final_reviews)\n# we are converting a dictionary with word as a key, and the idf as a value\ndictionary = dict(zip(model.get_feature_names(), list(model.idf_)))","d82895a6":"sent_vector_cv_avg_tfidf = text_to_vec_tf(X_cv)","92268b60":"sent_vector_test_avg_tfidf = text_to_vec_tf(X_test)","85a98e0f":"find_best_k(sent_vector_train_avg_tfidf,y_train, sent_vector_cv_avg_tfidf, 50)","1a70e73c":"get_testing_result(sent_vector_train_avg_tfidf, sent_vector_test_avg_tfidf,27)","788ea83c":"from graph we have to choose a datapoint where train auc decreases and the distance between train_auc and cv_auc is less.<br>\nBESK K = 27","cebaf1cd":"#### [4.4.1.2] TFIDF weighted W2v","894d1196":"### Function to convert text into vector for Avg W2V","5a746c8b":"# Amazon Fine Food Reviews Analysis With KNN","009645d6":"## [2.4] Deduplication","9d51c030":"## [2.6] Sampling and balancing the dataset\nAs data size is very large,due to computation limition, we will try to pick random data such that the final data should be balanced.","cbcd6f15":"# Conclusion","e6018568":"#### [4.4.1.1] Avg W2v","925d158a":"## Summary\nIn this step we did below steps to make data ready for preprocessing steps.\n- Mapped score to polarity i.e positive or negative\n- Cleaning of data by converting review text and summary of review to lower case, deduplication and formating time into datetime format from unix timestamp\n- We also removed anomoly based on rules\n- Finally we balanced the dataset","e2e04a02":"## [4.2] Bi-Grams and n-Grams.","aebab60e":"## Summary\nThis is also part of data cleaning where we handle below things.\n- Removed http address from text\n- Stemming\n- Stop word removed","d409df0a":"from graph we have to choose a datapoint where train auc decreases and the distance between train_auc and cv_auc is less.<br>\nBESK K = 3","136e32fd":"from graph we have to choose a datapoint where train auc decreases and the distance between train_auc and cv_auc is less.<br>\nBESK K = 17","edc63467":"## [1.1] Loading the data\n\nIn order to load the data, We have used the SQLITE dataset as it is easier to query the data and visualise the data efficiently.\n<br>","a1eae6ea":"### Function to convert tfidf word2vec.","189ccca7":"### Function to find optiimal k value","5405902e":"We have applied KNN on amazon dataset and found the accuracy for any of the technique is between 60-66%. We did featurization and tested output of all 4 models on which the best accuracy is 66% with AvgW2V model. Hence AvgW2V model is best in checking the polarity of the review.","ad4071f6":"## [3] Data Preprocessing ","0e4f5ee1":"## [2.1] Mapping of review score to polarity","88421c87":"### Function to get testing results(Confusion Matrix, classification report and Accuracy)","8a8f037f":"from graph we have to choose a datapoint where train auc decreases and the distance between train_auc and cv_auc is less.<br>\nBESK K = 5","4c0cd770":"A more detailed analysis is done in <a href = \"https:\/\/nycdatascience.com\/blog\/student-works\/amazon-fine-foods-visualization\/\">this link<\/a> ","49e2c68f":"Spliting data into train,test and validation set.","90d6a4db":"## [4.4.1] Converting text into vectors using Avg W2V, TFIDF-W2V","0e2ae2fd":"# [4] Feature Engineering and apply KNN\n\nIn this step we will convert text into numeric vector so that we can apply KNN algorithm.","deabf53b":"## 1.3 Summary\n\nWe found that the dataset have 10 attributes and below is the data description.\n1. Id: Id of review\n2. ProductId - unique identifier for the product\n3. UserId - unqiue identifier for the user\n4. ProfileName: Username who reviewed the item\n5. HelpfulnessNumerator - number of users who found the review helpful\n6. HelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\n7. Score - rating between 1 and 5\n8. Time - timestamp for the review(unix format)\n9. Summary - brief summary of the review\n10. Text - text of the review\n\nWe did data profiling to understand data better. Below are some key points which we found.\n\n- Number of unique products:72005\n- Number of unique users who provided reviews:  243414\n- First date of the review in the dataset:  1999-10-07\n- Last date of the review in the dataset: 2012-10-25\n- Timespan for which reviews has been provided:13 years \n- Distribution of data based on score\n    - Score 5 :69.06%\n    - Score 4 :15.34%\n    - Score 1 :9.94%\n    - Score 2 :5.66%","2c07f739":"## [2.2] Converting text to lowercase","46f77fe5":"## [4.3] TF-IDF","00c24ce3":"## [2.3] Converting Time to datetime format","4f99c041":"## Objective","7bcb159c":"Given a review determine polarity of the review( Polarity: Whether a review is positive or negative)\n\n<i><b>How to determine polarity of the review?<\/b><\/i>\n\nWe can use score column from the amazon dataset to determine whether a review is positive or negative.\n\n- Score(4 and 5) --> Positive review\n- Score(1 and 2) --> Negative review\n- Score 3 --> Neutral","a1511011":"The ipython notebook consist of below steps.\n- Reading data: The data source is present in two format but we used sqllite because it will be easy for us to query the database\n\n- Data Profiling: To find the basic statistics of the data.\n- Data Cleaning: This is one of the crucial step in the analysis. We performed below operation to clean the data so that it will be ready for preprocessing.\n    1. Mapping of review score to polarity\n    2. Converting text to lowercase\n    3. Converting Time to datetime format\n    4. Deduplication\n    5. Rule based filtering\n    6. Sampling and balancing the dataset\n\n- Data Preprocessing: After cleaning of data we preprocessed the data by following below steps.\n    1. Remove stopwords\n    2. Remove punctuations.\n    3. HTML tag removal\n- Split Data into train, validate and test dataset.\n- Feature Engineering: After the data is cleaned and ready to process, we did featurization to convert text to vector. For this , I follow below algorithms\n    1. Bag Of Words (BoW)\n    2. Term Frequency - inverse document frequency (tf-idf)\n    3. Word2Vec\n        - AvgW2V\n        - tf-idf weighted Word2Vec\n- Model Fitting and testing: For all the dataset genererated in feature engineering, we fit the model and did the testing to find accuracy of the model.","28818c9c":"So here we query for a userId= \"AR5J8UI46CURR\" and we found that all detail's are same other than productId. Even the timestamp \nis same for all the reviews. This is because some of the properties like color, type of product etc are different but the product is same. This is not good for our ML model and it is a duplicate record. So we have to keep only one copy of this record.","3a3701c4":"# 4.1.1 Testing data\n","dd5301b0":"# [2] Mapping and Data Cleaning","b3afea1b":"## [1.2] Data Profiling","e123c315":"In this step we will do data cleaning and mapping of the data.<br>\n\nData Mapping:\n - Convert Score to polarity\n \nData cleaning steps:\n- Converting text to lowercase\n- Converting datetime from unix timestamp to datetime format\n- Deduplication\n- Rule based filtering\n","93ebc368":"Now we have final_df which is somewhat balanced, clean and ready for preprocessing dataset","074755c6":"from graph we have to choose a datapoint where train auc decreases and the distance between train_auc and cv_auc is less.<br>\nBESK K = 17","412b0311":"## [4.1] BoW(Bag Of Words)","cc7e2379":"## [4.4] Word2Vec","a17b75cc":"## [2.5] Rule based filtering \n#### Rule = (HelpfulnessNumerator <= HelpfulnessDenominator)","ddb34dc7":"# [1]. Reading Data And Profling"}}