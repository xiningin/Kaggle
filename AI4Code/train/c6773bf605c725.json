{"cell_type":{"3bad7dbd":"code","db29af10":"code","a2ab0958":"code","45b42862":"code","4b977c3e":"code","294d16c3":"code","3f00752b":"code","50b5e16e":"code","46a27fa6":"code","b97eba46":"code","337ff62e":"code","584f1412":"code","42c756b6":"code","4eadd9c0":"code","d6bb4e7b":"code","e96e1074":"code","090081c2":"code","402eb648":"code","e1c650f9":"code","1ba7bed6":"code","22f7f362":"code","b0abd0a4":"code","95420864":"code","2b82eae8":"code","a2add409":"code","944491a8":"code","47178f6b":"code","649659e9":"code","912f30e1":"code","40e48c26":"code","5c20cc00":"code","1f44fdaa":"code","7ab56e20":"code","e05fa97b":"code","ee33a471":"code","04596b97":"code","5574a451":"code","cf387409":"code","145c095d":"code","7eaf76c4":"code","28dc18a9":"code","f656bb64":"code","f3c6a022":"code","1a9cb417":"code","bc08dbba":"code","2a7e279b":"code","db7727e5":"code","a326e1b9":"code","3479eda2":"code","393ffb6b":"code","92527502":"code","dc86bb3b":"code","08f7334b":"code","22a8ee8b":"code","c1d1195c":"code","3004d201":"code","28a8a73b":"code","1811d1a6":"code","0bba835e":"code","13924355":"code","eaed1992":"code","173f7d6a":"code","ec06dcba":"code","8c14bae5":"code","40ce698c":"code","08d6b063":"code","d6cf9205":"code","e7e2fb8d":"code","8d95daea":"code","47531132":"markdown","230a61ad":"markdown","3a3a68f3":"markdown","38eb8c98":"markdown","d83fc4e5":"markdown","6a53db50":"markdown","1b2fccaf":"markdown","90cbd011":"markdown","cabd7744":"markdown","45451543":"markdown","8481105b":"markdown","c5a26208":"markdown"},"source":{"3bad7dbd":"import os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport IPython.display as ipd\nimport librosa.display\nimport numpy as np\nimport pandas as pd\nfrom scipy.fftpack import fft\nfrom scipy import signal\nfrom scipy.io import wavfile\nimport librosa\nimport glob\nimport tensorflow as tf\nimport wave\nimport sys\nfrom tqdm.notebook import tqdm\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Dropout\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers.convolutional import MaxPooling1D,AveragePooling1D\nfrom keras.regularizers import l2\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nfrom kerastuner.tuners import RandomSearch\nfrom kerastuner.engine.hypermodel import HyperModel\nfrom kerastuner.engine.hyperparameters import HyperParameters\nfrom keras.utils import to_categorical\nfrom IPython import display\nfrom sklearn.utils import shuffle\nimport pandas as pd\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.callbacks import TensorBoard\nimport gc","db29af10":"!ls \"..\/input\/ravdess-emotional-song-audio\/audio_song_actors_01-24\"","a2ab0958":"audio_files= glob.glob('..\/input\/ravdess-emotional-song-audio\/audio_song_actors_01-24\/*')\naudio_files","45b42862":"male_song=[]\nfemale_song=[]\nmale_emotion=[]\nfemale_emotion=[]\nfor root,dirnames,filename in os.walk('..\/input\/ravdess-emotional-song-audio\/audio_song_actors_01-24\/'):\n  wave_file=glob.glob(root + \"\/*.wav\")\n  for wave in wave_file:\n    gender= int(wave.split('\/')[5].split('-')[6].split('.')[0])\n    emotion= int(wave.split('\/')[5].split('-')[2])\n    if gender%2 == 0:\n      female_song.append(glob.glob(root + \"\/*.wav\"))\n      if emotion == 1:\n        female_emotion.append('neutral')\n      elif emotion == 3:\n        female_emotion.append('happy')\n      elif emotion == 4:\n        female_emotion.append('sad')\n      elif emotion == 5:\n        female_emotion.append('angry')\n      elif emotion == 6:\n        female_emotion.append('fear')\n      elif emotion == 7:\n        female_emotion.append('disgust')\n      else:\n        female_emotion.append('surprise')\n    else:\n      male_song.append(glob.glob(root + \"\/*.wav\"))\n      if emotion == 1:\n        male_emotion.append('neutral')\n      elif emotion == 3:\n        male_emotion.append('happy')\n      elif emotion == 4:\n        male_emotion.append('sad')\n      elif emotion == 5:\n        male_emotion.append('angry')\n      elif emotion == 6:\n        male_emotion.append('fear')\n      elif emotion == 7:\n        male_emotion.append('disgust')\n      else:\n        male_emotion.append('surprise')","4b977c3e":"male_song[:50]","294d16c3":"female_song[:50]","3f00752b":"male_emotion[:50]","50b5e16e":"male_emotion_dir= glob.glob(\"..\/input\/surrey-audiovisual-expressed-emotion-savee\/ALL\/*\")\nmale_emotion_dir[0:5]","46a27fa6":"path_male_emotiondir=[]\nfor root, dirnames, filenames in os.walk('..\/input\/surrey-audiovisual-expressed-emotion-savee\/ALL\/'):\n  path_male_emotiondir.extend(glob.glob(root + \"\/*.wav\"))\n  print(path_male_emotiondir[:5])","b97eba46":"male_emotion_1= []\nfor male in male_emotion_dir:\n  emotion= male.split('\/')[4].split('_')[1]\n  if emotion.startswith('a'):\n    male_emotion_1.append('angry')\n  elif emotion.startswith('d'):\n    male_emotion_1.append('disgust')\n  elif emotion.startswith('f'):\n    male_emotion_1.append('fear')\n  elif emotion.startswith('h'):\n    male_emotion_1.append('happy')\n  elif emotion.startswith('n'):\n    male_emotion_1.append('neutral')\n  elif emotion.startswith('sa'):\n    male_emotion_1.append('sad')\n  elif emotion.startswith('su'):\n    male_emotion_1.append('surprise')\n  else:\n    male_emotion_1.append('Wrong emotion')\n  ","337ff62e":"male_emotion_1[:50]","584f1412":"!ls '..\/input\/toronto-emotional-speech-set-tess\/TESS Toronto emotional speech set data\/'","42c756b6":"female_emotion_1=[]\npath_female_emotiondir=[]\nfor root, dirnames, filenames in os.walk('..\/input\/toronto-emotional-speech-set-tess\/TESS Toronto emotional speech set data\/'):\n  for f in filenames:\n    emotion= root.split('\/')[4]\n    if emotion == 'OAF_angry' or emotion == 'YAF_angry':\n       female_emotion_1.append('angry')\n       path= os.path.join(root+f)\n       path_female_emotiondir.append(path)\n    elif emotion == 'OAF_disgust' or emotion == 'YAF_disgust':\n      female_emotion_1.append('disgust')\n      path= os.path.join(root+f)\n      path_female_emotiondir.append(path)\n    elif emotion == 'OAF_Fear' or emotion == 'YAF_fear':\n      female_emotion_1.append('fear')\n      path= os.path.join(root+f)\n      path_female_emotiondir.append(path)\n    elif emotion == 'OAF_happy' or emotion == 'YAF_happy':\n      female_emotion_1.append('happy')\n      path= os.path.join(root+f)\n      path_female_emotiondir.append(path)\n    elif emotion == 'OAF_Sad' or emotion == 'YAF_sad':\n      female_emotion_1.append('sad')\n      path= os.path.join(root+f)\n      path_female_emotiondir.append(path)\n    elif emotion == 'OAF_Pleasant_surprise' or emotion == 'YAF_pleasant_surprised':\n      female_emotion_1.append('surprise')\n      path= os.path.join(root+f)\n      path_female_emotiondir.append(path)\n    elif emotion == 'OAF_neutral' or emotion== 'YAF_neutral':\n      female_emotion_1.append('neutral')\n      path= os.path.join(root+f)\n      path_female_emotiondir.append(path)\n    else:\n      female_emotion_1.append('Wrong emotion')\n      path= os.path.join(root+f)\n      path_female_emotiondir.append(path)","4eadd9c0":"female_emotion[:50]","d6bb4e7b":"female_emotion_1[:50]","e96e1074":"emotion= male_emotion + male_emotion_1 + female_emotion + female_emotion_1","090081c2":"#Label dataframe\nemotion_df= pd.DataFrame(emotion,columns=['Labels'])\nprint(emotion_df.shape)\nemotion_df.head(10)","402eb648":"def log_spectrum(data):\n    spectrogram = librosa.feature.melspectrogram(sample,sr=sample_rate,n_mels=128,fmax=8000)\n    db_spec = librosa.power_to_db(spectrogram)\n    #temporally average spectrogram\n    log_spectrogram = np.mean(db_spec, axis = 0)\n    return log_spectrogram","e1c650f9":"def noise(data):\n    noise_amp = 0.04*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n\ndef stretch(data, rate=0.70):\n    return librosa.effects.time_stretch(data, rate)\n\ndef shift(data):\n    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n    return np.roll(data, shift_range)\n\ndef pitch(data, sampling_rate, pitch_factor=0.8):\n    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n\ndef higher_speed(data, speed_factor = 1.25):\n    return librosa.effects.time_stretch(data, speed_factor)\n\ndef lower_speed(data, speed_factor = 0.75):\n    return librosa.effects.time_stretch(data, speed_factor)","1ba7bed6":"data, sr = librosa.load(\"..\/input\/ravdess-emotional-song-audio\/audio_song_actors_01-24\/Actor_17\/03-02-03-01-01-01-17.wav\", res_type='kaiser_fast')\nplt.figure(figsize=(10,3))\nlibrosa.display.waveplot(y=data, sr=sr)\nipd.Audio(\"..\/input\/ravdess-emotional-song-audio\/audio_song_actors_01-24\/Actor_17\/03-02-03-01-01-01-17.wav\")","22f7f362":"data, sr = librosa.load(\"..\/input\/ravdess-emotional-song-audio\/audio_song_actors_01-24\/Actor_17\/03-02-02-02-01-02-17.wav\", res_type='kaiser_fast')\nplt.figure(figsize=(10,3))\nlibrosa.display.waveplot(y=data, sr=sr)\nipd.Audio(\"..\/input\/ravdess-emotional-song-audio\/audio_song_actors_01-24\/Actor_17\/03-02-02-02-01-02-17.wav\")","b0abd0a4":"sample_rate = 22050\ndf= pd.DataFrame(columns=['feature'])\ncount=0\nfor root,dirnames,filename in os.walk('..\/input\/ravdess-emotional-song-audio\/audio_song_actors_01-24\/'):\n  wave_file=glob.glob(root + \"\/*.wav\")\n  if wave_file in male_song:\n      for a_file in tqdm(wave_file):\n        sample, sample_rate = librosa.load(a_file,sr=sample_rate, res_type='kaiser_fast')\n        male_log_spec = log_spectrum(sample)\n        \n        \n        time_stretched= stretch(sample)\n        time_stretching= log_spectrum(time_stretched)\n        \n        shifted= shift(sample)\n        shifting =log_spectrum(shifted)\n\n        \n        pitch_scale= pitch(sample,sample_rate)\n        pitch_scaling= log_spectrum(pitch_scale)\n        \n\n        high_speed= higher_speed(sample)\n        high_speeding= log_spectrum(high_speed)\n        \n\n        low_speed= lower_speed(sample)\n        low_speeding = log_spectrum(low_speed)\n        \n        df.loc[count]= [male_log_spec+time_stretching+shifting+pitch_scaling+high_speeding+low_speeding]\n        count= count+1","95420864":"gc.collect()\ndf","2b82eae8":"sample_rate = 22050\ndf1= pd.DataFrame(columns=['feature'])\ncount1=0\nfor a_file in tqdm(path_male_emotiondir):\n        sample, sample_rate = librosa.load(a_file,sr=sample_rate, res_type='kaiser_fast')\n        male_log_spec_1 = log_spectrum(sample)\n\n        time_stretched= stretch(sample)\n        time_stretching_1= log_spectrum(time_stretched)\n\n        shifted= shift(sample)\n        shifting_1 =log_spectrum(shifted)\n        \n        pitch_scale= pitch(sample,sample_rate)\n        pitch_scaling_1= log_spectrum(pitch_scale)\n        \n        high_speed= higher_speed(sample)\n        high_speeding_1= log_spectrum(high_speed)\n\n        low_speed= lower_speed(sample)\n        low_speeding_1 = log_spectrum(low_speed)\n        \n        df1.loc[count1]= [male_log_spec_1+time_stretching_1+shifting_1+pitch_scaling_1+high_speeding_1+low_speeding_1]\n        count1= count1+1\n        ","a2add409":"gc.collect()\ndf1","944491a8":"data, sr = librosa.load(\"..\/input\/ravdess-emotional-song-audio\/audio_song_actors_01-24\/Actor_02\/03-02-05-02-01-01-02.wav\", res_type='kaiser_fast')\nplt.figure(figsize=(10,3))\nlibrosa.display.waveplot(y=data, sr=sr)\nipd.Audio(\"..\/input\/ravdess-emotional-song-audio\/audio_song_actors_01-24\/Actor_02\/03-02-05-02-01-01-02.wav\")","47178f6b":"sample_rate = 22050\ndf2= pd.DataFrame(columns=['feature'])\ncount2=0\nfor root,dirnames,filename in os.walk('..\/input\/ravdess-emotional-song-audio\/audio_song_actors_01-24\/'):\n  wave_file=glob.glob(root + \"\/*.wav\")\n  if wave_file in female_song:\n      for a_file in tqdm(wave_file):\n        sample, sample_rate = librosa.load(a_file,sr=sample_rate, res_type='kaiser_fast')\n        female_log_spec = log_spectrum(sample)\n                \n        time_stretched= stretch(sample)\n        time_stretching_2= log_spectrum(time_stretched)\n        \n        shifted= shift(sample)\n        shifting_2 =log_spectrum(shifted)\n        \n        pitch_scale= pitch(sample,sample_rate)\n        pitch_scaling_2= log_spectrum(pitch_scale)\n        \n        high_speed= higher_speed(sample)\n        high_speeding_2= log_spectrum(high_speed)\n        \n        low_speed= lower_speed(sample)\n        low_speeding_2 = log_spectrum(low_speed)\n        \n        df2.loc[count2]= [female_log_spec+time_stretching_2+shifting_2+pitch_scaling_2+high_speeding_2+low_speeding_2]\n        count2= count2+1","649659e9":"gc.collect()\ndf2","912f30e1":"path_emotiondir=[]\nfor root, dirnames, filenames in os.walk('..\/input\/toronto-emotional-speech-set-tess\/TESS Toronto emotional speech set data\/'):\n  path_emotiondir.extend(glob.glob(root + \"\/*.wav\"))\nprint(path_emotiondir[0:5])","40e48c26":"sample_rate = 22050\ndf3= pd.DataFrame(columns=['feature'])\ncount3=0\nfor a_file in tqdm(path_emotiondir):\n    sample, sample_rate = librosa.load(a_file,sr=sample_rate, res_type='kaiser_fast')\n    female_log_spec_1 = log_spectrum(sample)\n\n    time_stretched= stretch(sample)\n    time_stretching_3= log_spectrum(time_stretched)\n\n    shifted= shift(sample)\n    shifting_3 =log_spectrum(shifted)\n\n    pitch_scale= pitch(sample,sample_rate)\n    pitch_scaling_3= log_spectrum(pitch_scale)\n\n    high_speed= higher_speed(sample)\n    high_speeding_3= log_spectrum(high_speed)\n\n    low_speed= lower_speed(sample)\n    low_speeding_3 = log_spectrum(low_speed)\n    \n    df3.loc[count3]= [female_log_spec_1+time_stretching_3+shifting_3+pitch_scaling_3+high_speeding_3+low_speeding_3]\n    count3= count3+1","5c20cc00":"gc.collect()\ndf3","1f44fdaa":"frames= [df,df1,df2,df3]\nnew_df= pd.concat(frames)\nnew_df","7ab56e20":"#feature Dataframe\nnew_data= pd.DataFrame(new_df['feature'].values.tolist())\nnew_data","e05fa97b":"#merge feature and target variable\naudio_data= pd.concat([emotion_df,new_data],axis=1)\naudio_data","ee33a471":"audio_data= shuffle(audio_data)\naudio_data=audio_data.fillna(0)\naudio_data","04596b97":"le= LabelEncoder()\nemotions_and_encoded_val= pd.DataFrame()\nemotions_and_encoded_val['Emotions']= audio_data['Labels'].unique()\nemotions_and_encoded_val['Encoded value']= le.fit_transform(emotions_and_encoded_val['Emotions'])\nemotions_and_encoded_val","5574a451":"n_class= len(emotions_and_encoded_val['Encoded value'])\nn_class","cf387409":"#target variable- label and one hot encoding\nY= tf.keras.utils.to_categorical(le.fit_transform(audio_data['Labels']))\nY.shape","145c095d":"np.unique(Y)","7eaf76c4":"#feature Variable\nX= audio_data.iloc[:,1:].values\nX","28dc18a9":"print(X.shape,Y.shape)","f656bb64":"X_train,X_test,y_train,y_test= train_test_split(X,Y,test_size=0.2,random_state=5)\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","f3c6a022":"X_train = np.expand_dims(X_train, axis=2)\nX_test = np.expand_dims(X_test, axis=2)\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","1a9cb417":"def model(input_shape, num_classes):\n    inputs = tf.keras.layers.Input(shape=input_shape, name=\"input\")\n\n    x = Conv1D(64, 9, activation='relu',padding='same')(inputs)\n    x = Conv1D(64, 9, activation='relu',padding='same')(x)\n    x = MaxPooling1D(pool_size=2)(x)\n    #x= Dropout(0.2)(x)\n    x = Conv1D(128,9, activation='relu',padding='same')(x)\n    x = Conv1D(128,9, activation='relu',padding='same')(x)\n    x = MaxPooling1D(pool_size=2)(x)\n    #x= Dropout(0.2)(x)\n    x = Conv1D(256,9, activation='relu',padding='same')(x)\n    x = Conv1D(256,9, activation='relu',padding='same')(x)\n    x = MaxPooling1D(pool_size=2)(x)\n    x= Dropout(0.2)(x)\n    x= Flatten()(x)\n    x = Dense(128, activation=\"relu\")(x)\n\n    outputs = Dense(num_classes, activation=\"softmax\", name=\"output\")(x)\n\n    return models.Model(inputs=inputs, outputs=outputs)\n\n\nmodel = model((X_train.shape[1],1), n_class)\n\nmodel.summary()\n\n","bc08dbba":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory= model.fit(X,Y, epochs=40,validation_data=(X_test,y_test))","2a7e279b":"model.save(\"emotion_recognition.h5\")","db7727e5":"import os\nos.chdir(r'..\/working')\nfrom IPython.display import FileLink\nFileLink(r'emotion_recognition.h5')","a326e1b9":"def show_graphs(history):\n    epochs = [i for i in range(40)]\n    fig , ax = plt.subplots(1,2)\n    train_acc = history.history['accuracy']\n    train_loss = history.history['loss']\n    test_acc = history.history['val_accuracy']\n    test_loss = history.history['val_loss']\n\n    fig.set_size_inches(30,12)\n    ax[0].plot(epochs , train_loss , label = 'Training Loss')\n    ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n    ax[0].set_title('Training & Testing Loss')\n    ax[0].legend()\n    ax[0].set_xlabel(\"Epochs\")\n\n    ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n    ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n    ax[1].set_title('Training & Testing Accuracy')\n    ax[1].legend()\n    ax[1].set_xlabel(\"Epochs\")\n    plt.show()","3479eda2":"show_graphs(history)","393ffb6b":"from tensorflow import keras\nmodel = keras.models.load_model(\"emotion_recognition.h5\")","92527502":"song= os.listdir('..\/input\/song-data\/')\nsong= wavfile","dc86bb3b":"# convert .mp3 song into .wav \nfrom pydub import AudioSegment\ninput= '..\/input\/song-data\/John-Mayer-I-Guess-I-Just-Feel-Like.mp3'\noutput= '.\/John-Mayer-I-Guess-I-Just-Feel-Like.wav'\n\nsound= AudioSegment.from_mp3(input)\nsound.export(output,format=\"wav\")","08f7334b":"testsong= '.\/John-Mayer-I-Guess-I-Just-Feel-Like.wav'","22a8ee8b":"leng=[]\nfrom pydub.utils import make_chunks\nfrom pydub import AudioSegment\n\naudio = AudioSegment.from_file('.\/John-Mayer-I-Guess-I-Just-Feel-Like.wav' , \"wav\") \nchunk_length_ms = 7000 # pydub calculates in millisec\nchunks = make_chunks(audio, chunk_length_ms) #Make chunks of one sec\n\n#Export all of the individual chunks as wav files\n\nfor i, chunk in enumerate(chunks):\n    chunk_name = \"chunk{0}.wav\".format(i)\n    print (\"exporting\", chunk_name)\n    chunk.export(chunk_name, format=\"wav\")\n    leng.append(chunk_name)\n    filename = 'chunk'+str(i)+'.wav'\n    print(\"Processing chunk \"+str(i))\n    file = filename\n    print('Chunk File is',file)\n    song_sample, sample_rate = librosa.load(file,sr = 22050,res_type='kaiser_fast')\n    print(song_sample.shape)\n    audio_spectrum= log_spectrum(song_sample)\n    print(audio_spectrum.shape)","c1d1195c":"sample_rate = 22050\ntest_df= pd.DataFrame(columns=['feature'])\ncount=0\nfor i in leng:\n  sample, sample_rate = librosa.load(i,sr=sample_rate, res_type='kaiser_fast')\n  log_spec = log_spectrum(sample)\n\n\n  time_stretched= stretch(sample)\n  time_stretching= log_spectrum(time_stretched)\n\n  shifted= shift(sample)\n  shifting =log_spectrum(shifted)\n\n\n  pitch_scale= pitch(sample,sample_rate)\n  pitch_scaling= log_spectrum(pitch_scale)\n\n\n  high_speed= higher_speed(sample)\n  high_speeding= log_spectrum(high_speed)\n\n\n  low_speed= lower_speed(sample)\n  low_speeding = log_spectrum(low_speed)\n\n  test_df.loc[count]= [log_spec+time_stretching+shifting+pitch_scaling+high_speeding+low_speeding]\n  count=count+1","3004d201":"type(test_df)","28a8a73b":"test_data=np.array(test_df)","1811d1a6":"test_data.shape","0bba835e":"max_size_log= 308\ndef padded_log(Log_spec):\n  pad_log=[]\n  for i in Log_spec:\n    app=(max_size_log-len(i))*[0]\n    #print(app)\n    new_list=i.tolist()+app\n    pad_log.append(new_list)\n\n  arr=np.array([np.array(xi) for xi in pad_log])\n  \n  return arr","13924355":"test_data=padded_log(test_df['feature'])","eaed1992":"type(test_data)","173f7d6a":"test=test_data.reshape(-1,308,1)","ec06dcba":"ans= model.predict(test)","8c14bae5":"ans","40ce698c":"arr= ans.T\narr.shape","08d6b063":"arr[0]","d6cf9205":"lst=[]\nfor op in arr:\n    output= np.argmax(op)\n    print(type(output))\n    ","e7e2fb8d":"final_prediction=[]\nfor i in ans:\n  i=i.tolist()\n  maxpos = i.index(max(i))  \n  if(maxpos==0):\n    final_prediction.append(\"angry\")\n  elif(maxpos==1):\n    final_prediction.append(\"disgust\")\n  elif(maxpos==2):\n    final_prediction.append(\"fear\")\n  elif(maxpos==3):\n    final_prediction.append(\"happy\")\n  elif(maxpos==4):\n    final_prediction.append(\"neutral\")\n  elif(maxpos==5):\n    final_prediction.append(\"sad\")\n  elif(maxpos==6):\n    final_prediction.append(\"surprise\")\n  \n\n\n","8d95daea":"len(final_prediction)","47531132":"# **Get Feature Data(From audio wave file-by adding data augmentation techniques)**","230a61ad":"# **Preprocessing for model creation**","3a3a68f3":"**feature Dataframe**","38eb8c98":"# Process of getting female feature data","d83fc4e5":"# Process of getting male feature data","6a53db50":"**Prediction Of Song**","1b2fccaf":"## Model Creation","90cbd011":"# **Merge feature and target variable & Preprocessing for model creation**","cabd7744":"Filename identifiers\n\nModality (01 = full-AV, 02 = video-only, 03 = audio-only).\n\nVocal channel (01 = speech, 02 = song).\n\nEmotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n\nEmotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n\nStatement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n\nRepetition (01 = 1st repetition, 02 = 2nd repetition).\n\nActor (01 to 24. Odd numbered actors are male, even numbered actors are female).","45451543":"Filename example: 03-02-06-01-02-01-12.wav\n\nAudio-only (03)\nSong (02)\nFearful (06)\nNormal intensity (01)\nStatement \"dogs\" (02)\n1st Repetition (01)\n12th Actor (12)\nFemale, as the actor ID number is even.","8481105b":"**Label DataFrame**","c5a26208":"# **Get Label Data(Types of Emotions)**"}}