{"cell_type":{"55e4ce37":"code","18752229":"code","e2f99349":"code","96e83b1d":"code","35d7de94":"code","a72b0ae5":"code","0ad7c0c3":"code","c88da12a":"code","17310b06":"code","d68a7b2b":"code","e07c05c7":"code","d3504752":"code","5b992946":"code","458353d6":"code","05666d9e":"code","78603068":"code","042ef40c":"code","791610f5":"code","8d4a8032":"code","108044b1":"code","d7a3475f":"code","a9c75349":"code","4c2c129d":"code","c359b555":"code","a5a0bd8c":"code","04380cb8":"code","ee3d1a9e":"code","4c550911":"code","266fb479":"code","6104284c":"code","60b4a2c9":"code","41257cd0":"code","e4c16f33":"code","62b755cd":"code","5620290f":"code","527da014":"code","1ae38b2f":"code","2ab67cd9":"code","9f58467c":"code","c9fed250":"markdown","ce282abc":"markdown","1c1fc633":"markdown","4ad795a8":"markdown","14274ca9":"markdown","a92732ec":"markdown","eeeab1e7":"markdown","49df7386":"markdown","984dd5d3":"markdown","2eb7bb16":"markdown","6bc09d35":"markdown","82524c01":"markdown","79eaa371":"markdown","e4da16c2":"markdown"},"source":{"55e4ce37":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","18752229":"#from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import GradientBoostingClassifier\n#from sklearn.kernel_ridge import KernelRidge\n#from sklearn.svm import NuSVR\n#from sklearn.pipeline import make_pipeline\n#from sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","e2f99349":"# official way to get the data\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\nprint('Done!')","96e83b1d":"(market_train_df, news_train_df) = env.get_training_data()","35d7de94":"pd.set_option(\"display.max_rows\",10)\nmarket_train_df","a72b0ae5":"print(f'{market_train_df.shape[0]} samples and {market_train_df.shape[1]} features in the training market dataset.')","0ad7c0c3":"market_train_df.head()","c88da12a":"\nmarket_train_df['price_diff'] = market_train_df['close'] - market_train_df['open']\ngrouped = market_train_df.groupby('time').agg({'price_diff': ['std', 'min']}).reset_index()\nmarket_train_df\n","17310b06":"print(f\"Average standard deviation of price change within a day in {grouped['price_diff']['std'].mean():.4f}.\")","d68a7b2b":"market_train_df.sort_values('price_diff')[:10]","e07c05c7":"market_train_df['close_to_open'] =  np.abs(market_train_df['close'] \/ market_train_df['open'])","d3504752":"print(f\"In {(market_train_df['close_to_open'] >= 1.2).sum()} lines price increased by 20% or more.\")\nprint(f\"In {(market_train_df['close_to_open'] <= 0.8).sum()} lines price decreased by 20% or more.\")","5b992946":"print(f\"In {(market_train_df['close_to_open'] >= 2).sum()} lines price increased by 100% or more.\")\nprint(f\"In {(market_train_df['close_to_open'] <= 0.5).sum()} lines price decreased by 100% or more.\")","458353d6":"\nmarket_train_df['assetName_mean_open'] = market_train_df.groupby('assetName')['open'].transform('mean')\nmarket_train_df['assetName_mean_close'] = market_train_df.groupby('assetName')['close'].transform('mean')\n\n# if open price is too far from mean open price for this company, replace it. Otherwise replace close price.\nfor i, row in market_train_df.loc[market_train_df['close_to_open'] >= 2].iterrows():\n    if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n        market_train_df.iloc[i,5] = row['assetName_mean_open']\n    else:\n        market_train_df.iloc[i,4] = row['assetName_mean_close']\n        \nfor i, row in market_train_df.loc[market_train_df['close_to_open'] <= 0.5].iterrows():\n    if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n        market_train_df.iloc[i,5] = row['assetName_mean_open']\n    else:\n        market_train_df.iloc[i,4] = row['assetName_mean_close']\n","05666d9e":"market_train_df","78603068":"market_train_df.drop(columns=['price_diff', 'close_to_open', 'assetName_mean_open', 'assetName_mean_close'], inplace=True)","042ef40c":"#numerical columns\ncat_cols = ['assetCode']\nnum_cols = ['volume', 'close', 'open',\n       'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n       #'price_diff', 'close_to_open', 'assetName_mean_open', 'assetName_mean_close'\n           ]\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_indices, val_indices = train_test_split(market_train_df.index.values,test_size=0.25, random_state=23)","791610f5":"def encode(encoder, x):\n    len_encoder = len(encoder)\n    try:\n        id = encoder[x]\n    except KeyError:\n        id = len_encoder\n    return id\n\nencoders = [{} for cat in cat_cols]\n\n\nfor i, cat in enumerate(cat_cols):\n    print('encoding %s ...' % cat, end=' ')\n    encoders[i] = {l: id for id, l in enumerate(market_train_df.loc[train_indices, cat].astype(str).unique())}\n    market_train_df[cat] = market_train_df[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n    print('Done')\n\nembed_sizes = [len(encoder) + 1 for encoder in encoders] #+1 for possible unknown assets","8d4a8032":"from sklearn.preprocessing import StandardScaler\n \nmarket_train_df[num_cols] = market_train_df[num_cols].fillna(0)\nprint('scaling numerical columns')\n\nscaler = StandardScaler()\n\n#col_mean = market_train[col].mean()\n#market_train[col].fillna(col_mean, inplace=True)\nscaler = StandardScaler()\nmarket_train_df[num_cols] = scaler.fit_transform(market_train_df[num_cols])","108044b1":"market_train_df","d7a3475f":"\nclass NN_base:        \n        \n    def __init__(self):\n        \n        from keras.models import Model\n        from keras.layers import Input, Dense, Embedding, Concatenate, Flatten, BatchNormalization, Dropout\n        from keras.losses import binary_crossentropy, mse\n\n        categorical_inputs = []\n        for cat in cat_cols:\n            categorical_inputs.append(Input(shape=[1], name=cat))\n\n        categorical_embeddings = []\n        for i, cat in enumerate(cat_cols):\n            categorical_embeddings.append(Embedding(embed_sizes[i], 10)(categorical_inputs[i]))\n\n            \n        #categorical_logits = Concatenate()([Flatten()(cat_emb) for cat_emb in categorical_embeddings])\n        categorical_logits = Flatten()(categorical_embeddings[0])\n        categorical_logits = Dense(32,activation='relu')(categorical_logits)\n\n        #categorical_logits = Flatten()(categorical_embeddings[0])\n        #categorical_logits = Dense(32,activation='relu')(categorical_logits)\n        #categorical_logits = Dropout(0.5)(categorical_logits)\n        #categorical_logits = BatchNormalization()(categorical_logits)\n        #categorical_logits = Dense(32,activation='relu')(categorical_logits)\n        \n        \n        numerical_inputs = Input(shape=(11,), name='num')\n        numerical_logits = numerical_inputs\n        numerical_logits = BatchNormalization()(numerical_logits)\n\n        #numerical_logits = Dense(128,activation='relu')(numerical_logits)\n        #numerical_logits = Dropout(0.5)(numerical_logits)\n        #numerical_logits = BatchNormalization()(numerical_logits)\n        #numerical_logits = Dense(128,activation='relu')(numerical_logits)\n        #numerical_logits = Dense(64,activation='relu')(numerical_logits)\n         \n        numerical_logits = Dense(128,activation='relu')(numerical_logits)\n        numerical_logits = Dense(64,activation='relu')(numerical_logits)\n\n        logits = Concatenate()([numerical_logits,categorical_logits])\n        logits = Dense(64,activation='relu')(logits)\n        out = Dense(1, activation='sigmoid')(logits)\n\n        self.model = Model(inputs = categorical_inputs + [numerical_inputs], outputs=out)\n        self.model.compile(optimizer='adam',loss=binary_crossentropy)\n        \n    def fit(self,X_train,y_train):\n        from keras.callbacks import EarlyStopping, ModelCheckpoint\n\n        check_point = ModelCheckpoint('model.hdf5',verbose=True, save_best_only=True)\n        early_stop = EarlyStopping(patience=5,verbose=True)\n        self.model.fit(X_train,y_train.astype(int),\n                  #validation_data=(X_valid,y_valid.astype(int)),\n                  epochs=3,\n                  verbose=True,\n                  callbacks=[early_stop,check_point]) \n    \n    def predict(self,X_test):\n        return self.model.predict(X_test)\n    \n    def summary(self):\n        self.model.summary()\n\n","a9c75349":"def get_input(market_train, indices):\n    X_num = market_train.loc[indices, num_cols].values\n    X = {'num':X_num}\n    for cat in cat_cols:\n        X[cat] = market_train.loc[indices, cat_cols].values\n    y = (market_train.loc[indices,'returnsOpenNextMktres10'] >= 0).values\n    r = market_train.loc[indices,'returnsOpenNextMktres10'].values\n    u = market_train.loc[indices, 'universe']\n    d = market_train.loc[indices, 'time'].dt.date\n    return X,y,r,u,d\n\n# r, u and d are used to calculate the scoring metric\nX_train,y_train,r_train,u_train,d_train = get_input(market_train_df, train_indices)\nX_valid,y_valid,r_valid,u_valid,d_valid = get_input(market_train_df, val_indices)","4c2c129d":"NN_tmp = NN_base()","c359b555":"NN_tmp.fit(X_train,y_train)","a5a0bd8c":"#model_lgb_ = lgb.LGBMClassifier(objective='binary',learning_rate=0.05, bagging_fraction = 0.8,\n#                                bagging_freq = 5, n_estimators=100,boosting_type = 'dart',\n#                                num_leaves = 2452, min_child_samples = 212, reg_lambda=0.01)","04380cb8":"#model_lgb_.fit(X_train['num'],y_train)","ee3d1a9e":"#model_xgb_ = xgb.XGBClassifier(colsample_bytree=0.4603, gamma=0.0468, \n#                             learning_rate=0.05, max_depth=6, \n#                             min_child_weight=1.7817, n_estimators=100,\n#                             reg_alpha=0.4640, reg_lambda=0.8571,\n#                             subsample=0.5213, silent=1,\n#                             random_state =7, nthread = -1)","4c550911":"#model_xgb_.fit(X_train['num'],y_train)","266fb479":"from sklearn.metrics import accuracy_score\nconfidence_valid = NN_tmp.predict(X_valid)[:,0]*2 -1\nprint(accuracy_score(confidence_valid>0,y_valid))","6104284c":"# calculation of actual metric that is used to calculate final score\nr_valid = r_valid.clip(-1,1) # get rid of outliers. Where do they come from??\nx_t_i = confidence_valid * r_valid * u_valid\ndata = {'day' : d_valid, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean \/ std\nprint(score_valid)","60b4a2c9":"'''\nimport time\nimport copy\n\nclass StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        tmp_num = []\n        tmp_cat = []\n        for ty, model in base_models:\n            if ty == 'num':\n                tmp_num.append(model)\n            elif ty == 'cat':\n                tmp_cat.append(model)\n            else:\n                continue\n        \n        self.base_models_num = tuple(tmp_num)\n        self.base_models_cat = tuple(tmp_cat)\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n        \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_num_ = [list() for x in self.base_models_num]\n        self.base_models_cat_ = [list() for x in self.base_models_cat]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        X_num = X['num']\n        X_cat = X['assetCode']\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X['num'].shape[0], len(self.base_models_num)))\n        for i, model in enumerate(self.base_models_num):\n            for train_index, holdout_index in kfold.split(X_num, y):\n                ts = time.time()\n                instance = clone(model)\n                self.base_models_num_[i].append(instance)\n                instance.fit(X_num[train_index], y[train_index])\n                y_pred = instance.predict(X_num[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                print(\"{} model... complete at {}\".format(i,(time.time()-ts)))\n        \n        out_of_fold_predictions_c = np.zeros((X['num'].shape[0], len(self.base_models_cat)))\n        for i, model in enumerate(self.base_models_cat):\n            for train_index, holdout_index in kfold.split(X_cat, y):\n                ts = time.time()\n                instance = copy.deepcopy(model)\n                self.base_models_cat_[i].append(instance)\n                \n                X_t = {'assetCode' : X_cat[train_index], 'num': X_num[train_index]}\n                X_h = {'assetCode' : X_cat[holdout_index], 'num': X_num[holdout_index]}\n                instance.fit(X_t, y[train_index])\n                y_pred = (instance.predict(X_h) > 0.5 )\n                out_of_fold_predictions_c[holdout_index, i] = y_pred.flatten()\n                print(\"{} model... complete at {}\".format(i,(time.time()-ts)))        \n        \n        out_of_fold_predictions = np.concatenate((out_of_fold_predictions, out_of_fold_predictions_c), axis=1)\n        \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n    \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        X_num = X['num']\n        X_cat = X['assetCode']\n        \n        meta_features_num = np.column_stack([\n            np.column_stack([model.predict(X_num) for model in base_models_num]).mean(axis=1)\n            for base_models_num in self.base_models_num_ ])\n        \n        X_t = {'num': X_num, 'assetCode': X_cat}\n        if not self.base_model_cat:\n            meta_features_cat = np.column_stack([\n                np.column_stack([model.predict(X_t) for model in base_models_cat]).mean(axis=1)\n                for base_models_cat in self.base_models_cat_ ])\n            meta_features = np.concatenate((meta_features_num, meta_features_cat), axis=1)\n        else:\n            meta_features = meta_features_num\n        return self.meta_model_.predict_proba(meta_features)\n    \n    \n'''","41257cd0":"'''\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nNN = NN_base()\n\nKN_2 = KNeighborsClassifier(n_neighbors=2)\nKN_4 = KNeighborsClassifier(n_neighbors=4)\nKN_8 = KNeighborsClassifier(n_neighbors=8)\nlr = LogisticRegression()\n\n\n#GBoost = GradientBoostingClassifier(n_estimators=10, learning_rate=0.05,\n#                                   max_depth=6,min_samples_leaf=15, min_samples_split=10,random_state =5, verbose=2)\nmodel_lgb_ = lgb.LGBMClassifier(objective='binary',learning_rate=0.05, bagging_fraction = 0.8,\n                                bagging_freq = 5, n_estimators=100,boosting_type = 'dart',\n                                num_leaves = 2452, min_child_samples = 212, reg_lambda=0.01)\n\nmodel_xgb_ = xgb.XGBClassifier(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=6, \n                             min_child_weight=1.7817, n_estimators=100,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n'''","e4c16f33":"#model_lgb_meta = lgb.LGBMClassifier(objective='binary',learning_rate=0.05, n_estimators=100, bagging_fraction = 0.8,\n#                              bagging_freq = 5, boosting_type = 'dart')\n","62b755cd":"#stacked_averaged_models = StackingAveragedModels(base_models = (('num',model_lgb_),('num',model_xgb_)),\n#                                                 meta_model = model_lgb_meta)","5620290f":"#stacked_averaged_models.fit(X_train,y_train)","527da014":"#test={'num': X_train['num'][:10], 'assetCode': X_train['assetCode'][:10]}","1ae38b2f":"#stacked_averaged_models.predict(test)","2ab67cd9":"days = env.get_prediction_days()\nimport time\n\nn_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    if n_days % 50 == 0:\n        print(n_days,end=' ')\n    \n    t = time.time()\n    assetCode = market_obs_df['assetCode']\n\n    #market_obs_df['price_diff'] = market_obs_df['close'] - market_obs_df['open']\n    #market_obs_df['close_to_open'] =  np.abs(market_obs_df['close'] \/ market_obs_df['open'])\n    #market_obs_df['assetName_mean_open'] = market_obs_df.groupby('assetName')['open'].transform('mean')\n    #market_obs_df['assetName_mean_close'] = market_obs_df.groupby('assetName')['close'].transform('mean')\n    market_obs_df[num_cols] = market_obs_df[num_cols].fillna(0)\n    market_obs_df[num_cols] = scaler.fit_transform(market_obs_df[num_cols])\n    #market_obs_df = market_obs_df.loc[:, num_cols].fillna(0).values\n    X = {'num': market_obs_df[num_cols].values}\n    for i,cat in enumerate(cat_cols):\n        market_obs_df[cat+'_encoded'] = market_obs_df[cat].astype(str).apply(lambda x: encode(encoders[i],x))\n        X[cat] = market_obs_df[cat+'_encoded'].values\n    \n    \n    prep_time += time.time() - t\n    \n    t = time.time()\n    #lp = stacked_averaged_models.predict(X)\n    lp = NN_tmp.predict(X)[:,0]\n    #lp = model_lgb_.predict(X['num'])[:]\n    #lp = model_xgb_.predict(X['num'])[:]\n    prediction_time += time.time() -t\n    \n    t = time.time()\n    confidence = 2 * lp -1\n    preds = pd.DataFrame({'assetCode':assetCode,'confidence':confidence})\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)\n    packaging_time += time.time() - t\n    \nenv.write_submission_file()","9f58467c":"n_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\npredicted_confidences = np.array([])\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    print(n_days,end=' ')\n    \n    t = time.time()\n\n    market_obs_df['assetCode_encoded'] = market_obs_df[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n\n    market_obs_df[num_cols] = market_obs_df[num_cols].fillna(0)\n    market_obs_df[num_cols] = scaler.transform(market_obs_df[num_cols])\n    X_num_test = market_obs_df[num_cols].values\n    X_test = {'num':X_num_test}\n    X_test['assetCode'] = market_obs_df['assetCode_encoded'].values\n    \n    prep_time += time.time() - t\n    \n    t = time.time()\n    market_prediction = model.predict(X_test)[:,0]*2 -1\n    predicted_confidences = np.concatenate((predicted_confidences, market_prediction))\n    prediction_time += time.time() -t\n    \n    t = time.time()\n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':market_prediction})\n    # insert predictions to template\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)\n    packaging_time += time.time() - t\n\nenv.write_submission_file()\ntotal = prep_time + prediction_time + packaging_time\nprint(f'Preparing Data: {prep_time:.2f}s')\nprint(f'Making Predictions: {prediction_time:.2f}s')\nprint(f'Packing: {packaging_time:.2f}s')\nprint(f'Total: {total:.2f}s')","c9fed250":"We can see huge price fluctiations when market crashed. Just think about it... **But this is wrong!** There was no huge crash on January 2010... Let's dive into the data!","ce282abc":"### Getting data and importing libraries","1c1fc633":"For a quick fix I'll replace outliers in these lines with mean open or close price of this company.","4ad795a8":"I plot data for all periods because I'd like to show long-term trends.\nAssets are sampled randomly, but you should see that some companies' stocks started trading later, some dissappeared. Disappearence could be due to bankruptcy, acquisition or other reasons.","14274ca9":"At first let's take 10 random assets and plot them.","a92732ec":"We have two datasets, let's explore them separately.","eeeab1e7":"Now, let's look at these price drops in details.","49df7386":"## Modelling\n\nIt's time to build a model!\nI think that in this case we should build a binary classifier - we will simply predict whether the target goes up or down.","984dd5d3":"Well, this isn't much considering we have more than 4 million lines and a lot of these cases are due to price falls during market crash. Well just need to deal with outliers.","2eb7bb16":"## Market data\n\nWe have a really interesting dataset which contains stock prices for many companies over a decade!\n\nFor now let's have a look at the data itself and not think about the competition. We can see long-term trends, appearing and declining companies and many other things.","6bc09d35":"Reference\n\n\nhttps:\/\/www.kaggle.com\/artgor\/eda-feature-engineering-and-everything\n\nhttps:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n\nhttps:\/\/www.kaggle.com\/christofhenkel\/market-data-nn-baseline","82524c01":"Well, these were some random companies. But it would be more interesting to see general trends of prices.","79eaa371":"It is cool to be able to see how markets fall and rise again.\nI have shown 4 events when there were serious stock price drops on the market.\nYou could also notice that higher quantile prices have increased with time and lower quantile prices decreased.\nMaybe the gap between poor and rich increases... on the other hand maybe more \"little\" companies are ready to go to market and prices of their shares isn't very high.","e4da16c2":"### Possible data errors\n\nAt first let's simply sort data by the difference between open and close prices."}}