{"cell_type":{"9fea03b4":"code","97ec814d":"code","fc79ef51":"code","01d0cfde":"code","61b20bcc":"code","0f676dfe":"code","4c25c992":"code","79e8b474":"code","54f33538":"code","fecf731e":"code","c7e4c97b":"code","723f24e6":"code","fb80cc4c":"code","fe2256bb":"code","a65fd53a":"code","1ca6cb87":"code","9a96faae":"code","44d4d283":"code","8d277519":"code","d14ff957":"code","c65237a2":"code","77a3cec7":"code","baea2d5f":"code","1d7c5c1f":"code","71fc84a3":"code","c02405e7":"code","db600924":"code","86b62feb":"code","6be8c56e":"code","8326573a":"code","3c559c2e":"code","2f90c693":"code","c6c57c39":"code","3a148b1d":"code","6f926d47":"code","5ddb8222":"code","0fd6f74b":"code","03c44a3c":"code","c78723c0":"code","1c61c53e":"code","056c02bb":"code","0446da23":"markdown","897b3a51":"markdown","9b2dbf94":"markdown","9648f9ea":"markdown","9ef6bc97":"markdown","e234cbcb":"markdown","e396bc7e":"markdown","cb6e92dd":"markdown","52d25f4d":"markdown","9ec7d7cb":"markdown","743217ea":"markdown","e128ae40":"markdown"},"source":{"9fea03b4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","97ec814d":"creditcard_df = pd.read_csv(\"..\/input\/marketing-datacsv\/CC GENERAL.csv\")\n\n# CUSTID: Identification of Credit Card holder \n# BALANCE: Balance amount left in customer's account to make purchases\n# BALANCE_FREQUENCY: How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n# PURCHASES: Amount of purchases made from account\n# ONEOFFPURCHASES: Maximum purchase amount done in one-go\n# INSTALLMENTS_PURCHASES: Amount of purchase done in installment\n# CASH_ADVANCE: Cash in advance given by the user\n# PURCHASES_FREQUENCY: How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n# ONEOFF_PURCHASES_FREQUENCY: How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n# PURCHASES_INSTALLMENTS_FREQUENCY: How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n# CASH_ADVANCE_FREQUENCY: How frequently the cash in advance being paid\n# CASH_ADVANCE_TRX: Number of Transactions made with \"Cash in Advance\"\n# PURCHASES_TRX: Number of purchase transactions made\n# CREDIT_LIMIT: Limit of Credit Card for user\n# PAYMENTS: Amount of Payment done by user\n# MINIMUM_PAYMENTS: Minimum amount of payments made by user  \n# PRC_FULL_PAYMENT: Percent of full payment paid by user\n# TENURE: Tenure of credit card service for user","fc79ef51":"creditcard_df","01d0cfde":"creditcard_df.info()\n# Apply info and get additional insights on our dataframe\n# There are 18 features with 8950 points","61b20bcc":"# What is the average, minimum and maximum \"BALANCE\" amount?\nprint('Average, Min, Max = ', creditcard_df['BALANCE'].mean(),', ',creditcard_df['BALANCE'].min(), ', ', creditcard_df['BALANCE'].max())","0f676dfe":"# Let's apply describe() and get more statistical insights on our dataframe\ncreditcard_df.describe()\n# Mean balance is $1564 \n# Balance frequency is frequently updated on average ~0.9\n# Purchases average is $1000\n# one off purchase average is ~$600\n# Average purchases frequency is around 0.5\n# average ONEOFF_PURCHASES_FREQUENCY, PURCHASES_INSTALLMENTS_FREQUENCY, and CASH_ADVANCE_FREQUENCY are generally low\n# Average credit limit ~ 4500\n# Percent of full payment is 15%\n# Average tenure is 11 years","4c25c992":"# Obtain the features (row) of the customer who made the maximum \"ONEOFF_PURCHASES\"\ncreditcard_df[creditcard_df['ONEOFF_PURCHASES'] == 40761.250000]","79e8b474":"# Obtain the features of the customer who made the maximum cash advance transaction?\n# how many cash advance transactions did that customer make? How often did he\/she pay their bill?\ncreditcard_df['CASH_ADVANCE'].max()","54f33538":"creditcard_df[creditcard_df['CASH_ADVANCE'] == 47137.211760000006]","fecf731e":"# Let's see if we have any missing data, luckily we don't have many!\nsns.heatmap(creditcard_df.isnull(), yticklabels = False, cbar = True, cmap = 'Blues')  #cbar and cmap: color the bar and color the map resp. ","c7e4c97b":"# How many missing data ?\ncreditcard_df.isnull().sum()","723f24e6":"# Fill up the missing elements with mean of the 'MINIMUM_PAYMENT'\ncreditcard_df.loc[(creditcard_df['MINIMUM_PAYMENTS'].isnull() == True), 'MINIMUM_PAYMENTS'] = creditcard_df['MINIMUM_PAYMENTS'].mean()","fb80cc4c":"creditcard_df.isnull().sum()","fe2256bb":"# Fill out missing elements in the 'CREDIT_LIMIT' column\n# Double check and make sure that no missing elements are present\ncreditcard_df.loc[(creditcard_df['CREDIT_LIMIT'].isnull() == True), 'CREDIT_LIMIT'] = creditcard_df['CREDIT_LIMIT'].mean()","a65fd53a":"sns.heatmap(creditcard_df.isnull(), yticklabels = False, cbar = False, cmap = 'Blues')","1ca6cb87":"creditcard_df.isnull().sum()","9a96faae":"# Let's see if we have duplicated enteries in the data\ncreditcard_df.duplicated().sum()","44d4d283":"# Drop Customer ID column 'CUST_ID' and make sure that the column has been removed from the dataframe\ncreditcard_df.drop('CUST_ID', axis = 1, inplace = True)","8d277519":"creditcard_df","d14ff957":"n = len(creditcard_df.columns)\nn","c65237a2":"creditcard_df.columns","77a3cec7":"# distplot combines the matplotlib.hist function with seaborn kdeplot()\n# KDE Plot represents the Kernel Density Estimate.\n# KDE is used for visualizing the Probability Density of a continuous variable. \n# KDE demonstrates the probability density at different values in a continuous variable. \n\n# Mean of balance is $1500\n# 'Balance_Frequency' for most customers is updated frequently ~1\n# For 'PURCHASES_FREQUENCY', there are two distinct group of customers\n# For 'ONEOFF_PURCHASES_FREQUENCY' and 'PURCHASES_INSTALLMENT_FREQUENCY' most users don't do one off puchases or installment purchases frequently \n# Very small number of customers pay their balance in full 'PRC_FULL_PAYMENT'~0\n# Credit limit average is around $4500\n# Most customers are ~11 years tenure\n\nplt.figure(figsize = (10, 50))\nfor i in range(len(creditcard_df.columns)):\n    plt.subplot(17, 1, i+1)\n    sns.distplot(creditcard_df[creditcard_df.columns[i]], kde_kws = {\"color\":\"b\", \"lw\":3, \"label\":\"KDE\", \"bw\":0.1},hist_kws = {'color':'g', 'label':'Hist'}) # lw = line width\n    plt.title(creditcard_df.columns[i])\nplt.tight_layout()","baea2d5f":"# Obtain the correlation matrix between features\ncorrelations = creditcard_df.corr()\nf, ax = plt.subplots(figsize = (20, 10)) # fix the size\nsns.heatmap(correlations, annot = True)","1d7c5c1f":"# Let's scale the data first\nscaler = StandardScaler()\ncreditcard_df_scaled = scaler.fit_transform(creditcard_df)","71fc84a3":"creditcard_df_scaled.shape","c02405e7":"creditcard_df_scaled","db600924":"# Index(['BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES', 'ONEOFF_PURCHASES',\n#       'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'PURCHASES_FREQUENCY',\n#       'ONEOFF_PURCHASES_FREQUENCY', 'PURCHASES_INSTALLMENTS_FREQUENCY',\n#       'CASH_ADVANCE_FREQUENCY', 'CASH_ADVANCE_TRX', 'PURCHASES_TRX',\n#       'CREDIT_LIMIT', 'PAYMENTS', 'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT',\n#       'TENURE'], dtype='object')\n\nscores_1 = []\nrange_values = range(1,20)\n\nfor i in range_values:\n    kmeans = KMeans(n_clusters = i)\n    kmeans.fit(creditcard_df_scaled[:, :7]) # Run with first 7 features only\n    scores_1.append(kmeans.inertia_) # inertia will give you the value of WCSS\n    \nplt.plot(scores_1, 'bx-')\n\n# From this we can observe that, 4th cluster seems to be forming the elbow of the curve. \n# However, the values does not reduce linearly until 8th cluster. \n# Let's choose the number of clusters to be 7 or 8.","86b62feb":"kmeans = KMeans(7)\nkmeans.fit(creditcard_df_scaled)\nlabels = kmeans.labels_ # labels (cluster) associated to each data point","6be8c56e":"kmeans.cluster_centers_.shape","8326573a":"cluster_centers = pd.DataFrame(data = kmeans.cluster_centers_, columns = [creditcard_df.columns])\ncluster_centers","3c559c2e":"# In order to understand what these numbers mean, let's perform inverse transformation\ncluster_centers = scaler.inverse_transform(cluster_centers)\ncluster_centers = pd.DataFrame(data = cluster_centers, columns = [creditcard_df.columns])\ncluster_centers\n\n# Second Customers cluster (Transactors): Those are customers who pay least amount of intrerest charges and careful with their money, Cluster with lowest balance ($104) and cash advance ($303), Percentage of full payment = 23%\n# Fivth customers cluster (revolvers) who use credit card as a loan (most lucrative sector): highest balance ($5000) and cash advance (~$5000), low purchase frequency, high cash advance frequency (0.5), high cash advance transactions (16) and low percentage of full payment (3%)\n# Fourth customer cluster (VIP\/Prime): high credit limit $16K and highest percentage of full payment, target for increase credit limit and increase spending habits\n# First customer cluster (low tenure): these are customers with low tenure (7 years), low balance \n","2f90c693":"labels.shape # labels associated to each data point","c6c57c39":"labels.max()","3a148b1d":"labels.min()","6f926d47":"y_kmeans = kmeans.fit_predict(creditcard_df_scaled)\ny_kmeans","5ddb8222":"# concatenate the clusters labels to our original dataframe\ncreditcard_df_cluster = pd.concat([creditcard_df, pd.DataFrame({'cluster': labels})], axis = 1)\ncreditcard_df_cluster.head()","0fd6f74b":"# Plot the histogram of various clusters\nfor i in creditcard_df.columns:\n    plt.figure(figsize = (35,5))\n    for j in range(7):\n        plt.subplot(1,7,j+1)\n        cluster = creditcard_df_cluster[creditcard_df_cluster['cluster'] == j]\n        cluster[i].hist(bins = 20)\n        plt.title('{}   \\nCluster  {}'.format(i,j))\n        \n    plt.show()","03c44a3c":"# Obtain the principal components\npca = PCA(n_components = 2)\nprincipal_comp = pca.fit_transform(creditcard_df_scaled)\nprincipal_comp","c78723c0":"# Create a dataframe with the two components\npca_df = pd.DataFrame(data = principal_comp, columns = ['pca1', 'pca2'])\npca_df.head()","1c61c53e":"# Concatenate the clusters labels to the dataframe\npca_df = pd.concat([pca_df, pd.DataFrame({'cluster': labels})], axis = 1)\npca_df.head()","056c02bb":"plt.figure(figsize = (10,10))\nax = sns.scatterplot(x='pca1', y='pca2', hue=\"cluster\", data = pca_df, palette = ['red','green','blue','pink',\n                                                                                 'yellow','gray','purple'])\n# For 7 clusters","0446da23":"# How To Obtain The Optimal Number Of Clusters 'K' (ELBOW METHOD)","897b3a51":"# Apply Principal Component Analysis And Visualize The Results ","9b2dbf94":"# APPLY K-MEANS METHOD","9648f9ea":"# UNDERSTAND THE THEORY AND INTUTION BEHIND K-MEANS","9ef6bc97":"* The elbow method is a heuristic method of interpretation and validation of consistency within cluster analysis designed to help find the appropriate number of clusters in a dataset.\n* If the line chart looks like an arm, then the \"elbow\" on the arm is the value of k that is the best.","e234cbcb":"Within Cluster Sum of Squares(WCSS) = SUM(pi in cluster 1)distance(Pi,C1)^2 +\n                                      SUM(pi in cluster 2)distance(Pi,C2)^2 +\n                                      SUM(pi in cluster 3)distance(Pi,C3)^2\n        \nNumber of Clusters(K) = 3","e396bc7e":"In this project, you have been hired as a data scientist at a bank and you have been provided with extensive data on the bank's \ncustomers for the past 6 months.\nData includes transactions frequency, amount, tenure..etc.\nThe bank marketing team would like to leverage AL\/ML to launch a targeted marketing ad campaign that is tailored to specific group of\ncustomers.\nIn order for this campaign to be successful, the bank has to divide its customers into atleast 3 distinctive groups.\nThis process is known as \"marketing segmentation\" and it crucial for maximizing marketing campaign conversion rate.","cb6e92dd":"# VISUALIZE AND EXPLORE DATASET","52d25f4d":"Principal Component Analysis (PCA)\n1. PCA is an unsupervised machine learning algorithm.\n2. PCA performs dimensionality reductions while attempting at keeping the original information unchanged.\n3. PCA works by trying to find a new set of features called components.\n4. Components are composites of the uncorrelated given input features.","9ec7d7cb":"Q.Which of the following conditions could terminate the K-Means clustering algorithm?\n1. K-Means terminates after a fixed number of iterations.\n2. K-Means terminates when the centroid locations do not change between iterations.","743217ea":"K-MEANS INTUITION\n* K-Means is an unsupervised learning algorithm (clustering).\n* K-Means works by grouping some data points together (clustering) in an \n  unsupervised fashion.\n* The algorithm groups observations with similar attribute values togethr by \n  measuring the Euclidian distance between points","e128ae40":"K-MEANS ALGORITHM STEPS\n1. Choose number of clusters \"K\"\n2. Select random K points that are going to be the centroids for each      cluster. This technique is called \"elbow\".\n3. Assign each data point to the nearest centroid, doing so will enable us to create \"K\" number of clusters.\n4. Calculate a new centroid for each cluster.\n5. Reassign each data point to the new closest centroid.\n6. Go to step 4 and repeat."}}