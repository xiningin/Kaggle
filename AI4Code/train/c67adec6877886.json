{"cell_type":{"3bb03f4b":"code","9bc81453":"code","a6af5d44":"code","22410bcc":"code","430918f8":"code","0c0b0665":"code","51f204e8":"code","149b980a":"code","14ae387d":"code","99d02f08":"code","a7fa3790":"code","a87fdc41":"code","3f0db185":"code","f5ceb7de":"code","2a615b42":"code","30fca316":"code","b1f91419":"code","07f9b438":"code","1a0099ac":"code","9a4af245":"code","1f2f050e":"code","75f4f487":"code","9e816d6a":"code","1ecaddd8":"code","e1033b63":"code","320bd138":"code","2ad7cca4":"code","c05577d7":"code","c52e8056":"code","574af762":"code","79e86046":"code","0d062275":"code","6d61852a":"code","ec9c54b1":"code","9f4171db":"code","861b9530":"code","54b51579":"code","a0fe283e":"code","519c7988":"code","92c8c311":"code","48aed7b0":"code","3445a951":"code","58f7e9b1":"code","1e4327e9":"code","2745296a":"code","200bd547":"code","800c0df1":"code","58e409ba":"code","1c78e65a":"markdown","f94d212e":"markdown","6ec6a537":"markdown","053e2cb8":"markdown","75f0027b":"markdown","54d2be9f":"markdown","e6e59277":"markdown","8fb72836":"markdown","85f1c673":"markdown","594a6fe3":"markdown","19901485":"markdown","8d5ae8c6":"markdown","ada50aba":"markdown","f155b43a":"markdown","d182b1c3":"markdown","68ba838a":"markdown","e7c7314e":"markdown","569de0a0":"markdown","86953d7a":"markdown","92617051":"markdown","fa54a1c5":"markdown","ca9204b6":"markdown","f7d3725a":"markdown","4beddfe1":"markdown","22dca26a":"markdown","40ca7442":"markdown","39eebc52":"markdown","23cc71a8":"markdown","82d5558b":"markdown","1b4a04d1":"markdown","c439925c":"markdown","ac9bcaee":"markdown","5c0aebd4":"markdown","ba793354":"markdown","e883420a":"markdown","72d0279b":"markdown","1a4d4237":"markdown","5511976e":"markdown","e40dc436":"markdown","72beb029":"markdown","4e0a9811":"markdown","4f7a2e52":"markdown","c013bb30":"markdown","92a577a3":"markdown","679d0399":"markdown"},"source":{"3bb03f4b":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler","9bc81453":"test = pd.read_csv('..\/input\/housepricesdata\/test.csv')\ntrain = pd.read_csv('..\/input\/housepricesdata\/train.csv')","a6af5d44":"print ('Train dataframe: ', train.shape[0],'houses, and ', train.shape[1],'features')\nprint ('Test dataframe: ', test.shape[0],'houses, and ', test.shape[1],'features')","22410bcc":"# Determine quantitative and qualitative features\nquantitative = [i for i in train.columns if train.dtypes[i] != 'object']\nquantitative.remove('SalePrice')\nquantitative.remove('Id')\n\nqualitative = [i for i in train.columns if train.dtypes[i] == 'object']","430918f8":"print(list(quantitative))\nprint()\nprint('Train dataframe has: ', len(quantitative), 'quantitative features')","0c0b0665":"print(list(qualitative))\nprint()\nprint('Train dataframe has: ', len(qualitative), 'qualitative features')","51f204e8":"train.info()","149b980a":"# Check for duplicates\ntrain['Id'].value_counts().sort_values(ascending = False)","14ae387d":"# Visualize missing data\nsns.set_style('ticks')\nmissing = train.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(ascending=False, inplace=True)\nmissing.plot.bar()\n\nprint(missing)\nprint('The number of features with missing data: %i' % missing.count())","99d02f08":"# Inspect the values for Electrical\nprint(train['Electrical'].value_counts())","a7fa3790":"# Drop missing data\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\ntrain = train.drop((missing_data[missing_data['Total'] > 1]).index, 1)\n\n# Fill missing Electrical data\ntrain['Electrical'] = train['Electrical'].fillna(train['Electrical'].value_counts().index[0])\n\n# Display sum of features with missing data\ntrain.isnull().sum().max()","a87fdc41":"# Inspect the values for Electrical after handling missing value\nprint(train['Electrical'].value_counts())","3f0db185":"quantitative = [i for i in train.columns if train.dtypes[i] != 'object']\nquantitative.remove('SalePrice')\nquantitative.remove('Id')\n\nqualitative = [i for i in train.columns if train.dtypes[i] == 'object']","f5ceb7de":"print(list(quantitative))\nprint()\nprint('Train dataframe has: ', len(quantitative), 'quantitative features')","2a615b42":"print(list(qualitative))\nprint()\nprint('Train dataframe has: ', len(qualitative), 'qualitative features')","30fca316":"print(train['Neighborhood'].value_counts())","b1f91419":"# Encode categorical features as ordered numbers\ntrain = train.replace({'MSZoning' : {'RL': 1, 'RM': 2, 'C (all)': 3, 'FV': 4, 'RH': 5}, \n                       'Street' : {'Grvl' : 1, 'Pave' : 2}, \n                       'LotShape' : {'IR3' : 1, 'IR2' : 2, 'IR1' : 3, 'Reg' : 4}, \n                       'LandContour': {'Lvl': 1, 'Bnk': 2, 'Low': 3, 'HLS': 4}, \n                       'Utilities' : {'ELO' : 1, 'NoSeWa' : 2, 'NoSewr' : 3, 'AllPub' : 4}, \n                       'LotConfig' : {'Inside': 1, 'FR2': 2, 'Corner': 3, 'CulDSac': 4, 'FR3': 5}, \n                       'LandSlope' : {'Sev' : 1, 'Mod' : 2, 'Gtl' : 3}, \n                       'Neighborhood' : {'CollgCr' : 1, 'OldTown' : 2, 'Edwards' : 3, \n                                         'Somerst' : 4, 'Gilbert' : 5, 'NridgHt' : 6, \n                                         'Sawyer' : 7, 'NWAmes' : 8, 'SawyerW' : 9, \n                                         'BrkSide' : 10, 'Crawfor' : 11, 'Mitchel' : 12, \n                                         'NoRidge' : 13, 'Timber' : 14, 'IDOTRR' : 15, \n                                         'ClearCr' : 16, 'StoneBr' : 17, 'SWISU' : 18, \n                                         'Blmngtn' : 19, 'MeadowV' : 20, 'BrDale' : 21, \n                                         'Veenker' : 22, 'NPkVill' : 23, 'Blueste' : 24, \n                                         'NAmes' : 25}, \n                       'Condition1' : {'Norm': 1, 'Feedr': 2, 'PosN': 3, 'Artery': 4, 'RRAe': 5, \n                                       'RRNn': 6, 'RRAn': 7, 'PosA': 8, 'RRNe': 9}, \n                       'Condition2' : {'Norm': 1, 'Artery': 2, 'RRNn': 3, 'Feedr': 4, 'PosN': 5,\n                                       'PosA': 6, 'RRAn': 7, 'RRAe': 8}, \n                       'BldgType' : {'1Fam': 1, '2fmCon': 2, 'Duplex': 3, 'TwnhsE': 4, 'Twnhs': 5}, \n                       'HouseStyle' : {'2Story': 1, '1Story': 2, '1.5Fin': 3, '1.5Unf': 4, 'SFoyer': 5, \n                                       'SLvl': 6, '2.5Unf': 7, '2.5Fin': 8}, \n                       'RoofStyle' : {'Gable': 1, 'Hip': 2, 'Gambrel': 3, 'Mansard': 4, 'Flat': 5, \n                                      'Shed': 6}, \n                       'RoofMatl' : {'CompShg': 1, 'WdShngl': 2, 'Metal': 3, 'WdShake': 4, 'Membran': 5, \n                                     'Tar&Grv': 6, 'Roll': 7, 'ClyTile': 8}, \n                       'Exterior1st' : {'VinylSd': 1, 'MetalSd': 2, 'Wd Sdng': 3, 'HdBoard': 4, 'BrkFace': 5, \n                                        'WdShing': 6, 'CemntBd': 7, 'Plywood': 8, 'AsbShng': 9, 'Stucco': 10, \n                                        'BrkComm': 11, 'AsphShn': 12, 'Stone': 13, 'ImStucc': 14, 'CBlock': 15},\n                       'Exterior2nd' : {'VinylSd': 1, 'MetalSd': 2, 'Wd Shng': 3, 'HdBoard': 4, 'Plywood': 5, \n                                        'Wd Sdng': 6, 'CmentBd': 7, 'BrkFace': 8, 'Stucco': 9, 'AsbShng': 10, \n                                        'Brk Cmn': 11, 'ImStucc': 12, 'AsphShn': 13, 'Stone': 14, 'Other': 15, \n                                        'CBlock': 16}, \n                       'ExterQual' : {'Po' : 1, 'Fa' : 2, 'TA': 3, 'Gd': 4, 'Ex' : 5},\n                       'ExterCond' : {'Po' : 1, 'Fa' : 2, 'TA': 3, 'Gd': 4, 'Ex' : 5},\n                       'Foundation' : {'PConc': 1, 'CBlock': 2, 'BrkTil': 3, 'Wood': 4, 'Slab': 5, \n                                       'Stone': 6}, \n                       'Heating' : {'GasA': 1, 'GasW': 2, 'Grav': 3, 'Wall': 4, 'OthW': 5, \n                                    'Floor': 6}, \n                       'HeatingQC' : {'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n                       'CentralAir' : {'Y': 1, 'N': 2}, \n                       'Electrical' : {'SBrkr': 1, 'FuseF': 2, 'FuseA': 3, 'FuseP': 4, 'Mix': 5}, \n                       'KitchenQual' : {'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n                       'Functional' : {'Sal' : 1, 'Sev' : 2, 'Maj2' : 3, 'Maj1' : 4, 'Mod': 5, \n                                       'Min2' : 6, 'Min1' : 7, 'Typ' : 8},\n                       'PavedDrive' : {'N' : 1, 'P' : 2, 'Y' : 3}, \n                       'SaleType' : {'WD': 1, 'New': 2, 'COD': 3, 'ConLD': 4, 'ConLI': 5, \n                                     'CWD': 6, 'ConLw': 7, 'Con': 8, 'Oth': 9}, \n                       'SaleCondition' : {'Normal': 1, 'Abnorml': 2, 'Partial': 3, 'AdjLand': 4, 'Alloca': 5, \n                                          'Family': 6}})","07f9b438":"# Correlation matrix\/heatmap after encoding\ncorrelation = train.corr()\nf, ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(correlation, vmax=.8, \n            cmap=sns.diverging_palette(20, 220, n=200), \n            square=True)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right');","1a0099ac":"# Zoomed-in heatmap\nn = 15\ncols = correlation.nlargest(n, 'SalePrice')['SalePrice'].index\ncorrelation = np.corrcoef(train[cols].values.T)\nf, ax = plt.subplots(figsize=(15, 15))\nsns.heatmap(correlation, cbar=True, annot=True, \n                 cmap=sns.diverging_palette(20, 220, n=200), \n                 square=True, \n                 fmt='.2f', annot_kws={'size': 15}, \n                 yticklabels=cols.values, \n                 xticklabels=cols.values)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right');","9a4af245":"# Differentiate numerical features (minus the target) and categorical features\ncategorical_features = train.select_dtypes(include = ['object']).columns\nnumerical_features = train.select_dtypes(exclude = ['object']).columns\nnumerical_features = numerical_features.drop('SalePrice')\nprint('Numerical features : ' + str(len(numerical_features)))\nprint('Categorical features : ' + str(len(categorical_features)))\ntrain_num = train[numerical_features]\ntrain_cat = train[categorical_features]","1f2f050e":"cols_to_eval = ['SalePrice', 'OverallQual', 'GrLivArea', 'ExterQual', \n                'KitchenQual', 'GarageCars', 'GarageArea', 'TotalBsmtSF', '1stFlrSF', 'FullBath', \n                'TotRmsAbvGrd', 'YearBuilt', 'YearRemodAdd', 'Fireplaces', 'HeatingQC', 'Neighborhood']\n\ncols_to_eval1 = ['SalePrice', 'OverallQual', 'GrLivArea', 'ExterQual']\n\ncols_to_eval2 = ['SalePrice', 'KitchenQual', 'GarageCars', 'GarageArea']\n\ncols_to_eval3 = ['SalePrice', 'TotalBsmtSF', '1stFlrSF', 'FullBath']\n\ncols_to_eval4 = ['SalePrice', 'TotRmsAbvGrd', 'YearBuilt', 'YearRemodAdd']\n\ncols_to_eval5 = ['SalePrice', 'Fireplaces', 'HeatingQC', 'Neighborhood']","75f4f487":"sns.set()\n\nsns.pairplot(train[cols_to_eval1], height = 4, aspect = 1.2)\nplt.show();","9e816d6a":"sns.set()\n\nsns.pairplot(train[cols_to_eval2], height = 4, aspect = 1.2)\nplt.show();","1ecaddd8":"sns.set()\n\nsns.pairplot(train[cols_to_eval3], height = 4, aspect = 1.2)\nplt.show();","e1033b63":"sns.set()\n\nsns.pairplot(train[cols_to_eval4], height = 4, aspect = 1.2)\nplt.show();","320bd138":"sns.set()\n\nsns.pairplot(train[cols_to_eval5], height = 4, aspect = 1.2)\nplt.show();","2ad7cca4":"from scipy.stats import skew\n\n# Log transform skewed numerical features to lessen impact of outliers\ncols_quant = ['MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', \n              'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', \n              'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', \n              'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', \n              'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', \n              'MiscVal', 'MoSold', 'YrSold']\nskewness = train[cols_quant].apply(lambda x: skew(x))\nskewness = skewness[abs(skewness) > 0.5]\nprint(str(skewness.shape[0]) + \" skewed numerical features were log transformed\")\nskewed_features = skewness.index\ntrain[skewed_features] = np.log1p(train[skewed_features])","c05577d7":"# Check for normality\nplt.figure(1); plt.title('Normal')\nsns.distplot(train['SalePrice'], kde=False, fit=stats.norm)\n\nplt.figure(2); plt.title('Log Normal')\nsns.distplot(train['SalePrice'], kde=False, fit=stats.lognorm)\n\nplt.figure(3); plt.title('Johnson SU')\nsns.distplot(train['SalePrice'], kde=False, fit=stats.johnsonsu)","c52e8056":"# Probability plot\nres = stats.probplot(train['SalePrice'], plot=plt)","574af762":"# Summary statistics\ntrain['SalePrice'].describe()","79e86046":"print(\"Skewness: %.02f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %.02f\" % train['SalePrice'].kurt())","0d062275":"# Standardizing SalePrice\nsaleprice_scaled = StandardScaler().fit_transform(train['SalePrice'][:,np.newaxis]);\nlow_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\nhigh_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\nprint('outer range (low) of the distribution:')\nprint(low_range)\nprint('\\nouter range (high) of the distribution:')\nprint(high_range)","6d61852a":"# Bivariate analysis SalePrice\/YearBuilt\nvar = 'YearBuilt'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","ec9c54b1":"# Bivariate analysis SalePrice\/OverallQual\nvar = 'OverallQual'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","9f4171db":"# Bivariate analysis SalePrice\/OverallCond\nvar = 'OverallCond'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","861b9530":"# Bivariate analysis SalePrice\/Neighborhood\nvar = 'Neighborhood'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","54b51579":"# Bivariate analysis SalePrice\/GRLivArea\nvar = 'GrLivArea'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","a0fe283e":"# Bivariate analysis SalePrice\/KitchenQual\nvar = 'KitchenQual'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","519c7988":"# Delete outliers\ntrain = train[train.GrLivArea < 4500]\ntrain.reset_index(drop=True, inplace=True)\n\ntrain = train[train.TotalBsmtSF < 3000]\ntrain.reset_index(drop=True, inplace=True)","92c8c311":"# Log transformation\ntrain['SalePrice'] = np.log(train['SalePrice'])","48aed7b0":"# Visualize transformed histogram and normal probability plot\nsns.distplot(train['SalePrice'], fit=stats.norm)\nplt.figure()\n\nres = stats.probplot(train['SalePrice'], plot=plt)","3445a951":"# Combine categorical and numerical features\ncols_for_mod = ['MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', \n                'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', \n                'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', \n                'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', \n                'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', \n                'MiscVal', 'MoSold', 'YrSold', 'MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', \n                'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', \n                'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'ExterQual', 'ExterCond', 'Foundation', \n                'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'PavedDrive', \n                'SaleType', 'SaleCondition', 'SalePrice']\n\ntrain = train[cols_for_mod]\n\nprint('Shape of dataframe: ' + str(train.shape))\nprint()\nprint('Column names: ' + str(list(train.columns)))\nprint()\n\n# DataFrame with features only\ntrain_fr = train.copy()\ntrain_fr.drop(['SalePrice'], inplace = True, axis = 1)\n\n# Segregate features and labels into separate variables\nX,y = train_fr, train['SalePrice']\n\nprint('Number of independent variables: ' + str(X.shape[1]))\nprint()\n\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n\n# Standardize features\nstd = StandardScaler()\nX = std.fit_transform(X)\n\n# Partition the dataset in train + validation sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\nprint('X_train : ' + str(X_train.shape))\nprint('X_test : ' + str(X_test.shape))\nprint('y_train : ' + str(y_train.shape))\nprint('y_test : ' + str(y_test.shape))","58f7e9b1":"# RMSE\ndef rmse_cv_train(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv = 5))\n    return(rmse)\n\ndef rmse_cv_test(model):\n    rmse= np.sqrt(-cross_val_score(model, X_test, y_test, scoring='neg_mean_squared_error', cv = 5))\n    return(rmse)","1e4327e9":"# Linear Regression Without Regularization\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\n# Look at predictions on training and validation set\nprint('RMSE on Train set :', rmse_cv_train(lr).mean())\nprint('RMSE on Test set :', rmse_cv_test(lr).mean())\n\ny_train_pred = lr.predict(X_train)\ny_test_pred = lr.predict(X_test)\n\n# Plot residuals\nplt.scatter(y_train_pred, y_train_pred - y_train, c = 'blue', marker = 's', label = 'Train data')\nplt.scatter(y_test_pred, y_test_pred - y_test, c = 'lightgreen', marker = 's', label = 'Validation data')\nplt.title('Linear Regression')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.legend(loc = 'upper left')\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = 'red')\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_pred, y_train, c = 'blue', marker = 's', label = 'Train data')\nplt.scatter(y_test_pred, y_test, c = 'lightgreen', marker = 's', label = 'Validation data')\nplt.title('Linear Regression')\nplt.xlabel('Predicted Values')\nplt.ylabel('Real Values')\nplt.legend(loc = 'upper left')\nplt.plot([10.5, 13.5], [10.5, 13.5], c = 'red')\nplt.show()","2745296a":"X_df = pd.DataFrame(X_train, columns = train_fr.columns)\n\nprint(X_df.head())","200bd547":"# LASSO Regression With L1 Regularization\n\nlasso = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, \n                          0.3, 0.6, 1], \n                max_iter = 50000, cv = 10)\nlasso.fit(X_train, y_train)\nalpha = lasso.alpha_\nprint('Best alpha :', alpha)\n\nprint('More precision with alphas centered around ' + str(alpha))\nlasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, \n                          alpha * .85, alpha * .9, alpha * .95, alpha, alpha * 1.05, \n                          alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, alpha * 1.35, \n                          alpha * 1.4], \n                max_iter = 50000, cv = 10)\nlasso.fit(X_train, y_train)\nalpha = lasso.alpha_\nprint('Best alpha :', alpha)\n\nprint('LASSO RMSE on Train set :', rmse_cv_train(lasso).mean())\nprint('LASSO RMSE on Test set :', rmse_cv_test(lasso).mean())\ny_train_las = lasso.predict(X_train)\ny_test_las = lasso.predict(X_test)\n\n# Plot residuals\nplt.scatter(y_train_las, y_train_las - y_train, c = 'blue', marker = 's', label = 'Train data')\nplt.scatter(y_test_las, y_test_las - y_test, c = 'lightgreen', marker = 's', label = 'Validation data')\nplt.title('Linear Regression with LASSO Regularization')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.legend(loc = 'upper left')\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = 'red')\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_las, y_train, c = 'blue', marker = 's', label = 'Train data')\nplt.scatter(y_test_las, y_test, c = 'lightgreen', marker = 's', label = 'Validation data')\nplt.title('Linear Regression with LASSO Regularization')\nplt.xlabel('Predicted values')\nplt.ylabel('Real values')\nplt.legend(loc = 'upper left')\nplt.plot([10.5, 13.5], [10.5, 13.5], c = 'red')\nplt.show()\n\n# Plot important coefficients\ncoefs = pd.Series(lasso.coef_, index = X_df.columns)\nprint('LASSO picked ' + str(sum(coefs != 0)) + ' features and eliminated ' +  \\\n      str(sum(coefs == 0)) + ' features')\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = 'barh')\nplt.title('Coefficients in the LASSO Model')\nplt.show()","800c0df1":"# Ridge Regression With L2 Regularization\n\nridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])\nridge.fit(X_train, y_train)\nalpha = ridge.alpha_\nprint('Best alpha :', alpha)\n\nprint('More precision with alphas centered around ' + str(alpha))\nridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4], \n                cv = 10)\nridge.fit(X_train, y_train)\nalpha = ridge.alpha_\nprint('Best alpha :', alpha)\n\nprint('Ridge RMSE on Train set :', rmse_cv_train(ridge).mean())\nprint('Ridge RMSE on Test set :', rmse_cv_test(ridge).mean())\ny_train_rdg = ridge.predict(X_train)\ny_test_rdg = ridge.predict(X_test)\n\n# Plot residuals\nplt.scatter(y_train_rdg, y_train_rdg - y_train, c = 'blue', marker = 's', label = 'Train data')\nplt.scatter(y_test_rdg, y_test_rdg - y_test, c = 'lightgreen', marker = 's', label = 'Validation data')\nplt.title('Linear Regression with Ridge Regularization')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.legend(loc = 'upper left')\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = 'red')\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_rdg, y_train, c = 'blue', marker = 's', label = 'Train data')\nplt.scatter(y_test_rdg, y_test, c = 'lightgreen', marker = 's', label = 'Validation data')\nplt.title('Linear Regression with Ridge Regularization')\nplt.xlabel('Predicted Values')\nplt.ylabel('Real Values')\nplt.legend(loc = 'upper left')\nplt.plot([10.5, 13.5], [10.5, 13.5], c = 'red')\nplt.show()\n\n# Plot important coefficients\ncoefs = pd.Series(ridge.coef_, index = X_df.columns)\nprint('Ridge picked ' + str(sum(coefs != 0)) + ' features and eliminated ' +  \\\n      str(sum(coefs == 0)) + ' features')\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = 'barh')\nplt.title('Coefficients in the Ridge Model')\nplt.show()","58e409ba":"from sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\n\ngbr = GradientBoostingRegressor(n_estimators=3500, \n                                learning_rate=0.01, \n                                max_depth=3, \n                                max_features='sqrt', \n                                min_samples_leaf=15, \n                                min_samples_split=15, \n                                loss='huber', \n                                random_state=42)  \n\nxgboost = XGBRegressor(learning_rate=0.01, \n                       n_estimators=3500, \n                       max_depth=3, \n                       min_child_weight=0, \n                       gamma=0, \n                       subsample=0.7, \n                       colsample_bytree=0.7, \n                       objective='reg:squarederror', \n                       nthread=-1, \n                       scale_pos_weight=1, \n                       seed=42, \n                       reg_alpha=0.00006)\n\nprint('RMSE for Gradient Boosting on Train Set :', rmse_cv_train(gbr).mean())\nprint('RMSE for Gradient Boosting on Test Set :', rmse_cv_test(gbr).mean())\n\nprint()\n\nprint('RMSE for XGBoost on Train Set :', rmse_cv_train(xgboost).mean())\nprint('RMSE for XGBoost on Test Set :', rmse_cv_test(xgboost).mean())","1c78e65a":"# Examing Scatter Plots for Outliers","f94d212e":"There are no duplicate house entries in the dataset.","6ec6a537":"# References\n\nKaggle\n- https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n- https:\/\/www.kaggle.com\/jesucristo\/1-house-prices-solution-top-1?scriptVersionId=20214677\n- https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset\n- https:\/\/www.kaggle.com\/dgawlik\/house-prices-eda\n\nMedium\n- https:\/\/medium.com\/@kyawsawhtoon\/log-transformation-purpose-and-interpretation-9444b4b049c9\n- https:\/\/towardsdatascience.com\/how-do-you-check-the-quality-of-your-regression-model-in-python-fa61759ff685\n\nBooks\n- G\u00e9ron, Aur\u00e9lien: Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\n- Hair et al. (2013): https:\/\/amzn.to\/2uC3j9p\n\nOther\n- https:\/\/www.reneshbedre.com\/blog\/linear-regression.html\n- https:\/\/www.scikit-yb.org\/en\/latest\/api\/regressor\/residuals.html\n- https:\/\/www.statisticshowto.com\/regularized-regression\/\n- https:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/complete-guide-parameter-tuning-gradient-boosting-gbm-python\/\n- https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/","053e2cb8":"# Determining Most Important Variables ","75f0027b":"# Standardizing Data\n\nStandardization will convert the data to have a mean of 0 and a standard deviation of 1. ","54d2be9f":"# Data Processing and Summarizing Data","e6e59277":"Since there are both categorical and quantitative variables in the dataset, it's necessary to come up with a strategy to quantify the categorical features so that they can be included in the analysis and later in the models. Categorical features will be encoded using a dictionary to map all of the unique values to numbers that can be used in the model.","8fb72836":"### LASSO Regression","85f1c673":"The RMSE scores remain fairly consistent with those of the regression analysis. The Train versus Test RMSE scores remain close to each other, which shows that overfitting is not occurring. The following notes were used to tune the models.\n\nTuning the Gradient Boosting Model:\n- Tree-Specific Parameters -- affect each individual tree in the model.\n    - max_depth: Used to control overfitting. Higher depths allow for the model to learn more about the relationships in the data. Typical values: 3-10\n    - max_features: Generally, the square root of the total number of features works well, but checking 30-40% of the total number of features is a good idea. Higher values can lead to overfitting.\n    - min_samples_leaf: Used to control overfitting.\n    - min_samples_split: Used to control overfitting. The higher the minimum number of samples, the better the generalizations the model can make, however, too high can lead to underfitting since it is considering more samples at each node before splitting.\n- Boosting Parameters -- affect the boosting operation in the model.\n    - n_estimators: Should be balanced with the learning_rate. Higher numbers can overfit, but Gradient Boosting performs well with high numbers. Higher number of trees can also be computationally expensive.\n    - learning_rate: Lower rates allow for the model to generalize well, but a higher number of trees will be needed. This should be balanced because a higher number of trees requires increasing computational power. Generally, 0.01-0.2 is the range used.\n- Miscellaneous Parameters -- parameters for overall functioning.\n    - loss: The loss function to be minimized at each split. \n    - random_state: Needed so that same random numbers are generated every time for each parameter. It is helpful to keep the random_state constant across models so that performance metrics can be compared. \n                                \nTuning the XGBoost Model:\n- General Parameters -- overall functioning\n    - nthread: Used for parallel processing and number of cores\n- Booster Parameters -- guide the individual booster (tree\/regression) at each step\n    - n_estimators: Should be balanced with the learning_rate. Higher numbers can overfit, but Gradient Boosting performs well with high numbers. Higher number of trees can also be computationally expensive.\n    - learning_rate (eta): Lower rates allow for the model to generalize well, but a higher number of trees will be needed. This should be balanced because a higher number of trees requires increasing computational power. Typical values: 0.01-0.2 \n    - max_depth: Used to control overfitting. Higher depths allow for the model to learn more about the relationships in the data. Typical values: 3-10\n    - min_child_weight: Similar to min_child_leaf in GBM. Used to control overfitting. Higher values help generalize, but too high could lead to underfitting. Choose small values if dealing with a highly imbalanced class problem.\n    - gamma: Specifies the minimum loss reduction required to make a split. Makes the algorithm conservative.\n    - subsample: The fraction of observations to randomly sample for each tree. Lower values prevent overfitting (makes the algorithm conservative), but too low could lead to underfitting and high generalization. Typical values: 0.5-1\n    - colsample_bytree: Similar to max_features in GBM. Higher values can lead to overfitting. Typical values: 0.5-1\n    - reg_alpha: L1 regularization term on weight (LASSO) and can be used in cases where there is high dimensionality\n    - scale_pos_weight: A value greater than 0 can be used when there is high class imbalance. A value of 1 can be used in a case of high class imbalance.\n- Learning Task Parameters -- guide the optimization performed\n    - objective: Defines the loss function.\n    - seed: Random number seed.","594a6fe3":"There is a positive, linear trend between SalePrice and KitchenQual though the degree of scatter indicates a weak relationship that continues to weaken as KitchenQual increases. ","19901485":"The most important features in determining SalePrice are OverallQual, GrLivArea, ExterQual, KitchenQual, GarageCars, GarageArea, TotalBsmtSF, 1stFlrSF, FullBath, TotRmsAbvGrd, YearBuilt, and YearRemodAdd. Here, I am defining 'important features' as those having correlations greater than 0.50. So far, four out of six (YearBuilt, OverallQual, GrLivArea, and KitchenQual) of the features in my prediction have been shown to be moderately to highly correlated.","8d5ae8c6":"### Gradient Boosting and XGBoost","ada50aba":"Residuals are the difference between the observed value of the target value and the predicted value (the error of prediction). In the residual plot, the predicted values are on the x-axis and the error on the y-axis. The residuals are fairly randomly and uniformly disbursed around the zero line (meaning a linear regresion model is appropriate for the data) and cluster around it, so the model is performing well. \n\nThe Root Mean Squared Error (RMSE) score is pretty low on the Train set, but high in the Test set. The RMSE gives an estimate of spread of observed data points across the predicted regression line. There is a large difference between the RMSE on Train set (0.2058753430825945) and the RMSE on Test set (30350035354.640392), so it looks like the model is overfitting. The low RMSE score on the Training set shows that it is fitting the data well there. Let's take a look at other models to see if we can see improved performance.","f155b43a":"Regularized regression adds penalties to the loss function during training, which constrains the coefficient estimates to zero. The loss function is a measure that indicates how well the model\u2019s coefficients have fit the underlying data. Regularization assists with avoiding overfitting and reduces model complexity, which can occur in simple linear regression. It is a useful method for handling collinearity and filter out noise from data.\n\nLASSO regression (L1 regularization) limits the size of the coefficient by adding an L1 penalty equal to the absolute value of the magnitude of coefficients. That is, we sum the absolute value of the weights. This type of regression uses shrinkage, which is where data values are shrunk towards a central point such as the mean. LASSO stands for Least Absolute Shrinkage and Selection Operator. LASSO regression can help with feature selection as it can lead to zero coefficients for some of the features, which means they will be completely neglected for the evaluation of the output. This is useful when we have a high dimensional dataset with features that are irrelevant.","d182b1c3":"### Linear Regression","68ba838a":"SalePrice and YearBuilt have a somewhat linear positive relationship to each other with a shallow slope. This scatterplot reveals that these two variables are not strongly related. Some of the years have vertically clustered data points. This could be due to innacurate records since they tend to occur at 1900, 1920, 1940, etc. ","e7c7314e":"# Modeling","569de0a0":"# Analyzing SalePrice","86953d7a":"A closer look at these variables and their relationship to SalePrice shows that these variables are not as closely related as one might first intuit. Of the initial predictions, OverallQual, GrLivArea, and KitchenQual have the strongest relationship with SalePrice, though the relationships all seem to weaken as these features increase. It is surprising the YearBuilt and Neigborhood don't have a stronger relationship. The relationship seen here between SalePrice and YearBuilt can be explained by the fact that people tend to remodel homes and make improvements throughout their occupancy. Given that the Neighborhood feature is represented here in a nominal manner (no ordering is possible or implied in the levels), a relationship does not emerge in this analysis.","92617051":"# Handling Duplicate Data","fa54a1c5":"There does not appear to be a trend between SalePrice and Neighborhood. ","ca9204b6":"### Ridge Regression","f7d3725a":"There is a positive, slightly nonlinear trend between SalePrice and OverallQual. In general, as OverallQual increases, SalePrice increases. Because OverallQual is described using discrete values, data points are represented as vertically clustered lines at each x-value. ","4beddfe1":"Looking at this heat map, it is evident that OverallQual, GrLivArea, ExterQual, GarageCars, and GarageArea have strong relationships with SalePrice. Let's zoom into the features with the highest correlations.","22dca26a":"At a glance, various features are missing large amounts of values. Thankfully, the columns of interest for our prediction are pretty complete. Missing values will be examined further below and further analysis will be performed to determine variables that are most highly correlated with SalePrice.","40ca7442":"It is important to understand how much missing data exists in the dataset, where they exists, and whether they are missing at random or systemically. Understanding this will affect the strategy used to handle missing data and can help reveal whether bias exists.","39eebc52":"# Introduction\n\nWhen it comes to property values, it is common to hear the phrase, \"location, location, location\", as the variable that dominates in influence on overall home value. But what other factors come into play in determining the price of a home? These factors are examined in this dataset. My prediction is that variables such as YearBuilt, OverallQual, OverallCond, Neighborhood, GrLivArea, KitchenQual will play the biggest roles in determining SalePrice.","23cc71a8":"Gradient Boosting is an improvement on decision tress. New trees fit on an improved version of the training dataset by fitting the new predictor to the residual errors made by the previous predictor until no further improvements can be made. Decision tree-based algorithms are best for small to medium structured\/tabular datasets.\n\nXGBoost is another variant of the Gradient Boost Machine algorithm that is higher-performing. It is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. XGBoost can be used for regression, classification, ranking, and user-defined problems.","82d5558b":"SalePrice does not have a normal distribution, so it will need to be transformed before performing regression. For SalePrice, the best fit is the Unbounded Johnson Distribution as seen above. Log transformation can smooth out the data, removing the skew, to make it more 'normal', which will allow the statistical analysis results from these data to be more valid.","1b4a04d1":"Preparing data for modeling","c439925c":"# Analysis Strategy\n\nThe following is the analysis strategy to predict SalesPrice and determine which variables play the biggest roles in SalePrice. As the final step, various modeling techniques will be used on the entire dataset (without features that had too many missing values).\n\n1. Clean the data\n    - Process and summarize data\n    - Handle duplicate and missing data\n    - Remove irrelevant data\n\n2. EDA Plan\n    - Determine quantitative and qualitative features\n    - Encode qualitative features\n    - Determine most important features using heat maps\n    - Analyze dependent variable, SalePrice\n\n3. Modeling\n    - Linear Regression\n    - Lasso Regression\n    - Ridge Regression\n    - Gradient Boosting and XGBoost \n    \n4. Conclusions\n5. References","ac9bcaee":"The RMSE scores for LASSO regression show quite an improvement! The RMSE score on the Train set is 0.13368814791535225 and 0.15266185052830566 for the Test set. In addition, LASSO picked 41 features and eliminated 20 features. Alpha is the parameter that balances the amount of emphasis given to minimizing the residual sum of squares vs minimizing sum of square of coefficients.\n\nThe highest weights have been assigned to GrLivArea, OverallQual, YearBuilt, 1stFlrSF, and LotArea. This is somewaht in line with the earlier predictions that YearBuilt, OverallQual, and GrLivArea play the biggest roles in determining SalePrice! The features LASSO picked make sense on an intuitive level.","5c0aebd4":"# Handling Outliers\n\nIn general, a skewness with absolute value > 0.5 is considered at least moderately skewed","ba793354":"# Bivariate Analysis\n\nMy prediction was that YearBuilt, OverallQual, OverallCond, Neighborhood, GrLivArea, and KitchenQual play the biggest roles in determining SalePrice. Let's take a closer look at these relationships with bivariate analysis.","e883420a":"There is a positive, linear trend between SalePrice and GrLivArea whose relationship weakens as GrLivArea increases and has a moderately steep slope. There are some outliers above 4,000 square feet. The two that have a SalePrice of greater than 700,000 may be located in an area that is highly desireable. However, they follow the overall trend of the data, so they will remain in the analysis. The two that have a SalePrice of less than 200,000 may be located in rural areas or areas that are not desireable. These two data points are not representative of the rest of the dataset, so these can be removed to avoid errors in the analysis.  ","72d0279b":"# Normality\n\nNormality for SalePrice has already been examined by visualizing its distribution plot, examining skew and kurtosis, and observing its probability plot. It is not normal, shows peakedness, positive skewness, and does not follow a diagonal line in its probability plot. Log transformations can help with positive skewness.","1a4d4237":"The RMSE scores were slightly better with LASSO regression than with Ridge regression. The RMSE score on the Train set is 0.13582790751257817 and on the Test set is 0.15671943639084562 Ridge did not eliminate any features given that Ridge Regression does not make coefficients absolute zero whereas LASSO Regression will. Alpha is the parameter that balances the amount of emphasis given to minimizing the residual sum of squares vs minimizing sum of square of coefficients\n\nThe features with the highest weights are OverallQual, GrLivArea, and 1stFlrSF.","5511976e":"There are 1460 records of training data and 81 attributes\/features. 36 of these features are quantitative and 43 are categorical. The next step is to take a deeper dive into all of these features to determine which of these may weaken our analysis for reasons such as large amounts of missing data. It is also necessary to develop a strategy for the categorical features to make them usable in our models.","e40dc436":"# Imports and Data Loading","72beb029":"Ridge regression is used for estimating the coefficients where independent variables are highly correlated. The cost function is altered by adding a penalty equivalent to square of the magnitude of the coefficients. The cost function can be used to show how badly a model is performing. Ridge regression helps understand the degree of overfitting of the data by measuring the magnitude of the coefficients.","4e0a9811":"# Handling Missing Data","4f7a2e52":"# Conclusions\n\nLASSO and Ridge regression showed that OverallQual and GrLivArea are important features in predicting SalePrice. My initial prediction was that YearBuilt, OverallQual, OverallCond, Neighborhood, GrLivArea, KitchenQual were the features that would contribute most to SalePrice. Of my predictions, all but Neighborhood and KitchenQual were selected as important features in the various regression analyses. It was surprising that Neighborhood did not emerge as an important feature in the analyses. I believe this can be explained by the fact that the Neighborhood feature is represented in the dataset in a nominal manner (no ordering is possible or implied in the levels). A better analysis could be made if the feature were represented as neighborhood quality.","c013bb30":"Regression is a supervised learning task used to predict a target numeric value from a set of related features. Linear regression assumes a linear relationship between the target and inputs.","92a577a3":"There is a positive, fainlty linear trend between SalePrice and OverallQual whose relationship weakens as OverallQual increases and has a gentle slope.","679d0399":"The index range is 1460 with a total of 81 columns. Columns with more than 200 missing data can be removed as that is approximately 15% of the data missing. Attempting to fill these data will result in making these features weak predictors. It is better to leave them out entirely. \n\nThe various Garage- features can also be deleted. They don't seem to be features that would contribute heavily to the overall house price. The feature, 'GarageCars', is likely more correlated with the house price given that the number of cars that are able to be parked in the garage is the most important aspect of the garage size. \n\nThe Bsmnt- features could also be deleted given that the most important information is contained in the 'TotalBsmtSF' feature. \n\nThe MasVnr- features are not important as they don't show strong correlations with any of the other features in the heat map. \n\nFor 'Electrical', since there is only one record that is missing, the record can be filled with the most common value for this feature."}}