{"cell_type":{"d277e04e":"code","2b0ed7d9":"code","946d89f7":"code","94264dce":"code","9041e735":"code","e9830a4c":"code","51373c91":"code","dd8cd59e":"code","1ccf452f":"code","39f67143":"code","6a0caf5d":"code","1e63b3c8":"code","0862feee":"code","8b0cf0eb":"code","00a6221f":"code","3f0df8c0":"code","f59a1673":"code","14d1517b":"code","8f2a9871":"code","3684b7c5":"code","76be07af":"code","9977d01d":"code","b238c5af":"code","acecbd35":"code","fc0d4413":"markdown","e0f467ed":"markdown","270d318d":"markdown","6714b59a":"markdown","d07766e4":"markdown"},"source":{"d277e04e":"from  datetime import datetime, timedelta\nimport time\nimport gc\nimport numpy as np, pandas as pd\nimport gc","2b0ed7d9":"from fastai import *      # import * is considered as a bad coding practice! will have to change this!\nfrom fastai.tabular import *","946d89f7":"start_nb = time.time()","94264dce":"CAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n         \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n        \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\nPRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }\n\npd.options.display.max_columns = 50\nh = 28 \nmax_lags = 70\ntr_last = 1913\nfday = datetime(2016,4, 25) ","9041e735":"def create_dt(is_train = True, nrows = None, first_day = 1200, store_id = None):\n    \n    start = time.time()\n    if store_id == None and is_train:\n        print(\"ERROR: No store_id provided.Please provide an id [0-9]\")\n        return None\n        \n    prices = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/sell_prices.csv\", dtype = PRICE_DTYPES)\n    \n    \n    for col, col_dtype in PRICE_DTYPES.items():\n        if col_dtype == \"category\":\n            prices[col] = prices[col].cat.codes.astype(\"int16\")\n            prices[col] -= prices[col].min()\n            \n    cal = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/calendar.csv\", dtype = CAL_DTYPES)\n    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n    for col, col_dtype in CAL_DTYPES.items():\n        if col_dtype == \"category\":\n            cal[col] = cal[col].cat.codes.astype(\"int16\")\n            cal[col] -= cal[col].min()\n    \n    # Filter out the values for store_id\n    if is_train:\n        prices = prices[prices['store_id'] == store_id]\n        \n    print(f\"Shape of Store - {store_id} dataframe : \", prices.shape)\n    \n    \n    start_day = max(1 if is_train  else tr_last-max_lags, first_day)\n    numcols = [f\"d_{day}\" for day in range(start_day,tr_last+1)]\n    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n    dtype = {numcol:\"float32\" for numcol in numcols} \n    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n    dt = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/sales_train_validation.csv\", \n                     nrows = nrows, usecols = catcols + numcols, dtype = dtype)\n    \n    for col in catcols:\n        if col != \"id\":\n            dt[col] = dt[col].cat.codes.astype(\"int16\")\n            dt[col] -= dt[col].min()\n    \n    if not is_train:\n        for day in range(tr_last+1, tr_last+ 28 +1):\n            dt[f\"d_{day}\"] = np.nan\n    \n    dt = pd.melt(dt,\n                  id_vars = catcols,\n                  value_vars = [col for col in dt.columns if col.startswith(\"d_\")],\n                  var_name = \"d\",\n                  value_name = \"sales\")\n    \n    dt = dt.merge(cal, on= \"d\", copy = False)\n    dt = dt.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n    end = time.time()\n    \n    print(\"Processing time: \", (end-start))\n    return dt","e9830a4c":"def create_fea(dt):\n    start = time.time()\n    lags = [7, 28]\n    lag_cols = [f\"lag_{lag}\" for lag in lags ]\n    for lag, lag_col in zip(lags, lag_cols):\n        dt[lag_col] = dt[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n\n    wins = [7, 28]\n    for win in wins :\n        for lag,lag_col in zip(lags, lag_cols):\n            dt[f\"rmean_{lag}_{win}\"] = dt[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean())\n   \n    \n    date_features = {\n        \n        \"wday\": \"weekday\",\n        \"week\": \"weekofyear\",\n        \"month\": \"month\",\n        \"quarter\": \"quarter\",\n        \"year\": \"year\",\n        \"mday\": \"day\",\n#         \"ime\": \"is_month_end\",\n#         \"ims\": \"is_month_start\",\n    }\n    \n#     dt.drop([\"d\", \"wm_yr_wk\", \"weekday\"], axis=1, inplace = True)\n    \n    for date_feat_name, date_feat_func in date_features.items():\n        if date_feat_name in dt.columns:\n            dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n        else:\n            dt[date_feat_name] = getattr(dt[\"date\"].dt, date_feat_func).astype(\"int16\")\n    \n    # Drop NA values\n    dt.dropna(inplace = True)     \n    \n    # Sort the dataframe on 'saledate' so we can easily create a validation set that data is in the 'future' of what's in the training set\n    dt = dt.sort_values(by='date', ascending=False)\n    dt = dt.reset_index(drop=True)\n    end = time.time()\n    print(\"Processing time: \", (end- start))\n    \n    return dt","51373c91":"import shutil\n\ndef checkDiskSpace():\n    total, used, free = shutil.disk_usage(\"\/\")\n    \n    free = (free \/\/ (2**30))\n    \n    if free < 10:\n        return -1\n    else:\n        return 0 \n         \n\n# print(\"Total: %d GiB\" % (total \/\/ (2**30)))\n# print(\"Used: %d GiB\" % (used \/\/ (2**30)))\n# print(\"Free: %d GiB\" % (free \/\/ (2**30)))","dd8cd59e":"cat_feats = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id'] + [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\nuseless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\"]\n#train_cols = df.columns[~df.columns.isin(useless_cols)]","1ccf452f":"FIRST_DAY = 1 # If you want to load all the data set it to '1' -->  Great  memory overflow  risk !","39f67143":"idx = 1\ntrain_df = create_dt(is_train=True, first_day= FIRST_DAY, store_id = idx)\ntrain_df = create_fea(train_df)\nprint(train_df.shape)\nprint(train_df['date'].min(), train_df['date'].max())","6a0caf5d":"# Sort by date (used for train\/validation splits)\ntrain_df.sort_values(by='date', inplace=True)\n\n# convert sales value to log scale\ntrain_df['sales'] = np.log(train_df['sales'] + 1)  # Taking logarithm values for sales\n","1e63b3c8":"\n# Calculate where we should cut the validation set. We pick the most recent 'n' records in training set \n# where n is the number of entries in test set. \n\ncut = train_df['date'][(train_df['date'] == train_df['date'][62500])].index.max()\nprint(cut)\nvalid_idx = range(cut)\n","0862feee":"# Define categorical, continous & dependent variables\n\ncat_vars = ['wm_yr_wk', 'weekday', 'wday', 'month', 'year', 'event_name_1',\n           'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX',\n           'snap_WI', 'week', 'quarter', 'mday']\n\ncont_vars = ['lag_7', 'lag_28', 'rmean_7_7', 'rmean_28_7',\n           'rmean_7_28', 'rmean_28_28', 'sell_price']\n\ndep_var = 'sales'\n","8b0cf0eb":"# We want to limit the price range for our prediction to be within the history sale price range, so we need to calculate the y_range\n# Note that we multiplied the maximum of 'SalePrice' by 1.2 so when we apply sigmoid, the upper limit will also be covered. \nmax_y = np.max(train_df['sales'])*1.2\ny_range = torch.tensor([0, max_y], device=defaults.device)\nprint(y_range)","00a6221f":"\n# Defining pre-processing we want for our fast.ai DataBunch\nprocs=[FillMissing, Categorify, Normalize]\n\n# Use fast.ai datablock api to put our training data into the DataBunch, getting ready for training\ndata = (TabularList.from_df(train_df, cat_names=cat_vars, cont_names=cont_vars, procs=procs)\n                       .split_by_idx(valid_idx)\n                       .label_from_df(dep_var)\n                       .databunch())\n\n\n# Create our tabular learner. The dense layer is 1000 and 500 two layer NN. We used dropout, hai \nlearn = tabular_learner(data, layers=[512,256, 128], ps=[0.05,0.01, 0.5], emb_drop=0.04, \n                            y_range=y_range, metrics=rmse)\n","3f0df8c0":"learn.lr_find()","f59a1673":"# Plot the learning rates\nlearn.recorder.plot()","14d1517b":"# learn.fit_one_cycle(3, 1e-2, wd=0.2)\n    \n# print(f\"Saving model...export_{idx}\")\n# learn.export(file = Path(f\"\/kaggle\/working\/export_{idx}.pkl\")) # Save the model\n","8f2a9871":"learn = load_learner('\/kaggle\/input\/m5-forecasting-models\/', file=f'export_{idx}.pkl')","3684b7c5":"%%time\nte = create_dt(False)\nte = create_fea(te)","76be07af":"print(te.shape)\nte.head()","9977d01d":"learn.predict(te.loc[1])[1]","b238c5af":"print(\"Raw prediction: \",learn.predict(te.loc[1]))\nprint(\"Taking exponentials: \",(np.exp(learn.predict(te.loc[1])[1]) -1) )\n","acecbd35":"end_nb = time.time()\n\nprint(\"Notebook processing time: \", (end_nb- start_nb))","fc0d4413":"# Context\n\nTraditionally we transfer categorical variables to one-hot encoding. The disadvantage is that this assumes independence among entities & creates a sparse matrix. This can lead to out-of-memory (OOM) while training. \n\nEntity embeddings instead use a vector to represent each entity. So each categorical variable is represented as a vector of floating points. Embeddings can does capture richer relationships & complexities. \n\n\n**Related paper**\n\n[Entity Embeddings of Categorical Variables](https:\/\/arxiv.org\/abs\/1604.06737) \n\nRachal Thomas' blog on Fast.ai [Categorical Embeddings](https:\/\/www.fast.ai\/2018\/04\/29\/categorical-embeddings\/)\n\n# Process flow\n\n>> I am running this on CPU, but you can add a GPU for faster training (*around 3x speedup*)\n\n1. Feature engineering is done for each store (store_id). Features are similar to this [notebook by kkiler](https:\/\/www.kaggle.com\/kneroma\/m5-first-public-notebook-under-0-50) \n2. `TabularList` is used to create a DataBunch for store_id = 1\n3. Model is trained for a single store\n4. Separate models are trained for each store (*Not shown here. Models are stored in a public dataset*)\n5. Test data is processed\n6. [Trained models](https:\/\/www.kaggle.com\/skylord\/m5-forecasting-models) are used to then predict for each store in the test data\n\n","e0f467ed":"# Define helper functions","270d318d":"# Model training","6714b59a":"# Pre-processing of train data","d07766e4":"# Lets predict with a sample test row"}}