{"cell_type":{"616f63f2":"code","13e8bb4f":"code","96be086c":"code","5782bfc5":"code","dce421ff":"code","762f467f":"code","56079aa0":"code","241fcd1b":"code","2a3c20df":"code","42938a05":"code","a9d7650a":"code","b7ab5d3a":"markdown","2b584dca":"markdown","188d46ea":"markdown","d37acfd8":"markdown","04279135":"markdown","695dc0aa":"markdown","3e164c19":"markdown","5e881e26":"markdown","8a182de9":"markdown","4feca264":"markdown"},"source":{"616f63f2":"import numpy as np\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nimport time\nimport shutil\nimport PIL\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nfrom tensorflow.keras.layers import Dense, Activation,Dropout,BatchNormalization, Input\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\nfrom IPython.core.display import display, HTML","13e8bb4f":"def tr_plot(tr_data, start_epoch):\n    #Plot the training and validation data\n    tacc=tr_data.history['accuracy']\n    tloss=tr_data.history['loss']\n    vacc=tr_data.history['val_accuracy']\n    vloss=tr_data.history['val_loss']\n    Epoch_count=len(tacc)+ start_epoch\n    Epochs=[]\n    for i in range (start_epoch ,Epoch_count):\n        Epochs.append(i+1)   \n    index_loss=np.argmin(vloss)#  this is the epoch with the lowest validation loss\n    val_lowest=vloss[index_loss]\n    index_acc=np.argmax(vacc)\n    acc_highest=vacc[index_acc]\n    plt.style.use('fivethirtyeight')\n    sc_label='best epoch= '+ str(index_loss+1 +start_epoch)\n    vc_label='best epoch= '+ str(index_acc + 1+ start_epoch)\n    fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n    axes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\n    axes[0].set_title('Training and Validation Loss')\n    axes[0].set_xlabel('Epochs')\n    axes[0].set_ylabel('Loss')\n    axes[0].legend()\n    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n    axes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\n    axes[1].set_title('Training and Validation Accuracy')\n    axes[1].set_xlabel('Epochs')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].legend()\n    plt.tight_layout\n    #plt.style.use('fivethirtyeight')\n    plt.show()\n","96be086c":"def print_in_color(txt_msg,fore_tupple,back_tupple,):\n    #prints the text_msg in the foreground color specified by fore_tupple with the background specified by back_tupple \n    #text_msg is the text, fore_tupple is foregroud color tupple (r,g,b), back_tupple is background tupple (r,g,b)\n    rf,gf,bf=fore_tupple\n    rb,gb,bb=back_tupple\n    msg='{0}' + txt_msg\n    mat='\\33[38;2;' + str(rf) +';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' +str(gb) + ';' + str(bb) +'m' \n    print(msg .format(mat), flush=True)\n    print('\\33[0m', flush=True) # returns default print color to back to black\n    return","5782bfc5":"class ASK(keras.callbacks.Callback):\n    def __init__ (self, model, epochs,  ask_epoch): # initialization of the callback\n        super(ASK, self).__init__()\n        self.model=model               \n        self.ask_epoch=ask_epoch\n        self.epochs=epochs\n        self.ask=True # if True query the user on a specified epoch\n        \n    def on_train_begin(self, logs=None): # this runs on the beginning of training\n        if self.ask_epoch == 0: \n            print('you set ask_epoch = 0, ask_epoch will be set to 1', flush=True)\n            self.ask_epoch=1\n        if self.ask_epoch >= self.epochs: # you are running for epochs but ask_epoch>epochs\n            print('ask_epoch >= epochs, will train for ', epochs, ' epochs', flush=True)\n            self.ask=False # do not query the user\n        if self.epochs == 1:\n            self.ask=False # running only for 1 epoch so do not query user\n        else:\n            print('Training will proceed until epoch', ask_epoch,' then you will be asked to') \n            print(' enter H to halt training or enter an integer for how many more epochs to run then be asked again')           \n        self.start_time= time.time() # set the time at which training started\n        \n    def on_train_end(self, logs=None):   # runs at the end of training     \n        tr_duration=time.time() - self.start_time   # determine how long the training cycle lasted         \n        hours = tr_duration \/\/ 3600\n        minutes = (tr_duration - (hours * 3600)) \/\/ 60\n        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n        print (msg, flush=True) # print out training duration time\n        \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        if self.ask: # are the conditions right to query the user?\n            if epoch + 1 ==self.ask_epoch: # is this epoch the one for quering the user?\n                print('\\n Enter H to end training or  an integer for the number of additional epochs to run then ask again')\n                ans=input()\n                \n                if ans == 'H' or ans =='h' or ans == '0': # quit training for these conditions\n                    print ('you entered ', ans, ' Training halted on epoch ', epoch+1, ' due to user input\\n', flush=True)\n                    self.model.stop_training = True # halt training\n                else: # user wants to continue training\n                    self.ask_epoch += int(ans)\n                    if self.ask_epoch > self.epochs:\n                        print('\\nYou specified maximum epochs of as ', self.epochs, ' cannot train for ', self.ask_epoch, flush=True)\n                    else:\n                        print ('you entered ', ans, ' Training will continue to epoch ', self.ask_epoch, flush=True)","dce421ff":"train_dir=r'..\/input\/100-bird-species\/train'\ntest_dir=r'..\/input\/100-bird-species\/test'\nvalid_dir=r'..\/input\/100-bird-species\/valid'","762f467f":"img_shape=(128,128,3) # use 128 X128 versus 224 X224 to reduce training time\nimg_size=(img_shape[0], img_shape[1])\nmsg='For training set'\nprint_in_color(msg, (0,255,255), (55,65,80))\ntrain_ds=tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir, image_size=img_size, seed=123, batch_size=30, shuffle=True)\nmsg=' For validation set'\nprint_in_color(msg, (0,255,255), (55,65,80))\nvalid_ds=tf.keras.preprocessing.image_dataset_from_directory(directory=valid_dir, image_size=img_size, seed=123, batch_size=30, shuffle=True)\nmsg='For the test set'\nprint_in_color(msg, (0,255,255), (55,65,80))\ntest_ds=tf.keras.preprocessing.image_dataset_from_directory( directory=test_dir, image_size=img_size, shuffle=False, batch_size=30) # set shuffle=False to keep file order","56079aa0":"class_names=train_ds.class_names\nclass_count=len(class_names)\nplt.figure(figsize=(20,20))\nfor images, labels in train_ds.take(1):\n    for i in range (25):\n        plt.subplot(5,5,i +1)\n        img=images[i]\/255         \n        plt.title(class_names[labels[i]], color='blue', fontsize=12)\n        plt.imshow(img)\n        plt.axis('off')\n    plt.show()\n","241fcd1b":"base_model=tf.keras.applications.EfficientNetB3(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max') \nx=base_model.output\nx=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\nx = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\nx=Dropout(rate=.45, seed=123)(x)        \noutput=Dense(class_count, activation='softmax')(x)\nmodel=Model(inputs=base_model.input, outputs=output)\nmodel.compile(Adamax(learning_rate=.001), loss='sparse_categorical_crossentropy', metrics=['accuracy']) ","2a3c20df":"epochs=10\nask_epoch=2\nrlronp=tf.keras.callbacks.ReduceLROnPlateau( monitor=\"val_loss\", factor=0.5,  patience=1, verbose=1)\nASK(model=model, epochs=epochs,  ask_epoch=ask_epoch)\ncallbacks=[rlronp,ASK(model=model, epochs=epochs,  ask_epoch=ask_epoch)]\nhistory=model.fit( train_ds, validation_data=valid_ds, epochs=epochs, verbose=1, callbacks=callbacks)\n  ","42938a05":"tr_plot(history, 0)","a9d7650a":"classes=class_names\nytrue=[]\nfor images, label in test_ds:   \n    for e in label:\n        ytrue.append(class_names[e]) # list of class names associated with each image file in test dataset \nypred=[]\nerrors=0\ncount=0\npreds=model.predict(test_ds, verbose=1) # predict on the test data\nfor i, p in enumerate(preds):\n    count +=1\n    index=np.argmax(p) # get index of prediction with highest probability\n    klass=class_names[index] \n    ypred.append(klass)  \n    if klass != ytrue[i]:\n        errors +=1\nacc= (count-errors)* 100\/count\nmsg=f'there were {count-errors} correct predictions in {count} tests for an accuracy of {acc:6.2f} % '\nprint_in_color(msg, (0,255,255), (55,65,80)) \nypred=np.array(ypred)\nytrue=np.array(ytrue)\n# create confusion matrix        \nlength=len(classes)\nif length <=30: # only show confusion matrix if there are 30 or less classes otherwise it will not fit the page\n    cm = confusion_matrix(ytrue, ypred )\n    if length<8:\n        fig_width=8\n        fig_height=8\n    else:\n        fig_width= int(length * .5)\n        fig_height= int(length * .5)\n    plt.figure(figsize=(fig_width, fig_height))\n    sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \n    plt.xticks(np.arange(length)+.5, classes, rotation= 90)\n    plt.yticks(np.arange(length)+.5, classes, rotation=0)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(\"Confusion Matrix\")\n    plt.show()\nclr = classification_report(ytrue, ypred, target_names=classes)\nprint(\"Classification Report:\\n----------------------\\n\", clr)    ","b7ab5d3a":"### create a function to plot training data from model.fit","2b584dca":"### make predictions on test set, compute accuracy and create classification report","188d46ea":"### define function to print text in RGB foreground and background colors","d37acfd8":"### show some  images- ","04279135":"### create the model","695dc0aa":"### define a sub class of keras callbacks\nThe ASK custom callback is useful when you are training your model.  Initially you are probably not sure of how many epochs to train your model for. Often you might set a high value for epochs but on monitoring the model accuracy or validation accuracy during training you may want to halt training rather than wait for all epochs to be run. The ASK callback affords a convenient way to do that. The use of the  callback is of the form\n\ncallbacks=[ASK(model=model, epochs=epochs,  ask_epoch=ask_epoch)] where:\n\n - model is the variable name for your compiled model  \n - epochs is an integer representing the number of epochs the model is to be trained for\n - ask_epoch is  an integer that specifies how many epoch to train up to until the user is quired to either enter\n   H to halt training, an integer N where the training will continue for N more epochs then the user is queried again\n   \nAn example of use is shown belowepochs=40ask_epoch=5model=my_modelcallbacks=[ASK(model=model, epochs=epochs, ask_epoch=ask_epoch)]history=my_model.fit(x,y, epochs=epochs, callback=callbacks, etc..)Below is the code for the ASK callback","3e164c19":"### define the directories ","5e881e26":"### plot the training data","8a182de9":"### create the Datasets","4feca264":"### create reduce learning rate on plateau callback and instantiate ASk callback\n### then train the model"}}