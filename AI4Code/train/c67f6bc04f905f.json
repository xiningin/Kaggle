{"cell_type":{"f9cae72c":"code","3850d5f5":"code","df8426f2":"code","4af2f970":"code","dfb1009c":"code","0ce4e4fc":"code","5a4e9f00":"code","ac257d2a":"code","07779a8e":"code","4b46667c":"code","06a9cb23":"code","aa2001e7":"code","6816466e":"code","1652ed40":"code","5e3aa766":"code","564e4e88":"code","dd464b61":"code","0cab9648":"code","f355dc9a":"code","4bbf042c":"code","8201f96e":"code","d8b5fd80":"code","910e6f82":"code","b969b9f7":"code","a0c9fa06":"code","63dabdfc":"code","8aa614e1":"code","a0976757":"code","6a5ee2ee":"markdown","1242dad5":"markdown","55052d37":"markdown","2daf171a":"markdown","4b25ae92":"markdown","87ca8cf2":"markdown","509e12a1":"markdown","349320a2":"markdown","e4d0a9e6":"markdown","9d9df1c3":"markdown","5691770e":"markdown","246a002e":"markdown","11fb41c3":"markdown","97761311":"markdown","a0742db3":"markdown","df0c1f74":"markdown","dc1f56af":"markdown","a7bc8843":"markdown","3eebfe27":"markdown","9cdde904":"markdown","11d2d809":"markdown","5a1e73f1":"markdown","6962e01e":"markdown","b918d2ff":"markdown","f6824d68":"markdown","f1ec2c7c":"markdown","170d03e3":"markdown"},"source":{"f9cae72c":"! pip3 install textaugment","3850d5f5":"import re\n\nimport pandas as pd\nimport numpy as np\nimport csv\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence","df8426f2":"train_file = '\/kaggle\/input\/nlp-getting-started\/train.csv'","4af2f970":"max_sequence_length = 32","dfb1009c":"max_words = 3000","0ce4e4fc":"embedding_size = 32","5a4e9f00":"model_file = '\/kaggle\/working\/model.h5'","ac257d2a":"tokenizer_file = '\/kaggle\/working\/tokenizer.pickle'","07779a8e":"num_classes = 2","4b46667c":"def clean_str(string):\n    string = re.sub(r'http\\S+', 'link', string) # replace links by generic text link\n    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n    string = re.sub(r\"\\'s\", \" \\'s\", string)\n    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n    string = re.sub(r\"\\'re\", \" \\'re\", string)\n    string = re.sub(r\"\\'d\", \" \\'d\", string)\n    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n    string = re.sub(r\",\", \" , \", string)\n    string = re.sub(r\"!\", \" ! \", string)\n    string = re.sub(r\"\\(\", \" \\( \", string)\n    string = re.sub(r\"\\)\", \" \\) \", string)\n    string = re.sub(r\"\\?\", \" \\? \", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n\n    cleanr = re.compile('<.*?>')\n\n    string = re.sub(r'\\d+', '', string)\n    string = re.sub(cleanr, '', string)\n    string = re.sub(\"'\", '', string)\n    string = re.sub(r'\\W+', ' ', string)\n    string = string.replace('_', '')\n\n    return string.strip().lower()\n\n\ncleaned_str = clean_str('Horrible Accident | Man Died In Wings of Airplane\u00e5\u00ca(29-07-2015) http:\/\/t.co\/wq3wJsgPHL')\ncleaned_str","06a9cb23":"stop_words = set(stopwords.words('english'))\n\ndef remove_stopwords(word_list):\n    no_stop_words = [w for w in word_list if not w in stop_words]\n    return no_stop_words\n\n\nremove_stopwords(cleaned_str.split(\" \"))","aa2001e7":"# Import and Create an EDA object\nfrom textaugment import EDA\n\nt = EDA()","6816466e":"for i in range(3):\n    print(t.random_deletion('The pastor was not in the scene of the accident... who was the owner of the range rover?', p=0.2))","1652ed40":"for i in range(3):\n    print(t.random_swap('The pastor was not in the scene of the accident... who was the owner of the range rover?'))","5e3aa766":"for i in range(3):\n    print(t.synonym_replacement('The pastor was not in the scene of the accident... who was the owner of the range rover?'))","564e4e88":"for i in range(3):\n    print(t.random_insertion('The pastor was not in the scene of the accident... who was the owner of the range rover?'))","dd464b61":"data = pd.read_csv(train_file, sep=',', header=0, quotechar='\"')\n\ndata = data[['text', 'target']]\ndata","0cab9648":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.countplot(data.target)\nplt.xlabel('Target')\nplt.title('Number of disaster tweets')","f355dc9a":"# text cleaning\ndata['text'] = data['text'].apply(lambda x: clean_str(x))\n\nsequences = []\ntargets = []\n\nfor index, row in data.iterrows():\n    seqs = []\n    text = row['text']\n\n    # if empty text, skipping to next row data\n    if not text:\n        continue\n\n    seqs.append(text)\n    \n    # apply data augmentation\n    \n    # random deletion\n    seq2 = t.random_deletion(text, p=0.2)\n    if type(seq2) == type([]):\n        seqs.append(seq2[0])\n    else:\n        seqs.append(seq2)\n\n    # random swap\n    if len(text) > 1:\n        seqs.append(t.random_swap(text))\n\n    # synonym replacement and random insertion\n    for i in range(2):\n        seqs.append(t.synonym_replacement(text))    \n        try:\n            seqs.append(t.random_insertion(text))\n        except:\n            pass\n\n    \n    \"\"\"\n    All sequence variations created in the data augmentation process are grouped in bags. \n    This is important to avoid that in the process of splitting the data, variations of \n    the same sequence are allocated in different sets. For example, an X variation of \n    sequence A falls in the training set and an Y variation of sequence A falls in the test set.\n    \"\"\"\n    sequence_group = []\n    target_group = []\n\n    target = row['target']\n\n    for sequence in seqs: \n        word_list = text_to_word_sequence(sequence)\n        \n        # remove stop words\n        no_stop_words = remove_stopwords(word_list)\n        \n        if not no_stop_words:\n            continue\n\n        sequence_group.append(\" \".join(no_stop_words))\n        target_group.append(target)\n\n    sequences.append(sequence_group)\n    targets.append(target_group)\n\n\nX = sequences\nY = np.array(targets)\n\nprint(\"{bags_count} bags\".format(bags_count=len(X)))","4bbf042c":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1,\n                                                    random_state=42)\n\nX_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=0.25,\n                                                    random_state=42)\n\nX_train = [item for sublist in X_train for item in sublist]\nY_train = [item for sublist in Y_train for item in sublist]\n\nX_validation = [item for sublist in X_validation for item in sublist]\nY_validation = [item for sublist in Y_validation for item in sublist]\n\nX_test = [item for sublist in X_test for item in sublist]\nY_test = [item for sublist in Y_test for item in sublist]\n\nprint(\"Train: {train_size}\\nValidation: {validation_size}\\nTest: {test_size}\\n\".format(train_size=len(X_train), validation_size=len(X_validation), test_size=len(X_test)))","8201f96e":"# tokenizer\ntokenizer = Tokenizer(num_words=max_words)  \n\n\n\n# Updates internal vocabulary based on a list of texts. \n# This method creates the vocabulary index based on word frequency. \ntokenizer.fit_on_texts(X_train)\n\n\n# Transforms each row from texts to a sequence of integers. \n# So it basically takes each word in the text and replaces it \n# with its corresponding integer value from the\nX_train = tokenizer.texts_to_sequences(X_train)\nX_validation = tokenizer.texts_to_sequences(X_validation)\nX_test = tokenizer.texts_to_sequences(X_test)\n\n\n# Pad sequences\nX_train = pad_sequences(X_train, maxlen=max_sequence_length, dtype='int32', value=0)\nX_validation = pad_sequences(X_validation, maxlen=max_sequence_length, dtype='int32', value=0)\nX_test = pad_sequences(X_test, maxlen=max_sequence_length, dtype='int32', value=0)\n\n\nword_index = tokenizer.word_index\n\nX_train = np.array(X_train)\nY_train = np.array(Y_train)\nX_validation = np.array(X_validation)\nY_validation = np.array(Y_validation)\nX_test = np.array(X_test)\nY_test = np.array(Y_test)","d8b5fd80":"from tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.regularizers import l2\n\nl2_reg = l2(0.001)\n\ndef model_fn():\n    model = Sequential()\n\n    model.add(Embedding(max_words, embedding_size, input_length=max_sequence_length, embeddings_regularizer=l2_reg))\n    \n    model.add(SpatialDropout1D(0.5))\n    \n    model.add(LSTM(32, dropout=0.5, recurrent_dropout=0.5, kernel_regularizer=l2_reg, recurrent_regularizer=l2_reg, bias_regularizer=l2_reg))\n    \n    model.add(Dropout(0.5))\n    \n    model.add(Dense(512, activation='relu'))\n\n    model.add(Dense(1, activation='sigmoid'))\n    \n    optimizer = RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.0)\n    \n    model.compile(loss='binary_crossentropy',\n                  optimizer=optimizer,\n                  metrics=['accuracy'])\n\n    print(model.summary())\n\n    return model\n","910e6f82":"!rm \/kaggle\/working\/*","b969b9f7":"import os\nimport numpy as np\nimport pickle\n\n# epochs\nepochs = 10\n\n# number of samples to use for each gradient update\nbatch_size = 128\n\n# saving tokenizer\nwith open(tokenizer_file, 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\nmodel = model_fn()\n\n# loadin saved model\nif os.path.exists(model_file):\n    model.load_weights(model_file)\n\nhistory = model.fit(X_train, Y_train,\n          validation_data=(X_validation, Y_validation),\n          epochs=epochs,\n          batch_size=batch_size,\n          shuffle=True,\n          verbose=1)\n\n# saving model\nmodel.save_weights(model_file)","a0c9fa06":"plt.title('Loss')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='validation')\nplt.legend()\nplt.show();","63dabdfc":"plt.title('Accuracy')\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='validation')\nplt.legend()\nplt.show();","8aa614e1":"# evaluate model\nscores = model.evaluate(X_test, Y_test, verbose=0, batch_size=batch_size)\nprint(\"Acc: %.2f%%\" % (scores[1] * 100))","a0976757":"# Read the test data to create the submission.csv file\n\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\n# Clear text\ntest['text'] = test['text'].apply(lambda x: clean_str(x))\n\n# Remove stop words\ntest['text'] = test['text'].apply(lambda x: \" \".join(remove_stopwords(x.split(\" \"))))\n\n# Get text\ntest_X = list(test[\"text\"])\n\n# Convert text to sequences\nsequences = tokenizer.texts_to_sequences(test_X)\n\n# Pad sequences\nsequences = pad_sequences(sequences,\n                             maxlen=max_sequence_length,\n                             dtype='int32',\n                             value=0)\n\n# Predict sequences\npredicted = model.predict(sequences)\n\nbinary_predicted = np.array(predicted) >= 0.5\ntargets = binary_predicted.astype(int).reshape((len(binary_predicted)))\n\nmy_submission = pd.DataFrame({'id': test.id, 'target': targets})\nmy_submission.to_csv('submission.csv', index=False)\n\nprint(\"Submission file created!\")","6a5ee2ee":"# Text cleaning","1242dad5":"# Show loss and accuracy","55052d37":"### **Random Deletion (RD)**: Randomly remove each word in the sentence with probability p.","2daf171a":"## Training","4b25ae92":"### Training Dataset: \n\nThe sample of data used to fit the model.\n\n### Validation Dataset: \n\nThe sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration.\n    \n### Test Dataset: \n\nThe sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.","87ca8cf2":"# Settings","509e12a1":"Once trained, the weights of the LSTM neural network will be saved in this .h5 file. For more details on this format, see https:\/\/www.h5py.org. \n\nWith this file, you will then be able to reuse the already trained network to make other predictions, without having to do another training.","349320a2":"Binary classification","e4d0a9e6":"This file will be used to store the word dictionary and in the process of converting text to strings and backwards.","9d9df1c3":"See that the loss graph shows the beginning of an overfitting.\n\nOverfitting occurs when the model begins to identify individual elements in the training data and not their characteristics, causing it to hit many more samples from the training set than from the validation set.\n\n![](https:\/\/3gp10c1vpy442j63me73gy3s-wpengine.netdna-ssl.com\/wp-content\/uploads\/2018\/03\/Screen-Shot-2018-03-22-at-11.22.15-AM-e1527613915658.png)","5691770e":"## Load dataset","246a002e":"File path where the training data will be read.","11fb41c3":"This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.","97761311":"# Create a submission","a0742db3":"The textaugment library is used to apply data augmentation techniques. The data augmentation techniques applied are explained below. See more at https:\/\/pypi.org\/project\/textaugment\/","df0c1f74":"### **Synonym Replacement (SR)**: Randomly choose n words from the sentence that are not stop words. Replace each of these words with one of its synonyms chosen at random.","dc1f56af":"### **Random Insertion (RI)**: Find a random synonym of a random word in the sentence that is not a stop word. Insert that synonym into a random position in the sentence. Do this n times.","a7bc8843":"# Imports","3eebfe27":"In our input data, we will have twitters with different number of words. Some with many words and others with few words.\nOne way to facilitate the training of our model is to set a number of words that each entry will be passed to the model. To do this we can erase the words in excess and fill with zeros (later I explain better) twitters with few words.\n\nWe empirically set the fixed number of words in each entry to 32. Test different values for your problem.","9cdde904":"# Remove stop words","11d2d809":"# Install requirements","5a1e73f1":"When we create our dictionary of words, we will limit it to 3000 words.","6962e01e":"# Split data into train, validation and test sets","b918d2ff":"# Evaluate model in test set ","f6824d68":"# Data augmentation examples","f1ec2c7c":"## LSTM Model","170d03e3":"### **Random Swap (RS)**: Randomly choose two words in the sentence and swap their positions. Do this n times."}}