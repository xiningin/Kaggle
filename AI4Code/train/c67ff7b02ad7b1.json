{"cell_type":{"85877d3e":"code","5625e3a4":"code","7629b040":"code","0c58bd87":"code","027326d1":"code","9760a8fd":"code","30fa58d0":"code","43090263":"code","77f07136":"code","ded06bd2":"code","f77e1b72":"code","eb5477f7":"code","ef20304c":"code","e2c99ce0":"code","1608ede4":"code","36de7aa4":"code","a74b154d":"code","a6a32297":"code","24ded12a":"code","83b52d82":"code","8bbecc9e":"code","716a52ed":"code","c2e70958":"code","fc9f905e":"code","7f9ca0e8":"code","d6b0a55b":"code","166edd30":"code","15ebd167":"code","89aa7f0c":"code","91eb982a":"code","1d523fd5":"code","a4a23535":"code","b85698b0":"code","875d0211":"code","0d577546":"code","078cdc55":"code","9657c19a":"code","d38b0677":"code","15bbf504":"code","65f59db9":"code","2b86ce42":"code","53f3878b":"code","2f4bf2fc":"code","2af47ad9":"code","d1ea7639":"code","7f624973":"code","a3df5c35":"code","baf4088a":"code","2ddcd25d":"code","10ba22ea":"code","5310c867":"code","189c2efe":"code","224ed72a":"code","3d3b1094":"code","ccded676":"code","5ab629d4":"code","30d1d49d":"code","c86f8ea7":"code","01c325de":"markdown","fb319d52":"markdown","7ec996ee":"markdown","fb61f6c3":"markdown","1da6da07":"markdown","9b435620":"markdown","54e04a77":"markdown","f9e4cb1a":"markdown","9b0a20a8":"markdown","9342353f":"markdown","1ba67555":"markdown","9a8ea6e0":"markdown","791364ba":"markdown","48d3aa0e":"markdown","23d8112e":"markdown","419aea6e":"markdown","b645efc4":"markdown","6b4f17ee":"markdown","c12c8bd9":"markdown","2d766ff8":"markdown","a82ac4ab":"markdown","3d7816f2":"markdown","064de4b0":"markdown","56b0e97f":"markdown"},"source":{"85877d3e":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom scipy.stats import binned_statistic\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","5625e3a4":"train_csv = pd.read_csv('..\/input\/train.csv')\nfinal_csv = pd.read_csv('..\/input\/test.csv')\n# I won't call it test set here, since we're going to create our test set and validation set\n# and pick up the strongest model to make prediction in the \"test.csv\"","7629b040":"# first we wanna describe the target columns to make sure we're dealing with regression problem (predict value)\n# and we'll look into more detail when we have to do feature engineering\ntrain_csv['SalePrice'].describe()","0c58bd87":"print(\"How many feature candidates do we have? %d\" % (len(train_csv.columns) - 1))","027326d1":"# first we'll visualize null count\nnull_in_train_csv = train_csv.isnull().sum()\nnull_in_train_csv = null_in_train_csv[null_in_train_csv > 0]\nnull_in_train_csv.sort_values(inplace=True)\nnull_in_train_csv.plot.bar()","9760a8fd":"# visualize correlation map\nsns.heatmap(train_csv.corr(), vmax=.8, square=True);","30fa58d0":"arr_train_cor = train_csv.corr()['SalePrice']\nidx_train_cor_gt0 = arr_train_cor[arr_train_cor > 0].sort_values(ascending=False).index.tolist()\nprint(\"How many feature candidates have positive correlation with SalePrice(including itself)? %d\" % len(idx_train_cor_gt0))","43090263":"# we shall list them all, and pick up those we're interested\narr_train_cor[idx_train_cor_gt0]","77f07136":"idx_meta = ['SalePrice','GrLivArea', 'MasVnrArea', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'OverallQual', 'Fireplaces', 'GarageCars']\ntrain_meta = train_csv[idx_meta].copy()\ntrain_meta.head(n=5)","ded06bd2":"null_in_masvnrarea = train_meta[train_meta['MasVnrArea'].isnull()].index.tolist()\nzero_in_masvnrarea = train_meta['MasVnrArea'][train_meta['MasVnrArea'] == 0].index.tolist()\nprint(\"How many null value in MasVnrArea? %d \/ 1460\" % len(null_in_masvnrarea))\nprint(\"How many zero value in MasVnrArea? %d \/ 1460\" % len(zero_in_masvnrarea))","f77e1b72":"# we'll fill in the null value with 0 from the analysis above\ntrain_meta['MasVnrArea'][null_in_masvnrarea] = 0\nprint(\"How many null value in MasVnrArea after filling in null value? %d \/ 1460\" % train_meta['MasVnrArea'].isnull().sum())","eb5477f7":"# overview\nsns.pairplot(train_meta)","ef20304c":"# GrLivArea\ntrain_meta[(train_meta['GrLivArea'] > 4000) & (train_meta['SalePrice'] < 200000)].index.tolist()","e2c99ce0":"# TotalBsmtSF\ntrain_meta[(train_meta['TotalBsmtSF'] > 4000) & (train_meta['SalePrice'] < 200000)].index.tolist()","1608ede4":"train_meta[(train_meta['1stFlrSF'] > 4000) & (train_meta['SalePrice'] < 200000)].index.tolist()","36de7aa4":"# Thus, we'll remove [523, 1298]\ntrain_clean = train_meta.drop([523,1298])","a74b154d":"nonzero_in_masvnrarea = train_clean['MasVnrArea'][train_clean['MasVnrArea'] != 0].index.tolist()\nprint(\"How many non-zero value in MasVnrArea now? %d \/ 1458\" % len(nonzero_in_masvnrarea))","a6a32297":"# I'll categorize into zero and non-zero\ntrain_clean['has_MasVnrArea'] = 0\ntrain_clean['has_MasVnrArea'][nonzero_in_masvnrarea] = 1","24ded12a":"train_clean['TotalBsmtSF'][train_clean['TotalBsmtSF'] > 0].describe()","83b52d82":"bins_totalbsmtsf = [-1, 1, 1004, 4000]\ntrain_clean['binned_TotalBsmtSF'] = np.digitize(train_clean['TotalBsmtSF'], bins_totalbsmtsf)","8bbecc9e":"train_clean['1stFlrSF'].describe()","716a52ed":"bins_1stflrsf = [0, 882, 1086, 1390, 4000]\ntrain_clean['binned_1stFlrSF'] = np.digitize(train_clean['1stFlrSF'], bins_1stflrsf)","c2e70958":"train_clean['2ndFlrSF'][train_clean['2ndFlrSF'] > 0].describe()","fc9f905e":"bins_2ndflrsf = [-1, 1, 625, 772, 924, 4000]\ntrain_clean['binned_2ndFlrSF'] = np.digitize(train_clean['2ndFlrSF'], bins_2ndflrsf)","7f9ca0e8":"train_clean['SFcross'] = (train_clean['binned_TotalBsmtSF'] - 1) * (5 * 4) + (train_clean['binned_1stFlrSF'] - 1) * 5 + train_clean['binned_2ndFlrSF']","d6b0a55b":"def draw2by2log(arr):\n    fig = plt.figure();\n    plt.subplot(2,2,1)\n    sns.distplot(arr, fit=norm);\n    plt.subplot(2,2,3)\n    stats.probplot(arr, plot=plt);\n    plt.subplot(2,2,2)\n    sns.distplot(np.log(arr), fit=norm);\n    plt.subplot(2,2,4)\n    stats.probplot(np.log(arr), plot=plt);","166edd30":"draw2by2log(train_clean['SalePrice'])","15ebd167":"draw2by2log(train_clean['GrLivArea'])","89aa7f0c":"train_clean.head(n=5)","91eb982a":"idx_tree = ['SalePrice', 'GrLivArea', 'OverallQual', 'Fireplaces', 'GarageCars', 'has_MasVnrArea', 'SFcross']\ntrain_tree = train_clean[idx_tree]\ntrain_tree.head(n=5)","1d523fd5":"sns.pairplot(train_tree)","a4a23535":"print(\"Max Fireplaces value in train.csv: %d, in test.csv: %d\" % (train_csv['Fireplaces'].max(), final_csv['Fireplaces'].max()) )\nprint(\"Min Fireplaces value in train.csv: %d, in test.csv: %d\" % (train_csv['Fireplaces'].min(), final_csv['Fireplaces'].min()) )","b85698b0":"print(\"Max GarageCars value in train.csv: %d, in test.csv: %d\" % (train_csv['GarageCars'].max(), final_csv['GarageCars'].max()) )\nprint(\"Min GarageCars value in train.csv: %d, in test.csv: %d\" % (train_csv['GarageCars'].min(), final_csv['GarageCars'].min()) )","875d0211":"dummy_fields = ['OverallQual', 'Fireplaces', 'GarageCars', 'has_MasVnrArea', 'SFcross']\ntrain_dist = train_tree[['SalePrice', 'OverallQual', 'GrLivArea']].copy()\nfor field in dummy_fields:\n    dummies = pd.get_dummies(train_tree.loc[:, field], prefix=field)\n    train_dist = pd.concat([train_dist, dummies], axis = 1)\ntrain_dist['GarageCars_5'] = 0\ntrain_dist['Fireplaces_4'] = 0\ntrain_dist.head(n=5)","0d577546":"print(\"The dimension for the input of distance-based model is %d x %d\" % (train_dist.shape[0], train_dist.shape[1] - 1))\n# SalePrice is not input, so minus one","078cdc55":"from sklearn.model_selection import train_test_split\nrandom_state = 7","9657c19a":"xt_train_test, xt_valid, yt_train_test, yt_valid = train_test_split(train_tree['SalePrice'], train_tree.drop('SalePrice', axis=1), test_size=.2, random_state=random_state)\nxd_train_test, xd_valid, yd_train_test, yd_valid = train_test_split(train_dist['SalePrice'], train_dist.drop('SalePrice', axis=1), test_size=.2, random_state=random_state)","d38b0677":"xt_train, xt_test, yt_train, yt_test = train_test_split(yt_train_test, xt_train_test, test_size=.2, random_state=random_state)\nxd_train, xd_test, yd_train, yd_test = train_test_split(yd_train_test, xd_train_test, test_size=.2, random_state=random_state)","15bbf504":"print(\"number of training set: %d\\nnumber of testing set: %d\\nnumber of validation set: %d\\ntotal: %d\" % (len(xt_train), len(xt_test), len(xt_valid), (len(xt_train)+len(xt_test)+len(xt_valid))))","65f59db9":"def rmse(arr1, arr2):\n    return np.sqrt(np.mean((arr1-arr2)**2))","2b86ce42":"from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlm.fit(xd_train, yd_train)\nyd_lm = lm.predict(xd_test)\nrmse_linear = rmse(yd_test, yd_lm)\nsns.regplot(yd_test, yd_lm)\nprint(\"RMSE for Linear Regression Model in sklearn: %.2f\" % rmse_linear)","53f3878b":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense","2f4bf2fc":"def baseline_nn_model(dims):\n    model = Sequential()\n    model.add(Dense(dims, input_dim=dims,kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal'))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model","2af47ad9":"def larger_nn_model(dims):\n    model = Sequential()\n    model.add(Dense(dims, input_dim=dims,kernel_initializer='normal', activation='relu'))\n    model.add(Dense(35, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(15, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal'))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model","d1ea7639":"def use_keras_nn_model(nn_model, x, y, xx, yy, epoch):\n    print(\"start training\")\n    for step in range(epoch + 1):\n        cost = nn_model.train_on_batch(x.as_matrix(), y.as_matrix())\n        if step % 100 == 0:\n            print(\"train cost: %.2f\" % cost)\n    print(\"start testing\")\n    yy_predict = nn_model.predict(xx.as_matrix()).reshape(len(yy),)\n    res = rmse(yy, yy_predict)\n    sns.regplot(yy, yy_predict)\n    print(\"RMSE for NN Model in Keras(Tensorflow): %.2f\" % res)\n    return res","7f624973":"rmse_baselinenn = use_keras_nn_model(baseline_nn_model(xd_train.shape[1]), xd_train, yd_train, xd_test, yd_test, 700)","a3df5c35":"rmse_largernn = use_keras_nn_model(larger_nn_model(xd_train.shape[1]), xd_train, yd_train, xd_test, yd_test, 500)","baf4088a":"rmse_nn = min(rmse_baselinenn, rmse_largernn)","2ddcd25d":"import xgboost as xgb\nfrom xgboost import plot_importance","10ba22ea":"params = {\n    'booster': 'gbtree',\n    'objective': 'reg:gamma',\n    'gamma': 0.1,\n    'max_depth': 5,\n    'lambda': 3,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'min_child_weight': 3,\n    'silent': 1,\n    'eta': 0.1,\n    'seed': 1000,\n    'nthread': 4,\n}\n\nplst = params.items()\ndtrain = xgb.DMatrix(xt_train, yt_train)\ndtest = xgb.DMatrix(xt_test)\nnum_rounds = 500\nxgb_model = xgb.train(plst, dtrain, num_rounds)\nyt_xgb = xgb_model.predict(dtest)\nrmse_xgb = rmse(yt_test, yt_xgb)\nsns.regplot(yt_test, yt_xgb)\nprint(\"RMSE for xgboost: %.2f\" % rmse_xgb)","5310c867":"plot_importance(xgb_model)\n# it shows that the feature crossing is actually working","189c2efe":"print(\"The minimum RMSE goes to: %.2f\" % min([rmse_linear, rmse_nn, rmse_xgb]))\n# xgboost turns out to be a better model here","224ed72a":"idx_clean_final = idx_meta.copy()\nidx_clean_final.remove('SalePrice')\nfinal_clean = final_csv[idx_clean_final]\nfinal_clean.head(n=5)","3d3b1094":"final_clean['binned_TotalBsmtSF'] = np.digitize(final_clean['TotalBsmtSF'], bins_totalbsmtsf)\nfinal_clean['binned_1stFlrSF'] = np.digitize(final_clean['1stFlrSF'], bins_1stflrsf)\nfinal_clean['binned_2ndFlrSF'] = np.digitize(final_clean['2ndFlrSF'], bins_2ndflrsf)\nfinal_clean['SFcross'] = (final_clean['binned_TotalBsmtSF'] - 1) * (5 * 4) + (final_clean['binned_1stFlrSF'] - 1) * 5 + final_clean['binned_2ndFlrSF']\nfinal_clean['has_MasVnrArea'] = (final_clean['MasVnrArea'] > 0).astype(float)\nfinal_clean.head(n=5)","ccded676":"idx_tree_final = idx_tree.copy()\nidx_tree_final.remove('SalePrice')\nfinal_tree = final_clean[idx_tree_final]\nfinal_tree.head(n=5)","5ab629d4":"dtest_final = xgb.DMatrix(final_tree)\nyt_final = xgb_model.predict(dtest_final)\nsummission = pd.concat([final_csv['Id'], pd.DataFrame(yt_final)], axis=1)\nsummission.columns = ['Id', 'SalePrice']","30d1d49d":"sns.distplot(summission['SalePrice'])","c86f8ea7":"summission.to_csv('summission.csv', encoding='utf-8', index = False)","01c325de":"## Use on final","fb319d52":"### Categorize","7ec996ee":" # A Complete Guide for Regression Problem\n ## from Feature Engineering to Baseline Modeling\n \nMy goal is to demonstrate several practical strategies and useful visualization graphs to build up regression models including the linear, distance-based, and tree-based ones for the beginners(like me). \nI'll only do the baseline models, and leave the advanced model or training strategies like early stopping or cross validation for the readers.\nOr I'll elaborate on it when I have free time...\n \n ## implementation outlines\n \n ### data visualization\n- heatmap\n- barplot\n- distplot\n- probplot\n- regplot\n- pairplot\n \n###  feature engineering\n- feature crossing\n- one-hot encoding\n- binning\n- categorizing\n- log transformation\n- outliers removal\n- manipuluate both categoized and numerical data\n- dealing with null value\n\n### modeling\n- linear regression\n- distance-based method\n    - using NN in Keras with Tensorflow Backend\n- tree-based method\n    - using XGBoost\n\n### training strategies\n- train, test and validation set\n- ~~early stopping~~\n- ~~cross validation~~\n\nFor problems or you may consider I had snippets of your code, please feel to contact me!","fb61f6c3":"we'll first narrow down the number of features with 2 criteria:\n1. not too many null values\n2. positive correlation with our target, SalcePrice","1da6da07":"### test on final data","9b435620":"### training, test and validation sets (ignore k-fold cross validation)","54e04a77":"### Binning and feature cross\n- *TotalBsmtSF* -> 3\n- *1stFlrSF* -> 4\n- *2ndFlrSF* -> 5","f9e4cb1a":"## Modeling","9b0a20a8":"### feature engineering on final_csv","9342353f":"### Training Data Ready for Tree-based Algorithm","1ba67555":"## Feature Selection\n- we'll first have a glance at our data\n- we'll pick several features with positive correlation with our target, and do some feature engineering if needed","9a8ea6e0":"### one-hot encoding (dummy variables)","791364ba":"### linear model","48d3aa0e":"I'll only pick a few to further demonstrate feature engineering techniques, at most 10 candidates. \nAfter look it up in the [data description](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data), here's how I pick\n- *OverallQual* for sure, since its high correlation, and the delegate of categorized data [1-10]\n- *GrLivArea*, also because its high correlation and delegate of numerical data\n- I'll pick *GarageCars* rather than *GarageArea*, it's somehow like binned\n- I've noticed that there are *TotalBsmtSF*, *1stFlrSF*, and *2ndFlrSF*, they seem like related to *GrLivArea*, they're all about the area in the house, I'll do the feature cross techniques with it\n- I'll pick *MasVnrArea* although there're a few(8) null values in it, we can fill with mean or something else reasonable\n- I'll pick *Fireplaces* for another categorized example, since it sounds like a luxuriness indicator\n\nHere're why I don't pick\n- I'll drop *TotRmsAbvGrd* since it looks like highly related to *GrLivArea*, same applies on *GarageCars* and *GarageArea* cases\n- I'd rather not pick those time series related for simplicity\n\nSo, we have 8 feature candidates now.","23d8112e":"### xgboost","419aea6e":"- we can compare with the previous pairplot before feature engineering, we have less but more sophisticated features\n- then, we also need to construct the training data for distance-based algorithm","b645efc4":"### Neural Network in Keras using Tensorflow backend","6b4f17ee":"we can observe something in the pairplot\n- there're a few outliers\n- we may need to use binning to deal with those zero values in the numerical data\n- *GrLivArea* and *1stFlrSF* relation to *SalePrice* look alike, and that's reasonable\n- we may use log transformation to *SalePrice* and *GrLivArea*, however, it can't be applied to those numerical data with lots of zero","c12c8bd9":"### Log Transformation","2d766ff8":"### Outlier Removal","a82ac4ab":"## Reference","3d7816f2":"## Feature Engineering","064de4b0":"Although it's more \"normal\" after the log transformation, I didn't get a better result when using it as a feature. I'll still use un-transformed numerical data in data sets.","56b0e97f":"1. https:\/\/www.kaggle.com\/yiidtw\/house-price\/\n2. https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n3. https:\/\/www.kaggle.com\/dgawlik\/house-prices-eda\n4. http:\/\/violin-tao.blogspot.com\/2017\/05\/keras.html\n5. https:\/\/zhuanlan.zhihu.com\/p\/31182879 \n6. https:\/\/developers.google.com\/machine-learning\/crash-course\/\n7. https:\/\/yiidtw.github.io\/blog\/2018-06-14-class-note-of-machine-learning-crash-course\/\n8. https:\/\/machinelearningmastery.com\/regression-tutorial-keras-deep-learning-library-python\/"}}