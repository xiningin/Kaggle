{"cell_type":{"14aed2dd":"code","ce592c29":"code","fe63ee61":"code","6cf5f414":"code","ee308ecf":"code","34e0e410":"code","7453c716":"code","a6de5a4b":"code","d62a730f":"code","12fa4542":"code","d108fe39":"code","5a0f6d2c":"code","da4f6bac":"code","1bfb7936":"code","f5a0713a":"code","62830fd7":"code","b66321f4":"code","57a8af5e":"code","a8ef8dcb":"code","b9af03b6":"code","5dba4cc0":"code","228346e9":"code","1cfc11e3":"code","d2ff17f2":"code","531813ae":"code","b64438a0":"code","ae15f3f5":"code","fb92c759":"code","b4e84d44":"code","ba485a2c":"code","120a37c9":"code","a62a5129":"code","7866792d":"code","6a85adee":"code","d4808190":"code","d14bcc32":"code","8dc3f49b":"code","f233780e":"code","c4c8fadf":"code","b3251e7b":"markdown","1b1fc780":"markdown"},"source":{"14aed2dd":"#!\/usr\/bin\/env python\n# coding: utf-8\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy as np \nimport pandas as pd\nimport warnings \n\nget_ipython().run_line_magic('matplotlib', 'inline')\n\nwarnings.filterwarnings('ignore')\n\ndef display_all(df):\n    with pd.option_context(\"display.max_rows\", 100, \"display.max_columns\", 100):\n        display_all(df)","ce592c29":"cacao = pd.read_csv(\"..\/input\/chocolate-bar-ratings\/flavors_of_cacao.csv\")","fe63ee61":"cacao.head()","6cf5f414":"cacao.columns","ee308ecf":"# Hmmm Let's fix the column names...\ncacao.columns = cacao.columns.str.replace(\"\\n\", ' ', regex=True)\ncacao.columns","34e0e410":"#One more column to fix \ncacao = cacao.rename(columns={\"Company\\xa0 (Maker-if known)\": \"Company\"})","7453c716":"cacao.info()","a6de5a4b":"# Check for nulls \ncacao.isnull().sum()","d62a730f":"cacao[\"Bean Type\"].value_counts()","12fa4542":"# Let's just fill na with the mode...\ncacao[\"Bean Type\"] = cacao[\"Bean Type\"].fillna(\"Trinitario\")\ncacao[\"Bean Type\"].isna().sum()","d108fe39":"cacao[\"Broad Bean Origin\"].value_counts()","5a0f6d2c":"#Let's fill na with the mode again...\ncacao[\"Broad Bean Origin\"] = cacao[\"Broad Bean Origin\"].fillna(\"Venezuela\")","da4f6bac":"# Ok cool looks like no na values \ncacao.isna().sum()","1bfb7936":"for column in cacao.columns: \n    print(cacao[column].value_counts())","f5a0713a":"# Explore the dataset -> check distribution of Chocolate Rating (Approximately Normal)\nplt.figure(figsize=(12,5))\n\nsns.distplot(cacao[\"Rating\"], bins=15)\n\nplt.xlabel(\"Rating\", fontsize=17)\nplt.title(\"Chocolate Rating Distribution\", fontsize=20)\nplt.show()","62830fd7":"cacao[\"Cocoa Percent\"] = cacao[\"Cocoa Percent\"].apply(lambda x: int(str(x)[:2]))","b66321f4":"cacao[\"Cocoa Percent\"].value_counts()","57a8af5e":"plt.figure(figsize=(12,5))\n\nsns.distplot(cacao[\"Cocoa Percent\"], bins=20)\n\nplt.xlabel(\"Cocoa Percent\", fontsize=17)\nplt.title(\"Cocoa Percent Distribution\", fontsize=20)\nplt.show()","a8ef8dcb":"plt.figure(figsize=(12,5))\n\nsns.distplot(cacao[\"Review Date\"], bins=12)\n\nplt.xlabel(\"Review Date\", fontsize=17)\nplt.title(\"Review Date Distribution\", fontsize=20)\nplt.show()","b9af03b6":"# Check distribution of Target (Ratings)\n\nplt.figure(figsize=(12,5))\n\nsns.countplot(x=\"Rating\", data=cacao, palette='hls')\n\nplt.ylabel(\"Count\")\nplt.xlabel(\"Rating\")\nplt.title(\"Distribution of Chocolate Rating\")\nplt.show()","5dba4cc0":"# Target is to place them into five categories \ndef fix_ratings(rating):\n    if rating < 0.5: \n        return 0.0 \n    elif rating < 1.5: \n        return 1.0 \n    elif rating < 2.5: \n        return 2.0\n    elif rating < 3.5: \n        return 3.0\n    elif rating < 4.5: \n        return 4.0 \n    elif rating < 5.5: \n        return 5.0","228346e9":"cacao[\"Rating\"] = cacao[\"Rating\"].apply(lambda x: fix_ratings(x))","1cfc11e3":"plt.figure(figsize=(12,5))\n\nsns.countplot(x=\"Rating\", data=cacao, palette='hls')\n\nplt.xlabel(\"Rating\", fontsize=17)\nplt.ylabel(\"Count\", fontsize=17)\nplt.title(\"Rating by Count\", fontsize=20)\nplt.show()","d2ff17f2":"X = cacao.drop(\"Rating\", axis=1)\ny = cacao[\"Rating\"].values","531813ae":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","b64438a0":"cacao.info()","ae15f3f5":"from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(handle_unknown=\"ignore\")\nohe.fit(X_train)\nX_train_enc = ohe.transform(X_train)\nX_test_enc = ohe.transform(X_test)","fb92c759":"from sklearn.preprocessing import LabelEncoder \nle = LabelEncoder()\nle.fit(y_train)\ny_train_enc = le.transform(y_train)\ny_test_enc = le.transform(y_test)","b4e84d44":"from keras.utils import np_utils\ny_train_cat = np_utils.to_categorical(y_train_enc)\ny_test_cat = np_utils.to_categorical(y_test_enc)","ba485a2c":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler(with_mean=False)\nsc.fit(X_train_enc)\nX_train_enc_sc = sc.transform(X_train_enc)\nX_test_enc_sc = sc.transform(X_test_enc)","120a37c9":"X_train_enc_sc.shape","a62a5129":"y_train_cat.shape","7866792d":"# Creating holdout set to test on never seen before data (about 25%)\nholdout = int(.25 * X_train_enc_sc.shape[0])\nx_val = X_train_enc_sc[:holdout]\npartial_x_train = X_train_enc_sc[holdout:]\ny_val = y_train_cat[:holdout]\npartial_y_train = y_train_cat[holdout:]","6a85adee":"from keras import layers\nfrom keras import models\nfrom keras import regularizers\nfrom keras import optimizers\n\nmodel = models.Sequential()\n\nmodel.add(layers.Dense(8, activation='relu', input_dim=X_train_enc.shape[1], \n                       kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(5, activation='softmax'))\n\n# sgd = optimizers.SGD(lr = 0.01, momentum = 0.9)\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(partial_x_train, partial_y_train, validation_data=(x_val, y_val),\n                    batch_size=64, epochs=25, verbose=2)","d4808190":"history_dict = history.history","d14bcc32":"history_dict.keys()","8dc3f49b":"training_loss = history_dict[\"loss\"]\nval_loss = history_dict[\"val_loss\"]\n\nepochs = range(1, len(training_loss) + 1)\n\nplt.plot(epochs, training_loss, 'bo', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.legend()\nplt.show()","f233780e":"training_acc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\n\nepochs = range(1, len(val_acc) + 1)\n\nplt.plot(epochs, training_acc, 'bo', label=\"Training Accuracy\")\nplt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n\nplt.legend()\nplt.show()","c4c8fadf":"#Let's evaluate...\nscores = model.evaluate(X_test_enc_sc, y_test_cat, batch_size=128)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","b3251e7b":"# Exploration","1b1fc780":"This is it for today. Appreciate any advice, recommendations, or feedback on how I could improve. One thought is that I perhaps may be overfitting at just around 8 epochs. Neural Networks seem to be overkill for this problem as I had a very simple layer (just 8 hidden units). But of course I could be wrong here so would love to hear any couterarguments,etc. Thanks!"}}