{"cell_type":{"25e07946":"code","6274d08a":"code","458ab680":"code","e3f16446":"code","636d3ba7":"code","c73abcc9":"code","572860d2":"code","2bad52aa":"code","2cb262c1":"code","5113cfc3":"code","659981f4":"code","538dd5e8":"code","ac3ca73c":"code","d5ad6d7f":"code","14e4f695":"code","00c7ba3b":"markdown"},"source":{"25e07946":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6274d08a":"from sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.metrics import log_loss, accuracy_score\nimport xgboost as xgb\nfrom xgboost import XGBClassifier","458ab680":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport logging\nfrom heamy.dataset import Dataset\nfrom heamy.estimator import Classifier\nfrom heamy.pipeline import ModelsPipeline\nCACHE=False\nNFOLDS = 5\nDATA_DIR = \"..\/input\/learn-together\"\nSUBMISSION_FILE = \"{0}\/sample_submission.csv\".format(DATA_DIR)","e3f16446":"def addFeatures(df):\n    #horizontal and vertical distance to hydrology can be easily combined\n    cols = ['Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology']\n    df['distance_to_hydrology'] = df[cols].apply(np.linalg.norm, axis=1)\n    \n    #adding a few combinations of distance features to help enhance the classification\n    cols = ['Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points',\n            'Horizontal_Distance_To_Hydrology']\n    df['distance_mean'] = df[cols].mean(axis=1)\n    df['distance_sum'] = df[cols].sum(axis=1)\n    df['distance_dif_road_fire'] = df[cols[0]] - df[cols[1]]\n    df['distance_dif_hydro_road'] = df[cols[2]] - df[cols[0]]\n    df['distance_dif_hydro_fire'] = df[cols[2]] - df[cols[1]]\n    \n    #taking some factors influencing the amount of radiation\n    df['cosine_of_slope'] = np.cos(np.radians(df['Slope']) )\n    #X['Diff_azimuth_aspect_9am'] = np.cos(np.radians(123.29-X['Aspect']))\n    #X['Diff_azimuth_aspect_12noon'] = np.cos(np.radians(181.65-X['Aspect']))\n    #X['Diff_azimuth_aspect_3pm'] = np.cos(np.radians(238.56-X['Aspect']))\n\n    #sum of Hillshades\n    shades = ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\n    #df['Sum_of_shades'] = df[shades].sum(1)\n    weights = pd.Series([0.299, 0.587, 0.114], index=cols)\n    df['hillshade'] = (df[shades]*weights).sum(1)\n\n    df['elevation_vdh'] = df['Elevation'] - df['Vertical_Distance_To_Hydrology']\n    \n    #binned features\n    bin_defs = [\n        # col name, bin size, new name\n        ('Elevation', 200, 'Binned_Elevation'), \n        ('Aspect', 45, 'Binned_Aspect'),\n        ('Slope', 6, 'Binned_Slope'),\n    ]\n    \n    for col_name, bin_size, new_name in bin_defs:\n        df[new_name] = np.floor(df[col_name]\/bin_size)\n        \n    print('Total number of features : %d' % (df.shape)[1])\n    return df","636d3ba7":"weights = [7.593211270832795, 1, 1.939598531912596, 1.1429202760461918, 1.1032958863198228, 1.0883962288167983,\n           2.5458413312744073, 1.3142614728641315, 2.8449343430207925, 0.8136208789805158, 1.0709147648909365, \n           0.7707629737823462, 1.0, 0.8104528513180582, 1.0, 1.0749417474588394, 0.7654163512155585, 0.8030187040170972,\n           1.0329576637993996, 7.614421045866052, 1, 0.9476259741687187, 800.0, 533.3204003431088, 879.7676928167089, \n           670.0124183890873, 618.7672505753314, 552.7902772091511, 765.8603672921006, 800.0, 726.0527101376886, \n           597.4990430522444, 670.7967759239023, 482.17013705648037, 541.3176529658227, 800, 800.0, 697.5757747281089, \n           631.1298454088087, 642.4394575971372, 629.6475870178654, 779.8391046580264, 800.0, 281.70458567820396, \n           664.715674176997, 1127.2316339753593, 704.1050089801397, 791.4569525873807, 386.18744192290285, \n           666.9332363939681, 502.1957048164419, 681.803592224423, 601.6970391056689, 768.0795748801415, 800.0, \n           741.1224887205307, 732.1737832122601, 800, 800, 954.5115560927762, 638.6112784779291, 800, 798.1741071391841, \n           800.0, 800.0, 662.1283969196991]","c73abcc9":"def preprocessData():\n    \n    train = pd.read_csv('..\/input\/learn-together\/train.csv', index_col='Id')\n    test = pd.read_csv('..\/input\/learn-together\/test.csv', index_col='Id')\n    y_train = train['Cover_Type']\n    \n    classes = train.Cover_Type.unique()\n    num_classes = len(classes)\n    print(\"There are %i classes: %s \" % (num_classes, classes))\n    train.drop(['Cover_Type'], axis=1, inplace=True)\n\n    train = addFeatures(train)    \n    test = addFeatures(test)\n\n    dtrn_first_ten = train.loc[:,:'Horizontal_Distance_To_Fire_Points']\n    dtrn_wa_st = train.loc[:,'Wilderness_Area1':'Soil_Type40']\n    dtrn_added_features = train.loc[:,'distance_to_hydrology':]\n    dtrn_ = pd.concat([dtrn_first_ten,dtrn_added_features,dtrn_wa_st],axis=1)\n\n    dtst_first_ten = test.loc[:,:'Horizontal_Distance_To_Fire_Points']\n    dtst_wa_st = test.loc[:,'Wilderness_Area1':'Soil_Type40']\n    dtst_added_features = test.loc[:,'distance_to_hydrology':]\n    dtst_ = pd.concat([dtst_first_ten,dtst_added_features,dtst_wa_st],axis=1)\n    \n    for i in range(len(train.columns)):\n        c = train.columns[i]\n        train[c] *= weights[i]\n        test[c] *= weights[i]\n    \n    train = dtrn_.values\n    test = dtst_.values\n    y = y_train.ravel()-1\n   \n    return {'X_train': train, 'X_test': test, 'y_train': y}  #dtrn_, dtst_, y_train","572860d2":"dataset = Dataset(preprocessor=preprocessData, use_cache=True)","2bad52aa":"knn_param = {\n    'n_neighbors':1,\n    'p':1\n}\n\nrf_param = {    \n    'n_estimators':181, \n    'max_features':'sqrt', \n    'bootstrap':False,\n    'max_depth':60,\n    'min_samples_split':2,\n    'min_samples_leaf':1,\n    'random_state':1\n    }\n\net_param = {\n    'n_estimators':500,\n    'max_features':66,\n    'min_samples_split':5,\n    'min_samples_leaf':1,\n    'random_state':1\n    }\n\nlgb_param = {\n    'objective':'multiclass',\n    'num_class':7,\n    'learning_rate':0.2,\n    'num_leaves':149,\n    'random_state':1\n}\n\nlr_param = {\n    'multi_class':'multinomial', \n    'solver':'newton-cg', \n    'random_state':1\n}\n\nxg_params = {\n        'seed': 0,\n        'colsample_bytree': 0.7,\n        'silent': 1,\n        'subsample': 0.7,\n        'learning_rate': 0.1,\n        'objective': 'multi:softprob',   \n        'num_class': 7,\n        'max_depth': 4,\n        'min_child_weight': 1,\n        'eval_metric': 'mlogloss',\n        'nrounds': 200\n    }","2cb262c1":"knn = Classifier(dataset=dataset, estimator = KNeighborsClassifier, use_cache=CACHE, parameters=knn_param,name='knn')\nrf = Classifier(dataset=dataset, estimator = RandomForestClassifier, use_cache=CACHE, parameters=rf_param,name='rf')\net = Classifier(dataset=dataset, estimator=ExtraTreesClassifier, use_cache=CACHE, parameters=et_param,name='et')\nlgb = Classifier(dataset=dataset, estimator=LGBMClassifier, use_cache=CACHE, parameters=lgb_param,name='lgb')\nlr = Classifier(dataset=dataset, estimator=LogisticRegression, use_cache=CACHE, parameters=lr_param,name='lr')\nxgf = Classifier(dataset=dataset, estimator=XGBClassifier, use_cache=CACHE, parameters=xg_params,name='xgf')\n","5113cfc3":"pipeline = ModelsPipeline(knn, rf, et, lgb, lr, xgf) \nstack_ds = pipeline.stack(k=NFOLDS,seed=1)","659981f4":"# Train LogisticRegression on stacked data (second stage)\nlr = LogisticRegression\nlr_params = {'C': 5, 'random_state' : 1, 'solver' : 'liblinear', 'multi_class' : 'ovr',}\nstacker = Classifier(dataset=stack_ds, estimator=lr, use_cache=False, parameters=lr_params)","538dd5e8":"results = stacker.validate(k=NFOLDS,scorer=log_loss)","ac3ca73c":"dtrain = xgb.DMatrix(stack_ds.X_train, label=stack_ds.y_train)\ndtest = xgb.DMatrix(stack_ds.X_test)\n\nxgb_params = {\n    'seed': 0,\n    'colsample_bytree': 0.8,\n    'silent': 1,\n    'subsample': 0.6,\n    'learning_rate': 0.05,\n    'objective': 'multi:softprob',\n    'num_class': 7,        \n    'max_depth': 6,\n    'num_parallel_tree': 1,\n    'min_child_weight': 1,\n    'eval_metric': 'mlogloss',\n}\n\nres = xgb.cv(xgb_params, dtrain, num_boost_round=1000, \n             nfold=NFOLDS, seed=1, stratified=True,\n             early_stopping_rounds=20, verbose_eval=5, show_stdv=True)\n\nbest_nrounds = res.shape[0] - 1\ncv_mean = res.iloc[-1, 2]\ncv_std = res.iloc[-1, 3]\n\nprint('Ensemble-CV: {0}+{1}, best nrounds = {2}'.format(cv_mean, cv_std, best_nrounds))","d5ad6d7f":"model = xgb.train(xgb_params, dtrain, best_nrounds)\nxpreds = model.predict(dtest)\npredictions = np.round(np.argmax(xpreds, axis=1)).astype(int) + 1","14e4f695":"submission = pd.read_csv(SUBMISSION_FILE)\nsubmission['Cover_Type'] = predictions\nsubmission.to_csv('submission.csv', index=None)","00c7ba3b":"# Building on shared work.\n### Still trying to get Stacking to improve the score. I have used this [notebook](https:\/\/www.kaggle.com\/justfor\/ensembling-and-stacking-with-heamy) as my starting point."}}