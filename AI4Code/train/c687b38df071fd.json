{"cell_type":{"2641dd3c":"code","6abe5b51":"code","ba117e75":"code","95d22087":"code","40044f9d":"code","6e5dbbf6":"code","bf2ad87a":"code","85c03d45":"code","20c01c17":"code","688efac6":"code","d66157cc":"code","4b473e29":"code","f0efc244":"code","7404a17e":"code","c48e8adf":"markdown","66fea443":"markdown","6eb404ad":"markdown","c7cb0285":"markdown","2f0ce8ac":"markdown","e8301dfb":"markdown","1c73bfad":"markdown","2acfa7bd":"markdown"},"source":{"2641dd3c":"data = [\n    {'price': 850000, 'rooms': 4, 'neighborhood': 'Queen Anne'},\n    {'price': 700000, 'rooms': 3, 'neighborhood': 'Fremont'},\n    {'price': 650000, 'rooms': 3, 'neighborhood': 'Wallingford'},\n    {'price': 600000, 'rooms': 2, 'neighborhood': 'Fremont'}\n]","6abe5b51":"{'Queen Anne': 1, 'Fremont': 2, 'Wallingford': 3};","ba117e75":"from sklearn.feature_extraction import DictVectorizer\nvec = DictVectorizer(sparse=False, dtype=int)\nvec.fit_transform(data)","95d22087":"#To see the meaning of each column, you can inspect the feature names:\n#one clear disadvantage of this approach: if your category has many possible values, this can greatly increase the size of your dataset\n\nvec.get_feature_names()","40044f9d":"vec = DictVectorizer(sparse=True, dtype=int)\nvec.fit_transform(data)","6e5dbbf6":"sample = ['problem of evil', 'evil queen', 'horizon problem']","bf2ad87a":"from sklearn.feature_extraction.text import CountVectorizer\n\nvec = CountVectorizer()\nX = vec.fit_transform(sample)\nX","85c03d45":"import pandas as pd\npd.DataFrame(X.toarray(), columns=vec.get_feature_names())","20c01c17":"from sklearn.feature_extraction.text import TfidfVectorizer\nvec = TfidfVectorizer()\nX = vec.fit_transform(sample)\npd.DataFrame(X.toarray(), columns=vec.get_feature_names())","688efac6":"%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.array([1, 2, 3, 4, 5])\ny = np.array([4, 2, 1, 3, 7])\nplt.scatter(x, y);","d66157cc":"from sklearn.linear_model import LinearRegression\nX = x[:, np.newaxis]\nmodel = LinearRegression().fit(X, y)\nyfit = model.predict(X)\nplt.scatter(x, y)\nplt.plot(x, yfit);","4b473e29":"from sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=3, include_bias=False)\nX2 = poly.fit_transform(X)\nprint(X2)","f0efc244":"model = LinearRegression().fit(X2, y)\nyfit = model.predict(X2)\nplt.scatter(x, y)\nplt.plot(x, yfit);","7404a17e":"model = LinearRegression().fit(X2, y)\nyfit = model.predict(X2)\nplt.scatter(x, y)\nplt.plot(x, yfit);","c48e8adf":"# Text Features","66fea443":"There are some issues with this approach, however: the raw word counts lead to features which put too much weight on words that appear very frequently, and this can be sub-optimal in some classification algorithms.","6eb404ad":"For a vectorization of this data based on word count, we could construct a column representing the word \"problem,\" the word \"evil,\" the word \"horizon,\" and so on. While doing this by hand would be possible, the tedium can be avoided by using Scikit-Learn's CountVectorizer","c7cb0285":"# DictVectorizer","2f0ce8ac":"One approach to this is to transform the data, adding extra columns of features to drive more flexibility in the model. For example, we can add polynomial features to the data this way:","e8301dfb":"# Derived Features","1c73bfad":"# Feature Engineering - For Text and Image Data\n\nThis Notebook explain you features for representing categorical data, features for representing text, and features for representing images. Additionally, we will discuss derived features for increasing model complexity and imputation of missing data. Often this process is known as vectorization, as it involves converting arbitrary data into well-behaved vectors.","2acfa7bd":"Another useful type of feature is one that is mathematically derived from some input features. We saw an example of this in Hyperparameters and Model Validation when we constructed polynomial features from our input data. We saw that we could convert a linear regression into a polynomial regression not by changing the model, but by transforming the input! This is sometimes known as basis function regression, and is explored further in In Depth: Linear Regression."}}