{"cell_type":{"cd833313":"code","4eebdaab":"code","9d205d30":"code","db984d26":"code","37ec3b2f":"code","6533eee5":"code","f699c50e":"code","a399745c":"code","4de68612":"code","21efe457":"code","c5c9f38b":"code","dc39cd22":"code","1a7c4768":"code","9986515b":"code","4af78d63":"code","934dd0fd":"code","a9f36dc8":"code","ad0f215a":"code","8316fbe1":"code","4c8562f6":"code","77023c41":"code","b6649495":"code","eb2c46f4":"code","f1b280ec":"code","10d8d81a":"code","1d0d0ed5":"code","e7cc6b9b":"code","371d8e1e":"code","3058a06a":"code","35ecd9b4":"code","8f743e86":"code","584612ca":"code","0129adf2":"code","2b66944b":"code","b59519e3":"code","3b0d56f9":"code","91ada1b4":"code","59c23b96":"markdown","cebb757b":"markdown","d27d4c35":"markdown","789a2b1a":"markdown","15687f9d":"markdown","6ab993ac":"markdown","5fba9631":"markdown","d6dedb8b":"markdown","0cdbd26f":"markdown","95a1d75f":"markdown","6136bd08":"markdown","30cf0d7c":"markdown","442dffb4":"markdown","c18a142e":"markdown","fad9b3f5":"markdown","feb2012f":"markdown","5a447250":"markdown","aabc179a":"markdown","1f5419c9":"markdown","788dc82d":"markdown","f283e303":"markdown","40c03d9f":"markdown","a587975a":"markdown"},"source":{"cd833313":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/oct2017\/OCT2017 \/test\"))\n\n# Any results you write to the current directory are saved as output.","4eebdaab":"TRAIN_PATH='..\/input\/oct2017\/OCT2017 \/train'\nTEST_PATH='..\/input\/oct2017\/OCT2017 \/test'\nVAL_PATH='..\/input\/oct2017\/OCT2017 \/val'","9d205d30":"import os\n\nimport gc\nimport re\nimport operator \n\nimport numpy as np\nimport pandas as pd\n\nfrom gensim.models import KeyedVectors\n\nfrom sklearn import model_selection\n\n\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom keras.models import Sequential\nfrom keras.layers import Convolution2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Flatten\nfrom keras.layers import Dense\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Dropout\n\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten, Masking\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint,  Callback, EarlyStopping, ReduceLROnPlateau\n","db984d26":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.optim as optim\nimport torch.utils.data\nfrom torch.autograd import Variable","37ec3b2f":"def create_model(in_shape,pool_size,kernal_size):\n    inputs=Input(shape=in_shape)\n    x=Convolution2D(filters=32,kernel_size=kernal_size,activation='relu')(inputs)\n    x=MaxPooling2D(pool_size=pool_size)(x)\n    x=Dropout(0.3)(x)\n    x=Convolution2D(filters=64,kernel_size=kernal_size,activation='relu')(x)\n    x=MaxPooling2D(pool_size=pool_size)(x)\n    x=Dropout(0.3)(x)\n    x=Convolution2D(filters=128,kernel_size=kernal_size,activation='relu')(x)\n    x=MaxPooling2D(pool_size=pool_size)(x)\n    x=Dropout(0.3)(x)\n    x=Convolution2D(filters=128,kernel_size=kernal_size,activation='relu')(x)\n    x=MaxPooling2D(pool_size=pool_size)(x)\n    x=Dropout(0.3)(x)\n    x=Flatten()(x)\n    x=Dense(4,activation='softmax')(x)\n    return x,inputs\n    ","6533eee5":"out,ins=create_model(in_shape=(256,256,3),pool_size=(2,2),kernal_size=(3,3))","f699c50e":"model=Model(input=ins,output=out)","a399745c":"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])","4de68612":"from keras.callbacks import EarlyStopping\nearly_stop=EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n","21efe457":"import warnings\nwarnings.filterwarnings(\"ignore\")","c5c9f38b":"from keras.preprocessing.image import ImageDataGenerator\ndef model_trainer(model):\n    train_datagen = ImageDataGenerator(rescale = 1.\/150, \n                                   shear_range = 0.01, \n                                   zoom_range =[0.9, 1.25],\n                                   rotation_range=20,\n                                   zca_whitening=True,\n                                   vertical_flip=True,\n                                   fill_mode='nearest',\n                                   width_shift_range=0.1,\n                                   height_shift_range=0.1,\n                                   brightness_range=[0.5, 1.5],\n                                   horizontal_flip = True)\n    test_datagen = ImageDataGenerator(rescale = 1.\/160)\n\n    train_generator = train_datagen.flow_from_directory(\n        TRAIN_PATH,\n        target_size=(256,256),\n        batch_size=32,\n        class_mode='categorical')\n\n    validation_generator = test_datagen.flow_from_directory(\n        TEST_PATH,\n        target_size=(256,256),\n        batch_size=32,\n        class_mode='categorical')# multiclass then  categorical\n\n    hist=model.fit_generator(\n        train_generator,\n        steps_per_epoch=2000, # no of images in training set\n        epochs=10,\n        shuffle=True,\n        validation_data=validation_generator,\n        validation_steps=968,callbacks=[early_stop]) # no of  images in test\n    return hist,train_generator","dc39cd22":"hist,train_generator=model_trainer(model)\nprint(hist)\n","1a7c4768":"#VISUAL ANALYSIS \nimport numpy as np   \nfrom keras.preprocessing import image    \nim1_path=\"..\/input\/oct2017\/OCT2017 \/test\/NORMAL\/NORMAL-1025847-1.jpeg\"\ntest_image=image.load_img(im1_path,target_size=(256,256))","9986515b":"import matplotlib.pyplot as plt\nplt.imshow(test_image)\n# now to convert to 3 dimensional from 2d\ntest_image=image.img_to_array(test_image)\nprint(test_image.size)\n","4af78d63":"#test_image=image.img_to_array(test_image)\ntest_image= np.expand_dims(test_image,axis=0)\n","934dd0fd":"train_generator.class_indices","a9f36dc8":"result=np.argmax(model.predict(test_image))\nprint(result)","ad0f215a":"def get_name_layer_filters(model):\n    filter_whole=[]\n    layer_whole=[]\n    for layer in model.layers:\n        if 'conv' not in layer.name:\n            continue\n        filters,biases=layer.get_weights()\n        filter_whole.append(filters)\n        layer_whole.append(biases)\n        print(layer.name,filters.shape)\n    return filter_whole,layer_whole    \n        ","8316fbe1":"filter_whole,layer_whole=get_name_layer_filters(model)","4c8562f6":"filters,biases=model.layers[1].get_weights()","77023c41":"f_min,f_max=filters.min(),filters.max()\nfilters=(filters-f_min)\/(f_max-f_min)","b6649495":"from matplotlib import pyplot\nn_filters,ix=6,1\nfor i in range(n_filters):\n    f=filters[:,:,:,i]\n    #Plot each channel\n    for j in range(3):\n        ax=pyplot.subplot(n_filters,3,ix)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        #Plot filter channel\n        pyplot.imshow(f[:,:,j])\n        ix+=1\n        \npyplot.show()    ","eb2c46f4":"#FEATURE MAP\nmodel_feature=Model(inputs=model.inputs,outputs=model.layers[4].output)","f1b280ec":"#We use result from previous\nfeature_map=model_feature.predict(test_image)\n","10d8d81a":"feature_map.shape","1d0d0ed5":"#plot all 32 maps in an 8*4 squares\npyplot.figure(figsize=(30,30))        \n        \nsquare=8\nix=1\nfor _ in range(4):\n    for _ in range(8):\n        ax=pyplot.subplot(square,square,ix)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        pyplot.imshow(feature_map[0,:,:,ix-1])\n        ix+=1\n        \npyplot.show()","e7cc6b9b":"def get_convolutional_layers(model):\n    convolutions_models=[]\n    for layer in model.layers:\n        if 'conv2d' not in layer.name:\n            continue\n        model_temp=Model(inputs=model.inputs,outputs=layer.output)\n        convolutions_models.append(model_temp)\n    return convolutions_models    \n        \n        ","371d8e1e":"#To see each feature map systematically for every convolutional layer\ndef generate_feature_maps(model,test_image):\n    models=get_convolutional_layers(model)#Fetching convolution layers models\n    feature_maps=[]\n    \n    for model_temp in models:\n        feature_map=model_feature.predict(test_image)\n        feature_maps.append(feature_map)\n    return feature_maps,models    \n","3058a06a":"def plot_graph(feature_map):\n    \n    #plot all 32 maps in an 8*4 squares\n    pyplot.figure(figsize=(30,30))        \n        \n    square=8\n    ix=1\n    for _ in range(4):\n        for _ in range(8):\n            ax=pyplot.subplot(square,square,ix)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            pyplot.imshow(feature_map[0,:,:,ix-1])\n            ix+=1\n        \n    pyplot.show()","35ecd9b4":"def plots_generator(model):\n    print(\"IMAGE UNDER CONSIDERATION\")\n    test_image=image.load_img(im1_path,target_size=(256,256))\n    plt.imshow(test_image)\n    test_image=image.img_to_array(test_image)\n\n    test_image= np.expand_dims(test_image,axis=0)\n    print()\n    feature_maps,models=generate_feature_maps(model,test_image)\n    #ax=pyplot.subplot(square,square,ix)# only 32 filters will be shown of each layer\n    counter=1\n    for each_map in feature_maps:\n        print(\"Convolutional Layer Number {} \".format(counter))\n        counter+=1\n        #ax=pyplot.subplot(square,square,ix)\n        plot_graph(each_map)\n","8f743e86":"\nplots_generator(model)","584612ca":"hist.history['val_acc']","0129adf2":"def val_acc_plot():\n    print(\"FOR MODEL VALIDATION DATA\")\n    plt.plot(hist.epoch,hist.history['val_acc'])\n    plt.xlabel(\"EPOCHS\")\n    plt.ylabel(\"Validation Accuracy\")\n    ","2b66944b":"def generate_images(all_paths):\n    test_images=[]\n    interpret=train_generator.class_indices\n    test_y=[]\n    \n    for path in all_paths:\n        y=''\n        if 'DME' in path:\n            y='DME'\n        elif 'DRUSEN' in path:   \n            y='DRUSEN'\n        elif 'CNV' in path:   \n            y='CNV'\n        elif 'NORMAL' in path:   \n            y='NORMAL'    \n        \n        for image_path in os.listdir(path):\n            new_path=os.path.join('..\/input\/oct2017\/OCT2017 \/test',y)\n            #print(new_path)\n            \n            new_path=os.path.join(new_path,image_path)\n            if 'Store' in str(new_path):\n                continue\n            temp_images=image.load_img(new_path,target_size=(256,256))\n            temp_images=image.img_to_array(temp_images)\n            test_images.append(temp_images)\n            test_y.append(interpret[y])\n    return test_images,test_y        \n","b59519e3":"def generate_predictions(test_images,model):\n    predictions=np.argmax(model.predict(test_images),axis=1)\n    return predictions\n    ","3b0d56f9":"from sklearn import metrics\n\ndef convert_data(model):\n    # Now testing all test images to find test set accuracy.\n    #We first need to put all test images and convert them in desired format to predict\n    #Firstly we store path of test directory\n    PATH_TEST=\"..\/input\/oct2017\/OCT2017 \/test\"\n    all_paths=[]\n    print(\"GENERATING PATHS\")\n    for directory in os.listdir(PATH_TEST):\n        if 'Store' in directory:\n            continue\n        all_paths.append(os.path.join(PATH_TEST,directory))\n    \n    test_images,test_y=generate_images(all_paths)\n    print(\"PATH GENERATED\")\n    test_images=np.array(test_images)\n    print(\"GENERATING PREDICTIONS\")\n    predictions=generate_predictions(test_images,model)\n    print(\"PREDICTIONS GENERATED\")\n    print()\n    print(\"ACCURACY OF MODEL FOR TEST DATA IS {}\".format(metrics.accuracy_score(test_y, predictions)))\n    \n    \n","91ada1b4":"convert_data(model)","59c23b96":"**Now as we can observe as we move into deeper convolutional layers features get more refined and clearer. We can also add up custom filters which may lead to better feature detection, however let's keep it for our next kernel.**","cebb757b":"**Using ImageDataGenerator for model training also . Epochs can be increased to get better accuracy on validation data as much as I tested validation accuracy of about 0.94 was achieved by 25 epochs. Also a better accuracy will be met when steps_per_epoch is equal to number of images in training set**","d27d4c35":"**Starting with making model I made a simple model with 4 convolution layers. My primary objective is just to plot feature maps to see detections by filters.**","789a2b1a":"**Taking a sample image. The very first image in test folder under DME class.**","15687f9d":"**Creating call back for early stopping.**","6ab993ac":"**We can well observe that our model in its first convolutional layer is trying to detect features as evident from feature maps above.We can even observe edge detections as observed in the subplot(2,5).The model tries to detect the image and is studying its features. Some of these features can be easily seen by human eye. Higher the convoluional layers finer these detections become as becomes evident when we further plot next convolutional layers.**\n\n\n**Now implementing above feature map plotting for all convolution layer.**","5fba9631":"**Checking Validation accuracies**","d6dedb8b":"**Here we call the function above this function call returns filters and biases**","0cdbd26f":"**Getting convolutional layer filter maps. I just made the below function more like a utility function to quickly get all convolutional layers filters and biases. **","95a1d75f":"**Function to start with plotting feature maps.**","6136bd08":"**We will try to better accuracy and try increasing it above 90 atleast in continuation of this kernel later on.**","30cf0d7c":"**Plotting the filters**","442dffb4":"**Normalizing the filters**","c18a142e":"**This function returns feature maps and models list**","fad9b3f5":"**This function returns models from all convolutional layers in our model**","feb2012f":"**Using previous model to predict test_image**","5a447250":"**What Is Optical Coherence Tomography?**\n\n\nOptical coherence tomography (OCT) is a non-invasive imaging test. OCT uses light waves to take cross-section pictures of your retina.\n\nWith OCT, your ophthalmologist can see each of the retina\u2019s distinctive layers.  This allows your ophthalmologist to map and measure their thickness. These measurements help with diagnosis. They also provide treatment guidance for glaucoma and diseases of the retina. These retinal diseases include age-related macular degeneration (AMD) and diabetic eye disease.\n\n\n![RETINAL OCT OF A NORMAL EYE](https:\/\/www.aao.org\/image.axd?id=0acdb964-d6ca-4034-ab9c-a731c5f5edf2&t=635072561627800000)\n@IMAGE COURTESYhttps:\/\/www.aao.org \n\n\n\nOCT is useful in diagnosing many eye conditions, including:\n\n* macular hole\n* macular pucker\n* macular edema\n* age-related macular degeneration\n* glaucoma\n* central serous retinopathy\n* diabetic retinopathy\n* vitreous traction\n\n\n**ABOUT OUR DATA SET**\n\n* The dataset is organized into 3 folders **(train, test, val)**. It contains subfolders for each image category **(NORMAL,CNV,DME,DRUSEN)**. There are 84,495 X-Ray images (JPEG) and 4 categories (NORMAL,CNV,DME,DRUSEN). \n\n* Images are labeled as (disease)-(randomized patient ID)-(image number by this patient) and split into 4 directories: CNV, DME, DRUSEN, and NORMAL.\n \n\n**WHAT ALL TO EXPECT FROM THIS KERNEL**\n\n1. Feature map visualization and filter visualization.\n\n2. Modular code so that functions in this kernel can be used easily by everyone,all you need to do is define the model and pass it through function call to convert_data function as demonstrated in last line by convert_data(model). \n\n3. You can easily increase accuracy by adjusting hyper-parameters, for demonstration purposes,due to memory constraints and since I just wanted to show feature map visualization and filter visualization while maintaining modularity of code.\n\n4. Scalable approach thus you can easily put in any adaptive learning model or a bigger CNN too and pass the model to convert_data function. It will detect all convolutional layers and plot 32 feature maps of each convolutional layer in model.  \n\n\n**HOW WILL VISUALIZATION OF FEATURES AND FILTERS HELP**\n\n1. What I find best part about feature visualization is that it kind of tells us how our eye detects feature. If suppose we have an object we analyze many features of that object like its shape,colour,size etc, when we see an object say in a photograph our eye tries to find patterns in the photograph that is peculiar to something it has seen before.These patterns thus play a vital role in discerning the object we detect.What more images in these dataset is of an eye itself (talk about inception :P).\n\n2. Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. Thus visualization can help to gain an insight to what our model has learnt.\n\n3. The patterns so detected can be study to adjust hyperparameters and filters in a way to get more accuracy.\n\n**I would be grateful to anyone who can help me improve my code I am still learning. Any suggestions are appreciated. I tried to maintain modularity while coding for easy understanding. Thank you.**\n\n**So let's get started with this kernel**\n","aabc179a":"**Creating model**","1f5419c9":"**Plot feature maps**\n\n32 feature maps will be plotted. We will try to analyze these maps and interpret them in a useful way after plotting them.","788dc82d":"**This function plots 32 feature maps.**","f283e303":"**Starting with the plotting**\n\n**Making a model from feature map.We will start with making a model with outputs from 1st convolution layer of model and input from model.**\n","40c03d9f":"**We take exactly the first convolutional layer and get filters and biases**","a587975a":"**Generating plots and showing original image.**"}}