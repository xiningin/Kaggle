{"cell_type":{"44596cac":"code","f9efbaea":"code","bac52285":"code","33455bc5":"code","dd4dacee":"code","af86dda4":"code","6c70b5f7":"code","ae9c0708":"code","676b539d":"code","a6d52395":"code","98f8c693":"code","30972005":"code","29513aa0":"markdown","a0611d71":"markdown","534183e7":"markdown","2bb6194d":"markdown","7377c251":"markdown","b3ae00f3":"markdown","1a5e54fb":"markdown","f3eaa086":"markdown","52dbd12c":"markdown","a7f8a1f3":"markdown","4046e96a":"markdown","bf5fbe5b":"markdown","b5d30660":"markdown","3189d810":"markdown","8c01589c":"markdown","6d1b9ffe":"markdown","137e57b4":"markdown","73ecc135":"markdown"},"source":{"44596cac":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve,roc_auc_score,accuracy_score\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","f9efbaea":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')\ny = train['target']\n\ncols = ['f'+str(i) for i in range(100)] #feature columns","bac52285":"scaler = StandardScaler()\ntrain[cols] = scaler.fit_transform(train[cols])\ntest[cols] = scaler.transform(test[cols])","33455bc5":"model = LogisticRegression(solver='liblinear')\nmodel.fit(train[cols],y);","dd4dacee":"y_pred_proba = model.predict_proba(train[cols])[:, 1]\nauc = roc_auc_score(y, y_pred_proba)\n\nacc = accuracy_score(y, model.predict(train[cols]))\n\nprint(f\"accuracy: {round(acc*100,3)} , auc: {round(auc*100,3)}\")","af86dda4":"fpr, tpr, _ = roc_curve(y,  y_pred_proba)\nplt.plot(fpr,tpr,label=\"data, auc = \"+str(round(auc*100,2)))\nplt.legend(loc=4)\nplt.show()","6c70b5f7":"c0 = model.intercept_[0]\nci = model.coef_[0]","ae9c0708":"train['LRLC'] = c0\nfor i in range(100):\n    train['LRLC'] += ci[i] * train['f' + str(i)]","676b539d":"y0 = train[train['target'] == 0]\ny1 = train[train['target'] == 1]","a6d52395":"plt.hist(y0['LRLC'], bins = np.arange(-4, 4, 0.1), alpha = 0.3);\nplt.title('target = 0');\nplt.xlabel('LRLC');\nplt.axvline(0, color = 'k');\n\nplt.show()\n\nplt.hist(y1['LRLC'], bins = np.arange(-4, 4, 0.1), alpha = 0.3);\nplt.title('target = 1');\nplt.xlabel('LRLC');\nplt.axvline(0, color = 'k');","98f8c693":"h0 = np.histogram(y0['LRLC'], bins = np.arange(-2, 2, 0.1))\nh1 = np.histogram(y1['LRLC'], bins = np.arange(-2, 2, 0.1))\n\nlrlc = (h0[1][1:] + h0[1][:-1])\/2. #average in the bin\nfraction_of_0 = h0[0]\/(h0[0]+h1[0])\n\nplt.plot(lrlc, fraction_of_0, marker = 'o')\nplt.axhline(0.75, c = 'gray', ls = '--')\nplt.axhline(0.25, c = 'gray', ls = '--');\nplt.xlabel('LRLC')\nplt.ylabel('Fraction with target = 0');","30972005":"plt.plot(lrlc, fraction_of_0, marker = 'o')\nplt.axhline(0.75, c = 'gray', ls = '--')\nplt.axhline(0.25, c = 'gray', ls = '--');\nplt.xlabel('LRLC')\nplt.ylabel('Fraction with target = 0');\nplt.ylim([0.73,0.77]);\n\nplt.show()\n\nplt.plot(lrlc, fraction_of_0, marker = 'o')\nplt.axhline(0.75, c = 'gray', ls = '--')\nplt.axhline(0.25, c = 'gray', ls = '--');\nplt.xlabel('LRLC')\nplt.ylabel('Fraction with target = 0');\nplt.ylim([0.23,0.27]);","29513aa0":"Apply standard scaler to the data for faster training","a0611d71":"Fit logistic regression to the data - use all the data as we are not interested in CV here","534183e7":"In the ROC curve plot above we can notice a strange linear behavior on both ends... Let's dig a bit more into what is going on! We start by calculating the linear combination logistic regression found (let's label it (LRLC):","2bb6194d":"Let's define samples of emails marked as spam \/ ham for future convenience","7377c251":"And calculate it for each of our training examples","b3ae00f3":"By analyzing results of the logistic regression, we find it most likely that 25% of the train and test events were mislabeled. This would set an upper bound of AUC = 0.75 or this competition, which is consistent with the current public leaderboard.\n\nTo improve training, we suggest dropping mislabeled events from the training or flipping their target values.\n\nInspired by the ROC curve from [notebook](https:\/\/www.kaggle.com\/hamzaghanmi\/make-it-simple)","1a5e54fb":"This seems to confirm our expectations - further away from zero LRLC (where the logistic regression does not do a 100% perfect job separating the two categories), we find that the fraction of the samples with target = 0 plateaus to about 75% \/ **25%**. This looks like a nice, round number, which again agrees with our assertion that there is an intentional mislabelling going on.","f3eaa086":"# Digging deeper into what is going on","52dbd12c":"# Summary","a7f8a1f3":"Ok, this definitely looks super suspicious! Clearly there are two populations:\n\n1) one where the logistic regression variable LRLC does a pretty good job separating events with target = 0 and target = 1\n\n2) one where the value of LRLC does not seem to matter at all\n\nWe believe this is because after the data set was created, **the target value for part of the emails was intentionally mislabeled**. This scenario is not that far fetched, given an almost-perfect model was submitted within minutes of the competition start and organizers had to act quickly.\n\nLet's look at the percentage of mislabeled emails by comparing the corresponding histograms:","4046e96a":"Plot of roc curve","bf5fbe5b":"And plot the distributions of LRLC for both","b5d30660":"Let's assume we have a perfect classifier of the events before the mislabelling. Here we estimate what maximal AUC such a classifier would achieve. Some [background reading about AUC](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/roc-and-auc) for those who need a refresher.\n\nBefore the mislabelling, our classifier would correctly order the samples - 50% of the samples that are red (target = 0) to the left side, 50% of the samples that are green (target = 1) to the right. There is no mixing and we have AUC = 1.\n\nAfter the mislabelling, the left side is 75% red and 25% green, while the right side is 75% green and 25% red. AUC is then probability of drawing green \"marble\" right of a red \"marble\" when drawing one green and one red marble at random.\n\nWe have the following options:\n\n1) green marble from the right, red marble from the left (75% * 75% of the time)\n\n2) green marble from the left, red marble from the right (25% * 25% of the time)\n\n3) green marble from the right, red marble from the right (75% * 25% of the time)\n\n4) green marble from the left, red marble from the left (25% * 75% of the time)\n\nWe draw green \"marble\" to the right of a red \"marble\" for all 1), half of 3) and half of 4).\n\nThis gives together\n\n$$\nAUC_\\mathrm{max} = \n\\frac{3}{4}\\times\\frac{3}{4} \n+ \\frac{1}{2}\\times\\frac{3}{4}\\times\\frac{1}{4} \n+ \\frac{1}{2}\\times\\frac{1}{4}\\times\\frac{3}{4} \n= \n\\frac{3}{4}\n$$\n\nThis number seems to be in agreement with the public leaderboard, where noone has been able to push through this barrier. Some people might get CV over 0.75, but if our hypothesis is correct, this is just overfitting.","3189d810":"# Import libraries and data","8c01589c":"# What does it mean for maximal AUC","6d1b9ffe":"# Fit logistic regression and check its quality","137e57b4":"Quality of the model","73ecc135":"Zoom in on the edges"}}