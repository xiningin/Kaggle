{"cell_type":{"8098a8f4":"code","05867ee5":"code","ae074e91":"code","a80ae024":"code","40f89047":"code","bf2b4cc1":"code","60799a38":"code","3a7a2f70":"code","722d59e9":"code","684d90d6":"code","9021d80f":"code","95d8609f":"code","95a0bc32":"code","4441dc33":"code","0b0ab370":"code","e158aa46":"code","89b84829":"code","c2ecda73":"code","fbeedad0":"code","9bf756d4":"code","7b63c9b7":"code","4d191d34":"code","245c6aa4":"code","65181fb1":"code","c1a4aa61":"code","df05c0d2":"code","bcf112a0":"code","c9dc3b64":"code","17e06f7e":"code","4a6e6147":"code","9d8ad818":"code","f01d5b9b":"code","4bffe8ee":"code","5d2114d5":"markdown","6cb67499":"markdown","b6f106bf":"markdown","6580b170":"markdown","6bb693ed":"markdown","dc93de28":"markdown","ce3bd4ba":"markdown","2af14a09":"markdown","18452409":"markdown","edc8f9ac":"markdown","970dcd3c":"markdown","3e513206":"markdown","63a96995":"markdown","99ad516a":"markdown","5eda0315":"markdown","bfb62d76":"markdown","73f40a90":"markdown","2eb05489":"markdown","6417946f":"markdown","19dbb0e3":"markdown","0c0cd7e6":"markdown"},"source":{"8098a8f4":"# load packages and data\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport pickle\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk import pos_tag\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nimport lightgbm as lgb\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, precision_recall_curve, auc\nimport nltk\nnltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger', 'stopwords'])\n%matplotlib inline\nsns.set_style('darkgrid')","05867ee5":"df = pd.read_csv('..\/input\/animal-crossing\/user_reviews.csv')","ae074e91":"df.head()","a80ae024":"# check null values\ndf.info()","40f89047":"# check the distribution of grade\nplt.figure(figsize=(8,6))\nplt.bar(df.grade.value_counts().index, df.grade.value_counts().values)\nplt.xlabel('Review Grade')\nplt.ylabel('Count')\nplt.title('Distribution of Review Grade');","bf2b4cc1":"df['target'] = pd.cut(df.grade, 2, labels=[0, 1])\ndf.target.value_counts()","60799a38":"def tokenize(text):\n    \"\"\"Tokenize each review text\n    Args: text\n    Return: token lists after normalization and lemmatization\n    \"\"\"\n    # remove punctuation and change to lowercase\n    text = re.sub(r'[^a-zA-Z0-9]', ' ', text).lower()\n    # tokenize the word into words\n    tokens = word_tokenize(text)\n\n    # remove stopwords\n    stop_words  = set(stopwords.words('english'))\n    \n    tokens = [word for word in tokens if word not in stop_words]\n\n    # lemmatize the word\n    lemmatizer = WordNetLemmatizer()\n    clean_token = []\n    for token in tokens:\n        clean_token.append(lemmatizer.lemmatize(token, pos='v').lower().strip())\n    return clean_token","3a7a2f70":"# concatenate all comments in low grade group \ndict_low = df[df.target == 0].to_dict(orient='list')\nlow_grade = dict_low['text']\n\ndict_high = df[df.target == 1].to_dict(orient='list')\nhigh_grade = dict_high['text']","722d59e9":"# tokenize text for Counter\nlow_grade = ' '.join(low_grade)\nlow_tokens = tokenize(low_grade)\n\nhigh_grade = ' '.join(high_grade)\nhigh_tokens = tokenize(high_grade)","684d90d6":"# Count the most popular words \nlow_counter = Counter(low_tokens)\nlow_top20 = low_counter.most_common(20)\n\nhigh_counter = Counter(high_tokens)\nhigh_top20 = high_counter.most_common(20)","9021d80f":"plt.figure(figsize=(18, 6))\nplt.subplot(1, 2, 1)\nplt.barh(range(len(low_top20)), [val[1] for val in low_top20], align='center')\nplt.yticks(range(len(low_top20)), [val[0] for val in low_top20])\nplt.xlabel('Count')\nplt.ylabel('Top 20 Common Words')\nplt.title('Most 20 common words in low grading group')\n\nplt.subplot(1, 2, 2)\nplt.barh(range(len(high_top20)), [val[1] for val in high_top20], align='center')\nplt.yticks(range(len(high_top20)), [val[0] for val in high_top20])\nplt.xlabel('Count')\nplt.ylabel('Top 20 Common Words')\nplt.title('Most 20 common words in high grading group');","95d8609f":"# use pos_tag in NLP to select adjectives\nad_tokens_low = []\nfor word, tag in pos_tag(low_tokens):\n    if tag in ('JJ', 'JJR', 'JJS'):\n        ad_tokens_low.append(word)\n\nad_tokens_high = []\nfor word, tag in pos_tag(high_tokens):\n    if tag in ('JJ', 'JJR', 'JJS'):\n        ad_tokens_high.append(word)","95a0bc32":"# Count the most popular adjective\/adverb \nad_low_counter = Counter(ad_tokens_low)\nad_low_top20 = ad_low_counter.most_common(20)\n\nad_high_counter = Counter(ad_tokens_high)\nad_high_top20 = ad_high_counter.most_common(20)","4441dc33":"plt.figure(figsize=(18, 6))\nplt.subplot(1, 2, 1)\nplt.barh(range(len(ad_low_top20)), [val[1] for val in ad_low_top20], align='center')\nplt.yticks(range(len(ad_low_top20)), [val[0] for val in ad_low_top20])\nplt.xlabel('Count')\nplt.ylabel('Top 20 Common Adjective')\nplt.title('Most 20 common adjective in low grading group')\n\nplt.subplot(1, 2, 2)\nplt.barh(range(len(ad_high_top20)), [val[1] for val in ad_high_top20], align='center')\nplt.yticks(range(len(ad_high_top20)), [val[0] for val in ad_high_top20])\nplt.xlabel('Count')\nplt.ylabel('Top 20 Common adjective')\nplt.title('Most 20 common adjective in high grading group');","0b0ab370":"# setup stop words\nstop_words  = set(stopwords.words('english'))","e158aa46":"# update stop words\nstop_words.update(['this', 'game', 'the', 'play'])","89b84829":"# generate word cloud for low score group\nwordcloud = WordCloud(background_color='white', max_words=1000, contour_width=3,contour_color='firebrick', \n                      stopwords = stop_words)\n\nwordcloud.generate(re.sub(r'[^a-zA-Z0-9]', ' ', low_grade).lower())\nplt.figure(figsize=(12,8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","c2ecda73":"# generate word cloud for high score group\nwordcloud = WordCloud(background_color='white', max_words=1000, contour_width=3,contour_color='firebrick', \n                      stopwords = stop_words)\n\nwordcloud.generate(re.sub(r'[^a-zA-Z0-9]', ' ', high_grade).lower())\nplt.figure(figsize=(12,8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","fbeedad0":"class AdCount(BaseEstimator, TransformerMixin):\n    \"\"\" Custom transformer to count the number of adj and adv in text\n    Args: text\n    Return: Adjective and Adverb counts in the text\n    \"\"\"\n    def Ad_count(self, text):\n        text = re.sub(r'[^a-zA-Z0-9]', ' ', text).lower()\n        # tokenize the word into words\n        tokens = word_tokenize(text)\n        # remove stopwords\n        stop_words  = set(stopwords.words('english'))\n        tokens = [word for word in tokens if word not in stop_words]\n        count = 0\n        for word, tag in pos_tag(tokens):\n            if tag in ('RB', 'RBR', 'RBS', 'JJ', 'JJR', 'JJS'):\n                count+=1\n        return count\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        counts = pd.Series(X).apply(self.Ad_count)\n        return pd.DataFrame(counts)","9bf756d4":"def build_model():\n    \"\"\"\n    A Adaboost classifier machine learning pipeline for\n    natural language processing with tdidf, adcount, and gridsearch for optimization.\n    Args: X_train, y_train \n    Returns:\n        Fitted model\n    \"\"\"\n    pipeline = Pipeline([\n        ('features', FeatureUnion([\n            ('text_pipeline', Pipeline([\n                ('vect', CountVectorizer(tokenizer=tokenize)),\n                ('tfidf', TfidfTransformer())\n            ])),\n             ('ad_count', AdCount())\n             ])),\n        ('clf', lgb.LGBMClassifier(objective='binary', random_state=0))\n    ])\n    parameters = {\n        #'clf__n_estimators': [100],\n        'clf__learning_rate': [0.01, 1],\n        'clf__num_leaves': [31, 62]\n        #'clf__min_samples_split': [5]\n        #'clf__estimator__C': [1, 10],\n        #'clf__estimator__max_iter': [1000, 100000]\n    }\n\n    cv = GridSearchCV(pipeline, param_grid=parameters)\n    return cv","7b63c9b7":"def evaluate_model(cv, X_test, y_test):\n    \"\"\"Draw ROC curve for the model\n    Args:\n        Classification Model\n        X_test, y_test, Array-like\n    return: ROC curve and model pickles\n    \"\"\"\n    y_pred = cv.predict_proba(X_test)[:,1]\n    print('\\nBest Parameters:', cv.best_params_)\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n    roc_auc = auc(fpr, tpr) # compute area under the curve\n    plt.figure()\n    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % (roc_auc))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic')\n    plt.legend(loc=\"lower right\")","4d191d34":"# split train and test \nX = df.text\ny = df.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","245c6aa4":"# build model (randomforests)\nmodel = build_model()\nmodel.fit(X_train, y_train)","65181fb1":"# evaluate\npred = model.predict(X_test)\nprint('Accuracy_score is: {}'.format(accuracy_score(pred, y_test)))\nevaluate_model(model, X_test, y_test)","c1a4aa61":"f = open('rf'+'.pkl', 'wb')\npickle.dump(model, f)","df05c0d2":"# build model Adaboost\nmodel_ad = build_model()\nmodel_ad.fit(X_train, y_train)","bcf112a0":"# evaluate\npred_ad = model_ad.predict(X_test)\nprint('Accuracy_score is: {}'.format(accuracy_score(pred_ad, y_test)))\nevaluate_model(model_ad, X_test, y_test)","c9dc3b64":"f = open('ada'+'.pkl', 'wb')\npickle.dump(model, f)","17e06f7e":"# build model Adaboost\nmodel_lgb = build_model()\nmodel_lgb.fit(X_train, y_train)","4a6e6147":"# evaluate\npred_lgb = model_ad.predict(X_test)\nprint('Accuracy_score is: {}'.format(accuracy_score(pred_lgb, y_test)))\nprint('Best parameter is: {}'.format(model_lgb.best_params_))\nevaluate_model(model_lgb, X_test, y_test)","9d8ad818":"f = open('lgb'+'.pkl', 'wb')\npickle.dump(model_lgb, f)","f01d5b9b":"# fun test\ntest = [\"it is amazing. I'm totally adicted\"]\nprint(model.predict(test))\nprint(model_ad.predict(test))\nprint(model_lgb.predict(test))","4bffe8ee":"# fun test\nntest = [\"This game sucks, make me stressful\"]\nprint(model.predict(ntest))\nprint(model_ad.predict(ntest))\nprint(model_lgb.predict(ntest))","5d2114d5":"Again, the low grade group gave more description about the game, with some info may refer why they didn't like the game - per console? per switch? may indicate they want to play the game with multiple person. The high grade group have more positive words in general, like great, new, the best, good etc.","6cb67499":"### Word Counter","b6f106bf":"### RandomForester","6580b170":"We can see that the grade for animal crossing from users are bimodal. Many users gave fairly low score (0, 1) and fairly high score (9, 10). Relatively fewer users gave average score like 5, 6.\n\nSince the review scores are bimodally disgtributed, I will group users with 0-4 as lower (0), and users with 5-10 as high (1) for later analysis. ","6bb693ed":"There is a lot overlapping for the most common words. Some infomation we can extract is that in the high score group, users put **fun**, word **great**, **really** and **10** more often in their reviews.\n\nOne thing maybe interesting to see is to look at the most common adjective to see how people decribe their exprience. ","dc93de28":"Overall all models perform fairly well. Lgb is slighly faster than randomforest.","ce3bd4ba":"## Popular Words","2af14a09":"The user gave bad grade to the game generally think it's **bad**, **terrible**, **ridiculous**, while the people like the game think it's **amazing**, **fantastic**, **great**, **perfect**. Also, we can see that in generally people feel bad about the game describe it more objectively(?).","18452409":"### Adaboost","edc8f9ac":"Animal Crossing New Horizon is one of the most 'hit' game these days, but the comments for this game seems varies. Here we analyze some of the reviews and grades put by users. There are some interesting pattern: 1. people don't like this game seems put more objective comments. 2. The wordcloud may indicate that people don't like this game is because limited consoles\/villager for one switch. We also tried few techniques to build a model to predict the grade given a comment in the future. We can improve the model by tuning parameters more, or updating the stopwords (for instance nintendo, game, play etc are pretty repetitive. Also, make more custom features would be interesting. ","970dcd3c":"![](https:\/\/i.ibb.co\/qmb9wxs\/100785669-1595363370613145-5432053625553682432-o.jpg)","3e513206":"Animal Crossing is a social simulation video game developed and published by Nintendo. In Animal Crossing, the player character is a human who lives in a village inhabited by various anthropomorphic animals. The series is notable for its open-ended gameplay and extensive use of the video game console's internal clock and calendar to simulate real passage of time.\n\nIn the game, the player can carry out activities such as fishing, bug catching, mining etc. The game is much of a real-life simulator, players can build house, design clothes and even sell\/buy turnip (mock stock market) to make money. It is undoubly one of the most popular game these days. Some people find it very obsessive, but others find it stressful and time-consuming (like me, lol). Here, we aim to analyze the reviews of the users, and build a NLP pipeline to understand what is the key reasons people like\/dislike animal crossing.  \n\nIf you find this notebook interesting, please UPVOTE!","63a96995":"![](https:\/\/image-cdn.essentiallysports.com\/wp-content\/uploads\/20200508154130\/Animal-Crossing-New-Horizons.jpeg)","99ad516a":"![](http:\/\/https:\/\/scontent-lax3-1.xx.fbcdn.net\/v\/t1.0-9\/100785669_1595363370613145_5432053625553682432_o.jpg?_nc_cat=110&_nc_sid=dd7718&_nc_oc=AQmDREUGTSCH8rjCa6A6ZLs2f5LpmXItY8V65uqXpmQ3XfTJbw7Q6q81PAh-xG8jGJQ&_nc_ht=scontent-lax3-1.xx&oh=c4d7eb45d69ffccdf358fd427b40c4b0&oe=5EF376B5)","5eda0315":"## ETL","bfb62d76":"# Introduction","73f40a90":"## Conclusion","2eb05489":"Here is an image from my own village few weeks ago. LOL","6417946f":"## Model Build (NLP pipeline)","19dbb0e3":"### WordCloud","0c0cd7e6":"### Light LGB"}}