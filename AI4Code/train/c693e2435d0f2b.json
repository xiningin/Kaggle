{"cell_type":{"a4a3f062":"code","14121e0f":"code","f48da160":"code","17140d07":"code","956bbc24":"code","232ecd94":"code","236ab9a2":"code","8e734be8":"code","02206129":"code","e9d154b9":"code","f5bc3f5f":"markdown","65022055":"markdown","8b9e1582":"markdown","aaf5f21d":"markdown","cbe8f342":"markdown","b4e59f35":"markdown","0aae2183":"markdown","79be88cb":"markdown","1b34abc2":"markdown"},"source":{"a4a3f062":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split \nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale \nfrom sklearn import model_selection\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.utils import resample\nfrom sklearn.metrics import mean_squared_error","14121e0f":"X,y=make_regression(  n_samples=100,\n    n_features=10,\n    n_informative=5,\n    n_targets=1,\n    bias=0.0,\n    effective_rank=None,\n    tail_strength=0.5,\n    noise=0.0,\n    shuffle=True,\n    coef=False,\n    random_state=42)\nX=pd.DataFrame(X)\ny=pd.DataFrame(y)\ndf=pd.concat([X,y],axis=1)\ndf.columns=['f1','f2','f3','f4','f5','f6','f7','f8','f9','f10','T']\n","f48da160":"X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,test_size=0.25)","17140d07":"print(f'Sample size of train: {X_train.shape[0]}')","956bbc24":"def error_calculator(fitted_model,X_test=X_test,y_test=y_test):\n    pred=fitted_model.predict(X_test)\n    return (f'Root Mean Squared Error: {np.round(np.sqrt(mean_squared_error(y_test,pred)),2)}')","232ecd94":"bagged_trees=BaggingRegressor(base_estimator=DecisionTreeRegressor(),\n                              max_samples=0.5,\n                              n_estimators=5,\n                              random_state=42,\n                              oob_score=True) # Test the model performance with oob samples\nbagged_trees.fit(X_train,y_train)                            \nerror_calculator(bagged_trees)","236ab9a2":"for i in range (bagged_trees.n_estimators):\n    print(f'Estimator Number {i+1}: {bagged_trees.estimators_[i]}')\n    bagged_trees.estimators_[i].fit(X_train,y_train)\n    print(f'Individual {error_calculator(bagged_trees.estimators_[i])}')","8e734be8":"for i in range (bagged_trees.n_estimators):\n    print(f'Samples used in estimator {i+1}: {bagged_trees.estimators_samples_[i]}')\n    print(f'Total number of samples used for estimator {i+1}: {len(bagged_trees.estimators_samples_[i])}')\n    print()\n    print('*'*20)\n    print()","02206129":"def bagged_tree_regressor(df):\n    boot1=np.array(resample(df,replace=True,n_samples=int(len(df)\/2)))\n    out_of_samples1= []\n    for s in df.values:\n        if s not in boot1:\n            out_of_samples1.append(s)\n    out_of_samples1=np.array(out_of_samples1)\n    dt1=DecisionTreeRegressor()\n    X_train1=boot1[:,:10]\n    y_train1=boot1[:,-1]\n    X_test1=out_of_samples1[:,:10]\n    y_test1=out_of_samples1[:,-1]\n    dt1.fit(X_train1,y_train1)\n    \n    boot2=np.array(resample(df,replace=True,n_samples=int(len(df)\/2)))\n    out_of_samples2= []\n    for s in df.values:\n        if s not in boot2:\n            out_of_samples2.append(s)\n    out_of_samples2=np.array(out_of_samples2)\n    dt2=DecisionTreeRegressor()\n    X_train2=boot2[:,:10]\n    y_train2=boot2[:,-1]\n    X_test2=out_of_samples2[:,:10]\n    y_test2=out_of_samples2[:,-1]\n    dt2.fit(X_train2,y_train2)\n\n    boot3=np.array(resample(df,replace=True,n_samples=int(len(df)\/2)))\n    out_of_samples3= []\n    for s in df.values:\n        if s not in boot3:\n            out_of_samples3.append(s)\n    out_of_samples3=np.array(out_of_samples3)\n    dt3=DecisionTreeRegressor()\n    X_train3=boot3[:,:10]\n    y_train3=boot3[:,-1]\n    X_test3=out_of_samples3[:,:10]\n    y_test3=out_of_samples3[:,-1]\n    dt3.fit(X_train3,y_train3)\n    \n    boot4=np.array(resample(df,replace=True,n_samples=int(len(df)\/2)))\n    out_of_samples4= []\n    for s in df.values:\n        if s not in boot4:\n            out_of_samples4.append(s)\n    out_of_samples4=np.array(out_of_samples4)\n    dt4=DecisionTreeRegressor()\n    X_train4=boot4[:,:10]\n    y_train4=boot4[:,-1]\n    X_test4=out_of_samples4[:,:10]\n    y_test4=out_of_samples4[:,-1]\n    dt4.fit(X_train4,y_train4)\n    \n    boot5=np.array(resample(df,replace=True,n_samples=int(len(df)\/2)))\n    out_of_samples5= []\n    for s in df.values:\n        if s not in boot1:\n            out_of_samples5.append(s)\n    out_of_samples5=np.array(out_of_samples5)\n    dt5=DecisionTreeRegressor()\n    X_train5=boot5[:,:10]\n    y_train5=boot5[:,-1]\n    X_test5=out_of_samples5[:,:10]\n    y_test5=out_of_samples5[:,-1]\n    dt5.fit(X_train5,y_train5)\n    error1=error_calculator(dt1,X_test1,y_test1)\n    error2=error_calculator(dt2,X_test2,y_test2)\n    error3=error_calculator(dt3,X_test3,y_test3)\n    error4=error_calculator(dt4,X_test4,y_test4)\n    error5=error_calculator(dt5,X_test5,y_test5)\n    return error1,error2, error3,error4,error5","e9d154b9":"bagged_tree_regressor(df)","f5bc3f5f":"# Usege of the Bagged Tree Model","65022055":"# Building the Model from Ground 0","8b9e1582":"# Relevant Definitionds About the Topic\n**Ensemble Learning:** In ensemble learning, we try to boost the accuracy by using more than one base machine learning model rather than optimizing hyperparameters for one particular machine learning model. Most of the time the base model is Decision Trees.\n\n**Bagged Tree Model:** Bagged Tree model is a type of ensemble learning. In this model, we try to decrease the variance so overfitting would not be a problem for us (\"Bias-Variance Tradeoff\"). \n\nWe split the data via the \"bootstrap method\" and give data samples to each tree for training. Then we collect the outputs of these trees and combine these outputs to reach a model output.\n\n**Bootstrap Method:** In the bootstrap method, we randomly select samples for training. But after each selection, we put this sample in the real dataset again. So, during the resampling process, it is possible to pick a sample more than one time. It is also possible to not pick a sample at all. Another thing to consider is the sample size of the bootstrap subgroups. Each bootstrap subgroup can contain equal or less than the number of samples found in the original dataset. We will arrange this with \"max_samples\" parameter.\n\n**Out-of-bag Samples:** Samples that are not picked during bootstrap process is called \"Out-of-bag Samples\". They are used to test the skill of the base estimators.","aaf5f21d":"Let's see the performance of each estimator.","cbe8f342":"# Creating the Dataset","b4e59f35":"![image.png](attachment:80e26ebc-bd19-41a7-9434-ed7d38484244.png)","0aae2183":"Thanks...","79be88cb":"Let's see the samples that are used by each estimator and see whether the size of it equals approximately the half of the 75 (Since max_samples=0.5)","1b34abc2":"In this notebook, we will learn,\n1. What ensemble learning is\n2. What is the logic behind Bagged Tree Model\n3. How to use Bagged Tree Model\n4. How to make our own Bagged Tree Model from scratch\n\nHope you'll find something helpful!"}}