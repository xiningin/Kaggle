{"cell_type":{"50c2eab9":"code","dd946dbb":"code","b07c2a56":"code","17e58bfb":"code","c860a4d8":"code","d183ff03":"code","6655646f":"code","fa6eb817":"code","fe488a56":"code","89d53c4c":"code","743afb3e":"code","55bf1880":"code","608f01fa":"code","583b60c5":"code","af5f7e09":"code","7bf971fd":"code","874410e7":"code","95dd3695":"code","eeb31d35":"code","427b7b74":"code","6c45d2d5":"code","82f608f9":"code","b95e5fb8":"code","275c648c":"code","5f205839":"code","60011a53":"code","11bff3f1":"code","6f78220a":"code","fa1a7805":"code","d2d19048":"code","9086568d":"code","4dda8de9":"code","48775057":"code","851cffba":"code","112bc50b":"code","a865d126":"code","48aab2d9":"code","891893a8":"code","be3af923":"code","db832836":"code","d1a1701d":"code","84954189":"markdown","df9196e6":"markdown","1c706b75":"markdown","27caf5d5":"markdown","c2c28c4b":"markdown","b715df8f":"markdown","a40a46c5":"markdown","26a61c86":"markdown","84cf42f5":"markdown","b3f2116a":"markdown","94103de1":"markdown","3f91c944":"markdown","57591085":"markdown","b5c49a05":"markdown","6f92666e":"markdown","4e6c8722":"markdown","0fd26784":"markdown","5b0f3d33":"markdown","8bd3edea":"markdown","77f1d65c":"markdown","ace7e310":"markdown","986e4aea":"markdown","cacaa01c":"markdown","9c74bc82":"markdown","a45fe902":"markdown","6b1df447":"markdown","4e27152a":"markdown","6a780996":"markdown","49480684":"markdown","723a3419":"markdown","02eb0f3b":"markdown","d54e958f":"markdown","507e30dc":"markdown","b82078c5":"markdown","23baafca":"markdown","900bc92c":"markdown","2626cace":"markdown","b4aaf16c":"markdown","c39efb80":"markdown","8c5ba883":"markdown","f9bd9ac3":"markdown","375f7107":"markdown","93e9500c":"markdown","773d0d40":"markdown"},"source":{"50c2eab9":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nprint(os.listdir(\"..\/input\"))\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n%matplotlib inline","dd946dbb":"# Read files\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n# From EDA obvious outliers\ntrain = train[train.GrLivArea < 4500]\ntrain.reset_index(drop=True, inplace=True)\n\noutliers = [30, 88, 462, 631, 1322]\ntrain = train.drop(train.index[outliers])\n\n\nprint (train.columns)\nprint(test.columns)\nprint(train.shape,test.shape)\n","b07c2a56":"train.describe()","17e58bfb":"train.head(7)","c860a4d8":"train['SalePrice'].describe()\nsns.distplot(train['SalePrice']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","d183ff03":"from scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n# Plot histogram and probability\nfig = plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(train['SalePrice'] , fit=norm);\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nplt.subplot(1,2,2)\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.suptitle('Before transformation')\n\n# Apply transformation\ntrain.SalePrice = np.log1p(train.SalePrice )\n# New prediction\ny_train = train.SalePrice.values\ny_train_orig = train.SalePrice\n\n\n# Plot histogram and probability after transformation\nfig = plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(train['SalePrice'] , fit=norm);\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nplt.subplot(1,2,2)\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.suptitle('After transformation')","6655646f":"# y_train_orig = train.SalePrice\n# train.drop(\"SalePrice\", axis = 1, inplace = True)\ndata_features = pd.concat((train, test)).reset_index(drop=True)\nprint(data_features.shape)\n\n# print(train.SalePrice)","fa6eb817":"# Missing data in train\ndata_features_na = data_features.isnull().sum()\ndata_features_na = data_features_na[data_features_na>0]\ndata_features_na.sort_values(ascending=False)","fe488a56":"#missing data percent plot\ntotal = data_features.isnull().sum().sort_values(ascending=False)\npercent = (data_features.isnull().sum()\/data_features.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","89d53c4c":"str_vars = ['MSSubClass','YrSold','MoSold']\nfor var in str_vars:\n    data_features[var] = data_features[var].apply(str)\n","743afb3e":"# Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\n\ncommon_vars = ['Exterior1st','Exterior2nd','SaleType','Electrical','KitchenQual']\nfor var in common_vars:\n    data_features[var] = data_features[var].fillna(data_features[var].mode()[0])\n    \n# 'RL' is by far the most common value. So we can fill in missing values with 'RL'\ndata_features['MSZoning'] = data_features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))","55bf1880":"# # data description says NA means \"No Pool\", majority of houses have no Pool at all in general.\n# features[] = features[\"PoolQC\"].fillna(\"None\")\n# Replacing missing data with None\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond','BsmtQual',\n            'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\"PoolQC\"\n           ,'Alley','Fence','MiscFeature','FireplaceQu','MasVnrType','Utilities']:\n    data_features[col] = data_features[col].fillna('None')\n# # For all these categorical basement-related features, NaN means that there is no basement\n# for col in (:\n#     features[col] = features[col].fillna('None')","608f01fa":"# Replacing missing data with 0 (Since No garage = no cars in such garage.)\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars','MasVnrArea','BsmtFinSF1','BsmtFinSF2'\n           ,'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BsmtUnfSF','TotalBsmtSF'):\n    data_features[col] = data_features[col].fillna(0)\n\n# group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\ndata_features['LotFrontage'] = data_features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\nprint('Features size:', data_features.shape)","583b60c5":"# data description says NA means typical\ndata_features['Functional'] = data_features['Functional'].fillna('Typ')\n","af5f7e09":"#missing data\ntotal = data_features.isnull().sum().sort_values(ascending=False)\npercent = (data_features.isnull().sum()\/data_features.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(10)\n\n","7bf971fd":"# Differentiate numerical features (minus the target) and categorical features\ncategorical_features = data_features.select_dtypes(include=['object']).columns\nprint(categorical_features)\nnumerical_features = data_features.select_dtypes(exclude = [\"object\"]).columns\nprint(numerical_features)\n\nprint(\"Numerical features : \" + str(len(numerical_features)))\nprint(\"Categorical features : \" + str(len(categorical_features)))\nfeat_num = data_features[numerical_features]\nfeat_cat = data_features[categorical_features]","874410e7":"feat_num.head(10)\n","95dd3695":"feat_cat.head(10)","eeb31d35":"# Plot skew value for each numerical value\nfrom scipy.stats import skew \nskewness = feat_num.apply(lambda x: skew(x))\nskewness.sort_values(ascending=False)","427b7b74":"skewness = skewness[abs(skewness) > 0.5]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\nprint(\"Mean skewnees: {}\".format(np.mean(skewness)))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    feat_num[feat] = boxcox1p(feat_num[feat], boxcox_normmax(feat_num[feat] + 1))\n    data_features[feat] = boxcox1p(data_features[feat], boxcox_normmax(data_features[feat] + 1))\n    \n    \nfrom scipy.stats import skew \nskewness.sort_values(ascending=False)","6c45d2d5":"skewness = feat_num.apply(lambda x: skew(x))\nskewness = skewness[abs(skewness) > 0.5]\n\nprint(\"There are {} skewed numerical features after Box Cox transform\".format(skewness.shape[0]))\nprint(\"Mean skewnees: {}\".format(np.mean(skewness)))\nskewness.sort_values(ascending=False)\n","82f608f9":"# Calculating totals before droping less significant columns\n\n#  Adding total sqfootage feature \ndata_features['TotalSF']=data_features['TotalBsmtSF'] + data_features['1stFlrSF'] + data_features['2ndFlrSF']\n#  Adding total bathrooms feature\ndata_features['Total_Bathrooms'] = (data_features['FullBath'] + (0.5 * data_features['HalfBath']) +\n                               data_features['BsmtFullBath'] + (0.5 * data_features['BsmtHalfBath']))\n#  Adding total porch sqfootage feature\ndata_features['Total_porch_sf'] = (data_features['OpenPorchSF'] + data_features['3SsnPorch'] +\n                              data_features['EnclosedPorch'] + data_features['ScreenPorch'] +\n                              data_features['WoodDeckSF'])\n\n\n# data_features['Super_quality'] = OverallQual * \n# vars = ['OverallQual', 'GrLivArea', 'TotalBsmtSF', 'FullBath']\n","b95e5fb8":"data_features['haspool'] = data_features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndata_features['hasgarage'] = data_features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ndata_features['hasbsmt'] = data_features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndata_features['hasfireplace'] = data_features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\n\n# Not normaly distributed can not be normalised and has no central tendecy\ndata_features = data_features.drop(['MasVnrArea', 'OpenPorchSF', 'WoodDeckSF', 'BsmtFinSF1','2ndFlrSF'], axis=1)\n# data_features = data_features.drop(['MasVnrArea', 'OpenPorchSF', 'WoodDeckSF', 'BsmtFinSF1','2ndFlrSF',\n#                          'PoolArea','3SsnPorch','LowQualFinSF','MiscVal','BsmtHalfBath','ScreenPorch',\n#                          'ScreenPorch','KitchenAbvGr','BsmtFinSF2','EnclosedPorch','LotFrontage'\n#                          ,'BsmtUnfSF','GarageYrBlt'], axis=1)\n\nprint('data_features size:', data_features.shape)\n","275c648c":"train = data_features.iloc[:len(y_train), :]\ntest = data_features.iloc[len(y_train):, :]\nprint(['Train data shpe: ',train.shape,'Prediction on (Sales price) shape: ', y_train.shape,'Test shape: ', test.shape])","5f205839":"vars = data_features.columns\n# vars = numerical_features\nfigures_per_time = 4\ncount = 0 \ny = y_train\nfor var in vars:\n    x = train[var]\n#     print(y.shape,x.shape)\n    plt.figure(count\/\/figures_per_time,figsize=(25,5))\n    plt.subplot(1,figures_per_time,np.mod(count,4)+1)\n    plt.scatter(x, y);\n    plt.title('f model: T= {}'.format(var))\n    count+=1\n    \n","60011a53":"# Removes outliers \n# outliers = [30, 88, 462, 631, 1322]\n# train = train.drop(train.index[outliers])\ny_train = train['SalePrice']","11bff3f1":"\n# vars_box = ['OverallQual','YearBuilt','BedroomAbvGr']\nvars_box = feat_cat\nfor var in vars_box:\n    data = pd.concat([train['SalePrice'], train[var]], axis=1)\n    f, ax = plt.subplots(figsize=(8, 6))\n    fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)","6f78220a":"# Complete numerical correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=1, square=True);","fa1a7805":"# saleprice correlation matrix\ncorr_num = 15 #number of variables for heatmap\ncols_corr = corrmat.nlargest(corr_num, 'SalePrice')['SalePrice'].index\ncorr_mat_sales = np.corrcoef(train[cols_corr].values.T)\nsns.set(font_scale=1.25)\nf, ax = plt.subplots(figsize=(12, 9))\nhm = sns.heatmap(corr_mat_sales, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 7}, yticklabels=cols_corr.values, xticklabels=cols_corr.values)\nplt.show()","d2d19048":"# pair plots for variables with largest correlation\nvar_num = 8\nvars = cols_corr[0:var_num]\n\nsns.set()\nsns.pairplot(train[vars], size = 2.5)\nplt.show();\n","9086568d":"data_features = data_features.drop(\"SalePrice\", axis = 1)\nfinal_features = pd.get_dummies(data_features)\n\nprint(final_features.shape)\nX = final_features.iloc[:len(y), :]\nX_test = final_features.iloc[len(y):, :]\nX.shape, y_train.shape, X_test.shape\n\n\nprint(X.shape,y_train.shape,X_test.shape)\n","4dda8de9":"# Removes colums where the threshold of zero's is (> 99.95), means has only zero values \noverfit = []\nfor i in X.columns:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(X) * 100 > 99.95:\n        overfit.append(i)\n\noverfit = list(overfit)\noverfit.append('MSZoning_C (all)')\n\nX = X.drop(overfit, axis=1).copy()\nX_test = X_test.drop(overfit, axis=1).copy()\n\nprint(X.shape,y_train.shape,X_test.shape)\n","48775057":"from datetime import datetime\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error , make_scorer\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.linear_model import LinearRegression\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n","851cffba":"kfolds = KFold(n_splits=18, shuffle=True, random_state=42)\n\n# model scoring and validation function\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y,scoring=\"neg_mean_squared_error\",cv=kfolds))\n    return (rmse)\n\n# rmsle scoring function\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","112bc50b":"lightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4, #was 3\n                                       learning_rate=0.01, \n                                       n_estimators=9000, #8000\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2, # 'was 0.2'\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       )\n\n# xgboost = XGBRegressor(learning_rate=0.01,n_estimators=3460,\n#                                      max_depth=3, min_child_weight=0,\n#                                      gamma=0, subsample=0.7,\n#                                      colsample_bytree=0.7,\n#                                      objective='reg:linear', nthread=-1,\n#                                      scale_pos_weight=1, seed=27,\n#                                      reg_alpha=0.00006)\n\n\n\n# setup models hyperparameters using a pipline\n# The purpose of the pipeline is to assemble several steps that can be cross-validated together, while setting different parameters.\n# This is a range of values that the model considers each time in runs a CV\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n\n\n\n\n# Kernel Ridge Regression : made robust to outliers\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\n\n# LASSO Regression : made robust to outliers\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, \n                    alphas=alphas2,random_state=42, cv=kfolds))\n\n# Elastic Net Regression : made robust to outliers\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, \n                         alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))\n\n\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, lightgbm),\n                                meta_regressor=elasticnet,\n                                use_features_in_secondary=True)\n\n\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n\n# store models, scores and prediction values \nmodels = {'Ridge': ridge,\n          'Lasso': lasso, \n          'ElasticNet': elasticnet,\n          'lightgbm': lightgbm,\n          'Svd': svr}\n#           'xgboost': xgboost}\npredictions = {}\nscores = {}","a865d126":"for name, model in models.items():\n    \n    model.fit(X, y)\n    predictions[name] = np.expm1(model.predict(X))\n    \n    score = cv_rmse(model, X=X)\n    scores[name] = (score.mean(), score.std())","48aab2d9":"# get the performance of each model on training data(validation set)\nprint('---- Score with CV_RMSLE-----')\nscore = cv_rmse(ridge)\nprint(\"Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(lasso)\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(elasticnet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(lightgbm)\nprint(\"lightgbm score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(svr)\nprint(\"SVR: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n# scores['svr'] = (score.mean(), score.std())\n\n# score = cv_rmse(xgboost)\n# print(\"xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\n\n#Fit the training data X, y\nprint('----START Fit----',datetime.now())\nprint('Elasticnet')\nelastic_model = elasticnet.fit(X, y)\nprint('Lasso')\nlasso_model = lasso.fit(X, y)\nprint('Ridge')\nridge_model = ridge.fit(X, y)\nprint('lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)\nprint('Svr')\nsvr_model_full_data = svr.fit(X, y)\n\n# print('xgboost')\n# xgb_model_full_data = xgboost.fit(X, y)\n\n\nprint('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(X), np.array(y))\n","891893a8":"def blend_models_predict(X):\n    return ((0.16  * elastic_model.predict(X)) + \\\n            (0.16 * lasso_model.predict(X)) + \\\n            (0.11 * ridge_model.predict(X)) + \\\n            (0.2 * lgb_model_full_data.predict(X)) + \\\n            (0.1 * svr_model_full_data.predict(X)) + \\\n#             (0.1 * xgb_model_full_data.predict(X)) + \\\n            (0.27 * stack_gen_model.predict(np.array(X))))","be3af923":"print('RMSLE score on train data:')\nprint(rmsle(y, blend_models_predict(X)))","db832836":"print('Predict submission')\nsubmission = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmission.iloc[:,1] = (np.expm1(blend_models_predict(X_test)))","d1a1701d":"# q1 = submission['SalePrice'].quantile(0.0042)\n# q2 = submission['SalePrice'].quantile(0.99)\n# # Quantiles helping us get some extreme values for extremely low or high values \n# submission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\n# submission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\nsubmission.to_csv(\"submission.csv\", index=False)","84954189":"#### String Values\nFor numbers that have no significance and should actually be strings","df9196e6":"### 9.4 Training the models","1c706b75":"## 3. The predicted variable - Sales price Skew & kurtosis analysis\nThe predicted variable is probably the most important variable, therefore it should be inspected throughly. \n<br> It turns out models work better with symmetric gaussian distributions, therefore we want to get rid of the skewness by using log transformation. More on log transformation later\n<br> <br> Skew: \n\\begin{equation} \nskew \\left( X  \\right) = E[ \\frac{X-\\mu}{\\sigma} ]^3\n\\end{equation} https:\/\/en.wikipedia.org\/wiki\/Skewness\n![](https:\/\/www.managedfuturesinvesting.com\/images\/default-source\/default-album\/measure-of-skewness.jpg?sfvrsn=0)\n \n<br> Kurtosis: $$kurtosis(X) = E[ (\\frac{X-\\mu}{\\sigma})^4  ]$$\nhttps:\/\/en.wikipedia.org\/wiki\/Kurtosis\n![](https:\/\/siemensplm.i.lithium.com\/t5\/image\/serverpage\/image-id\/38460iB0F0D63C4F9B568A\/image-size\/large?v=1.0&px=999)","27caf5d5":"### 2.1 Importing data\nImporting, dropping ID and some outliers based on future EDA","c2c28c4b":"Fill numerical data - 0 or median","b715df8f":"## 2. Import And quiring data","a40a46c5":"### 3.1 Observing Sale price histogram\n","26a61c86":"### 9.1 Importing learning libraries","84cf42f5":"## 5. Numerical and Categorial features","b3f2116a":"### 4.2 Replacing the missing data\n\nCorrecting for the format, mostly filling NaN with \"No\" or \"0\"","94103de1":"### Optional: Box plot\n\nBox plot is heavy, one can manualy choose the intresting parameters","3f91c944":"### 2.2 Quiring the data\nJust watching what's out there","57591085":"### 6.2 Deleting features\nFeatures that cant be skewd or are unsignificant.","b5c49a05":"remove outliers","6f92666e":"### 7.2 Comparing data to sale price through correlation matrix","4e6c8722":"## 8. Preparing the data \nDropping Sale price, Creating dummy variable for the categorial variables and matching dimentions between train and test","0fd26784":"### 4.1 Locating missing data\n","5b0f3d33":"Uniqe","8bd3edea":"## 7.Plotting the data","77f1d65c":"### 7.1 Visually comparing data to sale prices\nOne can observe the behaviour of the variables, locate outlier and more.","ace7e310":"Most common (frequent) string transform","986e4aea":"### 3.2 Tansforming: \\begin{equation*} Y = log(1 + X)) \\end{equation*}\nShould correct for skew.\n<br> A random example of a different log transformation\n![](http:\/\/www.biostathandbook.com\/pix\/transformfig1.gif)","cacaa01c":"Concatenate train and test","9c74bc82":"Turn Nan to None ","a45fe902":"## 9. Creating the model","6b1df447":"### 5.9 Splitting the data back to train and test\n","4e27152a":"### 7.3 Pairplot for the most intresting parameters","6a780996":"### 9.5 Validating and training each model","49480684":"## 4. Missing data","723a3419":"### 5.1 Splitting the data into categorial and numerical features","02eb0f3b":"### 5.2 Box cox transform for skewd numerical data\nAnother transformation to reduce skew. \n<br> Equation:\n![](https:\/\/www.statisticshowto.datasciencecentral.com\/wp-content\/uploads\/2015\/07\/boxcox-formula-1.png)\n<br> Transformation example:\n![](https:\/\/www.itl.nist.gov\/div898\/handbook\/eda\/section3\/gif\/boxcox.gif)","d54e958f":"Removing overfit","507e30dc":"Observe the correction. \nWe can see that a lot of parameters remained skewd. I suspect that's for variables that have a lot of 0. ","b82078c5":"## Introduction\n\nHi\n\nThanks to all who upvoted for the gold medal! I really appriciate it.\n_____________________\nThis is my first attempt at DL\/ML of something other than images. Therefore, I wanted to start with well explored dataset that has a lot of good kernels, relatively small amount of data and this competiotion seems to suit it. This kernel is mostly a blend of top\/highly voted kernels, my attemp at learning from them and hopefully implementing them in future competitions. This work is still in progress and I will use it as learning platform to DL\/ML techniques\n\n<br> Upvotes, notes and remarks are very appriciated!\n<brr>\nSpecial thanks to this kernel: \n    <br> https:\/\/www.kaggle.com\/alfredmaboa\/advanced-regression-techniques-regularization\/data\n\n      \nAlso learneda lot and used those kernels: \n<br>\nSource1 : https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n<br>\nSource2 : https:\/\/www.kaggle.com\/bsivavenu\/house-price-calculation-methods-for-beginners\n<br> \nSource3 : https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n<br>\nModel source1 : https:\/\/www.kaggle.com\/niteshx2\/top-50-beginners-stacking-lgb-xgb\n\n\n### Table of interest:\n> ### 1. Importing libraries\n> ### 2. Importing and inquiring data\n> > #### 2.1 Importing data\n> > #### 2.2 Quiring data\n> ### 3. The predicted variable - Sales price Skew & kurtosis analysis\n> > #### 3.1 Observing histogram\n> > #### 3.2 Tansforming log or box cox: \n> ### 4. Missing data\n> > #### 4.1 Presenting and locating missing data\n> > #### 4.2 Replacing the missing data\n> > Examples: Replacing NaN with \"No\", \"0\" most common value and such\n> ### 5. Numerical and Categorial features\n> > #### 5.1 Splitting the data into categorial and numerical features\n> > #### 5.2 Box cox transform for skewd numerical data\n>  ### 6. Adding Features\n> > #### 6.1 Creating features from the data\n> > Adding features at this section to be able to view them at the visualization section next \n> > #### 6.2 Deleting features\n> > Features that cant be skewd or unsignificant.\n> ### 7.Plotting the data\n> > #### 7.1 Visually comparing data to sale prices\n> > Boxplot for categorial features and 2D plot for numerical\n> > #### 7.2 Comparing data to sale price through correlation matrix\n> > #### 7.3 Pairplot for the most intresting parameters\n> ### 8. Preparing the data \n> ### 9. Creating the model\n> > #### 9.1 Importing learning libraries\n> > #### 9.2 Defining folds and score functions\n> > #### 9.3 Defining models\n> > #### 9.4 Training the models\n> > #### 9.5 Validating each model\n> > #### 9.6 Blend model prediction\n> ### 10 Submission\n","23baafca":"## 6. Adding features\n","900bc92c":"### 9.3 Defining models","2626cace":"## 1. Importing libraries","b4aaf16c":"### 9.2 Defining folds and score functions","c39efb80":"## 10 Submission","8c5ba883":"Numerical values correlation matrix, to locate dependencies between different variables. ","f9bd9ac3":"#### Largest correlation with Sale Price\nIts important to remmber that this are 2D correlations, between sale price and another variable. When stacking all of the parameters the dependencies the picture gets more complex.","375f7107":"Encode categorial features: can and should be replaced.","93e9500c":"### 6.1 Creating features from the data\nAdding features at this section to be able to view them at the visualization section next \n","773d0d40":"### 9.6 Blend model prediction"}}