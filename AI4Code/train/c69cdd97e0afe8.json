{"cell_type":{"8705da7e":"code","1cc24ee9":"code","2550b91b":"code","7a5c9bbb":"code","b6289b3a":"code","cf0e4568":"code","c4d5d862":"code","cfdf4cff":"code","08e8207c":"code","1619f248":"code","d6c7e7d4":"code","0541dc97":"code","ee8f9a10":"code","11d3102a":"code","188165ce":"markdown","8eaead1e":"markdown"},"source":{"8705da7e":"## Import data analysis tools \nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV","1cc24ee9":"#Read the training dataset\ndf = pd.read_csv(\"..\/input\/titanic\/train.csv\") \ndf.shape","2550b91b":"#Check for the null values\ndf.isna().sum()","7a5c9bbb":"# Fill numeric rows with the median\nfor label, content in df.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            # Add a binary column which tells if the data was missing our not\n            df[label+\"_is_missing\"] = pd.isnull(content)\n            # Fill missing numeric values with median since it's more robust than the mean\n            df[label] = content.fillna(content.median())\n            \n            \n# Turn categorical variables into numbers\nfor label, content in df.items():\n    # Check columns which *aren't* numeric\n    if not pd.api.types.is_numeric_dtype(content):\n        # Add binary column to inidicate whether sample had missing value\n        df[label+\"_is_missing\"] = pd.isnull(content)\n        # We add the +1 because pandas encodes missing categories as -1\n        df[label] = pd.Categorical(content).codes+1   ","b6289b3a":"np.random.seed(42)\n\nX = df.drop(\"Survived\", axis=1)\ny = df.Survived\n\n# Split into train & test set\nX_train, X_test, y_train, y_test = train_test_split(X, # independent variables \n                                                    y, # dependent variable\n                                                    test_size = 0.2) # percentage of data to use for test set","cf0e4568":"# Put models in a dictionary\nmodels = {\"KNN\": KNeighborsClassifier(),\n          \"Logistic Regression\": LogisticRegression(), \n          \"Random Forest\": RandomForestClassifier()}\n\n# Create function to fit and score models\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models : a dict of different Scikit-Learn machine learning models\n    X_train : training data\n    \n    y_train : labels assosciated with training data\n    \n    \"\"\"\n    # Random seed for reproducible results\n    np.random.seed(42)\n    # Make a list to keep model scores\n    model_scores = {}\n    # Loop through models\n    for name, model in models.items():\n        # Fit the model to the data\n        model.fit(X_train, y_train)\n        # Evaluate the model and append its score to model_scores\n        model_scores[name] = model.score(X_test, y_test)\n    return model_scores\n\n\nmodel_scores = fit_and_score(models=models,\n                             X_train=X_train,\n                             X_test=X_test,\n                             y_train=y_train,\n                             y_test=y_test)\nmodel_scores\n\n\n\n","c4d5d862":"# Different RandomForestClassifier hyperparameters\nrf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2)}\n\n\n# Setup random seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions=rf_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\n# Fit random hyperparameter search model\nrs_rf.fit(X_train, y_train);","cfdf4cff":"#Checkout the best parameters\nrs_rf.best_params_","08e8207c":"%%time\n# Train model with the Most ideal hyperparameters\nideal_model = RandomForestClassifier(n_estimators=910,\n                                    min_samples_leaf=1,\n                                    min_samples_split=18,\n                                    max_depth = 10)\nideal_model.fit(X_train, y_train)","1619f248":"# Read the test data\ndf_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ndf_test.head()","d6c7e7d4":"# Fill numeric rows with the median\nfor label, content in df_test.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            # Add a binary column which tells if the data was missing our not\n            df_test[label+\"_is_missing\"] = pd.isnull(content)\n            # Fill missing numeric values with median since it's more robust than the mean\n            df_test[label] = content.fillna(content.median())\n            \n            \n# Turn categorical variables into numbers\nfor label, content in df_test.items():\n    # Check columns which *aren't* numeric\n    if not pd.api.types.is_numeric_dtype(content):\n        # Add binary column to inidicate whether sample had missing value\n        df_test[label+\"_is_missing\"] = pd.isnull(content)\n        # We add the +1 because pandas encodes missing categories as -1\n        df_test[label] = pd.Categorical(content).codes+1 ","0541dc97":"#This column is not present in training set, so drop this column\ndf_test.drop(\"Fare_is_missing\",axis=1, inplace=True)\n","ee8f9a10":"# Make predictions on the test dataset using the best model\ntest_preds = ideal_model.predict(df_test)","11d3102a":"# Create DataFrame compatible with Kaggle submission requirements\ndf_preds = pd.DataFrame()\ndf_preds[\"PassengerId\"] = df_test[\"PassengerId\"]\ndf_preds[\"Survived\"] = test_preds\ndf_preds\n","188165ce":"## Importing the data and preparing it for modelling","8eaead1e":"# Titanic ML competition\n\n## 1. Problem Definition\n\nUsing machine learning, create a model that predicts which passengers survived the Titanic shipwreck\n\n## 2. Data\n\nLooking at the [dataset from Kaggle](https:\/\/www.kaggle.com\/c\/titanic\/data), \nThis is a problem of supervised learning\n\nThere are 2 datasets:\n1. **Train.csv** - Data on which model will be trained\n2. **Test.csv** - Data on which prediction will be done \n\n## 3. Evaluation\n\nUse the model you trained to predict whether or not they survived the sinking of the Titanic.\n"}}