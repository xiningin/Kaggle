{"cell_type":{"2e7774d8":"code","5da120e3":"code","483a94ea":"code","654c6121":"code","a52125cc":"code","8bd896e8":"code","71b49d18":"code","46fc854e":"code","dc13082c":"code","b143fd33":"code","ae76b922":"code","e627fc02":"code","f6a9e514":"code","9790770a":"code","9fc3e26d":"code","930fee8b":"code","ab1c677c":"code","c68ac5da":"code","20dafcf8":"code","c1b9e759":"code","a44212d1":"code","843aa5c6":"code","a225d2f5":"code","a62ee5cd":"code","73442980":"code","84346180":"code","b68eb3bb":"code","2b650251":"code","741cc0b7":"code","3bf86952":"code","dca285e8":"code","c7d1c865":"code","d777b304":"code","d1ba2719":"code","50d72190":"code","1605c249":"code","78acd482":"code","759e9447":"code","16b7c39d":"code","0d5d5489":"code","6a8ac590":"code","80674c78":"code","78c29728":"code","8b3b5b27":"code","75797a86":"code","b266b6d8":"code","5ac6316c":"code","c6fec073":"code","661d5a95":"code","cf87eb14":"code","b8df3e52":"code","622a3e5e":"code","eb0652b2":"code","5aa66a64":"code","c7f208eb":"code","91659eed":"code","bce18823":"code","7956b8aa":"code","bfcb6923":"code","f4c3da43":"code","019a7c36":"code","57e4b416":"code","9ba06e14":"code","725f28bb":"code","785a2412":"code","e947f578":"code","a40dddab":"code","653adb70":"code","ad415b5b":"code","9ff4ed18":"code","df29614e":"code","6e70e1c2":"code","f78a02d3":"markdown","dfc13630":"markdown","51358dad":"markdown","4352c9e0":"markdown","6b701723":"markdown","1d54ee57":"markdown","78038a3d":"markdown","9528a26b":"markdown","697342c9":"markdown","40975789":"markdown","734a4747":"markdown","1f5513f2":"markdown","d2262af1":"markdown","7d27f2aa":"markdown","0c141f08":"markdown","ef083c24":"markdown","72838f4b":"markdown","6857ba40":"markdown","4e5823c2":"markdown"},"source":{"2e7774d8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5da120e3":"data = pd.read_csv('\/kaggle\/input\/mice-protein-expression\/Data_Cortex_Nuclear.csv', sep = ',')\ndata.head()","483a94ea":"print (\"Rows: \",data.shape[0])\nprint (\"Columns: \",data.shape[1])","654c6121":"print(\"Columns: \\n\",data.columns.tolist())","a52125cc":"# check data types and missing values\ndata.info()","8bd896e8":"data.isnull().sum()","71b49d18":"DYRK1A_percent_missing = data['DYRK1A_N'].isnull().sum() * 100 \/ len(data)\nDYRK1A_percent_missing","46fc854e":"data1 = data[data['DYRK1A_N'].notna()]\ndata1.info()","dc13082c":"#data1[data1['ELK_N', 'MEK_N', 'Bcatenin_N'].notna()]\ndata2 = data1.dropna(axis=0, subset=('ELK_N', 'MEK_N', 'Bcatenin_N'))\ndata2.shape","b143fd33":"data2.info()","ae76b922":"#BAD_N              849 non-null float64\n#BCL2_N             777 non-null float64\n#pCFOS_N            972 non-null float64\n#H3AcK18_N          867 non-null float64\n#EGR1_N             852 non-null float64\n#H3MeK4_N           777 non-null float64\nfor col in ['BAD_N', 'BCL2_N', 'pCFOS_N', 'H3AcK18_N', 'EGR1_N', 'H3MeK4_N']:\n    percent_missing = data2[col].isnull().sum() * 100 \/ len(data2)\n    print(col, 'percentage of missing values:', percent_missing)","e627fc02":"for col in ['BAD_N', 'BCL2_N', 'pCFOS_N', 'H3AcK18_N', 'EGR1_N', 'H3MeK4_N']:\n    data2[col].fillna(data2[col].mean(), inplace=True)\ndata2.info()","f6a9e514":"# An overview of the data\ndata2.describe(include = np.object)","9790770a":"data2.describe(include = np.number).round(3)","9fc3e26d":"data3 = data2.drop(['MouseID'], axis = 1)\ndata3.shape","930fee8b":"# Check Unique values in object feature\nobject_col = data3.columns[data3.dtypes==object].tolist()\n\nfor col in object_col:\n    print('The unique values and numbers of', col, 'are:')\n    print(data3[col].value_counts())\n    print('=========')","ab1c677c":"data4 = data3.drop(['Genotype', 'Treatment', 'Behavior'], axis = 1)\ndata4.shape","c68ac5da":"data4.head()","20dafcf8":"import altair as alt\nimport matplotlib.pyplot as plt","c1b9e759":"alt.Chart(data3, width=400).mark_bar().encode(x=alt.X('class', sort='-y'), y='count()').properties(\n    title='Number of Measurements for each class')","a44212d1":"# Boxplot for column 'DYRK1A_N'\ndata3.boxplot(column = 'DYRK1A_N')\nplt.title(\"Box Plot Distribution of 'DYRK1A_N' Column\")\nplt.ylabel('Expression Levels')\nplt.show()","843aa5c6":"for col in ['ITSN1_N', 'BDNF_N', 'NR1_N', 'NR2A_N', 'pAKT_N', 'pERK_N', 'pJNK_N', 'PKCA_N']:\n    data3.boxplot(column = col)\n    plt.title(\"Box Plot Distribution of Protein Expression Levels\")\n    plt.ylabel('Expression Levels')\n    plt.show()","a225d2f5":"alt.Chart(data3).mark_bar().encode(\n    alt.X(\"pBRAF_N\", bin=alt.Bin(extent=[0.1, 0.325], step=0.0125)),\n    y='count()').properties(title='Histogram distribution of protein expression')","a62ee5cd":"alt.Chart(data3).mark_bar().encode(alt.X('pCAMKII_N', bin=alt.Bin(extent=[1, 7.5], step=0.5)), y = 'count()').properties(\n        title='Histogram distribution of protein expression')","73442980":"alt.Chart(data3, width=500).mark_boxplot().encode(y='CaNA_N', x='class').properties(\n    title='Box Plot of CaNA_N level by Class')","84346180":"alt.Chart(data3, width=300).mark_boxplot().encode(y='EGR1_N', x='Genotype').properties(\n    title='Box Plot of EGR1_N level by Genotype')","b68eb3bb":"alt.Chart(data3).mark_point().encode(x='SNCA_N', y='Ubiquitin_N').properties(\n    title='Scatter plot for SNCA_N vs. Ubiquitin_N')","2b650251":"# descriptive features\nData = data4.drop(columns = 'class')\n# target feature\ntarget = data4['class']","741cc0b7":"target.shape","3bf86952":"target_names = data4['class'].unique()\ntarget.value_counts()","dca285e8":"target = target.replace({'c-SC-m': 0, 'c-CS-m':1, 't-SC-m':2, 't-CS-m':3, 't-SC-s':4, 'c-SC-s':5, 'c-CS-s':6, 't-CS-s':7})\ntarget.value_counts()","c7d1c865":"Data.shape","d777b304":"from sklearn import preprocessing\n\nData_df = Data.copy()\n\nData_scaler = preprocessing.MinMaxScaler()\nData_scaler.fit(Data)\nData = Data_scaler.fit_transform(Data)","d1ba2719":"pd.DataFrame(Data).head()","50d72190":"df = pd.DataFrame(Data, columns=Data_df.columns)\ndf.shape","1605c249":"df.head()","78acd482":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier","759e9447":"from sklearn.utils import shuffle\nnew_Ind = [] \ncur_MaxScore = 0\ncol_num = 77 \ncol_Ind_Random = shuffle(range(0, col_num), random_state=1)","16b7c39d":"for cur_f in range(0, col_num):\n    new_Ind.append(col_Ind_Random[cur_f])\n    newData = Data[:, new_Ind]\n    D_train, D_test, t_train, t_test = train_test_split(newData,\n                                                       target,\n                                                       test_size = 0.3,\n                                                       random_state=0)\n    clf = KNeighborsClassifier(5, weights='distance', p=1)\n    fit=clf.fit(D_train, t_train)\n    cur_Score = clf.score(D_test, t_test)\n    if cur_Score < cur_MaxScore:\n        new_Ind.remove(col_Ind_Random[cur_f])\n    else:\n        cur_MaxScore = cur_Score\n        print(\"Score with \" + str(len(new_Ind))+\" selected features: \"+str(cur_Score))","0d5d5489":"print(\"There are \" + str(len(new_Ind)) + \" features selected:\")","6a8ac590":"print(new_Ind)","80674c78":"dataset = pd.DataFrame(Data[:, new_Ind], columns=Data_df.columns[new_Ind])","78c29728":"dataset.shape","8b3b5b27":"dataset.head()","75797a86":"D_train, D_test, t_train, t_test = train_test_split(dataset,\n                                                   target,\n                                                   test_size=0.3,\n                                                   stratify=target.values,\n                                                   random_state=999)","b266b6d8":"print(D_train.shape)\nprint(D_test.shape)\nprint(t_train.shape)\nprint(t_test.shape)","5ac6316c":"KNN_5 = KNeighborsClassifier(5)\nfit = KNN_5.fit(D_train, t_train)\nt_pre = fit.predict(D_test)","c6fec073":"t_pre.shape","661d5a95":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(t_test, t_pre)\nprint(cm)","cf87eb14":"from sklearn.metrics import classification_report\nprint(classification_report(t_test, t_pre))","b8df3e52":"KNN_5_w = KNeighborsClassifier(5, weights = 'distance')\nfit = KNN_5_w.fit(D_train, t_train)\nt_pre = fit.predict(D_test)\ncm = confusion_matrix(t_test, t_pre)\nprint(cm)\nprint(classification_report(t_test, t_pre))","622a3e5e":"KNN_5_w_1 = KNeighborsClassifier(5, weights = 'distance', p = 1)\nfit = KNN_5_w_1.fit(D_train, t_train)\nt_pre = fit.predict(D_test)\ncm = confusion_matrix(t_test, t_pre)\nprint(cm)\nprint(classification_report(t_test, t_pre))","eb0652b2":"KNN = KNeighborsClassifier(4, weights = 'distance', p = 2)\nfit = KNN.fit(D_train, t_train)\nt_pre = fit.predict(D_test)\ncm = confusion_matrix(t_test, t_pre)\nprint(cm)\nprint(classification_report(t_test, t_pre))","5aa66a64":"from sklearn.model_selection import StratifiedKFold, GridSearchCV\n\ncv_method = StratifiedKFold(n_splits=3,shuffle=True, random_state=999)\n\n# define the parameter values\nKNN_para = {'n_neighbors': [1, 2, 3, 4, 5,6, 7], 'p': [1, 2]}\nKNN_gs = GridSearchCV(KNeighborsClassifier(weights = 'distance'), KNN_para, cv=cv_method, scoring = 'accuracy')","c7f208eb":"KNN_gs.fit(D_train, t_train)","91659eed":"KNN_gs.best_params_","bce18823":"KNN_gs.best_estimator_","7956b8aa":"KNN_gs.best_score_","bfcb6923":"KNN_results=pd.DataFrame(KNN_gs.cv_results_)[['mean_test_score', 'std_test_score', 'params']]\nKNN_results_sorted=KNN_results.sort_values('mean_test_score', ascending=False)\nKNN_results_sorted.head()","f4c3da43":"clf = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                     metric_params=None, n_jobs=None, n_neighbors=1, p=1,\n                     weights='uniform')\nfit = clf.fit(D_train, t_train)\nt_pre = fit.predict(D_test)\ncm = confusion_matrix(t_test, t_pre)\nprint(cm)\nprint(classification_report(t_test, t_pre))","019a7c36":"from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(random_state = 999)\nfit = clf.fit(D_train, t_train)\nt_pre = fit.predict(D_test)\nt_pre.shape","57e4b416":"cm = confusion_matrix(t_test, t_pre)\nprint(cm)\nprint(classification_report(t_test, t_pre))\nprint(clf.tree_.max_depth)","9ba06e14":"clf = DecisionTreeClassifier(criterion = 'entropy',random_state = 999)\nfit = clf.fit(D_train, t_train)\nt_pre = fit.predict(D_test)\nt_pre.shape\ncm = confusion_matrix(t_test, t_pre)\nprint(cm)\nprint(classification_report(t_test, t_pre))\nprint(clf.tree_.max_depth)","725f28bb":"clf = DecisionTreeClassifier(criterion = 'entropy',min_samples_split = 10, random_state = 999)\nfit = clf.fit(D_train, t_train)\nt_pre = fit.predict(D_test)\nt_pre.shape\ncm = confusion_matrix(t_test, t_pre)\nprint(cm)\nprint(classification_report(t_test, t_pre))\nprint(clf.tree_.max_depth)","785a2412":"clf = DecisionTreeClassifier(criterion = 'entropy',max_depth = 5,min_samples_split = 10, random_state = 999)\nfit = clf.fit(D_train, t_train)\nt_pre = fit.predict(D_test)\nt_pre.shape\ncm = confusion_matrix(t_test, t_pre)\nprint(cm)\nprint(classification_report(t_test, t_pre))\nprint(clf.tree_.max_depth)","e947f578":"clf = DecisionTreeClassifier(max_depth = 5, min_samples_split = 10, min_samples_leaf=10, random_state = 999)\nfit = clf.fit(D_train, t_train)\nt_pre = fit.predict(D_test)\nt_pre.shape\ncm = confusion_matrix(t_test, t_pre)\nprint(cm)\nprint(classification_report(t_test, t_pre))\nprint(clf.tree_.max_depth)","a40dddab":"params_DT = {'max_depth': [5, 8, 10, 12], 'min_samples_split': [5, 10, 15], 'min_samples_leaf': [5, 10, 15]}\n\ngs_DT = GridSearchCV(DecisionTreeClassifier(criterion = 'entropy'), \n                          param_grid=params_DT, \n                          cv=cv_method,\n                          scoring='accuracy') \n\ngs_DT.fit(D_train, t_train)","653adb70":"gs_DT.best_params_","ad415b5b":"gs_DT.best_estimator_","9ff4ed18":"gs_DT.best_score_","df29614e":"DT_results=pd.DataFrame(gs_DT.cv_results_)[['mean_test_score', 'std_test_score', 'params']]\nDT_results_sorted=DT_results.sort_values('mean_test_score', ascending=False)\nDT_results_sorted.head()","6e70e1c2":"clf = DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=8,\n                       max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=5, min_samples_split=10,\n                       min_weight_fraction_leaf=0.0, presort=False,\n                       random_state=None, splitter='best')\nfit = clf.fit(D_train, t_train)\nt_pre = fit.predict(D_test)\nt_pre.shape\ncm = confusion_matrix(t_test, t_pre)\nprint(cm)\nprint(classification_report(t_test, t_pre))\nprint(clf.tree_.max_depth)","f78a02d3":"Better results than others. However, t test will need to be conducted to know if this model is significantly better than others. Also, when k =1, the model is likely to be overfitting.","dfc13630":"Data types are all reasonable, but missing values exist. There are many protein columns that have 3 missing values, which indicate empty rows? As it only accounts for 0.28% of the rows, it's safe to remove.","51358dad":"There are several hyperparameters for KNN, for instance\n\n* number of neighbors\n* weights\n* the type of distance p\n\nFirst, build the KNN classifier with 5 neighbors","4352c9e0":"Using entropy slightly improve the performance. Now set max_depth as 5.","6b701723":"Let's use grid search to see what's the best number of neighbors to usem","1d54ee57":"It seems that weights helped with the accuracy. While keeping'n_neighbors = 5' and weights, we set p value to 1.","78038a3d":"# Hyperparameter tunning\nWhile keeping 'n_neighbors = 5', different parameters are set for 'weights' and 'p'.","9528a26b":"An overview of data size and the first 5 rows shows that no information is lost after retrieving from the file. Column names are:\n\nMouse ID\nValues of expression levels of 77 proteins; the names of proteins are followed by _N indicating that they were measured in the nuclear fraction.\nGenotype: control (c) or trisomy (t)\nTreatment type: memantine (m) or saline (s)\nBehavior: context-shock (CS) or shock-context (SC)\nClass: c-CS-s, c-CS-m, c-SC-s, c-SC-m, t-CS-s, t-CS-m, t-SC-s, t-SC-m","697342c9":"# Decision tree","40975789":"The Box Plot Distribution of 'DYRK1A_N' Column shows that this protein level distribution is quite skewed. In particular, there are some outliers shown in the box plot. It's unclear if they are true outliers or just skewed data, so they won't be removed at this stage. Further investigation is needed to understand this feature more.\n\nfor-loop is used to explore the box plot distribution of 8 other columns as below.","734a4747":"# Feature encoding and scaling","1f5513f2":"According to the output and the dataset description, column class contain the information in Genotype, Treatment and Behavior already. Therefore, they can be dropped in data4.\n\nUse data4 for modelling, but we'll use data3 for the exploration.","d2262af1":"# KNN","7d27f2aa":"Number of missing values in columns 'BAD_N', 'BCL2_N', 'pCFOS_N', 'H3AcK18_N', 'EGR1_N', 'H3MeK4_N' are all over 5%. It's not a good idea to remove NULL in this case. We'll replace NULL with mean.","0c141f08":"# Grid search ","ef083c24":"# **Data Exploration**","72838f4b":"We end up with a data with 77 descriptive features (proteins) and one target feature (class).\n\n","6857ba40":"# Feature selection","4e5823c2":"p=1 or p=2 produces similar accuracy rate. Let's try lower number of neighbors with p = 2."}}