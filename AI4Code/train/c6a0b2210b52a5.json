{"cell_type":{"20fe0072":"code","c2224ca9":"code","7db0b58a":"code","6a277126":"code","001cfe11":"code","7387360f":"code","8923d999":"code","50512b31":"code","28f95fc3":"code","fb3068b3":"code","8328b6c0":"code","2079c179":"code","49287820":"code","f9f8f377":"code","816bda3e":"code","d0c942db":"code","716a9ba8":"code","5c865d7b":"code","2ebd0ad2":"code","0dbd7c1d":"code","28850f3e":"code","271332a6":"code","90d3bead":"code","42b7d252":"code","afc6132e":"code","7e3f4fb2":"code","dfe3b5ec":"code","c410ad21":"code","4d8beae2":"code","bfd1bdd9":"code","6e85c0e2":"code","412a2a73":"code","cb9901c9":"markdown","d03d11fd":"markdown","8aee71d9":"markdown","53d76413":"markdown","2ea9fd22":"markdown","dc4b2f12":"markdown","6844dbb1":"markdown","894743e9":"markdown","c5f84c84":"markdown","b5a78ca9":"markdown","d59cdbe7":"markdown","52c00e0a":"markdown","8c264a84":"markdown","61d6e3a3":"markdown","a314dc34":"markdown","3527042c":"markdown","2fd258c4":"markdown","560858d5":"markdown","6555eaf3":"markdown","728d6970":"markdown","00f0350a":"markdown","fb6463b6":"markdown","4f714a7c":"markdown"},"source":{"20fe0072":"import os\ndata_dir = '..\/input\/aclmdb\/aclImdb'\n\nprint(os.listdir(data_dir))","c2224ca9":"classes = os.listdir(data_dir + \"\/train\")\nprint(classes)","7db0b58a":"pos_files = os.listdir(data_dir + \"\/train\/pos\")\nprint('No. of training examples for positivie sentiment:', len(pos_files))\nprint(pos_files[:5])","6a277126":"neg_files = os.listdir(data_dir + \"\/train\/neg\")\nprint('No. of training examples for negative sentiment:', len(neg_files))\nprint(neg_files[:5])","001cfe11":"from pathlib import Path\n\ndef read_imdb_split(split_dir):\n    split_dir = Path(split_dir)\n    texts = []\n    labels = []\n    for label_dir in [\"pos\", \"neg\"]:\n        for text_file in (split_dir\/label_dir).iterdir():\n            texts.append(text_file.read_text())\n            labels.append(0 if label_dir is \"neg\" else 1)\n\n    return texts, labels\n\ntrain_texts, train_labels = read_imdb_split('..\/input\/aclmdb\/aclImdb\/train')\ntest_texts, test_labels = read_imdb_split('..\/input\/aclmdb\/aclImdb\/test')","7387360f":"train_texts[0]","8923d999":"train_labels[0]","50512b31":"train_texts[17000]","28f95fc3":"train_labels[17000]","fb3068b3":"from sklearn.model_selection import train_test_split\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)","8328b6c0":"!pip install transformers\nfrom transformers import DistilBertTokenizerFast","2079c179":"tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')","49287820":"train_encodings = tokenizer(train_texts, truncation=True, padding=True)\nval_encodings = tokenizer(val_texts, truncation=True, padding=True)\ntest_encodings = tokenizer(test_texts, truncation=True, padding=True)","f9f8f377":"train_encodings[0:5]","816bda3e":"import torch\n\nclass IMDbDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = IMDbDataset(train_encodings, train_labels)\nval_dataset = IMDbDataset(val_encodings, val_labels)\ntest_dataset = IMDbDataset(test_encodings, test_labels)","d0c942db":"from tqdm.notebook import tqdm\nfrom torch.utils.data import DataLoader\nfrom transformers import DistilBertForSequenceClassification, AdamW\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\nmodel.to(device)\nmodel.train()\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\noptim = AdamW(model.parameters(), lr=5e-5)\n\nfor epoch in range(3):\n    for batch in tqdm(train_loader):\n        optim.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs[0]\n        loss.backward()\n        optim.step()\n        optim.zero_grad()\nmodel.eval()","716a9ba8":"import os\n\n# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n\noutput_dir = '.\/model_save\/'\n\n# Create output directory if needed\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nprint(\"Saving model to %s\" % output_dir)\n\n# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n# They can then be reloaded using `from_pretrained()`\nmodel_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed\/parallel training\nmodel_to_save.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\n# Good practice: save your training arguments together with the trained model\n#torch.save(args, os.path.join(output_dir, 'training_args.bin'))","5c865d7b":"import torch.nn.functional as F","2ebd0ad2":"val_loader = DataLoader(val_dataset, batch_size=16)","0dbd7c1d":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))","28850f3e":"def validation(validation_dataloader):\n  with torch.no_grad():\n    loss_val_list = []\n    preds_list = []\n    accuracy_list = []\n    accuracy_sum = 0\n    for batch in tqdm(validation_dataloader):\n      #print(batch.keys())\n      input_ids = batch['input_ids'].to(device)\n      attention_mask = batch['attention_mask'].to(device)\n      labels = batch['labels'].to(device)\n\n      outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n      loss = outputs[0]\n      logits = F.softmax(outputs[1], dim=1)   # Taking the softmax of output\n      _,preds = torch.max(logits, dim=1)      # Taking the predictions of our batch\n      acc = accuracy(logits,labels)           # Calculating the accuracy of current batch\n      accuracy_sum += acc                     # Taking sum of all the accuracies of all the batches. This sum will be divided by batch length to get mean accuracy for validation dataset\n\n      loss_val_list.append(loss)\n      preds_list.append(preds)\n      accuracy_list.append(acc)\n\n  mean_accuracy = accuracy_sum \/ len(validation_dataloader)\n  return mean_accuracy","271332a6":"validation(val_loader)","90d3bead":"test_loader = DataLoader(test_dataset, batch_size=16)","42b7d252":"validation(test_loader)","afc6132e":"review = ['The movie started with a very intense \"Batman\" like opening. This movie showed promise the first twenty minutes and even though nothing made sense, there was still hope that it would all be tied together and all make sense. Unfortunately, hope was lost quickly and the plot disappeared into a endless past-future alternate reality abyss and never came back. This movie lacked depth and seemed pretentious from Nolan. A truly intellectual \"flex\" that surely he had no idea what was going on either. If there was one good thing from Covid is- very few people have had to sit through this disaster of a movie.']","7e3f4fb2":"review_tokenised = tokenizer(review, truncation=True, padding=True)","dfe3b5ec":"review_dataset = IMDbDataset(review_tokenised, [0])","c410ad21":"review_loader = DataLoader(review_dataset, batch_size=1)","4d8beae2":"with torch.no_grad():\n  for batch in review_loader : \n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n    labels = batch['labels'].to(device)\n    prediction = model(input_ids, attention_mask=attention_mask, labels=labels)\n    logits = F.softmax(prediction[1])","bfd1bdd9":"logits","6e85c0e2":"_,preds = torch.max(logits, dim=1) ","412a2a73":"preds.item()","cb9901c9":"## Demo\nLets take a negative review of movie Tenet","d03d11fd":"## Exploring the folder structure\nWe can see that the data is arranged in test and train folders. The files 'README', 'imdb.vocab', 'imdbEr.txt' are metadata files and are not important in this context.","8aee71d9":"## Validation step","53d76413":"## Turning our labels and encodings into a Dataset object.","2ea9fd22":"The data can be divided into 3 classes (Again ignore metadata files) :\n\n1. pos - positive sentiment \n2. neg - negative sentiment \n3. unsup - unclear sentiment","dc4b2f12":"## Now let\u2019s tackle tokenization. We\u2019ll eventually train a classifier using pre-trained DistilBert, so let\u2019s use the DistilBert tokenizer.","6844dbb1":"The accuracy in test set is 90% !","894743e9":"## Testing our model in test dataset","c5f84c84":"## Splitting the training set into training and validation set","b5a78ca9":"## Fine-tuning with native PyTorch","d59cdbe7":"# Sequence Classification with IMDb Reviews\nWe will download, tokenize, and train a model on the IMDb reviews dataset.","52c00e0a":"## Saving the model","8c264a84":"Prediction step :","61d6e3a3":"The Prediction","a314dc34":"The model correctly predicts the review to be negaive.","3527042c":"So there are 12500 files each of positive and negative sentiment. Each file is scaled between 1-10 where. Note are no files with rating of 5 or 6 , this maybe because dataset creators didn't want any false positives \/ negatives .","2fd258c4":"Lets see the number of positive and negative sentiment example files we have :","560858d5":"## Assigning datasets for texts and it's corresponding labels.","6555eaf3":"Now we can simply pass our texts to the tokenizer. We\u2019ll pass truncation=True and padding=True, which will ensure that all of our sequences are padded to the same length and are truncated to be no longer model\u2019s maximum input length. This will allow us to feed batches of sequences into the model at the same time.","728d6970":"Turning our labels and encodings into a Dataset object :","00f0350a":"Tokenising :","fb6463b6":"The accuracy in validation set is 91 %","4f714a7c":"Below we can see that class 0 has 99.66% probability therefore the prediction will be 0 (negative)"}}