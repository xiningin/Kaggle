{"cell_type":{"d69775fb":"code","6b0d05f1":"code","042f2ca0":"code","ea8be4d4":"code","45ec67e2":"code","1742e896":"code","e00cb4a9":"code","128ac8bc":"code","5d2dccfb":"code","f727a038":"code","9728129d":"code","8711201f":"code","1f23a35f":"code","8cbb5ec4":"code","44b28550":"code","e9d39133":"code","717aa01b":"code","813fb378":"code","37462407":"code","6be4bbf7":"code","7e8e71ee":"code","f6cbb9b4":"code","c3c39f46":"code","df1c3325":"code","831a0a0a":"code","ffa87247":"code","3e4f69e1":"code","93ef6258":"code","277fc4f9":"code","2eb8b305":"code","4efb17e4":"code","e38991b1":"code","43241727":"code","715dfbb5":"code","d4b06e23":"code","aafd9706":"code","be644f9d":"code","4b1bbd35":"code","bf7f9a31":"code","04e51b8e":"code","047eda65":"code","b716c1f9":"code","19403aa6":"code","dd03fec5":"code","b340f034":"code","69ffcc23":"code","f56a6294":"code","0cc56349":"code","35a0d35b":"code","4e77ffe5":"code","55be4248":"code","805412c4":"markdown","7dcff2a9":"markdown","63a25994":"markdown","315a0e97":"markdown","fd721054":"markdown","cfa621a8":"markdown","4118f6e8":"markdown","f681bf6d":"markdown","791f3445":"markdown","c48d5ae2":"markdown","2ac246fe":"markdown","f3f918d0":"markdown","1e087f56":"markdown","303f2cb3":"markdown","8f9a84b8":"markdown","71ec488d":"markdown","bdd952fd":"markdown","f9958707":"markdown","81048cab":"markdown","34c9bcf0":"markdown","e0c0dba1":"markdown","6e3cc9b2":"markdown","5a1c72f8":"markdown","38219db2":"markdown","c5d3f576":"markdown","bf8a01dd":"markdown","cd1b0158":"markdown","2d3b4b8a":"markdown","1e1e5783":"markdown","a41bf998":"markdown","12711c4a":"markdown"},"source":{"d69775fb":"# import libraries\nimport pandas as pd\nimport numpy as np","6b0d05f1":"train = pd.read_csv(r'\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv(r'\/kaggle\/input\/titanic\/test.csv')","042f2ca0":"train.head(5)","ea8be4d4":"train.isna().sum()","45ec67e2":"dfs = [train ,test]\n\nfor df in dfs:\n    df['Age'].fillna(df['Age'].median(), inplace = True)","1742e896":"train.isna().sum()","e00cb4a9":"train['Cabin'].value_counts()","128ac8bc":"for df in dfs:\n    df['Cabin'].fillna(0, inplace = True)","5d2dccfb":"cabins = []\nfor i in train['Cabin']:\n    cabins.append(str(i))","f727a038":"letters = []\nfor i in cabins:\n    letter= i[0]\n    letters.append(letter)","9728129d":"train['Cabin'] = letters","8711201f":"cabins = []\nfor i in test['Cabin']:\n    cabins.append(str(i))","1f23a35f":"letters = []\nfor i in cabins:\n    letter = i[0]\n    letters.append(letter)","8cbb5ec4":"test['Cabin'] = letters","44b28550":"train['Cabin'].head()","e9d39133":"train['Embarked'].value_counts()","717aa01b":"for df in dfs:\n    df['Embarked'].fillna('S')","813fb378":"import seaborn as sns\nimport matplotlib.pyplot as plt\n#seaborn & matplotlib are excellent python libraries to perform clean visualizations.\n#I highly suggest you get familiar with them!\n\n#correlation matrix \ncorr_matrix = train.corr()\nfig, ax = plt.subplots(figsize = (10,8))\nsns.heatmap(corr_matrix, annot = True, fmt='.2g', vmin = -1,\n            vmax = 1, center = 0, cmap = 'coolwarm')","37462407":"train.dtypes","6be4bbf7":"#boxplot\nnumeric_cols = ['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\nfig, ax = plt.subplots(figsize = (10,5))\nsns.boxplot(data = train[numeric_cols], orient = 'h', palette = 'Set2')","7e8e71ee":"from pandas.plotting import scatter_matrix\nscatter_matrix(train[numeric_cols], figsize= (12,8))","f6cbb9b4":"train.hist(bins = 20, figsize = (12,8))","c3c39f46":"sns.countplot(train[train['Survived'] == 1]['Pclass']).set_title('Count Survived for each Class')","df1c3325":"len(train[train['Pclass'] == 1]), len(train[train['Pclass'] == 2]), len(train[train['Pclass'] == 3])","831a0a0a":"train[train['Pclass'] == 1]['Survived'].sum(), train[train['Pclass'] == 2]['Survived'].sum(), train[train['Pclass'] == 3]['Survived'].sum()   ","ffa87247":"percentages = []\nfirst = 136 \/ 216\nsecond = 87\/ 184\nthird = 119\/491\npercentages.append(first)\npercentages.append(second)\npercentages.append(third)","3e4f69e1":"percents = pd.DataFrame(percentages)\npercents.index+=1","93ef6258":"percents['PClass'] = ['1', '2', '3']\ncols= ['Percent', 'PClass']\npercents.columns = [i for i in cols]\nsns.barplot(y = 'Percent', x = 'PClass', data = percents).set_title('Percent Survived for Passenger Class')","277fc4f9":"train['Family'] = train.apply(lambda x: x['SibSp'] + x['Parch'], axis = 1)\ntest['Family'] = test.apply(lambda x: x['SibSp'] + x['Parch'], axis = 1)","2eb8b305":"#dropping columns from the dataframe \ntrain.drop(['SibSp', 'Parch', 'Name', 'Ticket'], axis = 1, inplace = True)\ntest.drop(['SibSp', 'Parch', 'Name', 'Ticket'], axis = 1, inplace = True)","4efb17e4":"train.head(5)","e38991b1":"test.isna().sum()","43241727":"test['Fare'].fillna(test['Fare'].median(), inplace = True)","715dfbb5":"train_df = pd.get_dummies(train)\ntest_df = pd.get_dummies(test)","d4b06e23":"#axis 1 refers to columns!\ntrain_df.drop('PassengerId', axis = 1, inplace = True)","aafd9706":"y = train_df['Survived']\ntrain_df.drop('Survived', axis = 1, inplace = True)\ntrain_df.drop('Cabin_T', axis = 1, inplace = True)\ntest_df.drop('PassengerId', axis = 1, inplace = True)","be644f9d":"X_test = test_df\nX_train = train_df","4b1bbd35":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV","bf7f9a31":"rfc = RandomForestClassifier()","04e51b8e":"param_grid = {\n    'n_estimators': [200, 500, 1000],\n    'max_features': ['auto'],\n    'max_depth': [6, 7, 8],\n    'criterion': ['entropy']\n}","047eda65":"CV = GridSearchCV(estimator = rfc, param_grid = param_grid, cv = 5)\nCV.fit(X_train, y)\nCV.best_estimator_","b716c1f9":"#impot some classification libraries\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier","19403aa6":"rfc = RandomForestClassifier(criterion = 'entropy', max_depth = 8, n_estimators = 500, random_state = 42)\ngbc = GradientBoostingClassifier()\nada = AdaBoostClassifier()","dd03fec5":"rfc.fit(X_train, y)\ngbc.fit(X_train, y)\nada.fit(X_train, y)","b340f034":"from mlxtend.classifier import StackingCVClassifier\nstack_gen = StackingCVClassifier(classifiers = (rfc, gbc, ada),\n                                        meta_classifier = rfc,\n                                        use_features_in_secondary = True)\nstack_gen.fit(X_train.values, y)\ny_pred = stack_gen.predict(X_test.values)","69ffcc23":"#if we also wanted to implement blending, we could do so like this although I don't recommend doing so on this dataset.\ndef blend(X_test):\n    y_pred = ((0.25 * gbc.predict(X_test)) + \\\n            0.25 * ada.predict(X_test) + \\\n           0.5 * stack_gen.predict(np.array(X_test)))\n    return y_pred\n\n#y_pred = np.round(blend(X_test))","f56a6294":"#reshape array so that it can be used in a dataframe for easy submission!\nsubmission = y_pred.reshape(-1, 1)","0cc56349":"sub_df = pd.DataFrame(submission)","35a0d35b":"sub_df['PassengerId'] = test['PassengerId']\nsub_df['Survived'] = submission\ncols = ['PassengerId',\n       'Survived']\nsub_df.drop(0, axis = 1, inplace = True)\nsub_df.columns = [i for i in cols]\nsub_df = sub_df.set_index('PassengerId')","4e77ffe5":"sub_df.head(10)","55be4248":"#put file path in string!\nsub_df.to_csv(r'submission.csv')","805412c4":"It's a good thing that we filled fare with the median value as there is an outlier.","7dcff2a9":"Our param grid is set up as a dictionary so that GridSearch can take in and read the parameters. This search will perform 3 X 1 X 3 X 1 = 9 different combinations and then fit them 5 times (cv = 5), resulting in 45 models trained. Calling best_estimator_ or best_params_ will give us the model that peformed the best.","63a25994":"Keep in mind that the model accuracy could be improved by finding titles for each of the passengers. For simplicity of this tutorial, we won't be covering that but if enough people request it I will make the change in following versions. ","315a0e97":"To ensure our data is trainable in our algorithm, we must take a look at any missing values. There are a combination of techniques to fix these missing values including:\n* Fill with either the median or mean. Using the median may be preferable as it is more robust to outliers. If you high extreme values on each end, then the mean may be affected severally. (i.e. mean income of the district that Bill Gates lives in.)\n* Drop the column if the majority of data is missing.\n* Fill with 0 if appropriate. Many times a missing value may signify \"no item\". This is why it is important to examine the columns with missing data closely.","fd721054":"### One Hot Encoding","cfa621a8":"It worked! We have grabbed the first letter from each row.","4118f6e8":"## Table of Contents:\n* [Cleaning the Dataframe](#Cleaning-the-Dataframe)  \n    * [Age](#Age)  \n    * [Cabin](#Cabin)  \n    * [Embarked](#Embarked)  \n* [Exploratory Analysis](#Exploratory-Analysis)\n* [Feature Engineering](#Feature-Engineering)  \n* [One Hot Encoding](#One-Hot-Encoding)\n* [Preparing the Data](#Preparing-the-Data)\n* [Tuning Hyperparameters](#Tuning-Hyperparameters)\n* [Stacking](#Implementing-Stacking)\n* [Submission to CSV](#Submission-To-CSV)","f681bf6d":"### Cabin","791f3445":"The most important part of each value is what cabin letter they are in. We will aim to pull only the first character (letter) from each row.","c48d5ae2":"### Submission To CSV","2ac246fe":"Take a look at your submission object now by calling (submission). It should print out the array reshaped. I won't include it here because it will make the reader scroll quite a bit to pass the section.","f3f918d0":"### Feature Engineering","1e087f56":"### Check Test DataFrame For Any Missing Values Too!","303f2cb3":"Now all of our missing data is filled in so we can go ahead with our model!","8f9a84b8":"# Titanic: A Beginner's Tutorial\n\nI created a github repo for fast reference to some common ML techniques. I use a lot of them in this notebook.  \nlink: https:\/\/github.com\/ZacharyJWyman\/ML-Techniques","71ec488d":"The majority of first class passengers survived with about slighly lower than 50% of 2nd class passengers surviving. The majority of third class passengers did not survive. Therefore, we can see that Passenger Class impacted your survival chance aboard the Titanic. ","bdd952fd":"### Tuning Hyperparameters ","f9958707":"### Embarked","81048cab":"## Cleaning the Dataframe","34c9bcf0":"Please feel free to ask any questions to clarify topics in the comment section. This dataset is a great guide to get your feet wet in predictive modeling and machine learning. ","e0c0dba1":"GridSearchCV and RandomizedSearchCV are excellent tools for determing the best hyperparameters for your models! This can increase your model accuracy significantly. The only downside is it takes quite some time to run so if using very large datasets you will want to convert to numpy arrays for much faster training time. The dataset in this competition is fairly small so we won't both with this.","6e3cc9b2":"We will fill with the mode of the data column. This being 'S' as it will alter out data the least. ","5a1c72f8":"One Hot Encoding, one of the most useful techniques that a data scientist can know. This techniques label encodes categorical columns resulting in a 1 if the value is true, with all associated values in that row taking value 0. Take for example our 'Embarked' column. Using one hot encoding will create embarked_S, embarked_C, and embarked_Q columns for each row. The True value will take a 1. This is crucial to preparing data for our model as it won't take kindly to non-numerics.","38219db2":"Thanks for following along. Make sure to give this notebook an upvote if it was helpful!","c5d3f576":"## Prepare Data","bf8a01dd":"I choose to fill the missing cabin columns with 0 instead of drop it becuase cabin may be associated with passenger class! We will have a look at a correlation matrix that includes categorical columns once we have used One Hot Encoding!","cd1b0158":"### Exploratory Analysis","2d3b4b8a":"## Implementing Stacking\n\nThis is a very powerful ensemble technique that generally results in increased model accuracy. Stacking forms prediction based on the models and then is peformed on the base model to create a more accurate set of predictions. If you want to achieve even higher accuracy than this notebook, hypertune each model before fitting. ","1e1e5783":"### Age ","a41bf998":"In the above code we iterate through each dataframe and fill the missing Age values with the median of each dataframe! Kinda cool right. If we check, we should see that Age now has 0 missing values.","12711c4a":"We can see above how one hot encoding alters the dataframe."}}