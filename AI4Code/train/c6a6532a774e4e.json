{"cell_type":{"c56b1d2c":"code","ef4fee99":"code","86a209aa":"code","93600ae6":"code","786970a0":"code","08c364e2":"code","cbc68e61":"code","97b9d8b8":"code","7b3a4e35":"code","7ccdfca5":"code","a9c3de68":"code","7d652b4c":"code","55d7ea50":"code","d3f58bc7":"code","1c1c4aa8":"code","387670ce":"code","3ec693ce":"code","27647894":"code","6ee7cfc8":"code","f26e0949":"code","35618f08":"code","8fad4e67":"code","ec708088":"code","d502f9ca":"code","77e2d8ec":"code","004497b2":"code","3093945e":"code","ebcbbbb6":"code","9207d022":"code","ce851b66":"code","3e5b1bff":"code","1ac08117":"code","2446bdf8":"code","620ab2dd":"code","b2fad3e1":"code","53fc988f":"code","cc628031":"code","52728459":"code","370c926c":"code","5b738b84":"markdown","5c0ea553":"markdown","7e1e51e0":"markdown","923f3200":"markdown","169fd9c8":"markdown","6dba3737":"markdown","43cacbce":"markdown","44b895bd":"markdown","0dc6532f":"markdown","b94b4e58":"markdown","57d998d7":"markdown","416f74fd":"markdown","3e3a2614":"markdown"},"source":{"c56b1d2c":"%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', None)\n#pd.set_option('display.max_rows', None)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\nplt.rcParams[\"figure.figsize\"] = (12, 8)\nplt.rcParams['axes.titlesize'] = 16\nplt.style.use('seaborn-whitegrid')\nsns.set_palette('Set3')\n\nimport cv2\nimport gc\n\nimport os\nprint(os.listdir('\/kaggle\/input\/shopee-product-matching\/'))\n\nfrom time import time, strftime, gmtime\nstart = time()\nimport datetime\nprint(str(datetime.datetime.now()))\n\nimport warnings\nwarnings.simplefilter('ignore')","ef4fee99":"base_dir = '\/kaggle\/input\/shopee-product-matching\/'","86a209aa":"train = pd.read_csv(base_dir + 'train.csv')\nprint(train.shape)\ntrain.head()","93600ae6":"test = pd.read_csv(base_dir + 'test.csv')\nprint(test.shape)\ntest.head()","786970a0":"sub = pd.read_csv(base_dir + 'sample_submission.csv')\nprint(sub.shape)\nsub.head()","08c364e2":"print(f'Number of train images: {len(os.listdir(base_dir + \"train_images\/\"))}')\nprint(f'Number of test images: {len(os.listdir(base_dir + \"test_images\/\"))}')","cbc68e61":"train.info()","97b9d8b8":"train['image_path'] = base_dir + 'train_images\/' + train['image']\ntest['image_path'] = base_dir + 'test_images\/' + test['image']\ndisplay(train.head(2), test.head(2))","7b3a4e35":"def display_images(paths, rows, cols, title = None):\n    fig, ax = plt.subplots(rows, cols, figsize = (16, 12))\n    ax = ax.flatten()\n    for i, path in enumerate(image_paths):\n        img = cv2.imread(path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n        ax[i].set_title(img.shape)\n        ax[i].imshow(img)\n        ax[i].grid(False)\n    if title:\n        plt.suptitle(title, fontsize = 15, y = 1.0)","7ccdfca5":"image_paths = np.random.choice(train['image_path'], 9)\ndisplay_images(image_paths, 3, 3, 'Display Random Train Images')","a9c3de68":"image_paths = np.random.choice(train['image_path'], 9)\ndisplay_images(image_paths, 3, 3)","7d652b4c":"image_paths = test['image_path'].values\ndisplay_images(image_paths, 1, 3)","55d7ea50":"train['label_group'].value_counts()","d3f58bc7":"image_paths = np.random.choice(train['image_path'][train['label_group'] == 3627744656].values, 9)\ndisplay_images(image_paths, 3, 3, 'Train Images with most frequent label group')","1c1c4aa8":"image_paths = np.random.choice(train['image_path'][train['label_group'] == 994676122].values, 9)\ndisplay_images(image_paths, 3, 3, 'Train Images with most frequent label group')","387670ce":"image_paths = train['image_path'][train['label_group'] == 1615893885].values\ndisplay_images(image_paths, 1, 2, 'Train Images with least frequent label group')","3ec693ce":"plt.title('Distribution of trainset title length')\nsns.histplot(train['title'].apply(lambda x: len(x)), kde = True);","27647894":"print(f'Number of unqiue titles in trainset: {train[\"title\"].nunique()}')","6ee7cfc8":"train['title_len'] = train['title'].apply(lambda x: len(x))\nprint(f'Max. title length: {train[\"title_len\"].max()}')\nprint(f'Min. title length: {train[\"title_len\"].min()}')","f26e0949":"train['title'].value_counts()","35618f08":"t = 'Koko syubbanul muslimin koko azzahir koko baju'\nimage_paths = np.random.choice(train['image_path'][train['title'] == t].values, 6)\ndisplay_images(image_paths, 2, 3, t)","8fad4e67":"t = 'Emina Glossy Stain'\nimage_paths = np.random.choice(train['image_path'][train['title'] == t].values, 6)\ndisplay_images(image_paths, 2, 3, t)","ec708088":"t = 'Viva Air Mawar'\nimage_paths = np.random.choice(train['image_path'][train['title'] == t].values, 6)\ndisplay_images(image_paths, 2, 3, t)","d502f9ca":"import tensorflow as tf\n\nfrom kaggle_datasets import KaggleDatasets\n\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.applications import EfficientNetB1\n\nprint(f'Tensorflow version: {tf.__version__}')","77e2d8ec":"#TPU CONFIG\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\nBATCH_SIZE = 128 * strategy.num_replicas_in_sync\nprint(BATCH_SIZE)\nAUTO = tf.data.experimental.AUTOTUNE","004497b2":"GCS_DS_PATH = KaggleDatasets().get_gcs_path()\nprint(GCS_DS_PATH)","3093945e":"train_paths = GCS_DS_PATH + '\/train_images\/' + train['image']\ntrain_paths[:5]","ebcbbbb6":"#Create TF Dataset\ndef decode_image(filename, label = None, image_size = (256, 256)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels = 3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    image = tf.image.resize(image, image_size)\n    return image","9207d022":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_paths))\n    .map(decode_image, num_parallel_calls = AUTO)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)","ce851b66":"for t in train_dataset.unbatch().batch(10):\n    print(t.numpy().shape)\n    break","3e5b1bff":"for img in train_dataset.take(1):\n    for i in range(12):\n        ax = plt.subplot(3, 4, i + 1)\n        plt.imshow(img[i].numpy())\n        plt.grid(False)\n        plt.axis('off')\n        plt.title(img[i].shape)","1ac08117":"model = EfficientNetB1(weights = 'imagenet', include_top = False, pooling = 'avg', input_shape = None)\nimage_embeddings = model.predict(train_dataset, verbose = 1)\nprint(f'Train Image Embeddings shape: {image_embeddings.shape}')","2446bdf8":"from sklearn.neighbors import NearestNeighbors\n\nknn = 20\nnn = NearestNeighbors(n_neighbors = knn)\nnn.fit(image_embeddings)\ndistances, indices = nn.kneighbors(image_embeddings)","620ab2dd":"def find_similar(index):\n    query_image = image_embeddings[index].reshape(1, -1)\n    distances, indices = nn.kneighbors(query_image)\n\n    dist = np.where(distances[0] < 3.0)[0]\n    idx = indices[0][dist]\n    posting_ids = train.iloc[idx]['posting_id'].values\n    #print(posting_ids)\n    return posting_ids","b2fad3e1":"def plot_similar(postings):\n    for i in range(6):\n        ax = plt.subplot(2, 3, i + 1)\n        img = cv2.imread(train['image_path'][train['posting_id'] == str(postings[i])].values[0])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n        img = cv2.resize(img, (256, 256))\n        plt.imshow(img)\n        plt.grid(False)\n        plt.axis('off')\n        if i == 0:\n            plt.title('Query Image')\n        else:\n            plt.title('Prediction Image')","53fc988f":"posting_ids = find_similar(1000)\nplot_similar(posting_ids)","cc628031":"posting_ids = find_similar(100)\nplot_similar(posting_ids)","52728459":"posting_ids = find_similar(1990)\nplot_similar(posting_ids)","370c926c":"finish = time()\nprint(strftime(\"%H:%M:%S\", gmtime(finish - start)))","5b738b84":"__Finding Similar Images using Nearest Neighbor__\n\n- Extract image embeddings using a pre-trained tensorflow model\n- Find nearest neighbor of an image based on Euclidean Distance using sklearn ","5c0ea553":"__Display random train Images__","7e1e51e0":"# Evaluation Metric\n\n__Submissions will be evaluated based on their mean F1 score.__","923f3200":"- Let's add a column in the train\/test set with the train\/test images path","169fd9c8":"__Display Test Images__","6dba3737":"__Predict similar images of few train images__","43cacbce":"__Check Images have loaded correctly__","44b895bd":"__Display Images by Label_Group__","0dc6532f":"__Display Images with same title__","b94b4e58":"- So there are images with same title in the dataset","57d998d7":"# Code Requirements\n\nSubmissions to this competition must be made through Notebooks. In order for the \"Submit\" button to be active after a commit, the following conditions must be met:\n\n- CPU Notebook <= 9\n- GPU Notebook <= 2\n- Internet access disabled\n- Freely & publicly available external data is allowed, including pre-trained models\n- Submission file must be named \"submission.csv\"","416f74fd":"__Shopee is the leading e-commerce platform in Southeast Asia and Taiwan.__","3e3a2614":"# Competition Goal\n\n__In this competition, you\u2019ll apply your machine learning skills to build a model that predicts which items are the same products.__"}}