{"cell_type":{"12a6741d":"code","ed978ce6":"code","3b05c845":"code","295c4bac":"code","3203fdbd":"code","6965aaa5":"code","3960ca20":"code","0c01c1cb":"code","c8d68258":"code","828b4eca":"code","318c56fe":"code","27ad1239":"code","d394efe8":"code","2372e90a":"code","47039a91":"code","882fd112":"code","17e8e5c8":"code","85f84501":"code","849e1264":"markdown","e9cebadf":"markdown","a566a8c7":"markdown","f1f0197e":"markdown","91ba21b6":"markdown","4b6b3105":"markdown","d03c78b4":"markdown","1102bb71":"markdown","fe2b75a0":"markdown","0338550a":"markdown","ec7c5501":"markdown","5c000cca":"markdown","08786bf3":"markdown","5be4f922":"markdown","d1f4891e":"markdown","174830ad":"markdown"},"source":{"12a6741d":"import numpy as np\nfrom joblib import Parallel, delayed\n\nimport jax\nfrom jax import jit\nimport jax.numpy as jnp\n\n%config IPCompleter.use_jedi = False","ed978ce6":"# If I set the seed, would I get the same sequence of random numbers every time?\n\nfor i in range(10):\n    # Set initial value by providing a seed value\n    seed = 0 \n    np.random.seed(seed)\n    \n    # Generate a random integer from a range of [0, 5)\n    random_number = np.random.randint(0, 5)\n    print(f\"Seed: {seed} -> Random number generated: {random_number}\")","3b05c845":"# Array of 10 values\narray = np.arange(10)\n\nfor i in range(5):\n    # Set initial value by providing a seed value\n    seed = 1234\n    np.random.seed(seed)\n    \n    # Choose array1 and array2 indices\n    array_1_idx = np.random.choice(array, size=8)\n    array_2_idx = np.random.choice(array, size=2)\n    \n    # Split the array into two sets\n    array_1 = array[array_1_idx]\n    array_2 = array[array_2_idx]\n    \n    print(f\"Iteration: {i+1}  Seed value: {seed}\\n\")\n    print(f\"First array: {array_1}  Second array: {array_2}\")\n    print(\"=\"*50)\n    print(\"\")","295c4bac":"# Array of 10 values\narray = np.arange(10)\n\n# Same example but with a different kind of random number generator\nfor i in range(5):\n    # Set initial value by providing a seed value\n    seed = 0\n    rng = np.random.default_rng(seed)\n    \n    # Choose array1 and array2 indices\n    array_1_idx = rng.choice(array, size=8)\n    array_2_idx = rng.choice(array, size=2)\n    \n    # Split the array into two sets\n    array_1 = array[array_1_idx]\n    array_2 = array[array_2_idx]\n    \n    print(f\"Iteration: {i+1}  Seed value: {seed}\\n\")\n    print(f\"First array: {array_1}  Second array: {array_2}\")\n    print(\"=\"*50)\n    print(\"\")","3203fdbd":"# Set the seed\nseed = 1234\nnp.random.seed(seed)\n\n# Sample a vector of size 10 \narray1 = np.random.randint(0, 10, size=10)\n\n# Sample 10 elements one at a time\nnp.random.seed(seed)\narray2 = np.stack([np.random.randint(0, 10) for _ in range(10)])\n\nprint(f\"Sampled all at once    => {array1}\")\nprint(f\"Sampled one at a time  => {array2}\")","6965aaa5":"def get_sequence(seed, size=5):\n    rng = np.random.default_rng(seed)\n    array = np.arange(10)\n    return rng.choice(array, size=size)","3960ca20":"# Instantiate SeedSequence\nseed = 1234\nss = np.random.SeedSequence(seed)\n\n# Spawn 2 child seed sequence\nchild_seeds = ss.spawn(2)\n\n# Run the function a few times in parallel to check if we get\n# same RNG sequence\nfor i in range(5):\n    res = []\n    for child_seed in child_seeds:\n        res.append(delayed(get_sequence)(child_seed))\n    res = Parallel(n_jobs=2)(res)\n    print(f\"Iteration: {i+1} Sequences: {res}\")\n    print(\"=\"*70)","0c01c1cb":"# Global seed\nnp.random.seed(1234)\n\ndef A():\n    return np.random.choice([\"a\", \"A\"])\n\ndef B():\n    return np.random.choice([\"b\", \"B\"])\n\nfor i in range(2):\n    C = A() + B()\n    print(f\"Iteration: {i+1}  C: {C}\")","c8d68258":"from jax import random","828b4eca":"# Define a state\nseed = 1234\nkey = random.PRNGKey(1234)\nkey","318c56fe":"# Passing the original key to a random function\nrandom_integers = random.randint(key=key, minval=0, maxval=10, shape=[5])\nprint(random_integers)","27ad1239":"# What if we want to call another function?\n# Don't use the same key. Split the original key, and then pass it\nprint(\"Original key: \", key)\n\n# Split the key. By default the number of splits is set to 2\n# You can specify explicitly how many splits you want to do\nkey, subkey = random.split(key, num=2)\n\nprint(\"New key: \",  key)\nprint(\"Subkey: \", subkey)","d394efe8":"# Call another random function with the new key\nrandom_floats = random.normal(key=key, shape=(5,), dtype=jnp.float32)\nprint(random_floats)","2372e90a":"# Splitting is deterministic!\n\nfor i in range(5):\n    key = random.PRNGKey(1234)\n    print(f\"Iteration: {i+1}\\n\")\n    print(f\"Original key: {key}\")\n    key, subkey = random.split(key)\n    print(f\"First subkey: {key}\")\n    print(f\"Second subkey: {subkey}\")\n    print(\"=\"*50)\n    print(\"\")","47039a91":"# You can generate multiple keys at one go with one split\nkey = random.PRNGKey(111)\nprint(f\"Original key: {key}\\n\")\n\nsubkeys = random.split(key, num=5)\n\nfor i, subkey in enumerate(subkeys):\n    print(f\"Subkey no: {i+1}  Subkey: {subkey}\")","882fd112":"# No more Sequential Equivalent Guarantee unlike numpy\n\nkey = random.PRNGKey(1234)\nrandom_integers_1 = random.randint(key=key, minval=0, maxval=10, shape=(5,))\n\nkey = random.PRNGKey(1234)\nkey, *subkeys = random.split(key, 5)\nrandom_integers_2 = []\n\nfor subkey in subkeys:\n    num = random.randint(key=subkey, minval=0, maxval=10, shape=(1,))\n    random_integers_2.append(num)\n\nrandom_integers_2 = np.stack(random_integers_2, axis=-1)[0]\n\nprint(\"Generated all at once: \", random_integers_1)\nprint(\"Generated sequentially: \", random_integers_2)","17e8e5c8":"# Possible highly correlated outputs. \n# Not a very good example but serves the demonstration purpose\n\ndef sampler1(key):\n    return random.uniform(key=key, minval=0, maxval=1, shape=(2,))\n\ndef sampler2(key):\n    return 2 * random.uniform(key=key, minval=0, maxval=1, shape=(2,))\n\nkey = random.PRNGKey(0)\nsample_1 = sampler1(key=key)\nsample_2 = sampler2(key=key)\n\nprint(\"First sample: \", sample_1)\nprint(\"Second sample: \", sample_2)","85f84501":"def sampler1():\n    return np.random.uniform(low=0, high=1, size=(2,))\n\ndef sampler2():\n    return 2 * np.random.uniform(low=0, high=1, size=(2,))\n\nnp.random.seed(0)\nsample_1 = sampler1()\nsample_2 = sampler2()\n\nprint(\"First sample: \", sample_1)\nprint(\"Second sample: \", sample_2)","849e1264":"## Cons\n\n1. Global state is bad for reproducibility: Global state is problematic especially if you are going to implement some sort of concurrency in your code. That's why the original way to set global seed in numpy isn't encouraged anymore\n2. With a shared global state, it\u2019s hard to reason about how it\u2019s being used and updated across different threads, processes, and devices, and it\u2019s very easy to screw up when the details of entropy production and consumption are hidden from the end-user.\n3. The **Mersenne Twister PRNG** used in most of the python and numpy code has several [initialization issues](https:\/\/dl.acm.org\/doi\/10.1145\/1276927.1276928)\n4. `SeedSequencing` makes it easy to get a reproducible sequence of random numbers when concurrency is involved but it still can't be used for JAX (we will see later in why exactly!) \n\nLet's take an example of `SeedSequencing` as well before we move to JAX PRNG design","e9cebadf":"# Random Numbers in JAX\n\nRNG in JAX is very different from RNG in numpy. A question that naturally comes to mind is this: Why would the JAX team implement a whole new PRNG in JAX when they could have just reused the same codebase from numpy? \u00af\\_(\u30c4)_\/\u00af\n\nLet's take a few examples to answer that question\n\nExecution of functions that use numpy code is enforced by Python. Let's say `A`, and `B` are two functions. The return values from `A` and `B` are assigned to `C`. So, the code looks like this: `C = A() + B()`","a566a8c7":"<div class=\"alert alert-warning\"> <b>Note: <\/b>The one we saw above is the <b>legacy<\/b> way to generate a sequence of random numbers in numpy. It uses a legacy generator provided by numpy <i><a href=\"https:\/\/numpy.org\/doc\/stable\/reference\/random\/legacy.html\">RandomState(...)<\/a><\/i>. But this is also the one that is most widely used. There is another functions (preferred way as per the docs) <i>np.random.default_rng()<\/i> that uses the default BitGenerator for generating random sequences.\n<\/div><br><br>\n\nLet's repeat the above example with `default_rng(...)` as well. Because this is a different RNG, we should expect a different sequence here.","f1f0197e":"**Update - 23rd Dec, 2021**\n\nWe have completed the TF-JAX tutorials series. 10 notebooks that covers every fundamental aspect of both TensorFlow and JAX. Here are the links to the notebooks along with the Github repo details:\n\n### TensorFlow Notebooks:\n\n* [TF_JAX_Tutorials - Part 1](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part1)\n* [TF_JAX_Tutorials - Part 2](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part2)\n* [TF_JAX_Tutorials - Part 3](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part3)\n\n### JAX Notebooks:\n\n* [TF_JAX_Tutorials - Part 4 (JAX and DeviceArray)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-4-jax-and-devicearray)\n* [TF_JAX_Tutorials - Part 5 (Pure Functions in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-5-pure-functions-in-jax\/)\n* [TF_JAX_Tutorials - Part 6 (PRNG in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-6-prng-in-jax\/)\n* [TF_JAX_Tutorials - Part 7 (JIT in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-7-jit-in-jax)\n* [TF_JAX_Tutorials - Part 8 (Vmap and Pmap)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-8-vmap-pmap)\n* [TF_JAX_Tutorials - Part 9 (Autodiff in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-9-autodiff-in-jax)\n* [TF_JAX_Tutorials - Part 10 (Pytrees in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-10-pytrees-in-jax)\n\n### Github Repo with all notebooks in one place\nhttps:\/\/github.com\/AakashKumarNain\/TF_JAX_tutorials\n\n---\n\n\n<img src=\"https:\/\/raw.githubusercontent.com\/google\/jax\/main\/images\/jax_logo_250px.png\" width=\"300\" height=\"300\" align=\"center\"\/><br>\n\nWelcome to another JAX tutorial. Till now, we have discussed **DeviceArray** and **Pure Functions** in JAX. Today, we will dive\ninto another important concept **`Pseudo Random Number Generation`** in JAX. We all have been using `random numbers` in libraries\nlike `numpy`, `scikit-learn`, `TensorFlow`, `PyTorch`, etc. We will see how PRNGs, as done in `numpy`, are not good enough and how JAXtries to overcome those limitations.\n\nAs usual, if you haven't gone through the previous tutorials, I highly suggest going through them. Here are the links:\n\n1. [TF_JAX_Tutorials - Part 1](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part1)\n2. [TF_JAX_Tutorials - Part 2](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part2)\n3. [TF_JAX_Tutorials - Part 3](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part3)\n4. [TF_JAX_Tutorials - Part 4 (JAX and DeviceArray)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-4-jax-and-devicearray)\n5. [TF_JAX_Tutorials - Part 5 (Pure Functions in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-5-pure-functions-in-jax\/)\n\n\nLets' start with the basic concepts of PRNGs first, and then we will look into the implementation differences between `numpy` and JAX for the same","91ba21b6":"# PRNG - An Introduction\n\nBefore we look into random numbers generation, here are a few questions that you should ask:<br>\n1. What is PRNG?\n2. What is PRNG used for?\n3. Why should you care about PRNG?\n\n\nLet's tackle those questions one by one and try to understand the `why` and `how` for the things that we are going to learn in this tutorial","4b6b3105":"## Cons\n\n1. The new PRNG design in JAX is only possible if we give up **Sequential Equivalent Guarantee**. Why? Because that property is incompatible with vectorization, the latter one is, in fact, a priority for JAX\n2. This is not a con as such but this is something an end-user can easily forget. Two things to consider here:\n    * If you call a function again and again with the same key, you will **always** get the same output. Consider that you want to sample 5 random numbers from a uniform distribution. If you pass the same key to your sampling function, you will end up with 5 duplicate numbers.\n    * If you pass the same key to different functions, in some cases you will get highly correlated results. The end-user should always split the key before passing to anything that uses a random function in whatsoever sense\n    \nLet's take an example of each to clarify these points","d03c78b4":"# JAX PRNG: Pros and Cons\n\nNow that we have seen the design of PRNG in JAX and how it is implemented and consumed, it is time to discuss the `pros` and `cons` of this approach. **After all, anything and everything has pros and cons**\n\n\n## Pros\n\n1. The JAX PRNG is a counter-based PRNG design and it uses the **Threefry hash function**. This design allows JAX to escape sequential execution order constraint, allowing everything to be vectorizable and to be parallelizable without giving up on reproducibility\n2. Every random function consumes the state but doesn't change it. Neither the `key` has to be returned from the function\n3. The `split` method is **deterministic**. So, if you start with a random key, and split it into `n` keys in your code, you can be assured that every time you run the code, you will get the same splits. We will see an example of this right away\n4. You can generate `n` number of keys from a key in a single go and keep passing them around","1102bb71":"So, a `key` is nothing but a `DeviceArray` of shape `(2, )`. This key is then passed to random functions. **Random functions consume the state but don't change it**, meaning if you keep passing the same key to the same function, it will always return the same output.\n\nBecause functions don't change the state, ever ytime we call a new random function, we need to pass a new key. How is the new key generated? By splitting the original key. Take a look at the example below","fe2b75a0":"Let's take a bit complex example. We will take an array and split the array into two arrays.","0338550a":"# Random Numbers in Numpy","ec7c5501":"## What is PRNG?\n\nIf we go by the definition, then **`Pseudo Random Number Generation`** is a process of generating a sequence of random numbers **algorithmically** such that the properties of the generated random numbers approximate the properties of a sequence of random numbers sampled from an appropriate distribution. And when we say `random`, it means that the probability of predicting this sequence is no better than a random guess.\n\nAlthough we are concerned about the randomness here, pseudo random number generation isn't *truly* a random process. Why? Because the sequence is determined by the initial value or initial state provided to the algorithm. The **algorithm** used to generate these sequences of random numbers is known as **`Pseudo Random Number Generator`**\n\n\n## What is PRNG used for?\n\nPRNG has a lot of use cases but among the most interesting ones are in `Cryptography`, `Simulations`, `Games`, `Data Science and Machine Learning`(of course), etc. You might have noticed that *most* people set the **`seed`** in their Data Science and Machine Learning workflow. The **seed** is known as the initial value!\n\n\n## Why should you care about PRNG?\n\nAlthough there are tons of use cases of PRNG, I would keep this very specific to Data Science and Machine Learning workflow. When we set a seed, what we try to solve is the `reproducibility` issue. Although `reproducibility` depends on a lot of things, I will use the term very loosely in this context. \n\nWe deal with random states in Machine Learning work more often than we think about. For example, splitting a dataset into training and validation sets, sampling the weights of a hidden layer from a given distribution in a neural network, sampling a noise vector from a Gaussian distribution, etc. So, when we say `reproducible` in this context, what we mean is that no matter how many times I run the same process, I should get the same sequence of random numbers. That's why setting a seed becomes important. \n\n**Note:** Saying it again, setting `seed` doesn't solve the reproducibility crisis of a workflow, it's just a first step to ensure it.\n\nLet's take an example to clear the point about reproducibility!","5c000cca":"You see that in JAX code, the outputs of two samplers were highly correlated while in numpy code we didn't get that perfect correlation. **Lesson?** Unless you want the same outputs, never reuse a `key` by passing it to different random functions in JAX. **Always split the key!**\n\nThat's it for Part 6! We will dive into other important concepts in the next tutorial. Stay tuned!\n\n# References\n1. https:\/\/jax.readthedocs.io\/en\/latest\/notebooks\/Common_Gotchas_in_JAX.html#rngs-and-state\n2. https:\/\/github.com\/google\/jax\/blob\/main\/design_notes\/prng.md\n3. https:\/\/numpy.org\/neps\/nep-0019-rng-policy.html\n4. https:\/\/albertcthomas.github.io\/good-practices-random-number-generators\/\n5. https:\/\/courses.physics.illinois.edu\/phys466\/fa2016\/lnotes\/PRNG.pdf","08786bf3":"Here the execution has a defined order. `A()` is always called before `B()`. But if you do the same thing in JAX (although JAX doesn't allow string type, this is just for the sake of an example) and `jit` it, then you don't know whether `A()` will be called first or `B()` will be called first. Why?\n\n1. XLA will execute them in the order that is most efficient not necessarily in the same order. Remember `tf.control_dependencies(...)` that we used to use in the old days? Nothing wrong with TensorFlow, it's just a way to instruct the compiler\n2. If you force the order of execution, then it contradicts the philosophy of JAX that if two transformations are independent of each other, then their execution can be parallelized.\n\n\nThis looks like a crisis. How? If you use a global state (as in numpy), you won't be able to infer which function was called first, hence the sequence of generated random numbers is irreproducible. What's the solution then?\n\n## RNG Design in JAX\n\nTo make sure that we can parallelize the transformations, and still get reproducible results, JAX applies two rules:\n1. Don't depend on the global seed for generating random sequences\n2. Random functions should explicitly consume a state(seed), this will ensure that these functions would reproduce the same result when the same seed. This can have some weird effects as well which we will see in a moment\n\nLet's take a few examples of how `state` is passed to random functions in JAX.\n\n**Note:** When people say `state`, `seed`, or `key` in the context of PRNG, they mean the same thing (unless it is something different). JAX uses the word `key` and `subkey` more often than the word `seed`. To keep it consistent with the docs, we will use the same terminology here","5be4f922":"Whattttttt!!! Let's try that in numpy now!","d1f4891e":"**Note:** Although we are calling them `key` and `subkey`, both are states and you can pass either of them to any random function or even the `split` function","174830ad":"# Numpy PRNG: Pros and Cons\n\nWe saw a few examples of how you can generate pseudo random numbers in numpy. But I am pretty sure that most of us overlook the `pros` and `cons` of these approaches. Today isn't that day. We will dive into the pros and cons right away.\n\n\n## Pros\n\n1. Setting a global seed is easy from most of the end users' perspectives. You set it once and be done with it\n2. With the new generator and **[SeedSequencing](https:\/\/numpy.org\/doc\/stable\/reference\/random\/bit_generators\/generated\/numpy.random.SeedSequence.html#numpy.random.SeedSequence)**, it is possible to produce repeatable pseudo-random numbers across multiple processes (local or distributed)\n3. **Sequential Equivalent Guarantee**: One of the good things about random number generation in `numpy` is that it ensures sequential equivalent guarantee. What does that mean? It means that whether you sample a vector of `n` elements at once, or sample `n` elements but one at a time, the final sequence will always be the same. Let's see this one in action"}}