{"cell_type":{"2527ffa4":"code","a2b68406":"code","9aa3ed4e":"code","12c49b8e":"code","3e8b3ec1":"code","4f9fe8d4":"code","3364ca07":"code","2fcbefd4":"code","c21a6170":"code","943e3144":"code","a61ad8e0":"code","96125616":"code","7ba073aa":"code","1bebfcf7":"code","9ce53680":"code","f769778b":"code","21a5d787":"code","4a6ebe3d":"code","c427dd7a":"code","4f33dc64":"code","6d265b38":"code","a0c609c7":"code","2dca7c7f":"code","c0ed8cc5":"code","b21c4e77":"code","45a13411":"code","2be9f3c4":"code","0d58fa95":"code","311751d0":"code","5a175364":"code","5b6dea07":"code","f2d5dc00":"markdown","497aeae9":"markdown","22c6ace7":"markdown","ff85c881":"markdown","52c299e5":"markdown","ae9a0d53":"markdown","c4b0f922":"markdown","44eb85c0":"markdown","0f5e3665":"markdown","951500a0":"markdown","f3df895b":"markdown","66015683":"markdown","4bffdb89":"markdown","fbd14bb4":"markdown","5e76dc73":"markdown","0558a94e":"markdown","346572fd":"markdown","91c4da34":"markdown"},"source":{"2527ffa4":"import numpy as np \nimport pandas as pd\nfrom math import sqrt\nfrom scipy.stats import skew\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error","a2b68406":"plt.style.use(style='fivethirtyeight')\nplt.rcParams['figure.figsize'] = (10, 6)","9aa3ed4e":"# load the datasets into dataframe\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","12c49b8e":"# show the first few records of train set\ntrain.head()","3e8b3ec1":"# check the number of records and columns in both of datasets\nprint('No. of records in train dataset: ', len(train.index))\nprint('No. of columns in train dataset: ', len(train.columns))\nprint('No. of records in test dataset: ', len(test.index))\nprint('No. of columns in test dataset: ', len(test.columns))","4f9fe8d4":"# check the missing values\nprint ('Total missing values in train set', sum(train.isna().sum()))\nprint ('Total missing values in test set', sum(test.isna().sum()))","3364ca07":"# check the missing values\nprint ('Total missing values in train set', sum(train.isna().sum()))\nprint ('Total missing values in test set', sum(test.isna().sum()))","2fcbefd4":"train['SalePrice'].describe()","c21a6170":"numeric_cols = train.select_dtypes(include = [np.number])\ncorr = numeric_cols.corr()\nprint ('The Most Correlated Features with SalePrice:'), print (corr['SalePrice'].sort_values(ascending = False)[:10], '\\n')\nprint ('The Most Uncorrelated Features with SalePrice:'), print (corr['SalePrice'].sort_values(ascending = False)[-5:])","943e3144":"plt.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice')\nplt.xlabel('GrLivArea (Above grade \"ground\" living area square feet)')","a61ad8e0":"plt.scatter(x = train['GarageArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice')\nplt.xlabel('GarageArea')","96125616":"# remove GrLivArea outliers\ntrain = train[train['GrLivArea'] < 4500]","7ba073aa":"# remove GarageArea outliers\ntrain = train[train['GarageArea'] < 1200]","1bebfcf7":"# drop columns with percentage of missing values > 80%\ntrain_percentage = train.isnull().sum() \/ train.shape[0]\nprint (train_percentage[train_percentage > 0.80])\ntrain = train.drop(train_percentage[train_percentage > 0.80].index, axis = 1)","9ce53680":"# do the same with test data\ntest_percentage = test.isnull().sum() \/ test.shape[0]\nprint (test_percentage[test_percentage > 0.80])\ntest = test.drop(test_percentage[test_percentage > 0.80].index, axis = 1)","f769778b":"# encode categorical variables\nle = preprocessing.LabelEncoder()\nfor name in train.columns:\n    if train[name].dtypes == 'O':\n        train[name] = train[name].astype(str)\n        le.fit(train[name])\n        train[name] = le.transform(train[name])","21a5d787":"# do the same for testset\nfor name in test.columns:\n    if test[name].dtypes == 'O':\n        test[name] = test[name].astype(str)\n        le.fit(test[name])\n        test[name] = le.transform(test[name])","4a6ebe3d":"# fill missing values based on probability of occurrence\nfor column in train.columns:\n    null_vals = train.isnull().values\n    a, b = np.unique(train.values[~null_vals], return_counts = 1)\n    train.loc[train[column].isna(), column] = np.random.choice(a, train[column].isnull().sum(), p = b \/ b.sum())","c427dd7a":"# apply log transformation to reduce skewness over .75 by taking log(feature + 1)\nskewed_train = train.apply(lambda x: skew(x.dropna()))\nskewed_train = skewed_train[skewed_train > .75]\ntrain[skewed_train.index] = np.log1p(train[skewed_train.index])","4f33dc64":"# deal with the skewness in the test data\nskewed_test = test.apply(lambda x: skew(x.dropna()))\nskewed_test = skewed_test[skewed_test > .75]\ntest[skewed_test.index] = np.log1p(test[skewed_test.index])","6d265b38":"X = train.drop(['SalePrice', 'Id'], axis = 1)\ny = train['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","a0c609c7":"lr = linear_model.LinearRegression()","2dca7c7f":"model = lr.fit(X_train, y_train)","c0ed8cc5":"# make predictions based on model\npredictions = model.predict(X_test)","b21c4e77":"print ('MAE is:', mean_absolute_error(y_test, predictions))\nprint ('MSE is:', mean_squared_error(y_test, predictions))\nprint ('RMSE is:', sqrt(mean_squared_error(y_test, predictions)))","45a13411":"# alpha helps to show overlapping data\nplt.scatter(predictions, y_test, alpha = 0.7, color = 'b')\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Linear Regression Model')","2be9f3c4":"submission = pd.DataFrame()\nsubmission['Id'] = test['Id'].astype(int)","0d58fa95":"temp = test.select_dtypes(include = [np.number]).drop(['Id'], axis = 1).interpolate()","311751d0":"predictions = model.predict(temp)","5a175364":"predictions = np.exp(predictions)\nsubmission['SalePrice'] = predictions","5b6dea07":"submission.to_csv('submission.csv', index = False)","f2d5dc00":"Then using the fit method to \"fit\" the model to the dataset. What this does is nothing but make the regressor \"study\" the data and \"learn\" from it.","497aeae9":"Split the data into training and testing set using scikit-learn train_test_split function. We are using 80% of the data for training and 20% for testing, train_test_split() returns four objects:\n\n- **X_train**: the subset of our features used for training\n- **X_test**: the subset which will be our \u2018hold-out\u2019 set \u2013 what we\u2019ll use to test the model\n- **y_train**: the target variable SalePrice which corresponds to X_train\n- **y_test**: the target variable SalePrice which corresponds to X_test\n\nNow we will import the linear regression class, create an object of that class, which is the linear regression model.","22c6ace7":"I will perform a simple linear regression on the dataset to predict house prices. In order to train out the regression model, we need to first split up the data into an X list that contains the features to train on, and a y list with the target variable, in this case, the Price column.","ff85c881":"# Exploratory Data Analysis","52c299e5":"At first glance, there are increases in living area correspond to increases in price, with few outliers.","ae9a0d53":"In this section the data is prepared (transformed, encoded, etc) to make it suitable for a building and training machine learning model. I chose to manually remove certain extreme outliers in the dataset to produce a better fit.","c4b0f922":"# Data Preprocessing","44eb85c0":"The aobve line code shows that the average sale price of a house is close to 180,000 with most of the values falling within the 130,000 to 215,000 range. Next step is to show the relationship between the columns to examine the correlations between the features and the target.","0f5e3665":"The most correlated features to sale price were the overall quality score (79%), above-ground living area (71%), garage area (64%), and number-of-car garage (62%). Next step is to plot each variable individually against SalePrice in a scatter plot to check outliers as outliers can affect the regression model by pulling the estimated regression line further away from the true population regression line.","951500a0":"So there are many homes with 0 for GarageArea and there are a few outliers as well!","f3df895b":"This Kaggle dataset consists of roughly 3,000 property listings (observations), each with 79 property attributes, and our target, sale price. The goal is to use EDA, data cleaning, preprocessing and linear model to predict home prices given the features of the home. I will follow these steps to a successful submission:\n\n1. Exploratory Data Analysis\n1. Data Preprocessing\n    1. Fixing Skewness and Outliers\n    1. Encoding Categorical Data\n    1. Imputing Missing Values\n1. Modelling\n1. Submission","66015683":"# Linear Regression Model for Predicting House Prices","4bffdb89":"R-squared is the measure of how close the data are to the fitted regression line, in other words it measures the strength of the relationship between the model and the SalePrice on a convenient 0 \u2013 100% scale.","fbd14bb4":"There are three primary metrics used to evaluate linear models. These are:\n* Mean absolute error (MAE)\n* Mean squared error (MSE)\n* Root mean squared error (RMSE)\n\n**MAE**: The easiest to understand. Represents average error.<br>\n**MSE**: Similar to MAE but noise is exaggerated and larger errors are \"punished\". It is harder to interpret than MAE as it's not in base units, however, it is generally more popular.<br>\n**RMSE**: Most popular metric, similar to MSE, however, the result is square rooted to make it more interpretable as it's in base units. It is recommended that RMSE be used as the primary metric to interpret your model.","5e76dc73":"# Submission","0558a94e":"There are many ways to handle NaN values, whether to fill with the mean or median, however strings cannot be averaged or median-ed. One way to fill missing values is to impute these missing values according to their probability of occuring in the dataset to avoid single-valued imputation that impacts the quality of inference and prediction.","346572fd":"# Modelling","91c4da34":"In this initial investigations on data will be performed to to develop an understanding of the data, discover patterns and spot anomalies."}}