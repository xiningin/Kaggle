{"cell_type":{"7328306b":"code","e5e900e9":"code","3fd1f987":"code","278b33b1":"code","881fa8ee":"code","c3da796e":"code","a89c0181":"code","406f5651":"code","834daedc":"markdown","6a7d9e75":"markdown","28d05d85":"markdown","b2aad6f1":"markdown"},"source":{"7328306b":"import numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e5e900e9":"import time\nimport torch\nimport random\n\ns = time.time()\n\nseed = 42 #int(np.random.randint(0, 1e9))\n\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Random seed:\", seed)\nprint(\"Device:\", device)\nbasic_cols = ['investment_id', 'target']\nnum_feat = 300 #total of 300 feats from f_0 to f_299\nN_INVID = 3774 #Max investment_id\nfeatures = [f'f_{i}' for i in range(num_feat)]\ncols = basic_cols + features\n\ncol_dtypes = {\n    #'row_id' : np.object,\n    #'time_id' : np.int32,\n    'investment_id' : np.int32,\n    'target' : np.float32,\n}\nfor i in range(300):\n    col_dtypes[f\"f_{i}\"] = np.float32","3fd1f987":"full_train_df = pd.read_csv(\n        \"..\/input\/ubiquant-market-prediction\/train.csv\",\n        dtype=col_dtypes,\n        usecols=cols\n)\nfull_train_df","278b33b1":"from torch import nn\n\nclass RegressionModel(torch.nn.Module):\n    def __init__(self, in_shape, out_shape, hidden, device='cpu'):\n        super().__init__()\n        self.in_shape = in_shape\n        self.out_shape = out_shape\n        self.hidden = hidden\n        self.device = device\n        self.initialize_weights()\n        \n    def initialize_weights(self):\n        self.w1 = torch.nn.Parameter(torch.randn((self.hidden, self.in_shape), device=self.device, requires_grad=True))\n        self.w2 = torch.nn.Parameter(torch.randn((self.out_shape, self.hidden), device=self.device, requires_grad=True))\n        self.b1 = torch.nn.Parameter(torch.randn(1, device=self.device, requires_grad=True))\n        self.b2 = torch.nn.Parameter(torch.randn(1, device=self.device, requires_grad=True))\n        self.drop = torch.nn.Dropout(p=0.1)\n    \n    def forward(self, x):\n        #basic linear computation\n        y_hat = torch.add(torch.mm(self.w1, x.t()), self.b1)\n        #Apply relu\n        y_hat = self.drop(torch.relu(y_hat))\n        #return regression out\n        return torch.add(torch.mm(self.w2, y_hat), self.b2)\n\nclass TimeSeriesModel(nn.Module):\n    def __init__(self, in_shape, out_shape=1, hidden_shape=64, embed_size=32, device=None):\n        super().__init__()\n        self.in_shape = in_shape\n        self.out_shape = out_shape\n        self.hidden_shape = hidden_shape\n        self.embed_size = embed_size\n        self.layer_dim = 16\n        self.device = device\n        self.init_layers()\n    \n    def init_layers(self):\n        self.embedding = nn.Embedding(N_INVID+1, self.embed_size).to(self.device)\n        self.gru = nn.GRU(self.in_shape, self.hidden_shape, self.layer_dim,\n                          batch_first=True, dropout=0.1).to(self.device)\n        self.out = RegressionModel(self.hidden_shape, self.out_shape, self.layer_dim, self.device)\n    \n    def forward(self, x):\n        #Embed investment_id\n        emb = self.embedding(x[0])\n        emb = torch.mul(x[1], emb)\n        emb = torch.reshape(emb, (x[0].size(0), -1, emb.size(1)))\n        emb = torch.reshape(x[1], (x[1].size(0), -1, x[1].size(1)))\n        \n        # Initializing hidden state for first input with zeros\n        h0 = torch.zeros(self.layer_dim, emb.size(0), self.hidden_shape,\n                         device=self.device, requires_grad=True)\n        \n        # Forward propagation by passing in the input and hidden state into the model\n        y_hat, _ = self.gru(emb, h0.detach())\n        y_hat = torch.reshape(y_hat, (y_hat.size(0)*y_hat.size(1), -1))\n        \n        y_hat = self.out(y_hat)\n        return y_hat[-1, :]","881fa8ee":"batch_size = 65536\nbatches = []\nfor i in range(0, full_train_df.shape[0], batch_size):\n    batches.append((i, min(full_train_df.shape[0], i+batch_size)))","c3da796e":"from sklearn.model_selection import train_test_split as tts\nfrom IPython.display import clear_output\nimport torch.optim as optim\n\ndef loss(y_predicted, y_target):\n    #RMSE Loss\n    return torch.sum((y_predicted - y_target)**2)\n\nmodel = TimeSeriesModel(num_feat, hidden_shape=64, embed_size=1, device=device)\nepochs = 1000\nverbose = max(1, epochs \/\/ 100)\ntol = epochs\/\/5\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n                                                 len(batches),\n                                                 last_epoch=-1,\n                                                 eta_min=1e-5,\n                                                 verbose=False)\nmin_loss = np.inf\ncnt = 0\n\ntrain_size = 0\nvalid_size = 0\n\nfor t in tqdm(range(1, epochs+1), desc=\"Training\"):\n    #clear_output(wait=True)\n    # Set the gradients to 0.\n    optimizer.zero_grad()\n    total_loss = 0.0\n    valid_loss = 0.0\n    total_train_size = 0\n    total_valid_size = 0\n    \n    for start, end in batches:\n        #Split train and validation data\n        x_dataset, x_valid, y_dataset, y_valid = tts(\n            full_train_df.iloc[start:end][['investment_id']+features],\n            full_train_df.iloc[start:end]['target'].values,\n            test_size=0.1, shuffle=True, random_state=seed)\n        total_train_size += x_dataset.shape[0]\n        total_valid_size += x_valid.shape[0]\n        \n        #Train\n        tx_data = torch.tensor(x_dataset[features].values, dtype=torch.float).to(device)\n        tinv_data = torch.tensor(x_dataset['investment_id'].values, dtype=torch.int).to(device)\n        ty_data = torch.tensor(y_dataset, dtype=torch.float).to(device)\n        \n        #Validation\n        vx_data = torch.tensor(x_valid[features].values, dtype=torch.float).to(device)\n        vinv_data = torch.tensor(x_valid[\"investment_id\"].values, dtype=torch.int).to(device)\n        vy_data = torch.tensor(y_valid, dtype=torch.float).to(device)\n    \n    \n        # Main optimization loop\n        model.train()\n        # Compute the current predicted y's from x_dataset\n        y_predicted = model((tinv_data, tx_data))\n        # See how far off the prediction is\n        current_loss = loss(y_predicted, ty_data)\n        total_loss += current_loss\n        \n        # Compute the gradient of the loss\n        current_loss.backward()\n        # Update model W and b accordingly.\n        optimizer.step()\n        # Update LR of optimizer\n        scheduler.step()\n        \n        #Compute validation loss\n        with torch.no_grad():\n            model.eval() #Change model to evaluation mode\n            vloss = loss(model((vinv_data, vx_data)), vy_data)\n            valid_loss += vloss\n\n    #Check for early stopping\n    if valid_loss >= min_loss:\n        cnt += 1\n        if cnt >= tol:\n            print(\"Early stopping!\")\n            break\n    else:\n        #Save the model weights\n        torch.save(model.state_dict(), \"model_weights.pth\")\n        min_loss = valid_loss\n        cnt = 0\n\n    if t%verbose==0:\n        print(f\"epoch = {t:2}\/{epochs}, \" +\n              f\"RMSE loss = {torch.sqrt(total_loss\/total_train_size):.6f}, \" +\n              f\"MSE loss = {(total_loss\/total_train_size):.6f}, \" +\n              f\"RMSE valid_loss = {torch.sqrt(valid_loss\/total_valid_size):.6f}, \" +\n              f\"min_loss = {torch.sqrt(min_loss\/total_valid_size):.6f}, \" +\n              f\"cnt={cnt}\"\n             )\n        model.train() #Return to train mode\n    \nprint(f\"Total time spent: {time.time()-s:.4f} seconds\")","a89c0181":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test()","406f5651":"#Switch to evaluation mode\nmodel.load_state_dict(torch.load(\"model_weights.pth\"))\nmodel.eval()\nfor (test_df, sample_prediction_df) in iter_test:\n    inv_x = torch.tensor(test_df['investment_id'].values, dtype=torch.int).to(device)\n    test_x = torch.tensor(test_df[features].values, dtype=torch.float).to(device)\n    pred = model((inv_x, test_x))\n    sample_prediction_df['target'] = pred.detach().cpu().numpy()\n    env.predict(sample_prediction_df)\n    display(sample_prediction_df)","834daedc":"# Setting Environmental Variables","6a7d9e75":"# Model creation and Training","28d05d85":"# Begin","b2aad6f1":"# Prediction"}}