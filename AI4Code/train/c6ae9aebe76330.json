{"cell_type":{"b92eeb77":"code","ab50ae74":"code","9755f808":"code","db200aef":"code","60f7306f":"code","8856d81c":"code","d81b1fa4":"code","a7c13c30":"code","ac2f248c":"code","794b5a99":"code","714d34ba":"code","37275174":"code","6bf9d617":"code","9ee2d649":"code","0c8dbc50":"code","f8fab564":"code","2bba4f75":"code","28cde316":"code","6fd0aa84":"code","3d636a96":"code","cfb0cb43":"code","d5977bcc":"code","5ac05dcb":"code","9794b910":"code","28030ef3":"code","47b2132c":"markdown","7b586b30":"markdown","cbb58142":"markdown","d3138ee5":"markdown","eab3a797":"markdown","b2328a0c":"markdown","fbbef2ca":"markdown","b43cbf2f":"markdown","204ab1e8":"markdown","8b2f734e":"markdown","4892b957":"markdown","1b3aaf99":"markdown","7d156cb4":"markdown","26721f86":"markdown","00717046":"markdown","3c5353a6":"markdown","95ecee5b":"markdown","672247e9":"markdown","1dbb3d65":"markdown"},"source":{"b92eeb77":"# Import necesssray library for the analysis and model\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nimport re\nimport string\nimport pandas_profiling\nimport random\nimport string\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\n\n#Load 3 different files-train, test and submission\ntrain = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nprint (train.shape, test.shape, sample_submission.shape)\n\n# See the data attributes \ntrain.head()","ab50ae74":"test.head()","9755f808":"sample_submission.head()","db200aef":"train.duplicated().sum()","60f7306f":"train = train.drop_duplicates().reset_index(drop=True)","8856d81c":"# Let see the disaster and non-disaster frequency in traininf set\nsns.countplot(y=train.target);","d81b1fa4":"# Lets check with the missing data\ntrain.isnull().sum()","a7c13c30":"test.isnull().sum()","ac2f248c":"# lets check the key word similarty in training and test data set\nprint (train.keyword.nunique(), test.keyword.nunique())\nprint (set(train.keyword.unique()) - set(test.keyword.unique()))","794b5a99":"# Most common keywords\nplt.figure(figsize=(9,6))\nsns.countplot(y=train.keyword, order = train.keyword.value_counts().iloc[:15].index)\nplt.title('Top 15 keywords')\nplt.show()\n# train.keyword.value_counts().head(10)","714d34ba":"top_d = train.groupby('keyword').mean()['target'].sort_values(ascending=False).head(10)\ntop_nd = train.groupby('keyword').mean()['target'].sort_values().head(10)\n\nplt.figure(figsize=(13,5))\nplt.subplot(121)\nsns.barplot(top_d, top_d.index, color='pink')\nplt.title('Keywords with highest % of disaster tweets')\nplt.subplot(122)\nsns.barplot(top_nd, top_nd.index, color='yellow')\nplt.title('Keywords with lowest % of disaster tweets')\nplt.show()","37275174":"raw_loc = train.location.value_counts()\ntop_loc = list(raw_loc[raw_loc>=10].index)\ntop_only = train[train.location.isin(top_loc)]\n\ntop_l = top_only.groupby('location').mean()['target'].sort_values(ascending=False)\nplt.figure(figsize=(14,6))\nsns.barplot(x=top_l.index, y=top_l)\nplt.axhline(np.mean(train.target))\nplt.xticks(rotation=80)\nplt.show()","6bf9d617":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n","9ee2d649":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","0c8dbc50":"import tokenization","f8fab564":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","2bba4f75":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","28cde316":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","6fd0aa84":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","3d636a96":"train_input = bert_encode(train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train.target.values","cfb0cb43":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","d5977bcc":"train_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    batch_size=16\n)\n\nmodel.save('model.h5')","5ac05dcb":"test_pred = model.predict(test_input)","9794b910":"sample_submission['target'] = test_pred.round().astype(int)\nsample_submission.to_csv('sample_submission.csv', index=False)","28030ef3":"sample_submission.head()","47b2132c":"# References\n***https:\/\/www.kaggle.com\/holfyuen\/basic-nlp-on-disaster-tweets\n\n****https:\/\/www.kaggle.com\/nkoprowicz\/a-very-simple-fine-tuned-bert-model\n\n****https:\/\/www.kaggle.com\/ratan123\/in-depth-guide-to-google-s-bert\n\n****https:\/\/www.kaggle.com\/ratan123\/in-depth-guide-to-google-s-bert?fbclid=IwAR3m3AL4vajTMr-IY2kIPPS-eFJJw7Lc9UqJ_nb5cmW5buliW5_qJlfpxgg","7b586b30":"##   Now Encoding the text into tokens, masks, and segment flags:","cbb58142":"### Location identification","d3138ee5":"# I am going to use BERT to identify the disaster and non-disaster for test problem set. \nCredits : This part was taken from these two (http:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub) and https:\/\/www.kaggle.com\/ratan123\/in-depth-guide-to-google-s-bert?fbclid=IwAR3m3AL4vajTMr-IY2kIPPS-eFJJw7Lc9UqJ_nb5cmW5buliW5_qJlfpxgg kernels please refer to these awesome kernels and consider upvoting.Thanks to the author of the kernel xhlulu","eab3a797":"<a href=\"#top\" class=\"btn btn-primary btn-lg active\" role=\"button\" aria-pressed=\"true\">Go to TOP<\/a>","b2328a0c":"###So, it is clear that train set have complete information. Target=1 means disaster and target=0 means non disaster. In test set target column is missing as it should be determind. Then the submission file should be included only Id and determined target for test data set.","fbbef2ca":"## Now lets build the model","b43cbf2f":"## Getting BERT from the Tensorflow Hub:","204ab1e8":"***  Helper Functions:","8b2f734e":"Agian import necessary modules","4892b957":"# Few kew words and large amount of location data is missing.","1b3aaf99":"### Taken tokenizer from the bert layer","7d156cb4":"###Understanding basic pattern of the data","26721f86":"##  Predict the target using developed model","00717046":"**Uncleared data indicates that Mumbai, India and nigeria are the top three disaster tweets location.","3c5353a6":"## Train the model","95ecee5b":"# Pick the tokenizer","672247e9":"# Both have same number of key words","1dbb3d65":"BERT stands for Bidirectional Encoder Representations from Transformers. It's a neural-network based technique for natural language process pretraining. BERT makes use of Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. In its vanilla form, Transformer includes two separate mechanisms \u2014 an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERT\u2019s goal is to generate a language model, only the encoder mechanism is necessary (https:\/\/towardsdatascience.com\/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270). "}}