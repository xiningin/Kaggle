{"cell_type":{"ede30194":"code","08ac4f13":"code","2db6746a":"code","cf495c1f":"code","b52082d7":"code","385aa143":"code","87d7a789":"code","16f822ca":"code","b4a55b05":"code","310b4e02":"code","fe06c882":"code","7ae36501":"code","31f68b01":"code","dbba1e6e":"code","2790342b":"markdown","d3c20b91":"markdown","c3baf53b":"markdown","a9296002":"markdown","4b2f01fb":"markdown","96faffbf":"markdown","cd5c7bbc":"markdown","352a6107":"markdown","5103eaae":"markdown","7ae430f7":"markdown","6a6a23f1":"markdown","e75beee0":"markdown","f80dd09b":"markdown"},"source":{"ede30194":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","08ac4f13":"train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\ntest.head()","2db6746a":"train.info()","cf495c1f":"test.info()","b52082d7":"train.head()","385aa143":"label = train.iloc[:,0].values\ntrain = train.iloc[:,1:].values\ntest = test.iloc[:,:].values","87d7a789":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()","16f822ca":"from sklearn.model_selection import cross_val_score\ncross_val_score(dtc,train,label,cv=10,scoring='accuracy').mean() ","b4a55b05":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()","310b4e02":"from sklearn.model_selection import cross_val_score\ncross_val_score(gnb,train,label,cv=10,scoring='accuracy').mean() ","fe06c882":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier(n_estimators=10,criterion='entropy')","7ae36501":"from sklearn.model_selection import cross_val_score\ncross_val_score(rfc,train,label,cv=10,scoring='accuracy').mean() ","31f68b01":"rfc.fit(train,label)\n\ny_pred = rfc.predict(test)","dbba1e6e":"output = pd.DataFrame({'ImageId': np.arange(1,len(y_pred)+1,1), 'Label': y_pred})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\") ","2790342b":"### 2.naive bayes","d3c20b91":"from sklearn.model_selection import cross_val_score\ncross_val_score(lr,train,label,cv=10,scoring='accuracy').mean() ","c3baf53b":"### 4.random forest","a9296002":"### 6.KNN","4b2f01fb":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(max_iter=3000)","96faffbf":"### 5.Logistic model","cd5c7bbc":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(metric='minkowski',p=2)","352a6107":"from sklearn.model_selection import cross_val_score\ncross_val_score(svc,train,label,cv=10,scoring='accuracy').mean() ","5103eaae":"### 3. SVM","7ae430f7":"#### Among all the models logistic classification, random forest are doing pretty good. naive bayes is doing worse. I will be choosing random forest for final evaluation. svm and knn are taking so long to run, i am not going in for them. since iam getting pretty good accuracy with the random forest model only(94%).","6a6a23f1":"from sklearn.model_selection import cross_val_score\ncross_val_score(knn,train,label,cv=10,scoring='accuracy').mean() ","e75beee0":"### 1.decission tree classifier","f80dd09b":"from sklearn.svm import SVC\nsvc = SVC(kernel = 'linear') "}}