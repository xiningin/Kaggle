{"cell_type":{"d9463ec8":"code","db18bd72":"code","cd1c7209":"code","049c6162":"code","f7aa9e8f":"code","4f2b6119":"code","ae72c43f":"code","5946b91c":"code","1474ba58":"code","eaa3139e":"code","1f6a085c":"code","4d75e4e3":"code","cacf05f5":"code","95f64ee0":"code","be5ab8f2":"code","93b7526d":"code","f5c8f521":"code","d8acc724":"code","f7bb8dfb":"code","e2591c3c":"code","b77aad2f":"code","92ed7e98":"code","b009f7cf":"code","bf3e4bd4":"code","c10abad8":"code","2a5f9341":"code","478ca4d2":"code","df21c98a":"code","9283da6a":"code","3d578edc":"code","357e5657":"code","cb52d473":"code","6771b55d":"code","7140655a":"code","d5b3a69e":"code","d329de22":"code","7d09bdd5":"code","097f5fca":"code","a522bfb4":"code","ce871e60":"code","606fe51a":"code","43207e8f":"code","8a93ab31":"code","4df7968b":"code","54f7105b":"code","e5949442":"code","00b257ec":"code","5eda98e9":"code","425b9abe":"code","de34d7b8":"code","64febf6a":"code","ef22e14f":"code","b1547eea":"code","6d0c66af":"code","337d1342":"code","b3bd17bb":"code","92eaaa11":"code","4241f5c8":"code","a0bbaf43":"code","73139fd9":"code","ae56cd18":"code","9b5ec89c":"code","fb6157b5":"code","a1c5e308":"code","1d4ef451":"code","2ab44811":"code","71b287ce":"code","b95b2ecb":"markdown","12f58d3c":"markdown","b2183565":"markdown","21c232c2":"markdown","a9b0c946":"markdown","97e1d69d":"markdown","f2b15ac1":"markdown","9910ee46":"markdown","81e4c1f0":"markdown","ba0202ad":"markdown","977cd6d5":"markdown","f0132e71":"markdown","b5cd528b":"markdown","5438e400":"markdown","65e484d6":"markdown","0341de97":"markdown","c69ed09f":"markdown","85d966e3":"markdown","3597785d":"markdown","a8ecb469":"markdown","4a789388":"markdown"},"source":{"d9463ec8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","db18bd72":"import numpy as np\nimport pandas as pd\nimport gc # \u0441\u0431\u043e\u0440\u0449\u0438\u043a \u043c\u0443\u0441\u043e\u0440\u0430\nimport math\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\nimport seaborn as sns\n%matplotlib inline","cd1c7209":"train = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\")\ntrain.head()","049c6162":"test = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\")\ntest.head()","f7aa9e8f":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n#                 if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n#                     df[col] = df[col].astype(np.float16)\n#                 elif\n\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","4f2b6119":"train = reduce_mem_usage(train)","ae72c43f":"test = reduce_mem_usage(test)","5946b91c":"train.shape","1474ba58":"test.shape","eaa3139e":"train.isna().sum().sum()","1f6a085c":"test.isna().sum().sum()","4d75e4e3":"train['Cover_Type'].value_counts()","cacf05f5":"train.loc[train['Cover_Type'] == 5, 'Cover_Type']","95f64ee0":"train.drop(labels = [3403875], axis = 0, inplace = True)","be5ab8f2":"for i in train.dtypes:\n      print(i)","93b7526d":"plt.figure(figsize = (15,10))\n\nsns.set(font_scale=1.4)\n\ncorr_matrix = train.corr()\n#print(X.corr())\ncorr_matrix = np.round(corr_matrix, 2)\ncorr_matrix[np.abs(corr_matrix) < 0.1] = 0  # \u041f\u0440\u043e\u0432\u0435\u0440\u044c\u0442\u0435, \u0447\u0442\u043e \u0431\u0443\u0434\u0435\u0442 \u0435\u0441\u043b\u0438 \u0443\u0431\u0440\u0430\u0442\u044c \u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0438\u0435 \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u0438\n\nsns.heatmap(corr_matrix, annot=True, linewidths=.5, cmap='coolwarm')\n\nplt.title('Correlation matrix')\nplt.show()","f5c8f521":"corr_with_target = train.corr().iloc[:-1, -1].sort_values(ascending=False)\n\nplt.figure(figsize=(10, 8))\n\nsns.barplot(x=corr_with_target.values, y=corr_with_target.index)\n\nplt.title('Correlation with target variable')\nplt.show()","d8acc724":"X = train.drop(['Cover_Type'], axis=1)\ny = train['Cover_Type']","f7bb8dfb":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = StandardScaler()\n# Fit on the training data\nscaler.fit(X)\n# Transform both the training and testing data\nX_standar = scaler.transform(X)\ntest_standar = scaler.transform(test)\nprint(X_standar)","e2591c3c":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Create the scaler object with a range of 0-1\nscaler = MinMaxScaler(feature_range=(0, 1))\n# Fit on the training data\nscaler.fit(X)\n# Transform both the training and testing data\nX_norm = scaler.transform(X)\ntest_norm = scaler.transform(test)\nprint(X_norm)","b77aad2f":"from imblearn.over_sampling import SMOTE, ADASYN, KMeansSMOTE\n\nos = SMOTE(random_state=0, sampling_strategy={1:1468136, 2:2262087, 3:500000, 4:50000, 6:50000, 7:100000})\nos1 = ADASYN(random_state=0, n_neighbors=1)\nos2 = KMeansSMOTE(random_state=0, k_neighbors=1, sampling_strategy='minority', kmeans_estimator=1)\n# feature vector\nX_train_full = train.drop(['Cover_Type'], axis=1) \n# target variable vector\ny_train_full = train['Cover_Type']\n\ncolumn = X_train_full.columns\n\nprint(\"\u0414\u043e \u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u043a\u0438\")\nprint(X_train_full.shape)\nprint(y_train_full.value_counts())\n\n# Let's apply the balancing algorithm\nos_data_X, os_data_y = os.fit_resample(X_train_full, y_train_full)\nos_data_X = pd.DataFrame(data=os_data_X, columns=column)\nos_data_y = pd.DataFrame(data=os_data_y, columns=['Cover_Type'])\n\nprint('_'*100)\nprint(\"\u041f\u043e\u0441\u043b\u0435 \u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u043a\u0438\")\nprint(os_data_X.shape)\nprint(os_data_y.value_counts())","92ed7e98":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) #X_norm","b009f7cf":"from sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(X_train, y_train)\ny_pred_test = svc.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred_test))","bf3e4bd4":"svc_f = SVC()\nsvc_f.fit(X, y)\ny_pred = svc_f.predict(test)","c10abad8":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=1000)\nclf.fit(X_train, y_train)\ny_pred_test = clf.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred_test))","2a5f9341":"clf_f = RandomForestClassifier(n_estimators=1000)\nclf_f.fit(X, y)\ny_pred = clf_f.predict(test)","478ca4d2":"import xgboost as xgb\n\nxgb1 = xgb.XGBClassifier(max_depth = 15, min_child_weight = 0.07, learning_rate = 0.01, tree_method='gpu_hist')\nxgb1.fit(X_train, y_train)\ny_pred_test = xgb1.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred_test))","df21c98a":"xgb_f = xgb.XGBClassifier(max_depth = 15, min_child_weight = 0.07, learning_rate = 0.01, tree_method='gpu_hist')\nxgb_f.fit(X, y)\ny_pred = xgb_f.predict(test)","9283da6a":"from catboost import CatBoostClassifier\n\ncatboost = CatBoostClassifier(task_type='GPU', n_estimators=40000, learning_rate=0.02, #\n                              #l2_leaf_reg = 2.5, \n                              depth = 9)#500 0.1  \ncatboost.fit(X_train, y_train)\ny_pred_test = catboost.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred_test))","3d578edc":"#catboost_f = CatBoostClassifier(n_estimators=15000, learning_rate=0.01, #l2_leaf_reg = 2.5, \n                              #depth = 5)#500 0.1 task_type='GPU', \n#catboost_f.fit(X, y)\ny_pred = catboost.predict(test)","357e5657":"from lightgbm import LGBMClassifier\n\nlogreg = LGBMClassifier(#max_depth=2,\n                      max_depth=11,#11 \n                      n_estimators=326,#326\n                      random_state=53,\n                      #objective = 'gamma',#gamma\n                      # min_data_in_leaf = 27)#27)\n)\nlogreg.fit(X_train, y_train)\ny_pred_test = logreg.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred_test))","cb52d473":"logreg_f = LGBMClassifier(#max_depth=2,\n                      max_depth=11,#11 \n                      n_estimators=326,#326\n                      random_state=53,\n                      #objective = 'gamma',#gamma\n                      # min_data_in_leaf = 27)#27)\n)\nlogreg_f.fit(X, y)\ny_pred = logreg_f.predict(test)","6771b55d":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\ngbc.fit(X_train, y_train)\ny_pred_test = gbc.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred_test))","7140655a":"gbc_f = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\ngbc_f.fit(X, y)\ny_pred = gbc_f.predict(test)","d5b3a69e":"from keras import models\nfrom keras import layers\nfrom keras.wrappers.scikit_learn import KerasClassifier","d329de22":"y_train","7d09bdd5":"y_train_n = y_train - 1\ny_test_n = y_test - 1\ny_train_n","097f5fca":"y_train_n.unique()","a522bfb4":"from sklearn.preprocessing import OneHotEncoder","ce871e60":"y_train_n = np.array(y_train_n).reshape((-1, 1))\n\nencoder = OneHotEncoder(categories='auto')\ny_train_n = encoder.fit_transform(y_train_n).toarray()\n\ny_train_n","606fe51a":"y_test_n = np.array(y_test_n).reshape((-1, 1))\n\nencoder = OneHotEncoder(categories='auto')\ny_test_n = encoder.fit_transform(y_test_n).toarray()\n\ny_test_n","43207e8f":"network = models.Sequential()\nnetwork.add(layers.Dense(units=20, activation=\"relu\", input_shape=(55,)))\nnetwork.add(layers.Dense(units=256, activation=\"relu\"))\n#network.add(layers.Dense(units=10, activation=\"relu\"))\nnetwork.add(layers.Dense(units=1024, activation=\"relu\"))\n#network.add(layers.Dense(units=1, activation=\"tanh\"))\nnetwork.add(layers.Dense(units=128, activation=\"relu\"))\nnetwork.add(layers.Dense(units=6, activation=\"relu\"))","8a93ab31":"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nearly_stopping = EarlyStopping(\n    monitor='accuracy', \n    min_delta=0, \n    patience=100, \n    verbose=0,\n    mode='min', \n    baseline=None, \n    restore_best_weights=True\n)\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='accuracy', \n    factor=0.2,\n    patience=200,\n    mode='min'\n)","4df7968b":"network.compile(\n    loss=\"categorical_crossentropy\",\n    optimizer=\"adam\",\n    metrics=[\"accuracy\"]\n)","54f7105b":"history = network.fit(\n    X_train,\n    y_train_n,\n    epochs=30,\n    verbose=1,\n    batch_size=100,\n    validation_data=(X_test, y_test_n),\n        callbacks=[\n            early_stopping,\n            reduce_lr,\n        ])","e5949442":"history = network.fit(\n    X,\n    y,\n    epochs=18,\n    verbose=1,\n    batch_size=100,\n        callbacks=[\n            early_stopping,\n            reduce_lr,\n        ])","00b257ec":"y_pred = network.predict(test)\ny_pred","5eda98e9":"import torch\nimport random\nimport numpy as np\n\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.backends.cudnn.deterministic = True","425b9abe":"X_train = X.to_numpy()\nX_train = torch.from_numpy(X_train).float()","de34d7b8":"y_train.loc[train['Cover_Type'] > 4] -= 1\ny_train['Cover_Type'] -= 1","64febf6a":"y_train = y.to_numpy()\ny_train = torch.from_numpy(y_train)","ef22e14f":"class ClassifierNet(torch.nn.Module):\n\n    def __init__(self, n_hidden_neurons):\n        super(ClassifierNet, self).__init__()\n\n        self.fc1 = torch.nn.Linear(54, n_hidden_neurons)\n        self.ac1 = torch.nn.ReLU()\n        self.batch_norm1 = torch.nn.BatchNorm1d(n_hidden_neurons)\n\n        self.fc2 = torch.nn.Linear(n_hidden_neurons, n_hidden_neurons)\n        self.ac2 = torch.nn.ReLU()\n        self.batch_norm2 = torch.nn.BatchNorm1d(n_hidden_neurons)\n\n        self.fc3 = torch.nn.Linear(n_hidden_neurons, n_hidden_neurons)\n        self.ac3 = torch.nn.ReLU()\n        self.batch_norm3 = torch.nn.BatchNorm1d(n_hidden_neurons)\n\n        self.fc4 = torch.nn.Linear(n_hidden_neurons, n_hidden_neurons \/\/ 2)\n        self.ac4 = torch.nn.ReLU()\n        self.batch_norm4 = torch.nn.BatchNorm1d(n_hidden_neurons \/\/ 2)\n\n        self.fc5 = torch.nn.Linear(n_hidden_neurons \/\/ 2, n_hidden_neurons \/\/ 4)\n        self.dr = torch.nn.Dropout(p=0.5)\n        self.ac5 = torch.nn.ReLU()\n        self.batch_norm5 = torch.nn.BatchNorm1d(n_hidden_neurons \/\/ 4)\n\n        self.fc6 = torch.nn.Linear(n_hidden_neurons \/\/ 4, 7)\n    \n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.ac1(x)\n        x = self.batch_norm1(x)\n        x = self.fc2(x)\n        x = self.ac2(x)\n        x = self.batch_norm2(x)\n        x = self.fc3(x)\n        x = self.ac3(x)\n        x = self.batch_norm3(x)\n        x = self.fc4(x)\n        x = self.ac4(x)\n        x = self.batch_norm4(x)\n        x = self.fc5(x)\n        x = self.ac5(x)\n        x = self.batch_norm5(x)\n        x = self.fc6(x)\n        \n        return x\n\nnet = ClassifierNet(128)","b1547eea":"torch.cuda.is_available()","6d0c66af":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nnet = net.to(device) ","337d1342":"loss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=1.0e-3)","b3bd17bb":"batch_size = 500000\n\nfor epoch in range(1000):\n    order = np.random.permutation(len(X_train))\n    for start_index in range(0, len(X_train), batch_size):\n        optimizer.zero_grad()\n        \n        batch_indexes = order[start_index:start_index+batch_size]\n        \n        X_batch = X_train[batch_indexes].to(device)\n        y_batch = y_train[batch_indexes].to(device)\n        \n        preds = net.forward(X_batch)\n        \n        loss_value = loss(preds, y_batch)\n        loss_value.backward()\n        \n        optimizer.step()\n    if epoch % 100 == 0:\n        print(epoch, (preds.argmax(dim=1) == y_batch).float().mean())","92eaaa11":"test_n = test.to_numpy()\ntest_n = torch.from_numpy(test_n).float()\ntest_n = test_n.to(device)","4241f5c8":"y_pred = net.forward(test_n)","a0bbaf43":"y_pred = y_pred.argmax(dim=1)","73139fd9":"y_pred = y_pred.cpu()","ae56cd18":"y_pred += 1\ny_test[y_test > 4] += 1","9b5ec89c":"indexs = [i for i in range(4000000, 5000000)]","fb6157b5":"y_pred = y_pred.tolist()","a1c5e308":"y_pred","1d4ef451":"for i in range(len(y_pred)):\n    y_pred[i] = y_pred[i][0]  ","2ab44811":"d = {'id': indexs, 'Cover_Type': y_pred}\ntest_pred = pd.DataFrame(data=d)\ntest_pred","71b287ce":"test_pred.to_csv(r'submission11.csv', index=None)","b95b2ecb":"### Balancing data","12f58d3c":"# Removing trash","b2183565":"### Catboost","21c232c2":"### Forming an answer","a9b0c946":"### Gradient boosting","97e1d69d":"### Correlations","f2b15ac1":"### Balancing classes","9910ee46":"# Reading Data","81e4c1f0":"### Choosing a model","ba0202ad":"### Data types","977cd6d5":"# One more neural network","f0132e71":"### Logistic regression","b5cd528b":"### Extreme Gradient Boosting","5438e400":"### Random Forest","65e484d6":"# Training and model selection","0341de97":"### Support Vector Machines","c69ed09f":"### Normalization and standardization","85d966e3":"### Neural network","3597785d":"### Removing the most unrepresentative class","a8ecb469":"### Data gaps","4a789388":"### Data dimension"}}