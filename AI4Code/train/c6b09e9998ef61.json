{"cell_type":{"2ee27c00":"code","e48c7ec1":"code","99e75852":"code","b446d1ed":"code","b97c5e05":"code","28a8e050":"code","cec03403":"code","2ef0df70":"code","acb5db0d":"code","27540fd7":"code","b437a77a":"code","e58e2661":"code","5ea61cb8":"code","85cc3e70":"code","7e538573":"code","47b88f96":"code","b463b4e6":"code","d2166f14":"code","8727118d":"code","c58b9b0d":"code","32fd7bd6":"code","6a84fa24":"code","5955c009":"code","f8fafb70":"code","61ce93e1":"code","ca21b9fd":"code","b1eb466d":"code","4e1a1459":"code","2805a686":"code","2d4e6250":"code","3e943b51":"code","f9c17b89":"code","94bd9aba":"code","ace80e78":"code","fb38125e":"code","830cc3ee":"code","8cc36e74":"code","135ad7a3":"code","375a3476":"code","33e81a47":"code","34179133":"code","a95d3fc7":"code","05690c6f":"code","393c1e93":"code","571dd77a":"code","14d24637":"code","042ec974":"code","980d0a19":"code","ff2a3f48":"code","5548fdfb":"code","7eba06c2":"code","73f095b7":"markdown","ed85bd6a":"markdown","0bfa38b8":"markdown","6e6b9bf7":"markdown","4a0e230f":"markdown","4e9db133":"markdown","ceb97b5a":"markdown","3522e58c":"markdown","ddd1aefc":"markdown","e24e3c4d":"markdown","a44da00e":"markdown"},"source":{"2ee27c00":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook\nfrom zipfile import ZipFile\nimport os\nimport cv2\nfrom matplotlib import cm\nimport matplotlib.pyplot as plt\n\nfrom keras.models import Sequential\nfrom keras_preprocessing.image import ImageDataGenerator\nfrom keras.layers import Activation, Dropout, Conv2D, GlobalMaxPooling2D, MaxPooling2D, Flatten, Dense, BatchNormalization\nfrom keras.optimizers import Adam, SGD\nfrom sklearn.utils import compute_class_weight\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.regularizers import l2\n\nfrom keras_tqdm import TQDMNotebookCallback\nfrom keras.callbacks import ModelCheckpoint","e48c7ec1":"#! ls ..\/input","99e75852":"#!rm -rf ..\/input\/trainedprocessed","b446d1ed":"base_path = \"..\/input\/\"\ntrainProcessed = \"trainedprocessed\"\naptop = 'aptos2019-blindness-detection\/'\nlocalModel = 'trained-on-local\/'\ntrain = \"train_images\"\nIMG_DIM = 96\nSEED = 6819","b97c5e05":"data_df = pd.read_csv(\"..\/input\/aptos2019-blindness-detection\/train.csv\")","28a8e050":"data_df['id_code'] = data_df['id_code']+\".png\" \ndata_df['diagnosis'] = data_df['diagnosis'].astype(\"str\")","cec03403":"data_df.head()","2ef0df70":"def crop_image_from_gray_sample(img,tol=7):\n    \"\"\"\n    Crop out black borders\n    https:\/\/www.kaggle.com\/ratthachat\/aptos-updated-preprocessing-ben-s-cropping\n    \"\"\" \n    if img.ndim ==2:\n        mask = img>tol\n        return img[np.ix_(mask.any(1),mask.any(0))]\n    elif img.ndim==3:\n        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        mask = gray_img>tol        \n        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n        if (check_shape == 0):\n            return img\n        else:\n            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n            img = np.stack([img1,img2,img3],axis=-1)\n        return img","acb5db0d":"imgs = cv2.imread(base_path+aptop+\"train_images\/000c1434d8d7.png\")\nimgs = crop_image_from_gray_sample(imgs)\nimgs = cv2.resize(imgs, (IMG_DIM, IMG_DIM))\nimgs = cv2.addWeighted(imgs,4, cv2.GaussianBlur(imgs, (0,0), 10), -4, 128)","27540fd7":"#Experiment\nplt.imshow(imgs)","b437a77a":"def crop_image_from_gray(img,tol=7):\n    \"\"\"\n    Crop out black borders\n    https:\/\/www.kaggle.com\/ratthachat\/aptos-updated-preprocessing-ben-s-cropping\n    \"\"\"  \n    if img.ndim ==2:\n        mask = img>tol\n        return img[np.ix_(mask.any(1),mask.any(0))]\n    elif img.ndim==3:\n        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        mask = gray_img>tol        \n        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n        if (check_shape == 0):\n            return img\n        else:\n            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n            img = np.stack([img1,img2,img3],axis=-1)\n        return img","e58e2661":"def pre_process(img):\n    img = crop_image_from_gray((img).astype(np.uint8))\n    img = cv2.resize(img, (IMG_DIM, IMG_DIM))\n    img = cv2.addWeighted(img,4, cv2.GaussianBlur(img, (0,0), 10), -4, 128)\n    return img.astype(np.float64)","5ea61cb8":"try:\n    os.mkdir(base_path+trainProcessed)\n    print(\"Directory Created :\"+trainProcessed)\n    \n    for file in tqdm_notebook(data_df[\"id_code\"].iteritems(), total=data_df.shape[0]):\n        img = cv2.imread(base_path+aptop+train+\"\/\"+file[1])\n        img = crop_image_from_gray(img)\n        img = cv2.resize(img, (400, 400))\n        img = cv2.addWeighted(img,4, cv2.GaussianBlur(img, (0,0), 10), -4, 128)\n        cv2.imwrite(base_path+trainProcessed+\"\/\"+file[1], img)\n    print(\"Conversion done.\")\nexcept:\n    print(\"Directory \"+trainProcessed+\" already exist !\")","85cc3e70":"batch_size=8\nepochs = 20","7e538573":"data_gen = ImageDataGenerator(vertical_flip=True,\n                             horizontal_flip=True,\n                             rescale=1.\/255, \n                             #preprocessing_function=pre_process,\n                             data_format='channels_last')","47b88f96":"tempGen = data_gen.flow_from_dataframe(dataframe=data_df, \n                                       directory=base_path+trainProcessed,\n                                       x_col=\"id_code\",\n                                       y_col=\"diagnosis\",\n                                       target_size=(IMG_DIM,IMG_DIM),\n                                       class_mode='categorical',\n                                       batch_size=batch_size,\n                                       seed=SEED)","b463b4e6":"tx, ty = next(tempGen)","d2166f14":"fig = plt.figure(figsize=(100, 100))\ncolumns = 4\nrow = 2\nfor i in range(1, columns*row+1):\n    fig.add_subplot(row, columns, i)\n    #plt.imshow((tx[i-1].reshape((400, 400))*255).astype(np.int32))\n    plt.imshow((tx[i-1]*255).astype(np.uint8))\nplt.show()","8727118d":"train_df = data_df.sample(frac=0.8, random_state=SEED)\nvalidate_df = data_df.loc[~data_df['id_code'].isin(train_df['id_code'].values)]","c58b9b0d":"len(train_df), len(validate_df)","32fd7bd6":"train_df.reset_index(inplace=True, drop=True)\nvalidate_df.reset_index(inplace=True, drop=True)","6a84fa24":"train_generator = data_gen.flow_from_dataframe(dataframe=train_df, \n                                       directory=base_path+trainProcessed,\n                                       x_col=\"id_code\",\n                                       y_col=\"diagnosis\",\n                                       target_size=(IMG_DIM,IMG_DIM),\n                                       class_mode='categorical',\n                                       batch_size=batch_size,\n                                       seed=SEED)\nvalidation_generator = data_gen.flow_from_dataframe(dataframe=validate_df, \n                                       directory=base_path+trainProcessed,\n                                       x_col=\"id_code\",\n                                       y_col=\"diagnosis\",\n                                       target_size=(IMG_DIM,IMG_DIM),\n                                       class_mode='categorical',\n                                       batch_size=batch_size,\n                                       seed=SEED)","5955c009":"X, y = next(train_generator)","f8fafb70":"X.shape, y.shape","61ce93e1":"class_weights = compute_class_weight(\"balanced\", \n                     np.unique(train_generator.classes), \n                     train_generator.classes)\nclass_weights","ca21b9fd":"model_resnet = ResNet50(include_top=False, \n                        weights = None,\n                        input_shape=(IMG_DIM, IMG_DIM, 3,))\nmodel_resnet.summary()","b1eb466d":"model = Sequential()\nmodel.add(model_resnet)\n\n#model.add(Flatten())\nmodel.add(GlobalMaxPooling2D())\nmodel.add(Dense(1000, activation='relu', activity_regularizer=l2(0.001)))\nmodel.add(Dense(500, activation='relu', activity_regularizer=l2(0.001)))\n#model.add(Dense(250, activation='relu', activity_regularizer=l2(0.001)))\n#model.add(BatchNormalization())\n#model.add(Dropout(0.5))\nmodel.add(Dense(5, activation = 'softmax'))","4e1a1459":"model.summary()","2805a686":"model.load_weights(base_path+localModel+\"model_resnet_aptos_0.2.hdf5\")","2d4e6250":"model.compile(loss='categorical_crossentropy',\n              #optimizer=SGD(lr=0.00001),\n              optimizer=Adam(lr=0.00001),\n              metrics=['accuracy'])","3e943b51":"filepath=\"weights-resnet-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n#filepath=\"weights.best.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint, TQDMNotebookCallback()]\n#callbacks_list = [TQDMNotebookCallback()]","f9c17b89":"train_step = train_df.size\/\/batch_size\nvalid_step = validate_df.size\/\/batch_size\nhistory = model.fit_generator(train_generator,\n                    validation_data=validation_generator,\n                    epochs=epochs, \n                    steps_per_epoch=train_step, \n                    validation_steps=valid_step, \n                    shuffle= True,\n                    workers=6, \n                    verbose=2,\n                    use_multiprocessing=False,\n                    #callbacks=callbacks_list, \n                    initial_epoch=0,\n                    class_weight=class_weights)","94bd9aba":"model.save('model_resnet_aptos_kaggle.hdf5')","ace80e78":"test = \"test_images\/\"\ntestProcessed = \"testProcessed\"","fb38125e":"test_df = pd.read_csv(base_path+aptop+\"test.csv\")","830cc3ee":"test_df['id_code'] = test_df['id_code']+\".png\"","8cc36e74":"test_df.head()","135ad7a3":"#!ls ..\/input\n#!rm -rf  ..\/input\/testProcessed","375a3476":"try:\n    os.mkdir(base_path+testProcessed)\n    print(\"Directory Created :\"+testProcessed)\n    \n    for file in tqdm_notebook(test_df[\"id_code\"].iteritems(), total=test_df.shape[0]):\n        img = cv2.imread(base_path+aptop+test+\"\/\"+file[1])\n        img = crop_image_from_gray(img)\n        img = cv2.resize(img, (IMG_DIM, IMG_DIM))\n        img = cv2.addWeighted(img,4, cv2.GaussianBlur(img, (0,0), 10), -4, 128)\n        cv2.imwrite(base_path+testProcessed+\"\/\"+file[1], img)\n    print(\"Conversion done.\")\nexcept:\n    print(\"Directory \"+testProcessed+\" already exist !\")","33e81a47":"test_datagen = ImageDataGenerator(rescale=1.\/255, \n                                  #preprocessing_function=pre_process,\n                                  data_format=\"channels_last\")","34179133":"testgen = test_datagen.flow_from_dataframe(dataframe=test_df,\n                                       directory=base_path+aptop+test, \n                                       target_size=(IMG_DIM,IMG_DIM), \n                                       x_col=\"id_code\", \n                                       batch_size=batch_size, \n                                       shuffle=False, \n                                       class_mode=None, \n                                       seed=SEED)","a95d3fc7":"testgen.reset()\nSTEP_TEST_GEN = test_df.size\/\/batch_size\npred = model.predict_generator(testgen,\n                               steps=STEP_TEST_GEN,\n                               verbose=1)","05690c6f":"pred_class_indices = np.argmax(pred, axis=1)","393c1e93":"labels = (train_generator.class_indices)","571dd77a":"labels","14d24637":"labels = (train_generator.class_indices)\nlabels = dict((c,v) for v, c in labels.items())\nprediction = [labels[k] for k in pred_class_indices]","042ec974":"filenames = testgen.filenames","980d0a19":"results = pd.DataFrame({\"id_code\": filenames, \n          \"diagnosis\": prediction})","ff2a3f48":"results.head()","5548fdfb":"results[\"id_code\"] = results[\"id_code\"].str.rstrip(\".png\")","7eba06c2":"results.to_csv(\"submission.csv\", index=False)","73f095b7":"# End","ed85bd6a":"# model","0bfa38b8":"# Data conversion","6e6b9bf7":"# Train validate generator","4a0e230f":"# imports","4e9db133":"### Reference\nhttps:\/\/www.kaggle.com\/ratthachat\/aptos-updatedv14-preprocessing-ben-s-cropping","ceb97b5a":"# Class Weight","3522e58c":"# Data Generator","ddd1aefc":"# Extract images","e24e3c4d":"# Test","a44da00e":"# Sample Image"}}