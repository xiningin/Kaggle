{"cell_type":{"c812cfa0":"code","938ad9d6":"code","f2b563d9":"code","7299aabd":"code","8e98bb2b":"code","ae076547":"code","37c90396":"code","b4386c8a":"code","cb19b071":"code","7ae8cc9e":"code","b3202ef5":"code","6056641f":"code","a467ddc8":"code","a10141db":"code","0c03ded6":"code","9f10b541":"code","237da72d":"code","b7fa095b":"code","2b301d99":"code","acc1d4f7":"code","e85cdb7d":"code","05347a46":"code","d736883c":"code","09955f8a":"code","96ddb259":"code","80adf1b0":"code","5cf51596":"code","5d74c0df":"code","343f35c1":"code","48528893":"code","6725b5a7":"code","c58738f1":"code","06ecb490":"code","f00e0b3f":"code","ed1d0e00":"code","21434b23":"code","85a2898a":"code","9219a0c7":"code","9d115495":"code","de36b3c1":"code","128d0959":"code","3475afae":"code","f36bb79b":"code","224182c7":"code","2bbbc3d8":"code","0a7e9af0":"code","8e158b08":"code","282ccc33":"code","6d6771b1":"code","7e8e81bb":"code","d0be659b":"code","252d8bd8":"code","d6b8aeea":"code","459a4c3c":"code","01b0e9a1":"code","25f1bfff":"code","1cee8768":"markdown","c2169f89":"markdown","6c473fe6":"markdown","84748577":"markdown","462be13f":"markdown","b86f05c8":"markdown","c667d221":"markdown","290957ae":"markdown","aa29c88f":"markdown","fbc3a4cb":"markdown","2de2564e":"markdown","7deaf092":"markdown","0b69a5b9":"markdown","529421d2":"markdown","37d5482a":"markdown","e8dc1607":"markdown","de741075":"markdown","f072ab3a":"markdown","c8ba92ee":"markdown","84d4f687":"markdown","701bebe3":"markdown","26df4c55":"markdown","87b0288d":"markdown"},"source":{"c812cfa0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# For data visualization\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nprint(\"Setup complete.\")","938ad9d6":"# Load the training dataset and look at the first few records.\ntrain_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nprint(train_df.shape)\ntrain_df.head()","f2b563d9":"# Load the test dataset and look at the first few records.\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nprint(test_df.shape)\ntest_df.head()","7299aabd":"# Look at summary statistics for the training dataset.\ntrain_df.describe()","8e98bb2b":"# Look at the number of passengers in the training set who survived.\ntrain_df['Survived'].value_counts()","ae076547":"# Look at the percentage of passengers in the training set who survived.\ntrain_df['Survived'].value_counts(normalize=True)","37c90396":"no_survivors_df = pd.DataFrame(test_df['PassengerId'])\nno_survivors_df['Survived'] = 0\nno_survivors_df.to_csv('no_survivors.csv', index=False)","b4386c8a":"# Look at summary statistics for the test dataset.\ntest_df.describe()","cb19b071":"# Count the missing values in each column of the training dataset.\ntrain_df.isnull().sum()","7ae8cc9e":"# Count the missing values in each column of the test dataset.\ntest_df.isnull().sum()","b3202ef5":"# Chart the survival rate by sex\nplt.title(\"Survival Rate by Sex\")\nax = sns.barplot(x=train_df['Sex'], y=train_df['Survived'], ci=None)\nax.bar_label(ax.containers[0])\n\n# Add label for vertical axis\nplt.ylabel(\"Survival Rate\")","6056641f":"train_df['Sex'].value_counts()","a467ddc8":"women_survived_df = no_survivors_df.copy()\nwomen_survived_df['Sex'] = test_df['Sex']\nwomen_survived_df.loc[women_survived_df['Sex'] == 'female', 'Survived'] = 1\nwomen_survived_df = women_survived_df.drop('Sex', axis=1)\nwomen_survived_df.to_csv('women_survived.csv', index=False)","a10141db":"# plot a histogram of the ages of passengers\nsns.histplot(train_df['Age'].dropna(), kde=False, bins=40)","0c03ded6":"train_df['Age'].describe()","9f10b541":"# Add a Child column with a default value of 0, set it to 1 if Age < 18\ntrain_df['Child'] = 0\ntrain_df.loc[train_df['Age'] < 18, 'Child'] = 1\ntrain_df","237da72d":"# How many survivors among men, women, and children?\ntrain_df.groupby(['Sex', 'Child'])['Survived'].sum()","b7fa095b":"# How many people were in each group, men, women, children?\ntrain_df.groupby(['Sex', 'Child'])['Survived'].count()","2b301d99":"# What is the proportion of survivors in each Sex and Age group?\ntrain_df.groupby(['Sex', 'Child']).apply(lambda x: x['Survived'].sum()\/len(x))","acc1d4f7":"train_df['Fare2'] = '30+'\ntrain_df.loc[train_df['Fare'] < 30, 'Fare2'] = '20-30'\ntrain_df.loc[train_df['Fare'] < 20, 'Fare2'] = '10-20'\ntrain_df.loc[train_df['Fare'] < 10, 'Fare2'] = '<10'\ntrain_df","e85cdb7d":"# What is the proportion of survivors in each Sex, Class, and Fare group?\ntrain_df.groupby(['Fare2', 'Pclass', 'Sex']).apply(lambda x: x['Survived'].sum()\/len(x))","05347a46":"train_df['SibSp'].value_counts()","d736883c":"# Chart the survival rate by SibSp\nplt.title(\"Survival Rate by Siblings\/Spouses Aboard\")\nax = sns.barplot(x=train_df['SibSp'], y=train_df['Survived'], ci=None)\nax.bar_label(ax.containers[0])\n\n# Add label for vertical axis\nplt.xlabel(\"Siblings\/Spouses\")\nplt.ylabel(\"Survival Rate\")","09955f8a":"train_df.groupby(['SibSp']).Age.agg([len, 'min', 'max', 'mean', 'median'])","96ddb259":"train_df['Pclass'].value_counts()","80adf1b0":"# Chart the survival rate by Pclass\nplt.title(\"Survival Rate by Passenger Class\")\nax = sns.barplot(x=train_df['Pclass'], y=train_df['Survived'], ci=None)\nax.bar_label(ax.containers[0])\n\n# Add label for vertical axis\nplt.xlabel(\"Passenger Class\")\nplt.ylabel(\"Survival Rate\")","5cf51596":"train_df.groupby(['Pclass']).Age.agg([len, 'min', 'max', 'mean', 'median'])","5d74c0df":"train_df['Parch'].value_counts()","343f35c1":"# Chart the survival rate by Parch\nplt.title(\"Survival Rate by Parents\/Children aboard\")\nax = sns.barplot(x=train_df['Parch'], y=train_df['Survived'], ci=None)\nax.bar_label(ax.containers[0])\n\n# Add label for vertical axis\nplt.xlabel(\"Parents\/Children\")\nplt.ylabel(\"Survival Rate\")","48528893":"train_df.groupby(['Parch']).Age.agg([len, 'min', 'max', 'mean', 'median'])","6725b5a7":"train_df['Embarked'].value_counts()","c58738f1":"# Select feaures to base the model on.\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\nX = train_df.loc[: ,features]\ny = train_df['Survived']\n\n# Scikit-learn doesn't like categorical features as strings, so we'll encode them as numbers.\nX['Sex'] = X['Sex'].map( {'male':1, 'female':0} )\n\n# Remember we had a couple of missing values in the Embarked column.\n# We'll fill those with 'S', the most common value.\nX['Embarked'] = X['Embarked'].fillna('S')\n\n# Age also had several missing values. Fill those with the average age for each passenger class.\nX.loc[X['Age'].isnull() & (X['Pclass'] == 1), 'Age'] = 38.2\nX.loc[X['Age'].isnull() & (X['Pclass'] == 2), 'Age'] = 29.9\nX.loc[X['Age'].isnull() & (X['Pclass'] == 3), 'Age'] = 25.1\n# X['Age'] = X['Age'].fillna(28.0)\n\nfrom sklearn.preprocessing import LabelEncoder\n\nemb_encoder = LabelEncoder()\nemb_encoder.fit(X['Embarked'])\nX['Embarked'] = emb_encoder.transform(X['Embarked'])\nX","06ecb490":"# import the libraries needed from scikit-learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n# split data into training and validation data, for both features and target\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n\n# create and fit the model\nmodel = DecisionTreeClassifier(random_state = 0)\nmodel.fit(train_X, train_y)\n\n# get predictions based on validation data\nval_predictions = model.predict(val_X)\n\n# compute the accuracy of the predictions\naccuracy = accuracy_score(val_y, val_predictions)\naccuracy","f00e0b3f":"train_df['Cabin'] = train_df['Cabin'].fillna('Unknown')\ntrain_df.groupby(['Cabin']).apply(lambda x: x['Survived'].sum()\/len(x))","ed1d0e00":"train_df['n_Cabins'] = train_df.apply(lambda row: len(row['Cabin'].split()), axis=1)\ntrain_df","21434b23":"train_df.groupby(['n_Cabins', 'Sex']).apply(lambda x: x['Survived'].sum()\/len(x))","85a2898a":"train_df['CabinPrefix'] = train_df.apply(lambda row: row['Cabin'][0], axis=1)\ntrain_df","9219a0c7":"train_df.groupby(['CabinPrefix', 'Sex']).apply(lambda x: x['Survived'].sum()\/len(x))","9d115495":"def split_ticket(ticket):\n    # special case. a few Tickets are only LINE\n    if ticket == 'LINE':\n        return pd.Series(['LINE', 0])\n    \n    parts = ticket.split()  # split the ticket on whitespace\n    if len(parts) == 1:\n        return pd.Series([\"NO_PREFIX\", int(parts[0].strip(' .'))])\n    elif len(parts) == 2:\n        return pd.Series([parts[0].strip(' .'), int(parts[1].strip(' .'))])\n    else:\n        # special case. One Ticket has a prefix separated by a space.\n        return pd.Series([parts[0].strip(' .') + parts[2].strip(' .'), int(parts[2].strip(' .'))])\n\ntrain_df[['Ticket_Prefix', 'Ticket_NUM']] = train_df.apply(lambda row: split_ticket(row['Ticket']), axis=1)\ntrain_df","de36b3c1":"train_df.groupby(['Ticket_Prefix']).apply(lambda x: x['Survived'].sum()\/len(x))","128d0959":"train_df['Title'] = train_df.apply(lambda row: row['Name'].split()[1], axis=1)\ntrain_df","3475afae":"train_df.groupby(['Title']).apply(lambda x: x['Survived'].sum()\/len(x))","f36bb79b":"# Select feaures to base the model on.\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', \n            'Embarked', 'n_Cabins', 'CabinPrefix', 'Ticket_Prefix', 'Ticket_NUM', 'Title']\nX = train_df.loc[: ,features]\ny = train_df['Survived']\n\n# Scikit-learn doesn't like categorical features as strings, so we'll encode them as numbers.\nX['Sex'] = X['Sex'].map( {'male':1, 'female':0} )\n\n# Remember we had a couple of missing values in the Embarked column.\n# We'll fill those with 'S', the most common value.\nX['Embarked'] = X['Embarked'].fillna('S')\n\n# Age also had several missing values. Fill those with the average age for each passenger class.\nX.loc[X['Age'].isnull() & (X['Pclass'] == 1), 'Age'] = 38.2\nX.loc[X['Age'].isnull() & (X['Pclass'] == 2), 'Age'] = 29.9\nX.loc[X['Age'].isnull() & (X['Pclass'] == 3), 'Age'] = 25.1\n# X['Age'] = X['Age'].fillna(28.0)\n\nemb_encoder = LabelEncoder()\nemb_encoder.fit(X['Embarked'])\nX['Embarked'] = emb_encoder.transform(X['Embarked'])\n\ncab_encoder = LabelEncoder()\ncab_encoder.fit(X['CabinPrefix'])\nX['CabinPrefix'] = cab_encoder.transform(X['CabinPrefix'])\n\ntic_encoder = LabelEncoder()\ntic_encoder.fit(X['Ticket_Prefix'])\nX['Ticket_Prefix'] = tic_encoder.transform(X['Ticket_Prefix'])\n\ntitle_encoder = LabelEncoder()\ntitle_encoder.fit(X['Title'])\nX['Title'] = title_encoder.transform(X['Title'])\nX","224182c7":"# split data into training and validation data, for both features and target\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=0)\n\n# create and fit the model\nmodel = DecisionTreeClassifier(random_state=0)\nmodel.fit(train_X, train_y)\n\n# get predictions based on validation data\nval_predictions = model.predict(val_X)\n\n# compute the accuracy of the predictions\naccuracy = accuracy_score(val_y, val_predictions)\naccuracy","2bbbc3d8":"# Let's see how a random forest classifier does on the same data.\nfrom sklearn.ensemble import RandomForestClassifier\n\n# create and fit the model\nmodel = RandomForestClassifier(random_state=0)\nmodel.fit(train_X, train_y)\n\n# get predictions based on validation data\nval_predictions = model.predict(val_X)\n\n# compute the accuracy of the predictions\naccuracy = accuracy_score(val_y, val_predictions)\naccuracy","0a7e9af0":"# Let's see how a gradient boost classifier does on the same data.\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# create and fit the model\nmodel = GradientBoostingClassifier(random_state=0)\nmodel.fit(train_X, train_y)\n\n# get predictions based on validation data\nval_predictions = model.predict(val_X)\n\n# compute the accuracy of the predictions\naccuracy = accuracy_score(val_y, val_predictions)\naccuracy","8e158b08":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nprint(train_df.shape)\ntrain_df.head()","282ccc33":"# Set aside the test PassengerId values for later.\npassenger_ids = test_df['PassengerId']\n\n# Add a fake value for Survived that we'll remove again later.\ntest_df['Survived'] = np.NaN\nprint(test_df.shape)\ntest_df.head()","6d6771b1":"combo_df = train_df.append(test_df)\nprint(combo_df.shape)","7e8e81bb":"# Add the engineered features to the combined dataset\ncombo_df['Cabin'] = combo_df['Cabin'].fillna('Unknown')\ncombo_df['n_Cabins'] = combo_df.apply(lambda row: len(row['Cabin'].split()), axis=1)\ncombo_df['CabinPrefix'] = combo_df.apply(lambda row: row['Cabin'][0], axis=1)\ncombo_df[['Ticket_Prefix', 'Ticket_NUM']] = combo_df.apply(lambda row: split_ticket(row['Ticket']), axis=1)\ncombo_df['Title'] = combo_df.apply(lambda row: row['Name'].split()[1], axis=1)\ncombo_df.describe()","d0be659b":"# Select feaures to base the model on.\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', \n            'Embarked', 'n_Cabins', 'CabinPrefix', 'Ticket_Prefix', 'Ticket_NUM', 'Title', 'Survived']\ncombo_df = combo_df[features]\n\n# Scikit-learn doesn't like categorical features as strings, so we'll encode them as numbers.\ncombo_df['Sex'] = combo_df['Sex'].map( {'male':1, 'female':0} )\n\n# Remember we had a couple of missing values in the Embarked column.\n# We'll fill those with 'S', the most common value.\ncombo_df['Embarked'] = combo_df['Embarked'].fillna('S')\n\n# Age also had several missing values. Fill those with the average age for each passenger class.\ncombo_df.loc[combo_df['Age'].isnull() & (combo_df['Pclass'] == 1), 'Age'] = 38.2\ncombo_df.loc[combo_df['Age'].isnull() & (combo_df['Pclass'] == 2), 'Age'] = 29.9\ncombo_df.loc[combo_df['Age'].isnull() & (combo_df['Pclass'] == 3), 'Age'] = 25.1\n\n# There was one missing Fare. Fill it with the median fare.\ncombo_df['Fare'] = combo_df['Fare'].fillna(14.45)\n\nemb_encoder = LabelEncoder()\nemb_encoder.fit(combo_df['Embarked'])\ncombo_df['Embarked'] = emb_encoder.transform(combo_df['Embarked'])\n\ncab_encoder = LabelEncoder()\ncab_encoder.fit(combo_df['CabinPrefix'])\ncombo_df['CabinPrefix'] = cab_encoder.transform(combo_df['CabinPrefix'])\n\ntic_encoder = LabelEncoder()\ntic_encoder.fit(combo_df['Ticket_Prefix'])\ncombo_df['Ticket_Prefix'] = tic_encoder.transform(combo_df['Ticket_Prefix'])\n\ntitle_encoder = LabelEncoder()\ntitle_encoder.fit(combo_df['Title'])\ncombo_df['Title'] = title_encoder.transform(combo_df['Title'])\n\ncombo_df.head()","252d8bd8":"# Split the combined dataframe back into train and test data.\ntrain_df = combo_df[combo_df['Survived'].notna()]\nprint(train_df.shape)\ntrain_df.describe()","d6b8aeea":"test_df = combo_df[combo_df['Survived'].isna()]\ntest_df = test_df.drop('Survived', axis=1)\nprint(test_df.shape)\ntest_df.describe()","459a4c3c":"# Split training features and target\ntrain_y = train_df['Survived']\ntrain_X = train_df.drop('Survived', axis=1)\n\n# create and fit the model\nmodel = DecisionTreeClassifier(random_state = 0)\nmodel.fit(train_X, train_y)\n\n# get predictions based on test data\ny_pred = model.predict(test_df)\n\nfinal_dt_predictions_df = pd.DataFrame({'PassengerId': passenger_ids, 'Survived': y_pred}, dtype=int)\nfinal_dt_predictions_df.to_csv('final_DT_predictions.csv', index=False)","01b0e9a1":"# create and fit the model\nmodel = RandomForestClassifier(random_state = 0)\nmodel.fit(train_X, train_y)\n\n# get predictions based on test data\ny_pred = model.predict(test_df)\n\nfinal_rf_predictions_df = pd.DataFrame({'PassengerId': passenger_ids, 'Survived': y_pred}, dtype=int)\nfinal_rf_predictions_df.to_csv('final_RF_predictions.csv', index=False)","25f1bfff":"# create and fit the model\nmodel = GradientBoostingClassifier(random_state = 0)\nmodel.fit(train_X, train_y)\n\n# get predictions based on test data\ny_pred = model.predict(test_df)\n\nfinal_gb_predictions_df = pd.DataFrame({'PassengerId': passenger_ids, 'Survived': y_pred}, dtype=int)\nfinal_gb_predictions_df.to_csv('final_GB_predictions.csv', index=False)","1cee8768":"For the three largest groups (SibSp 0-2) there is a very wide range of ages, so the average age is not much different from the overall average. This makes sense, since there are mostly unrelated people. The other groups are a little bit more interesting. These are larger families travelling together, so it makes sense that their ages would be a little bit closer together.\n\nLet's look at the same breakdowns for the Parch (Number of Parents\/Children Aboard) and Pclass (Passenger Class) features.","c2169f89":"The majority of females (child or adult) survive and the majority of males still did not, so there's nothing here to base a change in the model on.\n\nLet's take a quick look at the Class and Fare features to see how the survival rate changes with these variables. Class only has 3 values, but we'll need to bin the Fares to make them more managable.","6c473fe6":"Anyone who's watched the film Titanic knows that (mostly) women and children got to go in the lifeboats before the men, so this breakdown shouldn't be too surprising. But how many women were onboard the Titanic compmared to men?","84748577":"As we noted earlier, there are missing Age values in both the training and test data sets. About 20% of the values are missing in both cases. For starters, we could fill these values with the average age of all passengers whose age we do know. The mean and the median are about the same in the training dataset (29.7 and 28 years), so it probably won't matter much which one we choose. Either one would be considered an adult.","462be13f":"This solution performs very differently on the test dataset than it did on the training split. We'll include a random forest model as well as gradient boost to see how much better they do.","b86f05c8":"Over 84% with no hyper-parameter tuning is not bad at all. Let's tie all of these together into separate submission files to see how they perform on the test data.\n\n## Final Submission\n\nI'll start by reloading the training data set, since some columns were added and others were modified in earlier cells. I'll combine it with the test dataset so we can do all of the feature engineering steps on both datasets at once, then separate them again.","c667d221":"There were nearly twice as many men as women, but that's still quite a large chunk of the data where we guessed that the passengers died when we know that they probably didn't. Let's update our model with this new insight to guess that all of the women survived.","290957ae":"We're missing a lot of values for Age and Cabin in both datasets. We're missing the value for where two passengers embarked from in the training set, and one passenger's fare in the test dataset. We'll come back to these later. For now, let's look at the survival rate for passengers based on their sex.","aa29c88f":"That gives us better than 78% accuracy. (Remember this is an estimate based on the training data. Your results might be slightly different if you train this model on the entire data set and create a new submission.) That's close to the goal, but not quite there yet. Let's keep refining our model to get to 80% or better.\n\n## Feature Engineering\n\nThere are three columns we haven't included in any model yet, Name, Ticket, and Cabin. We said that Cabin had a lot of missing values, but maybe we can still extract some information from it. Let's fill in the missing values and aggregate the data to see if this is the case.","fbc3a4cb":"A notebook for exploring the Titanic dataset and trying out a few different models. I very loosely followed the first few parts of Trevor Stephens' [Titanic: Getting Started With R](https:\/\/trevorstephens.com\/kaggle-titanic-tutorial\/getting-started-with-r\/), but using Python and Pandas instead of R.\n\nSome other helpful resources were:\n- [Data Visualization](https:\/\/www.kaggle.com\/learn\/data-visualization)\n- [Intro to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)\n- [Feature Engineering](https:\/\/www.kaggle.com\/learn\/feature-engineering)\n- [Stack Overflow](https:\/\/stackoverflow.com\/) when I got stuck\n- The [Scikit-Learn API Reference](https:\/\/scikit-learn.org\/stable\/modules\/classes.html)","2de2564e":"If you submit that file, you'll see a score of 0.622, just as expected. This verifies that the training data is a good represenative sample of the whole population. An accuracy of 0.622 is better than random guessing, but it's still pretty bad. I think we can do better.\n\n## Improving on the baseline","7deaf092":"Now let's look at the aggregates of Fare, Class, and Age to see if any of the survival rates change.","0b69a5b9":"That's a lot of different prefixes, but it looks like there's potentially a lot of good information there as well.\n\nLet's now take a look at the Name feature. You wouldn't think there would be a lot of predictive value in a name, since everyone's name is (almost) unique, but notice that most of the names in this dataset include a title (Mr., Mrs., Miss, etc.). There are also severl more unique titles that indicate more than just the Sex column and marital status.\n\nLet's write another function to extract a passenger's Title from their Name.","529421d2":"## Decision Trees\n\nWe'll start by creating a model based on a subset of features from the training dataset. We'll first split the dataset to estimate the accuracy instead of creating a new submission right away. This way we can keep training new models until we find one that gives us a significantly higher score than before. We scored 76.5% with our \"women survived\" model, so let's try to get to 80% before we submit again.","37d5482a":"## A baseline model\nSince only 38% of people in the training dataset survived, a good \"baseline\" model would be to guess that all passengers in the test dataset died. That should give us an accuracy of around 62%.","e8dc1607":"It looks like there is some signal here, so we'll want to keep these new columns when we train our next model.\n\nNow let's move on to the ticket. It looks like every ticket has an optional prefix, followed by a space, followed by a number. There are also a couple of special cases that we can handle. We can write a function to apply to every row to split the Ticket value into two columns.","de741075":"That gets us to 81% (estimated) accuracy, which is over the goal with a little room to spare.","f072ab3a":"The accuracy jumped to 76.5% just by looking at one feature of the dataset! That's a huge improvement, but I think we can do much better. Let's go back to the Age column to see if that offers any further insights.","c8ba92ee":"For the most part, nothing changes from the \"women survived\" model, with one exception. Most of the women in 3rd class who paid more than $20 for their ticket did not survive. We could generate a new submission based on this insight, but let's wait. We'll start training machine learning models and let them learn this rule instead.\n\nBefore we do that, let's look at the distribution of the 'SibSp' column. This column indicates the number of Siblings\/Spouses each passenger has onboard.","84d4f687":"It looks like people travelling with only one other person were more likely to survive. That makes sense, since we already know that women were a lot more likely to survive than men. The half of the pairs of people who travelled with a spouse who survived were probably the wives.\n\nLet's look at one more thing related to SibSp before moving on. How does the average age change in each category?","701bebe3":"It looks like the large majority of passengers were travelling alone, with another large group that were travelling with only one other person, probably their spouse. Let's see how this effected survival rates.","26df4c55":"It looks like we extracted a few values that are not real titles, but this simple function seems to have worked fairly well. Let's ad these new columns to our feature set and train a new model. I'll repeat the encoding steps here so they're all in one place.","87b0288d":"Looking at a list of cabins, a couple of interesting ideas come to mind. Each cabin has a letter prefix A-G, and some passengers have more than one cabin. We might be able to extract these values to come up with more useful features."}}