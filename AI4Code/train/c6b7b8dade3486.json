{"cell_type":{"635a1c65":"code","be91fd83":"code","f547dc27":"code","bb1f98cb":"code","67adcb7d":"code","3c16cd99":"code","857bd846":"code","4345c079":"code","ba25739f":"code","795e75d8":"code","d7a18a2e":"code","24b3c215":"code","8b72d77c":"code","70cca4af":"code","47aae37b":"code","cff33969":"code","a13c88ad":"code","b7c51c13":"code","9b472ab1":"markdown","f64b392f":"markdown","a7d377a5":"markdown","3f00a3f9":"markdown"},"source":{"635a1c65":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')","be91fd83":"df = pd.read_csv(\"..\/input\/insurance\/insurance.csv\")","f547dc27":"## Data Info\ndf.info()","bb1f98cb":"df.head().T","67adcb7d":"## Missing Values\ndf.isnull().sum()","3c16cd99":"## Target Variable\n### Distribution\nsns.displot(df.charges, kde = True, color = \"b\")\nplt.show()","857bd846":"for column in df.columns:\n    if df[column].dtype == 'O':\n        print(column)\n        print(df[column].value_counts(), \"\\n\\n\")","4345c079":"## Feature Engineering\n\nfrom sklearn.preprocessing import LabelEncoder\n\nsex_map = {'male':1, 'female':0}\ndf['sex'] = df['sex'].map(sex_map).astype('int64')\n\nsmoker_map = {'yes':1, 'no':0}\ndf['smoker'] = df['smoker'].map(smoker_map).astype('int64')\n\nLE = LabelEncoder()\ndf['region'] = LE.fit_transform(df['region'])\n# Southeast region makes highest expense so let region southeast = 2 and others are 1\n# df['region'] = df['region'].replace(('southeast', 'southwest', 'northwest', 'northeast'), (2, 1, 1, 1))\n# df['region'].value_counts()","ba25739f":"print(df.shape[0])\ndf = df.dropna()\n\nprint(df.shape[0])","795e75d8":"from sklearn.preprocessing import LabelEncoder\n\nsex_map = {'male':1, 'female':0}\ndf['sex'] = df['sex'].map(sex_map)\n\nsmoker_map = {'yes':1, 'no':0}\ndf['smoker'] = df['smoker'].map(smoker_map)\n\nLE = LabelEncoder()\ndf['region'] = LE.fit_transform(df['region'])\n# Southeast region makes highest expense so let region southeast = 2 and others are 1\n# df['region'] = df['region'].replace(('southeast', 'southwest', 'northwest', 'northeast'), (2, 1, 1, 1))\n# df['region'].value_counts()","d7a18a2e":"from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score","24b3c215":"X = df.drop(columns = 'charges').values\ny = df['charges'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size  = 0.2, \n                                                    random_state=0)","8b72d77c":"print('Size of x_train = ', X_train.shape)\nprint('Size of x_test  = ', X_test.shape)\nprint('Size of y_train = ', y_train.shape)\nprint('Size of y_test  = ', y_test.shape)","70cca4af":"## Feature Scaling\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","47aae37b":"from sklearn.metrics import r2_score, mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\n## Model Selection\nmodels = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('RF', RandomForestRegressor()))\nmodels.append(('GBR', GradientBoostingRegressor()))\n\n## Model Evaluation\nresults = []\nnames = []\nfor name, model in models:\n    fit_model = model.fit(X_train, y_train)\n    y_pred = fit_model.predict(X_test)\n\n    r2 = r2_score(y_test, y_pred)\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n    \n    ## Cross Validation\n    cv = cross_val_score(model, X, y, cv = 7)\n\n    results.append((r2_score, rmse))\n    names.append(name)\n    print(\"Cross Validation - Reported accuracy should not have high variance\")\n    print(cv)\n    print()\n    print('{}:R2 {}% Accuracy - RMSE: Predicted Values +\/-{}'.format(name, (round(r2, 3)*100), rmse))","cff33969":"### Hyper Parameter Tuning\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'loss':['ls', 'lad', 'huber', 'quantile'],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'learning_rate':[0.05, 0.1, 0.2],\n    'max_depth':[1, 2, 10, 150],\n    'n_estimators':[100, 150, 500, 750, 1000]\n}\n\nGBR = GradientBoostingRegressor()\nGBR_cv = GridSearchCV(estimator = GBR, param_grid = param_grid, verbose = 1)\nGBR_cv.fit(X_train, y_train)\n\nparams = GBR_cv.best_params_\nprint(params)","a13c88ad":"### Pipeline\n# from sklearn.pipeline import make_pipeline\n\nmodel = GradientBoostingRegressor(learning_rate = params['learning_rate'], \n                                  loss=params['loss'], \n                                  max_features = params['max_features'])\nmodel.fit(X_train, y_train)","b7c51c13":"def evaluate_model(model, X_test, y_test, modelName, DataImb):\n    print('------------------------------------------------')\n    print(\"Model \", modelName, end=\"\\n\")\n    print(\"Data Balancing Type \", DataImb)\n    ### Model must be ran outside the function\n    y_pred = model.predict(X_test)\n    r2 = r2_score(y_test, y_pred)\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n    print(\"R2 Score\", r2)\n    print(\"RMSE\", rmse)\n    return[modelName, DataImb, r2, rmse]\n\nevaluate_model(model, X_test, y_test, 'Gradient Boosting Regressor', \"Auctual Data\")","9b472ab1":"Dataset: *Machine Leearning with R* Brett Lantz, with a region set in the USA. ","f64b392f":"Not much imporvement from our original accuracy, however the GBRegressor returns the r squared as ~ 90% and Root Mean Squared Error of 3998.0224.\n\nBetter Tuning methods and inputs can still be explored.","a7d377a5":"## Feature Engineering","3f00a3f9":"Linear Regression, RandomForestRegressor and Gradient Boosting Regressor models give increasingly reliable results in the same order.\n\nThe GradientBoost returns the r squared as 89.6% and Root Mean Squared Error of 4063.9423, this can be interpreted as an almost 90% accuracy of this model and a +\/- of 4024.6516 the predicted values from the auctual values.\n\nNow we tune the parameters of the best performing Regressor."}}