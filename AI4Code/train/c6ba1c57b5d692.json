{"cell_type":{"c3c45fe9":"code","71ae8816":"code","cbf821e4":"code","0054cd20":"code","bfd0971f":"code","1ce60863":"code","f1ccb3bc":"code","bed6093b":"code","ac85acd6":"code","69f51d8b":"code","11fec208":"code","77696eac":"code","ece04447":"code","609b4728":"code","0f3b69d9":"code","00e6f031":"markdown","e690d1db":"markdown","1f97e21c":"markdown","1a0ac38c":"markdown","b0db86ad":"markdown","6b9ecdb5":"markdown","124209d3":"markdown","f9e538cf":"markdown","65a560f2":"markdown","45569761":"markdown","6deed71e":"markdown","898c3d11":"markdown","25867b63":"markdown","387fc029":"markdown"},"source":{"c3c45fe9":"import pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nfrom random import randint\nfrom wordcloud import WordCloud\nfrom collections import Counter","71ae8816":"df_raw = pd.read_csv('..\/input\/data.csv',encoding=\"ISO-8859-1\")\nprint(df_raw.shape)","cbf821e4":"df_raw.dropna(axis = 0, subset = ['Description'], inplace = True)\nprint('Dataframe dimensions:', df_raw.shape)\nprint(df_raw['Description'].head())\nprint(df_raw.shape)","0054cd20":"df_desc = pd.DataFrame(df_raw['Description'].unique()).rename(columns = {0:'Description'})\nprint(df_desc.shape)\nprint(df_desc)","bfd0971f":"# explore the punctuations in the data\nfind_dict={}\nnewp='\"#$%&\\'()*+-\/:;<=>@[\\]^_`{|}~.?'\nfor i, row in df_desc.iterrows():\n    for c in newp:\n        #print(c)\n        #print(row)\n        if c in row['Description']:\n            val=find_dict.get(c,'')\n            find_dict[c]=val+'|'+row['Description']\n            #print('find:',c)\n           # print(row['Description'])\n#print(list(find_dict))","1ce60863":"def punc_processing(st):\n    for i,c in enumerate(list(st)):\n        if c == '\\'':\n            #clean example like 'n'\n            if i==0:\n                #print(st)\n                st_clean=st[1:(len(st)-1)]\n                #print(st_clean)\n                return st_clean\n            #clean example like b'fly\n            if i==1:\n                #print(st)\n                st_clean=st[2:]\n                #print(st_clean)\n                return st_clean\n            #clean example like mother's\n            if i!=0 and st[i-1]!=' ':\n                #print(st)\n                st_clean=st[:(i)]+st[(i+2):]\n                #print(st_clean)\n                return st_clean\n        if c == '\"':\n            #clean example like \"glamorous\"\n            if i==0:\n                #print(st)\n                st_clean=st[1:(len(st)-1)]\n                #print(st_clean)\n                return st_clean        \n    return st","f1ccb3bc":"newp='[\"&\\'()+-\/[\\].]'\nfront_quotation='[\\\"\\'(]'\nend_quotation='[\\\"\\')]'\nconnect_quotation='&\/+,'\ntoker = RegexpTokenizer('[a-z]+'+newp+'[a-z]+|[a-z]+|'+front_quotation+'[a-z]+'+end_quotation)\nwordnet_lemmatizer=WordNetLemmatizer()\nstopWords = set(stopwords.words('english'))\nstopWords.update(['small', 'large', 'jumbo', 'set', 'pink', 'blue', 'tag', 'red', 'white'])\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer(\"english\")\n\ndf_desc['token']=None\ndf_desc['token_list']=None\n\nfor i, row in df_desc.iterrows():\n    descp_st=row['Description'].lower()\n    for quot in connect_quotation:\n        descp_st=descp_st.replace(quot, ' '+quot+' ')\n    #Tokenization\n    descp_l=toker.tokenize(descp_st)\n    #Punctuation pre-processing\n    descp_l=[punc_processing(x) for x in descp_l]\n    #Stop words removal\n    descp_l2=[x for x in descp_l if x not in stopWords ]\n    \n    #test if use steming to pre-process\n    #descp_l2=[stemmer.stem(x) for x in descp_l2]\n    \n    #Lematization\n    descp_l2=[wordnet_lemmatizer.lemmatize(x) for x in descp_l2]\n    df_desc.loc[i,'token_list']=descp_l2\n    df_desc.loc[i, 'token']=' '.join(descp_l2)","bed6093b":"#test if stop word or words mislead clustering correctly\nprint('Number of common stop words:', len(stopwords.words('english')))\nprint('Example of common stop words(not all stop words listed):', stopwords.words('english')[:30], '\\n')\nprint('Number of unwanted words in these sample:', len(['small', 'large', 'jumbo', 'set', 'pink', 'blue', 'tag', 'red', 'white']))\nprint('Example of unwanted words in these sample:', ['small', 'large', 'jumbo', 'set', 'pink', 'blue', 'tag', 'red', 'white'])","ac85acd6":"#test if punctation are filttered correctly\nfind_dict={}\nnewp='\"#$%&\\'()*+-\/:;<=>@[\\]^_`{|}~.?'\nfor i, row in df_desc.iterrows():\n    for c in newp:\n        if c in row['Description']:\n            #print('Punctuation:',c)\n            #print('Original description:',row['Description'])\n            #print('After pre-processing:',row['token_list'], '\\n')\n            newp.replace(c, '')\n            break","69f51d8b":"#test if stopwords filttered correctly\n# col='OF'\n# for i, row in df_desc.iterrows():\n#     if col in row['Description']:\n#         print('Remove the word:',col)\n#         print(row['Description'])\n#         print(row['token_list'])\n#         print('\\n')","11fec208":"#test if lematize correctly\n# col='AGED'\n# for i, row in df_desc.iterrows():\n#     if col in row['Description']:\n#         print('Lematize the word:',col)\n#         print(row['Description'])\n#         print(row['token_list'])\n#         print('\\n')","77696eac":"print(df_desc[['Description','token_list']])\nvectorizer = CountVectorizer(min_df=1)\n\ndata_desc_doc = vectorizer.fit_transform(df_desc['token'])\nfeature_name = vectorizer.get_feature_names()\n\nprint('Number of words appeared in corpus:', len(feature_name))\nprint('Example of words appeared in corpus(not all listed):',feature_name[:30], '\\n')\nprint(df_desc.head(1))\nprint(data_desc_doc)","ece04447":"print(data_desc_doc.toarray())","609b4728":"def findBestN(matrix):\n    for n in range(3,15):\n        kmeans = KMeans(n_clusters = n, n_init=20, random_state=0 )\n        kmeans.fit(matrix)\n        clusters = kmeans.predict(matrix)\n        silhouette_avg = silhouette_score(matrix, clusters)\n        print(\"For n_clusters =\", n, \"The average silhouette_score is :\", silhouette_avg)\n        \nfindBestN(data_desc_doc)","0f3b69d9":"best_no_of_cluster= 12\n#k means clustering\nkmeans = KMeans(n_clusters = best_no_of_cluster, n_init=20, random_state=0 )\nkmeans.fit(data_desc_doc)\nkm_result=kmeans.predict(data_desc_doc)\ndf_desc['cluster_group']=pd.Series(km_result)\n\ndef desinate_color(word=None, position=None,orientation=None,font_size=None,  font_path=None, random_state=0):\n    h = randint(rand_tone*3,rand_tone*3)\n    s = randint(90,100)\n    l = randint(40,60)\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n\nfig = plt.figure(1, figsize=(20,20))\nfor a in range(0,best_no_of_cluster,1):    \n    df_temp=df_desc[df_desc['cluster_group']==a]\n    #print(a,' size:', df_temp.shape)\n    #print(df_temp)\n    \n    c = Counter()\n    rand_tone=a\n    for i, row in df_temp.iterrows():\n        c.update(row['token_list'])       \n    wordcloud = WordCloud(width=1000,height=400, background_color='lightgrey', color_func = desinate_color, relative_scaling=0.15, random_state=0)\n    wordcloud.generate_from_frequencies(c)\n    axis_1 = fig.add_subplot(4,3,(a+1))\n    axis_1.imshow(wordcloud)\n    axis_1.axis('off')\n    plt.title('Cluster n={}'.format(a+1))","00e6f031":"<b>Data exploration<\/b>****","e690d1db":"<b>Tokenization & Punctuation pre-processing & Stop words removal & Lematization<\/b>","1f97e21c":"Above are to print the result to test if pre-processing is correct","1a0ac38c":"<h1>K-means clustering<\/h1>","b0db86ad":"Only unqiue decriptions are filtered.","6b9ecdb5":"Description are tokenized into a list of tokens. During tokenization, words with punctuations are processed so that most semantic meaning remains.","124209d3":"<b> K-means clustering using 12 clustered group  <\/b>","f9e538cf":"<b>Testing if pre-processing succed<\/b>","65a560f2":"K-means clustering using n=12 is used. Resulted is generated in form of word cloud.","45569761":"<b>Vectorization as bag-of-words<\/b>","6deed71e":"Vectorization is applied to each row of data","898c3d11":"K-means clustering using different number of n is done. The result of greatest silhouette score is used as it gives best performance of clustering.","25867b63":"<h1>Data preprocessing<\/h1>","387fc029":"<b> Metrics evaluation using silhouette score <\/b>"}}