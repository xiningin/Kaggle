{"cell_type":{"20723140":"code","248c8618":"code","756162dd":"code","2b794880":"code","aec7f62c":"code","8247db89":"code","000ae1ff":"code","3b8c399b":"code","521fe1ab":"code","b622900f":"code","dd1d86ed":"code","d504abf8":"markdown","58fc84c5":"markdown","a526f309":"markdown","0dccc509":"markdown","5f714c2d":"markdown","20fb87e6":"markdown"},"source":{"20723140":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","248c8618":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords","756162dd":"nltk.download('punkt')\nnltk.download('stopwords')","2b794880":"df_train = pd.read_csv(\"\/kaggle\/input\/https-www-kaggle-com-c-2021-ml-tp5\/train.csv\",encoding=\"latin-1\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/https-www-kaggle-com-c-2021-ml-tp5\/test.csv\",encoding=\"latin-1\")\ndf_train.Data","aec7f62c":"X_train = df_train[\"Data\"]\ny_train=df_train[\"Class\"]\nX_test=df_test[\"Data\"]","8247db89":"# ------------------------------------------------\n# [Empty Module #1] \ud14d\uc2a4\ud2b8 \ub370\uc774\ud130 \uc804\ucc98\ub9ac\n# ------------------------------------------------\nfrom nltk.corpus import stopwords \nfrom nltk.stem.porter import PorterStemmer\n\ndef data_processing(text):\n    # ------------------------------------------------------------\n    # \uad6c\ud604 \uac00\uc774\ub4dc\ub77c\uc778 \n    # ------------------------------------------------------------\n    # [1] re.sub \uc0ac\uc6a9\ud574 text \uc18d '[^A-Za-z]' \uc678\uc758 \ubb38\uc790\ub9cc\uc744 \ucc3e\uc544\ub0b4 \uc81c\uac70\ud55c\ud6c4, pre_words \ubcc0\uc218\uc5d0 \uc800\uc7a5\n    #      1) pattern\uc740 '[^A-Za-z]', repl=' ' \ub85c \uac01\uac01 \uc124\uc815.\n    #      2) \uc774\ubaa8\uc9c0\ub098 \uc22b\uc790,\uc810\uacfc \uac19\uc740 \ubb38\uc790\uc678\uc758 \uac83\ub4e4\uc744 \uc81c\uac70\ud588\ub2e4. (\uc774\ubaa8\uc9c0\ub294 \uac10\uc815 \ubd84\uc11d\uacfc \uad00\ub828\ud574\uc11c \uba87\uac00\uc9c0 \uc758\ubbf8\ub97c \ub098\ud0c0\ub0b4\uc9c0\ub9cc \uc774 \ud14c\uc2a4\ud06c\uc5d0\uc11c\ub294 \uc774\ub7ec\ud55c \uc758\ubbf8\ub3c4 \uc81c\uac70\ud568.)\n    #\n    # [2] pre_words\uc758 lower \ub0b4\uc7a5 \ud568\uc218\ub97c \uc774\uc6a9\ud574 \ub300\ubb38\uc790\ub4e4\uc740 \uc18c\ubb38\uc790\ub85c \ubcc0\uacbd\n    #      1)  \ub300, \uc18c\ubb38\uc790\uac00 \uad6c\ubd84\ub418\uc5b4 \uc788\uc73c\uba74 \"Go\"\uc640 \"go\" \uc640 \uac19\uc774 \ub3d9\uc77c\ud55c \ub2e8\uc5b4\ub97c \uba38\uc2e0\uc740 \ub2e4\ub978 \ub2e8\uc5b4\ub85c \ucde8\uae09\ud55c\ub2e4. \ub530\ub77c\uc11c \ub300\ubb38\uc790\ub97c \ubaa8\ub450 \uc18c\ubb38\uc790\ub85c \ubcc0\uacbd.\n    #\n    # [3] word_tokenize \ud568\uc218\ub97c \uc774\uc6a9\ud574 pre_word \ub97c \ud1a0\ud070\ud654\ud558\uc5ec word\ub97c \ub9ac\uc2a4\ud2b8\ud654\ud55c \ud6c4 tokenized_words\ubcc0\uc218\uc5d0 \uc800\uc7a5\n    #\n    # [4] nltk \ub77c\uc774\ube0c\ub7ec\ub9ac\ub85c \ub2e4\uc6b4 \ubc1b\uc740 stopwords\uc758 \"words\" \ub0b4\uc7a5 \ud568\uc218\ub97c \uc774\uc6a9\ud574 english \ubd88\uc6a9\uc5b4\ub97c \ucc3e\uc544\uc11c stops \ubcc0\uc218\uc5d0 \uc800\uc7a5  \n    #      1) \ubd88\uc6a9\uc5b4: \ud14d\uc2a4\ud2b8 \ubd84\ub958\uc5d0\uc11c \ubd88\uc6a9\uc5b4\ub294 \ud14d\uc2a4\ud2b8\uc758 \uc911\uc694\ub3c4\uc744 \uacb0\uc815\ud558\ub294\ub370 \uc601\ud5a5\uc744 \ubbf8\uce58\uc9c0 \uc54a\ub294 \ub2e8\uc5b4\uc784. \n    #                    (ex: the, we, a , will), \ub530\ub77c\uc11c \ubd88\ud544\uc694\ud55c \ub2e8\uc5b4\uac00 \uc608\uce21 \ubaa8\ub378\uc5d0 \uc545\uc601\ud5a5\uc744 \ub07c\uce60 \uc218 \uc788\uae30 \ub54c\ubb38\uc5d0 \uc81c\uac70.\n    #\n    # [5] [3] \uc5d0\uc11c \ucc3e\uc740 \ubb38\uc790\uc5f4 \uc911 \ub2e8\uc5b4\uac00 [4] \uc5d0\uc11c \ucc3e\uc740 \ubd88\uc6a9\uc5b4 \uc18d\uc5d0 \uc5c6\uc744 \uacbd\uc6b0, tokenized_words_remove \ub9ac\uc2a4\ud2b8\uc5d0 append \n    #\n    # [6] PorterStemmer \uc18d stem \ub0b4\uc7a5 \ud568\uc218\ub97c \uc774\uc6a9\ud574 \ub3d9\uc77c \uc758\ubbf8\ub97c \uac16\ub294 \ub2e8\uc5b4\ub97c \ub3d9\uc77c\ud55c \ub2e8\uc5b4\ub85c \ubcc0\uacbd\ud558\ub294 \uacfc\uc815\uc744 \uac70\uce5c \ud6c4 \ub2e4\uc2dc \uc800\uc7a5.\n    #    \n    # ------------------------------------------------------------\n    ##############\n    \n    # [1]\n    pre_words =???\n    ##############\n   \n    # [2]\n    pre_words = ???\n    ##############\n    \n    # [3]\n    tokenized_words = ???\n    ##############\n    \n    # [4]\n    stops =???\n    \n    ##############\n    tokenized_words_remove=[]\n    \n    for w in tokenized_words: \n        # [5]\n        ???\n        ##############\n    \n    stemmer = PorterStemmer()\n    for i in range(len(tokenized_words_remove)):\n        # [6]\n        ???\n        ##############\n    \n    return ( \" \".join( tokenized_words_remove ))","000ae1ff":"X_train=[data_processing(i) for i in X_train]\nX_test=[data_processing(i) for i in X_test]","3b8c399b":"# ------------------------------------------------\n# [Empty Module #2] \ud14d\uc2a4\ud2b8 \ub370\uc774\ud130 Bag of word  feature  \ud654 \n# ------------------------------------------------\nfrom sklearn.feature_extraction.text import CountVectorizer\n# ------------------------------------------------------------\n# \uad6c\ud604 \uac00\uc774\ub4dc\ub77c\uc778 \n# ------------------------------------------------------------\n# [1]  CountVectorizer\ub97c \uc815\uc758 \n#           1) max_features\ub97c 100\uc73c\ub85c \uc9c0\uc815 \n# [2] X_train \uacfc X_test\ub97c numpy array\ub85c \ubcc0\ud658 \ud6c4 \ub370\uc774\ud130 \ud0c0\uc785\uc744 \"U\"\ub85c \ubcc0\uacbd\ud574 \uc800\uc7a5\n#\n# [3] CountVectorizer\ub97c \uc774\uc6a9\ud574 X_train\uc740 \ud559\uc2b5 \ubc0f \ubcc0\ud658\uc744 \ud558\uace0, X_test\ub294 \ubcc0\ud658\uc744 \uc9c4\ud589. \n# ------------------------------------------------------------\n###########\nfrom sklearn.feature_extraction.text import CountVectorizer\n# [1]\nvectorizer = ???\n             \n# [2]\n\n# [3]\nX_train_features = ???\nX_test_features = ???","521fe1ab":"y_train[y_train==\"ham\"]=0\ny_train[y_train==\"spam\"]=1\ny_train=y_train.astype(\"uint8\")","b622900f":"# ------------------------------------------------\n# [Empty Module #3] \ud14d\uc2a4\ud2b8 \ub370\uc774\ud130 Bag of word  feature  \ud654 \n# ------------------------------------------------\nfrom sklearn.svm import SVC\n# ------------------------------------------------------------\n# \uad6c\ud604 \uac00\uc774\ub4dc\ub77c\uc778 \n# ------------------------------------------------------------\n# [1]  SVC \uc815\uc758 \n#           1) gamma=\"auto\" \uc0ac\uc6a9 \n# [2] X_train_features\uacfc y_train\uc73c\ub85c SVC \ud559\uc2b5\uc9c4\ud589 \ud6c4, X_test_features\ub85c predict \uc9c4\ud589\n#\n# ------------------------------------------------------------\n###########\n# [1]\nsvc=???\n# [2]\n\ny_pred=???","dd1d86ed":"df_pred={\"ID\": range(np.array(y_pred).shape[0]),\"Class\":y_pred}\ndf_pred=pd.DataFrame(df_pred)\ndf_pred.to_csv(\"predict.csv\",index=False)","d504abf8":"# [Empty Module #1] \ud14d\uc2a4\ud2b8 \ub370\uc774\ud130 \uc804\ucc98\ub9ac  \n\n\ubaa9\ud45c: \ud14d\uc2a4\ud2b8 \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\uae30 \uc704\ud55c \uc5ec\ub7ec \uacfc\uc815\uc744 \uac70\uccd0, \uba38\uc2e0\uc744 \uc704\ud55c \ub370\uc774\ud130\ub97c \ub9cc\ub4e0\ub2e4. \n\n```\n[input]\n--------------\n- text: \ud14d\uc2a4\ud2b8 \ubb38\uc7a5 \ub370\uc774\ud130 \n\n[output]\n--------------\n- text: \uc804\ucc98\ub9ac\ub97c \uc644\ub8cc\ud55c \ubb38\uc7a5 \ub370\uc774\ud130 \n    \n```","58fc84c5":"# Predict to CSV","a526f309":"# \ubc18\ub4dc\uc2dc \ucc98\uc74c\ubd80\ud130 \ub05d\uae4c\uc9c0 \uc2a4\ucf08\ub808\ud1a4 \ucf54\ub4dc\ub97c \uc0b4\ud3b4\ubcf4\uace0 \uad6c\ud604\ud558\uae30 \uc2dc\uc791\ud558\uae38 \ubc14\ub780\ub2e4\n\n## 1. \uc2a4\ucf08\ub808\ud1a4 \ucf54\ub4dc\ub97c [\ubcf5\uc0ac \ubc0f \ud3b8\uc9d1] \ud558\uc5ec \uc0ac\uc6a9\ud55c\ub2e4.\n## 2. \uc544\ub798\uc758 [Empty Module 3\uac1c]\ub97c \uc9c1\uc811 \uad6c\ud604\ud55c\ub2e4.\n","0dccc509":"- nltk \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0\uc11c punkt \ub370\uc774\ud130\ub97c \ub2e4\uc6b4 \ubc1b\uc74c, \uc774 \ub370\uc774\ud130\ub294 \uc601\ud654 \ub9ac\ubdf0\uc640 \uac19\uc740 \ubb38\uc11c \ub370\uc774\ud130\ub85c \ubb38\uc790\uc758 tokeninizer\ub97c \uc704\ud574\uc11c \ud544\uc694\ud558\ub2e4\n- nltk \ub77c\uc774\ube0c\ub7ec\ub9ac\ub97c \uc774\uc6a9\ud574\uc11c \ubd88\uc6a9\uc5b4\ub97c \ub2e4\uc6b4 \ubc1b\uc74c","5f714c2d":"# [Empty Module #2] Bag of Word \n\n\ubaa9\ud45c: \ubb38\uc7a5 \ub370\uc774\ud130\ub97c \uba38\uc2e0\uc744 \ud559\uc2b5\ud558\uae30 \uc704\ud55c \uc2e4\uc218\ud654\ub41c Feature\ub85c \ubcc0\uacbd\ud558\uae30\ub85c\ud55c\ub2e4. \n\n- train \uacfc test \ub370\uc774\ud130 \uc804\ubd80 type\uc744 ('U')\ub85c \ubcc0\uacbd\ud558\uc5ec Countvectorizer\ub97c \uc0ac\uc6a9\ud55c\ub2e4. \n- [\uc124\uba85 \ub9c1\ud06c](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html?highlight=countvectorizer#sklearn.feature_extraction.text.CountVectorizer)","20fb87e6":"## [Empty Module #3] SVM: classifier\n\ubaa9\ud45c: SVC() \ub97c \ud65c\uc6a9\ud574 classification \uc744 \uc9c4\ud589\n\n- fit()\uc73c\ub85c train \uc5d0 \ub300\ud55c \uba38\uc2e0\ub7ec\ub2dd \ud559\uc2b5\n- predict()\uc73c\ub85c test \uc5d0 \ub300\ud55c \uc815\ub2f5\uc744 \ucd94\ub860 \ud558\uc5ec \ubc18\ud658"}}