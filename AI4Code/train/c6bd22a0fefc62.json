{"cell_type":{"ad90dce0":"code","5434e819":"code","c1e4663e":"code","553de58d":"code","a9795da3":"code","249bdffb":"code","1b8404c5":"code","aba03625":"code","c02996f1":"code","8ba4bd5b":"code","2f6db852":"code","9d05b945":"code","60d28a84":"code","5fe6db47":"code","8c97114d":"code","8189c442":"code","ec78b9d4":"markdown","b45ace32":"markdown","e6772a4f":"markdown","5d7de359":"markdown"},"source":{"ad90dce0":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom fastai import vision\nfrom fastai import metrics\n\nimport os\nprint(os.listdir(\"..\/input\"))","5434e819":"train_imgs_path = '..\/input\/train\/train'\ntest_imgs_path = '..\/input\/test\/test'\nlabels_path = '..\/input\/train.csv'\nin_path = '..\/input\/'","c1e4663e":"df = pd.read_csv(labels_path)\ndf['id'] = 'train\/train\/' + df['id']\ndf.head()","553de58d":"df.has_cactus.hist(grid=False, figsize=(5, 4), bins=np.arange(3)-0.3, width=0.6)\nplt.xticks([0, 1])\nplt.show()","a9795da3":"data = vision.ImageDataBunch.from_df(in_path, df, ds_tfms=vision.get_transforms(), size=224)\ndata = data.normalize(vision.imagenet_stats)","249bdffb":"data.show_batch(rows=3, figsize=(10, 8))","1b8404c5":"learn = vision.cnn_learner(data, vision.models.resnet34, metrics=metrics.accuracy)\nlearn.fit(2)","aba03625":"interp = vision.ClassificationInterpretation.from_learner(learn)\n\nlosses,idxs = interp.top_losses()","c02996f1":"interp.plot_top_losses(9, figsize=(15,11))","8ba4bd5b":"interp.plot_confusion_matrix(figsize=(5,5), dpi=60)","2f6db852":"submission_df = pd.read_csv('..\/input\/sample_submission.csv')\nfiles = submission_df['id'].values\nimg_paths = ('..\/input\/test\/test\/' + submission_df['id']).values","9d05b945":"img_paths[:10]","60d28a84":"from tqdm import tqdm\npreds = []\n\nfor p in tqdm(img_paths):\n    pred = learn.predict(vision.open_image(p))[-1].numpy()\n    preds.append(pred)","5fe6db47":"submission_df['has_cactus'] = np.array(preds)[:, 1]\nsubmission_df.head()","8c97114d":"np.sum(np.array(preds)[:, 1] > 0.5), submission_df.shape","8189c442":"submission_df.to_csv('submission.csv', index=False)","ec78b9d4":"Number of images with cactus are ~3 times more than number of images without cactus. This might cause a bias towards presence of cactus. Let's see if transforms in fastai can handle this!","b45ace32":"`learn.save('stage-1')` throws error as `..\/input\/models` is read only. Work around might be `torch.save(fastai.get_model(learn.model).state_dict(), 'stage-1')`. If you want to save optimizer as well then:\n\n```\nstate = {'model': fastai.get_model(learn.model).state_dict(), 'opt':learn.opt.state_dict()}\ntorch.save(state, 'stage-1')\n```\n\nWe can certainly unfreeze and then fine-tune the model! But the model above seems preety good to me!\n\nLet's interpret the results we got!","e6772a4f":"# My First Rodeo with FastAI\nThis is my first try with fastai so I'll keep it simple. Let me know where I could've done better.","5d7de359":"Since there are only 2 classes, we certainly doesn't need to call `learn.most_confused(...)`.\n\nFinally Predictions!"}}