{"cell_type":{"6c9cf0e6":"code","69d93cda":"code","d8d339e3":"code","436297a5":"code","78e22a56":"code","c3b3c4ca":"code","60bf08c3":"code","db43aa8b":"code","164bab59":"code","ac9e2ac1":"code","e8c2b4a1":"code","ef10b189":"code","b6654353":"code","a051883c":"code","8b9b65cd":"code","b8e20e4e":"code","91ceed92":"code","ad41f49a":"code","8a08e008":"code","0c4f9fa6":"code","79d8d7ac":"code","db7840da":"markdown","fbc1cf62":"markdown","724d3153":"markdown","eb144da1":"markdown","b1a75cb2":"markdown","6a2057c4":"markdown","0cd54fd2":"markdown","286e6851":"markdown","24963d3c":"markdown","cadda8b2":"markdown","2ee000c3":"markdown"},"source":{"6c9cf0e6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","69d93cda":"data = pd.read_csv(\"..\/input\/glass.csv\")","d8d339e3":"data.head(5)     # it seems like columns are showing possible elements and rows are showing the amount of them. And Type column is or label","436297a5":"data.tail()     # I see that data is sorted by label.   Data needs shuffling before training and splitting.","78e22a56":"data.info()          # all of the features are in float format and non-null which is perfect","c3b3c4ca":"data.describe()             # I see that some columns has generally big values (Si), while some has very small values (Fe).  Data needs normalization before training","60bf08c3":"import matplotlib.pyplot as plt\nimport seaborn as sns","db43aa8b":"plt.figure(figsize=(24,16))\n\nplt.subplot(3,3,1)\nsns.violinplot(data.Type,data.RI)\n\nplt.subplot(3,3,2)\nsns.violinplot(data.Type,data.Na)\n\nplt.subplot(3,3,3)\nsns.violinplot(data.Type,data.Mg)\n\nplt.subplot(3,3,4)\nsns.violinplot(data.Type,data.Al)\n\nplt.subplot(3,3,5)\nsns.violinplot(data.Type,data.Si)\n\nplt.subplot(3,3,6)\nsns.violinplot(data.Type,data.K)\n\nplt.subplot(3,3,7)\nsns.violinplot(data.Type,data.Ca)\n\nplt.subplot(3,3,8)\nsns.violinplot(data.Type,data.Ba)\n\nplt.subplot(3,3,9)\nsns.violinplot(data.Type,data.Fe)\n\nplt.show()","164bab59":"plt.figure(figsize=(24,20))\nsns.heatmap(data.corr(),annot=True,linecolor=\"white\",linewidths=(1,1),cmap=\"winter\")\nplt.show()","ac9e2ac1":"sns.pairplot(data=data,hue=\"Type\",vars=[\"RI\", \"Na\",\"Mg\",\"Al\",\"Si\",\"K\",\"Ca\",\"Ba\",\"Fe\"])\nplt.show()","e8c2b4a1":"data = data.sample(frac=1,random_state=22)","ef10b189":"data.head()  # as you can see below, it has changed rows randomly (even indexes are same as what they corresponded before)","b6654353":"y = data.Type.values.reshape(-1,1)\ndata.drop([\"Type\"],axis=1,inplace=True)\n\nx = data.values","a051883c":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,random_state=42,test_size=0.3)","8b9b65cd":"from sklearn.metrics import confusion_matrix","b8e20e4e":"from sklearn.linear_model import LogisticRegression\nlog_res_model = LogisticRegression(solver=\"newton-cg\",max_iter=400,multi_class=\"multinomial\",random_state=42)\n\nlog_res_model.fit(x_train,y_train.ravel())\ny_pred = log_res_model.predict(x_test)\n\ncm = confusion_matrix(y_test,y_pred)\n\nprint(\"Score of Logistic Regression: \",log_res_model.score(x_test,y_test))\n\nplt.figure(figsize=(10,10))\nsns.heatmap(cm, annot=True, cbar=False)\nplt.show()","91ceed92":"from sklearn.tree import DecisionTreeClassifier\ndec_tree_model = DecisionTreeClassifier(min_samples_split=4,random_state=42)\ndec_tree_model.fit(x_train,y_train)\ny_pred = dec_tree_model.predict(x_test)\n\ncm = confusion_matrix(y_test,y_pred)\n\nprint(\"Score of Decision Tree Classifier: \",dec_tree_model.score(x_test,y_test))\n\nplt.figure(figsize=(10,10))\nsns.heatmap(cm, annot=True, cbar=False)\nplt.show()","ad41f49a":"from sklearn.ensemble import RandomForestClassifier\nrfc_model = RandomForestClassifier(n_estimators=600,random_state=42,max_leaf_nodes=36)\nrfc_model.fit(x_train,y_train.ravel())\ny_pred = rfc_model.predict(x_test)\n\ncm = confusion_matrix(y_test,y_pred)\n\nprint(\"Score of Random Forest Classifier: \",rfc_model.score(x_test,y_test))\n\nplt.figure(figsize=(10,10))\nsns.heatmap(cm, annot=True, cbar=False)\nplt.show()","8a08e008":"from sklearn.naive_bayes import GaussianNB\nnb_model = GaussianNB()\nnb_model.fit(x_train,y_train.ravel())\ny_pred = nb_model.predict(x_test)\n\ncm = confusion_matrix(y_test,y_pred)\n\nprint(\"Score of Naive Bayes: \",nb_model.score(x_test,y_test))\n\nplt.figure(figsize=(10,10))\nsns.heatmap(cm, annot=True, cbar=False)\nplt.show()","0c4f9fa6":"from sklearn.neighbors import KNeighborsClassifier\nknn_model = KNeighborsClassifier(n_neighbors=1,p=1)\nknn_model.fit(x_train,y_train.ravel())\ny_pred = knn_model.predict(x_test)\n\ncm = confusion_matrix(y_test,y_pred)\n\nprint(\"Score of K Nearest Neighbors: \",knn_model.score(x_test,y_test))\n\nplt.figure(figsize=(10,10))\nsns.heatmap(cm, annot=True, cbar=False)\nplt.show()","79d8d7ac":"from sklearn.svm import SVC\nsvc_model = SVC(random_state=42,C=2)\nsvc_model.fit(x_train,y_train.ravel())\ny_pred = svc_model.predict(x_test)\n\ncm = confusion_matrix(y_test,y_pred)\n\nprint(\"Score of Support Vector Machine: \",svc_model.score(x_test,y_test))\n\nplt.figure(figsize=(10,10))\nsns.heatmap(cm, annot=True, cbar=False)\nplt.show()","db7840da":"Since we dont have another csv file for testing, I will use the train_test_split from sklearn","fbc1cf62":"# Hey\n\n## Introduction\nI found this dataset from UCI and decided to improve my skills on it. \n\nI will be analyze and visualize the data and then I will apply some ML models on it.   Lastly, I will do comparisons and have conclusion. enjoy!","724d3153":"Since we understand our dataset more, I would like to shuffle the whole dataset because I dont want to make my ML models to memorize first types mostly","eb144da1":"We see above that some Glass types has data points that has small differences ","b1a75cb2":"I will separate my Features and the Labels","6a2057c4":"# PreProcessing","0cd54fd2":"# Conclusion\n\nAs you see above, even if I do tuning I cannot reach high accuracies. \nI think the reason behind that is dataset itself. Because it has unbalanced + low amount of data    ( I just might be unsuccesful too :D )\n\nHowever, Random Forest Classifier and Decision Tree Classifier made a relatively good job with this data compared to other models.\n\n** Thanks for visiting my kernel. I am a learner and I would be happy if you have any advice or comments **","286e6851":"There are some highly correlated columns such as Ca and Rl","24963d3c":"# Classification Models","cadda8b2":"# Visual EDA","2ee000c3":"# EDA"}}