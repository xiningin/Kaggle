{"cell_type":{"9d341cb1":"code","85a96fbf":"code","7cea3b8f":"code","f305e2ad":"code","5b62248a":"code","f634bc91":"code","7ec6ac2e":"code","618e1a5a":"code","42da83a1":"code","7d101fcb":"code","e04f65cd":"code","2ab889e7":"code","6cd87432":"code","060f97de":"code","395f6342":"code","de6d3894":"code","99c532cd":"code","d9ca4aa1":"code","43e68489":"code","74d45cbc":"code","1200e520":"code","d127c8ab":"code","46eebc7d":"code","09f78659":"code","f8d60638":"code","431da42d":"code","33ce3622":"code","497e5b5f":"code","355d54e5":"code","ca3fc455":"code","57682a5d":"code","bce3fce4":"code","331f5b12":"code","311457f2":"code","feeea9fc":"markdown","44bf7313":"markdown","e77aa8ba":"markdown","6589f633":"markdown","f62f05ac":"markdown","8450be4b":"markdown","c37fc3db":"markdown","205015e4":"markdown","2d16f7e3":"markdown","2fc7343a":"markdown","56968bf4":"markdown","58ea545e":"markdown","f33e0e11":"markdown","85051b11":"markdown","e6615a93":"markdown","d5d39862":"markdown","bb271287":"markdown","04b1e065":"markdown","e780ed6d":"markdown","ac41716f":"markdown","21684f30":"markdown","21ef9615":"markdown","36d47d19":"markdown","b2a0e366":"markdown","1c46fd5c":"markdown","0f61ad2f":"markdown","22c6bfe4":"markdown","2bbb4b71":"markdown"},"source":{"9d341cb1":"\n\nimport pandas as pd\nimport numpy as np\nimport os\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n\nfrom plotly import tools\nimport plotly.offline as py\nimport plotly.figure_factory as ff\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\n\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import classification_report,confusion_matrix, f1_score\n\nfrom collections import defaultdict\nfrom collections import Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\n\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\n\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\n\n","85a96fbf":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\n\n","7cea3b8f":"print('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))\nprint('There are {} rows and {} columns in test'.format(test.shape[0],test.shape[1]))","f305e2ad":"train.head()\n","5b62248a":"x=train.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('Number of samples')","f634bc91":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train[train['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=train[train['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","7ec6ac2e":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train[train['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=train[train['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweet')\nplt.show()","618e1a5a":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=train[train['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('disaster')\nword=train[train['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Not disaster')\nfig.suptitle('Average word length in each tweet')\n\n","42da83a1":"cnt_ = train['location'].value_counts()\ncnt_.reset_index()\ncnt_ = cnt_[:20,]\ntrace1 = go.Bar(\n                x = cnt_.index,\n                y = cnt_.values,\n                name = \"Number of tweets in dataset according to location\",\n                marker = dict(color = 'rgba(200, 74, 55, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.5)),\n                )\n\ndata = [trace1]\nlayout = go.Layout(barmode = \"group\",title = 'Number of tweets depending on location')\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","7d101fcb":"train1_df = train[train[\"target\"]==1]\ntrain0_df = train[train[\"target\"]==0]\ncnt_1 = train1_df['location'].value_counts()\ncnt_1.reset_index()\ncnt_1 = cnt_1[:20,]\n\ncnt_0 = train0_df['location'].value_counts()\ncnt_0.reset_index()\ncnt_0 = cnt_0[:20,]\n\ntrace1 = go.Bar(\n                x = cnt_1.index,\n                y = cnt_1.values,\n                name = \"Number of real disaster tweets\",\n                marker = dict(color = 'rgba(255, 74, 55, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.5)),\n                )\ntrace0 = go.Bar(\n                x = cnt_0.index,\n                y = cnt_0.values,\n                name = \"Number of unreal disaster tweets\",\n                marker = dict(color = 'rgba(79, 82, 97, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.5)),\n                )\n\n\ndata = [trace0,trace1]\nlayout = go.Layout(barmode = 'stack',title = 'Number of tweets depending on location per class')\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","e04f65cd":"from wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train[train[\"target\"]==1], title=\"Word Cloud of real disaster tweets\")","2ab889e7":"plot_wordcloud(train[train[\"target\"]==0], title=\"Word Cloud of unreal disaster tweets\")\n\n","6cd87432":"def create_corpus(target):\n    corpus=[]\n    \n    for x in train[train['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","060f97de":"corpus=create_corpus(0)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]\n\n\n\nplt.rcParams['figure.figsize'] = (18.0, 6.0)\nx,y=zip(*top)\nplt.bar(x,y)","395f6342":"def clean_text(text):\n    import re\n    text = text.lower()\n    text = re.sub(r\"i'm\", \"i am\", text)\n    text = re.sub(r\"you'll\", \"you will\", text)\n    text = re.sub(r\"i'll\", \"i will\", text)\n    text = re.sub(r\"she'll\", \"she will\", text)\n    text = re.sub(r\"he'll\", \"he will\", text)\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"she's\", \"she is\", text)\n    text = re.sub(r\"that's\", \"that is\", text)\n    text = re.sub(r\"what's\", \"what is\", text)\n    text = re.sub(r\"where's\", \"where is\", text)\n    text = re.sub(r\"there's\", \"there is\", text)\n    text = re.sub(r\"here's\", \"here is\", text)\n    text = re.sub(r\"who's\", \"who is\", text)\n    text = re.sub(r\"how's\", \"how is\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"can't\", \"cannot\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"don't\", \"do not\", text)\n    text = re.sub(r\"shouldn't\", \"should not\", text)\n    text = re.sub(r\"n't\", \" not\", text)\n    text = re.sub(r\"   \", \" \", text) # Remove any extra spaces\n    return text\n\n\ntrain['clean_text'] = train['text'].apply(clean_text)\ntest['clean_text'] = test['text'].apply(clean_text)","de6d3894":"def massage_text(text):\n    import re\n    from nltk.corpus import stopwords\n    ## remove anything other then characters and put everything in lowercase\n    tweet = re.sub(\"[^a-zA-Z]\", ' ', text)\n    tweet = tweet.lower()\n    tweet = tweet.split()\n\n    from nltk.stem import WordNetLemmatizer\n    lem = WordNetLemmatizer()\n    tweet = [lem.lemmatize(word) for word in tweet\n             if word not in set(stopwords.words('english'))]\n    tweet = ' '.join(tweet)\n    return tweet\n    print('--here goes nothing')\n    print(text)\n    print(tweet)\n\ntrain['clean_text'] = train['text'].apply(massage_text)\ntest['clean_text'] = test['text'].apply(massage_text)","99c532cd":"train.iloc[0:10][['text','clean_text']]","d9ca4aa1":"import xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nX_train,X_test,y_train,y_test = train_test_split(train['clean_text'],train['target'])\n\ntfidf = TfidfVectorizer()\n\ntrain_vector = tfidf.fit_transform(X_train)\ntest_vector = tfidf.transform(X_test)","43e68489":"models = (xgb.XGBClassifier(random_state = 1),\n         RandomForestClassifier(random_state = 1),\n         GradientBoostingClassifier(random_state = 1),\n         lgb.LGBMClassifier(random_state = 1),\n         LogisticRegression(random_state = 1))\n\nfor model in models:\n    model.fit(train_vector, y_train)\n    predict = model.predict(test_vector)\n    print(f1_score(y_test,predict))","74d45cbc":"from vecstack import stacking\n\n","1200e520":"S_train, S_test = stacking(models,                   \n                           train_vector, y_train, test_vector,   \n                           regression=False, \n     \n                           mode='oof_pred_bag', \n       \n                           needs_proba=False,\n         \n                           save_dir=None, \n            \n                           metric=f1_score, \n    \n                           n_folds=4, \n                 \n                           stratified=True,\n            \n                           shuffle=True,  \n            \n                           random_state=0,    \n         \n                           verbose=2)","d127c8ab":"model = LogisticRegression(random_state=0)\n    \nmodel = model.fit(S_train, y_train)\n\ny_pred = model.predict(S_test)\n\nprint(f1_score(y_test,predict))","46eebc7d":"# We will use the official tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","09f78659":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport tokenization","f8d60638":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","431da42d":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","33ce3622":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","497e5b5f":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","355d54e5":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","ca3fc455":"train_input = bert_encode(train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train.target.values","57682a5d":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","bce3fce4":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=16\n)","331f5b12":"model.load_weights('model.h5')\ntest_pred = model.predict(test_input)","311457f2":"submission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","feeea9fc":"**1. Import libraries**\n","44bf7313":"3.6 Number of tweets depending on location per class\n","e77aa8ba":"# Model: Build, Train, Predict, Submit","6589f633":"3.2 Number of characters in tweets","f62f05ac":"3.3 Number of words in a tweet\n","8450be4b":"**6. Automated stacking**","c37fc3db":"**5. Baseline models**\n","205015e4":"**3. EDA**\n","2d16f7e3":"\n\nColumns\n\n*     id - a unique identifier for each tweet\n*     text - the text of the tweet\n*     location - the location the tweet was sent from (may be blank)\n*     keyword - a particular keyword from the tweet (may be blank)\n*     target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n\n","2fc7343a":"Class 1","56968bf4":"**2. Download data**","58ea545e":"CLass 0","f33e0e11":"**7. BERT**","85051b11":"This notebook wouldn't have been possible without these resources. Most of the solutions used in this notebooks are taken from the below mentioned resources, combining everything into one\n\n*     https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert\n*     https:\/\/www.kaggle.com\/ratan123\/start-from-here-disaster-tweets-eda-basic-model\n*     https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\n*     https:\/\/www.kaggle.com\/szelee\/simpletransformers-hyperparam-tuning-k-fold-cv\n*     https:\/\/www.kaggle.com\/basu369victor\/learning-bert-for-the-first-time\n*     https:\/\/www.kaggle.com\/slatawa\/tfidf-implementation-to-get-80-accuracy\n*     https:\/\/www.kaggle.com\/user123454321\/bert-starter-inference\n*     https:\/\/towardsdatascience.com\/automate-stacking-in-python-fc3e7834772e\n","e6615a93":"3.7 WordCloud\n","d5d39862":"# Helper Functions","bb271287":"3.4 Average word length in a tweet\n","04b1e065":"# Load and Preprocess\n\n- Load BERT from the Tensorflow Hub\n- Load CSV files containing training data\n- Load tokenizer from the bert layer\n- Encode the text into tokens, masks, and segment flags","e780ed6d":"Table of content\n\n1. Import libraries\n2. Download data\n3. EDA\n4. Data Cleaning\n5. Baseline models\n6. Automated stacking\n7. BERT\n","ac41716f":"Let's check the size of the train and test datasets.\n","21684f30":"**If you like this kernel, please upvote.**\n","21ef9615":"3.1 Class distribution\n","36d47d19":"In order to automate stacking I'll use vecstack package.","b2a0e366":"**4. Data cleaning**\n","1c46fd5c":"Preview of cleaned text\n","0f61ad2f":"F1 score is 74%, not bad, but I will try to use BERT model.","22c6bfe4":"3.8 Common stopwords\n","2bbb4b71":"3.5 Number of tweets in dataset according to location\n"}}