{"cell_type":{"2631d3a2":"code","ffc94fe7":"code","b86406b8":"code","48d6cfe9":"code","2c795525":"code","49547c63":"code","829732dd":"code","ffbf0e8d":"code","49ef5393":"code","dcaadfe8":"code","8dddca28":"code","d5b8ce41":"code","b04b03a1":"code","e1367d06":"code","c86cee87":"code","fecb6d5f":"code","d46d5da9":"code","fb50ee17":"code","27dac49e":"code","aa62afdf":"code","36a61716":"code","6575d64a":"code","2d1a452c":"code","da681b8d":"code","4fd6704b":"code","c5a63772":"code","2be61799":"code","360b9c1a":"code","e668d2a7":"code","59a7a0c4":"code","270f8ac1":"code","da582f88":"code","7b5cb656":"code","57f907cf":"code","877e7f79":"code","40da2b76":"code","3cf7c6a6":"code","35624951":"code","c100c1eb":"code","9bea094e":"code","ff119e8b":"code","2e065f94":"code","8ddb772f":"code","4b2a3bd7":"code","bfcf2aaa":"code","07389bd2":"code","c350a6f5":"code","75471a16":"code","c17fe656":"code","dffb6ff9":"code","459eaecd":"code","95975bdd":"code","3be37c17":"code","31fb931f":"code","3c6a0c3e":"code","f06ad04a":"code","90a32e3a":"code","38621862":"code","4117d91e":"code","011a255b":"code","3685f7b9":"code","6576b8ef":"code","171f5735":"code","9d73a0d7":"code","5d74b80c":"code","7e184017":"code","dfebe833":"code","02ebd3e0":"code","ba277286":"code","2b85088d":"code","26922df0":"code","cb0f4102":"code","a8f159a3":"code","4baf4377":"code","199f92c8":"code","0a9b6378":"code","649342dd":"code","e524ead9":"code","5ce94ba9":"code","d8aab7d3":"code","0329ab87":"code","d1f9e6b3":"code","e98e4f89":"code","0979d8a5":"code","78dbc0b7":"code","d5fd3615":"code","eab41c50":"code","66888e38":"code","08d69c23":"code","44d35b34":"code","12fc20ed":"code","dcbc25d3":"code","bf990cad":"markdown","f5d9588c":"markdown","bc246b1a":"markdown","a0cfa27a":"markdown","e1fbb351":"markdown","bcadd872":"markdown","fa01beda":"markdown","c8c29ca8":"markdown","b7d5251d":"markdown","6278c512":"markdown","1a4c9d90":"markdown","7e9a2565":"markdown","639c8225":"markdown","08ea1275":"markdown","4ed6aaa5":"markdown","8a59f1b4":"markdown","43f559f4":"markdown","cfda269a":"markdown","5fc5d4d5":"markdown","fbfb5e7d":"markdown","def33aa9":"markdown","f5e21e4a":"markdown","1a756adf":"markdown","17052305":"markdown","552e41d7":"markdown","b2dacee0":"markdown","b9ffd76f":"markdown","f055d3ff":"markdown","a68366c8":"markdown","c7c0d0a4":"markdown","77ec6144":"markdown","a61e3993":"markdown","fa7c23f5":"markdown","ef3a096a":"markdown","17e5d86a":"markdown","6cdf2495":"markdown","09e4dc1c":"markdown","d63eaa38":"markdown","d3e602d1":"markdown","1695079f":"markdown"},"source":{"2631d3a2":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(font_scale = 2.2)\nplt.style.use('seaborn')\n\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, ShuffleSplit\nfrom sklearn.metrics import f1_score\nimport itertools\n# \uc6a9\uc774\ud55c \ubc18\ubcf5 \uae30\ub2a5\uc744 \uc801\uc6a9\ud558\ub294 \ubaa8\ub4c8: https:\/\/hamait.tistory.com\/803\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport shap\nfrom tqdm import tqdm\nimport featuretools as ft\nimport warnings\nwarnings.filterwarnings('ignore')\nimport time","ffc94fe7":"df_train = pd.read_csv('..\/input\/costa-rican-household-poverty-prediction\/train.csv')\ndf_test = pd.read_csv('..\/input\/costa-rican-household-poverty-prediction\/test.csv')","b86406b8":"print('df_train_shape: ', df_train.shape, ' ', 'df_test_shape', df_test.shape)","48d6cfe9":"description = [\n(\"v2a1\",\" Monthly rent payment\"),\n(\"hacdor\",\" =1 Overcrowding by bedrooms\"),\n(\"rooms\",\"  number of all rooms in the house\"),\n(\"hacapo\",\" =1 Overcrowding by rooms\"),\n(\"v14a\",\" =1 has toilet in the household\"),\n(\"refrig\",\" =1 if the household has refrigerator\"),\n(\"v18q\",\" owns a tablet\"),\n(\"v18q1\",\" number of tablets household owns\"),\n(\"r4h1\",\" Males younger than 12 years of age\"),\n(\"r4h2\",\" Males 12 years of age and older\"),\n(\"r4h3\",\" Total males in the household\"),\n(\"r4m1\",\" Females younger than 12 years of age\"),\n(\"r4m2\",\" Females 12 years of age and older\"),\n(\"r4m3\",\" Total females in the household\"),\n(\"r4t1\",\" persons younger than 12 years of age\"),\n(\"r4t2\",\" persons 12 years of age and older\"),\n(\"r4t3\",\" Total persons in the household\"),\n(\"tamhog\",\" size of the household\"),\n(\"tamviv\",\" number of persons living in the household\"),\n(\"escolari\",\" years of schooling\"),\n(\"rez_esc\",\" Years behind in school\"),\n(\"hhsize\",\" household size\"),\n(\"paredblolad\",\" =1 if predominant material on the outside wall is block or brick\"),\n(\"paredzocalo\",\" =1 if predominant material on the outside wall is socket (wood, zinc or absbesto\"),\n(\"paredpreb\",\" =1 if predominant material on the outside wall is prefabricated or cement\"),\n(\"pareddes\",\" =1 if predominant material on the outside wall is waste material\"),\n(\"paredmad\",\" =1 if predominant material on the outside wall is wood\"),\n(\"paredzinc\",\" =1 if predominant material on the outside wall is zink\"),\n(\"paredfibras\",\" =1 if predominant material on the outside wall is natural fibers\"),\n(\"paredother\",\" =1 if predominant material on the outside wall is other\"),\n(\"pisomoscer\",\" =1 if predominant material on the floor is mosaic ceramic   terrazo\"),\n(\"pisocemento\",\" =1 if predominant material on the floor is cement\"),\n(\"pisoother\",\" =1 if predominant material on the floor is other\"),\n(\"pisonatur\",\" =1 if predominant material on the floor is  natural material\"),\n(\"pisonotiene\",\" =1 if no floor at the household\"),\n(\"pisomadera\",\" =1 if predominant material on the floor is wood\"),\n(\"techozinc\",\" =1 if predominant material on the roof is metal foil or zink\"),\n(\"techoentrepiso\",\" =1 if predominant material on the roof is fiber cement,   mezzanine \"),\n(\"techocane\",\" =1 if predominant material on the roof is natural fibers\"),\n(\"techootro\",\" =1 if predominant material on the roof is other\"),\n(\"cielorazo\",\" =1 if the house has ceiling\"),\n(\"abastaguadentro\",\" =1 if water provision inside the dwelling\"),\n(\"abastaguafuera\",\" =1 if water provision outside the dwelling\"),\n(\"abastaguano\",\" =1 if no water provision\"),\n(\"public\",\" =1 electricity from CNFL,  ICE, ESPH\/JASEC\"),\n(\"planpri\",\" =1 electricity from private plant\"),\n(\"noelec\",\" =1 no electricity in the dwelling\"),\n(\"coopele\",\" =1 electricity from cooperative\"),\n(\"sanitario1\",\" =1 no toilet in the dwelling\"),\n(\"sanitario2\",\" =1 toilet connected to sewer or cesspool\"),\n(\"sanitario3\",\" =1 toilet connected to  septic tank\"),\n(\"sanitario5\",\" =1 toilet connected to black hole or letrine\"),\n(\"sanitario6\",\" =1 toilet connected to other system\"),\n(\"energcocinar1\",\" =1 no main source of energy used for cooking (no kitchen)\"),\n(\"energcocinar2\",\" =1 main source of energy used for cooking electricity\"),\n(\"energcocinar3\",\" =1 main source of energy used for cooking gas\"),\n(\"energcocinar4\",\" =1 main source of energy used for cooking wood charcoal\"),\n(\"elimbasu1\",\" =1 if rubbish disposal mainly by tanker truck\"),\n(\"elimbasu2\",\" =1 if rubbish disposal mainly by botan hollow or buried\"),\n(\"elimbasu3\",\" =1 if rubbish disposal mainly by burning\"),\n(\"elimbasu4\",\" =1 if rubbish disposal mainly by throwing in an unoccupied space\"),\n(\"elimbasu5\",\" =1 if rubbish disposal mainly by throwing in river,   creek or sea\"),\n(\"elimbasu6\",\" =1 if rubbish disposal mainly other\"),\n(\"epared1\",\" =1 if walls are bad\"),\n(\"epared2\",\" =1 if walls are regular\"),\n(\"epared3\",\" =1 if walls are good\"),\n(\"etecho1\",\" =1 if roof are bad\"),\n(\"etecho2\",\" =1 if roof are regular\"),\n(\"etecho3\",\" =1 if roof are good\"),\n(\"eviv1\",\" =1 if floor are bad\"),\n(\"eviv2\",\" =1 if floor are regular\"),\n(\"eviv3\",\" =1 if floor are good\"),\n(\"dis\",\" =1 if disable person\"),\n(\"male\",\" =1 if male\"),\n(\"female\",\" =1 if female\"),\n(\"estadocivil1\",\" =1 if less than 10 years old\"),\n(\"estadocivil2\",\" =1 if free or coupled uunion\"),\n(\"estadocivil3\",\" =1 if married\"),\n(\"estadocivil4\",\" =1 if divorced\"),\n(\"estadocivil5\",\" =1 if separated\"),\n(\"estadocivil6\",\" =1 if widow\/er\"),\n(\"estadocivil7\",\" =1 if single\"),\n(\"parentesco1\",\" =1 if household head\"),\n(\"parentesco2\",\" =1 if spouse\/partner\"),\n(\"parentesco3\",\" =1 if son\/doughter\"),\n(\"parentesco4\",\" =1 if stepson\/doughter\"),\n(\"parentesco5\",\" =1 if son\/doughter in law\"),\n(\"parentesco6\",\" =1 if grandson\/doughter\"),\n(\"parentesco7\",\" =1 if mother\/father\"),\n(\"parentesco8\",\" =1 if father\/mother in law\"),\n(\"parentesco9\",\" =1 if brother\/sister\"),\n(\"parentesco10\",\" =1 if brother\/sister in law\"),\n(\"parentesco11\",\" =1 if other family member\"),\n(\"parentesco12\",\" =1 if other non family member\"),\n(\"idhogar\",\" Household level identifier\"),\n(\"hogar_nin\",\" Number of children 0 to 19 in household\"),\n(\"hogar_adul\",\" Number of adults in household\"),\n(\"hogar_mayor\",\" # of individuals 65+ in the household\"),\n(\"hogar_total\",\" # of total individuals in the household\"),\n(\"dependency\",\" Dependency rate\"),\n(\"edjefe\",\" years of education of male head of household\"),\n(\"edjefa\",\" years of education of female head of household\"),\n(\"meaneduc\",\"average years of education for adults (18+)\"),\n(\"instlevel1\",\" =1 no level of education\"),\n(\"instlevel2\",\" =1 incomplete primary\"),\n(\"instlevel3\",\" =1 complete primary\"),\n(\"instlevel4\",\" =1 incomplete academic secondary level\"),\n(\"instlevel5\",\" =1 complete academic secondary level\"),\n(\"instlevel6\",\" =1 incomplete technical secondary level\"),\n(\"instlevel7\",\" =1 complete technical secondary level\"),\n(\"instlevel8\",\" =1 undergraduate and higher education\"),\n(\"instlevel9\",\" =1 postgraduate higher education\"),\n(\"bedrooms\",\" number of bedrooms\"),\n(\"overcrowding\",\" # persons per room\"),\n(\"tipovivi1\",\" =1 own and fully paid house\"),\n(\"tipovivi2\",\" =1 own,   paying in installments\"),\n(\"tipovivi3\",\" =1 rented\"),\n(\"tipovivi4\",\" =1 precarious\"),\n(\"tipovivi5\",\" =1 other(assigned\"),\n(\"computer\",\" =1 if the household has notebook or desktop computer,   borrowed)\"),\n(\"television\",\" =1 if the household has TV\"),\n(\"mobilephone\",\" =1 if mobile phone\"),\n(\"qmobilephone\",\" # of mobile phones\"),\n(\"lugar1\",\" =1 region Central\"),\n(\"lugar2\",\" =1 region Chorotega\"),\n(\"lugar3\",\" =1 region Pac\u00c3\u0192\u00c2\u00adfico central\"),\n(\"lugar4\",\" =1 region Brunca\"),\n(\"lugar5\",\" =1 region Huetar Atl\u00c3\u0192\u00c2\u00a1ntica\"),\n(\"lugar6\",\" =1 region Huetar Norte\"),\n(\"area1\",\" =1 zona urbana\"),\n(\"area2\",\" =2 zona rural\"),\n(\"age\",\" Age in years\"),\n(\"SQBescolari\",\" escolari squared\"),\n(\"SQBage\",\" age squared\"),\n(\"SQBhogar_total\",\" hogar_total squared\"),\n(\"SQBedjefe\",\" edjefe squared\"),\n(\"SQBhogar_nin\",\" hogar_nin squared\"),\n(\"SQBovercrowding\",\" overcrowding squared\"),\n(\"SQBdependency\",\" dependency squared\"),\n(\"SQBmeaned\",\" meaned squared\"),\n(\"agesq\",\" Age squared\"),]\n\ndescription = pd.DataFrame(description, columns=['varname', 'description'])","2c795525":"description.head()","49547c63":"total = df_train.isnull().sum().sort_values(ascending=False)\npercent = 100 * (df_train.isnull().sum() \/ df_train.isnull().count()).sort_values(ascending=False)\nmissing_df = pd.concat([total, percent], axis = 1, keys=['Total','Percent'])\n\nmissing_df.head(20)","829732dd":"# if education is 'yes' and person is head of household, fill with escolari\n\ndf_train.loc[(df_train['edjefa'] == 'yes') & (df_train['parentesco1'] ==1), 'edjefa'] = df_train.loc[(df_train['edjefa'] == \"yes\") & (df_train['parentesco1'] == 1), \"escolari\"]\ndf_train.loc[(df_train['edjefe'] == \"yes\") & (df_train['parentesco1'] == 1), \"edjefe\"] = df_train.loc[(df_train['edjefe'] == \"yes\") & (df_train['parentesco1'] == 1), \"escolari\"]\n\ndf_test.loc[(df_test['edjefa'] == \"yes\") & (df_test['parentesco1'] == 1), \"edjefa\"] = df_test.loc[(df_test['edjefa'] == \"yes\") & (df_test['parentesco1'] == 1), \"escolari\"]\ndf_test.loc[(df_test['edjefe'] == \"yes\") & (df_test['parentesco1'] == 1), \"edjefe\"] = df_test.loc[(df_test['edjefe'] == \"yes\") & (df_test['parentesco1'] == 1), \"escolari\"]\n\n# this field is supposed to be interaction between gender and escolari, but it isn't clear what 'yes' means, let's fill it with 4\ndf_train.loc[df_train['edjefa']=='yes', 'edjefa'] = 4\ndf_train.loc[df_train['edjefe']=='yes', 'edjefe'] = 4\n\ndf_test.loc[df_test['edjefa']== 'yes', 'edjefa'] = 4\ndf_test.loc[df_test['edjefe']== 'yes', 'edjefe'] = 4\n\n# create feature with max education of either head of household\ndf_train['edjef'] = np.max(df_train[['edjefa','edjefe']], axis = 1)\ndf_test['edjef'] = np.max(df_test[['edjefa','edjefe']], axis=1)\n\n# fix some inconsistencies in the data - some rows indicate both that the household does and does not have a toilet,\n# if there is no water we'll assume they do not\ndf_train.loc[(df_train.v14a ==  1) & (df_train.sanitario1 ==  1) & (df_train.abastaguano == 0), \"v14a\"] = 0\ndf_train.loc[(df_train.v14a ==  1) & (df_train.sanitario1 ==  1) & (df_train.abastaguano == 0), \"sanitario1\"] = 0\n\ndf_test.loc[(df_test.v14a ==  1) & (df_test.sanitario1 ==  1) & (df_test.abastaguano == 0), \"v14a\"] = 0\ndf_test.loc[(df_test.v14a ==  1) & (df_test.sanitario1 ==  1) & (df_test.abastaguano == 0), \"sanitario1\"] = 0","ffbf0e8d":"df_train['rez_esc'].fillna(0, inplace =True)\ndf_test['rez_esc'].fillna(0, inplace =True)","49ef5393":"df_train['SQBmeaned'].fillna(0, inplace =True)\ndf_test['SQBmeaned'].fillna(0, inplace =True)","dcaadfe8":"df_train['meaneduc'].fillna(0, inplace=True)\ndf_test['meaneduc'].fillna(0, inplace=True)","8dddca28":"df_train['v18q'].value_counts()","d5b8ce41":"df_train.loc[df_train['v18q'] == 1, 'v18q1'].value_counts()","b04b03a1":"df_train.loc[df_train['v18q'] == 0, 'v18q1'].value_counts()","e1367d06":"df_train['v18q1'].fillna(0, inplace=True)\ndf_test['v18q1'].fillna(0, inplace=True)","c86cee87":"df_train['tipovivi3'].value_counts()","fecb6d5f":"plt.figure(figsize = (10,6))\n\nsns.kdeplot(df_train.loc[df_train['tipovivi3'] == 1, 'v2a1'],\n           label = 'Monthly rent payment of household(rented =1)', color = 'red')\nsns.kdeplot(df_train.loc[df_train['tipovivi3'] == 0, 'v2a1'],\n           label = 'Monthly rent payment of household(rented =0)', color = 'blue')\nplt.xscale('log');\nplt.legend(loc='upper right');\nplt.show();","d46d5da9":"df_train['v2a1'].fillna(0, inplace=True)\ndf_test['v2a1'].fillna(0, inplace=True)","fb50ee17":"total = df_train.isnull().sum().sort_values(ascending = False)\npercent = 100 * (df_train.isnull().sum() \/ df_train.isnull().count()).sort_values(ascending = False)\nmissing_df = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\nmissing_df.head(4)","27dac49e":"total = df_test.isnull().sum().sort_values(ascending = False)\npercent = 100 * (df_test.isnull().sum() \/ df_test.isnull().count()).sort_values(ascending = False)\nmissing_df = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\nmissing_df.head(4)","aa62afdf":"features_object = [col for col in df_train.columns if df_train[col].dtype == 'object']","36a61716":"features_object","6575d64a":"# some dependencies as Na, fill those with the square root of the square\ndf_train['dependency'] = np.sqrt(df_train['SQBdependency'])\ndf_test['dependency'] = np.sqrt(df_test['SQBdependency'])","2d1a452c":"def replace_edjefe(x):\n    if x == 'yes':\n        return 1\n    elif x == 'no':\n        return 0\n    else:\n        return x\n    \ndf_train['edjefe'] = df_train['edjefe'].apply(replace_edjefe).astype(float)\ndf_test['edjefe'] = df_test['edjefe'].apply(replace_edjefe).astype(float)","da681b8d":"def replace_edjefa(x):\n    if x == 'yes':\n        return 1\n    elif x == 'no':\n        return 0\n    else:\n        return x\n    \ndf_train['edjefa'] = df_train['edjefe'].apply(replace_edjefa).astype(float)\ndf_test['edjefa'] = df_test['edjefe'].apply(replace_edjefa).astype(float)","4fd6704b":"# create feature with max education of either head of household\ndf_train['edjef'] = np.max(df_train[['edjefa','edjefe']], axis = 1)\ndf_test['edjef'] = np.max(df_test[['edjefa','edjefe']], axis = 1)","c5a63772":"df_train['roof_waste_material'] = np.nan\ndf_test['roof_waste_material'] = np.nan\n\ndf_train['electricity_other'] = np.nan\ndf_test['electricity_other'] = np.nan\n\ndef fill_roof_exception(x):\n    if (x['techozinc'] == 0) and (x['techoentrepiso']== 0) and (x['techocane'] ==0) and (x['techootro'] == 0):\n        return 1\n    else: \n        return 0\n    \ndef fill_no_electricity(x):\n    if (x['public'] == 0) and (x['planpri'] == 0) and (x['noelec'] == 0) and (x['coopele'] == 0):\n        return 1\n    else:\n        return 0\n    \ndf_train['roof_waste_material'] = df_train.apply(lambda x : fill_roof_exception(x), axis=1)\ndf_test['roof_waste_material'] = df_test.apply(lambda x : fill_roof_exception(x), axis=1)\n\ndf_train['electricity_other'] = df_train.apply(lambda x : fill_no_electricity(x), axis=1)\ndf_test['electricity_other'] = df_test.apply(lambda x : fill_no_electricity(x), axis=1)","2be61799":"binary_cat_features = [col for col in df_train.columns if df_train[col].value_counts().shape[0] == 2]","360b9c1a":"continuous_features = [col for col in df_train.columns if col not in binary_cat_features]\ncontinuous_features = [col for col in continuous_features if col not in features_object]\ncontinuous_features = [col for col in continuous_features if col not in ['Id', 'Target', 'idhogar']]","e668d2a7":"print('there are {} continuous featrues'.format(len(continuous_features)))\nfor col in continuous_features:\n    print('{}: {}'.format(col, description.loc[description['varname'] == col, \n                                              'description'].values))","59a7a0c4":"df_train['edjef'].value_counts()","270f8ac1":"df_train.drop('tamhog', axis=1, inplace=True)\ndf_test.drop('tamhog', axis=1, inplace=True)","da582f88":"df_train['adult'] = df_train['hogar_adul'] - df_train['hogar_mayor']\ndf_train['dependency_count'] = df_train['hogar_nin'] + df_train['hogar_mayor']\ndf_train['dependency'] = df_train['dependency_count'] \/ df_train['adult']\ndf_train['child_percent'] = df_train['hogar_nin'] \/ df_train['hogar_total']\ndf_train['elder_percent'] = df_train['hogar_mayor'] \/ df_train['hogar_total']\ndf_train['adult_percent'] = df_train['hogar_adul'] \/ df_train['hogar_total']\ndf_train['males_younger_12_years_percent'] = df_train['r4h1'] \/ df_train['hogar_total']\ndf_train['males_older_12_years_percent'] = df_train['r4h2'] \/ df_train['hogar_total']\ndf_train['males_percent'] = df_train['r4h3'] \/ df_train['hogar_total']\ndf_train['females_younger_12_years_percent'] = df_train['r4m1'] \/ df_train['hogar_total']\ndf_train['females_older_12_years_percent'] = df_train['r4m2'] \/ df_train['hogar_total']\ndf_train['females_percent'] = df_train['r4m3'] \/ df_train['hogar_total']\ndf_train['persons_younger_12_years_percent'] = df_train['r4t1'] \/ df_train['hogar_total']\ndf_train['persons_older_12_years_percent'] = df_train['r4t2'] \/ df_train['hogar_total']\ndf_train['persons_percent'] = df_train['r4t3'] \/ df_train['hogar_total']","7b5cb656":"df_test['adult'] = df_test['hogar_adul'] - df_test['hogar_mayor']\ndf_test['dependency_count'] = df_test['hogar_nin'] + df_test['hogar_mayor']\ndf_test['dependency'] = df_test['dependency_count'] \/ df_test['adult']\ndf_test['child_percent'] = df_test['hogar_nin'] \/ df_test['hogar_total']\ndf_test['elder_percent'] = df_test['hogar_mayor'] \/ df_test['hogar_total']\ndf_test['adult_percent'] = df_test['hogar_adul'] \/ df_test['hogar_total']\ndf_test['males_younger_12_years_percent'] = df_test['r4h1'] \/ df_test['hogar_total']\ndf_test['males_older_12_years_percent'] = df_test['r4h2'] \/ df_test['hogar_total']\ndf_test['males_percent'] = df_test['r4h3'] \/ df_test['hogar_total']\ndf_test['females_younger_12_years_percent'] = df_test['r4m1'] \/ df_test['hogar_total']\ndf_test['females_older_12_years_percent'] = df_test['r4m2'] \/ df_test['hogar_total']\ndf_test['females_percent'] = df_test['r4m3'] \/ df_test['hogar_total']\ndf_test['persons_younger_12_years_percent'] = df_test['r4t1'] \/ df_test['hogar_total']\ndf_test['persons_older_12_years_percent'] = df_test['r4t2'] \/ df_test['hogar_total']\ndf_test['persons_percent'] = df_test['r4t3'] \/ df_test['hogar_total']","57f907cf":"df_train['males_younger_12_years_in_household_size'] = df_train['r4h1'] \/ df_train['hhsize']\ndf_train['males_older_12_years_in_household_size'] = df_train['r4h2'] \/ df_train['hhsize']\ndf_train['males_in_household_size'] = df_train['r4h3'] \/ df_train['hhsize']\ndf_train['females_younger_12_years_in_household_size'] = df_train['r4m1'] \/ df_train['hhsize']\ndf_train['females_older_12_years_in_household_size'] = df_train['r4m2'] \/ df_train['hhsize']\ndf_train['females_in_household_size'] = df_train['r4m3'] \/ df_train['hogar_total']\ndf_train['persons_younger_12_years_in_household_size'] = df_train['r4t1'] \/ df_train['hhsize']\ndf_train['persons_older_12_years_in_household_size'] = df_train['r4t2'] \/ df_train['hhsize']\ndf_train['persons_in_household_size'] = df_train['r4t3'] \/ df_train['hhsize']","877e7f79":"df_test['males_younger_12_years_in_household_size'] = df_test['r4h1'] \/ df_test['hhsize']\ndf_test['males_older_12_years_in_household_size'] = df_test['r4h2'] \/ df_test['hhsize']\ndf_test['males_in_household_size'] = df_test['r4h3'] \/ df_test['hhsize']\ndf_test['females_younger_12_years_in_household_size'] = df_test['r4m1'] \/ df_test['hhsize']\ndf_test['females_older_12_years_in_household_size'] = df_test['r4m2'] \/ df_test['hhsize']\ndf_test['females_in_household_size'] = df_test['r4m3'] \/ df_test['hogar_total']\ndf_test['persons_younger_12_years_in_household_size'] = df_test['r4t1'] \/ df_test['hhsize']\ndf_test['persons_older_12_years_in_household_size'] = df_test['r4t2'] \/ df_test['hhsize']\ndf_test['persons_in_household_size'] = df_test['r4t3'] \/ df_test['hhsize']","40da2b76":"df_train['overcrowding_room_and_bedroom'] = (df_train['hacdor'] + df_train['hacapo'])\/2\ndf_test['overcrowding_room_and_bedroom'] = (df_test['hacdor'] + df_test['hacapo'])\/2","3cf7c6a6":"df_train['escolari_age'] = df_train['escolari']\/df_train['age']\ndf_test['escolari_age'] = df_test['escolari']\/df_test['age']\n\ndf_train['age_12_19'] = df_train['hogar_nin'] - df_train['r4t1']\ndf_test['age_12_19'] = df_test['hogar_nin'] - df_test['r4t1']  ","35624951":"df_train['phones-per-capita'] = df_train['qmobilephone'] \/ df_train['tamviv']\ndf_train['tablets-per-capita'] = df_train['v18q1'] \/ df_train['tamviv']\ndf_train['rooms-per-capita'] = df_train['rooms'] \/ df_train['tamviv']\ndf_train['rent-per-capita'] = df_train['v2a1'] \/ df_train['tamviv']","c100c1eb":"df_test['phones-per-capita'] = df_test['qmobilephone'] \/ df_test['tamviv']\ndf_test['tablets-per-capita'] = df_test['v18q1'] \/ df_test['tamviv']\ndf_test['rooms-per-capita'] = df_test['rooms'] \/ df_test['tamviv']\ndf_test['rent-per-capita'] = df_test['v2a1'] \/ df_test['tamviv']","9bea094e":"(df_train['hogar_total'] == df_train['r4t3']).sum()","ff119e8b":"feature_lst = ['v2a1', 'rooms', 'bedrooms', 'v18q1', 'qmobilephone', 'rez_esc']\nfamily_size_features = ['adult', 'hogar_adul', 'hogar_mayor', 'hogar_nin', 'hogar_total', 'r4h1', \n                        'r4h2', 'r4h3', 'r4m1', 'r4m2', 'r4m3', 'r4t1', 'r4t2', 'r4t3', 'hhsize']\nnew_feats = [] \n\nfor f in feature_lst:\n    new_feats = []\n    for col in family_size_features:\n        new_col_name = 'new_{}_per_{}'.format(f, col)\n        new_feats.append(new_col_name)\n        df_train[new_col_name] = df_train[f] \/ df_train[col]\n        df_test[new_col_name] = df_test[f] \/ df_test[col]\n\n    for feat in new_feats:\n        df_train[feat].replace([np.inf], np.nan, inplace = True)\n        df_train[feat].fillna(0, inplace = True)\n\n        df_test[feat].replace([np.inf], np.nan, inplace = True)\n        df_test[feat].fillna(0, inplace = True)      ","2e065f94":"df_train['rez_esc_age'] = df_train['rez_esc'] \/ df_train['age']\ndf_train['rez_esc_escolari'] = df_train['rez_esc'] \/ df_train['escolari']\n\ndf_test['rez_esc_age'] = df_test['rez_esc'] \/ df_test['age']\ndf_test['rez_esc_escolari'] = df_test['rez_esc'] \/ df_test['escolari']","8ddb772f":"print(df_train.shape, df_test.shape)","4b2a3bd7":"df_train['tabulet_x_qmobilephone'] = df_train['v18q1'] * df_train['qmobilephone']\ndf_test['tabulet_x_qmobilephone'] = df_test['v18q1'] * df_test['qmobilephone']","bfcf2aaa":"# wall and roof\nfor col1 in ['epared1', 'epared2', 'epared3']:\n    for col2 in ['etecho1', 'etecho2', 'etecho3']:\n        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n        df_train[new_col_name] = df_train[col1] * df_train[col2]\n        df_test[new_col_name] = df_test[col1] * df_test[col2]\n        \n# wall and floor\nfor col1 in ['epared1', 'epared2', 'epared3']:\n    for col2 in ['eviv1', 'eviv2', 'eviv3']:\n        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n        df_train[new_col_name] = df_train[col1] * df_train[col2]\n        df_test[new_col_name] = df_test[col1] * df_test[col2]\n\n# roof and floor\nfor col1 in ['etecho1', 'etecho2', 'etecho3']:\n    for col2 in ['eviv1', 'eviv2', 'eviv3']:\n        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n        df_train[new_col_name] = df_train[col1] * df_train[col2]\n        df_test[new_col_name] = df_test[col1] * df_test[col2]","07389bd2":"for col1 in ['epared1', 'epared2', 'epared3']:\n    for col2 in ['etecho1', 'etecho2', 'etecho3']:\n        for col3 in ['eviv1', 'eviv2', 'eviv3']:\n            new_col_name = 'new_{}_x_{}_x_{}'.format(col1, col2, col3)\n            df_train[new_col_name] = df_train[col1] * df_train[col2] * df_train[col3]\n            df_test[new_col_name] = df_test[col1] * df_test[col2] * df_train[col3]","c350a6f5":"print(df_train.shape, df_test.shape)","75471a16":"for col1 in ['public', 'planpri', 'noelec', 'coopele']:\n    for col2 in ['energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4']:\n        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n        df_train[new_col_name] = df_train[col1] * df_train[col2]\n        df_test[new_col_name] = df_test[col1] * df_test[col2]","c17fe656":"for col1 in ['sanitario1', 'sanitario2', 'sanitario3', 'sanitario5', 'sanitario6']:\n    for col2 in ['elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', 'elimbasu5', 'elimbasu6']:\n        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n        df_train[new_col_name] = df_train[col1] * df_train[col2]\n        df_test[new_col_name] = df_test[col1] * df_test[col2]","dffb6ff9":"for col1 in ['abastaguadentro', 'abastaguafuera', 'abastaguano']:\n    for col2 in ['sanitario1', 'sanitario2', 'sanitario3', 'sanitario5', 'sanitario6']:\n        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n        df_train[new_col_name] = df_train[col1] * df_train[col2]\n        df_test[new_col_name] = df_test[col1] * df_test[col2]","459eaecd":"print(df_train.shape, df_test.shape)","95975bdd":"for col1 in ['area1', 'area2']:\n    for col2 in ['instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9']:\n        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n        df_train[new_col_name] = df_train[col1] * df_train[col2]\n        df_test[new_col_name] = df_test[col1] * df_test[col2]","3be37c17":"for col1 in ['lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6']:\n    for col2 in ['instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9']:\n        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n        df_train[new_col_name] = df_train[col1] \/ df_train[col2]\n        df_test[new_col_name] = df_test[col1] \/ df_test[col2]","31fb931f":"print(df_train.shape, df_test.shape)","3c6a0c3e":"df_train['electronics'] = df_train['computer'] * df_train['mobilephone'] * df_train['television'] * df_train['v18q'] * df_train['refrig']\ndf_test['electronics'] = df_test['computer'] * df_test['mobilephone'] * df_test['television'] * df_test['v18q'] * df_test['refrig']\n\ndf_train['no_appliances'] = df_train['refrig'] + df_train['computer'] + df_train['television'] + df_train['mobilephone']\ndf_test['no_appliances'] = df_test['refrig'] + df_test['computer'] + df_test['television'] + df_test['mobilephone']","f06ad04a":"for col1 in ['paredblolad', 'paredzocalo', 'paredpreb', 'pareddes', 'paredmad', 'paredzinc', 'paredfibras', 'paredother']:\n    for col2 in ['pisomoscer', 'pisocemento', 'pisoother', 'pisonatur', 'pisonotiene', 'pisomadera']:\n        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n        df_train[new_col_name] = df_train[col1] * df_train[col2]\n        df_test[new_col_name] = df_test[col1] * df_test[col2]\n\nfor col1 in ['pisomoscer', 'pisocemento', 'pisoother', 'pisonatur', 'pisonotiene', 'pisomadera']:\n    for col2 in ['techozinc', 'techoentrepiso', 'techocane', 'techootro']:\n        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n        df_train[new_col_name] = df_train[col1] * df_train[col2]\n        df_test[new_col_name] = df_test[col1] * df_test[col2]\n        \nfor col1 in ['paredblolad', 'paredzocalo', 'paredpreb', 'pareddes', 'paredmad', 'paredzinc', 'paredfibras', 'paredother']:\n    for col2 in ['techozinc', 'techoentrepiso', 'techocane', 'techootro']:\n        new_col_name = 'new_{}_x_{}'.format(col1, col2)\n        df_train[new_col_name] = df_train[col1] * df_train[col2]\n        df_test[new_col_name] = df_test[col1] * df_test[col2]        \n        \nfor col1 in ['paredblolad', 'paredzocalo', 'paredpreb', 'pareddes', 'paredmad', 'paredzinc', 'paredfibras', 'paredother']:\n    for col2 in ['pisomoscer', 'pisocemento', 'pisoother', 'pisonatur', 'pisonotiene', 'pisomadera']:\n        for col3 in ['techozinc', 'techoentrepiso', 'techocane', 'techootro']:\n            new_col_name = 'new_{}_x_{}_x_{}'.format(col1, col2, col3)\n            df_train[new_col_name] = df_train[col1] * df_train[col2] * df_train[col3]\n            df_test[new_col_name] = df_test[col1] * df_test[col2] * df_train[col3]","90a32e3a":"print(df_train.shape, df_test.shape)","38621862":"cols_with_only_one_value = []\nfor col in df_train.columns :\n    if col == 'Target':\n        continue\n    if df_train[col].value_counts().shape[0] == 1 or df_test[col].value_counts().shape[0] ==1 :\n        print(col)\n        cols_with_only_one_value.append(col)","4117d91e":"df_train.drop(cols_with_only_one_value, axis = 1, inplace = True)\ndf_test.drop(cols_with_only_one_value, axis = 1, inplace = True)","011a255b":"cols_train = np.array(sorted([col for col in df_train.columns if col != 'Target']))\ncols_test = np.array(sorted(df_test.columns))","3685f7b9":"(cols_train == cols_test).sum() == len(cols_train)","6576b8ef":"def max_min(x):\n    return x.max() - x.min()","171f5735":"agg_train = pd.DataFrame()\nagg_test = pd.DataFrame()\n\nfor item in tqdm(family_size_features):\n    for i, function in enumerate(['mean', 'std', 'min', 'max', 'sum', 'count', max_min]):\n        group_train = df_train[item].groupby(df_train['idhogar']).agg(function)\n        group_test = df_test[item].groupby(df_test['idhogar']).agg(function)\n        \n        if i == 6:\n            new_col = item + '_new' + 'max_min'\n        else: \n            new_col = item + '_new' + function\n        \n        agg_train[new_col] = group_train\n        agg_test[new_col] = group_test\n        \nprint('new aggregate train set has {} row, and {} features'.format(agg_train.shape[0], agg_train.shape[1]))\nprint('new aggregate test set has {} row, and {} features'.format(agg_test.shape[0], agg_test.shape[1]))","9d73a0d7":"aggr_list = ['rez_esc', 'dis', 'male', 'female', \n                  'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n                  'parentesco2', 'parentesco3', 'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9', 'parentesco10', \n                  'parentesco11', 'parentesco12',\n                  'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9',\n                 'epared1', 'epared2', 'epared3', 'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', 'refrig', 'television', 'mobilephone',\n            'area1', 'area2', 'v18q', 'edjef']\n\nfor item in tqdm(aggr_list):\n    for function in ['count', 'sum']:\n        group_train = df_train[item].groupby(df_train['idhogar']).agg(function)\n        group_test = df_test[item].groupby(df_test['idhogar']).agg(function)\n        new_col = item + '_new' + function\n        \n        agg_train[new_col] = group_train\n        agg_test[new_col] = group_test\n\nprint('new aggregate train set has {} rows, and {} features'.format(agg_train.shape[0], agg_train.shape[1]))\nprint('new aggregate test set has {} rows, and {} features'.format(agg_test.shape[0], agg_test.shape[1]))","5d74b80c":"aggr_list = ['escolari', 'age', 'escolari_age', 'dependency', 'bedrooms', 'overcrowding', 'rooms', 'qmobilephone', 'v18q1']\n\nfor item in tqdm(aggr_list):\n    for function in ['mean','std','min','max','sum', 'count', max_min]:\n        group_train = df_train[item].groupby(df_train['idhogar']).agg(function)\n        group_test = df_test[item].groupby(df_test['idhogar']).agg(function)\n        if i == 6:\n            new_col = item + '_new2_' + 'max_min'\n        else:\n            new_col = item + '_new2_' + function\n        agg_train[new_col] = group_train\n        agg_test[new_col] = group_test\n\nprint('new aggregate train set has {} rows, and {} features'.format(agg_train.shape[0], agg_train.shape[1]))\nprint('new aggregate test set has {} rows, and {} features'.format(agg_test.shape[0], agg_test.shape[1]))","7e184017":"agg_test = agg_test.reset_index()\nagg_train = agg_train.reset_index()\n\ntrain_agg = pd.merge(df_train, agg_train, on='idhogar')\ntest = pd.merge(df_test, agg_test, on='idhogar')\n\n#fill all na as 0\ntrain_agg.fillna(value=0, inplace=True)\ntest.fillna(value=0, inplace=True)\n\nprint('train shape:', train_agg.shape, 'test shape:', test.shape)","dfebe833":"aggr_list = ['rez_esc', 'dis', 'male', 'female', \n                  'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n                  'parentesco2', 'parentesco3', 'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9', 'parentesco10', \n                  'parentesco11', 'parentesco12',\n                  'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9',\n                 'epared1', 'epared2', 'epared3', 'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', 'refrig', 'television', 'mobilephone',\n            'area1', 'area2', 'v18q', 'edjef']\n    \n# dataframe indexing as \"list + list\" \nfor lugar in ['lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6']:\n    group_train = df_train[[lugar, 'idhogar'] + aggr_list].groupby([lugar, 'idhogar']).sum().reset_index()\n    group_train.columns = [lugar, 'idhogar'] + ['new3_{}_idhogar_{}'.format(lugar, col) for col in group_train][2:]\n\n    group_test = df_test[[lugar, 'idhogar'] + aggr_list].groupby([lugar, 'idhogar']).sum().reset_index()\n    group_test.columns = [lugar, 'idhogar'] + ['new3_{}_idhogar_{}'.format(lugar, col) for col in group_test][2:]\n\n    train_agg = pd.merge(train_agg, group_train, on=[lugar, 'idhogar'])\n    test = pd.merge(test, group_test, on=[lugar, 'idhogar'])\n    \nprint('train shape:', train_agg.shape, 'test shape:', test.shape)","02ebd3e0":"aggr_list = ['rez_esc', 'dis', 'male', 'female', \n                  'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n                  'parentesco2', 'parentesco3', 'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9', 'parentesco10', \n                  'parentesco11', 'parentesco12',\n                  'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9',\n                 'epared1', 'epared2', 'epared3', 'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', 'refrig', 'television', 'mobilephone',\n            'area1', 'area2', 'v18q', 'edjef']\n    \nfor lugar in ['lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6']:\n    group_train = df_train[[lugar, 'idhogar'] + aggr_list].groupby([lugar, 'idhogar']).sum().reset_index()\n    group_train.columns = [lugar, 'idhogar'] + ['new4_{}_idhogar_{}'.format(lugar, col) for col in group_train][2:]\n\n    group_test = df_test[[lugar, 'idhogar'] + aggr_list].groupby([lugar, 'idhogar']).sum().reset_index()\n    group_test.columns = [lugar, 'idhogar'] + ['new4_{}_idhogar_{}'.format(lugar, col) for col in group_test][2:]\n\n    train_agg = pd.merge(train_agg, group_train, on=[lugar, 'idhogar'])\n    test = pd.merge(test, group_test, on=[lugar, 'idhogar'])\n    \nprint('train shape:', train_agg.shape, 'test shape:', test.shape)","ba277286":"cols_nums = ['age', 'meaneduc', 'dependency', \n             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n             'bedrooms', 'overcrowding']\n\nfor function in tqdm(['mean','std','min','max','sum', 'count', max_min]):\n    for lugar in ['lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6']:\n        group_train = df_train[[lugar, 'idhogar'] + aggr_list].groupby([lugar, 'idhogar']).agg(function).reset_index()\n        group_train.columns = [lugar, 'idhogar'] + ['new5_{}_idhogar_{}_{}'.format(lugar, col, function) for col in group_train][2:]\n\n        group_test = df_test[[lugar, 'idhogar'] + aggr_list].groupby([lugar, 'idhogar']).agg(function).reset_index()\n        group_test.columns = [lugar, 'idhogar'] + ['new5_{}_idhogar_{}_{}'.format(lugar, col, function) for col in group_test][2:]\n\n        train_agg = pd.merge(train_agg, group_train, on=[lugar, 'idhogar'])\n        test = pd.merge(test, group_test, on=[lugar, 'idhogar'])\n        \nprint('train shape:', train_agg.shape, 'test shape:', test.shape)","2b85088d":"train = train_agg.query('parentesco1==1')","26922df0":"train['dependency'].replace(np.inf, 0, inplace=True)\ntest['dependency'].replace(np.inf, 0, inplace=True)","cb0f4102":"submission = test[['Id']]\n\n#Remove useless feature to reduce dimension\ntrain.drop(columns=['idhogar','Id', 'agesq', 'hogar_adul', 'SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned'], inplace=True)\ntest.drop(columns=['idhogar','Id',  'agesq', 'hogar_adul', 'SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned'], inplace=True)\n\ncorrelation = train.corr()\ncorrelation = correlation['Target'].sort_values(ascending=False)","a8f159a3":"print('final_data size', train.shape, test.shape)","4baf4377":"print(f'The most positive feature: \\n{correlation.head()}')","199f92c8":"print(f'The most negative feature: \\n{correlation.tail()}')","0a9b6378":"binary_cat_features = [col for col in train.columns if train[col].value_counts().shape[0] == 2]\nobject_features = ['edjefe', 'edjefa']\n\ncategorical_feats = binary_cat_features + object_features","649342dd":"def evaluate_macroF1_lgb(truth, predictions):  \n    # this follows the discussion in https:\/\/github.com\/Microsoft\/LightGBM\/issues\/1483\n    pred_labels = predictions.reshape(len(np.unique(truth)),-1).argmax(axis=0)\n    f1 = f1_score(truth, pred_labels, average='macro')\n    return ('macroF1', f1, True) ","e524ead9":"y = train['Target']\ntrain.drop(columns = ['Target'], inplace = True)","5ce94ba9":"def print_execution_time(start):\n    end = time.time()\n    hours, rem = divmod(end-start, 3600)\n    minutes, seconds = divmod(rem, 60)\n    print('*'*20, \"Execution ended in {:0>2}h {:0>2}m {:05.2f}s\".format(int(hours),int(minutes),seconds), '*'*20)","d8aab7d3":"def extract_good_features_using_shap_LGB(params, SEED):\n    clf = lgb.LGBMClassifier(objective='multiclass',\n                             random_state=1989,\n                             max_depth=params['max_depth'], \n                             learning_rate=params['learning_rate'],  \n                             silent=True, \n                             metric='multi_logloss',\n                             n_jobs=-1, n_estimators=10000, \n                             class_weight='balanced',\n                             colsample_bytree = params['colsample_bytree'], \n                             min_split_gain= params['min_split_gain'], \n                             bagging_freq = params['bagging_freq'],\n                             min_child_weight=params['min_child_weight'],\n                             num_leaves = params['num_leaves'], \n                             subsample = params['subsample'],\n                             reg_alpha= params['reg_alpha'],\n                             reg_lambda= params['reg_lambda'],\n                             num_class=len(np.unique(y)),\n                             bagging_seed=SEED,\n                             seed=SEED,\n                            )\n\n    kfold = 5\n    kf = StratifiedKFold(n_splits=kfold, shuffle=True)\n    feat_importance_df  = pd.DataFrame()\n\n    for i, (train_index, test_index) in enumerate(kf.split(train, y)):\n        print('='*30, '{} of {} folds'.format(i+1, kfold), '='*30)\n        start = time.time()\n        X_train, X_val = train.iloc[train_index], train.iloc[test_index]\n        y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n        clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], eval_metric=evaluate_macroF1_lgb, categorical_feature=categorical_feats,\n                early_stopping_rounds=100, verbose=500)\n        shap_values = shap.TreeExplainer(clf.booster_).shap_values(X_train)\n        fold_importance_df  = pd.DataFrame()\n        fold_importance_df['feature'] = X_train.columns\n        fold_importance_df['shap_values'] = abs(np.array(shap_values)[:, :].mean(1).mean(0))\n        fold_importance_df['feat_imp'] = clf.feature_importances_\n        feat_importance_df = pd.concat([feat_importance_df, fold_importance_df])\n        print_execution_time(start)\n\n    feat_importance_df_shap = feat_importance_df.groupby('feature').mean().sort_values('shap_values', ascending=False).reset_index()\n#     feat_importance_df_shap['shap_cumsum'] = feat_importance_df_shap['shap_values'].cumsum() \/ feat_importance_df_shap['shap_values'].sum()\n#     good_features = feat_importance_df_shap.loc[feat_importance_df_shap['shap_cumsum'] < 0.999].feature\n    return feat_importance_df_shap","0329ab87":"total_shap_df  = pd.DataFrame()\nNUM_ITERATIONS = 10\nfor SEED in range(NUM_ITERATIONS):\n    print('#'*40, '{} of {} iterations'.format(SEED+1, NUM_ITERATIONS), '#' * 40)\n    params = {'max_depth': np.random.choice([5, 6, 7, 8, 10, 12, -1]),\n             'learning_rate': np.random.rand() * 0.02,\n              'colsample_bytree': np.random.rand() * (1 - 0.5) + 0.5,\n              'subsample': np.random.rand() * (1 - 0.5) + 0.5,\n              'min_split_gain': np.random.rand() * 0.2,\n              'num_leaves': np.random.choice([32, 48, 64]),\n              'reg_alpha': np.random.rand() * 2,\n              'reg_lambda': np.random.rand() *2,\n              'bagging_freq': np.random.randint(4) +1,\n              'min_child_weight': np.random.randint(100) + 20\n             }\n    temp_shap_df = extract_good_features_using_shap_LGB(params, SEED)\n    total_shap_df = pd.concat([total_shap_df, temp_shap_df])","d1f9e6b3":"shap_sorted_df = total_shap_df.groupby('feature').mean().sort_values('shap_values', ascending=False).reset_index()\nfeat_imp_sorted_df = total_shap_df.groupby('feature').mean().sort_values('feat_imp', ascending=False).reset_index()\nfeatures_top_shap = shap_sorted_df['feature'][:500]\nfeatures_top_feat_imp = feat_imp_sorted_df['feature'][:500]\ntop_features = pd.Series(features_top_shap.tolist() + features_top_feat_imp.tolist())\ntop_features = top_features.unique()","e98e4f89":"new_train = train[top_features].copy()\nnew_test = test[top_features].copy()","0979d8a5":"print('new_train shape:', new_train.shape, 'new_test shape:', new_test.shape)","78dbc0b7":"new_categorical_feats = [col for col in top_features if col in categorical_feats]","d5fd3615":"def LGB_OOF(params, categorical_feats, N_FOLDs, SEED=1989):\n    clf = lgb.LGBMClassifier(objective='multiclass',\n                             random_state=1989,\n                             max_depth=params['max_depth'], \n                             learning_rate=params['learning_rate'],  \n                             silent=True, \n                             metric='multi_logloss',\n                             n_jobs=-1, n_estimators=10000, \n                             class_weight='balanced',\n                             colsample_bytree = params['colsample_bytree'], \n                             min_split_gain= params['min_split_gain'], \n                             bagging_freq = params['bagging_freq'],\n                             min_child_weight=params['min_child_weight'],\n                             num_leaves = params['num_leaves'], \n                             subsample = params['subsample'],\n                             reg_alpha= params['reg_alpha'],\n                             reg_lambda= params['reg_lambda'],\n                             num_class=len(np.unique(y)),\n                             bagging_seed=SEED,\n                             seed=SEED,\n                            )\n\n    kfold = 10\n    kf = StratifiedKFold(n_splits=kfold, shuffle=True)\n    feat_importance_df  = pd.DataFrame()\n    predicts_result = []\n\n    for i, (train_index, test_index) in enumerate(kf.split(new_train, y)):\n        print('='*30, '{} of {} folds'.format(i+1, kfold), '='*30)\n        start = time.time()\n        X_train, X_val = new_train.iloc[train_index], new_train.iloc[test_index]\n        y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n        clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], eval_metric=evaluate_macroF1_lgb,categorical_feature=new_categorical_feats,\n                early_stopping_rounds=100, verbose=500)\n        shap_values = shap.TreeExplainer(clf.booster_).shap_values(X_train)\n        fold_importance_df  = pd.DataFrame()\n        fold_importance_df['feature'] = X_train.columns\n        fold_importance_df['shap_values'] = abs(np.array(shap_values)[:, :].mean(1).mean(0))\n        fold_importance_df['feat_imp'] = clf.feature_importances_\n        feat_importance_df = pd.concat([feat_importance_df, fold_importance_df])\n        predicts_result.append(clf.predict(new_test))\n        print_execution_time(start)\n    return predicts_result, feat_importance_df","eab41c50":"params = {'max_depth': 6,\n         'learning_rate': 0.002,\n          'colsample_bytree': 0.8,\n          'subsample': 0.8,\n          'min_split_gain': 0.02,\n          'num_leaves': 48,\n          'reg_alpha': 0.04,\n          'reg_lambda': 0.073,\n          'bagging_freq': 2,\n          'min_child_weight': 40\n         }\n\nN_Folds = 20\nSEED = 1989\npredicts_result, feat_importance_df = LGB_OOF(params, new_categorical_feats, N_Folds, SEED=1989)","66888e38":"fig, ax = plt.subplots(1, 2, figsize=(20, 20))\nfeat_importance_df_shap = feat_importance_df.groupby('feature').mean().sort_values('shap_values', ascending=False).reset_index()\n\nnum_features = 50\nsns.barplot(x=feat_importance_df_shap.shap_values[:num_features], y=feat_importance_df_shap.feature[:num_features], ax=ax[0])\nax[0].set_title('Feature importance based on shap values')\n\nfeat_importance_df = feat_importance_df.groupby('feature').mean().sort_values('feat_imp', ascending=False).reset_index()\n\nnum_features = 50\nsns.barplot(x=feat_importance_df.shap_values[:num_features], y=feat_importance_df.feature[:num_features], ax=ax[1])\nax[1].set_title('Feaure importance based on feature importance from lgbm')\nplt.show()","08d69c23":"submission['Target'] = np.array(predicts_result).mean(axis=0).round().astype(int)\nsubmission.to_csv('submission.csv', index = False)","44d35b34":"optimized_param = None\nlowest_cv = 1000\ntotal_iteration = 50\nfor i in range(total_iteration):\n    print('-'*20, 'For {} of {} iterations'.format(i+1, total_iteration), '-'*20)\n    learning_rate = np.random.rand() * 0.02\n    n_folds = 3\n\n    num_class = len(np.unique(y))\n\n    params = {}\n    params['application'] = 'multiclass'\n    params['metric'] = 'multi_logloss'\n    params['num_class'] = num_class\n    params['class_weight'] = 'balanced'\n    params['num_leaves'] = np.random.randint(24, 48)\n    params['max_depth'] = np.random.randint(5, 8)\n    params['min_child_weight'] = np.random.randint(5, 50)\n    params['min_split_gain'] = np.random.rand() * 0.09\n    params['colsample_bytree'] = np.random.rand() * (0.9 - 0.1) + 0.1\n    params['subsample'] = np.random.rand() * (1 - 0.8) + 0.8\n    params['bagging_freq'] = np.random.randint(1, 5)\n    params['bagging_seed'] = np.random.randint(1, 5)\n    params['reg_alpha'] = np.random.rand() * 2\n    params['reg_lambda'] = np.random.rand() * 2\n    params['learning_rate'] = np.random.rand() * 0.02\n    params['seed']  =1989\n\n    d_train = lgb.Dataset(data=new_train, label=y.values-1, categorical_feature=new_categorical_feats, free_raw_data=False)\n    cv_results = lgb.cv(params=params, train_set=d_train, num_boost_round=10000, categorical_feature=new_categorical_feats,\n                        nfold=n_folds, stratified=True, shuffle=True, early_stopping_rounds=1, verbose_eval=1000)\n\n    min_cv_results = min(cv_results['multi_logloss-mean'])\n\n    if min_cv_results < lowest_cv:\n        lowest_cv = min_cv_results\n        optimized_param = params","12fc20ed":"N_Folds = 20\nSEED = 1989\npredicts_result, feat_importance_df = LGB_OOF(optimized_param, new_categorical_feats, N_Folds, SEED=1989)","dcbc25d3":"submission['Target'] = np.array(predicts_result).mean(axis=0).round().astype(int)\nsubmission.to_csv('submission_shap_randomized_search.csv', index = False)","bf990cad":"* Below cell is from this kernel\n\n#### url: https:\/\/www.kaggle.com\/skooch\/lgbm-w-random-split-2","f5d9588c":"## 1. Check datasets\n\n### 1.1 Read dataset","bc246b1a":"### preprocessing for family features\n#### 'v2a1', 'rooms', 'bedrooms', 'v18q1', 'qmobilephone', 'rez_esc'\n\n\n* I will reduce the number of features using shape, so let's generate many features.","a0cfa27a":"Welcome to another Kaggle challenge! The objective of the Costa Rican Household Poverty Level Prediction contest is to develop a machine learning model that can predict the poverty level of households using both individual and household characteristics. This \"data science for good\" project offers the opportunity to put our skills towards a task more beneficial to society than getting people to click on ads!\n\nIn this notebook, we will walk through a complete machine learning solution: first, get introduced to the problem, then perform a thorough Exploratory Data Analysis of the dataset, work on feature engineering, try out multiple machine learning models, select a model, work to optimize the model, and finally, inspect the outputs of the model and draw conclusions. __While this notebook may not get us to the top of the leaderboard, it is meant to be used as a teaching tool to give you a solid foundation to build on for future machine learning projects. Kaggle projects can teach us a lot about machine learning, but several of the strategies used to get to the very top of the leaderboard are not best practices, so here we'll stick to building a very good - although not quite first place - machine learning solution. While Kaggle projects are competitions, I think they are best described as \"a machine learning education\" disguised as a contest!\"\n\nIf you are looking to follow-up on this work, I have additional work including a [kernel on using Automated Feature Engineering](https:\/\/www.kaggle.com\/willkoehrsen\/featuretools-for-good) with [Featuretools](https:\/\/featuretools.alteryx.com\/en\/stable\/#minute-quick-start) for this problem (with slightly higher leaderboard score). (If you enjoy my writing style and explanations, I write for [Towards Data Science](https:\/\/williamkoehrsen.medium.com\/)\n\n### Problem and Data Explanation\n\nThe data for this competition is provided in two files: train.csv and test.csv. The training set has 9557 rows and 143 columns while the testing set has 23856 rows and 142 columns. **Each row represents one individual and each column is a feature, either unique to the individual, or for the household of the individual.** The training set has one additional column, Target, which represents the poverty level on a 1-4 scale and is the label for the competition. A value of 1 is the most extreme poverty.\n\n**This is a supervised multi-class classification machine learning problem:**\n\n* Supervised: provided with the labels for the training data\n* Multi-class classification: Labels are discrete values with 4 classes\n\n### Objective\n\nThe objective is to predict poverty on a **household level.** We are given data on the individual level with each individual having unique features but also information about their household. In order to create a dataset for the task, we'll have to perform some aggregations of the individual data for each household. Moreover, we have to make a prediction for every individual in the test set, but \"ONLY the heads of household are used in scoring\" which means we want to predict poverty on a household basis.\n\n**Important note: while all members of a household should have the same label in the training data, there are errors where individuals in the same household have different labels. In these cases, we are told to use the label for the head of each household, which can be identified by the rows where parentesco1 == 1.0.**\n\nWe will cover how to correct this in the notebook.\n\nThe Target values represent poverty levels as follows:\n\n* 1 = extreme poverty \n* 2 = moderate poverty \n* 3 = vulnerable households \n* 4 = non vulnerable households\n\nThe explanations for all 143 columns can be found in the competition documentation, but a few to note are below:\n\n* Id: a unique identifier for each individual, this should not be a feature that we use!\n* idhogar: a unique identifier for each household. This variable is not a feature, but will be used to group individuals by household as all individuals in a household will have the same identifier.\n* parentesco1: indicates if this person is the head of the household.\n* Target: the label, which should be equal for all members in a household\n\nWhen we make a model, we'll train on a household basis with the label for each household the poverty level of the head of household. \n\nThe raw data contains a mix of both household and individual characteristics and for the individual data, we will have to find a way to aggregate this for each household. Some of the individuals belong to a household with no head of household which means that unfortunately we can't use this data for training. \nThese issues with the data are completely typical of real-world data and hence this problem is great preparation for the datasets you'll encounter in a data science job!\n\n### Metric\n\nUltimately we want to build a machine learning model that can predict the integer poverty level of a household. Our predictions will be assessed by the **Macro F1 Score.** You may be familiar with the [standard F1 score](https:\/\/blog.exsilio.com\/all\/accuracy-precision-recall-f1-score-interpretation-of-performance-measures\/) for binary classification problems which is the harmonic mean of precision and recall:\n\n![image.png](attachment:186fe07b-9cd1-48ad-abde-9fcc939778cd.png)\n \nFor mutli-class problems, we have to average the F1 scores for each class. The macro F1 score averages the F1 score for each class without taking into account label imbalances.\n\n![image.png](attachment:0e3c7508-f06a-4dc3-9822-7da5bdc0b0db.png)\n\nIn other words, the number of occurrences of each label does not figure into the calculation when using macro (while it does when using the \"weighted\" score). If we want to assess our performance, we can use the code:\n\n* from sklearn.metrics import f1_score\n* f1_score(y_true, y_predicted, average = 'macro`)\n\nFor this problem, the labels are imbalanced, which makes it a little strange to use macro averaging for the evaluation metric, but that's a decision made by the organizers and not something we can change! In your own work, you want to be aware of label imbalances and choose a metric accordingly.","e1fbb351":"#### roof and electricity","bcadd872":"#### edjefe\n\n* edjefe : years of education of male head of household, based on the interaction of escolari (years of education), head of household and gender, yes ==1 and no == 0\n* replace yes >> 1 and no >> 0","fa01beda":"### rez_esz, SQBmeaned\n\n* rez_esc : Years behind in school >> filled with 0\n* SQBmeaned : Square of the mean years of education of adults (>=18) in the household agesq, Age squared >> same with rez_esc >> filled with 0","c8c29ca8":"## 2. Feature engineering\n\n### 2.1 Object featrues","b7d5251d":"* Multiply television, mobilephone, computer, tabulet, refrigerator >> electronics features","6278c512":"* I want to mix toilet and water provision features >> water features","1a4c9d90":"#### edjefa\n\n* edjefa : years of education of female head of household, based on the interaction of escolari (years of education), head of household and gender, yes ==1 and no == 0\n* replace yes >> 1 and no >> 0","7e9a2565":"#### Remove feature with only one value","639c8225":"### 1.3 Check Null data","08ea1275":"* Mix region and education","4ed6aaa5":"### Randomized Search","8a59f1b4":"### 2.3 Make new features using continuous feature","43f559f4":"### Rich features\n\n* I think the more richer, the larger number of phones and tabulet","cfda269a":"* Enumerate : \n1. \ubc18\ubcf5\ubb38 \uc0ac\uc6a9 \uc2dc \uba87 \ubc88\uc9f8 \ubc18\ubcf5\ubb38\uc778\uc9c0 \ud655\uc778\uc774 \ud544\uc694\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub54c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n2. \uc778\ub371\uc2a4 \ubc88\ud638\uc640 \uceec\ub809\uc158\uc758 \uc6d0\uc18c\ub97c tuple\ud615\ud0dc\ub85c \ubc18\ud658\ud569\ub2c8\ub2e4.\n-------------------------\n\n* Groupby, agg function:\n1. url: https:\/\/rfriend.tistory.com\/392","5fc5d4d5":"#### Squared features\n\n* There are many squared features. Acually, tree models like lightgbm don't need them. But at this kernel, want to use lighthbm as filter model and set entity-embedding as classifier. So let's keep them.\n\n### Family features\n\n* hogar_nin, hogar_adul, hogar_mayor, hogar_total, r4h1, r4h2, r4h3, r4m1, r4m2, r4m3, r4t1, r4t2, r4t3, tmbhog, tamvid, rez_esc, escolari\n\n* Family size features (subtract, ratio)","fbfb5e7d":"* I want to mix education and area features >> education_zone_features","def33aa9":"## 4. Model Development","f5e21e4a":"#### Dependency","1a756adf":"### 2.4 Aggregation features\n\n* In this competition, each samples are member of specific household(idhogar). So let's aggregate based on 'idhogar' values.","17052305":"* wall, roof, floor may be key factor.\n* Let's multiply each of them. Becuase they are binary cat features, so mulitification of each features generates new categorical features","552e41d7":"### 1.2 Make description of","b2dacee0":"* I want to mix toilet and rubbish disposal features >> other_infra features","b9ffd76f":"#### lightGBM parameters\n\nurl : https:\/\/gorakgarak.tistory.com\/1285\n\n* max_depth : \ub098\ubb34\uc758 \uae4a\uc774. \ub2e8\uc77c \uacb0\uc815\ub098\ubb34\uc5d0\uc11c\ub294 \ucda9\ubd84\ud788 \ub370\uc774\ud130\ub97c \uace0\ub824\ud558\uae30 \uc704\ud574 depth\ub97c \uc801\ub2f9\ud55c \uae4a\uc774\ub85c \ub9cc\ub4e4\uc9c0\ub9cc, \ubd80\uc2a4\ud305\uc5d0\uc11c\ub294 \uae4a\uc774 \ud558\ub098\uc9dc\ub9ac\ub3c4 \ub9cc\ub4dc\ub294 \ub4f1, \uae4a\uc774\uac00 \uc9e7\uc740\uac83\uc774 \ud06c\ub9ac\ud2f0\uceec\ud558\uc9c0 \uc54a\ub2e4. \uc5b4\ucc28\ud53c \ubcf4\uc815\ub418\ub2c8\uae4c..\n\n* min_data_in_leaf : \ud55c \uc78e\uc0ac\uadc0 \ub178\ub4dc\uc5d0 \ub4e4\uc5b4\uac08\uc218 \uc788\ub294 \ub370\uc774\ud130\uc758 \uac2f\uc218.\n\n* feature_fraction : \ubd80\uc2a4\ud305 \ub300\uc0c1 \ubaa8\ub378\uc774 \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\uc77c\ub54c, \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\ub294 feature\uc758 \uc77c\ubd80\ub9cc\uc744 \uc120\ud0dd\ud558\uc5ec \ud6c8\ub828\ud558\ub294\ub370, \uc774\ub97c \ud1b5\uc81c\ud558\uae30 \uc704\ud55c \ud30c\ub77c\ubbf8\ud130.\n\n* bagging_fraction : \ub370\uc774\ud130\uc758 \uc77c\ubd80\ub9cc\uc744 \uc0ac\uc6a9\ud558\ub294 bagging\uc758 \ube44\uc728\uc774\ub2e4. \uc608\ub97c\ub4e4\uc5b4 \uc624\ubc84\ud53c\ud305\uc744 \ubc29\uc9c0\ud558\uae30 \uc704\ud574 \ub370\uc774\ud130\uc758 \uc77c\ubd80\ub9cc\uc744 \uac00\uc838\uc640\uc11c \ud6c8\ub828\uc2dc\ud0a4\ub294\ub370, \uc774\ub294 \uc624\ubc84\ud53c\ud305\uc744 \ubc29\uc9c0\ud558\uba70 \uc57d\ud55c\uc608\uce21\uae30\ub97c \uc8c4\ub2e4 \ud569\uce60\uacbd\uc6b0\ub294 \uc624\ud788\ub824 \uc608\uce21\uc131\ub2a5\uc774 \uc88b\uc544\uc9c8\uac83\uc774\ub2e4.\n\n* early_stopping_round : \ub354\uc774\uc0c1 validation\ub370\uc774\ud130\uc5d0\uc11c \uc815\ud655\ub3c4\uac00 \uc88b\uc544\uc9c0\uc9c0 \uc54a\uc73c\uba74 \uba48\ucdb0\ubc84\ub9b0\ub2e4. \ud6c8\ub828\ub370\uc774\ud130\ub294 \uac70\uc758 \uc5d0\ub7ec\uc728\uc774 0\uc5d0 \uac00\uae5d\uac8c \uc88b\uc544\uc9c0\uae30 \ub9c8\ub828\uc778\ub370, validation\ub370\uc774\ud130\ub294 \ud6c8\ub828\uc5d0 \uc0ac\uc6a9\ub418\uc9c0 \uc54a\uae30\ub54c\ubb38\uc5d0 \uc77c\uc815\uc774\uc0c1 \uc88b\uc544\uc9c0\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc774\ub2e4. \uc774\ud6c4 \uc9c4\ud589\ud558\uac8c \ub418\uba74 \ucef4\ud4e8\ud130 \uc7ac\ub2a5\ub0ad\ube44\ub2e4.\n\n* lambda : \uc815\uaddc\ud654\uc5d0 \uc0ac\uc6a9\ub418\ub294 \ud30c\ub77c\ubbf8\ud130\n\n* min_gain_to_split : \ucd5c\uc18c \uc815\ubcf4\uc774\ub4dd\uc774 \uc788\uc5b4\uc57c \ubd84\uae30\uac00 \ub418\uac8c\ub054 \ub9cc\ub4e0\ub2e4.\n\n* max_cat_group : \ubc94\uc8fc\ud615 \ubcc0\uc218\uac00 \ub9ce\uc73c\uba74, \ud558\ub098\ub85c \ud241\uccd0\uc11c \ucc98\ub9ac\ud558\uac8c\ub054 \ub9cc\ub4dc\ub294 \ucd5c\uc18c\ub2e8\uc704.\n\n* objective : lightgbm\uc740 regression, binary, multicalss \ubaa8\ub450 \uac00\ub2a5\n\n* boosting: gbdt(gradient boosting decision tree), rf(random forest), dart(dropouts meet multiple additive regression trees), goss(Gradient-based One-Side Sampling)\n\n* num_leaves: \uacb0\uc815\ub098\ubb34\uc5d0 \uc788\uc744 \uc218 \uc788\ub294 \ucd5c\ub300 \uc78e\uc0ac\uadc0 \uc218. \uae30\ubcf8\uac12\uc740 0.31\n\n* learning_rate : \uc0c1\ub2f9\ud788 \uc911\uc694\ud55c \ud30c\ub77c\ubbf8\ud130\uc778\ub370, \uac01 \uc608\uce21\uae30\ub9c8\ub2e4 \uc5bc\ub9c8\ub098 \uac00\uc911\uce58\ub97c \uc8fc\uc5b4 \ud559\uc2b5\ud558\uac8c \ud560\uac83\uc778\uc9c0 \ub9cc\ub4e0\ub2e4. learning_rate\uc740 \uc544\ub798\uc758 num_boost_round\uc640 \uc798 \ub9de\ucdb0\uc838\uc57c \u314e\ub098\ub2e4.\n\n* num_boost_round : boosting\uc744 \uc5bc\ub9c8\ub098 \ub3cc\ub9b4\uc9c0 \uc9c0\uc815\ud55c\ub2e4. \uacbd\ud5d8\uc0c1 \ubcf4\ud1b5 100\uc815\ub3c4\uba74 \ub108\ubb34 \ube60\ub974\uac8c \ub05d\ub098\uba70, \uc2dc\ud5d8\uc6a9\uc774 \uc544\ub2c8\uba74 1000\uc815\ub3c4\ub294 \uc900\ub2e4. \uc5b4\ucc28\ud53c early_stopping_round\uac00 \uc9c0\uc815\ub418\uc5b4\uc788\uc73c\uba74 \ub354\uc774\uc0c1 \uc9c4\uc804\uc774 \uc5c6\uc744\uacbd\uc6b0 \uc54c\uc544\uc11c \uba48\ucd98\ub2e4.\n\n* device : gpu, cpu\n\n* metric: loss\ub97c \uce21\uc815\ud558\uae30 \uc704\ud55c \uae30\uc900. mae (mean absolute error), mse (mean squared error), \ub4f1\ub4f1\uc774 \uc788\ub2e4. \uc0ac\uc2e4 \uc5c4\uccad\ub9ce\ub2e4.\n\n* max_bin : \ucd5c\ub300 bin\n\n* categorical_feature : \ubc94\uc8fc\ud615 \ubcc0\uc218\uac00 \uc788\uc744\ub54c \uc5ec\uae30\uc5d0 \uc5b8\uae09\ud574\uc900\ub2e4. \ub2e4\ub9cc, categorical\uc740 string\uc740 \uc4f8\uc218 \uc5c6\ub294\uac83\uc73c\ub85c \ubcf4\uc778\ub2e4.\n\n* ignore_column : \uceec\ub7fc\uc744 \ubb34\uc2dc\ud55c\ub2e4. \ubb34\uc2dc\ud558\uc9c0 \uc54a\uc744\uacbd\uc6b0 \ubaa8\ub450 training\uc5d0 \ub123\ub294\ub370, \ubb54\uac00 \ub0a8\uaca8\ub193\uc544\uc57c\ud560 \uceec\ub7fc\uc774 \uc788\uc73c\uba74 \uc5ec\uae30\ub2e4\uac00 \uc778\uc790\ub97c \uc900\ub2e4.\n\n* save_binary: True\ub85c \ud574\ub193\uc73c\uba74 \uba54\ubaa8\ub9ac\ub97c \uc544\ub080\ub2e4.","f055d3ff":"* v2a1: number of tablets household owns -> if tipovivi3(rented?) == 1, there are some values. If not, there are also some values.\n\n* NaN value could be replaced by 0.","a68366c8":"* hhsize : household size\n* tamhog : size of the household\n> What is different?\n\n* As you can see, the meaning of two features are same but the exact number are different. Are they different?\n* I don't know. For now, I decided to drop one feature 'tamhog'.","c7c0d0a4":"### 1.4 Fill Missing Values","77ec6144":"* combination using three features","a61e3993":"### meaneduc\n\n* meaneduc: average years of education for adults (18+) >> filled with 0","fa7c23f5":"### 2.2 Extract cat features\n\n* According to data scripts, there are many binary category features.","ef3a096a":"## 4. Feature selection using shap","17e5d86a":"## Costa Rican Household Poverty Level Prediction\n\nGoing to data analysis about 'Costa Rican Household Poverty Level Prediction' dataset.\n\nFollowed the Will Koehrsen and YouHan Lee kernels of the datsset, great thanks to two contributers for sharing the kernels.\n\n* **Will Koerhsen_A Complete Introduction and Walkthrough :** [**URL**](https:\/\/www.kaggle.com\/willkoehrsen\/a-complete-introduction-and-walkthrough\/notebook)\n\n* **YouHan Lee_3250feats->532 feats using shap[LB: 0.436] :** [**URL**](https:\/\/www.kaggle.com\/youhanlee\/3250feats-532-feats-using-shap-lb-0-436#1.-Check-datasets)\n---------------------------------------------","6cdf2495":"* According to data descriptions,ONLY the heads of household are used in scoring. \/\n* All household members are included in test + the sample submission, but only heads of households are scored.","09e4dc1c":"#### v18q1\n\n* v18q1 : number of tablets household owns >> if v18q (do you own a tablet?) == 1, there are some values. If not, only NaN values in v18q1.","d63eaa38":"* I want to mix electricity and energy features >> energy features","d3e602d1":"#### Check whether both train and test have same features","1695079f":"* Mix wall matereial of root, florr, wall"}}