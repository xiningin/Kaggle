{"cell_type":{"f7f62c1b":"code","d5e74380":"code","74a1ccd4":"code","9d6b4d10":"code","8462a10d":"code","bf07c8b7":"code","0acd83a6":"code","029cce8d":"code","bf1037b8":"code","4b016de8":"code","96bb2d8d":"code","3443d7bf":"code","96b77138":"code","e1dab7c3":"code","23f8b3fc":"code","c45406d5":"code","d0311792":"code","88c455dd":"code","2b2b45aa":"code","799041bc":"code","2453fb3f":"markdown","719b1df0":"markdown","26a30f50":"markdown","2f0e2dd4":"markdown","0502cfb3":"markdown","8e3a5f2d":"markdown","32172c73":"markdown","2fc27fba":"markdown","08798cc1":"markdown","25ec342b":"markdown","97c2e9e0":"markdown","9602ae9f":"markdown","a616a821":"markdown","aefdf505":"markdown"},"source":{"f7f62c1b":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score \nfrom sklearn.metrics import classification_report \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV","d5e74380":"data = pd.read_csv('..\/input\/data.csv', index_col=False)\ndata.head(5)","74a1ccd4":"print(data.shape)","9d6b4d10":"print(data.columns)","8462a10d":"print(data.describe)","bf07c8b7":"data['diagnosis']=data['diagnosis'].apply(lambda x: '1' if x=='M' else '0')","0acd83a6":"data.head(10)","029cce8d":"unique_diagnosis=data.groupby('diagnosis').size()\nprint(unique_diagnosis)","bf1037b8":"unique_id=data.groupby('id').size()\nprint(unique_id)","4b016de8":"data.drop(['id','Unnamed: 32'],axis=1)","96bb2d8d":"data.plot(kind='density',layout=(5,7),subplots=True,sharex=False, legend=False, fontsize=1)\nplt.show()","3443d7bf":"data.plot(kind='density', subplots=True, layout=(5,7), sharex=False, legend=False, fontsize=1)\nplt.show()","96b77138":"data.hist(sharex=False,layout=(5,7) ,sharey=False, xlabelsize=1, ylabelsize=1)\nplt.show()","e1dab7c3":"y=data.diagnosis\nx=data.iloc[:,1:32]","23f8b3fc":"import seaborn as sns\nr = x.corr()\nfig, ax = plt.subplots(figsize=(20,20))         # Sample figsize in inches\nsns.heatmap(r, annot=True, linewidths=.5, ax=ax)\n#sns.heatmap(r, annot = True)\nplt.show()","c45406d5":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.3,random_state=7)","d0311792":"seed = 7\nscoring = 'accuracy'\nmodels=[]\nmodels.append(('KNN', KNeighborsClassifier()))\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=5, random_state=seed)\n    cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","88c455dd":"pipelines=[]\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN',\nKNeighborsClassifier())])))\nresults = []\nnames = []\nfor name, model in pipelines:\n    kfold = KFold(n_splits=5, random_state=seed)\n    cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","2b2b45aa":"scaler = StandardScaler().fit(x_train)\nprint(scaler)\nrescaledX = scaler.transform(x_train)\nneighbors = [1,3,5,7,9,11,13,15,17,19,21]\nparam_grid = dict(n_neighbors=neighbors)\nmodel = KNeighborsClassifier(weights='uniform', algorithm='auto',p=2, metric='minkowski')\nkfold = KFold(n_splits=5, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","799041bc":"scaler = StandardScaler().fit(x_train)\nrescaledX = scaler.transform(x_train)\nmodel = KNeighborsClassifier(n_neighbors=13,weights='distance', algorithm='auto',p=2, metric='minkowski')\nkfold = KFold(n_splits=5, random_state=seed)\nmodel.fit(rescaledX, y_train)\nrescaledValidationX = scaler.transform(x_test)\npredictions = model.predict(rescaledValidationX)\nprint(accuracy_score(y_test, predictions))    \nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y_test, predictions))","2453fb3f":"**Data Visulization**\n\nwe plot density plot we can determine shape of our features which represents gaussian distribution ","719b1df0":"**Heat map**\n\nIt looks like there is also some structure in the order of the attributes. The diagonal suggests that attributes that are next to each other are generally more correlated with each other. The black patches also suggest some moderate negative correlation the further\nattributes are away from each other in the ordering. ","26a30f50":"**Shape of Data**","2f0e2dd4":"<font size=\"5\">Prediction of Breast Cancer using KNN with 99% accuracy<\/font>\n\n\nBy using the Breast Cancer Wisconsin (Diagnostic) dataset. We create a KNN classifier that can predict the patients of Breast Cancer with accuracy of 99%.","0502cfb3":"<font size=\"5\">Data Cleaning<\/font>\n\n\nColumn 'Id' and 'Unnamed:32' have no too much impact on the prediction the Breast Cancer Patients. So remove both columns ","8e3a5f2d":"In our dataset column name 'diagnosis' is our target value. Now replace M=1 & B=0","32172c73":"**Tuning KNN**","2fc27fba":"**Validation Dataset**\n\nIt is a good idea to use a validation hold-out set. This is a sample of the data that we hold back from our analysis and modeling. We use it right at the end of our project to confirm the accuracy of our final model. It is a smoke test that we can use to see if we messed up and to give us confidence on our estimates of accuracy on unseen data. We will use 70% of the dataset for modeling and hold back 30% for test data.","08798cc1":"**Describe the dataset**","25ec342b":"**Read the csv file** ","97c2e9e0":"**Finalize Model**","9602ae9f":"* * * * ","a616a821":"We will use 10-fold cross validation. The dataset is not too small and this is\na good standard test harness con\fguration. We will evaluate algorithms using the accuracy\nmetric. This is a gross metric that will give a quick idea of how correct a given model is. More\nuseful on binary classi\fcation problems like this one.","aefdf505":"**Standardize Data**\n\nWe suspect that the differing distributions of the raw data may be negatively impacting the skill of some of the algorithm. Let's evaluate the same algorithm with a standardized copy of the dataset. This is where the data is transformed such that each attribute has a mean value of zero and a standard deviation of one. We also need to avoid data leakage when we transform the data. A good way to avoid leakage is to use pipelines that standardize the data and build the model for each fold in the cross validation test harness. That way we can get a fair estimation of how each model with standardized data might perform on unseen data."}}