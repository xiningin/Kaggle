{"cell_type":{"d650760f":"code","2858b9a4":"code","fed1c63e":"code","0aa1a82b":"code","6b0c7fbb":"code","000cf063":"code","ec7865dc":"code","b1dd50a6":"code","ebb4935b":"code","99e6bffa":"code","d8e46183":"code","12b767e7":"code","c9cc89d9":"code","ac51002f":"code","87c3ccde":"code","08827e0e":"code","16032eae":"code","1cf626eb":"code","018e171b":"code","d4b3f035":"code","d4975951":"code","2259a671":"code","08b9e9d6":"code","c87a82ee":"code","163107a3":"code","bedb469f":"code","bd28f007":"code","cbf8e8d9":"code","154e7e55":"code","49b327ab":"code","4e9da912":"code","61718036":"code","312a3425":"code","bf21c12b":"markdown","98705b65":"markdown","ae27bccf":"markdown","d11c4360":"markdown","3b413737":"markdown","1a13a555":"markdown","46e8bb4e":"markdown","7693acc6":"markdown","0dea5247":"markdown","4cb5d6b3":"markdown","a599fbf0":"markdown","1856006c":"markdown","edeac875":"markdown","9bc4843a":"markdown","21a823e0":"markdown","b01bc909":"markdown","a04d4850":"markdown","5e224e4d":"markdown"},"source":{"d650760f":"import pandas as pd\ndf = pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')\ndf.head()","2858b9a4":"df.shape","fed1c63e":"df.info()","0aa1a82b":"fig, axarr = plt.subplots(1, 2, figsize=(20, 8))\n\ndf.dtypes.value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',shadow=True,ax=axarr[1])\naxarr[1].set_title(\"type of our data \", fontsize=18)\n\ndf.dtypes.value_counts().plot(kind='bar',ax=axarr[0])\nplt.title('type of our data');\naxarr[0].set_title(\"type of our data \", fontsize=18)","6b0c7fbb":"df['RainToday'].replace({'No': 0, 'Yes': 1},inplace = True)\ndf['RainTomorrow'].replace({'No': 0, 'Yes': 1},inplace = True)","000cf063":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize = (8,5))\nfull_data.RainTomorrow.value_counts(normalize = True).plot(kind='bar', color= ['skyblue','navy'], alpha = 0.9, rot=0)\nplt.title('RainTomorrow Indicator No(0) and Yes(1) in the Imbalanced Dataset')\nplt.show()","ec7865dc":"from sklearn.utils import resample\n\nno = df[df.RainTomorrow == 0]\nyes = df[df.RainTomorrow == 1]\nyes_oversampled = resample(yes, replace=True, n_samples=len(no), random_state=123)\noversampled = pd.concat([no, yes_oversampled])\n\nfig = plt.figure(figsize = (8,5))\noversampled.RainTomorrow.value_counts(normalize = True).plot(kind='bar', color= ['skyblue','navy'], alpha = 0.9, rot=0)\nplt.title('RainTomorrow Indicator No(0) and Yes(1) after Oversampling (Balanced Dataset)')\nplt.show()","b1dd50a6":"# create a table with data missing \nmissing_values=df.isnull().sum() # missing values\n\npercent_missing = df.isnull().sum()\/df.shape[0]*100 # missing value %\n\nvalue = {\n    'missing_values ':missing_values,\n    'percent_missing %':percent_missing , \n     'data type' : df.dtypes\n}\nframe=pd.DataFrame(value)\nframe\n","ebb4935b":"# Missing Data Pattern in Training Data\nimport seaborn as sns\nsns.heatmap(oversampled.isnull(), cbar=False, cmap='PuBu')","99e6bffa":"total = oversampled.isnull().sum().sort_values(ascending=False)\npercent = (oversampled.isnull().sum()\/oversampled.isnull().count()).sort_values(ascending=False)\nmissing = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing.head(4)","d8e46183":"oversampled.select_dtypes(include=['object']).columns","12b767e7":"# Impute categorical var with Mode\noversampled['Date'] = oversampled['Date'].fillna(oversampled['Date'].mode()[0])\noversampled['Location'] = oversampled['Location'].fillna(oversampled['Location'].mode()[0])\noversampled['WindGustDir'] = oversampled['WindGustDir'].fillna(oversampled['WindGustDir'].mode()[0])\noversampled['WindDir9am'] = oversampled['WindDir9am'].fillna(oversampled['WindDir9am'].mode()[0])\noversampled['WindDir3pm'] = oversampled['WindDir3pm'].fillna(oversampled['WindDir3pm'].mode()[0])","c9cc89d9":"# Convert categorical features to continuous features with Label Encoding\nfrom sklearn.preprocessing import LabelEncoder\nlencoders = {}\nfor col in oversampled.select_dtypes(include=['object']).columns:\n    lencoders[col] = LabelEncoder()\n    oversampled[col] = lencoders[col].fit_transform(oversampled[col])","ac51002f":"import warnings\nwarnings.filterwarnings(\"ignore\")\n# Multiple Imputation by Chained Equations\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nMiceImputed = oversampled.copy(deep=True) \nmice_imputer = IterativeImputer()\nMiceImputed.iloc[:, :] = mice_imputer.fit_transform(oversampled)","87c3ccde":"# Detecting outliers with IQR\nQ1 = MiceImputed.quantile(0.25)\nQ3 = MiceImputed.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","08827e0e":"# Removing outliers from the dataset\nMiceImputed = MiceImputed[~((MiceImputed < (Q1 - 1.5 * IQR)) |(MiceImputed > (Q3 + 1.5 * IQR))).any(axis=1)]\nMiceImputed.shape","16032eae":"# Correlation Heatmap\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncorr = MiceImputed.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(20, 20))\ncmap = sns.diverging_palette(250, 25, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=None, center=0,square=True, annot=True, linewidths=.5, cbar_kws={\"shrink\": .9})","1cf626eb":"sns.pairplot( data=MiceImputed, vars=('MaxTemp','MinTemp','Pressure9am','Pressure3pm', 'Temp9am', 'Temp3pm', 'Evaporation'), hue='RainTomorrow' )\n","018e171b":"# Standardizing data\nfrom sklearn import preprocessing\nr_scaler = preprocessing.MinMaxScaler()\nr_scaler.fit(MiceImputed)\nmodified_data = pd.DataFrame(r_scaler.transform(MiceImputed), index=MiceImputed.index, columns=MiceImputed.columns)","d4b3f035":"# Feature Importance using Filter Method (Chi-Square)\nfrom sklearn.feature_selection import SelectKBest, chi2\nX = modified_data.loc[:,modified_data.columns!='RainTomorrow']\ny = modified_data[['RainTomorrow']]\nselector = SelectKBest(chi2, k=10)\nselector.fit(X, y)\nX_new = selector.transform(X)\nprint(X.columns[selector.get_support(indices=True)])","d4975951":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier as rf\n\nX = MiceImputed.drop('RainTomorrow', axis=1)\ny = MiceImputed['RainTomorrow']\nselector = SelectFromModel(rf(n_estimators=100, random_state=0))\nselector.fit(X, y)\nsupport = selector.get_support()\nfeatures = X.loc[:,support].columns.tolist()\nprint(features)\nprint(rf(n_estimators=100, random_state=0).fit(X,y).feature_importances_)","2259a671":"features = MiceImputed[['Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustDir', \n                       'WindGustSpeed', 'WindDir9am', 'WindDir3pm', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', \n                       'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm', \n                       'RainToday']]\ntarget = MiceImputed['RainTomorrow']\n\n# Split into test and train\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=12345)\n\n# Normalize Features\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","08b9e9d6":"def plot_roc_cur(fper, tper):  \n    plt.plot(fper, tper, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()","c87a82ee":"import time\nfrom sklearn.metrics import accuracy_score, roc_auc_score, cohen_kappa_score, plot_confusion_matrix, roc_curve, classification_report\ndef run_model(model, X_train, y_train, X_test, y_test, verbose=True):\n    t0=time.time()\n    if verbose == False:\n        model.fit(X_train,y_train, verbose=0)\n    else:\n        model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    roc_auc = roc_auc_score(y_test, y_pred) \n    coh_kap = cohen_kappa_score(y_test, y_pred)\n    time_taken = time.time()-t0\n    print(\"Accuracy = {}\".format(accuracy))\n    print(\"ROC Area under Curve = {}\".format(roc_auc))\n    print(\"Cohen's Kappa = {}\".format(coh_kap))\n    print(\"Time taken = {}\".format(time_taken))\n    print(classification_report(y_test,y_pred,digits=5))\n    \n    probs = model.predict_proba(X_test)  \n    probs = probs[:, 1]  \n    fper, tper, thresholds = roc_curve(y_test, probs) \n    plot_roc_cur(fper, tper)\n    \n    plot_confusion_matrix(model, X_test, y_test,cmap=plt.cm.Blues, normalize = 'all')\n    \n    return model, accuracy, roc_auc, coh_kap, time_taken","163107a3":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nparams_lr = {'penalty': 'l1', 'solver':'liblinear'}\n\nmodel_lr = LogisticRegression(**params_lr)\nmodel_lr, accuracy_lr, roc_auc_lr, coh_kap_lr, tt_lr = run_model(model_lr, X_train, y_train, X_test, y_test)\n\n","bedb469f":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\nparams_dt = {'max_depth': 16,\n             'max_features': \"sqrt\"}\n\nmodel_dt = DecisionTreeClassifier(**params_dt)\nmodel_dt, accuracy_dt, roc_auc_dt, coh_kap_dt, tt_dt = run_model(model_dt, X_train, y_train, X_test, y_test)\n\n","bd28f007":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nparams_rf = {'max_depth': 16,\n             'min_samples_leaf': 1,\n             'min_samples_split': 2,\n             'n_estimators': 100,\n             'random_state': 12345}\n\nmodel_rf = RandomForestClassifier(**params_rf)\nmodel_rf, accuracy_rf, roc_auc_rf, coh_kap_rf, tt_rf = run_model(model_rf, X_train, y_train, X_test, y_test)\n\n","cbf8e8d9":"# Light GBM\nimport lightgbm as lgb\nparams_lgb ={'colsample_bytree': 0.95, \n         'max_depth': 16, \n         'min_split_gain': 0.1, \n         'n_estimators': 200, \n         'num_leaves': 50, \n         'reg_alpha': 1.2, \n         'reg_lambda': 1.2, \n         'subsample': 0.95, \n         'subsample_freq': 20}\n\nmodel_lgb = lgb.LGBMClassifier(**params_lgb)\nmodel_lgb, accuracy_lgb, roc_auc_lgb, coh_kap_lgb, tt_lgb = run_model(model_lgb, X_train, y_train, X_test, y_test)\n\n","154e7e55":"# Catboost\n!pip install catboost\nimport catboost as cb\nparams_cb ={'iterations': 50,\n            'max_depth': 16}\n\nmodel_cb = cb.CatBoostClassifier(**params_cb)\nmodel_cb, accuracy_cb, roc_auc_cb, coh_kap_cb, tt_cb = run_model(model_cb, X_train, y_train, X_test, y_test, verbose=False)\n\n","49b327ab":"# XGBoost\nimport xgboost as xgb\nparams_xgb ={'n_estimators': 500,\n            'max_depth': 16}\n\nmodel_xgb = xgb.XGBClassifier(**params_xgb)\nmodel_xgb, accuracy_xgb, roc_auc_xgb, coh_kap_xgb, tt_xgb = run_model(model_xgb, X_train, y_train, X_test, y_test)","4e9da912":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport itertools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nimport catboost as cb\nimport xgboost as xgb\nfrom mlxtend.classifier import EnsembleVoteClassifier\nfrom mlxtend.plotting import plot_decision_regions\n\nvalue = 1.80\nwidth = 0.90\n\nclf1 = LogisticRegression(random_state=12345)\nclf2 = DecisionTreeClassifier(random_state=12345) \nclf3 = MLPClassifier(random_state=12345, verbose = 0)\nclf4 = RandomForestClassifier(random_state=12345)\nclf5 = lgb.LGBMClassifier(random_state=12345, verbose = 0)\nclf6 = cb.CatBoostClassifier(random_state=12345, verbose = 0)\nclf7 = xgb.XGBClassifier(random_state=12345)\neclf = EnsembleVoteClassifier(clfs=[clf4, clf5, clf6, clf7], weights=[1, 1, 1, 1], voting='soft')\n\nX_list = MiceImputed[[\"Sunshine\", \"Humidity9am\", \"Cloud3pm\"]] #took only really important features\nX = np.asarray(X_list, dtype=np.float32)\ny_list = MiceImputed[\"RainTomorrow\"]\ny = np.asarray(y_list, dtype=np.int32)\n\n# Plotting Decision Regions\ngs = gridspec.GridSpec(3,3)\nfig = plt.figure(figsize=(18, 14))\n\nlabels = ['Logistic Regression',\n          'Decision Tree',\n          'Neural Network',\n          'Random Forest',\n          'LightGBM',\n          'CatBoost',\n          'XGBoost',\n          'Ensemble']\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, clf4, clf5, clf6, clf7, eclf],\n                         labels,\n                         itertools.product([0, 1, 2],\n                         repeat=2)):\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, \n                                filler_feature_values={2: value}, \n                                filler_feature_ranges={2: width}, \n                                legend=2)\n    plt.title(lab)\n\nplt.show()","61718036":"accuracy_scores = [accuracy_lr, accuracy_dt, accuracy_rf, accuracy_lgb, accuracy_cb, accuracy_xgb]\nroc_auc_scores = [roc_auc_lr, roc_auc_dt, roc_auc_rf, roc_auc_lgb, roc_auc_cb, roc_auc_xgb]\ncoh_kap_scores = [coh_kap_lr, coh_kap_dt, coh_kap_rf, coh_kap_lgb, coh_kap_cb, coh_kap_xgb]\ntt = [tt_lr, tt_dt, tt_rf, tt_lgb, tt_cb, tt_xgb]\n\nmodel_data = {'Model': ['Logistic Regression','Decision Tree','Random Forest','LightGBM','Catboost','XGBoost'],\n              'Accuracy': accuracy_scores,\n              'ROC_AUC': roc_auc_scores,\n              'Cohen_Kappa': coh_kap_scores,\n              'Time taken': tt}\ndata = pd.DataFrame(model_data)\n\nfig, ax1 = plt.subplots(figsize=(12,10))\nax1.set_title('Model Comparison: Accuracy and Time taken for execution', fontsize=13)\ncolor = 'tab:green'\nax1.set_xlabel('Model', fontsize=13)\nax1.set_ylabel('Time taken', fontsize=13, color=color)\nax2 = sns.barplot(x='Model', y='Time taken', data = data, palette='summer')\nax1.tick_params(axis='y')\nax2 = ax1.twinx()\ncolor = 'tab:red'\nax2.set_ylabel('Accuracy', fontsize=13, color=color)\nax2 = sns.lineplot(x='Model', y='Accuracy', data = data, sort=False, color=color)\nax2.tick_params(axis='y', color=color)","312a3425":"fig, ax3 = plt.subplots(figsize=(12,10))\nax3.set_title('Model Comparison: Area under ROC and Cohens Kappa', fontsize=13)\ncolor = 'tab:blue'\nax3.set_xlabel('Model', fontsize=13)\nax3.set_ylabel('ROC_AUC', fontsize=13, color=color)\nax4 = sns.barplot(x='Model', y='ROC_AUC', data = data, palette='winter')\nax3.tick_params(axis='y')\nax4 = ax3.twinx()\ncolor = 'tab:red'\nax4.set_ylabel('Cohen_Kappa', fontsize=13, color=color)\nax4 = sns.lineplot(x='Model', y='Cohen_Kappa', data = data, sort=False, color=color)\nax4.tick_params(axis='y', color=color)\nplt.show()","bf21c12b":"## Imputation and Transformation\nWe will impute the categorical columns with mode, and then we will use the label encoder to convert them to numeric numbers. Once all the columns in the full data frame are converted to numeric columns, we will impute the missing values \u200b\u200busing the Multiple Imputation by Chained Equations (MICE) package.\n\nThen we will detect outliers using the interquartile range and remove them to get the final working dataset. Finally, we will check the correlation between the different variables, and if we find a pair of highly correlated variables, we will discard one while keeping the other.","98705b65":"## Data Exploration\nWe will first check the number of rows and columns. Next, we\u2019ll check the size of the dataset to decide if it needs size compression","ae27bccf":"Obviously, \u201cEvaporation\u201d, \u201cSunshine\u201d, \u201cCloud9am\u201d, \u201cCloud3pm\u201d are the features with a high missing percentage. So we will check the details of the missing data for these 4 features.","d11c4360":"The following feature pairs have a strong correlation with each other:\n\n     * MaxTemp and MinTemp\n    * Pressure9h and pressure3h\n    * Temp9am and Temp3pm\n    * Evaporation and MaxTemp\n    * MaxTemp and Temp3pm But in no case is the correlation value equal to a perfect \u201c1\u201d. We are therefore not removing any functionality\n\nHowever, we can delve deeper into the pairwise correlation between these highly correlated characteristics by examining the following pair diagram. Each of the paired plots shows very clearly distinct clusters of RainTomorrow\u2019s \u201cyes\u201d and \u201cno\u201d clusters. There is very minimal overlap between them.","3b413737":"## Handling Class Imbalance For Rainfall Prediction","1a13a555":"## Plotting Decision Region for all Models","46e8bb4e":"We observe that the 4 features have less than 50 per cent missing data. So instead of rejecting them completely, we\u2019ll consider them in our model with proper imputation.","7693acc6":"## Training Rainfall Prediction Model with Different Models\nWe will divide the dataset into training (75%) and test (25%) sets respectively to train the rainfall prediction model. For best results, we will standardize our X_train and X_test data:","0dea5247":"### Rainfall Prediction  is:\none of the difficult and uncertain tasks that have a significant impact on human society. Timely and accurate forecasting can proactively help reduce human and financial loss. This study presents a set of experiments that involve the use of common machine learning techniques to create models that can predict whether it will rain tomorrow or not based on the weather data for that day in major cities in Australia.\n\nI\u2019ve always liked knowing the parameters meteorologists take into account before making a weather forecast, so I found the dataset interesting. From an expert\u2019s point of view, however, this dataset is fairly straightforward. At the end of this article, you will learn:\n\n![](https:\/\/wallpaperaccess.com\/full\/688287.jpg)","4cb5d6b3":"Next, we will check if the dataset is unbalanced or balanced. If the data set is unbalanced, we need to either downsample the majority or oversample the minority to balance it.","a599fbf0":"## \u201cRainToday\u201d and \u201cRainTomorrow\u201d are objects (Yes \/ No). I will convert them to binary (1\/0) for our convenience.","1856006c":"We observe that the original dataset had the form (87927, 24). After running a code snippet for removing outliers, the dataset now has the form (86065, 24). As a result, the dataset is now free of 1862 outliers. We are now going to check multicollinearity, that is to say if a character is strongly correlated with another.","edeac875":"## Rainfall Prediction Model Comparison\nNow we need to decide which model performed best based on Precision Score, ROC_AUC, Cohen\u2019s Kappa and Total Run Time. One point to mention here is: we could have considered F1-Score as a better metric for judging model performance instead of accuracy, but we have already converted the unbalanced dataset to a balanced one, so consider accuracy as a metric for deciding the best model is justified in this case.\n\nFor a better decision, we chose \u201cCohen\u2019s Kappa\u201d which is actually an ideal choice as a metric to decide on the best model in case of unbalanced datasets. Let\u2019s check which model worked well on which front:","9bc4843a":"We can observe that the presence of \u201c0\u201d and \u201c1\u201d is almost in the 78:22 ratio. So there is a class imbalance and we have to deal with it. To fight against the class imbalance, we will use here the oversampling of the minority class. Since the size of the dataset is quite small, majority class subsampling wouldn\u2019t make much sense here.","21a823e0":"### Now, I will now check the missing data model in the dataset:","b01bc909":"## Feature Selection for Rainfall Prediction\nI will use both the filter method and the wrapper method for feature selection to train our rainfall prediction model.\n\nSelecting features by filtering method (chi-square value): before doing this, we must first normalize our data. We use MinMaxScaler instead of StandardScaler in order to avoid negative values.","a04d4850":"### We can observe that XGBoost, CatBoost and Random Forest performed better compared to other models. However, if speed is an important thing to consider, we can stick with Random Forest instead of XGBoost or CatBoost.","5e224e4d":"### Selection of features by wrapping method (random forest):"}}