{"cell_type":{"50ec8aec":"code","fdf13e04":"code","b3725650":"code","75ff7291":"code","34bbaf1e":"code","838e1fc9":"code","617359a8":"code","d60bc986":"code","41756b3c":"code","7d180ee0":"code","ae315df3":"code","3275d77e":"code","2073cc0a":"code","f405a17b":"code","7e02619d":"code","afb66242":"code","8b0692f1":"code","ebaedae6":"code","a5e5df69":"code","57016d3e":"code","108a07d3":"code","f64346fe":"code","10115a98":"code","c034399a":"code","0ccbaa29":"code","2586fa6d":"code","06eb4490":"code","af8c57ff":"markdown","66c19f84":"markdown","484ec768":"markdown","4dadec9d":"markdown","ca4f281b":"markdown","c6b6f0e8":"markdown","acad6a5e":"markdown","90abb0c8":"markdown","fa9dcbec":"markdown","1897c588":"markdown"},"source":{"50ec8aec":"%%HTML\n<style type=\"text\/css\">\ndiv.h1 {\n    background-color:#e17b34; \n    color: white; \n    padding: 8px; \n    padding-right: 300px; \n    font-size: 35px; \n    max-width: 1500px; \n    margin: auto; \n    margin-top: 50px;\n}\n\ndiv.h2 {\n    background-color:#83ccd2; \n    color: white; \n    padding: 8px; \n    padding-right: 300px; \n    font-size: 35px; \n    max-width: 1500px; \n    margin: auto; \n    margin-top: 50px;\n}\n<\/style>","fdf13e04":"import sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","b3725650":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import QuantileTransformer\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport warnings\nwarnings.filterwarnings('ignore')","75ff7291":"def seed_everything(seed=1903):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","34bbaf1e":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","838e1fc9":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","617359a8":"gnum = train_features[GENES].shape[1]\ngraphs = []\n\nfor i in range(0, gnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=771:\n            break\n        graph = sns.distplot(train_features[GENES].values[:,item], ax=axs[k])\n        graph.set_title(f\"g-{item}\")\n        graphs.append(graph)","d60bc986":"cnum = train_features[CELLS].shape[1]\ngraphs = []\n\nfor i in range(0, cnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=100:\n            break\n        graph = sns.distplot(train_features[CELLS].values[:,item], ax=axs[k])\n        graph.set_title(f\"c-{item}\")\n        graphs.append(graph)","41756b3c":"gnum = test_features[GENES].shape[1]\ngraphs = []\n\nfor i in range(0, gnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=771:\n            break\n        graph = sns.distplot(test_features[GENES].values[:,item], ax=axs[k])\n        graph.set_title(f\"g-{item}\")\n        graphs.append(graph)","7d180ee0":"cnum = test_features[CELLS].shape[1]\ngraphs = []\n\nfor i in range(0, cnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=100:\n            break\n        graph = sns.distplot(test_features[CELLS].values[:,item], ax=axs[k])\n        graph.set_title(f\"c-{item}\")\n        graphs.append(graph)","ae315df3":"# GENES\nn_comp = 28\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp, random_state=1903).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\n#CELLS\nn_comp = 5\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=1903).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","3275d77e":"train_features.shape","2073cc0a":"from sklearn.feature_selection import VarianceThreshold\n\n\nvar_thresh = VarianceThreshold(threshold=0.4)\ndata = train_features.append(test_features)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\ntrain_features","f405a17b":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train[train_targets_scored.columns]\n\ntrain = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","7e02619d":"target_cols = target.drop('sig_id', axis=1).columns.values.tolist()","afb66242":"folds = train.copy()\n\nmskf = MultilabelStratifiedKFold(n_splits=7)\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n    folds.loc[v_idx, 'kfold'] = int(f)\n\nfolds['kfold'] = folds['kfold'].astype(int)","8b0692f1":"def process_data(df):\n    df['time_dose'] = df['cp_time'].astype(str)+df['cp_dose']\n    \n    df = pd.get_dummies(df, columns=['cp_time','cp_dose','time_dose'])\n    \n    \n    features_g = list(df.columns[4:776])\n    features_c = list(df.columns[776:876])\n    \n    df['g_sum'] = df[features_g].sum(axis = 1)\n    df['g_mean'] = df[features_g].mean(axis = 1)\n    df['g_std'] = df[features_g].std(axis = 1)\n    df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n    df['g_skew'] = df[features_g].skew(axis = 1)\n    df['c_sum'] = df[features_c].sum(axis = 1)\n    df['c_mean'] = df[features_c].mean(axis = 1)\n    df['c_std'] = df[features_c].std(axis = 1)\n    df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n    df['c_skew'] = df[features_c].skew(axis = 1)\n    df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n    df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n    df['gc_std'] = df[features_g + features_c].std(axis = 1)\n    df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n    df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n    \n    features_c = list(df.columns[776:876])\n    for feature in features_c:\n        df[f'{feature}_squared'] = df[feature] ** 2\n    \n    return df","ebaedae6":"feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\nlen(feature_cols)","a5e5df69":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct","57016d3e":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        #print(inputs.shape)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n   ","108a07d3":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.2)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.leaky_relu(self.dense1(x), 1e-3)\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","f64346fe":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 26\nBATCH_SIZE = 256\nLEARNING_RATE = 6e-4\nWEIGHT_DECAY = 1e-5\nNFOLDS = 7\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = True\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=2048","10115a98":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    print(f\"FOLD: {fold}\")\n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_fn, trainloader, DEVICE)\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"EPOCH: {epoch}, train_loss: {train_loss}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","c034399a":"def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ \/ NFOLDS\n        oof += oof_\n        \n    return oof, predictions","0ccbaa29":"# Averaging on multiple SEEDS\n\nSEED = [1903, 1881]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    print(f\"SEED: {seed}\")\n    \n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ \/ len(SEED)\n    predictions += predictions_ \/ len(SEED)\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions","2586fa6d":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ \/ target.shape[1]\n    \nprint(\"CV log_loss: \", score)","06eb4490":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","af8c57ff":"# **PCA features**","66c19f84":"# **Feature Selection Using Variance Encoding**","484ec768":"## Updates :-\n\n### 1. **Change last dropout to 0.2(0.25->0.2)**\n### 2. **replace relu to leaky_relu with coeff 1e-3**","4dadec9d":"# *Feature Engineering  from [this notebook](http:\/\/www.kaggle.com\/ragnar123\/moa-dnn-feature-engineering) and combination of cp_time and cp_dose*","ca4f281b":"# **CV Folds**","c6b6f0e8":"# **Utilities**","acad6a5e":"## Updates :-\n\n### 1. **Epoches: 20 -> 26**\n### 2. **Batch size: 64 -> 256**\n### 3. **Learning rate: 1e-3 -> 6e-4**\n### 4. **N folds: 3 -> 7**\n### 5. **Early stopping steps: 10 -> 11**","90abb0c8":"### References :\n\n[1] **Improved version Of** : https:\/\/www.kaggle.com\/utkukubilay\/pytorch-moa-0-01867\n\n## If someone will be exited by these ideas,  If you like, please Upvote :)\n\n### Thank you!","fa9dcbec":"# <div class=\"h1\" >About this notebook <\/div> \n\n## - **CV** :  **0.014397605520767836**\n\n## - **LB** : **0.01860**","1897c588":"# **RankGauss**"}}