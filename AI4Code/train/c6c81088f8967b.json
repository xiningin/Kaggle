{"cell_type":{"4c2766fb":"code","008adc61":"code","491c79df":"code","d6e905db":"code","ea54b906":"code","02a686e4":"code","b06ed59e":"code","563bfb33":"code","0dce038a":"code","220b08d8":"code","7eaa0037":"code","fd560d58":"code","98af7464":"code","4549a9d0":"code","ca8dd21c":"code","18527027":"code","91671102":"code","c274f027":"code","72d53c17":"code","f11c44a3":"code","1767f7f3":"code","1a4f0d42":"code","1ded4b3b":"code","3c910327":"code","f4622db7":"code","d1689d07":"code","56fbe3d5":"code","b84977d0":"code","e231816c":"code","21dfc93e":"code","16be89db":"code","0761b080":"code","bd86a750":"code","295ed09f":"code","45a9519f":"code","5df8321a":"code","e3ddf424":"markdown","34f63634":"markdown","b6c7ef6d":"markdown","c5275d72":"markdown","4731f926":"markdown","bee78c42":"markdown","1124b66e":"markdown","e9b56c50":"markdown","b4b6248d":"markdown","0d54910e":"markdown","ce1f75cf":"markdown","d8378ab4":"markdown","dcff2f36":"markdown","eaf98afb":"markdown","3c51a568":"markdown","41d797b0":"markdown","8aac4e00":"markdown","d2d12984":"markdown","6dab0bbc":"markdown","f14aed7e":"markdown","fc09e01e":"markdown","fea15abe":"markdown","1620302a":"markdown","932057bf":"markdown"},"source":{"4c2766fb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","008adc61":"import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score \nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport warnings\nnp.random.seed(123)\nwarnings.filterwarnings('ignore')\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV","491c79df":"train_dataset = pd.read_csv(\"\/kaggle\/input\/steam-reviews\/train.csv\", delimiter=\",\")\ntrain_dataset","d6e905db":"test_dataset = pd.read_csv(\"\/kaggle\/input\/steam-reviews-test-dataset\/test.csv\", delimiter=\",\")\ntest_dataset['user_suggestion'] = None\ntest_dataset","ea54b906":"dataset = pd.concat([train_dataset, test_dataset], axis = 0)\ndataset.reset_index(drop = True, inplace = True)\ndataset","02a686e4":"from matplotlib import pyplot as plt\nimport seaborn as sns","b06ed59e":"# Visualizing the variable - 'year'\nplt.figure(figsize = (10,5))\nplt.xticks(rotation=90)\nsns.countplot(train_dataset['year'])","563bfb33":"import re\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    phrase = re.sub(r\"early access review\", \"early access review \", phrase)\n    phrase = re.sub(r\"\\+\", \" + \", phrase) \n    phrase = re.sub(r\"\\-\", \" - \", phrase)     \n    phrase = re.sub(r\"\/10\", \"\/10 \", phrase)     \n    phrase = re.sub(r\"10\/\", \" 10\/\", phrase)         \n    return phrase\n\n\ntrial = \"Hey I'm Yann, how're you and how's it going ? That's interesting: I'd love to hear more about it+info\"\ndecontracted(trial)","0dce038a":"from textblob import TextBlob\n# Define function to lemmatize each word with its POS tag\ndef lemmatize_with_postag(sentence):\n    sent = TextBlob(sentence)\n    tag_dict = {\"J\": 'a', \n                \"N\": 'n', \n                \"V\": 'v', \n                \"R\": 'r'}\n    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n    return \" \".join(lemmatized_list)\n\n# Lemmatize\ntrial = \"The striped bats are hanging on their feet for best\"\nlemmatize_with_postag(trial)","220b08d8":"import re\ndef clean_reviews(lst):\n    # remove URL links (httpxxx)\n    lst = np.vectorize(remove_pattern)(lst, \"https?:\/\/[A-Za-z0-9.\/]*\")\n    # remove special characters, numbers, punctuations (except for #)\n    lst = np.core.defchararray.replace(lst, \"[^a-zA-Z]\", \" \")\n    # remove amp with and\n    lst = np.vectorize(replace_pattern)(lst, \"amp\", \"and\")  \n    # remove hashtags\n    lst = np.vectorize(remove_pattern)(lst, \"#[A-Za-z0-9]+\")\n    lst = np.vectorize(remove_pattern)(lst, \"#[\\w]*\")    \n    return lst\ndef remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)        \n    return input_txt\ndef replace_pattern(input_txt, pattern, replace_text):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, replace_text, input_txt)        \n    return input_txt","7eaa0037":"# Applying pre-processing to user reviews\ntext2 = clean_reviews(list(dataset['user_review'].astype('str')))\ntext3 = [ta.lower() for ta in text2]\ntext4 = [''.join([i if ord(i) < 128 else ' ' for i in t]) for t in text3]\ntext5 = [decontracted(u) for u in text4]\ntext6 = [lemmatize_with_postag(u) for u in text5]\ntext6","fd560d58":"dataset.loc[4, 'user_review']","98af7464":"text6[4]","4549a9d0":"# Word Level Count Vectorizer \ncount_vect = CountVectorizer(analyzer='word', max_features = 1500, stop_words = \"english\")\ncountdf_user_review= count_vect.fit_transform(text6)\nprint(\"All tags are:\")\nprint(count_vect.get_feature_names())\nprint(\"Matrix looks like\")\nprint(countdf_user_review.shape)\nprint(countdf_user_review.toarray())","ca8dd21c":"countdf_user_review_df = pd.DataFrame(data = countdf_user_review.toarray(), index = dataset.index)\ncountdf_user_review_df.columns = count_vect.get_feature_names()\ncountdf_user_review_df.head()","18527027":"# Count Vectorizer for N-grams \ncount_vect2 = CountVectorizer(analyzer='word', max_features = 1500, ngram_range=(2,3), stop_words = \"english\")\ncountdf_user_review2= count_vect2.fit_transform(text6)\nprint(\"All tags are:\")\nprint(count_vect2.get_feature_names())\nprint(\"Matrix looks like\")\nprint(countdf_user_review2.shape)\nprint(countdf_user_review2.toarray())","91671102":"countdf_user_review_df2 = pd.DataFrame(data = countdf_user_review2.toarray(), index = dataset.index)\ncountdf_user_review_df2.columns = count_vect2.get_feature_names()\ncountdf_user_review_df2.head()","c274f027":"# Word level Tf-Idf\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vect = TfidfVectorizer(analyzer='word', max_features=1500, stop_words = \"english\")\ntfidf_user_review = tfidf_vect.fit_transform(text6)\nprint(\"All tags are:\")\nprint(tfidf_vect.get_feature_names())\nprint(\"Matrix looks like\")\nprint(tfidf_user_review.shape)\nprint(tfidf_user_review.toarray())","72d53c17":"tfidf_user_review_df = pd.DataFrame(data = tfidf_user_review.toarray(), index = dataset.index)\ntfidf_user_review_df.columns = tfidf_vect.get_feature_names()\ntfidf_user_review_df.head()","f11c44a3":"# Tf-Idf for N-grams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vect2 = TfidfVectorizer(analyzer='word', max_features=1500, ngram_range=(2,3), stop_words = \"english\")\ntfidf_user_review2 = tfidf_vect2.fit_transform(text6)\nprint(\"All tags are:\")\nprint(tfidf_vect2.get_feature_names())\nprint(\"Matrix looks like\")\nprint(tfidf_user_review2.shape)\nprint(tfidf_user_review2.toarray())","1767f7f3":"tfidf_user_review_df2 = pd.DataFrame(data = tfidf_user_review2.toarray(), index = dataset.index)\ntfidf_user_review_df2.columns = tfidf_vect2.get_feature_names()\ntfidf_user_review_df2.head()","1a4f0d42":"target = dataset['user_suggestion']","1ded4b3b":"dataset2 = pd.concat([tfidf_user_review_df, tfidf_user_review_df2], axis=1)\ndataset2","3c910327":"dataset2.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in dataset2.columns]","f4622db7":"cols=pd.Series(dataset2.columns)\n\nfor dup in cols[cols.duplicated()].unique(): \n    cols[cols[cols == dup].index.values.tolist()] = [dup + '.' + str(i) if i != 0 else dup for i in range(sum(cols == dup))]\n\n# rename the columns with the cols list.\ndataset2.columns=cols","d1689d07":"target[~target.isnull()]","56fbe3d5":"X = dataset2[~target.isnull()]\nY = target[~target.isnull()].astype(int)","b84977d0":"X_Train, X_Test, Y_Train, Y_Test = train_test_split(X,Y,test_size=0.2)","e231816c":"import lightgbm as lgb\n\ntrain_data = lgb.Dataset(X_Train, label = Y_Train)\n\n#setting parameters for lightgbm\nparam = {'num_leaves':50, 'objective':'binary','max_depth':4,'learning_rate':.1,'max_bin':100}\nparam['metric'] = ['auc', 'binary_logloss']\n\nlgbmodel = lgb.train(param, train_data, 500, verbose_eval=True)","21dfc93e":"from sklearn.metrics import roc_curve, auc\n\nY_Pred_Prob = lgbmodel.predict(X_Test)\n\nY_Pred = Y_Pred_Prob.copy()\n\nY_Pred[Y_Pred >= 0.5] = 1\nY_Pred[Y_Pred < 0.5] = 0\n\nfpr, tpr, thresholds = roc_curve(np.array(Y_Test), np.array(Y_Pred_Prob))\nroc_auc = auc(fpr, tpr)\nprint(\"ROC AUC of LightGBM  is {}\".format(roc_auc))\n\nfrom sklearn.metrics import f1_score\nf1_score2 = f1_score(Y_Test, Y_Pred, average = \"weighted\")\nprint(\"F1 Score of LightGBM  is {}\".format(f1_score2))\n\nfrom sklearn.metrics import accuracy_score\nacc = accuracy_score(Y_Test, Y_Pred)\nprint(\"Accuracy Score of LightGBM  is {}\".format(acc))","16be89db":"dataset2 = pd.concat([countdf_user_review_df, countdf_user_review_df2], axis=1)\ndataset2\n\ndataset2.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in dataset2.columns]\n\ncols=pd.Series(dataset2.columns)\n\nfor dup in cols[cols.duplicated()].unique(): \n    cols[cols[cols == dup].index.values.tolist()] = [dup + '.' + str(i) if i != 0 else dup for i in range(sum(cols == dup))]\n\n# rename the columns with the cols list.\ndataset2.columns=cols\n\ntarget[~target.isnull()]\n\nX = dataset2[~target.isnull()]\nY = target[~target.isnull()].astype(int)","0761b080":"X_Train, X_Test, Y_Train, Y_Test = train_test_split(X,Y,test_size=0.2)","bd86a750":"import lightgbm as lgb\n\ntrain_data = lgb.Dataset(X_Train, label = Y_Train)\n\n#setting parameters for lightgbm\nparam = {'num_leaves':50, 'objective':'binary','max_depth':4,'learning_rate':.1,'max_bin':100}\nparam['metric'] = ['auc', 'binary_logloss']\n\nlgbmodel = lgb.train(param, train_data, 500, verbose_eval=True)","295ed09f":"from sklearn.metrics import roc_curve, auc\n\nY_Pred_Prob = lgbmodel.predict(X_Test)\n\nY_Pred = Y_Pred_Prob.copy()\n\nY_Pred[Y_Pred >= 0.5] = 1\nY_Pred[Y_Pred < 0.5] = 0\n\nfpr, tpr, thresholds = roc_curve(np.array(Y_Test), np.array(Y_Pred_Prob))\nroc_auc = auc(fpr, tpr)\nprint(\"ROC AUC of LightGBM  is {}\".format(roc_auc))\n\nfrom sklearn.metrics import f1_score\nf1_score2 = f1_score(Y_Test, Y_Pred, average = \"weighted\")\nprint(\"F1 Score of LightGBM  is {}\".format(f1_score2))\n\nfrom sklearn.metrics import accuracy_score\nacc = accuracy_score(Y_Test, Y_Pred)\nprint(\"Accuracy Score of LightGBM  is {}\".format(acc))","45a9519f":"from sklearn.model_selection import StratifiedKFold\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1048)\npredictions = np.zeros((len(X_Test), ))\nfeature_importance_df = pd.DataFrame()\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X_Train.values, Y_Train.values)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(X_Train.iloc[trn_idx,:], label=Y_Train.iloc[trn_idx])\n    val_data = lgb.Dataset(X_Train.iloc[val_idx,:], label=Y_Train.iloc[val_idx])\n\n    num_round = 1000000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 1000)\n    temp = clf.predict(X_Train.iloc[val_idx,:], num_iteration=clf.best_iteration)\n    \n    predictions_val = temp.copy()\n    \n    predictions_val[predictions_val >= 0.5] = 1\n    predictions_val[predictions_val < 0.5] = 0\n    \n    print(\"CV score (Accuracy): {:<8.5f}\".format(accuracy_score(Y_Train.iloc[val_idx], predictions_val)))\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = X_Train.columns\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(X_Test, num_iteration=clf.best_iteration) \/ folds.n_splits","5df8321a":"Y_Pred = predictions.copy()\n\nY_Pred[Y_Pred >= 0.5] = 1\nY_Pred[Y_Pred < 0.5] = 0\n\nfpr, tpr, thresholds = roc_curve(np.array(Y_Test), np.array(predictions))\nroc_auc = auc(fpr, tpr)\nprint(\"ROC AUC of LightGBM  is {}\".format(roc_auc))\n\nfrom sklearn.metrics import f1_score\nf1_score2 = f1_score(Y_Test, Y_Pred, average = \"weighted\")\nprint(\"F1 Score of LightGBM  is {}\".format(f1_score2))\n\nfrom sklearn.metrics import accuracy_score\nacc = accuracy_score(Y_Test, Y_Pred)\nprint(\"Accuracy of LightGBM  is {}\".format(acc))","e3ddf424":"`lemmatize_with_postag` function reduces the words to their base form. I've used TextBlob library's implementation here, which is build on top of NLTK. Feel free to try out other functions. The main idea is to reduce additional vocabulary - which is helpul both compuationaly, and also helps in reduce overfitting to certain key-words. There are other methods like Stemming, but it is generally inferior to Lemmatization.","34f63634":"Once again, rows constitute sentences and individual tokens represent the columns. If the token is present within the sentence, based on the method discussed above a TFIDF score is calculated and filled in the respective column, otherwise the value is 0.","b6c7ef6d":"## Training using LGBM\n\nI've used Light GBM because of it's high accuracy and speed, among other Tree-Based Ensemble models in a lot of tasks. Feel free to experiment here. One good alternative could be XGBoost!","c5275d72":"Merging the two into a single Dataset","4731f926":"Test:","bee78c42":"## **Using Bag of Words, N-Grams, TF-IDF**\n\nThe approach below essentially covers some of the very first tools that anyone trying to experiment with NLP starts with. And, over time, lots and lots of libraries, like SpaCy and NLTK have popped up, which have simplified using this approach tremendously. There are also libraries like Textblob, which stand on the shoulders of mighty NLTK and provide a better and faster interface to perform a lot of NLTK operations and then some more. \n\nI'll try and give a quick overview of the methods and libraries, however, I would recommend going to the websites of each of the libraries (attached below) to understand their complete set of cabilities.","1124b66e":"Loading the Train and Test Datasets\n\nTrain:","e9b56c50":"The column names represent the token and rows represent individual sentences. If that token is present in the sentence, the respective column will have a value 1, otherwise 0.","b4b6248d":"After Pre-Processing!","0d54910e":"### Accuracy - 0.833\n\nSo, this is our baseline score! Based on Bag of Words and TF-IDF, this is the best we can get. Let's try to improve it further trying out more powerful techniques! Hope it helped you understand TF-IDF and Count-Vectorizer better and it's implementation in Python.\n\nThere are excellent online sources, especially at **DataCamp**, where online interactive courses cover the same techniques we discussed in this notebook. Feel free to check them out as well for more understanding! See you in the Part - 2 of this series.","ce1f75cf":"## Step 1. Pre-Processing\n#### Cleaning up the user reviews!\n\n`decontracted` function would convert short forms of general phrases into their longer versions","d8378ab4":"Let's dive right into it! For this and the series of tutorials, I'm only going to use the reviews, and no other columns. In practice, it's good to perform EDA to get a better sense of your data.","dcff2f36":"Same idea as earlier, however, this time we look for specific n-gram sequences in the sentence.","eaf98afb":"Further cleaning to remove links, punctuation, etc.","3c51a568":"Before Pre-processing!","41d797b0":"## Step 2. - Structuring the Data for ML\n\n## Using Count Vectorizer (Bag of Words)\n\nBag of words, put simply, indicates the count of appearance of a certain word in a review, irrespective of it's order. To do this, firstly we create a dictionary (or vocabulary) of all the words (or tokens) present in the reviews. Each token from the vocabulary is then converted to a column with it's `row[j]` indicating - \"How many times was the token - \"the\" present in `review[j]`?\"\n\nI've used a scikit-learn implementation below, but there are other libraries which can handle this well as well.","8aac4e00":"## **Understanding the Data**\n\nI've used the JantaHack NLP Hackathon dataset here. This dataset essentially consists of **Steam Reviews**, collected during 2015-2019. The goal here is to predict whether based on the user review, the user recommends or doesn't recommend the game.","d2d12984":"# Evolution of NLP - Part 1 - Bag of Words, TF-IDF\n\nThis is the first blog in a series of posts where I try to talk about the changes in modeling techniques for NLP tasks over past few years. Right from the basics, Bag of Words, we reach to the current State of The Art (SOTA) - Transformers! I hope you enjoy this journey ;)\n\nFor this post, we'll focus on using simple Bag-of-Words and TF-IDF based models, coupled with Ensemble Decision Trees to highest accuracy score!\n\nTo get a more detailed explanation, check out [this medium blog](https:\/\/www.kaggle.com\/jainkanishk95\/evolution-of-nlp-part-1-bag-of-words-tf-idf)","6dab0bbc":"Similar to **CountVectorizer**, we use n-grams here as well. ","f14aed7e":"### Accuracy - 0.833\n\nLet's try out Count Vectorizer Based apporach! ","fc09e01e":"**Appendix** - You can further try out k-fold validation to increase this score even more!","fea15abe":"### Using N-grams\n\nHowever, sometimes it's the combination of words that is important, and not just the words themselves. Example - \"not good\" and \"good\" would have same flag for \"good\" token. Hence it becomes important to find these phrases that occur in our corpus which might affect overall meaning of review. This is what we call N-grams\nHowever, the cost to find these grows polynomially as Vocab Size (V) increases, as in essence we are looking at potentially $O(V^2)$ combinations of phrases at worse (where V is the size of vocabulary).\n\nIn our implementation, we limit to 2 and 3 grams. We further the select a total of top 3000 features, sorted based on their apperance in data.","1620302a":"## **Using TF-IDF (Term Frequency - Inverse Document Frequency)**\n\nNow if you are wondering what is term frequency , it is the relative frequency of a word in a document and is given as (term instances\/total instances). Inverse Document Frequency is the relative count of documents containing the term is given as log(number of documents\/documents with term) The overall importance of each word to the documents in which they appear is equal to **TF * IDF**\n\nThis will give you a matrix where each column represents a word in the vocabulary (all the words that appear in at least one document) and each row represents a review, as before. This is done to reduce the importance of words that occur frequently in review and therefore, their significance in overall rating of the review.\n\nFortunately, scikit-learn gives you a built-in TfIdfVectorizer class that produces the TF-IDF matrix in a couple of lines. ","932057bf":"## Step 3. Modeling\n\nLet's first try out dataset with only TF-IDFs."}}