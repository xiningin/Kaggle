{"cell_type":{"5fd2a5ed":"code","ccadade4":"code","76d2b596":"code","bd2a8e30":"code","48ee47cf":"code","ba3c5e1d":"code","8f0dfeb1":"code","2996fbf8":"code","66cc9168":"code","87d36de1":"code","09d6c8d3":"code","1e0f6159":"code","2df03af6":"code","105a121c":"code","8a83fcef":"code","cf470d1b":"code","6c9a5ae9":"code","c3da74f8":"code","9d5ea52c":"code","5c0a8393":"code","cea83624":"code","d7235761":"code","d41a8dc3":"code","0266139d":"code","e42f344f":"code","902d4de7":"code","66884fe9":"code","e32dbcb3":"code","0ce79ea5":"code","ba658a75":"code","13f4bd3c":"code","16c3e449":"code","6d965830":"code","d10503a4":"code","7ab7c1d1":"code","624e7e40":"markdown","1438e4dc":"markdown","d1dbb90f":"markdown","f9b7ac38":"markdown","6c59176d":"markdown","0b4baf97":"markdown","178abfd1":"markdown","2789c3e4":"markdown","dac718cb":"markdown","e730b1cc":"markdown","6fe701d7":"markdown","f706ac8b":"markdown","972b9d77":"markdown","f8c7323b":"markdown","5d7768b9":"markdown","213aac67":"markdown","99d9dfa6":"markdown","665f2660":"markdown","b7dd1099":"markdown","e8df429c":"markdown","ca1153a8":"markdown","c08c1737":"markdown","b468fa00":"markdown","a9f9412b":"markdown","883e9997":"markdown","e9ba4902":"markdown","55014ef3":"markdown","88924476":"markdown","1f74b118":"markdown","4594e260":"markdown","0a369126":"markdown","b6c4d8ff":"markdown","2ecd17a8":"markdown","15bb3bdf":"markdown","52f78950":"markdown","603cf843":"markdown","db57fdba":"markdown"},"source":{"5fd2a5ed":"import numpy as np\nimport pandas as pd","ccadade4":"df_train = pd.read_csv(\"..\/input\/atividade-regressao-PMR3508\/train.csv\", index_col=['Id'])\ndf_test = pd.read_csv(\"..\/input\/atividade-regressao-PMR3508\/test.csv\", index_col=['Id'])","76d2b596":"print('Tamanho do DataFrame: ', df_train.shape)\ndf_train.head()","bd2a8e30":"df_train.info()","48ee47cf":"df_train.isna().sum()","ba3c5e1d":"df_test.isna().sum()","8f0dfeb1":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nmask = np.triu(np.ones_like(df_train.corr(), dtype=np.bool))\n\nplt.figure(figsize=(10,10))\n\nsns.heatmap(df_train.corr(),mask=mask, square = True, annot=True, vmin=-1, vmax=1, cmap='mako')\nplt.show()","2996fbf8":"import plotly.express as px\nfig = px.scatter_mapbox(df_train, lat=\"latitude\", lon=\"longitude\", color=\"median_house_value\", size=\"population\",\n                  color_continuous_scale=px.colors.cyclical.IceFire, size_max=15, zoom=4,\n                  mapbox_style=\"carto-positron\")\nfig.show()","66cc9168":"from mpl_toolkits.basemap import Basemap\n\nplt.ioff()\n\n# Pega as coordenadas da costa\nmap_base = Basemap(projection='robin',lon_0=0,resolution='l')\ncoast = map_base.drawcoastlines()\ncoordinates = np.vstack(coast.get_segments())\nlons_coast,lats_coast = map_base(coordinates[:,0],coordinates[:,1],inverse=True)\nplt.close()\n\n# Pega apenas regi\u00e3o da calif\u00f3rnia \nlons_cal = np.logical_and(lons_coast>-126,lons_coast<-113)\nlats_cal = np.logical_and(lats_coast>30,lats_coast<44)\ncalifornia = np.logical_and(lons_cal,lats_cal)\n\n# Dataframe com as dist\u00e2ncias\nocean_df = pd.DataFrame()\nocean_df['longitude'] = lons_coast[california]\nocean_df['latitude'] = lats_coast[california]\nocean_df.shape","87d36de1":"def min_dist_ocean(row):\n    dists = np.sqrt((row['latitude'] - ocean_df['latitude'])**2+\n                    (row['longitude'] - ocean_df['longitude'])**2)*111.12\n    \n    return min(dists)","09d6c8d3":"df_train['dist_ocean'] = pd.concat([df_train['latitude'],df_train['longitude']], axis=1).apply(min_dist_ocean, axis=1)\ndf_test['dist_ocean'] =  pd.concat([df_test['latitude'],df_test['longitude']], axis=1).apply(min_dist_ocean, axis=1)","1e0f6159":"cities = {'city':['San Francisco','Los Angeles', 'San Diego', 'San Jose', 'Long Beach'], \n     'latitude':  [37.773972, 34.0194, 32.8153, 37.33, 33.7763], \n     'longitude': [-122.431297, -118.411, -117.135, -121.89,-118.167]}\ncities_df = pd.DataFrame(data=cities)","2df03af6":"import geopy.distance as gp_d\ndef min_dist(row_df):\n    min_dist = 9999999\n    for index, row in cities_df.iterrows():\n        dist = gp_d.distance((row_df['latitude'],row_df['longitude']), (row['latitude'],row['longitude'])).km\n        min_dist = min(min_dist,dist)\n    return min_dist","105a121c":"df_train['big_city'] = pd.concat([df_train['latitude'],df_train['longitude']], axis=1).apply(min_dist, axis=1)\ndf_test['big_city'] = pd.concat([df_test['latitude'],df_test['longitude']], axis=1).apply(min_dist, axis=1)","8a83fcef":"df_train['rooms_per_household'] = df_train['total_rooms']\/df_train['households']\ndf_test['rooms_per_household'] = df_test['total_rooms']\/df_test['households']","cf470d1b":"df_train.head()","6c9a5ae9":"df_train.corr()['median_house_value'].sort_values()","c3da74f8":"df_train = df_train.drop(['longitude', 'population', 'total_bedrooms','households'], axis=1)\ndf_test = df_test.drop(['longitude', 'population', 'total_bedrooms','households'], axis = 1)","9d5ea52c":"df_train.drop_duplicates(keep='first', inplace=True)","5c0a8393":"Y_train = df_train.pop('median_house_value').values.astype(float)\nX_train = df_train","cea83624":"from sklearn.preprocessing import StandardScaler\n\npreprocessor = StandardScaler()","d7235761":"X_train = preprocessor.fit_transform(X_train)","d41a8dc3":"def rmsle(y_pred,y):\n    return np.sqrt(np.mean((np.log(np.abs(y_pred)+1) - np.log(np.abs(y)+1))**2))","0266139d":"from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.20)","e42f344f":"from sklearn.linear_model import LinearRegression\nfrom sklearn.compose import TransformedTargetRegressor\n\nmodel = LinearRegression()\n\ndef constrain(array):\n    return array * (array > 0)\n\nregr = TransformedTargetRegressor(regressor=model, func=constrain, inverse_func=constrain)","902d4de7":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import ShuffleSplit\n\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n\nscore = cross_val_score(regr, X_train, Y_train, cv = cv, scoring=\"neg_mean_squared_log_error\")\nprint(\"RMSLE:\", ((-score)**0.5).mean())","66884fe9":"from sklearn.linear_model import ElasticNet\n    \nmodel = ElasticNet(random_state=42)\n\nenr = TransformedTargetRegressor(regressor=model, func=constrain, inverse_func=constrain)","e32dbcb3":"score = cross_val_score(enr, X_train, Y_train, cv = cv, scoring=\"neg_mean_squared_log_error\")\nprint(\"RMSLE:\", ((-score)**0.5).mean())","0ce79ea5":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import make_scorer\n\n\nscorer = make_scorer(rmsle, greater_is_better=False)\n\nk_range = list(range(1, 40))\nweight_options = ['uniform', 'distance']\n\nparameters = dict(n_neighbors=k_range, weights=weight_options)\n\nknn = GridSearchCV(KNeighborsRegressor(), parameters, cv=10,scoring=scorer)\nknn.fit(X_train, Y_train)\nprint('Melhor K:', knn.best_estimator_)","ba658a75":"knn_best = knn.best_estimator_\nknn_best.fit(X_train, Y_train)\nY_pred = knn_best.predict(X_val)\nrmsle_knn = rmsle(Y_pred, Y_val)\nprint(\"RMSLE: \",rmsle_knn)","13f4bd3c":"X_test = df_test\nX_test = preprocessor.transform(X_test)","16c3e449":"Y_test = knn_best.predict(X_test)","6d965830":"submission = pd.DataFrame()\nsubmission[0] = df_test.index\nsubmission[1] = Y_test\nsubmission.columns = ['Id','median_house_value']","d10503a4":"submission.head()","7ab7c1d1":"submission.to_csv('submission.csv',index = False)","624e7e40":"Analisando o mapa, vemos que as casas pr\u00f3ximas do mar e pr\u00f3ximas de cidades grandes como: Los Angeles e S\u00e3o Francisco tem um valor mediano mais alto, ent\u00e3o iremos se basear nisso para criar algumas features.","1438e4dc":"Ser\u00e1 criado uma fun\u00e7\u00e3o para o c\u00e1lculo da dist\u00e2ncia m\u00ednima entre cada regi\u00e3o da california e o oceano. Para o c\u00e1lculo, seria poss\u00edvel realiz\u00e1-lo pela biblioteca geopy, mas n\u00e3o ser\u00e1 utilizado pois exige uma grande capacidade de processamento, como n\u00e3o \u00e9 necess\u00e1rio uma grande precis\u00e3o, ser\u00e1 usado a dist\u00e2ncia euclidiana, com uma constante para converter os graus em km.","d1dbb90f":"\u00c9 poss\u00edvel verificar que h\u00e1 uma alta correla\u00e7\u00e3o entre median_income e median_house_value. Contudo, as outras features n\u00e3o tem uma correla\u00e7\u00e3o muito alta. Dessa maneira, vamos criar novas features para tentar aumentar a correla\u00e7\u00e3o com a feature que usaremos para o modelo de predi\u00e7\u00e3o que desejamos.","f9b7ac38":"# **Conjunto de valida\u00e7\u00e3o**","6c59176d":"Comparando novamente as correla\u00e7\u00f5es das features com median_house_value, vemos que as novas features tem uma boa correla\u00e7\u00e3o, como dis_ocean com -0.49 e big_city com -0.46. Agora, iremos eliminar as features com pequena correla\u00e7\u00e3o: **longitude, population, total_bedrooms e household**.","0b4baf97":"# Sum\u00e1rio\n* 1.  [Inicializa\u00e7\u00e3o](#introduction)\n    *     1.1. [Importando as bibliotecas](#importando)\n    *     1.2. [Leitura dos dados](#leitura)\n    *     1.3. [Resolvendo os dados faltantes](#fix)\n* 2. [An\u00e1lise de dados](#analise)\n* 3. [Criando novas features](#features)\n    *     3.1. [Dist\u00e2ncia \u00e0 costa](#costa)\n    *     3.2. [Dist\u00e2ncia de grandes cidades](#grandes)\n    *     3.3. [Quantidade de quartos por casa](#quartos)\n* 4. [Pr\u00e9-processamento](#processamento)   \n* 5. [Predi\u00e7\u00e3o](#predicao)\n* 6. [Submi\u00e7\u00e3o](#submicao)","178abfd1":"Primeiramente, vamos criar um DataFrame para a submi\u00e7\u00e3o e depois vamos colocar o \u00edndice de cada dado com o seu respectivo r\u00f3tulo","2789c3e4":"**3.3. Quantidade de quartos por casa** <a name=\"quartos\"><\/a>","dac718cb":"Como \u00e9 poss\u00edvel observar, o melhor modelo \u00e9 o KNN, ent\u00e3o iremos utiliz\u00e1-lo.","e730b1cc":"O resultado que temos:","6fe701d7":"# 4.Pr\u00e9-processamento <a name=\"processamento\"><\/a>","f706ac8b":"# PMR3508 - Aprendizado de M\u00e1quina e Reconhecimento de Padr\u00f5es\n\nRegress\u00e3o - California Housing\n\nAutor: Jean Carlos Mello Xavier Faria","972b9d77":"# 3. Criando novas features <a name=\"features\"><\/a>","f8c7323b":"**1.2. Leitura dos dados**  <a name=\"leitura\"><\/a>","5d7768b9":"Antes de come\u00e7armos a criar as novas features, vamos dar uma olhada como est\u00e3o distribuidos o pre\u00e7o mediano das casas pelo mapa da calif\u00f3rnia:","213aac67":"# Elastic","99d9dfa6":"# **KNN**","665f2660":"Por fim, vamos exportar nosso DataFrame para um arquivo .csv","b7dd1099":"Agora iremos criar uma nova feature das cidades mais famosas da calif\u00f3rnia.","e8df429c":"# 2. An\u00e1lise dos dados <a name=\"analise\"><\/a>","ca1153a8":"**3.2. Dist\u00e2ncia a grandes cidades** <a name=\"grandes\"><\/a>","c08c1737":"Agora vamos testar alguns regressores e compar\u00e1-los para podermos escolher o melhor.","b468fa00":"Agora iremos retirar as linhas duplicadas.","a9f9412b":"Vemos que as novas features j\u00e1 est\u00e3o no DataFrame de treino.","883e9997":"**3.1. Dist\u00e2ncia \u00e0 costa** <a name=\"costa\"><\/a>","e9ba4902":"Normalizaremos os dados com o **StandarScaler** da biblioteca sklearn","55014ef3":"# 1.Inicializa\u00e7\u00e3o <a name=\"introduction\"><\/a>\n\n**1.1. Importando as bibliotecas**  <a name=\"importando\"><\/a>\n\nPrimeiramente vamos importar as bibliotecas b\u00e1sicas","88924476":"# **Regressor Linear**","1f74b118":"Criaremos uma fun\u00e7\u00e3o para o c\u00e1lclo do RMSLE, onde utilizaresmo em todos os regressores.","4594e260":"# 6.Submiss\u00e3o <a name=\"submicao\"><\/a>","0a369126":"Como n\u00e3o h\u00e1 dados faltantes n\u00e3o ser\u00e1 necess\u00e1rios tret\u00e1-los.","b6c4d8ff":"Calculando a dist\u00e2ncia entre as regi\u00f5es que temos no nosso DataFrame com as grandes cidades:","2ecd17a8":"Colocaremos na vari\u00e1vel Y_train a feature median_house_value e o restante em X_train","15bb3bdf":"E, realizar a predi\u00e7\u00e3o dos dados de teste:","52f78950":"Antes de realizar a submi\u00e7\u00e3o, vamos normalizar os nossos dados de teste:","603cf843":"# 5.Predi\u00e7\u00e3o <a name=\"predicao\"><\/a>","db57fdba":"1.3 Verificando os dados faltantes <a name=\"fix\"><\/a>"}}