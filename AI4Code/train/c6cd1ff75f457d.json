{"cell_type":{"993b2441":"code","f6a73fdf":"code","bc0f1f79":"code","26f90fa1":"code","d3fa0c41":"code","51fe2443":"code","55601a69":"code","8ee2a72c":"code","40073328":"code","dc36d4fa":"code","1bde5807":"code","2a273300":"code","c98f1175":"code","143140c2":"code","b642c403":"code","92fe6c5f":"markdown","1cd94354":"markdown"},"source":{"993b2441":"import optuna\nimport pandas as pd\nimport numpy as np\nimport time\nfrom datetime import datetime\nfrom xgboost import XGBRegressor \nfrom sklearn import preprocessing as pp\nfrom sklearn.preprocessing import StandardScaler,OrdinalEncoder, FunctionTransformer, RobustScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split,KFold\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom scipy import stats\npd.options.mode.chained_assignment = None","f6a73fdf":"#for submitting only\nSEED=42\nEVALUATE=False\nUSE_OPTUNA_TO_EVALUATE=False\nSUBMIT=True\nGPU=False\n#CPU may provide better results per my experience and\n#https:\/\/towardsdatascience.com\/xgboost-regression-training-on-cpu-and-gpu-in-python-5a8187a43395\nSCALE=True\nTREAT_OUTLIERS=True\nUSE_KFOLD=True\nFOLD_COUNT=5","bc0f1f79":"#function as well as strategy chosen (when invoked below) adapted from \n#https:\/\/www.kaggle.com\/sgedela\/30-days-of-ml-competition\ndef treatoutliers(df, columns=None, factor=1.5, method='IQR', treatment='cap'):\n\n    for column in columns:\n        if method == 'STD':\n            permissable_std = factor * df[column].std()\n            col_mean = df[column].mean()\n            floor, ceil = col_mean - permissable_std, col_mean + permissable_std\n        elif method == 'IQR':\n            Q1 = df[column].quantile(0.25)\n            Q3 = df[column].quantile(0.75)\n            IQR = Q3 - Q1\n            floor, ceil = Q1 - factor * IQR, Q3 + factor * IQR\n        if treatment == 'remove':\n            df = df[(df[column] >= floor) & (df[column] <= ceil)]\n        elif treatment == 'cap':\n            df[column] = df[column].clip(floor, ceil)\n\n    return df\n    \n     \n    \n#Quantile-based Flooring and Capping chosen, can try different columns\/different strategies\n#for colName in [['target','cont0','cont6','cont8']]:\n #   train = treatoutliers(train,columns=colName, treatment='cap')      \n\n","26f90fa1":"start_time=time.time()\ntest= pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")","d3fa0c41":"#train.drop(['cont10', 'cont8'],axis=1, inplace=True)\n#test.drop(['cont10', 'cont8'],axis=1, inplace=True)","51fe2443":"if(TREAT_OUTLIERS==True):\n    for colName in [['target','cont0','cont6','cont8']]:\n    #for colName in [['target','cont0','cont6']]:\n        train = treatoutliers(train,columns=colName, treatment='cap') ","55601a69":"y = train['target']\nfeatures = train.drop(['target'], axis=1)\nX = features.copy()\nX_test = test.copy()","8ee2a72c":"X_test_id=X_test[\"id\"]\nX.drop(\"id\", inplace=True, axis=1)\nX_test.drop(\"id\", inplace=True, axis=1)\n ","40073328":"numeric_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\nnon_numeric_columns= [cname for cname in X.columns if X[cname].dtype == \"object\"]","dc36d4fa":"#strategy adapted from https:\/\/www.kaggle.com\/sgedela\/30-days-of-ml-competition\nnumerical_transformer = Pipeline(steps=[\n       ('imputer', SimpleImputer(strategy='mean'))\n         ,('RobustScaler', RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True))  \n       ,('scaler', StandardScaler())\n ])\n\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')) \n       ,('scaler', OrdinalEncoder())\n])\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numeric_cols),\n        ('cat', categorical_transformer, non_numeric_columns)\n    ],\n    remainder=\"passthrough\"\n  )","1bde5807":"#for optuna, validation set is used for early stopping.\nif(EVALUATE==True and USE_OPTUNA_TO_EVALUATE==True):\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=SEED)\n","2a273300":"if(SCALE==True):\n    X=preprocessor.fit_transform(X)\n    X_test=preprocessor.transform(X_test)\n    if(EVALUATE==True and USE_OPTUNA_TO_EVALUATE==True):\n        X_valid=preprocessor.transform(X_valid)\nelse:\n    ordinal_encoder = pp.OrdinalEncoder()\n    X[non_numeric_columns] = ordinal_encoder.fit_transform(features[non_numeric_columns])\n    X_test[non_numeric_columns] = ordinal_encoder.transform(test[non_numeric_columns])\n    if(EVALUATE==True and USE_OPTUNA_TO_EVALUATE==True):\n        X_valid[non_numeric_columns] = ordinal_encoder.transform(X_valid[non_numeric_columns])\n","c98f1175":"def objective(trial,data=X,target=y):\n   \n    #param = {\n      #  'n_estimators': trial.suggest_int('n_estimators', 10500,11250),\n      #  'reg_lambda':   trial.suggest_loguniform('reg_lambda', .00001, .001),\n      #  'reg_alpha': trial.suggest_loguniform('reg_alpha', 17.0,21.5),\n     #   'colsample_bytree': trial.suggest_loguniform('colsample_bytree', .05,.09),\n      #  'subsample': trial.suggest_loguniform('subsample', .75,.83),\n     #   'gamma':  trial.suggest_loguniform('gamma',  0.0001,.005),\n      #  'learning_rate': trial.suggest_loguniform('learning_rate', 0.03, 0.05),\n      #  'max_depth': trial.suggest_categorical('max_depth', [2,3]),\n      #  'random_state': SEED  \n             #     }\n\n    param = {\n        'n_estimators': trial.suggest_int('n_estimators', 11250,12000),\n        'reg_lambda':   trial.suggest_loguniform('reg_lambda', .00001, .0001),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 17.25,18.0),\n        'colsample_bytree': trial.suggest_loguniform('colsample_bytree', .05,.09),\n        'subsample': trial.suggest_loguniform('subsample', .76,.89),\n        'gamma':  trial.suggest_loguniform('gamma',  0.0001,.005),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.0480, 0.0575),\n        'max_depth': trial.suggest_categorical('max_depth', [3]),\n        'random_state': SEED  \n                  }\n\n    if(GPU==True):\n        param['tree_method']='gpu_hist'\n\n    model = XGBRegressor(**param)  \n    model.fit(X,y,eval_set=[(X_valid,y_valid)],early_stopping_rounds=100,verbose=False)\n        \n    preds = model.predict(X_valid)\n    \n    rmse = mean_squared_error(y_valid, preds,squared=False)\n    \n    return rmse\n\n","143140c2":"#set this to final parameters if submitting\n#or to parameters you want to test if using KFOLD for testing a model.\n#Original optuna_findings\nparam_final_orig = {\n          # 'n_estimators': 10620, \n           # 'reg_lambda': 0.0012204823110481143, \n           # 'reg_alpha': 20.693298506499715, \n           # 'colsample_bytree': 0.10479707020226768, \n          #  'subsample': 0.8243541431063447, \n          #  'gamma': 0.003389336813429638, \n          #  'learning_rate': 0.04982149348329424, \n          #  'max_depth': 3,   \n          #   'random_state': SEED,\n          #   'n_jobs': 4\n    \n    #xval 0.7068011987787057\n    'n_estimators': 10750, \n            'reg_lambda': 0, \n            'reg_alpha': 18, \n            'colsample_bytree': 0.09, \n            'subsample': .80, \n            'gamma': 0.00375, \n            'learning_rate': 0.04, \n            'max_depth': 3,   \n             'random_state': SEED,\n             'n_jobs': 4\n                  }\nparam_final = {\n        'n_estimators': 11050, \n            'reg_lambda': 0, \n            'reg_alpha': 17.5, \n            'colsample_bytree': 0.11, \n            'subsample': .83, \n            'gamma': 0.00375, \n            'learning_rate': 0.03, \n            'max_depth': 3,   \n             'random_state': SEED,\n             'n_jobs': 4\n                  }\n\nif(GPU==True):\n    param_final['tree_method']='gpu_hist'\n    \n  ","b642c403":"if(EVALUATE==True): \n    #if True this will evaluate n_trials models\n    #if False, use param_final to set the paramenters to be tested\n    #and  set USE_FOLD = True to evaluate your model\n    if(USE_OPTUNA_TO_EVALUATE)==True:\n        study = optuna.create_study(direction='minimize')\n        study.optimize(objective, n_trials=120)\n        print(f'Number of finished trials: {len(study.trials)}')\n        print(f'Best trial: {study.best_trial.params}')\n        print(f'Best score: {study.best_trial.value}')\nif (USE_KFOLD==True):\n    print(f\"Using KFold Validation {datetime.now()}\")\n    model = XGBRegressor(**param_final)  \n    kf = KFold(n_splits=FOLD_COUNT, shuffle=True, random_state=SEED)\n    valid_rmses= []\n    test_rmses = []\n    preds_valid= [] \n    preds_test =  []\n    for i, (train_idx, test_idx) in enumerate(kf.split(X)): \n        print(f\"Fitting for fold {i}\")\n        if(isinstance(X,np.ndarray)==True):\n            X_train_split, X_valid_split = X[train_idx], X[test_idx]\n            y_train_split, y_valid_split = y[train_idx], y[test_idx]\n        else:\n            X_train_split, X_valid_split = X.iloc[train_idx], X.iloc[test_idx]\n            y_train_split, y_valid_split = y.iloc[train_idx], y.iloc[test_idx]\n        model.fit(X_train_split, y_train_split, verbose=0)\n        if(EVALUATE==True):\n            preds_valid_split = model.predict(X_valid_split)\n            rmse=mean_squared_error(preds_valid_split, y_valid_split, squared=False)\n            valid_rmses.append(rmse)\n            print(f\"rmse fold {i} = {rmse}\")\n            preds_valid.append(preds_valid_split)\n        if(SUBMIT==True):  \n            print(f'Saving values out for fold {i}')\n            y_pred_fold=model.predict(X_test)\n            preds_test.append(y_pred_fold)\n            fold_output=pd.DataFrame({'id': X_test_id,\n                   'target': y_pred_fold})\n            fold_output.to_csv('fold_' + str(i) + '.csv', index=False)\n    if(EVALUATE==True):\n        print(f\"Average Validation rmse: {np.mean(valid_rmses)}\")   \n    if(SUBMIT==True):\n        y_pred=0\n        for i in range(len(preds_test)):\n            y_pred+=preds_test[i] * (1\/len(preds_test))\nelse:\n    if(SUBMIT==True):\n    #if not using  kfold, fit on all the data \n        model = XGBRegressor(**param_final)\n        model.fit(X,y, verbose=0)\n        y_pred=model.predict(X_test)\n        \nif(SUBMIT==True):\n    output = pd.DataFrame({'id': X_test_id,\n           'target': y_pred})\n    output.to_csv('submission.csv', index=False)\n    print(f\"Submission complete {datetime.now()}\")    \n\ntime_elapsed=time.time()-start_time\nprint(f\"Finished at {datetime.now()} in {time_elapsed} seconds\")\n\n","92fe6c5f":"**This notbook permits:**\n* Evaluating a notebook via Optuna or Cross-Validation, or both\n* To use optuna, set parameter ranges within objective function below.  \n* To use cross-validation, set up parameters to be tested in param_final variable below\n* Also provides testing for preprocessing options (scaling, outliers), and opportunity to tweak parameters used for these \n* Also doubles as an inference notebook--set SUBMIT=True with parameter values chosen set  in param_final.\n","1cd94354":"**Output will look like below, can be used to set final parameters:**\n\nI 2021-08-29 23:40:18,666] Trial 118 finished with value: 0.6947695119951155 and parameters: {'n_estimators': 10476, 'reg_lambda': 0.0013570526201259184, 'reg_alpha': 21.25824391695087, 'colsample_bytree': 0.10443384333437186, 'subsample': 0.8267115316382952, 'gamma': 0.0003034011816721243, 'learning_rate': 0.049988594576110504, 'max_depth': 3}. Best is trial 104 with value: 0.6946291985066091.\n[I 2021-08-29 23:40:52,750] Trial 119 finished with value: 0.6963704873297191 and parameters: {'n_estimators': 10549, 'reg_lambda': 0.001146028000276721, 'reg_alpha': 20.880804898337985, 'colsample_bytree': 0.10588386620670776, 'subsample': 0.8196750939180626, 'gamma': 0.003415339467606639, 'learning_rate': 0.03715744868871302, 'max_depth': 3}. Best is trial 104 with value: 0.6946291985066091.\n\nNumber of finished trials: 120\nBest trial: {'n_estimators': 10620, 'reg_lambda': 0.0012204823110481143, 'reg_alpha': 20.693298506499715, 'colsample_bytree': 0.10479707020226768, 'subsample': 0.8243541431063447, 'gamma': 0.003389336813429638, 'learning_rate': 0.04982149348329424, 'max_depth': 3}\nFinished in 4059.3787190914154 seconds"}}