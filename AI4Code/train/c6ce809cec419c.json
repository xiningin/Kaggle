{"cell_type":{"c259f149":"code","51d70995":"code","59c36bbf":"code","cc5f04e7":"code","6b153e6c":"code","497ad7b1":"code","9c193328":"code","144c9eca":"code","aa5240b8":"code","c066f6e0":"code","23def29b":"code","93ff7eb2":"code","8bf43ddf":"code","59f83c72":"code","6d3f4c06":"code","13cc8455":"code","a06043cb":"code","b620ee76":"code","f4aef3c0":"code","e21bf94d":"code","fbab70c7":"code","952c2c07":"code","705231e3":"code","144bcec6":"markdown","48a87e01":"markdown","9c252bfa":"markdown","5bb2371a":"markdown","211d26de":"markdown","a898109b":"markdown"},"source":{"c259f149":"from IPython.utils import io\nwith io.capture_output() as captured:\n    !pip install scispacy\n    !pip install '\/kaggle\/input\/pythonmodule\/en_core_sci_md-0.2.4'","51d70995":"import numpy as np \nimport pandas as pd\nimport scispacy\nimport spacy\nfrom spacy.lang.en import English\nfrom gensim.models import Word2Vec\nfrom tqdm import tqdm\nimport en_core_sci_md\nimport logging \n\ndf = pd.read_csv('\/kaggle\/input\/cord19-body-text\/cord19_df.csv')\n\ndf.abstract = df.abstract.dropna()\ndf.abstract = df.abstract.astype(str)\nabstract_doc = ' '.join(df.abstract)\n\nnlp = English()\nnlp.add_pipe(nlp.create_pipe('sentencizer'))\nnlp.max_length = 40000000\ndoc = nlp(abstract_doc)\nsentences = [sent.string.strip() for sent in doc.sents]\n\nnlp_tkn = en_core_sci_md.load()\ncustomize_stop_words = ['Abstract', 'article']\nfor w in customize_stop_words:\n    nlp_tkn.vocab[w].is_stop = True\n\nabstract_tokens = []\nfor sentence in tqdm(sentences):\n    sent = nlp_tkn(sentence)\n    token_list = [token.lemma_.lower() for token in sent if not token.is_punct | token.is_space | token.is_stop] \n    abstract_tokens.append(token_list)\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\nmodel = Word2Vec(abstract_tokens, \n                 min_count=5,   # Ignore words that appear less than this\n                 size=100,      # Dimensionality of word embeddings\n                 workers=4,     # Number of processors (parallelisation)\n                 window=10,     # Context window for words during training) \n                 iter=10)     \n\nmodel.save('word2vec.model')\nmodel = Word2Vec.load(\"word2vec.model\")\nmodel.wv.save_word2vec_format('abstract_word2vec_embeddings')\nmodel.most_similar(positive=['transmission'], topn = 10)","59c36bbf":"import numpy as np \nimport pandas as pd \nimport re","cc5f04e7":"def count_freq(input_text, key_word):\n    '''\n    This function is to calculate the frequency of each key_word in an input text.\n    '''\n    count = sum(1 for _ in re.finditer(r'(?i)\\b%s\\b' % re.escape(key_word), input_text))\n    return count\n\ndef frequency_generator(key_words_weight, df, context_col):\n    '''\n    This function consumes the count_freq function and applies on the dataframe column (context_col) which has input text.\n    It creates new columns with frequency for each key word in the dataframe. \n    '''\n    key_words = [i[0] for i in key_words_weight]\n    df[context_col] = df[context_col].astype(str)\n    for word in key_words:\n        df['count_' + str(word)] = df[context_col].apply(lambda x: count_freq(x, word))\n    return df\n\ndef generate_training_data(df, context_col_name, sum_col_name, label_col_name, lower_b, neg_pos_ratio):\n    '''\n    This function has the following steps:\n        1.Drop null and duplicated data in the context column.\n        2.Generate positive examples which have at least n(lower_b) frequency of key words in the context.\n        3.Generate negative examples which have none of the key words in the context. Its size is neg_pos_ratio * the size of the postive examples.\n        4.Combine the positive and negative examples as the traning set and rename the label column.\n    '''\n    df = df.dropna(subset = [context_col_name])\n    df = df.drop_duplicates(subset = [context_col_name])\n    df_positive = df[df[sum_col_name] >= lower_b]\n    negative_set = df[df[sum_col_name] == 0]\n    df_negative = negative_set.sample(n = df_positive.shape[0] * neg_pos_ratio, replace=True, random_state=1)\n    df_training = df_positive.append(df_negative)[[context_col_name, label_col_name]]\n    df_training = df_training.rename(columns = {label_col_name:'label'})\n    return df_training\n\ndef generate_test_data(df, context_col_name, sum_col_name, label_col_name, lower_b, upper_b, positive_size, neg_pos_ratio):\n    '''\n    This funtion is to generate test set which is similar as generating the training data.\n    The distinctive point is the upper bound of positive set is defined in the arguements. \n    '''\n    df = df.dropna(subset = [context_col_name])\n    df = df.drop_duplicates(subset = [context_col_name])\n    test_positive = df[(df[sum_col_name] > lower_b) & (df[sum_col_name] < upper_b)].sample(n = positive_size, replace = True, random_state = 1)\n    test_negative = df[df[sum_col_name] == 0]\n    test_negative = test_negative.sample(n = test_positive.shape[0] * neg_pos_ratio, replace=True, random_state=1)\n    df_test = test_positive.append(test_negative)[[context_col_name, sum_col_name, label_col_name]]\n    df_test = df_test.rename(columns = {label_col_name:'label'})\n    return df_test","6b153e6c":"root_path = '\/kaggle\/input\/CORD-19-research-challenge'\nmetadata_path = f'{root_path}\/metadata.csv'\nmetadata_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})","497ad7b1":"'''Key words extracted from word2vec. For key words: transmission, incubation, virus shedding and shedding virus \nexisting in the first task and subtasks, the weights are 1 for each word. For the remaining ones extracted \nfrom word2vec, the weights are based on word2vec similarity scores.\n'''\nw2v_model = Word2Vec.load(\"\/kaggle\/working\/word2vec.model\")\nw2v_model.wv.save_word2vec_format('abstract_word2vec_embeddings')\nkey_words_weight = w2v_model.most_similar(positive=['transmission'], topn = 10)\nkey_words_weight = [('transmission', 1), \n ('incubation', 1),\n ('virus shedding', 1),\n ('shedding virus', 1),\n ('environmental stability', 1)] + key_words_weight\n\nprint(key_words_weight)\nkeywords_weight_dict = dict(key_words_weight)","9c193328":"neg_pos_ratio = 4 #4 means the ratio of negative examples to positive examples is 4:1\nthreshold = 3 #3 is the 97th percentile of the dot product of key words frequency and weights on the abstract context\nmeta_df = frequency_generator(key_words_weight, metadata_df, 'abstract')\ncol_list = ['count_' + i for i in list(keywords_weight_dict)]\nmeta_df['keywords_sum'] = meta_df[col_list].values.dot(pd.Series(keywords_weight_dict))\nmeta_df['keywords_exist'] = meta_df['keywords_sum'].apply(lambda x: 1 if x>= threshold else 0)\ntraining_set = generate_training_data(meta_df, 'abstract', 'keywords_sum', 'keywords_exist', threshold, neg_pos_ratio)\ntraining_set.to_csv('training_set_threshold_' + str(threshold) + '.csv')","144c9eca":"threshold = 38 #38 is the 97th percentile of the dot product of key words frequency and weights on the body text context\nfull_data = pd.read_csv('\/kaggle\/input\/cord19-body-text\/cord19_df.csv') #This dataset is the output of this notebook: https:\/\/www.kaggle.com\/danielwolffram\/cord-19-create-dataframe\nfull_text_freq = frequency_generator(key_words_weight, full_data, 'body_text')\nfull_text_freq['keywords_sum'] = full_text_freq[col_list].values.dot(pd.Series(keywords_weight_dict))\nfull_text_freq['keywords_exist'] = full_text_freq['keywords_sum'].apply(lambda x: 1 if x >= threshold else (0 if x == 0 else None))","aa5240b8":"#Take the first 3000 characters of body_text as test data\nfull_text_freq['body_text'] = full_text_freq['body_text'].apply(lambda x: re.sub(r'[^\\w\\s]','',x)[:3000])\nfull_text_freq = full_text_freq.rename(columns = {'keywords_exist': 'label'})\nfull_text_freq.to_csv('body_text_with_keywords_freq_38.csv')","c066f6e0":"! pip install transformers","23def29b":"import tensorflow as tf\nimport torch\n\ndevice_name = tf.test.gpu_device_name()\ndevice = torch.device(\"cuda\")\nif device_name == '\/device:GPU:0':\n    print('Found GPU at: {}'.format(device_name))\n    print('GPU:', torch.cuda.get_device_name(0))\nelse:\n    raise SystemError('GPU device not found')","93ff7eb2":"import pandas as pd\nimport csv\nimport random\nimport time\nimport datetime\nfrom transformers import BertTokenizer\nfrom torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig\nfrom transformers import get_linear_schedule_with_warmup","8bf43ddf":"def flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) \/ len(labels_flat)\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","59f83c72":"class train_bert:\n    def __init__(self, training_df, context_col, label_col):\n        self.df = training_df\n        self.df[context_col] = self.df[context_col].astype('str')\n        self.context = self.df[context_col].values\n        self.label = self.df[label_col].values\n    \n    def tokenizer(self):\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n        self.input_ids = []\n        self.attention_masks = []\n        for item in self.context:\n            self.encoded_dict = self.tokenizer.encode_plus(\n                    item,                      \n                    add_special_tokens = True, \n                    max_length = 300,           \n                    pad_to_max_length = True,\n                    return_attention_mask = True,  \n                    return_tensors = 'pt')\n            self.input_ids.append(self.encoded_dict['input_ids'])\n            self.attention_masks.append(self.encoded_dict['attention_mask'])\n        self.input_ids = torch.cat(self.input_ids, dim=0)\n        self.attention_masks = torch.cat(self.attention_masks, dim=0)\n        self.label = torch.tensor(self.label)    \n        return self\n    \n    def data_preparation(self, train_val_ratio): \n        self.dataset = TensorDataset(self.input_ids, self.attention_masks, self.label)\n        self.train_size = int(train_val_ratio * len(self.dataset))\n        self.val_size = len(self.dataset) - self.train_size\n        self.train_dataset, self.val_dataset = random_split(self.dataset, [self.train_size, self.val_size])\n        print('{:>5,} training samples'.format(self.train_size))\n        print('{:>5,} validation samples'.format(self.val_size))\n        return self\n    \n    def generate_data_loader(self, batch_size):\n        self.batch_size = batch_size\n        self.train_dataloader = DataLoader(\n                    self.train_dataset,\n                    sampler = RandomSampler(self.train_dataset),\n                    batch_size = self.batch_size)\n        self.validation_dataloader = DataLoader(\n                    self.val_dataset,\n                    sampler = SequentialSampler(self.val_dataset),\n                    batch_size = self.batch_size)\n        return self\n    \n    def model(self):\n        self.model = BertForSequenceClassification.from_pretrained(\n                    'bert-base-uncased', \n                    num_labels = 2,  \n                    output_attentions = False, \n                    output_hidden_states = False)\n        self.model.cuda()\n        return self\n    \n    def optimizer(self):\n        self.optimizer = AdamW(self.model.parameters(),\n                  lr = 2e-5, \n                  eps = 1e-8\n                )\n        return self\n    \n    def scheduler(self, epochs):\n        self.epochs = epochs\n        self.total_steps = len(self.train_dataloader) * self.epochs\n        self.scheduler = get_linear_schedule_with_warmup(\n                    self.optimizer, \n                    num_warmup_steps = 0,\n                    num_training_steps = self.total_steps)\n        return self\n    \n    def fine_tuning(self, seed_val):\n        self.seed_val = seed_val\n        random.seed(self.seed_val)\n        np.random.seed(self.seed_val)\n        torch.manual_seed(self.seed_val)\n        torch.cuda.manual_seed_all(self.seed_val)\n        self.training_stats = []\n        self.total_t0 = time.time()\n        for epoch_i in range(0, self.epochs):\n            print(\"\")\n            print('======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, self.epochs))\n            print('Training...')\n            self.t0 = time.time()\n            self.total_train_loss = 0\n            self.model.train()\n            \n            for step, batch in enumerate(self.train_dataloader):\n                if step % 40 == 0 and not step == 0:\n                    self.elapsed = format_time(time.time() - self.t0)\n                    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(self.train_dataloader), self.elapsed))\n                self.b_input_ids = batch[0].to(device)\n                self.b_input_mask = batch[1].to(device)\n                self.b_labels = batch[2].to(device)\n                self.model.zero_grad()        \n\n                self.loss, self.logits = self.model(self.b_input_ids, \n                             token_type_ids=None, \n                             attention_mask=self.b_input_mask, \n                             labels=self.b_labels)\n                self.total_train_loss += self.loss.item()\n                self.loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                self.optimizer.step()\n                self.scheduler.step()\n            self.avg_train_loss = self.total_train_loss \/ len(self.train_dataloader)        \n            self.training_time = format_time(time.time() - self.t0)\n            print(\"\")\n            print(\"  Average training loss: {0:.2f}\".format(self.avg_train_loss))\n            print(\"  Training epcoh took: {:}\".format(self.training_time))\n\n            print(\"\")\n            print(\"Running Validation...\")\n\n            self.t0 = time.time()\n            self.model.eval()\n\n            self.total_eval_accuracy = 0\n            self.total_eval_loss = 0\n            self.nb_eval_steps = 0\n\n            for batch in self.validation_dataloader:\n                self.b_input_ids = batch[0].to(device)\n                self.b_input_mask = batch[1].to(device)\n                self.b_labels = batch[2].to(device)\n                with torch.no_grad():\n                    (self.loss, self.logits) = self.model(self.b_input_ids, \n                                   token_type_ids=None, \n                                   attention_mask=self.b_input_mask,\n                                   labels=self.b_labels)\n                self.total_eval_loss += self.loss.item()\n                self.logits = self.logits.detach().cpu().numpy()\n                self.label_ids = self.b_labels.to('cpu').numpy()\n                self.total_eval_accuracy += flat_accuracy(self.logits, self.label_ids)\n            self.avg_val_accuracy = self.total_eval_accuracy \/ len(self.validation_dataloader)\n            print(\"  Accuracy: {0:.3f}\".format(self.avg_val_accuracy))\n            self.avg_val_loss = self.total_eval_loss \/ len(self.validation_dataloader)\n            self.validation_time = format_time(time.time() - self.t0)\n            print(\"  Validation Loss: {0:.3f}\".format(self.avg_val_loss))\n            print(\"  Validation took: {:}\".format(self.validation_time))\n            self.training_stats.append(\n                {\n            'epoch': epoch_i + 1,\n            'Training Loss': self.avg_train_loss,\n            'Valid. Loss': self.avg_val_loss,\n            'Valid. Accur.': self.avg_val_accuracy,\n            'Training Time': self.training_time,\n            'Validation Time': self.validation_time\n                }\n            )\n        \n        print(\"\")\n        print(\"Training complete!\")\n        print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()- self.total_t0)))\n        return self\n    \n    def save_model(self, file_name):\n        self.file_name = file_name\n        torch.save(self.model, self.file_name)","6d3f4c06":"df = pd.read_csv('\/kaggle\/working\/training_set_threshold_3.csv', engine = 'python', sep= ',', header=0)\nbert = train_bert(df, 'abstract', 'label' )\ntoken = bert.tokenizer().data_preparation(0.9).generate_data_loader(16).model().optimizer().scheduler(2).fine_tuning(42)","13cc8455":"token.save_model('bert_model_3')","a06043cb":"model_train = torch.load('\/kaggle\/working\/bert_model_3')","b620ee76":"test_df = pd.read_csv('\/kaggle\/input\/fulltestbodytext38\/full_test_body_text_38.csv')","f4aef3c0":"class prediction:\n    \n    def __init__(self, test_df, context_col, label_col, model_train):\n        self.df = test_df\n        self.context_col = context_col\n        self.model_train = model_train\n        self.df[context_col] = self.df[context_col].astype('str')\n        self.context = self.df[context_col].values\n        self.label = self.df[label_col].values\n        print('Number of {}: {:,}\\n'.format(context_col, df.shape[0]))\n    \n    def test_tokenizer(self):\n        train_bert.tokenizer(self)\n        return self\n    \n    def prediction(self, batch_size):\n        self.batch_size = batch_size\n        self.prediction_data = TensorDataset(self.input_ids, self.attention_masks, self.label)\n        self.prediction_sampler = SequentialSampler(self.prediction_data)\n        self.prediction_dataloader = DataLoader(self.prediction_data, sampler=self.prediction_sampler, batch_size=self.batch_size)\n        print('Predicting labels for {:,} test {}...'.format(len(self.input_ids), self.context_col))\n        self.model_train.eval()\n        self.predictions , self.true_labels = [], []\n        for batch in self.prediction_dataloader:\n            batch = tuple(t.to(device) for t in batch)\n            self.b_input_ids, self.b_input_mask, self.b_labels = batch\n            with torch.no_grad():\n                self.outputs = self.model_train(self.b_input_ids, token_type_ids=None, \n                      attention_mask=self.b_input_mask)\n            self.logits = self.outputs[0]\n            self.logits = self.logits.detach().cpu().numpy()\n            self.label_ids = self.b_labels.to('cpu').numpy()\n            self.predictions.append(self.logits)\n            self.true_labels.append(self.label_ids)\n        print('    DONE.')\n        return self","e21bf94d":"prediction = prediction(test_df, 'body_text', 'label', model_train).test_tokenizer().prediction(16)","fbab70c7":"flat_predictions = np.concatenate(prediction.predictions, axis=0)\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()\nflat_true_labels = np.concatenate(prediction.true_labels, axis=0)","952c2c07":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(flat_true_labels, flat_predictions)","705231e3":"tn, fp, fn, tp = confusion_matrix(flat_true_labels, flat_predictions).ravel()\nprecision = tp \/ (tp + fp)\nrecall = tp \/ (tp + fn)\nprint('The precision is {}'.format(precision))\nprint('The recall is {}'.format(recall))","144bcec6":"# Word2Vec on key word *transmission*","48a87e01":"# **Generat Training and Test Data**","9c252bfa":"### Generate test set on body text","5bb2371a":"# Bert Classification Model\n#### The model is based on this tutorial: https:\/\/mccormickml.com\/2019\/07\/22\/BERT-fine-tuning\/","211d26de":"### Generate training set on abstract","a898109b":"## Goal\n\nThis notebook aims to transfer task1 to a classification problem using Word2Vec and BERT. Given the key words in specific task questions, the program is able to predict if a paper is highly related to the questions or not. \n\n## Aproaches\n\n### **Generate training data**\n\n#### **Create key words set**\n \nThe training data is generated by Word2Vec. We manually chose key words in task1 (i.e. *transmission, incubation, environmental stability, virus shedding *and* shedding virus*). Then we ran Word2Vec on the key word *transmission* and found top 10 words which are higly similar with it. Two sets of key words are combined as the key words set and a weight is assigned to each word to calculate the frequency score in the abstract. For the manually pulled key words, the weight is 1. For the ones generated from algorithm, the weight is the Word2Vec similarity.\n\nThe key words and the weights are as follows ('key word', weight):\n\n[('transmission', 1),  \n ('incubation', 1),  \n ('environmental stability', 1),  \n ('virus shedding', 1),  \n ('shedding virus', 1),  \n ('spread', 0.7522934079170227),  \n ('spillover', 0.6482195854187012),  \n ('transmissibility', 0.6166895031929016),  \n ('contact', 0.6136766076087952),  \n ('transmit', 0.6054409742355347),  \n ('dispersal', 0.585004985332489),  \n ('importation', 0.551548182964325),  \n ('zoonotic', 0.5362335443496704),  \n ('epidemic', 0.5349624156951904),  \n ('movement', 0.5341535210609436)]\n \n#### **Calculate the sum of key words score**\n\nWe took abstract in *metadata.csv* as context to search for key words existence. We computed the key words score by the dot of frequency in the context and the weight of each word in the key words set. \n\n#### **Label extreme positive and negative examples**\nThen we extracted extreme positive and extreme negative examples from the dataset and gave them binary labels. \n\nThe defination of extreme positive and negative examples are as follows:  \n\n    Extreme positive examples: Abstracts which have key words score greater or equal than 3.   \n    Extreme negative examples: Abstracts which have key words score equal to 0. \n\nWe labeled extreme positive examples as 1 and negative examples as 0.\n\n### **Run pre-trained BERT model and fine-tuning**\n\nWe used BERT classification model to train data and fine-tuning the parameters. \nThe ratio of training set and validation set is 9:1. \nThe model was run in 16 batches and 2 epoches. \nWe saved the model as output and load it to run on the test set. \n\n### **Predict on test set**\n\nThe test set is created using body_text as context. Because body_text has longer context than abstract so the key words frequency in a body_text should be higher than that in an abstract. Since 3 is the 97th percentile of the key words score in abstracts. We found 38 as the threshold for body_text which is also the 97th percentile of the key words score in body_text. \n\nAfter generating the test set, we loaded the fine-tuned BERT model to predict if each body_text's label. \nThe performance metrics is also provided as part of the result. \n\nOnce we have the model ready, then we can use it to classify the papers which are highly related to the topic given the key words. \n\n### **Acknowledgement**\n\nSome of our work is based on other people's work:\n\nBody_text dataframe: https:\/\/www.kaggle.com\/danielwolffram\/cord-19-create-dataframe  \nBERT classification fine-tuning: https:\/\/mccormickml.com\/2019\/07\/22\/BERT-fine-tuning\/\n"}}