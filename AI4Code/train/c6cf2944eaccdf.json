{"cell_type":{"e2e31c11":"code","5fe92faa":"code","32247e33":"code","77d30d79":"code","1bf951ce":"code","40b87aee":"code","8bee0771":"code","2111993f":"code","b5a29b6a":"code","b646aa6b":"code","1a5d3d1e":"code","6b4a3c6d":"code","42f56813":"code","236f7258":"code","92a396eb":"code","ff89309c":"code","3673084f":"code","33bd0fb9":"code","dc07b890":"code","abb927a9":"code","026df92d":"code","18881fc2":"code","9034953a":"code","f741aea6":"code","f4e7f4cc":"code","ea9c245d":"code","a74249e2":"code","64349039":"code","a346d5bf":"code","774aea49":"markdown","ab359087":"markdown","8f8da98a":"markdown","19a50673":"markdown","5455b408":"markdown","987ab2df":"markdown","0ebe84d0":"markdown","db8d9e5f":"markdown","95d0d045":"markdown","0b5b5318":"markdown","86c66d1e":"markdown","0a884b21":"markdown","5ac2ad0f":"markdown","364a10bb":"markdown","8f42e03c":"markdown","9f424709":"markdown","0f42284d":"markdown","cea2d608":"markdown","a4443d7d":"markdown","cdd04921":"markdown","01bed3eb":"markdown","912d5273":"markdown","6d2cb023":"markdown","ec228b75":"markdown"},"source":{"e2e31c11":"import os\nimport glob\nimport random\nimport cv2\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\nfrom torch.utils.data import DataLoader\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nfrom torchvision.utils import make_grid\nimport torchvision.transforms as tt\nimport albumentations as A\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm","5fe92faa":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using {} device\".format(device))","32247e33":"def set_seed(seed = 0):\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    \nset_seed()","77d30d79":"ROOT_PATH = '..\/input\/lgg-mri-segmentation\/kaggle_3m\/'\n\nmask_files = glob.glob(ROOT_PATH + '*\/*_mask*')\nimage_files = [file.replace('_mask', '') for file in mask_files]\n\ndef diagnosis(mask_path):\n    return 1 if np.max(cv2.imread(mask_path)) > 0 else 0\n\nfiles_df = pd.DataFrame({\"image_path\": image_files,\n                  \"mask_path\": mask_files,\n                  \"diagnosis\": [diagnosis(x) for x in mask_files]})\n\nfiles_df","1bf951ce":"ax = files_df['diagnosis'].value_counts().plot(kind='bar', stacked=True, figsize=(6,6), color=['green', 'red'])\nax.set_title('Data Distribution', fontsize=15)\nax.set_ylabel('No. of Images', fontsize=15)\nax.set_xticklabels(['No Tumor', 'Tumor'], fontsize=12, rotation=0)\nfor i, rows in enumerate(files_df['diagnosis'].value_counts().values):\n    ax.annotate(int(rows), xy=(i, rows+12), ha='center', fontweight='bold', fontsize=12)","40b87aee":"train_df, val_df = train_test_split(files_df, stratify=files_df['diagnosis'], test_size=0.1, random_state=0)\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\n\ntrain_df, test_df = train_test_split(train_df, stratify=train_df['diagnosis'], test_size=0.15, random_state=0)\ntrain_df = train_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)\n\nprint(\"Train: {}\\nVal: {}\\nTest: {}\".format(train_df.shape, val_df.shape, test_df.shape))","8bee0771":"set_seed()\n\nimages, masks = [], []\ndf_positive = train_df[train_df['diagnosis']==1].sample(5).values\n\nfor sample in df_positive:\n    img = cv2.imread(sample[0])\n    mask = cv2.imread(sample[1])\n    images.append(img)\n    masks.append(mask)\nimages = np.hstack(np.array(images))\nmasks = np.hstack(np.array(masks))\n\nfig = plt.figure(figsize=(15,10))\ngrid = ImageGrid(fig, 111, nrows_ncols=(3,1), axes_pad=0.4)\n\ngrid[0].imshow(images)\ngrid[0].set_title('Images', fontsize=15)\ngrid[0].axis('off')\ngrid[1].imshow(masks)\ngrid[1].set_title('Masks', fontsize=15)\ngrid[1].axis('off')\ngrid[2].imshow(images)\ngrid[2].imshow(masks, alpha=0.4)\ngrid[2].set_title('Brain MRI with mask', fontsize=15)\ngrid[2].axis('off')","2111993f":"class BrainDataset(data.Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        image = cv2.imread(self.df.iloc[idx, 0])\n        image = np.array(image)\/255.\n        mask = cv2.imread(self.df.iloc[idx, 1], 0)\n        mask = np.array(mask)\/255.\n        \n        if self.transform is not None:\n            aug = self.transform(image=image, mask=mask)\n            image = aug['image']\n            mask = aug['mask']\n        \n        image = image.transpose((2,0,1))\n        image = torch.from_numpy(image).type(torch.float32)\n        image = tt.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))(image)\n        mask = np.expand_dims(mask, axis=-1).transpose((2,0,1))\n        mask = torch.from_numpy(mask).type(torch.float32)\n        \n        return image, mask","b5a29b6a":"train_transform = A.Compose([\n    A.Resize(width=128, height=128, p=1.0),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomRotate90(p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.01, scale_limit=0.04, rotate_limit=0, p=0.25),\n])\n\nval_transform = A.Compose([\n    A.Resize(width=128, height=128, p=1.0),\n    A.HorizontalFlip(p=0.5),\n])\n\ntest_transform = A.Compose([\n    A.Resize(width=128, height=128, p=1.0)\n])","b646aa6b":"set_seed()\n\ntrain_ds = BrainDataset(train_df, train_transform)\nval_ds = BrainDataset(val_df, val_transform)\ntest_ds = BrainDataset(test_df, test_transform)","1a5d3d1e":"def dataset_info(dataset): \n    print(f'Size of dataset: {len(dataset)}')\n    index = random.randint(1, 40)\n    img, label = dataset[index]\n    print(f'Sample-{index} Image size: {img.shape}, Mask: {label.shape}\\n')","6b4a3c6d":"print('Train dataset:')\ndataset_info(train_ds)\nprint('Validation dataset:')\ndataset_info(val_ds)\nprint('Test dataset:')\ndataset_info(test_ds)","42f56813":"batch_size = 64\n\nset_seed()\ntrain_dl = DataLoader(train_ds, \n                      batch_size, \n                      shuffle=True, \n                      num_workers=2,  \n                      pin_memory=True)  \n\nset_seed()\nval_dl = DataLoader(val_ds, \n                    batch_size,   \n                    num_workers=2, \n                    pin_memory=True)\n\ntest_dl = DataLoader(val_ds, \n                    batch_size,   \n                    num_workers=2, \n                    pin_memory=True)","236f7258":"images, masks = next(iter(train_dl))\nprint(images.shape)\nprint(masks.shape)","92a396eb":"def denormalize(images):\n    means = torch.tensor([0.485, 0.456, 0.406]).reshape(1, 3, 1, 1)\n    stds = torch.tensor([0.229, 0.224, 0.225]).reshape(1, 3, 1, 1)\n    return images * stds + means\n\ndef show_batch(dl):\n    for images, masks in dl:\n        fig1, ax1 = plt.subplots(figsize=(24, 24))\n        ax1.set_xticks([]); ax1.set_yticks([])\n        denorm_images = denormalize(images)\n        ax1.imshow(make_grid(denorm_images[:13], nrow=13).permute(1, 2, 0).clamp(0,1))\n        \n        fig2, ax2 = plt.subplots(figsize=(24, 24))\n        ax2.set_xticks([]); ax2.set_yticks([])\n        ax2.imshow(make_grid(masks[:13], nrow=13).permute(1, 2, 0).clamp(0,1))\n        break\n        \nshow_batch(train_dl)","ff89309c":"class DoubleConv(nn.Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True))\n    def forward(self, x):\n        return self.double_conv(x)\n    \nclass Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels))\n    def forward(self, x):\n        return self.maxpool_conv(x)\n    \nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels\/\/2)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels, in_channels\/\/2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels)\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n        \n        x1 = F.pad(x1, [diffX\/\/2, diffX-diffX\/\/2,\n                        diffY\/\/2, diffY-diffY\/\/2])\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n            nn.Sigmoid())\n    def forward(self, x):\n        return self.conv(x)","3673084f":"class UNet(nn.Module):\n    def __init__(self, n_channels, n_classes, bilinear=True):\n        super(UNet, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n        \n        self.inc = DoubleConv(n_channels, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        self.down3 = Down(256, 512)\n        factor = 2 if bilinear else 1\n        self.down4 = Down(512, 1024\/\/factor)\n        self.up1 = Up(1024, 512\/\/factor, bilinear)\n        self.up2 = Up(512, 256\/\/factor, bilinear)        \n        self.up3 = Up(256, 128\/\/factor, bilinear)        \n        self.up4 = Up(128, 64, bilinear)        \n        self.outc = OutConv(64, n_classes)\n    \n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        return logits","33bd0fb9":"model = UNet(3, 1).to(device)\nout = model(torch.randn(1, 3, 128, 128).to(device))\nprint(out.shape)","dc07b890":"def dice_coef_metric(pred, label):\n    intersection = 2.0 * (pred * label).sum()\n    union = pred.sum() + label.sum()\n    if pred.sum() == 0 and label.sum() == 0:\n        return 1.\n    return intersection \/ union\n\ndef dice_coef_loss(pred, label):\n    smooth = 1.0\n    intersection = 2.0 * (pred * label).sum() + smooth\n    union = pred.sum() + label.sum() + smooth\n    return 1 - (intersection \/ union)\n\ndef bce_dice_loss(pred, label):\n    dice_loss = dice_coef_loss(pred, label)\n    bce_loss = nn.BCELoss()(pred, label)\n    return dice_loss + bce_loss","abb927a9":"def train_loop(model, loader, loss_func):\n    model.train()\n    train_losses = []\n    train_dices = []\n    \n#     for i, (image, mask) in enumerate(tqdm(loader)):\n    for i, (image, mask) in enumerate(loader):\n        image = image.to(device)\n        mask = mask.to(device)\n        outputs = model(image)\n        out_cut = np.copy(outputs.data.cpu().numpy())\n        out_cut[np.nonzero(out_cut < 0.5)] = 0.0\n        out_cut[np.nonzero(out_cut >= 0.5)] = 1.0            \n\n        dice = dice_coef_metric(out_cut, mask.data.cpu().numpy())\n        loss = loss_func(outputs, mask)\n        train_losses.append(loss.item())\n        train_dices.append(dice)\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        \n    return train_dices, train_losses","026df92d":"def eval_loop(model, loader, loss_func, training=True):\n    model.eval()\n    val_loss = 0\n    val_dice = 0\n    with torch.no_grad():\n        for step, (image, mask) in enumerate(loader):\n            image = image.to(device)\n            mask = mask.to(device)\n    \n            outputs = model(image)\n            loss = loss_func(outputs, mask)\n            \n            out_cut = np.copy(outputs.data.cpu().numpy())\n            out_cut[np.nonzero(out_cut < 0.5)] = 0.0\n            out_cut[np.nonzero(out_cut >= 0.5)] = 1.0\n            dice = dice_coef_metric(out_cut, mask.data.cpu().numpy())\n            \n            val_loss += loss\n            val_dice += dice\n        \n        val_mean_dice = val_dice \/ step\n        val_mean_loss = val_loss \/ step\n        \n        if training:\n            scheduler.step(val_mean_dice)\n        \n    return val_mean_dice, val_mean_loss","18881fc2":"def train_model(train_loader, val_loader, loss_func, optimizer, scheduler, num_epochs):\n    train_loss_history = []\n    train_dice_history = []\n    val_loss_history = []\n    val_dice_history = []\n    \n    for epoch in range(num_epochs):\n        train_dices, train_losses = train_loop(model, train_loader, loss_func)\n        train_mean_dice = np.array(train_dices).mean()\n        train_mean_loss = np.array(train_losses).mean()\n        val_mean_dice, val_mean_loss = eval_loop(model, val_loader, loss_func)\n        \n        train_loss_history.append(np.array(train_losses).mean())\n        train_dice_history.append(np.array(train_dices).mean())\n        val_loss_history.append(val_mean_loss)\n        val_dice_history.append(val_mean_dice)\n        \n        print('Epoch: {}\/{} |  Train Loss: {:.3f}, Val Loss: {:.3f}, Train DICE: {:.3f}, Val DICE: {:.3f}'.format(epoch+1, num_epochs,\n                                                                                                                 train_mean_loss,\n                                                                                                                 val_mean_loss,\n                                                                                                                 train_mean_dice,\n                                                                                                                 val_mean_dice))\n        \n\n    return train_loss_history, train_dice_history, val_loss_history, val_dice_history","9034953a":"optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3)\nnum_epochs = 30","f741aea6":"%%time\ntrain_loss_history, train_dice_history, val_loss_history, val_dice_history = train_model(train_dl, val_dl, bce_dice_loss, optimizer, scheduler, num_epochs)","f4e7f4cc":"def plot_dice_history(model_name, train_dice_history, val_dice_history, num_epochs):\n    \n    x = np.arange(num_epochs)\n    fig = plt.figure(figsize=(10, 6))\n    plt.plot(x, train_dice_history, label='Train DICE', lw=3, c=\"b\")\n    plt.plot(x, val_dice_history, label='Validation DICE', lw=3, c=\"r\")\n\n    plt.title(f\"{model_name}\", fontsize=20)\n    plt.legend(fontsize=12)\n    plt.xlabel(\"Epoch\", fontsize=15)\n    plt.ylabel(\"DICE\", fontsize=15)\n\n    plt.show()\n    \nplot_dice_history('UNET', train_dice_history, val_dice_history, num_epochs)","ea9c245d":"def plot_loss_history(model_name, train_loss_history, val_loss_history, num_epochs):\n    \n    x = np.arange(num_epochs)\n    fig = plt.figure(figsize=(10, 6))\n    plt.plot(x, train_loss_history, label='Train Loss', lw=3, c=\"b\")\n    plt.plot(x, val_loss_history, label='Validation Loss', lw=3, c=\"r\")\n\n    plt.title(f\"{model_name}\", fontsize=20)\n    plt.legend(fontsize=12)\n    plt.xlabel(\"Epoch\", fontsize=15)\n    plt.ylabel(\"Loss\", fontsize=15)\n\n    plt.show()\n    \nplot_loss_history('UNET', train_loss_history, val_loss_history, num_epochs)","a74249e2":"%%time\ntest_dice, test_loss = eval_loop(model, test_dl, bce_dice_loss, training=False)\nprint(\"Mean IoU\/DICE: {:.3f}%, Loss: {:.3f}\".format((100*test_dice), test_loss))","64349039":"test_sample = test_df[test_df[\"diagnosis\"] == 1].sample(24).values[0]\nimage = cv2.resize(cv2.imread(test_sample[0]), (128, 128))\nmask = cv2.resize(cv2.imread(test_sample[1]), (128, 128))\n\n# pred\npred = torch.tensor(image.astype(np.float32) \/ 255.).unsqueeze(0).permute(0,3,1,2)\npred = tt.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))(pred)\npred = model(pred.to(device))\npred = pred.detach().cpu().numpy()[0,0,:,:]\n\npred_t = np.copy(pred)\npred_t[np.nonzero(pred_t < 0.3)] = 0.0\npred_t[np.nonzero(pred_t >= 0.3)] = 255.\npred_t = pred_t.astype(\"uint8\")\n\n# plot\nfig, ax = plt.subplots(nrows=2,  ncols=2, figsize=(10, 10))\n\nax[0, 0].imshow(image)\nax[0, 0].set_title(\"image\")\nax[0, 1].imshow(mask)\nax[0, 1].set_title(\"mask\")\nax[1, 0].imshow(pred)\nax[1, 0].set_title(\"prediction\")\nax[1, 1].imshow(pred_t)\nax[1, 1].set_title(\"prediction with threshold\")\nplt.show()","a346d5bf":"torch.save(model.state_dict(), 'brain-mri-unet.pth')","774aea49":"### Set device\nSet device to use CUDA if available","ab359087":"## Prediction on Test set","8f8da98a":"## Creating Dataloaders\n\nWe create dataloaders to load data in batches. ","19a50673":"## DICE Score History","5455b408":"## Viewing the dataset","987ab2df":"### Hyperparameters\n\nThe learning rate scheduler that we use here is **ReduceLROnPlateau** in `max` mode with `patience=3` which\nreduce the LR if quantity monitored stops increasing after 3 epochs.","0ebe84d0":"### Train Function","db8d9e5f":"The size of one batch is `64xCx128x128` where C is the number of channels in image\/mask.","95d0d045":"## Load files path in a dataframe\n\nWe first load the path of all the image files as well as their corresponding mask files in a dataframe. \n\nWe also create a function `diagnonsis` which will read through all the mask files and return those files which have numpy value > 0 (not blank black mask images).\nWe then add the list of non blank mask images as a diagnonsis column in the dataframe.","0b5b5318":"We have a total of 3929 samples, out of which 1373 are tumor positive, and 2556 are not.","86c66d1e":"### Augmentations","0a884b21":"## Saving the Model","5ac2ad0f":"## Training ","364a10bb":"## Train-Validation-Test split\n\nWe split the data into train, validation and test datasets using the `tran_test_split` function from **sklearn**. We use `stratify` parameter to evenly distribute the number of tumor positive samples among each set.","8f42e03c":"### Train Loop","9f424709":"## Understand data distribution","0f42284d":"### Set Seed\n\nSet seed for reproducibilty","cea2d608":"### Validation loop","a4443d7d":"## Defining the UNET model","cdd04921":"## Loss History","01bed3eb":"## Converting to PyTorch dataset format\n\nWe load the images & masks using **cv2**, and then divide by 225 after converting them to NumPy array, so that all images\/masks are in the range of [0,1].\nThen we apply transformations to it using the Albumentations library. \n\nFor the images, we first convert them from `HxWxC` to `CxHxW` format, then to `torch.tensor` of type `float32`, & then finally Normalize the color channels.\n\nFor the masks, since they only have `HxW`, we add another dimension so it becomes `HxWxC`, then we convert it to `CxHxW`. Then we convert it to `torch.tensor` of type `float32`. We don't need to normalize the masks.","912d5273":"## Import Libraries","6d2cb023":"## Metric & Loss fn\n\n\nTo measure accuracy, the metric we choose is the **DICE coefficient**. *Dice Coefficient* is `2 * the Area of Overlap divided by the total number of pixels in both images`.\n\n**DICE Loss** is equal to `1 - DICE Coefficient`.\n\nHowever, a better loss function is the `sum of BCELoss & DICE Loss`.","ec228b75":"### Viewing samples from a batch\n\nTo view the samples in a batch we first need to denormalize the images by passing in same *mean* & *std*."}}