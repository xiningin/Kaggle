{"cell_type":{"ced07b30":"code","1408ca7b":"code","c4b35fc6":"code","c0e90aeb":"code","53250558":"code","2dab5449":"code","c9beb39d":"code","c97e5edd":"code","dda613e9":"code","7fba128c":"code","49069057":"code","a2ca069f":"code","88f7d31f":"code","924eb26c":"code","2ac45714":"code","745dbc0e":"code","1bd20221":"code","7ba606d5":"code","70c2a7a9":"code","bbbb627d":"code","e097e6a9":"code","3823ad05":"code","0f955b5a":"code","a02654ed":"code","4877a70f":"code","a80fe93e":"code","856ce2c9":"code","f4729846":"code","68640667":"markdown","7c05e8f1":"markdown","e9866db5":"markdown","e4aff08c":"markdown","16e40aa4":"markdown","63523b91":"markdown","ecf6cebd":"markdown","c8a8ba56":"markdown","55b3256a":"markdown","de04e1bc":"markdown","5cd26267":"markdown","fd1690cb":"markdown","05c12c9f":"markdown"},"source":{"ced07b30":"!pip install git+https:\/\/github.com\/AutoViML\/deep_autoviml.git","1408ca7b":"from deep_autoviml import deep_autoviml as deepauto","c4b35fc6":"!pip install git+https:\/\/github.com\/keras-team\/keras-tuner.git","c0e90aeb":"!pip install autokeras","53250558":"import torch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport autokeras as ak","2dab5449":"import tensorflow as tf\ntf.__version__","c9beb39d":"TRAIN_DATA_URL = '\/kaggle\/input\/titanic\/train.csv'\nTEST_DATA_URL = '\/kaggle\/input\/titanic\/test.csv'","c97e5edd":"df=pd.read_csv(TRAIN_DATA_URL,encoding ='ISO-8859-1',sep=\",\")\nprint(df.shape)\ndf.head()","dda613e9":"### we find that the Cabin feature can be broken up into two parts: alpha and numeric\ndf['Cabin1'] = df['Cabin'].fillna('Missing').map(lambda x: x[:1])\ndf['Cabin2'] = df['Cabin'].fillna('Missing').map(lambda x: x[1:])\ndf.drop('Cabin',axis=1, inplace=True)\ndf.head(2)","7fba128c":"# Initialize the structured data classifier.\nclf = ak.StructuredDataClassifier(\n    overwrite=True, max_trials=5)","49069057":"### we are going to drop PassengerID from consideration since it's an ID variable\npreds = df.columns[2:].tolist()\ntargets = df.columns[:2].tolist()\ntarget = targets[1]\ntarget","a2ca069f":"x=df[preds].to_numpy()\nx[:2]","88f7d31f":"y = df[[target]].to_numpy()\ny[:2]","924eb26c":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size= 0.20, random_state= True,stratify=y) \nx_train.shape,x_test.shape,y_train.shape,y_test.shape","2ac45714":"y_train=y_train.reshape((-1))\ny_train.shape","745dbc0e":"y_test=y_test.reshape((-1))\ny_test.shape","1bd20221":"clf.fit(x_train, y_train, epochs=5)","7ba606d5":"# Predict with the best model.\npredicted_y = clf.predict(x_test).ravel()\npredicted_y[:4]","70c2a7a9":"y_test[:4]","bbbb627d":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nprint('Accuracy = %0.0f%%' %(100*accuracy_score(y_test, predicted_y)))\nprint('Confusion Matrix: \\n%s' %confusion_matrix(y_test, predicted_y))\nprint('Classification Report: \\n%s' %classification_report(y_test, predicted_y))","e097e6a9":"df.head(1)","3823ad05":"train = pd.DataFrame(np.c_[x_train,y_train], index=range(len(x_train)), columns = preds+[target])\ntest = pd.DataFrame(np.c_[x_test,y_test], index=range(len(x_test)), columns = preds+[target])\nprint(train.shape, test.shape)\ntrain.head(2)","0f955b5a":"################################################################################\nkeras_model_type =  \"auto\" ## always try \"fast\", then \"fast1\", \"fast2\" and \"auto\"\n### always set early_stopping to True first and then change it to False\n#### You always need 15 max_trials to get something decent #####\nkeras_options = {\"early_stopping\": True, 'lr_scheduler': ''}  \n#### always set tuner to \"storm\" and then \"optuna\". \n# NLP char limit kicks off NLP processing. Feature Cross later.\nmodel_options = {'tuner':\"storm\", \"max_trials\": 5, 'nlp_char_limit':10,\n                 'cat_feat_cross_flag':False, }\nproject_name = 'Titanic' ### this is the folder where the model will be saved\n################################################################################","a02654ed":"model, cat_vocab_dict = deepauto.fit(train, target, keras_model_type=keras_model_type,\n\t\tproject_name=project_name, keras_options=keras_options,  \n\t\tmodel_options=model_options, save_model_flag=True, use_my_model='',\n\t\tmodel_use_case='', verbose=0)","4877a70f":"predictions = deepauto.predict(model, project_name, test_dataset=test,\n                                 keras_model_type=keras_model_type, \n                                 cat_vocab_dict=cat_vocab_dict)","a80fe93e":"y_test = test[target].astype(int).values\ny_test[:4]","856ce2c9":"y_preds = predictions[-1]\ny_preds[:4]","f4729846":"from deep_autoviml import print_classification_model_stats\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy = %0.0f%%' %(100*accuracy_score(y_test, y_preds)))\nprint_classification_model_stats(y_test, y_preds)","68640667":"Please see more notebooks by my friend Marilia Prata for deep_autoviml:\n\nDeep_AutoViML on German Credit Data:\nhttps:\/\/www.kaggle.com\/mpwolke\/creditability-deep-autoviml\n\n","7c05e8f1":"# So deep_autoviml has the same or better performance in less than 1 minute on Titanic Data Set. But the biggest advantages of deep_autoviml are the following:\n1. The important advantage of using deep_autoviml is that you don't have to clean your data. You can feed it as it is.\n2. You don't have to preprocess text or categorical or string \/ NLP columns. It will handle preprocessing automatically.\n3. The best part is, your model comes with the preprocessing steps as keras layers. So you can immediately deploy the model and predict on raw test data without any preprocessing steps since the model will do that automatically","e9866db5":"# But this is where you get an extra bonus: AutoViML provides a much more customized model with preprocessing layers suited to Titanic in a Deep Learning setting so that you can deploy this best model.\n![image.png](attachment:b9b371b0-4c82-41f1-bb67-60d239c58cc1.png)","e4aff08c":"# You can see that overall accuracy is 83% which is the same as autokeras accuracy which had 80-82%. \n![image.png](attachment:ea1ad73b-899b-48bb-af4c-6cf7c63081c5.png)","16e40aa4":"# Please make sure you install deep_autoviml first and then auto-keras.\nOtherwise the next few steps will give errors","63523b91":"# We are going to compare two Deep Learning AutoML libraries on the classic Titanic classification dataset.\n## The first AutoML library we will try is Autokeras. You can see their web site here:\nhttps:\/\/autokeras.com\/\n## Autokeras was developed by DATA Lab at Texas A&M University. It has 1000's of stars on Github and maintained by an army of programmers.\n\n## The next AutoML library we will try is: Deep AutoViML.\n<img src=\"https:\/\/github.com\/AutoViML\/deep_autoviml\/raw\/master\/logo.jpg\" alt=\"banner\"\/>\n\n## Deep AV is a brand new library and is built from the ground-up using the latest in Tensorflow and Keras technology. It uses keras preprocessing layers which just came out and is based on Tensorflow 2.5.\n\nWe will use the same test-train split in both using the same random_states and everything. Only thing is we will test on the final heldout test.\n\n# If you want to see more on German Credit, you can see another great notebook by Marilia here:\n\nhttps:\/\/www.kaggle.com\/mpwolke\/creditability-deep-autoviml","ecf6cebd":"## The results on test set are better than Autokeras with 80% accuracy on Deep_AutoViML\n![image.png](attachment:4743d485-88f9-4aaa-8b22-4119969c6e3e.png)","c8a8ba56":"## The results on test set are pretty good. with 80-82% accuracy\n![image.png](attachment:60787855-8567-4d33-afdb-5de0ac91acf8.png)","55b3256a":"# In autokeras, you can set number of epochs to run and max-trials. We set them quite low but got pretty good results on validation. The validation accuracy is 75% - this is a decent number but not great. Let's test results on x-test dataset\n","de04e1bc":"# Let us now compare the results to Deep_AutoViML which has the following features:\n1. You don't have to clean your data. You can feed it as it is.\n2. You don't have to preprocess text or categorical or string \/ NLP columns. It will handle preprocessing automatically.\n3. The best part is, your model comes with the preprocessing steps as keras layers. So you can immediately deploy the model and predict on raw test data without any preprocessing steps since the model will do that automatically","5cd26267":"# Hope this notebook was helpful. If you liked it, pelase upvote it","fd1690cb":"## We can see that autokeras provides very good results. Now let's try another AutoML library called Deep AutoViML","05c12c9f":"## We will now test the model on the heldout test dataset. \nWe can see that accuracy drops a bit since the dataset is too small and model probably overfit on such a small dataset. However, we can try other keras_model_type=\"fast1\", \"fast2\" etc and see whether we can get better results"}}