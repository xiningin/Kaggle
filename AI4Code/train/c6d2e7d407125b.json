{"cell_type":{"8126294e":"code","bdde8e99":"code","b6136ca6":"code","ed8bc9bd":"code","7ae4da98":"code","2333280e":"code","b9edbe75":"code","991af2ed":"code","e8b5a9ba":"code","842ef310":"code","f609e02c":"code","58151a4a":"code","83f03451":"code","6b5571ff":"code","7ec846b4":"code","eab4c9e3":"markdown","a263b57c":"markdown","35ef4ba1":"markdown"},"source":{"8126294e":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom torchvision.utils import save_image","bdde8e99":"model = models.vgg19(pretrained=True).features","b6136ca6":"model","ed8bc9bd":"class VGG(nn.Module):\n    def __init__(self):\n        super(VGG, self).__init__()\n        \n        self.selected_layer = ['5', '10', '19', '28']\n        \n        self.model = models.vgg19(pretrained=True).features[:29]\n    \n    def forward(self, x):\n        features = []\n        \n        for layer_num, layer in enumerate(self.model):\n            x = layer(x)\n            if str(layer_num) in self.selected_layer:\n                features.append(x)\n            \n        return features\n                \n        ","7ae4da98":"def load_image(image_name):\n    image = Image.open(image_name)\n    image = loader(image).unsqueeze(0)\n    return image.to(device=device)","2333280e":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","b9edbe75":"image_size = 224\n\nloader = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.ToTensor()\n])","991af2ed":"!wget \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/c\/cd\/Anne_Hathaway_at_MIFF_%28cropped%29.jpg\" -O anna.jpg\n!wget \"https:\/\/pbs.twimg.com\/media\/DU5DVJ_WAAICIMg.jpg\" -O style.jpg","e8b5a9ba":"!wget \"https:\/\/web.whatsapp.com\/25d0ecc7-7b05-48f2-bb2b-ac42ccebe38d\" -O pran.jpg","842ef310":"original_image = load_image(\".\/anna.jpg\")\nstyle_image = load_image('.\/style.jpg')\n\n#initial i\ngenerated = original_image.clone().requires_grad_(True)","f609e02c":"plt.imshow(Image.open(\".\/anna.jpg\"))\nplt.show()\nplt.imshow(Image.open(\".\/style.jpg\"))","58151a4a":"model = VGG().to(device=device).eval()","83f03451":"total_steps = 6000\nlearning_rate = 0.001\nalpha = 1\nbeta = 0.01\noptimizer = optim.Adam([generated], lr =learning_rate)\n","6b5571ff":"for step in range(total_steps):\n    generated_features = model(generated)\n    original_features = model(original_image)\n    style_features = model(style_image)\n    \n    style_loss = original_loss = 0\n    \n    for gen_feat, orig_feat, style_feat in zip(generated_features, original_features, style_features):\n        batch_size, channel, height, width = gen_feat.shape\n        original_loss += torch.mean((gen_feat - orig_feat) ** 2)\n        \n        G = gen_feat.view(channel, height*width).mm(\n            gen_feat.view(channel, height*width).t()\n        )\n        \n        A = style_feat.view(channel, height*width).mm(\n            style_feat.view(channel, height*width).t()\n        )\n        \n        style_loss += torch.mean((G-A)**2)\n    \n    total_loss= alpha * original_loss + beta * style_loss\n    optimizer.zero_grad()\n    \n    total_loss.backward()\n    \n    optimizer.step()\n    \n    if (step % 10 == 0):\n        print(f\"At step : {step} Total loss: {total_loss}\") \n    \n    if (step % 200 == 0):\n        print(f\"Total loss: {total_loss}\")\n        save_image(generated, f\"{step}_generated.png\")","7ec846b4":"plt.imshow(Image.open(\".\/5800_generate.png\"))","eab4c9e3":"Hyperparameters","a263b57c":"# What is this cell for ? Neither markdown nor code.","35ef4ba1":"![](.\/.\/400_generated.png)"}}