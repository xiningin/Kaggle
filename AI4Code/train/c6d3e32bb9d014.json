{"cell_type":{"beb99313":"code","a28bab71":"code","794a1bd3":"code","c6064fb9":"code","e8a9d807":"code","eacdad10":"code","26cf630c":"code","efd9fa70":"code","f7bfec74":"code","b16a771b":"code","67dd9453":"code","8d5f9f41":"code","167e4658":"code","dba20619":"code","6a21b1b9":"code","bae6967b":"code","a6fc67ab":"code","8e614fd6":"code","eda1e446":"code","548d26fb":"code","e0c37335":"code","a7a452cf":"code","4c738bdf":"code","2104f658":"code","581b97b8":"code","02424166":"code","b72360b8":"code","1289aaff":"code","426aecfb":"code","d45cae0e":"code","33ceda84":"code","448bea3f":"code","0e96ff7b":"code","dc853ef9":"markdown","28de6cd5":"markdown","fd419107":"markdown","c8829ca7":"markdown","b3681c93":"markdown","89ddef42":"markdown","dba69dd9":"markdown","d193043d":"markdown","269605ab":"markdown","e0da9e2a":"markdown","19cadac8":"markdown"},"source":{"beb99313":"import os\nimport gc\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\nimport lightgbm as lgb\n\nimport warnings\nwarnings.simplefilter('ignore', FutureWarning)\n\nprint(os.listdir(\"..\/input\"))","a28bab71":"train = pd.read_csv('..\/input\/train.csv', parse_dates=[\"first_active_month\"])\ntest = pd.read_csv('..\/input\/test.csv', parse_dates=[\"first_active_month\"])\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv')","794a1bd3":"train.shape, test.shape, sample_submission.shape","c6064fb9":"train.head(10)","e8a9d807":"test.head(10)","eacdad10":"merchants = pd.read_csv('..\/input\/merchants.csv')\nhistorical_transactions = pd.read_csv('..\/input\/historical_transactions.csv')\nnew_merchant_transactions = pd.read_csv('..\/input\/new_merchant_transactions.csv')","26cf630c":"merchants.shape, historical_transactions.shape, new_merchant_transactions.shape","efd9fa70":"merchants.head()","f7bfec74":"historical_transactions.head()","b16a771b":"new_merchant_transactions.head()","67dd9453":"def missing_impute(df):\n    for i in df.columns:\n        if df[i].dtype == \"object\":\n            df[i] = df[i].fillna(\"other\")\n        elif (df[i].dtype == \"int64\" or df[i].dtype == \"float64\"):\n            df[i] = df[i].fillna(df[i].mean())\n        else:\n            pass\n    return df","8d5f9f41":"def datetime_extract(df, dt_col='first_active_month'):\n    df['date'] = df[dt_col].dt.date \n    df['day'] = df[dt_col].dt.day \n    df['dayofweek'] = df[dt_col].dt.dayofweek\n    df['dayofyear'] = df[dt_col].dt.dayofyear\n    df['days_in_month'] = df[dt_col].dt.days_in_month\n    df['daysinmonth'] = df[dt_col].dt.daysinmonth \n    df['month'] = df[dt_col].dt.month\n    df['week'] = df[dt_col].dt.week \n    df['weekday'] = df[dt_col].dt.weekday\n    df['weekofyear'] = df[dt_col].dt.weekofyear\n#     df['year'] = train[dt_col].dt.year\n\n    return df","167e4658":"# Do impute missing values\nfor df in [train, test, merchants, historical_transactions, new_merchant_transactions]:\n    missing_impute(df)","dba20619":"# Do extract datetime values\ntrain = datetime_extract(train, dt_col='first_active_month')\ntest = datetime_extract(test, dt_col='first_active_month')","6a21b1b9":"train.shape, test.shape","bae6967b":"train.head()","a6fc67ab":"# Define the aggregation procedure outside of the groupby operation\naggregations = {\n    'purchase_amount': ['sum', 'mean', 'std', 'min', 'max', 'size'],\n}\n\ngrouped = historical_transactions.groupby('card_id').agg(aggregations)\ngrouped.columns = grouped.columns.droplevel(level=0)\ngrouped.rename(columns={\n    \"sum\": \"sum_purchase_amount\", \n    \"mean\": \"mean_purchase_amount\",\n    \"std\": \"std_purchase_amount\", \n    \"min\": \"min_purchase_amount\",\n    \"max\": \"max_purchase_amount\", \n    \"size\": \"num_purchase_amount\"}, inplace=True)\ngrouped.reset_index(inplace=True)\n\ntrain = pd.merge(train, grouped, on=\"card_id\", how=\"left\")\ntest = pd.merge(test, grouped, on=\"card_id\", how=\"left\")\n\ndel grouped\ngc.collect()","8e614fd6":"train.shape, test.shape","eda1e446":"# Define the aggregation procedure outside of the groupby operation\naggregations = {\n    'purchase_amount': ['sum', 'mean', 'std', 'min', 'max', 'size'],\n}\n\ngrouped = new_merchant_transactions.groupby('card_id').agg(aggregations)\ngrouped.columns = grouped.columns.droplevel(level=0)\ngrouped.rename(columns={\n    \"sum\": \"sum_purchase_amount\", \n    \"mean\": \"mean_purchase_amount\",\n    \"std\": \"std_purchase_amount\", \n    \"min\": \"min_purchase_amount\",\n    \"max\": \"max_purchase_amount\", \n    \"size\": \"num_purchase_amount\"}, inplace=True)\ngrouped.reset_index(inplace=True)\n\ntrain = pd.merge(train, grouped, on=\"card_id\", how=\"left\")\ntest = pd.merge(test, grouped, on=\"card_id\", how=\"left\")\n\ndel grouped\ngc.collect()","548d26fb":"train.shape, test.shape","e0c37335":"# One-hot encode features\nohe_df_1 = pd.get_dummies(train['feature_1'], prefix='f1_')\nohe_df_2 = pd.get_dummies(train['feature_2'], prefix='f2_')\nohe_df_3 = pd.get_dummies(train['feature_3'], prefix='f3_')\n\nohe_df_4 = pd.get_dummies(test['feature_1'], prefix='f1_')\nohe_df_5 = pd.get_dummies(test['feature_2'], prefix='f2_')\nohe_df_6 = pd.get_dummies(test['feature_3'], prefix='f3_')\n\n# Numerical representation of the first active month\ntrain = pd.concat([train, ohe_df_1, ohe_df_2, ohe_df_3], axis=1, sort=False)\ntest = pd.concat([test, ohe_df_4, ohe_df_5, ohe_df_6], axis=1, sort=False)\n\ndel ohe_df_1, ohe_df_2, ohe_df_3\ndel ohe_df_4, ohe_df_5, ohe_df_6\ngc.collect()","a7a452cf":"train.shape, test.shape","4c738bdf":"excluded_features = ['first_active_month', 'card_id', 'target', 'date']\ntrain_features = [c for c in train.columns if c not in excluded_features]","2104f658":"for f in train_features:\n    print(f)","581b97b8":"train.isnull().sum()","02424166":"for col in train_features:\n    for df in [train, test]:\n        if df[col].dtype == \"float64\":\n            df[col] = df[col].fillna(df[col].mean())","b72360b8":"# Prepare data for training\nX = train.copy()\ny = X['target']\n\n# Split data with kfold\nkfolds = KFold(n_splits=5, shuffle=True, random_state=2018)\n\n# Make importance dataframe\nimportances = pd.DataFrame()\n\noof_preds = np.zeros(X.shape[0])\nsub_preds = np.zeros(test.shape[0])\n\nfor n_fold, (trn_idx, val_idx) in enumerate(kfolds.split(X, y)):\n    X_train, y_train = X[train_features].iloc[trn_idx], y.iloc[trn_idx]\n    X_valid, y_valid = X[train_features].iloc[val_idx], y.iloc[val_idx]\n    \n    # LightGBM Regressor estimator\n    model = lgb.LGBMRegressor(\n        num_leaves = 31,\n        learning_rate = 0.03,\n        n_estimators = 1000,\n        subsample = .9,\n        colsample_bytree = .9,\n        random_state = 1,\n        booster = \"gbtree\",\n        eval_metric = \"rmse\",\n        nthread = 4,\n        nrounds = 1500,\n        max_depth = 7\n    )\n    \n    # Fit\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n        verbose=None, eval_metric='rmse',\n        early_stopping_rounds=100\n    )\n    \n    # Feature importance\n    imp_df = pd.DataFrame()\n    imp_df['feature'] = train_features\n    imp_df['gain'] = model.booster_.feature_importance(importance_type='gain')\n    imp_df['fold'] = n_fold + 1\n    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n    \n    oof_preds[val_idx] = model.predict(X_valid, num_iteration=model.best_iteration_)\n    test_preds = model.predict(test[train_features], num_iteration=model.best_iteration_)\n    sub_preds += test_preds \/ kfolds.n_splits\n    \nmean_squared_error(y, oof_preds) ** .5","1289aaff":"importances['gain_log'] = importances['gain']\nmean_gain = importances[['gain', 'feature']].groupby('feature').mean()\nimportances['mean_gain'] = importances['feature'].map(mean_gain['gain'])\n\nplt.figure(figsize=(8, 12))\nsns.barplot(x='gain_log', y='feature', data=importances.sort_values('mean_gain', ascending=False))","426aecfb":"sub_preds","d45cae0e":"# Make submission 1\nsample_submission['target'] = sub_preds\nsample_submission.to_csv(\"submission1.csv\", index=False)\nsample_submission.head()","33ceda84":"# Make submission 2 - revert from submission 1\nsample_submission['target'] = -1*sub_preds\nsample_submission.to_csv(\"submission2.csv\", index=False)\nsample_submission.head()","448bea3f":"# Length of submission\nlen(sub_preds)","0e96ff7b":"# How many positive target values\nnp.sum(np.array(sub_preds) >= 0, axis=0)","dc853ef9":"# Featuring","28de6cd5":"# Preprocessing","fd419107":"# Display feature importances","c8829ca7":"--> Still missing values. So need to fill NA again","b3681c93":"# Make submission","89ddef42":"# Modeling with LightGBM","dba69dd9":"# Load datasets","d193043d":"**Merge train and test with new_merchant_transactions**","269605ab":"# Load packages","e0da9e2a":"# Notes\n\nThis folk from [https:\/\/www.kaggle.com\/truocpham\/feature-engineering-and-lightgbm-starter](https:\/\/www.kaggle.com\/truocpham\/feature-engineering-and-lightgbm-starter) (thanks Truoc) and makes some changes:\n* Remove the **year** feature\n* Update the params for the lgb model","19cadac8":"**Merge train and test with historical transactions**"}}