{"cell_type":{"a258a8c3":"code","875489d6":"code","e0e63322":"code","5c8de544":"code","5df82d2d":"code","8d31a5e2":"code","3a939fd8":"code","30e9cbe0":"code","81de83dc":"code","bedcb3e9":"code","579908e2":"markdown"},"source":{"a258a8c3":"from fastai.conv_learner import *\nfrom fastai.dataset import *\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit\nimport matplotlib.pyplot as plt\nimport math\n\narch = resnet50\nnum_workers = 4\nTRAIN = '..\/input\/humpback-whale-identification\/train\/'\nTEST = '..\/input\/humpback-whale-identification\/test\/'\nLABELS = '..\/input\/humpback-whale-identification\/train.csv'\nSAMPLE_SUB = '..\/input\/humpback-whale-identification\/sample_submission.csv'\nBBOX = '..\/input\/generating-whale-bounding-boxes\/bounding_boxes.csv'","875489d6":"df = pd.read_csv(LABELS).set_index('Image')\nnew_whale_df = df[df.Id == \"new_whale\"] # only new_whale dataset\ntrain_df = df[~(df.Id == \"new_whale\")] # no new_whale dataset, used for training\nunique_labels = np.unique(train_df.Id.values)\n\nlabels_dict = dict()\nlabels_list = []\nfor i in range(len(unique_labels)):\n    labels_dict[unique_labels[i]] = i\n    labels_list.append(unique_labels[i])\ntrain_df.Id = train_df.Id.apply(lambda x: labels_dict[x])\ntrain_labels = np.asarray(train_df.Id.values)\ntest_names = [f for f in os.listdir(TEST)]","e0e63322":"train_df['image_name'] = train_df.index\ntr_n = train_df['image_name'].values\n# Yes, we will validate on the subset of training data\nval_n = train_df['image_name']\nprint('Train\/val:', len(tr_n), len(val_n))\nprint('Train classes', len(train_df.loc[tr_n].Id.unique()))\nprint('Val classes', len(train_df.loc[val_n].Id.unique()))","5c8de544":"class HWIDataset(FilesDataset):\n    def __init__(self, fnames, path, transform):\n        self.train_df = train_df\n        super().__init__(fnames, transform, path)\n\n    def get_x(self, i):\n        img = open_image(os.path.join(self.path, self.fnames[i]))\n        bbox = bbox_df.loc[self.fnames[i]]\n        x0, y0, x1, y1 = bbox['x0'], bbox['y0'], bbox['x1'],  bbox['y1']\n        if not (x0 >= x1 or y0 >= y1):\n            img = img[y0:y1, x0:x1,:]\n        img = cv2.resize(img, (self.sz, self.sz))\n        return img\n\n    def get_y(self, i):\n        if (self.path == TEST): return 0\n        return self.train_df.loc[self.fnames[i]]['Id']\n\n    def get_c(self):\n        return len(unique_labels)","5df82d2d":"class HWIDataset(FilesDataset):\n    def __init__(self, fnames, path, transform):\n        self.train_df = train_df\n        super().__init__(fnames, transform, path)\n\n    def get_x(self, i):\n        img = open_image(os.path.join(self.path, self.fnames[i]))\n        bbox = bbox_df.loc[self.fnames[i]]\n        x0, y0, x1, y1 = bbox['x0'], bbox['y0'], bbox['x1'],  bbox['y1']\n        if not (x0 >= x1 or y0 >= y1):\n            img = img[y0:y1, x0:x1,:]\n        img = cv2.resize(img, (self.sz, self.sz))\n        return img\n\n    def get_y(self, i):\n        if (self.path == TEST): return 0\n        return self.train_df.loc[self.fnames[i]]['Id']\n\n    def get_c(self):\n        return len(unique_labels)","8d31a5e2":"def get_data(sz, batch_size):\n    \"\"\"\n    Read data and do augmentations\n    \"\"\"\n    aug_tfms = []\n    tfms = tfms_from_model(arch, sz, crop_type=CropType.NO, tfm_y=TfmType.NO,\n                           aug_tfms=aug_tfms)\n    ds = ImageData.get_ds(HWIDataset, (tr_n[:-(len(tr_n) % batch_size)], TRAIN),\n                          (val_n, TRAIN), tfms, test=(test_names, TEST))\n    md = ImageData(\".\/\", ds, batch_size, num_workers=num_workers, classes=None)\n    return md","3a939fd8":"image_size = 384\nbatch_size = 32\nmd = get_data(image_size, batch_size)","30e9cbe0":"best_th = 0.38","81de83dc":"preds_t = np.load(\"..\/input\/humpbackdata\/resnet101.npy\") + np.load(\"..\/input\/humpbackdata\/resnet50.npy\") \npreds_t \/= 2","bedcb3e9":"sample_df = pd.read_csv(SAMPLE_SUB)\nsample_list = list(sample_df.Image)\nlabels_list = [\"new_whale\"]+labels_list\npred_list = [[labels_list[i] for i in p.argsort()[-5:][::-1]] for p in preds_t]\npred_dic = dict((key, value) for (key, value) in zip(md.test_ds.fnames,pred_list))\npred_list_cor = [' '.join(pred_dic[id]) for id in sample_list]\ndf = pd.DataFrame({'Image':sample_list,'Id': pred_list_cor})\ndf.to_csv('submission.csv', header=True, index=False)\ndf.head()","579908e2":"Train the Resnet50 from this kernel: https:\/\/www.kaggle.com\/suicaokhoailang\/resnet50-bounding-boxes-0-628-lb\n\nTraining another Resnet101, just change the architecture.\n\nPlug your outputs to this kernel and you're good to go.\n\nThis kernel is based on the approach from https:\/\/www.kaggle.com\/suicaokhoailang\/ensembling-with-averaged-probabilities-0-701-lb"}}