{"cell_type":{"aab0d4a0":"code","042a8f8f":"code","5da9bd92":"code","f901f22b":"code","e4f255b6":"code","b57b57b7":"code","eee399cb":"code","cfab950b":"code","f03ec982":"code","15959dde":"code","b51b9f92":"code","09581673":"code","f4cc59a4":"code","13a7e80f":"code","cdff9b07":"code","f57d21ca":"code","844d2feb":"code","c21388ca":"code","787bd71e":"code","12a3cc9c":"code","5f2ff00c":"code","26e85b73":"code","ab2bc287":"code","b0070471":"code","fb5da3f6":"code","9e1abf17":"code","e751fa5a":"code","ef4610c3":"code","7740a916":"code","efe7ee35":"code","b3dd831b":"code","ac7c42ea":"code","781767f4":"code","4fe15a92":"code","1a77eb1a":"code","c227dbd8":"code","e62a4c7d":"code","2596f313":"code","fe863822":"code","493c87d8":"code","71085d76":"code","5d79522d":"code","c7759e6a":"code","16e8e6ac":"code","a854437b":"markdown","be2faea1":"markdown","277ccfa7":"markdown","410ac49f":"markdown","dc20c773":"markdown","4a038e3d":"markdown","694b9b85":"markdown","ee041169":"markdown","5515fedc":"markdown","50af636e":"markdown","6eaec770":"markdown","f73d4a74":"markdown","0ddaf289":"markdown","0dda9297":"markdown","090c0317":"markdown","c674b6e5":"markdown","4cf7925c":"markdown","0700eb5e":"markdown","75e449a7":"markdown","c565d1f7":"markdown","48671da4":"markdown","75721fc8":"markdown","d87d74da":"markdown","7fbf4dc1":"markdown","034f5b28":"markdown"},"source":{"aab0d4a0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom geopy.distance import great_circle #calculate distances\nfrom sklearn import metrics #evaluating models\n#from sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score #set splitting and validation\nfrom sklearn.linear_model import LinearRegression \nimport xgboost as xgb #XGBoost classifier\nimport matplotlib.pyplot as plt #plotting\nimport seaborn as sns #plotting\nfrom math import sin, cos, sqrt, atan2, radians\n%matplotlib inline","042a8f8f":"#Check for the comptetition files\nprint(os.listdir(\"..\/input\"))","5da9bd92":"#Load test dataset\ntest = pd.read_csv('..\/input\/test.csv')","f901f22b":"#Get types of values\ntest.dtypes","e4f255b6":"#Set lighter types to a dictionary to speed up train dataset (float64 is an overkill for GPS coordinates)\ntypes = {'fare_amount': 'float32',\n         'pickup_longitude': 'float32',\n         'pickup_latitude': 'float32',\n         'dropoff_longitude': 'float32',\n         'dropoff_latitude': 'float32',\n         'passenger_count': 'uint8'}","b57b57b7":"#Load portion of the dataset in the defined types\ntrain = pd.read_csv('..\/input\/train.csv',nrows=100000,dtype=types)","eee399cb":"train.head()","cfab950b":"train.describe()","f03ec982":"#Distribution plot of fares\nsns.distplot(train['fare_amount'])","15959dde":"#Distribution plot of passanger count\nsns.distplot(train['passenger_count'])","b51b9f92":"#Check how many rows have null values\ntrain.isnull().sum()","09581673":"#Drop nulls if exist\ntrain.dropna(inplace=True)","f4cc59a4":"#Clean up the trian dataset to eliminate out of range values\ntrain = train[train['fare_amount'] > 0]\ntrain = train[train['pickup_longitude'] < -72]\ntrain = train[(train['pickup_latitude'] > 40) & (train['pickup_latitude'] < 44)]\ntrain = train[train['dropoff_longitude'] < -72]\ntrain = train[(train['dropoff_latitude'] > 40) & (train['dropoff_latitude'] < 44)]\ntrain = train[(train['passenger_count'] > 0) & (train['passenger_count'] < 10)]","13a7e80f":"train.describe()","cdff9b07":"#Define function to calculate distance in km from coordinates\ndef dist_calc(df):\n    for i,row in df.iterrows():\n        df.at[i,'distance'] = great_circle((row['pickup_latitude'],row['pickup_longitude']),(row['dropoff_latitude'],row['dropoff_longitude'])).km","f57d21ca":"#Quicker but slightly less accurate\ndef quick_dist_calc(df):\n    R = 6373.0\n    for i,row in df.iterrows():\n\n        lat1 = radians(row['pickup_latitude'])\n        lon1 = radians(row['pickup_longitude'])\n        lat2 = radians(row['dropoff_latitude'])\n        lon2 = radians(row['dropoff_longitude'])\n\n        dlon = lon2 - lon1\n        dlat = lat2 - lat1\n\n        a = sin(dlat \/ 2)**2 + cos(lat1) * cos(lat2) * sin(dlon \/ 2)**2\n        c = 2 * atan2(sqrt(a), sqrt(1 - a))\n\n        distance = R * c\n        df.at[i,'distance'] = distance","844d2feb":"#Get distance for both sets\nquick_dist_calc(train)\nquick_dist_calc(test)","c21388ca":"#Get useable date for feature engineering\ntrain['pickup_datetime'] = train['pickup_datetime'].str.replace(\" UTC\", \"\")\ntrain['pickup_datetime'] = pd.to_datetime(train['pickup_datetime'], format='%Y-%m-%d %H:%M:%S')\n\ntest['pickup_datetime'] = test['pickup_datetime'].str.replace(\" UTC\", \"\")\ntest['pickup_datetime'] = pd.to_datetime(test['pickup_datetime'], format='%Y-%m-%d %H:%M:%S')","787bd71e":"#Getting interger numbers from the pickup_datetime\ntrain[\"hour\"] = train.pickup_datetime.dt.hour\ntrain[\"weekday\"] = train.pickup_datetime.dt.weekday\ntrain[\"month\"] = train.pickup_datetime.dt.month\ntrain[\"year\"] = train.pickup_datetime.dt.year\n\ntest[\"hour\"] = test.pickup_datetime.dt.hour\ntest[\"weekday\"] = test.pickup_datetime.dt.weekday\ntest[\"month\"] = test.pickup_datetime.dt.month\ntest[\"year\"] = test.pickup_datetime.dt.year","12a3cc9c":"#Function for distance calculation between coordinates as mapped variables\ndef sphere_dist(pickup_lat, pickup_lon, dropoff_lat, dropoff_lon):\n    #Define earth radius (km)\n    R_earth = 6371\n    #Convert degrees to radians\n    pickup_lat, pickup_lon, dropoff_lat, dropoff_lon = map(np.radians,\n                                                             [pickup_lat, pickup_lon, \n                                                              dropoff_lat, dropoff_lon])\n    #Compute distances along lat, lon dimensions\n    dlat = dropoff_lat - pickup_lat\n    dlon = dropoff_lon - pickup_lon\n    \n    #Compute haversine distance\n    a = np.sin(dlat\/2.0)**2 + np.cos(pickup_lat) * np.cos(dropoff_lat) * np.sin(dlon\/2.0)**2\n    \n    return 2 * R_earth * np.arcsin(np.sqrt(a))","5f2ff00c":"#Function for calculating distance between newly obtained distances from the hotspots.\ndef add_airport_dist(dataset):\n    jfk_coord = (40.639722, -73.778889)\n    ewr_coord = (40.6925, -74.168611)\n    lga_coord = (40.77725, -73.872611)\n    \n    pickup_lat = dataset['pickup_latitude']\n    dropoff_lat = dataset['dropoff_latitude']\n    pickup_lon = dataset['pickup_longitude']\n    dropoff_lon = dataset['dropoff_longitude']\n    \n    pickup_jfk = sphere_dist(pickup_lat, pickup_lon, jfk_coord[0], jfk_coord[1]) \n    dropoff_jfk = sphere_dist(jfk_coord[0], jfk_coord[1], dropoff_lat, dropoff_lon) \n    pickup_ewr = sphere_dist(pickup_lat, pickup_lon, ewr_coord[0], ewr_coord[1])\n    dropoff_ewr = sphere_dist(ewr_coord[0], ewr_coord[1], dropoff_lat, dropoff_lon) \n    pickup_lga = sphere_dist(pickup_lat, pickup_lon, lga_coord[0], lga_coord[1]) \n    dropoff_lga = sphere_dist(lga_coord[0], lga_coord[1], dropoff_lat, dropoff_lon) \n    \n    dataset['jfk_dist'] = pd.concat([pickup_jfk, dropoff_jfk], axis=1).min(axis=1)\n    dataset['ewr_dist'] = pd.concat([pickup_ewr, dropoff_ewr], axis=1).min(axis=1)\n    dataset['lga_dist'] = pd.concat([pickup_lga, dropoff_lga], axis=1).min(axis=1)\n    \n    return dataset","26e85b73":"#Run the functions to add the features to the dataset\ntrain = add_airport_dist(train)\ntest = add_airport_dist(test)","ab2bc287":"train.head()","b0070471":"test.head()","fb5da3f6":"#Plot heatmap of value correlations\nplt.figure(figsize=(15,8))\nsns.heatmap(train.drop(['key','pickup_datetime'],axis=1).corr(),annot=True,fmt='.4f')","9e1abf17":"X = train.drop(['key','fare_amount','pickup_datetime'],axis=1)\ny = train['fare_amount']","e751fa5a":"X.head()","ef4610c3":"y.head()","7740a916":"#Split train set into test and train subsets\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)","efe7ee35":"#Drop columns from test dataset we're not going to use\ntest_pred = test.drop(['key','pickup_datetime'],axis=1)","b3dd831b":"#Scale values if needed for a particular model\n#scaler = RobustScaler()\n#X_train_scaled = scaler.fit_transform(X_train)\n#X_test_scaled = scaler.fit_transform(X_test)\n#X_scaled = scaler.fit_transform(X)\n#test_scaled = scaler.fit_transform(test_pred)","ac7c42ea":"#Initilise a linear regression model, fit the data and get scores\nlm = LinearRegression()\nlm.fit(X_train,y_train)\nprint(lm.score(X_train,y_train))\nprint(lm.score(X_test,y_test))","781767f4":"#Predict fares and get a rmse for them\ny_pred = lm.predict(X)\nlrmse = np.sqrt(metrics.mean_squared_error(y_pred, y))\nlrmse","4fe15a92":"#Predict final fares for submission\nLinearPredictions = lm.predict(test_pred)\nLinearPredictions = np.round(LinearPredictions, decimals=2)\nLinearPredictions","1a77eb1a":"#Check predictions have the correct dimensions\nLinearPredictions.size","c227dbd8":"#Set up predictions for a submittable dataframe\nlinear_submission = pd.DataFrame({\"key\": test['key'],\"fare_amount\": LinearPredictions},columns = ['key','fare_amount'])","e62a4c7d":"#Check the submissions look reasonable\nlinear_submission.head()","2596f313":"#Define a XGB model and parameters\ndef XGBoost(X_train,X_test,y_train,y_test):\n    dtrain = xgb.DMatrix(X_train,label=y_train)\n    dtest = xgb.DMatrix(X_test,label=y_test)\n\n    return xgb.train(params={'objective':'reg:linear','eval_metric':'rmse'}\n                    ,dtrain=dtrain,num_boost_round=400, \n                    early_stopping_rounds=30,evals=[(dtest,'test')])","fe863822":"#Fit data and optimise the model, generate predictions\nxgbm = XGBoost(X_train,X_test,y_train,y_test)\nXGBPredictions = xgbm.predict(xgb.DMatrix(test_pred), ntree_limit = xgbm.best_ntree_limit)","493c87d8":"#Check if predictions seem to be realistic looking\nXGBPredictions","71085d76":"#Round predictions to 2 decimal numbers\nXGBPredictions = np.round(XGBPredictions, decimals=2)\nXGBPredictions","5d79522d":"#Prepare predictions for submission\nXGB_submission = pd.DataFrame({\"key\": test['key'],\"fare_amount\": XGBPredictions},columns = ['key','fare_amount'])\nXGB_submission.head()","c7759e6a":"#submission = linear_submission\nsubmission = XGB_submission","16e8e6ac":"#Generate the final submission csv file\nsubmission.to_csv('XGBSubmission.csv',index=False)","a854437b":"Now we can see there are no obvious inconsitentcies with the data.","be2faea1":"Based on just a quick look at the data, we can see that it's not 100% clean and some entries will contribute to higher error rates. As there are more than enough entries, we can easily sacrifice any rows that have null data. Note that depending on how large of a slice of training data you took, you might not see any null values, but there are some in the 6 million rows provided. Just in case let's drop them. Additionally we can drop all entries where fare_amount or passanger_count is less or equal to 0. We can do a similar cleanup with coordinates, as the dataset is on New York taxi fares, and some of the values go beyond New York coordinates, and some of them are clearly errors.","277ccfa7":"Now let's look at what the data look like and some general statistics about it.","410ac49f":"Let's start with a simple linear regression model and see how well it does for predicting the fare_amount. ","dc20c773":"Last but not least, we can look at fare correlation with the distance from certain hotspots around New York where prices will be higher or lower than usual. You can have a look at this great kernel https:\/\/www.kaggle.com\/shaz13\/simple-exploration-notebook-map-plots-v2 where the fare data is plotted geographically and gives an nice visual representation of such places. The most obvious ones would be airports and the center of Manhattan. Check out this kernel https:\/\/www.kaggle.com\/gunbl4d3\/xgboost-ing-taxi-fares for getting distance to and from higher fare hotspots. ","4a038e3d":"## Gradient Boosting","694b9b85":"Check the data after addition of all the new variables and make sure train and test datasets match.","ee041169":"## Linear Regression","5515fedc":"Linear regression seemed to do reasonably well, but you can get much more accurate predictions using a more finely tuned model such as gradient boosting optimization. First we define the model and some basic parameters for it (feel free to play around to see if you can get better results). ","50af636e":"We're only provided with a 8 columns of directly useable data, however we can extract much more information from it by engineering features from those columns or combinations of them. To start with we can get the distanct between the pickup and dropoff points, which should be a strong predictor for the fare. ","6eaec770":"# Model Training","f73d4a74":"Let's plot distribution of couple of the values to get an easier insight into the data and the way it's distributed. You can see that there are a few values which are 0 dollars or less on fare_amount or 0 passengers on passenger_count. Feel free to check distributions of other values.","0ddaf289":"As pointed out in this kernel https:\/\/www.kaggle.com\/szelee\/how-to-import-a-csv-file-of-55-million-rows there is no need to use float64 for GPS coordinates, and float32 will provide just as accurate results and allow you to import and process data much quicker.","0dda9297":"Now seeing that the XGB model has done much better than linear regression, let's prepare the data for submission by putting it in the correct format and match each prediction fare with the corresponding key. After which generate a csv file which will be ready to be uploaded for final submission.","090c0317":"# Submission","c674b6e5":"# Feature Engineering","4cf7925c":"Then if you look at the pickup_datetime column, we can extract quite a few datetime related features from it. First we need to format the string column into datetime format, after which it will be easy getting atomic features like hour, day, year, etc. of the pickup. ","0700eb5e":"That is it for this quick guide on some basic data exploration, analysis and prediction. Feel free to ask me any questions or comment any suggestions\/corrections if you have any. I hope you've found this useful.","75e449a7":"# Data Exploration and Cleanup","c565d1f7":"# Import Libraries and Data","48671da4":"As the coordinates columns seem to directly correlate with the fare_amount, I decided to leave them in the model fitting and prediction, along with all the newly generated features. Now we need to drop non-predicting columns and split the data into train and test sets for training the model. ","75721fc8":"You can plot a correlation between all the features using a heatmap. The further away the values from 0, the more of an impact they play on the final fare prediciton.","d87d74da":"# Google New York Taxi Fare Prediction","7fbf4dc1":"![](https:\/\/kaggle2.blob.core.windows.net\/competitions\/kaggle\/10170\/logos\/header.png?t=2018-07-12-22-07-30)","034f5b28":"This is a short kernel I made for myself and others as a reference guide\/tutorial whilst I'm working out the best way to make accurate predictions. It's probably the most useful to beginners and I did try to describe\/explain almost everything. I hope you find this useful, feel free to comment or contact me with any suggestions or questions."}}