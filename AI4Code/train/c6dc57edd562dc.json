{"cell_type":{"cc29356f":"code","b8548dcd":"code","e61e6632":"code","c8e80ef2":"code","6e167f5c":"code","d44e8422":"code","a6e66ee0":"code","d09edbc7":"code","e184ed70":"code","2abfa434":"code","70f0d4d7":"code","5def7593":"code","bf7e874b":"code","c2942b0f":"code","80296158":"code","a202ed13":"code","42e4e90e":"markdown"},"source":{"cc29356f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b8548dcd":"\ndata = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ndata = np.array(data)\nm, n = data.shape\nnp.random.shuffle(data) # shuffle before splitting into dev and training sets\n\ndata_dev = data[0:1000].T\nY_dev = data_dev[0]\nX_dev = data_dev[1:n]\nX_dev = X_dev \/ 255.\ndata_test = data[:m].T\nX_test = data_test[1:n]\nY_test = data_test[0]\ndata_train = data[1000:m].T\nY_train = data_train[0]\nX_train = data_train[1:n]\nX_train = X_train \/ 255.\n_,m_train = X_train.shape","e61e6632":"#W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.10, 500)\nX_train.shape","c8e80ef2":"    \nclass Layer :\n    def _init_(self,size):\n        self.size = size \n    def _initWeights_(self, parBef, parAct):\n        self.weights = np.random.rand(parAct, parBef) - 0.5\n        self.biais = np.random.rand(parAct, 1) - 0.5\n    def updateNodes(self,A):\n        self.A = A\n        #print(\"The new Values of nodes are : \",A.shape)\n    def updateWeights(self,W,b):\n        self.weights = W\n        self.biais = b","6e167f5c":"class NeuralNetwork:\n    Layers = []\n    Z, dZ, dW, dB=[],[],[],[]\n    def _init_(self, L):\n        self.cL = len(L)\n        #self.m ,self.n = data.shape\n        \n    def add_Layer(self,layer):\n        self.Layers.append(layer)\n        print(\"Layer \",len(self.Layers),\" is added.\\nit has :\",layer.size,\" nodes\")    \n    def forward_prop(self,X):\n        self.Layers[0].updateNodes(X)\n        #self.Z.append(X)\n        for i in range(1,len(self.Layers)-1):  \n            layer = self.Layers[i-1]\n            Zi=layer.weights.dot(layer.A)+layer.biais\n            #print(Zi)\n            self.Z.append(Zi)\n            A = tanH(Zi)\n            self.Layers[i].updateNodes(A)\n        Y = self.Layers[-2].weights.dot(self.Layers[-2].A)+self.Layers[-2].biais\n        self.Z.append(Y)\n        A = softmax(Y)\n        #print(A)\n        self.Layers[-1].updateNodes(A)\n        return A\n    def BackPropLastLayer(self,Y, m):\n        dZ = self.Layers[-1].A - one(Y)\n        dW = 1\/m * dZ.dot(self.Layers[-2].A.T)\n        db = 1\/m * np.sum(dZ)\n        return dZ,dW,db\n    def BackpropHiddenLayers(self,dZA,Z,A,Theta,m):\n        dZ = Theta.T.dot(dZA) * deriv_tanH(Z)\n        dW = 1\/m * dZ.dot(A.T)\n        #print(\"dZ :\",dZ.shape)\n        db = 1\/m * np.sum(dZ)\n        return dZ,dW, db\n    def get_predictions(self):\n        return np.argmax(self.Layers[-1].A, 0)\n    def get_accuracy(self, Y):\n        print(self.get_predictions(), Y)\n        return np.sum(self.get_predictions() == Y) \/ Y.size\n    def gradient_descent(self, X, Y, iter, alpha,m):\n        for i in range(iter):\n            self.Z,self.dZ, self.dW,self.dB=[],[],[],[]\n            self.forward_prop(X)\n            dZ,dW,db = self.BackPropLastLayer(Y,m)\n            #print(\"dZ \",i,\":\",dZ.shape,\"dW :\",dW.shape,\"db : \",db)\n            self.dZ.append(dZ)\n            self.dW.append(dW)\n            self.dB.append(db)\n            for j in reversed(range(1,len(self.Layers)-1)):\n                #New_NN1.BackpropHiddenLayers(dZA,Z,New_NN1.Layers[-2].A,New_NN1.Layers[-2].weights,239)\n                dZ,dW,db = self.BackpropHiddenLayers(self.dZ[-1],self.Z[j-1],self.Layers[j-1].A,New_NN1.Layers[j].weights,m)\n                self.dZ.append(dZ)\n                self.dW.append(dW)\n                self.dB.append(db)\n                #print(\"dZ :\",dZ.shape,\"db : \",db)\n            #print(len(self.dZ),len(self.dW),len(self.dB))\n            #print(self.dB)\n            c = -1\n            for layer in reversed(self.Layers):\n                c+=1\n                if c <= 0 : pass\n                else : \n                    layer.updateWeights(layer.weights-self.dW[c-1],layer.biais-self.dB[c-1])\n                    #print(layer.weights.shape,layer.biais.shape)\n            if i % 10 == 0:\n                print(\"Iteration: \", i)\n                predictions = self.get_predictions()\n                print(self.get_accuracy(Y))\n                print(\"--------------------------------\")\n    def predict(self,X):\n        return self.forward_prop(X)\n            ","d44e8422":"def one(Y):\n    one_Y = np.zeros((Y.size, int(Y.max()) + 1))\n    one_Y[np.arange(Y.size), Y] = 1\n    one_Y = one_Y.T\n    return one_Y\ndef tanH(x):\n    return np.tanh(x)\ndef deriv_tanH(x):\n    return 1-np.tanh(x)**2\n        \ndef softmax(Z):\n    return (np.exp(Z) \/ sum(np.exp(Z)))\ndef softmax_grad(softmax):\n    # Reshape the 1-d softmax to 2-d so that np.dot will do the matrix multiplication\n    s = softmax.reshape(-1,1)\n    return np.diagflat(s) - np.dot(s, s.T)","a6e66ee0":"def make_nn(L):\n    New_NN = NeuralNetwork()\n    New_NN._init_(L)\n    for i in range(len(L)):\n        layer = Layer()\n        layer._init_(L[i])\n        if i<len(L)-1: layer._initWeights_(L[i],L[i+1])\n        New_NN.add_Layer(layer)\n    return New_NN","d09edbc7":"nb_features, nb_output = 784, 10\nL =[nb_features, 12,13,nb_output]","e184ed70":"New_NN1 = NeuralNetwork()\nNew_NN1 = make_nn(L)\nNew_NN1.Layers","2abfa434":"#gradient_descent(self, X, Y, iter, alpha,m)\nNew_NN1.gradient_descent(X_train,Y_train,500,0.1,41000)","70f0d4d7":"def test_prediction(NN,index):\n    current_image = X_train[:, index,None]\n    prediction = NN.predict(X_train[:, index,None])\n    p = list(prediction)\n    Y_Pred = p.index(max(p))\n    print(\"Predicted Value: \",Y_Pred)\n    label = Y_train[index]\n    #print(\"Prediction: \", prediction)\n    print(\"Label: \", label)\n    current_image = current_image.reshape((28, 28)) * 255\n    plt.gray()\n    plt.imshow(current_image)\n    plt.show() ","5def7593":"test_prediction(New_NN1,56)","bf7e874b":"test_prediction(New_NN1,560)","c2942b0f":"test_prediction(New_NN1,2)","80296158":"test_prediction(New_NN1,10000)","a202ed13":"test_prediction(New_NN1,30000)","42e4e90e":"# **> Let's build our Flex NN :**"}}