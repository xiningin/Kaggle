{"cell_type":{"a52d21bf":"code","1995ea7c":"code","4decb3d9":"code","766a28cc":"code","17aa1c6e":"code","211a6fc8":"code","14ef4343":"code","3be45537":"code","7e0e86f7":"code","684e9f5d":"code","35052220":"code","bf5c22d2":"code","48a456ba":"code","1f28d7f1":"code","94f37302":"code","6c30906c":"code","1dbf6eed":"code","10acf20c":"code","00e6b27c":"code","992e6fe7":"code","de9697d9":"code","4726f926":"code","1c1d3432":"code","e374568e":"code","c5a87a97":"code","e40e63c6":"code","1ecdd078":"code","f9e5d3f9":"code","1a85cde4":"code","ff8e908a":"code","bf7f6ad0":"code","aeb70ba9":"code","6178987b":"code","2171ed17":"code","bafc9c4d":"code","f7989f86":"code","891d2eda":"code","ff277a27":"code","6aeebcaa":"code","f504f048":"code","c0e7843d":"code","b8244ba7":"code","5de1a7a9":"code","f9bc84f1":"markdown","4532aeb4":"markdown","d89aa3b9":"markdown","7b0ca14f":"markdown","81ec9ba8":"markdown","9a908316":"markdown","86911808":"markdown","8d6b7436":"markdown","0e44f3a8":"markdown","aee343f6":"markdown","fbde0f9f":"markdown","04646ae1":"markdown","3e2f0fd2":"markdown","40b4b621":"markdown","33a6db5f":"markdown","af32e991":"markdown","8ba3766d":"markdown","3e384d65":"markdown","1f85644c":"markdown","7a23d7b7":"markdown","e407b357":"markdown","6f46ee57":"markdown","bc1a5f49":"markdown","b922dacd":"markdown","11c95bd5":"markdown","237a75a7":"markdown","fa93ce90":"markdown","161632f2":"markdown","f454098e":"markdown","cb9b6b8c":"markdown","0fc38656":"markdown","c142c897":"markdown","828efd46":"markdown","ad00f532":"markdown","822687c5":"markdown","d3cdabd5":"markdown","ce2d3afc":"markdown","c1030404":"markdown","ce70152d":"markdown","51b8f61f":"markdown","60036acf":"markdown","61440803":"markdown","231190b6":"markdown","da68a3c6":"markdown","4d291443":"markdown","7557965e":"markdown","425734a1":"markdown"},"source":{"a52d21bf":"!pip install scikit-optimize\n!pip install sweetviz\n!pip install pycaret\n!pip install ppscore","1995ea7c":"#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nprint('-'*25)\n\nfrom IPython.core.interactiveshell import InteractiveShell\n\nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport sys #access to system parameters https:\/\/docs.python.org\/3\/library\/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__))\n\nimport matplotlib #collection of functions for scientific and publication-ready visualization\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\n\nimport numpy as np #foundational package for scientific computing\nprint(\"NumPy version: {}\". format(np.__version__))\n\nimport scipy as sp #collection of functions for scientific computing and advance mathematics\nprint(\"SciPy version: {}\". format(sp.__version__)) \n\nimport IPython\nfrom IPython import display #pretty printing of dataframes in Jupyter notebook\nprint(\"IPython version: {}\". format(IPython.__version__)) \n\nimport sklearn #collection of machine learning algorithms\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\n\n# Imbalance Data Handling\nfrom imblearn.over_sampling import SMOTE, SMOTENC\n\n# Permutation Importance\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\n#misc libraries\nimport random\nimport time\nimport tqdm\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\n\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8\n\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process, feature_selection\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns","4decb3d9":"## EDIT THIS CODE\n# =====================\ntrain_filename = \"..\/input\/<compe>\/train.csv\"\ntest_filename = \"..\/input\/<compe>\/test.csv\"\n# =====================\n\ntrain = pd.read_csv(train_filename)\ntest = pd.read_csv(test_filename)","766a28cc":"# Scaling Numerical Feature\nnp.seterr(divide='ignore', invalid='ignore')\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\ndef scale(data,cols,transformer = StandardScaler()):\n  new_data = data.copy()\n  pbar_scale = tqdm.tqdm(cols)\n  for i in pbar_scale:\n    pbar_scale.set_description(f\"Processing : {i}\")\n    new_data[i] = transformer.fit_transform(new_data[i].values.reshape(-1,1))\n  return new_data \n\n# Usage : \n# data = scale(data.copy(), col, StandardScaler())\n# data = scale(data.copy(), col, MinMaxScaler())","17aa1c6e":"# Define viable transformation\ntransformation_func = {\n    \"log\" : np.log,\n    \"sqrt\" : np.sqrt,\n    \"exp\" : np.exp,\n    \"rep\" : lambda x : 1\/x,\n    \n}","211a6fc8":"# Log Transformation\npbar_log = tqdm.tqdm(numerical)\nfor col in pbar_log : \n  pbar_log.set_description(f\"Processing : {col}\")\n  data[col + '_log'] = (data[col]+1).transform(transformation_func['log'])","14ef4343":"# Binning\npbar_bin = tqdm.tqdm(numerical)\nfor col in pbar_log : \n  pbar_bin.set_description(f\"Processing : {col}\")\n  data[col + '_bin'] = pd.qcut(data[col], q=5, duplicates='drop')","3be45537":"# Outlier Handling : Winsorization or use Robust Scaler\n# data = scale(data.copy(), col, RobustScaler())","7e0e86f7":"# Label Encoding\ncol = []\nle = LabelEncoder()\nfor i in col:\n  data[i] = le.fit_transform(list(data[i].values))","684e9f5d":"# One hot Encoding\ncol = []\ndata = pd.concat([data,pd.get_dummies(data[col], prefix=col)],axis=1)","35052220":"# One-hot encoding\ndef create_dummies_from_categorical_column (df,categorical_features):\n  df_copy = df.copy()\n  print('changing {}'.format(list(categorical_features)))\n  df_result = pd.DataFrame()\n  for feature in categorical_features:\n    df_temp = df_copy[feature].str.get_dummies()\n    df_temp.columns = ['{}_{}'.format(feature,column) for column in df_temp.columns]\n    df_copy = df_copy.join(df_temp,rsuffix='_dept')\n  df_copy.drop(categorical_features,axis=1,inplace=True)  \n  return df_copy","bf5c22d2":"# Format example: 25.01.2009\ndf['date'] = pd.to_datetime(df['date'])\ndf['year'] = pd.DatetimeIndex(pd.to_datetime(df['date'], format='%d.%m.%Y')).year\ndf['month'] = pd.DatetimeIndex(pd.to_datetime(df['date'], format='%d.%m.%Y')).month\ndf['day'] = pd.DatetimeIndex(pd.to_datetime(df['date'], format='%d.%m.%Y')).day","48a456ba":"# Find Out the shape of data\nprint(f\"Training Shape : {train.shape}, Testing Shape : {test.shape}\")\n\n# Describe the data : train\nprint(\"Train Data Describe:\")\ntrain.describe()\nprint()\n# Describe the data : test\nprint(\"Test Data Describe:\")\ntest.describe()\nprint()\n# Data Information\nprint(\"Data Information:\")\ntrain.info()\ntest.info()\nprint()\n\ncat_cols = list(train.select_dtypes(include=['object']).columns)\nnum_cols = list(train.select_dtypes(exclude=['object']).columns)\n\n# Print datatype | Front no preprocess\nprint(f\"Numerical data : {[col for col in num_cols]}\")\nprint(f\"Categorical | Object data : {[col for col in cat_cols]}\")","1f28d7f1":"# Profilling\ndef profilling(df, filename):\n  # importing sweetviz\n  import sweetviz as sv                 #analyzing the dataset\n  advert_report = sv.analyze(df)  #display the report\n  advert_report.show_html(filename)","94f37302":"# Correlation\nplt.figure(figsize=(16,12))\ncorr = train[current_cols].corr()\nlimit = 0.0001 # limit the correlation\nm = ~(corr.mask(np.eye(len(corr), dtype=bool)).abs() < limit).any()\nm.is_subscribed = True\npaint = corr.loc[m, m]\nsns.heatmap(paint,linewidth=.5,annot=True,cmap='coolwarm', mask=np.triu(paint))\ncorr_cols = [col for col in m.index if m[col]]\nplt.show()","6c30906c":"# Showing Missing Data\ndef show_missing_data(df):\n    print(f\"Shape : {df.shape}\")\n    print(f\"Missing Data : {df.isnull().sum()}\")\n    return None","1dbf6eed":"# Creating distribution plot of a feature\ndef draw_dist(df,features,subtitle,figsize=(20,3)):\n  n_charts = len(features)\n  fig, axes = plt.subplots(ncols=n_charts,figsize=figsize)\n  fig.suptitle(subtitle, fontsize=16)\n  for i in range(n_charts):\n    feature = features[i-1]\n    sns.distplot(df[feature],ax=axes[i-1])\n  df[feature].describe()","10acf20c":"# Creating pairplot\ndef create_pairplot(df, target):\n  sns.pairplot(df, hue=target)","00e6b27c":"#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data1)","992e6fe7":"# Draw layered boxplot\ndef create_layered_boxplot(df,features, target_feature, subtitle='layered boxplot', kind='count',figsize=(20,7)):\n  n_charts = len(features)\n  fig, axes = plt.subplots(ncols=n_charts,figsize=figsize)\n  fig.suptitle(subtitle, fontsize=16)\n  for i in range(n_charts):\n    feature = features[i-1]\n    sns.boxplot(x=target_feature,y=feature,data=df,ax=axes[i-1])\n  df[feature].describe()","de9697d9":"# Draw feature distribution\ndef draw_dist(df,features,subtitle,figsize=(20,3)):\n  n_charts = len(features)\n  fig, axes = plt.subplots(ncols=n_charts,figsize=figsize)\n  fig.suptitle(subtitle, fontsize=16)\n  for i in range(n_charts):\n    feature = features[i-1]\n    sns.distplot(df[feature],ax=axes[i-1])\n  df[feature].describe()","4726f926":"# VIF -> Variance Inflation Factor | Checking the multicollinearity of selected feature\ndef calc_vif(X):\n    from statsmodels.stats.outliers_influence import variance_inflation_factor\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n    return(vif)","1c1d3432":"# Implementing VIF \ndrop_col = []\ncols = [col for col in current_cols if col not in drop_col]\nX = train[cols]\ny = train[target]\nvif = calc_vif(X)\nvif # show table\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(12,16))\nsns.barplot(x=\"VIF\", y=\"variables\",data=vif)\nplt.show()  # show plot","e374568e":"def create_rfe_feature_selection(df, target_feature, n_features=3, model = linear_model.LogisticRegression()):\n  from pprint import pprint\n  columns_list = list(df.columns)\n  columns_list.remove(target_feature)\n  rfe = feature_selection.RFE(model, n_features)\n  rfe.fit(df[columns_list],df[target_feature].values.ravel())\n  rfe_support = rfe.get_support()\n  rfe_feature = df[columns_list].loc[:,rfe_support].columns.tolist()\n  return rfe_feature\n\n# Usage :\n# rfe_cols = create_rfe_feature_selection(train[[col for col in train.columns if col not in (categorical + unique)]], target, n_features=25, model = tree.ExtraTreeClassifier())\n# rfe_cols = rfe_cols + [target]\n# rfe_cols","c5a87a97":"def create_skb_feature_selection(df, target_feature, k=10):\n  X = df.drop([target_feature], axis = 1)\n  y = df[target_feature]\n  bestfeatures = feature_selection.SelectKBest(score_func=feature_selection.f_classif, k = k)\n  fit = bestfeatures.fit(X,y)\n  dfscores = pd.DataFrame(fit.scores_)\n  dfcolumns = pd.DataFrame(X.columns)\n\n  featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n  featureScores.columns = ['Feature','Score']  #naming the dataframe columns\n  return featureScores.nlargest(k,'Score')\n\n# Usage : \n# kb_cols = create_skb_feature_selection(train[[col for col in train.columns if col not in (categorical + unique)]], target , k = 25)\n# list(skb_cols.Feature.values)","e40e63c6":"# Permutation Importance\ndef show_importance(train, target, model=ensemble.RandomForestClassifier()):\n    from sklearn.model_selection import train_test_split\n    X = train.drop([target], axis=1)\n    y = train[target]\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n    fit_res = model.fit(X_train, y_train)\n    perm = PermutationImportance(fit_res, random_state=1).fit(X_test, y_test)\n    return perm, X_test.columns.tolist()\n\n# Usage : \n# perm, cols = show_importance(train, target, model = ensemble.RandomForestClassifier())\n# eli5.show_weights(perm, feature_names = cols)","1ecdd078":"# Correlation\nplt.figure(figsize=(16,12))\ncorr = train[current_cols].corr()\nm = ~(corr.mask(np.eye(len(corr), dtype=bool)).abs() < 0.00001).any()\nm.is_subscribed = True\npaint = corr.loc[m, m]\nsns.heatmap(paint,linewidth=.5,annot=True,cmap='coolwarm', mask=np.triu(paint))\ncorr_cols = [col for col in m.index if m[col]]\nplt.show()","f9e5d3f9":"import seaborn as sns\nplt.figure(figsize=(16,12))\nmatrix_df = pps.matrix(df)[['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore')\nsns.heatmap(matrix_df, vmin=0, vmax=1, cmap=\"Blues\", linewidths=0.5, annot=True)","1a85cde4":"from sklearn.utils import resample\n\nnot_fraud_downsampled = resample(not_fraud,\n                                replace = False, # sample without replacement\n                                n_samples = len(fraud), # match minority n\n                                random_state = 27) # reproducible results\n\n# Combine minority and downsampled majority\ndownsampled = pd.concat([not_fraud_downsampled, fraud])","ff8e908a":"# Oversampling techniques\nrandom_sampling = resample(fraud,\n                          replace=True,\n                           n_samples = len(not_fraud),\n                           random_state = 42\n                          )\n\n# Combine minority and upsample data\nupsample = pd.concat([not_fraud,random_sampling])","bf7f6ad0":"X = train[cols].drop([target], axis=1)\ny = train[target]\n\ncat_col_index = []\n\nX_balanced , y_balanced = SMOTENC(categorical_features=cat_col_index, random_state=123).fit_resample(X,y)","aeb70ba9":"X = train[cols].drop([target], axis=1)\ny = train[target]\n\nX_balanced , y_balanced = SMOTE(random_state=123).fit_resample(X,y)","6178987b":"# Split train and test data\nfrom sklearn.model_selection import train_test_split\n\nX = train.drop([target], axis = 1)\ny = train[target]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","2171ed17":"# Usage : \n# generate_logs_from_classifiers(df, target, [\n#    ensemble.RandomForestClassifier(),\n#    linear_model.LogisticRegression()\n#])\n\n# Showing logs for multiple classifiers scoring\ndef generate_logs_from_classifiers(df, target,classifiers):\n  from sklearn.metrics import accuracy_score, f1_score, log_loss\n  log_cols=[\"Classifier\", \"F1_Score\"]\n  log = pd.DataFrame(columns=log_cols)\n\n  for clf in classifiers:\n      name = clf.__class__.__name__\n      print('Processing {} classifier'.format(name))\n      f1 = validation_models(clf, df.drop([target],axis=1), df[target].values)\n      print(\"Done {}\".format(name))\n      log_entry = pd.DataFrame([[name, f1]], columns=log_cols)\n      log = log.append(log_entry)\n  visualize_log(log)\n  return log\n\n# Private function for generate_logs_from_classifier\ndef visualize_log(log):\n  sns.set_color_codes(\"muted\")\n  sns.barplot(x='F1_Score', y='Classifier', data=log, color=\"b\")\n\n  plt.xlabel('F1Score %')\n  plt.title('Classifier Accuracy')\n  plt.show()","bafc9c4d":"# Create Instant Logistic regression\ndef create_logistic_regressions(X_train,y_train,figsize=(10,10)):\n  logreg = linear_model.LogisticRegression(solver='lbfgs')\n  logreg.fit(X_train, y_train)\n\n  coefficients = logreg.coef_\n  intercept = logreg.intercept_\n  \n  df_logreg = pd.DataFrame({'Feature':X_train.columns,'Coef':logreg.coef_[0]})\n  fig, ax = plt.subplots(figsize=figsize)\n  sns.barplot(x=\"Coef\", y=\"Feature\", data=df_logreg, ax=ax)\n  return logreg, df_logreg.set_index('Feature')\n\n# Visualize tree\ndef create_and_visualize_tree(X_train,y_train,max_depth=3):\n  decision_tree = tree.DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=1,random_state=1)\n  decision_tree = decision_tree.fit(X_train, y_train)\n  tree_str = export_graphviz(decision_tree, feature_names=X_train.columns, \n                                     filled=True, out_file=None)\n  graph = pydotplus.graph_from_dot_data(tree_str)  \n  graph.write_png('dt.png')\n  display(Image('dt.png'))\n  return decision_tree","f7989f86":"# ROC model scoring visualization\n\ndef take_roc_curve(X_test,model):\n  y_preds = model.predict_proba(X_test)\n  preds = y_preds[:,1]\n\n  fpr, tpr, _ = metrics.roc_curve(y_test, preds)\n  precision, recall, _ = metrics.precision_recall_curve(y_test, preds)\n  auc_score = metrics.auc(fpr, tpr)\n\n  plt.figure(figsize=(10,5))\n\n  plt.subplot(1, 2, 1)\n  plt.title('ROC Curve '+type(model).__name__)\n  plt.plot(fpr, tpr, label='AUC = {:.2f}'.format(auc_score))\n  plt.plot([0,1],[0,1],'r--')\n  plt.xlim([-0.1,1.1])\n  plt.ylim([-0.1,1.1])\n  plt.ylabel('True Positive Rate')\n  plt.xlabel('False Positive Rate')\n  plt.legend(loc='lower right')\n  \n  plt.subplot(1, 2, 2)\n  plt.step(recall, precision, color='orange', where='post')\n  plt.xlabel('Recall')\n  plt.ylabel('Precision')\n  plt.ylim([0.0, 1.05])\n  plt.xlim([0.0, 1.0])\n  plt.title('Precision Recall Curve')\n  plt.grid(True)\n\n  plt.tight_layout()\n  plt.show()","891d2eda":"# Plotting Confussion Matrix\n# Usage : \n# generate_confusion_matrix(X_test, y_test, models,Xlabels=[True,False],target_names=[True,False],normalize=False) -> plot + returns stats\n\ndef plot_confusion_matrix(cm_list,target_names,title_list,cmap=None,normalize=True,float_format_str='{:,.2f}'):\n    plt.figure(figsize=(10,5))\n    print('{}_count={:d}\\n{}_count={:d}'.format(target_names[0],cm_list[0][0].sum(),target_names[1],cm_list[0][1].sum()))\n    stats_list = []\n\n    for i in range(len(cm_list)):\n      model_name = title_list[i]\n      cm = cm_list[i]\n\n      actual_phishy= cm[0]\n      actual_benign= cm[1]      \n      \n      TP = actual_phishy[0]\n      FN = actual_phishy[1]\n      FP = actual_benign[0]\n      TN = actual_benign[1]\n\n      accuracy = np.trace(cm) \/ float(np.sum(cm))\n      misclass = 1 - accuracy\n      precision = TP\/float(TP+FP)\n      recall  = TP\/float(TP+FN)\n      fn_rate = FN\/float(TN+FN)\n      fp_rate = FP\/float(TP+FP)\n      \n      if cmap is None:\n          cmap = plt.get_cmap('Blues')\n\n      plt.subplot(1, len(cm_list), i+1)\n      plt.imshow(cm, interpolation='nearest', cmap=cmap)\n      plt.title(\"Confusion Matrix \" + model_name, fontsize=10)\n\n      if target_names is not None:\n          tick_marks = np.arange(len(target_names))\n          plt.xticks(tick_marks, target_names, rotation=45)\n          plt.yticks(tick_marks, target_names)\n\n      if normalize:\n          cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n          cm = cm.round(3)\n\n      thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n      for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                horizontalalignment=\"center\",\n                color=\"red\",\n                fontsize=15 )\n      plt.ylabel('True label', fontsize=10)\n      plt.xlabel('Predicted label\\naccuracy={:0.3f}; misclass={:0.3f}; \\nprecision={:0.3f} ; recall={:0.3f} ; \\nfn_rate={:0.3f} ; fp_rate={:0.3f} '.format(accuracy, misclass,precision,recall,fn_rate,fp_rate), fontsize=10)\n      model_stats = {'model_name':model_name,'accuracy':accuracy,'misclass':misclass,'precision':precision,'recall':recall,'fn_rate':fn_rate,'fp_rate':fp_rate }\n      stats_list.append(model_stats)\n    plt.tight_layout()\n    plt.show()\n    return generate_stats_df(stats_list,float_format_str)\n\ndef generate_stats_df(stats_list,float_format_str):\n  pd.options.display.float_format = float_format_str.format\n  df_stats = pd.DataFrame(stats_list)\n  df_stats.set_index('model_name',inplace=True)\n  return df_stats\n\ndef generate_confusion_matrix(X_test, y_test, models,Xlabels=[True,False],target_names=[True,False],normalize=False):\n  confusion_matrices = []\n  title_list = []\n  for model in models:\n    confusion_matrices.append(confusion_matrix(y_test,model.predict(X_test),labels=Xlabels))\n    title_list.append(type(model).__name__)\n\n  plot_confusion_matrix(cm_list = confusion_matrices,\n                        target_names = target_names,\n                        normalize = normalize,\n                        title_list   = title_list,\n                        float_format_str='{:,.3f}')\n\n","ff277a27":"# Cross validation any models with customize cv and scoring\ndef validation_models(model, X, y,cv=20, scoring=\"f1\"):\n    from sklearn.model_selection import cross_val_score\n    mean_score = cross_val_score(model, X, y, scoring=scoring, cv = cv).mean()\n    return mean_score","6aeebcaa":"# Validation using stratisfied K-fold\ndef validation_k_fold(X,y,model=linear_model.LogisticRegression(), shuffle=True):\n  from sklearn.model_selection import StratifiedKFold\n  skf = StratifiedKFold(n_splits=5,shuffle=shuffle)\n  score = 0\n  for train_index, test_index in tqdm.tqdm(skf.split(X.copy(), y.copy())):\n      X_train_k_fold, X_test_k_fold = X.iloc[train_index], X.iloc[test_index]\n      y_train_k_fold, y_test_k_fold = y.iloc[train_index], y.iloc[test_index]\n      \n      model_k_fold = model\n      model_k_fold.fit(X_train_k_fold, y_train_k_fold)\n      curr_score = validation_models(model_k_fold, X_test_k_fold, y_test_k_fold)\n      score += curr_score\n  return score\/5\n\n# Usage : \n# validation_k_fold(X, y, model=linear_model.LogisticRegression())","f504f048":"# https:\/\/github.com\/justmarkham\/scikit-learn-tips\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_selector, make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n# setup model\nmodel = LogisticRegression()\n\n# set up preprocessing for numeric columns\nimp_median = SimpleImputer(strategy='median', add_indicator=True)\nscaler = StandardScaler()\n\n# set up preprocessing for categorical columns\nimp_constant = SimpleImputer(strategy='constant')\nohe = OneHotEncoder(handle_unknown='ignore')\n\n# select columns by data type\nnum_cols = make_column_selector(dtype_include='number')\ncat_cols = make_column_selector(dtype_exclude='number')\n\n# do all preprocessing\npreprocessor = make_column_transformer(\n    (make_pipeline(imp_median, scaler), num_cols),\n    (make_pipeline(imp_constant, ohe), cat_cols))\n\n# create a pipeline\npipe = make_pipeline(preprocessor, model)\n\n# cross-validate the pipeline\ncross_val_score(pipe, X, y).mean()\n\n# fit the pipeline and make predictions\npipe.fit(X, y)\npipe.predict(test)","c0e7843d":"# Creating csv files from dataframe easily\ndef create_file(paths, filename, df):\n  import os\n  from pathlib import Path\n  print(\"Starting creation ..\")\n  cwd = Path().cwd()\n  for path in paths:\n    cwd = cwd \/ path \n    if not os.path.exists(cwd):\n      os.mkdir(cwd)\n\n  df.to_csv(cwd \/ filename, index=False)\n  print(\"Done ...\")\n  return None\n\n# Ex: create_file(['submission', 'day1'], \"Day1Submission.csv\", df)","b8244ba7":"# Check if file exists\ndef check_file(paths, filename):\n  import os\n  from pathlib import Path\n  cwd = Path().cwd()\n  for path in paths:\n    cwd = cwd \/ path\n  if not os.path.exists(cwd):\n    return False\n  return True\n\n# Ex: check_file(['submission', 'day1'], \"Day1Submission.csv\") -> bool","5de1a7a9":"# Creating Submission Easily after creating model\n# ps : the sample submission id & test id should be the same (else need to copy \/ remember the test id first)\n\n# Using the best model to create submission\npred = best_model.predict(test.copy())   \n# ========================================================\nsample_filename = \"..\/input\/<compe>\/sample_submission.csv\"\nsample = pd.read_csv(sample_filename)\ntest_idx = sample.id.copy()\n\nsubmission = pd.DataFrame({'id':test_idx,target:pred.astype(int)})\ncreate_file(['submission'],'submission.csv', submission)\n\nsubmission.info()\nprint('-' * 25)\nsubmission[target].value_counts() # show value counts","f9bc84f1":"<h3 class=\"list-group-item list-group-item-action list-group-item-warning\"><a id=\"clf\"><\/a>Classification<\/h3> ","4532aeb4":"<h3 class=\"list-group-item list-group-item-action active\"><a id=\"util\"><\/a>Utility<\/h3> \n\n<h4>List of Trick \u30fb\u2025\u2026\u2501\u2501\u2501\u2606:<\/h4>\n<div style=\"background: #f9f9f9 none repeat scroll 0 0;border: 1px solid #aaa;display: table;font-size: 95%;margin-bottom: 1em;padding: 20px;width: 400px;\">\n<ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n    <li style=\"list-style: outside none none !important;\"><a href=\"#file\">Files<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#submission\">Submissions<\/a>\n<\/ul>\n<\/div>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents<\/a>","d89aa3b9":"### <a id=\"prob\"><\/a> Submission problems\n\n\nUsually on Kaggle it is allowed to select two final submissions, which will be checked against the private LB and contribute to the competitor's final position. A common practice is to select one submission with a **best validation score**, and another submission which **scored best on Public LB**.\n\n\n**Cause of LB Shuffle** <br>\n\u2022 Randomness <br>\n\u2022 Little amount of data <br>\n\u2022 Different public\/private distributions","7b0ca14f":"<h3 class=\"list-group-item list-group-item-action list-group-item-danger\"><a id=\"submission\"><\/a>Submission<\/h3> ","81ec9ba8":"<h3 class=\"list-group-item list-group-item-action active\"><a id=\"doc\"><\/a>Documentation<\/h3> \n\n<h4>List of Documentation \u30fb\u2025\u2026\u2501\u2501\u2501\u2606:<\/h4>\n<div style=\"background: #f9f9f9 none repeat scroll 0 0;border: 1px solid #aaa;display: table;font-size: 95%;margin-bottom: 1em;padding: 20px;width: 400px;\">\n<ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n    <li style=\"list-style: outside none none !important;\"><a href=\"#prob\">Submission Problem<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#note\">Notebook<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#compe\">Competition<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#ref\">Reference<\/a>\n<\/ul>\n<\/div>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents<\/a>","9a908316":"<h3 class=\"list-group-item list-group-item-action list-group-item-warning\"><a id=\"bayes\"><\/a>Bayesian Search<\/h3> ","86911808":"<h3 class=\"list-group-item list-group-item-action list-group-item-warning\"><a id=\"split\"><\/a>Splitting<\/h3> ","8d6b7436":"<h3 class=\"list-group-item list-group-item-action list-group-item-info\"><a id=\"rfe\"><\/a>Recursive Feature Elimination<\/h3> ","0e44f3a8":"<h3 class=\"list-group-item list-group-item-action list-group-item-info\"><a id=\"perm\"><\/a>Permutation Importance<\/h3> ","aee343f6":"### <a id=\"ref\"><\/a> Reference\n\n\n**List of Reference \u30fb\u2025\u2026\u2501\u2501\u2501\u2606:**\n* [\ud83d\udc68\u200d\ud83c\udfeb My Personal Data Science Notes](https:\/\/www.kaggle.com\/dionisiusdh\/my-personal-data-science-notes)\n* [\u2714\ufe0fTOP Machine Learning Algorithms -Beginner\ud83d\udca5 ](https:\/\/www.kaggle.com\/marcovasquez\/top-machine-learning-algorithms-beginner)\n* [A Data Science Framework: To Achieve 99% Accuracy](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy)\n* [Tutorial on Permutation Importance](https:\/\/www.kaggle.com\/dansbecker\/permutation-importance)\n* [Tutorial on Power Predictive Score](https:\/\/www.kaggle.com\/frtgnn\/predictive-power-score-vs-correlation)\n* [Personal Template](https:\/\/colab.research.google.com\/drive\/15taSvaoF0887oqnyIEzSGrTja4UP8HJZ)\n\nps: This reference list for crediting author","fbde0f9f":"<h3 class=\"list-group-item list-group-item-action active\"><a id=\"base\"><\/a>Baseline <\/h3> \n\n<h4>List of Trick \u30fb\u2025\u2026\u2501\u2501\u2501\u2606:<\/h4>\n<div style=\"background: #f9f9f9 none repeat scroll 0 0;border: 1px solid #aaa;display: table;font-size: 95%;margin-bottom: 1em;padding: 20px;width: 400px;\">\n<ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n    <li style=\"list-style: outside none none !important;\"><a href=\"#pipeline\">Pipeline Baseline<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#pycaret\">Pycaret Baseline<\/a>\n<\/ul>\n<\/div>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents<\/a>","04646ae1":"<h1 align=\"center\"  style='color:white;font-size:30px;background-color:pink;'> Personal Tabular Competition Notebooks <\/h1>\n<h2>\ud83d\udc09 Hi Teammates, (most likely my future self :>) <\/h2>\nHere are the <b>snippets<\/b> and fast working <b>codes<\/b> to get started with any competition, especially <b>tabular<\/b> competition\n\nIt serve as our database of notebook for journey accross many Data Science competition. If any of our tricks not in this notebook, kindly added your tricks into this notebook\n\n\n<h2><a id=\"toc\"><\/a>List of Data Science Trickery \u30fb\u2025\u2026\u2501\u2501\u2501\u2606:<\/h2>\n<div style=\"background: #f9f9f9 none repeat scroll 0 0;border: 1px solid #aaa;display: table;font-size: 95%;margin-bottom: 1em;padding: 20px;width: 400px;\">\n<ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n    <li style=\"list-style: outside none none !important;\"><a href=\"#common\">Common Library and Utilities<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#prep\">Data Types & Preprocessing<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#eda\">Exploratory Data Analysis<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#viz\">Visualization<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#stats\">Statistical<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#leakages\">Data Leakages<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#select\">Feature Selection<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#imb\">Imbalance Dataset<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#model\">Modelling<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#val\">Validation<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#base\">Baseline<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#tune\">Fine Tune Model<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#util\">Utility<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#doc\">Documentation<\/a>\n<\/ul>\n<\/div>\n\n<h1>NOTES! THIS IS WORKING IN PROGRESS NOTEBOOK \ud83e\udd11<\/h1>\n<p>If you were an affiliate with Realteam, feel free to add any kind of tricks to this notebook<\/p>","3e2f0fd2":"<h3 class=\"list-group-item list-group-item-action active\"><a id=\"eda\"><\/a>Exploratory Data Analysis<\/h3> \n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents<\/a>","40b4b621":"<h3 class=\"list-group-item list-group-item-action active\"><a id=\"imb\"><\/a>Imbalance Dataset<\/h3> \n\n<h4>List of Trick \u30fb\u2025\u2026\u2501\u2501\u2501\u2606:<\/h4>\n<div style=\"background: #f9f9f9 none repeat scroll 0 0;border: 1px solid #aaa;display: table;font-size: 95%;margin-bottom: 1em;padding: 20px;width: 400px;\">\n<ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n    <li style=\"list-style: outside none none !important;\"><a href=\"#downsample\">Downsampling<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#upsample\">Upsampling<\/a>\n    \n<\/ul>\n<\/div>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents<\/a>","33a6db5f":"<h3 class=\"list-group-item list-group-item-action active\"><a id=\"tune\"><\/a>Fine Tune Model<\/h3> \n\n<h4>List of Trick \u30fb\u2025\u2026\u2501\u2501\u2501\u2606:<\/h4>\n<div style=\"background: #f9f9f9 none repeat scroll 0 0;border: 1px solid #aaa;display: table;font-size: 95%;margin-bottom: 1em;padding: 20px;width: 400px;\">\n<ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n    <li style=\"list-style: outside none none !important;\"><a href=\"#grid\">Grid Search<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#random\">Random Search<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#bayes\">Bayesian Search<\/a>\n<\/ul>\n<\/div>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents<\/a>","af32e991":"<h3 class=\"list-group-item list-group-item-action active\"><a id=\"prep\"><\/a>Data Types & Preprocessing<\/h3> \n\n<h4>List of Trick \u30fb\u2025\u2026\u2501\u2501\u2501\u2606:<\/h4>\n<div style=\"background: #f9f9f9 none repeat scroll 0 0;border: 1px solid #aaa;display: table;font-size: 95%;margin-bottom: 1em;padding: 20px;width: 400px;\">\n<ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n    <li style=\"list-style: outside none none !important;\"><a href=\"#import\">Importing Data<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#num\">Numeric<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#ord\">Ordinal<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#cat\">Categorical<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#date\">DateTime<\/a>\n<\/ul>\n<\/div>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents<\/a>","8ba3766d":"<h3 class=\"list-group-item list-group-item-action list-group-item-success\"><a id=\"cat\"><\/a> Categorical<\/h3> ","3e384d65":"<h3 class=\"list-group-item list-group-item-action active\"><a id=\"val\"><\/a>Validation<\/h3> \n\n<h4>List of Trick \u30fb\u2025\u2026\u2501\u2501\u2501\u2606:<\/h4>\n<div style=\"background: #f9f9f9 none repeat scroll 0 0;border: 1px solid #aaa;display: table;font-size: 95%;margin-bottom: 1em;padding: 20px;width: 400px;\">\n<ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n    <li style=\"list-style: outside none none !important;\"><a href=\"#hold\">Holdout<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#kfold\">K Fold<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#loo\">Leave One Out<\/a>\n<\/ul>\n<\/div>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents<\/a>","1f85644c":"<h3 class=\"list-group-item list-group-item-action list-group-item-success\"><a id=\"date\"><\/a>DateTime<\/h3> ","7a23d7b7":"<h3 class=\"list-group-item list-group-item-action list-group-item-info\"><a id=\"pps\"><\/a>Power Predictive Score<\/h3> \n\nThe PPS is an asymmetric, data-type-agnostic score that can detect linear or non-linear relationships between two columns. The score ranges from 0 (no predictive power) to 1 (perfect predictive power). It can be used as an alternative to the correlation (matrix).","e407b357":"<h3 class=\"list-group-item list-group-item-action active\"><a id=\"stats\"><\/a>Statistical<\/h3> \n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents<\/a>","6f46ee57":"<h3 class=\"list-group-item list-group-item-action active\"><a id=\"leakages\"><\/a>Data Leakages <\/h3> \n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents<\/a>\n\nThe most common types of leaks:\n1. Date \/ Time\n2. Meta data\n3. Information in IDs -> Might be a hash of something\n4. Row order\n\n### Leaky Predictor\nThis occurs when your predictors include data that will not be available at the time you make predictions.\n\n### Additional external source\n* <a href=\"https:\/\/www.kaggle.com\/olegtrott\/the-perfect-score-script\">The \"Perfect Score\" Script<a>\n* <a href=\"https:\/\/www.kaggle.com\/dansbecker\/data-leakage\">Data Leakage<a>","bc1a5f49":"<h3 class=\"list-group-item list-group-item-action active\"><a id=\"viz\"><\/a>Visualization<\/h3> \n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents<\/a>","b922dacd":"<h3 class=\"list-group-item list-group-item-action active\"><a id=\"common\"><\/a>Common Library and Utilities<\/h3> \n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents<\/a>","11c95bd5":"<h3 class=\"list-group-item list-group-item-action list-group-item-warning\"><a id=\"reg\"><\/a>Regression<\/h3> ","237a75a7":"<h3 class=\"list-group-item list-group-item-action list-group-item-success\"><a id=\"ord\"><\/a> Ordinal<\/h3> ","fa93ce90":"<h3 class=\"list-group-item list-group-item-action list-group-item-danger\"><a id=\"downsample\"><\/a>Downsampling<\/h3> ","161632f2":"<h3 class=\"list-group-item list-group-item-action list-group-item-default\"><a id=\"pipeline\"><\/a>Pipeline Baseline<\/h3> ","f454098e":"### SMOTE NC\nGood for categorical columns","cb9b6b8c":"<h3 class=\"list-group-item list-group-item-action active\"><a id=\"select\"><\/a>Feature Selection<\/h3> \n\n<h4>List of Trick \u30fb\u2025\u2026\u2501\u2501\u2501\u2606:<\/h4>\n<div style=\"background: #f9f9f9 none repeat scroll 0 0;border: 1px solid #aaa;display: table;font-size: 95%;margin-bottom: 1em;padding: 20px;width: 400px;\">\n<ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n    <li style=\"list-style: outside none none !important;\"><a href=\"#rfe\">Recursive Feature Elimination(RFE)<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#skb\">Select Based Feature (SKBest)<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#perm\">Permutation Importance<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#corr\">Correlation<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#pps\">Power Predictive Score (PPS)<\/a>\n<\/ul>\n<\/div>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents<\/a>","0fc38656":"<h3 class=\"list-group-item list-group-item-action list-group-item-info\"><a id=\"skb\"><\/a>Select K Best<\/h3> ","c142c897":"<h3 class=\"list-group-item list-group-item-action list-group-item-info\"><a id=\"hold\"><\/a>Holdout<\/h3> ","828efd46":"### <a id=\"note\"><\/a> Notebook\n\n\n\n**List of Notebook \u30fb\u2025\u2026\u2501\u2501\u2501\u2606:**\n* [SELEKSI GAIB (Complete Notebook)](https:\/\/colab.research.google.com\/drive\/1YaGUnTLF0v2LAeGhYBjfdrNLQBaUMEB_?usp=sharing\n)\n* [Tabular Template](https:\/\/colab.research.google.com\/drive\/15taSvaoF0887oqnyIEzSGrTja4UP8HJZ?usp=sharing)\n* [Anforcom](https:\/\/www.kaggle.com\/williamong\/iubestgirl-anforcom)\n* [Realteam Drive](https:\/\/drive.google.com\/drive\/u\/2\/folders\/1hfaGCojLkDsp6Mjqq0osLXI_etgKHkFs)\n\nps: This notebook only useful as reference\n","ad00f532":"<h3 class=\"list-group-item list-group-item-action list-group-item-success\"><a id=\"num\"><\/a> Numeric<\/h3> ","822687c5":"### SMOTE only\nGood for non-categorical columns","d3cdabd5":"### <a id=\"compe\"><\/a> Competition\n\n\n\n**List of Competition \u30fb\u2025\u2026\u2501\u2501\u2501\u2606:**\n* [JOINTS 2020](https:\/\/www.kaggle.com\/c\/datmin-joints-2020)\n* [Seleksi GAIB](https:\/\/www.kaggle.com\/c\/seleksidukungaib)\n* [DATA MINING ANFORCOM 2020](https:\/\/www.kaggle.com\/c\/datmin-anforcom-2020)\n\nps: This competition list is useful as reminder of myself \ud83e\udd70. Tabular competition only!","ce2d3afc":"<h3 class=\"list-group-item list-group-item-action list-group-item-info\"><a id=\"corr\"><\/a>Correlation<\/h3> ","c1030404":"<h3 class=\"list-group-item list-group-item-action list-group-item-info\"><a id=\"loo\"><\/a>Leave One Out<\/h3> ","ce70152d":"<h3 class=\"list-group-item list-group-item-action list-group-item-info\"><a id=\"kfold\"><\/a>K Fold<\/h3> ","51b8f61f":"<h3 class=\"list-group-item list-group-item-action active\"><a id=\"model\"><\/a>Modelling<\/h3> \n\n<h4>List of Trick \u30fb\u2025\u2026\u2501\u2501\u2501\u2606:<\/h4>\n<div style=\"background: #f9f9f9 none repeat scroll 0 0;border: 1px solid #aaa;display: table;font-size: 95%;margin-bottom: 1em;padding: 20px;width: 400px;\">\n<ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n    <li style=\"list-style: outside none none !important;\"><a href=\"#split\">Splitting<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#clf\">Classification<\/a>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#reg\">Regression<\/a>\n<\/ul>\n<\/div>\n\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left;margin-right:10px;\" >  Back to the list of contents<\/a>","60036acf":"<h3 class=\"list-group-item list-group-item-action list-group-item-danger\"><a id=\"file\"><\/a>Files<\/h3> ","61440803":"### Common validation strategies (Reference)\n**Holdout scheme:**\n1. Split train data into two parts: partA and partB.\n2. Fit the model on partA, predict for partB.\n3. Use predictions for partB for estimating model quality. Find such hyper-parameters, that quality on partB is maximized.\n\n**K-Fold scheme:**\n1. Split train data into K folds.\n2. Iterate though each fold: retrain the model on all folds except current fold, predict for the current fold.\n3. Use the predictions to calculate quality on each fold. Find such hyper-parameters, that quality on each fold is maximized. You can also estimate mean and variance of the loss. This is very helpful in order to understand significance of improvement.\n\n**LOO (Leave-One-Out) scheme:**\n1. Iterate over samples: retrain the model on all samples except current sample, predict for the current sample. You will need to retrain the model N times (if N is the number of samples in the dataset).\n2. In the end you will get LOO predictions for every sample in the trainset and can calculate loss.","231190b6":"<h3 class=\"list-group-item list-group-item-action list-group-item-success\">Importing Data<\/h3> <a id=\"import\"><\/a>","da68a3c6":"<h3 class=\"list-group-item list-group-item-action list-group-item-secondary\"><a id=\"pycaret\"><\/a>Pycaret Baseline<\/h3> ","4d291443":"<h3 class=\"list-group-item list-group-item-action list-group-item-danger\"><a id=\"upsample\"><\/a>Upsampling<\/h3> ","7557965e":"<h3 class=\"list-group-item list-group-item-action list-group-item-warning\"><a id=\"grid\"><\/a>Grid Search<\/h3> ","425734a1":"<h3 class=\"list-group-item list-group-item-action list-group-item-warning\"><a id=\"random\"><\/a>Random Search<\/h3> "}}