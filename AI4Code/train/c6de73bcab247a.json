{"cell_type":{"db3cc251":"code","c8f27498":"code","b082b841":"code","a8d9c3ef":"code","41739054":"code","635a96ee":"code","2bf49e04":"code","d2b49d89":"code","eebdae5c":"code","2cb1416d":"code","999225f4":"code","fbfc4052":"code","b0fb167f":"code","667dfdb9":"code","e3f401f3":"code","9b8a1a9e":"code","89a2c512":"code","b8a5f1f3":"code","1f732735":"code","5d162027":"code","0aef6a34":"code","3887a668":"code","17032e7c":"code","5cecf668":"code","9dd76a00":"code","718a2e92":"code","b013a178":"code","fa2d3320":"code","9338a5f4":"code","7f8123b7":"code","d205d7c1":"code","37950c18":"code","550db3fe":"code","832fea67":"code","340efb10":"code","4402bb3f":"code","ea6beb24":"code","38779984":"code","46610ca6":"code","609f24a1":"code","ba07ada1":"code","05704648":"code","62b5b14d":"code","2a8267d6":"code","a0dc205e":"code","e3dc2d9f":"code","6e770c57":"code","29457169":"code","13585b5b":"code","388459d5":"code","501492ea":"code","69176119":"code","a60f6e41":"code","ec3865f1":"code","31be73a6":"code","d0bba20a":"code","c867e156":"code","f613a065":"code","50ecdabe":"code","4a2b9b5d":"code","b38714e8":"code","09418fe2":"code","d446de4f":"code","83e2217f":"code","9b708349":"code","2fa7bfcf":"code","94081eb2":"code","e7007dde":"code","a4f18176":"code","8e81487a":"markdown","20bdf423":"markdown","4fb18246":"markdown","0e7f4933":"markdown","e4a06543":"markdown","7d37a0b1":"markdown","420cae69":"markdown","ee5b453f":"markdown","38c3da6b":"markdown","2198e2b7":"markdown","090b5182":"markdown","31c74aa6":"markdown","a3d9d269":"markdown","e449f4f4":"markdown","06ee7784":"markdown","f71b7685":"markdown","ae020d41":"markdown","8aa5c891":"markdown","177b47b3":"markdown","09c99eb7":"markdown","3da6ffb7":"markdown","7af0a5e9":"markdown","31ce5b7c":"markdown","a6b075d4":"markdown","9794a540":"markdown","81dd662a":"markdown","dbdee167":"markdown","2a425bbb":"markdown","fb787e50":"markdown","7ebe0039":"markdown","9bef62fc":"markdown"},"source":{"db3cc251":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","c8f27498":"!pip install --upgrade tensorflow\n!pip install gensim\n!pip install paramiko\n!pip install keras\n!pip install -U numpy\n\n","b082b841":"%%time\n  !pip install \"tensorflow_hub>=0.6.0\"\n\n  import tensorflow as tf\n  import tensorflow_hub as hub","a8d9c3ef":"pip install sentencepiece","41739054":"#!pip install simpletransformers\n#!pip install transformers\n#!pip install tensorboardx","635a96ee":"# # rom simpletransformers.classification import ClassificationModel\n# import pandas as pd\n# import logging\n# from simpletransformers.classification import ClassificationModel\n# import sys\n# import sklearn\n# import os, re, string\n# import random\n\n# import numpy as np\n# import pandas as pd\n# import sklearn\n","2bf49e04":"# Import packages\nimport os\nimport time\nimport math\nimport copy\nimport keras\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\n\nimport gc\nimport re\nimport string\nimport operator\nfrom collections import defaultdict\n\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport tokenization\nfrom wordcloud import STOPWORDS\n\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow import keras\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n\n\nfrom keras import *\nfrom keras import layers\nfrom gensim.models import *\nfrom sklearn.metrics import *\nfrom keras.models import Model\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing import *\nfrom collections import defaultdict\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n\n\n#from simpletransformers.classification import ClassificationModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\n\n# Matplotlib parameters\nplt.style.use('seaborn-paper')\nplt.rcParams['figure.figsize'] = [12, 8] ## plot size\nplt.rcParams['axes.linewidth'] = 2.0 #set the value globally\n\n# Notebook style properties\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:95% !important; }<\/style>\"))\ndisplay(HTML(\"<style> .output_png { display: table-cell; text-align: center; vertical-align: middle; } <\/style>\"))\ndisplay(HTML(\"<style>.MathJax {font-size: 300%;}<\/style>\"))\n\n# Ignore warnings\ndef warn(*args, **kwargs):\n    pass\n\nwarnings.warn = warn\n\n# Set random initialization seeds for reproducible results\nfrom numpy.random import seed\nseed(1)","d2b49d89":"def getMetrics(trueLabels, predictedLabels, predictedLabelsProbabilities):\n    \"\"\"Takes as input true labels, predictions, and prediction confidence scores and computes all metrics\"\"\"\n    \n    aucScore = round(roc_auc_score(trueLabels, predictedLabelsProbabilities) * 100, 1)\n    precisionLow = round(precision_score(trueLabels, predictedLabels, average=None)[0] * 100, 1)\n    precisionHigh = round(precision_score(trueLabels, predictedLabels, average=None)[1] * 100, 1)\n    recallLow = round(recall_score(trueLabels, predictedLabels, average=None)[0] * 100, 1)\n    recallHigh = round(recall_score(trueLabels, predictedLabels, average=None)[1] * 100, 1)\n    fpr, tpr, threshold = roc_curve(trueLabels, predictedLabelsProbabilities)\n    \n    return aucScore, precisionLow, precisionHigh, recallLow, recallHigh, fpr, tpr\n\ndef isnan(object):\n    \"\"\"Check for a null value\"\"\"\n    return object != object\n\ndef removeNans(inputList):\n    \"\"\"Remove null values\"\"\"\n    inputList = [item if isnan(item) == False else 0 for item in inputList]\n    return inputList\n\ndef loadTabDelimitedDataForEmbeddings(dataPath):\n    \"\"\"Load data from a tab delimited file. Used for embeddings\"\"\"\n    data = pd.DataFrame(pd.read_csv(dataPath, \"\\t\"))[\"Word\"].values.tolist()\n    return data\n\ndef splitRowIntoWords(row, length):\n    \"\"\"Takes a variable length text input and convert it into a list of words with length equal to 'length' in the function parameter\"\"\"\n    words = keras.preprocessing.text.text_to_word_sequence(row, filters=' !#$%&()*+,-.\/:;<=>?@[\\\\]^_{|}~\\t\\n\"\\'', lower=True, split=\" \")\n    \n    # If length is less than required length, add zeros\n    while len(words) < length:\n        words.append(0)\n        \n    # If greater, remove stuff at the end\n    if len(words) >= length:\n        words = words[:length]\n        \n    return words\n\ndef convertCharDataIntoArrays(inputText, characters, charsDict, MAX_CHARACTERS_TO_KEEP):\n    \"\"\"Takes as input a list of text rows and convert each text row into a list of character embeddings\"\"\"\n    characterFeatures = []\n    for row in inputText:\n        characterFeature = np.zeros((MAX_CHARACTERS_TO_KEEP, len(characters)+1))\n        for c in range(0, min(len(row), MAX_CHARACTERS_TO_KEEP)):\n            character = row[c]\n            # Explicitly converting each character into a character embedding i.e just a one hot encoded representation\n            charPresence = np.zeros(len(characters)+1)\n            \n            # Add 1 where the character is present\n            if character in charsDict:\n                charPresence[charsDict[character]] = 1\n                characterFeature[c] = charPresence\n                \n        characterFeatures.append(characterFeature)\n\n    return np.array(characterFeatures)\n\ndef joinRepresentations(representationList):\n    \"\"\"Takes multiple forms of text input and concatenate them together.\"\"\"\n    joinedReprsentations = ''.join(str(representation) + \"\\t\" for representation in representationList)\n    return joinedReprsentations\n\ndef removeNan(inputList):\n    \"\"\"Remove null values from input\"\"\"\n    copiedList = list(inputList)\n    if isnan(copiedList[-1]) == True:\n        del copiedList[-1]\n        \n    return copiedList\n\ndef normalizeStructuredData(drugDict, inputList):\n    \"\"\"Takes as input a drug list(drugDict) and a structure array list and converts variables into nominal and categorical variables\"\"\"\n    \n    drug = inputList[0]\n    gender = inputList[1]\n    age = inputList[2]\n    days = inputList[3]\n    \n    drugArray = [0] * (len(drugDict) + 1) # convert drug into a categorical variable (one hot encoding of a drug)\n    if drug in drugDict:\n        drugArray[drugDict[drug]] = 1\n        \n    age = float(age \/ 100)\n    \n    # convert gender into a categorical variable\n    if gender == 1:\n        genderArray = [1,0]\n    else:\n        genderArray = [0,1]\n    \n    # if more than 1000 days, just keep them to 1000\n    days = days \/ 1000\n    if days >= 1:\n        days = 1\n    \n    # add normalized data to a new list and return that list\n    normalizedList = []\n    normalizedList.extend(drugArray)\n    normalizedList.extend(genderArray)\n    normalizedList.append(age)\n    normalizedList.append(days)\n    \n    return normalizedList\n    \ndef addText(xticks, yticks, results):\n    \"\"\"Add text in the plot\"\"\"\n    for i in range(2):\n        for j in range(2):\n            pltText = plt.text(j, i, results[i][j], ha=\"center\", va=\"center\", color=\"white\", size=15) ### size here is the size of text inside a single box in the heatmap    \n    \n    \ndef displayConfusionMatrix(confusionMatrix):\n    \"\"\"Confusion matrix plot\"\"\"\n    \n    confusionMatrix = np.transpose(confusionMatrix)\n    \n    ## calculate class level precision and recall from confusion matrix\n    precisionLow = round((confusionMatrix[0][0] \/ (confusionMatrix[0][0] + confusionMatrix[0][1]))*100, 1)\n    precisionHigh = round((confusionMatrix[1][1] \/ (confusionMatrix[1][0] + confusionMatrix[1][1]))*100, 1)\n    recallLow = round((confusionMatrix[0][0] \/ (confusionMatrix[0][0] + confusionMatrix[1][0]))*100, 1)\n    recallHigh = round((confusionMatrix[1][1] \/ (confusionMatrix[0][1] + confusionMatrix[1][1]))*100, 1)\n\n    ## show heatmap\n    plt.imshow(confusionMatrix, interpolation='nearest',cmap=plt.cm.Blues,vmin=0, vmax=100)\n    \n    ## axis labeling\n    PLOT_FONT_SIZE = 14\n    xticks = np.array([-0.5,0,1,1.5])\n    plt.gca().set_xticks(xticks)\n    plt.gca().set_yticks(xticks)\n    plt.gca().set_xticklabels([\"\",\"LowSev \\n Recall=\" + str(recallLow), \"HighSev \\n Recall=\" + str(recallHigh),\"\"], fontsize=PLOT_FONT_SIZE)\n    plt.gca().set_yticklabels([\"\",\"LowSev \\n Precision=\" + str(precisionLow), \"HighSev \\n Precision=\" + str(precisionHigh),\"\"], fontsize=PLOT_FONT_SIZE)\n    plt.ylabel(\"Predicted Class\", fontsize=PLOT_FONT_SIZE)\n    plt.xlabel(\"Actual Class\", fontsize=PLOT_FONT_SIZE)\n\n    ## add text in heatmap boxes\n    addText(xticks, xticks, confusionMatrix)","eebdae5c":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)","2cb1416d":"#Simpletransformers Format\n\n#delimiter = \"\\t\"\n#inputDataTrain = pd.DataFrame(pd.read_csv(\"\/content\/drive\/My Drive\/Colab Files\/train_DrugExp_Text (4).tsv\", delimiter, header=None, names = ['labels','text']))\n#inputDataTest = pd.DataFrame(pd.read_csv(\"\/content\/drive\/My Drive\/Colab Files\/test_DrugExp_Text (1).tsv\", delimiter, header=None, names = ['target','text']))\n#df_train.loc[df_train.target != 1, 'target'] = 0\n#inputDataTrain.loc[inputDataTrain.labels != 1, 'labels'] = 0","999225f4":"# Uncased BERT Join\n\nstart = time.time()\ndelimiter = \"\\t\"\n\ninputDataTrain = pd.DataFrame(pd.read_csv(\"..\/input\/ux-classification-data\/text data\/train_DrugExp_Text (5).tsv\", delimiter, header=None, names = ['target','text']))\ninputDataTest = pd.DataFrame(pd.read_csv(\"..\/input\/ux-classification-data\/text data\/test_DrugExp_Text (2).tsv\", delimiter, header=None, names = ['target','text']))\ninputDataValidation = pd.DataFrame(pd.read_csv(\"..\/input\/ux-classification-data\/text data\/validation_DrugExp_Text (2).tsv\", delimiter, header=None, names =  ['target','text']))\nSTRUCTURED_DATA_FILE = \"_DrugExp_Structured.tsv\"\ninputDataTrainStructured = pd.DataFrame(pd.read_csv(\"..\/input\/ux-classification-data\/structured data files\/train_DrugExp_Structured (4).tsv\" + STRUCTURED_DATA_FILE, delimiter, header=None))\ninputDataTestStructured = pd.DataFrame(pd.read_csv(\"..\/input\/ux-classification-data\/structured data files\/test_DrugExp_Structured (1).tsv\" + STRUCTURED_DATA_FILE, delimiter, header=None))\ninputDataValidationStructured = pd.DataFrame(pd.read_csv(\"S..\/input\/ux-classification-data\/structured data files\/validation_DrugExp_Structured (1).tsv\" + STRUCTURED_DATA_FILE, delimiter, header=None))\n\n## display\nprint(inputDataTrainStructured.values.tolist()[5])\n\n## extract frn features from data frames\nallDrugs = list(sorted(list(set([item[1] for item in inputDataTrainStructured.values.tolist()]))))\nallDrugs = dict([(allDrugs[index], index) for index in range(0, len(allDrugs))])\n\ntrainStructured = np.array([normalizeStructuredData(allDrugs,item[1:]) for item in inputDataTrainStructured.values.tolist()])\ntestStructured = np.array([normalizeStructuredData(allDrugs,item[1:]) for item in inputDataTestStructured.values.tolist()])\nvalidationStructured = np.array([normalizeStructuredData(allDrugs,item[1:]) for item in inputDataValidationStructured.values.tolist()])\n\ninputDataTrainStructured = pd.DataFrame(pd.read_csv(\"..\/input\/ux-classification-data\/structured data files\/train_DrugExp_Structured (4).tsv\" + STRUCTURED_DATA_FILE, delimiter, header=None))\ninputDataTestStructured = pd.DataFrame(pd.read_csv(\"..\/input\/ux-classification-data\/text data\/test_DrugExp_Text (2).tsv\" + STRUCTURED_DATA_FILE, delimiter, header=None))\ninputDataValidationStructured = pd.DataFrame(pd.read_csv(\"..\/input\/ux-classification-data\/text data\/validation_DrugExp_Text (2).tsv\" + STRUCTURED_DATA_FILE, delimiter, header=None))\ndf_train = inputDataTrain.join(inputDataTrainStructured, lsuffix='_caller', rsuffix='_other')\ndf_test = inputDataTest.join(inputDataTestStructured, lsuffix='_caller', rsuffix='_other')\ndf_valid = inputDataValidation.join(inputDataValidationStructured, lsuffix='_caller', rsuffix='_other')\ndf_train.loc[df_train.target != 1, 'target'] = 0\ndf_valid.loc[df_valid.target != 1, 'target'] = 0\ndf_train.columns =['target','text','target2','keyword','col1','col2','col3']    \ndf_test.columns =['target','text','target2','keyword','col1','col2','col3'] \ndf_valid.columns =['target','text','target2','keyword','col1','col2','col3'] ","fbfc4052":"glove_embeddings ={}\nwith open('..\/input\/ux-classification-data\/glove.6B.300d (1).txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        glove_embeddings[word]=vectors\nf.close()","b0fb167f":"fasttext_embeddings = pd.read_pickle('..\/input\/ux-classification-data\/crawl-300d-2M (1).pkl","667dfdb9":"# Used a manual cleaning function from tweet sentiment analysis to spot edit contractions\n\n%%time\n\ndef clean(tweet): \n            \n    # Special characters\n    tweet = re.sub(r\"\\x89\u00db_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00d2\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00d3\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00cfWhen\", \"When\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00cf\", \"\", tweet)\n    tweet = re.sub(r\"China\\x89\u00db\u00aas\", \"China's\", tweet)\n    tweet = re.sub(r\"let\\x89\u00db\u00aas\", \"let's\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00f7\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00aa\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00a2\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00a2\u00e5\u00ca\", \"\", tweet)\n    tweet = re.sub(r\"from\u00e5\u00cawounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"\u00e5\u00ca\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c8\", \"\", tweet)\n    tweet = re.sub(r\"Jap\u00cc_n\", \"Japan\", tweet)    \n    tweet = re.sub(r\"\u00cc\u00a9\", \"e\", tweet)\n    tweet = re.sub(r\"\u00e5\u00a8\", \"\", tweet)\n    tweet = re.sub(r\"Suru\u00cc\u00a4\", \"Suruc\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c7\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5\u00a33million\", \"3 million\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c0\", \"\", tweet)\n    \n    # Contractions\n    tweet = re.sub(r\"crohn s\", \"Chrons\", tweet)\n    tweet = re.sub(r\"Crohn s\", \"Chrons\", tweet)\n    tweet = re.sub(r\"crohn's\", \"Chrons\", tweet)\n    tweet = re.sub(r\"Crohn's\", \"Chrons\", tweet)\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aam\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89\u00db\u00aas\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89\u00db\u00aas\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aave\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"don t\", \"do not\", tweet)\n    tweet = re.sub(r\"can't\", \"can not\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"won t\", \"will not\", tweet)\n    tweet = re.sub(r\"can t\", \"can not\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89\u00db\u00aas\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89\u00db\u00aare\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89\u00db\u00aat\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89\u00db\u00aat\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"didn t\", \"did not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"haven t\", \"Have not\", tweet)\n    tweet = re.sub(r\"havent\", \"Have not\", tweet)\n    tweet = re.sub(r\"wasnt\", \"was not\", tweet)\n    tweet = re.sub(r\"couldn t\", \"could not\", tweet)\n    tweet = re.sub(r\"I ve\", \"I have\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"i ve\", \"I have\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"i ll\", \"I will\", tweet)\n    tweet = re.sub(r\"it'll\", \"It will\", tweet)\n    tweet = re.sub(r\"it ll\", \"It will\", tweet)\n    tweet = re.sub(r\"could've\", \"could have\", tweet)\n    tweet = re.sub(r\"could ve\", \"could have\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"should ve\", \"should have\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"would ve\", \"would have\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"Couldn t\", \"Could not\", tweet)\n    tweet = re.sub(r\"Couldn't\", \"Could not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"Wouldn't\", \"Would not\", tweet)\n    tweet = re.sub(r\"wouldn t\", \"would not\", tweet)\n    tweet = re.sub(r\"Wouldn t\", \"Would not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"hasn t\", \"has not\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"doesn t\", \"does not\", tweet)\n    tweet = re.sub(r\"does'nt\", \"does not\", tweet)\n    tweet = re.sub(r\"Doesn't\", \"Does not\", tweet)\n    tweet = re.sub(r\"Doesn t\", \"Does not\", tweet)\n    tweet = re.sub(r\"Does'nt\", \"Does not\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"wasn t\", \"was not\", tweet)\n    tweet = re.sub(r\"had nt\", \"was not\", tweet)\n    tweet = re.sub(r\"had'nt\", \"had not\", tweet)\n    tweet = re.sub(r\"was nt\", \"was not\", tweet)\n    tweet = re.sub(r\"was'nt\", \"was not\", tweet)\n    tweet = re.sub(r\"Was nt\", \"Was not\", tweet)\n    tweet = re.sub(r\"Was'nt\", \"Was not\", tweet)\n    tweet = re.sub(r\"is nt\", \"is not\", tweet)\n    tweet = re.sub(r\"is'nt\", \"is not\", tweet)\n    tweet = re.sub(r\"isnt\", \"is not\", tweet)\n    tweet = re.sub(r\"could nt\", \"could not\", tweet)\n    tweet = re.sub(r\"could'nt\", \"could not\", tweet)\n    tweet = re.sub(r\"Could nt\", \"Could not\", tweet)\n    tweet = re.sub(r\"Could'nt\", \"Could not\", tweet)\n    tweet = re.sub(r\"sideeffects\", \"side effects\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n    tweet = re.sub(r\"mg\", \"milligrams\", tweet) \n    tweet = re.sub(r\"don\u00e5\u00abt\", \"do not\", tweet)   \n            \n    # Character entity references\n    tweet = re.sub(r\"&gt;\", \">\", tweet)\n    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n    \n    # Typos, slang and informal abbreviations\n    tweet = re.sub(r\"w\/e\", \"whatever\", tweet)\n    tweet = re.sub(r\"w\/\", \"with\", tweet)\n    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n    tweet = re.sub(r\"c diff\", \"Clostridioides difficile\", tweet)\n    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n    tweet = re.sub(r\"<3\", \"love\", tweet)\n    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n    tweet = re.sub(r\"8\/5\/2015\", \"2015-08-05\", tweet)\n    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n    tweet = re.sub(r\"8\/6\/2015\", \"2015-08-06\", tweet)\n    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \n    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n    # Words with punctuations and special characters\n    punctuations = '@#!?+&*[]-%.:\/();$=><|{}^' + \"'`\"\n    for p in punctuations:\n        tweet = tweet.replace(p, f' {p} ')\n        \n    # ... and ..\n    tweet = tweet.replace('...', ' ... ')\n    if '...' not in tweet:\n        tweet = tweet.replace('..', ' ... ')      \n        \n       \n    # Grouping same words without embeddings\n    tweet = re.sub(r\"Bestnaijamade\", \"bestnaijamade\", tweet)\n    tweet = re.sub(r\"SOUDELOR\", \"Soudelor\", tweet)\n    \n    return tweet\n\n\ninputDataTrain['text']=inputDataTrain['text'].apply(lambda s :  clean(s))\ninputDataTest['text']=inputDataTest['text'].apply(lambda s :  clean(s))\ninputDataValidation['text']=inputDataValidation['text'].apply(lambda s :  clean(s))\ndf_train['text_cleaned'] =df_train['text'].apply(lambda s :  clean(s))\ndf_test['text_cleaned'] =df_test['text'].apply(lambda s :  clean(s))\ndf_valid['text_cleaned']=df_valid['text'].apply(lambda s :  clean(s))","e3f401f3":"inputDataTrain['text']=inputDataTrain['text'].apply(lambda x :  remove_punct(x))\ninputDataTest['text']=inputDataTest['text'].apply(lambda x :  remove_punct(x))\ninputDataValidation['text']=inputDataValidation['text'].apply(lambda x :  remove_punct(x))","9b8a1a9e":"import operator\n\ndef build_vocab(X):\n    \n    entry = X.apply(lambda s: s.split()).values      \n    vocab = {}\n    \n    for entry in entry:\n        for word in entry:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1                \n    return vocab\n\n\ndef check_embeddings_coverage(X, embeddings):\n    \n    vocab = build_vocab(X)    \n    \n    covered = {}\n    oov = {}    \n    n_covered = 0\n    n_oov = 0\n    \n    for word in vocab:\n        try:\n            covered[word] = embeddings[word]\n            n_covered += vocab[word]\n        except:\n            oov[word] = vocab[word]\n            n_oov += vocab[word]\n            \n    vocab_coverage = len(covered) \/ len(vocab)\n    text_coverage = (n_covered \/ (n_covered + n_oov))\n    \n    sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n    return sorted_oov, vocab_coverage, text_coverage\n\ntrain_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(inputDataTrain['text'], glove_embeddings)\ntest_glove_oov, test_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(inputDataTest['text'], glove_embeddings)\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_glove_vocab_coverage, train_glove_text_coverage))\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_glove_vocab_coverage, test_glove_text_coverage))\n\ntrain_fasttext_oov, train_fasttext_vocab_coverage, train_fasttext_text_coverage = check_embeddings_coverage(inputDataTrain['text'], fasttext_embeddings)\ntest_fasttext_oov, test_fasttext_vocab_coverage, test_fasttext_text_coverage = check_embeddings_coverage(inputDataTest['text'], fasttext_embeddings)\nprint('FastText Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_fasttext_vocab_coverage, train_fasttext_text_coverage))\nprint('FastText Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_fasttext_vocab_coverage, test_fasttext_text_coverage))","89a2c512":"train_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(df_train['text_cleaned'], glove_embeddings)\ntest_glove_oov, test_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(df_test['text_cleaned'], glove_embeddings)\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_glove_vocab_coverage, train_glove_text_coverage))\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_glove_vocab_coverage, test_glove_text_coverage))\n\ntrain_fasttext_oov, train_fasttext_vocab_coverage, train_fasttext_text_coverage = check_embeddings_coverage(df_train['text_cleaned'], fasttext_embeddings)\ntest_fasttext_oov, test_fasttext_vocab_coverage, test_fasttext_text_coverage = check_embeddings_coverage(df_test['text_cleaned'], fasttext_embeddings)\nprint('FastText Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_fasttext_vocab_coverage, train_fasttext_text_coverage))\n# print('FastText Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_fasttext_vocab_coverage, test_fasttext_text_coverage))","b8a5f1f3":"## 2.1 Delete Embeddings to save memory\n\ndel glove_embeddings, fasttext_embeddings, train_glove_oov, test_glove_oov, train_fasttext_oov, test_fasttext_oov\ngc.collect()","1f732735":"# n=5\n# kf = KFold(n_splits=n, random_state=seed, shuffle=True)\n# results = []\n\n# for train_index, val_index in kf.split(inputDataTrain):\n#     train_df = inputDataTrain.iloc[train_index]\n#     eval_df = inputDataTrain.iloc[val_index]","5d162027":"# model_type = \"bert\"\n\n# if model_type == \"bert\":\n#     model_name = \"bert-base-cased\"\n\n# elif model_type == \"roberta\":\n#     model_name = \"roberta-base\"\n\n# elif model_type == \"distilbert\":\n#     model_name = \"distilbert-base-cased\"\n\n# elif model_type == \"distilroberta\":\n#     model_type = \"roberta\"\n#     model_name = \"distilroberta-base\"\n\n# elif model_type == \"electra-base\":\n#     model_type = \"electra\"\n#     model_name = \"google\/electra-base-discriminator\"\n\n# elif model_type == \"electra-small\":\n#     model_type = \"electra\"\n#     model_name = \"google\/electra-small-discriminator\"\n    \n# elif model_type == \"xlnet\":\n#     model_name = \"xlnet-base-cased\"\n\n# train_args = {\n# \"output_dir\": \"\/content\/drive\/My Drive\/Colab Files\/outputs\/\",\n# \"cache_dir\": \"\/content\/drive\/My Drive\/Colab Files\/cache\/\",\n# \"best_model_dir\": \"\/content\/drive\/My Drive\/Colab Files\/outputs\/best_model\/\",\n\n# \"fp16\": False,\n# \"max_seq_length\": 128,\n# \"train_batch_size\": 8,\n# \"eval_batch_size\": 8,\n# \"gradient_accumulation_steps\": 1,\n# \"num_train_epochs\": 5,\n# \"weight_decay\": 0,\n# \"learning_rate\": 4e-5,\n# \"adam_epsilon\": 1e-8,\n# \"warmup_ratio\": 0.06,\n# \"warmup_steps\": 0,\n# \"max_grad_norm\": 1.0,\n# \"do_lower_case\": True,\n# 'sliding_window': True,                                                         \n                                                             \n# \"logging_steps\": 50,\n# \"evaluate_during_training\": True,\n# \"evaluate_during_training_steps\": 2000,\n# \"evaluate_during_training_verbose\": True,\n# \"use_cached_eval_features\": False,\n# \"save_eval_checkpoints\": True,\n# \"save_steps\": 2000,\n# \"no_cache\": False,\n# \"save_model_every_epoch\": True,\n# \"tensorboard_dir\": None,\n# \"overwrite_output_dir\": True,  \n# }\n\n# if model_type == \"xlnet\":\n#     train_args[\"train_batch_size\"] = 64\n#     train_args[\"gradient_accumulation_steps\"] = 2\n\n\n# # Create a ClassificationModel\n# model = ClassificationModel(model_type, model_name, args=train_args)\n\n# # Train the model\n# model.train_model(train_df, eval_df=eval_df)\n\n# # # # Evaluate the model\n# result, model_outputs, wrong_predictions = model.eval_model(eval_df, acc=sklearn.metrics.accuracy_score)\n# # ","0aef6a34":"# predictions, raw_outputs = model.predict(inputDataTest['text'])\n# dataset = pd.DataFrame(predictions)\n# dataset = pd.DataFrame({ 'Id' : dataset.index , 'Prediction': predictions })\n# print(dataset)\n# dataset.to_csv('Test.csv', index=False)","3887a668":"## Relabel for BERT\nSEED = 1337\ndf_train['target_relabeled'] = df_train['target']\nK = 2\nskf = StratifiedKFold(n_splits=K, random_state=SEED, shuffle=True)\n\nADVERSE = df_train['target'] == -1\nprint('Whole Training Set Shape = {}'.format(df_train.shape))\nprint('Whole Training Set Unique keyword Count = {}'.format(df_train['keyword'].nunique()))\nprint('Whole Training Set Target Rate (Adverse) {}\/{} (Not Adverse)'.format(df_train[ADVERSE]['target_relabeled'].count(), df_train[~ADVERSE]['target_relabeled'].count()))\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(df_train['text_cleaned'], df_train['target']), 1):\n    print('\\nFold {} Training Set Shape = {} - Validation Set Shape = {}'.format(fold, df_train.loc[trn_idx, 'text_cleaned'].shape, df_train.loc[val_idx, 'text_cleaned'].shape))\n    print('Fold {} Training Set Unique Drug Count = {} - Validation Set Unique Drug Count = {}'.format(fold, df_train.loc[trn_idx, 'keyword'].nunique(), df_train.loc[val_idx, 'keyword'].nunique()))    \n   ","17032e7c":"class ClassificationReport(Callback):\n    \n    def __init__(self, train_data=(), validation_data=()):\n        super(Callback, self).__init__()\n        \n        self.X_train, self.y_train = train_data\n        self.train_precision_scores = []\n        self.train_recall_scores = []\n        self.train_f1_scores = []\n        \n        self.X_val, self.y_val = validation_data\n        self.val_precision_scores = []\n        self.val_recall_scores = []\n        self.val_f1_scores = [] \n               \n    def on_epoch_end(self, epoch, logs={}):\n        train_predictions = np.round(self.model.predict(self.X_train, verbose=0))        \n        train_precision = precision_score(self.y_train, train_predictions, average='macro')\n        train_recall = recall_score(self.y_train, train_predictions, average='macro')\n        train_f1 = f1_score(self.y_train, train_predictions, average='macro')\n        self.train_precision_scores.append(train_precision)        \n        self.train_recall_scores.append(train_recall)\n        self.train_f1_scores.append(train_f1)\n        \n        val_predictions = np.round(self.model.predict(self.X_val, verbose=0))\n        val_precision = precision_score(self.y_val, val_predictions, average='macro')\n        val_recall = recall_score(self.y_val, val_predictions, average='macro')\n        val_f1 = f1_score(self.y_val, val_predictions, average='macro')\n        self.val_precision_scores.append(val_precision)        \n        self.val_recall_scores.append(val_recall)        \n        self.val_f1_scores.append(val_f1)\n        \n        print('\\nEpoch: {} - Training Precision: {:.6} - Training Recall: {:.6} - Training F1: {:.6}'.format(epoch + 1, train_precision, train_recall, train_f1))\n        print('Epoch: {} - Validation Precision: {:.6} - Validation Recall: {:.6} - Validation F1: {:.6}'.format(epoch + 1, val_precision, val_recall, val_f1))  ","5cecf668":"%%time\n\nbert_layer = hub.KerasLayer('https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1', trainable=True)","9dd76a00":"class BERTUncased:\n    \n    def __init__(self, bert_layer, max_seq_length=128, lr=0.0001, epochs=15, batch_size=32):\n        \n        # BERT and Tokenization params\n        self.bert_layer = bert_layer\n        \n        self.max_seq_length = max_seq_length        \n        vocab_file = self.bert_layer.resolved_object.vocab_file.asset_path.numpy()\n        do_lower_case = self.bert_layer.resolved_object.do_lower_case.numpy()\n        self.tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n        \n        # Learning control params\n        self.lr = lr\n        self.epochs = epochs\n        self.batch_size = batch_size\n        \n        self.models = []\n        self.scores = {}\n        \n        \n    def encode(self, texts):\n                \n        all_tokens = []\n        all_masks = []\n        all_segments = []\n\n        for text in texts:\n            text = self.tokenizer.tokenize(text)\n            text = text[:self.max_seq_length - 2]\n            input_sequence = ['[CLS]'] + text + ['[SEP]']\n            pad_len = self.max_seq_length - len(input_sequence)\n\n            tokens = self.tokenizer.convert_tokens_to_ids(input_sequence)\n            tokens += [0] * pad_len\n            pad_masks = [1] * len(input_sequence) + [0] * pad_len\n            segment_ids = [0] * self.max_seq_length\n\n            all_tokens.append(tokens)\n            all_masks.append(pad_masks)\n            all_segments.append(segment_ids)\n\n        return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n    \n    \n    def build_model(self):\n        \n        input_word_ids = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='input_word_ids')\n        input_mask = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='input_mask')\n        segment_ids = Input(shape=(self.max_seq_length,), dtype=tf.int32, name='segment_ids')    \n        \n        pooled_output, sequence_output = self.bert_layer([input_word_ids, input_mask, segment_ids])   \n        clf_output = sequence_output[:, 0, :]\n        out = Dense(1, activation='sigmoid')(clf_output)\n        \n        model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n        optimizer = SGD(learning_rate=self.lr, momentum=0.8)\n        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n        \n        return model\n    \n    \n    def train(self, X):\n        \n        for fold, (trn_idx, val_idx) in enumerate(skf.split(X['text_cleaned'], X['keyword'])):\n            \n            print('\\nFold {}\\n'.format(fold))\n        \n            X_trn_encoded = self.encode(X.loc[trn_idx, 'text_cleaned'].str.lower())\n            y_trn = X.loc[trn_idx, 'target_relabeled']\n            X_val_encoded = self.encode(X.loc[val_idx, 'text_cleaned'].str.lower())\n            y_val = X.loc[val_idx, 'target_relabeled']\n        \n            # Callbacks\n            metrics = ClassificationReport(train_data=(X_trn_encoded, y_trn), validation_data=(X_val_encoded, y_val))\n            \n            # Model\n            model = self.build_model()        \n            model.fit(X_trn_encoded, y_trn, validation_data=(X_val_encoded, y_val), callbacks=[metrics], epochs=self.epochs, batch_size=self.batch_size)\n            \n            self.models.append(model)\n            self.scores[fold] = {\n                'train': {\n                    'precision': metrics.train_precision_scores,\n                    'recall': metrics.train_recall_scores,\n                    'f1': metrics.train_f1_scores                    \n                },\n                'validation': {\n                    'precision': metrics.val_precision_scores,\n                    'recall': metrics.val_recall_scores,\n                    'f1': metrics.val_f1_scores                    \n                }\n            }\n                    \n                \n    def plot_learning_curve(self):\n        \n        fig, axes = plt.subplots(nrows=K, ncols=2, figsize=(20, K * 6), dpi=100)\n    \n        for i in range(K):\n            \n            # Classification Report curve\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[i].history.history['val_accuracy'], ax=axes[i][0], label='val_accuracy')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['precision'], ax=axes[i][0], label='val_precision')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['recall'], ax=axes[i][0], label='val_recall')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.scores[i]['validation']['f1'], ax=axes[i][0], label='val_f1')        \n\n            axes[i][0].legend() \n            axes[i][0].set_title('Fold {} Validation Classification Report'.format(i), fontsize=14)\n\n            # Loss curve\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[0].history.history['loss'], ax=axes[i][1], label='train_loss')\n            sns.lineplot(x=np.arange(1, self.epochs + 1), y=clf.models[0].history.history['val_loss'], ax=axes[i][1], label='val_loss')\n\n            axes[i][1].legend() \n            axes[i][1].set_title('Fold {} Train \/ Validation Loss'.format(i), fontsize=14)\n\n            for j in range(2):\n                axes[i][j].set_xlabel('Epoch', size=12)\n                axes[i][j].tick_params(axis='x', labelsize=12)\n                axes[i][j].tick_params(axis='y', labelsize=12)\n\n        plt.show()\n        \n        \n    def predict(self, X):\n        \n        X_test_encoded = self.encode(X['text_cleaned'].str.lower())\n        y_pred = np.zeros((X_test_encoded[0].shape[0], 1))\n\n        for model in self.models:\n            y_pred += model.predict(X_test_encoded) \/ len(self.models)\n\n        return y_pred","718a2e92":"# Takes ~ 7Hrs to train\n\n#clf = BERTUncased(bert_layer, max_seq_length=128, lr=0.0001, epochs=12, batch_size=32)\n#clf.train(df_train)","b013a178":"#y_pred = clf.predict(df_test)\n#pd.DataFrame(y_pred).to_csv(\"model_submission5.csv\")\n#dataset = pd.DataFrame(y_pred)","fa2d3320":"#dataset = pd.DataFrame({ 'Id' : dataset.index , 'Prediction': y_pred[:, 0] })\n#print(dataset)\n#dataset.to_csv('model_submission2.csv', index=False)","9338a5f4":"from wordcloud import STOPWORDS\nAdverse_Reactions = inputDataTrain['target'] == -1\n\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]\n\nN = 100\n\n# Unigrams\nHighSev_unigrams = defaultdict(int)\nLowSev_unigrams = defaultdict(int)\n\nfor entry in inputDataTrain[Adverse_Reactions]['text']:\n    for word in generate_ngrams(entry):\n        HighSev_unigrams[word] += 1\n        \nfor entry in inputDataTrain[~Adverse_Reactions]['text']:\n    for word in generate_ngrams(entry):\n        LowSev_unigrams[word] += 1\n        \ndf_HighSev_unigrams = pd.DataFrame(sorted(HighSev_unigrams.items(), key=lambda x: x[1])[::-1])\ndf_LowSev_unigrams = pd.DataFrame(sorted(LowSev_unigrams.items(), key=lambda x: x[1])[::-1])\n\n# Bigrams\nHighSev_bigrams = defaultdict(int)\nLowSev_bigrams = defaultdict(int)\n\nfor entry in inputDataTrain[Adverse_Reactions]['text']:\n    for word in generate_ngrams(entry, n_gram=2):\n        HighSev_bigrams[word] += 1\n        \nfor entry in inputDataTrain[~Adverse_Reactions]['text']:\n    for word in generate_ngrams(entry, n_gram=2):\n        LowSev_bigrams[word] += 1\n        \ndf_HighSev_bigrams = pd.DataFrame(sorted(HighSev_bigrams.items(), key=lambda x: x[1])[::-1])\ndf_LowSev_bigrams = pd.DataFrame(sorted(LowSev_bigrams.items(), key=lambda x: x[1])[::-1])\n\n# Trigrams\nHighSev_trigrams = defaultdict(int)\nLowSev_trigrams = defaultdict(int)\n\nfor entry in inputDataTrain[Adverse_Reactions]['text']:\n    for word in generate_ngrams(entry, n_gram=3):\n        HighSev_trigrams[word] += 1\n        \nfor entry in inputDataTrain[~Adverse_Reactions]['text']:\n    for word in generate_ngrams(entry, n_gram=3):\n        LowSev_trigrams[word] += 1\n        \ndf_HighSev_trigrams = pd.DataFrame(sorted(HighSev_trigrams.items(), key=lambda x: x[1])[::-1])\ndf_LowSev_trigrams = pd.DataFrame(sorted(LowSev_trigrams.items(), key=lambda x: x[1])[::-1])","7f8123b7":"# word_count\ndf_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\ndf_test['word_count'] = df_test['text'].apply(lambda x: len(str(x).split()))\n\n# unique_word_count\ndf_train['unique_word_count'] = df_train['text'].apply(lambda x: len(set(str(x).split())))\ndf_test['unique_word_count'] = df_test['text'].apply(lambda x: len(set(str(x).split())))\n\n# stop_word_count\ndf_train['stop_word_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ndf_test['stop_word_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n# mean_word_length\ndf_train['mean_word_length'] = df_train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ndf_test['mean_word_length'] = df_test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\n# char_count\ndf_train['char_count'] = df_train['text'].apply(lambda x: len(str(x)))\ndf_test['char_count'] = df_test['text'].apply(lambda x: len(str(x)))\n\n# punctuation_count\ndf_train['punctuation_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ndf_test['punctuation_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))","d205d7c1":"METAFEATURES = ['word_count', 'unique_word_count', 'stop_word_count', 'mean_word_length',\n                'char_count', 'punctuation_count']\nHighSevADR = df_train['target'] == 0\n\nfig, axes = plt.subplots(ncols=2, nrows=len(METAFEATURES), figsize=(20, 50), dpi=100)\n\nfor i, feature in enumerate(METAFEATURES):\n    sns.distplot(df_train.loc[~HighSevADR][feature], label='LowSev', ax=axes[i][0], color='green')\n    sns.distplot(df_train.loc[HighSevADR][feature], label='HighSev', ax=axes[i][0], color='red')\n\n    sns.distplot(df_train[feature], label='Training', ax=axes[i][1])\n    sns.distplot(df_test[feature], label='Test', ax=axes[i][1])\n    \n    for j in range(2):\n        axes[i][j].set_xlabel('')\n        axes[i][j].tick_params(axis='x', labelsize=12)\n        axes[i][j].tick_params(axis='y', labelsize=12)\n        axes[i][j].legend()\n    \n    axes[i][0].set_title(f'{feature} Target Distribution in Training Set', fontsize=13)\n    axes[i][1].set_title(f'{feature} Training & Test Set Distribution', fontsize=13)\n\nplt.show()","37950c18":"fig, axes = plt.subplots(ncols=2, figsize=(20, 100), dpi=100)\n\nsns.barplot(y=df_HighSev_trigrams[0].values[:N], x=df_HighSev_trigrams[1].values[:N], ax=axes[0], color='darkorange')\nsns.barplot(y=df_LowSev_trigrams[0].values[:N], x=df_LowSev_trigrams[1].values[:N], ax=axes[1], color='darkblue')\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=11)\n\naxes[0].set_title(f'Top {N} most common trigrams in High Severity Reaction', fontsize=15)\naxes[1].set_title(f'Top {N} most common trigrams in Low Severity Reaction', fontsize=15)\n\nplt.show()","550db3fe":"fig, axes = plt.subplots(ncols=2, figsize=(18, 50), dpi=100)\nplt.tight_layout()\n\nsns.barplot(y=df_HighSev_bigrams[0].values[:N], x=df_HighSev_bigrams[1].values[:N], ax=axes[0], color='darkorange')\nsns.barplot(y=df_LowSev_bigrams[0].values[:N], x=df_LowSev_bigrams[1].values[:N], ax=axes[1], color='darkblue')\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {N} most common bigrams in Adverse Reaction', fontsize=15)\naxes[1].set_title(f'Top {N} most common bigrams in Non Adverse Reaction', fontsize=15)\n\nplt.show()","832fea67":"fig, axes = plt.subplots(ncols=2, figsize=(18, 50), dpi=100)\nplt.tight_layout()\n\nsns.barplot(y=df_HighSev_unigrams[0].values[:N], x=df_HighSev_unigrams[1].values[:N], ax=axes[0], color='darkorange')\nsns.barplot(y=df_LowSev_unigrams[0].values[:N], x=df_LowSev_unigrams[1].values[:N], ax=axes[1], color='darkblue')\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {N} most common unigrams in Adverse Reaction', fontsize=15)\naxes[1].set_title(f'Top {N} most common unigrams in Non Adverse Reaction', fontsize=15)\n\nplt.show()","340efb10":"## extract text from data frames\ntrainText = [item[1] for item in inputDataTrain.values.tolist()]\ntestText = [item[1] for item in inputDataTest.values.tolist()]\nvalidationText = [item[1] for item in inputDataValidation.values.tolist()]\n\n## extract labels\ntrainingLabels = [0 if item[0] == -1 else 1 for item in inputDataTrain.values.tolist()]\nvalidationLabels = [0 if item[0] == -1 else 1 for item in inputDataValidation.values.tolist()]\n\nprint(\"Total number of rows\", len(trainText), len(testText), len(validationText))\nprint(\"Data loading took %d seconds\" % (time.time() - start))","4402bb3f":"start = time.time()\ndelimiter = \",\"\n\n\n#FRN_TOP_N_FEATURES = 1000\n\n#inputDataTrainFRN = pd.DataFrame(pd.read_csv(\"..\/Shared Data (Read Only)\/DrugExpDeepLearning\/Data\/FRNMatrixFiles\/DrugExp_Train_TopN_\" + str(FRN_TOP_N_FEATURES) + \".csv\", delimiter))\n#inputDataTestFRN = pd.DataFrame(pd.read_csv(\"..\/Shared Data (Read Only)\/DrugExpDeepLearning\/Data\/FRNMatrixFiles\/DrugExp_Test_TopN_\" + str(FRN_TOP_N_FEATURES) + \".csv\", delimiter))\n#inputDataValidationFRN = pd.DataFrame(pd.read_csv(\"..\/Shared Data (Read Only)\/DrugExpDeepLearning\/Data\/FRNMatrixFiles\/DrugExp_Validation_TopN_\" + str(FRN_TOP_N_FEATURES) + \".csv\", delimiter))\n\n\n## extract frn features from data frames\n#trainFRN = np.array([item[1:] for item in inputDataTrainFRN.values.tolist()])\n#testFRN = np.array([item[1:] for item in inputDataTestFRN.values.tolist()])\n#validationFRN = np.array([item[1:] for item in inputDataValidationFRN.values.tolist()])\n\n#print(\"Data shape\", trainFRN.shape, testFRN.shape, validationFRN.shape)\n#print(\"Total number of rows\", len(trainFRN), len(testFRN), len(validationFRN))\n#print(\"Data loading took %d seconds\" % (time.time() - start))","ea6beb24":"start = time.time()\ndelimiter = \"\\t\"\n\n#STRUCTURED_DATA_FILE = \"_DrugExp_Structured.tsv\"\n#inputDataTrainStructured = pd.DataFrame(pd.read_csv(\"..\/input\/ux-classification-data\/structured data files\/train_DrugExp_Structured (4).tsv\" + STRUCTURED_DATA_FILE, delimiter, header=None))\n#inputDataTestStructured = pd.DataFrame(pd.read_csv(\"..\/input\/ux-classification-data\/structured data files\/test_DrugExp_Structured (1).tsv\" + STRUCTURED_DATA_FILE, delimiter, header=None))\n#inputDataValidationStructured = pd.DataFrame(pd.read_csv(\"..\/input\/ux-classification-data\/structured data files\/validation_DrugExp_Structured (1).tsv\" + STRUCTURED_DATA_FILE, delimiter, header=None))\n\n## display\n#print(inputDataTrainStructured.values.tolist()[5])\n\n## extract frn features from data frames\n#allDrugs = list(sorted(list(set([item[1] for item in inputDataTrainStructured.values.tolist()]))))\n#allDrugs = dict([(allDrugs[index], index) for index in range(0, len(allDrugs))])\n\n#trainStructured = np.array([normalizeStructuredData(allDrugs,item[1:]) for item in inputDataTrainStructured.values.tolist()])\n#testStructured = np.array([normalizeStructuredData(allDrugs,item[1:]) for item in inputDataTestStructured.values.tolist()])\n#validationStructured = np.array([normalizeStructuredData(allDrugs,item[1:]) for item in inputDataValidationStructured.values.tolist()])\n\n#print(\"Data shape\", trainStructured.shape, testStructured.shape, validationStructured.shape)\n#print(\"Total number of rows\", len(trainStructured), len(testStructured), len(validationStructured))\n#print(\"Data loading took %d seconds\" % (time.time() - start))","38779984":"# Input file for creating word embeddings. \ndelimiter = \"\\t\"\nINPUT_FILE = \"..\/input\/ux-classification-data\/text data\/test_DrugExp_Text (2).tsv\"\nINPUT_DATA = pd.DataFrame(pd.read_csv(\"..\/input\/ux-classification-data\/text data\/test_DrugExp_Text (2).tsv\", delimiter, header=None)).values.tolist()\n\n# Hyperparameters\nOUTPUT_EMBEDDING_FILE_NAME = \"Embeddings\/WordEmbeddings.txt\"\nWINDOW_SIZE = 3\nEMBEDDING_DIMENSION = 128\nMIN_FREQUENCY_THRESHOLD = 2\nIS_SKIPGRAM = 1 # make this 0 to use continuous bag of words (CBOW) model\nIS_LOWER_WORDS = True\nITERATIONS = 50 # iterations that we use in creating word embeddings\n\n# Convert data into a list of words\nrowWords = []\nfor row in INPUT_DATA:\n    rowText = row[1]\n    words  = keras.preprocessing.text.text_to_word_sequence(rowText, filters=' !#$%&()*+,-.\/:;<=>?@[\\\\]^_{|}~\\t\\n\"\\'', lower=IS_LOWER_WORDS, split=\" \") #ignoring the punctuations\n    rowWords.append(words)\n\n# Train model\nstart = time.time()\nmodel = word2vec.Word2Vec(rowWords, size=EMBEDDING_DIMENSION,window=WINDOW_SIZE, min_count=MIN_FREQUENCY_THRESHOLD, sg=IS_SKIPGRAM, workers=8, iter=ITERATIONS)\n\n# Save\nmodel.wv.save_word2vec_format(OUTPUT_EMBEDDING_FILE_NAME)\nos.system(\"sed -i '1d' \" + OUTPUT_EMBEDDING_FILE_NAME) ## delete first column of the embedding\nprint('Your embeddings have been created in %.2f seconds' % (time.time() - start))","46610ca6":"def loadDataForWordsWithoutPreTrainedEmbeddings(trainText, validationText, testText, VOCABULARY_SIZE):\n    \"\"\"This function takes as input three text files and a vocabulary size (words to keep in the model) and returns lists containing word ids and the total number of words for the model.\n    The word ids will be used the keras.layers.Embedding function to create word embeddings\"\"\"\n    \n    # If vocabulary size is not defined, use all words\n    if VOCABULARY_SIZE == None:\n        tokenizer = Tokenizer()\n        tokenizer.fit_on_texts(trainText)\n        WORDS_TO_KEEP = len(tokenizer.word_index)\n        \n    # Delete words based on frequency and keep words equal to the vocabulary size\n    else:\n        tokenizer = Tokenizer(num_words=VOCABULARY_SIZE)\n        tokenizer.fit_on_texts(trainText)\n        WORDS_TO_KEEP = VOCABULARY_SIZE\n    \n    ## convert words into word ids\n    meanLength = np.mean([len(item.split(\" \")) for item in trainText])\n    MAX_SENTENCE_LENGTH = int(meanLength + 100) # we let a sentence go 100 words longer than the mean sentence length.\n    \n    ## convert train, validation, and test text into lists with word ids\n    trainWordFeatures = tokenizer.texts_to_sequences(trainText)\n    trainWordFeatures = pad_sequences(trainWordFeatures, MAX_SENTENCE_LENGTH, padding='post')\n\n    validationWordFeatures = tokenizer.texts_to_sequences(validationText)\n    validationWordFeatures = pad_sequences(validationWordFeatures, MAX_SENTENCE_LENGTH, padding='post')\n\n    testWordFeatures = tokenizer.texts_to_sequences(testText)\n    testWordFeatures = pad_sequences(testWordFeatures, MAX_SENTENCE_LENGTH, padding='post')\n    \n    return trainWordFeatures, validationWordFeatures, testWordFeatures, WORDS_TO_KEEP\n\ndef loadDataForWordsWithPreTrainedEmbeddings(trainText, validationText, testText, EMBEDDING_PATH):\n    \"\"\"This function takes as input three text files and and a pre-trained word embedding file and returns arrays containing word embeddings for each word in the text. These arrays can be used \n    directly in a keras model without the use of keras.layers.Embedding layer.\"\"\"\n    \n    # Load embeddings\n    #embeddingsData = pd.DataFrame(io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore'),\" \").values.tolist()\n    #embeddingsDataDictionary = dict([(item[0], item[1:len(item)-1]) for item in embeddingsData]) # create dictionary of key=word, value=word embedding from the embedding file\n    embeddingsDataDictionary = pd.read_pickle('..\/Case03 - Deep Learning Playbook\/crawl-300d-2M.pkl')\n    ##embeddingsData = pd.DataFrame(pd.read_csv(EMBEDDING_PATH, \" \")).values.tolist()\n    ##embeddingsDataDictionary = dict([(item[0], item[1:len(item)-1]) for item in embeddingsData]) # create dictionary of key=word, value=word embedding from the embedding file\n    EMBEDDING_SIZE = int(len(embeddingsDataDictionary[random.choice(list(embeddingsDataDictionary.keys()))]))\n    \n    ## convert words into word ids\n    meanLength = np.mean([len(item.split(\" \")) for item in trainText])\n    MAX_SENTENCE_LENGTH = int(meanLength + 100) # we let a sentence go 100 words longer than the mean sentence length.\n    \n    ## convert train, validation, and test text into lists with word ids\n    trainTextWords = [splitRowIntoWords(row, MAX_SENTENCE_LENGTH) for row in trainText]\n    trainWordFeatures = []\n    for row in trainTextWords:\n        rowEmbeddings = [embeddingsDataDictionary[word] if word in embeddingsDataDictionary else [0]*EMBEDDING_SIZE for word in row]\n        trainWordFeatures.append(rowEmbeddings)\n\n    validationTextWords = [splitRowIntoWords(row, MAX_SENTENCE_LENGTH) for row in validationText]\n    validationWordFeatures = []\n    for row in validationTextWords:\n        rowEmbeddings = [embeddingsDataDictionary[word] if word in embeddingsDataDictionary else [0]*EMBEDDING_SIZE for word in row]\n        validationWordFeatures.append(rowEmbeddings)\n\n    testTextWords = [splitRowIntoWords(row, MAX_SENTENCE_LENGTH) for row in testText]\n    testWordFeatures = []\n    for row in testTextWords:\n        rowEmbeddings = [embeddingsDataDictionary[word] if word in embeddingsDataDictionary else [0]*EMBEDDING_SIZE for word in row]\n        testWordFeatures.append(rowEmbeddings)\n    \n    return np.array(trainWordFeatures), np.array(validationWordFeatures), np.array(testWordFeatures), None","609f24a1":"def loadDataForChars(trainText, validationText, testText, MAX_CHARACTERS_TO_KEEP, USE_UPPER_CASE_CHARACTERS=True):\n    \"\"\"Create input feature lists for character based models\"\"\"\n    \n    # create characters string based on the set parameter. You can also keep\/remove punctuation and see if it affects results\n    if USE_UPPER_CASE_CHARACTERS:\n        characters = \" ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-,'.!?();:-_=+@#$%^&*\/\\\"\"\n    else:\n        characters = \" abcdefghijklmnopqrstuvwxyz0123456789-,'.!?();:-_=+@#$%^&*\/\\\"\"\n    \n    charsDict = {}\n    numsDict = {}\n\n    ## assign an id to each character\n    for c in range(0,len(characters)):\n        charsDict[characters[c]] = c+1\n        numsDict[c+1] = characters[c]\n\n    ## create feature vectors for each row of text (Train)\n    characterFeaturesTrain = convertCharDataIntoArrays(trainText, characters, charsDict, MAX_CHARACTERS_TO_KEEP) \n\n    ## create feature vectors for each row of text (Test)\n    characterFeaturesTest = convertCharDataIntoArrays(testText, characters, charsDict, MAX_CHARACTERS_TO_KEEP) \n\n    ## create feature vectors for each row of text (Validation)\n    characterFeaturesValidation = convertCharDataIntoArrays(validationText, characters, charsDict, MAX_CHARACTERS_TO_KEEP) \n\n    return characterFeaturesTrain, characterFeaturesValidation, characterFeaturesTest","ba07ada1":"def createWordCNN(trainFeatures, validationFeatures, testFeatures, numConvLayers, vocabularyWords, embeddingsDimensionality, numFilters, kernel, isPreTrainedEmbeddings): \n    \"\"\"Create a word cnn\"\"\"\n    \n    ## create basic cnn model\n    wordInput = Input(shape=trainFeatures.shape[1:], dtype='float32')\n \n    ## word convolutional neural network\n    if isPreTrainedEmbeddings == False:\n        # Create embeddings using keras built in function.\n        wordCNN = keras.layers.Embedding(input_dim=vocabularyWords + 1, \n                                   output_dim=embeddingsDimensionality, \n                                   input_length=len(trainFeatures[0]))(wordInput)\n    \n        # Add CNN layers equal to numConvLayers\n        for i in range(numConvLayers):\n            wordCNN = keras.layers.Conv1D(numFilters, kernel, activation='relu')(wordCNN)\n            wordCNN = keras.layers.Dropout(0.5)(wordCNN)\n    else:\n        \n        # Here, we are using pre-trained embeddings. Therefore, we don't need to call layers.embeddings function.\n        wordCNN = keras.layers.Conv1D(numFilters, kernel, activation='relu', input_shape=trainFeatures.shape[1:])(wordInput)\n        wordCNN = keras.layers.Dropout(0.5)(wordCNN)\n        for i in range(numConvLayers - 1):\n            wordCNN = keras.layers.Conv1D(numFilters, kernel, activation='relu')(wordCNN)\n            wordCNN = keras.layers.Conv1D(numFilters, kernel, activation='relu')(wordCNN)       \n            wordCNN = keras.layers.Dropout(0.5)(wordCNN)\n    \n    # GlobalMaxPooling is a good function to use for pooling operations, let's keep it like this\n    wordCNN = keras.layers.GlobalMaxPooling1D()(wordCNN)\n    wordCNN = keras.layers.Dropout(0.7)(wordCNN)\n    \n    # You can change the number of nodes in the dense layer. Right now, it's set to 64.\n    denseLayer = keras.layers.Dense(64)(wordCNN)\n    \n    return denseLayer, wordInput","05704648":"def createWordLSTM(trainFeatures, validationFeatures, testFeatures, numLstmLayers, vocabularyWords, embeddingsDimensionality, lstmNodes, bidirectional, isPreTrainedEmbeddings):  \n    \"\"\"Create a word lstm\"\"\"\n    \n    ## create basic cnn model\n    wordInput = Input(shape=trainFeatures.shape[1:], dtype='float32')\n \n    ## word convolutional neural network\n    if isPreTrainedEmbeddings == False:\n        # Create embeddings using keras built in function.\n        wordLSTM = keras.layers.Embedding(input_dim=vocabularyWords + 1, \n                                   output_dim=embeddingsDimensionality, \n                                   input_length=len(trainFeatures[0]))(wordInput)\n    \n        # Add CNN layers\n        for i in range(numLstmLayers):\n            if bidirectional == False:\n                wordLSTM = keras.layers.LSTM(lstmNodes)(wordLSTM)\n            else:\n                wordLSTM = keras.layers.Bidirectional(keras.layers.LSTM(lstmNodes))(wordLSTM)\n            wordLSTM = keras.layers.Dropout(0.5)(wordLSTM)\n    else:\n        # Here, we are using pre-trained embeddings. Therefore, we don't need to call layers.embeddings function.\n        if bidirectional == False:\n            wordLSTM = keras.layers.LSTM(lstmNodes, input_shape=trainFeatures.shape[1:])(wordInput)\n        else:\n            wordLSTM = keras.layers.Bidirectional(keras.layers.LSTM(lstmNodes, input_shape=trainFeatures.shape[1:]), input_shape=trainFeatures.shape[1:])(wordInput)\n                \n        wordLSTM = keras.layers.Dropout(0.5)(wordLSTM)\n        for i in range(numLstmLayers - 1):\n            if bidirectional == False:\n                wordLSTM = keras.layers.LSTM(lstmNodes)(wordLSTM)\n            else:\n                wordLSTM = keras.layers.Bidirectional(keras.layers.LSTM(lstmNodes))(wordLSTM)\n            wordLSTM = keras.layers.Dropout(0.5)(wordLSTM)\n    \n    # You can change the number of nodes in the dense layer. Right now, it's set to 64.\n    denseLayer = keras.layers.Dense(64)(wordLSTM)\n    \n    return denseLayer, wordInput","62b5b14d":"def createCharCNN(trainFeatures, validationFeatures, testFeatures, numConvLayers, numFilters, kernel): \n    \"\"\"Create a char cnn\"\"\"\n    \n    ## create basic cnn model\n    charInput = Input(shape=trainFeatures.shape[1:], dtype='float32')\n \n    # Here, we are using one hot encoded representation of characters. Therefore, we don't need to call layers.embeddings function.\n    charCNN = keras.layers.Conv1D(numFilters, kernel, activation='relu', input_shape=trainFeatures.shape[1:])(charInput)\n    charCNN = keras.layers.Dropout(0.2)(charCNN)\n    for i in range(numConvLayers - 1):\n        charCNN = keras.layers.Conv1D(numFilters, kernel, activation='relu')(charCNN)\n        charCNN = keras.layers.Dropout(0.2)(charCNN)\n    \n    charCNN = keras.layers.GlobalMaxPooling1D()(charCNN)\n    \n    # You can change the number of nodes in the dense layer. Right now, it's set to 64.\n    denseLayer = keras.layers.Dense(64)(charCNN)\n    \n    return denseLayer, charInput","2a8267d6":"def createFeedForwardNeuralNetwork(trainFeatures, validationFeatures, testFeatures, numLayers, layerNodes): \n    \"\"\"Create a feed forward neural network\"\"\"\n    \n    ## create basic cnn model\n    modelInput = Input(shape=trainFeatures.shape[1:], dtype='float32')\n    neuralNetworkLayer = keras.layers.Dense(layerNodes, activation='relu', input_shape=trainFeatures.shape[1:])(modelInput)\n    neuralNetworkLayer = keras.layers.Dropout(0.5)(neuralNetworkLayer)\n    for i in range(numLayers - 1):\n        neuralNetworkLayer = keras.layers.Dense(layerNodes, activation='relu')(neuralNetworkLayer)\n        neuralNetworkLayer = keras.layers.Dropout(0.5)(neuralNetworkLayer)\n    \n    # You can change the number of nodes in the dense layer. Right now, it's set to 32.\n    denseLayer = keras.layers.Dense(128)(neuralNetworkLayer)\n    \n    return denseLayer, modelInput","a0dc205e":"from keras.optimizers import adam","e3dc2d9f":"def concatenateModels(modelDenseLayers):\n    \"\"\"Get a list if dense layers and concatenate them together\"\"\"\n    concatenatedModel = keras.layers.concatenate(modelDenseLayers)\n    \n    \"\"\"You can add more layers here after the concatenation of models e.g you can add single\/multiple dense layers after the concatenation layer to give the model more power\"\"\"\n    return concatenatedModel\n\ndef attachOutputLayerToModel(lastDenseLayer, modelInputs):\n    \"\"\"Take as input a dense layer and attach an output layer\"\"\"\n    output = keras.layers.Dense(1, activation='sigmoid')(lastDenseLayer)\n    model = Model(inputs=modelInputs, outputs=output)\n    return model\n\ndef buildAndTrainModel(model, learningRate, batchSize, epochs, trainingData, validationData, testingData, trainingLabels, validationLabels, MODEL_NAME, isPrintModel=True):\n    \"\"\"Take the model and model parameters, build and train the model\"\"\"\n    \n    # Build and compile model\n    # To use other optimizers, refer to: https:\/\/keras.io\/optimizers\/\n    # Please do not change the loss function\n    \n    optimizer = optimizers.Adam(lr=learningRate)\n    model.compile(optimizer=optimizer,\n                  loss='binary_crossentropy', \n                  metrics=['accuracy'])\n    \n    if isPrintModel:\n        print(model.summary())\n        \n    # Train model\n    aucScore, precisionLow, precisionHigh, recallLow, recallHigh, fpr, tpr = None, None, None, None, None, None, None ##initializing best test metrics\n    confusionMatrix = None\n    bestValidationAccuracy = 0.0\n    bestTestModel = None\n\n\n    rocCurvesData = {}\n    for epoch in range(0, epochs):\n        model.fit(trainingData, trainingLabels,\n                        epochs=1,\n                        verbose=0,\n                        batch_size=batchSize,\n                        shuffle=False)\n        \n        # Evaluate model\n        trainLoss, trainAccuracy = model.evaluate(trainingData, trainingLabels, verbose=False)\n        valLoss, valAccuracy = model.evaluate(validationData, validationLabels, verbose=False)\n        \n        # Get metrics\n        predictionsProbs = model.predict(validationData)\n        predictions = [1 if value >= 0.5 else 0 for value in predictionsProbs]\n        predictionsProbs = [value for value in predictionsProbs]\n        aucScoreEpoch, _, _, _, _, fprEpoch, tprEpoch = getMetrics(validationLabels, predictions, predictionsProbs)\n        rocCurvesData[epoch] = {\"fpr\":fprEpoch, \"tpr\":tprEpoch, \"auc\":aucScoreEpoch}\n        \n        print('Epoch: %d \\nTraining: %.3f \\nValidation: %.3f\\n' % (epoch, trainAccuracy, valAccuracy))\n        if valAccuracy >= bestValidationAccuracy:\n            \n            bestValidationAccuracy = float(valAccuracy)\n            \n            ## get metrics\n            predictionsProbs = model.predict(validationData)\n            predictions = [1 if value >= 0.5 else 0 for value in predictionsProbs]\n            predictionsProbs = [value for value in predictionsProbs]\n            aucScore, precisionLow, precisionHigh, recallLow, recallHigh, fpr, tpr = getMetrics(validationLabels, predictions, predictionsProbs)\n            confusionMatrix = confusion_matrix(validationLabels, predictions)\n            \n            ##################################################### STORE PREDICTIONS FOR USE IN KAGGLE ##################################################\n            # You will find the following file if you right click on Jupyter logo on the top left and click \"Open Link in New Tab\". Next, go to 'AllModelsPredictions' folder.\n            print(\"Writing test predictions for kaggle for epoch %d\" % epoch)\n            predictionsProbsTest = model.predict(testingData)\n            predictionsFile = open(\"AllModelsPredictions\/\" + str(MODEL_NAME) + \"_predictions.txt\", \"w\")\n            predictionsFile.write(\"Id,Prediction\\n\")\n            for i in range(0, len(predictionsProbsTest)):\n                predictionsFile.write(str(i+1) + \",\" + str(predictionsProbsTest[i][0]) + \"\\n\")\n            predictionsFile.close()\n            \n            \n    # Results dictionary contains all the best test results. You can use these to build ROC plots, ROC Variance plots, and more.\n    resultsDictionary = {\"confusionMatrix\":confusionMatrix, \"bestTestAccuracy\":bestValidationAccuracy, \"bestAucScoreTest\":aucScore, \"bestPrecisionLowTest\":precisionLow, \"bestPrecisionHighTest\":precisionHigh,\n                        \"recallLowTest\":recallLow, \"recallHighTest\":recallHigh, \"FalsePositiveRateTest\":fpr, \"TruePositiveRateTest\":tpr}\n    \n    return resultsDictionary, rocCurvesData\n\ndef createROCCurve(resultsDictionary, rocCurveTitle, color):\n    \"\"\"Create ROC curve from resultsDictionary that is returned from the above function\"\"\"\n    \n    fpr = resultsDictionary[\"FalsePositiveRateTest\"]\n    tpr = resultsDictionary[\"TruePositiveRateTest\"]\n    plt.plot(fpr, tpr, linewidth=3.0, color=color)\n    \ndef createROCCurvesForAllEpochs(rocCurvesForAllEpochs, title, isVarianceCurve = True):\n    \"\"\"Create a ROC Variance Curve\"\"\"\n    \n    epochs = list(sorted(list(rocCurvesForAllEpochs.keys())))\n    \n    # Calculate min and max auc score from all epochs\n    maxAuc = max(list([item[\"auc\"] for item in rocCurvesForAllEpochs.values()]))\n    minAuc = min(list([item[\"auc\"] for item in rocCurvesForAllEpochs.values()]))\n    \n    for epoch in epochs:\n        fprs = rocCurvesForAllEpochs[epoch][\"fpr\"]\n        tprs = rocCurvesForAllEpochs[epoch][\"tpr\"]\n        auc = rocCurvesForAllEpochs[epoch][\"auc\"]\n                                \n        if auc != maxAuc and auc != minAuc and isVarianceCurve == True:\n            # alpha is the value of how transparent a line is\n            plt.plot(fprs, tprs, linewidth=3.0, alpha=0.2)\n        else:\n            plt.plot(fprs, tprs, linewidth=3.0, alpha=1.0)\n\n    FONT_SIZE = 14\n    plt.legend(epochs, fontsize=FONT_SIZE)\n    plt.xticks(fontsize=FONT_SIZE)\n    plt.yticks(fontsize=FONT_SIZE)\n    plt.xlabel(\"False Positive Rate\", fontsize=FONT_SIZE)\n    plt.ylabel(\"True Positive Rate\", fontsize=FONT_SIZE)\n    plt.title(\"ROC Variance Curve\", fontsize=FONT_SIZE + 2)\n    plt.show()","6e770c57":"# Import packages\nimport os\nimport time\nimport math\nimport copy\nimport keras\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom keras import *\nfrom keras import layers\nfrom gensim.models import *\nfrom sklearn.metrics import *\nfrom keras.models import Model\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing import *\nfrom collections import defaultdict\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Matplotlib parameters\nplt.style.use('seaborn-paper')\nplt.rcParams['figure.figsize'] = [12, 8] ## plot size\nplt.rcParams['axes.linewidth'] = 2.0 #set the value globally\n\n# Notebook style properties\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:95% !important; }<\/style>\"))\ndisplay(HTML(\"<style> .output_png { display: table-cell; text-align: center; vertical-align: middle; } <\/style>\"))\ndisplay(HTML(\"<style>.MathJax {font-size: 300%;}<\/style>\"))\n\n# Ignore warnings\ndef warn(*args, **kwargs):\n    pass\n\nwarnings.warn = warn\n\n# Set random initialization seeds for reproducible results\nfrom numpy.random import seed\nseed(1)","29457169":"start = time.time()\n\n# Assign feature arrays to new arrays (just for consistent naming)\ntrainFeatures, validationFeatures, testFeatures = trainStructured, validationStructured, testStructured\n\n# Create model\ndenseLayerStructured, modelInputStructured = createFeedForwardNeuralNetwork(trainFeatures, validationFeatures, testFeatures, 1, 32)\n\n# Attach output\nstructuredNNModel = attachOutputLayerToModel(denseLayerStructured, modelInputStructured)\n\n# Train model\nLEARNING_RATE = 0.0001\nBATCH_SIZE = 32\nEPOCHS = 120\nstructureNNResults, epochLevelResults = buildAndTrainModel(structuredNNModel, LEARNING_RATE, BATCH_SIZE, EPOCHS, trainFeatures, validationFeatures, testFeatures, trainingLabels, validationLabels, \"FFStructured\")\n\n# Results\nbestTestAUC = structureNNResults[\"bestAucScoreTest\"]\nbestTestAccuracy = structureNNResults[\"bestTestAccuracy\"]\nprint('Best Test Accuracy: %.3f \\nBest Test AUC: %.3f' % (bestTestAccuracy, bestTestAUC))\nprint(\"Model training and run time took %d seconds\" % (time.time() - start))\n\n########################################################################## Create ROC curve and confusion matrix ##############################################################################\n# These are only made in this example, you can copy paste the code in other cells to create the same stuff.\n# Create ROC curve\nplt.rcParams['figure.figsize'] = [12, 8] ## plot size\ncreateROCCurvesForAllEpochs(epochLevelResults, \"FeedforwardNeuralNetwork\", isVarianceCurve = True)\n\n# Confusion matrix\nplt.cla()\nplt.clf()\nconfusionMatrix = structureNNResults[\"confusionMatrix\"]\ndisplayConfusionMatrix(confusionMatrix)\nplt.show()\n","13585b5b":"start = time.time()\n\n# Assign feature arrays to new arrays (just for consistent naming)\ntrainFeatures, validationFeatures, testFeatures = trainFRN, validationFRN, testFRN\n\n# Create model\ndenseLayerFRN, modelInputFRN = createFeedForwardNeuralNetwork(trainFeatures, validationFeatures, testFeatures, 1, 32)\n\n# Attach output\nFRNNNModel = attachOutputLayerToModel(denseLayerFRN, modelInputFRN)\n\n# Train model\nLEARNING_RATE = 0.00001\nBATCH_SIZE = 32\nEPOCHS = 250\nFRNNNResults, FRNEpochLevelResults = buildAndTrainModel(FRNNNModel, LEARNING_RATE, BATCH_SIZE, EPOCHS, trainFeatures, validationFeatures, testFeatures, trainingLabels, validationLabels, \"FFFRN\")\n\n# Results\nbestTestAUC = FRNNNResults[\"bestAucScoreTest\"]\nbestTestAccuracy = FRNNNResults[\"bestTestAccuracy\"]\nprint('Best Test Accuracy: %.3f \\nBest Test AUC: %.3f' % (bestTestAccuracy, bestTestAUC))\nprint(\"Model training and run time took %d seconds\" % (time.time() - start))","388459d5":"start = time.time()\n\n# Create input feature arrays\nVOCABULARY_SIZE = 10000\ntrainFeatures, validationFeatures, testFeatures, WORDS_TO_KEEP = loadDataForWordsWithoutPreTrainedEmbeddings(trainText, validationText, testText, VOCABULARY_SIZE)\n\n# Build WordCNN model\nEMBEDDINGS_DIMENSIONALITY = 64\nFILTERS_SIZE = 64\nKERNEL_SIZE = 5\nNUM_CNN_LAYERS = 1\nwordCNNDenseLayer, wordCNNInput = createWordCNN(trainFeatures, validationFeatures, testFeatures, NUM_CNN_LAYERS, WORDS_TO_KEEP, EMBEDDINGS_DIMENSIONALITY, FILTERS_SIZE, KERNEL_SIZE, isPreTrainedEmbeddings=False)\n\n# Attach the output layer with the model\nwordCNNModel = attachOutputLayerToModel(wordCNNDenseLayer, wordCNNInput)\n\n# Train model\nLEARNING_RATE = 0.0001\nBATCH_SIZE = 32\nEPOCHS = 40\nwordCNNResults, wordCNNEpochLevelResults = buildAndTrainModel(wordCNNModel, LEARNING_RATE, BATCH_SIZE, EPOCHS, trainFeatures, validationFeatures, testFeatures, trainingLabels, validationLabels, \"WordCNN\")\n\n# Results\nbestTestAUC = wordCNNResults[\"bestAucScoreTest\"]\nbestTestAccuracy = wordCNNResults[\"bestTestAccuracy\"]\nprint('Best Test Accuracy: %.3f \\nBest Test AUC: %.3f' % (bestTestAccuracy, bestTestAUC))\nprint(\"Model training and run time took %d seconds\" % (time.time() - start))","501492ea":"start = time.time()\n\n\"\"\"\nYou might observe that models with pre-trained word embeddings do not perform very well. Therefore, you might want to use these models in conjunction with other models to get a good model. This happens because when we create\npre-trained word embeddings, they are not targeted for a certain task, they are genetic. In contrast, keras.layers.Embeddings creates embeddings based on a specific classification task that the model is trying to solve. Hence, \na mix of generic and targeted embeddings might work well with model fusion and ensembling.\n\"\"\"\n\n# Create input feature arrays\n##################################################### You can set the embedding path to REPRESENTATION EMBEDDINGS too which you can find in \"RepresentationEmbeddings\" folder ################################\nEMBEDDING_PATH = '..\/Case03 - Deep Learning Playbook\/crawl-300d-2M.pkl'\n\ntrainFeatures, validationFeatures, testFeatures, WORDS_TO_KEEP = loadDataForWordsWithPreTrainedEmbeddings(trainText, validationText, testText, EMBEDDING_PATH)\n\n# Build WordCNN model\nFILTERS_SIZE = 64\nEMBEDDINGS_DIMENSIONALITY = 64 # don't need this now\nKERNEL_SIZE = 5\nNUM_CNN_LAYERS = 1\nwordCNNDenseLayer, wordCNNInput = createWordCNN(trainFeatures, validationFeatures, testFeatures, NUM_CNN_LAYERS, WORDS_TO_KEEP, EMBEDDINGS_DIMENSIONALITY, FILTERS_SIZE, KERNEL_SIZE, isPreTrainedEmbeddings=True)\n\n# Attach the output layer with the model\nwordCNNModel = attachOutputLayerToModel(wordCNNDenseLayer, wordCNNInput)\n\n# Train model\nLEARNING_RATE = 0.0001\nBATCH_SIZE = 32\nEPOCHS = 80\nwordCNNResults, epochLevelResultsWordCNN = buildAndTrainModel(wordCNNModel, LEARNING_RATE, BATCH_SIZE, EPOCHS, trainFeatures, validationFeatures, testFeatures, trainingLabels, validationLabels, \"WordCNNWithoutPretrainedEmbeddings\")\n\n# Results\nbestTestAUC = wordCNNResults[\"bestAucScoreTest\"]\nbestTestAccuracy = wordCNNResults[\"bestTestAccuracy\"]\nprint('Best Test Accuracy: %.3f \\nBest Test AUC: %.3f' % (bestTestAccuracy, bestTestAUC))\nprint(\"Model training and run time took %d seconds\" % (time.time() - start))","69176119":"start = time.time()\n\n# Create input feature arrays\nVOCABULARY_SIZE = 10000\ntrainFeatures, validationFeatures, testFeatures, WORDS_TO_KEEP = loadDataForWordsWithoutPreTrainedEmbeddings(trainText, validationText, testText, VOCABULARY_SIZE)\n\n# Build WordLSTM model\nEMBEDDINGS_DIMENSIONALITY = 64\nLSTM_NODES_IN_LAYERS = 64\nNUM_LSTM_LAYERS = 1\n\n#trainFeatures, validationFeatures, testFeatures, numLstmLayers, vocabularyWords, embeddingsDimensionality, lstmNodes, isPreTrainedEmbeddings\nwordLSTMDenseLayer, wordLSTMInput = createWordLSTM(trainFeatures, validationFeatures, testFeatures, NUM_LSTM_LAYERS, WORDS_TO_KEEP, EMBEDDINGS_DIMENSIONALITY, LSTM_NODES_IN_LAYERS, bidirectional=True, isPreTrainedEmbeddings=False)\n\n# Attach the output layer with the model\nwordLSTMModel = attachOutputLayerToModel(wordLSTMDenseLayer, wordLSTMInput)\n\n# Train model\nLEARNING_RATE = 0.0001\nBATCH_SIZE = 16\nEPOCHS = 10\nwordLSTMResults, epochLevelWordLSTMResults = buildAndTrainModel(wordLSTMModel, LEARNING_RATE, BATCH_SIZE, EPOCHS, trainFeatures, validationFeatures, testFeatures, trainingLabels, validationLabels, \"wordLSTM\")\n\n# Results\nbestTestAUC = wordLSTMResults[\"bestAucScoreTest\"]\nbestTestAccuracy = wordLSTMResults[\"bestTestAccuracy\"]\nprint('Best Test Accuracy: %.3f \\nBest Test AUC: %.3f' % (bestTestAccuracy, bestTestAUC))\nprint(\"Model training and run time took %d seconds\" % (time.time() - start))","a60f6e41":"start = time.time()\n\n# Create input feature arrays\nEMBEDDING_PATH = '..\/Case03 - Deep Learning Playbook\/crawl-300d-2M.pkl'\ntrainFeatures, validationFeatures, testFeatures, WORDS_TO_KEEP = loadDataForWordsWithPreTrainedEmbeddings(trainText, validationText, testText, EMBEDDING_PATH)\n\n# Build WordLSTM model\nEMBEDDINGS_DIMENSIONALITY = 64\nLSTM_NODES_IN_LAYERS = 64\nNUM_LSTM_LAYERS = 1\n\n#trainFeatures, validationFeatures, testFeatures, numLstmLayers, vocabularyWords, embeddingsDimensionality, lstmNodes, isPreTrainedEmbeddings\nwordLSTMDenseLayer, wordLSTMInput = createWordLSTM(trainFeatures, validationFeatures, testFeatures, NUM_LSTM_LAYERS, WORDS_TO_KEEP, EMBEDDINGS_DIMENSIONALITY, LSTM_NODES_IN_LAYERS, bidirectional=True, isPreTrainedEmbeddings=True)\n\n# Attach the output layer with the model\nwordLSTMModel = attachOutputLayerToModel(wordLSTMDenseLayer, wordLSTMInput)\n\n# Train model\nLEARNING_RATE = 0.0001\nBATCH_SIZE = 32\nEPOCHS = 80\nwordLSTMResults, epochLevelWordLSTMResults = buildAndTrainModel(wordLSTMModel, LEARNING_RATE, BATCH_SIZE, EPOCHS, trainFeatures, validationFeatures, testFeatures, trainingLabels, validationLabels, \"wordLSTMWithPretrainedEmbds\")\n\n# Results\nbestTestAUC = wordLSTMResults[\"bestAucScoreTest\"]\nbestTestAccuracy = wordLSTMResults[\"bestTestAccuracy\"]\nprint('Best Test Accuracy: %.3f \\nBest Test AUC: %.3f' % (bestTestAccuracy, bestTestAUC))\nprint(\"Model training and run time took %d seconds\" % (time.time() - start))","ec3865f1":"start = time.time()\n\n# Create input feature arrays\nMAX_CHARACTERS_TO_KEEP = 500\nUSE_UPPER_CASE_CHARACTERS = False\ntrainFeatures, validationFeatures, testFeatures = loadDataForChars(trainText, validationText, testText, MAX_CHARACTERS_TO_KEEP, USE_UPPER_CASE_CHARACTERS)\n\n# Build CharCNN model\nEMBEDDINGS_DIMENSIONALITY = 64\nFILTERS_SIZE = 128\nKERNEL_SIZE = 5\nNUM_CNN_LAYERS = 1\ncharCNNDenseLayer, charCNNInput = createCharCNN(trainFeatures, validationFeatures, testFeatures, NUM_CNN_LAYERS, FILTERS_SIZE, KERNEL_SIZE)\n\n# Attach the output layer with the model\ncharCNNModel = attachOutputLayerToModel(charCNNDenseLayer, charCNNInput)\n\n# Train model\nLEARNING_RATE = 0.001\nBATCH_SIZE = 12\nEPOCHS = 15\ncharCNNResults, epochLevelCharCNNResults = buildAndTrainModel(charCNNModel, LEARNING_RATE, BATCH_SIZE, EPOCHS, trainFeatures, validationFeatures, testFeatures, trainingLabels, validationLabels, \"charCNN\")\n\n# Results\nbestTestAUC = charCNNResults[\"bestAucScoreTest\"]\nbestTestAccuracy = charCNNResults[\"bestTestAccuracy\"]\nprint('Best Test Accuracy: %.3f \\nBest Test AUC: %.3f' % (bestTestAccuracy, bestTestAUC))\nprint(\"Model training and run time took %d seconds\" % (time.time() - start))","31be73a6":"########################### STRUCTURED NN MODEL\n# Assign feature arrays to new arrays (just for consistent naming)\ntrainFeaturesStructured, validationFeaturesStructured, testFeaturesStructured = trainStructured, validationStructured, testStructured\n\n# Create model\ndenseLayerStructured, modelInputStructured = createFeedForwardNeuralNetwork(trainFeaturesStructured, validationFeaturesStructured, testFeaturesStructured, 1, 32)\n\n########################### FRN MODEL\n# Assign feature arrays to new arrays (just for consistent naming)\ntrainFeaturesGBS, validationFeaturesGBS, testFeaturesGBS = trainFRN, validationFRN, testFRN\n\n# Create model\ndenseLayerGBS, modelInputGBS = createFeedForwardNeuralNetwork(trainFeaturesGBS, validationFeaturesGBS, testFeaturesGBS, 1, 32)\n\n########################### WORD CNN MODEL\nVOCABULARY_SIZE = 5000\ntrainFeaturesWordCNN, validationFeaturesWordCNN, testFeaturesWordCNN, WORDS_TO_KEEP = loadDataForWordsWithoutPreTrainedEmbeddings(trainText, validationText, testText, VOCABULARY_SIZE)\n\n# Build WordCNN model\nEMBEDDINGS_DIMENSIONALITY = 32\nFILTERS_SIZE = 16\nKERNEL_SIZE = 5\nNUM_CNN_LAYERS = 1\nwordCNNDenseLayer, wordCNNInput = createWordCNN(trainFeaturesWordCNN, validationFeaturesWordCNN, testFeaturesWordCNN, NUM_CNN_LAYERS, WORDS_TO_KEEP, EMBEDDINGS_DIMENSIONALITY, FILTERS_SIZE, KERNEL_SIZE, isPreTrainedEmbeddings=False)\n\n########################## FUSION\n# Now that we have dense layers from different models, we can concatenate them together. This will make the model leverage all individual models during training. \n# We call the 'concatenateModels' function that takes as parameter a list of dense layers from different neural networks and concatenates them together.\nconcatenatedDenseLayer = concatenateModels([denseLayerStructured, denseLayerGBS, wordCNNDenseLayer])\n\n# Now, we need to attach the output layer to the concatenated model. Here, we pass the concatenated layer to the first argument and a list of inputs of concatenated models to the second argument. You should notice\n# the difference between attachOutputLayerToModel in separate models above vs the concatenated model below\nconcatenatedCNNModel = attachOutputLayerToModel(concatenatedDenseLayer, [modelInputStructured, modelInputGBS, wordCNNInput])\n\n######################## TRAINING\n# Train model\nLEARNING_RATE = 0.0001\nBATCH_SIZE = 32\nEPOCHS = 35\n\n# Below, instead of passing a single trainFeatures list, we now need to pass feature lists from both of our models that we have concatenated. Notice the difference in the line below between separate and fused models.\n# The order of the input is very important here. During concatenation, the order we use in the input list has to be the same that we use below during training while inputting the training, testing, and validation inputs.\nconcatenatedModelResults, epochLevelResultsCombinedModel = buildAndTrainModel(concatenatedCNNModel, LEARNING_RATE, BATCH_SIZE, EPOCHS, [trainFeaturesStructured, trainFeaturesGBS, trainFeaturesWordCNN], \n                                              [validationFeaturesStructured, validationFeaturesGBS, validationFeaturesWordCNN], [testFeaturesStructured, testFeaturesGBS, testFeaturesWordCNN], trainingLabels, validationLabels, \"combinedModel\")\n\n# Results\nbestTestAUC = concatenatedModelResults[\"bestAucScoreTest\"]\nbestTestAccuracy = concatenatedModelResults[\"bestTestAccuracy\"]\nprint('Best Test Accuracy: %.3f \\nBest Test AUC: %.3f' % (bestTestAccuracy, bestTestAUC))","d0bba20a":"######## If you go SavedModels folder, you'll observe that all the best model predictions are saved there. You can combine them to make an ensemble.","c867e156":"BertPredictions = pd.DataFrame()","f613a065":"import pandas as pd\nBertPredictions = pd.DataFrame(pd.read_csv(\"AllModelsPredictions\/XLNpredictions.csv\"))","50ecdabe":"BertPredictions","4a2b9b5d":"import pandas as pd\nmodelOnePredictions = pd.DataFrame(pd.read_csv(\"AllModelsPredictions\/FFFRN_predictions.txt\", \",\")).values.tolist()\nmodelThreePredictions = pd.DataFrame(pd.read_csv(\"AllModelsPredictions\/WordCNN_predictions.txt\", \",\")).values.tolist()\nmodelFourPredictions = pd.DataFrame(pd.read_csv(\"AllModelsPredictions\/WordCNNWithoutPretrainedEmbeddings_predictions.txt\", \",\")).values.tolist()\nmodelFivePredictions  = pd.DataFrame(pd.read_csv(\"AllModelsPredictions\/charCNN_predictions.txt\", \",\")).values.tolist()\nmodelSixPredictions = pd.DataFrame(pd.read_csv(\"AllModelsPredictions\/wordLSTM_predictions.txt\", \",\")).values.tolist() \nmodelSevenPredictions = pd.DataFrame(pd.read_csv(\"AllModelsPredictions\/WordCNN_predictions.txt\", \",\")).values.tolist()\nmodelEightPredictions= pd.DataFrame(pd.read_csv(\"AllModelsPredictions\/combinedModel_predictions.txt\", \",\")).values.tolist()\nmodelTwoPredictions = pd.DataFrame(pd.read_csv(\"AllModelsPredictions\/ELECTRApredictions2.csv\", \",\")).values.tolist()\nXLNPredictions = pd.DataFrame(pd.read_csv(\"AllModelsPredictions\/XLN Fixed.csv\")).values.tolist()\n\n#FINAL LEADERBOARD WEIGHTS\nweightOne = .083\nweightTwo =  .166\nweightThree = .166 \nweightFour =  .083\nweightFive = .083\nweightSix = .083\nweightSeven = .083\nweightEight =  .083\nXLNWeight = .166\n\n\n#0.1875\n#0.1875\n#0.046875\u202c\n\n#weightOne = 0.0625\n#weightTwo = 0.0625\n#weightThree = 0.1875 \n#weightFour = 0.1875 \n#weightFive = 0.0625\n#weightSix = 0.0625\n#weightSeven = 0.1875\n#weightEight = 0.1875\n\n#weightOne = (.05)\n#weightTwo = (.05)\n#weightThree = (0.05)\u202c\n#weightFour = (.1666)\n#weightFive = (.05)\u202c\n#weightSix = (.05)\n#weightSeven = (.1666)\n#weightEight = (.1666)\n#weightBert =(.25)\n\n## Leaderboard Weights\n\n#weightOne = .05\n#weightTwo = .05\n#weightThree = .05 \n#weightFour = .083\n#weightFive = .05\n#weightSix = .05\n#weightSeven = .083\n#weightEight = .083\n#weightBert = .5\n\n#weightOne = 0.03125 \n#weightTwo = 0.03125\n#weightThree = 0.03125\n#weightFour = 0.03125\n#weightFive = 0.03125\n#weightSix = 0.03125\u202c \n#weightSeven = 0.03125\u202c\n#weightEight = 0.03125\n#weightBert = .75\n\n\n#weightOne = 0.066\n#weightTwo = 0.066\n#weightThree = 0.11\n#weightFour =  0.11\n#weightFive = 0.066\n#weightSix = 0.11\n#weightSeven = 0.066\n#weightEight = 0.066\n#weightBert = .33\n\n\n\n\n## 1 is 0.0625\n## 2 is 0.125\n## 3 is 0.1875\n## 4 is 0.25\n\"\"\"To add more models, just add a few more lines and use them during average. For instance, adding a third model would require 2 more lines above. modelThreePredictions and weightThree. Next, you only need to \nuse the predictions from that during averaging below. We will have 'average = (weightOne * modelOnePredictions[i][1]) + (weightTwo * modelTwoPredictions[i][1]) + (weightThree * modelThreePredictions[i][1])' \"\"\"\n\n\naveragePredictions = []\n\nfor i in range(0, len(modelOnePredictions)):\n    average = (weightOne * modelOnePredictions[i][1]) + (weightTwo * modelTwoPredictions[i][1]) + (weightThree * modelThreePredictions[i][1]) + (weightFour * modelFourPredictions[i][1]) + (weightFive * modelFivePredictions[i][1]) + (weightSix * modelSixPredictions[i][1]) + (weightSeven * modelSevenPredictions[i][1]) + (weightEight * modelEightPredictions[i][1]) + (XLNWeight * XLNPredictions[i][1])\n    averagePredictions.append(average)\n    \naverageHardPredictions = [1 if item >= 0.5 else 0 for item in averagePredictions]\n\n# Create file for kaggle submission\nkaggleFile = open(\"AllModelsPredictions\/ensemble_predictions.txt\",\"w\")\nkaggleFile.write(\"Id,Prediction\\n\")\nfor i in range(0, len(averagePredictions)):\n    kaggleFile.write(str(i+1) + \",\" + str(averagePredictions[i]) + \"\\n\")\nkaggleFile.close()","b38714e8":"import pandas as pd\nmodelTwoPredictions = pd.DataFrame(pd.read_csv(\"AllModelsPredictions\/ELECTRApredictions2.csv\"))\nmodelTwoPredictions.index += 1 \nmodelTwoPredictions['Id'] =  modelTwoPredictions.index\nmodelTwoPredictions.columns = ['Prediction','Id']\nmodelTwoPredictions = modelTwoPredictions[['Id','Prediction',]]\nmodelTwoPredictions.to_csv(\"AllModelsPredictions\/XLN Fixed.csv\", index=False)","09418fe2":"import pandas as pd\nmodelTwoPredictions = pd.DataFrame(pd.read_csv(\"AllModelsPredictions\/XLNpredictions (2).csv\"))\nmodelTwoPredictions.index += 1 \nmodelTwoPredictions['Id'] =  modelTwoPredictions.index\nmodelTwoPredictions.columns = ['Prediction','Id']\nmodelTwoPredictions = modelTwoPredictions[['Id','Prediction',]]\nmodelTwoPredictions.to_csv(\"AllModelsPredictions\/Electra Fixed.csv\", index=False)","d446de4f":"modelTwoPredictions","83e2217f":"modelTwoPredictions.values.tolist()","9b708349":"kaggleFile.write(\"Id,Prediction\\n\")\nfor i in range(0, len(modelTwoPredictions)):\n    kaggleFile.write(str(i+1) + \",\" + str(modelTwoPredictions[i]) + \"\\n\")\nkaggleFile.close()","2fa7bfcf":"modelTwoPredictions","94081eb2":"modelOnePredictions = pd.DataFrame(pd.read_csv(\"AllModelsPredictions\/FFStructured_predictions.txt\", \",\")).values.tolist()","e7007dde":"modelOnePredictions","a4f18176":"# Create file for kaggle submission\nkaggleFile = open(\"AllModelsPredictions\/ensemble_predictions.txt\",\"w\")\nkaggleFile.write(\"Id,Prediction\\n\")\nfor i in range(0, len(modelTwoPredictions)):\n    kaggleFile.write(str(i+1) + \",\" + str(modelTwoPredictions[i]) + \"\\n\")\nkaggleFile.close()\n","8e81487a":"## 5.4 Feed Forward Neural Network","20bdf423":"# 6. Helper Functions for Deep Learning Models","4fb18246":"![image.png](attachment:image.png)","0e7f4933":"## Add Manual Corrections Here","e4a06543":"## 7.7 Char CNN Model","7d37a0b1":"## 7.6 Word LSTM with Pre-trained Embeddings","420cae69":"![image.png](attachment:image.png)","ee5b453f":"## 5.3 Character Convolutional Neural Network","38c3da6b":"# 4. Data Preparation for Deep Learning Models\nDifferent deep learning models take different forms of data as input. For instance, character CNN takes as input one-hot encoded character representations while word CNN uses word representations. The functions below will make your job easier in loading data for different deep learning models.\n\n## 4.1 Loading Data for Word Based Models","2198e2b7":"## 2.2 Loading FRN TopN Features Data","090b5182":"## 7.4 Word CNN Model with Pre-trained Embeddings","31c74aa6":"## 8.2 Ensembling Models\nIn class, we learned the difference between ensembling and fusion. Ensembling usually works very well if you can find the right weights for each model (always start with equal weights, for example 0.5 if ensembling two models). Below is an example of ensembling multiple (two) models.","a3d9d269":"## 7.3 WordCNN Model Without Pre-trained Embeddings","e449f4f4":"## Loading Embeddings","06ee7784":"# 1. Original Helper Functions","f71b7685":"# 7. Running Separate Models\n## 7.1 Feedforward Neural Network on Structured Data","ae020d41":"## 2.3 Loading Structured Data","8aa5c891":"![image.png](attachment:image.png)","177b47b3":"## 7.2 Feedforward Neural Network on FRN Top Features","09c99eb7":"# 8. Combining Models\n## 8.1 Fusing Multiple Deep Learning Models\nMost of the time, we do not train a single neural network. Given the power of neural networks, it is usually better to combine a bunch of different models and fuse\/ensemble them together to get better results. In this notebook, you can fuse as many models as you like. Below, we show an example of fusing multiple models.","3da6ffb7":"![image.png](attachment:image.png)","7af0a5e9":"## Transformer Evaluation ","31ce5b7c":"## 7.5 Word LSTM Model without Pre-trained Embeddings","a6b075d4":"## Check NGRAMS","9794a540":"## 4.2 Loading data for Character based Models","81dd662a":"![image.png](attachment:image.png)","dbdee167":"## Check Embeddings Coverage","2a425bbb":"# 5. Deep Learning Models\n## 5.1 Word Convolutional Neural Network","fb787e50":"## 5.2 Word Recurrent Neural Network","7ebe0039":"![image.png](attachment:image.png)","9bef62fc":"# 2. Data Loading\n## 2.1 Loading Text Data\nWe have three different types of data inputs for our models.\n1. Text\n2. Structured Variables\n3. FRN"}}