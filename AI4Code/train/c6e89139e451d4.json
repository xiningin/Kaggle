{"cell_type":{"a4769ad8":"code","c54aeab3":"code","8164913f":"code","e38c4fd1":"code","d83fc4c3":"code","746e5aae":"code","9caa3d0a":"code","9a7cc68b":"code","17560dc6":"code","fd95efea":"code","16404f0e":"code","e241b42e":"code","50afa143":"code","fd878108":"code","58dda492":"code","4dd813d7":"code","26801bb3":"code","4432d705":"code","61a19743":"code","eedf6ac1":"code","9cda66fd":"code","2b1bf637":"code","8d047a29":"code","6c30d357":"code","e647794a":"code","c2db9c30":"code","75ac298b":"code","3270f49b":"code","f2480590":"code","bc1ebeaa":"code","5be546e3":"code","e13f8071":"code","63904d3e":"code","65491873":"code","f2da2018":"code","6a49aa02":"code","ddf2833a":"code","4b4eae68":"code","321cda2f":"code","c78ed66f":"code","2eab82d1":"code","7d729957":"code","5150b5c3":"code","ceeb1406":"code","94bf31ce":"code","ac4a08b4":"markdown","13a3d7c7":"markdown","292cfc0a":"markdown","a09b007c":"markdown"},"source":{"a4769ad8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c54aeab3":"from nltk.corpus import stopwords  \nfrom nltk.tokenize import word_tokenize \nfrom nltk.stem import WordNetLemmatizer \nimport numpy as np \nimport string \nimport gensim\nfrom gensim.models import Word2Vec\nimport re\nimport string","8164913f":"f=open(\"..\/input\/nlp-tutorial\/nlpcorpus.txt\",\"r\")\n\nt=f.read()","e38c4fd1":"#lowercase\n\nt=t.lower()","d83fc4c3":"#remove stopwords\nstop_words = set(stopwords.words('english'))  \n  \nword_tokens = word_tokenize(t)  \n  \nfiltered_sentence = [w for w in word_tokens if not w in stop_words]  \n  \nfiltered_text = \"\"\n  \nfor w in word_tokens:  \n    if w not in stop_words:  \n        filtered_text+=w \n        filtered_text+=\" \"","746e5aae":"def remove_numbers_char(text):\n    # define the pattern to keep\n    pattern = r'[^a-zA-z.,!?\/:;\\\"\\'\\s]' \n    return re.sub(pattern, '', text)","9caa3d0a":"filtered_text=remove_numbers_char(filtered_text)","9a7cc68b":"def remove_punctuation(text):\n    text = ''.join([c for c in text if c not in string.punctuation])\n    return text","17560dc6":"filtered_text=remove_punctuation(filtered_text)","fd95efea":"filtered_text","16404f0e":"#Lemmetization\n\nlemmatizer = WordNetLemmatizer() \n\nlist2 = word_tokenize(filtered_text)\n\nlemmatized_string = ' '.join([lemmatizer.lemmatize(words) for words in list2]) ","e241b42e":"lemmatized_string","50afa143":"l=lemmatized_string.split(\".\")","fd878108":"l","58dda492":"word_tokens = word_tokenize(lemmatized_string)  \n","4dd813d7":"def softmax(x): \n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x)) \n    return e_x \/ e_x.sum() ","26801bb3":"class word2vec(object): \n    def __init__(self): \n        self.N = 10\n        self.X_train = [] \n        self.y_train = [] \n        self.window_size = 2\n        self.alpha = 0.001\n        self.words = [] \n        self.word_index = {} \n   \n    def initialize(self,V,data): \n        self.V = V \n        self.W = np.random.uniform(-0.8, 0.8, (self.V, self.N)) \n        self.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V)) \n           \n        self.words = data \n        for i in range(len(data)): \n            self.word_index[data[i]] = i \n   \n       \n    def feed_forward(self,X): \n        self.h = np.dot(self.W.T,X).reshape(self.N,1) \n        self.u = np.dot(self.W1.T,self.h) \n        #print(self.u) \n        self.y = softmax(self.u)   \n        return self.y \n           \n    def backpropagate(self,x,t): \n        e = self.y - np.asarray(t).reshape(self.V,1) \n        \n        dLdW1 = np.dot(self.h,e.T) \n        \n        X = np.array(x).reshape(self.V,1) \n        \n        dLdW = np.dot(X, np.dot(self.W1,e).T) \n        \n        self.W1 = self.W1 - self.alpha*dLdW1 \n        \n        self.W = self.W - self.alpha*dLdW \n           \n    def train(self,epochs): \n        for x in range(1,epochs):         \n            self.loss = 0\n            for j in range(len(self.X_train)): \n                self.feed_forward(self.X_train[j]) \n                self.backpropagate(self.X_train[j],self.y_train[j]) \n                C = 0\n                for m in range(self.V): \n                    if(self.y_train[j][m]): \n                        self.loss += -1*self.u[m][0] \n                        C += 1\n                self.loss += C*np.log(np.sum(np.exp(self.u))) \n            print(\"epoch \",x, \" loss = \",self.loss) \n            self.alpha *= 1\/( (1+self.alpha*x) ) \n              \n    def predict(self,word,number_of_predictions): \n        if word in self.words: \n            index = self.word_index[word] \n            X = [0 for i in range(self.V)] \n            X[index] = 1\n            prediction = self.feed_forward(X) \n            output = {} \n            for i in range(self.V): \n                output[prediction[i][0]] = i \n               \n            top_context_words = [] \n            for k in sorted(output,reverse=True): \n                top_context_words.append(self.words[output[k]]) \n                if(len(top_context_words)>=number_of_predictions): \n                    break\n       \n            return top_context_words \n        else: \n            print(\"Word not found in dicitonary\") ","4432d705":"def prepare_data_for_training(sentences,w2v): \n    data = {} \n     \n    for word in sentences.split(\" \"): \n        if word not in data: \n            data[word] = 1\n        else: \n            data[word] += 1\n    \n    \n    V = len(data) \n    data = sorted(list(data.keys())) \n    \n    print(data)\n    \n    \n    vocab = {} \n    for i in range(len(data)): \n        vocab[data[i]] = i \n       \n    #for i in range(len(words)): \n    \n    sentences=sentences.split(\" \")\n    for i in range(len(sentences)): \n        center_word = [0 for x in range(V)] \n        center_word[vocab[sentences[i]]] = 1\n        context = [0 for x in range(V)] \n              \n        for j in range(i-w2v.window_size,i+w2v.window_size): \n            if i!=j and j>=0 and j<len(sentences): \n                context[vocab[sentences[j]]] += 1\n        w2v.X_train.append(center_word) \n        w2v.y_train.append(context) \n    w2v.initialize(V,data) \n   \n    \n    return w2v.X_train,w2v.y_train    \n","61a19743":"w2v = word2vec() \n  \nprepare_data_for_training(lemmatized_string,w2v) \nw2v.train(1)  \n\nprint(w2v.words)\n","eedf6ac1":"print(w2v.predict(\"jaipur\",3))     ","9cda66fd":"vocab = set(lemmatized_string.split(\" \"))\n","2b1bf637":"vocab","8d047a29":"vocab_size=len(vocab)","6c30d357":"embed_dim = 10\ncontext_size = 2","e647794a":"word_to_ix = {word: i for i, word in enumerate(vocab)}\nix_to_word = {i: word for i, word in enumerate(vocab)}","c2db9c30":"data = []\nl=lemmatized_string.split(\" \")\nfor i in range(2, len(l) - 2):\n    context = [l[i - 2], l[i - 1], l[i + 1], l[i + 2]]\n    target = l[i]\n    data.append((context, target))\nprint(data[:5])\n","75ac298b":"embeddings =  np.random.random_sample((vocab_size, embed_dim))","3270f49b":"def linear(m, theta):\n    w = theta\n    return m.dot(w)","f2480590":"def log_softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return np.log(e_x \/ e_x.sum())\n","bc1ebeaa":"def NLLLoss(logs, targets):\n    out = logs[range(len(targets)), targets]\n    return -out.sum()\/len(out)\n","5be546e3":"def log_softmax_crossentropy_with_logits(logits,target):\n\n    out = np.zeros_like(logits)\n    out[np.arange(len(logits)),target] = 1\n    \n    softmax = np.exp(logits) \/ np.exp(logits).sum(axis=-1,keepdims=True)\n    \n    return (- out + softmax) \/ logits.shape[0]","e13f8071":"def forward(context_idxs, theta):\n    m = embeddings[context_idxs].reshape(1, -1)\n    n = linear(m, theta)\n    o = log_softmax(n)\n    \n    return m, n, o","63904d3e":"def backward(preds, theta, target_idxs):\n    m, n, o = preds\n    \n    dlog = log_softmax_crossentropy_with_logits(n, target_idxs)\n    dw = m.T.dot(dlog)\n    \n    return dw","65491873":"def optimize(theta, grad, lr=0.03):\n    theta -= grad * lr\n    return theta","f2da2018":"theta = np.random.uniform(-1, 1, (2 * context_size * embed_dim, vocab_size))","6a49aa02":"epoch_losses = {}\n\nfor epoch in range(80):\n\n    losses =  []\n\n    for context, target in data:\n        context_idxs = np.array([word_to_ix[w] for w in context])\n        preds = forward(context_idxs, theta)\n\n        target_idxs = np.array([word_to_ix[target]])\n        loss = NLLLoss(preds[-1], target_idxs)\n\n        losses.append(loss)\n\n        grad = backward(preds, theta, target_idxs)\n        theta = optimize(theta, grad, lr=0.03)\n        \n     \n    epoch_losses[epoch] = losses","ddf2833a":"def predict(words):\n    context_idxs = np.array([word_to_ix[w] for w in words])\n    preds = forward(context_idxs, theta)\n    word = ix_to_word[np.argmax(preds[-1])]\n    \n    return word","4b4eae68":"predict(['jaipur','capital','city','indian'])","321cda2f":"import pickle\nfrom gensim.test.utils import datapath, get_tmpfile\nfrom gensim.models import KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec","c78ed66f":"\nword2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\nglove2word2vec(\"..\/input\/glove6b\/glove.6B.100d.txt\", word2vec_glove_file)\n\nmodel = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n\nfilename = 'glove2word2vec_model.sav'\npickle.dump(model, open(filename, 'wb'))","2eab82d1":"filename = 'glove2word2vec_model.sav'\n\nmodel = pickle.load(open(filename, 'rb'))\n\ndef append_list(sim_words, words):\n    \n    list_of_words = []\n    \n    for i in range(len(sim_words)):\n        \n        sim_words_list = list(sim_words[i])\n        sim_words_list.append(words)\n        sim_words_tuple = tuple(sim_words_list)\n        list_of_words.append(sim_words_tuple)\n        \n    return list_of_words\n\ninput_word = lemmatized_string\nuser_input = [x.strip() for x in input_word.split(' ')]\nresult_word = []\n    \nfor words in user_input:\n    \n        try:\n            sim_words = model.most_similar(words, topn = 5)\n            sim_words = append_list(sim_words, words)\n            \n            result_word.extend(sim_words)\n            \n        except:\n            continue\n    \nsimilar_word = [word[0] for word in result_word]\nsimilarity = [word[1] for word in result_word] \nsimilar_word.extend(user_input)\nlabels = [word[2] for word in result_word]\nlabel_dict = dict([(y,x+1) for x,y in enumerate(set(labels))])\ncolor_map = [label_dict[x] for x in labels]","7d729957":"\nimport plotly\nimport numpy as np\nimport plotly.graph_objs as go\nfrom sklearn.manifold import TSNE\n\ndef display_tsne_scatterplot_3D(model, user_input=None, words=None, label=None, color_map=None, perplexity = 0, learning_rate = 0, iteration = 0, topn=5, sample=10):\n\n    if words == None:\n        if sample > 0:\n            words = np.random.choice(list(model.vocab.keys()), sample)\n        else:\n            words = [ word for word in model.vocab ]\n    \n    \n    \n    word_vectors = []\n    \n    for w in words:\n        try:\n            word_vectors.append(model[w])\n        except:\n            continue\n            \n    print(word_vectors)\n    three_dim = TSNE(n_components = 3, random_state=0, perplexity = perplexity, learning_rate = learning_rate, n_iter = iteration).fit_transform(word_vectors)[:,:3]\n\n\n    # For 2D, change the three_dim variable into something like two_dim like the following:\n    # two_dim = TSNE(n_components = 2, random_state=0, perplexity = perplexity, learning_rate = learning_rate, n_iter = iteration).fit_transform(word_vectors)[:,:2]\n\n    data = []\n\n\n    count = 0\n    for i in range (len(user_input)):\n\n                trace = go.Scatter3d(\n                    x = three_dim[count:count+topn,0], \n                    y = three_dim[count:count+topn,1],  \n                    z = three_dim[count:count+topn,2],\n                    text = words[count:count+topn],\n                    name = user_input[i],\n                    textposition = \"top center\",\n                    textfont_size = 20,\n                    mode = 'markers+text',\n                    marker = {\n                        'size': 10,\n                        'opacity': 0.8,\n                        'color': 2\n                    }\n       \n                )\n                \n                # For 2D, instead of using go.Scatter3d, we need to use go.Scatter and delete the z variable. Also, instead of using\n                # variable three_dim, use the variable that we have declared earlier (e.g two_dim)\n            \n                data.append(trace)\n                count = count+topn\n\n    trace_input = go.Scatter3d(\n                    x = three_dim[count:,0], \n                    y = three_dim[count:,1],  \n                    z = three_dim[count:,2],\n                    text = words[count:],\n                    name = 'input words',\n                    textposition = \"top center\",\n                    textfont_size = 20,\n                    mode = 'markers+text',\n                    marker = {\n                        'size': 10,\n                        'opacity': 1,\n                        'color': 'black'\n                    }\n                    )\n\n    # For 2D, instead of using go.Scatter3d, we need to use go.Scatter and delete the z variable.  Also, instead of using\n    # variable three_dim, use the variable that we have declared earlier (e.g two_dim)\n            \n    data.append(trace_input)\n    \n# Configure the layout\n\n    layout = go.Layout(\n        margin = {'l': 0, 'r': 0, 'b': 0, 't': 0},\n        showlegend=True,\n        legend=dict(\n        x=1,\n        y=0.5,\n        font=dict(\n            family=\"Courier New\",\n            size=25,\n            color=\"black\"\n        )),\n        font = dict(\n            family = \" Courier New \",\n            size = 15),\n        autosize = False,\n        width = 1000,\n        height = 1000\n        )\n\n\n    plot_figure = go.Figure(data = data, layout = layout)\n    plot_figure.show()\n    \n","5150b5c3":"display_tsne_scatterplot_3D(model, user_input, similar_word, labels, color_map, 5, 500, 10000)","ceeb1406":"import plotly\nimport numpy as np\nimport plotly.graph_objs as go\nfrom sklearn.manifold import TSNE\n\ndef display_tsne_scatterplot_2D(model, user_input=None, words=None, label=None, color_map=None, perplexity = 0, learning_rate = 0, iteration = 0, topn=5, sample=10):\n\n    if words == None:\n        if sample > 0:\n            words = np.random.choice(list(model.vocab.keys()), sample)\n        else:\n            words = [ word for word in model.vocab ]\n    \n    word_vectors = np.array([model[w] for w in words])\n    \n\n    # For 2D, change the three_dim variable into something like two_dim like the following:\n    word_vectors = []\n    \n    for w in words:\n        try:\n            word_vectors.append(model[w])\n        except:\n            continue\n            \n    print(word_vectors)\n    \n    data = []\n\n\n    count = 0\n    for i in range (len(user_input)):\n\n                trace = go.Scatter3d(\n                    x = two_dim[count:count+topn,0], \n                    y = two_dim[count:count+topn,1],  \n                    text = words[count:count+topn],\n                    name = user_input[i],\n                    textposition = \"top center\",\n                    textfont_size = 20,\n                    mode = 'markers+text',\n                    marker = {\n                        'size': 10,\n                        'opacity': 0.8,\n                        'color': 2\n                    }\n       \n                )\n                \n                # For 2D, instead of using go.Scatter3d, we need to use go.Scatter and delete the z variable. Also, instead of using\n                # variable three_dim, use the variable that we have declared earlier (e.g two_dim)\n            \n                data.append(trace)\n                count = count+topn\n\n    trace_input = go.Scatter(\n                    x = two_dim[count:,0], \n                    y = two_dim[count:,1],  \n                    \n                    text = words[count:],\n                    name = 'input words',\n                    textposition = \"top center\",\n                    textfont_size = 20,\n                    mode = 'markers+text',\n                    marker = {\n                        'size': 10,\n                        'opacity': 1,\n                        'color': 'black'\n                    }\n                    )\n\n    # For 2D, instead of using go.Scatter3d, we need to use go.Scatter and delete the z variable.  Also, instead of using\n    # variable three_dim, use the variable that we have declared earlier (e.g two_dim)\n            \n    data.append(trace_input)\n    \n# Configure the layout\n\n    layout = go.Layout(\n        margin = {'l': 0, 'r': 0, 'b': 0, 't': 0},\n        showlegend=True,\n        legend=dict(\n        x=1,\n        y=0.5,\n        font=dict(\n            family=\"Courier New\",\n            size=25,\n            color=\"black\"\n        )),\n        font = dict(\n            family = \" Courier New \",\n            size = 15),\n        autosize = False,\n        width = 1000,\n        height = 1000\n        )\n\n\n    plot_figure = go.Figure(data = data, layout = layout)\n    plot_figure.show()\n    \n","94bf31ce":"display_tsne_scatterplot_2D(model, user_input, similar_word, labels, color_map, 5, 500, 10000)","ac4a08b4":"# 2d and 3d word visualization ","13a3d7c7":"# Text Pre-processing","292cfc0a":"## CBOW","a09b007c":"## Skip Gram Model"}}