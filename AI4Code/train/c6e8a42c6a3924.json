{"cell_type":{"912abb5c":"code","95cb3bb6":"code","a83a514b":"code","3d764c90":"code","cc043016":"code","b3f0d2b5":"code","c424b164":"code","cf6c8b5d":"code","9a59cdef":"code","4364e19b":"code","21b37d68":"code","75d210fe":"code","2fd6dd63":"code","22b13745":"code","b5eb5e37":"code","f26611ac":"code","4ea1c599":"code","67d9d34d":"code","399c5521":"code","89c13010":"code","278144ac":"code","63cba1bd":"code","f05261f3":"code","942ba383":"code","4d2dc4a7":"code","cbe5eaeb":"code","45bc2dc6":"code","dbca5174":"code","45b8c68b":"code","6535d913":"code","b373f7f0":"code","5e4b317f":"code","68bcb152":"code","e75961bb":"code","cbcd8f9a":"code","d9cffb1e":"code","0af0fbfb":"code","d656b1f3":"code","a2bceba7":"code","dc0a31d5":"code","161f868b":"code","426b7cc8":"code","c26ae9b8":"code","5250bb43":"code","6d122613":"code","3381ded3":"code","e346c699":"code","11dd271e":"code","cea2abe7":"code","a8bb3266":"code","3a69c9f4":"code","5ef0b5e5":"code","e542c765":"code","a1d9b0f3":"code","f78d6414":"code","08447039":"code","4108953a":"code","f414851a":"code","14f96837":"code","3683454f":"code","868fcc12":"code","d7e14682":"code","0c699eb4":"code","9c31c2c0":"code","858b372b":"code","696a4a19":"code","2c25493f":"code","daf4d21f":"code","1e426f32":"markdown","57349d6b":"markdown","75ed220e":"markdown","2fb4463c":"markdown","4bc82319":"markdown","e8aa64b4":"markdown","a475f137":"markdown","0346f9f1":"markdown","cc9e1814":"markdown","bfec0ce2":"markdown","08a7a83f":"markdown","89b4191b":"markdown","8770bf36":"markdown","023d93f8":"markdown","541f98a4":"markdown","fa2413b1":"markdown","c5480095":"markdown","f3f8aef0":"markdown","962fc1aa":"markdown","4409b12e":"markdown","532e7335":"markdown","3c1c3736":"markdown","4fa160d9":"markdown","a3a54c24":"markdown","16114269":"markdown","1656b6e7":"markdown","84940ab7":"markdown","047d862d":"markdown","be034073":"markdown","584cc855":"markdown","49e9e383":"markdown","cd4f3c95":"markdown","e9028cb3":"markdown","1b49f253":"markdown","1c940bd3":"markdown","2550c52c":"markdown","08a2e384":"markdown","1e632230":"markdown","d46c0e02":"markdown","b89f1a86":"markdown","b69480f5":"markdown","dea6f384":"markdown","5c82fb9d":"markdown","15198f3a":"markdown"},"source":{"912abb5c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","95cb3bb6":"train, test = pd.read_csv('..\/input\/titanic\/train.csv'), pd.read_csv('..\/input\/titanic\/test.csv')","a83a514b":"train.head()","3d764c90":"train.info()","cc043016":"train['Sex'].value_counts()","b3f0d2b5":"train.describe()","c424b164":"train['Pclass'].value_counts()","cf6c8b5d":"train.hist(bins=50, figsize=(15, 10))","9a59cdef":"train_c = train.copy()","4364e19b":"def cat_survival_rate(column_name):\n    \"\"\"\n    Counting the people survived in each class. And calculating the survial ratio for each. \n    \"\"\"\n    cat_survived = train_c.groupby(column_name).agg({'PassengerId':'count', 'Survived':'sum'}\n                                                   ).reset_index()\n    cat_survived['survival_rate'] = cat_survived.Survived\/cat_survived.PassengerId\n    return cat_survived.rename(columns={'PassengerId':'PassengerCount'})","21b37d68":"pclass_survival_ratio = cat_survival_rate('Pclass')\npclass_survival_ratio","75d210fe":"def plot_cat_survived(df, colum_name):\n    fig = plt.figure(figsize=(8, 5))\n    plt.bar(df[colum_name]-.2, df.PassengerCount, width=.3,label='Passengers Count')\n    plt.bar(df[colum_name]+.1, df.Survived, width=.3, label='Survived')\n    plt.legend()\n    plt.show()\nplot_cat_survived(pclass_survival_ratio, 'Pclass')","2fd6dd63":"def plot_cat_survival_ratio(df, column_name):\n    f, ax = plt.subplots(figsize=(8, 5))\n    ax.plot(df[column_name], df.survival_rate,label='Survial Ratio', marker='s')\n#     ax.set_xticks([1, 2, 3])\n#     ax.set_xticklabels(['class 1', 'class 2', 'class 3'])\n    plt.legend()\n    plt.show()\nplot_cat_survival_ratio(pclass_survival_ratio, 'Pclass')","22b13745":"parch_survival_ratio = cat_survival_rate('Parch')\nparch_survival_ratio","b5eb5e37":"plot_cat_survival_ratio(parch_survival_ratio, 'Parch')","f26611ac":"sibsp_survival_ratio = cat_survival_rate('SibSp')\nsibsp_survival_ratio","4ea1c599":"plot_cat_survival_ratio(sibsp_survival_ratio, 'SibSp')","67d9d34d":"train_c.groupby('Pclass').sum()","399c5521":"corr_matrix = train_c.corr()","89c13010":"corr_matrix","278144ac":"corr_matrix['Survived'].sort_values(ascending=False)","63cba1bd":"train_c['Parch_SibSp'] = train_c['SibSp'] + train_c['Parch']\ntrain_c['age_parch'] = train_c['Parch']\/train_c['Age']\ntrain_c['age_Sibsp'] = train_c['Age']*train_c['SibSp']\n","f05261f3":"corr_matrix = train_c.corr()\ncorr_matrix['Survived'].sort_values(ascending=False)","942ba383":"train_c = train.drop('Survived', axis=1)\ntrain_c_labels = train['Survived'].copy()","4d2dc4a7":"train_c.info()","cbe5eaeb":"train_c.Age.hist()\nplt.show()","45bc2dc6":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='mean')","dbca5174":"num_attrs = ['Age', 'Fare', 'Pclass', 'SibSp', 'Parch']\nimputer.fit(train_c[num_attrs])  \ntrain_num = imputer.transform(train_c[num_attrs])","45b8c68b":"# converting the output back to a dataframe. \ntrain_num = pd.DataFrame(train_num, columns=train_c[num_attrs].columns, index=train_c[num_attrs].index)","6535d913":"train_num.info()","b373f7f0":"train_c.drop('Cabin', axis=1)","5e4b317f":"imputer_cat = SimpleImputer(strategy='most_frequent')\ncat_attrs = ['Sex', 'Embarked']\ntrain_cat = imputer_cat.fit_transform(train_c[cat_attrs])\ntrain_cat = pd.DataFrame(train_cat, columns=train_c[cat_attrs].columns, index=train_c[cat_attrs].index)\ntrain_cat.info()","68bcb152":"from sklearn.preprocessing import OneHotEncoder\ncat_encoder = OneHotEncoder()\ncat_ecnoded = cat_encoder.fit_transform(train_cat)\ncat_ecnoded.toarray()","e75961bb":"train_c.head()","cbcd8f9a":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, idx):\n        self.idx = idx\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):        \n        parch_SibSp = X[:,self.idx[1]]*X[:, self.idx[2]]\n        age_parch = X[:, self.idx[2]]\/X[:, self.idx[0]]\n        age_Sibsp = X[:, self.idx[2]]*X[:, self.idx[1]]\n        return np.c_[X, parch_SibSp, age_parch, age_Sibsp]\n        \nattr_adder = CombinedAttributesAdder(idx=[4, 5, 6])\ntrain_extra_attrs = attr_adder.transform(train_c.values)","d9cffb1e":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('attrs_adder', CombinedAttributesAdder(idx=[0, 1, 2])),\n    ('std_scaler', StandardScaler()),\n])\n\ncat_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('encoder', OneHotEncoder())\n])","0af0fbfb":"from sklearn.compose import ColumnTransformer\nnum_attrs = ['Age', 'SibSp', 'Parch', 'Fare', 'Pclass']\ncat_attrs = ['Sex', 'Embarked']\n\nfull_pipeline = ColumnTransformer([\n    ('num', num_pipeline, num_attrs),\n    ('cat', cat_pipeline, cat_attrs)\n])\n\ntrain_prepared = full_pipeline.fit_transform(train_c)\ntest_prepared = full_pipeline.fit_transform(test)","d656b1f3":"from sklearn.linear_model import SGDClassifier\nsgd_clf = SGDClassifier()","a2bceba7":"from sklearn.model_selection import cross_val_score\ncross_val_score(sgd_clf, train_prepared, train_c_labels, cv=3, scoring='accuracy')","dc0a31d5":"train['Survived'].value_counts() #549\/(342 + 549) ~= 61","161f868b":"from sklearn.model_selection import cross_val_predict\npreds = cross_val_predict(sgd_clf, train_prepared, train_c_labels, cv=3)","426b7cc8":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(train_c_labels, preds)","c26ae9b8":"from sklearn.metrics import precision_score, recall_score\nprecision_score(train_c_labels, preds)","5250bb43":"recall_score(train_c_labels, preds)","6d122613":"from sklearn.metrics import f1_score\nf1_score(train_c_labels, preds)","3381ded3":"scores = cross_val_predict(sgd_clf, train_prepared, train_c_labels, cv=3, method='decision_function')","e346c699":"from sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, thresholds = precision_recall_curve(train_c_labels, scores)","11dd271e":"def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\") \n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.legend()\n    \nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.show()","cea2abe7":"threshold_80_precision = thresholds[np.argmax([precisions >= .80])]\npreds_80_precision = (scores >= threshold_80_precision)","a8bb3266":"precision_score(train_c_labels, preds_80_precision)","3a69c9f4":"recall_score(train_c_labels, preds_80_precision)","5ef0b5e5":"from sklearn.ensemble import RandomForestClassifier\nforest_clf = RandomForestClassifier(random_state=42)","e542c765":"preds = cross_val_predict(forest_clf, train_prepared, train_c_labels, cv=3)\nprecision_score(train_c_labels, preds)","a1d9b0f3":"recall_score(train_c_labels, preds)","f78d6414":"f1_score(train_c_labels, preds)","08447039":"from sklearn.linear_model import LogisticRegression\npreds = cross_val_predict(LogisticRegression(), train_prepared, train_c_labels, cv=3)\nprecision_score(train_c_labels, preds)","4108953a":"recall_score(train_c_labels, preds)","f414851a":"f1_score(train_c_labels, preds)","14f96837":"from sklearn.model_selection import GridSearchCV\n\nparam_grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l2\"]}\nlogistic_clf = GridSearchCV(LogisticRegression(), param_grid, cv=5, verbose=0)\nlogistic_clf_grid = logistic_clf.fit(train_prepared, train_c_labels)","3683454f":"logistic_clf_grid.best_params_","868fcc12":"lg_best = LogisticRegression(C = 0.1, penalty='l2')\npreds = cross_val_predict(lg_best, train_prepared, train_c_labels, cv=3)\nf1_score(train_c_labels, preds)","d7e14682":"logistic_final_preds = logistic_clf_grid.predict(test_prepared)","0c699eb4":"submit = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsubmit['Survived'] = logistic_final_preds\nsubmit.to_csv('logistic_submission.csv', index=False)","9c31c2c0":"param_grid = { \n    'n_estimators': [80, 100, 120, 140, 150],\n    'max_features': ['auto', 'sqrt', 'log2']\n}\nforest_clf = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, verbose=0)\nforest_clf_grid = forest_clf.fit(train_prepared, train_c_labels)","858b372b":"forest_clf_grid.best_params_","696a4a19":"forest_best = RandomForestClassifier(max_features='sqrt', n_estimators=140)\npreds = cross_val_predict(forest_best, train_prepared, train_c_labels, cv=3)\nf1_score(train_c_labels, preds)","2c25493f":"forest_final_preds = forest_clf_grid.predict(test_prepared)","daf4d21f":"submit = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsubmit['Survived'] = forest_final_preds\nsubmit.to_csv('forest_submission.csv', index=False)","1e426f32":"For the attribute **Parch**(parent\/children), the line isn't always decreasing. However, still the higher the ratio, the higher the possibility of the passenger to not survive. ","57349d6b":"# Conclusion\n\nThe purpose of this notebook is to get you started and to help you practice some of the basic data analysis and machine learning skills. We have not done everything possible to increase the accuracy of the classifier. We could have spent more time understanding our features and adding more to score better. This **Forest** submission's score is **0.77272**. \n\nThank you!","75ed220e":"#### Confusion Matrix\n\n\nSince accuracy is not a preferred performance measure for this problem. We will look at somethign called **confusion matrix**. Each row in a confusion matrix represents an actual class whereas each column represents a predicted class. To calculate our confusion matrix, we need to have predictions for the target label.\n","2fb4463c":"The **info** function basicly shows information about the data. column names, types..etc\n\n**Something to notice here is that some values are missing, so we will have to take care of these later.**","4bc82319":"Great! This is even higher than what RandomForestClassifier achieved.\n\n## Lets Fine-Tune two of our models\n\nFinally, we will fine tune our models and test them on the test set. ","e8aa64b4":"**Great**! We got more features and even some of them are more related to our target attribute!","a475f137":"Let's graph this information.","0346f9f1":"Let's see how every attribute is related to our target label **Survived**.","cc9e1814":"At this moment. We used transformers provided by Sklearn, and we built ours. One important thing to do is to do these transformations in order. To make things easier, we will use pipelines. First, we start with a pipeline for the numerical then the categorical data.","bfec0ce2":"Enough huh? Time to look for correlations!","08a7a83f":"Let's get a classifier that has 80% precision.","89b4191b":"Now, to handle all the columns together, we are going to use **ColumnTransformer** from Sklearn.","8770bf36":"# Creating a Test Set\n\nAt this point, before touching the data we would've splitted the data and put our test set aside. However, there is no need for that, Kaggle, did this step for us. ","023d93f8":"That is it! The data is ready for the machine learning algorithms. Time to select & train a mode.","541f98a4":"* The histogram of the attribute **Pclass** proves what we mentioned earlier. Pclass is indeed a categorical attribute, or **ordinal** attribute. Though we need to if the order does actually matter.\n\n* Suprisinlgy two more columns could be considered as categorical attribures: **Parch** and **SibSp**\n\n\n* Finally, we will need to have all the values to be on the same scale.","fa2413b1":"# Loading the data","c5480095":"Furthomore, we may look at the distributions of the attributes.","f3f8aef0":"If you run this cell multiple times, you will end up with different scores. However, the score is usually in the seventies. \n\n\nWhat if we build a classifier than classifes every instance as no Survived (0)? Our classifier's accuracy would be 61. which is close to what we are getting already. That's why we better not trust accuracy as a performance measure for classifiers. ","962fc1aa":"And fill the 2 missing values in **Embarked** with the most frequent value.\n\nWe do this on all the categorical values that we are planning to feed to the ML algorithm.","4409b12e":"# A quick glance at the data","532e7335":"* The first row is for the negative class (did not survived). 450 of them were correctly classified, while 99 were not. \n* The next row, however, is for the positive class  survived). 131 of them were wrongly classified, while 211 were correctly classified. \n\n\n\n#### The confusion matrix gives us lots of information. We need more concise matrices. Like: \n* Precision: the accuracy of the positive predictions. \n* Recall: the ratio of the positive instances(survived) that are detected correctly.","3c1c3736":"Let us investigate a couple of more columns the same way with the same functions we created. ","4fa160d9":"Only three attributes: \n* Age: 177 missing values.\n* Cabin: 687 missing values. \n* Embarked: 2 missing values. \n\nLet's start with the first one: **Age**. It has 177 missing values. Therefore, we won't drop the whole attribute or even drop the corresponding rows. We will go with option three \"filling with some value\". To choose that value, it is worthy looking at the distribution of that attribute. ","a3a54c24":"Filling the missing values with the **mean** seems reasonable. However, one could go deeper to understand why those values are missing in the first place, and make assumptions.\n\nWhen it comes to the next attribute, **Cabin**. Dropping the whole attribute makes sense, since approximatly 80% of the values are misssing. \n\nFinally, for the the last one, **Embarked**. It is only missing 2 values! We can just fill these with the most frequent one. \n\nFor this task. We will be using the **SimpleImputer** from Sklearn.","16114269":"The attribute **Pclass** contains 3 categories or classes. \n* Let's see the number of passengers that fall in each class. \n* The number of passengers that suvived from each class.\n* The survival ratio of each class: Survived\/PassengerCount.\n\nThe following function does that for us. ","1656b6e7":"This is a huge improvement! The f1 score of the RandomForestClassifier is much better than the SGDClassifier. \n\n\nLet us try another algorithm: **LogisticRegression**","84940ab7":"### Handling Text and Categorical Attributes\n\nWe have been ignoring the categorical attributes. Now it is the time to handle them. Convert them from text to numbers. \n\nWe will only consider two categorical attributes: \n* Sex\n* Embarked\n\nWe will use **OneHotEncoder** to convert the categorical values to one hot vectors. ","047d862d":"### Missing Values\n\nWe need to fill the missing values in our data. We may do any of the following to fix this: \n1. Drop the whole attribute.\n2. Drop those training exampls.(passengers)\n3. Fill in those values with some value(e.g. mean, median, etc.)\n\nLet's show what attributes have missing value again. ","be034073":"Very low recall as expected! Let us try another algorithm.","584cc855":"Looks like some columns are pretending to be **numerical** but actually they are not! E.g. the attribute **Pclass**'s value counts are as shown below. The values belong to one of 3 **categories**, classes (1, 2, 3). Similar attributes are **Survived** which is our target label. We will take care of these later. ","49e9e383":"The ratio in the above graph is decreaseing all the way up(except for the first category). Looks like the more sibligs you have(since the denominator is either 1 or 0, the attribute just means the number of sibilings of a passenger) given that you have one spouse on board, the lower the chance of you surviving.","cd4f3c95":"# Looking For Correlations\n\n\nLet's compute the standard correlation coefficient between every pair. ","e9028cb3":"A good way to combine both scores is the F1 score. which is the harmoic mean(gives more weight to the low values) of both. F1 score then prefers classifiers that have similar precisoin and recall. However, sometimes a problem may give more importance to the precision or the recall. ","1b49f253":"Similarly, to have a closer look at the numerical attributes, we do the following: ","1c940bd3":"* Class 1's survival ratio is close to **.6**. \n* Class 2's survival ratio is close to **.5**. \n* Class 3's survival ratio is close to **.25**.\n\n**We can conclude that the higher the class(low in value), the higher the survival ratio is.**\n","2550c52c":"Now, as you see. The **Age** attribute does not have missing values anymore. \n\nLet's Drop the column **Cabin**.","08a2e384":"### Transformers & Pipelines\n\n**Great**! We have transformed our data in different ways, but in different places. This could be more organized. Let's user Transformers and pipelines to do this.\n\n\nWe will build a transformer to perform the combination of the attributes we did eariler.","1e632230":"#### Precision\/Recall Trade-off\n\nIncreasing precision reduces recal, and vice versa. Usually, a threshold decides this. We will compute precisions and recalls for all possible thresholds. ","d46c0e02":"# Data Cleaning\n\n\nFirst, we will go on a copy the original data again and drop the target attribute. And save the labels to a variable as well.\n","b89f1a86":"# Select and Train a Model\n\nWe will start with a **Stochastic Gradient Descent** (SGD) classifier. It deals with training instances independently. Also, we will evaluate the model using cross vlidation. ","b69480f5":"Pretty much what we expected for some attributes. The **Pclass** is showing a high correlation. It is negative because the lower the value(1st class) the higher the survival ratio. Also, automatically the Pclass attribute will be related to the **Fare** attribute.\n\n**We may go on and try some meaningful combinations to get more features!**","dea6f384":"Some of the columns are of type **object**, meaning that those values could be any Python object. However, since the data are coming from a CSV file, they must be **text attributes**, known as **categorical attributes**.  \n\n\nTo look closely at the categorical attributes for now we may use the function **value_counts**.","5c82fb9d":"# Discovering the Data to Gain Insights\n\nWe will create a copy to work on and keep our original data clean. ","15198f3a":"Somethings to notice:\n* There are more passengers in class 3 than class 2, and class 2 has more passengers than class 1. This could tell us that class 1's ticket is **expensive**. \n* The **survival ratio** is different for each class. Apparently, Many of class 1 survived. Let's see the follwoing graph to see this."}}