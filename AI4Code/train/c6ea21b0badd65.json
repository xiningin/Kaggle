{"cell_type":{"c68ee9fa":"code","ada10dc9":"code","0d1cb9a9":"code","849852ed":"code","b5b4b269":"code","500b194d":"code","21d66ca6":"code","31420d4d":"code","feb9dffc":"code","f93872a2":"code","e22bffe4":"code","fcd2705e":"code","d1d1fb3e":"code","83d63f38":"code","cfdcc576":"code","599a0e45":"code","11832abd":"code","ccf3c8e4":"code","72c917cd":"code","0545963d":"code","7d5f7ecb":"code","cd58c975":"code","3280e88c":"code","d2dba46c":"code","ab50c826":"code","c3e14ce8":"code","f3d4f900":"code","815c84c6":"code","42ea825c":"code","d5eedc5c":"code","5abfb121":"code","bce5deb5":"code","bc686293":"code","3533c039":"code","6f51f9ee":"code","813bf327":"markdown","496322d8":"markdown","ab7e0ae2":"markdown","ac1afcfe":"markdown","4aab731e":"markdown","90035b1a":"markdown","53a6c08c":"markdown","852ea4af":"markdown"},"source":{"c68ee9fa":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objs as go","ada10dc9":"from dataprep.eda import *\n","0d1cb9a9":"df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\n","849852ed":"df.head()","b5b4b269":"df.columns\n","500b194d":"df.select_dtypes(exclude=['int64','float64']).columns\n","21d66ca6":"df.gender.replace({'Male': 1, 'Female': 0}, inplace=True)\n\ndf.ever_married.replace({'No': 0, 'Yes': 1}, inplace=True)\n\ndf.work_type.replace({'Private': 0, 'Self-employed': 1, 'children': 2,'Govt_job':3,'Never_worked':4}, inplace=True)\n\ndf.Residence_type.replace({'Urban': 0, 'Rural': 1}, inplace=True)\n\ndf.smoking_status.replace({'never smoked': 0, 'Unknown': 1,'formerly smoked':2,'smokes':3}, inplace=True)\n\n","31420d4d":"df['gender']=pd.get_dummies(df['gender'])","feb9dffc":"create_report(df)\n","f93872a2":"df.head()","e22bffe4":"df=df.drop(columns='id',axis=1)\n","fcd2705e":"df.head()","d1d1fb3e":"df.fillna(df.mode())","83d63f38":"df=df.dropna()","cfdcc576":"trace0 = go.Box(\n    name = \"gender\",\n    y = df[\"gender\"]\n)\n\ntrace1 = go.Box(\n    name = \"age\",\n    y = df[\"age\"]\n)\n\ntrace2 = go.Box(\n    name = \"hypertension\",\n    y = df[\"hypertension\"]\n)\n\ntrace3 = go.Box(\n    name = \"heart_disease\",\n    y = df[\"heart_disease\"] \n)\n\ntrace4 = go.Box(\n    name = \"ever_married\",\n    y = df[\"ever_married\"]\n)\n\ntrace5 = go.Box(\n    name = \"work_type\",\n    y = df[\"work_type\"]\n)\n\ntrace6 = go.Box(\n    name = \"Residence_type\",\n    y = df[\"Residence_type\"]\n)\n\ntrace7 = go.Box(\n    name = \"avg_glucose_level\",\n    y = df[\"avg_glucose_level\"]\n)\n\ntrace8 = go.Box(\n    name = \"bmi\",\n    y = df[\"bmi\"]\n)\n\ntrace9 = go.Box(\n    name = \"smoking_status\",\n    y = df[\"smoking_status\"]\n)\n\ntrace10 = go.Box(\n    name = \"stroke\",\n    y = df[\"stroke\"]\n)\ndata = [trace0, trace1, trace2 , trace3 , trace4 , trace5 ,trace6, trace7, trace8 , trace9 , trace10  ]\nplotly.offline.iplot(data)","599a0e45":"plot_correlation(df, \"stroke\")\n","11832abd":"fig = px.scatter_matrix(df, dimensions=['gender', 'age', 'hypertension', 'stroke'])\nfig.show()","ccf3c8e4":"fig = px.scatter_matrix(df, dimensions=[ 'heart_disease', 'ever_married',\n       'work_type', 'stroke'])\nfig.show()","72c917cd":"fig = px.scatter_matrix(df, dimensions=['Residence_type', 'avg_glucose_level', 'bmi',\n                                        'stroke'])\nfig.show()","0545963d":"fig = px.scatter_matrix(df, dimensions=['Residence_type', 'avg_glucose_level', 'bmi',\n                                        'smoking_status','stroke'])\nfig.show()","7d5f7ecb":"df[df.isnull().any(axis=1)]","cd58c975":"X = df.drop(columns=[\"stroke\"])\ny = df[\"stroke\"]","3280e88c":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","d2dba46c":"from sklearn.ensemble import RandomForestClassifier\nrf_classifier=RandomForestClassifier(n_estimators=10).fit(X_train,y_train)\nprediction=rf_classifier.predict(X_test)","ab50c826":"from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\nprint(confusion_matrix(y_test,prediction))\nprint(accuracy_score(y_test,prediction))\nprint(classification_report(y_test,prediction))","c3e14ce8":"### Manual Hyperparameter Tuning\nmodel=RandomForestClassifier(n_estimators=300,criterion='entropy',\n                             max_features='sqrt',min_samples_leaf=10,random_state=100).fit(X_train,y_train)\npredictions=model.predict(X_test)\nprint(confusion_matrix(y_test,predictions))\nprint(accuracy_score(y_test,predictions))\nprint(classification_report(y_test,predictions))","f3d4f900":"import numpy as np\nfrom sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt','log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 1000,10)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10,14]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4,6,8]\n# Create the random grid\nparam = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n              'criterion':['entropy','gini']}\nprint(param)","815c84c6":"from tpot import TPOTClassifier\n\n\ntpot_classifier = TPOTClassifier(generations= 5, population_size= 24, offspring_size= 12,\n                                 verbosity= 2, early_stop= 12,\n                                 config_dict={'sklearn.ensemble.RandomForestClassifier': param}, \n                                 cv = 4, scoring = 'accuracy')\ntpot_classifier.fit(X_train,y_train)","42ea825c":"accuracy = tpot_classifier.score(X_test, y_test)\nprint(accuracy)","d5eedc5c":"import optuna\nimport sklearn.svm\ndef objective(trial):\n\n    classifier = trial.suggest_categorical('classifier', ['RandomForest', 'SVC'])\n    \n    if classifier == 'RandomForest':\n        n_estimators = trial.suggest_int('n_estimators', 200, 2000,10)\n        max_depth = int(trial.suggest_float('max_depth', 10, 100, log=True))\n\n        clf = sklearn.ensemble.RandomForestClassifier(\n            n_estimators=n_estimators, max_depth=max_depth)\n    else:\n        c = trial.suggest_float('svc_c', 1e-10, 1e10, log=True)\n        \n        clf = sklearn.svm.SVC(C=c, gamma='auto')\n\n    return sklearn.model_selection.cross_val_score(\n        clf,X_train,y_train, n_jobs=-1, cv=3).mean()\n","5abfb121":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\n\ntrial = study.best_trial\n\nprint('Accuracy: {}'.format(trial.value))\nprint(\"Best hyperparameters: {}\".format(trial.params))","bce5deb5":"trial","bc686293":"study.best_params","3533c039":"rf=RandomForestClassifier(n_estimators=330,max_depth=30)\nrf.fit(X_train,y_train)","6f51f9ee":"y_pred=rf.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(accuracy_score(y_test,y_pred))\nprint(classification_report(y_test,y_pred))","813bf327":"### Dataset  link\nhttps:\/\/www.kaggle.com\/fedesoriano\/stroke-prediction-dataset","496322d8":"## Project Name : Stroke Prediction\n\nThe main aim of this project is to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status.","ab7e0ae2":"## All the Lifecycle In A Data Science Projects\n1. Data Analysis\n2. Feature Engineering\n3. Feature Selection\n4. Model Building\n5. Model Deployment","ac1afcfe":"### Optimize hyperparameters of the model using Optuna\nThe hyperparameters of the above algorithm are n_estimators and max_depth for which we can try different values to see if the model accuracy can be improved. The objective function is modified to accept a trial object. This trial has several methods for sampling hyperparameters. We create a study to run the hyperparameter optimization and finally read the best hyperparameters.","4aab731e":"### RandomForestClassifier","90035b1a":"****[!pip install dataprep](http:\/\/)[](http:\/\/)\n","53a6c08c":"###  Genetic Algorithms\u00b6\nGenetic Algorithms tries to apply natural selection mechanisms to Machine Learning contexts.\n\nLet's immagine we create a population of N Machine Learning models with some predifined Hyperparameters. We can then calculate the accuracy of each model and decide to keep just half of the models (the ones that performs best). We can now generate some offsprings having similar Hyperparameters to the ones of the best models so that go get again a population of N models. At this point we can again caltulate the accuracy of each model and repeate the cycle for a defined number of generations. In this way, just the best models will survive at the end of the process.","852ea4af":"The main parameters used by a Random Forest Classifier are:\n\n- criterion = the function used to evaluate the quality of a split.\n- max_depth = maximum number of levels allowed in each tree.\n- max_features = maximum number of features considered when splitting a node.\n- min_samples_leaf = minimum number of samples which can be stored in a tree leaf.\n- min_samples_split = minimum number of samples necessary in a node to cause node splitting.\n- n_estimators = number of trees in the ensamble."}}