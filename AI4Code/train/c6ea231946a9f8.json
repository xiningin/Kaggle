{"cell_type":{"285a7569":"code","d670a930":"code","8fc9c6c5":"code","b600fd3b":"code","fac390e2":"code","af97513a":"code","1863c43a":"code","33f4636d":"code","7cd8307f":"code","c02e10d4":"code","e7ed54ce":"code","9c72424c":"code","17d728bb":"code","0927613f":"code","cb39a1a2":"code","4e932869":"code","3cfe822d":"code","dfc0e108":"code","85aac5e2":"code","a97d7978":"code","2048a471":"code","b67bb397":"code","0c03b68c":"code","eaeafbb5":"code","2868a002":"code","3c37c1cd":"code","9f963814":"code","ce23e360":"code","981c1389":"code","4b6a9115":"code","9faa8216":"code","d4a67d45":"code","29a9b9b1":"code","17583dad":"code","e61e8b68":"code","75d49828":"code","f9915923":"code","bfa3f845":"code","61b4d863":"code","52a62e01":"code","c2a311ea":"code","e27de2f4":"code","dc71733d":"code","0ba547cd":"code","bdd9f5d4":"code","59c32dda":"code","47ce6377":"code","4a7396ab":"code","812d71d6":"code","6fad778b":"code","0108ead1":"code","b9c66ba3":"code","771181f9":"code","a37de667":"code","39092c6d":"code","683b1edf":"code","fb8213e4":"code","300859ad":"code","df262667":"code","fa7c4196":"code","8bfc358e":"code","d0c37fc0":"code","f1cdaa37":"code","ea62e0a0":"code","dc55fcec":"code","44135160":"code","084c45fb":"code","57d78d6f":"code","fa159b93":"code","1a80d6b5":"code","906bd8a2":"code","0e988731":"code","a52d3f7e":"code","d069ec94":"code","348fd72f":"code","4c260544":"code","4b025a3c":"code","1bfd75a7":"code","2cdb0251":"code","6f4f4695":"code","e4ccab4e":"code","5ffa49d7":"code","5bfba4cf":"code","70aa4657":"code","3b4b862b":"code","85a8783f":"code","227605c7":"code","a1264729":"code","27aeb462":"code","f9719b38":"code","51622220":"code","cb7de26a":"code","d6a018dc":"code","61e20190":"code","b56b54d9":"code","3bc15b09":"code","f39596ed":"code","15d2825c":"code","8754e51d":"code","4f95e602":"code","9d22749f":"code","e0a800e2":"code","547db44d":"code","e93df467":"code","30ea6ca1":"code","205915ed":"code","40360e18":"code","07b0e29d":"code","46cb5b33":"markdown","0650b0ed":"markdown","dec7fe5c":"markdown","2de0d74f":"markdown","75223c35":"markdown","17598662":"markdown","047de7c8":"markdown","e8283144":"markdown","3f132e34":"markdown","b3e1cfbc":"markdown","328fc448":"markdown","7176a863":"markdown","b14e82d5":"markdown","36e941ce":"markdown","d485b3e2":"markdown","6ab02384":"markdown","7e697105":"markdown","c13cca5c":"markdown","8e1e8e80":"markdown","7c8978e3":"markdown","f696a906":"markdown","b005fbe5":"markdown","106c6f86":"markdown","b485516c":"markdown","35fd6d51":"markdown","436fc035":"markdown","263d320f":"markdown","3b7e481b":"markdown","3835071e":"markdown","ea9ec6c3":"markdown","bea4ad6e":"markdown","1185140f":"markdown","b8ea662a":"markdown","048695c2":"markdown"},"source":{"285a7569":"#!pip install catboost","d670a930":"# Import Dependencies\n%matplotlib inline\n\n# Start Python Imports\nimport math, time, random, datetime\n\n# Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Visualization \nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\n# Preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize\n\n# Machine learning\nimport catboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\n\n# Let's be rebels and ignore warnings for now\nimport warnings\nwarnings.filterwarnings('ignore')","8fc9c6c5":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ngender_submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv') # example of what a submission should look like","b600fd3b":"# View the training data\ntrain.head(15)","fac390e2":"f,ax=plt.subplots(1,2,figsize=(18,8))\ntrain['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived',data=train,ax=ax[1])\nax[1].set_title('Survived')\nplt.show()","af97513a":"train.Age.plot.hist()","1863c43a":"# View the test data (same columns as the training data)\ntest.head() # head = view first 5 lines","33f4636d":"# View the example submisison dataframe\ngender_submission.head()","7cd8307f":"train.describe()","c02e10d4":"#!pip install missingno","e7ed54ce":"import missingno\n# Plot graphic of missing values\nmissingno.matrix(train, figsize = (30,10))","9c72424c":"f,ax=plt.subplots(1,2,figsize=(18,8))\ntrain[['Sex','Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Sex')\nsns.countplot('Sex',hue='Survived',data=train,ax=ax[1])\nax[1].set_title('Sex:Survived vs Dead')\nplt.show()","17d728bb":"pd.crosstab(train.Pclass,train.Survived,margins=True).style.background_gradient(cmap='summer_r')","0927613f":"f,ax=plt.subplots(1,2,figsize=(18,8))\ntrain['Pclass'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'],ax=ax[0])\nax[0].set_title('Number Of Passengers By Pclass')\nax[0].set_ylabel('Count')\nsns.countplot('Pclass',hue='Survived',data=train,ax=ax[1])\nax[1].set_title('Pclass:Survived vs Dead')\nplt.show()","cb39a1a2":"pd.crosstab([train.Sex,train.Survived],train.Pclass,margins=True).style.background_gradient(cmap='summer_r')","4e932869":"sns.factorplot('Pclass','Survived',hue='Sex',data=train)\nplt.show()","3cfe822d":"print('Oldest Passenger was of:',train['Age'].max(),'Years')\nprint('Youngest Passenger was of:',train['Age'].min(),'Years')\nprint('Average Age on the ship:',train['Age'].mean(),'Years')","dfc0e108":"f,ax=plt.subplots(1,2,figsize=(18,8))\nsns.violinplot(\"Pclass\",\"Age\", hue=\"Survived\", data=train,split=True,ax=ax[0])\nax[0].set_title('Pclass and Age vs Survived')\nax[0].set_yticks(range(0,110,10))\nsns.violinplot(\"Sex\",\"Age\", hue=\"Survived\", data=train,split=True,ax=ax[1])\nax[1].set_title('Sex and Age vs Survived')\nax[1].set_yticks(range(0,110,10))\nplt.show()","85aac5e2":"train['Initial']=0\nfor i in train:\n    train['Initial']=train.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations","a97d7978":"pd.crosstab(train.Initial,train.Sex).T.style.background_gradient(cmap='summer_r') #Checking the Initials with the Sex","2048a471":"## Assigning the NaN Values with the Ceil values of the mean ages\ntrain.loc[(train.Age.isnull())&(train.Initial=='Mr'),'Age']=33\ntrain.loc[(train.Age.isnull())&(train.Initial=='Mrs'),'Age']=36\ntrain.loc[(train.Age.isnull())&(train.Initial=='Master'),'Age']=5\ntrain.loc[(train.Age.isnull())&(train.Initial=='Miss'),'Age']=22\ntrain.loc[(train.Age.isnull())&(train.Initial=='Other'),'Age']=46","b67bb397":"train.Age.isnull().any() #So no null values left finally ","0c03b68c":"f,ax=plt.subplots(1,2,figsize=(20,10))\ntrain[train['Survived']==0].Age.plot.hist(ax=ax[0],bins=20,edgecolor='black',color='red')\nax[0].set_title('Survived= 0')\nx1=list(range(0,85,5))\nax[0].set_xticks(x1)\ntrain[train['Survived']==1].Age.plot.hist(ax=ax[1],color='green',bins=20,edgecolor='black')\nax[1].set_title('Survived= 1')\nx2=list(range(0,85,5))\nax[1].set_xticks(x2)\nplt.show()","eaeafbb5":"sns.factorplot('Pclass','Survived',col='Initial',data=train)\nplt.show()","2868a002":"pd.crosstab([train.Embarked,train.Pclass],[train.Sex,train.Survived],margins=True).style.background_gradient(cmap='summer_r')","3c37c1cd":"sns.factorplot('Embarked','Survived',data=train)\nfig=plt.gcf()\nfig.set_size_inches(5,3)\nplt.show()","9f963814":"f,ax=plt.subplots(2,2,figsize=(20,15))\nsns.countplot('Embarked',data=train,ax=ax[0,0])\nax[0,0].set_title('No. Of Passengers Boarded')\nsns.countplot('Embarked',hue='Sex',data=train,ax=ax[0,1])\nax[0,1].set_title('Male-Female Split for Embarked')\nsns.countplot('Embarked',hue='Survived',data=train,ax=ax[1,0])\nax[1,0].set_title('Embarked vs Survived')\nsns.countplot('Embarked',hue='Pclass',data=train,ax=ax[1,1])\nax[1,1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace=0.2,hspace=0.5)\nplt.show()","ce23e360":"sns.factorplot('Pclass','Survived',hue='Sex',col='Embarked',data=train)\nplt.show()","981c1389":"train['Embarked'].fillna('S',inplace=True)","4b6a9115":"train.Embarked.isnull().any()# Finally No NaN values","9faa8216":"pd.crosstab([train.SibSp],train.Survived).style.background_gradient(cmap='summer_r')","d4a67d45":"f,ax=plt.subplots(1,2,figsize=(20,8))\nsns.barplot('SibSp','Survived',data=train,ax=ax[0])\nax[0].set_title('SibSp vs Survived')\nsns.factorplot('SibSp','Survived',data=train,ax=ax[1])\nax[1].set_title('SibSp vs Survived')\nplt.close(2)\nplt.show()","29a9b9b1":"pd.crosstab(train.SibSp,train.Pclass).style.background_gradient(cmap='summer_r')","17583dad":"pd.crosstab(train.Parch,train.Pclass).style.background_gradient(cmap='summer_r')","e61e8b68":"f,ax=plt.subplots(1,2,figsize=(20,8))\nsns.barplot('Parch','Survived',data=train,ax=ax[0])\nax[0].set_title('Parch vs Survived')\nsns.factorplot('Parch','Survived',data=train,ax=ax[1])\nax[1].set_title('Parch vs Survived')\nplt.close(2)\nplt.show()","75d49828":"print('Highest Fare was:',train['Fare'].max())\nprint('Lowest Fare was:',train['Fare'].min())\nprint('Average Fare was:',train['Fare'].mean())","f9915923":"f,ax=plt.subplots(1,3,figsize=(20,8))\nsns.distplot(train[train['Pclass']==1].Fare,ax=ax[0])\nax[0].set_title('Fares in Pclass 1')\nsns.distplot(train[train['Pclass']==2].Fare,ax=ax[1])\nax[1].set_title('Fares in Pclass 2')\nsns.distplot(train[train['Pclass']==3].Fare,ax=ax[2])\nax[2].set_title('Fares in Pclass 3')\nplt.show()","bfa3f845":"sns.heatmap(train.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) #data.corr()-->correlation matrix\nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.show()","61b4d863":"train['Age_band']=0\ntrain.loc[train['Age']<=16,'Age_band']=0\ntrain.loc[(train['Age']>16)&(train['Age']<=32),'Age_band']=1\ntrain.loc[(train['Age']>32)&(train['Age']<=48),'Age_band']=2\ntrain.loc[(train['Age']>48)&(train['Age']<=64),'Age_band']=3\ntrain.loc[train['Age']>64,'Age_band']=4\ntrain.head(2)","52a62e01":"train['Age_band'].value_counts().to_frame().style.background_gradient(cmap='summer')#checking the number of passenegers in each band","c2a311ea":"sns.factorplot('Age_band','Survived',data=train,col='Pclass')\nplt.show()","e27de2f4":"train['Family_Size']=0\ntrain['Family_Size']=train['Parch']+train['SibSp']#family size\ntrain['Alone']=0\ntrain.loc[train.Family_Size==0,'Alone']=1#Alone\n\nf,ax=plt.subplots(1,2,figsize=(18,6))\nsns.factorplot('Family_Size','Survived',data=train,ax=ax[0])\nax[0].set_title('Family_Size vs Survived')\nsns.factorplot('Alone','Survived',data=train,ax=ax[1])\nax[1].set_title('Alone vs Survived')\nplt.close(2)\nplt.close(3)\nplt.show()","dc71733d":"sns.factorplot('Alone','Survived',data=train,hue='Sex',col='Pclass')\nplt.show()","0ba547cd":"train['Fare_Range']=pd.qcut(train['Fare'],4)\ntrain.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","bdd9f5d4":"train['Fare_cat']=0\ntrain.loc[train['Fare']<=7.91,'Fare_cat']=0\ntrain.loc[(train['Fare']>7.91)&(train['Fare']<=14.454),'Fare_cat']=1\ntrain.loc[(train['Fare']>14.454)&(train['Fare']<=31),'Fare_cat']=2\ntrain.loc[(train['Fare']>31)&(train['Fare']<=513),'Fare_cat']=3","59c32dda":"sns.factorplot('Fare_cat','Survived',data=train,hue='Sex')\nplt.show()","47ce6377":"train.drop(['Name','Age','Ticket','Fare','Cabin','Fare_Range','PassengerId'],axis=1,inplace=True)\nsns.heatmap(train.corr(),annot=True,cmap='RdYlGn',linewidths=0.2,annot_kws={'size':20})\nfig=plt.gcf()\nfig.set_size_inches(18,15)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","4a7396ab":"# Alternatively, you can see the number of missing values like this\ntrain.isnull().sum()","812d71d6":"df_bin = pd.DataFrame() # for discretised continuous variables\ndf_con = pd.DataFrame() # for continuous variables","6fad778b":"# Different data types in the dataset\ntrain.dtypes","0108ead1":"train.head()","b9c66ba3":"# How many people survived?\nfig = plt.figure(figsize=(20,1))\nsns.countplot(y='Survived', data=train);\nprint(train.Survived.value_counts())","771181f9":"# Let's add this to our subset dataframes\ndf_bin['Survived'] = train['Survived']\ndf_con['Survived'] = train['Survived']","a37de667":"df_bin.head()","39092c6d":"df_con.head()","683b1edf":"sns.distplot(train.Pclass)","fb8213e4":"# How many missing variables does Pclass have?\ntrain.Pclass.isnull().sum()","300859ad":"df_bin['Pclass'] = train['Pclass']\ndf_con['Pclass'] = train['Pclass']","df262667":"# Let's view the distribution of Sex\nplt.figure(figsize=(20, 5))\nsns.countplot(y=\"Sex\", data=train);","fa7c4196":"# Are there any missing values in the Sex column?\ntrain.Sex.isnull().sum()","8bfc358e":"train.Sex.head()","d0c37fc0":"# add Sex to the subset dataframes\ndf_bin['Sex'] = train['Sex']\ndf_bin['Sex'] = np.where(df_bin['Sex'] == 'female', 1, 0) # change sex to 0 for male and 1 for female\n\ndf_con['Sex'] = train['Sex']","f1cdaa37":"# How many missing values does SibSp have?\ntrain.SibSp.isnull().sum()","ea62e0a0":"# What values are there?\ntrain.SibSp.value_counts()","dc55fcec":"def plot_count_dist(data, bin_df, label_column, target_column, figsize=(20, 5), use_bin_df=False):\n    \"\"\"\n    Function to plot counts and distributions of a label variable and \n    target variable side by side.\n    ::param_data:: = target dataframe\n    ::param_bin_df:: = binned dataframe for countplot\n    ::param_label_column:: = binary labelled column\n    ::param_target_column:: = column you want to view counts and distributions\n    ::param_figsize:: = size of figure (width, height)\n    ::param_use_bin_df:: = whether or not to use the bin_df, default False\n    \"\"\"\n    if use_bin_df: \n        fig = plt.figure(figsize=figsize)\n        plt.subplot(1, 2, 1)\n        sns.countplot(y=target_column, data=bin_df);\n        plt.subplot(1, 2, 2)\n        sns.distplot(data.loc[data[label_column] == 1][target_column], \n                     kde_kws={\"label\": \"Survived\"});\n        sns.distplot(data.loc[data[label_column] == 0][target_column], \n                     kde_kws={\"label\": \"Did not survive\"});\n    else:\n        fig = plt.figure(figsize=figsize)\n        plt.subplot(1, 2, 1)\n        sns.countplot(y=target_column, data=data);\n        plt.subplot(1, 2, 2)\n        sns.distplot(data.loc[data[label_column] == 1][target_column], \n                     kde_kws={\"label\": \"Survived\"});\n        sns.distplot(data.loc[data[label_column] == 0][target_column], \n                     kde_kws={\"label\": \"Did not survive\"});","44135160":"# Add SibSp to subset dataframes\ndf_bin['SibSp'] = train['SibSp']\ndf_con['SibSp'] = train['SibSp']","084c45fb":"# Visualise the counts of SibSp and the distribution of the values\n# against Survived\nplot_count_dist(train, \n                bin_df=df_bin, \n                label_column='Survived', \n                target_column='SibSp', \n                figsize=(20, 10))","57d78d6f":"# What values are there?\ntrain.Parch.value_counts()","fa159b93":"# Add Parch to subset dataframes\ndf_bin['Parch'] = train['Parch']\ndf_con['Parch'] = train['Parch']","1a80d6b5":"# What do the counts look like?\nsns.countplot(y='Embarked', data=train);","906bd8a2":"# Add Embarked to sub dataframes\ndf_bin['Embarked'] = train['Embarked']\ndf_con['Embarked'] = train['Embarked']","0e988731":"# Remove Embarked rows which are missing values\nprint(len(df_con))\ndf_con = df_con.dropna(subset=['Embarked'])\ndf_bin = df_bin.dropna(subset=['Embarked'])\nprint(len(df_con))","a52d3f7e":"# One-hot encode binned variables\none_hot_cols = df_bin.columns.tolist()\none_hot_cols.remove('Survived')\ndf_bin_enc = pd.get_dummies(df_bin, columns=one_hot_cols)\n\ndf_bin_enc.head()","d069ec94":"# One hot encode the categorical columns\ndf_embarked_one_hot = pd.get_dummies(df_con['Embarked'], \n                                     prefix='embarked')\n\ndf_sex_one_hot = pd.get_dummies(df_con['Sex'], \n                                prefix='sex')\n\ndf_plcass_one_hot = pd.get_dummies(df_con['Pclass'], \n                                   prefix='pclass')","348fd72f":"# Combine the one hot encoded columns with df_con_enc\ndf_con_enc = pd.concat([df_con, \n                        df_embarked_one_hot, \n                        df_sex_one_hot, \n                        df_plcass_one_hot], axis=1)\n\n# Drop the original categorical columns (because now they've been one hot encoded)\ndf_con_enc = df_con_enc.drop(['Pclass', 'Sex', 'Embarked'], axis=1)","4c260544":"# Seclect the dataframe we want to use first for predictions\nselected_df = df_con_enc","4b025a3c":"# Split the dataframe into data and labels\nX_train = selected_df.drop('Survived', axis=1) # data\ny_train = selected_df.Survived # labels","1bfd75a7":"# Shape of the data (without labels)\nX_train.shape","2cdb0251":"# Shape of the labels\ny_train.shape","6f4f4695":"# Function that runs the requested algorithm and returns the accuracy metrics\ndef fit_ml_algo(algo, X_train, y_train, cv):\n    \n    # One Pass\n    model = algo.fit(X_train, y_train)\n    acc = round(model.score(X_train, y_train) * 100, 2)\n    \n    # Cross Validation \n    train_pred = model_selection.cross_val_predict(algo, \n                                                  X_train, \n                                                  y_train, \n                                                  cv=cv, \n                                                  n_jobs = -1)\n    # Cross-validation accuracy metric\n    acc_cv = round(metrics.accuracy_score(y_train, train_pred) * 100, 2)\n    \n    return train_pred, acc, acc_cv","e4ccab4e":"#importing all the required ML packages\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn import svm #support vector Machine\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.naive_bayes import GaussianNB #Naive bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix","5ffa49d7":"# Logistic Regression\nstart_time = time.time()\ntrain_pred_log, acc_log, acc_cv_log = fit_ml_algo(LogisticRegression(), \n                                                               X_train, \n                                                               y_train, \n                                                                    10)\nlog_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_log)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_log)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=log_time))","5bfba4cf":"# k-Nearest Neighbours\nstart_time = time.time()\ntrain_pred_knn, acc_knn, acc_cv_knn = fit_ml_algo(KNeighborsClassifier(), \n                                                  X_train, \n                                                  y_train, \n                                                  10)\nknn_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_knn)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_knn)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=knn_time))","70aa4657":"# Gaussian Naive Bayes\nstart_time = time.time()\ntrain_pred_gaussian, acc_gaussian, acc_cv_gaussian = fit_ml_algo(GaussianNB(), \n                                                                      X_train, \n                                                                      y_train, \n                                                                           10)\ngaussian_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_gaussian)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_gaussian)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=gaussian_time))","3b4b862b":"# Linear SVC\nstart_time = time.time()\ntrain_pred_svc, acc_linear_svc, acc_cv_linear_svc = fit_ml_algo(LinearSVC(),\n                                                                X_train, \n                                                                y_train, \n                                                                10)\nlinear_svc_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_linear_svc)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_linear_svc)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=linear_svc_time))","85a8783f":"# Stochastic Gradient Descent\nstart_time = time.time()\ntrain_pred_sgd, acc_sgd, acc_cv_sgd = fit_ml_algo(SGDClassifier(), \n                                                  X_train, \n                                                  y_train,\n                                                  10)\nsgd_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_sgd)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_sgd)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=sgd_time))","227605c7":"# Decision Tree Classifier\nstart_time = time.time()\ntrain_pred_dt, acc_dt, acc_cv_dt = fit_ml_algo(DecisionTreeClassifier(), \n                                                                X_train, \n                                                                y_train,\n                                                                10)\ndt_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_dt)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_dt)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=dt_time))","a1264729":"# Gradient Boosting Trees\nstart_time = time.time()\ntrain_pred_gbt, acc_gbt, acc_cv_gbt = fit_ml_algo(GradientBoostingClassifier(), \n                                                                       X_train, \n                                                                       y_train,\n                                                                       10)\ngbt_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_gbt)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_gbt)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=gbt_time))","27aeb462":"# Define the categorical features for the CatBoost model\ncat_features = np.where(X_train.dtypes != np.float)[0]\ncat_features","f9719b38":"# Use the CatBoost Pool() function to pool together the training data and categorical feature labels\ntrain_pool = Pool(X_train, \n                  y_train,\n                  cat_features)","51622220":"# CatBoost model definition\ncatboost_model = CatBoostClassifier(iterations=1000,\n                                    custom_loss=['Accuracy'],\n                                    loss_function='Logloss')\n\n# Fit CatBoost model\ncatboost_model.fit(train_pool,\n                   plot=True)\n\n# CatBoost accuracy\nacc_catboost = round(catboost_model.score(X_train, y_train) * 100, 2)","cb7de26a":"# How long will this take?\nstart_time = time.time()\n\n# Set params for cross-validation as same as initial model\ncv_params = catboost_model.get_params()\n\n# Run the cross-validation for 10-folds (same as the other models)\ncv_data = cv(train_pool,\n             cv_params,\n             fold_count=10,\n             plot=True)\n\n# How long did it take?\ncatboost_time = (time.time() - start_time)\n\n# CatBoost CV results save into a dataframe (cv_data), let's withdraw the maximum accuracy score\nacc_cv_catboost = round(np.max(cv_data['test-Accuracy-mean']) * 100, 2)","d6a018dc":"# Print out the CatBoost model metrics\nprint(\"---CatBoost Metrics---\")\nprint(\"Accuracy: {}\".format(acc_catboost))\nprint(\"Accuracy cross-validation 10-Fold: {}\".format(acc_cv_catboost))\nprint(\"Running Time: {}\".format(datetime.timedelta(seconds=catboost_time)))","61e20190":"models = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree', 'Gradient Boosting Trees',\n              'CatBoost'],\n    'Score': [\n        acc_knn, \n        acc_log,  \n        acc_gaussian, \n        acc_sgd, \n        acc_linear_svc, \n        acc_dt,\n        acc_gbt,\n        acc_catboost\n    ]})\nprint(\"---Reuglar Accuracy Scores---\")\nmodels.sort_values(by='Score', ascending=False)","b56b54d9":"cv_models = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree', 'Gradient Boosting Trees',\n              'CatBoost'],\n    'Score': [\n        acc_cv_knn, \n        acc_cv_log,      \n        acc_cv_gaussian, \n        acc_cv_sgd, \n        acc_cv_linear_svc, \n        acc_cv_dt,\n        acc_cv_gbt,\n        acc_cv_catboost\n    ]})\nprint('---Cross-validation Accuracy Scores---')\ncv_models.sort_values(by='Score', ascending=False)","3bc15b09":"# Feature Importance\ndef feature_importance(model, data):\n    \"\"\"\n    Function to show which features are most important in the model.\n    ::param_model:: Which model to use?\n    ::param_data:: What data to use?\n    \"\"\"\n    fea_imp = pd.DataFrame({'imp': model.feature_importances_, 'col': data.columns})\n    fea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False]).iloc[-30:]\n    _ = fea_imp.plot(kind='barh', x='col', y='imp', figsize=(20, 10))\n    return fea_imp\n    #plt.savefig('catboost_feature_importance.png')","f39596ed":"# Plot the feature importance scores\nfeature_importance(catboost_model, X_train)","15d2825c":"metrics = ['Precision', 'Recall', 'F1', 'AUC']\n\neval_metrics = catboost_model.eval_metrics(train_pool,\n                                           metrics=metrics,\n                                           plot=True)\n\nfor metric in metrics:\n    print(str(metric)+\": {}\".format(np.mean(eval_metrics[metric])))","8754e51d":"# One hot encode the columns in the test data frame (like X_train)\ntest_embarked_one_hot = pd.get_dummies(test['Embarked'], \n                                       prefix='embarked')\n\ntest_sex_one_hot = pd.get_dummies(test['Sex'], \n                                prefix='sex')\n\ntest_plcass_one_hot = pd.get_dummies(test['Pclass'], \n                                   prefix='pclass')","4f95e602":"# Combine the test one hot encoded columns with test\ntest = pd.concat([test, \n                  test_embarked_one_hot, \n                  test_sex_one_hot, \n                  test_plcass_one_hot], axis=1)","9d22749f":"# Create a list of columns to be used for the predictions\nwanted_test_columns = X_train.columns\nwanted_test_columns","e0a800e2":"# Make a prediction using the CatBoost model on the wanted columns\npredictions = catboost_model.predict(test[wanted_test_columns])","547db44d":"# Our predictions array is comprised of 0's and 1's (Survived or Did Not Survive)\npredictions[:20]","e93df467":"# Create a submisison dataframe and append the relevant columns\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = test['PassengerId']\nsubmission['Survived'] = predictions # our model predictions on the test dataset\nsubmission.head()","30ea6ca1":"# Let's convert our submission dataframe 'Survived' column to ints\nsubmission['Survived'] = submission['Survived'].astype(int)\nprint('Converted Survived column to integers.')","205915ed":"# Are our test and submission dataframes the same length?\nif len(submission) == len(test):\n    print(\"Submission dataframe is the same length as test ({} rows).\".format(len(submission)))\nelse:\n    print(\"Dataframes mismatched, won't be able to submit to Kaggle.\")","40360e18":"# Convert submisison dataframe to csv for submission to csv \n# for Kaggle submisison\nsubmission.to_csv('..\/titanic_submission.csv', index=False)\nprint('Submission CSV is ready!')","07b0e29d":"# Check the submission csv to make sure it's in the right format\nsubmissions_check = pd.read_csv(\"..\/titanic_submission.csv\")\nsubmissions_check.head()","46cb5b33":"## Embarked--> Categorical Value","0650b0ed":"### Observations:\n\nHere too the results are quite similar. Passengers with their parents onboard have greater chance of survival. It however reduces as the number goes up.\n\nThe chances of survival is good for somebody who has 1-3 parents on the ship. Being alone also proves to be fatal and the chances for survival decreases when somebody has >4 parents on the ship.","dec7fe5c":"The chances for survival for Port C is highest around 0.55 while it is lowest for S.","2de0d74f":"### Chances for Survival by Port Of Embarkation","75223c35":"1)The number of children increases with Pclass and the survival rate for passenegers below Age 10(i.e children) looks to be good irrespective of the Pclass.\n\n2)Survival chances for Passenegers aged 20-50 from Pclass1 is high and is even better for Women.\n\n3)For males, the survival chances decreases with an increase in age.\n\nAs we had seen earlier, the Age feature has 177 null values. To replace these NaN values, we can assign them the mean age of the dataset.","17598662":"### Interpreting The Heatmap\n\nThe first thing to note is that only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. Before understanding the plot, let us see what exactly correlation is.\n\n**POSITIVE CORRELATION:** If an **increase in feature A leads to increase in feature B, then they are positively correlated**. A value **1 means perfect positive correlation**.\n\n**NEGATIVE CORRELATION:** If an **increase in feature A leads to decrease in feature B, then they are negatively correlated**. A value **-1 means perfect negative correlation**.\n\nNow lets say that two features are highly or perfectly correlated, so the increase in one leads to increase in the other. This means that both the features are containing highly similar information and there is very little or no variance in information. This is known as **MultiColinearity** as both of them contains almost the same information.\n\nSo do you think we should use both of them as **one of them is redundant**. While making or training models, we should try to eliminate redundant features as it reduces training time and many such advantages.\n\nNow from the above heatmap,we can see that the features are not much correlated. The highest correlation is between **SibSp and Parch i.e 0.41**. So we can carry on with all features.","047de7c8":"## Fare--> Continous Feature","e8283144":"## Parch","3f132e34":"### Observations:\n1)Maximum passenegers boarded from S. Majority of them being from Pclass3.\n\n2)The Passengers from C look to be lucky as a good proportion of them survived. The reason for this maybe the rescue of all the Pclass1 and Pclass2 Passengers.\n\n3)The Embark S looks to the port from where majority of the rich people boarded. Still the chances for survival is low here, that is because many passengers from Pclass3 around **81%** didn't survive. \n\n4)Port Q had almost 95% of the passengers were from Pclass3.","b3e1cfbc":"### How many Survived??","328fc448":"Observations:\n\n1)The survival chances are almost 1 for women for Pclass1 and Pclass2 irrespective of the Pclass.\n\n2)Port S looks to be very unlucky for Pclass3 Passenegers as the survival rate for both men and women is very low.(Money Matters)\n\n3)Port Q looks looks to be unlukiest for Men, as almost all were from Pclass 3.\n","7176a863":"As discussed above, we can clearly see that as the **fare_range increases, the chances of survival increases.**\n\nNow we cannot pass the Fare_Range values as it is. We should convert it into singleton values same as we did in **Age_Band**","b14e82d5":"The crosstab again shows that larger families were in Pclass3.","36e941ce":"## Observations in a Nutshell for all features:\n**Sex:** The chance of survival for women is high as compared to men.\n\n**Pclass:**There is a visible trend that being a **1st class passenger** gives you better chances of survival. The survival rate for **Pclass3 is very low**. For **women**, the chance of survival from **Pclass1** is almost 1 and is high too for those from **Pclass2**.   **Money Wins!!!**. \n\n**Age:** Children less than 5-10 years do have a high chance of survival. Passengers between age group 15 to 35 died a lot.\n\n**Embarked:** This is a very interesting feature. **The chances of survival at C looks to be better than even though the majority of Pclass1 passengers got up at S.** Passengers at Q were all from **Pclass3**. \n\n**Parch+SibSp:** Having 1-2 siblings,spouse on board or 1-3 Parents shows a greater chance of probablity rather than being alone or having a large family travelling with you.","d485b3e2":"## Age_band\n\n#### Problem With Age Feature:\nAs I have mentioned earlier that **Age is a continous feature**, there is a problem with Continous Variables in Machine Learning Models.\n\n**Eg:**If I say to group or arrange Sports Person by **Sex**, We can easily segregate them by Male and Female.\n\nNow if I say to group them by their **Age**, then how would you do it? If there are 30 Persons, there may be 30 age values. Now this is problematic.\n\nWe need to convert these **continous values into categorical values** by either Binning or Normalisation. I will be using binning i.e group a range of ages into a single bin or assign them a single value.\n\nOkay so the maximum age of a passenger was 80. So lets divide the range from 0-80 into 5 bins. So 80\/5=16.\nSo bins of size 16.","6ab02384":"True that..the survival rate decreases as the age increases irrespective of the Pclass.\n\n## Family_Size and Alone\nAt this point, we can create a new feature called \"Family_size\" and \"Alone\" and analyse it. This feature is the summation of Parch and SibSp. It gives us a combined data so that we can check if survival rate have anything to do with family size of the passengers. Alone will denote whether a passenger is alone or not.","7e697105":"## Part1: Exploratory Data Analysis(EDA)","c13cca5c":"### Observations:\n1)The Toddlers(age<5) were saved in large numbers(The Women and Child First Policy).\n\n2)The oldest Passenger was saved(80 years).\n\n3)Maximum number of deaths were in the age group of 30-40.","8e1e8e80":"If You Like the notebook and think that it helped you..**PLEASE UPVOTE**. It will keep me motivated.<br\/>\nI am looking forward to receive your feedbacks, they will help me to improve my skills.","7c8978e3":"## Correlation Between The Features","f696a906":"This looks interesting. The number of men on the ship is lot more than the number of women. Still the number of women saved is almost twice the number of males saved. The survival rates for a women on the ship is around 75% while that for men in around 18-19%.\n\nThis looks to be a very important feature for modeling. But is it the best?? Lets check other features.","b005fbe5":"## Types Of Features\n\n### Categorical Features:\nA categorical variable is one that has two or more categories and each value in that feature can be categorised by them.For example, gender is a categorical variable having two categories (male and female). Now we cannot sort or give any ordering to such variables. They are also known as **Nominal Variables**.\n\n**Categorical Features in the dataset: Sex,Embarked.**\n\n### Ordinal Features:\nAn ordinal variable is similar to categorical values, but the difference between them is that we can have relative ordering or sorting between the values. For eg: If we have a feature like **Height** with values **Tall, Medium, Short**, then Height is a ordinal variable. Here we can have a relative sort in the variable.\n\n**Ordinal Features in the dataset: PClass**\n\n### Continous Feature:\nA feature is said to be continous if it can take values between any two points or between the minimum or maximum values in the features column.\n\n**Continous Features in the dataset: Age**","106c6f86":"## SibSip-->Discrete Feature\nThis feature represents whether a person is alone or with his family members.\n\nSibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife ","b485516c":"## Age--> Continous Feature","35fd6d51":"People say Money Can't Buy Everything. But we can clearly see that Passenegers Of Pclass 1 were given a very high priority while rescue. Even though the number of Passengers in Pclass 3 were a lot higher, still the number of survival from them is very low, somewhere around 25%.\n\nFor Pclass 1 %survived is around 63% while for Pclass2 is around 48%. So money and status matters. Such a materialistic world.\n\nLets Dive in little bit more and check for other interesting observations. Lets check survival rate with Sex and Pclass Together.\n","436fc035":"### Filling Embarked NaN\n\nAs we saw that maximum passengers boarded from Port S, we replace NaN with S.","263d320f":"It is visible that being alone is harmful irrespective of Sex or Pclass except for Pclass3 where the chances of females who are alone is high than those with family.\n\n## Fare_Range\n\nSince fare is also a continous feature, we need to convert it into ordinal value. For this we will use **pandas.qcut**.\n\nSo what **qcut** does is it splits or arranges the values according the number of bins we have passed. So if we pass for 5 bins, it will arrange the values equally spaced into 5 seperate bins or value ranges.","3b7e481b":"We use FactorPlot in this case, because they make the seperation of categorical values easy.\n\nLooking at the CrossTab and the FactorPlot, we can easily infer that survival for Women from Pclass1 is about 95-96%, as only 3 out of 94 Women from Pclass1 died.\n\nIt is evident that irrespective of Pclass, Women were given first priority while rescue. Even Men from Pclass1 have a very low survival rate.\n\nLooks like Pclass is also an important feature. Lets analyse other features.\n","3835071e":"### Dropping UnNeeded Features\n\n**Name**--> We don't need name feature as it cannot be converted into any categorical value.\n\n**Age**--> We have the Age_band feature, so no need of this.\n\n**Ticket**--> It is any random string that cannot be categorised.\n\n**Fare**--> We have the Fare_cat feature, so unneeded\n\n**Cabin**--> A lot of NaN values and also many passengers have multiple cabins. So this is a useless feature.\n\n**Fare_Range**--> We have the fare_cat feature.\n\n**PassengerId**--> Cannot be categorised.","ea9ec6c3":"Now the above correlation plot, we can see some positively related features. Some of them being **SibSp andd Family_Size** and **Parch and Family_Size** and some negative ones like **Alone and Family_Size.**","bea4ad6e":"### Observations:\n\n\nThe barplot and factorplot shows that if a passenger is alone onboard with no siblings, he have 34.5% survival rate. The graph roughly decreases if the number of siblings increase. This makes sense. That is, if I have a family on board, I will try to save them instead of saving myself first. Surprisingly the survival for families with 5-8 members is **0%**. The reason may be Pclass??\n\nThe reason is **Pclass**. The crosstab shows that Person with SibSp>3 were all in Pclass3. It is imminent that all the large families in Pclass3(>3) died.","1185140f":"\n## Contents of the Notebook:\n\n#### Part1: Exploratory Data Analysis(EDA):\n1)Analysis of the features.\n\n2)Finding any relations or trends considering multiple features.\n#### Part2: Feature Engineering and Data Cleaning:\n1)Adding any few features.\n\n2)Removing redundant features.\n\n3)Converting features into suitable form for modeling.\n#### Part3: Predictive Modeling\n1)Running Basic Algorithms.\n\n2)Cross Validation.\n\n3)Ensembling.\n\n4)Important Features Extraction.","b8ea662a":"## Feature Engineering and Data Cleaning\n\nNow what is Feature Engineering?\n\nWhenever we are given a dataset with features, it is not necessary that all the features will be important. There maybe be many redundant features which should be eliminated. Also we can get or add new features by observing or extracting information from other features.\n\nAn example would be getting the Initals feature using the Name Feature. Lets see if we can get any new features and eliminate a few. Also we will tranform the existing relevant features to suitable form for Predictive Modeling.","048695c2":"### Filling NaN Ages"}}