{"cell_type":{"2dbc43b1":"code","aff740e1":"code","faab9bdf":"code","2f20ab05":"code","2d48acc2":"code","a061b5b5":"code","05d0fa8b":"code","dbc2b0f9":"code","a002826b":"markdown"},"source":{"2dbc43b1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\npd.options.display.max_rows = 100","aff740e1":"train1 = pd.read_csv('..\/input\/ames-regression-cleaned\/train.1H.noskew.csv', \n                     index_col='Id')\ntest1 = pd.read_csv('..\/input\/ames-regression-cleaned\/test.1H.noskew.csv', \n                    index_col='Id')\ntrain2 = pd.read_csv('..\/input\/ames-regression-cleaned\/train.1H.skew.csv', \n                     index_col='Id')\ntest2 = pd.read_csv('..\/input\/ames-regression-cleaned\/test.1H.skew.csv', \n                    index_col='Id')\ntrain3 = pd.read_csv('..\/input\/ames-regression-cleaned\/train.ALL.outno.csv', \n                     index_col='Id')\ntest3 = pd.read_csv('..\/input\/ames-regression-cleaned\/test.ALL.outno.csv', \n                    index_col='Id')\ntrain4 = pd.read_csv('..\/input\/ames-regression-cleaned\/train.ALL.outyes.csv', \n                     index_col='Id')\ntest4 = pd.read_csv('..\/input\/ames-regression-cleaned\/test.ALL.outyes.csv', \n                    index_col='Id')\ntrain5 = pd.read_csv('..\/input\/ames-regression-cleaned\/train.TA.noskew.csv', \n                     index_col='Id')\ntest5 = pd.read_csv('..\/input\/ames-regression-cleaned\/test.TA.noskew.csv', \n                    index_col='Id')\ntrain6 = pd.read_csv('..\/input\/ames-regression-cleaned\/train.TA.skew.csv', \n                     index_col='Id')\ntest6 = pd.read_csv('..\/input\/ames-regression-cleaned\/test.TA.skew.csv', \n                    index_col='Id')\n\ntrain_sets = [train1, train2, train3, train4, train5, train6]\ntest_sets = [test1, test2, test3, test4, test5, test6]\nset_names = ['1H.noskew', '1H.skew', 'ALL.outno', 'ALL.outyes', 'TA.noskew', 'TA.skew']","faab9bdf":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.model_selection import cross_val_score\n\nkfolds = KFold(n_splits=5, shuffle=True, random_state=7)\n\ndef rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(np.expm1(y), np.expm1(y_pred)))\n\ndef rmsle(y, y_pred):  # because y and y_pred have been log transformed already\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X, y):\n    rmsle = np.sqrt(-cross_val_score(model, X, y,\n                                    scoring=\"neg_mean_squared_error\",\n                                    cv=kfolds))\n    return rmsle","2f20ab05":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import Ridge, ElasticNet, Lasso, LassoLars\nfrom hyperopt import hp,fmin,tpe,STATUS_OK,Trials\nfrom hyperopt.pyll.base import scope","2d48acc2":"space_lasso = {\n    'alpha' : hp.loguniform('alpha', -10, 10)\n}\nspace_ridge = {\n    'alpha' : hp.loguniform('alpha', -10, 10)\n}\nspace_elastic = {\n    'alpha' : hp.loguniform('alpha', -10, 10),\n    'l1_ratio' : hp.uniform('l1_ratio', 0, 1)\n}\n\nspace_xgb = {\n    'max_depth' : scope.int(hp.uniform('max_depth', 1, 11)),\n    'learning_rate' : hp.loguniform('learning_rate', np.log(0.0001), np.log(0.5)) - 0.0001,\n    'n_estimators' : scope.int(hp.quniform('n_estimators', 100, 6000, 200)),\n    'gamma' : hp.loguniform('gamma', np.log(0.0001), np.log(5)) - 0.0001,\n    'min_child_weight': scope.int(hp.loguniform('min_child_weight', np.log(1), np.log(100))),\n    'subsample': hp.uniform('subsample', 0.5, 1), \n    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1), \n    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1), \n    'reg_alpha': hp.loguniform('reg_alpha', np.log(0.0001), np.log(1)) - 0.0001, \n    'reg_lambda': hp.loguniform('reg_lambda', np.log(1), np.log(4))\n}\n\nspace_lgbm = {\n    'boosting_type': hp.choice('boosting_type', ['gbdt', 'goss']),\n    'max_depth' : scope.int(hp.uniform('max_depth', 1, 11)),\n    'num_leaves' : scope.int(hp.uniform('num_leaves', 2, 121)),\n    'learning_rate' : hp.loguniform('learning_rate', np.log(0.0001), np.log(0.5)) - 0.0001,\n    'n_estimators' : scope.int(hp.quniform('n_estimators', 100, 6000, 200)),\n    'min_split_gain' : hp.loguniform('min_split_gain', np.log(0.0001), np.log(5)) - 0.0001,\n    'min_child_weight': hp.loguniform('min_child_weight', np.log(0.0001), np.log(10)),\n    'subsample': hp.uniform('subsample', 0.5, 1), \n    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1), \n    'reg_alpha': hp.loguniform('reg_alpha', np.log(0.0001), np.log(1)) - 0.0001, \n    'reg_lambda': hp.loguniform('reg_lambda', np.log(1), np.log(4)),\n}","a061b5b5":"from xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor","05d0fa8b":"params = []\n\nfor set_name, train, test in zip(set_names, train_sets, test_sets):\n    y = train.pop('SalePrice')\n    X = train\n    \n    def objective(space_):\n        rmsle = cv_rmse(LGBMRegressor(**space_, random_state=7), X, y).mean()\n        return {'loss': rmsle, 'status': STATUS_OK }\n\n    trials = Trials()\n    best = fmin(fn = objective, space = space_lgbm, algo = tpe.suggest, \n                max_evals = 250, trials = trials)\n    best['boosting_type'] = ['gbdt', 'goss'][best['boosting_type']]\n    print('For LGBMRegressor, with', set_name, 'the best parameters are', best, '\\n')\n    \n    best['set_name'] = set_name\n    best['best_loss'] = min(trials.losses())\n    params.append(best)\n    \n    best_predictor = LGBMRegressor(boosting_type = best['boosting_type'],\n                               colsample_bytree = best['colsample_bytree'],\n                               learning_rate = best['learning_rate'],\n                               max_depth = int(best['max_depth']),\n                               min_child_weight = best['min_child_weight'],\n                               min_split_gain = best['min_split_gain'],\n                               n_estimators = int(best['n_estimators']),\n                               num_leaves = int(best['num_leaves']),\n                               reg_alpha = best['reg_alpha'],\n                               reg_lambda = best['reg_lambda'],\n                               subsample = best['subsample'],\n                               random_state=7)\n    best_predictor.fit(X, y)\n    y_pred = best_predictor.predict(test)\n    submission = pd.read_csv('..\/input\/ames-regression-cleaned\/sample_submission.csv', \n                             index_col='Id')\n    submission['SalePrice'] = np.expm1(y_pred)\n    submission.to_csv(f'LGBMRegressor.{set_name}.csv')","dbc2b0f9":"pd.DataFrame(params)[['set_name', 'best_loss', 'colsample_bytree', 'learning_rate', 'max_depth', 'min_child_weight', 'min_split_gain', 'n_estimators', 'num_leaves', 'reg_alpha', 'reg_lambda', 'subsample']]","a002826b":"### Running gradient boost models : XGBoost\nTuning the hyperparameters with `Hyperopt`"}}