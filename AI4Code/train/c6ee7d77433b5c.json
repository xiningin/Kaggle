{"cell_type":{"22ce26ee":"code","93e8a6bb":"code","d6bf6474":"code","ed647071":"code","b5042033":"code","f6396dd3":"code","b157dc8e":"code","2e0277ba":"code","dab437f4":"code","73034caa":"code","6bfc9766":"code","9fced0a4":"code","b231fc03":"code","a1ea331e":"code","4e4eba09":"code","6146fecb":"code","6e809bd8":"code","c92d35ee":"code","19f0aeeb":"code","7bd3edf6":"code","d6aec8aa":"code","d8459f18":"code","0947e607":"code","abeb6422":"code","aa3e85fe":"code","72b585c9":"code","182df257":"code","1109f9ba":"code","b0e46275":"code","e6dbaa0a":"code","027d26ea":"code","a4d05152":"code","638c04a5":"code","37b4b359":"code","36544cc1":"code","bf7bf865":"code","305a43f1":"code","208b010d":"code","816ef9dd":"code","dc96dfbb":"code","30a84983":"code","28ebb83d":"code","5ee60f30":"code","f076056b":"code","b5531ac8":"code","0266f706":"code","eca2eed4":"code","9bc0f981":"code","e677dad3":"code","d6135c28":"code","5a4b9f18":"code","a9af774b":"code","1be44a9b":"code","79ef7e15":"code","69e49712":"code","314dc96e":"code","7bed8373":"code","b1efbece":"code","e2da6fa1":"code","cd06b4a1":"code","e888bd71":"code","d6081553":"code","b2d344bb":"code","363d64f3":"code","093c54c8":"code","a88f76c3":"code","64f8772b":"code","73f65c43":"code","14d5b83c":"code","7b1ad8fa":"code","05898443":"code","5004f151":"code","83abc989":"code","b8fc8522":"code","bd7d2b04":"code","ca9c2bee":"code","358bb3a0":"code","b2465f25":"code","20f5f2ca":"code","b10a06c8":"code","e87d30a1":"code","842ffe46":"code","a00a1c80":"code","1b303190":"code","adfcfb7e":"code","87f8be4e":"code","a24644e5":"code","05a3db43":"code","2ae4f483":"code","f088b051":"code","cc3fb514":"code","1bcb43bb":"code","4fc9689c":"code","89146c2a":"code","bebc61b2":"code","f8979061":"code","c1bcccd2":"code","ca77e590":"code","5d801496":"code","ee5210bd":"code","e32d68de":"code","f8a828ea":"code","6c967b9f":"code","a74002db":"code","8034f2a4":"code","323df39d":"code","1fa3cad2":"code","d317302b":"code","9a2b6520":"code","76121310":"code","a5775d91":"code","5e1f740c":"code","29c90d61":"code","cf470745":"code","437636aa":"code","5539826d":"code","79bf6c0e":"code","c6d7b66f":"code","578adb16":"code","1e6e0dd3":"code","981dd6fe":"code","2cc8f60a":"code","dc3e75f7":"code","f9a4eba8":"code","31f4a04a":"code","60220abe":"code","23d0477b":"code","d7c1a3c2":"code","b5148e2f":"code","523dae61":"code","ad31045f":"code","cd2c829e":"code","570acd4c":"code","4a94b193":"code","227ab984":"code","1dc7be96":"code","3ded852d":"code","b29fb839":"code","6594fcae":"markdown","6ff93a6d":"markdown","4019441f":"markdown","a49ad12b":"markdown","ebc59538":"markdown","20202a65":"markdown","642b17bf":"markdown","9571e18a":"markdown","93418b73":"markdown","5f98c9f4":"markdown","05bad932":"markdown","72e9c72b":"markdown","a3d3bdb3":"markdown","7779796c":"markdown","b9652991":"markdown","16394e61":"markdown","d0849c08":"markdown","b1fc479a":"markdown","69b9d207":"markdown"},"source":{"22ce26ee":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport sys\nimport tensorflow as tf\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nprint(\"Started\")\nprint(os.getcwd())\n#print(os.fspath('..'))\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    #print(\"In loop\")\n    #print(filenames)\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n","93e8a6bb":"# Other import stuff\n\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Layer, Dense, Embedding, Input, LSTM, Masking, BatchNormalization, Dropout, Activation\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.optimizers import Adam, Nadam\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\nfrom tensorflow.keras.constraints import max_norm\nimport time\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# for jupyter notebook\n#%matplotlib notebook\n# for jupyter-lab\n#%matplotlib widget\n# Import the train_test_split function and uncomment\nfrom sklearn.model_selection import train_test_split","d6bf6474":"# Pre-trained models, etc. from sklearn\n\nimport sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, SVR, LinearSVC, LinearSVR\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier, SGDRegressor\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn import tree\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV","ed647071":"# Other imports\n\nfrom tensorflow_addons.metrics import RSquare # tensorflow addons\nimport keras_tuner as kt\n\n# Load the TensorBoard notebook extension\n%load_ext tensorboard\n\n# Clear any logs from previous runs\n!rm -rf .\/logs\/\n\nfrom tensorboard.plugins.hparams import api as hp\n\nimport category_encoders as ce","b5042033":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\nPassengerId_train = train_data.PassengerId\ntrain_data = train_data.set_index('PassengerId')\ntrain_data_original = train_data.copy()\ntrain_data['is_test'] = 0\ndisplay(train_data)","f6396dd3":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nPassengerId_test = test_data.PassengerId\ntest_data = test_data.set_index('PassengerId')\ntest_data_original = test_data.copy()\ntest_data['is_test'] = 1\ndisplay(test_data)","b157dc8e":"# Compare the train and test data\n\ndisplay(train_data.describe())\ndisplay(test_data.describe())","2e0277ba":"def build_age_prediction_regression_model(hp):\n    model = Sequential()\n    hp_kernel_initializer = hp.Choice('kernel_initializer', init_mode)\n    hp_activation = hp.Choice('activation', activation)\n    hp_kernel_constraint = hp.Choice('kernel_constraint', weight_constraint)\n    hp_dropout_rate = hp.Choice('dropout_rate', dropout_rate)\n    hp_learning_rate = hp.Choice('learning_rate', values=learning_rate)\n    hp_l2_regularizer = hp.Choice('l2_weight_decay', values=l2_weight_decay)\n    \n    # Input layer\n    \n    model.add(Dense(input_dim=input_dim, \n                    units=hp.Int('units_input', min_value=32, max_value=1280, step=32),\n                    kernel_regularizer=regularizers.l2(hp_l2_regularizer),\n                    kernel_initializer=hp_kernel_initializer,\n                    bias_initializer='zeros', \n                    activation=hp_activation,\n                    kernel_constraint=max_norm(hp_kernel_constraint)))\n    model.add(Dropout(rate=hp_dropout_rate))\n    \n    # Hidden layers\n    \n    for i in range(hp.Int('layers', 0, 3)):\n        model.add(Dense(units=hp.Int('units_' + str(i), min_value=32, max_value=1280, step=32), \n                        kernel_regularizer=regularizers.l2(hp_l2_regularizer),\n                        kernel_initializer=hp_kernel_initializer,\n                        bias_initializer='zeros', \n                        activation=hp_activation,\n                        kernel_constraint=max_norm(hp_kernel_constraint) \n                        ))\n        model.add(Dropout(rate=hp_dropout_rate))\n    \n    # Add the output layer\n    \n    model.add(Dense(units=1, \n                    kernel_regularizer=regularizers.l2(hp_l2_regularizer),\n                    kernel_initializer=hp_kernel_initializer, \n                    bias_initializer='zeros', \n                    activation='linear'))\n\n    # Optimizer\n\n    hp_optimizer = hp.Choice('optimizer', optimizer)\n\n    model.compile(loss='mean_squared_error', \n            optimizer=hp_optimizer,\n            metrics=['accuracy'])\n    return model","dab437f4":"def build_survival_prediction_classification_model(hp):\n    model = Sequential()\n    hp_kernel_initializer = hp.Choice('kernel_initializer', init_mode)\n    hp_activation = hp.Choice('activation', activation)\n    hp_kernel_constraint = hp.Choice('kernel_constraint', weight_constraint)\n    hp_dropout_rate = hp.Choice('dropout_rate', dropout_rate)\n    hp_learning_rate = hp.Choice('learning_rate', values=learning_rate)\n    hp_l2_regularizer = hp.Choice('l2_weight_decay', values=l2_weight_decay)\n\n    # Input layer\n\n    model.add(Dense(input_dim=input_dim, \n                    units=hp.Int('units_input', min_value=32, max_value=1600, step=32),\n                    kernel_regularizer=regularizers.l2(hp_l2_regularizer),\n                    kernel_initializer=hp_kernel_initializer,\n                    bias_initializer='zeros', \n                    activation=hp_activation,\n                    kernel_constraint=max_norm(hp_kernel_constraint)))\n    model.add(Dropout(rate=hp_dropout_rate))\n    \n    # Hidden layers\n    \n    for i in range(hp.Int('layers', 0, 3)):\n        model.add(Dense(units=hp.Int('units_' + str(i), min_value=32, max_value=1280, step=32), \n                        kernel_regularizer=regularizers.l2(hp_l2_regularizer),\n                        kernel_initializer=hp_kernel_initializer,\n                        bias_initializer='zeros', \n                        activation=hp_activation,\n                        kernel_constraint=max_norm(hp_kernel_constraint) \n                        ))\n        model.add(Dropout(rate=hp_dropout_rate))\n    \n    # Add the output layer\n\n    model.add(Dense(units=1, \n                    kernel_regularizer=regularizers.l2(hp_l2_regularizer),\n                    kernel_initializer=hp_kernel_initializer, \n                    bias_initializer='zeros', \n                    activation='sigmoid'))\n\n    # Optimizer\n\n    hp_optimizer = hp.Choice('optimizer', optimizer)\n\n    model.compile(loss='binary_crossentropy', \n                optimizer=hp_optimizer,\n                metrics=['binary_accuracy'])\n    return model","73034caa":"# Setup callback\n\ndef get_callbacks():\n    \"\"\"\n    This function should create and return a tuple (early_stopping, learning_rate_reduction) callbacks.\n    The callbacks should be instantiated according to the above requirements.\n    \"\"\"\n    \n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=100, verbose=1)\n\n    learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=5, verbose=1)\n    \n    checkpoint_path = 'checkpoints_every_epoch\/checkpoint_{epoch:03d}'\n    checkpoint_epoch = ModelCheckpoint(filepath=checkpoint_path,\n                                 frequency='epoch',\n                                 save_weights_only=True,\n                                 verbose=0)\n    \n    checkpoint_best_path = 'checkpoints_best_only\/checkpoint'\n    checkpoint_best = ModelCheckpoint(filepath=checkpoint_best_path,\n                                      save_weights_only=True,\n                                      save_freq='epoch',\n                                      monitor='val_loss',\n                                      save_best_only=True,\n                                      verbose=0)\n\n    #return [early_stopping, learning_rate_reduction, checkpoint_epoch, checkpoint_best]\n    \n    return [early_stopping, learning_rate_reduction, checkpoint_epoch, checkpoint_best]\n\n\n#callbacks = get_callbacks()","6bfc9766":"# Define a 'Score' function similar to Sklearn's RandomForestRegressor, etc. for regression analysis\n\ndef coeff_determination(y_true, y_pred):\n    SS_res =  np.sum(np.square( y_true-y_pred ))\n    SS_tot = np.sum(np.square( y_true - np.mean(y_true) ) )\n    return (round( (1 - SS_res\/(SS_tot + 1e-7)) * 100, 2))\n\n'''\n# Compute R^2 - for my edification\n\nresidual_sum_of_squares = np.sum(np.square(np.squeeze(y_with_age.values) - np.squeeze(age_model_best.predict(X_with_age))))\ntotal_sum_of_squares = np.sum(np.square(np.squeeze(y_with_age.values) - np.mean(y_with_age.values)))\nR_squared = 1 - (residual_sum_of_squares\/total_sum_of_squares)\nprint(round(R_squared*100,2))\n\n\n# Estimate coefficient of datermination (R-squared)\n\nage_model_best = hypermodel\nloss_rsquare_my_age_model = RSquare()\nloss_rsquare_my_age_model.update_state(np.squeeze(y_with_age.values), np.squeeze(age_model_best.predict(X_with_age)))\nacc_my_age_model = round(loss_rsquare_my_age_model.result().numpy() * 100, 2)\n'''","9fced0a4":"# Analyze a 'feature_name' for 'Survived' rate by classification, without modifying the 'feature_name'\n\ndef analyze_feature_name(dataset, feature_name):\n    feature_name = feature_name\n    dataset = dataset\n    list_feature_name = dataset[feature_name].unique()\n    list_feature_name = np.sort(list_feature_name)\n    i_feature_name = 0\n    for feature in list_feature_name:\n        filter = dataset[feature_name] == feature\n        temp_df = dataset.loc[:, [feature_name, 'Survived']]\n        temp_df = temp_df.where(filter)\n        print(feature_name, \"-\",feature, \" survival rate = {0:4.1f}%\".format((temp_df.Survived.sum()\/temp_df.Survived.count())*100))\n        i_feature_name += 1","b231fc03":"# Convert string in a 'feature_name' (such as 'Name') to pick out the last name of the person and analyze the 'feature_name'\n\ndef analyze_feature_name_to_return_last_name(dataset, feature_name):\n    feature_name = feature_name\n    dataset = dataset\n    dataset.loc[:, feature_name] = dataset[feature_name].apply(lambda p: p[:(p.find(','))])\n    \n    # Analyze the data\n    \n    list_feature_name = dataset[feature_name].unique()\n    i_feature_name = 0\n    for feature in list_feature_name:\n        filter = dataset[feature_name] == feature\n        temp_df = dataset.loc[:, [feature_name, 'Survived']]\n        temp_df = temp_df.where(filter)\n        print(feature_name, \"-\",feature, \" survival rate = {0:4.1f}%\".format((temp_df.Survived.sum()\/temp_df.Survived.count())*100))\n        i_feature_name += 1\n        ","a1ea331e":"# Convert string in a 'feature_name' to pick out the first 3 letters analyze the 'feature_name' (such as 'Ticket')\n\ndef analyze_feature_name_to_return_first_3_letters(dataset, feature_name):\n    feature_name = feature_name\n    dataset = dataset\n    dataset.loc[:, feature_name] = dataset[feature_name].apply(lambda p: str(p)[:3])\n    \n    # Analyze the data\n    \n    list_feature_name = dataset[feature_name].unique()\n    i_feature_name = 0\n    for feature in list_feature_name:\n        filter = dataset[feature_name] == feature\n        temp_df = dataset.loc[:, [feature_name, 'Survived']]\n        temp_df = temp_df.where(filter)\n        print(feature_name, \"-\",feature, \" survival rate = {0:4.1f}%\".format((temp_df.Survived.sum()\/temp_df.Survived.count())*100))\n        i_feature_name += 1","4e4eba09":"# Convert a unique class (string or numerical) 'feature_name' to categorical\n\ndef convert_unique_feature_name_to_caterorical(dataset, feature_name):\n    feature_name = feature_name\n    dataset = dataset\n    unique_names = dataset[feature_name]\n    list_feature_name = unique_names.unique()\n    list_feature_name = np.sort(list_feature_name)\n    i_feature_name = 0\n    for feature in list_feature_name:\n        dataset[feature_name].replace(feature, i_feature_name, inplace=True)\n        i_feature_name += 1","6146fecb":"# Convert a numerical-value 'feature_name' to categorical based on ranges\n\n# This is another way to do the same:\n\n#dataset['AgeBand'] = pd.cut(dataset['Age'], 5)\n#dataset[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)\n\ndef convert_numerical_feature_name_to_categorical(dataset, feature_name, min_value, max_value, steps):\n    feature_name = feature_name\n    dataset = dataset\n    min_value = min_value\n    max_value = max_value\n    steps = steps\n    \n    step_length = (max_value - min_value)\/steps\n    for step_count in np.arange(steps):\n        dataset.loc[((dataset[feature_name] > (step_count - 1e-8)*(step_length)) & ((dataset[feature_name] <= (step_count+1)*step_length))), feature_name] = step_count\n\n    return (dataset)","6e809bd8":"# One hot encode a list of features\n\ndef convert_feature_to_one_hot(database, features):\n    features = features\n    database = database\n    i = 0\n    for feature in features:\n        #print(feature)\n        #print(database.shape)\n        #display(database.head())\n        feature_one_hot = pd.get_dummies(database[feature], prefix=feature)\n        #print(feature_one_hot.shape)\n        #display(feature_one_hot)\n        database = database.drop(feature, axis=1)\n        database = database.join(feature_one_hot)\n        #display(database)\n        #print(database.shape)\n            \n    return database","c92d35ee":"# Convert data to binary a list of features based on ordinal values\n\ndef convert_feature_to_binary(database, features):\n    features = features\n    database = database\n    i = 0\n    for feature in features:\n        encoder = ce.BinaryEncoder(cols=[feature])\n        database = encoder.fit_transform(database)\n        \n    return database","19f0aeeb":"#print(train_data.shape)\nlist_columns = list(train_data.columns)\n#print(list_columns)\nfor column in list_columns:\n    print(\"train_data Nan in \", column, \" = \", train_data[column].isna().sum())\n    if column != 'Survived':\n        print(\"test_data Nan in \", column, \" = \", test_data[column].isna().sum())","7bd3edf6":"# Fill or remove missing data in train_data and test_data for items that will be used in our analyses:\n\n# 'PassengerId': will not be updated since it will not be used in analyses\n# test_data currently does not have Survived column.\n\n# The following features need to be updated for Nan values:\n#     Age - this is an important feature and it will be updated by estimting the missing values.\n#     Fare - only one value is missing. An average value will be assigned based on the Pclass of the missing Fare value.\n#     Cabin - a vast majority of the values for Cabin are 'Nan'. We will simply remove this column from our evaluation.\n#     Embarked - we will replace the Nan values with the most frequent (mode) value for Embarked.","d6aec8aa":"# All the features will be updated, as required, after combining the test_data and train_data into one combined all_data\n# You can use a different approach from what is shown below. You can make a combined 'list' like all_data = [train_data, test_data] and do the same operations on the\n# combined list. You may have to use 'with dataset in all_data:' to do the operations on both datasets.\n\nall_data = pd.concat((train_data, test_data), axis=0, sort=False) # NOTE: this will add a 'Survived' column witn NaN for test_data portion of data.\n\n# 'Pclass' in all_data will be updated using the most frequent value to replace NaN - THIS IS NOT REQUIRED SINCE THERE ARE NO NaN VALUES.\nall_data[['Pclass']] = all_data[['Pclass']].fillna(value=all_data['Pclass'].value_counts().idxmax())\n\n# 'Name' in all_data will be updated using 'Unknown' to replace NaN\nall_data[['Name']] = all_data[['Name']].fillna('Unknown')\n\n# 'Age': is an important feature with many missing data (NaN). Build various models to predict 'Age' for all NaN in all_data.\n\n# 'SibSp' in all_data will be updated using the most frequent value (mode) to replace NaN - THIS IS NOT REQUIRED SINCE THERE ARE NO NaN VALUES.\nall_data[['SibSp']] = all_data[['SibSp']].fillna(value=all_data['SibSp'].value_counts().idxmax())\n\n# 'Parch' in all_data will be updated using the most frequent value (mode) to replace NaN - THIS IS NOT REQUIRED SINCE THERE ARE NO NaN VALUES.\nall_data[['Parch']] = all_data[['Parch']].fillna(value=all_data['Parch'].value_counts().idxmax())\n\n# 'Ticket': will not be currently updated. If required, it will be modified for analysis or it will be removed.\n\n# 'Fare': in all_data will be updated using the mean value for each 'Pclass'.\nFare_mean = all_data.groupby('Pclass')['Fare'].mean()\nall_data['Fare'].fillna(all_data['Pclass'].map(Fare_mean), inplace=True)\n\n### Another way to do this is using 'transform':\n### Fare_mean = all_data.groupby('Pclass')['Fare'].transform('mean')\n### all_data['Fare'].fillna(Fare_mean, inplace=True)\n\n##### One more option: all_data[\"Fare\"] = all_data.groupby(\"Pclass\").transform(lambda x: x.fillna(x.mean()))\n\n# 'Cabin': has too many NaNs will not be used in analyses\n\n# 'Embarked' in all_data will be updated using the most frequent value to replace NaN\nall_data[['Embarked']] = all_data[['Embarked']].fillna(value=all_data['Embarked'].value_counts().idxmax())\n\n# 'is_test': will not be updated since it will not be used in analyses\n\nall_data","d8459f18":"list_columns = list(all_data.columns)\n#print(list_columns)\nfor column in list_columns:\n    print(\"all_data Nan in \", column, \" = \", all_data[column].isna().sum())","0947e607":"# Analyze 'Name' and convert to categorical based on last name\n\nall_data_copy = all_data.copy()\ndisplay(all_data_copy.head())\nanalyze_feature_name_to_return_last_name(all_data_copy, 'Name')\ndisplay(all_data_copy.head())","abeb6422":"# As seen above, there are hundreds of last names and they are quite useless in predicting the survivability of a person. Discard 'all_data_copy'.\n# Instead of categorizing last names, let us extract the 'Title' of each person. Titles are generally related to the age of a person\n# and may be useful in estimating the 'Age' feature as well as the 'Survived' feature.\n\ndel all_data_copy\n\nall_data['Title'] = all_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(all_data['Title'], all_data['Sex'])","aa3e85fe":"# Analyze 'Title'\n\n'''\nThis code is for if 'train' and 'test' are combined into a 'list' of datasets.\n\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col', \n                                                 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n'''\n\nall_data['Title'] = all_data['Title'].replace(['Lady', 'Countess','Capt', 'Col', \n                                             'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\nall_data['Title'] = all_data['Title'].replace('Mlle', 'Miss')\nall_data['Title'] = all_data['Title'].replace('Ms', 'Miss')\nall_data['Title'] = all_data['Title'].replace('Mme', 'Mrs')\n    \ndisplay(all_data[['Title', 'Survived']].groupby(['Title'], as_index=False).mean())\ndisplay(all_data[['Title', 'Age']].groupby(['Title'], as_index=False).min())\ndisplay(all_data[['Title', 'Age']].groupby(['Title'], as_index=False).max())\ndisplay(all_data[['Title', 'Age']].groupby(['Title'], as_index=False).mean())","72b585c9":"# From above, 'Title' is usefull in estimating 'Age'\n\n# Convert Title to categorical-nominal values\n\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n#for dataset in combine:\n#    dataset['Title'] = dataset['Title'].map(title_mapping)\n#    dataset['Title'] = dataset['Title'].fillna(0)\n\nall_data['Title'] = all_data['Title'].map(title_mapping)\nall_data['Title'] = all_data['Title'].fillna(0)\n\nall_data.head()","182df257":"display(all_data)","1109f9ba":"# Evaluate 'Embarked'\n\ndisplay(all_data.head())\nanalyze_feature_name(all_data, 'Embarked')","b0e46275":"# Convert 'Embarked' to categorical-nominal since survival rate is dependent on feature 'Embarked'\n\nconvert_unique_feature_name_to_caterorical(all_data, 'Embarked')\ndisplay(all_data.head())","e6dbaa0a":"# Analyze 'Sex'\ndisplay(all_data)\nanalyze_feature_name(all_data, 'Sex')","027d26ea":"# Clearly survival rate is dependent on 'Sex'. Convert 'Sex' to categorical-nominal\n\nconvert_unique_feature_name_to_caterorical(all_data, 'Sex')\ndisplay(all_data)\n\n# Could have also done it this way:\n#all_data['Sex'] = all_data['Sex'].map( {'female': 1, 'male': 0} ).astype(int)","a4d05152":"# Analyze 'Pclass'.\n\ndisplay(all_data.head())\nanalyze_feature_name(all_data, 'Pclass')","638c04a5":"# Clearly survival rate is dependent on 'Pclass'. But, Pclass has only three values 1, 2 and 3. So there is no need to categorize.\n\n#convert_unique_feature_name_to_caterorical(train_data, 'Pclass')\ndisplay(all_data.head())","37b4b359":"# Analyze 'SibSp'.\n\ndisplay(all_data)\nanalyze_feature_name(all_data, 'SibSp')","36544cc1":"# Survival rate is dependent on 'SibSp'. But, 'SibSp' has only six values 0 to 5 and 8. So there is no need to categorize.\n# Later on this will be binarized to reduce it to four features.\n\n#convert_unique_feature_name_to_caterorical(all_data, 'SibSp')\ndisplay(all_data)","bf7bf865":"# Analyze 'Parch'.\n\ndisplay(all_data)\nanalyze_feature_name(all_data, 'Parch')","305a43f1":"# Survival rate is dependent on 'Parch'. But, 'Parch' has only eight values 0 to 6 and 9. So there is no need to categorize.\n# Later on this will be binarized to reduce it to four features.\n\n#convert_unique_feature_name_to_caterorical(all_data, 'Parch')\ndisplay(all_data)","208b010d":"# Combine SibSp and Parch as 'Family_members'\n\nall_data['Family_members'] = all_data['SibSp'] + all_data['Parch']\ndisplay(all_data)","816ef9dd":"# Drop SibSp and Parch\n\nall_data = all_data.drop(['SibSp', 'Parch'], axis=1)\ndisplay(all_data)","dc96dfbb":"# Analyze 'Family_members'.\n\ndisplay(all_data)\nanalyze_feature_name(all_data, 'Family_members')","30a84983":"# Survival rate is dependent on 'Family_members'. But, 'Family_members' has only eight values 0 to 7 and 10. So there is no need to categorize.\n# Later on this will be binarized to reduce it to four features.\n\n#convert_unique_feature_name_to_caterorical(all_data, 'Parch')\ndisplay(all_data)","28ebb83d":"# Almost all 'Fare' values are unique. So 'Fare' will be converted to categorical-range and analyzed.\n# Later on this will be binarized to reduce it to four features.\n\ndisplay(all_data.head())\nconvert_numerical_feature_name_to_categorical(all_data, 'Fare', 0, 520, 16)\nanalyze_feature_name(all_data, 'Fare')\ndisplay(all_data.head())","5ee60f30":"# Obtain the first 3 letter in 'Ticket'. Then convert 'Ticket' to categorical.\n\nall_data_copy = all_data.copy()\ndisplay(all_data_copy)\nanalyze_feature_name_to_return_first_3_letters(all_data_copy, 'Ticket')","f076056b":"# From above, the ticket numbers have to correlation to survivability or 'Age'. So this will be dropped. Delete 'all_data_copy'.\n\ndel all_data_copy\n#convert_unique_feature_name_to_caterorical(all_data, 'Ticket')\ndisplay(all_data.head())","b5531ac8":"# Remove unwanted columns:\n#     Remove 'Ticket'\n#     Remove 'Cabin'\n#     Now that 'Title' have be extracted, remove 'Name'\n\nall_data.drop('Ticket', axis=1, inplace=True)\nall_data.drop('Cabin', axis=1, inplace=True)\nall_data.drop('Name', axis=1, inplace=True)\n\ndisplay(all_data)","0266f706":"# One hot encode some of the features\n\n#print(all_data.shape)\n#display(all_data.head())\nfeatures = ['Pclass', 'Sex', 'Embarked']\nall_data = convert_feature_to_one_hot(all_data, features)\ndisplay(all_data)","eca2eed4":"# Binarize some features.\n\nprint(all_data.shape)\ndisplay(all_data.head())\nfeatures = ['Family_members', 'Fare', 'Title']\nall_data = convert_feature_to_binary(all_data, features)\ndisplay(all_data)","9bc0f981":"print(all_data.columns)","e677dad3":"# 'Age' is an important feature. Build model to predict missing ages and fill in the missing data.\n# Generate DataFrames for 'Age' analyses and prediction\n\n# Based on intuition, a combination of 'Title', Pclass' and 'Fare' should provide a reasonable estimate of age. It is assumed \n# that for each passenger class (1,2,3) the ticket value will be based on age (discounted rates for children and seniors, \n# regular rates for adults). Although the charts don't really show this. We will let ML figure this out.\n\n# None of the other features are in any way mathematically related to 'Age'\n\n# We will use the following steps to estimate 'Age'.\n# 1 - Create a new DataFrame from 'all_data'\n# 2 - Separate out rows 'with age' and 'without age'.\n# 3 - Split the 'with age' data into 'train' and 'check' sets.\n\nfeatures_for_Age = ['Title', 'Pclass', 'Fare', 'Age']\nfeatures_for_Age_columns = []\nfor feature in features_for_Age:\n    features_for_Age_columns.extend([col for col in all_data.columns if feature in col])\n#print(features_for_Age_columns)\n\ndf_for_age_all = all_data[features_for_Age_columns]\n#display(df_for_age)\n\n# DataFrame of data with 'Age'\ndf_with_age = df_for_age_all.dropna(subset=['Age'])\n\n# DataFrame of data without 'Age'\ndf_without_age =df_for_age_all.loc[df_for_age_all['Age'].isna()]\ndf_without_age.pop('Age')\n#display(df_without_age)\n\n# Split the 'with age' data into 'train' and 'check' sets.\n\nX_with_age_to_split = df_with_age.copy()\ny_with_age_to_split = X_with_age_to_split.pop('Age')","d6135c28":"# Experiment dropping some features.\n'''\nfeatures_for_Age_to_drop = ['Title', 'Pclass', 'Fare', 'Age']\nfeatures_for_Age_to_drop_columns = []\nfor feature in features_for_Age_to_drop:\n    features_for_Age_to_drop_columns.extend([col for col in all_data.columns if feature in col])\n#print(features_for_Age_columns)\n\ndf_for_age = all_data[features_for_Age_to_drop_columns]\n#display(df_for_age)\n'''","5a4b9f18":"# Split a small chunk for independent 'checking'. Set the random_state in such a way that the percent suvived in the 'train_y' split is equal to about 38.3%\n'''\nimport math\nfor i in np.arange(10000):\n    train_X_temp, check_X_temp, train_y_temp, check_y_temp = train_test_split(X_with_age_to_split, y_with_age_to_split, test_size=0.2, random_state=i)\n    if((math.isclose(np.mean(train_y_temp), np.mean(check_y_temp), abs_tol=1e-2))):\n        print(f'train_y_temp mean age = {np.mean(train_y_temp):.2f}')\n        print(f'check_y_temp mean age = {np.mean(check_y_temp):.2f}')\n        print(f\"random_state={i}\")\n        break\n        \n# This gives the result: random_state = 82\n#'''","a9af774b":"X_with_age_train, X_with_age_check, y_with_age_train, y_with_age_check = train_test_split(X_with_age_to_split, \\\n                                                                                                      y_with_age_to_split, test_size=0.2, random_state=82)\n\n# Specific data for 'Age' analyses and prediction - NO NEED TO PERFORM StandardScaler conversion\n'''\nX_with_age_train = train_X_df_with_age[['Pclass', 'Fare', 'Title']]\nsc = StandardScaler()\nX_with_age_train = sc.fit_transform(X_with_age_train)\ny_with_age_train = train_y_df_with_age\n\nX_with_age_check = check_X_df_with_age[['Pclass', 'Fare', 'Title']]\nX_with_age_check = sc.transform(X_with_age_check)\ny_with_age_check = check_y_df_with_age\n\n\nsc_without_age = df_without_age[['Pclass', 'Fare', 'Title']]\nsc_without_age = sc.transform(sc_without_age)\n'''","1be44a9b":"# define the grid search parameters\n'''\n# Partial list of available options\n\ninput_dim = X_with_age.shape[1]\nbatch_size = [10, 20, 40, 60, 80, 100]\nepochs = [10, 50, 100]\noptimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\nlearn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\nmomentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\ninit_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\nactivation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\nweight_constraint = [1, 2, 3, 4, 5]\ndropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\nneurons = [1, 5, 10, 15, 20, 25, 30]\n'''\n\n#'''\n# First try\n\ninput_dim = X_with_age_train.shape[1]\n#epochs = [100, 200]\noptimizer = ['Adam', 'Nadam'] #\nlearning_rate = [0.000001, 0.0001, 0.3] #\nmomentum = [0.4, 0.9]\ninit_mode = ['glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform'] #\nactivation = ['relu', 'tanh'] #\nweight_constraint = [3, 5] #\ndropout_rate = [0.2, 0.5] #\nl2_weight_decay = [1e-6, 1e-5, 1e-3, 1e-2] #\n# cv = 5\n\n#'''\n\n'''\n# Second try based on best values from above:\n\ninput_dim = X_with_age_train.shape[1]\nbatch_size = [10, 60, 100]\nepochs = [600]\noptimizer = ['Adam', 'Nadam']\nlearn_rate = [0.001, 0.3]\nmomentum = [0.9, 0.4]\ninit_mode = ['glorot_uniform', 'he_normal']\nactivation = ['relu']\nweight_constraint = [5, 3]\ndropout_rate = [0.2]\nneurons = [60, 150]\n\n#'''\n\n'''\n\n# Third try based on best values from 2nd Try:\n\ninput_dim = X_with_age_train.shape[1]\nbatch_size = [32, 64]\nepochs = [600]\noptimizer = ['Adam', 'Nadam']\nlearn_rate = [0.001, 0.3]\nmomentum = [0.9, 0.4]\ninit_mode = ['glorot_uniform']\nactivation = ['relu']\nweight_constraint = [5, 3]\ndropout_rate = [0.2]\nneurons = [64, 320, 640, 1280]\n\n#'''","79ef7e15":"'''\ntuner = kt.BayesianOptimization(\n    build_age_prediction_regression_model,\n    objective='val_loss',\n    directory='kt_01',\n    project_name='Titanic_keras_tuner',\n    max_trials=30,\n    executions_per_trial=1,\n    overwrite=True)\n'''","69e49712":"tuner_age_model = kt.Hyperband(\n    build_age_prediction_regression_model,\n    objective='val_loss',\n    directory='kt_02_01',\n    project_name='Titanic_keras_tuner',\n    #max_trials = 30,\n    factor=3,\n    max_epochs=200,\n    executions_per_trial=5,\n    overwrite=True)\n\ntuner_age_model.search_space_summary()","314dc96e":"# Setup callback\ncallback_early_stopping = EarlyStopping(monitor='loss', patience = 25)\ncheckpoint_path = 'mlp_checkpoints_every_epoch\/checkpoint_{epoch:03d}'\ncallback_checkpoint_epoch = ModelCheckpoint(filepath=checkpoint_path,\n                                 frequency='epoch',\n                                 save_weights_only=True,\n                                 verbose=0)","7bed8373":"callbacks_age = get_callbacks()\ntuner_age_model.search(X_with_age_train, y_with_age_train.values, validation_split=0.2, batch_size=32, epochs=600, callbacks=[callbacks_age])","b1efbece":"model_best_age = tuner_age_model.get_best_models()[0]\nmodel_best_age.summary()","e2da6fa1":"tuner_age_model.results_summary()","cd06b4a1":"# Get best hyperparameters\n\nbest_hps=tuner_age_model.get_best_hyperparameters(num_trials=1)[0]\nbest_hps.get('optimizer')","e888bd71":"# Build the model with the optimal hyperparameters and train it on the data for 600 epochs\n\ntuner_age_hypermodel = tuner_age_model.hypermodel.build(best_hps)","d6081553":"# Extract the best epoch when val_loss_per_epoch is minimum.\n\nhistory = tuner_age_hypermodel.fit(X_with_age_train, y_with_age_train.values, epochs=600, validation_split=0.2, verbose=0, callbacks=[callbacks_age])\nval_acc_per_epoch = history.history['val_accuracy']\nval_loss_per_epoch = history.history['val_loss']\nbest_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\nprint('Best epoch: %d' % (best_epoch,))","b2d344bb":"# Retrain the model to the best number of epochs.\n\ntuner_age_hypermodel.fit(X_with_age_train, y_with_age_train.values, epochs=best_epoch, validation_split=0.2)","363d64f3":"eval_result = tuner_age_hypermodel.evaluate(X_with_age_check, y_with_age_check.values)\nprint(\"[check loss (mse), check accuracy (r-squared)]:\", eval_result)","093c54c8":"# Plot the training and validation loss\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\nplt.show()","a88f76c3":"acc_my_age_model = coeff_determination(y_with_age_check.values, np.squeeze(tuner_age_hypermodel.predict(X_with_age_check)))\nprint(acc_my_age_model)\npred_my_age_model = tuner_age_hypermodel.predict(df_without_age)","64f8772b":"# Plot the 'check' data\n\nplt.rcParams[\"figure.figsize\"] = (20,3)\n#plt.plot(np.squeeze(y_with_age_check.values))\nX_axis = np.arange(len(y_with_age_check.values))\nplt.bar(X_axis - 0.2, np.squeeze(y_with_age_check.values), 0.4)\n#plt.plot(np.squeeze(tuner_age_hypermodel.predict(X_with_age_check)))\nplt.bar(X_axis + 0.2, np.squeeze(tuner_age_hypermodel.predict(X_with_age_check)), 0.4)\nplt.title('ID vs. Age')\nplt.ylabel('Age')\nplt.xlabel('ID')\nplt.legend(['Y', 'Prediction'], loc='upper right')\nplt.show()","73f65c43":"# What is the score if you use a mean value for age from training data?\n\nacc_my_age_mean = coeff_determination(y_with_age_check.values, np.mean(y_with_age_train.values))\nprint(acc_my_age_mean)","14d5b83c":"# What is the score if you use a median value for age from training data?\n\nacc_my_age_median = coeff_determination(y_with_age_check.values, np.median(y_with_age_train.values))\nprint(acc_my_age_median)","7b1ad8fa":"# Perceptron and Naive Bayes are classifiers.","05898443":"# Support Vector Machines - linear support vector regressor\n\nsvr_age = SVR()\nsvr_age.fit(X_with_age_train, np.squeeze(y_with_age_train.values))\nY_pred_svr = svr_age.predict(df_without_age)\nacc_svr_age = round(svr_age.score(X_with_age_check, np.squeeze(y_with_age_check.values)) * 100, 2)\nacc_svr_age","5004f151":"# Plot the training and validation loss\n\nplt.rcParams[\"figure.figsize\"] = (20,3)\n#plt.plot(np.squeeze(y_with_age_check.values))\n#plt.plot(np.squeeze(svr_age.predict(X_with_age_check)))\nplt.bar(X_axis - 0.2, np.squeeze(y_with_age_check.values), 0.4)\nplt.bar(X_axis + 0.2, np.squeeze(svr_age.predict(X_with_age_check)), 0.4)\nplt.title('ID vs. Age')\nplt.ylabel('Age')\nplt.xlabel('ID')\nplt.legend(['Y', 'Prediction'], loc='upper right')\nplt.show()","83abc989":"#help(DecisionTreeRegressor.score)","b8fc8522":"# Decision Tree Regressor\n\nfig = plt.figure(figsize=(200,100))\n#fig.set_size_inches(40, 10)\n\ndecision_tree_regressor = DecisionTreeRegressor()\ndecision_tree_regressor = decision_tree_regressor.fit(X_with_age_train, np.squeeze(y_with_age_train.values))\nY_pred_decision_tree_regressor = decision_tree_regressor.predict(df_without_age)\nacc_decision_tree_regressor = round(decision_tree_regressor.score(X_with_age_check, np.squeeze(y_with_age_check.values))*100 , 2)\nprint(acc_decision_tree_regressor)\n#acc_decision_tree_regressor_1 = coeff_determination(y_with_age_check.values, decision_tree_regressor.predict(X_with_age_check))\n#print(acc_decision_tree_regressor_1)","bd7d2b04":"tree.plot_tree(decision_tree_regressor, filled=True)\n#plt.show()\n#plt.savefig('decision_tree.png')\nfig.savefig(\"decision_tree.png\")","ca9c2bee":"# Plot the training and validation loss\n\n#plt.rcParams[\"figure.figsize\"] = (20,3)\n#plt.plot(np.squeeze(y_with_age.values))\n#plt.plot(np.squeeze(decision_tree_regressor.predict(X_with_age)))\nplt.bar(X_axis - 0.2, np.squeeze(y_with_age_check.values), 0.4)\nplt.bar(X_axis + 0.2, np.squeeze(decision_tree_regressor.predict(X_with_age_check)), 0.4)\nplt.title('ID vs. Age')\nplt.ylabel('Age')\nplt.xlabel('ID')\nplt.legend(['Y', 'Prediction'], loc='upper right')\nplt.show()","358bb3a0":"# Random Forest Regressor\n\nrandom_forest_regressor = RandomForestRegressor(n_estimators=100)\nrandom_forest_regressor = random_forest_regressor.fit(X_with_age_train, np.squeeze(y_with_age_train.values))\nY_pred_random_forest_regressor = random_forest_regressor.predict(df_without_age)\nacc_random_forest_regressor = round(random_forest_regressor.score(X_with_age_check, np.squeeze(y_with_age_check.values)) * 100, 2)\nacc_random_forest_regressor","b2465f25":"# Plot the training and validation loss\n\n#plt.rcParams[\"figure.figsize\"] = (20,3)\n#plt.plot(np.squeeze(y_with_age.values))\n#plt.plot(np.squeeze(random_forest_regressor.predict(X_with_age)))\nplt.bar(X_axis - 0.2, np.squeeze(y_with_age_check.values), 0.4)\nplt.bar(X_axis + 0.2, np.squeeze(random_forest_regressor.predict(X_with_age_check)), 0.4)\nplt.title('ID vs. Age')\nplt.ylabel('Age')\nplt.xlabel('ID')\nplt.legend(['Y', 'Prediction'], loc='upper right')\nplt.show()","20f5f2ca":"#help(RandomForestRegressor.score)","b10a06c8":"# Stochastic Gradient Descent\n\nsgd_regressor = SGDRegressor()\nsgd_regressor = sgd_regressor.fit(X_with_age_train, np.squeeze(y_with_age_train.values))\nY_pred_sgd_regressor = sgd_regressor.predict(df_without_age)\nacc_sgd_regressor = round(sgd_regressor.score(X_with_age_check, np.squeeze(y_with_age_check.values)) * 100, 2)\nacc_sgd_regressor","e87d30a1":"# Plot the training and validation loss\n\n#plt.rcParams[\"figure.figsize\"] = (20,3)\n#plt.plot(np.squeeze(y_with_age.values))\n#plt.plot(np.squeeze(sgd_regressor.predict(X_with_age)))\nplt.bar(X_axis - 0.2, np.squeeze(y_with_age_check.values), 0.4)\nplt.bar(X_axis + 0.2, np.squeeze(sgd_regressor.predict(X_with_age_check)), 0.4)\nplt.title('ID vs. Age')\nplt.ylabel('Age')\nplt.xlabel('ID')\nplt.legend(['Y', 'Prediction'], loc='upper right')\nplt.show()","842ffe46":"# Linear SVR\n\nlinear_svr = LinearSVR(max_iter=100000)\nlinear_svr.fit(X_with_age_train, np.squeeze(y_with_age_train.values))\nY_pred_linear_svr = linear_svr.predict(df_without_age)\nacc_linear_svr = round(linear_svr.score(X_with_age_check, np.squeeze(y_with_age_check.values)) * 100, 2)\nacc_linear_svr","a00a1c80":"# Plot the training and validation loss\n\n#plt.rcParams[\"figure.figsize\"] = (20,3)\n#plt.plot(np.squeeze(y_with_age.values))\n#plt.plot(np.squeeze(linear_svr.predict(X_with_age)))\nplt.bar(X_axis - 0.2, np.squeeze(y_with_age_check.values), 0.4)\nplt.bar(X_axis + 0.2, np.squeeze(linear_svr.predict(X_with_age_check)), 0.4)\nplt.title('ID vs. Age')\nplt.ylabel('Age')\nplt.xlabel('ID')\nplt.legend(['Y', 'Prediction'], loc='upper right')\nplt.show()","1b303190":"# KNR confidence score\n\nknr = KNeighborsRegressor(n_neighbors = 3)\nknr.fit(X_with_age_train, np.squeeze(y_with_age_train.values))\nY_pred_knr = knr.predict(df_without_age)\nacc_knr = round(knr.score(X_with_age_check, np.squeeze(y_with_age_check.values)) * 100, 2)\nacc_knr","adfcfb7e":"# Plot the training and validation loss\n\n#plt.plot(np.squeeze(y_with_age.values))\n#plt.plot(np.squeeze(knr.predict(X_with_age)))\nplt.bar(X_axis - 0.2, np.squeeze(y_with_age_check.values), 0.4)\nplt.bar(X_axis + 0.2, np.squeeze(knr.predict(X_with_age_check)), 0.4)\nplt.title('ID vs. Age')\nplt.ylabel('Age')\nplt.xlabel('ID')\nplt.legend(['Y', 'Prediction'], loc='upper right')\nplt.show()","87f8be4e":"# Summarize age prediction model results\n\nprediction_models = pd.DataFrame({\n    'Model': ['My Age Model', 'Support Vector Machines', 'KNR',  \n              'Random Forest', 'Stochastic Gradient Decent', 'Linear SVR', \n              'Decision Tree'],\n    'Score': [acc_my_age_model, acc_svr_age, acc_knr,  \n              acc_random_forest_regressor, \n              acc_sgd_regressor, acc_linear_svr, acc_decision_tree_regressor]})\nprediction_models.sort_values(by='Score', ascending=False)","a24644e5":"display(all_data)\ndisplay(df_with_age)\ndisplay(df_without_age)","05a3db43":"# Use predicted age values from most accurate model. In this case, it will be my age model.\n\n#predicted_ages = age_model.predict(sc_without_age[['Pclass', 'Fare']].values)\npredicted_ages = pred_my_age_model\n\n# DataFrame of data with 'Age'\nall_data_with_age = all_data.dropna(subset=['Age'])\n\n# DataFrame of data without 'Age'\nall_data_without_age =all_data.loc[all_data['Age'].isna()]\nall_data_without_age.pop('Age')\n\nall_data_without_age.loc[:,'Age'] = predicted_ages[:]\n\n# Recombine to form the complete train and test data\n\nall_data_with_age = pd.concat((all_data_with_age, all_data_without_age), axis=0, sort=False)\nall_data_with_age.sort_index(inplace=True)\n\n# Rebuild all_data\n\nall_data = all_data_with_age.copy()\ndisplay(all_data)","2ae4f483":"# Check NaN status\nlist_columns = list(all_data.columns)\nprint(list_columns)\nprint(train_data.columns)\nfor column in list_columns:\n    print(\"all_data Nan in \", column, \" = \", all_data[column].isna().sum())","f088b051":"# Analyze 'Age' and convert to categorical\n\ndisplay(all_data.head())\nconvert_numerical_feature_name_to_categorical(all_data, 'Age', 0, 80, 16)\nanalyze_feature_name(all_data, 'Age')\ndisplay(all_data.head())","cc3fb514":"# Binarize 'Age'\n\nprint(all_data.shape)\ndisplay(all_data.head())\nfeatures = ['Age']\nall_data = convert_feature_to_binary(all_data, features)","1bcb43bb":"display(all_data.head())","4fc9689c":"print(all_data.columns)","89146c2a":"# Experiment dropping some features (except 'is_test').\n#'''\ncopy_of_all_data = all_data.copy()\n\nfeatures_for_Prediction_to_drop = ['Title']\nfeatures_for_Prediction_to_drop_columns = []\nfor feature in features_for_Prediction_to_drop:\n    features_for_Prediction_to_drop_columns.extend([col for col in all_data.columns if feature in col])\nprint(features_for_Prediction_to_drop_columns)\n\nall_data = all_data.drop(features_for_Prediction_to_drop_columns, axis=1)\ndisplay(all_data)\n#'''","bebc61b2":"# If a mistake is made in dropping columns, restore 'all_data'\n\n#all_data = copy_of_all_data.copy()","f8979061":"# Rebuild train_data and test_data\n\ntrain_data = all_data[all_data['is_test'] == 0]\n# Now that the train_data is separated, remove the 'is_test' column\ntrain_data.pop('is_test')\nprint(train_data.shape)\ndisplay(train_data.head())\n\ntest_data = all_data[all_data['is_test'] == 1]\n# Now that the test_data is separated, remove the 'is_test' column\ntest_data.pop('is_test')\nprint(test_data.shape)\ndisplay(test_data.head())\n","c1bcccd2":"# Finalize train and test data\n\ntrain_X = train_data.copy()\ny_column_name = \"Survived\"\ntrain_y = train_X.pop(y_column_name)\n\n# Similarly setup test data\ntest_X = test_data.copy()\ntest_X.pop(y_column_name)\n\ntrain_X_df = train_X\ntrain_y_df = train_y\ntest_X_df = test_X\n\ndisplay(train_X)\ndisplay(test_X)","ca77e590":"# Convert to ndarray\ntrain_X_all = np.asarray(train_X)\ntrain_y_all = np.asarray(train_y)\ntest_X = np.asarray(test_X)","5d801496":"# Split a small chunk for independent 'checking'. Set the random_state in such a way that the percent suvived in the 'train_y' split is equal to about 38.3%\n'''\nimport math\nfor i in np.arange(10000):\n    train_X_temp, check_X_temp, train_y_temp, check_y_temp = train_test_split(train_X_all, train_y_all, test_size=0.2, random_state=i)\n    if((math.isclose(np.mean(train_y_temp)*100, np.mean(check_y_temp)*100, abs_tol=0.25))):\n    #if((38.33 < (np.mean(train_y_temp)*100) < 38.43)):\n        print(f'train_y survival rate = {np.mean(train_y_temp)*100:.2f}')\n        print(f'check_y survival rate = {np.mean(check_y_temp)*100:.2f}')\n        print(f\"random_state={i}\")\n        break\n        \n# This gives the result: random_state = 0\n#'''","ee5210bd":"# The above cell was used to estimate 'random_state = 0' which gives a train and check split with survival rate of about 38.3%\n\ntrain_X, check_X, train_y, check_y = train_test_split(train_X_all, train_y_all, test_size=0.2, random_state=0)","e32d68de":"''' Since the data is catagerical, do not normalize data - does this make sense?\n# Normalize the train and check data\n\ndef data_normalize(data, mean_data =None, std_data =None):\n    if not mean_data:\n        mean_data = np.mean(data)\n    if not std_data:\n        std_data = np.std(data)\n    norm_data = (data-mean_data)\/std_data\n    return norm_data, mean_data, std_data\n\ntrain_X, mean_data, std_data = data_normalize(train_X)\ncheck_X, _, _ = data_normalize(check_X, mean_data, std_data)\n\n'''","f8a828ea":"# define the grid search parameters\n'''\n# Partial list of available options\n\ninput_dim = train_X.shape[1]\nbatch_size = [10, 20, 40, 60, 80, 100]\nepochs = [10, 50, 100]\noptimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\nlearn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\nmomentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\ninit_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\nactivation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\nweight_constraint = [1, 2, 3, 4, 5]\ndropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\nneurons = [1, 5, 10, 15, 20, 25, 30]\n'''\n\n#'''\n# First try\n\ninput_dim = train_X.shape[1]\n#epochs = [100, 200]\noptimizer = ['Adam', 'Nadam'] #\nlearning_rate = [0.000001, 0.0001, 0.3] #\nmomentum = [0.4, 0.9]\ninit_mode = ['glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform'] #\nactivation = ['relu', 'tanh'] #\nweight_constraint = [3, 5] #\ndropout_rate = [0.2, 0.5] #\nweight_decay = [1e-6, 1e-5, 1e-3, 1e-2] #\n# cv = 5\n\n\n#'''\n","6c967b9f":"tuner_survived = kt.Hyperband(\n    build_survival_prediction_classification_model,\n    objective='val_binary_accuracy',\n    directory='kt_02_01',\n    project_name='Titanic_keras_tuner',\n    factor=3,\n    max_epochs=50,\n    executions_per_trial=1,\n    overwrite=True)\n\ntuner_survived.search_space_summary()","a74002db":"# Search for the best model.\ncallbacks_survived = get_callbacks()\ntuner_survived.search(train_X, train_y, validation_split=0.2, batch_size=8, epochs=200, callbacks=[callbacks_survived])","8034f2a4":"# Extract the best model\n\nbest_model_survived = tuner_survived.get_best_models()[0]\nbest_model_survived.summary()","323df39d":"tuner_survived.results_summary()","1fa3cad2":"# Get best hyperparameters\n\nbest_hps_survived=tuner_survived.get_best_hyperparameters(num_trials=1)[0]\nbest_hps_survived.get('optimizer')","d317302b":"# Build the model with the optimal hyperparameters and train it on the data for 600 epochs\ntuner_survived_hypermodel = tuner_survived.hypermodel.build(best_hps_survived)","9a2b6520":"# Extract the best epoch when val_loss_per_epoch is minimum.\n\nhistory = tuner_survived_hypermodel.fit(train_X, train_y, epochs=600, validation_split=0.2, batch_size=8, verbose=0, callbacks=[callbacks_survived])\nval_bin_acc_per_epoch = history.history['val_binary_accuracy']\nval_loss_per_epoch = history.history['val_loss']\nbest_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\nprint('Best epoch: %d' % (best_epoch,))","76121310":"# Retrain the model to the best epoch.\n\ntuner_survived_hypermodel.fit(train_X, train_y, epochs=best_epoch, batch_size=8, validation_split=0.2)","a5775d91":"eval_result = tuner_survived_hypermodel.evaluate(check_X, check_y)\nprint(\"[check loss (val_loss), check accuracy (val_binary_accuracy)]:\", eval_result)","5e1f740c":"predictions_check = np.squeeze(tuner_survived_hypermodel.predict(check_X, verbose=0))\npredictions_check = predictions_check > 0.5\npredictions_check = predictions_check.astype('int32')\nacc_my_survived_model = round(np.mean(predictions_check == check_y)*100, 2)\nprint(acc_my_survived_model)","29c90d61":"# Plot the 'check' data\n\nplt.rcParams[\"figure.figsize\"] = (20,3)\n#plt.plot(np.squeeze(y_with_age_check.values))\nX_axis = np.arange(len(check_y))\nplt.bar(X_axis - 0.2, check_y, 0.4)\n#plt.plot(np.squeeze(tuner_survived_hypermodel.predict(X_with_age_check)))\nplt.bar(X_axis + 0.2, np.squeeze(predictions_check), 0.4)\nplt.title('ID vs. Age')\nplt.ylabel('Age')\nplt.xlabel('ID')\nplt.legend(['Y', 'Prediction'], loc='upper right')\nplt.show()","cf470745":"# Load weights from last epoch or the best epoch\n\ndef get_model_last_epoch(model):\n    \"\"\"\n    This function should create a new instance of the CNN you created earlier,\n    load on the weights from the last training epoch, and return this model.\n    \"\"\"\n    filepath = tf.train.latest_checkpoint('checkpoints_every_epoch')\n    model.load_weights(filepath)\n    \n    return model\n    \ndef get_model_best_epoch(model):\n    \"\"\"\n    This function should create a new instance of the CNN you created earlier, load \n    on the weights leading to the highest validation accuracy, and return this model.\n    \"\"\"\n    filepath = tf.train.latest_checkpoint('checkpoints_best_only')\n    model.load_weights(filepath)\n    \n    return model\n    ","437636aa":"# Plot the training and validation loss\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\nplt.show()","5539826d":"# Confusion matrix\n\npredict_check_y = tuner_survived_hypermodel.predict(check_X)\npredict_check_y = (predict_check_y > 0.5)\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(check_y, predict_check_y)","79bf6c0e":"# Heat map\n\nimport seaborn as sns\ncols = ['check_y', 'predict_check_y']\nsns.heatmap(cm, annot=True, yticklabels=cols, xticklabels=cols).set_title('Given vs Predicted')","c6d7b66f":"# Manual check of 'train' prediction\n\npredictions_train = tuner_survived_hypermodel.predict(train_X, verbose=0)\npredictions_train = np.where(predictions_train > 0.5, 1, 0)\npredictions_train = predictions_train.astype('int32')\ntrain_y = train_y.reshape(predictions_train.shape[0],1)\n\npredictions_manual_df = pd.DataFrame({\n    'predictions_train' : list(np.squeeze(predictions_train)),\n    'train_y' : list(np.squeeze(train_y.astype('int32')))\n})\ndisplay(predictions_manual_df.describe())\npercent_survived_y = round(np.mean(train_y)*100, 2)\npercent_survived_train = round(np.mean(predictions_train)*100, 2)","578adb16":"pred_my_survived_model = tuner_survived_hypermodel.predict(test_X)\npred_my_survived_model = pred_my_survived_model > 0.5\npred_my_survived_model = pred_my_survived_model.astype('int32')","1e6e0dd3":"pred_my_survived_model_df = pd.DataFrame(zip(PassengerId_test, np.squeeze(pred_my_survived_model)))\npred_my_survived_model_df.set_axis(['PassengerId', 'Predictions_My_Survived_Model'], axis=1, inplace=True)\npred_my_survived_model_df.set_index('PassengerId', drop=False, inplace=True)\ntest_data_final = pd.concat([test_data_original, pred_my_survived_model_df], axis=1)\ndisplay(test_data_final)","981dd6fe":"display(pred_my_survived_model_df.describe())\npercent_survived_test_my_model = (np.mean(pred_my_survived_model_df.Predictions_My_Survived_Model)*100)\nprint(f'{percent_survived_test_my_model:.1f}')","2cc8f60a":"layer_dimensions=''\nfor layer in tuner_survived_hypermodel.layers:\n    if layer_dimensions == '':\n        layer_dimensions=layer_dimensions + str(layer.get_output_at(0).get_shape().as_list()[1])\n    else:\n        layer_dimensions=layer_dimensions + 'x' + str(layer.get_output_at(0).get_shape().as_list()[1])\n    \n#print(layer_dimensions)","dc3e75f7":"#sys.exit('Stopping the code at this location.')","f9a4eba8":"# Generate alias variable names\n\nX_train = train_X\nY_train = np.squeeze(train_y)\nX_test = test_X","31f4a04a":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred_log = logreg.predict(X_test)\nacc_log = round(logreg.score(check_X, check_y) * 100, 2)\nacc_log","60220abe":"coeff_df = pd.DataFrame(train_X_df.columns)\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)\n","23d0477b":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred_svc = svc.predict(X_test)\nacc_svc = round(svc.score(check_X, check_y) * 100, 2)\nacc_svc","d7c1a3c2":"# KNN confidence score is better than Logistic Regression but worse than SVM\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred_knn = knn.predict(X_test)\nacc_knn = round(knn.score(check_X, check_y) * 100, 2)\nacc_knn","b5148e2f":"percent_survived_test_knn = round(np.mean(Y_pred_knn)*100,2)\nprint(percent_survived_test_knn)","523dae61":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred_gaussian = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(check_X, check_y) * 100, 2)\nacc_gaussian","ad31045f":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred_preceptron = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(check_X, check_y) * 100, 2)\nacc_perceptron","cd2c829e":"# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred_linear_svc = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(check_X, check_y) * 100, 2)\nacc_linear_svc","570acd4c":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred_sgd = sgd.predict(X_test)\nacc_sgd = round(sgd.score(check_X, check_y) * 100, 2)\nacc_sgd","4a94b193":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree = decision_tree.fit(X_train, Y_train)\nY_pred_decision_tree = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(check_X, check_y) * 100, 2)\nprint(acc_decision_tree)\n#tree.plot_tree(decision_tree)","227ab984":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred_random_forest = random_forest.predict(X_test)\npercent_survived_test_random_forest = round(np.mean(Y_pred_random_forest) * 100, 2)\nprint(percent_survived_test_random_forest)\n#random_forest.score(X_train, Y_train)\nacc_random_forest_train = round(random_forest.score(X_train, Y_train) * 100, 2)\n#print(acc_random_forest_train)\nacc_random_forest_check = round(random_forest.score(check_X, check_y) * 100, 2)\nacc_random_forest_check","1dc7be96":"# Summarize results\n\nprediction_models = pd.DataFrame({\n    'Model': ['My Model', 'Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_my_survived_model, acc_svc, acc_knn, acc_log, \n              acc_random_forest_check, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nprediction_models.sort_values(by='Score', ascending=False)","3ded852d":"predictions_test_df = pd.DataFrame({\n    \"PassengerId\" : PassengerId_test, \n    #\"Survived\" : np.squeeze(Y_pred_svc).astype('int32')\n    #\"Survived\" : np.squeeze(Y_pred_knn).astype('int32')\n    #\"Survived\" : np.squeeze(Y_pred_svc).astype('int32')\n    \"Survived\" : np.squeeze(pred_my_survived_model).astype('int32')\n})\nharishsenapathy_submission = predictions_test_df.copy()\n#display(harishsenapathy_submission)\npredictions_test_df.set_index('PassengerId', drop=True, inplace=True)\n#display(predictions_test_df)\ntest_data_final = pd.concat([test_data_original, predictions_test_df], axis=1)\ndisplay(test_data_final)\n#acc_selected_model = acc_knn\nacc_selected_model = acc_my_survived_model\n#selected_percent_survived = percent_survived_test_knn\nselected_percent_survived = percent_survived_test_my_model\nharishsenapathy_submission.to_csv(f'harishsenapathy_submission_11_{layer_dimensions}_{acc_selected_model:.1f}_{percent_survived_y:.1f}_{percent_survived_train:.1f}_{selected_percent_survived:.1f}.csv', encoding='utf-8', index=False)","b29fb839":"#display(test_X_df)\nwith pd.option_context(\"display.max_rows\", 10000):\n    display(test_X_df)\n","6594fcae":"<a id=\"TOC-13\"><\/a>\n# Estimate 'Survived' - using a Classification model","6ff93a6d":"<a id=\"TOC-18\"><\/a>\n# Prepare Submission","4019441f":"<a id=\"TOC-10\"><\/a>\n# Predict missing 'Age'","a49ad12b":"<a id=\"TOC-04\"><\/a>\n# Convert to Categorical\n\n>Examine each feature and convert to categorical, and remove unwanted features.","ebc59538":"# Fork of Titanic prediction using Tensorflow and Keras\n\nA very detailed approach to analyzing the Titanic prediction problem. Broadly speaking, the problem is divided into two prolems:\n\n* Estimating the missing values for 'Age'.\n* Predicting 'Survived'\n\nA lot of output screens between code cells are left on so we can evaluate the data and move to the next step.\n\nI am a beginner in ML programming. So please excuse any glaring errors in the coding.\n    \nTable of Contents:\n*     [Import data and do a preliminary analysis](#TOC-01)\n*     [Define pre-processing functions and model functions](#TOC-02)\n*     [Examine 'NaN'](#TOC-03)\n*     [Convert to Categorical](#TOC-04)\n*     [Estimate 'Age' - using a Regression model](#TOC-05)\n     -     [Set up Keras Tuner parameters and obtain best Hyperparameters](#TOC-06)\n     -     [Plot learning curves for age-prediction](#TOC-07)\n     -     [Use various predictions to predict 'Age' and compare](#TOC-08)\n     -     [Summarize accuracy of 'Age' prediction by various methods](#TOC-09)\n     -     [Predict missing 'Age'](#TOC-10)\n*     [Prepare data for 'Survived' prediction](#TOC-11)\n*     [Select appropriate radom_state value to split the input data into 'train' and 'check' datasets](#TOC-12)\n*     [Estimate 'Survived' - using a Classification model](#TOC-13)\n     -     [Setup Keras Tuner and Optimize Hyperparameters](#TOC-14)\n     -     [Plot the 'Survived' learning curves](#TOC-15)\n     -     [Predict using various methods and compare](#TOC-16)\n     -     [Summarize accuracy of 'Survived' results by various methods](#TOC-17)\n     -     [Prepare Submission](#TOC-18)","20202a65":"<a id=\"TOC-09\"><\/a>\n# Summarize accuracy of 'Age' prediction by various methods","642b17bf":"<a id=\"TOC-16\"><\/a>\n# Predict using various methods and compare","9571e18a":"<a id=\"TOC-12\"><\/a>\n# Select appropriate radom_state value to split the input data into 'train' and 'check' datasets\n\nIn the 'train' data we have the following:<br>\n    <ul>\n    <li>Total passengers = 891 (passengers and crew)<\/li>\n    <li>Survived = 342<\/li>\n    <li>Percent survived = 38.38%<\/li>\n    <\/ul>\n    \nFrom the internet we have thefollowing (https:\/\/comparecamp.com\/titanic-statistics\/):<br>\n    <ul>\n    <li>Total passengers = 2,223 (passengers and crew)<\/li>\n    <li>Total passengers = 1,316 (passengers only)<\/li>\n    <li>Total crew = 885 (crew only)<\/li>\n    <li>Survived = 723 (approximately)<\/li>\n    <li>Percent survived = 33.04%<\/li>\n    <\/ul>\n    <ul>\n    <li>There were a total of 325 first-class passengers on the ship.<\/li>\n    <li>Cost of first-class (parlor suite) one-way ticket was \u00a3870 or $4,350 ($83,200 today).<\/li>\n    <li>285 was the number of second-class passengers aboard the RMS Titanic.<\/li>\n    <li>Second-class tickets cost \u00a312 or $60 ($1200 today).<\/li>\n    <li>Third-class passengers on board were 706.<\/li>\n    <li>\u00a33 to \u00a38 or $40 ($298 to $793 today) was the cost of third-class tickets.<\/li>\n    <\/ul>\n    \nFor the train data we have the following:<br>\n    <ul>\n    <li>Total passengers = 417 (passengers and crew)<\/li>\n        <blockquote>The overall survival rate was 33.04%. <br>The 'train' set survival rate is 38.38%.\n        <br>TIf the 'train' and 'test' sample together represent the overall data, then\n        the approximate survival rate in the 'test' data should be:\n            <ul>\n            <li>Survived overall = (891 + 417)*0.3304 = 432<\/li>\n            <li>Survived 'test' = 432 - 342 = 90<\/li>\n            <li>Survived 'test' percentage = 21.61%<\/li>\n            <\/blockquote>\n    \nBased on this, if we select a 'train_y' split data with 'Survived' = 38.3%, then when the accuracy is 100%,\nthe 'check_y' data should also give 'Survived' = 38.3%.\n    \nIn addition, the 'test' data should give 'Survived' = 21.61%. Although, this estimate may not be correct\nbecause it is based on the assumpion that the <br>overall data ('train' and 'test' data) is an accurate\nrepresentation of the original Titanic data. But, if the end result does show a survival <br>rate of around 21%,\nI will be quite happy.","93418b73":"<a id=\"TOC-01\"><\/a>\n# Import data and perform a preliminary analysis","5f98c9f4":"<a id=\"TOC-15\"><\/a>\n# Plot the 'Survived' learning curves","05bad932":"<a id=\"TOC-08\"><\/a>\n# Use various predictions to predict 'Age' and compare","72e9c72b":"<a id=\"TOC-03\"><\/a>\n# Examine 'NaN'\n\n>The first step is to examine if there 'NaN' in the data and rectify it where required in the analysis.","a3d3bdb3":"<a id=\"TOC-05\"><\/a>\n# Estimate 'Age' - using a Regression model\n\n>Age is an important component to predict survivability of a passenger. Remove all 'NaN' values by estimating age using a model.","7779796c":"<a id=\"TOC-02\"><\/a>\n# Define pre-processing functions and model functions\n\n>Various functions to convert features to categorical-nominal, categorical-ordinal, categorical-range values","b9652991":"<a id=\"TOC-11\"><\/a>\n# Prepare data for 'Survived' prediction","16394e61":"<a id=\"TOC-07\"><\/a>\n# Plot learning curves for age-prediction","d0849c08":"<a id=\"TOC-14\"><\/a>\n# Setup Keras Tuner and Optimize Hyperparameters\n    \n    Experimented with GridSearchCV. Had to run the model overnight. Same search (though not as thorough) done with\n    Keras Tuner (Hyperband and BayesOptimization) took less than 1 hour. Though Keras Tuner is a bit of a black box\n    it appreast to give reasonable results.","b1fc479a":"<a id=\"TOC-17\"><\/a>\n# Summarize accuracy of 'Survived' results by various methods","69b9d207":"<a id=\"TOC-06\"><\/a>\n# Set up Keras Tuner parameters and obtain best Hyperparameters\n    \n    Experimented with GridSearchCV. Had to run the model overnight. Same search (though not as thorough) done with\n    Keras Tuner (Hyperband and BayesOptimization) took less than 1 hour. Though Keras Tuner is a bit of a black box\n    it appreast to give reasonable results."}}