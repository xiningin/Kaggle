{"cell_type":{"693c564c":"code","010315cb":"code","0b06506f":"code","92a2bac0":"code","47f18a2f":"code","1652f94c":"code","8257595a":"code","7a544dff":"code","f8868b7c":"code","dae6e1f0":"code","cf08ec0b":"code","72db36f0":"code","c19d0393":"code","06b59d03":"code","ee156eaa":"code","599d0c9c":"code","4fe78a1e":"code","93664e55":"code","149fe852":"code","23e7697a":"code","57a3d0bc":"code","6a505996":"code","bbffcb62":"code","f67f1d64":"code","258d3891":"code","a3b309fe":"code","062c5a8b":"code","9cd930bc":"code","9d50154e":"code","70e85554":"code","9241e7a7":"code","5a47d054":"code","bca9de34":"code","57002844":"code","f969985d":"code","947c8a15":"code","3cafb691":"code","04d0ff0a":"code","cb1a46d6":"code","6500af01":"code","5bfdaa9d":"code","dae74276":"code","4c946e59":"code","4b4f2877":"code","ab4ed00c":"code","eb56a8b0":"code","ee4bffee":"code","47218079":"code","daddb828":"code","0cf9d995":"code","adb3d37f":"code","c5488b66":"code","c7592994":"code","396b89ad":"code","9b94a5a4":"code","5cc3cc27":"code","8bd663dc":"code","99221ff7":"code","97f136d4":"code","7324fdf3":"code","d0d194e5":"code","1afd4c35":"code","cbdae843":"code","ab50c73c":"code","a02d37d8":"code","0b9cf7cf":"code","0d3620ce":"code","56686524":"code","38c6775d":"code","277b159a":"code","9c6ab65a":"code","85d8a1ef":"code","385eaf01":"code","d56793c7":"code","1a665365":"code","52dcb7e7":"code","d069cc0e":"code","5c5e31db":"code","61eba58a":"code","0c2449fc":"code","4060fcae":"code","5bb8ceb9":"markdown","ae856e0f":"markdown","2db43cea":"markdown","3d678f1b":"markdown","b7e6def4":"markdown","c8d3d8c1":"markdown","6c437c65":"markdown","b6a19fad":"markdown","037111a1":"markdown","1f88db24":"markdown","2993882f":"markdown","2614c6f3":"markdown","a3f46bfe":"markdown","5b56b758":"markdown","fd2dbc39":"markdown","e81331ec":"markdown","e63a1464":"markdown"},"source":{"693c564c":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","010315cb":"df = pd.read_csv(\"..\/input\/credit-card-customers\/BankChurners.csv\")","0b06506f":"df.head()","92a2bac0":"df['Attrition_Flag'].replace({'Existing Customer': 0} , inplace = True)\ndf['Attrition_Flag'].replace({'Attrited Customer': 1} , inplace = True)\n","47f18a2f":"#Removing Last two columns from our Analysis \ndf = df.iloc[:, :-2]\ndf.head(50)","1652f94c":"#Checking the shape fo the Dataframe\ndf.shape","8257595a":"# Replace spaces in the column names with underscore\ndf.columns = [c.replace(' ', '_') for c in df.columns]","7a544dff":"df.info()","f8868b7c":"#Show Basic Statistics\ndf.describe()","dae6e1f0":"duplicate = df[df.duplicated()] \n  \nprint(\"Duplicate Rows :\") \n  \n# Print the resultant Dataframe \nduplicate \n\n# This shows none of the rows are duplicate.","cf08ec0b":"#Setting CLIENTNUM as Index\ndf.set_index('CLIENTNUM', inplace=True)","72db36f0":"df.head()","c19d0393":"#Checking for Number of NA values\ndf.isna().sum()","06b59d03":"(df == 'Unknown').sum()","ee156eaa":"len(df[(df == 'Unknown').any(axis=1)].index)","599d0c9c":"plt.hist(df.loc[df['Income_Category']!='Unknown']['Income_Category']) \nplt.show()","4fe78a1e":"mostfrequentcategory_Marital_Status = df['Marital_Status'].mode()[0]\nmostfrequentcategory_Marital_Status","93664e55":"df['Marital_Status'].replace({'Unknown': mostfrequentcategory_Marital_Status} , inplace = True)","149fe852":"df['Marital_Status'].head(10)","23e7697a":"mostfrequentcategory_Income_Category = df['Income_Category'].mode()[0]\nmostfrequentcategory_Income_Category\ndf['Income_Category'].replace({'Unknown': mostfrequentcategory_Income_Category} , inplace = True)","57a3d0bc":"mostfrequentcategory_Education_Level = df['Education_Level'].mode()[0]\nmostfrequentcategory_Education_Level\ndf['Education_Level'].replace({'Unknown': mostfrequentcategory_Education_Level} , inplace = True)","6a505996":"(df == 'Unknown').sum()","bbffcb62":"df = pd.get_dummies(df, prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None)","f67f1d64":"df.head()","258d3891":"df.shape","a3b309fe":"X = df.drop('Attrition_Flag', axis=1)\ny = df['Attrition_Flag']","062c5a8b":"from sklearn.ensemble import ExtraTreesClassifier","9cd930bc":"model = ExtraTreesClassifier()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(30).plot(kind='barh')\nplt.show()","9d50154e":"#get correlations of each features in dataset\ncorrmat = df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","70e85554":"num_feats=20\nfeature_name = X.columns.tolist()\ndef cor_selector(X, y,num_feats):\n    cor_list = []\n    feature_name = X.columns.tolist()\n    # calculate the correlation with y for each feature\n    for i in X.columns.tolist():\n        cor = np.corrcoef(X[i], y)[0, 1]\n        cor_list.append(cor)\n    # replace NaN with 0\n    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n    # feature name\n    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n    # feature selection? 0 for not select, 1 for select\n    cor_support = [True if i in cor_feature else False for i in feature_name]\n    return cor_support, cor_feature\ncor_support, cor_feature = cor_selector(X, y,num_feats)\nprint(str(len(cor_feature)), 'selected features')\n","9241e7a7":"cor_feature","5a47d054":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import MinMaxScaler\nX_norm = MinMaxScaler().fit_transform(X)\nchi_selector = SelectKBest(chi2, k=num_feats)\nchi_selector.fit(X_norm, y)\nchi_support = chi_selector.get_support()\nchi_feature = X.loc[:,chi_support].columns.tolist()\nprint(str(len(chi_feature)), 'selected features')","bca9de34":"chi_feature","57002844":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nrfe_selector = RFE(estimator=LogisticRegression(solver = 'liblinear'), n_features_to_select=num_feats, step=10,  verbose=5)\nrfe_selector.fit(X_norm, y)","f969985d":"rfe_support = rfe_selector.get_support()\nrfe_feature = X.loc[:,rfe_support].columns.tolist()\nprint(str(len(rfe_feature)), 'selected features')","947c8a15":"rfe_feature","3cafb691":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression\n\nembeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l1\", solver = 'liblinear'), max_features=num_feats)\nembeded_lr_selector.fit(X_norm, y)","04d0ff0a":"embeded_lr_support = embeded_lr_selector.get_support()\nembeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()\nprint(str(len(embeded_lr_feature)), 'selected features')","cb1a46d6":"embeded_lr_feature","6500af01":"pd.set_option('display.max_rows', None)\n# put all selection together\nfeature_selection_df = pd.DataFrame({'Feature':feature_name, 'Pearson':cor_support, 'Chi-2':chi_support, 'RFE':rfe_support, 'Logistics':embeded_lr_support,})\n# count the selected times for each feature\nfeature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n# display the top 100\nfeature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\nfeature_selection_df.index = range(1, len(feature_selection_df)+1)\nfeature_selection_df.head(num_feats)","5bfdaa9d":"features = ['Total_Trans_Ct' , 'Total_Trans_Amt' , 'Total_Revolving_Bal' , 'Total_Relationship_Count' , 'Total_Ct_Chng_Q4_Q1',\n            'Total_Amt_Chng_Q4_Q1' , 'Months_Inactive_12_mon' , 'Marital_Status_Single' , 'Gender_F' , 'Credit_Limit' , \n            'Contacts_Count_12_mon']","dae74276":"X = X[features]","4c946e59":"from sklearn.model_selection import train_test_split","4b4f2877":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.30, random_state=42)","ab4ed00c":"from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, plot_confusion_matrix, plot_roc_curve\n","eb56a8b0":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix","ee4bffee":"model_lr = LogisticRegression(solver = 'liblinear')\nmodel_lr.fit(X_train, y_train)","47218079":"print(\"Coefficients of the Logistic Regression model\")\ncoef = model_lr.coef_\nintercept = model_lr.intercept_\nprint(\"Coef:\", coef)\nprint(\"Intercept:\", intercept)","daddb828":"predicted_classes_lr = model_lr.predict(X_test)\n","0cf9d995":"predicted_classes_lr_prob = model_lr.predict_proba(X_test)\n","adb3d37f":"print(\"Confusion matrix for LR model:\")\nconf_mat_lr = confusion_matrix(y_test.tolist(), predicted_classes_lr)\nprint(conf_mat_lr)\nsns.heatmap(conf_mat_lr, annot = True , fmt = 'g')\nplt.xlabel(\"Predicted classes\")\nplt.ylabel(\"Actual classes\")\nplt.show()","c5488b66":"accuracy_lr = accuracy_score(y_test, predicted_classes_lr)\nprint(\"accuracy score for LR model::\", accuracy_lr)","c7592994":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve","396b89ad":"logit_roc_auc = roc_auc_score(y_test, model_lr.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, model_lr.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","9b94a5a4":"def find_best_threshold(threshould, fpr, tpr):\n   t = threshould[np.argmax(tpr*(1-fpr))]\n   # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n   print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n   return t","5cc3cc27":"from math import *","8bd663dc":"# calculate roc curves\nfpr, tpr, thresholds = roc_curve(y_test, model_lr.predict_proba(X_test)[:,1])\n# get the best threshold\nJ = tpr - fpr\nix = np.argmax(J)\nbest_thresh = thresholds[ix]\nprint('Best Threshold=%f' % (best_thresh))","99221ff7":"THRESHOLD = 0.183734","97f136d4":"from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, roc_auc_score, precision_score","7324fdf3":"preds = np.where(model_lr.predict_proba(X_test)[:,1] > THRESHOLD, 1, 0)\n\npd.DataFrame(data=[accuracy_score(y_test, preds), recall_score(y_test, preds),\n                   precision_score(y_test, preds), roc_auc_score(y_test, preds)], \n             index=[\"accuracy\", \"recall\", \"precision\", \"roc_auc_score\"])","d0d194e5":"model_nb = GaussianNB()\nmodel_nb.fit(X_train, y_train)","1afd4c35":"predicted_classes_nb = model_nb.predict(X_test)","cbdae843":"print(\"Confusion matrix for NB model:\")\nconf_mat_nb = confusion_matrix(y_test.tolist(), predicted_classes_nb)\nprint(conf_mat_nb)\nsns.heatmap(conf_mat_nb, annot = True , fmt = 'g')\nplt.xlabel(\"Predicted classes\")\nplt.ylabel(\"Actual classes\")\nplt.show()","ab50c73c":"accuracy_nb = accuracy_score(y_test, predicted_classes_nb)\nprint(\"accuracy score for NB model::\", accuracy_nb)","a02d37d8":"model_knn = KNeighborsClassifier(n_neighbors = 2)\n# fitting model\nmodel_knn.fit(X_train,y_train)\n#predict\npredicted_classes_knn = model_knn.predict(X_test)","0b9cf7cf":"print(\"Confusion matrix for KNN model:\")\nconf_mat_knn = confusion_matrix(y_test.tolist(), predicted_classes_knn)\nprint(conf_mat_knn)\nsns.heatmap(conf_mat_knn, annot = True , fmt = 'g')\nplt.xlabel(\"Predicted classes\")\nplt.ylabel(\"Actual classes\")\nplt.show()","0d3620ce":"accuracy_knn = accuracy_score(y_test, predicted_classes_knn)\nprint(\"accuracy score for KNN model::\", accuracy_knn)","56686524":"from sklearn.model_selection import cross_val_score","38c6775d":"neighbors = list(range(1,10,2))\n# empty list that will hold cv scores\ncv_scores = [ ]\n#perform 10-fold cross-validation\nfor K in neighbors:\n    model_knn = KNeighborsClassifier(n_neighbors = K)\n    scores = cross_val_score(model_knn,X_train,y_train,cv = 10,scoring =\n    \"accuracy\")\n    cv_scores.append(scores.mean())","277b159a":"def plot_accuracy(knn_list_scores):\n    pd.DataFrame({\"K\":[i for i in range(1,10,2)],\n                  \"Accuracy\":knn_list_scores}).set_index(\"K\").plot.bar(figsize= (9,6),ylim=(0.78,1.00),rot=0)\n    plt.show()\nplot_accuracy(cv_scores)","9c6ab65a":"model_knn = KNeighborsClassifier(n_neighbors = 3)\n# fitting model\nmodel_knn.fit(X_train,y_train)\n#predict\npredicted_classes_knn = model_knn.predict(X_test)","85d8a1ef":"print(\"Confusion matrix for KNN model:\")\nconf_mat_knn = confusion_matrix(y_test.tolist(), predicted_classes_knn)\nprint(conf_mat_knn)\nsns.heatmap(conf_mat_knn, annot = True , fmt = 'g')\nplt.xlabel(\"Predicted classes\")\nplt.ylabel(\"Actual classes\")\nplt.show()","385eaf01":"accuracy_knn = accuracy_score(y_test, predicted_classes_knn)\nprint(\"accuracy score for KNN model::\", accuracy_knn)","d56793c7":"from sklearn.tree import DecisionTreeClassifier\n","1a665365":"model_dt = DecisionTreeClassifier()\nmodel_dt.fit(X_train,y_train)\nDecisionTreeClassifier()\npredicted_classes_dt = model_dt.predict(X_test)\nprint(\"Confusion matrix for DT model:\")\nconf_mat_dt = confusion_matrix(y_test.tolist(), predicted_classes_dt)\nprint(conf_mat_dt)\nsns.heatmap(conf_mat_dt, annot = True , fmt = 'g')\nplt.xlabel(\"Predicted classes\")\nplt.ylabel(\"Actual classes\")\nplt.show()\n\naccuracy_dt = accuracy_score(y_test, predicted_classes_dt)\nprint(\"accuracy score  for DT model::\", accuracy_dt)","52dcb7e7":"model_dt = DecisionTreeClassifier(criterion = 'entropy')\nmodel_dt.fit(X_train,y_train)\npredicted_classes_dt = model_dt.predict(X_test)\n\nprint(\"Confusion matrix for DT model:\")\nconf_mat_dt = confusion_matrix(y_test.tolist(), predicted_classes_dt)\nprint(conf_mat_dt)\nsns.heatmap(conf_mat_dt, annot = True , fmt = 'g')\nplt.xlabel(\"Predicted classes\")\nplt.ylabel(\"Actual classes\")\nplt.show()\n\naccuracy_dt = accuracy_score(y_test, predicted_classes_dt)\nprint(\"accuracy score  for DT model::\", accuracy_dt)","d069cc0e":"from sklearn.ensemble import RandomForestClassifier\nmodel_rf = RandomForestClassifier()\nmodel_rf.fit(X_train,y_train)\n\npredicted_classes_rf = model_rf.predict(X_test)\nprint(\"Confusion matrix for RF model:\")\nconf_mat_rf = confusion_matrix(y_test.tolist(), predicted_classes_rf)\nprint(conf_mat_rf)\nsns.heatmap(conf_mat_rf, annot = True , fmt = 'g')\nplt.xlabel(\"Predicted classes\")\nplt.ylabel(\"Actual classes\")\nplt.show()\n\naccuracy_rf = accuracy_score(y_test, predicted_classes_rf)\nprint(\"accuracy score for RF model::\", accuracy_rf)","5c5e31db":"model_rf = RandomForestClassifier(criterion = 'entropy')\nmodel_rf.fit(X_train,y_train)\npredicted_classes_rf = model_rf.predict(X_test)\n\nprint(\"Confusion matrix for RF model:\")\nconf_mat_rf = confusion_matrix(y_test.tolist(), predicted_classes_rf)\nprint(conf_mat_rf)\nsns.heatmap(conf_mat_rf, annot = True , fmt = 'g')\nplt.xlabel(\"Predicted classes\")\nplt.ylabel(\"Actual classes\")\nplt.show()\n\naccuracy_rf = accuracy_score(y_test, predicted_classes_rf)\nprint(\"accuracy score for RF model::\", accuracy_rf)","61eba58a":"from sklearn.svm import SVC\nmodel_svm = SVC()\nmodel_svm.fit(X_train,y_train)\n\npredicted_classes_svm = model_svm.predict(X_test)\nprint(\"Confusion matrix for SVM model:\")\nconf_mat_svm = confusion_matrix(y_test.tolist(), predicted_classes_svm)\nprint(conf_mat_svm)\nsns.heatmap(conf_mat_svm, annot = True , fmt = 'g')\nplt.xlabel(\"Predicted classes\")\nplt.ylabel(\"Actual classes\")\nplt.show()\n\n\naccuracy_svm = accuracy_score(y_test, predicted_classes_svm)\nprint(\"accuracy score for SVM model::\", accuracy_svm)","0c2449fc":"from sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import SGDClassifier\nmodel_sgdc = OneVsRestClassifier(SGDClassifier())\nmodel_sgdc.fit(X_train,y_train)\n\npredicted_classes_sgdc = model_sgdc.predict(X_test)\nprint(\"Confusion matrix for SGDC model:\")\nconf_mat_sgdc = confusion_matrix(y_test.tolist(), predicted_classes_sgdc)\nprint(conf_mat_sgdc)\nsns.heatmap(conf_mat_sgdc, annot = True , fmt = 'g')\nplt.xlabel(\"Predicted classes\")\nplt.ylabel(\"Actual classes\")\nplt.show()\n\n\naccuracy_sgdc = accuracy_score(y_test, predicted_classes_sgdc)\nprint(\"accuracy score for SGDC model::\", accuracy_sgdc)","4060fcae":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nmodel_lda = LinearDiscriminantAnalysis()\nmodel_lda.fit(X_train,y_train)\npredicted_classes_lda = model_lda.predict(X_test)\n\nprint(\"Confusion matrix for LDA model:\")\nconf_mat_lda = confusion_matrix(y_test.tolist(), predicted_classes_lda)\nprint(conf_mat_lda)\nsns.heatmap(conf_mat_lda, annot = True , fmt = 'g')\nplt.xlabel(\"Predicted classes\")\nplt.ylabel(\"Actual classes\")\nplt.show()\n\naccuracy_lda = accuracy_score(y_test, predicted_classes_lda)\nprint(\"accuracy score for LDA model::\", accuracy_lda)","5bb8ceb9":"2) Gaussian Naive Bayes","ae856e0f":"1) Logistic Regression","2db43cea":"MODEL BUILDING ","3d678f1b":"Chi-Square Features","b7e6def4":"n = 3 is the optimal value","c8d3d8c1":"Lasso: SelectFromMode","6c437c65":"3) KNN Nodel","b6a19fad":"7) SGDC Classifier","037111a1":"This Shows there isn't any NA values in the dataframe","1f88db24":"Recursive Feature Elimination","2993882f":"ROC Curve and then calculate threshold probabilty\n\n*   List item\n*   List item\n\n","2614c6f3":"Random Forest","a3f46bfe":"Pearson Correlation","5b56b758":"Feature Selection ","fd2dbc39":"8) Linear Discriminant Analysis","e81331ec":"6) Support Vector Machine","e63a1464":"4) Decision Tree"}}