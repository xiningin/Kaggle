{"cell_type":{"7bc9b1b7":"code","db0a04c1":"code","41354e7f":"code","20c90e47":"code","9b1d4ba2":"code","224795b3":"code","187e3417":"code","693bd80c":"code","ccfa54bf":"code","6d37f685":"code","15f656c3":"code","b3fee8eb":"code","6993456a":"code","a7266d1f":"code","701c35c4":"code","0abbeb90":"code","6fbd657c":"code","107e27f3":"code","d27e48a9":"code","4496b4d9":"code","5fe09af5":"code","e592ca33":"code","4b485010":"code","e1700869":"code","265d81fd":"code","d6f06a4a":"code","5b57b08d":"code","a12bccbe":"code","c417f871":"code","228557ee":"code","0fce941c":"code","5a056eef":"code","9f8c5bfc":"code","ea2e75b8":"code","6f6a81f5":"code","146f616b":"code","8d32166e":"code","79ddb26a":"code","7aa9cfd2":"code","e03c9774":"code","6af0e881":"markdown","fa548df1":"markdown","60a828ef":"markdown","f6b8606e":"markdown","be18501c":"markdown","59f3db9b":"markdown","334ff46f":"markdown","5c7c56cb":"markdown","68a2eb57":"markdown","d768d6ce":"markdown","db2a6a79":"markdown","3e080e81":"markdown"},"source":{"7bc9b1b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","db0a04c1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)","41354e7f":"from scipy import stats\nfrom scipy.stats import norm, skew #for some statistics","20c90e47":"pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points","9b1d4ba2":"#train = pd.read_csv('train.csv',parse_dates=[['weblog_date'],['date_of_advert'],['last_advert_online']])\n#test = pd.read_csv('test.csv',parse_dates=[weblog_date],[[date_of_advert],[last_advert_online])\ntrain = pd.read_csv('\/kaggle\/input\/intercampusai2019\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/intercampusai2019\/test.csv')","224795b3":"#save and drop the target varriable\ny_train=train['Promoted_or_Not']","187e3417":"#Save the 'Id' column\ntrain_ID = train['EmployeeNo']\ntest_ID = test['EmployeeNo']","693bd80c":"#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"EmployeeNo\", axis = 1, inplace = True)\ntest.drop(\"EmployeeNo\", axis = 1, inplace = True)\n\n","ccfa54bf":"#dropping the target varriable\ntrain.drop('Promoted_or_Not',axis=1,inplace=True)","6d37f685":"#Data PreProcessing\n# check number & percentage of missing value in the columns\ndef missing_values_table(df):\n  mis_val = df.isnull().sum() #total missing values\n  mis_val_percent = 100 * df.isnull().sum() \/ len(df) #percentage of missing values\n  mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1) #make a table with the results\n  mis_val_table_ren_columns = mis_val_table.rename(\n  columns = {0 : 'Missing Values', 1 : '% of Total Values'}) #rename the columns\n     # sort the table by percentage of missing value\n  mis_val_table_ren_columns = mis_val_table_ren_columns[\n  mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n\n        #print same summary information\n  print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n\n          # return the dataframe with missing information\n  return mis_val_table_ren_columns\n  \nmissing_values = missing_values_table(train)\nmissing_values.head()","15f656c3":"#Handling Missing values\ntrain['Qualification'].fillna('uncertified',inplace=True)\ntest['Qualification'].fillna('uncertified',inplace=True)","b3fee8eb":"#function to defined the school ranking based on foreign school and qualification\ndef School_rank(Foreign_schooled,Qualification):\n    if Foreign_schooled == 'Yes' and Qualification == 'MSc, MBA and PhD':\n     return 5\n    if Foreign_schooled == 'Yes' and Qualification == 'First Degree or HND':\n     return 4\n    if Foreign_schooled == 'No' and Qualification == 'MSc, MBA and PhD':\n     return 3\n    if Foreign_schooled == 'No' and Qualification == 'First Degree or HND':\n     return 2\n    else:\n     return 1","6993456a":"#we could add hirarchical feature of people that foreigned schooled and PHD:4,with First degree :3,Local(PHD):3,Local Bsc:2,uneducated:1,noinfo:1 \ntrain['School_rank']=train.apply(lambda x:School_rank(x['Foreign_schooled'],x['Qualification']),axis=1)\ntest['School_rank']=test.apply(lambda x:School_rank(x['Foreign_schooled'],x['Qualification']),axis=1)","a7266d1f":"#adding new division description\ndef ConvertDivisionToFeature(desc):\n  Division={\n      'Commercial Sales and Marketing':'CSM',\n      'Customer Support and Field Operations':'CSFO',\n      'Information and Strategy':'IS',\n      'Information Technology and Solution Support':'ITSS',\n      'Sourcing and Purchasing':'SP',\n      'Business Finance Operations':'BFO',\n      'People\/HR Management':'PHM',\n      'Research and Innovation':'RI',\n      'Regulatory and Legal services':'RLS'\n      }\n  return Division[desc]","701c35c4":"train['Division']=train['Division'].apply(ConvertDivisionToFeature)\ntest['Division']=test['Division'].apply(ConvertDivisionToFeature)","0abbeb90":"#function for channel \ndef convertChannelToFeature(desc):\n  Channel={\n      'Direct Internal process':'DIP',\n      'Agency and others':'AO',\n      'Referral and Special candidates':'RSC'\n      }\n  return Channel[desc]\n\ntrain['Channel_of_Recruitment']=train['Channel_of_Recruitment'].apply(convertChannelToFeature)\ntest['Channel_of_Recruitment']=test['Channel_of_Recruitment'].apply(convertChannelToFeature)","6fbd657c":"#categorized state into six geo-political zones\ndef ConvertToGeoPoliticalZone(desc):\n  \n  State={\n      \n      'BENUE':'NC',\n      'KOGI':'NC',\n      'KWARA':'NC',\n      'NASSARAWA':'NC',\n      'NIGER':'NC',\n      'PLATEAU':'NC',\n      'FCT':'NC',\n      \n      'ADAMAWA':'NE',\n      'BAUCHI':'NE',\n      'BORNO':'NE',\n      'GOMBE':'NE',\n      'TARABA':'NE',\n      'YOBE':'NE',\n      \n      \n          \n      'JIGAWA':'NW',\n      'KADUNA':'NW',\n      'KANO':'NW',\n      'KATSINA':'NW',\n      'KEBBI':'NW',\n      'SOKOTO':'NW',\n      'ZAMFARA':'NW',\n      \n          \n      'ABIA':'SE',\n      'ANAMBRA':'SE',\n      'EBONYI':'SE',\n      'ENUGU':'SE',\n      'IMO':'SE',\n      \n      'AKWA IBOM':'SS',\n      'BAYELSA':'SS',\n      'CROSS RIVER':'SS',\n      'RIVERS':'SS',\n      'DELTA':'SS',\n      'EDO':'SS',\n     \n        \n      'EKITI':'SW',\n      'LAGOS':'SW',\n      'OGUN':'SW',\n      'ONDO':'SW',\n      'OSUN':'SW',\n      'OYO':'SW'\n      }\n  return State[desc]\n\n#one hot encode\/label encode\ntrain['State_Of_Origin']=train['State_Of_Origin'].apply(ConvertToGeoPoliticalZone)\ntest['State_Of_Origin']=test['State_Of_Origin'].apply(ConvertToGeoPoliticalZone)","107e27f3":"#function to handle number of previous employers rank\ndef ConvertNumberOfPreviousEmployerFeature(desc):\n  Past={\n      '0':'0',\n      '1':'1',\n      '2':'2',\n      '3':'3',\n      '4':'4',\n      '5':'5',\n      'More than 5':'7'\n      }\n  return Past[desc]\n\n#This column is not actually numerical col change to numerical and retry for other models like LGB,XGboost,Randomforest\ntrain['No_of_previous_employers']=train['No_of_previous_employers'].apply(ConvertNumberOfPreviousEmployerFeature)\ntest['No_of_previous_employers']=test['No_of_previous_employers'].apply(ConvertNumberOfPreviousEmployerFeature)","d27e48a9":"#function to calculate diff in year\nfrom datetime import date\ndef CalculateYear(year):\n  today=date.today()\n  age=today.year-year\n  return age","4496b4d9":"train['No_Of_Year_Spent']=train['Year_of_recruitment'].apply(CalculateYear)\ntest['No_Of_Year_Spent']=test['Year_of_recruitment'].apply(CalculateYear)\n\ntrain['Age_in_years']=train['Year_of_birth'].apply(CalculateYear)\ntest['Age_in_years']=test['Year_of_birth'].apply(CalculateYear)","5fe09af5":"#dropping Year_of_recruitment and Year_of_birth\ntrain.drop('Year_of_recruitment', axis = 1, inplace = True)\ntest.drop('Year_of_recruitment', axis = 1, inplace = True)\n\ntrain.drop('Year_of_birth', axis = 1, inplace = True)\ntest.drop('Year_of_birth', axis = 1, inplace = True)","e592ca33":"#rounding up last performance score to integer\ntrain['Last_performance_score']=train['Last_performance_score'].round().astype(int)\ntest['Last_performance_score']=test['Last_performance_score'].round().astype(int)","4b485010":"#label encode one hot encode categorical varribles\n\n\nqualitative_new=['Division',\n 'Qualification',\n 'Gender',\n 'Channel_of_Recruitment',\n 'State_Of_Origin',\n 'Foreign_schooled',\n 'Marital_Status',\n 'Past_Disciplinary_Action',\n 'Previous_IntraDepartmental_Movement',\n  'No_of_previous_employers'\n  ]\n\n#LabelEncoder\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nfor column in qualitative_new :\n    train[column] = le.fit_transform(train[column])\n    \n\nfor column in qualitative_new :\n    test[column] = le.fit_transform(test[column])\n    ","e1700869":"#creating copies of train and test set\ntrain_processed_copy=train.copy()\ntest_processed_copy=test.copy()","265d81fd":"train","d6f06a4a":"import xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn import metrics   #Additional scklearn functions\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV   #Perforing grid search\n\nimport matplotlib.pylab as plt\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4\n\nfrom sklearn.model_selection import train_test_split","5b57b08d":"train.dtypes","a12bccbe":"def modelfit(alg,dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n    \n    if useTrainCV:\n        xgb_param = alg.get_xgb_params()\n        #x_trn,x_valid, y_trn, y_valid = train_test_split(train,y_train, test_size = 0.2, random_state = 42)\n        xgtrain = xgb.DMatrix(train.values, label=y_train.values)\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n        alg.set_params(n_estimators=cvresult.shape[0])\n    \n    #Fit the algorithm on the data\n    alg.fit(dtrain[predictors],y_train,eval_metric='auc')\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain[predictors])\n    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n        \n    #Print model report:\n    print (\"\\nModel Report\")\n    print (\"Accuracy : %.4g\" % metrics.accuracy_score(y_train.values, dtrain_predictions))\n    print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(y_train, dtrain_predprob))\n                    \n    feat_imp = pd.Series(alg.get_booster().get_fscore()).sort_values(ascending=False)\n    feat_imp.plot(kind='bar', title='Feature Importances')\n    plt.ylabel('Feature Importance Score')","c417f871":"#Choose all predictors except target & IDcols\npredictors =train.columns\nxgb1 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\nmodelfit(xgb1,train,predictors)","228557ee":"param_test1 = {\n 'max_depth':range(3,10,2),\n 'min_child_weight':range(1,6,2)\n}\ngsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=1000, max_depth=5,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch1.fit(train[predictors],y_train)\ngsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_","0fce941c":"param_test2 = {\n 'max_depth':[4,5,7],\n 'min_child_weight':[6,8,10,12]\n}\ngsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=200, max_depth=5,\n min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch2.fit(train[predictors],y_train)\ngsearch2.cv_results_, gsearch2.best_params_, gsearch2.best_score_","5a056eef":"param_test2b = {\n 'min_child_weight':[2,4,6,8]\n}\ngsearch2b = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=1000, max_depth=5,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test2b, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch2b.fit(train[predictors],y_train)\ngsearch2.cv_results_, gsearch2.best_params_, gsearch2.best_score_'''","9f8c5bfc":"modelfit(gsearch3.best_estimator_, train, predictors)\ngsearch2b.grid_scores_, gsearch2b.best_params_, gsearch2b.best_score_","ea2e75b8":"param_test3 = {\n 'gamma':[i\/10.0 for i in range(0,5)]\n}\ngsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=1000, max_depth=4,\n min_child_weight=10, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch3.fit(train[predictors],y_train)\ngsearch3.cv_results_, gsearch3.best_params_, gsearch3.best_score_","6f6a81f5":"xgb2 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=7,\n min_child_weight=2,\n gamma=0.2,\n subsample=0.9,\n colsample_bytree=0.7,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\nmodelfit(xgb2, train, predictors)","146f616b":"param_test4 = {\n 'subsample':[i\/10.0 for i in range(6,10)],\n 'colsample_bytree':[i\/10.0 for i in range(6,10)]\n}\ngsearch4 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=1000, max_depth=7,\n min_child_weight=2, gamma=0.2, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test4, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch4.fit(train[predictors],y_train)\ngsearch4.cv_results_, gsearch4.best_params_, gsearch4.best_score_","8d32166e":"param_test5 = {\n 'subsample':[i\/100.0 for i in range(75,90,5)],\n 'colsample_bytree':[i\/100.0 for i in range(75,90,5)]\n}\ngsearch5 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=1000, max_depth=7,\n min_child_weight=2, gamma=0.2, subsample=0.9, colsample_bytree=0.7,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test5, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch5.fit(train[predictors],y_train)\ngsearch5.cv_results_, gsearch5.best_params_, gsearch5.best_score_'''","79ddb26a":"param_test6 = {\n 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n}\ngsearch6 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=1000, max_depth=7,\n min_child_weight=2, gamma=0.2, subsample=0.9, colsample_bytree=0.7,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test6, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch6.fit(train[predictors],y_train)\ngsearch6.cv_results_, gsearch6.best_params_, gsearch6.best_score_","7aa9cfd2":"\nxgb3 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=7,\n min_child_weight=2,\n gamma=0.2,\n subsample=0.9,\n colsample_bytree=0.7,\n reg_alpha=1,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\nmodelfit(xgb3, train, predictors)","e03c9774":"xgb4 = XGBClassifier(\n learning_rate =0.03,\n n_estimators=5000,\n max_depth=7,\n min_child_weight=2,\n gamma=.2,\n subsample=0.9,\n colsample_bytree=0.7,\n reg_alpha=1,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\nmodelfit(xgb4, train, predictors)","6af0e881":" So the final parameters are:\n\nmax_depth: 7 min_child_weight: 2 gamma: 0.2\n\nStep 4: Tune subsample and colsample_bytree trying different subsample and colsample_bytree values.  did this in 2 stages  and take values 0.6,0.7,0.8,0.9 for both to start with.","fa548df1":"Step 1:\nMy first step is to Fix the learning rate(say 0.1) and number of estimators(estimators:1000) for xgboost in order to decide on other boosting parameters,i  set some initial values of other parameters too. Lets take the following values first for max_depth,min_child_weight,gamma,subsample,:\n\nmax_depth = 5 :usually from documentation This should be between 3-10. I started with 5 but you can choose a different number as well. 4-6 can be good starting points.\nmin_child_weight = 1 : A smaller value is chosen because it is a highly imbalanced class problem and leaf nodes can have smaller size groups.\ngamma = 0 : A smaller value like 0.1-0.2 can also be chosen for starting. This will anyways be tuned later.\nsubsample, colsample_bytree = 0.8 : This is a commonly used as  start value. Typical values range is between 0.5-0.9.\nscale_pos_weight = 1: Because of high class imbalance.\n\nPlease note that all the above are just initial estimates and will be tuned later. Lets take the default learning rate of 0.1 here and check the optimum number of trees using cv function of xgboost. The function defined below will do it for us.","60a828ef":"Here  0.9 and 0.7 is the optimum value for both subsample and colsample_bytree. Now we should  0.05 interval around these was tried.","f6b8606e":" value of gamma, i.e. 0.2 is the optimum one. Before proceeding, a good idea would be to re-calibrate the number of boosting rounds for the updated parameters.","be18501c":"Step 2:now lets tune max_depth and min_child_weight with the learning rate and n_estimators constant. We tune  max_depth and min_child_weight first as they will have the highest impact on model outcome.To start with, let\u2019s set wider ranges and then we will perform another iteration for smaller ranges.\n\nImportant Note:grid search is done in this section which can take 15-30 mins or even more time to run depending on your system. You can vary the number of values you are testing based on what your system can handle.","59f3db9b":" got the same values as first. Thus the optimum values are:\n\nsubsample: 0.9 colsample_bytree: 0.7\n\nStep 5: Tuning Regularization Parameters(reg_lambda,reg_alpha) to reduce overfitting.tunining \u2018reg_alpha\u2019 value got 0.05 .","334ff46f":"Here, we get the optimum values as 4 for max_depth and 6 for min_child_weight. Also, we can see the CV score increasing slightly. Note that as the model performance increases, it becomes exponentially difficult to achieve even marginal gains in performance. You would have noticed that here we got 6 as optimum value for min_child_weight but we haven\u2019t tried values more than 6. We can do that as follow:.","5c7c56cb":"REFERENCES\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/","68a2eb57":"\nStep 6: Reducing Learning Rate Lastly,lowering the learning rate and add more trees and see.","d768d6ce":"regularization parameters increase my score a bit.","db2a6a79":"Step 3: Tune gamma Now lets tune gamma value using the parameters already tuned above. Gamma can take various values but I\u2019ll check for 5 values here. You can go into more precise values as.","3e080e81":"The ideal values are 7 for max_depth and 2 for min_child_weight. Lets go one step deeper and look for optimum values.\nusing values 1 above and below the optimum values to check results"}}