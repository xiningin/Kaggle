{"cell_type":{"d481e67a":"code","3225e8d7":"code","e5b5e61c":"code","2f9aa953":"code","8cd3e5be":"code","4514c003":"code","44871c8c":"code","6246c5fe":"code","936142e3":"code","2a4da7c8":"code","fdc61cbd":"code","bc9a7e3b":"code","6f129918":"code","2f901e8e":"code","8248b0a1":"code","c8951e1a":"code","b4ecee0b":"code","519ad145":"code","297f5536":"code","e5e04d02":"code","99f95c5e":"code","5ba6464a":"code","5af18427":"code","b73f6c9a":"code","2b6ed993":"code","fb370a12":"code","3d7e7f9a":"code","9b573dfe":"code","c755a804":"code","beb740ad":"code","2bc8d1cc":"code","340a90b6":"code","c2c04680":"code","2b89e6e3":"code","473db2f8":"code","83127625":"code","cf77b6a5":"code","f7aadb3e":"code","a97d19e4":"code","ffd3cbb1":"code","174f6e1d":"code","4b7fc492":"code","e7b40a04":"code","63342d7a":"code","3c7c51f7":"code","4d662768":"code","743bde01":"code","901dff54":"code","a14db1de":"code","53d6eb02":"code","470ee626":"code","105f9290":"code","5e4e5b4b":"code","3cb4823e":"code","d94b352d":"code","eb88b47d":"code","cd0b63db":"code","d735674a":"code","e01db200":"code","e25da045":"code","d61b5e57":"code","045fb8d4":"code","0de50165":"code","f876603d":"code","ad81d4ab":"code","bd082933":"code","7df4d3be":"code","0c0889fe":"markdown","7ffdd683":"markdown","7b42dd98":"markdown","e1852903":"markdown","5da818d0":"markdown","e88c54b3":"markdown","635ea87e":"markdown","358bf484":"markdown","c57b5db0":"markdown","b54d7993":"markdown","0b6d7a0f":"markdown","d3f3bb55":"markdown","1b485bf7":"markdown","cc613ac4":"markdown","cbf33b3b":"markdown","2ce79173":"markdown"},"source":{"d481e67a":"# Imports and Required Libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report, plot_roc_curve, roc_curve\nfrom sklearn.model_selection import train_test_split, cross_validate, KFold\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV, cross_validate, RandomizedSearchCV, validation_curve, cross_val_score\nfrom sklearn.calibration import calibration_curve\nimport warnings\nimport optuna\nfrom optuna import Trial, visualization\nfrom optuna.samplers import TPESampler\n\n\n#Pandas Adjustments\npd.set_option('display.max_columns', None)\nwarnings.simplefilter(action='ignore', category=Warning)\npd.set_option('display.float_format', lambda x: '%.5f' % x)","3225e8d7":"#Data Reading\ncredit_data = pd.read_csv(\"..\/input\/cr-loan-data\/cr_loan2.csv\")\ncredit_data.head()\n","e5b5e61c":"credit_data.info() ","2f9aa953":"#I'm gonna take a copy of the real dataset in case if we need the raw dataset.\ncredit_new = credit_data.copy()","8cd3e5be":"#Descriptive Statistics\n\ndef descriptive_stats(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\ndescriptive_stats(credit_new)","4514c003":"#Frequency Plot of the Loan Status\nsns.countplot(x = \"loan_status\", data = credit_new)\nplt.show()","44871c8c":"#Exploring with Cross Tables(It's just pivot actually.)\n#cross table of person home ownership, loan status by loan interest rate\npd.crosstab(credit_new[\"person_home_ownership\"], credit_new[\"loan_status\"], values = credit_new[\"loan_int_rate\"], aggfunc= \"mean\").round(2)","6246c5fe":"#cross table of the loan intent and loan status\npd.crosstab(credit_new[\"loan_intent\"], credit_new[\"loan_status\"], margins = True)","936142e3":"#cross table of home ownership, loan status, and grade\npd.crosstab(credit_new[\"person_home_ownership\"],[credit_new[\"loan_status\"],credit_new[\"loan_grade\"]])","2a4da7c8":"#Cross table of home ownership, loan status, and average percent income\npd.crosstab(credit_new[\"person_home_ownership\"], credit_new[\"loan_status\"],\n              values=credit_new[\"loan_percent_income\"], aggfunc=\"mean\")","fdc61cbd":"#Distribution of Loan Amounts\n\nn, bins, patches = plt.hist(x = credit_new[\"loan_amnt\"], bins = \"auto\", color = \"blue\", alpha = 0.7, rwidth = 0.95)\nplt.xlabel(\"Loan Amount\")\nplt.show()","bc9a7e3b":"#Scatter plot of Income Against Age\nplt.scatter(credit_new[\"person_income\"], credit_new[\"person_age\"], c = \"blue\", alpha = 0.5)\nplt.xlabel(\"Income\")\nplt.ylabel(\"Age\")\nplt.show()","6f129918":"#Box plot of the percentage income by loan status\ncredit_new.boxplot(column = ['loan_percent_income'], by = 'loan_status')\nplt.title('Average Percent Income by Loan Status')\nplt.suptitle('')\nplt.show()","2f901e8e":"def grab_col_names(dataframe, cat_th=10, car_th=20):\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and dataframe[col].dtypes == \"O\"]\n\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car,num_but_cat\n\ncat_cols, num_cols, cat_but_car,num_but_cat =grab_col_names(credit_new)","8248b0a1":"num_but_cat\n#Its the dependent variable so we're gonna ignore that.","c8951e1a":"cat_cols\n#there are 5 categorical variables.\n","b4ecee0b":"num_cols\n\n#there are 7 numerical variables.","519ad145":"#Grouping the mean of the numerical variables according to the credit default\n\ndef target_summary_with_num(dataframe, target, numerical_col):\n    print(dataframe.groupby(target).agg({numerical_col: \"mean\"}), end=\"\\n\\n\\n\")\nfor col in num_cols:\n    target_summary_with_num(credit_new, \"loan_status\",col)","297f5536":"#Let's define our threshold function.\ndef outlier_thresholds(dataframe, col_name, q1=0.01, q3=0.99):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n\nfor col in num_cols:\n    print(col, check_outlier(credit_new, col))\n#person_age, person_income and person_emp_lenght have some outlier observations.","e5e04d02":"outlier_thresholds(credit_new, num_cols)\n","99f95c5e":"credit_new[credit_new[\"person_age\"] > 93].index\n#There are six people who are older then 93.","5ba6464a":"#Replacing Outliers\n\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n\nfor col in num_cols:\n    replace_with_thresholds(credit_new, col)\n\n#Check\nfor col in num_cols:\n    print(col, check_outlier(credit_new, col))\n\n#Now there are no outliers.\n","5af18427":"credit_new.isnull().sum()\n\n","b73f6c9a":"(3116 + 895) \/ len(credit_new)\n# We have null values in 2 numeric variabels and they are 12% percent of our dataset.\n# We need to fill them because we can't lose 12% of the data.\n# We can fill them with median, mean and KNN Imputer. I'm gonna fill them with median.","2b6ed993":"credit_new[\"person_emp_length\"].fillna((credit_new[\"person_emp_length\"].median()), inplace = True)\ncredit_new[\"loan_int_rate\"].fillna((credit_new[\"loan_int_rate\"].mean()), inplace = True)\n#check\ncredit_new[\"loan_int_rate\"].isnull().sum()\n#Now there are no missing values in our data.","fb370a12":"def rare_analyser(dataframe, target, cat_cols):\n    for col in cat_cols:\n        print(col, \":\", len(dataframe[col].value_counts()))\n        print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(),\n                            \"RATIO\": dataframe[col].value_counts() \/ len(dataframe),\n                            \"TARGET_MEAN\": dataframe.groupby(col)[target].mean()}), end=\"\\n\\n\\n\")\nrare_analyser(credit_new, \"loan_status\", cat_cols)\n\n#One Hot Encoding\ndef one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe\nohe_cols = [col for col in credit_new.columns if 10 >= credit_new[col].nunique() >= 2 and col not in \"loan_status\"]\ncredit_new2 = one_hot_encoder(credit_new, ohe_cols,drop_first=True)\n\ncredit_new2.columns","3d7e7f9a":"#Multivariate Logistic Regression\n\ny = credit_new2[\"loan_status\"]\nX = credit_new2.drop([\"loan_status\"], axis=1)\n\n#Model\nlogit = LogisticRegression().fit(X,y)","9b573dfe":"logit.coef_","c755a804":"#Predictions\ny_pred = logit.predict(X)\ny_pred","beb740ad":"# Confusion Matrix\ndef plot_confusion_matrix(y, y_pred):\n    acc = round(accuracy_score(y, y_pred), 2)\n    cm = confusion_matrix(y, y_pred)\n    sns.heatmap(cm, annot=True, fmt=\".0f\")\n    plt.xlabel('y_pred')\n    plt.ylabel('y')\n    plt.title('Accuracy Score: {0}'.format(acc), size=10)\n    plt.show()\n\nplot_confusion_matrix(y, y_pred)","2bc8d1cc":"#Model Validation: Holdout\n#Splitting the Data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .20, random_state = 112224)","340a90b6":"#Building Model with Train Set\nlogit_model = LogisticRegression().fit(X_train, y_train)\n\n#Predicting the test set\ny_pred = logit_model.predict(X_test)\n\n#For AUC Score(Prob of Being Default)\ny_prob = logit_model.predict_proba(X_test)[:, 1]\n\n\n\n#Classification Report\nprint(classification_report(y_test, y_pred))\n\n\n#F-1 score is very low.","c2c04680":"#ROC Curve & ROC AUC Score\n\n# ROC Curve\nplot_roc_curve(logit_model, X_test, y_test)\nplt.title('ROC Curve')\nplt.plot([0, 1], [0, 1], 'r--')\nplt.show()\n\nroc_auc_score(y_test, y_prob)","2b89e6e3":"#Model Validation: 5-Fold Cross Validation\n\ny = credit_new2[\"loan_status\"]\nX = credit_new2.drop([\"loan_status\"], axis=1)\n\n#Model\nlogit = LogisticRegression().fit(X,y)\n\n\n","473db2f8":"cv_logit_acc = cross_val_score(logit, X,y,cv = 10, scoring = \"accuracy\")\ncv_logit_f1 = cross_val_score(logit, X,y,cv = 10, scoring = \"f1\")\ncv_logit_roc = cross_val_score(logit, X,y,cv = 10, scoring = \"roc_auc\")\n","83127625":"cv_logit_acc.mean()\n#Accuracy = 0.8025226171619755","cf77b6a5":"cv_logit_f1.mean()\n#F-1 Score = 0.249315735453883","f7aadb3e":"cv_logit_roc.mean()\n#ROC AUC Score = 0.7690047691764843","a97d19e4":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .20, random_state = 112224)","ffd3cbb1":"#Hyperparameter Optimization(Model Tuning)\n\n\n#Objective function\ndef objective(trial):\n\n    data = credit_new2\n\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=112224)\n\n\n\n    params = {\n\n        'objective': 'binary:logistic',\n\n        'max_depth': trial.suggest_int('max_depth', 1, 9),\n\n        'n_estimators': trial.suggest_int('n_estimators', 10, 1000),\n\n        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-8, 1.0)\n\n    }\n\n\n\n    clf_gbt = xgb.XGBClassifier(**params)\n\n    clf_gbt.fit(X_train, y_train, eval_metric = 'error')\n\n\n\n    gbt_preds = clf_gbt.predict(X_test)\n\n\n\n    accuracy = accuracy_score(y_test, gbt_preds)\n\n    return (1-accuracy)\n\n\n\nif __name__ == '__main__':\n\n\n\n    study = optuna.create_study()\n\n    study.optimize(objective, n_trials=50)\n\n\n\n    print(study.best_params)\n   \n    ","174f6e1d":"#Training the model\nclf_gbt = xgb.XGBClassifier(**study.best_params).fit(X_train, np.ravel(y_train), eval_metric = \"error\")\n#Creating prob of predictions\ngbt_preds = clf_gbt.predict_proba(X_test)\n\n#Comparison of the true and predicted values\npreds_df = pd.DataFrame(gbt_preds[:,1], columns = ['gbt_prob_default'])\ntrue_df = y_test\n\nprint(pd.concat([true_df.reset_index(drop = True), preds_df], axis = 1).head(50))","4b7fc492":"#Portfolio Performance\ny_prob_df = pd.DataFrame(y_prob, columns = [\"lr_prob_default\"])\nportfolio = pd.concat([preds_df.reset_index(drop=True), y_prob_df], axis = 1)\n\nloans = X_test[\"loan_amnt\"]\nloans.reset_index(drop=True, inplace=True)\nportfolio[\"loan_amount\"] = loans\nportfolio.head()","e7b40a04":"#We assume that the loss given default(lgd) is 0,2 and it's constant.\n#Creating Expected Loss Columns\nlgd = 0.2\nportfolio[\"gbt_expected_loss\"] = portfolio[\"gbt_prob_default\"] * lgd * portfolio[\"loan_amount\"]\nportfolio[\"lr_expected_loss\"] = portfolio[\"lr_prob_default\"] * lgd * portfolio[\"loan_amount\"]\n\nportfolio.head()","63342d7a":"# Sum of the expected loss for lr\nprint('LR expected loss: ', np.sum(portfolio['lr_expected_loss']))","3c7c51f7":"# Sum of the expected loss for gbt\nprint('XGB expected loss: ', np.sum(portfolio['gbt_expected_loss']))","4d662768":"LR_expected_loss =  3108547.5195690366\nGBT_expected_loss = 3044509.2289176947\nprint(LR_expected_loss - GBT_expected_loss)","743bde01":"# Prediction labels for loan status\ngbt_preds = clf_gbt.predict(X_test)\n\n# Classification report of the model\ntarget_names = ['Non-Default', 'Default']\nprint(classification_report(y_test, gbt_preds, target_names=target_names))\n\n#XGBoost model is superior than the logit model.","901dff54":"#Variable Importance for XGBoost\n\nprint(clf_gbt.get_booster().get_score(importance_type = 'weight'))","a14db1de":"#Cross Validation for XGBoost Model \n\ncv_results_accuracy= cross_val_score(clf_gbt, X,y,cv = 10, scoring = \"accuracy\")\n","53d6eb02":"cv_results_accuracy.mean()","470ee626":"cv_results_f1= cross_val_score(clf_gbt, X,y,cv = 10, scoring = \"f1\")\n","105f9290":"cv_results_f1.mean()","5e4e5b4b":"cv_results_roc= cross_val_score(clf_gbt, X,y,cv = 10, scoring = \"roc_auc\")\n\n","3cb4823e":"cv_results_roc.mean()","d94b352d":"#Comparing ROC AUC Scores of the Models (Hold-Out)\n\nclf_logistic_preds = logit_model.predict(X_test)\nclf_gbt_preds = clf_gbt.predict(X_test)\n\n# Print the logistic regression AUC with formatting\nprint(\"Logistic Regression AUC Score: %0.2f\" % roc_auc_score(y_test, clf_logistic_preds))","eb88b47d":"# Print the gradient boosted tree AUC with formatting\nprint(\"Gradient Boosted Tree AUC Score: %0.2f\" % roc_auc_score(y_test, clf_gbt_preds))","cd0b63db":"#First we need to create a dataset which contains XGBoost's prob of defaults and actual loan status.\n\ngbt_preds = clf_gbt.predict_proba(X_test)\n\npreds_df = pd.DataFrame(gbt_preds[:,1], columns = ['gbt_prob_default'])\ntrue_df = y_test\n\ntest_pred_df = pd.concat([true_df.reset_index(drop = True), preds_df], axis = 1)\n\n#Summary Statistics\nprint(test_pred_df[\"gbt_prob_default\"].describe())","d735674a":"#Assume that we want to accept the loans which have probability below 85% .\n#Threshold\nthreshold_85 = np.quantile(test_pred_df['gbt_prob_default'], 0.85)\nthreshold_85\n#So we will reject the loans above our threshold rate.","e01db200":"#Applying Acceptance Rate\ntest_pred_df['pred_loan_status'] = test_pred_df['gbt_prob_default'].apply(lambda x: 1 if x > threshold_85 else 0)\n\nprint(test_pred_df['pred_loan_status'].value_counts())","e25da045":"#Visualizing the Acceptance Quantiles\n\nplt.hist(preds_df, color = 'blue', bins = 40)\nplt.axvline(x = threshold_85, color = 'red')\nplt.show()","d61b5e57":"# Subset of only accepted loans\naccepted_loans = test_pred_df[test_pred_df['pred_loan_status'] == 0]\n\n# Calculation of the bad rate\nnp.sum(accepted_loans['loan_status']) \/ accepted_loans['loan_status'].count()\n#0.07564542336161761\n\n#This means that of all the loans we've decided to accept from the test set, only 7.5% were actual defaults.","045fb8d4":"#Creating Strategy Table\naccept_rates = [1.0, 0.95, 0.9, 0.85, 0.8, 0.75, 0.7, 0.65, 0.6, 0.55, 0.5, 0.45, 0.4, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1, 0.05]\nthresholds = []\nbad_rates = []\n# Populate the arrays for the strategy table with a for loop\nfor rate in accept_rates:\n    # Calculation of the threshold for the acceptance rate\n    thresh = np.quantile(preds_df['gbt_prob_default'], rate).round(3)\n    # Adding the threshold value to the list of thresholds\n    thresholds.append(np.quantile(preds_df['gbt_prob_default'], rate).round(3))\n    # Reassigning the loan_status value using the threshold\n    test_pred_df['pred_loan_status'] = test_pred_df['gbt_prob_default'].apply(lambda x: 1 if x > thresh else 0)\n    # Set of accepted loans using this acceptance rate\n    accepted_loans = test_pred_df[test_pred_df[\"pred_loan_status\"] == 0]\n    # Calculate and append the bad rate using the acceptance rate\n    bad_rates.append(np.sum((accepted_loans[\"loan_status\"]) \/ len(accepted_loans[\"loan_status\"])).round(3))\n\n# Strategy table\nstrat_df = pd.DataFrame(zip(accept_rates, thresholds, bad_rates),\n                        columns = ['Acceptance Rate','Threshold','Bad Rate'])\nprint(strat_df)","0de50165":"#Visualizing Strategy Table\n\nstrat_df.boxplot()\nplt.show()","f876603d":"#Visualizing The Strategy Curve\nplt.plot(strat_df[\"Acceptance Rate\"], strat_df[\"Bad Rate\"])\nplt.xlabel('Acceptance Rate')\nplt.ylabel('Bad Rate')\nplt.title('Acceptance and Bad Rates')\n\nplt.show()","ad81d4ab":"#Calculation of Expected Losses\nstrat_df[\"Num Accepted Loans\"] = len(y_test) * strat_df[\"Acceptance Rate\"]\nstrat_df[\"Avg Loan Amnt\"] = X_test[\"loan_amnt\"].mean()\nstrat_df[\"Estimated Value\"] = ((strat_df[\"Num Accepted Loans\"] - strat_df[\"Num Accepted Loans\"] * strat_df[\"Bad Rate\"]) * strat_df[\"Avg Loan Amnt\"]) - \\\n                              (strat_df[\"Num Accepted Loans\"] * strat_df[\"Bad Rate\"] * strat_df[\"Avg Loan Amnt\"])\nstrat_df.head()","bd082933":"#Line plot of estimated value\nplt.plot(strat_df[\"Acceptance Rate\"],strat_df[\"Estimated Value\"])\nplt.title('Estimated Value by Acceptance Rate')\nplt.xlabel('Acceptance Rate')\nplt.ylabel('Estimated Value')\nplt.show()\n","7df4d3be":"#Total Expected Loss\ntest_pred_df[\"loan_amnt\"] = X_test[\"loan_amnt\"]\n#Assume that loss given default is 0.8.\ntest_pred_df[\"loss_given_default\"] = 0.8\n#Calculation of the Bank's Expected Loss\ntest_pred_df[\"expected_loss\"] =  test_pred_df[\"gbt_prob_default\"] * test_pred_df[\"loan_amnt\"]\\\n                                * test_pred_df[\"loss_given_default\"]\ntot_exp_loss = round(np.sum(test_pred_df[\"expected_loss\"]),2)\n\nprint('Total expected loss: ', '${:,.2f}'.format(tot_exp_loss))","0c0889fe":"<a id=\"section-five\"><\/a>\n# Missing Values","7ffdd683":"**Total expected loss:  $11,882,372.79.**\n**It may seem like a lot but it would be much higher with logistic regression.**\n**In conclusion, we can say that XGBoost model outperformed the logit model.**","7b42dd98":"<a id=\"section-seven\"><\/a>\n# Creating the Model\n# Multivariate Logistic Regression","e1852903":"<a id=\"section-four\"><\/a>\n# Dealing with Outliers","5da818d0":"![f1.png](attachment:57f6e316-a246-4515-b881-d6f3bffafeaa.png)","e88c54b3":"<a id=\"section-eight\"><\/a>\n# Extreme Gradient Boosting with Model Tuning","635ea87e":"<a id=\"section-twelve\"><\/a>\n# Credit Strategy and Strategy Table","358bf484":"<a id=\"section-ten\"><\/a>\n# Credit Acceptance Rate","c57b5db0":"<a id=\"section-six\"><\/a>\n# Rare Analysing & One-Hot Encoding\n**As we are used to, we encode our data before inserting it into the machine learning model.**\n**With one-hot, we convert each categorical value into a new categorical column and assign a binary value of 1 or 0 to those columns.**\n**Each integer value is represented as a binary vector.**\n","b54d7993":"<a id=\"section-eleven\"><\/a>\n# Bad Rate\n\n","0b6d7a0f":"<a id=\"section-two\"><\/a>\n# EDA","d3f3bb55":"<a id=\"section-one\"><\/a>\n# Introduction\n**This dataset was taken from the DataCamp. It contains  32581 observations with 12 variables, which are:**\n\n****************************************\nperson_age\n****************************************\nperson_income\n****************************************\nperson_home_ownership\n****************************************\nperson_emp_length\n****************************************\nloan_intent\n****************************************\nloan_grade\n****************************************\nloan_amnt\n****************************************\nloan_int_rate\n****************************************\nloan_status(Dependent Variable\/Target)\n****************************************\nloan_percent_income\n****************************************\ncb_person_default_on_file\n****************************************\ncb_person_cred_hist_length\n****************************************\n![credit-concept-online-banking-card-260nw-1831795663.jpg](attachment:4ee9109c-5def-4115-829d-24c0ab444e20.jpg)","1b485bf7":"<a id=\"section-nine\"><\/a>\n# Portfolio Performance","cc613ac4":"![expectedf_loss.png](attachment:7e35355f-44e5-497b-b26f-a7dfcdcd0405.png)","cbf33b3b":"<a id=\"section-thirteen\"><\/a>\n# Total Expected Loss\n![teloss.JPG](attachment:70efba66-16be-4079-aeac-4a1e94341172.JPG)","2ce79173":"<a id=\"section-three\"><\/a>\n# Preprocessing\n\n**Gathering the variables according to their data type.**"}}