{"cell_type":{"85d3119d":"code","5ca62c2c":"code","da50b2d0":"code","f638ba42":"code","f52e7ea1":"code","e8c3504e":"code","dd373687":"code","401dcab0":"code","588cefbb":"code","95dcbfe0":"code","b8e85c95":"code","e36d0e44":"code","7557972e":"code","cb14a60b":"code","4ae69605":"code","28ff06ab":"code","72508869":"code","75dfb8a6":"code","9c483957":"code","099718ea":"code","52fb0eba":"code","8addd2c6":"code","5d01b04e":"markdown"},"source":{"85d3119d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5ca62c2c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","da50b2d0":"df = pd.read_csv(\"..\/input\/protein-secondary-structure\/train.set\", header = None, sep=' ',names=[\"PDB\"]) \ndf = pd.DataFrame({'PDBID':df['PDB'].iloc[::4].values, 'Length':df['PDB'].iloc[1::4].values, 'AA':df['PDB'].iloc[2::4].values, 'SecStruct':df['PDB'].iloc[3::4].values})\ndf","f638ba42":"print(df.shape)","f52e7ea1":"df.info()","e8c3504e":"df[['Length']] = df[['Length']].apply(pd.to_numeric)","dd373687":"df.Length.hist(bins=100)","401dcab0":"def seq2ngrams(seqs, n=3):\n    return np.array([[seq[i:i+n] for i in range(len(seq))] for seq in seqs])","588cefbb":"maxlen_seq = max(df['Length'])\ninput_seqs, target_seqs = df[['AA', 'SecStruct']][df['Length'] <= maxlen_seq].values.T\ninput_grams = seq2ngrams(input_seqs)\nprint(len(input_seqs))","95dcbfe0":"print(input_seqs, target_seqs)","b8e85c95":"print(input_grams)","e36d0e44":"from keras.preprocessing import text, sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils.np_utils import to_categorical\n\ntokenizer_encoder = Tokenizer()\ntokenizer_encoder.fit_on_texts(input_grams)\ninput_data = tokenizer_encoder.texts_to_sequences(input_grams)\ninput_data = sequence.pad_sequences(input_data, maxlen=maxlen_seq, padding='post')","7557972e":"tokenizer_decoder = Tokenizer(char_level=True)\ntokenizer_decoder.fit_on_texts(target_seqs)\ntarget_data = tokenizer_decoder.texts_to_sequences(target_seqs)\ntarget_data = sequence.pad_sequences(target_data, maxlen=maxlen_seq, padding='post')\ntarget_data = to_categorical(target_data)\ninput_data.shape, target_data.shape","cb14a60b":"print(input_data, target_data)","4ae69605":"from keras.models import Model, Input\nfrom keras.layers import LSTM, Embedding, Dense, TimeDistributed, Bidirectional\n\nn_words = len(tokenizer_encoder.word_index) + 1\nn_tags = len(tokenizer_decoder.word_index) + 1\nprint(n_words, n_tags)","28ff06ab":"input = Input(shape=(maxlen_seq,))\nx = Embedding(input_dim=n_words, output_dim=128, input_length=maxlen_seq)(input)\nx = Bidirectional(LSTM(units=64, return_sequences=True, recurrent_dropout=0.1))(x)\ny = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(x)\nmodel = Model(input, y)\nmodel.summary()","72508869":"from sklearn.model_selection import train_test_split\nfrom keras.metrics import categorical_accuracy\nfrom keras import backend  as K\nimport tensorflow as tf","75dfb8a6":"from keras.callbacks import EarlyStopping\nes = EarlyStopping(\n    monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto',\n    baseline=None, restore_best_weights=False\n)","9c483957":"model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\nX_train, X_test, y_train, y_test = train_test_split(input_data, target_data, test_size=0.2, random_state=0)\nseq_train, seq_test, target_train, target_test = train_test_split(input_seqs, target_seqs, test_size=0.2, random_state=0)\n\nmodel.fit(X_train, y_train, batch_size=128, epochs=100, validation_data=(X_test, y_test), verbose=1, callbacks=[es])","099718ea":"from sklearn.externals.joblib import dump, load\ndump(model, 'model.pkl')","52fb0eba":"model = joblib.load('model.pkl')","8addd2c6":"def onehot_to_seq(oh_seq, index):\n    s = ''\n    for o in oh_seq:\n        i = np.argmax(o)\n        if i != 0:\n            s += index[i]\n        else:\n            break\n    return s\n\ndef results(x, y, y_):\n    print(\"---\")\n    print(\"Input: \" + str(x))\n    print(\"Target: \" + str(onehot_to_seq(y, revsere_decoder_index).upper()))\n    print(\"Result: \" + str(onehot_to_seq(y_, revsere_decoder_index).upper()))\n    \nrevsere_decoder_index = {value:key for key,value in tokenizer_decoder.word_index.items()}\nrevsere_encoder_index = {value:key for key,value in tokenizer_encoder.word_index.items()}\n\nN=3\ny_train_pred = model.predict(X_train[:N])\ny_test_pred = model.predict(X_test[:N])\nprint('training')\nfor i in range(N):\n    results(seq_train[i], y_train[i], y_train_pred[i])\nprint('testing')\nfor i in range(N):\n    results(seq_test[i], y_test[i], y_test_pred[i])","5d01b04e":"##### G = 3-turn helix (310 helix). Min length 3 residues.\n##### H = 4-turn helix (\u03b1 helix). Minimum length 4 residues.\n##### I = 5-turn helix (\u03c0 helix). Minimum length 5 residues.\n##### T = hydrogen bonded turn (3, 4 or 5 turn)\n##### E = extended strand in parallel and\/or anti-parallel \u03b2-sheet conformation. Min length 2 residues.\n##### B = residue in isolated \u03b2-bridge (single pair \u03b2-sheet hydrogen bond formation)\n##### S = bend (the only non-hydrogen-bond based assignment).\n##### C = coil (residues which are not in any of the above conformations)."}}