{"cell_type":{"cfcf32d5":"code","ed10ba12":"code","8f1c6a61":"code","3291fb09":"code","bf1c7d6f":"code","7e4bfcbd":"code","5a1719ee":"code","90a7506b":"code","badb7869":"code","966966bc":"code","8f3ee9d8":"code","e4689dfc":"code","3ce2c6aa":"code","d6ebd36a":"code","0800dbdb":"code","f7ee576c":"code","63e302fa":"code","af09d5f9":"code","5fd99df9":"code","d1190271":"code","709ad013":"code","44829b8d":"code","f4e3d95c":"code","f49fe1e6":"code","92c6789e":"code","4acf7650":"code","92d9ef7e":"code","7db987b8":"code","c87bb693":"code","2e32556c":"code","35cd6ac5":"code","31ba0c09":"code","1f316009":"code","179b326e":"code","8ee2d4a2":"code","316403f2":"code","75498784":"code","72502aff":"code","c2ddf8a1":"code","947b5788":"code","37b198e9":"code","b0246a6e":"code","6f9d00f4":"code","540548c4":"code","180e2be3":"code","e70ca13c":"code","fd6d090a":"code","8803554f":"code","5031d57b":"code","de56b357":"code","fa50413f":"code","256e8bf4":"code","c0682f2a":"code","f898d4ee":"code","1b7becdb":"code","4f30255f":"code","2922af75":"code","eac2b6dd":"code","28239373":"code","e1805075":"code","f08725f9":"code","e3564d36":"code","96a32471":"code","22b23531":"code","a8a35e70":"code","603e1a97":"code","fc566292":"code","ccb19634":"code","ab599144":"code","2a73c1da":"code","5d9dce19":"code","d1710bab":"code","519b250f":"code","d32b445a":"code","d6dad0ed":"code","3cd71375":"code","5f01edc2":"code","088abdfb":"code","3f028fe5":"markdown","2ad2bb97":"markdown","2e078f84":"markdown","164823e1":"markdown","4450b940":"markdown","831e0e17":"markdown","03b8b67b":"markdown","75f6f05b":"markdown","9e692bee":"markdown","029e746f":"markdown","95c12ce4":"markdown","164d1ab8":"markdown","c8a3ea7a":"markdown","90fa4c91":"markdown","ba4382c8":"markdown","f1ca86a7":"markdown","c8f5f9ec":"markdown","2724b53b":"markdown","5f609cba":"markdown","437189af":"markdown","f7645c9a":"markdown","f15d796f":"markdown","00cbc1e7":"markdown","e2f800d5":"markdown","0c840d71":"markdown","40696dc9":"markdown","b998bdcc":"markdown","54ca5f5a":"markdown","bafb5f6a":"markdown","3295e83a":"markdown","8fbfc707":"markdown","d27056f2":"markdown","ca779a87":"markdown"},"source":{"cfcf32d5":"import re\nimport numpy as np\nfrom datetime import datetime, timedelta\n\nimport pandas as pd\npd.set_option('display.max_columns', 0)\npd.set_option('display.max_rows', 1000)\n\nfrom fastai import *\nfrom fastai.tabular import * \n\nfrom pathlib import Path","ed10ba12":"path=Path(\"..\/input\/rossmann-store-sales\/\")","8f1c6a61":"traindf=pd.read_csv(path\/\"train.csv\",low_memory=False,parse_dates=[\"Date\"])\ntraindf.shape","3291fb09":"traindf.head()","bf1c7d6f":"traindf.dtypes","7e4bfcbd":"traindf.info()","5a1719ee":"testdf=pd.read_csv(path\/\"test.csv\",low_memory=False,parse_dates=[\"Date\"])\ntestdf.shape","90a7506b":"testdf.info()","badb7869":"#Temporarliy combine test and train data. We do this do make feature engieering easier\ndata=traindf.append(testdf,sort=False)\ndata.sort_values([\"Store\",\"Date\"],inplace=True,ascending=False)\ndata.reset_index(inplace=True,drop=True)\ndata.shape","966966bc":"#The test set is 47 days. Use the last 47 days of the training data for validation\n# Set Sales and Customers to np.nan in the validation period set and add back before saving. This is to avoid overfitting from some of the feature engeering. For example calculating average sales also in the validation period\ncreate_validation_set=False\nif create_validation_set:\n    validation_days=47\n    valid_idx=data[(data.Date>=(traindf.Date.max()- timedelta(days=validation_days)))  & (data.Date<=traindf.Date.max())].index.tolist()\n    valid_data=data.loc[valid_idx][[\"Sales\",\"Customers\"]]\n    data.loc[valid_idx,[\"Sales\",\"Customers\"]]=np.nan","8f3ee9d8":"data.sort_values([\"Store\",\"Date\"],inplace=True,ascending=False)\ndata.reset_index(inplace=True,drop=True)","e4689dfc":"#Set datatypes\ndata.Promo=data.Promo.astype(bool)\ndata.SchoolHoliday=data.SchoolHoliday.astype(bool)\n#StateHoliday has values:['0', 'a','b', 'c']\ndata[\"StateHolidayBool\"]=data.StateHoliday!='0'\ndata.StateHoliday=data.StateHoliday.astype(\"category\")","3ce2c6aa":"data.dtypes","d6ebd36a":"#Add extra date columns\nadd_datepart(data,\"Date\",drop=False)\ndata[\"Quarter\"]=data.Date.dt.quarter.astype(np.int64)\ndata[\"DaysSince2010\"]=(data[\"Date\"] - pd.Timestamp('2010-1-1')).dt.days\n\n#mondays of every week. Much safer to use week start in groupby than [\"Year\",\"Week\"]. With [\"Year\",\"week\"] you will get wrong results around newyear\ndata['Weekstart']=data['Date'] - pd.to_timedelta(arg=data['Date'].dt.weekday, unit='D') ","0800dbdb":"#There are some days where the store is open with zero sales. Probably a mistake\n#This might be important for learning because there are higher sales before and after closed dates\n#List mistakes\n#data[(data.Open==1) & (data.Sales==0)] \n#fixing:\ndata.loc[data.Sales==0,\"Open\"]=0\ndata.Open=data.Open.astype(bool)\n\n#Store 622 has some NaNs in test data, assume store is closed\ndata.loc[data.Open.isna(),\"Open\"]=False\n#Also usefull to have the Closed column\ndata[\"Closed\"]=~data[\"Open\"]","f7ee576c":"store_dtypes= {\n    \"StoreType\":\"category\",\n    \"Assortment\":\"category\",\n    \"Promo2\":\"bool\"\n}\n\nstoredf=pd.read_csv(path\/\"store.csv\",low_memory=False,dtype=store_dtypes)\nlen(storedf)","63e302fa":"storedf.head()","af09d5f9":"storedf.info()","5fd99df9":"data=data.merge(storedf,how=\"left\",on=\"Store\")\nlen(data[data.Assortment.isnull()])","d1190271":"#Convert OpenSinceYear and OpenSinceMonth to one column with the date the competition open\ndata[\"CompetitionOpenSince\"] = pd.to_datetime(dict(year=data.CompetitionOpenSinceYear,\n                                                 month=data.CompetitionOpenSinceMonth, day=15))\n#Number of days to or since competition open.\n#Negative numbers if competition will open in the future\ndata[\"CompetitionDaysOpen\"] = data.Date.subtract(data.CompetitionOpenSince).dt.days","709ad013":"#Create column that indicates that CompetitionOpenSince is missing\ndata[\"CompetitionOpenNA\"]=False\ndata.loc[data.CompetitionOpenSinceYear.isna(),\"CompetitionOpenNA\"]=True\n\ndata[\"CompetitionDistanceNA\"]=False\ndata.loc[data.CompetitionDistance.isna(),\"CompetitionDistanceNA\"]=True","44829b8d":"#Fill missing values\n#Assume that missing CompetitionOpenSince data is because no competition has opened yet and that they will open in 100 days\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nCompetitionOpen = datetime.datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) + relativedelta(days=100)\n\ndata['CompetitionOpenSinceYear'] = data.CompetitionOpenSinceYear.fillna(CompetitionOpen.year).astype(np.int32)\ndata['CompetitionOpenSinceMonth'] = data.CompetitionOpenSinceMonth.fillna(CompetitionOpen.month).astype(np.int32)\ndata['CompetitionOpenSince'] = data.CompetitionOpenSince.fillna(CompetitionOpen)\ndata['CompetitionDaysOpen'] = data.CompetitionDaysOpen.fillna(100).astype(np.int32)\n\n#Assume missing CompetitionDistance data is beacuse the competition is to far away to be registered\ndata.loc[data.CompetitionDistance.isna(),\"CompetitionDistance\"]= data.CompetitionDistance.max()*2  ","f4e3d95c":"# Create a categorical datapoint.\n# Assume that events more than 12 months in the past or future has small effect.\n# Reduce number of categories \ndata[\"CompetitionMonthsOpen\"] = data[\"CompetitionDaysOpen\"]\/\/30\ndata.loc[data.CompetitionMonthsOpen>12, \"CompetitionMonthsOpen\"] = 12\ndata.loc[data.CompetitionMonthsOpen<-12, \"CompetitionMonthsOpen\"] = -12\ndata[\"CompetitionYearsOpen\"] = data[\"CompetitionDaysOpen\"]\/\/365\ndata.loc[data.CompetitionYearsOpen<-2, \"CompetitionYearsOpen\"] = -3\n\n#data.CompetitionMonthsOpen.unique()","f49fe1e6":"data[\"Promo2Na\"]=False\ndata.loc[data.PromoInterval.isna(),\"Promo2Na\"]=True\n                \n#Fill missing values\n#Assume that missing Promo2 data is because no competition has opened yet and that they will open in 100 days\nfrom dateutil.relativedelta import relativedelta\nPromo2Open = datetime.datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) + relativedelta(days=100)\n\n#assume that missing data is because no promo2 has happend at this store yet\ndata['Promo2SinceYear'] = data.Promo2SinceYear.fillna(Promo2Open.year).astype(np.int32)\ndata['Promo2SinceWeek'] = data.Promo2SinceWeek.fillna(Promo2Open.isocalendar()[1]).astype(np.int32)","92c6789e":"#Convert Promo2SinceYear and Promo2SinceWeek to one column with the date the competition open\n# Adding -1 for day, this is required by to_datetime\ndateYWw=data.Promo2SinceYear.astype(str) +\"-\" + data.Promo2SinceWeek.astype(str)+\"-1\"\ndata[\"Promo2Since\"]=pd.to_datetime(dateYWw,format=\"%Y-%W-%w\")","4acf7650":"#Number of dayes to or since Promo2 started.\n#Negative numbers if Promo2 will start in the future\ndata[\"Promo2Days\"] = data.Date.subtract(data[\"Promo2Since\"]).dt.days","92d9ef7e":"# Create a categorical datapoint.\n# Assume that events more than 24 weeks in the past or future has small effect.\ndata[\"Promo2Weeks\"] = data[\"Promo2Days\"]\/\/7\ndata.loc[data.Promo2Weeks<-25, \"Promo2Weeks\"] = -25\ndata.loc[data.Promo2Weeks>25, \"Promo2Weeks\"] = 25\ndata[\"Promo2Years\"] = data[\"Promo2Days\"]\/\/365","7db987b8":"#Promo2 is only active in some months create column that reflects this\n#  1,2,3: Promo2 active and in promointerval\n#  0:     Promo2 active but not in promointerval\n#  -1:    Promo2 not started for this store yet\n#  -2:    Promo2 never active for this store\n\ndata[\"Promo2ActiveMonth\"]=0\n#Promo2 active\ndata.loc[data.Month.isin([1,4,7,10]) & (data.PromoInterval==\"Jan,Apr,Jul,Oct\"),\"Promo2ActiveMonth\"]=1\ndata.loc[data.Month.isin([2,5,8,11]) & (data.PromoInterval==\"Feb,May,Aug,Nov\"),\"Promo2ActiveMonth\"]=2\ndata.loc[data.Month.isin([3,6,9,12]) & (data.PromoInterval==\"Mar,Jun,Sept,Dec\"),\"Promo2ActiveMonth\"]=3\n#Promo2 not started yet\ndata.loc[(data[\"Promo2Since\"]>data[\"Date\"]) & data[\"Promo2\"],\"Promo2ActiveMonth\"]=-1\n#Promo2 never active for this store\ndata.loc[data[\"Promo2\"]==0,\"Promo2ActiveMonth\"]=-2\ndata.Promo2ActiveMonth=data.Promo2ActiveMonth.astype(\"category\")\n#Convert to category\ndata.PromoInterval=data.PromoInterval.astype(\"category\")","c87bb693":"data.dtypes","2e32556c":"path=Path(\"..\/input\/rossmann-store-extra\/\")","35cd6ac5":"store_states=pd.read_csv(path\/\"store_states.csv\",low_memory=False,dtype={\"State\":\"category\"})\nstore_states.shape","31ba0c09":"store_states.head()","1f316009":"data=data.merge(store_states,how=\"left\",on=\"Store\")\nlen(data[data.State.isnull()])","179b326e":"state_names=pd.read_csv(path\/\"state_names.csv\",low_memory=False)\nstate_names.shape","8ee2d4a2":"state_names.head()","316403f2":"data=data.merge(state_names,how=\"left\",on=\"State\")\nlen(data[data.StateName.isnull()])","75498784":"#Set column datatype\ndata.State=data.State.astype(\"category\")\ndata.StateName=data.StateName.astype(\"category\")","72502aff":"googletrend=pd.read_csv(path\/\"googletrend.csv\",low_memory=False)\ngoogletrend.shape","c2ddf8a1":"googletrend.head()","947b5788":"googletrend.info()","37b198e9":"strdate=googletrend.week.str.split(' - ', expand=True)[0]\ngoogletrend['Date']=pd.to_datetime(strdate, format='%Y-%m-%d')\ngoogletrend['State'] = googletrend.file.str.split('_', expand=True)[2]\ngoogletrend.loc[googletrend.State=='NI', \"State\"] = 'HB,NI'\ngoogletrend[\"Year\"]=googletrend.Date.dt.year\ngoogletrend[\"Week\"]=googletrend.Date.dt.week\n","b0246a6e":"data=data.merge(googletrend[[\"State\",\"Year\",\"Week\",\"trend\"]],how=\"left\",on=[\"State\",\"Year\", \"Week\"])\nlen(data[data.trend.isnull()])","6f9d00f4":"#Extract data for Germany as a seperat column\ntrend_de = googletrend[googletrend.file == 'Rossmann_DE'][[\"Year\",\"Week\",\"trend\"]]\ntrend_de.rename(columns={\"trend\":\"trend_DE\"},inplace=True)\ntrend_de.head()","540548c4":"data = data.merge(trend_de, 'left', [\"Year\", \"Week\"])\nlen(data[data.trend_DE.isnull()])","180e2be3":"weather=pd.read_csv(path\/\"weather.csv\",low_memory=False,parse_dates=[\"Date\"])\nweather.rename(columns={\"file\":\"StateName\"},inplace=True)\nweather.head()","e70ca13c":"weather[\"Max_TemperatureC_chnage\"]=weather.groupby(\"StateName\")[\"Max_TemperatureC\"].diff().fillna(0)","fd6d090a":"weather.info()","8803554f":"weather.Events.unique()","5031d57b":"#Convert to one column per type\nevents_dummies=weather.Events.str.get_dummies(sep='-')\nweather=weather.join(events_dummies)\n#weather.drop(\"Events\",inplace=True,axis=1) ","de56b357":"#Check missing\n#weather[weather.Max_VisibilityKm.isna()].sort_values([\"StateName\",\"Date\"])\n#Only some dates in between that is missing. Filling with next day\nweather.Max_VisibilityKm=weather.Max_VisibilityKm.fillna(method=\"ffill\")\nweather.Mean_VisibilityKm=weather.Mean_VisibilityKm.fillna(method=\"ffill\")\nweather.Min_VisibilitykM=weather.Min_VisibilitykM.fillna(method=\"ffill\")\nweather.CloudCover=weather.CloudCover.fillna(method=\"ffill\")\n\n#Lots of missing, most likley not usefull. Dropping\nweather.drop(\"Max_Gust_SpeedKm_h\",inplace=True,axis=1)","fa50413f":"data = data.merge(weather,how=\"left\", on=[\"StateName\",\"Date\"])\nlen(data[data.Mean_TemperatureC.isnull()])","256e8bf4":"def add_days_since_last_event_in_group(df,group,date,event,ascending=True,fillna=True,zero_on_event=False):\n    \"\"\"\n    It is common when working with time series data to extract data that explains relationships \n    across rows as opposed to columns, e.g When there is an event it is usefull to track Time until \n    next event Time since last event\n    \n    https:\/\/stackoverflow.com\/questions\/45022226\/find-days-since-last-event-pandas-dataframe\n    \n    This version is supposed to be equal to the the one developed by Jeremy Howard in the fast.ai course. \n    The only difference is that this version is vectorized so it runs a bit faster\n        \n    https:\/\/github.com\/fastai\/course-v3\/blob\/master\/nbs\/dl1\/rossman_data_clean.ipynb\n    \"\"\"\n    \n    #Basic error checking\n    #if df[date].dtype != np.dtype('datetime64[ns]'):\n    #    raise (\"Date must be datetime64 object\")\n        \n    #Set colname\n    if ascending==True:\n        colname= \"Before\" + event\n    elif ascending==False:\n        colname= \"After\"  + event\n    else:\n        raise (\"Ascending must be a boolean\")\n        \n    \n    temp=df[[group,date,event]].sort_values([group,date],ascending=ascending)\n    temp['days_since_last_record'] = temp.groupby(group)[date].diff().abs() #dt.days\n    cumalative=temp[event].shift().cumsum()\n    \n    df.loc[:,colname]= temp.groupby(['Store', cumalative])['days_since_last_record'].cumsum()\n    if fillna:\n        df[colname].fillna(0,inplace=True)\n    if zero_on_event:\n        df.loc[df[event]==True,colname]=0","c0682f2a":"data[\"index\"]=data.index","f898d4ee":"add_days_since_last_event_in_group(data,\"Store\",\"index\",\"SchoolHoliday\",ascending=True)\nadd_days_since_last_event_in_group(data,\"Store\",\"index\",\"SchoolHoliday\",ascending=False)\n\nadd_days_since_last_event_in_group(data,\"Store\",\"index\",\"Closed\",ascending=True)\nadd_days_since_last_event_in_group(data,\"Store\",\"index\",\"Closed\",ascending=False)\n\nadd_days_since_last_event_in_group(data,\"Store\",\"index\",\"Promo\",ascending=True)\nadd_days_since_last_event_in_group(data,\"Store\",\"index\",\"Promo\",ascending=False)\n","1b7becdb":"add_days_since_last_event_in_group(data,\"Store\",\"index\",\"StateHolidayBool\",ascending=True)\nadd_days_since_last_event_in_group(data,\"Store\",\"index\",\"StateHolidayBool\",ascending=False)","4f30255f":"data.loc[:,\"Promo2ActiveMonthBool\"]=(data.Promo2ActiveMonth!=0)\nadd_days_since_last_event_in_group(data,\"Store\",\"index\",\"Promo2ActiveMonthBool\",ascending=True)\nadd_days_since_last_event_in_group(data,\"Store\",\"index\",\"Promo2ActiveMonthBool\",ascending=False)","2922af75":"#data.set_index(\"Date\",inplace=True)","eac2b6dd":"columns = ['SchoolHoliday', 'StateHolidayBool', 'Promo',\"Closed\",\"Promo2ActiveMonthBool\"]","28239373":"bwd = data[['Store']+columns].sort_index().groupby(\"Store\").rolling(7, min_periods=1).sum().drop(\"Store\",axis=1).reset_index(0,drop=True)\ndata=data.join(bwd, rsuffix='_bw')","e1805075":"fwd = data[['Store']+columns].sort_index(ascending=False).groupby(\"Store\").rolling(7, min_periods=1).sum().drop(\"Store\",axis=1).reset_index(0,drop=True)\ndata=data.join(fwd, rsuffix='_fw')","f08725f9":"wsales=data[data.Sales>0]","e3564d36":"month= wsales[[\"Month\",\"Store\",\"Sales\"]].groupby([\"Month\",\"Store\"],as_index=False).agg([\"mean\",\"median\",\"std\",\"max\",\"min\"])\nmonth.columns=pd.Index([\"Month_\"+ a +\"_\" + b for a,b in month.columns.tolist()])\ndata=data.merge(month,how=\"left\",on=[\"Month\",\"Store\"])\nlen(data[data.Month_Sales_mean.isnull()])","96a32471":"year= wsales[[\"Year\",\"Store\",\"Sales\"]].groupby([\"Year\",\"Store\"],as_index=False).agg([\"mean\",\"median\",\"std\",\"max\",\"min\"])\nyear.columns=pd.Index([\"Year_\"+ a +\"_\" + b for a,b in year.columns.tolist()])\ndata=data.merge(year,how=\"left\",on=[\"Year\",\"Store\"])\nlen(data[data.Year_Sales_mean.isnull()])","22b23531":"Dayofweek= wsales[[\"Dayofweek\",\"Year\",\"Store\",\"Sales\"]].groupby([\"Year\",\"Dayofweek\",\"Store\"]).agg([\"mean\",\"median\",\"std\",\"max\",\"min\"])\ndayofweek_cols=[\"Dayofweek_\"+ a +\"_\" + b for a,b in Dayofweek.columns.tolist()]\nDayofweek.columns=pd.Index(dayofweek_cols)\n\n\nDayofweek.reset_index(inplace=True)\ndata=data.merge(Dayofweek,how=\"left\",on=[\"Year\",\"Dayofweek\",\"Store\"])\nlen(data[data.Dayofweek_Sales_mean.isnull()])","a8a35e70":"#Most days are never open on sundays, fill missing data with 0\ndata[dayofweek_cols]=data[dayofweek_cols].fillna(0)\nlen(data[data.Dayofweek_Sales_mean.isnull()])","603e1a97":"Dayofweekpromo= wsales[[\"Year\",\"Dayofweek\",\"Store\",\"Promo\",\"Sales\"]].groupby([\"Year\",\"Dayofweek\",\"Promo\",\"Store\"]).agg([\"mean\",\"median\",\"std\",\"max\",\"min\"])\nDayofweekpromo_cols=[\"Dayofweek_promo_\"+ a +\"_\" + b for a,b in Dayofweekpromo.columns.tolist()]\nDayofweekpromo.columns=pd.Index(Dayofweekpromo_cols)\n\nDayofweekpromo.reset_index(inplace=True)\ndata=data.merge(Dayofweekpromo,how=\"left\",on=[\"Year\",\"Dayofweek\",\"Store\",\"Promo\"])\nlen(data[data.Dayofweek_promo_Sales_mean.isnull()])","fc566292":"#Most days are never open on sundays, fill missing data with 0\ndata[Dayofweekpromo_cols]=data[Dayofweekpromo_cols].fillna(0)\nlen(data[data.Dayofweek_promo_Sales_mean.isnull()])","ccb19634":"agg=data[[\"Store\",\"Sales\",\"Customers\"]].groupby(\"Store\").sum()\nsales_customer_ratio=(agg.Sales\/agg.Customers).to_frame(name=\"ratio-sales-customer\").reset_index()\ndata=data.merge(sales_customer_ratio,how=\"left\",on=\"Store\")","ab599144":"week=data[data.Dayofweek.isin([0,1,2,3,4])][[\"Sales\",\"Store\"]].groupby([\"Store\"]).mean()\n\nsaturday=data[data.Dayofweek==5][[\"Sales\",\"Store\"]].groupby([\"Store\"]).mean()\nsaturday_week=(saturday\/week).reset_index().rename(columns={\"Sales\":\"ratio-saturday-week\"})\ndata=data.merge(saturday_week,how=\"left\",on=\"Store\")\n","2a73c1da":"sunday=data[data.Dayofweek==6][[\"Sales\",\"Store\"]].groupby([\"Store\"]).mean()\nsunday_week=(sunday\/week).reset_index().rename(columns={\"Sales\":\"ratio-sunday-week\"})\ndata=data.merge(sunday_week,how=\"left\",on=\"Store\")","5d9dce19":"promo=data[[\"Store\",\"Sales\",\"Promo\"]].groupby([\"Store\",\"Promo\"]).mean().reset_index().pivot_table(values=\"Sales\",index=\"Store\",columns=\"Promo\")\npromo_ratio=(promo[True]\/promo[False]).to_frame(name=\"ratio-promo-nopromo\")\ndata=data.merge(promo_ratio,how=\"left\",on=\"Store\")","d1710bab":"#Todo","519b250f":"#Todo","d32b445a":"thisweek=data[[\"Store\",\"Weekstart\",\"Promo\",\"Open\",\"StateHolidayBool\",\"SchoolHoliday\"]].groupby([\"Weekstart\",\"Store\"]).agg({'Promo':'sum','Open':'sum','StateHolidayBool':'sum','SchoolHoliday':'sum'})","d6dad0ed":"prevweek=thisweek.groupby(\"Store\").shift(1).reset_index().fillna(method=\"bfill\")\nnextweek=thisweek.groupby(\"Store\").shift(-1).reset_index().fillna(method=\"ffill\")\nthisweek.reset_index(inplace=True)","3cd71375":"data=data.merge(thisweek,on=[\"Store\",\"Weekstart\"],how=\"left\",suffixes=(\"\",\"_thisweek\"),)\ndata=data.merge(prevweek,on=[\"Store\",\"Weekstart\"],how=\"left\",suffixes=(\"\",\"_prevweek\"))\ndata=data.merge(nextweek,on=[\"Store\",\"Weekstart\"],how=\"left\",suffixes=(\"\",\"_nextweek\"))","5f01edc2":"if create_validation_set:\n    data.loc[valid_idx,[\"Sales\",\"Customers\"]]=valid_data\n    trainname=\"train-and-validation.feather\"\nelse:\n    trainname=\"train.feather\"\n\n    \ntestdata=data[data.Sales.isna()].reset_index(drop=True)\ntraindata=data[data.Sales.notna()].reset_index(drop=True)","088abdfb":"traindata.to_feather(trainname)\ntestdata.to_feather(\"test.feather\")","3f028fe5":"# Data engineering\n","2ad2bb97":"## Sales ratios","2e078f84":"# Rossmann store sales dataset","164823e1":"## Thisweek, nextweek and prevweek","4450b940":"# Save data","831e0e17":"### Schoolholiday\/noholiday","03b8b67b":"# Setup environment","75f6f05b":"## Store_state table","9e692bee":"This notebook is part of series of notebooks analyzing the Rossmann store data set:\n\n 1. [Deep learning with fast.ai v1 simple version](https:\/\/www.kaggle.com\/omgrodas\/rossmann-deep-learning-with-fast-ai-v1-simplen)\n 2. [Exploratory data analysis](https:\/\/www.kaggle.com\/omgrodas\/rossmann-exploratory-data-analysis)\n 2. [Data engineering](https:\/\/www.kaggle.com\/omgrodas\/rossmann-data-engineering) \n 3. [Deep Learning with fast.ai](https:\/\/www.kaggle.com\/omgrodas\/rossmann-deep-learning-with-fast-ai-v1) \n 4. Hyper parameter search with hyperopt\n \nThese notebooks are based one the notebook used in lesson 3 of the fast.ai deep learning for coders course.\n\nhttps:\/\/github.com\/fastai\/fastai\/blob\/master\/courses\/dl1\/lesson3-rossman.ipynb\n\nIdeas for extra features are taken from:\n\nhttps:\/\/www.kaggle.com\/c\/rossmann-store-sales\/discussion\/17896","029e746f":"### Customers\/Sales","95c12ce4":"## Google Trend  table","164d1ab8":"### Promo\/nopromo","c8a3ea7a":"### Set datatypes","90fa4c91":"## Rolling aggregations ","ba4382c8":"## Days since last event","f1ca86a7":"## Test table","c8f5f9ec":"## Store table","2724b53b":"### Weekend\/week","5f609cba":"### Competition columns","437189af":"### Date columns","f7645c9a":"Someone found some extra external data that they used and shared with the Kaggle community.\n\nUsing external data was allowed on this competition, but from a ML standpoint using this data is a bit like cheating.The point of the competition is to estimate future sales. Some of the extra data like weather and google trend data is provided into the test period. This data would normally not be available when we make predictions into the future. Using this data will make it look like the algoritm is better at predicting future sales than it really is.","f15d796f":"## Train table","00cbc1e7":"## State_names table","e2f800d5":"### Merge Weather","0c840d71":"### Open column","40696dc9":"### Stateholiday\/nohoilday","b998bdcc":"### Promo columns","54ca5f5a":"### Events column","bafb5f6a":"## Statistics on Year, Month, Week and DayOfWeek","3295e83a":"## Weather table","8fbfc707":"## Create main df","d27056f2":"# rossmann store extra dataset","ca779a87":"# Add timeseries columns"}}