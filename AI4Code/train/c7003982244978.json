{"cell_type":{"1169bb6c":"code","df54f137":"code","ce67031e":"code","29bb2574":"code","b2b3ad7f":"code","389e7405":"code","e003dc7c":"code","102f16a1":"code","5293b176":"code","e0e356ca":"code","d7ddcc58":"code","dd7b2dc8":"code","5f19cd71":"code","750dd05a":"code","178b1311":"code","9fcd45a2":"code","6b2d4cca":"code","3b36c39f":"code","e6e30789":"code","c038f71e":"code","74955e72":"code","2c9a8050":"code","e94b2807":"code","57a55108":"code","b3e6ae7e":"code","bbecb598":"code","d2df04dd":"code","60772824":"code","b8b9d205":"code","c6f713cf":"code","7abe1d78":"markdown","32fa93ed":"markdown","d0ab15b8":"markdown","5e5caf2d":"markdown","24249cba":"markdown","27fa884e":"markdown","e5d2a85b":"markdown","dadc59e3":"markdown","1b450c16":"markdown","2568bd0d":"markdown","55e8bdf4":"markdown","ef59443b":"markdown","850ab8dc":"markdown","383d923e":"markdown","e65d9f52":"markdown","d9595680":"markdown"},"source":{"1169bb6c":"!pip install timm\n!pip install --upgrade -q wandb","df54f137":"import os\nimport gc\nimport cv2\nimport copy\nimport time\nimport random\nfrom PIL import Image\n\n# Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\n\n# Util Imports\n# https:\/\/docs.python.org\/3\/library\/collections.htmlos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n# https:\/\/stackoverflow.com\/questions\/5900578\/how-does-collections-defaultdict-work\nimport joblib \nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n# Scikit-Learn Imports\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\n# https:\/\/rwightman.github.io\/pytorch-image-models\/\nimport timm\n\n# Albumentations for Augmentations\n# https:\/\/albumentations.ai\/docs\/\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# `CUDA_LAUNCH_BLOCKING` make cuda report the error where it actually occurs.\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","ce67031e":"# Configuring the Weights & Biases\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_api = user_secrets.get_secret(\"wandb_api\")\nwandb.login(key = wandb_api)\nanony = None","29bb2574":"ROOT_DIR = \"..\/input\/petfinder-pawpularity-score\"\nTRAIN_DIR = \"..\/input\/petfinder-pawpularity-score\/train\"\nTEST_DIR = \"..\/input\/petfinder-pawpularity-score\/test\"","b2b3ad7f":"CONFIG = dict(\n    seed = 42, model_name = 'tf_efficientnet_b4_ns', train_batch_size = 16,\n    valid_batch_size = 32, img_size = 512, epochs = 5, learning_rate = 1e-4,\n    scheduler = 'CosineAnnealingLR', min_lr = 1e-6, T_max = 20, T_0 = 25,\n    warmup_epochs = 0, weight_decay = 1e-6, n_accumulate = 1, n_fold = 5, \n    num_classes = 1, device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n    competition = 'PetFinder', _wandb_kernel = 'ele'\n)","389e7405":"# Sets the seed for the entire notebook, so that we can reproduce our results\ndef set_seed(seed = 42):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    # If True, causes cuDNN to only use deterministic convolutional algorithms\n    torch.backends.cudnn.deterministic = True\n    # If True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest\n    torch.backends.cudnn.benchmark = False\n    \nset_seed(CONFIG['seed'])","e003dc7c":"def get_train_file_path(id):\n    return f\"{TRAIN_DIR}\/{id}.jpg\"","102f16a1":"df = pd.read_csv(f\"{ROOT_DIR}\/train.csv\")\ndf['file_path'] = df['Id'].apply(get_train_file_path)\nprint(df.shape)","5293b176":"# Finding out the feature columns\nfeature_cols = [col for col in df.columns if col not in ['Id', 'Pawpularity', 'file_path']]\nprint(feature_cols)","e0e356ca":"# https:\/\/docs.wandb.ai\/ref\/python\/init\n# Creating a Weights & Biases Run for Visualization Purposes\nrun = wandb.init(project = 'PetFinder', config = CONFIG, \n    job_type = 'Visualization', anonymous = 'must')","d7ddcc58":"# Making the list of columns for W&B Visualization\ncol_list = list(df.columns)\n# No use of 'file_path'\ncol_list.remove('file_path')\n# Adding a new field (Image retrieved with the help of file_path)\ncol_list.insert(1, 'Image')\nprint(col_list)","dd7b2dc8":"preview_table = wandb.Table(columns = col_list)\n\n# Randomly Sampling 100 points for Visualization\ndf_temp = df.sample(100, random_state = CONFIG['seed']).reset_index(drop = True)\n\nfor i in tqdm(range(df_temp.shape[0])):\n    data = df_temp.loc[i]\n    img = Image.open(data.file_path)\n    preview_table.add_data(data[0], wandb.Image(img), *data[1:-1])\n    \nwandb.log({'Visualization': preview_table})\nrun.finish()","5f19cd71":"# This is just to display the W&B run page in this interactive session.\nfrom IPython import display\n\n# We create an IFrame and set the width & height\niframe = display.IFrame(run.url, width = 1080, height = 720)\niframe","750dd05a":"# https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.cut.html\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.KFold.html\n\ndef create_folds(df, n_splits = 5, n_groups = None):\n    df['kfold'] = 1\n    \n    # Corresponds to the case when we have a classification setting\n    # Will be creating folds on the basis of the target variable simply\n    if n_groups is None:\n        fold = KFold(n_splits = n_splits, random_state = CONFIG['seed'])\n        target = df['Pawpularity']\n        \n    # Corresponds to the case when we have a regression setting\n    # We will bin the target variable first, which will give us a setting similar to that of\n    # classification, and then, we will create the folds on the basis of the binned target variable\n    else:\n        fold = StratifiedKFold(n_splits = n_splits, shuffle = True, random_state = CONFIG['seed'])\n        target = pd.cut(df['Pawpularity'], n_groups, labels = False)\n        \n    for fold_no, (train_indices, val_indices) in enumerate(fold.split(target, target)):\n        df.loc[val_indices, 'kfold'] = fold_no\n        \n    return df","178b1311":"df = create_folds(df, n_splits = CONFIG['n_fold'], n_groups = 14)\ndf.head()","9fcd45a2":"# By default, the imread function reads the image in BGR format\n# cvtColor takes the image from one color space to another color space, in this case, from BGR to RGB\nclass PawpularityDataset(Dataset):\n    def __init__(self, root_dir, df, transforms = None):\n        self.root_dir = root_dir\n        self.df = df\n        self.file_names = df['file_path'].values\n        self.targets = df['Pawpularity'].values\n        self.meta = df[feature_cols].values\n        self.transforms = transforms\n        \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, index):\n        img_path = self.file_names[index]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        meta = self.meta[index, : ]\n        target = self.targets[index]\n        \n        if self.transforms:\n            img = self.transforms(image = img)[\"image\"]\n        \n        return img, meta, target","6b2d4cca":"# https:\/\/albumentations.ai\/docs\/\ndata_transforms = {\n    \"train\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.HorizontalFlip(p = 0.5),\n        A.Normalize(),\n        ToTensorV2()\n    ]),\n    \"val\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.Normalize(),\n        ToTensorV2()\n    ]),\n}","3b36c39f":"class PawpularityModel(nn.Module):\n    def __init__(self, model_name, pretrained = True):\n        super(PawpularityModel, self).__init__()\n        self.model = timm.create_model(model_name, pretrained = pretrained)\n        self.n_features = self.model.classifier.in_features\n        self.model.reset_classifier(0)\n        self.fc = nn.Linear(self.n_features + 12, CONFIG['num_classes'])\n        self.dropout = nn.Dropout(p = 0.3)\n        \n    def forward(self, images, meta):\n        # features.shape = (batch_size, num_embeddings)\n        features = self.model(images)\n        features = self.dropout(features)\n        \n        # features.shape = (batch_size, num_embeddings + meta)\n        features = torch.cat([features, meta], dim = 1)\n        \n        # outputs = (batch_size, num_classes)\n        output = self.fc(features)\n        return output\n    \nmodel = PawpularityModel(CONFIG['model_name'])\nmodel.to(CONFIG['device'])","e6e30789":"def criterion(outputs, targets):\n    return torch.sqrt(nn.MSELoss()(outputs.view(-1), targets.view(-1)))","c038f71e":"def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n    \n    # GradScaler makes the gradient values have a larger magnitude, so that, they don\u2019t flush to zero.\n    # https:\/\/pytorch.org\/docs\/stable\/amp.html#gradient-scaling\n    scaler = amp.GradScaler()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    # Defining the Iterator for TQDM\n    bar = tqdm(enumerate(dataloader), total = len(dataloader))\n    for step, (images, meta, targets) in bar:\n        images = images.to(device, dtype = torch.float)\n        meta = meta.to(device, dtype = torch.float)\n        targets = targets.to(device, dtype = torch.float)\n        \n        # Defining the Batch Size\n        batch_size = images.size(0)\n        \n        # Enabling Autocast for Automatic Mixed Precision \n        # https:\/\/developer.nvidia.com\/automatic-mixed-precision\n        with amp.autocast(enabled = True):\n            outputs = model(images, meta)\n            loss = criterion(outputs, targets)\n            loss = loss \/ CONFIG['n_accumulate']\n        scaler.scale(loss).backward()\n        \n        # When we have to train large models, and use small batch sizes, it takes a lot of computation\n        # time. In order to reduce that, we can do backprop after every few steps, instead of doing it\n        # after every step. In other words, we accumulate gradients for a 'n_accumulate' steps, and then\n        # we perform back-prop. \n        if (step + 1) % CONFIG['n_accumulate'] == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            \n            # Zero out the Paraneter Gradients\n            optimizer.zero_grad()\n            \n            if scheduler is not None:\n                scheduler.step()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        epoch_loss = running_loss \/ dataset_size\n        \n        # set_postfix allows us to display \n        # https:\/\/github.com\/tqdm\/tqdm\n        bar.set_postfix(Epoch = epoch, Train_Loss = epoch_loss, LR = optimizer.param_groups[0]['lr'])\n        \n    # All objects regardless of how long they have been in memory are considered for collection.\n    # However, objects that are referenced in managed code are not collected. Use this method to\n    # force the system to try to reclaim the maximum amount of available memory.\n    gc.collect()\n    \n    return epoch_loss","74955e72":"# Since, we don't want to train the model while iterating on the validation set, hence we have \n# diasbled the gradient calculations in this function.\n# https:\/\/pytorch.org\/docs\/stable\/generated\/torch.no_grad.html\n@torch.no_grad()\n\ndef val_one_epoch(model, dataloader, device, epoch):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    TARGETS = []\n    PREDS = []\n    \n    # Defining the Iterator for TQDM\n    bar = tqdm(enumerate(dataloader), total = len(dataloader))\n    for step, (images, meta, targets) in bar:\n        images = images.to(device, dtype = torch.float)\n        meta = meta.to(device, dtype = torch.float)\n        targets = targets.to(device, dtype = torch.float)\n        \n        # Defining the Batch Size\n        batch_size = images.size(0)\n        \n        outputs = model(images, meta)\n        loss = criterion(outputs, targets)\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        epoch_loss = running_loss \/ dataset_size\n        \n        PREDS.append(outputs.view(-1).cpu().detach().numpy())\n        TARGETS.append(targets.view(-1).cpu().detach().numpy())\n        \n        # set_postfix allows us to display \n        # https:\/\/github.com\/tqdm\/tqdm\n        bar.set_postfix(Epoch = epoch, Val_Loss = epoch_loss, LR = optimizer.param_groups[0]['lr'])\n    \n    TARGETS = np.concatenate(TARGETS)\n    PREDS = np.concatenate(PREDS)\n    val_rmse = MSE(TARGETS, PREDS, squared=False)\n    \n    # All objects regardless of how long they have been in memory are considered for collection.\n    # However, objects that are referenced in managed code are not collected. Use this method to\n    # force the system to try to reclaim the maximum amount of available memory.\n    gc.collect()\n    \n    return epoch_loss, val_rmse","2c9a8050":"# train_loader & val_loader are initialized before this function is called\ndef run_training(model, optimizer, scheduler, device, num_epochs):\n    # To automatically log gradients\n    # https:\/\/docs.wandb.ai\/ref\/python\/watch\n    wandb.watch(model, log_freq = 100)\n    \n    if torch.cuda.is_available():\n        print('[INFO] Using GPU: {}\\n'.format(torch.cuda.get_device_name()))\n        \n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_epoch_rmse = np.inf\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs+1):\n        gc.collect()\n        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, dataloader = train_loader,\n            device = CONFIG['device'], epoch = epoch)\n        val_epoch_loss, val_epoch_rmse = val_one_epoch(model, dataloader = val_loader, \n             device = CONFIG['device'], epoch = epoch)\n        \n        history['Train Loss'].append(train_epoch_loss)\n        history['Valid Loss'].append(val_epoch_loss)\n        history['Valid RMSE'].append(val_epoch_rmse)\n    \n        # Log the Metrics\n        wandb.log({\"Train Loss\": train_epoch_loss})\n        wandb.log({\"Val Loss\": val_epoch_loss})\n        wandb.log({'Valid RMSE': val_epoch_rmse})\n        \n        print(\"Val RMSE:\", val_epoch_rmse)\n        \n        # Deep Copy the Model\n        if val_epoch_rmse <= best_epoch_rmse:\n            print(f\"Validation Loss Improved ({best_epoch_rmse} - {val_epoch_rmse})\")\n            best_epoch_rmse = val_epoch_rmse\n            run.summary[\"Best RMSE\"] = best_epoch_rmse\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = \"RMSE{:.4f}_epoch{:.0f}.bin\".format(best_epoch_rmse, epoch)\n            torch.save(model.state_dict(), PATH)\n            \n            # Save a model file from the curreny directory\n            wandb.save(PATH)\n            print(\"Model Saved\")\n            \n        print()\n        \n    end = time.time()\n    time_elapsed = end - start\n    print(\"Training complete in {:.0f}h {:.0f}m {:.0f}s\".format(\n        time_elapsed \/\/ 3600, (time_elapsed % 3600) \/\/ 60, (time_elapsed % 3600) % 60))\n    print(\"Best RMSE: {:.4f}\".format(best_epoch_rmse))\n    \n    # Load the best model weights\n    model.load_state_dict(best_model_wts)\n    \n    return model, history","e94b2807":"def prepare_loaders(fold):\n    df_train = df[df.kfold != fold].reset_index(drop = True)\n    df_val = df[df.kfold != fold].reset_index(drop = True)\n    \n    train_dataset = PawpularityDataset(TRAIN_DIR, df_train, transforms = data_transforms['train'])\n    val_dataset = PawpularityDataset(TRAIN_DIR, df_val, transforms = data_transforms['train'])\n    \n    train_loader = DataLoader(train_dataset, batch_size = CONFIG['train_batch_size'],\n        num_workers = 4, shuffle = True, pin_memory = True, drop_last = True)\n    val_loader = DataLoader(val_dataset, batch_size = CONFIG['valid_batch_size'],\n        num_workers = 4, shuffle = True, pin_memory = True, drop_last = True)\n    \n    return train_loader, val_loader","57a55108":"def fetch_scheduler(optimizer):\n    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max = CONFIG['T_max'], \n            eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = CONFIG['T_0'], \n                                                             eta_min = CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == None:\n        return None\n        \n    return scheduler","b3e6ae7e":"# Create Dataloaders\ntrain_loader, val_loader = prepare_loaders(fold = 0)","bbecb598":"# Define Optimizer & Scheduler\noptimizer = optim.Adam(model.parameters(), lr = CONFIG['learning_rate'], \n    weight_decay = CONFIG['weight_decay'])\nscheduler = fetch_scheduler(optimizer)","d2df04dd":"# Creating the training run for Weights & Biases\nrun = wandb.init(project = 'PetFinder', config = CONFIG, job_type = 'Train', anonymous = 'must')","60772824":"# Start Training\nmodel, history = run_training(model, optimizer, scheduler, CONFIG['device'], CONFIG['epochs'])","b8b9d205":"# Finishing the Run\nrun.finish()","c6f713cf":"# This is just to display the W&B run page in this interactive session.\nfrom IPython import display\n\n# We create an IFrame and set the width and height\niframe = display.IFrame(run.url, width = 1000, height = 720)\niframe","7abe1d78":"# Training Configuration","32fa93ed":"# Training & Inferencing","d0ab15b8":"# Creating the Dataset Class","5e5caf2d":"# Validation Function","24249cba":"# Defining the Augmentations","27fa884e":"# Set Seed for Reproducibility","e5d2a85b":"# Visualization","dadc59e3":"# Creating Folds","1b450c16":"# Training Function","2568bd0d":"# Run Training","55e8bdf4":"# Defining the Model Architecture","ef59443b":"# PetFinder.my\n- Hola amigos, this notebook covers my code for the **PetFinder.my - Pawpularity Contest**, which can be found [here](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score).\n- Reference Notebooks:\n    - [[Pytorch + W&B] Pawpularity Training](https:\/\/www.kaggle.com\/debarshichanda\/pytorch-w-b-pawpularity-training?scriptVersionId=75559544)\n    - [Experiment Tracking with Weights & Biases](https:\/\/www.kaggle.com\/ayuraj\/experiment-tracking-with-weights-and-biases\/notebook)\n    - [Interactive EDA using W&B Tables](https:\/\/www.kaggle.com\/ayuraj\/interactive-eda-using-w-b-tables)\n    - [Continuous Target Stratification](https:\/\/www.kaggle.com\/tolgadincer\/continuous-target-stratification?scriptVersionId=52551118&cellId=6)\n\n<br>\n\n![](https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/Petfinder\/PetFinder%20-%20Logo.png)","850ab8dc":"# Read the Data","383d923e":"# Visualizations","e65d9f52":"# Defining the Loss Function","d9595680":"# Installing and Importing Packages"}}