{"cell_type":{"8c669a8d":"code","54c729d7":"code","67e223a6":"code","3c51d718":"code","77c2429c":"code","ea82f8f6":"code","29d82a3d":"code","4b58b551":"code","7dfbc563":"code","758a0cae":"code","24f08df8":"code","73d1c3d2":"code","8f15fce9":"code","2bb1043f":"code","98331dcf":"code","37d17d07":"code","fab095a2":"code","a5ebd31e":"code","889630f7":"code","28fb44c4":"code","a2660d10":"code","f96e511d":"code","e0bdea17":"code","6072fcf1":"code","b7556d54":"code","9d899fa8":"code","e4eaa790":"code","7abf47c9":"code","c2009a2d":"code","98b2f5c7":"code","889c88ef":"code","274b7b80":"code","0cab5ef2":"code","dfb7feb7":"code","d678f15b":"code","89f5c224":"code","c0bf77f6":"markdown","d9d569e7":"markdown","fe55b665":"markdown","9c070c26":"markdown","74576278":"markdown","f0d754d5":"markdown","6827d2f2":"markdown","f656baf8":"markdown","da4c805c":"markdown","5d9c28ff":"markdown","e1ca8972":"markdown","040dd26e":"markdown","ad8778e8":"markdown","b0ad76ca":"markdown","978befe5":"markdown","3853b4b7":"markdown","502f2e38":"markdown","51acf978":"markdown","5cb6487c":"markdown","b5d383b8":"markdown","acbe5326":"markdown","52fd8c28":"markdown","d280e5af":"markdown","c8b2b16f":"markdown","65e1741e":"markdown","00db7181":"markdown","a254eca9":"markdown","d0aaa0f8":"markdown","92607e25":"markdown","85fe74a7":"markdown","b241f187":"markdown","8880fab2":"markdown","bc990e71":"markdown","2023c980":"markdown","75d813f2":"markdown","74f4ba8a":"markdown","f49824ba":"markdown","74d988ce":"markdown","246caf02":"markdown","a2631893":"markdown","4e7dc597":"markdown","eb76ad85":"markdown","a55c9606":"markdown","96bbcccc":"markdown","e9359284":"markdown","6a82762b":"markdown","6ca92b05":"markdown","76214657":"markdown","2561dc9a":"markdown"},"source":{"8c669a8d":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import regularizers\nfrom keras.callbacks import EarlyStopping\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Embedding, Input, LSTM, Dense, Conv1D, GlobalMaxPooling1D, MaxPooling1D, LeakyReLU,Dropout,AvgPool2D, UpSampling2D, ReLU, MaxPooling2D, Reshape, Softmax, Activation, Flatten, Lambda, Conv2DTranspose\nfrom tensorflow.keras.losses import MSE, categorical_crossentropy, binary_crossentropy\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow.keras.backend as K\nimport matplotlib.pyplot as plt\n\nimport time \nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom IPython.core.display import display, HTML\nimport plotly.graph_objects as go\nimport re\n\nimport nltk  \nnltk.download('stopwords') \nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer \nfrom collections import Counter\nimport cufflinks as cf\ncf.go_offline()\nfrom sklearn.utils import shuffle  \n\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score","54c729d7":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission =  pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","67e223a6":"train.head()","3c51d718":"train.info()","77c2429c":"def Plot_world(text):\n    \n    comment_words = ' '\n    stopwords = set(STOPWORDS) \n    \n    for val in text: \n\n        # typecaste each val to string \n        val = str(val) \n\n        # split the value \n        tokens = val.split() \n\n        # Converts each token into lowercase \n        for i in range(len(tokens)): \n            tokens[i] = tokens[i].lower() \n\n        for words in tokens: \n            comment_words = comment_words + words + ' '\n\n\n    wordcloud = WordCloud(width = 5000, height = 4000, \n                    background_color ='black', \n                    stopwords = stopwords, \n                    min_font_size = 10).generate(comment_words) \n\n    # plot the WordCloud image                        \n    plt.figure(figsize = (12, 12), facecolor = 'k', edgecolor = 'k' ) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n\n    plt.show()","ea82f8f6":"text = train.text.values\nPlot_world(text)","29d82a3d":"train.loc[train['text'].str.contains('http')].target.value_counts()","4b58b551":"keywords = train.keyword.values\nPlot_world(keywords)","7dfbc563":"targ_zero = 0\ntarg_one = 0\nfor ii in range(len(train.target)):\n    if train.target[ii] == 1:\n        targ_one = targ_one+1\n    elif train.target[ii] == 0:\n        targ_zero = targ_zero + 1\n        \nprint(f\"There are {targ_zero} tweets with target 0 and {targ_one} tweets with target 1\")","758a0cae":"pattern = re.compile('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n\ndef remove_html(text):\n    no_html= pattern.sub('',text)\n    return no_html\n\ntrain['text']=train['text'].apply(lambda x : remove_html(x))\ntest['text']=test['text'].apply(lambda x : remove_html(x))\ntrain.loc[train['text'].str.contains('http')].target.value_counts()\n\ntext = 'Make sure to check out our 5LSL0 Kaggle competition contribution! https:\/\/www.kaggle.com\/c\/nlp-getting-started'\nprint('Original: '+ text)\nprint('Removed URL: '+ remove_html(text))","24f08df8":"def clean_text(text):\n \n    text = re.sub('[^a-zA-Z]', ' ', text)  # means any character that IS NOT a-z OR A-Z \n    \n    # re.compile is used to save the resulting regular expression object\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               u\"\\U0001f926-\\U0001f937\"\n                               u\"\\U00010000-\\U0010ffff\"\n                               u\"\\u2640-\\u2642\"\n                               u\"\\u2600-\\u2B55\"\n                               u\"\\u200d\"\n                               u\"\\u23cf\"\n                               u\"\\u23e9\"\n                               u\"\\u231a\"\n                               u\"\\ufe0f\"  # dingbats\n                               u\"\\u3030\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = text.lower()  # To convert to lower case\n\n    # split to array(default delimiter is \" \") \n    text = text.split()  \n\n    text = ' '.join(text)    \n            \n    return text\n\ntrain['text'] = train['text'].apply(lambda x : clean_text(x))\ntest['text']=test['text'].apply(lambda x : clean_text(x))\n\ntext = 'After using the regularization method dropout, we increased our accuracy by 5%!!! :)'\nprint('Original: '+ text)\nprint('Cleaned: ' + clean_text(text))","73d1c3d2":"def decontracted(phrase):\n    \n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    \n    return phrase\n\ntrain['text'] = train['text'].apply(lambda x : decontracted(x))\ntest['text']=test['text'].apply(lambda x : decontracted(x))\n\ntext = 'We\\'re proud that our algorithm\\'ll save lives in the future!'\nprint('Original: '+text)\nprint('Decontracted: '+ decontracted(text))","8f15fce9":"abbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n     \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n     \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}\n\ndef convert_abbrev_in_text(text):\n    t=[]\n    words=text.split()\n    t = [abbreviations[w.lower()] if w.lower() in abbreviations.keys() else w for w in words]\n    return ' '.join(t)    \n\ntrain['text'] = train['text'].apply(lambda x : convert_abbrev_in_text(x))\ntest['text']=test['text'].apply(lambda x : convert_abbrev_in_text(x))\n\ntext = 'Long short-term memory (LSTM) networks are the wtg'\n\nprint('Original: '+text)\nprint('Decontracted: '+convert_abbrev_in_text(text))","2bb1043f":"white_list = {'no', 'not'}\nstop_words = set(stopwords.words('english'))\n\nfinal_stop_words = set([word for word in stop_words if word not in white_list])\n\nprint(train['text'][40])\ntrain['tokens'] = train.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\ntest['tokens'] = test.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n\ntrain['tokens']=train['tokens'].apply(lambda row: [word for word in row if word not in final_stop_words])\ntest['tokens']=test['tokens'].apply(lambda row: [word for word in row if word not in final_stop_words])\n\ntrain['text'] = train.apply( lambda row: \" \".join(row['tokens']), axis=1)\ntest['text'] = test.apply( lambda row: \" \".join(row['tokens']), axis=1)\n\nprint(train['tokens'][40])\nprint(train['text'][40])","98331dcf":"def counter_word (text):\n    count = Counter()\n    for i in text.values:\n        for word in i.split():\n            count[word] += 1\n    return count\n\ntext_values = train[\"text\"]\ncounter = counter_word(text_values)\nprint(f\"The len of unique words is: {len(counter)}\")","37d17d07":"max_len = 0\nfor ii in range(len(train.tokens)):\n    if len(train.tokens[ii]) > max_len:\n        max_len = len(train.tokens[ii])\n\nprint(f\"The longest tweet contains {max_len} words\")","fab095a2":"vocab_size = len(counter)\n\nmax_length = 25\ntrunc_type='post'\npadding_type='post'\n\noov_tok = \"<XXX>\"","a5ebd31e":"val_split = 0.2\ntraining_size = round((1-val_split)*len(train))\n\ntrain = shuffle(train) \ntraining_sentences = train.text[0:training_size]\ntraining_labels = train.target[0:training_size]\n\ntesting_sentences = train.text[training_size:]\ntesting_labels = train.target[training_size:]\n\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(training_sentences)\nword_index = tokenizer.word_index\n\nprint(\"The first word indices are: \")\nfor x in list(word_index)[1:15]:\n    print (\" {},  {} \".format(x,  word_index[x]))","889630f7":"training_sequences = tokenizer.texts_to_sequences(training_sentences)\ntraining_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\nprint(train.text[0])\nprint(testing_padded[0])","28fb44c4":"from pathlib import Path\n\nGLOVE_DIM = 200  # Number of dimensions of the GloVe word embeddings\n\nroot = Path('..\/')\ninput_path = root \/ 'input\/' \n\nglove_file = 'glove.twitter.27B.' + str(GLOVE_DIM) + 'd.txt'\nglove_dir = 'glove-global-vectors-for-word-representation\/'\nemb_dict = {}\nglove = open(input_path \/ glove_dir \/ glove_file)\nfor line in glove:\n    values = line.split()\n    word = values[0]\n    vector = np.asarray(values[1:], dtype='float32')\n    emb_dict[word] = vector\nglove.close()\n\nemb_matrix = np.zeros((vocab_size, GLOVE_DIM))\n\nfor w, i in word_index.items():\n    if i < vocab_size:\n        vect = emb_dict.get(w)\n        # Check if the word from the training data occurs in the GloVe word embeddings\n        # Otherwise the vector is kept with only zeros. \n        if vect is not None:\n            emb_matrix[i] = vect\n    else:\n        break","a2660d10":"def plot_loss_acc(model_hist):\n    plt.subplot(1,2,1)\n    plt.plot(model_hist.history['loss'])\n    plt.plot(model_hist.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.grid(b='true')\n    plt.legend(['Training loss', 'Validation loss'], loc='best')\n\n    plt.subplot(1,2,2)\n    plt.plot(model_hist.history['accuracy'])\n    plt.plot(model_hist.history['val_accuracy'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.grid(b='true')\n    plt.legend(['Training accuracy', 'Validation accuracy'], loc='best')\n    plt.tight_layout()\n    plt.show()","f96e511d":"keras.backend.clear_session()\nDC_model_1 = Sequential()\nDC_model_1.add(Embedding(vocab_size, GLOVE_DIM, input_length=max_length, weights =[emb_matrix], trainable=True))\nDC_model_1.add(Conv1D(32, 5, activation='relu', padding='same')) \nDC_model_1.add(GlobalMaxPooling1D())\nDC_model_1.add(Dense(1, activation='sigmoid'))\n\noptimizer = tf.keras.optimizers.Adam(lr=0.0001)\nDC_model_1.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\nDC_model_1.summary()\n\n#Train the model\nnum_epochs = 20\nstart_time = time.time()\n\nDC_history = DC_model_1.fit(training_padded, training_labels, \\\n                        epochs=num_epochs, validation_data=(testing_padded, testing_labels), \\\n                        shuffle=True, \\\n                        verbose=1)\n\n \n\nprint(f'The time in seconds: {(time.time()- start_time)}')\n\n#Plot results\nplot_loss_acc(DC_history)","e0bdea17":"keras.backend.clear_session()\nDC_model_2 = Sequential()\nDC_model_2.add(Embedding(vocab_size, GLOVE_DIM, input_length=max_length, weights =[emb_matrix], trainable=True))\nDC_model_2.add(Dropout(0.3))\nDC_model_2.add(Conv1D(32, 5, activation='relu', padding='same')) \nDC_model_2.add(GlobalMaxPooling1D())\nDC_model_2.add(Dense(1, activation='sigmoid'))\n\noptimizer = tf.keras.optimizers.Adam(lr=0.0001)\nDC_model_2.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\nDC_model_2.summary()\n\n#Train the model\nnum_epochs = 20\nstart_time = time.time()\nearlystop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=3, verbose=1, mode='auto')\n\nDC_ESDO_history = DC_model_2.fit(training_padded, training_labels, \\\n                        epochs=num_epochs, validation_data=(testing_padded, testing_labels), \\\n                        shuffle=True, \\\n                        callbacks=[earlystop],\n                        verbose=1)\n\n \n\nprint(f'The time in seconds: {(time.time()- start_time)}')\n\n#Plot results\nplot_loss_acc(DC_ESDO_history)","6072fcf1":"keras.backend.clear_session()\nDC_model_3 = Sequential()\nDC_model_3.add(Embedding(vocab_size, GLOVE_DIM, input_length=max_length, weights =[emb_matrix], trainable=True))\nDC_model_3.add(Conv1D(32, 5, kernel_regularizer=regularizers.l2(0.05), activation='relu', padding='same')) \nDC_model_3.add(GlobalMaxPooling1D())\nDC_model_3.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.05)))\n\noptimizer = tf.keras.optimizers.Adam(lr=0.0001)\n\nDC_model_3.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\nDC_model_3.summary()\n\n#Train the model\nnum_epochs = 20\nstart_time = time.time()\nearlystop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=3, verbose=1, mode='auto')\n\nDC_ESL2_history = DC_model_3.fit(training_padded, training_labels, \\\n                        epochs=num_epochs, validation_data=(testing_padded, testing_labels), \\\n                        shuffle=True, \\\n                        callbacks=[earlystop],\n                        verbose=1)\n\n \n\nprint(f'The time in seconds: {(time.time()- start_time)}')\n\n#Plot results\nplot_loss_acc(DC_ESL2_history)","b7556d54":"plot_loss_acc(DC_history)\nplot_loss_acc(DC_ESDO_history)\nplot_loss_acc(DC_ESL2_history)","9d899fa8":"keras.backend.clear_session()\nLSTM_model_1 = Sequential()\nLSTM_model_1.add(Embedding(vocab_size, GLOVE_DIM, input_length=max_length, weights =[emb_matrix], trainable=True))\nLSTM_model_1.add(LSTM(16))\nLSTM_model_1.add(Dense(8, activation='tanh'))\nLSTM_model_1.add(Dense(1, activation='sigmoid'))  \n\noptimizer = tf.keras.optimizers.Adam(lr=0.0001)\n\nLSTM_model_1.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])","e4eaa790":"num_epochs = 20\nstart_time = time.time()\n\nLSTM_history_1 = LSTM_model_1.fit(training_padded, training_labels, \\\n                        epochs=num_epochs, validation_data=(testing_padded, testing_labels), \\\n                        shuffle=True,\n                        verbose=0)\n\nplot_loss_acc(LSTM_history_1)","7abf47c9":"keras.backend.clear_session()\nLSTM_model_2 = Sequential()\nLSTM_model_2.add(Embedding(vocab_size, GLOVE_DIM, input_length=max_length, weights =[emb_matrix], trainable=True))\nLSTM_model_2.add(LSTM(16, recurrent_dropout=0.3))\nLSTM_model_2.add(Dense(8, activation='tanh', kernel_regularizer=regularizers.l2(0.05)))\nLSTM_model_2.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.05)))  \n\noptimizer = tf.keras.optimizers.Adam(lr=0.0001)\n\nLSTM_model_2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])","c2009a2d":"num_epochs = 30\nstart_time = time.time()\nearlystop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=3, verbose=1, mode='auto')\n\nLSTM_history_2 = LSTM_model_2.fit(training_padded, training_labels, \\\n                        epochs=num_epochs, validation_data=(testing_padded, testing_labels), \\\n                        shuffle=True, \\\n                        callbacks=[earlystop],\n                        verbose=0)\n\nplot_loss_acc(LSTM_history_2)","98b2f5c7":"keras.backend.clear_session()\nLSTM_model_3 = Sequential()\nLSTM_model_3.add(Embedding(vocab_size, GLOVE_DIM, input_length=max_length, weights =[emb_matrix], trainable=True))\nLSTM_model_3.add(LSTM(16, dropout=0.3, recurrent_dropout=0.3))\nLSTM_model_3.add(Dense(8, activation='tanh'))\nLSTM_model_3.add(Dense(1, activation='sigmoid'))  \n\noptimizer = tf.keras.optimizers.Adam(lr=0.0001)\n\nLSTM_model_3.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])","889c88ef":"num_epochs = 20\nstart_time = time.time()\nearlystop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=3, verbose=1, mode='auto')\n\nLSTM_history_3 = LSTM_model_3.fit(training_padded, training_labels, \\\n                        epochs=num_epochs, validation_data=(testing_padded, testing_labels), \\\n                        shuffle=True, \\\n                        callbacks=[earlystop],\n                        verbose=0)\n\nplot_loss_acc(LSTM_history_3)","274b7b80":"# calculating metrics for a neural network model using sklearn\n\n\n# Calculating predictions from LSTM and CNN model(s)\npredictions_DCmodel_1 = DC_model_1.predict_classes(testing_padded)\npredictions_DCmodel_2 = DC_model_2.predict_classes(testing_padded)\npredictions_DCmodel_3 = DC_model_3.predict_classes(testing_padded)\n\npredictions_LSTM_model_1 = LSTM_model_1.predict_classes(testing_padded)\npredictions_LSTM_model_2 = LSTM_model_2.predict_classes(testing_padded)\npredictions_LSTM_model_3 = LSTM_model_3.predict_classes(testing_padded)\n\n\n# Calculating accuracy for each models\n\n# For CNN and LSTM \n# accuracy: (tp + tn) \/ (p + n)\naccuracy_CNN = accuracy_score(testing_labels, predictions_DCmodel_1)\naccuracy_LSTM = accuracy_score(testing_labels, predictions_LSTM_model_1)\nprint('Without regularization or dropout, accuracy of CNN = %f, and accuracy of LSTM = %f' % (accuracy_CNN, accuracy_LSTM))\n\naccuracy_CNN = accuracy_score(testing_labels, predictions_DCmodel_2)\naccuracy_LSTM = accuracy_score(testing_labels, predictions_LSTM_model_2)\nprint('With early stopping and dropout, accuracy of CNN = %f, and accuracy of LSTM = %f' % (accuracy_CNN, accuracy_LSTM))\n\naccuracy_CNN = accuracy_score(testing_labels, predictions_DCmodel_3)\naccuracy_LSTM = accuracy_score(testing_labels, predictions_LSTM_model_3)\nprint('With early stopping and regularization, accuracy of CNN = %f, and accuracy of LSTM = %f' % (accuracy_CNN, accuracy_LSTM))\n\n\n# Calculating precision for each models\n\n# For CNN\n# precision tp \/ (tp + fp)\nprecision = precision_score(testing_labels, predictions_DCmodel_1)\n#print('Precision without regularization or dropout: %f' % precision)\n\nprecision = precision_score(testing_labels, predictions_DCmodel_2)\n#print('Precision with early stopping and dropout: %f' % precision)\n\nprecision = precision_score(testing_labels, predictions_DCmodel_3)\n#print('Precision with early stopping and regularization: %f' % precision)\n\n\n# For LSTM\n# precision tp \/ (tp + fp)\nprecision = precision_score(testing_labels, predictions_LSTM_model_1)\n#print('Precision without regularization or dropout: %f' % precision)\n\nprecision = precision_score(testing_labels, predictions_LSTM_model_2)\n#print('Precision with early stopping and dropout: %f' % precision)\n\nprecision = precision_score(testing_labels, predictions_LSTM_model_3)\n#print('Precision with early stopping and regularization: %f' % precision)\n","0cab5ef2":"# Calculating precision for each models\n\n# For CNN\n# precision tp \/ (tp + fp)\nprecision = precision_score(testing_labels, predictions_DCmodel_1)\n#print('Precision without regularization or dropout: %f' % precision)\n\nprecision = precision_score(testing_labels, predictions_DCmodel_2)\n#print('Precision with early stopping and dropout: %f' % precision)\n\nprecision = precision_score(testing_labels, predictions_DCmodel_3)\n#print('Precision with early stopping and regularization: %f' % precision)\n\n\n# For LSTM\n# precision tp \/ (tp + fp)\nprecision = precision_score(testing_labels, predictions_LSTM_model_1)\n#print('Precision without regularization or dropout: %f' % precision)\n\nprecision = precision_score(testing_labels, predictions_LSTM_model_2)\n#print('Precision with early stopping and dropout: %f' % precision)\n\nprecision = precision_score(testing_labels, predictions_LSTM_model_3)\n#print('Precision with early stopping and regularization: %f' % precision)\n","dfb7feb7":"# Calculating recall for each models\n\n# For CNN\n# recall: tp \/ (tp + fnpredictions\nrecall = recall_score(testing_labels, predictions_DCmodel_1)\n#print('Recall without regularization or dropout: %f' % recall)\n\nrecall = recall_score(testing_labels, predictions_DCmodel_2)\n#print('Recall with early stopping and dropout: %f' % recall)\n\nrecall = recall_score(testing_labels, predictions_DCmodel_3)\n#print('Recall with early stopping and regularization : %f' % recall)\n\n\n\n# For LSTM\n# recall: tp \/ (tp + fnpredictions\nrecall = recall_score(testing_labels, predictions_LSTM_model_1)\n#print('Recall without regularization or dropout: %f' % recall)\n\nrecall = recall_score(testing_labels, predictions_LSTM_model_2)\n#print('Recall with early stopping and dropout: %f' % recall)\n\nrecall = recall_score(testing_labels, predictions_LSTM_model_3)\n#print('Recall with early stopping and regularization : %f' % recall)","d678f15b":"# Calculating F1-score for each models\n\n# For CNN and LSTM \n\n# f1: 2 tp \/ (2 tp + fp + fn)\nf1_CNN = f1_score(testing_labels, predictions_DCmodel_1)\nf1_LSTM = f1_score(testing_labels, predictions_LSTM_model_1)\nprint('Without regularization or dropout, F1 score of CNN = %f, and F1 score of LSTM = %f' % (f1_CNN, f1_LSTM))\n\nf1_CNN = f1_score(testing_labels, predictions_DCmodel_2)\nf1_LSTM = f1_score(testing_labels, predictions_LSTM_model_2)\nprint('With early stopping and dropout, F1 score of CNN = %f, and F1 score of LSTM = %f' % (f1_CNN, f1_LSTM))\n\nf1_CNN = f1_score(testing_labels, predictions_DCmodel_3)\nf1_LSTM = f1_score(testing_labels, predictions_LSTM_model_3)\nprint('With early stopping and regularization, F1 score of CNN = %f, and F1 score of LSTM = %f' % (f1_CNN, f1_LSTM))","89f5c224":"test1 = tokenizer.texts_to_sequences(test.text)\ntest1 = pad_sequences(test1,maxlen = max_length, padding = padding_type,truncating = trunc_type)\npredictions = LSTM_model_3.predict(test1).reshape(-1)\npredictions = np.where(predictions<0.5,0,1)\nmy_submission= pd.DataFrame({'id': test.id,'target':predictions})\nmy_submission.to_csv('submission.csv',index=False)","c0bf77f6":"From this, one can conclude that within the train set:\n\n1. 99,19% of the data contains a keyword.\n2. 66,73% of the data contains a location.\n3. 100.00% of the data contains text.\n\nWhile this could be useful information, it is important to first asses the quality of these data entries before they should be used. To get a feeling for the information present in each of these lists, a function is written to visualize the most appearing words in each of these lists. This function uses the stopwords from the Natural Language Toolkit (NLTK) to remove words that are used very often in the english language.","d9d569e7":"From this, it can be seen that there are a total of 16011 words in the training set. This means that vectorization of the text will result in a total of 16011 unique numbers, each representing one of the unique words in the tweets. In order to perform this tokenization in a succesful way, a few important choices have to be made. The first of these decisions is the number of words that is fed into the network. Since the tweets are already relatively short compared to other texts, it is decided to take all the words of a tweet as an input. For this, it is important to know the maximum number of words that are contained in the text.","fe55b665":"From these results, it can be seen that the accuracy on the validation data is again 82%. Furthermore, it can be seen that the early stopping terminates the training of the model after 15 epochs, as opposed to the 20 epochs which were used before. This is done as the stopping criterion is defined as to stop training if the validation accuracy does not improve anymore for 3 consecutive epochs. This epoch at which the early stopping terminates the training coincides with the previously made observation that the validation loss tends to increase at that region. ","9c070c26":"## 4.3 EarlyStopping & L2-Regularization\n\nAs seen in the previous section, it is desired to investigate generalization methods to reduce the overfitting of the network. To this end, first early stopping and l2 regularization will be investigated. By using early stopping, one saves parameters that yield the best validation loss and stops training when parameters have not improved loss for specified number of iterations. This means that whenever a similar phenomenon is observed as is the case in the previous section, the model stops training and thus prevents overfitting on the data. On the other hand, using L2 regularization will add a norm penalty to the parameters, thus favouring values closer to zero and favoring smoother parameters. The model below shows the total architecture with both these regularization methods implemented. ","74576278":"From these results, it can be seen that a validation accuracy of 82% can be reached on the given data. Furthermore, from the left figure it can be seen that the validation loss tends to increase after around 10 epochs. This indicates that the network is training learning the specific features of the training data instead of generalizing on the data, thus causing overfitting. Due to this reason, it is interesting to investigate whether some of the generalization tactics can be applied succesfully to the Convolutional network. ","f0d754d5":"From this code, it can be seen that compared to the model seen before now two regularization terms are added on the dense layers. With the following code, this model will be trained and evaluated on its performance while at the same time introducing early stopping.","6827d2f2":"## 4.1 No Regularization\n\nFirst of all, a plain Convolutional network will be implemented on the embedded data without using any specific measures to enhance generalization. The code below shows the model architecture that is used for this end. ","f656baf8":"Therefore it can be seen that 57% of the data has target of zero and that 43% of the data has a target of one. Since this is relatively proportional, the data is therefore concluded to be balanced which means that no further subsequent steps will be taken to change the distribution of data. ","da4c805c":"Therefore it is now known that the longest tweet contains 25 words which is why 25 will be the input shape of the network. Since not all tweets contain 25 words, it is decided to use post padding to give all tweets the same input shape of 25. The code below converts these observations into the settings that can be used by the model later. ","5d9c28ff":"### 2.1. Removing URL's\n\nAs seen in the previous section, URL's are occuring very often in the text of the tweets. Since the text of these URL's do not have any relevant features the algorithm can learn, it is better to remove them from the data as a form of decluttering. The following code performs this task succesfully.","e1ca8972":"### 2.5. Vectorization\nNow that the most important preprocessing steps are taken, the next step is to vectorize the text. Vectorization is done to convert the words into numbers thereby making the processable for the network. The main reason for this is that vectorization allows the computer to deal with words in a manageble form (numbers). In order to vectorize correctly, it is nice to know first how many different words there are within the train set. The code below shows how this can be calculated. ","040dd26e":"### 5.3. LSTM with earlyStopping & Dropout\n![](http:\/\/)The last generalization technique that will be investigated is dropout. Dropout is implemented in the network similarly to the Convolutional network.","ad8778e8":"# 3. Functions to Visualize Performance\nNow all the pre-processing steps are taken, the next step is to train the models. Since one wants to compare these models on a fair basis, the function below automates plotting the accuracies and losses of models. ","b0ad76ca":"Finally, after training the model it is time to check performance of the model on the test data. The code below predicts on the test data and converts the predictions into the format('.csv') which is required to subimit in Kaggle.","978befe5":"# 4. Convolutional Neural Network (CNN)","3853b4b7":"Similar to the previous sections, this model can be trained and evaluated on its performance by the following code. ","502f2e38":"From these results, it can be seen that a validation accuracy of 81% can be reached on the given data. Furthermore, from the left figure it can be seen that the validation loss tends to increase after around 10 epochs. This indicates that the network is training learning the specific features of the training data instead of generalizing on the data, thus causing overfitting. Due to this reason, it is interesting to investigate whether some of the generalization tactics can be applied succesfully to the LSTM network. ","51acf978":"### 2.3. Expanding contractions and abbreviations\nThe third step for preprocessing the data is to convert all contractions into its expanded form. An example of such a conversion is given by\n\n>can't -> cannot\n\nThe main bennefit of such a method of transforming each contraction to its expanded, original form is that it helps with text standardization. By converting a word like \"can't\" to \"cannot\", the model will get the same input for both words. This is desirable since the word \"can't\" bears the exact same meaning as \"cannot\". Besides that, it can help with the vectorization that will be done in a latter section.","5cb6487c":"### 6. Performance comparison of CNN and LSTM in terms of accuracy and F1-score\nOne of the metrics to evaluate performance of a classifier is F1-score. It is a harmonic mean between Precision and Recall. F1 Score is useful if there is some sort of balance between precision & recall in the system.\n\n\nFor the problem of classifying disastrous tweets, it is important to classify disastrous tweets accurately than non-disastrous tweets. In other words, the classifier must minimize false negatives (i.e. a disastrous tweet should not be  classified as non-disastrous tweet). In such problems, F1-score is a better measure compared to accuracy. The accuracy considers total true positives and true negatives in relation to all predictions. \n\nNext, we have evalulated the performance of our LSTM and CNN model(s) based on both accuracy and F1-score.\n","b5d383b8":"From this it can be seen that several layers are applied sequentially, these being:\n\n1. An embedding layer to apply the embeddings as described before.\n2. An LSTM layer to learn the long term dependencies in a sentence.\n3. Two dense layers of which the last one has a sigmoid activation function to map all the data to a probability between 0 and 1.\n\nIn order to asses the performance of this network, the next part will train the network using the fit command from tensorflow.","acbe5326":"This is an important observation for the subsequent section where we will pre process the data. Later, a decission will be made to remove all urls from the data to clean the data from these frequent occurences of words. Below, a similar analysis is performed for the keywords.","52fd8c28":"# 1. Loading and visualizing data\nIn order to get a feel for the data that is being used and to give insights in possible pre-processing strategies that might be effective for this particular challenge, the first step is to load in the data and to visualize it accordingly. The code below first of all loads the data from the *Kaggle* page of the challenge.","d280e5af":"### 2.4. Removing stop words and Tokenization\n\nStop words in a text consist of relatively meaningless words that are used often in the English language, examples of these stopwords are\n\n> Acronyms like \u2018a\u2018 and \u2018an\u2018.\n\n> Possesive pronouns like 'my', 'your', 'his', 'her', etc.\n\n> Cuasal words like 'since', 'so', 'therefore' and 'because', etc. \n\nWhile these words are often used by humans, their individual meaning is not indicative of a specific label in the data. Therefore, these words represent a form of noise and will therefore be removed prior to training the model. Furthermore, other benefits are achieved by removing stop words like reducing the texts to smaller sizes. A deviation from this has been made for words that denote the oposite of something since a word like 'not' might actually be useful for classification. Someone may for instance use 'not normal' for abnormal to classify a certain event. The code below first tokenizes the sentences. After that the stopwords are removed from these lists of words.","c8b2b16f":"From these figures, it can be seen that the obtained accuracy is again 82%. However now the loss does not start to increase after about 10 epochs as was the case for the plain model.","65e1741e":"From this wordweb, it can be seen that the keword attribute possibly also contains relevant features for the neural network to train on. Therefore these keywords can possibly pose as an additional input to the network, which will be investigated later. The last thing that will be checked is the balance of the data, which is calculated by the following piece of code.","00db7181":"Similar to the contractions, a lot of abbreviations are used often that contain the exact same information as their decontracted counterpart. The code below (retrieved from https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert) converts an entire list of common abbreviations to its decontracted counterpart.","a254eca9":"# 0. Importing libraries\nThe first step is to import al the appropriate libraries that can be used for the classification of the tweets. The most important libraries that are used for this project are:\n\n1. Numpy - linear algebra.\n2. Pandas - data processing.\n3. Seaborn - data visualization.\n4. Matplotlib - data visualization.\n5. Tensorflow + Keras - AI models.\n6. NLTK - Symbolic and statistical natural language processing.","d0aaa0f8":"### 6.1 Results for performance comparison\nWe see that the accuracy and F1-score for both LSTM and CNN are comparable. Specifically in the case when we have used early stopping and regularization. ","92607e25":"# 5. Long short-term memory network (LSTM)\nNow the performance of using a convolutional neural network is investigated, the next step is to use a long short-term memory network (LSTM) to do the classification. The reason that an LSTM network poses an interesting candidate, is that these networks are valid for exposing long range dependencies in data which is imperative for sentences. While this same benefit can be obtained by using a recurrent neural network in general, the main benefit of an LSTM is that the long term dependencies are captured better than is the case for a general RNN. \n\nIn this section, first a general LSTM network will be trained, after which various methods will be discussed to enhance generalization similar to the convolutional neural network. ","85fe74a7":"# Machine Learning for Signal Processing (5LSL0) - Team 5\n\nIn the last decade, social media has become an important communication channel in times of a disaster. With people rushing to facebook and twitter to share their observations during these hectic times, valueable opportunities have arisen for many entities such as disaster relief organizations and news agencies. One of these promising opportunities is to monitor tweets real time by smart algorithms to detect such emergencies, greatly reducing the time that these news stations or disaster relief organisations are aware of possible disasters. This could in turn lead to faster aid and a faster spread of information, leading to fewer casualties. \n\nHowever, implementing such an algorithm comes with a variety of challenges. One of such a challenge of a possible implementation is illustrated by the following two tweets:\n\n> Tweet 1: \"Forest *fire* near La Ronge Sask. *Canada*\"\n\n> Tweet 2: \"*Miami* Ultra festival was *fire*!\"\n\nWhile both of the tweets contain a clear location and the word \"fire\", the first informs the public about a real forest fire while the second tweet has nothing to do with an actual disaster. In the remainder of this kernel we propose an algorithm that can succesfully classify whether a tweet is about a real disaster or not. To do this, data is used from the \"NLP with Disaster Tweets\" challenge, which can be found via:\n\nhttps:\/\/www.kaggle.com\/c\/nlp-getting-started\n","b241f187":"### 5.1. Plain LSTM network\nFirst of all, a plain LSTM network will be implemented on the embedded data without using any specific measures to enhance generalization. The code below shows the model architecture that is used for this end. ","8880fab2":"To evaluate the performance of the model. The data is shuffled and split up in a training and test set. \n\nAll of the words are then assigned a unique number to make sure the model can take them as an input.","bc990e71":"### 2.2. Removing other characters and converting to lower case\nSimilar to what is the case for URL's, a couple of other common characters have to be removed to clean the data. These characters contain emojies, symbols, pictograms, chinese characters etc. Besides that, since the words will be vectorized later, all of the letters that are upper case will be converted to lower case letters to avoid ununiqueness of these words otherwise.","2023c980":"From these figures, it can be seen that the obtained accuracy is 83% which is higher than the previously obtained accuracies. In the next section, the performance of all the networks will be compared. ","75d813f2":"## 4.4 Compare the Convolutional models","74f4ba8a":"### 5.2. LSTM with EarlyStopping & L2-Regularization\nAs seen in the previous section, it is desired to investigate generalization methods to reduce the overfitting of the network. Similar to the convolutional network, EarlyStopping and L2-regularization are investigated to mitigate the overfitting of the network.","f49824ba":"# 8. Conclusions\nIn this notebook, the contribution of team 5 for the \"NLP with Disaster Tweets\" challenge as part of the 5LSL0 course on Machine Learning for Signal Processing is shown. In it, we have obtained a validation accuracy of 82% by using clever preprocessing and using LSTM and CNN models for training on the training data. For LSTM with earlyStopping and dropout model, we achieved a score of 0.79987 on the test set. While this score is above the average as obtained by other kaggle users, there still consists other opportunities to increase the performance further. One of these opportunities has to do with the parameter values used in the models and in the preprocessing. While there were some considerations for choosing these values, there still exists an opportunity to fine tune all of these parameters to yield the best possible generalization accuracy. Besides using different parameter values, it is also possible to use more state of the art networks such as bidirectional LSTM's to boost the accuracy even further. Since these networks were out of the scope of the 5LSL0 course, it has been decided not to pursue this path further therefore posing an interesting opportunity for further improvement. \n\n\n\n\n\n\n### Group Members: \n1. Abhilash Naik - 1475533\n2. Faiza Khan - 1424297\n3. K.C.E. van de Camp - 0997889\n4. L.H. Peeters - 1009662\n5. Varsha Kalidas - 1339990\n\n","74d988ce":"From these data entries it can be concluded that each data entry consists of four pieces of information: an *id*, a *keyword* corresponding to the tweet, the *location* from where the tweet is sent and the *text* of the tweet itself. Along with these features, a label (or \"target\") is present for each tweet as well, indicating if it is a real disaster tweet or not. While all of the last three of these features (thus excluding the id of the tweet) could in theory be relevant for classification purposes, it can also be seen that not all of the keywords and locations have an actual value. To gain inside in whether or not a sufficient amount of tweets contains this information, we can perform the following analysis.","246caf02":"Now this function is written, it can be applied to the text, location and keyword lists to get a feeling for the type of words that are included in each of them. The following figure shows the wordcloud for the text of the tweets.","a2631893":"From these results, it can be seen that the accuracy on the validation data is 81%. Furthermore, it can be seen that the early stopping terminates the training of the model after 13 epochs, as opposed to the 20 epochs which were used before. This epoch at which the early stopping terminates the training coincides with the previously made observation that the validation loss tends to increase at that region. ","4e7dc597":"## 4.2 EarlyStopping & Dropout\n\nThe generalization technique that will be investigated is dropout which is a practical alternative for ensemble methods. When thinking about ensemble method, one is essentially training various models to each give its own prediction. By taking a majority voting or an average, one can improve the accuracy with respect to using a single network this way (since all models might have learned some other features of the data). Since the ensemble method is highly computational expensive, dropout is introduced which is a practical alternative of such an ensemble method. Dropout is implemented by multiplying the output of a hidden unit by a binary value which is sampled independently from a probability distribution. Therefore, the architecture keeps changing which means that in effect a lot of different models are trained giving similar benefits as the ensemble method. The code below shows the practical implementation of this dropout.","eb76ad85":"Now all tweets that are shorter than 25 words are padded with zeros at the end. The final result of the vectorization is presented too. Indeed, the processed sentence is split up in a list of numbers that can be fed into the network.","a55c9606":"Before we start training the neural network, the input needs to represented in the form of meaningful numerical features. While each word can be represented in the form of a number or a one-hot encoded vector, it does not capture the linguistic and semantic structure of the language. It would be better if the linguistics and semantics of the words can also be captured in the representation. For this we use word embeddings to encode the input text data. A word embedding is a learned representation for text where words that have the same meaning have a similar representation. For instance, similar words like ice, snow and cold have similar representations and liquid, water, floods, and river are represented similarly. For our training data, we see that a vocabulary of ~16000 words is too less to learn meaningful representations. Hence we incorporated pre-trained GloVe twitter embeddings. These embeddings were trained on a vocabulary of 1.2million words and have good representations. To incorporate these embeddings into our network, we first extract the GloVe embeddings for the words in our vocabulary to yield an embedding matrix relevant to this data-set. Then we pass this embedding matrix as a parameter to the Embedding layer in the neural network so that the network represents each input token as a vector from this matrix. The GloVe embeddings are available in 4 different choices of embedding dimensions: 25, 50, 100, and 200. We note that the performance increases as we increase the embedding dimension. We could reach a maximum  accuracy of 81.6% using the 200-dim embeddings. This is attributed to the fact that higher dimensional vectors are able to capture more information that reflects in better learning by the model.","96bbcccc":"# 2. Data preprocessing\nNow the data is loaded and visualized, the following step is to pre-process the data in a viable way in order to enable the algorithm to extract as many relevant features from it as possible to achieve the beste possible accuracy on the test set. For this NLP challenge, these most important pre-processing steps can be summed up as follows:\n\n1. Removing URL's. \n2. Removing other characters and abbreviations.\n3. Expanding contractions.\n4. Removing stop word and tokenization\n5. Vectorization \n6. Embedding\n\nThe subsequent steps will implement all these pre-processing steps and alongside it explain the specific relevance of each implementation.","e9359284":"From this wordweb, it can immediately be seen that a lot of possibly relevant words are present in a lot of tweets such as \"fire\" while otherwise many words are present that do not immediately link to a disaster. One of the things that is interesting is that \"http\" and \"https\" seem to be occuring very often.","6a82762b":"It can be seen that the total dataset is divided in a train set to train our model and a test set to evaluate the performance of our model later on. In the code below, we can see some of the data entries that are present within the train set (similar data is present in the test set). ","6ca92b05":"### 2.6. Embedding","76214657":"In this Section all Convolutional Neural Networks are presented. The base structure is similar, however to prevent overfit regularization techniques are used.\n\nThe base architecture is as follows:\n1. Embedding layer with the weights from the obtained emb_matrix\n2. Convolutional layer with ReLU activation function\n3. Pooling layer \n4. Dense layer to map network output to a integer. Sigmoid activation function is used to obtain a probability of the estimate of the model.\n\nThe model uses the Adam-optimizer with a fixed learning rate of 0.0001 and Binary-Crossentropy loss function.","2561dc9a":"# 7. Prediction on test data: For submission"}}