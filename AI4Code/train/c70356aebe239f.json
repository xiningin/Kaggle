{"cell_type":{"91a94e5b":"code","8e8b1434":"code","8008a629":"code","fe613a9e":"code","281f7922":"code","d50e4299":"code","de7201d2":"code","e03b12e5":"code","6704b6c1":"code","d73f3c64":"code","abca185f":"code","119653fa":"code","72bb5214":"code","256c4a3c":"code","1e413c82":"code","360262a0":"code","0322b151":"code","2bdf78b3":"code","9ef97571":"code","8196b716":"code","7d2c2732":"code","852f4de7":"code","f3e54ab1":"code","45b64f06":"code","7b0f0fd9":"code","c7aa1139":"code","3cc3b3f4":"code","63483fb9":"code","026b37cc":"code","c3826213":"code","c2df3ca3":"code","8ffcdb0d":"code","500bdc04":"code","f2cd562c":"code","d3080527":"code","7bd446fd":"code","b7e36002":"code","877d4bfa":"code","dad3d97d":"code","882516e3":"code","58029029":"code","0e35c84e":"code","4e6afdd3":"markdown","43638c3d":"markdown","29b55e1d":"markdown","4e166a3b":"markdown","82b3d620":"markdown","79b0b220":"markdown","cba52b7e":"markdown","6a965dd5":"markdown","7239307b":"markdown","173c6bb7":"markdown","10413dee":"markdown","9afec8c7":"markdown","e0942ab9":"markdown","07e26e8d":"markdown"},"source":{"91a94e5b":"import numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom copy import deepcopy\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\n\nimport gc, sys\ngc.enable()\n\nimport os\nprint(os.listdir(\"..\/input\"))","8e8b1434":"# Thanks to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n#        else:\n#            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","8008a629":"def take_part_of_data(df, part):\n    \n    match_ids = df['matchId'].unique()\n    match_ids_part = np.random.choice(match_ids, int(part * len(match_ids)))\n    \n    df = df[df['matchId'].isin(match_ids_part)]\n    \n    del match_ids\n    del match_ids_part","fe613a9e":"def add_new_features_1(df):\n    \n    # calculate total distance\n    df['totalDistance'] = df['rideDistance'] + df['walkDistance'] + df['swimDistance']\n    \n    # sum heals and boosts\n    df['healsAndBoosts'] = df['heals'] + df['boosts']\n    \n    # headshot rate\n    df['headshotKillsOverKills'] = df['headshotKills'] \/ df['kills']\n    df['headshotKillsOverKills'].fillna(0, inplace=True)\n    \n    # kill streake rate\n    df['killStreaksOverKills'] = df['killStreaks'] \/ df['kills']\n    df['killStreaksOverKills'].fillna(0, inplace=True)\n    \n    # kills and assists\n    df['killsAndAssists'] = df['kills'] + df['assists']\n    \n    # teamwork\n    df['assistsAndRevives'] = df['assists'] + df['revives']","281f7922":"def add_new_features_2(df):\n    \n    # number of players joined\n    df['playersJoined'] = df.groupby('matchId')['matchId'].transform('count')\n    \n    # normalize features by number of players joined\n    df['killsAndAssistsOverPlayersJoined'] = df['killsAndAssists'] * ((100 - df['playersJoined']) \/ 100 + 1)\n    df['matchDurationOverPlayersJoined'] = df['matchDuration'] * ((100 - df['playersJoined']) \/ 100 + 1)\n    df['damageDealtOverPlayersJoined'] = df['damageDealt'] * ((100 - df['playersJoined']) \/ 100 + 1)","d50e4299":"def add_new_features_3(df):\n    \n    # total distance over kills and assists\n    df['totalDistanceOverKillsAndAssists'] = df['totalDistance'] \/ df['killsAndAssists']\n    df['totalDistanceOverKillsAndAssists'].fillna(0, inplace=True)\n    df['totalDistanceOverKillsAndAssists'].replace(np.inf, 0, inplace=True)\n    \n    # total distance over heals and boosts\n    df['totalDistanceOverHealsAndBoosts'] = df['totalDistance'] \/ df['healsAndBoosts']\n    df['totalDistanceOverHealsAndBoosts'].fillna(0, inplace=True)\n    df['totalDistanceOverHealsAndBoosts'].replace(np.inf, 0, inplace=True)","de7201d2":"def add_new_features_4(df):\n    \n    df['headshotRate'] = df['kills'] \/ df['headshotKills']\n    df['killStreakRate'] = df['killStreaks'] \/ df['kills']\n    df['healsAndBoosts'] = df['heals'] + df['boosts']\n    df['totalDistance'] = df['rideDistance'] + df['walkDistance'] + df['swimDistance']\n    df['killPlaceOverMaxPlace'] = df['killPlace'] \/ df['maxPlace']\n    df['headshotKillsOverKills'] = df['headshotKills'] \/ df['kills']\n    df['distanceOverWeapons'] = df['totalDistance'] \/ df['weaponsAcquired']\n    df['walkDistanceOverHeals'] = df['walkDistance'] \/ df['heals']\n    df['walkDistanceOverKills'] = df['walkDistance'] \/ df['kills']\n    df['killsPerWalkDistance'] = df['kills'] \/ df['walkDistance']\n    df[\"skill\"] = df['headshotKills'] + df['roadKills']\n    \n    df[df == np.Inf] = np.NaN\n    df[df == np.NINF] = np.NaN\n    \n    df.fillna(0, inplace=True)","e03b12e5":"def feature_engineering(df, is_train=True):\n    \n    # fix rank points\n    df['rankPoints'] = np.where(df['rankPoints'] <= 0, 0, df['rankPoints'])\n    \n    features = list(df.columns)\n    features.remove(\"matchId\")\n    features.remove(\"groupId\")\n    features.remove(\"matchDuration\")\n    features.remove(\"matchType\")\n    if 'winPlacePerc' in features:\n        features.remove('winPlacePerc')\n    \n    y = None\n    \n    # average y for training dataset\n    if is_train:\n        y = df.groupby(['matchId','groupId'])['winPlacePerc'].agg('mean')\n    elif 'winPlacePerc' in df.columns:\n        y = df['winPlacePerc']\n    \n    # mean by match and group\n    agg = df.groupby(['matchId','groupId'])[features].agg('mean')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    \n    if is_train:\n        df_out = agg.reset_index()[['matchId','groupId']]\n    else:\n        df_out = df[['matchId','groupId']]\n    \n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    # max by match and group\n    agg = df.groupby(['matchId','groupId'])[features].agg('max')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    \n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    # max by match and group\n    agg = df.groupby(['matchId','groupId'])[features].agg('min')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    \n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    # number of players in group\n    agg = df.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n    \n    df_out = df_out.merge(agg, how='left', on=['matchId', 'groupId'])\n    \n    # mean by match\n    agg = df.groupby(['matchId'])[features].agg('mean').reset_index()\n    \n    df_out = df_out.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n    \n    # number of groups in match\n    agg = df.groupby(['matchId']).size().reset_index(name='match_size')\n    \n    df_out = df_out.merge(agg, how='left', on=['matchId'])\n    \n    # drop match id and group id\n    df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n    \n    del agg, agg_rank\n    \n    return df_out, y","6704b6c1":"class Estimator(object):\n    \n    def fit(self, x_train, y_train, x_valid, y_valid):\n        raise NotImplementedException\n    \n    def predict(self, x):\n        raise NotImplementedException","d73f3c64":"class ScikitLearnEstimator(Estimator):\n    \n    def __init__(self, estimator):\n        self.estimator = estimator\n    \n    def fit(self, x_train, y_train, x_valid, y_valid):\n        self.estimator.fit(x_train, y_train)\n    \n    def predict(self, x):\n        return self.estimator.predict(x)","abca185f":"def fit_predict_step(estimator, x_train, y_train, train_idx, valid_idx, x_test, oof):\n    \n    # prepare train and validation data\n    x_train_train = x_train[train_idx]\n    y_train_train = y_train[train_idx]\n    x_train_valid = x_train[valid_idx]\n    y_train_valid = y_train[valid_idx]\n    \n    # fit estimator\n    estimator.fit(x_train_train, y_train_train, x_train_valid, y_train_valid)\n    \n    # collect OOF\n    oof_part = estimator.predict(x_train_valid)\n    \n    print('MAE:', mean_absolute_error(y_train_valid, oof_part))\n    \n    oof[valid_idx] = oof_part\n    \n    # make predictions for test data\n    y_part = estimator.predict(x_test)\n    \n    return y_part","119653fa":"def fit_predict(estimator, x_train, y_train, x_test):\n    \n    oof = np.zeros(x_train.shape[0])\n    \n    y = np.zeros(x_test.shape[0])\n    \n    kf = KFold(n_splits=5, random_state=42)\n    \n    for train_idx, valid_idx in kf.split(x_train):\n        \n        y_part = fit_predict_step(estimator, x_train, y_train, train_idx, valid_idx, x_test, oof)\n        \n        # average predictions for test data\n        y += y_part \/ kf.n_splits\n    \n    print('Final MAE:', mean_absolute_error(y_train, oof))\n    \n    return oof, y","72bb5214":"def fit_step(estimator, x_train, y_train, train_idx, valid_idx, oof):\n    \n    # prepare train and validation data\n    x_train_train = x_train[train_idx]\n    y_train_train = y_train[train_idx]\n    x_train_valid = x_train[valid_idx]\n    y_train_valid = y_train[valid_idx]\n    \n    # fit estimator\n    estimator.fit(x_train_train, y_train_train, x_train_valid, y_train_valid)\n    \n    # collect OOF\n    oof_part = estimator.predict(x_train_valid)\n    \n    mae = mean_absolute_error(y_train_valid, oof_part)\n    print('MAE:', mae)\n    \n    oof[valid_idx] = oof_part\n    \n    return estimator, mae","256c4a3c":"def fit(estimator, x_train, y_train):\n    \n    oof = np.zeros(x_train.shape[0])\n    \n    kf = KFold(n_splits=5, random_state=42)\n    \n    trained_estimators = []\n    \n    for train_idx, valid_idx in kf.split(x_train):\n        \n        e, mae = fit_step(estimator, x_train, y_train, train_idx, valid_idx, oof)\n        \n        trained_estimators.append(deepcopy(e))\n    \n    print('Final MAE:', mean_absolute_error(y_train, oof))\n    \n    return oof, trained_estimators","1e413c82":"def predict(trained_estimators, x_test):\n    \n    y = np.zeros(x_test.shape[0])\n    \n    for estimator in trained_estimators:\n        \n        y_part = estimator.predict(x_test)\n        \n        # average predictions for test data\n        y += y_part \/ len(trained_estimators)\n    \n    return y","360262a0":"def pipeline_fit(estimator, df_train, scaler=None):\n    \n    # add new features\n    add_new_features_4(df_train)\n    \n    # feature engineering\n    x_train, y_train = feature_engineering(df_train, is_train=True)\n    x_train = reduce_mem_usage(x_train)\n    gc.collect()\n    \n    # scale\n    if not (scaler is None):\n        scaler.fit(x_train)\n        scaled_x_train = scaler.transform(x_train)\n    else:\n        scaled_x_train = x_train.values\n    \n    # fit\n    oof, trained_estimators = fit(estimator, scaled_x_train, y_train.values)\n    \n    del x_train\n    del scaled_x_train\n    del y_train\n    gc.collect()\n    \n    return oof, trained_estimators","0322b151":"def pipeline_predict(trained_estimators, df_test, scaler=None):\n    \n    # add new features\n    add_new_features_4(df_test)\n    \n    # feature engineering\n    x_test, _ = feature_engineering(df_test, is_train=False)\n    x_test = reduce_mem_usage(x_test)\n    gc.collect()\n    \n    # scale\n    if not (scaler is None):\n        scaled_x_test = scaler.transform(x_test)\n    else:\n        scaled_x_test = x_test.values\n    \n    # predict\n    y = predict(trained_estimators, scaled_x_test)\n    \n    del x_test\n    del scaled_x_test\n    gc.collect()\n    \n    return y","2bdf78b3":"df_train = pd.read_csv('..\/input\/train_V2.csv', index_col='Id')\ndf_train.shape","9ef97571":"df_train = reduce_mem_usage(df_train)","8196b716":"df_train.head().T","7d2c2732":"gc.collect()","852f4de7":"df_train.drop(df_train[df_train['winPlacePerc'].isnull()].index, inplace=True)","f3e54ab1":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn\nfrom torch.nn.utils.weight_norm import weight_norm","45b64f06":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","7b0f0fd9":"class PyTorch(Estimator):\n    \n    def fit(self, x_train, y_train, x_valid, y_valid):\n        \n        train_tensor = TensorDataset(\n            torch.from_numpy(x_train.astype('float32')),\n            torch.from_numpy(y_train.astype('float32')))\n        train_loader = DataLoader(train_tensor, batch_size=256, shuffle=True)\n        \n        self.model = nn.Sequential(\n            weight_norm(nn.Linear(x_train.shape[1], 128)),\n            nn.ReLU(),\n            weight_norm(nn.Linear(128, 128)),\n            nn.ReLU(),\n            weight_norm(nn.Linear(128, 128)),\n            nn.ReLU(),\n            weight_norm(nn.Linear(128, 128)),\n            nn.ReLU(),\n            weight_norm(nn.Linear(128, 1))).to(device)\n        \n        for m in self.model:\n            if isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight_v)\n                nn.init.kaiming_normal_(m.weight_g)\n                nn.init.constant_(m.bias, 0)\n        \n        criterion = nn.L1Loss()\n        optimizer = torch.optim.Adam(self.model.parameters(), betas=(0.9, 0.999), lr=1e-3)\n\n        self.model.train()\n        n_epochs = 50\n        for epoch in range(n_epochs):\n            epoch_loss = 0.0\n            for train_part, y_part in train_loader:\n                optimizer.zero_grad()\n                y_pred_part = self.model(train_part.to(device))\n                loss = criterion(y_pred_part.reshape(-1), y_part.to(device))\n                loss.backward()\n                optimizer.step()\n                epoch_loss += y_pred_part.shape[0] * loss.item()\n            print('Epoch %3d \/ %3d. Loss = %.5f' % (epoch + 1, n_epochs, epoch_loss \/ x_train.shape[0]))\n    \n    def predict(self, x):\n        self.model.eval()\n        x_tensor = torch.from_numpy(x.astype('float32'))\n        x_loader = DataLoader(x_tensor, batch_size=256, shuffle=False)\n        y_pred = np.empty(0)\n        with torch.no_grad():\n            for x_part in x_loader:\n                y_pred_part = self.model(x_part.to(device)).data.cpu().numpy().reshape(-1)\n                y_pred = np.append(y_pred, y_pred_part)\n        return y_pred","c7aa1139":"%%time\n\nscaler = StandardScaler()\noof, trained_estimators = pipeline_fit(PyTorch(), df_train, scaler)","3cc3b3f4":"del df_train\n\ngc.collect()","63483fb9":"df_test = pd.read_csv('..\/input\/test_V2.csv', index_col = 'Id')\ndf_test.shape","026b37cc":"df_test = reduce_mem_usage(df_test)","c3826213":"df_test_id = pd.DataFrame(index=df_test.index)","c2df3ca3":"gc.collect()","8ffcdb0d":"y = pipeline_predict(trained_estimators, df_test, scaler)","500bdc04":"del df_test\n\ngc.collect()","f2cd562c":"df_oof = pd.DataFrame()\ndf_oof['pytorch_oof'] = oof\ndf_oof.to_csv('pytorch_oof.csv', index_label='id')","d3080527":"df_submission = pd.DataFrame(index=df_test_id.index)\ndf_submission['winPlacePerc'] = y\ndf_submission.to_csv('pytorch_raw.csv', index_label='Id')","7bd446fd":"df_test = pd.read_csv('..\/input\/test_V2.csv')\ndf_test.shape","b7e36002":"df_submission = df_submission.merge(df_test[['Id', 'matchId', 'groupId', 'maxPlace', 'numGroups']], on='Id', how='left')\ndf_submission.head()","877d4bfa":"df_submission_group = df_submission.groupby(['matchId', 'groupId']).first().reset_index()\n\ndf_submission_group['rank'] = df_submission_group.groupby(['matchId'])['winPlacePerc'].rank()\n\ndf_submission_group = df_submission_group.merge(df_submission_group.groupby('matchId')['rank'].max().to_frame('max_rank').reset_index(), on='matchId', how='left')\n\ndf_submission_group['adjusted_perc'] = (df_submission_group['rank'] - 1) \/ (df_submission_group['numGroups'] - 1)\n\ndf_submission = df_submission.merge(df_submission_group[['adjusted_perc', 'matchId', 'groupId']], on=['matchId', 'groupId'], how='left')\n\ndf_submission['winPlacePerc'] = df_submission['adjusted_perc']\n\ndf_submission.head()","dad3d97d":"df_submission.loc[df_submission.maxPlace == 0, 'winPlacePerc'] = 0\ndf_submission.loc[df_submission.maxPlace == 1, 'winPlacePerc'] = 1","882516e3":"# Thanks to https:\/\/www.kaggle.com\/anycode\/simple-nn-baseline-4\nt = df_submission.loc[df_submission.maxPlace > 1]\ngap = 1.0 \/ (t.maxPlace.values - 1)\nfixed_perc = np.around(t.winPlacePerc.values \/ gap) * gap\ndf_submission.loc[df_submission.maxPlace > 1, 'winPlacePerc'] = fixed_perc","58029029":"df_submission.loc[(df_submission.maxPlace > 1) & (df_submission.numGroups == 1), 'winPlacePerc'] = 0\n\nassert df_submission['winPlacePerc'].isnull().sum() == 0","0e35c84e":"df_submission[['Id', 'winPlacePerc']].to_csv('pytorch_adjusted.csv', index=False)","4e6afdd3":"## Remove bad row","43638c3d":"### Machine learning","29b55e1d":"## PyTorch","4e166a3b":"## PyTorch","82b3d620":"### Common functions","79b0b220":"## Adjust predictions","cba52b7e":"## Submission","6a965dd5":"### Data preparation","7239307b":"## Load test data","173c6bb7":"## Load train data","10413dee":"## Framework","9afec8c7":"### Feature engineering","e0942ab9":"## Save OOF","07e26e8d":"# PUBG: PyTorch"}}