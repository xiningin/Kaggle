{"cell_type":{"b68d2181":"code","851f9511":"code","a364f8a6":"code","4dd8347a":"code","5af46ca6":"code","ef39bf45":"code","545ac5d6":"code","2c8f0f79":"code","a498e477":"code","e7b592a8":"code","a65b5f57":"code","682a01bc":"code","bc068ef4":"code","03e134f4":"code","1bce2a28":"code","9d75780d":"code","67605515":"code","1886f9e9":"code","319866d0":"code","f54b2112":"code","8a64add5":"code","d02d8715":"code","27edfc17":"code","45032c18":"code","f230da94":"code","184a3150":"code","56bade36":"code","c92b6b22":"code","217d3ed7":"code","4f2845cb":"code","e3aa1802":"code","dbf17eed":"code","08163667":"code","14845bea":"code","05d10139":"code","1bb8e60f":"code","743b49e8":"code","078fa233":"code","2e8cd55b":"code","42445aa4":"code","fd302dcb":"code","c1e85217":"code","81e459c7":"code","71a1bcc4":"code","dcd5344c":"code","ad6404e0":"code","30a76317":"code","8a295c67":"code","d680ad72":"code","826f0708":"code","e70c899e":"code","be3654ff":"code","3dcc37eb":"code","77ad1bcd":"code","66bc1373":"code","10b67553":"code","c0065112":"code","3e4136ee":"code","d74393fd":"code","6125fdb5":"code","edc69aa0":"code","5a05c05f":"code","99f15dc3":"code","bf7be522":"code","4c909876":"code","3afa8d90":"code","b4768c49":"code","4152fdab":"code","3c71e0e7":"code","5bd4467b":"code","2e6afbfd":"code","a4a3ae65":"code","c5829626":"code","707785e6":"code","9391a2b7":"code","55fcee54":"code","6c1677d7":"code","93aed033":"code","edd6682e":"code","0e273409":"code","11897f6f":"code","86f83e43":"code","39169884":"code","0f25db52":"code","bb848942":"code","be546460":"code","b5a5ec9c":"code","e71b58c3":"code","f75ddbad":"code","bdce3a16":"code","236bb900":"code","b3aba148":"code","53c9ea82":"code","7ba06f8d":"code","b65de8e4":"code","5558ef65":"code","4df39906":"code","97af9b77":"code","ed65e5dd":"code","9fa0ef98":"code","f37bc23e":"markdown","c0ac9a38":"markdown","dcc5f4c3":"markdown","03d00917":"markdown","bca36a3f":"markdown","c9542d35":"markdown","835f9976":"markdown","51e2f1e3":"markdown","8a8b45a1":"markdown","74d7bf91":"markdown","193aeb9d":"markdown","bb3019ae":"markdown","27a5b268":"markdown","d63966b0":"markdown","c704b425":"markdown","881fea13":"markdown","7ba4cd8b":"markdown","19b305aa":"markdown","819072fe":"markdown","c93de41b":"markdown","2ead6a37":"markdown","157585a7":"markdown","5509a800":"markdown","272b7b7a":"markdown","8cb32f5d":"markdown","719dfb67":"markdown","809ac747":"markdown","722b65b1":"markdown","2b4ca7e2":"markdown","06bd610e":"markdown","d24b3cc7":"markdown","7654249e":"markdown","167a3634":"markdown","e4f913ee":"markdown","ff59a4c2":"markdown","104a7069":"markdown","dc6c55ac":"markdown","bfc1d5a6":"markdown","81c97422":"markdown","d678365e":"markdown","0c1e7748":"markdown","d85a864e":"markdown","033767a1":"markdown","9c28df74":"markdown","8aa8ff46":"markdown","54aa6148":"markdown","3408154d":"markdown","bff3a10b":"markdown","4c6a8171":"markdown","a2c863b5":"markdown","ee418cef":"markdown","8aff77ab":"markdown","726c9fca":"markdown","4dd77a5b":"markdown","8bad9488":"markdown","7e308864":"markdown","b91e3296":"markdown","00499775":"markdown","63647749":"markdown","e3039ea0":"markdown","69a0b898":"markdown","854c3f34":"markdown","5588e7cd":"markdown","b64eb4b9":"markdown","a74e6b92":"markdown","6416b116":"markdown","3f7c5dde":"markdown","da501cd2":"markdown","e456c62d":"markdown","db9e2629":"markdown","3cda4c76":"markdown","d9295ba4":"markdown","edd360ce":"markdown","dc1c983f":"markdown","8a6419d0":"markdown","43e21efa":"markdown","da8897df":"markdown","82f80e01":"markdown","caaaed7c":"markdown","9c0e2cf7":"markdown","169e39a8":"markdown","7a8ecbdb":"markdown","643cf0ea":"markdown","8a6b877a":"markdown"},"source":{"b68d2181":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\n# models\nfrom sklearn.linear_model import LogisticRegression, Perceptron, RidgeClassifier, SGDClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier \nfrom sklearn.ensemble import BaggingClassifier, VotingClassifier \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\n# NN models\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras import optimizers\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\n# model tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe, space_eval\n\n# import warnings filter\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)","851f9511":"cv_number = 5","a364f8a6":"traindf = pd.read_csv('..\/input\/titanic\/train.csv').set_index('PassengerId')\ntestdf = pd.read_csv('..\/input\/titanic\/test.csv').set_index('PassengerId')\nsubmission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","4dd8347a":"#Thanks to:\n# https:\/\/www.kaggle.com\/mauricef\/titanic\n# https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-one-line-of-the-prediction-code\n#\ndf = pd.concat([traindf, testdf], axis=0, sort=False)\ndf['Title'] = df.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\ndf['Title'] = df.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\ndf['IsWomanOrBoy'] = ((df.Title == 'Master') | (df.Sex == 'female'))\ndf['LastName'] = df.Name.str.split(',').str[0]\nfamily = df.groupby(df.LastName).Survived\ndf['WomanOrBoyCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).count())\ndf['WomanOrBoyCount'] = df.mask(df.IsWomanOrBoy, df.WomanOrBoyCount - 1, axis=0)\ndf['FamilySurvivedCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).sum())\ndf['FamilySurvivedCount'] = df.mask(df.IsWomanOrBoy, df.FamilySurvivedCount - \\\n                                    df.Survived.fillna(0), axis=0)\ndf['WomanOrBoySurvived'] = df.FamilySurvivedCount \/ df.WomanOrBoyCount.replace(0, np.nan)\ndf.WomanOrBoyCount = df.WomanOrBoyCount.replace(np.nan, 0)\ndf['Alone'] = (df.WomanOrBoyCount == 0)\n\n#Thanks to https:\/\/www.kaggle.com\/kpacocha\/top-6-titanic-machine-learning-from-disaster\n#\"Title\" improvement\ndf['Title'] = df['Title'].replace('Ms','Miss')\ndf['Title'] = df['Title'].replace('Mlle','Miss')\ndf['Title'] = df['Title'].replace('Mme','Mrs')\n# Embarked\ndf['Embarked'] = df['Embarked'].fillna('S')\n# Cabin, Deck\ndf['Deck'] = df['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\ndf.loc[(df['Deck'] == 'T'), 'Deck'] = 'A'\n\n# Thanks to https:\/\/www.kaggle.com\/erinsweet\/simpledetect\n# Fare\nmed_fare = df.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\ndf['Fare'] = df['Fare'].fillna(med_fare)\n#Age\ndf['Age'] = df.groupby(['Sex', 'Pclass', 'Title'])['Age'].apply(lambda x: x.fillna(x.median()))\n# Family_Size\ndf['Family_Size'] = df['SibSp'] + df['Parch'] + 1\n\n# Thanks to https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-cluster-analysis\ncols_to_drop = ['Name','Ticket','Cabin']\ndf = df.drop(cols_to_drop, axis=1)\n\ndf.WomanOrBoySurvived = df.WomanOrBoySurvived.fillna(0)\ndf.WomanOrBoyCount = df.WomanOrBoyCount.fillna(0)\ndf.FamilySurvivedCount = df.FamilySurvivedCount.fillna(0)\ndf.Alone = df.Alone.fillna(0)","5af46ca6":"target = df.Survived.loc[traindf.index]\ndf = df.drop(['Survived'], axis=1)\ntrain, test = df.loc[traindf.index], df.loc[testdf.index]","ef39bf45":"train.head(3)","545ac5d6":"test.head(3)","2c8f0f79":"target[:3]","a498e477":"# Determination categorical features\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\nfeatures = train.columns.values.tolist()\nfor col in features:\n    if train[col].dtype in numerics: continue\n    categorical_columns.append(col)\ncategorical_columns","e7b592a8":"# Encoding categorical features\nfor col in categorical_columns:\n    if col in train.columns:\n        le = LabelEncoder()\n        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values))   ","a65b5f57":"train.info()","682a01bc":"test.info()","bc068ef4":"#%% split training set to validation set\nSEED = 100\nXtrain, Xval, Ztrain, Zval = train_test_split(train, target, test_size=0.3, random_state=SEED)","03e134f4":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(train, target)\nY_pred = logreg.predict(test).astype(int)\nacc_log = round(logreg.score(train, target) * 100, 2)\nacc_log","1bce2a28":"submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\n#submission.to_csv('output\/submission_logreg.csv', index=False)\nLB_log_all = 0.79904  # old version","9d75780d":"coeff_df = pd.DataFrame(train.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","67605515":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(train, target)\nY_pred = svc.predict(test).astype(int)\nacc_svc = round(svc.score(train, target) * 100, 2)\nacc_svc","1886f9e9":"submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\n#submission.to_csv('output\/submission_svm.csv', index=False)\nLB_svc_all = 0.62200  # old version","319866d0":"# Linear SVC\n\nlinear_svc = LinearSVC(dual=False)  # dual=False when n_samples > n_features.\nlinear_svc.fit(train, target)\nY_pred = linear_svc.predict(test).astype(int)\nacc_linear_svc = round(linear_svc.score(train, target) * 100, 2)\nacc_linear_svc","f54b2112":"submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\n#submission.to_csv('output\/submission_linear_svc.csv', index=False)\nLB_linear_svc_all = 0.81339  # old version","8a64add5":"# k-Nearest Neighbors algorithm\n\nknn = GridSearchCV(estimator=KNeighborsClassifier(), param_grid={'n_neighbors': [2, 3, 4]}, cv=cv_number).fit(train, target)\nY_pred = knn.predict(test).astype(int)\nacc_knn = round(knn.score(train, target) * 100, 2)\nprint(acc_knn, knn.best_params_)","d02d8715":"submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\n#submission.to_csv('output\/submission_knn.csv', index=False)\nLB_knn_all = 0.62679  # old version","27edfc17":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(train, target)\nY_pred = gaussian.predict(test).astype(int)\nacc_gaussian = round(gaussian.score(train, target) * 100, 2)\nacc_gaussian","45032c18":"submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\n#submission.to_csv('output\/submission_GaussianNB.csv', index=False)\nLB_gaussian_all = 0.73205  # old version","f230da94":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(train, target)\nY_pred = perceptron.predict(test).astype(int)\nacc_perceptron = round(perceptron.score(train, target) * 100, 2)\nacc_perceptron","184a3150":"submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\n#submission.to_csv('output\/submission_perceptron.csv', index=False)\nLB_perceptron_all = 0.46889  # old version","56bade36":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(train, target)\nY_pred = sgd.predict(test).astype(int)\nacc_sgd = round(sgd.score(train, target) * 100, 2)\nacc_sgd","c92b6b22":"submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\n#submission.to_csv('output\/submission_sgd.csv', index=False)\nLB_sgd_all = 0.64593  # old version","217d3ed7":"# Decision Tree Classifier\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(train, target)\nY_pred = decision_tree.predict(test).astype(int)\nacc_decision_tree = round(decision_tree.score(train, target) * 100, 2)\nacc_decision_tree","4f2845cb":"submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\n#submission.to_csv('output\/submission_decision_tree.csv', index=False)\nLB_decision_tree_all = 0.77990  # old version","e3aa1802":"# Random Forest\n\nrandom_forest = GridSearchCV(estimator=RandomForestClassifier(), param_grid={'n_estimators': [200, 300, 400, 500]}, cv=cv_number).fit(train, target)\nrandom_forest.fit(train, target)\nY_pred = random_forest.predict(test).astype(int)\nrandom_forest.score(train, target)\nacc_random_forest = round(random_forest.score(train, target) * 100, 2)\nprint(acc_random_forest,random_forest.best_params_)","dbf17eed":"submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\n#submission.to_csv('output\/submission_random_forest.csv', index=False)\nLB_random_forest_all = 0.81339  # old version","08163667":"%%time\ndef hyperopt_xgb_score(params):\n    clf = XGBClassifier(**params)\n    current_score = cross_val_score(clf, train, target, cv=cv_number).mean()\n    print(current_score, params)\n    return current_score \n \nspace_xgb = {\n            'learning_rate': hp.quniform('learning_rate', 0, 0.05, 0.0001),\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'max_depth':  hp.choice('max_depth', np.arange(5, 8, dtype=int)),\n            'min_child_weight': hp.quniform('min_child_weight', 1, 9, 0.025),\n            'subsample': hp.quniform('subsample', 0.5, 1, 0.005),\n            'gamma': hp.quniform('gamma', 0.5, 1, 0.005),\n            'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.005),\n            'eval_metric': 'auc',\n            'objective': 'binary:logistic',\n            'booster': 'gbtree',\n            'tree_method': 'exact',\n            'silent': 1,\n            'missing': None\n        }\n \nbest = fmin(fn=hyperopt_xgb_score, space=space_xgb, algo=tpe.suggest, max_evals=10)\nprint('best:')\nprint(best)","14845bea":"params = space_eval(space_xgb, best)\nparams","05d10139":"XGB_Classifier = XGBClassifier(**params)\nXGB_Classifier.fit(train, target)\nY_pred = XGB_Classifier.predict(test).astype(int)\nXGB_Classifier.score(train, target)\nacc_XGB_Classifier = round(XGB_Classifier.score(train, target) * 100, 2)\nacc_XGB_Classifier","1bb8e60f":"submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\n#submission.to_csv('output\/submission_XGB_Classifier.csv', index=False)\nLB_XGB_Classifier_all = 0.80861  # old version","743b49e8":"fig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nxgb.plot_importance(XGB_Classifier,ax = axes,height =0.5)\nplt.show();\nplt.close()","078fa233":"%%time\ndef hyperopt_lgb_score(params):\n    clf = LGBMClassifier(**params)\n    current_score = cross_val_score(clf, train, target, cv=cv_number).mean()\n    print(current_score, params)\n    return current_score \n \nspace_lgb = {\n            'learning_rate': hp.quniform('learning_rate', 0, 0.05, 0.0001),\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'max_depth':  hp.choice('max_depth', np.arange(4, 7, dtype=int)),\n            'num_leaves': hp.choice('num_leaves', 2*np.arange(20, 2**6, dtype=int)),\n            'min_child_weight': hp.quniform('min_child_weight', 1, 9, 0.025),\n            'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.005),\n            'objective': 'binary',\n            'boosting_type': 'gbdt',\n            }\n \nbest = fmin(fn=hyperopt_lgb_score, space=space_lgb, algo=tpe.suggest, max_evals=10)\nprint('best:')\nprint(best)","2e8cd55b":"params = space_eval(space_lgb, best)\nparams","42445aa4":"LGB_Classifier = LGBMClassifier(**params)\nLGB_Classifier.fit(train, target)\nY_pred = LGB_Classifier.predict(test).astype(int)\nLGB_Classifier.score(train, target)\nacc_LGB_Classifier = round(LGB_Classifier.score(train, target) * 100, 2)\nacc_LGB_Classifier","fd302dcb":"submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\n#submission.to_csv('output\/submission_LGB_Classifier.csv', index=False)\nLB_LGB_Classifier_all = 0.82296  # old version","c1e85217":"fig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgb.plot_importance(LGB_Classifier,ax = axes,height = 0.5)\nplt.show();\nplt.close()","81e459c7":"%%time\ndef hyperopt_gb_score(params):\n    clf = GradientBoostingClassifier(**params)\n    current_score = cross_val_score(clf, train, target, cv=cv_number).mean()\n    print(current_score, params)\n    return current_score \n \nspace_gb = {\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'max_depth': hp.choice('max_depth', np.arange(5, 8, dtype=int))            \n        }\n \nbest = fmin(fn=hyperopt_gb_score, space=space_gb, algo=tpe.suggest, max_evals=10)\nprint('best:')\nprint(best)","71a1bcc4":"params = space_eval(space_gb, best)\nparams","dcd5344c":"# Gradient Boosting Classifier\n\ngradient_boosting = GradientBoostingClassifier(**params)\ngradient_boosting.fit(train, target)\nY_pred = gradient_boosting.predict(test).astype(int)\ngradient_boosting.score(train, target)\nacc_gradient_boosting = round(gradient_boosting.score(train, target) * 100, 2)\nacc_gradient_boosting","ad6404e0":"submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\n#submission.to_csv('output\/submission_gradient_boosting.csv', index=False)\nLB_GBC_all = 0.82296  # old version","30a76317":"# Ridge Classifier\n\nridge_classifier = RidgeClassifier()\nridge_classifier.fit(train, target)\nY_pred = ridge_classifier.predict(test).astype(int)\nridge_classifier.score(train, target)\nacc_ridge_classifier = round(ridge_classifier.score(train, target) * 100, 2)\nacc_ridge_classifier","8a295c67":"submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\n#submission.to_csv('output\/submission_ridge_classifier.csv', index=False)\nLB_RidgeClassifier_all = 0.80861  # old version","d680ad72":"# Bagging Classifier\n\nbagging_classifier = BaggingClassifier()\nbagging_classifier.fit(train, target)\nY_pred = bagging_classifier.predict(test).astype(int)\nbagging_classifier.score(train, target)\nacc_bagging_classifier = round(bagging_classifier.score(train, target) * 100, 2)\nacc_bagging_classifier","826f0708":"submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\n#submission.to_csv('output\/submission_bagging_classifier.csv', index=False)\nLB_bagging_classifier_all = 0.80861  # old version","e70c899e":"def hyperopt_etc_score(params):\n    clf = ExtraTreesClassifier(**params)\n    current_score = cross_val_score(clf, train, target, cv=cv_number).mean()\n    print(current_score, params)\n    return current_score \n \nspace_etc = {\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'max_features': hp.choice('max_features', np.arange(2, 10, dtype=int)),\n            'min_samples_leaf': hp.choice('min_samples_leaf', np.arange(1, 5, dtype=int)),\n            'max_depth':  hp.choice('max_depth', np.arange(4, 8, dtype=int)),\n        }\n \nbest = fmin(fn=hyperopt_etc_score, space=space_etc, algo=tpe.suggest, max_evals=10)\nprint('best:')\nprint(best)","be3654ff":"params = space_eval(space_etc, best)\nparams","3dcc37eb":"# Extra Trees Classifier\n\nextra_trees_classifier = ExtraTreesClassifier(**params)\nextra_trees_classifier.fit(train, target)\nY_pred = extra_trees_classifier.predict(test).astype(int)\nextra_trees_classifier.score(train, target)\nacc_etc = round(extra_trees_classifier.score(train, target) * 100, 2)\nacc_etc","77ad1bcd":"submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\n#submission.to_csv('output\/submission_etc.csv', index=False)\nLB_ETC_all = 0.80861  # old version","66bc1373":"def build_ann(optimizer='adam'):\n    \n    # Initializing the ANN\n    ann = Sequential()\n    \n    # Adding the input layer and the first hidden layer of the ANN with dropout\n    ann.add(Dense(units=32, kernel_initializer='glorot_uniform', activation='relu', input_shape=(16,)))\n    \n    # Add other layers, it is not necessary to pass the shape because there is a layer before\n    ann.add(Dense(units=64, kernel_initializer='glorot_uniform', activation='relu'))\n    ann.add(Dropout(rate=0.5))\n    ann.add(Dense(units=64, kernel_initializer='glorot_uniform', activation='relu'))\n    ann.add(Dropout(rate=0.5))\n    \n    # Adding the output layer\n    ann.add(Dense(units=1, kernel_initializer='glorot_uniform', activation='sigmoid'))\n    \n    # Compiling the ANN\n    ann.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return ann","10b67553":"opt = optimizers.Adam(lr=0.001)\nann = build_ann(opt)\n# Training the ANN\nhistory = ann.fit(Xtrain, Ztrain, batch_size=16, epochs=100, validation_data=(Xval, Zval))","c0065112":"# Predicting the Test set results\nY_pred = ann.predict(test)\nY_pred = (Y_pred > 0.5)*1 # convert probabilities to binary output","3e4136ee":"# Predicting the Train set results\nann_prediction = ann.predict(train)\nann_prediction = (ann_prediction > 0.5)*1 # convert probabilities to binary output\n\n# Compute error between predicted data and true response and display it in confusion matrix\nacc_ann1 = round(metrics.accuracy_score(target, ann_prediction) * 100, 2)\nacc_ann1","d74393fd":"submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": np.reshape(Y_pred, len(Y_pred))})\n#submission.to_csv('output\/submission_ann1.csv', index=False)\nLB_ann1_all = 0.59330  # old version","6125fdb5":"# Model\nmodel = Sequential()\nmodel.add(Dense(16, input_dim = train.shape[1], init = 'he_normal', activation = 'relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(64, init = 'he_normal', activation = 'relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(32, init = 'he_normal', activation = 'relu'))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.summary()","edc69aa0":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","5a05c05f":"es = EarlyStopping(monitor='val_accuracy', patience=20, mode='max')\nhist = model.fit(train, target, batch_size=64, validation_data=(Xval, Zval), \n               epochs=500, verbose=1, callbacks=[es])","99f15dc3":"plt.plot(hist.history['accuracy'], label='acc')\nplt.plot(hist.history['val_accuracy'], label='val_acc')\n# plt.plot(hist.history['acc'], label='acc')\n# plt.plot(hist.history['val_acc'], label='val_acc')\nplt.ylim((0, 1))\nplt.legend()","bf7be522":"# Predicting the Test set results\nY_pred = model.predict(test)\nY_pred = (Y_pred > 0.5)*1 # convert probabilities to binary output","4c909876":"# Predicting the Train set results\nnn_prediction = model.predict(train)\nnn_prediction = (nn_prediction > 0.5)*1 # convert probabilities to binary output\n\n# Compute error between predicted data and true response\nacc_ann2 = round(metrics.accuracy_score(target, nn_prediction) * 100, 2)\nacc_ann2","3afa8d90":"submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": np.reshape(Y_pred, len(Y_pred))})\n#submission.to_csv('output\/submission_ann2.csv', index=False)\nLB_ann2_all = 0.64114  # old version","b4768c49":"Voting_Classifier_hard = VotingClassifier(estimators=[('lr', logreg), ('rf', random_forest), ('gbc', gradient_boosting)], voting='hard')\nfor clf, label in zip([logreg, random_forest, gradient_boosting, Voting_Classifier_hard], \n                      ['Logistic Regression', 'Random Forest', 'Gradient Boosting Classifier', 'Ensemble']):\n    scores = cross_val_score(clf, train, target, cv=cv_number, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))","4152fdab":"Voting_Classifier_hard.fit(train, target)\nY_pred = Voting_Classifier_hard.predict(test).astype(int)\nVoting_Classifier_hard.score(train, target)\nacc_VC_hard = round(Voting_Classifier_hard.score(train, target) * 100, 2)\nacc_VC_hard","3c71e0e7":"submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\n#submission.to_csv('output\/submission_VC_hard.csv', index=False)\nLB_VC_hard_all = 0.81339  # old version","5bd4467b":"eclf = VotingClassifier(estimators=[('lr', logreg), ('rf', random_forest), ('gbc', gradient_boosting)], voting='soft')\nparams = {'lr__C': [1.0, 100.0], 'gbc__learning_rate': [0.05, 1]}\nVoting_Classifier_soft = GridSearchCV(estimator=eclf, param_grid=params, cv=cv_number)\nVoting_Classifier_soft.fit(train, target)\nY_pred = Voting_Classifier_soft.predict(test).astype(int)\nVoting_Classifier_soft.score(train, target)\nacc_VC_soft = round(Voting_Classifier_soft.score(train, target) * 100, 2)\nacc_VC_soft","2e6afbfd":"submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\n#submission.to_csv('output\/submission_VC_soft.csv', index=False)\nLB_VC_soft_all = 0.81339  # old version","a4a3ae65":"Y_pred = (((test.WomanOrBoySurvived <= 0.238) & (test.Sex > 0.5) & (test.Alone > 0.5)) | \\\n          ((test.WomanOrBoySurvived > 0.238) & \\\n           ~((test.WomanOrBoySurvived > 0.55) & (test.WomanOrBoySurvived <= 0.633))))","c5829626":"simple_rule_model = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n                       max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, presort=False,\n                       random_state=1118, splitter='best') \nsimple_rule_model.fit(train, target)\nY_pred = simple_rule_model.predict(test).astype(int)\nsimple_rule_model.score(train, target)\nacc_simple_rule = round(simple_rule_model.score(train, target) * 100, 2)\nacc_simple_rule","707785e6":"submission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\n#submission.to_csv('output\/submission_simple_rule.csv', index=False)\nLB_simple_rule_all = 0.83253  # old version","9391a2b7":"# Preparing datasets for only 3 features ('WomanOrBoySurvived', 'Sex', 'Alone')\ncols_to_drop3 = ['SibSp', 'Parch', 'Fare', 'LastName', 'Deck',\n               'Pclass', 'Age', 'Embarked', 'Title', 'IsWomanOrBoy',\n               'WomanOrBoyCount', 'FamilySurvivedCount', 'Family_Size']\ntrain = train.drop(cols_to_drop3, axis=1)\ntest = test.drop(cols_to_drop3, axis=1)\nXtrain, Xval, Ztrain, Zval = train_test_split(train, target, test_size=0.3, random_state=SEED)\ntrain.info()","55fcee54":"# 1. Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(train, target)\nY_pred = logreg.predict(test).astype(int)\nacc3_log = round(logreg.score(train, target) * 100, 2)\nsubmission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nsubmission.to_csv('submission_logreg3.csv', index=False)\nLB_log = 0.77033","6c1677d7":"# 2. Support Vector Machines\n\nsvc = SVC()\nsvc.fit(train, target)\nY_pred = svc.predict(test).astype(int)\nacc3_svc = round(svc.score(train, target) * 100, 2)\nsubmission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nsubmission.to_csv('submission_svm3.csv', index=False)\nLB_svc = 0.79904","93aed033":"# 3. Linear SVC\n\nlinear_svc = LinearSVC(dual=False)\nlinear_svc.fit(train, target)\nY_pred = linear_svc.predict(test).astype(int)\nacc3_linear_svc = round(linear_svc.score(train, target) * 100, 2)\nsubmission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nsubmission.to_csv('submission_linear_svc3.csv', index=False)\nLB_linear_svc = 0.77033","edd6682e":"# 4. k-Nearest Neighbors algorithm\n\nknn = GridSearchCV(estimator=KNeighborsClassifier(), param_grid={'n_neighbors': [2, 3, 4]}, cv=cv_number).fit(train, target)\nY_pred = knn.predict(test).astype(int)\nacc3_knn = round(knn.score(train, target) * 100, 2)\nsubmission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nsubmission.to_csv('submission_knn3.csv', index=False)\nLB_knn = 0.77751","0e273409":"# 5. Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(train, target)\nY_pred = gaussian.predict(test).astype(int)\nacc3_gaussian = round(gaussian.score(train, target) * 100, 2)\nsubmission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nsubmission.to_csv('submission_GaussianNB3.csv', index=False)\nLB_gaussian = 0.68899","11897f6f":"# 6. Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(train, target)\nY_pred = perceptron.predict(test).astype(int)\nacc3_perceptron = round(perceptron.score(train, target) * 100, 2)\nsubmission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nsubmission.to_csv('submission_perceptron3.csv', index=False)\nLB_perceptron = 0.77511","86f83e43":"# 7. Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(train, target)\nY_pred = sgd.predict(test).astype(int)\nacc3_sgd = round(sgd.score(train, target) * 100, 2)\nsubmission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nsubmission.to_csv('submission_sgd3.csv', index=False)\nLB_sgd = 0.77511","39169884":"# 8. Decision Tree Classifier\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(train, target)\nY_pred = decision_tree.predict(test).astype(int)\nacc3_decision_tree = round(decision_tree.score(train, target) * 100, 2)\nsubmission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nsubmission.to_csv('submission_decision_tree3.csv', index=False)\nLB_decision_tree = 0.80382","0f25db52":"# 9. Random Forest\n\nrandom_forest = GridSearchCV(estimator=RandomForestClassifier(), param_grid={'n_estimators': [200, 300, 400, 500]}, cv=cv_number).fit(train, target)\nrandom_forest.fit(train, target)\nY_pred = random_forest.predict(test).astype(int)\nrandom_forest.score(train, target)\nacc3_random_forest = round(random_forest.score(train, target) * 100, 2)\nsubmission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nsubmission.to_csv('submission_random_forest3.csv', index=False)\nLB_random_forest = 0.80382","bb848942":"# 10. XGB_Classifier\n\ndef hyperopt_xgb_score(params):\n    clf = XGBClassifier(**params)\n    current_score = cross_val_score(clf, train, target, cv=cv_number).mean()\n    print(current_score, params)\n    return current_score \n \nspace_xgb = {\n            'learning_rate': hp.quniform('learning_rate', 0, 0.05, 0.0001),\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'eta': hp.quniform('eta', 0.025, 0.5, 0.005),\n            'max_depth':  hp.choice('max_depth', np.arange(4, 8, dtype=int)),\n            'min_child_weight': hp.quniform('min_child_weight', 1, 9, 0.025),\n            'subsample': hp.quniform('subsample', 0.5, 1, 0.005),\n            'gamma': hp.quniform('gamma', 0.5, 1, 0.005),\n            'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.005),\n            'eval_metric': 'auc',\n            'objective': 'binary:logistic',\n            'booster': 'gbtree',\n            'tree_method': 'exact',\n            'silent': 1,\n            'missing': None\n        }\n \nbest = fmin(fn=hyperopt_xgb_score, space=space_xgb, algo=tpe.suggest, max_evals=10)\nparams = space_eval(space_xgb, best)\nXGB_Classifier = XGBClassifier(**params)\nXGB_Classifier.fit(train, target)\nY_pred = XGB_Classifier.predict(test).astype(int)\nXGB_Classifier.score(train, target)\nacc3_XGB_Classifier = round(XGB_Classifier.score(train, target) * 100, 2)\nsubmission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nsubmission.to_csv('submission_XGB_Classifier3.csv', index=False)\nLB_XGB_Classifier = 0.68899\nprint(params)","be546460":"# 11. LGBM_Classifier\n\ndef hyperopt_lgb_score(params):\n    clf = LGBMClassifier(**params)\n    current_score = cross_val_score(clf, train, target, cv=cv_number).mean()\n    print(current_score, params)\n    return current_score \n \nspace_lgb = {\n            'learning_rate': hp.quniform('learning_rate', 0, 0.05, 0.0001),\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'max_depth':  hp.choice('max_depth', np.arange(4, 7, dtype=int)),\n            'num_leaves': hp.choice('num_leaves', 2*np.arange(20, 2**6, dtype=int)),\n            'min_child_weight': hp.quniform('min_child_weight', 1, 9, 0.025),\n            'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.005),\n            'objective': 'binary',\n            'boosting_type': 'gbdt',\n            }\n \nbest = fmin(fn=hyperopt_lgb_score, space=space_lgb, algo=tpe.suggest, max_evals=10)\nparams = space_eval(space_lgb, best)\nLGB_Classifier = LGBMClassifier(**params)\nLGB_Classifier.fit(train, target)\nY_pred = LGB_Classifier.predict(test).astype(int)\nLGB_Classifier.score(train, target)\nacc3_LGB_Classifier = round(LGB_Classifier.score(train, target) * 100, 2)\nsubmission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nsubmission.to_csv('submission_LGB_Classifier3.csv', index=False)\nLB_LGB_Classifier = 0.62200\nprint(params)","b5a5ec9c":"# 12. GradientBoostingClassifier\n\ndef hyperopt_gb_score(params):\n    clf = GradientBoostingClassifier(**params)\n    current_score = cross_val_score(clf, train, target, cv=cv_number).mean()\n    print(current_score, params)\n    return current_score \n \nspace_gb = {\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'max_depth': hp.choice('max_depth', np.arange(4, 8, dtype=int)),\n            'max_features': None\n        }\n \nbest = fmin(fn=hyperopt_gb_score, space=space_gb, algo=tpe.suggest, max_evals=5)\nparams = space_eval(space_gb, best)\ngradient_boosting = GradientBoostingClassifier(**params)\ngradient_boosting.fit(train, target)\nY_pred = gradient_boosting.predict(test).astype(int)\ngradient_boosting.score(train, target)\nacc3_gradient_boosting = round(gradient_boosting.score(train, target) * 100, 2)\nsubmission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nsubmission.to_csv('submission_gradient_boosting3.csv', index=False)\nLB_GBC = 0.80382\nprint(params)","e71b58c3":"# 13. Ridge Classifier\n\nridge_classifier = RidgeClassifier()\nridge_classifier.fit(train, target)\nY_pred = ridge_classifier.predict(test).astype(int)\nridge_classifier.score(train, target)\nacc3_ridge_classifier = round(ridge_classifier.score(train, target) * 100, 2)\nsubmission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nsubmission.to_csv('submission_ridge_classifier3.csv', index=False)\nLB_RidgeClassifier = 0.77511","f75ddbad":"# 14. Bagging Classifier\n\nbagging_classifier = BaggingClassifier()\nbagging_classifier.fit(train, target)\nY_pred = bagging_classifier.predict(test).astype(int)\nbagging_classifier.score(train, target)\nacc3_bagging_classifier = round(bagging_classifier.score(train, target) * 100, 2)\nsubmission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nsubmission.to_csv('submission_bagging_classifier3.csv', index=False)\nLB_bagging_classifier = 0.80382","bdce3a16":"# 15. Extra Trees Classifier\n\ndef hyperopt_etc_score(params):\n    clf = ExtraTreesClassifier(**params)\n    current_score = cross_val_score(clf, train, target, cv=cv_number).mean()\n    print(current_score, params)\n    return current_score \n \nspace_etc = {\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'max_features': hp.choice('max_features', np.arange(2, 10, dtype=int)),\n            'min_samples_leaf': hp.choice('min_samples_leaf', np.arange(1, 5, dtype=int)),\n            'max_depth':  hp.choice('max_depth', np.arange(2, 8, dtype=int)),\n            'max_features': None\n        }\n \nbest = fmin(fn=hyperopt_etc_score, space=space_etc, algo=tpe.suggest, max_evals=5)\nparams = space_eval(space_etc, best)\nextra_trees_classifier = ExtraTreesClassifier(**params)\nextra_trees_classifier.fit(train, target)\nY_pred = extra_trees_classifier.predict(test).astype(int)\nextra_trees_classifier.score(train, target)\nacc3_etc = round(extra_trees_classifier.score(train, target) * 100, 2)\nsubmission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nsubmission.to_csv('submission_etc3.csv', index=False)\nLB_ETC = 0.79904\nprint(params)","236bb900":"# 16. Neural Network 1 \n\ndef build_ann(optimizer='adam'):\n    \n    # Initializing the ANN\n    ann = Sequential()\n    \n    # Adding the input layer and the first hidden layer of the ANN with dropout\n    ann.add(Dense(units=32, kernel_initializer='glorot_uniform', activation='relu', input_shape=(3,)))\n    \n    # Add other layers, it is not necessary to pass the shape because there is a layer before\n    ann.add(Dense(units=64, kernel_initializer='glorot_uniform', activation='relu'))\n    ann.add(Dropout(rate=0.5))\n    ann.add(Dense(units=64, kernel_initializer='glorot_uniform', activation='relu'))\n    ann.add(Dropout(rate=0.5))\n    \n    # Adding the output layer\n    ann.add(Dense(units=1, kernel_initializer='glorot_uniform', activation='sigmoid'))\n    \n    # Compiling the ANN\n    ann.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return ann\nopt = optimizers.Adam(lr=0.001)\nann = build_ann(opt)\nhistory = ann.fit(Xtrain, Ztrain, batch_size=16, epochs=100, validation_data=(Xval, Zval))\nY_pred = ann.predict(test)\nY_pred = (Y_pred > 0.5)*1 # convert probabilities to binary output\nann_prediction = ann.predict(train)\nann_prediction = (ann_prediction > 0.5)*1 # convert probabilities to binary output\nacc3_ann1 = round(metrics.accuracy_score(target, ann_prediction) * 100, 2)\nsubmission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": np.reshape(Y_pred, len(Y_pred))})\nsubmission.to_csv('submission_ann1_3.csv', index=False)\nLB_ann1 = 0.79904","b3aba148":"# 17. Neural Network 2\n\n# Model\nmodel = Sequential()\nmodel.add(Dense(16, input_dim = train.shape[1], init = 'he_normal', activation = 'relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(64, init = 'he_normal', activation = 'relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(32, init = 'he_normal', activation = 'relu'))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nes = EarlyStopping(monitor='val_accuracy', patience=20, mode='max')\nhist = model.fit(train, target, batch_size=64, validation_data=(Xval, Zval), \n               epochs=500, verbose=1, callbacks=[es])\nY_pred = model.predict(test)\nY_pred = (Y_pred > 0.5)*1 # convert probabilities to binary output\nnn_prediction = model.predict(train)\nnn_prediction = (nn_prediction > 0.5)*1 # convert probabilities to binary output\nacc3_ann2 = round(metrics.accuracy_score(target, nn_prediction) * 100, 2)\nsubmission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": np.reshape(Y_pred, len(Y_pred))})\nsubmission.to_csv('submission_ann2_3.csv', index=False)\nLB_ann2 = 0.79665","53c9ea82":"# 5.18 VotingClassifier (hard voting)\n\nVoting_Classifier_hard = VotingClassifier(estimators=[('lr', logreg), ('rf', random_forest), ('gbc', gradient_boosting)], voting='hard')\nfor clf, label in zip([logreg, random_forest, gradient_boosting, Voting_Classifier_hard], \n                      ['Logistic Regression', 'Random Forest', 'Gradient Boosting Classifier', 'Ensemble']):\n    scores = cross_val_score(clf, train, target, cv=cv_number, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\nVoting_Classifier_hard.fit(train, target)\nY_pred = Voting_Classifier_hard.predict(test).astype(int)\nVoting_Classifier_hard.score(train, target)\nacc3_VC_hard = round(Voting_Classifier_hard.score(train, target) * 100, 2)\nsubmission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nsubmission.to_csv('submission_VC_hard3.csv', index=False)\nLB_VC_hard = 0.80382","7ba06f8d":"# 5.19 VotingClassifier (soft voting)\n\neclf = VotingClassifier(estimators=[('lr', logreg), ('rf', random_forest), ('gbc', gradient_boosting)], voting='soft')\nparams = {'lr__C': [1.0, 100.0], 'gbc__learning_rate': [0.05, 1]}\nVoting_Classifier_soft = GridSearchCV(estimator=eclf, param_grid=params, cv=cv_number)\nVoting_Classifier_soft.fit(train, target)\nY_pred = Voting_Classifier_soft.predict(test).astype(int)\nVoting_Classifier_soft.score(train, target)\nacc3_VC_soft = round(Voting_Classifier_soft.score(train, target) * 100, 2)\nsubmission = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nsubmission.to_csv('submission_VC_soft3.csv', index=False)\nLB_VC_soft = 0.80382","b65de8e4":"# 5.20 The simple rule in one line\nY_pred = (((test.WomanOrBoySurvived <= 0.238) & (test.Sex > 0.5) & (test.Alone > 0.5)) | \\\n          ((test.WomanOrBoySurvived > 0.238) & \\\n           ~((test.WomanOrBoySurvived > 0.55) & (test.WomanOrBoySurvived <= 0.633))))\nacc3_simple_rule = acc_simple_rule\nLB_simple_rule = 0.80382","5558ef65":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Support Vector Machines', 'Linear SVC', 'k-Nearest Neighbors', 'Naive Bayes', \n              'Perceptron', 'Stochastic Gradient Decent', \n              'Decision Tree Classifier', 'Random Forest',  'XGBClassifier', 'LGBMClassifier',\n              'GradientBoostingClassifier', 'RidgeClassifier', 'BaggingClassifier', 'ExtraTreesClassifier', \n              'Neural Network 1', 'Neural Network 2', \n              'VotingClassifier-hard voiting', 'VotingClassifier-soft voting',\n              'Simple rule'],\n    \n    'Score16': [acc_log, acc_svc, acc_linear_svc, acc_knn, acc_gaussian, \n              acc_perceptron, acc_sgd, \n              acc_decision_tree, acc_random_forest, acc_XGB_Classifier, acc_LGB_Classifier,\n              acc_gradient_boosting, acc_ridge_classifier, acc_bagging_classifier, acc_etc, \n              acc_ann1, acc_ann2, \n              acc_VC_hard, acc_VC_soft,\n              acc_simple_rule],\n\n    'Score3': [acc3_log, acc3_svc, acc3_linear_svc, acc3_knn, acc3_gaussian, \n              acc3_perceptron, acc3_sgd, \n              acc3_decision_tree, acc3_random_forest, acc3_XGB_Classifier, acc3_LGB_Classifier,\n              acc3_gradient_boosting, acc3_ridge_classifier, acc3_bagging_classifier, acc3_etc, \n              acc3_ann1, acc3_ann2, \n              acc3_VC_hard, acc3_VC_soft,\n              acc3_simple_rule],\n\n    'LB_all': [LB_log_all, LB_svc_all, LB_linear_svc_all, LB_knn_all, LB_gaussian_all, \n              LB_perceptron_all, LB_sgd_all, \n              LB_decision_tree_all, LB_random_forest_all, LB_XGB_Classifier_all, LB_LGB_Classifier_all,\n              LB_GBC_all, LB_RidgeClassifier_all, LB_bagging_classifier_all, LB_ETC_all, \n              LB_ann1_all, LB_ann2_all, \n              LB_VC_hard_all, LB_VC_soft_all,\n              LB_simple_rule_all],\n    \n    'LB':    [LB_log, LB_svc, LB_linear_svc, LB_knn, LB_gaussian, \n              LB_perceptron, LB_sgd, \n              LB_decision_tree, LB_random_forest, LB_XGB_Classifier, LB_LGB_Classifier,\n              LB_GBC, LB_RidgeClassifier, LB_bagging_classifier, LB_ETC, \n              LB_ann1, LB_ann2, \n              LB_VC_hard, LB_VC_soft,\n              LB_simple_rule]})","4df39906":"models.sort_values(by=['Score16', 'LB_all', 'LB'], ascending=False)","97af9b77":"models.sort_values(by=['Score3', 'LB_all', 'LB'], ascending=False)","ed65e5dd":"models.sort_values(by=['LB_all', 'LB', 'Score3'], ascending=False)","9fa0ef98":"models.sort_values(by=['LB', 'LB_all', 'Score3'], ascending=False)","f37bc23e":"### 5.10 XGB Classifier <a class=\"anchor\" id=\"5.10\"><\/a>\n\n[Back to Table of Contents](#0.1)","c0ac9a38":"Light GBM is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithms. It splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word \u2018Light\u2019. Reference [Analytics Vidhya](https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/which-algorithm-takes-the-crown-light-gbm-vs-xgboost\/).","dcc5f4c3":"All models were tuned for a complete set of 16 features, solutions were calculated, that were uploaded to the competition, which made it possible to determine the error LB_all.\nAfter which the decision was taken into account\n* https:\/\/www.kaggle.com\/mauricef\/titanic\n* https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-one-line-of-the-prediction-code\n\ncalculated on only three features ('WomanOrBoySurvived', 'Alone', 'Sex') - an error LB is defined for this option.","03d00917":"## 7. Models evaluation <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","bca36a3f":"## 4. Preparing to modeling <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","c9542d35":"[Go to Top](#0)","835f9976":"We can use Logistic Regression to validate our assumptions and decisions for feature creating and completing goals. This can be done by calculating the coefficient of the features in the decision function.\n\nPositive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).\n\n- Alone is highest positivie coefficient, implying as the Alone value increases (0 to 1), the probability of Survived=1 increases the most.\n- Inversely as Sex increases (male: 0 to female: 1), probability of Survived=1 decreases the most.\n- This way Age has second highest negative correlation with Survived.\n- So is LastName as second highest positive correlation.","51e2f1e3":"<a class=\"anchor\" id=\"0\"><\/a>\n\n## FE, tuning and comparison of the 20 popular models with  predictions on the example of competition \"Titanic: Machine Learning from Disaster\"","8a8b45a1":"My kernels\n\n* [Titanic : one line of the prediction code](https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-one-line-of-the-prediction-code)\n* [Titanic : cluster analysis](https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-cluster-analysis)\n\npresents a solutions using a simple rule and only 3 features ('WomanOrBoySurvived', 'Sex', 'Alone'). Let's look at how all these models are tuned for those 3 features and whether we can find an even better solution.","74d7bf91":"Thanks to https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions","193aeb9d":"### 5.8 Decision Tree Classifier <a class=\"anchor\" id=\"5.8\"><\/a>\n\n[Back to Table of Contents](#0.1)","bb3019ae":"### 5.9 Random Forests <a class=\"anchor\" id=\"5.9\"><\/a>\n\n[Back to Table of Contents](#0.1)","27a5b268":"**Support Vector Machines** are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training samples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new test samples to one category or the other, making it a non-probabilistic binary linear classifier. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Support_vector_machine).","d63966b0":"### 4.1 Encoding categorical features <a class=\"anchor\" id=\"4.1\"><\/a>","c704b425":"Thanks to https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions","881fea13":"Thanks to https:\/\/www.kaggle.com\/kabure\/titanic-eda-model-pipeline-keras-nn","7ba4cd8b":"- The best model is the **simple rule in one line** from [\"Titanic Top 3% : one line of the prediction code\"](https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-one-line-of-the-prediction-code). Surprisingly, that the simple rule in one line gives the best result. This once again proves the enormous value of features engineering. The optimal selection of features is the key to success!\n\n- Models **GradientBoostingClassifier, Random Forests, VotingClassifiers, BaggingClassifier, Decision Tree Classifier** have provided the same accuracy LB on the test dataset as the simple rule, although on the training dataset they are much more accurate up to 100%.\n\n- The **VotingClassifier** for both voting options (\"*hard*\" and \"*soft*\") aggregation gave the same result for all the variants of features, that is, the solution found is indeed optimal, although a **Logistic Regression**, which is not one of the best, was selected for voting. This confirms the high efficiency of this method of aggregating (ensembling) predictions.\n\n- The models **GradientBoostingClassifier, BaggingClassifier, Random Forests, VotingClassifiers** did a good job of optimizing the features themselves, providing comparable accuracy for the different number of features in the test dataset, but the models **Decision Tree Classifier, Stochastic Gradient Descent, Support Vector Machines, Perceptron, Neural Networks, k-Nearest Neighbors algorithm** are very sensitive to the feature sets, because the accuracy of LB_all and LB is very different. Models **XGB Classifier, LGBM Classifier, ExtraTreesClassifier, Logistic Regression, Linear SVC, Naive Bayes, RidgeClassifier** depend on FE, but not so significantly.\n\n- The methods **LGBM Classifier, Perceptron, Neural Networks, Linear SVC, Naive Bayes, Logistic Regression, k-Nearest Neighbors algorithm, RidgeClassifier** have low accuracy LB, especially methods **Naive Bayes, Logistic Regression, Linear SVC**, compared to other models, although the \"***Titanic: Machine Learning from Disaster***\" contest is not indicative for the machine learning tasks because it contains too little data.\n\n- In all models, except **LGBM Classifier, Linear SVC, RidgeClassifier, Logistic Regression, Naive Bayes**, the prediction of test dataset based on 3 features yielded a more accurate result, possibly because these models perform worse under conditions of low number of features or under conditions of significant data dependence (in fact, feature \"*WomanOrBoySurvived*\" is, in part derived from others features \"*Sex*\" and \"*Alone*\") or with a small number of points (millions of points may vary greatly). Particularly interesting is that the **LGBM Classifier** model and method gave comparatively low accuracy on both the training and test datasets for the variant with three features, unlike **XGB Classifier** and other decision tree-based methods in which LB > LB_all.\n\n- To increase the accuracy of predictions, its need to increase the number of features and further improve their processing, that is FE (for example, add the processed feature \"**Tickets**\" - context is consist of a good kernels with have examples of such processing ([\"Advanced Feature Engineering Tutorial with Titanic\"](https:\/\/www.kaggle.com\/gunesevitan\/advanced-feature-engineering-tutorial-with-titanic) etc.).","19b305aa":"In pattern recognition, the **k-Nearest Neighbors algorithm** (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm).","819072fe":"### 5.17 Neural Network 2 <a class=\"anchor\" id=\"5.17\"><\/a>\n\n[Back to Table of Contents](#0.1)","c93de41b":"### 5.15 ExtraTreesClassifier <a class=\"anchor\" id=\"5.15\"><\/a>\n\n[Back to Table of Contents](#0.1)","2ead6a37":"Thanks to https:\/\/www.kaggle.com\/nhlr21\/complete-titanic-tutorial-with-ml-nn-ensembling","157585a7":"The **Perceptron** is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Perceptron).","5509a800":"### 5.19 VotingClassifier (soft voting) <a class=\"anchor\" id=\"5.19\"><\/a>\n\n[Back to Table of Contents](#0.1)","272b7b7a":"This model uses a **Decision Tree** as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Decision_tree_learning).","8cb32f5d":"## 6. Tuning models and test for 3 features <a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","719dfb67":"### 5.18 VotingClassifier (hard voting) <a class=\"anchor\" id=\"5.18\"><\/a>\n\n[Back to Table of Contents](#0.1)","809ac747":"Thanks for most popular models to:\n* https:\/\/www.kaggle.com\/kabure\/titanic-eda-model-pipeline-keras-nn\n* https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions \n* https:\/\/www.kaggle.com\/nhlr21\/complete-titanic-tutorial-with-ml-nn-ensembling\n\nThanks for FE:\n* https:\/\/www.kaggle.com\/vbmokin\/three-lines-of-code-for-titanic-top-15\n* https:\/\/www.kaggle.com\/vbmokin\/three-lines-of-code-for-titanic-top-20\n* https:\/\/www.kaggle.com\/mauricef\/titanic\n* https:\/\/www.kaggle.com\/kpacocha\/top-6-titanic-machine-learning-from-disaster\n* https:\/\/www.kaggle.com\/erinsweet\/simpledetect","722b65b1":"### 5.14 BaggingClassifier <a class=\"anchor\" id=\"5.14\"><\/a>\n\n[Back to Table of Contents](#0.1)","2b4ca7e2":"### 5.7 Stochastic Gradient Descent <a class=\"anchor\" id=\"5.7\"><\/a>\n\n[Back to Table of Contents](#0.1)","06bd610e":"### 5.3 Linear SVC <a class=\"anchor\" id=\"5.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","d24b3cc7":"Thanks to https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions","7654249e":"**Neural networks** are more complex and more powerful algorithm than standars machine learning, it belongs to deep learning models. To build a neural network use Keras. Keras is a high level API for tensorflow, which is a tensor-manipulation framework made by google. Keras allows you to build neural networks by assembling blocks (which are the layers of neural network). ","167a3634":"### 5.11 LGBM Classifier <a class=\"anchor\" id=\"5.11\"><\/a>\n\n[Back to Table of Contents](#0.1)","e4f913ee":"**ExtraTreesClassifier** implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The default values for the parameters controlling the size of the trees (e.g. max_depth, min_samples_leaf, etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values. Reference [sklearn documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html). \n\nIn extremely randomized trees, randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias. Reference [sklearn documentation](https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#Extremely%20Randomized%20Trees).","ff59a4c2":"Thanks to:\n* https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-one-line-of-the-prediction-code\n* https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-cluster-analysis\n* https:\/\/www.kaggle.com\/mauricef\/titanic","104a7069":"In machine learning, **Naive Bayes classifiers** are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Naive_Bayes_classifier).","dc6c55ac":"## Acknowledgements","bfc1d5a6":"### 5.1 Logistic Regression <a class=\"anchor\" id=\"5.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","81c97422":"**Logistic Regression** is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Logistic_regression).\n\nNote the confidence score generated by the model based on our training dataset.","d678365e":"### 4.2 Creation of training and validation sets <a class=\"anchor\" id=\"4.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","0c1e7748":"**Stochastic gradient descent** (often abbreviated **SGD**) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in big data applications this reduces the computational burden, achieving faster iterations in trade for a slightly lower convergence rate. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Stochastic_gradient_descent).","d85a864e":"### 5.20 The simple rule in one line <a class=\"anchor\" id=\"5.20\"><\/a>\n\n[Back to Table of Contents](#0.1)","033767a1":"### 5.16 Neural Network 1 <a class=\"anchor\" id=\"5.16\"><\/a>\n\n[Back to Table of Contents](#0.1)","9c28df74":"We can now rank our evaluation of all the models to choose the best one for our problem.","8aa8ff46":"### 5.6 Perceptron <a class=\"anchor\" id=\"5.6\"><\/a>\n\n[Back to Table of Contents](#0.1)","54aa6148":"## 5. Tuning models and test for all 16 features <a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","3408154d":"Features engineering (FE) from Titanic Top 3%\n\nBuild of the 20 most popular models, the most complex models from them are tuned (optimized)\n\nComparison of the optimal for each type models by CV and LB","bff3a10b":"XGBoost is an ensemble tree method that apply the principle of boosting weak learners (CARTs generally) using the gradient descent architecture. XGBoost improves upon the base Gradient Boosting Machines (GBM) framework through systems optimization and algorithmic enhancements. Reference [Towards Data Science.](https:\/\/towardsdatascience.com\/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d)","4c6a8171":"## 1. Import libraries <a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","a2c863b5":"Thanks to https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions","ee418cef":"### 5.12 GradientBoostingClassifier <a class=\"anchor\" id=\"5.12\"><\/a>\n\n[Back to Table of Contents](#0.1)","8aff77ab":"**SVC** is a similar to SVM method. Its also builds on kernel functions but is appropriate for unsupervised learning. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Support-vector_machine#Support-vector_clustering_(SVC).","726c9fca":"Bootstrap aggregating, also called **bagging**, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach. Bagging leads to \"improvements for unstable procedures\", which include, for example, artificial neural networks, classification and regression trees, and subset selection in linear regression. On the other hand, it can mildly degrade the performance of stable methods such as K-nearest neighbors. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Bootstrap_aggregating).\n\nA **Bagging classifier** is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. Reference [sklearn documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingClassifier.html).","4dd77a5b":"Thanks to https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions","8bad9488":"It's solution generate tuned DecisionTreeClassifier by the GridSearchCV from kernels:\nhttps:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-one-line-of-the-prediction-code","7e308864":"Thanks to https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions","b91e3296":"Your comments and feedback are most welcome.","00499775":"In contrast to majority voting (hard voting), **soft voting** returns the class label as argmax of the **sum of predicted probabilities**.\nSpecific weights can be assigned to each classifier via the weights parameter. When weights are provided, the predicted class probabilities for each classifier are collected, multiplied by the classifier weight, and averaged. The final class label is then derived from the class label with the highest average probability. Reference [sklearn documentation](https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#Voting%20Classifier).","63647749":"Thanks to https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n\nNow we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n\n- Logistic Regression\n- Support Vector Machines and Linear SVC\n- KNN or k-Nearest Neighbors\n- Naive Bayes Classifier or Gaussian Naive Bayes\n- Stochastic Gradient Descent, GradientBoostingClassifier, RidgeClassifier, BaggingClassifier\n- Decision Tree Classifier, Random Forest, XGB Classifier, LGBM Classifier, ExtraTreesClassifier\n- Perceptron, Neural Networks with different archictures (Deep Learning)\n- VotingClassifier (hard or soft voting)","e3039ea0":"### 5.2 Support Vector Machines <a class=\"anchor\" id=\"5.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","69a0b898":"### 5.4 k-Nearest Neighbors algorithm <a class=\"anchor\" id=\"5.4\"><\/a>\n\n[Back to Table of Contents](#0.1)","854c3f34":"## 3. Features engineering (FE) <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","5588e7cd":"**Random Forests** is one of the most popular model. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators= [100, 300]) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Random_forest).","b64eb4b9":"Thanks to https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions","a74e6b92":"Tikhonov Regularization, colloquially known as **Ridge Regression**, is the most commonly used regression algorithm to approximate an answer for an equation with no unique solution. This type of problem is very common in machine learning tasks, where the \"best\" solution must be chosen using limited data. If a unique solution exists, algorithm will return the optimal value. However, if multiple solutions exist, it may choose any of them. Reference [Brilliant.org](https:\/\/brilliant.org\/wiki\/ridge-regression\/).","6416b116":"Thanks to https:\/\/www.kaggle.com\/kabure\/titanic-eda-model-pipeline-keras-nn","3f7c5dde":"### 5.13 RidgeClassifier <a class=\"anchor\" id=\"5.13\"><\/a>\n\n[Back to Table of Contents](#0.1)","da501cd2":"### 5.5 Naive Bayes <a class=\"anchor\" id=\"5.5\"><\/a>\n\n[Back to Table of Contents](#0.1)","e456c62d":"Thanks to https:\/\/www.kaggle.com\/kabure\/titanic-eda-model-pipeline-keras-nn","db9e2629":"Thanks for the example of ensemling different models from \nhttps:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#Extremely%20Randomized%20Trees","3cda4c76":"We will tuning the hyperparameters of the LGBMClassifier model using the HyperOpt and 10-fold crossvalidation","d9295ba4":"Thanks to https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions","edd360ce":"**Gradient Boosting** builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced. The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data and max_features=n_features, if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, random_state has to be fixed. Reference [sklearn documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html).","dc1c983f":"<a class=\"anchor\" id=\"0.1\"><\/a>\n\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [Download datasets](#2)\n1. [Features engineering (FE)](#3)\n1. [Preparing to modeling](#4)\n    -  [Encoding categorical features](#4.1)\n    -  [Creation of training and validation sets](#4.2)\n1. [Tuning models and test for all 16 features](#5)\n    -  [Logistic Regression](#5.1)\n    -  [Support Vector Machines](#5.2)\n    -  [Linear SVC](#5.3)\n    -  [k-Nearest Neighbors algorithm with GridSearchCV](#5.4)\n    -  [Naive Bayes](#5.5)\n    -  [Perceptron](#5.6)\n    -  [Stochastic Gradient Descent](#5.7)\n    -  [Decision Tree Classifier](#5.8)\n    -  [Random Forests with GridSearchCV](#5.9)\n    -  [XGB Classifier with HyperOpt](#5.10)\n    -  [LGBM Classifier with HyperOpt](#5.11)\n    -  [GradientBoostingClassifier with HyperOpt](#5.12)\n    -  [RidgeClassifier](#5.13)\n    -  [BaggingClassifier](#5.14)\n    -  [ExtraTreesClassifier with HyperOpt](#5.15)\n    -  [Neural Network 1](#5.16)\n    -  [Neural Network 2](#5.17)\n    -  [VotingClassifier (hard voting)](#5.18)\n    -  [VotingClassifier (soft voting) with GridSearchCV](#5.19)\n    -  [The simple rule in one line](#5.20)\n1. [Tuning models and test for 3 features](#6)\n1. [Models evaluation](#7)\n1. [Conclusion](#8)\n","8a6419d0":"Thanks for the example of ensemling different models from \nhttps:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#Extremely%20Randomized%20Trees","43e21efa":"## 2. Download datasets <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","da8897df":"Thanks to https:\/\/www.kaggle.com\/junheeshin\/titanic-analyze-and-predict-nn","82f80e01":"## 8. Conclusion <a class=\"anchor\" id=\"8\"><\/a>\n\n[Back to Table of Contents](#0.1)","caaaed7c":"Thanks to https:\/\/www.kaggle.com\/kabure\/titanic-eda-model-pipeline-keras-nn","9c0e2cf7":"We will tuning the hyperparameters of the XGBClassifier model using the HyperOpt and 10-fold crossvalidation","169e39a8":"I hope you find this kernel useful and enjoyable.","7a8ecbdb":"The idea behind the **VotingClassifier** is to combine conceptually different machine learning classifiers and use a majority vote (hard vote) or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses. Reference [sklearn documentation](https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#Voting%20Classifier).","643cf0ea":"Thanks to https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions","8a6b877a":"The VotingClassifier (with **hard voting**) would classify the sample as \u201cclass 1\u201d based on the **majority class label**. Reference [sklearn documentation](https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#Voting%20Classifier)."}}