{"cell_type":{"42a8c8af":"code","c8bcd555":"code","4327e63a":"code","ed3fdd34":"code","8a634346":"code","2c54dbf1":"code","dedf378e":"code","f5e7e29e":"code","244cfa11":"code","3e9f71df":"code","a92e1faa":"code","7aa2608a":"code","56efe5b3":"code","9e64f01a":"code","32046081":"code","15efc70b":"code","58584a17":"code","c8a7df9a":"code","af680a4f":"code","6639e2a4":"code","abf71cf6":"code","23d74baf":"code","7b9c8447":"code","a0153111":"code","191b9e0b":"code","efb0fbec":"code","2815ab55":"code","3e553374":"code","2d35a5d9":"code","929cabff":"code","bdbf3894":"code","d2e04443":"code","30021bc1":"code","9da6d0aa":"code","983c610e":"code","0eff1d4d":"code","73f12bf3":"code","9fd95794":"code","d2f19e5b":"code","5a043c52":"code","2b8bd57b":"markdown","a05c0aed":"markdown","922115de":"markdown","5377b56d":"markdown","9f56091b":"markdown","fa956483":"markdown","248ebc48":"markdown","8c7d622d":"markdown","582c2e84":"markdown","9671d49f":"markdown"},"source":{"42a8c8af":"from statsmodels.tsa.ar_model import AutoReg, ar_select_order\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.api import acf, pacf, graphics\nfrom typing import List, Tuple, Union, NoReturn\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.offline as py\nimport plotly.express as px\nimport cufflinks as cf\nimport plotly\nfrom statsmodels.robust import mad\nimport matplotlib.pyplot as plt\nfrom scipy.signal import butter\nfrom scipy import signal\nimport seaborn as sns\nfrom sklearn import *\nimport pandas as pd \nimport numpy as np\nimport warnings\nimport scipy\nimport pywt\nimport os\nimport gc\nfrom sklearn.metrics import cohen_kappa_score,f1_score,confusion_matrix\n\ncf.go_offline()\npy.init_notebook_mode()\ncf.getThemes()\ncf.set_config_file(theme='ggplot')\nwarnings.simplefilter('ignore')\npd.plotting.register_matplotlib_converters()\nsns.mpl.rc('figure',figsize=(16, 6))\nplt.style.use('ggplot')\nsns.set_style('darkgrid')\n","c8bcd555":"base = os.path.abspath('\/kaggle\/input\/liverpool-ion-switching\/')\ntrain = pd.read_csv(os.path.join(base + '\/train.csv'))\ntest  = pd.read_csv(os.path.join(base + '\/test.csv'))","4327e63a":"train2 = train.copy()\n\na=500000; b=600000 # CLEAN TRAIN BATCH 2\ntrain2.loc[train.index[a:b],'signal2'] = train2.signal[a:b].values - 3*(train2.time.values[a:b] - 50)\/10.\n\ndef f(x,low,high,mid): return -((-low+high)\/625)*(x-mid)**2+high -low\n\n# CLEAN TRAIN BATCH 7\nbatch = 7; a = 500000*(batch-1); b = 500000*batch\ntrain2.loc[train2.index[a:b],'signal2'] = train.signal.values[a:b] - f(train.time[a:b].values,-1.817,3.186,325)\n# CLEAN TRAIN BATCH 8\nbatch = 8; a = 500000*(batch-1); b = 500000*batch\ntrain2.loc[train2.index[a:b],'signal2'] = train.signal.values[a:b] - f(train.time[a:b].values,-0.094,4.936,375)\n# CLEAN TRAIN BATCH 9\nbatch = 9; a = 500000*(batch-1); b = 500000*batch\ntrain2.loc[train2.index[a:b],'signal2'] = train.signal.values[a:b] - f(train.time[a:b].values,1.715,6.689,425)\n# CLEAN TRAIN BATCH 10\nbatch = 10; a = 500000*(batch-1); b = 500000*batch\ntrain2.loc[train2.index[a:b],'signal2'] = train.signal.values[a:b] - f(train.time[a:b].values,3.361,8.45,475)","ed3fdd34":"test2 = test.copy()\n\n# REMOVE BATCH 1 DRIFT\nstart=500\na = 0; b = 100000\ntest2.loc[test2.index[a:b],'signal2'] = test2.signal.values[a:b] - 3*(test2.time.values[a:b]-start)\/10.\nstart=510\na = 100000; b = 200000\ntest2.loc[test2.index[a:b],'signal2'] = test2.signal.values[a:b] - 3*(test2.time.values[a:b]-start)\/10.\nstart=540\na = 400000; b = 500000\ntest2.loc[test2.index[a:b],'signal2'] = test2.signal.values[a:b] - 3*(test2.time.values[a:b]-start)\/10.\n\n# REMOVE BATCH 2 DRIFT\nstart=560\na = 600000; b = 700000\ntest2.loc[test2.index[a:b],'signal2'] = test2.signal.values[a:b] - 3*(test2.time.values[a:b]-start)\/10.\nstart=570\na = 700000; b = 800000\ntest2.loc[test2.index[a:b],'signal2'] = test2.signal.values[a:b] - 3*(test2.time.values[a:b]-start)\/10.\nstart=580\na = 800000; b = 900000\ntest2.loc[test2.index[a:b],'signal2'] = test2.signal.values[a:b] - 3*(test2.time.values[a:b]-start)\/10.\n\n# REMOVE BATCH 3 DRIFT\ndef f(x):\n    return -(0.00788)*(x-625)**2+2.345 +2.58\na = 1000000; b = 1500000\ntest2.loc[test2.index[a:b],'signal2'] = test2.signal.values[a:b] - f(test2.time[a:b].values)","8a634346":"def features(df):\n    df = df.sort_values(by=['time']).reset_index(drop=True)\n    df.index = ((df.time * 10_000) - 1).values\n    df['batch'] = df.index \/\/ 25_000\n    df['batch_index'] = df.index  - (df.batch * 25_000)\n    df['batch_slices'] = df['batch_index']  \/\/ 2500\n    df['batch_slices2'] = df.apply(lambda r: '_'.join([str(r['batch']).zfill(3), str(r['batch_slices']).zfill(3)]), axis=1)\n    \n    for j in ['signal']:\n        for c in ['batch','batch_slices2']:\n            d = {}\n            d['mean'+c+j] = df.groupby([c])[j].mean()\n            d['median'+c+j] = df.groupby([c])[j].median()\n            d['max'+c+j] = df.groupby([c])[j].max()\n            d['min'+c+j] = df.groupby([c])[j].min()\n            d['std'+c+j] = df.groupby([c])[j].std()\n            d['mean_abs_chg'+c+j] = df.groupby([c])[j].apply(lambda x: np.mean(np.abs(np.diff(x))))\n            d['abs_max'+c+j] = df.groupby([c])[j].apply(lambda x: np.max(np.abs(x)))\n            d['abs_min'+c+j] = df.groupby([c])[j].apply(lambda x: np.min(np.abs(x)))\n            d['range'+c+j] = d['max'+c+j] - d['min'+c+j]\n            d['maxtomin'+c+j] = d['max'+c+j] \/ d['min'+c+j]\n            d['abs_avg'+c+j] = (d['abs_min'+c+j] + d['abs_max'+c+j]) \/ 2\n            for v in d:\n                df[v] = df[c].map(d[v].to_dict())\n    #add shifts\n        df['signal_shift_+1'+j] = df[j].shift(1)\n        df['signal_shift_-1'+j] = df[j].shift(-1)\n        \n            \n    for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal', 'open_channels', 'batch', 'batch_index', 'batch_slices', 'batch_slices2']]:\n        df[c+'_msignal'] = df[c] - df['signal']\n        \n   # df['binning']=pd.cut(df['signal'], [-np.inf] + list(np.linspace(df['signal'].min(),df['signal'].max(),12)) + [np.inf], labels = range(int(df['signal'].min()),int(df['signal'].max())))\n    \n    return df\n    \ntrain2 = features(train2)\ntest = features(test2)","2c54dbf1":"col = [c for c in train2.columns if c not in ['time', 'open_channels', 'batch', 'batch_index', 'batch_slices', 'batch_slices2']]","dedf378e":"def MacroF1Metric(preds, dtrain):\n    labels = dtrain.get_label()\n    preds = np.round(np.clip(preds, 0, 10)).astype(int)\n    score = metrics.f1_score(labels, preds, average = 'macro')\n    return ('MacroF1Metric', score, True)","f5e7e29e":"model_batch = [[1,2],[3,7],[4,8],[6,9],[5,10]]","244cfa11":"sub = pd.read_csv('..\/input\/liverpool-ion-switching\/sample_submission.csv')","3e9f71df":"valid_predict_LGBM =[]\nvalid_target_LGBM =[]\nvalid_predict_XGB =[]\nvalid_target_XGB =[]\nvalid_predict_CAT =[]\nvalid_target_CAT =[]","a92e1faa":"sub['LGBM_preds']=sub.open_channels\nsub['XGB_preds']=sub.open_channels\nsub['CAT_preds']=sub.open_channels","7aa2608a":"for j,idx in enumerate(model_batch[0]):\n    if j ==0:\n        batch = idx; a = 500000*(batch-1); b = 500000*batch\n        train_model_split = train2[col].loc[train2.index[a:b]]\n        train_model_target = train2.loc[train2.index[a:b],'open_channels']\n    else:\n        batch = idx; a = 500000*(batch-1); b = 500000*batch\n        train_model_split1 = train2[col].loc[train2.index[a:b]]\n        train_model_target1 = train2.loc[train2.index[a:b],'open_channels']\n        train_model_split = np.concatenate((train_model_split,train_model_split1),axis = 0)\n        train_model_target = np.concatenate((train_model_target,train_model_target1),axis = 0)\n\nx1, x2, y1, y2 = model_selection.train_test_split(train_model_split, train_model_target , test_size=0.2, random_state=49)\n\n\nprint('============================ start training LGBM ==================================')\n\n#First model\nimport lightgbm as lgb\nparams = {'learning_rate': 0.1, 'max_depth': -1, 'num_leaves':2**7+1, 'metric': 'rmse', 'random_state': 7, 'n_jobs':-1, 'sample_fraction':0.33} \nmodel = lgb.train(params, lgb.Dataset(x1, y1), 22222,  lgb.Dataset(x2, y2), verbose_eval=100, early_stopping_rounds=250, feval=MacroF1Metric)\n#preds_lgb = (model.predict(test[col], num_iteration=model.best_iteration)).astype(np.float16)\noof_valid_lgb = (model.predict(x2, num_iteration=model.best_iteration)).astype(np.float16)\nprint(\"BEST VALIDATION_SCORE (F1): \", f1_score(np.round(np.clip(oof_valid_lgb, 0, 10)).astype(int),np.array(y2) ,average = 'macro'))\nvalid_predict_LGBM = valid_predict_LGBM + oof_valid_lgb.tolist()\nvalid_target_LGBM = valid_target_LGBM + np.array(y2).tolist()\n\nprint('============================ start LGBM predict==================================')\na = 0 \ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),2] = (model.predict(test_batch, num_iteration=model.best_iteration)).astype(np.float16)\n\na = 3\ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),2] = (model.predict(test_batch, num_iteration=model.best_iteration)).astype(np.float16)\n\na = 8\ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),2] = (model.predict(test_batch, num_iteration=model.best_iteration)).astype(np.float16)\n\ntest_batch = test[col].loc[test.index[1000000:2000000]]\nprint ('start test last subsample'+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[1000000:2000000,2] = (model.predict(test_batch, num_iteration=model.best_iteration)).astype(np.float16)\n\nprint (gc.collect())\n\n\nprint('============================ start training XGBBOOST ==================================')\nimport xgboost as xgb\nparams = {'colsample_bytree': 0.375,'learning_rate': 0.1,'max_depth': 10, 'subsample': 1, 'objective':'reg:squarederror',\n          'eval_metric':'rmse', 'n_estimators':22222,   'tree_method':'gpu_hist',}\ntrain_set = xgb.DMatrix(x1, y1)\nval_set = xgb.DMatrix(x2, y2)\nmodel = xgb.train(params, train_set, num_boost_round=2222, evals=[(train_set, 'train'), (val_set, 'val')], \n                         verbose_eval=100, early_stopping_rounds=250)\n#preds_xgb = model.predict(xgb.DMatrix(test[col]), ntree_limit=model.best_ntree_limit)\noof_valid_xgb = (model.predict(xgb.DMatrix(x2), ntree_limit=model.best_ntree_limit)).astype(np.float16)\nprint(\"BEST VALIDATION_SCORE (F1): \", f1_score(np.round(np.clip(oof_valid_xgb, 0, 10)).astype(int),np.array(y2) ,average = 'macro'))\ndel train_set, val_set; gc.collect()\nvalid_predict_XGB = valid_predict_XGB + oof_valid_xgb.tolist()\nvalid_target_XGB = valid_target_XGB + np.array(y2).tolist()\n\nprint('============================ start XGB predict==================================')\na = 0 \ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),3] = (model.predict(xgb.DMatrix(np.array(test_batch)), ntree_limit=model.best_ntree_limit)).astype(np.float16)\n\na = 3\ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),3] = (model.predict(xgb.DMatrix(np.array(test_batch)), ntree_limit=model.best_ntree_limit)).astype(np.float16)\n\na = 8\ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),3] = (model.predict(xgb.DMatrix(np.array(test_batch)), ntree_limit=model.best_ntree_limit)).astype(np.float16)\n\ntest_batch = test[col].loc[test.index[1000000:2000000]]\nprint ('start test last subsample'+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[1000000:2000000,3] = (model.predict(xgb.DMatrix(np.array(test_batch)), ntree_limit=model.best_ntree_limit)).astype(np.float16)\n\nprint('============================ start training Catboost ==================================')\nfrom catboost import Pool,CatBoostRegressor\nmodel = CatBoostRegressor(task_type = 'GPU', iterations=22222, learning_rate=0.1, random_seed = 7, depth=7, eval_metric='RMSE')\ntrain_dataset = Pool(x1,  y1)          \neval_dataset = Pool(x2,  y2)\nmodel.fit(train_dataset, eval_set=eval_dataset, verbose=100, early_stopping_rounds=250)\n#preds_cb = (model.predict(test[col])).astype(np.float16)\noof_valid_cb = (model.predict(x2)).astype(np.float16)\nprint(\"BEST VALIDATION_SCORE (F1): \", f1_score(np.round(np.clip(oof_valid_cb, 0, 10)).astype(int),np.array(y2) ,average = 'macro'))\ndel train_dataset, eval_dataset \nprint (gc.collect())\nvalid_predict_CAT = valid_predict_CAT + oof_valid_cb.tolist()\nvalid_target_CAT = valid_target_CAT + np.array(y2).tolist()\n\nprint('============================ start Catboost predict==================================')\na = 0 \ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),4] = (model.predict(test_batch)).astype(np.float16)\n\na = 3\ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),4] = (model.predict(test_batch)).astype(np.float16)\n\na = 8\ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),4] = (model.predict(test_batch)).astype(np.float16)\n\ntest_batch = test[col].loc[test.index[1000000:2000000]]\nprint ('start test last subsample'+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[1000000:2000000,4] = (model.predict(test_batch)).astype(np.float16)","56efe5b3":"for j,idx in enumerate(model_batch[1]):\n    if j ==0:\n        batch = idx; a = 500000*(batch-1); b = 500000*batch\n        train_model_split = train2[col].loc[train2.index[a:b]]\n        train_model_target = train2.loc[train2.index[a:b],'open_channels']\n    else:\n        batch = idx; a = 500000*(batch-1); b = 500000*batch\n        train_model_split1 = train2[col].loc[train2.index[a:b]]\n        train_model_target1 = train2.loc[train2.index[a:b],'open_channels']\n        train_model_split = np.concatenate((train_model_split,train_model_split1),axis = 0)\n        train_model_target = np.concatenate((train_model_target,train_model_target1),axis = 0)\n\nx1, x2, y1, y2 = model_selection.train_test_split(train_model_split, train_model_target , test_size=0.2, random_state=49)\n\nprint('============================ start training LGBM ==================================')\n\n#Second model\nimport lightgbm as lgb\nparams = {'learning_rate': 0.1, 'max_depth': -1, 'num_leaves':2**7+1, 'metric': 'rmse', 'random_state': 7, 'n_jobs':-1, 'sample_fraction':0.33} \nmodel = lgb.train(params, lgb.Dataset(x1, y1), 22222,  lgb.Dataset(x2, y2), verbose_eval=100, early_stopping_rounds=250, feval=MacroF1Metric)\n#preds_lgb = (model.predict(test[col], num_iteration=model.best_iteration)).astype(np.float16)\noof_valid_lgb = (model.predict(x2, num_iteration=model.best_iteration)).astype(np.float16)\n\nprint(\"BEST VALIDATION_SCORE (F1): \", f1_score(np.round(np.clip(oof_valid_lgb, 0, 10)).astype(int),np.array(y2) ,average = 'macro'))\nvalid_predict_LGBM = valid_predict_LGBM + oof_valid_lgb.tolist()\nvalid_target_LGBM = valid_target_LGBM + np.array(y2).tolist()\n\nprint('============================ start LGBM predict==================================')\na = 4 \ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),2] = (model.predict(test_batch, num_iteration=model.best_iteration)).astype(np.float16)\n\ngc.collect()\n\n\nprint('============================ start training XGBBOOST ==================================')\nimport xgboost as xgb\nparams = {'colsample_bytree': 0.375,'learning_rate': 0.1,'max_depth': 10, 'subsample': 1, 'objective':'reg:squarederror',\n          'eval_metric':'rmse', 'n_estimators':22222,   'tree_method':'gpu_hist',}\ntrain_set = xgb.DMatrix(x1, y1)\nval_set = xgb.DMatrix(x2, y2)\nmodel = xgb.train(params, train_set, num_boost_round=2222, evals=[(train_set, 'train'), (val_set, 'val')], \n                         verbose_eval=100, early_stopping_rounds=250)\n#preds_xgb = model.predict(xgb.DMatrix(test[col]), ntree_limit=model.best_ntree_limit)\noof_valid_xgb = (model.predict(xgb.DMatrix(x2), ntree_limit=model.best_ntree_limit)).astype(np.float16)\nprint(\"BEST VALIDATION_SCORE (F1): \", f1_score(np.round(np.clip(oof_valid_xgb, 0, 10)).astype(int),np.array(y2) ,average = 'macro'))\ndel train_set, val_set; gc.collect()\nvalid_predict_XGB = valid_predict_XGB + oof_valid_xgb.tolist()\nvalid_target_XGB = valid_target_XGB + np.array(y2).tolist()\n\nprint('============================ start XGB predict==================================')\na = 4 \ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),3] = (model.predict(xgb.DMatrix(np.array(test_batch)), ntree_limit=model.best_ntree_limit)).astype(np.float16)\n\nprint('============================ start training Catboost ==================================')\nfrom catboost import Pool,CatBoostRegressor\nmodel = CatBoostRegressor(task_type = 'GPU', iterations=22222, learning_rate=0.1, random_seed = 7, depth=7, eval_metric='RMSE')\ntrain_dataset = Pool(x1,  y1)          \neval_dataset = Pool(x2,  y2)\nmodel.fit(train_dataset, eval_set=eval_dataset, verbose=100, early_stopping_rounds=250)\n#preds_cb = (model.predict(test[col])).astype(np.float16)\noof_valid_cb = (model.predict(x2)).astype(np.float16)\nprint(\"BEST VALIDATION_SCORE (F1): \", f1_score(np.round(np.clip(oof_valid_cb, 0, 10)).astype(int),np.array(y2) ,average = 'macro'))\ndel train_dataset, eval_dataset; gc.collect()\nvalid_predict_CAT = valid_predict_CAT + oof_valid_cb.tolist()\nvalid_target_CAT = valid_target_CAT + np.array(y2).tolist()\n\nprint('============================ start Catboost predict==================================')\na = 4 \ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),4] = (model.predict(test_batch)).astype(np.float16)\n","9e64f01a":"for j,idx in enumerate(model_batch[2]):\n    if j ==0:\n        batch = idx; a = 500000*(batch-1); b = 500000*batch\n        train_model_split = train2[col].loc[train2.index[a:b]]\n        train_model_target = train2.loc[train2.index[a:b],'open_channels']\n    else:\n        batch = idx; a = 500000*(batch-1); b = 500000*batch\n        train_model_split1 = train2[col].loc[train2.index[a:b]]\n        train_model_target1 = train2.loc[train2.index[a:b],'open_channels']\n        train_model_split = np.concatenate((train_model_split,train_model_split1),axis = 0)\n        train_model_target = np.concatenate((train_model_target,train_model_target1),axis = 0)\n\nx1, x2, y1, y2 = model_selection.train_test_split(train_model_split, train_model_target , test_size=0.2, random_state=49)\n\nprint('============================ start training LGBM ==================================')\n#Third model\nimport lightgbm as lgb\nparams = {'learning_rate': 0.1, 'max_depth': -1, 'num_leaves':2**7+1, 'metric': 'rmse', 'random_state': 7, 'n_jobs':-1, 'sample_fraction':0.33} \nmodel = lgb.train(params, lgb.Dataset(x1, y1), 22222,  lgb.Dataset(x2, y2), verbose_eval=100, early_stopping_rounds=250, feval=MacroF1Metric)\n#preds_lgb = (model.predict(test[col], num_iteration=model.best_iteration)).astype(np.float16)\noof_valid_lgb = (model.predict(x2, num_iteration=model.best_iteration)).astype(np.float16)\n\nprint(\"BEST VALIDATION_SCORE (F1): \", f1_score(np.round(np.clip(oof_valid_lgb, 0, 10)).astype(int),np.array(y2) ,average = 'macro'))\nvalid_predict_LGBM = valid_predict_LGBM + oof_valid_lgb.tolist()\nvalid_target_LGBM = valid_target_LGBM + np.array(y2).tolist()\n\nprint('============================ start LGBM predict==================================')\na = 1 \ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),2] = (model.predict(test_batch, num_iteration=model.best_iteration)).astype(np.float16)\n\na = 9 \ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),2] = (model.predict(test_batch, num_iteration=model.best_iteration)).astype(np.float16)\n\n\ngc.collect()\n\nprint('============================ start training XGBBOOST ==================================')\nimport xgboost as xgb\nparams = {'colsample_bytree': 0.375,'learning_rate': 0.1,'max_depth': 10, 'subsample': 1, 'objective':'reg:squarederror',\n          'eval_metric':'rmse', 'n_estimators':22222,   'tree_method':'gpu_hist',}\ntrain_set = xgb.DMatrix(x1, y1)\nval_set = xgb.DMatrix(x2, y2)\nmodel = xgb.train(params, train_set, num_boost_round=2222, evals=[(train_set, 'train'), (val_set, 'val')], \n                         verbose_eval=100, early_stopping_rounds=250)\n#preds_xgb = model.predict(xgb.DMatrix(test[col]), ntree_limit=model.best_ntree_limit)\noof_valid_xgb = (model.predict(xgb.DMatrix(x2), ntree_limit=model.best_ntree_limit)).astype(np.float16)\nprint(\"BEST VALIDATION_SCORE (F1): \", f1_score(np.round(np.clip(oof_valid_xgb, 0, 10)).astype(int),np.array(y2) ,average = 'macro'))\ndel train_set, val_set; gc.collect()\nvalid_predict_XGB = valid_predict_XGB + oof_valid_xgb.tolist()\nvalid_target_XGB = valid_target_XGB + np.array(y2).tolist()\n\nprint('============================ start XGB predict==================================')\na = 1 \ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),3] = (model.predict(xgb.DMatrix(np.array(test_batch)), ntree_limit=model.best_ntree_limit)).astype(np.float16)\n\na = 9 \ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),3] = (model.predict(xgb.DMatrix(np.array(test_batch)), ntree_limit=model.best_ntree_limit)).astype(np.float16)\n\nprint('============================ start training Catboost ==================================')\nfrom catboost import Pool,CatBoostRegressor\nmodel = CatBoostRegressor(task_type = 'GPU', iterations=22222, learning_rate=0.1, random_seed = 7, depth=7, eval_metric='RMSE')\ntrain_dataset = Pool(x1,  y1)          \neval_dataset = Pool(x2,  y2)\nmodel.fit(train_dataset, eval_set=eval_dataset, verbose=100, early_stopping_rounds=250)\n#preds_cb = (model.predict(test[col])).astype(np.float16)\noof_valid_cb = (model.predict(x2)).astype(np.float16)\nprint(\"BEST VALIDATION_SCORE (F1): \", f1_score(np.round(np.clip(oof_valid_cb, 0, 10)).astype(int),np.array(y2) ,average = 'macro'))\ndel train_dataset, eval_dataset; gc.collect()\nvalid_predict_CAT = valid_predict_CAT + oof_valid_cb.tolist()\nvalid_target_CAT = valid_target_CAT + np.array(y2).tolist()\n\nprint('============================ start Catboost predict==================================')\na = 1 \ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),4] = (model.predict(test_batch)).astype(np.float16)\n\na = 9\ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),4] = (model.predict(test_batch)).astype(np.float16)","32046081":"for j,idx in enumerate(model_batch[3]):\n    if j ==0:\n        batch = idx; a = 500000*(batch-1); b = 500000*batch\n        train_model_split = train2[col].loc[train2.index[a:b]]\n        train_model_target = train2.loc[train2.index[a:b],'open_channels']\n    else:\n        batch = idx; a = 500000*(batch-1); b = 500000*batch\n        train_model_split1 = train2[col].loc[train2.index[a:b]]\n        train_model_target1 = train2.loc[train2.index[a:b],'open_channels']\n        train_model_split = np.concatenate((train_model_split,train_model_split1),axis = 0)\n        train_model_target = np.concatenate((train_model_target,train_model_target1),axis = 0)\n\nx1, x2, y1, y2 = model_selection.train_test_split(train_model_split, train_model_target , test_size=0.2, random_state=49)\n\nprint('============================ start training LGBM ==================================')\n#Third model\nimport lightgbm as lgb\nparams = {'learning_rate': 0.1, 'max_depth': -1, 'num_leaves':2**7+1, 'metric': 'rmse', 'random_state': 7, 'n_jobs':-1, 'sample_fraction':0.33} \nmodel = lgb.train(params, lgb.Dataset(x1, y1), 22222,  lgb.Dataset(x2, y2), verbose_eval=100, early_stopping_rounds=250, feval=MacroF1Metric)\n#preds_lgb = (model.predict(test[col], num_iteration=model.best_iteration)).astype(np.float16)\noof_valid_lgb = (model.predict(x2, num_iteration=model.best_iteration)).astype(np.float16)\n\nprint(\"BEST VALIDATION_SCORE (F1): \", f1_score(np.round(np.clip(oof_valid_lgb, 0, 10)).astype(int),np.array(y2) ,average = 'macro'))\nvalid_predict_LGBM = valid_predict_LGBM + oof_valid_lgb.tolist()\nvalid_target_LGBM = valid_target_LGBM + np.array(y2).tolist()\n\nprint('============================ start LGBM predict==================================')\na = 2 \ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),2] = (model.predict(test_batch, num_iteration=model.best_iteration)).astype(np.float16)\n\na = 6 \ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),2] = (model.predict(test_batch, num_iteration=model.best_iteration)).astype(np.float16)\n\n\ngc.collect()\n\nprint('============================ start training XGBBOOST ==================================')\nimport xgboost as xgb\nparams = {'colsample_bytree': 0.375,'learning_rate': 0.1,'max_depth': 10, 'subsample': 1, 'objective':'reg:squarederror',\n          'eval_metric':'rmse', 'n_estimators':22222,   'tree_method':'gpu_hist',}\ntrain_set = xgb.DMatrix(x1, y1)\nval_set = xgb.DMatrix(x2, y2)\nmodel = xgb.train(params, train_set, num_boost_round=2222, evals=[(train_set, 'train'), (val_set, 'val')], \n                         verbose_eval=100, early_stopping_rounds=250)\n#preds_xgb = model.predict(xgb.DMatrix(test[col]), ntree_limit=model.best_ntree_limit)\noof_valid_xgb = (model.predict(xgb.DMatrix(x2), ntree_limit=model.best_ntree_limit)).astype(np.float16)\nprint(\"BEST VALIDATION_SCORE (F1): \", f1_score(np.round(np.clip(oof_valid_xgb, 0, 10)).astype(int),np.array(y2) ,average = 'macro'))\ndel train_set, val_set; gc.collect()\nvalid_predict_XGB = valid_predict_XGB + oof_valid_xgb.tolist()\nvalid_target_XGB = valid_target_XGB + np.array(y2).tolist()\n\nprint('============================ start XGB predict==================================')\na = 2 \ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),3] = (model.predict(xgb.DMatrix(np.array(test_batch)), ntree_limit=model.best_ntree_limit)).astype(np.float16)\n\na = 6 \ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),3] = (model.predict(xgb.DMatrix(np.array(test_batch)), ntree_limit=model.best_ntree_limit)).astype(np.float16)\n\nprint('============================ start training Catboost ==================================')\nfrom catboost import Pool,CatBoostRegressor\nmodel = CatBoostRegressor(task_type = 'GPU', iterations=22222, learning_rate=0.1, random_seed = 7, depth=7, eval_metric='RMSE')\ntrain_dataset = Pool(x1,  y1)\neval_dataset = Pool(x2,  y2)\nmodel.fit(train_dataset, eval_set=eval_dataset, verbose=100, early_stopping_rounds=250)\n#preds_cb = (model.predict(test[col])).astype(np.float16)\noof_valid_cb = (model.predict(x2)).astype(np.float16)\nprint(\"BEST VALIDATION_SCORE (F1): \", f1_score(np.round(np.clip(oof_valid_cb, 0, 10)).astype(int),np.array(y2) ,average = 'macro'))\ndel train_dataset, eval_dataset; gc.collect()\nvalid_predict_CAT = valid_predict_CAT + oof_valid_cb.tolist()\nvalid_target_CAT = valid_target_CAT + np.array(y2).tolist()\n\nprint('============================ start Catboost predict==================================')\na = 2 \ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),4] = (model.predict(test_batch)).astype(np.float16)\n\na = 6\ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),4] = (model.predict(test_batch)).astype(np.float16)","15efc70b":"for j,idx in enumerate(model_batch[4]):\n    if j ==0:\n        batch = idx; a = 500000*(batch-1); b = 500000*batch\n        train_model_split = train2[col].loc[train2.index[a:b]]\n        train_model_target = train2.loc[train2.index[a:b],'open_channels']\n    else:\n        batch = idx; a = 500000*(batch-1); b = 500000*batch\n        train_model_split1 = train2[col].loc[train2.index[a:b]]\n        train_model_target1 = train2.loc[train2.index[a:b],'open_channels']\n        train_model_split = np.concatenate((train_model_split,train_model_split1),axis = 0)\n        train_model_target = np.concatenate((train_model_target,train_model_target1),axis = 0)\n\nx1, x2, y1, y2 = model_selection.train_test_split(train_model_split, train_model_target , test_size=0.2, random_state=49)\n\nprint('============================ start training LGBM ==================================')\n#Fifth model\nimport lightgbm as lgb\nparams = {'learning_rate': 0.1, 'max_depth': -1, 'num_leaves':2**7+1, 'metric': 'rmse', 'random_state': 7, 'n_jobs':-1, 'sample_fraction':0.33} \nmodel = lgb.train(params, lgb.Dataset(x1, y1), 22222,  lgb.Dataset(x2, y2), verbose_eval=100, early_stopping_rounds=250, feval=MacroF1Metric)\n#preds_lgb = (model.predict(test[col], num_iteration=model.best_iteration)).astype(np.float16)\noof_valid_lgb = (model.predict(x2, num_iteration=model.best_iteration)).astype(np.float16)\n\nprint(\"BEST VALIDATION_SCORE (F1): \", f1_score(np.round(np.clip(oof_valid_lgb, 0, 10)).astype(int),np.array(y2) ,average = 'macro'))\nvalid_predict_LGBM = valid_predict_LGBM + oof_valid_lgb.tolist()\nvalid_target_LGBM = valid_target_LGBM + np.array(y2).tolist()\n\nprint('============================ start LGBM predict==================================')\na = 5 \ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),2] = (model.predict(test_batch, num_iteration=model.best_iteration)).astype(np.float16)\n\na = 7 \ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),2] = (model.predict(test_batch, num_iteration=model.best_iteration)).astype(np.float16)\n\n\ngc.collect()\n\nprint('============================ start training XGBBOOST ==================================')\nimport xgboost as xgb\nparams = {'colsample_bytree': 0.375,'learning_rate': 0.1,'max_depth': 10, 'subsample': 1, 'objective':'reg:squarederror',\n          'eval_metric':'rmse', 'n_estimators':22222,   'tree_method':'gpu_hist',}\ntrain_set = xgb.DMatrix(x1, y1)\nval_set = xgb.DMatrix(x2, y2)\nmodel = xgb.train(params, train_set, num_boost_round=2222, evals=[(train_set, 'train'), (val_set, 'val')], \n                         verbose_eval=100, early_stopping_rounds=250)\n#preds_xgb = model.predict(xgb.DMatrix(test[col]), ntree_limit=model.best_ntree_limit)\noof_valid_xgb = (model.predict(xgb.DMatrix(x2), ntree_limit=model.best_ntree_limit)).astype(np.float16)\nprint(\"BEST VALIDATION_SCORE (F1): \", f1_score(np.round(np.clip(oof_valid_xgb, 0, 10)).astype(int),np.array(y2) ,average = 'macro'))\ndel train_set, val_set; gc.collect()\nvalid_predict_XGB = valid_predict_XGB + oof_valid_xgb.tolist()\nvalid_target_XGB = valid_target_XGB + np.array(y2).tolist()\n\nprint('============================ start XGB predict==================================')\na = 5 \ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),3] = (model.predict(xgb.DMatrix(np.array(test_batch)), ntree_limit=model.best_ntree_limit)).astype(np.float16)\n\na = 7 \ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),3] = (model.predict(xgb.DMatrix(np.array(test_batch)), ntree_limit=model.best_ntree_limit)).astype(np.float16)\n\nprint('============================ start training Catboost ==================================')\nfrom catboost import Pool,CatBoostRegressor\nmodel = CatBoostRegressor(task_type = 'GPU', iterations=22222, learning_rate=0.1, random_seed = 7, depth=7, eval_metric='RMSE')\ntrain_dataset = Pool(x1,  y1)          \neval_dataset = Pool(x2,  y2)\nmodel.fit(train_dataset, eval_set=eval_dataset, verbose=100, early_stopping_rounds=250)\n#preds_cb = (model.predict(test[col])).astype(np.float16)\noof_valid_cb = (model.predict(x2)).astype(np.float16)\nprint(\"BEST VALIDATION_SCORE (F1): \", f1_score(np.round(np.clip(oof_valid_cb, 0, 10)).astype(int),np.array(y2) ,average = 'macro'))\ndel train_dataset, eval_dataset; gc.collect()\nvalid_predict_CAT = valid_predict_CAT + oof_valid_cb.tolist()\nvalid_target_CAT = valid_target_CAT + np.array(y2).tolist()\n\nprint('============================ start Catboost predict==================================')\na = 5 \ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),4] = (model.predict(test_batch)).astype(np.float16)\n\na = 7\ntest_batch = test[col].loc[test.index[100000*a:100000*(a+1)]]\nprint ('start test subsample {}'.format(a)+', test_batch shape= ' + str(test_batch.shape))\nsub.iloc[100000*a:100000*(a+1),4] = (model.predict(test_batch)).astype(np.float16)\n","58584a17":"print(\"LGBM BEST VALIDATION_SCORE (F1): \", f1_score(np.round(np.clip(valid_predict_LGBM, 0, 10)).astype(int),np.array(valid_target_LGBM) ,average = 'macro'))\nprint(\"XGB BEST VALIDATION_SCORE (F1): \", f1_score(np.round(np.clip(valid_predict_XGB, 0, 10)).astype(int),np.array(valid_target_XGB) ,average = 'macro'))\nprint(\"XGB BEST VALIDATION_SCORE (F1): \", f1_score(np.round(np.clip(valid_predict_CAT, 0, 10)).astype(int),np.array(valid_target_CAT) ,average = 'macro'))","c8a7df9a":"Ensemble_predict =np.array(valid_predict_LGBM)*0.4+np.array(valid_predict_XGB)*0.4+np.array(valid_predict_CAT)*0.2\nprint(\"Ensemble BEST VALIDATION_SCORE (F1): \", f1_score(np.round(np.clip(Ensemble_predict, 0, 10)).astype(int),np.array(valid_target_XGB) ,average = 'macro'))","af680a4f":"from functools import partial\nimport scipy as sp\nclass OptimizedRounder(object):\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize Quadratic Weighted Kappa (QWK) score\n    # https:\/\/www.kaggle.com\/naveenasaithambi\/optimizedrounder-improved\n    \"\"\"\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        return -f1_score(y, X_p,average = 'macro')\n\n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n\n    def coefficients(self):\n        return self.coef_['x']","6639e2a4":"sub.head()","abf71cf6":"%%time\noptR = OptimizedRounder()\noptR.fit(np.array(Ensemble_predict).reshape(-1,), np.array(valid_target_XGB))\ncoefficients = optR.coefficients()\nprint(coefficients)","23d74baf":"opt_preds = optR.predict(np.array(Ensemble_predict).reshape(-1, ), coefficients)\nprint('f1', metrics.f1_score(np.array(valid_target_XGB), opt_preds, average = 'macro'))","7b9c8447":"predict_logic = sub.LGBM_preds*0.4+sub.XGB_preds*0.4+sub.CAT_preds*0.2\nopt_test_preds = optR.predict(np.array(predict_logic).reshape(-1, ), coefficients)","a0153111":"sub.head()","191b9e0b":"submit = pd.read_csv('..\/input\/liverpool-ion-switching\/sample_submission.csv')\nsubmit.open_channels = opt_test_preds.astype(int)\nsubmit.to_csv('submission_lgb.csv', index=False, float_format='%.4f')","efb0fbec":"def plot_time_channel_data(data_df, title=\"Time variation data\"):\n    plt.figure(figsize=(18,8))\n    plt.plot(data_df[\"time\"], data_df[\"signal\"], color='b', label='Signal')\n    plt.plot(data_df[\"time\"], data_df[\"open_channels\"], color='r', label='Open channel')\n    plt.title(title, fontsize=24)\n    plt.xlabel(\"Time [sec]\", fontsize=20)\n    plt.ylabel(\"Signal & Open channel data\", fontsize=20)\n    plt.legend(loc='upper right')\n    plt.grid(True)\n    plt.show()","2815ab55":"test2['open_channels'] = submit['open_channels'].astype(int)","3e553374":"test2.open_channels.head()","2d35a5d9":"plot_time_channel_data(test2[:100000],'Train data: signal & open channel data (0.7-0.75 sec.)')","929cabff":"plot_time_channel_data(test2[100000:200000],'Train data: signal & open channel data (0.7-0.75 sec.)')","bdbf3894":"plot_time_channel_data(test2[200000:300000],'Train data: signal & open channel data (0.7-0.75 sec.)')","d2e04443":"plot_time_channel_data(test2[300000:400000],'Train data: signal & open channel data (0.7-0.75 sec.)')","30021bc1":"plot_time_channel_data(test2[400000:500000],'Train data: signal & open channel data (0.7-0.75 sec.)')","9da6d0aa":"plot_time_channel_data(test2[500000:600000],'Train data: signal & open channel data (0.7-0.75 sec.)')","983c610e":"plot_time_channel_data(test2[600000:700000],'Train data: signal & open channel data (0.7-0.75 sec.)')","0eff1d4d":"plot_time_channel_data(test2[700000:800000],'Train data: signal & open channel data (0.7-0.75 sec.)')","73f12bf3":"plot_time_channel_data(test2[800000:900000],'Train data: signal & open channel data (0.7-0.75 sec.)')","9fd95794":"plot_time_channel_data(test2[900000:1000000],'Train data: signal & open channel data (0.7-0.75 sec.)')","d2f19e5b":"plot_time_channel_data(test2[1000000:2000000],'Train data: signal & open channel data (0.7-0.75 sec.)')","5a043c52":"plot_time_channel_data(test2,'Train data: signal & open channel data (0.7-0.75 sec.)')","2b8bd57b":"## Model 3 Open Channel","a05c0aed":"## Model 1 Fast Open Channel","922115de":"## Optimize the thresholds","5377b56d":"## Show all test target and signal","9f56091b":"## Model 1 Slow Open Channel","fa956483":"## Create feature","248ebc48":"## Model 10 Open Channel","8c7d622d":"## Reference kernel\n* https:\/\/www.kaggle.com\/cdeotte\/one-feature-model-0-930","582c2e84":"## Model 5 Open Channel","9671d49f":"## Remove Training\/Test Data Drift"}}