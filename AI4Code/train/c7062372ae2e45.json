{"cell_type":{"d205a863":"code","f08305de":"code","9918035e":"code","11c0f9c7":"code","5a705dac":"code","0d585f59":"code","1ed9f2a2":"code","3884ef89":"code","1cfb4add":"code","e9448b5f":"code","9206c232":"code","d2a9a639":"code","7d62a532":"code","caa594f3":"code","92def751":"code","cea535a3":"code","ba9b4a68":"code","91a69507":"code","47697975":"code","40cc7dc0":"code","3319d226":"code","563d486a":"code","8419253f":"code","59c0180a":"code","48044561":"code","dc92b522":"code","30a75eb7":"code","25672613":"code","c7fecf1b":"code","53416ff0":"code","c61e232e":"code","7a825c88":"code","70ac5994":"code","f3a8add3":"code","d1d05003":"code","11ecfb49":"code","d7b41fa9":"code","42d492f1":"code","3f813c61":"code","d3910e71":"code","968e00a0":"code","4cec4669":"code","a70dd2f9":"code","1455acee":"code","5b7140da":"code","e9247162":"code","b72397b5":"code","97a90915":"code","095fe113":"code","d3c9e2ba":"code","efb4497d":"code","34834fe1":"code","80eef41a":"code","628fdbeb":"code","f2d8201a":"code","a0bcf5f6":"code","caddca1a":"code","b9590541":"code","e63a6845":"code","ea355722":"code","089b7884":"code","c9fbf636":"code","9869a0eb":"code","1beb50b7":"code","fe16ea32":"code","15181959":"code","063f699e":"code","aecf4229":"code","263ae522":"code","ac9a63eb":"code","38c2ffe9":"code","f5cd16fb":"code","954ca17a":"code","ac9c8bef":"code","12c59c32":"code","720c028c":"code","c40d9ee8":"code","f8f5e318":"code","3a6e5dcd":"code","82cbb7b6":"code","fcdaa4e5":"code","a1b90a82":"code","a66c4f9b":"code","dc8e9dff":"code","873eb6eb":"code","fa10f092":"code","470c6846":"code","dd51fd7d":"code","eb509fa6":"code","b5f067f6":"code","a229614c":"code","cae8cb09":"code","fca2cc22":"code","6abaf0d4":"code","be091730":"code","5e1830b8":"code","29e3536b":"code","c375aec4":"code","d5694ac4":"code","4270239e":"code","76b078c9":"code","151df9ef":"code","5d7a8ff7":"code","d0ba568d":"code","20d9f1d2":"code","9df30dd9":"code","57f66361":"code","236a535e":"code","3ffa4026":"code","711739c3":"code","e64d3f22":"code","0b4a571e":"code","9f38326a":"code","ee7e91a5":"code","dc8e71e0":"code","3526f913":"code","28908a19":"code","32ff86a5":"code","b7b68dcd":"code","2c1b0351":"code","4a0ce94f":"code","e79aef29":"code","43cae44f":"code","c62fca80":"code","b3113220":"code","2131a7cb":"code","6be7206a":"code","cdd50f01":"code","a8de58bd":"code","45559f44":"code","9d4e7619":"code","046514fa":"code","3fe8dced":"code","ce23e1f9":"code","57b1c5c4":"code","1bab9063":"code","61d2ae72":"markdown","6f7e6ac5":"markdown","81352fc3":"markdown","7cab0540":"markdown","db86e125":"markdown","dbf1d189":"markdown","219e9cce":"markdown","166d10d6":"markdown","4d3fd588":"markdown","2b87c3fd":"markdown","1034fe6b":"markdown","7879c59d":"markdown","3c668d54":"markdown","4a25fbf5":"markdown","f5e616eb":"markdown","7c3818ce":"markdown","712dd72d":"markdown","6b5728e7":"markdown","b9192a3a":"markdown","4d027964":"markdown","09e5f81b":"markdown","b7cde224":"markdown","2501b1e4":"markdown","af34aaf9":"markdown","f7fd3d41":"markdown","62486bea":"markdown","e88c6b26":"markdown","363059db":"markdown","603b3010":"markdown","1e01aa77":"markdown","137debb3":"markdown","9017809e":"markdown","7cee7eac":"markdown","9e46ce9a":"markdown","27704902":"markdown","87291e8a":"markdown","68af5642":"markdown","8d0141f4":"markdown","11f51672":"markdown","d86c8aef":"markdown","dd9c5652":"markdown","ec05c3b1":"markdown","7a036a75":"markdown","092cb186":"markdown","9e438972":"markdown","8cfc261b":"markdown","91eead7c":"markdown","f2e405d7":"markdown","ffcb5556":"markdown","608402b1":"markdown","24de8254":"markdown","b6447513":"markdown","c4177911":"markdown"},"source":{"d205a863":"# for basic operations\nimport numpy as np \nimport pandas as pd \n\n# for visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\n# for modeling \nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.svm import OneClassSVM\n\n# to avoid warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n# for providing path\nimport os\nprint(os.listdir(\"..\/input\"))\n","f08305de":"# reading the data\ndata = pd.read_csv('..\/input\/uci-secom.csv')\n\n# getting the shape of the data\n# we have 1,567 rows and 592 columns\nprint(data.shape)\n","9918035e":"# getting the head of the data\n\ndata.head()","11c0f9c7":"# checking if the dataset contains any NULL values\n# we do have NaN values if we see the output above\n\ndata.isnull().any().any()","5a705dac":"# Replacing all the NaN values with 0 as the values correspond to the test results.\n# since, the values are not present that means the values are not available or calculated\n# Absence of a signal is assumed to be no signal in the dataset\n# so better we not take median or mean and replace them with zeros\n\ndata = data.replace(np.NaN, 0)\n\n# again, checking if there is any NULL values left\ndata.isnull().any().any()","0d585f59":"unique_vals = data['Pass\/Fail'].unique()  # [0, 1, 2]\ntargets = [data.loc[data['Pass\/Fail'] == val] for val in unique_vals]","1ed9f2a2":"fig = plt.figure(figsize=(20,20))\n\nplt.subplot(2, 2, 1)\nfor target in targets:\n    sns.distplot(target['1'], hist=True, rug=True)\nplt.title('First Sensor Measurements', fontsize = 20)\n\nplt.subplot(2, 2, 2)\nfor target in targets:\n    sns.distplot(target['2'], hist=True, rug=True)\nplt.title('Second Sensor Measurements', fontsize = 20)\n\nplt.subplot(2, 2, 3)\nfor target in targets:\n    sns.distplot(target['3'], hist=True, rug=True)\nplt.title('Third Sensor Measurements', fontsize = 20)\n\nplt.subplot(2, 2, 4)\nfor target in targets:\n    sns.distplot(target['4'], hist=True, rug=True)\nplt.title('Fourth Sensor Measurements', fontsize = 20)\n\n#sns.add_legend()\n#plt.legend()\nfig.legend(labels=['Pass','Fail'])\nplt.show()\n","3884ef89":"# pie chart\n# We have highly imbalanced class with only 6.6% failures and 93.4% pass\n\nlabels = ['Pass', 'Fail']\nsize = data['Pass\/Fail'].value_counts()\ncolors = ['blue', 'green']\nexplode = [0, 0.1]\n\nplt.style.use('seaborn-deep')\nplt.rcParams['figure.figsize'] = (8, 8)\nplt.pie(size, labels =labels, colors = colors, explode = explode, autopct = \"%.2f%%\", shadow = True)\nplt.axis('off')\nplt.title('Target: Pass or Fail', fontsize = 20)\nplt.legend()\nplt.show()\ndata['Pass\/Fail'].value_counts().plot(kind=\"bar\");","1cfb4add":"# heatmap to get correlation\n\nplt.rcParams['figure.figsize'] = (18, 18)\nsns.heatmap(data.corr(), cmap = \"YlGnBu\")\nplt.title('Correlation heatmap for the Data', fontsize = 20)","e9448b5f":"#Remove the highly collinear features from data\ndef remove_collinear_features(x, threshold):\n    '''\n    Objective:\n        Remove collinear features in a dataframe with a correlation coefficient\n        greater than the threshold. Removing collinear features can help a model \n        to generalize and improves the interpretability of the model.\n\n    Inputs: \n        x: features dataframe\n        threshold: features with correlations greater than this value are removed\n\n    Output: \n        dataframe that contains only the non-highly-collinear features\n    '''\n\n    # Calculate the correlation matrix\n    corr_matrix = x.corr()\n    iters = range(len(corr_matrix.columns) - 1)\n    drop_cols = []\n\n    # Iterate through the correlation matrix and compare correlations\n    for i in iters:\n        for j in range(i+1):\n            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n            col = item.columns\n            row = item.index\n            val = abs(item.values)\n\n            # If correlation exceeds the threshold\n            if val >= threshold:\n                # Print the correlated features and the correlation value\n                print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n                drop_cols.append(col.values[0])\n\n    # Drop one of each pair of correlated columns\n    drops = set(drop_cols)\n    x = x.drop(columns=drops)\n\n    return x","9206c232":"#Remove columns having more than 70% correlation\n#Both positive and negative correlations are considered here\ndata = remove_collinear_features(data,0.70)","d2a9a639":"# deleting the first column\n\ndata = data.drop(columns = ['Time'], axis = 1)\n\n# checking the shape of the data after deleting a column\ndata.shape","7d62a532":"data.head()","caa594f3":"# separating the dependent and independent data\n\nx = data.iloc[:,:306]\ny = data[\"Pass\/Fail\"]\n\n# getting the shapes of new data sets x and y\nprint(\"shape of x:\", x.shape)\nprint(\"shape of y:\", y.shape)","92def751":"# splitting them into train test and split\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)\n\n# gettiing the shapes\nprint(\"shape of x_train: \", x_train.shape)\nprint(\"shape of x_test: \", x_test.shape)\nprint(\"shape of y_train: \", y_train.shape)\nprint(\"shape of y_test: \", y_test.shape)","cea535a3":"# standardization\n\nfrom sklearn.preprocessing import StandardScaler\n\n# creating a standard scaler\nsc = StandardScaler()\n\n# fitting independent data to the model\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\n","ba9b4a68":"model = XGBClassifier(random_state=1)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)","91a69507":"cm = confusion_matrix(y_test, y_pred)\nplt.rcParams['figure.figsize'] = (5, 5)\nsns.set(style = 'dark', font_scale = 1.4)\nsns.heatmap(cm, annot = True, annot_kws = {\"size\": 15})","47697975":"print(\"Accuracy: \", model.score(x_test,y_test)*100)","40cc7dc0":"model = RandomForestClassifier(n_estimators=100, random_state=1,verbose=0 )\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)","3319d226":"cm = confusion_matrix(y_test, y_pred)\nplt.rcParams['figure.figsize'] = (5, 5)\nsns.set(style = 'dark', font_scale = 1.4)\nsns.heatmap(cm, annot = True, annot_kws = {\"size\": 15})","563d486a":"print(\"Accuracy: \", model.score(x_test,y_test)*100)","8419253f":"lr = LogisticRegression(random_state=1)\nlr.fit(x_train, y_train) \ny_pred = lr.predict(x_test)","59c0180a":"cm = confusion_matrix(y_test, y_pred)\nplt.rcParams['figure.figsize'] = (5, 5)\nsns.set(style = 'dark', font_scale = 1.4)\nsns.heatmap(cm, annot = True, annot_kws = {\"size\": 15})","48044561":"print(\"Accuracy: \", lr.score(x_test,y_test)*100)","dc92b522":"\nlasso = Lasso(alpha=0.1,random_state=1)\nlasso.fit(x_train,y_train)\n#print (\"Lasso model:\", (lasso.coef_))\n\ny_pred = lasso.predict(x_test)\n\n#Convert the sign of the predicted values as the classifier\ny_pred2 = np.sign(y_pred)","30a75eb7":"print(\"Accuracy: \", lasso.score(x_test,y_test)*100)","25672613":"cm = confusion_matrix(y_test, y_pred2)\nsns.heatmap(cm, annot = True, cmap = 'rainbow')","c7fecf1b":"# Under Sampling - Check how many failure observations are there\n# We have 104 such observations\n\nfailed_tests = np.array(data[data['Pass\/Fail'] == 1].index)\nno_failed_tests = len(failed_tests)\n\nprint(no_failed_tests)","53416ff0":"# Check how many pass observations are there\n# We have 1,463 such observations\n\nnormal_indices = data[data['Pass\/Fail'] == -1]\nno_normal_indices = len(normal_indices)\n\nprint(no_normal_indices)","c61e232e":"# Get 104 random observations from the pass class as well\n\nrandom_normal_indices = np.random.choice(no_normal_indices, size = no_failed_tests, replace = True)\nrandom_normal_indices = np.array(random_normal_indices)\n\nprint(len(random_normal_indices))","7a825c88":"#Getting a 50-50 representation from both pass and fail classes\nunder_sample = np.concatenate([failed_tests, random_normal_indices])\nprint(len(under_sample))","70ac5994":"# creating the undersample data\n\nundersample_data = data.iloc[under_sample, :]","f3a8add3":"\n# splitting the undersample dataset into x and y sets\n\nx = undersample_data.iloc[:, undersample_data.columns != 'Pass\/Fail'] \ny = undersample_data.iloc[:, undersample_data.columns == 'Pass\/Fail']\n\nprint(x.shape)\nprint(y.shape)","d1d05003":"from sklearn.model_selection import train_test_split\n\nx_train_us, x_test_us, y_train_us, y_test_us = train_test_split(x, y, test_size = 0.3, random_state = 1)\n\nprint(x_train_us.shape)\nprint(y_train_us.shape)\nprint(x_test_us.shape)\nprint(y_test_us.shape)","11ecfb49":"# standardization\n\nsc = StandardScaler()\nx_train_us = sc.fit_transform(x_train_us)\nx_test_us = sc.transform(x_test_us)","d7b41fa9":"\n\nmodel = XGBClassifier(random_state=1)\n\nmodel.fit(x_train_us, y_train_us)\n\ny_pred = model.predict(x_test_us)","42d492f1":"\n\ncm = confusion_matrix(y_test_us, y_pred)\n\n\nplt.rcParams['figure.figsize'] = (5, 5)\nsns.set(style = 'dark', font_scale = 1.4)\nsns.heatmap(cm, annot = True, annot_kws = {\"size\": 15})\n\n# It is able to predict 26 defected semiconductors among 35 Semi-Conductors","3f813c61":"print(\"Accuracy: \", model.score(x_test,y_test)*100)","d3910e71":"# Applying Grid Search CV to find the best model with the best parameters\n\n\n\nparameters = [{'max_depth' : [1, 2, 3, 4, 5, 6],\n              'cv' : [2,4,6,8,10],\n              'random_state' : [1]}]\n\ngrid_search = GridSearchCV(estimator = model, param_grid = parameters, scoring = 'accuracy',  n_jobs = -1)\n\ngrid_search = grid_search.fit(x_train_us, y_train_us)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_","968e00a0":"print(\"Best Accuracy: \", best_accuracy*100)\nprint(\"Best Parameter: \", best_parameters)","4cec4669":"\n\nweights = (y == 0).sum()\/(1.0*(y == -1).sum())\nmodel = XGBClassifier(max_depth = 1, scale_pos_weights = weights, n_jobs = 4,random_state=1,cv=2)\n\nmodel.fit(x_train_us, y_train_us)\n\ny_pred = model.predict(x_test_us)","a70dd2f9":"print(\"Accuracy: \", model.score(x_test,y_test)*100)","1455acee":"cm = confusion_matrix(y_test_us, y_pred)\n\n\nplt.rcParams['figure.figsize'] = (5, 5)\nsns.set(style = 'dark', font_scale = 1.4)\nsns.heatmap(cm, annot = True, annot_kws = {\"size\": 15})","5b7140da":"# plotting the feature importances\n\ncolors = plt.cm.spring(np.linspace(0, 1, 9))\nxgb.plot_importance(model, height = 1, color = colors, grid = True, importance_type = 'cover', show_values = False)\n\nplt.rcParams['figure.figsize'] = (100, 100)\nplt.xlabel('The F-Score for each features')\nplt.ylabel('Importances')\nplt.show()","e9247162":"\n\nx_resample, y_resample  = SMOTE(random_state=1).fit_sample(x, y.values.ravel())\n\nprint(x_resample.shape)\nprint(y_resample.shape)","b72397b5":"\n\nx_train_os, x_test_os, y_train_os, y_test_os = train_test_split(x, y, test_size = 0.3, random_state = 1)\n\nprint(x_train_os.shape)\nprint(y_train_os.shape)\nprint(x_test_os.shape)\nprint(y_test_os.shape)","97a90915":"# standardization\n\n\n\nsc = StandardScaler()\nx_train_os = sc.fit_transform(x_train_os)\nx_test_os = sc.transform(x_test_os)","095fe113":"import xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\n\nmodel = XGBClassifier(random_state=1)\n\nmodel.fit(x_train_os, y_train_os)\n\ny_pred = model.predict(x_test_os)","d3c9e2ba":"# Applying Grid Search CV to find the best model with the best parameters\n\nfrom sklearn.model_selection import GridSearchCV\n\n# making a parameters list\nparameters = [{'max_depth' : [1, 2, 3, 4, 5, 6],\n              'cv' : [2,4,6,8,10],\n              'random_state' : [1]}]\n\n# making a grid search model\ngrid_search = GridSearchCV(estimator = model, param_grid = parameters, scoring = 'accuracy', n_jobs = -1)\ngrid_search = grid_search.fit(x_train_os, y_train_os)\n\n# getting the results\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\n\n","efb4497d":"print(\"Best Accuracy: \", best_accuracy)\nprint(\"Best Parameter: \", best_parameters)","34834fe1":"\n\nweights = (y == 0).sum()\/(1.0*(y == -1).sum())\nmodel = XGBClassifier(max_depth = 1, scale_pos_weights = weights, n_jobs = 4,random_state=1,cv=2)\n\nmodel.fit(x_train_os, y_train_os)\n\ny_pred = model.predict(x_test_os)\n","80eef41a":"\n\ncm = confusion_matrix(y_test_os, y_pred)\n\n\nplt.rcParams['figure.figsize'] = (5, 5)\nsns.set(style = 'dark', font_scale = 1.4)\nsns.heatmap(cm, annot = True, annot_kws = {\"size\": 15}, cmap = 'spring')","628fdbeb":"print(\"Accuracy: \", model.score(x_test,y_test)*100)","f2d8201a":"model = RandomForestClassifier(n_estimators=100, random_state=1,verbose=0 )\nmodel.fit(x_train_os, y_train_os)\n#scores_prediction = model.decision_function(x_train)\ny_pred = model.predict(x_test_os)\nprint(\"Accuracy: \", model.score(x_test_os,y_test_os)*100)","a0bcf5f6":"# printing the confusion matrix\ncm = confusion_matrix(y_test_os, y_pred)\nsns.heatmap(cm, annot = True, cmap = 'rainbow')","caddca1a":"lr = LogisticRegression(random_state=1)\nlr.fit(x_train_os, y_train_os) \ny_pred = lr.predict(x_test_os)\n\nprint(\"Accuracy: \", lr.score(x_test_os,y_test_os)*100)","b9590541":"cm = confusion_matrix(y_test_os, y_pred)\nsns.heatmap(cm, annot = True, cmap = 'rainbow')","e63a6845":"\nmodel = RandomForestClassifier(n_estimators=100, random_state=1,verbose=0 )\nmodel.fit(x_train_us, y_train_us)\n#scores_prediction = model.decision_function(x_train)\ny_pred = model.predict(x_test_us)","ea355722":"# evaluating the model\n\n# printing the confusion matrix\ncm = confusion_matrix(y_test_us, y_pred)\nsns.heatmap(cm, annot = True, cmap = 'rainbow')","089b7884":"print(\"Accuracy: \", model.score(x_test,y_test)*100)","c9fbf636":"\nlasso = Lasso(alpha=0.1,random_state=1)\nlasso.fit(x_train_us,y_train_us)\n#print (\"Lasso model:\", (lasso.coef_))","9869a0eb":"y_pred = lasso.predict(x_test_us)","1beb50b7":"print(y_pred)","fe16ea32":"print(y_test_us)","15181959":"#Convert the sign of the predicted values as the classifier\ny_pred2 = np.sign(y_pred)","063f699e":"cm = confusion_matrix(y_test_us, y_pred2)\nsns.heatmap(cm, annot = True, cmap = 'rainbow')","aecf4229":"print(\"Accuracy: \", lasso.score(x_test_us,y_test_us)*100)","263ae522":"lr = LogisticRegression(random_state=1)\nlr.fit(x_train_us, y_train_us) ","ac9a63eb":"y_pred = lr.predict(x_test_us)","38c2ffe9":"cm = confusion_matrix(y_test_us, y_pred)\nsns.heatmap(cm, annot = True, cmap = 'rainbow')","f5cd16fb":"print(\"Accuracy: \", lr.score(x_test,y_test)*100)","954ca17a":"\n\nmodel = OneClassSVM(kernel ='rbf', degree=3, gamma=0.1,nu=0.005, max_iter=-1, random_state=1)\n\nmodel.fit(x_train_us, y_train_us)\ny_pred = model.fit_predict(x_test_us)\n","ac9c8bef":"\n# evaluating the model\n# printing the confusion matrix\ncm = confusion_matrix(y_test_us, y_pred)\nsns.heatmap(cm ,annot = True, cmap = 'winter')\n","12c59c32":"#print(\"Accuracy: \", model.score(x_test,y_test)*100)","720c028c":"model = OneClassSVM(kernel ='rbf', degree=3, gamma=0.1,nu=0.005, max_iter=-1, random_state=1)\n\nmodel.fit(x_train_os, y_train_os)\ny_pred = model.fit_predict(x_test_os)","c40d9ee8":"# evaluating the model\n# printing the confusion matrix\ncm = confusion_matrix(y_test_os, y_pred)\nsns.heatmap(cm ,annot = True, cmap = 'winter')","f8f5e318":"#Scaling the data before applying PCA\nfrom scipy.stats import zscore\ndata_new=data.iloc[:,:306].apply(zscore)\ndata_new.head()","3a6e5dcd":"data_new.isnull().any().any()","82cbb7b6":"data_new = data_new.replace(np.NaN, 0)","fcdaa4e5":"data_new.isnull().any().any()","a1b90a82":"# separating the dependent and independent data\n\nx = data_new.iloc[:,:306]\ny = data[\"Pass\/Fail\"]\n\n# getting the shapes of new data sets x and y\nprint(\"shape of x:\", x.shape)\nprint(\"shape of y:\", y.shape)","a66c4f9b":"# PCA\n# Step 1 - Create covariance matrix\n\ncov_matrix = np.cov(x.T)\nprint('Covariance Matrix \\n%s', cov_matrix)","dc8e9dff":"# Step 2- Get eigen values and eigen vector\neig_vals, eig_vecs = np.linalg.eig(cov_matrix)\nprint('Eigen Vectors \\n%s', eig_vecs)\nprint('\\n Eigen Values \\n%s', eig_vals)","873eb6eb":"tot = sum(eig_vals)\nvar_exp = [( i \/tot ) * 100 for i in sorted(eig_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\nprint(\"Cumulative Variance Explained\", cum_var_exp)","fa10f092":"plt.plot(var_exp)","470c6846":"# Ploting \nplt.figure(figsize=(10 , 5))\nplt.bar(range(1, eig_vals.size + 1), var_exp, alpha = 0.5, align = 'center', label = 'Individual explained variance')\nplt.step(range(1, eig_vals.size + 1), cum_var_exp, where='mid', label = 'Cumulative explained variance')\nplt.ylabel('Explained Variance Ratio')\nplt.xlabel('Principal Components')\nplt.legend(loc = 'best')\nplt.tight_layout()\nplt.show()","dd51fd7d":"len(cum_var_exp)","eb509fa6":"# Using scikit learn PCA here. It does all the above steps and maps data to PCA dimensions in one shot\nfrom sklearn.decomposition import PCA\n\n# NOTE - we are generating only 130 PCA dimensions (dimensionality reduction from 306 to 130)\n# For 130 components we are getting approximately 90% of the variance\npca = PCA(n_components=130)\ndata_reduced = pca.fit_transform(x)\ndata_reduced.transpose()","b5f067f6":"pca.components_","a229614c":"df_comp = pd.DataFrame(pca.components_,columns=list(x))\ndf_comp.head()","cae8cb09":"plt.figure(figsize=(12,6))\nsns.heatmap(df_comp,cmap='plasma',)","fca2cc22":"data_reduced.shape","6abaf0d4":"df_red2 = pd.DataFrame(data_reduced)\ndf_red2.head()","be091730":"df_red3 = df_red2.copy()\ndf_red4 = df_red3\ndf_red4[\"Pass\/Fail\"] = data[\"Pass\/Fail\"]","5e1830b8":"df_red4.head()","29e3536b":"df_red4.shape","c375aec4":"#Causing system crash and taking too much time, so commented out\n#sns.pairplot(df_red4,diag_kind='kde')","d5694ac4":"#Sample boxplot shows that there are outliers in the data, let us fix them\ndf_red4.boxplot(column = [df_red4.columns[0],\n                          df_red4.columns[1],\n                          df_red4.columns[2],\n                          df_red4.columns[3], \n                          df_red4.columns[4],\n                          df_red4.columns[5],\n                         ]\n                          , by = 'Pass\/Fail', figsize=(20,20))","4270239e":"#Create a copy of the dataset for maintain data after outlier removal\n#Here after identifying outliers we replace with median\npd_data = df_red4.copy()\n#pd_data.head()\n\n#pd_data2 = pd_data.drop(columns=['name'],axis=1)\n#pd_data2 = pd_data2.apply(replace,axis=1)\nfrom scipy import stats\n\n#Define a function to remove outliers on max side\ndef outlier_removal_max(var):\n    var = np.where(var > var.quantile(0.75)+ stats.iqr(var),var.quantile(0.50),var)\n    return var\n\n#Define a function to remove outliers on min side\ndef outlier_removal_min(var):\n    var = np.where(var < var.quantile(0.25) - stats.iqr(var),var.quantile(0.50),var)\n    return var\n\n#Loop over the columns and remove the outliers on min and max side\nfor column in pd_data:\n    pd_data[column] = outlier_removal_max(pd_data[column])\n    pd_data[column] = outlier_removal_min(pd_data[column])","76b078c9":"#Sample boxplot shows that outliers are fixed, but we are loosing observations belonging to failure \n#class (Pass\/Fail = 1) So we should not remove outliers here \npd_data.boxplot( column =[df_red4.columns[0],\n                          df_red4.columns[1],\n                          df_red4.columns[2],\n                          df_red4.columns[3], \n                          df_red4.columns[4],\n                          df_red4.columns[5],\n                         ],by = 'Pass\/Fail', figsize=(20,20))","151df9ef":"# separating the dependent and independent data\n\nx = df_red4.iloc[:, df_red4.columns != 'Pass\/Fail'] \ny = df_red4.iloc[:, df_red4.columns == 'Pass\/Fail']\n\n# getting the shapes of new data sets x and y\nprint(\"shape of x:\", x.shape)\nprint(\"shape of y:\", y.shape)","5d7a8ff7":"# Under Sampling - Check how many failure observations are there\n# We have 104 such observations\n\nfailed_tests = np.array(df_red4[df_red4['Pass\/Fail'] == 1].index)\nno_failed_tests = len(failed_tests)\n\nprint(no_failed_tests)","d0ba568d":"# Check how many pass observations are there\n# We have 1,463 such observations\n\nnormal_indices = df_red4[df_red4['Pass\/Fail'] == -1]\nno_normal_indices = len(normal_indices)\n\nprint(no_normal_indices)","20d9f1d2":"# Get 104 random observations from the pass class as well\n\nrandom_normal_indices = np.random.choice(no_normal_indices, size = no_failed_tests, replace = True)\nrandom_normal_indices = np.array(random_normal_indices)\n\nprint(len(random_normal_indices))","9df30dd9":"#Getting a 50-50 representation from both pass and fail classes\nunder_sample = np.concatenate([failed_tests, random_normal_indices])\nprint(len(under_sample))","57f66361":"# creating the undersample data\n\nundersample_data = df_red4.iloc[under_sample, :]\n\n# splitting the undersample dataset into x and y sets\n\nx = undersample_data.iloc[:, undersample_data.columns != 'Pass\/Fail'] \ny = undersample_data.iloc[:, undersample_data.columns == 'Pass\/Fail']\n\nprint(x.shape)\nprint(y.shape)","236a535e":"from sklearn.model_selection import train_test_split\n\nx_train_us, x_test_us, y_train_us, y_test_us = train_test_split(x, y, test_size = 0.3, random_state = 1)\n\nprint(x_train_us.shape)\nprint(y_train_us.shape)\nprint(x_test_us.shape)\nprint(y_test_us.shape)","3ffa4026":"# standardization - completed before PCA itself\n\n#sc = StandardScaler()\n#x_train_us = sc.fit_transform(x_train_us)\n#x_test_us = sc.transform(x_test_us)\n","711739c3":"model = XGBClassifier(random_state=1)\n\nmodel.fit(x_train_us, y_train_us)\n\ny_pred = model.predict(x_test_us)\n\ncm = confusion_matrix(y_test_us, y_pred)","e64d3f22":"plt.rcParams['figure.figsize'] = (5, 5)\nsns.set(style = 'dark', font_scale = 1.4)\nsns.heatmap(cm, annot = True, annot_kws = {\"size\": 15})\n\n","0b4a571e":"print(\"Accuracy: \", model.score(x_test_us,y_test_us)*100)","9f38326a":"# Applying Grid Search CV to find the best model with the best parameters\n\n\n\nparameters = [{'max_depth' : [1, 2, 3, 4, 5, 6],\n              'cv' : [2,4,6,8,10],\n             'random_state' : [1]}]\n\ngrid_search = GridSearchCV(estimator = model, param_grid = parameters, scoring = 'accuracy', n_jobs = -1)\n\ngrid_search = grid_search.fit(x_train_us, y_train_us)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\n\nprint(\"Best Accuracy: \", best_accuracy*100)\nprint(\"Best Parameter: \", best_parameters)","ee7e91a5":"weights = (y == 0).sum()\/(1.0*(y == -1).sum())\nmodel = XGBClassifier(max_depth = 1, scale_pos_weights = weights, n_jobs = 4,random_state=1,cv=2)\n\nmodel.fit(x_train_us, y_train_us)\n\ny_pred = model.predict(x_test_us)\n\nprint(\"Accuracy: \", model.score(x_test_us,y_test_us)*100)\n","dc8e71e0":"cm = confusion_matrix(y_test_us, y_pred)\n\n\nplt.rcParams['figure.figsize'] = (5, 5)\nsns.set(style = 'dark', font_scale = 1.4)\nsns.heatmap(cm, annot = True, annot_kws = {\"size\": 15})\n","3526f913":"model = RandomForestClassifier(n_estimators=100, random_state=1,verbose=0 )\nmodel.fit(x_train_us, y_train_us)\n#scores_prediction = model.decision_function(x_train)\ny_pred = model.predict(x_test_us)\n\n# evaluating the model\n\n# printing the confusion matrix\ncm = confusion_matrix(y_test_us, y_pred)\nsns.heatmap(cm, annot = True, cmap = 'rainbow')\n\n","28908a19":"print(\"Accuracy: \", model.score(x_test_us,y_test_us)*100)","32ff86a5":"lr = LogisticRegression(random_state=1)\nlr.fit(x_train_us, y_train_us) \ny_pred = lr.predict(x_test_us)\ncm = confusion_matrix(y_test_us, y_pred)\nsns.heatmap(cm, annot = True, cmap = 'rainbow')\n","b7b68dcd":"print(\"Accuracy: \", lr.score(x_test_us,y_test_us)*100)","2c1b0351":"lasso = Lasso(alpha=0.1,random_state=1)\nlasso.fit(x_train_us,y_train_us)\n#print (\"Lasso model:\", (lasso.coef_))\n\ny_pred = lasso.predict(x_test_us)\n\n#Convert the sign of the predicted values as the classifier\ny_pred2 = np.sign(y_pred)","4a0ce94f":"actual_cost = list(y_test_us)\nactual_cost = np.asarray(actual_cost)\ny_pred_lass = lasso.predict(x_test_us)","e79aef29":"print(\"Accuracy: \", lasso.score(x_test_us, y_test_us)*100)","43cae44f":"\ncm = confusion_matrix(y_test_us, y_pred2)\nsns.heatmap(cm, annot = True, cmap = 'rainbow')","c62fca80":"# defining outlier fraction for Elliptic Envelop, Isolation Forest and Local Outlier Factor\n\nFraud = data[data['Pass\/Fail']==1]\nValid = data[data['Pass\/Fail']==-1]\n\noutlier_fraction = len(Fraud)\/float(len(Valid))\nprint(\"Outlier Fraction :\", outlier_fraction)","b3113220":"from sklearn.covariance import EllipticEnvelope\nmodel = EllipticEnvelope(contamination=outlier_fraction, random_state=1)\nmodel.fit(x_train, y_train)\n#scores_prediction = model.decision_function(x_train)\ny_pred = model.predict(x_test)","2131a7cb":"# printing the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot = True, cmap = 'rainbow')","6be7206a":"from sklearn.ensemble import IsolationForest\n\nmodel = IsolationForest(n_estimators=100, max_samples=len(x_train), \n                                       contamination=outlier_fraction, random_state=1, verbose=0)\nmodel.fit(x_train, y_train)\nscores_prediction = model.decision_function(x_train)\ny_pred = model.predict(x_test)","cdd50f01":"# printing the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot = True, cmap = 'rainbow')","a8de58bd":"from sklearn.neighbors import LocalOutlierFactor\n\n\nmodel = LocalOutlierFactor(n_neighbors=20, algorithm='auto', leaf_size=30, metric='minkowski', p=2, \n                           metric_params=None, contamination=outlier_fraction)\n\nmodel.fit(x_train, y_train)\ny_pred = model.fit_predict(x_test)","45559f44":"# printing the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot = True, cmap = 'rainbow')","9d4e7619":"Recall = np.array([84.4, 87.9, 81.3,9.4,0,0,0])\nlabel = np.array(['Isolation Forest', 'Local Outlier Factor', 'Elliptic Envelop',\n                  'Logistic','XGBoost','Random Forest','Lasso'])\nindices = np.argsort(Recall)\ncolor = plt.cm.rainbow(np.linspace(0, 1, 9))\n\nplt.rcParams['figure.figsize'] = (18, 7)\nplt.bar(range(len(indices)), Recall[indices], color = color)\nplt.xticks(range(len(indices)), label[indices])\nplt.title('Recall Accuracy - Normal Data', fontsize = 30)\nplt.grid()\nplt.tight_layout()\nplt.show()","046514fa":"Recall = np.array([74.3,77.1,71.4,77.1,71.4,17.1])\nlabel = np.array(['XG Boost','XG Boost - Grid Search','Random Forest','Logistic','Lasso','OneClass SVM'])\nindices = np.argsort(Recall)\ncolor = plt.cm.rainbow(np.linspace(0, 1, 9))\n\nplt.rcParams['figure.figsize'] = (18, 7)\nplt.bar(range(len(indices)), Recall[indices], color = color)\nplt.xticks(range(len(indices)), label[indices])\nplt.title('Recall Accuracy - Undersampled Data', fontsize = 30)\nplt.grid()\nplt.tight_layout()\nplt.show()","3fe8dced":"Recall = np.array([74.3,77.1,71.4,77.1,71.4,17.1])\nlabel = np.array(['XG Boost','XG Boost - Grid Search','Random Forest','Logistic','Lasso','OneClass SVM'])\nindices = np.argsort(Recall)\ncolor = plt.cm.rainbow(np.linspace(0, 1, 9))\n\nplt.rcParams['figure.figsize'] = (18, 7)\nplt.bar(range(len(indices)), Recall[indices], color = color)\nplt.xticks(range(len(indices)), label[indices])\nplt.title('Recall Accuracy - Oversampled Data', fontsize = 30)\nplt.grid()\nplt.tight_layout()\nplt.show()","ce23e1f9":"Recall = np.array([54.5,63.6,48.5,69.7,51.5])\nlabel = np.array(['XG Boost','XG Boost - Grid Search','Random Forest','Logistic','Lasso'])\nindices = np.argsort(Recall)\ncolor = plt.cm.rainbow(np.linspace(0, 1, 9))\n\nplt.rcParams['figure.figsize'] = (18, 7)\nplt.bar(range(len(indices)), Recall[indices], color = color)\nplt.xticks(range(len(indices)), label[indices])\nplt.title('Recall Accuracy - PCA Data', fontsize = 30)\nplt.grid()\nplt.tight_layout()\nplt.show()","57b1c5c4":"# plotting the feature importances\n\nfrom numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom matplotlib import pyplot\npyplot.rcParams.update({'font.size': 22})\nfrom matplotlib.pyplot import figure\nfigure(num=None, figsize=(100, 100), dpi=80, facecolor='w', edgecolor='k')\n# fit model no training data\nmodel = XGBClassifier()\nmodel.fit(x_train_us, y_train_us)\n# plot feature importance\nplot_importance(model)\npyplot.show()","1bab9063":"featureImp = []\nfor feat, importance in zip(data.columns, model.feature_importances_):  \n    temp = [feat, importance*100]\n    featureImp.append(temp)\n\nfT_df = pd.DataFrame(featureImp, columns = ['Feature', 'Importance'])\nprint (fT_df.sort_values('Importance', ascending = False))","61d2ae72":"## Comparison of All Algorithms' Recall Rate","6f7e6ac5":"## Confusion matrix XG Boost - Grid Search (Undersample)","81352fc3":"## Lasso (Normal Data)","7cab0540":"## Reading the Data and understanding the attributes","db86e125":"#### In the failure class we have only 1 observation classified correctly for XGBoost but still has 94.5% accuracy as we predicted correctly on the observations that passed","dbf1d189":"## Confusion matrix for Lasso (Undersampled)","219e9cce":"## Confusion Matrix after OverSampling with XgBoost","166d10d6":"## Confusion Matrix for One Class SVM","4d3fd588":"## XG Boost - PCA - Grid Search - Undersampled","2b87c3fd":"#### In the failure class we have 4 observation classified correctly for logistic regression and 88% accuracy as we predicted correctly on the observations that passed. So even though this model has lesser accuracy it is preferable over previous models as at least it is classifying more observations in the failure class correctly","1034fe6b":"## Confusion Matrix for Lasso - PCA - Undersampled","7879c59d":"## Confusion matrix for OneClassSVM - (Oversampled)","3c668d54":"![image.png](attachment:image.png)","4a25fbf5":"## Random Forest Classifier (Normal Data)","f5e616eb":"## OneClassSVM - (Oversampled) ","7c3818ce":"## Using Lasso (Undersampled)","712dd72d":"#### In the failure class we have no observation classified correctly for Random Forest but still has 94.5% accuracy as we predicted correctly on the observations that passed","6b5728e7":"## Confusion Matrix for Random Forest (Undersampled)","b9192a3a":"## Logistic Regression (Undersampled)","4d027964":"## Random Forest (Oversampling)","09e5f81b":"## Isolation Forest technique","b7cde224":"## One Class SVM (Undersampled)","2501b1e4":"> In one-class SVM, the support vector model is trained on data that has only one class, which is the \u201cnormal\u201d class. It infers the properties of normal cases and from these properties can predict which examples are unlike the normal examples. This is useful for anomaly detection because the scarcity of training examples is what defines anomalies: that is, typically there are very few examples of the network intrusion, fraud, or other anomalous behavior.","af34aaf9":"## XGBoost - PCA (undersampled)","f7fd3d41":"## Data Visualization","62486bea":"## Lasso - PCA - Undersampled","e88c6b26":"## Logistic Regression - PCA - Undersampled","363059db":"## Logistic regression (Oversampled) - Confusion matrix","603b3010":"**Most Important Features of the Model**","1e01aa77":"## Confusion matrix for XG Boost - PCA - (Undersampled)","137debb3":"## Elliptic Envelop technique","9017809e":"## Confusion Matrix after UnderSampling with XgBoost","7cee7eac":"\n# Product entity yield type prediction\n\n## Domain\n### Semiconductor manufacturing process\n## Business Context\n#### A complex modern semiconductor manufacturing process is normally under constant\n#### surveillance via the monitoring of signals\/variables collected from sensors and or\n#### process measurement points. However, not all of these signals are equally valuable in\n#### a specific monitoring system.\n#### The measured signals contain a combination of useful information, irrelevant\n#### information as well as noise. Engineers typically have a much larger number of signals\n#### than are actually required. If we consider each type of signal as a feature, then feature\n#### selection may be applied to identify the most relevant signals. The Process Engineers\n#### may then use these signals to determine key factors contributing to yield excursions\n#### downstream in the process. This will enable an increase in process throughput,\n#### decreased time to learning and reduce the per unit production costs.\n#### These signals can be used as features to predict the yield type. And by analyzing and\n#### trying out different combinations of features, essential signals that are impacting the\n#### yield type can be identified.\n\n## Objective\n#### We will build a classifier to predict the Pass\/Fail yield of a particular process entity and\n#### analyze whether all the features are required to build the model or not.\n\n## Dataset description \n\n#### sensor-data.csv : (1567, 592)\n#### The data consists of 1567 examples each with 591 features.\n#### The dataset presented in this case represents a selection of such features where each\n#### example represents a single production entity with associated measured\n#### features and the labels represent a simple pass\/fail yield for in house line\n#### testing. Target column \u201c \u20131\u201d corresponds to a pass and \u201c1\u201d corresponds to a fail and\n#### the data time stamp is for that specific test point.\n\n## Steps\n1. Import the necessary liberraries and read the provided CSV as a dataframe and\nperform the below steps. \na. Check a few observations and shape of the dataframe\nb. Check for missing values. Impute the missing values if there is any\nc. Univariate analysis - check frequency count of target column and\ndistribution of the first few features (sensors)\nd. Perform bivariate analysis and check for the correlation\ne. Drop irrelevant columns\n2. Standardize the data \n3. Segregate the dependent column (\"Pass\/Fail\") from the data frame. And split\nthe dataset into training and testing set ( 70:30 split) \n4. Build a logistic regression, random forest, and xgboost classifier model and print\nconfusion matrix for the test data \n5. Apply sampling techniques to handle the imbalanced classes \n6. Build a logistic regression, random forest, and xgboost classifier model after resampling the data and print the confusion matrix for the test data \n7. Apply Grid Search CV to get the best hyper parameters for any one of the above\nmodel \n8. Build a classifier model using the above best hyper parameters and check the\naccuracy and confusion matrix \n9. Report feature importance and mention your comments \n\n10. Report your findings and inferences \nFurther Questions ( Optional) -\n1. Check for outliers and impute them as required.\n2. Apply PCA to get rid of redundant features and reduce dimension of the data\n3. Try cross validation techniques to get better results\n4. Try OneCLassSVM model to get better recall\n\n### Learning Outcomes\n* Feature Importance\n* Sampling\n* SMOTE\n* Grid Search\n* Random Forest\n* Exploratory Data Analysis\n* Logistic Regression","9e46ce9a":"## Import Some Basic Libraries","27704902":"## XGBoost Algorithm (Normal data)","87291e8a":"## Using PCA for demensionality reduction","68af5642":"> **Local Outlier Factor Classifier,** Succesfully detected 11 defected items out of 13, that makes an impeccable accuracy of 85% Recall Accuracy. This algorithm would have been worked even better if the data was a little bigger with more instances of defected items.","8d0141f4":"## UnderSampling of the Dataset","11f51672":"## Grid Search - XG Boost (Undersampling)","d86c8aef":"## Data Cleaning\/Preprocessing","dd9c5652":"## Random Forest - PCA - Undersampled","ec05c3b1":"## Logistic regression - (Oversampled)","7a036a75":"## Random Forest - (Undersampled)","092cb186":"## Confusion matrix for Logistic Regression (Undersampled)","9e438972":"## Confusion matrix of XG Boost - Grid Search - PCA - Undersampled","8cfc261b":"## Over-Sampling with SMOTE","91eead7c":"## Local Outlier Factor Technique","f2e405d7":"![image.png](attachment:image.png)","ffcb5556":"## Xg-Boost Classifier (Undersampling)","608402b1":"## We have tried multiple models Logistic Regression, Random Forest, XG Boost (with and without Grid Search),OneClassSVM, Elliptic Envelop, Isolation Forest and Local Outlier Factor for the imbalanced classes\n## Across methods OneClassSVM performed the worst while Local Outlier Factor performed the best in terms of recall accuracy\n## We saw that for imbalanced classes accuracy and recall are invertially proportional to each other. Better recall models have lower accuracy and vice versa.\n## We have tried two sampling techniques -first one using SMOTE (oversampling) and second one \n## using random based method (undersampling), Oversampling gave better results than undersampling in \n## terms of accuracy. Recall score was similar for both undersampling and oversampling.\n## We did Z score scaling on both the datasets and took PCA with n_components as 130 (90% variance coverage). However PCA did not improve either accuracy or recall probably as we were loosing information due to dropping dimensions.\n## We tried K-fold cross validation within XG Boost itself with bext value as 2 for all the models\n## Using feature importance, we found that 72, 74 and 45 are the top three important \n## features. The best recall value at 88% was for Local Outlier Factor algorithm, the best part was no sampling was required as the algorithm took care of sampling as well as outliers. \n## classifier on the failure observations","24de8254":"## Random Forest - (Oversampled) - Confusion Matrix","b6447513":"## Xg-Boost Classifier - Grid Search (Oversampling)","c4177911":"## Logistic Regression (Normal Data)"}}