{"cell_type":{"c4c4a4bd":"code","d0bfce67":"code","cce2ad16":"code","6374ff8b":"code","520a5476":"code","4fe0bc4f":"code","b47e8c5a":"code","d90e246a":"code","b974fed9":"code","93dade39":"code","eb8d9b54":"code","231d788f":"code","95d9c06e":"code","a19e7327":"code","cb631ec2":"code","5494b9fa":"code","634d268f":"code","c98ddf23":"code","ae09e958":"code","a66f2b82":"code","f2e4075b":"code","091dbbb0":"code","3982b744":"code","e6df0973":"code","f09f35c9":"code","beeaed23":"code","dcccd3cf":"code","e847417d":"code","1ff80363":"code","9af195cf":"code","8fd63009":"code","91785021":"code","e709aeb5":"code","586f2ba0":"code","b9285047":"code","f1d53648":"code","db4fe8bc":"code","f4ae5a5e":"code","d3e0510a":"code","dc549860":"code","ffcf7224":"code","57a13d46":"code","5382dea8":"code","e40a28f6":"code","38d7b27c":"code","fec28f8b":"code","26f13849":"code","c1db0d31":"code","9f002150":"code","ceb95e3a":"code","34cfa5b5":"code","b1e74a45":"code","4395bd27":"code","544201b8":"code","b691b6fc":"code","2aa97155":"code","2ca3f632":"code","db2596f0":"code","098f1a6f":"code","d8128342":"code","433be96f":"code","097e2cc3":"code","e1cde7b7":"code","691de752":"code","80c54743":"code","7f3e5a14":"code","5c5de1b8":"code","9a96563c":"code","fd923d68":"code","23ff368b":"code","812e2b31":"markdown","48e84cc5":"markdown","90d86bda":"markdown"},"source":{"c4c4a4bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport time\nfrom datetime import datetime\nfrom scipy import integrate, optimize\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ML libraries\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Ridge, RidgeCV\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d0bfce67":"counntries_df = pd.read_csv(\"\/kaggle\/input\/countriesdata\/enriched_covid_19.csv\")","cce2ad16":"train_df = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-3\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-3\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-3\/submission.csv\")","6374ff8b":"train_df['Date'].max()","520a5476":"train_df.shape\n#train_df.info()","4fe0bc4f":"train_df[\"key\"]=train_df[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\ntest_df[\"key\"]=test_df[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\n\n#test_new=pd.merge(test,train, how=\"left\", left_on=[\"key\",\"Date\"], right_on=[\"key\",\"Date\"] )\n#train.to_csv(directory + \"transfomed.csv\")\n\ntarget1=\"ConfirmedCases\"\ntarget2=\"Fatalities\"\nkey=\"key\"\ndef rate(frame, key, target, new_target_name=\"rate\"):\n   \n    corrections = 0\n    group_keys = frame[ key].values.tolist()\n    target = frame[target].values.tolist()\n    rate=[1.0 for k in range (len(target))]\n\n    for i in range(1, len(group_keys) - 1):\n        previous_group = group_keys[i - 1]\n        current_group = group_keys[i]\n\n        previous_value = target[i - 1]\n        current_value = target[i]\n         \n        if current_group == previous_group:\n                if previous_value!=0.0:\n                     rate[i]=current_value\/previous_value\n\n                 \n        rate[i] =max(1,rate[i] )#correct negative values\n\n    frame[new_target_name] = np.array(rate)","b47e8c5a":"dates_overlap = ['2020-03-26', '2020-03-27', '2020-03-28', '2020-03-29', '2020-03-30', '2020-03-31','2020-04-01', '2020-04-02', '2020-04-03']\ntrain2 = train_df.loc[~train_df['Date'].isin(dates_overlap)]\nall_data_train_test = pd.concat([train2, test_df], axis = 0, sort=False)","d90e246a":"all_data_train_test=train_df\n","b974fed9":"all_data_train_test['Province_State'].fillna('NONE', inplace=True)\nall_data_train_test['ConfirmedCases'].fillna(0, inplace=True)\nall_data_train_test['Fatalities'].fillna(0, inplace=True)\nall_data_train_test['Id'].fillna(-1, inplace=True)\n#all_data_train_test['ForecastId'].fillna(-1, inplace=True)","93dade39":"all_data_train_test['mean_rate_case_last7']=1\nall_data_train_test['mean_rate_fat_last7']=1\nall_data_train_test['max_rate_case']=1\nall_data_train_test['min_rate_case']=1\nall_data_train_test['std_rate_case']=1\nall_data_train_test['mode_rate_case']=1\nall_data_train_test['max_rate_fat']=1\nall_data_train_test['min_rate_fat']=1\nall_data_train_test['std_rate_fat']=1\nall_data_train_test['mode_rate_fat']=1","eb8d9b54":"rate(all_data_train_test, key, target1, new_target_name=\"rate_\" +target1)\nrate(all_data_train_test, key, target2, new_target_name=\"rate_\" +target2)\nall_data_train_test.head()","231d788f":"all_data_train_test[all_data_train_test['Country_Region']=='Italy'].tail(10)","95d9c06e":"def infection_from_first_confirmed_cases(train_df,country):\n    \n    confirmed_cases = train_df[(train_df['Country_Region']==country_dict[country]) & train_df['ConfirmedCases']!=0].groupby(['Date']).agg({'ConfirmedCases':['sum']})\n    fatalities_cases = train_df[(train_df['Country_Region']==country_dict[country]) & train_df['ConfirmedCases']!=0].groupby(['Date']).agg({'Fatalities':['sum']})\n    total_cases = confirmed_cases.join(fatalities_cases)\n    country_cases = [i for i in total_cases.ConfirmedCases['sum'].values]\n    country_cases_filter = country_cases[0:70] \n       \n    return country_cases_filter ","a19e7327":"#get the mean foe last seven vlaues for fatalities and confirmed cases\ngroup_keys = all_data_train_test['key'].values.tolist()\nfor k in group_keys:\n    all_data_train_test['mean_rate_case_last7'][all_data_train_test['key']==k]=all_data_train_test['rate_ConfirmedCases'][all_data_train_test['key']==k].tail(7).mean()\n    all_data_train_test['mean_rate_fat_last7'][all_data_train_test['key']==k]=all_data_train_test['rate_Fatalities'][all_data_train_test['key']==k].tail(7).mean()\n    all_data_train_test['std_rate_case'][all_data_train_test['key']==k]=all_data_train_test['rate_ConfirmedCases'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)].std()\n    all_data_train_test['max_rate_case'][all_data_train_test['key']==k]=all_data_train_test['rate_ConfirmedCases'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)].max()\n    all_data_train_test['min_rate_case'][all_data_train_test['key']==k]=all_data_train_test['rate_ConfirmedCases'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)].min()\n    all_data_train_test['mode_rate_case'][all_data_train_test['key']==k]=all_data_train_test['rate_ConfirmedCases'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)].mode()\n    all_data_train_test['std_rate_fat'][all_data_train_test['key']==k]=all_data_train_test['rate_Fatalities'][(all_data_train_test['key']==k) & (all_data_train_test['Fatalities']!=0)].std()\n    all_data_train_test['max_rate_fat'][all_data_train_test['key']==k]=all_data_train_test['rate_Fatalities'][(all_data_train_test['key']==k) & (all_data_train_test['Fatalities']!=0)].max()\n    all_data_train_test['min_rate_fat'][all_data_train_test['key']==k]=all_data_train_test['rate_Fatalities'][(all_data_train_test['key']==k) & (all_data_train_test['Fatalities']!=0)].min()\n    all_data_train_test['mode_rate_fat'][all_data_train_test['key']==k]=all_data_train_test['rate_Fatalities'][(all_data_train_test['key']==k) & (all_data_train_test['Fatalities']!=0)].mode()\n   # all_data_train_test['mean_rate_fat'][all_data_train_test['key']==k]=all_data_train_test['mean_rate_fat'][all_data_train_test['key'].tail(5).mean()","cb631ec2":"all_data_train_test[all_data_train_test['Country_Region']=='Qatar'].tail(40)","5494b9fa":"all_data_train_test","634d268f":"counntries_df.rename(columns={'Country\/Region':'Country_Region','Province\/State':'Province_State'},inplace = True)","c98ddf23":"counntries_df.columns","ae09e958":"contries_data_columns=['Province_State', 'Country_Region', 'Lat', 'Long', 'Date',\n       'ConfirmedCases', 'Fatalities', 'age_0-4', 'age_5-9', 'age_10-14',\n       'age_15-19', 'age_20-24', 'age_25-29', 'age_30-34', 'age_35-39',\n       'age_40-44', 'age_45-49', 'age_50-54', 'age_55-59', 'age_60-64',\n       'age_65-69', 'age_70-74', 'age_75-79', 'age_80-84', 'age_85-89',\n       'age_90-94', 'age_95-99', 'age_100+', 'total_pop', 'smokers_perc',\n       'density', 'urbanpop', 'hospibed', 'lung', 'femalelung', 'malelung',\n       'restrictions', 'quarantine', 'schools']","a66f2b82":"grouped_countries=counntries_df.groupby(['Country_Region'], as_index=False).agg({\"Lat\": \"max\",\"Long\": \"max\",'total_pop': \"max\",\n                                                                                         'smokers_perc': \"max\",'density': \"max\",'urbanpop': \"max\",\n                                                                                         'hospibed': \"max\",'lung': \"max\",'femalelung': \"max\",\n                                                                                         'malelung': \"max\",'restrictions': \"max\",'quarantine': \"max\",\n                                                                                         'schools': \"max\",'age_0-4': \"max\",'age_100+': \"max\",'age_5-9': \"max\",\n                                                                                         'age_95-99': \"max\",'age_90-94': \"max\",'age_85-89': \"max\",\n                                                                                         'age_80-84': \"max\",'age_75-79': \"max\",'age_70-74': \"max\",\n                                                                                         'age_65-69': \"max\",'age_60-64': \"max\",'age_55-59': \"max\",\n                                                                                         'age_50-54': \"max\",'age_45-49': \"max\",'age_40-44': \"max\",\n                                                                                         'age_35-39': \"max\",'age_30-34': \"max\",'age_25-29': \"max\",\n                                                                                         'age_20-24': \"max\",'age_15-19': \"max\",'age_10-14': \"max\",\n                                                                                         })","f2e4075b":"grouped_merged_train_df=merged_train_df.groupby(['key'], as_index=False).agg({\"rate_ConfirmedCases\": \"mean\",\"rate_Fatalities\": \"mean\"})","091dbbb0":"grouped_merged_train_df[grouped_merged_train_df['key']=='nan_Italy']","3982b744":"merged_train_df = pd.merge(all_data_train_test,\n                 grouped_countries[['Lat', 'Long','Country_Region',\n        'age_0-4', 'age_5-9', 'age_10-14',\n       'age_15-19', 'age_20-24', 'age_25-29', 'age_30-34', 'age_35-39',\n       'age_40-44', 'age_45-49', 'age_50-54', 'age_55-59', 'age_60-64',\n       'age_65-69', 'age_70-74', 'age_75-79', 'age_80-84', 'age_85-89',\n       'age_90-94', 'age_95-99', 'age_100+', 'total_pop', 'smokers_perc',\n       'density', 'urbanpop', 'hospibed', 'lung', 'femalelung', 'malelung',\n       'restrictions', 'quarantine', 'schools']],\n                 on='Country_Region',how='left')","e6df0973":"merged_train_df.shape","f09f35c9":"merged_train_df","beeaed23":"merged_train_df[merged_train_df.isnull().any(axis=1)]","dcccd3cf":"merged_test_df[merged_train_df.isnull().any(axis=1)]","e847417d":"train_df[train_df.isnull().any(axis=1)]","1ff80363":"merged_train_df['Province_State'].fillna('NONE', inplace=True)","9af195cf":"merged_train_df.fillna(0, inplace=True)","8fd63009":"merged_train_df.info()","91785021":"number_c = merged_train_df['Country_Region']\ncountries = (merged_train_df['Country_Region'])\ncountry_dict = dict(zip(countries, number_c)) \ncountry_dict","e709aeb5":"def countryplot(train_df,country):\n    \n    confirmed_total = train_df[train_df['Country_Region']==country_dict[country]].groupby(['Date']).agg({'ConfirmedCases':['sum']})\n    fatalities_total = train_df[train_df['Country_Region']==country_dict[country]].groupby(['Date']).agg({'Fatalities':['sum']})\n    total = confirmed_total.join(fatalities_total)\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17,7))\n    total.plot(ax=ax1)\n    ax1.set_title(\"Global confirmed cases\", size=13,)\n    ax1.set_ylabel(\"Number of cases\", size=13)\n    ax1.set_xlabel(\"Date\", size=13)\n    fatalities_total.plot(ax=ax2, color='red')\n    ax2.set_title(\"Global deceased cases\", size=13)\n    ax2.set_ylabel(\"Number of cases\", size=13)\n    ax2.set_xlabel(\"Date\", size=13)","586f2ba0":"country='US'\ncountryplot(train_df,country)","b9285047":"def infection_from_first_confirmed_cases(train_df,country):\n    \n    confirmed_cases = train_df[(train_df['Country_Region']==country_dict[country]) & train_df['ConfirmedCases']!=0].groupby(['Date']).agg({'ConfirmedCases':['sum']})\n    fatalities_cases = train_df[(train_df['Country_Region']==country_dict[country]) & train_df['ConfirmedCases']!=0].groupby(['Date']).agg({'Fatalities':['sum']})\n    total_cases = confirmed_cases.join(fatalities_cases)\n    country_cases = [i for i in total_cases.ConfirmedCases['sum'].values]\n    country_cases_filter = country_cases[0:70] \n       \n    return country_cases_filter  ","f1d53648":"plt.figure(figsize=(12,6))\nplt.plot(infection_from_first_confirmed_cases(train_df,'Italy'))\nplt.plot(infection_from_first_confirmed_cases(train_df,'Spain'))\nplt.plot(infection_from_first_confirmed_cases(train_df,'Singapore'))\nplt.plot(infection_from_first_confirmed_cases(train_df,'United Kingdom'))\nplt.plot(infection_from_first_confirmed_cases(train_df,'US'))\nplt.legend([\"Italy\", \"Spain\", \"UK\", \"Singapore\",'U.S.A'], loc='upper left')\nplt.title(\"COVID-19 infections from the first confirmed case\", size=15)\nplt.xlabel(\"Days\", size=13)\nplt.ylabel(\"Infected cases\", size=13)\nplt.ylim(0, 80000)\nplt.show()","db4fe8bc":"merged_train_df.columns","f4ae5a5e":"merged_train_df['Date_num'] = pd.to_datetime(merged_train_df['Date'])\nmerged_train_df['Date_num'] = merged_train_df['Date_num'].dt.strftime(\"%m%d\")\nmerged_train_df['Date_num']","d3e0510a":"merged_test_df['Date_num'] = pd.to_datetime(merged_test_df['Date'])\nmerged_test_df['Date_num'] = merged_test_df['Date_num'].dt.strftime(\"%m%d\")\nmerged_test_df['Date_num']","dc549860":"features_selector = ['Lat', 'Long', 'age_0-4', 'age_5-9', 'age_10-14',\n       'age_15-19', 'age_20-24', 'age_25-29', 'age_30-34', 'age_35-39',\n       'age_40-44', 'age_45-49', 'age_50-54', 'age_55-59', 'age_60-64',\n       'age_65-69', 'age_70-74', 'age_75-79', 'age_80-84', 'age_85-89',\n       'age_90-94', 'age_95-99', 'age_100+', 'total_pop', 'smokers_perc',\n       'density', 'urbanpop', 'hospibed', 'lung', 'femalelung', 'malelung',\n       'restrictions', 'quarantine', 'schools','Date_num']","ffcf7224":"from sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\nmerged_train_df[['Lat', 'Long', 'age_0-4', 'age_5-9', 'age_10-14',\n       'age_15-19', 'age_20-24', 'age_25-29', 'age_30-34', 'age_35-39',\n       'age_40-44', 'age_45-49', 'age_50-54', 'age_55-59', 'age_60-64',\n       'age_65-69', 'age_70-74', 'age_75-79', 'age_80-84', 'age_85-89',\n       'age_90-94', 'age_95-99', 'age_100+', 'total_pop', 'smokers_perc',\n       'density', 'urbanpop', 'hospibed', 'lung', 'femalelung', 'malelung',\n       'restrictions', 'quarantine', 'schools']] = mms.fit_transform(merged_train_df[['Lat', 'Long', 'age_0-4', 'age_5-9', 'age_10-14',\n       'age_15-19', 'age_20-24', 'age_25-29', 'age_30-34', 'age_35-39',\n       'age_40-44', 'age_45-49', 'age_50-54', 'age_55-59', 'age_60-64',\n       'age_65-69', 'age_70-74', 'age_75-79', 'age_80-84', 'age_85-89',\n       'age_90-94', 'age_95-99', 'age_100+', 'total_pop', 'smokers_perc',\n       'density', 'urbanpop', 'hospibed', 'lung', 'femalelung', 'malelung',\n       'restrictions', 'quarantine', 'schools']])\n","57a13d46":"from sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\nmerged_test_df[['Lat', 'Long', 'age_0-4', 'age_5-9', 'age_10-14',\n       'age_15-19', 'age_20-24', 'age_25-29', 'age_30-34', 'age_35-39',\n       'age_40-44', 'age_45-49', 'age_50-54', 'age_55-59', 'age_60-64',\n       'age_65-69', 'age_70-74', 'age_75-79', 'age_80-84', 'age_85-89',\n       'age_90-94', 'age_95-99', 'age_100+', 'total_pop', 'smokers_perc',\n       'density', 'urbanpop', 'hospibed', 'lung', 'femalelung', 'malelung',\n       'restrictions', 'quarantine', 'schools']] = mms.fit_transform(merged_test_df[['Lat', 'Long', 'age_0-4', 'age_5-9', 'age_10-14',\n       'age_15-19', 'age_20-24', 'age_25-29', 'age_30-34', 'age_35-39',\n       'age_40-44', 'age_45-49', 'age_50-54', 'age_55-59', 'age_60-64',\n       'age_65-69', 'age_70-74', 'age_75-79', 'age_80-84', 'age_85-89',\n       'age_90-94', 'age_95-99', 'age_100+', 'total_pop', 'smokers_perc',\n       'density', 'urbanpop', 'hospibed', 'lung', 'femalelung', 'malelung',\n       'restrictions', 'quarantine', 'schools']])","5382dea8":"merged_train_df","e40a28f6":"merged_test_df.fillna(0, inplace=True)","38d7b27c":"merged_train_df['Date_num']=merged_train_df['Date_num'].astype('int')\nmerged_test_df['Date_num']=merged_test_df['Date_num'].astype('int')\nmerged_train_df.info()","fec28f8b":"merged_train_df_sele=merged_train_df[features_selector]\nmerged_test_df_sele=merged_test_df[features_selector]\nx_train = merged_train_df\nx_test = merged_test_df\n\n\ncountry_list = merged_train_df['Country_Region'].unique()\n\n\n# %% [code] {\"scrolled\":true}\nfrom warnings import filterwarnings\n\nfilterwarnings('ignore')\n\nfrom sklearn import preprocessing\n\nfrom xgboost import XGBRegressor\n\nencoder = preprocessing.LabelEncoder()\n\nsub_xgb = []\nfor country in country_list:\n    province_list = x_train.loc[x_train['Country_Region'] == country].Province_State.unique()\n    for province in province_list:\n        X_train = x_train.loc[(x_train['Country_Region'] == country) & (x_train['Province_State'] == province),['Lat', 'Long', 'age_0-4', 'age_5-9', 'age_10-14',\n       'age_15-19', 'age_20-24', 'age_25-29', 'age_30-34', 'age_35-39',\n       'age_40-44', 'age_45-49', 'age_50-54', 'age_55-59', 'age_60-64',\n       'age_65-69', 'age_70-74', 'age_75-79', 'age_80-84', 'age_85-89',\n       'age_90-94', 'age_95-99', 'age_100+', 'total_pop', 'smokers_perc',\n       'density', 'urbanpop', 'hospibed', 'lung', 'femalelung', 'malelung',\n       'restrictions', 'quarantine', 'schools','Date_num']]\n        Y_train_c =x_train.loc[(x_train['Country_Region'] == country) & (x_train['Province_State'] == province),['ConfirmedCases']]\n        Y_train_f = x_train.loc[(x_train['Country_Region'] == country) & (x_train['Province_State'] == province),['Fatalities']]\n        X_test = x_test.loc[(x_test['Country_Region'] == country) & (x_test['Province_State'] == province), ['Lat', 'Long', 'age_0-4', 'age_5-9', 'age_10-14',\n       'age_15-19', 'age_20-24', 'age_25-29', 'age_30-34', 'age_35-39',\n       'age_40-44', 'age_45-49', 'age_50-54', 'age_55-59', 'age_60-64',\n       'age_65-69', 'age_70-74', 'age_75-79', 'age_80-84', 'age_85-89',\n       'age_90-94', 'age_95-99', 'age_100+', 'total_pop', 'smokers_perc',\n       'density', 'urbanpop', 'hospibed', 'lung', 'femalelung', 'malelung',\n       'restrictions', 'quarantine', 'schools','Date_num']]\n        X_forecastId = x_test.loc[(x_test['Country_Region'] == country) & (x_test['Province_State'] == province), ['ForecastId']]\n        X_forecastId = X_forecastId.values.tolist()\n        X_forecastId = [v[0] for v in X_forecastId]\n        model_c = XGBRegressor(n_estimators=100,subsample=0.7,max_depth=5)\n        model_c.fit(X_train, Y_train_c)\n        Y_pred_c = model_c.predict(X_test)\n        model_f = XGBRegressor(n_estimators=100,subsample=0.7,max_depth=5)\n        model_f.fit(X_train, Y_train_f)\n        Y_pred_f = model_f.predict(X_test)\n        for j in range(len(Y_pred_c)):\n            dic = { 'ForecastId': X_forecastId[j], 'ConfirmedCases': Y_pred_c[j], 'Fatalities': Y_pred_f[j]}\n            sub_xgb.append(dic)\n\n# %% [code]\nsubmission = pd.DataFrame(sub_xgb)\n#submission[['ForecastId','ConfirmedCases','Fatalities']].to_csv(path_or_buf='submission.csv',index=False)","26f13849":"test = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-3\/test.csv\")","c1db0d31":"Predicted_data = pd.merge(submission,\n                 test[['ForecastId','Province_State','Country_Region','Date']],\n                 on='ForecastId',how='left')","9f002150":"Predicted_data[Predicted_data['Country_Region']=='Qatar']","ceb95e3a":"merged_train_df.columns","34cfa5b5":"features_selector = ['Lat', 'Long', 'age_0-4', 'age_5-9', 'age_10-14',\n       'age_15-19', 'age_20-24', 'age_25-29', 'age_30-34', 'age_35-39',\n       'age_40-44', 'age_45-49', 'age_50-54', 'age_55-59', 'age_60-64',\n       'age_65-69', 'age_70-74', 'age_75-79', 'age_80-84', 'age_85-89',\n       'age_90-94', 'age_95-99', 'age_100+', 'total_pop', 'smokers_perc',\n       'density', 'urbanpop', 'hospibed', 'lung', 'femalelung', 'malelung',\n       'restrictions', 'quarantine', 'schools','Date_num']","b1e74a45":"merged_train_df.columns","4395bd27":"#'rate_ConfirmedCases','rate_Fatalities',\nfrom sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\nmerged_train_df[['total_pop', 'smokers_perc','mean_rate_case_last7', 'mean_rate_fat_last7','std_rate_case','std_rate_fat',\n       'density']] = mms.fit_transform(merged_train_df[['total_pop', 'smokers_perc','mean_rate_case_last7', 'mean_rate_fat_last7','std_rate_case','std_rate_fat',\n       'density']])","544201b8":"merged_train_df","b691b6fc":"import pandas as pd\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport xgboost as xgb\n\nfrom tensorflow.keras.optimizers import Nadam\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport tensorflow.keras.layers as KL\nfrom datetime import timedelta\nimport numpy as np\nimport pandas as pd\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression, Ridge\n\nimport datetime\nimport gc\nfrom tqdm import tqdm\ndef get_cpmp_sub(save_oof=False, save_public_test=False):\n    train = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-3\/train.csv')\n    train['Province_State'].fillna('', inplace=True)\n    train['Date'] = pd.to_datetime(train['Date'])\n    train['day'] = train.Date.dt.dayofyear\n    #train = train[train.day <= 85]\n    train['geo'] = ['_'.join(x) for x in zip(train['Country_Region'], train['Province_State'])]\n    train\n\n    test = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-3\/test.csv')\n    test['Province_State'].fillna('', inplace=True)\n    test['Date'] = pd.to_datetime(test['Date'])\n    test['day'] = test.Date.dt.dayofyear\n    test['geo'] = ['_'.join(x) for x in zip(test['Country_Region'], test['Province_State'])]\n    test\n\n    day_min = train['day'].min()\n    train['day'] -= day_min\n    test['day'] -= day_min\n\n    min_test_val_day = test.day.min()\n    max_test_val_day = train.day.max()\n    max_test_day = test.day.max()\n    num_days = max_test_day + 1\n\n    min_test_val_day, max_test_val_day, num_days\n\n    train['ForecastId'] = -1\n    test['Id'] = -1\n    test['ConfirmedCases'] = 0\n    test['Fatalities'] = 0\n\n    debug = False\n\n    data = pd.concat([train,\n                      test[test.day > max_test_val_day][train.columns]\n                     ]).reset_index(drop=True)\n    if debug:\n        data = data[data['geo'] >= 'France_'].reset_index(drop=True)\n    #del train, test\n    gc.collect()\n\n    dates = data[data['geo'] == 'France_'].Date.values\n\n    if 0:\n        gr = data.groupby('geo')\n        data['ConfirmedCases'] = gr.ConfirmedCases.transform('cummax')\n        data['Fatalities'] = gr.Fatalities.transform('cummax')\n\n    geo_data = data.pivot(index='geo', columns='day', values='ForecastId')\n    num_geo = geo_data.shape[0]\n    geo_data\n\n    geo_id = {}\n    for i,g in enumerate(geo_data.index):\n        geo_id[g] = i\n\n\n    ConfirmedCases = data.pivot(index='geo', columns='day', values='ConfirmedCases')\n    Fatalities = data.pivot(index='geo', columns='day', values='Fatalities')\n\n    if debug:\n        cases = ConfirmedCases.values\n        deaths = Fatalities.values\n    else:\n        cases = np.log1p(ConfirmedCases.values)\n        deaths = np.log1p(Fatalities.values)\n\n\n    def get_dataset(start_pred, num_train, lag_period): #63,5,14\n        days = np.arange( start_pred - num_train + 1, start_pred + 1)#(59,1,64)\n        lag_cases = np.vstack([cases[:, d - lag_period : d] for d in days])\n        lag_deaths = np.vstack([deaths[:, d - lag_period : d] for d in days])\n        target_cases = np.vstack([cases[:, d : d + 1] for d in days])\n        target_deaths = np.vstack([deaths[:, d : d + 1] for d in days])\n        geo_ids = np.vstack([geo_ids_base for d in days])\n        country_ids = np.vstack([country_ids_base for d in days])\n        return lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days\n\n    def update_valid_dataset(data, pred_death, pred_case):\n        lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days = data\n        day = days[-1] + 1\n        new_lag_cases = np.hstack([lag_cases[:, 1:], pred_case])\n        new_lag_deaths = np.hstack([lag_deaths[:, 1:], pred_death]) \n        new_target_cases = cases[:, day:day+1]\n        new_target_deaths = deaths[:, day:day+1] \n        new_geo_ids = geo_ids  \n        new_country_ids = country_ids  \n        new_days = 1 + days\n        return new_lag_cases, new_lag_deaths, new_target_cases, new_target_deaths, new_geo_ids, new_country_ids, new_days\n\n    def fit_eval(lr_death, lr_case, data, start_lag_death, end_lag_death, num_lag_case, fit, score):\n        lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days = data\n\n        X_death = np.hstack([lag_cases[:, -start_lag_death:-end_lag_death], country_ids])\n        X_death = np.hstack([lag_deaths[:, -num_lag_case:], country_ids])\n        X_death = np.hstack([lag_cases[:, -start_lag_death:-end_lag_death], lag_deaths[:, -num_lag_case:], country_ids])\n        y_death = target_deaths\n        y_death_prev = lag_deaths[:, -1:]\n        if fit:\n            if 0:\n                keep = (y_death > 0).ravel()\n                X_death = X_death[keep]\n                y_death = y_death[keep]\n                y_death_prev = y_death_prev[keep]\n            lr_death.fit(X_death, y_death)\n        y_pred_death = lr_death.predict(X_death)\n        y_pred_death = np.maximum(y_pred_death, y_death_prev)\n\n        X_case = np.hstack([lag_cases[:, -num_lag_case:], geo_ids])\n        X_case = lag_cases[:, -num_lag_case:]\n        y_case = target_cases\n        y_case_prev = lag_cases[:, -1:]\n        if fit:\n            lr_case.fit(X_case, y_case)\n        y_pred_case = lr_case.predict(X_case)\n        y_pred_case = np.maximum(y_pred_case, y_case_prev)\n\n        if score:\n            death_score = val_score(y_death, y_pred_death)\n            case_score = val_score(y_case, y_pred_case)\n        else:\n            death_score = 0\n            case_score = 0\n\n        return death_score, case_score, y_pred_death, y_pred_case\n\n    def train_model(train, valid, start_lag_death, end_lag_death, num_lag_case, num_val, score=True):\n        alpha = 3\n        lr_death = Ridge(alpha=alpha, fit_intercept=False)\n        lr_case = Ridge(alpha=alpha, fit_intercept=True)\n\n        (train_death_score, train_case_score, train_pred_death, train_pred_case,\n        ) = fit_eval(lr_death, lr_case, train, start_lag_death, end_lag_death, num_lag_case, fit=True, score=score)\n\n        death_scores = []\n        case_scores = []\n\n        death_pred = []\n        case_pred = []\n\n        for i in range(num_val):\n\n            (valid_death_score, valid_case_score, valid_pred_death, valid_pred_case,\n            ) = fit_eval(lr_death, lr_case, valid, start_lag_death, end_lag_death, num_lag_case, fit=False, score=score)\n\n            death_scores.append(valid_death_score)\n            case_scores.append(valid_case_score)\n            death_pred.append(valid_pred_death)\n            case_pred.append(valid_pred_case)\n\n            if 0:\n                print('val death: %0.3f' %  valid_death_score,\n                      'val case: %0.3f' %  valid_case_score,\n                      'val : %0.3f' %  np.mean([valid_death_score, valid_case_score]),\n                      flush=True)\n            valid = update_valid_dataset(valid, valid_pred_death, valid_pred_case)\n\n        if score:\n            death_scores = np.sqrt(np.mean([s**2 for s in death_scores]))\n            case_scores = np.sqrt(np.mean([s**2 for s in case_scores]))\n            if 0:\n                print('train death: %0.3f' %  train_death_score,\n                      'train case: %0.3f' %  train_case_score,\n                      'val death: %0.3f' %  death_scores,\n                      'val case: %0.3f' %  case_scores,\n                      'val : %0.3f' % ( (death_scores + case_scores) \/ 2),\n                      flush=True)\n            else:\n                print('%0.4f' %  case_scores,\n                      ', %0.4f' %  death_scores,\n                      '= %0.4f' % ( (death_scores + case_scores) \/ 2),\n                      flush=True)\n        death_pred = np.hstack(death_pred)\n        case_pred = np.hstack(case_pred)\n        return death_scores, case_scores, death_pred, case_pred\n\n    countries = [g.split('_')[0] for g in geo_data.index]\n    countries = pd.factorize(countries)[0]\n\n    country_ids_base = countries.reshape((-1, 1))\n    ohe = OneHotEncoder(sparse=False)\n    country_ids_base = 0.2 * ohe.fit_transform(country_ids_base)\n    country_ids_base.shape\n\n    geo_ids_base = np.arange(num_geo).reshape((-1, 1))\n    ohe = OneHotEncoder(sparse=False)\n    geo_ids_base = 0.1 * ohe.fit_transform(geo_ids_base)\n    geo_ids_base.shape\n\n    def val_score(true, pred):\n        pred = np.log1p(np.round(np.expm1(pred) - 0.2))\n        return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\n    def val_score(true, pred):\n        return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\n#14,6,5,14\n\n    start_lag_death, end_lag_death = 14, 6,\n    num_train = 7\n    num_lag_case = 14\n    lag_period = max(start_lag_death, num_lag_case)  #14\n\n    def get_oof(start_val_delta=0):   \n        start_val = min_test_val_day + start_val_delta\n        last_train = start_val - 1\n        num_val = max_test_val_day - start_val + 1\n        print(dates[start_val], start_val, num_val)\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        _, _, val_death_preds, val_case_preds = train_model(train_data, valid_data, \n                                                            start_lag_death, end_lag_death, num_lag_case, num_val)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n\n        sub = train[['Date', 'Id', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        #sub = sub.fillna(0)\n        sub = sub[sub.day >= start_val]\n        sub = sub[['Id', 'ConfirmedCases', 'Fatalities']].copy()\n        return sub\n\n#'2020-03-27', '2020-03-24', '2020-03-21', '2020-03-18'\n#'2020-03-22', '2020-03-19', '2020-03-16', '2020-03-13'\n    if save_oof:\n        for start_val_delta, date in zip(range(3, -8, -3),\n                                  ['2020-03-27', '2020-03-24', '2020-03-21', '2020-03-18']):\n            print(date, end=' ')\n            oof = get_oof(start_val_delta)\n            oof.to_csv('..\/submissions\/cpmp-%s.csv' % date, index=None)\n\n    def get_sub(start_val_delta=0):   \n        start_val = min_test_val_day + start_val_delta  #64\n        last_train = start_val - 1   #63\n        num_val = max_test_val_day - start_val + 1 #11\n        print(dates[last_train], start_val, num_val)\n        num_lag_case = 14  #14\n        train_data = get_dataset(last_train, num_train, lag_period)#63,5,14\n        valid_data = get_dataset(start_val, 1, lag_period) #64,1,14\n        _, _, val_death_preds, val_case_preds = train_model(train_data, valid_data, \n                                                            start_lag_death, end_lag_death, num_lag_case, num_val)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n\n        sub = test[['Date', 'ForecastId', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        sub = sub.fillna(0)\n        sub = sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n        return sub\n        return sub\n\n\n    known_test = train[['geo', 'day', 'ConfirmedCases', 'Fatalities']\n              ].merge(test[['geo', 'day', 'ForecastId']], how='left', on=['geo', 'day'])\n    known_test = known_test[['ForecastId', 'ConfirmedCases', 'Fatalities']][known_test.ForecastId.notnull()].copy()\n    known_test\n\n    unknow_test = test[test.day > max_test_val_day]\n    unknow_test\n\n    def get_final_sub():   \n        start_val = max_test_val_day + 1\n        last_train = start_val - 1\n        num_val = max_test_day - start_val + 1\n        print(dates[last_train], start_val, num_val)\n        num_lag_case = num_val + 3\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        (_, _, val_death_preds, val_case_preds\n        ) = train_model(train_data, valid_data, start_lag_death, end_lag_death, num_lag_case, num_val, score=False)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n        print(unknow_test.shape, pred_deaths.shape, pred_cases.shape)\n\n        sub = unknow_test[['Date', 'ForecastId', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        #sub = sub.fillna(0)\n        sub = sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n        sub = pd.concat([known_test, sub])\n        return sub\n\n    if save_public_test:\n        sub = get_sub()\n    else:\n        sub = get_final_sub()\n    return sub","2aa97155":"def get_nn_sub():\n    df = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-3\/train.csv\")\n    sub_df = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-3\/test.csv\")\n\n    coo_df = merged_train_df.groupby(\"Country_Region\")[['total_pop', 'smokers_perc','mean_rate_case_last7', 'mean_rate_fat_last7','std_rate_case','std_rate_fat','Lat','Long',\n       'density']].mean().reset_index()\n   # coo_df = coo_df[coo_df[\"Country_Region\"].notnull()]\n\n    loc_group = [\"Province_State\", \"Country_Region\"]\n\n\n    def preprocess(df):\n        df[\"Date\"] = df[\"Date\"].astype(\"datetime64[ms]\")\n        df[\"days\"] = (df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n    #    df[\"weekend\"] = df[\"Date\"].dt.dayofweek\/\/5\n\n        df = df.merge(coo_df, how=\"left\", on=\"Country_Region\")\n        df[\"Lat\"] = (df[\"Lat\"] \/\/ 30).astype(np.float32).fillna(0)\n        df[\"Long\"] = (df[\"Long\"] \/\/ 60).astype(np.float32).fillna(0)\n\n        for col in loc_group:\n            df[col].fillna(\"none\", inplace=True)\n        return df\n\n    df = preprocess(df)\n    sub_df = preprocess(sub_df)\n\n    print(df.shape)\n\n    TARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n    for col in TARGETS:\n        df[col] = np.log1p(df[col])\n\n    NUM_SHIFT = 7\n#'1-smokers_perc','density'\n#2-'smokers_perc','density','mean_rate_case_last7'\n#3-'smokers_perc','density','mean_rate_case_last7','mean_rate_fat_last7'\n#4-'smokers_perc','density','mean_rate_fat_last7'\n#5-'mean_rate_case_last7','mean_rate_fat_last7'\n    features = ['mean_rate_case_last7','mean_rate_fat_last7']\n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            df[\"prev_{}_{}\".format(col, s)] = df.groupby(loc_group)[col].shift(s)\n            features.append(\"prev_{}_{}\".format(col, s))\n\n    df = df[df[\"Date\"] >= df[\"Date\"].min() + timedelta(days=NUM_SHIFT)].copy()\n\n    TEST_FIRST = sub_df[\"Date\"].min() # pd.to_datetime(\"2020-03-13\") #\n    TEST_DAYS = (df[\"Date\"].max() - TEST_FIRST).days + 1\n\n    dev_df, test_df = df[df[\"Date\"] < TEST_FIRST].copy(), df[df[\"Date\"] >= TEST_FIRST].copy()\n\n    def nn_block(input_layer, size, dropout_rate, activation):\n        out_layer = KL.Dense(size, activation=None)(input_layer)\n        #out_layer = KL.BatchNormalization()(out_layer)\n        out_layer = KL.Activation(activation)(out_layer)\n        out_layer = KL.Dropout(dropout_rate)(out_layer)\n        return out_layer\n\n\n    def get_model():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, \"relu\")\n        gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, \"relu\")\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n\n    get_model().summary()\n\n    def get_input(df):\n        return [df[features]]\n\n    NUM_MODELS = 10\n\n\n    def train_models(df, save=False):\n        models = []\n        for i in range(NUM_MODELS):\n            model = get_model()\n            model.compile(loss=\"mean_squared_error\", optimizer=Nadam(lr=1e-4))\n            hist = model.fit(get_input(df), df[TARGETS],\n                             batch_size=2048, epochs=500, verbose=0, shuffle=True)\n            if save:\n                model.save_weights(\"model{}.h5\".format(i))\n            models.append(model)\n        return models\n\n    models = train_models(dev_df)\n\n\n    prev_targets = ['prev_ConfirmedCases_1', 'prev_Fatalities_1']\n\n    def predict_one(df, models):\n        pred = np.zeros((df.shape[0], 2))\n        for model in models:\n            pred += model.predict(get_input(df))\/len(models)\n        pred = np.maximum(pred, df[prev_targets].values)\n        pred[:, 0] = np.log1p(np.expm1(pred[:, 0]) + 0.1)\n        pred[:, 1] = np.log1p(np.expm1(pred[:, 1]) + 0.01)\n        return np.clip(pred, None, 15)\n\n    print([mean_squared_error(dev_df[TARGETS[i]], predict_one(dev_df, models)[:, i]) for i in range(len(TARGETS))])\n\n\n    def rmse(y_true, y_pred):\n        return np.sqrt(mean_squared_error(y_true, y_pred))\n\n    def evaluate(df):\n        error = 0\n        for col in TARGETS:\n            error += rmse(df[col].values, df[\"pred_{}\".format(col)].values)\n        return np.round(error\/len(TARGETS), 5)\n\n\n    def predict(test_df, first_day, num_days, models, val=False):\n        temp_df = test_df.loc[test_df[\"Date\"] == first_day].copy()\n        y_pred = predict_one(temp_df, models)\n\n        for i, col in enumerate(TARGETS):\n            test_df[\"pred_{}\".format(col)] = 0\n            test_df.loc[test_df[\"Date\"] == first_day, \"pred_{}\".format(col)] = y_pred[:, i]\n\n        print(first_day, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n        if val:\n            print(evaluate(test_df[test_df[\"Date\"] == first_day]))\n\n\n        y_prevs = [None]*NUM_SHIFT\n\n        for i in range(1, NUM_SHIFT):\n            y_prevs[i] = temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]].values\n\n        for d in range(1, num_days):\n            date = first_day + timedelta(days=d)\n            print(date, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n\n            temp_df = test_df.loc[test_df[\"Date\"] == date].copy()\n            temp_df[prev_targets] = y_pred\n            for i in range(2, NUM_SHIFT+1):\n                temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]] = y_prevs[i-1]\n\n            y_pred, y_prevs = predict_one(temp_df, models), [None, y_pred] + y_prevs[1:-1]\n\n\n            for i, col in enumerate(TARGETS):\n                test_df.loc[test_df[\"Date\"] == date, \"pred_{}\".format(col)] = y_pred[:, i]\n\n            if val:\n                print(evaluate(test_df[test_df[\"Date\"] == date]))\n\n        return test_df\n\n    test_df = predict(test_df, TEST_FIRST, TEST_DAYS, models, val=True)\n    print(evaluate(test_df))\n\n    for col in TARGETS:\n        test_df[col] = np.expm1(test_df[col])\n        test_df[\"pred_{}\".format(col)] = np.expm1(test_df[\"pred_{}\".format(col)])\n\n    models = train_models(df, save=True)\n\n    sub_df_public = sub_df[sub_df[\"Date\"] <= df[\"Date\"].max()].copy()\n    sub_df_private = sub_df[sub_df[\"Date\"] > df[\"Date\"].max()].copy()\n\n    pred_cols = [\"pred_{}\".format(col) for col in TARGETS]\n    #sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + pred_cols].rename(columns={col: col[5:] for col in pred_cols}), \n    #                                    how=\"left\", on=[\"Date\"] + loc_group)\n    sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + TARGETS], how=\"left\", on=[\"Date\"] + loc_group)\n\n    SUB_FIRST = sub_df_private[\"Date\"].min()\n    SUB_DAYS = (sub_df_private[\"Date\"].max() - sub_df_private[\"Date\"].min()).days + 1\n\n    sub_df_private = df.append(sub_df_private, sort=False)\n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            sub_df_private[\"prev_{}_{}\".format(col, s)] = sub_df_private.groupby(loc_group)[col].shift(s)\n\n    sub_df_private = sub_df_private[sub_df_private[\"Date\"] >= SUB_FIRST].copy()\n\n    sub_df_private = predict(sub_df_private, SUB_FIRST, SUB_DAYS, models)\n\n    for col in TARGETS:\n        sub_df_private[col] = np.expm1(sub_df_private[\"pred_{}\".format(col)])\n\n    sub_df = sub_df_public.append(sub_df_private, sort=False)\n    sub_df[\"ForecastId\"] = sub_df[\"ForecastId\"].astype(np.int16)\n\n    return sub_df[[\"ForecastId\"] + TARGETS]","2ca3f632":"sub1 = get_cpmp_sub()\nsub1['ForecastId'] = sub1['ForecastId'].astype('int')","db2596f0":"sub2 = get_nn_sub()","098f1a6f":"sub1.sort_values(\"ForecastId\", inplace=True)\nsub2.sort_values(\"ForecastId\", inplace=True)","d8128342":"from sklearn.metrics import mean_squared_error\n\nTARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n[np.sqrt(mean_squared_error(np.log1p(sub1[t].values), np.log1p(sub2[t].values))) for t in TARGETS]","433be96f":"sub_df = sub1.copy()\nfor t in TARGETS:\n    sub_df[t] = np.expm1(np.log1p(sub1[t].values)*0.55+ np.log1p(sub2[t].values)*0.45).astype('int')\n    \nsub_df.to_csv(\"submission.csv\", index=False)","097e2cc3":"Predicted_data = pd.merge(sub_df ,\n                 test_df[['ForecastId','Province_State','Country_Region','Date']],\n                 on='ForecastId',how='left')","e1cde7b7":"Predicted_data[Predicted_data['Country_Region']=='Qatar']\n","691de752":"confirmed_total_date = Predicted_data.groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date = Predicted_data.groupby(['Date']).agg({'Fatalities':['sum']})\nprint(confirmed_total_date)\nprint(fatalities_total_date)","80c54743":"merged_train_df.columns","7f3e5a14":"from sklearn.tree import DecisionTreeRegressor\nDecisionTreeRegressor(ccp_alpha=0.01, criterion='mse', max_depth=None,\n                      max_features=None, max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=1, min_samples_split=2,\n                      min_weight_fraction_leaf=0.0, presort='deprecated',\n                      random_state=1, splitter='best')\n\n\n\ny_train_cc = np.array(merged_train_df['ConfirmedCases'].astype(int))\ny_train_ft = np.array(merged_train_df['Fatalities'].astype(int))\ncols = ['ConfirmedCases', 'Fatalities','Id','age_0-4', 'age_5-9', 'age_10-14',\n       'age_15-19', 'age_20-24', 'age_25-29', 'age_30-34', 'age_35-39',\n       'age_40-44', 'age_45-49', 'age_50-54', 'age_55-59', 'age_60-64',\n       'age_65-69', 'age_70-74', 'age_75-79', 'age_80-84', 'age_85-89',\n       'age_90-94', 'age_95-99', 'age_100+', 'total_pop', 'smokers_perc',\n       'density', 'urbanpop', 'hospibed', 'lung', 'femalelung', 'malelung',\n       'restrictions', 'quarantine', 'schools', 'Day_num', 'Date_num']\n\nfull_df = pd.concat([merged_train_df.drop(cols, axis=1), merged_test_df])\nindex_split = merged_train_df.shape[0]\nfull_df = pd.get_dummies(full_df, columns=full_df.columns)\n\nx_train = full_df[:index_split]\nx_test= full_df[index_split:]\n","5c5de1b8":"dt = DecisionTreeRegressor(random_state=1)\ndt.fit(x_train,y_train_cc)\ny_pred_cc = dt.predict(x_test)\ny_pred_cc = y_pred_cc.astype(int)\ny_pred_cc[y_pred_cc <0]=0\ndt_f = DecisionTreeRegressor()\ndt_f.fit(x_train,y_train_ft)\ny_pred_ft = dt_f.predict(x_test)\ny_pred_ft = y_pred_ft.astype(int)\ny_pred_ft[y_pred_ft <0]=0\npredicted_df_dt = pd.DataFrame([y_pred_cc, y_pred_ft], index = ['ConfirmedCases','Fatalities'], columns= np.arange(1, y_pred_cc.shape[0] + 1)).T\npredicted_df_dt.to_csv('submission.csv', index_label = \"ForecastId\")","9a96563c":"predicted_df_dt","fd923d68":"def countryplot(train_df,country):\n    \n    confirmed_total = train_df[train_df['Country_Region']==country_dict[country]].groupby(['Date']).agg({'ConfirmedCases':['sum']})\n    fatalities_total = train_df[train_df['Country_Region']==country_dict[country]].groupby(['Date']).agg({'Fatalities':['sum']})\n    total = confirmed_total.join(fatalities_total)\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17,7))\n    total.plot(ax=ax1)\n    ax1.set_title(country, size=13,)\n    ax1.set_ylabel(\"Number of cases\", size=13)\n    ax1.set_xlabel(\"Date\", size=13)\n    #ax1.set_xticklabels(\"Date\",rotation= 90)\n    plt.xticks(rotation=90)\n    ax1.grid()\n    fatalities_total.plot(ax=ax2, color='red')\n    plt.grid()\n    ax2.set_title(\"Global deceased cases\", size=13)\n    ax2.set_ylabel(\"Number of cases\", size=13)\n    ax2.set_xlabel(\"Date\", size=13)\n    plt.xticks(rotation=90)","23ff368b":"countrylist=['Qatar','Italy','Spain','United Arab Emirates','Egypt','Germany','France','Korea, South','Turkey','US','Saudi Arabia','United Kingdom']\n\n \n\nfor co in countrylist:\n    countryplot(Predicted_data,co)\n    #country='Italy'\n","812e2b31":"# RNN","48e84cc5":"# decision tree regressor with multiple variables","90d86bda":"## Combiuned"}}