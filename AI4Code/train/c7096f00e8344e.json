{"cell_type":{"6d4e5d7e":"code","c6d43e75":"code","18b6c6b9":"code","37871e2c":"code","b8f637ab":"code","a8c4db27":"code","c0d5fe8b":"code","de88a1d1":"code","79c3e128":"code","87404869":"code","aadcafe7":"code","97cadbb2":"code","f69fb248":"code","a4dd08d3":"code","38995324":"code","f5116b83":"code","538097e2":"code","dbbfb9d9":"code","d3764581":"code","fdf6d1b6":"code","dd7375ef":"code","0cc9e6ab":"code","38af2f58":"code","13a6fe99":"code","d02e0800":"code","607049d9":"code","54ba0c0d":"code","61870501":"code","deddce47":"code","a27a8907":"code","ccaaca38":"code","92ae50a0":"code","4a05514e":"code","f981b185":"code","40a49ccc":"code","61e3bb70":"code","fa67c260":"markdown","8c5dfed6":"markdown","131481b2":"markdown","d75fbfad":"markdown","cc54abce":"markdown","01864a4f":"markdown","71a53f7b":"markdown","4c1bdcca":"markdown","fa2797ef":"markdown","01117bea":"markdown","760c14dd":"markdown","5d640feb":"markdown","36cd6e28":"markdown","2be2e8a6":"markdown"},"source":{"6d4e5d7e":"import logging\nimport os\nimport re\nimport gc\nimport json\nfrom tqdm.auto import tqdm\n\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt","c6d43e75":"!pip install ..\/input\/pytorch-16\/torch-1.6.0cu101-cp37-cp37m-linux_x86_64.whl","18b6c6b9":"!pip install ..\/input\/pytorch-16\/torchvision-0.7.0cu101-cp37-cp37m-linux_x86_64.whl","37871e2c":"!pip install ..\/input\/pretrainedmodels\/pretrainedmodels-0.7.4\/pretrainedmodels-0.7.4\/ > \/dev\/null # no output","b8f637ab":"!pip install ..\/input\/wheat-pkgs\/EfficientNet-PyTorch-master\/EfficientNet-PyTorch-master\/ > \/dev\/null # no output","a8c4db27":"!pip install ..\/input\/wheat-pkgs\/timm-0.1.20-py3-none-any.whl > \/dev\/null # no output","c0d5fe8b":"!pip install ..\/input\/wheat-pkgs\/segmentation_models.pytorch-master\/segmentation_models.pytorch-master > \/dev\/null # no output","de88a1d1":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport segmentation_models_pytorch as smp\n\nfrom wheat_infer_utils import *\nfrom wheat_centernet_models import PoseBiFPNNet\nfrom wheat_train_helpers import (\n    set_seed,\n    create_logging,\n    WheatDataset,\n    FastDataLoader,\n    collate,\n    ModleWithLoss,\n    CtdetLoss,\n    ModelEMA,\n    get_cosine_schedule_with_warmup,\n    train_one_epoch,\n    get_train_transforms\n)","79c3e128":"bifpn_path_0 = '..\/input\/wheat-weights\/model_centernet_effnetb5_bifpn_00099.pth'\nbifpn_path_1 = '..\/input\/wheat-weights\/model_centernet_effnetb5_bifpn_fold1_00099.pth'\nbifpn_path_3 = '..\/input\/wheat-weights\/model_centernet_effnetb5_bifpn_fold3_lb_ema_00099.pth'","87404869":"class Config:\n    arch = 'timm-efficientnet-b5'\n    heads = {'hm': 1,\n             'wh': 2,\n             'reg': 2}\n    head_conv = 64\n    reg_offset = True\n    cat_spec_wh = False\n    \n    # Image\n    img_size = 1024\n    in_scale = 1024 \/ img_size\n    down_ratio = 4\n    \n    mean = [0.315290, 0.317253, 0.214556], \n    std = [0.245211, 0.238036, 0.193879]\n    num_classes = 1\n    \n    pad = 63\n    \n    # Test\n    \n    batch_size = 8\n    K = 128\n    max_per_image = 128\n    \n    fix_res = False\n    test_scales = [1]\n    flip_test = False\n    nms = False\n    gpus = [0]\n    amp = True\n    \nopt = Config()\ndevice = torch.device('cuda') if opt.gpus[0] >= 0 else torch.device('cpu')","aadcafe7":"def change_key(d):\n    for _ in range(len(d)):\n        k, v = d.popitem(False)\n        d['.'.join(k.split('.')[1:])] = v","97cadbb2":"DIR_INPUT = '..\/input\/global-wheat-detection'\nDIR_TRAIN = f'{DIR_INPUT}\/train'\nDIR_TEST = f'{DIR_INPUT}\/test'\n\ntrain_df = pd.read_csv(f'{DIR_INPUT}\/train.csv')\ntrain_df.shape","f69fb248":"train_df['x'] = -1\ntrain_df['y'] = -1\ntrain_df['w'] = -1\ntrain_df['h'] = -1\n\ndef expand_bbox(x):\n    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x))\n    if len(r) == 0:\n        r = [-1, -1, -1, -1]\n    return r\n\ntrain_df[['x', 'y', 'w', 'h']] = np.stack(train_df['bbox'].apply(lambda x: expand_bbox(x)))\ntrain_df.drop(columns=['bbox'], inplace=True)\ntrain_df['x'] = train_df['x'].astype(np.float)\ntrain_df['y'] = train_df['y'].astype(np.float)\ntrain_df['w'] = train_df['w'].astype(np.float)\ntrain_df['h'] = train_df['h'].astype(np.float)\n\ntrain_df.head()","a4dd08d3":"# DEBUG\n# DIR_TEST = '..\/input\/wheat-fake-test'","38995324":"class WheatDatasetTest(torch.utils.data.Dataset):\n    def __init__(self, opt, image_dir, transforms=None,\n                 mean=[0.315290, 0.317253, 0.214556], \n                 std=[0.245211, 0.238036, 0.193879]):\n        \n        self.opt = opt\n        \n        self.image_dir = image_dir\n        self.img_id = os.listdir(self.image_dir)\n        \n        self.transforms = transforms\n        \n        self.mean = np.array(mean, dtype=np.float32).reshape(1, 1, 3)\n        self.std = np.array(std, dtype=np.float32).reshape(1, 1, 3)\n        \n    def __len__(self):\n        return len(self.img_id)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.image_dir, self.img_id[idx])\n        \n        image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n        h0, w0 = image.shape[0:2]\n        \n        if self.transforms is not None:\n            image = self.transforms(image)\n        image_resized = cv2.resize(image, (self.opt.img_size, self.opt.img_size))\n        return image_resized, self.img_id[idx], image, h0, w0","f5116b83":"def flip_lr(img):\n    return np.ascontiguousarray(img[:, ::-1, :])\n\ndef deaug_lr(img, boxes):\n    h, w = img.shape[:2]\n    boxes[:, (0, 2)] = w - boxes[:, (2, 0)]\n    return boxes\n\ndef flip_ud(img):\n    return np.ascontiguousarray(img[::-1, :, :])\n\ndef deaug_ud(img, boxes):\n    h, w = img.shape[:2]\n    boxes[:, (1, 3)] = w - boxes[:, (3, 1)]\n    return boxes","538097e2":"BOX_COLOR_PRED = (255, 0, 0)\nTEXT_COLOR = (255, 255, 255)\n\n\ndef visualize_bbox(img, bbox, score, color, thickness=2):\n    x_min, y_min, x_max, y_max = bbox\n    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n    ((text_width, text_height), _) = cv2.getTextSize(\"{:.4f}\".format(score), cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    \n    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), color, -1)\n    cv2.putText(img, \"{:.4f}\".format(score), (x_min, y_min - int(0.3 * text_height)), cv2.FONT_HERSHEY_SIMPLEX, 0.35,TEXT_COLOR, lineType=cv2.LINE_AA)\n    return img\n\n\ndef visualize(annotations):\n    img = annotations['image'].copy()\n    for bbox, score in zip(annotations['bboxes'], annotations['scores']):\n        img = visualize_bbox(img, bbox, score, color=BOX_COLOR_PRED)\n    plt.figure(figsize=(12, 12))\n    plt.imshow(img)","dbbfb9d9":"testdataset = WheatDatasetTest(opt, DIR_TEST)\nprint('Total number of images in test set: {}'.format(len(testdataset)))\n\ntestdataset_lr = WheatDatasetTest(opt, DIR_TEST, transforms=flip_lr)\ntestdataset_ud = WheatDatasetTest(opt, DIR_TEST, transforms=flip_ud)","d3764581":"# Inference helper\ndef do_predict(opt, model, threshold, flip_type=0, return_ids=False, return_shapes=False):\n    \n    if flip_type == 0:\n        test_dataset = testdataset\n        deaug_transform = None\n    elif flip_type == 1:\n        test_dataset = testdataset_lr\n        deaug_transform = deaug_lr\n    elif flip_type == 2:\n        test_dataset = testdataset_ud\n        deaug_transform = deaug_ud\n        \n    detector = CtdetDetector(opt, model)\n    \n    pred_boxes = []\n    pred_scores = []\n    \n    height_list = []\n    width_list = []\n    if return_ids:\n        img_ids = []\n    \n    for img, img_id, img0, h0, w0 in tqdm(test_dataset):\n        \n        ret = detector.run(img)\n        results = ret['results'][1]\n        results = results[results[:, 4] > threshold]\n        \n        pred_box = results[:, :4]\n        if flip_type != 0:\n            pred_box = deaug_transform(img, pred_box)\n        \n        # rescale & clip\n        pred_box[:, 0] = np.clip(pred_box[:, 0] \/ opt.img_size * w0, 0, w0-1)\n        pred_box[:, 1] = np.clip(pred_box[:, 1] \/ opt.img_size * h0, 0 ,h0-1)\n        pred_box[:, 2] = np.clip(pred_box[:, 2] \/ opt.img_size * w0, 0, w0-1)\n        pred_box[:, 3] = np.clip(pred_box[:, 3] \/ opt.img_size * h0, 0 ,h0-1)\n            \n        pred_boxes.append(pred_box)\n        pred_scores.append(results[:, 4])\n        if return_ids:\n            img_ids.append(os.path.splitext(img_id)[0])\n        \n        if return_shapes:\n            height_list.append(h0)\n            width_list.append(w0)\n    \n    if return_shapes:\n        if return_ids:\n            return pred_boxes, pred_scores, height_list, width_list, img_ids\n        else:\n            return pred_boxes, pred_scores, height_list, width_list\n    else:\n        if return_ids:\n            return pred_boxes, pred_scores, img_ids\n        else:\n            return pred_boxes, pred_scores","fdf6d1b6":"bifpn_model = PoseBiFPNNet(opt.arch, opt.heads, opt.head_conv)\ncheckpoint = torch.load(bifpn_path_0, map_location=device)\n\nchange_key(checkpoint['model'])\nbifpn_model.load_state_dict(checkpoint['model'])\nbifpn_model.to(device)\n\ndel checkpoint\ngc.collect()","dd7375ef":"opt.pad = 63\n\nopt.test_scales = [1.1, ]\nthreshold = 0.30\n\nbifpn0_pred_boxes_0   , bifpn0_pred_scores_0, h0_list, w0_list, img_ids = do_predict(opt, bifpn_model, threshold=threshold, flip_type=0, return_ids=True, return_shapes=True)\nbifpn0_pred_boxes_0_lr, bifpn0_pred_scores_0_lr = do_predict(opt, bifpn_model, threshold=threshold, flip_type=1, return_ids=False, return_shapes=False)\nbifpn0_pred_boxes_0_ud, bifpn0_pred_scores_0_ud = do_predict(opt, bifpn_model, threshold=threshold, flip_type=2, return_ids=False, return_shapes=False)\n\n\nopt.test_scales = [1.25, ]\nthreshold = 0.28\n\nbifpn0_pred_boxes_l   , bifpn0_pred_scores_l    = do_predict(opt, bifpn_model, threshold=threshold, flip_type=0, return_ids=False, return_shapes=False)\nbifpn0_pred_boxes_l_lr, bifpn0_pred_scores_l_lr = do_predict(opt, bifpn_model, threshold=threshold, flip_type=1, return_ids=False, return_shapes=False)\nbifpn0_pred_boxes_l_ud, bifpn0_pred_scores_l_ud = do_predict(opt, bifpn_model, threshold=threshold, flip_type=2, return_ids=False, return_shapes=False)\n\n\nopt.test_scales = [1.35, ]\nthreshold = 0.27\n\nbifpn0_pred_boxes_x   , bifpn0_pred_scores_x    = do_predict(opt, bifpn_model, threshold=threshold, flip_type=0, return_ids=False, return_shapes=False)\nbifpn0_pred_boxes_x_lr, bifpn0_pred_scores_x_lr = do_predict(opt, bifpn_model, threshold=threshold, flip_type=1, return_ids=False, return_shapes=False)\nbifpn0_pred_boxes_x_ud, bifpn0_pred_scores_x_ud = do_predict(opt, bifpn_model, threshold=threshold, flip_type=2, return_ids=False, return_shapes=False)","0cc9e6ab":"del bifpn_model\ngc.collect()\ntorch.cuda.empty_cache()","38af2f58":"bifpn_model = PoseBiFPNNet(opt.arch, opt.heads, opt.head_conv)\ncheckpoint = torch.load(bifpn_path_1, map_location=device)\n\nchange_key(checkpoint['model'])\nbifpn_model.load_state_dict(checkpoint['model'])\nbifpn_model.to(device)\n\ndel checkpoint\ngc.collect()","13a6fe99":"opt.pad = 63\n\nopt.test_scales = [1.1, ]\nthreshold = 0.30\n\nbifpn1_pred_boxes_0   , bifpn1_pred_scores_0    = do_predict(opt, bifpn_model, threshold=threshold, flip_type=0, return_ids=False, return_shapes=False)\nbifpn1_pred_boxes_0_lr, bifpn1_pred_scores_0_lr = do_predict(opt, bifpn_model, threshold=threshold, flip_type=1, return_ids=False, return_shapes=False)\nbifpn1_pred_boxes_0_ud, bifpn1_pred_scores_0_ud = do_predict(opt, bifpn_model, threshold=threshold, flip_type=2, return_ids=False, return_shapes=False)\n\n\nopt.test_scales = [1.25, ]\nthreshold = 0.28\n\nbifpn1_pred_boxes_l   , bifpn1_pred_scores_l    = do_predict(opt, bifpn_model, threshold=threshold, flip_type=0, return_ids=False, return_shapes=False)\nbifpn1_pred_boxes_l_lr, bifpn1_pred_scores_l_lr = do_predict(opt, bifpn_model, threshold=threshold, flip_type=1, return_ids=False, return_shapes=False)\nbifpn1_pred_boxes_l_ud, bifpn1_pred_scores_l_ud = do_predict(opt, bifpn_model, threshold=threshold, flip_type=2, return_ids=False, return_shapes=False)\n\n\n\nopt.test_scales = [1.35, ]\nthreshold = 0.27\n\nbifpn1_pred_boxes_x   , bifpn1_pred_scores_x    = do_predict(opt, bifpn_model, threshold=threshold, flip_type=0, return_ids=False, return_shapes=False)\nbifpn1_pred_boxes_x_lr, bifpn1_pred_scores_x_lr = do_predict(opt, bifpn_model, threshold=threshold, flip_type=1, return_ids=False, return_shapes=False)\nbifpn1_pred_boxes_x_ud, bifpn1_pred_scores_x_ud = do_predict(opt, bifpn_model, threshold=threshold, flip_type=2, return_ids=False, return_shapes=False)","d02e0800":"del bifpn_model\ngc.collect()\ntorch.cuda.empty_cache()","607049d9":"bifpn_model = PoseBiFPNNet(opt.arch, opt.heads, opt.head_conv)\ncheckpoint = torch.load(bifpn_path_3, map_location=device)\n\nchange_key(checkpoint['model'])\nbifpn_model.load_state_dict(checkpoint['model'])\nbifpn_model.to(device)\n\ndel checkpoint\ngc.collect()","54ba0c0d":"opt.pad = 63\n\nopt.test_scales = [1.1, ]\nthreshold = 0.30\n\nbifpn3_pred_boxes_0   , bifpn3_pred_scores_0    = do_predict(opt, bifpn_model, threshold=threshold, flip_type=0, return_ids=False, return_shapes=False)\nbifpn3_pred_boxes_0_lr, bifpn3_pred_scores_0_lr = do_predict(opt, bifpn_model, threshold=threshold, flip_type=1, return_ids=False, return_shapes=False)\nbifpn3_pred_boxes_0_ud, bifpn3_pred_scores_0_ud = do_predict(opt, bifpn_model, threshold=threshold, flip_type=2, return_ids=False, return_shapes=False)\n\n\nopt.test_scales = [1.25, ]\nthreshold = 0.28\n\nbifpn3_pred_boxes_l   , bifpn3_pred_scores_l    = do_predict(opt, bifpn_model, threshold=threshold, flip_type=0, return_ids=False, return_shapes=False)\nbifpn3_pred_boxes_l_lr, bifpn3_pred_scores_l_lr = do_predict(opt, bifpn_model, threshold=threshold, flip_type=1, return_ids=False, return_shapes=False)\nbifpn3_pred_boxes_l_ud, bifpn3_pred_scores_l_ud = do_predict(opt, bifpn_model, threshold=threshold, flip_type=2, return_ids=False, return_shapes=False)\n\n\nopt.test_scales = [1.35, ]\nthreshold = 0.27\n\nbifpn3_pred_boxes_x   , bifpn3_pred_scores_x    = do_predict(opt, bifpn_model, threshold=threshold, flip_type=0, return_ids=False, return_shapes=False)\nbifpn3_pred_boxes_x_lr, bifpn3_pred_scores_x_lr = do_predict(opt, bifpn_model, threshold=threshold, flip_type=1, return_ids=False, return_shapes=False)\nbifpn3_pred_boxes_x_ud, bifpn3_pred_scores_x_ud = do_predict(opt, bifpn_model, threshold=threshold, flip_type=2, return_ids=False, return_shapes=False)","61870501":"del bifpn_model\ngc.collect()\ntorch.cuda.empty_cache()","deddce47":"def normalize_boxes(boxes, h0, w0):\n    boxes[:, 0] = boxes[:, 0] \/ w0\n    boxes[:, 1] = boxes[:, 1] \/ h0\n    boxes[:, 2] = boxes[:, 2] \/ w0\n    boxes[:, 3] = boxes[:, 3] \/ h0\n    return boxes\n\ndef denormalize_clip_boxes(boxes, h0, w0):\n    boxes[:, 0] = np.clip(boxes[:, 0] * w0, 0, w0-1)\n    boxes[:, 1] = np.clip(boxes[:, 1] * h0, 0, h0-1)\n    boxes[:, 2] = np.clip(boxes[:, 2] * w0, 0, w0-1)\n    boxes[:, 3] = np.clip(boxes[:, 3] * h0, 0, h0-1)\n    return boxes","a27a8907":"import sys\nsys.path.insert(0, \"..\/input\/weightedboxesfusion\")\nimport ensemble_boxes\n\niou_thr = 0.44\nskip_box_thr = 0.00001\n\npred_boxes_ensemble = []\npred_scores_ensemble = []\nfor (b00, b01, b02, b03, b04, b05, b06, b07, b08,\n     b10, b11, b12, b13, b14, b15, b16, b17, b18,\n     b20, b21, b22, b23, b24, b25, b26, b27, b28,\n     s00, s01, s02, s03, s04, s05, s06, s07, s08,\n     s10, s11, s12, s13, s14, s15, s16, s17, s18,\n     s20, s21, s22, s23, s24, s25, s26, s27, s28,\n     h0, w0) in zip(\n    tqdm(bifpn0_pred_boxes_0), \n    bifpn0_pred_boxes_0_lr, \n    bifpn0_pred_boxes_0_ud,\n    bifpn0_pred_boxes_l,\n    bifpn0_pred_boxes_l_lr,\n    bifpn0_pred_boxes_l_ud,\n    bifpn0_pred_boxes_x,\n    bifpn0_pred_boxes_x_lr,\n    bifpn0_pred_boxes_x_ud,\n    \n    bifpn1_pred_boxes_0, \n    bifpn1_pred_boxes_0_lr, \n    bifpn1_pred_boxes_0_ud,\n    bifpn1_pred_boxes_l,\n    bifpn1_pred_boxes_l_lr,\n    bifpn1_pred_boxes_l_ud,\n    bifpn1_pred_boxes_x,\n    bifpn1_pred_boxes_x_lr,\n    bifpn1_pred_boxes_x_ud,\n    \n    bifpn3_pred_boxes_0, \n    bifpn3_pred_boxes_0_lr, \n    bifpn3_pred_boxes_0_ud,\n    bifpn3_pred_boxes_l,\n    bifpn3_pred_boxes_l_lr,\n    bifpn3_pred_boxes_l_ud,\n    bifpn3_pred_boxes_x,\n    bifpn3_pred_boxes_x_lr,\n    bifpn3_pred_boxes_x_ud,\n    \n    \n    bifpn0_pred_scores_0,\n    bifpn0_pred_scores_0_lr, \n    bifpn0_pred_scores_0_ud,\n    bifpn0_pred_scores_l,\n    bifpn0_pred_scores_l_lr,\n    bifpn0_pred_scores_l_ud,\n    bifpn0_pred_scores_x,\n    bifpn0_pred_scores_x_lr,\n    bifpn0_pred_scores_x_ud,\n    \n    bifpn1_pred_scores_0,\n    bifpn1_pred_scores_0_lr, \n    bifpn1_pred_scores_0_ud,\n    bifpn1_pred_scores_l,\n    bifpn1_pred_scores_l_lr,\n    bifpn1_pred_scores_l_ud,\n    bifpn1_pred_scores_x,\n    bifpn1_pred_scores_x_lr,\n    bifpn1_pred_scores_x_ud,\n    \n    bifpn3_pred_scores_0,\n    bifpn3_pred_scores_0_lr, \n    bifpn3_pred_scores_0_ud,\n    bifpn3_pred_scores_l,\n    bifpn3_pred_scores_l_lr,\n    bifpn3_pred_scores_l_ud,\n    bifpn3_pred_scores_x,\n    bifpn3_pred_scores_x_lr,\n    bifpn3_pred_scores_x_ud,\n\n    h0_list,\n    w0_list):\n    \n    \n    boxes_list = [\n        normalize_boxes(b00, h0, w0).tolist(),\n        normalize_boxes(b01, h0, w0).tolist(),\n        normalize_boxes(b02, h0, w0).tolist(),\n        normalize_boxes(b03, h0, w0).tolist(),\n        normalize_boxes(b04, h0, w0).tolist(),\n        normalize_boxes(b05, h0, w0).tolist(),\n        normalize_boxes(b06, h0, w0).tolist(),\n        normalize_boxes(b07, h0, w0).tolist(),\n        normalize_boxes(b08, h0, w0).tolist(),\n        \n        normalize_boxes(b10, h0, w0).tolist(),\n        normalize_boxes(b11, h0, w0).tolist(),\n        normalize_boxes(b12, h0, w0).tolist(),\n        normalize_boxes(b13, h0, w0).tolist(),\n        normalize_boxes(b14, h0, w0).tolist(),\n        normalize_boxes(b15, h0, w0).tolist(),\n        normalize_boxes(b16, h0, w0).tolist(),\n        normalize_boxes(b17, h0, w0).tolist(),\n        normalize_boxes(b18, h0, w0).tolist(),\n        \n        normalize_boxes(b20, h0, w0).tolist(),\n        normalize_boxes(b21, h0, w0).tolist(),\n        normalize_boxes(b22, h0, w0).tolist(),\n        normalize_boxes(b23, h0, w0).tolist(),\n        normalize_boxes(b24, h0, w0).tolist(),\n        normalize_boxes(b25, h0, w0).tolist(),\n        normalize_boxes(b26, h0, w0).tolist(),\n        normalize_boxes(b27, h0, w0).tolist(),\n        normalize_boxes(b28, h0, w0).tolist()\n    ]\n    \n    scores_list = [\n        s00.tolist(),\n        s01.tolist(),\n        s02.tolist(),\n        s03.tolist(),\n        s04.tolist(),\n        s05.tolist(),\n        s06.tolist(),\n        s07.tolist(),\n        s08.tolist(),\n\n        s10.tolist(),\n        s11.tolist(),\n        s12.tolist(),\n        s13.tolist(),\n        s14.tolist(),\n        s15.tolist(),\n        s16.tolist(),\n        s17.tolist(),\n        s18.tolist(),\n        \n        s20.tolist(),\n        s21.tolist(),\n        s22.tolist(),\n        s23.tolist(),\n        s24.tolist(),\n        s25.tolist(),\n        s26.tolist(),\n        s27.tolist(),\n        s28.tolist()\n    ]\n    \n    labels_list = [\n        [0] * len(b00),\n        [0] * len(b01),\n        [0] * len(b02),\n        [0] * len(b03),\n        [0] * len(b04),\n        [0] * len(b05),\n        [0] * len(b06),\n        [0] * len(b07),\n        [0] * len(b08),\n        \n        [0] * len(b10),\n        [0] * len(b11),\n        [0] * len(b12),\n        [0] * len(b13),\n        [0] * len(b14),\n        [0] * len(b15),\n        [0] * len(b16),\n        [0] * len(b17),\n        [0] * len(b18),\n        \n        [0] * len(b20),\n        [0] * len(b21),\n        [0] * len(b22),\n        [0] * len(b23),\n        [0] * len(b24),\n        [0] * len(b25),\n        [0] * len(b26),\n        [0] * len(b27),\n        [0] * len(b28)\n    ]\n    \n    boxes, scores, _ = ensemble_boxes.ensemble_boxes_wbf.weighted_boxes_fusion(boxes_list, scores_list, labels_list, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    pred_boxes_ensemble.append(boxes)\n    pred_scores_ensemble.append(scores)","ccaaca38":"pred_boxes_ensemble = [denormalize_clip_boxes(a, h0, w0) for a, h0, w0 in zip(pred_boxes_ensemble, h0_list, w0_list)]\npred_scores_ensemble = [a for a in pred_scores_ensemble]","92ae50a0":"# visualization\nidx = -7\nimg = testdataset[idx][2]\nprint(testdataset[idx][1])\nvisualize({'image': img, 'bboxes': (pred_boxes_ensemble[idx]).astype(int), 'scores': pred_scores_ensemble[idx]})","4a05514e":"def format_prediction_string(boxes, scores):\n    pred_strings = []\n    for s, b in zip(scores, boxes.astype(int)):\n        # xmin, ymin, w, h\n        pred_strings.append(f'{s:.4f} {b[0]} {b[1]} {b[2]} {b[3]}')\n    #print(\" \".join(pred_strings))\n    return \" \".join(pred_strings)","f981b185":"pred_strs = []\nfor bboxes, scores in zip(pred_boxes_ensemble, pred_scores_ensemble):\n    \n    if len(bboxes) > 0:\n        \n        bboxes[:, 2] -= bboxes[:, 0]\n        bboxes[:, 3] -= bboxes[:, 1]\n        bboxes = bboxes.round()\n\n        pred_strs.append(format_prediction_string(bboxes, scores))\n        \n    else:\n        pred_strs.append('')","40a49ccc":"test_df = pd.DataFrame({'image_id': img_ids, 'PredictionString':pred_strs})\ntest_df","61e3bb70":"test_df.to_csv('submission.csv', index=False)","fa67c260":"## Load Fold 0 weights","8c5dfed6":"# Import dependencies","131481b2":"# Configuration","d75fbfad":"# On BiFPN model","cc54abce":"## Ensemble","01864a4f":"# Generate Submission file","71a53f7b":"# Preapre labels","4c1bdcca":"# Import more dependencies","fa2797ef":"# Install packages","01117bea":"## Load Fold 3 weights","760c14dd":"## Load Fold 1 weights","5d640feb":"# Visualization helpers","36cd6e28":"# Define test dataset","2be2e8a6":"# Inference"}}