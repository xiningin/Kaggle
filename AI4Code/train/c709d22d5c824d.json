{"cell_type":{"9c40f240":"code","d383c8d9":"code","1b053491":"code","351af2bf":"code","df6b6385":"code","5442f1d3":"code","31f5cff9":"code","4b02fadd":"code","7334c909":"code","2bc83670":"code","e3ab2670":"code","0a2a74a1":"code","8ac82572":"markdown","9fa1cad3":"markdown","144ba38a":"markdown","9b454597":"markdown","2e929527":"markdown","277a7dcc":"markdown"},"source":{"9c40f240":"# Install required modules only once\nimport sys\nimport subprocess\nimport pkg_resources\n\nrequired = {'iterative-stratification', 'pytorch-lightning'}\ninstalled = {pkg.key for pkg in pkg_resources.working_set}\nmissing = required - installed\n\nif missing:\n    python = sys.executable\n    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)","d383c8d9":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nimport torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport os\nimport optuna\nimport torch.nn.functional as F\nfrom optuna.integration import PyTorchLightningPruningCallback","1b053491":"test_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\ntrain_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","351af2bf":"# From https:\/\/github.com\/eladhoffer\/utils.pytorch\/blob\/master\/cross_entropy.py\ndef binary_cross_entropy(inputs, target, weight=None, reduction='mean', smooth_eps=None, from_logits=False):\n    \"\"\"cross entropy loss, with support for label smoothing https:\/\/arxiv.org\/abs\/1512.00567\"\"\"\n    smooth_eps = smooth_eps or 0\n    if smooth_eps > 0:\n        target = target.float()\n        target.add_(smooth_eps).div_(2.)\n    if from_logits:\n        return F.binary_cross_entropy_with_logits(inputs, target, weight=weight, reduction=reduction)\n    else:\n        return F.binary_cross_entropy(inputs, target, weight=weight, reduction=reduction)\n\n\ndef binary_cross_entropy_with_logits(inputs, target, weight=None, reduction='mean', smooth_eps=None, from_logits=True):\n    return binary_cross_entropy(inputs, target, weight, reduction, smooth_eps, from_logits)\n\n\nclass BCELoss(nn.BCELoss):\n    def __init__(self, weight=None, size_average=None, reduce=None, reduction='mean', smooth_eps=None, from_logits=False):\n        super(BCELoss, self).__init__(weight, size_average, reduce, reduction)\n        self.smooth_eps = smooth_eps\n        self.from_logits = from_logits\n\n    def forward(self, input, target):\n        return binary_cross_entropy(input, target,\n                                    weight=self.weight, reduction=self.reduction,\n                                    smooth_eps=self.smooth_eps, from_logits=self.from_logits)\n\n\nclass BCEWithLogitsLoss(BCELoss):\n    def __init__(self, weight=None, size_average=None, reduce=None, reduction='mean', smooth_eps=None, from_logits=True):\n        super(BCEWithLogitsLoss, self).__init__(weight, size_average,\n                                                reduce, reduction, smooth_eps=smooth_eps, from_logits=from_logits)","df6b6385":"class MoADataset(Dataset):\n    \n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return self.features.shape[0]\n        \n    def __getitem__(self, index):\n        return {\n            \"x\": torch.tensor(self.features[index, :], dtype=torch.float),\n            \"y\": torch.tensor(self.targets[index, :], dtype=torch.float)\n        }\n    ","5442f1d3":"class MoADataModule(pl.LightningDataModule):\n    def __init__(self, batch_size=2048, fold= 0):\n        super().__init__()\n        self.batch_size = batch_size\n        self.fold = fold\n        \n    def prepare_data(self):\n        # Even in multi-GPU training. this method is called only from a single GPU. \n        # So this method ideal for download, stratification etc. \n        # Startification on multi-label dataset is tricky. \n        # scikit-learn stratified KFold cannot be used. \n        # So we are using interative-stratification\n        if os.path.isfile(\"train_folds.csv\"):\n            return\n        complete_training_data = self._read_data()        \n        self._startify_and_save(complete_training_data)        \n        \n    def _read_data(self):\n        features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\n        # Convert categorical features into OHE\n        features = pd.concat([features, pd.get_dummies(features['cp_time'], prefix='cp_time')], axis=1)\n        features = pd.concat([features, pd.get_dummies(features['cp_dose'], prefix='cp_dose')], axis=1)\n        features = pd.concat([features, pd.get_dummies(features['cp_type'], prefix='cp_type')], axis=1)\n        # Delete original categorical features\n        features = features.drop(['cp_time', 'cp_dose', 'cp_type'], axis=1)\n        targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\n        merged = features.merge(targets_scored, how=\"inner\", on=\"sig_id\")\n        return merged\n        \n    def _startify_and_save(self, data):\n        # New column to hold the fold number\n        data.loc[:, \"kfold\"] = -1\n\n        # Shuffle the dataframe\n        data = data.sample(frac=1).reset_index(drop=True)        \n        \n        # 5 Folds\n        mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=False, random_state=None) \n        # trn_ and val_ are indices\n        targets = data.drop(['kfold', 'sig_id'], axis=1)                                                                                       \n        for fold_, (trn_,val_) in enumerate(mskf.split(X=data, y=targets.iloc[:, 879:])): \n            # We are just filling the vaidation indices. \n            # All other data are for training (trn indices are not required)\n            data.loc[val_, \"kfold\"] = fold_\n    \n        # We are saving the result to the disk so that other GPUs can pick it from there. \n        # Rather if we do \"self.startified_data = train_targets_scored\", \n        # other GPUs will not be able to read this \n        data.to_csv(\"train_folds.csv\", index=False)   \n        \n    def setup(self, stage=None):\n        # In multi-GPU training, this method is run on each GPU. \n        # So ideal for each training\/valid split\n        data = pd.read_csv(\"train_folds.csv\")\n        \n        training_data = data[data.kfold != self.fold]\n        training_data = training_data.drop(['kfold', 'sig_id'], axis=1)\n        validation_data = data[data.kfold == self.fold]\n        validation_data = validation_data.drop(['kfold', 'sig_id'], axis=1)\n        self.train_dataset = MoADataset(training_data.iloc[:, :879].values, training_data.iloc[:, 879:].values)\n        self.valid_dataset = MoADataset(validation_data.iloc[:, :879].values, validation_data.iloc[:, 879:].values)        \n\n    \n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, self.batch_size, num_workers=4, shuffle=True)\n    \n    def val_dataloader(self):\n        return DataLoader(self.valid_dataset, self.batch_size, num_workers=4, shuffle=False)    \n            \n        ","31f5cff9":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, trial):\n        super().__init__()\n        n_layers = 5  # Let us not play with this\n        f_dropout = trial.suggest_float('f_dropout', 0.2, 0.5)\n        LAYER_OUTPUTS = [2048, 4096, 2048, 1024, 512]\n        layers = []\n\n        # Intermediate layers\n        in_size = 879   \n        for i in range(n_layers):\n            out_size = LAYER_OUTPUTS[i]\n#             out_size = trial.suggest_int('n_units_{}'.format(i), 256, 4096)\n            layers.append(torch.nn.Linear(in_size, LAYER_OUTPUTS[i], bias=False))\n            layers.append(nn.BatchNorm1d(out_size))\n            layers.append(nn.Dropout(f_dropout))\n            layers.append(nn.PReLU())\n            in_size = out_size\n\n        # Final layer\n        layers.append(torch.nn.Linear(in_size, 206))\n    \n        self.model = torch.nn.Sequential(*layers)\n        \n        # Initialize weights\n        self.model.apply(self._init_weights)\n        \n    def _init_weights(self, m):\n        if type(m) == nn.Linear:\n            nn.init.xavier_uniform_(m.weight)\n            if m.bias != None:\n                m.bias.data.fill_(0.01)\n        \n    def forward(self, x):\n        x = self.model(x)\n        return x","4b02fadd":"class PLitMoAModule(pl.LightningModule):\n    def __init__(self, hparams, trial):\n        super(PLitMoAModule, self).__init__()\n        self.hparams = hparams\n        self.hparams[\"lr\"] = trial.suggest_float('lr',\n                                             1e-5, 1e-2, log=True)\n        self.model= Model(879, 206, trial) # Input Features, Output Targets\n        \n#         smoothing_factor = trial.suggest_float('smoothing_factor', 0.01, 0.2)\n#         self.tr_criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n        self.criterion = nn.BCEWithLogitsLoss()\n        \n    def forward(self, x):\n        return self.model(x)\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.hparams[\"lr\"])\n        scheduler = {\"scheduler\": \n                     torch.optim.lr_scheduler.ReduceLROnPlateau(\n                        optimizer, patience=2, \n                        threshold=0.0001, \n                        mode='min', verbose=True),\n                    \"interval\": \"epoch\",\n                    \"monitor\": \"val_loss\"}\n        return [optimizer], [scheduler]\n    \n    def training_step(self, batch, batch_index):\n        features = batch['x']\n        targets = batch['y']\n        out = self(features)\n        loss = self.criterion(out, targets)\n        logs = {\"train_loss\" : loss}\n        return {\"loss\": loss, \"log\": logs, \"progress_bar\": logs}\n    \n    def training_epoch_end(self, outputs):\n        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n        logs = {\"train_loss\": avg_loss}\n        return {\"log\": logs, \"progress_bar\": logs}\n            \n    def validation_step(self, batch, batch_index):\n        features = batch['x']\n        targets = batch['y']\n        out = self(features)\n        loss = self.criterion(out, targets)\n        logs = {\"val_loss\" : loss}\n        return {\"loss\": loss, \"log\": logs, \"progress_bar\": logs}\n    \n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n        logs = {\"val_loss\": avg_loss}\n        return {\"log\": logs, \"progress_bar\": logs}","7334c909":"# train_targets_positive = train_targets_scored.sum()[1:]\n# train_targets_negative = train_targets_scored.shape[0] - train_targets_scored.sum()[1:]\n# pos_weight = train_targets_negative\/train_targets_positive   \n# pos_weight = torch.tensor(pos_weight)  ","2bc83670":"def objective(trial):\n\n    metrics_callback = MetricsCallback()\n    trainer = pl.Trainer(\n        logger=False,\n        max_epochs=50,\n        gpus=-1 if torch.cuda.is_available() else None,\n        callbacks=[metrics_callback],\n        checkpoint_callback=False, # Do not save any checkpoints\n        early_stop_callback=PyTorchLightningPruningCallback(trial, monitor=\"val_loss\"),\n    )  \n    \n#     model = PLitMoAModule(hparams={\"lr\":1e-3}, trial=trial, pos_weight=pos_weight)\n    model = PLitMoAModule(hparams={\"lr\":1e-3}, trial=trial)\n    dm = MoADataModule(fold=1)\n    trainer.fit(model, dm)\n\n    return metrics_callback.metrics[-1][\"val_loss\"].item()","e3ab2670":"from pytorch_lightning import Callback\nclass MetricsCallback(Callback):\n    \"\"\"PyTorch Lightning metric callback.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.metrics = []\n\n    def on_validation_end(self, trainer, pl_module):\n        self.metrics.append(trainer.callback_metrics)","0a2a74a1":"pruner = optuna.pruners.MedianPruner() \n# pruner = optuna.pruners.NopPruner()\n\nstudy = optuna.create_study(direction=\"minimize\", pruner=pruner)\nstudy.optimize(objective, n_trials=100, gc_after_trial=True, timeout=None)\n\nprint(\"Number of finished trials: {}\".format(len(study.trials)))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","8ac82572":"### Lightning Model","9fa1cad3":"### Dataset\nDataset is used by the Pytorch framework to read data, and find total length. This is where we convert data into Pytorch Tensors.","144ba38a":"https:\/\/github.com\/optuna\/optuna\/blob\/master\/examples\/pytorch_lightning_simple.py","9b454597":"### Optimize","2e929527":"### NN Model","277a7dcc":"### Datamodule"}}