{"cell_type":{"da2dcc37":"code","6ac8f299":"code","bcbe649b":"code","28534ca9":"code","dabc3768":"code","9d394e83":"code","ba032536":"code","a85ffcea":"code","749f525e":"code","f0c7e323":"code","b57f6261":"code","fc572f7b":"code","937aeee9":"code","ff309b20":"code","f0bcf508":"code","50751077":"code","dbd081fa":"code","0e62daaf":"code","21a7aa49":"code","9b896e1f":"markdown","95356800":"markdown","e113d1ed":"markdown","6391324b":"markdown","7588f438":"markdown","488ffb03":"markdown","27be844a":"markdown","8ade80f4":"markdown","f04ddb6c":"markdown","e82e11e8":"markdown","0e1aafcb":"markdown","1176c4fb":"markdown","94a615ff":"markdown","875b2462":"markdown","bef11fe3":"markdown","eebad9ac":"markdown","7fa4517d":"markdown","3fc7fce4":"markdown"},"source":{"da2dcc37":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.feature_selection import f_classif\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","6ac8f299":"train_data = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv', index_col='id')\ntest_data = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv', index_col='id')\nsample_submission = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/sample_submission.csv')","bcbe649b":"train_data.info()","28534ca9":"test_data.info()","dabc3768":"train_data.head()","9d394e83":"test_data.head()","ba032536":"categorical_cols = [col for col in train_data.columns if 'cat' in col]\ncontinuous_cols = [col for col in train_data.columns if 'cont' in col]\n\nprint(len(categorical_cols), len(continuous_cols))","a85ffcea":"sample_submission.head()","749f525e":"train_data.describe()","f0c7e323":"test_data.describe()","b57f6261":"for col in categorical_cols:\n    print(train_data[col].value_counts().sort_index())","fc572f7b":"for col in categorical_cols:\n    print(test_data[col].value_counts().sort_index())","937aeee9":"encoder = OrdinalEncoder()\ntrain_data[categorical_cols] = encoder.fit_transform(train_data[categorical_cols])\ntest_data[categorical_cols] = encoder.transform(test_data[categorical_cols])","ff309b20":"corr = train_data.corr()\ncorr","f0bcf508":"# Creating a correlation plot of size 20x20\nfig, ax = plt.subplots(figsize=(10, 10))\n\nplt.title(\"Correlation Plot\")\nsns.heatmap(corr,\n            cmap=sns.diverging_palette(230, 10, as_cmap=True),\n            square=True,\n            ax=ax)\nplt.show()","50751077":"train_data[continuous_cols+['target']].corr()['target']","dbd081fa":"train_data[continuous_cols+['target']].corr(method='spearman')['target']","0e62daaf":"f_classif(train_data[categorical_cols], train_data['target'])[0]","21a7aa49":"train_data[categorical_cols+['target']].corr(method='kendall')['target']","9b896e1f":"As evident from the above results there is not much correlation between features and the target but still we can use the above values to try if removing any features from our data gives better results.","95356800":"# Finding correlation between features and target","e113d1ed":"## Correlation with target for categorical features","6391324b":"The percentile values, mean and std are similar for all features for both datasets. Thus, we can conclude that the training data is a good representation of the test data and it might not be necessary to apply regularization in the models.","7588f438":"There are no null values in the dataset.","488ffb03":"The scale on the right of the plot shows that values close to 1 are plotted as red.\n\nThe only red boxes in the plot are along the diagonal which is expected as each feature will definitely be like itself. So, correlation between a feature with itself will be equal to 1.\n\nThe whole plot is blue, which suggests that there is less correlation between features. Thus, we can conclude that none of the features are redundant.\n\nSo, all the features should be used for model training and feature reduction techniques will do harm to model accuracy.","27be844a":"# Finding Redundant features\n\nI will use Pearson's correlation coefficient to find correlation between features and will remove features, if any, with high correlation values.","8ade80f4":"# Initialization","f04ddb6c":"There are 24 columns in the dataset, 10 are categorical and 14 are continuous.","e82e11e8":"The number of values in features vary from 2 to 15. Since the number of categories in features is high I use the OrdinalEncoder. Another approach would be to use OrdinalEncoder for last four features will a lot of categories and OneHotEncoder for others.\n\nAnother thing to notice is that there are no categories in test data which are not previously observed in training data.","0e1aafcb":"I will use the information provided in ML Mastery article [How to Choose a Feature Selection Method For Machine Learning](https:\/\/machinelearningmastery.com\/feature-selection-with-real-and-categorical-data\/) to find correlation between target and features.","1176c4fb":"In this notebook I will try to gain insights from data. This notebook will help in later deciding what model would be best for this problem.\n\nI would love to hear feedback on how to improve this further.","94a615ff":"# Checking basic data info\n\nFirst, I will check for basic info which can be gathered from data.","875b2462":"# Encoding Categorical features\n\nFirst, I will find the number of categories in features. If the number of categories per feature is less then I will be using OneHotEncoding otherwise I will use OrdinalEncoding.","bef11fe3":"The submission file must contain only two columns, id and target.","eebad9ac":"## Correlation with target for numerical features\n\nI will use Pearson's correlation coefficient to find linear correlation between numerical features and target and Spearman's correlation coefficient to find non-linear correlation.","7fa4517d":"# Numerical features","3fc7fce4":"Since the correlation matrix is big I will create a correlation plot to visualize the results."}}