{"cell_type":{"be64a55c":"code","774d02fd":"code","9c9c3f2a":"code","3ed07acc":"code","bebd728f":"code","b105c891":"code","4e4bf0b1":"code","1fb332c6":"code","9efee331":"code","95f56048":"code","68ef24f3":"code","31796d29":"code","2a7ae8a9":"code","612b5716":"code","eac333b7":"code","9cd0c92a":"code","1eae7312":"code","b0a1c2d8":"code","ea20b528":"code","efb43c09":"code","7bce5bdb":"code","ec3aa497":"code","e784bb3e":"code","0dfb2430":"code","d02e4636":"code","11b121f1":"code","d3a54e08":"markdown","838b5c15":"markdown","000db6b7":"markdown","9c7166c4":"markdown","c3cbd17c":"markdown"},"source":{"be64a55c":"!pip install lmfit --quiet\n!pip install numdifftools --quiet","774d02fd":"import numpy as np\nimport glob\nimport copy\nfrom lmfit import minimize, Parameters\nimport scipy.optimize\nimport plotly.graph_objects as go\nfrom uncertainties import ufloat\nfrom uncertainties import unumpy as unp","9c9c3f2a":"files = glob.glob('\/kaggle\/input\/*\/*.dat')\nfiles","3ed07acc":"def get_uncertainty(rawdata):\n    \"\"\"\n    From the rawdata provided make an estimate of the uncertainty for each pair of attenuator settings\n    Use the std, but if it is too small, use an estimate based on assuming a uniform distribution\n    on a quantizer.   This works for ANDO power meters because they only have 4 digit resolution\n    on the high sensitivity (measure low optical power) ranges.\n    \"\"\"\n    Nfit = rawdata.shape[0]\n    N = rawdata.shape[1]\n    std = rawdata.std(axis=1, ddof=1)\n    avg = rawdata.mean(axis=1)\n    min_unc = np.zeros(avg.shape)\n\n    # Use estimate quantization error as lower bound\n    #   this works for the ando power meters we are using to monitor\n    min_unc[avg > 1e-9] = 1e-12 * 0.5 \/ (3**0.5)\n    min_unc[avg > 1e-6] = 1e-9 * 0.5 \/ (3**0.5)\n    min_unc[avg > 1e-3] = 1e-6 * 0.5 \/ (3**0.5)\n\n    # replace std with uncertainty from a uniform distribution in the\n    # quantization of the power meter if the std is too small\n    unc = np.where(std < min_unc, min_unc, std)\n    return unc","bebd728f":"data = np.loadtxt(files[0])\n# column format of text file... there are 7 columns\n# time, 1, monitor power, tau_setting, att_setting, range, power_reading\n#\n# The data is also taken in blocks of N at each setting...\n#\n# At the end of the file, there may be extra data to look for drift\n#\n\n\nrngIdx = -2  # Use to be -2, when there was no \"splitter\" measurements\ntauIdx = -3  # column number which indicates the setting for \"tau\"\nattIdx = -4  # column for att setting\npowIdx = -1\n\n# determine N from the data file... this is the number measurements at an attenuator pair setting\nN = (data[:, tauIdx] > 0).nonzero()[0][0]\n\n# delete extra data at the end of the file that could be used to check for drift\nextra = data.shape[0] % (2*N)\ndata = data[:-extra,:]\n\nranges = np.unique(data[:,rngIdx])\nranges.sort()\nranges = ranges[::-1].astype(int)\n# create a dictionary that contains the data.  The keys are the ranges of the power meter (-10, -20, ...)\nd = {}\nfor rng in ranges:\n    d[rng]={}\n    d[rng]['v'] = data[(data[:,rngIdx] == rng) & (data[:,tauIdx]==0),powIdx].reshape((-1,N))\n    d[rng]['vt'] = data[(data[:,rngIdx] == rng) & (data[:,tauIdx]==3),powIdx].reshape((-1,N))\n    d[rng]['att'] = data[(data[:,rngIdx] == rng) & (data[:,tauIdx]==3),attIdx][::N]\n    d[rng]['vstd'] = get_uncertainty(d[rng]['v'])\n    d[rng]['vtstd'] = get_uncertainty(d[rng]['vt'])\n    d[rng]['range'] = rng\n    d[rng]['N'] = N\n# make a copy of the dictionary... But, have it store just the avg values instead of all the readings\ndavg = copy.deepcopy(d)\nfor rng in ranges:\n    davg[rng]['v'] = davg[rng]['v'].mean(axis=1)\n    davg[rng]['vt'] = davg[rng]['vt'].mean(axis=1)\n    davg[rng]['v+unc'] = unp.uarray(davg[rng]['v'], davg[rng]['vstd'])\n    \nfig = go.Figure()\nfig.add_scatter(y=data[:,2])\nfig.update_layout(\n    title=\"Check Laser drift\",\n    xaxis_title=\"sample number\/time\",\n    yaxis_title=r\"Power from att1\",\n    legend_title=\"\",\n)\n\nfig.show()","b105c891":"# create the parameters to do a fit over 1 power meter range... \n#.  Using the lmfit package\ndef init_params(N=3):\n    \"\"\"\n    Create parameters used by lmfit... This is for data on a single range\n    \"\"\"\n    params = Parameters()\n    params.add('tau', value=1) # using fixed and unknown attenuator technique\n    for i in range(2, (N+1)):\n        params.add(f'b{i}',1)\n    return params\n\n# create the parameters to do a fit over all ranges in the files\ndef init_params_ranges(N=[], ranges=[]):\n    \"\"\"\n    Create a new set of parameters for fitting... this will cover all ranges simultaneously\n    e.g.\n        parameter b102 is the v**2 coefficeint for the -10 range\n        parameter b303 is the v**3 coefficient for the -30 range\n    \"\"\"\n    params = Parameters()\n    params.add('tau', value=0.5)\n    #params.add('tau', value=0.5062319209579774, vary=False)#, min=0, max=1) # using fixed and unknown attenuator technique\n    N_idx = 0\n    for rng in ranges:\n        for i in range(2, (N[N_idx]+1)):\n            params.add(f\"b{-rng}{i}\",0)#, min=-1e6, max=1e6)\n        N_idx += 1\n    return params","4e4bf0b1":"# Function that uses parameters from a fit to calculate the normalized conversion function\n#.  for an array of readings\ndef P(params, v):\n    \"\"\"\n    Compute linearized power p (normalized conversion function) given the parameters of the polynomial and the readings v\n    \"\"\"\n    out = v+0;  # make a new copy of v for out\n    #out = np.array(v) # make a new copy of v four out\n    k=2\n    name = f'b{k}'\n    while name in params:\n        out += params[name]*(v**k)\n        k += 1\n        name = f'b{k}'\n    return out\n\n#  Compute the first derivative of the normalized conversion function for an array of readings\ndef dP(params, v):\n    \"\"\"\n    Compute dP\/dv\n    \"\"\"\n    out = 1;\n    k=2\n    name = f'b{k}'\n    while name in params:\n        out += k * params[name]*(v**(k-1))\n        k += 1\n        name = f'b{k}'\n    return out","1fb332c6":"# Compute residuals from polynomial fit to data over one range\n#.  weight the residuals by estimat of uncertainty\ndef residuals(params, v, vt, eps):\n  #  assumes params is an lmfit Parameters\n  #  v and vt are numpy.arrays\n  #  eps two column vector with uncertainties for v and vt as a column\n  out = vt - params['tau']*v\n  k=2\n  name = f'b{k}'\n  while name in params:\n    out += params[name]*(vt**k - params['tau'] * (v**k))\n    k += 1\n    name = f'b{k}'\n  out \/= (eps[:,1]**2 + (params['tau']*eps[:,0])**2)**0.5\n  return out\n\n# compute residuals from polynomial fit to data over range. \n#.  Use more complete calculation of uncertainty using the derivative\ndef residuals2(params, v, vt, eps):\n  #  assumes params is an lmfit Parameters\n  #  v and vt are numpy.arrays\n  #  eps two column vector with uncertainties for v and vt as a column\n  out = vt - params['tau']*v\n  k=2\n  name = f'b{k}'\n  while name in params:\n    out += params[name]*(vt**k - params['tau'] * (v**k))\n    k += 1\n    name = f'b{k}'\n  unc = (dP(params, vt)**2)*(eps[:,1]**2) + ((params['tau']*eps[:,0])**2)*(dP(params, v)**2)\n  out \/= unc**0.5\n  return out","9efee331":"for rng in ranges:\n    nrows = len(d[rng]['att'])\n    v = davg[rng]['v']\n    vt = davg[rng]['vt']\n    eps = np.column_stack([d[rng]['vstd'], d[rng]['vtstd']])\n\n    for i in range(1, 6):\n        params = init_params(i)\n        fit = minimize(residuals, params, args=(v, vt, eps), method='powell')\n        print(i, fit.redchi, fit.aic, fit.bic)\n        #fit = minimize(residuals2, params, args=(v, vt, eps), method='leastsq')\n        #print(i, fit.redchi, fit.aic, fit.bic)\n        if i==1:\n            best = i\n            best_redchi = fit.redchi\n        elif fit.redchi < best_redchi:\n            best = i\n            best_redchi = fit.redchi\n            \n    print('*'*10, rng, best, best_redchi)\n    d[rng]['order'] = best\n    davg[rng]['order'] = best","95f56048":"weighted = 0\nweights = 0\ntaus = []\nfor rng in ranges:\n    v = davg[rng]['v']\n    vt = davg[rng]['vt']\n    eps = np.column_stack([d[rng]['vstd'], d[rng]['vtstd']])\n\n    params = init_params(davg[rng]['order'])\n    fit = minimize(residuals, params, args=(v, vt, eps), method='powell', calc_covar=True)\n    print(fit.params['tau'], fit.redchi)\n    fit = minimize(residuals, fit.params, args=(v, vt, eps), method='leastsq', calc_covar=True)\n    print(fit.params['tau'], fit.redchi)\n    taus.append(fit.params['tau'].value)\n    weighted += fit.params['tau'].value \/  ((fit.params['tau'].stderr)**2)\n    weights += 1 \/  ((fit.params['tau'].stderr)**2)\nweighted\/weights, 1\/weights**0.5, np.array(taus).mean(), np.array(taus).std()","68ef24f3":"for rng in ranges:\n    v = davg[rng]['v']\n    vt = davg[rng]['vt']\n    eps = np.column_stack([d[rng]['vstd'], d[rng]['vtstd']])\n\n    for i in range(1, 8):\n        params = init_params(i)\n        params['tau'].vary = False\n        params['tau'].value = weighted\/weights\n        fit = minimize(residuals, params, args=(v, vt, eps), method='leastsq')\n        print(i, fit.redchi, fit.aic, fit.bic)\n        if i==1:\n            best = i\n            best_redchi = fit.redchi\n        elif fit.redchi < best_redchi:\n            best = i\n            best_redchi = fit.redchi\n            \n    #print(rng, best, best_redchi, fit.params['tau'])\n    d[rng]['order'] = best\n    \n    params = init_params(d[rng]['order'])\n    params['tau'].vary = False\n    params['tau'].value = weighted\/weights\n    fit = minimize(residuals, params, args=(v, vt, eps), method='leastsq')\n    print('*'*10, rng, best, fit.redchi, fit.aic, fit.bic, fit.params)\n    davg[rng]['fit']=fit","31796d29":"#. Computes the residuals that will be minimized in a least square sense for fitting all the ranges\n#    Of the power meter at once for a common \\tau.  \n#    I was lazy and use a global variable that contains all the data we are fitting over\n#    The output is a large array that would be an array of 0's if you could fit the data perfectly.\n#.   Otherwise, it is essentially the difference between what we are fitting to and the output of \n#.   of the polynomial that is trying to fit the data.\n#\n#.  Theses errors\/residuals will be squared, summed, and minimized by the lmfit package to \n#.    to find the best fit\n\ndef residuals3(params):\n    #  assumes params is an lmfit Parameters\n    #  get raw data via global variable davg\n    global davg\n    results = []\n    for rng in davg.keys():  # Loop over each power meter range\n        v = davg[rng]['v']\n        vt = davg[rng]['vt']\n        vunc = davg[rng]['vstd']\n        vtunc = davg[rng]['vtstd']\n        k = 2\n        out = vt - params['tau']*v\n        name = f'b{k-rng*10}'\n        #print(name)\n        while name in params:\n            #print(name,params[name])\n            out += params[name]*(vt**k - params['tau'] * (v**k))\n            k += 1\n            name = f'b{k-rng*10}'\n        # Estimate uncertainty for out and divide the residual by it.\n        #.  estimate is rough 1st order estimate\n        out \/= (vtunc**2 + (params['tau']*vunc)**2)**0.5\n        results.append(out)  # Append errors to the list or errors for previous ranges\n    results = np.hstack(results)\n    return results","2a7ae8a9":"# First create a list of parameters that we will fit over\n#   The first parameter is a list of the order of polynomial that will be used\n#.  The second parameter is an array of the ranges we will fit over\n#.   The length of the two need to be the same.\n#.  [4,3,2,2,2,2]  and [-10, -20, -30, -40, -50, -60]\n#.  polyomial of order 4 will used on the -10 range\n#.  polynomial of order 3 will be used for the -20 range\n#.  etcc..\nparams = init_params_ranges([4, 3, 2, 2, 2, 2], ranges)\n\n#. Perform a fit of the requested orders using the lmit package...\nfit = minimize(residuals3, params, method='leastsq')\nfit","612b5716":"#. A different function that we will use to calculate the redchi... This will be minimized to\n#.  To determine the optimum order to use for each range.\n#\n#   We can use one of the scipy.optimize methods for the optimization\n\ndef redchi(N_list):\n    N_list = N_list.astype(int)\n    params = init_params_ranges(N_list, ranges)\n    fit = minimize(residuals3, params, method='leastsq')\n    return fit.redchi","eac333b7":"N_fit = scipy.optimize.differential_evolution(redchi, bounds=[(1,5)]*6)\nN_list = N_fit.x.astype(int)\nN_fit, N_list","9cd0c92a":"#N_list = np.array([3, 3, 3, 3, 2, 2])\nparams = init_params_ranges(N_list, ranges)\nfit = minimize(residuals3, params, method='leastsq')\nfit","1eae7312":"from uncertainties import ufloat\nfrom uncertainties import unumpy as unp","b0a1c2d8":"def P_range(params, rng, v):\n    #  assumes params is an lmfit Parameters\n    global davg\n    results = []\n    k = 2\n    out = v + 0\n    name = f'b{k-rng*10}'\n    while name in params:\n        out += params[name]*(v**k)\n        k += 1\n        name = f'b{k-rng*10}'\n    return out\n\ndef P_range_unc(params, rng, v):\n    #  assumes params is an lmfit Parameters\n    global davg\n    results = []\n    k = 2\n    out = v + 0\n    name = f'b{k-rng*10}'\n    while name in params:\n        coeff = ufloat(params[name].value, params[name].stderr)\n        out += coeff*(v**k)\n        #print(coeff*(v**k))\n        k += 1\n        name = f'b{k-rng*10}'\n    return out","ea20b528":"r_list = []\nfor rng in [-20, -30, -40, -50, -60]:\n    overlap = set(davg[rng]['att']).intersection(davg[rng+10]['att'])\n    idx1 = [list(davg[rng]['att']).index(x) for x in overlap]\n    idx2 = [list(davg[rng+10]['att']).index(x) for x in overlap]\n    \n    #r = P_range(fit.params, rng+10, davg[rng+10]['v'][idx2]) \/ P_range(fit.params, rng, davg[rng]['v'][idx1])\n    r = P_range(fit.params, rng+10, np.hstack([davg[rng+10]['v'][idx2], davg[rng+10]['vt'][idx2]])) \/ \\\n    P_range(fit.params, rng, np.hstack([davg[rng]['v'][idx1], davg[rng]['vt'][idx1]]))\n    r_list.append(ufloat(r.mean(), r.std()))","efb43c09":"r_list, np.array(r_list).cumprod()","7bce5bdb":"# for rng in ranges:\n#     #print(rng)\n#     davg[rng]['v+unc'] = unp.uarray(davg[rng]['v'], davg[rng]['vstd'])\n\nr_list = []\nfor rng in [-20, -30, -40, -50, -60]:\n    overlap = set(davg[rng]['att']).intersection(davg[rng+10]['att'])\n    idx1 = [list(davg[rng]['att']).index(x) for x in overlap]\n    idx2 = [list(davg[rng+10]['att']).index(x) for x in overlap]\n    \n    r = P_range_unc(fit.params, rng+10, davg[rng+10]['v+unc'][idx2]) \/ \\\n        P_range_unc(fit.params, rng, davg[rng]['v+unc'][idx1])\n    \n    #print(r)\n    r_list.append(r.mean() + ufloat(0, unp.nominal_values(r).std()))\n    print(rng, r.mean().std_dev, unp.nominal_values(r).std())\nr_list","ec3aa497":"rng_disc = {}\nrng_disc[-10] = 1\nfor rng in [-20, -30, -40, -50, -60]:\n    rng_disc[rng] = np.array(r_list).cumprod()[int(rng\/-10) - 2]\nrng_disc","e784bb3e":"rng = -10\nimport matplotlib.pyplot as plt\n#plt.rcParams['text.usetex'] = True\n#plt.clf()\nfig = go.Figure()\nfor rng in ranges:\n    x = davg[rng]['att']\n    #y = P_range_unc(fit.params, rng, davg[rng]['v+unc'])\/ davg[rng]['v']\n    y = P_range_unc(fit.params, rng, davg[rng]['v+unc'])*rng_disc[rng]\n    error_y=dict(\n        type='data', # value of error bar given in data coordinates\n        array=unp.std_devs(y)*100,\n        visible=True\n    )\n    y = unp.nominal_values(y)\n\n    #plt.semilogx(x,y-1,'o')\n    fig.add_scatter(x=x,y=y,error_y=error_y,name=f'range: {rng}', mode='markers')\n#plt.xlabel('reading')\n#plt.ylabel(r'$\\frac{p(reading)}{reading}-1$', fontsize=36)\nfig.update_layout(xaxis_type=\"linear\")\nfig.update_layout(yaxis_type=\"log\")\nfig.update_layout(\n    title=\"Error bars multiplied by 100\",\n    xaxis_title=\"Attenuation setting\",\n    yaxis_title=r\"$p(reading)*rng_disc$\",\n    legend_title=\"\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        color=\"RebeccaPurple\"\n    )\n)\n\nfig.show()","0dfb2430":"error_y","d02e4636":"fig = go.Figure()\nfor rng in ranges:\n    x = davg[rng]['v']\n    y = P_range_unc(fit.params, rng, davg[rng]['v+unc']) \/ davg[rng]['v']\n    y = unp.nominal_values(y)\n    #plt.semilogx(x,y-1,'o')\n    fig.add_scatter(x=x,y=y,name=f'range: {rng}', mode='markers')\n#plt.xlabel('reading')\n#plt.ylabel(r'$\\frac{p(reading)}{reading}-1$', fontsize=36)\nfig.update_layout(xaxis_type=\"log\") #, yaxis_type=\"log\")\nfig.update_layout(\n    title=\"Correction for each range\",\n    xaxis_title=\"reading\",\n    yaxis_title=r\"$\\alpha$\",\n    legend_title=\"\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        color=\"RebeccaPurple\"\n    )\n)\n","11b121f1":"unp.std_devs(r), r","d3a54e08":" Using a weighted average to estimate tau\n $$\n \\tau = \\frac{\\sum{w_i \\tau_i}}{\\sum{w_i}}\n $$\n\n $$ w_i = \\frac{1}{\\sigma_i^2}$$\n\n $$ \\sigma_{\\tau}^2 = \\frac{1}{\\sum{w_i}}$$\n\n\n where $\\sigma_i$ is the uncertainty for $\\tau_i$","838b5c15":"## Code to look at range discontinuity errors","000db6b7":"### Code for fitting over one range at a time","9c7166c4":"## Code for trying to fit over all ranges at once","c3cbd17c":"Equation to use to parameterize the non-linearity (normalized conversion function)\n$$\nP(V) = V + \\sum_{k=2}^N b_k V^k\n$$\n$$ \\sigma_P^2 = [1 + \\sum_{k=2}^{N} k b_k V^{k-1}]^2 \\sigma_V^2 $$"}}