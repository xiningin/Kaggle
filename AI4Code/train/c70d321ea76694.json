{"cell_type":{"e4839daf":"code","4b08575d":"code","f5082fbf":"code","100a804b":"code","0a67b063":"code","7fc9364c":"code","4678982e":"code","cb182c59":"code","26587f52":"code","49da1f5d":"code","91cb54e5":"code","d84ecee4":"code","ea6adf61":"code","2017140d":"code","6501f126":"code","dfb5a4d8":"code","8bee7b86":"code","07a51c59":"code","169e5432":"code","b916e3aa":"code","b4979d8d":"code","c73afeca":"code","4cd59b6e":"code","13672bc0":"code","d29c0214":"code","69eb797b":"code","a94b7977":"code","5ceccc6c":"code","af528b08":"code","7e0977c4":"code","b81fa7fb":"code","b7d22960":"code","f6c4b7ce":"code","48af1b9f":"code","990aa7ea":"code","1d5aef4f":"code","f797d1a5":"code","9b22e583":"code","db165c01":"code","9c4a1f43":"code","415e7feb":"code","9b366118":"markdown","e1e787f2":"markdown","9a5dc917":"markdown"},"source":{"e4839daf":"!tar xf \/kaggle\/input\/ffmpeg-static-build\/ffmpeg-git-amd64-static.tar.xz\n# !ln -s \/kaggle\/input\/fastai-audio\/audio .\n# !ln -s \/kaggle\/input\/fastai\/fastai .\n# !pip install -qU \"\/kaggle\/input\/fastai2-wheels\/fastprogress-0.2.2-py3-none-any.whl\"","4b08575d":"# !pip install -qU git+https:\/\/github.com\/fastai\/fastai\n# !pip install -qU torch torchaudio torchvision\n# !git clone https:\/\/github.com\/mogwai\/fastai_audio\n# %cd fastai_audio\n# !.\/install.sh\n# %cd ..\/\n# !ln -s \/kaggle\/input\/fastai_audio\/audio .","f5082fbf":"# from audio import *  \n# from fastai.basics import *","100a804b":"# import gc\n# from functools import partial\nfrom pathlib import Path\n\nimport torchvision\nfrom fastai.vision import *\nfrom tqdm.notebook import tqdm\n\n\nhome = Path(\".\")\ninput_dir = Path(\"..\/input\/deepfake-detection-challenge\")","0a67b063":"# from fastai.utils import *\n# show_install(1)","7fc9364c":"# torch.__version__","4678982e":"# import torchvision\n# torchvision.__version__","cb182c59":"# torchaudio.__version__","26587f52":"# !apt-get --assume-yes install sox libsox-dev libsox-fmt-all libsndfile1","49da1f5d":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(42)","91cb54e5":"labels = pd.read_json(input_dir\/\"train_sample_videos\/metadata.json\").T","d84ecee4":"ext = \".wav\"","ea6adf61":"audio_path = Path(\"test_audio\")\naudio_path.mkdir(exist_ok=True)\ntrain_audio_path = Path(\"train_audio\")\ntrain_audio_path.mkdir(exist_ok=True)","2017140d":"def mp4_to_wav(filenames, out):\n    Path(out).mkdir(exist_ok=True)\n    for fn in tqdm(filenames):\n        out_fn = f\"{out\/fn.stem}{ext}\"\n        command = f\"\/kaggle\/working\/ffmpeg-git-20191209-amd64-static\/ffmpeg -i '{fn}' -ar 44100 -vn '{out_fn}'\"\n        subprocess.call(command, shell=True)","6501f126":"# mp4_to_wav((input_dir\/\"test_videos\").ls(), audio_path)","dfb5a4d8":"# mp4_to_wav((input_dir\/\"train_sample_videos\").ls(), train_audio_path)","8bee7b86":"# config = AudioConfig()\n# config.duration = 10_000","07a51c59":"# get_y = lambda x: labels.loc[f\"{x.stem}.mp4\"].label","169e5432":"# audios = (AudioList.from_folder(train_audio_path, config=config)\n#           .split_by_rand_pct(.2, seed=42)\n#           .label_from_func(get_y))","b916e3aa":"BSA=1","b4979d8d":"# db = audios.databunch(bs=BSA)\n# db.show_batch()","c73afeca":"# learn = audio_learner(db)","4cd59b6e":"# learn = load_learner(\"\/kaggle\/input\/deepfake\", \"export.pkl\")","13672bc0":"# test = AudioList.from_folder(audio_path, config=config); test","d29c0214":"# learn.predict(test[-1])","69eb797b":"# preds = []\n# for t in test:\n#     preds.append(learn.data.classes[np.argmax(learn.predict(t)[2])])\n# preds[:5]","a94b7977":"# def predict_from_file(wav_file, learner, verbose=True):  \n#     item = AudioItem(path=wav_file)\n#     if verbose: display(item)\n#     al = AudioList([item], path=item.path, config=config)\n#     ai = AudioList.open(al, item.path)\n#     y, pred, raw_pred = learner.predict(ai)\n#     if verbose: print(y)\n#     if verbose: print(pred.item())\n#     if verbose: print(raw_pred)","5ceccc6c":"# predict_from_file(test[0].path, learner )","af528b08":"# BS = 1 #CPU\nBS = 864","7e0977c4":"!ln -s \/kaggle\/input\/blazeface-pytorch\/* .\/\n!ln -s \/kaggle\/input\/deepfakes-inference-demo\/helpers .\/","b81fa7fb":"from blazeface import BlazeFace\nfacedet = BlazeFace().to(torch.device(\"cuda:0\"));\n# facedet = BlazeFace();\nfacedet.load_weights(\"blazeface.pth\")\nfacedet.load_anchors(\"anchors.npy\")\n_ = facedet.train(False)\n\nfrom helpers.read_video_1 import VideoReader\nfrom helpers.face_extract_1 import FaceExtractor\n\nframes_per_video = 17\n\nvideo_reader = VideoReader()\nvideo_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video)\nface_extractor = FaceExtractor(video_read_fn, facedet)","b7d22960":"def extract_faces(video_path, batch_size):\n    # Find the faces for N frames in the video.\n    faces = face_extractor.process_video(video_path)\n\n    # Only look at one face per frame.\n    face_extractor.keep_only_best_face(faces)\n    \n    if len(faces) > 0:\n        # NOTE: When running on the CPU, the batch size must be fixed\n        # or else memory usage will blow up. (Bug in PyTorch?)\n        x = []\n\n        # If we found any faces, prepare them for the model.\n        n = 0\n        for frame_data in faces:\n            for face in frame_data[\"faces\"]:\n                x.append(face)\n                n += 1\n    return x\n\ndef predict_on_mp4(names, learner, bs=17):\n    preds = []\n    for fn in tqdm(names):\n        if fn.is_file():\n            faces = extract_faces(fn, batch_size=bs)\n            pred = [learner.predict(Image(torchvision.transforms.ToTensor()(f)))[2] for f in faces]\n            if not pred:\n                display(f\"No pred from {fn}\")\n            preds.append(pred)\n    return preds","f6c4b7ce":"def mace(pred:Tensor, targ:Tensor)->Rank0Tensor:\n    \"Mean absolute error between clamped `pred` and `targ`.\"\n    pred,targ = flatten_check(pred,targ)\n    return torch.abs(targ - pred.clamp(0., 1.)).mean()","48af1b9f":"learn = load_learner(\"\/kaggle\/input\/deepfake\", \"big_shots_not_random_0_13.pkl\", bs=BS).to_fp32()\n# learn = load_learner(\"\/kaggle\/input\/deepfake\", \"shots.pkl\", bs=BS).to_fp32()","990aa7ea":"raw_preds = predict_on_mp4((input_dir\/\"test_videos\").ls(), learn); len(raw_preds)","1d5aef4f":"# # classes\n# preds = []\n# for p in raw_preds:\n#     try:\n#         preds.append(torch.stack(p, dim=0).argmax(dim=1).float().mean().item())\n#     except:\n#         preds.append(0.5)","f797d1a5":"preds = []\nfor p in raw_preds:\n    try:\n        preds.append(torch.stack(p, dim=0).mean().clamp(0.1,0.9).item())\n    except:\n        preds.append(0.5)","9b22e583":"subm = pd.read_csv(input_dir\/\"sample_submission.csv\")","db165c01":"subm[\"label\"] = preds\n# subm[\"label\"] = 1 - subm[\"label\"] # for binary classification where 0 is FAKE and 1 is REAL\n# subm[\"label\"].value_counts(bins=2)","9c4a1f43":"subm.to_csv(\"submission.csv\", index=False, float_format='%.20f')","415e7feb":"pd.read_csv(\"submission.csv\")","9b366118":"# Video","e1e787f2":"In this notebook I inference using my pretrained model, including pre-processing. The model was pretrained on a small subset of the data.\n\nSources:\n\nhttps:\/\/www.kaggle.com\/humananalog\/inference-demo","9a5dc917":"# Audio"}}