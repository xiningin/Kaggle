{"cell_type":{"147017d6":"code","5e3de6c4":"code","9b759902":"code","1bcd28cf":"code","e0564804":"code","9e2c96fe":"code","b8304ab0":"code","b325775e":"code","4922204e":"code","db8f1069":"code","66109666":"code","4d0b86df":"code","5c99d6f0":"code","578d5fc2":"code","f849615e":"code","ef184071":"code","058e8695":"code","182e3700":"code","65d171af":"code","668ccf30":"code","c42ea05a":"code","124c0f30":"code","a01e6eaa":"code","26a73acd":"code","bea3a37d":"code","61293dd7":"code","5e3ed8e5":"code","a5df73e2":"code","d9d2ce91":"code","0eb4f7bb":"code","2df2e80f":"code","6f72376c":"code","863a6866":"code","622afdc8":"code","a2a1387a":"code","1cf5bcc9":"code","10bbe406":"code","235e4a57":"code","35c8de6f":"code","f5332b3a":"code","f9053bea":"code","d4e1e677":"code","c9e11e0d":"code","2cf59273":"code","0beaf675":"code","23c49c76":"code","5f08a00c":"code","e2327f62":"code","454952a2":"code","443449bd":"code","64c686c5":"code","9bcb73cc":"code","f66b0c8d":"code","1c67afd9":"code","05a002b6":"code","6d80f3c0":"code","f110898a":"code","e329a6e1":"code","f2936df3":"code","931e7ddb":"markdown","86146930":"markdown","1e298180":"markdown","a998d962":"markdown","04642732":"markdown","d6cd6187":"markdown","818922b5":"markdown","0c9c8459":"markdown","31cd93dd":"markdown","7dc96379":"markdown","bc992dee":"markdown","a113a673":"markdown","b6306256":"markdown","f1b9796b":"markdown","f2bf155f":"markdown","66abe6e4":"markdown","77078a78":"markdown","070eccc1":"markdown","da3df4e0":"markdown","84824254":"markdown","e3b260c8":"markdown","55695a3a":"markdown","8a3c6d3e":"markdown","a0730002":"markdown","1f0a5b73":"markdown","4bfdf295":"markdown"},"source":{"147017d6":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport nltk\nfrom collections import Counter\n\nfrom plotly import graph_objs as go\nfrom sklearn import preprocessing \nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom keras.preprocessing import text, sequence\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,Dropout, Bidirectional, Conv2D\nfrom keras.callbacks import ReduceLROnPlateau\nimport tensorflow as tf\nimport transformers\nfrom tokenizers import BertWordPieceTokenizer\nfrom keras.layers import LSTM,Dense,Bidirectional,Input\nfrom keras.models import Model\nimport torch\nimport transformers","5e3de6c4":"df = pd.read_csv(\"..\/input\/60k-stack-overflow-questions-with-quality-rate\/data.csv\")\ndf.head()","9b759902":"df.columns","1bcd28cf":"# Adding the title and body of query\ndf['text'] = df['Title'] + \" \" + df['Body']\n\n# Drop columns not used for modelling\ncols_to_drop = ['Id', 'Tags', 'CreationDate', 'Title', 'Body']\ndf.drop(cols_to_drop, axis=1, inplace=True)\n\n# Rename category column to be more meaningful\ndf = df.rename(columns={\"Y\": \"class\"})\n\nprint(\"Total number of samples:\", len(df))\n\ndf.head()","e0564804":"temp = df.groupby('class').count()['text'].reset_index()\n\nfig = go.Figure(go.Funnelarea(\n    text = temp['class'],\n    values = temp.text,\n    title = {\"position\" : \"top center\", \"text\" : \"Funnel Chart for target distribution\"}\n    ))\nfig.show()","9e2c96fe":"temp = df['class'].value_counts()\n\nfig = px.bar(temp)\nfig.update_layout(\n    title_text='Data distribution for each category',\n    yaxis=dict(\n        title='Count'\n    ),\n    xaxis=dict(\n        title='Label'\n    )\n)\nfig.show()","b8304ab0":"high = df[df['class']=='HQ']['text'].str.split().map(lambda x: len(x) if len(x) < 500 else 500)\nlow_open = df[df['class']=='LQ_EDIT']['text'].str.split().map(lambda x: len(x) if len(x) < 500 else 500)\nlow_closed = df[df['class']=='LQ_CLOSE']['text'].str.split().map(lambda x: len(x) if len(x) < 500 else 500)\n\nfig = go.Figure()\nfig.add_trace(go.Histogram(x=high, histfunc='avg', name=\"HQ\", opacity=0.6, histnorm='probability density'))\nfig.add_trace(go.Histogram(x=low_open, histfunc='avg', name=\"LQ_EDIT\", opacity=0.6, histnorm='probability density'))\nfig.add_trace(go.Histogram(x=low_closed, histfunc='avg', name=\"LQ_CLOSE\", opacity=0.6, histnorm='probability density'))\n\nfig.update_layout(\n    title_text='Number of words in post', # title of plot\n    xaxis_title_text='Value', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    bargap=0.2,\n    bargroupgap=0.1,\n    barmode='overlay'\n)\nfig.show()","b325775e":"df.isna().sum() # Checking for nan Values","4922204e":"stop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)","db8f1069":"# Clean the data\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r'[^(a-zA-Z)\\s]','', text)\n    return text\ndf['text'] = df['text'].apply(clean_text)","66109666":"plt.figure(figsize = (20,20)) # Text that is of high quality\nwc = WordCloud(max_words = 1000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(df[df['class'] == 'HQ'].text))\nplt.imshow(wc , interpolation = 'bilinear')","4d0b86df":"plt.figure(figsize = (20,20)) # Text that is of low quality(closed)\nwc = WordCloud(max_words = 1000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(df[df['class'] == 'LQ_EDIT'].text))\nplt.imshow(wc , interpolation = 'bilinear')","5c99d6f0":"plt.figure(figsize = (20,20)) # Text that is of low quality(open)\nwc = WordCloud(max_words = 1000 , width = 1400 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(df[df['class'] == 'LQ_CLOSE'].text))\nplt.imshow(wc , interpolation = 'bilinear')","578d5fc2":"# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \n# Encode labels in column 'class'. \ndf['class']= label_encoder.fit_transform(df['class']) \n  \ndf['class'].unique() ","f849615e":"df['temp_list'] = df['text'].apply(lambda x:str(x).split())\n\ntop = Counter([item for sublist in df['temp_list'].loc[df['class'] == 0] for item in sublist])\ntop_hq = pd.DataFrame(top.most_common(15))\ntop_hq.columns = ['Common_words','count']\n\nfig = px.bar(top_hq, x='count',y='Common_words',title='Common words in High Quality posts',orientation='h',width=700,height=500,color='Common_words')\nfig.show()\n\nfig = px.treemap(top_hq, path=['Common_words'], values='count',title='Tree of Common words in High Quality posts')\nfig.show()","ef184071":"fig = px.pie(top_hq,\n             values='count',\n             names='Common_words',\n             title='Word distribution in High Quality posts')\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.show()","058e8695":"top = Counter([item for sublist in df['temp_list'].loc[df['class'] == 1] for item in sublist])\ntop_lq = pd.DataFrame(top.most_common(15))\ntop_lq.columns = ['Common_words','count']\n\nfig = px.bar(top_lq, x='count',y='Common_words',title='Common words in Low Quality posts(Closed)',orientation='h',width=700,height=500,color='Common_words')\nfig.show()\n\nfig = px.treemap(top_lq, path=['Common_words'], values='count',title='Tree of Common words in Low Quality posts')\nfig.show()","182e3700":"fig = px.pie(top_lq,\n             values='count',\n             names='Common_words',\n             title='Word distribution in Low Quality posts(Closed)')\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.show()","65d171af":"top = Counter([item for sublist in df['temp_list'].loc[df['class'] == 2] for item in sublist])\ntop_lq = pd.DataFrame(top.most_common(15))\ntop_lq.columns = ['Common_words','count']\n\nfig = px.bar(top_lq, x='count',y='Common_words',title='Common words in Low Quality posts(Open)',orientation='h',width=700,height=500,color='Common_words')\nfig.show()\n\nfig = px.treemap(top_lq, path=['Common_words'], values='count',title='Tree of Common words in Low Quality posts')\nfig.show()","668ccf30":"del df['temp_list']","c42ea05a":"fig = px.pie(top_lq,\n             values='count',\n             names='Common_words',\n             title='Word distribution in Low Quality posts(Open)')\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.show()","124c0f30":"df.head()","a01e6eaa":"fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(16,6))\ntext_len=df[df['class']==0]['text'].str.len()\nax1.hist(text_len,color='lightcoral')\nax1.set_title('High Quality')\ntext_len=df[df['class']==1]['text'].str.len()\nax2.hist(text_len,color='lightgreen')\nax2.set_title('Low Quality(closed)')\ntext_len=df[df['class']==2]['text'].str.len()\nax3.hist(text_len,color='lightskyblue')\nax3.set_title('Low Quality(open)')\nfig.suptitle('Characters in texts')\nplt.show()","26a73acd":"fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(16,6))\nword=df[df['class']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='lightcoral')\nax1.set_title('High Quality')\nword=df[df['class']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='lightgreen')\nax2.set_title('Low Quality(closed)')\nword=df[df['class']==2]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax3,color='lightskyblue')\nax3.set_title('Low Quality(open)')\nfig.suptitle('Average word length in each text')","bea3a37d":"df.head()","61293dd7":"from sklearn.feature_extraction.text import CountVectorizer\ndef get_top_text_ngrams(corpus, n, g):\n    vec = CountVectorizer(ngram_range=(g, g)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","5e3ed8e5":"plt.figure(figsize = (16,5))\nmost_common_uni = get_top_text_ngrams(df.text,10,1)\nmost_common_uni = dict(most_common_uni)\nsns.set_palette(\"husl\")\nsns.barplot(x=list(most_common_uni.values()),y=list(most_common_uni.keys()))","a5df73e2":"plt.figure(figsize = (16,5))\nmost_common_bi = get_top_text_ngrams(df.text,10,2)\nmost_common_bi = dict(most_common_bi)\nsns.set_palette(\"husl\")\nsns.barplot(x=list(most_common_bi.values()),y=list(most_common_bi.keys()))","d9d2ce91":"plt.figure(figsize = (16,5))\nmost_common_tri = get_top_text_ngrams(df.text,10,3)\nmost_common_tri = dict(most_common_tri)\nsns.set_palette(\"husl\")\nsns.barplot(x=list(most_common_tri.values()),y=list(most_common_tri.keys()))","0eb4f7bb":"df.head()","2df2e80f":"x_train,x_test,y_train,y_test = train_test_split(df['text'], df['class'], test_size = 0.2, random_state = 42, stratify = df['class'])","6f72376c":"max_features = 10000\nmaxlen = 300","863a6866":"tokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(x_train)\ntokenized_train = tokenizer.texts_to_sequences(x_train)\nx_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)","622afdc8":"tokenized_test = tokenizer.texts_to_sequences(x_test)\nX_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)","a2a1387a":"EMBEDDING_FILE = '..\/input\/glove-twitter\/glove.twitter.27B.200d.txt'","1cf5bcc9":"def get_coefs(word, *arr): \n    return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))","10bbe406":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\n#change below line if computing normal stats is too slow\nembedding_matrix = embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","235e4a57":"batch_size = 256\nepochs = 5\nembed_size = 200","35c8de6f":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.4, min_lr=0.0000001)","f5332b3a":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n    #Defining Neural Network\n    model = Sequential()\n    #Non-trainable embeddidng layer\n    model.add(Embedding(max_features, output_dim=embed_size, weights=[embedding_matrix], input_length=maxlen, trainable=True))\n    #LSTM \n    model.add(Bidirectional(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.4 , dropout = 0.4)))\n    model.add(Bidirectional(LSTM(units=128 , recurrent_dropout = 0.2 , dropout = 0.2)))\n    model.add(Dense(3, activation='softmax'))\n    model.compile(optimizer=keras.optimizers.Adam(lr = 0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","f9053bea":"model.summary()","d4e1e677":"history = model.fit(x_train, y_train, batch_size = batch_size , validation_data = (X_test,y_test) , epochs = epochs , callbacks = [learning_rate_reduction])","c9e11e0d":"print(\"Accuracy of the model on Training Data is - \" , model.evaluate(x_train,y_train)[1]*100 , \"%\")\nprint(\"Accuracy of the model on Testing Data is - \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")","2cf59273":"epochs = [i for i in range(5)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\nax[0].set_title('Training & Testing Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')\nax[1].set_title('Training & Testing Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Loss\")\nplt.show()","0beaf675":"pred = model.predict_classes(X_test)","23c49c76":"print(classification_report(y_test, pred, target_names = ['HQ', 'LQ(Close)', 'LQ(Open)']))","5f08a00c":"cm = confusion_matrix(y_test,pred)\ncm","e2327f62":"cm = pd.DataFrame(cm , index = ['HQ', 'LQ(Close)', 'LQ(Open)'] , columns = ['HQ', 'LQ(Close)', 'LQ(Open)'])\n\nplt.figure(figsize = (10,10))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['HQ', 'LQ(Close)', 'LQ(Open)'] , yticklabels = ['HQ', 'LQ(Close)', 'LQ(Open)'])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")","454952a2":"x_train,x_test,y_train,y_test = train_test_split(df['text'], df['class'], random_state = 0 , stratify = df['class'])","443449bd":"# First load the real tokenizer\ntokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased' , lower = True)\n# Save the loaded tokenizer locally\ntokenizer.save_pretrained('.')\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=True)\nfast_tokenizer","64c686c5":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=200):\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in range(0, len(texts), chunk_size):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","9bcb73cc":"x_train = fast_encode(x_train.values, fast_tokenizer, maxlen=200)\nx_test = fast_encode(x_test.values, fast_tokenizer, maxlen=200)","f66b0c8d":"# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n    max_len=200\n    transformer =  transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(3, activation='softmax')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(optimizer=keras.optimizers.Adam(lr=7e-6), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","1c67afd9":"model.summary()","05a002b6":"history = model.fit(x_train,y_train,batch_size = 64 ,validation_data=(x_test,y_test),epochs = 5)","6d80f3c0":"print(\"Accuracy of the model on Testing Data is - \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")","f110898a":"epochs = [i for i in range(5)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\nax[0].set_title('Training & Testing Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')\nax[1].set_title('Training & Testing Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Loss\")\nplt.show()","e329a6e1":"pred = model.predict(x_test)","f2936df3":"cm = pd.DataFrame(cm , index = ['HQ', 'LQ(Close)', 'LQ(Open)'] , columns = ['HQ', 'LQ(Close)', 'LQ(Open)'])\n\nplt.figure(figsize = (10,10))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['HQ', 'LQ(Close)', 'LQ(Open)'] , yticklabels = ['HQ', 'LQ(Close)', 'LQ(Open)'])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")","931e7ddb":"### [WORDCLOUD FOR LOW QUALITY POSTS(CLOSED):](http:\/\/)","86146930":"# [How do I ask a good question?](http:\/\/)\n### When someone tells me about an issue they are facing in development, in most cases, my immediate response would be: \u201cWhat did you find on Stack Overflow?\u201d\n### And, there are people who give weird answers like: \u201cI don\u2019t know how to use it, I have been banned for asking poor questions, people always downvote my posts or give me links on how to ask questions,\u201d etc.\n![](https:\/\/miro.medium.com\/max\/1050\/1*IF-ngtmrMVZGKLrvfEEf-w.png)","1e298180":"## [BRIEF DESCRIPTION OF DATASET:](http:\/\/)\nThis is an original dataset, made publicly available for researchers.\nWe collected 60,000 Stack Overflow questions from 2016-2020 and classified them into three categories:\n1. **HQ**: High-quality posts with 30+ score and without a single edit.\n3. **LQ_EDIT**: Low-quality posts with a negative score and with multiple community edits. However, they still remain open after the edits.\n3. **LQ_CLOSE**: Low-quality posts that were closed by the community without a single edit.","a998d962":"### [AVERAGE WORD-LENGTH IN POST:](http:\/\/)","04642732":"### [WORDCLOUD FOR HIGH QUALITY POSTS:](http:\/\/)","d6cd6187":"**Splitting the data into 2 parts - training and testing data**","818922b5":"## [Introduction to GloVe](http:\/\/)\n**GloVe method is built on an important idea, You can derive semantic relationships between words from the co-occurrence matrix. Given a corpus having V words, the co-occurrence matrix X will be a V x V matrix, where the i th row and j th column of X, X_ij denotes how many times word i has co-occurred with word j. An example co-occurrence matrix might look as follows.**\n![](https:\/\/miro.medium.com\/max\/521\/1*QWcK8CIDs8kMkOwsOxvywA.png)\n**The co-occurrence matrix for the sentence \u201cthe cat sat on the mat\u201d with a window size of 1. As you probably noticed it is a symmetric matrix. How do we get a metric that measures semantic similarity between words from this? For that, you will need three words at a time. Let me concretely lay down this statement.**\n![](https:\/\/miro.medium.com\/max\/860\/1*8EI7ODLsxIX9hr7gEJnLSw.png)\n**The behavior of P_ik\/P_jk for various words Consider the entity P_ik\/P_jk where P_ik = X_ik\/X_i Here P_ik denotes the probability of seeing word i and k together, which is computed by dividing the number of times i and k appeared together (X_ik) by the total number of times word i appeared in the corpus (X_i). You can see that given two words, i.e. ice and steam, if the third word k (also called the \u201cprobe word\u201d), is very similar to ice but irrelevant to steam (e.g. k=solid), P_ik\/P_jk will be very high (>1), is very similar to steam but irrelevant to ice (e.g. k=gas), P_ik\/P_jk will be very small (<1), is related or unrelated to either words, then P_ik\/P_jk will be close to 1 So, if we can find a way to incorporate P_ik\/P_jk to computing word vectors we will be achieving the goal of using global statistics when learning word vectors.**\n\n**Source Credits - https:\/\/towardsdatascience.com\/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010**","0c9c8459":"### [UNIGRAM ANALYSIS:](http:\/\/)","31cd93dd":"### [WORDCLOUD FOR LOW QUALITY POSTS(OPEN):](http:\/\/)","7dc96379":"## [PLS UPVOTE THIS NOTEBOOK IF YOU LIKE IT! THANKS FOR YOUR TIME !](http:\/\/)","bc992dee":"### [COMMON WORDS IN LOW QUALITY POSTS(CLOSED):](http:\/\/)","a113a673":"### [ANALYSIS AFTER TRAINING OF MODEL](http:\/\/)","b6306256":"### [WHAT ARE STOPWORDS?](http:\/\/)\n\n**Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like the, he, have etc. Such words are already captured this in corpus named corpus. We first download it to our python environment.**","f1b9796b":"### [TRIGRAM ANALYSIS](http:\/\/)","f2bf155f":"### [COMMON WORDS IN HIGH QUALITY POSTS:](http:\/\/)","66abe6e4":"## [Introduction to BERT](http:\/\/)\n**BERT makes use of Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. In its vanilla form, Transformer includes two separate mechanisms \u2014 an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERT\u2019s goal is to generate a language model, only the encoder mechanism is necessary.**\n\n**The chart below is a high-level description of the Transformer encoder. The input is a sequence of tokens, which are first embedded into vectors and then processed in the neural network. The output is a sequence of vectors of size H, in which each vector corresponds to an input token with the same index.**\n![](https:\/\/miro.medium.com\/max\/1050\/0*ViwaI3Vvbnd-CJSQ.png)\n**As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that it\u2019s non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word).**\n\n**Source Credits - https:\/\/towardsdatascience.com\/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270**","77078a78":"## [DATA VISUALIZATION AND PREPROCESSING](http:\/\/)","070eccc1":"### [NUMBER OF CHARACTERS IN POSTS:](http:\/\/)","da3df4e0":"### [COMMON WORDS IN LOW QUALITY POSTS(OPEN):](http:\/\/)","84824254":"## [DATA CLEANING:](http:\/\/)","e3b260c8":"### SO, WE CAN SEE THAT THE DATASET IS BALANCED","55695a3a":"**Tokenizing Text -> Repsesenting each word by a number**\n\n**Mapping of orginal word to number is preserved in word_index property of tokenizer**\n\n**Tokenized applies basic processing like changing it to lower case, explicitely setting that as False**\n\n**Lets keep all news to 300, add padding to news with less than 300 words and truncating long ones**","8a3c6d3e":"\n### Stack Overflow is one of the most helpful and most visited websites on the internet, but it is also one of the most brutal platforms on the internet. People will show no mercy if you make a mistake or ask a stupid question, and that\u2019s how the platform has maintained its standards ever since its inception. So, don\u2019t expect anything less.\n### Instead, get used to it and change the way you ask questions. Enough said, let\u2019s see what some of the important things are that you have to take care of while making a post on Stackoverflow.","a0730002":"### [BIGRAM ANALYSIS](http:\/\/)","1f0a5b73":"### [TRAINING THE MODEL](http:\/\/)","4bfdf295":"### The three histrograms show cleary, that high quality posts are generally longer than low quality posts."}}