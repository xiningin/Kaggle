{"cell_type":{"85029bc5":"code","1974f9bf":"code","ccb8fbe2":"code","bbd07550":"code","cf4a5ec4":"code","18b19876":"code","a76faf78":"code","f4d3f551":"code","0e8cb691":"code","ed1f2c18":"code","7b3069ad":"code","8f0c8181":"code","12e98f7e":"code","1a5a321e":"code","19edbe33":"code","72a06623":"code","f6ca3873":"code","a033222c":"code","0211f3f2":"code","9202bc34":"markdown","f6e91792":"markdown","16d618f0":"markdown","e73403d9":"markdown","c040fe71":"markdown","deb8b7cd":"markdown","e5fc0774":"markdown","2ac0bc73":"markdown","40979ba0":"markdown","d16dac91":"markdown","e2fbcbef":"markdown","c2a6b2b0":"markdown","36028e18":"markdown"},"source":{"85029bc5":"# Install ktrain\n!pip install --upgrade pip -q\n!pip install -q ktrain","1974f9bf":"# Generic\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings, gc\nwarnings.filterwarnings(\"ignore\")\n\n# Tensorflow\nimport tensorflow as tf\n\n# ktrain\nimport ktrain\nfrom ktrain import text\n\n# sklearn\nfrom sklearn.model_selection import train_test_split","ccb8fbe2":"# Load\nurl = '..\/input\/newyork-room-rentalads\/room-rental-ads.csv'\ndf = pd.read_csv(url, header='infer')\n\n# Dropping Null Values\ndf.dropna(inplace=True)\n\n# Total Records\nprint(\"Total Records: \", df.shape[0])\n\n# Inspect\ndf.head()","bbd07550":"# Data Split\ntarget = ['Vague\/Not']\ndata = ['Description']\n\nX = df[data]\ny = df[target]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.1, random_state=42)","cf4a5ec4":"# Common Parameters\nmax_len = 500\nbatch_size = 6\nlearning_rate = 5e-5\nepochs = 1","18b19876":"# Transformer Model\nmodel_ = 'roberta-base'\nt_mod = text.Transformer(model_, maxlen=max_len, classes = [0,1])\n\n\n'''Converting split data to list [so it can processed]'''\n#train\nX_tr = X_train['Description'].tolist()\ny_tr = y_train['Vague\/Not'].tolist()\n\n#test\nX_ts = X_test['Description'].tolist()\ny_ts = y_test['Vague\/Not'].tolist()\n\n\n# Pre-processing training & test data\ntrain = t_mod.preprocess_train(X_tr,y_tr)\ntest = t_mod.preprocess_train(X_ts,y_ts)\n\n# Model Classifier\nmodel = t_mod.get_classifier()\n\nlearner = ktrain.get_learner(model, train_data=train, val_data=test, batch_size=batch_size)","a76faf78":"# Train Model\nlearner.fit_onecycle(learning_rate, epochs)","f4d3f551":"# Evaluate\nx = learner.validate(class_names=t_mod.get_classes())","0e8cb691":"# Prediction\nclasses = ['Vague', 'Not Vague']\npredictor = ktrain.get_predictor(learner.model, preproc=t_mod)\npred_class = predictor.predict(X_test['Description'][67])\nprint(\"Predicted Class: \", classes[pred_class])","ed1f2c18":"# Transformer Model\nmodel_ = 'bert-base-uncased'\nt_mod = text.Transformer(model_, maxlen=500, classes = [0,1])\n\n\n'''Converting split data to list [so it can processed]'''\n#train\nX_tr = X_train['Description'].tolist()\ny_tr = y_train['Vague\/Not'].tolist()\n\n#test\nX_ts = X_test['Description'].tolist()\ny_ts = y_test['Vague\/Not'].tolist()\n\n\n# Pre-processing training & test data\ntrain = t_mod.preprocess_train(X_tr,y_tr)\ntest = t_mod.preprocess_train(X_ts,y_ts)\n\n# Model Classifier\nmodel = t_mod.get_classifier()\n\nlearner = ktrain.get_learner(model, train_data=train, val_data=test, batch_size=6)","7b3069ad":"# Train Model\nlearner.fit_onecycle(learning_rate, epochs)","8f0c8181":"# Evaluate\nx = learner.validate(class_names=t_mod.get_classes())","12e98f7e":"# Transformer Model\nmodel_ = 'distilbert-base-uncased'\nt_mod = text.Transformer(model_, maxlen=500, classes = [0,1])\n\n\n'''Converting split data to list [so it can processed]'''\n#train\nX_tr = X_train['Description'].tolist()\ny_tr = y_train['Vague\/Not'].tolist()\n\n#test\nX_ts = X_test['Description'].tolist()\ny_ts = y_test['Vague\/Not'].tolist()\n\n\n# Pre-processing training & test data\ntrain = t_mod.preprocess_train(X_tr,y_tr)\ntest = t_mod.preprocess_train(X_ts,y_ts)\n\n# Model Classifier\nmodel = t_mod.get_classifier()\n\nlearner = ktrain.get_learner(model, train_data=train, val_data=test, batch_size=6)","1a5a321e":"# Train Model\nlearner.fit_onecycle(learning_rate, epochs)","19edbe33":"# Evaluate\nx = learner.validate(class_names=t_mod.get_classes())","72a06623":"# Transformer Model\nmodel_ = 'xlm-roberta-base'\nt_mod = text.Transformer(model_, maxlen=500, classes = [0,1])\n\n\n'''Converting split data to list [so it can processed]'''\n#train\nX_tr = X_train['Description'].tolist()\ny_tr = y_train['Vague\/Not'].tolist()\n\n#test\nX_ts = X_test['Description'].tolist()\ny_ts = y_test['Vague\/Not'].tolist()\n\n\n# Pre-processing training & test data\ntrain = t_mod.preprocess_train(X_tr,y_tr)\ntest = t_mod.preprocess_train(X_ts,y_ts)\n\n# Model Classifier\nmodel = t_mod.get_classifier()\n\nlearner = ktrain.get_learner(model, train_data=train, val_data=test, batch_size=6)","f6ca3873":"# Train Model\nlearner.fit_onecycle(learning_rate, epochs)","a033222c":"# Evaluate\nx = learner.validate(class_names=t_mod.get_classes())","0211f3f2":"# Garbage Collect\ngc.collect()","9202bc34":"### Accuracy of ~ 81% achieved with Bert","f6e91792":"## With Transformer = distilbert-base-uncased","16d618f0":"## Parameters","e73403d9":"## With Transformer = bert-base-uncased","c040fe71":"# Text Classification with HuggingFace & ktrain\n\nIn this notebook, we'll perform text classification on the [NY Room Rental Ads](https:\/\/www.kaggle.com\/vaishnavivenkatesan\/newyork-room-rentalads) dataset with **HuggingFace Transformer Model** using **ktrain**\n\n**ktrain** is a Python library that makes deep learning and AI more accessible and easier to apply.\n\n\nFollowing are some of the pre-trained Transformer Model that we'll use & calculate their accuracy \n\n* roberta-base\n* bert-base-uncased\n* distilbert-base-uncased\n* xlm-roberta-base\n\n\n**Note**: 0 = Vague & 1 = Not Vague\n\n\nAs always will keep this notebook well commented & organized for easy reading. Please do UPVOTE if you find it helpful :)","deb8b7cd":"## With Transformer = xlm-roberta-base","e5fc0774":"### Accuracy of ~ 71% achieved with DistilBert","2ac0bc73":"## Data","40979ba0":"## Libraries","d16dac91":"### Accuracy of ~ 77% achieved with Roberta","e2fbcbef":"## With Transformer = Roberata-base","c2a6b2b0":"### Accuracy of ~ 45% achieved with XLM-Roberta","36028e18":"### Hope that was helpful & gave you an idea of ktrain & using pre-trained HuggingFace Models."}}