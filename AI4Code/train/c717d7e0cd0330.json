{"cell_type":{"e52ca9dc":"code","25ba1bae":"code","9eb0bfa3":"code","d650c1b6":"code","0c6c22a9":"code","fef98992":"code","529602c8":"code","c2db2e14":"code","0de7ae4e":"code","d3dcdb17":"code","7c9a2b77":"code","b15dabb2":"code","f8393724":"code","8e899421":"code","3a67bdce":"code","5764d865":"code","a56734c7":"code","c105d06d":"code","0c8e00d8":"code","8fd491da":"code","8f254780":"code","cc11fd17":"code","5d0dceb7":"code","ee0cfba0":"code","e5f4c937":"code","814933a6":"code","c1b102a0":"code","61e14e24":"code","e72da18a":"code","49eaebd7":"code","8292b377":"code","7e0dc800":"code","26c448ef":"code","85aa0e72":"code","110f533b":"code","12e5ab3c":"code","6c1c660d":"code","fa060556":"code","2df9fda3":"code","e22877cc":"code","2ec60659":"code","13880e48":"code","a4e380fc":"markdown","92b23af6":"markdown","85ff5853":"markdown","d089a4d4":"markdown","45134441":"markdown","a5300a4e":"markdown","8d2e1ee4":"markdown","a8c4662c":"markdown","16fbb9e8":"markdown","4326f55b":"markdown","265fc657":"markdown","818e5072":"markdown","02b3a8f0":"markdown","3965f291":"markdown","e26cce24":"markdown","829316a5":"markdown","30b46ff7":"markdown","b7693b68":"markdown","9dd8ef7b":"markdown","6819d2de":"markdown","2249bc3e":"markdown","13fffb5b":"markdown","4b79a53c":"markdown","d1b00f24":"markdown","5fe76edd":"markdown","30029f3f":"markdown"},"source":{"e52ca9dc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","25ba1bae":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime","9eb0bfa3":"import datetime\nimport scipy.stats as stats\nfrom datetime import datetime","d650c1b6":"path = '..\/input\/g-research-crypto-forecasting\/'","0c6c22a9":"!ls {path}","fef98992":"df_train = pd.read_csv(path+\"train.csv\")","529602c8":"print(df_train.shape)\ndf_train.head()","c2db2e14":"df_asset_details = pd.read_csv(path+\"asset_details.csv\")","0de7ae4e":"print(df_asset_details.shape)\nprint(df_asset_details.sort_values(\"Asset_ID\"))","d3dcdb17":"assets = df_asset_details['Asset_Name'].tolist()","7c9a2b77":"print(assets)","b15dabb2":"df_supp_train = pd.read_csv(path+\"supplemental_train.csv\")\nprint(df_supp_train.shape)\ndf_supp_train.head()","f8393724":"print(df_train['timestamp'].astype('datetime64[s]').max())\nprint(df_supp_train['timestamp'].astype('datetime64[s]').min())","8e899421":"df_test = pd.read_csv(path+\"example_test.csv\")\nprint(df_test.shape)\ndf_test.head()","3a67bdce":"print(set(df_test['group_num']))","5764d865":"df = df_train.set_index('Asset_ID').join(df_asset_details.set_index('Asset_ID'))","a56734c7":"#dropping this unnecessary column\ndf = df.drop(['Weight'], axis = 1)\nprint(df.shape)\ndf.head()","c105d06d":"df.isnull().sum()","0c8e00d8":"del(df_test)\ndel(df_supp_train)","8fd491da":"df['Diff'] = df['Close'] - df['Open']\ndf['timestamp'] = df['timestamp'].astype('datetime64[s]')\ndf['date'] = pd.to_datetime(df['timestamp']).dt.date","8f254780":"df.head()","cc11fd17":"grouped_df = df.groupby(['Asset_Name', 'date'], as_index = False).agg({'Open': ['min'], 'Close': ['max']})\n","5d0dceb7":"grouped_df.columns","ee0cfba0":"grouped_df['diff_val'] = grouped_df[('Close', 'max')] - grouped_df[('Open', 'min')]\ngrouped_df.head(10)","e5f4c937":"fig, ax = plt.subplots(nrows=14, ncols=1)\n#ax = plt.axes()\ncount = 0\n\nfor asset in assets:\n    \n    grouped_df[grouped_df['Asset_Name'] == asset]['diff_val'].plot.density(figsize = (10, 30),\n                       linewidth = 1.5, ax = ax[count], label = asset)\n    ax[count].legend()\n    ax[count].grid()\n    count = count + 1\n","814933a6":"#To save memory\ndel(df_grouped_df)\ndel(fig, ax)","c1b102a0":"grouped_df = df.groupby(['Asset_Name', 'date'], as_index = False).agg({'Count': ['sum']})","61e14e24":"grouped_df.head()","e72da18a":"fig, ax = plt.subplots(nrows=14, ncols=1)\n\ncount = 0\n\nfor asset in assets:\n    \n    grouped_df[grouped_df['Asset_Name'] == asset].plot(x = 'date', y = ('Count', 'sum'), figsize = (15, 50),\n                                                      linewidth = 1.5, ax = ax[count], label = asset)\n    \n    ax[count].legend(loc='upper right')\n    ax[count].grid()\n    count = count + 1\n#ax.legend(assets)","49eaebd7":"fig, ax = plt.subplots(nrows=14, ncols=1)\n\ncount = 0\n\nfor asset in assets:\n    \n    df[df['Asset_Name'] == asset].plot(x = 'timestamp', y = 'Close', figsize = (15, 60),\n                                                      linewidth = 1.5, ax = ax[count], label = asset)\n    \n    ax[count].legend(loc='upper right')\n    ax[count].grid()\n    count = count + 1","8292b377":"del(fig, ax)","7e0dc800":"# This is the function to define compute log returns as shared in the Tutorial notebook !\ndef log_return(series, periods=1):\n    return np.log(series).diff(periods=periods)","26c448ef":"# removing asset_ID as index\ndf.reset_index(inplace = True)","85aa0e72":"# setting timestamp as my new index\ndf.set_index('timestamp', inplace=True)","110f533b":"# Here we go !\ndf.head()","12e5ab3c":"fig, ax = plt.subplots(nrows=14, ncols=1)\n\ncount = 0\n\nfor asset in assets:\n    \n    sample_df = df[(df['date']>datetime.date(2021,1,1)) & (df['Asset_Name'] == asset)]\n    log_sample = log_return(sample_df.Close)[1:]\n    log_sample.plot(figsize = (15, 60), linewidth = 1.5, ax = ax[count], label = asset)\n    \n    ax[count].legend(loc='upper right')\n    ax[count].grid()\n    count = count + 1\n    del(sample_df)","6c1c660d":"# create dataframe with returns for the above assets\nassets_samples = pd.DataFrame([])\n\nfor i in range(0, len(assets)):\n    \n    asset_name = assets[i]\n    df_1 = df[(df['date']>datetime.date(2021,1,1)) & (df['Asset_Name'] == asset_name)]\n    lret_df = log_return(df_1.Close.fillna(0))[1:]\n    lret_df.rename(asset_name)\n    \n    assets_samples = assets_samples.join(lret_df, rsuffix=asset_name, how = 'outer')\n    \n    del(df_1)\n    del(lret_df)\n    ","fa060556":"assets_samples.columns = assets\nassets_samples.head()","2df9fda3":"corr = assets_samples.corr()\ncorr.style.background_gradient(cmap='coolwarm')","e22877cc":"pairs_assets = [('Bitcoin Cash', 'Litecoin'), ('Bitcoin', 'Litecoin'), ('Bitcoin', 'Ethereum'), ('Ethereum', 'Litecoin')]","2ec60659":"print(pairs_assets)","13880e48":"good_pairs = len(pairs_assets)\n\nfig, ax = plt.subplots(nrows=good_pairs, ncols=1)\n\ncount = 0\n\nfor i in range(good_pairs):\n    df_1 = df[df['Asset_Name'] == pairs_assets[i][0]]\n    lret_df_1 = log_return(df_1.Close)[1:]\n    lret_df_1.rename('lret_df_1', inplace=True)\n    \n    df_2 = df[df['Asset_Name'] == pairs_assets[i][1]]\n    lret_df_2 = log_return(df_2.Close)[1:]\n    lret_df_2.rename('lret_df_2', inplace=True)\n    \n    two_assets = pd.concat([lret_df_1, lret_df_2], axis=1)\n    two_assets.reset_index(inplace = True)\n    \n    two_assets['timestamp'] = two_assets.timestamp.astype('int64') \/\/ 10**9\n    two_assets.set_index('timestamp')\n    \n    # group consecutive rows and use .corr() for correlation between columns\n    corr_time = two_assets.groupby(two_assets.index\/\/(1000*20)).corr().loc[:,\"lret_df_1\"].loc[:,\"lret_df_2\"]\n\n    #corr_time.plot()\n    corr_time.plot(figsize = (15, 25), linewidth = 1.5, ax = ax[count], label = str(pairs_assets[i]), color = 'r')\n    ax[count].legend(loc='upper left')\n    ax[count].set_xlabel('TimeStamp')\n    ax[count].set_ylabel(\"Correlation Coefficient\")\n    ax[count].set_title(\"Correlation between the two assets\"+str(pairs_assets[i]))\n    ax[count].grid()\n    \n    count = count + 1\n    \n    del(df_1)\n    del(df_2)\n    del(lret_df_1)\n    del(lret_df_2)\n    del(two_assets)\n    del(corr_time)","a4e380fc":"#### Distribution of number of trades","92b23af6":"Intrepretation:\nNumber of trades have sharply increased at the beginning of the year 2021 compared to their corresponding levels before. Not only that, for all 14 assets the count of trades decreased in the middle of year 2021 and onwards. It showed be noted that we are talking about similarity in pattern here and not in count. The count varies from asset to asset. Bitcoin, Ethereum, and Ethereum Classic has more trades throughout the time in this set. ","85ff5853":"### Statistical Analysis","d089a4d4":"#### Correlation between Cryptocurrencies:\nWe first compute the correlation between all the assets for the samples of trade or minutes after Jan 1, 2021.","45134441":"#### Daywise Close - Open for all assets","a5300a4e":"#### Interpretation:\nThe above charts show the log returns for all 14 assets, centered and fluctuating around zero all of them, across the entire time (*makes sense!*). We should have a probability distribution plot for this as well !!! might have helped more.","8d2e1ee4":"Let us look at the distribution of the column for each of the crypto assets individually !","a8c4662c":"It does not look like the records in supplemental set come after in time, the records in the training set.\nLet's first explore what we have in the training set, then will come back to this !","16fbb9e8":"I am removing the datasets other than the training set and the crypto-assets details set to avoid overusage of the memory.","4326f55b":"As stated above, these correlation numbers are computed over a sample of time series data after the date Jan 1, 2021. However, in such a dataset, correlation between the columns \/ assets can vary in time.\nLet us check the consistency in correlation in time for the assets that have high correlation between them. We set the threshold of 0.75 and shortlist the pairs that have correlation number higher than this.\n\n* Bitcoin Cash - Litecoin\n* Bitcoin - Litecoin\n* Bitcoin - Ethereum\n* Ethereum - Litecoin","265fc657":"Does this dataset merely contain additional\/supplemental rows for the training set ? Let's check the timestamps.","818e5072":"#### Interpretations:\nFirst, these results are shown regardless of what the opening value was !\nNext, notice the similarity in patterns on how the Close values for the all the assets have increased in the beginning of the year 2021 and then abated in the middle of the year. This is in line with the number of trades made for each asset. There is a strong correlation then (*we haven't looked at it yet but we should quantify the correlation !*)","02b3a8f0":"Let's investigate how the closed values varied in time for all 14 cryptocurrencies ?","3965f291":"Let's start loading datasets and explore the dimensions !","e26cce24":"### How Close varies in time for different assets ?","829316a5":"##### Interpretation of the above charts:\n* For Bitcoin, the Daywise average difference between Close and Open has more weight towards the the positive scale and reaches as far as the value +20,000. On the negative side of the scale, it has picked up values even lower than -5000. This range is higher than any other asset.\n* Ethereum Classic and Maker also show highe range of difference values and have considerable amount of skew towards the positive differences.","30b46ff7":"#### Checking of Missing values ","b7693b68":"The meanings of the features included in the set are the following:\n* timestamp: All timestamps are returned as second Unix timestamps (the number of seconds elapsed since 1970-01-01 00:00:00.000 UTC). Timestamps in this dataset are multiple of 60, indicating minute-by-minute data.\n* Asset_ID: The asset ID corresponding to one of the crytocurrencies (e.g. Asset_ID = 1 for Bitcoin). The mapping from Asset_ID to crypto asset is contained in asset_details.csv.\n* Count: Total number of trades in the time interval (last minute).\n* Open: Opening price of the time interval (in USD).\n* High: Highest price reached during time interval (in USD).\n* Low: Lowest price reached during time interval (in USD).\n* Close: Closing price of the time interval (in USD).\n* Volume: Quantity of asset bought or sold, displayed in base currency USD.\n* VWAP: The average price of the asset over the time interval, weighted by volume. VWAP is an aggregated form of trade data.\n* Target: Residual log-returns for the asset over a 15 minute horizon.","9dd8ef7b":"Let's check how daywise Open-Close is distributed for each crypto-asset ?","6819d2de":"Example test set has a column 'group_num' and an additional column in row_id, and missing target column from the training set.\nGroup_num column is a mystery to me for now. Let's take a look at the distinct values of this column.","2249bc3e":"Let us join the two datasets on the AssetID column.","13fffb5b":"If the count column represents the trades made in a minute then let's look at the distribution of number of trades made on daily basis for all 14 cryptoassets.","4b79a53c":"####  Log returns for different assets","d1b00f24":"Seems like there are no NULLs except in the Target column.","5fe76edd":"### Dimensions of the datasets","30029f3f":"Interestingly ! They have different correlation numbers in time but they have a very similar pattern. We can:\n* Make finer the granularity level by wich the timestamps are grouped and check to what extent it holds.\n* Decrease the threshold on the correlation computed on the latest sample to include more pairs in the list and see how the pattern varies.\n* In the time window where these assets-pairs have low correlations, which other assets show higher correlation to these assets."}}