{"cell_type":{"466f65bb":"code","4a6dce10":"code","5f5b16ef":"code","3695a3e6":"code","c23f3269":"code","76311a11":"code","7b8d164b":"code","237ad9b8":"code","dc3abf2f":"code","79c3a167":"code","660cd850":"code","591a7c36":"code","68d02693":"code","d2c33b33":"code","8db008c9":"code","c98be007":"code","dd29e4b8":"code","850895ce":"code","798ee15d":"code","07596814":"code","71ce549b":"code","a0d95ae1":"code","19c8d08c":"code","4d07dc76":"markdown","b1f34f7d":"markdown","16997541":"markdown","0744e777":"markdown","0fe92d88":"markdown","4a1107c2":"markdown","faed9321":"markdown","4be779c9":"markdown"},"source":{"466f65bb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv) \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas_profiling as pp \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report, plot_roc_curve\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nimport plotly.express as px\nsns.set_theme(style='darkgrid')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4a6dce10":"df =pd.read_csv(\"\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")","5f5b16ef":"df.head()","3695a3e6":"df.shape","c23f3269":"df.describe","76311a11":"df.dtypes","7b8d164b":"pip install openpyxl","237ad9b8":"pp.ProfileReport(df)","dc3abf2f":"# check for how many womens are prone to heart-attack\nwomen_stroke = df.loc[df.sex == 0]['output']\nwomen_stroke_percentage = sum(women_stroke)\/len(women_stroke)\nprint('The % of women prone to heart-attack: {}%'.format(women_stroke_percentage*100))","79c3a167":"def find_correlational_map(data):\n    plt.figure(figsize=(16,12))\n    sns.heatmap(data.corr(), annot=True, cmap='OrRd')\n    plt.title('Correlational Map', weight='bold')\n    print('---'*50)\n    print(data.corr().output.sort_values(ascending = False))\n    plt.tight_layout()\n    \nfind_correlational_map(df)","660cd850":"df.head()","591a7c36":"X = df.iloc[:,:-1].values\ny = df.iloc[:,-1].values","68d02693":"# Let's split the date into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\nX_train.shape, X_test.shape","d2c33b33":"y_train.shape, y_test.shape","8db008c9":"# let's create a pipline \npipeline = make_pipeline(RobustScaler()) # creating pipeline for model building\n\nLR = make_pipeline(pipeline, LogisticRegression(random_state=0)) # LogisticRegression pipeline\nDT = make_pipeline(pipeline, DecisionTreeClassifier(random_state=0)) # DecisionTree Classifier pipeline\nRF = make_pipeline(pipeline, RandomForestClassifier(random_state=0)) # RandomForest Classifier pipeline\nAC = make_pipeline(pipeline, AdaBoostClassifier(random_state=0)) # Adaboost Classifier pipeline\nNB = make_pipeline(pipeline, GaussianNB()) # Naive bayes pipeline\nKN = make_pipeline(pipeline, KNeighborsClassifier()) # KNeighbor pipeline\nSV = make_pipeline(pipeline, SVC(random_state=0)) # Support vector pipeline","c98be007":"# creating model_dict\nmodel_dictionary = {\n    'Logistic_Regression':LR,\n    'DecisionTree_Classifier':DT,\n    'RandomForest_classifier':RF,\n    'Adaboost_Classifier':AC,\n    'Naivebayes_Classifier':NB,\n    'KNeighbors_classifier':KN,\n    'Support_Vector':SV\n}","dd29e4b8":"print(model_dictionary)","850895ce":"# define a function to fit the model and return it's accuracy, classification report and confusion matrix\ndef model_fitting(model):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print('The accuracy score of the model is: {}%'.format(accuracy_score(y_test, y_pred)* 100))\n    print('-----'*20)\n    print(confusion_matrix(y_test, y_pred))\n    print(classification_report(y_test, y_pred))","798ee15d":"for name, model in model_dictionary.items():\n    print('---'*10)\n    print(name)\n    model_fitting(model)","07596814":"model = AdaBoostClassifier(random_state=0)\nmodel.fit(X_train, y_train)","71ce549b":"y_pred = model.predict(X_test)\naccuracy_score(y_test, y_pred)","a0d95ae1":"def find_confusion_matrix(y_test, y_pred):\n    cm = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, cmap='OrRd')\n    plt.title('Confusion Matrix', weight='bold')\n    print(classification_report(y_test, y_pred))\n    plot_roc_curve(model, X_test, y_test)\n    \n    \nfind_confusion_matrix(y_test, y_pred)","19c8d08c":"print('The accuracy of the model is: {}%'.format(round(accuracy_score(y_test, y_pred)*100, 2)))","4d07dc76":"**Correlation**","b1f34f7d":"Let's perform auto EDA using Pandas Profiling as the data set isn't that big.","16997541":"Pandas Profiling has provided us some quick insights. \nLuckily, there are no missing values. ","0744e777":"**Model Building**","0fe92d88":"As we can see:\n\nAda boost has got 90% accuracy with only 6 misclassified classes.\nIt' has a precision of 0.86 for classes 0 and 0.94 for classes 1, which is better than all other algorithms.\n**Let's use Adaboost Model**","4a1107c2":"According to sex let's manually figure out the percentage of male\/ female affected by heart attack.","faed9321":"**Selecting the best model**","4be779c9":"**Data Splitting**"}}