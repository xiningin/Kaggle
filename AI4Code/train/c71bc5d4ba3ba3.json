{"cell_type":{"5f5d8633":"code","c477365a":"code","3bc02e8b":"code","fab25a0a":"code","083080fd":"code","0d56773e":"code","72b1cd35":"code","17b2a99e":"code","7eba5332":"code","715710fb":"code","46aa6f30":"code","b8cb4d83":"code","93d7c08b":"code","536fabc3":"code","5906eae2":"code","37509354":"code","f1a8b0bf":"code","87a5d522":"code","da3ea998":"code","dca6c076":"code","af1764ea":"code","35256211":"code","50db053e":"code","0fa81c65":"code","e4a648a2":"code","fd86213a":"code","aa74cfd0":"code","85a24cba":"code","1c383733":"code","ae3552dd":"code","9474641b":"code","26d6aa48":"code","e2898138":"code","05c43154":"code","fc0c193a":"code","95a73298":"code","b553f38b":"code","623936e2":"code","6e6ba3a5":"code","b8cf8bd7":"code","5b2afa9d":"code","7a79428c":"code","a16080e0":"code","42c966cd":"code","733a66ff":"code","67a8d27e":"code","2ac41dc2":"code","350ee740":"code","479fa9a8":"code","137873cc":"code","2cf845cc":"code","d6823706":"code","5174612b":"code","0154754b":"code","a2b8a92a":"code","974a19c7":"code","364134eb":"code","60227754":"code","fa0526da":"code","e40c4e31":"code","0f32390d":"code","d03d93f0":"code","e0003901":"code","440f5813":"code","8b8ec00f":"code","f6af47e5":"code","f976ee05":"code","ff27387a":"code","f1da34e8":"code","9b688b15":"code","a7cc5ff0":"code","6e786ba1":"code","abc10596":"code","88695967":"code","97bc43c5":"code","154203f2":"code","dc6561c4":"code","f487199a":"code","ec8895af":"code","854ea7a6":"code","12905d3a":"code","89969a10":"code","fd77644f":"code","1c315f7e":"code","04107ed1":"markdown","da38edf7":"markdown","e60b788b":"markdown","8deb5463":"markdown","3de943ff":"markdown","7bfda3d3":"markdown","bf5568f8":"markdown","5b5d1cc9":"markdown","6eeb5c43":"markdown","ebcae47f":"markdown","6e77d83c":"markdown","cdaf1165":"markdown","a257fbc8":"markdown","608319d6":"markdown","b8a76aec":"markdown","c5a0c32d":"markdown","940ce825":"markdown"},"source":{"5f5d8633":"import numpy as np \nimport pandas as pd \nimport os ","c477365a":"PATH = \"..\/input\/mymusicalprefrences\/\"\ntrain = pd.read_csv(f\"{PATH}train.csv\")\ntest = pd.read_csv(f\"{PATH}test.csv\")\n\ntest","3bc02e8b":"train.isnull().sum()","fab25a0a":"test.isnull().sum()","083080fd":"files = [train, test]\nt = ['Version', 'Album_type','Album','Artists', \"Vocal \", 'Labels', 'Country']\n\nfor i in t:\n    train[i] = train[i].fillna('none')\n    test[i] = test[i].fillna('none')","0d56773e":"Energy = train.Energy.mean() \nDancebility = train.Dancebility.mean()\nHappiness = train.Happiness.mean()\n\ntrain['Energy'] = train['Energy'].fillna(Energy)\ntrain['Dancebility'] = train['Dancebility'].fillna(Dancebility)\ntrain['Happiness'] = train['Happiness'].fillna(Happiness)\n","72b1cd35":"train.isnull().sum()","17b2a99e":"test","7eba5332":"#a = ['Artists', 'Track', 'Version', 'Album', 'Album_type', 'Labels', 'Key', 'Country', 'Vocal ']\n\ndef binary(x):\n    if x == 'M':\n        return 0\n    elif x == 'F':\n        return 1\n    else:\n        return 2\n\ntrain['Vocal '] = train['Vocal '].apply(binary)\ntest['Vocal '] = test['Vocal '].apply(binary)","715710fb":"train.head()","46aa6f30":"def tone(x):\n    x = x.split()\n    if x[1] == 'Minor':\n        return 0\n    else:\n        return 1\n\ntrain['Tone'] =  list(map(lambda x: tone(x), train['Key']))\ntest['Tone'] =  list(map(lambda x: tone(x), test['Key']))","b8cb4d83":"train.head()","93d7c08b":"train['Key'].unique()","536fabc3":"def key(x):\n    x = x.split()\n    x = x[0]\n    if x == \"D\u266d\":\n        return \"C#\"\n    elif x == \"E\u266d\":\n        return \"D#\"\n    elif x == \"G\u266d\":\n        return \"F#\"\n    elif x == \"A\u266d\":\n        return \"G#\"\n    elif x == \"B\u266d\":\n        return \"A#\"\n    return x\n\ntrain['Key'] =  train['Key'].apply(key)\ntest['Key'] =  test['Key'].apply(key)","5906eae2":"train['Key']","37509354":"train['Key'].unique()","f1a8b0bf":"train.head()","87a5d522":"train['Country']","da3ea998":"def continents(x):\n    if x in ['SWE', 'GB', 'UA', 'RUS', 'IRL', 'NED', 'ITL', 'FR', 'DE', 'FIN', 'DK', 'POR']:\n        return \"Europe\"\n    elif x in ['CA', 'USA']:\n        return \"North America\"\n    elif x == 'AU':\n        return \"Australia\"\n    elif x in ['KR', 'JP', 'KZ', 'AZE']:\n        return 'Asia'\n    else:\n        return \"else\"\n\ntrain['Country'] =  train['Country'].apply(continents)\ntest['Country'] =  test['Country'].apply(continents)","dca6c076":"Genres = ['Soul','Country\/Folk','Metal', 'Rock', 'Pop', 'Jazz',\n          'Classic', 'Dance', 'indie', 'rap','reggae','other']\nfor i in Genres:\n    train[i] = 0\n\nindex = -1\nfor j in train['Artists_Genres']:\n    j = j.split('|')\n    index += 1\n    for i in j:\n        if i in ['country','folk']:\n            train.loc[index, \"Country\/Folk\"] = 1 \n        elif i in [ 'folkmetal','metal','numetal','classicmetal','epicmetal']:\n            train.loc[index, \"Metal\"] = 1\n        elif i == 'soul':\n            train.loc[index, \"Soul\"] = 1 \n        elif i in ['hardrock','rock','allrock','folkrock','prog','rnr']:\n            train.loc[index, \"Rock\"] = 1\n        elif i == \"pop\":\n            train.loc[index, \"Pop\"] = 1\n        elif i in [ 'jazz','tradjazz','conjazz']:\n            train.loc[index, \"Jazz\"] = 1\n        elif i in ['classicalmasterpieces','classical']:\n            train.loc[index, \"Classic\"] = 1\n        elif i in [ 'dance','house']:\n            train.loc[index, \"Dance\"] = 1\n        elif i in [ 'alternative','indie','local-indie']:\n            train.loc[index, \"indie\"] = 1\n        elif i in [ 'rap','foreignrap']:\n            train.loc[index, \"rap\"] = 1\n        elif i in ['reggae','reggaeton']:\n            train.loc[index, \"reggae\"] = 1\n        else:\n            train.loc[index, \"other\"] = 1\n            \n\nfor i in Genres:\n    test[i] = 0\n\nindex = -1\nfor j in train['Artists_Genres']:\n    j = j.split('|')\n    index += 1\n    for i in j:\n        if i in ['country','folk']:\n            test.loc[index, \"Country\/Folk\"] = 1 \n        elif i in [ 'folkmetal','metal','numetal','classicmetal','epicmetal']:\n            test.loc[index, \"Metal\"] = 1\n        elif i == 'soul':\n            test.loc[index, \"Soul\"] = 1 \n        elif i in ['hardrock','rock','allrock','folkrock','prog','rnr']:\n            test.loc[index, \"Rock\"] = 1\n        elif i == \"pop\":\n            test.loc[index, \"Pop\"] = 1\n        elif i in [ 'jazz','tradjazz','conjazz']:\n            test.loc[index, \"Jazz\"] = 1\n        elif i in ['classicalmasterpieces','classical']:\n            test.loc[index, \"Classic\"] = 1\n        elif i in [ 'dance','house']:\n            test.loc[index, \"Dance\"] = 1\n        elif i in [ 'alternative','indie','local-indie']:\n            test.loc[index, \"indie\"] = 1\n        elif i in [ 'rap','foreignrap']:\n            test.loc[index, \"rap\"] = 1\n        elif i in ['reggae','reggaeton']:\n            test.loc[index, \"reggae\"] = 1\n        else:\n            test.loc[index, \"other\"] = 1","af1764ea":"train['Release_year'].unique()","35256211":"def year(x):\n    if x < 1990: \n        return 1980\n    elif x < 2000:\n        return 1990\n    elif x < 2010:\n        return 2000\n    elif x < 2020:\n        return 2010\n    elif x < 2030:\n        return 2020\n    else:\n        return '1970 or older'\n\ntrain['Release_year'] =  train['Release_year'].apply(year)\ntest['Release_year'] =  test['Release_year'].apply(year)\n\nprint(test['Release_year'].unique())\nprint(train['Release_year'].unique())","50db053e":"train = pd.get_dummies(train, columns = ['Version', 'Album_type', 'Key', 'Country'])\ntest = pd.get_dummies(test, columns = ['Version', 'Album_type', 'Key', 'Country'])","0fa81c65":"train = pd.get_dummies(train, columns = ['Album'])\ntest = pd.get_dummies(test, columns = ['Album'])","e4a648a2":"import matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score","fd86213a":"Albums_train = list(train.columns)[30:]\nAlbum_traindata = train.loc[:, Albums_train]\nscaler = StandardScaler() \nscaled_train = scaler.fit_transform(Album_traindata) \n\n# Normalizing the Data \nnormalized_train = normalize(scaled_train) \n\n# Converting the numpy array into a pandas DataFrame \nnormalized_train = pd.DataFrame(normalized_train) \n\n# Reducing the dimensions of the data \npca = PCA(n_components = 2) \nX_principal = pca.fit_transform(normalized_train) \nX_principal = pd.DataFrame(X_principal) \nX_principal.columns = ['x','y'] \n\nX_principal.head(2)","aa74cfd0":"sse = {}\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(X_principal)\n    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of cluster\")\nplt.ylabel(\"SSE\")\nplt.show()\nsilhouette_scores = [] \n\nfor n_cluster in range(2, 8):\n    silhouette_scores.append( \n        silhouette_score(X_principal, KMeans(n_clusters = n_cluster).fit_predict(X_principal))) \n\n# Plotting a bar graph to compare the results \nk = [2, 3, 4, 5, 6,7] \nplt.bar(k, silhouette_scores) \nplt.xlabel('Number of clusters', fontsize = 10) \nplt.ylabel('Silhouette Score', fontsize = 10) \nplt.show() \nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X_principal)\n# Visualizing the clustering \nplt.scatter(X_principal['x'], X_principal['y'],  \n           c = KMeans(n_clusters = 3).fit_predict(X_principal), cmap =plt.cm.winter) \nplt.show()\n\ntrain['Album_1'] = X_principal['x']\ntrain['Album_2'] = X_principal['y'] \n\ntrain = train.drop(columns=Albums_train)\ntrain = train.drop(columns='Artists_Genres')","85a24cba":"Albums_test = list(test.columns)[29:]\nAlbum_testdata = test.loc[:, Albums_test]\nscaler = StandardScaler() \nscaled_test = scaler.fit_transform(Album_testdata) \n\n# Normalizing the Data \nnormalized_test = normalize(scaled_test) \n\n# Converting the numpy array into a pandas DataFrame \nnormalized_test = pd.DataFrame(normalized_test) \n\n# Reducing the dimensions of the data \npca = PCA(n_components = 2) \nX_principal = pca.fit_transform(normalized_test) \nX_principal = pd.DataFrame(X_principal) \nX_principal.columns = ['x','y'] \n\nX_principal.head(2)","1c383733":"sse = {}\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(X_principal)\n    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of cluster\")\nplt.ylabel(\"SSE\")\nplt.show()\nsilhouette_scores = [] \n\nfor n_cluster in range(2, 8):\n    silhouette_scores.append( \n        silhouette_score(X_principal, KMeans(n_clusters = n_cluster).fit_predict(X_principal))) \n\n# Plotting a bar graph to compare the results \nk = [2, 3, 4, 5, 6,7] \nplt.bar(k, silhouette_scores) \nplt.xlabel('Number of clusters', fontsize = 10) \nplt.ylabel('Silhouette Score', fontsize = 10) \nplt.show() \nkmeans = KMeans(n_clusters=4)\nkmeans.fit(X_principal)\n# Visualizing the clustering \nplt.scatter(X_principal['x'], X_principal['y'],  \n           c = KMeans(n_clusters = 3).fit_predict(X_principal), cmap =plt.cm.winter) \nplt.show()\n\ntest['Album_1'] = X_principal['x']\ntest['Album_2'] = X_principal['y'] \n\ntest = test.drop(columns=Albums_test)\ntest = test.drop(columns='Artists_Genres')","ae3552dd":"train.head()","9474641b":"for i in train['Artists']:\n        i = i.split('|')\n        for j in i:\n            train[j] = 0\n            \nindex = -1\nfor j in train['Artists']:\n    j = j.split('|')\n    index += 1\n    for i in j:\n        train.loc[index, i] = 1\n\nArtists = list(train.columns)[31:]\n\nfor i in Artists:\n    train[i] = train[i].fillna(0)\n\nArtists_data = train.loc[:, Artists]\nscaler = StandardScaler() \nscaled_train = scaler.fit_transform(Artists_data) ","26d6aa48":"# Normalizing the Data \nnormalized_train = normalize(scaled_train) \n\n# Converting the numpy array into a pandas DataFrame \nnormalized_train = pd.DataFrame(normalized_train) \n\n# Reducing the dimensions of the data \npca = PCA(n_components = 2) \nX_principal = pca.fit_transform(normalized_train) \nX_principal = pd.DataFrame(X_principal) \nX_principal.columns = ['x','y'] \n\nX_principal.head(2)","e2898138":"sse = {}\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(X_principal)\n    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of cluster\")\nplt.ylabel(\"SSE\")\nplt.show()","05c43154":"kmeans = KMeans(n_clusters=3)\nkmeans.fit(X_principal)\n\nplt.scatter(X_principal['x'], X_principal['y'],  \n       c = KMeans(n_clusters = 3).fit_predict(X_principal), cmap =plt.cm.winter) \nplt.show()\n\ntrain['Artist_1'] = X_principal['x']\ntrain['Artist_2'] = X_principal['y']\ntrain = train.drop(columns=Artists)\ntrain = train.drop(columns='Artists')","fc0c193a":"for i in test['Artists']:\n    if type(i) == str:\n        i = i.split('|')\n        for j in i:\n            test[j] = 0\n            \n        \nindex = -1\nfor j in test['Artists']:\n    if type(j) == str:\n        j = j.split('|')\n        index += 1\n        for i in j:\n            test.loc[index, i] = 1\n\nArtists = list(test.columns)[30:]\n\nfor i in Artists:\n    test[i] = test[i].fillna(0)\n\nArtists_data = test.loc[:, Artists]\nscaler = StandardScaler() \nscaled_train = scaler.fit_transform(Artists_data) ","95a73298":"# Normalizing the Data \nnormalized_train = normalize(scaled_train) \n\n# Converting the numpy array into a pandas DataFrame \nnormalized_train = pd.DataFrame(normalized_train) \n\n# Reducing the dimensions of the data \npca = PCA(n_components = 2) \nX_principal = pca.fit_transform(normalized_train) \nX_principal = pd.DataFrame(X_principal) \nX_principal.columns = ['x','y'] \n\nX_principal.head(2)","b553f38b":"sse = {}\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(X_principal)\n    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of cluster\")\nplt.ylabel(\"SSE\")\nplt.show()","623936e2":"kmeans = KMeans(n_clusters=3)\nkmeans.fit(X_principal)\n\nplt.scatter(X_principal['x'], X_principal['y'],  \n       c = KMeans(n_clusters = 3).fit_predict(X_principal), cmap =plt.cm.winter) \nplt.show()\n\ntest['Artist_1'] = X_principal['x']\ntest['Artist_2'] = X_principal['y']\ntest = test.drop(columns=Artists)\ntest = test.drop(columns='Artists')","6e6ba3a5":"for i in train['Labels']:\n    if i is str:\n        i = i.split('|')\n        for j in i:\n            train[j] = 0\n\nindex = -1\nfor j in train['Labels']:\n    j = j.split('|')\n    index += 1\n    for i in j:\n        train.loc[index, i] = 1\n        \nLabels = list(train.columns)[32:]\n\nfor i in Labels:\n    train[i] = train[i].fillna(0)","b8cf8bd7":"Labels_data = train.loc[:, Labels]\nscaler = StandardScaler() \nscaled_train = scaler.fit_transform(Labels_data) \n  \n# Normalizing the Data \nnormalized_train = normalize(scaled_train) \n  \n# Converting the numpy array into a pandas DataFrame \nnormalized_train = pd.DataFrame(normalized_train) \n  \n# Reducing the dimensions of the data \npca = PCA(n_components = 2) \nX_principal = pca.fit_transform(normalized_train) \nX_principal = pd.DataFrame(X_principal) \nX_principal.columns = ['Labels1','Labels2'] \n  \nX_principal.head(2)","5b2afa9d":"sse = {}\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(X_principal)\n    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of cluster\")\nplt.ylabel(\"SSE\")\nplt.show()","7a79428c":"kmeans = KMeans(n_clusters=3)\nkmeans.fit(X_principal)","a16080e0":"plt.scatter(X_principal['Labels1'], X_principal['Labels2'],  \n           c = KMeans(n_clusters = 3).fit_predict(X_principal), cmap =plt.cm.winter) \nplt.show() ","42c966cd":"train['Labels1'] = X_principal['Labels1']\ntrain['Labels2'] = X_principal['Labels2']\ntrain = train.drop(columns=Labels)\ntrain = train.drop(columns='Labels')","733a66ff":"for i in test['Labels']:\n    if type(i) == str:\n        i = i.split('|')\n        for j in i:\n            test[j] = 0\n            \nindex = -1\nfor j in test['Labels']:\n    if type(j) == str:\n        j = j.split('|')\n        index += 1\n        for i in j:\n            test.loc[index, i] = 1\n        \nLabels = list(test.columns)[31:]\n\nfor i in Labels:\n    test[i] = test[i].fillna(0)\n    \nLabels_data = test.loc[:, Labels]\nscaler = StandardScaler() \nscaled_train = scaler.fit_transform(Labels_data) \n  \n# Normalizing the Data \nnormalized_train = normalize(scaled_train) \n  \n# Converting the numpy array into a pandas DataFrame \nnormalized_train = pd.DataFrame(normalized_train) \n  \n# Reducing the dimensions of the data \npca = PCA(n_components = 2) \nX_principal = pca.fit_transform(normalized_train) \nX_principal = pd.DataFrame(X_principal) \nX_principal.columns = ['Labels1','Labels2'] \n  \nX_principal.head(2)","67a8d27e":"sse = {}\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(X_principal)\n    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of cluster\")\nplt.ylabel(\"SSE\")\nplt.show()","2ac41dc2":"kmeans = KMeans(n_clusters=4)\nkmeans.fit(X_principal)","350ee740":"plt.scatter(X_principal['Labels1'], X_principal['Labels2'],  \n           c = KMeans(n_clusters = 4).fit_predict(X_principal), cmap =plt.cm.winter) \nplt.show() ","479fa9a8":"test['Labels1'] = X_principal['Labels1']\ntest['Labels2'] = X_principal['Labels2']\ntest = test.drop(columns=Labels)\ntest = test.drop(columns='Labels')","137873cc":"train = train.drop(columns='Track')\ntest = test.drop(columns='Track')","2cf845cc":"train","d6823706":"test = test.loc[:299]\ntest","5174612b":"dataset = train.values","0154754b":"dataset","a2b8a92a":"X = dataset[:,2:] \nY = dataset[:,1]","974a19c7":"from sklearn import preprocessing","364134eb":"min_max_scaler = preprocessing.MinMaxScaler()\nX_scale = min_max_scaler.fit_transform(X)","60227754":"from sklearn.model_selection import train_test_split","fa0526da":"X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.3)\nX_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5) \nprint(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)","e40c4e31":"from keras.models import Sequential\nfrom keras.layers import Dense","0f32390d":"model = Sequential([\n    Dense(32, activation='relu', input_shape=(30,)),\n    Dense(32, activation='relu'),\n    Dense(1, activation='sigmoid'),\n])","d03d93f0":"model.compile(optimizer='sgd',  #sgd\u2019 refers to stochastic gradient descent (over here, it refers to mini-batch gradient descent),\n              loss='binary_crossentropy', #The loss function for outputs that take the values 1 or 0 is called binary cross entropy.\n              metrics=['accuracy'])","e0003901":"result = model.fit(X_train, Y_train,\n          batch_size=42, epochs=100, # these parameters can significantly change your accuracy.\n          validation_data=(X_val, Y_val))","440f5813":"plt.plot(result.history['loss'])\nplt.plot(result.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='upper right')\nplt.show()","8b8ec00f":"plt.plot(result.history['accuracy'])\nplt.plot(result.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='lower right')\nplt.show()","f6af47e5":"errors = abs(np.array(result.history['loss']) - np.array(result.history['val_loss']))\nprint('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\nmape = 100 * (errors \/ result.history['accuracy'])\n# Calculate and display accuracy\naccuracy = 100 - np.mean(mape)\nprint('Accuracy:', round(accuracy, 2), '%.')\nprint()","f976ee05":"deploy = test.drop(columns='Id')\ndeploy","ff27387a":"deploy_scale = np.array(min_max_scaler.fit_transform(deploy))","f1da34e8":"test['Like_NN'] = model.predict(deploy_scale).round().astype(int)","9b688b15":"test[['Id', 'Like_NN']]","a7cc5ff0":"X_train  = train.drop(['Category', 'Id'],axis=1)\ny_train = train['Category']\n\nX_test  = test.drop(['Id', 'Like_NN'],axis=1)","6e786ba1":"x_data  = train.drop(['Category', 'Id'],axis=1)\ny = train['Category']\n\nX_train2, X_test2, y_train2, y_test2 = train_test_split(x_data,y,test_size=0.3)","abc10596":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nscaler.fit(X_test)\n\nscaler.fit(X_train2)\nX_train2 = pd.DataFrame(data=scaler.transform(X_train2),columns = X_train2.columns,index=X_train2.index)\nX_test2 = pd.DataFrame(data=scaler.transform(X_test2),columns = X_test2.columns,index=X_test2.index)","88695967":"X_train = pd.DataFrame(data=scaler.transform(X_train),columns = X_train.columns,index=X_train.index)\nX_test = pd.DataFrame(data=scaler.transform(X_test),columns = X_test.columns,index=X_test.index)\nX_test","97bc43c5":"from sklearn.svm import SVC\nsvclassifier = SVC(kernel='linear')\nsvclassifier.fit(X_train, y_train)\ny_predSVC = svclassifier.predict(X_test)\ntest['Like_SVC'] = y_predSVC\ntest[['Id', 'Like_SVC']]\n\nsvclassifier.fit(X_train2, y_train2)\n\ny_pred2 = svclassifier.predict(X_test2)\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test2,y_pred2))\nprint(classification_report(y_test2,y_pred2))","154203f2":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(X_train,y_train)\nrf_pred = rf.predict(X_test)\ntest['Like_RF'] = rf_pred\ntest[['Id', 'Like_RF']]\n\nrf.fit(X_train2,y_train2)\nrf_pred2 = rf.predict(X_test2)\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test2,rf_pred2))\nprint(classification_report(y_test2,rf_pred2))","dc6561c4":"from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\nfrom sklearn import linear_model, tree, ensemble\nkf =KFold(n_splits=5, shuffle=True, random_state=42)\n\ncnt = 1\n# split()  method generate indices to split data into training and test set.\nfor train_index, test_index in kf.split(X_train2, y_train2):\n    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n    cnt += 1\n\nn_estimators = [50, 100, 150, 200, 250, 300, 350]\n\nfor val in n_estimators:\n    score = cross_val_score(ensemble.RandomForestClassifier(n_estimators= val, random_state= 42), X, y, cv= kf, scoring=\"accuracy\")\n    print(f'Average score({val}): {\"{:.3f}\".format(score.mean())}')\n    \nrf = ensemble.RandomForestClassifier(n_estimators= 100, random_state= 42)\nrf.fit(X_train2,y_train2)\nrf_pred2 = rf.predict(X_test2)\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test2,rf_pred2))\nprint(classification_report(y_test2,rf_pred2))\n\nrf.fit(X_train,y_train)\nrf_pred2 = rf.predict(X_test)\nrf_pred2","f487199a":"from sklearn.neighbors import KNeighborsClassifier\nKN = KNeighborsClassifier()\nKN.fit(X_train,y_train)\nKN_pred = KN.predict(X_test)\n\ntest['Like_KN'] = KN_pred\ntest[['Id', 'Like_KN']]\n\nKN.fit(X_train2,y_train2)\nKN_pred2 = KN.predict(X_test2)\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test2,KN_pred2))\nprint(classification_report(y_test2,KN_pred2))","ec8895af":"from xgboost import XGBClassifier","854ea7a6":"from sklearn.metrics import accuracy_score\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\n# make predictions for test data\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\n\nmodel.fit(X_train2, y_train2)\n# make predictions for test data\ny_pred2 = model.predict(X_test2)\npredictions = [round(value) for value in y_pred2]\n\naccuracy = accuracy_score(y_test2, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","12905d3a":"test['Like_XB'] = y_pred","89969a10":"test[['Id', 'Like_NN', 'Like_SVC', 'Like_KN', 'Like_XB']].head(20)","fd77644f":"sample = pd.read_csv(\"..\/input\/mymusicalprefrences\/sample_submition.csv\")\nsample[\"Category\"] = model.predict(deploy_scale).round().astype(int)\nsample.to_csv(\"deploy.csv\", index=False)","1c315f7e":"sample.head(10)","04107ed1":"# Dealing with NaNs","da38edf7":"# **Random Forest**","e60b788b":"# Genres","8deb5463":"# KMeans Clustering","3de943ff":"# Major\/Minor","7bfda3d3":"# Year","bf5568f8":"# Scaling","5b5d1cc9":"# Predictions","6eeb5c43":"# Country","ebcae47f":"# **KNeighbors**","6e77d83c":"# Artists","cdaf1165":"# xgbboost","a257fbc8":"# Neural Network","608319d6":"# **SVC**","b8a76aec":"# Albums","c5a0c32d":"# Labels","940ce825":"# Vocals "}}