{"cell_type":{"42ac58fb":"code","4d5bc258":"code","143e863f":"code","dd9eafc8":"code","6bad90a2":"code","42e44348":"code","47cd85d2":"code","98434762":"code","ce955bd2":"code","f02623e9":"code","802785b2":"code","943bbac7":"code","e06c0cf0":"code","636c396f":"code","2fb9245f":"code","27cabf01":"code","6a03db93":"code","d0878f14":"code","7161cbf9":"code","9962fced":"code","323f5307":"code","139684ca":"code","2340d714":"code","fb0746f7":"code","8fc4d6c0":"code","b93a567a":"code","74598a9c":"code","13017b49":"code","855f4cf3":"code","a3ee7b6c":"code","a9a15090":"code","a064923b":"code","0273ce33":"code","af5f2d27":"code","cb458be4":"code","c7c48c9e":"markdown","038d58af":"markdown","44c75e75":"markdown","6371ef5d":"markdown","9b7fb1c7":"markdown","0cf67a76":"markdown","5c28eba6":"markdown","ba7acf41":"markdown","42a10dca":"markdown","bf8c5981":"markdown","d6a58307":"markdown","7949ba4f":"markdown","f95e0dd3":"markdown","4da1452a":"markdown","8535e499":"markdown","05318010":"markdown","aa276319":"markdown","c9254f45":"markdown","46dd2246":"markdown","7bd6a147":"markdown","969a8f41":"markdown","11562d43":"markdown","90884927":"markdown","b346d082":"markdown","f747b8e2":"markdown","d8806a20":"markdown","fb41e7da":"markdown","23855cbd":"markdown","16d14a63":"markdown"},"source":{"42ac58fb":"import pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport warnings\nwarnings.simplefilter('ignore') #supress all future warnings\n#from sklearn.tree import DecisionTreeRegressor\n\n#load training and test data\nhome_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n\n\n","4d5bc258":"corr_vals = home_data.corrwith(home_data.SalePrice) #correlating variables\ncorr_vals.sort_values(ascending = False,inplace = True) #sorting them from high to low\n\nplt.rcParams[\"figure.figsize\"]=25,5; fig, ax = plt.subplots(2)\nax[0].bar(corr_vals.index[1:21],corr_vals[1:21]);ax[0].set_ylabel('Correlation')\nax[1].bar(corr_vals.index[1:21],(home_data.shape[0]- home_data[corr_vals.index[1:21]].count()),color = 'tab:red');plt.ylabel('# of Missing Values') #total entries - present entries\nplt.show()","143e863f":"plt.rcParams[\"figure.figsize\"]=25,2.5;fig2,ax2 = plt.subplots(); \nax2.bar(corr_vals.index[1:21],(test_data.shape[0]- test_data[corr_vals.index[1:21]].count()),color = 'tab:red');plt.ylabel('# of Missing Values') ;plt.show()","dd9eafc8":"def train_by_feat(y,features,home_data):\n    #Trains model based on list of features, splits data into training and validation by the features.\n    X = home_data[features]\n    train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n    \n    model = RandomForestRegressor(random_state = 1)\n    model.fit(train_X,train_y)\n    feat_val_predictions = model.predict(val_X)\n    feat_val_mae = mean_absolute_error(feat_val_predictions, val_y)\n    \n    return feat_val_mae","6bad90a2":"first_opti = pd.Series({num:train_by_feat(home_data.SalePrice,corr_vals.index[1:num+1],home_data) for num in np.arange(10)+1}) #storing MEA vs # of model params\n\nplt.rcParams[\"figure.figsize\"]=8,5;plt.plot(first_opti.index,first_opti.data);plt.xlabel('# of parameters');plt.ylabel('MEA');plt.title('MEA vs # of parameters')","42e44348":"from datetime import date\n# creating date time parameters as a serial value\nall_datetime_serial= pd.Series({idx:date.toordinal(pd.to_datetime(str(home_data.YrSold[idx])+'\/'+str(home_data.MoSold[idx]))) for idx in home_data.index})\n\nprint(\"Correlation of combine Year and Month: %.3f\" %(all_datetime_serial.corr(home_data.SalePrice)))\nprint(\"Correlation of Year %.3f\" %(home_data.YrSold.corr(home_data.SalePrice)))\nprint(\"Correlation of Month %.3f\" %(home_data.MoSold.corr(home_data.SalePrice)))","47cd85d2":"all_datetime = pd.Series({idx:pd.to_datetime(str(home_data.YrSold[idx])+'\/'+str(home_data.MoSold[idx])) for idx in home_data.index})\nplt.rcParams[\"figure.figsize\"]=16,5; plt.scatter(all_datetime,home_data.SalePrice); plt.xlabel('Date') ; plt.ylabel('Sale Price USD$');plt.title('House sale price over time');","98434762":"#calculating average house price of each month\navg_month = pd.Series({mo:home_data.SalePrice[home_data.MoSold == mo].mean() for mo in pd.Series(home_data.MoSold.unique()).sort_values()}) \nplt.rcParams[\"figure.figsize\"]=7,5;plt.scatter(avg_month.index,avg_month.data);plt.xlabel('Month');plt.ylabel('Average Sale Price');plt.title('Avg. sale price over 12 months');","ce955bd2":"category =  np.setdiff1d(home_data.columns,corr_vals.index)\ncategory","f02623e9":"numerical_home_data = home_data[corr_vals.index].copy() # creating a new dataframe but with only numerical data, to which we will append our dummy variables","802785b2":"numerical_home_data.shape ","943bbac7":"num_home_data_dummy = numerical_home_data.add(pd.get_dummies(home_data[category]),fill_value =0) # creating dummy variables","e06c0cf0":"num_home_data_dummy.shape","636c396f":"#all the top 20 correlations\ndummy_corr = num_home_data_dummy.corrwith(num_home_data_dummy.SalePrice)\nplt.rcParams[\"figure.figsize\"]=28,4\nfig, ax = plt.subplots()\nax = plt.bar(dummy_corr.sort_values(ascending = False).iloc[1:20].index,dummy_corr.sort_values(ascending = False).iloc[1:20]) ; plt.xlabel('Variable') ; plt.ylabel('Correlation');\nfor bar in ax.patches:\n    bar.set_facecolor('#888888')\nax.patches[7].set_facecolor('#aa3333');ax.patches[11].set_facecolor('#aa3333');ax.patches[12].set_facecolor('#aa3333');ax.patches[16].set_facecolor('#aa3333');ax.patches[17].set_facecolor('#aa3333');ax.patches[18].set_facecolor('#aa3333')\n","2fb9245f":"def smart_fill(to_fill,frame):\n    '''\n    Takes dataframe and specific variable in it. Fills the missing variables in to_fill through intepolation with correlated variables.\n    '''\n    top_corr = frame.corrwith(frame[to_fill]).sort_values(ascending = False).index[1:4] #top 3 correlating values to variable being\n    frame.sort_values(by=list(top_corr),inplace= True) #sorting frame with highest correlating variables with missing series\n    frame[to_fill].interpolate(inplace = True) #filling nans with interpolation, inferring position by the sorting\n    frame[to_fill].fillna(method = 'bfill',inplace= True)\n    frame[to_fill].fillna(method = 'ffill',inplace= True)\n    #bfill & ffill needed as you can't interpolate for edge values, and missing edge values are left NaN by interpolate method\n    return frame  ","27cabf01":"filled_cols = []\nfor col in num_home_data_dummy.columns:\n    if num_home_data_dummy[col].isnull().any()==1: #only run for columns with missing data\n        filled_cols.append(col)\n        smart_fill(col,num_home_data_dummy)","6a03db93":"filled_cols # variables which have had their missing data filled!","d0878f14":"print(\"Missing data in test data: {0}\".format(test_data.isnull().any().count()))","7161cbf9":"\ntest_cat = test_data[corr_vals.index.drop(corr_vals.index[0])].add(pd.get_dummies(test_data[category]),fill_value = 0).copy()\n#train_data = home_data[corr_vals.index].add(pd.get_dummies(home_data[category]),fill_value = 0).copy()\n","9962fced":"filled_cols2 = []\nfor col in test_cat.columns:\n    if test_cat[col].isnull().any()==1: #only run for columns with missing data\n        filled_cols2.append(col)\n        test_cat = smart_fill(col,test_cat)","323f5307":"print(\"The number of missing data filled is {0}\".format(len(filled_cols2))) # a lot of missing data was filled in the categorical data!","139684ca":"#creating training data\ntrain_data = home_data[corr_vals.index].add(pd.get_dummies(home_data[category]),fill_value = 0).copy()\n\nfilled_cols2 = []\nfor col in train_data.columns:\n    if train_data[col].isnull().any()==1: #only run for columns with missing data\n        filled_cols2.append(col)\n        train_data = smart_fill(col,train_data)","2340d714":"def train_by_feat(y,features,home_data):\n    '''\n    Trains a model based on a specified list of features and returns the MAE value for that model.\n    '''\n    X = home_data[features]\n    train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n    \n    model = RandomForestRegressor(random_state = 1)\n    model.fit(train_X,train_y)\n    feat_val_predictions = model.predict(val_X)\n    feat_val_mae = mean_absolute_error(feat_val_predictions, val_y)\n    \n    return feat_val_mae","fb0746f7":"features = train_data.corrwith(train_data.SalePrice).sort_values(ascending = False).keys()","8fc4d6c0":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","b93a567a":"opti = pd.Series({num:train_by_feat(train_data.SalePrice,features[1:num+2],train_data) for num in np.arange(100)}) #need +2 to offset the fact we are starting at index 1","74598a9c":"plt.rcParams[\"figure.figsize\"]=7,5;plt.plot((1+np.arange(100)),opti);plt.xlabel('Number of Features');plt.ylabel('MAE');plt.title('Optimization of Random Forrest Regressor');","13017b49":"print(\"Mininmum MAE of {0} with {1} features\".format(opti[opti.idxmin()],opti.idxmin()))","855f4cf3":"#Creating model of the 85 features\ny = train_data.SalePrice\nX = train_data[features[1:87]]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n    \nmodel = RandomForestRegressor(random_state = 1)\nmodel.fit(train_X,train_y)\nfeat_val_predictions = model.predict(val_X)\nmean_absolute_error(feat_val_predictions, val_y) # shows we are assuming a low error in our predictions","a3ee7b6c":"X_test = test_cat[features[1:87]]\ny_pred = model.predict(X_test)","a9a15090":"output = pd.DataFrame({'Id': test_data.Id,'SalePrice': y_pred})\n\ngood_result = pd.read_csv(\"..\/input\/erincb-house-submission\/submission_good.csv\")\ngood_result.rename(columns ={'SalePrice':'SalePriceCorrect'},inplace = True)\ngood_result['SalePriceWrong'] = y_pred\ngood_result.head()","a064923b":"#create categorical dummy sets\ntrain_cat = pd.get_dummies(home_data)\ntest_cat = pd.get_dummies(test_data)\n\n#creating list of indexes in correlating order\ncat_corr = train_cat.corrwith(train_cat.SalePrice).sort_values(ascending = False)\n\n#filling missing values\ntrain_cat.fillna(method ='pad',inplace= True)\ntest_cat.fillna(method = 'pad',inplace = True)\n\n#determining optimum number of features ()\nopti = pd.Series({num:train_by_feat(train_cat.SalePrice,cat_corr.index[1:num+2],train_cat) for num in np.arange(30)}) #30 chosen arbitrarily from trial and error on kaggle score\n\nprint(\"Number of optimum features {0}\".format(opti.idxmin()))\n","0273ce33":"#creatingthe model\nX = train_cat[cat_corr.index[1:28]]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n    \n\nmodel = RandomForestRegressor(random_state = 1)\nmodel.fit(train_X,train_y)\nfeat_val_predictions = model.predict(val_X)\nfeat_val_mae = mean_absolute_error(feat_val_predictions, val_y)","af5f2d27":"#making predictions\npredictions = model.predict(test_cat[cat_corr.index[1:28]])\nouty = pd.DataFrame({'Id': test_data.Id,'SalePrice': predictions})\nouty.head()","cb458be4":"outy.to_csv('submission.csv',index = False)","c7c48c9e":"### Optimizing based on parameter\nThe function below will create a model based on input data and will return the Mean Average Error for that particular model. I made this function so I could get a plot of \"Number of Paramaters vs. Prediction Accuracy\" -where MEA is being used as my measure of prediction accuracy. As of yet, the missing data has not been replaced, so only the highet 10 correlating variables can be used so far (model error on predicitng with missing values).","038d58af":"# Problems with this approach, what I learned, and my solution the the issue I created\nAs mentioned, I ran into some issues with this model, I would create a model on the above 85 features, then make a submission to kaggle and my score would be extremely high (bad). I couldn't work out what the issue was for a while. Until I looked at the predictions being made and compared them to what was given in the tutorial. \n\nThis showed just how wrong the predictions were.","44c75e75":"Now the number of paramters that can be modelled from has increased from 38 to 290! \n\nThe graph below shows the correlation chart from before, but with new entries of categorical dummies highlighted in red.","6371ef5d":"From this it is pretty clear that there is low correlation with month due to how scattered price is with month.","9b7fb1c7":"## Improvements\n","0cf67a76":"### Combining sale Month and sale Year into single DateTime value\nIn some of the forum discussions people were having on the tutorial, a common mention was how the housing market was faring post- market crash. Currently month and year are sitting pretty low in correlation. So I wondered if this is because they were separated, and wanted to know what the correlation would be if you combined them.\n","5c28eba6":"# Applying improvements to the test data\n\nApplying both the caterogical and data cleaning changes from above to the test data.","ba7acf41":"Looking at our high correlation variables we can we see that for the most part in our training data we have most of the top 20 not missing!\n\nThe plot below shows the number of missing datapoints for the testing data set for the same parameters. From which there appears to be missing data in similar places which will need to be filled in.","42a10dca":"# My first improvments to me first Kaggle tutorial method\nThe other week I completed my first set of kaggle tutorials for [Intro to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning). This tutorial involved learning the basics of machine learning to making predictions of house prices bases on previous sale data. After this I experimented with trying to improve my score\/predictions based on what I thought I could do to improve it.\n\nI have shown some of the main ideas I had, those which did work, and those which did not! I hope I can share my thought process behind what I have done, and hopefully once I have learned more I will be able to come back and make some improvements!","bf8c5981":"This time 11 variables have had missing data filled in the test data, as opposed to only 3 in the training data.","d6a58307":"Even though this notebook was quite long (something to improve) the final bits of code were not too complex.\n\nEven without the _smart_fill_ function, going up to >30 predictions still results in large error. This is something I may wish to look into with using random forest models.","7949ba4f":"## Filling missing data\nHaving trying to use sale month + year, and adding categorical features, the last improvement to the data I wanted to make was to fill any missing values.","f95e0dd3":"### Possible Improvements\nSome of the ideas I had to improve the prediction inclded:\n\n-Interpolating missing values based on top correlated variables using multi-column sorting and _fillna(method=ffill)_\n\n-Combining month and year to date-time for year-of-sale context (i.e. housing market crash), see how this raises time values in correlation\n\n-Converting categorical data (which there is plenty of) into dummy data then re-checking correlations. Capturing propety features which are not currently possible to model as they are non-numerical\n","4da1452a":"### Converting categorial fields to dummy-numerical fields\nTo increase the number of modelling parameters available, I used _pd.get_dummies()_ to allow for predictions based on the qualatative values in the data set.","8535e499":"Above is a list of all the categorical variables that currently can't be used","05318010":"Adding categorical variables to the test data.","aa276319":"It is clear that there is a pattern which occurs over the months with prices peaking in second third of each year with more variance in the middle.","c9254f45":"## Overall correlation and missing data\nThe parameters in the tutorial were given to us without context. I had recently learnt about _.corr_with_, and wanted to apply it hear to choose parameters with a stronger relation to what we are tring to predict- SalePrice.\n\nBelow are two plots of the 20 highest correlation paramters to sale price. One showing the correlation coefficient of the variable with price, and the second showing the number of missing values (not-including infinites) for each variable.","46dd2246":"From this we can see that the correlation of combining the two is actually quite low. Which makes sense, combining two parameters with a weak realtion to Sale Price won't increase correlation.\n\n_We can tell that from this that there is also a very slight decrease in house sale price with the low correlation of SalePrice with year_\n\nWith this line which creates a timeseries series, we can look at how the average price moves over each year","7bd6a147":"# Creating and optimizing the model\n\nNow that both the training and the testing data sets have been prepared, I am going to create the model using the _Random Forest Regression_ model as suggested by the tutorial. One day once I am familiar with other models I may come back and use another.\n\nI am applying a simple optimization here- inspired by the MAE optimization in the tutorial. However, instead of tree depth, I am going to vary the number of model parameters until a minimum MAE has been found.","969a8f41":"_features_ is the new ordered list of parameters' correlated with SalePrice indexes.","11562d43":"From this alone we can see that there are large improvements to be made in the accuracy of the model by increasing the number of parameters. Something I which I tried to improve on in a variety of ways.","90884927":"Filling the missing values","b346d082":"Only a few variables needed filling for the training data.","f747b8e2":"# Submitted predictions:\n\nIn my latest (and last for the time being) submission to kaggle I opted to use a simpler approach for missing values, and I kept the model optimization.","d8806a20":"Determining optimum variables...","fb41e7da":"Using a dictionary comprehension to loop through all the top 100 features.","23855cbd":"This shows just how wrong the predictions were, I eventually isolated the problem to my _smart_fill_ function. It appeared to be changing the the values in places where I didn't want them to change.\n\nFrom this I learned:\n\n-that trying to simplify my methods will make troubleshooting easier (_smart_fill_)\n\n-MAE does not always mean that the predictions will be accurate\n\n-and in general from writing this notebook- to keep my explanations shorter and less bloated (sorry reader!)","16d14a63":"The graph above shows how MAE decreases by adding more features to the model, and the the \"optimum\" number of model featues is the top 85. However, unfortunately I would run into issues using this model."}}