{"cell_type":{"19ce9c36":"code","895fd299":"code","eca7697d":"code","ac74a62c":"code","4686d10d":"code","c10c9770":"code","58bb0703":"code","69dc4dd1":"code","775cc1fa":"code","b87ec85b":"code","501037ff":"code","74cc945f":"code","267243d4":"code","55e2190e":"code","1ec5ad03":"code","fe18fa37":"code","153567d7":"code","e97e01f0":"code","284af257":"code","2eb0e438":"code","10443bea":"code","3d18a71d":"code","2666370e":"code","3c229d08":"code","7fa0a4fb":"code","4654abc8":"code","8fdd34b0":"code","b48dff88":"markdown","cba0050a":"markdown","e3193b37":"markdown","3190e858":"markdown","8df23da1":"markdown","a8d06222":"markdown","39f40358":"markdown"},"source":{"19ce9c36":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","895fd299":"import pandas as pd\nimport numpy as np\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\npd.options.display.max_columns = 500\npd.options.display.max_rows = 500\n\nimport matplotlib\nmatplotlib.rc(\"font\", family = \"AppleGothic\")\nmatplotlib.rc(\"axes\", unicode_minus = False)\n\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats(\"retina\")","eca7697d":"train = pd.read_csv(\"\/kaggle\/input\/otto-group-product-classification-challenge\/train.csv\",  index_col = \"id\")\n\nprint(train.shape)\ntrain.head()","ac74a62c":"test = pd.read_csv(\"\/kaggle\/input\/otto-group-product-classification-challenge\/test.csv\",  index_col = \"id\")\n\nprint(test.shape)\ntest.head()","4686d10d":"!conda install -c conda-forge -y lightgbm","c10c9770":"# train \ud655\uc778\uacb0\uacfc \uc774\ub294 feature_name\uc744 \uc120\ubcc4\uc801\uc73c\ub85c \uc120\ud0dd\ud558\ub294 \uac83\uc774 \uc544\ub2c8\ub77c \uc804\uccb4\ub97c \uc885\ud569\uc801\uc73c\ub85c \uc0ac\uc6a9\ud55c \ub4a4 modeling\ud574\uc57c\ud558\ub294 \ubb38\uc81c\uc774\ub2e4\n# \ub530\ub77c \ub370\uc774\ud130\ubd84\uc11d\ubcf8\ub2e4\ub294 \uc815\ud655\ud55c \uc608\uce21\ubaa8\ud615\uc744 \ub9cc\ub4dc\ub294\uac8c \ub354 \uc911\uc694\ud558\ub2e4\uace0 \ubcfc\uc218 \uc788\uc74c","58bb0703":"# \uc808\ub300\uc801\uc778 \uc218\uce58\ub294 class_2\uac00 \uc81c\uc77c \ub9ce\uace0 \uadf8\ub2e4\uc74c class_6\uc774 \ub9ce\ub2e4 \n# \uac00\uc7a5 \ub9ce\uc740 \ub370\uc774\ud130\uac00 \ubb34\uc5c7\uc778\uc9c0 \ud30c\uc545\ud558\ub294 \ucc28\uc6d0\uc5d0\uc11c \ubcf4\ub294 \uac83\uc774\uace0 \uc774\uac83\uc774 label_name\uc744 \uacb0\uc815\ud558\ub294\ub370 \ud070 \uc601\ud5a5\uc744 \uc8fc\uc9c0\ub294 \uc54a\ub294\ub2e4\n\nsns.countplot(x = \"target\", data = train)","69dc4dd1":"# \uac00\uc7a5 \uc218\uac00 \ub9ce\uc740 class_2\ub97c \uc77c\ubd80 \ubf51\uc544\uc11c \uc5b4\ub5a4 feat\uc774 \uac00\uc7a5 \ub9ce\uc740\uc9c0 \ud655\uc778\ud574\ubcf8\ub2e4\n# \uacb0\uacfc \ubc11\ucc98\ub7fc feat_14 \/ feat_40 \/ feat_15 \uc21c\uc73c\ub85c \uc22b\uc790\uac00 \ub9ce\uc740 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4\n\n# \ud558\uc9c0\ub9cc \uc5ec\uae30\uc11c\ub294 \uc774\ub7f0\uc2dd\uc73c\ub85c \ub3c5\ub9bd\uc801\uc778 \ubd84\uc11d\uc744 \uc9c4\ud589\ud558\uba74 \uc548\ub41c\ub2e4\n# feat_14\uac00 \ud569\uc0b0\uc744 \ud588\uc744 \ub54c \uac00\uc7a5 \ud070 \uc22b\uc790\uc774\uae34 \ud558\uc9c0\ub9cc, \uc774\uc678\uc5d0 feat_9\uc758 \uacbd\uc6b0 \uc77c\uc815 \ud070 \uc218\uac00 \uc788\ub294 \uacbd\uc6b0 \uc774\uac83\ub3c4 class_2\uc758 \uacb0\uacfc\ub85c \uc774\uc5b4\uc9c0\ub294 \ubaa8\uc2b5\uc744 \ubcf4\uc5ec\uc900\ub2e4\n# \uc774\ub294 \uc77c\uc815\uc218\uc758 feat\ubcc4\ub85c \uc5f0\uad00\uc131\uc744 \uac00\uc838 \ub3c5\ub9bd\uc131\uc774 \uc544\ub2cc, \uc5f0\uc18d\uc131\uc744 \ub748 \ub370\uc774\ud130\ub77c\uace0 \ubd10\uc57c\ud55c\ub2e4 \n\n# \uc544\ub9c8 0,1,2,3,4,5\uc640 \uac19\uc740 single product\ub97c \uc5b4\ub290\ucabd\uc5d0\uc11c\ub294 feat1\ub85c , \uc5b4\ub290\ucabd\uc740 feat2\ub85c \ubcf4\uc558\ub2e4\ub294 \uc774\uc57c\uae30\uc774\uace0 \n# \ub530\ub77c\uc11c \uc774\ub7ec\ud55c \uc0c1\uad00\uad00\uacc4\ub97c \uc5f0\uacb0\ud560 \uc218 \uc788\ub294 \ubaa8\ub378\uc744 \ucc3e\uc544\uc57c\ud568\uc744 \ubcf4\uc5ec\uc900\ub2e4 \n\ntrain_feat_2 = train[train[\"target\"] == \"Class_2\"]\ntrain_feat_2 = train_feat_2.drop(\"target\", axis = 1)\ntrain_feat_2 = train_feat_2.T\ntrain_feat_2[\"Total\"] = train_feat_2.sum(axis = 1)\ntrain_feat_2\n\ntrain_feat_2.groupby(train_feat_2.index)[\"Total\"].sum().reset_index().\\\nsort_values(by = \"Total\", ascending = False).head()","775cc1fa":"# \uac01 feat\ubcc4\ub85c \uc0c1\uad00\uad00\uacc4\ub97c \ud30c\uc545\ud558\uc5ec \uc5b4\ub290\uc815\ub3c4 \uc5f0\uad00\uc131\uc774 \uc788\ub294\uc9c0 \ucd94\uce21\ud574\ubcf8\ub2e4. \n# \uc5ec\uae30\uc11c \uc54c \uc218 \uc788\ub294 \uc0ac\uc2e4\uc740 feature_name\uc744 \uc9c4\ud589\ud560 \ub54c \uc804 feat\ub97c \uc0ac\uc6a9\ud574\uc57c \ud55c\ub2e4\ub294 \uac83\uc774\ub2e4 \n# \uac01 feat\uac00 \uc5b4\ub290\uc815\ub3c4 \uc5f0\uad00\uc131\uc744 \uac00\uc9c0\uace0 \uc11c\ub85c\uc5d0\uac8c \uc601\ud5a5\uc744 \uc8fc\uba70 \uc6c0\uc9c1\uc774\uace0 \uc788\uc74c\uc744 \uc54c \uc218 \uc788\uace0 \ud655\uace0\ud558\uac8c \uc601\ud5a5\uc744 \ubbf8\uce58\ub294 \uac83\uc740 \uc801\ub2e4\ub294 \uc810\uc744 \uc54c \uc218 \uc788\uae30\uc5d0 \n# \uc774 \ubb38\uc81c\uac00 log_loss\ub97c \ud1b5\ud574 \ubb38\uc81c\ub97c \ud480\uc5b4\uc57c\ud558\ub294 \uc774\uc720\ub97c \uc124\uba85\ud574\uc904\uc218 \uc788\ub2e4\uace0 \ubcf8\ub2e4\n\n# \uc8fc\ubaa9\ud560 \uc810\uc740 feat_4\uc640 feat_19\n# \ubcf4\ud1b5 correspond\ub294 -1~1\uc0ac\uc774\uc758 \uac12\uc744 \ub098\ud0c0\ub0b4\ub294\ub370 \uc774\ub4e4\uc740 \uac01\uae30 \ub2e4\ub978 feat\uc640\uc758 \uc0c1\uad00\uad00\uacc4\uac00 \uac70\uc758 \ub2e4 \uc774\ub97c \ub118\uc5b4\uc120\ub2e4\n# \uc774 \uc774\uc57c\uae30\ub294 \uc774 \ub450\uac00\uc9c0 feat\ub294 \ub2e4\ub978 feat\uc5d0 \uc601\ud5a5\uc744 \uc8fc\uac70\ub098 \ubc1b\uc9c0 \uc54a\uace0 \ub3c5\uc790\uc801\uc73c\ub85c \uc6c0\uc9c1\uc77c \uac00\ub2a5\uc131\uc774 \ub192\ub2e4\ub294 \uc774\uc57c\uae30\uac00 \ub41c\ub2e4\n# \uc544\ub9c8 \uc774 \ub458\uc740 \ub3c5\uc790\uc801\uc73c\ub85c class\ub97c \ucc3e\uc544\uac08 \uac00\ub2a5\uc131\uc774 \uc788\ub2e4\n# feature_name \uad6c\ud560\ub54c\ub294 \ud544\uc694\ud55c \uc815\ubcf4\uac00 \ub420 \uc218 \uc788\uc74c\uc73c\ub85c(\ub3c5\ub9bd\uc131\uc774 \uc788\uc5b4\uc11c \ud655\uc2e4\ud55c class\ub97c \ubcf4\uc7a5\ud560 \uac00\ub2a5\uc131\uc774 \ud06c\ub2e4) \uadf8\ub300\ub85c \ub2f4\ub294\ub2e4\n\ntrain_correlation = train.corr()\ntrain_correlation","b87ec85b":"train_correlation[[\"feat_4\"]].sort_values(by = \"feat_4\", ascending = False).head()#.value_counts()","501037ff":"# label_name\uc744 \uc81c\uc678\ud55c \uc804\ubd80\uc758 \uceec\ub7fc\uc744 feature_name\ud654 \ud55c\ub2e4\n\nlabel_name = \"target\"\nfeature_names = train.columns.difference([label_name])\n\nx_train = train[feature_names]\ny_train = train[label_name]\nx_test = test[feature_names]\n\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)","74cc945f":"# \uc801\uc6a9 \uc804 hyperparameter\uc5d0\uc11c \uc5b4\ub5a0\ud55c \uc0c1\uad00\uad00\uacc4\uac00 \uc788\ub294\uc9c0 \ubd84\uc11d\ud574\ubcf8\ub2e4 \n# tree\ub97c \uc5b4\ub290\uc815\ub3c4 \uccd0\uc57c\ud558\ub294\uc9c0, \uc22b\uc790\ub294 \ub300\ub7b5\uc801\uc73c\ub85c \uc5bc\ub9c8\ub098 \ub418\ub294\uc9c0 \ud655\uc778\ud558\uc5ec \ud6c4\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \ud55c\ub2e4 ","267243d4":"# holdout validation\uc744 \ud65c\uc6a9\ud558\uc5ec \ub370\uc774\ud130\ub97c \ubd84\uc0b0\ud574\uc11c \ud655\uc778\ud55c\ub2e4 \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\n\nx_train_holdout, x_test_holdout, y_train_holdout, y_test_holdout = \\\ntrain_test_split(x_train, y_train, test_size = 0.3, random_state = 42)\n\nprint(x_train_holdout.shape)\nprint(x_test_holdout.shape)\nprint(y_train_holdout.shape)\nprint(y_test_holdout.shape)","55e2190e":"# DecisionTreeClassifier\ub85c \ub300\ub7b5\uc801\uc73c\ub85c \uc870\uc0ac\ub97c \ud588\uc744\ub54c 70%\uc815\ub3c4\uc758 \uc815\ud655\uc131\uc744 \ubcf4\uc5ec\uc900\ub2e4 \n\nfrom sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(random_state = 42)\nmodel.fit(x_train_holdout, y_train_holdout)\n\ny_train_predict = model.predict(x_train_holdout)\ny_test_predict = model.predict(x_test_holdout)\n\ntrain_accuracy = (y_train_predict == y_train_holdout).mean()\ntest_accuracy = (y_test_predict == y_test_holdout).mean()\n\ntrain_accuracy, test_accuracy","1ec5ad03":"# max_depth\uc744 \uc870\uc0ac\ud588\uc744\ub54c \ub300\ubd80\ubd84\uc758 model\uacfc \ub9c8\ucc2c\uac00\uc9c0\ub85c max_depth\uac00 \uc77c\uc815\uc218\uac00 \uc99d\uac00\ud560\ub54c score\uac00 \uc88b\uc544\uc9c4\ub2e4 \n# \uc774\ub97c \ud1b5\ud574 \ud6c4\uc5d0 \uc801\uc6a9\uc2dc max_depth\uc774 \uc5b4\ub290\uc815\ub3c4\uc758 \ud06c\uae30\uac00 \ub418\uc5b4\uc788\uc5b4\uc57c \ud55c\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4\n\nmax_depth_list = range(30,51)\nhyperparameter = []\n\nfor max_depth in max_depth_list:\n   \n    model = DecisionTreeClassifier(random_state = 42, max_depth = max_depth)\n    model.fit(x_train_holdout, y_train_holdout)\n\n    y_train_predict = model.predict(x_train_holdout)\n    y_test_predict = model.predict(x_test_holdout)\n\n    train_accuracy = (y_train_predict == y_train_holdout).mean()\n    test_accuracy = (y_test_predict == y_test_holdout).mean()\n    \n    print(f\"max_depth = {max_depth}, train = {train_accuracy :.6f}, test = {test_accuracy:.6f}\") \n    # hyperparameter.append({\"max_depth\" : max_depth, \"train\" : train_accuracy, \"test\" :test_accuracy})","fe18fa37":"# model\uc744 \uc801\ud569\uc131\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud574 log_loss\ub97c \uc801\uc6a9\ud574\uc57c \ud55c\ub2e4.(\ubb38\uc81c\uc758 \uc870\uac74\uc784)\n# log_loss\uc758 \ucd5c\uc801\uc870\uac74\uc744 \ub9de\ucd9c \uc218 \uc788\ub294 model\uc744 \uc120\uc815\ud574\uc57c\ud568\uc73c\ub85c \uc774\ub97c \uad6c\ud558\uae30 \uc704\ud574 \n# LGBMClassifier \/ RandomForestClassifer \ub450\uac00\uc9c0\ub97c hold-out\uc73c\ub85c \ube44\uad50\ud558\uc5ec \uc810\uc218\uac00 \ub354 \uc88b\uc740 \ubaa8\ub378\uc744 \uc120\uc815\ud558\ub294 \uc791\uc5c5\uc744 \uc9c4\ud589\ud55c\ub2e4 ","153567d7":"from lightgbm import LGBMClassifier\nmodel_first = LGBMClassifier(boosting_type='gbdt')\nmodel_first","e97e01f0":"from sklearn.ensemble import RandomForestClassifier\nmodel_second = RandomForestClassifier()\nmodel_second","284af257":"# LGBMClassifier\ub85c log_loss \uce21\uc815\uc2dc prediction\uc740 0.51\ub85c \uce21\uc815\ub418\uace0\n# RandomForestClassifier\ub85c log_loss \uce21\uc815\uc2dc prediction\uc740 1.49~ 1.51\ub85c \uce21\uc815\uc774 \ub41c\ub2e4\n# log_loss\ub294 0 ~ 1 \uc0ac\uc774\uc758 \uac12\uc744 \uc8fc\ub85c \ub098\ud0c0\ub0b4\uace0 \uc774 \ubc94\uc704\ub97c \ubc97\uc5b4\ub098\uba74 \ubb34\ud55c\ub300\ub85c \uce21\uc815, \uc989 \uc624\ucc28\uac00 \uc2ec\ud55c\uacb0\uacfc\ub97c \uac00\uc838\uc624\ub294\ub370 \n# RandomForestClassifier\uc758 \uac12\uc774 \ub9ce\uc774 \uc5b4\uae0b\ub09c\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc900\ub2e4.\n# \uc989 \uc774 \ubb38\uc81c\uc5d0\uc11c\ub294 Gradient Boosting\uc744 \ud1b5\ud574 bias\ub97c \uc870\uc808\ud558\ub294 \uc791\uc5c5\uc744 \uc9c4\ud589\ud574\uc57c \ud568\uc744 \ubcf4\uc5ec\uc900\ub2e4 \n\n\n# LGBMClassifier\ub85c log_loss \uce21\uc815\nmodel_first.fit(x_train_holdout, y_train_holdout)\ny_test_predict = model_first.predict_proba(x_test_holdout)\n\nprediction_LGBM = log_loss(y_test_holdout, y_test_predict)\n\n\n# RandomForestClassifier\ub85c log_loss \uce21\uc815\nmodel_second.fit(x_train_holdout, y_train_holdout)\ny_test_predict = model_second.predict_proba(x_test_holdout)\n\nprediction_Random = log_loss(y_test_holdout, y_test_predict)\n\n\n\n# \ucd5c\uc885\uacb0\uacfc\nprint(\"LGBMClassifier is :\", prediction_LGBM)\nprint(\"RandomForest is :\", prediction_Random)","2eb0e438":"# Gradient Boosting\uc548\uc5d0 \uc788\ub294 hyperparameters\ub4e4\uc744 \uc870\uc808\ud558\ub294 \uc791\uc5c5\uc744 \uc9c4\ud589\ud55c\ub2e4 \n# boosting_type\uc744 dart, gbdt \ub458\uc911 \ud55c\uac1c\ub97c \uc120\ud0dd\ud574\uc57c \ud558\ub294\ub370 \n# \ubcf4\ud1b5\uc740 dart\uac00 \ub354 \uc88b\ub2e4\uace0 \ud558\ub098 \uc5ec\uae30\uc11c\ub294 gbdt\uc758 \uc810\uc218\uac00 \ub354 \ub192\ub2e4(default\uac00\uc815)\n# \ub530\ub77c\uc11c gbdt\ub85c \uc9c4\ud589\ud55c\ub2e4\n\nfrom lightgbm import LGBMClassifier\nmodel = LGBMClassifier(boosting_type = \"gbdt\")\nmodel","10443bea":"# hyperparameters \uc911 \uc911\uc694\ud55c \uba87\uac1c\ub4e4\uc744 \ubcc4\ub3c4 \uc138\ud305\uc744 \uc9c4\ud589\ud574\uc900\ub2e4 \n# n_estimators \/ learning_rate\uac00 \uc81c\uc77c \uc911\uc694\ud55c \uac83\uc774\ub2c8 \ubcc4\ub3c4 \uc138\ud305 \uc9c4\ud589\ud574\uc8fc\uace0\n# \uad6c\uc5ed\uc744 \ub098\ub204\uc5b4\uc8fc\ub294 max_bin, \uac00\uc9c0\uc218 \uacb0\uc815\ud558\ub294 num_leaves\n# random\ud558\uac8c \uc11c\uce58\ud558\ub294\ub370 \ud544\uc694\ud55c colsample_bytree, subsample, subsample_freq, min_child_samples \uc815\ub9ac\ud55c\ub2e4\n\n# \ub300\ub7b5 \uba87\uac1c\uc758 \uc22b\uc790\ub97c \ub123\uace0 \ub3cc\ub824\ubcf8\ub2e4 \ub9de\ub294\uc9c0 \ud655\uc778\uccb4\ud06c\n\nrandom_search = 10\n\nfor number in range(random_search):\n    \n    n_estimators = np.random.randint(1, 10)\n    learning_rate = 10 ** -np.random.uniform(0, 1)\n    max_bin = np.random.randint(2, 500)\n    num_leaves = np.random.randint(10, 300)\n    min_child_samples = np.random.randint(2, 300)\n    colsample_bytree = np.random.uniform(0.1, 1)\n    subsample = np.random.uniform(0.4, 1)\n    \n    model = LGBMClassifier(n_estimators = n_estimators,\n                           learning_rate = learning_rate,\n                           max_bin = max_bin,\n                           num_leaves = num_leaves,\n                           min_child_samples = min_child_samples,\n                           colsample_bytree = colsample_bytree,\n                           subsample = subsample,\n                           subsample_freq = 1,\n                           n_jobs = -1,\n                           random_state = 42)\n    \n    model.fit(x_train_holdout, y_train_holdout)\n    y_test_predict = model.predict_proba(x_test_holdout) \n    score = log_loss(y_test_holdout, y_test_predict)  \n    \n    print(number, score)","3d18a71d":"# [random_search]\n# basic = 100 \/ final = 10 \n\n# [n_estimators]\n# \uc774 \ub458\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ud30c\uc545\ud558\uae30 \uc704\ud574 n_estimators\ub97c 1000,2000 \ud574\ubcf8\ub2e4\n\n# [learning_rate]\n# learning_rate = 10 ** -np.random.uniform(1, 10)\n# final learning_rate = 10 ** -np.random.uniform(0.9, 3) \n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import log_loss\n\nrandom_search = 10\nhyperparameter_list= []\nearly_stopping_rounds = 10\n\nfor loop in range(random_search):\n    \n    # n_estimators = np.random.randint(1000, 3000)\n    n_estimators = np.random.randint(10, 30)\n    # learning_rate = 10 ** -np.random.uniform(1, 10)\n    learning_rate = 10 ** -np.random.uniform(0.9, 3)\n    max_bin = np.random.randint(2, 500)\n    num_leaves = np.random.randint(10, 300)\n    min_child_samples = np.random.randint(2, 300)\n    colsample_bytree = np.random.uniform(0.1, 1)\n    subsample = np.random.uniform(0.4, 1)\n    reg_alpha = 10 ** -np.random.uniform(1, 10)\n    reg_lambda = 10 ** -np.random.uniform(1, 15)\n    \n    model = LGBMClassifier(n_estimators = n_estimators,\n                           learning_rate = learning_rate,\n                           max_bin = max_bin,\n                           num_leaves = num_leaves,\n                           min_child_samples = min_child_samples,\n                           colsample_bytree = colsample_bytree,\n                           subsample = subsample,\n                           subsample_freq = 1,\n                           n_jobs = -1,\n                           random_state = 42)\n    \n    \n    model.fit(x_train_holdout, y_train_holdout, eval_set = [(x_test_holdout, y_test_holdout)],\n              early_stopping_rounds = early_stopping_rounds, verbose = 0)\n    y_test_predict = model.predict_proba(x_test_holdout)\n    \n          # model.best_score_[\"valid_0\"]['multi_logloss']\n    score = log_loss(y_test_holdout, y_test_predict)\n    \n    hyperparameter = {\"score\" : score, \"learning_rate\" : learning_rate,\n                     \"max_bin\" : max_bin,\n                     \"num_leaves\" : num_leaves,\n                     \"min_child_samples\" : min_child_samples,\n                     \"colsample_bytree\" : colsample_bytree,\n                     \"subsample\" : subsample,\n                     \"min_child_samples\" : min_child_samples,\n                     \"reg_alpha\" : reg_alpha,\n                     \"reg_lambda\" : reg_lambda}\n    \n    hyperparameter_list.append(hyperparameter)\n    \n    print(f\"score = {score:.6f}, n_estimators = {n_estimators},learning = {learning_rate:.6f},\\\n    max_bin = {max_bin}, num_leaves = {num_leaves}, subsample = {subsample:.6f},\\\n    colsample_bytree = {colsample_bytree}, min_child_samples = {min_child_samples}\")","2666370e":"final_list = pd.DataFrame.from_dict(hyperparameter_list)\nfinal_list = final_list.sort_values(by = \"score\", ascending = True)\nfinal_list.head(10)","3c229d08":"model.best_score_[\"valid_0\"]['multi_logloss']","7fa0a4fb":"# \uacb0\uacfc(result) \n\n# 1. <n_estimators = 1,000\uc778 \uacbd\uc6b0 \ub2e4\uc74c\uacfc \uac19\uc774  score\uac00 \ub0ae\uc740 \ubaa8\uc2b5\uc744 \ubcf4\uc778\ub2e4>\n# score = 0.448931, n_estimators = 1000,learning = 0.021010,    max_bin = 38, num_leaves = 235, subsample = 0.560212\n# score = 0.453496, n_estimators = 1000,learning = 0.022231,    max_bin = 118, num_leaves = 291, subsample = 0.963156\n# score = 0.455559, n_estimators = 1000,learning = 0.032189,    max_bin = 382, num_leaves = 70, subsample = 0.561532\n# score = 0.457469, n_estimators = 1000,learning = 0.018955,    max_bin = 94, num_leaves = 205, subsample = 0.733559\n\n# 2. <n_estimators = 2,000\uc778 \uacbd\uc6b0 \ub2e4\uc74c\uacfc \uac19\uc774  score\uac00 \ub0ae\uc740 \ubaa8\uc2b5\uc744 \ubcf4\uc778\ub2e4>\n# score = 0.452683, n_estimators = 2000,learning = 0.011920,    max_bin = 387, num_leaves = 46, subsample = 0.652579,\n# score = 0.454024, n_estimators = 2000,learning = 0.013580,    max_bin = 281, num_leaves = 270, subsample = 0.757202\n# score = 0.455261, n_estimators = 2000,learning = 0.011020,    max_bin = 492, num_leaves = 179, subsample = 0.676981,\n# score = 0.455836, n_estimators = 2000,learning = 0.011370,    max_bin = 9, num_leaves = 281, subsample = 0.716674,\n\n# n_estimators\uac00 1000\uc778 \uacbd\uc6b0 0.02XXX , n_estimators\uac00 2000\uc778 \uacbd\uc6b0 0.01XXX\uc5d0\uc11c \uac15\ud55c \uc810\uc218\ub97c \ubcf4\uc600\ub2e4 \n# \uc774\ub294 n_estimators\ubcc4\ub85c \ud2b9\uc815\uc704\uce58\uc758 learning_rate\ub97c \uac00\uc9c8\ub54c \ucd5c\ub300\uc758 \ud6a8\uacfc\ub97c \uac00\uc9c4\ub2e4\uace0 \ubd10\uc57c\ud55c\ub2e4. \n\n# 3. <n_estimators = 1,000~3,000\uc778 \uacbd\uc6b0 \ub2e4\uc74c\uacfc \uac19\uc774  score\uac00 \ub0ae\uc740 \ubaa8\uc2b5\uc744 \ubcf4\uc778\ub2e4>\n# score = 0.447990, n_estimators = 1917,learning = 0.008747,    max_bin = 257, num_leaves = 138, subsample = 0.683348,\n# score = 0.450463, n_estimators = 1850,learning = 0.013366,    max_bin = 355, num_leaves = 269, subsample = 0.638961,\n# score = 0.451037, n_estimators = 2196,learning = 0.009747,    max_bin = 17, num_leaves = 265, subsample = 0.900356,\n# score = 0.453210, n_estimators = 2800,learning = 0.005284,    max_bin = 61, num_leaves = 242, subsample = 0.491945,\n\n#  3\ubc88\uc758 score = 0.447990","4654abc8":"from lightgbm import LGBMClassifier\nmodel = LGBMClassifier(boosting_type = \"gbdt\",\n                       n_estimators = 1917,\n                       learning_rate = 0.008747,\n                       max_bin = 257,\n                       min_child_samples = 118,\n                       colsample_bytree = 0.784344330044348,\n                       num_leaves = 138, \n                       subsample = 0.683348,\n                       subsample_freq = 1,\n                       n_jobs = -1, \n                       random_state = 42)\nmodel","8fdd34b0":"# model.fit(x_train, y_train)","b48dff88":"# Outperform","cba0050a":"### model optimization process","e3193b37":"### Select  [feature_names, label_name]","3190e858":"### model_selection","8df23da1":"## coefficient\u00a0of correlation","a8d06222":"# preparation","39f40358":"# explortory"}}