{"cell_type":{"5c1bfffb":"code","9ccb4a0f":"code","7b011d3e":"code","381393fc":"code","78da366d":"code","55a82e96":"code","f1553241":"code","5455d167":"code","13d4f726":"code","04fb7f2a":"code","f5aebeee":"code","1ce7d20f":"code","63b06da6":"code","f86280d9":"code","5f313bf1":"code","9b4d4d91":"code","4276bd25":"code","6ca021fe":"code","1466bb3f":"code","85260694":"code","af3a43c1":"code","c6e08958":"code","723436fe":"code","fb192bd9":"code","61f80b4a":"code","d39f95d2":"code","2dc42a71":"code","65ca3245":"code","8f65ea65":"markdown","57fdfa92":"markdown","086c0438":"markdown","f2b7aedd":"markdown","fc2777f9":"markdown","d1d7386a":"markdown","2764ef35":"markdown","00d604df":"markdown","ffa8f685":"markdown","94451800":"markdown","c9b08bca":"markdown","0c71a642":"markdown","5dc47254":"markdown","64073c0f":"markdown","2bd5c895":"markdown","89115d86":"markdown","32e25c99":"markdown","dcf7cff5":"markdown","bdf00766":"markdown","61ef820c":"markdown","1f8844e7":"markdown","b92cbb88":"markdown","269313c7":"markdown","161f07a8":"markdown","907ae353":"markdown"},"source":{"5c1bfffb":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport nltk\nimport string\nimport re","9ccb4a0f":"def load_data():\n    data = pd.read_csv('..\/input\/sentiment-analysis-for-financial-news\/all-data.csv', sep=',', encoding='latin-1',names = [\"category\",\"comment\"])\n    return data","7b011d3e":"tweet_df = load_data()\ndf=load_data()\ntweet_df.head()\n","381393fc":"\nprint(tweet_df.shape)\nprint(\"COLUMN NAMES\" , tweet_df.columns)\n\nprint(tweet_df.info())","78da366d":"#TEXT VISUALIZATION \nsns.countplot(x=\"category\",data=tweet_df)","55a82e96":"#remove punctuations\ndef remove_punct(text):\n    text  = \"\".join([char for char in text if char not in string.punctuation])\n    text = re.sub('[0-9]+', '', text)\n    return text\n\ntweet_df['comment'] = tweet_df['comment'].apply(lambda x: remove_punct(x))\ntweet_df.head(10)","f1553241":"#stopwords removal\nimport nltk\nnltk.download('stopwords')\nstopword = nltk.corpus.stopwords.words('english')\nprint(stopword)\n\n","5455d167":"from nltk.corpus import stopwords\n\", \".join(stopwords.words('english'))\nSTOPWORDS = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\ntweet_df[\"text_wo_stop\"] = tweet_df[\"comment\"].apply(lambda text: remove_stopwords(text))\ntweet_df.head()\n\n","13d4f726":"#remove \n%matplotlib inline\npd.set_option('display.max_colwidth', 100)\n\ntweet_df.head(20)","04fb7f2a":"#stemming and lemmatization\nfrom nltk.stem.porter import PorterStemmer\nstemmer = PorterStemmer()\ndef stem_words(text):\n    return \" \".join([stemmer.stem(word) for word in text.split()])\ntweet_df[\"text_stemmed\"] = tweet_df[\"text_wo_stop\"].apply(lambda text: stem_words(text))\ntweet_df.head()\n\n","f5aebeee":"#remove frequent words - countvectorization\nfrom collections import Counter\ncnt = Counter()\nfor text in tweet_df[\"text_stemmed\"].values:\n    for word in text.split():\n        cnt[word] += 1\n        \ncnt.most_common(20)","1ce7d20f":"FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\ndef remove_freqwords(text):\n    \"\"\"custom function to remove the frequent words\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n\ntweet_df[\"text__stopfreq\"] = tweet_df[\"text_stemmed\"].apply(lambda text: remove_freqwords(text))\ntweet_df.head()","63b06da6":"\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\ndef lemmatize_words(text):\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\ntweet_df[\"text_lemmatized\"] = tweet_df[\"text__stopfreq\"].apply(lambda text: lemmatize_words(text))\ntweet_df.head()","f86280d9":"#drop the columns\ntweet_df=tweet_df.drop([\"text_stemmed\",\"text__stopfreq\"],axis=1)","5f313bf1":"#label encoding\nfrom sklearn.preprocessing import LabelEncoder\ntweet_df['encoded_category'] = LabelEncoder().fit_transform(tweet_df['category'])\ntweet_df[[\"category\", \"encoded_category\"]] ","9b4d4d91":"def clean_review(text):\n    clean_text = []\n    for w in word_tokenize(text):\n        if w.lower() not in stop:\n            pos = pos_tag([w])\n            new_w = lemmatizer.lemmatize(w, pos=get_simple_pos(pos[0][1]))\n            clean_text.append(new_w)\n    return clean_text\n\ndef join_text(text):\n    return \" \".join(text)","4276bd25":"tweet_df=tweet_df.drop([\"category\",\"text_wo_stop\",\"comment\"],axis=1)","6ca021fe":"tweet_df.head(10)","1466bb3f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier","85260694":"\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix,classification_report\n\nx_train,x_test,y_train,y_test = train_test_split(tweet_df.text_lemmatized,tweet_df.encoded_category,test_size = 0.3 , random_state = 0)\n\nx_train.shape,x_test.shape,y_train.shape,y_test.shape","af3a43c1":"pipe = Pipeline([('tfidf', TfidfVectorizer()),\n                 ('model', LinearSVC())])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"MODEL - LINEAR SVC\")\nprint(\"accuracy score: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","c6e08958":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', LogisticRegression())])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"MODEL - LOGISTIC REGRESSION\")\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","723436fe":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', MultinomialNB())])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"MULTINOMIAL NAIVE BAYES\")\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","fb192bd9":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', BernoulliNB())])\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"BERNOULLIS NAIVE BAYES\")\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","61f80b4a":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', GradientBoostingClassifier(loss = 'deviance',\n                                                   learning_rate = 0.01,\n                                                   n_estimators = 10,\n                                                   max_depth = 5,\n                                                   random_state=55))])\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"GRADIENT BOOST\")\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","d39f95d2":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', XGBClassifier(loss = 'deviance',\n                                                   learning_rate = 0.01,\n                                                   n_estimators = 10,\n                                                   max_depth = 5,\n                                                   random_state=2020))])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"XGBOOST\")\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","2dc42a71":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', DecisionTreeClassifier(criterion= 'entropy',\n                                           max_depth = 10, \n                                           splitter='best', \n                                           random_state=2020))])\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"DECISION TREE\")\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","65ca3245":"pipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', KNeighborsClassifier(n_neighbors = 10,weights = 'distance',algorithm = 'brute'))])\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"K NEAREST NEIGHBOR\")\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\nprint(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","8f65ea65":"DATA LOADING","57fdfa92":"This is my first kernel and first attempt in text analytics. Critics are expected as in to improve me further. Thanks in Advance!! :)","086c0438":"4. BERNOULLI NAIVE BAYES ","f2b7aedd":"1. LINEAR SUPPORT VECTOR MACHINE","fc2777f9":"# CLASSIFICATION MODEL BUILDING","d1d7386a":"5. GRADIENT BOOSTING CLASSIFICATION MODEL","2764ef35":"7. LABEL ENCODING OF THE CATEGORICAL VARIABLES","00d604df":"6. DROPPING THE UN-USED COLUMNS","ffa8f685":"IMPORTING THE LIBRARY FILES","94451800":"2. STOPWORDS REMOVAL","c9b08bca":"6. XGBOOST CLASSIFICATION MODEL","0c71a642":"# TEXT PRE-PROCESSING","5dc47254":"4. COUNT VECTORIZATION","64073c0f":"# SENTIMENT ANALYSIS FOR FINANCIAL NEWS","2bd5c895":"7. DECISION TREE CLASSIFICATION MODEL","89115d86":"SPLITTING OF TRAIN AND TEST DATA","32e25c99":"3. STEMMING AND LEMMATIZATION OF TEXT DATA\n","dcf7cff5":"1. REMOVING PUNCTUATIONS","bdf00766":"CONCLUSION\n\nBased on the above model comparison we can infer that Linear SVC model predicts the text classification at a better rate of accuracy than other models.\n\n","61ef820c":"PREVIEW OF THE CLEAN AND PRE-PROCESSED TEXT","1f8844e7":"5. REMOVAL OF THE MOST FREQUENT WORDS","b92cbb88":"8. K- NEAREST NEIGHBOUR CLASSIFIER MODEL","269313c7":"VISUALIZATION OF CATEGORIES OF TEXT DATA - EXPLORATORY DATA ANALYSIS","161f07a8":"2. LOGISTIC REGRESSION","907ae353":"3. MULTINOMIAL NAIVE BAYES"}}