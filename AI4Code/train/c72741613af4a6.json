{"cell_type":{"6dbe9d97":"code","7c040868":"code","35200ca1":"code","9ec56558":"code","ee701645":"code","6a980563":"code","1aaec7fc":"code","ba26d11b":"code","444b0ea8":"code","2274755a":"code","31a84fe4":"code","dd5d6856":"code","33e5c1ab":"code","276d427a":"code","25ac060e":"code","b342d925":"code","0cf28619":"code","f671b7a9":"code","5ce06138":"code","9ffdf84c":"code","07cfc9ab":"code","7437ade1":"code","ab4e5343":"code","4abe0200":"code","ef00181d":"code","823eac62":"code","4fbb8bda":"code","b4cf6a0e":"code","678cbd5d":"code","5ae76c9b":"code","42894553":"code","ceb20fbb":"code","18b45129":"code","bb5d60a2":"code","e348eb26":"code","428bdf98":"code","e05bc98c":"code","3ca79726":"code","530cb4ad":"code","b57ba189":"code","5ae18905":"code","74aea221":"code","5396f273":"code","679b88d5":"code","9e2d24fc":"code","242dd2cd":"code","923d314c":"code","a8563cbd":"code","2794cf7a":"code","338db038":"code","de06fff6":"code","a9e097ce":"code","fe7755a3":"code","ea2e1b63":"code","d611c4d1":"code","4c71a70e":"code","11255ea7":"code","0ca27135":"code","e742ddac":"code","0bc22328":"code","c40c8541":"code","05a7be08":"code","0cedc0ee":"code","e1e43f1e":"code","9122e485":"code","d2e2d772":"code","33e525d2":"code","60b41a37":"code","c6cd59ef":"code","eee02bd5":"code","21eb6797":"markdown","85d4c5a9":"markdown","7d74b882":"markdown","6e335877":"markdown","98b2ed94":"markdown","a4bedfb9":"markdown","76d10cea":"markdown","8fbe7153":"markdown","3da26d5d":"markdown","893666e3":"markdown","850689c3":"markdown","7a3a21d2":"markdown","0436d0fd":"markdown","e63f0807":"markdown","eb2a9e0e":"markdown","2efd9bf3":"markdown","437be00e":"markdown","dbbd369c":"markdown","979475da":"markdown","5b6703c1":"markdown","ba621820":"markdown","a480d074":"markdown","422d479f":"markdown","e18749da":"markdown","53e66fab":"markdown","26e083bf":"markdown","f50a6308":"markdown","fc9a4312":"markdown","e2f6cca9":"markdown"},"source":{"6dbe9d97":"# Importing all the libraries\nimport pandas as pd\nimport numpy as np\nimport warnings\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option(\"display.max_columns\", 120)\npd.set_option(\"display.max_rows\", 120)\n","7c040868":"# Reading the data using pandas dataframe\nfeatures = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/features.csv.zip')\ntrain = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/train.csv.zip')\nstores = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv')\ntest = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/test.csv.zip')\nsample_submission = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/sampleSubmission.csv.zip')","35200ca1":"print(features.head())\nprint(\"------------------------------------------------------------\\n\")\nprint(stores.head())\nprint(\"------------------------------------------------------------\\n\")\nprint(train.head())\nprint(\"------------------------------------------------------------\\n\")\nprint(test.head())\nprint(\"------------------------------------------------------------\\n\")\nprint(sample_submission.head())","9ec56558":"# Finding the number of rowns and columns in dataframe\nfeatures.shape, train.shape, stores.shape, test.shape","ee701645":"# Some basic information of differnt column's data type of dataframe\nprint(features.dtypes)\nprint(\"------------------------------------------------------------\\n\")\nprint(train.dtypes)\nprint(\"------------------------------------------------------------\\n\")\nprint(stores.dtypes)\nprint(\"------------------------------------------------------------\\n\")\nprint(test.dtypes)","6a980563":"feature_store = features.merge(stores, how='inner', on = \"Store\")","1aaec7fc":"train = train.merge(feature_store, how='inner', on=['Store','Date','IsHoliday'])","ba26d11b":"test = test.merge(feature_store, how='inner', on=['Store','Date','IsHoliday'])","444b0ea8":"# Another useful step is to facilate the acces to the 'Date' attribute by splitting it into its componenents (i.e. Year, Month and week,day).\ntrain = train.copy()\ntest = test.copy()\n\ntrain['Date'] = pd.to_datetime(train['Date'])\ntrain['Year'] = pd.to_datetime(train['Date']).dt.year\ntrain['Month'] = pd.to_datetime(train['Date']).dt.month\ntrain['Week'] = pd.to_datetime(train['Date']).dt.week\ntrain['Day'] = pd.to_datetime(train['Date']).dt.day\ntrain.replace({'A': 1, 'B': 2,'C':3},inplace=True)\n\ntest['Date'] = pd.to_datetime(test['Date'])\ntest['Year'] = pd.to_datetime(test['Date']).dt.year\ntest['Month'] = pd.to_datetime(test['Date']).dt.month\ntest['Week'] = pd.to_datetime(test['Date']).dt.week\ntest['Day'] = pd.to_datetime(test['Date']).dt.day\ntest.replace({'A': 1, 'B': 2,'C':3},inplace=True)\n","2274755a":"print(train.head())\nprint(\"------------------------------------------------------------\\n\")\nprint(test.head())","31a84fe4":"weekly_sales = train.groupby(['Year','Week']).agg({'Weekly_Sales': ['mean', 'median']})\nweekly_sales2010 = train.loc[train['Year']==2010].groupby(['Week']).agg({'Weekly_Sales': ['mean', 'median']})\nweekly_sales2011 = train.loc[train['Year']==2011].groupby(['Week']).agg({'Weekly_Sales': ['mean', 'median']})\nweekly_sales2012 = train.loc[train['Year']==2012].groupby(['Week']).agg({'Weekly_Sales': ['mean', 'median']})\nplt.figure(figsize=(20, 7))\nsns.lineplot(weekly_sales2010['Weekly_Sales']['mean'].index, weekly_sales2010['Weekly_Sales']['mean'].values)\nsns.lineplot(weekly_sales2011['Weekly_Sales']['mean'].index, weekly_sales2011['Weekly_Sales']['mean'].values)\nsns.lineplot(weekly_sales2012['Weekly_Sales']['mean'].index, weekly_sales2012['Weekly_Sales']['mean'].values)\n\nplt.grid()\nplt.xticks(np.arange(1, 53, step=1))\nplt.legend(['2010', '2011', '2012'])\nplt.show()","dd5d6856":"Y_train = train['Weekly_Sales']","33e5c1ab":"targets = Y_train.copy()","276d427a":"train= train.drop(['Weekly_Sales'],axis=1)\n","25ac060e":"# Let's also identify the numeric and categorical columns.\nnumeric_cols = train.select_dtypes(include=np.number).columns.tolist()\ncategorical_cols = train.select_dtypes('object').columns.tolist()","b342d925":"print(numeric_cols)\nprint(\"------------------------------------------------------------\\n\")\nprint(categorical_cols)","0cf28619":"# Check if there is any null value in train dataframe\ntrain.isnull().sum()","f671b7a9":"# Check if there is any null value test in dataframe\ntest.isnull().sum()","5ce06138":"# Create the imputer\nimputer = SimpleImputer(missing_values= np.NaN, strategy='mean')","9ffdf84c":"# Fit the imputer to the numeric columns\nimputer.fit(train[numeric_cols])","07cfc9ab":"#Replace all the null values\ntrain[numeric_cols] =imputer.transform(train[numeric_cols])","7437ade1":"# Check if there is any null value\ntrain.isnull().sum()","ab4e5343":"# importing MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler","4abe0200":"# Create the scaler\nscaler = MinMaxScaler()","ef00181d":"# Fit the scaler to the numeric columns\nscaler.fit(train[numeric_cols])","823eac62":"# Transform and replace the numeric columns\ntrain[numeric_cols] = scaler.transform(train[numeric_cols])","4fbb8bda":"train[numeric_cols].describe().loc[['min', 'max']]","b4cf6a0e":"# 'Date' is irrelevant and Drop it from data.\ntrain= train.drop(['Date'],axis=1)\ntest = test.drop(['Date'], axis=1)","678cbd5d":"# Preparing the dataset:\nX_train =train[['Store','Dept','IsHoliday','Size','Week','Type','Year']]\nX_test = test[['Store', 'Dept','IsHoliday', 'Size', 'Week', 'Type', 'Year']]","5ae76c9b":"print(X_train.columns)\nprint(X_test.columns)","42894553":"# Splitting and training\ntrain_inputs, val_inputs, train_targets, val_targets = train_test_split(X_train, Y_train, test_size=0.25, random_state=42)","ceb20fbb":"# importing XGBRegressor\nfrom xgboost import XGBRegressor","18b45129":"# fitting the model\nmodel = XGBRegressor(random_state=42, n_jobs=-1, n_estimators=20, max_depth=4)","bb5d60a2":"model.fit(train_inputs,train_targets)","e348eb26":"#Let's turn this into a dataframe and visualize the most important features.\nimportance_df = pd.DataFrame({\n    'feature': X_test.columns,\n    'importance':model.feature_importances_\n}).sort_values('importance', ascending=False)","428bdf98":"import seaborn as sns\nplt.figure(figsize=(10,6))\nplt.title('Feature Importance')\nsns.barplot(data=importance_df.head(10), x='importance', y='feature');","e05bc98c":"# Make and evaluate predictions:\nx_pred = model.predict(train_inputs)\nx_pred","3ca79726":"# calculating mean_squared_error\ndef rmse(a, b):\n    return mean_squared_error(a, b, squared=False)","530cb4ad":"rmse(x_pred,train_targets)","b57ba189":"x_preds=model.predict(X_test)\nx_preds","5ae18905":"Final = X_test[['Store', 'Dept', 'Week']]\ntest['Weekly_Sales']= x_preds","74aea221":"sample_submission['Weekly_Sales'] = test['Weekly_Sales']\nsample_submission.to_csv('submission_2.csv',index=False)","5396f273":"preds1=pd.read_csv('submission_2.csv')\npreds1","679b88d5":"#ploting prediction\nplt.figure(figsize=(10,6))\nsns.barplot(data=preds1.head(10), x='Id', y='Weekly_Sales');","9e2d24fc":"def test_params(**params):\n    model = RandomForestRegressor(random_state=42, n_jobs=-1, **params).fit(train_inputs, train_targets)\n    train_rmse = mean_squared_error(model.predict(train_inputs), train_targets, squared=False)\n    val_rmse = mean_squared_error(model.predict(val_inputs), val_targets, squared=False)\n    return train_rmse, val_rmse","242dd2cd":"test_params(n_estimators=20, max_depth=20)","923d314c":"test_params(n_estimators=50, max_depth=10,min_samples_split=3, min_samples_leaf=4, max_features=0.4)","a8563cbd":"def test_param_and_plot(param_name, param_values):\n    train_errors, val_errors = [], [] \n    for value in param_values:\n        params = {param_name: value}\n        train_rmse, val_rmse = test_params(**params)\n        train_errors.append(train_rmse)\n        val_errors.append(val_rmse)\n    plt.figure(figsize=(10,6))\n    plt.title('Overfitting curve: ' + param_name)\n    plt.plot(param_values, train_errors, 'b-o')\n    plt.plot(param_values, val_errors, 'r-o')\n    plt.xlabel(param_name)\n    plt.ylabel('RMSE')\n    plt.legend(['Training', 'Validation'])","2794cf7a":"test_param_and_plot('max_depth', [5, 10, 15, 20, 25, 30, 35])","338db038":"test_param_and_plot('n_estimators', [5, 10, 15, 20, 25, 30, 35])","de06fff6":"# fitting the model with Hyperparameter Overfitting \nRF = RandomForestRegressor(n_estimators=58, max_depth=27, max_features=6, min_samples_split=3, min_samples_leaf=1)\nRF.fit(train_inputs,train_targets)","a9e097ce":"RF.score(train_inputs, train_targets)","fe7755a3":"RF.score(val_inputs, val_targets)","ea2e1b63":"# Make and evaluate predictions:\ntrain_preds = RF.predict(train_inputs)\ntrain_preds","d611c4d1":"rmse(train_targets,train_preds)","4c71a70e":"# Let's turn this into a dataframe and visualize the most important features.\nimportance_df = pd.DataFrame({\n    'feature': X_test.columns,\n    'importance': RF.feature_importances_\n}).sort_values('importance', ascending=False)","11255ea7":"import seaborn as sns\nplt.figure(figsize=(10,6))\nplt.title('Feature Importance')\nsns.barplot(data=importance_df.head(10), x='importance', y='feature');","0ca27135":"predict = RF.predict(X_test)\npredict","e742ddac":"Final = X_test[['Store', 'Dept', 'Week']]\ntest['Weekly_Sales']= predict","0bc22328":"sample_submission['Weekly_Sales'] = test['Weekly_Sales']\nsample_submission.to_csv('submission.csv',index=False)\npredicts=pd.read_csv('submission.csv')\npredicts","c40c8541":"#ploting prediction\nplt.figure(figsize=(10,6))\nsns.barplot(data=predicts.head(10), x='Id', y='Weekly_Sales');","05a7be08":"# importing the LinearRegression algorithm\nfrom sklearn.linear_model import LinearRegression","0cedc0ee":"# fitting the model\nlr=LinearRegression()\nlr.fit(train_inputs,train_targets)","e1e43f1e":"Y_pred=lr.predict(train_inputs)\nY_pred","9122e485":"rmse(train_targets,Y_pred)","d2e2d772":"y_pred=lr.predict(X_test)\ny_pred","33e525d2":"Final = X_test[['Store', 'Dept', 'Week']]\ntest['Weekly_Sales']= y_pred","60b41a37":"sample_submission['Weekly_Sales'] = test['Weekly_Sales']\nsample_submission.to_csv('submission_1.csv',index=False)","c6cd59ef":"preds=pd.read_csv('submission_1.csv')","eee02bd5":"#ploting prediction\nimport seaborn as sns\nplt.figure(figsize=(10,6))\nsns.barplot(data=preds.head(10),x='Id', y='Weekly_Sales');","21eb6797":"## Feature Importance\nBased on the gini index computations, a decision tree assigns an \"importance\" value to each feature. These values can be used to interpret the results given by a decision tree.\n","85d4c5a9":"# Training and Validation Set","7d74b882":"##  Descriptive statistics & data visualizations:\n### Weekly_Sales\nThe plot makes the right skewness clear, so most weeks have sales around the median.\nAlso, we can see that the Weekly_Sales attribute has a large kurtosis which indicates the presence of extreme values, in other words, some weeks have high sales. It would be a good idea to know the origins of these extreme values.","6e335877":"### Evaluation","98b2ed94":"## Making Predictions","a4bedfb9":"## RandomForestRegressor","76d10cea":"### 2.2 Data Cleaning\nLet's start by cleaning the data of both datasets. We will see if they have missing values, duplicates and see if eliminate them if thats the case.\n\nVery important to take into account that both datasets are going to merge. Therefore, they must have one key column that has the same values. Hence, We will also see if the values are consistent in both datasets.","8fbe7153":"## Making Predictions","3da26d5d":"### Evaluation","893666e3":"### To convert df to csv file","850689c3":"# Prepare the Dataset for Training","7a3a21d2":"## 2.1  Explore the Data","0436d0fd":"# walmart-recruiting-store-sales-forecasting predictions","e63f0807":"We can see that the test dataset don't contain the features included in the train dataset, taking into consideration that these features (Temperature, Fuel price, MarkDowns, CPI and Unemployment) cannot be used in the test dataset due to their high dependences on the date, so it will be a good idea to delete them. but before that, we will make sure that these features don't provide any information on the target 'Weekly_Sales'.","eb2a9e0e":"## XGBRegressor","2efd9bf3":"##### We can compute the accuracy of the model on the training and validation sets using RF.score","437be00e":"# Make Predictions and Evaluate Your Model","dbbd369c":"# Evaluate Algorithms\nAfter analysing, cleaning and preparing the data, the next step is to select the best algorithm with the optimal parameters to obtain the best results.\nThis step requiers manually selecting the type of data normalization, manually selecting algorithms and tune all hyperparameters. \n\nMany algorithms assume normal distribution of the data, especially when features have different ranges like our case, so it is necessary to implement this step in our pipeline.\n\n#### For data normalization, Lale will have the following choices :\n\n1. MinMaxscaler\n#### Algorithms used for spot-checking :\n\n1. LinearRegression\n2. RandomForestRegressor\n3. GradientBoostingRegressor","979475da":"#### To plot the graph between training error and validation error.","5b6703c1":"## LinearRegression","ba621820":"## 2. Data Loading, Preparation & Cleaning","a480d074":"# Training the Best Model","422d479f":"# Impute Numerical Data","e18749da":"## 1. Introduction\nFollowing provided with historical sales data for 45 Walmart stores located in different regions. Each store contains a number of departments, and you are tasked with predicting the department-wide sales for each store.\n\nIn addition, Walmart runs several promotional markdown events throughout the year. These markdowns precede prominent holidays, the four largest of which are the Super Bowl, Labor Day, Thanksgiving, and Christmas. The weeks including these holidays are weighted five times higher in the evaluation than non-holiday weeks. Part of the challenge presented by this competition is modeling the effects of markdowns on these holiday weeks in the absence of complete\/ideal historical data.\n\n### Store Sales Forecasting & Discount Strategy:\nGoal:\n\n1. Exploratory Data Analysis to describe and clean the data, and to understand attributes\n2. Feature selection to keep only important attributes\n3. Developing a framework to evaluate and spot-check algorithms\n4. Predicting and explaining future sales\n5. Identifying the right time for discount strategies","53e66fab":"## Hyperparameter Tuning\nFor hyperparameter tuning, Lale give us the choice to use its search space or schemas as is, or we can customize the schemas to fit our purposes","26e083bf":"## Feature Importance\nBased on the gini index computations, a decision tree assigns an \"importance\" value to each feature. These values can be used to interpret the results given by a decision tree.","f50a6308":"## Making Predictions","fc9a4312":"### Evaluation","e2f6cca9":"# Making Predictions"}}