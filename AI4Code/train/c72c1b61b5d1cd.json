{"cell_type":{"78d673a2":"code","3384790a":"code","adce25a9":"code","fbcfc37a":"code","bb896851":"code","04ea5d70":"code","6436373e":"code","4cf2ab1f":"code","ec5781a4":"code","137b5ea5":"code","ee0bccbe":"code","101d8c15":"code","e271cc72":"code","46600051":"code","ad90a080":"code","5abfaeb1":"code","882cf471":"code","0cda6601":"code","0da57a72":"code","362d3578":"code","d0a1526e":"code","3ee77ef1":"code","0ba55328":"code","6450724b":"code","08b25e30":"code","f3cc049f":"code","363d7839":"code","ee692e2d":"code","d7418e6c":"code","c7d50b4c":"code","6fd6f9e7":"code","0534e7e9":"code","574f2609":"code","9cfaf8a8":"code","0e285be6":"code","d05b84db":"code","9d222e55":"code","98dcb60b":"code","206ea506":"code","1dd07c6d":"code","1437f541":"code","f5bc1f4b":"code","b03a4a78":"code","13c2e629":"code","fe054826":"code","d9c8fc00":"code","7d48d5d2":"code","e544f909":"code","cd29619d":"code","9a60ebfe":"code","e69261af":"code","570b3d31":"code","23d070ad":"code","6e83726b":"code","4bfecf88":"code","ebfe2ebd":"code","98cbe318":"code","0cd413a2":"code","c67f1a6d":"code","ad5e1507":"code","590ddd16":"code","f17813d9":"code","b39ce714":"code","72b19b03":"code","530f5894":"code","5ae4cb14":"code","0fdac894":"code","3091d78e":"code","7ba5854c":"code","bbe73b23":"code","4014a98b":"code","fe982b43":"code","266ab3f7":"code","c1614fd4":"code","81dbad8f":"code","83d10f9c":"code","70848bdc":"code","541e5f3e":"code","dbef53c7":"code","bf4a17b8":"code","0374ea8e":"markdown","c5081a14":"markdown","8d1e8b0a":"markdown","eead35e6":"markdown","48099e82":"markdown","cb21c916":"markdown","fc7419fd":"markdown","b876c75d":"markdown","58921742":"markdown","a00dab07":"markdown","9e5c8040":"markdown","53aa503c":"markdown","0ff0b69d":"markdown","3a48490b":"markdown","e74b5662":"markdown","e2b72bf9":"markdown","3d5e8871":"markdown"},"source":{"78d673a2":"def text_cleaning(df):\n    # text cleaning\n    from nltk.tokenize import word_tokenize\n    from nltk.stem import PorterStemmer as ps\n    from nltk.corpus import stopwords\n    import numpy as np\n    import string\n\n    i = 0\n    df['clean_text'] = ''\n    for row in df.text:\n        # add spaces to prevent word merging\n        row = row.replace('.', '. ', row.count('.')).replace(',', ', ', row.count(','))\n        # split into words\n        tokens = word_tokenize(row)\n\n        # convert to lower case\n        tokens = [token.lower() for token in tokens]\n\n        # remove punctuation\n        table = str.maketrans('', '', string.punctuation)\n        words = [token.translate(table) for token in tokens]\n\n        # remove non-alphabetic or numeric tokens\n        words = [word for word in words if word.isalnum()]\n\n        # filter stop words\n        stop_words = set(stopwords.words('english'))\n        words = [word for word in words if not word in stop_words]\n        \n        #print(words)\n        df['clean_text'][i] = ' '.join(words)\n        i += 1\n    df.clean_text = df.clean_text\n    return df","3384790a":"def tfidf_count_vectorize(df, tfidf_vect = None):\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn import preprocessing\n\n    # word-level tf-idf\n    if tfidf_vect == None:\n        tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n        tfidf_vect.fit(df.clean_text)\n    \n#     # ngram-level tf-idf \n#     tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2, 3), max_features=5000)\n#     tfidf_vect_ngram.fit(df.clean_text) # measures bi-grams and tri-grams\n    \n    tfidf_matrix = tfidf_vect.transform(df.clean_text)\n    terms = tfidf_vect.get_feature_names()\n    tfidf_array = tfidf_matrix.toarray()\n    \n    encoder = preprocessing.LabelEncoder()\n    label = encoder.fit_transform(df.label)\n    \n    return tfidf_matrix, tfidf_array, tfidf_vect, label","adce25a9":"def preprocess_and_tfidf_gen(path, flag = 0, tfidf_vect = None):\n    # create dataframes\n    import pandas as pd\n\n    dataset = open(path).read()\n\n    d_labels, d_texts = [], []\n    for i, line in enumerate(dataset.split('\\n')):\n        content = line.split('\\t')\n        if len(content) > 1:\n            d_texts.append(content[0])\n            d_labels.append(content[1])\n    \n    df = pd.DataFrame()\n    df['label'] = d_labels\n    df['text'] = d_texts\n    \n    print(df)\n    df = text_cleaning(df)\n    if flag == 0:\n        (tfidf_matrix, tfidf_array, tfidf_vect, label) = tfidf_count_vectorize(df)\n        return df, tfidf_matrix, tfidf_array, tfidf_vect, label\n    else:\n        (tfidf_matrix, tfidf_array, tfidf_vect, label) = tfidf_count_vectorize(df, tfidf_vect)\n        return df, tfidf_matrix, tfidf_array, label","fbcfc37a":"def select_k_best(tfidf_train_matrix, tfidf_test_matrix, train_data):\n    from sklearn.feature_selection import chi2,SelectKBest,mutual_info_regression\n    \n    vectorizer_chi2 = SelectKBest(chi2,k=500)\n    chi_train_tfidf = vectorizer_chi2.fit_transform(tfidf_train_matrix,train_label)\n    chi_test_tfidf = vectorizer_chi2.transform(tfidf_test_matrix)\n    \n    vectorizer_ig = SelectKBest(mutual_info_regression,k=500)\n    ig_train_tfidf = vectorizer_ig.fit_transform(tfidf_train_matrix,train_label)\n    ig_test_tfidf = vectorizer_ig.transform(tfidf_test_matrix)\n    \n    return chi_train_tfidf, chi_test_tfidf, ig_train_tfidf, ig_test_tfidf","bb896851":"def preprocess_and_tfidf_gen2(path, flag = 0, tfidf_vect = None):\n    # create dataframes\n    import pandas as pd\n\n    dataset = open(path).read()\n\n    d_labels, d_texts = [], []\n    for i, line in enumerate(dataset.split('\\n')):\n        d_texts.append(line)\n        d_labels.append(0)\n    \n    for i in range(len(d_texts)):\n        if i < len(d_texts)\/2:\n            d_labels[i] = 1\n        else:\n            d_labels[i] = 0\n    \n    df = pd.DataFrame()\n    df['label'] = d_labels\n    df['text'] = d_texts\n    df = df[:-1]\n    \n    df = text_cleaning(df)\n    if flag == 0:\n        (tfidf_matrix, tfidf_array, tfidf_vect, label) = tfidf_count_vectorize(df)\n        return df, tfidf_matrix, tfidf_array, tfidf_vect, label\n    else:\n        (tfidf_matrix, tfidf_array, tfidf_vect, label) = tfidf_count_vectorize(df, tfidf_vect)\n        return df, tfidf_matrix, tfidf_array, label","04ea5d70":"def cosine_sim_function(a, b):\n    cos_sim = np.dot(a, b)\/(np.linalg.norm(a)*np.linalg.norm(b))\n    return cos_sim","6436373e":"def cosine_sim_library(tfidf_matrix):\n    from sklearn.metrics.pairwise import cosine_similarity\n    dist = 1 - cosine_similarity(tfidf_matrix)\n    print(dist)\n    return dist","4cf2ab1f":"def train_data_unsupervised(tfidf_array):\n    import pandas as pd\n    df1 = pd.DataFrame(tfidf_train_array).T\n    df1['Doc_Freq'] = df1[df1[:] > 0.0].count(axis = 1)\n    # df1['Doc_Freq'].unique()\n\n    df1 = df1.T\n    print(min(df1.loc['Doc_Freq']), max(df1.loc['Doc_Freq']))\n    sub_df = df1.loc[:, (df1.loc['Doc_Freq'] >= 4) & (df1.loc['Doc_Freq'] < 80)]\n    sub_df['Array'] = sub_df[sub_df[:] > 0.0].count(axis = 1)\n    \n    tfidf_df = sub_df\n    \n    tfidf_df_outlierremoved = tfidf_df[tfidf_df['Array'] >= 3]\n    df1 = df1.T[tfidf_df_outlierremoved.index].T\n    \n    return tfidf_df_outlierremoved, df1","ec5781a4":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntrain_file_path = input(\"Train file path: \")\ntest_file_path = input(\"Test file path: \")\n\n(train_data, tfidf_train_matrix, tfidf_train_array, tfidf_vect, train_label) = preprocess_and_tfidf_gen(train_file_path, 0)\n(test_data,tfidf_test_matrix, tfidf_test_array, test_label) = preprocess_and_tfidf_gen(test_file_path, 1, tfidf_vect)","137b5ea5":"pd.DataFrame(tfidf_train_array)","ee0bccbe":"max(train_data.text)","101d8c15":"(tfidf_train_outlierremoved, new_train_tfidf) = train_data_unsupervised(tfidf_train_array)\ntfidf_train_outlierremoved = tfidf_train_outlierremoved.iloc[:-1, :-1]\nnew_train_tfidf = new_train_tfidf.iloc[:-1, :-1]\nprint(tfidf_train_outlierremoved, new_train_tfidf)","e271cc72":"tfidf_test_array = tfidf_test_matrix.T[tfidf_train_outlierremoved.T.index].T.toarray()","46600051":"test_file_path2 = \"..\/input\/sentiment-labelled-sentences-data-set\/sentiment labelled sentences\/imdb_labelled.txt\"\n(test_data2,tfidf_test_matrix2, tfidf_test_array2, test_label2) = preprocess_and_tfidf_gen(test_file_path2, 1, tfidf_vect)","ad90a080":"(chi_train_tfidf, chi_test_tfidf, ig_train_tfidf, ig_test_tfidf) = select_k_best(tfidf_train_matrix, tfidf_test_matrix, train_data)","5abfaeb1":"# print(chi_train_tfidf)\n# print(ig_train_tfidf)","882cf471":"agregated_data = pd.concat([train_data, test_data])\n# agregated_data = pd.concat([agregated_data, test_data2])\n# agregated_data","0cda6601":"from nltk.tokenize import word_tokenize\ndata = agregated_data.clean_text.map(word_tokenize).values\ntotal_vocabulary = set(word for line in data for word in line)\nprint('Unique tokens in texts: {}'.format(len(total_vocabulary)))","0da57a72":"# word embedding\nfrom gensim.models import Word2Vec\n\nw2v_model = Word2Vec(data, window=5, min_count=1, workers=4)\nw2v_model.train(data, total_examples=w2v_model.corpus_count, epochs=10)\nword_vectors = w2v_model.wv","362d3578":"'kitchen' in word_vectors.key_to_index","d0a1526e":"print('Words Most Similar to amazon:')\ndisplay(word_vectors.most_similar('amazon'))","3ee77ef1":"print('Words Most Similar to imdb:')\ndisplay(word_vectors.most_similar('imdb'))","0ba55328":"print('Cosine Similarity between amazon and imdb:')\ndisplay(word_vectors.similarity('amazon', 'imdb'))","6450724b":"data_df.sentiment = data_df.sentiment.astype(int)\ntrain_df.sentiment = train_df.sentiment.astype(int)\ntest_df.sentiment = test_df.sentiment.astype(int)","08b25e30":"data_df[data_df.sentiment == 0].count()","f3cc049f":"positive = data_df[data_df.sentiment==1].count()\nnegative = data_df[data_df.sentiment==0].count()\np_per = (positive \/ (positive + negative)) * 100\nn_per = (negative \/ (positive + negative)) * 100","363d7839":"# # #Creating PieCart\n# # labels = ['Positive ['+ p_per +'%]','Negative ['+ n_per +'%]']\n# # sizes = [p_per, n_per]\n# # colors = ['yellowgreen','red']\n# # patches, texts = plt.pie(sizes,colors=colors, startangle=90)\n# # plt.style.use('default')\n# # plt.legend(labels)\n# # plt.title(\"Sentiment Analysis Result for keyword= \"+keyword+\"\" )\n# # plt.axis(\"equal\")\n# # plt.show()\n\n# data_df.plot.pie(y = 'sentiment');","ee692e2d":"def cosine_similarity(k, query):\n    print(\"Cosine Similarity\")\n    tokens = query\n    \n#     print(\"\\nQuery:\", query)\n    print(\"\")\n    print(tokens)\n    \n    d_cosines = []\n    \n    query_vector = gen_vector(tokens)\n    print(query_vector.all() == 0)\n    \n    for d in D:\n        d_cosines.append(cosine_sim(query_vector, d))\n        \n    out = np.array(d_cosines).argsort()[-k:][::-1]\n        \n    print(out)\n\n    for i in out:\n        print(i, df[i][0])\n\nfor i in range(1):\n    Q = cosine_similarity(2, df.clean_text[i])","d7418e6c":"df = train_data","c7d50b4c":"import nltk\nimport numpy as np\nimport random\nimport string\n\nwordfreq = {}\nfor sentence in df.clean_text:\n    tokens = nltk.word_tokenize(sentence)\n    for token in tokens:\n        if token not in wordfreq.keys():\n            wordfreq[token] = 1\n        else:\n            wordfreq[token] += 1\n\nimport heapq\nmost_freq = heapq.nlargest(len(wordfreq), wordfreq, key=wordfreq.get)","6fd6f9e7":"wordfreq_array = np.array(list(wordfreq.items()))","0534e7e9":"pd.DataFrame(list(wordfreq.items())).sort_values(by = [1], ascending = False)","574f2609":"min_max = wordfreq[most_freq[0]], wordfreq[most_freq[-1]]\nmin_max","9cfaf8a8":"word_idf_values = {}\nfor token in most_freq:\n    doc_containing_word = 0\n    for document in df.clean_text:\n        if token in nltk.word_tokenize(document):\n            doc_containing_word += 1\n    word_idf_values[token] = np.log(len(df.clean_text)\/(1 + doc_containing_word))","0e285be6":"word_idf_values.items[0:15]","d05b84db":"word_tf_values = {}\nfor token in most_freq:\n    sent_tf_vector = []\n    for document in df.clean_text:\n        doc_freq = 0\n        for word in nltk.word_tokenize(document):\n            if token == word:\n                  doc_freq += 1\n        word_tf = doc_freq\/len(nltk.word_tokenize(document))\n        sent_tf_vector.append(word_tf)\n    word_tf_values[token] = sent_tf_vector","9d222e55":"pd.DataFrame(word_tf_values)","98dcb60b":"tfidf_values = []\nfor token in word_tf_values.keys():\n    tfidf_sentences = []\n    for tf_sentence in word_tf_values[token]:\n        tf_idf_score = tf_sentence * word_idf_values[token]\n        tfidf_sentences.append(tf_idf_score)\n    tfidf_values.append(tfidf_sentences)","206ea506":"tf_idf_model = np.asarray(tfidf_values)\ntf_idf_model = np.transpose(tf_idf_model)\npd.DataFrame(tf_idf_model)","1dd07c6d":"tfidf_train_array.shape","1437f541":"type(tfidf_train_outlierremoved.index)","f5bc1f4b":"train_data.label = [int(i) for i in train_data.label]","b03a4a78":"from sklearn.cluster import KMeans\nfrom sklearn import metrics\n\nnum_clusters = 2\nkm = KMeans(n_clusters=num_clusters, max_iter=1000, n_init=50)\nkm.fit(tfidf_train_outlierremoved)\nclusters = km.labels_.tolist()\n\noriginal_labels = train_data.label[tfidf_train_outlierremoved.index]\n\naccuracy_score = metrics.accuracy_score(clusters, original_labels)\nprint(accuracy_score)\n\n# label2 = original_labels.copy()\n# for i in range(len(original_labels)):\n#     if original_labels[i] == 1:\n#         label2[i] = 0\n#     elif original_labels[i] == 0:\n#         label2[i] = 1\n\naccuracy_score1 = 1- accuracy_score\nprint(accuracy_score1)","13c2e629":"from sklearn.cluster import KMeans\nfrom sklearn import metrics\n\nnum_clusters = 2\nkm = KMeans(n_clusters=num_clusters, max_iter=1000, n_init=50)\nkm.fit(new_train_tfidf)\nclusters = km.labels_.tolist()\n\noriginal_labels = train_data.label[new_train_tfidf.index]\n\naccuracy_score = metrics.accuracy_score(clusters, original_labels)\nprint(accuracy_score)\n\n# label2 = original_labels.copy()\n# for i in range(len(original_labels)):\n#     if original_labels[i] == 1:\n#         label2[i] = 0\n#     elif original_labels[i] == 0:\n#         label2[i] = 1\n\naccuracy_score1 = 1- accuracy_score\nprint(accuracy_score1)","fe054826":"km.fit(tfidf_test_array)\nclusters = km.labels_.tolist()\n\noriginal_labels = test_data.label\n\naccuracy_score = metrics.accuracy_score(clusters, list(map(int, original_labels)))\nprint(accuracy_score)\n\nlabel2 = list(map(int, original_labels)).copy()\nfor i in range(len(original_labels)):\n    if original_labels.iloc[i] == '1':\n        label2[i] = 0\n    elif original_labels.iloc[i] == '0':\n        label2[i] = 1\n\naccuracy_score = metrics.accuracy_score(clusters, label2)\nprint(accuracy_score)","d9c8fc00":"from sklearn.cluster import KMeans\nfrom sklearn import metrics\n\nnum_clusters = 2\nkm = KMeans(n_clusters=num_clusters, max_iter=1000, n_init=50)\nkm.fit(ig_train_tfidf)\nclusters = km.labels_.tolist()\n\noriginal_labels = train_label\n\naccuracy_score = metrics.accuracy_score(clusters, original_labels)\nprint(accuracy_score)\n\nlabel2 = original_labels.copy()\nfor i in range(len(original_labels)):\n    if original_labels[i] == 1:\n        label2[i] = 0\n    elif original_labels[i] == 0:\n        label2[i] = 1\n\naccuracy_score1 = metrics.accuracy_score(clusters, label2)\nprint(accuracy_score1)\n\n# if(accuracy_score1 > accuracy_score):\n#     print(accuracy_score1)\n# else:\n#     print(accuracy_score)","7d48d5d2":"clusters = km.predict(ig_test_tfidf)\n\noriginal_labels = test_data.label\n\naccuracy_score = metrics.accuracy_score(clusters, list(map(int, original_labels)))\nprint(accuracy_score)\n\nlabel2 = list(map(int, original_labels)).copy()\nfor i in range(len(original_labels)):\n    if original_labels.iloc[i] == 1:\n        label2[i] = 0\n    elif original_labels.iloc[i] == 0:\n        label2[i] = 1\n\naccuracy_score = metrics.accuracy_score(clusters, label2)\nprint(accuracy_score)","e544f909":"from sklearn.cluster import KMeans\nfrom sklearn import metrics\n\nnum_clusters = 2\nkm = KMeans(n_clusters=num_clusters, max_iter=1000, n_init=50)\nkm.fit(chi_train_tfidf)\nclusters = km.labels_.tolist()\n\noriginal_labels = train_label\n\naccuracy_score = metrics.accuracy_score(clusters, original_labels)\nprint(accuracy_score)\n\nlabel2 = original_labels.copy()\nfor i in range(len(original_labels)):\n    if original_labels[i] == 1:\n        label2[i] = 0\n    elif original_labels[i] == 0:\n        label2[i] = 1\n\naccuracy_score1 = metrics.accuracy_score(clusters, label2)\nprint(accuracy_score1)\n\n# if(accuracy_score1 > accuracy_score):\n#     print(accuracy_score1)\n# else:\n#     print(accuracy_score)","cd29619d":"clusters = km.predict(chi_test_tfidf)\n\noriginal_labels = test_data.label\n\naccuracy_score = metrics.accuracy_score(clusters, list(map(int, original_labels)))\nprint(accuracy_score)\n\nlabel2 = list(map(int, original_labels)).copy()\nfor i in range(len(original_labels)):\n    if original_labels.iloc[i] == 1:\n        label2[i] = 0\n    elif original_labels.iloc[i] == 0:\n        label2[i] = 1\n\naccuracy_score = metrics.accuracy_score(clusters, label2)\nprint(accuracy_score)","9a60ebfe":"def train_data_unsupervised2(tfidf_array, i, j):\n    import pandas as pd\n    df1 = pd.DataFrame(tfidf_train_array).T\n    df1['Doc_Freq'] = df1[df1[:] > 0.0].count(axis = 1)\n    # df1['Doc_Freq'].unique()\n\n    df1 = df1.T\n#     print(min(df1.loc['Doc_Freq']), max(df1.loc['Doc_Freq']))\n    sub_df = df1.loc[:, (df1.loc['Doc_Freq'] >= i) & (df1.loc['Doc_Freq'] <= j)]\n    sub_df['Array'] = sub_df[sub_df[:] > 0.0].count(axis = 1)\n    df1 = sub_df.T\n    \n    tfidf_df = df1.T\n    \n    tfidf_df_outlierremoved = tfidf_df[tfidf_df['Array'] >= 50]\n    \n    return tfidf_df_outlierremoved","e69261af":"from sklearn.cluster import KMeans\nfrom sklearn import metrics\n\nnum_clusters = 2\nkm = KMeans(n_clusters=num_clusters, max_iter=1000, n_init=50)","570b3d31":"def kmeans(tfidf_train_outlierremoved):\n    km.fit(tfidf_train_outlierremoved)\n    clusters = km.labels_.tolist()\n    \n    original_labels = train_data.loc[tfidf_train_outlierremoved.index].label\n    label2 = list(map(int, original_labels)).copy()\n    for k in range(len(original_labels)):\n        if original_labels.iloc[k] == 1:\n            label2[k] = 0\n        elif original_labels.iloc[k] == 0:\n            label2[k] = 1\n    \n    accuracy_score = metrics.accuracy_score(clusters, list(map(int, original_labels)))\n    # print(accuracy_score)\n    \n    accuracy_score1 = metrics.accuracy_score(clusters, label2)\n    # print(accuracy_score1)\n    \n    if(accuracy_score1 > accuracy_score):\n        final_acc = accuracy_score1\n    else:\n        final_acc = accuracy_score\n    print(final_acc)\n    return final_acc","23d070ad":"accuracy_df = pd.DataFrame(columns = ['min_df', 'max_df', 'noOfDocuments', 'noOfFeatures', 'featuresInDocs', 'accuracy'])\nfor i in range(0, 10):\n    for j in range(1250, 100, -100):\n        tfidf_train_outlierremoved = train_data_unsupervised2(tfidf_train_array, i, j)\n        tfidf_train_outlierremoved = tfidf_train_outlierremoved.iloc[:-1, :-1]\n#         print(tfidf_train_outlierremoved.shape)\n\n        tfidf_test_array = tfidf_test_matrix.T[tfidf_train_outlierremoved.T.index].T.toarray()\n        \n        final_acc = kmeans(tfidf_train_outlierremoved)\n        accuracy_df = accuracy_df.append({'min_df':i, 'max_df':j, 'noOfDocuments':tfidf_train_outlierremoved.shape[0], 'noOfFeatures':tfidf_train_outlierremoved.shape[1], 'featuresInDocs':50, 'accuracy':final_acc}, ignore_index=True)","6e83726b":"accuracy_df[accuracy_df.accuracy == max(accuracy_df['accuracy'])]","4bfecf88":"accuracy_df[accuracy_df.accuracy == max(accuracy_df['accuracy'])]","ebfe2ebd":"accuracy_df.to_csv('kmeans_outlierremoval_accuracy.csv')","98cbe318":"N = tfidf_train_array.shape[0]","0cd413a2":"index = [1, 2]\ncentroid = (tfidf_train_array[index])\nprint(centroid.shape, tfidf_train_array.shape)","c67f1a6d":"from scipy.spatial import distance\nimport numpy as np\nindex = list(np.random.choice(np.arange(0, N), 2, replace=False))\nprint(index)\ncentroids = tfidf_train_array[index]\nnew_centroids = np.zeros((centroids.shape[0], centroids.shape[1]))\n# centroids = centroids.toarray()\nlabel = []\nfor i in range(N):\n    label.append(-1)\nlabel = np.array(label)\nflag = 0\nm=0\n# print(dist[1], centroids[1])\nwhile(flag == 0):\n    sum_arr = np.zeros((2, centroids.shape[1]))\n    count = np.zeros((2))\n    for i in range(0, N):\n        ans = 999\n        for j in range(0, 2):\n            ds = cosine_sim_function(centroids[j], tfidf_train_array[i].T)\n            if ds < ans:\n                ans = ds\n                label[i] = j\n#         print(label[i])\n        sum_arr[label[i]] = np.add(sum_arr[label[i]], tfidf_train_array[i])\n        count[label[i]] = count[label[i]] + 1\n    print(count)\n    for i in range(0, 2):\n        new_centroids[i] = sum_arr[i] \/ count[i]\n        new_centroids[i] = np.around(new_centroids[i], decimals=1)\n    if (new_centroids == centroids).all():\n        flag+=1\n        # for j in range(centroids.shape[1]):\n        #     if new_centroids[i][j] == centroids[i][j]:\n        #         flag+=1\n    print(\"\\n\\nflag = \", flag)\n    print(\"itr: \", m, \"\\n\")\n    if flag == 1:\n        break\n    else:\n        flag = 0\n    centroids = new_centroids.copy()\n    m+=1\nprint(label)","ad5e1507":"from sklearn import metrics\ncount1 = 0\ncount2 = 0\nfor i in range(len(label)):\n    if label[i] == 0:\n        count1 = count1 + 1\n    else:\n        count2 = count2 + 1\n\nprint(count1, count2)\n\noriginal_labels = train_data.label\nprint(len(original_labels))\n\naccuracy_score = metrics.accuracy_score(label, list(map(int, original_labels)))\nprint(accuracy_score)","590ddd16":"from sklearn.cluster import DBSCAN\nclustering = DBSCAN(eps=0.2, min_samples=5).fit(tfidf_train_array)\nclusters = clustering.labels_\nprint(np.unique(clusters))\n\ncount1 = 0\ncount2 = 0\nfor i in range(len(clusters)):\n    if clusters[i] == 0:\n        count1 = count1 + 1\n    else:\n        count2 = count2 + 1\n\ncount1, count2","f17813d9":"from sklearn import svm\nfrom sklearn.metrics import f1_score,confusion_matrix\n\nsvm_classifier = svm.SVC(gamma=0.9)\nsvm_classifier.fit(tfidf_train_array,train_label)\nsvm_pred = svm_classifier.predict(tfidf_test_array)\n\nsvm_accuracy = svm_classifier.score(tfidf_test_array,test_label)\nsvm_cm = confusion_matrix(svm_pred.astype(int),test_label[0:len(tfidf_test_array)])\nsvm_f1 = f1_score(svm_pred.astype(int),test_label[0:len(tfidf_test_array)])\n\nprint(\"The SVM Accuracy is: \",svm_accuracy)\nprint(\"The F1 score is: \",svm_f1)\nprint(\"The confusion matrix is:\")\nprint(svm_cm)","b39ce714":"tfidf_test_array.shape","72b19b03":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score,confusion_matrix\n\nlr_classifier = LogisticRegression()\nlr_classifier.fit(tfidf_train_array,train_label)\nlr_pred = lr_classifier.predict(tfidf_test_array)\n\nlr_acc = float((sum(lr_pred==test_label))\/len(test_label))\nlr_f1 = f1_score(lr_pred.astype(int),test_label)\nlr_cm = confusion_matrix(lr_pred.astype(int),test_label)\nprint(\"The accuracy is :\",lr_acc)\nprint(\"The f1 score is :\",lr_f1)\nprint(\"confusion matrix is:\")\nprint(lr_cm)","530f5894":"from sklearn.linear_model import LogisticRegression\n\nlr_classifier = LogisticRegression()\nlr_classifier.fit(chi_train_tfidf,train_label)\nlr_pred = lr_classifier.predict(chi_test_tfidf)\n\nlr_acc = float((sum(lr_pred==test_label))\/len(test_label))\nlr_f1 = f1_score(lr_pred.astype(int),test_label)\nlr_cm = confusion_matrix(lr_pred.astype(int),test_label)\n\nprint(\"The accuracy is :\",lr_acc)\nprint(\"The f1 score is :\",lr_f1)\nprint(\"confusion matrix is:\")\nprint(lr_cm)","5ae4cb14":"def train_model(classifier, train_features, label, test_features, label_test):\n    from sklearn import metrics\n    classifier.fit(train_features, label)\n    \n    predictions = classifier.predict(test_features)\n    \n    accuracy = metrics.accuracy_score(predictions, label_test)\n    f1score = metrics.f1_score(predictions,label_test)\n    conf_matrix = metrics.confusion_matrix(predictions, label_test)\n    \n    return accuracy, f1score, conf_matrix","0fdac894":"# Naive Bayes\nfrom sklearn import naive_bayes\n\n(accuracy, f1score, conf_matrix) = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, y_train, xtest_tfidf, y_test)\nprint(\"[Naive Bayes] TF-IDF Accuracy:\", round(accuracy, 3))\nprint(\"[Naive Bayes] TF-IDF f1-score:\", round(f1score, 3))\nprint(\"[Naive Bayes] TF-IDF Confusion matrix:\\n\", conf_matrix)","3091d78e":"# Naive Bayes\nfrom sklearn import naive_bayes\n\n(accuracy, f1score, conf_matrix) = train_model(naive_bayes.MultinomialNB(), tfidf_train_matrix, train_label, tfidf_test_matrix, test_label)\nprint(\"[Naive Bayes] TF-IDF Accuracy:\", round(accuracy, 3))\nprint(\"[Naive Bayes] TF-IDF f1-score:\", round(f1score, 3))\nprint(\"[Naive Bayes] TF-IDF Confusion matrix:\\n\", conf_matrix)","7ba5854c":"# Naive Bayes\nfrom sklearn import naive_bayes\n\n(accuracy, f1score, conf_matrix) = train_model(naive_bayes.MultinomialNB(), chi_train_tfidf, train_label, chi_test_tfidf, test_label)\nprint(\"[Naive Bayes] TF-IDF Accuracy:\", round(accuracy, 3))\nprint(\"[Naive Bayes] TF-IDF f1-score:\", round(f1score, 3))\nprint(\"[Naive Bayes] TF-IDF Confusion matrix:\\n\", conf_matrix)","bbe73b23":"# Naive Bayes\nfrom sklearn import naive_bayes\n\n(accuracy, f1score, conf_matrix) = train_model(naive_bayes.MultinomialNB(), ig_train_tfidf, train_label, ig_test_tfidf, test_label)\nprint(\"[Naive Bayes] TF-IDF Accuracy:\", round(accuracy, 3))\nprint(\"[Naive Bayes] TF-IDF f1-score:\", round(f1score, 3))\nprint(\"[Naive Bayes] TF-IDF Confusion matrix:\\n\", conf_matrix)","4014a98b":"def select_k_best2(tfidf_train_matrix, tfidf_test_matrix, train_data, i):\n    from sklearn.feature_selection import chi2,SelectKBest,mutual_info_regression\n    \n    vectorizer_chi2 = SelectKBest(chi2,k=i)\n    chi_train_tfidf = vectorizer_chi2.fit_transform(tfidf_train_matrix,train_label)\n    chi_test_tfidf = vectorizer_chi2.transform(tfidf_test_matrix)\n    \n    vectorizer_ig = SelectKBest(mutual_info_regression,k=i)\n    ig_train_tfidf = vectorizer_ig.fit_transform(tfidf_train_matrix,train_label)\n    ig_test_tfidf = vectorizer_ig.transform(tfidf_test_matrix)\n    \n    return chi_train_tfidf, chi_test_tfidf, ig_train_tfidf, ig_test_tfidf","fe982b43":"def train_model2(classifier, train_features, label, test_features, label_test):\n    from sklearn import metrics\n    classifier.fit(train_features, label)\n    \n    predictions = classifier.predict(test_features)\n    \n    accuracy = metrics.accuracy_score(predictions, label_test)\n#     f1score = metrics.f1_score(predictions,label_test)\n#     conf_matrix = metrics.confusion_matrix(predictions, label_test)\n    \n    return accuracy","266ab3f7":"tfidf_train_array.shape","c1614fd4":"from sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import naive_bayes\n\nmodels = [svm.SVC(), LogisticRegression(), naive_bayes.MultinomialNB()]\naccuracy_df_s = pd.DataFrame(columns = ['modelname', 'featureSelectionMethod', 'totalDocs', 'noOfFeatures', 'accuracy'])","81dbad8f":"for i in range(500, 20000, 1000):\n    (chi_train_tfidf, chi_test_tfidf, ig_train_tfidf, ig_test_tfidf) = select_k_best2(tfidf_train_matrix, tfidf_test_matrix, train_data, i)\n    print(i)\n    for j in range(0, 3):\n        accuracy = train_model2(models[j], chi_train_tfidf, train_label, chi_test_tfidf, test_label)\n        accuracy_df_s = accuracy_df_s.append({'modelname':models[j], 'featureSelectionMethod':'Chi_Square', 'totalDocs':tfidf_train_array.shape[0], 'noOfFeatures':i, 'accuracy':accuracy}, ignore_index=True)\n        # print(\"[SVM] TF-IDF f1-score:\", round(f1score, 3))\n        # print(\"[SVM] TF-IDF Confusion matrix:\\n\", conf_matrix)\n        \n        accuracy = train_model2(models[j], ig_train_tfidf, train_label, ig_test_tfidf, test_label)\n        accuracy_df_s = accuracy_df_s.append({'modelname':models[j], 'featureSelectionMethod':'Info_Gain', 'totalDocs':tfidf_train_array.shape[0], 'noOfFeatures':i, 'accuracy':accuracy}, ignore_index=True)\n        # print(\"[SVM] TF-IDF f1-score:\", round(f1score, 3))\n        # print(\"[SVM] TF-IDF Confusion matrix:\\n\", conf_matrix)\n        \n        print(j)","83d10f9c":"svm_chi_acc = accuracy_df_s[accuracy_df_s['modelname'] == \"LogisticRegression()\"]\nsvm_chi_acc","70848bdc":"accuracy_df_s.to_csv('supervised_chi_&_ig_acc.csv')","541e5f3e":"pip install libtlda","dbef53c7":"tfidf_test_matrix.toarray().shape","bf4a17b8":"from libtlda.iw import ImportanceWeightedClassifier\n\nclassifier = ImportanceWeightedClassifier().fit(tfidf_train_matrix.toarray(), train_label, tfidf_test_matrix.toarray)\ny_pred = classifier.predict(tfidf_test_matrix.toarray())\naccuracy = metrics.accuracy_score(predictions, test_label)\nf1score = metrics.f1_score(predictions,test_label)\nconf_matrix = metrics.confusion_matrix(predictions, test_label)","0374ea8e":"## Kmeans Library function (euclidian distance)","c5081a14":"### Infor Gain or Chi square","8d1e8b0a":"## DBSCAN","eead35e6":"### tfidf outlierremoved function","48099e82":"## Visualization","cb21c916":"## Kmeans from scratch, Cosine Distance","fc7419fd":"#### find dominant topic for each document","b876c75d":"## Naive bayes","58921742":"## Cosine Similarity","a00dab07":"## Reading and text cleaning","9e5c8040":"## TLDA","53aa503c":"### Vectorizing TFIDF","0ff0b69d":"## All Supervised Models - using chisquare and info gain with different number of features","3a48490b":"## Part 3","e74b5662":"## Logistic regression","e2b72bf9":"## SVM","3d5e8871":"## Kmeans report generation Function"}}