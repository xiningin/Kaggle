{"cell_type":{"0a355871":"code","035d78ee":"code","6ade7ea1":"code","e7fa20cc":"code","2bc56b7e":"code","01058275":"code","99cc3521":"code","5f5c58af":"code","e3ff9c33":"code","3b44aa2a":"code","1b42f0f1":"code","7b0b45aa":"code","c672aadc":"code","cde102ec":"code","04cd10a9":"code","cf5e3d3a":"code","79d063c6":"code","e4312390":"code","e122879c":"code","d00e5a14":"code","5ae5b2f1":"code","60ca485f":"code","c085ee79":"code","04759d0a":"code","da09cf64":"code","c9591e80":"code","edd5604b":"code","218ce119":"code","36f58a01":"code","e752dd42":"code","00c2a7a8":"code","431fcf2d":"code","f36993fb":"code","290ded30":"code","0f0b840a":"code","f9e2039c":"code","5f07faad":"code","cc31f10a":"code","7edf3b54":"code","1b1509fc":"code","33d26055":"code","7247b960":"code","7cfbeeab":"code","e1b8103b":"code","21ebca30":"code","5cced454":"code","2faefe75":"code","d1adacfa":"code","1929d57c":"code","36a3237b":"code","eaf1cf01":"code","817a3cb4":"code","dabb9d9e":"code","f7912794":"code","bc230fc2":"code","94604f59":"code","74afa373":"code","6936320d":"code","0bab6b60":"code","4c763913":"code","58d63f29":"code","06fcf6bc":"code","89e42168":"code","1301a6ca":"code","e08e48f1":"code","21061368":"code","7da3872d":"code","336602b7":"code","666e5dcc":"code","a03cb002":"code","61d5f250":"markdown","d28a03d2":"markdown","cc3c4e23":"markdown","93c301ea":"markdown","8be52ca3":"markdown","8a0f2ded":"markdown","2e31d5f1":"markdown","429541c1":"markdown","d201bc11":"markdown","23424b13":"markdown","ec937988":"markdown","0a44ec94":"markdown","26764647":"markdown","15c2869a":"markdown","e2f3abb0":"markdown","a7b8a491":"markdown","e065b71d":"markdown","d9ca9ea0":"markdown","8b6440ae":"markdown","431d4a66":"markdown","6a8b1f3a":"markdown","86492a57":"markdown","aae1c7ce":"markdown","7e29285a":"markdown","24f04be6":"markdown","022a319d":"markdown","137a6cb6":"markdown","a74df483":"markdown","2d002613":"markdown","305f4a13":"markdown","388c7a19":"markdown","7f2ff56c":"markdown","8a4fb8d2":"markdown","1522f670":"markdown","a09177e6":"markdown","029ee184":"markdown","fc6ebe1f":"markdown","3e06ad56":"markdown","ccfc0174":"markdown","54029a15":"markdown","a17fc231":"markdown","fdd7a339":"markdown","9a20e01c":"markdown","d4826d79":"markdown","1a21d8e2":"markdown"},"source":{"0a355871":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","035d78ee":"df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","6ade7ea1":"df.head()","e7fa20cc":"df.info()","2bc56b7e":"# number of unique keywords\ndf.loc[:,'keyword'].nunique()","01058275":"# frequency of using keywords\ndf.loc[:,'keyword'].value_counts()","99cc3521":"# number of unique locations\ndf.loc[:,'location'].nunique()","5f5c58af":"# frequency of using different locations\ndf.loc[:,'location'].value_counts()","e3ff9c33":"# data description\nprint(df.groupby('target')['target'].count())\n\n# percent of target=1\ntarget_count = df[df['target'] == 1]['target'].count()\ntotal = df['target'].count()\ntarget_share = target_count\/total\nprint(\"Percentage of data showing real incidents \\\"target=1\\\" {0:.2f}\".format(target_share))\n\n# histogram\ndf[df['target'] == 0]['target'].astype(int).hist(label='Fake', grid = False, bins=1, rwidth=0.8)\ndf[df['target'] == 1]['target'].astype(int).hist(label='Real', grid = False, bins=1, rwidth=0.8)\nplt.xticks((0,1),('Fake', 'Real'))\nplt.show()","3b44aa2a":"# since the location attribute lacks the coordinates necessary to build the map, we add a new world-cities-database\n# in this database latitude \/ longitude coordinates are assigned to each country or city\n\nlatlong = pd.read_csv(\"\/kaggle\/input\/world-cities-database\/worldcitiespop.csv\")\nlatlong.head()","1b42f0f1":"# rename the AccentCity column to Location to merge with our original base\nlatlong.rename(columns={\"AccentCity\": \"location\"}, inplace=True)\nlatlong.head()","7b0b45aa":"# Get unique combinations of location and latitude \/ longitude to add to the source database, remove duplicate lines and missing locations\nlatlong_grouped = latlong[['location', 'Latitude', 'Longitude', 'Population']].drop_duplicates()\nlatlong_grouped = latlong_grouped[latlong_grouped.location != 'None']","c672aadc":"# let sort the data first alphabetically by location (ascending) and then by population (descending)\nlatlong_grouped.sort_values(['location', 'Population'], ascending=[True, False], inplace=True)","cde102ec":"latlong_grouped.shape","04cd10a9":"# remove duplicate cities from the base, leaving only those cities from duplicates for which the population is maximum\n# to do this, compare the current line with the next one (shift shifts one line forward), if they are repeated, then we do not include them in the base\nlatlong_cleaned = latlong_grouped[latlong_grouped['location'] != latlong_grouped['location'].shift()]\n# check the dimension of the base\nlatlong_cleaned.shape","cf5e3d3a":"# for the base with coordinates, in which duplicates were not deleted\nlatlong_grouped[latlong_grouped['location'] == 'Birmingham']","79d063c6":"# for the base with coordinates, in which duplicate locations were cleared\nlatlong_cleaned[latlong_cleaned['location'] == 'Birmingham']","e4312390":"# Add coordinates to our original tweet base\ndf_latlong = pd.merge(df, latlong_cleaned, left_on='location', right_on='location', how='left')\ndf_latlong.head()","e122879c":"df_latlong.shape","d00e5a14":"# Display locations for which coordinates were determined\ndf_latlong[~df_latlong['Latitude'].isna()]","5ae5b2f1":"# number of the real tweets with coordinates\ndf_latlong_real = df_latlong[(~df_latlong['Latitude'].isna()) & (df_latlong['target'] == 1)]\nprint( len( df_latlong_real ) )","60ca485f":"# an example of real tweets with coordinates\ndf_latlong_real.head()","c085ee79":"# Let's put real tweets on the map\nimport folium\nfrom folium.plugins import HeatMap\n\ndf_latlong_real.Latitude.fillna(0, inplace = True)\ndf_latlong_real.Longitude.fillna(0, inplace = True) \ntwits = df_latlong_real[['Latitude', 'Longitude']]\n\nRealTwitsMap=folium.Map(location=[0,0],zoom_start=2)\nHeatMap(data=twits, radius=12).add_to(RealTwitsMap)\nRealTwitsMap","04759d0a":"# a number of fake tweets with the coordinates\ndf_latlong_fake = df_latlong[(~df_latlong['Latitude'].isna()) & (df_latlong['target'] == 0)]\nprint( len( df_latlong_fake ) )","da09cf64":"# an example of fake tweets with the coordinates\ndf_latlong_fake.head()","c9591e80":"# let's put the fake tweets on a map\nimport folium\nfrom folium.plugins import HeatMap\n\ndf_latlong_fake.Latitude.fillna(0, inplace = True)\ndf_latlong_fake.Longitude.fillna(0, inplace = True) \ntwits = df_latlong_fake[['Latitude', 'Longitude']]\n\nFakeTwitsMap=folium.Map(location=[0,0],zoom_start=2)\nHeatMap(data=twits, radius=12).add_to(FakeTwitsMap)\nFakeTwitsMap","edd5604b":"# function for drawing a cloud of frequently used words\nfrom wordcloud import WordCloud,STOPWORDS\n\ndef wordcloud_img(data):\n    plt.figure(figsize = (20,20))\n    wc = WordCloud(min_font_size = 3, \n                   background_color=\"white\",  \n                   max_words = 3000, \n                   width = 1000, \n                   height = 600, \n                   stopwords = STOPWORDS).generate(str(\" \".join(data)))\n    plt.imshow(wc,interpolation = 'bilinear')","218ce119":"wordcloud_img(df[df['target'] == 1]['text'])","36f58a01":"wordcloud_img(df[df['target'] == 0]['text'])","e752dd42":"from urllib.parse import unquote","00c2a7a8":"# display the words with urlencoded characters (they always have a \"%\" sign)\nold_phrase = ''\nfor phrase in df['keyword']:\n    phrase = str(phrase)\n    if('%' in phrase and phrase != old_phrase):\n        print(phrase)\n        old_phrase = phrase","431fcf2d":"# let's make a replacement\nfor i, phrase in enumerate(df['keyword']):\n    phrase  = str(phrase)\n    if('%20' in phrase):\n        df.loc[i, 'keyword'] = df.loc[i, 'keyword'].replace('%20', ' ')\n\n# let's check\n# let's display all the words with urlencoded characters (they always have a \"%\" sign)\nfor phrase in df['keyword']:\n    phrase  = str(phrase)\n    if('%' in phrase):\n        print(phrase)","f36993fb":"# check for gaps in columns\ndf.isna().sum()","290ded30":"def location_and_keyword_fill_nan(df):\n    # replace the gaps for keywords with None\n    df.keyword.fillna('None', inplace = True)\n\n    # replace the gaps for location with None\n    df.location.fillna('None', inplace = True)\n\nlocation_and_keyword_fill_nan(df)\n# let's check\ndf.isna().sum()","0f0b840a":"# return the indices of those elements of the target array (target variable), where the value is 0\ntarget_np = df['target'].astype(int).to_numpy()\nfake_twits_ids = np.argwhere(target_np == 0).flatten()\nprint('Total fake_twits: ', len(fake_twits_ids))\nfake_twits_ids","f9e2039c":"# mix the array with the id of fake tweets\nfrom sklearn.utils import shuffle\nfake_twits_ids = shuffle(fake_twits_ids, random_state = 42)\n\n# select \"extra\" id of fake tweets in it\n# i.e. take all elements after number 3271 from the jumbled array fake_twits_ids\nfake_twits_ids_to_drop = fake_twits_ids[len(np.argwhere(target_np == 1).flatten()):]\n\n# display the number of fake tweets that need to be discarded from the sample for balancing, as well as their id\nprint(len(fake_twits_ids_to_drop))\nfake_twits_ids_to_drop","5f07faad":"# since data are mixed, then after discarding unnecessary elements, the sample will be representative\ndf_balanced = df.drop(df.index[fake_twits_ids_to_drop])\n\n# display the total size of the dataset features\ndf_balanced.shape","cc31f10a":"# now we see that the classes are balanced.\ndf_balanced['target'].value_counts()","7edf3b54":"# histogram\ndf_balanced[df_balanced['target'] == 0]['target'].astype(int).hist(label='Fake', grid = False, bins=1, rwidth=0.8)\ndf_balanced[df_balanced['target'] == 1]['target'].astype(int).hist(label='Real', grid = False, bins=1, rwidth=0.8)\nplt.xticks((0,1),('Fake', 'Real'))\nplt.show()","1b1509fc":"def location_and_keyword_to_bool_category(df):\n    df.loc[df['location'] == 'None', 'location_bool'] = 0\n    df.loc[df['location'] != 'None', 'location_bool'] = 1\n    df.loc[df['keyword'] == 'None', 'keyword_bool'] = 0\n    df.loc[df['keyword'] != 'None', 'keyword_bool'] = 1\n\nlocation_and_keyword_to_bool_category(df_balanced)\n\n# let's check\ndf_balanced","33d26055":"# do similar transformations before splitting into train and test, because. such transformation of each text occurs independently of the sample\n# besides, X_train and X_test, which are formed after train_test_split are copies of the base and it is more difficult to do such transformations on copies (an error may be returned that work is being done on the copy, etc.)\n# regular expressions library\nimport re\n\n# function for converting single text\ndef single_text_clean(text):\n    # convert text to lowercase\n    text = text.lower()\n    # transform https: \/\/ etc. addresses in the text \"URL\"\n    text = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+))','URL',text)\n    # convert username @username to \"AT_USER\"\n    text = re.sub('@[^\\s]+','AT_USER', text)\n    # convert multiple spaces to one space\n    text = re.sub('[\\s]+', ' ', text)\n    # convert hashtag # topic to \"topic\"\n    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n    return text\n\n# function to convert text in different columns of the dataframe\n# (applies the previous single_text_clean function to different columns)\ndef text_columns_clean(df):\n    text_columns_to_clean = ['keyword', 'location', 'text']\n    for column in text_columns_to_clean:\n        df.loc[:, column] = df[column].apply(single_text_clean)\n\ntext_columns_clean(df_balanced)\n\n# let's check\ndf_balanced","7247b960":"import nltk\nfrom nltk.corpus import stopwords\nimport string\n\nstop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)","7cfbeeab":"# function for defining adjectives (ADJ), verbs (VERB), nouns (NOUN) and adverbs (ADV)\nfrom nltk.corpus import wordnet as wn\n\ndef get_simple_pos(tag):\n    if tag.startswith('J'):\n        return wn.ADJ\n    elif tag.startswith('V'):\n        return wn.VERB\n    elif tag.startswith('N'):\n        return wn.NOUN\n    elif tag.startswith('R'):\n        return wn.ADV\n    else:\n        return wn.NOUN\n\n# Lemmatization\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import pos_tag\n\n# (discard all unnecessary in the sentence, reduce words to normal form and get a list of them)\n# 'Once upone a time a man walked into a door' -> ['upone', 'time', 'man', 'walk', 'door']\nlemmatizer = WordNetLemmatizer()\ndef lemmatize_words(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            pos = pos_tag([i.strip()])\n            word = lemmatizer.lemmatize(i.strip(),get_simple_pos(pos[0][1]))\n            final_text.append(word.lower())\n    return final_text   \n\n# Combine the lemmatized list into a sentence\n# ['upone', 'time', 'man', 'walk', 'door'] -> 'upone time man walk door '\ndef join_text(text):\n    string = ''\n    for i in text:\n        string += i.strip() +' '\n    return string\n\n# Start lemmatization and create a new field with lemmatized clauses 'text_lemma'\ndf_balanced.loc[:, 'text'] = df_balanced['text'].apply(lemmatize_words).apply(join_text)\n\n# let's check\ndf_balanced","e1b8103b":"# remove the additional id field that we no longer need (it is in the index)\ndf_balanced = df_balanced.drop(['id'], axis=1)\n\nfrom sklearn.model_selection import train_test_split\n\n# split the sample into X and y\ny = df_balanced['target']\nX = df_balanced.drop(['target'], axis=1)\n\n# splitting the sample into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42)\n\n# let's check\nX_train.shape","21ebca30":"X_train","5cced454":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Assigning weights to words using TfidfVectorizer\ntv = TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,2))\ntv_X_train = tv.fit_transform(X_train['text'])\ntv_X_test = tv.transform(X_test['text'])\n\nprint('TfidfVectorizer_train:', tv_X_train.shape)\nprint('TfidfVectorizer_test:', tv_X_test.shape)","2faefe75":"# flip columns of words from a sparse matrix with one column to a numpy array with many columns\ntv_X_train = tv_X_train.toarray()\ntv_X_test = tv_X_test.toarray()","d1adacfa":"# Normalize the data in tv_X_train \/ test\nfrom sklearn.preprocessing import StandardScaler\n\ndef scaling_train(tv_X_train):\n    scaler_tv = StandardScaler(copy=False)\n    tv_X_train_scaled = scaler_tv.fit_transform(tv_X_train)\n    # transform tv_X_train \/ test sparse numpy matrices into pandas Data Frame\n    tv_X_train_pd_scaled = pd.DataFrame(data=tv_X_train_scaled, \n                             index=X_train.index, \n                             columns=np.arange(0, np.size(tv_X_train_scaled,1)))\n    return tv_X_train_pd_scaled, scaler_tv\n\ndef scaling_test(tv_X_test, scaler_tv):\n    tv_X_test_scaled = scaler_tv.transform(tv_X_test)\n    # transform tv_X_train \/ test sparse numpy matrices into pandas Data Frame\n    tv_X_test_pd_scaled = pd.DataFrame(data=tv_X_test_scaled, \n                             index=X_test.index, \n                             columns=np.arange(0, np.size(tv_X_test_scaled,1)))\n    return tv_X_test_pd_scaled\n\ntv_X_train_pd_scaled, scaler_tv = scaling_train(tv_X_train)\ntv_X_test_pd_scaled = scaling_test(tv_X_test, scaler_tv)\n\n# let's check\ntv_X_train_pd_scaled.shape, tv_X_test_pd_scaled.shape","1929d57c":"# convert pandas dataframe to numpy array for pytorch model\ntv_X_train_pd_scaled_arr = tv_X_train_pd_scaled.to_numpy()\ntv_X_test_pd_scaled_arr = tv_X_test_pd_scaled.to_numpy()","36a3237b":"tv_X_test_pd_scaled_arr","eaf1cf01":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\n# funtools - working with higher order functions\n# partial - changes the number of arguments for the passed functions and runs the functions one by one\nfrom functools import partial\n\n# make the results of random divisions of samples reproducible (this is analogous to random_state in sklearn)\ntorch.manual_seed(42)","817a3cb4":"# Determine the device on which the training will take place (if we have a CPU, it will be on the CPU, if the GPU, then not the GPU)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice","dabb9d9e":"# class for transforming and loading data into Pytorch\n# it inherits from the Dataset class\n# df_tfidf is df converted to tf-idf (i.e. it is tv_X_train or tv_X_test matrix)\n# df is a balanced base df_balanced\n\nclass TwitsDataset(Dataset):\n    def __init__(self, tv_X_train_array, y_train):\n        df = pd.DataFrame(index=y_train.index)\n        \n        # the text was previously cleaned up, lematized and turned into tokens in the previous steps\n        df['tfidf_vector'] = [vector.tolist() for vector in tv_X_train_array]\n        \n        self.tfidf_vector = df.tfidf_vector.tolist()\n        self.targets = y_train.tolist()\n    \n    def __getitem__(self, i):\n        return (\n            self.tfidf_vector[i],\n            self.targets[i]\n        )\n    \n    def __len__(self):\n        return len(self.targets)","f7912794":"for i, vector in enumerate(tv_X_train):\n    print(vector)\n    vector.tolist()\n    if(i ==  5): break","bc230fc2":"# loading data into a class and transforming them\n\n# normalized tf-idf\ndataset = TwitsDataset(tv_X_train_pd_scaled_arr, y_train)","94604f59":"# Because we have already formed a test sample, then at this step we will not separate it from the current data (the test sample is not included in this dataset)\nfrom torch.utils.data.dataset import random_split\n\ndef split_train_valid_test(corpus, valid_ratio=0.1, test_ratio=0.1):\n    \"\"\"Split dataset into train, validation, and test.\"\"\"\n    test_length = int(len(corpus) * test_ratio)\n    valid_length = int(len(corpus) * valid_ratio)\n    train_length = len(corpus) - valid_length - test_length\n    return random_split(\n        corpus, lengths=[train_length, valid_length, test_length],\n    )\n\ntrain_dataset, valid_dataset, test_dataset = split_train_valid_test(dataset, valid_ratio=0.2, test_ratio=0.0)\nlen(train_dataset), len(valid_dataset), len(test_dataset)\n# this does not mean that we have Test-selection = 0, it just is defined elsewhere and called tv_X_test\n# (and at this step, we simply did not select the test sample from tv_X_train, since we already have tv_X_test)","74afa373":"# check the contents of train_dataset\nprint('Number of records:', len(train_dataset), '\\n')\n\nimport random\n# generating the same random variables\nrandom.seed(a=42, version=2)\n\nrandom_idx = random.randint(0,len(train_dataset)-1)\nprint('Random index from dataset:', random_idx, '\\n')\ntfidf_vector, sample_target = train_dataset[random_idx]\nprint('TF-IDF Vector Size:', len(tfidf_vector), '\\n')\nprint('Target example:', sample_target, '\\n')","6936320d":"# \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u043c\u043e\u0435 valid_dataset\nprint('Number of records:', len(valid_dataset), '\\n')\n\nrandom.seed(a=42, version=2)\nrandom_idx = random.randint(0,len(valid_dataset)-1)\nprint('Random index from dataset:', random_idx, '\\n')\ntfidf_vector, sample_target = valid_dataset[random_idx]\nprint('TF-IDF Vector Size:', len(tfidf_vector), '\\n')\nprint('Target example:', sample_target, '\\n')","0bab6b60":"# loading data for Pytorch (loads data by batch)\nBATCH_SIZE = 512\n\ndef collate(batch):\n    tfidf = torch.FloatTensor([item[0] for item in batch])\n    target = torch.LongTensor([item[1] for item in batch])\n    return tfidf, target\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate)\nvalid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=collate)","4c763913":"# see the contents of train_loader\nprint('Number of training batches:', len(train_loader), '\\n')\n\nrandom.seed(a=42, version=2)\nbatch_idx = random.randint(0, len(train_loader)-1)\nexample_idx = random.randint(0, BATCH_SIZE-1)\n\nprint(\"Batch index: \", batch_idx)\nprint(\"Example index: \", example_idx)\n\nfor i, fields in enumerate(train_loader):\n    tfidf, target = fields\n    if i == batch_idx:\n        print('TF-IDF vector size:', len(tfidf[example_idx]), '\\n')\n        print('Type of TF-IDF: ', type(tfidf[example_idx]), '\\n')\n        print('Target type: ', type(target[example_idx]), '\\n')","58d63f29":"# building a neural network (CNN on normalized TF-IDF)\n\nclass Flatten(nn.Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)\n\nclass Reorder(nn.Module):\n    def forward(self, input):\n        return input.permute((0, 2, 1))\n\nclass FeedfowardTextClassifier(nn.Module):\n    def __init__(self, device, vocab_size, hidden1, hidden2, hidden3, hidden4, num_labels, batch_size):\n        super(FeedfowardTextClassifier, self).__init__()\n        self.device = device\n        self.batch_size = batch_size\n\n        self.convnet = nn.Sequential(\n            nn.Conv1d(in_channels=vocab_size,\n                      out_channels=hidden1, \n                      kernel_size=1),\n            nn.ELU(),\n            nn.Dropout(p=0.5),\n            nn.Conv1d(in_channels=hidden1, \n                      out_channels=hidden2,\n                      kernel_size=1, \n                      #stride=1\n                     ),\n            nn.ELU(),\n            nn.Dropout(p=0.5),\n        )\n        self.fc = nn.Linear(hidden2, 2)\n    \n    def forward(self, x):\n        batch_size = len(x)\n        if batch_size != self.batch_size:\n            self.batch_size = batch_size\n\n        features = self.convnet(x)\n        \n        # after executing convnet, delete the 3rd dimension (index = 2) with dimension 1\n        features = features.squeeze(dim=2)\n        \n        prediction_vector = self.fc(features)\n        return prediction_vector\n        \n# define the size of hidden layers\nHIDDEN1 = 512\nHIDDEN2 = 128\nHIDDEN3 = 128\nHIDDEN4 = 128\n\ntfidf_model = FeedfowardTextClassifier(\n    vocab_size=len(tfidf_vector),\n    hidden1=HIDDEN1,\n    hidden2=HIDDEN2,\n    hidden3=HIDDEN3,\n    hidden4=HIDDEN4,\n    num_labels=2,\n    device=device,\n    batch_size=BATCH_SIZE,\n)","06fcf6bc":"# portable to GPU\ntfidf_model = tfidf_model.to(device)\ntfidf_model","89e42168":"# Loss function and optimizer\n\nfrom torch import optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nLEARNING_RATE = 4e-2\n\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = optim.Adam(\n    filter(lambda p: p.requires_grad, tfidf_model.parameters()),\n    lr=LEARNING_RATE,\n)\n\nscheduler = CosineAnnealingLR(optimizer, 1)","1301a6ca":"# define the function and necessary steps for training\n\ndef train_epoch(model, optimizer, train_loader):\n    model.train()\n    total_loss, total = 0, 0\n    for i, (tfidf, target) in enumerate(train_loader):\n        \n        inputs = tfidf\n        # add a new dimension: channel to enter cnn      \n        inputs = inputs.unsqueeze(dim=2)\n        \n        #print(inputs.shape)\n        \n        # port to GPU\n        inputs = inputs.to(device)\n        target = target.to(device)\n        \n        # Reset gradient\n        optimizer.zero_grad()\n        \n        # Forward pass\n        output = model(inputs)\n        \n        # Compute loss\n        loss = criterion(output, target)\n        \n        # Perform gradient descent, backwards pass\n        loss.backward()\n\n        # Take a step in the right direction\n        optimizer.step()\n        scheduler.step()\n\n        # Record metrics\n        total_loss += loss.item()\n        total += len(target)\n\n    return total_loss \/ total\n\n# function and steps for validation\ndef validate_epoch(model, valid_loader):\n    model.eval()\n    total_loss, total = 0, 0\n    with torch.no_grad():\n        for tfidf, target in valid_loader:\n            inputs = tfidf\n            \n            # add a new dimension: channel to enter cnn\n            inputs = inputs.unsqueeze(dim=2)\n            \n            # port to GPU\n            inputs = inputs.to(device)\n            target = target.to(device)\n            \n            #print(\"Val: \", inputs.shape)\n            \n            # Forward pass\n            output = model(inputs)\n\n            # Calculate how wrong the model is\n            loss = criterion(output, target)\n\n            # Record metrics\n            total_loss += loss.item()\n            total += len(target)\n\n    return total_loss \/ total","e08e48f1":"# let's train the model\nfrom tqdm import tqdm\n\nmax_epochs = 50\nn_epochs = 0\ntrain_losses = []\nvalid_losses = []\n\nfor epoch_num in range(max_epochs):\n\n    train_loss = train_epoch(tfidf_model, optimizer, train_loader)\n    valid_loss = validate_epoch(tfidf_model, valid_loader)\n    \n    tqdm.write(\n        f'Epoch #{n_epochs + 1:3d}\\ttrain_loss: {train_loss:.2e}\\tvalid_loss: {valid_loss:.2e}\\n',\n    )\n    \n    # Early stopping if the current valid_loss (value of the error function for the validated sample) is more than 10 of the last valid losses    \n    if len(valid_losses) > 7 and all(valid_loss >= loss for loss in valid_losses[-8:]):\n        print('Stopping early')\n        break\n    \n    \n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    \n    n_epochs += 1","21061368":"# plotting the loss function after training\nepoch_ticks = range(1, n_epochs + 1)\nplt.plot(epoch_ticks, train_losses)\nplt.plot(epoch_ticks, valid_losses)\nplt.legend(['Train Loss', 'Valid Loss'])\nplt.title('Losses') \nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.xticks(epoch_ticks)\nplt.show()","7da3872d":"# loading data into a class and transforming them\ntest_dataset = TwitsDataset(tv_X_test, y_test)","336602b7":"# loading data for Pytorch (loads data by batch)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate)","666e5dcc":"# check the contents of test_loader\nprint('Number of training batches:', len(test_loader), '\\n')\n\nrandom.seed(a=42, version=2)\nbatch_idx = random.randint(0, len(test_loader)-1)\nexample_idx = random.randint(0, BATCH_SIZE-1)\n\nfor i, fields in enumerate(test_loader):\n    tfidf, target = fields\n    if i == batch_idx:\n        print('TF-IDF  vector size:', len(tfidf[example_idx]), '\\n')\n        print('Random target: ', target[example_idx], '\\n')","a03cb002":"# Efficiency of Pytorch CNN model on TFIDF\nfrom sklearn.metrics import classification_report\n\ntfidf_model.eval()\ntest_accuracy, n_examples = 0, 0\ny_true, y_pred = [], []\n\nwith torch.no_grad():\n    for tfidf, target in test_loader:\n        # add a new dimension: channel to enter cnn\n        tfidf = tfidf.unsqueeze(dim=2)\n        \n        inputs = tfidf.to(device)\n        target = target.to(device)\n        \n        probs = tfidf_model(inputs)\n        \n        probs = probs.detach().cpu().numpy()\n        predictions = np.argmax(probs, axis=1)\n        target = target.cpu().numpy()\n        \n        y_true.extend(predictions)\n        y_pred.extend(target)\n        \nprint(classification_report(y_true, y_pred))","61d5f250":"# Pytorch model","d28a03d2":"To solve the problem, we have the following attributes: keyword (the key phrase in the tweet that characterizes the incident), location (the geographical location where the incident could have occurred) and text (the text of the tweet)","cc3c4e23":"# 4. Features preparation","93c301ea":"# 4.1. Data transformation","8be52ca3":"in all cases, we only have a space encoded with the \"% 20\" sign, so we just replace it with a regular space","8a0f2ded":"### Divide the samples into training and validation\n* Training: we will train on it\n* Validation: sampling for tuning of hyperparameters (so that the model does not overfit)\n* Testing: for the final assessment of the effectiveness of the model - we already have this sample (it was determined in the previous steps above)","2e31d5f1":"We use Stemming (search for the basis of a word) Lemmatization (reduction to normal form: nouns to singular, masculine, verbs to infinitives, etc.) to reduce the number of word forms: am, are, is -> be; car, cars, car's, cars' -> car etc.","429541c1":"# 4.3. Transforming text for modeling","d201bc11":"Let's introduce 2 new categorical variables:\n1. location_bool - the fact of the presence \/ absence of a location in a tweet.\n2. keyword_bool - the fact of the presence \/ absence of a keyword in the tweet.","23424b13":"The number of fake tweets for which the coordinates were determined = 504","ec937988":"Word cloud for the fake tweets","0a44ec94":"## Features analysis","26764647":"# 3. EDA","15c2869a":"Because no phrases were displayed, but all replacements were successfully made, hence there are no more urlencoded phrases","e2f3abb0":"# 4.2. Splitting into train and test","a7b8a491":"### keywords decoding","e065b71d":"### Stemming and Lemmatization","d9ca9ea0":"Word cloud for the real tweets","8b6440ae":"### Word cloud for real and fake tweets","431d4a66":"**Conclusions:** superficially comparing the two clouds, at first glance, you can see clear differences. For example, messages about fires (fire), as well as messages with the prefix New and such disasters as flood (floods) and storm (storm), messages about death (death) are more often real (these words are less often used in a figurative sense), comparig to tweets containing words such as https, Love, the will turnover (meaning that something will happen in the future) are often figurative (in the context of our project - fake) and have no real basis.","6a8b1f3a":"there are no more duplicates in the cleared latlong_cleaned database - we left only the cities with the maximum population","86492a57":"### Transform the text into a more convenient form (we will process the URL, hashtags and users in the text)","aae1c7ce":"TFIDFVectorizer sets the frequency of occurrence for each word, and the value increases in proportion to the count, but shifts by the frequency of the word in the corpus.","7e29285a":"### Distribution of real and fake tweets by geography","24f04be6":"# 2. Data loading","022a319d":"**Map of the real tweets**","137a6cb6":"### TFIDF","a74df483":"Create a dictionary of stop words (which do not affect the analysis, such as he, have, it, the, etc.), add punctuation to the stop words","2d002613":"**A map of the fake tweets**","305f4a13":"# 5. Model training and validation\n","388c7a19":"## Checking data for balance","7f2ff56c":"### Fill in the blanks","8a4fb8d2":"Conclusion: the data is almost balanced, but not completely. \nIn the next section, we will carry out a full balancing by discarding unnecessary fake tweets (shuffle the sample of fake tweets and remove the extra 1071 elements from there (= 4342 real tweets - 3271 fake tweets)).","1522f670":"Checking for duplicate locations in the base with coordinates","a09177e6":"### Let's balance the data","029ee184":"We have more fake tweets - 4342, and fewer real ones - 3271.\nTo balance the data, we need to shuffle the fake tweets and cut them off by an extra amount (by 1071 pieces).\nThe number of extra tweets will be calculated as the number of fake tweets minus the number of real tweets, i.e .:\n> 4342 - 3271 = 1071","fc6ebe1f":"Comparing 2 maps (for real and fake tweets about incidents), we can conclude that there are no cardinal differences in the geography of tweets, at least visually it cannot be said that in some geographic regions there are more fake or real tweets (we deliberately combined tweets geographically on a map using a heatmap to draw conclusions based on more or less representative samples, because for each city separately, there is not enough data for representative conclusions (maximum 40-50 tweets per city)).","3e06ad56":"### New categorical variables","ccfc0174":"## Analysis of dependencies between features and target variable","54029a15":"In total, we managed to determine the coordinates for 895 locations. Let's build maps based on these locations.","a17fc231":"# Predicting the target for the test sample","fdd7a339":"In total, we have 391 real tweets with coordinates","9a20e01c":"you can see that some words need to be decoded, because they are in urlencoded format (% 20 for example means a space)","d4826d79":"### Stop words","1a21d8e2":"# 1. The goal\n* Based on the text of the tweet about the disaster, we will determine whether it is a real disaster (\"New forest fires in Florida\") or the text presents a figurative meaning (\"it burns out with love\", etc.) .P.).\n* As a final model we will use Pytorch\n* For quality assessment we will use F1, as well as Precision and Recall"}}