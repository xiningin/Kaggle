{"cell_type":{"afb05440":"code","b4fdf56b":"code","ce5f41aa":"code","1f767b4f":"code","a56e9f2e":"code","658f4a8f":"code","b3f522e4":"code","0ecabf68":"code","5e62a70f":"code","bc20a9b9":"code","8f58ed58":"code","e599c532":"code","8e10ffb5":"code","caf30c1b":"code","1233bf8a":"code","a201da0c":"code","884288c4":"code","c93582d7":"code","6ca9513e":"code","7ab0969b":"code","402ead0b":"code","a151d464":"code","f0fd01b5":"code","47a8d679":"code","27e15418":"code","edf2200d":"code","6c28ec31":"code","79f459d9":"code","e16f3db0":"markdown","507ad29d":"markdown","c72e5e40":"markdown","070331be":"markdown","e3bfad3b":"markdown","7c99a217":"markdown","4684d90f":"markdown","b75e3b18":"markdown","751001f4":"markdown","4b5dee6a":"markdown","3a847ac9":"markdown","081ee246":"markdown","db6ea6c4":"markdown","40a25342":"markdown","b9a56f36":"markdown","4ebb25e6":"markdown","ada052b5":"markdown","22117b15":"markdown","fc5d585b":"markdown","c1795fc4":"markdown","cc6b18f4":"markdown","92823176":"markdown","853c17ef":"markdown","519c708c":"markdown"},"source":{"afb05440":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt","b4fdf56b":"application = pd.read_csv('..\/input\/credit-card-approval-prediction\/application_record.csv')\ncredit = pd.read_csv('..\/input\/credit-card-approval-prediction\/credit_record.csv')\n\n# only model in the intersection cases between 2 dataset\nids = set(application['ID']).intersection(set(credit['ID']))\napplication = application[application['ID'].isin(ids)]\ncredit = credit[credit['ID'].isin(ids)]","ce5f41aa":"application.info()","1f767b4f":"def calc_woe(feature, target):\n    lst = []\n    feature = feature.fillna(\"NULL\")\n    \n    if len(feature.unique()) > 10:\n        raise ValueError('This method currently only supports categorical features with cardinality less than 10')\n    \n    for group in list(feature.unique()):\n        good = len(feature[(feature == group) & (target == 0)])\n        bad = len(feature[(feature == group) & (target == 1)])\n        lst.append([group, good, bad])\n\n    data = pd.DataFrame(lst, columns=['Group', 'Good', 'Bad'])\n    data['Distribution Good'] = data['Good'] \/ data['Good'].sum()\n    data['Distribution Bad'] = data['Bad'] \/ data['Bad'].sum()\n    data['WoE'] = np.log(data['Distribution Good'] \/ data['Distribution Bad'])\n    \n    data = data.replace({'WoE': {np.inf: 0, -np.inf: 0}})\n    \n    return data","a56e9f2e":"def woe_transform(feature_train, feature_test, target_train):\n    feature_train = feature_train.copy()\n    feature_test = feature_test.copy()\n    target_train = target_train.copy()\n    \n    result = calc_woe(feature_train, target_train)\n    \n    for i in range(len(result)):\n        feature_train[feature_train == result.iloc[i]['Group']] = result.iloc[i]['WoE']\n        feature_test[feature_test == result.iloc[i]['Group']] = result.iloc[i]['WoE']\n        \n    return feature_train, feature_test","658f4a8f":"def impute_occupation_type(application):\n    \n    x = application.copy()\n    \n    probability = x['OCCUPATION_TYPE'].value_counts().to_numpy()\/x['OCCUPATION_TYPE'].value_counts().sum()\n    job_list = x['OCCUPATION_TYPE'].value_counts().index.to_numpy()\n    indexes = range(len(x['OCCUPATION_TYPE'].value_counts()))\n    null_size = len(x[x['OCCUPATION_TYPE'].isnull()]['OCCUPATION_TYPE'])\n    \n    random_index = np.random.choice(a=indexes, size=null_size, p=probability)\n    \n    x.loc[:,'IMPUTED_OCCUPATION_TYPE'] = 0\n    x.loc[x['OCCUPATION_TYPE'].isnull(),'IMPUTED_OCCUPATION_TYPE'] = 1\n    x.loc[x['OCCUPATION_TYPE'].isnull(),'OCCUPATION_TYPE'] = job_list[random_index]\n\n    return x","b3f522e4":"def create_unemployed_column(application):\n    x = application.copy()\n    \n    x.loc[x['DAYS_EMPLOYED']<=0,'UNEMPLOYED'] = 0 \n    x.loc[x['DAYS_EMPLOYED']>0,'UNEMPLOYED'] = 1 \n    \n    return x","0ecabf68":"continuous_columns = ['CNT_CHILDREN','CNT_FAM_MEMBERS','DAYS_BIRTH','DAYS_EMPLOYED','CNT_FAM_MEMBERS']\ndef remove_outliers(data, column):\n    if column == 'DAYS_EMPLOYED':\n        Q1 = data[data[column]<=0][column].quantile(0.25)\n        Q3 = data[data[column]<=0][column].quantile(0.75)        \n        IQR = Q3-Q1\n        return data[((Q1-1.5*IQR <= data[column]) & (data[column] <= Q3+1.5*IQR)) | data[column]>0]\n    else:\n        Q1 = data[column].quantile(0.25)\n        Q3 = data[column].quantile(0.75)\n        IQR = Q3-Q1\n        return data[(Q1-1.5*IQR <= data[column]) & (data[column] <= Q3+1.5*IQR)]","5e62a70f":"before = application['CNT_FAM_MEMBERS'].skew()\nafter = np.log(application['CNT_FAM_MEMBERS']).skew()\nprint('Skewness coefficient')\nprint('CNT_FAM_MEMBERS ------')\nprint(f'Before: {before}')\nprint(f'After:  {after}')\n\nbefore = application['CNT_CHILDREN'].skew()\nafter = np.power(application['CNT_CHILDREN'],1\/7).skew()\nprint('CNT_CHILDREN ------')\nprint(f'Before: {before}')\nprint(f'After:  {after}')\n\nbefore = application['AMT_INCOME_TOTAL'].skew()\nafter = np.log(application['AMT_INCOME_TOTAL']).skew()\nprint('AMT_INCOME_TOTAL ------')\nprint(f'Before: {before}')\nprint(f'After:  {after}')\n\n# Only transform the ones < 0 (customers currently being employed)\nbefore = application.loc[application['DAYS_EMPLOYED']<0,'DAYS_EMPLOYED'].skew()\nafter = (-1*np.sqrt(-1*application.loc[application['DAYS_EMPLOYED']<0,'DAYS_EMPLOYED'])).skew()\nprint('DAYS_EMPLOYED ------')\nprint(f'Before: {before}')\nprint(f'After:  {after}')","bc20a9b9":"def transform_skewed_data(application):\n    x = application.copy()\n    \n    x.loc[:,'CNT_FAM_MEMBERS'] = np.log(x['CNT_FAM_MEMBERS'])\n    x.loc[:,'CNT_CHILDREN'] = np.power(x['CNT_CHILDREN'],1\/7)\n    x.loc[:,'AMT_INCOME_TOTAL'] = np.log(x['AMT_INCOME_TOTAL'])\n    x.loc[application['DAYS_EMPLOYED']<0,'DAYS_EMPLOYED']  = -1*np.sqrt(-1*x.loc[application['DAYS_EMPLOYED']<0,'DAYS_EMPLOYED'])\n    \n    return x","8f58ed58":"from sklearn.preprocessing import OneHotEncoder\ncategorical_columns = ['CODE_GENDER','FLAG_OWN_CAR', 'FLAG_OWN_REALTY','NAME_INCOME_TYPE','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS',\n                       'NAME_HOUSING_TYPE','OCCUPATION_TYPE']\n\ndef encode(features, encode_cols):\n    encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n    encoder.fit(application[encode_cols].dropna())\n    \n    x = features.copy().reset_index()\n    x = x.join(pd.DataFrame(encoder.transform(x[encode_cols])))\n    x = x.drop(categorical_columns,axis=1)\n    return x","e599c532":"def get_credit_status(credit):\n    group=credit.groupby('ID')\n    pivot_tb = credit.pivot(index = 'ID', columns = 'MONTHS_BALANCE', values = 'STATUS')\n    pivot_tb['open_month'] = group['MONTHS_BALANCE'].min()\n    pivot_tb['end_month'] = group['MONTHS_BALANCE'].max() \n    pivot_tb['ID'] = pivot_tb.index\n    pivot_tb = pivot_tb[['ID', 'open_month', 'end_month']]\n    pivot_tb['window'] = pivot_tb['end_month'] - pivot_tb['open_month'] \n    pivot_tb.reset_index(drop = True, inplace = True)\n    credit0 = credit.copy()\n    credit0 = pd.merge(credit0, pivot_tb, on = 'ID', how = 'left') \n    credit0=credit0[credit0['window']>=20]\n    credit0['status']=np.where((credit0['STATUS']=='2')| (credit0['STATUS']=='3')|(credit0['STATUS']=='4')|(credit0['STATUS']=='5'),1,0)\n    \n    return credit0","8e10ffb5":"from imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\ndef oversampling(X, y, sampling_strategy=0.5):\n\n    sm = SMOTE(sampling_strategy=sampling_strategy)\n    X, y = sm.fit_resample(X,y)\n    return X, y\n\ndef downsampling(X, y, sampling_strategy=0.5):\n    us = RandomUnderSampler(sampling_strategy=sampling_strategy)\n    X, y = us.fit_resample(X,y)\n    return X,y","caf30c1b":"def process_datasets(x_train, x_test, y_train, y_test, oversampling_strategy=0.5, down_sampling_strategy=0.5, transform_skewed = True, woe_cols = [], encode_cols = categorical_columns):\n    \"\"\"\n    This function is a wrapper function for all of the preproccessing steps\n    \n    - x_train: unprocessed train application dataset\n    _ x_test: unprocessed test application dataset\n    - y_train: unprocessed train credit dataset\n    - y_test : unprocessed test credit dataset\n    - oversampling_factor:  oversample the positive cases by this factor (because the current label method only has 3% as positive)\n    - down_sampling_amount: remove this amount of negative cases to balance positive\/negative cases\n    _ transform_skewed: if True, transform any skewed continuous data in the application datasset\n    \"\"\"\n    \n    # encode x\n    x_train = impute_occupation_type(x_train)\n    x_train = create_unemployed_column(x_train)\n    if transform_skewed:\n        x_train = transform_skewed_data(x_train)\n    \n    x_test = impute_occupation_type(x_test)\n    x_test = create_unemployed_column(x_test)\n    if transform_skewed:\n        x_test = transform_skewed_data(x_test)\n    \n    # encode y\n    y_train = get_credit_status(y_train)[['ID','status']]\n    y_test = get_credit_status(y_test)[['ID','status']]\n\n    y_train = y_train.groupby('ID').any().reset_index()\n    y_test = y_test.groupby('ID').any().reset_index()\n    \n    # encode x\n    encode_cols = encode_cols.copy()\n    for col in woe_cols:\n        x_train[col], x_test[col] = woe_transform(x_train[col], x_test[col], y_train['status']) \n        if col in encode_cols:\n            encode_cols.remove(col)\n\n    x_train = encode(x_train, encode_cols)\n    x_test = encode(x_test, encode_cols)\n    \n    # Merge x and y together to make sure the ids matches\n\n    merged_train = x_train.merge(y_train, on='ID')\n    merged_test = x_test.merge(y_test, on='ID')\n    \n    # remove outliers\n    for col in continuous_columns:\n        merged_train = remove_outliers(merged_train, col)\n    \n    x_train = merged_train.drop(['ID', 'status'],axis=1)\n    x_test = merged_test.drop(['ID', 'status'],axis=1)\n    y_train = merged_train['status']\n    y_test = merged_test['status']\n    \n    # oversampling\/downsampling\n    if oversampling_strategy:\n        x_train, y_train = oversampling(x_train, y_train, sampling_strategy=oversampling_strategy)\n    if down_sampling_strategy:\n        x_train, y_train = downsampling(x_train, y_train, sampling_strategy=down_sampling_strategy)\n    \n    return x_train, x_test, y_train, y_test","1233bf8a":"from sklearn.utils import shuffle\n\n# train\/val\/test\n# 64\/16\/20\ntrain_size = len(application)*64\/\/100\nval_size = len(application)*16\/\/100\ntest_size = len(application)- train_size - val_size\nfold_size = val_size\nprint(f'Train size: {train_size}, Validation size: {val_size}, Test size: {test_size}')","a201da0c":"application = shuffle(application)\n\ncv_application = application[:train_size+val_size].copy()\ntest_application = application[train_size+val_size:].copy()","884288c4":"from sklearn.linear_model import LogisticRegression\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow.keras.layers as layers\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n    \ndef initiate_model(model_name):\n    if model_name == 'LogisticRegression':\n        return LogisticRegression(max_iter=400)\n    elif model_name == 'KNN':\n        return KNeighborsClassifier(n_neighbors=900)\n    elif model_name == 'RandomForest':\n        return RandomForestClassifier(n_jobs=-1, max_samples=0.8, max_features='sqrt') \n    elif model_name == 'NeuroNetwork':\n        nn_model = keras.Sequential([layers.Dense(64, activation='relu'),\n                                     layers.Dense(128, activation='relu'),\n                                     layers.Dense(128, activation='relu'),\n                                     layers.Dense(1, activation='sigmoid')])\n\n        nn_model.compile(keras.optimizers.Adam(), keras.losses.BinaryCrossentropy(), metrics=[keras.metrics.BinaryAccuracy()])\n        \n        return nn_model\n    else:\n        raise ValueError('This model is not currently supported.')","c93582d7":"from sklearn.metrics import fbeta_score, accuracy_score, precision_score, recall_score, precision_recall_curve, auc\n\nf_beta = 0.5\n\ndef cross_validation(model_name, application, credit, epochs=10, oversampling_strategy=0.5, down_sampling_strategy=0.5, transform_skewed = True, woe_cols=[]):\n    \"\"\"\n    This function performs cross validation and acts as a wrapper function for preproccessing, fitting, and evaluating steps\n    \n    - model: machine learning model\n    - application: application dataset\n    - credit: credit dataset\n    - epochs: this only apply to the Neural Network, number of epochs to train\n    - oversampling_strategy:  oversample the positive cases so that postive\/negative = oversampling_strategy (because the current label method only has 3% as positive)\n    - down_sampling_strategy: remove negative cases to so that (negative-postive)\/positive = down_sampling_strategy\n    _ transform_skewed: if True, transform any skewed continuous data in the application datasset\n    - threshold: this only apply to the Neural Network, threshold for the decision boundary\n    \"\"\"\n    application = shuffle(application)\n    total_acc = 0\n    total_f05 = 0\n    total_precision = 0\n    total_recall = 0\n    models = list()\n    thresholds_list = list()\n    \n    for i in range(5):\n        model = initiate_model(model_name)\n        \n        x_train = application[:fold_size*i+1].append(application[fold_size*(i+1)-1:]).copy()\n        x_test = application[fold_size*i:fold_size*(i+1)].copy()\n\n        y_train = credit[credit['ID'].isin(x_train['ID'])].copy()\n        y_test = credit[credit['ID'].isin(x_test['ID'])].copy()\n    \n    \n        x_train, x_test, y_train, y_test = process_datasets(x_train, x_test, y_train, y_test, oversampling_strategy=oversampling_strategy, \n                                                            down_sampling_strategy=down_sampling_strategy, transform_skewed=transform_skewed, woe_cols=woe_cols)\n        \n        if str(type(model)) == \"<class 'tensorflow.python.keras.engine.sequential.Sequential'>\":\n            model.fit(x_train, y_train, epochs=epochs, verbose=0)\n        else:\n            model = model.fit(x_train, y_train)\n        \n        predictions = model.predict(x_test)\n        if str(type(model)) == \"<class 'tensorflow.python.keras.engine.sequential.Sequential'>\" or str(type(model)) == \"<class 'sklearn.linear_model._logistic.LogisticRegression'>\":\n            if str(type(model)) == \"<class 'tensorflow.python.keras.engine.sequential.Sequential'>\":\n                probs = predictions\n            else:\n                probs = model.predict_proba(x_test)[:,1]\n            \n            pre, rec, thresholds = precision_recall_curve(y_test, probs)\n            f = (1+np.power(f_beta,2)) * pre * rec \/ (np.power(f_beta,2)*pre + rec)\n            threshold = thresholds[np.argmax(f)]\n            thresholds_list.append(threshold)\n            predictions = probs >= threshold\n            print('Threshold:{} , F05: {}'.format(threshold, f[np.argmax(f)]))\n        \n        models.append(model)\n        total_acc = total_acc + accuracy_score(y_test, predictions)\n        total_f05 = total_f05 + fbeta_score(y_test, predictions, beta=0.5)\n        total_precision = total_precision + precision_score(y_test, predictions)\n        total_recall = total_recall + recall_score(y_test,predictions)\n    \n    if thresholds_list:\n        return total_acc\/5, total_f05\/5, total_precision\/5, total_recall\/5, models, thresholds_list\n    else:\n        return total_acc\/5, total_f05\/5, total_precision\/5, total_recall\/5, models","6ca9513e":"woe_columns = ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_INCOME_TYPE','NAME_EDUCATION_TYPE',\n               'NAME_FAMILY_STATUS','NAME_HOUSING_TYPE','FLAG_MOBIL','FLAG_WORK_PHONE','FLAG_PHONE','FLAG_EMAIL']\n\nacc, f05, precision, recall, models, lr_thresholds = cross_validation('LogisticRegression', cv_application, credit, oversampling_strategy=0.5, \n                                                       down_sampling_strategy=0.5, transform_skewed = True, woe_cols=woe_columns)","7ab0969b":"print(f'Acc: {acc}')\nprint(f'f05: {f05}')\nprint(f'Precision: {precision}')\nprint(f'Recall: {recall}')","402ead0b":"acc, f05, precision, recall, models = cross_validation('KNN', cv_application, credit, oversampling_strategy=0.5, down_sampling_strategy=0.5, transform_skewed = True)","a151d464":"print(f'Acc: {acc}')\nprint(f'f05: {f05}')\nprint(f'Precision: {precision}')\nprint(f'Recall: {recall}')","f0fd01b5":"acc, f05, precision, recall, models = cross_validation('RandomForest', cv_application, credit, oversampling_strategy=0.5, down_sampling_strategy=0.5, transform_skewed = True)","47a8d679":"print(f'Acc: {acc}')\nprint(f'f05: {f05}')\nprint(f'Precision: {precision}')\nprint(f'Recall: {recall}')","27e15418":"acc, f05, precision, recall, models, nn_thresholds = cross_validation('NeuroNetwork', cv_application, credit, epochs=10, oversampling_strategy=0.5, \n                                                        down_sampling_strategy=0.5, transform_skewed = True)","edf2200d":"print(f'Acc: {acc}')\nprint(f'f05: {f05}')\nprint(f'Precision: {precision}')\nprint(f'Recall: {recall}')","6c28ec31":"application = shuffle(application)\n\nx_train = cv_application.copy()\nx_test = test_application.copy()\n\ny_train = credit[credit['ID'].isin(x_train['ID'])].copy()\ny_test = credit[credit['ID'].isin(x_test['ID'])].copy()\n\nwoe_columns = ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_INCOME_TYPE','NAME_EDUCATION_TYPE',\n               'NAME_FAMILY_STATUS','NAME_HOUSING_TYPE','FLAG_MOBIL','FLAG_WORK_PHONE','FLAG_PHONE','FLAG_EMAIL']\n\nx_train, x_test, y_train, y_test = process_datasets(x_train, x_test, y_train, y_test, oversampling_strategy=0.5, \n                                                    down_sampling_strategy=0.5, transform_skewed = True, woe_cols=woe_columns)","79f459d9":"model = initiate_model('LogisticRegression')\nmodel.fit(x_train, y_train)\n\nprobs = model.predict_proba(x_test)[:,1]\n\n# if using Logistic Regression or NN, use the average threshold from CV\nthreshold = sum(lr_thresholds)\/len(lr_thresholds)\npredictions = probs >= threshold\n\nprint(f'Acc: {accuracy_score(y_test, predictions)}')\nprint(f'f05: {fbeta_score(y_test, predictions, beta=0.5)}')\nprint(f'Precision: {precision_score(y_test, predictions)}')\nprint(f'Recall: {recall_score(y_test,predictions)}')","e16f3db0":"# **Cross Validation**","507ad29d":"# **Testing**","c72e5e40":"* In this notebook, we performed different feature engineering, transformations, over\/downsampling methods to train a model to classify good\/bad credits with the provided unbalanced dataset.\n* For mode selection, we use f05 score as the main criteria to deal with this unbalanced data.\n* From the 4 models we tried to train above, Logistic Regression, KNN, and RandomForest produce similar results for cross validation. Random Forest has lower f05 score than others. For potential future needs for inferencing, we select the fitted Logistic Regression as the final model. Then, we run the Logistic Regression with the test set and it produces a similar result to its CV f05 score, which is about 0.8.\n* WOE transformation didn't show much effect on the Logistic Regression, probably because we only performed transformation on categorical variable with cardinality lower than 20, left out the ones with high cardinality and continuous variable.","070331be":"This is the best model chosen from model selection","e3bfad3b":"OCCUPATION TYPE","7c99a217":"# **Models Selection**","4684d90f":"> **KNN**","b75e3b18":"> **Logistic Regression**","751001f4":"Remove outliers","4b5dee6a":"DAYS_EMPLOYED","3a847ac9":"Encode dataset","081ee246":"# **Conclusion**","db6ea6c4":"# **Feature Engineering**","40a25342":"Split cross-validation\/test","b9a56f36":"> **Random Forest**","4ebb25e6":"> **Neuronetwork**\n","ada052b5":"# **Labels**","22117b15":"Because jobs in OCCUPATION_TYPE are generic, they will be less prone to overfit. Besides,dropping any value may cause a noticable loss of information.","fc5d585b":"# **Acknowledgements**\n*  https:\/\/www.kaggle.com\/rikdifos\/eda-vintage-analysis\n*  https:\/\/www.listendata.com\/2015\/03\/weight-of-evidence-woe-and-information.html","c1795fc4":"I'm going to use WOE transformation with Logistic Regression model to see if it can help improve the result. WOE transformation helps secure monotonic relationship between independent variable and dependent variable.\n\nhttps:\/\/www.listendata.com\/2015\/03\/weight-of-evidence-woe-and-information.html","cc6b18f4":"# **Processing Data**","92823176":"From https:\/\/www.kaggle.com\/rikdifos\/eda-vintage-analysis, we choose to discard all records that last for less than 20 months to reduce noises (too short obeservation window won't be able to show the behaviour of the customers).Any customers that default for 60 days or more are labeled as bad customers. Otherwise, they are labeled as good.","853c17ef":"# **WOE**","519c708c":"Transform Skewed Data"}}