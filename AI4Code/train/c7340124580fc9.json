{"cell_type":{"57b31eea":"code","0f2b1a73":"code","55da0b56":"code","f038a115":"code","64671396":"code","e7452ff0":"code","fe579e01":"code","ed920941":"code","f28914ca":"code","950e29ab":"code","261d22b0":"code","d7c6cb24":"code","84560af8":"code","70d35d67":"code","4ffef7aa":"code","7c3c1ab5":"code","3db1624b":"code","7741ffc4":"code","fed04b8d":"code","20ff3689":"code","54dfe49a":"code","19d16f2d":"code","d1ad5292":"code","aa8f4498":"code","108e6f9d":"markdown","5338b5b9":"markdown","089d8871":"markdown","43b3abcd":"markdown","06d2b28f":"markdown"},"source":{"57b31eea":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0f2b1a73":"class LDA:\n    \n    # Compare LDA with my PCA graph and see what is the difference between two\n    # number_of_important_feature is the component axes in mathematical terms\n    def __init__(self, number_of_important_features=2):\n        self.number_of_important_features=number_of_important_features\n        self.LDs=None\n    \n    def fit(self, X,y):\n        feature_count=X.shape[1]\n        # Getting unique classes in y\n        type_of_class_in_y=np.unique(y)\n        # Calculating mean of all samples\n        mean_all=np.mean(X,axis=0)\n        # Initialising with zeros these below matrix\n        separation_within_class=np.zeros((feature_count,feature_count))\n        separation_between_class=np.zeros((feature_count,feature_count))\n        # Iterating over each type of unique classes of y\n        for c in type_of_class_in_y:\n            X_of_each_class=X[y==c]\n            # Calculating the mean of each unique class\n            mean_of_each_class=np.mean(X_of_each_class,axis=0)\n            # Calculating separation within class(squared) and summing over it\n            separation_within_class=separation_within_class+np.dot((X_of_each_class-mean_of_each_class).T,(X_of_each_class-mean_of_each_class))\n            # Calculating difference between mean of each class with mean of overall samples\n            mean_difference_with_overall_mean=(mean_of_each_class-mean_all).reshape(feature_count,1)\n            # Calculating and summing over separation between classes\n            separation_between_class=separation_between_class+(X.shape[0]*np.dot(mean_difference_with_overall_mean,mean_difference_with_overall_mean.T))\n            # calculating these formula (d1(squared)+d2(squared)+d3(squared)..)\/s1(squared)+s2(squared)+s3(squared)\n            # separation_within_class(inverse)xseparation_between_class==>mat_trans\n            mat_trans=np.dot(np.linalg.inv(separation_within_class),separation_between_class)\n            # Same as PCA \n            # Refer to PCA for explanation and dimensions\n            # Link https:\/\/www.kaggle.com\/ankan1998\/pca-from-scratch\n            # Details on Eigenvectors\n            # For more resources visit https:\/\/www.kaggle.com\/getting-started\/176613\n            eigenvalues,eigenvector=np.linalg.eig(mat_trans)\n            eigenvector=eigenvector.T\n            indexs=np.argsort(eigenvalues)[::-1]\n            eigenvector=eigenvector[indexs]\n            eigenvalues=eigenvalues[indexs]\n            self.LDs=eigenvector[:self.number_of_important_features]\n            print(indexs)\n            \n    def apply(self,X):\n        # Projecting on New Axis\n        return np.dot(X,self.LDs.T)\n    \n        \n        ","55da0b56":"dataset=pd.read_csv(\"..\/input\/wine-pca\/Wine.csv\")","f038a115":"dataset.head()","64671396":"dataset.isnull().sum()","e7452ff0":"dataset=dataset.sample(frac=1)","fe579e01":"len(dataset)","ed920941":"X=dataset.iloc[:,:-1]\ny=dataset.iloc[:,-1]","f28914ca":"# Splitting\nX_train=dataset.iloc[:150,:-1]\nX_test=dataset.iloc[150:,:-1]\ny_train=dataset.iloc[:150,-1]\ny_test=dataset.iloc[150:,-1]","950e29ab":"# Standardizing\nfrom sklearn.preprocessing import StandardScaler\nX = StandardScaler().fit_transform(X)\nprint(X[0:5])","261d22b0":"lda=LDA(2)","d7c6cb24":"lda.fit(X,y)","84560af8":"# Projecting\nprojected=lda.apply(X)","70d35d67":"x0=projected[:,0]\nx1=projected[:,1]","4ffef7aa":"plt.scatter(x0,x1,c=y)","7c3c1ab5":"import seaborn as sns","3db1624b":"sns.kdeplot(x0,x1,shade=True,cmap=\"Purples_d\",cbar=True)","7741ffc4":"from sklearn.neighbors import KNeighborsClassifier","fed04b8d":"knn=KNeighborsClassifier(3)","20ff3689":"knn.fit(X_train,y_train)","54dfe49a":"pred=knn.predict(X_test)","19d16f2d":"from sklearn.metrics import confusion_matrix","d1ad5292":"cn=confusion_matrix(y_test,pred)","aa8f4498":"cn","108e6f9d":"### loading Iris Dataset\n#### 4 features and 1 output","5338b5b9":"### Apply LDA","089d8871":"#### This kernel density distribution tells about the probablity density of two features","43b3abcd":"## Refer to PCA for explanation and dimensions\n## Link https:\/\/www.kaggle.com\/ankan1998\/pca-from-scratch\n## Details on Eigenvectors\n## For more resources visit https:\/\/www.kaggle.com\/getting-started\/176613","06d2b28f":"## LDA \n#### Supervised Dimensionality Reduction Technique\n###### It increases class separability within class"}}