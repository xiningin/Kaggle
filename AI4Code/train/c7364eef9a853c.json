{"cell_type":{"0ea40e1c":"code","bd3ff004":"code","2610e3c3":"code","6f6d3263":"code","b548dd00":"code","7e2987eb":"code","1f90376f":"code","98abec95":"code","ae04aeb0":"code","3c12acab":"code","91dd3f47":"code","191efb91":"code","9e7b171f":"code","fa3cc3b0":"code","2be35b25":"code","a6c392d9":"code","80e111d2":"code","bc006cef":"code","d462b025":"code","1a39fa1d":"code","37620964":"code","4123b844":"code","86bc2d4d":"code","c816b29e":"code","2e2bcd64":"code","ea630467":"code","6e4e066c":"markdown","b47c49e0":"markdown","3628759a":"markdown","0fec1ac8":"markdown","df8de180":"markdown"},"source":{"0ea40e1c":"!pip install -q bpe","bd3ff004":"import numpy as np\nimport pandas as pd\nimport os,sys\nimport glob\nimport re\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport matplotlib\n%matplotlib inline\nfrom matplotlib import pyplot as plt\n\nimport torchtext\nimport nltk\nfrom torch.nn.utils.rnn import pack_sequence,pad_sequence,pack_padded_sequence,pad_packed_sequence\nimport random\nfrom sklearn.metrics import mean_absolute_error,log_loss\n\nimport multiprocessing\n\n\nimport time\nfrom bpe import Encoder\n\ndata_path = \"..\/input\/sentencecorpus\/SentenceCorpus\/labeled_articles\"\narticle_files = glob.glob(data_path+'\/*')\nprint(len(article_files),' article files found')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","2610e3c3":"def clean_line(line):\n    line = re.sub(r'\\n','',line)\n    return line\n\ndef remove_lines(article): #remove lines that start with ###\n    ar = [ None if re.match('^\\s*###.*',line) else line    for line in article ]\n    ar = list(filter(None,ar))\n    return ar\n\ndef read_file(filename):\n    try:\n        with open(filename) as f:\n            return list(map(clean_line,f.readlines()))\n    except Exception as e:\n        print(e)\n\ndef article_to_pd(article,columns=['Category','Sentence']):\n    try:\n        article = [ re.match('^([A-Z]+)\\s*(.*)',line).groups() for line in article ]\n        return pd.DataFrame(article,columns=columns)\n    except Exception as e:\n        print(e,article)\n\ndef load_article_sentences(article_files):\n    articles = list(map(read_file,article_files))\n    articles = list(map(remove_lines,articles))\n    pd_articles = pd.concat(list(map(article_to_pd,articles)),ignore_index=True)    \n    return pd_articles\n\nclass PadSequence:\n    def __call__(self, batch):\n        sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)\n        sequences = [x[0] for x in sorted_batch]\n        sequences_padded = pad_sequence(sequences, batch_first=True)\n        lengths = torch.LongTensor([len(x) for x in sequences])\n        labels = torch.LongTensor(map(lambda x: x[1], sorted_batch))\n        return sequences_padded, lengths, labels\n\nclass bert_enc:\n  def __init__(self):\n    self.load()\n  \n  def fit(self,sentences):\n        pass #for uniformity purposes\n    \n  def load(self):\n    self.tokenizer = torch.hub.load('huggingface\/pytorch-pretrained-BERT', 'bertTokenizer', 'bert-base-cased', do_basic_tokenize=False)\n    self.model = torch.hub.load('huggingface\/pytorch-pretrained-BERT', 'bertModel', 'bert-base-cased')\n    self.model.eval()\n  \n  def enc_sent(self,sentence,index=-1):\n    sep_token = ' [SEP] '\n    sentence = sep_token+sentence+sep_token\n    \n    indexed_tokens = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(sentence))\n    tokens_tensor = torch.tensor([indexed_tokens])\n    \n    with torch.no_grad():\n      encoded_layers, _ = self.model(tokens_tensor)\n    \n    return encoded_layers[index]\n  \n  def transform(self,doc,index=-1):\n    sentences = nltk.sent_tokenize(doc)\n    encs = []\n    for sentence in sentences:\n      encs.append(self.enc_sent(sentence,index=-1))\n    if len(encs)==1:\n        encs = encs[0]\n    return encs# torch.cat(encs,dim=0)\n\n\ndef get_imp_wc(senteces_lst,important_words):\n    def get_wc(sent):\n        fd = dict(nltk.FreqDist(nltk.word_tokenize(sent)))\n        fd_pd = pd.DataFrame(fd,index=[0])\n        common = fd_pd[list(set(fd_pd.columns)&set(important_words))]\n        return common\n\n    pool = multiprocessing.Pool()\n    map_func = [pool.map,map][1]\n    print('Getting word frequencies')\n    wcs = list(map_func(get_wc,senteces_lst))\n    pool.close()\n    print('Merging dataframes')\n    return pd.concat(wcs,axis=0,ignore_index=True,sort=False)\n\ndef wc_name(filename):\n    return '.'.join(os.path.basename(filename).split('.')[:-1])\n\n\ndef get_imp_word_count_data(sentences,word_lists_file=\"..\/input\/sentencecorpus\/SentenceCorpus\/word_lists\"):\n    files = glob.glob(word_lists_file+'\/*')\n    word_types = dict(zip(list(map(wc_name,files)),list(map(read_file,files))))\n    data_df = {}\n    for d in word_types:\n        data_df[d] = [word_types[d]]\n    data_df = pd.DataFrame(data_df).drop(['stopwords'],axis=1)\n    important_words = []\n    for c in data_df.columns:\n        important_words = important_words + data_df.loc[0,c]\n\n    important_words = list(set(important_words))\n    print('Found ',len(important_words),'important words')\n    \n    sentences_wc = list(sentences.Sentence)\n    t1 = time.time()\n    imp_word_count_data = get_imp_wc(sentences_wc,important_words).fillna(0)\n    imp_word_count_data['Category'] = sentences.Category\n    print(time.time()-t1)\n    return imp_word_count_data\n\ndef split_labelled_sentences(sentences_data,device='cpu',encoder_to_use='bpe',pack=True):\n    def enc_sentence(sentence):\n        if encoder_to_use=='bpe':\n            return torch.unsqueeze(torch.tensor(next(encoder.transform([sentence])),dtype=torch.float),dim=1)\n        return encoder.transform(sentence)\n    \n    encoders = {'bpe':Encoder,'BERT':bert_enc}\n    encoder = encoders[encoder_to_use]()\n    encoder.fit(sentences.Sentence)\n    t1 = time.time()\n    X = list(map(enc_sentence,list(sentences_data.Sentence)))\n    y = sentences.Category\n    print(time.time()-t1,'Seconds for encoding sentences using ',encoder_to_use)\n    \n    if encoder_to_use=='BERT':\n        X = list(map(lambda t:torch.squeeze(torch.tensor(t)),X))\n    \n    train_X,test_X,train_y,test_y = train_test_split(X,y)\n    print('Lengths: train_X,train_y,test_X,test_y:',list(map(len,[train_X,train_y,test_X,test_y])))\n    \n    maxlen_found = torch.tensor(max(list(map(len,X)))).to(device)\n    \n    if pack:\n        train_X = pack_sequence(sorted(train_X,key=len,reverse=True)).to(device)\n        train_y = torch.tensor(list(train_y)).to(device)\n\n        test_X = pack_sequence(sorted(test_X,key=len,reverse=True)).to(device)\n        test_y = torch.tensor(list(test_y)).to(device)\n    \n    return train_X,train_y,test_X,test_y,maxlen_found","6f6d3263":"sentences = load_article_sentences(article_files)\nprint('Number of sentences ',len(sentences))\n\nle = LabelEncoder()\nle.fit(list(set(sentences.Category)))\nsentences.Category = le.transform(list(sentences.Category))\noutput_size = len(le.classes_)\nprint('classes',le.classes_)\n\nsentences.head()","b548dd00":"imp_word_count_data = get_imp_word_count_data(sentences)\nimp_word_count_data.head()","7e2987eb":"len(imp_word_count_data.columns)","1f90376f":"from xgboost import XGBRegressor\nwc_X = imp_word_count_data.drop(['Category'],axis=1).values\nwc_y = imp_word_count_data[['Category']].values\nwc_train_X,wc_test_X,wc_train_y,wc_test_y = train_test_split(wc_X,wc_y)\nxgb_model = XGBRegressor(n_estimators=10000,learning_reate=0.01,n_jobs=10,max_depth=6)\nxgb_model.fit(wc_train_X,wc_train_y,verbose=False,early_stopping_rounds=100, eval_set=[(wc_test_X, wc_test_y)])","98abec95":"predictions = xgb_model.predict(wc_X)\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, wc_y)))\nns = sentences.copy()\nns.Category = le.inverse_transform(list(ns.Category))\nns['Category_pred'] = le.inverse_transform(list(map(lambda v:max(0,int(np.round(v).tolist())),predictions))).tolist()\nns.head()","ae04aeb0":"list(range(1,2))","3c12acab":"class SCFC(nn.Module):\n    def __init__(self,input_size,hidden_size,output_size,num_layers=2):\n        super(SCFC,self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n        self.fcs = []\n        if num_layers<2:\n            hidden_size = output_size\n        self.fcs.append(nn.Linear(input_size,hidden_size))\n        \n        for i in range(1,num_layers):\n            self.fcs.append(nn.Linear(hidden_size,hidden_size))\n            \n        if num_layers>1:\n            self.fcs.append(nn.Linear(hidden_size,output_size))\n        self.lsfmax = nn.LogSoftmax(dim=1)\n    \n    def forward(input):\n        output=input\n        for i in range(0,self.num_layers):\n            output = F.relu(self.fcs[i](output))\n        output = self.lsfmax(output)\n        return output","91dd3f47":"device = torch.device('cuda')\nscnn = SCFC(len(imp_word_count_data.columns),100,len(le.classes_),2).to(device)","191efb91":"def train(model,train_X,train_y,iters=100,lr=0.1):\n    opz = optim.Adam(model.parameters(),lr=lr)\n    loss_fn = nn.NLLLoss\n    for i in iters:\n        loss+=loss_fn(model(train_X),train_y)\n        loss.backward()\n        opz.step()\n        print(loss.item())","9e7b171f":"train(scnn,wc_train_X,wc_train_y)","fa3cc3b0":"class RecurrentCell(nn.Module):\n    def __init__(self,input_size,hidden_size,output_size):\n        super(RecurrentCell,self).__init__()\n        self.input_size=input_size\n        self.hidden_size=hidden_size\n        self.rnn = nn.GRU(input_size,hidden_size, batch_first=True)\n        self.LogSoftmax = nn.LogSoftmax(dim=1)\n        self.output_fc = nn.Linear(hidden_size,output_size)\n        \n    def forward(self,batch):\n#         x, x_lengths, _ = batch\n#         x_pack = pack_padded_sequence(x, x_lengths, batch_first=True)\n        output,hidden = self.rnn(batch)\n        hidden = hidden.squeeze()\n        result = self.LogSoftmax(self.output_fc(hidden))\n        return result\n    \n    def init_hidden(self,shape):\n        return torch.rand(shape)\n\ndef train(device, model,train_X,train_y,test_X,test_y, iters=100,lr=0.01,print_every=None):\n    print_every = int(iters\/10) if print_every is None else print_every\n    opz = optim.Adam(model.parameters(),lr=lr)\n    loss_func = nn.NLLLoss().to(device)\n    train_losses = []\n    test_losses = []\n    model.zero_grad()\n    opz.zero_grad()\n    \n    print('Iters  Train Loss       Test Loss')\n    for i in range(1,iters+1):\n        yhat = model(train_X)\n        loss = loss_func(yhat,train_y)\/maxlen_found\n        loss.backward()\n        opz.step()\n        \n        trnl = round(loss.cpu().item(),4)\n        tstl = round(test(device,model,test_X,test_y),4)\n        if i%print_every==0 or i==1:\n            print(str(i).rjust(3,'0'),'\\t',trnl,'\\t',tstl)\n        train_losses.append(trnl)\n        test_losses.append(tstl)\n\n\n    plt.plot(train_losses)\n    plt.plot(test_losses)\n    plt.legend(['train loss','test loss'])\n\ndef test(device,model,test_X,test_y):\n    loss_func = nn.NLLLoss().to(device)\n    with torch.no_grad():\n        yhat = model(test_X)\n        loss = loss_func(yhat,test_y)\/maxlen_found\n        return loss.item()\n    \ndef num_words(sentence):\n    return len(nltk.word_tokenize(sentence))\n\n\ndef evaluate(model,data):#,device=None):\n    #device = torch.device('cpu') if device is None else device\n    #model = model.to(device)\n    tx = pack_sequence(sorted(data,key=len,reverse=True)).to(device)\n    with torch.no_grad():\n        yhat = model(tx)\n        yh = []\n        for t in yhat:\n            val, index = t.max(0)\n            yh.append(index.item())\n        return yh","2be35b25":"input_size = 1 #if encoder_to_use=='bpe' else 768\nhidden_size = 5\n\ntrain_X,train_y,test_X,test_y,maxlen_found = split_labelled_sentences(sentences,device,encoder_to_use='bpe')\nmaxlen_found\/=10","a6c392d9":"model1 = RecurrentCell(input_size,hidden_size,output_size).to(device)\ntrain(device,model1,train_X,train_y,test_X,test_y, iters=10000,lr=0.0001)","80e111d2":"!pip install -q syft","bc006cef":"import syft as sy\nfrom syft.frameworks.torch.pointers import PointerTensor\nfrom syft.frameworks.torch.tensors.decorators import LoggingTensor","d462b025":"def create_worker_objs(num_workers,hooks=None):\n    workers = []\n    if hooks is None:\n        hooks = [ sy.TorchHook(torch) for _ in range(num_workers) ]\n    workers = [sy.VirtualWorker(hooks[i],id=\"worker\"+str(i+1)) for i in range(num_workers)]\n    return workers\n\ndef train_workers(model,workers,iters,optimizer,lr):\n    print_every = int(iters\/10)\n#     with tqdm(total=iters*len(workers)) as pbar:\n    with tqdm(total=iters) as pbar:\n        for j in range(iters):\n            send_model(model,workers)\n            for i in workers:\n                workers[i]['opz'] = optimizer(workers[i]['model'].parameters(),lr)\n                workers[i]['opz'].zero_grad()\n                pred = workers[i]['model'](workers[i]['data']['train_X'])\n                workers[i]['loss'] = F.nll_loss(pred,workers[i]['data']['train_y'])\n                workers[i]['loss'].backward()\n                workers[i]['opz'].step()\n            pbar.update(1)\n            pbar.set_description(str(round(workers[i]['loss'].cpu().item(),4)).rjust(6))\n            #if j%print_every==0:\n                #print(i, workers[i]['loss'].cpu().item())\n            weights = aggregate_weights(model,workers)\n            model.load_state_dict(weights) #update the model at end of each iteration\n    return model\n\ndef get_params(worker_item):\n    return list(worker_item[1]['model'].named_parameters())\n\ndef aggregate_weights(model,workers,eta=1):\n    model_parameters_lst = list(map(get_params,workers.items()))\n    num_params = len(model_parameters_lst[0])\n    num_workers = len(workers)\n    reference_params = list(model.named_parameters())\n    \n    avg_params = [] \n    for i in range(num_params):\n        avg_params.append([model_parameters_lst[0][i][0],torch.zeros_like(model_parameters_lst[0][i][1])])\n\n    for i in range(num_params):\n        for j in range(num_workers):\n            avg_params[i][1].add_(model_parameters_lst[j][i][1]-reference_params[i][1])\n    \n    for i in range(num_params):\n        avg_params[i][1] = torch.div(avg_params[i][1],num_workers*eta)\n        \n    return dict(avg_params)\n#     model.load_state_dict(dict(avg_params))\n#     return model\n\ndef send_model(model,workers):\n    for w in workers:\n        workers[w]['model'] = model.copy()#.send(workers[w]['obj'])\n\ndef model_params_diff(params1,params2,num_params):\n    diffs = []\n    for i in range(num_params):\n        diffs.append(params2[i]-params1[i])\n    return diffs\n\ndef test_model(model,test_X,test_y,device=None):\n    device = torch.device('cpu') if device is None else device\n    loss_func = nn.NLLLoss().to(device)\n    tx = pack_sequence(sorted(test_X,key=len,reverse=True)).to(device)#.send(worker_objs[i])\n    ty = torch.tensor(list(test_y)).to(device)#.send(worker_objs[i])\n    with torch.no_grad():\n        yhat = model(tx)\n        yh = []\n        for t in yhat:\n            val, index = t.max(0)\n            yh.append(index.item())\n        loss = loss_func(yhat,ty)\n        return list(map(lambda v:round(v,4),[loss.item(),100*accuracy_score(test_y,yh)]))\n\n        \n\n# @sy.func2plan()\n# def train_worker(woker,iters):\n#     model,data,opz = workers['worker1']['model'],workers['worker1']['data'],workers['worker1']['model']\n#     opz.zero_grad()\n#     train_X = pack_sequence(sorted(data['train_X'],key=len,reverse=True))\n#     train_y = torch.tensor(list(data['train_y']))\n#     for i in range(iters):\n#         pred = model(train_X)\n#         loss = F.nll_loss(pred,train_y)\n#         loss.backward()\n#         opz.step()\n#         if i%print_every==0:\n#             print(loss.item())\n","1a39fa1d":"device = torch.device('cpu')\ntrain_X,train_y,test_X,test_y,maxlen_found = split_labelled_sentences(sentences,device,encoder_to_use='bpe',pack=False)","37620964":"train_y,test_y = list(train_y),list(test_y)","4123b844":"train_y[0:116]","86bc2d4d":"num_workers = 20\nlr = 0.0001\ndata_share = int(len(train_X)\/num_workers)\nworker_objs = create_worker_objs(num_workers)\noptimizer = optim.Adam\nmodel2 = RecurrentCell(input_size,hidden_size,output_size)#.to(device)\n\nworkers = {}\nfor i in range(num_workers):\n    tx = pack_sequence(sorted(train_X[i*data_share:(i+1)*data_share],key=len,reverse=True))#.send(worker_objs[i])\n    ty = torch.tensor(list(train_y[(i*data_share):((i+1)*data_share)]))#.send(worker_objs[i])\n    worker_data = {'train_X':tx,'train_y':ty}\n    workers[\"worker\"+str(i+1)] = {'obj':worker_objs[i],'data':worker_data}","c816b29e":"workers[random.choice(list(workers.keys()))]","2e2bcd64":"model2 = train_workers(model2,workers,10,optimizer,lr=0.0001)","ea630467":"test_model(model1,test_X,test_y,device),test_model(model2,test_X,test_y)","6e4e066c":"# Data preparation","b47c49e0":"# Using Fully connected nn","3628759a":"# Trees and Forests","0fec1ac8":"# Federated learning","df8de180":"# Using recurrent networks"}}