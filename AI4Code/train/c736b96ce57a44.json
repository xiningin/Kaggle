{"cell_type":{"67fc715a":"code","9554f493":"code","95fbad95":"code","df128f96":"code","d932d6d4":"code","04e212da":"code","361812a6":"code","d0282177":"code","33a939fe":"code","8d2e7eee":"code","70e6bbe5":"code","dcea8c61":"code","5cb47347":"code","b8a36041":"code","7ef36d9d":"code","8cabbd30":"code","f686d23f":"code","0bcbcabd":"code","38cc3592":"code","58de7823":"markdown","37edb2f2":"markdown","81188afe":"markdown","6e2d5840":"markdown","7b2412f0":"markdown","0ebcc9b7":"markdown","9a2d80ff":"markdown","f861b709":"markdown"},"source":{"67fc715a":"# Importing Libraries\nimport pandas as pd # Intermediate DS\nimport numpy as np # Scientific Operations\nimport copy\n\nfrom sklearn import preprocessing # Preprocesing library for Encoding, etc.\nfrom sklearn.model_selection import KFold # Segregating the train data to fragments for predorming Cross Validation\nfrom sklearn.model_selection import train_test_split # For splitting train and test data\nfrom sklearn.metrics import mean_squared_error # For Calculating Mean Squared Error\nimport xgboost as xgb # Light GBM Regressor\n\nimport optuna # For Hyper Parameter Tuning\nfrom functools import partial\nimport multiprocessing\n\nfrom datetime import datetime # For storing date and timers for operations\n\nimport seaborn as sns # Plotting data\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')","9554f493":"# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\n# Preview the data\ntrain.head()\ntest.head()","95fbad95":"y_train = train.target\nX_train = train.drop(['target'], axis=1)\nX_test = test.copy()\n\n# Preview features\nX_train.head()","df128f96":"# Extract the Categorical Columns\ncat_cols = [feature for feature in X_train.columns if 'cat' in feature]\nprint(cat_cols)\nlow_cardinality_cols = [col for col in cat_cols if X_train[col].nunique() < 8]\nprint(low_cardinality_cols)\nhigh_cardinality_cols = list(set(cat_cols) - set(low_cardinality_cols))\nprint(high_cardinality_cols)\n\n# Copy of original data to prevent overwwritting them\nlabel_X_train = X_train.copy()\nlabel_X_test = X_test.copy()\n\nOH_encoder = preprocessing.OneHotEncoder(handle_unknown='ignore', sparse=False)\n\nOH_cols_X = pd.DataFrame(OH_encoder.fit_transform(label_X_train[low_cardinality_cols]))\nOH_cols_X_test = pd.DataFrame(OH_encoder.fit_transform(label_X_test[low_cardinality_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_X.index = label_X_train.index\nOH_cols_X_test.index = label_X_test.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X = label_X_train.drop(low_cardinality_cols, axis=1)\nnum_X_test = label_X_test.drop(low_cardinality_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nlabel_X_train = pd.concat([num_X, OH_cols_X], axis=1)\nlabel_X_test = pd.concat([num_X_test, OH_cols_X_test], axis=1)","d932d6d4":"ord_encoder = preprocessing.OrdinalEncoder()\n\nlabel_X_train[high_cardinality_cols] = ord_encoder.fit_transform(label_X_train[high_cardinality_cols])\nlabel_X_test[high_cardinality_cols] = ord_encoder.fit_transform(label_X_test[high_cardinality_cols])","04e212da":"label_X_train.head()","361812a6":"y_train.head()","d0282177":"label_X_test.head()","33a939fe":"!nvidia-smi","8d2e7eee":"corrmat = label_X_train.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n# Plot heat map\ng=sns.heatmap(label_X_train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","70e6bbe5":"def objective(trial, X, y):\n    train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.2)\n    \n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.15, log=True)\n    reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n    reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n    subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n    max_depth = trial.suggest_int(\"max_depth\", 1, 7)\n    n_estimators = trial.suggest_int(\"n_estimators\", 1, 1000)\n\n\n    xgb_regressor = xgb.XGBRegressor(tree_method=\"gpu_hist\",\n                                     gpu_id=0,\n                                     predictor=\"gpu_predictor\",\n                                     n_estimators=n_estimators,\n                                     learning_rate=learning_rate,\n                                     reg_lambda=reg_lambda,\n                                     reg_alpha=reg_alpha,\n                                     subsample=subsample,\n                                     colsample_bytree=colsample_bytree,\n                                     max_depth=max_depth)\n    \n    xgb_model = xgb_regressor.fit(train_x, train_y, early_stopping_rounds=250, eval_set=[(valid_x, valid_y)], verbose=1000)\n    \n    train_score = np.round(mean_squared_error(train_y, xgb_model.predict(train_x), squared=False), 5)\n    test_score = np.round(mean_squared_error(valid_y, xgb_model.predict(valid_x), squared=False), 5)\n    \n    print(f'TRAIN RMSE : {train_score} || TEST RMSE : {test_score}')\n    \n    return test_score","dcea8c61":"optimize = partial(objective, X=label_X_train, y=y_train)\nstudy_xgb = optuna.create_study(direction='minimize')\nstudy_xgb.optimize(optimize, n_trials=500)","5cb47347":"best_para = study_xgb.best_params\nprint(best_para)","b8a36041":"hist = study_xgb.trials_dataframe()\nhist.head()","7ef36d9d":"optimized_params={\n    'learning_rate': 0.12620129660954435, \n    'reg_lambda': 0.09466950885165284, \n    'reg_alpha': 1.849292841634709e-05, \n    'subsample': 0.5984327124580388, \n    'colsample_bytree': 0.1935161355279864, \n    'max_depth': 3, \n    'n_estimators': 977}","8cabbd30":"split = KFold(n_splits=10, random_state=91, shuffle=True)\npreds_list_final = []\nrmse_list = []\n\nsplit_index=1\nfor train_idx, val_idx in split.split(label_X_train):\n    X_tr = label_X_train.iloc[train_idx]\n    X_val = label_X_train.iloc[val_idx]\n    y_tr = y_train.iloc[train_idx]\n    y_val = y_train.iloc[val_idx]\n    \n    params = copy.deepcopy(optimized_params)\n    \n    xgb_regressor_optimized = xgb.XGBRegressor(**params, \n                                               tree_method=\"gpu_hist\",\n                                               gpu_id=0,\n                                               predictor=\"gpu_predictor\")\n    xgb_model = xgb_regressor_optimized.fit(X_tr, y_tr,\n                                            verbose = False,\n                                            eval_set = [(X_val, y_val)],\n                                            eval_metric = \"rmse\",\n                                            early_stopping_rounds = 250)\n    print(f'RMSE for split no: {str(split_index)} for base model is {mean_squared_error(y_val, xgb_model.predict(X_val), squared=False)}')\n    rmse_list.append(mean_squared_error(y_val, xgb_model.predict(X_val), squared=False))\n    preds_list_final.append(xgb_model.predict(label_X_test))\n    split_index+=1\n    \nprint('Overall RMSE: %s' %(str(sum(rmse_list)\/len(rmse_list))))","f686d23f":"y_preds_final = np.array(preds_list_final).mean(axis=0)\ny_preds_final","0bcbcabd":"print(y_preds_final)\nprint(len(y_preds_final))","38cc3592":"# Save the predictions\nsubmission = pd.DataFrame({'id':label_X_test.index,\n                           'target':y_preds_final})\nsubmission.to_csv('submission.csv', index=False)","58de7823":"# The next section helps to identify the categorical columns and treat those specific columns by using Ordinal Encoder\nStep 1: Perform One Hot Encoding for low cardinality columns\nStep 2: Perform Ordinal Encoding for high cardinlaity columns","37edb2f2":"# Step 2: Load the data\n\nNext, we'll load the training and test data.  ","81188afe":"# Step 3: Optimize the XG Boost Classifier","6e2d5840":"# Step 4: Setup Optimized XG Boost Params and Run a new Regressor on top of Optuna Objective function\nRun the light gbm classifier with optimized hyper-parameters","7b2412f0":"![image.png](attachment:2f3fb86d-06f3-41a4-bd97-80920dda3593.png)","0ebcc9b7":"# Step 5: Use Optimized Regressor with best params to perform predictions on test data","9a2d80ff":"# Step 1: Import helpful libraries","f861b709":"The next code cell separates the target (which we assign to y) from the training features."}}