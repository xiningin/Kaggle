{"cell_type":{"17248a30":"code","72927052":"code","4104019f":"code","a8a2e596":"code","4e29c23c":"code","baec5478":"code","b170e441":"code","f3fe519b":"code","dfdf407a":"code","11d45f18":"code","11ac3c42":"code","29038cea":"code","02658968":"code","a754e633":"code","59435147":"code","505059ac":"code","a6d95871":"code","a0f618ae":"code","41b1ea6e":"code","3dc5fb55":"code","5e02d8f2":"code","77b05a58":"code","987be67e":"code","46abd255":"code","4652d3da":"code","e3785c4e":"code","643fde8d":"code","341c1620":"code","17544e86":"code","fa20307a":"code","0583fac0":"code","219b8b84":"code","0ff83e01":"code","3ebf2b30":"code","3afbb539":"code","ccdd4250":"code","169a3000":"code","35ff8a64":"code","73cbbcf6":"code","41654627":"code","a49b8228":"code","a90ccda2":"code","1c95c979":"markdown","5954a40b":"markdown","73782b63":"markdown","3ea684bc":"markdown","55992214":"markdown","25e136b8":"markdown","0188d6f9":"markdown","030c704e":"markdown","3ec15796":"markdown","88230de4":"markdown","09265ed7":"markdown","f7d4c698":"markdown","3461315a":"markdown","05f1c218":"markdown","e1031c42":"markdown","dca2db9f":"markdown","26762d9a":"markdown","30d1c40b":"markdown","4f05b5ec":"markdown","c020a0fe":"markdown","af3e7466":"markdown","a400e735":"markdown","ffbff4fd":"markdown","e137c519":"markdown","1fdaaae9":"markdown","ed8c418a":"markdown","81232f4b":"markdown","d43f4b09":"markdown","1d42c430":"markdown","e3caba95":"markdown","9d93f443":"markdown","fdbd4384":"markdown","134a4fd5":"markdown","d1bc5eda":"markdown","0daa7ab3":"markdown","a6ddff3a":"markdown","b318f9f9":"markdown","6399e9fe":"markdown","6f929a25":"markdown","6744e368":"markdown","5ac8daf7":"markdown","869e58fb":"markdown","0854b989":"markdown","80057e4f":"markdown","1ad94aec":"markdown","ffa1843e":"markdown"},"source":{"17248a30":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","72927052":"import seaborn as sns\nimport matplotlib.pyplot as plt","4104019f":"train=pd.read_csv(\n    \"\/kaggle\/input\/emotions-dataset-for-nlp\/train.txt\",\n    sep=\";\",\n    names=[\"Description\",\"Emotion\"])\n\nvalidate=pd.read_csv(\n    \"\/kaggle\/input\/emotions-dataset-for-nlp\/val.txt\",\n    sep=\";\",\n    names=[\"Description\",\"Emotion\"])\n\ntest=pd.read_csv(\n    \"\/kaggle\/input\/emotions-dataset-for-nlp\/test.txt\",\n    sep=\";\",\n    names=[\"Description\",\"Emotion\"])","a8a2e596":"train.head()","4e29c23c":"validate.head()","baec5478":"test.head()","b170e441":"def describe_data(data_type,data,label):\n    print(data_type,\" DESCRIPTION\")\n    print(\"--------------------------\")\n    size=data.shape\n    null_values=data.isnull().sum().sum()\n    \n    label_count=data[label].value_counts()\n    print(data_type,\" shape:\",size,\"\\n\")\n    print(data_type,\" contains:\",null_values,\" null values\\n\")\n    print(\"Label counts:\")  \n    print(label_count)\n    print()\n    \n    print(\"*****\",data_type,\" label count plot ****\")\n    sns.countplot(\n        data=data,\n        x=label\n    )","f3fe519b":"describe_data(\"Training Data\",train,\"Emotion\")","dfdf407a":"describe_data(\"Validation Data\",validate,\"Emotion\")","11d45f18":"describe_data(\"Test Data\",test,\"Emotion\")","11ac3c42":"train[\"Text Length\"]=train[\"Description\"].apply(len)\n\nfig=plt.figure(figsize=(10,6))\n\nsns.kdeplot(\n    x=train[\"Text Length\"],\n    hue=train[\"Emotion\"]\n)\nplt.show()","29038cea":"train[[\"Text Length\"]].describe()","02658968":"def label_encode(data,label):\n    labels=data[label].map(\n    {\n        \"joy\":0,\n        \"sadness\":1,\n        \"anger\":2,\n        \"fear\":3,\n        \"love\":4,\n        \"surprise\":5\n    }\n    )\n    return labels","a754e633":"train[\"Label\"]=label_encode(train,\"Emotion\")\nvalidate[\"Label\"]=label_encode(validate,\"Emotion\")\ntest[\"Label\"]=label_encode(test,\"Emotion\")","59435147":"train.head()","505059ac":"test.head()","a6d95871":"validate.head()","a0f618ae":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","41b1ea6e":"vocab_size=10000\nsentence_len=150","3dc5fb55":"def data_preparation(data,description):\n    stemmer=PorterStemmer()\n    \n    corpus=[]\n    \n    for text in data[description]:\n        text=re.sub(\"[^a-zA-Z]\",\" \",text)\n        text=text.lower()\n        text=text.split()\n        \n        text=[stemmer.stem(words)\n             for words in text\n              if words not in stopwords.words(\"english\")\n             ]\n        text=\" \".join(text)\n        corpus.append(text)\n        \n    oneHot_doc=[one_hot(input_text=words,n=vocab_size)\n               for words in corpus\n               ]\n    \n    embedded_doc=pad_sequences(sequences=oneHot_doc,\n                              maxlen=sentence_len,\n                              padding=\"pre\")\n    return embedded_doc","5e02d8f2":"X_train=data_preparation(train,\"Description\")\nX_validate=data_preparation(validate,\"Description\")\nX_test=data_preparation(test,\"Description\")","77b05a58":"y_train=train[\"Label\"]\ny_validate=validate[\"Label\"]\ny_test=test[\"Label\"]","987be67e":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.optimizers import Adam","46abd255":"def build_model(hp):\n\n    model=Sequential()\n    \n    model.add(\n    Embedding(\n        input_dim=vocab_size,\n        output_dim=hp.Int(\n            \"output_dim:\",\n            min_value=40,\n            max_value=120,\n            step=10\n        ),\n        input_length=sentence_len\n    )\n    )\n    \n    model.add(\n        LSTM(\n        units=128\n        )\n    )\n    for j in range(hp.Int(\n        \"Dense Layers\",\n        min_value=1,\n        max_value=5,\n        step=1\n    )):\n        model.add(\n        Dense(\n            units=hp.Int(\n                \"units_\"+str(j),\n                min_value=32,\n                max_value=256,\n                step=32\n            ),\n            activation=\"relu\",\n            kernel_initializer=hp.Choice(\n                \"kernel_init\"+str(j),\n                values=[\"he_uniform\",\"he_normal\"]\n            )\n        )   \n        )\n        model.add(\n        Dropout(\n        rate=hp.Float(\n            \"drop_rate\"+str(j),\n            min_value=0.1,\n            max_value=0.5,\n            step=0.1\n        )\n        )\n        )\n    \n    model.add(\n    Dense(\n\n        units=6,\n        activation=\"softmax\"\n    )\n    )\n        \n    model.compile(\n        optimizer=Adam(\n        learning_rate=hp.Choice(\n            \"learnRate\",\n            values=[0.01,0.001,0.0001]\n        )   \n        ),\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"]\n    )\n    \n    return model\n    ","4652d3da":"from kerastuner.tuners import RandomSearch","e3785c4e":"tuner=RandomSearch(\n    build_model,\n    objective=\"val_accuracy\",\n    max_trials=2,\n    executions_per_trial=2,\n    directory=\"emotionNLP\",\n    project_name=\"hypertuningNLP\"\n    )","643fde8d":"tuner.search(\n    X_train,\n    y_train,\n    validation_data=(X_validate,\n                     y_validate),\n    epochs=3\n    )","341c1620":"model=tuner.get_best_models(num_models=1)[0]","17544e86":"model.summary()","fa20307a":"predict_classes=model.predict(X_test)","0583fac0":"y_pred=[np.argmax(label) for label in predict_classes]","219b8b84":"predict=pd.DataFrame(\n    y_pred,\n    columns=[\"Predicted\"]\n)","0ff83e01":"predict[\"Predicted Label\"]=predict[\"Predicted\"].map(\n    {\n        0:\"joy\",\n        1:\"sadness\",\n        2:\"anger\",\n        3:\"fear\",\n        4:\"love\",\n        5:\"surprise\"\n    }\n)","3ebf2b30":"predict_df=pd.concat([test[\"Description\"],\n                      test[\"Emotion\"],\n                      predict[\"Predicted Label\"]],\n                    axis=1)","3afbb539":"predict_df.sample(10)","ccdd4250":"from sklearn.metrics import accuracy_score,confusion_matrix","169a3000":"score=accuracy_score(y_test,y_pred)\ncm=confusion_matrix(y_test,y_pred)","35ff8a64":"print(\"Test Score:{:.2f}%\".format(score*100))","73cbbcf6":"labels=[\"joy\",\"sadness\",\"anger\",\"fear\",\"love\",\"surprise\"]\nfig=plt.figure(figsize=(10,6))\nsns.heatmap(cm,\n            annot=True,\n            fmt=\"d\",\n            xticklabels=labels,\n            yticklabels=labels,\n            cmap=\"coolwarm\",\n            linewidths=0.5\n           )\nplt.title(\"Confusion Matrix Visualization\")\nplt.yticks(rotation=0)\nplt.show()","41654627":"def classify_emotions(model,message):\n\n    for sentences in message:\n        sentences=nltk.sent_tokenize(message)\n        \n        for sentence in sentences:\n            words=re.sub(\"[^a-zA-Z]\",\" \",sentence)\n            \n            if words not in set(stopwords.words('english')):\n                word=nltk.word_tokenize(words)\n                word=\" \".join(word)\n    \n    oneHot=[one_hot(word,n=vocab_size)]\n   \n    text=pad_sequences(oneHot,maxlen=sentence_len,padding=\"pre\")\n    \n    predict_classes=model.predict(text)\n    \n    y_pred=[np.argmax(label) for label in predict_classes]\n    \n    if y_pred==[0]:\n        print(\"It describes joy\")\n        \n    elif y_pred==[1]:   \n        print(\"It describes sadness\")\n        \n    elif y_pred==[2]:   \n        print(\"It describes anger\")\n        \n    elif y_pred==[3]:   \n        print(\"It describes fear\")\n        \n    elif y_pred==[4]:   \n        print(\"It describes love\")\n        \n    elif y_pred==[5]:   \n        print(\"It describes surprise\") \n    ","a49b8228":"emotion1=\"I have tried several times to get back with you, but this aint working with us.\"","a90ccda2":"classify_emotions(model,emotion1)","1c95c979":"**Get the model summary**","5954a40b":"**Our model have classified most of the text description in the right classes, we notice that model have miss-classified 39 joy text description into love which is the highest miss-classification the model have done so far, however it is logical because joy and love would can have often mean same things.**","73782b63":"# Model Evaluation","3ea684bc":"**Function build_model will return model which can be hypertuned using kerastuner later**","55992214":"**We got test score of 86.40% which isnt bad for limmited number of executions\/epochs, however as mentioned earlier you can still improve the model accuracy score by increasing number of max_trials, executions_per_trial & epochs.**","25e136b8":"# Data Understanding\n\n**describe_data function will help us understand each dataset**","0188d6f9":"**I will take vocabulary size of 10,000 and a sentence length of 150 for my text preparation and Model Building later. This is the reason we have to get insights of the traning text description lengths**\n","030c704e":"**Concatenate the Predicted Label of predict dataframe with the original test data to compare them**","3ec15796":"**You can visit the link https:\/\/www.tensorflow.org\/tutorials\/keras\/keras_tuner for detailed eplanations on keras tuner**","88230de4":"**Initialize the model with one of the best models generated by the tuner on training and validate data**","09265ed7":"# Text Preparation","f7d4c698":"#  About Data\nThis dataset contains emotions expressed through text and the emotions included in the data are [\"joy\",\"sadness\",\"anger\",\"fear\",\"love\",\"surprise\"].\n\nThe emotions-dataset-for-nlp contains  train.txt, test.txt & val.txt for training, testing and validation purposes seperately. The text files consist of text descriptions and its corresponding label which are seperated by semicolon(;).","3461315a":"**The kdeplot shows that the curve is right-skewed, and its so because of few lengthy text description which have pulled the curved towards right.**\n\n**However we can see that all messages are concentrated in the range of 1-150. This plays vital role in text preparation as I have mentioned earlier**","05f1c218":"**Remember we took softmax activation on our output layer, hence unlike binary classification where we use sigmoid to predict the sentiment labels,softmax activation function turns numbers aka logits into probabilities that sum to one. Softmax function outputs a vector that represents the probability distributions of a list of potential outcomes.**\n\n**Thus to get the predicted sentiment class we take the vector with the highest probability and hence the output. Which can be performed by the code below**","e1031c42":"**Display head of all dataset**","dca2db9f":"**Label encode in all the dataset**","26762d9a":"**We have mininum Description length of 7 and maximum length of 300 however most of them are concentrated in between 1-150**","30d1c40b":"**Import Libraries for text preparation**","4f05b5ec":"**Read the datasets**","c020a0fe":"**Libraries for visualization**","af3e7466":"# Problem Statement\n\nSince it deals with the text and we are classifying the text into respective emotions(labels) this is clearly a NLP text classfication problem. We have multiple emotions(labels) in our dataset hence it is a multi-classification problem.\n\n**Natural language processing (NLP)**: is the ability of a computer program to understand human language as it is spoken and written -- referred to as natural language.\n\n","a400e735":"# Problem Approach:\n\nI have done similar spam vs ham nlp binary text classfication and handling with imbalanced dataset with detailed explanations of steps and approaches, you can visit the notebook using the link https:\/\/www.kaggle.com\/ngawangchoeda\/spam-classifier-using-lstm. \n\n# 1. Data Understanding\n\nI have performed exploratory data analysis on the three dataset using function \"**describe_data**\", which outputs the size, null values and label counts of the dataset. The data did not had any null values so I skipped Data Cleaning part, which indeed is the most hectic part in Data Science lifecycle phase. \n\n# 2. Text Preparation\n\nSince all the dataset were free of any null values, I jumped to the next step which is preparing text for my model.\n\nAs there is seperate files for train,test & validation I have used a function \"**data_preparation**\" for preparing text.\n\nText Preperation includes:\n* Text Tokenization\n* Processing tokenized text using one_hot library from keras.preprocessing.text\n* Getting embedded_document from one_hot document using pad_sequences\n\n# 3. Model Building\n\nI have used function \"**build_model**\" to build my sequentail model. This is so to perform hypertuning for my model, to dive in more in hypertuning in keras you can read keras hypertuning document.\n\nThe Model consist of Embedding Layer which takes input vector which is the traning embedded document. The output of Embedding layer is then passed to LSTM with units=128. The output from LSTM is then passed into Dense layers which I have passed range of parameters. Then comes to the output Dense layer with softmax activaton and a unit of 6 which is equivalent to the number of sentiment classes.\n\nThe **build_model** function returns the model which is then hypertuned using RandomSearch from kerastuner. Fetch the best model with the tuned parameters, which is further used for model evaluation.\n\n# 4. Model Evaluation\nThe model is evaluated on the test data using the metrics from scikit-learn","ffbff4fd":"**The model classify emotion1 as an anger and we can agree with that**","e137c519":"**From the 10 random samples I have printed, my model have predicted all right so we are moving in a right direction**","1fdaaae9":"**Initialize a tuner for the model.**","ed8c418a":"**import classification metrics to evaluate our model**","81232f4b":"**create a dataframe out of predicted labels and name its column as Predicted**","d43f4b09":"**Our model is doing fine with the testing dataset now its time how our model predicts with a complete new data**\n\n**I have defined a function that will take take description from and outputs the emotions expressed by the model**","1d42c430":"**In NLP the distribution of text lengths plays a vital role in text preparation and model building, so lets try to extract insights of text lenghts in the dataset. I will specifically choose training data for this because traning data will be used for model building later.**","e3caba95":"**Lets visualize the confusion matrix using seaborn heatmap**","9d93f443":"**Conclusion: All the dataset are free of null values. The test and validate data size are equal which is smaller than the traning data, which is obvious that more data is required in traning model than validating and testing model. The Distribution of joy and sadness labels are denser than any other labels in all dataset.**","fdbd4384":"**I have my model ready so lets evaluate it on the test data to see how it performs on the test data.**","134a4fd5":"**Describe validation data**","d1bc5eda":"**Describe training data**","0daa7ab3":"**Check the dataset**","a6ddff3a":"**Map the labels into original sentiment classes**","b318f9f9":"# Model Building\n\n**Import libraries for model building**","6399e9fe":"**Extract the target varaibles**","6f929a25":"**Describe test data**","6744e368":"**Create embedded document for training,validation and testing data which will be used for traning model, validating model and testing model respectively later.**\n\n**The three below embedded document serves as the independent variable, and for convinience I have named it with familiar names, which we use in typical ML and DL dataset.**","5ac8daf7":"# Data Preparation\n\n**Before preparing text for the model wenoticed that data labels with object datatype. So to feed our data to the model it must be converted into machine understandable form. For that reason let I have label encoded the labels using the funcion \"label_encode\"**","869e58fb":"**The function takes raw text(description) then tokenize it, perform one_hot, and return the embedded_document which can be fed to the Sequential model later.**\n\n**If you want to get the detailed explanations of each steps employed in the text preparation you can visit my ham vs spam classifier notebook with the link** https:\/\/www.kaggle.com\/ngawangchoeda\/spam-classifier-using-lstm","0854b989":"***Note: One can increase the number of trails and epochs to improve the accuracy scores***","80057e4f":"**search through the parameters for using traning and validation data**","1ad94aec":"**import library for model hypertuning**","ffa1843e":"**Lets feed two text description and lets see how my model predicts**"}}