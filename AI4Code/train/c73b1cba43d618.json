{"cell_type":{"cac8c030":"code","92962412":"code","302bf322":"code","3a6da48c":"code","52a55cea":"code","ddbf604f":"code","4d452144":"code","f41300fc":"code","ab61d1f4":"code","ea62dc3b":"code","b80abcf4":"code","29a4581f":"code","47a69eeb":"code","3bb04c5e":"code","76c550d5":"code","371b10ae":"code","989c3d36":"code","d83fcd25":"code","e702ab02":"markdown","9a745e26":"markdown","3d29d321":"markdown","2d0a3d72":"markdown","c40b6e07":"markdown"},"source":{"cac8c030":"# Basic data science packages\nimport pandas as pd\nimport numpy as np\n\n# Decision Trees and visualization\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree, _tree\nfrom sklearn.tree._tree import TREE_UNDEFINED\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix\nfrom sklearn.utils import class_weight\nimport matplotlib.pyplot as plt","92962412":"def dataset_proportion():\n    for block_size in (128, 64, 32, 16):\n        df = pd.read_csv(f'\/kaggle\/input\/vvc-block-split\/Dataset-{block_size}.csv', engine='python')\n\n        print(f'{block_size}x{block_size}')\n        split = len(df.loc[(df['Split'] == 'Split')])\n        non_split = len(df.loc[(df['Split'] == 'Non-split')])\n        total = split + non_split\n        print(f\"Split: {split} ({split * 100 \/ total:.2f}%)\")\n        print(f\"Non-split: {non_split} ({non_split * 100 \/ total:.2f}%)\\n\")\n\ndataset_proportion()","302bf322":"df = pd.read_csv(f'\/kaggle\/input\/vvc-block-split\/Dataset-64.csv')\n\ncolumn_name = {\n    'Block-Variance': 'blockVariance',\n    'Video': 'video',\n    'Split': 'split',\n    'Width': 'width',\n    'Height': 'height',\n    'CFG': 'cfg',\n    'Previous-Split': 'previousSplit',\n    'Diff-Variance': 'diffVariance',\n}\n\ndf = df.rename(columns=column_name)\n\ndf.head()","3a6da48c":"df[\"video\"].unique()","52a55cea":"training_videos = ['Tango2', 'DaylightRoad2', 'Cactus', 'BasketballDrive']\ntest_videos = ['FoodMarket4', 'RitualDance', 'MarketPlace', 'Campfire', 'CatRobot', 'BQTerrace', 'ParkRunning3']\n\ntraining_df = df.query(f'video in {training_videos}')\ntraining_df.sample(frac=1).reset_index(drop=True)  # Shuffle dataframe \n\ntest_df = df.query(f'video in {test_videos}')\ntest_df.sample(frac=1).reset_index(drop=True)  # Shuffle dataframe \n\ntraining_df['video'].unique()","ddbf604f":"def show_correlation():\n    f = plt.figure(figsize=(12, 9))\n\n    plt.matshow(training_df.corr(), fignum=f.number)\n    plt.xticks(range(training_df.select_dtypes(['number']).shape[1]), training_df.select_dtypes(['number']).columns, fontsize=13)\n    plt.yticks(range(training_df.select_dtypes(['number']).shape[1]), training_df.select_dtypes(['number']).columns, fontsize=14)\n    cb = plt.colorbar()\n    cb.ax.tick_params(labelsize=14)\n\n    plt.show()\n\nshow_correlation()","4d452144":"cfg_dict = {\n    \"LD\": 1,\n    \"RA\": 0,\n    True: 1,\n    False: 0\n}\n\n# df = df.replace(cfg_dict)\ntraining_df = training_df.replace(cfg_dict)\ntest_df = test_df.replace(cfg_dict)\n\n# X_train = training_df.drop('Split', axis=1).drop('CFG', axis=1).drop('Video', axis=1).drop('Height', axis=1).drop('Block-Variance', axis=1).drop('Previous-Split', axis=1).copy()\n# X_test = test_df.drop('Split', axis=1).drop('CFG', axis=1).drop('Video', axis=1).drop('Height', axis=1).drop('Block-Variance', axis=1).drop('Previous-Split', axis=1).copy()\n\nX_train = training_df.drop('split', axis=1).drop('video', axis=1).drop('width', axis=1).drop('cfg', axis=1).drop('blockVariance', axis=1).copy()\nX_test = test_df.drop('split', axis=1).drop('video', axis=1).drop('width', axis=1).drop('cfg', axis=1).drop('blockVariance', axis=1).copy()\n\n# X = df.drop('Split', axis=1).drop('Video', axis=1).drop('Width', axis=1).drop('CFG', axis=1).drop('Block-Variance', axis=1).copy()\n\nX_train.head()","f41300fc":"split_dict = {\n    \"Split\": 1,\n    \"Non-split\": 0\n}\n\ntraining_df = training_df.replace(split_dict)\ntest_df = test_df.replace(split_dict)\n\n# df = df.replace(split_dict)\n\ny_train = training_df['split'].copy()\ny_test = test_df['split'].copy()\n\n# y = df['Split'].copy()\n\ny_train.head()\n# y.head()","ab61d1f4":"fig, ax = plt.subplots(figsize=(8, 8))\n\nclf_dt = DecisionTreeClassifier(class_weight='balanced', max_depth=2, random_state=15)\nclf_dt = clf_dt.fit(X_train, y_train)\n\nprint(clf_dt.score(X_test, y_test))\n\nplot_confusion_matrix(clf_dt, X_test, y_test, display_labels=[\"Didn't split\", \"Did split\"], ax=ax)","ea62dc3b":"plt.figure(figsize=(19, 15))\nplot_tree(clf_dt,\n          filled=True,\n          rounded=True,\n          class_names=[\"Non-split\", \"Split\"],\n          feature_names=X_train.columns)","b80abcf4":"import math\n\ndef prune_tree(clf_dt):\n    # Pruning the Tree\n    path = clf_dt.cost_complexity_pruning_path(X_train, y_train)\n    ccp_alphas = path.ccp_alphas\n    ccp_alphas = ccp_alphas[:-1]\n\n    print(len(ccp_alphas))\n\n    alphas = list()\n\n    for ccp_alpha in ccp_alphas:\n        clf_dt = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n        scores = cross_val_score(clf_dt, X_train, y_train, cv=5)\n        alphas.append([ccp_alpha, np.mean(scores), np.std(scores)])\n\n    return alphas\n\nalphas = prune_tree(clf_dt)","29a4581f":"def plot_alpha_results(alphas):\n    alpha_results = pd.DataFrame(alphas, columns=['alpha', 'mean_accuracy', 'std'])\n\n    alpha_results.plot(x='alpha',\n                       y='mean_accuracy',\n                       yerr='std',\n                       marker='o',\n                       linestyle='--')\n\n    return alpha_results\n\nalpha_results = plot_alpha_results(alphas)","47a69eeb":"alpha_results","3bb04c5e":"def get_ideal_tree(alpha_results):\n    ideal_ccp_alpha = alpha_results[(alpha_results['mean_accuracy'] == alpha_results['mean_accuracy'].max())]['alpha']\n    ideal_ccp_alpha = float(ideal_ccp_alpha.sample(n=1))\n    print(ideal_ccp_alpha)\n\n    clf_dt_pruned = DecisionTreeClassifier(ccp_alpha=ideal_ccp_alpha, class_weight='balanced', random_state=0)\n    clf_dt_pruned = clf_dt_pruned.fit(X_train, y_train)\n\n    plot_confusion_matrix(clf_dt_pruned, X_test, y_test, display_labels=[\"Didn't split\", \"Did split\"])\n    print(clf_dt_pruned.score(X_test, y_test))\n    return clf_dt_pruned\n\nclf_dt_pruned = get_ideal_tree(alpha_results)","76c550d5":"def plot_ideal_tree(clf_dt):\n    plt.figure(figsize=(19, 15))\n    plot_tree(clf_dt,\n          filled=True,\n          rounded=True,\n          class_names=[\"Non-split\", \"Split\"],\n          feature_names=X_train.columns)\n\nplot_ideal_tree(clf_dt_pruned)","371b10ae":"def get_accuracy_confidence(tree):\n    tree_ = tree.tree_\n    confidences_split = list()\n    confidences_non_split = list()\n    \n    def recurse(node):  \n        if tree_.feature[node] == TREE_UNDEFINED:\n            split, non_split = tree_.value[node][0]\n                 \n            if split > non_split:\n                confidence = split \/ (non_split + split)\n                confidences_split.append(confidence)\n            else:\n                confidence = non_split \/ (non_split + split)\n                confidences_non_split.append(confidence)\n            \n            return\n        else:\n            left = tree_.children_left[node]\n            right = tree_.children_right[node]\n            \n            recurse(left)       \n            recurse(right) \n    \n    recurse(0)\n    \n    return confidences_split, confidences_non_split\n\nconfidences_split, confidences_non_split = get_accuracy_confidence(clf_dt_pruned)\nprint(confidences_split, confidences_non_split)","989c3d36":"THRESHOLD_PERCENTAGE = 0.6\n\ndef get_confidence_thresholds(confidences):\n    min_confidence = min(confidences)\n    max_confidence = max(confidences)\n    \n    confidence_range = max_confidence - min_confidence\n    \n    confidence_threshold = min_confidence + (THRESHOLD_PERCENTAGE * confidence_range)\n    \n    return confidence_threshold\n    \nconfidence_threshold_split = get_confidence_thresholds(confidences_split)\nconfidence_threshold_non_split = get_confidence_thresholds(confidences_non_split)\nprint(confidence_threshold_split, confidence_threshold_non_split)\n\nconfidence_dict = {\n    \"true\": confidence_threshold_split,\n    \"false\": confidence_threshold_non_split,\n}","d83fcd25":"def tree_to_code(tree, feature_names, confidence_dict):\n    tree_ = tree.tree_\n    feature_name = [feature_names[i] for i in tree_.feature]\n\n    open_bracket = '{'\n    close_bracket = '}'\n\n    def recurse(node, depth):\n        indent = \"    \" * depth\n        \n        # Node is not a leaf node\n        if tree_.feature[node] != TREE_UNDEFINED:\n            name = feature_name[node]\n            threshold = tree_.threshold[node]\n            \n            left = tree_.children_left[node]\n            right = tree_.children_right[node]\n\n            print(f\"{indent}if ({name} <= {threshold:.4f}) {open_bracket}\")\n            recurse(left, depth + 1)\n\n            print(f\"{indent}{close_bracket} else {open_bracket}  \/\/ if {name} > {threshold:.4f}\")\n            recurse(right, depth + 1)\n\n            print(f\"{indent}{close_bracket}\")\n        else:\n            split, non_split = tree_.value[node][0]\n\n            result = \"true\" if split > non_split else \"false\"\n\n            print(f\"{indent}skipCheckRD = {result};\")\n            \n            confidence = split \/ (non_split + split) if split > non_split else non_split \/ (non_split + split)\n            \n            confidence_threshold = confidence_dict[result]\n            \n            confidence_boolean = \"true\" if confidence >= confidence_threshold else \"false\"\n            \n            print(f\"{indent}confidenceDT = {confidence_boolean};\")\n\n    recurse(0, 0)\n\ntree_to_code(clf_dt_pruned, X_train.columns, confidence_dict)","e702ab02":"# VVC Split Decision Trees","9a745e26":"## Data exploration & feature engineering","3d29d321":"## Prune the Decision Tree","2d0a3d72":"## Build and evaluate initial Decision Tree","c40b6e07":"## Convert tree to C code"}}