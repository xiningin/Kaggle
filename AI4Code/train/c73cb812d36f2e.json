{"cell_type":{"acc87ec0":"code","d865784e":"code","ffd6747b":"code","deebd40c":"code","17230158":"code","a70cdb71":"code","8b3e431f":"code","bf584cbf":"code","de15b2a2":"code","6a1fc637":"code","22c1a38f":"code","17073df8":"code","8c28188b":"code","40b5cfa4":"code","3e943a5a":"code","5d11742d":"code","aafb188f":"code","1390b733":"code","59c613d8":"code","25f761cd":"code","bfa90e82":"code","a82389f4":"code","cba25cc6":"code","688c0eba":"code","1aec20c4":"code","2be1d27b":"code","231da7bb":"code","778ecaac":"code","66e52bac":"code","7b56d5dc":"code","3cb96417":"code","3de02266":"code","4c652f92":"code","108623a6":"code","5fd667d2":"code","e993fa69":"code","598ac011":"markdown","af741f06":"markdown","27dc5444":"markdown","396e48a3":"markdown","2b6c9d8c":"markdown","6b7a9778":"markdown"},"source":{"acc87ec0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d865784e":"import re                       # to clean text and replace charactors to simplfy the conversation\nimport tensorflow as tf ","ffd6747b":"# read 'movie_lines.txt' file  \nlines = open('\/kaggle\/input\/cornell-moviedialog-corpus\/movie_lines.txt', encoding = 'UTF-8', errors = 'ignore').read().split('\\n')","deebd40c":"# read 'conversations.txt' file  \nconversations = open('\/kaggle\/input\/cornell-moviedialog-corpus\/movie_conversations.txt', encoding = 'UTF-8', errors = 'ignore').read().split('\\n')","17230158":"# display first lines of file 'movie_lines.txt'\nwith open('\/kaggle\/input\/cornell-moviedialog-corpus\/movie_lines.txt') as m_lines:\n        head = [next(m_lines) for x in range(1)]\n        print(head)","a70cdb71":"# display first lines of file 'movie_conversations.txt'\nwith open('\/kaggle\/input\/cornell-moviedialog-corpus\/movie_conversations.txt') as m_conv:\n        head = [next(m_conv) for x in range(1)]\n        print(head)","8b3e431f":"# create a dictionary that maps each lines and id\nid2line = {} \nfor line in lines:\n    _line = line.split(' +++$+++ ')                 # split the line with sep ' +++$+++ '\n    if len(_line) == 5:                             # len of line should be 5    \n        id2line[_line[0]] = _line[4]                # assign id of line (index'0') with movie line (index'4')  ","bf584cbf":"# print id2line value\nprint(dict(list(id2line.items())[0: 5]))  ","de15b2a2":"# create list of all the conversations\nconversations_ids = []\nfor conversation in conversations[:-1]:    # last row of conversations is empty\n    _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \",\"\")\n    conversations_ids.append(_conversation.split(','))","6a1fc637":"# print first 10 conversations_ids values \nconversations_ids[0:10]","22c1a38f":"# getting seperately questions and answers \nquestions = []\nanswers = []\nfor conversation in conversations_ids:\n    for i in range(len(conversation)-1):           # iterate through all the ids of a conversation_ids list\n        questions.append(id2line[conversation[i]])  \n        answers.append(id2line[conversation[i+1]])","17073df8":"# dataframe showing seperate questions ans anwers obtained from above lines \nQA_df = pd.DataFrame({'Question': list(questions), 'Answers': list(answers)} )\nQA_df.head(5)","8c28188b":"# text cleaning\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"i'm\", \"i am\", text)\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"she's\", \"she is\", text) \n    text = re.sub(r\"that's\", \"that is\", text)\n    text = re.sub(r\"what's\", \"what is\", text)\n    text = re.sub(r\"where's\", \"where is\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"can't\", \"can not\", text)\n    text = re.sub(r\"[-{}+=();*&^%$#@!~`><:]\",\"\", text)\n    return text","40b5cfa4":"# clean questions\nclean_questions = []\nfor question in questions:\n    clean_questions.append(clean_text(question))\n    ","3e943a5a":"clean_answers[0:5]","5d11742d":"# clean answers\nclean_answers = []\nfor answer in answers:\n    clean_answers.append(clean_text(answer))","aafb188f":"# create a dict that maps a word with number of its occurance\nword2count = {}\nfor question in clean_questions:\n    for word in question.split():\n        if word not in word2count:\n            word2count[word] =  1\n        else:\n            word2count[word] +=1\n\nfor answer in clean_answers:\n    for word in answer.split():\n        if word not in word2count:\n            word2count[word] =  1\n        else:\n            word2count[word] +=1","1390b733":"# print word2count value\nprint(dict(list(word2count.items())[0: 5])) ","59c613d8":"# get frequency of questions word  and answers word \n\nthreshold = 20\nword_number = 0 \nquestionword2int = {}\nfor word,count in word2count.items():\n    if count>threshold:\n        questionword2int[word] = word_number\n        word_number +=1\n\nanswerword2int = {}\nfor word,count in word2count.items():\n    if count>threshold:\n        answerword2int[word] = word_number\n        word_number +=1","25f761cd":"dict(list(answerword2int.items())[-5:])","bfa90e82":"# adding  last tokens to above dict\ntokens = ['<PAD>','<EOS>','<OUT>', '<SOS>']\nfor token in tokens:\n    questionword2int[token] = len(questionword2int)+1\n\nfor token in tokens:\n    answerword2int[token] = len(answerword2int)+1","a82389f4":"#creating inverse of dict answerword2int\nanswerint2word = {w_i:w for w, w_i in answerword2int.items() }","cba25cc6":"sorted(dict(list(answerint2word.items())[-5:]))","688c0eba":"# adding End of String token to to end of every answer\nfor i in range(len(clean_answers)):\n    clean_answers[i] += ' <EOS>'\n","1aec20c4":"clean_answers[0:2]","2be1d27b":"# Translating questions and answers  into integers. Replacing word which are filtered out by <OUT> token\n\nquestion_to_int = []\nfor question in clean_questions:\n    ints = []\n    for word in question.split():\n        if word not in questionword2int:\n            ints.append(questionword2int['<OUT>'])\n        else:\n            ints.append(questionword2int[word])\n    question_to_int.append(ints)\n\nanswer_to_int = []\nfor answer in clean_answers:\n    ints = []\n    for word in answer.split():\n        if word not in answerword2int:\n            ints.append(answerword2int['<OUT>'])\n        else:\n            ints.append(answerword2int[word])\n    answer_to_int.append(ints)","231da7bb":"#question_to_int[0:2]","778ecaac":"# sort question and answer by length of questions\nsorted_clean_questions = []\nsorted_clean_answers = []\nfor length in range(1, 25+1):\n    for i in enumerate(question_to_int):\n        if len(i[1])== length:\n            sorted_clean_questions.append(question_to_int[i[0]])\n            sorted_clean_answers.append(answer_to_int[i[0]])\n            ","66e52bac":"sorted_clean_questions[-1:]","7b56d5dc":"# Creating placeholders for the inputs and the targets\ndef model_inputs():\n    inputs = tf.placeholder(tf.int32, [None, None], name = 'input')\n    targets = tf.placeholder(tf.int32, [None, None], name = 'target')\n    lr = tf.placeholder(tf.float32, name = 'learning_rate')\n    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n    return inputs, targets, lr, keep_prob\n \n","3cb96417":"# Preprocessing the targets\ndef preprocess_targets(targets, word2int, batch_size):\n    left_side = tf.fill([batch_size, 1], word2int['<SOS>'])\n    right_side = tf.strided_slice(targets, [0,0], [batch_size, -1], [1,1])\n    preprocessed_targets = tf.concat([left_side, right_side], 1)\n    return preprocessed_targets","3de02266":"# Creating the Encoder RNN\ndef encoder_rnn(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n    encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n    encoder_output, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell,\n                                                                    cell_bw = encoder_cell,\n                                                                    sequence_length = sequence_length,\n                                                                    inputs = rnn_inputs,\n                                                                    dtype = tf.float32)\n    return encoder_state","4c652f92":"# Decoding the training set\ndef decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n    training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n                                                                              attention_keys,\n                                                                              attention_values,\n                                                                              attention_score_function,\n                                                                              attention_construct_function,\n                                                                              name = \"attn_dec_train\")\n    decoder_output, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n                                                                                                              training_decoder_function,\n                                                                                                              decoder_embedded_input,\n                                                                                                              sequence_length,\n                                                                                                              scope = decoding_scope)\n    decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n    return output_function(decoder_output_dropout)","108623a6":"# Decoding the test\/validation set\ndef decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, sos_id, eos_id, maximum_length, num_words, decoding_scope, output_function, keep_prob, batch_size):\n    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n    test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function,\n                                                                              encoder_state[0],\n                                                                              attention_keys,\n                                                                              attention_values,\n                                                                              attention_score_function,\n                                                                              attention_construct_function,\n                                                                              decoder_embeddings_matrix,\n                                                                              sos_id,\n                                                                              eos_id,\n                                                                              maximum_length,\n                                                                              num_words,\n                                                                              name = \"attn_dec_inf\")\n    test_predictions, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n                                                                                                                test_decoder_function,\n                                                                                                                scope = decoding_scope)\n    return test_predictions\n \n","5fd667d2":"# Creating the Decoder RNN\ndef decoder_rnn(decoder_embedded_input, decoder_embeddings_matrix, encoder_state, num_words, sequence_length, rnn_size, num_layers, word2int, keep_prob, batch_size):\n    with tf.variable_scope(\"decoding\") as decoding_scope:\n        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n        lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n        decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n        weights = tf.truncated_normal_initializer(stddev = 0.1)\n        biases = tf.zeros_initializer()\n        output_function = lambda x: tf.contrib.layers.fully_connected(x,\n                                                                      num_words,\n                                                                      None,\n                                                                      scope = decoding_scope,\n                                                                      weights_initializer = weights,\n                                                                      biases_initializer = biases)\n        training_predictions = decode_training_set(encoder_state,\n                                                   decoder_cell,\n                                                   decoder_embedded_input,\n                                                   sequence_length,\n                                                   decoding_scope,\n                                                   output_function,\n                                                   keep_prob,\n                                                   batch_size)\n        decoding_scope.reuse_variables()\n        test_predictions = decode_test_set(encoder_state,\n                                           decoder_cell,\n                                           decoder_embeddings_matrix,\n                                           word2int['<SOS>'],\n                                           word2int['<EOS>'],\n                                           sequence_length - 1,\n                                           num_words,\n                                           decoding_scope,\n                                           output_function,\n                                           keep_prob,\n                                           batch_size)\n    return training_predictions, test_predictions","e993fa69":"# Building the seq2seq model\ndef seq2seq_model(inputs, targets, keep_prob, batch_size, sequence_length, answers_num_words, questions_num_words, encoder_embedding_size, decoder_embedding_size, rnn_size, num_layers, questionswords2int):\n    encoder_embedded_input = tf.contrib.layers.embed_sequence(inputs,\n                                                              answers_num_words + 1,\n                                                              encoder_embedding_size,\n                                                              initializer = tf.random_uniform_initializer(0, 1))\n    encoder_state = encoder_rnn(encoder_embedded_input, rnn_size, num_layers, keep_prob, sequence_length)\n    preprocessed_targets = preprocess_targets(targets, questionswords2int, batch_size)\n    decoder_embeddings_matrix = tf.Variable(tf.random_uniform([questions_num_words + 1, decoder_embedding_size], 0, 1))\n    decoder_embedded_input = tf.nn.embedding_lookup(decoder_embeddings_matrix, preprocessed_targets)\n    training_predictions, test_predictions = decoder_rnn(decoder_embedded_input,\n                                                         decoder_embeddings_matrix,\n                                                         encoder_state,\n                                                         questions_num_words,\n                                                         sequence_length,\n                                                         rnn_size,\n                                                         num_layers,\n                                                         questionswords2int,\n                                                         keep_prob,\n                                                         batch_size)\n    return training_predictions, test_predictions","598ac011":"*  why dict: \n*     i\/o for chatbots: lines of conversation\n*     o\/p of chatbots: word prediected \n*  Dict can help to keep track of this conversation","af741f06":"* Here if 'L194' represents question then 'L195' is the response, simillarly 'L196' coulde be next line from 'u0' then 'L197' is response of that from 'u2'\n","27dc5444":"* why \n   * As it'll speed up the training and help to reduce the loss\n   * How: it'll reduce the pading during training ","396e48a3":"* L1045 : id uniquely associated to line\n* u0 : performer or charactor\n* BIANCA: charactor name ","2b6c9d8c":"* u0,u2 : perfomers \n* m0 : movie\n* ['L194', 'L195', 'L196', 'L197'] list of ids ","6b7a9778":"# Data Preprocessing"}}