{"cell_type":{"60c86e85":"code","7e9135b6":"code","e3040313":"code","275d9461":"code","5dcfa513":"markdown","c1221959":"markdown","12fff5d9":"markdown","a8ffc839":"markdown"},"source":{"60c86e85":"import pandas as pd\nimport datetime\nimport numpy as np","7e9135b6":"input_path = '\/kaggle\/input\/order-brushing-shopee-code-league\/order_brush_order.csv'\ndf = pd.read_csv(input_path)\ndf = df.drop(columns = [\"orderid\"])\ntime = pd.to_datetime(df.event_time)\ndf['event_time'] = time\ndf = df.sort_values(by=\"event_time\").reset_index(drop = True)\ngrouped = df.groupby('shopid')\ndelta = pd.Timedelta(hours=1)\nsec = pd.Timedelta(seconds = 1)","e3040313":"userdict = {}\nfor name,group in grouped:\n    \n    #step 1 look for repeated buyers\n    #generate possible_idxs\n    possible_idxs = []\n    userlist = []\n    dups_user = group.pivot_table(index=['userid'], aggfunc='size')\n    if dups_user.max() >=3:\n        max_user = (dups_user[dups_user.values >=3 ])\n        userlist = (max_user.index.values)\n    if len(userlist):\n        for user in userlist:\n            suspected_df = group[group['userid'] == user]\n            #step 2 only pick 3 purchases first\n            for start_idx in range(len(suspected_df)-2):\n                start_time = suspected_df.iloc[start_idx].event_time\n                end_idx = start_idx + 2\n                end_time = suspected_df.iloc[end_idx].event_time\n                #this if condition is to make sure the three purchases are made within 1 hour (this is step 1 actually \n                #but this is how programming works)\n                if end_time <= start_time + delta:\n                    possible_idxs.append([suspected_df.iloc[start_idx].name, suspected_df.iloc[end_idx].name])       \n    \n    #so now we have all the possible time period order brushing likely to happen\n    #we need to expand them into 1 hour and see whether they still fulfil the requirement\n    #step 3\n    confirmed_idxs = []\n    for idx in possible_idxs:\n        # here we define the starting, ending, previous and next time event to perform calculation\n        start_loc = idx[0]\n        start_iloc = np.flatnonzero(group.index==start_loc)[0]\n        current_user = group.iloc[start_iloc].userid\n        start_time = group.iloc[start_iloc].event_time\n        end_loc = idx[1]\n        end_iloc = np.flatnonzero(group.index==end_loc)[0]\n        end_time = group.iloc[end_iloc].event_time\n        #if the period happpens at the boundary, we assign the time before\/after for it to be large so we do not\n        #need to include more purchase in\n        pre_loc = start_iloc - 1 if start_iloc >0 else None\n        nex_loc = end_iloc + 1 if end_iloc < (len(group)-1) else None\n        pre_loc_time = group.iloc[pre_loc].event_time + sec if pre_loc else start_time - 1000*delta\n        nex_loc_time = group.iloc[nex_loc].event_time - sec if nex_loc else end_time + 1000*delta\n        pre_user = group.iloc[pre_loc].userid if pre_loc else 0\n        nex_user = group.iloc[nex_loc].userid if nex_loc else 0    \n        # first we check is it fulfil 1 hour time already\n        while nex_loc_time - pre_loc_time < delta:\n            pre_pre = pre_loc -1 if pre_loc else None\n            nex_nex = nex_loc +1 if nex_loc < (len(group)-1) else None\n            pre_pre_time = group.iloc[pre_pre].event_time + sec if pre_pre else pre_loc_time - 1000*delta\n            nex_nex_time = group.iloc[nex_nex].event_time - sec if nex_nex else nex_loc_time + 1000*delta\n            td1= pre_loc_time - pre_pre_time\n            td2= nex_nex_time - nex_loc_time\n            #if not fulfil 1 hour time\n            #we first choose the neighbour point by the userid\n            #we want to include the same buyer aka consecutive purchase which will increate the concentration rate\n            if pre_user == current_user:\n                start_iloc = pre_loc\n                pre_loc = pre_pre\n                pre_loc_time = pre_pre_time\n                pre_user = group.iloc[pre_loc].userid if pre_loc else 0\n                continue\n            elif nex_user == current_user:\n                end_iloc = nex_loc\n                nex_loc = nex_nex\n                nex_loc_time = nex_nex_time\n                nex_user = group.iloc[nex_loc].userid if nex_loc else 0\n                continue\n            #next we pick the one with larger space to include minimum number of purchase from other buyers\n            #this is a little bit tricky but you can think of it\n            if td1> td2:\n                start_iloc = pre_loc\n                pre_loc = pre_pre\n                pre_loc_time = pre_pre_time\n                pre_user = group.iloc[pre_loc].userid if pre_loc else 0\n            else:\n                end_iloc = nex_loc\n                nex_loc = nex_nex\n                nex_loc_time = nex_nex_time\n                nex_user = group.iloc[nex_loc].userid if nex_loc else 0\n        #now we check the concentration rate\n        #calculate concentration_rate\n        pur_cnt = end_iloc - start_iloc +1\n        user_cnt = group.iloc[start_iloc:end_iloc+1].userid.nunique()\n        c_rate = pur_cnt\/user_cnt\n        if c_rate >= 3:\n            current_list = [x for x in range(start_iloc,end_iloc+1)]\n            confirmed_idxs += current_list\n\n    #here we might include some duplicated value\n    #remove duplicate value\n\n    confirmed_idxs = list(set(confirmed_idxs))\n    confirmed_idxs.sort()\n    \n    #now we have all the timeframe with order brushing\n    #we just need to find the user with the largest number of purchase here\n    #I used pivot table there to calculate\n    answer = 0\n    if confirmed_idxs:\n        confirmed_df = group.iloc[confirmed_idxs]\n        confirmed_dups_user = confirmed_df.pivot_table(index=['userid'], aggfunc='size')\n        max_user = (confirmed_dups_user[confirmed_dups_user == confirmed_dups_user.max()])\n        answer = '&'.join(str(x) for x in max_user.index)\n\n    userdict[name]= answer\n    \n    \n#now we have the answer!!\nprint(userdict)\nprint(len(userdict))","275d9461":"output = pd.DataFrame()\noutput['shopid'] = userdict.keys()\noutput['userid'] = userdict.values()\noutput.to_csv('\/kaggle\/working\/prediction.csv',index=False)","5dcfa513":"Just to share another method to get score 1.00  \nthis might not be the optimal method but I hope we can have more discussions!\nThe main steps are: \n1. Look for repeated buyers for specific store within 1 hour time.(Possible order brushing)\n(at least 3 purchases, else it won't fulfil the concentration rate >= 3)\n(the lowest purchase will be 3 purchases done by the same buyer)\n\n2. Even we see four or more purchases within one hour we still pick 3 first.\n(Here we are looking for 3 consecutive purchases happened without other buyers purchasing)\n(sometimes 3 purchases will the concentration rate requirement, if we consider the forth one, if another purchase happended after the third one, it will not fulfill the concentration rate requirement and we missed one value)\n(**REALISE that this will not fulfill one hour time period**)\n(**We only pick the most proprobable users with multiple purchases)\n\n3. Now we expand the suspective purchases into 1 hour period\n(we will need to see the purchase before it starts and after it ends)\n(in particular [time of (starting index-1)+ 1 sec and (ending index+1) - 1 sec]) (this 1 sec means we don't include them)\n(the best scenario is the period is larger than 1 hour which means we do not need to add more purchases in)\n(**One special case is if it is located at the begining or the end, in this case we just pick a very large value (before\/after) it, so this case will be larger than 1 hour as well**)\n\n4. Choose which one to expand? (previous or next one)\n(if we encounter that time between (starting index-1)+ 1 sec and (ending index+1) - 1 sec is less than 1 hour)\n(we need to choose based on two criterias 1. the buyer 2. the time in between them)\n(the first priority is the buyer, we must include the one with the same buyer first)\n(**note that we dropped some of them before, now we need to take them back! we want to include all the consecutive purchases!!**)\n(next is the time in between, we will pick the one will more space in between so we will include the least possible amount of other user to fulfil the concentration rate)\n(**NOW we have all the suspected users and include the time period of one hour)\n\n5. Now we can perform the concentration rate calculation to check which period is order brushing\n(this is easy not much tricks)\n\n6. Now we filter out all the non-order bushing time, leave with all the order brushing time then just simply find the userid with maximum number\n(this is easy too)\n","c1221959":"export the data","12fff5d9":"Here we do some preprocessing of data\n1. drop meaningless data (orderid)\n2. convert to datetime to perform time calculation\n3. groupby shopid (separate the data in to each shopid)\n4. define some useful constant (1 hour and 1 sec) ","a8ffc839":"Now we find the possible order brushing period"}}