{"cell_type":{"0341e721":"code","828011cb":"code","ae9c60bb":"code","8232db04":"code","f208f1ff":"code","68a64f2d":"code","4c7cc2b7":"code","c0068e43":"code","07e38da4":"code","3c0ede21":"code","81629b7c":"code","1e857495":"code","25f6b8b2":"code","ddea9d58":"code","8a721972":"code","9124c798":"markdown","8e3475aa":"markdown","44b45ae0":"markdown","afe91c79":"markdown","2aad6206":"markdown","811e8f09":"markdown","ce098eda":"markdown","6630d29f":"markdown","91dd1573":"markdown","9cb84d0d":"markdown","81527163":"markdown","01582299":"markdown","1645bfd1":"markdown","e4c6867f":"markdown","1d5f7540":"markdown"},"source":{"0341e721":"import math\nfrom collections import Counter\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport scipy.stats as ss\nimport matplotlib.pyplot as plt\nimport sklearn.preprocessing as sp\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom subprocess import check_output\n\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\ndata = pd.read_csv('..\/input\/mushrooms.csv')\ndata.head()","828011cb":"data.shape","ae9c60bb":"data.isnull().values.any()","8232db04":"for feature in data.columns:\n    uniq = np.unique(data[feature])\n    print('{}: {} distinct values -  {}'.format(feature,len(uniq),uniq))","f208f1ff":"data = data.drop(['veil-type'], axis=1)","68a64f2d":"print('Known mushrooms: {}\\nUnique mushrooms: {}'.format(len(data.index),len(data.drop_duplicates().index)))","4c7cc2b7":"print('Known mushrooms: {}\\nMushrooms with same features: {}'.format(\n    len(data.index),len(data.drop_duplicates(subset=data.drop(['class'],axis=1).columns).index)))","c0068e43":"def conditional_entropy(x,y):\n    # entropy of x given y\n    y_counter = Counter(y)\n    xy_counter = Counter(list(zip(x,y)))\n    total_occurrences = sum(y_counter.values())\n    entropy = 0\n    for xy in xy_counter.keys():\n        p_xy = xy_counter[xy] \/ total_occurrences\n        p_y = y_counter[xy[1]] \/ total_occurrences\n        entropy += p_xy * math.log(p_y\/p_xy)\n    return entropy\n\ndef theil_u(x,y):\n    s_xy = conditional_entropy(x,y)\n    x_counter = Counter(x)\n    total_occurrences = sum(x_counter.values())\n    p_x = list(map(lambda n: n\/total_occurrences, x_counter.values()))\n    s_x = ss.entropy(p_x)\n    if s_x == 0:\n        return 1\n    else:\n        return (s_x - s_xy) \/ s_x","07e38da4":"theilu = pd.DataFrame(index=['class'],columns=data.columns)\ncolumns = data.columns\nfor j in range(0,len(columns)):\n    u = theil_u(data['class'].tolist(),data[columns[j]].tolist())\n    theilu.loc[:,columns[j]] = u\ntheilu.fillna(value=np.nan,inplace=True)\nplt.figure(figsize=(20,1))\nsns.heatmap(theilu,annot=True,fmt='.2f')\nplt.show()","3c0ede21":"sns.set(rc={'figure.figsize':(15,8)})\nax=sns.countplot(x='odor',hue='class',data=data)\nfor p in ax.patches:\n    patch_height = p.get_height()\n    if np.isnan(patch_height):\n        patch_height = 0\n    ax.annotate('{}'.format(int(patch_height)), (p.get_x()+0.05, patch_height+10))\nplt.show()","81629b7c":"no_odor = data[data['odor'].isin(['n'])]\nfor j in range(0,len(columns)):\n    u = theil_u(no_odor['class'].tolist(),no_odor[columns[j]].tolist())\n    theilu.loc[:,columns[j]] = u\ntheilu.fillna(value=np.nan,inplace=True)\nplt.figure(figsize=(20,1))\nsns.heatmap(theilu,annot=True,fmt='.2f')\nplt.show()","1e857495":"sns.set(rc={'figure.figsize':(15,8)})\nax=sns.countplot(x='spore-print-color',hue='class',data=no_odor)\nfor p in ax.patches:\n    patch_height = p.get_height()\n    if np.isnan(patch_height):\n        patch_height = 0\n    ax.annotate('{}'.format(int(patch_height)), (p.get_x()+0.05, patch_height+10))\nplt.show()","25f6b8b2":"no_odor_w = no_odor[no_odor['spore-print-color'].isin(['w'])]\n(len(data.index) - len(no_odor_w.index)) \/ len(data.index)","ddea9d58":"factorized_nw = no_odor_w.copy()\nfor column in factorized_nw.columns.values:\n    f, _ = pd.factorize(factorized_nw[column])\n    factorized_nw.loc[:,column] = f\nohe = sp.OneHotEncoder()\nX = factorized_nw.drop(['class'],axis=1)\ny = factorized_nw['class'].tolist()\nohe.fit(X)\nX = ohe.transform(X).toarray()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\nfor i in range(1,6):\n    tree = DecisionTreeClassifier(max_depth=i, random_state=42)\n    tree.fit(X_train,y_train)\n    y_pred = tree.predict(X_test)\n    print(\"Max depth: {} - accuracy:\".format(i), accuracy_score(y_test, y_pred, normalize=True))","8a721972":"print(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))","9124c798":"Ok, so we know about 8124 mushrooms, with 22 different attributes (23 includes the `class` column). That's a lot! You can't possibly memorise all this data. Let's try to break it down. What else do we now of our data?","8e3475aa":"Seems like we got we wanted, and we only used 50% of the data for training - so we have half of the data as our test set. Let's see some more numbers:","44b45ae0":"Now the `spore-print-color` seems like a helpful feature: ","afe91c79":"# Alone in the woods\nYou shouldn't have taken that bet. \n\nYou'll never say that to anyone if you'll ever get out of this forest - but now you should probably focus on that _getting out of the forest_ part.\n\nYes, you're an awesome navigator, but so deep in an unknown forest, where you can hardly see the sun, even your great skills are can't give you a clue about where you're headed.\n_\"I have a map\"_, you said before taking the first step into the woods. True, but you lost it when you almost drowned in that river four days ago. The map didn't said how deep it was.\n\nBut you're here now, and you can still win that bet - if you'll get out here soon. But you're out of food, and you haven't seen a single fruit tree since you day one - and boy, you're a lousy hunter. \nBut hey, there are mushrooms all around. hundreds, if not thousands. You can eat those - if you'd only know which ones you can. Well, can you?\n\n_\"This bet isn't worth this\"_ you think. Yes, that's probably right.","2aad6206":"No, good. Wait! Are there any mushrooms with the same features but different classes?","811e8f09":"Ok, that's great! We can tell if a mushroom is ediable or not just by smelling it! But what about all those mushrooms with no odor (`odor='n'`)? Let's do the same trick again, but this time only for the data of these mushrooms: ","ce098eda":"Hmmm.. that's interesting - it seems as all known mushrooms have the same `veil-type`. We can drop this feature then, it won't help us.","6630d29f":"Beore we continue, let's check if there are any duplicates in our data - meaning, do we know of two or more mushrooms with exactly the same features?","91dd1573":"Seems like `odor` can be useful. Lets' take a look at that:","9cb84d0d":"So we can now determine the class of 92% of the mushrooms we know of. \n\n**Let's summarize our findings:**\n* if the mushroom smells like almond (`a`) or anise (`l`), it's ediable.\n* if it has no odor, check its spore color - if it's not green (`r`), it's ediable. If it's white, we're not sure.\n\n### A little ML to speed things up:\nWe can keep going like this, but 92% percent classified mushroom is enough to help you find some food in the woods - and you only to remeber two things.\n\nFor the remaining 8%, let's use a Decision Tree (well, you are in the forest, aren't you?). The tree will pretty mush do what we did so far - ask the right questions to slice the data in the best way for classifications. Let's see how many branches it needs to get to perfect classification. For this, we'll need to factorize the remaining data and use one-hot encoding.","81527163":"Let's see how many mushrooms we know about, and what do we know about them:","01582299":"Amazing! We got it all classified. Come on, now - no more excuses, you got this bet now! **Good luck, and bon appetit!**","1645bfd1":"Great success! We can tell odorless mushrooms apart by their spore color - unless it's white, then it's still ambiguous.\n\nLet's see how many mushrooms are we already able telling if they're ediable or not:","e4c6867f":"Ok, that's good. That's really good. Being unable to distinguish mushrooms can be deadly. \n\nTime to start diving deeper into the data. Let's see if there's a feature that can a better notion about the mushromm's class. More formealy, we want to know if there's a certain feature that maximizes the information about the class. For this, we will use **[Theil's U](https:\/\/en.wikipedia.org\/wiki\/Uncertainty_coefficient)**, also known as the _Uncertainty Coefficient_. Formaly marked as `U(x|y)`, this coefficient provides a value in the range of `[0,1]`, where 0 means that feature `y` provides no information about feature `x`, and 1 means that feature `y` provides full information abpout features `x`'s value.","1d5f7540":"Good, no missing data points. What's the varience of each of the features?"}}