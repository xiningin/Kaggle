{"cell_type":{"2209000d":"code","73682e9c":"code","7a55cc36":"code","022f51e2":"code","2e372076":"code","d1297388":"code","8e7e3abc":"code","95f0ef56":"code","e3ebe28e":"code","cdd27645":"code","e14b034c":"code","02748189":"code","ab90c6e7":"code","8fa24173":"code","f19704bd":"code","323686b2":"code","803f7f60":"code","49a5d5d7":"code","92b9d421":"code","17751324":"code","65e8cec9":"code","978111f7":"code","5556c805":"code","cc46edd2":"code","415b243c":"code","610633e9":"code","f04d86b5":"code","619f8187":"code","7ef33a34":"code","5c86d655":"code","30d5fa46":"code","54f63a39":"code","305f641b":"code","1af00f6c":"code","91b44fce":"code","791af280":"code","1b1d22c9":"code","27e01666":"code","5216801d":"code","c784626b":"code","3219b51b":"code","494eb05c":"code","15aba47b":"code","5cf4b79f":"code","1caa96c1":"code","bf0fa16f":"code","edbe9dee":"code","52cf8cf4":"code","3263ba04":"code","9cd31392":"code","2c9e08c4":"code","5624cce4":"code","44dc9790":"code","2a0bb638":"code","0d881162":"code","999e98c0":"code","c603e92c":"code","8a8b3f8f":"code","f23a70e0":"code","d0a5e0e9":"code","fbdec8b6":"code","b0ef2690":"code","dfc1827e":"code","76056c6e":"code","ac77b4eb":"code","485f968e":"code","3d4bc904":"code","fb21d2ec":"code","606d8a27":"code","47b1814a":"code","203a5940":"code","1e053819":"code","d6f33d3d":"code","2c7b5708":"code","d8684e2f":"code","22be5d0e":"code","39be2c18":"code","ade9d43c":"code","0ffc8fe7":"code","36ae9902":"code","d32c48d8":"code","443888df":"code","2e56168b":"code","4be16ef1":"markdown","fc01041c":"markdown","f9cf3361":"markdown","c988b8b0":"markdown","41650e07":"markdown","07428091":"markdown","75103aa3":"markdown","bf0f1757":"markdown","22d59188":"markdown","0ed8126d":"markdown","b4e8da88":"markdown","93fc7601":"markdown"},"source":{"2209000d":"# Upload the data\nfrom google.colab import files\n\nuploaded = files.upload()","73682e9c":"!pip install tensorflow_text","7a55cc36":"# Import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom collections import OrderedDict\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom gensim.summarization.summarizer import summarize\nfrom gensim.summarization import keywords\nfrom sklearn.preprocessing import MultiLabelBinarizer\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_hub as hub\nfrom tensorflow_hub import KerasLayer\nimport tensorflow_text as text\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import classification_report, f1_score, precision_score, recall_score, roc_auc_score, accuracy_score, hamming_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, StackingClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nimport pickle\nimport time","022f51e2":"import nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","2e372076":"# Read the csv file\ndata = pd.read_csv(\"\/content\/dineout_data_cleaned_final.csv\")","d1297388":"# Check for null values\ndata.isnull().sum()","8e7e3abc":"# Check the distribution of Votes and Reviews where Stars are null\n\ndata[data['Stars'].isnull() == True][['Vote_Count', 'Review_Count']].hist()\nplt.show()","95f0ef56":"data['Stars'].fillna(0, inplace=True)","e3ebe28e":"# drop the null values for address\ndata.dropna(axis=0, inplace=True)","cdd27645":"# Cuisine analysis\n\n# Store all the cuisines in a list\ncuisine_list = []\nfor cuisines in data['Cuisine']:\n  for cuisine in cuisines.split(','):\n    cuisine_list.append(cuisine.lower()[1:])\n\n# Convert into set for only uniques values\ncuisine_set = set(cuisine_list)\nprint(\"Total number of cuisines in the data is {}\".format(len(cuisine_set)))\nprint()\n\n# Convert the list into array for visualising the occurence of each cuisin\ncuisine_array = np.asarray(cuisine_list)\n                           \n# Plot the bar chart\nlabels = np.unique(cuisine_array, return_counts=True)[0]\nvalues = np.unique(cuisine_array, return_counts=True)[1]\n\n# Create a dictionary of the cuisine and its count\ncuisine_count = {}\nfor cuisine, count in zip(labels, values):\n  cuisine_count[cuisine] = count\n\n# sort the dictionay bye the key\ncuisine_count = OrderedDict(sorted(cuisine_count.items(), key = lambda x : x[1], reverse=True))\n\nplt.figure(figsize=(20,5))\nplt.style.use('ggplot')\nplt.bar(x=cuisine_count.keys(), height=cuisine_count.values())\nplt.xlabel(\"Cuisines\")\nplt.ylabel(\"Cuisine Count\")\nplt.title(\"Frequency of each cuisine across the data\")\nplt.xticks(rotation=-90)\nplt.show()","e14b034c":"# Distribution of restaurants across number of outlets\n\n'''Considering only those restaurants which have more than 10 outlets, here I am considering the restaurant count as a proxy for successful business.'''\n\nrest_count = data.groupby('Name').count()['Cost_for_2'].to_dict()\nrest_count_sorted = OrderedDict(sorted(dict([(rest, count) for rest, count in rest_count.items() if count > 10]).items(), key=lambda x:x[1], reverse=True))\n\nx = rest_count_sorted.keys()\nheight = rest_count_sorted.values()\n\n# Plot the bar graph\nplt.figure(figsize=(20,5))\nplt.bar(x=x, height=height)\nplt.xlabel(\"Restaurants\")\nplt.ylabel(\"Restaurant Count\")\nplt.xticks(rotation=-90)\nplt.title(\"Restaurant Count Distribution\")\nplt.show()","02748189":"# Creating a new feature \"Restaurant_Count\"\ndata['Restaurant_Count'] = data['Name'].apply(lambda x : rest_count[x])\n\n# Filter the data with restaurant count > 10\nfrequent_rest = data[data['Restaurant_Count'] > 10].reset_index().drop('index', axis=1)\nothers_rest = data[data['Restaurant_Count'] <= 10].reset_index().drop('index', axis=1)","ab90c6e7":"# Ditribution of average cost for 2, stars, vote count, review_count for restaurants having > 10 outlets\ncols_to_consider = ['Cost_for_2', 'Stars', 'Vote_Count', 'Review_Count']\n\n# Group the data by name and take the mean\ngrouped_data = frequent_rest.groupby('Name').mean()\n\nplt.figure(figsize=(20,30))\nfor i, col in enumerate(cols_to_consider):\n  plt.subplot(2, 2, i+1)\n  plt.bar(x=grouped_data[col].index, height=grouped_data[col])\n  plt.xlabel(col)\n  plt.xticks(rotation=-90)\n  plt.ylabel(\"Frequency\")\n  #plt.title(\"Distribution of mean {} for restaurants with >10 outlets\".format(col))\nplt.show()","8fa24173":"# Plot the count of the data points for both categories of the data\n\nh1 = frequent_rest.shape[0]\nh2 = others_rest.shape[0]\n\nplt.bar(x = ['>10 outlets', '<=10 outlets'], height = [h1, h2])\nplt.title(\"Number of Data points for the 2 categories of restaurants\")\nplt.show()","f19704bd":"'''\nNext, a comparison is done w.r.t Cost_for_2, stars, vote count, review count accross all the restaurants \nand those restaurant with more than 10 outlets and the restaurants having less than 10 outlets\n'''\n\n# 1. Cost for 2\nfig, ax = plt.subplots(1, 3, figsize=(20, 5))\n\nax[0].hist(data['Cost_for_2'])\nax[0].set_xlabel(\"Cost For 2\")\nax[0].set_ylabel(\"Frequency\")\nax[0].set_title(\"Distribution of Cost For 2 for all the restaurants\")\n\nax[1].hist(frequent_rest['Cost_for_2'])\nax[1].set_xlabel(\"Cost For 2\")\nax[1].set_ylabel(\"Frequency\")\nax[1].set_title(\"Distribution of Cost for restaurants with > 10 outlets\")\n\nax[2].hist(others_rest['Cost_for_2'])\nax[2].set_xlabel(\"Cost For 2\")\nax[2].set_ylabel(\"Frequency\")\nax[2].set_title(\"Distribution of Cost for restaurants with < 10 outlets\")\n\nfig.show()","323686b2":"# 2. Stars\nfig, ax = plt.subplots(1, 3, figsize=(20, 5))\n\nax[0].hist(data['Stars'])\nax[0].set_xlabel(\"Stars\")\nax[0].set_ylabel(\"Frequency\")\nax[0].set_title(\"Distribution of Stars for all the restaurants\")\n\nax[1].hist(frequent_rest['Stars'])\nax[1].set_xlabel(\"Stars\")\nax[1].set_ylabel(\"Frequency\")\nax[1].set_title(\"Distribution of Stars for restaurants with > 10 outlets\")\n\nax[2].hist(others_rest['Stars'])\nax[2].set_xlabel(\"Stars\")\nax[2].set_ylabel(\"Frequency\")\nax[2].set_title(\"Distribution of Stars for restaurants with < 10 outlets\")\n\nfig.show()","803f7f60":"# 3. Vote_Count\nfig, ax = plt.subplots(1, 3, figsize=(22, 5))\n\nax[0].hist(data['Vote_Count'], bins=50)\nax[0].set_xlabel(\"Vote Count\")\nax[0].set_ylabel(\"Frequency\")\nax[0].set_title(\"Distribution of Vote Count for all the restaurants\")\n\nax[1].hist(frequent_rest['Vote_Count'], bins=50)\nax[1].set_xlabel(\"Vote Count\")\nax[1].set_ylabel(\"Frequency\")\nax[1].set_title(\"Distribution of Vote Count for restaurants with > 10 outlets\")\n\nax[2].hist(others_rest['Vote_Count'], bins=50)\nax[2].set_xlabel(\"Vote Count\")\nax[2].set_ylabel(\"Frequency\")\nax[2].set_title(\"Distribution of Vote Count for restaurants with < 10 outlets\")\n\nfig.show()","49a5d5d7":"# Relation between stars and the cost for 2\nfig, ax = plt.subplots(1, 2, figsize=(20, 6))\n\nax[0].scatter(x=data['Stars'], y=data['Cost_for_2'])\nax[0].set_xlabel(\"Average Number of Starts\")\nax[0].set_ylabel(\"Cost For 2\")\nax[0].set_title(\"Cost For 2 v\/s Avg. Stars\")\n\nsns.boxplot(data=data, x='Stars', y='Cost_for_2', ax=ax[1])\nplt.xlabel(\"Average Number of Starts\")\nplt.xticks(rotation=-90)\nplt.ylabel(\"Cost For 2\")\nplt.title(\"Cost For 2 v\/s Avg. Starts\")\nplt.show()","92b9d421":"# Relation between stars and the vote count\nfig, ax = plt.subplots(1, 2, figsize=(20, 6))\n\nax[0].scatter(x=data['Stars'], y=data['Vote_Count'])\nax[0].set_xlabel(\"Average Number of Starts\")\nax[0].set_ylabel(\"Vote Count\")\nax[0].set_title(\"Vote Count v\/s Avg. Stars\")\n\n# Relation between stars and the Review count\nax[1].scatter(x=data['Stars'], y=data['Review_Count'])\nax[1].set_xlabel(\"Average Number of Starts\")\nax[1].set_ylabel(\"Review Count\")\nax[1].set_title(\"Review Count v\/s Avg. Stars\")\n\nfig.show()","17751324":"# Vote Count v\/s Cost_for_2\nplt.scatter(x=data['Vote_Count'], y=data['Cost_for_2'])\nplt.xlabel(\"Vote Count\")\nplt.ylabel(\"Cost For 2\")\nplt.title(\"Cost For 2 v\/s Vote Count\")\nplt.show()","65e8cec9":"# Variation of review count with cost for 2\nplt.scatter(data['Review_Count'], data['Cost_for_2'])\nplt.xlabel(\"Review Count\")\nplt.ylabel(\"Cost for 2\")\nplt.title(\"Cost for 2 v\/s Review Count\")\nplt.show()","978111f7":"# Variation of review count with vote count\nplt.scatter(data['Review_Count'], data['Vote_Count'])\nplt.xlabel(\"Review Count\")\nplt.ylabel(\"Vote Count\")\nplt.title(\" Review Count v\/s Vote Count\")\nplt.show()","5556c805":"# Plot the heatmap of Pearson Correlation matrix to quantify the same\ncorr_matrix = data.corr()\nplt.figure(figsize=(10, 10))\nsns.heatmap(corr_matrix, annot=True, cbar=False)\nplt.xticks(rotation=-90)\nplt.yticks(rotation=0)\nplt.title(\"Pearson Correlation Matrix\")\nplt.show()","cc46edd2":"# Extract Pincode from address\ndef extract_pincode(x):\n\n  try:\n    pincode = int(x[-6:])\n  except:\n    pincode = np.nan\n  \n  return pincode\ndata['Pincode'] = data['Address'].apply(lambda x : extract_pincode(x))","415b243c":"data[data['Pincode'].isnull() == True]","610633e9":"address1 = data[data['Pincode'].isnull() == True]['Address'].iloc[0]\naddress2 = data[data['Pincode'].isnull() == True]['Address'].iloc[1]","f04d86b5":"# fill the null values\nnull_indexes = list(data[data['Pincode'].isnull() == True].index)\nvalues = [110092, 110009]\n\nfor idx, val in zip(null_indexes, values):\n  data.at[idx, 'Pincode'] = val","619f8187":"data['Pincode'] = data['Pincode'].astype('str')","7ef33a34":"# Visualise the distribution of pincode\npincode_count = OrderedDict(sorted(dict(data['Pincode'].value_counts()).items(), key=lambda x : x[1], reverse=False))\npincode_count_10 = {pc : count for (pc, count) in pincode_count.items() if count > 10}\n\n# plot the bar graph\nplt.figure(figsize=(20, 20))\ny = pincode_count_10.keys()\nwidth = pincode_count_10.values()\n\nplt.barh(y=[*pincode_count_10], width=width)\nplt.xlabel(\"Pincode Count\")\nplt.ylabel(\"Pincodes\")\nplt.title(\"Pincode Count Distribution\")\nplt.show()","5c86d655":"# pincode wise distribution of Cost for 2\n\n# Create a column called pincode count and extract only the data for those pincodes having more than 50 occurrences\ndata['Pincode_Count'] = data['Pincode'].apply(lambda x : pincode_count[x])\ntop_pincodes = data[data['Pincode_Count'] >= 50]\n\n# Group the data by Pincodes and plot the Mean and Median stats for Cost for 2, Stars, Vote Count, Review Count\ngrouped_mean_data = top_pincodes.groupby('Pincode').mean()\n\nx = grouped_mean_data.sort_values(by='Cost_for_2', ascending=False).index\nheight = grouped_mean_data.sort_values(by='Cost_for_2', ascending=False)['Cost_for_2']\n\nfig, ax = plt.subplots(1, 3, figsize=(25, 5))\nax[0].bar(x=x, height=height)\nax[0].set_xlabel(\"Pincodes\")\nax[0].set_ylabel(\"Average cost for 2\")\nax[0].tick_params(rotation=-90, axis='x')\n\nx = grouped_mean_data.sort_values(by='Vote_Count', ascending=False).index\nheight = grouped_mean_data.sort_values(by='Vote_Count', ascending=False)['Vote_Count']\n\nax[1].bar(x=x, height=height)\nax[1].set_xlabel(\"Vote_Count\")\nax[1].set_ylabel(\"Average Vote Count\")\nax[1].tick_params(rotation=-90, axis='x')\n\nx = grouped_mean_data.sort_values(by='Review_Count', ascending=False).index\nheight = grouped_mean_data.sort_values(by='Review_Count', ascending=False)['Review_Count']\n\nax[2].bar(x=x, height=height)\nax[2].set_xlabel(\"Pincodes\")\nax[2].set_ylabel(\"Average Review Count\")\nax[2].tick_params(rotation=-90, axis='x')\n\nfig.show()","30d5fa46":"# Keyword analytics\n\n'''\nNext we extract keywords using Bag of words Pipeline from About, Type, Facilities, and Cuisine Columns.\n'''\n\ndef preprocess_facilites(x):\n  string_list = re.sub(r'[\\([{})\\'\\'\\]]', '',x).split(',')\n  string_list_final = []\n  for string in string_list:\n    string_list_final.append(string.lstrip())\n  return string_list_final\n\ndata['Facilities'] = data['Facilities'].apply(lambda x : preprocess_facilites(x))\n\nfacility_list = []\nfor facilities in data['Facilities']:\n  for facility in facilities:\n    facility_list.append(facility)","54f63a39":"# Generate wordcloud\nwordcloud_facility = WordCloud().generate(\" \".join(facility_list))\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud_facility)\nplt.axis('off')\nplt.title(\"Facility Word Count\")\nplt.show()","305f641b":"facility_count = {}\n\nfor (facility, count) in zip(np.unique(facility_list, return_counts=True)[0], np.unique(facility_list, return_counts=True)[1]):\n  facility_count[facility] = count\n\nfacility_count_10 = OrderedDict(sorted({k : v for k,v in facility_count.items() if v > 10}.items(), key = lambda x : x[1], reverse=True))","1af00f6c":"x = facility_count_10.keys()\nheight = facility_count_10.values()\n\n# Plot the bar graph for most frequent facilities\nplt.figure(figsize=(12,8))\nplt.bar(x=x, height=height)\nplt.xlabel(\"Top Facilities\")\nplt.ylabel(\"Facility Count\")\nplt.title(\"Top Facilities Count\")\nplt.xticks(rotation=-90)\nplt.show()","91b44fce":"# Extract only the text data\ntext_data = data[['Name', 'About', 'Type', 'Facilities', 'Cuisine']]","791af280":"# word cloud analysis of the Type feature\ndef preprocess_type(x):\n\n  type_list = x.split(',')\n  final_type_list = []\n  for word in type_list:\n    final_type_list.append(word.lstrip())\n\n  return final_type_list\n\ntext_data['Type'] = text_data['Type'].apply(lambda x : preprocess_type(x))\n\n# Create a list of keywords\ntype_list = []\nfor types in text_data['Type']:\n  for type_ in types:\n    type_list.append(type_)\n\n# Create a wordcloud\n\nwordcloud_type = WordCloud().generate(\" \".join(type_list))\nplt.figure(figsize=(10,5))\nplt.imshow(wordcloud_type)\nplt.axis('off')\nplt.title(\"Type Word Cloud\")\nplt.show()","1b1d22c9":"# keyword extraction in About Feature\n\nsw = stopwords.words('english')\n\ndef preprocess_text(x, remove_noise=False, remove_symbols=False, noise=None):\n\n  # Convert in lower_case\n  x = x.lower()\n\n  if remove_symbols:\n    x = re.sub(r'[^\\w]', ' ', x)\n\n  # tokenize the data\n  tokenized_sentence = word_tokenize(x)\n\n  # Remove the noise\n  if remove_noise:\n    try:\n\n      for i in range(len(noise)):\n        tokenized_sentence.remove(noise[i]) # noise in this case is no_about, no_facilities etc.\n      tokenized_sentence.remove('read le')\n      tokenized_sentence.remove('read less')\n    except:\n      pass\n\n  # Stopword removal\n  clean_sentence = [word for word in tokenized_sentence if word not in sw]\n\n  # lemmatizing\n  lemmatizer_object = WordNetLemmatizer()\n  lemmatized_sentence = [lemmatizer_object.lemmatize(word) for word in clean_sentence]\n\n  final_text = \" \".join(lemmatized_sentence)\n  return final_text\n\nclean_text = text_data['About'].apply(lambda x : preprocess_text(x, remove_noise=True, noise=['no_about'])).values.tolist()","27e01666":"# Remove duplicates and space\nclean_text = list(filter(None,clean_text))","5216801d":"clean_text_string = \" \".join(clean_text)","c784626b":"len(clean_text_string)","3219b51b":"# Extract minimum 10 keywords from every 200 words\nabout_keywords = []\ni = 0\nwhile (i  < len(clean_text_string)) and (i+200 < len(clean_text_string)):\n  top_keywords = keywords(clean_text_string[i :200+i], words=10)\n  about_keywords.append(top_keywords)\n  i = i + 200","494eb05c":"# Store the keywords in a file\nwith open(\"about_keywords.txt\",\"w\") as f:\n  f.write(\" \".join(about_keywords))\nf.close()","15aba47b":"# Prepare data for modelling\nclf_data = data[['Name', 'About', 'Type', 'Facilities', 'Cuisine']]","5cf4b79f":"# Preprocess the target variable, convert the strings into lower case, split on the basis of , and strip the first space\ndef preprocess_cuisine(x):\n  cuisine = x.split(',')\n  cuisine_list = []\n  for c in cuisine:\n    c = c.lstrip()\n    cuisine_list.append(c.lower())\n  return cuisine_list\n\n# Apply the above fn\nclf_data['Cuisine'] = clf_data['Cuisine'].apply(lambda x : preprocess_cuisine(x))","1caa96c1":"# Prepare the target variable with MultiLabelBinarizer\n\n# MultiLabelBinarizer Object\nmlb = MultiLabelBinarizer()\ntargets = mlb.fit_transform(clf_data['Cuisine'])","bf0fa16f":"clf_data['Facilities'] = clf_data['Facilities'].apply(lambda x : \", \".join(x))","edbe9dee":"text_data_clf = clf_data['Name'] + ' ' + clf_data['About'] + ' ' + clf_data['Type'] + ' ' + clf_data['Facilities'] ","52cf8cf4":"text_data_cleaned = text_data_clf.apply(lambda x : preprocess_text(x, remove_noise=True, remove_symbols=True, noise=['no_about', 'no_type', 'no_facilities', 'no_facility']))","3263ba04":"text_data_cleaned = text_data_cleaned.apply(lambda x : x.replace('read le', ''))","9cd31392":"# Convert into vectors using tfidf\ntfidf = TfidfVectorizer()\n\n# Create vectors\nword_vectors = tfidf.fit_transform(text_data_cleaned)","2c9e08c4":"# Split the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(word_vectors, targets, test_size=0.2, random_state=42)","5624cce4":"# Compute metrics\ndef compute_metrics(y_true, y_pred, model_name):\n\n  # 1. Accuracy score\n  acc = accuracy_score(y_true, y_pred)\n\n  # 2. Precision (Micro Average)\n  prec_micro = precision_score(y_true, y_pred, average='micro')\n\n  # 3. Precision (Macro Average)\n  prec_macro = precision_score(y_true, y_pred, average='macro')\n\n  # 4. Recall (Micro Average)\n  rec_micro = recall_score(y_true, y_pred, average='micro')\n\n  # 5. Recall (Micro Average)\n  rec_macro = recall_score(y_true, y_pred, average='macro')\n\n  # 6. f1 score (Micro)\n  f1_micro = f1_score(y_true, y_pred, average='micro')\n\n  # 7. f1 score (Macro)\n  f1_macro = f1_score(y_true, y_pred, average='macro')\n\n  # Return micro and macro metrics\n  micro_metrics = {'Accuracy_Score' : acc,\n                   'Precision_Micro' : prec_micro,\n                   'Recall_Micro' : rec_micro,\n                   'f1_Micro': f1_micro\n                  }\n  # Convert into dataframe\n  micro_metrics_df = pd.DataFrame(micro_metrics, index = [model_name])\n\n  macro_metrics = {'Accuracy_Score': acc,\n                   'Precision_Macro' : prec_macro,\n                   'Recall_Macro' : rec_macro,\n                   'f1_Macro' : f1_macro\n                   }\n  # Convert into dataframe\n  macro_metrics_df = pd.DataFrame(macro_metrics, index = [model_name])\n\n  return micro_metrics_df, macro_metrics_df\n# Build Models\ndef build_models(X_train, X_test, y_train, y_test):\n\n  # Prepare a model dictionary\n  models = {\"LogisticRegression\" : LogisticRegression(),\n            \"NaiveBayes\" : MultinomialNB(),\n            \"DecisionTree\" : DecisionTreeClassifier(),\n            \"RandomForest\" :  RandomForestClassifier(),\n            \"ExtraTrees\" : ExtraTreesClassifier(),\n            \"XGBoost\" : XGBClassifier(),\n            \"LightGBM\" : LGBMClassifier()\n            }\n\n  # Create a dataframe for storing results\n  train_micro_metrics = pd.DataFrame()\n  train_macro_metrics = pd.DataFrame()\n  test_micro_metrics = pd.DataFrame()\n  test_macro_metrics = pd.DataFrame()\n\n  for model_name, model in models.items():\n    \n    # Fit the model in oneVSrest Classifier\n    start_time = time.time()\n    clf = OneVsRestClassifier(model, n_jobs=-1).fit(X_train, y_train)\n    print(\"Building {} Model\".format(model_name))\n\n    # Compute metrics\n    train_prediction = clf.predict(X_train)\n    test_prediction = clf.predict(X_test)\n\n    micro_metrics_train, macro_metrics_train = compute_metrcis(y_train, train_prediction, model_name)\n    micro_metrics_test, macro_metrics_test = compute_metrics(y_test, test_prediction, model_name)\n\n    # Store the training metrics\n    train_micro_metrics = pd.concat((train_micro_metrics, micro_metrics_train))\n    train_macro_metrics = pd.concat((train_macro_metrics, macro_metrics_train))\n\n    # Store the testing metrics\n    test_micro_metrics = pd.concat((test_micro_metrics, micro_metrics_test))\n    test_macro_metrics = pd.concat((test_macro_metrics, macro_metrics_test))\n\n    end_time = time.time()\n    print(\"{} Model Built\".format(model_name))\n    print(\"Total time taken {:.3f} seconds\".format(end_time-start_time))\n    print()\n\n  return train_micro_metrics, train_macro_metrics, test_micro_metrics, test_macro_metrics","44dc9790":"# Call the function\ntrain_micro_metrics, train_macro_metrics, test_micro_metrics, test_macro_metrics = build_models(X_train, X_test, y_train, y_test)","2a0bb638":"test_micro_metrics = test_micro_metrics.add_suffix('_Test')","0d881162":"train_micro_metrics = train_micro_metrics.add_suffix('_Train')","999e98c0":"# Concat the train and test metrics\nmodel_metrics = pd.concat((train_micro_metrics, test_micro_metrics), axis=1).sort_values(by=['f1_Micro_Test','Precision_Micro_Test'], ascending=False)","c603e92c":"model_metrics[['f1_Micro_Train', 'f1_Micro_Test']]","8a8b3f8f":"model_metrics[['Precision_Micro_Train', 'Precision_Micro_Test']]","f23a70e0":"# 1. ExtraTrees Classifier\n\n# set parameters\nparams = {'n_estimators' : [50, 100, 150, 200, 250, 300, 500, 1000],\n          'criterion' : ['ginin', 'entropy'],\n          'max_depth' : [None, 10, 30, 50, 70, 100, 200, 500],\n          'min_samples_split' : [2, 3, 4, 5, 6, 7, 8, 9, 10],\n          'min_samples_leaf' : [1, 3, 5, 7, 9],\n          'min_weight_fraction_leaf' : np.arange(0, 1, 0.2, dtype='float'),\n          'max_features' : [\"auto\", \"sqrt\", \"log2\"],\n          'bootstrap' : [True, False],\n          'oob_score' : [True, False]\n          }\n\n# Default Model Object\next_trees_clf = ExtraTreesClassifier()\n\n# Randomized Search CV\nrscv_ext_clf = RandomizedSearchCV(ext_trees_clf, param_distributions=params, n_iter=100, scoring='f1_micro', n_jobs=-1, cv=3, verbose=4, random_state=42)\n\n# Fit the model\nrscv_ext_clf.fit(X_train, y_train)","d0a5e0e9":"# Extract the best estimator\next_best_clf = rscv_ext_clf.best_estimator_\n\n# Make Predictions\nrscv_pred_ext_train = ext_best_clf.predict(X_train)\nrscv_pred_ext_test = ext_best_clf.predict(X_test)\n\n# Compute Metrics\nmicro_metrics_rscv_df_train, macro_metrics_rscv_df_train = compute_metrics(y_train, rscv_pred_ext_train, \"ExtraTrees_Best\")\nmicro_metrics_rscv_df_test, macro_metrics_rscv_df_test = compute_metrics(y_test, rscv_pred_ext_test, \"ExtraTrees_Best\")","fbdec8b6":"# Concat the metrics\nmicro_metrics_rscv_df_train = micro_metrics_rscv_df_train.add_suffix('_Train')\nmicro_metrics_rscv_df_test = micro_metrics_rscv_df_test.add_suffix('_Test')\nht_model_metrics = pd.concat((micro_metrics_rscv_df_train, micro_metrics_rscv_df_test), axis=1).sort_values(by=['f1_Micro_Test', 'Precision_Micro_Test'], ascending=False)","b0ef2690":"# Merge the default scores and the tuned model\nmodel_metrics = pd.concat((model_metrics, ht_model_metrics), axis=0).sort_values(by = ['f1_Micro_Test', 'Precision_Micro_Test'], ascending=False)","dfc1827e":"model_metrics[['f1_Micro_Test', 'Precision_Micro_Test']]","76056c6e":"# Train the XGBoost Model on the entire data and save the model\nxgb_clf = OneVsRestClassifier(XGBClassifier())\nxgb_clf.fit(word_vectors, targets)","ac77b4eb":"# Save the xgboost model\npickle.dump(xgb_clf, open(\"XGBoost.pkl\",\"wb\"))","485f968e":"# Split the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(text_data_cleaned, targets, test_size=0.2)","3d4bc904":"# Links for the bert preprocesser and the bert encoder\nprep_link = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3\"\nbert_model_link = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/3\"\n\n# Download the model using the keras layer\nbert_preprocess = hub.KerasLayer(prep_link)\nbert_encoder = hub.KerasLayer(bert_model_link)\n\n\n# Function to generate word embeddings\ndef get_sentence_embedding(data):\n  preprocessed_text = bert_preprocess(data)\n  embeddings = bert_encoder(preprocessed_text)['pooled_output']\n  return embeddings","fb21d2ec":"n_outputs = y_train.shape[1]","606d8a27":"# Build the functional API\n\n# Input Layer\ntext_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='Text')\n\n# Embeddings generated using BERT Model\npreprocessed_text = bert_preprocess(text_input)\nembeddings = bert_encoder(preprocessed_text)['pooled_output']\n\n# Fully Connceted Layer\n#fc1 = tf.keras.layers.Dense(100, activation='relu')(embeddings)\n\n# Dropout Layer\ndropout = tf.keras.layers.Dropout(0.10)(embeddings)\n\n# Output Layer\noutput_layer = tf.keras.layers.Dense(n_outputs, activation='sigmoid', name=\"Output\")(dropout)\n\n# Build the model\nmodel = tf.keras.Model(inputs = [text_input], outputs = [output_layer])\n\n# print the model summary\nmodel.summary()","47b1814a":"# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Fit the model\nhist = model.fit(X_train, y_train, batch_size=64, epochs=400, validation_split=0.2)","203a5940":"plt.figure(figsize=(12,8))\nplt.plot(hist.history['loss'], label = 'Train Loss')\nplt.plot(hist.history['val_loss'], label = 'Validation Loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss: Binary Cross Entropy\")\nplt.title(\"Loss v\/s Epochs\")\nplt.legend()\nplt.show()","1e053819":"plt.figure(figsize=(12,8))\nplt.plot(hist.history['accuracy'], label = 'Train Accuracy')\nplt.plot(hist.history['val_accuracy'], label = 'Validation Accuracy')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy v\/s Epochs\")\nplt.legend()\nplt.show()","d6f33d3d":"# Save the model\nmodel.save('BERT_model_2.h5')","2c7b5708":"# Load the save model\nsaved_bert_model = tf.keras.models.load_model('\/content\/BERT_model_2.h5', custom_objects={'KerasLayer' : KerasLayer})","d8684e2f":"# Test and train predictions\ny_pred_bert = saved_bert_model.predict(X_test)\ny_pred_bert_train = saved_bert_model.predict(X_train)","22be5d0e":"# Compute train and test metrics\n\n# 1. Train metrics\ntrain_micro_metrics_bert, train_macro_metrics_bert = compute_metrcis(y_train, y_pred_bert_train.round(), 'BERT')\n\n# 2. Test metrics \ntest_micro_metrics_bert, test_macro_metrics_bert = compute_metrcis(y_test, y_pred_bert.round(), 'BERT')","39be2c18":"# Concatenate the results and prepare final results\ntrain_micro_metrics_bert = train_micro_metrics_bert.add_suffix('_Train')\ntest_micro_metrics_bert = test_micro_metrics_bert.add_suffix('_Test')\n\nbert_results = pd.concat((train_micro_metrics_bert, test_micro_metrics_bert), axis=1)\nfinal_results = pd.concat((model_metrics, bert_results), axis=0).sort_values(by=['f1_Micro_Test', 'Precision_Micro_Test'], ascending=False)","ade9d43c":"final_results[['f1_Micro_Test', 'Precision_Micro_Test']]","0ffc8fe7":"# Dump the MultiLabel Binarizer Object and the tfidf vectors as well\npickle.dump(mlb, open('MultiLabelBinarizer_obj.pkl', 'wb'))\npickle.dump(tfidf, open(\"TFIDF_Vectors.pkl\", 'wb'))","36ae9902":"# Make Prediction\ntest_input1 = \"McDonald's serves delicious range of Burgers and wraps. They also offer cripy fries with Piri piri mix. It is an American QSR.\"\ntest_input2 = \"PIND BALLUCHI is one of the most famous Punjabi, Multi Cuisine restaurant chains in India. It offers a wide variety of cuisines and dishes. It gives the feel of Punjab.Some of the most famous dishes are Butter Chicken, Chicken Tikka, Dal Makhani, Butter Naan, Sarso Da Saagi. It also offers chinese dishes like Noodles, Manchurian, Chilli Chicken and Chilli Potato. We offer dining as well as delivery service.\"","d32c48d8":"# Performance of XGBoost Model on test_input1\n\nprep_input1 = preprocess_text(test_input1, remove_symbols=True)\nvector_input1 = tfidf.transform([prep_input1])\nxgb_pred1 = xgb_clf.predict(vector_input1)\nxgb_labels1 = mlb.inverse_transform(xgb_pred1)\n\n# Performance of XGBoost Model on test_input2\nprep_input2 = preprocess_text(test_input2, remove_symbols=True)\nvector_input2 = tfidf.transform([prep_input2])\nxgb_pred2 = xgb_clf.predict(vector_input2)\nxgb_labels2 = mlb.inverse_transform(xgb_pred2)","443888df":"print(\"Cuisine Categories for test_input_1 are \", xgb_labels1[0])\nprint(\"Cuisine Categories for test_input_1 are \", xgb_labels2[0])","2e56168b":"# Performance of BERT Model on test_input1\nbert_predictions1 = saved_bert_model.predict([[test_input1]]).round()\nbert_labels1 = mlb.inverse_transform(bert_predictions1)\n\n# Performance of BERT Model on test_input2\nbert_predictions2 = saved_bert_model.predict([test_input2]).round()\nbert_labels2 = mlb.inverse_transform(bert_predictions2)\n\nprint(\"Cuisine Categories for test_input_1 are \", bert_labels1[0])\nprint(\"Cuisine Categories for test_input_1 are \", bert_labels2[0])","4be16ef1":"**Clearly, Stars column has null values where Vote_Count and Review_Count is 0**","fc01041c":"From the above plot it can be inferred that:\n\n\n1.   North Indian food is most preferred in Delhi-NCR\n2.   Fast Food is at the second position.\n3.   Chinese cuisine is at the 3rd position.\n\n","f9cf3361":"**Approximate Linear Relation can be observed between the Value Count and Review Count.**","c988b8b0":"## Machine Learning Models","41650e07":"\n\n1.   Pincode for address 1 is 110092\n2.   Pincdoe for address 2 is 110009\n\nSource: Google\n","07428091":"### Hyperparameter Tuning\n\n\n\nExtraTrees Classifier\n\n","75103aa3":"## BERT Model","bf0f1757":"preprocesser : https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3\n\nBERT_Model : https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/3","22d59188":"**Only 2 indexes have null values.**\n\nWe can use google to extract the pincode for these 2 locations","0ed8126d":"\n\n*   Restaurants having average stars between 4.1 and 4.5 have higher median and maximum cost for 2.\n*   Cost for 2 increases initially with increase in stars but then dip in cost can be observed with further increase in average number of stars.\n\n","b4e8da88":"# Cuisine Classification","93fc7601":"**A similar distribution for all the columns can be observed for all the 3 categories.**"}}