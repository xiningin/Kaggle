{"cell_type":{"de98e358":"code","f93c94b1":"code","b21eb5a7":"code","98ef3795":"code","23f46268":"code","9ab30739":"code","9fc6a4fe":"code","78266ae1":"code","1288a45a":"code","9545c63e":"code","7c5e48a1":"code","49f4f4c6":"code","4a129064":"code","16ca27a9":"code","3f17549d":"markdown","a726cd4d":"markdown"},"source":{"de98e358":"!pip install \/kaggle\/input\/humapsubmit\/timm-0.4.5-py3-none-any.whl\n!pip install \/kaggle\/input\/humapsubmit\/coolname-1.1.0-py2.py3-none-any.whl\n!pip install \/kaggle\/input\/humapsubmit\/runx-0.0.10-py3-none-any.whl","f93c94b1":"import sys\nsys.path.insert(0, \"\/kaggle\/input\/humapcode\/\")\nsys.path.insert(0, \"\/kaggle\/input\/humapsubmit\/\")\nsys.path.insert(0, \"\/kaggle\/input\/humapcode\/semantic_segmentation2\")","b21eb5a7":"import numpy as np, pandas as pd, os, random, cv2, gc\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nfrom torch.cuda.amp import autocast, GradScaler\nfrom pathlib import Path\nimport albumentations as A\nfrom matplotlib import pyplot as plt","98ef3795":"IMAGE_MEAN = [0.640, 0.473, 0.684]\nIMAGE_STD = [0.160, 0.227, 0.143]\n\n\n\ndef seed_torchv2(seed: int = 42) -> None:\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\n\n## kaggle help function\n# https:\/\/www.kaggle.com\/paulorzp\/rle-functions-run-lenght-encode-decode\ndef mask2rle(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\n\ndef rle2mask(mask_rle, shape=(1600, 256)):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (width,height) of array to return\n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T\n\n# https:\/\/www.kaggle.com\/bguberfain\/memory-aware-rle-encoding\n# with transposed mask\ndef rle_encode_less_memory(img):\n    # the image should be transposed\n    pixels = img.T.flatten()\n\n    # This simplified method requires first and last pixel to be zero\n    pixels[0] = 0\n    pixels[-1] = 0\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    runs[1::2] -= runs[::2]\n\n    return ' '.join(str(x) for x in runs)\n\n\n\n\ndef make_grid(shape, window=256, min_overlap=32):\n    \"\"\"\n        Return Array of size (N,4), where N - number of tiles,\n        2nd axis represente slices: x1,x2,y1,y2\n    \"\"\"\n    x, y = shape\n    nx = x \/\/ (window - min_overlap) + 1\n    x1 = np.linspace(0, x, num=nx, endpoint=False, dtype=np.int64)\n    x1[-1] = x - window\n    x2 = (x1 + window).clip(0, x)\n    ny = y \/\/ (window - min_overlap) + 1\n    y1 = np.linspace(0, y, num=ny, endpoint=False, dtype=np.int64)\n    y1[-1] = y - window\n    y2 = (y1 + window).clip(0, y)\n    slices = np.zeros((nx, ny, 4), dtype=np.int64)\n\n    for i in range(nx):\n        for j in range(ny):\n            slices[i, j] = x1[i], x2[i], y1[j], y2[j]\n    return slices.reshape(nx * ny, 4)\n","23f46268":"##  model\nfrom unet import Smp_Unet, Smp_Unetplusplus\nfrom semantic_segmentation2.network.deepv3 import DeepV3PlusSRNX50, DeepV3PlusSRNX101\n\n\nimport rasterio\nfrom rasterio.windows import Window\n","9ab30739":"os.listdir(\"\/kaggle\/input\/\")","9fc6a4fe":"class Config:\n    seed = 42\n    folds = 5\n\n    WINDOW = 1536\n    WINDOW_OVERLAP = WINDOW \/\/ 4\n\n    S_TH = 40\n    P_TH = 1000 * (WINDOW \/\/ 384) ** 2\n\n\n    root_dir = Path(\"\/kaggle\/input\/hubmap-kidney-segmentation\/test\/\")\n\n\n    dataset_params = {\n        \"test\": {\n            \"transformers\": A.Compose([\n\n            ]),\n            \"batch_size\": 16,\n            \"shuffle\": False,\n            \"pin_memory\": False,\n            \"num_workers\": 0,\n        },\n\n    }\n","78266ae1":"class HuMapTestDataset(Dataset):\n    def __init__(self, name, phase, transformers=None):\n        self.name = name\n        self.data = rasterio.open(name)\n        self.layers = []\n        if self.data.count != 3:\n            for i, subdataset in enumerate(self.data.subdatasets):\n                self.layers.append(rasterio.open(subdataset))\n        else:\n            assert self.data.count == 3, f\"{self.data.count}\"\n        \n        \n        self.phase = phase\n        self.transformers = transformers\n        self.window = Config.WINDOW\n        self.window_overlap = Config.WINDOW_OVERLAP\n        self.slices = make_grid((self.data.shape[1], self.data.shape[0]), window=Config.WINDOW, min_overlap=Config.WINDOW_OVERLAP)\n        self.S_TH = Config.S_TH\n        self.P_TH = Config.P_TH\n\n    def normalize(self, image):\n        image = image.astype(np.float32)\n        image = image \/ 255.\n        image -= IMAGE_MEAN\n        image \/= IMAGE_STD\n        return np.transpose(image, axes=[2, 0, 1])\n\n    def get_data_hw(self):\n        return self.data.shape[0], self.data.shape[1]\n\n    def __len__(self):\n        return len(self.slices)\n\n    def grab(self, slice):\n        x1,x2,y1,y2 = slice\n        if self.data.count == 3:\n            image = self.data.read([1, 2, 3], window=Window.from_slices((y1, y2), (x1, x2)))\n            image = np.moveaxis(image, 0, -1)\n        else:\n            image_l = []\n            for i, layer in enumerate(self.layers):\n                image_l.append(layer.read(1, window=Window.from_slices((y1, y2), (x1, x2))))\n            image = np.stack(image_l, axis=-1)\n\n        if image.shape[0] != self.window or image.shape[1] != self.window:\n            assert y2 == self.data.shape[0] or x2 == self.data.shape[1], f\"{y2} {self.data.shape[0]} {x2} {self.data.shape[1]}\"\n            pad0 = pad1 = 0\n            if image.shape[0] != self.window:\n                pad0 = self.window - image.shape[0]\n            if image.shape[1] != self.window:\n                pad1 = self.window - image.shape[1]\n            image = np.pad(image, ((0,pad0),(0,pad1),(0,0)), mode=\"constant\", constant_values=0)\n\n        assert image.shape[0] == image.shape[1] == Config.WINDOW and image.shape[\n            2] == 3, f\"{image.shape[0]} {image.shape[1]} {image.shape[2]}\"\n\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        h, s, v = cv2.split(hsv)\n\n\n        if (s > self.S_TH).sum() <= self.P_TH or image.sum() <= self.P_TH:\n            valid_image = False\n        else:\n            valid_image = True\n\n        return image, valid_image\n\n\n    def __getitem__(self, item):\n        sliceh = self.slices[item]\n        image, valid_image = self.grab(sliceh)\n\n        if self.transformers is not None:\n            aug = self.transformers(image=image)\n            image = aug[\"image\"]\n\n        return {\n            \"image\": self.normalize(image),\n            \"slice\": sliceh,\n            \"valid_image\": valid_image\n        }\n\n\ndef collect_fn(batches):\n    results = {}\n    for k in batches[0].keys():\n        results[k] = []\n    for batch in batches:\n        for k,v in batch.items():\n            if k == \"image\":\n                v = torch.from_numpy(v)\n            results[k].append(v)\n    if \"image\" in results.keys():\n        results[\"image\"] = torch.stack(results[\"image\"], dim=0)\n\n    return results\n\n\n\ndef get_dataloader(name, phase):\n    dataset_params = Config.dataset_params[phase]\n\n    datasets = HuMapTestDataset(name, phase, transformers=dataset_params[\"transformers\"])\n\n    dataloader = DataLoader(\n        datasets,\n        batch_size=dataset_params[\"batch_size\"],\n        shuffle=dataset_params[\"shuffle\"],\n        pin_memory=dataset_params[\"pin_memory\"],\n        num_workers=dataset_params[\"num_workers\"],\n        drop_last=False,\n        collate_fn=collect_fn,\n    )\n    return datasets, dataloader\n    ","1288a45a":"## classification model\nclass Flatten(nn.Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)\n\nclass AdaptiveConcatPool2d(nn.Module):\n    def __init__(self, sz=None):\n        super().__init__()\n        sz = sz or (1,1)\n        self.ap = nn.AdaptiveAvgPool2d(sz)\n        self.mp = nn.AdaptiveMaxPool2d(sz)\n    def forward(self, x):\n        return torch.cat([self.mp(x), self.ap(x)], 1)\n\ndef gem(x, p=3, eps=1e-6):\n    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1.\/p)\n\n\nfrom torch.nn import Parameter\nclass GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super(GeM,self).__init__()\n        self.p = Parameter(torch.ones(1)*p)\n        self.eps = eps\n\n    def forward(self, x):\n        return gem(x, p=self.p, eps=self.eps)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'\n\nfrom pytorch_image_models.timm.models.resnet import resnet18, resnet34, resnet50, resnet50d, resnext50_32x4d,\\\n    ssl_resnext50_32x4d, ssl_resnext101_32x4d, ssl_resnet18\nfrom pytorch_image_models.timm.models.senet import legacy_seresnext50_32x4d, legacy_seresnext101_32x4d, legacy_senet154\n\nfrom pytorch_image_models.timm.models.densenet import densenet121, densenet169\nfrom pytorch_image_models.timm.models.efficientnet import *\nfrom pytorch_image_models.timm.models.resnest import resnest50d, resnest50d_4s2x40d, resnest50d_1s4x24d, resnest101e\nfrom pytorch_image_models.timm.models.resnet import *\nfrom pytorch_image_models.timm.models.resnet import ecaresnet50d, ecaresnet50t\nfrom pytorch_image_models.timm.models.resnet import seresnext50_32x4d\nfrom pytorch_image_models.timm.models.res2net import *\nfrom pytorch_image_models.timm.models.res2net import res2net50_26w_4s\n\nfrom pytorch_image_models.timm.models.nfnet import *\n\nclass ResNest50OrgBackbone(nn.Module):\n    def __init__(self, name, pretrained=True):\n        super(ResNest50OrgBackbone, self).__init__()\n        print(torch.hub.list(\"zhanghang1989\/ResNeSt\"))\n        self.net = torch.hub.load(\"zhanghang1989\/ResNeSt\", name, pretrained=pretrained)\n        self.num_features = 2048\n\n    def forward(self, x):\n        x = self.net.conv1(x)\n        x = self.net.bn1(x)\n        x = self.net.relu(x)\n\n        x = self.net.maxpool(x)\n        x = self.net.layer1(x)\n        x = self.net.layer2(x)\n        x = self.net.layer3(x)\n        x = self.net.layer4(x)\n\n        return x\n    def forward_features(self, x):\n        x = self.net.conv1(x)\n        x = self.net.bn1(x)\n        x = self.net.relu(x)\n\n        x = self.net.maxpool(x)\n        x = self.net.layer1(x)\n        x = self.net.layer2(x)\n        x = self.net.layer3(x)\n        x = self.net.layer4(x)\n\n        return x\n\n\nclass Net(nn.Module):\n    def __init__(self, backbone, multidrop=False, pool_type=\"avg\", pretrained=False):\n        super(Net, self).__init__()\n        if backbone== \"resnest50\" or backbone == \"resnest101\" or backbone == \"resnest50_fast_1s1x64d\":\n            self.model = ResNest50OrgBackbone(backbone, pretrained=pretrained)\n        else:\n            self.model = eval(f\"{backbone}(pretrained=pretrained)\")\n\n        self.model_type_name = type(self.model).__name__\n\n        if pool_type == \"avg\":\n            self.avgpool = nn.AdaptiveAvgPool2d(1)\n        elif pool_type == \"max\":\n            self.avgpool = nn.AdaptiveMaxPool2d(1)\n        elif pool_type == \"gem\":\n            self.avgpool = GeM()\n        elif pool_type == \"none\":\n            self.avgpool = nn.Identity()\n        else:\n            self.avgpool = AdaptiveConcatPool2d(1)\n\n        self.multi_drop = multidrop\n        if self.multi_drop:\n            self.flat = Flatten()\n            self.dropouts = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.logits = nn.Linear(self.model.num_features, 1)\n        else:\n            self.logits = nn.Sequential(\n                Flatten(),\n                nn.Dropout(0.5),\n                nn.Linear(self.model.num_features, 1)\n            )\n\n    def forward(self, x):\n        x = self.model.forward_features(x)\n        x = self.avgpool(x)\n\n        if self.multi_drop:\n            x = self.flat(x)\n            for i, dropout in enumerate(self.dropouts):\n                if i == 0:\n                    out = self.logits(dropout(x))\n                else:\n                    out += self.logits(dropout(x))\n            out \/= len(self.dropouts)\n            x = out\n        else:\n            x = self.logits(x)\n        return x\n","9545c63e":"def get_trans(img, I):\n\n    if I >= 4:\n        img = img.transpose(2, 3)\n    if I % 4 == 0:\n        return img\n    elif I % 4 == 1:\n        return img.flip(2)\n    elif I % 4 == 2:\n        return img.flip(3)\n    elif I % 4 == 3:\n        return img.flip(2).flip(3)\n\n    \ndef get_trans_invert(img, I):\n    if I % 4 == 0:\n        img = img\n    elif I % 4 == 1:\n        img = img.flip(2)\n    elif I % 4 == 2:\n        img = img.flip(3)\n    elif I % 4 == 3:\n        img = img.flip(3).flip(2)\n\n    if I >= 4:\n        img = img.transpose(2, 3)\n\n    return img\n\ndef model_predict_wrapper(model, inputs, model_mode, n_tta=1):\n    assert model_mode in [\"cls\", \"seg\"]\n    oups = []\n    for I in range(n_tta):\n        oup = model(get_trans(inputs, I))\n        if model_mode == \"cls\":\n            oups.append(oup)\n        else:\n            oups.append(get_trans_invert(oup, I))\n    return sum(oups) \/ len(oups)\n\n    \n    ","7c5e48a1":"CLS_THRESHOLD = 0.42\nMASK_THRESHOLD = 0.5\nprint(\"window is {}  overlap is {}  mask_threshold is {}\".format(Config.WINDOW, Config.WINDOW_OVERLAP, MASK_THRESHOLD))\n\nTTA = 8","49f4f4c6":"def submit():\n    print()\n    print(\"your window is {}  overlap-window is {}\".format(Config.WINDOW, Config.WINDOW_OVERLAP))\n    print(\"cls threshold is \", CLS_THRESHOLD)\n    print(\"mask threshold is \", MASK_THRESHOLD)\n    print()\n\n    # size window window-overlap\n    cls_log_dirs_model_name = [\n#         [Path(\"\/kaggle\/input\/humapsubmit3\/eca_nfnet_l1_focal_384_1536_384_multidrop_avg\/\"), \"eca_nfnet_l1\", 384],\n#         [Path(\"\/kaggle\/input\/humapsubmit3\/resnest50d_focal_384_1536_384_multidrop_avg\/\"), \"resnest50d\", 384],\n#         [Path(\"\/kaggle\/input\/humapsubmit3\/tf_efficientnet_b3_ns_focal_384_1536_384_multidrop_avg\/\"), \"tf_efficientnet_b3_ns\", 384],\n    ]\n\n    seg_log_dirs_model_name = [\n        [Path(\"\/kaggle\/input\/humapsubmitpseudo\/eca_nfnet_l1_bce_384_1536_384\"), \"eca_nfnet_l1\", 384],\n        [Path(\"\/kaggle\/input\/humapsubmitpseudo\/resnest50d_bce+lovasz_384_1536_384\"), \"resnest50d\", 384],\n        [Path(\"\/kaggle\/input\/humapsubmitpseudo\/resnest50d_bce01lovasz_768_1536_384\"), \"resnest50d\", 768],\n    ]\n\n    print(\"cls model loading!!!!\")\n    cls_models = []\n    for cls_log_dir, cls_model_name, size in cls_log_dirs_model_name:\n        for fold in [0,1,2,3,4 ]:\n            cls_model = Net(backbone=cls_model_name, pretrained=False, multidrop=True, pool_type=\"avg\")\n            cls_ckpt_path = str(cls_log_dir \/ f\"model_{fold}_swa.pth\")\n            print(\"cls ckpt path : \", cls_ckpt_path)\n            cls_model.load_state_dict(torch.load(cls_ckpt_path))\n            cls_model.cuda()\n            cls_model.eval()\n            cls_models.append([cls_model, size])\n\n    print(\"seg model loading!!!!!\")\n    seg_models = []\n    for seg_log_dir, seg_model_name, size in seg_log_dirs_model_name:\n        for fold in [0,1,2,3,4 ]:\n            seg_model = Smp_Unet(backbone=seg_model_name, pretrained=False, deepsup=False, center=False, kaggle=True)\n#             seg_model = eval(f\"{seg_model_name}(1, None, pretrained=False)\")\n            if \"768\" in str(seg_log_dir):\n                seg_ckpt_path = seg_log_dir \/ f\"model_{fold}.pth\"\n            else:\n                seg_ckpt_path = seg_log_dir \/ f\"model_{fold}_swa.pth\"\n            print(\"seg ckpt path : \", seg_ckpt_path)\n            seg_model.load_state_dict(torch.load(seg_ckpt_path))\n            seg_model.cuda()\n            seg_model.eval()\n            seg_models.append([seg_model, size])\n\n    subm = {}\n\n    names = list(Config.root_dir.glob(\"*.tiff\"))\n    print(\"the number of test samples is \", len(names))\n    for name_ind, name in enumerate(names[::-1]):\n        image_id = name.stem\n        print(\"image_id is \", image_id)\n        test_dataset, test_dl = get_dataloader(name, \"test\")\n\n        preds = np.zeros(test_dataset.get_data_hw(), dtype=np.uint8)\n\n        for bi, batch in enumerate(tqdm(test_dl)):\n            images = batch[\"image\"]\n            valid_inputs = batch[\"valid_image\"]\n            images = images[np.array(valid_inputs, dtype=np.bool)]\n\n            slices = np.array(batch[\"slice\"], dtype=np.int64)[np.array(valid_inputs, dtype=np.bool)]\n            #             assert ((slices[:, 1] - slices[:, 0]) == Config.WINDOW).all()\n            #             assert ((slices[:, 3] - slices[:, 2]) == Config.WINDOW).all()\n            assert images.size(2) == images.size(\n                3) == Config.WINDOW, f\"{images.size(2)} {images.size(3)} {Config.WINDOW}\"\n            if images.size(0) == 0:\n                continue\n\n            images = images.cuda()\n\n            cls_outputs = []\n            for model, size in cls_models:\n                if size != Config.WINDOW:\n                    inputs = F.interpolate(images, size=(size, size), mode=\"area\")\n                    output = model_predict_wrapper(model, inputs, \"cls\", TTA)\n                    output = torch.sigmoid(output)\n                    cls_outputs.append(output)\n                else:\n                    assert False\n\n            if len(cls_outputs) != 0:\n                cls_outputs = sum(cls_outputs) \/ len(cls_outputs)\n                cls_outputs = (cls_outputs > CLS_THRESHOLD).detach()\n\n                images = images[cls_outputs.squeeze(1)]\n                slices = slices[cls_outputs.squeeze(1).cpu().numpy()]\n                assert images.shape[0] == slices.shape[0], f\"{images.shape[0]} - {slices.shape[0]}\"\n                if images.size(0) == 0:\n                    continue\n\n            outputs = []\n            for model, size in seg_models:\n                if size != Config.WINDOW:\n                    inputs = F.interpolate(images, size=(size, size), mode=\"area\")\n                    #                     output = model(inputs)\n                    output = model_predict_wrapper(model, inputs, \"seg\", TTA)\n                    output = F.interpolate(output, size=(Config.WINDOW, Config.WINDOW), mode=\"bilinear\")\n                else:\n                    assert False\n                    inputs = images\n                    #                     output = model(inputs)\n                    output = model_predict_wrapper(model, inputs, TTA)\n                output = torch.sigmoid(output)\n                outputs.append(output)\n\n            outputs = sum(outputs) \/ len(outputs)\n\n            outputs = (outputs >= MASK_THRESHOLD).detach().cpu().numpy().astype(np.uint8)\n\n            assert len(outputs) == len(slices)\n            for o, sl in zip(outputs, slices):\n                x1, x2, y1, y2 = sl\n                try:\n                    preds[y1:y2, x1:x2] = (\n                                (o[0][:(y2 - y1), :(x2 - x1)].astype(np.uint8) + preds[y1:y2, x1:x2]) > 0).astype(\n                        np.uint8)\n                except:\n                    print(x1, x2, y1, y2)\n                    print(o[0].shape)\n                    assert False\n\n        subm[name_ind] = {\"id\": image_id, \"predicted\": rle_encode_less_memory(preds)}\n\n        del preds\n        del test_dl\n        gc.collect();\n\n    submission = pd.DataFrame.from_dict(subm, orient='index')\n    submission.to_csv('submission.csv', index=False)\n    submission.head()\n\n","4a129064":"seed_torchv2(42)\n\ntorch.set_grad_enabled(False)\n\n\nsubmit()\n","16ca27a9":"# sub = pd.read_csv(\"\/kaggle\/input\/humapresume\/submission_934.csv\")\n\n# sub.to_csv(\"submission.csv\", index=False)","3f17549d":"# config","a726cd4d":"# CONFIG"}}