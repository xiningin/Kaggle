{"cell_type":{"21ae1a6d":"code","90cac805":"code","209e7410":"code","4f3ce4b0":"code","2caa48cd":"code","9dd74fe2":"code","3b647ff5":"code","5e981a89":"code","15dd102a":"code","45d5e259":"code","0d983099":"code","4b98f763":"code","ce3e527e":"code","1b61e9c0":"code","f974305b":"code","336ed833":"code","3467cff0":"code","90f450df":"code","d3bbc47c":"code","dfb4f541":"code","2d35b26f":"code","533cd775":"code","86682ed8":"code","2d40b746":"code","2f0d874a":"code","b0376f67":"code","29fc6457":"code","7f4afe98":"code","aa3e5396":"code","77026332":"code","13b2f2a2":"code","1bbfd838":"code","bee42c92":"markdown","c1bd396c":"markdown","14d3870f":"markdown","1fe44747":"markdown","652cc259":"markdown","261531d5":"markdown","72b5784c":"markdown","7009c508":"markdown","63ea86db":"markdown","3d0c4109":"markdown","317688fb":"markdown","ade3d198":"markdown","977aef0d":"markdown","8857c3a2":"markdown"},"source":{"21ae1a6d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to loazz\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","90cac805":"#importing the dataset\ndf = pd.read_csv('\/kaggle\/input\/mushroom-classification\/mushrooms.csv')","209e7410":"#what does the data look like ?\ndf.head(5)","4f3ce4b0":"df.info()","2caa48cd":"for col in df.columns:\n    print( df[col].value_counts())","9dd74fe2":"# resolving the missing values defect\nmissing_stalk_root = (df['stalk-root'].value_counts()['?']\/df.shape[0])\nsns.displot( x = 'stalk-root',data = df)\nplt.show()\nprint(\"Over {} of the data for stalk root is missing\".format(missing_stalk_root*100))\nprint(\"It seems reasonable to drop the stalk root columns\")","3b647ff5":"df.drop(['stalk-root'],axis = 1,inplace = True)","5e981a89":"df.columns","15dd102a":"# let us find the unique values in each columns\nfor col in df:\n    print(col, df[col].unique())","45d5e259":"#count of poisonous and edible musrooms\nax = sns.countplot(x = \"class\",data = df)\nplt.show()\nprint(df['class'].value_counts())","0d983099":"#c orrelation of count of poi and non poi with each categorical feature \nfor col in df.columns[1:]:\n    plt.figure(figsize=(15,8))\n    ax = sns.countplot(x=\"class\", hue=col, data=df)\n    ax.set_title(col)\n    ax.legend(bbox_to_anchor= (0.9,1))\n    plt.show()","4b98f763":"#removing redundant features\ndf.drop('veil-type',axis = 1,inplace = True)","ce3e527e":"df = df.sample(frac=1).reset_index(drop=True)","1b61e9c0":"#performing one hot encoding\ndf_x = df.iloc[:,1:]\ndf_y = df.iloc[:,0]\n\n\n\ndf_x_orig = df_x\ndf_y_orig = df_y\n\ndef convert_categorical_to_binary(df,columns):\n    print(columns)\n    df_temp = pd.DataFrame()\n    n = df.shape[0];\n    for col in columns:\n        print(col)\n        vec = df[col].unique();\n        m = len(df[col].unique())-1;\n        if(m == 0):\n            continue;\n        cat2bin = np.zeros((n,m));\n        print(cat2bin.shape)\n        for i in range(n):\n            curr_category = df[col].loc[i]\n#             print(curr_category)\n            for j in range(m):\n                if(curr_category == vec[j]):\n                    cat2bin[i][j] = 1;\n                    break\n        df_temp = pd.concat((df_temp,pd.DataFrame(cat2bin)),axis = 1)\n    return df_temp\n\ndf_x = convert_categorical_to_binary(df,df.columns[1:])\n\ndf_y = convert_categorical_to_binary(df,[df.columns[0]])\n\ndf_x.columns = np.array([i for i in range(df_x.shape[1])])","f974305b":"from sklearn.feature_selection import SelectKBest, chi2\nf_p_values = chi2(df_x,df_y)\nf_p_values = pd.DataFrame(f_p_values[0])\n","336ed833":"# f_p_values.sort_index(ascending = False,inplace = True,axis = 0)\nf_p_values.columns = ['F Score']\nf_p_values.sort_values(ascending = False,by=['F Score'], inplace=True)\nf_p_values","3467cff0":"f_p_values[f_p_values['F Score'] >= 100].shape","90f450df":"filtered_index = f_p_values[f_p_values['F Score'] >= 100].index","d3bbc47c":"df_x = df_x[filtered_index]","dfb4f541":"train_size,test_size,cv_size = int(.7*df.shape[0]), int(.2*df.shape[0]),int(.1*df.shape[0])\nprint(train_size,test_size,cv_size)\n\nx_train,x_test,x_cv = df_x[:train_size],df_x[train_size:train_size+test_size],df_x[train_size+test_size:]\ny_train,y_test,y_cv = df_y[:train_size],df_y[train_size:train_size+test_size],df_y[train_size+test_size:]","2d35b26f":"from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score","533cd775":"#Applying k - nn\nfrom sklearn.neighbors import KNeighborsClassifier\nneighCount = []\nscores_test = []\nscores_train = []\nprecision_test =[]\nprecision_train = []\nrecall_test = []\nrecall_train = []\nf1_test = []\nf1_train = []\nfor nc in range(1,100):\n    neigh = KNeighborsClassifier(n_neighbors=nc)\n    neighCount.append(nc)\n    \n    neigh.fit(x_train, y_train.values.ravel())\n    \n    y_train_pred = neigh.predict(x_train)\n    y_test_pred = neigh.predict(x_test)\n    \n    precision_test.append(precision_score(y_test.values.ravel(),y_test_pred))\n    recall_test.append(recall_score(y_test.values.ravel(),y_test_pred))\n    f1_test.append(f1_score(y_test.values.ravel(),y_test_pred))\n    \n    precision_train.append(precision_score(y_train.values.ravel(),y_train_pred))\n    recall_train.append(recall_score(y_train.values.ravel(),y_train_pred))\n    f1_train.append(f1_score(y_train.values.ravel(),y_train_pred))\n    \n    scores_test.append(neigh.score(x_test, y_test.values.ravel()))\n    scores_train.append(neigh.score(x_train,y_train.values.ravel()))","86682ed8":"plt.figure(figsize=(8, 6), dpi=80)\nplt.plot(neighCount, scores_train, c = 'r',label = \"Train data\")\nplt.plot(neighCount,scores_test, c = 'b', label = \"Test data\")\nplt.title(\"Accuracy versus number of neighbours \")\nplt.legend()\nplt.show()","2d40b746":"np.argmax(scores_test)\nprint(\"Max accuracy for test set\" , scores_test[np.argmax(scores_test)])\nprint(\"max number of nearest neighbours for best score \" , neighCount[len(scores_test) - np.argmax(scores_test[::-1]) -1])","2f0d874a":"plt.figure(figsize=(8, 6), dpi=80)\nplt.plot(neighCount, f1_train, c = 'r',label = \"Train data\")\nplt.plot(neighCount,f1_test, c = 'b', label = \"Test data\")\nplt.title(\"f1-score versus number of neighbours \")\nplt.legend()\nplt.show()","b0376f67":"np.argmax(f1_test)\nprint(\"Max f1-score for test set\" , f1_test[np.argmax(f1_test)])\nprint(\"max number of nearest neighbours for best score \" , neighCount[len(f1_test) - np.argmax(f1_test[::-1]) -1])","29fc6457":"plt.figure(figsize=(8, 6), dpi=80)\nplt.plot(neighCount, precision_train, c = 'r',label = \"Train data\")\nplt.plot(neighCount , precision_test, c = 'b', label = \"Test data\")\nplt.title(\"precision versus number of neighbours \")\nplt.legend()\nplt.show()","7f4afe98":"np.argmax(precision_test)\nprint(\"Max f1-score for test set\" , precision_test[np.argmax(precision_test)])\nprint(\"max number of nearest neighbours for best score \"  , neighCount[len(precision_test) - np.argmax(precision_test[::-1]) -1])","aa3e5396":"plt.figure(figsize=(8, 6), dpi=80)\nplt.plot(neighCount, recall_train, c = 'r',label = \"Train data\")\nplt.plot(neighCount , recall_test, c = 'b', label = \"Test data\")\nplt.title(\"recall versus number of neighbours \")\nplt.legend()\nplt.show()","77026332":"np.argmax(recall_test)\nprint(\"Max recall-score for test set\" , recall_test[np.argmax(recall_test)])\nprint(\"max number of nearest neighbours for best score\" , neighCount[len(recall_test) - np.argmax(recall_test[::-1]) -1])","13b2f2a2":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nneigh = KNeighborsClassifier(n_neighbors=8)\nneigh.fit(x_train, y_train.values.ravel())\ny_test_pred = neigh.predict(x_test)\ncm = confusion_matrix(y_test,y_test_pred)\nprint(classification_report(y_test, y_test_pred))\nprint(cm)","1bbfd838":"#Evaluation on the cross validation set\nneigh = KNeighborsClassifier(n_neighbors=8)\nneigh.fit(x_train, y_train.values.ravel())\ny_cv_pred = neigh.predict(x_cv)\ncm = confusion_matrix(y_cv,y_cv_pred)\nprint(classification_report(y_cv, y_cv_pred))\nprint(cm)","bee42c92":"### what all will we analyse ??\n1. Count of poisonous and non poisonous mushrooms\n2. correlation of count of poi and non poi with each categorical feature ( hopefully get some insight out of that )","c1bd396c":"# Model training","14d3870f":"* each record seems contains only categorical data","1fe44747":"* Taking the number of nearest neightbours as 8 seems most apt in this case, since it gives the best f1-score on train and test size.","652cc259":"* The dataset is well balanced","261531d5":"### Conclusion - We have succesfully created a KNN based model for mushroom classification\n","72b5784c":"There seems to be just too many features, let us try to perform some dimensionality reduction technique","7009c508":"# EDA\n","63ea86db":"Before proceeding further, let us shuffle the dataset","3d0c4109":"For now let us take all parameters with f score greater than 100","317688fb":"# KNN","ade3d198":"### The set of redundant features are\n1. Veil-type \n","977aef0d":"### stalk-root has 2480 missing values","8857c3a2":"# Chi Square method for dimensionality reduction"}}