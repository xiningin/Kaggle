{"cell_type":{"dedf6d9a":"code","15fab49f":"code","c9edf419":"code","10237f01":"code","af52f6c0":"code","1915dbff":"code","6edf21db":"code","d0a5ebf1":"code","5ad18a64":"code","a36b9d5b":"code","5b1df4c0":"code","10e3525e":"code","9cc0d809":"code","8aed75a7":"code","08ff7d8d":"code","57a0ac9a":"code","3d2443c8":"code","d7991fbf":"code","675a45b8":"code","8a941cd1":"code","b4685087":"code","456ba07d":"markdown","233f5793":"markdown","6f4777d9":"markdown","dec1f0df":"markdown","16703c40":"markdown","6f4cef46":"markdown","b3dadf42":"markdown","22102c51":"markdown","caa4d6ca":"markdown"},"source":{"dedf6d9a":"#Import the necessary libraries\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy\nimport sklearn","15fab49f":"#We combine the data from \"Test.xlsx\" and \"Train.xlsx\"\n#This is because we want the data to undergo same transformation so that it will be easier for our model to make predictions for... \n#..\"Test.xlsx\"\ndata_train=pd.read_excel('..\/input\/Train.csv')\nprint(f'Train data contains {data_train.shape} rows X columns')\ndata_test=pd.read_excel('..\/input\/Test.csv')\nprint(f'Test data contains {data_test.shape} rows X columns')\ndata=pd.concat([data_train,data_test],axis=0,ignore_index=True)\nprint(f'Combined dataset contains {data.shape} rows X columns')\ndata.head()","c9edf419":"#Check for duplicate entries in the rows and in the ID coulmn\nx=data.duplicated(keep='first').sum()\nprint(f'Number of duplicate rows={x}')\nx=data['ID'].duplicated(keep='first').sum()\nprint(f'Number of IDs={x}')","10237f01":"#check the data types in each column to ensure that they are in line with the column name.. eg. \"Age\" should contain only numbers\ndata.dtypes","af52f6c0":"#Check for any inconsistencies in categorial features. We find that there are no inconsistencies! \nfor x in [\"Divison\",\"Battalion\",\"Education Stream\", 'Gender','Channel of Recruitment','Recommendation from Superior',\"Last Year's Performance Rating\"]:\n    print(data[x].value_counts())","1915dbff":"#Check whether there are any missing (NA) values in the data. The 5479 missing values in \"Recognition\" column can be ignored....\n#... as they are the values that we want to predict. There are a large number of missing values in \"Education Stream\"...\n#... and \"Last Year's Performance Rating\"\ndata.isnull().sum()","6edf21db":"#We replace the missing values by the most frequently appearing values in the columns \"Education Stream\" & \"Last Year's Performance Rating\"\ndata['Education Stream'].fillna('Arts',inplace=True) \ndata[\"Last Year's Performance Rating\"].fillna('Good',inplace=True)\ndata.isnull().sum()","d0a5ebf1":"#We check to see whether there are any inconsistencies in continuous features. We see no inconsistencies\ndata.describe()","5ad18a64":"#Now we drop the ID label as it is of no use in making predictions.\n#We convert the categorical variables into numerical variables using one hot encoding. The features that have no ordinal.....\n#...relatoinship are converted to dummy variables\ndata.drop(labels=\"ID\",inplace=True,axis=1)\ndata=pd.get_dummies(data=data,columns=[\"Divison\",\"Battalion\",\"Education Stream\",\"Gender\",\"Channel of Recruitment\"]);\ndata[\"Last Year's Performance Rating\"].replace(['Good', 'Sufficient', 'Very Good', 'Satisfactory', 'Excellent'],[2,0,3,1,4], inplace=True)\ndata[\"Recommendation from Superior\"].replace(['No','Yes'],[0,1],inplace=True)\nfrom sklearn.preprocessing import MinMaxScaler\nscl=MinMaxScaler((0,1),copy=False)\ndata[[\"Age\",\"Number of Operations Conducted\",\"Service Length(in years)\",\"Training Score(out of 100)\"]]= scl.fit_transform(data[[\"Age\",\"Number of Operations Conducted\",\"Service Length(in years)\",\"Training Score(out of 100)\"]])\ndata.head()","a36b9d5b":"#Now that the transformation is complete, we segregate the data into test and train\ndata_train=data.iloc[:(data_train.shape[0]),:]\ndata_test=data.iloc[:(data_test.shape[0]),:]\nprint(data_train.shape,data_test.shape)","5b1df4c0":"#We calculate the proportions of 1 i.e. Medal awarded and 0 i.e. Medal not awarded in the \"Recognition\" column\n#The result shows that the dataset is highly imbalanced (as expected), hence we would have to take this into consideration...\n#... while developing our model\ncount_rej=len(data_train[data_train['Recognition']==0])\ncount_accept = len(data_train[data_train['Recognition']==1])\npct_of_rej = count_rej\/(count_rej+count_accept)\nprint(\"Percentage of non-awardees\", float(\"{0:.2f}\".format(pct_of_rej*100)))\npct_of_accept = count_accept\/(count_rej+count_accept)\nprint(\"Percentage of awardees\", float(\"{0:.2f}\".format(pct_of_accept*100)))\nsns.countplot(x='Recognition',data=data_train,palette='hls')\nplt.show()","10e3525e":"#We plot the histograms for continuous variables to check if there are any outliers present\n#We can see from the plot that there are no outliers\ndata_train.hist(column=[\"Age\",\"Number of Operations Conducted\",\"Service Length(in years)\",\"Number of Awards Won in Past\",\"Training Score(out of 100)\"],figsize=(15,15), bins=40)\nplt.show()","9cc0d809":"#Multi-collinearity in the features can lead to creation of inferior models\n#We check the correlation between the features and find that the features are not highly correlated\ndata1 = data_train[[\"Age\",\"Number of Operations Conducted\",\"Service Length(in years)\",\"Number of Awards Won in Past\",\"Training Score(out of 100)\"]]\ncorrmat=data1.corr()\nfig=plt.figure(figsize=(7,5))\nsns.heatmap(corrmat,cmap=\"YlGnBu\",square=True, annot=True)\nplt.show()","8aed75a7":"y=data_train['Recognition']\nX=data_train.drop('Recognition',axis=1)\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nos = SMOTE(random_state=0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\ncolumns = X_train.columns\nos_data_train_X,os_data_train_y=os.fit_sample(X_train, y_train)\nos_data_train_X = pd.DataFrame(data=os_data_train_X,columns=columns)\nos_data_train_y= pd.DataFrame(data=os_data_train_y,columns=['y'])\nprint(\"length of oversampled data_train is \",len(os_data_train_X))\nprint(\"Number of non-awardees in oversampled data_train\",len(os_data_train_y[os_data_train_y['y']==0]))\nprint(\"Number of awardees\",len(os_data_train_y[os_data_train_y['y']==1]))\nprint(\"Proportion of awardees in oversampled data_train is \",len(os_data_train_y[os_data_train_y['y']==0])\/len(os_data_train_X))\nprint(\"Proportion of non-awardees in oversampled data_train is \",len(os_data_train_y[os_data_train_y['y']==1])\/len(os_data_train_X))\nX_train=os_data_train_X\ny_train=os_data_train_y.values.ravel()","08ff7d8d":"#We have to first optimise the hyperparameters of the XGBCLassifier\n#We use Random seach for this as it is computationally less expensive and our laptops don't have sufficient processing powers..\n#...to optimise parameters using gridsearchcv in less time\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom xgboost import XGBClassifier\nXGB=XGBClassifier()\nrandom_grid = {\n    'max_depth':[10,11,12],\n    'min_child_weight':[1,2],\n    'gamma':[i\/10.0 for i in range(1,5)],\n    'subsample':[i\/10.0 for i in range(6,10)],\n    'colsample_bytree':[i\/10.0 for i in range(6,10)],\n    'reg_alpha':[0.01, 0.05, 0.1]\n}\nxg=RandomizedSearchCV(estimator = XGB, param_distributions = random_grid, n_iter = 100, cv = 2, verbose=2, random_state=42, n_jobs = -1)\nxg.fit(X_train, y_train)","57a0ac9a":"#We get the best hyperparameters from the RandomSearchCV which can be used for our model\nprint(random_grid)\nxg.best_params_","3d2443c8":"#We train the model and test it using performance metrics such as classification report, confusion matrix and accuracy score..\n#...though accuracy score is not a relevent metric here as the data is imbalanced. Hence more importance should be given to...\n#...the weighted average F1 score from classification report. F! score for our model is 0.93\nfrom xgboost import XGBClassifier\nmodel=XGBClassifier(subsample= 0.8,\n reg_alpha= 0.05,\n min_child_weight= 1,\n max_depth= 12,\n gamma= 0.1,\n colsample_bytree= 0.8,objective= 'binary:logistic',n_jobs = -1, seed=27)\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)\nprint('Accuracy of classifier on test set: {:.2f}'.format(model.score(X_test, y_test)))\nfrom yellowbrick.classifier import ConfusionMatrix\ncm = ConfusionMatrix(model, classes=[0,1])\ncm.fit(X_train, y_train)\ncm.score(X_test, y_test)\ncm.poof()\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","d7991fbf":"#We also use ROC AUC as a metric for evaluating classificatio models. It gives us and idea of how good our model is as compared..\n#...to a random model that predicts the majority class every time.=\n#The ROC AUC is 0.69 for our model while that of a random model is 0.50\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nlogit_roc_auc = roc_auc_score(y_test, model.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='XGBClassifier (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","675a45b8":"#Drop the \"Recognition\" column from the test data to make predictions from the constructed model\ndata_test=data_test.drop('Recognition',axis=1)\ndata_test.shape","8a941cd1":"#Make predictions for the test data\npredictions=model.predict(data_test)\npredicted_data=pd.read_excel('..\/input\/Test.csv')\npredicted_data['Recognition']=predictions\npredicted_data.head()","b4685087":"predicted_data[\"Recognition\"].value_counts()","456ba07d":"# END OF CODE","233f5793":"## Section 2: Data Validation","6f4777d9":"# Predicting Recognition","dec1f0df":"## Section 5: Using XGBoost for fitting the training set\nXGBoost is an implementation of gradient boosted decision trees designed for speed and performance. The library is focused on computational speed and model performance.\n\nhttps:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html\n\nhttps:\/\/machinelearningmastery.com\/gentle-introduction-xgboost-applied-machine-learning\/\n","16703c40":"## Section 6: Making predictions for the test dataset","6f4cef46":"## Section 4: SMOTE (Synthetic Minority Oversampling Technique)\nThe data set that we are provided with is imbalanced; hence we use SMOTE algorithm(Synthetic Minority Oversampling Technique). It works by creating synthetic samples from the minor class (claim rejected) instead of creating copies. \n\nhttps:\/\/arxiv.org\/pdf\/1106.1813\n\nhttps:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.over_sampling.SMOTE.html\n","b3dadf42":"#save the file with predictions\nwriter = pd.ExcelWriter('Test.xlsx',engine='xlsxwriter')\npredicted_data.to_excel(index=False,excel_writer=writer,sheet_name='Sheet1')\nwriter.save()","22102c51":"## Section 3: Data preprocessing","caa4d6ca":"## Section 1: Importing relevant packages and preparing the data set\nNow we will import pandas to read our data from a xlsx file and manipulate it for further use. We will also use numpy to convert the data into a format suitable to feed our classification model. We'll use seaborn and matplotlib for visualizations. Other packages that we will require include sklearn and scipy."}}