{"cell_type":{"1bae0f2c":"code","054330cb":"code","b047a735":"code","7720a9d7":"code","133b0725":"code","0f5cb640":"code","2df7f3e9":"code","36d8708c":"code","6d38ae46":"code","3b187066":"code","b41f3a98":"code","ecbbf7ea":"code","7c022adc":"code","cc31bcbf":"code","13587b5e":"code","3e1292b2":"code","443e183b":"code","35421363":"code","6ce84861":"code","f3a17825":"code","ce9a6a19":"code","02c0e547":"code","d1ea4874":"code","b6ed0391":"code","e2f1e1c3":"code","4b2fbf0a":"code","06b4d8e3":"code","1d05059d":"code","43239942":"code","9c5f2ae2":"code","e567006b":"code","c73b5246":"code","46e45154":"code","5bbe5207":"code","b1cb16a1":"code","9437fd09":"code","f4a100ef":"code","610446e8":"code","1d1ce0b1":"code","2bf6a6ea":"code","0227bb5e":"code","89944494":"code","2d1dcf47":"code","ed5f74cd":"code","1b3c7908":"code","c94beda6":"code","432cc360":"code","3516eac9":"code","fd35c96a":"code","fd37f69f":"code","d34d9666":"code","42decf92":"code","3db07cb4":"code","04a50cd4":"code","3da80e13":"code","966bbd0c":"code","9df8c712":"code","de951961":"code","315525a7":"code","ab28fc8f":"code","1fa154e3":"code","1779fe4f":"code","005dfadb":"code","bec8b483":"code","28fa0bb4":"code","44cc4316":"code","d612d653":"code","75d6dfb1":"code","14827b9a":"code","da0b4b79":"code","ca41ed54":"code","ba2c1969":"code","7f5bb82f":"code","d3642881":"code","9f0a9bc1":"code","b92d9f49":"code","cf708a91":"code","74976bd1":"code","91a068b7":"code","7c0c9bd4":"code","fd000128":"code","67d6398c":"code","f0f9e83b":"code","8ff3bbf3":"code","0eff8bec":"code","0f4fdbf8":"code","a7271563":"code","74303f8b":"code","edd44d9b":"code","dc21a469":"code","4707a97b":"code","55674a09":"code","5f7bf924":"code","9638931c":"code","52106697":"code","3152063b":"code","d9344b34":"code","1b706cdd":"code","580301e2":"code","4c737b86":"code","22bec288":"code","075fe9a9":"code","3732b6bd":"code","9f267146":"code","f4b95ca7":"code","fc8e1d31":"code","ef56ba25":"code","b0500e75":"code","b26b9721":"code","73331bcd":"code","896bd377":"code","32002f43":"code","081e494c":"code","5eefe34b":"code","5c6190fa":"code","bad18699":"code","93b7d38d":"markdown","f5dd5884":"markdown","137b2fff":"markdown","9098d60c":"markdown","3f74bb44":"markdown","fa03e0e4":"markdown","dcf3b288":"markdown","a0dfa5b3":"markdown","016fd7c9":"markdown","4c37cc54":"markdown","043033a3":"markdown","9d914edf":"markdown","4b539c81":"markdown","657b238a":"markdown","444deacc":"markdown","8045f489":"markdown","4bce4536":"markdown","051d95ce":"markdown","82b89dbe":"markdown","e930d0c0":"markdown","daf2a07f":"markdown","d91e0ef2":"markdown","6e4b4d37":"markdown","5e2a8c88":"markdown","21f0e0ba":"markdown","3c46ee2f":"markdown","f99d8625":"markdown","c2c0ef24":"markdown","92a5f6e2":"markdown","6e3ea6ff":"markdown","99565b03":"markdown","190cbedd":"markdown","654bfc4b":"markdown","b5f2d052":"markdown","8c6a1bfd":"markdown","00394766":"markdown","c643757b":"markdown","79b2f6d7":"markdown","f3cd6824":"markdown","b87d2c3d":"markdown","e7d0d6e4":"markdown","40731fd9":"markdown","78b37057":"markdown","bd7a8055":"markdown","af9c195b":"markdown","34b27681":"markdown","5495faaa":"markdown","a28ee864":"markdown","f1a6cce8":"markdown","826d1df7":"markdown","e6c49554":"markdown","ed0ea894":"markdown","b3c68e59":"markdown","85132348":"markdown","b6836c09":"markdown","850dd990":"markdown","c3386601":"markdown","90b64ecb":"markdown","dd871a08":"markdown","d43112e7":"markdown","cf2f7302":"markdown","2cc2f535":"markdown","e9e3c9c7":"markdown","783ef78a":"markdown","005fc978":"markdown","75d5769d":"markdown","c778b56a":"markdown","312c757e":"markdown","4ed4b120":"markdown","995f6f73":"markdown","2b156bf2":"markdown","5a2a1e3a":"markdown","23ba97ee":"markdown","75f2721d":"markdown","8b10e83e":"markdown"},"source":{"1bae0f2c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\n\n#ignoring all the warnings because we don't need them\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsb.set()\nsb.set_style(\"white\")\n%matplotlib inline\nrcParams['figure.figsize'] = [9,6]\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","054330cb":"# let us bring our guns\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n\n# perform our final prediction on\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n# Save the Id\ntest_id = test['Id']","b047a735":"train.head(6)","7720a9d7":"# Lets check that test dataset has all the columns in train dataset except SalePrice\ndiff_train_test = set(train.columns) - set(test.columns)\ndiff_train_test","133b0725":"# looking at the decoration of our data\ntrain.columns","0f5cb640":"print((train.shape), (test.shape), '\\n')\nprint(\"Shape of train.csv: \", train.shape)\nprint(\"Shape of test.csv: \", test.shape)","2df7f3e9":"train.info()","36d8708c":"# missing_values_bool = train.isnull().any()\n# display(missing_values_bool[missing_values_bool == True])\n\nmissing_values = train.isnull().sum()\ndisplay(missing_values[missing_values>0].sort_values(ascending=False).to_frame(name='Missing Values'))","6d38ae46":"percent = ((train.isnull().sum()\/train.isnull().count()) * 100).sort_values(ascending=True)\npercent = percent[percent>0]\n\nplt.xticks(rotation=90); plt.title('Percent Missing Values')\nsb.barplot(x=percent.index, y=percent, palette=\"viridis\")","3b187066":"train['SalePrice'].describe()","b41f3a98":"plt.subplots(figsize=(10, 5))\nplt.figure(1)\nax = sb.distplot(train['SalePrice'], bins=30, fit=norm, color=\"mediumslateblue\")\nax.set(xlabel='SalePrice', ylabel='Frequency')\nplt.xticks(rotation=-45)\n\nplt.subplots(figsize=(10,5))\nplt.figure(2)\nstats.probplot(train['SalePrice'], plot=plt)","ecbbf7ea":"sell = train['SalePrice']\n\nprint(f\"Skewness: {sell.skew()}\")\nprint(f\"Kurtosis: {sell.kurt()}\")","7c022adc":"plt.figure(1); plt.title('Boxen plot of the Sales Prices')\nsb.boxenplot(train['SalePrice'],color=\"mediumslateblue\")\n\nplt.figure(2); plt.title('Violin plot of the Sales Prices')\nsb.violinplot(train['SalePrice'], color=\"mediumslateblue\")\n\nplt.figure(3); plt.title('Strip plot of the Sales Prices')\nsb.stripplot(train['SalePrice'], alpha=0.6, color=\"mediumslateblue\")","cc31bcbf":"#swarmplot of overall quality\nplt.subplots(figsize=(10, 6))\nplt.title('Overall Quality of homes')\nsb.swarmplot(train['OverallQual'], train['Id'], palette=\"viridis\")","13587b5e":"train['OverallQual'].value_counts()","3e1292b2":"# sb.pairplot(train[features_ver1])\n\ndata_seg1 = pd.concat([train['SalePrice'], train['OverallQual']], axis=1)\n\nplt.subplots(figsize=(9,5))\nplt.figure(1); plt.title(\"SalePrice vs Overall Quality\")\nsb.boxplot(x='OverallQual', y='SalePrice', data=data_seg1, color=\"mediumslateblue\")\n\nplt.subplots(figsize=(9,5))\nplt.figure(2); plt.title(\"SalePrice vs Overall Quality\")\nsb.lineplot(x='OverallQual', y='SalePrice', data=data_seg1, color=\"mediumslateblue\")","443e183b":"data_seg2 = pd.concat([train['SalePrice'], train['OverallCond']], axis=1)\n\nplt.subplots(figsize=(9,5))\nplt.figure(1); plt.title(\"SalePrice vs Overall Condition\")\nsb.boxplot(x='OverallCond', y='SalePrice', data=data_seg2, color=\"mediumslateblue\")\n\nplt.subplots(figsize=(9,5))\nplt.figure(2); plt.title(\"SalePrice vs Overall Condition\")\nsb.lineplot(x='OverallCond', y='SalePrice', data=data_seg2, color=\"mediumslateblue\")","35421363":"data_seg3 = pd.concat([train['SalePrice'], train['GrLivArea']], axis=1)\n\nplt.subplots(figsize=(10, 6))\nplt.figure(1); plt.title('SalePrice vs Above Ground Living Area')\nsb.scatterplot(x='GrLivArea', y='SalePrice', data=data_seg3, alpha=0.8, color=\"mediumslateblue\")","6ce84861":"# drop the outliers\n# we drop the rows containing value of GrLivArea greater than 4000 and \n# SalePrice less than 200000\n# these are huge outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<200000)].index)\n\nplt.subplots(figsize=(10,6))\nsb.scatterplot(train['GrLivArea'], train['SalePrice'], alpha=0.8, color=\"mediumslateblue\")","f3a17825":"plt.subplots(figsize=(10,6))\nsb.scatterplot(train['TotalBsmtSF'], train['SalePrice'], alpha=0.8, color=\"mediumslateblue\")","ce9a6a19":"# we change the graph size for this particular graph using subplots unlike rcParams which changes for the entire notebook\nplt.subplots(figsize=(20,8))\nplt.xticks(rotation=90)\nplt.figure(1); plt.title(\"SalePrice vs YearBuilt\")\nsb.stripplot(train['YearBuilt'], train['SalePrice'])\n\nplt.subplots(figsize=(20,8))\nplt.xticks(rotation=90)\nplt.figure(2); plt.title(\"SalePrice vs YearBuilt\")\nsb.boxenplot(train['YearBuilt'], train['SalePrice'])","02c0e547":"# this is a really important code section\n# look at how we compute the correlation with df.corr() function and then feed it to sb.heatmap()\n# play around by tweaking argument values\n\ncorrmat = train.corr()\nplt.subplots(figsize=(17,17))\nplt.title(\"Correlation Matrix\")\n# sb.heatmap(corrmat, vmax=0.9, vmin=0.5, square=True, cmap='YlGnBu')\nsb.heatmap(corrmat, vmax=0.9, square=True, cmap=\"Oranges\", annot=True, fmt='.1f', linewidth='.1')","d1ea4874":"# we can convert a series object to a dataframe using to_frame() method on the series\nimp_ftr = corrmat['SalePrice'].sort_values(ascending=False).head(11).to_frame()\n\nimp_ftr","b6ed0391":"# first graph\nplt.subplots(figsize=(5,8))\nplt.title('SalePrice Correlation Matrix')\nsb.heatmap(imp_ftr, vmax=0.9, annot=True, fmt='.2f', cmap=\"Oranges\", linewidth='.1')","e2f1e1c3":"degree_correlation = corrmat['SalePrice'].sort_values(ascending=False)\ndegree_correlation.to_frame()","4b2fbf0a":"plt.subplots(figsize=(10,12))\nplt.title('Correlation with SalePrice')\ndegree_correlation.plot(kind='barh', color=\"mediumslateblue\")","06b4d8e3":"plt.subplots(figsize=(15, 15))\nsb.heatmap(corrmat>0.8, annot=True, square=True, cmap=\"Oranges\", linewidth='.1')","1d05059d":"# Dropping unwanted features\ntrain.drop(columns=['GarageArea','TotRmsAbvGrd','GarageYrBlt','1stFlrSF'],axis=1,inplace=True) \ntest.drop(columns=['GarageArea','TotRmsAbvGrd','GarageYrBlt','1stFlrSF'],axis=1,inplace=True)","43239942":"degree_correlation = corrmat['SalePrice'].sort_values(ascending=False)\ndegree_correlation.to_frame()","9c5f2ae2":"numerical_data = train.select_dtypes(exclude=['object']).drop(['SalePrice', 'Id'],axis=1).copy()\ncategorical_data = train.select_dtypes(include=['object']).columns","e567006b":"fig = plt.figure(figsize=(17,22))\nfor i in range(len(numerical_data.columns)):\n    fig.add_subplot(9,4,i+1)\n    sb.distplot(numerical_data.iloc[:,i].dropna(), hist=False, kde_kws={'bw':0.1}, color='mediumslateblue')\n    plt.xlabel(numerical_data.columns[i])\nplt.tight_layout()\nplt.show()","c73b5246":"cmap = sb.cubehelix_palette(dark=.3, light=.8, as_cmap=True)\nfig1 = plt.figure(figsize=(17,22))\nfor i in range(len(numerical_data.columns)):\n    fig1.add_subplot(9, 4, i+1)\n    sb.scatterplot(numerical_data.iloc[:, i],train['SalePrice'], palette='spring', marker='+', hue=train['SalePrice'], legend=False)\nplt.tight_layout()\nplt.show()","46e45154":"columns = (len(categorical_data)\/5)+1\n\nfg, ax = plt.subplots(figsize=(18, 30))\n\nfor i, col in enumerate(categorical_data):\n    fg.add_subplot(columns, 5, i+1)\n    sb.countplot(train[col], palette='spring')\n    plt.xlabel(col)\n    plt.xticks(rotation=90)\n\nplt.tight_layout()\nplt.show()","5bbe5207":"train['SalePrice'] = np.log1p(train['SalePrice'])\n\nplt.subplots(figsize=(11, 5))\nplt.figure(1)\nax = sb.distplot(train['SalePrice'], bins=100, fit=norm, color=\"mediumslateblue\")\nax.set(xlabel='SalePrice', ylabel='Frequency')\nplt.xticks(rotation=-45)\n\nplt.subplots(figsize=(10,5))\nplt.figure(2)\nstats.probplot(train['SalePrice'], plot=plt)","b1cb16a1":"#MSSubClass=The building class\ntrain['MSSubClass'] = train['MSSubClass'].apply(str)\ntest['MSSubClass'] = test['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\ntrain['OverallCond'] = train['OverallCond'].astype(str)\ntest['OverallCond'] = test['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\ntrain['YrSold'] = train['YrSold'].astype(str)\ntest['YrSold'] = test['YrSold'].astype(str)\n\ntrain['MoSold'] = train['MoSold'].astype(str)\ntest['MoSold'] = test['MoSold'].astype(str)","9437fd09":"# Find columns with missing values\ncols_with_missing = [col for col in train.columns if train[col].isnull().any()]\ncols_with_missing","f4a100ef":"# Finding total number of missing values in each column\ntotal_missing = train.isnull().sum().sort_values(ascending=False)\ntotal_missing = total_missing[total_missing>0]\n\npercent = ((train.isnull().sum()\/train.isnull().count()) * 100).sort_values(ascending=False)\npercent = percent[percent>0]\n\ndata_missing = pd.concat([total_missing, percent], axis=1, keys=['Total', 'Percent'])\ndata_missing","610446e8":"object_cols = [col for col in train.columns if train[col].dtype == \"object\"]\nobject_cols","1d1ce0b1":"# Get number of unique entries in each column with categorical data\nobject_nunique = list(map(lambda col: train[col].nunique(), object_cols))\nd = dict(zip(object_cols, object_nunique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x: x[1])","2bf6a6ea":"for df in [train, test]:\n    for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n                  'BsmtFinType2', 'Neighborhood', 'BldgType', 'HouseStyle', 'MasVnrType', 'FireplaceQu', 'Fence', 'PoolQC', 'MiscFeature', 'Alley'):\n        df[col] = df[col].fillna('None')","0227bb5e":"train['MasVnrArea'] = train['MasVnrArea'].fillna(0)\ntest['MasVnrArea'] = test['MasVnrArea'].fillna(0)","89944494":"# Check for missing values\n\nmissing = train.isnull().sum().sort_values(ascending=False)\nmissing = missing[missing>0]\nmissing","2d1dcf47":"train['Electrical'].describe()","ed5f74cd":"train['Electrical'].mode()[0]","1b3c7908":"train['Electrical'] = train['Electrical'].fillna(train['Electrical'].mode()[0])\ntest['Electrical'] = test['Electrical'].fillna(test['Electrical'].mode()[0])","c94beda6":"train['LotFrontage'] = train.groupby('Neighborhood')['LotFrontage'].apply(lambda x: x.fillna(x.median()))\ntest['LotFrontage'] = test.groupby('Neighborhood')['LotFrontage'].apply(lambda x: x.fillna(x.median()))","432cc360":"# Check for missing values\n\nmissing = train.isnull().sum().sort_values(ascending=False)\nmissing = missing[missing>0]\nmissing","3516eac9":"# Categorical missing values in test data\nmissing = test.select_dtypes(include='object').isnull().sum().sort_values(ascending=False)\nmissing = missing[missing>0]\nmissing","fd35c96a":"# Numerical missing values in test data\nmissing = test.select_dtypes(exclude='object').isnull().sum().sort_values(ascending=False)\nmissing[missing>0]","fd37f69f":"# Handling MSZoning\ntrain['MSZoning'] = train.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\ntest['MSZoning'] = test.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))","d34d9666":"display(test['Utilities'].describe())\ndisplay(train['Utilities'].describe())","42decf92":"test = test.drop(['Utilities'], axis=1)\ntrain = train.drop(['Utilities'], axis=1)","3db07cb4":"for df in [train, test]:\n    for col in ('Functional', 'SaleType', 'Exterior2nd', 'Exterior1st', 'KitchenQual'):\n        df[col] = df[col].fillna(df[col].mode()[0])","04a50cd4":"# Categorical missing values in test data\nmissing = test.select_dtypes(include='object').isnull().sum().sort_values(ascending=False)\nmissing = missing[missing>0]\nmissing","3da80e13":"for df in [train, test]:\n    for col in ('MasVnrArea', 'BsmtFullBath', 'BsmtHalfBath', 'GarageCars', 'TotalBsmtSF',\n               'BsmtUnfSF', 'BsmtFinSF2', 'BsmtFinSF1'):\n        df[col] = df[col].fillna(0)","966bbd0c":"# Numerical missing values in test data\nmissing = test.select_dtypes(exclude='object').isnull().sum().sort_values(ascending=False)\nmissing[missing>0]","9df8c712":"train.shape, test.shape","de951961":"train.head(10)","315525a7":"sf = [col for col in train.columns if 'SF' in col]\nsf","ab28fc8f":"train['TotalSF'] = train['TotalBsmtSF'] + train['2ndFlrSF']\ntest['TotalSF'] = test['TotalBsmtSF'] + test['2ndFlrSF']","1fa154e3":"train[['TotalBsmtSF', '2ndFlrSF', 'TotalSF']].head()","1779fe4f":"plt.subplots(figsize=(10, 7))\nsb.scatterplot(x=train['TotalSF'], y=train['SalePrice'], color=\"mediumslateblue\")","005dfadb":"bath = [col for col in train.columns if 'Bath' in col]\nbath","bec8b483":"train['TotalBath'] = train['BsmtFullBath'] + train['BsmtHalfBath'] + train['FullBath'] + train['HalfBath']\ntest['TotalBath'] = test['BsmtFullBath'] + test['BsmtHalfBath'] + test['FullBath'] + test['HalfBath']","28fa0bb4":"train[['BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'TotalBath']].head()","44cc4316":"plt.subplots(figsize=(10, 7))\nsb.boxenplot(x=train['TotalBath'], y=train['SalePrice'], color=\"mediumslateblue\")","d612d653":"porch = [col for col in train.columns if 'Porch' in col]\nporch","75d6dfb1":"train['PorchSF'] = train['OpenPorchSF'] + train['EnclosedPorch'] + train['3SsnPorch'] + train['ScreenPorch'] + train['WoodDeckSF']\ntest['PorchSF'] = test['OpenPorchSF'] + test['EnclosedPorch'] + test['3SsnPorch'] + test['ScreenPorch'] + test['WoodDeckSF']","14827b9a":"train[['OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'WoodDeckSF', 'PorchSF']].head()","da0b4b79":"plt.subplots(figsize=(10, 7))\nsb.scatterplot(x=train['PorchSF'], y=train['SalePrice'], color=\"mediumslateblue\")","ca41ed54":"pool = [col for col in train.columns if 'Pool' in col]\npool","ba2c1969":"# train data\ntrain['HasBsmt'] = train['TotalBsmtSF'].apply(lambda x: 1 if x>0 else 0)\ntrain['HasPool'] = train['PoolArea'].apply(lambda x: 1 if x>0 else 0)\ntrain['Has2ndFlr'] = train['2ndFlrSF'].apply(lambda x: 1 if x>0 else 0)\ntrain['HasFirePlace'] = train['Fireplaces'].apply(lambda x: 1 if x>0 else 0)\ntrain['HasGarage'] = train['GarageCars'].apply(lambda x: 1 if x>0 else 0)\n\n# test data\ntest['HasBsmt'] = test['TotalBsmtSF'].apply(lambda x: 1 if x>0 else 0)\ntest['HasPool'] = test['PoolArea'].apply(lambda x: 1 if x>0 else 0)\ntest['Has2ndFlr'] = test['2ndFlrSF'].apply(lambda x: 1 if x>0 else 0)\ntest['HasFirePlace'] = test['Fireplaces'].apply(lambda x: 1 if x>0 else 0)\ntest['HasGarage'] = test['GarageCars'].apply(lambda x: 1 if x>0 else 0)","7f5bb82f":"train[['HasBsmt', 'HasPool', 'Has2ndFlr', 'HasFirePlace', 'HasGarage']].head()","d3642881":"plt.figure(1)\nsb.boxenplot(train['HasGarage'], train['SalePrice'], color=\"mediumslateblue\")\n\nplt.figure(2)\nsb.boxenplot(train['HasBsmt'], train['SalePrice'], color=\"mediumslateblue\")\n\nplt.figure(3)\nsb.boxenplot(train['HasPool'], train['SalePrice'], color=\"mediumslateblue\")\n\nplt.figure(4)\nsb.boxenplot(train['Has2ndFlr'], train['SalePrice'], color=\"mediumslateblue\")\n\nplt.figure(5)\nsb.boxenplot(train['HasFirePlace'], train['SalePrice'], color=\"mediumslateblue\")","9f0a9bc1":"print(train.shape, test.shape)","b92d9f49":"train['MasVnrArea'].head()","cf708a91":"train['MasVnrArea'] = train['MasVnrArea'].astype(int)\ntest['MasVnrArea'] = test['MasVnrArea'].astype(int)","74976bd1":"train['MasVnrArea'].head()","91a068b7":"plt.subplots(figsize=(10, 7))\nsb.scatterplot(train['MasVnrArea'], train['SalePrice'], color=\"mediumslateblue\")","7c0c9bd4":"# Import library\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_enc_variables = ['FireplaceQu', 'LotShape', 'OverallCond', 'ExterQual',\n                       'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n                       'BsmtFinType2', 'HeatingQC', 'KitchenQual', 'GarageFinish', 'GarageQual',\n                       'GarageCond', 'PavedDrive', 'PoolQC', 'Fence']\n\n\n# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col in label_enc_variables:\n    train[col] = label_encoder.fit_transform(train[col])\n    test[col] = label_encoder.transform(test[col])","fd000128":"print(train.shape, test.shape)","67d6398c":"# Perform train test split on this data\n\nX = train.drop(['SalePrice'], axis=1)\ny = train['SalePrice']","f0f9e83b":"# Filling numerical columns\nnum_cols = [col for col in X.columns if X[col].dtype!='object']\nX.update(X[num_cols].fillna(0))\ntest.update(test[num_cols].fillna(0))\n\n# Filling categorical columns\ncat_cols = [col for col in X.columns if X[col].dtype=='object']\nX.update(X[cat_cols].fillna('None'))\ntest.update(test[cat_cols].fillna('None'))","8ff3bbf3":"print(X.shape, test.shape)","0eff8bec":"# Importing our libraries\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\n# Added in version 7 of this notebook\nfrom sklearn.model_selection import GridSearchCV\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import Ridge, RidgeCV, ElasticNetCV\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.svm import SVR","0f4fdbf8":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=2)","a7271563":"numerical_cols = [col for col in X_train.columns\n                  if X_train[col].dtype in ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']]\n\ncategorical_cols= [col for col in X_train.columns\n                   if X_train[col].nunique()<=30 and X_train[col].dtype=='object']\n\nfinal_cols = numerical_cols + categorical_cols\nX_train = X_train[final_cols].copy()\nX_valid = X_valid[final_cols].copy()\n\ntest = test[final_cols].copy()","74303f8b":"X_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\ntest = pd.get_dummies(test)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\nX_train, test = X_train.align(test, join='left', axis=1)","edd44d9b":"print(\"X_train Shape\", X_train.shape)\nprint(\"X_valid Shape\", X_valid.shape)\nprint(\"test shape\", test.shape)","dc21a469":"# Reversing log transform on y ('SalePrice')\ndef rev_y(trans_y):\n    return np.expm1(trans_y)","4707a97b":"estimators = [2000, 2500, 3000, 3500]\nfor n in estimators:\n    my_dict = dict()\n    model = XGBRegressor(n_estimators=n, learning_rate=0.01, colsample_bytree=0.45, max_depth=3)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    mae = mean_absolute_error(rev_y(preds), rev_y(y_valid))\n    my_dict[f\"n_est {n}\"] = mae\n    print(my_dict)","55674a09":"my_dict = dict()\nn=3500\nmodel = XGBRegressor(n_estimators=n, learning_rate=0.01, colsample_bytree=0.45, max_depth=3,\n                     gamma=0, subsample=0.4, reg_alpha=0, reg_lambda=1, objective='reg:squarederror')\n    \nmodel.fit(X_train, y_train)\npreds = model.predict(X_valid)\nmae = mean_absolute_error(rev_y(preds), rev_y(y_valid))\nmy_dict[f\"n_est {n}\"] = mae\nprint(my_dict)","5f7bf924":"# Version 7\nn = 3500\nmy_dict = dict()\n\n# XGBoost Regressor\nxgb = XGBRegressor(n_estimators=n,\n                   learning_rate=0.01,\n                   colsample_bytree=0.45,\n                   max_depth=3,\n                   gamma=0,\n                   subsample=0.4,\n                   reg_alpha=0,\n                   reg_lambda=1,\n                   objective='reg:squarederror')\n\n# Light Gradient Boosting Regressor\nlgb = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=n,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)\n\n# Ridge Regressor\nridge_alphas = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas))\n\n# ElasticNet Regressor\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, l1_ratio=e_l1ratio))\n\n# Support Vector Regressor\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n\n# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=n,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)\n\n# Stacking models\nstack = StackingCVRegressor(regressors=(ridge, gbr, xgb, elasticnet, svr, lgb),\n                                meta_regressor=xgb,\n                                use_features_in_secondary=True)","9638931c":"Scores = {}","52106697":"# Function for checking Cross-val scores                              \ndef rmse(model, X, y):\n    scores = np.sqrt(-1 * cross_val_score(model, X, y,\n                        cv=10, \n                        scoring='neg_mean_squared_error'))\n    return scores\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","3152063b":"# Cross-val scores                              \nscores = rmse(xgb, X_train, y_train)\nprint(\"XGBoost Regressor\\n\")\nprint(\"Root Mean Square Error (RMSE)\", str(scores.mean()))\nprint(\"Error Standard Deviation\", str(scores.std()))\nScores['XGB'] = scores.mean()","d9344b34":"scores = rmse(lgb, X_train, y_train)\nprint(\"Light Gradient Boosting Regressor\\n\")\nprint(\"Root Mean Square Error (RMSE)\", str(scores.mean()))\nprint(\"Error Standard Deviation\", str(scores.std()))\nScores['LGB'] = scores.mean()","1b706cdd":"scores = rmse(ridge, X_train, y_train)\nprint(\"Ridge Regressor\\n\")\nprint(\"Root Mean Square Error (RMSE)\", str(scores.mean()))\nprint(\"Error Standard Deviation\", str(scores.std()))\nScores['Ridge'] = scores.mean()","580301e2":"scores = rmse(svr, X_train, y_train)\nprint(\"Support Vector Regressor\\n\")\nprint(\"Root Mean Square Error (RMSE)\", str(scores.mean()))\nprint(\"Error Standard Deviation\", str(scores.std()))\nScores['SVR'] = scores.mean()","4c737b86":"scores = rmse(gbr, X_train, y_train)\nprint(\"Gradient Boosting Regressor\\n\")\nprint(\"Root Mean Square Error (RMSE)\", str(scores.mean()))\nprint(\"Error Standard Deviation\", str(scores.std()))\nScores['GBR'] = scores.mean()","22bec288":"scores = rmse(elasticnet, X_train, y_train)\nprint(\"ElasticNet Regressor\\n\")\nprint(\"Root Mean Square Error (RMSE)\", str(scores.mean()))\nprint(\"Error Standard Deviation\", str(scores.std()))\nScores['ElasticNet'] = scores.mean()","075fe9a9":"xgb.fit(X_train, y_train)","3732b6bd":"lgb.fit(X_train, y_train)","9f267146":"ridge.fit(X_train, y_train)","f4b95ca7":"svr.fit(X_train, y_train)","fc8e1d31":"gbr.fit(X_train, y_train)","ef56ba25":"elasticnet.fit(X_train, y_train)","b0500e75":"stack.fit(np.array(X_train), np.array(y_train))","b26b9721":"def blended_predictions(X):\n    return ((0.2 * ridge.predict(X)) + \\\n            (0.2 * elasticnet.predict(X)) + \\\n            (0.05 * svr.predict(X)) + \\\n            (0.1 * gbr.predict(X)) + \\\n            (0.1 * xgb.predict(X)) + \\\n            (0.1 * lgb.predict(X)) + \\\n           (0.25 * stack.predict(np.array(X))))","73331bcd":"X_valid = X_valid.fillna(0)\ntest = test.fillna(0)","896bd377":"blended_score = rmsle(y_valid, blended_predictions(X_valid))\n\nScores['Blended'] = blended_score\nblended_score","32002f43":"plt.subplots(figsize=(15, 7));plt.title(\"Scores of different models\") \nplt.xticks(rotation=45)\nsb.pointplot(x=list(Scores.keys()), y=[score for score in Scores.values()], markers=['o'], linestyles=['-'], color=\"mediumslateblue\")","081e494c":"# Read in sample_submission dataframe\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.shape","5eefe34b":"# Append predictions from blended models\nsubmission.iloc[:,1] = np.floor(rev_y(blended_predictions(test)))","5c6190fa":"# Brutal approach to deal with outliers\nq1 = submission['SalePrice'].quantile(0.0045)\nq2 = submission['SalePrice'].quantile(0.99)\n\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)","bad18699":"submission['SalePrice'] *= 1.001619\nsubmission.to_csv(\"new_submission.csv\", index=False)","93b7d38d":"As we can see that there are a total of 18 variables. PoolQC (Pool Quality) has approximately 99 percent of data missing. This is alarming for a lot of people that how can it be possible. And if it has so much data missing then should we drop the column all together?\n\nThe answer is no!\n\nIf we look at the 'data_description.txt', we can see that a 'NA' in PoolQC means No Pool and it makes sense that a lot of homes do not have pool. We can fill all the cells containing 'NA' with 'None', which means the house does not have a pool.\n\nWe can similarly transform many categorical columns.\n\n## Imputing Missing Values","f5dd5884":"There is a large amount of data floating around in the heatmap above. All we have to do is to understand how to interpret it. \n\nFirst of all, one thing is clear to us and that is we are only concerned with the target variable i.e. SalePrice and all the variables effecting it on a higher magnitude.\n\nLets interpret the relation of SalePrice with other variables by ovserving the heatmap. On the left hand side, bottom corner we can see SalePrice (the first row in the vertical column). Here, we are only concerned with that row. So, lets ignore all the other rows. Now, on the extreme right side we can see something like a color palette with different values. It indicates the degree of relation. If the color is very dark e.g. 0.8, it means that the corresponding variable effects SalePrice sharply.\n\nComing back to the bottom row, we can see that 'OverallQual' has a very dark color and hence it effects the SalePrice directly and so on for other variables.","137b2fff":"> SalePrice now looks normally distributed and does not show skewness. And yes, it does puts a smile on my face!","9098d60c":"Since, Electrical has only one missing value and 'SBrkr' is the most frequent value. We can fill it with that.","3f74bb44":"\n\n# Modelling\n\nAfter cleaning up of the data and transforming it. We are ready with our data and we can call in our models now to perform the cool stuff.","fa03e0e4":"Alright, at this point we have eliminated all the missing values from our data.","dcf3b288":"From above output, we can conclude that n_estimators of ~2500 gives much better result with a learning rate of 0.02.\n\nSo, we drop other values.","a0dfa5b3":"Let us look at the data after dropping variables.","016fd7c9":"# Handling Missing Values","4c37cc54":"## Looking at the Bigger Picture\nFinally, we create a scatter plot between SalePrice and most important related variables in order to look at the entire picture.","043033a3":"Let's tackle all the missing **categorical** test data.\n\nWe will handle the first missing variable which is 'MSZoning'.\n\nMSZoning means the general zoning classification: Unique values of MSZoning are:\n\n* 'A' for Agriculture\n* 'C' Commercial\n* 'FV' Floating Village Residential\n* 'I' Industrial\n* 'RH' Residential High Density\n* 'RL' Residential Low Density\n* 'RP' Residential Low Density Park\n* 'RM' Residential Medium Density\n\nClearly this feature is correlated to MSSubClass, which is the building class, so we fill in the median based on MSSubClass","9d914edf":"# Final Model and Submission","4b539c81":"## SalePrice vs YearBuilt","657b238a":"There are no more missing values in our 'train' data. It is a moment of enjoyment and happiness. But, before all that let's check out the test data real quick for missing values.\n\n### Additional Missing values in test data","444deacc":"It can be clearly seen that the rating of 5 is very common in both *'overall quality'*.\n\nWe see similar kind of result when we perform this operation on *'overall condition'*.\n\nNow we can move one step and include *Sale Price* in our charts and compare it with *Overall Quality* and *Overall Condition* of the house.","8045f489":"**Bath**","4bce4536":"## Cross-Validation Scores","051d95ce":"**Percent of Missing Values**\n\nLet us find out the total number and percent of missing values in each of the column.","82b89dbe":"Let us first find out the columns which have missing values. Further, we will learn to handle all these missing values.\n\n.\n\n\n> Pay attention to the code below to understand how we can find the names of columns which have missing values.","e930d0c0":"Now, lets handle '**LotFrontage**' which has 259 missing values.\n\nSince the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood.","daf2a07f":"From the above chart, we can clearly see that:\n* The Sales Price deviates from Normal Distribution.\n* It shows peakedness.\n* Its positively skew.\n\nAfter looking at the skewness of our SalePrice data, let us understand more about its skewness and kurtosis.\nBut first, we will learn what is this **skewness** and **kurtosis** that we keep talking about?\n\n## Skewness:\nSkewness measures the lack of symmetry in our distribution. A distribution could be either positively skew or negatively skew.\n","d91e0ef2":"After careful examination of 'Utilities' variable we can drop it. We can clearly see that in the test dataset there is only one unique value in the data and two in training set. This will not add any importance to our dataset. \n\nSo, we simply throw it away.","6e4b4d37":"Now, we will only keep all the categorical columns which has a cardinality of less than 30. We do this because of OneHotEncoding as we would get a lot of columns. ","5e2a8c88":"### Reverse log transform on SalePrice","21f0e0ba":"## SalePrice and GrLivArea","3c46ee2f":"## SalePrice and Overall Condition","f99d8625":"From above graph, we can conclude that 'SalePrice' enjoys company of homes that were recently built. Hence, the prices of newer homes are high when compared to older houses.","c2c0ef24":"**Thank You for reading this notebook!!**\n\n**If you find this notebook useful or you just like it, please Upvote it as it would really mean a lot to me and it would keep me motivated to update this notebook regularly.**\n\n**If you use any part of this notebook, please mention a link in your notebook, it would be much appreciated.**\n\n**And if you have any question, or find out something isn't right please comment below.**","92a5f6e2":"### Submission","6e3ea6ff":"In this kaggle competition, we are going to predict the house prices using XGBRegression Technique. We will also try to climb up in the leaderboard.\n\n> Reference: \n\n* > [Comprehensive data exploration with Python By Pedro Marcelino](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\/notebook?select=test.csv)\n\n* > [Stacked Regressions By Serigne](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard)\n* > [Ultimate Predictions Guide [v6] By Preston Fan](https:\/\/www.kaggle.com\/prestonfan\/ultimate-predictions-guide-v6)","99565b03":"What's new in this version:\n* Stacking\n* Blending\n\nModels Used:\n* XGBoost Regressor\n* Light Gradient Boosting Regressor\n* Ridge Regressor\n* Support Vector Regressor\n* Gradient Boosting Regressor \n* ElasticNet Regressor\n***\n","190cbedd":"## Fit the models","654bfc4b":"As we can see from the graph above that we have successfully removed the outliers. This graph looks more acceptable and we have less to worry now. Lets move on to the next variable.","b5f2d052":"### Lets look at all the **Surface Area** variables.","8c6a1bfd":"## The house has it or not?\nWe can create a new variable 'HasBsmt'. It will specify that if a house has a basement of not.\n\nSimilarly, we can create different variables starting with 'Has...'. It will specify that if a house has a particular feature or not. These variables will make the dataset more clear for the buyers.","00394766":"## One-Hot Encoding\nWe use pandas for one-hot encoding to shorten our code","c643757b":"## Log Transform on SalePrice\nSalePrice is skewed and deviates from Normal Distribution. Let us do a log transform on 'SalePrice'.\n\nWe perform this step right before fitting our model and we then reverse it back to normal after we have made our predictions (at the time of submitting SalePrice to competition).","79b2f6d7":"'MasVnrArea' is categorical in the data but it shouldn't be. Its should be of numerical data type. Thats why we fill the missing values with 0 and later on we will convert it to int data type.","f3cd6824":"We have successfully addressed all the categorical missing values. Now we will move on to **numerical** missing values.","b87d2c3d":"![compress-image.jpg](attachment:compress-image.jpg)","e7d0d6e4":"Unlike Overall Quality, Overall Condition represents non-linear graph. Therefore, we can conclude that, Overall Condition doesn't has a high impact on the SalePrice on a higher magnitude and thus we can skip it.\nOverall condition is a categorical variable and we will change it later on when we do feature engineering.","40731fd9":"As we can see from the output above that the data has several columns which represent different attributes of a house.\n\nSo, we have every thing we need about homes to do our magic. Let us proceed further into the depth and see what we can find.","78b37057":"## Lets Summarize\nFrom above graphs, we can summarize that:\n* SalePrice and OverallQuality have a strong relationship. SalePrice increases with the increase in Overall Quality.\n* SalePrice and GrLivArea are linearly related. When above ground living area increases, sale price also increases.\n* SalePrice and YearBuilt are related.\n* SalePrice and TotBsmtSF are linearly related just as GrLivArea.","bd7a8055":"### Creating a '*TotalSF*' variable from 'TotalBsmtSF' and '2ndFlrSF'.","af9c195b":"We have successfully created an entire new variable in our train and test dataset. How about that?\n\nLets see if we can add some more variables which can add value to our dataset.","34b27681":"# Analyzing Most Related Feature Values","5495faaa":"## SalePrice Correlation Matrix using HeatMap\n\nNow, moving on to the most important step in finding out the relationship of important vaiables with SalePrice.\n\nLet us create a 'SalePrice' correlation matrix using heatmap.","a28ee864":"## If you are here, then please UpVote this notebook. It would only take a second and would mean a lot to me!!\n## Thank You\n### Have a wonderful day!","f1a6cce8":"We right our wrongs, move forward and repeat this process again and again.","826d1df7":"### Importing libraries","e6c49554":"## SalePrice vs Total Basement Area","ed0ea894":"## Disguise\n\nFew categorical values are disguised as numerical values. We will transform these ASAP!!","b3c68e59":"Let us find out the skewness and kurtosis of the above distribution.","85132348":"We have successfully imported everything we need to solve this challenge. Now let us get started with importing the data and getting our hands dirty.","b6836c09":"## Analyzing the target variable","850dd990":"## Final Filling of Missing Data (tying up loose ends)\n\nFilling missing numerical values with 0 and categorical values with 'None'.","c3386601":"**Porch**","90b64ecb":"There are no huge outliers in this data. This looks good and thus we can move forward.","dd871a08":"## The importance of 'Total'\nSimilarly, we can create different features just like 'TotalSF' containing different useful information.\n\nLets check them out.","d43112e7":"Lets find out highly correlated variables (>0.8) as it smells like something unexpected is cooking. You may find out that they are the same things with two different names. Such as YearBuilt and GarageYrBuilt are same as the garage must have been built when the home was built.","cf2f7302":"## SalePrice and Overall Quality","2cc2f535":"### HeatMap\nFurther, we will see the use of heatmap to find out the relation between all the variables.\n\n# Feature Engineering\nWe want to analyze all the variables against SalePrice to find out that how much does it effect the SalePrice.\n\nWe will perform these operations in order to find out the SalePrice relationship with all the other variables:\n* Correlation matrix (using heatmap)\n* 'SalePrice' correlation matrix\n* Scatter plots between the most correlated variables\n\n","e9e3c9c7":"From the heatmap above, we can clearly see the highly-correlated features.\n* YearBuilt vs GarageYrBlt\n* 1stFlrSF vs TotalBsmtSF\n* GrLivArea vs TotRmsAbvGrd\n* GarageCars vs GarageArea\n\nWe know that we have to drop one from each pair. So, we look at the main correlation matrix and find our which is less correlated to 'SalePrice' in the pair of two and we drop that feature.","783ef78a":"# Creating Models\n\nI thought of stacking different models for prediction after version 6.\n\nSo, If you only want to use XGBoost Regressor you can use it. The code above provides with the best parameters for XGBoost.\n\nIf you want to improve your model score, follow along. We will have fun with this one!","005fc978":"# Blending","75d5769d":"# Label Encoding (Manual)\n\nSome categorical variables may contain information in their ordering set and hence we should apply LabelEncoding instead of OneHotEncoding on these variables.","c778b56a":"Let's visualise this data in in *boxenplot(), violinplot() and stripplot()*:","312c757e":"### We have *Outliers* in our data:\n\nFrom the image above, we can clearly see that two values of 'GrLivArea' are performing very strange as they show to have huge above ground surface area while the SalePrice is considerably low, which is not possible. These values are called **Outliers**. These two points can cause us problems. Hence, we must drop these two values.\n\nLets remove these outliers for good.","4ed4b120":"## Parameter Tuning (XGBoost)","995f6f73":"* Lets create a correlation matrix using heatmap.","2b156bf2":"# Extra Feature Engineering\n\nNow, we are going to create some variables in our train and test data which we think are important in training our model and add value to the overall data.","5a2a1e3a":"Lets find out all the categorical columns in our data.","23ba97ee":"### A spy in disguise\n\nLets convert MasVnrArea from float to int","75f2721d":"So, after gaining a more in depth view of Sale Price, we can conclude that:\n\n> *The minimum value is greater than 0, which is a good thing as we have to work less on cleaning the data and replacing the values with mean.*\n\nWe can move forward with our project.","8b10e83e":"Take a look at the code below. This is the center of attraction in above code segment.\n\n> **train[**  (train['GrLivArea'] > 4000)  &  (train['SalePrice'] < 200000)  **]**.index"}}