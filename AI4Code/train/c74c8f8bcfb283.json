{"cell_type":{"b4731f2b":"code","a648ad9c":"code","08276949":"code","b7940ae0":"code","21950b51":"code","575fce51":"code","2ed80ab3":"code","0e9f7018":"code","b6a1c11b":"code","f7150e7c":"code","03d20a70":"code","d9de2326":"code","f74f16df":"code","70951a4e":"code","f7d31a85":"code","5dc2f452":"code","dd41549f":"code","85d4691d":"code","bd5eea7f":"code","3399d2f2":"code","78d4f4d6":"code","04c776bd":"code","36769035":"code","960d194a":"code","dee289cb":"code","c57d17a9":"code","c95aa3b7":"code","31709263":"code","8bbb85ac":"code","0b259e0c":"code","8126fe67":"code","aae19dcd":"code","8faa71e5":"code","d7789a5e":"code","f4a60ea3":"markdown","0cf797f4":"markdown","1b6ed384":"markdown","b945781a":"markdown","becf805b":"markdown","d19b778b":"markdown","749ccdaf":"markdown","ee5970df":"markdown","5837c37a":"markdown","6be43d68":"markdown","f2fa82b0":"markdown","4e596d39":"markdown","c2f14f2c":"markdown","1ad2e359":"markdown","66ebf630":"markdown","ac32468e":"markdown","81c24513":"markdown","6cce9ec8":"markdown","002ab5cc":"markdown","bfa5da31":"markdown","a174353f":"markdown","26112bdd":"markdown","6ab66130":"markdown","8795090c":"markdown","855b5aa6":"markdown","01ecbe99":"markdown","72ded330":"markdown","2ae4b605":"markdown","d0fc5c21":"markdown","b702d341":"markdown","40cd8bad":"markdown","efb6eb68":"markdown","5d722e4d":"markdown","7d85148e":"markdown","bc460abe":"markdown"},"source":{"b4731f2b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a648ad9c":"import pandas as pd\n\nks = pd.read_csv('..\/input\/kickstarter-projects\/ks-projects-201801.csv',\n                 parse_dates=['deadline', 'launched'])","08276949":"ks.head(6)","b7940ae0":"print('Unique values in `state` column:', list(ks.state.unique()))","21950b51":"# Drop live projects\nks = ks.query('state != \"live\"')\n\n# Add outcome column, \"successful\" == 1, others are 0\nks = ks.assign(outcome=(ks['state'] == 'successful').astype(int))","575fce51":"ks = ks.assign(hour=ks.launched.dt.hour,\n               day=ks.launched.dt.day,\n               month=ks.launched.dt.month,\n               year=ks.launched.dt.year)","2ed80ab3":"from sklearn.preprocessing import LabelEncoder\n\ncat_features = ['category', 'currency', 'country']\nencoder = LabelEncoder()\n\n# Apply the label encoder to each column\nencoded = ks[cat_features].apply(encoder.fit_transform)","0e9f7018":"# Since ks and encoded have the same index and I can easily join them\ndata = ks[['goal', 'hour', 'day', 'month', 'year', 'outcome']].join(encoded)\ndata.head()","b6a1c11b":"valid_fraction = 0.1\nvalid_size = int(len(data) * valid_fraction)\n\ntrain = data[:-2 * valid_size]\nvalid = data[-2 * valid_size:-valid_size]\ntest = data[-valid_size:]","f7150e7c":"import lightgbm as lgb\n\nfeature_cols = train.columns.drop('outcome')\n\ndtrain = lgb.Dataset(train[feature_cols], label=train['outcome'])\ndvalid = lgb.Dataset(valid[feature_cols], label=valid['outcome'])\n\nparam = {'num_leaves': 64, 'objective': 'binary'}\nparam['metric'] = 'auc'\nnum_round = 1000\nbst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=10, verbose_eval=False)","03d20a70":"from sklearn import metrics\nypred = bst.predict(test[feature_cols])\nscore = metrics.roc_auc_score(test['outcome'], ypred)\n\nprint(f\"Test AUC score: {score}\")","d9de2326":"from sklearn.preprocessing import LabelEncoder\n\n# Drop live projects\nks = ks.query('state != \"live\"')\n\n# Add outcome column, \"successful\" == 1, others are 0\nks = ks.assign(outcome=(ks['state'] == 'successful').astype(int))\n\n# Timestamp features\nks = ks.assign(hour=ks.launched.dt.hour,\n               day=ks.launched.dt.day,\n               month=ks.launched.dt.month,\n               year=ks.launched.dt.year)\n\n# Label encoding\ncat_features = ['category', 'currency', 'country']\nencoder = LabelEncoder()\nencoded = ks[cat_features].apply(encoder.fit_transform)\n\ndata_cols = ['goal', 'hour', 'day', 'month', 'year', 'outcome']\ndata = ks[data_cols].join(encoded)\n\n# Defining  functions that will help us test our encodings\nimport lightgbm as lgb\nfrom sklearn import metrics\n\ndef get_data_splits(dataframe, valid_fraction=0.1):\n    valid_fraction = 0.1\n    valid_size = int(len(dataframe) * valid_fraction)\n\n    train = dataframe[:-valid_size * 2]\n    # valid size == test size, last two sections of the data\n    valid = dataframe[-valid_size * 2:-valid_size]\n    test = dataframe[-valid_size:]\n    \n    return train, valid, test\n\ndef train_model(train, valid):\n    feature_cols = train.columns.drop('outcome')\n\n    dtrain = lgb.Dataset(train[feature_cols], label=train['outcome'])\n    dvalid = lgb.Dataset(valid[feature_cols], label=valid['outcome'])\n\n    param = {'num_leaves': 64, 'objective': 'binary', \n             'metric': 'auc', 'seed': 7}\n    bst = lgb.train(param, dtrain, num_boost_round=1000, valid_sets=[dvalid], \n                    early_stopping_rounds=10, verbose_eval=False)\n\n    valid_pred = bst.predict(valid[feature_cols])\n    valid_score = metrics.roc_auc_score(valid['outcome'], valid_pred)\n    print(f\"Validation AUC score: {valid_score:.4f}\")","f74f16df":"# Train a model (on the baseline data)\ntrain, valid, test = get_data_splits(data)\ntrain_model(train, valid)","70951a4e":"import category_encoders as ce\n\ncat_features = ['category', 'currency', 'country']\n\n# Create the encoder\ncount_enc = ce.CountEncoder()\n\n# Transform the features, rename the columns with the _count suffix, and join to dataframe\ncount_encoded = count_enc.fit_transform(ks[cat_features])\ndata = data.join(count_encoded.add_suffix(\"_count\"))\n\n# Train a model \ntrain, valid, test = get_data_splits(data)\ntrain_model(train, valid)","f7d31a85":"# Create the encoder\ntarget_enc = ce.TargetEncoder(cols=cat_features)\ntarget_enc.fit(train[cat_features], train['outcome'])\n\n# Transform the features, rename the columns with _target suffix, and join to dataframe\ntrain_TE = train.join(target_enc.transform(train[cat_features]).add_suffix('_target'))\nvalid_TE = valid.join(target_enc.transform(valid[cat_features]).add_suffix('_target'))\n\n# Train a model\ntrain_model(train_TE, valid_TE)","5dc2f452":"# Create the encoder\ntarget_enc = ce.CatBoostEncoder(cols=cat_features)\ntarget_enc.fit(train[cat_features], train['outcome'])\n\n# Transform the features, rename columns with _cb suffix, and join to dataframe\ntrain_CBE = train.join(target_enc.transform(train[cat_features]).add_suffix('_cb'))\nvalid_CBE = valid.join(target_enc.transform(valid[cat_features]).add_suffix('_cb'))\n\n# Train a model\ntrain_model(train_CBE, valid_CBE)","dd41549f":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\nfrom sklearn.preprocessing import LabelEncoder\n\n# Drop live projects\nks = ks.query('state != \"live\"')\n\n# Add outcome column, \"successful\" == 1, others are 0\nks = ks.assign(outcome=(ks['state'] == 'successful').astype(int))\n\n# Timestamp features\nks = ks.assign(hour=ks.launched.dt.hour,\n               day=ks.launched.dt.day,\n               month=ks.launched.dt.month,\n               year=ks.launched.dt.year)\n\n# Label encoding\ncat_features = ['category', 'currency', 'country']\nencoder = LabelEncoder()\nencoded = ks[cat_features].apply(encoder.fit_transform)\n\ndata_cols = ['goal', 'hour', 'day', 'month', 'year', 'outcome']\nbaseline_data = ks[data_cols].join(encoded)","85d4691d":"interactions = ks['category'] + \"_\" + ks['country']\nprint(interactions.head(5))","bd5eea7f":"label_enc = LabelEncoder()\ndata_interaction = baseline_data.assign(category_country=label_enc.fit_transform(interactions))\ndata_interaction.head()","3399d2f2":"# First, create a Series with a timestamp index\nlaunched = pd.Series(ks.index, index=ks.launched, name=\"count_7_days\").sort_index()\nlaunched.head(20)","78d4f4d6":"count_7_days = launched.rolling('7d').count() - 1\nprint(count_7_days.head(20))\n\n# Ignore records with broken launch dates\nplt.plot(count_7_days[7:]);\nplt.title(\"Number of projects launched over periods of 7 days\");","04c776bd":"count_7_days.index = launched.values\ncount_7_days = count_7_days.reindex(ks.index)","36769035":"baseline_data.join(count_7_days).head(10)","960d194a":"def time_since_last_project(series):\n    # Return the time in hours\n    return series.diff().dt.total_seconds() \/ 3600.\n\ndf = ks[['category', 'launched']].sort_values('launched')\ntimedeltas = df.groupby('category').transform(time_since_last_project)\ntimedeltas.head(20)","dee289cb":"timedeltas = timedeltas.fillna(timedeltas.median()).reindex(baseline_data.index)\ntimedeltas.head(20)","c57d17a9":"plt.hist(ks.goal, range=(0, 100000), bins=50);\nplt.title('Goal');","c95aa3b7":"plt.hist(np.sqrt(ks.goal), range=(0, 400), bins=50);\nplt.title('Sqrt(Goal)');","31709263":"plt.hist(np.log(ks.goal), range=(0, 25), bins=50);\nplt.title('Log(Goal)');","8bbb85ac":"%matplotlib inline\n\nimport itertools\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import metrics\n\n# Drop live projects\nks = ks.query('state != \"live\"')\n\n# Add outcome column, \"successful\" == 1, others are 0\nks = ks.assign(outcome=(ks['state'] == 'successful').astype(int))\n\n# Timestamp features\nks = ks.assign(hour=ks.launched.dt.hour,\n               day=ks.launched.dt.day,\n               month=ks.launched.dt.month,\n               year=ks.launched.dt.year)\n\n# Label encoding\ncat_features = ['category', 'currency', 'country']\nencoder = LabelEncoder()\nencoded = ks[cat_features].apply(encoder.fit_transform)\n\ndata_cols = ['goal', 'hour', 'day', 'month', 'year', 'outcome']\nbaseline_data = ks[data_cols].join(encoded)\n\ncat_features = ['category', 'currency', 'country']\ninteractions = pd.DataFrame(index=ks.index)\nfor col1, col2 in itertools.combinations(cat_features, 2):\n    new_col_name = '_'.join([col1, col2])\n    # Convert to strings and combine\n    new_values = ks[col1].map(str) + \"_\" + ks[col2].map(str)\n    label_enc = LabelEncoder()\n    interactions[new_col_name] = label_enc.fit_transform(new_values)\nbaseline_data = baseline_data.join(interactions)\n\nlaunched = pd.Series(ks.index, index=ks.launched, name=\"count_7_days\").sort_index()\ncount_7_days = launched.rolling('7d').count() - 1\ncount_7_days.index = launched.values\ncount_7_days = count_7_days.reindex(ks.index)\n\nbaseline_data = baseline_data.join(count_7_days)\n\ndef time_since_last_project(series):\n    # Return the time in hours\n    return series.diff().dt.total_seconds() \/ 3600.\n\ndf = ks[['category', 'launched']].sort_values('launched')\ntimedeltas = df.groupby('category').transform(time_since_last_project)\ntimedeltas = timedeltas.fillna(timedeltas.max())\n\nbaseline_data = baseline_data.join(timedeltas.rename({'launched': 'time_since_last_project'}, axis=1))\n\ndef get_data_splits(dataframe, valid_fraction=0.1):\n    valid_fraction = 0.1\n    valid_size = int(len(dataframe) * valid_fraction)\n\n    train = dataframe[:-valid_size * 2]\n    # valid size == test size, last two sections of the data\n    valid = dataframe[-valid_size * 2:-valid_size]\n    test = dataframe[-valid_size:]\n    \n    return train, valid, test\n\ndef train_model(train, valid):\n    feature_cols = train.columns.drop('outcome')\n\n    dtrain = lgb.Dataset(train[feature_cols], label=train['outcome'])\n    dvalid = lgb.Dataset(valid[feature_cols], label=valid['outcome'])\n\n    param = {'num_leaves': 64, 'objective': 'binary', \n             'metric': 'auc', 'seed': 7}\n    print(\"Training model!\")\n    bst = lgb.train(param, dtrain, num_boost_round=1000, valid_sets=[dvalid], \n                    early_stopping_rounds=10, verbose_eval=False)\n\n    valid_pred = bst.predict(valid[feature_cols])\n    valid_score = metrics.roc_auc_score(valid['outcome'], valid_pred)\n    print(f\"Validation AUC score: {valid_score:.4f}\")\n    return bst","0b259e0c":"from sklearn.feature_selection import SelectKBest, f_classif\n\nfeature_cols = baseline_data.columns.drop('outcome')\ntrain, valid, _ = get_data_splits(baseline_data)\n\n# Keep 5 features\nselector = SelectKBest(f_classif, k=5)\n\nX_new = selector.fit_transform(train[feature_cols], train['outcome'])\nX_new","8126fe67":"# Get back the features we've kept, zero out all other features\nselected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                 index=train.index, \n                                 columns=feature_cols)\nselected_features.head()","aae19dcd":"# Dropped columns have values of all 0s, so var is 0, drop them\nselected_columns = selected_features.columns[selected_features.var() != 0]\n\n# Get the valid dataset with the selected features.\nvalid[selected_columns].head()","8faa71e5":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\n\ntrain, valid, _ = get_data_splits(baseline_data)\n\nX, y = train[train.columns.drop(\"outcome\")], train['outcome']\n\n# Set the regularization parameter C=1\nlogistic = LogisticRegression(C=1, penalty=\"l1\", solver='liblinear', random_state=7).fit(X, y)\nmodel = SelectFromModel(logistic, prefit=True)\n\nX_new = model.transform(X)\nX_new","d7789a5e":"# Get back the kept features as a DataFrame with dropped columns as all 0s\nselected_features = pd.DataFrame(model.inverse_transform(X_new), \n                                 index=X.index,\n                                 columns=X.columns)\n\n# Dropped columns have values of all 0s, keep other columns \nselected_columns = selected_features.columns[selected_features.var() != 0]","f4a60ea3":"**Prepare the target column**\n\nFirst we'll convert the `state` column into a target we can use in a model. Data cleaning isn't the current focus, so we'll simplify this example by:\n\n* Dropping projects that are \"live\"\n* Counting \"successful\" states as `outcome = 1`\n* Combining every other state as `outcome = 0`","0cf797f4":"# Feature Generation","1b6ed384":"We collect all of these features in a new dataframe that we can use to train a model.","b945781a":"You should notice that the selected features are different than when I used the entire dataset. Now we have our selected features, but it's only the feature values for the training set. To drop the rejected features from the validation and test sets, we need to figure out which columns in the dataset were kept with `SelectKBest`. To do this, we can use `.inverse_transform` to get back an array with the shape of the original data.","becf805b":"**CatBoost Encoding**\n\nFinally, we'll look at CatBoost encoding. This is similar to target encoding in that it's based on the target probablity for a given value. However with CatBoost, for each row, the target probability is calculated only from the rows before it.","d19b778b":"**Number of projects in the last week**\n\nNext, we'll count the number of projects launched in the preceeding week for each record. I'll use the `.rolling` method on a series with the `\"launched\"` column as the index. I'll create the series, using `ks.launched` as the index and `ks.index` as the values, then sort the times. Using a time series as the index allows us to define the rolling window size in terms of hours, days, weeks, etc.","749ccdaf":"There are seven projects that have obviously wrong launch dates, but we'll just ignore them. Again, this is something you'd handle when cleaning the data, but it's not the focus of this mini-course.\n\nWith a timeseries index, you can use `.rolling` to select time periods as the window. For example `launched.rolling('7d')` creates a rolling window that contains all the data in the previous 7 days. The window contains the current record, so if we want to count all the previous projects but not the current one, we'll need to subtract 1. We'll plot the results to make sure it looks right.","ee5970df":"**Convert timestamps**\n\nNext, we convert the `launched` feature into categorical features we can use in a model. Since we loaded the columns as timestamp data, we access date and time values through the `.dt` attribute on the timestamp column.","5837c37a":"**Count Encoding**\n\nCount encoding replaces each categorical value with the number of times it appears in the dataset. For example, if the value \"GB\" occured 10 times in the country feature, then each \"GB\" would be replaced with the number 10.\n\nWe'll use the `categorical-encodings` [package](https:\/\/github.com\/scikit-learn-contrib\/category_encoders) to get this encoding. The encoder itself is available as `CountEncoder`. This encoder and the others in `categorical-encodings` work like scikit-learn transformers with `.fit` and `.transform methods`.","6be43d68":"# Categorical Encodings","f2fa82b0":"Adding the count encoding features increase the validation score from 0.7467 to 0.7486, only a slight improvement.","4e596d39":"**L1 regularization**\n\nUnivariate methods consider only one feature at a time when making a selection decision. Instead, we can make our selection using all of the features by including them in a linear model with L1 regularization. This type of regularization (sometimes called Lasso) penalizes the absolute magnitude of the coefficients, as compared to L2 (Ridge) regression which penalizes the square of the coefficients.\n\nAs the strength of regularization is increased, features which are less important for predicting the target are set to 0. This allows us to perform feature selection by adjusting the regularization parameter. We choose the parameter by finding the best performance on a hold-out set, or decide ahead of time how many features to keep.\n\nFor regression problems you can use `sklearn.linear_model.Lasso`, or `sklearn.linear_model.LogisticRegression` for classification. These can be used along with `sklearn.feature_selection.SelectFromModel` to select the non-zero coefficients. Otherwise, the code is similar to the univariate tests.","c2f14f2c":"**Univariate Feature Selection**\n\nThe simplest and fastest methods are based on univariate statistical tests. For each feature, measure how strongly the target depends on the feature using a statistical test like  \ud835\udf122  or ANOVA.\n\nFrom the scikit-learn feature selection module, `feature_selection.SelectKBest` returns the K best features given some scoring function. For our classification problem, the module provides three different scoring functions:  \ud835\udf122 , ANOVA F-value, and the mutual information score. The F-value measures the linear dependency between the feature variable and the target. This means the score might underestimate the relation between a feature and the target if the relationship is nonlinear. The mutual information score is nonparametric and so can capture nonlinear relationships.\n\nWith `SelectKBest`, we define the number of features to keep, based on the score from the scoring function. Using `.fit_transform(features, target)` we get back an array with only the selected features.","1ad2e359":"# Feature Selection","66ebf630":"**Prep categorical variables**\n\nNow for the categorical variables -- `category`, `currency`, and `country` -- we'll need to convert them into integers so our model can use the data. For this we'll use scikit-learn's `LabelEncoder`. This assigns an integer to each value of the categorical feature.","ac32468e":"**Transforming numerical features**\n\nThe distribution of the values in `\"goal\"` shows that most projects have goals less than 5000 USD. However, there is a long tail of goals going up to $100,000. Some models work better when the features are normally distributed, so it might help to transform the goal values. Common choices for this are the square root and natural logarithm. These transformations can also help constrain outliers.\n\nHere I'll transform the goal feature using the square root and log functions as examples.","81c24513":"You are already familiar with the most basic encodings: one-hot encoding and label encoding. In this tutorial, you'll learn about **count encoding**, **target encoding**, and **CatBoost encoding**.","6cce9ec8":"Similar to the univariate tests, we get back an array with the selected features. Again, we will want to convert these to a DataFrame so we can get the selected columns.","002ab5cc":"Then, we can label encode the interaction feature and add it to our data.","bfa5da31":"**Make predictions & evaluate the model**\n\nFinally, let's make predictions on the test set with the model and see how well it performs. An important thing to remember is that you can overfit to the validation data. This is why we need a test set that the model never sees until the final evaluation.","a174353f":"The `state` column shows the outcome of the project.","26112bdd":"We get `NaNs` here for projects that are the first in their category. We'll need to fill those in with something like the mean or median. We'll also need to reset the index so we can join it with the other data.","6ab66130":"Now join the new feature with the other data again using `.join` since we've matched the index.","8795090c":"This returns a DataFrame with the same index and columns as the training set, but all the dropped columns are filled with zeros. We can find the selected columns by choosing features where the variance is non-zero.","855b5aa6":"# Baseline Model","01ecbe99":"Now that we have the counts, we need to adjust the index so we can join it with the other training data.","72ded330":"In this case with the L1 parameter `C=1`, we're dropping the time_since_last_project column.\n\nIn general, feature selection with L1 regularization is more powerful the univariate tests, but it can also be very slow when you have a lot of data and a lot of features. Univariate tests will be much faster on large datasets, but also will likely perform worse.","2ae4b605":"**Interactions**\n\nOne of the easiest ways to create new features is by combining categorical variables. For example, if one record has the country `\"CA\"` and category `\"Music\"`, you can create a new value `\"CA_Music\"`. This is a new categorical feature that can provide information about correlations between categorical variables. This type of feature is typically called an **interaction**.\n\nIn general, you would build interaction features from all pairs of categorical features. You can make interactions from three or more features as well, but you'll tend to get diminishing returns.\n\nPandas lets us simply add string columns together like normal Python strings.","d0fc5c21":"This does slightly better than target encoding.","b702d341":"The log transformation won't help our model since tree-based models are scale invariant. However, this should help if we had a linear model or neural network.\n\nOther transformations include squares and other powers, exponentials, etc. These might help the model discriminate, like the kernel trick for SVMs. Again, it takes a bit of experimentation to see what works. One method is to create a bunch of new features and later choose the best ones with feature selection algorithms.","40cd8bad":"**Train a model**\n\nFor this course we'll be using a LightGBM model. This is a tree-based model that typically provides the best performance, even compared to XGBoost. It's also relatively fast to train.\n\nWe won't do hyperparameter optimization because that isn't the goal of this course. So, our models won't be the absolute best performance you can get. But you'll still see model performance improve as we do feature engineering.","efb6eb68":"**Time since the last project in the same category**\n\nDo projects in the same category compete for donors? If you're trying to fund a video game and another game project was just launched, you might not get as much money. We can capture this by calculating the time since the last launch project in the same category.\n\nA handy method for performing operations within groups is to use `.groupby` then `.transform`. The `.transform` method takes a function then passes a series or dataframe to that function for each group. This returns a dataframe with the same indices as the original dataframe. In our case, we'll perform a groupby on `\"category\"` and use transform to calculate the time differences for each category.","5d722e4d":"Often you'll have hundreds or thousands of features after various encodings and feature generation. This can lead to two problems. First, the more features you have, the more likely you are to overfit to the training and validation sets. This will cause your model to perform worse at generalizing to new data.\n\nSecondly, the more features you have, the longer it will take to train your model and optimize hyperparameters. Also, when building user-facing products, you'll want to make inference as fast as possible. Using fewer features can speed up inference at the cost of predictive performance.\n\nTo help with these issues, you'll want to use feature selection techniques to keep the most informative features for your model.","7d85148e":"**Create training, validation, and test splits**\n\nWe need to create data sets for training, validation, and testing. We'll use a fairly simple approach and split the data using slices. We'll use 10% of the data as a validation set, 10% for testing, and the other 80% for training.","bc460abe":"The validation score is higher again, from 0.7467 to 0.7491."}}