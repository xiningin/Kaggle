{"cell_type":{"5784cd57":"code","594c17df":"code","af94d98b":"code","a20bb86a":"code","a8cfc643":"code","3d8c4bbd":"code","54873688":"code","2cd80b0b":"code","517570c5":"code","7d85a5ab":"code","20179efd":"code","894c160f":"code","de5ecd7f":"code","886e2c4e":"code","4f44deda":"code","172b6512":"code","40528b39":"code","884b9ea8":"code","74048eb8":"code","bb670749":"code","25df9ab2":"code","c022e561":"markdown","2c6d12e0":"markdown","fca891bd":"markdown","e6556b28":"markdown","efafd3e6":"markdown","c9602ec4":"markdown","c65d49c8":"markdown","3578c69a":"markdown","168b4928":"markdown","dcedbef5":"markdown"},"source":{"5784cd57":"import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt","594c17df":"df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv', index_col=0)\ndf.head()","af94d98b":"df.keyword.value_counts()","a20bb86a":"print(f\"Actual disaster tweets {(df.target==1).sum()\/len(df.target):.2f}\")\nprint(f\"Fake disaster tweets {(df.target==0).sum()\/len(df.target):.2f}\")","a8cfc643":"df.keyword.value_counts(dropna=False)","3d8c4bbd":"# data_keywords = df.keyword\ndf.keyword.str.extractall(r\"(\\w+)+\")","54873688":"df.location.value_counts(dropna=False)","2cd80b0b":"glove = {}\nwith open(\"..\/input\/glove6b\/glove.6B.100d.txt\") as f:\n    for line in f:\n        glove[line.split()[0]] = line.split()[1:]","517570c5":"num_words = df.text.str.split().apply(len)\nplt.hist(num_words)","7d85a5ab":"word_counts = df.text.str.lower().str.split().explode().value_counts()\nword_counts.cumsum()[10000] \/ word_counts.sum()","20179efd":"NUM_WORDS = 10000\nMAXLEN = 30\n\ntexts = df.text.str.lower()\ntokenizer = keras.preprocessing.text.Tokenizer(num_words=NUM_WORDS)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\nword_index = tokenizer.word_index\ndata = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=MAXLEN)","894c160f":"labels = df.target","de5ecd7f":"x_train = data\ny_train = labels","886e2c4e":"EMBEDDING_DIM = len(glove[\"the\"])\nembedding_matrix = np.zeros((NUM_WORDS, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    if i < NUM_WORDS:\n        embedding_vector = glove.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector","4f44deda":"(embedding_matrix == 0).sum() \/ embedding_matrix.ravel().shape","172b6512":"from tensorflow.keras import layers\nmodel = keras.Sequential([\n    layers.Embedding(NUM_WORDS, EMBEDDING_DIM, input_length=MAXLEN,\n                    name='embedding'),\n    layers.Bidirectional(layers.GRU(32, \n                                    dropout=.2, \n                                    recurrent_dropout=.2, )),\n#     layers.Conv1D(8, 16, activation='relu'),\n#     layers.MaxPool1D(),\n#     layers.Conv1D(16, 4, activation='relu'),\n#     layers.MaxPool1D(),\n#     layers.Flatten(),\n#     layers.Dense(32, activation='relu'),\n#     layers.Dropout(.2),\n    layers.Dense(1, activation='sigmoid'),\n])\nmodel.get_layer('embedding').set_weights([embedding_matrix])\nmodel.get_layer('embedding').trainable = False\noptimizer = keras.optimizers.RMSprop(lr=1e-2)\nmodel.compile(loss='binary_crossentropy', \n              optimizer=optimizer,\n              metrics=['acc'])\nmodel.summary()","40528b39":"early_stopping = keras.callbacks.EarlyStopping(\n    patience=10, \n    restore_best_weights=True,\n)\nlr_decay = keras.callbacks.ReduceLROnPlateau()\nhistory = model.fit(\n     x_train, y_train,\n     epochs=50,\n     batch_size=32,\n     validation_split=.2,\n     callbacks=[early_stopping, lr_decay]\n)","884b9ea8":"for label in ['acc','val_acc','loss', 'val_loss']:\n    metric = history.history[label]\n    steps = range(1, len(metric)+1)\n    plt.plot(steps, metric, label=label)\n\nplt.legend()","74048eb8":"test_df = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\",\n                     index_col=0)\ndef preprocess(texts, labels=None, tokenizer=tokenizer):\n    NUM_WORDS = 10000\n    MAXLEN = 30\n    texts = pd.Series(texts).str.lower()\n    sequences = tokenizer.texts_to_sequences(texts)\n    data = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=MAXLEN)\n    return data, np.array(labels)\n\nx_test,_ = preprocess(test_df.text)\npreds = model.predict(x_test)\npreds","bb670749":"answer_df = pd.read_csv(\n    '..\/input\/nlp-getting-started\/sample_submission.csv',\n    index_col=0\n)\nanswer_df['target'] = (preds > 0.5).astype('uint8')\nanswer_df.to_csv('submission.csv')\n!head submission.csv","25df9ab2":"!head ..\/input\/nlp-getting-started\/sample_submission.csv","c022e561":"Yes. Pretty much.  \nWhat's in other two columns?","2c6d12e0":"Remember to make .str.lower() on predictions.","fca891bd":"Is this dataset balanced?","e6556b28":"I think we'll just note if this column is empty or not as abinary label.","efafd3e6":"At how many words should we pad the sequences?","c9602ec4":"We need a word embedding layer. Let's read it.","c65d49c8":"We'll feed this into a dense network with embedding.","3578c69a":"30 seems ok.  \nHow many distinct words are there?","168b4928":"So 26% of all items in matrix is zeros. Yikes.","dcedbef5":"10000 seems ok."}}