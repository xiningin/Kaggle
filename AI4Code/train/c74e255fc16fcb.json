{"cell_type":{"ef02c6d2":"code","3733e407":"code","ab0987d7":"code","cb0dfa1f":"code","0fbf1d37":"code","75b58a79":"code","9c018f18":"code","579cccd6":"code","0a12509c":"code","c98b80a2":"code","58ae700c":"code","b3679e8e":"code","b8f1bcab":"code","29af2c00":"code","e9d067b2":"code","98feb121":"code","d441ac93":"code","008f68d1":"code","c69ad34d":"code","01ca14df":"code","673a2faa":"markdown","0d1e33d3":"markdown","4b987af3":"markdown","919f5a1c":"markdown","122f574e":"markdown","dfa20e4b":"markdown","094cd298":"markdown","a23f02b2":"markdown"},"source":{"ef02c6d2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","3733e407":"data=pd.read_csv('..\/input\/titanic\/train.csv')\ndata.head(8)","ab0987d7":"data.tail(8)","cb0dfa1f":"X=data.copy()\ny=X.pop('Survived')\n#X=data.drop(['PassengerId', 'Survived'], axis=1)\nX.describe()","0fbf1d37":"X.isnull().sum()\/len(X)*100","75b58a79":"y.value_counts()\n#the target column is also almost balanced","9c018f18":"#correlation matrix between features\nX.corr()","579cccd6":"#grouping by Survived and counting classes\ncount=data.groupby('Survived')['Pclass'].value_counts()\nsurv, nsurv={}, {}\nfor cl, val in zip(count.index[:3], count.values[:3]):nsurv[cl[1]]=val\nfor cl, val in zip(count.index[3:], count.values[3:]):surv[cl[1]]=val\n\nsns.set_style('darkgrid')\nfig, axes=plt.subplots(nrows=1, ncols=2, figsize=(8,5))\nsns.barplot(ax=axes[0], x=list(surv.keys()), y=list(surv.values()))\naxes[0].set_title(\"Survived passengers VS Class\")\nsns.barplot(ax=axes[1], x=list(nsurv.keys()), y=list(nsurv.values()))\naxes[1].set_title(\"Dead passengers VS Class\")\nplt.show()","0a12509c":"#series plot of fares of all passengers\nplt.figure(figsize=(15,5))\nsns.lineplot(x=np.arange(0, len(data)), y=data['Fare'])\nplt.show()","c98b80a2":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler,LabelEncoder\nfrom sklearn.impute import SimpleImputer\nimport category_encoders as ce\n\ndef drop_cols(data):\n    data['Family']=data['SibSp']+data['Parch']\n    cols=['PassengerId', 'Name', 'Cabin', 'Ticket', 'Parch', 'SibSp']\n    return data.drop(cols, axis=1)\n\ndef fare_handling(data):\n    for i in range(len(data.Fare)):\n        if data.Fare[i]<32.0: data.loc[i, 'Fare']=0\n        elif 32.0<data.Fare[i]<100.0: data.loc[i, 'Fare']=1\n        elif 100.0<data.Fare[i]<170.0: data.loc[i, 'Fare']=2\n        else: data.loc[i, 'Fare']=3\n    return data.Fare\n\ndef preprocess(train, test):\n    train=drop_cols(train)\n    test=drop_cols(test)\n    si_num=SimpleImputer(strategy='median')\n    si_cat=SimpleImputer(strategy='most_frequent')\n    le=LabelEncoder()\n    ms=MinMaxScaler()\n    \n    #fill NAN values\n    train['Age']=si_num.fit_transform(train['Age'].values.reshape(-1,1))\n    test['Age']=si_num.transform(test['Age'].values.reshape(-1,1))\n    train['Sex']=si_cat.fit_transform(train['Sex'].values.reshape(-1,1))\n    test['Sex']=si_cat.transform(test['Sex'].values.reshape(-1,1))\n    train['Embarked']=si_cat.fit_transform(train['Embarked'].values.reshape(-1,1))\n    test['Embarked']=si_cat.transform(test['Embarked'].values.reshape(-1,1))\n    \n    #doing transforms\n    train.Sex=le.fit_transform(train.Sex)\n    test.Sex=le.transform(test.Sex)\n    \n    train.Age=ms.fit_transform(train.Age.values.reshape(-1,1))\n    test.Age=ms.transform(test.Age.values.reshape(-1,1))\n    \n    train.Fare=fare_handling(train)\n    test.Fare=fare_handling(test)\n    \n    OHE = ce.OneHotEncoder(cols=['Embarked'],use_cat_names=True)\n    train = OHE.fit_transform(train)\n    test = OHE.transform(test)\n    \n    return train.values, test.values\n    ","58ae700c":"X_train, X_test, y_train, y_test=train_test_split(X, y,test_size=0.25,random_state=0)\nX_test=X_test.reset_index(drop=True)\nX_train=X_train.reset_index(drop=True)\nX_train, X_test = preprocess(X_train, X_test)\nX_test.shape","b3679e8e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nlr=LogisticRegression()\nlr.fit(X_train, y_train)\nres=lr.predict(X_test)\nprint(accuracy_score(res, y_test))\nprint(classification_report(res, y_test))","b8f1bcab":"from sklearn.ensemble import RandomForestClassifier\nrf_model=RandomForestClassifier(random_state=0)\nrf_model.fit(X_train, y_train)\nprint(accuracy_score(rf_model.predict(X_test), y_test)*100)","29af2c00":"test=pd.read_csv('..\/input\/titanic\/test.csv')\nids=test.PassengerId\ntrain, test = preprocess(X, test)","e9d067b2":"from sklearn.model_selection import GridSearchCV\nhyperparameters={\n    \n    'n_estimators': [80, 90, 100, 120, 160, 180],\n    'max_depth' : [4,5,6, 7,8, 9],\n    'max_features':[\"auto\", \"sqrt\", \"log2\"],\n    'bootstrap':[False, True],\n    \n}\nrf_model=RandomForestClassifier(random_state=0)\nrf_gs = GridSearchCV(estimator=rf_model, param_grid=hyperparameters, cv= 3,\n                     verbose=1, refit='true', n_jobs=-1)\nrf_gs.fit(train, y.values)\nprint(rf_gs.best_params_)\n#print(accuracy_score(rf_gs.predict(X_test), y_test)*100)","98feb121":"#final prediction\nresult=rf_gs.predict(test)\noutput = pd.DataFrame({'PassengerId': ids, 'Survived': result})\noutput.to_csv('.\/submission.csv', index=False)","d441ac93":"models=[LogisticRegression(max_iter=1000), RandomForestClassifier(random_state=0), DecisionTreeClassifier(random_state=0)]\nmodel_list=[]\nfor m in models:\n    cv = KFold(n_splits=5, random_state=0, shuffle=True)\n    score=cross_val_score(m, X_train, y_train,  scoring = 'accuracy', cv=cv)\n    model_list.append(score)\nprint(model_list)","008f68d1":"from sklearn.model_selection import GridSearchCV\nhyperparameters={\n    \n    'n_estimators': [180, 200, 240,],\n    'max_depth' : [8, 9, 10],\n    'max_features':['auto'],\n    'criterion':['entropy']\n}\nrforest=RandomForestClassifier(random_state=0)\nrf = GridSearchCV(estimator=rforest, param_grid=hyperparameters, cv= 5, verbose=1, refit='true')\nrf.fit(x_train, y_train)\nprint(rf.best_params_)","c69ad34d":"result=rf.predict(x_test)","01ca14df":"output = pd.DataFrame({'PassengerId': test_set.PassengerId, 'Survived': result})\noutput.to_csv('.\/submission.csv', index=False)","673a2faa":"### Experiments to be tried during preprocessing:\n1. Drop Fare and keep pclass\n2. Drop embarkment\n3. Keep Fare as continous value","0d1e33d3":"We see in Survived category Class 1 survived the most. While in Dead category Class 3 people didn't have much luck like Class 1. Hence Class plays the role of very important feature.","4b987af3":"Logistic Regression ","919f5a1c":"We observe good correlation = 0.41 between sibsp and parch, hence we can add them up to 1 single column.\n\nNext we observe good negatie correlation = -0.55 between fares and class, obviously high fares have low numbered(high ranked) classes. So both almost convey same information. Now it is experimental determined to drop one of them or keep both. We shall try both.","122f574e":"Fare has 3rd quartile at 31, and mean at 32 and std dev = 50 suggesting high magnitude outliers. We have a very few number of people with fares above 31 majority are around 30-40.","dfa20e4b":"we observe cabin has highest % of null values reaching 77%.","094cd298":"Fare already had the 3rd quartile at 32. Hence we can categorise the passengers based on fare. Let <32, >32 & <100, >100 & <170, >170","a23f02b2":"Random Forest"}}