{"cell_type":{"ec89ed06":"code","8cff23e6":"code","37b84743":"code","23ca5b96":"code","7a4980f6":"code","07fc2f45":"code","7118b7ae":"code","bf04044a":"code","34cb16da":"code","6885c174":"code","2c31fdf7":"code","e82d2eac":"markdown","87388c4e":"markdown","ddba0014":"markdown","5682fdc8":"markdown","9d916389":"markdown"},"source":{"ec89ed06":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.callbacks import Callback\nfrom keras import regularizers\nfrom keras import optimizers\n\nfrom functools import partial\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom ipywidgets import *\n\nimport numpy as np","8cff23e6":"# Dot count \nsamples = 1000\ntest_n = 200","37b84743":"# X interval\nx_int = (-9, 9)\n# Y interval\ny_int = (-1, 1)","23ca5b96":"# Generate\ndata = [(np.random.random() * (x_int[1] - x_int[0]) + x_int[0], \n                  np.random.random() * (y_int[1] - y_int[0]) + y_int[0]) for i in range(samples + test_n)]\n#print(data)\ntrain_data = np.array(data[test_n:])\ntest_data = np.array(data[:test_n])\nprint(train_data)","7a4980f6":"\n# Your variant function\ndef main_func(x):\n    return np.cos(x)\ndef main_func_noisy(x):\n    return main_func(x) + np.cos(4*x + 1) \/ 5\n\ndef result_func(xy):\n    return main_func(xy[0]) > xy[1]\n\ndef result_func_noisy(xy):\n    return main_func_noisy(xy[0]) > xy[1]","07fc2f45":"train_answers = np.apply_along_axis(result_func_noisy, arr=train_data, axis=1)\n#answers = np.reshape(answers, (samples,1))\nprint(train_answers.shape)\n\ntest_answers = np.apply_along_axis(result_func, arr=test_data, axis=1)\nprint(test_answers.shape)","7118b7ae":"train_data_true = train_data[train_answers]\ntrain_data_false = train_data[np.logical_not(train_answers)]\nprint(train_data_true.shape)\nprint(train_data_false.shape)","bf04044a":"fig, ax = plt.subplots()\n\nxx = np.linspace(*x_int, 100)\n\nax.plot(train_data_true[:,0], train_data_true[:,1], 'g.')\nax.plot(train_data_false[:,0], train_data_false[:,1], 'r.')\nax.plot(xx, main_func(xx), 'b--')\nax.plot(xx, main_func_noisy(xx), 'y-')","34cb16da":"def plot_accuracy(history):\n    # Plot training & validation accuracy values\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    # Plot training & validation loss values\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()","6885c174":"def plot_contour_graph(model):\n    xx = np.linspace(*x_int, 100)\n    yy = np.linspace(*y_int, 200)\n\n    xm, ym = np.meshgrid(xx, yy)\n    xys = np.array([xm, ym]).reshape((2, -1))\n\n    z_actual = result_func(xys).reshape(200,100)\n\n    xys = np.moveaxis(xys, 0, 1)\n    z = model.predict(xys).reshape(200, 100)\n\n    \n    plt.contourf(xm, ym, z)\n    plt.title('Neural net vision of the function')\n    plt.colorbar()\n    plt.show()\n    \n    plt.contourf(xm, ym, z_actual)\n    plt.title('Actual function')\n    plt.colorbar()\n    plt.show()","2c31fdf7":"style = {'description_width': 'initial'}\n@interact_manual(\n    n_layers = IntSlider(min=0, max=4, value=2, description='Layer: '),\n    layer1_size = IntSlider(min=0, max=20, value=6, description='Neurons count in layer 1: ', style=style),\n    layer2_size = IntSlider(min=0, max=20, value=3, description='Neurons count in layer 2: ', style=style),\n    layer3_size = IntSlider(min=0, max=20, value=0, description='Neurons count in layer 1: ', style=style),\n    layer4_size = IntSlider(min=0, max=20, value=0, description='Neurons count in layer 1: ', style=style),\n    layer1_type = Dropdown(options=['softmax', 'relu', 'tanh', 'sigmoid', 'linear'], value='relu',\n                          description='Layer 1 activation type: ', style=style),\n    layer2_type = Dropdown(options=['softmax', 'relu', 'tanh', 'sigmoid', 'linear'], value='relu',\n                          description='Layer 2 activation type: ', style=style),\n    layer3_type = Dropdown(options=['softmax', 'relu', 'tanh', 'sigmoid', 'linear'], value='relu',\n                          description='Layer 3 activation type: ', style=style),\n    layer4_type = Dropdown(options=['softmax', 'relu', 'tanh', 'sigmoid', 'linear'], value='relu',\n                          description='Layer 4 activation type: ', style=style),\n    layer_out_type = Dropdown(options=['softmax', 'relu', 'tanh', 'sigmoid', 'linear'], value='sigmoid',\n                          description='Output layer activation type: ', style=style),\n    loss_func = Dropdown(options={\n        'Standard deviation': 'mse', \n        'Mean absolute deviation': 'mae',\n        'Binary crossentropy': 'binary_crossentropy'\n    }, value='binary_crossentropy', description='Loss function: ', style=style),\n    epochs = IntSlider(min=10, max=10000, step=10, value=300, description='Epoch count: ', style=style),\n    batch_size = IntSlider(min=0, max=len(data), value=100, description='Batch size: ', style=style),\n    lr = ToggleButtons(options=[\"-0.1\", \"0\", \"0.001\", \"0.01\", \"0.05\", \"0.1\", \"0.5\", \"1\", \"5\"], \n                               value=\"0.01\", description='Learn rate: ', style=style),\n    l1=ToggleButtons(options=[\"-0.1\", \"0\", \"0.0001\", \"0.0005\", \"0.001\", \"0.005\", \"0.01\", \"0.05\", \"0.1\"], \n                               value=\"0.0001\", description='Regularization  L1: ', style=style),\n    l2=ToggleButtons(options=[\"-0.1\", \"0\", \"0.0001\", \"0.0005\", \"0.001\", \"0.005\", \"0.01\", \"0.05\", \"0.1\"], \n                               value=\"0.0001\", description='Regularization  L2: ', style=style)\n)\ndef interactive_learning(n_layers, loss_func, batch_size, lr, l1, l2, layer_out_type, epochs,\n                         layer1_size, layer2_size, layer3_size, layer4_size,\n                         layer1_type, layer2_type, layer3_type, layer4_type):\n    layer_sizes = [layer1_size, layer2_size, layer3_size, layer4_size]\n    layer_types = [layer1_type, layer2_type, layer3_type, layer4_type]\n    lr = float(lr)\n    l1 = float(l1)\n    l2 = float(l2)\n    \n    model = Sequential()\n    \n    if n_layers == 0:\n        model.add(Dense(1, activation=layer_out_type, \n                        input_shape=(2,), kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2)))\n    else:\n        model.add(Dense(layer_sizes[0], activation=layer_types[0], \n                        input_shape=(2,), kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2)))\n        for i in range(1, n_layers):\n            model.add(Dense(layer_sizes[i], activation=layer_types[i],\n                           kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2)))\n        model.add(Dense(1, activation=layer_out_type, kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2)))\n    \n    model.compile(\n     optimizer = optimizers.sgd(lr=lr),\n     loss = loss_func,\n     metrics = [\"accuracy\"]\n    )\n    \n    \n    history = model.fit(\n     train_data, train_answers,\n     epochs=epochs,\n     batch_size=batch_size,\n     validation_data=(test_data, test_answers),\n     verbose=0\n    )\n    \n    print('Accuracy: ', history.history['val_acc'][-1])\n    plot_accuracy(history)\n    plot_contour_graph(model)","e82d2eac":"# Plot Drawing","87388c4e":"## Experimentation","ddba0014":"# Generate data","5682fdc8":"## Setting parameters and contur plot drawing","9d916389":"# Import dependence"}}