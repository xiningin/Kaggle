{"cell_type":{"11df747f":"code","975fd9c6":"code","b472a0cd":"code","728890f7":"code","06e384a0":"code","ab07a86f":"code","f3bacd64":"code","db80d3ae":"code","dda1aeae":"code","48903132":"code","f5856373":"code","ce84848f":"code","4b10b100":"code","f2141084":"code","488e3eb6":"code","f613ceae":"code","9381aa7e":"code","888399be":"code","42a99beb":"code","90fb74b0":"code","85f7047a":"code","81ca87f0":"code","8dd22c65":"code","172cefa3":"code","9152d145":"code","b5e2f371":"code","8331fc03":"code","9843ac14":"code","35b4949e":"code","73e1ff73":"code","519a8048":"markdown","4c56f3a3":"markdown","2b3718a7":"markdown","c348f9bd":"markdown","3bd96270":"markdown","845c7ecd":"markdown"},"source":{"11df747f":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization, Input\nprint(f'Tensorflow version: {tf.__version__}')","975fd9c6":"df = pd.read_csv(\"..\/input\/audio-cats-and-dogs\/train_test_split.csv\")\n\ndf.head()","b472a0cd":"dog_train_path = \"..\/input\/audio-cats-and-dogs\/cats_dogs\/train\/dog\/\"\ndog_test_path = \"..\/input\/audio-cats-and-dogs\/cats_dogs\/test\/test\/\"\n\ncat_train_path = \"..\/input\/audio-cats-and-dogs\/cats_dogs\/train\/cat\/\"\ncat_test_path = \"..\/input\/audio-cats-and-dogs\/cats_dogs\/test\/cats\/\"\n\ntrain_cat_fnames=os.listdir(cat_train_path)\ntrain_cat_fnames[:10]","728890f7":"test_cat = df[['test_cat']].dropna().rename(index=str, columns={\"test_cat\": \"file\"}).assign(label=0)\ntest_dog = df[['test_dog']].dropna().rename(index=str, columns={\"test_dog\": \"file\"}).assign(label=1)\ntrain_cat = df[['train_cat']].dropna().rename(index=str, columns={\"train_cat\": \"file\"}).assign(label=0)\ntrain_dog = df[['train_dog']].dropna().rename(index=str, columns={\"train_dog\": \"file\"}).assign(label=1)\n\ntest_df = pd.concat([test_cat, test_dog]).reset_index(drop=True)\ntrain_df = pd.concat([train_cat, train_dog]).reset_index(drop=True)\n\ntrain_df.head(300)","06e384a0":"path=os.path.join(cat_train_path, df['train_cat'].loc[0])","ab07a86f":"from IPython.display import Audio\n\nAudio(path)","f3bacd64":"from pydub import AudioSegment\n\nwav_file = AudioSegment.from_file(file=path, format=\"wav\") \n\nprint(f\"Data type: {type(wav_file)}\") \nprint(f\"Frame rate: {wav_file.frame_rate\/1000} kHz\")\nprint(f\"Channels: {wav_file.channels}\") \nprint(f\"Number of bytes per sample: {wav_file.sample_width*8} bit\") \nprint(f\"Maximum amplitude: {wav_file.max}\")\nprint(f\"Length: {len(wav_file) \/ 1000.0} s\")\nprint(f\"Loudness dBFS: {wav_file.dBFS:.2f} dB\")\nprint(f\"Loudness max_dBFS: {wav_file.max_dBFS:.2f} dB\")\nprint(f\"Loudness RMS{wav_file.rms}\")\n\nsamples = wav_file.get_array_of_samples()\nsamples = np.array(samples)\nprint(samples)","db80d3ae":"from scipy.io import wavfile\n\nsamplerate, data_arr = wavfile.read(path)\nprint(samplerate)\nprint(data_arr)\n\nprint(data_arr.dtype)\nif data_arr.dtype == 'int16':\n    nb_bits = 16  # -> 16-bit wav files\nelif data_arr.dtype == 'int32':\n    nb_bits = 32  # -> 32-bit wav files\n    \nmax_nb_bit = float(2 ** (nb_bits - 1))\nsamples = data_arr \/ (max_nb_bit + 1)\nprint(samples)\n\nfig, axs = plt.subplots(1, 2, figsize=(16,4))\naxs[0].plot(data_arr)\naxs[1].plot(samples);","dda1aeae":"import librosa\n\ny, sr = librosa.load(path, sr=None)\nprint(y)\nprint(sr)","48903132":"import librosa.display\nplt.figure(figsize=(14, 5))\nlibrosa.display.waveplot(y, sr=sr);","f5856373":"fig, ax = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(16, 6))\n\nD = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\nimg=librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz', ax=ax[0])\nax[0].set_title('Linear-frequency power spectrogram')\nax[0].label_outer()\n\nhop_length = 1024\nD = librosa.amplitude_to_db(np.abs(librosa.stft(y, hop_length=hop_length)), ref=np.max)\nlibrosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log', ax=ax[1])\nax[1].set_title('Log-frequency power spectrogram')\nax[1].label_outer()\n\nfig.colorbar(img, ax=ax, format=\"%+2.f dB\");","ce84848f":"mfccs = librosa.feature.mfcc(y, sr=sr)\nprint(mfccs.shape)\n\nplt.figure(figsize=(16, 6))\nlibrosa.display.specshow(mfccs, sr=sr, x_axis='time');","4b10b100":"hop_length=12\nchromagram = librosa.feature.chroma_stft(y, sr=sr, hop_length=hop_length)\n\nplt.figure(figsize=(16, 6))\nlibrosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='coolwarm');","f2141084":"train_dir='..\/input\/audio-cats-and-dogs\/cats_dogs\/train'\ntest_dir='..\/input\/audio-cats-and-dogs\/cats_dogs\/test'\n\nfile_train = tf.io.gfile.glob(train_dir + '\/*\/*')\nfile_train = tf.random.shuffle(file_train)\ntrain_ds=file_train[:168]\nval_ds = file_train[168:168+42]\n\nfile_test = tf.io.gfile.glob(test_dir + '\/*\/*')\nfile_test = tf.random.shuffle(file_test)\ntest_ds=file_test\n\nprint(train_ds.shape)\nprint(val_ds.shape)\nprint(test_ds.shape)","488e3eb6":"def get_waveform_label(file):\n    lab = tf.strings.split(file, os.path.sep)[-2]\n    audio_binary = tf.io.read_file(file)\n    audio, _ = tf.audio.decode_wav(audio_binary)\n    waveform=tf.squeeze(audio, axis=-1)\n    return waveform, lab\n\nAUTO = tf.data.AUTOTUNE\nfiles_ds = tf.data.Dataset.from_tensor_slices(train_ds)\nwaveform_ds = files_ds.map(get_waveform_label, num_parallel_calls=AUTO)","f613ceae":"fig, axs = plt.subplots(3,3,figsize=(14,14))\n\nfor i, (audio,label) in enumerate(waveform_ds.take(3*3)):\n    r = i\/\/3\n    c = i%3\n    ax = axs[r][c]\n    ax.plot(audio.numpy())\n    label = label.numpy().decode('utf-8')\n    ax.set_title(label)","9381aa7e":"def get_spectrogram_label(audio, label):\n    padding = tf.zeros([300000]-tf.shape(audio), dtype=tf.float32)\n    wave = tf.cast(audio, tf.float32)\n    eq_length = tf.concat([wave, padding], 0)\n    spectrogram = tf.signal.stft(eq_length, frame_length=210, frame_step=110)    \n    spectrogram = tf.abs(spectrogram)\n    spectrogram = tf.expand_dims(spectrogram, -1)\n    label_id = tf.argmax(label == labels)\n    return spectrogram, label_id\n\nName=[\"cat\", \"dog\", \"test\"]\nlabels = np.array(Name)\nspectrogram_ds = waveform_ds.map(get_spectrogram_label, num_parallel_calls=AUTO)","888399be":"def plot_spect(spectrogram, ax):\n    log_spec = np.log(spectrogram.T)\n    height = log_spec.shape[0]\n    width = log_spec.shape[1]\n    X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n    Y = range(height)\n    ax.pcolormesh(X, Y, log_spec)\n\nfig, axes = plt.subplots(3,3, figsize=(14,14))\n\nfor i, (spectrogram,label_id) in enumerate(spectrogram_ds.take(3*3)):\n    r = i\/\/3\n    c = i%3\n    ax = axes[r][c]\n    plot_spect(np.squeeze(spectrogram.numpy()), ax)\n    ax.set_title(labels[label_id.numpy()])\n    ax.axis('off')","42a99beb":"def preprocess(file):\n    files_ds = tf.data.Dataset.from_tensor_slices(file)\n    output_ds = files_ds.map(get_waveform_label,num_parallel_calls=AUTO)\n    output_ds = output_ds.map(get_spectrogram_label,num_parallel_calls=AUTO)\n    return output_ds\n\ntrain_ds = spectrogram_ds\nval_ds = preprocess(val_ds)\ntest_ds = preprocess(test_ds)\n\nbatch_size = 64\ntrain_ds = train_ds.batch(batch_size)\nval_ds = val_ds.batch(batch_size)\n\ntrain_ds = train_ds.cache().prefetch(AUTO)\nval_ds = val_ds.cache().prefetch(AUTO)","90fb74b0":"for spectrogram,_ in spectrogram_ds.take(1):\n    input_shape = spectrogram.shape\n\nnum_labels = len(labels)\nnorm_layer = preprocessing.Normalization()\nnorm_layer.adapt(spectrogram_ds.map(lambda x, _: x))\n\nprint(input_shape)\nprint(num_labels)","85f7047a":"model = Sequential([\n   Input(shape=input_shape), preprocessing.Resizing(32, 32), norm_layer,\n   Conv2D(32,3, activation='relu'),\n   Conv2D(64,3, activation='relu'),\n   MaxPool2D(),\n   Dropout(0.5),\n   Conv2D(128,7, activation='relu'),\n   Conv2D(256,7, activation='relu'),\n   MaxPool2D(),\n   Dropout(0.5),\n   Flatten(),\n   Dense(128, activation='relu'),\n   Dropout(0.2),\n   Dense(16, activation='relu'),\n   Dense(num_labels),\n])\n\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nmodel.summary()","81ca87f0":"his = model.fit(train_ds, epochs=30, validation_data=val_ds)","8dd22c65":"fig, ax = plt.subplots(figsize=(10,5))\n\nplt.plot(his.epoch, his.history['loss'], his.history['val_loss'])\nplt.legend(['loss', 'val_loss']);","172cefa3":"fig, ax = plt.subplots(figsize=(10,5))\n\nplt.plot(his.epoch, his.history['accuracy'], his.history['val_accuracy'])\nplt.legend(['accuracy', 'val_accuracy']);","9152d145":"test_audio = []\ntest_labels = []\n\nfor audio, label in test_ds:\n    test_audio.append(audio.numpy())\n    test_labels.append(label.numpy())\n\ntest_audio = np.array(test_audio)\ntest_labels = np.array(test_labels)\ntest_labels","b5e2f371":"y_pred = np.argmax(model.predict(test_audio), axis=1)\nprint(y_pred)\n\nt_labels=[]\npred_labels=[]\n\nfor item in test_labels:\n    if item == 2:\n        t_labels+=[1]\n    else:\n        t_labels+=[0]\n        \nfor item in y_pred:\n    if item == 2:\n        pred_labels+=[1]\n    else:\n        pred_labels+=[item]\n\ny_true = np.array(t_labels)\n\ny_pred = np.array(pred_labels)\nprint(y_pred)","8331fc03":"test_acc = sum(y_pred == y_true) \/ len(y_true)\nprint(f'Test set accuracy: {test_acc:.0%}')","9843ac14":"from sklearn.metrics import classification_report, log_loss, accuracy_score\n\nprint(classification_report(y_true,y_pred))","35b4949e":"cm = tf.math.confusion_matrix(y_true, y_pred)\n\nf, ax = plt.subplots(figsize=(15, 6))\nsns.heatmap(cm, xticklabels=Name[:2], yticklabels=Name[:2], annot=True, cmap='Blues', square=True, linewidths=0.01, linecolor='grey')\nplt.title('Confustion matrix')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')","73e1ff73":"sample_file = '..\/input\/audio-cats-and-dogs\/cats_dogs\/test\/cats\/cat_129.wav'\n\nsample_ds = preprocess([str(sample_file)])\n\nfor spectrogram, label in sample_ds.batch(1):\n    prediction = model(spectrogram)\n    plt.bar(Name, tf.nn.softmax(prediction[0]))\n    plt.title(f'Predictions for \"{Name[label[0]]}\"')","519a8048":"## Chroma feature","4c56f3a3":"## Spectrogram\nA spectrogram is a visual representation of the spectrum when the frequency of sound or other signals changes with time.","2b3718a7":"# Preprocessing","c348f9bd":"## Mel-Frequency Cepstral Coefficients(MFCCs)\nThe Mel frequency cepstral coefficients (MFCCs) of a signal are a small set of features (usually about 10\u201320) which concisely describe the overall shape of a spectral envelope. It models the characteristics of the human voice.","3bd96270":"# Model","845c7ecd":"## Waveform"}}