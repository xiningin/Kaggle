{"cell_type":{"1866fbc1":"code","105cae07":"code","431b17df":"code","4ca2cf2c":"code","816e8512":"code","961bfdb5":"code","5f794a92":"code","84da830f":"code","460c65fb":"code","44439829":"code","d738cb66":"code","2a16913d":"code","e7bbdf9c":"code","c8ab2d06":"code","355e96ed":"code","ed8d2e25":"code","529b57d7":"code","ef92469e":"code","f7a354d3":"code","17d32fdf":"code","709a3282":"code","885f1c65":"code","b12b7cd0":"code","d21e2b3f":"code","03b84699":"code","c31bb371":"code","1700865a":"code","c0d989c2":"code","e7525f2f":"code","17991515":"code","c6c6a554":"code","3c0982af":"code","76015d07":"code","b8cb815b":"code","aa9c594c":"code","10a934da":"code","b3257f3b":"code","04be3c84":"code","68401c2f":"code","fdf4b273":"markdown","f4af6e26":"markdown","a9e1d22a":"markdown","3b53b45d":"markdown","93cbabdd":"markdown","1c95e65d":"markdown","c7d15e88":"markdown","2254f37f":"markdown","7d5b6019":"markdown","8eb65e61":"markdown"},"source":{"1866fbc1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport cv2\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow as tf\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = [16, 8]\n\nprint('Using Tensorflow version:', tf.__version__)","105cae07":"try:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","431b17df":"# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Data access\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()\n\n# Configuration\nEPOCHS = 12\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync","4ca2cf2c":"train_df = pd.read_csv('\/kaggle\/input\/shopee-product-detection-open\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/shopee-product-detection-open\/test.csv')\n\ntrain_df.shape, test_df.shape","816e8512":"train_df.head()","961bfdb5":"def show_train_img(category):\n    \n    fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(24, 10))\n    \n    train_path = '\/kaggle\/input\/shopee-product-detection-open\/train\/train\/train\/'\n    ten_random_samples = pd.Series(os.listdir(os.path.join(train_path, category))).sample(10).values\n    \n    for idx, image in enumerate(ten_random_samples):\n        final_path = os.path.join(train_path, category, image)\n        img = cv2.imread(final_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        axes.ravel()[idx].imshow(img)\n        axes.ravel()[idx].axis('off')\n    plt.tight_layout()","5f794a92":"# category 33 only has 573 images == class imbalanced\n# category 11, 37, 17 also has less than 2000 images\n\nshow_train_img('33')","84da830f":"show_train_img('12')","460c65fb":"show_train_img('32')","44439829":"def show_test_img():\n    \n    fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(24, 10))\n    \n    test_path = '\/kaggle\/input\/shopee-product-detection-open\/test\/test\/test\/'\n    ten_random_samples = pd.Series(os.listdir(test_path)).sample(10).values\n    \n    for idx, image in enumerate(ten_random_samples):\n        final_path = os.path.join(test_path, image)\n        img = cv2.imread(final_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        axes.ravel()[idx].imshow(img)\n        axes.ravel()[idx].axis('off')\n    plt.tight_layout()","d738cb66":"show_test_img()","2a16913d":"# pick random samples\n\ndataset_path = {}\n\ncategories = np.sort(train_df['category'].unique())\n\nfor cat in categories:\n    try:\n        dataset_path[cat] = train_df[train_df['category'] == cat]['filename'].sample(2500)\n    except:\n        dataset_path[cat] = train_df[train_df['category'] == cat]['filename'].sample(frac=1.)","e7bbdf9c":"category_list = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09',\n                 '10', '11', '12', '13', '14', '15', '16', '17', '18', '19',\n                 '20', '21', '22', '23', '24', '25', '26', '27', '28', '29',\n                 '30', '31', '32', '33', '34', '35', '36', '37', '38', '39',\n                 '40', '41']","c8ab2d06":"train_paths = []\n\nfor idx, key in enumerate(dataset_path.keys()):\n    if key == idx:\n        for path in dataset_path[idx]:\n            train_paths.append(os.path.join(GCS_DS_PATH, 'train', 'train', 'train', category_list[idx], path))","355e96ed":"labels = []\n\nfor label in dataset_path.keys():\n    labels.extend([label] * len(dataset_path[label]))","ed8d2e25":"from tensorflow.keras.utils import to_categorical\n\n# convert to numpy array\ntrain_paths = np.array(train_paths)\n\n# convert to one-hot-encoding-labels\ntrain_labels = to_categorical(labels)","529b57d7":"from sklearn.model_selection import train_test_split\n\ntrain_paths, valid_paths, train_labels, valid_labels = train_test_split(train_paths, \n                                                                        train_labels, \n                                                                        stratify=train_labels,\n                                                                        test_size=0.1, \n                                                                        random_state=2020)\n\ntrain_paths.shape, valid_paths.shape, train_labels.shape, valid_labels.shape","ef92469e":"test_paths = []\n\nfor path in test_df['filename']:\n    test_paths.append(os.path.join(GCS_DS_PATH,  'test', 'test', 'test', path))\n    \ntest_paths = np.array(test_paths)","f7a354d3":"def decode_image(filename, label=None, image_size=(456, 456)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    image = tf.image.resize(image, image_size)\n    \n    if label is None:\n        return image\n    else:\n        return image, label","17d32fdf":"def data_augment(image, label=None):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_brightness(image, max_delta=0.25)\n    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n#     image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n#     image = tf.image.random_hue(image, max_delta=0.2)\n    \n    if label is None:\n        return image\n    else:\n        return image, label","709a3282":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .map(data_augment, num_parallel_calls=AUTO)\n    .cache()\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((valid_paths, valid_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_paths)\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n)","885f1c65":"!pip install -q efficientnet","b12b7cd0":"from tensorflow.keras.layers import Dense, Lambda, Input\nfrom tensorflow.keras.models import Model\nfrom efficientnet.tfkeras import EfficientNetB5","d21e2b3f":"import keras.backend as K\n\ndef categorical_focal_loss_with_label_smoothing(gamma=2.0, alpha=0.25, ls=0.1, classes=42.0):\n    \"\"\"\n    Implementation of Focal Loss from the paper in multiclass classification\n    Formula:\n        loss = -alpha*((1-p)^gamma)*log(p)\n        y_ls = (1 - \u03b1) * y_hot + \u03b1 \/ classes\n    Parameters:\n        alpha -- the same as wighting factor in balanced cross entropy\n        gamma -- focusing parameter for modulating factor (1-p)\n        ls    -- label smoothing parameter(alpha)\n        classes     -- No. of classes\n    Default value:\n        gamma -- 2.0 as mentioned in the paper\n        alpha -- 0.25 as mentioned in the paper\n        ls    -- 0.1\n        classes     -- 42\n    \"\"\"\n    def focal_loss(y_true, y_pred):\n        # Define epsilon so that the backpropagation will not result in NaN\n        # for 0 divisor case\n        epsilon = K.epsilon()\n        # Add the epsilon to prediction value\n        #y_pred = y_pred + epsilon\n        #label smoothing\n        y_pred_ls = (1 - ls) * y_pred + ls \/ classes\n        # Clip the prediction value\n        y_pred_ls = K.clip(y_pred_ls, epsilon, 1.0-epsilon)\n        # Calculate cross entropy\n        cross_entropy = -y_true*K.log(y_pred_ls)\n        # Calculate weight that consists of  modulating factor and weighting factor\n        weight = alpha * y_true * K.pow((1-y_pred_ls), gamma)\n        # Calculate focal loss\n        loss = weight * cross_entropy\n        # Sum the losses in mini_batch\n        loss = K.sum(loss, axis=1)\n        return loss\n    \n    return focal_loss","03b84699":"# %%time\n\n# with strategy.scope():\n#     model = tf.keras.Sequential([\n#         EfficientNetB5(weights='noisy-student', # imagenet\n#                        include_top=False,\n#                        pooling='avg'), # max\n#         Dense(42, activation='softmax')\n#     ])\n    \n#     model.layers[0].trainable = False\n    \n#     model.compile(optimizer = 'adam',\n#                   loss = categorical_focal_loss_with_label_smoothing(gamma=2.0, alpha=0.75, ls=0.125, classes=42.0), # num classes\n#                   metrics=['accuracy'])\n    \n#     model.summary()","c31bb371":"def outer_product(x):\n    #Einstein Notation  [batch,1,1,depth] x [batch,1,1,depth] -> [batch,depth,depth]\n    phi_I = tf.einsum('ijkm,ijkn->imn', x[0], x[1])\n    \n    # Reshape from [batch_size,depth,depth] to [batch_size, depth*depth]\n    phi_I = tf.reshape(phi_I,[-1,x[0].shape[3]*x[1].shape[3]])\n    \n    # Divide by feature map size [sizexsize]\n    size1 = int(x[1].shape[1])\n    size2 = int(x[1].shape[2])\n    phi_I = tf.divide(phi_I, size1*size2)\n    \n    # Take signed square root of phi_I\n    y_ssqrt = tf.multiply(tf.sign(phi_I),tf.sqrt(tf.abs(phi_I)+1e-12))\n    \n    # Apply l2 normalization\n    z_l2 = tf.nn.l2_normalize(y_ssqrt, axis=1)\n    return z_l2","1700865a":"IMG_SIZE_h = 456 \nIMG_SIZE_w = 456\n\ndef get_model():\n    \n    input_tensor = Input(shape=(IMG_SIZE_h, IMG_SIZE_w, 3))\n    \n    model1 = EfficientNetB5(weights='imagenet', include_top=False, input_tensor=input_tensor,input_shape=(IMG_SIZE_h, IMG_SIZE_w, 3))\n    model2 = EfficientNetB5(weights='noisy-student', include_top=False, input_tensor=input_tensor,input_shape=(IMG_SIZE_h, IMG_SIZE_w, 3))    \n    \n    for layer in model1.layers:\n        layer._name = 'model1_' + layer.name\n\n    last_layer1 = model1.get_layer('model1_top_conv')\n    last_output1 = last_layer1.output\n\n    for layer in model2.layers:\n        layer._name = 'model2_' + layer.name\n\n    last_layer2 = model2.get_layer('model2_top_conv')\n    last_output2 = last_layer2.output\n    \n    \n    model1_ = Model(inputs=model1.input, outputs=last_output1)\n    model2_ = Model(inputs=model2.input, outputs=last_output2)\n    \n    model1_.trainable = False\n    model2_.trainable = False\n    \n    model1_.compile('adam', loss=categorical_focal_loss_with_label_smoothing(gamma=2.0, alpha=0.75, ls=0.125, classes=42.0))\n    model2_.compile('adam', loss=categorical_focal_loss_with_label_smoothing(gamma=2.0, alpha=0.75, ls=0.125, classes=42.0))\n    \n    d1 = model1_.output\n    d2 = model2_.output\n\n    bilinear = Lambda(outer_product, name='outer_product1')([d1,d2])\n    \n    predictions=Dense(42, activation='softmax', name='predictions')(bilinear)\n    model = Model(inputs=model1.input, outputs=predictions)\n\n    return model","c0d989c2":"with strategy.scope():\n    \n    model = get_model()\n\n    model.compile(optimizer = 'adam',\n                  loss = categorical_focal_loss_with_label_smoothing(gamma=2.0, alpha=0.75, ls=0.125, classes=42.0), # num classes\n                  metrics=['categorical_accuracy'])","e7525f2f":"import math\n\nLR = 0.0005\nEPOCHS = 16\nWARMUP = 4\n\ndef get_cosine_schedule_with_warmup(lr, num_warmup_steps, num_training_steps, num_cycles=0.5):\n    \"\"\"\n    Modified the get_cosine_schedule_with_warmup from huggingface for tensorflow\n    (https:\/\/huggingface.co\/transformers\/_modules\/transformers\/optimization.html#get_cosine_schedule_with_warmup)\n\n    Create a schedule with a learning rate that decreases following the\n    values of the cosine function between 0 and `pi * cycles` after a warmup\n    period during which it increases linearly between 0 and 1.\n    \"\"\"\n\n    def lrfn(epoch):\n        if epoch < num_warmup_steps:\n            return float(epoch) \/ float(max(1, num_warmup_steps)) * lr\n        progress = float(epoch - num_warmup_steps) \/ float(max(1, num_training_steps - num_warmup_steps))\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr\n\n    return tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n\nlr_schedule= get_cosine_schedule_with_warmup(lr=LR, num_warmup_steps=WARMUP, num_training_steps=EPOCHS)","17991515":"n_steps = train_labels.shape[0] \/\/ BATCH_SIZE  # 89632 \/ 128 = 700\n\nhistory = model.fit(\n    train_dataset, \n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS,\n    callbacks=[lr_schedule]\n)","c6c6a554":"# Get training and test loss histories\ntraining_loss = history.history['loss']\ntest_loss = history.history['val_loss']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, test_loss, 'b-')\nplt.legend(['Training Loss', 'Test Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","3c0982af":"# pred = model.predict(test_dataset, verbose=1)","76015d07":"# # drop existing feature\n# test_df = test_df.drop('category', axis=1)\n\n# # change with prediction\n# test_df['category'] = pred.argmax(axis=1)\n\n# # then add zero-padding\n# test_df['category'] = test_df['category'].apply(lambda x: str(x).zfill(2))","b8cb815b":"# test_df.to_csv('submission.csv', index=False)","aa9c594c":"test_dataset_tta = (\n        tf.data.Dataset\n        .from_tensor_slices(test_paths)\n        .map(decode_image, num_parallel_calls=AUTO)\n        .cache()\n        .map(data_augment, num_parallel_calls=AUTO)\n        .batch(BATCH_SIZE)\n)\n\ntta_times = 5\nprobabilities = []\n\nfor i in range(tta_times+1):\n    print('TTA Number: ', i, '\\n')\n    probabilities.append(model.predict(test_dataset_tta, verbose=1))\n    \ntta_pred = np.mean(probabilities, axis=0)","10a934da":"# change with prediction\ntest_df['category'] = tta_pred.argmax(axis=1)\n\n# then add zero-padding\ntest_df['category'] = test_df['category'].apply(lambda x: str(x).zfill(2))","b3257f3b":"test_df.to_csv('sub_with_tta_transform.csv', index=False)","04be3c84":"# !git clone https:\/\/github.com\/bckenstler\/CLR.git","68401c2f":"# from CLR.clr_callback import CyclicLR\n\n# clr = CyclicLR(base_lr = 0.003,\n#               max_lr = 0.1,\n#               step_size = (4 * (train_labels.shape[0] \/\/ BATCH_SIZE)),\n#               mode = 'triangular')\n\n# model.fit(X_train, Y_train, callbacks=[clr])","fdf4b273":"<font size=\"+2\" color=\"blue\"><b>Notes: This model did not make a great prediction, only 0.82<\/b><\/font><br><a id=\"1\"><\/a>\n\n* Kaggle kernel has a lot of limitation: cannot use ModelCheckpoint, the amount of data that can be trained & training time is limited etc\n* Anyway, this is only baseline for anyone who wants to train a model uses TPU at Kaggle kernel\n* To get a great score, we must use stacking-ensemble method (only for competition, not for real-life)\n\n\n> I'm still new to data science, so don't trust me right away, but do an experiment first. I believe you can do better than me, \u52a0\u6cb9\uff01","f4af6e26":"## Bilinear Efficient-Net B5\n\n![bilinear cnn](https:\/\/miro.medium.com\/max\/820\/1*Y-629BmgDNFpLumnklJyaA.png)\n\nI got inspiration from [this notebook](https:\/\/www.kaggle.com\/jimitshah777\/bilinear-efficientnet-focal-loss-label-smoothing\/data), but make a little change to Efficient-Net B5","a9e1d22a":"## Use focal loss with label smoothing\n\nWhen using deep learning models for classification tasks, we usually encounter the following problems: overfitting, and overconfidence. Overfitting is well studied and can be tackled with early stopping, dropout, weight regularization etc. On the other hand, we have less tools to tackle overconfidence. Label smoothing is a regularization technique that addresses both problems.\n\nLabel smoothing is used when the loss function is cross entropy, and the model applies the softmax function to the penultimate layer\u2019s logit vectors z to compute its output probabilities p.\n\n> Whenever a classification neural network suffers from overfitting and\/or overconfidence, we can try label smoothing.","3b53b45d":"<font size=\"+2\" color=\"purple\"><b>Show images in test dataset<\/b><\/font><br><a id=\"1\"><\/a>","93cbabdd":"## Efficient-Net\n\nIn May 2019, Google published both a very exciting paper and source code for a newly designed CNN called EfficientNet, that set new records for both accuracy and computational efficiency. Here\u2019s the results of EfficientNet, scaled to different block layers (B1, B2, etc) vs. most other popular CNN\u2019s.\n\n![Architecture](https:\/\/miro.medium.com\/max\/985\/1*nQ5HYZ1xiIGn092Y5H5SIQ.jpeg)\n\nAs the image shows, EfficientNet tops the current state of the art both in accuracy and in computational efficiency. How did they do this?\n\n### Model scaling\n\nThey learned that CNN\u2019s must be scaled up in **depth, width, and input image resolution together** to improve the performance of the model. The scaling method is named **compound scaling** and suggests that instead of scaling only one model attribute out of depth, width, and resolution; strategically scaling all three of them together delivers better results.\n\nThere is a synergy in scaling depth, width and image-resolution together, and after an extensive grid search derived the theoretically optimal formula of \u201ccompound scaling\u201d using the following co-efficients:\n\n* Depth = 1.20\n* Width = 1.10\n* Resolution = 1.15\n\nDepth simply means how deep the networks is which is equivalent to the number of layers in it. Width simply means how wide the network is. One measure of width, for example, is the number of channels in a Conv layer whereas Resolution is simply the image resolution that is being passed to a CNN.\n\nIn other words, to scale up the CNN, the depth of layers should increase 20%, the width 10% and the image resolution 15% to keep things as efficient as possible while expanding the implementation and improving the CNN accuracy. This compound scaling formula is used to scale up the EfficientNet from B0-B7\n\n### Swish Activation\n\n![swish-activation](https:\/\/miro.medium.com\/max\/1400\/0*EhAHcCmGOzQUgQ0k)\n\nReLu works pretty well but it got a problem, it nullifies negative values and thus derivatives are zero for all negative values. There are many known alternatives to tackle this problem like leaky ReLu, Elu, Selu etc., but none of them has proven consistent.\n\nGoogle Brain team suggested a newer activation that tends to work better for deeper networks than ReLU which is a Swish activation. They proved that if we replace Swish with ReLu on InceptionResNetV2, we can achieve 0.6% more accuracy on ImageNet dataset.\n\n> Swish(x) = x * sigmoid(x)\n\nThere are other things like MBConv Block etc. If you want to know more details, you can read the articles in reference below","1c95e65d":"# Reference:\n\n[EfficientNet from Google \u2014 Optimally Scaling CNN model architectures with \u201ccompound scaling\u201d](https:\/\/medium.com\/@lessw\/efficientnet-from-google-optimally-scaling-cnn-model-architectures-with-compound-scaling-e094d84d19d4)\n\n[Image Classification with EfficientNet: Better performance with computational efficiency](https:\/\/medium.com\/analytics-vidhya\/image-classification-with-efficientnet-better-performance-with-computational-efficiency-f480fdb00ac6)\n\n[EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https:\/\/medium.com\/@nainaakash012\/efficientnet-rethinking-model-scaling-for-convolutional-neural-networks-92941c5bfb95)\n\n[What is Label Smoothing?](https:\/\/towardsdatascience.com\/what-is-label-smoothing-108debd7ef06)\n\n[Bilinear CNNs for Fine-grained Visual Recognition](https:\/\/arxiv.org\/pdf\/1812.01187.pdf)","c7d15e88":"Original Model","2254f37f":"<font size=\"+2\" color=\"chocolate\"><b>Pick random sample, 2500 image for each categories<\/b><\/font><br><a id=\"1\"><\/a>\n\n* I think this is not the right way, because a lot of noisy images for each category (with different resolution too)\n* I suggest we do **webscraping** for add more training images, and **removing irrelevant ones**\n* For do that, we need a teamwork + time (a lot)","7d5b6019":"## Data Augmentation\n\n* Data augmentation is more of an art than a science. If you do it wrong, you hurt the accuracy.\n* Some may believe that the more types of data augmentation we include, the better the model. This is not true and varies with context.\n\n> You also can use gridmask \/ cutmix \/ mixup etc for experiment","8eb65e61":"## Make a submission with TTA\n\n![tta](https:\/\/preview.ibb.co\/kH61v0\/pipeline.png)\nsource: [Test Time Augmentation (TTA) ... worth it?](https:\/\/www.kaggle.com\/andrewkh\/test-time-augmentation-tta-worth-it)\n\n* TTA is simply to apply different transformations to test image like: rotations, flipping and translations.\n* Then feed these different transformed images to the trained model and **average the results** to get more confident answer."}}