{"cell_type":{"27f111ca":"code","2eb7d79e":"code","3e1284d7":"code","df0e06d0":"code","014ee7e4":"code","968e8524":"code","8cf2d536":"code","d1e48743":"code","ffa88ac0":"code","f5020516":"code","5955d910":"code","dd1de1d9":"code","fcde71ca":"markdown","72216c91":"markdown","57e0fddc":"markdown","ab2f72d8":"markdown","85efb448":"markdown"},"source":{"27f111ca":"!wget --no-check-certificate \\\n    https:\/\/storage.googleapis.com\/laurencemoroney-blog.appspot.com\/rps.zip \\\n    -O \/tmp\/rps.zip\n  \n!wget --no-check-certificate \\\n    https:\/\/storage.googleapis.com\/laurencemoroney-blog.appspot.com\/rps-test-set.zip \\\n    -O \/tmp\/rps-test-set.zip","2eb7d79e":"import os\nimport zipfile\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport tensorflow as tf\nimport numpy as np\nimport random\n\nfrom google.colab import files\nfrom keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\nfrom tensorflow import keras","3e1284d7":"local_zip = '\/tmp\/rps.zip'\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('\/tmp\/')\nzip_ref.close()\n\nlocal_zip = '\/tmp\/rps-test-set.zip'\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('\/tmp\/')\nzip_ref.close()","df0e06d0":"rock_dir = os.path.join('\/tmp\/rps\/rock')\npaper_dir = os.path.join('\/tmp\/rps\/paper')\nscissors_dir = os.path.join('\/tmp\/rps\/scissors')\n\nrock_files = os.listdir(rock_dir)\nprint(rock_files[:10])\n\npaper_files = os.listdir(paper_dir)\nprint(paper_files[:10])\n\nscissors_files = os.listdir(scissors_dir)\nprint(scissors_files[:10])\n\nprint('total training rock images:', len(os.listdir(rock_dir)))\nprint('total training paper images:', len(os.listdir(paper_dir)))\nprint('total training scissors images:', len(os.listdir(scissors_dir)))","014ee7e4":"nrows = 4\nncols = 4\npic_index = 0\n\nfig = plt.gcf()\nfig.set_size_inches(ncols * 4, nrows * 4)\n\npic_index += 4\n\nnext_rock_pix = [os.path.join(rock_dir, fname) \n                for fname in rock_files[pic_index-4:pic_index]]\n\nnext_paper_pix = [os.path.join(paper_dir, fname) \n                for fname in paper_files[pic_index-4:pic_index]]\n\nnext_scissors_pix = [os.path.join(scissors_dir, fname) \n                for fname in scissors_files[pic_index-4:pic_index]]                \n\n\nfor i, img_path in enumerate(next_rock_pix+next_paper_pix+next_scissors_pix):\n  sp = plt.subplot(nrows, ncols, i + 1 )\n  sp.axis('Off') \n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()","968e8524":"TRAINING_DIR = \"\/tmp\/rps\/\"\ntraining_datagen = ImageDataGenerator(\n                                        rescale = 1.\/255,\n\t                                    rotation_range=40,\n                                        width_shift_range=0.2,\n                                        height_shift_range=0.2,\n                                        shear_range=0.2,\n                                        zoom_range=0.2,\n                                        horizontal_flip=True,\n                                        fill_mode='nearest'\n                                        )\n\nVALIDATION_DIR = \"\/tmp\/rps-test-set\/\"\nvalidation_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ntrain_generator = training_datagen.flow_from_directory(\n\t                                                            TRAINING_DIR,\n\t                                                            target_size=(150,150),\n\t                                                            class_mode='categorical',\n                                                                batch_size=126\n                                                        )\n\nvalidation_generator = validation_datagen.flow_from_directory(\n\t                                                            VALIDATION_DIR,\n\t                                                            target_size=(150,150),\n\t                                                            class_mode='categorical',\n                                                                batch_size=126\n                                                            )","8cf2d536":"class myCallback(tf.keras.callbacks.Callback):\n        def on_epoch_end(self, epoch, logs={}):\n            if(logs.get('accuracy')>=.97):\n                print(\"\\nReached 97 % accuracy so cancelling training!\")\n                self.model.stop_training = True\ncallbacks = myCallback()","d1e48743":"model = tf.keras.models.Sequential([\n                                    \n                tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),\n                tf.keras.layers.MaxPooling2D(2, 2),\n\n                tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n                tf.keras.layers.MaxPooling2D(2,2),\n                \n                tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n                tf.keras.layers.MaxPooling2D(2,2),\n\n                tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n                tf.keras.layers.MaxPooling2D(2,2),\n\n                tf.keras.layers.Flatten(),\n                tf.keras.layers.Dropout(0.5),\n                tf.keras.layers.Dense(512, activation='relu'),\n                tf.keras.layers.Dense(3, activation='softmax')\n])\n\n\nmodel.summary()","ffa88ac0":"model.compile(\n                loss='binary_crossentropy',\n                optimizer='adam',\n                metrics=['accuracy'])","f5020516":"history=model.fit(\n                    train_generator,  \n                    validation_data = validation_generator,\n                    epochs=20,\n                    callbacks=[callbacks]\n                  )","5955d910":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1 , len(acc) + 1)\n\nplt.plot(epochs , acc , 'b' , label = 'Training accuracy' )\nplt.plot(epochs , val_acc, 'r' , label = 'Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\n\nplt.figure()\n\nplt.plot(epochs , loss , 'b' , label = 'Training loss' )\nplt.plot(epochs , val_loss , 'r' , label = 'Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\n\nplt.legend()\nplt.rc('font', size = 15)\nplt.rc('figure', figsize=[10,10])\nplt.show()","dd1de1d9":"uploaded = files.upload()\n\nfor fn in uploaded.keys():\n \n  # predicting images\n  path = fn\n  img = image.load_img(path, target_size=(150, 150))\n  x = image.img_to_array(img)\n  x = np.expand_dims(x, axis=0)\n\n  images = np.vstack([x])\n  classes = model.predict(images, batch_size=10)\n  print(fn)\n  print(classes)","fcde71ca":"<a href=\"https:\/\/colab.research.google.com\/github\/mohnabil2020\/machine_learning\/blob\/master\/Rock_Paper_Scissors_classifier.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","72216c91":"**This chart shows that I stopped overfitting from being occuered**","57e0fddc":"**This notebook is for making multi classification between rock & paper & scissors in different shape and colors of hands**\n\n\n---\n\n\n\n---\n***My target is :***\n\n\n1.   Increasing accuracy rate and decreasing loss rate as possible\n\n1.   Prevent overfitting\n2.   Reach accuracy >= 97% and stop the training immediately\n\n**hint**\n\nI used dataset from  [HERE](https:\/\/www.kaggle.com\/eng0mohamed0nabil\/rock-paper-scissors-dataset)\n**To view the notebook better you can view it from my github [HERE](https:\/\/github.com\/mohnabil2020\/machine_learning\/blob\/master\/Rock_Paper_Scissors_classifier.ipynb)**\n","ab2f72d8":"**predictions are sorted alphabetically**\n\nfor example :\n\n[[1. 0. 0.]]==paper\n\n[[0. 1. 0.]]==rock\n\n[[0. 0. 1.]]==scissors","85efb448":"**To make predictions of the model I download validation images from [this link](https:\/\/storage.googleapis.com\/laurencemoroney-blog.appspot.com\/rps-validation.zip)**"}}