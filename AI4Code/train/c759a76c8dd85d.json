{"cell_type":{"959f2511":"code","ab145523":"code","e15cdb0c":"code","1326aea5":"code","ac69c0d7":"code","796f895f":"code","2a58df07":"code","84cd1132":"code","8d219947":"code","0498af66":"code","c4ca66ce":"code","d06504ce":"code","5cdbb7ac":"code","6c06f012":"code","bc1e21fe":"code","3d6ca17f":"code","9d12d94f":"code","63cad380":"code","78fce424":"code","dacb81af":"code","74498b31":"code","53933dcf":"code","422cebac":"code","883ce16f":"code","a16592ae":"code","4cc03ecd":"code","2ea632f9":"code","a11ab6b9":"code","d3126b23":"code","3de3592d":"code","93f8843a":"code","c770d798":"code","7a0471ed":"code","1ceb3db8":"code","6451ab2c":"code","b7e4a29f":"code","3cdd5862":"code","48ebf7dc":"code","3bb140e1":"code","3f2ccefa":"code","cdbb589d":"code","0d975e8a":"code","6643fd9f":"code","372c5180":"code","06b88de4":"code","b86189ae":"code","927beb42":"code","5d6c8ac6":"code","271220fc":"code","49b0361b":"code","b99dfb6b":"code","0de1b99c":"code","f64ac05a":"code","e075820d":"code","fa6b87fc":"code","56fa0f4e":"code","40293e0e":"code","2bbdbfa3":"code","4ebe0836":"code","523b43b5":"code","87bbd61e":"code","371c3c1c":"code","5adc88fa":"code","dbd986b5":"code","7dbbff2f":"code","72884c4c":"code","4364c282":"code","fb6faf6c":"code","e7656fe2":"code","0e1527f3":"code","4febdb54":"code","9542c31f":"code","62298b84":"code","ed8002d4":"code","8c43f805":"code","a9b48ae7":"code","ec5fd264":"code","05a98101":"code","01673ce2":"code","df6a2446":"code","f044a57c":"code","1b1f4f76":"code","e9bb31cc":"code","2f8948c6":"code","4b72e55b":"code","38bc5d63":"code","4b9c94fd":"code","e1d7491e":"code","03344cfa":"code","9c11e238":"code","811d04b9":"code","ff51af19":"code","e3b4fafe":"code","402975ed":"code","93261d9b":"code","c75e703a":"code","2d4f970c":"code","be1110f3":"code","85287fed":"code","e775402a":"code","d24758b8":"code","2048ecfd":"code","02bb054f":"code","377c306e":"code","b0403d9b":"code","0d803c7d":"code","230824f1":"code","a051f1aa":"code","79c76824":"code","70786e8b":"code","90a831bb":"code","cea29d98":"code","a2035b98":"code","1431b8cb":"code","b17e38ce":"code","46d8f7cb":"code","78b45002":"code","45421503":"code","63b943a2":"code","cf729d5d":"code","eeea3a54":"code","8a74f24e":"code","880e8927":"code","c9dd1696":"code","4dfb192f":"code","0518ab17":"code","a5dbb315":"code","23029257":"code","3c5cac96":"code","a9a936bc":"code","0355d27e":"code","844013bc":"code","c33f55d1":"code","e247e3ae":"code","bf6cf1f2":"code","41a1533a":"code","bc2b5cbd":"code","1549c7b6":"code","a4da5a19":"code","f0b3f341":"code","ef8eb37f":"code","182a8584":"code","163aea23":"code","9d4514b8":"code","67044658":"code","570f07c1":"code","3abdbbd8":"code","344676dd":"code","91915c9b":"code","6eb06f5f":"code","9f18f705":"code","2344b9bf":"code","ff95ab2f":"code","b6667625":"code","24a0c6ad":"code","1b39bff5":"code","77bb33b4":"code","f6670345":"code","5ce8cf18":"code","30b72fc0":"code","ca8a7575":"code","b4711009":"code","dd04ba65":"code","8e2955ac":"code","fd11cc00":"code","e9ea706b":"code","d3def12c":"code","32d1be31":"code","3fdbf14c":"code","81464dc8":"code","cb83bd35":"code","61c036c5":"code","39440276":"code","7848aaf1":"code","2279e03f":"code","2cbd6eec":"code","f73eb2b6":"code","5bb29614":"code","02fddf20":"code","d26306e1":"code","4152d53e":"code","ffb3a8f1":"code","f0e3c9f4":"code","5dde1ecd":"code","87cfd2a0":"code","e1f21654":"code","8c1a86f8":"code","af388598":"code","1b341669":"code","a0830aa7":"code","30f2c0c2":"code","1d196317":"code","c56bd9db":"code","07006c35":"code","8f023b0c":"code","c30db1f6":"code","3389c249":"code","7b14875d":"code","3c46a21d":"code","3b570797":"code","7c81e7fa":"code","2b334c0a":"code","c7145b1e":"code","99ec162c":"code","4dfde831":"code","8940c040":"code","4e105e19":"code","0717e510":"code","fd3ba49c":"code","a0621426":"code","7865cf9a":"code","158673b2":"code","aa18a096":"code","a047b89a":"code","a1390d9d":"code","8808e9bd":"code","f415d9d8":"code","a4b5c517":"code","b52d435d":"code","24512738":"code","918edd05":"code","1c4e9c59":"code","bf987193":"code","2bd4fa64":"code","76dcf53e":"code","ad12c3ca":"code","f3440cea":"code","907cbd56":"code","036a1d4f":"code","4da53028":"code","02f1595b":"code","b1e1f0cb":"code","7bd555b3":"code","6eb016e1":"code","435e3a22":"code","b0ceb962":"code","9811eb33":"code","c85fabc9":"code","f333423f":"code","35191bbe":"code","f428bfa5":"code","f382e789":"code","afdcaa41":"code","41ed27f5":"code","37a4c7bc":"code","c3dd6af6":"code","03db0b62":"code","7c70e828":"code","159b16ae":"code","f5caad12":"code","dfc69b0b":"code","3d59e190":"code","0f5bb18b":"code","90c910d1":"code","6c2ea9b0":"code","71aec4e3":"code","49954527":"code","85a57cc3":"code","17293c88":"code","b2e61dde":"code","35d66974":"code","bbcb446e":"code","c7074abf":"code","ec2a37c1":"code","56fc2458":"code","ad46a80c":"code","b940f1d9":"code","d28ce4d3":"code","45ea7786":"code","b0cb1a23":"code","5f310cb7":"code","5eaa0483":"code","7e6d3f3e":"code","26924784":"code","3486dfe9":"code","4506b8a0":"code","d111d61b":"code","f7e79bd0":"code","aa0824fa":"code","19141a07":"code","ab0dfb39":"code","101514e3":"code","14ab5fcb":"code","6e72f776":"code","709aeb57":"code","c19d11ec":"code","770bf99e":"code","7147b0f1":"code","575789e2":"code","54d2550d":"code","2a028f86":"code","35c7dc51":"code","d8abad7f":"code","b164218f":"code","956ce451":"code","68746ccd":"code","1c6374fa":"code","a3c29a10":"code","2fd5bed5":"code","43b73097":"code","1f4037aa":"code","ea3c1cb0":"code","652023ee":"code","f9348a59":"markdown","1c964d72":"markdown","572d9d6b":"markdown","e2d2d067":"markdown","067254bb":"markdown","a866d6c5":"markdown","8158ef13":"markdown","d2847287":"markdown","a6d90f18":"markdown","5f5abeb8":"markdown","feb49e17":"markdown","3d8eb32b":"markdown","1efa305d":"markdown","cde588ca":"markdown","19d8c67c":"markdown","c1541900":"markdown","5e7c47af":"markdown","8a0e5583":"markdown","0f76be14":"markdown","52e80fed":"markdown","ceae0f4d":"markdown","d26e37d1":"markdown","16f1cb2f":"markdown","cbe7f4cf":"markdown","585730c7":"markdown","9a67c730":"markdown","18827153":"markdown","5c8a9ac1":"markdown","dfd1aad4":"markdown","745c8c20":"markdown","cc030d4b":"markdown","6bf42971":"markdown","e609b011":"markdown","fe33ff76":"markdown","7b065db4":"markdown","abca75c5":"markdown","8b308fc8":"markdown","6ce00822":"markdown","6f5c46dd":"markdown","6340830c":"markdown","f7782a8f":"markdown","21351d83":"markdown","b01cd410":"markdown","003b8f23":"markdown","8f22962d":"markdown","43a902f0":"markdown","3cc271e0":"markdown","c15b062e":"markdown","001c1255":"markdown","b92200aa":"markdown","b76a321a":"markdown","0e3d0892":"markdown","e39bdd83":"markdown","f16c1646":"markdown","8138a93f":"markdown","17df9e99":"markdown","487ddf53":"markdown","fddbcd41":"markdown","d506eebd":"markdown","ecdb14a8":"markdown","a2e12c74":"markdown","f62875a5":"markdown","944ba87e":"markdown","fe8274c8":"markdown","683b2b96":"markdown","4f4471d5":"markdown","8a5db8cb":"markdown","f46159de":"markdown","717f51fa":"markdown","fb77e18e":"markdown","3d960b86":"markdown","8b2c65ba":"markdown","b5b21886":"markdown","e94b4a78":"markdown","24b1c6d0":"markdown","214ce55c":"markdown","1bf5b94c":"markdown","d8ab22f0":"markdown","739b51d7":"markdown","8858e775":"markdown","e2c5b5af":"markdown","fba9c5b4":"markdown","d016e674":"markdown","7eae8183":"markdown","b10edbef":"markdown","be8efc7d":"markdown","5e2db19b":"markdown","20148962":"markdown","c1da0b35":"markdown","338f8f12":"markdown","90c3b15c":"markdown","e7bc2f04":"markdown","8ad5bd83":"markdown","a04fd79d":"markdown","a266ae57":"markdown","25d272bb":"markdown","fb5fc743":"markdown","4a178883":"markdown","94dcf90d":"markdown","32800855":"markdown","57cef999":"markdown","d3a38e66":"markdown","2ddde606":"markdown","8c6fba97":"markdown","d8a37e64":"markdown","7469630d":"markdown","6c34de4d":"markdown","b2db4319":"markdown","2a1aff9b":"markdown","aa49c951":"markdown","b0b0481a":"markdown","c360f204":"markdown","2c65b372":"markdown","845cf91c":"markdown","7c438e89":"markdown","ca61a6a5":"markdown","5aa732d8":"markdown","026e8d30":"markdown","e0057023":"markdown","a67edc5f":"markdown","4464649b":"markdown","f1fdd76d":"markdown","57a89713":"markdown","c6968514":"markdown","287eb120":"markdown","107b1bf0":"markdown","76fbd2ca":"markdown","e3c50d66":"markdown","ed522630":"markdown","ebfed4ac":"markdown","33f616b4":"markdown","b107fa5a":"markdown","dfe62237":"markdown","707a7b91":"markdown","ed254e7f":"markdown","a382940d":"markdown","6e68b411":"markdown","a14e47f7":"markdown","485c953e":"markdown","afc37a54":"markdown","ee9ecd84":"markdown","23958ce9":"markdown","39ff6ec0":"markdown","f103389c":"markdown","5d25e29a":"markdown","4984e43d":"markdown","d8292551":"markdown","407ab337":"markdown","8920f39f":"markdown","6d9cf4c2":"markdown","53562957":"markdown","229311f1":"markdown","07ced34b":"markdown","ef98ad32":"markdown","04783a61":"markdown","6ede87bb":"markdown","7be114c0":"markdown","49e757cf":"markdown","a49897bb":"markdown","437f51ef":"markdown","d0ce70d6":"markdown","6572a591":"markdown","5fb40d74":"markdown","38856e86":"markdown","df958a8b":"markdown","c25e44bc":"markdown","e4d2c8b4":"markdown","12fe2fd9":"markdown","7a24e5ca":"markdown","ba8486c8":"markdown","7638ca4c":"markdown","b8cf7365":"markdown","8fbe3204":"markdown","840f9f9b":"markdown","ca8c4189":"markdown","f7244a60":"markdown","973c2d87":"markdown","8a4544cf":"markdown","ef998695":"markdown","80a6eb57":"markdown","806aefb0":"markdown","a36aee5f":"markdown","f2c43f1a":"markdown","c4e7958b":"markdown","ad9e0f05":"markdown","933b3534":"markdown","402a9c97":"markdown","5694f7b0":"markdown","bceca4b9":"markdown","c7d0120e":"markdown","6a316f01":"markdown","5ae1c833":"markdown","a5589c4a":"markdown","c2af6349":"markdown","3754478c":"markdown","eeffee43":"markdown","abb8f34b":"markdown","1b0b5ae3":"markdown","7dbbe9e6":"markdown","6fc563dd":"markdown","d17981d3":"markdown","c82d8334":"markdown","b242bf95":"markdown","1bc81e38":"markdown","d6f7f659":"markdown","8986bd55":"markdown","85ad5779":"markdown","bfe146ac":"markdown","1ba88435":"markdown","947829ab":"markdown"},"source":{"959f2511":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns\nimport os\nimport re\nimport datetime\nfrom calendar import Calendar\nfrom itertools import product\nimport gc","ab145523":"# changes in matplotlib default parameters\ndef mydefault_plt_parameters(figsize=(12, 8), mult_param=1.0):\n    plt.rcParams['figure.figsize'] = figsize\n    plt.rcParams['font.size'] = np.around(18 * mult_param)\n    plt.rcParams['axes.titlepad'] = np.around(20 * mult_param)\n    plt.rcParams['axes.labelpad'] = np.around(15 * mult_param)\n    plt.rcParams['figure.titleweight'] = 'bold'\n    plt.rcParams['axes.titleweight'] = 'bold'\n    plt.rcParams['legend.framealpha'] = 1\n    plt.rcParams['legend.facecolor'] = (0.95,0.95,0.95)\n    plt.rcParams['legend.edgecolor'] = (0.95,0.95,0.95)\n    plt.rcParams['savefig.orientation'] = 'landscape'\n    plt.rcParams['savefig.dpi'] = 300\n    plt.rcParams['savefig.bbox'] = 'tight'","e15cdb0c":"def set_sns_colors(is_mono=False, color_palette=None,\n                   ncolors=None, desat=None):\n    if color_palette is None:\n        if is_mono:\n            sns.set_palette(sns.light_palette(\"navy\"))\n        else:\n            sns.set_palette(\"Set2\")\n    else:\n        sns.set_palette(color_palette, ncolors, desat)\n    return sns.color_palette()","1326aea5":"sns.set_style(\"whitegrid\")\nlist_colors = set_sns_colors()\nmydefault_plt_parameters()","ac69c0d7":"SKIP_NEGATIVE_COUNTS = False\nSKIP_NEGATIVE_SHOPITEM_SENIORITY = True\nKEEP_CATEGORY_DATA = True\nKEEP_MAIN_CATEGORY_DATA = True\nLAGS_ON_RAW_VALUES = True\nLAGS_ON_CHANGE_VALUES = True\nLAGS_ON_RELATIVE_VALUES = True\nLAGS_EXTENDED_PERIODS = True","796f895f":"def downcast_series(df_series):\n    if df_series.dtype == \"float64\":\n        df_series = df_series.astype(np.float32)\n    if df_series.dtype in [\"int64\", \"int32\"]:\n        df_series = df_series.astype(np.int16)\n    return df_series","2a58df07":"folder = \"..\/input\/competitive-data-science-predict-future-sales\/\"\nfilename = folder + \"sales_train.csv\"\nsales = pd.read_csv(filename)\nfilename = folder + \"test.csv\"\ntopredict = pd.read_csv(filename)\n\nfolder = \"..\/input\/future-sales-in-english\/\"\nfilename = folder + \"shops_english.csv\"\nshops = pd.read_csv(filename)\nfilename = folder + \"items_english.csv\"\nitems = pd.read_csv(filename)\nfilename = folder + \"item_categories_english.csv\"\ncategories = pd.read_csv(filename)","84cd1132":"if SKIP_NEGATIVE_COUNTS:\n    sales = sales[sales.item_cnt_day >= 0]","8d219947":"print(\"Number of rows : {:,.0f}\".format(len(items)))\nitems.head()","0498af66":"print(\"number of duplicates : {}\".format(items.duplicated(\n    subset=[\"item_name\", \"item_category_id\"]).sum()))","c4ca66ce":"check = items[items.duplicated(\n    subset=[\"item_name\", \"item_category_id\"], keep=False)].copy()\ncheck.sort_values(by=[\"item_name\", \"item_id\"], inplace=True)\ncheck","d06504ce":"list_id = list(topredict.item_id.unique())\nfor idx in check.index:\n    id = check.loc[idx, \"item_id\"]\n    check.loc[idx, \"to_predict\"] = id in list_id\ncheck","5cdbb7ac":"def change_item_id(idxpredict, indexes):\n    global items\n    global sales\n    to_id = check.loc[idxpredict[0], \"item_id\"]\n    for idx in indexes:\n        if idx not in idxpredict:\n            from_id = check.loc[idx, \"item_id\"]\n            idx_to_change = items[items.item_id == from_id].index\n            items.loc[idx_to_change, \"item_id\"] = to_id\n            idx_to_change = sales[sales.item_id == from_id].index\n            sales.loc[idx_to_change, \"item_id\"] = to_id","6c06f012":"ref_name = \"\"\nlist_idx = []\nlist_topredict = []\nfor idx in check.index:\n    if ref_name != check.loc[idx, \"item_name\"]:\n        if (len(list_topredict) == 0) & (ref_name != \"\"):\n            list_topredict = [list_idx[-1]]\n        if len(list_topredict) == 1:\n            change_item_id(list_topredict, list_idx)\n        ref_name = check.loc[idx, \"item_name\"]\n        list_idx = []\n        list_topredict = []\n    list_idx.append(idx)\n    if check.loc[idx, \"to_predict\"]:\n        list_topredict.append(idx)\n# last round for last pair\nif (len(list_topredict) == 0) & (ref_name != \"\"):\n    list_topredict = list_idx[-1]\nif len(list_topredict) == 1:\n    change_item_id(check, list_topredict, list_idx)","bc1e21fe":"del check\ngc.collect()\nitems = items.drop_duplicates()\nprint(\"Number of rows : {:,.0f}\".format(len(items)))","3d6ca17f":"print(\"Number of rows : {:,.0f}\".format(len(categories)))\ncategories.head()","9d12d94f":"print(\"number of duplicates : {}\".format(\n    categories.duplicated(subset=[\"item_category_name\"]).sum()))","63cad380":"list(categories.item_category_name)","78fce424":"categories.loc[[8, 9, 32, 79, 81, 82, 83], \"item_category_name\"]","dacb81af":"categories.loc[[8, 9, 32, 79, 81, 82, 83], \"item_category_name\"] = \\\n    [\"Service - Tickets (Number)\",\n     \"Service - Product Delivery\",\n     \"Payment cards - Cinema Music Games\",\n     \"Service - Other\",\n     \"Service - Clean media (spike)\",\n     \"Service - Clean media (piece by piece)\",\n     \"Batteries - Batteries\"]","74498b31":"categories[\"main_category\"] = \\\n    [c[0].strip()\n     for c in categories.item_category_name.str.split(\"-\")]","53933dcf":"categories.head()","422cebac":"cat_consoles = list(\n    categories[categories.main_category == \"Game consoles\"].\\\n    item_category_id)\ncat_games = list(\n    categories[categories.main_category == \"Games\"].\\\n    item_category_id)\nlist_consoles = \\\n    items[np.isin(items.item_category_id, cat_consoles)]\\\n    [[\"item_id\", \"item_name\"]]\nlist_games = \\\n    items[np.isin(items.item_category_id, cat_games)]\\\n    [[\"item_id\", \"item_name\"]]","883ce16f":"list_consoles.head(10)","a16592ae":"list_games.head(10)","4cc03ecd":"del list_consoles\ndel list_games\ngc.collect()","2ea632f9":"print(\"Number of rows : {:,.0f}\".format(len(shops)))\nshops.head()","a11ab6b9":"print(\"number of duplicates : {}\".format(shops.duplicated().sum()))","d3126b23":"list(shops.shop_name)","3de3592d":"shops.loc[[9, 12, 55], \"shop_name\"]","93f8843a":"shops.loc[[9, 12, 55], \"shop_name\"] = [\"No-city {}\".format(\n    shops.loc[i,\"shop_name\"]) for i in [9, 12, 55]]\nshops.loc[[9, 12, 55], \"shop_name\"]","c770d798":"shops.loc[42, \"shop_name\"] = re.sub(\n    \"St. Petersburg\", \"Saint-Petersburg\", shops.loc[42, \"shop_name\"])\nshops.loc[43, \"shop_name\"] = re.sub(\n    \"SPb\", \"Saint-Petersburg\", shops.loc[43, \"shop_name\"])\nshops.loc[[42, 43], \"shop_name\"]","7a0471ed":"shops[\"shop_city\"] = [s[0].strip()\n                      for s in shops.shop_name.str.split()]\ninter_other = [re.sub(c, \"\", n).strip()\n               for c, n in zip(shops.shop_city, shops.shop_name)]\nlist_types = [\"shopping center\", \"shopping mall\",\n              \"shopping and entertainment center\", \"sale\",\n              \"shop\", \"ITRC\", \"trade center\", \"itinerant\", \"online\"]\ninter_type = []\ninter_address = []\nfor shop_other in inter_other:\n    type_found = False\n    for t in list_types:\n        if re.search(t + r\"\\b\", shop_other, flags=re.IGNORECASE):\n            inter_type.append(t)\n            inter_address.append(re.sub(\n                t, \"\", shop_other, flags=re.IGNORECASE).strip())\n            type_found = True\n    if not type_found:\n        inter_type.append(\"Other\")\n        inter_address.append(shop_other)\nshops[\"shop_type\"] = inter_type\nshops[\"shop_address\"] = inter_address","1ceb3db8":"shops.sort_values(by=[\"shop_city\", \"shop_type\", \"shop_id\"])\n","6451ab2c":"check = pd.DataFrame(\n    [[23, 24], [39, 40, 41], [0, 57], [1, 58], [10, 11]],\n    columns=[\"shop_1\", \"shop_2\", \"shop_3\"])\nfor idx in check.index:\n    for col in [1, 2, 3]:\n        id = check.loc[idx, \"shop_{}\".format(col)]\n        if not np.isnan(id):\n            check.loc[idx, \"name_{}\".format(col)] = \\\n                shops[shops.shop_id == id].shop_address.values[0]\n        else:\n            check.loc[idx, \"name_{}\".format(col)] = \"\"\ncheck","b7e4a29f":"list_id = list(topredict.shop_id.unique())\nfor idx in check.index:\n    for col in [1, 2, 3]:\n        id = check.loc[idx, \"shop_{}\".format(col)]\n        check.loc[idx, \"to_predict_{}\".format(col)] = id in list_id\ncheck","3cdd5862":"def check_shops_dates(shop1_id, shop2_id):\n    shop1_dates = sales[sales.shop_id == shop1_id].date_block_num\n    shop2_dates = sales[sales.shop_id == shop2_id].date_block_num\n    check_dates = pd.DataFrame(\n        {\"shop1\": [shop1_dates.min(), shop1_dates.max()],\n         \"shop2\": [shop2_dates.min(), shop2_dates.max()]},\n        index=[\"first date\", \"last date\"])\n    return check_dates","48ebf7dc":"check_shops_dates(23, 24)","3bb140e1":"check_shops_dates(39, 40)","3f2ccefa":"check_shops_dates(40, 41)","cdbb589d":"check = sales[np.isin(sales.shop_id, [39, 40, 41])].copy()\ncheck[\"income\"] = check.item_price * check.item_cnt_day\ncheck = check[[\"date_block_num\", \"shop_id\", \"income\"]].\\\n    groupby([\"date_block_num\", \"shop_id\"]).sum()\ncheck.reset_index(inplace=True)","0d975e8a":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Revenues by month of shops 39, 40 and 41\")\nsns.barplot(x=check.date_block_num, y=check.income,\n            hue=check.shop_id, ax=ax)\nax.tick_params(axis='x', rotation=90)\nax.set_xlabel(\"Month\")\nax.set_ylabel(\"Income (rubles)\")\nplt.show()","6643fd9f":"check[check.date_block_num < 14][[\"shop_id\", \"income\"]].\\\n    groupby(\"shop_id\").mean()","372c5180":"check[(check.date_block_num >= 14) & (check.date_block_num <= 24)]\\\n    [[\"shop_id\", \"income\"]].groupby(\"shop_id\").mean()","06b88de4":"check[check.date_block_num > 24][[\"shop_id\", \"income\"]].\\\n    groupby(\"shop_id\").mean()","b86189ae":"check_shops_dates(0, 57)","927beb42":"def remove_shop(id_to_keep, id_to_remove):\n    global shops\n    global sales\n    idx_to_change = sales[sales.shop_id == id_to_remove].index\n    sales.loc[idx_to_change, \"shop_id\"] = id_to_keep\n    shops = shops[shops.shop_id != id_to_remove]","5d6c8ac6":"remove_shop(id_to_keep=57, id_to_remove=0)","271220fc":"check_shops_dates(1, 58)","49b0361b":"remove_shop(id_to_keep=58, id_to_remove=1)","b99dfb6b":"check_shops_dates(10, 11)","0de1b99c":"sales[(sales.shop_id == 10) & (sales.date_block_num == 25)]","f64ac05a":"remove_shop(id_to_keep=10, id_to_remove=11)","e075820d":"del check\ngc.collect()\nprint(\"Number of rows : {:,.0f}\".format(len(shops)))","fa6b87fc":"print(\"Number of rows : {:,.0f}\".format(len(sales)))\nsales.head()","56fa0f4e":"print(\"number of duplicates : {}\".format(\n    sales.duplicated(subset=[\"date\", \"shop_id\", \"item_id\"]).sum()))","40293e0e":"check = sales[sales.duplicated(\n    subset=[\"date\", \"shop_id\", \"item_id\"], keep=False)].copy()\nitems_to_check = check.item_id.unique()\nprint(\"There are {} items with duplicates : {}\".\\\n      format(len(items_to_check), items_to_check))","2bbdbfa3":"folder = \"..\/input\/competitive-data-science-predict-future-sales\/\"\nfilename = folder + \"sales_train.csv\"\ninitial_sale_data = pd.read_csv(filename)\nprint(\"number of duplicates on initial_data : {}\".format(\n      initial_sale_data.duplicated().sum()))","4ebe0836":"idx_to_remove = \\\n    initial_sale_data[initial_sale_data.duplicated()].index\nsales = sales.drop(index=idx_to_remove)","523b43b5":"print(\"number of remaining duplicates : {}\".format(\n    sales.duplicated(subset=[\"date\", \"shop_id\", \"item_id\"]).sum()))","87bbd61e":"check = sales[sales.duplicated(subset=[\"date\", \"shop_id\", \"item_id\"],\n                               keep=False)].copy()\nref = [\"01.01.1999\", 0, 0]\nidx_ref = 0\nsum_price = 0.0\nsum_item_cnt = 0\nfor idx in check.index:\n    if (check.loc[idx, \"date\"] == ref[0])\\\n        & (check.loc[idx, \"shop_id\"] == ref[1])\\\n        & (check.loc[idx, \"item_id\"] == ref[2]):\n        sum_price += check.loc[idx, \"item_cnt_day\"] * \\\n            check.loc[idx, \"item_price\"]\n        sum_item_cnt += check.loc[idx, \"item_cnt_day\"]\n        if sum_item_cnt != 0:\n            sales.loc[idx_ref, \"item_price\"] = \\\n                sum_price \/ sum_item_cnt\n        else:\n            sales.loc[idx_ref, \"item_price\"] = 0\n        sales.loc[idx_ref, \"item_cnt_day\"] = sum_item_cnt\n    else:\n        idx_ref = idx\n        ref = [check.loc[idx, \"date\"],\n               check.loc[idx, \"shop_id\"],\n               check.loc[idx, \"item_id\"]]\n        sum_price = check.loc[idx, \"item_cnt_day\"] * \\\n            check.loc[idx, \"item_price\"]\n        sum_item_cnt = check.loc[idx, \"item_cnt_day\"]","371c3c1c":"sales = sales.drop_duplicates(subset=[\"date\", \"shop_id\", \"item_id\"])","5adc88fa":"del check\ndel initial_sale_data\ngc.collect()\nprint(\"Number of rows : {:,.0f}\".format(len(sales)))","dbd986b5":"data = sales.merge(shops, how=\"left\", on=\"shop_id\")\ndata = data.merge(items, how=\"left\", on=\"item_id\")\ndata = data.merge(categories, how=\"left\", on=\"item_category_id\")\ndata.head()","7dbbff2f":"data.info(null_counts=True)","72884c4c":"data.describe(include=\"all\")","4364c282":"data[\"day_date\"] = pd.to_datetime(data.date, dayfirst=True)\ndata.day_date.describe()","fb6faf6c":"data.drop(columns=[\"date\"], inplace=True)","e7656fe2":"pd.set_option('display.float_format', lambda x: '%.2f' % x)\ndata.date_block_num.describe()","0e1527f3":"check = data[[\"day_date\", \"date_block_num\"]].copy()\ncheck[\"month\"] = check.day_date.dt.month\ncheck[\"year\"] = check.day_date.dt.year\ncheck = check.groupby(\"date_block_num\").nunique()\nprint(\"Minimum of unique values for each date_block_num\")\nprint(check.min())\nprint(\"Maximum of unique values for each date_block_num\")\nprint(check.max())","4febdb54":"data.item_price.describe()","9542c31f":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Checking Item Prices\")\nsns.boxplot(x=data.item_price, ax=ax)\nplt.show()","62298b84":"data[data.item_price >= 100000]","ed8002d4":"data[data.item_id == 6066]","8c43f805":"topredict[topredict.item_id == 6066]","a9b48ae7":"data = data[data.item_price < 100000]","ec5fd264":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Checking Item Prices\")\nsns.boxplot(x=data.item_price, ax=ax)\nplt.show()","05a98101":"data[data.item_price >= 40000]","01673ce2":"data[data.item_id == 11365].item_price.describe()","df6a2446":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Checking Item Prices for item number 11365\")\nsns.boxplot(x=data[data.item_id == 11365].item_price, ax=ax)\nplt.show()","f044a57c":"new_price = data[data.item_id == 11365].item_price.median()\nidx_to_change = data[(data.item_id == 11365) &\n                     (data.item_price > 50000)].index\ndata.loc[idx_to_change, \"item_price\"] = new_price","1b1f4f76":"data[data.item_id == 13199].item_price.describe()","e9bb31cc":"data = data[data.item_id != 13199]","2f8948c6":"data[data.item_id == 7238].item_price.describe()","4b72e55b":"data[data.item_id == 14173].item_price.describe()","38bc5d63":"data[data.item_id == 7241].item_price.describe()","4b9c94fd":"data[np.isin(data.item_id, [7238, 14173, 7241])].item_name.unique()","e1d7491e":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Checking Item Prices for category 'Programs for home and office'\")\nsns.boxplot(x=data[data.item_category_id == 75].item_price, ax=ax)\nplt.show()","03344cfa":"print(\"There are {} sales with a null or negative price\".\\\n      format(len(data[data.item_price <= 0])))","9c11e238":"data[data.item_price < 0]","811d04b9":"data[data.item_price == 0].item_cnt_day.unique()","ff51af19":"data = data[data.item_price > 0]","e3b4fafe":"data.item_price.describe()","402975ed":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Item Prices after corrections\")\nsns.boxplot(x=data.item_price, ax=ax)\nplt.show()","93261d9b":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Item Prices after corrections - zoom\")\nsns.boxplot(x=data.item_price, ax=ax)\nax.set_xlim([-100, 2500])\nplt.show()","c75e703a":"data.item_cnt_day.describe()","2d4f970c":"data[data.item_cnt_day < 0].item_cnt_day.describe()","be1110f3":"data[data.item_cnt_day == -22]","85287fed":"data[(data.item_id == 8023) & (data.shop_id == 12)]","e775402a":"data[data.item_cnt_day == 0]","d24758b8":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Checking Item Count Day\")\nsns.boxplot(x=data.item_cnt_day, ax=ax)\nplt.show()","2048ecfd":"data[data.item_cnt_day >= 500].sort_values(\n    by=\"item_cnt_day\", ascending=False)","02bb054f":"data[\"day_of_week\"] = data.day_date.dt.dayofweek\ndata[\"month\"] = data.day_date.dt.month\ndata[\"year\"] = data.day_date.dt.year\ndata[\"item_revenue\"] = data.item_price * data.item_cnt_day","377c306e":"data_month = data[[\"date_block_num\", \"month\", \"year\", \"shop_id\",\n                   \"item_id\", \"item_price\"]].\\\n    groupby([\"date_block_num\", \"shop_id\", \"item_id\"]).mean()\ndata_month.reset_index(inplace=True)\ncheck = data[[\"date_block_num\", \"shop_id\", \"item_id\",\n              \"item_cnt_day\", \"item_revenue\"]].\\\n    groupby([\"date_block_num\", \"shop_id\", \"item_id\"]).sum()\ncheck.columns = [\"item_cnt_month\", \"item_revenue\"]\ncheck.reset_index(inplace=True)\ndata_month = data_month.merge(\n    check, how=\"left\", on=[\"date_block_num\", \"shop_id\", \"item_id\"])","b0403d9b":"data_month = data_month[data_month.item_cnt_month > 0]\ndata_month[\"item_mean_price_month\"] = \\\n    data_month.item_revenue \/ data_month.item_cnt_month\ndata_month = data_month[data_month.item_mean_price_month > 0]","0d803c7d":"data_month = data_month.merge(shops, how=\"left\", on=\"shop_id\")\ndata_month = data_month.merge(items, how=\"left\", on=\"item_id\")\ndata_month = data_month.merge(categories, how=\"left\",\n                              on=\"item_category_id\")","230824f1":"for col in [\"month\", \"year\", \"item_cnt_month\"]:\n    data_month[col] = data_month[col].astype(np.int32)","a051f1aa":"data_month.head()","79c76824":"del check\ngc.collect()","70786e8b":"graph_dates = data_month[[\"date_block_num\", \"month\", \"year\"]].\\\n    groupby(\"date_block_num\").mean()\ngraph_dates.reset_index(inplace=True)\ngraph_dates[\"month_year\"] = [\"{}-{}\".format(m, y) \n    for m, y in zip(graph_dates.month, graph_dates.year)]","90a831bb":"check = \\\n    data_month[[\"date_block_num\", \"item_cnt_month\", \"item_revenue\"]].\\\n    groupby(\"date_block_num\").sum()\ncheck = check.merge(\n    graph_dates[[\"date_block_num\", \"month_year\"]], how=\"left\",\n    left_index=True, right_on=\"date_block_num\")","cea29d98":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Total number of sales of 1C Company\")\nsns.barplot(x=check.month_year, y=check.item_cnt_month, ax=ax)\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xlabel(\"Month\")\nax.set_ylabel(\"Number of sales\")\nplt.show()","a2035b98":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Total income of 1C Company\")\nsns.barplot(x=check.month_year, y=check.item_revenue, ax=ax)\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xlabel(\"Month\")\nax.set_ylabel(\"Income (rubles)\")\nplt.show()","1431b8cb":"check = data[[\"day_of_week\", \"item_cnt_day\", \"item_revenue\"]].\\\n    groupby([\"day_of_week\"]).sum()\nweek_days = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\",\n            \"Friday\", \"Saturday\", \"Sunday\"]","b17e38ce":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Total number of sales of 1C Company\")\nsns.barplot(x=check.index, y=check.item_cnt_day, ax=ax)\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xticklabels(week_days)\nax.set_xlabel(\"Day of week\")\nax.set_ylabel(\"Number of sales\")\nplt.show()","46d8f7cb":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Total income of 1C Company\")\nsns.barplot(x=check.index, y=check.item_revenue, ax=ax)\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xticklabels(week_days)\nax.set_xlabel(\"Day of week\")\nax.set_ylabel(\"Income (rubles)\")\nplt.show()","78b45002":"check = data_month[[\"shop_name\", \"item_cnt_month\", \"item_revenue\"]].\\\n    groupby(\"shop_name\").sum()\ncheck.reset_index(inplace=True)","45421503":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Total number of sales by shop\")\nsns.barplot(x=check.shop_name, y=check.item_cnt_month, ax=ax)\nax.tick_params(axis=\"x\", rotation=90, labelsize=12)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Number of sales\")\nplt.show()","63b943a2":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Total income by shop\")\nsns.barplot(x=check.shop_name, y=check.item_revenue, ax=ax)\nax.tick_params(axis=\"x\", rotation=90, labelsize=12)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Income (rubles)\")\nplt.show()","cf729d5d":"check = data_month[[\"shop_type\", \"item_cnt_month\", \"item_revenue\"]].\\\n    groupby(\"shop_type\").sum()\ncheck.reset_index(inplace=True)","eeea3a54":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Total number of sales by shop type\")\nsns.barplot(x=check.shop_type, y=check.item_cnt_month, ax=ax)\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Number of sales\")\nplt.show()","8a74f24e":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Total income by shop type\")\nsns.barplot(x=check.shop_type, y=check.item_revenue, ax=ax)\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Income (rubles)\")\nplt.show()","880e8927":"check = data_month[[\"shop_city\", \"item_cnt_month\", \"item_revenue\"]].\\\n    groupby(\"shop_city\").sum()\ncheck.reset_index(inplace=True)","c9dd1696":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Total number of sales by shop city\")\nsns.barplot(x=check.shop_city, y=check.item_cnt_month, ax=ax)\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Number of sales\")\nplt.show()","4dfb192f":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Total number of sales by shop city (log-scale\")\ng = sns.barplot(x=check.shop_city, y=check.item_cnt_month, ax=ax)\ng.set_yscale(\"log\")\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Number of sales (log-scale)\")\nplt.show()","0518ab17":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Total income by shop city\")\nsns.barplot(x=check.shop_city, y=check.item_revenue, ax=ax)\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Income (rubles)\")\nplt.show()","a5dbb315":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Total income by shop city (log scale)\")\ng = sns.barplot(x=check.shop_city, y=check.item_revenue, ax=ax)\ng.set_yscale(\"log\")\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Income (rubles) - log scale\")\nplt.show()","23029257":"check = data_month[[\"shop_name\", \"shop_city\", \"shop_type\",\n                    \"item_cnt_month\", \"item_mean_price_month\"]].copy()","3c5cac96":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Distribution of Item Count Month by shop\")\nsns.boxplot(x=\"shop_name\", y=\"item_cnt_month\", data=check, ax=ax)\nax.tick_params(axis=\"x\", rotation=90, labelsize=12)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Monthly quantity\")\nplt.show()","a9a936bc":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Distribution of average monthly price by shop\")\nsns.boxplot(x=\"shop_name\", y=\"item_mean_price_month\",\n            data=check, ax=ax)\nax.tick_params(axis=\"x\", rotation=90, labelsize=12)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Average monthly price (rubles)\")\nplt.show()","0355d27e":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Distribution of Item Count Month by shop city\")\nsns.boxplot(x=\"shop_city\", y=\"item_cnt_month\", data=check, ax=ax)\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Monthly quantity\")\nplt.show()","844013bc":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Distribution of average monthly price by shop city\")\nsns.boxplot(x=\"shop_city\", y=\"item_mean_price_month\",\n            data=check, ax=ax)\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Average monthly price (rubles)\")\nplt.show()","c33f55d1":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Distribution of Item Count Month by shop type\")\nsns.boxplot(x=\"shop_type\", y=\"item_cnt_month\", data=check, ax=ax)\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Monthly quantity\")\nplt.show()","e247e3ae":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Distribution of average monthly price by shop type\")\nsns.boxplot(x=\"shop_type\", y=\"item_mean_price_month\",\n            data=check, ax=ax)\nax.tick_params(axis=\"x\", rotation=90, labelsize=12)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Average monthly price (rubles)\")\nplt.show()","bf6cf1f2":"check = data_month[[\"date_block_num\", \"shop_name\", \"item_cnt_month\",\n                    \"item_revenue\"]].groupby([\"date_block_num\",\n                                              \"shop_name\"]).sum()\ncheck.reset_index(inplace=True)","41a1533a":"data_graph = check.pivot_table(\n    values=\"item_cnt_month\", columns=\"date_block_num\",\n    index=\"shop_name\", fill_value=0)\nfig, (ax, cbar_ax) = plt.subplots(\n    2, gridspec_kw={\"height_ratios\": (.9, .05), \"hspace\": .3},\n    figsize=(12, 18))\nfig.suptitle(\"Evolution of sales by shop over time\")\nsns.heatmap(data_graph, cmap=\"RdYlGn\", ax=ax, cbar_ax=cbar_ax,\n            cbar_kws={\"orientation\": \"horizontal\",\n                      \"label\": \"Number of sales\"})\nax.tick_params(axis=\"x\", rotation=90)\nax.tick_params(axis=\"y\", labelsize=10)\nax.set_xticklabels(\n    graph_dates.sort_values(by=\"date_block_num\").month_year)\nax.yaxis.set_ticks(np.arange(0.5, len(data_graph.index), 1))\nax.set_yticklabels(data_graph.index)\nax.set_xlabel(\"\")\nax.set_ylabel(\"\")\nplt.show()","bc2b5cbd":"data_graph = check.pivot_table(\n    values=\"item_revenue\", columns=\"date_block_num\",\n    index=\"shop_name\", fill_value=0)\nfig, (ax, cbar_ax) = plt.subplots(\n    2, gridspec_kw={\"height_ratios\": (.9, .05), \"hspace\": .3},\n    figsize=(12, 18))\nfig.suptitle(\"Evolution of income by shop over time\")\nsns.heatmap(data_graph, cmap=\"RdYlGn\", ax=ax, cbar_ax=cbar_ax,\n            cbar_kws={\"orientation\": \"horizontal\",\n                      \"label\": \"Income (rubles)\"})\nax.tick_params(axis=\"x\", rotation=90)\nax.tick_params(axis=\"y\", labelsize=10)\nax.set_xticklabels(\n    graph_dates.sort_values(by=\"date_block_num\").month_year)\nax.yaxis.set_ticks(np.arange(0.5, len(data_graph.index), 1))\nax.set_yticklabels(data_graph.index)\nax.set_xlabel(\"\")\nax.set_ylabel(\"\")\nplt.show()","1549c7b6":"check = data_month[[\"date_block_num\", \"shop_type\", \"item_cnt_month\",\n                    \"item_revenue\"]].groupby([\"date_block_num\",\n                                              \"shop_type\"]).sum()\ncheck.reset_index(inplace=True)","a4da5a19":"data_graph = check.pivot_table(\n    values=\"item_cnt_month\", columns=\"date_block_num\",\n    index=\"shop_type\", fill_value=0)\nfig, (ax, cbar_ax) = plt.subplots(\n    2, gridspec_kw={\"height_ratios\": (.9, .05), \"hspace\": .3},\n    figsize=(12, 8))\nfig.suptitle(\"Evolution of sales by shop type over time\")\nsns.heatmap(data_graph, cmap=\"RdYlGn\", ax=ax, cbar_ax=cbar_ax,\n            cbar_kws={\"orientation\": \"horizontal\",\n                      \"label\": \"Number of sales\"})\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xticklabels(\n    graph_dates.sort_values(by=\"date_block_num\").month_year)\nax.set_xlabel(\"\")\nax.set_ylabel(\"\")\nplt.show()","f0b3f341":"data_graph = check.pivot_table(\n    values=\"item_revenue\", columns=\"date_block_num\",\n    index=\"shop_type\", fill_value=0)\nfig, (ax, cbar_ax) = plt.subplots(\n    2, gridspec_kw={\"height_ratios\": (.9, .05), \"hspace\": .3},\n    figsize=(12, 8))\nfig.suptitle(\"Evolution of income by shop type over time\")\nsns.heatmap(data_graph, cmap=\"RdYlGn\", ax=ax, cbar_ax=cbar_ax,\n            cbar_kws={\"orientation\": \"horizontal\",\n                      \"label\": \"Income (rubles)\"})\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xticklabels(\n    graph_dates.sort_values(by=\"date_block_num\").month_year)\nax.set_xlabel(\"\")\nax.set_ylabel(\"\")\nplt.show()","ef8eb37f":"check = data_month[[\"date_block_num\", \"shop_city\", \"item_cnt_month\",\n                    \"item_revenue\"]].groupby([\"date_block_num\",\n                                              \"shop_city\"]).sum()\ncheck.reset_index(inplace=True)","182a8584":"data_graph = check.pivot_table(\n    values=\"item_cnt_month\", columns=\"date_block_num\",\n    index=\"shop_city\", fill_value=0)\nfig, (ax, cbar_ax) = plt.subplots(\n    2, gridspec_kw={\"height_ratios\": (.9, .05), \"hspace\": .3},\n    figsize=(12, 10))\nfig.suptitle(\"Evolution of sales by city over time\")\nsns.heatmap(data_graph, cmap=\"RdYlGn\", ax=ax, cbar_ax=cbar_ax,\n            cbar_kws={\"orientation\": \"horizontal\",\n                      \"label\": \"Number of sales\"})\nax.tick_params(axis=\"x\", rotation=90)\nax.tick_params(axis=\"y\", labelsize=12)\nax.set_xticklabels(\n    graph_dates.sort_values(by=\"date_block_num\").month_year)\nax.yaxis.set_ticks(np.arange(0.5, len(data_graph.index), 1))\nax.set_yticklabels(data_graph.index)\nax.set_xlabel(\"\")\nax.set_ylabel(\"\")\nplt.show()","163aea23":"data_graph = check.pivot_table(\n    values=\"item_revenue\", columns=\"date_block_num\",\n    index=\"shop_city\", fill_value=0)\nfig, (ax, cbar_ax) = plt.subplots(\n    2, gridspec_kw={\"height_ratios\": (.9, .05), \"hspace\": .3},\n    figsize=(12, 8))\nfig.suptitle(\"Evolution of income by city over time\")\nsns.heatmap(data_graph, cmap=\"RdYlGn\", ax=ax, cbar_ax=cbar_ax,\n            cbar_kws={\"orientation\": \"horizontal\",\n                      \"label\": \"Income (rubles)\"})\nax.tick_params(axis=\"x\", rotation=90)\nax.tick_params(axis=\"y\", labelsize=12)\nax.set_xticklabels(\n    graph_dates.sort_values(by=\"date_block_num\").month_year)\nax.yaxis.set_ticks(np.arange(0.5, len(data_graph.index), 1))\nax.set_yticklabels(data_graph.index)\nax.set_xlabel(\"\")\nax.set_ylabel(\"\")\nplt.show()","9d4514b8":"check = \\\n    data_month[[\"item_category_name\", \"item_cnt_month\",\n                \"item_revenue\"]].groupby(\"item_category_name\").sum()\ncheck.reset_index(inplace=True)","67044658":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Total number of sales by category\")\nsns.barplot(\n    x=check.item_category_name, y=check.item_cnt_month, ax=ax)\nax.tick_params(axis=\"x\", rotation=90, labelsize=10)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Number of sales\")\nplt.show()","570f07c1":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Total income by category\")\nsns.barplot(\n    x=check.item_category_name, y=check.item_revenue, ax=ax)\nax.tick_params(axis=\"x\", rotation=90, labelsize=10)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Income (rubles)\")\nplt.show()","3abdbbd8":"check = data_month[[\"main_category\", \"item_cnt_month\",\n                    \"item_revenue\"]].groupby(\"main_category\").sum()\ncheck.reset_index(inplace=True)","344676dd":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Total number of sales by main category\")\nsns.barplot(x=check.main_category, y=check.item_cnt_month, ax=ax)\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Number of sales\")\nplt.show()","91915c9b":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Total income by main category\")\nsns.barplot(x=check.main_category, y=check.item_revenue, ax=ax)\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Income (rubles)\")\nplt.show()","6eb06f5f":"check = data_month[[\"date_block_num\", \"main_category\",\n                    \"item_cnt_month\", \"item_revenue\"]].groupby(\n    [\"date_block_num\", \"main_category\"]).sum()\ncheck.reset_index(inplace=True)","9f18f705":"data_graph = check.pivot_table(\n    values=\"item_cnt_month\", columns=\"date_block_num\",\n    index=\"main_category\", fill_value=0)\nfig, (ax, cbar_ax) = plt.subplots(\n    2, gridspec_kw={\"height_ratios\": (.9, .05), \"hspace\": .3},\n    figsize=(12, 8))\nfig.suptitle(\"Evolution of sales by main category over time\")\nsns.heatmap(data_graph, cmap=\"RdYlGn\", ax=ax, cbar_ax=cbar_ax,\n            cbar_kws={\"orientation\": \"horizontal\",\n                      \"label\": \"Number of sales\"})\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xticklabels(\n    graph_dates.sort_values(by=\"date_block_num\").month_year)\nax.set_xlabel(\"\")\nax.set_ylabel(\"\")\nplt.show()","2344b9bf":"data_graph = check.pivot_table(\n    values=\"item_revenue\", columns=\"date_block_num\",\n    index=\"main_category\", fill_value=0)\nfig, (ax, cbar_ax) = plt.subplots(\n    2, gridspec_kw={\"height_ratios\": (.9, .05), \"hspace\": .3},\n    figsize=(12, 8))\nfig.suptitle(\"Evolution of income by main category over time\")\nsns.heatmap(data_graph, cmap=\"RdYlGn\", ax=ax, cbar_ax=cbar_ax,\n            cbar_kws={\"orientation\": \"horizontal\",\n                      \"label\": \"Income (rubles)\"})\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xticklabels(\n    graph_dates.sort_values(by=\"date_block_num\").month_year)\nax.set_xlabel(\"\")\nax.set_ylabel(\"\")\nplt.show()","ff95ab2f":"check = data_month[[\"item_category_name\", \"main_category\",\n                    \"item_cnt_month\", \"item_mean_price_month\"]].copy()","b6667625":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Distribution of Item Count Month by category\")\nsns.boxplot(x=\"item_category_name\", y=\"item_cnt_month\",\n            data=check, ax=ax)\nax.tick_params(axis=\"x\", rotation=90, labelsize=10)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Monthly quantity\")\nplt.show()","24a0c6ad":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Distribution of Item Count Month by category\")\nsns.boxplot(x=\"item_category_name\", y=\"item_cnt_month\",\n            data=check, ax=ax)\nax.tick_params(axis=\"x\", rotation=90, labelsize=10)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Monthly quantity\")\nax.set_ylim([0, 250])\nplt.show()","1b39bff5":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Distribution of average monthly price by category\")\nsns.boxplot(x=\"item_category_name\", y=\"item_mean_price_month\",\n            data=check, ax=ax)\nax.tick_params(axis=\"x\", rotation=90, labelsize=10)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Average monthly price (rubles)\")\nplt.show()","77bb33b4":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Distribution of Item Count Month by main category\")\nsns.boxplot(x=\"main_category\", y=\"item_cnt_month\",\n            data=check, ax=ax)\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Monthly quantity\")\nplt.show()","f6670345":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Distribution of Item Count Month by main category\")\nsns.boxplot(x=\"main_category\", y=\"item_cnt_month\",\n            data=check, ax=ax)\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Monthly quantity\")\nax.set_ylim([0, 50])\nplt.show()","5ce8cf18":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Distribution of average monthly price by main category\")\nsns.boxplot(x=\"main_category\", y=\"item_mean_price_month\",\n            data=check, ax=ax)\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Average monthly price (rubles)\")\nplt.show()","30b72fc0":"check = data_month[[\"item_id\", \"item_name\", \"item_cnt_month\",\n                    \"item_revenue\"]].groupby(\n    [\"item_id\", \"item_name\"]).sum()\ncheck.reset_index(inplace=True)","ca8a7575":"check.sort_values(by=\"item_cnt_month\", inplace=True)\nlistdata = check.item_cnt_month.values\nlorenz = np.cumsum(np.sort(listdata)) \/ listdata.sum()\nlorenz = np.append([0], lorenz)\naire_ss_courbe = lorenz[:-1].sum() \/ len(lorenz)\nS = 0.5 - aire_ss_courbe\ngini = 2 * S","b4711009":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nax.plot(np.linspace(0, 1, len(lorenz)), label=\"First bissectrix\")\nax.plot(lorenz, label=\"Lorenz curve\")\nax.set_ylim([-0.025, 1.025])\nax.set_xlim([-100, len(lorenz) + 100])\nax.axhline(y=0.2, xmax=0.8*(len(lorenz)+100)\/(len(lorenz)+200),\n           color=list_colors[2],\n           label=\"80% of sales by 20% of items\")\nax.axvline(x=0.8*len(lorenz), ymax=(0.2+0.025)\/1.05,\n           color=list_colors[2])\nax.set_ylabel(\"Cumulative frequency of sales (%)\")\nax.set_xlabel(\"Items\")\nax.legend(loc=\"upper left\")\nplt.text(1500, 0.85, \" Gini = {:.2f} \".format(gini),\\\n         horizontalalignment=\"center\", verticalalignment=\"top\",\n         color=\"black\", backgroundcolor=(0.95, 0.95, 0.95))\nplt.title(\"Concentration of sales by items\")\nplt.show()","dd04ba65":"idx = np.searchsorted(lorenz, 0.8)\nidx -= 1\nprint(\"Lorenz value for last {} items = {:.2%}\".format(\n    len(lorenz) - idx, lorenz[idx]))","8e2955ac":"items_to_graph = check.iloc[idx:].item_id.values","fd11cc00":"check.sort_values(by=\"item_revenue\", inplace=True)\nlistdata = check.item_revenue.values\nlorenz = np.cumsum(np.sort(listdata)) \/ listdata.sum()\nlorenz = np.append([0], lorenz)\naire_ss_courbe = lorenz[:-1].sum() \/ len(lorenz)\nS = 0.5 - aire_ss_courbe\ngini = 2 * S","e9ea706b":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nax.plot(np.linspace(0, 1, len(lorenz)), label=\"First bissectrix\")\nax.plot(lorenz, label=\"Lorenz curve\")\nax.set_ylim([-0.025, 1.025])\nax.set_xlim([-100, len(lorenz)+100])\nax.axhline(y=0.2, xmax=0.8*(len(lorenz)+100)\/(len(lorenz)+200),\n           color=list_colors[2],\n           label=\"80% of income by 20% of items\")\nax.axvline(x=0.8*len(lorenz),\n           ymax=(0.2+0.025)\/1.05, color=list_colors[2])\nax.set_ylabel(\"Cumulative frequency of income (%)\")\nax.set_xlabel(\"Items\")\nax.legend(loc=\"upper left\")\nplt.text(1500, 0.85, \" Gini = {:.2f} \".format(gini),\\\n         horizontalalignment=\"center\", verticalalignment=\"top\",\n         color=\"black\", backgroundcolor=(0.95, 0.95, 0.95))\nplt.title(\"Concentration of income by items\")\nplt.show()","d3def12c":"check = data_month[[\"shop_id\", \"shop_name\", \"item_cnt_month\",\n                    \"item_revenue\"]].groupby(\n    [\"shop_id\", \"shop_name\"]).sum()\ncheck.reset_index(inplace=True)","32d1be31":"check.sort_values(by=\"item_cnt_month\", inplace=True)\nlistdata = check.item_cnt_month.values\nlorenz = np.cumsum(np.sort(listdata)) \/ listdata.sum()\nlorenz = np.append([0], lorenz)\naire_ss_courbe = lorenz[:-1].sum() \/ len(lorenz)\nS = 0.5 - aire_ss_courbe\ngini = 2 * S","3fdbf14c":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nax.plot(np.linspace(0, 1, len(lorenz)), label=\"First bissectrix\")\nax.plot(lorenz, label=\"Lorenz curve\")\nax.set_ylim([-0.025, 1.025])\nax.set_xlim([-1, len(lorenz)+1])\nax.axhline(y=0.4, xmax=0.666*(len(lorenz)+1)\/(len(lorenz)+2),\n           color=list_colors[2],\n           label=\"60% of sales by 33% of shops\")\nax.axvline(x=0.666*len(lorenz),\n           ymax=(0.4+0.025)\/1.05, color=list_colors[2])\nax.set_ylabel(\"Cumulative frequency of sales (%)\")\nax.set_xlabel(\"Shops\")\nax.legend(loc=\"upper left\")\nplt.text(5, 0.85,\" Gini = {:.2f} \".format(gini),\\\n         horizontalalignment=\"center\", verticalalignment=\"top\",\n         color=\"black\", backgroundcolor=(0.95, 0.95, 0.95))\nplt.title(\"Concentration of sales by shops\")\nplt.show()","81464dc8":"check.sort_values(by=\"item_revenue\", inplace=True)\nlistdata = check.item_revenue.values\nlorenz = np.cumsum(np.sort(listdata)) \/ listdata.sum()\nlorenz = np.append([0], lorenz)\naire_ss_courbe = lorenz[:-1].sum() \/ len(lorenz)\nS = 0.5 - aire_ss_courbe\ngini = 2 * S","cb83bd35":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nax.plot(np.linspace(0, 1, len(lorenz)), label=\"First bissectrix\")\nax.plot(lorenz, label=\"Lorenz curve\")\nax.set_ylim([-0.025, 1.025])\nax.set_xlim([-1, len(lorenz)+1])\nax.axhline(y=0.4, xmax=0.666*(len(lorenz)+1)\/(len(lorenz)+2),\n           color=list_colors[2],\n           label=\"60% of income by 33% of shops\")\nax.axvline(x=0.666*len(lorenz), ymax=(0.4+0.025)\/1.05,\n           color=list_colors[2])\nax.set_ylabel(\"Cumulative frequency of income (%)\")\nax.set_xlabel(\"Shops\")\nax.legend(loc=\"upper left\")\nplt.text(5, 0.85,\" Gini = {:.2f} \".format(gini),\\\n         horizontalalignment=\"center\", verticalalignment=\"top\",\n         color=\"black\", backgroundcolor=(0.95, 0.95, 0.95))\nplt.title(\"Concentration of income by items\")\nplt.show()","61c036c5":"check = data_month[np.isin(data_month.item_id, items_to_graph)]\\\n    [[\"item_id\", \"item_name\", \"item_cnt_month\", \"item_revenue\"]].\\\n    groupby([\"item_id\", \"item_name\"]).sum()\ncheck.reset_index(inplace=True)","39440276":"fig = plt.figure(figsize=(16, 8))\nax = plt.axes()\nfig.suptitle(\"Total number of sales for the 98 most sold items\")\nsns.barplot(x=check.item_name, y=check.item_cnt_month, ax=ax)\nax.tick_params(axis='x', rotation=90, labelsize=10)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Number of sales\")\nplt.show()","7848aaf1":"check[check.item_cnt_month > 25000]","2279e03f":"data_month[data_month.item_id == 20949]\\\n    [[\"date_block_num\", \"shop_id\", \"item_price\"]].nunique()","2cbd6eec":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Checking prices of Item n\u00b020949\")\nsns.boxplot(\n    x=data_month[data_month.item_id==20949].item_price.unique(),\n    ax=ax)\nplt.show()","f73eb2b6":"fig = plt.figure(figsize=(16, 8))\nax = plt.axes()\nfig.suptitle(\"Total income the 98 most sold items\")\nsns.barplot(x=check.item_name, y=check.item_revenue, ax=ax)\nax.tick_params(axis='x', rotation=90, labelsize=10)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Income (rubles)\")\nplt.show()","5bb29614":"check = data_month[np.isin(data_month.item_id, items_to_graph)]\\\n    [[\"date_block_num\", \"item_name\", \"item_cnt_month\",\n      \"item_revenue\"]].groupby(\n    [\"date_block_num\", \"item_name\"]).sum()\ncheck.reset_index(inplace=True)","02fddf20":"data_graph = check.pivot_table(\n    values=\"item_cnt_month\", columns=\"date_block_num\",\n    index=\"item_name\", fill_value=0)\nfig, (ax, cbar_ax) = plt.subplots(\n    2, gridspec_kw={\"height_ratios\": (.9, .05), \"hspace\": .3},\n    figsize=(12, 30))\nfig.suptitle(\"Evolution of sales over time for the 98 most \" +\n             \"sold items\")\nsns.heatmap(data_graph, cmap=\"RdYlGn\", ax=ax, cbar_ax=cbar_ax,\n            cbar_kws={\"orientation\": \"horizontal\",\n                      \"label\": \"Number of sales\"})\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xticklabels(\n    graph_dates.sort_values(by=\"date_block_num\").month_year)\nax.set_xlabel(\"\")\nax.set_ylabel(\"\")\nplt.show()","d26306e1":"item_name = items[items.item_id == 20949].item_name.values[0]\ncheck = check[check.item_name != item_name]","4152d53e":"data_graph = check.pivot_table(\n    values=\"item_cnt_month\", columns=\"date_block_num\",\n    index=\"item_name\", fill_value=0)\nfig, (ax, cbar_ax) = plt.subplots(\n    2, gridspec_kw={\"height_ratios\": (.9, .05), \"hspace\": .3},\n    figsize=(12, 30))\nfig.suptitle(\"Evolution of sales over time for 97 of \" +\n             \"the 98 most sold items\")\nsns.heatmap(data_graph, cmap=\"RdYlGn\", ax=ax, cbar_ax=cbar_ax,\n            cbar_kws={\"orientation\": \"horizontal\",\n                      \"label\": \"Number of sales\"})\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xticklabels(\n    graph_dates.sort_values(by=\"date_block_num\").month_year)\nax.set_xlabel(\"\")\nax.set_ylabel(\"\")\nplt.show()","ffb3a8f1":"check = data_month[np.isin(data_month.item_id, items_to_graph)]\\\n    [[\"item_name\", \"item_cnt_month\", \"item_mean_price_month\"]].copy()","f0e3c9f4":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Distribution of Item Count Month by item\")\nsns.boxplot(x=\"item_name\", y=\"item_cnt_month\",\n            data=check, ax=ax)\nax.tick_params(axis=\"x\", rotation=90, labelsize=10)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Monthly quantity\")\nplt.show()","5dde1ecd":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Distribution of Item Count Month by item\")\nsns.boxplot(x=\"item_name\", y=\"item_cnt_month\",\n            data=check, ax=ax)\nax.tick_params(axis=\"x\", rotation=90, labelsize=10)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Monthly quantity\")\nax.set_ylim([0, 250])\nplt.show()","87cfd2a0":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Distribution of average monthly price by item\")\nsns.boxplot(x=\"item_name\", y=\"item_mean_price_month\",\n            data=check, ax=ax)\nax.tick_params(axis=\"x\", rotation=90, labelsize=10)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Average monthly price (rubles)\")\nplt.show()","e1f21654":"items_id = topredict.item_id.unique()\nprint(\"We have to predict sales for {} items\".format(len(items_id)) +\n      \" (on a total number of {} items)\".format(len(items)))","8c1a86f8":"print(\"There are {} new items in the prediction list\".format(\n    len(items_id) - \n    np.isin(items_id, data_month.item_id.unique()).sum()))","af388598":"topredict_check = \\\n    data_month[np.isin(data_month.item_id, items_id)].copy()","1b341669":"check = topredict_check.groupby(\"item_id\")[[\"date_block_num\"]].max()\ncheck.reset_index(inplace=True)\ncheck[\"months_since_last_sale\"] = 33 - check.date_block_num\ncheck = check.groupby(\"months_since_last_sale\").count()","a0830aa7":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Inactive items among items to predict\")\nsns.barplot(x=check.index, y=check.item_id, ax=ax)\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xlabel(\"Number of months since last sale occured\")\nax.set_ylabel(\"Number of items\")\nplt.show()","30f2c0c2":"check.loc[\"3 and more\", \"item_id\"] = check.loc[3:, \"item_id\"].sum()\ncheck = check.iloc[[0, 1, 2, -1]]","1d196317":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Inactive items among items to predict\")\nsns.barplot(x=check.index, y=check.item_id, ax=ax)\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xlabel(\"Number of months since last sale occured\")\nax.set_ylabel(\"Number of items\")\nfor idx, nb in enumerate(check.item_id):\n    ax.text(idx, nb+50, nb, ha=\"center\", va=\"bottom\",\n            color=list_colors[idx],\n            backgroundcolor=(0.95, 0.95, 0.95))\nplt.show()","c56bd9db":"shops_id = topredict.shop_id.unique()\nprint(\"We have to predict sales for \" +\n      \"{} shops (on a total number of {} shops)\".format(\n          len(shops_id), len(shops)))","07006c35":"print(\"There are {} new shops in the prediction list\".format(\n    len(shops_id) -\n    np.isin(shops_id, data_month.shop_id.unique()).sum()))","8f023b0c":"topredict_check = \\\n    data_month[np.isin(data_month.shop_id, shops_id)].copy()","c30db1f6":"check = topredict_check.groupby(\"shop_id\")[[\"date_block_num\"]].max()\ncheck.reset_index(inplace=True)\ncheck[\"months_since_last_sale\"] = 33 - check.date_block_num\ncheck = check.groupby(\"months_since_last_sale\").count()","3389c249":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Inactive shops among shops to predict\")\nsns.barplot(x=check.index, y=check.shop_id, ax=ax)\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xlabel(\"Number of months since last sale occured\")\nax.set_ylabel(\"Number of shops\")\nplt.show()","7b14875d":"check = data_month.groupby([\"shop_id\", \"shop_name\"])\\\n    [[\"date_block_num\"]].max()\ncheck.reset_index(inplace=True)\ncheck[\"shop_months_inactivity\"] = 33 - check.date_block_num\ncheck[\"shop_topredict\"] = np.isin(check.shop_id, shops_id)","3c46a21d":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Inactive shops\")\nsns.barplot(x=check.shop_name, y=check.shop_months_inactivity,\n            hue=check.shop_topredict, ax=ax)\nax.tick_params(axis=\"x\", rotation=90, labelsize=10)\nfor i, xt in enumerate(ax.get_xticklabels()):\n    if check.iloc[i].shop_topredict:\n        xt.set_color(list_colors[1])\n    else:\n        xt.set_color(list_colors[0])\nax.set_xlabel(\"\")\nax.set_ylabel(\"Number of months since last sale occured\")\nplt.show()","3b570797":"check = data[np.isin(data.shop_id, shops_id)].\\\n    groupby([\"shop_id\", \"shop_name\", \"day_of_week\"])\\\n    [[\"item_cnt_day\", \"item_revenue\"]].sum()\ncheck.reset_index(inplace=True)","7c81e7fa":"data_graph = check.pivot_table(\n    values=\"item_cnt_day\", columns=\"day_of_week\",\n    index=\"shop_name\", aggfunc=\"sum\", fill_value=0)\nfig, (ax, cbar_ax) = plt.subplots(\n    2, gridspec_kw={\"height_ratios\": (.9, .05), \"hspace\": .3},\n    figsize=(12, 12))\nfig.suptitle(\"Sales by day of week - only shops to predict\")\nsns.heatmap(data_graph, cmap=\"RdYlGn\", ax=ax, cbar_ax=cbar_ax,\n            cbar_kws={\"orientation\": \"horizontal\",\n                      \"label\": \"Number of sales\"})\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xticklabels(week_days)\nax.set_xlabel(\"\")\nax.set_ylabel(\"\")\nplt.show()","2b334c0a":"check = data_month[np.isin(data_month.shop_id, shops_id)].\\\n    groupby([\"shop_id\", \"shop_name\", \"item_category_name\"])\\\n    [[\"item_cnt_month\", \"item_revenue\"]].sum()\ncheck.reset_index(inplace=True)","c7145b1e":"data_graph = check.pivot_table(\n    values=\"item_cnt_month\", columns=\"item_category_name\",\n    index=\"shop_name\", aggfunc=\"sum\", fill_value=0)\nfig, (ax, cbar_ax) = plt.subplots(\n    2, gridspec_kw={\"height_ratios\": (1., .05), \"hspace\": .9},\n    figsize=(18, 16))\nfig.suptitle(\"Sales by category - only shops to predict\")\nsns.heatmap(data_graph, cmap=\"RdYlGn\", ax=ax, cbar_ax=cbar_ax,\n            cbar_kws={\"orientation\": \"horizontal\",\n                      \"label\": \"Number of sales\"})\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xlabel(\"\")\nax.set_ylabel(\"\")\nplt.show()","99ec162c":"data_graph = check[check.item_category_name != \"Movie - DVD\"].\\\n    pivot_table(\n    values=\"item_cnt_month\", columns=\"item_category_name\",\n    index=\"shop_name\", aggfunc=\"sum\", fill_value=0)\nfig, (ax, cbar_ax) = plt.subplots(\n    2, gridspec_kw={\"height_ratios\": (1., .05), \"hspace\": .9},\n    figsize=(18, 16))\nfig.suptitle(\"Sales by category - only shops to predict\")\nsns.heatmap(data_graph, cmap=\"RdYlGn\", ax=ax, cbar_ax=cbar_ax,\n            cbar_kws={\"orientation\": \"horizontal\",\n                      \"label\": \"Number of sales\"})\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xlabel(\"\")\nax.set_ylabel(\"\")\nplt.show()","4dfde831":"check = data_month[np.isin(data_month.shop_id, shops_id)].\\\n    groupby(\"item_category_name\")[[\"shop_id\"]].nunique()","8940c040":"fig = plt.figure(figsize=(12, 8))\nax = plt.axes()\nfig.suptitle(\"Number of shops selling a category\")\nsns.barplot(x=check.index, y=check.shop_id, ax=ax)\nax.axhline(y=1, color=\"black\")\nax.tick_params(axis=\"x\", rotation=90, labelsize=10)\nax.set_xlabel(\"\")\nax.set_ylabel(\"Number of shops\")\nplt.show()","4e105e19":"cat_to_check = check[check.shop_id == 1].index\nprint(\"Category name   -   Shop name\")\nfor cat in cat_to_check:\n    check = data_month[(np.isin(data_month.shop_id, shops_id)) &\n                       (data_month.item_category_name == cat)].\\\n                       shop_name.unique()\n    print(\"{} - {}\".format(cat, check))","0717e510":"categories[categories.item_category_name.str.contains(\"Number\")]","fd3ba49c":"data_month[(np.isin(data_month.shop_id, shops_id)) &\n    (data_month.item_category_name == \"Service - Tickets (Number)\")].\\\n    shop_name.unique()","a0621426":"check = data_month[np.isin(data_month.shop_id, shops_id)].\\\n    groupby([\"shop_id\", \"shop_name\", \"main_category\"])\\\n    [[\"item_cnt_month\", \"item_revenue\"]].sum()\ncheck.reset_index(inplace=True)","7865cf9a":"data_graph = check.pivot_table(\n    values=\"item_cnt_month\", columns=\"main_category\",\n    index=\"shop_name\", aggfunc=\"sum\", fill_value=0)\nfig, (ax, cbar_ax) = plt.subplots(\n    2, gridspec_kw={\"height_ratios\": (0.9, .05), \"hspace\": .3},\n    figsize=(12, 16))\nfig.suptitle(\"Sales by main category - only shops to predict\")\nsns.heatmap(data_graph, cmap=\"RdYlGn\", ax=ax, cbar_ax=cbar_ax,\n            cbar_kws={\"orientation\": \"horizontal\",\n                      \"label\": \"Number of sales\"})\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xlabel(\"\")\nax.set_ylabel(\"\")\nplt.show()","158673b2":"data_graph = check[check.main_category != \"Movie\"].\\\n    pivot_table(values=\"item_cnt_month\", columns=\"main_category\",\n                index=\"shop_name\", aggfunc=\"sum\", fill_value=0)\nfig, (ax, cbar_ax) = plt.subplots(\n    2, gridspec_kw={\"height_ratios\": (0.9, .05), \"hspace\": .3},\n    figsize=(12, 16))\nfig.suptitle(\"Sales by main category - only shops to predict\")\nsns.heatmap(data_graph, cmap=\"RdYlGn\", ax=ax, cbar_ax=cbar_ax,\n            cbar_kws={\"orientation\": \"horizontal\",\n                      \"label\": \"Number of sales\"})\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xlabel(\"\")\nax.set_ylabel(\"\")\nplt.show()","aa18a096":"check = data_month[np.isin(data_month.shop_id, shops_id) &\n                   np.isin(data_month.item_id, items_to_graph)].\\\n    groupby([\"shop_id\", \"shop_name\", \"item_id\", \"item_name\"])\\\n    [[\"item_cnt_month\", \"item_revenue\"]].sum()\ncheck.reset_index(inplace=True)","a047b89a":"data_graph = check.pivot_table(\n    values=\"item_cnt_month\", columns=\"shop_name\",\n    index=\"item_name\", aggfunc=\"sum\", fill_value=0)\nfig, (ax, cbar_ax) = plt.subplots(\n    2, gridspec_kw={\"height_ratios\": (0.9, .05), \"hspace\": 0.5},\n    figsize=(12, 30))\nfig.suptitle(\"Sales of the 98 top items - only shops to predict\")\nsns.heatmap(data_graph, cmap=\"RdYlGn\", ax=ax, cbar_ax=cbar_ax,\n            cbar_kws={\"orientation\": \"horizontal\",\n                      \"label\": \"Number of sales\"})\nax.tick_params(axis=\"x\", rotation=90)\nax.set_xlabel(\"\")\nax.set_ylabel(\"\")\nplt.show()","a1390d9d":"data_graph = check[check.item_id != 20949].\\\n    pivot_table(values=\"item_cnt_month\", columns=\"shop_name\",\n                index=\"item_name\", aggfunc=\"sum\", fill_value=0)\nfig, (ax, cbar_ax) = plt.subplots(\n    2, gridspec_kw={\"height_ratios\": (0.9, .05), \"hspace\": 0.5},\n    figsize=(12, 30))\nfig.suptitle(\"Sales of the 98 top items - only shops to predict\")\nsns.heatmap(data_graph, cmap=\"RdYlGn\", ax=ax, cbar_ax=cbar_ax,\n            cbar_kws={\"orientation\": \"horizontal\",\n                      \"label\": \"Number of sales\"})\nax.tick_params(axis=\"x\", rotation=90, width=8)\nax.set_xlabel(\"\")\nax.set_ylabel(\"\")\nplt.show()","8808e9bd":"all_months = range(34)\nall_data = []\nfor m in range(34):\n    all_data.append(np.array(list(product(\n        [m], data.shop_id.unique(), data.item_id.unique()))))\nall_data = pd.DataFrame(\n    np.vstack(all_data),\n    columns=[\"date_block_num\", \"shop_id\", \"item_id\"])","f415d9d8":"topredict[\"date_block_num\"] = 34\ntopredict = topredict[[\"date_block_num\", \"shop_id\", \"item_id\"]]\nall_data = pd.concat([all_data, topredict], ignore_index=True)","a4b5c517":"all_data[\"shopitem_id\"] = all_data.item_id * 100 + all_data.shop_id\ndata_month[\"shopitem_id\"] = \\\n    data_month.item_id * 100 + data_month.shop_id\ntopredict[\"shopitem_id\"] = \\\n    topredict.item_id * 100 + topredict.shop_id\nshopitems_id = topredict.shopitem_id.unique()","b52d435d":"all_data.sort_values([\"date_block_num\", \"shop_id\", \"item_id\"],\n                     inplace=True)","24512738":"grouped = \\\n    data_month[[\"item_id\", \"date_block_num\"]].groupby(\"item_id\")\ncheck = grouped.min()\ncheck.columns = [\"item_firstmonth_sell\"]\ncheck[\"item_lastmonth_sell\"] = grouped.max()\ncheck.reset_index(inplace=True)\nall_data = all_data.merge(check, how=\"left\", on=\"item_id\")\nall_data.fillna(34, inplace=True)\nall_data[\"item_seniority\"] = \\\n    all_data.date_block_num - all_data.item_firstmonth_sell\nall_data[\"item_stopped\"] = \\\n    all_data.date_block_num - all_data.item_lastmonth_sell - 1","918edd05":"grouped = \\\n    data_month[[\"shop_id\", \"date_block_num\"]].groupby(\"shop_id\")\ncheck = grouped.min()\ncheck.columns = [\"shop_firstmonth_sell\"]\ncheck[\"shop_lastmonth_sell\"] = grouped.max()\ncheck.reset_index(inplace=True)\nall_data = all_data.merge(check, how=\"left\", on=\"shop_id\")\nall_data.fillna(34, inplace=True)\nall_data[\"shop_seniority\"] = \\\n    all_data.date_block_num - all_data.shop_firstmonth_sell\nall_data[\"shop_stopped\"] = \\\n    all_data.date_block_num - all_data.shop_lastmonth_sell - 1","1c4e9c59":"grouped = data_month[[\"shopitem_id\", \"date_block_num\"]].\\\n    groupby(\"shopitem_id\")\ncheck = grouped.min()\ncheck.columns = [\"shopitem_firstmonth_sell\"]\ncheck[\"shopitem_lastmonth_sell\"] = grouped.max()\ncheck.reset_index(inplace=True)\nall_data = all_data.merge(check, how=\"left\", on=\"shopitem_id\")\nall_data.fillna(34, inplace=True)\nall_data[\"shopitem_seniority\"] = \\\n    all_data.date_block_num - all_data.shopitem_firstmonth_sell\nall_data[\"shopitem_stopped\"] = \\\n    all_data.date_block_num - all_data.shopitem_lastmonth_sell - 1","bf987193":"all_data = all_data[all_data.item_seniority >= 0]\nall_data = all_data[all_data.shop_seniority >= 0]\nif SKIP_NEGATIVE_SHOPITEM_SENIORITY:\n    all_data = all_data[all_data.shopitem_seniority >= 0]","2bd4fa64":"for col in [\"date_block_num\", \"shop_id\",\n    \"item_firstmonth_sell\", \"shop_firstmonth_sell\",\n    \"shopitem_firstmonth_sell\", \"item_lastmonth_sell\",\n    \"shop_lastmonth_sell\", \"shopitem_lastmonth_sell\",\n    \"item_stopped\", \"shop_stopped\", \"shopitem_stopped\",\n    \"item_seniority\", \"shop_seniority\", \"shopitem_seniority\"]:\n    all_data[col] = all_data[col].astype(np.int8)\nall_data[\"item_id\"] = all_data.item_id.astype(np.int16)\nall_data[\"shopitem_id\"] = all_data.shopitem_id.astype(np.int32)","76dcf53e":"print(\"Matrix is created\")","ad12c3ca":"check = all_data[all_data.date_block_num == 34]\\\n    [[\"item_id\", \"item_seniority\"]].groupby(\"item_id\").mean()\nprint(\"Number of new items = {}\".format(\n    len(check[check.item_seniority == 0])))\ncheck = all_data[all_data.date_block_num == 34]\\\n    [[\"shop_id\", \"shop_seniority\"]].groupby(\"shop_id\").mean()\nprint(\"Number of new shops = {}\".format(\n    len(check[check.shop_seniority == 0])))\ncheck = len(topredict) - np.isin(shopitems_id,\n    all_data[all_data.date_block_num < 34].shopitem_id.unique()).sum()\nprint(\"Number of new pairs shop\/item in data to predict = {}\".\\\n      format(check))","f3440cea":"del check\ngc.collect()","907cbd56":"check = shops[[\"shop_id\", \"shop_city\", \"shop_type\"]].copy()\ncheck[\"shop_isonline\"] = check.shop_type == \"online\"\ncheck.drop(columns=[\"shop_type\"], inplace=True)\nall_data = all_data.merge(check, how=\"left\", on=\"shop_id\")","036a1d4f":"all_data = all_data.merge(items, how=\"left\", on=\"item_id\")","4da53028":"check = categories[[\"item_category_id\", \"main_category\"]].copy()\ncat_id_select = [26, 27, 31, 34, 36, 44, 54, 74, 76, 78]\ncheck[\"category_online\"] = np.isin(\n    check.item_category_id, cat_id_select)\ncat_id_select = 9\ncheck[\"category_emergency\"] = check.item_category_id == 9\nall_data = all_data.merge(check, how=\"left\", on=\"item_category_id\")","02f1595b":"all_data[\"item_category_id\"] = \\\n    all_data.item_category_id.astype(np.int8)","b1e1f0cb":"print(\"Qualitative data is added\")\ndel check\ngc.collect()","7bd555b3":"items_id = topredict.item_id.unique()\nshops_id = topredict.shop_id.unique()\nshopitems_id = topredict.shopitem_id.unique()","6eb016e1":"all_data[\"item_topredict\"] = np.isin(all_data.item_id, items_id)\nall_data[\"shop_topredict\"] = np.isin(all_data.shop_id, shops_id)\nall_data[\"shopitem_topredict\"] = \\\n    np.isin(all_data.shopitem_id, shopitems_id)","435e3a22":"del topredict\ngc.collect()","b0ceb962":"refdate = pd.DataFrame({\"date_block_num\": range(35)})\nmydate = datetime.date(2013, 1, 1)\nrefdate[\"first_day_of_month\"] = pd.to_datetime([\n    datetime.date(mydate.year + (mydate.month + m - 1) \/\/ 12,\n                  mydate.month + (mydate.month + m - 1) % 12,\n                  mydate.day)\n    for m in range(35)])\nrefdate[\"month\"] = refdate.first_day_of_month.dt.month\nrefdate[\"year\"] = refdate.first_day_of_month.dt.year\nmycalendar = [np.array([dw for (dm, dw) in \n                        list(Calendar().itermonthdays2(y, m))\n    if dm > 0]) for (y, m) in zip(refdate.year, refdate.month)]\nrefdate[\"nb_days\"] = [len(mycalendar[i]) for i in range(35)]\nrefdate[\"nb_mondays\"] = [len(mycalendar[i][mycalendar[i]==0])\n                         for i in range(35)]\nrefdate[\"nb_tuesdays\"] = [len(mycalendar[i][mycalendar[i]==1])\n                          for i in range(35)]\nrefdate[\"nb_wednesdays\"] = [len(mycalendar[i][mycalendar[i]==2])\n                            for i in range(35)]\nrefdate[\"nb_thursdays\"] = [len(mycalendar[i][mycalendar[i]==3])\n                           for i in range(35)]\nrefdate[\"nb_fridays\"] = [len(mycalendar[i][mycalendar[i]==4])\n                         for i in range(35)]\nrefdate[\"nb_saturdays\"] = [len(mycalendar[i][mycalendar[i]==5])\n                           for i in range(35)]\nrefdate[\"nb_sundays\"] = [len(mycalendar[i][mycalendar[i]==6])\n                         for i in range(35)]\nrefdate.drop(columns=[\"year\"], inplace=True)\nall_data = all_data.merge(refdate, how=\"left\", on=\"date_block_num\")","9811eb33":"grouped = all_data[(all_data.date_block_num<34) &\n                   (all_data.shopitem_seniority==0)]\\\n    [[\"shop_id\", \"item_seniority\"]].groupby(\"shop_id\")\ncheck = grouped.mean()\ncheck.columns = [\"shop_avg_itemseniority_firstsell\"]\ncheck[\"shop_min_itemseniority_firstsell\"] = grouped.min()\ncheck[\"shop_max_itemseniority_firstsell\"] = grouped.max()\ncheck.reset_index(inplace=True)\nall_data = all_data.merge(check, how=\"left\", on=\"shop_id\")","c85fabc9":"grouped = all_data[(all_data.date_block_num<34) &\n                   (all_data.shopitem_seniority==0)]\\\n    [[\"item_category_id\", \"item_seniority\"]].\\\n    groupby(\"item_category_id\")\ncheck = grouped.mean()\ncheck.columns = [\"category_avg_itemseniority_firstsell\"]\ncheck[\"category_min_itemseniority_firstsell\"] = grouped.min()\ncheck[\"category_max_itemseniority_firstsell\"] = grouped.max()\ncheck.reset_index(inplace=True)\nall_data = all_data.merge(check, how=\"left\", on=\"item_category_id\")","f333423f":"grouped = all_data[(all_data.date_block_num<34) &\n                   (all_data.shopitem_seniority==0)]\\\n    [[\"main_category\", \"item_seniority\"]].groupby(\"main_category\")\ncheck = grouped.mean()\ncheck.columns = [\"maincategory_avg_itemseniority_firstsell\"]\ncheck[\"maincategory_min_itemseniority_firstsell\"] = grouped.min()\ncheck[\"maincategory_max_itemseniority_firstsell\"] = grouped.max()\ncheck.reset_index(inplace=True)\nall_data = all_data.merge(check, how=\"left\", on=\"main_category\")","35191bbe":"del refdate\ndel grouped\ndel check\ngc.collect()","f428bfa5":"for col in [\"month\", \"nb_days\", \"nb_mondays\", \"nb_tuesdays\",\n            \"nb_wednesdays\", \"nb_thursdays\", \"nb_fridays\",\n            \"nb_saturdays\", \"nb_sundays\",\n            \"shop_min_itemseniority_firstsell\",\n            \"shop_max_itemseniority_firstsell\",\n            \"category_min_itemseniority_firstsell\",\n            \"category_max_itemseniority_firstsell\",\n            \"maincategory_min_itemseniority_firstsell\",\n            \"maincategory_max_itemseniority_firstsell\"]:\n    all_data[col] = all_data[col].astype(np.int8)","f382e789":"check = \\\n    data_month[[\"date_block_num\", \"shop_id\", \"item_id\",\n    \"item_cnt_month\", \"item_revenue\", \"item_mean_price_month\"]].copy()\ncheck.columns = \\\n    [\"date_block_num\", \"shop_id\", \"item_id\", \"item_cnt_month\",\n     \"month_shopitem_revenue\", \"month_shopitem_price\"]\nall_data = all_data.merge(check, how=\"left\",\n                          on=[\"date_block_num\", \"shop_id\", \"item_id\"])","afdcaa41":"all_data[\"month_shopitem_hassales\"] = all_data.item_cnt_month.notna()","41ed27f5":"all_data.sort_values(by=[\"item_id\", \"shop_id\", \"date_block_num\"],\n                     inplace=True)\nall_data[\"month_shopitem_price\"] = all_data[[\"item_id\", \"shop_id\",\n    \"date_block_num\", \"month_shopitem_price\"]].\\\n    groupby([\"item_id\", \"shop_id\"])[\"month_shopitem_price\"].\\\n    fillna(method=\"ffill\")\nall_data.sort_values([\"date_block_num\", \"shop_id\", \"item_id\"],\n                     inplace=True)","37a4c7bc":"all_data.fillna(0, inplace=True)","c3dd6af6":"check = all_data[[\"date_block_num\", \"shop_id\", \"item_id\",\n                  \"item_cnt_month\"]].copy()\ncheck[\"prev_value\"] = check.groupby([\"shop_id\", \"item_id\"]).\\\n    item_cnt_month.shift()\ncheck[\"item_cnt_month_change\"] = \\\n    check.item_cnt_month \/ check.prev_value - 1\ncheck[\"item_cnt_month_change\"] = check.item_cnt_month_change.fillna(0)\ncheck[\"item_cnt_month_change\"] = \\\n    check.item_cnt_month_change.replace(np.inf, 1.0)\ncheck.drop(columns=[\"item_cnt_month\", \"prev_value\"], inplace=True)\nall_data = all_data.merge(check, how=\"left\",\n                          on=[\"date_block_num\", \"shop_id\", \"item_id\"])","03db0b62":"check = all_data[[\"date_block_num\", \"shop_id\", \"item_id\",\n                  \"month_shopitem_price\"]].copy()\ncheck[\"prev_value\"] = check.groupby([\"shop_id\", \"item_id\"]).\\\n    month_shopitem_price.shift()\ncheck[\"month_shopitem_price_change\"] = \\\n    check.month_shopitem_price \/ check.prev_value - 1\ncheck[\"month_shopitem_price_change\"] = \\\n    check.month_shopitem_price_change.fillna(0)\ncheck[\"month_shopitem_price_change\"] = \\\n    check.month_shopitem_price_change.replace(np.inf, 1.0)\ncheck.drop(columns=[\"month_shopitem_price\", \"prev_value\"],\n           inplace=True)\nall_data = all_data.merge(check, how=\"left\",\n                          on=[\"date_block_num\", \"shop_id\", \"item_id\"])","7c70e828":"check = all_data[[\"date_block_num\", \"shopitem_id\",\n                  \"item_cnt_month\", \"month_shopitem_price\"]].\\\n    groupby([\"shopitem_id\", \"date_block_num\"]).mean().reset_index()\ninter = check.groupby(\"shopitem_id\")[\"month_shopitem_price\"].\\\n    expanding().mean().reset_index()\ncheck[\"avg_month_shopitem_price\"] = \\\n    inter.month_shopitem_price\ninter = check.groupby(\"shopitem_id\")[\"item_cnt_month\"].\\\n    expanding().mean().reset_index()\ncheck[\"avg_month_shopitem_sales\"] = \\\n    inter.item_cnt_month\ncheck[\"month_shopitem_relative_price\"] = \\\n    check.month_shopitem_price \/ check.avg_month_shopitem_price - 1\ncheck[\"month_shopitem_relative_sales\"] = \\\n    check.item_cnt_month \/ check.avg_month_shopitem_sales - 1\ncheck.drop(columns=[\"item_cnt_month\", \"month_shopitem_price\",\n                    \"avg_month_shopitem_price\",\n                    \"avg_month_shopitem_sales\"], inplace=True)","159b16ae":"all_data = all_data.merge(check, how=\"left\",\n                          on=[\"shopitem_id\", \"date_block_num\"])","f5caad12":"all_data.fillna(0, inplace=True)","dfc69b0b":"print(\"Data by pair shop\/item is added\")","3d59e190":"all_data.sort_values(by=[\"date_block_num\", \"shop_id\", \"item_id\"],\n                     inplace=True)\ncol_base = [\"date_block_num\", \"shop_id\", \"item_id\", \"item_cnt_month\"]","0f5bb18b":"col_features = [\n    \"item_firstmonth_sell\", \"item_seniority\",\n    \"shop_firstmonth_sell\", \"shop_seniority\",\n    \"shopitem_firstmonth_sell\", \"shopitem_seniority\",\n    \"item_lastmonth_sell\", \"item_stopped\",\n    \"shop_lastmonth_sell\", \"shop_stopped\",\n    \"shopitem_lastmonth_sell\", \"shopitem_stopped\",\n    \"shop_city\", \"shop_isonline\",\n    \"item_category_id\", \"main_category\",\n    \"category_online\", \"category_emergency\",\n    \"first_day_of_month\", \"month\", \"nb_days\",\n    \"nb_mondays\", \"nb_tuesdays\", \"nb_wednesdays\", \"nb_thursdays\",\n    \"nb_fridays\", \"nb_saturdays\", \"nb_sundays\",\n    \"item_topredict\", \"shop_topredict\", \"shopitem_topredict\",\n    \"shop_avg_itemseniority_firstsell\",\n    \"shop_min_itemseniority_firstsell\",\n    \"shop_max_itemseniority_firstsell\",\n    \"category_avg_itemseniority_firstsell\",\n    \"category_min_itemseniority_firstsell\",\n    \"category_max_itemseniority_firstsell\",\n    \"maincategory_avg_itemseniority_firstsell\",\n    \"maincategory_min_itemseniority_firstsell\",\n    \"maincategory_max_itemseniority_firstsell\"]\ncol_to_save = col_base.copy()\ncol_to_save.extend(col_features)\nall_data[col_to_save].to_csv(\"alldata_descriptive.csv\", index=False)","90c910d1":"for col in [\"item_category_id\", \"main_category\"]:\n    col_features.remove(col)  # I need to keep those 2 features for category lags calculation\nall_data.drop(columns=col_features, inplace=True)","6c2ea9b0":"del check\ndel items\ndel categories\ndel shops\ndel sales\ndel data\ndel data_month\ngc.collect()","71aec4e3":"all_data.head()","49954527":"check = all_data[[\"date_block_num\", \"item_cnt_month\",\n                  \"month_shopitem_revenue\"]].\\\n    groupby(\"date_block_num\").sum()\ncheck.columns = [\"month_global_sumsales\", \"month_global_revenue\"]\ncheck.reset_index(inplace=True)","85a57cc3":"check[\"prev_value\"] = check.month_global_sumsales.shift()\ncheck[\"month_global_sumsales_change\"] = \\\n    check.month_global_sumsales \/ check.prev_value - 1\ncheck[\"prev_value\"] = check.month_global_revenue.shift()\ncheck[\"month_global_revenue_change\"] = \\\n    check.month_global_revenue \/ check.prev_value - 1\ncheck.drop(columns=[\"prev_value\"], inplace=True)","17293c88":"check[\"avg_month_global_sumsales\"] = \\\n    check.month_global_sumsales.expanding().mean()\ncheck[\"avg_month_global_revenue\"] = \\\n    check.month_global_revenue.expanding().mean()\ncheck[\"month_global_relative_sumsales\"] = \\\n    check.month_global_sumsales \/ check.avg_month_global_sumsales - 1\ncheck[\"month_global_relative_revenue\"] = \\\n    check.month_global_revenue \/ check.avg_month_global_revenue - 1\ncheck.drop(columns=[\"avg_month_global_sumsales\",\n                    \"avg_month_global_revenue\"], inplace=True)","b2e61dde":"all_data = all_data.merge(check, how=\"left\", on=[\"date_block_num\"])\nall_data.fillna(0, inplace=True)","35d66974":"print(\"Global values are calculated\")\ndel check\ngc.collect()","bbcb446e":"check = all_data[[\"item_id\", \"date_block_num\", \"item_cnt_month\",\n    \"month_shopitem_revenue\", \"month_shopitem_hassales\"]].\\\n    groupby([\"item_id\", \"date_block_num\"]).sum()\ncheck[\"month_item_price\"] = \\\n    check.month_shopitem_revenue \/ check.item_cnt_month\ncheck[\"month_item_sales\"] = \\\n    check.item_cnt_month \/ check.month_shopitem_hassales\ncheck.drop(columns=[\"item_cnt_month\", \"month_shopitem_revenue\",\n                    \"month_shopitem_hassales\"], inplace=True)\ncheck.reset_index(inplace=True)","c7074abf":"check[\"month_item_price\"] = check.groupby(\"item_id\")\\\n    [\"month_item_price\"].fillna(method=\"ffill\")\ncheck.fillna(0, inplace=True)","ec2a37c1":"check[\"prev_value\"] = \\\n    check.groupby(\"item_id\").month_item_price.shift()\ncheck[\"month_item_price_change\"] = \\\n    check.month_item_price \/ check.prev_value - 1\ncheck[\"prev_value\"] = \\\n    check.groupby(\"item_id\").month_item_sales.shift()\ncheck[\"month_item_sales_change\"] = \\\n    check.month_item_sales \/ check.prev_value - 1\ncheck[\"month_item_sales_change\"] = \\\n    check.month_item_sales_change.fillna(0)\ncheck[\"month_item_sales_change\"] = \\\n    check.month_item_sales_change.replace(np.inf, 1.0)\ncheck.drop(columns=[\"prev_value\"], inplace=True)","56fc2458":"inter = check.groupby(\"item_id\")[\"month_item_price\"].\\\n    expanding().mean().reset_index()\ncheck[\"avg_month_item_price\"] = inter.month_item_price\ninter = check.groupby(\"item_id\")[\"month_item_sales\"].\\\n    expanding().mean().reset_index()\ncheck[\"avg_month_item_sales\"] = inter.month_item_sales\ncheck[\"month_item_relative_price\"] = \\\n    check.month_item_price \/ check.avg_month_item_price - 1\ncheck[\"month_item_relative_sales\"] = \\\n    check.month_item_sales \/ check.avg_month_item_sales - 1\ncheck.drop(columns=[\"avg_month_item_price\",\n                    \"avg_month_item_sales\"], inplace=True)","ad46a80c":"all_data = all_data.merge(check, how=\"left\",\n                          on=[\"item_id\", \"date_block_num\"])\nall_data.fillna(0, inplace=True)","b940f1d9":"all_data[\"month_shopitem_compared_price\"] = \\\n    all_data[\"month_shopitem_price\"] \/ \\\n    all_data[\"month_item_price\"] - 1\nall_data[\"month_shopitem_compared_sales\"] = \\\n    all_data[\"item_cnt_month\"] \/ all_data[\"month_item_sales\"] - 1","d28ce4d3":"all_data.fillna(0, inplace=True)","45ea7786":"print(\"Data by item are calculated\")\ndel check\ndel inter\ngc.collect()","b0cb1a23":"check = all_data[[\"shop_id\", \"date_block_num\", \"item_cnt_month\",\n    \"month_shopitem_revenue\"]].\\\n    groupby([\"shop_id\", \"date_block_num\"]).sum()\ncheck.columns = [\"month_shop_sumsales\", \"month_shop_revenue\"]\ncheck.reset_index(inplace=True)","5f310cb7":"check.fillna(0, inplace=True)","5eaa0483":"check[\"prev_value\"] = check.groupby(\"shop_id\").\\\n    month_shop_sumsales.shift()\ncheck[\"month_shop_sumsales_change\"] = \\\n    check.month_shop_sumsales \/ check.prev_value - 1\ncheck[\"month_shop_sumsales_change\"] = \\\n    check.month_shop_sumsales_change.fillna(0)\ncheck[\"month_shop_sumsales_change\"] = \\\n    check.month_shop_sumsales_change.replace(np.inf, 1.0)\ncheck[\"prev_value\"] = \\\n    check.groupby(\"shop_id\").month_shop_revenue.shift()\ncheck[\"month_shop_revenue_change\"] = \\\n    check.month_shop_revenue \/ check.prev_value - 1\ncheck[\"month_shop_revenue_change\"] = \\\n    check.month_shop_revenue_change.fillna(0)\ncheck[\"month_shop_revenue_change\"] = \\\n    check.month_shop_revenue_change.replace(np.inf, 1.0)\ncheck.drop(columns=[\"prev_value\"], inplace=True)","7e6d3f3e":"inter = check.groupby(\"shop_id\")[\"month_shop_sumsales\"].\\\n    expanding().mean().reset_index()\ncheck[\"avg_month_shop_sumsales\"] = inter.month_shop_sumsales\ninter = check.groupby(\"shop_id\")[\"month_shop_revenue\"].\\\n    expanding().mean().reset_index()\ncheck[\"avg_month_shop_revenue\"] = inter.month_shop_revenue\ncheck[\"month_shop_relative_sumsales\"] = \\\n    check.month_shop_sumsales \/ check.avg_month_shop_sumsales - 1\ncheck[\"month_shop_relative_revenue\"] = \\\n    check.month_shop_revenue \/ check.avg_month_shop_revenue - 1\ncheck.drop(columns=[\"avg_month_shop_sumsales\",\n                    \"avg_month_shop_revenue\"], inplace=True)","26924784":"all_data = all_data.merge(check, how=\"left\",\n                          on=[\"shop_id\", \"date_block_num\"])\nall_data.fillna(0, inplace=True)","3486dfe9":"print(\"Data by shop are calculated\")\ndel check\ndel inter\ngc.collect()","4506b8a0":"if KEEP_CATEGORY_DATA:\n    check = all_data[[\"date_block_num\", \"item_category_id\",\n                      \"item_cnt_month\", \"month_shopitem_hassales\"]].\\\n        groupby([\"item_category_id\", \"date_block_num\"]).sum()\n    check[\"month_category_sales\"] = \\\n        check.item_cnt_month \/ check.month_shopitem_hassales\n    check.fillna(0, inplace=True)\n    check.reset_index(inplace=True)\n    check[\"prev_value\"] = check.groupby(\"item_category_id\").\\\n        month_category_sales.shift()\n    check[\"month_category_sales_change\"] = \\\n        check.month_category_sales \/ check.prev_value - 1\n    check[\"month_category_sales_change\"] = \\\n        check.month_category_sales_change.fillna(0)\n    check[\"month_category_sales_change\"] = \\\n        check.month_category_sales_change.replace(np.inf, 1.0)\n    check.drop(columns=[\"item_cnt_month\", \"month_shopitem_hassales\",\n                        \"prev_value\"], inplace=True)","d111d61b":"if KEEP_CATEGORY_DATA:\n    inter = check.groupby(\"item_category_id\")[\"month_category_sales\"].\\\n        expanding().mean().reset_index()\n    check[\"avg_month_category_sales\"] = inter.month_category_sales\n    check[\"month_category_relative_sales\"] = \\\n        check.month_category_sales \/ check.avg_month_category_sales - 1\n    check.drop(columns=[\"avg_month_category_sales\"], inplace=True)","f7e79bd0":"if KEEP_CATEGORY_DATA:\n    all_data = \\\n        all_data.merge(check, how=\"left\",\n                       on=[\"item_category_id\", \"date_block_num\"])","aa0824fa":"if KEEP_CATEGORY_DATA:\n    print(\"Data by category are calculated\")\n    del check\n    gc.collect()","19141a07":"if KEEP_MAIN_CATEGORY_DATA:\n    check = all_data[[\"date_block_num\", \"main_category\",\n                      \"item_cnt_month\", \"month_shopitem_hassales\"]].\\\n        groupby([\"main_category\", \"date_block_num\"]).sum()\n    check[\"month_maincategory_sales\"] = \\\n        check.item_cnt_month \/ check.month_shopitem_hassales\n    check.fillna(0, inplace=True)\n    check.reset_index(inplace=True)\n    check[\"prev_value\"] = check.groupby(\"main_category\").\\\n        month_maincategory_sales.shift()\n    check[\"month_maincategory_sales_change\"] = \\\n        check.month_maincategory_sales \/ check.prev_value - 1\n    check[\"month_maincategory_sales_change\"] = \\\n        check.month_maincategory_sales_change.fillna(0)\n    check[\"month_maincategory_sales_change\"] = \\\n        check.month_maincategory_sales_change.replace(np.inf, 1.0)\n    check.drop(columns=[\"item_cnt_month\", \"month_shopitem_hassales\",\n                        \"prev_value\"], inplace=True)","ab0dfb39":"if KEEP_MAIN_CATEGORY_DATA:\n    inter = check.groupby(\"main_category\")[\"month_maincategory_sales\"].\\\n        expanding().mean().reset_index()\n    check[\"avg_month_maincategory_sales\"] = inter.month_maincategory_sales\n    check[\"month_maincategory_relative_sales\"] = \\\n        check.month_maincategory_sales \/ check.avg_month_maincategory_sales - 1\n    check.drop(columns=[\"avg_month_maincategory_sales\"], inplace=True)","101514e3":"if KEEP_MAIN_CATEGORY_DATA:\n    all_data = \\\n        all_data.merge(check, how=\"left\",\n                       on=[\"main_category\", \"date_block_num\"])","14ab5fcb":"if KEEP_MAIN_CATEGORY_DATA:\n    print(\"Data by main category are calculated\")\n    del check\n    gc.collect()","6e72f776":"if KEEP_CATEGORY_DATA:\n    check = all_data[[\"date_block_num\", \"shop_id\", \"item_category_id\",\n        \"item_cnt_month\", \"month_shopitem_hassales\"]].\\\n        groupby([\"shop_id\", \"item_category_id\",\n                 \"date_block_num\"]).sum()\n    check[\"month_shopcategory_sales\"] = \\\n        check.item_cnt_month \/ check.month_shopitem_hassales\n    check.fillna(0, inplace=True)\n    check.reset_index(inplace=True)\n    check[\"prev_value\"] = \\\n        check.groupby([\"shop_id\", \"item_category_id\"]).\\\n        month_shopcategory_sales.shift()\n    check[\"month_shopcategory_sales_change\"] = \\\n        check.month_shopcategory_sales \/ check.prev_value - 1\n    check[\"month_shopcategory_sales_change\"] = \\\n        check.month_shopcategory_sales_change.fillna(0)\n    check[\"month_shopcategory_sales_change\"] = \\\n        check.month_shopcategory_sales_change.replace(np.inf, 1.0)\n    check.drop(columns=[\"item_cnt_month\", \"month_shopitem_hassales\",\n                        \"prev_value\"], inplace=True)","709aeb57":"if KEEP_CATEGORY_DATA:\n    inter = check.groupby([\"shop_id\", \"item_category_id\"])\\\n        [\"month_shopcategory_sales\"].expanding().mean().reset_index()\n    check[\"avg_month_shopcategory_sales\"] = inter.month_shopcategory_sales\n    check[\"month_shopcategory_relative_sales\"] = \\\n        check.month_shopcategory_sales \/ check.avg_month_shopcategory_sales - 1\n    check.drop(columns=[\"avg_month_shopcategory_sales\"], inplace=True)","c19d11ec":"if KEEP_CATEGORY_DATA:\n    all_data = all_data.merge(check, how=\"left\",\n        on=[\"shop_id\", \"item_category_id\", \"date_block_num\"])","770bf99e":"if KEEP_CATEGORY_DATA:\n    print(\"Data by pair shop\/category are calculated\")\n    del check\n    gc.collect()","7147b0f1":"if KEEP_MAIN_CATEGORY_DATA:\n    check = all_data[[\"date_block_num\", \"shop_id\", \"main_category\",\n        \"item_cnt_month\", \"month_shopitem_hassales\"]].\\\n        groupby([\"shop_id\", \"main_category\", \"date_block_num\"]).sum()\n    check[\"month_shopmaincategory_sales\"] = \\\n        check.item_cnt_month \/ check.month_shopitem_hassales\n    check.fillna(0, inplace=True)\n    check.reset_index(inplace=True)\n    check[\"prev_value\"] = \\\n        check.groupby([\"shop_id\", \"main_category\"]).\\\n        month_shopmaincategory_sales.shift()\n    check[\"month_shopmaincategory_sales_change\"] = \\\n        check.month_shopmaincategory_sales \/ check.prev_value - 1\n    check[\"month_shopmaincategory_sales_change\"] = \\\n        check.month_shopmaincategory_sales_change.fillna(0)\n    check[\"month_shopmaincategory_sales_change\"] = \\\n        check.month_shopmaincategory_sales_change.replace(np.inf, 1.0)\n    check.drop(columns=[\"item_cnt_month\", \"month_shopitem_hassales\",\n                        \"prev_value\"], inplace=True)","575789e2":"if KEEP_MAIN_CATEGORY_DATA:\n    inter = check.groupby([\"shop_id\", \"main_category\"])\\\n        [\"month_shopmaincategory_sales\"].expanding().mean().reset_index()\n    check[\"avg_month_shopmaincategory_sales\"] = inter.month_shopmaincategory_sales\n    check[\"month_shopmaincategory_relative_sales\"] = \\\n        check.month_shopmaincategory_sales \/ check.avg_month_shopmaincategory_sales - 1\n    check.drop(columns=[\"avg_month_shopmaincategory_sales\"], inplace=True)","54d2550d":"if KEEP_MAIN_CATEGORY_DATA:\n    all_data = all_data.merge(check, how=\"left\",\n        on=[\"shop_id\", \"main_category\", \"date_block_num\"])","2a028f86":"if KEEP_MAIN_CATEGORY_DATA:\n    print(\"Data by pair shop\/main category are calculated\")\n    del check\n    gc.collect()","35c7dc51":"all_data.drop(columns=[\"shopitem_id\"], inplace=True)\nall_data.info()","d8abad7f":"for col in all_data.columns:\n    if col != \"shopitem_id\":\n        all_data[col] = downcast_series(all_data[col])","b164218f":"all_data.info()","956ce451":"all_data.sort_values(by=[\"date_block_num\", \"shop_id\", \"item_id\"],\n                     inplace=True)\ncol_base = [\"date_block_num\", \"shop_id\", \"item_id\"]","68746ccd":"all_data[all_data.date_block_num < 34].drop(\n    columns=[\"item_cnt_month\", \"item_category_id\", \"main_category\"]).\\\n    to_csv(\"alldata_nolags.csv\", index=False)","1c6374fa":"mylags = [1, 2, 3]\nmylags_extended = [1, 2, 3]\nif LAGS_EXTENDED_PERIODS:\n    mylags_extended.extend([6, 12])\nmin_block_num = max(mylags_extended)","a3c29a10":"col_features = [\"item_cnt_month\", \"month_shopitem_price\"]\ncol_groupby = [\"shopitem\", \"shopitem\"]\nif LAGS_ON_RAW_VALUES:\n    col_features.extend(\n        [\"month_global_sumsales\", \"month_global_revenue\",\n         \"month_item_price\", \"month_item_sales\",\n         \"month_shop_sumsales\", \"month_shop_revenue\"])\n    col_groupby.extend([\"none\", \"none\", \"item\", \"item\",\n                        \"shop\", \"shop\"])\ncol_to_save = col_base.copy()","2fd5bed5":"for col, cgb in zip(col_features, col_groupby):\n    for lag in mylags_extended:\n        newcol = \"{}_lag_{}\".format(col, lag)\n        if cgb == \"none\":\n            check = all_data.groupby(\"date_block_num\")[[col]].\\\n                mean().shift(lag)\n            check.columns = [newcol]\n            all_data = \\\n                all_data.merge(check.reset_index(), how=\"left\",\n                               on=\"date_block_num\")\n        elif cgb == \"shop\":\n            check = all_data.groupby([\"shop_id\", \"date_block_num\"])\\\n                [[col]].mean()\n            check.columns = [newcol]\n            check[newcol] = \\\n                check.groupby(\"shop_id\")[newcol].shift(lag)\n            all_data = \\\n                all_data.merge(check.reset_index(), how=\"left\",\n                               on=[\"shop_id\", \"date_block_num\"])\n        elif cgb == \"item\":\n            check = all_data.groupby([\"item_id\", \"date_block_num\"])\\\n                [[col]].mean()\n            check.columns = [newcol]\n            check[newcol] = \\\n                check.groupby(\"item_id\")[newcol].shift(lag)\n            all_data = \\\n                all_data.merge(check.reset_index(), how=\"left\",\n                               on=[\"item_id\", \"date_block_num\"])\n        elif cgb == \"shopitem\":\n            all_data[newcol] = \\\n                all_data[[\"date_block_num\", \"shop_id\", \"item_id\",\n                          col]].groupby([\"shop_id\", \"item_id\"])\\\n                [col].shift(lag)\n        else:\n            print(\"unknown cgb\")\n        all_data[newcol] = downcast_series(all_data[newcol])\n        col_to_save.append(newcol)\n    print(\"Lags added for {}\".format(col))\n    all_data.drop(columns=col, inplace=True)","43b73097":"all_data.fillna(0, inplace=True)\nall_data[all_data.date_block_num >= min_block_num][col_to_save].\\\n    to_csv(\"alldata_rawlags.csv\", index=False)\nfor col in col_base:\n    col_to_save.remove(col)\nall_data.drop(columns=col_to_save, inplace=True)","1f4037aa":"if LAGS_ON_CHANGE_VALUES:\n    col_features = [\n        \"item_cnt_month_change\", \"month_shopitem_price_change\",\n        \"month_global_sumsales_change\", \"month_global_revenue_change\",\n        \"month_item_price_change\", \"month_item_sales_change\",\n        \"month_shop_sumsales_change\", \"month_shop_revenue_change\"]\n    col_groupby = [\"shopitem\", \"shopitem\", \"none\", \"none\",\n                   \"item\", \"item\", \"shop\", \"shop\"]\n    col_to_save = col_base.copy()\n    \n    for col, cgb in zip(col_features, col_groupby):\n        for lag in mylags:\n            newcol = \"{}_lag_{}\".format(col, lag)\n            if cgb == \"none\":\n                check = all_data.groupby(\"date_block_num\")[[col]].\\\n                    mean().shift(lag)\n                check.columns = [newcol]\n                all_data = \\\n                    all_data.merge(check.reset_index(), how=\"left\",\n                                   on=\"date_block_num\")\n            elif cgb == \"shop\":\n                check = \\\n                    all_data.groupby([\"shop_id\", \"date_block_num\"])\\\n                    [[col]].mean()\n                check.columns = [newcol]\n                check[newcol] = \\\n                    check.groupby(\"shop_id\")[newcol].shift(lag)\n                all_data = \\\n                    all_data.merge(check.reset_index(), how=\"left\",\n                                   on=[\"shop_id\", \"date_block_num\"])\n            elif cgb == \"item\":\n                check = \\\n                    all_data.groupby([\"item_id\", \"date_block_num\"])\\\n                    [[col]].mean()\n                check.columns = [newcol]\n                check[newcol] = \\\n                    check.groupby(\"item_id\")[newcol].shift(lag)\n                all_data = \\\n                    all_data.merge(check.reset_index(), how=\"left\",\n                                   on=[\"item_id\", \"date_block_num\"])\n            elif cgb == \"shopitem\":\n                all_data[newcol] = \\\n                    all_data[[\"date_block_num\", \"shop_id\", \"item_id\",\n                              col]].groupby([\"shop_id\", \"item_id\"])\\\n                    [col].shift(lag)\n            else:\n                print(\"unknown cgb\")\n            all_data[newcol] = downcast_series(all_data[newcol])\n            col_to_save.append(newcol)\n        print(\"Lags added for {}\".format(col))\n        all_data.drop(columns=col, inplace=True)\n    \n    all_data.fillna(0, inplace=True)\n    all_data[all_data.date_block_num >= min_block_num][col_to_save].\\\n        to_csv(\"alldata_changelags.csv\", index=False)\n    for col in col_base:\n        col_to_save.remove(col)\n    all_data.drop(columns=col_to_save, inplace=True)","ea3c1cb0":"if LAGS_ON_RELATIVE_VALUES:\n    col_features = [\"month_shopitem_relative_price\",\n                    \"month_shopitem_relative_sales\",\n                    \"month_global_relative_sumsales\",\n                    \"month_global_relative_revenue\",\n                    \"month_item_relative_price\",\n                    \"month_item_relative_sales\",\n                    \"month_shop_relative_sumsales\",\n                    \"month_shop_relative_revenue\",\n                    \"month_shopitem_compared_price\",\n                    \"month_shopitem_compared_sales\"]\n    col_groupby = [\"shopitem\", \"shopitem\", \"none\", \"none\",\n                  \"item\", \"item\", \"shop\", \"shop\",\n                  \"shopitem\", \"shopitem\"]\n    col_to_save = col_base.copy()\n    \n    for col, cgb in zip(col_features, col_groupby):\n        for lag in mylags:\n            newcol = \"{}_lag_{}\".format(col, lag)\n            if cgb == \"none\":\n                check = all_data.groupby(\"date_block_num\")[[col]].\\\n                    mean().shift(lag)\n                check.columns = [newcol]\n                all_data = \\\n                    all_data.merge(check.reset_index(), how=\"left\",\n                                   on=\"date_block_num\")\n            elif cgb == \"shop\":\n                check = \\\n                    all_data.groupby([\"shop_id\", \"date_block_num\"])\\\n                    [[col]].mean()\n                check.columns = [newcol]\n                check[newcol] = \\\n                    check.groupby(\"shop_id\")[newcol].shift(lag)\n                all_data = \\\n                    all_data.merge(check.reset_index(), how=\"left\",\n                                   on=[\"shop_id\", \"date_block_num\"])\n            elif cgb == \"item\":\n                check = \\\n                    all_data.groupby([\"item_id\", \"date_block_num\"])\\\n                    [[col]].mean()\n                check.columns = [newcol]\n                check[newcol] = \\\n                    check.groupby(\"item_id\")[newcol].shift(lag)\n                all_data = \\\n                    all_data.merge(check.reset_index(), how=\"left\",\n                                   on=[\"item_id\", \"date_block_num\"])\n            elif cgb == \"shopitem\":\n                all_data[newcol] = \\\n                    all_data[[\"date_block_num\", \"shop_id\", \"item_id\",\n                              col]].groupby([\"shop_id\", \"item_id\"])\\\n                    [col].shift(lag)\n            else:\n                print(\"unknown cgb\")\n            all_data[newcol] = downcast_series(all_data[newcol])\n            col_to_save.append(newcol)\n        print(\"Lags added for {}\".format(col))\n        all_data.drop(columns=col, inplace=True)\n    \n    all_data.fillna(0, inplace=True)\n    all_data[all_data.date_block_num >= min_block_num][col_to_save].\\\n        to_csv(\"alldata_relativelags.csv\", index=False)\n    for col in col_base:\n        col_to_save.remove(col)\n    all_data.drop(columns=col_to_save, inplace=True)","652023ee":"if KEEP_CATEGORY_DATA | KEEP_MAIN_CATEGORY_DATA:\n    col_features = []\n    col_groupby = []\n    if KEEP_CATEGORY_DATA:\n        col_features = [\"month_category_sales\",\n                        \"month_shopcategory_sales\",\n                        \"month_category_sales_change\",\n                        \"month_shopcategory_sales_change\",\n                        \"month_category_relative_sales\",\n                        \"month_shopcategory_relative_sales\"]\n        col_groupby = [\"category\", \"shopcategory\",\n                       \"category\", \"shopcategory\",\n                       \"category\", \"shopcategory\"]\n    if KEEP_MAIN_CATEGORY_DATA:\n        col_features.append(\"month_maincategory_sales\")\n        col_groupby.append(\"maincategory\")\n        col_features.append(\"month_shopmaincategory_sales\")\n        col_groupby.append(\"shopmaincategory\")\n        col_features.append(\"month_maincategory_sales_change\")\n        col_groupby.append(\"maincategory\")\n        col_features.append(\"month_shopmaincategory_sales_change\")\n        col_groupby.append(\"shopmaincategory\")\n        col_features.append(\"month_maincategory_relative_sales\")\n        col_groupby.append(\"maincategory\")\n        col_features.append(\"month_shopmaincategory_relative_sales\")\n        col_groupby.append(\"shopmaincategory\")\n    col_to_save = col_base.copy()\n    \n    for col, cgb in zip(col_features, col_groupby):\n        for lag in mylags:\n            newcol = \"{}_lag_{}\".format(col, lag)\n            if cgb == \"category\":\n                check = all_data.groupby(\n                    [\"item_category_id\", \"date_block_num\"])\\\n                    [[col]].mean()\n                check.columns = [newcol]\n                check[newcol] = \\\n                    check.groupby(\"item_category_id\")[newcol].shift(lag)\n                all_data = \\\n                    all_data.merge(check.reset_index(), how=\"left\",\n                                   on=[\"item_category_id\",\n                                       \"date_block_num\"])\n            elif cgb == \"shopcategory\":\n                check = all_data.groupby(\n                    [\"shop_id\", \"item_category_id\",\n                     \"date_block_num\"])[[col]].mean()\n                check.columns = [newcol]\n                check[newcol] = \\\n                    check.groupby([\"shop_id\", \"item_category_id\"])\\\n                    [newcol].shift(lag)\n                all_data = \\\n                    all_data.merge(check.reset_index(), how=\"left\",\n                                   on=[\"shop_id\", \"item_category_id\",\n                                       \"date_block_num\"])\n            elif cgb == \"maincategory\":\n                check = all_data.groupby(\n                    [\"main_category\", \"date_block_num\"])\\\n                    [[col]].mean()\n                check.columns = [newcol]\n                check[newcol] = \\\n                    check.groupby(\"main_category\")[newcol].shift(lag)\n                all_data = \\\n                    all_data.merge(check.reset_index(), how=\"left\",\n                                   on=[\"main_category\",\n                                       \"date_block_num\"])\n            elif cgb == \"shopmaincategory\":\n                check = all_data.groupby(\n                    [\"shop_id\", \"main_category\", \"date_block_num\"])\\\n                    [[col]].mean()\n                check.columns = [newcol]\n                check[newcol] = \\\n                    check.groupby([\"shop_id\", \"main_category\"])\\\n                    [newcol].shift(lag)\n                all_data = \\\n                    all_data.merge(check.reset_index(), how=\"left\",\n                                   on=[\"shop_id\", \"main_category\",\n                                       \"date_block_num\"])\n            else:\n                print(\"unknown cgb\")\n            all_data[newcol] = downcast_series(all_data[newcol])\n            col_to_save.append(newcol)\n        print(\"Lags added for {}\".format(col))\n        all_data.drop(columns=col, inplace=True)\n    \n    all_data.fillna(0, inplace=True)\n    all_data[all_data.date_block_num >= min_block_num][col_to_save].\\\n        to_csv(\"alldata_categorylags.csv\", index=False)","f9348a59":"All shops to predict are active.","1c964d72":"I notice a negative price. Prices shouldn't be null or negative. I'll check that.  \nThe max price seems really large in comparison with the Q3 price. I'll check that.","572d9d6b":"I focus on the 98 items that account for 80% of sales.","e2d2d067":"<a id=\"306\"><\/a>\n## 3.6. Sales by item","067254bb":"<a id=\"412\"><\/a>\n## 4.12. Data by pair shop\/main category","a866d6c5":"---\n<a id=\"200\"><\/a>","8158ef13":"First date is 01\/01\/2013 and last date is 31\/10\/2015  \nWe have 2 years and 10 months of data, that is 34 months of data.","d2847287":"There is only one sale with a negative price. I remove it.","a6d90f18":"First, I downcast data to save memory.","5f5abeb8":"<a id=\"305\"><\/a>\n## 3.5. Concentration analysis","feb49e17":"Averages aren't relevant at the level of the company. I prefer to keep track of total sales and total revenue for each month.","3d8eb32b":"**Items to predict**","1efa305d":"I remove the date column to keep the day_date column, with a datetime format.","cde588ca":"### 1.1.4. Sales","19d8c67c":"---\n<a id=\"0\"><\/a>\n# <font color=blue>Notebook set-up<\/font>","c1541900":"New in v8 : calculation of relative prices and quantities (in comparison to the mean over all periods) for each shop\/item pair. I take care of avoiding leakage by using an expanding mean (see point 4.4 for explanations).","5e7c47af":"In November 2020, 60,000 rubles is about 800 USD. I check the prices for those items except 13403, which has several sales at a price over 40000 rubles.","8a0e5583":"One item has sales much higher than others. Let's look at it.","0f76be14":"First I add four columns to the dataframe data : day of week, month and year of the sale, and item_revenue.","52e80fed":"Concentration by shop is similar for sales and income.","ceae0f4d":"**Shops 10 and 11**","d26e37d1":"---\n<a id=\"100\"><\/a>","16f1cb2f":"# <font color=blue>1. Exploration and cleaning<\/font>","cbe7f4cf":"**Item 13199**  \nItem_id 13199 is a collection product according to its name. Its price may be ok.","585730c7":"I add the seniority of items, shops and pairs item\/shop. The seniority is the number of months since the first sale occured in the set. If not available, first month is set to 34 (new item or pair item\/shop).  \nI then can delete all lines with negative seniority : I don't care about data on non-existing items or shops. And I don't want the means to be impacted by their zero values.","9a67c730":"There's no duplicate. But I see that I can group categories by main category. As I have only 84 rows, I can easily take a deeper look at the categories.","18827153":"Given my analysis, I decide to keep following data :\n- shops : shop name, shop city, and a boolean to know if it is an online store ;\n- items : item name ;\n- categories : main category, a boolean to know if the category is sold only by the online store and another boolean to know if the category is sold only by the emergency store.","5c8a9ac1":"There is no missing data. That's a good point.  \nThe number of rows is coherent with my sales DataFrame.","dfd1aad4":"Categories and main categories are pertinent indicators to get a clue about the number of sold items.","745c8c20":"I can get rid of other dataframes to save memory space.","cc030d4b":"**Inactive shops**","6bf42971":"New in v8 : calculation of a relative feature","e609b011":"**Downcasting of date features**","fe33ff76":"**Sales by category for shops to predict**","7b065db4":"**Shops 0 and 57**","abca75c5":"Each date block num matches a single month. As a result, we have 34 different values for the date_block_num. This feature will be useful to group data by month.","8b308fc8":"I add 2 features to see how the price (in %) and the sales (in value) change for each item from one month to another in relative values.","6ce00822":"Let's check what is the shop for each of the 14 categories with only one shop.","6f5c46dd":"### 1.4.4. Item Count Day","6340830c":"**Shops 39, 40 and 41**","f7782a8f":"I add the values of interest that I have allready calculated in the data_month dataframe. I select the sum of sales, the sum of revenues, and the mean price of sales for each pair shop\/item and month.  \nI add a column that acts as a flag to know if sum of sales is different from 0. Average price will be defined by the sum of revenues divided by the sum of sales. Average sales will be the result of the sum of sales divided by the count of true flags. This allows us to get averages with an economic meaning because average price and average sales shouldn't be impacted by rows without sales.","21351d83":"<a id=\"304\"><\/a>\n## 3.4. Sales by category","b01cd410":"I can easily check the new items or pairs of shop\/items.","003b8f23":"I add the relative sales and revenues of the shop, in comparison to its mean.  \nNote : beware, those features are a kind of leakage to avoid if you use the average on all periods (like in several notebooks). You can't use an average calculated on months 0 to 33 at months 0 to 32, or you're using future data that you didn't know at that moment. It is the same as calculating the change between current month and next month.  \nI calculate relative values with an expanding mean to avoid the leakage.","8f22962d":"Those 3 items have been sold only once. They belong to the same category : Programs for home and office. Their name shows they are professional, specialized and multi-licence programs for offices. Their price may be ok. I check the prices for their category (id = 75).","43a902f0":"Total income mainly comes from games (for consoles and PC) and game consoles. For our purpose of counting sales, most vending items are movies. Those two charts seem to confirm that sales numbers depend on the item price. Cheaper products are more easily sold.","3cc271e0":"Most sales are done in shopping center, during this period. An analysis of the evolution over time would give us more information.","c15b062e":"I calculate only the average sales as I don't consider prices relevant for categories.","001c1255":"I check duplicates on the item_name, that belong to the same category.","b92200aa":"<a id=\"302\"><\/a>\n## 3.2. Sales by shop","b76a321a":"New in v8 : calculation of a relative feature","0e3d0892":"**Check if \"games\" and \"game consoles\" main categories are duplicates**","e39bdd83":"# <font color=blue>4. Feature Engineering<\/font>","f16c1646":"This item has been sold during 31 weeks, in 52 different shops, at 126 different prices. It's not a one-shot sale.","8138a93f":"<a id=\"408\"><\/a>\n## 4.8. Data by shop","17df9e99":"**Graphs set-up**","487ddf53":"**Study of negative values**","fddbcd41":"Those two shops seem to be duplicates. They haven't operated at the same time. Shop 57 begun just after shop 0 closed. So I remove shop n\u00b00 and replace its sales by shop n\u00b057.","d506eebd":"v2 : addition of the table of contents  \nv3 : data with lags are split in several csv files (+ a little adjustment)  \nv4 : changes in lags calculation ; addition of a feature on category sales changes from month to month  \nv5 : move some lags from \"relative\" to \"leakage\" flag ; improve data saving : fillna and remove first months of data (when lags are #na due to no prior data) before saving  \nv6 : fix a bug on category lags calculation ; no more save the leakage features  \nv7 : add a filter on negative item_cnt_month if the option skip_negative_counts is set to false ; add a new expanding mean calculation to get relative features without leakage ; add the month when an item was sold for the last time ; make code more PEP8 compliant ;-)  \nv8 : a bit more analysis ; a few more relative features ; a change in the order of features creation and saving (csv files) so that the RAM can handle it all with the new features\nv9 : None  \nv10 : Quantity changes are modified to percentage to be in line with other change features  \nv12 : handling changes with np.inf value","ecdb14a8":"I calculate only the average sales. Prices could be relevant for main category, but I prefer to focus on sales as I think I have enough features.","a2e12c74":"I add average sales and average price by item, for each month. I calculate average sales only on positive monthly sales.  \nI fill forward missing monthly item average price.","f62875a5":"**Shops 23 and 24**","944ba87e":"<a id=\"410\"><\/a>\n## 4.10. Data by main category","fe8274c8":"Comparison of sales and incomes show that the sales are linked to the price of the product. For example, expansive items like game consoles are a small part of sales but participate significantly to total income.  \nI move to an analysis by main category for the following graphs.","683b2b96":"There's only a sale with a price over 100,000 rubles. I check the regular price for this item.","4f4471d5":"**Concentration by items**","8a5db8cb":"**Shops to predict**","f46159de":"**Sum other duplicated sales**","717f51fa":"The sales by shops are more balances : a third of shops represent about 60% of sales.","fb77e18e":"Addition of qualitative information","3d960b86":"If I consider that items not sold in the last 3 months are inactive, I can group them in an inactive group and have their count.","8b2c65ba":"This price of almost 60,000 rubles for delivery seems to be an outlier. I could consider that 2 zeros have been added to the price by mistake. But I have no clue about that, so I'll change this price to median price of this item.","b5b21886":"**Study of high values**","e94b4a78":"Some services related to online sales seem to be increasing : cash acceptance and delivery to the delivery point.","24b1c6d0":"The max price seems to be a real outlier. Let's check the product with this price.","214ce55c":"**Sales of top 98 items for shops to predict**","1bf5b94c":"<a id=\"413\"><\/a>\n## 4.13. Lags","d8ab22f0":"I now look at the homogeneity of sold quantities and mean prices by shop. I think I should have different counts and prices as a shop sell different products from different categories.","739b51d7":"Some shops keep have stopped their activity.  \nThe shop Moscow Sale only sells in October (must be a kind of seasonal outlet).  \nWhile some shops increase their sales and incomes (as Moscow Atrium, St Petersburg Nevsky Center or the online shop), other ones exhibit a decrease in activity (for example Yakutsk Central).","8858e775":"Most shops have a similar sales pattern. They sell more on Friday and on week-end days. There are two important exceptions to this rule : online services. The online shop seems to have almost constant sales during all the week. The emergency online is not very active during the week-end.","e2c5b5af":"### 1.4.3. Item Price","fba9c5b4":"<a id=\"407\"><\/a>\n## 4.7. Data by item","d016e674":"<a id=\"104\"><\/a>\n## 1.4. Checking numeric values","7eae8183":"I check if some item_id have to be predicted while others are not to predict, among duplicates.","b10edbef":"I run the following steps :\n- Exploring and cleaning data ;\n- Grouping data by month ;\n- Analysing the data. I think I can push deeper the analysis. I'll do that in a future version of this notebook ;\n- Feature engineering. I create a lot of features. Some features are useful for some models. Other features are useful for ohter models. I won't use the same features for a SARIMAX model or for a XGBoost Model ;  \n- Testing the distribution of features, in case I want to use them with some models that imply some distribution hypotheses.","be8efc7d":"There are no more sales with a null quantity.","5e2db19b":"Evolution over time is more meaningful by shop than by city or shop type. We can notice that online sales are increasing while store sales are decreasing in all cities.","20148962":"**Item 11365**  \nA delivery price of 800 dollars seems stellar.","c1da0b35":"Some shops seem to sell cheaper products on average. But this can be explained by an evolution of sales over time.","338f8f12":"**Inactive items**","90c3b15c":"With no surprise, the vast majority of sales are done in Moscow. Let's have a look at the same data with a log scale.","e7bc2f04":"New in v8 : calculation of a relative feature","8ad5bd83":"Income of shop 39 stay stable and income of shop 41 decrease when shop 40 closes. We can't say for sure that shop 40 has to be added to shop 39. I don't merge them.","a04fd79d":"**Seniority of items when first sold by a shop**  \nI keep track of the mean seniority of items when they are first sold by a shop. I also keep in memory the min and max value.  \nI do the same calculation for category and main_category.","a266ae57":"<a id=\"401\"><\/a>\n## 4.1. Creation of the matrix","25d272bb":"The company sells less items from one year to another. But the average price goes up as revenues don't decrease that much.  \nSales tend to increase in November and December, due to Christmas purchases. This seasonality will impact our forecasts as we predict sales for November.","fb5fc743":"<a id=\"411\"><\/a>\n## 4.11. Data by pair shop\/category","4a178883":"Negative counts are the cancellation of a previous sale. I keep them. I will have to check that I don't have negative count when data will be grouped by month.","94dcf90d":"I check sales with a price over 40000 rubles.","32800855":"I must check full duplicates on initial sales data, because I may have created duplicates by changing references in items.","57cef999":"There are no big differences between shops, except for the sold quantity of a few top selling products. Same for differences between cities and between shop types.  \nI notice that the sale shop has the highest median price. This is weird for what I suppose to be an outlet store. Either it's not an outlet, either it's an outlet store that sells only high-price items.","d3a38e66":"<a id=\"403\"><\/a>\n## 4.3. Items and shops to predict","2ddde606":"<a id=\"101\"><\/a>\n## 1.1. Checking duplicates","8c6fba97":"**Features related to the month**  \nMonth (seasonal impact).  \nNumber of days in the month (can explain some differences in sales).","d8a37e64":"<a id=\"405\"><\/a>\n## 4.5. Data related to the pair shop \/ item","7469630d":"I check inactivity accross all shops, not only the ones to predict.","6c34de4d":"Each date block num includes only one month and one year, and has between 28 and 31 days. Nothing to notice.","b2db4319":"All categories with number in their name seem to be sold only by the online shop. I check that point.  \nService - Product Delivery is sold only by the online store of emergency.","2a1aff9b":"Monthly quantities and average prices are different according to items.","aa49c951":"Games and game consoles main categories are different.","b0b0481a":"### 1.4.1. Date","c360f204":"**Concentration by shops**","2c65b372":"I don't save the month 34 in no lags files. I don't have data for this month. So I don't need to save data that come from some fillna.  \nItem_cnt_month is allready saved in the description csv file.","845cf91c":"I calculate only the average sales.","7c438e89":"**Final values of item prices after corrections**","ca61a6a5":"I directly downcast data to save memory.","5aa732d8":"I keep track of the rows of interest : does the item or the shop (or both) belong to the ones to predict ?","026e8d30":"Income is even more concentrated : 20% of items account for more than 90% of total income.","e0057023":"I check duplicates on the category name.","a67edc5f":"**Study of null or negative prices**","4464649b":"The same products appear several times among most saled products. So I keep those outliers, that are high but not aberrant.","f1fdd76d":"I notice one important thing : some categories are sold only by the online shop. Let's have a closer look at that.","57a89713":"Those two shops seem to be duplicates. They haven't operated at the same time. Shop 58 begun just after shop 1 closed. So I remove shop n\u00b01 and replace its sales by shop n\u00b058.","c6968514":"# <font color=blue> 2. Grouping data by month<\/font>","287eb120":"**Table of contents**  \n[Notebook setup](#0)  \n[1. Exploration and cleaning](#100)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.1. Checking duplicates](#101)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.2. Putting all data together](#102)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.3. Checking missing values](#103)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[1.4. Checking numeric values](#104)  \n[2. Grouping data by month](#200)  \n[3. Analysis](#300)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.1. Total sales](#301)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.2. Sales by shop](#302)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.3. Change of sales by shop over time](#303)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.4. Sales by category](#304)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.5. Concentration analysis](#305)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.6. Sales by item](#306)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.7. Items and shops to predict](#307)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.8. Shops : more in-depth analysis](#308)  \n[4. Feature Engineering](#400)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[4.1. Creation of the matrix](#401)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[4.2. Qualitative data](#402)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[4.3. Items and shops to predict](#403)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[4.4. Features related to the date](#404)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[4.5. Data related to the pair shop \/ item](#405)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[4.6. Global values](#406)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[4.7. Data by item](#407)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[4.8. Data by shop](#408)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[4.9. Data by category](#409)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[4.10. Data by main category](#410)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[4.11. Data by pair shop\/category](#411)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[4.12. Data by pair shop\/main category](#412)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[4.13. Lags](#413)","107b1bf0":"### 1.4.2. Date block num","76fbd2ca":"<a id=\"301\"><\/a>\n## 3.1. Total sales","e3c50d66":"I add relative features to see how the sumsales and revenue of a shop compares with its expanding mean (to avoid leakage).","ed522630":"**Inactive shops**","ebfed4ac":"<a id=\"402\"><\/a>\n## 4.2. Categorical data","33f616b4":"It's a cheap item with a regular price of around 5 rubles, and a few sales occuring at a lower price.","b107fa5a":"Almost all items are sold under 2500 rubles.","dfe62237":"I also add the price change, in percentage.","707a7b91":"I think we can't fill all missing values for the mean price with 0. If an item has not been sold during a given month by a given shop, that doesn't mean its price is 0.  \nAs I have no other information about price changes in a specific store, and to keep it pretty easy, I proceed in a 2-step process for missing mean prices :\n- Firstly, I fill forward the missing values : I consider that a price doesn't change until the next month when this item is sold by the shop ;\n- Secondly, I fill remaining missing values with 0 : I can't know the price if this item has never been sold before by the shop (only for the case when we keep shop\/items pairs data with negative seniority).","ed254e7f":"Those two shops seem to be duplicates. Shop n\u00b011 includes sales for only one month. I don't have sales at shop n\u00b010 for this month. Shop n\u00b011 seems to be a mistaping of the name of shop n\u00b011. So, I remove shop n\u00b011 and replace its sales by shop n\u00b010.","a382940d":"I group the data by \"theme\" before saving to csv files, in order to get lighter csv files.  \nI firstly save the dataframe without lags. It may be useful to select only a few columns for a time-serie approach like SARIMAX.  \nThen, I use the combination of groupby and shift functions of pandas to calculate the lag features. I delete features when their lags are created. I calculate lags for a lot of features. I prefer to have a lot of available features. According to models, I may not use all of them.  \nI fillna with zero values (this is needed for items, shops, ... that are not present from month 0) and I drop the first data_block_num that correspond to the maximum lag period calculated on features.","6e68b411":"### 1.1.1. Items","a14e47f7":"The date is in a text format. I create a new column with a date format.","485c953e":"# <font color=blue>3. Analysis<\/blue>","afc37a54":"I calculate the relative values in comparison to an expanding mean price and an expanding mean sales by item (expanding mean to avoid leakage).","ee9ecd84":"**Data options**","23958ce9":"**Notebook functions**","39ff6ec0":"**Remove duplicates on initial sales data**","f103389c":"I save the descriptive informations, together with the target (item_cnt_month). Then, I remove the descriptive columns from the all_data dataframe in order to save memory.","5d25e29a":"<a id=\"102\"><\/a>\n## 1.2. Putting all data together","4984e43d":"**Duplicates on item_name**","d8292551":"Beware ! All this analysis was run in the first version of the notebook (v1), without skiping negative item_day_cnt. So small differences may appear in a few data if this option is changed.","407ab337":"<a id=\"307\"><\/a>\n## 3.7. Items and shops to predict","8920f39f":"### 1.1.2. Categories","6d9cf4c2":"I think we have two cases :\n- full duplicates on initial sales data : we should delete one of them ;\n- duplicates on [date, shop_id, item_id] : they are due to different prices for the same good. We should sum the item_cnt_day and calculate the mean item_price.","53562957":"There's only one sale of this product. We don't have to estimate its future sales. I remove this sale from the dataframe.","229311f1":"The 3 top selling shops are located in Moscow.  \nLet's have a look at figures by shop type and shop city.","07ced34b":"As I don't keep data prior to shop opening (with the seniority check), I shouldn't have missing values for shops as long as they are in activity.","ef98ad32":"The downcasting decreases more than 1GB the size of the dataframe.","04783a61":"<a id=\"409\"><\/a>\n## 4.9. Data by category","6ede87bb":"We see that the sales of an item tend to decrease when it gets older. The most sold item is an exception with sales at a very high level for a long time.  \nFor a better readability, I do one more time the same graph without that particular item.","7be114c0":"**Average quantity and price by category and main category**","49e757cf":"<a id=\"406\"><\/a>\n## 4.6. Global values","a49897bb":"I add relative features to see how the price of an item in a shop compares with the average price of this item. I do the same with the sales.  \nBeware : the following feature name changes in v8, due to addition of another realtive feature at point 4.3 and to keep an uniform name between features.","437f51ef":"I do the same graph without the category \"Movie - DVD\" to have a better analysis of other data.","d0ce70d6":"We don't have to predict for shops with one month or more of inactivity. We also don't need to predict for the shops with the types \"sale\" and \"itinerant\". Remember that those 2 shops seem to sell only in October.","6572a591":"I check if the following shops are duplicates :\n- shops number 23 and 24 in Moscow ;\n- shops number 39 to 41 in Rostov : Mega, Horizon, Mega Horizon ?\n- shops number 0 and 57 in Yakutsk ;\n- shops number 1 and 58 in Yakutsk ;\n- shops number 10 and 11 in Zhukovsky.","5fb40d74":"**Item 7238, 7241 and 14173**","38856e86":"I notice negative values for some item sales on a day. It can be due to some people bringing back some items with defaults. I check that point.  \nThe max number of items sold on a day seems really large in comparison with the Q3 number. Maybe a few products concentrate most of the sales.","df958a8b":"**Study of null values**","c25e44bc":"20% of sales come from only 98 items. I can select those items to have an idea of sales evolution by items, as I can't graph all items.","e4d2c8b4":"The company sales more on Friday and week-end days.","12fe2fd9":"**Sales by main category for shops to predict**","7a24e5ca":"**Study of high prices**","ba8486c8":"356 items haven't been sold in the last 3 months or more. I think their sales can be fixed to 0 for the next month. I keep track of the number of months of inactivity in the data_month dataframe. I also add a boolean to track items that will be predicted. By doing so, I can easily change the threshold for inactivity and the strategy of getting rid of not-to-predict items or of inactive items.","7638ca4c":"**Sales by day of week for shop to predict**","b8cf7365":"We need to create a dataframe with every combination of months, shops and items, so that we have rows with zero values for items not sold in a given shop for a given month.","8fbe3204":"This graph is not very interesting.  \nOne of the most sold items is the delivery service.","840f9f9b":"The 20% most sold items represent a little more than 80% of sales. This is a usual ratio in business.","ca8c4189":"---\n<a id=\"400\"><\/a>","f7244a60":"<a id=\"103\"><\/a>\n## 1.3. Checking missing values","973c2d87":"<a id=\"303\"><\/a>\n## 3.3. Change of sales by shop over time","8a4544cf":"Those three shops don't seem to be duplicates. They have operated at the same time. Shop nnumber 40 has closed in month n\u00b024.  \nI must check if it's not a delivery point for shop 39 (same opening date) that has closed. In this case, I can add them up. I check the changes in income of the three shops.","ef998695":"The company seems to have begun a double diversification. Firstly, it develops a new distribution chanel (we previously saw that online sales are increasing). It has also diversified its offer : sales of books and services are pretty new and are increasing.","80a6eb57":"<font size=6>Predicting Future Sales<\/font>  \n<font size=5>A feature engineering based on an economical and financial approach<\/font>\n","806aefb0":"The 21 sales with a zero price have an item_cnt_day of zero. I can remove them. ","a36aee5f":"Different cases :\n- none of the duplicated items are to predict : I keep the item_id of my choice (the largest id as it may be the more recent one) and I add the sales of the other item_id ;\n- only one of the duplicated items is to predict : I remove other item_id items and move their sales to the item to predict.\n- two or more of the duplicated items are to predict : I must keep all items.","f2c43f1a":"I add a feature : the difference of items sold from one month to another. By doing so, I am able to opt for absolute values (number of items that are sold in one month) or relative values (change in the number of items that are sold).  \nFor the first month, I replace NA values by 0.","c4e7958b":"Except for the tickets, all categories tagged with number are sold only by the online store.","ad9e0f05":"Average prices and sales aren't meaningful at the level of a shop, as it can sell very different products. Total sales and revenues are more relevant.","933b3534":"SKIP_NEGATIVE_COUNTS : if true, I delete all item_cnt_day with negative values. If false by default : returned products decrease the number of sales. Ddefault value is False. I take care of special cases when the item_cnt_month is negative since v7.  \nSKIP_NEGATIVE_SHOPITEM_SENIORITY : if true, I delete all lines where seniority of the pair shop\/item is negative. Set to true by default : data is not kept if the item has never been sold before by the shop.  \nKEEP_CATEGORY_DATA : if true, I add data by category and data by pair shop\/category. Set to true by default : I use data by category.  \nKEEP_MAIN_CATEGORY_DATA : if false, I don't keep data by main category and data by pair shop\/main category. Set to true by default : I use data by main category.  \nLAGS_ON_RAW_VALUES = if true, I calculate the lag values on raw data (true by default).  \nLAGS_ON_CHANGE_VALUES = if true, I calculate the lag values on changes (true by default).  \nLAGS_ON_RELATIVE_VALUES = if true, I calculate the lag values on relative values (true by default).  \nLAGS_EXTENDED_PERIODS : if false, I calculate lags for 1, 2 and 3 months. Set to true by default : I calculate lags for 1, 2, 3, 6 and 12 months. The extended periods are applied only to raw values, to deal with RAM capacity.","402a9c97":"**Create the \"main category\" feature**","5694f7b0":"I check that each date block num is coherent with calendar months.","bceca4b9":"I add the change in sales and revenues for every shop.","c7d0120e":"Several actions to take :  \n1. create a main category feature : allmost all category names are composed according to the same naming scheme : main category name ; hyphen ; category name.  \n2. change 6 category names, so that they have the same naming scheme as others.  \n3. check if the main categories \"games\" and \"game consoles\" are duplicates.","6a316f01":"There's only one sale of this product. As it is mentionned \"collection\", its price may be ok. But I remove it as it can unbalanced its category.","5ae1c833":"**Shops 1 and 58**","a5589c4a":"New in v8 : calculation of a relative feature","c2af6349":"<a id=\"308\"><\/a>\n## 3.8. Shops : more in-depth analysis","3754478c":"Average sales, average price, total sales and total revenue by pair shop\/main category, for each month.","eeffee43":"I take a first general look at the values of the columns.","abb8f34b":"I group data by month for each pair of shop and item. I remove negative monthly counts and negative monthly prices (they can happen if we keep the negative daily counts).  \nI compute a weighted mean price, unlike most of kernels I've seen. I think that if 9 products are sold at the price of 10 rubles and 1 product is sold at the price of 20 rubles, the mean price should be 11 rubles (and not 15 rubles as in a lot of kernels).","1b0b5ae3":"<a id=\"404\"><\/a>\n## 4.4. Features related to the date","7dbbe9e6":"Those two shops don't seem to be duplicates. They have operated at the same time. One has closed in month n\u00b03.","6fc563dd":"I also calculate the change of global sales and revenues (in percentage), and their monthly relative value in comparison to the mean over all periods.","d17981d3":"I add data to predict.","c82d8334":"This competition is the final project of a data science course. The goal is to predict the sales of items in a selection of shops of a Russian tech company. There no predictions in this kernel. Only data processing.  \nI've seen several interesting notebooks about data cleaning et feature engineering. But I have the feeling that there are some mistakes on some points. Some calculations don't seem logical from an economical point of view.  \nFor example, I base average price calculation on the division of the sum of incomes by the sum of sales count. If you sell 8 products at 10 USD, 1 product at 15 USD and 1 product at 20 USD, the average price isn't 15 USD (as often calculated in the notebooks I have read) but 11.5 USD. Average sales count is the sum of sales count divided by the number of sales.  \nI also try to define smart features from an economic point of view, as the change in price or the change in sales from one month to another.  \nI have detected leakage features in several notebooks. One shouldn't use a ratio with the average of a value on all periods, as you don't know this average until the last month. So on month 0, 1, 2... you didn't know this average and using it is like using future data.  \nFinally, I think some notebooks fail in creating rows for month when a shop no more sells an item it has sold before. For example, if a shop starts selling an item from month 6 to month 15, then it no more sells this item at month 16, 17 and 18, before resuming the sale at month 19, I should have rows for this item at month 16, 17 and 18 with a sales count of 0 and a price to define. I choose to define this price as the average price of sales of the last month when this item was sold by this shop (in my example, the average price of this item in this shop at month 15). So I fill missing prices forward and missing sales whith a 0 quantity.  \nI have been inspired by some interesting notebooks like :\n- https:\/\/www.kaggle.com\/gordotron85\/future-sales-xgboost-top-3#Modelling ;\n- https:\/\/www.kaggle.com\/dlarionov\/feature-engineering-xgboost ;\n- https:\/\/www.kaggle.com\/kyakovlev\/1st-place-solution-part-1-hands-on-data","b242bf95":"---\n<a id=\"300\"><\/a>","1bc81e38":"There's no duplicate.  \nLooking at the names, it seems the shop Name can be split in 3 parts : City, Shop Type (shopping center, shopping mall, maybe others), Name or Address of the Shop. As there is only 60 shops, I have a closer look at the names.","d6f7f659":"**Average monthly quantity and price by item (for the 98 most sold items)**","8986bd55":"It's a category with a lot of above-average prices. Those prices seem allright. I keep those sales.","85ad5779":"This column needs a deep cleaning. Here is what I notice:  \n- I see 7 types of shops : shopping center, shopping mall, shopping and entertainment center, Sale, Shop, ITRC, Trade Center ;\n- 3 shops without city : Itinerant trade, Online store of emergency, Digital warehouse 1C-Online. 2 are linked to online sales. One is an itinerant shop.\n- City Names to change : SPb and St. Petersburg to Saint-Petersburg, to get full name before the first split.  \n  \nThen, I can create the 2 columns shop_city and shop_type","bfe146ac":"### 1.1.3. Shops","1ba88435":"First, I check if some item_id have to be predicted while others are not to predict.  \nThen, if necessary, I'll check if those pairs of shops sell the same items and the same time.","947829ab":"I just need to check date, date_block_num, item_price and item_cnt_day. Other values are already cleaned."}}