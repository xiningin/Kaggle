{"cell_type":{"33dc68b5":"code","6f1553b3":"code","c3704901":"code","e75128bc":"code","e781b935":"code","13518d79":"code","edec730e":"code","d78c064d":"code","8e154b01":"code","ac120373":"code","68f71672":"code","74dc9bd0":"code","ba3b0c58":"code","17641329":"code","1cf11470":"code","6216726a":"code","b5e36b99":"code","a6adf61a":"code","43a4ec78":"code","e56d7816":"code","d2da59da":"code","4051a979":"markdown","74852532":"markdown","3a037a96":"markdown","4ee29c12":"markdown","30b62aca":"markdown","c0520744":"markdown"},"source":{"33dc68b5":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.decomposition import PCA","6f1553b3":"data = pd.read_csv('..\/input\/divorce-prediction\/divorce_data.csv', delimiter=';')","c3704901":"data","e75128bc":"y = data['Divorce'].copy()\nX = data.drop('Divorce', axis=1).copy()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)","e781b935":"X_train","13518d79":"y_train","edec730e":"model = LogisticRegression()\nmodel.fit(X_train, y_train)\n\nprint(\"Test Accuracy: {:.2f}%\".format(model.score(X_test, y_test) * 100))","d78c064d":"X_train","8e154b01":"n_components = 8\n\npca = PCA(n_components=n_components)\npca.fit(X_train)\n\nX_train_reduced = pd.DataFrame(pca.transform(X_train), index=X_train.index, columns=[\"PC\" + str(i) for i in range(1, n_components + 1)])\nX_test_reduced = pd.DataFrame(pca.transform(X_test), index=X_test.index, columns=[\"PC\" + str(i) for i in range(1, n_components + 1)])","ac120373":"X_train_reduced","68f71672":"plt.figure(figsize=(16, 10))\nsns.barplot(x=pca.explained_variance_ratio_, y=[\"PC\" + str(i) for i in range(1, n_components + 1)], orient='h', palette='husl')\nplt.xlim(0., 1.)\nplt.xlabel(\"Proportion of Variance in Original Data\")\nplt.title(\"Principal Component Variance\")\nplt.show()","74dc9bd0":"reduced_model = LogisticRegression()\nreduced_model.fit(X_train_reduced, y_train)\n\nprint(\"Test Accuracy ({} Components): {:.2f}%\".format(n_components, reduced_model.score(X_test_reduced, y_test) * 100))","ba3b0c58":"n_components = 2\n\npca = PCA(n_components=n_components)\npca.fit(X_train)\n\nX_train_reduced = pd.DataFrame(pca.transform(X_train), index=X_train.index, columns=[\"PC\" + str(i) for i in range(1, n_components + 1)])\nX_test_reduced = pd.DataFrame(pca.transform(X_test), index=X_test.index, columns=[\"PC\" + str(i) for i in range(1, n_components + 1)])","17641329":"X_train_reduced","1cf11470":"plt.figure(figsize=(16, 10))\nplt.scatter(X_train_reduced['PC1'], X_train_reduced['PC2'])\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.title(\"Train Set\")\nplt.show()","6216726a":"X_train_reduced.loc[y_train == 0, 'PC2']","b5e36b99":"plt.figure(figsize=(16, 10))\nplt.scatter(X_train_reduced.loc[y_train == 0, 'PC1'], X_train_reduced.loc[y_train == 0, 'PC2'], label=\"Married\", color='blue')\nplt.scatter(X_train_reduced.loc[y_train == 1, 'PC1'], X_train_reduced.loc[y_train == 1, 'PC2'], label=\"Divorced\", color='orange')\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.title(\"Train Set\")\nplt.legend()\nplt.show()","a6adf61a":"plt.figure(figsize=(16, 10))\nplt.scatter(X_test_reduced.loc[y_test == 0, 'PC1'], X_test_reduced.loc[y_test == 0, 'PC2'], label=\"Married\", color='blue')\nplt.scatter(X_test_reduced.loc[y_test == 1, 'PC1'], X_test_reduced.loc[y_test == 1, 'PC2'], label=\"Divorced\", color='orange')\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.title(\"Test Set\")\nplt.legend()\nplt.show()","43a4ec78":"reduced_model = LogisticRegression()\nreduced_model.fit(X_train_reduced, y_train)\n\nprint(\"Test Accuracy ({} Components): {:.2f}%\".format(n_components, reduced_model.score(X_test_reduced, y_test) * 100))","e56d7816":"misclassifications = X_test_reduced.loc[reduced_model.predict(X_test_reduced) != y_test, :]\nmisclassifications","d2da59da":"plt.figure(figsize=(16, 10))\nplt.scatter(X_test_reduced.loc[y_test == 0, 'PC1'], X_test_reduced.loc[y_test == 0, 'PC2'], label=\"Married\", color='blue')\nplt.scatter(X_test_reduced.loc[y_test == 1, 'PC1'], X_test_reduced.loc[y_test == 1, 'PC2'], label=\"Divorced\", color='orange')\nplt.scatter(misclassifications['PC1'], misclassifications['PC2'], label=\"Misclassified\", color='cyan')\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.title(\"Misclassified Examples in the Test Set\")\nplt.legend()\nplt.show()","4051a979":"# Training a Model","74852532":"# Task for Today  \n\n***\n\n## Divorce Prediction  \n\nGiven *survey data from couples in Turkey*, let's try to predict if a given couple is **divorced**.\n\nWe will use a logistic regression model to make our predictions.  \n  \nWe will use principal component analysis to reduce the dimension of the data and show that the same results can be achieved with a smaller number of features, as well as to visualize the data.","3a037a96":"# Using PCA for Visualization","4ee29c12":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps:\/\/youtu.be\/JVMtedgNywo","30b62aca":"# Using PCA for Dimensionality Reduction","c0520744":"# Getting Started"}}