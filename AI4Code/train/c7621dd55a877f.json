{"cell_type":{"fcd73809":"code","610a1c89":"code","102c6dc8":"code","8a634e42":"code","c54cc7ad":"code","fb489313":"code","b43422af":"code","d9a78a8c":"code","25bfb865":"code","42691b8e":"code","addf4409":"code","6c8ba69e":"code","03d89b34":"code","410f8b17":"code","377bc189":"code","d4ad0ef1":"code","492cf785":"code","708b31a7":"code","9661cfec":"code","c7a62deb":"code","8bf4f265":"code","3e2d302a":"code","f04eb510":"code","9c108218":"code","1843e521":"code","f8f02970":"code","4bdbbb09":"markdown","6ed20257":"markdown","47b6e886":"markdown"},"source":{"fcd73809":"#Required Libraries\nimport gym\nimport numpy as np\nimport random as rm\nimport matplotlib.pyplot as plt","610a1c89":"# Custom Map\n# custom_map = [\n#     'SFFFHFHFHF', \n#     'FFHFHFFHFH',\n#     'HFFFHHFFHF',\n#     'HFHFFHHFFG', \n# ]\n\nenv = gym.make('FrozenLake-v0')\nenv.reset()\nenv.render()\n ","102c6dc8":"env.action_space.sample()","8a634e42":"env.observation_space.n\n","c54cc7ad":"env.action_space.n","fb489313":"env.P[1][0]\n# trans_prob, next_state, reward, if terminal node","b43422af":"def value_iteration(env, gamma):\n    \n    # Initialize value table with Zeros\n    value_table = np.zeros(env.observation_space.n)\n    \n    # Set number of iterations and threshold\n    max_iters = 10000\n    threshold = 1e-10\n    changed_value_table = []\n    for i in range(max_iters):\n        \n        # copy the value table to the updated_value_table\n        updated_value_table = np.copy(value_table)\n        \n        # Now we calculate Q value for each actions in the state, and update the value of a state with maximum Q value\n        \n        for state in range(env.observation_space.n):\n            Q_value = []\n            \n            for action in range(env.action_space.n):\n                next_state_rewards = []\n                \n                for next_sr in env.P[state][action]:\n                    trans_prob, next_state, reward, _ = next_sr\n                    next_state_rewards.append((trans_prob * (reward + gamma * updated_value_table[next_state])))\n                \n                Q_value.append(np.sum(next_state_rewards))\n                \n            value_table[state] = max(Q_value)\n        \n        changed_value_table.append(np.sum(np.fabs(updated_value_table - value_table)))\n        # Convergance condition\n        if (np.sum(np.fabs(updated_value_table - value_table)) <= threshold):\n            print('Value iteration converged at iteration #', (i + 1))\n            break\n        \n    return value_table, changed_value_table\n            ","d9a78a8c":"a = np.array([1, 2, 3])\nb = np.array([1.2, 2,2])\nnp.sum(np.fabs(a - b))","25bfb865":"value_tbl, changed_value_tbl = value_iteration(env, 0.92)\nvalue_tbl","42691b8e":"plt.plot(changed_value_tbl)","addf4409":"value_tbl = value_iteration(env, 0.9)\nvalue_tbl","6c8ba69e":"def extract_policy(value_table, gamma):\n \n    #Initialize the policy with zeros\n    policy = np.zeros(env.observation_space.n) \n    \n    \n    for state in range(env.observation_space.n):\n        \n        #Initialize the Q table for a state\n        Q_table = np.zeros(env.action_space.n)\n        \n        #Compute Q value for all ations in the state\n        for action in range(env.action_space.n):\n            for next_sr in env.P[state][action]: \n                trans_prob, next_state, reward, _ = next_sr \n                Q_table[action] += (trans_prob * (reward + gamma * value_table[next_state]))\n        \n        #Select the action which has maximum Q value as an optimal action of the state\n        policy[state] = np.argmax(Q_table)\n    \n    return policy","03d89b34":"optimal_value_function, _ = value_iteration(env = env, gamma = 0.92)","410f8b17":"optimal_value_function","377bc189":"optimal_policy = extract_policy(optimal_value_function, gamma = 0.92)","d4ad0ef1":"print(optimal_policy)","492cf785":"#Run a Game with Optimal Policy\n\nstate = env.reset()\ndone = False\ntot_reward = 0\nwhile not done: \n    #Select the action accordingly to the policy\n    next_state, reward, done, _ = env.step(optimal_policy[state])\n    state = next_state\n    tot_reward += reward \n    if done:\n        break\ntot_reward","708b31a7":"#Run so many episodes\ndef run_episodes(env, policy, num_games = 1000):\n    \n    tot_rew = 0\n    state = env.reset()\n\n    for _ in range(num_games):\n        done = False\n        \n        while not done:\n            #Select the action accordingly to the policy\n            next_state, reward, done, _ = env.step(policy[state])\n                \n            state = next_state\n            tot_rew += reward \n            if done:\n                state = env.reset()\n\n    print('Won {} of {} games!'.format(tot_rew, num_games))","9661cfec":"run_episodes(env, optimal_policy, num_games = 1000)","c7a62deb":"env = gym.make(\"FrozenLake-v0\")\nenv.reset()\noptimal_value_function = value_iteration(env = env, gamma = 0.99)","8bf4f265":"optimal_policy = extract_policy(optimal_value_function, gamma = 0.99)","3e2d302a":"run_episodes(env, optimal_policy, num_games = 1000)","f04eb510":"# Custom Map\ncustom_map = [\n    'SFFFH', \n    'FFHFF',\n    'HFFFH',\n    'HFFFH',\n    'HHHFG'\n]\n\nenv = gym.make('FrozenLake-v0', desc = custom_map)\nenv.reset()\nenv.render()","9c108218":"optimal_value_function = value_iteration(env = env, gamma = 0.9)\noptimal_value_function","1843e521":"optimal_policy = extract_policy(optimal_value_function, gamma = 0.9)\noptimal_policy","f8f02970":"run_episodes(env, optimal_policy, num_games = 1000)","4bdbbb09":"# Try another env","6ed20257":"##### 0: left\n##### 1: down\n##### 2: right\n##### 3: up","47b6e886":"## value iteration"}}