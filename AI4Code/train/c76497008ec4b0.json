{"cell_type":{"401f5bf1":"code","f4a08146":"code","e1aa5f4e":"code","04620786":"code","46afa6d3":"code","5304bf81":"code","19e0ead3":"code","8c557191":"code","886630c6":"code","6b964aec":"code","2c5e6780":"code","1ddcce03":"code","9b562793":"code","d3b4c0cc":"code","b7823b9e":"code","502e7996":"code","c662b726":"code","14ca5e34":"code","210c4b17":"code","0dca37b3":"code","d616c21f":"code","08b32127":"code","aa64a54b":"code","d51dc7f8":"code","9fdc01f9":"code","4ba9be36":"code","b4a1d65c":"code","823717cf":"markdown","44e36858":"markdown","96c9a813":"markdown","73bb2c3b":"markdown","dbd4f4bb":"markdown","9a4cf62d":"markdown"},"source":{"401f5bf1":"# imports\nimport numpy as np\nimport pandas as pd\nimport geopandas as geopd\nimport os\nimport folium\nimport collections\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")","f4a08146":"# initializations\nshapefile_set = {'shapefile_data': {}, 'shapefile_crs': {}, 'shapefile_geom_type': {}}\nACS_set = {}\ncensus_shapefile_set = {}\nintersection_set = {}\ndistricts_set = {}\ncombined_set = {}\n\nroot_dir = \"..\/input\/data-and-shape\/kaggledata - copy\/kaggleData - Copy\"","e1aa5f4e":"# Get list of directories\ndepartments = [name for name in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, name))]\ndepartments.remove(\"census-tract-shapefile\")\nprint(\"Found departments: \", departments, \"\\n\")","04620786":"# Read in shapefile data for each department using geopandas\nfor department in departments:\n    sub_directories = os.listdir(os.path.join(root_dir, department))\n    shapefile_directory  = ''.join([f for f in sub_directories if \"Shapefiles\" in f])\n    shapefiles = os.listdir(os.path.join(root_dir, department, shapefile_directory))\n    shapefiles = [f for f in shapefiles if f.endswith('.shp')]\n    if len(shapefiles) > 1:\n        shapefiles = shapefiles[0]\n    shapefile = ''.join(shapefiles)\n    data_file = os.path.join(root_dir, department, shapefile_directory, shapefile)\n    \n    data = geopd.read_file(data_file)\n    shapefile_set['shapefile_data'][shapefile] = data\n    shapefile_set['shapefile_crs'][shapefile] = data.crs\n    shapefile_set['shapefile_geom_type'][shapefile] = data.geom_type.unique().tolist()\n                \n#GET SHAPEFILES THAT ARE POINTS BC CANNOT DO INTERSECTION AND AREA CALCS WITH POINTS!\npoint_shapefiles = [s for s,g in shapefile_set['shapefile_geom_type'].items() if 'Point' in g or 'Line' in g]\n#GET SHAPEFILES WITH NO COORDINATE SYSTEM BC CANNOT DO INTERSECT WITHOUT CRS!\nCRS_missing_shapefiles = [s for s,g in shapefile_set['shapefile_crs'].items() if not g]\n\n#PRINT DATA FINDINGS\nprint(\"Found shapefiles: %s\" % (tuple(shapefile_set['shapefile_data'].keys()),), '\\n')\n#SHOW WARNINGS FOR SHAPEFILES THAT WILL NOT BE PROCESSED\nif point_shapefiles:\n    print(\"WARNING: Shapefiles with incorrect geometry found. Cannot porcess shapefiles: %s\" % (point_shapefiles,))\nif CRS_missing_shapefiles:\n    print(\"WARNING: Shapefiles with missing CRS found. Cannot porcess shapefiles: %s\" % (CRS_missing_shapefiles,))","46afa6d3":"# Read in ACS data for each department using pandas\nfor department in departments:\n    sub_directories = os.listdir(os.path.join(root_dir, department))\n    ACS_directory  = ''.join([f for f in sub_directories if \"ACS\" in f])\n    ACS_sub_directories = os.listdir(os.path.join(root_dir, department, ACS_directory))\n    ACS_set[ACS_directory] = {}\n    for ACS_sub_directory in ACS_sub_directories:\n        if os.path.isdir(os.path.join(root_dir, department, ACS_directory, ACS_sub_directory)) == True:\n            ACS_data_type = ACS_sub_directory.split('_')[-1]\n            for ACS_file in os.listdir(os.path.join(root_dir, department, ACS_directory, ACS_sub_directory)):\n                if 'metadata' not in ACS_file:\n                    data_file = os.path.join(root_dir, department, ACS_directory, ACS_sub_directory, ACS_file)\n                    data = pd.read_csv(data_file)\n                    if data.empty:\n                        pass\n                    else:\n                        data.columns = data.iloc[0,:]\n                        data=data.iloc[1:,:]\n                    ACS_set[ACS_directory][ACS_data_type] = data\n                        \n# Print findings\nprint (\"Found ACS data: \", ACS_set.keys(), \"\\n\")\nprint (\"Found types for boston: \", ACS_set['11-00091_ACS_data'].keys(), \"\\n\")","5304bf81":"# dict to correspond city to census key\ncity_to_census_state = {'boston': '25',\n                        'indianapolis': '18',\n                        'minneapolis': '27',\n                        'st. paul': '27',\n                        'orlando': '12',\n                        'charlotte': '37',\n                        'austin': '48',\n                        'dallas': '48',\n                        'seattle': '53',\n                        'los angeles': '06',\n                        'oakland': '06',\n                        'san francisco': '06'}\n\n# Read in census tract data\ncensus_dir = 'census-tract-shapefile'\ncensus_directories = os.listdir(os.path.join(root_dir, census_dir))\nfor c_sub_directory in census_directories:\n    files = os.listdir(os.path.join(root_dir, census_dir, c_sub_directory))\n    for file in [f for f in files if f.endswith('.shp')]:\n        state_ID = file.split('_')[2]\n        shapefile = os.path.join(root_dir, census_dir, c_sub_directory, file)\n        data = geopd.read_file(shapefile)\n        census_shapefile_set[state_ID] = data\n        \nprint(\"census shapefiles loaded for: \", census_shapefile_set.keys())\nprint(\"Census example data:\")\nprint(census_shapefile_set[city_to_census_state['indianapolis']].iloc[0])","19e0ead3":"# dict relating city to its shapefile\ncity_to_shape = {'boston':'boston_police_districts_f55.shp',\n                'indianapolis':'Indianapolis_Police_Zones.shp',\n                'minneapolis':'Minneapolis_Police_Precincts.shp',\n                'st. paul':'StPaul_geo_export_6646246d-0f26-48c5-a924-f5a99bb51c47.shp',\n                'orlando':'OrlandoPoliceDistricts.shp',\n                'charlotte':'CMPD_Police_Division_Offices.shp',\n                'austin':'APD_DIST.shp',\n                'dallas':'EPIC.shp',\n                'seattle':'SPD_BEATS_WGS84.shp',\n                'los angeles':'lapd division.shp',\n                'oakland':'Oakland_geo_export_e0633584-8d50-406e-b9d6-e33545fdae16.shp',\n                'san francisco':'SFPD_geo_export_8a7e3c3b-8d43-4359-bebe-b7ef6a6e21f3.shp'}\n\n# dict relating city to it's CRS information\ncity_utm_crs = {'boston': None,\n                'indianapolis': None,\n                'minneapolis': None,\n                'st. paul': None,\n                'orlando': None,\n                'charlotte': {'init': 'epsg:4326'},\n                'austin': {'init': 'epsg:102739'},\n                'dallas': {'init': 'epsg:102738'},\n                'seattle': {'init': 'epsg:32610'},\n                'los angeles': None,\n                'oakland': None,\n                'san francisco': None}","8c557191":"# square meters to square feet\nsqm_to_sqft = 10.7639\n\n# plot figures or not\nplotting = False\nloc = -1\n\n# iterate through cities and compute intersect for each police district\nfor city in city_to_shape.keys():\n    loc += 1\n    try:\n        # update CRS if necessary\n        crs = city_utm_crs[city]\n        if crs is not None:\n            districts = shapefile_set['shapefile_data'][city_to_shape[city]].to_crs(crs=crs)\n            tracts = census_shapefile_set[city_to_census_state[city]].to_crs(crs=crs)\n            print (\"updated: \", city)\n        else:\n            districts = shapefile_set['shapefile_data'][city_to_shape[city]]\n            tracts = census_shapefile_set[city_to_census_state[city]]\n                \n        # compute areas\n        tracts['area_sqm_tract'] = tracts.geometry.area\n        tracts['area_sqft_tract'] = tracts.geometry.area*sqm_to_sqft\n\n        # compute intersection between district and census tracts\n        # this line below is where the code breaks in kaggle kernel\n        intersection = geopd.overlay(districts, tracts, how='intersection')\n        print (\"computed intersects for\", city)\n        \n        # plotting\n        if plotting:\n            fig,ax = plt.subplots(figsize=(12,8))\n            ax.set_aspect('equal')\n            fig.suptitle(str(city) + ' District-Tracts Intersect', fontsize=20)\n            intersection.plot(ax=ax, color='lightblue', edgecolor='red', alpha=0.7)\n            districts.plot(ax=ax, color='none', facecolor='none', edgecolor='black')\n            plt.savefig('figs\\\\'+str(city)+' District-Tracts Intersection.png')\n            plt.show()\n        \n    except:\n        print (\"error computing intersects for\", city)\n        continue\n        \n    # compute areas after intersection\n    intersection['area_sqm_tract_inter'] = intersection.geometry.area\n    intersection['area_sqft_tract_inter'] = intersection.geometry.area*sqm_to_sqft\n    \n    # compute percentage of tract in police district\n    intersection['tract_perc_in_div'] =  intersection['area_sqm_tract_inter']\/intersection['area_sqm_tract'] \n    \n    # merge in ACS data based on GEOID\n    for ii in ACS_set[list(ACS_set.keys())[loc]].keys():\n        ACS_set[list(ACS_set.keys())[loc]][ii]['GEOID'] = ACS_set[list(ACS_set.keys())[loc]][ii]['Id2']\n        intersection = intersection.merge(ACS_set[list(ACS_set.keys())[loc]][ii], on='GEOID')\n        \n    print (intersection.shape)\n    intersection.to_csv(\"intersection-files\\\\raw-intersection\\\\intersection for \"+str(city)+\".csv\")\n    intersection_set[city] = intersection","886630c6":"from IPython.display import Image\npath_to_figs = \"..\/input\/cpe-outputs-2\/outputs\/figs\"\nImage(os.path.join(path_to_figs, \"indianapolis District-Tracts Intersection.png\"))","6b964aec":"Image(os.path.join(path_to_figs, \"minneapolis District-Tracts Intersection.png\"))","2c5e6780":"Image(os.path.join(path_to_figs, \"st. paul District-Tracts Intersection.png\"))","1ddcce03":"Image(os.path.join(path_to_figs, \"seattle District-Tracts Intersection.png\"))","9b562793":"Image(os.path.join(path_to_figs, \"los angeles District-Tracts Intersection.png\"))","d3b4c0cc":"Image(os.path.join(path_to_figs, \"oakland District-Tracts Intersection.png\"))","b7823b9e":"Image(os.path.join(path_to_figs, \"san francisco District-Tracts Intersection.png\"))","502e7996":"# list of column names of ACS data to use\ncolumn_names = [\"GEOID\", \"geometry\", \"tract_perc_in_div\",\n                \"Percent; Estimate; Population 25 years and over - High school graduate (includes equivalency)\",\n                \"Percent; Estimate; Population 25 years and over - Bachelor's degree\",\n#                 \"Total; Estimate; MEDIAN EARNINGS IN THE PAST 12 MONTHS (IN 2016 INFLATION-ADJUSTED DOLLARS) - Population 25 years and over with earnings\",\n#                 \"Males; Estimate; MEDIAN EARNINGS IN THE PAST 12 MONTHS (IN 2016 INFLATION-ADJUSTED DOLLARS) - Population 25 years and over with earnings\",\n#                 \"Females; Estimate; MEDIAN EARNINGS IN THE PAST 12 MONTHS (IN 2016 INFLATION-ADJUSTED DOLLARS) - Population 25 years and over with earnings\",\n                \"Employment\/Population Ratio; Estimate; Population 16 years and over\",\n                \"Unemployment rate; Estimate; Population 16 years and over\",\n                \"Median income (dollars); Estimate; Households\",\n                \"Median income (dollars); Estimate; Households - One race-- - White\",\n                \"Median income (dollars); Estimate; Households - One race-- - Black or African American\",\n                \"Median income (dollars); Estimate; Households - One race-- - Asian\",\n                \"Median income (dollars); Estimate; Households - One race-- - Some other race\",\n                \"Occupied housing units; Estimate; Occupied housing units\",\n                \"Owner-occupied housing units; Estimate; Occupied housing units\",\n                \"Renter-occupied housing units; Estimate; Occupied housing units\",\n                \"Percent below poverty level; Estimate; Population for whom poverty status is determined\",\n                \"Estimate; SEX AND AGE - Total population\",\n                \"Percent; SEX AND AGE - Total population - Male\",\n                \"Percent; SEX AND AGE - Total population - Female\",\n                \"Estimate; SEX AND AGE - Median age (years)\",\n                \"Percent; RACE - One race - White\",\n                \"Percent; RACE - One race - Black or African American\",\n                \"Percent; RACE - One race - Asian\",\n                \"Percent; RACE - One race - Some other race\"]","c662b726":"# district dictionary to map cities to identifying column\ndistrict_identifiers = {'indianapolis':'DISTRICT', \n                        'minneapolis':'PRECINCT', \n                        'los angeles':'external_i', \n                        'oakland':'pol_dist',\n                        'san francisco':'district',\n                        'seattle':'beat'}\n\n# iterate over cities \nfor city in intersection_set.keys():\n    \n    # ignore st. paul b\/c it only has vehicle data which we dont really care about\n    if city != \"st. paul\":\n        \n        # retrieve city data from set\n        intersection_data = intersection_set[city]\n        \n        # insert identifier into column list but remove it after\n        column_names.insert(0, district_identifiers[city])\n        intersection_data = intersection_data[column_names]\n        del column_names[0]\n\n        # make sure all values that we are doing calculations on are floats\n        intersection_data.iloc[:,3:] = intersection_data.iloc[:,3:].apply(pd.to_numeric, errors='coerce')\n        intersection_data.iloc[:,3:] = intersection_data.iloc[:,3:].fillna(0)\n        intersection_data.iloc[:,3:] = intersection_data.iloc[:,3:].astype(float)\n\n        # multiply all census columns by district ratios\n        intersection_data.iloc[:,4:] = intersection_data.iloc[:,4:].multiply(intersection_data.iloc[:,3], axis='index')\n\n        # dissolve intersection data to aggregate information\n        districts_aggregated = intersection_data.dissolve(by=district_identifiers[city], aggfunc='sum')\n        \n        # clean up data, write to csv, and store in dict\n        districts_printable = districts_aggregated.drop(labels=['geometry', 'tract_perc_in_div'], axis=1)\n        districts_printable.to_csv(\"intersection-files\\\\dissolved-intersection\\\\agg for \"+str(city)+\".csv\")\n        districts_set[city] = districts_printable\n        print (\"finished processing: \", str(city))","14ca5e34":"# import crime files\n\nBos = pd.read_csv(os.path.join(root_dir, \"Dept_11-00091\/\/11-00091_Field-Interviews_2011-2015.csv\"))\nBos = Bos.drop(Bos.index[0])\n\nInd = pd.read_csv(os.path.join(root_dir, \"Dept_23-00089\/\/23-00089_UOF-P.csv\"))\nInd = Ind.drop(Ind.index[0])\n\nMinn = pd.read_csv(os.path.join(root_dir, \"Dept_24-00013\/\/24-00013_UOF_2008-2017_prepped.csv\"))\nMinn = Minn.drop(Minn.index[0])\n\nSea = pd.read_csv(os.path.join(root_dir, \"Dept_49-00009\/\/49-0009_UOF.csv\"))\nSea = Sea.drop(Sea.index[0])\n\nLa = pd.read_csv(os.path.join(root_dir, \"Dept_49-00033\/\/49-00033_Arrests_2015.csv\"))\nLa = La.drop(La.index[0])\n\nOak = pd.read_csv(os.path.join(root_dir, \"Dept_49-00035\/\/49-00035_Incidents_2016.csv\"))\nOak = Oak.drop(Oak.index[0])\n\nSf = pd.read_csv(os.path.join(root_dir, \"Dept_49-00081\/\/49-00081_Incident-Reports_2012_to_May_2015.csv\"))\nSf = Sf.drop(Sf.index[0])","210c4b17":"# Seattle\n\n# convert M\/D\/Y to date type then only the year\nSea['INCIDENT_DATE'] = pd.to_datetime(Sea['INCIDENT_DATE'])\nSea['INCIDENT_DATE'] = Sea['INCIDENT_DATE'].dt.year\n\n# copy the raw data into the new data frame, change the header titles, one hot encode Use of force, race, and gender\nSeattle = Sea[['INCIDENT_DATE', 'TYPE_OF_FORCE_USED', 'LOCATION_DISTRICT.2', 'SUBJECT_RACE', 'SUBJECT_GENDER']].copy()\nSeattle.columns = 'Date', 'UOF', 'District', 'Race', 'Gender'\nSeattle = pd.get_dummies(data=Seattle, columns=['UOF', 'Race', 'Gender'])\n\n# group by district and year and sum over the labels, see total count per district and year\nSeaGroup = Seattle.groupby(['District', 'Date']).sum()\n\n# change header titles, add empty columns for data that other cities have, sort the columns in consistent order\nSeaGroup.columns = 'UOF_Use of Physical Force', 'UOF_Use of Non-Lethal Weapon', 'UOF_Use of Lethal Weapon', 'UOF_Fatal Use of Weapon', 'Race_Native American', 'Race_Asian', 'Race_Black', 'Race_Latinx', 'Race_Pacific Islander','Race_Not Listed','Race_White', 'Gender_Female', 'Gender_Male', 'Gender_Not Listed'\nSeaGroup = SeaGroup.assign(**{'City':'Seattle','Race_Bi-Racial': np.nan, 'Crime_Violent': np.nan, 'Crime_Non-Violent':np.nan, 'Crime_Non-Criminal':np.nan})\nSeaGroup = SeaGroup.reset_index(level=['District', 'Date'])\nSeaGroup = SeaGroup[['District','City', 'Date', 'UOF_Use of Physical Force', 'UOF_Use of Non-Lethal Weapon', 'UOF_Use of Lethal Weapon', 'UOF_Fatal Use of Weapon', 'Crime_Violent', 'Crime_Non-Violent', 'Crime_Non-Criminal', 'Race_Native American', 'Race_Asian', 'Race_Black', 'Race_Latinx', 'Race_Pacific Islander','Race_Not Listed','Race_White', 'Race_Bi-Racial', 'Gender_Male', 'Gender_Female', 'Gender_Not Listed']].copy()\n\n# find the mean of the total count of labels over year\nSeaGroup = SeaGroup.drop('Date', axis=1)\nSeaGroup = SeaGroup.groupby(['District']).mean()\nSeaGroup = SeaGroup.assign(**{'City':'Seattle'})","0dca37b3":"# Indianapolis\n\n# convert M\/D\/Y to date type then only the year\nInd['INCIDENT_DATE'] = pd.to_datetime(Ind['INCIDENT_DATE'])\nInd['INCIDENT_DATE'] = Ind['INCIDENT_DATE'].dt.year\n\n# copy the raw data into the new data frame, change the header titles\nIndianapolis = Ind[['INCIDENT_DATE', 'LOCATION_DISTRICT', 'TYPE_OF_FORCE_USED', 'SUBJECT_RACT', 'SUBJECT_GENDER']].copy()\nIndianapolis.columns = 'Date', 'District', 'UOF', 'Race', 'Gender'\n\n# manually group use of force types into the 3 categories, replace the data points with the category labels\nIndianapolis['UOF'] = Indianapolis['UOF'].replace(['Less Lethal-Pepperball','Less Lethal-Personal CS\/OC spray','Less Lethal-Taser','Canine Bite','Less Lethal-Bean Bag', 'Less Lethal-Baton', 'Less Lethal-CS\/OC', 'Less Lethal-BPS Gas', 'Less Lethal-Clearout OC', 'Less Lethal-CS Fogger', 'Less Lethal-Flash Bang', 'Less Lethal-Bps Gas', 'Less Lethal-CS Grenade'], 'Use of Non-Lethal Weapon')\nIndianapolis['UOF'] = Indianapolis['UOF'].replace(['Less Lethal-Burning CS','Less Lethal-Other','Physical-Hands, Fist, Feet','Physical-Handcuffing', 'Physical-Weight Leverage', 'Physical-Joint\/Pressure', 'Physical-Take Down', 'Physical-Leg Sweep', 'Physical-Kick', 'Physical-Palm Strike', 'Physical-Knee Strike','Physical-Fist Strike', 'Physical-Elbow Strike', 'Physical-Push','Physical-Other' ], 'Use of Physical Force')\nIndianapolis['UOF'] = Indianapolis['UOF'].replace(['Lethal-Vehicle','Lethal-Handgun'], 'Use of Lethal Weapon')\n# replace race datapoint with common name\nIndianapolis['Race'] = Indianapolis['Race'].replace('Other', 'Not Listed')\n\n# one hot encode use of force, race, and gender\nIndianapolis = pd.get_dummies(data=Indianapolis, columns=['UOF', 'Race', 'Gender'])\n\n# group by year and district\nIndGroup = Indianapolis.groupby(['District', 'Date']).sum()\n\n# change header titles, add empty columns for data that other cities have, sort the columns in consistent order\nIndGroup.columns = 'UOF_Use of Lethal Weapon', 'UOF_Use of Non-Lethal Weapon', 'UOF_Use of Physical Force', 'Race_Asian', 'Race_Bi-Racial', 'Race_Black', 'Race_Latinx', 'Race_Native American', 'Race_Other', 'Race_Pacific Islander', 'Race_Not Listed', 'Race_White', 'Gender_Female','Gender_Male', 'Gender_Not Listed'\nIndGroup = IndGroup.assign(**{'City':'Indianapolis','Race_Bi-Racial': np.nan, 'Crime_Violent': np.nan, 'Crime_Non-Violent':np.nan, 'Crime_Non-Criminal':np.nan, 'UOF_Fatal Use of Weapon':np.nan})\nIndGroup = IndGroup.reset_index(level=['District', 'Date'])\nIndGroup = IndGroup[['District','City', 'Date', 'UOF_Use of Physical Force', 'UOF_Use of Non-Lethal Weapon', 'UOF_Use of Lethal Weapon', 'UOF_Fatal Use of Weapon', 'Crime_Violent', 'Crime_Non-Violent', 'Crime_Non-Criminal', 'Race_Native American', 'Race_Asian', 'Race_Black', 'Race_Latinx', 'Race_Pacific Islander','Race_Not Listed','Race_White', 'Race_Bi-Racial', 'Gender_Male', 'Gender_Female', 'Gender_Not Listed']].copy()\n\n# find the mean of the total count of labels over year\nIndGroup = IndGroup.drop('Date', axis=1)\nIndGroup = IndGroup.groupby(['District']).mean()\nIndGroup = IndGroup.assign(**{'City':'Indianapolis'})","d616c21f":"# Minneapolis\n\n# convert M\/D\/Y to date type then only the year\nMinn['INCIDENT_DATE'] = pd.to_datetime(Minn['INCIDENT_DATE'])\nMinn['INCIDENT_DATE'] = Minn['INCIDENT_DATE'].dt.year\n\n# copy the raw data into the new data frame, change the header titles\n# make sure column has same type\nMinneapolis = Minn[['INCIDENT_DATE', 'LOCATION_DISTRICT', 'TYPE_OF_FORCE_USED', 'SUBJECT_RACE', 'SUBJECT_GENDER']].copy()\nMinneapolis.columns = 'Date', 'District', 'UOF', 'Race', 'Gender'\nMinneapolis['Race'] = Minneapolis['Race'].astype(object)\n\n# manually group use of force types into the 3 categories, replace the data points with the category labels\nMinneapolis['UOF'] = Minneapolis['UOF'].replace(['BodilyForceType', 'ForceGeneral'], 'Use of Physical Force')\nMinneapolis['UOF'] = Minneapolis['UOF'].replace(['TaserDeployed', 'ChemIrritant', 'ImprovisedWeaponType', 'K9Lead', 'BatonForce', 'ProjectileType'], 'Use of Non-Lethal Weapon')\nMinneapolis['UOF'] = Minneapolis['UOF'].replace('FirearmType', 'Use of Lethal Weapon')\n\n# make data points for race and gender match for encoding\nMinneapolis['Race'] = Minneapolis['Race'].replace('not recorded', 'Unknown')\nMinneapolis['Gender'] = Minneapolis['Gender'].replace('not recorded', 'Unknown')\n\n# one hot encode use of force, race, and gender\nMinneapolis = pd.get_dummies(data=Minneapolis, columns=['UOF', 'Race', 'Gender'])\n\n# group by district and year\nMinnGroup = Minneapolis.groupby(['District', 'Date']).sum()\n\n# change header titles, add empty columns for data that other cities have, sort the columns in consistent order\nMinnGroup.columns = 'UOF_Use of Lethal Weapon', 'UOF_Use of Non-Lethal Weapon','UOF_Use of Physical Force', 'Race_Asian', 'Race_Black', 'Race_Native American', 'Race_Bi-Racial', 'Race_Pacific Islander', 'Race_Not Listed', 'Race_White', 'Gender_Female', 'Gender_Male', 'Gender_Not Listed'\nMinnGroup = MinnGroup.assign(**{'City':'Minneapolis','Race_Bi-Racial': np.nan, 'Crime_Violent': np.nan, 'Crime_Non-Violent':np.nan, 'Crime_Non-Criminal':np.nan, 'UOF_Fatal Use of Weapon':np.nan, 'Race_Latinx':np.nan})\nMinnGroup = MinnGroup.reset_index(level=['District', 'Date'])\nMinnGroup = MinnGroup[['District','City', 'Date', 'UOF_Use of Physical Force', 'UOF_Use of Non-Lethal Weapon', 'UOF_Use of Lethal Weapon', 'UOF_Fatal Use of Weapon', 'Crime_Violent', 'Crime_Non-Violent', 'Crime_Non-Criminal', 'Race_Native American', 'Race_Asian', 'Race_Black', 'Race_Latinx', 'Race_Pacific Islander','Race_Not Listed','Race_White', 'Race_Bi-Racial', 'Gender_Male', 'Gender_Female', 'Gender_Not Listed']].copy()\n\n# find the mean of the total count of labels over year\nMinnGroup = MinnGroup.drop('Date', axis=1)\nMinnGroup = MinnGroup.groupby(['District']).mean()\nMinnGroup = MinnGroup.assign(**{'City':'Minneapolis'})","08b32127":"# San Francisco\n\n# convert M\/D\/Y to date type then only the year\nSf['INCIDENT_DATE'] = pd.to_datetime(Sf['INCIDENT_DATE'])\nSf['INCIDENT_DATE'] = Sf['INCIDENT_DATE'].dt.year\n\n# copy the raw data into the new data frame, change the header titles\nSanFran = Sf[['INCIDENT_DATE', 'LOCATION_DISTRICT', 'INCIDENT_REASON']].copy()\nSanFran.columns = 'Date', 'District', 'Crime'\n\n# manually group specific crimes into the 3 crime categories\nSanFran['Crime'] = SanFran['Crime'].replace(['SUSPICIOUS OCC', 'OTHER OFFENSES','LARCENY\/THEFT','DRUG\/NARCOTIC','VANDALISM', 'BURGLARY', 'VEHICLE THEFT','FRAUD', 'DRIVING UNDER THE INFLUENCE', 'WARRANTS', 'TRESPASS', 'EXTORTION', 'EMBEZZLEMENT', 'STOLEN PROPERTY', 'SECONDARY CODES', 'WEAPON LAWS','DISORDERLY CONDUCT', 'WEAPON LAWS', 'DISORDERLY CONDUCT', 'RUNAWAY', 'PROSTITUTION', 'FORGERY\/COUNTERFEITING', 'BAD CHECKS', 'DRUNKENNESS', 'LIQUOR LAWS', 'BRIBERY', 'LOITERING', 'FAMILY OFFENSES', 'GAMBLING', 'PORNOGRAPHY\/OBSCENE MAT', 'TREA'], 'Non-Violent')\nSanFran['Crime'] = SanFran['Crime'].replace(['ASSAULT','SEX OFFENSES, FORCIBLE', 'ROBBERY', 'KIDNAPPING', 'ARSON','SEX OFFENSES, NON FORCIBLE'], 'Violent')\nSanFran['Crime'] = SanFran['Crime'].replace(['NON-CRIMINAL','SUICIDE', 'MISSING PERSON', 'RECOVERED VEHICLE',  ], 'Non-Criminal')\n\n# one hot encode crime\nSanFran = pd.get_dummies(data=SanFran, columns=['Crime'])\n\n# group by district and year, sum over total occurances of crime category\nSfGroup = SanFran.groupby(['District', 'Date']).sum()\n\n# add empty columns for data that other cities have, sort the columns in consistent order\nSfGroup = SfGroup.assign(**{'City':'San Francisco','Race_Bi-Racial': np.nan, 'UOF_Use of Physical Force': np.nan, 'UOF_Use of Non-Lethal Weapon':np.nan, 'UOF_Use of Lethal Weapon':np.nan, 'UOF_Fatal Use of Weapon':np.nan, 'Race_Native American':np.nan, 'Race_Asian':np.nan, 'Race_Black':np.nan, 'Race_Latinx':np.nan, 'Race_Pacific Islander':np.nan, 'Race_Not Listed':np.nan, 'Race_White':np.nan, 'Gender_Male':np.nan, 'Gender_Female':np.nan, 'Gender_Not Listed':np.nan })\nSfGroup = SfGroup.reset_index(level=['District', 'Date'])\nSfGroup = SfGroup[['District','City', 'Date', 'UOF_Use of Physical Force', 'UOF_Use of Non-Lethal Weapon', 'UOF_Use of Lethal Weapon', 'UOF_Fatal Use of Weapon', 'Crime_Violent', 'Crime_Non-Violent', 'Crime_Non-Criminal', 'Race_Native American', 'Race_Asian', 'Race_Black', 'Race_Latinx', 'Race_Pacific Islander','Race_Not Listed','Race_White', 'Race_Bi-Racial', 'Gender_Male', 'Gender_Female', 'Gender_Not Listed']].copy()\n\n# find the mean of the total count of labels over year\nSfGroup = SfGroup.drop('Date', axis=1)\nSfGroup = SfGroup.groupby(['District']).mean()\nSfGroup = SfGroup.assign(**{'City':'San Francisco'})","aa64a54b":"# Los Angeles\n\n# convert M\/D\/Y to date type then only the year\nLa['INCIDENT_DATE'] = pd.to_datetime(La['INCIDENT_DATE'])\nLa['INCIDENT_DATE'] = La['INCIDENT_DATE'].dt.year\nLa.loc[:, 'INCIDENT_DATE'] = pd.to_numeric(La.loc[:, 'INCIDENT_DATE'])\nLa.loc[:,'LOCATION_DISTRICT'] = pd.to_numeric(La.loc[:,'LOCATION_DISTRICT'])\n\n# copy the raw data into the new data frame, change the header titles\nLosA = La[['INCIDENT_DATE', 'LOCATION_DISTRICT','INCIDENT_REASON', 'SUBJECT_RACE', 'SUBJECT_GENDER']]\nLosA.columns = 'Date', 'District', 'Crime', 'Race', 'Gender'\n\n# replace race single char codes with string names, based on LA data collection practices\nLosA['Race'] = LosA['Race'].replace(['A', 'C', 'D', 'F', 'J', 'K', 'L','V', 'Z'], 'Asian')\nLosA['Race'] = LosA['Race'].replace(['G','P', 'S', 'U'], 'Pacific Islander')\nLosA['Race'] = LosA['Race'].replace('B', 'Black')\nLosA['Race'] = LosA['Race'].replace('H', 'Latinx')\nLosA['Race'] = LosA['Race'].replace(['O', 'X'], 'Not Listed')\nLosA['Race'] = LosA['Race'].replace('I', 'Native American')\nLosA['Race'] = LosA['Race'].replace('W', 'White')\n\n# replace gender codes with string names\nLosA['Gender'] = LosA['Gender'].replace('M', 'Male')\nLosA['Gender'] = LosA['Gender'].replace('F', 'Female')\n\n# manually group specific crimes into the 3 crime categories\nLosA['Crime'] = LosA['Crime'].replace(['Homicide','Rape','Aggravated Assault', 'Sex (except rape\/prst)', 'Against Family\/Child', 'Robbery', 'Other Assaults'], 'Violent')\nLosA['Crime'] = LosA['Crime'].replace(['Federal Offenses','Gambling','Disturbing the Peace','Fraud\/Embezzlement','Pre-Delinquency','Receive Stolen Property','Prostitution\/Allied','Narcotic Drug Laws','Driving Under Influence', 'Narcotic Drug Laws', 'Weapon (carry\/poss)','Moving Traffic Violations', 'Miscellaneous Other Violations','Vehicle Theft','Liquor Laws','Drunkeness','Burglary', 'Disorderly Conduct','Larceny','Forgery\/Counterfeit'], 'Non-Violent')\nLosA['Crime'] = LosA['Crime'].replace(['Non-Criminal Detention'], 'Non-Criminal')\n\n# one hot encode, race, crime, gender\nLosA = pd.get_dummies(data=LosA, columns=['Crime', 'Race', 'Gender'])\n\n# group by district, date, and sum over all labels\nLaGroup = LosA.groupby(['District', 'Date']).sum()\n\n# add empty columns for data that other cities have, sort the columns in consistent order\nLaGroup = LaGroup.assign(**{'City':'San Francisco','UOF_Use of Physical Force':np.nan, 'UOF_Use of Non-Lethal Weapon':np.nan, 'UOF_Fatal Use of Weapon':np.nan, 'UOF_Use of Lethal Weapon':np.nan, 'Race_Bi-Racial':np.nan, 'Gender_Not Listed':np.nan})\nLaGroup = LaGroup.reset_index(level=['District', 'Date'])\nLaGroup = LaGroup[['District','City', 'Date', 'UOF_Use of Physical Force', 'UOF_Use of Non-Lethal Weapon', 'UOF_Use of Lethal Weapon', 'UOF_Fatal Use of Weapon', 'Crime_Violent', 'Crime_Non-Violent', 'Crime_Non-Criminal', 'Race_Native American', 'Race_Asian', 'Race_Black', 'Race_Latinx', 'Race_Pacific Islander','Race_Not Listed','Race_White', 'Race_Bi-Racial', 'Gender_Male', 'Gender_Female', 'Gender_Not Listed']].copy()\n\n# find the mean of the total count of labels per year\nLaGroup = LaGroup.drop('Date', axis=1)\nLaGroup = LaGroup.groupby(['District']).mean()\nLaGroup = LaGroup.assign(**{'City':'Los Angeles'})","d51dc7f8":"# Oakland\n\n# convert M\/D\/Y to date type then only the year\nOak['INCIDENT_DATE'] = pd.to_datetime(Oak['INCIDENT_DATE'])\nOak['INCIDENT_DATE'] = Oak['INCIDENT_DATE'].dt.year\nOak.loc[:, 'INCIDENT_DATE'] = pd.to_numeric(Oak.loc[:, 'INCIDENT_DATE'])\n\n# copy the raw data into the new data frame, change the header titles\nOakland = Oak[['INCIDENT_DATE', 'LOCATION_DISTRICT', 'CRIME_TYPE']]\nOakland.columns = 'Date', 'District', 'Crime'\n\n# manually group specific crimes into the 3 crime categories\nOakland['Crime'] = Oakland['Crime'].replace(['OTHER','BURG - COMMERCIAL','MISDEMEANOR WARRANT','BRANDISHING','INCIDENT TYPE','DRUNKENNESS','FELONY WARRANT','POSSESSION - STOLEN PROPERTY','FORGERY & COUNTERFEITING','DUI','CURFEW & LOITERING','MISCELLANEOUS TRAFFIC CRIME','BURG - OTHER','STOLEN AND RECOVERED VEHICLE','WEAPONS','MISDEMEANOR ASSAULT','PROSTITUTION','FRAUD','PETTY THEFT','BURG - RESIDENTIAL','GRAND THEFT','VANDALISM','BURG - AUTO','STOLEN VEHICLE','NARCOTICS','DISORDERLY CONDUCT'],'Non-Violent')\nOakland['Crime'] = Oakland['Crime'].replace(['CHILD ABUSE','OTHER SEX OFFENSES','HOMICIDE','THREATS','ROBBERY','ARSON','DOMESTIC VIOLENCE','FELONY ASSAULT',],'Violent')\nOakland['Crime'] = Oakland['Crime'].replace(['MISSING','RECOVERED VEHICLE - OAKLAND STOLEN','TOWED VEHICLE','RECOVERED O\/S STOLEN',], 'Non-Criminal')\n\n# manually group specific districts to match the unit provided by the census data\nOakland['District'] = Oakland['District'].replace(['07X', '05Y', '02Y', '02X', '05X', '06X', '03Y', '04X', '03Y','03X', '01X', '4X'], '1')\nOakland['District'] = Oakland['District'].replace(['12Y', '11X', '10X', '10Y', '08X', '09X', '12X', '13X', '13Y', '13Z'],'2')\nOakland['District'] = Oakland['District'].replace(['14X', '14Y', '15X', '19X', '17X', '17Y', '18X', '18Y', '21X', '20X', '21Y', '16X', '16Y', '22X', '22Y'], '3')\nOakland['District'] = Oakland['District'].replace(['26X', '23X', '26Y', '27Y', '27X', '24X', '24Y', '28X', '25X', '25Y'], '4')\nOakland['District'] = Oakland['District'].replace(['31X', '31X', '31Z', '32X', '33X', '34X', '30X', '29X', '30Y', '35X', '32Y', '35Y', '31Y'], '5')\n\n# one hot encode crime\nOakland = pd.get_dummies(data=Oakland, columns=['Crime'])\n\n# group by district, date, and sum over all labels\nOakGroup = Oakland.groupby(['District', 'Date']).sum()\n\n# add empty columns for data that other cities have, sort the columns in consistent order\nOakGroup = OakGroup.assign(**{'City':'Oakland','Race_Bi-Racial': np.nan, 'UOF_Use of Physical Force': np.nan, 'UOF_Use of Non-Lethal Weapon':np.nan, 'UOF_Use of Lethal Weapon':np.nan, 'UOF_Fatal Use of Weapon':np.nan, 'Race_Native American':np.nan, 'Race_Asian':np.nan, 'Race_Black':np.nan, 'Race_Latinx':np.nan, 'Race_Pacific Islander':np.nan, 'Race_Not Listed':np.nan, 'Race_White':np.nan, 'Gender_Male':np.nan, 'Gender_Female':np.nan, 'Gender_Not Listed':np.nan })\nOakGroup = OakGroup.reset_index(level=['District', 'Date'])\nOakGroup = OakGroup[['District', 'City', 'Date', 'UOF_Use of Physical Force', 'UOF_Use of Non-Lethal Weapon', 'UOF_Use of Lethal Weapon', 'UOF_Fatal Use of Weapon', 'Crime_Violent', 'Crime_Non-Violent', 'Crime_Non-Criminal', 'Race_Native American', 'Race_Asian', 'Race_Black', 'Race_Latinx', 'Race_Pacific Islander','Race_Not Listed','Race_White', 'Race_Bi-Racial', 'Gender_Male', 'Gender_Female', 'Gender_Not Listed']].copy()\n\n# find the mean of the total count of labels per year\nOakGroup = OakGroup.drop('Date', axis=1)\nOakGroup = OakGroup.groupby(['District']).mean()\nOakGroup = OakGroup.assign(**{'City':'Oakland'})","9fdc01f9":"# mapping dictionary to link actual city name and data object name\nmapping = {'indianapolis':IndGroup,\n           'minneapolis':MinnGroup,\n           'seattle':SeaGroup,\n           'los angeles':LaGroup,\n           'oakland':OakGroup,\n           'san francisco':SfGroup}\n\n# iterate over each city\nfor city in districts_set.keys():\n    \n    # initialize both dataframes\n    district_data = districts_set[city]\n    crime_data = mapping[city]\n    \n    # catch fringe cases with datatypes\n    if city == \"los angeles\":\n        district_data.index = district_data.index.astype('int')\n    if city == \"indianapolis\":\n        crime_data.index = crime_data.index.map(lambda x: x.replace('District', '').strip())\n\n    # perform the merge\n    combined = district_data.merge(crime_data, how='left', left_index=True, right_index=True)\n    \n    # write the file and save it in a dict\n    combined.to_csv(\"intersection-files\\\\combined\\\\combined for \"+str(city)+\".csv\")\n    combined_set[city] = combined","4ba9be36":"# initialize empty dataframe\ndataset = pd.DataFrame()\n\n# iterate over each city\nfor city in combined_set.keys():\n    \n    # concatenate each combined dataframe\n    dataset = pd.concat([dataset, combined_set[city]])\n    \n# write the file for later clustering\ndataset.to_csv(\"CPE DATASET.csv\")","b4a1d65c":"path_to_output = \"..\/input\/cpe-outputs-2\/outputs\/\"\ndataset = pd.read_csv(os.path.join(path_to_output, \"CPE DATASET.csv\"))\ndataset.head(15)","823717cf":"### This is where the critical error occurs in the code when interacting with the kaggle kernel.  On our local versions, we are able to compute inersections for 7 of the 12 departments. As you can see from the cell below, they all fail to compute in this case.  As a workaround, we have attached the results below this cell.","44e36858":"# Part 2: Crime and Use of Force Preprocessing","96c9a813":"# Final Aggregated Dataset by District","73bb2c3b":"# Welcome!\n\nDevelopers: Joseph Bentivegna, Ariana Freitag, Matthew Grattan  \nAdvisor: Professor Sam Keene  \nInstitution: The Cooper Union for the Advancement of Science and Art  \n\n## Motivation\nThe approach we took to analyzing the provided data is focused primarily around the challenge stated in the problem statement: automating the combination of police data, census-level data, and other socioeconomic factors. Since the ultimate goal of the CPE is to inform police agencies where they can make improvements by identifying areas of racial disparity, our kernel aims to provide a tool to the CPE that **combines police data and census-level data into a comprehensive dataset that is ready for out-of-the-box unsupervised machine learning**.  We believe that applying unsupervised machine learning algorithms to police and census data can prove useful to identify areas of racial disparity because they can capture inter-district discrepancies in quality of service.\n\n## Approach\nOur methodology consists of 3 main parts.  \n\n#### 1. Census Data Preprocessing \nThis first part consists of mapping census-level data to police districts by using the provided department shapefiles.  Since we are going to be overlaying district mappings and census mappings, we also need shapefiles for the census data.  We used [Data](https:\/\/www.census.gov\/geo\/maps-data\/data\/cbf\/cbf_tracts.html) from the US Government Census Tracts to import state-wide census-tract data for all of the necessary states.  For each department we then calculated the *intersection* of each district with the census tract.  This allowed us to get the percentage of geographical area that each census-tract is in each district.  When combined with the ACS data, these percentages enable us to calculate accurate numbers for demographic data in each district, in each department.\n\n#### 2. Crime and Use of Force Prerocessing \nIn order to look at the six cities we selected, the data needed to aggregated with common labels and grouped by the police districts within each city. This step was focused on processing the individual city data, focusing on race, gender, and crime or use of force records. We aggregated the data as average per year, over each district in the city. Crimes were grouped into the labels 'non-violent', 'violent', and 'non-criminal', and Use of Force was grouped into 'Use of Physical Force', 'Use of Non-Lethal Weapon', 'Use of Lethal Weapon', and 'Fatal Use of Weapon'. Gender was grouped by 'Male', 'Female', and 'Not Listed'. The Race label was grouped by 'Native American', 'Asian', 'Black', 'Latinx', 'Pacific Islander', 'White', 'Bi-Racial', and 'Not Listed'. These specific group labels were based on the data recorded by each of the police forces  and  online resources describing what a violent vs non-violent crime, and the different categories of use of force. The output of this section is six independent dataframes, one for each city, with district as the primary identifier and common labels accross all six.\n\n#### 3. Bringing it All Together \nThis final step brings together the outputs from the previous steps.  We essentially perform a left merge on the district level census data and the police data to produce a single dataframe with all of the columns (features) we want for each district. Then, we concatenate all of the individual departments together to create one comprehensive *CPE DATASET*.  This dataset contains approximately 100 observations from districts across the U.S. and 39 unique features that describe aggregates of crime, use of force, and demographic information. \n\n\n## Notes\/Assumptions\/References\n1. Much of the inspiration for the first part of our code was taken from \nCrystal Blankenship's [GeoAnalysis: Map Police Dept. to Census for Equity](https:\/\/www.kaggle.com\/crystalblankenship\/geoanalysis-map-police-dept-to-census-for-equity\/code)\n2. We selected not to use departments where we were unable to read the shapefiles as is. \n3. We assumed that in the census data population is uniformly distributed over the land of each tract.\n4. Census data is aggregared using the addition operator. This choice was made to keep consistency, as aggregating on mean does not make sense in this context. For example, if a tract was only a small percentage of geographical area of a district, the seemingly small median income would greatly skew the data.\n5. There is bias in the grouping of Use of Force and Crime, as threre is no strict definition to what the different types of force and crime are considered. These data points were grouped based on the sources below.  \n[Use of Force Levels](https:\/\/www.nij.gov\/topics\/law-enforcement\/officer-safety\/use-of-force\/Pages\/continuum.aspx)  \n[Violent vs Non-Violent Crimes](https:\/\/www.fightforyourrightsmn.com\/blog\/2016\/12\/the-difference-between-violent-and-non-violent-crimes.shtml)\n6. The race codes for Los Angeles were found in [LA's Open Data](https:\/\/data.lacity.org\/A-Safe-City\/Arrest-Data-from-2010-to-Present\/yru6-6re4).  The different countries located in Asia were aggregated under the 'Asian' label.\n\n## Final Note...\nUnfortunately **this notebook is not going to actually produce anything when run**.  The code worked on our local system when developing in Jupyter Notebook but we encountered an error we were unable to rectify when porting the code to kaggle.  That being said, not all hope is lost. Since the code runs on our local machines we have simply saved the intermediary steps and imported them into this submission so that they can be used by all :)","dbd4f4bb":"# Part 1: Census Data Preprocessing","9a4cf62d":"# Part 3: Bringing it All Together"}}