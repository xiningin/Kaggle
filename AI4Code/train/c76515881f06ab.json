{"cell_type":{"3bb2b938":"code","585a6a1c":"code","be7f5b58":"code","ba3c4fbd":"code","6e12426f":"code","3fdfbeb6":"code","617ae0cd":"code","8d35e0b8":"code","316dbe10":"code","4db63971":"code","200475b0":"code","92483a40":"code","8deefffe":"code","da60bf24":"markdown","37157860":"markdown","a3802339":"markdown","604a5128":"markdown","6a46d667":"markdown","5aa5dffa":"markdown","2994cee4":"markdown"},"source":{"3bb2b938":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom nltk.tokenize import TweetTokenizer\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import metrics\npd.set_option('max_colwidth',400)\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\n\nimport time\nimport os\nprint(os.listdir(\"..\/input\"))\n","585a6a1c":"train = pd.read_csv('..\/input\/jigsaw-public-files\/train.csv')\ntest = pd.read_csv('..\/input\/jigsaw-public-files\/test.csv')\n# after processing some of the texts are emply\ntrain['comment_text'] = train['comment_text'].fillna('')\ntest['comment_text'] = test['comment_text'].fillna('')\nsub = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv')","be7f5b58":"full_text = list(train['comment_text'].values) + list(test['comment_text'].values)\nmax_features = 300000\ntk = Tokenizer(lower = True, filters='', num_words=max_features)\ntk.fit_on_texts(full_text)","ba3c4fbd":"embedding_path1 = \"..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec\"\nembedding_path2 = \"..\/input\/glove840b300dtxt\/glove.840B.300d.txt\"\nembed_size = 300","6e12426f":"def get_coefs(word,*arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef build_matrix(embedding_path, tokenizer):\n    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n\n    word_index = tk.word_index\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.zeros((nb_words + 1, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features:\n            continue\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix\n\n# combining embeddings from this kernel: https:\/\/www.kaggle.com\/tanreinama\/simple-lstm-using-identity-parameters-solution\nembedding_matrix = np.concatenate([build_matrix(embedding_path1, tk), build_matrix(embedding_path2, tk)], axis=-1)","3fdfbeb6":"y = np.where(train['target'] >= 0.5, True, False) * 1","617ae0cd":"identity_columns = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish', 'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\nfor col in identity_columns + ['target']:\n    train[col] = np.where(train[col] >= 0.5, True, False)\n\nn_fold = 5\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=11)","8d35e0b8":"SUBGROUP_AUC = 'subgroup_auc'\nBPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\nBNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n\ndef compute_auc(y_true, y_pred):\n    try:\n        return metrics.roc_auc_score(y_true, y_pred)\n    except ValueError:\n        return np.nan\n\ndef compute_subgroup_auc(df, subgroup, label, oof_name):\n    subgroup_examples = df[df[subgroup]]\n    return compute_auc(subgroup_examples[label], subgroup_examples[oof_name])\n\ndef compute_bpsn_auc(df, subgroup, label, oof_name):\n    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n    return compute_auc(examples[label], examples[oof_name])\n\ndef compute_bnsp_auc(df, subgroup, label, oof_name):\n    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n    subgroup_positive_examples = df[df[subgroup] & df[label]]\n    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n    return compute_auc(examples[label], examples[oof_name])\n\ndef compute_bias_metrics_for_model(dataset,\n                                   subgroups,\n                                   model,\n                                   label_col,\n                                   include_asegs=False):\n    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n    records = []\n    for subgroup in subgroups:\n        record = {\n            'subgroup': subgroup,\n            'subgroup_size': len(dataset[dataset[subgroup]])\n        }\n        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n        records.append(record)\n    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n\ndef calculate_overall_auc(df, oof_name):\n    true_labels = df['target']\n    predicted_labels = df[oof_name]\n    return metrics.roc_auc_score(true_labels, predicted_labels)\n\ndef power_mean(series, p):\n    total = sum(np.power(series, p))\n    return np.power(total \/ len(series), 1 \/ p)\n\ndef get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n    bias_score = np.average([\n        power_mean(bias_df[SUBGROUP_AUC], POWER),\n        power_mean(bias_df[BPSN_AUC], POWER),\n        power_mean(bias_df[BNSP_AUC], POWER)\n    ])\n    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n    ","316dbe10":"# adding attention from this kernel: https:\/\/www.kaggle.com\/christofhenkel\/keras-baseline-lstm-attention-5-fold\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim\n    \n\ndef build_model(X_train, y_train, X_valid, y_valid, max_len, max_features, embed_size, embedding_matrix, lr=0.0, lr_d=0.0, spatial_dr=0.0,\n                dense_units=128, conv_size=128, dr=0.2, patience=3, fold_id=1):\n    file_path = f\"best_model_fold_{fold_id}.hdf5\"\n    check_point = ModelCheckpoint(file_path, monitor=\"val_loss\", verbose=1,save_best_only=True, mode=\"min\")\n    early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=patience)\n    \n    inp = Input(shape = (max_len,))\n    x = Embedding(max_features + 1, embed_size * 2, weights=[embedding_matrix], trainable=False)(inp)\n    x1 = SpatialDropout1D(spatial_dr)(x)\n    att = Attention(max_len)(x1)\n    # from benchmark kernel\n    x = Conv1D(conv_size, 2, activation='relu', padding='same')(x1)\n    x = MaxPooling1D(5, padding='same')(x)\n    x = Conv1D(conv_size, 3, activation='relu', padding='same')(x)\n    x = MaxPooling1D(5, padding='same')(x)\n    x = Flatten()(x)\n    x = concatenate([x, att])\n    \n    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n    x = Dense(1, activation=\"sigmoid\")(x)\n    \n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=[\"accuracy\"])\n    model.fit(X_train, y_train, batch_size=128, epochs=3, validation_data=(X_valid, y_valid), \n                        verbose=2, callbacks=[early_stop, check_point])\n    return model","4db63971":"def train_model(X, X_test, y, tokenizer, max_len):\n    \n    oof = np.zeros((len(X), 1))\n    prediction = np.zeros((len(X_test), 1))\n    scores = []\n    test_tokenized = tokenizer.texts_to_sequences(test['comment_text'])\n    X_test = pad_sequences(test_tokenized, maxlen = max_len)\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n        valid_df = X_valid.copy()    \n\n        train_tokenized = tokenizer.texts_to_sequences(X_train['comment_text'])\n        valid_tokenized = tokenizer.texts_to_sequences(X_valid['comment_text'])\n\n        X_train = pad_sequences(train_tokenized, maxlen = max_len)\n        X_valid = pad_sequences(valid_tokenized, maxlen = max_len)\n        \n        model = build_model(X_train, y_train, X_valid, y_valid, max_len, max_features, embed_size, embedding_matrix,\n                            lr = 1e-3, lr_d = 0, spatial_dr = 0.1, dense_units=128, conv_size=128, dr=0.1, patience=3, fold_id=fold_n)\n        \n        pred_valid = model.predict(X_valid)\n        oof[valid_index] = pred_valid\n        valid_df[oof_name] = pred_valid\n        \n        bias_metrics_df = compute_bias_metrics_for_model(valid_df, identity_columns, oof_name, 'target')\n        scores.append(get_final_metric(bias_metrics_df, calculate_overall_auc(valid_df, oof_name)))\n        \n        prediction += model.predict(X_test, batch_size = 1024, verbose = 1)\n    \n    prediction \/= n_fold\n    \n    # print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    return oof, prediction, scores","200475b0":"oof_name = 'predicted_target'\nmax_len = 250\noof, prediction, scores = train_model(X=train, X_test=test, y=train['target'], tokenizer=tk, max_len=max_len)\nprint('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))","92483a40":"plt.hist(prediction);\nplt.hist(oof);\nplt.title('Distribution of predictions vs oof predictions');","8deefffe":"sub['prediction'] = prediction\nsub.to_csv('submission.csv', index=False)","da60bf24":"<a id=\"run\"><\/a>\n## Train and predict","37157860":"<a id=\"train\"><\/a>\n## Training fuction","a3802339":"## General information\nThis is a basic kernel with CNN. In this kernel I train a CNN model on folds and calculate the competition metric (not simple auc).","604a5128":"<a id=\"model\"><\/a>\n## Model","6a46d667":"<a id=\"valid\"><\/a>\n## Validation functions","5aa5dffa":"<a id=\"load\"><\/a>\n## Loading data\n\nI'll load preprocessed data from my dataset","2994cee4":"## Content\n\n* [1 Loading and processing data](#load)\n* [2 Validation functions](#valid)\n* [3 Model](#model)\n* [4 Training function](#train)\n* [5 Train and predict](#run)"}}