{"cell_type":{"2a59927b":"code","a7b5530f":"code","eb83972b":"code","8acd1a6d":"code","f0622a01":"code","c511a34e":"code","a5eaeca6":"code","42ade96e":"code","fd95ffab":"code","afa7c7c8":"code","d6dca7b9":"code","19778af0":"code","1312d053":"code","3ca8fe22":"code","42727e26":"code","c6b112c6":"code","10d78e01":"code","368700e5":"code","211d8a65":"code","2cdba671":"code","7815a01d":"code","994d0468":"code","ef3958b1":"code","5f162af8":"code","22b64d3a":"code","ce3ca072":"code","ebf06bac":"code","9756328e":"code","416b79f5":"code","e311bfa0":"code","32b378ae":"code","6cdf5e3e":"code","f3eb114e":"code","cf4f6055":"code","1ed0f424":"code","e841ea5b":"code","9b380ac9":"code","63f820e4":"code","33cb2253":"code","eec87088":"code","e70459aa":"code","2bb9ccff":"markdown","dda02fb1":"markdown","d20526e7":"markdown","e6a88f58":"markdown","05343292":"markdown","5e7f4802":"markdown","fd53586c":"markdown","814b861a":"markdown","7de8f283":"markdown","d49af079":"markdown","67891fee":"markdown","51936d67":"markdown","190d0a71":"markdown","0438a8ac":"markdown","f9289d42":"markdown","2008946d":"markdown","32ad993b":"markdown","c10ae377":"markdown","31e74f9e":"markdown","2cf80b81":"markdown"},"source":{"2a59927b":"# Keep Internet \"On\" which is present in right side -> Settings Panel\n!rm -r \/opt\/conda\/lib\/python3.6\/site-packages\/lightgbm\n!git clone --recursive https:\/\/github.com\/Microsoft\/LightGBM","a7b5530f":"# Keep Internet \"On\" which is present in right side -> Settings Panel\n!apt-get install -y -qq libboost-all-dev","eb83972b":"%%bash\ncd LightGBM\nrm -r build\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=\/usr\/local\/cuda\/lib64\/libOpenCL.so -DOpenCL_INCLUDE_DIR=\/usr\/local\/cuda\/include\/ ..\nmake -j$(nproc)","8acd1a6d":"# Keep Internet \"On\" which is present in right side -> Settings Panel\n!cd LightGBM\/python-package\/;python3 setup.py install --precompile","f0622a01":"!mkdir -p \/etc\/OpenCL\/vendors && echo \"libnvidia-opencl.so.1\" > \/etc\/OpenCL\/vendors\/nvidia.icd\n!rm -r LightGBM","c511a34e":"# Import Required Python Packages :\n\n# Scientific and Data Manipulation Libraries :\n\nimport numpy as np\nimport pandas as pd\n\n# Data Viz & Regular Expression Libraries :\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nget_ipython().run_line_magic('matplotlib', 'inline')\n\n# Scikit-Learn Pre-Processing Libraries :\n\nfrom sklearn.preprocessing import *\n\n# Garbage Collection Libraries :\n\nimport gc\n\n# Boosting Algorithm Libraries :\n\nfrom xgboost                          import XGBClassifier\nfrom catboost                         import CatBoostClassifier, Pool\nfrom lightgbm                         import LGBMClassifier\nfrom sklearn.ensemble                 import RandomForestClassifier, VotingClassifier\n\n# Model Evaluation Metric & Cross Validation Libraries :\nfrom sklearn.metrics                  import roc_auc_score, auc, roc_curve\nfrom sklearn.model_selection          import StratifiedKFold,KFold, RepeatedStratifiedKFold, train_test_split\n\n# Setting SEED to Reproduce Same Results even with \"GPU\" :\nseed_value = 1994\nimport os\nos.environ['PYTHONHASHSEED'] = str(seed_value)\nimport random\nrandom.seed(seed_value)\nimport numpy as np\nnp.random.seed(seed_value)\nSEED=seed_value","a5eaeca6":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","42ade96e":"# Loading data from train, test and submission csv files :\n\ntrain = pd.read_csv('..\/input\/avcrosssellhackathon\/train.csv')\ntest = pd.read_csv('..\/input\/avcrosssellhackathon\/test.csv')\nsub = pd.read_csv('..\/input\/avcrosssellhackathon\/sample_submission.csv')","fd95ffab":"# Python Method 1 : Displays Data Information :\n\ndef display_data_information(data, data_types, dataframe_name):\n    print(\" Information of \",dataframe_name,\": Rows = \",data.shape[0],\"| Columns = \",data.shape[1],\"\\n\")\n    data.info()\n    print(\"\\n\")\n    for VARIABLE in data_types :\n        data_type = data.select_dtypes(include=[ VARIABLE ]).dtypes\n        if len(data_type) > 0 :\n            print(str(len(data_type))+\" \"+VARIABLE+\" Features\\n\"+str(data_type)+\"\\n\"  )        \n\n# Display Data Information of \"train\" :\n\ndata_types  = [\"float32\",\"float64\",\"int32\",\"int64\",\"object\",\"category\",\"datetime64[ns]\"]\ndisplay_data_information(train, data_types, \"train\")","afa7c7c8":"# Display Data Information of \"test\" :\n\ndisplay_data_information(test, data_types, \"test\")","d6dca7b9":"# Python Method 2 : Displays Data Head (Top Rows) and Tail (Bottom Rows) of the Dataframe (Table) :\n\ndef display_head_tail(data, head_rows, tail_rows):\n    display(\"Data Head & Tail :\")\n    display(data.head(head_rows).append(data.tail(tail_rows)))\n#     return True\n\n# Displays Data Head (Top Rows) and Tail (Bottom Rows) of the Dataframe (Table)\n# Pass Dataframe as \"train\", No. of Rows in Head = 3 and No. of Rows in Tail = 2 :\n\ndisplay_head_tail(train, head_rows=3, tail_rows=2)","19778af0":"display_head_tail(test, head_rows=3, tail_rows=2)","1312d053":"# Python Method 3 : Displays Data Description using Statistics :\n\ndef display_data_description(data, numeric_data_types, categorical_data_types):\n    \n    print(\"Data Description :\")\n    display(data.describe( include = numeric_data_types))\n    print(\"\")\n    display(data.describe( include = categorical_data_types))\n\n# Display Data Description of \"train\" :\n\ndisplay_data_description(train, data_types[0:4], data_types[4:7])","3ca8fe22":"# Display Data Description of \"test\" :\n\ndisplay_data_description(test, data_types[0:4], data_types[4:7])","42727e26":"# Checking Percentage(%) of Common ID's  between train and test data using Unique train values :\n\nid_col_in_data = 'id'\nprint(np.intersect1d(train[id_col_in_data], test[id_col_in_data]).shape[0]\/train[id_col_in_data].nunique())\ncommon_ids = len(set(test[id_col_in_data].unique()).intersection(set(train[id_col_in_data].unique())))\nprint(\"Common IDs : \",common_ids)\n\n# No Data Leak, there are 127037 ID's in Common\n\nprint(\"Unique IDs : \",test.shape[0] - common_ids)","c6b112c6":"# Python Method 4 : Removes Data Duplicates while Retaining the First one - Similar to SQL DISTINCT :\n\ndef remove_duplicate(data):\n    \n    print(\"BEFORE REMOVING DUPLICATES - No. of Rows = \",data.shape[0])\n    data.drop_duplicates(keep=\"first\", inplace=True) \n    print(\"AFTER REMOVING DUPLICATES  - No. of Rows = \",data.shape[0])\n    \n    return data\n\n# Remove Duplicates from \"train\" data :\n\ntrain = remove_duplicate(train)\n\n# No Duplicates at all !!!","10d78e01":"# Python Method 5 : Fills or Imputes Missing values with Various Methods : \n\ndef fill_missing_values(data, fill_value, fill_types, columns, dataframe_name):\n    \n    print(\"Missing Values BEFORE REMOVAL in \",dataframe_name,\" data\")\n    display(data.isnull().sum())\n    for column in columns :\n        \n        # Fill Missing Values with Specific Value :\n        if \"Value_Fill\" in fill_types :\n            data[ column ] = data[ column ].fillna(fill_value)\n#             print(\"Value_Fill\")\n\n        # Fill Missing Values with Forward Fill  (Previous Row Value as Current Row in Table) :\n        if \"Forward_Fill\" in fill_types :\n            data[ column ] = data[ column ].ffill(axis = 0)\n#             print(\"Forward_Fill\")\n\n        # Fill Missing Values with Backward Fill (Next Row Value as Current Row in Table) :\n        if \"Backward_Fill\" in fill_types :\n            data[ column ] = data[ column ].bfill(axis = 0)\n#             print(\"Backward_Fill\")\n    \n    print(\"Missing Values AFTER REMOVAL in \",dataframe_name,\" data\")\n    display(data.isnull().sum())\n    \n    return data\n\nfill_types = [ \"Forward_Fill\"]\nfill_value = 0\n# Fills or Imputes Missing values in \"Registration_Date\" Column with \"Forward_Fill\" Method in \"train\" : \ntrain = fill_missing_values(train, fill_value, fill_types, [\"Gender\"],\"train\")\n\n# Fills or Imputes Missing values in \"Registration_Date\" Column with \"Forward_Fill\" Method in \"train\" :\ntest  = fill_missing_values(test, fill_value, fill_types, [\"Gender\"],\"test\")\n\n# No Missing Values !!!","368700e5":"# Python Method 6 : Displays Unique Values in Each Column of the Dataframe(Table) :\n\ndef display_unique(data):\n    for column in data.columns :\n        \n        print(\"No of Unique Values in \"+column+\" Column are : \"+str(data[column].nunique()))\n#         print(\"Actual Unique Values in \"+column+\" Column are : \"+str(data[column].sort_values(ascending=True,na_position='last').unique() ))\n#         print(\"NULL Values :\")\n#         print(data[ column ].isnull().sum())\n        print(\"Value Counts :\")\n        print(data[column].value_counts())\n        print(\"\")\n        \n# Displays Unique Values in Each Column of \"train\" :\n# Check \"train\" data for Values of each Column - Long Form :\n\ndisplay_unique(train)\n\n# Display this info in a Table Format - Improvements coming In Part 2","211d8a65":"# Check \"train\" data for Values of each Column - Short Form :\n# Use Whichever you feel good working with :\n\nfor i in train:\n    print(f\"column {i} unique values {train[i].unique()}\")","2cdba671":"# Combine train and test data into single DataFrame - combine_set :\n\ncombine_set=pd.concat([train,test],axis=0)","7815a01d":"# converting object to int type :\n\ncombine_set['Vehicle_Age']=combine_set['Vehicle_Age'].replace({'< 1 Year':0,'1-2 Year':1,'> 2 Years':2})\ncombine_set['Gender']=combine_set['Gender'].replace({'Male':1,'Female':0})\ncombine_set['Vehicle_Damage']=combine_set['Vehicle_Damage'].replace({'Yes':1,'No':0})","994d0468":"sns.heatmap(combine_set.corr())","ef3958b1":"# HOLD - CV - 0.8589 - BEST EVER :\ncombine_set['Vehicle_Damage_per_Vehicle_Age'] = combine_set.groupby(['Region_Code','Age'])['Vehicle_Damage'].transform('sum')\n\n# Score - 0.858657 (This Feature + Removed Scale_Pos_weight in LGBM) | Rank - 20 \ncombine_set['Customer_Term_in_Years'] = combine_set['Vintage'] \/ 365\n\n# combine_set['Customer_Term'] = (combine_set['Vintage'] \/ 365).astype('str')\n\n# Score - 0.85855 | Rank - 20 \ncombine_set['Vehicle_Damage_per_Policy_Sales_Channel'] = combine_set.groupby(['Region_Code','Policy_Sales_Channel'])['Vehicle_Damage'].transform('sum')\n\n# Score - 0.858527 | Rank - 22\ncombine_set['Vehicle_Damage_per_Vehicle_Age'] = combine_set.groupby(['Region_Code','Vehicle_Age'])['Vehicle_Damage'].transform('sum')\n\n# Score - 0.858510 | Rank - 23\ncombine_set[\"RANK\"] = combine_set.groupby(\"id\")['id'].rank(method=\"first\", ascending=True)\ncombine_set[\"RANK_avg\"] = combine_set.groupby(\"id\")['id'].rank(method=\"average\", ascending=True)\ncombine_set[\"RANK_max\"] = combine_set.groupby(\"id\")['id'].rank(method=\"max\", ascending=True)\ncombine_set[\"RANK_min\"] = combine_set.groupby(\"id\")['id'].rank(method=\"min\", ascending=True)\ncombine_set[\"RANK_DIFF\"] = combine_set['RANK_max'] - combine_set['RANK_min']\n\n# Score - 0.85838 | Rank - 15\ncombine_set.loc[(combine_set['Vehicle_Age'] == 2) & (combine_set['Vehicle_Damage'] == 1), 'Old_Vehicle'] = 1 \ncombine_set.loc[(combine_set['Vehicle_Age'] != 2) & (combine_set['Vehicle_Damage'] == 0), 'Old_Vehicle'] = 0\nfrequency_encoding = combine_set.groupby([\"Old_Vehicle\"]).size()\/len(combine_set)\ncombine_set[\"Old_Vehicle\"]   = combine_set[\"Old_Vehicle\"].map(frequency_encoding)\n\n# Score - 0.85837 | Rank - 15\ncombine_set['Vehicle_Damage_per_Region_Code'] = combine_set.groupby(['Region_Code'])['Vehicle_Damage'].transform('sum')","5f162af8":"sns.distplot(combine_set['Annual_Premium'])","22b64d3a":"# Score - 0.8584, Rank - 18\n# Data is left Skewed as we can see from above distplot\n\ncombine_set['Annual_Premium']=np.log(combine_set['Annual_Premium'])\nsns.distplot(combine_set['Annual_Premium'])","ce3ca072":"combine_set.columns","ebf06bac":"# Getting back Train and Test after Preprocessing :\n\ntrain=combine_set[combine_set['Response'].isnull()==False]\ntest=combine_set[combine_set['Response'].isnull()==True].drop(['Response'],axis=1)","9756328e":"train.columns","416b79f5":"\ndef add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series=None, \n                  tst_series=None, \n                  target=None, \n                  min_samples_leaf=1, \n                  smoothing=1,\n                  noise_level=0):\n    \"\"\"\n    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n    https:\/\/kaggle2.blob.core.windows.net\/forum-message-attachments\/225952\/7441\/high%20cardinality%20categoricals.pdf\n    trn_series : training categorical feature as a pd.Series\n    tst_series : test categorical feature as a pd.Series\n    target : target data as a pd.Series\n    min_samples_leaf (int) : minimum samples to take category average into account\n    smoothing (int) : smoothing effect to balance categorical average vs prior  \n    \"\"\" \n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean \n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 \/ (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) \/ smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index \n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)\n\n# Score - 0.85857 | Rank - \n\ntr_g, te_g = target_encode(train[\"Vehicle_Damage\"], \n                         test[\"Vehicle_Damage\"], \n                         target=train[\"Response\"], \n                         min_samples_leaf=200,\n                         smoothing=20,\n                         noise_level=0.02)\ntrain['Vehicle_Damage_me']=tr_g\ntest['Vehicle_Damage_me']=te_g\n\n# tr_g, te_g = target_encode(train[\"Customer_Term_in_Years\"], \n#                          test[\"Customer_Term_in_Years\"], \n#                          target=train[\"Response\"], \n#                          min_samples_leaf=200,\n#                          smoothing=20,\n#                          noise_level=0.02)\n# train['Customer_Term_in_Years_me']=tr_g\n# test['Customer_Term_in_Years_me']=te_g\n\n# train.drop([\"Customer_Term_in_Years\"],axis=1)\n# test.drop([\"Customer_Term_in_Years\"],axis=1)\n\n# Customer_Term_in_Months","e311bfa0":"# Split the Train data into predictors and target :\npredictor_train = train.drop(['Response','id'],axis=1)\ntarget_train    = train['Response']\npredictor_train.head()","32b378ae":"# Get the Test data by dropping 'id' :\npredictor_test = test.drop(['id'],axis=1)","6cdf5e3e":"def data_scaling( scaling_strategy , scaling_data , scaling_columns ):\n    \n    if    scaling_strategy ==\"RobustScaler\" :\n        scaling_data[scaling_columns] = RobustScaler().fit_transform(scaling_data[scaling_columns])\n        \n    elif  scaling_strategy ==\"StandardScaler\" :\n        scaling_data[scaling_columns] = StandardScaler().fit_transform(scaling_data[scaling_columns])\n        \n    elif  scaling_strategy ==\"MinMaxScaler\" :\n        scaling_data[scaling_columns] = MinMaxScaler().fit_transform(scaling_data[scaling_columns])\n        \n    elif  scaling_strategy ==\"MaxAbsScaler\" :\n        scaling_data[scaling_columns] = MaxAbsScaler().fit_transform(scaling_data[scaling_columns])\n        \n    else :  # If any other scaling send by mistake still perform Robust Scalar\n        scaling_data[scaling_columns] = RobustScaler().fit_transform(scaling_data[scaling_columns])\n    \n    return scaling_data\n\n# All 3 Types of Scaling didnt work - Choosing StandardScaler which gives BEST Score :\n# StandardScaler - 0.8581 | MinMaxScaler - 0.8580 | RobustScaler - 0.8444\n\nscaling_strategy = [\"RobustScaler\", \"StandardScaler\",\"MinMaxScaler\",\"MaxAbsScaler\"]\n\n# Scaling didn't give an Improvement in Score so commneted :\n\n# predictor_train_scale = data_scaling( scaling_strategy[1] , predictor_train , predictor_train.columns )\n# predictor_test_scale = data_scaling( scaling_strategy [1] , predictor_test , predictor_test.columns )\npredictor_train_scale = predictor_train\npredictor_test_scale = predictor_test","f3eb114e":"print(\"predictor_train_encode SHAPE   : \",predictor_train_scale.shape)\ndisplay(\"predictor_train_encode COLUMNS : \",predictor_train_scale.head())\n\nprint(\"predictor_test_encode SHAPE   : \",predictor_test_scale.shape)\ndisplay(\"predictor_test_encode COLUMNS : \",predictor_test_scale.head())","cf4f6055":"# Skipped Now !","1ed0f424":"predictor_train_scale.info()","e841ea5b":"\n\nkf=KFold(n_splits=10,shuffle=True)\n\npreds_1   = list()\ny_pred_1  = []\nrocauc_score = []\n\nfor i,(train_idx,val_idx) in enumerate(kf.split(predictor_train_scale)):    \n    \n    X_train, y_train = predictor_train_scale.iloc[train_idx,:], target_train.iloc[train_idx]    \n    X_val, y_val = predictor_train_scale.iloc[val_idx, :], target_train.iloc[val_idx]\n   \n    print('\\nFold: {}\\n'.format(i+1))\n\n    lg= LGBMClassifier( metric= 'auc',\n                       \n                       # GPU PARAMETERS #\n                       device = \"gpu\",\n                       gpu_device_id =0,\n                       max_bin = 63,\n                       gpu_platform_id=1,\n                       # GPU PARAMETERS #\n                       \n                       n_estimators=50000,    \n                       bagging_fraction=0.95, \n                       subsample_freq = 2, \n                       objective =\"binary\",\n                       min_samples_leaf= 2,\n                       importance_type = \"gain\",\n                       verbosity = -1,\n                       random_state=294,\n                       num_leaves = 300,\n                       boosting_type = 'gbdt',\n                       learning_rate=0.15,\n                       max_depth=4, \n                     # scale_pos_weight=2, # Score - 0.85865 | Rank - 18\n                       n_jobs=-1 \n                      )\n\n    lg.fit(X_train, y_train\n          ,eval_set=[(X_train, y_train),(X_val, y_val)]           \n          ,early_stopping_rounds=100\n          ,verbose=100\n          )\n\n    roc_auc = roc_auc_score(y_val,lg.predict_proba(X_val)[:, 1])\n    rocauc_score.append(roc_auc)\n    preds_1.append(lg.predict_proba(predictor_test_scale[predictor_test_scale.columns])[:, 1])\n    \ny_pred_final_1          = np.mean(preds_1,axis=0)    \nsub['Response']=y_pred_final_1\n\nBlend_model_1 = sub.copy()","9b380ac9":"print('ROC_AUC - CV Score: {}'.format((sum(rocauc_score)\/10)),'\\n')\nprint(\"Score : \",rocauc_score)","63f820e4":"# Download and Show Submission File :\n\ndisplay(\"sample_submmission\",sub)\nsub_file_name_1 = \"S1. LGBM_GPU_TargetEnc_Vehicle_Damage_me_1994SEED_LGBM_NoScaler_MyStyle.csv\"\nsub.to_csv(sub_file_name_1,index=False)\nsub.head(5)","33cb2253":"from sklearn.model_selection import KFold,StratifiedKFold,RepeatedStratifiedKFold\n\nkf=KFold(n_splits=10,shuffle=True)\n\npreds_2   = list()\ny_pred_2  = []\nrocauc_score = []\n\nfor i,(train_idx,val_idx) in enumerate(kf.split(predictor_train_scale)):    \n    \n    X_train, y_train = predictor_train_scale.iloc[train_idx,:], target_train.iloc[train_idx]    \n    X_val, y_val = predictor_train_scale.iloc[val_idx, :], target_train.iloc[val_idx]\n   \n    print('\\nFold: {}\\n'.format(i+1))\n    \n    cb = CatBoostClassifier( eval_metric='AUC',\n                            \n                            # GPU PARAMETERS #\n                            task_type='GPU',\n                             devices=\"0\",\n                            # GPU PARAMETERS #\n                            \n                            learning_rate=0.15, \n                            n_estimators=494, \n                            max_depth=7, \n#                             scale_pos_weight=2\n                           )\n\n    \n    cb.fit(X_train, y_train\n           ,eval_set=[(X_val, y_val)]\n           ,early_stopping_rounds=100\n           ,verbose=100\n                       )\n\n    roc_auc = roc_auc_score(y_val,cb.predict_proba(X_val)[:, 1])\n    rocauc_score.append(roc_auc)\n    preds_2.append(cb.predict_proba(predictor_test_scale[predictor_test_scale.columns])[:, 1])\n    \ny_pred_final_2          = np.mean(preds_2,axis=0)    \nsub['Response']=y_pred_final_2\n\nprint('ROC_AUC - CV Score: {}'.format((sum(rocauc_score)\/10)),'\\n')\nprint(\"Score : \",rocauc_score)\n\n# Download and Show Submission File :\n\ndisplay(\"sample_submmission\",sub)\nsub_file_name_2 = \"S2. CB_GPU_TargetEnc_Vehicle_Damage_me_1994SEED_LGBM_NoScaler_MyStyle.csv\"\nsub.to_csv(sub_file_name_2,index=False)\nBlend_model_2 = sub.copy()\nsub.head(5)","eec87088":"from sklearn.model_selection import KFold,StratifiedKFold,RepeatedStratifiedKFold\n\nkf=KFold(n_splits=10,shuffle=True)\n\npreds_3   = list()\ny_pred_3  = []\nrocauc_score = []\n\nfor i,(train_idx,val_idx) in enumerate(kf.split(predictor_train_scale)):    \n    \n    X_train, y_train = predictor_train_scale.iloc[train_idx,:], target_train.iloc[train_idx]    \n    X_val, y_val = predictor_train_scale.iloc[val_idx, :], target_train.iloc[val_idx]\n   \n    print('\\nFold: {}\\n'.format(i+1))\n\n    xg=XGBClassifier( eval_metric='auc',\n        \n                     # GPU PARAMETERS #\n                     tree_method='gpu_hist', \n                     gpu_id=0,\n                     # GPU PARAMETERS #\n        \n                     random_state=294,\n                     learning_rate=0.15, \n                     max_depth=4,\n                     n_estimators=494, \n                     objective='binary:logistic'\n                    )\n\n    xg.fit(X_train, y_train\n           ,eval_set=[(X_train, y_train),(X_val, y_val)]\n           ,early_stopping_rounds=100\n           ,verbose=100\n           )\n\n    roc_auc = roc_auc_score(y_val,xg.predict_proba(X_val)[:, 1])\n    rocauc_score.append(roc_auc)\n    preds_3.append(xg.predict_proba(predictor_test_scale[predictor_test_scale.columns])[:, 1])\n    \ny_pred_final_3         = np.mean(preds_3,axis=0)    \nsub['Response']=y_pred_final_3\n\nprint('ROC_AUC - CV Score: {}'.format((sum(rocauc_score)\/10)),'\\n')\nprint(\"Score : \",rocauc_score)\n\n# Download and Show Submission File :\n\ndisplay(\"sample_submmission\",sub)\nsub_file_name_3 = \"S3. XGB_GPU_TargetEnc_Vehicle_Damage_me_1994SEED_LGBM_NoScaler_MyStyle.csv\"\nsub.to_csv(sub_file_name_3,index=False)\nBlend_model_3 = sub.copy()\nsub.head(5)","e70459aa":"one = Blend_model_2['id'].copy()\n\nBlend_model_1.drop(\"id\", axis=1, inplace=True)\nBlend_model_2.drop(\"id\", axis=1, inplace=True)\nBlend_model_3.drop(\"id\", axis=1, inplace=True)\n\nBlend = (Blend_model_1 + Blend_model_2 + Blend_model_3)\/3\n\nid_df = pd.DataFrame(one, columns=['id'])\nid_df.info()\nBlend = pd.concat([ id_df,Blend], axis=1)\nBlend.info()\n\nBlend.to_csv('S4. Blend of 3 Models - LGBM_CB_XGB.csv',index=False)\ndisplay(\"S4. Blend of 3 Models : \",Blend.head())","2bb9ccff":"### DATASET can be downloaded here -> https:\/\/www.kaggle.com\/vetrirah\/avcrosssellhackathon","dda02fb1":"\"10\" Things that Worked in this AV Cross Sell Hackathon :\n===========================================\n\n1. **2 BEST Features** - Target Encoding of Vehicle_Damage and Sum of Vehicle_Damage Grouped by Region_Code - Based on Feature Importance - Gave a Good Boost in CV(10-Fold Cross Validation) and LB(Public LeaderBoard)\n1. **Domain Based Feature** : Frequency Encoding of Old Vehicle - Gave a little Boost. LB Score - 0.85838 | LB Rank - 15\n1. **Rank Features from Hackathon Solutions** - Gave a Huge Boost. LB Score - 0.858510 | LB Rank - 23\n1. **Dropping \"id\"** Column - Gave a Good Boost.\n1. **Domain Based Feature** : Vehicle_Damage per Vehicle_Age & Region_Code - Gave a little Boost. LB Score - 0.858527 | LB Rank - 22\n1. **Removing Skew in Annual_Premium** - Gave a Huge Boost. Score - 0.85857 LB Score - 0.85855 | LB Rank - 20\n1. **Domain Based Feature** : Vehicle_Damage per Region_Code and Policy_Sales_Channel - Based on Feature Importance - Gave a little Boost. LB Score - 0.85856 | LB Rank - 20 \n1. **Tuned all 3 Models with Hyper parameters and 10-Fold CV** over a 5-Fold CV gave a Robust Strategy and Best results with early_stopping_rounds=50 or 100. Scale_pos_weight didnt work much here. \n1. **Domain Based Feature :** Customer Term in Years because other features are also in Years and Insurance Response will be based on Number of Years. LB Score - 0.858657 | LB Rank - 18\n1. **Ensemble \/ Blending of all 3 Best Individual Models - LightGBM, CatBoost and XGBoost gave the Best Private Score.**","d20526e7":"### Binary Classification Problem - Target has ONLY 2 Categories - \n### Target - Response has 2 Values of Customers [1,0]","e6a88f58":"\"5\" Things that didn't Work :\n===========================================\n\n1. FEATURES that DIDN'T WORK OUT :\n\n#### [Sum of Vehicle_Damage grouped by Age , Sum of Vehicle_Damage grouped by Previously_Insured, Count of Vehicle_Damage grouped by Region_Code , Max of Vehicle_Damage grouped by Region_Code, Min of Vehicle_Damage grouped by Region_Code, Frequency Encoding of Old Age and Old Vehicle, Frequency Encoding of Vehicle_Age, EMI per Month = Annual_Premium \/ 12 , Sum of Vehicle_Damage grouped by  Policy_Sales_Channel, Sum of Vehicle_Damage grouped by Vehicle_Age,Sum of Vehicle_Damage grouped by Driving_License]\n\n2. **Dropping Driving_License Column** which was not much correlating with Response.\n3. **One Hot Encoding \/ Dummy Encoding** of All Features.\n4. **All 3 Types of Scaling didn't work** compared to Unscaled Data - StandardScaler gave BEST LB Score among them. StandardScaler - 0.8581 | MinMaxScaler - 0.8580 | RobustScaler - 0.8444\n5. **Removing Duplicates on Region_Code** between Train and Testbased didn't Work at all.","05343292":"## 2. Perform EDA (Exploratory Data Analysis) - Understanding the Datasets :    \n\n### 2.1 Explore Train and Test Data and get to know what each Column \/ Feature denotes :","5e7f4802":"## 9. Improve ML Model,Fine Tune with MODEL Evaluation METRIC - \"ROC_AUC\" with k-FOLD Cross Validation and Predict Target \"Response\" :","fd53586c":"## 7.  Data Encoding - Label Encoding :","814b861a":"### **To use LightGBM GPU Model : \"Turn On Internet Button\"** on the right side in Settings - Run all 5 Cells !!!","7de8f283":"## 5.  Feature Engineering\n\n### 5.1 Feature Selection - Selection of Most Important Existing Features\n### 5.2 Feature Creation  - Creation  of New Features from the Existing Features \/ Predictors :","d49af079":"## 4.  Fill\/Impute Missing Values Continuous - Mean\/Median\/Any Specific Value & Categorical - Others\/ForwardFill\/BackFill :","67891fee":"## 6.  Split Train Data into Train and Validation Data with Predictors(Independent) & Target(Dependent) :","51936d67":"![BEST_PRIVATE_Score.png](attachment:BEST_PRIVATE_Score.png)","190d0a71":"## 10. Result Submission , Check Leaderboard & Improve \"ROC_AUC\" :","0438a8ac":"## 3.  Remove Duplicate Rows from Train data if present :","f9289d42":"### **<center>\ud83d\ude0a Reached Rank 55 in Public LB and 67 in Private LB - Would have reached Rank 46 in Private LB as shown above, if I had made the correct submission of 3 Models Ensemble. Thanks for reading Friends. See you all in Part 2 for more Analysis and Modelling - ENCOURAGE if you liked this Notebook \ud83d\ude0a<\/center>**\n\n### **<center>\ud83d\ude0a For Learning Purpose - You can still participate in your free time to see your Public and Private Scores & Rank, though it won't reflect on Leaderboard \ud83d\ude0a<\/center>**\n\n### **<center>\ud83d\ude0a Ask your doubts & Share your thoughts, ideas & feedbacks in Comments below \ud83d\ude0a<\/center>**","2008946d":"## Steps for Applied Machine Learning (ML) for Hackathons :\n\n1.  Understand the Problem Statement & Import Packages and Datasets.  \n\n2.  Perform EDA (Exploratory Data Analysis) - Understanding the Datasets :\n\n       *       Explore Train and Test Data and get to know what each Column \/ Feature denotes.\n       *       Check for Imbalance of Target Column in Datasets.\n       *       Visualize Count Plots & Unique Values to infer from Datasets.\n            \n3.  Remove Duplicate Rows from Train Data if present.\n\n4.  Fill\/Impute Missing Values Continuous - Mean\/Median\/Any Specific Value & Categorical - Others\/ForwardFill\/BackFill.\n\n5.  Feature Engineering \n\n      *       Feature Selection - Selection of Most Important Existing Features.\n      *       Feature Creation  - Creation  of New Feature from the Existing Features.\n      \n6.  Split Train Data into Train and Validation Data with Predictors(Independent) & Target(Dependent).      \n7.  Data Encoding - Label Encoding, OneHot Encoding and Data Scaling - MinMaxScaler, StandardScaler, RobustScaler\n8.  Create Baseline ML Model for Binary Classification Problem\n9.  Improve ML Model,Fine Tune with MODEL Evaluation METRIC - \"ROC_AUC\" with k-FOLD Cross Validation and Predict Target \"Response\"\n10. Result Submission, Check Leaderboard & Improve \"ROC_AUC\" Score","32ad993b":"![Prob_Stmt.jpg](attachment:Prob_Stmt.jpg)","c10ae377":"## 1.  Understand the Problem Statement & Import Packages and Datasets :","31e74f9e":"## 8.  Create Baseline ML Model :","2cf80b81":"## References : \n\n**LightGBM :**\n\nhttps:\/\/lightgbm.readthedocs.io\/en\/latest\/GPU-Tutorial.html\n\nhttps:\/\/www.kaggle.com\/vinhnguyen\/gpu-acceleration-for-lightgbm\n\n**CatBoost :**\n\nhttps:\/\/www.kaggle.com\/baomengjiao\/5kflod-catboost-using-gpu\/comments#1015590\n\n**XGBoost :**\n\nhttps:\/\/www.kaggle.com\/xhlulu\/ieee-fraud-xgboost-with-gpu-fit-in-40s"}}