{"cell_type":{"a1b9c753":"code","9838b021":"code","55818381":"code","a40a9d9e":"code","1fe4d769":"code","7ecbe22f":"code","46ac471a":"code","032e6759":"code","57c55d0e":"code","b9928933":"code","65f55c00":"code","77e1afff":"markdown"},"source":{"a1b9c753":"import pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nimport lightgbm as lgb\nfrom scipy.sparse import vstack, hstack, csr_matrix, spmatrix\nfrom scipy.stats import binom\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_extraction.text import CountVectorizer as CV\nimport datetime\nimport gc\nimport re","9838b021":"#\u6587\u5b57\u5217\u4e2d\u306e\u8a18\u53f7\u3092\u9664\u53bb\u3059\u308b\u95a2\u6570\n#The function to remove noisy marks in text\ndef arrange_words(text):\n    text = text.replace('!', '')\n    text = text.replace('?', '')\n    text = text.replace(',', '')\n    text = text.replace('.', '')\n    text = text.replace('\u201c', '')\n    text = text.replace('\u201d', '')\n    text = text.replace('\u2018', '')\n    text = text.replace('\u2019', '')\n    text = text.replace('\u2022', '')\n    text = text.replace('\u30fb', '')\n    text = text.replace('\u2026', '')\n    text = text.replace(':', '')\n    text = text.replace(';', '')\n    text = text.replace('(', '')\n    text = text.replace(')', '')\n    text = text.replace('{', '')\n    text = text.replace('}', '')\n    text = text.replace('[', '')\n    text = text.replace(']', '')\n    text = text.replace('<', '')\n    text = text.replace('>', '')\n    text = text.replace('\\'', '')\n    text = text.replace('\\\/', '')\n    text = text.replace('\"', '')\n    text = text.replace('-', ' ')\n    text = text.replace('_', ' ')\n    text = text.replace('\\n', ' ')\n    text = text.replace('\\r', ' ')\n    text = text.replace('#', '')\n    text = re.sub(r'[0-9]+', \"0\", text)\n    text = ' ' + text + ' '\n    return text","55818381":"#Series\u5168\u4f53\u306b\u542b\u307e\u308c\u308b\u5168\u3066\u306e\u5358\u8a9e\u3068\u305d\u306e\u500b\u6570\u3092\u96c6\u8a08\u3059\u308b\u95a2\u6570\n#The function to count all words and the number of those through overall texts\ndef get_word_counts(texts):\n    word_counts = defaultdict(int)\n    for text in texts.values:\n        for word in text.split(' '):\n            word_counts[word.lower()] += 1\n    return word_counts","a40a9d9e":"#\u4e8c\u9805\u5206\u5e03\u3092\u7528\u3044\u3066\u6709\u52b9\u306a\u5358\u8a9e\u3092\u62bd\u51fa\u3059\u308b\u95a2\u6570\n#The function to select useful words based on probability of binomial distribution\ndef extract_useful_column(dataset, all_words, significance_level):\n    texts = dataset.copy()\n    useful_words = []\n    p = texts['target'].sum() \/ texts['comment_text_arranged'].count()\n    for word in all_words:\n        texts['comment_text_arranged'] = dataset['comment_text_arranged'].map(lambda x: 1 if ' ' + word + ' ' in x else 0)\n        k = texts[texts['comment_text_arranged']==1]['target'].sum()\n        N = texts[texts['comment_text_arranged']==1]['comment_text_arranged'].count()\n#        print(binom.cdf(k, N, p))\n#        if (binom.cdf(k, N, p)<(significance_level\/2)) or (binom.cdf(k, N, p)>(1-significance_level\/2)):\n        p_value = binom.cdf(k, N, p)\n        if (p_value<(significance_level\/2)) or (p_value>(1-significance_level\/2)):\n            print(word)\n            useful_words.append(word)\n    return useful_words","1fe4d769":"#\u30c7\u30fc\u30bf\u578b\u3092\u5b9a\u7fa9\n#Define data types\ndtypes = {\n        'id':                                             'category',\n        'target':                                       'float16', \n        'comment_text':                           'category', \n        'severe_toxicity':                           'float16', \n        'obscene':                                    'float16', \n        'identity_attack':                           'float16', \n        'insult':                                         'float16', \n        'threat':                                        'float16', \n        'asian':                                         'float16', \n        'atheist':                                       'float16', \n        'bisexual':                                     'float16', \n        'black':                                         'float16', \n        'buddhist':                                    'float16', \n        'christian':                                    'float16', \n        'female':                                       'float16', \n        'heterosexual':                              'float16', \n        'hindu':                                         'float16', \n        'homosexual_gay_or_lesbian':        'float16', \n        'intellectual_or_learning_disability': 'float16', \n        'jewish':                                        'float16', \n        'latino':                                         'float16', \n        'male':                                          'float16', \n        'muslim':                                       'float16', \n        'other_disability':                           'float16', \n        'other_gender':                             'float16', \n        'other_race_or_ethnicity':              'float16', \n        'other_religion':                             'float16', \n        'other_sexual_orientation':             'float16', \n        'physical_disability':                       'float16', \n        'psychiatric_or_mental_illness':       'float16', \n        'transgender':                                'float16', \n        'white':                                          'float16', \n        'created_date':                              'category', \n        'publication_id':                             'category', \n        'parent_id':                                    'category', \n        'article_id':                                     'category', \n        'rating':                                         'category', \n        'funny':                                         'int8', \n        'wow':                                           'int8', \n        'sad':                                             'int8', \n        'likes':                                            'int8', \n        'disagree':                                     'int8', \n        'sexual_explicit':                             'float16', \n        'identity_annotator_count':             'int8', \n        'toxicity_annotator_count':             'int8', \n        }","7ecbe22f":"#\u8a13\u7df4\u30c7\u30fc\u30bf\u30fb\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9\n#Load DataSet\ntrain = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv', dtype=dtypes)\ntest  = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv',  dtype=dtypes)\ntrain_ids = train.index\ntest_ids  = test.index\ntrain_y = train['target'].apply(lambda x: 1 if x>=0.5 else 0)\ntrain_X = train.drop('target', axis=1)\ntest_X = test\ngc.collect()","46ac471a":"#comment_text\u3067\u4f7f\u308f\u308c\u3066\u3044\u308b\u5168\u3066\u306e\u5358\u8a9e\u306e\u500b\u6570\u3092\u53d6\u5f97\n#get the all words used in the column 'comment_text'\ntrain_X['comment_text_arranged'] = train_X['comment_text'].map(arrange_words)\ntest_X['comment_text_arranged'] = test_X['comment_text'].map(arrange_words)\ntrain_word_counts = get_word_counts(train_X['comment_text_arranged'])\ntest_word_counts = get_word_counts(test_X['comment_text_arranged'])\ntrain_word_counts_df = pd.DataFrame(list(train_word_counts.items()), columns=['word', 'count'])\ntest_word_counts_df = pd.DataFrame(list(test_word_counts.items()), columns=['word', 'count'])","032e6759":"#\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u53cc\u65b9\u306b\u5b58\u5728\u3059\u308b\u5358\u8a9e\u306e\u307f\u3092\u62bd\u51fa\n#extract just the words in both train data and test data\nword_counts_df = train_word_counts_df.merge(test_word_counts_df, on='word', how='inner')","57c55d0e":"#\u8a13\u7df4\u30c7\u30fc\u30bf\u3067\u6975\u7aef\u306b\u500b\u6570\u304c\u5c11\u306a\u3044\u3001\u53c8\u306f\u8a13\u7df4\u30c7\u30fc\u30bf\u30fb\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3067\u500b\u6570\u304c\u6975\u7aef\u306b\u504f\u3063\u3066\u3044\u308b\u5358\u8a9e\u4ee5\u5916\u3092\u62bd\u51fa\n#drop the words extremely few or biased between train data and test data\nword_counts_df['scaled_total'] = word_counts_df['count_x'] + word_counts_df['count_y'] * 18\nword_counts_df = word_counts_df[word_counts_df['count_x']>100]\nword_counts_df = word_counts_df[(word_counts_df['count_x']\/word_counts_df['scaled_total']>0.2) & (word_counts_df['count_x']\/word_counts_df['scaled_total']<0.8)]","b9928933":"#\u305d\u308c\u305e\u308c\u306e\u5358\u8a9e\u3092\u7279\u5fb4\u91cf\u3068\u3059\u308b\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f5c\u6210\n#create dataset which has each words as features;\n\ncv = CV(vocabulary=word_counts_df['word'].tolist())\nprint(datetime.datetime.now())\ntrain_X_flattened = cv.fit_transform(list(train_X['comment_text_arranged'].values))\ntest_X_flattened = cv.fit_transform(list(test_X['comment_text_arranged'].values))\nprint(datetime.datetime.now())","65f55c00":"#LightGBM\u3067\u8a13\u7df4\u3057\u3001\u4e88\u6e2c\u5024\u3092\u4f5c\u6210\u3002\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306b\u306fStratifiedKFold\u3092\u7528\u3044\u308b\u3002\n#train and predict using LightGBM. Validate using StratifiedKFold.\n\nlgb_test_result  = np.zeros(test_ids.shape[0])\nm = 100000\ncounter = 0\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nskf.get_n_splits(train_ids, np.array(train_y))\n\nfor train_index, test_index in skf.split(train_ids, train_y):\n    \n    print('Fold {}\\n'.format(counter + 1))\n    X_fit = train_X_flattened[train_index]\n    X_val = train_X_flattened[test_index]\n    X_fit, X_val = csr_matrix(X_fit, dtype='float32'), csr_matrix(X_val, dtype='float32')\n    y_fit, y_val = train_y[train_index], train_y[test_index]\n    \n    gc.collect()\n\n    lgb_model = lgb.LGBMClassifier(max_depth=-1,\n                                   n_estimators=1000,\n                                   learning_rate=0.05,\n                                   num_leaves=2**9-1,\n                                   colsample_bytree=0.28,\n                                   objective='binary', \n                                   n_jobs=-1)\n                                   \n    \n                               \n    lgb_model.fit(X_fit, y_fit, eval_metric='auc', \n                  eval_set=[(X_val, y_val)], \n                  verbose=100, early_stopping_rounds=100)\n                  \n    \n    del X_fit, X_val, y_fit, y_val, train_index, test_index\n    gc.collect()\n    \n    test = csr_matrix(test_X_flattened, dtype='float32')\n    lgb_test_result += lgb_model.predict_proba(test)[:,1]\n    counter += 1\n    \n    del test\n    gc.collect()\n    \n\nsubmission = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv')\nsubmission['prediction'] = lgb_test_result \/ counter\nsubmission.to_csv('.\/submission.csv', index=False)","77e1afff":"LightGBM\u3092\u7528\u3044\u305f\u30b7\u30f3\u30d7\u30eb\u306a\u30b3\u30fc\u30c9\u3092\u4f5c\u6210\u3057\u307e\u3057\u305f\u3002  \n\u30a4\u30f3\u30d0\u30e9\u30f3\u30b9\u30c9\u30c7\u30fc\u30bf\u3078\u306e\u5bfe\u51e6\u30fb\u30e2\u30c7\u30eb\u306e\u8a13\u7df4\u30fb\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306e\u90e8\u5206\u306f  \n\u4e0b\u8a18\u306ekernel\u306e\u5f71\u97ff\u3092\u5927\u304d\u304f\u53d7\u3051\u3066\u3044\u307e\u3059\u3002\n\nSimple code using LightGBM.  \nThe parts of dealing with inbalanced data, fiiting model and validation   \nare highly influenced by following kernel.    \n\n\n[LightGBM. Baseline Model Using Sparse Matrix](https:\/\/www.kaggle.com\/bogorodvo\/lightgbm-baseline-model-using-sparse-matrix)\n\n"}}