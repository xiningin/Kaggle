{"cell_type":{"9c619c17":"code","601df4e2":"code","33187c77":"code","b08df562":"code","24477f5e":"code","a07cccad":"code","3053e834":"code","ccccca98":"code","54fd33b6":"code","adb30ec4":"code","e985bb03":"code","329a308f":"code","3fcd7c1a":"code","c3cc7631":"code","493b8f8c":"code","124a4f14":"code","8e35c917":"code","514d4920":"code","da29b520":"code","d1bd81a5":"code","78bde2c4":"code","84590104":"code","cccc7928":"code","eeb87a54":"code","1f2d50b0":"code","272b3de0":"code","06fed4f9":"code","12a793dc":"code","b889304f":"code","1eeeabd6":"code","ad3fd6b9":"code","f9ef0deb":"code","c20a7bab":"code","689f778f":"code","d9a24076":"code","da03cb4f":"code","14207243":"code","ab26726b":"code","c20cf3f3":"code","d04ff8cd":"code","b8b2de19":"code","41811dc9":"code","4809f9fa":"code","8273aa74":"code","c55478d1":"code","bb6e6a29":"code","7b3c341d":"code","06528427":"code","18bb3488":"code","8ca00da5":"code","b8d06b49":"code","18b0df50":"code","d1db4d23":"code","195c11a6":"code","106d4c0e":"markdown","1b568ab3":"markdown","34ae84e4":"markdown","f4c6e82e":"markdown","2088586a":"markdown","2986bb7a":"markdown","b68f8e2a":"markdown","56240e0a":"markdown","c4053e3e":"markdown","34f053fa":"markdown","09f72638":"markdown","55609d2a":"markdown","8498e049":"markdown","8bd56f6c":"markdown","5a260742":"markdown","92339dfa":"markdown","0004c717":"markdown","4c1225a5":"markdown","d0ba5536":"markdown","b6d67c21":"markdown","f345f617":"markdown","af7cb63c":"markdown","50ba40c6":"markdown","d16518ad":"markdown","40e97959":"markdown","f8edb8ec":"markdown","d2381300":"markdown","8fc27538":"markdown","59a3f468":"markdown","a711c92f":"markdown","4f16df25":"markdown"},"source":{"9c619c17":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression  \nfrom sklearn.neighbors import KNeighborsClassifier  \nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import  GradientBoostingRegressor\nfrom sklearn import preprocessing \nfrom sklearn.preprocessing import RobustScaler\nimport plotly.express as px\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import confusion_matrix, mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report\nimport xgboost as xgb\nfrom imblearn.over_sampling import SMOTE\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nwarnings.filterwarnings(\"ignore\", category=FutureWarning) \nwarnings.filterwarnings(\"ignore\", category=UserWarning) \n\n%config InlineBackend.figure_format = 'retina'\n\n# to display all columns and rows:\npd.set_option('display.max_columns', None); pd.set_option('display.max_rows', None);","601df4e2":"df = pd.read_csv(r\"..\/input\/churndata\/churn.csv\" )","33187c77":"df.head()","b08df562":"def info_(dataframe):\n    print(\"shape:\",dataframe.shape)\n    print(dataframe.info())\n    print(\"Index:\",dataframe.index)\n    print(\"Columns:\",dataframe.columns)\n    print(\"ANY_NAN:\",dataframe.isnull().values.any())","24477f5e":"info_(df)","a07cccad":"df.skew()","3053e834":"cat_cols = [col for col in df.columns if df[col].nunique() < 10 \n                and col not in \"Exited\" and col not in \"CustomerId\" ]\n\nnum_cols = [col for col in df.columns if df[col].nunique() > 10\n                and df[col].dtypes != \"O\"\n                and col not in \"Exited\" \n                and col not in cat_cols and col not in \"CustomerId\" and col not in \"RowNumber\"]\n\nother_cols = [col for col in df.columns if col not in cat_cols \n                  and col not in num_cols and col not in \"CustomerId\"\n                 and col not in \"Exited\"]","ccccca98":"print(\"Categorical Variables : \" , cat_cols)\nprint(\"Numerical Variables : \" , num_cols)\nprint(\"Other Variables : \" , other_cols)","54fd33b6":"for col in cat_cols :\n    print (\"Categorical variables feature : \" , col, \"\\n\" )\n    print( df[col].value_counts())","adb30ec4":"def outlier_thresholds(dataframe, variable):\n    quartile1 = dataframe[variable].quantile(0.10)\n    quartile3 = dataframe[variable].quantile(0.90)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit","e985bb03":"def has_outliers(dataframe, num_col_names, plot=False):\n    variable_names = []\n    for col in num_col_names:\n        low_limit, up_limit = outlier_thresholds(dataframe, col)\n        if dataframe[(dataframe[col] > up_limit) | (dataframe[col] < low_limit)].any(axis=None):\n            number_of_outliers = dataframe[(dataframe[col] > up_limit) | (dataframe[col] < low_limit)].shape[0]\n            print(col, \":\", number_of_outliers)\n            variable_names.append(col)\n            if plot:\n                sns.boxplot(x=dataframe[col])\n                plt.show()\n    return variable_names","329a308f":"has_outliers(df , num_cols )","3fcd7c1a":"def target(dataframe, target):\n    f, ax = plt.subplots(1, 2, figsize=(18, 8))\n    df[target].value_counts().plot.pie(explode=[0, 0.1], autopct='%1.2f%%', ax=ax[0], shadow=True)\n    ax[0].set_title(target + ' Distribution')\n    ax[0].set_ylabel('')\n    sns.countplot(target, data=df, ax=ax[1])\n    ax[1].set_title(target + \" Count Plot\")\n    plt.show()","c3cc7631":"target(df , \"Exited\")","493b8f8c":"corr_matrix = df.corr()\nsns.clustermap(corr_matrix, annot = True, fmt = \".2f\", cmap = \"icefire\", figsize=(9,9))\nplt.title(\"Correlation Between Features\")\nplt.show()","124a4f14":"df[num_cols].describe([0.05,0.10,0.25,0.50,0.75,0.90,0.95,0.99]).T","8e35c917":"def hist_of_num(data):\n    num_cols = [col for col in data.columns if data[col].dtypes != \"O\" and col not in \"CustomerId\" and col not in \"Exited\" ]\n    counting_col = 0\n    for col in num_cols:\n        data[col].hist()\n        plt.xlabel(col)\n        plt.title(col)\n        plt.show()\n        counting_col += 1\n        \n    ","514d4920":"hist_of_num(df)","da29b520":"df[\"Age\"].describe()","d1bd81a5":"sns.boxplot(x=df[\"Age\"]);\n\nq1 = df[\"Age\"].quantile(0.25)\nq3 = df[\"Age\"].quantile(0.75)\niqr = q3 - q1\nup = q3 + 1.5 * iqr\nlow = q1 - 1.5 * iqr","78bde2c4":"df[(df[\"Age\"] < low) | (df[\"Age\"] > up)][[\"Age\"]].shape[0]\n","84590104":"df[(df[\"Age\"] < low) | (df[\"Age\"] > up)].any(axis=None)","cccc7928":"def pxfornum (dataframe):\n    for col in dataframe[num_cols]:\n        fig = px.histogram(\n        df, x=col, color='Exited',\n        marginal='box', nbins=50,\n        color_discrete_map={0: '#D62728', 1: '#3366CC'},\n        barmode='overlay')\n        fig.update_layout(height=600, width=800, \n        title_text='Credit Score Feature')\n        fig.show()","eeb87a54":"num_cols","1f2d50b0":"pxfornum(df)","272b3de0":"def graphs(dataframe, col, target, cat_analyze=True):\n    if cat_analyze == True:\n        fig, axarr = plt.subplots(1, 2, figsize=(10, 6))\n        a = sns.countplot(x=dataframe[col], hue=dataframe[target], data=dataframe, ax=axarr[0]).set_title(\n            'Count by class')\n        axarr[0].set_xticklabels(axarr[0].get_xticklabels(), rotation=75);\n        axarr[1].set_title('Rate by class')\n        b = sns.barplot(x=dataframe[col], y=dataframe[target], data=dataframe, ax=axarr[1]).set_ylabel('Rate')\n        axarr[1].set_xticklabels(axarr[1].get_xticklabels(), rotation=75);\n        plt.show()\n    else:\n        fig, axarr = plt.subplots(1, 3, figsize=(15, 6))\n        axarr[0].set_title('Distribution')\n        f = sns.distplot(dataframe[col], color='g', bins=40, ax=axarr[0])\n        axarr[1].set_title('Distribution for the two subpopulations')\n        g = sns.kdeplot(dataframe[col].loc[dataframe[target] == 1],\n                        shade=True, ax=axarr[1], label='Risk').set_xlabel(col)\n        g = sns.kdeplot(dataframe[col].loc[dataframe[target] == 0],\n                        shade=True, ax=axarr[1], label='Good', legend=True)\n        axarr[2].set_title('Outlier')\n        m = sns.boxplot(x=col, data=dataframe)\n        plt.legend();","06fed4f9":"graphs(df , \"Age\" , \"Exited\" , cat_analyze=False)","12a793dc":"for col in num_cols :\n    graphs(df,col , \"Exited\" , cat_analyze = False)","b889304f":"def stalk(dataframe, col, target, num_cols):\n    print(\"{}  | type: {}\\n\".format(col, dataframe[col].dtype))\n    print(pd.DataFrame({\"n\": dataframe[col].value_counts(),\n                        \"Ratio\": 100 * dataframe[col].value_counts() \/ len(dataframe),\n                        # \"TARGET_MEDIAN\": dataframe.groupby(col)[target].median(),\n                        \"Target_MEAN\": dataframe.groupby(col)[target].mean()}), end=\"\\n\\n\\n\")\n    graphs(dataframe, col, target)\n    display(dataframe.groupby(col).agg({num_cols[0]: [np.mean],\n                                        num_cols[1]: [np.mean],\n                                        num_cols[2]: [np.mean],\n                                        num_cols[3]: [np.mean, np.median],\n                                        num_cols[4]: [np.mean],\n                                        target: \"mean\"}))","1eeeabd6":"stalk(df, \"Geography\" , \"Exited\" , num_cols)","ad3fd6b9":"stalk(df,\"HasCrCard\", \"Exited\" , num_cols)","f9ef0deb":"stalk(df , \"IsActiveMember\", \"Exited\" , num_cols)","c20a7bab":"stalk(df , \"NumOfProducts\" , \"Exited\" , num_cols)","689f778f":"for col in cat_cols :\n    sns.catplot(x=col, y=\"Balance\", kind=\"boxen\",\n            data=df.sort_values(\"Balance\"));","d9a24076":"for col in cat_cols :\n    sns.catplot(x=col, y=\"EstimatedSalary\", kind=\"boxen\",\n            data=df.sort_values(\"EstimatedSalary\"));","da03cb4f":"for col in cat_cols :\n    sns.catplot(x=col, y=\"Age\", kind=\"boxen\",\n            data=df.sort_values(\"Age\"));","14207243":"df.loc[(df[\"Age\"] > 0) & (df[\"Age\"] <= 43),\"Generation\"] = 1\ndf.loc[(df[\"Age\"] > 43) & (df[\"Age\"] < 65),\"Generation\"] = 3\ndf.loc[(df[\"Age\"] >= 65),\"Generation\"] = 2","ab26726b":"df[\"EstimatedTenure\"]=df[\"EstimatedSalary\"]\/(df[\"Tenure\"]+1)","c20cf3f3":"df['BalanceToSalaryRatio'] = df['Balance'] \/ df['EstimatedSalary']","d04ff8cd":"df[\"Balance\/Age\"]= df[\"Balance\"] \/ df[\"Age\"]","b8b2de19":"df[\"CreditScoretoTenure\"]= df[\"CreditScore\"] \/ (df[\"Tenure\"]+1)","41811dc9":"df[\"EstimatedSalary\/Age\"] = df[\"EstimatedSalary\"] \/ df[\"Age\"]","4809f9fa":"df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)\n","8273aa74":"def one_hot_encoder(dataframe, categorical_cols, nan_as_category=True):\n    original_columns = list(dataframe.columns)\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, dummy_na=nan_as_category, drop_first=True)\n    new_columns = [c for c in dataframe.columns if c not in original_columns]\n    return dataframe, new_columns","c55478d1":"df, new_cols_ohe = one_hot_encoder(df, cat_cols)","bb6e6a29":"df[\"Tenure\"].describe()","7b3c341d":"df[\"CreditScoretoTenure\"].describe()","06528427":"X = df.drop('Exited', axis=1)\ny = df[[\"Exited\"]]\n","18bb3488":"rf_model = RandomForestClassifier(random_state=12345).fit(X, y)\n\ncross_val_score(rf_model, X, y, cv=10).mean()\n\n","8ca00da5":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.20, \n                                                    random_state=12345)\n\ngbm_model=GradientBoostingClassifier().fit(X_train,y_train)\ny_pred = gbm_model.predict(X_test)\n\n\ncv_results = cross_val_score(gbm_model, X_train, y_train, cv = 10, scoring= \"accuracy\")\n\nprint(cv_results.mean())\nprint(accuracy_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","b8d06b49":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","18b0df50":"Smote = SMOTE(random_state=12345)\nX_res, y_res = Smote.fit_sample(X, y)","d1db4d23":"# LightGBM\n\n\nlgbm = LGBMClassifier(random_state=12345)\ncross_val_score(lgbm, X, y, cv=10).mean()\n\n# model tuning\nlgbm_params = {\"learning_rate\": [0.01, 0.1, 0.5],\n               \"n_estimators\": [500, 1000, 1500],\n               \"max_depth\": [3, 5, 8]}\n\ngs_cv = GridSearchCV(lgbm,\n                     lgbm_params,\n                     cv=5,\n                     n_jobs=-1,\n                     verbose=2).fit(X, y)\n\nlgbm_tuned = LGBMClassifier(**gs_cv.best_params_).fit(X, y)\nauc=cross_val_score(lgbm_tuned, X, y, cv=10).mean()\nprint(\"Auc Score : \" , auc)\n\nfeature_imp = pd.Series(lgbm_tuned.feature_importances_,\n                        index=X.columns).sort_values(ascending=False)\n\nsns.barplot(x=feature_imp, y=feature_imp.index)\nplt.xlabel('Variables importance scores')\nplt.ylabel('Variables')\nplt.title(\"Variables importance levels\")\nplt.show()","195c11a6":"LGBM = 0.86389999999999","106d4c0e":" # Libraries","1b568ab3":"* Since we have an imbalanced dataset, we will increase the number of samples by SMOTE technique","34ae84e4":"![churn-rate@2x.jpg](attachment:churn-rate@2x.jpg)","f4c6e82e":"* None of the customers with a balance value of \"0\" churn.","2088586a":"\n## Data Understanding","2986bb7a":"We analyzed categorical variables according to their Estimated Salary.","b68f8e2a":"# GBM","56240e0a":"*  CreditScore : The average values of the credit scores of the countries are very close to each other and France is a very low level behind other countries..\n*  Age: The average age values of the countries are still very close to each other, Germany is seen to be slightly above.\n*  Tenure: The values showing the average number of years of customers in countries are almost the same.\n*  Balance: In the average balance values of the countries, France and Spain are seen to be close to each other, but almost twice as high as Germany.\n*  EstimatedSalary : Average estimated salary values of the countries are close to each other, again, Germany is one step ahead of the other two countries.","c4053e3e":"Null value does not appear.","34f053fa":"# # MODEL TUNING","09f72638":"# # Correlation between variables","55609d2a":"# Feature Engineering","8498e049":"## Reading Data","8bd56f6c":"# Numerical Variable Analysis","5a260742":"* The age variable was classified according to their churn status.","92339dfa":"  If we compare according to Churn situations : \n*      Germany ranks first with a value of 0.324432..\n*      Spain ranks second with a value of 0.166734.\n*      In the last row is France, which is very close to Spain, and the value of France is 0.161548. .","0004c717":"* 45% customers are inactive and have not exited from the organization. We can safely assume that these people either have forgotten about their account or else have kept their money in savings. The cause of concern is that 36% customers who were active in using the services have exited.","4c1225a5":"We analyzed categorical variables according to their age.","d0ba5536":"Categorical and Numerical variables will be determined.","b6d67c21":"* We have seen that the Number Of Product variable is effective for churning. It is especially remarkable that all 4 ones have churned.","f345f617":"# LGBM","af7cb63c":"* We saw that the * Exited variable is distributed as 80 '20. Generally, it is common to see imbalanced data in Churn and Fraud datasets.","50ba40c6":"# **#Categorical Variable Analysis**","d16518ad":"#  CHURN OR NOT ?","40e97959":"* Let's examine our target variable \"Exited\" for this data.","f8edb8ec":"* We analyzed categorical variables according to their Balance status.","d2381300":"* While the minimum value of the Tenure variable is 0, there may be infinite values in the newly created variable, so I chose to add 1.","8fc27538":"* CreditScore : Average and median values are overlap and  normal distribution can be observed.\n* Age: Average age and median values seem close to each other besides, the maximum value is observed as 92..\n* Tenure: Average mean and median values almost same. \n* Balance: Average value higher than median value.\n* EstimatedSalary : Average and median values are considered the same. outlier status not observed.","59a3f468":"# # # Model","a711c92f":"# **Problem:\nCan you devolop a machine learning model that can predict the customers leaving the company?\n\n* The aim of this project  to predict situation a bank's customers leave the bank or not..\n* \n* The action that defines customer abandonment is the customer's bank account closure...\n\n**Data Set Story:\n**\nIt consists of 10000 observations and 12 variables.\nIndependent variables contain information about customers.\nDependent variable expresses customer abandonment.\nVariables:\n\n* Surname : Customer's Surname\n* CreditScore : Customer's credit score\n* Geography : Country\n* Gender : (Female\/Male)\n* Age : Customer's Age\n* Tenure : How many years of customer.\n* Balance : Customer's balance\n* NumOfProducts : Number of bank products are used\n* HasCrCard : Credit card status (0=No,1=Yes)\n* IsActiveMember : Active membership status (0=No,1=Yes)\n* EstimatedSalary : Customers estimated salary\n* Exited : Churn or not? (0=No,1=Yes)****\n","4f16df25":"# #RandomForestClassifier"}}