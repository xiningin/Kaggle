{"cell_type":{"daf8f8d5":"code","5dec903b":"code","975d2c53":"code","0d82b76d":"code","ae44421c":"code","50dc71a5":"code","642a83e5":"code","bb3c89fb":"code","4ce2af96":"code","020da35d":"code","fb9449f6":"code","c91acfc3":"code","4cd92924":"code","081534d0":"code","15ad9062":"code","88a2c1d6":"code","251dc3bc":"code","e0416df1":"code","fa53a907":"code","ef32788d":"code","f63a84f6":"code","e9e51eab":"code","5365fda0":"code","ce0e3e5c":"code","d4c2bcc4":"code","e585a434":"code","bdee564c":"code","ae17b901":"markdown","058c5d35":"markdown","9dc883c5":"markdown","47f0bf26":"markdown","84ba79c7":"markdown","8f0c4cad":"markdown","8c692832":"markdown","94328535":"markdown","ff17627f":"markdown","12ed2ff3":"markdown","6e6cb0dd":"markdown","e4d9be00":"markdown","ae6438df":"markdown","e28c7bec":"markdown","fd69385f":"markdown","0a5a1922":"markdown","07f9ac6b":"markdown","101ae6c8":"markdown","6300f2f5":"markdown","296ccdb2":"markdown","863884ff":"markdown","debc878c":"markdown","90801354":"markdown","d7fbeeeb":"markdown","c646ee87":"markdown","b89ff163":"markdown","afc1093f":"markdown","2f4f44bc":"markdown","d087dce0":"markdown","cb466d6b":"markdown","a42b32fb":"markdown","085bbf97":"markdown","d10db464":"markdown","4183f2f9":"markdown","8ac10080":"markdown","ff8a3182":"markdown","309074ac":"markdown","e282b75f":"markdown","b099b47e":"markdown","d9294254":"markdown","d55229d3":"markdown","d323b63b":"markdown","b0a19623":"markdown","549605ae":"markdown","50dc6232":"markdown","083b4874":"markdown","f538fe07":"markdown","bf963974":"markdown"},"source":{"daf8f8d5":"import os\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport datetime\nimport math\nimport statistics as stats\nimport scipy\nimport seaborn","5dec903b":"data_raw = pd.read_csv('..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv')\ndata_raw.head()","975d2c53":"print(\"Dimensions: \", data_raw.shape)\nnum_row = data_raw.shape[0]\nnum_col = data_raw.shape[1]\nprint(data_raw.dtypes)","0d82b76d":"data = data_raw.copy() # Make new copy for cleaned data","ae44421c":"data = data.drop(columns=['host_name']) # Not needed thanks to host_id. Anonymizes the data.","50dc71a5":"#Number of NaN values\nprint(data_raw.isna().sum())\nprint(\"Out of\",num_row,\"rows\")","642a83e5":"data['name'] = data['name'].fillna('')\ndata['reviews_per_month'] = data['reviews_per_month'].fillna(0)","bb3c89fb":"data['last_review_DT'] = pd.to_datetime(data['last_review'])\n\n#Fill \nmean_date = data['last_review_DT'].mean()\nmax_date = data['last_review_DT'].max()\ndata['last_review_DT'] = data['last_review_DT'].fillna(mean_date)\n\ndef how_many_days_ago(datetime):\n    return (max_date - datetime).days\n\ndata['days_since_last_review'] = data['last_review_DT'].apply(how_many_days_ago)","4ce2af96":"from sklearn import preprocessing\nfrom sklearn.compose import ColumnTransformer\n\n# Here are the categorical features we are going to create one-hot encoded features for\ncategorical_features = ['neighbourhood_group','room_type','neighbourhood'] \n\nencoder = preprocessing.OneHotEncoder(handle_unknown='ignore')\none_hot_features = encoder.fit_transform(data[categorical_features])\none_hot_names = encoder.get_feature_names()\n\nprint(\"Type of one_hot_columns is:\",type(one_hot_features))","020da35d":"one_hot_df = pd.DataFrame.sparse.from_spmatrix(one_hot_features)\none_hot_df.columns = one_hot_names # Now we can see the actual meaning of the one-hot feature in the DataFrame\none_hot_df.head()","fb9449f6":"import seaborn as sns\nfrom sklearn import preprocessing\n\nmin_max_scaler = preprocessing.MinMaxScaler()\n\nnumerical_features = ['latitude','longitude','price','minimum_nights','number_of_reviews','reviews_per_month',\n                      'days_since_last_review', 'calculated_host_listings_count','availability_365']\n\ndataScaled = pd.DataFrame(min_max_scaler.fit_transform(data[numerical_features]), columns=numerical_features)\n#viz_2=sns.violinplot(data=data, y=['price','number_of_reviews'])\nax = sns.boxplot(data=dataScaled, orient=\"h\")\nax.set_title(\"Box plots for min-max scaled features\")","c91acfc3":"sns.distplot(data['price']).set_title(\"Distribution of AirBnB prices\")","4cd92924":"# I'll transform the following columns by taking log(1+x)\ntransform_cols = ['price','minimum_nights','number_of_reviews','reviews_per_month','calculated_host_listings_count']\nfor col in transform_cols:\n    col_log1p = col + '_log1p'\n    data[col_log1p] = data[col].apply(math.log1p)","081534d0":"min_max_scaler = preprocessing.MinMaxScaler()\n\n# Now let's plot the numerical features, but take the transformed values for the columns we applied log1p to\nnumerical_features_log1p = numerical_features\ndef take_log_col(col):\n    if col in transform_cols: return col + '_log1p'\n    else: return col\nnumerical_features_log1p[:] = [take_log_col(col) for col in numerical_features_log1p]\n\ndataScaled_log1p = pd.DataFrame(min_max_scaler.fit_transform(data[numerical_features_log1p]), columns=numerical_features_log1p)\nax = sns.boxplot(data=dataScaled_log1p, orient=\"h\")\nax.set_title(\"Box plots for min-max scaled features\")","15ad9062":"sns.distplot(data['price_log1p']).set_title(\"Distribution of log(1 + price)\")","88a2c1d6":"from sklearn.model_selection import train_test_split\n\nnumerical_feature_names = ['latitude', 'longitude', 'minimum_nights_log1p','number_of_reviews_log1p','reviews_per_month_log1p', \n                          'days_since_last_review', 'calculated_host_listings_count_log1p', 'availability_365']\nnumerical_features = data[numerical_feature_names]\nscaler = preprocessing.MinMaxScaler()\nnumerical_features = scaler.fit_transform(numerical_features) # Need to scale numerical features for ridge regression\n\n# Combine numerical features with one-hot-encoded features\nfeatures = scipy.sparse.hstack((numerical_features, one_hot_features),format='csr') \nall_feature_names = np.hstack((numerical_feature_names,one_hot_names)) # Store names of all features for later interpretation\n\ntarget_column = ['price_log1p'] # We will fit log(1 + price) \ntarget = data[target_column].values\n\n# Perform train and test split of data\nrand_seed = 51 # For other models we will use the same random seed, so that we're always using the same train-test split\nfeatures_train, features_test, target_train, target_test = train_test_split(\n    features, target, test_size=0.2, random_state=rand_seed)","251dc3bc":"%%time\nfrom sklearn import linear_model\n\nridge_fit = linear_model.RidgeCV(cv=5)\nridge_fit.fit(features_train, target_train)\nprint(\"RidgeCV found an optimal regularization parameter alpha =\",ridge_fit.alpha_)\ntest_score_no_text = ridge_fit.score(features_test,target_test)\nprint(\"Test score for Ridge Regression without text features:\", test_score_no_text)","e0416df1":"from sklearn.feature_extraction.text import CountVectorizer\n\n# Same train-test split as before (same random seed)\ndata_train, data_test = train_test_split(data, test_size=0.2, random_state=rand_seed)\n\ntraining_corpus = data_train['name'].values # Only use the training set to define the features we are going to extract\nvectorizer = CountVectorizer(min_df=3) \n# min_df is the minimum number of times a word needs to appear in the corpus in order to be assigned a vector\nvectorizer.fit(training_corpus)\nnum_words = len(vectorizer.vocabulary_) # Total number of words \nprint(\"Number of distinct words to be used as features:\",num_words)","fa53a907":"full_corpus = data['name'].values\nword_features = vectorizer.transform(full_corpus) # This is a sparse matrix of our word-occurrence features \nwords = vectorizer.get_feature_names() # The actual words corresponding to the columns of the above feature matrix\nword_frequencies = np.array(word_features.sum(axis=0))[0] # The total number of occurrences of each word in the dataset\nprint(\"Shape of word-occurrence feature matrix:\",word_features.shape)","ef32788d":"num_non_text = features.shape[1]\nfeatures_with_text = scipy.sparse.hstack((features, word_features),format='csr') \n# We want to keep the feature matrix in a sparse format for efficiency\nfeature_names = np.hstack((all_feature_names, words))   \n\n# Same train-test split as before (same random seed)\nfeatures_with_text_train, features_with_text_test, target_train, target_test = train_test_split(\n    features_with_text, target, test_size=0.2, random_state=rand_seed)\n\nnum_features = num_non_text + num_words\n\nprint(\"Number of non-text features: \",num_non_text)\nprint(\"Number of vectorized text features (word occurrences): \",num_words)\nprint(\"Features shape including text features: \",features_with_text.shape)","f63a84f6":"%%time\nfrom sklearn import linear_model\n\nridge_fit = linear_model.RidgeCV(cv=5)\nridge_fit.fit(features_with_text_train, target_train)\nprint(\"RidgeCV found an optimal regularization parameter alpha =\",ridge_fit.alpha_)\ntest_score_with_text = ridge_fit.score(features_with_text_test,target_test)\nprint(\"Test score for Ridge Regression WITHOUT text features:\", test_score_no_text)\nprint(\"Test score for Ridge Regression WITH text features:\", test_score_with_text)","e9e51eab":"coefs = ridge_fit.coef_[0] # Coefficients of the linear fit\n\n# I'll make a num_features-sized array of zeros, and then fill the indices corresponding to the word-occurrence features\n# with the total number of counts for the word in that dataset. So features that don't correspond to words have \n# word_counts = 0.\nnum_features = features_with_text.shape[1]\nword_counts = np.zeros(num_features, dtype=int)\nword_counts[num_non_text:] = word_frequencies\n\n# Make a DataFrame of feature names, coefficients, and word counts, and sort it by magnitude of the coefficient.\ncoef_df = pd.DataFrame(data={'names': feature_names, 'coefs': coefs, 'total_word_counts': word_counts})\ncoef_df_sorted = coef_df.reindex(coef_df['coefs'].abs().sort_values(ascending=False).index)","5365fda0":"with pd.option_context('display.max_rows', None): \n    print(coef_df_sorted.head(200))","ce0e3e5c":"vectorizer = CountVectorizer(ngram_range=(1, 2), # Tokenize both one-word and two-word phrases\n                             min_df=3, # Token should occur at least 3 times in training set\n                             token_pattern=\"[a-zA-Z0-9]{1,30}\" # Regular expression defining the form of a word or 1-gram\n                            ) \nvectorizer.fit(training_corpus)\nnum_words = len(vectorizer.vocabulary_) # Total number of words \nprint(\"Number of distinct tokens to be used as features:\",num_words)","d4c2bcc4":"word_features = vectorizer.transform(full_corpus) # This is is a sparse matrix of our word-occurrence features \nwords = vectorizer.get_feature_names() # The actual words corresponding to the columns of the above feature matrix\nword_frequencies = np.array(word_features.sum(axis=0))[0] # The total number of occurrences of each word in the dataset\n\nfeatures_with_text = scipy.sparse.hstack((features, word_features),format='csr') \nfeature_names = np.hstack((all_feature_names, words))   \n\nfeatures_with_text_train, features_with_text_test, target_train, target_test = train_test_split(\n    features_with_text, target, test_size=0.2, random_state=rand_seed)\n\nnum_features = num_non_text + num_words\n\nprint(\"Number of non-text features: \",num_non_text)\nprint(\"Number of vectorized text features (word occurrences): \",num_words)\nprint(\"Features shape including text features: \",features_with_text.shape)","e585a434":"%%time\nfrom sklearn import linear_model\n\nridge_fit = linear_model.RidgeCV(cv=5)\nridge_fit.fit(features_with_text_train, target_train)\nprint(\"RidgeCV found an optimal regularization parameter alpha =\",ridge_fit.alpha_)\ntest_score_with_bigrams = ridge_fit.score(features_with_text_test,target_test)\nprint(\"Test score for Ridge Regression WITHOUT text features:\", test_score_no_text)\nprint(\"Test score for Ridge Regression with single-word tokens:\", test_score_with_text)\nprint(\"Test score for Ridge Regression with bigram tokens:\", test_score_with_bigrams)","bdee564c":"coefs = ridge_fit.coef_[0] # Coefficients of the linear fit\n\n# I'll make a num_features-sized array of zeros, and then fill the indices corresponding to the word-occurrence features\n# with the total number of counts for the word in that dataset. So features that don't correspond to words have \n# word_counts = 0.\nnum_features = features_with_text.shape[1]\nword_counts = np.zeros(num_features, dtype=int)\nword_counts[num_non_text:] = word_frequencies\n\n# Make a DataFrame of feature names, coefficients, and word counts, and sort it by magnitude of the coefficient.\ncoef_df = pd.DataFrame(data={'names': feature_names, 'coefs': coefs, 'total_word_counts': word_counts})\ncoef_df_sorted = coef_df.reindex(coef_df['coefs'].abs().sort_values(ascending=False).index)\n\nwith pd.option_context('display.max_rows', None): \n    print(coef_df_sorted.head(200))","ae17b901":"# Afterword","058c5d35":"We'll need to encode all the categorical features in a way that is friendly for the linear model we will use (ridge regression). One-hot encoding is the default choice. The one-hot encoded features are very sparse so we'll leave them in a SciPY CSR format.","9dc883c5":"### Drop some unused columns","47f0bf26":"Ridge regression has one hyperparameter $\\alpha$, controlling the amount of regularization. SciKit-learn has a handy method \"RidgeCV\" which automatically selects the best value of $\\alpha$ by cross-validation.","84ba79c7":"### Transform data and repeat","8f0c4cad":"### Define feature and target vectors and perform train-test split","8c692832":"Now we'll apply the vectorizer (which was fit to just the training set) to the whole dataset.","94328535":"Note that the above test score is an $R^2$ score, which is derived from mean-squared-error or MSE, which is what ridge regression aims to minimize. But we're actually fitting log(1 + price) with this model, so in terms of price we've actually been trying to minimize the mean-square-logarithmic-error or MSLE. This is a reasonable goal for this problem since we probably care more about fractional accuracy of prices rather than absolute accuracy.  ","ff17627f":"I'm not going to go through much EDA in this kernel, though it's of course an important first step for handling any dataset. So if you haven't done any EDA with this data yet I recommend checking out some of the other kernels for this data on Kaggle. Here I'll just focus on the issue of skewness in the data and a simple fix to make it more suitable for fitting a linear model.","12ed2ff3":"### Check for NaN values","6e6cb0dd":"Let's scan the top 200 features","e4d9be00":"We see that several features (including the eventual target, price) have very skewed distributions. Most of these make sense: For example, \"calculated_host_listings_count\" is skewed since most AirBnB hosts simply rent our their own residence occasionally, but some are professionals managing many listings. \n\nThese skewed distributions are not ideal for fitting linear models. There are many ways to handle this, but I'll take the simple approach of logarithmically transforming the skewed variables, including price. We'll then use a linear model to fit log(1 + price).","ae6438df":"We have seen that the NYC AirBnB dataset is a nice playground for applying some simple approaches for extracting text-based features. Clearly I've only scratched the surface of how one could use the text data. For example, one could try combining the counts for tokens like \"3 bedroom\", \"3bd\", \"3br\", \"three bedroom\" etc. into one feature. Or one may try to extract the size of the rental by looking for numbers preceding the strings \"square feet\", \"sqft\", etc. And of course, one could try fitting different regression models to compare how they perform and how well they utilize these text-based features.\n\nPlease feel free to comment on or critique anything in this kernel!","e28c7bec":"# Interpreting the effect of features, including text","fd69385f":"To properly validate the model, we should define our set of feature vectors by looking at the training set only, and then apply the feature extraction procedure to the test set as well. So for example if some word like \"splendiferous\" appears in the test set but NOT in the training set, we should not create a feature corresponding to it.\n\nApart from setting a \"min_df\" (see below), I use scikit-learn's CountVectorizer \"out of the box.\"","0a5a1922":"# Adding features from text using the \"Bag of Words\" vectorization","07f9ac6b":"Motivated by the above, let's repeat the analysis, but this time ask CountVectorizer to return occurrences of two-word phrases or \"bigrams\" as well as single words (1-grams). Also, by default CountVectorizer only considers words with two or more characters, but since we also want to count phrases like \"3 bedroom\" we will change the token_pattern used to identify words to include single-character words.","101ae6c8":"### Fit ridge regression again and compare","6300f2f5":"We get a bit of improvement in the score using bigrams. Let's see what the coefficients of the features look like now.","296ccdb2":"### Look at scaled box plot for outliers etc.","863884ff":"Now we're ready to start training a model to predict prices.","debc878c":"This gives some insight into what factors drive the AirBnB prices. \"superbowl\" having the largest positive coefficient probably indicates that prices were greatly inflated around the time of the 2014 Superbowl which took place in the area. Locations suitable for \"events\" or \"shoots\" charge a premium. We can also see which neighborhoods have higher or lower prices. \n\nWe also note that features like \"4bd\" and \"5bath\" are useful, but actually most listings are probably conveying that information in multiple words, i.e. \"4 bedroom\". The approach of counting invidiual words does not capture info like this that is usually expressed in two words. This motivates us to try extracting features corresponding to two-word phrases as well.","90801354":"# Some very minimal EDA: Dealing with skewness\/outliers","d7fbeeeb":"### Show dimensions, columns, and datatypes","c646ee87":"### Fill some NaNs","b89ff163":"# Introduction","afc1093f":"### Combine previous features and vectorized text features","2f4f44bc":"First we'll use only the numerical features and one-hot-encoded categorical features as predictors. The numerical features will need to be scaled to be used in a regularized linear model.","d087dce0":"![](http:\/\/)This is a little nicer-- the outliers are a lot less extreme (the max values are only few times larger than the quartiles, rather than 100x larger).  ","cb466d6b":"### Fit the model and check $R^2$ scores","a42b32fb":"### Convert date strings to DateTime and fill NaNs","085bbf97":"# Import raw data and inspect","d10db464":"Just to get an idea of what the one-hot encoded features look like, let's take a peek at them in DataFrame format","4183f2f9":"### Apply vectorizer to full dataset","8ac10080":"### Vectorize the text feature 'name'","ff8a3182":"Just for fun let's take a look at how the different features actually contribute the fit, i.e. what are their coefficients in the linear fit. For example, if the feature corresponding to the word \"luxurious\" as a large positive coefficient, then it is correlated with higher prices, and vice versa for negative coefficients.","309074ac":"This kernel is meant to give a simple example of feature extraction from text data and its usefulness for modeling. This is a subject I am just beginning to learn about, and I think that the NYC AirBnB dataset provides a nice practical demonstration. So hopefully this kernel can be useful to others interested in learning about extracting useful features from text.\n\nMany other kernels for this dataset go through exploratory data analysis (EDA) and fit a variety of models. I will restrict my focus to using ridge regression to predict AirBnB prices-- first without using the textual data, and then again using text features derived from a Bag of Words representation. The latter gives a significant improvement in target score, and the feature coefficients have interesting interpretations.","e282b75f":"A pretty nice improvement! And still pretty quick to train despite the large number of features. Using a sparse format for the features was important for efficiency, I find that it takes about 50x longer if we use an ordinary dense numpy array.","b099b47e":"# Using bigrams to extract better features","d9294254":"We'll fit using ridge regression exactly as before (with the hyperparameter $\\alpha$ automatically chosen by cross-validation).","d55229d3":"### Add one-hot encoding for categorical features","d323b63b":"Let's just add these vectorized text features to the features we use previously.","b0a19623":"# Import packages","549605ae":"Here's our first look at the data. Note that there are no feature columns for things like \"number of bedrooms\" or other descriptions of the rental besides the \"room_type.\" But in the 'name' column each row has a description written by the host, which seems like it could be a very rich source of additional features.","50dc6232":"# Cleaning the data","083b4874":"We see that phrases like \"5 bedroom\" are indeed being used as features, which may account for he improvement in score. However, certain features are redundant, like \"bowl\" and \"super bowl\" which always occur simultaneously. An improved approach to feature extraction might avoid such redundancies.","f538fe07":"Now let's try to use the rich text data in the 'name' column to improve our predictions. I'm going to use one of the simplest approaches to feature extraction from text, namely the \"Bag of Words\" model. This simply counts the occurence of words (e.g. \"spacious\" or \"budget\") in each text entry, without attempting to keep track of order or sentence structure. For each word it creates a feature column which records the count of that word in each text sample. Since our data has many short text entries, the resulting feature space will be high-dimensional but sparse.\n\nSee https:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html#text-feature-extraction for an explanation of Bag of Words and its implementation in scikit-learn.","bf963974":"# Ridge Regression without using text features"}}