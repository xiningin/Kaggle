{"cell_type":{"75041118":"code","f109ce8a":"code","11c43741":"code","5f7c17aa":"code","ac6d35ad":"code","09b3be9f":"code","347511a0":"code","e833de94":"code","c855e4bb":"code","9839f491":"code","25980ce8":"code","037bb0d6":"code","91a20ab3":"code","89182f61":"code","17611f1d":"code","b3bef80b":"code","39ce3376":"code","e36c6bc7":"code","6e5d5794":"code","385c6325":"code","d0d9461d":"markdown","70b960b2":"markdown","19884fa6":"markdown","0a40ae0a":"markdown","138a086b":"markdown","49c10254":"markdown","9caaaacd":"markdown","473315ee":"markdown","71f0d5b2":"markdown","cea12490":"markdown","4d8c5e95":"markdown","1cb7ce69":"markdown","d16259f4":"markdown","3e0bdce5":"markdown","b626bc4f":"markdown","78e3c0da":"markdown","733cb18f":"markdown","20514d20":"markdown","2c491b4f":"markdown","de5340a7":"markdown"},"source":{"75041118":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f109ce8a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import RobustScaler\nfrom scipy import stats\nimport numpy as np","11c43741":"dfs = pd.read_csv('..\/input\/yeh-concret-data\/Concrete_Data_Yeh.csv')","5f7c17aa":"dfs.head()","ac6d35ad":"dfs.columns = ['cement', 'blast_furnace_slag', 'fly_ash','water','superplast','course_aggregate','fine_aggregate','age','compressive_strength']\n#Checking for null values\ndfs.isnull().sum()","09b3be9f":"dfs.describe()","347511a0":"plt.figure(figsize = (20,10))\nplt.title(\"Correlation Heatmap\")\nsns.heatmap(dfs.corr(), annot=True,cmap=\"YlGnBu\")","e833de94":"fig, ax = plt.subplots() #initialization","c855e4bb":"import matplotlib.pyplot as plt\n# Green markers indicating outliers in the feature\n\ngreen_diamond = dict(markerfacecolor='g', marker='D')\n\n\nax1 = fig.add_subplot(111)\nax2 = fig.add_subplot(122)\nax3 = fig.add_subplot(133)\n\n\n\nfig, (ax1, ax2,ax3) = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n\nax1.set_title('Cement')\nax1.boxplot(dfs['cement'], flierprops=green_diamond)\n\nax2.set_title('Blast Furnace Slag')\nax2.boxplot(dfs['blast_furnace_slag'], flierprops=green_diamond)\n\nax3.set_title('Fly Ash')\nax3.boxplot(dfs['fly_ash'], flierprops=green_diamond)","9839f491":"ax4 = fig.add_subplot(111)\nax5 = fig.add_subplot(122)\nax6 = fig.add_subplot(133)\nfig, (ax4, ax5, ax6) = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n\nax4.set_title('Water')\nax4.boxplot(dfs['water'], flierprops=green_diamond)\n\nax5.set_title('Super Plasticizer')\nax5.boxplot(dfs['superplast'], flierprops=green_diamond)\n\nax6.set_title('Coarse Aggregate')\nax6.boxplot(dfs['course_aggregate'], flierprops=green_diamond)\n\n","25980ce8":"ax7 = fig.add_subplot(111)\nax8 = fig.add_subplot(122)\nax9 = fig.add_subplot(133)\n\nfig, (ax7, ax8, ax9) = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n\nax7.set_title('Fine Aggregate')\nax7.boxplot(dfs['fine_aggregate'], flierprops=green_diamond)\n\nax8.set_title('Age')\nax8.boxplot(dfs['age'], flierprops=green_diamond)\n\nax9.set_title('Compressive Strength')\nax9.boxplot(dfs['compressive_strength'], flierprops=green_diamond)","037bb0d6":"from sklearn.model_selection import train_test_split\nX= dfs.iloc[:, dfs.columns != 'compressive_strength']\ny = dfs.iloc[:, dfs.columns == 'compressive_strength']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=0)\n\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\n#Predicted Variable\ny_pred = regressor.predict(X_test)\n\n# MAE\nmae = mean_absolute_error(y_test, y_pred)\n\n#RMSE \nrmse = mean_squared_error(y_test,y_pred, squared = False)\n\n#R2_Score\nr2 = r2_score(y_test,y_pred)\nprint(\"For a linear model the errors are as follows-\")\nprint(\"Mean Absolute Error:\", mae)\nprint(\"Root Mean Squared Error:\", rmse)\nprint(\"R2 Score:\",r2)","91a20ab3":"#Visualizing the correlation among the variables\n\ng = sns.pairplot(dfs, diag_kind=\"kde\")\ng.map_lower(sns.kdeplot, levels=4, color=\".2\")","89182f61":"#Plotting a Compressive Strength vs Cement, Age, Water plot\nfig_dims = (15, 8)\nfig, ax = plt.subplots(figsize=fig_dims)\nx = dfs['compressive_strength']\ny= dfs['cement']\nsns.scatterplot(y=y, x=x, hue=\"water\",size=\"age\", data=dfs, sizes=(50, 300))\nax.set_title('Compressive Strength vs Cement, Age, Water')","17611f1d":"dfs = pd.read_csv('..\/input\/yeh-concret-data\/Concrete_Data_Yeh.csv')\nfrom sklearn.preprocessing import RobustScaler\nrs = RobustScaler()\ndfs_scaled = pd.DataFrame(rs.fit_transform(dfs),columns = dfs.columns)\n\nX= dfs_scaled.iloc[:, dfs.columns != 'compressive_strength']\ny = dfs_scaled.iloc[:, dfs.columns == 'compressive_strength']\n\n\n#Removing outliers using interquartile range\nQ1 = dfs.quantile(0.25)\nQ3 = dfs.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)\ndfs_out = dfs[~((dfs < (Q1 - 1.5 * IQR)) |(dfs > (Q3 + 1.5 * IQR))).any(axis=1)]","b3bef80b":"# Pearson's correlation feature selection for numeric input and numeric output\nfrom sklearn.datasets import make_regression\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\n# generate dataset\nX= dfs_out.iloc[:, [0,1,2,3,4,5,6,7]]\ny = dfs_out.iloc[:, 8]\n\n# define feature selection SELECTING THE BEST FEATURES 7 OUT OF 8\nfs = SelectKBest(score_func=f_regression, k=7)\n# apply feature selection\nX_selected = fs.fit_transform(X, y)\nprint(X_selected.shape)\n","39ce3376":"X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size= 0.2, random_state=0)\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\ny_pre = regressor.predict(X_test)\nfrom sklearn.metrics import mean_absolute_error\nmaep = mean_absolute_error(y_test, y_pre)\n\nrmsep = rmse = mean_squared_error(y_test,y_pre, squared = False)\nfrom sklearn.metrics import r2_score\nr2p = r2_score(y_test,y_pre)\n\nprint(\"For a linear model the errors are as follows-\")\nprint(\"Mean Absolute Error:\", maep)\nprint(\"Root Mean Squared Error:\", rmsep)\nprint(\"R2 Score:\",r2p)","e36c6bc7":"from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.svm import SVR\nX= dfs.iloc[:, dfs.columns != 'compressive_strength']\ny = dfs.iloc[:, 8]\nestimator = SVR(kernel=\"linear\")\nselector = RFE(estimator, n_features_to_select=5, step=1)\nselector = selector.fit(X, y)\nselector.ranking_","6e5d5794":"#Selecting the number 1 ranking variables only\n# The variables are: cement, blast_furnace_slag,  water, superplast and age\nX= dfs_out.iloc[:, [0,1,3,4,7]]\ny = dfs_out.iloc[:, 8]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=0)\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\ny_pre = regressor.predict(X_test)\nfrom sklearn.metrics import mean_absolute_error\nmaef = mean_absolute_error(y_test, y_pre)\n\nrmsef = mean_squared_error(y_test,y_pre, squared = False)\nfrom sklearn.metrics import r2_score\nr2f = r2_score(y_test,y_pre)\n\nprint(\"For a linear model the errors are as follows-\")\nprint(\"Mean Absolute Error:\", maef)\nprint(\"Root Mean Squared Error:\", rmsef)\nprint(\"R2 Score:\",r2f)","385c6325":"X= dfs_scaled.iloc[:, dfs.columns != 'compressive_strength']\ny = dfs_scaled.iloc[:, 8]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=0)\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 4)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\nexplained_variance = pca.explained_variance_ratio_\n\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\ny_pr = regressor.predict(X_test)\n\nfrom sklearn.metrics import mean_absolute_error\nmaepca = mean_absolute_error(y_test, y_pr)\n\nrmsepca = mean_squared_error(y_test,y_pr, squared = False)\nfrom sklearn.metrics import r2_score\nr2pca = r2_score(y_test,y_pr)\n\nprint(\"For a linear model the errors are as follows-\")\nprint(\"Mean Absolute Error:\", maepca)\nprint(\"Root Mean Squared Error:\", rmsepca)\nprint(\"R2 Score:\",r2pca)","d0d9461d":"#### Observations from the plot are:\n* Compressive Strength increases when Cement composition increases.\n* Compressive Strength increases with Age as darker colors indicate greater age.\n* Compressive Strength increases when less Water is used.","70b960b2":"Welcome to my analysis on Concrete Data. From exploring the data to fitting linear models and using feature engineering and\/or selection methods to improve the fit of data on the linear model. I also explored the impact of outliers on the fitting of data onto the linear model.","19884fa6":"Reading the dataset","0a40ae0a":"A quick summary of the values in the dataset. Later we will check for outliers.","138a086b":"Selecting the features using Principal Component Analysis","49c10254":"The green diamonds indicate the outlier points in the specific variable ","9caaaacd":"### Plotting the correlation heatmap for all the features of the dataset\n","473315ee":"As you can see there are some outliers in the data mostly in water, compression strength and age. The impact of these can be accessed during predictive modelling.","71f0d5b2":"The following linear model is the most simple model possible. The data is blindly fitted onto the model. The error metrics are defined in the next cell","cea12490":"Using different techniques of feature selection and\/or feature engineering to improve the performance of the model from last task. Calculate MAE, RMSE and R^2 performance metric\n\nFirst, we will visualize the data and make observations about the features.","4d8c5e95":"## Feature Engineering and Selection Methods","1cb7ce69":"Here we obtain the best ranking features using SVR estimator.\nThe best ranking features are: cement, blast_furnace_slag,  water, superplast and age","d16259f4":"#### There is a 16% improvement over a simple linear model\nSelecting the features using Feature ranking with recursive feature elimination method is used to assess the model\n","3e0bdce5":"### Thank you for reading. Any suggestions or feedback is welcome!","b626bc4f":"# **Conclusion**","78e3c0da":"The KDE distributions are a bit off due to the outliers in the data.","733cb18f":"## Checking for outliers\nThe easiest way to check for outliers is by plotting boxplots","20514d20":"## Fitting the data onto a Linear Model\n\n* Split the data set in train and test sets with a proprtion of 20% for test set\n* Apply linear regression\n* Calculate MAE, RMSE and R^2 performance metric for the above model","2c491b4f":"Using Pearson's correlation coefficient to find the we obtain the highest Rsquared coefficient indicating a better fit fo the model. However using a different algorithm to predict the outcomes such as decision trees or random forest might be an alternative approach but they tend to overfit the data easily.\n\nUsing PCA methods with 4 features best fits the data into a linear model.\n\nThe best fitting linear model is with using PCA with R2_Score of ~0.8779\n\nAlso, it can be concluded that removal of outliers tends to improve the fit of the data onto a linear model. \n","de5340a7":"Now the efficiency of model is accessed by first scaling the data to standardize and then removing the outliers"}}