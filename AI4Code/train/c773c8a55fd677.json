{"cell_type":{"416ac575":"code","279291ae":"code","9e476272":"code","242aaf59":"code","31617212":"code","0235c7c7":"code","04de5ef1":"code","1d7a8ee8":"code","3467d2e9":"code","692fe74a":"code","143d4b29":"code","0f4a7d49":"code","0027128a":"code","7da7e024":"code","bd701b9f":"code","d4c0effa":"code","4fb4bec6":"code","eb25251b":"code","c2dd4b6e":"code","38d8b01e":"code","ee8af9af":"code","2eb1e7cd":"code","2d2dede5":"code","d5de4c43":"code","edd616e1":"code","2aa29a35":"code","2b44438c":"code","7b1cf5d6":"code","c86aab7c":"code","ad8410ab":"code","8f3b1145":"code","adec2e18":"code","058502b2":"code","833cda15":"code","897dfe6c":"code","599f6333":"markdown","db080b0f":"markdown","02111818":"markdown","8a878abe":"markdown","4304b513":"markdown","5fc964e9":"markdown","c05ef7dd":"markdown","96b935b1":"markdown","9e645819":"markdown","e5c83015":"markdown","22f4f972":"markdown","34a3132f":"markdown","7b24a03c":"markdown","f0406397":"markdown","31335ee9":"markdown","c91e7667":"markdown","93732e68":"markdown","3bd95e9e":"markdown","ef4548dd":"markdown","3997258e":"markdown","13a1de27":"markdown","4b142a43":"markdown","47b54b16":"markdown","090c5fdd":"markdown","dcb9c813":"markdown","b495a560":"markdown","ac9a5c0d":"markdown","e315057c":"markdown","f3c39b20":"markdown"},"source":{"416ac575":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.decomposition import PCA\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom yellowbrick.cluster import SilhouetteVisualizer\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.mixture import GaussianMixture\nfrom kmodes.kprototypes import KPrototypes","279291ae":"# Load data\ndf = pd.read_csv('..\/input\/customer-personality-analysis\/marketing_campaign.csv',sep='\\t')\ndf.head()","9e476272":"df.describe(include='all') ","242aaf59":"#Dropping redundant features\ndf.drop(columns=['ID','Z_CostContact','Z_Revenue'],inplace=True)\n\ndf.info()","31617212":"# exploring the categorial features\nprint(df[\"Marital_Status\"].value_counts())\nprint(df[\"Education\"].value_counts())","0235c7c7":"df[\"Marital_Status\"].replace({'Alone': 'Single', 'Absurd': 'Single','YOLO':'Single'},inplace=True)\n\n# calculate the age instead of 'Year-Birth'\ndf['Year_Birth']= 2021-df['Year_Birth']\ndf.rename(columns={\"Year_Birth\": \"Age\"},inplace=True)\n\n#Total spendings on all items\ndf[\"Total_Spent\"] = df[\"MntWines\"]+ df[\"MntFruits\"]+ df[\"MntMeatProducts\"]+ df[\"MntFishProducts\"]+ df[\"MntSweetProducts\"]+ df[\"MntGoldProds\"]","04de5ef1":"# creating a new feature indicates the number of days customer engaged to the company\ndf['Dt_Customer'] = pd.to_datetime(df.Dt_Customer)\nnewest_customer = df['Dt_Customer'].max()\ndf['newest_customer'] = newest_customer\ndf['days_engaged'] = (df['newest_customer'] - df['Dt_Customer']).dt.days\nprint(df['days_engaged'])\n\ndf.drop(columns=['Dt_Customer','newest_customer'],inplace=True)","1d7a8ee8":"df.describe()","3467d2e9":"# treating NaN values\ndf.isna().sum()","692fe74a":"df.dropna(inplace = True)","143d4b29":"# Outliers\/Noise exploration\nplt.figure()\nsns.pairplot(df[['Income','Age', \"Total_Spent\"]]) #checking on the relevant features\nplt.show()","0f4a7d49":"# Dropping the outliers \ndf = df[(df[\"Age\"]<100)]\ndf = df[(df[\"Income\"]<600000)]","0027128a":"# num of samples after cleaning\nprint(len(df))","7da7e024":"plt.figure(figsize=(16,9))\nsns.heatmap(df.corr(), cmap='viridis', annot = True)\nplt.title('Correlation Matrix')\nplt.show()","bd701b9f":"# Encode ordinal features with OrdinalEncoder\neducation_order = ['Basic', '2n Cycle','Graduation','Master', 'PhD']\noe = OrdinalEncoder(categories = [education_order], dtype=int)\neducation_oe = oe.fit_transform(df[['Education']])\ndf_enc= df.assign(Education_encode=education_oe)\nprint(df_enc.shape)\nprint(df_enc[['Education', 'Education_encode']])","d4c0effa":"# Encode nominal features with OneHotEncoder\nohe =  OneHotEncoder(sparse=False, dtype='int')\nMarital_ohe = ohe.fit_transform(df[['Marital_Status']])\nMarital_ohe = pd.DataFrame(data=Marital_ohe,columns=ohe.get_feature_names(['Marital_Status']), index=df.index,)\ndf_enc = pd.concat([df_enc,Marital_ohe],axis=1)\ndf_enc.drop(columns=['Marital_Status','Education'],inplace=True)","4fb4bec6":"df_enc.info()","eb25251b":"# binary(dummy) features do not require normalisation\nbinary_columns = ['Marital_Status_Divorced','Marital_Status_Married', 'Marital_Status_Single','Marital_Status_Together','Marital_Status_Widow'\n                 ,'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1','AcceptedCmp2', 'Complain', 'Response']\ndf_to_scaler = df_enc.drop(columns=binary_columns)\n\n#scaling the features\nscaler = StandardScaler().fit_transform(df_to_scaler)\n\n#creating a dataframe and returning the binary features after scaling the numeric data\nscaled_df = pd.DataFrame(scaler,columns= df_to_scaler.columns )\nbinary_series = df_enc[binary_columns]\nscaled_df = pd.concat([scaled_df,binary_series],axis=1)\n\n#filling the nan values generated\nscaled_df.isna().sum()\nscaled_df = scaled_df.fillna(0)","c2dd4b6e":"#Initiating PCA to reduce dimentions features to 3\npca = PCA(n_components=3)\npca.fit(scaled_df)\nPCA_df = pd.DataFrame(pca.transform(scaled_df), columns=([\"feature1\",\"feature2\", \"feature3\"]))","38d8b01e":"# A 3D Projection Of Data In The Reduced Dimension\nplt.figure(figsize=(10,8))\nplt.axes(projection='3d').scatter(PCA_df[\"feature1\"], PCA_df[\"feature2\"], PCA_df[\"feature3\"])\nplt.title(\"A 3D Projection Of Data In The Reduced Dimension\")\nplt.show()","ee8af9af":"Elbow_M = KElbowVisualizer(KMeans(), k=(2,11))\nElbow_M.fit(PCA_df)\nElbow_M.show()","2eb1e7cd":"for k in range(2,11):\n    model = KMeans(k)\n    visualizer = SilhouetteVisualizer(model, colors='yellowbrick')\n    visualizer.fit(PCA_df) \n    visualizer.show()  ","2d2dede5":"#Initiating the Agglomerative Clustering model \nAC = AgglomerativeClustering(n_clusters=4)\n# fit model and predict clusters\nyhat_AC = AC.fit_predict(PCA_df)\nPCA_df[\"Clusters\"] = yhat_AC\n#Adding the Clusters feature to the orignal dataframe.\nscaled_df[\"Clusters\"]= yhat_AC","d5de4c43":"#Plotting the clusters\nfig = plt.figure(figsize=(10,8))\nplt.axes(projection='3d').scatter(PCA_df[\"feature1\"], PCA_df[\"feature2\"], PCA_df[\"feature3\"], c=PCA_df[\"Clusters\"], marker='o', cmap = 'viridis')\nplt.title(\"The Plot Of The Clusters by Agglomerative model\")\n","edd616e1":"pl = sns.scatterplot(data = scaled_df,x=scaled_df[\"Total_Spent\"], y=scaled_df[\"Income\"],hue=scaled_df[\"Clusters\"])\npl.set_title(\"Cluster's Profile Based On Income And Spending\")\nplt.legend()\nplt.show()","2aa29a35":"#Initiating the BBSCAN Clustering model \nDB = DBSCAN(eps=0.1, min_samples=5)\n# fit model and predict clusters\nlabels = DB.fit_predict(PCA_df)\nPCA_df[\"Clusters\"] = labels\n#Adding the Clusters feature to the orignal dataframe.\nscaled_df[\"Clusters\"]= labels","2b44438c":"#Plotting the clusters\nfig = plt.figure(figsize=(10,8))\nax = plt.subplot(111, projection='3d')\nax.scatter(PCA_df[\"feature1\"], PCA_df[\"feature2\"], PCA_df[\"feature3\"], c=PCA_df[\"Clusters\"], marker='o', cmap = 'viridis' )\nax.set_title(\"The Plot Of The Clusters by DBSCAN model \")\nplt.show()","7b1cf5d6":"#Initiating the KMeans Clustering model \nkmeans = KMeans(n_clusters =4 , init = 'k-means++', random_state = 50)\n# fit model and predict clusters\nlabels = kmeans.fit_predict(PCA_df)\nPCA_df[\"Clusters\"] = labels\n#Adding the Clusters feature to the orignal dataframe.\nscaled_df[\"Clusters\"]= labels\n\n\n#Plotting the clusters\nfig = plt.figure(figsize=(10,8))\nax = plt.subplot(111, projection='3d', label=\"bla\")\nax.scatter(PCA_df[\"feature1\"], PCA_df[\"feature2\"], PCA_df[\"feature3\"], s=40, c=PCA_df[\"Clusters\"], marker='o', cmap = 'viridis' )\nax.set_title(\"The Plot Of The Clusters\")\nplt.show()","c86aab7c":"#Initiating the GMM Clustering model \ngmm = GaussianMixture(n_components = 4, covariance_type = 'spherical', max_iter = 3000, random_state = 228).fit(PCA_df)\n# fit model and predict clusters\nlabels = gmm.predict(PCA_df)\nPCA_df[\"Clusters\"] = labels\n#Adding the Clusters feature to the orignal dataframe.\nscaled_df[\"Clusters\"]= labels\n\n#Plotting the clusters\nfig = plt.figure(figsize=(10,8))\nax = plt.subplot(111, projection='3d', label=\"bla\")\nax.scatter(PCA_df[\"feature1\"], PCA_df[\"feature2\"], PCA_df[\"feature3\"], s=40, c=PCA_df[\"Clusters\"], marker='o', cmap = 'viridis' )\nax.set_title(\"The Plot Of The Clusters\")\nplt.show()","ad8410ab":"df_copy = df","8f3b1145":"numerical_col = ['Age','Income','Kidhome','Teenhome','Recency','MntWines','MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts',\n             'MntGoldProds','NumDealsPurchases','NumWebPurchases','NumCatalogPurchases','NumStorePurchases','NumWebVisitsMonth','Total_Spent','days_engaged']\n\nscaled_numerical = StandardScaler().fit_transform(df_copy[numerical_col])\ndf_copy[numerical_col] = scaled_numerical","adec2e18":"# K-prototypes model gets a numpy array, thus converting the df to an array\ndf_to_array = df_copy.values\n\n#converting numerical columns datatype as float\nnumerical_col_index = [0,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,25,26]\ndf_to_array[:,numerical_col_index] = df_to_array[:,numerical_col_index].astype(float)\n\n#index of categorical columns\ncategorical_index = list(range(1,3)) + list(range(18,25))","058502b2":"# Initiating the KPrototypes Clustering model \nKPro = KPrototypes(n_clusters=4, init='Huang', random_state=42, n_jobs=-1)\n# fit model and predict clusters\nKPro.fit_predict(df_to_array, categorical=categorical_index)\nprint(KPro.cost_)\n#new column for cluster labels associated with each subject\ndf_copy['clusters'] = KPro.labels_","833cda15":"pd.set_option('max_rows',None)\ndf_copy['clusters'] = KPro.labels_\ndf_copy.groupby('clusters').agg(['median' ,'mean']).T","897dfe6c":"pl = sns.scatterplot(data = df_copy,x=df_copy[\"Total_Spent\"], y=df_copy[\"Income\"],hue=df_copy['clusters'])\npl.set_title(\"Cluster's Profile Based On Income And Spending\")\nplt.legend()\nplt.show()","599f6333":"**Silhouette score - Plotting a Silhouette score to find the optimum number of clusters**\n","db080b0f":"The max-age is 128 years, As I calculated the age that would be today (i.e. 2021) and the data is old.","02111818":"some evaluations:","8a878abe":"**elbow curve - Plotting an elbow curve to find the optimum number of clusters**","4304b513":"# GMM","5fc964e9":"**Scaling numerical features**","c05ef7dd":"# Feature Engineering","96b935b1":"# Exploratory Data Analysis","9e645819":"**Features Scaling**","e5c83015":"All features are now numerical","22f4f972":"Now I'll try to run K-Prototypes model.\n\n\"K-prototypes\" is a clustering method based on partitioning. Its algorithm is an improvement of the K-Means and K-Mode clustering algorithm to handle clustering with the mixed data types - data with a mix of categorical and numeric features.\nIt uses a distance measure which mixes the Hamming distance for categorical features and the Euclidean distance for numeric features.\n\nI will make a copy of the original df for this model, since there is no need to perform encoding to the categorical features.","34a3132f":"# Agglomerative","7b24a03c":"# Dimensionality reduction with PCA","f0406397":"# Clustering","31335ee9":"**Encode categorical features**\n* we use OrdinalEncoder for ordinal data\n* OneHotEncoder for nominal(unordered) data","c91e7667":"Preprocessing the data to perform clustering operations","93732e68":"# K-Prototypes","3bd95e9e":"# Data Preprocessing","ef4548dd":"we can see the that features \"Z_CostContact\" and \"Z_Revenue\" are non-informative, contains only 1 value, thus w'll remove them.","3997258e":"The elbow score indicates that 4 will be an optimal number of clusters for this data.","13a1de27":"Looks like the silhouette also gives the best score with k= 2","4b142a43":"(high correlation between \"Total_Spent\" and \"Income\",\"Total_Spent\" and \"Mnt..\" which expected)","47b54b16":"we can see that:\n\n1. **Dt_Customer** is type object instead of DateTime.\n2. **Education** and **Marital_Status** are categorical features- as there are in dtype: object. So w'll need to encode them into numeric forms later.\n3. **Income** has missing values, w'll clean them.\n","090c5fdd":"# DBSCAN","dcb9c813":"**Evaluation**","b495a560":"# K-Means","ac9a5c0d":"There are a few outliers in the Income and Age features. I'll delete them.","e315057c":"**Looking at the correlation amongst the features**","f3c39b20":"# **Data Cleaning**"}}