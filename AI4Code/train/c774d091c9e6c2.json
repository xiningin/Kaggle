{"cell_type":{"223151bc":"code","3a1c128f":"code","2589782d":"code","18b4c558":"code","c30f758b":"code","d9f58f30":"code","639bb40d":"code","f1355abd":"code","eb926154":"code","e93d9f73":"code","ec36b5a2":"code","802ebada":"code","13947e72":"code","8428a61a":"code","c757cdc4":"code","cc522e34":"code","9d295171":"code","e6ad86b6":"code","7bd3529d":"code","b4ab6590":"code","815fb73f":"code","822ae1e7":"code","b43fb688":"code","1c93875f":"code","18c491a4":"code","1900444d":"code","8ec47345":"code","906ce224":"code","b1445a5c":"code","2365792f":"code","db5b6223":"code","26b90a6c":"code","c4a2207c":"code","345e3784":"code","bbabf028":"code","1e125b72":"code","4c1afcf0":"code","a851bbf4":"code","18d164a3":"code","f696a652":"code","2518cefc":"code","81b2c66c":"code","5b4de9d8":"code","d0edcf0c":"code","9a9bf629":"code","e33cd1c3":"code","e3c5002a":"code","358059d6":"code","06c29d4c":"code","808e6564":"markdown","d4117b9e":"markdown","8b4567f2":"markdown","bfcc9c74":"markdown","d746e012":"markdown","b830c3ff":"markdown","0c3ecd09":"markdown","6c9e8cb3":"markdown","955c21fa":"markdown","af9d4bf2":"markdown","f3b163a9":"markdown","73814c7d":"markdown","d723db75":"markdown","0d1b4224":"markdown","85107622":"markdown","95dedf46":"markdown","40ad8ca2":"markdown","51fc568a":"markdown","7e4f6565":"markdown","f7462db6":"markdown","5925327e":"markdown","65380d4d":"markdown","9cc81d6f":"markdown","762daea0":"markdown"},"source":{"223151bc":"#Importing Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3a1c128f":"df = pd.read_csv('\/kaggle\/input\/diabetes\/diabetes.csv')","2589782d":"df.head()  #Returns first 5 rows of DataFrame, if parameter is passed i.e. head(n) it returns 'n' rows of DataFrame","18b4c558":"df.shape   #Returns shape of DataFrame i.e. number of rows and columns  ","c30f758b":"df.columns   #Returns the column name of DataFrame","d9f58f30":"df.groupby('Outcome').size()","639bb40d":"df.dtypes     #Returns the data type of each column","f1355abd":"df.info()    #Returns a concise summary of DataFrame","eb926154":"df.describe().T    #Returns basic statistics on all numeric columns","e93d9f73":"df.isna().any()    #Returns True if there is missing value, else False","ec36b5a2":"df = df.rename(columns={'BloodPressure':'BP', 'DiabetesPedigreeFunction':'DPF'})","802ebada":"df.head()","13947e72":"sns.countplot(x='Outcome', data=df)\nplt.xlabel('Diabetic')\nplt.ylabel('Count')\nplt.xticks([0, 1], ['No', 'Yes'])\nplt.show()","8428a61a":"df.hist(figsize=(15,15))\nplt.show()","c757cdc4":"print('Total:',df[df['BP']==0].shape[0])","cc522e34":"df[df['BP']==0].groupby('Outcome')['Age'].count()","9d295171":"print('Total:',df[df['Glucose']==0].shape[0])","e6ad86b6":"df[df['Glucose']==0].groupby('Outcome')['Age'].count()","7bd3529d":"print('Total:',df[df['SkinThickness']==0].shape[0])","b4ab6590":"df[df['SkinThickness']==0].groupby('Outcome')['Age'].count()","815fb73f":"print('Total:', df[df['BMI']==0].shape[0])","822ae1e7":"df[df['BMI']==0].groupby('Outcome')['Age'].count()","b43fb688":"print('Total:', df[df['Insulin']==0].shape[0])","1c93875f":"df[df['Insulin']==0].groupby('Outcome')['Age'].count()","18c491a4":"df[['Glucose', 'BP', 'SkinThickness', 'Insulin',\n         'BMI', 'DPF']] = df[['Glucose', 'BP', 'SkinThickness', 'Insulin',\n       'BMI', 'DPF']].replace(0, np.NaN)","1900444d":"df.head()","8ec47345":"df['Glucose'].fillna(df['Glucose'].mean(), inplace=True)\ndf['BP'].fillna(df['BP'].mean(), inplace=True)\ndf['SkinThickness'].fillna(df['SkinThickness'].median(), inplace=True)\ndf['Insulin'].fillna(df['Insulin'].median(), inplace=True)\ndf['BMI'].fillna(df['BMI'].median(), inplace=True)","906ce224":"df.head()","b1445a5c":"df.hist(figsize=(15,15))\nplt.show()","2365792f":"df.isnull().sum()","db5b6223":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import ShuffleSplit\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier","26b90a6c":"X = df.drop(columns=['Outcome'])\n\ny = df['Outcome']","c4a2207c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=df.Outcome, random_state=0)\n\nprint('X_train size: {}, X_test size: {}'.format(X_train.shape, X_test.shape))","345e3784":"ss = StandardScaler()\nX_train = ss.fit_transform(X_train)\nX_test = ss.transform(X_test)","bbabf028":"def best_model(X, y):\n    models = {\n        'LR': {\n            'model': LogisticRegression(solver='lbfgs', multi_class='auto'),\n            'parameters': {\n                'C': [1,5,10]\n               }\n        },\n       \n        'DT': {\n            'model': DecisionTreeClassifier(splitter='best'),\n            'parameters': {\n                'criterion': ['gini', 'entropy'],\n                'max_depth': [5,10]\n            }\n        },\n        \n        'RF': {\n            'model': RandomForestClassifier(criterion='gini'),\n            'parameters': {\n                'n_estimators': [10,15,20,50,100,200]\n            }\n        },\n         \n        'KNN': {\n            'model': KNeighborsClassifier(algorithm='auto'),\n            'parameters': {\n                'n_neighbors': [5,10,15,20,25],\n                'weights' : ['uniform', 'distance'] \n               }\n        },\n        \n        'SVC': {\n            'model': SVC(gamma='auto'),\n            'parameters': {\n                'C': [1,10,20],\n                'kernel': ['rbf','linear']\n            }\n        },\n        \n        'GB': {\n            'model': GradientBoostingClassifier(criterion='friedman_mse'),\n            'parameters': {\n                'loss': ['deviance', 'exponential']\n               }\n        }\n\n    }\n    \n    scores = [] \n    cv_shuffle = ShuffleSplit(n_splits=5, test_size=0.20, random_state=0)\n        \n    for model_name, model_params in models.items():\n        gs = GridSearchCV(model_params['model'], model_params['parameters'], cv = cv_shuffle, return_train_score=False)\n        gs.fit(X, y)\n        scores.append({\n            'model': model_name,\n            'best_parameters': gs.best_params_,\n            'score': gs.best_score_\n        })\n        \n    return pd.DataFrame(scores, columns=['model', 'best_parameters', 'score'])\n\nbest_model(X_train, y_train)","1e125b72":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(RandomForestClassifier(n_estimators=20, random_state=0), X_train, y_train, cv=5)\nprint('Average Accuracy : {}%'.format(round(sum(scores)*100\/len(scores)), 3))","4c1afcf0":"classifier = RandomForestClassifier(n_estimators=20, random_state=0)\nclassifier.fit(X_train, y_train)","a851bbf4":"# Confusion matrix for test set\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\ncm","18d164a3":"plt.figure(figsize=(10,7))\np = sns.heatmap(cm, annot=True, cmap=\"Purples\", fmt='g')\nplt.title('Confusion matrix for Random Forest Classifier Model - Test Set')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.show()","f696a652":"score = round(accuracy_score(y_test, y_pred),4)*100\nprint(\"Accuracy on test set: {}%\".format(score))","2518cefc":"print(classification_report(y_test, y_pred))","81b2c66c":"# Creating a confusion matrix for training set\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\ny_pred_train = classifier.predict(X_train)\ncm = confusion_matrix(y_train, y_pred_train)\ncm","5b4de9d8":"plt.figure(figsize=(10,7))\np = sns.heatmap(cm, annot=True, cmap=\"Purples\", fmt='g')\nplt.title('Confusion matrix for Random Forest Classifier Model - Train Set')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.show()","d0edcf0c":"score = round(accuracy_score(y_train, y_pred_train),4)*100\nprint(\"Accuracy on trainning set: {}%\".format(score))","9a9bf629":"print(classification_report(y_train, y_pred_train))","e33cd1c3":"def predict_diabetes(Pregnancies, Glucose, BP, SkinThickness, Insulin, BMI, DPF, Age):\n    preg = int(Pregnancies)\n    glucose = float(Glucose)\n    bp = float(BP)\n    st = float(SkinThickness)\n    insulin = float(Insulin)\n    bmi = float(BMI)\n    dpf = float(DPF)\n    age = int(Age)\n\n    x = [[preg, glucose, bp, st, insulin, bmi, dpf, age]]\n    x = ss.transform(x)\n\n    return classifier.predict(x)","e3c5002a":"prediction = predict_diabetes(4, 85, 79, 18, 72, 28.3, 0.498, 27)[0]\nif prediction:\n  print('Sorry! You have diabetes.')\nelse:\n  print(\"Voila! You don't have diabetes.\")","358059d6":"prediction = predict_diabetes(6, 122, 95, 13, 88, 24.7, 0.491, 63)[0]\nif prediction:\n  print('Sorry! You have diabetes.')\nelse:\n  print(\"Voila! You don't have diabetes.\")","06c29d4c":"prediction = predict_diabetes(1, 120, 90, 27, 149, 33.4, 0.393, 42)[0]\nif prediction:\n  print('Sorry! You have diabetes.')\nelse:\n  print(\"Voila! You don't have diabetes.\")","808e6564":"**Insulin:** In a rare situation a person can have zero insulin.Lets observe our dataset.","d4117b9e":"**BMI:** Shouldn't be zero or close to zero unless the person is really underweight which could be life-threatening","8b4567f2":"By observing the data we can see 13 counts where the value is 0.","bfcc9c74":"# Model Selection","d746e012":"**SkinThickness:** For normal people skin fold thickness can't be less than 10mm better yet zero.","b830c3ff":"By observing the data we can see 573 counts where the value is 0.","0c3ecd09":"**Blood Pressure(BP):** By observing the data we can see that there are 0 values for BP and it is evident that the readings of the dataset seem wrong because a living person cannot have a diastolic BP of 0.Lets observe the data and find the count of 0 readings.","6c9e8cb3":"We can identify that out of 2000 persons, 1316 are labeled as 0(non-diabetic) and 684 as 1(diabetic).","955c21fa":"'Outcome' is the column which we are going to predict, which says if the patient is diabetic or not. 1 means the person is diabetic and 0 means a person is not.","af9d4bf2":"# Model Evaluation","f3b163a9":"**Glucose Levels:** Even after fasting glucose levels can't be zero.Therefore zero is an invalid reading.","73814c7d":"### Handling Outliers","d723db75":"We can observe that the dataset contain 2000 rows and 9 columns.","0d1b4224":"We can observe that there are no data points missing in the dataset.","85107622":"When analyzing the histograms we can identify that there are some outliers in some columns.Lets analyze those outliers further.","95dedf46":"# Load Dataset","40ad8ca2":"By observing the data we can see 90 counts where the value is 0.","51fc568a":"By observing the data we can see 28 counts where the value is 0.","7e4f6565":"# Exploring Dataset","f7462db6":"# Model Predictions","5925327e":"By observing tha data we can find that there is total of 956 counts.","65380d4d":"# Diabetes Prediction\n\n###  Dataset Link: https:\/\/www.kaggle.com\/johndasilva\/diabetes","9cc81d6f":"Lets replace all the 0s with NaN and then put their mean\/median values according to our observations.","762daea0":"# Data Cleaning"}}