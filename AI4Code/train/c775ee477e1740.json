{"cell_type":{"5385326b":"code","3ec58362":"code","15ad67d2":"code","2f8360ea":"code","71c1dc39":"code","5b420de5":"code","24a55948":"code","39dd4770":"code","dfc1e8b4":"code","12af443d":"code","689e7c21":"code","9b8c8cef":"code","38024be9":"code","34fef05c":"code","3f1aa1da":"code","a40e6069":"code","663b06fb":"code","ec02c873":"code","f45dde55":"code","5976f63a":"code","80fb39c0":"code","201696b1":"code","ab512092":"code","18bd4e67":"code","d63b15d2":"code","e6937374":"code","1bbae088":"code","a60426b4":"code","e4433c33":"code","6046de0e":"code","029605ed":"code","45b8046b":"code","2da339dc":"code","e331a487":"code","d6675335":"code","7157e353":"code","c78f8f62":"code","77718627":"code","450beef5":"code","28dd7043":"code","f8db9a9a":"code","7e8dd832":"code","4f8b8d82":"code","04a05ad0":"code","b7bdd3de":"code","b447c10a":"code","5d21c9f2":"code","dadd5f45":"code","76c0c23c":"code","3884ace8":"code","8cba1e7a":"code","559157cd":"code","177107e1":"code","84ad26b7":"code","d5bcfaa4":"code","7078434d":"code","02664b9e":"code","72ab5ff3":"code","227ef655":"code","1ba5c05b":"code","0ac9631b":"code","5642a3f2":"code","9865b445":"code","dad2d289":"code","f6cd84b1":"code","0a544043":"code","fb30a58c":"code","91155049":"code","ca044779":"code","081ad774":"code","deec4792":"code","a5da0356":"code","e8966c6e":"code","f3051f26":"code","56769f06":"code","d8761c71":"code","035b51c1":"code","1b2f46dd":"code","0164b443":"code","6bc0b29b":"code","7dbd008e":"code","d37da249":"code","5f1c343e":"code","b31799d7":"code","281c9127":"code","74db87ea":"code","15baa97c":"code","4c94320c":"code","5b6803ca":"code","67ed92d4":"code","6cb27e01":"code","cc209916":"code","414dc795":"markdown","c5fdc768":"markdown","d2903e8c":"markdown","7734de02":"markdown","ad93923a":"markdown","0d354fa9":"markdown","9813e62f":"markdown","4511eecc":"markdown","bf482890":"markdown","9d3c82f6":"markdown","5249e666":"markdown","86e1dc44":"markdown","15fb7154":"markdown","c365c61f":"markdown","26782578":"markdown","4650a011":"markdown","7d5df253":"markdown","d7d931d2":"markdown","4909423f":"markdown","6b1f3018":"markdown","2389ffb9":"markdown","3d0d8d53":"markdown","66005fcc":"markdown","c328d8d3":"markdown","e8afa7c5":"markdown","3ff3e42c":"markdown","8e74bad9":"markdown","96890a33":"markdown","e3b101db":"markdown","7b44b251":"markdown","11635ced":"markdown","712f0fe4":"markdown","ae64bf3d":"markdown","27095f8b":"markdown","79ad04c7":"markdown","2ade1c88":"markdown","2ee7a0af":"markdown","c8d61441":"markdown","c4c01245":"markdown","9d94921c":"markdown","59478eb4":"markdown","720f8baf":"markdown","ef1bba4a":"markdown","d324f2f6":"markdown","cfa34239":"markdown","90c1ab85":"markdown","9daae8f4":"markdown","80c0a976":"markdown","af8aeff2":"markdown","1cf0a52b":"markdown","3df4bd6e":"markdown"},"source":{"5385326b":"# linear algebra and calculus\nimport numpy as np \n\n# data manupulation and processing library, (e.g. pd.read_csv)\nimport pandas as pd \n\nfrom matplotlib import pyplot as plt\n\nimport seaborn as sns\n\nfrom plotly import express as px\n\n# Table Data configurations setup\npd.set_option('max.column', None) #  show all columns  names","3ec58362":"# OS related functionality\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))   ","15ad67d2":"%%time\n\ndata = pd.read_csv('\/kaggle\/input\/melbourne-housing-snapshot\/melb_data.csv')","2f8360ea":"data.shape","71c1dc39":"data.head()","5b420de5":"data.tail()","24a55948":"data.sample(10)","39dd4770":"\nTARGET_FEATURE = 'Price'\n\nY = data[TARGET_FEATURE]\n\nY.head()","dfc1e8b4":"data.info()","12af443d":"data.describe()","689e7c21":"data.Price.describe([.2, .4, .6, .8])","9b8c8cef":"numeric_features = data.select_dtypes(['int', 'float']).columns\n\nnumeric_features , len(numeric_features)","38024be9":"categorical_features = data.select_dtypes('object').columns\n\ncategorical_features, len(categorical_features)","34fef05c":"print(\"Number of `Numerical` Features are:\", len(numeric_features) )\nprint(\"Number of `Categorical` Features are:\", len(categorical_features) )\nprint(\"`Total Number` of features are :\", len(numeric_features | categorical_features) )\n","3f1aa1da":"data.isna().sum().sort_values(ascending=False)","a40e6069":"(data.isna().sum() * 100 \/ data.isna().count()).sort_values(ascending=False)","663b06fb":"data[['BuildingArea', 'YearBuilt', 'CouncilArea', 'Car']].describe(include='all')","ec02c873":"# Let's check how many percentage values are NaN.\n\ndata.BuildingArea.value_counts(normalize=True, dropna=False) * 100","f45dde55":"# Check the numerical feature's statastical values.\n\ndata.BuildingArea.describe()","5976f63a":"data['CouncilArea'] = data['CouncilArea'].fillna('Moreland')","80fb39c0":"data['YearBuilt'] = data['YearBuilt'].fillna(data['YearBuilt'].mode()[0])","201696b1":"data['BuildingArea'] = data['BuildingArea'].fillna(data['BuildingArea'].mean())","ab512092":"data['Car'] = data['Car'].fillna(data['Car'].median())","18bd4e67":"# Now, is there any missing values are there?\ndata.isna().any()","d63b15d2":"print(\"Total Records :\", len(data) )\n\nfor col in categorical_features:\n    print(\"Total Unique Records of \"+ col + \" =\",  len(data[col].unique()))","e6937374":"categorical_features = categorical_features.drop('Address')\ncategorical_features = categorical_features.drop('SellerG')\ncategorical_features = categorical_features.drop('Suburb')\n","1bbae088":"# Let's see again the number of unique records\n\nprint(\"Total Records :\", len(data) )\n\nfor col in categorical_features:\n    print(\"Total Unique Records of \"+ col + \" =\",  len(data[col].unique()))","a60426b4":"data.sample(4)\n","e4433c33":"plt.figure(figsize=(15, 7))\nsns.scatterplot(data.Date, data.Price);\n","6046de0e":"plt.figure(figsize=(10, 5))\nsns.kdeplot(data.YearBuilt,Label='YearBuilt', color='r');\n","029605ed":"plt.figure(figsize=(10, 5))\nsns.kdeplot(data.Rooms,Label='Rooms', color='b');","45b8046b":"# plt.figure(figsize=(10, 5))\n\n# [ 'Regionname', 'Method', 'Method', 'Type' ]\nfor idx, column in enumerate(categorical_features):\n    \n    df = data.copy()\n    \n    unique = df[column].value_counts(); # ascending=True\n \n    plt.subplot(len(categorical_features), 2, idx+1)    \n    \n    fig = px.bar(x=unique.index, y=unique.values)\n    fig.show()\n    \n    plt.title(\"Count of \"+ column)\n#     plt.bar(unique.index, unique.values);\n    \n    plt.xlabel(column)\n    plt.ylabel(\"Number of \"+ column)\n    \n# plt.tight_layout()\nplt.show()   ","2da339dc":"fig = px.histogram(data, x='Method', y='Price', color='Method');\nfig.show()","e331a487":"fig = px.histogram(data, x='Rooms', y='Price', color='Rooms');\nfig.show()","d6675335":"corr_ = data[numeric_features].corr()\ncorr_","7157e353":"plt.figure(figsize=(15, 8))\n\n\nsns.heatmap(corr_, fmt='.2f', linewidths=.1, annot=True, cmap='coolwarm')\nplt.show()","c78f8f62":"data[categorical_features].value_counts()","77718627":"#  Import LabelEncoder from sklearn.preproccessing module","450beef5":"from sklearn.preprocessing import LabelEncoder","28dd7043":"# Here we need to define feature_columns the we convert to number in the below cell\n\ncategorical_features = [ 'Type', 'Method', 'CouncilArea', 'Regionname']\ncategorical_features","f8db9a9a":"# Encoding ...\n\nfor column in categorical_features:\n    \n    l_encoder = LabelEncoder()\n    \n    data[column] = l_encoder.fit_transform(data[column])\n    ","7e8dd832":"\n# training_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', \n#                         'YearBuilt', 'Lattitude', 'Longtitude']\n\ntraining_features = list(numeric_features) + list(categorical_features)\n\n# Remove 'Price' Feature from list\ntraining_features.remove('Price')\n\n# show the final list\ntraining_features","4f8b8d82":"from sklearn.preprocessing import MinMaxScaler","04a05ad0":"# Let's Normalize the data for training and testing\n\nminMaxNorm = MinMaxScaler()\n\nminMaxNorm.fit(data[training_features])\n\n","b7bdd3de":"X = minMaxNorm.transform(data[training_features]) ","b447c10a":"X","5d21c9f2":"Y = data['Price']  \nY","dadd5f45":"from sklearn.model_selection import train_test_split","76c0c23c":"train_X, test_X, train_Y, test_Y = train_test_split(X, Y, random_state = 0)","3884ace8":"print(\"Total size: \", data.shape[0])\nprint(\"Train size: \", train_X.shape, train_Y.shape)\nprint(\"Test size: \", test_X.shape, test_Y.shape)","8cba1e7a":"models_summary = pd.DataFrame([],\n                              columns=['model_name', \n                                       'prediction_score',\n                                       'mean_absolute_error'\n                                      ])\nmodels_summary","559157cd":"from sklearn.linear_model import LinearRegression\n\n\nfrom sklearn.metrics import mean_absolute_error","177107e1":"lr_model = LinearRegression()\n\nlr_model.fit(train_X, train_Y)","84ad26b7":"lr_model_predicted = lr_model.predict(test_X)\nlr_model_predicted","d5bcfaa4":"lr_model_score = lr_model.score(test_X, test_Y )\n\nlr_model_score","7078434d":"mae = mean_absolute_error(test_Y, lr_model_predicted)","02664b9e":"models_summary = models_summary.append({\n    'model_name': lr_model.__class__.__name__,\n    'prediction_score': lr_model_score,\n    'mean_absolute_error' : mae\n}, ignore_index=True)","72ab5ff3":"models_summary","227ef655":"from sklearn.tree import DecisionTreeRegressor","1ba5c05b":"Dtree_model = DecisionTreeRegressor(random_state=1)","0ac9631b":"Dtree_model.fit(train_X, train_Y)","5642a3f2":"Dtree_model_predicted = Dtree_model.predict(test_X)","9865b445":"Dtree_model_score = Dtree_model.score(test_X, test_Y)\nDtree_model_score","dad2d289":"mae = mean_absolute_error(Dtree_model_predicted, test_Y)","f6cd84b1":"models_summary = models_summary.append({\n    'model_name': Dtree_model.__class__.__name__,\n    'prediction_score': Dtree_model_score,\n    'mean_absolute_error' : mae\n}, ignore_index=True)\n\nmodels_summary.sort_values('prediction_score', ascending=False)","0a544043":"from sklearn.ensemble import  RandomForestRegressor\nfrom sklearn.ensemble import  BaggingRegressor \nfrom sklearn.ensemble import  AdaBoostRegressor\nfrom sklearn.ensemble import  GradientBoostingRegressor","fb30a58c":"RFRModel = RandomForestRegressor(max_leaf_nodes=100, random_state=1)","91155049":"RFRModel.fit(train_X, train_Y)","ca044779":"RFRModel_predicted = RFRModel.predict(test_X)","081ad774":"RFRModel_score = RFRModel.score(test_X, test_Y)\nRFRModel_score","deec4792":"mae = mean_absolute_error(RFRModel_predicted, test_Y)","a5da0356":"models_summary = models_summary.append({\n    'model_name': RFRModel.__class__.__name__,\n    'prediction_score': RFRModel_score,\n    'mean_absolute_error' : mae\n}, ignore_index=True)\n\nmodels_summary.sort_values('prediction_score', ascending=False)\n ","e8966c6e":"# Creating Model\nBGR_model = BaggingRegressor()\n\n# Model Fitting\nBGR_model.fit(train_X, train_Y)\n\n# Model Prediction\nBGR_model_predicted = BGR_model.predict(test_X)\n\n# Model Score\nBGR_model_score = BGR_model.score(test_X, test_Y)\n\n# find Mean Absolute Error\nmae = mean_absolute_error(test_Y, BGR_model_predicted)","f3051f26":"models_summary = models_summary.append({\n    'model_name': BGR_model.__class__.__name__,\n    'prediction_score': BGR_model_score,\n    'mean_absolute_error' : mae\n}, ignore_index=True)\n\nmodels_summary.sort_values('prediction_score', ascending=False)","56769f06":"    \n# Creating Model\nADB_model = AdaBoostRegressor()\n\n# Model Fitting\nADB_model.fit(train_X, train_Y)\n\n# Model Prediction\nADB_model_predicted = ADB_model.predict(test_X)\n\n# Model Score\nADB_model_score = ADB_model.score(test_X, test_Y)\n\n# find Mean Absolute Error\nmae = mean_absolute_error(test_Y, ADB_model_predicted)","d8761c71":"models_summary = models_summary.append({\n    'model_name': ADB_model.__class__.__name__,\n    'prediction_score': ADB_model_score,\n    'mean_absolute_error' : mae\n}, ignore_index=True)\n\nmodels_summary.sort_values('prediction_score', ascending=False)","035b51c1":"# Creating Model\nGBR_model = GradientBoostingRegressor(n_estimators=150, random_state=1)\n\n# Model Fitting\nGBR_model.fit(train_X, train_Y)\n\n# Model Prediction\nGBR_model_predicted = GBR_model.predict(test_X)\n\n# Model Score\nGBR_model_score = GBR_model.score(test_X, test_Y)\n\n# find Mean Absolute Error\nmae = mean_absolute_error(test_Y, GBR_model_predicted)","1b2f46dd":"models_summary = models_summary.append({\n    'model_name': GBR_model.__class__.__name__,\n    'prediction_score': GBR_model_score,\n    'mean_absolute_error' : mae\n}, ignore_index=True)\n\nmodels_summary.sort_values('prediction_score', ascending=False)","0164b443":"from xgboost import XGBRegressor","6bc0b29b":"XGBR_model = XGBRegressor()","7dbd008e":"XGBR_model.fit(train_X, train_Y)","d37da249":"XGBR_model_predicted = XGBR_model.predict(test_X)","5f1c343e":"XGBR_model_score = XGBR_model.score(test_X, test_Y)\nXGBR_model_score","b31799d7":"mae = mean_absolute_error(test_Y, XGBR_model_predicted)\nmae","281c9127":"models_summary = models_summary.append({\n    'model_name': XGBR_model.__class__.__name__,\n    'prediction_score': XGBR_model_score,\n    'mean_absolute_error' : mae\n}, ignore_index=True)\n\nmodels_summary.sort_values('prediction_score', ascending=False)\n ","74db87ea":"XGBR_model_500 = XGBRegressor(n_estimators=500, max_depth=10, learning_rate=0.05)\nXGBR_model_500.fit(train_X, train_Y)\n\nXGBR_model_500_predicted = XGBR_model_500.predict(test_X)\n\nXGBR_model_500_score = XGBR_model_500.score(test_X, test_Y)\nprint(XGBR_model_500_score)\n\nmae = mean_absolute_error(test_Y, XGBR_model_500_predicted)\nprint(mae)","15baa97c":"models_summary = models_summary.append({\n    'model_name': 'XGBRegressor(n_estimators=500, max_depth=10, learning_rate=0.05)',\n    'prediction_score': XGBR_model_500_score,\n    'mean_absolute_error' : mae\n}, ignore_index=True)\n\nmodels_summary.sort_values('prediction_score', ascending=False)\n ","4c94320c":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV","5b6803ca":"xgbr_model = XGBRegressor() # {'objective': 'reg:squarederror' }\n\nparams = {\n    'n_estimators': [110, 120, 130, 140], \n    'learning_rate': [ 0.05, 0.075, 0.1],\n    'max_depth': [ 7, 9],\n    'reg_lambda': [0.3, 0.5]\n}\n\nxgb_reg = GridSearchCV(estimator=xgbr_model, param_grid=params, cv=5, n_jobs=-1)\n\nxgb_reg.fit(train_X, train_Y)\n\nxgbr_model_score = xgb_reg.best_score_\n\nxgbr_model_pred = xgb_reg.predict(test_X)\n\nmae = mean_absolute_error(test_Y, xgbr_model_pred)\n\nprint(\"Best score: %0.3f\" % xgb_reg.best_score_)\nprint(\"Best parameters set:\", xgb_reg.best_params_)\n\nprint(\"mean_absolute_error :\", mae)\n","67ed92d4":"models_summary = models_summary.append({\n    'model_name': 'XGBRegressor_HyperParamsTunning',\n    'prediction_score': xgbr_model_score,\n    'mean_absolute_error' : mae\n}, ignore_index=True)\n\nmodels_summary.sort_values('prediction_score', ascending=False)\n ","6cb27e01":"rfr_model = RandomForestRegressor(random_state=35)\n\nrfr_params_grid = {\n    'n_estimators' : [600,750,800,850],\n    'max_depth' : [7],\n    'max_features': [5],\n    'min_samples_leaf' : [3],\n    'min_samples_split' : [4, 6 ,9]\n}\n\ngscv_rfr_cv = GridSearchCV(estimator=rfr_model, \n                                      param_grid=rfr_params_grid,\n                                      cv = 5 ,\n                                      n_jobs = -1,\n                                      verbose = 5)\n\ngscv_rfr_cv.fit(train_X, train_Y)\n\ngscv_rfr_cv_score = gscv_rfr_cv.best_score_\n\ngscv_rfr_cv_pred = gscv_rfr_cv.predict(test_X)\n\nmae = mean_absolute_error(test_Y, gscv_rfr_cv_pred)\n\nprint(\"Best score: %0.3f\" % gscv_rfr_cv.best_score_)\nprint(\"Best parameters set:\", gscv_rfr_cv.best_params_)\n\nprint(\"mean_absolute_error :\", mae)","cc209916":"models_summary = models_summary.append({\n    'model_name': 'Random Forest Regressor Hyper Params Tunning',\n    'prediction_score': gscv_rfr_cv_score,\n    'mean_absolute_error' : mae\n}, ignore_index=True)\n\nmodels_summary.sort_values('prediction_score', ascending=False)\n ","414dc795":"## Splite Train and Test Dataset\n\nBefore we splite the data, We need to import train_test_split module from sklearn package","c5fdc768":"### Decision Tree Regressor Model","d2903e8c":"<center>\n    <div >\n         <img src=\"https:\/\/miro.medium.com\/max\/1200\/0*xT7BFMlyBbRwmtFe\" alt=\"Melbourne House Price\" \/> \n    <\/div>\n    <\/center>","7734de02":"Create `X` data and assignning from `training feature` columns from `data` and make it normalized.","ad93923a":"## Feature Selection\n","0d354fa9":"Let's implement some hyper param tunning using `n_estimators=500, max_depth=10, learning_rate=0.05`.","9813e62f":"Here, We convert the number of missing values into percentages. So, we can easly understand to how many percentage of missing values available.","4511eecc":"--- \n---\n\n<div class=\"text-center\">\n    <h1>That's it Guys,<\/h1>\n    <h1>\ud83d\ude4f<\/h1>\n    \n        \n        I Hope you guys you like and enjoy it, and learn something interesting things from this notebook, \n        \n        Even I learn a lots of things while I'm creating this notebook\n    \n        Keep Learning,\n        Regards,\n        Vikas Ukani.\n    \n<\/div>\n\n---\n---\n\n<img src=\"https:\/\/static.wixstatic.com\/media\/3592ed_5453a1ea302b4c4588413007ac4fcb93~mv2.gif\" align=\"center\" alt=\"Thank You\" style=\"min-height:20%; max-height:20%\" width=\"90%\" \/>\n\n","bf482890":"## Data Description \n \n* `Rooms`: Number of rooms\n\n* `Price`: Price in dollars\n\n* `Method`: S - property sold; SP - property sold prior; PI - property passed in; PN - sold prior not disclosed; SN - sold not disclosed; NB - no bid; VB - vendor bid; W - withdrawn prior to auction; SA - sold after auction; SS - sold after auction price not disclosed. N\/A - price or highest bid not available.\n\n* `Type`: br - bedroom(s); h - house,cottage,villa, semi,terrace; u - unit, duplex; t - townhouse; dev site - development site; o res - other residential.\n\n* `SellerG`: Real Estate Agent\n\n* `Date`: Date sold\n\n* `Distance`: Distance from CBD\n\n* `Regionname`: General Region (West, North West, North, North east \u2026etc)\n\n* `Propertycount`: Number of properties that exist in the suburb.\n\n* `Bedroom2` : Scraped # of Bedrooms (from different source)\n\n* `Bathroom`: Number of Bathrooms\n\n* `Car`: Number of carspots\n\n* `Landsize`: Land Size\n\n* `BuildingArea`: Building Size\n\n* `CouncilArea`: Governing council for the area\n","9d3c82f6":"#### Categorical Features","5249e666":"- As we can see There is one feature have categorical values and rest are numerical features.\n","86e1dc44":"\n\n- Let's check the shape of our datasets `( number of rows, number of columns\/features )`","15fb7154":"- Let's check the first five rows from data","c365c61f":"###### Removing columns which has huge number of unique values","26782578":"- In abouve output, We can clearly see that, There are only four feature `'BuildingArea', 'YearBuilt', 'CouncilArea'` and `'Car'` has null values. So we have to fill some statastical values.","4650a011":"# Model Building","7d5df253":"Here, We implement other models from ensemble package.","d7d931d2":"# HYPER PARAMS TUNNING \n\n- Using GridSearchCV.\n","4909423f":"### Gradient Boosting Regressor Model","6b1f3018":"### Random Forest Regressor Model","2389ffb9":"- Here, We need to convert categorical values to numerical values","3d0d8d53":"### Introductions\n\n<div class=\" text-center\"> \n        Melbourne has 4694 properties available for rent and 661 properties for sale. The median unit price in Melbourne last year was 480,000 dollor. If you are looking for an investment property, consider houses in Melbourne rent out for 410 dollor PW with an annual rental yield of 3.7% and units rent for 470 dollor PW with a rental yield of 5.1%. Based on five years of sales, Melbourne has seen a compound growth rate of -4.2% for houses and -0.9% for units. \n     \n---\n    \n    \nThe dataset includes Address, Type of Real estate, Suburb, Method of Selling, Rooms, Price, Real Estate Agent, Date of Sale and distance from C.B.D.\n<\/div>\nFor more information about Melbourne  : <a href=\"https:\/\/www.realestate.com.au\/neighbourhoods\/melbourne-3000-vic\" alt=\"here\" > Here <\/a> ","66005fcc":"As we can see in the output.\n\n1. There are **13580 entries**\n1. There are total **21 features (0 to 20)**\n1. There are three types of datatype dtypes: **float64(12), int64(1), object(8)**\n1. It's Memory usage that is, memory usage: **2.2+ MB**\n1. Also, We can check how many missing values available in the **Non-Null Count** column","c328d8d3":"- Implementing `LabelEncoder` to convert categorical values to numerical values.","e8afa7c5":"- Let's check the random ten number of data samples, Every time it will print the random five sample of records from original datasets. So we can easly understand the behaviour and what types of data type stored in particular features.","3ff3e42c":"- For Categorical feature, We fill the highest occorance value. In this case, highest occurrence value is `'Moreland'`. so we fill with this values. And other feature are fill with `mean(), median(),` and `mode()`.","8e74bad9":"Here,`describe()` method provides us the complete calculations details about the dataset. i.e. let's take the `price` feature for example. It shows the what's the `min`, `max`, `mean(average)` and `std(standard deviation)` of price feature.","96890a33":"### Find the Missing Values","e3b101db":"### Categorical Features","7b44b251":"# 3. Data Visualizations\n","11635ced":"Now, Let's find the `mean absolute error` using test_Y and predicted vales","712f0fe4":"<div class=\"text-primary font-bold \"><h3> <b>Table Of Contains<\/b><\/h3><\/div>\n\n--- \n\n> ### Steps are:\n\n\n1. [Gathering Data](#1)\n- [Exploratory Data Analysis](#2)\n- [Data Visualizations](#3)\n- [Model Implementation.](#4)\n- [ML Model Selecting and Model PredPrediction](#5)\n- [HyperTunning the ML Model](#6)\n- [Deploy Model](#7)\n\n\n","ae64bf3d":"### Target Feature","27095f8b":"# 1. Gathering Data\n","79ad04c7":"#### Before Implementing ML Models we can create dataframe to stores the prediction values by each models that we are implementing below here.","2ade1c88":"Here, We need to find how many missing values are there in our datasets.","2ee7a0af":"- Creating Training Feature to train the model to predict the beter accuracy.","c8d61441":"### Ada Boost Regressor Model","c4c01245":"### Scalling Dataset\n\n- Here, We are using `MinMaxScaler` to notmalize our dataset. Firstly, we need to import that class from `sklearn.preprocessing` package","9d94921c":"### Numerical Features","59478eb4":"##### Check the train and test sized","720f8baf":"# 2. Exploratory Data Analysis\n\n","ef1bba4a":"### Linear Reression Model","d324f2f6":"### Filling Missing Values","cfa34239":"### Random Forest Regressor Model","90c1ab85":"Now, let's see the model_summary dataframe","9daae8f4":"- Reading the housing information data using read_csv to represent in tabular format ","80c0a976":"### Splites the main data\n\n- split data into training and validation data, for both features and target. The split is based on a random number generator.\n- Supplying a numeric value to the random_state argument guarantees we get the same split eve run this script.","af8aeff2":"- Let's check the last five rows from data","1cf0a52b":"<center> \n    <h1>\n        \ud83c\udfe1 Melbourne House Price Prediction Using ML Models \ud83e\udd16\n    <\/h1>\n<\/center>","3df4bd6e":"###### Store model and it's predictoin score in dataframe that we created below the model building section"}}