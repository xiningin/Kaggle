{"cell_type":{"4360ec32":"code","b91ab7fa":"code","e6f3881d":"code","fba62bc9":"code","0160a6df":"code","3e9f809d":"code","be9ee4a7":"code","852ec699":"code","0f1652ed":"code","9e449fa8":"code","d828fd0f":"code","039bc6ca":"code","5000675a":"code","bea8b1fd":"code","36086417":"code","be40023f":"code","bead38b6":"code","69638737":"code","d6ff5672":"code","b43b7a8c":"code","e6c904f6":"code","065f0707":"code","7925439a":"code","d8666c60":"markdown","e878de30":"markdown","8b5c51eb":"markdown","f9e28ac2":"markdown"},"source":{"4360ec32":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nnp.set_printoptions(suppress=True)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\npd.set_option('display.max_columns', None)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport zipfile\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b91ab7fa":"\nwith zipfile.ZipFile(\"\/kaggle\/input\/sberbank-russian-housing-market\/train.csv.zip\", 'r') as zip_ref:\n    zip_ref.extractall(\"\/train\")\n\nwith zipfile.ZipFile(\"\/kaggle\/input\/sberbank-russian-housing-market\/test.csv.zip\", 'r') as zip_ref:\n    zip_ref.extractall(\"\/test\")","e6f3881d":"df_train = pd.read_csv('\/train\/train.csv')\ndf_test = pd.read_csv('\/test\/test.csv')\ntrain = df_train\ntest = df_test\nnp.random.seed(0)","fba62bc9":"# Missing Values\nfeatures_with_na = [feature for feature in train.columns if train[feature].isnull().sum()>1]\n\nfor feature in features_with_na:\n    print(f\"The amount of NA in\", feature, np.round(train[feature].isnull().mean(),3))","0160a6df":"\n# The relationship between the missing values and the target variable\nfor feature in features_with_na:\n    data = train.copy()\n\n    # let's make a variable that indicates 1 if the observation was missing or zero otherwise\n    data[feature] = np.where(data[feature].isnull(), 1, 0)\n\n    # let's calculate the mean price_doc where the information is missing or present\n\n    data.groupby(feature)['price_doc'].mean().plot.bar()\n    plt.ticklabel_format(style='plain', axis='y')\n    plt.title(feature)\n    plt.show()","3e9f809d":"# List of numerical variables\nnumerical_features = [feature for feature in train.columns if train[feature].dtype!='O']\n\n# Number of num features\nprint('Number of num features:', len(numerical_features)) # shape of data (30471, 292)","be9ee4a7":"# List of categorical variables\ncategorical_features = [feature for feature in train.columns if train[feature].dtype == 'O']\n\n# Number of num features\nprint('Number of num features:', len(categorical_features)) # shape of data (30471, 292)","852ec699":"# Missing values in Categorical variables\nfor feature in categorical_features:\n    print('The feature is {} and number of categories are {}'.format(feature,len(train[feature].unique())))\n","0f1652ed":"# The effect of the Missing data on the target variable\nfor feature in categorical_features:\n    data=train.copy()\n    data.groupby(feature)['price_doc'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('price_doc')\n    plt.title(feature)\n    plt.show()","9e449fa8":"# Replace missing value with the label \"Missing\"\ndef replace_missing_value(train,features_with_na, value):\n    data = train.copy()\n    data[features_with_na]=data[features_with_na].fillna(value)\n    return data\n\ntrain=replace_missing_value(train,categorical_features, 'Missing')\ntest=replace_missing_value(test,categorical_features, 'Missing')\n\nprint(train[categorical_features].isnull().sum())\nprint(test[categorical_features].isnull().sum())","d828fd0f":"numerical_features.remove('price_doc')\n# Replace NA in numerical features with median\ndef replace_missing_median(data,features):\n    for feature in features:\n    ## We will replace by using median since there are outliers\n        median_value=data[feature].median()\n        data[feature].fillna(median_value,inplace=True)\n    return data\ntrain = replace_missing_median(train,numerical_features)\ntest = replace_missing_median(test,numerical_features)\n","039bc6ca":"# Dealing with timestamp to get the year sold\ndef from_timestamp(train):\n    train['timestamp_parsed'] = pd.to_datetime(train['timestamp'], format='%Y-%m-%d') # Format 2011-08-20 : %Y-%m-%d\n    train['sold_year'] = train['timestamp_parsed'].dt.year\n    train['sold_month'] = train['timestamp_parsed'].dt.month\n    train['sold_day'] = train['timestamp_parsed'].dt.day\n    return train\n\ntrain = from_timestamp(train)\ntest = from_timestamp(test)","5000675a":"train = train.drop(['timestamp','timestamp_parsed'], axis=1)\ntest = test.drop(['timestamp','timestamp_parsed'], axis=1)\n\nsns.histplot(train['sold_year'], kde=False, bins=5)\n","bea8b1fd":"categorical_features.remove('timestamp')\ndef feature_scaling(train, categorical_features):\n    for feature in categorical_features:\n        lb = LabelEncoder()\n        train[feature] = lb.fit_transform(train[feature])\n    return train\n\ntrain = feature_scaling(train, categorical_features)\ntest = feature_scaling(test, categorical_features)","36086417":"# Check the correlation between features\nnumerical_features.remove('id')\n\ncorrmat = train[numerical_features].corr()\nfig, ax = plt.subplots()\nsns.heatmap(corrmat)","be40023f":"def correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr\n\ncorr_features = correlation(train[numerical_features], 0.8)\nprint('Number of correlated features:', len(set(corr_features)))","bead38b6":"train = train.drop(corr_features,axis=1)\ntest = test.drop(corr_features,axis=1)\n\nX_train = train.drop(['price_doc'], axis = 1)\ny_train = train['price_doc']\nX_test = test\n# Feature Scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test= sc.fit_transform(X_test)","69638737":"sns.histplot(y_train, bins=50, kde=True)","d6ff5672":"sns.histplot(np.log10(y_train), bins=50, kde=True)","b43b7a8c":"y_train = np.log10(y_train)","e6c904f6":"# The first set of params\nparams = {\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"num_leaves\": 64,\n    \"learning_rate\": 0.01,\n    'max_depth': -1,\n    'colsample_bytree': 0.9,\n    'num_leaves': 150,\n    \"bagging_seed\": 42,\n    \"verbosity\": 1,\n    \"seed\": 42,\n}\n\nlgtrain = lgb.Dataset(X_train, label=y_train)\nmodel = lgb.train(params, lgtrain, 5000)\ny_pred = model.predict(X_test, num_iteration=model.best_iteration)\n\nprint(\"LightGBM Training Completed...\")\n# The Submission File => RMSE = 0.48122","065f0707":"# The second set of params\nparams = {\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    'learning_rate': 0.3777518392924809,\n    'sub_feature': 0.5424987750103974,\n    'max_depth': 94,\n    'colsample_bytree': 0.9,\n    'num_leaves': 194,\n    \"bagging_seed\": 42,\n    'min_data': 31,\n    \"verbosity\": 1,\n    \"seed\": 42,\n    'boosting_type': 'dart',\n}\n\nlgtrain = lgb.Dataset(X_train, label=y_train)\nmodel = lgb.train(params, lgtrain, 5000)\ny_pred = model.predict(X_test, num_iteration=model.best_iteration)\n\nprint(\"LightGBM Training Completed...\")\n# The Submission File => RMSE = 0.40039","7925439a":"\ntransformed_y_pred = 10 ** y_pred\n# Submitting the file\nmy_submission = pd.DataFrame({'id': df_test.id, 'price_doc': transformed_y_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)\n","d8666c60":"# Feature Engineering","e878de30":"# EDA","8b5c51eb":"# Target Engineering","f9e28ac2":"# LGBM Model\n\nIn the first try I did the LGBM with the parameters below, after the submission, I had an rmse of 0.48122 I tried to improve the model using the Random Grid Search, and I got an RMSE of 0.40039"}}