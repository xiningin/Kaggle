{"cell_type":{"b4afa671":"code","fb220192":"code","5d022632":"code","be8e3e5f":"code","33595bc6":"code","61dadeeb":"code","058da1e1":"code","f8ad9944":"code","fe3a7b1e":"code","8c887baa":"code","a7dc2d55":"code","74613ce9":"code","7e25758f":"code","16e11036":"code","2a4aa4f3":"code","36c69217":"code","f39ffa9f":"code","1fa24a50":"code","92059fe3":"code","638c49ca":"code","0e2aef34":"code","3c6d77ff":"code","24939909":"code","4025f753":"code","416aa311":"code","d7ff305d":"code","a2d7c60e":"code","22356a44":"code","30d11325":"code","619e0ba0":"markdown","af0d682d":"markdown","a4a43157":"markdown","eda6274b":"markdown","50ec5b33":"markdown","1f5158be":"markdown","6563efdb":"markdown","54ed66c4":"markdown","88630da5":"markdown","225ae2b7":"markdown","52e41a6a":"markdown","7b7273fc":"markdown","fe2cbd7f":"markdown","bb0de8d5":"markdown","c93dec20":"markdown","6dfa2208":"markdown","6faab1dd":"markdown","f23b396d":"markdown","dd746e89":"markdown","10c376f1":"markdown","a9e250e8":"markdown","2005c310":"markdown","23925693":"markdown","3701545c":"markdown","81f6da2f":"markdown","683f5c2d":"markdown","d959fe3e":"markdown","45b7bcfe":"markdown","fbfd8aff":"markdown","176b87b0":"markdown","3ae4d26e":"markdown","5dfe8185":"markdown","d25e657c":"markdown","e8243373":"markdown","a21c476e":"markdown","aaf6d30a":"markdown","309c223b":"markdown","7069e3f8":"markdown"},"source":{"b4afa671":"input_img_url=\"https:\/\/www.vangoghgallery.com\/img\/starry_night_full.jpg\"","fb220192":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.applications import inception_v3\nfrom tensorflow.keras.models import load_model,Model\nfrom PIL import Image\nimport IPython.display as display\nimport time\nimport requests\nfrom io import BytesIO\nfrom tqdm.notebook import tqdm","5d022632":"def load_image(image_path,max_dim=512):\n    img=Image.open(image_path)\n    img=img.convert(\"RGB\")\n    img.thumbnail([max_dim,max_dim])\n    img=np.array(img,dtype=np.uint8)\n    img=np.expand_dims(img,axis=0)\n    return img","be8e3e5f":"def load_url_image(url, max_dim=512):\n    img_request=requests.get(url)\n    img= Image.open(BytesIO(img_request.content))\n    img=img.convert(\"RGB\")\n    img.thumbnail([max_dim,max_dim])\n    img=np.array(img,dtype=np.uint8)\n    img=np.expand_dims(img,axis=0)\n    return img","33595bc6":"def deprocess_inception_image(img):\n    img = 255*(img+1.0)\/2.0\n    return np.array(img, np.uint8)","61dadeeb":"def array_to_img(array,deprocessing=False):\n    if deprocessing:\n        array=deprocess_inception_image(array)\n    if np.ndim(array)>3:\n        assert array.shape[0]==1\n        array=array[0]\n    return Image.fromarray(array)","058da1e1":"def show_image(img):\n    image=array_to_img(img)\n    display.display(image)","f8ad9944":"input_image=load_url_image(input_img_url,max_dim=512)\nprint(input_image.shape)\nshow_image(input_image)","fe3a7b1e":"preprocessed_image=inception_v3.preprocess_input(input_image)\nshow_image(deprocess_inception_image(preprocessed_image))","8c887baa":"def deep_dream_model(model,layer_names):\n    model.trainable=False\n    outputs=[model.get_layer(name).output for name in layer_names]\n    new_model=Model(inputs=model.input,outputs=outputs)\n    return new_model","a7dc2d55":"inception=inception_v3.InceptionV3(weights=\"imagenet\",include_top=False)\ninception.summary()","74613ce9":"layers_contributions=['mixed3', 'mixed5']","7e25758f":"dream_model=deep_dream_model(inception,layers_contributions)","16e11036":"deep_outputs=dream_model(preprocessed_image)\nfor layer_name,outputs in zip(layers_contributions,deep_outputs):\n    print(layer_name)\n    print(outputs.shape)\n    print(outputs.numpy().mean())","2a4aa4f3":"model_output= lambda model,inputs:model(inputs)","36c69217":"def get_loss(activations):\n    loss=[]\n    for activation in activations:\n        loss.append(tf.math.reduce_mean(activation))\n    return tf.reduce_sum(loss)","f39ffa9f":"def get_loss_and_gradient(model,inputs,total_variation_weight=0):\n    with tf.GradientTape() as tape:\n        tape.watch(inputs)\n        activations=model_output(model,inputs)\n        loss=get_loss(activations)\n        loss=loss+total_variation_weight*tf.image.total_variation(inputs)\n    grads=tape.gradient(loss,inputs)\n    grads \/= tf.math.reduce_std(grads) + 1e-8 \n    return loss,grads","1fa24a50":"def run_gradient_ascent(model,inputs,epochs=1,steps_per_epoch=1,weight=0.01,total_variation_weight=0):\n    img = tf.convert_to_tensor(inputs)\n    start=time.time()\n    for i in range(epochs):\n        print(f\"epoch: {i+1}\",end=' ')\n        for j in tqdm(range(steps_per_epoch)):\n            loss,grads=get_loss_and_gradient(model,img,total_variation_weight)\n            img = img + grads*weight\n            img = tf.clip_by_value(img, -1.0, 1.0)\n    end=time.time()\n    print(f\"Time elapsed: {end-start:1f}sec\")\n    return img.numpy()","92059fe3":"image_array=run_gradient_ascent(dream_model,preprocessed_image,epochs=2,\n                                steps_per_epoch=100,\n                                weight=0.01)","638c49ca":"show_image(deprocess_inception_image(image_array))\nresultant_image=array_to_img(image_array,True)\nresultant_image.save(\"deep_dream_simple.jpg\")","0e2aef34":"def run_gradient_ascent_with_octaves(model,inputs,epochs=1,steps_per_epoch=1,\n                                     num_octaves=2,octave_size=1.3,\n                                     weight=0.01,\n                                     total_variation_weight=0):\n    img=tf.convert_to_tensor(inputs)\n    assert len(inputs.shape)<=4 or len(inputs.shape)>=3\n    if len(inputs.shape)==3:\n        base_shape=img.shape[:-1]\n    base_shape=img.shape[1:-1]\n    for n in range(-num_octaves,1):\n        print(f'Processing Octave: {n*-1}')\n        new_shape=tuple([int(dim * (octave_size**n)) for dim in base_shape])\n        img=tf.image.resize(img,new_shape)\n        img=run_gradient_ascent(model,img,epochs,steps_per_epoch,weight,total_variation_weight)\n    return tf.image.resize(img,base_shape).numpy()","3c6d77ff":"image_array=run_gradient_ascent_with_octaves(dream_model,preprocessed_image,epochs=1,\n                                             steps_per_epoch=100,num_octaves=3,octave_size=1.3,\n                                             weight=0.01)","24939909":"show_image(deprocess_inception_image(image_array))\nimage=array_to_img(image_array,True)\nimage.save(\"deep_dream_with_octave.jpg\")","4025f753":"# Randomly rolls the image to avoid tiled boundaries.\ndef random_image_tiling(img, maxdim):\n    shift = tf.random.uniform(shape=[2], minval=-maxdim, maxval=maxdim, dtype=tf.int32)\n    shift_r,shift_d=shift\n    img_rolled = tf.roll(img, shift=[shift_r,shift_d], axis=[1,0])\n    return shift_r, shift_d, img_rolled","416aa311":"shift_r,shift_d,img_tiled=random_image_tiling(input_image[0], 512)\nshow_image(img_tiled.numpy())","d7ff305d":"def get_loss_and_grads_with_tiling(model,inputs,tile_size=512,total_variation_weight=0.004):\n    shift_r,shift_d,rolled_image=random_image_tiling(inputs[0],tile_size)\n    grads=tf.zeros_like(rolled_image)\n    # create a tensor from 0 to rolled_image width with step of tile size\n    x_range = tf.range(0, rolled_image.shape[0], tile_size)[:-1]\n    # check if x_range is not empty\n    if not tf.cast(len(x_range), bool):\n        x_range= tf.constant([0])\n    # create a tensor from 0 to rolled_image height with step of tile size\n    y_range = tf.range(0, rolled_image.shape[1], tile_size)[:-1] \n    # check if y_range is not empty\n    if not tf.cast(len(y_range), bool):\n        y_range=tf.constant([0])\n    for x in x_range:\n        for y in y_range:\n            with tf.GradientTape() as tape:\n                tape.watch(rolled_image)\n                # here we create tile from rolled image of size=tile_size\n                image_tile= tf.expand_dims(rolled_image[x:x+tile_size, y:y+tile_size],axis=0)\n                activations=model_output(model,image_tile)\n                loss=get_loss(activations)\n                loss=loss+total_variation_weight*tf.image.total_variation(image_tile) \n            grads=grads+tape.gradient(loss,rolled_image)\n    grads = tf.roll(grads, shift=[-shift_r,-shift_d], axis=[1,0]) #reverse shifting of rolled image\n    grads \/= tf.math.reduce_std(grads) + 1e-8\n    return loss,grads","a2d7c60e":"def run_gradient_ascent_with_octave_tiling(model,inputs,steps_per_octave=100,\n                                           num_octaves=2,octave_size=1.3,tile_size=512,\n                                           weight=0.01,total_variation_weight=0.0004):\n    img=tf.convert_to_tensor(inputs)\n    weight=tf.convert_to_tensor(weight)\n    assert len(inputs.shape)<=4 or len(inputs.shape)>=3\n    if len(inputs.shape)==3:\n        base_shape=img.shape[:-1]\n    base_shape=img.shape[1:-1]\n    start=time.time()\n    for n in range(-num_octaves,num_octaves+1):\n        print(f'Processing Octave: {n+num_octaves+1}')\n        new_shape=tuple([int(dim*(octave_size**n)) for dim in base_shape])\n        img=tf.image.resize(img,new_shape)\n        for step in tqdm(range(steps_per_octave)):\n            loss,grads=get_loss_and_grads_with_tiling(model,img,tile_size,total_variation_weight)\n            img = img + grads*weight\n            img = tf.clip_by_value(img, -1.0, 1.0)\n    end=time.time()\n    print(f\"Time elapsed: {end-start:.1f} sec\")\n    return tf.image.resize(img, base_shape).numpy()","22356a44":"image_array=run_gradient_ascent_with_octave_tiling(dream_model,preprocessed_image,\n                                                   steps_per_octave=100,\n                                                   num_octaves=3,octave_size=1.3,\n                                                   tile_size=512,weight=0.01,\n                                                   total_variation_weight=0)","30d11325":"show_image(deprocess_inception_image(image_array))\nimage=array_to_img(image_array,True)\nimage.save(\"deep_dream_with_octave_tiling.jpg\")","619e0ba0":"## Deep Dreaming using Image Tiling\n\nAs we start processing bigger images we need more RAM to put it into memory for calculating gradients. Also we cannot process more octaves using above techniques. \n\nThis issue can be fixed by using image tilings, In this technique we split image into tiles and gradient is calculated for each tile seperately. \n\nBy tiling images into small sizes and processing these tiles solves the issue as we have to process small tiles of image not the entire image.\n\nWhile tiling we make sure that it is random else we get seam in our image after processing.","af0d682d":"the above function takes image as input and randomly rolls the image to avoid tiled boundaries. It returns shifted image and positions from where image was shifted. We have used tensorflow *roll* function to shift images. It create roll of an array along different axis from shift positions specified.","a4a43157":"And this time we got some exciting results.","eda6274b":"Now since we are using inception model so lets create a inception model using keras and print its layers","50ec5b33":"Next we will import all dependencies which we will need for creating deepdream. \n\n- **numpy :** for arrays manipulation\n- **tensorflow :** for tensor operations\n- **tensorflow.keras :** high level neural network library for tensorflow for creating neural networks\n- **pillow :** for converting an image to numpy array and numpy array to image, saving out output image.\n- **Ipython.display :** for displaying images in notebook\n- **time :** for calculating time of each iteration\n\nWe are using Inception pretrained model for this as it produces better outputs of deepdreams and original implementation also used Inception model.\n\n> Dreams in Inception movie.\ud83d\ude01\ud83d\ude01","1f5158be":"the above functions \n- loads image from path or url based on function choosen\n- convert it into RGB format\n- resize it with max dimension specified while maintaining aspect ratio\n- converting image to numpy array and creating batch of single image since neural networks expects the input to be in batches. ","6563efdb":"Lets define a way to get gradients from tiled image. In above function we first get random rolled image and its rolling positions using *random_image_tiling* function. We then have some logic to create a tile from rolled image of size *tile_size* specified. This tile image is then passed to model and loss is calculated finally gradients are calculated for that tile and added to *grads* tensor. We process small tiles of rolled image till we have iterated whole image (rolled image) and all gradients of each tile are summed together. Then we reverse the shiftings of rolled image back to original image and finally scaling and returning the graadients.","54ed66c4":"Also check if our deprocess_image function is working as expected","88630da5":"### Gradient Ascent\n\ngradient ascent is opposite of gradient descent. Both are optimization algorithms. As gradient descent finds out minima of a function gradient ascent finds out maxima of a function. The process of gradient ascent is same as gradient descent we first find out gradient(derivative) of function with respect to our training parameters and then change training parameters so as to maximize instead of minimizing by moving it in opposite direction of gradient descent. ","225ae2b7":"Thanks, If you like this consider upvoting. \u270c\u270c\u270c","52e41a6a":"Now its time to create deep dream image, we apply gradient ascent for some epochs and save our image into a variable which is a numpy array","7b7273fc":"## Deep Dreaming using Tensorflow\n\nDeepdream is image modification alogorithm that uses representation learned by convolution neural networks to modify images. It was released by google in 2015. The popularity of deepdream caused due to its crappy artifacts in images, from eyes to feathers to dog faces. It was initially created to help scientists and engineers to see what a deep neural network is seeing when it is looks given input.\n\nDeepdream is based on one of the technique of visualizing learnings of convnets. Using that technique we can visualize patterns that activates given layer of convolutional neural network or visual pattern that each filter respond to in convolutional layers. This is done by applying gradient ascent in input space, which maximizes response of specific filter in convnets.","fe2cbd7f":"the above function creates a deepdream model. Since we are not training our model so set trainable to false. Our deepdream model takes input as image and outputs the activations of layers which we will use to embed patterns learned by that layers into our input image","bb0de8d5":"Now we have gradients of loss with respect to input image, we define a function that will do gradient ascent by changing our input image in direction of gradients. This will maximize input space which will eventually increase activations of layers.\nThis function also takes *epochs* as parameter which is number of iteration for which we want to process image. *weight* parameter is strength of patterns embed to image. Method also prints stats of each epoch","c93dec20":"Now lets load our input image and display it.","6dfa2208":"Now its time to create deep dream image. Time taken to create dream depends on size of input image passed. It also uses octave technique previously discussed to imporove quality of image but now we can process more octaves.","6faab1dd":"the above function defines our loss function which we will maximize using gradient ascent. It is simply sum of mean of activations","f23b396d":"the above function cancels out effects of preprocessing applied by inception's preprocess_input function. preprocess_input function for inception model scales down pixels of image to be in range -1 to 1 so this function will scale pixels to be in range 0 to 255","dd746e89":"Now we are ready to see how our input image looks like. We first deprocess our numpy array and then convert it to image and finally save output image to hard drive as image.","10c376f1":"Lets describe layers whose patterns we want to embed into our input image. Here we are using *mixed3* and *mixed5* layers which are concatenation of different convolution layers.","a9e250e8":"the above function runs gradient ascent using octave technique. It takes *num_octaves* parameter which is number of octaves you want to process. Default is 2 that means it process 2 octaves and 1 original image. \n\nImage is resized using tensorflow *image.resize* function. New shape is calculated by raising height and width of image to power of octave number to process. As you can notice loops starts from *-num_octaves to 0 (excluding 1)*. Negative power will scale down the image from its base shape. We can also run loop from *-num_octaves to +num_octave* but as image size increases it consume more RAM. \n\n*octave_size* parameter tells by what factor we want to scale images ","2005c310":"the above funtion is same as *run_gradient_ascent_with_octave* but instead of using *get_loss_and_grads* function here we have used *get_loss_and_grads_with_tiling* to get gradients using tiling images strategy.","23925693":"So finally we are ready to see some deep dreams of neural networks. Play with it and share exciting results.","3701545c":"and this time we got some more exicting results also we can create bigger resolution dream.","81f6da2f":"the above function will convert array to image. if deprocessing is true it will first deprocess inception preprocessing and then convert array to image","683f5c2d":"the above function returns gradient (derivative) of our loss function with respect to our input image. We use tensoflow *GradientTape* to calculate gradients. First we have to watch our input image since it is not tensorflow variable then we get our model outputs which are activations of layers in *layers_contributions* and we will use these activations to find loss and finally find out gradient using *tape.gradient* method we also standarize our gradients by dividing it with standard deviation of gradients. A small number *1e-8* is also added to prevent accidently division by 0.\n\nThere is also *total_variation_weight* parameter this will be used for adding some amount of *total_variation* loss into our loss function.\n\nThe total variation loss is the sum of the absolute differences for neighboring pixel-values in the input images. This measures how much noise is in the images.\n\n*total variation loss* is not necessary for deep dream outputs but can be used to smooth out result. play with *total_variation_weight* parameter to find result of your liking.","d959fe3e":"Now we will create dream model using *deep_dream_model* function which we had defined earlier","45b7bcfe":"Implementing deepdream teaches a lot of other concepts of deep learning. It breaks the rule of traditional *model.fit* in every deep learning problem. Also playing with result is quite intresting so get ready for deepdream.","fbfd8aff":"For visualizing patterns learned by convnets we have to maximize response of specific filters. In simple words we have response(activations) of specific filter in a convolutional layer and we change our input space so as to maximize that filter's response by using gradient ascent.","176b87b0":"Lets test how we can extract and manipulate activations of layers which we have defined in *layers_contributions* using our deep dream model","3ae4d26e":"Starting with our input image in which deepdream patterns will be shown as output. First we will define path of our input image.","5dfe8185":"the above function will show image in notebook by first converting array to image","d25e657c":"Now lets define a helper function which will return output of model on providing input. Above we have defined a *lambda* function which takes model and input image as parameter and return output of model *ie..*  activations of layers which we have defined in *layers_contributions*","e8243373":"Now its time to create deep dream image, It takes more time to create deep dream but it worth.","a21c476e":"lets test of how random tiled image function transforms our image and randomly roll it so that we do not get same tile everytime we process and seam across image.","aaf6d30a":"### Steps to create Deepdream\n\n- we start with a image and pass it to a pretrained convolutional neural network like inception or vgg\n- we try to maximize activation of entire layer rather than specific filter for this we define a simple loss function which will maximize activations of layers on maximizing that loss function. So we use mean of activations of layers as loss function.\n- finally we will change our input space(image) by applying that gradient to image which eventually will maximize out loss function.\n- additional steps like tiling and octaves are needed in order to work with large images so that it can be fit efficiently\non RAM and provide better results.","309c223b":"## Deep Dreaming using octaves\n\nTo improve quality of patterns in image we can use octaves technique. In this technique input image is processed at different scale. Each different size image is an octave this improve quality of patterns on image.\n\n### Steps:\n- first base shape of image is saved to a variable\n- input image is then scaled to different sizes smaller and greater than base shape\n- these octaves (different scaled images) are then passed to *run_gradient_ascent* function to apply gradient ascent to each octave.\n- finally resultant image is again resized to base shape","7069e3f8":"# References\n\n- [Tensorflow Tutorials Keras Book](https:\/\/www.tensorflow.org\/tutorials\/generative\/deepdream)\n\n- [Tensorflow offical docs](https:\/\/www.tensorflow.org\/tutorials\/generative\/deepdream)"}}