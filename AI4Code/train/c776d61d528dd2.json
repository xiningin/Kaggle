{"cell_type":{"02e882d1":"code","f0888b34":"code","159e9965":"code","09271d58":"code","a36f2e49":"code","2c6fbd1a":"code","9bb605ac":"code","88146543":"code","c7c65c5f":"code","8294bcc1":"code","72ff44da":"code","29dc7650":"code","67e281c4":"code","f02616f9":"code","4305fd2d":"code","6f4c9408":"code","dbbb5998":"code","98d04c4b":"markdown","837a5b5e":"markdown","013f8017":"markdown","45fab2fc":"markdown","a1b019d7":"markdown","03461f9d":"markdown","4edcb9db":"markdown","de1b7140":"markdown","a6d65ae9":"markdown","4471276e":"markdown","f2589542":"markdown","be657467":"markdown","b068b5a5":"markdown","fe6a2032":"markdown","0290f916":"markdown","783d2f3b":"markdown","6eff5cf3":"markdown"},"source":{"02e882d1":"from keras.datasets import mnist\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom keras.layers import Dense,Flatten ,Reshape\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam","f0888b34":"img_rows = 28\nimg_cols = 28\nchannels = 1\n\n# Input image dimensions\nimg_shape = (img_rows, img_cols, channels)\n\n# Size of the noise vector, used as input to the Generator\nz_dim = 100","159e9965":"def build_generator(img_shape, z_dim):\n\n    model = Sequential()\n\n    # Fully connected layer\n    model.add(Dense(128, input_dim=z_dim))\n\n    # Leaky ReLU activation\n    model.add(LeakyReLU(alpha=0.01))\n\n    # Output layer with tanh activation\n    model.add(Dense(28 * 28 * 1, activation='tanh'))\n\n    # Reshape the Generator output to image dimensions\n    model.add(Reshape(img_shape))\n\n    return model","09271d58":"def build_discriminator(img_shape):\n\n    model = Sequential()\n\n    # Flatten the input image\n    model.add(Flatten(input_shape=img_shape))\n\n    # Fully connected layer\n    model.add(Dense(128))\n\n    # Leaky ReLU activation\n    model.add(LeakyReLU(alpha=0.01))\n\n    # Output layer with sigmoid activation\n    model.add(Dense(1, activation='sigmoid'))\n\n    return model","a36f2e49":"def build_gan(generator, discriminator):\n\n    model = Sequential()\n\n    # Combined Generator -> Discriminator model\n    model.add(generator)\n    model.add(discriminator)\n\n    return model\n\ndiscriminator = build_discriminator(img_shape)\ndiscriminator.compile(loss='binary_crossentropy',\n                      optimizer=Adam(),\n                      metrics=['accuracy'])\n\n\n# Build the Generator\ngenerator = build_generator(img_shape, z_dim)\n\n# Keep Discriminator\u2019s parameters constant for Generator training\ndiscriminator.trainable = False  #This step is very important in order to avoid over training of our discriminator \n\n# Build and compile GAN model with fixed Discriminator to train the Generator\ngan = build_gan(generator, discriminator)\ngan.compile(loss='binary_crossentropy', optimizer=Adam())\n","2c6fbd1a":"\nlosses = []\naccuracies = []\niteration_checkpoints = []\n\n\ndef train(iterations, batch_size, sample_interval):\n\n    # Load the MNIST dataset\n    (X_train, _), (_, _) = mnist.load_data()\n\n    # Rescale [0, 255] grayscale pixel values to [-1, 1]\n    X_train = X_train \/ 127.5 - 1.0\n    X_train = np.expand_dims(X_train, axis=3)\n\n    # Labels for real images: all ones\n    real = np.ones((batch_size, 1))\n\n    # Labels for fake images: all zeros\n    fake = np.zeros((batch_size, 1))\n\n    for iteration in range(iterations):\n\n        # -------------------------\n        #  Train the Discriminator\n        # -------------------------\n\n        # Get a random batch of real images\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        imgs = X_train[idx]\n\n        # Generate a batch of fake images\n        z = np.random.normal(0, 1, (batch_size, 100))\n        gen_imgs = generator.predict(z)\n        \n        \n        # Train Discriminator\n        d_loss_real = discriminator.train_on_batch(imgs, real)\n        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n        d_loss, accuracy = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # ---------------------\n        #  Train the Generator\n        # ---------------------\n\n        # Generate a batch of fake images\n        z = np.random.normal(0, 1, (batch_size, 100))\n        gen_imgs = generator.predict(z)\n\n        # Train Generator\n        g_loss = gan.train_on_batch(z, real)\n\n        if (iteration + 1) % sample_interval == 0:\n\n            # Save losses and accuracies so they can be plotted after training\n            losses.append((d_loss, g_loss))\n            accuracies.append(100.0 * accuracy)\n            iteration_checkpoints.append(iteration + 1)\n\n            # Output training progress\n            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" %\n                  (iteration + 1, d_loss, 100.0 * accuracy, g_loss))\n\n            # Output a sample of generated image\n\n            sample_images(generator)\n\n","9bb605ac":"def sample_images(generator, image_grid_rows=4, image_grid_columns=4):\n\n    # Sample random noise\n    z = np.random.normal(0, 1, (image_grid_rows * image_grid_columns, z_dim))\n\n    # Generate images from random noise\n    gen_imgs = generator.predict(z)\n\n    # Rescale image pixel values to [0, 1]\n    gen_imgs = 0.5 * gen_imgs + 0.5\n\n    # Set image grid\n    fig, axs = plt.subplots(image_grid_rows,\n                            image_grid_columns,\n                            figsize=(4, 4),\n                            sharey=True,\n                            sharex=True)\n\n    cnt = 0\n    for i in range(image_grid_rows):\n        for j in range(image_grid_columns):\n            # Output a grid of images\n            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n            axs[i, j].axis('off')\n            cnt += 1","88146543":"iterations = 20000\nbatch_size = 128\nsample_interval = 1000\n\n# Train the GAN for the specified number of iterations\ntrain(iterations, batch_size, sample_interval)","c7c65c5f":"import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom keras.datasets import mnist\nfrom keras.layers import Activation, BatchNormalization, Dense, Dropout, Flatten, Reshape\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\n","8294bcc1":"img_rows = 28\nimg_cols = 28\nchannels = 1\n\n# Input image dimensions\nimg_shape = (img_rows, img_cols, channels)\n\n# Size of the noise vector, used as input to the Generator\nz_dim = 100\n","72ff44da":"def build_generator(z_dim):\n\n    model = Sequential()\n\n    # Reshape input into 7x7x256 tensor via a fully connected layer\n    model.add(Dense(256 * 7 * 7, input_dim=z_dim))\n    model.add(Reshape((7, 7, 256)))\n\n    # Transposed convolution layer, from 7x7x256 into 14x14x128 tensor\n    model.add(Conv2DTranspose(128, kernel_size=3, strides=2, padding='same'))\n\n    # Batch normalization\n    model.add(BatchNormalization())\n\n    # Leaky ReLU activation\n    model.add(LeakyReLU(alpha=0.01))\n\n    # Transposed convolution layer, from 14x14x128 to 14x14x64 tensor\n    model.add(Conv2DTranspose(64, kernel_size=3, strides=1, padding='same'))\n\n    # Batch normalization\n    model.add(BatchNormalization())\n\n    # Leaky ReLU activation\n    model.add(LeakyReLU(alpha=0.01))\n\n    # Transposed convolution layer, from 14x14x64 to 28x28x1 tensor\n    model.add(Conv2DTranspose(1, kernel_size=3, strides=2, padding='same'))\n\n    # Output layer with tanh activation\n    model.add(Activation('tanh'))\n\n    return model","29dc7650":"def build_discriminator(img_shape):\n\n    model = Sequential()\n\n    # Convolutional layer, from 28x28x1 into 14x14x32 tensor\n    model.add(\n        Conv2D(32,\n               kernel_size=3,\n               strides=2,\n               input_shape=img_shape,\n               padding='same'))\n\n    # Leaky ReLU activation\n    model.add(LeakyReLU(alpha=0.01))\n\n    # Convolutional layer, from 14x14x32 into 7x7x64 tensor\n    model.add(\n        Conv2D(64,\n               kernel_size=3,\n               strides=2,\n               input_shape=img_shape,\n               padding='same'))\n\n    # Batch normalization\n    model.add(BatchNormalization())\n\n    # Leaky ReLU activation\n    model.add(LeakyReLU(alpha=0.01))\n\n    # Convolutional layer, from 7x7x64 tensor into 3x3x128 tensor\n    model.add(\n        Conv2D(128,\n               kernel_size=3,\n               strides=2,\n               input_shape=img_shape,\n               padding='same'))\n\n    # Batch normalization\n    model.add(BatchNormalization())\n\n    # Leaky ReLU activation\n    model.add(LeakyReLU(alpha=0.01))\n\n    # Output layer with sigmoid activation\n    model.add(Flatten())\n    model.add(Dense(1, activation='sigmoid'))\n\n    return model","67e281c4":"def build_gan(generator, discriminator):\n\n    model = Sequential()\n\n    # Combined Generator -> Discriminator model\n    model.add(generator)\n    model.add(discriminator)\n\n    return model\n","f02616f9":"# Build and compile the Discriminator\ndiscriminator = build_discriminator(img_shape)\ndiscriminator.compile(loss='binary_crossentropy',\n                      optimizer=Adam(),\n                      metrics=['accuracy'])\n\n# Build the Generator\ngenerator = build_generator(z_dim)\n\n# Keep Discriminator\u2019s parameters constant for Generator training\ndiscriminator.trainable = False\n\n# Build and compile GAN model with fixed Discriminator to train the Generator\ngan = build_gan(generator, discriminator)\ngan.compile(loss='binary_crossentropy', optimizer=Adam())","4305fd2d":"losses = []\naccuracies = []\niteration_checkpoints = []\n\n\ndef train(iterations, batch_size, sample_interval):\n\n    # Load the MNIST dataset\n    (X_train, _), (_, _) = mnist.load_data()\n\n    # Rescale [0, 255] grayscale pixel values to [-1, 1]\n    X_train = X_train \/ 127.5 - 1.0\n    X_train = np.expand_dims(X_train, axis=3)\n\n    # Labels for real images: all ones\n    real = np.ones((batch_size, 1))\n\n    # Labels for fake images: all zeros\n    fake = np.zeros((batch_size, 1))\n\n    for iteration in range(iterations):\n\n        # -------------------------\n        #  Train the Discriminator\n        # -------------------------\n\n        # Get a random batch of real images\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        imgs = X_train[idx]\n\n        # Generate a batch of fake images\n        z = np.random.normal(0, 1, (batch_size, 100))\n        gen_imgs = generator.predict(z)\n\n        # Train Discriminator\n        d_loss_real = discriminator.train_on_batch(imgs, real)\n        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n        d_loss, accuracy = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # ---------------------\n        #  Train the Generator\n        # ---------------------\n\n        # Generate a batch of fake images\n        z = np.random.normal(0, 1, (batch_size, 100))\n        gen_imgs = generator.predict(z)\n\n        # Train Generator\n        g_loss = gan.train_on_batch(z, real)\n\n        if (iteration + 1) % sample_interval == 0:\n\n            # Save losses and accuracies so they can be plotted after training\n            losses.append((d_loss, g_loss))\n            accuracies.append(100.0 * accuracy)\n            iteration_checkpoints.append(iteration + 1)\n\n            # Output training progress\n            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" %\n                  (iteration + 1, d_loss, 100.0 * accuracy, g_loss))\n\n            # Output a sample of generated image\n            sample_images(generator)","6f4c9408":"def sample_images(generator, image_grid_rows=4, image_grid_columns=4):\n\n    # Sample random noise\n    z = np.random.normal(0, 1, (image_grid_rows * image_grid_columns, z_dim))\n\n    # Generate images from random noise\n    gen_imgs = generator.predict(z)\n\n    # Rescale image pixel values to [0, 1]\n    gen_imgs = 0.5 * gen_imgs + 0.5\n\n    # Set image grid\n    fig, axs = plt.subplots(image_grid_rows,\n                            image_grid_columns,\n                            figsize=(4, 4),\n                            sharey=True,\n                            sharex=True)\n\n    cnt = 0\n    for i in range(image_grid_rows):\n        for j in range(image_grid_columns):\n            # Output a grid of images\n            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n            axs[i, j].axis('off')\n            cnt += 1","dbbb5998":"# Set hyperparameters\niterations = 20000\nbatch_size = 128\nsample_interval = 1000\n\n# Train the DCGAN for the specified number of iterations\ntrain(iterations, batch_size, sample_interval)","98d04c4b":"# DCGAN\n\n> we implemented a GAN whose Generator and Discriminator were simple feed-forward neural networks with a single hidden layer. Despite this simplicity, many of the images of handwritten digits that the GAN\u2019s Generator produced after being fully trained were remarkably convincing. Even the ones that were not recognizable as human-written numerals had many of the hallmarks of handwritten symbols, such as discernible line edges and shapes\u2014especially when compared to the random noise used as the Generator\u2019s raw input.\n> Imagine what we could accomplish with more powerful network architecture. In this chapter, we will do just that: instead of simple two-layer feed-forward networks, both our Generator and Discriminator will be implemented as convolutional neural networks (CNNs, or ConvNets). The resulting GAN architecture is known as Deep Convolutional GAN, or DCGAN for short.","837a5b5e":"# 5.Training the model","013f8017":"# REFRENCES\n* https:\/\/pathmind.com\/wiki\/generative-adversarial-network-gan\n* GANs in Acition (https:\/\/learning.oreilly.com\/library\/view\/gans-in-action\/9781617295560\/OEBPS\/Text\/kindle_split_001.html)\n* https:\/\/papers.nips.cc\/paper\/5423-generative-adversarial-nets.pdf","45fab2fc":"# WORKING OF GANs\n\nOne neural network, called the generator, generates new data instances, while the other, the discriminator, evaluates them for authenticity; i.e. the discriminator decides whether each instance of data that it reviews belongs to the actual training dataset or not.\n\nLet\u2019s say we\u2019re trying to do something more banal than mimic the Mona Lisa. We\u2019re going to generate hand-written numerals like those found in the MNIST dataset, which is taken from the real world. The goal of the discriminator, when shown an instance from the true MNIST dataset, is to recognize those that are authentic.\n\nMeanwhile, the generator is creating new, synthetic images that it passes to the discriminator. It does so in the hopes that they, too, will be deemed authentic, even though they are fake. The goal of the generator is to generate passable hand-written digits: to lie without being caught. The goal of the discriminator is to identify images coming from the generator as fake.\n\nHere are the steps a GAN takes:\n\nThe generator takes in random numbers and returns an image.\nThis generated image is fed into the discriminator alongside a stream of images taken from the actual, ground-truth dataset.\nThe discriminator takes in both real and fake images and returns probabilities, a number between 0 and 1, with 1 representing a prediction of authenticity and 0 representing fake.\nSo you have a double feedback loop:\n\nThe discriminator is in a feedback loop with the ground truth of the images, which we know.\nThe generator is in a feedback loop with the discriminator.","a1b019d7":"# 6. Generating Images","03461f9d":"# 2.The Generator","4edcb9db":"# 1.Importing Libraries","de1b7140":"# 4.GAN algorithm","a6d65ae9":"# 5. Training Model","4471276e":"# 3. The Discriminator","f2589542":"# Simple GAN for generating MNIST data\n\n# 1.Importing Libraries","be657467":"# 4. GAN Algorithm","b068b5a5":"# GANs\n\n>Generative Adversarial Networks (GANs) are a class of machine learning techniques that consist of two simultaneously trained models: one (the Generator) trained to generate fake data, and the other (the Discriminator) trained to discern the fake data from real examples. ","fe6a2032":"# 2.The Generator ","0290f916":"![image.png](attachment:image.png)","783d2f3b":"# 6.Generating the Output Images","6eff5cf3":"# 3.The Discriminator "}}