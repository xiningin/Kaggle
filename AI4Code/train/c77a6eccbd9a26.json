{"cell_type":{"443870b6":"code","9a8a7f83":"code","87d80a20":"code","1828c2fb":"code","24619d3f":"code","f3bbc9d9":"code","0acb458a":"code","038e5231":"code","e43a0bc0":"code","8de54a30":"code","2725a46a":"code","84fadd02":"code","aef289a1":"code","06aa8412":"code","a3b5624b":"code","5826fa8b":"code","066344bc":"code","4542928a":"code","b6dd111d":"code","869c430c":"code","66c9ed5e":"code","940491fa":"code","1591d1a7":"code","992e45ac":"code","81498804":"code","abdf9f2d":"code","8a100689":"code","251af247":"code","60e3da02":"code","a8f96d41":"code","ebdd172c":"code","53c67bfd":"code","f9bd0658":"code","e7078795":"code","7c50a2fc":"code","9c2152a2":"code","f837c516":"code","fedaf65e":"code","53106405":"code","adb974a3":"code","ad725024":"code","8684c7c0":"code","ec774f88":"code","cbbe2255":"markdown","a09aa57c":"markdown","51f33ec0":"markdown","c3d48a22":"markdown","8333297f":"markdown","e29dab2f":"markdown","732e1593":"markdown","fe3680d4":"markdown","2de037d2":"markdown","f24916d8":"markdown","2da740b2":"markdown","a7674d5a":"markdown","7b86d970":"markdown","11f9abea":"markdown","59456ec0":"markdown","ab66a3d2":"markdown","f98181fd":"markdown","a401b6e0":"markdown","22ca5576":"markdown"},"source":{"443870b6":"!pip install chart_studio\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom wordcloud import WordCloud, STOPWORDS\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nimport re\nimport string\n\nimport matplotlib.pyplot as plt\nfrom plotly import tools\nimport chart_studio.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9a8a7f83":"train = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv')","87d80a20":"train['text'] = train['text'].str.replace('[{}]'.format(string.punctuation), '')\ntest['text'] = test['text'].str.replace('[{}]'.format(string.punctuation), '')","1828c2fb":"train.head(3)","24619d3f":"test.head(3)","f3bbc9d9":"print(train.size)\nprint(train.shape)\nprint(test.shape)\nprint(test.size)","0acb458a":"train.describe()","038e5231":"sns.countplot(train['sentiment'])","e43a0bc0":"# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(10.0,10.0), color = 'white',\n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color=color,\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=600, \n                    height=300,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train.loc[train['sentiment'] == 'neutral', 'text'].append(test.loc[test['sentiment'] == 'neutral', 'text']), title=\"Word Cloud of Neutral tweets\",color = 'white')","8de54a30":"plot_wordcloud(train.loc[train['sentiment'] == 'positive', 'text'].append(test.loc[test['sentiment'] == 'positive', 'text']), title=\"Word Cloud of Positive tweets\",color = 'green')","2725a46a":"plot_wordcloud(train.loc[train['sentiment'] == 'negative', 'text'].append(test.loc[test['sentiment'] == 'negative', 'text']), title=\"Word Cloud of negative tweets\",color = 'red')","84fadd02":"from collections import defaultdict\ntrain0_df = train[train[\"sentiment\"]=='positive'].dropna().append(test[test[\"sentiment\"]=='positive'].dropna())\ntrain1_df = train[train[\"sentiment\"]=='neutral'].dropna().append(test[test[\"sentiment\"]=='neutral'].dropna())\ntrain2_df = train[train[\"sentiment\"]=='negative'].dropna().append(test[test[\"sentiment\"]=='neutral'].dropna())\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(25), 'red')\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(25), 'green')\n\n## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train2_df[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(25), 'orange')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=1, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of positive tweets\", \"Frequent words of neutral tweets\",\n                                          \"Frequent words of negative tweets\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 2, 1)\nfig.append_trace(trace2, 3, 1)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\niplot(fig, filename='word-plots')\n","aef289a1":"freq_dict = defaultdict(int)\nfor sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(25), 'gray')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(25), 'orange')\n\nfreq_dict = defaultdict(int)\nfor sent in train2_df[\"text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(25), 'brown')\n\n\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=1, vertical_spacing=0.04,horizontal_spacing=0.25,\n                          subplot_titles=[\"Bigram plots of Positive tweets\", \n                                          \"Bigram plots of Neutral tweets\",\n                                          \"Bigram plots of Negative tweets\"\n                                          ])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 2, 1)\nfig.append_trace(trace2, 3, 1)\n\n\nfig['layout'].update(height=1000, width=800, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Plots\")\niplot(fig, filename='word-plots')","06aa8412":"for sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(25), 'blue')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(25), 'green')\n\nfreq_dict = defaultdict(int)\nfor sent in train2_df[\"text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(25), 'violet')\n\n\n\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=1, vertical_spacing=0.04, horizontal_spacing=0.05,\n                          subplot_titles=[\"Tri-gram plots of Positive tweets\", \n                                          \"Tri-gram plots of Neutral tweets\",\n                                          \"Tri-gram plots of Negative tweets\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 2, 1)\nfig.append_trace(trace2, 3, 1)\nfig['layout'].update(height=1200, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Trigram Count Plots\")\niplot(fig, filename='word-plots')","a3b5624b":"train[\"num_words\"] = train[\"text\"].apply(lambda x: len(str(x).split()))\ntest[\"num_words\"] = test[\"text\"].apply(lambda x: len(str(x).split()))\ntrain['select_num_words'] = train[\"selected_text\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\ntrain[\"num_unique_words\"] = train[\"text\"].apply(lambda x: len(set(str(x).split())))\ntest[\"num_unique_words\"] = test[\"text\"].apply(lambda x: len(set(str(x).split())))\ntrain['select_num_unique_words'] = train[\"selected_text\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\ntrain[\"num_chars\"] = train[\"text\"].apply(lambda x: len(str(x)))\ntest[\"num_chars\"] = test[\"text\"].apply(lambda x: len(str(x)))\ntrain['select_num_chars'] = train[\"selected_text\"].apply(lambda x: len(str(x)))","5826fa8b":"fig = go.Figure()\nfig.add_trace(go.Histogram(x=train['num_words'],name = 'Number of words in text of train data'))\nfig.add_trace(go.Histogram(x=test['num_words'],name = 'Number of words in text of test data'))\nfig.add_trace(go.Histogram(x=train['select_num_words'],name = 'Number of words in selected text'))\n\n# Overlay both histograms\nfig.update_layout(barmode='stack')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\nfig.show()","066344bc":"fig_ = go.Figure()\nfig_.add_trace(go.Histogram(x=train['num_chars'],name = 'Number of characters in text of train data',marker = dict(color = 'rgba(222, 111, 33, 0.8)')))\nfig_.add_trace(go.Histogram(x=test['num_chars'],name = 'Number of characters in text of test data',marker = dict(color = 'rgba(33, 1, 222, 0.8)')))\nfig_.add_trace(go.Histogram(x=train['select_num_chars'],name = 'Number of characters in selected text',marker = dict(color = 'rgba(108, 25, 7, 0.8)')))\n\n# Overlay both histograms\nfig_.update_layout(barmode='stack')\n# Reduce opacity to see both histograms\nfig_.update_traces(opacity=0.75)\nfig_.show()","4542928a":"fig_ = go.Figure()\nfig_.add_trace(go.Histogram(x=train['num_unique_words'],name = 'Number of unique words in text of train data',marker = dict(color = 'rgba(222, 1, 3, 0.8)')))\nfig_.add_trace(go.Histogram(x=test['num_unique_words'],name = 'Number of unique words in text of test data',marker = dict(color = 'rgba(3, 221, 2, 0.8)')))\nfig_.add_trace(go.Histogram(x=train['select_num_unique_words'],name = 'Number of unique words in selected text',marker = dict(color = 'rgba(1, 2, 237, 0.8)')))\n\n# Overlay both histograms\nfig_.update_layout(barmode='stack')\n# Reduce opacity to see both histograms\nfig_.update_traces(opacity=0.75)\nfig_.show()","b6dd111d":"train.head()","869c430c":"train.tail()","66c9ed5e":"test.head()","940491fa":"number_of_class_labels=len(train['sentiment'].unique())\nnumber_of_class_labels","1591d1a7":"Count_Row=train.shape[0] \nCount_Col=train.shape[1] \nprint(Count_Col)\nprint(Count_Row)\nprint(train.shape)","992e45ac":"class_prob_df = pd.DataFrame(columns=['sentiment', 'probability'], index=range(number_of_class_labels))\nclass_prob_df","81498804":"i=0\nfor val, cnt in train['sentiment'].value_counts().iteritems():\n    print ('value', val, 'was found', cnt, 'times')\n    class_prob_df.loc[i].sentiment = val\n    class_prob_df.loc[i].probability = cnt\/Count_Row\n    i = i +1\n    \nclass_prob_df","abdf9f2d":"import nltk\nnltk.download('punkt')","8a100689":"train['selected_text'].dtype","251af247":"train.head(2)","60e3da02":"train.head()","a8f96d41":"train['selected_text']=train['selected_text'].apply(str)","ebdd172c":"train.head()","53c67bfd":"import re\nfrom nltk import sent_tokenize\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\nall_tokens = []\n\nfor idx, row in train.iterrows():\n    for word in word_tokenize(row.selected_text):\n        all_tokens.append(word)\n    \nprint(len(all_tokens), all_tokens)","f9bd0658":"all_tokens_unique = set(all_tokens)\nprint(len(all_tokens_unique), all_tokens_unique)","e7078795":"stop_words = set(stopwords.words('english'))\n\ntokens = [w for w in all_tokens_unique if not w in stop_words]\nprint(len(tokens), tokens)\n\ntokens1=[]\ntokens = [word for word in tokens if word.isalpha()]\nprint(len(tokens), tokens)","7c50a2fc":"word = ['@', 'rr', '!', '$', '@', 'jfjf', '&','(', ')', ',']\nfor word in word:\n    if word.isalpha():\n        print(\"yes it is alpha: \", word)","9c2152a2":"train.values","f837c516":"merged_train_df = train.groupby('sentiment')['selected_text'].apply(' '.join).reset_index()\n\nmerged_train_df","fedaf65e":"for idx, row in merged_train_df.iterrows():\n    \n    temp1_tokens = []\n    for word in word_tokenize(row.selected_text):\n        temp1_tokens.append(word)\n    \n    temp1_tokens = set(temp1_tokens)\n         \n    temp2_tokens = []\n    for word in temp1_tokens:\n        if not word in stop_words:\n            temp2_tokens.append(word)           \n    \n    temp3_tokens = []\n    for word in temp2_tokens:\n        if word.isalpha():\n            temp3_tokens.append(word)\n            \n    print(temp3_tokens)\n    temp4_tokens = \" \".join(temp3_tokens)\n    print(temp4_tokens)\n    \n    merged_train_df.at[idx, 'selected_text'] = temp4_tokens\n    merged_train_df.at[idx, 'no_of_words_in_category'] = len(temp3_tokens)","53106405":"merged_train_df.head()","adb974a3":"merged_train_df = pd.merge(merged_train_df, class_prob_df[['sentiment', 'probability']], on='sentiment')\nmerged_train_df","ad725024":"final_df = pd.DataFrame()\n\nrow_counter = 0\n\nfor idx, row in merged_train_df.iterrows():\n    for token in tokens:\n        # find the number of occurances of the token in the current category of documents\n        no_of_occurances = row.selected_text.count(token)\n        no_of_words_in_category = row.no_of_words_in_category\n        no_unique_words_all = len(tokens)\n        \n        prob_of_token = (no_of_occurances+ 1)\/ (no_of_words_in_category+ no_unique_words_all)\n        #print(row.class_label, token, no_of_occurances, prob_of_token)\n        final_df.at[row_counter, 'Result'] = row.sentiment\n        final_df.at[row_counter, 'token'] = token\n        final_df.at[row_counter, 'no_of_occurances'] = no_of_occurances\n        final_df.at[row_counter, 'no_of_words_in_category'] = no_of_words_in_category\n        final_df.at[row_counter, 'no_unique_words_all'] = no_unique_words_all\n        final_df.at[row_counter, 'prob_of_token_category'] = prob_of_token\n        \n        row_counter = row_counter + 1","8684c7c0":"final_df","ec774f88":"# Calculate P(Category\/Document) \n#      = P(Category) * P(Word1\/Category) * P(Word2\/Category) * P(Word3\/Category)\n\n# P(Auto\/D6) = P(Auto) * P(Engine\/Auto) * P(Noises\/Auto) * P(Car\/Auto)\nfor idx, row in test.iterrows():\n    \n    # tokenize & unique words\n    temp1_tokens = []\n    for word in word_tokenize(row.sentiment):\n        temp1_tokens.append(word)\n        #temp1_tokens = set(temp1_tokens)\n        \n    # remove stop words\n    temp2_tokens = []\n    for word in temp1_tokens:\n        if not word in stop_words:\n            temp2_tokens.append(word)\n          \n    # remove punctuations\n    temp3_tokens = []\n    for word in temp2_tokens:\n        if word.isalpha():\n            temp3_tokens.append(word)\n            \n    #temp4_tokens = \" \".join(temp3_tokens)\n    #print(temp4_tokens)\n    \n    prob = 1 \n    \n    # process for each class_label\n    for idx1, row1 in merged_train_df.iterrows():\n        print(\"class: \"+ row1.sentiment)\n        for token in temp3_tokens:\n            # find the token in final_df for the given category, get the probability\n            # row1.class_label & token\n        \n            print(\"      : \"+ token)  \n        \n            temp_df = final_df[(final_df['Result'] == row1.sentiment) & (final_df['token'] == token)]\n\n            # process for exception\n            if (temp_df.shape[0] == 0):\n                token_prob = 1\/(row1.no_of_words_in_category+ no_unique_words_all)\n                print(\"       no token found prob :\", token_prob)\n                prob = prob * token_prob\n            else:\n                token_prob = temp_df.get_value(temp_df.index[0],'prob_of_token_category')\n                print(\"       token prob          :\", token_prob)\n                prob = prob * token_prob\n\n            prob = prob * row1.probability\n\n        col_at = 'prob_'+row1.sentiment\n\n        test.at[idx, col_at] = prob\n\n\ntest","cbbe2255":"# Word clouds of Text:","a09aa57c":"# Histogram plots of Number of characters","51f33ec0":"## 4.Histogram plot of Number of words","c3d48a22":"From above Ngaram analysis we can observe that neutral tweets and negative tweets had more amount of repeteted words than positive tweets.","8333297f":" WE HAVE CALCULATED PROBABILITIES OF BECOMING POSITIVE , NEGATIVE and NEUTRAL FOR EACH AND EVERY SENTENCE","e29dab2f":"We can see that number of unique words in train and test sets range from 1 to 26. In selected text most number  ","732e1593":"We can observe from above histogram plot that the number of words in train text and test text ranges from 1 to 30.Selected text words mostly fall in range of 1-10. ","fe3680d4":"**NOW BASIC EDA, TEXT PREPROCESSING IS COMPLETED **","2de037d2":"# Text Data Preprocessing","f24916d8":"# Reading the data","2da740b2":"PLEASE **UPVOTE **IF YOU LIKED IT OR USEFUL AND KEEP ME MOTIVATED \n","a7674d5a":"We got probability of each and every word. By using this probabiity we can determine whether the word is positive, negative or neutral","7b86d970":"## 3.Tri-gram Plots:","11f9abea":"From above plot we can see that number of characters in test and train set was in same range.In selected text the range flows from 3 to 138 Characters.","59456ec0":"## 2.Bi-gram Plots:","ab66a3d2":"**THANK YOU IN ADVANCE AND ALL THE BEST**","f98181fd":"## 1.Ngram Analysis:","a401b6e0":"**BASED ON THIS PROBABILITY WE CAN CLASSIFY WETHER THE SNTENCE IS NEGATIVE, NEUTRAL or POSITIVE**","22ca5576":"# Pos-Neg-Neutral Classification"}}