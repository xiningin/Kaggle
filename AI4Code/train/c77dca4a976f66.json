{"cell_type":{"ebc0a338":"code","20966fee":"code","a326eaa3":"code","ae73741d":"code","9ca0657f":"code","cd123a7b":"code","7cd7c833":"code","c551e0fa":"code","adfaf5b6":"code","c856dfa6":"code","2f309518":"code","60dbb596":"code","1efa6af5":"code","9ec67c82":"code","41cb5239":"code","9a7e193f":"code","ac115955":"code","50f05599":"code","a837e845":"code","7eaa49cd":"code","7aeac4c8":"code","96d3b70e":"code","54a96dac":"code","f7aa04f2":"code","468dc1d1":"code","24ae8d1d":"code","b3a338fe":"code","0ebacc9c":"code","fc66fc85":"code","be4bac8a":"code","853ab75d":"code","953754fe":"code","9995e99f":"code","5746a0ff":"code","f0cb78e2":"code","2714b801":"code","cdfca20f":"code","12140049":"code","879eec7f":"code","c619d6e4":"code","e08e71dd":"code","193eb2b6":"code","eb6074b5":"code","0aa2b065":"code","a129aa3e":"markdown","da13b0ed":"markdown","6a9197a1":"markdown","46337485":"markdown","9f7abd4b":"markdown","94d6993e":"markdown","49ef1816":"markdown","ee7968c0":"markdown","cddf60b4":"markdown","31078961":"markdown","65a99537":"markdown","d662d375":"markdown","7f6c5dc6":"markdown","c0856319":"markdown","edb7a54e":"markdown","f94ad83a":"markdown","3b5f39e3":"markdown","5bddf818":"markdown","9c5421c8":"markdown"},"source":{"ebc0a338":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","20966fee":"import re\nfrom collections import Counter\nimport time\nimport pickle\nimport itertools\n\n# Visualization\nimport matplotlib.pyplot as plt\nplt.style.use(\"fivethirtyeight\")\nimport seaborn as sns\n\n# data preprocessing\nfrom sklearn.model_selection import train_test_split\n\n# Validation\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\n# text preprocessing\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\n# Kearas\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense, Embedding, LSTM, Dropout, BatchNormalization, Conv1D, Flatten, MaxPooling1D\nfrom keras import utils\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\n\n# Word2Vec\nimport gensim","a326eaa3":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsample = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","ae73741d":"train.head()","9ca0657f":"test.head()","cd123a7b":"sample.head()","7cd7c833":"print(\"train data size:\", train.shape)\nprint(\"test data size:\", test.shape)","c551e0fa":"print(\"train_data null count\\n\", train.isnull().sum())","adfaf5b6":"print(\"test_data null count\\n\", test.isnull().sum())","c856dfa6":"print(\"train_data data type\\n\", train.dtypes)","2f309518":"print(\"test_data data type\\n\", test.dtypes)","60dbb596":"target_cnt = Counter(train[\"target\"])\nprint(target_cnt)\n\nplt.figure(figsize=(10,6))\nplt.bar([str(i) for i in target_cnt.keys()], target_cnt.values())\nplt.xlabel(\"Target flag\")\nplt.ylabel(\"Count\")\nplt.title(\"target count distribution\")","1efa6af5":"# key \ntrain_keyw = pd.DataFrame({\"keyword\":train[\"keyword\"].value_counts().index,\n              \"Train_count\":train[\"keyword\"].value_counts()})\ntest_keyw = pd.DataFrame({\"keyword\":test[\"keyword\"].value_counts().index,\n              \"Test_count\":test[\"keyword\"].value_counts()})\nkeyw = pd.merge(train_keyw, test_keyw, left_on=\"keyword\", right_on=\"keyword\", how=\"outer\").reset_index().fillna(0)\nprint(\"keyword sample data shape:\", keyw.shape)\nkeyw.head()","9ec67c82":"# Visualization\nplt.figure(figsize=(30,6))\nplt.bar(keyw.keyword, keyw.Train_count, width=0.4)\nplt.bar(keyw.keyword, keyw.Test_count, width=0.4)\nplt.xlabel(\"Key word\")\nplt.xticks(rotation=90)\nplt.ylabel(\"Count\")\nplt.title(\"Keyword null count\\nTrain_data:{}\/Test_data:{}\".format(train.keyword.isnull().sum(), test.keyword.isnull().sum()))","41cb5239":"# location\ntrain_loc = pd.DataFrame({\"location\":train[\"location\"].value_counts().index,\n              \"Train_count\":train[\"location\"].value_counts()})\ntest_loc = pd.DataFrame({\"location\":test[\"location\"].value_counts().index,\n              \"Test_count\":test[\"location\"].value_counts()})\nloc = pd.merge(train_loc, test_loc, left_on=\"location\", right_on=\"location\", how=\"outer\").reset_index().fillna(0)\nprint(\"loc sample data shape:\", loc.shape)\nloc.head()","9a7e193f":"# Stop word\nstop_words = stopwords.words(\"english\")\nstemmer = SnowballStemmer(\"english\")\n\ndef preprocessing_text(text, stem=False):\n    text = re.sub(r\",\", '', str(text).lower())\n    text = re.sub(r\"https?:\\S\", ' ', str(text))\n    text = re.sub(r\"[^a-zA-Z]+\", ' ', str(text)).strip()\n    \n    tokens = []\n    for token in text.split():\n        if token not in stop_words:\n            if stem:\n                tokens.append(stemmer.ste,(token))\n            else:\n                tokens.append(token)\n    return ' '.join(tokens)","ac115955":"# train data set\ntrain[\"cleaned_text\"] = train[\"text\"].apply(lambda x:preprocessing_text(x))","50f05599":"# test data set\ntest[\"cleaned_text\"] = test[\"text\"].apply(lambda x:preprocessing_text(x))","a837e845":"# fillna\ntrain[\"keyword\"].fillna(\"Nan_keyw\", inplace=True)\ntrain[\"location\"].fillna(\"Nan_loc\", inplace=True)\ntest[\"keyword\"].fillna(\"Nan_keyw\", inplace=True)\ntest[\"location\"].fillna(\"Nan_loc\", inplace=True)","7eaa49cd":"# Combine keyword + location to text\ntrain[\"cleaned_text\"] = train[\"keyword\"] + str(\" \") + train[\"location\"] +  str(\" \") + train[\"cleaned_text\"]\ntest[\"cleaned_text\"] = test[\"keyword\"] +  str(\" \") + test[\"location\"] +  str(\" \") + test[\"cleaned_text\"]","7aeac4c8":"# Create train and val data\nX = train[\"cleaned_text\"]\ny = train[\"target\"]","96d3b70e":"doc = [w.split() for w in X.values]","54a96dac":"# Checking\nnp.array(doc)[0]","f7aa04f2":"# Word2Vec model\nw2v = gensim.models.word2vec.Word2Vec(size = 42, \n                                      window = 8,\n                                      alpha = 0.03,\n                                      workers = 8)\n\nw2v.build_vocab(doc)","468dc1d1":"words = w2v.wv.vocab.keys()\nprint(\"words size:{}\".format(len(words)))","24ae8d1d":"%%time\n# Training w2v model\nw2v.train(doc, total_examples=len(doc), epochs=64)","b3a338fe":"# example : like\nw2v.most_similar(\"like\")","0ebacc9c":"# example : sad\nw2v.most_similar(\"sad\")","fc66fc85":"# example : nice\nw2v.most_similar(\"nice\")","be4bac8a":"# Create train and val data\nX = train[\"cleaned_text\"]\ny = train[\"target\"]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=20)\n\nprint(\"X_train_data size:\", len(X_train))\nprint(\"X_test_data size:\", len(X_val))\n\n\n# tokenizer and keras setting\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X)\n\nX = pad_sequences(tokenizer.texts_to_sequences(X), maxlen=42)\nX_train = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=42)\nX_val = pad_sequences(tokenizer.texts_to_sequences(X_val), maxlen=42)","853ab75d":"# Data dimension check\nprint(\"X_train shape:\", X_train.shape)\nprint(\"X_val shape:\", X_val.shape)","953754fe":"# target data dimension change and check\ny_train = np.array(y_train).reshape(-1,1)\ny_val = np.array(y_val).reshape(-1,1)\n\nprint(\"y_train shape:\", y_train.shape)\nprint(\"y_val shape:\", y_val.shape)","9995e99f":"# Embeddint matrix\nembedding_matrix = np.zeros((len(words)+1, 42))\nfor word, i in tokenizer.word_index.items():\n    if word in w2v.wv:\n        embedding_matrix[i] = w2v.wv[word]\n\nembedding_l = Embedding(len(words)+1, 42, weights=[embedding_matrix], input_length=42, trainable=False)\n\n# Create keras model\ndef define_model():\n    model = Sequential()\n    model.add(embedding_l)\n    model.add(Dropout(0.15))\n    \n    model.add(Dense(256, activation=\"relu\"))\n    model.add(Conv1D(filters=4, strides=1, kernel_size=2))\n    model.add(BatchNormalization())\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Dropout(0.3))\n    \n    model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n    \n    model.add(Dense(16, activation=\"relu\"))\n    model.add(Dropout(0.2))\n    \n    model.add(Dense(1, activation='sigmoid'))\n    \n    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=0.0001), metrics=[\"accuracy\"])\n    \n    return model","5746a0ff":"model = define_model()\n\n# callbacks\nes = EarlyStopping(monitor=\"val_loss\", patience=20)\nms = ModelCheckpoint(\"emb_lstm_v1\", monitor=\"val_loss\", save_best_only=True, verbose=1)\n\nmodel.summary()","f0cb78e2":"%%time\n# Execute fitting\nhistory = model.fit(X_train, y_train, batch_size=256, epochs=300, validation_data=(X_val, y_val), verbose=1, callbacks=[es,ms])","2714b801":"# Visualization\n \ntrain_loss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\ntrain_acc = history.history[\"accuracy\"]\nval_acc = history.history[\"val_accuracy\"]\n\n# Visualization\nfig, ax = plt.subplots(1,2,figsize=(20,6))\nax[0].plot(range(len(train_loss)), train_loss, label=\"train_loss\")\nax[0].plot(range(len(val_loss)), val_loss, label=\"val_loss\")\nax[0].set_xlabel(\"epoch\")\nax[0].set_ylabel(\"loss\")\nax[0].legend()\n\nax[1].plot(range(len(train_acc)), train_acc, label=\"train_acc\")\nax[1].plot(range(len(val_acc)), val_acc, label=\"val_acc\")\nax[1].set_xlabel(\"epoch\")\nax[1].set_ylabel(\"accuracy\")\nax[1].legend()","cdfca20f":"# Validation of val data\ny_pred = load_model(\"emb_lstm_v1\").predict(X_val)","12140049":"y_prediction = []\nfor i in range(len(y_pred)):\n    y_ = y_pred[i][0]\n    if y_ >= 0.5:\n        res = 1\n    else:\n        res = 0\n    y_prediction.append(res)","879eec7f":"print(\"accuracy_score\\n\", accuracy_score(y_val, y_prediction))\nprint(\"confusion_matrix\\n\", confusion_matrix(y_val, y_prediction))\nprint(\"classification_report\\n\", classification_report(y_val, y_prediction))","c619d6e4":"# Test data preprocessing\nX_test = test[\"cleaned_text\"]\n\nX_test = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=42)","e08e71dd":"y_test_pred = load_model(\"emb_lstm_v1\").predict(X_test)\n\ny_test_prediction = []\nfor i in range(len(y_test_pred)):\n    y_ = y_test_pred[i][0]\n    if y_ >= 0.5:\n        res = 1\n    else:\n        res = 0\n    y_test_prediction.append(res)","193eb2b6":"sample[\"target\"] = y_test_prediction","eb6074b5":"sample[\"target\"].value_counts()","0aa2b065":"sample.to_csv(\"submission.csv\", index=False)","a129aa3e":"### data head","da13b0ed":"# Predict classification tweets, real or not","6a9197a1":"This is my first try of text analysis.\n\n- Create Word2Vec model\n- Prediction of classification, with vetorize by tokenizer and LSTM & CNN model","46337485":"# Data loading and Data checking","9f7abd4b":"I'll fill it with string \"Nan\" in Nan because I'll combine it as text later.","94d6993e":"### data size","49ef1816":"## Keyword ditribution","ee7968c0":"## train data target data count","cddf60b4":"The word similarity is not so good. It may be said that the words are categorized like the opposite.","31078961":"## Location ditribution","65a99537":"# EDA","d662d375":"### Null data","7f6c5dc6":"# Create Prediction model","c0856319":"### Confirming Word2Vec model by Similarity result","edb7a54e":"### data type","f94ad83a":"# Create Word2Vec model","3b5f39e3":"# Prediction","5bddf818":"# Text data preprocessing","9c5421c8":"## Libraries"}}