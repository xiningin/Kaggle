{"cell_type":{"066c5fa3":"code","58970162":"code","17593428":"code","1b74f225":"code","0ab069fb":"code","0dff2f1f":"code","5a808bd7":"code","c14e9bd6":"code","2ad8865b":"code","7714b749":"code","b33fdb05":"code","f6a57873":"code","64fb7ed4":"code","7e7a4983":"code","f1143b2b":"code","e659c366":"code","cad67ed8":"code","1b3db588":"code","7e21d966":"code","4225fd64":"code","775da536":"code","7340b970":"code","ede8a324":"code","836f5bc5":"code","a73abac5":"code","58af8778":"markdown","d8df2826":"markdown","ab5a615e":"markdown","c9d604f3":"markdown","d58eb60d":"markdown","9b61b4bf":"markdown","16a1d22a":"markdown","8ab77b67":"markdown","56b43c4a":"markdown","bb3889a8":"markdown"},"source":{"066c5fa3":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf","58970162":"df = pd.read_csv('\/kaggle\/input\/sunspots\/Sunspots.csv')\n\nprint(df.shape)\ndf.head()","17593428":"time = df.iloc[:, 0]\nseries = df.iloc[:, 2]\n\nprint(time.shape)\nprint(series.shape)","1b74f225":"def plot_series(time, series, fmt='-', start=0, end=None):\n    plt.plot(time[start:end], series[start:end], fmt)\n    plt.xlabel('Time')\n    plt.ylabel('Value')\n    plt.grid('on')","0ab069fb":"plt.figure(figsize=(15, 5))\nplot_series(time, series)","0dff2f1f":"plt.figure(figsize=(10, 6))\nplot_series(time, series, start=1000, end=1300)","5a808bd7":"split_time = 3000\ntime_train = time[:split_time]\nX_train = series[:split_time]\ntime_val = time[split_time:]\nX_val = series[split_time:]\n\nplt.figure(figsize=(15, 5))\nplot_series(time_train, X_train)\nplot_series(time_val, X_val)\nplt.legend(['Train', 'Validation'])","c14e9bd6":"def windowed_dataset(series, window_size, batch_size, shuffle=True, shuffle_buffer=None):\n    series = tf.expand_dims(series, axis=-1)\n    dataset = tf.data.Dataset.from_tensor_slices(series)\n    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)   # step once and slice series into (window_shape + 1) windows. [+1 for output]\n    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))  # convert them into tensors\n    if shuffle:\n        dataset = dataset.shuffle(shuffle_buffer)  # shuffling windows to get rid of 'sequence bias'\n    dataset = dataset.map(lambda window: (window[:-1], window[1:]))   # making (x, y) split\n    dataset = dataset.batch(batch_size).prefetch(1)   # batching (x, y) into batch_size sets\n    return dataset","2ad8865b":"def model_forecast(model, series, window_size, batch_size):\n    dataset = tf.data.Dataset.from_tensor_slices(series)\n    dataset = dataset.window(window_size, shift=1, drop_remainder=True)\n    dataset = dataset.flat_map(lambda window: window.batch(window_size)) \n    dataset = dataset.batch(batch_size).prefetch(1)\n    forecast = model.predict(dataset)\n    return forecast","7714b749":"# Hyperparameters\nwindow_size = 64\ntrain_batch_size = 256\nval_batch_size = 32","b33fdb05":"tf.keras.backend.clear_session()\ntrain_set = windowed_dataset(X_train, window_size, train_batch_size, shuffle_buffer=len(X_train))\nval_set = windowed_dataset(X_val, window_size, val_batch_size, shuffle=False)\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv1D(filters=32, kernel_size=5,\n                          strides=1, padding='causal',\n                          activation='relu',\n                          input_shape=[None, 1]),\n    tf.keras.layers.LSTM(64, return_sequences=True),\n    tf.keras.layers.LSTM(64, return_sequences=True),\n    tf.keras.layers.Dense(30, activation='relu'),\n    tf.keras.layers.Dense(10, activation='relu'),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x: x * 400)\n])\n\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch \/ 20))\noptimizer  =tf.keras.optimizers.SGD(learning_rate=1e-8, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=['mae'])\nhistory = model.fit(train_set, \n                    epochs=100,\n                    validation_data=val_set,\n                    callbacks=[lr_scheduler])","f6a57873":"# Training, Validation loss & mae\nplt.figure(figsize=(15, 5))\nplt.subplot(121)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend(['Training', 'Validation'])\nplt.title('Training & Validation Loss')\n\nplt.subplot(122)\nplt.plot(history.history['mae'])\nplt.plot(history.history['val_mae'])\nplt.xlabel('epochs')\nplt.ylabel('mae')\nplt.legend(['Training', 'Validation'])\nplt.title('Training & Validation MAE')\nplt.show()","64fb7ed4":"# plotting learning-rate vs loss\nplt.semilogx(history.history['lr'], history.history['loss'])\nplt.axis([1e-8, 1e-3, 0, 80])\nplt.axvline(8e-6, color='orange', alpha=0.4)","7e7a4983":"# Hyperparameters\nwindow_size = 60\ntrain_batch_size = 100\nval_batch_size = 32","f1143b2b":"tf.keras.backend.clear_session()\ntrain_set = windowed_dataset(X_train, window_size, train_batch_size, shuffle_buffer=len(X_train))\nval_set = windowed_dataset(X_val, window_size, val_batch_size, shuffle=False)\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv1D(filters=60, kernel_size=5,\n                          strides=1, padding='causal',\n                          activation='relu',\n                          input_shape=[None, 1]),\n    tf.keras.layers.LSTM(60, return_sequences=True),\n    tf.keras.layers.LSTM(60, return_sequences=True),\n    tf.keras.layers.Dense(30, activation='relu'),\n    tf.keras.layers.Dense(10, activation='relu'),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x: x * 400)\n])\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=8e-6, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=['mae'])\nhistory = model.fit(train_set, \n                    epochs=500,\n                    validation_data=val_set)","e659c366":"# Training, Validation loss & mae\nplt.figure(figsize=(15, 5))\nplt.subplot(121)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend(['Training', 'Validation'])\nplt.title('Training & Validation Loss')\n\nplt.subplot(122)\nplt.plot(history.history['loss'][200:])\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend(['Training', 'Validation'])\nplt.title('Zoomed Training Loss')","cad67ed8":"forecast = model_forecast(model, series[:, np.newaxis], window_size, batch_size=32)\nforecast = forecast[split_time-window_size : -1, -1, 0]","1b3db588":"plt.figure(figsize=(10, 6))\nplot_series(time_val, X_val)\nplot_series(time_val, forecast)","7e21d966":"tf.keras.metrics.mean_absolute_error(X_val, forecast).numpy()","4225fd64":"# Hyperparameters\nwindow_size = 60\ntrain_batch_size = 100\nval_batch_size = 32","775da536":"tf.keras.backend.clear_session()\ntrain_set = windowed_dataset(X_train, window_size, train_batch_size, shuffle_buffer=len(X_train))\nval_set = windowed_dataset(X_val, window_size, val_batch_size, shuffle=False)\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv1D(filters=60, kernel_size=5,\n                          strides=1, padding='causal',\n                          activation='relu',\n                          input_shape=[None, 1]),\n    tf.keras.layers.LSTM(60, return_sequences=True),\n    tf.keras.layers.LSTM(60, return_sequences=True),\n    tf.keras.layers.Dense(30, activation='relu'),\n    tf.keras.layers.Dense(10, activation='relu'),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x: x * 400)\n])\n\n\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n                                                 factor=0.5, \n                                                 patience=5, \n                                                 min_lr=1e-7)\nearlystop = tf.keras.callbacks.EarlyStopping(monitor='val_mae', mode='min', patience=15)\noptimizer = tf.keras.optimizers.Adam()\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=['mae'])\nhistory = model.fit(train_set, \n                    epochs=500,\n                    validation_data=val_set, \n                    callbacks=[reduce_lr, earlystop])","7340b970":"# Training, Validation loss & mae\nplt.figure(figsize=(8, 5))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend(['Training', 'Validation'])\nplt.title('Training & Validation Loss')","ede8a324":"forecast = model_forecast(model, series[:, np.newaxis], window_size, batch_size=32)\nforecast = forecast[split_time-window_size : -1, -1, 0]","836f5bc5":"plt.figure(figsize=(10, 6))\nplot_series(time_val, X_val)\nplot_series(time_val, forecast)","a73abac5":"tf.keras.metrics.mean_absolute_error(X_val, forecast).numpy()","58af8778":"### Visualizing data","d8df2826":"#### Predict","ab5a615e":"#### Model with SGD optimizer","c9d604f3":"#### Model with Adam optimizer","d58eb60d":"### Models, Train, Predict","9b61b4bf":"### Load data","16a1d22a":"Till lr = 8e-6, loss seems stable. So let's try with <b>lr = 8e-6<\/b>.","8ab77b67":"Data has a bit of <b>seasonality<\/b>. Each season is approximately 132 time steps (132 months = 11 years) long.","56b43c4a":"#### Predict","bb3889a8":"### Preparing data"}}