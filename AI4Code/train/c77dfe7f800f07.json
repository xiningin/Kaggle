{"cell_type":{"7f19c919":"code","044be43c":"code","ce88daeb":"code","8574a800":"code","a6ee11a9":"code","5958b55e":"code","60feca90":"code","da1e86db":"code","4ce8f5c0":"code","c3eeeb34":"code","f44793e8":"code","9474edcd":"code","3c0fafe2":"code","43c29cda":"code","c473d138":"code","129c1040":"code","8ea4014b":"code","d1a4758b":"code","550bdbb9":"code","0411d29c":"code","d67cdd7d":"code","1b65c150":"code","7056eced":"code","6e1d7ec7":"code","78a157e2":"code","72ac3332":"code","0ea90dcf":"markdown"},"source":{"7f19c919":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport keras \nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom keras.layers import LSTM, Dense, Dropout\nfrom keras.callbacks import EarlyStopping \nfrom keras.wrappers.scikit_learn import KerasRegressor","044be43c":"#read in data\ndf = pd.read_csv('..\/input\/hourly-nm\/hourly_nm.csv',index_col='Date\/Time')\ndf.head()","ce88daeb":"print(df.info())\nprint()\nprint(df.describe())","8574a800":"#split data into training and testing, testing data will be one month\nstart_test = '2018-11-31'\n\ntrain, test = df.loc[:start_test], df.loc[start_test:]","a6ee11a9":"train.tail(1)","5958b55e":"test.head(1)","60feca90":"print(len(train))\nprint(len(test))","da1e86db":"# scale the data using MinMax Scaler from -1 to 1 as LSTM has a default tanh activation function\nSCALER = MinMaxScaler(feature_range=(-1,1))\n\nscaler = SCALER.fit(train.to_numpy())\n\ntrain_scaled = scaler.transform(train.to_numpy())\ntest_scaled = scaler.transform(test.to_numpy())","4ce8f5c0":"# create a function to split the datasets into two week windows\ntimestep = 24*7*2 # 24hours,7days,2weeks\n\ndef create_dataset(dataset, timestep=timestep):\n    \"\"\"\n    Function which creates two week chunks of x_train data, and a single\n    value for y_train.\n    \"\"\"\n    X, y = [], []\n    for i in range(len(dataset)):\n        target_value = i + timestep\n        if target_value == len(dataset):\n            break\n        feature_chunk, target = dataset[i:target_value, 1:], dataset[target_value, 0]\n        X.append(feature_chunk)\n        y.append(target)\n    \n    return np.array(X), np.array(y) ","c3eeeb34":"#create x_train, y_train, X_test,y_test\nX_train, y_train = create_dataset(train_scaled)\nX_test, y_test = create_dataset(test_scaled)","f44793e8":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","9474edcd":"# use sample of th data to train network to have a rough understanding of hyperparameters\nsamp_len = int(len(X_train)*0.5)\n\nX_sample_train, y_sample_train = X_train[:samp_len], y_train[:samp_len]","3c0fafe2":"print(X_sample_train.shape)\nprint(y_sample_train.shape)","43c29cda":"# create X_train, y_train, X_test, y_test datasets\n# create a function to build a stacked LSTM model\n# input needs to be [samples, timesteps, features]\ndef create_model(X_train, y_train):\n    units = 32\n    dropout = 0.05\n    epochs = 35\n    batch_size = 14\n    optimizer = keras.optimizers.Adam(learning_rate=0.0005)\n    early_stopping = EarlyStopping(patience=7, monitor='loss')\n\n    model = keras.Sequential()\n\n    model.add(LSTM(units=units, dropout=dropout, return_sequences=True,\n                   input_shape=(X_train.shape[1], X_train.shape[2])))\n\n    model.add(LSTM(units=units, dropout=dropout))\n\n    model.add(Dense(units=1))\n\n    model.compile(optimizer=optimizer, loss='mean_squared_error')\n    history = model.fit(X_train, y_train, validation_split=0.3, shuffle=False,\n              epochs=epochs, batch_size=batch_size, verbose=1, callbacks=[early_stopping])\n\n    return model, history","c473d138":"# function to predict a single value \ndef single_prediction(model, history, timestep=timestep):\n        \n        history = np.array(history)\n        history = history.reshape(history.shape[0]*history.shape[1], history.shape[2])\n        \n        input_value = history[-timestep:]\n        input_value = input_value.reshape(1, input_value.shape[0], input_value.shape[1])\n        \n        yhat = model.predict(input_value, verbose=0)\n        return yhat","129c1040":"# function which takes first test chunk, makes a prediction, add the test chunk back into training data \n#to make next prediction\n\ndef walk_forward_prediction(X_train, y_train, X_test, timestep):\n    \n    MODEL, history = create_model(X_train=X_train, y_train=y_train)\n    hist_train = [i for i in X_train]\n    predictions = []\n    \n    for i in range(len(X_test)):\n        test = X_test[i]\n        yhat = single_prediction(model=MODEL, history=hist_train, timestep=timestep)\n        predictions.append(yhat) \n        hist_train.append(test)\n    \n    return predictions, history, MODEL","8ea4014b":"def prior_inverse(features, targets):\n    '''\n    Append prediction value to test dataset and return a test shape format.\n    '''\n    dataset = []\n    \n    for i in range(features.shape[0]):\n        last_row, target = features[i][0], targets[i]\n        appended = np.append(last_row, target)\n        dataset.append(appended)\n    \n    return np.array(dataset) ","d1a4758b":"#run experiemnt returning the real, predicted values\ndef experiment(X_train, y_train, X_test, timestep):\n    \n    pred_seq, history, MODEL = walk_forward_prediction(X_train, y_train, X_test, timestep)\n    \n    pred_seq = np.array(pred_seq).reshape(-1)\n\n    pred = prior_inverse(X_test, pred_seq)\n    real = prior_inverse(X_test, y_test)\n\n    inv_pred = scaler.inverse_transform(pred)\n    inv_real = scaler.inverse_transform(real)\n\n    power_pred = inv_pred[:,-1]\n    power_real = inv_real[:,-1]\n    \n    return power_real, power_pred, history, MODEL","550bdbb9":"power_real, power_pred, history, MODEL = experiment(X_train, y_train, X_test, timestep)\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']","0411d29c":"#plot validation and training convergence graph\nplt.figure(figsize=(10,5))\nplt.plot(loss, label='train')\nplt.plot(val_loss, label='validation')\nplt.legend()\nplt.xlabel('epochs')\nplt.ylabel('MSE')\nplt.title('LSTM Training Validation Loss')\nplt.tight_layout()\nplt.show()","d67cdd7d":"x_plot = test[timestep:].index\npred_df = pd.DataFrame({'Date':x_plot, 'Prediction': power_pred, 'True': power_real})\npred_df.set_index('Date', inplace=True)","1b65c150":"pred_df2 = pred_df['2018-12-15 01:00:00\t':'2018-12-29 02:00:00 ']","7056eced":"#plot predictions\npred_df2.plot(rot='60',figsize=(10,5))\nplt.title('Predicted Power vs Actual Power with LSTM an Model.')\nplt.ylabel('Power(KWh)')\nplt.tight_layout()\nplt.show()","6e1d7ec7":"#compute metrics\nrmse = np.sqrt(mean_squared_error(pred_df2['True'], pred_df2['Prediction']))\nmae = mean_absolute_error(pred_df2['True'], pred_df2['Prediction'])\nr2 = r2_score(pred_df2['True'], pred_df2['Prediction'])\nprint('RMSE: {}\\nMAE: {}\\nR2: {}'.format(round(rmse,2),round(mae,2), round(r2,2)))","78a157e2":"results = {'Test':[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22],\n           'sample_size':[0.25,0.25,0.25,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,1,1,1,1,1,1,1,1,1],\n           'units':[8,8,8,8,8,12,16,16,16,32,32,32,32,64,64,32,32,32,32,32,32,32],\n           'layers':[2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2],\n           'drop_out':[0.2,0.2,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05],\n           'batch_size':[14,14,14,14,14,14,14,14,14,14,14,7,21,21,14,14,14,21,14,14,14,14],\n           'learning_rate':[0.0001,0.0005,0.0005,0.0005,0.001,0.001,0.001,0.001,0.0005,0.0005,0.001,0.0005,0.0005,0.0005,0.0005,0.0005,0.001,0.0005,0.0005,0.0005,0.0005,0.0005],\n           'epochs':[50,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,40,40,40,40,30,40],\n           'RMSE':[689.87,603.55,577.52,525.36,507.46,484.93,478.51,477.91,489.13,476.17,478.19,487.14,479.56,535.83,497.55,487.31,498.46,535.63,496.71,493.76,496.2,496.35],\n           'MAE':[544.42,509.95,476.48,381.44,370.11,342.71,343.28,348.48,332.21,334.14,344.09,349.62,336.19,394.55,343.45,320.26,328.72,389.17,337.31,334.41,338.35,342.69],\n           'R2':['-','-','-','-','-','-','-','-',0.86,0.87,0.87,0.86,0.87,0.83,0.86,0.86,0.86,0.83,0.86,0.86,0.86,0.86]}","72ac3332":"results_df = pd.DataFrame(data=results)\nresults_df","0ea90dcf":"I decided to manually change the hyperparameters and record the results of the network to see the impact on the training and validation loss as well as making small adjustments to see the overall effect of the parameter. "}}