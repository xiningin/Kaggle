{"cell_type":{"0cc360e2":"code","ad0e5e9d":"code","38aed52b":"code","30f2a7dd":"code","01a982b7":"code","51916545":"code","e8ea9005":"code","0e5aff2f":"code","4049e388":"code","fe4f0cdb":"code","8c3073f0":"code","5afcf273":"code","d5c393f1":"code","defe282e":"code","08b833a4":"code","8adf6c84":"code","f07a1844":"code","aab86898":"code","1ccb4668":"code","a7e7f46a":"code","95cc2bf3":"code","a0125031":"code","c4303fc8":"code","4061e7b7":"code","df520db9":"code","03779688":"code","989a5f7a":"code","58cdda8b":"code","bc38b713":"code","4dd85cde":"code","c3de3f03":"code","b474f0ea":"code","7d605488":"code","98e78864":"code","af1da1e7":"code","b66fde81":"code","fc929aa2":"code","011fe426":"code","8ed0ba45":"code","33706aa2":"markdown","0027df59":"markdown","3e0273fb":"markdown","bd1207eb":"markdown","52b4b74c":"markdown","54e4970f":"markdown","bfda9ede":"markdown","0c90a78d":"markdown","a587459b":"markdown","99483817":"markdown","2eb9d214":"markdown","b49cabf6":"markdown","08a00b9a":"markdown","d58e0bac":"markdown","6cb26996":"markdown","00b47170":"markdown","8b70c747":"markdown","232cefbb":"markdown","9bd1be88":"markdown","c5ba3dec":"markdown","bf813a28":"markdown","8702393d":"markdown","42f351b1":"markdown","ca5a51d6":"markdown","bb97712b":"markdown","679ffbdd":"markdown","0f8255e3":"markdown","3ea6a6e5":"markdown","b471532a":"markdown","3500fe60":"markdown","efd598d3":"markdown","7cb6b1f5":"markdown","25797e01":"markdown","e8f7ac6a":"markdown","62581103":"markdown","be1b613b":"markdown","66cbbe76":"markdown","5229dc40":"markdown","f7730c40":"markdown","52fb1e2e":"markdown","d626a947":"markdown","c5c62f01":"markdown"},"source":{"0cc360e2":"import pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport scipy.stats as st\nfrom sklearn import ensemble, tree, linear_model\nimport missingno as msno","ad0e5e9d":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","38aed52b":"train.describe()","30f2a7dd":"train.head()","01a982b7":"train.tail()","51916545":"train.shape , test.shape","e8ea9005":"numeric_features = train.select_dtypes(include=[np.number])\n\nnumeric_features.columns","0e5aff2f":"categorical_features = train.select_dtypes(include=[np.object])\n\ncategorical_features.columns","4049e388":"msno.matrix(train.sample(250))","fe4f0cdb":"msno.heatmap(train)","8c3073f0":"msno.bar(train.sample(1000))","5afcf273":"msno.dendrogram(train)","d5c393f1":"train.skew(), train.kurt()","defe282e":"y = train['SalePrice']\nplt.figure(1); plt.title('Johnson SU')\nsns.distplot(y, kde=False, fit=st.johnsonsu)\nplt.figure(2); plt.title('Normal')\nsns.distplot(y, kde=False, fit=st.norm)\nplt.figure(3); plt.title('Log Normal')\nsns.distplot(y, kde=False, fit=st.lognorm)","08b833a4":"sns.distplot(train.skew(),color='blue',axlabel ='Skewness')","8adf6c84":"plt.figure(figsize = (12,8))\nsns.distplot(train.kurt(),color='r',axlabel ='Kurtosis',norm_hist= False, kde = True,rug = False)\n#plt.hist(train.kurt(),orientation = 'vertical',histtype = 'bar',label ='Kurtosis', color ='blue')\nplt.show()","f07a1844":"plt.hist(train['SalePrice'],orientation = 'vertical',histtype = 'bar', color ='blue')\nplt.show()","aab86898":"target = np.log(train['SalePrice'])\ntarget.skew()\nplt.hist(target,color='blue')","1ccb4668":"correlation = numeric_features.corr()\nprint(correlation['SalePrice'].sort_values(ascending = False),'\\n')","a7e7f46a":"f , ax = plt.subplots(figsize = (14,12))\n\nplt.title('Correlation of Numeric Features with Sale Price',y=1,size=16)\n\nsns.heatmap(correlation,square = True,  vmax=0.8)","95cc2bf3":"k= 11\ncols = correlation.nlargest(k,'SalePrice')['SalePrice'].index\nprint(cols)\ncm = np.corrcoef(train[cols].values.T)\nf , ax = plt.subplots(figsize = (14,12))\nsns.heatmap(cm, vmax=.8, linewidths=0.01,square=True,annot=True,cmap='viridis',\n            linecolor=\"white\",xticklabels = cols.values ,annot_kws = {'size':12},yticklabels = cols.values)","a0125031":"sns.set()\ncolumns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\nsns.pairplot(train[columns],size = 2 ,kind ='scatter',diag_kind='kde')\nplt.show()","c4303fc8":"fig, ((ax1, ax2), (ax3, ax4),(ax5,ax6)) = plt.subplots(nrows=3, ncols=2, figsize=(14,10))\nOverallQual_scatter_plot = pd.concat([train['SalePrice'],train['OverallQual']],axis = 1)\nsns.regplot(x='OverallQual',y = 'SalePrice',data = OverallQual_scatter_plot,scatter= True, fit_reg=True, ax=ax1)\nTotalBsmtSF_scatter_plot = pd.concat([train['SalePrice'],train['TotalBsmtSF']],axis = 1)\nsns.regplot(x='TotalBsmtSF',y = 'SalePrice',data = TotalBsmtSF_scatter_plot,scatter= True, fit_reg=True, ax=ax2)\nGrLivArea_scatter_plot = pd.concat([train['SalePrice'],train['GrLivArea']],axis = 1)\nsns.regplot(x='GrLivArea',y = 'SalePrice',data = GrLivArea_scatter_plot,scatter= True, fit_reg=True, ax=ax3)\nGarageArea_scatter_plot = pd.concat([train['SalePrice'],train['GarageArea']],axis = 1)\nsns.regplot(x='GarageArea',y = 'SalePrice',data = GarageArea_scatter_plot,scatter= True, fit_reg=True, ax=ax4)\nFullBath_scatter_plot = pd.concat([train['SalePrice'],train['FullBath']],axis = 1)\nsns.regplot(x='FullBath',y = 'SalePrice',data = FullBath_scatter_plot,scatter= True, fit_reg=True, ax=ax5)\nYearBuilt_scatter_plot = pd.concat([train['SalePrice'],train['YearBuilt']],axis = 1)\nsns.regplot(x='YearBuilt',y = 'SalePrice',data = YearBuilt_scatter_plot,scatter= True, fit_reg=True, ax=ax6)\nYearRemodAdd_scatter_plot = pd.concat([train['SalePrice'],train['YearRemodAdd']],axis = 1)\nYearRemodAdd_scatter_plot.plot.scatter('YearRemodAdd','SalePrice')","4061e7b7":"saleprice_overall_quality= train.pivot_table(index ='OverallQual',values = 'SalePrice', aggfunc = np.median)\nsaleprice_overall_quality.plot(kind = 'bar',color = 'blue')\nplt.xlabel('Overall Quality')\nplt.ylabel('Median Sale Price')\nplt.show()","df520db9":"var = 'OverallQual'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(12, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","03779688":"var = 'Neighborhood'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 10))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nxt = plt.xticks(rotation=45)","989a5f7a":"plt.figure(figsize = (12, 6))\nsns.countplot(x = 'Neighborhood', data = data)\nxt = plt.xticks(rotation=45)","58cdda8b":"for c in categorical_features:\n    train[c] = train[c].astype('category')\n    if train[c].isnull().any():\n        train[c] = train[c].cat.add_categories(['MISSING'])\n        train[c] = train[c].fillna('MISSING')\n\ndef boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\nf = pd.melt(train, id_vars=['SalePrice'], value_vars=categorical_features)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, size=5)\ng = g.map(boxplot, \"value\", \"SalePrice\")","bc38b713":"var = 'SaleType'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 10))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nxt = plt.xticks(rotation=45)","4dd85cde":"var = 'SaleCondition'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 10))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nxt = plt.xticks(rotation=45)","c3de3f03":"sns.violinplot('Functional', 'SalePrice', data = train)","b474f0ea":"sns.factorplot('FireplaceQu', 'SalePrice', data = train, color = 'm', \\\n               estimator = np.median, order = ['Ex', 'Gd', 'TA', 'Fa', 'Po'], size = 4.5,  aspect=1.35)","7d605488":"g = sns.FacetGrid(train, col = 'FireplaceQu', col_wrap = 3, col_order=['Ex', 'Gd', 'TA', 'Fa', 'Po'])\ng.map(sns.boxplot, 'Fireplaces', 'SalePrice', order = [1, 2, 3], palette = 'Set2')","98e78864":"plt.figure(figsize=(8,10))\ng1 = sns.pointplot(x='Neighborhood', y='SalePrice', \n                   data=train, hue='LotShape')\ng1.set_xticklabels(g1.get_xticklabels(),rotation=90)\ng1.set_title(\"Lotshape Based on Neighborhood\", fontsize=15)\ng1.set_xlabel(\"Neighborhood\")\ng1.set_ylabel(\"Sale Price\", fontsize=12)\nplt.show()","af1da1e7":"total = numeric_features.isnull().sum().sort_values(ascending=False)\npercent = (numeric_features.isnull().sum()\/numeric_features.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1,join='outer', keys=['Total Missing Count', '% of Total Observations'])\nmissing_data.index.name =' Numeric Feature'\n\nmissing_data.head(20)","b66fde81":"missing_values = numeric_features.isnull().sum(axis=0).reset_index()\nmissing_values.columns = ['column_name', 'missing_count']\nmissing_values = missing_values.loc[missing_values['missing_count']>0]\nmissing_values = missing_values.sort_values(by='missing_count')\n\nind = np.arange(missing_values.shape[0])\nwidth = 0.1\nfig, ax = plt.subplots(figsize=(12,3))\nrects = ax.barh(ind, missing_values.missing_count.values, color='b')\nax.set_yticks(ind)\nax.set_yticklabels(missing_values.column_name.values, rotation='horizontal')\nax.set_xlabel(\"Missing Observations Count\")\nax.set_title(\"Missing Observations Count - Numeric Features\")\nplt.show()\n","fc929aa2":"total = categorical_features.isnull().sum().sort_values(ascending=False)\npercent = (categorical_features.isnull().sum()\/categorical_features.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1,join='outer', keys=['Total Missing Count', ' % of Total Observations'])\nmissing_data.index.name ='Feature'\nmissing_data.head(20)","011fe426":"missing_values = categorical_features.isnull().sum(axis=0).reset_index()\nmissing_values.columns = ['column_name', 'missing_count']\nmissing_values = missing_values.loc[missing_values['missing_count']>0]\nmissing_values = missing_values.sort_values(by='missing_count')\n\nind = np.arange(missing_values.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,18))\nrects = ax.barh(ind, missing_values.missing_count.values, color='red')\nax.set_yticks(ind)\nax.set_yticklabels(missing_values.column_name.values, rotation='horizontal')\nax.set_xlabel(\"Missing Observations Count\")\nax.set_title(\"Missing Observations Count - Categorical Features\")\nplt.show()","8ed0ba45":"for column_name in train.columns:\n    if train[column_name].dtypes == 'object':\n        train[column_name] = train[column_name].fillna(train[column_name].mode().iloc[0])\n        unique_category = len(train[column_name].unique())\n        print(\"Feature '{column_name}' has '{unique_category}' unique categories\".format(column_name = column_name,\n                                                                                         unique_category=unique_category))\n \nfor column_name in test.columns:\n    if test[column_name].dtypes == 'object':\n        test[column_name] = test[column_name].fillna(test[column_name].mode().iloc[0])\n        unique_category = len(test[column_name].unique())\n        print(\"Features in test set '{column_name}' has '{unique_category}' unique categories\".format(column_name = column_name, unique_category=unique_category))","33706aa2":"Now that you have got a general idea about your data set, it\u2019s also a good idea to take a closer look at the data itself. With the help of the head() and tail() functions of the Pandas library, you can easily check out the first and last lines of your DataFrame, respectively.\n\nLet us look at some sample data","0027df59":"Based on the above observation can group those Neighborhoods with similar housing price into a same bucket for dimension-reduction.Let us see this in the preprocessing stage","3e0273fb":"#### FactorPlot - FirePlaceQC vs. SalePrice ","bd1207eb":"#### Dendrogram\n\nThe dendrogram allows you to more fully correlate variable completion, revealing trends deeper than the pairwise ones visible in the correlation heatmap:","52b4b74c":"#### Count Plot - Neighborhood","54e4970f":"It is apparent that SalePrice doesn't follow normal distribution, so before performing regression it has to be transformed. While log transformation does pretty good job, best fit is unbounded Johnson distribution.","bfda9ede":"#### Housing Price vs Sales\n\n- Sale Type & Condition\n- Sales Seasonality","0c90a78d":"Finding Correlation coefficients between numeric features and SalePrice","a587459b":"### Pair Plot ","99483817":"#### Box plot - Neighborhood","2eb9d214":"#### Categorical Features","b49cabf6":"#### SalePrice Correlation matrix","08a00b9a":"#### Scatter plots between the most correlated variables","d58e0bac":"The heatmap is the best way to get a quick overview of correlated features thanks to seaborn!\n\nAt initial glance it is observed that there are two red colored squares that get my attention. \n1. The first one refers to the 'TotalBsmtSF' and '1stFlrSF' variables.\n2. Second one refers to the 'GarageX' variables. \nBoth cases show how significant the correlation is between these variables. Actually, this correlation is so strong that it can indicate a situation of multicollinearity. If we think about these variables, we can conclude that they give almost the same information so multicollinearity really occurs. \n\nHeatmaps are great to detect this kind of multicollinearity situations and in problems related to feature selection like this project, it comes as an excellent exploratory tool.\n\nAnother aspect I observed here is the 'SalePrice' correlations.As it is observed that 'GrLivArea', 'TotalBsmtSF', and 'OverallQual' saying a big 'Hello !' to SalePrice, however we cannot exclude the fact that rest of the features have some level of correlation to the SalePrice. To observe this correlation closer let us see it in Zoomed Heat Map ","6cb26996":" ### Missing Value Analysis \n \n #### Numeric Features","00b47170":"#### Facet Grid Plot - FirePlace QC vs.SalePrice","8b70c747":"### Correlation Heat Map","232cefbb":"### Categorical Feature Exploration","9bd1be88":"**The Challenges of Your Data**\n\nNow that we have gathered some basic information on your data, it\u2019s a good idea to just go a little bit deeper into the challenges that the data might pose.\n\nThere are two factors mostly observed in EDA exercise which are **missing values** and **outliers**\nFor understanding in detail on how to handle missing values in detail please visit \nhttps:\/\/www.kaggle.com\/pavansanagapati\/simple-tutorial-on-how-to-handle-missing-data\nFor determining the outliers boxplot is used in the later part of this kernel\n\n**Estimate Skewness and Kurtosis**","c5ba3dec":"From above zoomed heatmap it is observed that GarageCars & GarageArea are closely correlated .\nSimilarly TotalBsmtSF and 1stFlrSF are also closely correlated.\n","bf813a28":"Visualisation of 'OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd' features \nwith respect to SalePrice in the form of pair plot & scatter pair plot for better understanding.","8702393d":"## Import Libraries","42f351b1":"#### Missing values for all numeric features in Bar chart Representation","ca5a51d6":"#### Pair Plot between 'SalePrice' and correlated variables ","bb97712b":"#### ViolinPlot - Functional vs.SalePrice","679ffbdd":"My observations :\n- 'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'.\n- 'GarageCars' and 'GarageArea' are strongly correlated variables. It is because the number of cars that fit into the garage is a consequence of the garage area. 'GarageCars' and 'GarageArea' are like twin brothers. So it is hard to distinguish between the two. Therefore, we just need one of these variables in our analysis (we can keep 'GarageCars' since its correlation with 'SalePrice' is higher).\n- 'TotalBsmtSF' and '1stFloor' also seem to be twins. In this case let us keep 'TotalBsmtSF'\n- 'TotRmsAbvGrd' and 'GrLivArea', twins\n- 'YearBuilt' it appears like is slightly correlated with 'SalePrice'. This required more analysis to arrive at a conclusion may be do some time series analysis.","0f8255e3":"#### PointPlot","3ea6a6e5":"Although we already know some of the main figures, this pair plot gives us a reasonable overview insight about the correlated features .Here are some of my analysis.\n\n- One interesting observation is between 'TotalBsmtSF' and 'GrLiveArea'. In this figure we can see the dots drawing a linear line, which almost acts like a border. It totally makes sense that the majority of the dots stay below that line. Basement areas can be equal to the above ground living area, but it is not expected a basement area bigger than the above ground living area.\n\n- One more interesting observation is between 'SalePrice' and 'YearBuilt'. In the bottom of the 'dots cloud', we see what almost appears to be a exponential function.We can also see this same tendency in the upper limit of the 'dots cloud' \n- Last observation is that prices are increasing faster now with respect to previous years.","b471532a":"One of the most elementary steps to do this is by getting a basic description of your data. A basic description of your data is indeed a very broad term: you can interpret it as a quick and dirty way to get some information on your data, as a way of getting some simple, easy-to-understand information on your data, to get a basic feel for your data. We can use the describe() function to get various summary statistics that exclude NaN values. ","3500fe60":"To explore further we will start with the following visualisation methods to analyze the data better:\n\n - Correlation Heat Map\n - Zoomed Heat Map\n - Pair Plot \n - Scatter Plot","efd598d3":"#### Heatmap\n\nThe **missingno** correlation heatmap measures nullity correlation: how strongly the presence or absence of one variable affects the presence of another:","7cb6b1f5":"Let us examine categorical features in the train dataset","25797e01":"### Scatter Plot ","e8f7ac6a":"Let us examine numerical features in the train dataset","62581103":"The dendrogram uses a hierarchical clustering algorithm (courtesy of scipy) to bin variables against one another by their nullity correlation (measured in terms of binary distance). At each step of the tree the variables are split up based on which combination minimizes the distance of the remaining clusters. The more monotone the set of variables, the closer their total distance is to zero, and the closer their average distance (the y-axis) is to zero.\n\nTo interpret this graph, read it from a top-down perspective. Cluster leaves which linked together at a distance of zero fully predict one another's presence\u2014one variable might always be empty when another is filled, or they might always both be filled or both empty, and so on. In this specific example the dendrogram glues together the variables which are required and therefore present in every record.\n\nCluster leaves which split close to zero, but not at it, predict one another very well, but still imperfectly. If your own interpretation of the dataset is that these columns actually are or ought to be match each other in nullity , then the height of the cluster leaf tells you, in absolute terms, how often the records are \"mismatched\" or incorrectly filed\u2014that is, how many values you would have to fill in or drop, if you are so inclined.\n\nAs with matrix, only up to 50 labeled columns will comfortably display in this configuration. However the dendrogram more elegantly handles extremely large datasets by simply flipping to a horizontal configuration.","be1b613b":"## If you like this  kernel Greatly Appreciate if you can  UPVOTE .Thank you\n\n# A Simple Tutorial on Exploratory Data Analysis\n\n### What is Exploratory Data Analysis (EDA)?\n- How to ensure you are ready to use machine learning algorithms in a project? \n- How to choose the most suitable algorithms for your data set?\n- How to define the feature variables that can potentially be used for machine learning?\n\n**Exploratory Data Analysis (EDA)** helps to answer all these questions, ensuring the best outcomes for the project. It is an approach for summarizing, visualizing, and becoming intimately familiar with the important characteristics of a data set.\n\n### Value of Exploratory Data Analysis\nExploratory Data Analysis is valuable to data science projects since it allows to get closer to the certainty that the future results will be valid, correctly interpreted, and applicable to the desired business contexts. Such level of certainty can be achieved only after raw data is validated and checked for anomalies, ensuring that the data set was collected without errors. EDA also helps to find insights that were not evident or worth investigating to business stakeholders and data scientists but can be very informative about a particular business.\n\nEDA is performed in order to define and refine the selection of feature variables that will be used for machine learning. Once data scientists become familiar with the data set, they often have to return to feature engineering step, since the initial features may turn out not to be serving their intended purpose. Once the EDA stage is complete, data scientists get a firm feature set they need for supervised and unsupervised machine learning.\n\n### Methods of Exploratory Data Analysis\nIt is always better to explore each data set using multiple exploratory techniques and compare the results. Once the data set is fully understood, it is quite possible that data scientist will have to go back to data collection and cleansing phases in order to transform the data set according to the desired business outcomes. The goal of this step is to become confident that the data set is ready to be used in a machine learning algorithm.\n\nExploratory Data Analysis is majorly performed using the following methods:\n\n- Univariate visualization\u200a\u2014\u200aprovides summary statistics for each field in the raw data set\n- Bivariate visualization\u200a\u2014\u200ais performed to find the relationship between each variable in the dataset and the target variable of interest\n- Multivariate visualization\u200a\u2014\u200ais performed to understand interactions between different fields in the dataset\n- Dimensionality reduction\u200a\u2014\u200ahelps to understand the fields in the data that account for the most variance between observations and allow for the processing of a reduced volume of data.\nThrough these methods, the data scientist validates assumptions and identifies patterns that will allow for the understanding of the problem and model selection and validates that the data has been generated in the way it was expected to. So, value distribution of each field is checked, a number of missing values is defined, and the possible ways of replacing them are found.\n\nAdditional benefits Exploratory Data Analysis brings to projects\nAnother side benefit of EDA is that it allows to specify or even define the questions you are trying to get the answer to from your data. Companies, that are only starting to leverage Data Science and AI technologies, often face the situation when they realize, that they have a lot of data and no ideas of what value that data can bring to their business decision making.\n\nHowever, the questions always come first in data analysis. It doesn\u2019t matter how much data company has, how many tools they have available, whether the data is historical or real time unless business stakeholders have the questions they are trying to solve with their data. EDA can help such companies to start formalizing the right questions, since with wrong questions you get the wrong answers, and take the wrong decisions.\n\n#### Why skipping Exploratory Data Analysis is a bad idea?\n\nIn a hurry to get to the machine learning stage or simply impress business stakeholders very fast, data scientists tend to either entirely skip the exploratory process or do a very shallow work. It is a very serious and, sadly, common mistake of amateur data science consulting \u201cprofessionals\u201d.\n\nSuch inconsiderate behavior can lead to skewed data, with outliers and too many missing values and, therefore, some sad outcomes for the project:\n\n- generating inaccurate models;\n- generating accurate models on the wrong data;\n- choosing the wrong variables for the model;\n- inefficient use of the resources, including the rebuilding of the model.\n\nExploratory Data Analysis (EDA) is used on the one hand to answer questions, test business assumptions, generate hypotheses for further analysis. On the other hand, you can also use it to prepare the data for modeling. \n\nThe thing that these two probably have in common is a good knowledge of your data to either get the answers that you need or to develop an intuition for interpreting the results of future modeling.\n\nThere are a lot of ways to reach these goals as follows:\n\n1. Import the data\n\n2. Get a feel of the data ,describe the data,look at a sample of data like first and last rows\n\n3. Take a deeper look into the data by querying or indexing the data\n\n4. Identify features of interest\n\n5. Recognise the challenges posed by data - missing values, outliers\n\n6. Discover patterns in the data\n\nOne of the important things about EDA is  Data profiling. \n\n**Data profiling** is concerned with summarizing your dataset through descriptive statistics. You want to use a variety of measurements to better understand your dataset. The goal of data profiling is to have a solid understanding of your data so you can afterwards start querying and visualizing your data in various ways. However, this doesn\u2019t mean that you don\u2019t have to iterate: exactly because data profiling is concerned with summarizing your dataset, it is frequently used to assess the data quality. Depending on the result of the data profiling, you might decide to correct, discard or handle your data differently.\n\n\n### Key Concepts of Exploratory Data Analysis\n\n- <b>2 types of Data Analysis<\/b>\n   - Confirmatory Data Analysis\n   \n   - Exploratory Data Analysis\n\n- <b>4 Objectives of EDA<\/b>\n   - Discover Patterns\n   \n   - Spot Anomalies\n   \n   - Frame Hypothesis\n   \n   - Check Assumptions\n\n- <b>2 methods for exploration<\/b>\n   - Univariate Analysis\n   \n   - Bivariate Analysis\n\n- <b>Stuff done during EDA<\/b>\n   - Trends\n   \n   - Distribution\n   \n   - Mean\n   \n   - Median\n   \n   - Outlier\n   \n   - Spread measurement (SD)\n   \n   - Correlations\n   \n   - Hypothesis testing\n   \n   - Visual Exploration\n   \n## Overview\n\nThis is an exploratory data analysis on the House Prices Kaggle Competition found at \n\nhttps:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\n\n### Description\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\nThere are 1460 instances of  training data and 1460 of test data. Total number of attributes equals 81, of which 36 are numerical, 43 are categorical + Id and SalePrice.\n\nNumerical Features: 1stFlrSF, 2ndFlrSF, 3SsnPorch, BedroomAbvGr, BsmtFinSF1, BsmtFinSF2, BsmtFullBath, BsmtHalfBath, BsmtUnfSF, EnclosedPorch, Fireplaces, FullBath, GarageArea, GarageCars, GarageYrBlt, GrLivArea, HalfBath, KitchenAbvGr, LotArea, LotFrontage, LowQualFinSF, MSSubClass, MasVnrArea, MiscVal, MoSold, OpenPorchSF, OverallCond, OverallQual, PoolArea, ScreenPorch, TotRmsAbvGrd, TotalBsmtSF, WoodDeckSF, YearBuilt, YearRemodAdd, YrSold\n\n\nCategorical Features: Alley, BldgType, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtQual, CentralAir, Condition1, Condition2, Electrical, ExterCond, ExterQual, Exterior1st, Exterior2nd, Fence, FireplaceQu, Foundation, Functional, GarageCond, GarageFinish, GarageQual, GarageType, Heating, HeatingQC, HouseStyle, KitchenQual, LandContour, LandSlope, LotConfig, LotShape, MSZoning, MasVnrType,  MiscFeature, Neighborhood, PavedDrive, PoolQC, RoofMatl, RoofStyle, SaleCondition, SaleType, Street, Utilitif","66cbbe76":"With qualitative variables we can check distribution of SalePrice with respect to variable values and enumerate them. ","5229dc40":"To start exploring your data, you\u2019ll need to start by actually loading in your data. You\u2019ll probably know this already, but thanks to the Pandas library, this becomes an easy task: you import the package as pd, following the convention, and you use the read_csv() function, to which you pass the URL in which the data can be found and a header argument. This last argument is one that you can use to make sure that your data is read in correctly: the first row of your data won\u2019t be interpreted as the column names of your DataFrame.\n\nAlternatively, there are also other arguments that you can specify to ensure that your data is read in correctly: you can specify the delimiter to use with the sep or delimiter arguments, the column names to use with names or the column to use as the row labels for the resulting DataFrame with index_col.","f7730c40":"#### Missing values for  Categorical features in Bar chart Representation","52fb1e2e":"### Zoomed HeatMap","d626a947":"#### Box plot - OverallQual","c5c62f01":"Visualising missing values for a sample of 250 "}}