{"cell_type":{"76d55ee4":"code","320765bd":"code","95d39785":"code","1f88c1c0":"code","c89b9988":"code","2970a3d0":"code","8e9d20a4":"code","37ec9790":"code","cf1ca7c2":"code","1d3e3f2e":"code","c087d6fc":"code","15f39aba":"code","66b52fee":"code","7a914dcb":"code","83ead7f1":"markdown"},"source":{"76d55ee4":"DATA_PATH = '..\/input\/parmalats-soft-toys'","320765bd":"import tensorflow as keras\nimport tensorflow as tf\nimport numpy as np\nfrom PIL import Image, ImageDraw, ImageFont\nfrom IPython.display import display\nfrom seaborn import color_palette\nimport cv2\nimport struct\nimport numpy as np\nfrom  tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.layers import ZeroPadding2D\nfrom tensorflow.keras.layers import UpSampling2D\nfrom tensorflow.keras.layers import add, concatenate\nfrom tensorflow.keras.models import Model","95d39785":"!wget https:\/\/pjreddie.com\/media\/files\/yolov3.weights","1f88c1c0":"# create a YOLOv3 Keras model and save it to file\n# based on https:\/\/github.com\/experiencor\/keras-yolo3\n\n \ndef _conv_block(inp, convs, skip=True):\n    x = inp\n    count = 0\n    for conv in convs:\n        if count == (len(convs) - 2) and skip:\n            skip_connection = x\n        count += 1\n        if conv['stride'] > 1: x = ZeroPadding2D(((1,0),(1,0)))(x) # peculiar padding as darknet prefer left and top\n        x = Conv2D(conv['filter'],\n                   conv['kernel'],\n                   strides=conv['stride'],\n                   padding='valid' if conv['stride'] > 1 else 'same', # peculiar padding as darknet prefer left and top\n                   name='conv_' + str(conv['layer_idx']),\n                   use_bias=False if conv['bnorm'] else True)(x)\n        if conv['bnorm']: x = BatchNormalization(epsilon=0.001, name='bnorm_' + str(conv['layer_idx']))(x)\n        if conv['leaky']: x = LeakyReLU(alpha=0.1, name='leaky_' + str(conv['layer_idx']))(x)\n    return add([skip_connection, x]) if skip else x\n\ndef make_yolov3_model():\n    input_image = Input(shape=(None, None, 3))\n    # Layer  0 => 4\n    x = _conv_block(input_image, [{'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 0},\n                                  {'filter': 64, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 1},\n                                  {'filter': 32, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 2},\n                                  {'filter': 64, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 3}])\n    # Layer  5 => 8\n    x = _conv_block(x, [{'filter': 128, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 5},\n                        {'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 6},\n                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 7}])\n    # Layer  9 => 11\n    x = _conv_block(x, [{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 9},\n                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 10}])\n    # Layer 12 => 15\n    x = _conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 12},\n                        {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 13},\n                        {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 14}])\n    # Layer 16 => 36\n    for i in range(7):\n        x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 16+i*3},\n                            {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 17+i*3}])\n    skip_36 = x\n    # Layer 37 => 40\n    x = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 37},\n                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 38},\n                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 39}])\n    # Layer 41 => 61\n    for i in range(7):\n        x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 41+i*3},\n                            {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 42+i*3}])\n    skip_61 = x\n    # Layer 62 => 65\n    x = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},\n                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},\n                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])\n    # Layer 66 => 74\n    for i in range(3):\n        x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},\n                            {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])\n    # Layer 75 => 79\n    x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},\n                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},\n                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},\n                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},\n                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], skip=False)\n    # Layer 80 => 82\n    yolo_82 = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 80},\n                              {'filter':  255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 81}], skip=False)\n    # Layer 83 => 86\n    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 84}], skip=False)\n    x = UpSampling2D(2)(x)\n    x = concatenate([x, skip_61])\n    # Layer 87 => 91\n    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 87},\n                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 88},\n                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 89},\n                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 90},\n                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 91}], skip=False)\n    # Layer 92 => 94\n    yolo_94 = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 92},\n                              {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 93}], skip=False)\n    # Layer 95 => 98\n    x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True,   'layer_idx': 96}], skip=False)\n    x = UpSampling2D(2)(x)\n    x = concatenate([x, skip_36])\n    # Layer 99 => 106\n    yolo_106 = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 99},\n                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 100},\n                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 101},\n                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 102},\n                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 103},\n                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 104},\n                               {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 105}], skip=False)\n    model = Model(input_image, [yolo_82, yolo_94, yolo_106])\n    return model\n\nclass WeightReader:\n    def __init__(self, weight_file):\n        with open(weight_file, 'rb') as w_f:\n            major,\t= struct.unpack('i', w_f.read(4))\n            minor,\t= struct.unpack('i', w_f.read(4))\n            revision, = struct.unpack('i', w_f.read(4))\n            if (major*10 + minor) >= 2 and major < 1000 and minor < 1000:\n                w_f.read(8)\n            else:\n                w_f.read(4)\n            transpose = (major > 1000) or (minor > 1000)\n            binary = w_f.read()\n        self.offset = 0\n        self.all_weights = np.frombuffer(binary, dtype='float32')\n \n    def read_bytes(self, size):\n        self.offset = self.offset + size\n        return self.all_weights[self.offset-size:self.offset]\n\n    def load_weights(self, model):\n        for i in range(106):\n            try:\n                conv_layer = model.get_layer('conv_' + str(i))\n                print(\"loading weights of convolution #\" + str(i))\n                if i not in [81, 93, 105]:\n                    norm_layer = model.get_layer('bnorm_' + str(i))\n                    size = np.prod(norm_layer.get_weights()[0].shape)\n                    beta  = self.read_bytes(size) # bias\n                    gamma = self.read_bytes(size) # scale\n                    mean  = self.read_bytes(size) # mean\n                    var   = self.read_bytes(size) # variance\n                    weights = norm_layer.set_weights([gamma, beta, mean, var])\n                if len(conv_layer.get_weights()) > 1:\n                    bias   = self.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n                    kernel = kernel.transpose([2,3,1,0])\n                    conv_layer.set_weights([kernel, bias])\n                else:\n                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n                    kernel = kernel.transpose([2,3,1,0])\n                    conv_layer.set_weights([kernel])\n            except ValueError:\n                print(\"no convolution #\" + str(i))\n\n    def reset(self):\n        self.offset = 0","c89b9988":"from matplotlib import pyplot\nfrom matplotlib.patches import Rectangle\n \nclass BoundBox:\n    def __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n        self.xmin = xmin\n        self.ymin = ymin\n        self.xmax = xmax\n        self.ymax = ymax\n        self.objness = objness\n        self.classes = classes\n        self.label = -1\n        self.score = -1\n\n    def get_label(self):\n        if self.label == -1:\n            self.label = np.argmax(self.classes)\n\n        return self.label\n\n    def get_score(self):\n        if self.score == -1:\n            self.score = self.classes[self.get_label()]\n \n        return self.score\n \ndef _sigmoid(x):\n    return 1. \/ (1. + np.exp(-x))\n \ndef decode_netout(netout, anchors, obj_thresh, net_h, net_w):\n    grid_h, grid_w = netout.shape[:2]\n    nb_box = 3\n    netout = netout.reshape((grid_h, grid_w, nb_box, -1))\n    nb_class = netout.shape[-1] - 5\n    boxes = []\n    netout[..., :2]  = _sigmoid(netout[..., :2])\n    netout[..., 4:]  = _sigmoid(netout[..., 4:])\n    netout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]\n    netout[..., 5:] *= netout[..., 5:] > obj_thresh\n \n    for i in range(grid_h*grid_w):\n        row = i \/ grid_w\n        col = i % grid_w\n        for b in range(nb_box):\n            # 4th element is objectness score\n            objectness = netout[int(row)][int(col)][b][4]\n            if(objectness.all() <= obj_thresh): continue\n            # first 4 elements are x, y, w, and h\n            x, y, w, h = netout[int(row)][int(col)][b][:4]\n            x = (col + x) \/ grid_w # center position, unit: image width\n            y = (row + y) \/ grid_h # center position, unit: image height\n            w = anchors[2 * b + 0] * np.exp(w) \/ net_w # unit: image width\n            h = anchors[2 * b + 1] * np.exp(h) \/ net_h # unit: image height\n            # last elements are class probabilities\n            classes = netout[int(row)][col][b][5:]\n            box = BoundBox(x-w\/2, y-h\/2, x+w\/2, y+h\/2, objectness, classes)\n            boxes.append(box)\n    return boxes\n \ndef correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):\n    new_w, new_h = net_w, net_h\n    for i in range(len(boxes)):\n        x_offset, x_scale = (net_w - new_w)\/2.\/net_w, float(new_w)\/net_w\n        y_offset, y_scale = (net_h - new_h)\/2.\/net_h, float(new_h)\/net_h\n        boxes[i].xmin = int((boxes[i].xmin - x_offset) \/ x_scale * image_w)\n        boxes[i].xmax = int((boxes[i].xmax - x_offset) \/ x_scale * image_w)\n        boxes[i].ymin = int((boxes[i].ymin - y_offset) \/ y_scale * image_h)\n        boxes[i].ymax = int((boxes[i].ymax - y_offset) \/ y_scale * image_h)\n\ndef _interval_overlap(interval_a, interval_b):\n    x1, x2 = interval_a\n    x3, x4 = interval_b\n    if x3 < x1:\n        if x4 < x1:\n            return 0\n        else:\n            return min(x2,x4) - x1\n    else:\n        if x2 < x3:\n            return 0\n        else:\n            return min(x2,x4) - x3\n\ndef bbox_iou(box1, box2):\n    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n    intersect = intersect_w * intersect_h\n    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n    union = w1*h1 + w2*h2 - intersect\n    return float(intersect) \/ union\n \ndef do_nms(boxes, nms_thresh):\n    if len(boxes) > 0:\n        nb_class = len(boxes[0].classes)\n    else:\n        return\n    for c in range(nb_class):\n        sorted_indices = np.argsort([-box.classes[c] for box in boxes])\n        for i in range(len(sorted_indices)):\n            index_i = sorted_indices[i]\n            if boxes[index_i].classes[c] == 0: continue\n            for j in range(i+1, len(sorted_indices)):\n                index_j = sorted_indices[j]\n                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n                    boxes[index_j].classes[c] = 0\n\n# load and prepare an image\ndef load_image_pixels(filename, shape):\n    # load the image to get its shape\n    image = load_img(filename)\n    width, height = image.size\n    # load the image with the required size\n    image = load_img(filename, target_size=shape)\n    # convert to numpy array\n    image = img_to_array(image)\n    # scale pixel values to [0, 1]\n    image = image.astype('float32')\n    image \/= 255.0\n    # add a dimension so that we have one sample\n    image = expand_dims(image, 0)\n    return image, width, height\n \n# get all of the results above a threshold\ndef get_boxes(boxes, labels, thresh):\n    v_boxes, v_labels, v_scores = list(), list(), list()\n    # enumerate all boxes\n    for box in boxes:\n        # enumerate all possible labels\n        for i in range(len(labels)):\n            # check if the threshold for this label is high enough\n            if box.classes[i] > thresh:\n                v_boxes.append(box)\n                v_labels.append(labels[i])\n                v_scores.append(box.classes[i]*100)\n                # don't break, many labels may trigger for one box\n    return v_boxes, v_labels, v_scores\n \n# draw all results\ndef draw_boxes(filename, v_boxes, v_labels, v_scores):\n    # load the image\n    data = pyplot.imread(filename)\n    # plot the image\n    pyplot.imshow(data)\n    # get the context for drawing boxes\n    ax = pyplot.gca()\n    # plot each box\n    for i in range(len(v_boxes)):\n        box = v_boxes[i]\n        # get coordinates\n        y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\n        # calculate width and height of the box\n        width, height = x2 - x1, y2 - y1\n        # create the shape\n        rect = Rectangle((x1, y1), width, height, fill=False, color='white')\n        # draw the box\n        ax.add_patch(rect)\n        # draw text and score in top left corner\n        label = \"%s (%.3f)\" % (v_labels[i], v_scores[i])\n        pyplot.text(x1, y1, label, color='white')\n    # show the plot\n    pyplot.show()","2970a3d0":"!ls .\/\n","8e9d20a4":"loc = '.\/yolov3.weights'","37ec9790":"# define the model\nmodel = make_yolov3_model()\n\n\n# load the model weights\n# I have loaded the pretrained weights in a separate dataset\nweight_reader = WeightReader(loc)\n\n# set the model weights into the model\nweight_reader.load_weights(model)\n\n# save the model to file\nmodel.save('model.h5')","cf1ca7c2":"# load yolov3 model\nfrom tensorflow.keras.models import load_model\nmodel = load_model('model.h5')","1d3e3f2e":"model.summary()","c087d6fc":"# Parameters used in the Dataset, on which YOLOv3 was pretrained\nanchors = [[116,90, 156,198, 373,326], [30,61, 62,45, 59,119], [10,13, 16,30, 33,23]]\n\n# define the expected input shape for the model\nWIDTH, HEIGHT = 416, 416\n\n# define the probability threshold for detected objects\nclass_threshold = 0.3","15f39aba":"import os\nfrom matplotlib import pyplot as plt\nimages = os.listdir('..\/input\/parmalats-soft-toys\/originais')[:10]\n# images = os.listdir('..\/input\/parmalats-soft-toys\/')\n# images = os.listdir('..\/input\/parmalats-soft-toys\/skunk')","66b52fee":"from numpy import expand_dims\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n\n# load and prepare an image\ndef load_image_pixels(filename, shape):\n    '''\n    Function preprocess the images to 416x416, which is the standard input shape for YOLOv3, \n    and also keeps track of the originl shape, which is later used to draw the boxes.\n    \n    paramters:\n    filename {String}: path to the image\n    shape {tuple}: shape of the input dimensions of the network\n    \n    returns:\n    image {PIL}: image of shape 'shape'\n    width {int}: original width of the picture\n    height {int}: original height of the picture\n    '''\n    # load the image to get its shape\n    image = load_img(filename)\n    width, height = image.size\n    \n    # load the image with the required size\n    image = load_img(filename, target_size=shape)\n    \n    # convert to numpy array\n    image = img_to_array(image)\n    \n    # scale pixel values to [0, 1]\n    image = image.astype('float32')\n    image \/= 255.0\n    \n    # add a dimension so that we have one sample\n    image = expand_dims(image, 0)\n    return image, width, height","7a914dcb":"for file in images:\n    photo_filename = DATA_PATH + '\/originais\/' + file\n#     photo_filename = DATA_PATH + '\/skunk\/' + file\n    \n    # load picture with old dimensions\n    image, image_w, image_h = load_image_pixels(photo_filename, (WIDTH, HEIGHT))\n    \n    # Predict image\n    yhat = model.predict(image)\n    \n    # Create boxes\n    boxes = list()\n    for i in range(len(yhat)):\n        # decode the output of the network\n        boxes += decode_netout(yhat[i][0], anchors[i], class_threshold, HEIGHT, WIDTH)\n\n    # correct the sizes of the bounding boxes for the shape of the image\n    correct_yolo_boxes(boxes, image_h, image_w, HEIGHT, WIDTH)\n\n    # suppress non-maximal boxes\n    do_nms(boxes, 0.5)\n\n    # define the labels (Filtered only the ones relevant for this task, which were used in pretraining the YOLOv3 model)\n    #labels = [\"cow\", \"dog\", \"elephant\", \"giraffe\", \"gnu\", \"pig\", \"ram\",\"seal\",'skunk','tiger']\n    labels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\",\n    \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\",\n    \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\",\n    \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\",\n    \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n    \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\",\n    \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\",\n    \"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\",\n    \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\",\n    \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]\n\n    # get the details of the detected objects\n    v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)\n\n    # summarize what we found\n    for i in range(len(v_boxes)):\n\n        print(v_labels[i], v_scores[i])\n\n    # draw what we found\n    draw_boxes(photo_filename, v_boxes, v_labels, v_scores)","83ead7f1":"# Apresenta\u00e7\u00e3o do Terceiro Trabalho de Deep Learning.\n\n## Utilizamos a biblioteca tensorflow keras e o modelo YOLO v3\n\n## dataset produzido por Franklin Perseu\n\n## Trabalho efetuado por Felipe Brasil Guimar\u00e3es e Franklin Perseu de Lima e Lima\n\n\n\n![UEA](data:image\/png;base64,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)\n"}}