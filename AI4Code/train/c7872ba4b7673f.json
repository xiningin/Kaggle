{"cell_type":{"df20f40e":"code","3df0d102":"code","5e93a42b":"code","3f273042":"code","e40ea1dd":"code","8d58b697":"code","0a9adf30":"code","a1b1bf0b":"code","a7ac1140":"code","6f89230f":"code","0c574b4d":"code","b36374b6":"code","398e20e2":"code","429c2287":"code","92535594":"code","75da0f39":"code","657f8c45":"code","ec0f2119":"code","b1d83cde":"code","d965b5ea":"code","ed26edc6":"code","c975eb29":"code","3f8e53bb":"code","87f312a3":"code","e766f8fe":"markdown","0883ca3b":"markdown","a1e9f44d":"markdown","eb2a42c1":"markdown","43145b2b":"markdown","fa2e144c":"markdown","37b5c9ff":"markdown"},"source":{"df20f40e":"import random \nrandom.seed(123)\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn\n\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.tree import DecisionTreeRegressor\nimport sklearn.metrics as skm\nfrom sklearn.model_selection import train_test_split\nimport operator as op\n\nimport seaborn as sns\nsns.set(rc={'figure.figsize': (12,8)})\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3df0d102":"df = pd.read_csv('\/kaggle\/input\/restaurant-revenue-prediction\/train.csv.zip')\ndf.head()","5e93a42b":"# df_test = pd.read_csv('\/kaggle\/input\/restaurant-revenue-prediction\/test.csv.zip')\n# df_test.head()","3f273042":"# General info on the dataset\nprint(df.info())","e40ea1dd":"# Count the number of null values\ndf.isnull().sum()","8d58b697":"df.describe()","0a9adf30":"# Encoding Categorical Variable\ncat_vars = ['City', 'City Group', 'Type']\nfor i in cat_vars:\n    df[i+\"_cat\"] = df[i].astype('category').cat.codes\n# df.drop(cat_vars, axis=1, inplace=True)\ndf.head()","a1b1bf0b":"# Creating a plot for the correlation of features to the target variable\nfig = plt.figure(figsize=(20,16))\ntarget_corr = df[df.columns[1:]].corr()['revenue']\norder_corr = target_corr.sort_values()\ny = pd.DataFrame(order_corr).index[:-1]\nx = pd.DataFrame(order_corr).revenue[:-1]\nsns.barplot(x, y, orient='h')\nplt.show()","a7ac1140":"# Distribution of target variable (revenue) by Type and by City Group\nfig = plt.figure(figsize=(12,10))\ngs = fig.add_gridspec(1, 2, hspace=0.7, wspace=0.1)\n(ax1, ax2) = gs.subplots(sharex='col', sharey='row')\nfig.suptitle('Distributions of Total Revenue')\nfig.subplots_adjust(top=0.85)\n\nsns.histplot(data=df, x='revenue', bins=25,hue='Type', ax=ax1)\nsns.histplot(data=df, x='revenue', bins=25,hue='City Group', ax=ax2)\n\nplt.show()","6f89230f":"# Time Series of revenues generated\ndf_timeseries = pd.read_csv('\/kaggle\/input\/restaurant-revenue-prediction\/train.csv.zip',\n                             parse_dates=['Open Date'],\n                             index_col= ['Open Date'],\n                             na_values=['999.99'])\nsns.lineplot(data=df_timeseries, x='Open Date', y='revenue')\nplt.show()","0c574b4d":"# Changing the open_date into datetime\n# df['Open Date'] = pd.to_datetime(df['Open Date'])","b36374b6":"# Checking the options in both city, city_group, and Type\n# to evaluate if categorical encoding may be necessary\nprint(df.City.unique(), len(df.City.unique()))\nprint(df['City Group'].unique(), len(df['City Group'].unique()))\nprint(df['Type'].unique(), len(df['Type'].unique()))","398e20e2":"def encode_and_bind(df, feature):\n    dummies = pd.get_dummies(df[[feature]], prefix='')\n    return pd.concat([df, dummies], axis=1)\ndf = encode_and_bind(df, 'City')\ndf = encode_and_bind(df, 'City Group')\ndf = encode_and_bind(df, 'Type')\ndf.head(3)","429c2287":"df.head()","92535594":"# Features and Target \nX = df.drop(['revenue', 'Open Date', 'City', 'City Group', 'Type'], axis=1).values\ny = df['revenue'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1\/3, random_state=202)","75da0f39":"# Multiple Linear Regression\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X_train, y_train)\ny_pred = reg.predict(X_test)\nrmse = skm.mean_squared_error(y_test, y_pred, squared=False)\nprint(\"RMSE: {0}\".format(rmse))","657f8c45":"# Single Tree Regression\nreg = DecisionTreeRegressor(max_depth=10)\nreg.fit(X_train, y_train)\ny_pred = reg.predict(X_test)\nrmse = skm.mean_squared_error(y_test, y_pred, squared=False)\nprint(\"RMSE: {0}\".format(rmse))","ec0f2119":"# SVR - linear\nreg = SVR(kernel='linear', C=100, gamma='auto')\nreg.fit(X_train, y_train)\ny_pred = reg.predict(X_test)\nrmse = skm.mean_squared_error(y_test, y_pred, squared=False)\nprint(\"RMSE: {0}\".format(rmse))","b1d83cde":"# SVR - rbf\nreg = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\nreg.fit(X_train, y_train)\ny_pred = reg.predict(X_test)\nrmse = skm.mean_squared_error(y_test, y_pred, squared=False)\nprint(\"RMSE: {0}\".format(rmse))","d965b5ea":"# SVR - poly\nreg = SVR(kernel='poly', C=100, gamma='auto', degree=2, epsilon=.1,\n               coef0=1)\nreg.fit(X_train, y_train)\ny_pred = reg.predict(X_test)\nrmse = skm.mean_squared_error(y_test, y_pred, squared=False)\nprint(\"RMSE: {0}\".format(rmse))","ed26edc6":"# Random Forest\nreg = RandomForestRegressor(n_estimators=1000, criterion=\"mse\")\nreg.fit(X_train, y_train)\ny_pred = reg.predict(X_test)\nrmse = skm.mean_squared_error(y_test, y_pred, squared=False)\nprint(\"RMSE: {0}\".format(rmse))","c975eb29":"# Gradient Boosting\nfrom sklearn.ensemble import GradientBoostingRegressor\nreg = GradientBoostingRegressor(random_state=0)\nreg.fit(X_train, y_train)\ny_pred = reg.predict(X_test)\nrmse = skm.mean_squared_error(y_test, y_pred, squared=False)\nprint(\"RMSE: {0}\".format(rmse))","3f8e53bb":"# Ada Boosting\nfrom sklearn.ensemble import AdaBoostRegressor\nreg = AdaBoostRegressor(random_state=0, n_estimators=1000)\nreg.fit(X_train, y_train)\ny_pred = reg.predict(X_test)\nrmse = skm.mean_squared_error(y_test, y_pred, squared=False)\nprint(\"RMSE: {0}\".format(rmse))","87f312a3":"from sklearn.ensemble import BaggingRegressor\nreg = BaggingRegressor(base_estimator=SVR(),\n                        n_estimators=1000, random_state=0)\nreg.fit(X_train, y_train)\ny_pred = reg.predict(X_test)\nrmse = skm.mean_squared_error(y_test, y_pred, squared=False)\nprint(\"RMSE: {0}\".format(rmse))","e766f8fe":"## 2 - Ensemble Learning","0883ca3b":"## 1- Single Models","a1e9f44d":"### What's next? Putting all the modeling into 4 respective functions for split\/cv for single and ensemble learning methods.","eb2a42c1":"# Data Importing ","43145b2b":"# Exploratory Data Analysis","fa2e144c":"# Feature Transformation","37b5c9ff":"# Model Building"}}