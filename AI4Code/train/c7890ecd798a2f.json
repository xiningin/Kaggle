{"cell_type":{"aca5bbc7":"code","7cd8c5d1":"code","f63892f3":"code","8912eed6":"code","80dec0ff":"code","f257effa":"code","37ee40c4":"code","c18c24b3":"code","a8f3bbd0":"code","2f6e0fd4":"code","3396e20f":"code","d79cf723":"code","e58dbde0":"code","c2651c90":"code","5e927dc1":"code","3bb2ed4b":"code","e9381895":"code","3d5878c0":"code","d6c42386":"code","18db98fd":"code","9f5bc4b5":"code","9a1d7acf":"code","d06ae5f0":"code","c850693c":"code","9279c552":"code","807a0ee7":"code","41f70b8f":"code","155c5713":"code","4fb1218d":"code","af943501":"code","69225bfc":"code","b9ec4fcd":"code","76cbd235":"code","c2670add":"code","2528315f":"code","751d54eb":"code","55bc2c58":"code","9c98bfd0":"code","ef202a28":"code","b36f07d9":"code","c2554a11":"code","67adc81c":"code","f459c6bb":"code","74dabe84":"code","2530c909":"code","1ebe84d8":"code","167e5f4b":"code","d0632cde":"code","02d96cb0":"code","b72e2359":"code","98b50d12":"code","acba17b2":"code","648e6807":"code","83cc40b3":"code","9b100bf5":"code","b06103ad":"code","d95da138":"code","957cf205":"markdown","66931818":"markdown","889a6a17":"markdown","c021f9ba":"markdown","9e0c5c74":"markdown","0c4ab845":"markdown","1bc9b114":"markdown","2195105b":"markdown","3cfd783a":"markdown","b75149f8":"markdown","048bcbb1":"markdown","1e175a51":"markdown","0234fe8d":"markdown","bef4514b":"markdown","b3f35e16":"markdown","3e9e15cc":"markdown","5255ddbf":"markdown","b3bf8104":"markdown","f6986096":"markdown","e9cd1f10":"markdown","69e4b193":"markdown","d5010867":"markdown","9ffbaf94":"markdown","f12e9ff3":"markdown","676844c6":"markdown","91e87bd6":"markdown","6450777e":"markdown","8277bd54":"markdown","157d41d7":"markdown","0e64137f":"markdown","98e8ec92":"markdown","a38bd63f":"markdown","03a8d8f9":"markdown","8da972d2":"markdown","8894e044":"markdown","ba3ea74d":"markdown","18281277":"markdown","618227f8":"markdown","f982141e":"markdown","698afacd":"markdown","8e3f6cc1":"markdown","b7f5656b":"markdown","cc5c9f77":"markdown","63a08fea":"markdown","c1d891da":"markdown","344266fb":"markdown"},"source":{"aca5bbc7":"%%bash\npip install -q transformers","7cd8c5d1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pathlib import Path \n\nimport os\n\nimport torch\nimport torch.optim as optim\n\nimport random \n\n# fastai\nfrom fastai import *\nfrom fastai.text import *\nfrom fastai.callbacks import *\n\n# transformers\nfrom transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n\nfrom transformers import BertForSequenceClassification, BertTokenizer, BertConfig\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\nfrom transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\nfrom transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig\nfrom transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig","f63892f3":"import fastai\nimport transformers\nprint('fastai version :', fastai.__version__)\nprint('transformers version :', transformers.__version__)","8912eed6":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","80dec0ff":"DATA_ROOT = Path(\"..\") \/ \"\/kaggle\/input\/sentiment-analysis-on-movie-reviews\"\ntrain = pd.read_csv(DATA_ROOT \/ 'train.tsv.zip', sep=\"\\t\")\ntest = pd.read_csv(DATA_ROOT \/ 'test.tsv.zip', sep=\"\\t\")\nprint(train.shape,test.shape)\ntrain.head()","f257effa":"MODEL_CLASSES = {\n    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n    'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),\n    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig)\n}","37ee40c4":"# Parameters\nseed = 42\nuse_fp16 = False\nbs = 16\n\nmodel_type = 'roberta'\npretrained_model_name = 'roberta-base'\n\n# model_type = 'bert'\n# pretrained_model_name='bert-base-uncased'\n\n# model_type = 'distilbert'\n# pretrained_model_name = 'distilbert-base-uncased'\n\n#model_type = 'xlm'\n#pretrained_model_name = 'xlm-clm-enfr-1024'\n\n# model_type = 'xlnet'\n# pretrained_model_name = 'xlnet-base-cased'","c18c24b3":"model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]","a8f3bbd0":"model_class.pretrained_model_archive_map.keys()","2f6e0fd4":"def seed_all(seed_value):\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False","3396e20f":"seed_all(seed)","d79cf723":"class TransformersBaseTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', **kwargs):\n        self._pretrained_tokenizer = pretrained_tokenizer\n        self.max_seq_len = pretrained_tokenizer.max_len\n        self.model_type = model_type\n\n    def __call__(self, *args, **kwargs): \n        return self\n\n    def tokenizer(self, t:str) -> List[str]:\n        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\n        CLS = self._pretrained_tokenizer.cls_token\n        SEP = self._pretrained_tokenizer.sep_token\n        if self.model_type in ['roberta']:\n            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n            tokens = [CLS] + tokens + [SEP]\n        else:\n            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n            if self.model_type in ['xlnet']:\n                tokens = tokens + [SEP] +  [CLS]\n            else:\n                tokens = [CLS] + tokens + [SEP]\n        return tokens","e58dbde0":"transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\ntransformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\nfastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])","c2651c90":"class TransformersVocab(Vocab):\n    def __init__(self, tokenizer: PreTrainedTokenizer):\n        super(TransformersVocab, self).__init__(itos = [])\n        self.tokenizer = tokenizer\n    \n    def numericalize(self, t:Collection[str]) -> List[int]:\n        \"Convert a list of tokens `t` to their ids.\"\n        return self.tokenizer.convert_tokens_to_ids(t)\n        #return self.tokenizer.encode(t)\n\n    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n        \"Convert a list of `nums` to their tokens.\"\n        nums = np.array(nums).tolist()\n        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)\n    \n    def __getstate__(self):\n        return {'itos':self.itos, 'tokenizer':self.tokenizer}\n\n    def __setstate__(self, state:dict):\n        self.itos = state['itos']\n        self.tokenizer = state['tokenizer']\n        self.stoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.itos)})","5e927dc1":"transformer_vocab =  TransformersVocab(tokenizer = transformer_tokenizer)\nnumericalize_processor = NumericalizeProcessor(vocab=transformer_vocab)\n\ntokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, include_bos=False, include_eos=False)\n\ntransformer_processor = [tokenize_processor, numericalize_processor]","3bb2ed4b":"pad_first = bool(model_type in ['xlnet'])\npad_idx = transformer_tokenizer.pad_token_id","e9381895":"tokens = transformer_tokenizer.tokenize('Salut c est moi, Hello it s me')\nprint(tokens)\nids = transformer_tokenizer.convert_tokens_to_ids(tokens)\nprint(ids)\ntransformer_tokenizer.convert_ids_to_tokens(ids)\n","3d5878c0":"databunch = (TextList.from_df(train, cols='Phrase', processor=transformer_processor)\n             .split_by_rand_pct(0.1,seed=seed)\n             .label_from_df(cols= 'Sentiment')\n             .add_test(test)\n             .databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))","d6c42386":"print('[CLS] token :', transformer_tokenizer.cls_token)\nprint('[SEP] token :', transformer_tokenizer.sep_token)\nprint('[PAD] token :', transformer_tokenizer.pad_token)\ndatabunch.show_batch()","18db98fd":"print('[CLS] id :', transformer_tokenizer.cls_token_id)\nprint('[SEP] id :', transformer_tokenizer.sep_token_id)\nprint('[PAD] id :', pad_idx)\ntest_one_batch = databunch.one_batch()[0]\nprint('Batch shape : ',test_one_batch.shape)\nprint(test_one_batch)","9f5bc4b5":"# defining our model architecture \nclass CustomTransformerModel(nn.Module):\n    def __init__(self, transformer_model: PreTrainedModel):\n        super(CustomTransformerModel,self).__init__()\n        self.transformer = transformer_model\n        \n    def forward(self, input_ids, attention_mask=None):\n        \n        # attention_mask\n        # Mask to avoid performing attention on padding token indices.\n        # Mask values selected in ``[0, 1]``:\n        # ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n        attention_mask = (input_ids!=pad_idx).type(input_ids.type()) \n        \n        logits = self.transformer(input_ids,\n                                  attention_mask = attention_mask)[0]   \n        return logits","9a1d7acf":"config = config_class.from_pretrained(pretrained_model_name)\nconfig.num_labels = 5\nconfig.use_bfloat16 = use_fp16\nprint(config)","d06ae5f0":"transformer_model = model_class.from_pretrained(pretrained_model_name, config = config)\n# transformer_model = model_class.from_pretrained(pretrained_model_name, num_labels = 5)\n\ncustom_transformer_model = CustomTransformerModel(transformer_model = transformer_model)","c850693c":"from fastai.callbacks import *\nfrom transformers import AdamW\nfrom functools import partial\n\nCustomAdamW = partial(AdamW, correct_bias=False)\n\nlearner = Learner(databunch, \n                  custom_transformer_model, \n                  opt_func = CustomAdamW, \n                  metrics=[accuracy, error_rate])\n\n# Show graph of learner stats and metrics after each epoch.\nlearner.callbacks.append(ShowGraph(learner))\n\n# Put learn in FP16 precision mode. --> Seems to not working\nif use_fp16: learner = learner.to_fp16()","9279c552":"print(learner.model)","807a0ee7":"# For DistilBERT\n# list_layers = [learner.model.transformer.distilbert.embeddings,\n#                learner.model.transformer.distilbert.transformer.layer[0],\n#                learner.model.transformer.distilbert.transformer.layer[1],\n#                learner.model.transformer.distilbert.transformer.layer[2],\n#                learner.model.transformer.distilbert.transformer.layer[3],\n#                learner.model.transformer.distilbert.transformer.layer[4],\n#                learner.model.transformer.distilbert.transformer.layer[5],\n#                learner.model.transformer.pre_classifier]\n\n# For xlnet-base-cased\n# list_layers = [learner.model.transformer.transformer.word_embedding,\n#               learner.model.transformer.transformer.layer[0],\n#               learner.model.transformer.transformer.layer[1],\n#               learner.model.transformer.transformer.layer[2],\n#               learner.model.transformer.transformer.layer[3],\n#               learner.model.transformer.transformer.layer[4],\n#               learner.model.transformer.transformer.layer[5],\n#               learner.model.transformer.transformer.layer[6],\n#               learner.model.transformer.transformer.layer[7],\n#               learner.model.transformer.transformer.layer[8],\n#               learner.model.transformer.transformer.layer[9],\n#               learner.model.transformer.transformer.layer[10],\n#               learner.model.transformer.transformer.layer[11],\n#               learner.model.transformer.sequence_summary]\n\n# For roberta-base\nlist_layers = [learner.model.transformer.roberta.embeddings,\n              learner.model.transformer.roberta.encoder.layer[0],\n              learner.model.transformer.roberta.encoder.layer[1],\n              learner.model.transformer.roberta.encoder.layer[2],\n              learner.model.transformer.roberta.encoder.layer[3],\n              learner.model.transformer.roberta.encoder.layer[4],\n              learner.model.transformer.roberta.encoder.layer[5],\n              learner.model.transformer.roberta.encoder.layer[6],\n              learner.model.transformer.roberta.encoder.layer[7],\n              learner.model.transformer.roberta.encoder.layer[8],\n              learner.model.transformer.roberta.encoder.layer[9],\n              learner.model.transformer.roberta.encoder.layer[10],\n              learner.model.transformer.roberta.encoder.layer[11],\n              learner.model.transformer.roberta.pooler]","41f70b8f":"learner.split(list_layers)\nnum_groups = len(learner.layer_groups)\nprint('Learner split in',num_groups,'groups')\nprint(learner.layer_groups)\n","155c5713":"learner.save('untrain')","4fb1218d":"seed_all(seed)\nlearner.load('untrain');","af943501":"learner.freeze_to(-1)","69225bfc":"learner.summary()","b9ec4fcd":"learner.lr_find()","76cbd235":"learner.recorder.plot(skip_end=10,suggestion=True)","c2670add":"learner.fit_one_cycle(1,max_lr=2e-03,moms=(0.8,0.7))","2528315f":"learner.save('first_cycle')","751d54eb":"seed_all(seed)\nlearner.load('first_cycle');","55bc2c58":"learner.freeze_to(-2)","9c98bfd0":"lr = 1e-5","ef202a28":"learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))","b36f07d9":"learner.save('second_cycle')","c2554a11":"seed_all(seed)\nlearner.load('second_cycle');","67adc81c":"learner.freeze_to(-3)","f459c6bb":"learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))","74dabe84":"learner.save('third_cycle')","2530c909":"seed_all(seed)\nlearner.load('third_cycle');","1ebe84d8":"learner.unfreeze()","167e5f4b":"learner.fit_one_cycle(2, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))","d0632cde":"learner.predict('This is the best movie of 2020')","02d96cb0":"learner.predict('This is the worst movie of 2020')","b72e2359":"learner.export(file = 'transformer.pkl');","98b50d12":"path = '\/kaggle\/working'\nexport_learner = load_learner(path, file = 'transformer.pkl')","acba17b2":"export_learner.predict('This is the worst movie of 2020')","648e6807":"def get_preds_as_nparray(ds_type) -> np.ndarray:\n    \"\"\"\n    the get_preds method does not yield the elements in order by default\n    we borrow the code from the RNNLearner to resort the elements into their correct order\n    \"\"\"\n    preds = learner.get_preds(ds_type)[0].detach().cpu().numpy()\n    sampler = [i for i in databunch.dl(ds_type).sampler]\n    reverse_sampler = np.argsort(sampler)\n    return preds[reverse_sampler, :]\n\ntest_preds = get_preds_as_nparray(DatasetType.Test)","83cc40b3":"sample_submission = pd.read_csv(DATA_ROOT \/ 'sampleSubmission.csv')\nsample_submission['Sentiment'] = np.argmax(test_preds,axis=1)\nsample_submission.to_csv(\"predictions.csv\", index=False)","9b100bf5":"test.head()","b06103ad":"sample_submission.head()","d95da138":"from IPython.display import HTML\n\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}<\/a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe which was saved with .to_csv method\ncreate_download_link(filename='predictions.csv')","957cf205":"## Data pre-processing\n\nTo match pre-training, we have to format the model input sequence in a specific format.\nTo do so, you have to first **tokenize** and then **numericalize** the texts correctly.\nThe difficulty here is that each pre-trained model, that we will fine-tune, requires exactly the same specific pre-process\u200a-\u200a**tokenization** & **numericalization**\u200a-\u200athan the pre-process used during the pre-train part.\nFortunately, the **tokenizer class** from ``transformers`` provides the correct pre-process tools that correspond to each pre-trained model.\n\nIn the ``fastai`` library, data pre-processing is done automatically during the creation of the ``DataBunch``. \nAs you will see in the ``DataBunch`` implementation, the **tokenizer** and **numericalizer** are passed in the processor argument under the following format :\n\n``processor = [TokenizeProcessor(tokenizer=tokenizer,...), NumericalizeProcessor(vocab=vocab,...)]``\n\nLet's first analyse how we can integrate the ``transformers`` **tokenizer** within the ``TokenizeProcessor`` function.\n\n### Custom Tokenizer\nThis part can be a little bit confusing because a lot of classes are wrapped in each other and with similar names.\nTo resume, if we look attentively at the ``fastai`` implementation, we notice that\u00a0:\n1. The [``TokenizeProcessor`` object](https:\/\/docs.fast.ai\/text.data.html#TokenizeProcessor) takes as ``tokenizer`` argument a ``Tokenizer`` object.\n2. The [``Tokenizer`` object](https:\/\/docs.fast.ai\/text.transform.html#Tokenizer) takes as ``tok_func`` argument a ``BaseTokenizer`` object.\n3. The [``BaseTokenizer`` object](https:\/\/docs.fast.ai\/text.transform.html#BaseTokenizer) implement the function ``tokenizer(t:str) \u2192 List[str]`` that take a text ``t`` and returns the list of its tokens.\n\nTherefore, we can simply create a new class ``TransformersBaseTokenizer`` that inherits from ``BaseTokenizer`` and overwrite a new ``tokenizer`` function.\n","66931818":"Now, you can predict examples with:","889a6a17":"### Custom model\nAs mentioned [here](https:\/\/github.com\/huggingface\/transformers#models-always-output-tuples), every model's forward method always outputs a ``tuple`` with various elements depending on the model and the configuration parameters. In our case, we are interested to access only to the logits.\u00a0\nOne way to access them is to create a custom model.","c021f9ba":"# References\n* Hugging Face, Transformers GitHub (Nov 2019), [https:\/\/github.com\/huggingface\/transformers](https:\/\/github.com\/huggingface\/transformers)\n* Fast.ai, Fastai documentation (Nov 2019), [https:\/\/docs.fast.ai\/text.html](https:\/\/docs.fast.ai\/text.html)\n* Jeremy Howard & Sebastian Ruder, Universal Language Model Fine-tuning for Text Classification (May 2018), [https:\/\/arxiv.org\/abs\/1801.06146](https:\/\/arxiv.org\/abs\/1801.06146)\n* Keita Kurita's article\u00a0: [A Tutorial to Fine-Tuning BERT with Fast AI](https:\/\/mlexplained.com\/2019\/05\/13\/a-tutorial-to-fine-tuning-bert-with-fast-ai\/)\u00a0(May 2019)\n* Dev Sharma's article\u00a0: [Using RoBERTa with Fastai for NLP](https:\/\/medium.com\/analytics-vidhya\/using-roberta-with-fastai-for-nlp-7ed3fed21f6c) (Sep 2019)","9e0c5c74":"## Train\nNow we can finally use all the fastai build-in features to train our model. Like the ULMFiT method, we will use **Slanted Triangular Learning Rates**, **Discriminate Learning Rate** and **gradually unfreeze the model**.","0c4ab845":"We check the order.","1bc9b114":"In this implementation, be carefull about 3 things :\n1. As we are not using RNN, we have to limit the sequence length to the model input size.\n2. Most of the models require special tokens placed at the beginning and end of the sequences.\n3. Some models like RoBERTa require a space to start the input string. For those models, the encoding methods should be called with ``add_prefix_space`` set to ``True``.\n\nBelow, you can find the resume of each pre-process requirement for the 5 model types used in this tutorial. You can also find this information on the [HuggingFace documentation](https:\/\/huggingface.co\/transformers\/) in each model section.\n\n    bert:       [CLS] + tokens + [SEP] + padding\n\n    roberta:    [CLS] + prefix_space + tokens + [SEP] + padding\n    \n    distilbert: [CLS] + tokens + [SEP] + padding\n\n    xlm:        [CLS] + tokens + [SEP] + padding\n\n    xlnet:      padding + tokens + [SEP] + [CLS]\n    \nIt is worth noting that we don't add padding in this part of the implementation.\u00a0\nAs we will see later, ``fastai`` manage it automatically during the creation of the ``DataBunch``.","2195105b":"To make our transformers adapted to multiclass classification, before loading the pre-trained model, we need to precise the number of labels. To do so, you can modify the config instance or either modify like in [Keita Kurita's article](https:\/\/mlexplained.com\/2019\/05\/13\/a-tutorial-to-fine-tuning-bert-with-fast-ai\/) (Section: *Initializing the Learner*) the ``num_labels`` argument.","3cfd783a":"We then unfreeze the second group of layers and repeat the operations.","b75149f8":"## Discriminative Fine-tuning and Gradual unfreezing (Optional)\nTo use **discriminative layer training** and **gradual unfreezing**, ``fastai`` provides one tool that allows to \"split\" the structure model into groups. An instruction to perform that \"split\" is described in the fastai documentation [here](https:\/\/docs.fast.ai\/basic_train.html#Discriminative-layer-training).\n\nUnfortunately,  the model architectures are too different to create a unique generic function that can \"split\" all the model types in a convenient way. Thereby, you will have to implement a custom \"split\" for each different model architecture.\n\nFor example, if we use the RobBERTa model and that we observe his architecture by making ``print(learner.model)``.","048bcbb1":"## \ud83c\udfac The example\u00a0task\nThe chosen task is a multi-class text classification on [Movie Reviews](https:\/\/www.kaggle.com\/c\/sentiment-analysis-on-movie-reviews\/overview).\n\nFor each text movie review, the model has to predict a label for the sentiment. We evaluate the outputs of the model on classification accuracy. The sentiment labels are:\n* 0 \u2192 Negative\n* 1 \u2192 Somewhat negative\n* 2 \u2192 Neutral\n* 3 \u2192 Somewhat positive\n* 4 \u2192 Positive\n\nThe data is loaded into a ``DataFrame`` using ``pandas``.","1e175a51":"We check which layer are trainable.","0234fe8d":"## Util function","bef4514b":"# Fastai with HuggingFace \ud83e\udd17Transformers (BERT, RoBERTa, XLNet, XLM, DistilBERT)\n\n![fastai + Transformers](https:\/\/i.ibb.co\/qspmrcm\/fastai-transformers-1.png)","b3f35e16":"Therefore, we first freeze all the groups but the classifier with\u00a0:","3e9e15cc":"# Introduction : Story of transfer learning in NLP\nIn early 2018, Jeremy Howard (co-founder of fast.ai) and Sebastian Ruder introduced the  [Universal Language Model Fine-tuning for Text Classification](https:\/\/medium.com\/r\/?url=https%3A%2F%2Farxiv.org%2Fpdf%2F1801.06146.pdf) (ULMFiT) method. ULMFiT was the first **Transfer Learning** method applied to NLP. As a result, besides significantly outperforming many state-of-the-art tasks, it allowed, with only 100 labeled examples, to match performances equivalent to models trained on 100\u00d7  more data.\n\nThe first time I heard about ULMFiT was during a [fast.ai course](https:\/\/course.fast.ai\/videos\/?lesson=4) given by Jeremy Howard. He demonstrated how it was easy \u200a-\u200a thanks to the ``fastai`` library \u200a-\u200a to implement the complete ULMFit method with only a few lines of codes. In his demo, he used an AWD-LSTM neural network pre-trained on Wikitext-103 and get rapidly state-of-the-art results. He also explained key techniques - also demonstrated in ULMFiT - to fine-tune the models like **Discriminate Learning Rate**, **Gradual Unfreezing** or **Slanted Triangular Learning Rates**.\n\nSince the introduction of ULMFiT, **Transfer Learning** became very popular in NLP and yet Google (BERT, Transformer-XL, XLNet), Facebook (RoBERTa, XLM) or even OpenAI (GPT, GPT-2) begin to pre-train their own model on very large corpora. This time, instead of using the AWD-LSTM neural network, they all used a more powerful architecture based on the Transformer (cf. [Attention is all you need](https:\/\/arxiv.org\/abs\/1706.03762)).\n\nAlthough these models are powerful, ``fastai`` do not integrate all of them. Fortunately, [HuggingFace](https:\/\/huggingface.co\/) \ud83e\udd17 created the well know [transformers library](https:\/\/github.com\/huggingface\/transformers). Formerly knew as ``pytorch-transformers`` or ``pytorch-pretrained-bert``, this library brings together over 40 state-of-the-art pre-trained NLP models (BERT, GPT-2, RoBERTa, CTRL\u2026). The implementation gives interesting additional utilities like tokenizer, optimizer or scheduler.\n\nThe ``transformers`` library can be self-sufficient but incorporating it within the ``fastai`` library provides simpler implementation compatible with powerful fastai tools like  **Discriminate Learning Rate**, **Gradual Unfreezing** or **Slanted Triangular Learning Rates**. The point here is to allow anyone \u2014 expert or non-expert \u2014 to get easily state-of-the-art results and to \u201cmake NLP uncool again\u201d.\n\nIt worth noting that the integration of the HuggingFace ``transformers`` library in ``fastai`` has already been demonstrated in:\n* Keita Kurita's article [A Tutorial to Fine-Tuning BERT with Fast AI](https:\/\/mlexplained.com\/2019\/05\/13\/a-tutorial-to-fine-tuning-bert-with-fast-ai\/) which makes ``pytorch_pretrained_bert`` library compatible with ``fastai``.\n* Dev Sharma's article [Using RoBERTa with Fastai for NLP](https:\/\/medium.com\/analytics-vidhya\/using-roberta-with-fastai-for-nlp-7ed3fed21f6c) which makes ``pytorch_transformers`` library compatible with ``fastai``.\n\nAlthough these articles are of high quality, some part of their demonstration is not anymore compatible with the last version of ``transformers``.\n","5255ddbf":"Note here that we use slice to create separate learning rate for each group.","b3bf8104":"Check batch and numericalizer :","f6986096":"Note that I didn't found any document that has studied the influence of **Discriminative Fine-tuning** and **Gradual unfreezing** or even **Slanted Triangular Learning Rates** with transformers. Therefore, using these tools does not guarantee better results. If you found any interesting documents, please let us know in the comment.","e9cd1f10":"Check batch and tokenizer :","69e4b193":"You will see later, that those classes share a common class method ``from_pretrained(pretrained_model_name,\u00a0...)``. In our case, the parameter ``pretrained_model_name`` is a string with the shortcut name of a pre-trained model\/tokenizer\/configuration to load, e.g ``'bert-base-uncased'``. We can find all the shortcut names in the transformers documentation [here](https:\/\/huggingface.co\/transformers\/pretrained_models.html#pretrained-models).","d5010867":"N.B. This implementation is a supplement of the Medium article [\"Fastai with \ud83e\udd17Transformers (BERT, RoBERTa, XLNet, XLM, DistilBERT)\"](https:\/\/medium.com\/p\/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2?source=email-29c8f5cf1dc4--writer.postDistributed&sk=119c3e5d748b2827af3ea863faae6376).\n\n**Also, remember the upvote button is next to the fork button, and it's free too!** \ud83d\ude09","9ffbaf94":"## Learner\u00a0: Custom Optimizer \/ Custom\u00a0Metric\nIn ``pytorch-transformers``, HuggingFace had implemented two specific optimizers \u200a-\u200a BertAdam and OpenAIAdam \u200a-\u200a that have been replaced by a single AdamW optimizer.\nThis optimizer matches Pytorch Adam optimizer Api, therefore, it becomes straightforward to integrate it within ``fastai``.\nIt is worth noting that for reproducing BertAdam specific behavior, you have to set ``correct_bias = False``.\n","f12e9ff3":"The current versions of the fastai and transformers libraries are respectively 1.0.58 and 2.5.1.","676844c6":"Print the available values for ``pretrained_model_name`` (shortcut names) corresponding to the ``model_type`` used.","91e87bd6":"Here, we unfreeze all the groups.","6450777e":"Check groups : ","8277bd54":"## Export Learner\nIn order to export and load the learner you can do these operations:","157d41d7":"### Custom processor\nNow that we have our custom **tokenizer** and **numericalizer**, we can create the custom **processor**. Notice we are passing the ``include_bos = False`` and ``include_eos = False`` options. This is because ``fastai`` adds its own special tokens by default which interferes with the ``[CLS]`` and ``[SEP]`` tokens added by our custom tokenizer.","0e64137f":"As mentioned [here](https:\/\/docs.fast.ai\/basic_train.html#load_learner), you have to be careful that each custom classes - like ``TransformersVocab`` - are first defined before executing ``load_learner``.","98e8ec92":"### Custom Numericalizer\n\nIn ``fastai``, [``NumericalizeProcessor``  object](https:\/\/docs.fast.ai\/text.data.html#NumericalizeProcessor) takes as ``vocab`` argument a [``Vocab`` object](https:\/\/docs.fast.ai\/text.transform.html#Vocab). \nFrom this analyse, we suggest two ways to adapt the fastai numericalizer:\n1. You can, like decribed in the [Dev Sharma's article](https:\/\/medium.com\/analytics-vidhya\/using-roberta-with-fastai-for-nlp-7ed3fed21f6c) (Section *1. Setting Up the Tokenizer*), retreive the list of tokens and create a ``Vocab`` object.\n2. Create a new class ``TransformersVocab`` that inherits from ``Vocab`` and overwrite ``numericalize`` and ``textify`` functions.\n\nEven if the first solution seems to be simpler, ``Transformers`` does not provide, for all models, a straightforward way to retreive his list of tokens. \nTherefore, I implemented the second solution, which runs for each model type.\nIt consists of using the functions ``convert_tokens_to_ids`` and ``convert_ids_to_tokens`` in respectively ``numericalize`` and ``textify``.","a38bd63f":"Function to set the seed for generating random numbers.","03a8d8f9":"We will pick a value a bit before the minimum, where the loss still improves. Here 2x10^-3 seems to be a good value.\n\nNext we will use ``fit_one_cycle`` with the chosen learning rate as the maximum learning rate. ","8da972d2":"# Conclusion\n\nIn this NoteBook, I explain how to combine the ``transformers`` library with the beloved ``fastai`` library. It aims to make you understand where to look and modify both libraries to make them work together. Likely, it allows you to use **Slanted Triangular Learning Rates**, **Discriminate Learning Rate** and even **Gradual Unfreezing**. As a result, without even tunning the parameters, you can obtain rapidly state-of-the-art results.\n\nThis year, the transformers became an essential tool to NLP. Because of that, I think that pre-trained transformers architectures will be integrated soon to future versions of fastai. Meanwhile, this tutorial is a good starter.\n\nI hope you enjoyed this first article and found it useful.\u00a0\nThanks for reading and don't hesitate in leaving questions or suggestions.\n","8894e044":"It is worth noting that in the dataset there are no individual movie reviews but rather phrases taken out of context and split into smaller parts, each with an assigned sentiment label.","ba3ea74d":"For **Slanted Triangular Learning Rates** you have to use the function ``one_cycle``. For more information please check the fastai documentation [here](https:\/\/docs.fast.ai\/callbacks.one_cycle.html).\u00a0\n\nTo use our ``one_cycle`` we will need an optimum learning rate. We can find this learning rate by using a learning rate finder which can be called by using ``lr_find``.","18281277":"## Libraries Installation\nBefore starting the implementation, you will need to install the ``fastai`` and ``transformers`` libraries. To do so, just follow the instructions [here](https:\/\/github.com\/fastai\/fastai\/blob\/master\/README.md#installation) and [here](https:\/\/github.com\/huggingface\/transformers#installation).\n\nIn Kaggle, the ``fastai`` library is already installed. So you just have to instal ``transformers`` with :","618227f8":"It is worth noting that in this case, we use the ``transformers`` library only for a multi-class text classification task. For that reason, this tutorial integrates only the transformer architectures that have a model for sequence classification implemented. These model types are\u00a0:\n* BERT (from Google)\n* XLNet (from Google\/CMU)\n* XLM (from Facebook)\n* RoBERTa (from Facebook)\n* DistilBERT (from HuggingFace)\n\nHowever, if you want to go further\u200a-\u200aby implementing another type of model or NLP task\u200a-\u200athis tutorial still an excellent starter.","f982141e":"NB: The functions ``__gestate__`` and ``__setstate__`` allow the functions [export](https:\/\/docs.fast.ai\/basic_train.html#Learner.export) and [load_learner](https:\/\/docs.fast.ai\/basic_train.html#load_learner) to work correctly with ``TransformersVocab``.","698afacd":"## Creating prediction\nNow that the model is trained, we want to generate predictions from the test dataset.\n\nAs specified in Keita Kurita's [article](https:\/\/mlexplained.com\/2019\/05\/13\/a-tutorial-to-fine-tuning-bert-with-fast-ai\/), as the function ``get_preds`` does not return elements in order by default, you will have to resort the elements into their correct order.","8e3f6cc1":"# \ud83d\udee0 Integrating transformers with fastai for multiclass classification\nBefore beginning the implementation, note that integrating ``transformers`` within ``fastai`` can be done in multiple different ways. For that reason, I decided to bring simple solutions, that are the most generic and flexible. More precisely, I try to make the minimum of modification in both libraries while making them compatible with the maximum amount of transformer architectures.\n\nNote that in addition to this NoteBook and the [Medium article](https:\/\/medium.com\/p\/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2?source=email-29c8f5cf1dc4--writer.postDistributed&sk=119c3e5d748b2827af3ea863faae6376), I made another version available on my GitHub(TODO add link).","b7f5656b":"There is multible ways to create a DataBunch, in our implementation, we use [the data block API](https:\/\/docs.fast.ai\/data_block.html#The-data-block-API), which gives more flexibility.","cc5c9f77":"We can decide to divide the model in 14 blocks\u00a0:\n* 1 Embedding\n* 12 transformer\n* 1 classifier\n\nIn this case, we can split our model in this way\u00a0:","63a08fea":"## Main transformers classes\nIn ``transformers``, each model architecture is associated with 3 main types of classes:\n* A **model class** to load\/store a particular pre-train model.\n* A **tokenizer class** to pre-process the data and make it compatible with a particular model.\n* A **configuration class** to load\/store the configuration of a particular model.\n\nFor example, if you want to use the Bert architecture for text classification, you would use [``BertForSequenceClassification``](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#bertforsequenceclassification) for the **model class**, [``BertTokenizer``](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#berttokenizer) for the **tokenizer class** and [``BertConfig``](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#bertconfig) for the **configuration class**.\u00a0\n\nIn order to switch easily between classes \u200a-\u200a each related to a specific model type \u200a-\u200a I created a dictionary that allows loading the correct classes by just specifying the correct model type name.","c1d891da":"## Setting up the Databunch\nFor the DataBunch creation, you have to pay attention to set the processor argument to our new custom processor ``transformer_processor`` and manage correctly the padding.\n\nAs mentioned in the HuggingFace documentation, BERT, RoBERTa, XLM and DistilBERT are models with absolute position embeddings, so it's usually advised to pad the inputs on the right rather than the left. Regarding XLNET, it is a model with relative position embeddings, therefore, you can either pad the inputs on the right or on the left.","344266fb":"We can now submit our predictions to Kaggle !  In our example, without playing too much with the parameters, we get a score of 0.70059, which leads us to the 5th position on the leaderboard! "}}