{"cell_type":{"6927fab4":"code","b11164a9":"code","e87d2f5b":"code","4fd169c5":"code","8c300e50":"code","b24f70d7":"code","b9df5c90":"code","dd2a5728":"code","f65c156c":"code","7610cc97":"code","ba781d51":"code","d32fa132":"code","44d7a6d6":"code","27ca8a75":"code","5d7f1b22":"code","0b005920":"code","15eb386e":"code","276d9645":"code","5c9b0b6a":"code","7d9ecd6c":"code","527d80b3":"code","c414e3d9":"code","502a8a74":"code","62c7dc03":"code","ed0df731":"code","8f8a2d72":"code","f2cdcdf2":"code","b85d1462":"code","9ea761ef":"code","aa652dda":"code","a1089c90":"code","11f02b6b":"code","26af9ad5":"code","683c5925":"code","0f0776b9":"code","56efb3c1":"code","ecbe9324":"code","e2945c20":"code","61d692fd":"markdown","f375fc8b":"markdown","8c6a60a3":"markdown","421ccb83":"markdown","60d83287":"markdown","54da8181":"markdown","6e0936dd":"markdown","9addd571":"markdown","366e5228":"markdown","80d32670":"markdown","461eac3f":"markdown","bfef5551":"markdown","8f1607d5":"markdown","a8a1ed77":"markdown","9b973825":"markdown","aa2a1bab":"markdown","86ef0b56":"markdown","6993008f":"markdown","a02cd51c":"markdown","a2e9be6f":"markdown","40fd8e2b":"markdown","e94f0535":"markdown","a08dd928":"markdown","9b21eb8d":"markdown","7eec7681":"markdown"},"source":{"6927fab4":"# import necessary libraries\n\nimport numpy as np\nimport keras\n\n# MNIST dataset\nfrom keras.datasets import mnist\n\n# to build the model\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras import backend as K\n\n# \nfrom matplotlib import pyplot as plt\n\n# (train,test) data split\nfrom sklearn.model_selection import train_test_split\n\n# to calculate accuracy\nimport sklearn.metrics as metrics\n\n# learning rate\nfrom keras.callbacks import LearningRateScheduler\n\n# to create artifical train images\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# global variables\nannealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x, verbose=0)\n","b11164a9":"# Load the data\n(X_train, y_train), (X_test, y_test) = mnist.load_data()","e87d2f5b":"# shape of train, test data\nprint({'train data': X_train.shape, 'test data': X_test.shape})","4fd169c5":"# Images from the train data set\nplt.figure(figsize=(15,10))\nx, y = 10, 5\nfor i in range(50):  \n    plt.subplot(y, x, i+1)\n    plt.imshow(X_train[i].reshape((28,28)),interpolation='nearest')\nplt.show()","8c300e50":"# reshape the input image using 'image_data_format()'\n\nprint(\"Image data format:\", K.image_data_format())\n\nX_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\nX_test  = X_test.reshape(X_test.shape[0], 28, 28, 1)\n\nprint(\"train:\", X_train.shape)\nprint(\"test:\",  X_test.shape)","b24f70d7":"# check the output of output variable 'Y'\nprint('train_output:', y_train)\nprint('test_output:', y_test)","b9df5c90":"# Change the format of output 'Y' using 'keras.utils.to_categorical'\n\ny_train = keras.utils.to_categorical(y_train, 10)\ny_test = keras.utils.to_categorical(y_test, 10)\n\n# print('train_output:', y_train)\n# print('test_output:', y_test)","dd2a5728":"model = [0,0,0]\n\nfor i in range(3):\n    \n    model[i] = Sequential()    \n    model[i].add(Conv2D(32, kernel_size = 5, padding='same', activation = 'relu', input_shape = (28,28,1)))\n    model[i].add(MaxPooling2D())\n\n    if i>0:\n        model[i].add(Conv2D(48, kernel_size = 5, padding='same', activation = 'relu', input_shape = (28,28,1)))\n        model[i].add(MaxPooling2D())    \n\n    if i>1:\n        model[i].add(Conv2D(64, kernel_size = 5, padding='same', activation = 'relu', input_shape = (28,28,1)))\n        model[i].add(MaxPooling2D(padding='same'))\n        \n# Now flatten the model and compile \n\n    model[i].add(Flatten())\n    model[i].add(Dense(256, activation='relu'))\n    model[i].add(Dense(10, activation='softmax'))\n    model[i].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","f65c156c":"# to display the summary of models use \"model[i].summary()\"\"\n\nmodel[2].summary()","7610cc97":"# Split the data\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X_train, y_train, test_size = 0.3)\n\n# fit the model\nhistory = [0] * 3\nnames = [\"1layer\",\"2layers\",\"3layers\"]\nepochs = 20\nfor j in range(3):\n    history[j] = model[j].fit(X_train1,y_train1, callbacks=[annealer], batch_size=100, epochs = epochs, validation_data = (X_test1,y_test1), verbose=0)        \n    print(\"Conv2D {0}: Epochs={1:d}, train accuracy={2:.5f}, test accuracy={3:.5f}\".format(\n        names[j],epochs,max(history[j].history['accuracy']),max(history[j].history['val_accuracy']) ))","ba781d51":"# Display the test accuracy against epoch \nplt.figure(figsize=(15,5))\nfor i in range(3):\n    plt.plot(history[i].history['val_accuracy'])  \n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(names, loc='upper left')","d32fa132":"model = [0,0,0,0]\n\nfor i in range(4):\n    \n    model[i] = Sequential()\n    model[i].add(Conv2D((8*2**i), kernel_size = 5, activation = 'relu', input_shape = (28,28,1)))\n    model[i].add(MaxPooling2D())\n\n    model[i].add(Conv2D((16*2**i), kernel_size = 5, activation = 'relu', input_shape = (28,28,1)))\n    model[i].add(MaxPooling2D())\n\n# Now flatten the model and compile \n\n    model[i].add(Flatten())\n    model[i].add(Dense(256, activation='relu'))\n    model[i].add(Dense(10, activation='softmax'))\n    model[i].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","44d7a6d6":"# to display the summary of models use \"model[i].summary()\"\"\n\nmodel[2].summary()","27ca8a75":"# Split the data\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X_train, y_train, test_size = 0.3)\n\n# fit the model\nhistory = [0] * 4\nnames = [\"(8,16) maps\",\"(16,32) maps\",\"(32,64) maps\", \"(64,128) maps\"]\nepochs = 20\nfor j in range(4):\n    history[j] = model[j].fit(X_train1,y_train1, callbacks=[annealer], batch_size=100, epochs = epochs, validation_data = (X_test1,y_test1), verbose=0)        \n    print(\"Conv2D {0}: Epochs={1:d}, train accuracy={2:.5f}, test accuracy={3:.5f}\".format(\n        names[j],epochs,max(history[j].history['accuracy']),max(history[j].history['val_accuracy']) ))","5d7f1b22":"# Display the test accuracy against epoch \nplt.figure(figsize=(15,5))\nfor i in range(4):\n    plt.plot(history[i].history['val_accuracy'])  \n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(names, loc='upper left')","0b005920":"model = [0]*5\n\nfor i in range(5):\n\n    model[i] = Sequential()\n    model[i].add(Conv2D(32, kernel_size = 5, activation = 'relu', input_shape = (28,28,1)))\n    model[i].add(MaxPooling2D())\n\n    model[i].add(Conv2D(64, kernel_size = 5, activation = 'relu', input_shape = (28,28,1)))\n    model[i].add(MaxPooling2D())\n    model[i].add(Flatten())\n\n  # Now iterate over Dense layer\n    if i >= 0:\n        model[i].add(Dense(32*2**i, activation='relu'))\n\n    model[i].add(Dense(10, activation='softmax'))\n    model[i].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","15eb386e":"# to display the summary of models use \"model[i].summary()\"\"\n\nmodel[4].summary()","276d9645":"# Split the data\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X_train, y_train, test_size = 0.3)\n\n# fit the model\nhistory = [0] * 5\nnames = [\"32 inputs\",\"64 inputs\",\"128 inputs\", \"256 inputs\", \"512 inputs\"]\nepochs = 20\nfor j in range(5):\n    history[j] = model[j].fit(X_train1,y_train1, callbacks=[annealer], batch_size=100, epochs = epochs, validation_data = (X_test1,y_test1), verbose=0)        \n    print(\"Conv2D {0}: Epochs={1:d}, train accuracy={2:.5f}, test accuracy={3:.5f}\".format(\n        names[j],epochs,max(history[j].history['accuracy']),max(history[j].history['val_accuracy']) ))","5c9b0b6a":"model = [0]*5\n\nfor i in range(5):\n    \n    model[i] = Sequential()\n    model[i].add(Conv2D(32,kernel_size=5,activation='relu',input_shape=(28,28,1)))\n    model[i].add(MaxPooling2D())\n    model[i].add(Dropout(i*0.1))\n    model[i].add(Conv2D(64,kernel_size=5,activation='relu'))\n    model[i].add(MaxPooling2D())\n    model[i].add(Dropout(i*0.1))\n    model[i].add(Flatten())\n    model[i].add(Dense(128, activation='relu'))\n    model[i].add(Dropout(i*0.1))\n    model[i].add(Dense(10, activation='softmax'))\n    model[i].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","7d9ecd6c":"# Split the data\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X_train, y_train, test_size = 0.3)\n\n# fit the model\nhistory = [0] * 5\nnames = [\"0% drop\",\"10% drop\",\"20% drop\", \"30% drop\", \"40% drop\"]\nepochs = 20\nfor j in range(5):\n    history[j] = model[j].fit(X_train1,y_train1, callbacks=[annealer], batch_size=100, epochs = epochs, validation_data = (X_test1,y_test1), verbose=0)        \n    print(\"Conv2D {0}: Epochs={1:d}, train accuracy={2:.5f}, test accuracy={3:.5f}\".format(\n        names[j],epochs,max(history[j].history['accuracy']),max(history[j].history['val_accuracy']) ))","527d80b3":"# Display the test accuracy against epoch \nplt.figure(figsize=(15,5))\nfor i in range(5):\n    plt.plot(history[i].history['val_accuracy'])  \n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(names, loc='upper left')","c414e3d9":"# Build the final model\n\nmodel = Sequential()\nmodel.add(Conv2D(32,kernel_size=5,activation='relu',input_shape=(28,28,1)))\nmodel.add(MaxPooling2D())\nmodel.add(Dropout(0.3))\nmodel.add(Conv2D(64,kernel_size=5,activation='relu'))\nmodel.add(MaxPooling2D())\nmodel.add(Dropout(0.3))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","502a8a74":"model.summary()","62c7dc03":"model.fit(X_train, y_train, batch_size = 100,\n          epochs = 20, verbose = 1, validation_data = (X_test, y_test))\n","ed0df731":"# Predict 'X_test'\ny_test_pred = np.argmax(model.predict(X_test), axis=-1)\n\n# Convert categorical values of y_test\ny_actual = np.argmax(y_test, axis=1)\n\n# Accuracy of test data\nprint('Accuracy:', metrics.accuracy_score(y_actual,y_test_pred))","8f8a2d72":"# Lets add 3*3 kernel_size instead of 5*5\nmodel = Sequential()\n\n# Start - Add two 3*3 kernel_size\nmodel.add(Conv2D(32,kernel_size=3,activation='relu',input_shape=(28,28,1)))\nmodel.add(Conv2D(32,kernel_size=3,activation='relu'))\n# End - Add two 3*3 kernel_size\n\nmodel.add(MaxPooling2D())\nmodel.add(Dropout(0.3))\n\n# Start - Add two 3*3 kernel_size\nmodel.add(Conv2D(64,kernel_size=3,activation='relu'))\nmodel.add(Conv2D(64,kernel_size=3,activation='relu'))\n# End - Add two 3*3 kernel_size\n\nmodel.add(MaxPooling2D())\nmodel.add(Dropout(0.3))\n\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","f2cdcdf2":"model.summary()","b85d1462":"model.fit(X_train, y_train, batch_size = 100,\n          epochs = 20, verbose = 1, validation_data = (X_test, y_test))","9ea761ef":"# Predict 'X_test'\ny_test_pred = np.argmax(model.predict(X_test), axis=-1)\n\n# Convert categorical values of y_test\ny_actual = np.argmax(y_test, axis=1)\n\n# Accuracy of test data\nprint('Accuracy:', metrics.accuracy_score(y_actual,y_test_pred))","aa652dda":"model = Sequential()\n\nmodel.add(Conv2D(32,kernel_size=5,activation='relu',input_shape=(28,28,1)))\nmodel.add(Conv2D(32,kernel_size=5, strides = 2, padding = 'same', activation='relu'))\n\n#model.add(MaxPooling2D())\nmodel.add(Dropout(0.3))\n\n \nmodel.add(Conv2D(64,kernel_size=5,activation='relu'))\nmodel.add(Conv2D(64,kernel_size=5, strides = 2, padding = 'same', activation='relu'))\n \n#model.add(MaxPooling2D())\nmodel.add(Dropout(0.3))\n\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","a1089c90":"model.summary()","11f02b6b":"model.fit(X_train, y_train, batch_size = 100,\n          epochs = 20, verbose = 1, validation_data = (X_test, y_test))","26af9ad5":"# Predict 'X_test'\ny_test_pred = np.argmax(model.predict(X_test), axis=-1)\n\n# Convert categorical values of y_test\ny_actual = np.argmax(y_test, axis=1)\n\n# Accuracy of test data\nprint('Accuracy:', metrics.accuracy_score(y_actual,y_test_pred))","683c5925":"# Image data augmentation is used to expand the training dataset in order to improve the performance and ability of the model to generalize. \n\nimage_data_gen = ImageDataGenerator(\n        rotation_range=10,  \n        zoom_range = 0.10,  \n        width_shift_range=0.1, \n        height_shift_range=0.1)\n","0f0776b9":"# Basic Model\n\nmodel = Sequential()\nmodel.add(Conv2D(32,kernel_size=5,activation='relu',input_shape=(28,28,1)))\nmodel.add(MaxPooling2D())\nmodel.add(Dropout(0.3))\nmodel.add(Conv2D(64,kernel_size=5,activation='relu'))\nmodel.add(MaxPooling2D())\nmodel.add(Dropout(0.3))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","56efb3c1":"# Currently i'm using 'above model' for 'Data Augmentation'\n\nmodel.fit_generator(image_data_gen.flow(X_train,y_train, batch_size = 100), epochs = 50, \n    steps_per_epoch = X_train.shape[0]\/\/100, callbacks=[annealer], verbose=0)\n","ecbe9324":"X_train.shape","e2945c20":"# Predict 'X_test'\ny_test_pred = np.argmax(model.predict(X_test), axis=-1)\n\n# Convert categorical values of y_test\ny_actual = np.argmax(y_test, axis=1)\n\n# Accuracy of test data\nprint('Accuracy:', metrics.accuracy_score(y_actual,y_test_pred))","61d692fd":"### How to choose the best CNN architecture?\n1. Selection of Conv2D subsampling layers\n2. Selection of Conv2D feature maps\n3. Selection of Conv2D Dense layer\n4. Selection of Conv2D Dropput layer","f375fc8b":"## Selection of Conv2D subsampling layers","8c6a60a3":"### Test the model by replacing kernel size = 5 with two kernel size = 3","421ccb83":"# Selection of Dense layer","60d83287":"## Conclusion:\n1. ### For each 'combination change' in the basic model, there is an improvement in test accuracy\n2. ### Similarly, we can use all combinations in a single model and improve the test accuracy.\n# And this notebook is inspired from the works of Kaggle grandmaster ('Chris Deotte')\n","54da8181":"## Final model combination\n> two Conv2D subsampling, (32,64) feature maps, 128 inputs, 30% dropout","6e0936dd":"### Test the model with strides = 2 instead of Maxpooling (2,2)","9addd571":"# Selection of %Drop out","366e5228":"  **Considering the image size (28 * 28), we can go for 3 subsampling layers, further layers will make the size of the image very less.**\n\n1. after one layer image size becomes (14 * 14), \n2. after second layer image size becomes (7 * 7), \n3. after third layer image size becomes either (3 * 3) or (4 * 4). ","80d32670":"#### lets test for below feature maps \n>  (8,16), (16,32), (32,64) and (64, 128)","461eac3f":"# Selection of feature maps","bfef5551":"### Test the model with 'Data Augmentation'","8f1607d5":"#### We can add 'Batch normalization' to the above models and check for test accuracy","a8a1ed77":"### Conclusion:\n> 30% dropout gives better test accuracy","9b973825":"### Conclusion: \n> We can go either with 2 layers or 3 layers. lets go with 2 layers in final model","aa2a1bab":"## <center>Lets start with \"finding the best CNN architecture\"<center>\n ","86ef0b56":"# <center> Basic CNN architecture <center>","6993008f":"### Conclusion:\n>  dense layer with 128 inputs gives better test accuracy","a02cd51c":"#### With 'Data augmentation' more train images are generated by (rotation, shifting, ..)\n### For each epoch, 60K different images are generated, so model is trained with (60k * 50) = 3 million images","a2e9be6f":"> please go through below link for understanding 'Data augmentation'\n\n> https:\/\/machinelearningmastery.com\/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks\/","40fd8e2b":"### Table of Contents\n1. Choose the best CNN architecture\n2. Build the model with best CNN architecture\n","e94f0535":"### Conclusion\n> (32,64) and (64,128) maps gives better train\/test accuracy  \n\n> Lets go with 32 maps in layer1 and 64 maps in layer2 in final model","a08dd928":"## Additional ways to improve accuracy\n1. test the model with two (3 * 3) kernal_size instead of (5 * 5)\n2. test the model with strides = 2 instead of maxpooling \n3. test the model with batch normalization\n4. test the model with Data Augmentation","9b21eb8d":"![CNN.PNG](attachment:CNN.PNG) ","7eec7681":"> lets try with (32, 64, 128, 256, 512) dense layers"}}