{"cell_type":{"1bcaac12":"code","f124e2ea":"code","04648913":"code","eee4a320":"code","9e87f696":"code","600c3f65":"code","0db10468":"code","81985bd9":"code","e14b011e":"code","5beaa537":"code","975d53ad":"code","42bc0479":"code","711f324b":"code","8968c80e":"code","ce79eaaf":"markdown","9e2d45f1":"markdown","95974304":"markdown","c256e0b1":"markdown","c93a26c3":"markdown","85b59864":"markdown","ad8e911d":"markdown","15432bed":"markdown","59b20079":"markdown","42a0ed4c":"markdown","4eb09972":"markdown","fac5e6ac":"markdown","0147edcd":"markdown","0326bf53":"markdown","202a8045":"markdown","cc6eeeaf":"markdown","87eb8d48":"markdown","4cffe8eb":"markdown","8d67150d":"markdown"},"source":{"1bcaac12":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\nimport math\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import jaccard_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import mean_squared_error\nfrom scipy import stats\nfrom sklearn.impute import SimpleImputer\nimport warnings \nfrom warnings import simplefilter\n# ignore all types of warnings\npd.options.mode.chained_assignment = None \nsimplefilter(action='ignore', category=UserWarning) \nsimplefilter(action='ignore', category=FutureWarning) \n#warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.width', None)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f124e2ea":"## %%time\ntrain = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv')#,nrows = 1*(10**5))\n#train.shape\n#train = train[train['date'] > 85]\n#train = train[train['weight'] > 0]\n#train['action'] = (train ['resp'] > 0).astype(int)\n#train['action'] =  (train['weight'] < 3) & ((train['resp_1'] > 0 ) | (train['resp_2'] > 0 ) | (train['resp_3'] > 0 )\\\n #                   | (train['resp_4'] > 0) | (train['resp'] > 0 ) ).astype('int')","04648913":"train['resp_1'] = train['resp_1']*train['weight']\ntrain['resp_2'] = train['resp_2']*train['weight']\ntrain['resp_3'] = train['resp_3']*train['weight']\ntrain['resp_4'] = train['resp_4']*train['weight']\ntrain['resp'] = train['resp']*train['weight']","eee4a320":"# train = pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv')\n#train = train.query('date > 85').reset_index(drop = True) \ntrain = train[train['date'] > 85]\n#train = train[train['date'] != 161]\n#train = train[train['date'] != 168]\n#train = train[train['date'] != 270]\n#train = train[train['date'] != 294]\n#train = train[train['date'] != 459]\ntrain = train[train['weight'] != 0]\nfeatures = [c for c in train.columns if \"feature\" in c]\nfeatures.remove('feature_0')\ntrain.fillna(train[features].mean(),inplace=True)\ntrain['action'] = ((train['resp'].values) > 0).astype(int)","9e87f696":"\nf_mean = np.mean(train[features].values,axis=0)\n#fmean = np.mean(train[features].ewm(com=5,axis=0).mean())\n#f_mean = fmean.values\nprint (f_mean.shape)\n#print(f_mean[:5])\n\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n\n#X_train = train.loc[:, train.columns.str.contains('feature')]\nX_train=train[features].values\n#y_train = (train.loc[:, 'action'])\n\ny_train = (np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T)\nprint (y_train[:,4])","600c3f65":"import lightgbm as lgbm\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\n# modeling step \nmodels = [] # list of model , we will train\n#th = 0.50000\n#model_acc = 0\nparams={\"num_leaves\":300,\n       \"max_bin\":450,\n       \"feature_fraction\":0.52,\n       \"bagging_fraction\":0.52,\n       \"objective\":\"binary\",\n       \"learning_rate\": 0.05,\n       \"boosting_type\":\"gbdt\",\n       \"metric\":\"auc\"\n       }\n\nfor i in range(y_train.shape[1]):\n    xtr,xval,ytr,yval = train_test_split(X_train ,y_train[:,i],test_size=0.2,\n                                                 stratify=y_train[:,i],)\n   \n    d_train = lgbm.Dataset(xtr,label=ytr)\n    d_eval = lgbm.Dataset(xval,label=yval,reference=d_train)\n    clf = lgbm.train(params,d_train,valid_sets=[d_train,d_eval],num_boost_round=1000,\\\n    early_stopping_rounds=50,verbose_eval=50)\n    models.append(clf)\n    #break\n    #preds = clf.predict(xval) #np.mean[model.predict(xval) for model in models]\n    #preds = np.where(preds >= th, 1, 0).astype(int)\n    #pred_labels = np.rint(preds)\n    #accuracy = sklearn.metrics.accuracy_score(yval, preds)\n    #print ('Learning_rate {}'.format(lr),accuracy)\n    #if accuracy > model_acc:\n     #   model_acc = accuracy\n      #  Bst_lr = lr\n#print ('Final Accuracy is {} with best learning rate {} '.format(model_acc,Bst_lr))\n","0db10468":"#features.append('date')","81985bd9":"#features","e14b011e":"#train['action'] = train.apply(lambda x: action_value(x['weight']),axis=1)","5beaa537":"# Recall 0.02 & 0.99 For (weight < 1) & (OR Combination with all resp) accuracy: 0.68\n# Recall 0.01 & 0.99 For (weight < 2) & (OR Combination with all resp) accuracy: 0.70\n# Recall 0.59 & 0.89 For (weight < 3) & (OR Combination with all resp) accuracy:0.77 GroupKFold","975d53ad":"# Recall 0.38 and 0.76 for weight < 3 accuracy : 0.56 \n# Recall 0.48 and 0.63 for weight < 2 accuracy : 0.56 \n# Recall 0.63 and 0.49 for weight < 1 accuracy : 0.56 \n# Recall 0.36 and 0.65 for resp    accuracy : 0.51 \n# Recall 0.41 and 0.61 for resp_3  accuracy : 0.51 \n\n","42bc0479":"%%time\nf = np.median\nth = 0.5000\nlst = []\nimport janestreet\nenv = janestreet.make_env()\nfor (test_df, pred_df) in env.iter_test():\n    #if test_df['weight'].item() > 0:\n        #x_tt = test_df.loc[:, features].values\n    x_tt=test_df[features].values\n    if np.isnan(x_tt[:,:].sum()):\n        x_tt[:,:] = np.nan_to_num(x_tt[:,:]) + np.isnan(x_tt[:,:]) * f_mean\n    pred = np.mean([model.predict(x_tt) for model in models],axis=0)\n    pred = f(pred)\n    pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n    lst.append(pred)\n    #else:\n     #   pred_df.action = 0\n    env.predict(pred_df)","711f324b":"print(lst)","8968c80e":"#print('AUC:', round(roc_auc_score(y,ypred),3)) # 0.499 for resp_3 and whole data\n#print ('JS:', round(jaccard_score(y,ypred),3)) # 0.368 same as above","ce79eaaf":"import matplotlib.pyplot as plt\n# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","9e2d45f1":"lst = []\nfor i in range(130):\n    lst.append('feature_{}'.format(i))\nprint (lst)","95974304":"env.predict(sample_prediction_df)","c256e0b1":"from sklearn.metrics import classification_report \nprint (classification_report(y_test, yhat))","c93a26c3":"#f_mean = np.mean(train[features[1:]].values,axis=0)\n\nresp_cols = ['resp', 'resp_1', 'resp_2', 'resp_3', 'resp_4']\n\n#X_train = train.loc[:, train.columns.str.contains('feature')]\n#X_train=train[features].values\n#y_train = (train.loc[:, 'action'])\n\nz_train = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T\nz_train","85b59864":"from sklearn.model_selection import GroupKFold\nfrom sklearn import preprocessing\n#X = train.iloc[:,8:-2]\nX = train[features].values\nprint (X.shape)\nimp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\nX = imp_mean.fit(X).transform(X)\nX = np.asarray(X)\nX = preprocessing.StandardScaler().fit(X).transform(X)\ny =  np.asarray(train[['action']])\ngroups = np.array(train['date'])\ngroup_kfold = GroupKFold(n_splits=5)\nfor train, test in group_kfold.split(X,y,groups):\n    X_train, X_test = X[train], X[test]\n    y_train, y_test = y[train], y[test]\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\nprint ('=====================================')\nprint(\"GroupKFold done\")","ad8e911d":"from sklearn.metrics import classification_report\nprint (classification_report(y_test, yhat))","15432bed":"#opt = tf.keras.optimizers.Adam(learning_rate=0.1)\nadam = Adam(lr=.001,amsgrad=True)\nmodel = keras.Sequential([\n    keras.layers.Flatten(input_shape=(129,)),\n    keras.layers.Dense(80, activation=tf.nn.relu),\n    tf.keras.layers.Dropout(.1, input_shape=(129,)),\n    keras.layers.Dense(64, activation=tf.nn.relu),\n    tf.keras.layers.Dropout(.1, input_shape=(129,)),\n    keras.layers.Dense(1, activation=tf.nn.sigmoid),\n])\n#model.add(keras.layers.Dropout(0.5))\n# compile the model\nmodel.compile(loss='binary_crossentropy', optimizer=adam, metrics=['acc'])\n\n# fit the model\nhistory=model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=256,verbose=1)\n","59b20079":"s = model.predict_classes(X_test)\ns[:20]","42a0ed4c":"y = y_test[:15218]\ny.shape","4eb09972":"%%time\nLR = LogisticRegression(C=0.01, solver='saga',max_iter=1000,\n                                n_jobs=-1, random_state=42).fit(X_train,y_train.ravel()) \nyhat = LR.predict(X_test) \nyhat_prob = LR.predict_proba(X_test)\nfrom sklearn.metrics import jaccard_score\nprint (round(jaccard_score(y_test, yhat),3)) ","fac5e6ac":"features = [c for c in train.columns if \"feature\" in c]\nfeatures.remove('feature_0')\nf_mean = np.mean(train[features].values,axis=0)","0147edcd":"import tensorflow as tf\nfrom tensorflow import keras\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam,SGD,RMSprop\nfrom sklearn.model_selection import train_test_split","0326bf53":"from sklearn.metrics import log_loss\nlog_loss(y_test, yhat_prob)","202a8045":"(test_df, sample_prediction_df) = next(iter_test)\ntest_df","cc6eeeaf":"test_df.iloc[:,2:-1]","87eb8d48":"from sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn import preprocessing\nimport lightgbm as lgbm\nfrom lightgbm import LGBMClassifier\nsk_fold = StratifiedKFold(n_splits=5,random_state=42,shuffle=True)\nresp_cols = ['resp_1','resp_2','resp_3','resp_4','resp']\nmodels = []\njc_score = []\n\nparams={\"num_leaves\":300,\n       \"max_bin\":450,\n       \"feature_fraction\":0.52,\n       \"bagging_fraction\":0.52,\n       \"objective\":\"binary\",\n       \"learning_rate\":0.05,\n       \"boosting_type\":\"gbdt\",\n       \"metric\":\"auc\"\n       }\nfor col in resp_cols:\n    train = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv')#,\\\n                        #nrows = 1*(10**5))\n    train = train[train['date'] > 85]\n    train = train[train['weight'] > 0]\n    train['action'] = (train [[col]] > 0).astype(int)\n    lbl = LabelEncoder()\n    lbl.fit(list(train['action'].values) + list(train['action'].values))\n    train['action'] = lbl.transform(list(train['action'].values))\n    #X = train.iloc[:,8:-2]\n    X = train[features].values\n#print (X.shape)\n    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n    X = imp_mean.fit(X).transform(X)\n    X = np.asarray(X)\n    X = preprocessing.StandardScaler().fit(X).transform(X)\n\n#resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n#y = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T\n    y =  np.asarray(train['action'])\n    #print (\"y\",y[:5])\n    for train, test in sk_fold.split(X,y):\n        X_train, X_test = X[train], X[test]\n        y_train, y_test = y[train], y[test]\n    print ('Train set:', X_train.shape,  y_train.shape)\n    print ('Test set:', X_test.shape,  y_test.shape)\n    print ('=====================================')\n    print(\"StratifiedKFold done\")\n    d_train = lgbm.Dataset(X_train,label=y_train.ravel())\n    d_eval = lgbm.Dataset(X_test,label=y_test,reference=d_train)\n    model = lgbm.train(params,d_train,valid_sets=[d_train,d_eval],num_boost_round=1000,\\\n                    early_stopping_rounds=50,verbose_eval=50)\n    #jc_score.append((jc,col))\n    models.append(model)\n    #break","4cffe8eb":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","8d67150d":"lbl = LabelEncoder()\nlbl.fit(list(train['action'].values) + list(train['action'].values))\ntrain['action'] = lbl.transform(list(train['action'].values))\n    "}}