{"cell_type":{"e217754d":"code","76f92c44":"code","5514b2c8":"code","82bbb6fe":"code","3fb185de":"code","5f2ac394":"code","a4358802":"code","0a0755de":"code","58f4ba76":"code","aa06ba86":"code","89256593":"code","bf33432c":"code","f41a65b4":"code","201e70dc":"code","c202bd54":"code","af6b7666":"code","6aee95de":"code","8bebd4cf":"code","a91e2126":"code","2f29eebf":"code","f96bbbfb":"code","0e16402e":"code","c6bb1210":"code","e66454dc":"code","54b492f3":"code","6d700617":"code","75a73ecb":"code","3fe40766":"code","3a925992":"code","092fec50":"code","6669e302":"code","0d25b96e":"code","2feb5592":"code","7a9b69e1":"code","e3232971":"code","c19b3671":"markdown","04549a56":"markdown"},"source":{"e217754d":"import tensorflow as tf\nprint(tf.version.VERSION)","76f92c44":"!git clone --depth 1 -b v2.4.0 https:\/\/github.com\/tensorflow\/models.git","5514b2c8":"# install requirements to use tensorflow\/models repository\n!pip install -Uqr models\/official\/requirements.txt\n# you may have to restart the runtime afterwards, also ignore any ERRORS popping up at this step","82bbb6fe":"import numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport sys\nsys.path.append('models')\nfrom official.nlp.data import classifier_data_lib\nfrom official.nlp.bert import tokenization\nfrom official.nlp import optimization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()","3fb185de":"print(\"TF Version: \", tf.__version__)\nprint(\"Eager mode: \", tf.executing_eagerly())\nprint(\"Hub version: \", hub.__version__)\nprint(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")","5f2ac394":"# TO LOAD DATA FROM KAGGLE\nimport numpy as np\nimport pandas as pd\n\ntrain_df = pd.read_csv('..\/input\/60k-stack-overflow-questions-with-quality-rate\/train.csv')\nvalid_df = pd.read_csv('..\/input\/60k-stack-overflow-questions-with-quality-rate\/valid.csv')\nprint(train_df.shape)\nprint(valid_df.shape)","a4358802":"train_df.head()","0a0755de":"valid_df.head()","58f4ba76":"train_df = train_df.drop(columns=['Title', 'Tags', 'CreationDate'])","aa06ba86":"valid_df = valid_df.drop(columns=['Title', 'Tags', 'CreationDate'])","89256593":"import re\n\ndef drop(body):\n    body2 = body.replace(\"\\n\", \" \")\n    body3 = body2.replace(\"<p>\", \"\")\n    body4 = body3.replace(\"<\/p>\", \"\")\n    return body4","bf33432c":"train_df['Body'] = train_df['Body'].apply(lambda x: drop(x))\nvalid_df['Body'] = valid_df['Body'].apply(lambda x: drop(x))","f41a65b4":"train_df.Y = pd.Categorical(train_df.Y)\nvalid_df.Y = pd.Categorical(valid_df.Y)","201e70dc":"train_df['Y_cat_code'] = train_df.Y.cat.codes\nvalid_df['Y_cat_code'] = valid_df.Y.cat.codes","c202bd54":"train_df.head(20)","af6b7666":"valid_df.head(20)","6aee95de":"train_df = train_df.drop(['Y'], axis=1)\nvalid_df = valid_df.drop(['Y'], axis=1)","8bebd4cf":"# TRAIN SET \ntrain_df['Body'].apply(lambda x: len(x.split())).plot(kind='hist');\nplt.yscale('log');\nplt.title('Distribution of question text length in words')","a91e2126":"# TRAIN SET\ntrain_df['Body'].apply(lambda x: len(x)).plot(kind='hist');\nplt.yscale('log');\nplt.title('Distribution of question text length in characters')","2f29eebf":"# VALIDATION SET\nvalid_df['Body'].apply(lambda x: len(x.split())).plot(kind='hist');\nplt.yscale('log');\nplt.title('Distribution of question text length in words')","f96bbbfb":"# we want the dataset to be created and processed on the cpu \nwith tf.device('\/cpu:0'):\n    train_data = tf.data.Dataset.from_tensor_slices((train_df['Body'].values, train_df['Y_cat_code'].values))\n    valid_data = tf.data.Dataset.from_tensor_slices((valid_df['Body'].values, valid_df['Y_cat_code'].values))\n    # lets look at 3 samples from train set\n    for text,label in train_data.take(3):\n        print(text)\n        print(label)\n","0e16402e":"print(len(train_data))\nprint(len(valid_data))","c6bb1210":"# Setting some parameters\n\nconfig = {'label_list' : [0, 1, 2], # Label categories\n          'max_seq_length' : 300, # maximum length of (token) input sequences\n          'train_batch_size' : 32,\n          'learning_rate': 2e-5,\n          'epochs':5,\n          'optimizer': 'adam',\n          'dropout': 0.5,\n          'train_samples': len(train_data),\n          'valid_samples': len(valid_data),\n          'train_split':0.1,\n          'valid_split': 0.01\n         }","e66454dc":"# Get BERT layer and tokenizer:\n# All details here: https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/2\n\nbert_layer = hub.KerasLayer('https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/2',trainable=True)\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy() # checks if the bert layer we are using is uncased or not\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","54b492f3":"# This provides a function to convert row to input features and label, \n# this uses the classifier_data_lib which is a class defined in the tensorflow model garden we installed earlier\ndef create_feature(text, label, label_list=config['label_list'], max_seq_length=config['max_seq_length'], tokenizer=tokenizer):\n    \"\"\"\n    converts the datapoint into usable features for BERT using the classifier_data_lib\n\n    Parameters:\n    text: Input text string\n    label: label associated with the text\n    label_list: (list) all possible labels\n    max_seq_length: (int) maximum sequence length set for bert\n    tokenizer: the tokenizer object instantiated by the files in model assets\n\n    Returns:\n    feature.input_ids: The token ids for the input text string\n    feature.input_masks: The padding mask generated \n    feature.segment_ids: essentially here a vector of 0s since classification\n    feature.label_id: the corresponding label id from lable_list [0, 1] here\n\n    \"\"\"\n    # since we only have 1 sentence for classification purpose, text_b is None\n    example = classifier_data_lib.InputExample(guid = None,\n                                            text_a = text.numpy(), \n                                            text_b = text.numpy(), \n                                            label = label.numpy())\n    # since only 1 example, the index=0\n    feature = classifier_data_lib.convert_single_example(0, example, label_list,\n                                    max_seq_length, tokenizer)\n\n    return (feature.input_ids, feature.input_mask, feature.segment_ids, feature.label_id)","6d700617":"def create_feature_map(text, label):\n    \"\"\"\n    A tensorflow function wrapper to apply the transformation on the dataset.\n    Parameters:\n    Text: the input text string.\n    label: the classification ground truth label associated with the input string\n\n    Returns:\n    A tuple of a dictionary and a corresponding label_id with it. The dictionary \n    contains the input_word_ids, input_mask, input_type_ids  \n    \"\"\"\n    input_ids, input_mask, segment_ids, label_id = tf.py_function(create_feature, inp=[text, label], \n                                Tout=[tf.int32, tf.int32, tf.int32, tf.int32])\n    max_seq_length = config['max_seq_length']\n\n    # py_func doesn't set the shape of the returned tensors.\n    input_ids.set_shape([max_seq_length])\n    input_mask.set_shape([max_seq_length])\n    segment_ids.set_shape([max_seq_length])\n    label_id.set_shape([])\n\n    x = {\n        'input_word_ids': input_ids,\n        'input_mask': input_mask,\n        'input_type_ids': segment_ids\n    }\n    return (x, label_id)\n\n    # the final datapoint passed to the model is of the format a dictionary as x and labels.\n    # the dictionary have keys which should obv match","75a73ecb":"# Now we will simply apply the transformation to our train and test datasets\nwith tf.device('\/cpu:0'):\n  # train\n  train_data = (train_data.map(create_feature_map,\n                              num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n                          .shuffle(1000)\n                          .batch(32, drop_remainder=True)\n                          .prefetch(tf.data.experimental.AUTOTUNE))\n\n  # valid\n  valid_data = (valid_data.map(create_feature_map, \n                               num_parallel_calls=tf.data.experimental.AUTOTUNE)\n                          .batch(32, drop_remainder=True)\n                          .prefetch(tf.data.experimental.AUTOTUNE)) ","3fe40766":"# train data spec, we can finally see the input datapoint is now converted to the BERT specific input tensor\ntrain_data.element_spec","3a925992":"# valid data spec\nvalid_data.element_spec","092fec50":"# Building the model, input ---> BERT Layer ---> Classification Head\ndef create_model():\n    \n    input_word_ids = tf.keras.layers.Input(shape=(config['max_seq_length'],), dtype=tf.int32,\n                                       name=\"input_word_ids\")\n    input_mask = tf.keras.layers.Input(shape=(config['max_seq_length'],), dtype=tf.int32,\n                                   name=\"input_mask\")\n    input_type_ids = tf.keras.layers.Input(shape=(config['max_seq_length'],), dtype=tf.int32,\n                                    name=\"input_type_ids\")\n\n\n    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])\n    # for classification we only care about the pooled-output\n    # at this point we can play around with the classification head based on the downstream tasks and its complexity\n\n    drop = tf.keras.layers.Dropout(config['dropout'])(pooled_output)\n    output = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(drop)\n\n    # inputs coming from the function\n    model = tf.keras.Model(\n      inputs={\n        'input_word_ids': input_word_ids,\n        'input_mask': input_mask,\n        'input_type_ids': input_type_ids}, \n      outputs=output)\n\n    return model","6669e302":"# Calling the create model function to get the keras based functional model\nmodel = create_model()","0d25b96e":"# using adam with a lr of 2*(10^-5), loss as binary cross entropy as only 2 classes and similarly binary accuracy\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=config['learning_rate']),\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=[tf.keras.metrics.BinaryAccuracy(),\n                       tf.keras.metrics.PrecisionAtRecall(0.5),\n                       tf.keras.metrics.Precision(),\n                       tf.keras.metrics.Recall()])\nmodel.summary()","2feb5592":"tf.keras.utils.plot_model(model=model, show_shapes=True, dpi=76, )","7a9b69e1":"# Update CONFIG dict with the name of the model.\nconfig['model_name'] = 'BERT_EN_UNCASED'\nprint('Training configuration: ', config)\n","e3232971":"# Train model\n# setting low epochs as It starts to overfit with this limited data, please feel free to change\nepochs = config['epochs']\nhistory = model.fit(train_data,\n                    validation_data=valid_data,\n                    epochs=epochs,\n                    verbose=\"auto\")\nrun.finish()","c19b3671":"We will be using GPU accelerated Kernel for this tutorial as we would require a GPU to fine-tune BERT.\n\n## Prerequisites:\n- Willingness to learn: Growth Mindset is all you need \n- Some basic idea about Tensorflow\/Keras \n- Some Python to follow along with the code ","04549a56":"<h1> Fine-Tune BERT for Text Classification with TensorFlow<\/h1>"}}