{"cell_type":{"4392bb0d":"code","e41111bf":"code","5537fb15":"code","3161da0b":"code","4176c9ff":"code","447beb84":"code","fd308ef7":"code","f925bedf":"code","595c8671":"code","5b38a392":"markdown","ce2e1b9b":"markdown","3802e207":"markdown"},"source":{"4392bb0d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gresearch_crypto\nimport matplotlib.pyplot as plt\nenv = gresearch_crypto.make_env()\nfrom datetime import datetime\nimport gc\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import StratifiedKFold,KFold\nfrom sklearn.model_selection import train_test_split\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e41111bf":"data_dir1 = \"\/kaggle\/input\/training\/\"\nx_train = pd.read_csv(data_dir1+\"x_train.csv\")\ny_train = pd.read_csv(data_dir1+\"y_train.csv\")\ny_test = pd.read_csv(data_dir1+\"y_test.csv\")\nweight_train = pd.read_csv(data_dir1+\"weight_train.csv\")\nweight_test = pd.read_csv(data_dir1+\"weight_test.csv\")","5537fb15":"data_dir2 = \"\/kaggle\/input\/testxx\/\"\nx_test = pd.read_csv(data_dir2+\"x_test.csv\")","3161da0b":"x_train = x_train.drop([\"timestamp\",\"filt\"],axis = 1)\nx_test = x_test.drop([\"timestamp\",\"filt\"],axis = 1)\ny_train = y_train.drop([\"timestamp\"],axis = 1)\ny_test = y_test.drop([\"timestamp\"],axis = 1)\nx_train['Mean'] = x_train[['Open', 'High', 'Low', 'Close']].mean(axis=1)    \nx_train['High\/Mean'] = x_train['High'] \/ x_train['Mean']\nx_train['Low\/Mean'] = x_train['Low'] \/ x_train['Mean']\nx_train['Volume\/Count'] = x_train['Volume'] \/ (x_train['Count'] + 1)\nweight_train = weight_train.drop([\"timestamp\"],axis = 1).values\nweight_test = weight_test.drop([\"timestamp\"],axis = 1).values","4176c9ff":"import lightgbm as lgb","447beb84":"def wmean(x, w):\n    return np.sum(x * w) \/ np.sum(w)\n\ndef wcov(x, y, w):\n    return np.sum(w * (x - wmean(x, w)) * (y - wmean(y, w))) \/ np.sum(w)\n\ndef wcorr(x, y, w):\n    return wcov(x, y, w) \/ np.sqrt(wcov(x, x, w) * wcov(y, y, w))\n\ndef eval_wcorr(preds, dataset):\n    y =dataset.get_label()    \n    w = dataset.get_weight()\n    return 'eval_wcorr', wcorr(preds, y, w), True\n\nparams = {'n_estimators': 500,\n        'objective': 'regression_l1',  'metric': 'None',\n        'boosting_type': 'gbdt',\n        'max_depth': -1,\n        'learning_rate': 0.001,\n        'subsample': 0.4,\n        'subsample_freq': 4,\n        'feature_fraction': 0.4,\n        'lambda_l1': 1,\n        'lambda_l2': 1,\n        'seed': 123,\n        'verbose': -1,\n        }\npred = []\nscore=[]\nevals_result = {}\nmodels = []\ntest_dataset = lgb.Dataset(x_test, y_test,weight = weight_test)\nfor idx_t,idx_v in KFold(n_splits = 3, shuffle = True, random_state = 2).split(x_train):\n    X_t,y_t,X_v,y_v = x_train.iloc[idx_t],y_train.iloc[idx_t],x_train.iloc[idx_v],y_train.iloc[idx_v]\n    weight_t,weight_v = weight_train[idx_t],weight_train[idx_v]\n    train_dataset = lgb.Dataset(X_t, y_t,weight = weight_t)\n    val_dataset = lgb.Dataset(X_v, y_v,weight =weight_v)    \n    model = lgb.train(params,\n                          #early_stopping_rounds=1000,\n                          verbose_eval = 100,\n                          feval=eval_wcorr,\n                          train_set = train_dataset, \n                          valid_sets = [val_dataset],\n                          evals_result = evals_result \n                         )\n    models.append(model)\n    #preds2 = model.predict(x_test)\n    #sc = eval_wcorr(preds2, test_dataset)\n    #score = score.append(sc)\n    ","fd308ef7":"#preds2.shape","f925bedf":"del x_train\ndel x_test\ndel train_dataset\ndel test_dataset\ndel val_dataset\ndel y_train\ndel y_test\nimport gc\ngc.collect()","595c8671":"iter_test = env.iter_test()# an iterator which loops over the test set and sample submission\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df['change'] = test_df['Close']\/test_df['Open']\n    test_df['variation'] = test_df['High']\/test_df['Low']\n    test_df['change_diff'] = test_df['Close'] - test_df['Open']\n    test_df['variation_diff'] = test_df['High'] - test_df['Low']\n    def hlco_ratio(df): return (df['High'] - df['Low'])\/(df['Close']-df['Open'])\n    def upper_shadow(df): return df['High'] - np.maximum(df['Close'], df['Open'])\n    def lower_shadow(df): return np.minimum(df['Close'], df['Open']) - df['Low']\n    test_df['Upper_Shadow'] = upper_shadow(test_df)\n    test_df['Lower_Shadow'] = lower_shadow(test_df)\n    test_df['hlco_ration'] = hlco_ratio(test_df)\n    test_df['Mean'] = test_df[['Open', 'High', 'Low', 'Close']].mean(axis=1)    \n    test_df['High\/Mean'] = test_df['High'] \/ test_df['Mean']\n    test_df['Low\/Mean'] = test_df['Low'] \/ test_df['Mean']\n    test_df['Volume\/Count'] = test_df['Volume'] \/ (test_df['Count'] + 1)\n\n    preds2 = []\n    for i in range(3):\n        model = models[i]\n        preds = model.predict(test_df.drop([\"timestamp\",\"row_id\"],axis=1))\n        preds2.append(preds)\n    sample_prediction_df['Target'] = np.mean(np.column_stack(preds2),axis=1)  # make your predictions here\n    env.predict(sample_prediction_df) ","5b38a392":"Here I define the metrics as per rules, but in practice implementing this on test data took up too much space.","ce2e1b9b":"Dropping unnecessary columns","3802e207":"# Model build and submission\n\nThe steps for data creation can be found here: \n\n[https:\/\/www.kaggle.com\/craniket\/g-research-data-creation](http:\/\/).\n\nHere I build a basic LGBM model and submit the predictions."}}