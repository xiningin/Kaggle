{"cell_type":{"641a6e49":"code","f0694d1f":"code","f500abb2":"code","2f57bc93":"code","a8a6d8ee":"code","74d460f1":"code","2b56365a":"markdown","0d64e1a8":"markdown","2e683fee":"markdown","6373e77f":"markdown"},"source":{"641a6e49":"%%capture\npip install torchfile","f0694d1f":"import torchfile\nimport torch.nn as nn\nimport torch\nimport os","f500abb2":"def Conv(in_channels, out_channels, kerner_size=3, stride=1, padding=1):\n    out_channels = int(out_channels)\n    in_channels = int(in_channels)\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kerner_size, stride, padding, bias=False),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(True),\n    )","2f57bc93":"class DLDLv2(nn.Module):\n    def __init__(self, max_age=101, c=0.5):\n        super(DLDLv2, self).__init__()\n        self.conv1 = Conv(3, 64*c)\n        self.conv2 = Conv(64*c, 64*c)\n        self.pool1 = nn.MaxPool2d(2, 2)\n        self.conv3 = Conv(64*c, 128*c)\n        self.conv4 = Conv(128*c, 128*c)\n        self.pool2 = nn.MaxPool2d(2, 2)\n        self.conv5 = Conv(128*c, 256*c)\n        self.conv6 = Conv(256*c, 256*c)\n        self.conv7 = Conv(256*c, 256*c)\n        self.pool3 = nn.MaxPool2d(2, 2)\n        self.conv8 = Conv(256*c, 512*c)\n        self.conv9 = Conv(512*c, 512*c)\n        self.conv10 = Conv(512*c, 512*c)\n        self.pool4 = nn.MaxPool2d(2, 2)\n        self.conv11 = Conv(512*c, 512*c)\n        self.conv12 = Conv(512*c, 512*c)\n        self.conv13 = Conv(512*c, 512*c)\n        \n        self.HP = nn.Sequential(\n            nn.MaxPool2d(2, 2),\n            nn.AvgPool2d(kernel_size=7, stride=1)\n        )\n        \n        self.fc1 = nn.Sequential(\n            nn.Linear(int(512*c), max_age),\n            nn.Softmax(dim = 1)\n        )\n                \n        self.ages = torch.tensor(list(range(max_age+1))[1:]).t().float()\n        self.device = \"cpu\"\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.pool1(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.pool2(x)\n        x = self.conv5(x)\n        x = self.conv6(x)\n        x = self.conv7(x)\n        x = self.pool3(x)\n        x = self.conv8(x)\n        x = self.conv9(x)\n        x = self.conv10(x)\n        x = self.pool4(x)\n        x = self.conv11(x)\n        x = self.conv12(x)\n        x = self.conv13(x) \n        x = self.HP(x)\n        x = x.view((x.size(0), -1))\n        x = self.fc1(x.view((x.size(0), -1)))\n        \n        return x\n    \n    def to(self, device):\n        module = super(DLDLv2, self).to(device)\n        module.ages = self.ages.to(device)\n        self.device = device\n        return module\n\n    #train model on a batch\n    def train_batch(self, x, y):\n        x, y  = x.to(device), y.to(device)\n        b_x, b_y = Variable(x), Variable(y)\n        \n        outputs = self.forward(b_x)\n        age = torch.matmul(outputs,self.ages)\n        \n        return custom_loss(outputs, age, b_y)\n    \n    #predict age of a batch\n    def predict_age(self, x):\n        x = x.to(device)\n        with torch.no_grad():\n            outputs = self.forward(x)\n            \n        return torch.matmul(outputs,self.ages)","a8a6d8ee":"def convert_and_save_model(path):\n    #init model\n    model = DLDLv2(101, 0.5)\n    \n    #load parameters from file\n    model_params = torchfile.load(path)\n\n    #load weights in correct layers\n    model.conv1[0].weight = nn.Parameter(torch.Tensor(model_params.modules[0].weight))\n    model.conv1[1].weight = nn.Parameter(torch.Tensor(model_params.modules[1].weight))\n    model.conv1[1].bias = nn.Parameter(torch.Tensor(model_params.modules[1].bias)) \n        \n    model.conv2[0].weight = nn.Parameter(torch.Tensor(model_params.modules[3].weight))\n    model.conv2[1].weight = nn.Parameter(torch.Tensor(model_params.modules[4].weight))\n    model.conv2[1].bias = nn.Parameter(torch.Tensor(model_params.modules[4].bias))\n    \n    model.conv3[0].weight = nn.Parameter(torch.Tensor(model_params.modules[7].weight))\n    model.conv3[1].weight = nn.Parameter(torch.Tensor(model_params.modules[8].weight))\n    model.conv3[1].bias = nn.Parameter(torch.Tensor(model_params.modules[8].bias))\n       \n    model.conv4[0].weight = nn.Parameter(torch.Tensor(model_params.modules[10].weight))\n    model.conv4[1].weight = nn.Parameter(torch.Tensor(model_params.modules[11].weight))\n    model.conv4[1].bias = nn.Parameter(torch.Tensor(model_params.modules[11].bias))  \n    \n    model.conv5[0].weight = nn.Parameter(torch.Tensor(model_params.modules[14].weight))\n    model.conv5[1].weight = nn.Parameter(torch.Tensor(model_params.modules[15].weight))\n    model.conv5[1].bias = nn.Parameter(torch.Tensor(model_params.modules[15].bias))\n    \n    model.conv6[0].weight = nn.Parameter(torch.Tensor(model_params.modules[17].weight))\n    model.conv6[1].weight = nn.Parameter(torch.Tensor(model_params.modules[18].weight))\n    model.conv6[1].bias = nn.Parameter(torch.Tensor(model_params.modules[18].bias))\n    \n    model.conv7[0].weight = nn.Parameter(torch.Tensor(model_params.modules[20].weight))\n    model.conv7[1].weight = nn.Parameter(torch.Tensor(model_params.modules[21].weight))\n    model.conv7[1].bias = nn.Parameter(torch.Tensor(model_params.modules[21].bias))\n    \n    model.conv8[0].weight = nn.Parameter(torch.Tensor(model_params.modules[24].weight))\n    model.conv8[1].weight = nn.Parameter(torch.Tensor(model_params.modules[25].weight))\n    model.conv8[1].bias = nn.Parameter(torch.Tensor(model_params.modules[25].bias))\n    \n    model.conv9[0].weight = nn.Parameter(torch.Tensor(model_params.modules[27].weight))\n    model.conv9[1].weight = nn.Parameter(torch.Tensor(model_params.modules[28].weight))\n    model.conv9[1].bias = nn.Parameter(torch.Tensor(model_params.modules[28].bias))\n    \n    model.conv10[0].weight = nn.Parameter(torch.Tensor(model_params.modules[30].weight))\n    model.conv10[1].weight = nn.Parameter(torch.Tensor(model_params.modules[31].weight))\n    model.conv10[1].bias = nn.Parameter(torch.Tensor(model_params.modules[31].bias))\n    \n    model.conv11[0].weight = nn.Parameter(torch.Tensor(model_params.modules[34].weight))\n    model.conv11[1].weight = nn.Parameter(torch.Tensor(model_params.modules[35].weight))\n    model.conv11[1].bias = nn.Parameter(torch.Tensor(model_params.modules[35].bias))\n    \n    model.conv12[0].weight = nn.Parameter(torch.Tensor(model_params.modules[37].weight))\n    model.conv12[1].weight = nn.Parameter(torch.Tensor(model_params.modules[38].weight))\n    model.conv12[1].bias = nn.Parameter(torch.Tensor(model_params.modules[38].bias))\n    \n    model.conv13[0].weight = nn.Parameter(torch.Tensor(model_params.modules[40].weight))\n    model.conv13[1].weight = nn.Parameter(torch.Tensor(model_params.modules[41].weight))\n    model.conv13[1].bias = nn.Parameter(torch.Tensor(model_params.modules[41].bias))\n    \n    model.fc1[0].weight = nn.Parameter(torch.Tensor(model_params.modules[46].weight))\n    model.fc1[0].bias = nn.Parameter(torch.Tensor(model_params.modules[46].bias))\n\n    #save model\n    filename = path.split(\"\/\")[-1]\n    filepath = \"\/kaggle\/working\/\"+filename.split(\".\")[0]+\".pt\"\n    torch.save(model.state_dict(), filepath)","74d460f1":"path = \"\/kaggle\/input\/dldlv2-trainedthinmodels\/\"\nfor file in os.listdir(path):\n    convert_and_save_model(path+file)","2b56365a":"# import modules","0d64e1a8":"# define model","2e683fee":"# convert all models","6373e77f":"# define function for converting to torch model"}}