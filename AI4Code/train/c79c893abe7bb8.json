{"cell_type":{"ba9c5af7":"code","2e8428d6":"code","0f944ef8":"code","d98ed798":"code","1a1a443c":"code","81414602":"code","b6e5f923":"code","364078ea":"code","c08f1519":"code","05ad8c86":"code","1b684e55":"code","590da48e":"code","68fed6f1":"code","bd4c5b2c":"code","e64ae0ff":"code","213db403":"code","e56fd965":"code","170d7a8b":"code","742e1d73":"code","24af9bf7":"code","d0e2af86":"code","56c2e7b7":"code","242c8c47":"code","d8c65426":"code","f1f37f06":"code","d6fefa49":"code","edd6f416":"code","4e4b1586":"code","c4b9c622":"code","819676ef":"code","1055411a":"code","21e3e32e":"code","af436077":"code","877d03ca":"code","1b35bc99":"code","40b551e5":"markdown","3748f0e5":"markdown","34870d81":"markdown","b4939e12":"markdown","5e80348f":"markdown","da895a09":"markdown","de02550a":"markdown","484af8f2":"markdown","3e62c115":"markdown","4f16ed41":"markdown","fadf38f2":"markdown","582c688b":"markdown","5a0219b2":"markdown","86dcc7e2":"markdown","73643ea6":"markdown","633151bc":"markdown","ecd80604":"markdown","11088549":"markdown","1d2d9256":"markdown","e6e5f45f":"markdown","064ed7fa":"markdown","581ee4d6":"markdown","98617aaf":"markdown","963eeeb5":"markdown","08f7a809":"markdown","572c3c0f":"markdown","865e260e":"markdown","105610dc":"markdown","29b64429":"markdown","dbd35765":"markdown"},"source":{"ba9c5af7":"import numpy as np \nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","2e8428d6":"# save the Id column for prediction dataframe\ntest_ID = test['Id']\n\n# drop the Id columns since Id's are not correlated with house prices\ntrain.drop('Id', axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)","0f944ef8":"# visualize outliers in a scatter plot\nfig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","d98ed798":"# delete outliers: large GrLivArea but low price\ntrain = train.drop(train[(train['GrLivArea'] > 4000) & (train['SalePrice'] < 300000)].index)\n\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","1a1a443c":"sns.distplot(train['SalePrice'] , fit=norm);\n\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n# plot SalePrice distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n# QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","81414602":"# use np.log1p which applies log(1+x) to all elements of the column\ntrain['SalePrice'] = np.log1p(train['SalePrice'])\n\n# plot the two graphs again\nsns.distplot(train['SalePrice'] , fit=norm);\n\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution after log-transformation')\n\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","b6e5f923":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat([train, test], sort=False).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint('all_data size is : {}'.format(all_data.shape))","364078ea":"# visualize correlation among variables in a heatmap\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","c08f1519":"k = 10\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.4)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","05ad8c86":"all_data.drop(['GarageArea'], axis=1, inplace=True)\nprint('Number of feature columns : {}'.format(len(all_data.columns)))","1b684e55":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index)\nprint('There are {} columns with missing values: {}'.format(len(all_data_na.index), all_data_na.index.values))","590da48e":"all_data[all_data_na.index].select_dtypes(include=[np.number]).columns.tolist()","68fed6f1":"# numerical\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtUnfSF',\n            'GarageCars', 'GarageYrBlt', 'MasVnrArea', 'TotalBsmtSF'):\n    all_data[col] = all_data[col].fillna(0)\n\n# group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","bd4c5b2c":"all_data[all_data_na.index].select_dtypes(include=[np.object]).columns.tolist()","e64ae0ff":"# categorical\nfor col in ('Alley', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'BsmtQual', 'Fence', 'FireplaceQu', 'GarageCond', 'GarageFinish', 'GarageQual',\n       'GarageType', 'MasVnrType', 'MiscFeature', 'PoolQC'):\n    all_data[col] = all_data[col].fillna('None')\n\nfor col in ('Electrical', 'Exterior1st', 'Exterior2nd', 'KitchenQual', 'MSZoning', 'SaleType'):\n    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n    \n# All records are \"AllPub\", except for one \"NoSeWa\" and 2 NA, Utilities won't help in predictive modelling.\nall_data = all_data.drop(['Utilities'], axis=1)\nall_data['Functional'] = all_data['Functional'].fillna('Typ')","213db403":"# check if there are still missing values\nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nall_data_na","e56fd965":"# MSSubClass: The building class should be categorical\nall_data['MSSubClass'] = all_data['MSSubClass'].astype(str)\n\n# convert OverallCond to a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n# Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","170d7a8b":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\nprint('Shape of all_data: {}'.format(all_data.shape))","742e1d73":"# add total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","24af9bf7":"# filter numerical features\nnumeric_feats = all_data.dtypes[all_data.dtypes != 'object'].index\n\n# check the skewness of numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness = skewness[abs(skewness) > 0.75]\nprint('There are {} skewed numerical features to Box Cox transform'.format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    all_data[feat] = boxcox1p(all_data[feat], lam)    ","d0e2af86":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","56c2e7b7":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","242c8c47":"from sklearn.linear_model import ElasticNet, Lasso\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","d8c65426":"n_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring='neg_mean_squared_error', cv = kf))\n    return(rmse)","f1f37f06":"from sklearn.model_selection import GridSearchCV\nlasso = Lasso()\nlasso_parameters = {'alpha': [1e-4, 2e-4, 5e-4, 7e-4, 9e-4, 1e-3, 2e-3, 5e-3, 7e-3, 9e-3, 1e-2, 2e-2, 5e-2]}\nlasso_regressor = GridSearchCV(lasso, lasso_parameters, scoring='neg_mean_squared_error', cv=5)\nlasso_regressor.fit(train, y_train)\nprint('Best parameters for lasso regression: {}'.format(lasso_regressor.best_params_))","d6fefa49":"ENet = ElasticNet()\nENet_parameters = {'alpha': [1e-3, 2e-3, 5e-3, 7e-3, 9e3, 1e-2, 2e-2, 5e-2],\n                  'l1_ratio': [.1, .2, .3, .4, .5, .6, .7, .8, .9],\n                  'random_state': range(0, 10)}\nENet_regressor = GridSearchCV(ENet, ENet_parameters, scoring='neg_mean_squared_error', cv=5)\nENet_regressor.fit(train, y_train)\nprint('Best parameters for elastic net: {}'.format(ENet_regressor.best_params_))","edd6f416":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.001, l1_ratio=.4, random_state=0))\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state=7, nthread=-1)\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin=55, bagging_fraction=0.8,\n                              bagging_freq=5, feature_fraction=0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf=6, min_sum_hessian_in_leaf=11)","4e4b1586":"score = rmsle_cv(lasso)\nprint('Lasso score: {:.4f} ({:.4f})'.format(score.mean(), score.std()))\n\nscore = rmsle_cv(ENet)\nprint('ElasticNet score: {:.4f} ({:.4f})'.format(score.mean(), score.std()))\n\nscore = rmsle_cv(KRR)\nprint('Kernel Ridge score: {:.4f} ({:.4f})'.format(score.mean(), score.std()))\n\nscore = rmsle_cv(GBoost)\nprint('Gradient Boosting score: {:.4f} ({:.4f})'.format(score.mean(), score.std()))\n\nscore = rmsle_cv(model_xgb)\nprint('Xgboost score: {:.4f} ({:.4f})'.format(score.mean(), score.std()))\n\nscore = rmsle_cv(model_lgb)\nprint('LGBM score: {:.4f} ({:.4f})'.format(score.mean(), score.std()))","c4b9c622":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                # train cloned base models\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                # predict the untouched holdout with the trained first stage model\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # train the cloned meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    # do the predictions of all base models on the test data and use the averaged predictions as \n    # meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","819676ef":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, model_xgb),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint('Stacking Averaged models score: {:.4f} ({:.4f})'.format(score.mean(), score.std()))","1055411a":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","21e3e32e":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","af436077":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))\n\nmodel_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train, stacked_train_pred*0.70 + xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","877d03ca":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","1b35bc99":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('house_prices_predictions.csv',index=False)","40b551e5":"We use multiple regression models [Lasso](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso), [Elastic Net](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.ElasticNet.html), [Kernel Ridge](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge), [Gradient Boosting](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor), [XGBoost](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html) and [LightGBM](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html). To find out the best parameters for the models, we can use [GridSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV).","3748f0e5":"## deal with missing data","34870d81":"# Modelling","b4939e12":"Generally, if a variable is numerical: we replace null with 0 or mean\/median. If it is categorical, we replace null with none or mode. We can use select_dtypes to return all numerical features.","5e80348f":"## output predictions","da895a09":"## remove highly correlated features","de02550a":"We need to apply `np.expm1` (applies exp(x)-1 to all elements) to the predictions because in the beginning we have `np.log1p(train['SalePrice'])`. $\\exp(\\log(1+x))-1 = x$ is the original result.","484af8f2":"## log-transform target variable","3e62c115":"## combine train and test","4f16ed41":"BsmtFinSF1, BsmtFinSF2, BsmtFullBath, BsmtHalfBath, BsmtUnfSF, GarageCars, GarageYrBlt, LotFrontage, MasVnrArea, TotalBsmtSF are numerical. LotFrontage (Linear feet of street connected to property) is non-zero. We fill the rest with 0.","fadf38f2":"# Features engineering","582c688b":"## label encode some categorical variables","5a0219b2":"## define a cross validation strategy","86dcc7e2":"# Data processing","73643ea6":"## convert numerical variables to categorical variables","633151bc":"## remove ID column","ecd80604":"## analyze target variable: SalePrice","11088549":"## Summary\n\n1. Data preprocessing\n    * remove ID column: `df.drop('column', axis=1, inplace=True)`\n    * remove outliers: Visualize outliers by scatter plots\n    * analyze target variable: `sns.distplot(df['column'], fit=norm)` and QQ-plot `stats.probplot(df['column'], plot=plt)`\n    * log-transform target variable if it is not normal\n1. Features engineering\n    * combine train and test `pd.concat([train, test], sort=False).reset_index(drop=True)`\n    * remove highly correlated features\n    * deal with missing data: numerical -> 0 or mean\/median, categorical -> none or mode\n    * convert numerical variables to categorical variables: `df['numerical_feature'].astype(str)`\n    * label encode categorical variables\n    * create new feature(s)\n    * transform skewed features\n    * convert categorical variables to dummy variables: `pd.get_dummies(df)`\n1. Modelling\n    * define a cross validation strategy\n    * set up base models with hyperparameter optimization using GridSearchCV\n    * evaluate base models\n    * stack averaged models with a meta model\n    * output predictions as csv:\n    ```\n    df = pd.DataFrame({ 'column': values, 'column2', values2 })\n    df.to_csv('name.csv', index=False)\n    ```","1d2d9256":"## stack averaged models with a meta model","e6e5f45f":"## convert categorical variables to dummy variables.","064ed7fa":"For the other model parameters, I referred to this [kernel](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard).","581ee4d6":"# House Prices Prediction","98617aaf":"The target variable is right skewed. Since linear models require variables to be normally distributed, we can transform this variable and make it more normally distributed.","963eeeb5":"## set up base models with hyperparameter optimization (using GridSearchCV)","08f7a809":"## evaluate base models","572c3c0f":"GarageCars and GarageArea are highly correlated variables, which makes sense since the number of cars that fit into the garage is proportional to the garage area. Therefore, we can remove GarageArea and keep GarageCars since GarageCars has a higher correlation with SalePrice (0.64 vs 0.62).","865e260e":"## create new feature(s)","105610dc":"## transform skewed features","29b64429":"## remove outliers","dbd35765":"# References\n* [Comprehensive data exploration with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python)\n* [Stacked Regressions : Top 4% on LeaderBoard](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard)"}}