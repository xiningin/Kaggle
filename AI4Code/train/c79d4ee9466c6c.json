{"cell_type":{"eeb603be":"code","8578055f":"code","757aa596":"code","03bbf3f5":"code","eb99276c":"code","f9fccf9f":"code","73a83abc":"code","2726b910":"code","3665b7e0":"code","0e70fa4a":"code","69602f24":"code","a09984b7":"code","0000478c":"code","22740cc4":"code","c9d45743":"code","c9594998":"code","6bd81576":"code","b007993c":"code","373527f3":"markdown","cd259153":"markdown","19b8d36b":"markdown","0475eef3":"markdown","8853b602":"markdown","b1c15e47":"markdown","f79b37e6":"markdown","5bc965cf":"markdown","fe365b49":"markdown","0fa72bf0":"markdown"},"source":{"eeb603be":"!pip install 'kaggle-environments==0.1.6' > \/dev\/null 2>&1","8578055f":"import numpy as np\nimport gym\nimport random\nimport matplotlib.pyplot as plt\nfrom random import choice\nfrom tqdm.notebook import tqdm\nfrom kaggle_environments import evaluate, make","757aa596":"class ConnectX(gym.Env):\n    def __init__(self, switch_prob=0.5):\n        self.env = make('connectx', debug=True)\n        self.pair = [None, 'negamax']  # Train our model against negamax\n        self.trainer = self.env.train(self.pair)\n        self.switch_prob = switch_prob\n        \n        # Define required gym fields (examples):\n        config = self.env.configuration\n        self.action_space = gym.spaces.Discrete(config.columns)\n        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n\n    def switch_trainer(self):\n        self.pair = self.pair[::-1]\n        self.trainer = self.env.train(self.pair)\n\n    def step(self, action):\n        return self.trainer.step(action)\n    \n    def reset(self):\n        if random.uniform(0, 1) < self.switch_prob:\n            self.switch_trainer()\n        return self.trainer.reset()\n    \n    def render(self, **kwargs):\n        return self.env.render(**kwargs)\n\n\nclass QTable:\n    def __init__(self, action_space):\n        self.table = dict()\n        self.action_space = action_space\n        \n    def add_item(self, state_key):\n        self.table[state_key] = list(np.zeros(self.action_space.n))\n        \n    def __call__(self, state):\n        board = state.board[:] # Get a copy\n        board.append(state.mark)\n        state_key = np.array(board).astype(str)\n        state_key = hex(int(''.join(state_key), 3))[2:]\n        if state_key not in self.table.keys():\n            self.add_item(state_key)\n        \n        return self.table[state_key]","03bbf3f5":"env = ConnectX()","eb99276c":"alpha = 0.1\ngamma = 0.6\nepsilon = 0.99    # probability that the learner will take random action instead of based \n                  # on Q value. We decay this epsilon over time.\nmin_epsilon = 0.1\n\nepisodes = 3000  # number of games the learner trained on\n\nalpha_decay_step = 1000\nalpha_decay_rate = 0.9\nepsilon_decay_rate = 0.9999","f9fccf9f":"%%time\nq_table = QTable(env.action_space)\n\nall_epochs = []\nall_total_rewards = []\nall_avg_rewards = [] # Last 100 steps\nall_qtable_rows = []\nall_epsilons = []\n\nfor i in tqdm(range(episodes)):\n    state = env.reset()\n\n    epsilon = max(min_epsilon, epsilon * epsilon_decay_rate)\n    epochs, total_rewards = 0, 0\n    done = False\n    \n    while not done:\n        # get the Q value for each possible action given the state (board)\n        row = q_table(state)\n        if random.uniform(0, 1) < epsilon:\n            # take random action with the probability epsilon\n            action = choice([c for c in range(env.action_space.n) if state.board[c] == 0])\n        else:\n            selected_items = [row[j] if state.board[j] == 0 else -1e7 \n                              for j in range(env.action_space.n)]\n            action = int(np.argmax(selected_items))\n\n        next_state, result, done, info = env.step(action)\n\n        # Compute effective reward\n        if done:\n            if result == 1: # Won\n                reward = 20\n            elif result == 0: # Lost\n                reward = -20\n            else: # Draw\n                reward = 10\n        else:\n            reward = -0.05 # Try to prevent the agent from taking a long move\n\n        old_value = row[action]\n        next_max = np.max(q_table(next_state))\n        \n        # Update Q-value to new value\n        q_table(state)[action] = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n\n        state = next_state\n        epochs += 1\n        total_rewards += reward\n\n\n    all_epochs.append(epochs)\n    all_total_rewards.append(total_rewards)\n    avg_rewards = np.mean(all_total_rewards[max(0, i-100):(i+1)])\n    all_avg_rewards.append(avg_rewards)\n    all_qtable_rows.append(len(q_table.table))\n    all_epsilons.append(epsilon)\n\n    if (i+1) % alpha_decay_step == 0:\n        alpha *= alpha_decay_rate","73a83abc":"len(q_table.table)","2726b910":"# for k in q_table.table.keys():\n#     print('State:', k)\n#     print('Action-Value:', list(q_table.table[k]), '\\n')","3665b7e0":"# plt.plot(all_epochs)\n# plt.xlabel('Episode')\n# plt.ylabel('Num of steps')\n# plt.show()","0e70fa4a":"# plt.plot(all_total_rewards)\n# plt.xlabel('Episode')\n# plt.ylabel('Total rewards')\n# plt.show()","69602f24":"plt.plot(all_avg_rewards)\nplt.xlabel('Episode')\nplt.ylabel('Avg rewards (100)')\nplt.show()","a09984b7":"plt.plot(all_qtable_rows)\nplt.xlabel('Episode')\nplt.ylabel('Explored states')\nplt.show()","0000478c":"plt.plot(all_epsilons)\nplt.xlabel('Episode')\nplt.ylabel('Epsilon')\nplt.show()","22740cc4":"# Extract the actions for each state\ntmp_dict_q_table = q_table.table.copy()\ndict_q_table = dict()\n\nfor k in tmp_dict_q_table:\n    if np.count_nonzero(tmp_dict_q_table[k]) > 0:\n        dict_q_table[k] = int(np.argmax(tmp_dict_q_table[k]))","c9d45743":"my_agent_str = (\"\"\"\nfrom random import choice\n\nq_table = \"\"\" + str(dict_q_table).replace(' ', '') +\n\"\"\"\n\ndef my_agent(observation, configuration):\n    board = observation.board[:]\n    board.append(observation.mark)\n    state_key = list(map(str, board))\n    state_key = hex(int(''.join(state_key), 3))[2:]\n\n    if state_key not in q_table:\n        return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n\n    action = q_table[state_key]\n\n    if observation.board[action] != 0:\n        return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n\n    return action\n\"\"\"\n)","c9594998":"with open('submission.py', 'w') as f:\n    f.write(my_agent_str)","6bd81576":"from submission import my_agent","b007993c":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) \/ sum(r[0] + r[1] for r in rewards)\n\n# Run multiple episodes to estimate agent's performance.\nprint(\"My Agent vs Random Agent:\",\n      mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs Negamax Agent:\",\n      mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))","373527f3":"<a class=\"anchor\" id=\"evaluate_the_agent\"><\/a>\n# Evaluate the agent\n[Back to Table of Contents](#ToC)","cd259153":"<a class=\"anchor\" id=\"install_libraries\"><\/a>\n# Install libraries\n[Back to Table of Contents](#ToC)","19b8d36b":"# About Q-Learning algorithm\n> \"Q-learning is a model-free reinforcement learning algorithm. The goal of Q-learning is to learn a policy, which tells an agent what action to take under what circumstances. It does not require a model (hence the connotation \"model-free\") of the environment, and it can handle problems with stochastic transitions and rewards, without requiring adaptations.\"\n[*wiki*](https:\/\/en.wikipedia.org\/wiki\/Q-learning)","0475eef3":"<a class=\"anchor\" id=\"create_an_agent\"><\/a>\n# Create an Agent\n[Back to Table of Contents](#ToC)","8853b602":"<a class=\"anchor\" id=\"train_the_agent\"><\/a>\n# Train the agent\n[Back to Table of Contents](#ToC)","b1c15e47":"<a class=\"anchor\" id=\"create_connectx_environment\"><\/a>\n# Create ConnectX environment\n[Back to Table of Contents](#ToC)","f79b37e6":"<a class=\"anchor\" id=\"configure_hyper_parameters\"><\/a>\n# Configure hyper-parameters\n[Back to Table of Contents](#ToC)","5bc965cf":"<a class=\"anchor\" id=\"ToC\"><\/a>\n# Table of Contents\n1. [Install libraries](#install_libraries)\n1. [Import libraries](#import_libraries)\n1. [Define useful classes](#define_useful_classes)\n1. [Create ConnectX environment](#create_connectx_environment)\n1. [Configure hyper-parameters](#configure_hyper_parameters)\n1. [Train the agent](#train_the_agent)\n1. [Create an agent](#create_an_agent)\n1. [Evaluate the agent](#evaluate_the_agent)","fe365b49":"<a class=\"anchor\" id=\"define_useful_classes\"><\/a>\n# Define useful classes\nNOTE: It's not easy to generate a Q-Table with all possible states; and even if I can do so, the huge number of states will cost much of memory. So, I use the approach that dynamically adding newly discovered states into an object of QTable class created below.\n\n[Back to Table of Contents](#ToC)","0fa72bf0":"<a class=\"anchor\" id=\"import_libraries\"><\/a>\n# Import libraries\n[Back to Table of Contents](#ToC)"}}