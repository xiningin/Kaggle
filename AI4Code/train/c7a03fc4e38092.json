{"cell_type":{"25730c04":"code","bdf1ade2":"code","3e49d928":"code","8b7aa2f1":"code","7d7f858a":"code","34bade54":"code","e992d089":"code","1123bede":"code","3e187908":"code","940bd5dc":"code","ac6ae3aa":"code","cccf252c":"code","04b8310a":"code","3770952b":"code","453456ac":"code","927ea245":"code","3a02ba76":"code","67d8633d":"code","3b0f3b15":"code","e11f00d8":"code","b8b2cf81":"code","db666373":"code","5396ed9f":"code","d2b4feda":"code","ce258a11":"code","04a0b3a7":"code","c4457c5e":"code","a71e0120":"code","0a9c3cb9":"code","fcee617e":"code","23c124e8":"markdown","79d33291":"markdown","d66633fc":"markdown","b87a285e":"markdown","1c0a2478":"markdown","75b1d549":"markdown","c6713a1d":"markdown","c5c07e7d":"markdown","b55cc7cf":"markdown","bc6528ff":"markdown","79964430":"markdown"},"source":{"25730c04":"import re\nimport nltk\nimport spacy\nimport gensim\nimport logging\nimport warnings\nimport pyLDAvis\nimport numpy as np\nimport pandas as pd\nimport pyLDAvis.gensim\nimport gensim.corpora as corpora\n\n\n\nfrom pprint import pprint\nfrom collections import OrderedDict\nfrom gensim.models import CoherenceModel\nfrom gensim.utils import simple_preprocess\nfrom wordcloud import WordCloud, STOPWORDS \nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","bdf1ade2":"dataW = pd.read_csv(\"Cleaned_Washington.csv\")\ndataS = pd.read_csv(\"Cleaned_StarTribune.csv\")","3e49d928":"dataW['Headline'] = dataW['Headline'].str.replace('\\r\\n','')\ndataW['Headline'] = dataW['Headline'].str.replace('\\r','') \ndataW['Headline'] = dataW['Headline'].str.replace('\\n','')\n\ndataS['Headline'] = dataS['Headline'].str.replace('\\r\\n','')\ndataS['Headline'] = dataS['Headline'].str.replace('\\r','') \ndataS['Headline'] = dataS['Headline'].str.replace('\\n','')\n\ndataW['Report'] = dataW['Report'].str.replace('\\r\\n','')\ndataW['Report'] = dataW['Report'].str.replace('\\r','') \ndataW['Report'] = dataW['Report'].str.replace('\\n','')\n\ndataS['Report'] = dataS['Report'].str.replace('\\r\\n','')\ndataS['Report'] = dataS['Report'].str.replace('\\r','') \ndataS['Report'] = dataS['Report'].str.replace('\\n','')","8b7aa2f1":"analyser = SentimentIntensityAnalyzer()\ndef sentiment_analyzer_scores(sentence):\n    score = analyser.polarity_scores(sentence)\n\n    # sent = 0\n    # if ( (score['neg']+score['neu']) > (score['pos']+score['neu']) ) and score['compound'] < 0:\n    #   sent = -1\n    # elif ( (score['neg']+score['neu']) < (score['pos']+score['neu']) ) and score['compound'] > 0:\n    #   sent = 1\n    # elif ( (score['neg']+score['neu']) == (score['pos']+score['neu']) ) and score['compound'] == 0:\n    #   sent = 0\n    # else:\n    #   sent = -2\n    \n    return score['compound']\n    \ndataW['Headline_Sentiment'] = dataW['Headline'].apply(sentiment_analyzer_scores)\ndataS['Headline_Sentiment'] = dataS['Headline'].apply(sentiment_analyzer_scores)\n\ndataW['Body_Sentiment'] = dataW['Report'].apply(sentiment_analyzer_scores)\ndataS['Body_Sentiment'] = dataS['Report'].apply(sentiment_analyzer_scores)","7d7f858a":"nlp = spacy.load('en_core_web_sm')\n\nclass TextRank4Keyword():\n    \"\"\"Extract keywords from text\"\"\"\n    \n    def __init__(self):\n        self.d = 0.85 # damping coefficient, usually is .85\n        self.min_diff = 1e-5 # convergence threshold\n        self.steps = 10 # iteration steps\n        self.node_weight = None # save keywords and its weight\n\n    \n    def set_stopwords(self, stopwords):  \n        \"\"\"Set stop words\"\"\"\n        for word in STOP_WORDS.union(set(stopwords)):\n            lexeme = nlp.vocab[word]\n            lexeme.is_stop = True\n    \n    def sentence_segment(self, doc, candidate_pos, lower):\n        \"\"\"Store those words only in cadidate_pos\"\"\"\n        sentences = []\n        for sent in doc.sents:\n            selected_words = []\n            for token in sent:\n                # Store words only with cadidate POS tag\n                if token.pos_ in candidate_pos and token.is_stop is False:\n                    if lower is True:\n                        selected_words.append(token.text.lower())\n                    else:\n                        selected_words.append(token.text)\n            sentences.append(selected_words)\n        return sentences\n        \n    def get_vocab(self, sentences):\n        \"\"\"Get all tokens\"\"\"\n        vocab = OrderedDict()\n        i = 0\n        for sentence in sentences:\n            for word in sentence:\n                if word not in vocab:\n                    vocab[word] = i\n                    i += 1\n        return vocab\n    \n    def get_token_pairs(self, window_size, sentences):\n        \"\"\"Build token_pairs from windows in sentences\"\"\"\n        token_pairs = list()\n        for sentence in sentences:\n            for i, word in enumerate(sentence):\n                for j in range(i+1, i+window_size):\n                    if j >= len(sentence):\n                        break\n                    pair = (word, sentence[j])\n                    if pair not in token_pairs:\n                        token_pairs.append(pair)\n        return token_pairs\n        \n    def symmetrize(self, a):\n        return a + a.T - np.diag(a.diagonal())\n    \n    def get_matrix(self, vocab, token_pairs):\n        \"\"\"Get normalized matrix\"\"\"\n        # Build matrix\n        vocab_size = len(vocab)\n        g = np.zeros((vocab_size, vocab_size), dtype='float')\n        for word1, word2 in token_pairs:\n            i, j = vocab[word1], vocab[word2]\n            g[i][j] = 1\n            \n        # Get Symmeric matrix\n        g = self.symmetrize(g)\n        \n        # Normalize matrix by column\n        norm = np.sum(g, axis=0)\n        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n        \n        return g_norm\n\n    \n    def get_keywords(self, number=10):\n        \"\"\"Print top number keywords\"\"\"\n        key_list = []\n        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n        for i, (key, value) in enumerate(node_weight.items()):\n#             print(key + ' - ' + str(value))\n#             print(key)\n            key_list.append(key)\n            if i > number:\n                break\n        return key_list\n        \n        \n    def analyze(self, text, \n                candidate_pos=['NOUN', 'PROPN'], \n                window_size=4, lower=False, stopwords=list()):\n        \"\"\"Main function to analyze text\"\"\"\n        \n        # Set stop words\n        self.set_stopwords(stopwords)\n        \n        # Pare text by spaCy\n        doc = nlp(text)\n        \n        # Filter sentences\n        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n        \n        # Build vocabulary\n        vocab = self.get_vocab(sentences)\n        \n        # Get token_pairs from windows\n        token_pairs = self.get_token_pairs(window_size, sentences)\n        \n        # Get normalized matrix\n        g = self.get_matrix(vocab, token_pairs)\n        \n        # Initionlization for weight(pagerank value)\n        pr = np.array([1] * len(vocab))\n        \n        # Iteration\n        previous_pr = 0\n        for epoch in range(self.steps):\n            pr = (1-self.d) + self.d * np.dot(g, pr)\n            if abs(previous_pr - sum(pr))  < self.min_diff:\n                break\n            else:\n                previous_pr = sum(pr)\n\n        # Get weight for each node\n        node_weight = dict()\n        for word, index in vocab.items():\n\n            node_weight[word] = pr[index]\n        \n        self.node_weight = node_weight\n\ndef getHeadlineKeywords(sentence):\n  tr4w = TextRank4Keyword()\n  tr4w.analyze(sentence, candidate_pos = ['NOUN', 'PROPN'], window_size=4, lower=False)\n  return (tr4w.get_keywords(5))\n      \ndef getBodyKeywords(sentence):\n  tr4w = TextRank4Keyword()\n  tr4w.analyze(sentence, candidate_pos = ['NOUN', 'PROPN'], window_size=4, lower=False)\n  return (tr4w.get_keywords(12))\n\ndataW['Headline_Keywords'] = dataW['Headline'].apply(getHeadlineKeywords)\ndataS['Headline_Keywords'] = dataS['Headline'].apply(getHeadlineKeywords)\n\ndataW['Body_Keywords'] = dataW['Report'].apply(getBodyKeywords)\ndataS['Body_Keywords'] = dataS['Report'].apply(getBodyKeywords)","34bade54":"!python -m spacy download en","e992d089":"nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n\n\ndef sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\n# Define functions for stopwords, bigrams, trigrams and lemmatization\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef make_bigrams(texts,bigram_mod):\n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https:\/\/spacy.io\/api\/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out\n\n\ndef format_topics_sentences(ldamodel=None, corpus=None, texts=None):\n  # Init output\n  sent_topics_df = pd.DataFrame()\n\n  # Get main topic in each document\n  for i, row_list in enumerate(ldamodel[corpus]):\n      row = row_list[0] if ldamodel.per_word_topics else row_list            \n      # print(row)\n      row = sorted(row, key=lambda x: (x[1]), reverse=True)\n      # Get the Dominant topic, Perc Contribution and Keywords for each document\n      for j, (topic_num, prop_topic) in enumerate(row):\n          if j == 0:  # => dominant topic\n              wp = ldamodel.show_topic(topic_num)\n              topic_keywords = \", \".join([word for word, prop in wp])\n              sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n          else:\n              break\n  sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n  # Add original text to the end of the output\n  contents = pd.Series(texts)\n  sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n  return(sent_topics_df)\n\n\ndef computeLDA(data):\n  # Clean\n  data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n  data = [re.sub('\\s+', ' ', sent) for sent in data]\n  data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n\n  # Tokenize into words\n  data_words = list(sent_to_words(data))\n\n  # Build the bigram and trigram models\n  bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n  trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n\n  # Faster way to get a sentence clubbed as a trigram\/bigram\n  bigram_mod = gensim.models.phrases.Phraser(bigram)\n  trigram_mod = gensim.models.phrases.Phraser(trigram)\n\n  # Remove Stop Words\n  data_words_nostops = remove_stopwords(data_words)\n\n  # Form Bigrams\n  data_words_bigrams = make_bigrams(data_words_nostops,bigram_mod)\n\n\n  # Do lemmatization keeping only noun, adj, vb, adv\n  data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\n\n  # Create Dictionary\n  id2word = corpora.Dictionary(data_lemmatized)\n\n  # Create Corpus\n  texts = data_lemmatized\n\n  # Term Document Frequency\n  corpus = [id2word.doc2bow(text) for text in texts]\n\n\n  # Build LDA model\n  lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                            id2word=id2word,\n                                            num_topics=20, \n                                            random_state=100,\n                                            update_every=1,\n                                            chunksize=100,\n                                            passes=10,\n                                            alpha='auto',\n                                            per_word_topics=True)\n  \n  # # Print the Keyword in the 10 topics\n  # pprint(lda_model.print_topics())\n  # doc_lda = lda_model[corpus]\n\n  # # Compute Perplexity\n  # print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n\n  # # Compute Coherence Score\n  # coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n  # coherence_lda = coherence_model_lda.get_coherence()\n  # print('\\nCoherence Score: ', coherence_lda)\n\n  # # Visualize the topics\n  # pyLDAvis.enable_notebook()\n  # vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n  # vis\n\n\n  df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data)\n\n  # Format\n  df_dominant_topic = df_topic_sents_keywords.reset_index()\n  df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n  return df_dominant_topic\n\n# Headline\ndf = dataW.Headline.values.tolist()\ndataW['Headline_Topic'] = computeLDA(df)['Keywords'].tolist()\n\ndf = dataS.Headline.values.tolist()\ndataS['Headline_Topic'] = computeLDA(df)['Keywords'].tolist()\n\n# Body\ndf = dataW.Report.values.tolist()\ndataW['Body_Topic'] = computeLDA(df)['Keywords'].tolist()\n\ndf = dataS.Report.values.tolist()\ndataS['Body_Topic'] = computeLDA(df)['Keywords'].tolist()","1123bede":"nlp = spacy.load(\"en_core_web_sm\")\ndef getEntities(sentence):\n  doc = nlp(sentence)\n  entity = {}\n  pos = {}\n  tags = {}\n  for ent in doc.ents:\n      entity[ent.label_]=ent.text\n  \n  for token in doc:\n    pos[token.tag_]=token.text\n\n  tags['NER'] = entity\n  tags['POS'] = pos\n\n  return tags\n# getEntities(\"The corona virus has spread all over Bangladesh\")\n\ndataW['Headline_NER_POS'] = dataW['Headline'].apply(getEntities)\ndataS['Headline_NER_POS'] = dataS['Headline'].apply(getEntities)\n\ndataW['Body_NER_POS'] = dataW['Report'].apply(getEntities)\ndataS['Body_NER_POS'] = dataS['Report'].apply(getEntities)","3e187908":"# Generate word clouds per month\n# from wordcloud import WordCloud, STOPWORDS \n\nnlp = spacy.load(\"en_core_web_sm\")\ndef removePOS(sentence):\n  doc = nlp(sentence)\n\n  for token in doc:\n    if token.tag_ == 'VBD' or token.tag_ == 'MD' or token.tag_ == 'PRP' or token.tag_ == 'VB' or token.tag_ == 'CC' or token.tag_ == 'JJ':\n      sentence = sentence.replace(token.text,'')\n  \n  return sentence\n\ncomment_words = '' \nstopwords = set(STOPWORDS) \n\n# Washington Feb - May\ndataW1 = dataW[(dataW['Corrected_Date']>='2020-02-01') & (dataW['Corrected_Date']<'2020-03-01')]['Report'].apply(removePOS)\n\ndataW2 = dataW[(dataW['Corrected_Date']>='2020-03-01') & (dataW['Corrected_Date']<'2020-04-01')]['Report'].apply(removePOS)\n\ndataW3 = dataW[(dataW['Corrected_Date']>='2020-04-01') & (dataW['Corrected_Date']<'2020-05-01')]['Report'].apply(removePOS)\n\ndataW4 = dataW[(dataW['Corrected_Date']>='2020-05-01') & (dataW['Corrected_Date']<'2020-05-30')]['Report'].apply(removePOS)\n\n# Star Tribune Feb - May\ndataS1 = dataS[(dataS['Corrected_Date']>='2020-02-01') & (dataS['Corrected_Date']<'2020-03-01')]['Report'].apply(removePOS)\n\ndataS2 = dataS[(dataS['Corrected_Date']>='2020-03-01') & (dataS['Corrected_Date']<'2020-04-01')]['Report'].apply(removePOS)\n\ndataS3 = dataS[(dataS['Corrected_Date']>='2020-04-01') & (dataS['Corrected_Date']<'2020-05-01')]['Report'].apply(removePOS)\n\ndataS4 = dataS[(dataS['Corrected_Date']>='2020-05-01') & (dataS['Corrected_Date']<'2020-05-30')]['Report'].apply(removePOS)\n","940bd5dc":"# iterate through the csv file\ndef displayWordCloud(data,label):\n    comment_words = '' \n    stopwords = set(STOPWORDS) \n    for val in data: \n\n        # typecaste each val to string \n        val = str(val) \n\n        # split the value \n        tokens = val.split() \n\n        # Converts each token into lowercase \n        for i in range(len(tokens)): \n            tokens[i] = tokens[i].lower() \n\n        comment_words += \" \".join(tokens)+\" \"\n        comment_words = comment_words.replace('wh','')\n        comment_words = comment_words.replace('ing','')\n        comment_words = comment_words.replace('en','')\n        comment_words = comment_words.replace('ich','')\n        comment_words = comment_words.replace('tir','')\n        comment_words = comment_words.replace('ek','')\n        comment_words = comment_words.replace('re','')\n        comment_words = comment_words.replace('ty','')\n        comment_words = comment_words.replace('wn','')\n\n    wordcloud = WordCloud(width = 800, height = 800, \n                    background_color ='white', \n                    stopwords = stopwords, \n                    min_font_size = 10).generate(comment_words) \n\n\n    # plot the WordCloud image\n    fig = plt.figure(figsize = (8, 8), facecolor = None)\n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n\n    plt.show()\n    fig.savefig(label+'.png')\ndisplayWordCloud(dataS4,'StarTribune - May')","ac6ae3aa":"total_cases = []\nlookup = ['diagnosed','infected','killed','case','cases','death','deaths','died','quarantining','quarantine','quarantined']\nfor paragraphs in dataS['Report'].tolist():\n    cases = []\n    for sentence in paragraphs.split('.'):\n    #     print(sentence)\n        for lookupwords in lookup:\n            if lookupwords in sentence:\n                for words in sentence.split():\n                    words = words.replace(',','')\n                    words = words.replace('-plus','')\n                    if words.isdigit():\n#                         print(words)\n                        cases.append(int(words))\n                break\n#                 print(\"\\n\")\n    total_cases.append(sum(cases))","cccf252c":"len(total_cases)","04b8310a":"dataS['Corona_Cases_number'] = total_cases","3770952b":"import matplotlib.pyplot as plt\nw1 = ['_Feb','_Mar','_Apr','_May']\nw2 = [dataW[(dataW['Corrected_Date']>='2020-02-01') & (dataW['Corrected_Date']<'2020-03-01')]['Corona_Cases_number'].sum(),\n             dataW[(dataW['Corrected_Date']>='2020-03-01') & (dataW['Corrected_Date']<'2020-04-01')]['Corona_Cases_number'].sum(),\n             dataW[(dataW['Corrected_Date']>='2020-04-01') & (dataW['Corrected_Date']<'2020-05-01')]['Corona_Cases_number'].sum(),\n             dataW[(dataW['Corrected_Date']>='2020-05-01') & (dataW['Corrected_Date']<'2020-05-30')]['Corona_Cases_number'].sum()]\ns1 = ['_Feb','_Mar','_Apr','_May']\ns2 = [dataS[(dataS['Corrected_Date']>='2020-02-01') & (dataS['Corrected_Date']<'2020-03-01')]['Corona_Cases_number'].sum(),\n             dataS[(dataS['Corrected_Date']>='2020-03-01') & (dataS['Corrected_Date']<'2020-04-01')]['Corona_Cases_number'].sum(),\n             dataS[(dataS['Corrected_Date']>='2020-04-01') & (dataS['Corrected_Date']<'2020-05-01')]['Corona_Cases_number'].sum(),\n             dataS[(dataS['Corrected_Date']>='2020-05-01') & (dataS['Corrected_Date']<'2020-05-30')]['Corona_Cases_number'].sum()]\nplt.figure(figsize=(12,8))\nplt.plot(w1, w2, label = \"Washington Post\")\nplt.plot(s1, s2, label = \"StartTribune\")\nplt.xlabel('Month')\nplt.ylabel('Cases')\nplt.title('Cases of Covid-19 by month')\nplt.legend()\n\nplt.show()\nplt.savefig('Corona_Virus_Cases.jpeg',dpi = 300)\n# df = pd.DataFrame()\n# df['Month']=['Feb','Mar','Apr','May']\n# df['Month']=[0,1,2,3]\n# df['Value']=[dataW[(dataW['Corrected_Date']>='2020-02-01') & (dataW['Corrected_Date']<'2020-03-01')]['Corona_Cases_number'].sum(),\n#              dataW[(dataW['Corrected_Date']>='2020-03-01') & (dataW['Corrected_Date']<'2020-04-01')]['Corona_Cases_number'].sum(),\n#              dataW[(dataW['Corrected_Date']>='2020-04-01') & (dataW['Corrected_Date']<'2020-05-01')]['Corona_Cases_number'].sum(),\n#              dataW[(dataW['Corrected_Date']>='2020-05-01') & (dataW['Corrected_Date']<'2020-05-30')]['Corona_Cases_number'].sum()]","453456ac":"dataW.to_csv('Cleaned_Washington.csv',index=False)\ndataS.to_csv('Cleaned_StarTribune.csv',index=False)","927ea245":"import pandas as pd\ndataW= pd.read_csv('Cleaned_Washington.csv')\ndataS= pd.read_csv('Cleaned_StarTribune.csv')","3a02ba76":"dataW","67d8633d":"temp_str='<|endoftext|>'.join(dataS['Headline'].tolist())","3b0f3b15":"len(temp_str)","e11f00d8":"text_file = open(\"StarTribune-Headline.txt\", \"w\",encoding='utf-8')\nn = text_file.write(temp_str)\ntext_file.close()","b8b2cf81":"# dataD = pd.read_csv('Bangladesh - DailyStar.csv')\ndataP = pd.read_csv('Bangladesh - Prothom_Alo.csv')","db666373":"# dataD = dataD[0:25]\ndataP = dataP[0:25]","5396ed9f":"dataP","d2b4feda":"month = ['Feb','Mar','Apr']\n\ncorrected_date = []\nfor d in dataP['Date'].str.split(','):\n#     print(d[0])\n    for m in month:\n        if m in d[0]:\n#             print(month.index(m))\n#             print(d[0].replace(m,'').strip())\n            s = \"2020\/\" + str(month.index(m)+2)+ \"\/\" + d[0].replace(m,'').strip()\n            corrected_date.append(s)","ce258a11":"dataP['corrected_date'] = corrected_date","04a0b3a7":"dataP","c4457c5e":"dataP['corrected_date'] = pd.to_datetime(dataP['corrected_date'], format=\"%Y\/%m\/%d\")","a71e0120":"dataD","0a9c3cb9":"# Generate word clouds per month\n# from wordcloud import WordCloud, STOPWORDS \n\nnlp = spacy.load(\"en_core_web_sm\")\ndef removePOS(sentence):\n  doc = nlp(sentence)\n\n  for token in doc:\n    if token.tag_ == 'VBD' or token.tag_ == 'MD' or token.tag_ == 'PRP' or token.tag_ == 'VB' or token.tag_ == 'CC' or token.tag_ == 'JJ':\n      sentence = sentence.replace(token.text,'')\n  \n  return sentence\n\ncomment_words = '' \nstopwords = set(STOPWORDS) \n\n# DailyStar Mar - Apr\ndataD1 = dataD[(dataD['corrected_date']>='2020-03-01') & (dataD['corrected_date']<'2020-04-01')]['Report'].apply(removePOS)\n\ndataD2 = dataD[(dataD['corrected_date']>='2020-04-01') & (dataD['corrected_date']<'2020-04-30')]['Report'].apply(removePOS)\n\n# Prothom Alo Mar - Apr\ndataP1 = dataP[(dataP['corrected_date']>='2020-03-01') & (dataP['corrected_date']<'2020-04-01')]['Report'].apply(removePOS)\n\ndataP2 = dataP[(dataP['corrected_date']>='2020-04-01') & (dataP['corrected_date']<'2020-04-30')]['Report'].apply(removePOS)\n","fcee617e":"# iterate through the csv file\ndef displayWordCloud(data,label):\n    comment_words = '' \n    stopwords = set(STOPWORDS) \n    for val in data: \n\n        # typecaste each val to string \n        val = str(val) \n\n        # split the value \n        tokens = val.split() \n\n        # Converts each token into lowercase \n        for i in range(len(tokens)): \n            tokens[i] = tokens[i].lower() \n\n        comment_words += \" \".join(tokens)+\" \"\n        comment_words = comment_words.replace('wh','')\n        comment_words = comment_words.replace('ing','')\n        comment_words = comment_words.replace('en','')\n        comment_words = comment_words.replace('ich','')\n        comment_words = comment_words.replace('tir','')\n        comment_words = comment_words.replace('ek','')\n        comment_words = comment_words.replace('re','')\n        comment_words = comment_words.replace('ty','')\n        comment_words = comment_words.replace('wn','')\n\n    wordcloud = WordCloud(width = 800, height = 800, \n                    background_color ='white', \n                    stopwords = stopwords, \n                    min_font_size = 10).generate(comment_words) \n\n\n    # plot the WordCloud image\n    fig = plt.figure(figsize = (8, 8), facecolor = None)\n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n\n    plt.show()\n    fig.savefig(label+'.png')\ndisplayWordCloud(dataP2,'Prothom Alo - Apr')","23c124e8":"### Text Preprocess","79d33291":"### Topic Extraction using LDA","d66633fc":"### Imports","b87a285e":"### Read Data","1c0a2478":"### Visualizations","75b1d549":"### Fixing Date","c6713a1d":"### NER-POS","c5c07e7d":"### Keyword Extraction using TextRank","b55cc7cf":"### Sentiment Analysis using Vader Sentiment","bc6528ff":"#### Word Cloud","79964430":"#### Corona cases"}}