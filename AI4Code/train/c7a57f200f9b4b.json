{"cell_type":{"dc4039f4":"code","9245e70c":"code","aadfd819":"code","f109fa2f":"code","cf0ce6c3":"code","60d717b4":"code","16cb00fe":"code","285504da":"code","0b1fea43":"code","f82b8e71":"code","ae6e2231":"code","e2ac9fc2":"code","433f76a6":"code","78f5c8e2":"code","63e3f239":"code","c22b9c7a":"code","f48d128f":"code","bf241b55":"code","9395d817":"code","2ab09bee":"code","dc740e89":"code","233e497a":"code","f5b74627":"code","ee8bed83":"code","708af094":"code","7bdacac4":"code","35f396d6":"code","64206ffc":"markdown","bdef6db7":"markdown","f58d7317":"markdown","71627edf":"markdown","f5e756fc":"markdown","8b560985":"markdown","542f3bf0":"markdown","80c1db89":"markdown","cf66aad6":"markdown","1b670b81":"markdown","25b0f6fd":"markdown","7f5d46f6":"markdown"},"source":{"dc4039f4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as pp\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/\"))\n\n# Any results you write to the current directory are saved as output.","9245e70c":"\n#Load our Dataset for Logistic Regression\ncomponents = pd.read_csv('..\/input\/ex2data2.txt', header=None, names = ['feature 1', 'feature 2', 'faulty'])\ncomponents.head()","aadfd819":"# get positive and negative samples for plotting\npos = components['faulty'] == 1\nneg = components['faulty'] == 0\n","f109fa2f":"# Visualize Data\nfig, axes = pp.subplots();\naxes.set_xlabel('Feature 1')\naxes.set_ylabel('Feature 2')\naxes.scatter(components.loc[pos, 'feature 1'], components.loc[pos, 'feature 2'], color = 'r', marker='x', label='Faulty')\naxes.scatter(components.loc[neg, 'feature 1'], components.loc[neg, 'feature 2'], color = 'g', marker='o', label='Good')\naxes.legend(title='Legend', loc = 'best' )\naxes.set_xlim(-1,1.5)\naxes.set_xlim(-1,1.5)","cf0ce6c3":"# define function to map higher order polynomial features\ndef mapFeature(X1, X2, degree):\n    res = np.ones(X1.shape[0])\n    for i in range(1,degree + 1):\n        for j in range(0,i + 1):\n            res = np.column_stack((res, (X1 ** (i-j)) * (X2 ** j)))\n    \n    return res","60d717b4":"# Get the features \nX = components.iloc[:, :2]","16cb00fe":"degree = 2","285504da":"X_poly = mapFeature(X.iloc[:, 0], X.iloc[:, 1], degree)","0b1fea43":"# Get the target variable\ny = components.iloc[:, 2]","f82b8e71":"def sigmoid(z):\n    return 1 \/ (1 + np.exp(-z))","ae6e2231":"def costFunc(theta, X, y):\n    m = y.shape[0]\n    z = X.dot(theta)\n    h = sigmoid(z)\n    term1 = y * np.log(h)\n    term2 = (1- y) * np.log(1 - h)\n    J = -np.sum(term1 + term2, axis = 0) \/ m\n    return J ","e2ac9fc2":"# Set initial values for our parameters\ninitial_theta = np.zeros(X_poly.shape[1]).reshape(X_poly.shape[1], 1)","433f76a6":"# Now call the optimization routine\n#NOTE: This automatically picks the learning rate\nfrom scipy.optimize import minimize\nres = minimize(costFunc, initial_theta, args=(X_poly, y))","78f5c8e2":"# our optimizated coefficients\ntheta = res.x","63e3f239":"# define a function to plot the decision boundary\ndef plotDecisionBoundary(theta,degree, axes):\n    u = np.linspace(-1, 1.5, 50)\n    v = np.linspace(-1, 1.5, 50)\n    U,V = np.meshgrid(u,v)\n    # convert U, V to vectors for calculating additional features\n    # using vectorized implementation\n    U = np.ravel(U)\n    V = np.ravel(V)\n    Z = np.zeros((len(u) * len(v)))\n    \n    X_poly = mapFeature(U, V, degree)\n    Z = X_poly.dot(theta)\n    \n    # reshape U, V, Z back to matrix\n    U = U.reshape((len(u), len(v)))\n    V = V.reshape((len(u), len(v)))\n    Z = Z.reshape((len(u), len(v)))\n    \n    cs = axes.contour(U,V,Z,levels=[0],cmap= \"Greys_r\")\n    axes.legend(labels=['good', 'faulty', 'Decision Boundary'])\n    return cs","c22b9c7a":"# Plot Decision boundary\nfig, axes = pp.subplots();\naxes.set_xlabel('Feature 1')\naxes.set_ylabel('Feature 2')\naxes.scatter(components.loc[pos, 'feature 1'], components.loc[pos, 'feature 2'], color = 'r', marker='x', label='Faulty')\naxes.scatter(components.loc[neg, 'feature 1'], components.loc[neg, 'feature 2'], color = 'g', marker='o', label='Good')\n#axes.legend(title='Legend', loc = 'best' )\n\nplotDecisionBoundary(theta, degree, axes)","f48d128f":"# set degree = 1\ndegree = 1\n# map features to the degree\nX_poly = mapFeature(X.iloc[:, 0], X.iloc[:, 1], degree)\n# set initial parameters\ninitial_theta = np.zeros(X_poly.shape[1]).reshape(X_poly.shape[1], 1)","bf241b55":"# Run the optimzation function\nres = minimize(costFunc, initial_theta, args=(X_poly, y))\ntheta = res.x.reshape(res.x.shape[0], 1)\n\n# Plot Decision boundary\nfig, axes = pp.subplots();\naxes.set_xlabel('Feature 1')\naxes.set_ylabel('Feature 2')\naxes.scatter(components.loc[pos, 'feature 1'], components.loc[pos, 'feature 2'], color = 'r', marker='x', label='Faulty')\naxes.scatter(components.loc[neg, 'feature 1'], components.loc[neg, 'feature 2'], color = 'g', marker='o', label='Good')\n#axes.legend(title='Legend', loc = 'best' )\n\nplotDecisionBoundary(theta, degree, axes)","9395d817":"# set degree = 6\ndegree = 6\n# map features to the degree\nX_poly = mapFeature(X.iloc[:, 0], X.iloc[:, 1], degree)\n# set initial parameters\ninitial_theta = np.zeros(X_poly.shape[1]).reshape(X_poly.shape[1], 1)","2ab09bee":"# Run the optimzation function\nres = minimize(costFunc, initial_theta, args=(X_poly, y))\ntheta = res.x.reshape(res.x.shape[0], 1)\n\n# Plot Decision boundary\nfig, axes = pp.subplots();\naxes.set_xlabel('Feature 1')\naxes.set_ylabel('Feature 2')\naxes.scatter(components.loc[pos, 'feature 1'], components.loc[pos, 'feature 2'], color = 'r', marker='x', label='Faulty')\naxes.scatter(components.loc[neg, 'feature 1'], components.loc[neg, 'feature 2'], color = 'g', marker='o', label='Good')\n#axes.legend(title='Legend', loc = 'best' )\n\nplotDecisionBoundary(theta, degree, axes)","dc740e89":"# define the regularized cost function\ndef costFuncReg(theta, X, y, reg_factor):\n    m = y.shape[0]\n    z = X.dot(theta)\n    h = sigmoid(z)\n    term1 = y * np.log(h)\n    term2 = (1- y) * np.log(1 - h)\n    J = -np.sum(term1 + term2, axis = 0) \/ m\n    \n    # Regularization Term\n    reg_term = (reg_factor * sum(theta[1:] ** 2)) \/ (2 * m)\n    J = J + reg_term\n    return J  ","233e497a":"# Set the regularization factor to 1\nreg_factor = 1\n","f5b74627":"# set degree = 6\ndegree = 6\n# map features to the degree\nX_poly = mapFeature(X.iloc[:, 0], X.iloc[:, 1], degree)\n# set initial parameters\ninitial_theta = np.zeros(X_poly.shape[1]).reshape(X_poly.shape[1], 1)","ee8bed83":"# Run the optimzation function with regularization factor passed to the cost function\nres = minimize(costFuncReg, initial_theta, args=(X_poly, y, reg_factor))\ntheta = res.x.reshape(res.x.shape[0], 1)\n\n# Plot Decision boundary\nfig, axes = pp.subplots();\naxes.set_xlabel('Feature 1')\naxes.set_ylabel('Feature 2')\naxes.scatter(components.loc[pos, 'feature 1'], components.loc[pos, 'feature 2'], color = 'r', marker='x', label='Faulty')\naxes.scatter(components.loc[neg, 'feature 1'], components.loc[neg, 'feature 2'], color = 'g', marker='o', label='Good')\n#axes.legend(title='Legend', loc = 'best' )\n\nplotDecisionBoundary(theta, degree, axes)","708af094":"# set the regularization factor to 100\nreg_factor = 100","7bdacac4":"# set degree = 6\ndegree = 6\n# map features to the degree\nX_poly = mapFeature(X.iloc[:, 0], X.iloc[:, 1], degree)\n# set initial parameters\ninitial_theta = np.zeros(X_poly.shape[1]).reshape(X_poly.shape[1], 1)","35f396d6":"# Run the optimzation function with regularization factor passed to the cost function\nres = minimize(costFuncReg, initial_theta, args=(X_poly, y, reg_factor))\ntheta = res.x.reshape(res.x.shape[0], 1)\n\n# Plot Decision boundary\nfig, axes = pp.subplots();\naxes.set_xlabel('Feature 1')\naxes.set_ylabel('Feature 2')\naxes.scatter(components.loc[pos, 'feature 1'], components.loc[pos, 'feature 2'], color = 'r', marker='x', label='Faulty')\naxes.scatter(components.loc[neg, 'feature 1'], components.loc[neg, 'feature 2'], color = 'g', marker='o', label='Good')\n#axes.legend(title='Legend', loc = 'best' )\n\nplotDecisionBoundary(theta, degree, axes)","64206ffc":"Define the sigmoid function for logistic regression.\n\\begin{equation*}\nh(z)= \\frac{1} {1 + e^{-z}}\n\\end{equation*}\nwhere \n$z=\\mathbf{\\theta}^\\intercal \\mathbf{x}$\n\n","bdef6db7":"As we can see the model tries pretty hard to capture every single example perfectly and overfits the data.\nThis kind of model is said to have low bias and high variance. i.e The model has not pre-conceived notion about the seperation of the positive and negative examples and pretty much can fit any kind of data.\nSuch model will fail in predicting the correct classification when it sees new examples.","f58d7317":"As expected this is severly underfitting the data. The model in this case is said to be high bias and low variance.\ni.e The model is biased towards assuming the data in linearly seperable while the fact is that its not.","71627edf":"Define the Cost function for Logistic Regression \n\n$J(\\theta ) = \\frac{1}{m} \\sum_{i=1}^m[-y_i log(h_\\theta (z_i) \u2013 (1 \u2013 y_i) log(1-h_\\theta (z_i))]$","f5e756fc":"Now Lets try to see the other extreme end of the model by giving a very high degree for the polynomial\nfeatures","8b560985":"Now Lets see the effect of regularization in our previous overfitted model","542f3bf0":"Lets define the degree so that we can easily see the effects varying this","80c1db89":"As we can see that the positive and negative examples are not linearly seperable.\nSo we have to add additional higher order polynomial features.\n","cf66aad6":"Linear Seperation is just a special case\nEven though we know the data is not linearly seperable we can see what kind of decision boundary this would yield\n","1b670b81":"As we can see the having too much regularization negatively affects the model, since it now high bias towards a certain combinations of \"feature1\" and \"feature2\".","25b0f6fd":"One of techiques is to use regularization. The idea is to penalize the algorithm when it tries to overfit by adding a regularization term to the cost function. \n\nThe New Cost function with the regularization is specified as \n\n$J(\\theta ) = \\frac{1}{m} \\sum_{i=1}^m[-y_i log(h_\\theta (z_i) \u2013 (1 \u2013 y_i) log(1-h_\\theta (z_i))] + \n                    \\frac{\\lambda}{2m} \\sum_{j=1}^n[\\theta_j^2]$\n                    \nwhere $\\lambda$ = regularization factor <br>\n           n = number of features.\n           <br>\n (NOTE: The regularization term does include the intercept term $\\theta_0$","7f5d46f6":"As we see that the regularization prevented the degree = 6 overfitted model, to now provide a much smoother decision boundary.\n\nBut its also good to see the negative effects of too much regularization on our model"}}