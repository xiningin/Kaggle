{"cell_type":{"3233628a":"code","d403a59c":"code","c9025ece":"code","968fd203":"code","76c2d72c":"code","6483c6b1":"code","6c0f6485":"code","75294a9b":"code","d34e2c8e":"code","be3e551f":"code","e09abf79":"code","b93f1295":"code","18118c4f":"code","d1f45833":"code","c396c129":"code","84738149":"code","7873a0b1":"code","c0adb3a8":"code","b904e912":"code","8f9be072":"code","b37983fd":"code","6a7d3be7":"code","b729fbc6":"code","b915e206":"markdown","9ef9af5e":"markdown","2e0a336a":"markdown","00b931b4":"markdown","9a9bc79d":"markdown","a44007b0":"markdown","0e537515":"markdown","f77726f8":"markdown","355b9c94":"markdown","860e7fbc":"markdown","447d2ba6":"markdown","614a3873":"markdown","e8821c6a":"markdown","297718ee":"markdown","5f625ecf":"markdown","a5d84a4a":"markdown"},"source":{"3233628a":"from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torchvision.utils import make_grid\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n%matplotlib inline\nfrom IPython.display import HTML\nimport numpy as np\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torchvision.utils import save_image \n\n\nimport cv2\n","d403a59c":"\n\nDATA_PATH = '..\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba'\n# number of worker threads for loading the data \nN_WORKERS =  4\n# in the DCGAN paper, they used a batch_size of 128 \nbatch_size = 128\n#in this notebook we will use an images of 3 x 64 x 64 (if we desire to change it, we need to changes also in the gen and des architectures)\nimage_size =64 \nnc = 3 # the number of channels (3 for the RGB images)\nnz = 100 # length of the latent vector \nngf = 64 # the depth of the feature maps in  the genegator \nndf =  64 # the depth of the feature maps in the descriminator \n\nEPOCHS = 50\nlr = 0.0002 # the learning rate for the optimizer (we choose 0.0002 relatively to the paper of the DCGAN)\nbeta1 = 0.5  # the hyperparameter for the adam optim\n","c9025ece":"dataset = dset.ImageFolder(root=\"..\/input\/celeba-dataset\/img_align_celeba\",\n                           transform=transforms.Compose([\n                               transforms.Resize(image_size),\n                               transforms.CenterCrop(image_size),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ]))","968fd203":"# creating the dataloader based on the dataset \n# dl = th.utils.data.DataLoader(dataset,batch_size = batch_size , shuffle = True,num_workers = N_WORKERS)\n\n# As the dataset is very large more then 200k intances we are going to use just 50k instances \nn_intances = 10000\nindices = np.random.permutation(n_intances)\nsampler = SubsetRandomSampler(indices)\ndl = th.utils.data.DataLoader(dataset , batch_size  , sampler=sampler)\n","76c2d72c":"# setting the default device \ndevice = th.device(\"cuda\" if th.cuda.is_available() else 'cpu')","6483c6b1":"#creating a helper fonction to show some real images samples \ndef show_batch(dl): \n    for images, _ in dl : \n        fig, ax = plt.subplots(figsize = (16,10))\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.imshow(make_grid(images[:20],10).permute(1,2,0))\n        break","6c0f6485":"#plotting some samples \nshow_batch(dl)","75294a9b":"def weight_init(model): \n    class_name = model.__class__.__name__\n    if class_name.find(\"Conv\") !=-1:  # the convolutional and convolutional-transpose layers\n        #nn.init.normal_(tensor) fonction that fils the input tensor with values from the normale distribution \n        nn.init.normal_(model.weight.data,mean=0.0,std=0.02)\n    elif  class_name.find(\"BatchNorm\") !=-1: # the batch normalization layer\n        nn.init.normal_(model.weight.data, 1.0 , 0.02)\n        # nn.init.constant_(tensor , const) fils the input tensor with the constant \"const\"\n        nn.init.constant_(model.bias.data,0)\n","d34e2c8e":"class Generator(nn.Module): \n    def __init__(self): \n        super(Generator,self).__init__()\n        self.main = nn.Sequential(\n            # we are going to fit the latent vector into a convolutional\n            nn.ConvTranspose2d(nz,ngf*8,4,1,0,bias=False),\n            # ngf * 8 is the ouput of the ConvTranspose layer\n            nn.BatchNorm2d(ngf*8),\n            nn.ReLU(True),\n            # as we have inputs of shape 3 x 64 x 64 so after applying the convTranspose operation we will have\n            # output of shape (ngf*8) x 4 x 4  \n            \n            nn.ConvTranspose2d(ngf * 8 , ngf * 4 , 4 , stride=2,padding=1,bias=False),\n            #as we set tha stride =2 and we are appying the ConvTranspose operation \n            # so we will multiply the output shape by 2,contrary to the conv2d operation which devide by the stride\n            nn.BatchNorm2d(ngf*4),\n            nn.ReLU(True),\n            # output shape will be  (ngf*4) x 8 x 8\n\n            nn.ConvTranspose2d(ngf*4 , ngf*2, 4 , 2, 1,bias=False), # by applying a stride of 2 so the output shape will multiplyed by 2 \n            nn.BatchNorm2d(ngf*2),\n            nn.ReLU(True),\n            # output shape will be  (ngf*2) x 16 x 16\n            \n            # we will continue our convTranspose operation until we obtain the shape of our images (3x64x64)\n            nn.ConvTranspose2d(ngf*2 , ngf, 4 , 2, 1,bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            # output shape will be  ngf x 32 x 32\n            \n            nn.ConvTranspose2d(ngf,nc, 4 , 2 , 1,bias=False),\n            nn.Tanh(),\n            # output shape will be  nc x 64 x 64            \n        )\n    def forward(self,x):\n        return self.main(x)\n    ","be3e551f":"Gen =Generator()\nprint(Gen.main[0].weight.data[0])","e09abf79":"Gen = Generator().to(device)\n\n# Apply the weights_init function to randomly initialize all weights to mean=0, stdev=0.02\n\nGen.apply(weight_init)\n\nprint(Gen)","b93f1295":"Gen.main[0].weight.data[0]","18118c4f":"class Descriminator(nn.Module):\n    def __init__(self):\n        super(Descriminator,self).__init__()\n        self.main = nn.Sequential(\n        # the input will be an image of : nc x 64 x 64 image\n            nn.Conv2d(nc,ndf,4,2,1,bias=False),\n#             nn.BatchNorm2d(ndf),\n            nn.LeakyReLU(0.2,inplace=True), # slop of 0.2 is recommended by the comunity \n            #as we are using the conv2d layers with stride of 2, so the shapes of each channels will be devided by 2 \n            # output shape :  ndf x 32 x 32\n            \n            nn.Conv2d(ndf, ndf*2,4,2,1,bias=False),\n            nn.BatchNorm2d(ndf*2),\n            nn.LeakyReLU(0.2,inplace=True),\n            #output shape : (ndf*2) x 16 x 16\n\n            nn.Conv2d(ndf*2,ndf*4,4,2,1,bias=False),\n            nn.BatchNorm2d(ndf*4),\n            nn.LeakyReLU(0.2,inplace=True),\n            # output size : (ndf*4) x 8 x 8\n            nn.Conv2d(ndf*4,ndf*8,4,2,1,bias=False),\n            nn.BatchNorm2d(ndf*8),\n            nn.LeakyReLU(0.2,inplace=True),\n            #output size : ndf*8 x 4 x 4\n            nn.Conv2d(ndf*8,1,4,1,0,bias=False),\n            #output size  1 x 1 x 1\n            nn.Sigmoid(),\n        )\n    def forward(self,input):\n        return self.main(input)\n","d1f45833":"Des = Descriminator().to(device)\n\nDes.apply(weight_init)\n\nprint(Des)\n","c396c129":"criterion = nn.BCELoss()\n\n# creating the latent vector that is used to keep track of the G progression \nlatent_vector = torch.randn(64,nz,1,1,device=device)\n\noptim_G = th.optim.Adam(Gen.parameters(),lr=lr,betas = (beta1, 0.999))\noptim_D = th.optim.Adam(Des.parameters(),lr=lr,betas = (beta1, 0.999))\n\n    ","84738149":"# creating a helper fonction that train the Den and Gen\ndef train_des(images,label):\n    Des.zero_grad()\n    \n    #### Training the Des on the real images\n    \n    #Generating the labels for the real images\n    # we are using the th.full() that create a tensor with the same value, and give us the possibility to change the values \n    \n    label.fill_(1) # 1 for the real images\n    # we will have an output of 128 x 1x 1 from the Descriminator so we use .view(-1) to flatten into vector \n    output = Des(images).view(-1)\n    real_loss = criterion(output,label)\n    D_x = output.mean().item()\n    real_loss.backward()\n    \n    \n    \n    #### Training the Des on the fake images\n    # Generating the labels for the fake images \n    label.fill_(0) # zero for the fake labels \n    \n    # creating the latent vector \n    latent_vector = th.randn(len(images), nz , 1,1, device=device)\n    fake_images = Gen(latent_vector) \n    output = Des(fake_images.detach()).view(-1)\n    fake_loss = criterion(output, label)\n    fake_loss.backward()\n    D_Z_x = output.mean().item()\n    #Combine the losses \n    loss_d = real_loss + fake_loss \n    \n    \n    # set the optim a step \n    optim_D.step()\n    return loss_d , D_x , D_Z_x \n\ndef train_gen(label): \n    Gen.zero_grad()\n    label.fill_(1)\n    latent_vector = th.randn(label.size(0), nz , 1,1, device=device)\n    fake_images = Gen(latent_vector)\n    output = Des(fake_images).view(-1)\n    loss_g = criterion(output , label)\n    loss_g.backward()\n    D_G_Z  = output.mean().item()\n    optim_G.step()\n    return loss_g , D_G_Z,\n\n\n    \n    \n    \n    \n    ","7873a0b1":"for images,_ in dl : \n    latent_vector = th.randn(batch_size, nz , 1,1, device=device)\n    fake_images = Gen(latent_vector) \n    output = Des(fake_images).view(-1)\n#     output = Des(images.to(device)).view(-1)\n    print(output.mean().item())\n    print(criterion(output, th.ones(len(images)).to(device)).item())\n    break","c0adb3a8":"# now we define the training fonction \ndef train(dl,epochs= 20):\n    fake_images , d_losses,g_losses, real_scores, fake_scores = [] , [] , [] , [] , []\n    \n    for epoch in range(1,epochs+1): \n        for i , (images,_) in enumerate(dl): \n            # creating the label var\n            batch_size = images.size(0)\n            label = th.full((batch_size,),1,dtype=th.float,device=device)\n            images = images.to(device)\n            loss_d , real_score , fake_score = train_des(images,label)\n            loss_g , D_G_Z = train_gen(label)\n            \n            \n            if i % (len(dl)\/\/4) == 0:\n                d_losses.append(loss_d.item())\n                g_losses.append(loss_g.item())\n                real_scores.append(real_score)\n                fake_scores.append(fake_score)\n                print(f\"[{epoch}\/{epochs}] , {i}\/{len(dl)} , loss_g = {round(loss_g.item(),3)} , loss_d = {round(loss_d.item(),3)} , D(x) = {round(real_score,3)} D(G(z)) = {round(fake_score,3)} \")\n                with th.no_grad():\n                    print(f\"Saving image for iteration {i} in epoch {epoch}\")\n                    fake = Gen(latent_vector).detach().cpu()\n                fake_images.append(vutils.make_grid(fake, padding=2, normalize=True))\n                \n    return fake_images , d_losses,g_losses, real_score, fake_score\n    ","b904e912":"EPOCHS = 50 #100\nfake_images , d_losses,g_losses, real_score, fake_score =  train(dl,EPOCHS)","8f9be072":"EPOCHS = 200\nfake_images , d_losses,g_losses, real_score, fake_score =  train(dl,EPOCHS)","b37983fd":"plt.figure(figsize=(8,8))\nplt.title(\"Changing losses during Training\")\nplt.plot(g_losses, label=\"Gen loss\")\nplt.plot(d_losses, label=\"Des loss\")\nplt.xlabel(\"Iterations\") \nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","6a7d3be7":"#ploting the generated batch of images (128 image as the latent vector size)\nfig,ax = plt.subplots(figsize=(16,16))\nax.imshow(fake_images[240].permute(1,2,0))","b729fbc6":"real_batch = None\nfor images,_ in dl : \n    real_batch = images\n    break\n    \nprint(\"Visualizing the Real images\")\nplt.figure(figsize = (15,15))\nplt.subplot(1,2,1)\nplt.axis(\"off\")\nplt.title(\"Real Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch.to(device)[:128],padding=5,normalize=True).cpu() ,(1,2,0)))\n\nplt.subplot(1,2,2)\nplt.axis(\"off\")\nplt.title(\"Fake Images\")\nplt.imshow(np.transpose(fake_images[-1],(1,2,0)))\nplt.show()\n","b915e206":"We see that the losses are converging to a small values and this is good with 50 epochs ,we are good...","9ef9af5e":"## Visualizing the result \n\nnow that we trained the model (Gen and Des) we are going to vizualize the resuls \n1. first we will see the changing of the losses for both the Des and Gen \n2. then we will vizualize the output of the generator on the latent vector \n3. finaly we will show a batch of real data next to a generated batch from the Gen \n","2e0a336a":"## Generator\n\nThe generator try to map the latent space vector with the data space (real values), and this by creating a RGB images like the training images (3x64x64).\n\nWe use 2d convolutional transpose layer with 2d batch normalization and ReLU fonction, and the output is fed through a Tanh fonction to return it into input data ranfge [-1,1]\n\n**for the generator we are going to use the ConvTranspose2d layer which can be seen as the inverse of the conv2d layer (but it's not realy the inverse because it doesn't give as the correct inverse)**","00b931b4":"## Import the used packages","9a9bc79d":"for observing the progression of the Gen we are going to create an animation based on images on the fake_images list ","a44007b0":"We are going to do the same initialization for the Descriminator ","0e537515":"Finaly , we see that we have good results after comparing the real images with the generated ones.\nas an extention we can try to add some layers into the Gen and Des to see if we will perform our DCGAN.\n\nThis was the end of the notebook, hoping you enjoyed reading it :) \nDon't even hesitate to contacte me if you have some questions or if you want to give me some advices or additions on the notebook. \n\n----------------------------------------------------------","f77726f8":"We will define some inputs to have with them in all the notebook ","355b9c94":"## Loss fonction \n\nWe are going to use the **Binary Cross Entropy** as a loss fonction .\nwe will define the real labels as 1 and the fake labels as 0.\nfor the optimizer we are using the Adam optimizer with a learning_rate = 0.0002 and beta1=0.5 (as mentioned in the DCGAN paper).\nfor keeping track of the Learning of the generator, we are fixing just one batch (Gaussian distribution) and using it as latent vector.\n","860e7fbc":"For the initialization of the model, we are going to use a random weights and bias from the Normale Distribution with mean=0 and std = 0.02 as specified in the DCGAN paper.\nfor this we are creating the **weight_init** fonction that take a initialized model and re-initialize the weights and the bias of the convolutional, convolutional-transpose and batchNormalize layers with from the normale dist.","447d2ba6":"# DCGANs\n\nDCGAN means a GAN that uses the convolutional and convolutional-transpose layers in the descriminator and generator models respectively  \n1. **Descriminator**: is composed of conv2d, batch normalization layers and uses the LeakyReLU activation with 0.2 as slope. it's input is a 3 x 64 x 64 images and the output is the proba that this image is real (it belongs to the real data distribution) \n2. **Generator** : for the gen, it uses convolutional-transpose and batch normalization layer, with the ReLU activation,the input is a latent vector z and the output is an 3 x 64 x 64  image ( The strided conv-transpose layers allow the latent vector to be transformed into a volume with the same shape as an image)\n\nThe Transformed Conv is almost doing the deconvolutional operation , but it doesn't create exctly the inverse results of the convolutional operation","614a3873":"## Descriminator\n\nThe descriminator will be a classifier that output the proba that the image is real, for this we are going to use a series of conv2d, batch normalization layers with the Leaky Relu activation fonction with slope of 0.2,and also a sigmoid fonction to convert the outputs to probabilities, we can extend this arcitecture with more layers,\n\nin the DCGAN paper we mentioned that it's a good practise to use strided conv2d layers instead of using the maxpooling and that because it lets the network to learn it's own pooling fonction.","e8821c6a":"## Training  \nWe are going to split the training into 2 phases, \n1. We are starting by the Descriminator : we are going first to pass a batch of training set (real images) to the Des and calculate the loss. Secondly we will generate a batch of images (fake images)  from the current generator and pass it to the descriminator and recalculate the loss , then we accumulate the gradient and call a step of the optimizer \n2. Then we will train the generator : and this by passing a latent vector (that we craete earlier) and generate fake images. For the evaluation we will use the Descriminator to evaluate the Generator and this by passing the generated images to the Des and see how he can predict that the images are fake or real by using the real labels (**all Ones**) before computing the Gen gradient (our goal is that the Des predict the images as Reals) and that's in an effort to generate better fake images \n\nfinaly in the end of each epoch we will pass the fixed latent_vector to the Gen to visualize the progression process of the Generator, and we are also going to save some statistic information like loss_D which is the loss the Des for the real and fake images \n\nthe D(x) and D(G(x)) (with fake batch)  are the average output for the Des and the Gen respectively, first will start at 1 and the second at 0 and then will converge to 0.5 as Gen get's better, and that's because When Generator gets better it means that the Descriminator will not pick the diffrence for between the fake and real images so it outputs 0.5 it means that it is **incertain** ","297718ee":"We are using the ImageFolder class to extract our data and doing some transformation on it \n\nwe are resizing all the images to the appropriate size that we define earlier and cropping the image on the center, then normalizing it with mean and std","5f625ecf":"now we will save all the generated images in a video so that we can see the progression of the Gen ","a5d84a4a":"Here that we build the Generator network, we will fit it to the weight init fonction so that we initialize the weights from the normal distribution"}}