{"cell_type":{"a54e145a":"code","99ce0256":"code","a05ee668":"code","72366b31":"code","bca9af72":"code","e2480d35":"code","08dd129e":"code","1f8e8dac":"code","a582f4d0":"code","8a00edd5":"code","812ab56b":"code","f51526f4":"code","d7032fd2":"code","586b7fc3":"code","d3b9b6c1":"code","07e80cd5":"markdown","94cb1bb0":"markdown","717509cb":"markdown"},"source":{"a54e145a":"import pandas as pd\nfrom PIL import Image, ImageDraw\nfrom pathlib import Path\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2 as cv\nfrom tqdm.notebook import tqdm\nfrom joblib import Parallel, delayed","99ce0256":"fast_sub = (len(pd.read_csv('..\/input\/nfl-health-and-safety-helmet-assignment\/test_baseline_helmets.csv')) == 72386)\nfast_sub","a05ee668":"# code from: https:\/\/www.kaggle.com\/robikscube\/nfl-helmet-assignment-getting-started-guide\ndef add_track_features(tracks, fps=59.94, snap_frame=10):\n    \"\"\"\n    Add column features helpful for syncing with video data.\n    \"\"\"\n    tracks = tracks.copy()\n    tracks[\"game_play\"] = (\n        tracks[\"gameKey\"].astype(\"str\")\n        + \"_\"\n        + tracks[\"playID\"].astype(\"str\").str.zfill(6)\n    )\n    tracks[\"time\"] = pd.to_datetime(tracks[\"time\"])\n    snap_dict = (\n        tracks.query('event == \"ball_snap\"')\n        .groupby(\"game_play\")[\"time\"]\n        .first()\n        .to_dict()\n    )\n    tracks[\"snap\"] = tracks[\"game_play\"].map(snap_dict)\n    tracks[\"isSnap\"] = tracks[\"snap\"] == tracks[\"time\"]\n    tracks[\"team\"] = tracks[\"player\"].str[0].replace(\"H\", \"Home\").replace(\"V\", \"Away\")\n    tracks[\"snap_offset\"] = (tracks[\"time\"] - tracks[\"snap\"]).astype(\n        \"timedelta64[ms]\"\n    ) \/ 1_000\n    # Estimated video frame\n    tracks[\"est_frame\"] = (\n        ((tracks[\"snap_offset\"] * fps) + snap_frame).round().astype(\"int\")\n    )\n    return tracks\n\ndef add_video_features(videos):\n    videos['game_play'] = videos['video_frame'].apply(lambda x: '_'.join(x.split('_')[:2]))\n    videos['camera'] = videos['video_frame'].apply(lambda x: x.split('_')[2])\n    videos['frame'] = videos['video_frame'].apply(lambda x: x.split('_')[-1])\n    videos['xc'] = (videos['left'] + videos['width']\/2).astype(int).values\n    videos['yc'] = (videos['top'] + videos['height']\/2).astype(int).values\n    return videos\n\n# TODO, add interpolation of tracking_df and replace nearest\nclass get_keypoints():\n    \n    def __init__(self, video_df = None, track_df = None):\n        if video_df is None:\n            video_df = pd.read_csv('..\/input\/nfl-health-and-safety-helmet-assignment\/test_baseline_helmets.csv')\n            self.video_df = add_video_features(video_df)\n        if track_df is None:\n            tracking_df = pd.read_csv('..\/input\/nfl-health-and-safety-helmet-assignment\/test_player_tracking.csv')\n            tracking_df = add_track_features(tracking_df)\n            self.tracking_df = tracking_df.query(\"est_frame > 0\")\n            \n    def __call__(self, game_play, frame, min_conf = 0.6, topk = 22, normalized = True, debug = False):\n        \n        kpS = self.video_df.query(\n            f\"game_play == '{game_play}' and frame == '{frame}' and camera == 'Sideline' and conf > {min_conf}\").nlargest(topk, 'conf')\n        kpE = self.video_df.query(\n            f\"game_play == '{game_play}' and frame == '{frame}' and camera == 'Endzone'and conf > {min_conf}\").nlargest(topk, 'conf')\n        \n        keypoints = dict()\n        keypoints['Sideline'] = kpS[['xc', 'yc']].values\n        keypoints['Endzone'] = kpE[['xc', 'yc']].values\n        frames = self.tracking_df.query(\n            f\"game_play == '{game_play}'\")['est_frame'].unique()\n        if frame not in frames:\n            index = (np.absolute(frames-frame)).argmin()\n            frame = frames[index]\n        keypoints['Tracking'] = self.tracking_df.query(\n            f\"game_play == '{game_play}' and est_frame == {frame}\")[['x', 'y']].values\n        if debug: print(keypoints)\n        if normalized:\n            for k, v in keypoints.items():\n                if len(v)> 0:\n                    keypoints[k] = (v - v.min(axis = 0)) \/ (v.max(axis = 0) - v.min(axis = 0))\n                \n        keypoints['Sideline'][:,1] = 1-keypoints['Sideline'][:,1]\n        \n        keypoints['Players'] = self.tracking_df.query(\n            f\"game_play == '{game_play}' and est_frame == {frame}\")['player'].values\n        \n        keypoints['BBoxes'] = {'Sideline':kpS,'Endzone':kpE}\n                \n        self.keypoints = keypoints\n            \n        return keypoints\n    \n    def plot(self, add_no = False):\n        if not hasattr(self, 'keypoints'):\n            print('you must run the function first...')\n        else:\n            kp = self.keypoints\n            plt.figure(figsize=(12, 6))\n            plt.scatter(kp['Endzone'][:,0], kp['Endzone'][:,1], marker = 'x', color = 'red')\n            plt.scatter(kp['Sideline'][:,0], kp['Sideline'][:,1], marker = '^', color = 'red')\n            plt.scatter(kp['Tracking'][:,0], kp['Tracking'][:,1], marker = 'o', color = 'green')  \n            if add_no:\n                for i in range(len(kp['Tracking'][:,0])):\n                    plt.annotate(i, (kp['Tracking'][i,0], kp['Tracking'][i,1]))\n                for i in range(len(kp['Sideline'][:,0])):\n                    plt.annotate(i, (kp['Sideline'][i,0], kp['Sideline'][i,1]))\n    \nget_kp = get_keypoints()","72366b31":"import torch","bca9af72":"def min_mse(preds, targets):\n    d = torch.cdist(preds.squeeze(2), targets.squeeze(2))\n    loss = (d.min(dim = 1).values**2).mean().sqrt()\n    return loss","e2480d35":"def step(src, trg, m, lr = 3e-3, prt = True):\n    preds = torch.matmul(m, src) # Homography transform\n    loss = min_mse(preds, trg)   # mse between the closes pair of points\n    if prt: print(f'loss: {(loss.item()):.5f}')\n    loss.backward()\n    m.data -= lr * m.grad.data\n    m.grad = None","08dd129e":"def fit_predict(src, trg, init_rot = 0, init_scale = [1,1,1], lr = 3e-3, n_steps = 1000, verbose = True):\n    t = np.pi * init_rot \/ 180\n    m_rot = torch.tensor([[np.cos(t),-np.sin(t), 0],\n                          [np.sin(t), np.cos(t), 0],\n                          [        0,         0, 1]], dtype = torch.double)\n    m_scale = torch.tensor([[init_scale[0], 0, 0],\n                            [0, init_scale[0], 0],\n                            [0, 0, init_scale[0]]], dtype = torch.double)\n    m = m_scale @ m_rot\n    m.requires_grad_()\n    for i in range(n_steps): \n        if not (i % (n_steps\/\/10)) and verbose:\n            step(src, trg, m, lr=lr)\n        else:\n            step(src, trg, m, lr=lr, prt=False)\n            \n    with torch.no_grad():\n        tfm = torch.matmul(m, src)\n        \n    if verbose:\n        plt.scatter(src[:,0], src[:,1], marker = 'o', color = 'red', label = 'source')\n        plt.scatter(trg[:,0], trg[:,1], marker = '^', color = 'green', label = 'target')  \n        plt.scatter(tfm[:,0], tfm[:,1], marker = 'o', color = 'blue', label = 'result')\n        plt.legend();\n        \n    return tfm","1f8e8dac":"def matching(tfm, trg, players):\n\n    d = torch.cdist(tfm[:,:2,0], trg[:,:2,0])\n    \n    greedy_order = d.min(axis = 1).values.argsort()\n    players=players[greedy_order]\n    d = d[greedy_order]\n    players_matched = []\n    for ix, p in enumerate(players):\n        iy = d[ix].argmin().item()\n        players_matched.append(iy)\n        d[:,iy] = np.Inf\n        if (d == np.Inf).all():\n            break\n\n    return players[torch.tensor(players_matched).argsort()]","a582f4d0":"def end2end_prediction(video_frame):\n    \n    game, play, camera, frame = video_frame.split('_')\n    game_play = '_'.join([game, play])\n    frame = int(frame)\n    k = get_kp(game_play, frame)\n    src = torch.cat([torch.tensor(k['Tracking']), torch.ones(len(k['Tracking'])).unsqueeze(1)], axis = -1).unsqueeze(2)\n    trg = torch.cat([torch.tensor(k[camera]), torch.ones(len(k[camera])).unsqueeze(1)], axis = -1).unsqueeze(2)\n    tfm = fit_predict(src, trg, verbose = False)\n    \n    lbls = matching(tfm, trg, k['Players'])\n\n    pred = k[\"BBoxes\"][camera][['video_frame','left','width','top','height']].copy()\n#     print(pred)\n#     print(lbls)\n    pred['label'] = lbls\n    return pred","8a00edd5":"def check_submission(sub):\n    # Maximum of 22 boxes per frame.\n    max_box_per_frame = sub.groupby([\"video_frame\"])[\"label\"].count().max()\n    if max_box_per_frame > 22:\n        print(\"Has more than 22 boxes in a single frame\")\n        return False\n    # Only one label allowed per frame.\n    has_duplicate_labels = sub[[\"video_frame\", \"label\"]].duplicated().any()\n    if has_duplicate_labels:\n        print(\"Has duplicate labels\")\n        return False\n    # Check for unique boxes\n    has_duplicate_boxes = (\n        sub[[\"video_frame\", \"left\", \"width\", \"top\", \"height\"]].duplicated().any()\n    )\n    if has_duplicate_boxes:\n        print(\"Has duplicate boxes\")\n        return False\n    return True","812ab56b":"if fast_sub:\n    print(end2end_prediction('57906_000718_Sideline_1'))","f51526f4":"# sample_sub = pd.read_csv('..\/input\/nfl-health-and-safety-helmet-assignment\/sample_submission.csv')\nvideo_frames = pd.read_csv('..\/input\/nfl-health-and-safety-helmet-assignment\/test_baseline_helmets.csv')['video_frame'].unique()\nif fast_sub:\n    video_frames = video_frames[:16]","d7032fd2":"# preds = []\n# for video_frame in tqdm(video_frames):\n#     preds.append(end2end_prediction(video_frame))","586b7fc3":"preds = Parallel(n_jobs=4)(delayed(end2end_prediction)(x) for x in tqdm(video_frames))","d3b9b6c1":"submission = pd.concat(preds).drop_duplicates(subset=['video_frame', 'label']).reset_index(drop = True)\nsub_ok = check_submission(submission)\nif sub_ok:\n    print('Submission passed, saving it now to submission.csv file...')\n    submission.to_csv('submission.csv', index = False)\nelse:\n    print('Submission FAILED')","07e80cd5":"# Keypoint matching using Pytorch","94cb1bb0":"# Camera-Tracking Matching with Gradient Descent\n\nIf you wanna understand how I came up with this notebook please check the detailed explanation on https:\/\/www.kaggle.com\/coldfir3\/camera-tracking-matching-with-gradient-descent\/edit","717509cb":"## Get predictions"}}