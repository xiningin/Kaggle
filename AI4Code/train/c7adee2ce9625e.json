{"cell_type":{"66b9e689":"code","9b6ab81a":"code","961a9d1a":"code","7f35a44a":"code","ec09e62b":"code","d64fbb43":"code","12f46725":"code","002da9a6":"code","5e0654cf":"code","4c8b8787":"code","cdbc3cc4":"code","289425cb":"code","f6a76305":"code","ff08865b":"code","ea21bab8":"code","ae3c7946":"code","33bfdf4d":"code","d28e56ab":"code","12ba21df":"code","26734a06":"code","1e3cab80":"code","5494a873":"code","79102376":"code","63c93f27":"code","142da4f0":"code","2e96ac21":"code","06cd36c6":"code","fbdf649e":"code","b2c76604":"code","1defb71b":"code","844e88bc":"code","ec3420f1":"code","fc634f79":"code","c0165308":"code","3d9b38c4":"code","3b6b451c":"code","a5326425":"code","6ced0b2f":"code","649b79d2":"code","1d244ce0":"code","8d591998":"code","5e7ceab9":"code","9d34442c":"code","b50e2fb9":"code","9d9166a8":"code","113c15cd":"code","0824bc47":"code","4176f890":"code","b8a8dcbb":"code","3e80539d":"code","ffe51162":"code","cc9e5e39":"code","91c9f9fb":"code","c6a1aad3":"code","c7ba666a":"code","d2e3f24d":"code","b4c38402":"code","34f91098":"code","f4ef14e0":"code","00016414":"code","fb2c833d":"code","45f507ac":"code","8035388b":"code","67e6e586":"code","c3d5f296":"code","393850df":"code","9b0c31a2":"code","270d94ac":"markdown","1e1c279e":"markdown","e4d58e9a":"markdown","06455555":"markdown","972314d1":"markdown","1e132d32":"markdown","54787198":"markdown","f244eea3":"markdown","c973897d":"markdown","80e33d13":"markdown","c3e96d3c":"markdown","a2225ae0":"markdown","12084eee":"markdown","e40e0fe3":"markdown","8f87575b":"markdown","8cd6dd4c":"markdown","41cbc229":"markdown","cd9978fa":"markdown","e2ac8171":"markdown","2cd45c04":"markdown","a160f432":"markdown","e8315f97":"markdown","0909dd01":"markdown","717828a4":"markdown","52661094":"markdown","4ca04230":"markdown","9d24b3b5":"markdown","456284a2":"markdown","d52c695d":"markdown","fef41a27":"markdown","972d29d1":"markdown","7b7b7ad3":"markdown","48911f75":"markdown","19195964":"markdown","7f71416b":"markdown","271d8f8e":"markdown","cfe7b73b":"markdown"},"source":{"66b9e689":"%load_ext autoreload\n%autoreload 2dne dejdeosnd djd sj\n\n%matplotlib inline","9b6ab81a":"import torch\nimport matplotlib.pyplot as plt","961a9d1a":"import ipywidgets as widgets #is to make buttoms and stuff","7f35a44a":"def f(o): print('hi')","ec09e62b":"w = widgets.Button(description='Click me')","d64fbb43":"w","12f46725":"w.on_click(f)","002da9a6":"from time import sleep","5e0654cf":"def slow_calculation():\n    res = 0\n    for i in range(5):\n        res += i*i\n        sleep(1) #wait a secound \n    return res","4c8b8787":"slow_calculation() #so it will take 5 seound to mak the calulation ","cdbc3cc4":"def slow_calculation(cb=None): #cb=None callback we accept that it can take a parameter like funciton and let call it cb \n    res = 0\n    for i in range(5):\n        res += i*i\n        sleep(1) #wait a sec.\n        if cb: cb(i) #if there is a callback (cb) call it and pass in i #it could be the epoch number \n    return res","289425cb":"def show_progress(epoch):\n    print(f\"Awesome! We've finished epoch {epoch}!\")","f6a76305":"#so we take one function and pass it into another\nslow_calculation(show_progress) #it count from 0 to 4 since the res+=i*i dosent influence i only range does that in slow_calculation\n#do also note that it takes 5 sec. ","ff08865b":"slow_calculation() #lambda o: print(f\"Awesome! We've finished epoch {o}!\")","ea21bab8":"def show_progress(exclamation, epoch):\n    print(f\"{exclamation}! We've finished epoch {epoch}!\")","ae3c7946":"#but since we above wrote a function we only used ones, we can rewrite it and use lambda\nslow_calculation(lambda o: show_progress(\"OK I guess\", o)) # #lambda notation is just another way of whriting a function but we only uses it one\n#so insted of 'def' we say 'lambda' and insted of parentheses we write the argument (o) and then what we want it to do. \n#note here that 'show_progress(\"OK I guess\"' is the exclamation and 'o 'is epoch in the show_progress function above. ","33bfdf4d":"#so lets say we want to make a function just takes exclamation we can do the below and we dont want to write the lambda in slow_calculation\n\ndef make_show_progress(exclamation):\n    _inner = lambda epoch: print(f\"{exclamation}! We've finished epoch {epoch}!\") \n    return _inner","d28e56ab":"slow_calculation(make_show_progress(\"Nice!\"))","12ba21df":"\ndef make_show_progress(exclamation):\n    # Leading \"_\" is generally understood to be \"private\"\n    def _inner(epoch): print(f\"{exclamation}! We've finished epoch {epoch}!\")\n    return _inner","26734a06":"slow_calculation(make_show_progress(\"Nice!\"))","1e3cab80":"f2 = make_show_progress(\"Terrific\")","5494a873":"slow_calculation(f2)","79102376":"slow_calculation(make_show_progress(\"Amazing\"))","63c93f27":"from functools import partial","142da4f0":"slow_calculation(partial(show_progress, \"OK I guess\")) #so here we just pass in the function show_progress\n#as the one parameter and then use \"OK I guess\" as an argument for the next parameter  \n#so this now return a new function that just takes one parameter where the secound parameter is always given ","2e96ac21":"f3 = partial(show_progress, \"OK I guess\")","06cd36c6":"# f3() #now this function just takes one parameter which is epoch. since show_progress took to parameters one for epoch and the \n#secound was exclamation. but now exclamation is alwas \"OK I guess\". so the f2 only take the one parameter (epoch)","fbdf649e":"f3(1)","b2c76604":"class ProgressShowingCallback():\n    def __init__(self, exclamation=\"Awesome\"): self.exclamation = exclamation #same a last lecture but we just store \n        #the exclamation value in a function\n    def __call__(self, epoch): print(f\"{self.exclamation}! We've finished epoch {epoch}!\") #__call__ taking a objeckt(class-->ProgressShowingCallback) \n        #and treat it as if it was a function ","1defb71b":"cb = ProgressShowingCallback(\"Just super\")","844e88bc":"#so we can call ProgressShowingCallback as if it was a function with paratenthess\n# cb('hi')","ec3420f1":"slow_calculation(cb)","fc634f79":"def f(*args, **kwargs): print(f\"args: {args}; kwargs: {kwargs}\")","c0165308":"f(3, 'a', thing1=\"hello\")","3d9b38c4":"f(3, 'a','b',9, thing1=\"hello\", nine=9) #do remember as it is right here, the position of which you pass the argument er importen","3b6b451c":"def slow_calculation(cb=None):\n    res = 0\n    for i in range(5):\n        if cb: cb.before_calc(i) #i we use it in PrintStatusCallback but not in PrintStepCallback\n        res += i*i\n        sleep(1)\n        if cb: cb.after_calc(i, val=res) #i, val=res we use them in PrintStatusCallback but not in PrintStepCallback\n    return res","a5326425":"class PrintStepCallback():\n    def __init__(self): pass\n    def before_calc(self, *args, **kwargs): print(f\"About to start\") #even though we dont use the arguments given from slow_calculation\n        # we still have to make place for them and we do that with *args, **kwargs. \n    def after_calc (self, *args, **kwargs): print(f\"Done step\")","6ced0b2f":"slow_calculation(PrintStepCallback())","649b79d2":"class PrintStatusCallback():\n    def __init__(self): pass\n    def before_calc(self, epoch, **kwargs): print(f\"About to start: {epoch}\") #here we add **kwargs to make sure the code doesnt break if another argument are add to the function in the fucture\n    def after_calc (self, epoch, val, **kwargs): print(f\"After {epoch}: {val}\") #but note we still use the arguments given from the function (slow_calculation)","1d244ce0":"slow_calculation(PrintStatusCallback())","8d591998":"#early stopping \ndef slow_calculation(cb=None):\n    res = 0\n    for i in range(5):\n        if cb and hasattr(cb,'before_calc'): cb.before_calc(i) #check if 'before_calc' exsist and call it if it is\n        res += i*i\n        sleep(1)\n        if cb and hasattr(cb,'after_calc'): #chack if there is callback(cb) called 'after_calc' and only call it is it is \n            if cb.after_calc(i, res): #check the return value \n                print(\"stopping early\") #and do something based on the returned value \n                break\n    return res","5e7ceab9":"class PrintAfterCallback():\n    def after_calc (self, epoch, val):\n        print(f\"After {epoch}: {val}\")\n        if val>10: return True #cancel if our loop if the 'val' is greater then 10  \n","9d34442c":"slow_calculation(PrintAfterCallback()) #and here we see that it stops at 14, since 14 is greater then 10 ","b50e2fb9":"#change the calulation \nclass SlowCalculator():\n    def __init__(self, cb=None): self.cb,self.res = cb,0 #defines what we need \n    \n    #to use calc(function in this class) in ModifyingCallback we have to make the below function \n    def callback(self, cb_name, *args): #here u can also use __call__ and in the calc function u can just use self insted of self.callback\n        if not self.cb: return #check to see if the given callback is defined \n        cb = getattr(self.cb,cb_name, None) #if it is it will grap it ...\n        if cb: return cb(self, *args) #... and pass it into the calulator object itself (self) \n    \n    #so we take our calulation function and putting it into a class so now the value it is calulation(res) is a attribute of the class \n    def calc(self):\n        for i in range(5):\n            self.callback('before_calc', i)\n            self.res += i*i\n            sleep(1)\n            if self.callback('after_calc', i):\n                print(\"stopping early\")\n                break","9d9166a8":"class ModifyingCallback():\n    def after_calc (self, calc, epoch): #note the calculator (calc) functions calls on this funtions \n        print(f\"After {epoch}: {calc.res}\")\n        if calc.res>10: return True #so we can now go into the calulator function and stop if the result gets greater then 10\n        if calc.res<3: calc.res = calc.res*2 #and we can double the result by multipying with 2 if it is less then 3 ","113c15cd":"calculator = SlowCalculator(ModifyingCallback()) #for the changes from ModifyingCallback to be valid we pass it into the SlowCalculator class","0824bc47":"calculator.calc() #have to call it like it is a class\ncalculator.res","4176f890":"# __dunder__ thingles","b8a8dcbb":"#exsampel\nclass SloppyAdder():\n    def __init__(self,o): self.o=o #construck o \n    def __add__(self,b): return SloppyAdder(self.o + b.o + 0.01) #add o with b + 0.01 since it is sloppy\n    def __repr__(self): return str(self.o)#printing ","3e80539d":"a = SloppyAdder(1)\nb = SloppyAdder(2)\na+b","ffe51162":"t = torch.tensor([1.,2.,4.,18])","cc9e5e39":"m = t.mean(); m","91c9f9fb":"(t-m).mean()","c6a1aad3":"(t-m).pow(2).mean() #taking the power of 2 to the difference for a veribel and the mean(m) to the mean of it all ","c7ba666a":"(t-m).abs().mean() #taking the absolut value of the  difference for a veribel and the mean(m) to the mean of it all ","d2e3f24d":"(t-m).pow(2).mean().sqrt() #we have to take the \"kvadratrod\" to get to the right scale again","b4c38402":"(t-m).pow(2).mean(), (t*t).mean() - (m*m)","34f91098":"t #we use same data from variance ","f4ef14e0":"#exsampel\n# `u` is twice `t`, plus a bit of randomness\nu = t*2 #multiply t with 2 and set it = to u\nu *= torch.randn_like(t)\/10+0.95 #and put in some random noise \n\nplt.scatter(t, u); #plot","00016414":"prod = (t-t.mean())*(u-u.mean()); prod #so now let compare the difference from 't' and its mean. With the difference in u and its mean and let multiply them together","fb2c833d":"prod.mean() #and let take the mean of that ","45f507ac":"#so now lets take some random in a new verible and call it 'v'\nv = torch.randn_like(t)\nplt.scatter(t, v); #plot 't' and 'v'","8035388b":"((t-t.mean())*(v-v.mean())).mean() #and let us calulate the same product as before and take the mean of it \n#we see that this new number is much smaller then before (tensor(105.0522))","67e6e586":"cov = (t*v).mean() - t.mean()*v.mean(); cov","c3d5f296":"cov \/ (t.std() * v.std())","393850df":"def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()","9b0c31a2":"import numpy\n#n : int or array_like of ints\n#p : float or array_like of floats\n\ndef binomial(n,p):numpy.random.binomial(n, p, size=None)\n    ","270d94ac":"Let's see that in code. So now we need two vectors.","1e1c279e":"* the button widget is used to handle mouse clicks. The on_click method of the Button can be used to register function to be called when the button is clicked","e4d58e9a":"## This is last part of fastai part 1 lesson 9, though it is mixed with my notes, eksperiments and what I found usefull, if you want the pure version, check fastai github or the following link: https:\/\/github.com\/fastai\/course-v3\/tree\/master\/nbs\/dl2","06455555":"\nwhich is:\n\n$$\\hbox{logsoftmax(x)}_{i} = x_{i} - \\log \\sum_{j} e^{x_{j}}$$\nAnd our cross entropy loss is:$$-\\log(p_{i})$$\n\n","972314d1":"\nSpecial methods you should probably know about (see data model link above) are:\n\n     __getitem__\n     __getattr__\n     __setattr__\n     __del__\n     __init__\n     __new__\n     __enter__\n     __exit__\n     __len__\n     __repr__\n     __str__","1e132d32":"so softmax is not good for images with more object in it. Or if there is non of the object you are looking for. \nmany use softmax anyhow since imagenet has tough us to do so. but non the less mistaken (imagenet has a picture with one object in it, so here is softmax good) since there in real life there can be more then one object in each picture. so softmax will nomatter what make a prediction and that there fx is a fish in the picture with high confidens even if its a picture of the color yellow. ","54787198":"## Browsing source code\n\n\n* Jump to tag\/symbol by with (with completions)\n* Jump to current tag\n* Jump to library tags\n* Go back\n* Search\n* Outlining \/ folding","f244eea3":"So every thing pass as a positional arguments (number, string) will *args turns it into a tuble and **kwargs turns the keyword arguments (ten=10, nine=9) into a dictionary ","c973897d":"![image.png](attachment:image.png)","80e33d13":"the next thing we want write callback and to change something ","c3e96d3c":"## __dunder__ thingies","a2225ae0":"\nThey're still different. Why?\n\nNote that we have one outlier (18). In the version where we square everything, it makes that much bigger than everything else.\n\n(t-m).pow(2).mean() is refered to as variance. It's a measure of how spread out the data is, and is particularly sensitive to outliers.\n\nWhen we take the sqrt of the variance, we get the standard deviation. Since it's on the same kind of scale as the original data, it's generally more interpretable. However, since sqrt(1)==1, it doesn't much matter which we use when talking about unit variance for initializing neural nets.\n\n(t-m).abs().mean() is referred to as the mean absolute deviation. It isn't used nearly as much as it deserves to be, because mathematicians don't like how awkward it is to work with. But that shouldn't stop us, because we have computers and stuff.\n\nHere's a useful thing to note about variance:","12084eee":"## Softmax\nHere's our final logsoftmax definition:","e40e0fe3":"You can see why these are equal if you want to work thru the algebra. Or not.\n\nBut, what's important here is that the latter is generally much easier to work with. In particular, you only have to track two things: the sum of the data, and the sum of squares of the data. Whereas in the first form you actually have to go thru all the data twice (once to calculate the mean, once to calculate the differences).\n\nLet's go steal the LaTeX from Wikipedia:\n\n$$\\operatorname{E}\\left[X^2 \\right] - \\operatorname{E}[X]^2$$\n\n## Covariance and correlation\nHere's how Wikipedia defines covariance:\n\n$$\\operatorname{cov}(X,Y) = \\operatorname{E}{\\big[(X - \\operatorname{E}[X])(Y - \\operatorname{E}[Y])\\big]}$$","8f87575b":"## Multiple callback funcs; *args and **kwargs","8cd6dd4c":"\nBut the first of these is now a totally different scale, since we squared. So let's undo that at the end.","41cbc229":"## Callbacks as callable classes","cd9978fa":"![image.png](attachment:image.png)\n\n     1.02\/(1-1.02)","e2ac8171":"so softmax \n![image.png](attachment:image.png)","2cd45c04":"\nIt's generally more conveniently defined like so:\n\n$$\\operatorname{E}\\left[X Y\\right] - \\operatorname{E}\\left[X\\right] \\operatorname{E}\\left[Y\\right]$$","a160f432":"\nFrom now on, you're not allowed to look at an equation (or especially type it in LaTeX) without also typing it in Python and actually calculating some values. Ideally, you should also plot some values.\n\nFinally, here is the Pearson correlation coefficient:\n\n$$\\rho_{X,Y}= \\frac{\\operatorname{cov}(X,Y)}{\\sigma_X \\sigma_Y}$$","e8315f97":"# Callbacks\n## Callbacks as GUI events","0909dd01":"It is so normal we want to only wanno give a function one parameter but 2 are regured so \nwe can use partial to solve this problem ","717828a4":"Anything that looks like \n\n\n    __this__ \nis, in some way, special. Python, or some library, can define some functions that they will call at certain documented times. For instance, when your class is setting up a new object, python will call \n\n    __init__\nThese are defined as part of the python data model.\n\nFor instance, if python sees +, then it will call the special method \n\n    __add__\nIf you try to display an object in Jupyter (or lots of other places in Python) it will call \n\n    __repr__","52661094":"\nNB: When callbacks are used in this way they are often called \"events\".\n\nDid you know what you can create interactive apps in Jupyter with these widgets? Here's an example from plotly:","4ca04230":"## Creating your own callback","9d24b3b5":"NB: We've been guilty of over-using kwargs in fastai - it's very convenient for the developer, but is annoying for the end-user unless care is taken to ensure docs show all kwargs too. kwargs can also hide bugs (because it might not tell you about a typo in a param name). In R there's a very similar issue (R uses ... for the same thing), and matplotlib uses kwargs a lot too.","456284a2":"## Lambdas and partials","d52c695d":"insted of using softmax when there are more object or non-objects in pictures. here we should use binomial.\nSince better for this case. it check fx how much fish there is in the picture and this can be none or very close to none ","fef41a27":"so output is just the last activation for an image here. \nthe exp is just the 'e' to the power of the last activation. in the buttom is the sum af exp at 12.70\nso softmax is just 'e' to the power of the last activation divided by the sum of all exp. \nexsampel: \ncat has a last activiation on 0.002 in this image \nexp = e^0.002 (EXP(0.002))= 1.02\nsoftmax = 1.02\/12.70 = 0.08 \n","972d29d1":"    (t*t).mean() - (m*m) #this is the math formular from below. And we want to use this veriation since it only go through the dataset ones while the other (traditunal) method goes through the dataset twice ","7b7b7ad3":"## Variance and stuff\n### Variance\nVariance is the average of how far away each data point is from the mean. E.g.:","48911f75":"the reason there is a big difference from the to coveriance is because in the first case it was all positive numbers in a line, so when we add them together they just give a big positive number. But in secound case the numbers was random and also choud be neagive, so this will give a very different result.","19195964":"So the to good methods to find variance are\n\n    (t-m).abs().mean()  #called mean absolut diviation \n\nand \n\n    (t-m).pow(2).mean().sqrt() #this is also called the standard diviation (std)\n","7f71416b":"\nOops. We can't do that. Because by definition the positives and negatives cancel out. So we can fix that in one of (at least) two ways:","271d8f8e":"## Modifying behavior","cfe7b73b":"It's just a scaled version of the same thing. Question: Why is it scaled by standard deviation, and not by variance or mean or something else?"}}