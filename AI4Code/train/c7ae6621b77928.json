{"cell_type":{"44a2e7d5":"code","d64c4b8a":"code","1c0a8d12":"code","8463a413":"code","090a11dc":"code","934212f1":"code","e074b091":"code","85d7449f":"code","2dfe5376":"code","b9bb47aa":"markdown","7503250a":"markdown","94302c65":"markdown","90494238":"markdown","fb7aac69":"markdown","df190d70":"markdown"},"source":{"44a2e7d5":"\n\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\ndata = pd.read_csv(\"\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv\")\nprint(data.head())\n\n\n    ","d64c4b8a":"y = data['status'].tolist()\nctr1 = 0\nctr2 = 0\nY = []\nfor i in range(0,len(y)):\n    if y[i]=='Placed':\n        ctr1 = ctr1 + 1\n        Y.append(1)\n    else:\n        ctr2 = ctr2 + 1\n        Y.append(0)\nctr_1 = [ctr1, ctr2]\nlabels1 = ['placed', 'not placed']\nypos1 = np.arange(len(labels1))\nplt.xticks(ypos1, labels1)\nplt.ylabel('Number')\nplt.bar(ypos1, ctr_1)\nplt.show()\n\n","1c0a8d12":"ctr3 = 0\nctr4 = 0\ngen = data['gender'].tolist()\nfor i in range(0,len(gen)):\n    if gen[i] == 'M':\n            if Y[i] == 1:\n                ctr3 = ctr3 + 1\n    elif gen[i] == 'F':\n        if Y[i] == 1:\n               ctr4 = ctr4 + 1\nctr_2 = [ctr3 ,ctr4]\nlabels2 = ['Male', 'Female']\nypos2 = np.arange(len(labels2))\nplt.xticks(ypos2, labels2)\nplt.ylabel('Placed number')\nplt.bar(ypos2, ctr_2, color = ['red', 'green'])\n\nplt.show()","8463a413":"ctr5 = 0\nctr6 = 0\nctr7 = 0\nst = data['hsc_s'].tolist()\nfor i in range(0,len(st)):\n    if (st[i].lower()) == 'commerce':\n        if Y[i] == 1:\n            ctr5 = ctr5 + 1\n    elif (st[i].lower()) == 'science':\n        if Y[i] == 1:\n            ctr6 = ctr6 + 1\n    elif (st[i].lower()) == 'arts':\n        if Y[i] == 1:\n            ctr7 = ctr7 + 1\nctr_3 = [ctr5, ctr6, ctr7]\nlabels3 = ['Commerce', 'Science', 'Arts']\nypos3 = np.arange(len(labels3))\nplt.xticks(ypos3, labels3)\nplt.ylabel('Placed number')\nplt.bar(ypos3, ctr_3, color = ['red', 'green', 'blue'])","090a11dc":"deg = data['degree_t'].tolist()\nctr7 = 0\nctr8 = 0\nfor i in range(0,len(deg)):\n    if (deg[i].lower()) == 'sci&tech':\n        if Y[i] == 1:\n            ctr7 = ctr7 + 1\n    elif (deg[i].lower()) == 'comm&mgmt':\n        if Y[i] == 1:\n            ctr8 = ctr8 + 1\nctr_4 = [ctr7, ctr8]\nlabels4 = ['Sci&tech', 'Comm&mgmt']\nypos4 = np.arange(0,len(labels4))\nplt.xticks(ypos4, labels4)\nplt.ylabel(\"Placed number\")\nplt.bar(ypos4, ctr_4, color = ['red', 'green'])","934212f1":"#Numberical data\n\nprint(data.head())\nX1 = data[['gender', 'ssc_b','hsc_b','hsc_s','degree_t', 'workex', 'specialisation']]\nX1 = pd.get_dummies(X1)\nprint(X1.shape)\n","e074b091":"X2 = data[['ssc_p','hsc_p', 'degree_p','etest_p', 'mba_p']]\n\n#concat the data\n\nX = pd.concat([X1, X2], axis=1, sort=False)\n\ny = pd.DataFrame(Y) # 1 represents placed and 0 represents not placed\n\n","85d7449f":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics, preprocessing\nfrom sklearn.feature_selection import SelectFromModel\n\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2)  # Spliting the data\nX_train = preprocessing.StandardScaler().fit(X_train).transform(X_train)   # Preprocessing the data\nX_test = preprocessing.StandardScaler().fit(X_test).transform(X_test)\nLR = LogisticRegression()\nLR.fit(X_train, y_train)            # Fiting the logistic regression\nyhat = LR.predict(X_test)\nprint(\"Logistic regression accuracy:\", metrics.accuracy_score(y_test, yhat)) #Finding out the accuracy\nprint(X_train)\n#Feature selection using L1 regularization\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\nsel = SelectFromModel(LogisticRegression())\nsel.fit(X_train, y_train)\nselected_feat = X_train.columns[(sel.get_support())]\nprint(\"Optimum number of features from L1 regularisation:\", len(selected_feat))\nX_train_lasso = sel.fit_transform(X_train, y_train)\nX_test_lasso = sel.transform(X_test)\nmdl_lasso = LogisticRegression()\nmdl_lasso.fit(X_train_lasso, y_train)\nscore_lasso = mdl_lasso.score(X_test_lasso, y_test)\nprint(\"Score with L1 regularisation:\",score_lasso)\n\n","2dfe5376":"from sklearn.svm import SVC\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2)  # Spliting the data\nX_train = preprocessing.StandardScaler().fit(X_train).transform(X_train)   # Preprocessing the data\nX_test = preprocessing.StandardScaler().fit(X_test).transform(X_test)\nmdl = SVC(gamma='auto')\nmdl.fit(X_train, y_train)\nyhat_svm = mdl.predict(X_test)\nprint(\"Support vector machine accuracy:\", metrics.accuracy_score(yhat_svm, y_test))\nsvc_accuracy = metrics.accuracy_score(yhat_svm, y_test)","b9bb47aa":"In the next block I will try to fit the logistic regression and will carry out feature selection","7503250a":"This problem is a  classification problem \nVarious classification alogrithms are:\n1. Logistic regression\n2. Support vector machines(SVM)\n3. Clustering\n4. K-mean alogrithm\netc\nIn the data we are provided with label dataset hence it is a case of supervised\nLogistic regression and SVM could be used to find out the required classes","94302c65":"In the above code I have done one-hot encoding so as to deal with categorical data\nOne-hot encoding is a necessary task because:\n1. Models can only take numerical values\n2. We have non-ordinal data\n\nIn the next block of code I will extract numberical data and then concatenate the datas","90494238":"The data consist of both categorical and numerical and thus must be cleaned before predictiona","fb7aac69":"As visible from the above code blocks both Support vector machine and logistic regression models showed reasonable accuracy\nHence any of the model can be used while doing classification in this case.","df190d70":"From the above code it was clear that the model didn't required any feature selection \nLogistic regression provied reasonable accuracy\n\nIn the next block I will try to fit SVM"}}