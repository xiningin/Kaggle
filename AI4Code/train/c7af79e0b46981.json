{"cell_type":{"377a7dc7":"code","4318d404":"code","61442993":"code","2259121f":"code","b0f43d9b":"code","436814fc":"code","19fb1514":"code","5a9f2328":"code","c5d5ee2d":"code","531f0247":"code","7bd07135":"code","0dcb62c7":"code","e76da8f8":"code","b2a7b91e":"code","b3197667":"code","b28ed9e2":"code","868cf4c3":"code","71c4e141":"code","eeea4c45":"code","d77ba33f":"code","45214cd6":"markdown","1b933677":"markdown","fc6005d7":"markdown","676dbb40":"markdown","0a5cb8ca":"markdown","7dce9c29":"markdown","6fc5d337":"markdown","63f8be79":"markdown","364a7d78":"markdown","60afa47b":"markdown","32a4606d":"markdown","03478772":"markdown","82373631":"markdown","465ee2c8":"markdown","ef62cee7":"markdown","75a22004":"markdown","695401fd":"markdown","299989a2":"markdown","7d84431e":"markdown","70613003":"markdown","dfd90dd2":"markdown","0085b795":"markdown"},"source":{"377a7dc7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndata = pd.read_json(\"\/kaggle\/input\/covid-patient-datasets\/covid.json\")","4318d404":"data.info()\ndata.head(500)","61442993":"normalData = data.drop(['#',\"age\"],axis = 1)\nfor param in normalData:\n    normalData[param] = normalData[param].apply(lambda x: 0 if x=='no' else 1)\n    #normalData[param] = normalData[param].map(dict(yes=1, no=0))\nnormalData.head(500)","2259121f":"normalData.drop_duplicates(inplace=True)\nnormalData.head(500)","b0f43d9b":"normalData[\"result\"] = 1\nnormalData.head(500)","436814fc":"from itertools import product\n\nuniques = [normalData[i].unique().tolist() for i in normalData.columns ]\n\n# conver lable to 0\nuniques[-1][0] = 0\nfalseData = pd.DataFrame(product(*uniques), columns = normalData.columns)\n\n\nfinalData = normalData.append(falseData)\nfinalData.head(5000000000)","19fb1514":"feature_cols = finalData.columns;\n#remove the result column\nfeature_cols =feature_cols[:-1]\n\nfinalData.drop_duplicates(feature_cols,inplace=True)\nfinalData.head(5000000000)","5a9f2328":"finalData.info()\nfinalData.describe().transpose()","c5d5ee2d":"from math import log2\n\n# calculate the entropy \ndef entropy(trueResultCount,falseResultCount):  \n    p1 = trueResultCount \/ (trueResultCount + falseResultCount)\n    p2 = falseResultCount \/ (trueResultCount + falseResultCount)\n    return -((p1) * log2(p1) + (p2) * log2(p2))\n\n# calculate the gain of item \ndef gain(atr):\n    trueResultCount = finalData[finalData[atr] == 1][atr].count() \n    falseResultCount = finalData[finalData[atr] == 0][atr].count() \n    \n    trueWhenResultIsTrueCount = finalData.loc[(finalData[atr] == 1) & (finalData['result'] == 1)][atr].count()\n    falseWhenResultIsTrueCount = finalData.loc[(finalData[atr] == 0) & (finalData['result'] == 1)][atr].count() \n    trueWhenResultIsFalseCount = finalData.loc[(finalData[atr] == 1) & (finalData['result'] == 0)][atr].count()\n    falseWhenResultIsFalseCount = finalData.loc[(finalData[atr] == 0) & (finalData['result'] == 0)][atr].count() \n    \n    return entropyS - ((trueWhenResultIsTrueCount + trueWhenResultIsFalseCount) \/ (trueResultCount + falseResultCount)) * entropy(trueWhenResultIsTrueCount, trueWhenResultIsFalseCount) + ((falseWhenResultIsTrueCount + falseWhenResultIsFalseCount) \/ (trueResultCount + falseResultCount)) * entropy(falseWhenResultIsTrueCount, falseWhenResultIsFalseCount)\n  \n    \n #### ------------------ ####   \n    \ntrueResultCount = finalData[finalData.result == 1].result.count()  \nfalseResultCount = finalData[finalData.result == 0].result.count()\n\n## system entropy\nentropyS = round(entropy(trueResultCount, falseResultCount),8)\n\n\n## list of gain attributes\ngainList = []\nfor atr in feature_cols:\n    gainResult = gain(atr)\n    gainList.append([atr,round(gainResult, 8)])","531f0247":"print(\"system entropy: \",entropyS)\ndf = pd.DataFrame(gainList, columns =['attribute', 'gain']) \ndf = df.sort_values(by='gain', ascending=False)\ndf.head(50)","7bd07135":"#training function to implement find-s algorithm\ndef FindS(c,t):\n    for i, val in enumerate(t):\n        if val == 1:\n            specific_hypothesis = c[i].copy()\n            break\n             \n    for i, val in enumerate(c):\n        if t[i] == 1:\n            for x in range(len(specific_hypothesis)):\n                if val[x] != specific_hypothesis[x]:\n                    specific_hypothesis[x] = -1\n                else:\n                    pass\n    \n    # replace =1 with ? and return\n    return  [x if x != -1 else '?' for x in specific_hypothesis] ","0dcb62c7":"# make concept and target (array)\ntarget = np.array(finalData.result) \nconcept = np.array(finalData.drop(['result'], axis=1))\n\n\n# making an array of all the attributes\nd = [None] * len(finalData.columns)\n\nh = FindS(concept,target)\nprint(h)","e76da8f8":"def CE(concepts, target): \n    \n    specific_h = concepts[0].copy()\n\n    print(\"Initialization of specific_h and general_h\")\n\n    print(\"specific_h: \",specific_h)\n\n    general_h = [[\"?\" for i in range(len(specific_h))] for i in range(len(specific_h))]\n\n    print(\"general_h: \",general_h)\n\n    print(\"concepts: \",concepts)\n\n    for i, h in enumerate(concepts):\n\n        if target[i] == \"yes\":\n\n            for x in range(len(specific_h)):\n\n                #print(\"h[x]\",h[x])\n\n                if h[x] != specific_h[x]:\n\n                    specific_h[x] = '?'\n\n                    general_h[x][x] = '?'\n\n        if target[i] == \"no\":\n\n            for x in range(len(specific_h)):\n\n                if h[x] != specific_h[x]:\n\n                    general_h[x][x] = specific_h[x]\n\n                else:\n\n                    general_h[x][x] = '?'\n\n    print(\"\\nSteps of Candidate Elimination Algorithm: \",i+1)\n\n    print(\"Specific_h: \",i+1)\n\n    print(specific_h,\"\\n\")\n\n    print(\"general_h :\", i+1)\n\n    print(general_h)\n\n    indices = [i for i, val in enumerate(general_h) if val == ['?', '?', '?', '?', '?', '?']]\n\n    print(\"\\nIndices\",indices)\n\n    for i in indices:\n\n        general_h.remove(['?', '?', '?', '?', '?', '?'])\n\n    return specific_h, general_h\n\n\n\n\n\n\n\ns_final, g_final = CE(concept,target)\n\nprint(\"Final Specific_h: \", s_final, sep=\"\\n\")\nprint(\"Final General_h: \", g_final, sep=\"\\n\")","b2a7b91e":"X = finalData.drop(['result'],axis = 1)\ny = finalData.result.values\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","b3197667":"import matplotlib.pyplot as plt\n\n\n\n# Training the Naive Bayes model on the Training set\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nac = accuracy_score(y_test,y_pred)\ncm = confusion_matrix(y_test, y_pred)\n\nprint(\"NB accuracy score: \", ac )","b28ed9e2":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors=3)\n#classifier.fit(X_train, y_train)\n\n##Predict the response for test dataset\n#y_pred = classifier.predict(X_test)\n\n##Import scikit-learn metrics module for accuracy calculation\n#from sklearn import metrics\n## Model Accuracy, how often is the classifier correct?\n#print(\"Knn Accuracy: \",metrics.accuracy_score(y_test, y_pred))","868cf4c3":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=3).fit(finalData)\ncentroids = kmeans.cluster_centers_\nprint(centroids)\n\n# plt.scatter(finalData['x'], finalData['y'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)\n# plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)\n# plt.show()","71c4e141":"from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n\n# Create Decision Tree classifer object\nclf = DecisionTreeClassifier()\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,y_train)\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"DecisionTree accuracy: \",metrics.accuracy_score(y_test, y_pred))","eeea4c45":"import math\nfrom collections import deque\n\nclass Node:\n    \"\"\"Contains the information of the node and another nodes of the Decision Tree.\"\"\"\n\n    def __init__(self):\n        self.value = None\n        self.next = None\n        self.childs = None\n\n\nclass DecisionTreeClassifier:\n    \"\"\"Decision Tree Classifier using ID3 algorithm.\"\"\"\n\n    def __init__(self, X, feature_names, labels):\n        self.X = X\n        self.feature_names = feature_names\n        self.labels = labels\n        self.labelCategories = list(set(labels))\n        self.labelCategoriesCount = [list(labels).count(x) for x in self.labelCategories]\n        self.node = None\n        self.entropy = self._get_entropy([x for x in range(len(self.labels))])  # calculates the initial entropy\n\n    def _get_entropy(self, x_ids):\n        \"\"\" Calculates the entropy.\n        Parameters\n        __________\n        :param x_ids: list, List containing the instances ID's\n        __________\n        :return: entropy: float, Entropy.\n        \"\"\"\n        # sorted labels by instance id\n        labels = [self.labels[i] for i in x_ids]\n        # count number of instances of each category\n        label_count = [labels.count(x) for x in self.labelCategories]\n        # calculate the entropy for each category and sum them\n        entropy = sum([-count \/ len(x_ids) * math.log(count \/ len(x_ids), 2) if count else 0 for count in label_count])\n        return entropy\n\n    def _get_information_gain(self, x_ids, feature_id):\n        \"\"\"Calculates the information gain for a given feature based on its entropy and the total entropy of the system.\n        Parameters\n        __________\n        :param x_ids: list, List containing the instances ID's\n        :param feature_id: int, feature ID\n        __________\n        :return: info_gain: float, the information gain for a given feature.\n        \"\"\"\n        # calculate total entropy\n        info_gain = self._get_entropy(x_ids)\n        # store in a list all the values of the chosen feature\n        x_features = [self.X[x][feature_id] for x in x_ids]\n        # get unique values\n        feature_vals = list(set(x_features))\n        # get frequency of each value\n        feature_vals_count = [x_features.count(x) for x in feature_vals]\n        # get the feature values ids\n        feature_vals_id = [\n            [x_ids[i]\n            for i, x in enumerate(x_features)\n            if x == y]\n            for y in feature_vals\n        ]\n\n        # compute the information gain with the chosen feature\n        info_gain = info_gain - sum([val_counts \/ len(x_ids) * self._get_entropy(val_ids)\n                                     for val_counts, val_ids in zip(feature_vals_count, feature_vals_id)])\n\n        return info_gain\n\n    def _get_feature_max_information_gain(self, x_ids, feature_ids):\n        \"\"\"Finds the attribute\/feature that maximizes the information gain.\n        Parameters\n        __________\n        :param x_ids: list, List containing the samples ID's\n        :param feature_ids: list, List containing the feature ID's\n        __________\n        :returns: string and int, feature and feature id of the feature that maximizes the information gain\n        \"\"\"\n        # get the entropy for each feature\n        features_entropy = [self._get_information_gain(x_ids, feature_id) for feature_id in feature_ids]\n        # find the feature that maximises the information gain\n        max_id = feature_ids[features_entropy.index(max(features_entropy))]\n\n        return self.feature_names[max_id], max_id\n\n    def id3(self):\n        \"\"\"Initializes ID3 algorithm to build a Decision Tree Classifier.\n        :return: None\n        \"\"\"\n        x_ids = [x for x in range(len(self.X))]\n        feature_ids = [x for x in range(len(self.feature_names))]\n        self.node = self._id3_recv(x_ids, feature_ids, self.node)\n        print('')\n\n    def _id3_recv(self, x_ids, feature_ids, node):\n        \"\"\"ID3 algorithm. It is called recursively until some criteria is met.\n        Parameters\n        __________\n        :param x_ids: list, list containing the samples ID's\n        :param feature_ids: list, List containing the feature ID's\n        :param node: object, An instance of the class Nodes\n        __________\n        :returns: An instance of the class Node containing all the information of the nodes in the Decision Tree\n        \"\"\"\n        if not node:\n            node = Node()  # initialize nodes\n        # sorted labels by instance id\n        labels_in_features = [self.labels[x] for x in x_ids]\n        # if all the example have the same class (pure node), return node\n        if len(set(labels_in_features)) == 1:\n            node.value = self.labels[x_ids[0]]\n            return node\n        # if there are not more feature to compute, return node with the most probable class\n        if len(feature_ids) == 0:\n            node.value = max(set(labels_in_features), key=labels_in_features.count)  # compute mode\n            return node\n        # else...\n        # choose the feature that maximizes the information gain\n        best_feature_name, best_feature_id = self._get_feature_max_information_gain(x_ids, feature_ids)\n        node.value = best_feature_name\n        node.childs = []\n        # value of the chosen feature for each instance\n        feature_values = list(set([self.X[x][best_feature_id] for x in x_ids]))\n        # loop through all the values\n        for value in feature_values:\n            child = Node()\n            child.value = value  # add a branch from the node to each feature value in our feature\n            node.childs.append(child)  # append new child node to current node\n            child_x_ids = [x for x in x_ids if self.X[x][best_feature_id] == value]\n            if not child_x_ids:\n                child.next = max(set(labels_in_features), key=labels_in_features.count)\n                print('')\n            else:\n                if feature_ids and best_feature_id in feature_ids:\n                    to_remove = feature_ids.index(best_feature_id)\n                    feature_ids.pop(to_remove)\n                # recursively call the algorithm\n                child.next = self._id3_recv(child_x_ids, feature_ids, child.next)\n        return node\n\n    def printTree(self):\n        if not self.node:\n            return\n        nodes = deque()\n        nodes.append(self.node)\n        while len(nodes) > 0:\n            node = nodes.popleft()\n            print(node.value)\n            if node.childs:\n                for child in node.childs:\n                    print('({})'.format(child.value))\n                    nodes.append(child.next)\n            elif node.next:\n                print(node.next)","d77ba33f":"tree = DecisionTreeClassifier(X=concept, feature_names=finalData.columns, labels=target)\nprint(\"System entropy {:.8f}\".format(tree.entropy))\ntree.printTree()","45214cd6":"\u06af\u0627\u0645 \u0627\u0648\u0644 \u0628\u0627 \u062c\u0645\u0639 \u0622\u0648\u0631\u06cc \u062f\u0627\u062f\u0647 \u0648 \u0646\u0631\u0645\u0627\u0644 \u06a9\u0631\u062f\u0646 \u0648 \u0627\u06cc\u062c\u0627\u062f \u0631\u06a9\u06cc\u0628 \u0647\u0627\u06cc \u0645\u0648\u0686\u0648\u062f \u062a\u0645\u0627\u0645 \u0634\u062f","1b933677":"## 6-  \u0645\u062d\u0627\u0633\u0628\u0647 \u0627\u0644\u06af\u0648\u0631\u06cc\u062a\u0645 \u06a9\u0644\u0627\u0633\u062a\u0631\u06cc\u0646\u06af","fc6005d7":"\u0647\u0645\u0627\u0646 \u0637\u0648\u0631 \u06a9\u0647 \u062f\u0631 \u062c\u062f\u0648\u0644 \u0622\u0645\u062f\u0647 \u0627\u0633\u062a 5 \u0648\u06cc\u0698\u06af\u06cc \u0622\u062e\u0631 \u0627\u0632 \u06a9\u0645 \u0627\u0647\u0645\u06cc\u062a \u062a\u0631\u06cc\u0646 \u0648\u06cc\u0698\u06af\u06cc \u0647\u0627 \u0645\u06cc \u0628\u0627\u0634\u0646\u062f","676dbb40":"\u062d\u0627\u0644 \u0628\u0627\u06cc\u062f \u062a\u0645\u0627\u0645\u06cc \u062a\u0631\u06a9\u06cc\u0628\u0627\u062a \u0631\u0627 \u0628\u062f\u0633\u062a \u0628\u06cc\u0627\u0648\u0631\u06cc\u0645 \u0648 \u0622\u0646 \u0647\u0627\u0627\u06cc\u06cc \u06a9\u0647 \u062f\u0631 \u062f\u0627\u062f\u0647 \u0647\u0627 \u0646\u06cc\u0633\u062a\u0646\u062f \u0628\u0631\u0686\u0633\u0628 0 \u0628\u0632\u0646\u06cc\u0645 \u0648 \u0628\u0627 \u062f\u0627\u062f\u0647 \u0647\u0627\u06cc \u0642\u0628\u0644\u06cc \u062a\u0631\u06a9\u06cc\u0628 \u06a9\u0646\u06cc\u0645","0a5cb8ca":"## 2- \u0645\u062d\u0627\u0633\u0628\u0647 \u0627\u0644\u06af\u0648\u0631\u06cc\u062a\u0645 find-S","7dce9c29":"\u0627\u0644\u0627\u0646 \u062f\u0627\u062f\u0647 \u0647\u0627\u06cc\u06cc \u06a9\u0647 \u062a\u06a9\u0631\u0627\u0631\u06cc \u06a9\u0647 \u0628\u0631\u0686\u0633\u0628 0 \u062f\u0627\u0631\u0646\u062f \u0628\u0627\u06cc\u062f \u062d\u0630\u0641 \u0628\u0634\u0648\u0646\u062f","6fc5d337":"## 7-  \u0645\u062d\u0627\u0633\u0628\u0647 \u0627\u0644\u06af\u0648\u0631\u06cc\u062a\u0645 \u062f\u0631\u062e\u062a \u062a\u0635\u0645\u06cc\u0645 \u062a\u0635\u0627\u062f\u0641\u06cc","63f8be79":"\u0628\u0647 \u062c\u0632 \u0645\u062d\u0627\u0633\u0628\u0647 \u06af\u06cc\u0646 \u0633\u0627\u06cc\u0631 \u0627\u0644\u06af\u0648\u0631\u06cc\u062a\u0645 \u0647\u0627 \u0627\u0632 \u0627\u06cc\u0646\u062a\u0631\u0646\u062a \u0686\u0645\u0639 \u0622\u0648\u0631\u06cc \u0634\u062f\u0647 \u0627\u0633\u062a\n## 1- \u0628\u062f\u0633\u062a \u0622\u0648\u0631\u062f\u0646 5 \u0648\u06cc\u0698\u06af\u06cc \u06a9\u0645 \u0627\u0647\u0645\u06cc\u062a\n\u0628\u0631\u0627\u06cc \u0627\u06cc\u0646 \u06a9\u0627\u0631 \u0627\u0632 \u0622\u0646\u062a\u0631\u0648\u067e\u06cc \u0628\u0631\u0627\u06cc \u0645\u062d\u0627\u0633\u06cc\u0647 \n<br> gain <br> \n\u0627\u0633\u062a\u0641\u0627\u062f\u0647 \u0645\u06cc\u06a9\u0646\u06cc\u0645","364a7d78":" \u0627\u0628\u062a\u062f\u0627 \u06a9\u062a\u0627\u0628\u062e\u0627\u0646\u0647 \u0647\u0627\u06cc \u0645\u0648\u0631\u062f \u0646\u06cc\u0627\u0632 \u0648  \u0641\u0627\u06cc\u0644 \u062f\u06cc\u062a\u0627\u0633\u062a \u0631\u0627 \u0644\u0648\u062f \u0645\u06cc\u06a9\u0646\u06cc\u0645 ","60afa47b":"## 3- \u0645\u062d\u0627\u0633\u0628\u0647 \u0627\u0644\u06af\u0648\u0631\u06cc\u062a\u0645 CE\n","32a4606d":"\u0627\u0632 487 \u062f\u0627\u062f\u0647\n<br>\n487 - 285 = 202 \n<br>\n\u062a\u06a9\u0631\u0627\u0631\u06cc \u0628\u0648\u062f\u0646\u062f\n<br>\n\u062d\u0627\u0644 \u062a\u0645\u0627\u0645\u06cc \u0627\u06cc\u0646 \u062f\u0627\u062f\u0647 \u0647\u0627 \u062f\u0627\u0631\u0627\u06cc \u0628\u0631\u0686\u0633\u0628 1 \u0647\u0633\u062a\u0646\u062f \u06a9\u0647 \u0628\u0627\u06cc\u062f \u0627\u06cc\u0646 \u0648\u06cc\u0698\u06af\u06cc \u0628\u0647 \u062f\u0627\u062f\u0647 \u0647\u0627 \u0627\u0636\u0627\u0641\u0647 \u0634\u0648\u062f\n","03478772":"\u0628\u0647 \u062f\u0644\u06cc\u0644 \u0632\u0645\u0627\u0646 \u0628\u0631 \u0628\u0648\u062f\u0646 \u06a9\u0627\u0645\u0646\u062a \u0634\u062f\u0647 \u0627\u0633\u062a ","82373631":"\u0686\u0648\u0646 \u0648\u06cc\u0698\u06af\u06cc \u0647\u0627\u06cc \u0645\u0627 \u0628\u06cc\u0634\u062a\u0631 \u0627\u0632 2 \u0628\u0639\u062f \u0647\u0633\u062a \u0646\u0645\u06cc \u0634\u0648\u062f \u0622\u0646 \u0647\u0627 \u0631\u0627 \u0631\u0648\u06cc \u0646\u0645\u0648\u062f\u0627\u0631 \u0646\u0645\u0627\u06cc\u0634 \u062f\u0627\u062f \n","465ee2c8":"488 <br>\n\u0633\u0637\u0631 \u062f\u0627\u062f\u0647 \u062f\u0627\u0631\u06cc\u0645 \u06a9\u0647 \u0628\u0627\u06cc\u062f \u0646\u0631\u0645\u0627\u0644\u06cc\u0632\u0647 \u0634\u0648\u0646\u062f. \u0686\u0648\u0646 \u0633\u0646 \u0628\u0627\u06cc\u0646\u0631\u06cc \u0646\u0645\u06cc \u0634\u0648\u062f \u0648 \u0633\u062a\u0648\u0646 # \u0628\u0631\u0627\u06cc \u0634\u0645\u0627\u0631\u0646\u062f\u0647 \u0647\u0633\u062a \u0628\u0627\u06cc\u062f \u062d\u0630\u0641 \u0634\u0648\u062f. \u0648 \u0633\u0627\u06cc\u0631 \u062f\u0627\u062f\u0647 \u0647\u0627 \u0628\u0647 <br> \nno => 0  <br>\nyes => 1 <br>\n\u0646\u0628\u062f\u06cc\u0644 \u0634\u0648\u062f\n","ef62cee7":"\u062d\u0627\u0644\u0627 \u0628\u0627\u06cc\u062f \u062f\u0627\u062f\u0647 \u0647\u0627\u06cc \u062a\u06a9\u0631\u0627\u0631\u06cc \u0631\u0627 \u062d\u0630\u0641 \u06a9\u0646\u06cc\u0645","75a22004":"## 5-  \u0645\u062d\u0627\u0633\u0628\u0647 \u0627\u0644\u06af\u0648\u0631\u06cc\u062a\u0645 knn\n","695401fd":"## 8-  \u0645\u062d\u0627\u0633\u0628\u0647 \u0627\u0644\u06af\u0648\u0631\u06cc\u062a\u0645 \u062f\u0631\u062e\u062a \u062a\u0635\u0645\u06cc\u0645 ID3","299989a2":"## 4-  \u0645\u062d\u0627\u0633\u0628\u0647 \u0627\u0644\u06af\u0648\u0631\u06cc\u062a\u0645 \u0628\u06cc\u0632","7d84431e":"# \u06af\u0627\u0645 \u062f\u0648\u0645: \u0627\u062c\u0631\u0627\u06cc \u0627\u0644\u06af\u0648\u0631\u06cc\u062a\u0645 \u0647\u0627\u06cc \u06cc\u0627\u062f\u06af\u06cc\u0631\u06cc \u0645\u0627\u0634\u06cc\u0646","70613003":"## \u06af\u0627\u0645 \u0627\u0648\u0644: \u0622\u0645\u0627\u062f\u0647 \u0633\u0627\u0632\u06cc \u062f\u0627\u062f\u0647 \u0647\u0627 ","dfd90dd2":"### \u0633\u0627\u062d\u062a \u062f\u0627\u062f\u0647 \u062a\u0633\u062a ","0085b795":"# **\u067e\u0631\u0648\u0698\u0647 \u067e\u0627\u06cc\u0627\u0646\u06cc - \u067e\u06cc\u0627\u062f\u0647 \u0633\u0627\u0632\u06cc \u0627\u0644\u06af\u0648\u0631\u06cc\u062a\u0645 \u0647\u0627\u06cc \u06cc\u0627\u062f\u06af\u06cc\u0631\u06cc \u0645\u0627\u0634\u06cc\u0646 \u0628\u0631 \u0631\u0648\u06cc \u062f\u0627\u062f\u0647 \u0647\u0627\u06cc \u06a9\u0648\u0648\u06cc\u062f**"}}