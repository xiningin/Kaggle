{"cell_type":{"f1b5b2d3":"code","90e35f85":"code","a1ca7f64":"code","a1071e53":"code","b2faa0f8":"code","6fee6617":"code","9601cd5e":"code","0bfc5f84":"code","bda4aadc":"code","9ad614ee":"code","a2729eca":"code","9746d6d6":"code","5bd73878":"code","b8bf3056":"code","d07f4c2f":"code","9855b33d":"code","0a1ea182":"code","66770a7d":"code","1421763c":"code","02194f13":"code","e86161b0":"code","74423766":"code","949baf12":"code","6ff199e0":"code","f1872060":"markdown","430e257c":"markdown","f151a9c1":"markdown","50b52dee":"markdown","c5ce5985":"markdown","9331fb19":"markdown","db4e0fcb":"markdown","33da8db5":"markdown","c5ad1c3a":"markdown","cbc912cf":"markdown","4c8d78c9":"markdown","dd9e7618":"markdown","2b004b70":"markdown"},"source":{"f1b5b2d3":"import datetime as dt\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nsns.set_style('whitegrid')\n\n\nimport os\nfrom keras.applications import xception\n# from keras.applications import ResNet50\nfrom keras.preprocessing import image\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nimport cv2\nfrom scipy.stats import uniform\n\nfrom tqdm import tqdm\nfrom glob import glob\n\n\nfrom keras.models import Model, Sequential\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Masking\nfrom keras.utils import np_utils, to_categorical\n\n\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","90e35f85":"#copying the pretrained models to the cache directory\ncache_dir = os.path.expanduser(os.path.join('~', '.keras'))\nif not os.path.exists(cache_dir):\n    os.makedirs(cache_dir)\nmodels_dir = os.path.join(cache_dir, 'models')\nif not os.path.exists(models_dir):\n    os.makedirs(models_dir)\n\n#copy the Xception models\n!cp ..\/input\/keras-pretrained-models\/xception* ~\/.keras\/models\/\n#show\n!ls ~\/.keras\/models","a1ca7f64":"# base_folder = '..\/input\/fire-dataset'\n# data_folder = '..\/input\/fire-dataset\/fire_dataset'\n# train_data_folder = '..\/input\/fire-dataset\/fire_dataset\/fire_images'\n# test_date_folder  = '..\/input\/fire-dataset\/fire_dataset\/non_fire_images'\n\n# base_folder = '..\/input\/fire-detection\/Fire Detection'\n# data_folder = '..\/input\/fire-detection\/Fire Detection\/Train'\n# train_data_folder = '..\/input\/fire-detection\/Fire Detection\/Train\/positive'\n# test_date_folder  = '..\/input\/fire-detection\/Fire Detection\/Train\/negetive'\n\n# base_folder = '..\/input\/ctdataset\/ct'\n# data_folder = '..\/input\/ctdataset\/ct\/train'\n# test_data_folder  = '..\/input\/ctdataset\/ct\/test'\n\n# base_folder = '..\/input\/covid19\/COVID-19 CT'\n\ndata_folder = '..\/input\/covid19\/COVID-19 CT\/Train'\ntest_data_folder  = '..\/input\/ctdataset\/ct\/test'\n\n# categories = ['fire_images', 'non_fire_images']\n# categories = ['positive', 'negetive']\n# categories = ['COVID', 'NonCOVID']\ncategories = ['negetive', 'positive']\ncategories_test = ['NonCOVID', 'COVID']\n\n# len_categories = len(categories)","a1071e53":"image_count = {}\ntrain_data = []\n\nfor i , category in tqdm(enumerate(categories)):\n    class_folder = os.path.join(data_folder, category)\n    label = category\n    image_count[category] = []\n    \n    for path in os.listdir(os.path.join(class_folder)):\n        image_count[category].append(category)\n        train_data.append(['{}\/{}'.format(category, path), i, category])","b2faa0f8":"#show image count\nfor key, value in image_count.items():\n    print('{0} -> {1}'.format(key, len(value)))","6fee6617":"#create a dataframe\ndf = pd.DataFrame(train_data, columns=['file', 'id', 'label'])\ndf.shape\ndf.head()","9601cd5e":"image_count = {}\ntest_data = []\n\nfor i , category in tqdm(enumerate(categories_test)):\n    class_folder = os.path.join(test_data_folder, category)\n    label = category\n    image_count[category] = []\n    \n    for path in os.listdir(os.path.join(class_folder)):\n        image_count[category].append(category)\n        test_data.append(['{}\/{}'.format(category, path), i, category])","0bfc5f84":"#show image count\nfor key, value in image_count.items():\n    print('{0} -> {1}'.format(key, len(value)))","bda4aadc":"df_test = pd.DataFrame(test_data, columns=['file', 'id', 'label'])\ndf_test.shape\ndf_test.head()","9ad614ee":"#masking function\ndef create_mask_for_plant(image):\n    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n    lower_hsv = np.array([0,0,250])\n    upper_hsv = np.array([250,255,255])\n    \n    mask = cv2.inRange(image_hsv, lower_hsv, upper_hsv)\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n    \n    return mask\n\n#image segmentation function\ndef segment_image(image):\n    mask = create_mask_for_plant(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output\/255\n\n#sharpen the image\ndef sharpen_image(image):\n    image_blurred = cv2.GaussianBlur(image, (0, 0), 3)\n    image_sharp = cv2.addWeighted(image, 1.5, image_blurred, -0.5, 0)\n    return image_sharp\n\n# function to get an image\ndef read_img(filepath, size):\n    img = image.load_img(os.path.join(data_folder, filepath), target_size=size)\n    #convert image to array\n    img = image.img_to_array(img)\n    return img\n\n# function to get an image on test \ndef read_img_test(filepath, size):\n    img = image.load_img(os.path.join(test_data_folder, filepath), target_size=size)\n    #convert image to array\n    img = image.img_to_array(img)\n    return img","a2729eca":"INPUT_SIZE=255\n\n##preprocess the input\nX_train = np.zeros((len(df), INPUT_SIZE, INPUT_SIZE, df.shape[1]), dtype='float')\nfor i, file in tqdm(enumerate(df['file'])):\n    #read image\n    img = read_img(file,(INPUT_SIZE,INPUT_SIZE))\n    #masking and segmentation\n    image_segmented = segment_image(img)\n    #sharpen\n    image_sharpen = sharpen_image(image_segmented)\n    x = xception.preprocess_input(np.expand_dims(image_sharpen.copy(), axis=0))\n#     x = xception.preprocess_input(np.expand_dims(image_sharpen.copy(), axis=0))\n    X_train[i] = x","9746d6d6":"print('Train Image Shape: ', X_train.shape)\nprint('Train Image Size: ', X_train.size)","5bd73878":"INPUT_SIZE=255\n\n##preprocess the test\nX_test = np.zeros((len(df_test), INPUT_SIZE, INPUT_SIZE, df_test.shape[1]), dtype='float')\nfor i, file in tqdm(enumerate(df_test['file'])):\n    #read image\n    img = read_img_test(file,(INPUT_SIZE,INPUT_SIZE))\n    #masking and segmentation\n    image_segmented = segment_image(img)\n    #sharpen\n    image_sharpen = sharpen_image(image_segmented)\n    x = xception.preprocess_input(np.expand_dims(image_sharpen.copy(), axis=0))\n#     x = xception.preprocess_input(np.expand_dims(image_sharpen.copy(), axis=0))\n    X_test[i] = x","b8bf3056":"print('Test Image Shape: ', X_test.shape)\nprint('Test Image Size: ', X_test.size)","d07f4c2f":"y_train = df['id']\ny_test = df_test['id']\n# train_x, train_val, y_train, y_val = train_test_split(X_train, y, test_size=0.2, random_state=101)","9855b33d":"print('COVID IMAGES ON TRAINING DATA: ',y_train[y_train==0].shape[0])\nprint('NON-COVID IMAGES ON TRAINING DATA: ',y_train[y_train==1].shape[0])\nprint('COVID IMAGES ON TEST DATA: ',y_test[y_test==0].shape[0])\nprint('NON-COVID IMAGES ON TEST DATA: ',y_test[y_test==1].shape[0])","0a1ea182":"##get the features\nxception_bf = xception.Xception(weights='imagenet', include_top=False, pooling='avg')\nbf_train_x = xception_bf.predict(X_train, batch_size=32, verbose=1)\nbf_test = xception_bf.predict(X_test, batch_size=32, verbose=1)\n\n# xception_bf = ResNet50.ResNet50(weights='imagenet', include_top=False, pooling='avg')\n# bf_train_x = xception_bf.predict(train_x, batch_size=32, verbose=1)\n# bf_train_val = xception_bf.predict(train_val, batch_size=32, verbose=1)","66770a7d":"#print shape of feature and size\nprint('Train Shape: ', bf_train_x.shape)\nprint('Train Size: ', bf_train_x.size)\n\nprint('Validation Shape: ', bf_test.shape)\nprint('Validation Size: ', bf_test.size)","1421763c":"#keras Sequential model\nmodel = Sequential()\nmodel.add(Dense(units = 256 , activation = 'relu', input_dim=bf_train_x.shape[1]))\nmodel.add(Dense(units = 64 , activation = 'relu'))\nmodel.add(Dense(units = 1, activation = 'sigmoid'))\nmodel.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\nmodel.summary()","02194f13":"#train the model @ 100 epochs\nhistory = model.fit(bf_train_x, y_train, epochs=100, batch_size=32);","e86161b0":"fig, ax = plt.subplots(1,2,figsize=(14,5))\nax[0].set_title('TRAINING LOSS');\nax[1].set_title('TRAINING ACCURACY');\n\n\nax[0].plot(history.history['loss'], color= 'salmon',lw=2);\nax[1].plot(history.history['accuracy'], color= 'green',lw=2);","74423766":"#predict the validation data\npredictions = model.predict_classes(bf_test)","949baf12":"print(classification_report(y_test, predictions))","6ff199e0":"#### CONFUSION MATRIX","f1872060":"### DEEP LEARNING MODEL","430e257c":"#### LOSS AND ACCURACY","f151a9c1":"nb_rows = 3\nnb_cols = 5\nfig, axs = plt.subplots(nb_rows, nb_cols, figsize=(10, 5));\nplt.suptitle('SAMPLE IMAGES');\nfor i in range(0, nb_rows):\n    for j in range(0, nb_cols):\n        axs[i, j].xaxis.set_ticklabels([]);\n        axs[i, j].yaxis.set_ticklabels([]);\n        axs[i, j].imshow((read_img(df['file'][np.random.randint(100)], (255,255)))\/255.);\nplt.show();","50b52dee":"#### CLASSIFICATION REPORT","c5ce5985":"confusion_mat = confusion_matrix(y_val, predictions)\n\nplt.figure(figsize=(10,10))\nsns.heatmap(confusion_mat, square=True, annot=True,\n            yticklabels=['FIRE_IMG', 'NON_FIRE_IMG'],\n            xticklabels=['FIRE_IMG', 'NON_FIRE_IMG']);\nplt.title('CONFUSION MATRIX');\nplt.xlabel('Y_TRUE');\nplt.ylabel(\"PREDICTIONS\");","9331fb19":"#get an image\nimg = read_img(df['file'][102],(255,255))\n#mask\nimage_mask = create_mask_for_plant(img)\n#segmentation\nimage_segmented = segment_image(img)\n#sharpen the image\nimage_sharpen = sharpen_image(image_segmented)\n\nfig, ax = plt.subplots(1, 4, figsize=(10, 5));\nplt.suptitle('SAMPLE PROCESSED IMAGE', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIG.', fontsize=12)\nax[1].set_title('MASK', fontsize=12)\nax[2].set_title('SEGMENTED', fontsize=12)\nax[3].set_title('SHARPEN', fontsize=12)\n\n\nax[0].imshow(img\/255);\nax[1].imshow(image_mask);\nax[2].imshow(image_segmented);\nax[3].imshow(image_sharpen);\n\n","db4e0fcb":"#### SPLIT THE DATA(no)","33da8db5":"### XCEPTION BOTTLENECK FEATURE EXTRACTION","c5ad1c3a":"confusion_mat = confusion_matrix(y_test, predictions)\n\nplt.figure(figsize=(10,10))\nsns.heatmap(confusion_mat, square=True, annot=True,\n            yticklabels=['FIRE_IMG', 'NON_FIRE_IMG'],\n            xticklabels=['FIRE_IMG', 'NON_FIRE_IMG']);\nplt.title('CONFUSION MATRIX');\nplt.xlabel('Y_TRUE');\nplt.ylabel(\"PREDICTIONS\");","cbc912cf":"### SHOW SAMPLE PROCESSED IMAGE\n","4c8d78c9":"### IMAGE PREPROCESSING","dd9e7618":"## OVERVIEW\n---\n* Image Preprocessing with OpenCV\n    * Masking\n    * Segmentation\n    * Image Sharpening\n* Transfer Learning with Keras Pretrained Model\n* Feature Extraction\n* Deep Learning Model to Classify the Images","2b004b70":"### SHOW SAMPLE IMAGES"}}