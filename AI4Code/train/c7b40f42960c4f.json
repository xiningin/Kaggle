{"cell_type":{"7804c9af":"code","8f38806b":"code","7f304995":"code","33b0fbab":"code","0b75125f":"code","0ee47e22":"code","ad2a23d3":"code","5113f02f":"code","93b80c81":"code","75699b89":"code","ee7a21bd":"code","fbf2b9ca":"code","fb441360":"code","13f6caeb":"markdown","3ce81127":"markdown"},"source":{"7804c9af":"import numpy as np\nimport pandas as pd\nimport os\n\nPATH = \"\/kaggle\/input\/applications-of-deep-learning-wustl-fall-2020\/final-kaggle-data\/\"\nPATH_TRAIN = os.path.join(PATH, \"train.csv\")\nPATH_TEST = os.path.join(PATH, \"test.csv\")","8f38806b":"# # detect and init the TPU\n# import tensorflow as tf\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n# tf.config.experimental_connect_to_cluster(tpu)\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n\n# # instantiate a distribution strategy\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","7f304995":"df_train = pd.read_csv(PATH_TRAIN)\ndf_test = pd.read_csv(PATH_TEST)\n\ndf_train = df_train[df_train.id != 1300]\n\ndf_train['filename'] = df_train[\"id\"].astype(str)+\".png\"\ndf_train['stable'] = df_train['stable'].astype(str)\n\ndf_test['filename'] = df_test[\"id\"].astype(str)+\".png\"","33b0fbab":"TRAIN_PCT = 0.8\nTRAIN_CUT = int(len(df_train) * TRAIN_PCT)\n\ndf_train_cut = df_train[0:TRAIN_CUT]\ndf_validate_cut = df_train[TRAIN_CUT:]\n\nprint(f\"Training size: {len(df_train_cut)}\")\nprint(f\"Validate size: {len(df_validate_cut)}\")","0b75125f":"# image = Image.open(\"..\/input\/applications-of-deep-learning-wustl-fall-2020\/final-kaggle-data\/95.png\")\n# image","0ee47e22":"import tensorflow as tf\nimport keras_preprocessing\nfrom keras_preprocessing import image\nfrom keras_preprocessing.image import ImageDataGenerator\n\nWIDTH = 640\nHEIGHT = 400\ntraining_datagen = ImageDataGenerator(\n  rescale = 1.\/255,\n  horizontal_flip=True,\n#   featurewise_center = True,\n#   zca_epsilon = 0.001,\n#   zca_whitening = True,\n#   zoom_range = [0.7,0.8],\n  brightness_range = [0.9,1.6],\n  #Original set it to True, I will set False\n  vertical_flip = False,\n  fill_mode='nearest')\n\ntrain_generator = training_datagen.flow_from_dataframe(\n        dataframe=df_train_cut,\n        directory=PATH,\n        x_col=\"filename\",\n        y_col=\"stable\",\n        target_size=(HEIGHT, WIDTH),\n#I reduce the batch size further because in the \"hidden leaderboard\" my model was overfitting a little bit.\n        batch_size=16,\n        class_mode='binary')\n\nvalidation_datagen = ImageDataGenerator(rescale = 1.\/255,\n#                                           featurewise_center = True,\n#                                           zca_epsilon = 0.001,\n#                                           zca_whitening = True,\n#                                           zoom_range = [0.7,0.8],\n                                          brightness_range = [1.3,1.6])\n\nval_generator = validation_datagen.flow_from_dataframe(\n        dataframe=df_validate_cut,\n        directory=PATH,\n        x_col=\"filename\",\n        y_col=\"stable\",\n        target_size=(HEIGHT, WIDTH),\n        class_mode='binary')","ad2a23d3":"from tensorflow.keras.callbacks import EarlyStopping\nfrom keras.applications import *\nfrom keras.models import Model\nimport keras\nfrom keras.models import load_model\n#with tpu_strategy.scope():\nxception_model = load_model(\"..\/input\/xception-weight\/Xception_Tune_Oct_7.h5\")\n\n#Because I decreased the batch size, I need to decrease the learning rate too.\ndef lr_schedule(epoch):\n    lr = 0.00008\n    if epoch > 3:\n        lr = 0.00003\n    return lr\n\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule, verbose=True)\nmonitor = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=5, verbose=1, mode='auto',\n        restore_best_weights=True)\n\ncallback_list = [monitor, lr_callback]\n\n\nhistory = xception_model.fit(train_generator,  \n  verbose = 1, \n  validation_data=val_generator, \n  steps_per_epoch=2049,\n#I set validation step bigger so it can converge.\n  validation_steps=800,\n  epochs=8,callbacks=callback_list)","5113f02f":"xception_model.save(\".\/Xception_Nov4.h5\")","93b80c81":"from IPython.display import FileLink\nFileLink(r'Xception_Nov4.h5')","75699b89":"submit_datagen = ImageDataGenerator(rescale = 1.\/255)\n\nsubmit_generator = submit_datagen.flow_from_dataframe(\n        dataframe=df_test,\n        directory=PATH,\n        x_col=\"filename\",\n        batch_size = 1,\n        shuffle = False,\n        target_size=(HEIGHT, WIDTH),\n        class_mode=None)\n\nsubmit_generator.reset()\npred = xception_model.predict(submit_generator,steps=len(df_test))","ee7a21bd":"df_submit = pd.DataFrame({\"id\":df_test['id'],'stable':pred.flatten()})\ndf_submit.to_csv(\".\/submit.csv\",index = False)","fbf2b9ca":"from IPython.display import FileLink\nFileLink(r'submit.csv')","fb441360":"import tensorflow as tf\nimport keras_preprocessing\nfrom keras_preprocessing import image\nfrom keras_preprocessing.image import ImageDataGenerator\nfrom keras.models import Model\nfrom keras.models import load_model\n\nxception_model = load_model(\"..\/input\/xception-weight\/Xception_Tune_Oct_7.h5\")\nsubmit_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   horizontal_flip=False,\n                                   vertical_flip=False)\n\nsubmit_generator = submit_datagen.flow_from_dataframe(\n        dataframe=df_test,\n        directory=PATH,\n        x_col=\"filename\",\n        batch_size = 1,\n        shuffle = False,\n        target_size=(640, 400),\n        class_mode=None)\n\nsubmit_generator.reset()\npred = xception_model.predict(submit_generator,steps=len(df_test))\ndf_submit = pd.DataFrame({\"id\":df_test['id'],'stable':pred.flatten()})\ndf_submit.to_csv(\".\/submit.csv\",index = False)\nfrom IPython.display import FileLink\nFileLink(r'submit.csv')","13f6caeb":"## Predict with the new model","3ce81127":"# Build Submission\n\nNow that the neural network is trained; we need to generate a submit CSV file to send to Kaggle.  We will use nearly the same technique to build the submit file.  However, these essential points that we must address:\n\n* We do not want the data generator to create an infinite date like we did when training.  We have a fixed number of cases to score for the Kaggle submit; we only want to process them.\n* We do not want the data generator to randomize the samples' order like it did when training. Therefore we set shuffle to false.\n* We want to always start at the beginning of the data, so we reset the generator.\n\nThese ensure that the predictions align with the id's."}}