{"cell_type":{"d5519d85":"code","39d14c34":"code","e2fb70fa":"code","fd125383":"code","97370a89":"code","95c17736":"code","ff1e0c5b":"code","045deaec":"code","064ead00":"code","f0b2d2f7":"code","dc418012":"code","3f3cd9be":"markdown","ab104bfd":"markdown","0afd0231":"markdown","ec60d9fa":"markdown","a6ff591b":"markdown"},"source":{"d5519d85":"!pip install -q efficientnet","39d14c34":"import math, re, os\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n\n## ______________ \u0411\u041b\u041e\u041a \u0421 \u0418\u041c\u041f\u041e\u0420\u0422\u0410\u041c\u0418 \u0410\u0420\u0425\u0418\u0422\u0415\u041a\u0422\u0423\u0420 ____________________\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.vgg19 import VGG19\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom tensorflow.keras.applications.densenet import DenseNet121, DenseNet169, DenseNet201 \nfrom tensorflow.keras.applications.xception import Xception\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.applications.resnet_v2 import ResNet50V2, ResNet101V2, ResNet152V2\nfrom tensorflow.keras.applications.nasnet import NASNetLarge\nfrom efficientnet.tfkeras import EfficientNetB7, EfficientNetL2\n## ______________ \u041a\u041e\u041d\u0415\u0426 \u0411\u041b\u041e\u041a\u0410 \u0421 \u0418\u041c\u041f\u041e\u0420\u0422\u0410\u041c\u0418 \u0410\u0420\u0425\u0418\u0422\u0415\u041a\u0422\u0423\u0420 ____________________\n\n# \u0438\u043c\u043f\u043e\u0440\u0442 \u0434\u0440\u0443\u0433\u0438\u0445 \u043f\u043e\u043b\u0435\u0437\u043d\u044b\u0445 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u043e\u0432: \u0441\u043b\u043e\u0435\u0432, \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440\u043e\u0432, \u0444\u0443\u043d\u043a\u0446\u0438\u0439 \u043e\u0431\u0440\u0430\u0442\u043d\u043e\u0439 \u0441\u0432\u044f\u0437\u0438\nfrom tensorflow.keras.layers import Flatten,Dense,Dropout,BatchNormalization\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications.densenet import DenseNet201\nfrom tensorflow.keras.layers import Dropout, BatchNormalization, GaussianDropout\nfrom efficientnet.tfkeras import EfficientNetB5\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.applications.xception import Xception\n#  \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0430 \u0434\u043b\u044f \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u043d\u0430\u0431\u043e\u0440\u0430\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 Kaggle\nfrom kaggle_datasets import KaggleDatasets\nimport re\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\n%matplotlib inline \nprint(\"Tensorflow version \" + tf.__version__)","e2fb70fa":"AUTO = tf.data.experimental.AUTOTUNE\n# \u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u0435 \u043e\u0431\u043e\u0440\u0443\u0434\u043e\u0432\u0430\u043d\u0438\u0435 \u0438 \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0443\u044e \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u044e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    # \u043e\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u0435 TPU. \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043d\u0435 \u0442\u0440\u0435\u0431\u0443\u044e\u0442\u0441\u044f \u0435\u0441\u043b\u0438 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u0435\u043d\u043d\u0430 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u043e\u043a\u0440\u0443\u0436\u0435\u043d\u0438\u044f TPU_NAME. \u041d\u0430 Kaggle \u044d\u0442\u043e \u0432\u0441\u0435\u0433\u0434\u0430 True.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # \u0435\u0441\u043b\u0438 TPU \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442, \u0442\u043e \u0438\u0441\u043f\u043b\u044c\u0437\u0443\u0435\u043c \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u044e \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e \u0434\u043b\u044f TF (CPU or GPU)\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\n# \u041f\u0443\u0442\u044c \u043a \u0434\u0430\u043d\u043d\u044b\u043c. \u0415\u0441\u043b\u0438 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442\u0435 \u043d\u0430 Google Colaboratory, \u0442\u043e \u0437\u0430\u043c\u0435\u043d\u0438\u0442\u0435 KaggleDatasets().get_gcs_path() \u043d\u0430 \u043f\u0443\u0442\u044c \u043a \u0434\u0430\u043d\u043d\u044b\u043c, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u0443\u0434\u0435\u0442 \u0443 \u0432\u0430\u0441\nGCS_DS_PATH = KaggleDatasets().get_gcs_path(\"plant-pathology-2020-fgvc7\")\n\n# \u041a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f\nEPOCHS = 40\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync","fd125383":"# \u0444\u0443\u043d\u043a\u0446\u0438\u044f, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043f\u0440\u0435\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0430\u0439\u0434\u0438 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u0432 \u043f\u043e\u043b\u043d\u044b\u0439 \u043f\u0443\u0442\u044c \u043a \u043d\u0435\u0439\ndef format_path(st):\n    return GCS_DS_PATH + '\/images\/' + st + '.jpg'","97370a89":"train = pd.read_csv('\/kaggle\/input\/plant-pathology-2020-fgvc7\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/plant-pathology-2020-fgvc7\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/plant-pathology-2020-fgvc7\/sample_submission.csv')\n\ntrain_paths = train.image_id.apply(format_path).values\ntest_paths = test.image_id.apply(format_path).values\n\ntrain_labels = train.loc[:, 'healthy':].values\n\n## \u0435\u0441\u043b\u0438 \u043f\u043b\u0430\u043d\u0438\u0440\u0443\u0435\u0442\u0435 \u043e\u0431\u0443\u0447\u0430\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c \u0441 \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u0443\u044e\u0449\u0438\u043c \u043d\u0430\u0431\u043e\u0440\u043e\u043c \u0434\u0430\u043d\u043d\u044b\u0445\ntrain_paths, valid_paths, train_labels, valid_labels = train_test_split(\n     train_paths, train_labels, test_size=0.15, random_state=2020)","95c17736":"# \u0443\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u043c \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u044b\u0435 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435\nimg_size = 768\n\n# \u0444\u0443\u043d\u043a\u0446\u0438\u044f, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0447\u0438\u0442\u0430\u0435\u0442 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0438\u0437 \u0444\u0430\u0439\u043b\u0430 \u0438 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u044b\u0432\u0430\u0435\u0442 \u0435\u0433\u043e \u043a \u043d\u0443\u0436\u043d\u043e\u043c\u0443 \u0440\u0430\u0437\u043c\u0435\u0440\u0443, \u0430 \u0442\u0430\u043a \u0436\u0435 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0435\u0442\ndef decode_image(filename, label=None, image_size=(img_size, img_size)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    image = tf.image.resize(image, image_size)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n# \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0440\u0430\u0441\u0448\u0438\u0440\u0435\u043d\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445\ndef data_augment(image, label=None, seed=2020):\n    image = tf.image.random_flip_left_right(image, seed=seed)\n    image = tf.image.random_flip_up_down(image, seed=seed)\n    \n    if label is None:\n        return image\n    else:\n        return image, label","ff1e0c5b":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .map(data_augment, num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(512)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n## \u0435\u0441\u043b\u0438 \u043f\u043b\u0430\u043d\u0438\u0440\u0443\u0435\u0442\u0435 \u043e\u0431\u0443\u0447\u0430\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c \u0441 \u0432\u0430\u043b\u0438\u0434\u0438\u0440\u0443\u044e\u0449\u0438\u043c \u043d\u0430\u0431\u043e\u0440\u043e\u043c \u0434\u0430\u043d\u043d\u044b\u0445\nvalid_dataset = (\n     tf.data.Dataset\n     .from_tensor_slices((valid_paths, valid_labels))\n     .map(decode_image, num_parallel_calls=AUTO)\n     .batch(BATCH_SIZE)\n     .cache()\n     .prefetch(AUTO)\n )\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_paths)\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n)","045deaec":"def get_model(use_model):\n    base_model =  use_model(weights='noisy-student',\n                                 include_top=False, pooling='avg',\n                                 input_shape=(img_size, img_size, 3))\n    x = base_model.output\n    predictions = Dense(train_labels.shape[1], activation=\"softmax\")(x)\n    model = Model(inputs=base_model.input, outputs=predictions) \n    model.compile(optimizer='nadam', loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n    return model\n\nwith strategy.scope():    \n    model1 = get_model(EfficientNetB7) # \u0442\u0443\u0442 \u043f\u043e\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u0441\u0432\u043e\u044e \u043c\u043e\u0434\u0435\u043b\u044c\nmodel1.load_weights(\"\/kaggle\/input\/tf-zoo-models-on-tpu-efficientnetb7\/my_ef_net_b7.h5\")\n","064ead00":"def get_model(use_model):\n    base_model =  use_model(weights='imagenet',\n                                 include_top=False, pooling='avg',\n                                 input_shape=(img_size, img_size, 3))\n    x = base_model.output\n    predictions = Dense(train_labels.shape[1], activation=\"softmax\")(x)\n    model = Model(inputs=base_model.input, outputs=predictions) \n    model.compile(optimizer='nadam', loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n    return model\n\nwith strategy.scope():    \n    model2 = get_model(DenseNet201) # \u0442\u0443\u0442 \u043f\u043e\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u0441\u0432\u043e\u044e \u043c\u043e\u0434\u0435\u043b\u044c\nmodel2.load_weights(\"\/kaggle\/input\/tf-zoo-models-on-tpu-densenet201\/my_dense_net_201.h5\")\n","f0b2d2f7":"def get_model(use_model):\n    base_model =  use_model(weights='imagenet',\n                                 include_top=False, pooling='avg',\n                                 input_shape=(img_size, img_size, 3))\n    x = base_model.output\n    predictions = Dense(train_labels.shape[1], activation=\"softmax\")(x)\n    model = Model(inputs=base_model.input, outputs=predictions) \n    model.compile(optimizer='nadam', loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n    return model\n\nwith strategy.scope():    \n    model3 = get_model(InceptionResNetV2) # \u0442\u0443\u0442 \u043f\u043e\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u0441\u0432\u043e\u044e \u043c\u043e\u0434\u0435\u043b\u044c\nmodel3.load_weights(\"\/kaggle\/input\/tf-zoo-models-on-tpu\/InceptionResNetV2.h5\")\n","dc418012":"best_alpha = 0.52\nbad_alpha = 0.33\nprint('\u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f...')\nprobabilities1 = model1.predict(test_dataset, verbose=1)\n#probabilities2 = model2.predict(test_dataset, verbose=1)\nprobabilities3 = model3.predict(test_dataset, verbose=1)\n\n#probabilities = best_alpha * probabilities1 + (1 - best_alpha - bad_alpha) * probabilities2 + bad_alpha * probabilities3\nprobabilities = best_alpha * probabilities1 + (1 - best_alpha) * probabilities3 \n\nprint(probabilities)\n\nsub.loc[:, 'healthy':] = probabilities\nsub.to_csv('submission.csv', index=False)\nsub.head()","3f3cd9be":"## \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043e\u0431\u044a\u0435\u043a\u0442\u044b \u043d\u0430\u0431\u043e\u0440\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445","ab104bfd":"# \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043c\u0435\u0442\u043a\u0438 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0438 \u043f\u0443\u0442\u0438 \u043a \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\u043c","0afd0231":"# \u0414\u0435\u043b\u0430\u0435\u043c \u043f\u0440\u043e\u0433\u043d\u043e\u0437 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0433\u043e\u0442\u043e\u0432\u0438\u043c \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438","ec60d9fa":"# \u041f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 TPU (\u0438\u043b\u0438 GPU, \u0438\u043b\u0438 CPU...) \u0441 Tensorflow 2.1!","a6ff591b":"# \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c, \u043a\u0430\u043a\u043e\u0439 \u0443\u0441\u043a\u043e\u0440\u0438\u0442\u0435\u043b\u044c \u043c\u043e\u0436\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c"}}