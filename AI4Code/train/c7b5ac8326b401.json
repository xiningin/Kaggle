{"cell_type":{"e076073e":"code","1430b9be":"code","e43eaa3b":"code","60db8a22":"code","d7c86546":"code","673142e2":"code","1a032fca":"code","bfab02bb":"code","7b3fe124":"code","cb73f2f9":"code","9069566f":"code","6f5983f6":"code","ee3ebf8f":"code","72579713":"code","0b5e6448":"code","a2e0d7a1":"code","ac965623":"code","73e028ba":"code","74af92bf":"code","e719e9a6":"code","24e04032":"code","99152e71":"code","ad0e733e":"code","9bdc7d64":"code","ec835294":"code","9ff1a225":"code","97513068":"code","425f2e13":"code","8698b429":"code","efd335ed":"code","de71ab0f":"code","1d02a798":"code","5c8ce5e1":"code","f3837770":"code","8bc51654":"code","27edc0b7":"code","67ceda70":"code","ed5bfc84":"code","5eba9c46":"code","40d10ca5":"code","f10c564f":"code","8b3035bd":"code","d41c0ec2":"code","5bf887f8":"code","9df5df30":"code","6d22974e":"code","2a685847":"code","7c95ef99":"code","7d6d329e":"code","1f375388":"code","d91f1914":"code","179cf3e6":"code","fdcf6102":"markdown","9e1b9a5a":"markdown","d8b6950b":"markdown","caa013b9":"markdown","c29e1733":"markdown"},"source":{"e076073e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom matplotlib.colors import Normalize, rgb2hex\nfrom IPython.display import HTML\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1430b9be":"tf.__version__","e43eaa3b":"# Load the data\ndata = pd.read_csv(\"..\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv\" , index_col=0)\ndata.head()","60db8a22":"data.shape","d7c86546":"# Removing null review texts\ndata = data[~data['Review Text'].isnull()]\ndata.shape","673142e2":"# check if labels are missing\ndata['Recommended IND'].isnull().sum()","1a032fca":"# check the review length\ndata['Review Text'].str.split().apply(lambda x: len(x)).describe()","bfab02bb":"# check label distribution\ndata['Recommended IND'].value_counts()","7b3fe124":"# one hot encode y label (will be used in the interpretability section)\nlabels = tf.keras.utils.to_categorical(data['Recommended IND'])\noutput_shape = labels.shape[1]\nlabels, output_shape","cb73f2f9":"X = data['Review Text'].values\nX[:3]","9069566f":"# split into train, test, val\nx, X_test, y, y_test = train_test_split(X, labels, test_size=0.1, random_state=53)\nX_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.1, random_state=53)","6f5983f6":"print(X_train.shape, y_train.shape)\nprint(X_val.shape, y_val.shape)\nprint(X_test.shape, y_test.shape)","ee3ebf8f":"# set hyper parameters\nv_size = 2000\nmax_len = 100  # roughly 90th percentile\ne_dim = 64\nbatch_size = 256","72579713":"# create a tf textvectorization later\npre_processing_layer = TextVectorization(max_tokens=v_size, \n                                         output_sequence_length=max_len, \n                                         name='Notes_preprocessing_layer')","0b5e6448":"# fit on training vocab\npre_processing_layer.adapt(X_train)\n# get the vocab\nvocab = pre_processing_layer.get_vocabulary()","a2e0d7a1":"# create a simple bi-directional rnn model\ntf.keras.backend.clear_session()\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(input_dim=v_size,\n                              output_dim=e_dim, \n                             name='embedding', \n                             mask_zero=True),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dense(output_shape, activation='softmax')\n])\n\nmetrics = [tf.keras.metrics.CategoricalAccuracy()]\n\nmodel.summary()","ac965623":"# compile the model\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n              metrics=metrics)\nprint(\"Ready to Train\")","73e028ba":"# convert inputs to tf Datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\nvalid_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\ntest_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n\n# during distributed training, TF can shard on files or data, it defaults to\n# files and throws a warning that it is switching to data.\n# supress that warning by adding options\noptions = tf.data.Options()\noptions.experimental_distribute.auto_shard_policy = \\\n    tf.data.experimental.AutoShardPolicy.DATA\n\nraw_train_ds = \\\n    train_dataset.shuffle(X_train.shape[0]).batch(batch_size).with_options(options)\nraw_val_ds = valid_dataset.batch(batch_size).with_options(options)\nraw_test_ds = test_dataset.batch(batch_size).with_options(options)","74af92bf":"# vectorize the text inputs\n\n@tf.autograph.experimental.do_not_convert\ndef vectorize_text(text, label):\n\n    \"\"\" convert text to tokens \"\"\"\n\n    text = tf.expand_dims(text, -1)\n    return pre_processing_layer(text), label\n\n\n# print an example\ntext_batch, label_batch = next(iter(raw_train_ds.shuffle(50)))\nfirst_review, first_label = text_batch[0], label_batch[0]\nprint(\"Review: \", first_review)\nprint(\"Label: \", tf.argmax(first_label))\nprint(\"Vectorized review\", vectorize_text(first_review, first_label))\n","e719e9a6":"# tokenize all datasets and prepare for training\ntrain_ds = raw_train_ds.map(vectorize_text)\nval_ds = raw_val_ds.map(vectorize_text)\ntest_ds = raw_test_ds.map(vectorize_text)\n\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n","24e04032":"# train the model (3 epochs for quick implementation)\nmodel.fit(train_ds,\n          validation_data=val_ds,\n          epochs=3, verbose=1)","99152e71":"model.evaluate(test_ds)","ad0e733e":"## get predictions\ntest_probs = model.predict(test_ds)\ntest_preds = tf.argmax(test_probs, axis=1)\ny_test_flat = tf.argmax(y_test, axis=1)","9bdc7d64":"# build confusion matrix\ncm = tf.math.confusion_matrix(y_test_flat, test_preds)\n\n# plot confusion matrix\nsns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=[0, 1],\n            yticklabels=[0, 1], cbar=False)\nplt.show()","ec835294":"# extract embedding layer\nembed_layer = model.get_layer('embedding')\n\n# build new model with all layers after embedding layer\nnew_model = tf.keras.Sequential()\nfor layer in model.layers[1:]:\n    new_model.add(layer)","9ff1a225":"# take some test data\nsample_texts = next(raw_test_ds.take(1).as_numpy_iterator())[0]\nsample_vectors = pre_processing_layer(sample_texts)\n# sample_vectors = next(test_ds.take(1).as_numpy_iterator())[0]\nsample_labels = next(test_ds.take(1).as_numpy_iterator())[1]","97513068":"# select a random index\nindex = tf.cast(tf.random.uniform(shape=[1],\n                                  minval=sample_vectors.shape[0]),\n                dtype=tf.int8).numpy()[0]\n# generate a random sample\nsample_text = sample_texts[index]\nsample_vector = sample_vectors[index]\nsample_label = tf.argmax(sample_labels, axis=1)[index]\n# get embeddings\nsample_embed = embed_layer(sample_vector)\n# Create a Baseline vector with zero embeddings\nbaseline_embed = tf.zeros(shape=tf.shape(sample_embed))\n# get preds for sample\nsample_preds = model(sample_vectors)[index]\n# print the results with color codes\nwords = [vocab[i] for i in sample_vector]","425f2e13":"def interpolate_texts(baseline, text, m_steps):\n\n    \"\"\" Linearly interpolate the input vector\n    (embedding layer output of the sample vector)\"\"\"\n\n    # Generate m_steps intervals for integral_approximation() below.\n    alphas = tf.linspace(start=0.0, stop=1.0, num=m_steps+1)\n    # text = tf.cast(text, tf.float32)\n    alphas_x = alphas[:, tf.newaxis, tf.newaxis]\n    delta = text - baseline\n    texts = baseline + alphas_x * delta\n    return texts","8698b429":"n_steps = 50\ninterpolated_texts = interpolate_texts(baseline_embed,\n                                       sample_embed,\n                                       n_steps)","efd335ed":"interpolated_texts.shape\n# (num_interpolations, seq_len, embed_dim)","de71ab0f":"def compute_gradients(t, target_class_idx):\n\n    \"\"\" compute the gradient wrt to embedding layer output \"\"\"\n\n    with tf.GradientTape() as tape:\n        tape.watch(t)\n        probs = new_model(t)[:, target_class_idx]\n    grads = tape.gradient(probs, t)\n    return grads","1d02a798":"target_label = sample_label\n# target_label = 2   \n# change target_label to see attributions for that particular class; this is why we one-hot encoded the y label\npath_gradients = compute_gradients(interpolated_texts, target_label)","5c8ce5e1":"path_gradients.shape\n# (num_interpolations, seq_len, embed_dim)","f3837770":"# sum the grads of the interpolated vectors\nall_grads = tf.reduce_sum(path_gradients, axis=0) \/ n_steps\n# mulitply grads by (input - baseline); baseline is zero vectors\nx_grads = tf.math.multiply(all_grads, sample_embed)\n# sum all gradients across the embedding dimension\nigs = tf.reduce_sum(x_grads, axis=-1).numpy()","8bc51654":"# igs","27edc0b7":"# Helper functions to color the feature importances\n\ndef  hlstr(string, color='white'):\n    \"\"\"\n    Return HTML markup highlighting text with the desired color.\n    \"\"\"\n    return f\"<mark style=background-color:{color}>{string} <\/mark>\"\n\n\ndef colorize(attrs, cmap='PiYG'):\n    \"\"\"\n    Compute hex colors based on the attributions for a single instance.\n    Uses a diverging colorscale by default and normalizes and scales\n    the colormap so that colors are consistent with the attributions.\n    \"\"\"\n    \n    cmap_bound = tf.reduce_max(tf.abs(attrs))\n    norm = Normalize(vmin=-cmap_bound, vmax=cmap_bound)\n    cmap = mpl.cm.get_cmap(cmap)\n\n    # now compute hex values of colors\n    colors = list(map(lambda x: rgb2hex(cmap(norm(x))), attrs))\n    return colors","67ceda70":"colors = colorize(igs)\n_LABEL_NAMES = [0, 1]","ed5bfc84":"# print the sample and predictions\nprint(f\"Sample Text: {sample_text}\\n\")\n# print(f\"Sample Vector: {sample_vector}\")\n# print(f\"True Label: {_LABEL_NAMES[sample_label]}\")\n# print(f\"Predicted Label: \"\n#       f\"{_LABEL_NAMES[tf.argmax(sample_preds).numpy()]}\")\nprint(\"Predictions : \")\nfor index in tf.argsort(sample_preds,\n                        axis=-1, direction='DESCENDING').numpy():\n    print(f\"\\t{_LABEL_NAMES[index]} --> {sample_preds[index]*100:0.2f}%\")\n\nprint(f\"\\nTrue Label: {_LABEL_NAMES[sample_label]}\")\nprint(f\"\\nAttributions for Label: {_LABEL_NAMES[target_label]}\")\nprint(f\"\\nTop 5 Important words: \"\n      f\"{[words[i] for i in tf.argsort(igs, -1, 'DESCENDING')[:5]]}\\n\")\nprint(\"\\nGreen is high importance\/attribution whereas pink is negative importance\/attribution\\n\")\nHTML(\"\".join(list(map(hlstr, words, colors))))\n","5eba9c46":"# build confusion matrix\ncm = tf.math.confusion_matrix(y_test_flat, test_preds)\n\n# plot confusion matrix\nsns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=[0, 1],\n            yticklabels=[0, 1], cbar=False)\nplt.show()","40d10ca5":"# test_preds, y_test_flat","f10c564f":"false_positive_indices = tf.where((test_preds == 1) & (y_test_flat == 0))\nfalse_negative_indices = tf.where((test_preds == 0) & (y_test_flat == 1))","8b3035bd":"def get_igs(sample_embed, target_label, n_steps=50):\n    baseline_embed = tf.zeros(shape=tf.shape(sample_embed))\n    interpolated_texts = interpolate_texts(baseline_embed, sample_embed, n_steps)\n    path_gradients = compute_gradients(interpolated_texts, target_label)\n    # sum the grads of the interpolated vectors\n    all_grads = tf.reduce_sum(path_gradients, axis=0) \/ n_steps\n    # mulitply grads by (input - baseline); baseline is zero vectors\n    x_grads = tf.math.multiply(all_grads, sample_embed)\n    # sum all gradients across the embedding dimension\n    igs = tf.reduce_sum(x_grads, axis=-1).numpy()\n    \n    return tf.reshape(igs, -1)","d41c0ec2":"index = tf.cast(tf.random.uniform(shape=[1], minval=false_negative_indices.shape[0]), dtype=tf.int8).numpy()[0]\nsample_index = false_negative_indices[index].numpy()[0]\nsample_index","5bf887f8":"sample_text = X_test[sample_index]\nsample_label = y_test_flat[sample_index].numpy()\nsample_vector = pre_processing_layer([sample_text])\nsample_embed = embed_layer(sample_vector)\nsample_preds = tf.reshape(new_model(sample_embed), -1)\nwords = [vocab[i] for i in sample_vector[0]]\npredicted_label = tf.argmax(sample_preds).numpy()\ntarget_label = predicted_label\n# target_label = sample_label\nigs = get_igs(sample_embed, target_label)\ncolors = colorize(igs)","9df5df30":"# print the sample and predictions\nprint(f\"Sample Text: {sample_text}\\n\")\n# print(f\"Sample Vector: {sample_vector}\")\n# print(f\"True Label: {_LABEL_NAMES[sample_label]}\")\n# print(f\"Predicted Label: \"\n#       f\"{_LABEL_NAMES[tf.argmax(sample_preds).numpy()]}\")\nprint(\"Predictions : \")\nfor index in tf.argsort(sample_preds,\n                        axis=-1, direction='DESCENDING').numpy():\n    print(f\"\\t{_LABEL_NAMES[index]} --> {tf.reshape(sample_preds, -1)[index]*100:0.2f}%\")\n\nprint(f\"\\nTrue Label: {_LABEL_NAMES[sample_label]}\")\nprint(f\"\\nAttributions for Label: {_LABEL_NAMES[target_label]}\")\nprint(f\"\\nTop 5 Important words: \"\n      f\"{[words[i] for i in tf.argsort(igs, -1, 'DESCENDING')[:5]]}\\n\")\nprint(\"\\nGreen is high importance\/attribution whereas pink is negative importance\/attribution\\n\")\nHTML(\"\".join(list(map(hlstr, words, colors))))","6d22974e":"# 270 -> remove symbols\n# 504; the word `unraveling` and `fraying`, 'chic', downhill, 'scrumptious' are not in vocab\n# missed word 'pesky'\n# the first word oof the sentense is \"this\", it is highlighted green for laabel 1","2a685847":"def tf_lower_and_split_punct(text):\n    text = tf.strings.lower(text)\n    text = tf.strings.regex_replace(text, '[^ a-z0-9\\']', ' ')\n    text = tf.strings.strip(text)\n    return text","7c95ef99":"# [vocab[i] for i in pre_processing_layer([tf_lower_and_split_punct('''If you know one.september and you know what's good for you, then size the heck down. maybe even two sizes if your particularly slim or want the dress to fit tighter. any way, this dress is simply scrumptious! the embroidery at the bodice is so detailed and pretty. the layers are light and soft like silk (but it's rayon\/poly) and the sleeves are sheer and voluminous. i actually prefer the sleeves rolled up because \n# they are too long. neckline features a slit-v with two hooks to close if wanted. co''')])[0]]","7d6d329e":"# t = TextVectorization(output_sequence_length=max_len, \n#                                          name='t')","1f375388":"# t.adapt(X_train)","d91f1914":"# tocab = t.get_vocabulary()","179cf3e6":"# tocab.index('unraveling')","fdcf6102":"Run after this block for a different example","9e1b9a5a":"# Interpretability\n\n### INTEGRATED GRADIENTS for understanding feature importance\nRefer to https:\/\/arxiv.org\/pdf\/1703.01365.pdf for all the details\n\n\n##### VERY IMPORTANT : in Tensorflow, gradients dont pass through Embedding layer; so will get the embedding layer out, and build the rest of the model as `new_model`","d8b6950b":"# Load Data","caa013b9":"# Start Building","c29e1733":"#### Lets look at some examples where predictions were wrong"}}