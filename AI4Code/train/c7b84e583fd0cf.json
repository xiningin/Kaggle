{"cell_type":{"51b5387e":"code","493077de":"code","950852db":"code","35b1d3ed":"code","e5cf39a4":"code","b698ab16":"code","76e303a7":"code","8611e2d6":"code","ace77d8e":"code","d5cd746f":"code","d9049be1":"markdown","e8fa5246":"markdown"},"source":{"51b5387e":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npd.options.display.latex.repr=True\n\nfile_path = '..\/input\/'\n\nload_data_dtype = {'order_id': np.uint32,\n                   'user_id': np.uint32,\n                   'eval_set': 'category',\n                   'order_number': np.uint8,\n                   'order_dow': np.uint8,\n                   'order_hour_of_day': np.uint8,\n                   # pandas 'gotcha'; leave as float:\n                   'days_since_prior_order': np.float16,\n                   'product_id': np.uint16,\n                   'add_to_cart_order': np.uint8,\n                   'reordered': np.bool\n                   }\n\ndf_aisles = pd.read_csv(file_path + 'aisles.csv')\ndf_departments = pd.read_csv(file_path + 'departments.csv')\ndf_products = pd.read_csv(file_path + 'products.csv')\n\n# Specify dtype to reduce memory utilization\ndf_order_products_prior = pd.read_csv(file_path + 'order_products__prior.csv',\n                                      dtype=load_data_dtype\n                                      )\ndf_order_products_train = pd.read_csv(file_path + 'order_products__train.csv',\n                                      dtype=load_data_dtype\n                                      )\ndf_orders = pd.read_csv(file_path + 'orders.csv',\n                        dtype=load_data_dtype\n                        )\n\n# df_prior = full products from all prior orders \ndf_prior = pd.merge(df_orders[df_orders['eval_set'] == 'prior'],\n              df_order_products_prior,\n              on='order_id'\n              )","493077de":"from sklearn.model_selection import train_test_split\n\n# Names of dataset partitions\ndsets = ['train',\n         'test',\n         'kaggle']\n\nusers = dict.fromkeys(dsets)\n\n# Use sklearn utility to partition project users into train and test user lists.\nusers['train'], users['test'] = train_test_split(list(df_orders[df_orders.eval_set == 'train']['user_id']),\n                                                 test_size=0.2,\n                                           random_state=20190513)\n\n# Kaggle submissions test set\nusers['kaggle'] = list(df_orders[df_orders.eval_set == 'test']['user_id'])#.to_list()","950852db":"# Split DataFrames we will use in feature construction into dicts of DataFrames\nprior = dict.fromkeys(dsets)\norders = dict.fromkeys(dsets)\n\nfor ds in dsets:\n    prior[ds] = df_prior[df_prior['user_id'].isin(users[ds])]\n    orders[ds] = df_orders[df_orders['user_id'].isin(users[ds]) & (df_orders.eval_set == 'prior')]","35b1d3ed":"# scipy sparse matrix of number of times particular user has ordered particular product\nUP_orders_num = dict.fromkeys(dsets)\n\nfor ds in dsets:\n    UP_orders_num[ds], _, _ = (prior[ds].groupby(['user_id', 'product_id'])['order_id']\n                             .count()\n                             .apply(pd.to_numeric, downcast='unsigned')\n                             .to_sparse()\n                             .to_coo())","e5cf39a4":"del (df_aisles,\n     df_departments,\n     df_order_products_prior,\n     df_order_products_train,\n     df_orders,\n     df_prior,\n     df_products,\n     prior,\n     orders)","b698ab16":"from sklearn.decomposition import LatentDirichletAllocation\n\nlda = LatentDirichletAllocation(n_jobs=1, # need more disk? PickleError\n                                  learning_method='online'\n                               )","76e303a7":"from sklearn.model_selection import GridSearchCV\n\nparams = {\n    'n_components': [6, 8, 10, 12], # Maybe we can get away with fewer n_components (memory)\n    'learning_decay': [0.75, 0.80, 0.85]\n}\n\nlda_search = GridSearchCV(lda,\n                          param_grid=params,\n                          cv=3,\n                          return_train_score=False)\n\nlda_search.fit(UP_orders_num['train'])\n\nresults = pd.DataFrame(lda_search.cv_results_)","8611e2d6":"current_palette = sns.color_palette(\"Set2\", 3)\n\nplt.figure(figsize=(12,8))\n\nsns.lineplot(data=results,\n             x='param_n_components',\n             y='mean_test_score',\n             hue='param_learning_decay',\n             # https:\/\/github.com\/mwaskom\/seaborn\/issues\/1515\n             palette=current_palette,\n             marker='o'\n            )\n\n# # Would need a good way to show three error bars\n# plt.errorbar(x=results['param_n_components'],\n#              y=results.mean_test_score,\n#              yerr=results.std_test_score,\n#              fmt='none',\n#              color=current_palette[0])\n\nplt.show()","ace77d8e":"# Values for the above plot\nresults[['param_learning_decay',\n         'param_n_components',\n         'mean_test_score']].pivot('param_learning_decay',\n                                  'param_n_components')","d5cd746f":"# std for the values in the above plot\nresults[['param_learning_decay',\n         'param_n_components',\n         'std_test_score']].pivot('param_learning_decay',\n                                  'param_n_components')","d9049be1":"# Latent Dirichlet Allocation GridSearchCV Part 2: Fine Search","e8fa5246":"While the score is better at `n_components = 12`, and is likely even better closer to `n_components = 15`, `n_components = 10` and `learning_decay = 0.85` (or greater) seem reasonable."}}