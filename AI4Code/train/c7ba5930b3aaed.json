{"cell_type":{"4e27a9a1":"code","e9d9a375":"code","c582fcc9":"code","002095ac":"code","42d6091d":"code","ead44133":"code","7d56b496":"code","20321f5d":"code","8d542f3b":"code","8eb1b86e":"code","210f391e":"code","acc85d97":"code","bcc00a03":"code","e362319a":"code","550d175c":"markdown","d9a15f75":"markdown","a22f5a9e":"markdown","d939d423":"markdown","3405c142":"markdown","59048ed0":"markdown"},"source":{"4e27a9a1":"import numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport tensorflow.keras as ks\n\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","e9d9a375":"train_df = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\nprint(f'the shape of training data is {train_df.shape}')\ntest_df = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\nprint(f'the shape of test data is {test_df.shape}')\ntrain_df.sample(3)","c582fcc9":"train_labels = train_df['label']\ntrain_images = train_df.drop(['label'], axis=1)\n\n\ntrain_images = np.array(train_images).reshape(42000, 28, 28, 1)\ntest_images = np.array(test_df).reshape(28000, 28, 28, 1)\n\ntraining_images, validation_images, training_labels, validation_labels = train_test_split(train_images,\n                                                                                          train_labels, \n                                                                                         random_state=0, \n                                                                                         test_size=0.1)\ntrain_labels = to_categorical(training_labels)\nvalidation_labels = to_categorical(validation_labels)","002095ac":"train_datagen = ImageDataGenerator(\n    rescale=1.\/255,\n    rotation_range=10, \n    shear_range=.1, \n    width_shift_range=0.05, \n    height_shift_range=0.05,\n    zoom_range=0.05,\n    horizontal_flip=False, \n    fill_mode='nearest'\n)\n\ntrain_generator = train_datagen.flow(training_images, train_labels, batch_size=100)\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntest_generator = test_datagen.flow(validation_images, validation_labels, batch_size=10)","42d6091d":"model = ks.models.Sequential()\n\nmodel.add(ks.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)))\nmodel.add(ks.layers.BatchNormalization())\nmodel.add(ks.layers.MaxPooling2D((2, 2)))\nmodel.add(ks.layers.Dropout(0.2))\n\nmodel.add(ks.layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\nmodel.add(ks.layers.BatchNormalization())\nmodel.add(ks.layers.MaxPooling2D((2, 2)))\nmodel.add(ks.layers.Dropout(0.2))\n\nmodel.add(ks.layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\nmodel.add(ks.layers.BatchNormalization())\nmodel.add(ks.layers.MaxPooling2D((2, 2)))\nmodel.add(ks.layers.Dropout(0.2))\n\nmodel.add(ks.layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\nmodel.add(ks.layers.BatchNormalization())\nmodel.add(ks.layers.MaxPooling2D((2, 2)))\nmodel.add(ks.layers.Dropout(0.2))\n\n\nmodel.add(ks.layers.Flatten())\n\nmodel.add(ks.layers.Dense(1024, activation='relu'))\nmodel.add(ks.layers.Dense(512, activation='relu'))\nmodel.add(ks.layers.Dense(256, activation='relu'))\n\nmodel.add(ks.layers.Dense(10, activation='softmax'))\n\nmodel.compile(optimizer=ks.optimizers.Adam(lr=1e-3),\n              loss='categorical_crossentropy',\n              metrics=['acc'])","ead44133":"model.summary()","7d56b496":"lr_reduction = ReduceLROnPlateau(monitor = 'val_acc', factor = 0.5, min_lr = 1e-6)\n\ncheckpoint = ModelCheckpoint('.\/trainedModel.hdf5',monitor = 'val_acc', mode = \"max\", save_best_model = True)","20321f5d":"history = model.fit_generator(train_generator, \n                    epochs=50, validation_data=test_generator, \n                   callbacks=[lr_reduction, checkpoint])","8d542f3b":"plt.style.use('ggplot')\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.figure(figsize=(6, 5))\n\nplt.plot(epochs, acc, 'r', label='training_accuracy')\nplt.plot(epochs, val_acc, 'b', label='validation_accuracy')\nplt.ylim(0.98, 1)\nplt.title('Training and Validation Accuarcy')\nplt.xlabel('-----epochs--->')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.figure(figsize=(6, 5))\n\nplt.plot(epochs, loss, 'r', label='training_loss')\nplt.plot(epochs, val_loss, 'b', label='validation_loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('----epochs--->')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","8eb1b86e":"preds = model.predict(test_images)\npreds","210f391e":"sub = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')\nprint(sub.shape)\n\npreds = np.argmax(preds, axis=-1)\nsub.head()","acc85d97":"sub['Label'] = preds","bcc00a03":"sub.head()","e362319a":"sub.to_csv('submission2.csv', index=False)","550d175c":"### Reshaping the images and spliting the training data into train and validation data","d9a15f75":"### Augmentation ","a22f5a9e":"### Importing Libraries","d939d423":"### Validating model using Accuracy and Loss chart ","3405c142":"### Reading the train and test data","59048ed0":"### Model"}}