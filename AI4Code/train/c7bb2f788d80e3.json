{"cell_type":{"6611a15c":"code","001f7f90":"code","82018c47":"code","749b7cd8":"code","42a6eee0":"code","ea43da53":"code","2f82ef5d":"code","bffa158a":"code","ac50fefa":"code","4fd02a2a":"code","4c64707c":"code","9ba2b326":"code","f0574f9b":"code","550bc2c9":"code","cd27f68a":"code","4f2460dd":"code","5256ada8":"code","fce0f909":"code","073b6508":"code","62ee3914":"code","66e07ff9":"code","6038189b":"code","c4df68da":"code","1a58e3e0":"code","f453560e":"code","13e71d72":"code","970a47d5":"code","35a05fa2":"code","0ca155bd":"code","a50d858b":"code","fe5b1cba":"code","6c19caed":"code","e5c272ea":"code","1cd87077":"code","705ae5b1":"code","d5dc98ec":"code","962e7f0c":"code","390617fe":"code","5f863f8d":"code","2247695e":"code","29bfefe7":"code","cfbb0739":"code","fd8af3ae":"code","21ecf898":"code","0d215897":"code","37dc29fa":"code","ada3cfbb":"code","3cfafdf6":"code","3e7205c6":"code","e6a51c99":"code","4cc5afd0":"code","9f53d526":"markdown","8bfdee3e":"markdown","062a4cd2":"markdown","db43645c":"markdown","6497b358":"markdown","175fa4e0":"markdown","c789adef":"markdown","4e7482d6":"markdown","b7b465fc":"markdown","35167aa7":"markdown","86459552":"markdown","09f698da":"markdown","2b141dd9":"markdown","5a583341":"markdown","61ec728e":"markdown","2c10304f":"markdown","cfb3acb3":"markdown"},"source":{"6611a15c":"DO_SUBMISSION = True\nDO_TRAIN_FOR_ENSEMBLE = False\nDO_VIRTUAL_SUBMISSION = False\nassert (sum([DO_SUBMISSION, DO_TRAIN_FOR_ENSEMBLE, DO_VIRTUAL_SUBMISSION]) == 1), \"select `ONE` mode\"","001f7f90":"%%time\n# for tabnet\n!pip install --no-index --find-links ..\/input\/pytorchtabnet\/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet","82018c47":"%%time\n# for nn.py\n!pip install ..\/input\/iterative-stratification\/iterative-stratification-master\/","749b7cd8":"import os\nimport gc\nimport sys\nimport random\nimport shutil\nimport warnings\nimport typing as tp\nfrom pathlib import Path\nfrom copy import deepcopy\n\nimport yaml\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import log_loss\n\nwarnings.resetwarnings()\nwarnings.simplefilter('ignore', FutureWarning)\nwarnings.simplefilter('ignore', DeprecationWarning)\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"","42a6eee0":"import torch\nfrom torch import nn\nfrom torch.utils import data\ntorch.backends.cudnn.benchmark = True\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nsys.path.append(\"..\/input\/pytorch-pfn-extras\/pytorch-pfn-extras-0.3.1\/\")\nimport pytorch_pfn_extras as ppe\nfrom pytorch_pfn_extras.training import extensions as ppe_extensions","ea43da53":"def generate_virtual_private(test_feat, smpl_sub, rate_to_pub=4):\n    \n    assert (test_feat.sig_id == smpl_sub.sig_id).all()\n    \n    # public \u306f\u542b\u307e\u308c\u308b\u306e\u3067\u3001\u305d\u306e\u307e\u307e\u5165\u308c\u308b.\n    test_feat_list = [test_feat]\n    smpl_sub_list = [smpl_sub]\n    \n    for i in range(1, rate_to_pub):\n        tmp_test_feat = test_feat.copy()\n        tmp_smpl_sub = smpl_sub.copy()\n        \n        # #  id \u3092\u5909\u66f4\n        tmp_test_feat.sig_id = tmp_test_feat.sig_id + f\"_{i}\"\n        tmp_smpl_sub.sig_id = tmp_smpl_sub.sig_id + f\"_{i}\"\n        assert (tmp_test_feat.sig_id == tmp_smpl_sub.sig_id).all()\n        \n        # # `c-*` \u3068 `g-*` \u306b\u9069\u5f53\u306a\u5024\u3092\u52a0\u3048\u308b. \u8907\u88fd\u3057\u3066\u5897\u3084\u3059\u3060\u3051\u3060\u3068\u3059\u308a\u629c\u3051\u308b\u5834\u5408\u304c\u3042\u3063\u305f\u305f\u3081.\n        tmp_test_feat.iloc[:, 4:] += i * 10\n        assert (tmp_test_feat.iloc[:, 4:] != test_feat.iloc[:, 4:]).all().all()\n        \n        test_feat_list.append(tmp_test_feat)\n        smpl_sub_list.append(tmp_smpl_sub)\n        \n    # # \u7d50\u5408\n    test_feat_concat = pd.concat(test_feat_list, axis=0, ignore_index=True)\n    smpl_sub_concat = pd.concat(smpl_sub_list, axis=0, ignore_index=True)\n    \n    return test_feat_concat, smpl_sub_concat","2f82ef5d":"MODEL_NAMES = [\"NN(drugCV)\", \"TabNet\", \"ResNet\", \"ThrNN\", \"ThrNN(drugCV)\"]","bffa158a":"%%time\nif DO_SUBMISSION:\n    !python ..\/input\/moa-nn-tabnet-fix5\/nn-inference-0.01833.py\n\nelif DO_VIRTUAL_SUBMISSION:\n    !python ..\/input\/moa-nn-tabnet-fix5\/virtual-nn-use-train-public-inference.py\n\nelif DO_TRAIN_FOR_ENSEMBLE:\n    pass\nelse:\n    raise ValueError","ac50fefa":"%%time\nif DO_SUBMISSION:\n    !python ..\/input\/moa-nn-tabnet-fix5\/tabnet-inference-0.01840.py\n\nelif DO_VIRTUAL_SUBMISSION:\n    !python ..\/input\/moa-nn-tabnet-fix5\/virtual-tabnet-inference-add-param-n-shared.py\n\nelif DO_TRAIN_FOR_ENSEMBLE:\n    pass\nelse:\n    raise ValueError","4fd02a2a":"%%time\nif DO_SUBMISSION:\n    !python ..\/input\/moa-takapy-script\/tf-rn-transfer-1layerother-selcol100.py  # 0.01862\n\nelif DO_VIRTUAL_SUBMISSION:\n    !python ..\/input\/moa-takapy-script\/virtual-tf-rn-transfer-1layerother-selcol100.py\n\nelif DO_TRAIN_FOR_ENSEMBLE:\n    pass\nelse:\n    raise ValueError","4c64707c":"%%time\nif DO_SUBMISSION:\n    !python ..\/input\/moa-tawara-scripts-for-final-submission\/moa-for-final-thrnn-seed-cv-0.01836.py\n\nelif DO_VIRTUAL_SUBMISSION:\n    !python ..\/input\/moa-tawara-scripts-for-final-submission\/virtual-moa-for-final-thrnn-seed-cv-0.01836.py\n\nelif DO_TRAIN_FOR_ENSEMBLE:\n    pass\nelse:\n    raise ValueError","9ba2b326":"%%time\nif DO_SUBMISSION:\n    !python ..\/input\/moa-tawara-scripts-for-final-submission\/moa-for-final-thrnn-drug-seed-cv-0.01841.py\n\nelif DO_VIRTUAL_SUBMISSION:\n    !python ..\/input\/moa-tawara-scripts-for-final-submission\/virtual-moa-for-final-thrnn-drug-seed-cv-0.01841.py\n\nelif DO_TRAIN_FOR_ENSEMBLE:\n    pass\nelse:\n    raise ValueError","f0574f9b":"TARGET_COL = ['5-alpha_reductase_inhibitor', '11-beta-hsd1_inhibitor', 'acat_inhibitor', 'acetylcholine_receptor_agonist', 'acetylcholine_receptor_antagonist', 'acetylcholinesterase_inhibitor', 'adenosine_receptor_agonist', 'adenosine_receptor_antagonist', 'adenylyl_cyclase_activator', 'adrenergic_receptor_agonist', 'adrenergic_receptor_antagonist', 'akt_inhibitor', 'aldehyde_dehydrogenase_inhibitor', 'alk_inhibitor', 'ampk_activator', 'analgesic', 'androgen_receptor_agonist', 'androgen_receptor_antagonist', 'anesthetic_-_local', 'angiogenesis_inhibitor', 'angiotensin_receptor_antagonist', 'anti-inflammatory', 'antiarrhythmic', 'antibiotic', 'anticonvulsant', 'antifungal', 'antihistamine', 'antimalarial', 'antioxidant', 'antiprotozoal', 'antiviral', 'apoptosis_stimulant', 'aromatase_inhibitor', 'atm_kinase_inhibitor', 'atp-sensitive_potassium_channel_antagonist', 'atp_synthase_inhibitor', 'atpase_inhibitor', 'atr_kinase_inhibitor', 'aurora_kinase_inhibitor', 'autotaxin_inhibitor', 'bacterial_30s_ribosomal_subunit_inhibitor', 'bacterial_50s_ribosomal_subunit_inhibitor', 'bacterial_antifolate', 'bacterial_cell_wall_synthesis_inhibitor', 'bacterial_dna_gyrase_inhibitor', 'bacterial_dna_inhibitor', 'bacterial_membrane_integrity_inhibitor', 'bcl_inhibitor', 'bcr-abl_inhibitor', 'benzodiazepine_receptor_agonist', 'beta_amyloid_inhibitor', 'bromodomain_inhibitor', 'btk_inhibitor', 'calcineurin_inhibitor', 'calcium_channel_blocker', 'cannabinoid_receptor_agonist', 'cannabinoid_receptor_antagonist', 'carbonic_anhydrase_inhibitor', 'casein_kinase_inhibitor', 'caspase_activator', 'catechol_o_methyltransferase_inhibitor', 'cc_chemokine_receptor_antagonist', 'cck_receptor_antagonist', 'cdk_inhibitor', 'chelating_agent', 'chk_inhibitor', 'chloride_channel_blocker', 'cholesterol_inhibitor', 'cholinergic_receptor_antagonist', 'coagulation_factor_inhibitor', 'corticosteroid_agonist', 'cyclooxygenase_inhibitor', 'cytochrome_p450_inhibitor', 'dihydrofolate_reductase_inhibitor', 'dipeptidyl_peptidase_inhibitor', 'diuretic', 'dna_alkylating_agent', 'dna_inhibitor', 'dopamine_receptor_agonist', 'dopamine_receptor_antagonist', 'egfr_inhibitor', 'elastase_inhibitor', 'erbb2_inhibitor', 'estrogen_receptor_agonist', 'estrogen_receptor_antagonist', 'faah_inhibitor', 'farnesyltransferase_inhibitor', 'fatty_acid_receptor_agonist', 'fgfr_inhibitor', 'flt3_inhibitor', 'focal_adhesion_kinase_inhibitor', 'free_radical_scavenger', 'fungal_squalene_epoxidase_inhibitor', 'gaba_receptor_agonist', 'gaba_receptor_antagonist', 'gamma_secretase_inhibitor', 'glucocorticoid_receptor_agonist', 'glutamate_inhibitor', 'glutamate_receptor_agonist', 'glutamate_receptor_antagonist', 'gonadotropin_receptor_agonist', 'gsk_inhibitor', 'hcv_inhibitor', 'hdac_inhibitor', 'histamine_receptor_agonist', 'histamine_receptor_antagonist', 'histone_lysine_demethylase_inhibitor', 'histone_lysine_methyltransferase_inhibitor', 'hiv_inhibitor', 'hmgcr_inhibitor', 'hsp_inhibitor', 'igf-1_inhibitor', 'ikk_inhibitor', 'imidazoline_receptor_agonist', 'immunosuppressant', 'insulin_secretagogue', 'insulin_sensitizer', 'integrin_inhibitor', 'jak_inhibitor', 'kit_inhibitor', 'laxative', 'leukotriene_inhibitor', 'leukotriene_receptor_antagonist', 'lipase_inhibitor', 'lipoxygenase_inhibitor', 'lxr_agonist', 'mdm_inhibitor', 'mek_inhibitor', 'membrane_integrity_inhibitor', 'mineralocorticoid_receptor_antagonist', 'monoacylglycerol_lipase_inhibitor', 'monoamine_oxidase_inhibitor', 'monopolar_spindle_1_kinase_inhibitor', 'mtor_inhibitor', 'mucolytic_agent', 'neuropeptide_receptor_antagonist', 'nfkb_inhibitor', 'nicotinic_receptor_agonist', 'nitric_oxide_donor', 'nitric_oxide_production_inhibitor', 'nitric_oxide_synthase_inhibitor', 'norepinephrine_reuptake_inhibitor', 'nrf2_activator', 'opioid_receptor_agonist', 'opioid_receptor_antagonist', 'orexin_receptor_antagonist', 'p38_mapk_inhibitor', 'p-glycoprotein_inhibitor', 'parp_inhibitor', 'pdgfr_inhibitor', 'pdk_inhibitor', 'phosphodiesterase_inhibitor', 'phospholipase_inhibitor', 'pi3k_inhibitor', 'pkc_inhibitor', 'potassium_channel_activator', 'potassium_channel_antagonist', 'ppar_receptor_agonist', 'ppar_receptor_antagonist', 'progesterone_receptor_agonist', 'progesterone_receptor_antagonist', 'prostaglandin_inhibitor', 'prostanoid_receptor_antagonist', 'proteasome_inhibitor', 'protein_kinase_inhibitor', 'protein_phosphatase_inhibitor', 'protein_synthesis_inhibitor', 'protein_tyrosine_kinase_inhibitor', 'radiopaque_medium', 'raf_inhibitor', 'ras_gtpase_inhibitor', 'retinoid_receptor_agonist', 'retinoid_receptor_antagonist', 'rho_associated_kinase_inhibitor', 'ribonucleoside_reductase_inhibitor', 'rna_polymerase_inhibitor', 'serotonin_receptor_agonist', 'serotonin_receptor_antagonist', 'serotonin_reuptake_inhibitor', 'sigma_receptor_agonist', 'sigma_receptor_antagonist', 'smoothened_receptor_antagonist', 'sodium_channel_inhibitor', 'sphingosine_receptor_agonist', 'src_inhibitor', 'steroid', 'syk_inhibitor', 'tachykinin_antagonist', 'tgf-beta_receptor_inhibitor', 'thrombin_inhibitor', 'thymidylate_synthase_inhibitor', 'tlr_agonist', 'tlr_antagonist', 'tnf_inhibitor', 'topoisomerase_inhibitor', 'transient_receptor_potential_channel_antagonist', 'tropomyosin_receptor_kinase_inhibitor', 'trpv_agonist', 'trpv_antagonist', 'tubulin_inhibitor', 'tyrosine_kinase_inhibitor', 'ubiquitin_specific_protease_inhibitor', 'vegfr_inhibitor', 'vitamin_b', 'vitamin_d_receptor_agonist', 'wnt_inhibitor']","550bc2c9":"# # functions for sorting sig_id\n\ndef del_control(df: pd.DataFrame()) -> pd.DataFrame():\n    train = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\n    df['cp_type'] = train['cp_type']\n    df = df.loc[df['cp_type']=='trt_cp'].reset_index(drop=True)\n    df = df.drop('cp_type', axis=1)\n    return df\n\ndef drop_sig_id(df: pd.DataFrame()) -> pd.DataFrame():\n    return df.drop('sig_id', axis=1)\n\ndef fix_oof(df: pd.DataFrame()) -> pd.DataFrame():\n    if df.shape[0] != 21948:\n        df = del_control(df)\n    if 'sig_id' in df.columns.tolist():\n        df = drop_sig_id(df)\n    return df","cd27f68a":"oof_list = []\n\n# nn(drugCV)\noof_list.append(fix_oof(\n    pd.read_pickle('..\/input\/nn-use-train-public\/oof.pkl')))\n\n# tabnet\noof_list.append(fix_oof(\n    pd.DataFrame(\n        np.load('..\/input\/tabnet-train-public-add-n-shared-1\/oof_tabnet.npy'),\n        columns=TARGET_COL))\n)\n\n# # resnet\noof_list.append(fix_oof(\n    pd.read_csv(\"..\/input\/moa-takapy-tf-resnet-transfer\/oof_prediction_takapy_resnet.csv\")))\n\n# thrnn\noof_list.append(fix_oof(\n    pd.read_csv(\"..\/input\/moa-weight-thrnn-seed-cv\/oof_prediction.csv\")))\n\n# thrnn(drugCV)\noof_list.append(fix_oof(\n    pd.read_csv(\"..\/input\/moa-weight-thrnn-drug-seed-cv\/oof_prediction.csv\")))","4f2460dd":"y_true = fix_oof(pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv'))\n\nfor i, name in enumerate(MODEL_NAMES):\n    print(f\"[{name}]\")\n    print(\"shape:\", oof_list[i].shape)\n    print(\"local logloss:\", log_loss(y_true.values.ravel(), oof_list[i].values.ravel()))","5256ada8":"class MoAStackingDataset(data.Dataset):\n    \n    def __init__(self, feat: np.ndarray, label: np.ndarray = None):\n        \"\"\"\"\"\"\n        self.feat = feat\n        if label is None:\n            self.label = np.full((len(feat), 1), -1)\n        else:\n            self.label = label\n        self.model_order = None\n        \n    def __len__(self):\n        \"\"\"\"\"\"\n        return len(self.feat)\n    \n    def __getitem__(self, index: int):\n        \"\"\"\"\"\"\n        return [\n            torch.from_numpy(self.feat[index]).float(),\n            torch.from_numpy(self.label[index]).float()\n        ]\n    \n    def reset_model_order(self):\n        \"\"\"Dummy Method.\"\"\"\n        pass\n        \n    def shuffle_model_order(self, seed):\n        \"\"\"Dummy Method.\"\"\"\n        pass\n\n\nclass MoAStackingDatasetForCNN(data.Dataset):\n    \n    def __init__(self, feat: np.ndarray, label: np.ndarray = None):\n        \"\"\"\"\"\"\n        self.feat = feat\n        if label is None:\n            self.label = np.full((len(feat), 1), -1)\n        else:\n            self.label = label\n        self.reset_model_order()\n        \n    def reset_model_order(self):\n        self.model_order = np.arange(self.feat.shape[-1])\n        \n    def shuffle_model_order(self, seed):\n        np.random.seed(seed)\n        self.model_order = np.random.permutation(self.model_order)\n        \n    def __len__(self):\n        \"\"\"\"\"\"\n        return len(self.feat)\n    \n    def __getitem__(self, index: int):\n        \"\"\"\"\"\"\n        return [\n            torch.from_numpy(self.feat[index][..., self.model_order]).float(),\n            torch.from_numpy(self.label[index]).float()\n        ]\n    \n    \nclass MoAStackingDatasetForGCN(data.Dataset):\n    \n    def __init__(self, feat: np.ndarray, label: np.ndarray = None):\n        \"\"\"\"\"\"\n        self.feat = feat\n        if label is None:\n            self.label = np.full((len(feat), 1), -1)\n        else:\n            self.label = label\n        self.model_order = None\n        \n    def reset_model_order(self):\n        \"\"\"Dummy Method.\"\"\"\n        pass\n        \n    def shuffle_model_order(self, seed):\n        \"\"\"Dummy Method.\"\"\"\n        pass\n        \n    def __len__(self):\n        \"\"\"\"\"\"\n        return len(self.feat)\n    \n    def __getitem__(self, index: int):\n        \"\"\"\"\"\"\n        return [\n            torch.from_numpy(self.feat[index]).float(),\n            torch.from_numpy(self.label[index]).float()\n        ]","fce0f909":"def get_activation(activ_name: str=\"relu\"):\n    \"\"\"\"\"\"\n    act_dict = {\n        \"relu\": nn.ReLU(),\n        \"tanh\": nn.Tanh(),\n        \"sigmoid\": nn.Sigmoid(),\n        \"identity\": nn.Identity()}\n    if activ_name in act_dict:\n        return act_dict[activ_name]\n    elif re.match(r\"^htanh\\_\\d{4}$\", activ_name):\n        bound = int(activ_name[-4:]) \/ 1000\n        return nn.Hardtanh(-bound, bound)\n    else:\n        raise NotImplementedError\n\nclass LBAD(nn.Module):\n    \"\"\"Linear (-> BN) -> Activation (-> Dropout)\"\"\"\n    \n    def __init__(\n        self, in_features: int, out_features: int, drop_rate: float=0.0,\n        use_bn: bool=False, use_wn: bool=False, activ: str=\"relu\"\n    ):\n        \"\"\"\"\"\"\n        super(LBAD, self).__init__()\n        layers = [nn.Linear(in_features, out_features)]\n        if use_wn:\n            layers[0] = nn.utils.weight_norm(layers[0])\n        \n        if use_bn:\n            layers.append(nn.BatchNorm1d(out_features))\n        \n        layers.append(get_activation(activ))\n        \n        if drop_rate > 0:\n            layers.append(nn.Dropout(drop_rate))\n        \n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        return self.layers(x)\n    \n    \nclass BDLA(nn.Module):\n    \"\"\"(BN -> Dropout ->) Linear -> Activation\"\"\"\n    \n    def __init__(\n        self, in_features: int, out_features: int, drop_rate: float=0.0,\n        use_bn: bool=False, use_wn: bool=False, activ: str=\"relu\"\n    ):\n        \"\"\"\"\"\"\n        super(BDLA, self).__init__()\n        layers = []\n        if use_bn:\n            layers.append(nn.BatchNorm1d(in_features))\n            \n        if drop_rate > 0:\n            layers.append(nn.Dropout(drop_rate))\n        \n        layers.append(nn.Linear(in_features, out_features))\n        if use_wn:\n            layers[-1] = nn.utils.weight_norm(layers[-1])\n            \n        layers.append(get_activation(activ))\n        \n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        return self.layers(x)\n    \n\nclass LABD(nn.Module):\n    \"\"\"Linear -> Activation (-> BN -> Dropout) \"\"\"\n    \n    def __init__(\n        self, in_features: int, out_features: int, drop_rate: float=0.0,\n        use_bn: bool=False, use_wn: bool=False, activ: str=\"relu\"\n    ):\n        \"\"\"\"\"\"\n        super(LABD, self).__init__()\n        layers = [nn.Linear(in_features, out_features), get_activation(activ)]\n        \n        if use_wn:\n            layers[0] = nn.utils.weight_norm(layers[0])\n        \n        if use_bn:\n            layers.append(nn.BatchNorm1d(out_features))\n        \n        if drop_rate > 0:\n            layers.append(nn.Dropout(drop_rate))\n        \n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        return self.layers(x)","073b6508":"# # for GCNs\ndef vector_wise_matmul(X: torch.Tensor, W: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    See input matrixes X as bags of vectors, and multiply corresponding weight matrices by vector.\n    \n    Args:\n        X: Input Tensor, shape: (batch_size, **n_vectors**, in_features)\n        W: Weight Tensor, shape: (**n_vectors**, out_features, in_features)\n    \"\"\"\n    X = torch.transpose(X, 0, 1)  # shape: (n_vectors, batch_size, in_features)\n    W = torch.transpose(W, 1, 2)  # shape: (n_vectors, in_features, out_features)\n    H = torch.matmul(X, W)        # shape: (n_vectors, batch_size, out_features)\n    H = torch.transpose(H, 0, 1)  # shape: (batch_size, n_vectors, out_features)\n    \n    return H\n\n\ndef vector_wise_shared_matmul(X: torch.Tensor, W: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    See input matrixes X as bags of vectors, and multiply **shared** weight matrices.\n    \n    Args:\n        X: Input Tensor, shape: (batch_size, **n_vectors**, in_features)\n        W: Weight Tensor, shape: (out_features, in_features)\n    \"\"\"\n    # W = torch.transpose(W, 0, 1)  # shape: (in_features, out_features)\n    # H = torch.matmul(X, W)        # shape: (batch_size, n_vectors, out_features)\n    \n    H = nn.functional.linear(X, W)  # shape: (batch_size, n_vectors, out_features)\n    \n    return H","62ee3914":"def _calculate_fan_in_and_fan_out_for_vwl(tensor) -> tp.Tuple[int]:\n    \"\"\"\n    Input tensor: (n_vectors, out_features, in_features) or (out_features, in_features)\n    \"\"\"\n    dimensions = tensor.dim()\n    if dimensions < 2:\n        raise ValueError(\"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\")\n\n    fan_in = tensor.size(-1)\n    fan_out = tensor.size(-2)\n\n    return fan_in, fan_out\n    \n\ndef _calculate_correct_fan_for_vwl(tensor, mode) -> int:\n    \"\"\"\"\"\"\n    mode = mode.lower()\n    valid_modes = ['fan_in', 'fan_out']\n    if mode not in valid_modes:\n        raise ValueError(\"Mode {} not supported, please use one of {}\".format(mode, valid_modes))\n\n    fan_in, fan_out = _calculate_fan_in_and_fan_out_for_vwl(tensor)\n    return fan_in if mode == 'fan_in' else fan_out\n\n\ndef kaiming_uniform_for_vwl(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu'):\n    \"\"\"\"\"\"\n    fan = _calculate_correct_fan_for_vwl(tensor, mode)\n    gain = nn.init.calculate_gain(nonlinearity, a)\n    std = gain \/ np.sqrt(fan)\n    bound = np.sqrt(3.0) * std  # Calculate uniform bounds from standard deviation\n    with torch.no_grad():\n        return tensor.uniform_(-bound, bound)","66e07ff9":"class VectorWiseLinear(nn.Module):\n    \"\"\"\n    For mini batch which have several matrices,\n    see as these matrixes as bags of vectors, and multiply weight matrices by vector.\n    \n    input    X: (batch_size, **n_vectors**, in_features)\n    weight W: (**n_vector**, out_features, in_features)\n    output  Y: (batch_size, **n_vectors**, out_features)\n\n    **Note**: For simplicity, bias is not described.\n    \n    X and W are can be seen as below.\n    X: [\n            [vec_{ 1, 1}, vec_{ 1, 2}, ... vec_{ 1, n_vectors}],\n            [vec_{ 2, 1}, vec_{ 2, 2}, ... vec_{ 2, n_vectors}],\n                                            .\n                                            .\n            [vec_{bs, 1}, vec_{bs, 2}, ... vec_{bs, n_vectors}]\n        ]\n    W: [\n            Mat_{1}, Mat_{2}, ... , Mat_{n_vectors}\n        ]\n    Then Y is calclauted as:\n    Y: [\n        [ Mat_{1} vec_{ 1, 1}, Mat_{2} vec_{ 1, 2}, ... Mat_{n_vectors} vec_{ 1, n_vectors}],\n        [ Mat_{1} vec_{ 2, 1}, Mat_{2} vec_{ 2, 2}, ... Mat_{n_vectors} vec_{ 2, n_vectors}],\n        .\n        .\n        [ Mat_{1} vec_{bs, 1}, Mat_{2} vec_{bs, 2}, ... Mat_{n_vectors} vec_{bs, n_vectors}],\n    ]\n    \"\"\"\n    \n    def __init__(\n        self,\n        in_features: int, out_features: int, n_vectors: int,\n        bias: bool=True, weight_shared: bool=True\n    ) -> None:\n        \"\"\"Initialize.\"\"\"\n        super(VectorWiseLinear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.n_vectors = n_vectors\n        self.weight_shared = weight_shared\n        \n        if self.weight_shared:\n            self.weight = nn.Parameter(\n                torch.Tensor(self.out_features, self.in_features))\n            self.matmul_func = vector_wise_shared_matmul\n        else:\n            self.weight = nn.Parameter(\n                torch.Tensor(self.n_vectors, self.out_features, self.in_features))\n            self.matmul_func = vector_wise_matmul\n            \n        if bias:\n            self.bias = nn.Parameter(torch.Tensor(out_features))\n        else:\n            self.register_parameter('bias', None)\n            \n        self.reset_parameters()\n        \n    def reset_parameters(self) -> None:\n        \"\"\"Initialize weight and bias.\"\"\"\n        kaiming_uniform_for_vwl(self.weight, a=np.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = _calculate_fan_in_and_fan_out_for_vwl(self.weight)\n            bound = 1 \/ np.sqrt(fan_in)\n            nn.init.uniform_(self.bias, -bound, bound)\n             \n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        \"\"\"Forward.\"\"\"\n        H = self.matmul_func(X, self.weight)\n        if self.bias is not None:\n            H = H + self.bias\n        \n        return H","6038189b":"class GraphConv(nn.Module):\n    \"\"\"Basic Graph Convolution Layer.\"\"\"\n    \n    def __init__(\n        self, \n        in_channels: int, out_channels: int, n_nodes: int, shrare_msg: bool=True,\n        model_self: bool=True, share_model_self: bool=True,\n        bias: bool=True, share_bias: bool=True\n    ) -> None:\n        \"\"\"Intialize.\"\"\"\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.n_nodes = n_nodes\n        self.model_self = model_self\n        super(GraphConv, self).__init__()\n        \n        # # message\n        self.msg = VectorWiseLinear(\n            in_channels, out_channels, n_nodes, False, shrare_msg)\n\n        # # self-modeling\n        if model_self:\n            self.model_self = VectorWiseLinear(\n                in_channels, out_channels, n_nodes, False, share_model_self)\n        \n        # # bias\n        if bias:\n            if share_bias:\n                self.bias = nn.Parameter(torch.Tensor(out_channels))\n            else:\n                self.bias = nn.Parameter(torch.Tensor(n_nodes, out_channels))\n            bound = 1 \/ np.sqrt(out_channels)\n            nn.init.uniform_(self.bias, -bound, bound)\n        else:\n            self.register_parameter('bias', None)\n            \n    \n    def forward(self, X: torch.Tensor, A: torch.Tensor, W: torch.Tensor=None) -> torch.Tensor:\n        \"\"\"Forward.\n        \n        Args:\n            X: (batch_size, n_nodes, n_channels)\n                Array which represents bags of vectors.\n                X[:, i, :] are corresponded to feature vectors of node i.\n            A: (batch_size, n_nodes, n_nodes)\n                Array which represents adjacency matrices.\n                A[:, i, j] are corresponded to weights (scalar) of edges from node j to node i.\n            W: (batch_size, n_nodes, n_nodes)\n                Array which represents weight matrices between nodes.\n        \"\"\"\n        if W is not None:\n            A = A * W  # shape: (batch_size, n_nodes, n_nodes)\n        \n        # # update message\n        M = X  #  shape: (batch_size, n_nodes, in_channels)\n        # # # send message\n        M = self.msg(M)  # shape: (batch_size, n_nodes, out_channels)\n        # # # aggregate\n        M = torch.matmul(A, M)  # shape: (batch_size, n_nodes, out_channels)\n            \n        # # update node\n        # # # self-modeling\n        H = M\n        if self.model_self:\n            H = H + self.model_self(X)\n        if self.bias is not None:\n            H = H + self.bias\n        \n        return H","c4df68da":"class MLP(nn.Module):\n    \"\"\"Stacked Dense layers\"\"\"\n    \n    def __init__(\n        self, n_features_list: tp.List[int], use_tail_as_out: bool=False,\n        drop_rate: float=0.0, use_bn: bool=False, use_wn: bool=False,\n        activ:str=\"relu\", block_name: str=\"LBAD\"\n    ):\n        \"\"\"\"\"\"\n        super(MLP, self).__init__()\n        n_layers = len(n_features_list) - 1\n        block_class = {\n            \"LBAD\": LBAD, \"BDLA\": BDLA, \"LABD\": LABD}[block_name]\n        layers = []\n        for i in range(n_layers):\n            in_feats, out_feats = n_features_list[i: i + 2]\n            if i == n_layers - 1 and use_tail_as_out:\n                if block_name in [\"BDLA\"]:\n                    layer = block_class(in_feats, out_feats, drop_rate, use_bn,  use_wn, \"identity\")\n                else:\n                    layer = nn.Linear(in_feats, out_feats)\n                    if use_wn:\n                        layer = nn.utils.weight_norm(layer)\n            else:\n                layer = block_class(in_feats, out_feats, drop_rate, use_bn,  use_wn, activ)\n            layers.append(layer)\n                \n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        return self.layers(x)\n\n\nclass CNNStacking1d(nn.Module):\n    \"\"\"1D-CNN for Stacking.\"\"\"\n    \n    def __init__(\n        self, n_models: int,\n        n_channels_list: tp.List[int], use_bias: bool=False,\n        kwargs_head: tp.Dict={},\n    ):\n        \"\"\"\"\"\"\n        super(CNNStacking1d, self).__init__()\n        self.n_conv_layers = len(n_channels_list) - 1\n        for i in range(self.n_conv_layers):\n            in_ch = n_channels_list[i]\n            out_ch = n_channels_list[i + 1]\n            layer = nn.Sequential(\n                nn.Conv1d(\n                    in_ch, out_ch, kernel_size=3, stride=1, padding=0, bias=use_bias),\n                # nn.BatchNorm1d(out_ch),\n                nn.ReLU(inplace=True))\n            setattr(self, \"conv{}\".format(i + 1), layer)\n        \n        kwargs_head[\"n_features_list\"][0] = (n_models - 2 * self.n_conv_layers) * n_channels_list[-1]\n        self.head = MLP(**kwargs_head)\n    \n    def forward(self, x: torch.FloatTensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        bs = x.shape[0]\n        h = x  # shape: (bs, n_classes, n_models)\n        for i in range(self.n_conv_layers):\n            h = getattr(self, \"conv{}\".format(i + 1))(h)\n            \n        h = torch.reshape(h, (bs, -1))\n        h = self.head(h)\n        return h\n    \n    \n    \nclass CNNStacking2d(nn.Module):\n    \"\"\"2D-CNN for Stacking.\"\"\"\n    \n    def __init__(\n        self, n_models: int, n_classes: int,\n        n_channels_list: tp.List[int], use_bias: bool=False,\n        kwargs_head: tp.Dict={},\n    ):\n        \"\"\"\"\"\"\n        super(CNNStacking2d, self).__init__()\n        self.n_conv_layers = len(n_channels_list) - 1\n        for i in range(self.n_conv_layers):\n            in_ch = n_channels_list[i]\n            out_ch = n_channels_list[i + 1]\n            layer = nn.Sequential(\n                nn.Conv2d(\n                    in_ch, out_ch, kernel_size=(1, 3), stride=1, padding=0, bias=use_bias),\n                # nn.BatchNorm2d(out_ch),\n                nn.ReLU(inplace=True))\n            setattr(self, \"conv{}\".format(i + 1), layer)\n        \n        kwargs_head[\"n_features_list\"][0] = (n_models - 2 * self.n_conv_layers) * n_classes * n_channels_list[-1]\n        self.head = MLP(**kwargs_head)\n    \n    def forward(self, x: torch.FloatTensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        bs = x.shape[0]\n        h = x  # shape: (bs, 1, n_classes, n_models)\n        for i in range(self.n_conv_layers):\n            h = getattr(self, \"conv{}\".format(i + 1))(h)\n        \n        h = torch.reshape(h, (bs, -1))\n        h = self.head(h)\n        return h\n    \n    \nclass GCNStacking(nn.Module):\n    \"\"\"GCN for Stacking.\"\"\"\n    \n    def __init__(\n        self, n_classes: int,\n        n_channels_list: tp.List[int],\n        add_self_loop: bool=False,\n        kwargs_head: tp.Dict={},\n    ):\n        \"\"\"\"\"\"\n        super(GCNStacking, self).__init__()\n        self.n_conv_layers = len(n_channels_list) - 1\n        for i in range(self.n_conv_layers):\n            in_ch = n_channels_list[i]\n            out_ch = n_channels_list[i + 1]\n            # layer = CustomGraphConv(in_ch, out_ch, n_classes)\n            layer = GraphConv(\n                in_ch, out_ch, n_classes,\n                shrare_msg=False, share_model_self=False, share_bias=False)\n            setattr(self, \"conv{}\".format(i + 1), layer)\n        \n        self.relu = nn.ReLU(inplace=True)\n        if add_self_loop:\n            adj_mat = torch.ones(n_classes, n_classes) \/ n_classes\n        else:\n            adj_mat = (1 - torch.eye(n_classes, n_classes)) \/ (n_classes - 1) \n        self.register_buffer(\"A\", adj_mat.float())\n               \n        kwargs_head[\"n_features_list\"][0] = n_classes * n_channels_list[-1]\n        self.head = MLP(**kwargs_head)\n    \n    def forward(self, X: torch.FloatTensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        bs, n_classes = X.shape[:2]\n        H = X  # shape: (bs, n_classes, n_models)\n        for i in range(self.n_conv_layers):\n            H = getattr(self, \"conv{}\".format(i + 1))(H, self.A[None, ...])\n            H = self.relu(H)\n        \n        h = torch.reshape(H, (bs, -1))\n        h = self.head(h)\n        return h","1a58e3e0":"class EvalFuncManager(nn.Module):\n    \"\"\"Manager Class for evaluation at the end of epoch\"\"\"\n\n    def __init__(\n        self, iters_per_epoch: int,\n        evalfunc_dict: tp.Dict[str, nn.Module], prefix: str = \"val\"\n    ) -> None:\n        \"\"\"Initialize\"\"\"\n        super(EvalFuncManager, self).__init__()\n        self.tmp_iter = 0\n        self.iters_per_epoch = iters_per_epoch\n        self.prefix = prefix\n        self.metric_names = []\n        for k, v in evalfunc_dict.items():\n            setattr(self, k, v)\n            self.metric_names.append(k)\n        self.reset()\n\n    def reset(self) -> None:\n        \"\"\"Reset State.\"\"\"\n        self.tmp_iter = 0\n        for name in self.metric_names:\n            getattr(self, name).reset()\n\n    def __call__(self, y: torch.Tensor, t: torch.Tensor) -> None:\n        \"\"\"Forward.\"\"\"\n        for name in self.metric_names:\n            getattr(self, name).update(y, t)\n        self.tmp_iter += 1\n\n        if self.tmp_iter == self.iters_per_epoch:\n            ppe.reporting.report({\n                \"{}\/{}\".format(self.prefix, name): getattr(self, name).compute()\n                for name in self.metric_names})\n            self.reset()\n\n\nclass MeanLoss(nn.Module):\n    \n    def __init__(self):\n        super(MeanLoss, self).__init__()\n        self.loss_sum = 0\n        self.n_examples = 0\n        \n    def forward(self, y: torch.Tensor, t: torch.Tensor):\n        \"\"\"Compute metric at once\"\"\"\n        return self.loss_func(y, t)\n\n    def reset(self):\n        \"\"\"Reset state\"\"\"\n        self.loss_sum = 0\n        self.n_examples = 0\n    \n    def update(self, y: torch.Tensor, t: torch.Tensor):\n        \"\"\"Update metric by mini batch\"\"\"\n        self.loss_sum += self(y, t).item() * y.shape[0]\n        self.n_examples += y.shape[0]\n        \n    def compute(self):\n        \"\"\"Compute metric for dataset\"\"\"\n        return self.loss_sum \/ self.n_examples\n    \n\nclass MyLogLoss(MeanLoss):\n    \n    def __init__(self, **params):\n        super(MyLogLoss, self).__init__()\n        self.loss_func = nn.BCEWithLogitsLoss(**params)\n\n\nclass LSBCEWithLogitsLoss(nn.Module):\n    \"\"\"\"\"\"\n    \n    def __init__(self, k: int, alpha: float=0.01):\n        \"\"\"\"\"\"\n        super(LSBCEWithLogitsLoss, self).__init__()\n        self.k = k\n        self.alpha = alpha\n        self.loss_func = nn.BCEWithLogitsLoss()\n        \n    def forward(self, y, t):\n        \"\"\"\"\"\"\n        t_s = t * (1 - self.alpha) + self.alpha \/ self.k\n        loss = self.loss_func(y, t_s)\n        return loss\n\n\nclass MyLSLogLoss(MeanLoss):\n    \n    def __init__(self, **params):\n        super(MyLSLogLoss, self).__init__()\n        self.loss_func = LSBCEWithLogitsLoss(**params)","f453560e":"def run_train_loop(\n    manager, args, model, device,\n    train_loader, optimizer, scheduler, loss_func\n):\n    \"\"\"Run minibatch training loop\"\"\"\n    while not manager.stop_trigger:\n        model.train()\n        for batch in train_loader:\n            x, t = batch\n            with manager.run_iteration():\n                optimizer.zero_grad()\n                y = model(x.to(device))\n                loss = loss_func(y, t.to(device))\n                ppe.reporting.report({'train\/loss': loss.item()})\n                loss.backward()\n                optimizer.step()\n            if scheduler is not None:\n                scheduler.step()\n            \ndef run_eval(args, model, device, batch, eval_manager):\n    \"\"\"Run evaliation for val or test. this function is applied to each batch.\"\"\"\n    model.eval()\n    x, t = batch\n    y = model(x.to(device))\n    eval_manager(y, t.to(device))","13e71d72":"def get_optimizer(settings, model):\n    optimizer = getattr(torch.optim, settings[\"optimizer\"][\"name\"])(\n        model.parameters(), **settings[\"optimizer\"][\"params\"])\n    return optimizer\n\ndef get_scheduler(settings, train_loader, optimizer):\n    if settings[\"scheduler\"][\"name\"] is None:\n        scheduler = None\n    else:\n        if settings[\"scheduler\"][\"name\"] == \"OneCycleLR\":\n            settings[\"scheduler\"][\"params\"][\"epochs\"] = settings[\"globals\"][\"max_epoch\"]\n            settings[\"scheduler\"][\"params\"][\"steps_per_epoch\"] = len(train_loader)\n\n        scheduler = getattr(torch.optim.lr_scheduler, settings[\"scheduler\"][\"name\"])(\n            optimizer, **settings[\"scheduler\"][\"params\"])        \n    return scheduler\n    \ndef get_loss_function(settings):\n    if hasattr(nn, settings[\"loss\"][\"name\"]):\n        loss_func = getattr(nn, settings[\"loss\"][\"name\"])(**settings[\"loss\"][\"params\"])\n    else:\n        loss_func = eval(settings[\"loss\"][\"name\"])(**settings[\"loss\"][\"params\"])\n    return loss_func\n\ndef get_manager(\n    settings, model, device, train_loader, val_loader, optimizer, eval_manager, output_path\n):\n    trigger = ppe.training.triggers.EarlyStoppingTrigger(\n        check_trigger=(1, 'epoch'), monitor='val\/metric', mode=\"min\",\n        patience=settings[\"globals\"][\"patience\"], verbose=True,\n        max_trigger=(settings[\"globals\"][\"max_epoch\"], 'epoch'))\n    \n    manager = ppe.training.ExtensionsManager(\n        model, optimizer, settings[\"globals\"][\"max_epoch\"],\n        iters_per_epoch=len(train_loader), stop_trigger=trigger, out_dir=output_path)\n    \n    log_extentions = [\n        ppe_extensions.observe_lr(optimizer=optimizer),\n        ppe_extensions.LogReport(),\n        ppe_extensions.PlotReport([\"train\/loss\", \"val\/loss\"], 'epoch', filename='loss.png'),\n        ppe_extensions.PlotReport([\"lr\"], 'epoch', filename='lr.png'),\n        ppe_extensions.PrintReport([\n            \"epoch\", \"iteration\", \"lr\", \"train\/loss\", \"val\/loss\", \"val\/metric\", \"elapsed_time\"])\n    ]\n    for ext in log_extentions:\n        manager.extend(ext)\n        \n    manager.extend( # evaluation\n        ppe_extensions.Evaluator(\n            val_loader, model,\n            eval_func=lambda *batch: run_eval(settings, model, device, batch, eval_manager)),\n        trigger=(1, \"epoch\"))\n    \n    manager.extend(  # model snapshot\n        ppe_extensions.snapshot(target=model, filename=\"snapshot_epoch_{.epoch}.pth\"),\n        trigger=ppe.training.triggers.MinValueTrigger(key=\"val\/metric\", trigger=(1, 'epoch')))\n\n    return manager","970a47d5":"def set_random_seed(seed: int = 42, deterministic: bool = False):\n    \"\"\"Set seeds\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n    if deterministic:\n        torch.backends.cudnn.deterministic = True  # type: ignore","35a05fa2":"def run_train_one_fold(\n    settings, model, train_all_dataset, train_val_index, device, output_path):\n    \"\"\"Run training for one fold\"\"\"\n    train_dataset = data.Subset(train_all_dataset, train_val_index[0])\n    val_dataset = data.Subset(train_all_dataset, train_val_index[1])\n    train_loader = data.DataLoader(train_dataset, **settings[\"loader\"][\"train\"])\n    val_loader = data.DataLoader(val_dataset, **settings[\"loader\"][\"val\"])\n    print(\"train: {}, val: {}\".format(len(train_dataset), len(val_dataset)))\n        \n    model.to(device)\n    optimizer = get_optimizer(settings, model)\n    scheduler = get_scheduler(settings, train_loader, optimizer)\n    loss_func = get_loss_function(settings)\n    loss_func.to(device)\n\n    eval_mgr = EvalFuncManager(len(val_loader), {\n        \"loss\": loss_func, \n        \"metric\": MyLogLoss(),\n    })\n    \n    manager = get_manager(\n        settings, model, device, train_loader, val_loader,\n        optimizer, eval_mgr, output_path)\n    \n    run_train_loop(\n        manager, settings, model, device, train_loader, optimizer, scheduler, loss_func)","0ca155bd":"def get_drug_multilabel_index(target_in_drug_df, target_cols_list, vc_num, n_folds, seed):\n    \"\"\"Reference: https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/195195\"\"\"\n\n    # LOCATE DRUGS\n    vc = target_in_drug_df.drug_id.value_counts()\n    vc1 = vc.loc[vc <= vc_num].index\n    vc2 = vc.loc[vc > vc_num].index\n\n    # STRATIFY DRUGS vc_num OR LESS\n    dct1 = {}\n    dct2 = {}\n    skf = MultilabelStratifiedKFold(n_splits=n_folds, random_state=seed, shuffle=True)\n    tmp = target_in_drug_df.groupby('drug_id')[target_cols_list].mean().loc[vc1]\n    if tmp.shape[0] != 0:\n        for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols_list])):\n            dd = {k:fold for k in tmp.index[idxV].values}\n            dct1.update(dd)\n    # STRATIFY DRUGS MORE THAN vc_num\n    skf = MultilabelStratifiedKFold(n_splits=n_folds, random_state=seed, shuffle=True)\n    tmp = target_in_drug_df.loc[target_in_drug_df.drug_id.isin(vc2)].reset_index(drop = True)\n    if tmp.shape[0] != 0:\n        for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[target_cols_list])):\n            dd = {k:fold for k in tmp.sig_id[idxV].values}\n            dct2.update(dd)\n\n    # ASSIGN FOLDS\n    folds = []\n    target_in_drug_df['fold'] = target_in_drug_df.drug_id.map(dct1)\n    target_in_drug_df.loc[target_in_drug_df.fold.isna(),'fold'] = target_in_drug_df.loc[target_in_drug_df.fold.isna(),'sig_id'].map(dct2)\n    target_in_drug_df.fold = target_in_drug_df.fold.astype('int8')\n    folds.append(target_in_drug_df.fold.values)\n\n    return np.stack(folds)[0]\n\n\ndef drug_multilabel_stratified_kfold(k, target_in_drug_df, target_cols_list, n_folds, seed):\n    \"\"\"Split CV considering drug id\"\"\"\n    folds_array = get_drug_multilabel_index(target_in_drug_df, target_cols_list, 18, n_folds, seed)\n    train_idx = [i for i, x in enumerate(folds_array) if x != k]\n    validation_idx = [i for i, x in enumerate(folds_array) if x == k]\n\n    return train_idx, validation_idx","a50d858b":"target_in_drug = pd.merge(\n    pd.read_csv(\"..\/input\/lish-moa\/train_targets_scored.csv\"),\n    pd.read_csv(\"..\/input\/lish-moa\/train_drug.csv\"),\n    how='left', on='sig_id')\n\nprint(target_in_drug.shape)\ntarget_in_drug_trt = del_control(target_in_drug)\nprint(target_in_drug_trt.shape)","fe5b1cba":"X = np.concatenate([df.values for df in oof_list], axis=1)\nprint(X.shape)\n\ny = y_true.values\nprint(y.shape)\n\nX_2d = np.stack([df.values for df in oof_list], axis=2)\nprint(X_2d.shape)\n\nX_3d = X_2d[:, None, ...]\nprint(X_3d.shape)\n\nX_node = np.stack([df.values for df in oof_list], axis=2)","6c19caed":"# train_all_dataset = MoAStackingDataset(X, y)\ntrain_all_dataset = MoAStackingDatasetForCNN(X_2d, y)\n# train_all_dataset = MoAStackingDatasetForCNN(X_3d, y)\n# train_all_dataset = MoAStackingDatasetForGCN(X_node, y)","e5c272ea":"TRAIN_OUTPUT = Path.cwd().parent \/ \"tmp\"\nTRAIN_OUTPUT.mkdir(exist_ok=True)\n\nWORK_DIR = Path.cwd()","1cd87077":"settings_str = \"\"\"\nglobals:\n  seed: 1990\n  seeds_for_avg: [\n    1990,\n    42, 0, 1086, 39\n  ]\n  max_epoch: 20\n  n_folds_split: 7\n  patience: 10\n  cuda_visible_devices: 0\n  device: cuda\n  fast_commit: False\n\nloader:\n  train: {\n    batch_size: 128, shuffle: True, num_workers: 2,\n    pin_memory: True, drop_last: True}\n  val: {\n    batch_size: 256, shuffle: False, num_workers: 2,\n    pin_memory: True, drop_last: False}\n\nmodel:\n  name: CNNStacking1d\n  params:\n    n_models: 5\n    n_channels_list: [206, 512, 1024]\n    use_bias: True\n    kwargs_head:\n        n_features_list: [1024, 2048, 206]\n        use_tail_as_out: True\n        drop_rate: 0.8\n        use_bn: False\n        use_wn: True\n        block_name: LABD\n        \nloss:\n  name: MyLSLogLoss\n  params: {k: 2, alpha: 1.0e-03}\n\noptimizer:\n  name: Adam\n  params: {lr: 1.0e-03}\n\nscheduler:\n  name: OneCycleLR\n  params: {pct_start: 0.1, div_factor: 1.0e+3, max_lr: 1.0e-02}\n\"\"\"\n\nsettings = yaml.safe_load(settings_str)","705ae5b1":"gc.collect()","d5dc98ec":"device = torch.device(settings[\"globals\"][\"device\"])\ncv_score_df_list = []\n\n# if settings[\"globals\"][\"fast_commit\"] and len() == 3624:\nif settings[\"globals\"][\"fast_commit\"]:\n    settings[\"globals\"][\"max_epoch\"] = 1\n\nstgs_list = []\n\n# # seed avg\nfor tmp_seed in settings[\"globals\"][\"seeds_for_avg\"]:\n    stgs_list.append(deepcopy(settings))\n    stgs_list[-1][\"globals\"][\"seed\"] = tmp_seed","962e7f0c":"for m_id, tmp_stgs in enumerate(stgs_list):\n    n_folds = settings[\"globals\"][\"n_folds_split\"]\n    tmp_seed = tmp_stgs[\"globals\"][\"seed\"]\n    train_val_indexs = [\n        drug_multilabel_stratified_kfold(\n            fold_id, target_in_drug_trt, TARGET_COL, n_folds, tmp_seed)\n        for fold_id in range(n_folds)]\n    # # shffule order\n    train_all_dataset.reset_model_order()\n    train_all_dataset.shuffle_model_order(tmp_seed)\n    print(\"[model {}'s order]: {}\".format(m_id, train_all_dataset.model_order))\n    \n    for fold_id in range(n_folds):\n        print(\"[fold: {} - model: {}]\".format(fold_id, m_id))\n        set_random_seed(tmp_seed , True)\n        \n        # # load model\n        model = eval(tmp_stgs[\"model\"][\"name\"])(**tmp_stgs[\"model\"][\"params\"])\n        run_train_one_fold(\n            tmp_stgs, model, train_all_dataset, train_val_indexs[fold_id], device,\n            TRAIN_OUTPUT \/ \"fold{}_model{}\".format(fold_id, m_id))\n        \n        del model\n        gc.collect()\n    \n    best_eval_list = []\n    for fold_id in range(n_folds):\n        output_dir = TRAIN_OUTPUT \/ \"fold{}_model{}\".format(fold_id, m_id)\n        log_df = pd.read_json(output_dir \/ \"log\")\n        best_log = log_df.iloc[log_df[\"val\/metric\"].idxmin()][[\"epoch\", \"train\/loss\", \"val\/loss\", \"val\/metric\"]]\n        best_eval_list.append(best_log)\n\n        # # copy best model\n        best_epoch = int(best_log[\"epoch\"])\n        model_path = output_dir \/ \"snapshot_epoch_{}.pth\".format(best_epoch)\n        cp_path = WORK_DIR \/ \"best_model_fold{}_model{}.pth\".format(fold_id, m_id)\n        shutil.copy(model_path, cp_path)\n\n        # # remove models\n        for model_path in output_dir.glob(\"*.pth\"):\n            model_path.unlink()\n\n    cv_score_df = pd.DataFrame(best_eval_list)\n    cv_score_df.insert(0, \"fold\", range(settings[\"globals\"][\"n_folds_split\"]))\n    cv_score_df.insert(0, \"model\", m_id)\n    del best_eval_list\n    cv_score_df_list.append(cv_score_df)","390617fe":"def order_sub(sub) : \n    return sub.sort_values('sig_id').reset_index(drop=True)","5f863f8d":"if DO_SUBMISSION or DO_VIRTUAL_SUBMISSION:\n    sub_list = [\n        pd.read_csv('.\/submission-sinchir0-nn.csv'),\n        pd.read_csv('.\/submission_sinchir0_tabnet.csv'),\n        pd.read_csv(\".\/submission_takapy_tf-resnet.csv\"),\n        pd.read_csv('.\/submission_tawara_thrnn_seed_cv.csv'),\n        pd.read_csv('.\/submission_tawara_thrnn_drug_seed_cv.csv'),\n    ]\nelse:\n    sub_list = [\n        pd.read_csv('..\/input\/nn-use-train-public\/submission.csv'),\n        pd.read_csv('..\/input\/tabnet-train-public-add-n-shared-1\/submission.csv'),\n        pd.read_csv(\"..\/input\/moa-takapy-tf-resnet-transfer\/submission.csv\"),\n        pd.read_csv('..\/input\/moa-weight-thrnn-seed-cv\/submission.csv'),\n        pd.read_csv('..\/input\/moa-weight-thrnn-drug-seed-cv\/submission.csv'),\n    ]\n    \nfor i, name in enumerate([\"NN(drugCV)\", \"TabNet\", \"ResNet\", \"ThrNN\", \"ThrNN(drugCV)\"]):\n    print(f\"[{name}]:\", sub_list[i].shape)","2247695e":"sub_list = [order_sub(sub_df) for sub_df in sub_list]","29bfefe7":"def inference_function(settings, model, loader, device):\n    model.to(device)\n    model.eval()\n    pred_list = []\n    with torch.no_grad():\n        for x, t in loader:\n            y = model(x.to(device))\n            pred_list.append(y.sigmoid().detach().cpu().numpy())\n        \n        pred_arr = np.concatenate(pred_list)\n        del pred_list\n    return pred_arr","cfbb0739":"16 * 206","fd8af3ae":"X_test = np.concatenate([\n    sub_df.iloc[:, 1:].values for sub_df in sub_list], axis=1)\nprint(X_test.shape)\n\nX_test_2d = np.stack([\n    sub_df.iloc[:, 1:].values for sub_df in sub_list], axis=2)\nprint(X_test_2d.shape)\n\nX_test_3d = X_test_2d[:, None, ...]\nprint(X_test_3d.shape)\n\nX_test_node = np.stack([\n    sub_df.iloc[:, 1:].values for sub_df in sub_list], axis=2)\n\n# test_dataset = MoAStackingDataset(X_test, None)\ntest_dataset = MoAStackingDatasetForCNN(X_test_2d, None)\n# test_dataset = MoAStackingDatasetForCNN(X_test_3d, None)\n# test_dataset = MoAStackingDatasetForGCN(X_test_node,None)","21ecf898":"oof_pred_arr_avg = np.zeros((len(X), 206))\ntest_pred_arr_avg = np.zeros((len(X_test), 206))\n\nfor m_id, tmp_stgs in enumerate(stgs_list):\n    n_folds = tmp_stgs[\"globals\"][\"n_folds_split\"]\n    tmp_seed = tmp_stgs[\"globals\"][\"seed\"]\n    train_val_indexs = [\n        drug_multilabel_stratified_kfold(\n            fold_id, target_in_drug_trt, TARGET_COL, n_folds, tmp_seed)\n        for fold_id in range(n_folds)]\n    \n    oof_pred_arr = np.zeros((len(X), 206))\n    test_preds_arr = np.zeros((n_folds, len(X_test), 206))\n    train_all_dataset.reset_model_order()\n    train_all_dataset.shuffle_model_order(tmp_seed)\n    test_dataset.model_order = train_all_dataset.model_order\n    print(\"[model {}'s order]: {}\".format(m_id, train_all_dataset.model_order))\n    \n    for fold_id in range(n_folds):\n        print(\"[fold: {} - model: {}]\".format(fold_id, m_id))\n        model_path = WORK_DIR \/ \"best_model_fold{}_model{}.pth\".format(fold_id, m_id)\n        model = eval(tmp_stgs[\"model\"][\"name\"])(**tmp_stgs[\"model\"][\"params\"])\n        model.load_state_dict(torch.load(model_path))\n\n        val_index = train_val_indexs[fold_id][1]\n        val_dataset = data.Subset(train_all_dataset, val_index)\n        val_loader = data.DataLoader(val_dataset, **tmp_stgs[\"loader\"][\"val\"])\n\n        val_pred = inference_function(tmp_stgs, model, val_loader, device)\n        oof_pred_arr[val_index] = val_pred\n        del val_dataset; del val_loader; del val_pred\n\n        test_loader = data.DataLoader(test_dataset, **tmp_stgs[\"loader\"][\"val\"])\n        test_pred = inference_function(tmp_stgs, model, test_loader, device)\n        test_preds_arr[fold_id] = test_pred\n\n        del test_loader; del test_pred;\n        gc.collect()\n    \n    oof_pred_arr_avg += oof_pred_arr\n    test_pred_arr_avg += test_preds_arr.mean(axis=0)\n    \n    oof_score = log_loss(y.ravel(), oof_pred_arr.ravel())\n\n    cv_score_df_list[m_id] = cv_score_df_list[m_id].append(\n        {\"model\": m_id, \"fold\": \"oof\",  \"val\/metric\": oof_score,},\n        ignore_index=True)\n    \noof_pred_arr_avg \/= len(stgs_list)\ntest_pred_arr_avg \/= len(stgs_list)","0d215897":"cv_score_all = pd.concat(cv_score_df_list, axis=0)\ncv_score_all","37dc29fa":"cv_score_all.query(\"epoch != epoch\")","ada3cfbb":"print(log_loss(y.ravel(), oof_pred_arr_avg.ravel()))","3cfafdf6":"BLEND = sub_list[0].copy()\nBLEND.iloc[:, 1:] = test_pred_arr_avg\n\nprint(\"shape:\", BLEND.shape)\ndisplay(BLEND.head())","3e7205c6":"if DO_SUBMISSION or DO_TRAIN_FOR_ENSEMBLE:\n    df_test = pd.read_csv(\"..\/input\/lish-moa\/test_features.csv\")\n    submission = pd.read_csv(\"..\/input\/lish-moa\/sample_submission.csv\")\n\nelif DO_VIRTUAL_SUBMISSION:\n    df_test = pd.read_csv(\"..\/input\/lish-moa\/test_features.csv\")\n    submission = pd.read_csv(\"..\/input\/lish-moa\/sample_submission.csv\")\n    print(df_test.shape, submission.shape)\n    df_test, submission = generate_virtual_private(df_test, submission)\n    print(df_test.shape, submission.shape)\n\nelse:\n    raise ValueError","e6a51c99":"# submission = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\n\n# df = pd.read_csv(\"\/kaggle\/input\/lish-moa\/sample_submission.csv\")\ndf = submission.copy()\n\npublic_id = list(df['sig_id'].values)\n\n# df_test = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ntest_id = list(df_test['sig_id'].values)\n\nprivate_id = list(set(test_id)-set(public_id))\n\ndf_submit = pd.DataFrame(index = public_id+private_id, columns=TARGET_COL)\ndf_submit.index.name = 'sig_id'\ndf_submit[:] = 0\ndf_predict = BLEND.copy()\ndf_submit.loc[df_predict.sig_id,:] = df_predict[TARGET_COL].values\ndf_submit.loc[df_test[df_test.cp_type =='ctl_vehicle'].sig_id] = 0\ndf_submit.to_csv('submission.csv',index=True)","4cc5afd0":"print(df_submit.shape)\ndf_submit.head()","9f53d526":"## definition","8bfdee3e":"## preparation","062a4cd2":"### utils for training","db43645c":"### models","6497b358":"## import","175fa4e0":"## pip install","c789adef":"# Preparation","4e7482d6":"## training","b7b465fc":"## load test prediction by Stage1 Models","35167aa7":"## inference test data","86459552":"## mode config","09f698da":"# Inference by Stacking Model","2b141dd9":"## Load OOF predcitions","5a583341":"## Inference Test by Stage1 Models","61ec728e":"# Training Stacking Model","2c10304f":"### dataset","cfb3acb3":"## Make Submission"}}