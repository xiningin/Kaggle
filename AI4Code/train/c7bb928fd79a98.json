{"cell_type":{"1349c56e":"code","305b2df8":"code","d991608f":"code","fda29197":"code","13b6c271":"code","b5a54bce":"code","d9fe2a4c":"code","c3082a6a":"code","548b3130":"code","428f2227":"code","c180de6a":"code","278d9dac":"code","a23386c5":"code","d3a3ed71":"code","9173135b":"code","0cea9df8":"code","bbd2ddb8":"code","e5099842":"markdown","9ea6fdb6":"markdown","e6088c0b":"markdown","f049a986":"markdown","1d62a411":"markdown","0d154255":"markdown","1daaba21":"markdown","5e49381a":"markdown","f6792bd8":"markdown","4462fbf4":"markdown","944bae84":"markdown","11141f75":"markdown","b2eea24c":"markdown","bb5d669b":"markdown"},"source":{"1349c56e":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation","305b2df8":"data = pd.read_csv(\"..\/input\/calcofi\/bottle.csv\")\ndata.info()","d991608f":"#just some typical data cleaning\ndata.dropna(axis=1, how='all', thresh = data.shape[0]*0.3, inplace=True)\ndata.drop(['Cst_Cnt','Btl_Cnt','Sta_ID','Depth_ID'],axis = 1, inplace = True)\ndata = data.loc[:,data.nunique()>100]\ndata = data[data['Salnty'].notna()]\ndata","fda29197":"data.corrwith(data['Salnty'])","13b6c271":"x = data.drop(['Depthm','Salnty','NO2uM','R_Depth','R_SALINITY','R_TEMP','R_POTEMP',\n              'R_SIGMA','R_O2','R_O2Sat','R_PO4','R_NO3','R_NO2','R_SIO3'], axis = 1)","b5a54bce":"x.info()","d9fe2a4c":"x.fillna(value = x.mean(), inplace=True)","c3082a6a":"x.info()\n#Looks good!","548b3130":"X = x.to_numpy()\ny = data['Salnty'].to_numpy().reshape((-1,1))\nX = (X - X.mean(axis=0)) \/ X.std(axis=0)\nX = np.concatenate((np.ones((X.shape[0],1)), X),axis=1)","428f2227":"def initialise(input_size):\n    return np.random.randn(input_size,1)","c180de6a":"def predict(x, params):\n    return np.dot(x, params).reshape((-1,1))","278d9dac":"def loss(y_pred, y_actual):\n    return (1\/(2*y_actual.shape[0])) * (np.sum((y_pred-y_actual)**2))","a23386c5":"def grad(y_pred, y_actual, x):\n    return (1\/x.shape[0]) * np.dot(x.T,(y_pred - y_actual))","d3a3ed71":"def optimise(w, grad, alpha):\n    w = w - alpha * grad\n    return w","9173135b":"# Finally here is our training algorithm\ndef train(X, y_actual, alpha, batch_size, epoch):\n    w = initialise(X.shape[1])\n    log = []\n    for i in range(epoch):\n        index = np.random.randint(0,817509,size=batch_size)\n        x_train = X[index, :]\n        y_train = y_actual[index, :]\n        y_pred = predict(x_train, w)\n        grad_w = grad(y_pred, y_train, x_train)\n        w = optimise(w, grad_w, alpha)\n        log.append(loss(y_pred, y_train))\n    return log","0cea9df8":"alpha = 0.1\nbatch_size = 512\nepoch = 1000\nlog = train(X, y, alpha, batch_size, epoch)","bbd2ddb8":"plt.plot(np.arange(0,epoch), log, 'r-')\nplt.title(\"Loss over epochs\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\n\nplt.show()","e5099842":"We can then do a neat little trick here, $c$, being our bias term, can be seen as $w_{0}x_{0}$, where $x_{0} = 1$, and it will similarly give us a constant!\n\nWe thus can simplify our equation to:\n\n$$\ny = \\sum_{i=0}^{n}w_{i}x_{i}\n$$\nor \n\n$$\ny = WX^{T}\n$$\n\nDo note that vector $X$ represents a single example, so it is a $n*1$ vector.\n\n<img src=\"https:\/\/www.androidcentral.com\/sites\/androidcentral.com\/files\/styles\/xlarge\/public\/article_images\/2020\/10\/genshin-impact-bennett_2.png\" width=\"125\"\/>\n<center>Nice!<\/center>","9ea6fdb6":"*The univariate linear regression equation:*\n$$\ny = bx + c\n$$\n\n*The multivariate linear regression equation:*\n$$\ny = WX^{T} + c\n$$\n\nIn the first equation, we see clearly that it is a secondary school level linear equation.\nThe second equation is slightly more complex as it is presented in a vectorised form, the key is simply to understand that *X* is a **vector** and\n\n$$\nX\\in R^{n} \\to W\\in R^{n}\n$$\n\n\nWe have to train the models by supplying data beforehand to the machine, such that it produces a set of co-efficients *W* and *C*, whereby when we feed a new example of data, *X*, into the machine, our prediction, *Y*, will be relatively accurate.\n\n$$\ny_{new} = w_{1}x_{1} + w_{2}x_{2} ... w_{n}x_{n} + c\n$$\n\nand <center>$y_{new}$ is sufficienly close to $y_{actual}$<\/center>\n","e6088c0b":"I have to comment that after I found the correlation with salinity, I realised that most of the R_{name} are actually repeats of the variables above! That is why they have incredibly similar correlations, I therefore had to remove these labels manually and finally we have our set of input data X.","f049a986":"### Understanding the theory","1d62a411":"And we did it! The graph above shows the change in loss over time! Note that if we use our W now to predict, we will actually still be a bit off, but this is rather to be expected because the values of salinity are all very clustered together, and it is hard for us to get the decimals right unless we run a huge number of training loops and also significantly increase the precision in our parameters and data, this notebook is done mainly to illuminate the ideas behind linear regression, and not so much to create an accurate model itself!","0d154255":"Linear regression is a basic technique frequently utilised in predictive analysis,attempting to predict the output(Y) given a set of features\/inputs(x). **It assumes a linear relationship between the inputs and output.** This is essentially choosing a suitable mathematical function to model the relationship between the inputs and the output.\n![Multivariate linear regression](https:\/\/miro.medium.com\/max\/1120\/0*AqzOn7p--nveVULA.png \"title\")*This image shows an optimal plane found in multivariate linear regression, here we try to find a plane instead of a line since our input is not a single variable*","1daaba21":"### Putting it into practice","5e49381a":"#### finding derivative\n\nWe now just need to find the **partial derivatives** of $L$ with respect to the weights, $W$, taking the equation from the top, we can easily see that:\n\n$$\n\\frac{\\delta L}{\\delta W_{j}} = \\frac{1}{m}\\sum_{i=1}^{m}(X^{(i)}W^{T}-Y^{(i)}_{actual})x^{(i)}_{j}\n$$\n\nThe vectorised implementation\n\n$$\n{\\nabla_{W} L} = \\frac{1}{m}(X^{T}(XW^{T}-Y_{actual}))\n$$\n\nWe will then perform the gradient update, $\\alpha$ here represents the learning rate, you can think of it as a control of how fast we roll down the gradient hill:\n\n$$\nW_{i} := W_{i} - {\\alpha}(\\frac{\\delta L}{\\delta W_{j}})\n$$\n\nEnough said! Let's get to the code.","f6792bd8":"#### Data pre-processing","4462fbf4":"#### Gradient descent\n\nNow would be a great time to talk about gradient descent. Let's see an example of a typical loss function, the mean squared error, it is given by:\n\n$$\nL(Y_{pred},Y_{actual}) = \\frac{1}{2m}\\sum_{i=1}^{m}{(y^{(i)}_{pred}-y^{(i)}_{actual})^2}\n$$\n\nIf you recall, our prediction is based on the equation $ Y=WX^{T} $, so if we sub this into the equation at the top, and convert it to a vectorised implementation:\n\n$$\nL(X,W,Y_{actual}) = \\frac{1}{2m}\\sum((XW^{T})-Y_{actual})^2\n$$\n\n$X$ now represents a matrix, and each row of $X$ represents a vector $X^{(i)}$ which is the $i^{th}$ example in our data\n\nNow to get our prediction as close to the actual as possible, all we want is actually to minimise $L$! And our past experience in secondary school tells us that a minimum is achieved when we hit $\\frac{dy}{dx} = 0$, so what we need to do is just to differentiate $L$ with respect to $W$, since $W$ is the only variable we can actually control, and then move in the direction of the gradient!\n<img src='https:\/\/miro.medium.com\/max\/2648\/1*y8nYa2Ij4ic_lPdD1c_dtw.png' width='700'\/>","944bae84":"Another point to note is that our hyperplane is not going to look like a valley, instead imagine a real life mountain range, it will have a lot of peaks and valleys, so sometimes when you run an algorithm, you can get trapped in a local minima, or you can suddenly see a jump in your loss! Don't worry about it, this do happen from time to time and there are things that can be done to reduce the chance(dynamic learning rate etc.), but I hope this exercise has gave you a clearer idea of linear regression. Cheers!","11141f75":"### Linear regression from scratch\n\nIt is time for the real thing! We will now move our data to numpy, in order to really implement everything ourselves and I promise you we will only see numpy and matplotlib from now on.\n\nLet us break down the individual steps in linear regression:\n\n1. Initialise a model\n2. Make predictions of the training data with our model\n3. Find out the loss between our prediction of the salinity and the actual salinity\n4. Carry out gradient descent\n5. Update the weights","b2eea24c":"Now we see a much smaller set of variables and these are the ones that we will use for our model :)\nBut first, we will need to do away with the NULL values.\nHere, we have a few choices:\n1. Remove the missing values - in our case this would work just fine since we have so much data!\n2. Replace with mean, median, mode wtv\n3. Predict them using the linear model we have built, and then use it for training.\n\nIn this case, I will just take the easy way out and go with no.2, I am not a big fan of losing data despite us having a huuuge dataset to start with :D","bb5d669b":"We will take on the task of **predicting the salinity of water** using data collected from CalCOFI. The description of the dataset is as follows:\n\n\"\nThe CalCOFI data set represents the longest (1949-present) and most complete (more than 50,000 sampling stations) time series of oceanographic and larval fish data in the world. It includes abundance data on the larvae of over 250 species of fish; larval length frequency data and egg abundance data on key commercial species; and oceanographic and plankton data. The physical, chemical, and biological data collected at regular time and space intervals quickly became valuable for documenting climatic cycles in the California Current and a range of biological responses to them. CalCOFI research drew world attention to the biological response to the dramatic Pacific-warming event in 1957-58 and introduced the term \u201cEl Ni\u00f1o\u201d into the scientific literature.\n\"\n\nMore information about this dataset can be found [here](https:\/\/new.data.calcofi.org\/index.php\/database\/calcofi-database\/bottle-field-descriptions).\n\nIn this task, I will first read the data into a Pandas Dataframe, remove the redundant variables, and then use the remaining to perform a multivariate regression analysis. There will be little to no elaborations from this point on and any comments crucial for understanding will be within the code."}}