{"cell_type":{"b7625add":"code","51a40c5b":"code","8aef074a":"code","5669b935":"code","8ef5efce":"code","66788e97":"code","a0b83e8a":"code","42a3afa9":"code","ebe5db9e":"code","8cbedb76":"code","7a5e4350":"code","ea03e0e6":"code","3e583f5b":"code","639243a6":"code","35379319":"code","c0e9be75":"code","b93d0348":"code","6a1d0b4e":"code","dc1b364e":"code","0ea3f23f":"code","0575d3ab":"code","533361d8":"markdown","1368036e":"markdown","ca9b0925":"markdown","f329626d":"markdown"},"source":{"b7625add":"%reset -sf","51a40c5b":"!pip install sacrebleu jieba hanziconv","8aef074a":"import re, os\nimport collections\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_colwidth', None)","5669b935":"import jieba\nseg_list = jieba.cut(\"iFairies \u958b\u53e3\u53ef\u8abf\u7bc0\u6212\u6307\u2605ifairies\u301056472\u3011\u301056472\u3011\", cut_all=False)\nprint(\"Default Mode: \" + \"\/ \".join(seg_list))  # \u7cbe\u786e\u6a21\u5f0f","8ef5efce":"from hanziconv import HanziConv\nHanziConv.toSimplified(\"iFairies \u958b\u53e3\u53ef\u8abf\u7bc0\u6212\u6307\u2605ifairies\u301056472\u3011\u301056472\u3011\")","66788e97":"!ls '\/kaggle\/input\/'","a0b83e8a":"FOLDER = '\/kaggle\/input\/student-shopee-code-league-product-translation\/'\nLOC_TRAIN_EN = FOLDER + 'train_en.csv'\nLOC_TRAIN_TCN = FOLDER + 'train_tcn.csv'\nLOC_DEV_EN = FOLDER + 'dev_en.csv'\nLOC_DEV_TCN = FOLDER + 'dev_tcn.csv'\nLOC_TEST_TCN = FOLDER + 'test_tcn.csv'\n\ndf_train_en = pd.read_csv(LOC_TRAIN_EN)\ndf_train_tcn = pd.read_csv(LOC_TRAIN_TCN)[:100]\ndf_dev_tcn = pd.read_csv(LOC_DEV_TCN)\ndf_dev_tcn[\"en_ref\"] = pd.read_csv(LOC_DEV_EN)[\"translation_output\"]\ndf_test_tcn = pd.read_csv(LOC_TEST_TCN)","42a3afa9":"%%time\ndf_dev_tcn[\"text\"] = df_dev_tcn[\"text\"].astype(str).apply(HanziConv.toSimplified)\ndf_dev_tcn[\"segments\"] = df_dev_tcn[\"text\"].apply(lambda x: list(jieba.cut(x, cut_all=False)))\n\ndf_test_tcn[\"text\"] = df_test_tcn[\"text\"].astype(str).apply(HanziConv.toSimplified)\ndf_test_tcn[\"segments\"] = df_test_tcn[\"text\"].apply(lambda x: list(jieba.cut(x, cut_all=False)))\n\ndf_train_tcn[\"text\"] = df_train_tcn[\"product_title\"].astype(str).apply(HanziConv.toSimplified)\ndf_train_tcn[\"segments\"] = df_train_tcn[\"text\"].apply(lambda x: list(jieba.cut(x, cut_all=False)))","ebe5db9e":"%%time\ndev_phrases = set()\ntest_phrases = set()\ntrain_phrases = set()\n\nfor phrases in list(df_dev_tcn[\"segments\"]): dev_phrases.update(set(phrases))\nfor phrases in list(df_test_tcn[\"segments\"]): test_phrases.update(set(phrases))\nfor phrases in list(df_train_tcn[\"segments\"]): train_phrases.update(set(phrases))","8cbedb76":"def clean_set(phrase_set):\n    phrase_set = set(phrase for phrase in phrase_set if phrase and re.findall(r\"[\\u4e00-\\u9fff]+\", phrase) != [])\n    return phrase_set\ndev_phrases = clean_set(dev_phrases)\ntest_phrases = clean_set(test_phrases)\ntrain_phrases = clean_set(train_phrases)\npublic_phrases = train_phrases | dev_phrases","7a5e4350":"phrase_set = [dev_phrases, train_phrases, public_phrases, test_phrases]\nmatrix = [[0 for _ in phrase_set] for _ in phrase_set]\nfor i,set_x in enumerate(phrase_set):\n    for j,set_y in enumerate(phrase_set):\n        matrix[i][j] = len(set_x - set_y)\nfor i,set_x in enumerate(phrase_set):\n    matrix[i][i] = len(set_x)\n    \n#   x\n# y\n# for non-diagonal entries - shows how many phrases in y that are not covered by x\n# there are 1971\/11084 Chinese phrases in the test set that are not covered by the train and dev set\nnp.array(matrix)","ea03e0e6":"# example of some phrases in the test set not covered in the public sets\nlist(test_phrases - public_phrases)[:10]","3e583f5b":"df_dev_phrases = pd.DataFrame(list(dev_phrases), columns=[\"phrases\"])\ndf_test_phrases = pd.DataFrame(list(dev_phrases), columns=[\"phrases\"])\ndf_train_phrases = pd.DataFrame(list(dev_phrases), columns=[\"phrases\"])\ndf_public_phrases = pd.DataFrame(list(dev_phrases), columns=[\"phrases\"])","639243a6":"df_dev_phrases.to_csv(\"dev_phrases.csv\", index=False)\ndf_test_phrases.to_csv(\"test_phrases.csv\", index=False)\ndf_train_phrases.to_csv(\"train_phrases.csv\", index=False)\ndf_public_phrases.to_csv(\"public_phrases.csv\", index=False)","35379319":"df_dev_phrase_map = pd.read_csv('\/kaggle\/input\/week4\/public_phrases.csv')\ntcn_to_en = dict(df_dev_phrase_map.values)","c0e9be75":"df_dev_tcn[\"pred\"] = df_dev_tcn[\"segments\"].apply(lambda x: \" \".join(tcn_to_en[phr] if phr in tcn_to_en else phr for phr in x))\ndf_test_tcn[\"pred\"] = df_test_tcn[\"segments\"].apply(lambda x: \" \".join(tcn_to_en[phr] if phr in tcn_to_en else phr for phr in x))","b93d0348":"# remove remaning chinese characters\ndf_dev_tcn[\"pred\"] = df_dev_tcn[\"pred\"].str.replace(r\"[\\u4e00-\\u9fff]+\", \" \")\ndf_test_tcn[\"pred\"] = df_test_tcn[\"pred\"].str.replace(r\"[\\u4e00-\\u9fff]+\", \" \")\ndf_dev_tcn[\"pred\"] = df_dev_tcn[\"pred\"].str.replace(r\"\\s+\", \" \")\ndf_test_tcn[\"pred\"] = df_test_tcn[\"pred\"].str.replace(r\"\\s+\", \" \")","6a1d0b4e":"import re\nfrom typing import List\n\nimport regex\nfrom sacrebleu import corpus_bleu\n\nOTHERS_PATTERN: re.Pattern = regex.compile(r'\\p{So}')\n\n\ndef eval(preds: List[str], refs: List[str]) -> float:\n    \"\"\"BLEU score computation.\n\n    Strips all characters belonging to the unicode category \"So\".\n    Tokenize with standard WMT \"13a\" tokenizer.\n    Compute 4-BLEU.\n\n    Args:\n        preds (List[str]): List of translated texts.\n        refs (List[str]): List of target reference texts.\n    \"\"\"\n    preds = [OTHERS_PATTERN.sub(' ', text) for text in preds]\n    refs = [OTHERS_PATTERN.sub(' ', text) for text in refs]\n    return corpus_bleu(\n        preds, [refs],\n        lowercase=True,\n        tokenize='13a',\n        use_effective_order= False\n    ).score","dc1b364e":"# method score\nen_ref_statements = df_dev_tcn[\"en_ref\"]\nen_pred_statements = df_dev_tcn[\"pred\"]\n\nimport sacrebleu\neval(en_pred_statements, en_ref_statements)","0ea3f23f":"df_test_tcn[\"translation_output\"] = df_test_tcn[\"pred\"]\ndf_test_tcn[[\"translation_output\"]].to_csv(\"submission.csv\", index=False)","0575d3ab":"!head submission.csv","533361d8":"# Test modules","1368036e":"# Cut parts","ca9b0925":"# Load files","f329626d":"# Teehee"}}