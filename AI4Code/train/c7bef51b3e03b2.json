{"cell_type":{"90dd8939":"code","ffb6e1f5":"code","b43a4fb8":"code","4d21bae3":"code","b975e079":"code","24a36736":"code","6785b719":"code","6805dd45":"code","1d5277c4":"code","00eacfc2":"code","bb43ec23":"code","18f1602b":"code","ae161915":"code","482079e0":"code","cd2b2d9f":"code","b4f173a5":"code","849e8702":"code","9d282d6b":"code","21fd0459":"code","f0b9bcee":"code","94b36d2b":"code","4bc6948d":"code","5dba0dd8":"code","ab2217ad":"code","45c762da":"code","57c8aba0":"code","b0d115c4":"code","7ac09eac":"code","fa1fbd73":"code","04383785":"code","4c95e700":"code","98a970e9":"code","75be3df2":"code","d0dcf879":"code","cfcd0e7f":"code","54798bca":"code","bff07c12":"code","02398dc3":"code","c59746eb":"code","f5a0bc26":"markdown","0fd2a489":"markdown","c2dbec72":"markdown","694bec74":"markdown","b2822e8d":"markdown","b069b7af":"markdown","eae8f7b7":"markdown","db97a3d3":"markdown","5f5e96f7":"markdown","7ee640f6":"markdown","f3d75f91":"markdown","9713130a":"markdown","62288f8e":"markdown","2c0ade38":"markdown","d58e5f3c":"markdown","b1c82c23":"markdown","84d258e3":"markdown","cdcd7b48":"markdown","ecd74b76":"markdown","1e5b6d0f":"markdown","02bea00d":"markdown","dcfe558c":"markdown","21a8c72c":"markdown","de55be77":"markdown","07b871a4":"markdown","8b2f4b16":"markdown","cf418cce":"markdown","c7a551df":"markdown","4a005a73":"markdown","4937cb76":"markdown","388c26f6":"markdown","3f26135e":"markdown","db3b4f14":"markdown","ca058f9b":"markdown","bc3844d0":"markdown","9e5a807f":"markdown","535bc32d":"markdown","3e73e2ff":"markdown","f7746c7e":"markdown","cdb9b5ac":"markdown"},"source":{"90dd8939":"#The Basics\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n%matplotlib inline\nimport os, string\n\n#Text Processing\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport re\n\n#Optimisation\nimport pickle\nimport optuna\noptuna.logging.set_verbosity(optuna.logging.WARNING) #Disable Warnings\n\n#ML Model\nimport lightgbm as lgbm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn import decomposition\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n#Reproducible results\nfrom numpy.random import seed\nseed(0)\nimport tensorflow\ntensorflow.random.set_seed(0)\n\n#ANN\nimport keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import RootMeanSquaredError","ffb6e1f5":"def basic_EDA(df):\n    size = df.shape\n    sum_duplicates = df.duplicated().sum()\n    sum_null = df.isnull().sum().sum()\n    is_NaN = df. isnull()\n    row_has_NaN = is_NaN. any(axis=1)\n    rows_with_NaN = df[row_has_NaN]\n    count_NaN_rows = rows_with_NaN.shape\n    return print(\"Number of Samples: %d,\\nNumber of Features: %d,\\nDuplicated Entries: %d,\\nNull Entries: %d,\\nNumber of Rows with Null Entries: %d %.1f%%\" %(size[0],size[1], sum_duplicates, sum_null,count_NaN_rows[0],(count_NaN_rows[0] \/ df.shape[0])*100))\n\ndef summary_table(df):\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    return summary\n\ndef count_words(df,words_list):\n    count_list = []\n    for text in df:\n        count=0\n        text = preprocess_sentence(text)\n        for word in text.split():\n            if word in words_list: count+=1\n        count_list.append(count)\n    return count_list\n\n#Compute Percentiles\ndef percentile_drop(df,column):\n    q25, q50, q75 = np.percentile(df[column], 25), np.percentile(df[column], 50), np.percentile(df[column], 75)\n    print ('Lower 25th Quartile: %.2f \\nMedian: %.2f \\nUpper 75th Quartile: %.2f' % (q25,q50,q75))\n    q25_index = df[(df[column] <=q25)].index\n    q25_q50_index = df[(df[column] > q25)|(df[column] <= q50)].index\n    q50_q75_index = df[(df[column] > q50)|(df[column] <= q75)].index\n    q75_index = df[(df[column] >=q75)].index\n    return q25, q50, q75\n\ndef process_list(text): #returns a list of preprocessed words\n        word_list = []\n        #for t in text:            \n        text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text) #remove punctuations\n        text = text.lower() #lower case\n        tokenized_word=word_tokenize(text) #separate into words\n        for word in tokenized_word:\n            if word not in stop_words: #filter stop-words\n                word = stem.stem(word) #stemming\n                word_list.append(word) #append to general list\n        return word_list\n    \ndef build_freqs(texts, category): #computes the frequency of words \n    categorylist = np.squeeze(category).tolist()\n    freqs = {}\n    words_sample = []\n    for text, cat in zip(texts,categorylist):\n        for word in process_list(text):\n            words_sample.append(word)\n            pair = (word, cat)\n            freqs[pair] = freqs.get(pair, 0) + 1  \n    return freqs,words_sample\n\n\ndef preprocess_sentence(df): #returns the whole sentence, with preprocessed text\n    word_list = []\n    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', df) #remove punctuations\n    text = text.lower() #lower case\n    tokenized_word=word_tokenize(text) #separate into words\n    for word in tokenized_word:\n        if word not in stop_words: #filter stop-words\n            word = stem.stem(word) #stemming\n            word_list.append(word) #append to general list\n    return ' '.join(word_list) #rejoins the sentence without the stopwords\n\ndef Count_Repeated(df):\n    output = []\n    for text in df:\n        count_repeated = 0\n        freqs = {}\n        words_sample = []\n        for word in process_list(text):\n            words_sample.append(word)\n            freqs[word] = freqs.get(word, 0) + 1  \n        words_repeated = [word for word, occurrences in freqs.items() if occurrences >= 3] #count the number of words that occur more than 3 times\n        #print(words_repeated)\n        count_repeated = len(words_repeated)\n        output.append(count_repeated)\n        #print(count_repeated)\n    return output\n\nstop_words=set(stopwords.words(\"english\"))\nstem = PorterStemmer()","b43a4fb8":"df_train = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ndf_test = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n\nprint(\"**Training set**\")\nbasic_EDA(df_train)\nprint(\"**Test set**\")\nbasic_EDA(df_test)","4d21bae3":"summary_table(df_train)","b975e079":"df_train.head(5)","24a36736":"#Importing Formal Words - Already Stemmed Words\nformalWords = pd.read_csv('..\/input\/formalwords\/formal_list.csv', delimiter = ',', header=0, names = ['words'])\nformalWords = formalWords['words'].tolist()\n#Importing Common Words\ncommon_words = pd.read_csv('..\/input\/commonwords3000\/CommonWords.csv', delimiter = ',', header=0, names = ['words'])\ncommon_words = common_words['words'].tolist()\n#Stemming \ncommon_words_stem = [stem.stem(word) for word in common_words]","6785b719":"punctuations = string.punctuation\n#Adding new features\ndf_train['word_count'] = df_train['excerpt'].apply(lambda x : len(x.split()))\ndf_train['char_count'] = df_train['excerpt'].apply(lambda x : len(x.replace(\" \",\"\")))\ndf_train['word_density'] = df_train['word_count'] \/ (df_train['char_count'] + 1)\n\n#Adding +1 to allow ratio calculation\ndf_train['uppercase'] = df_train['excerpt'].str.findall(r'[A-Z]').str.len()+1\ndf_train['lowercase'] = df_train['excerpt'].str.findall(r'[a-z]').str.len()+1\ndf_train['upplowRatio'] = df_train['uppercase'] \/ (df_train['lowercase'] + 1)\ndf_train['punc_count'] = df_train['excerpt'].apply(lambda x : len([a for a in x if a in punctuations]))\ndf_train['paragraphs'] = df_train['excerpt'].apply(lambda x : (x.count(\"\\n\")))\ndf_train['sentLength'] = df_train['word_count'] \/ (df_train['punc_count'] + 1)\ndf_train['commonWords'] = count_words(df_train['excerpt'], common_words_stem)\ndf_train['formalWords'] = count_words(df_train['excerpt'], formalWords)\ndf_train['repeatedWords'] = Count_Repeated(df_train['excerpt'])","6805dd45":"df_train.drop(['url_legal','license'],axis = 1,inplace = True)\ndf_test.drop(['url_legal','license'],axis = 1,inplace = True)","1d5277c4":"df_train.head(3)","00eacfc2":"feat_list = list(df_train.columns[2:])\n\nfor i in feat_list:\n    plt.figure(figsize=(20,5))\n    ax = sns.kdeplot(data = df_train, x = i, linewidth=1,alpha=.3, fill = True,palette = 'husl') \n    ax.set_xlabel(i)\n    plt.title('KDE Plot - ' + i, fontsize = 16,weight = 'bold',pad=20);  \n    sns.despine(top=True, right=True, left=False, bottom=False)","bb43ec23":"pd.set_option('display.max_colwidth', None)\nprint('Max Target Value:') \nprint(df_train.loc[df_train['target'].idxmax(),['excerpt','target']])","18f1602b":"print('Min Target Value:') \nprint(df_train.loc[df_train['target'].idxmin(),['excerpt','target']])","ae161915":"plt.figure(figsize=(20,8));\ng = sns.pairplot(df_train,vars=feat_list,corner=True,\n                 plot_kws=dict(s = 8),\n                 diag_kws=dict(linewidth=0,alpha=.5));\ng.fig.suptitle('Relationship between Features',fontsize=16, weight = 'bold');","482079e0":"q25, q50, q75 = percentile_drop(df_train,'target')\n\ndf_train = df_train.assign(targetQuartile = df_train['target'])\ndf_train.loc[(df_train['target']) <= q25, 'targetQuartile'] = '<=Q25'\ndf_train.loc[(df_train['target'] > q25)&(df_train['target'] <= q50), 'targetQuartile'] = 'Q25-Q50'\ndf_train.loc[(df_train['target'] > q50)&(df_train['target'] <= q75), 'targetQuartile'] = 'Q50-Q75'\ndf_train.loc[df_train['target'] >= q75, 'targetQuartile'] = '>=Q75'","cd2b2d9f":"#builds dictionary for each quartile '(word,quartile): word frequency', e.g. ('young', 'Q50-Q75'): 93\nfreqs_quartile, words = build_freqs(df_train['excerpt'], df_train['targetQuartile'])\n\nfreq_words = []\n\nfor word in words:\n    Q25=0;Q25Q50=0;Q50Q75=0;Q75 = 0\n    if (word, '<=Q25') in freqs_quartile:\n        Q25 = freqs_quartile[(word, '<=Q25')]\n    if (word, 'Q25-Q50') in freqs_quartile:\n        Q25Q50 = freqs_quartile[(word, 'Q25-Q50')]\n    if (word, 'Q50-Q75') in freqs_quartile:\n        Q50Q75 = freqs_quartile[(word, 'Q50-Q75')]                            \n    if (word, '>=Q75') in freqs_quartile:\n        Q75 = freqs_quartile[(word, '>=Q75')]      \n    freq_words.append([word, Q25,Q25Q50,Q50Q75,Q75])   \n\nfreq_wordsDF = pd.DataFrame(freq_words, columns = ['word', 'Q25','Q25-Q50','Q50-Q75','Q75'])    \nfreq_wordsDF['sum'] =  freq_wordsDF.loc[:, ['Q25','Q25-Q50','Q50-Q75','Q75']].sum(axis=1)\nfreq_wordsDF.sort_values('sum', ascending=False,inplace=True)\nfreq_wordsDF.drop_duplicates(inplace=True)\nfreq_wordsDF.head(3)","b4f173a5":"z = 0; j = 0\nfig, axarr = plt.subplots(1,4, figsize=(20,4))\n\nquartiles_abbr = ['Q25','Q25-Q50','Q50-Q75','Q75']\n\nfor i in quartiles_abbr:\n    df = freq_wordsDF.loc[:,['word',i]]\n    df.sort_values(i, ascending=False,inplace=True)\n    ax = sns.lineplot(data=df[0:20],x=\"word\", y=i, marker='o',ax=axarr[z])\n    axarr[z].tick_params(axis='x', rotation=70)    \n    axarr[z].set_xlabel('The 20 Most Common Words for ' + i,fontsize = 12,weight = 'bold')\n    axarr[z].set_ylabel('Count',fontsize = 13,weight = 'bold')\n    axarr[z].set_title(i, fontsize = 14,weight = 'bold');\n    sns.despine(top=True, right=True, left=False, bottom=False)\n    z+=1\n    \nfig.tight_layout(pad=3.0)\nplt.suptitle('Word Frequency per Target Quartile - Top 20 Words',fontsize=16, weight = 'bold');\nplt.show()","849e8702":"cm = sns.diverging_palette(220, 20, sep=5, as_cmap=True)\ndf_quartile = df_train.iloc[:,2:]\n\nQcount_mean = df_quartile.groupby(['targetQuartile']).mean().round(3)\nQcount_min = df_quartile.groupby(['targetQuartile']).min().round(3)\nQcount_max = df_quartile.groupby(['targetQuartile']).max().round(3)\n\nQcount_mean= Qcount_mean.reindex(['<=Q25', 'Q25-Q50', 'Q50-Q75','>=Q75'])\nQcount_min= Qcount_min.reindex(['<=Q25', 'Q25-Q50', 'Q50-Q75','>=Q75'])\nQcount_max = Qcount_max.reindex(['<=Q25', 'Q25-Q50', 'Q50-Q75','>=Q75'])\n\nMeanV = Qcount_mean.round(3).style.background_gradient(cmap=cm)\nMinV = Qcount_min.round(3).style.background_gradient(cmap=cm)\nMaxV = Qcount_max.round(3).style.background_gradient(cmap=cm)","9d282d6b":"MeanV","21fd0459":"MinV","f0b9bcee":"MaxV","94b36d2b":"text_data = df_train['excerpt'].apply(lambda x : preprocess_sentence(x)) #preprocess text data\n\ntfidf_vec = TfidfVectorizer(dtype=np.float32, sublinear_tf=True, use_idf=True, smooth_idf=True) #define tfidf matrix parameters\ntext_data_tfidf = tfidf_vec.fit_transform(text_data)#transform our text data into a TF.IDF matrix\n\nsvd = decomposition.TruncatedSVD(n_components=2)#Define the SVD parameters\ntext_data_svd = svd.fit_transform(text_data_tfidf)#Reduce dimensionality of our TF.IDF matrix","4bc6948d":"plt.figure(figsize=(20,5))\nsns.set(style=\"ticks\", font_scale = 1)\nax = sns.scatterplot(data=text_data_svd, x=text_data_svd[:,0], y=text_data_svd[:,1], \n                     hue = np.ravel(df_quartile['targetQuartile']), hue_order = ['<=Q25', 'Q25-Q50', 'Q50-Q75','>=Q75']);\n\nleg = ax.axes.get_legend()#add legend title\nleg.set_title('Target Value Quartile')#add legend title\nsns.despine(top=True, right=True, left=False, bottom=False)\nplt.xticks(rotation=0,fontsize = 12)\nax.set_xlabel('x1',fontsize = 14,weight = 'bold')\nax.set_ylabel('x2',fontsize = 14,weight = 'bold')\nplt.title('Scatter Plot by Quartile Target Value after Dimensionality Reduction', fontsize = 16,weight = 'bold');","5dba0dd8":"fig = px.scatter_3d(x=text_data_svd[:,0], y=text_data_svd[:,1], z=np.ravel(df_train['target']),\n                    opacity=0.9)\nfig.update_traces(marker=dict(size=3))#, selector=dict(type='scatter3d'))\n\nfig.update_layout(scene = dict(xaxis_title='x1',yaxis_title='x2',zaxis_title='Target Value',\n                              xaxis = dict(backgroundcolor=\"rgb(200, 200, 230)\"),\n                              yaxis = dict(backgroundcolor=\"rgb(230, 200,230)\"),\n                              zaxis = dict(backgroundcolor=\"rgb(230, 230,200)\")))\n\nfig.update_traces(hovertemplate='x1: %{x} <br>x2: %{y} <br>Target Value: %{z}') #\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))","ab2217ad":"df_train_ML = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ndf_test_ML = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n\ndf_train_ML['text'] = df_train['excerpt'].apply(lambda x : preprocess_sentence(x))\ndf_test_ML['text'] = df_test['excerpt'].apply(lambda x : preprocess_sentence(x))\n\nfeatures_train = df_train_ML.text.values\ntarget_train = df_train_ML['target']\n\nfeatures_test = df_test_ML.text.values\n\n# Create First Train and Test sets\nx_train_text, x_test_text, y_train, y_test = train_test_split(features_train, target_train, test_size=0.20,random_state=123)\n\nprint (\"Training set size\", x_train_text.shape[0])\nprint (\"Test set size\",x_test_text.shape[0])\n\n#Define K-Fold Validation\nkf = KFold(n_splits=5, shuffle = True, random_state = 123)","45c762da":"features_train_params = df_train.iloc[:, 4:-1]\n\n# Create First Train and Test sets\nx_train_params, x_test_params, y_train, y_test = train_test_split(features_train_params, target_train, test_size=0.20,random_state=123)\n\nsc_X = MinMaxScaler()\n\nx_train_transform = sc_X.fit_transform(x_train_params)\nx_test_transform = sc_X.transform(x_test_params)","57c8aba0":"# Fitting TFIDF to training and test sets\ntfidf_vec = TfidfVectorizer(dtype=np.float32, sublinear_tf=True, use_idf=True, smooth_idf=True) #define tfidf matrix parameters\nx_train_tfidf = tfidf_vec.fit_transform(x_train_text)\nx_test_tfidf = tfidf_vec.transform(x_test_text)\n\nfinal_test_tfidf = tfidf_vec.transform(features_test)","b0d115c4":"def FitANN(X_train, X_val,Y_train, Y_val):\n    keras.backend.clear_session()\n    history = ANNmodel.fit(X_train,Y_train, validation_data=(X_val,Y_val),epochs=epochs,callbacks=[callbacks_list],verbose=0)\n    return ANNmodel\n\ndef FitLGBM(X_train, X_val,Y_train, Y_val):\n    LGBMModel.fit(X_train, Y_train, eval_metric='rmse', eval_set=[(X_val, Y_val)],verbose=0)\n    return LGBMModel\n\ndef KFoldSets(x_train,x_test,MLmodel):\n    RMSE = []\n    for train_index, test_index in kf.split(x_train):\n            X_train, X_val = x_train[train_index], x_train[test_index]\n            Y_train, Y_val = y_train.iloc[train_index], y_train.iloc[test_index]\n            if MLmodel == 'ANN':\n                model = FitANN(X_train, X_val,Y_train, Y_val)\n            if MLmodel =='LGBM':\n                model = FitLGBM(X_train, X_val,Y_train, Y_val)\n            predictions = model.predict(X_val)\n            RMSE.append(np.sqrt(mean_squared_error(Y_val, predictions)))\n    test_score = np.sqrt(mean_squared_error(y_test, model.predict(x_test)))\n    mean_res = np.mean(RMSE)\n    std_dev = np.std(RMSE)\n    print(MLmodel)\n    print(\"Training LogLoss: %.3f +\/- %.4f \\n Test Set LogLoss: %.3f\" % (mean_res,std_dev,test_score))\n    return model ","7ac09eac":"LGBMModel = lgbm.LGBMRegressor(objective='regression',\n                               metric = 'rmse',\n                               verbose=-1, \n                               learning_rate=0.019, \n                               max_depth=360, \n                               num_leaves=680, \n                               early_stopping_round = 50, \n                               n_estimators = 10000,\n                               reg_alpha = 0.021314,\n                               reg_lambda = 0.00856,\n                                force_col_wise=True)","fa1fbd73":"test = []\ntrain = []\ni_list = list(range(5,100,5))\nfor i in i_list:# Apply SVD and lgbm\n    svd = decomposition.TruncatedSVD(n_components=i)\n    xtrain_svd = svd.fit_transform(x_train_tfidf)\n    xtest_svd = svd.transform(x_test_tfidf)\n    LGBMModel.fit(xtrain_svd, y_train, eval_metric='rmse', eval_set=[(xtest_svd, y_test)],verbose=0)    \n    predictions_train = LGBMModel.predict(xtrain_svd)\n    predictions_test = LGBMModel.predict(xtest_svd)\n    train_rmse = np.sqrt(mean_squared_error(y_train, predictions_train))\n    test_rmse = np.sqrt(mean_squared_error(y_test, predictions_test))\n    \n    train.append(train_rmse)\n    test.append(test_rmse)","04383785":"plt.figure(figsize=(20,5))\nsns.despine(top=True, right=True, left=False, bottom=False)\n\nax = sns.lineplot(x=i_list, y=train,markers=True, dashes=False,color='red',marker=\"o\",label=\"Validation\")\nax = sns.lineplot(x=i_list, y=test,markers=True, dashes=False,color='green',marker=\"o\", label=\"Test\")\nax.axhline(min(test), ls='--', c = 'green')\nax.text(1,min(test),\"Min.RMSE Test Result\",\n            bbox=dict(facecolor='white', edgecolor='none'))\n\nax.set_ylabel('RMSE')\nax.set_xlabel('Number of SVD Components')\n   \nplt.title('Validation and Test Set RMSE by Number of Features',fontsize=16, weight = 'bold');\nplt.xticks(np.arange(min(i_list), max(i_list)+1, 5.0))  \nplt.show()","4c95e700":"# Apply SVD\nsvd = decomposition.TruncatedSVD(n_components=20)\nxtrain_svd = svd.fit_transform(x_train_tfidf)\nxtest_svd = svd.transform(x_test_tfidf)\n\nfinal_test_svd = svd.transform(final_test_tfidf)","98a970e9":"x_train_final = np.concatenate([xtrain_svd, x_train_transform], axis=1)\nx_test_final = np.concatenate([xtest_svd, x_test_transform], axis=1)","75be3df2":"ANNmodel = Sequential()\nANNmodel.add(Dense(20, input_dim=x_train_final.shape[1],activation = 'tanh'))\n#ANNmodel.add(Dense(50, activation='tanh'))\nANNmodel.add(Dense(1, activation='linear'))\noptimiser = Adam(lr=0.02, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\nANNmodel.compile(loss='mean_squared_error', optimizer=optimiser, metrics = [RootMeanSquaredError()])\n\nearly_stopping_monitor = EarlyStopping(patience=50,monitor='val_loss', mode = 'min',verbose=0)\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=20, verbose=0, factor=0.5, min_lr=0.00001)\ncallbacks_list = [learning_rate_reduction,early_stopping_monitor]\n       \n#Model Parameters\nepochs = 100\nbatch_size = 16","d0dcf879":"LGBM_Model = KFoldSets(x_train_final,x_test_final,'LGBM')\nANN_Model = KFoldSets(x_train_final,x_test_final,'ANN')","cfcd0e7f":"y_pred = ANN_Model.predict(x_test_final)","54798bca":"#plot settings\nfig, ax = plt.subplots(figsize=(10, 10))\n#ax.set_yscale('log')\n#ax.set_xscale('log')\nax.scatter(y_test, y_pred, color='red')\nax.plot(y_test, y_test, color='blue')  \nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\nplt.xlabel('Actual Target Values')\nplt.ylabel('Predicted Target Values')\nplt.title('Target - Predicted and Actual Results for ANN model');","bff07c12":"X_train_text = tfidf_vec.fit_transform(features_train)\nX_train_text = svd.fit_transform(X_train_text)\n\nX_train_Feats = sc_X.fit_transform(features_train_params)\nX_train = np.concatenate([X_train_text, X_train_Feats], axis=1)\n\nANNmodel.fit(X_train,target_train, validation_split=.2,epochs=epochs,callbacks=[callbacks_list],verbose=0)","02398dc3":"df_test['word_count'] = df_test['excerpt'].apply(lambda x : len(x.split()))\ndf_test['char_count'] = df_test['excerpt'].apply(lambda x : len(x.replace(\" \",\"\")))\ndf_test['word_density'] = df_test['word_count'] \/ (df_test['char_count'] + 1)\ndf_test['uppercase'] = df_test['excerpt'].str.findall(r'[A-Z]').str.len()+1\ndf_test['lowercase'] = df_test['excerpt'].str.findall(r'[a-z]').str.len()+1\ndf_test['upplowRatio'] = df_test['uppercase'] \/ (df_test['lowercase'] + 1)\ndf_test['punc_count'] = df_train['excerpt'].apply(lambda x : len([a for a in x if a in punctuations]))\ndf_test['paragraphs'] = df_test['excerpt'].apply(lambda x : (x.count(\"\\n\")))\ndf_test['sentLength'] = df_test['word_count'] \/ (df_test['punc_count'] + 1)\ndf_test['commonWords'] = count_words(df_test['excerpt'], common_words_stem)\ndf_test['formalWords'] = count_words(df_test['excerpt'], formalWords)\ndf_test['repeatedWords'] = Count_Repeated(df_test['excerpt'])\n\nX_test_text = tfidf_vec.transform(features_test)\nX_test_text = svd.transform(X_test_text)\n\nfeatures_test_params = df_test.iloc[:, 2:]\nX_test_Feats = sc_X.fit_transform(features_test_params)\nX_test = np.concatenate([X_test_text, X_test_Feats], axis=1)\n","c59746eb":"df_test['target'] = ANNmodel.predict(X_test)\ndf_test[['id','target']].to_csv('submission.csv', index=False)","f5a0bc26":"Importing the Training and Test set:","0fd2a489":"> Code to run K-Fold and measure model performance","c2dbec72":"# 1. Overview\n\nIn this challenge we are given classroom text data that are used as reading passages for grade 3-12. The goal is to use ML as a regression model to define the readability score of each text passage:\n\n> \"*In this competition, you\u2019ll build algorithms to rate the complexity of reading passages for grade 3-12 classroom use. To accomplish this, you'll pair your machine learning skills with a dataset that includes readers from a wide variety of age groups and a large collection of texts taken from various domains. Winning models will be sure to incorporate text cohesion and semantics.<br>\n>  If successful, you'll aid administrators, teachers, and students. Literacy curriculum developers and teachers who choose passages will be able to quickly and accurately evaluate works for their classrooms. Plus, these formulas will become more accessible for all. Perhaps most importantly, students will benefit from feedback on the complexity and readability of their work, making it far easier to improve essential reading skills.*\"\n\n## 1.1 Acknowledgements\n\nIn the Kaggle competition page, CommonLit extends a special thanks to Professor Scott Crossley's research team at the Georgia State University Departments of Applied Linguistics and Learning Sciences for their partnership on this project.In addition, Schmidt Futures is also highlighted for their advice and support for making this challenge possible.","694bec74":"# 4. Results and Conclusion ","b2822e8d":"* The **target** and most of the features present a Normal-like distribution. It is important to understand what the MAX and MIN values for the target mean\n* Since most features present a normal distribution, no log or power transformation is required\n* The **paragraphs** feature displays an unusual shape with several peaks as most samples have a unique value, i.e. one, two or three paragraphs.\n* It will be interesting to analyse if text samples with more paragraphs are have a higher readability score","b069b7af":"To obtain the optimal number of SVD parameters, a simple code was created to iterate over the number of components. The optimal value is the one that achieves the best RMSE on the test set, while displaying a similar result in the K-Fold validation set RMSE. Similar RMSE on test and validation sets gives assurance that the model is not prone to overfit and the hyperparameters are properly defined.","eae8f7b7":">Preparing the Test Set as we did with the Training Set","db97a3d3":"Here the new features we created at the beginning of the Notebook are being extracted and divided into Training and Test sets:","5f5e96f7":" >code to define the number of SVD components","7ee640f6":"Building the ANN model:","f3d75f91":"# 2. The Data\n\nIn this section we understand the dataset, analyse any data cleansing requirements and perform Data Analysis. The description of the features are shown below, extracted from the Challenge Organisers:\n\n* **id** - unique ID for excerpt\n* **url_legal** - URL of source - this is blank in the test set.\n* **license** - license of source material - this is blank in the test set.\n* **excerpt** - text to predict reading ease of\n* **target** - reading ease\n* **standard_error** - measure of spread of scores among multiple raters for each excerpt. Not included for test data.","9713130a":"Looking at the current Leaderboard we are far from the competition Top Results. For now, the ANN model is our best attempt. A Scatter Plot where the Predicted and Actual Target values are plotted can help us understand where our current model is struggling:","62288f8e":"# 2.3 Target Feature and Feature Relantionships\n\nIn this section we further analyse the Feature Relantionships. First let's try to understand what the **Target** value ranges might mean.\n\nBelow is a sample of the text with **MAX** target value:","2c0ade38":">LGBM definition","d58e5f3c":"Now everything is ready for us to analyse the most common words according to the Quartile Ranges:","b1c82c23":"The graph below shows the relationship between the RMSE for Validation and Test sets according to the number of SVD components. ","84d258e3":"Now, let's analyse the Mean, Minimum and Max values for each feature by quartiles:","cdcd7b48":">Data Preprocessing, Training and Test sets definition","ecd74b76":"As good news, there are no Null Entries in the excerpt feature. As such, no strategie to replace or eliminate the rows is required. In short, the only columns that are relevant for us are **excerpt** and **target**. Below is a sample of the training set. \n\nNext, we separate the training set into Features and Target to continue our analysis.","1e5b6d0f":"The columns not used for the Data Analysis (**url_legal and license**) were removed. Below is the result of the DataFrame with ALL the new features created:","02bea00d":"* Looking at the max table, the char_count and uppercase shows a good difference between the lower and upper quartiles. Uppercase and punctuation count also shows a great difference between the extreme quartiles.\n* The Minimum values table is quite homogeneous among the quartiles, where char_count and word_density display a higher difference.\n\nAs a final analysis, we apply dimensionality reduction using truncated single value decomposition (SVD). In the context of text analysis, when applied to TF.IDF or Word Count matrices it is also referred as Latent Semantic Analysis. \n\nTo apply SVD:\n* we first preprocess the text (lower case, remove stop words, remove symbols)\n* Define the TF.IDF matrix and transform our text input as a TF.IDF matrix\n* Apply the sickit learn decomposition TruncatedSVD, transforming our TF.IDF matrix into 2 components, allowing data visualisation","dcfe558c":"* **Target and Common Words:** there is no clear pattern or relationship between the two features. The ratio of common words does not seem to indicate specific target value ranges\n* **Target and Paragraphs:** as the number of paragraphs increases, the samples have larger target values. It makes sense, paragraphs makes the reading easier on the eyes\n* **Target and Word Case:** as the target values increase, the number of lowercase words decrease. The same pattern is not seen with the Uppercase feature\n* **Target and Word Density:** the Word Density (num words\/num chars+1) seems to increase as the target increases. Some of the lower density samples are displaying the lower target values, it could be that higher complexity samples use longer words (more characters)\n* **Target and Character Count:** presents a similar relationship presented with the Lowercase feature. Higher target values present smaller character counts\n","21a8c72c":">Defining the final SVD parameter","de55be77":"* The **char_count**, **lowercase** and **punc_count** are showing interesting ranges by quartile\n* For the lower 25th quartile we have a higher number of charactersv(**char_count**) even though the **word_count** does not differ that much between quartiles (from 176 to 170)\n* On average, the upper quartile (less complex texts) have higher number of uppercase letters, probably due to smaller sentences. This trend probably influences also the higher number of paragraphs the upper quartiles are presenting\n* Note how the lower quartile and upper quartile samples (lower than 25th and higher than 75th) are presenting a higher (mean) for the standard_error. It could indicate that such samples were more difficult to define a common rate, as the measure of scores among multiple raters have a higher spread","07b871a4":"# 2.2 KDE Plots\n\nIt is usually helpful to start the Data Analysis with KDE distribution plot to understand the Features value range and pattern of values distribution. Next, the KDE of all Features is displayed:","8b2f4b16":"# 2.1 Creating Additional Features\n\nSimilar to what I have done when performing Text Analysis in other Notebooks, the creation of additional features can help us understand what drives the \"Target\" Readability Score.\n\n* **Word Count** - Total number of words in the text\n* **Character Count** - Total number of characters in the text excluding spaces\n* **Word Density** - Average length of the words used in the text\n* **Punctuation Count** - Total number of punctuations used in the text\n* **Upper-Case to Lower-Case Words ratio** - ratio of upper case words used and lower case words used in the text\n* **Number of Paragraphs** - By looking at the passages, it was noted the \\n symbol. The feature counts the number of \\n in the text\n* **Number of Formal Words** - Number of Complex Words* found in passage\n* **Number of Common Words** - Number of Common Words** found in passage\n* **Number of Repeated Words** - Counts non-stopwords that are used repeatedly in the text (*easier to read samples seem to repeat keywords more often*)\n\n> *Formal Words:* Fragment of Manchester PhraseBank https:\/\/www.phrasebank.manchester.ac.uk\/ <br>\n> *Common Words:* List of ~3.000 of the most commonly used English words extracted from Vocabulary.com","cf418cce":"Merge the SVD features and the New features we created into a single training set and test set:","c7a551df":"Now we build a word frequency dictionary for each quartile limit. The output are **four** dictionaries, one for each region of the normal curve displayed above. The results are stored in a DataFrame and displayed below:","4a005a73":"Ideally, our scatter plot should not be as distributed as it currently shows. The closer to the blue line, the closer our model is able to predict the actual target values. \n\nFor example, looking at the extreme left of the x-axis, smaller taget values are being predicted as higher values. While positive values, e.g. towards 0 and 1 values in the x-axis, are being predicted as smaller values. We also visualise several outliers at the boundary positions of the scatter plot. It is interesting to see how this scatter improves as new ideas and improved models are used.\n\nNow let's train the ANN in the whole training set and submit to the competition:","4937cb76":"By reading the passages we can understand that ***lower target values have more complex text*** when compared to higher target values. With that in mind, let's analyse the Relationship between the Features using Scatter Plots:","388c26f6":">Defining the TF.IDF Matrix","3f26135e":"For the final results and submission, we train our whole dataset with our final model and submit. Our partial results for each Algorithm are the following:","db3b4f14":"# 2.4 Target Quartiles\n\nAs we could not find any patterns regarding the Target values and Common or Complex words, let's see if dividing the target feature into Quartiles we can learn something new about the data provided. We can take advantage of this since the Target feature has a distribution very similar to a Normal distribution.\n\nThe image below is just a visualisation of how we will divide the target values. The first quartile (Q1) is the 25th percentile, it cuts off the lowest 25% of the data. The 75th percentile cuts off the lowest 75% (or highest 25%) of the data is Q3. The second quartile is the 50th percentile providing the center of the data distribution ([source](https:\/\/www.sciencedirect.com\/science\/article\/pii\/B9780123814791000022)):\n\n<img src=\"https:\/\/ars.els-cdn.com\/content\/image\/3-s2.0-B9780123814791000022-f02-02-9780123814791.jpg\" width=\"500\">\n\nFirst, we obtain the Q1, Q2 and Q3 values for the Target Feature:","ca058f9b":"# 3. Model\n\nAs this is first attempt on this competition, the goal is to keep it simple and understand the results and where can the model be improved. After a few initial tests, the following strategy was found satisfactory:\n\n* Preprocess the text data and prepare the additional Features to be used by ML model\n* Separate into 80-20 training and test ratio\n* Use K-Fold for training and validation of model parameters. Five fold keeps a good ratio of training and validation sets for each fold\n* Create a TF.IDF matrix. It has shown better results when compared to Word Count \n* Applying SVD has improved the Test set results. An analysis is shown on how the number of SVD components was selected\n* Here I use two of my favourite algorithms, LGBM and ANN is used to predict the Target.\n\n","bc3844d0":"* The values seems to be spread according to the Target values range, even though there is a lot of overlap between the four quartiles \n* Most of the blue samples seem to be towards the left of the graph, while the red samples (with higher target values) are more concentrated towards the right of the graph\n* Red and Green samples, containing values higher than the median value (from Q2 threshold and above, Q50-Q75 and >=Q75 samples), seem to be more prone to outliers than blue and orange samples\n\nA nice 3D plot is shown next:","9e5a807f":"> My Functions","535bc32d":"<img src=\"https:\/\/images.unsplash.com\/photo-1491841550275-ad7854e35ca6?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=967&q=80\" width=\"500\">\nPhoto by <a href=\"https:\/\/unsplash.com\/@aaronburden?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Aaron Burden<\/a> on <a href=\"https:\/\/unsplash.com\/s\/photos\/reading-children?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash<\/a>\n\n# CommonLit Readability Prize Competition\n\nCommonLit, Inc., is a nonprofit education technology organization serving over 20 million teachers and students with free digital reading and writing lessons for grades 3-12. Together with Georgia State University, an R1 public research university in Atlanta, they are challenging Kagglers to improve readability rating methods.\n\n## <center style=\"background-color:Gainsboro; width:40%;\">Contents<\/center>\n1. [Overview](#1.-Overview)<br>\n    1.1 [Acknowledgements](#1.2.-Acknowledgements)<br>\n2. [The Data](#2.-The-Model)<br>\n    2.1 [Creating Additional Features](#2.1.-Creating-Additional-Features)<br>\n    2.2 [KDE Plots](#2.2.-KDE-Plots)<br>\n    2.3 [Target Feature and Feature Relantionships](#2.3.-Target-Feature-and-Feature-Relantionships)<br>\n    2.4 [Target Quartiles](#2.2.-Target-Quartiles)<br>\n3. [Model](#3.-Model)<br>   \n4. [Results and Conclusion](#4.-Results-and-Conclusion)<br>\n\n***Please remember to upvote if you find this Notebook helpful!***","3e73e2ff":"* The dataset is rather small, with less than 3.000 samples. It is interesting that the test set contains only seven examples available\n* Several Null entries have been found. It is necessary to understand if these empty values are from relevant features or can be ignored\n* No duplicated entries, no need to remove samples. It seems like no data cleansing is going to be required\n\nNow, let's understand where the missing values are coming from:","f7746c7e":"And here is a sample of the text with **MIN** target value:","cdb9b5ac":"## Thanks for reading it this far. Make sure to upvote if you found it helpful"}}