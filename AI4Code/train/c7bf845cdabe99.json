{"cell_type":{"c9d655ff":"code","79f29ab6":"code","1cabe928":"code","70c38b64":"code","1ee60819":"code","f6134853":"code","e22872a7":"code","357966df":"code","ce4a7422":"code","9a9d231e":"code","63caaad8":"code","f16fedba":"code","91cfed45":"code","1548f1b9":"code","abe3420d":"code","ca840829":"code","69b071c7":"code","440f5f20":"code","fd0e81d7":"code","514c7974":"markdown","4c2a9fe2":"markdown","ed32bf45":"markdown","3deef4f2":"markdown","baa629fa":"markdown","16b2f957":"markdown","bb8e6948":"markdown","d6beb1d9":"markdown","84b06fb3":"markdown","e8c88ec4":"markdown","4c180d14":"markdown","4e2885bb":"markdown","bfa8e8ea":"markdown","8ce6fa07":"markdown","50933a4b":"markdown","6e4fa99c":"markdown","d548f866":"markdown","2c8caafb":"markdown"},"source":{"c9d655ff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', None)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","79f29ab6":"iris = pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')","1cabe928":"iris.head()","70c38b64":"iris.info()","1ee60819":"iris.describe()","f6134853":"iris[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']].corr()","e22872a7":"plt.figure(figsize=(10,8))\nsns.heatmap(iris[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']].corr(), vmin=-1.0, vmax=1.0, annot=True, linewidths=2)\nplt.show()","357966df":"iris.groupby('Species').describe()","ce4a7422":"plt.figure(figsize=(8,8))\nax = sns.boxplot(x=\"Species\", y=\"SepalLengthCm\", data=iris).set_title('Sepal Length')\nplt.show()","9a9d231e":"plt.figure(figsize=(8,8))\nax = sns.boxplot(x=\"Species\", y=\"SepalWidthCm\", data=iris).set_title('Sepal Width')\nplt.show()","63caaad8":"plt.figure(figsize=(8,8))\nax = sns.boxplot(x=\"Species\", y=\"PetalLengthCm\", data=iris).set_title('Petal Length')\nplt.show()","f16fedba":"plt.figure(figsize=(8,8))\nax = sns.boxplot(x=\"Species\", y=\"PetalWidthCm\", data=iris).set_title('Petal Width')\nplt.show()","91cfed45":"# We take 80% of data into training, and 20% into test\n# For each set, a third belonds to each type of Iris\niris.drop(['Id'], axis=1, inplace=True)\ntraining = pd.concat([iris[:40], iris[50:90], iris[100:140]])\ntest = pd.concat([iris[40:50], iris[90:100], iris[140:]])\ntraining_X = training[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]\ntraining_y = training['Species']\ntest_X  = test[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]\ntest_y = test['Species']","1548f1b9":"print('Training set:', training_X.shape)\nprint('Test set:', test_X.shape)","abe3420d":"from sklearn.linear_model import LogisticRegression\nLR_classifier = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=200).fit(training_X, training_y)\nprint('Training accuracy:', LR_classifier.score(training_X, training_y))\nprint('Test accuracy:', LR_classifier.score(test_X, test_y))","ca840829":"from sklearn.tree import DecisionTreeClassifier\ndTree_classifier = DecisionTreeClassifier(criterion=\"entropy\").fit(training_X, training_y)\nprint('Training accuracy:', dTree_classifier.score(training_X, training_y))\nprint('Test accuracy:', dTree_classifier.score(test_X, test_y))","69b071c7":"from sklearn.neighbors import KNeighborsClassifier\nKNN_classifier = KNeighborsClassifier().fit(training_X, training_y)\nprint('Training accuracy:', KNN_classifier.score(training_X, training_y))\nprint('Test accuracy:', KNN_classifier.score(test_X, test_y))","440f5f20":"from sklearn.svm import LinearSVC\nSVC_classifier = LinearSVC(multi_class='crammer_singer', max_iter=3000).fit(training_X, training_y)\nSVC_classifier.score(training_X, training_y)\nprint('Training accuracy:', SVC_classifier.score(training_X, training_y))\nprint('Test accuracy:', SVC_classifier.score(test_X, test_y))","fd0e81d7":"from sklearn.tree import plot_tree\nplt.figure(figsize=(10,10))\nplot_tree(dTree_classifier)\nplt.show()","514c7974":"## Import Libraries","4c2a9fe2":"# Iris Flower Classication\n","ed32bf45":"## Import Dataset","3deef4f2":"### Logistic Regression","baa629fa":"## Modeling\n","16b2f957":"### K Nearest Neighbor(KNN)","bb8e6948":"We will run a couple classification algorithms on the dataset and see how well they work.\n* Logistic Regression\n* Decision Tree\n* K Nearest Neighbor(KNN)\n* Support Vector Machine","d6beb1d9":"First, let's check out the variables, and see if there's any correlation with each other.","84b06fb3":"All four algorithms performs perfectly on the test set, achieving 100% accuracy. We can now use the algorithms to predict the type of Iris when we have new measurements.","e8c88ec4":"This is one of the most famous dataset in data science. The task is to classify the sample into three types iris flower-Iris-setosa, Iris-versicolor, and Iris-virginica based on sepal length, sepal width, petal length, and petal width. This is a classic multi-variable multi-class classification problem.","4c180d14":"Now let's take a look at the distribution of each variables with respect to each type of Iris.","4e2885bb":"We start with separating the dataset into training, validation, and test set.","bfa8e8ea":"## Initial Exploration\nBefore running any algorithms, let's first try to understand the dataset a bit.","8ce6fa07":"At last, let's visualize the decision tree.","50933a4b":"### Support Vector Machine","6e4fa99c":"### Decision Tree","d548f866":"From the four graphs above, we can see that there's a large difference in distribution of data especially for sepal length, petal length, petal width, not so much for sepal width. Decision tree seems like a sensible algorithm to model the problem.","2c8caafb":"We can see that there's a really strong positive correlation between petal length and petal width, a strong positive correlation between sepal length and petal length, and a strong positive correlation between sepal length and petal width. If there's a huge dataset with large amount of features, we can run a dimensionality reduction algorithm to take out the redundant features, but because this is such a small dataset, we can keep all the features."}}