{"cell_type":{"91f12eee":"code","8b8965af":"code","e272c60c":"code","eb10ef90":"code","feaee69e":"code","cb3289a7":"code","6142a61b":"code","f83ce2d0":"code","50d8e8f3":"code","e24a964e":"code","8f800949":"code","81d1051b":"code","934b86d9":"code","b1ced6e2":"code","9c3cb0bf":"code","da405caa":"markdown","72544dc7":"markdown","ec33d10f":"markdown","25da899c":"markdown","4c2a32f8":"markdown","075ba2c9":"markdown","ee5b1ae8":"markdown","a79ceefd":"markdown","cd7d3c32":"markdown","c436d9b1":"markdown","06c4efa0":"markdown","91bdb1cf":"markdown","14f92132":"markdown","9b6a94a0":"markdown","46e235d7":"markdown","0e62f445":"markdown","a2f2b229":"markdown","0b4e951d":"markdown","aebcef67":"markdown","47649bca":"markdown"},"source":{"91f12eee":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8b8965af":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\n\n\ninput_path  = '\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/'\ntrain_data  = pd.read_csv(os.path.join(input_path, 'train.csv.zip'))\nstores_data = pd.read_csv(os.path.join(input_path, 'stores.csv'))\nfeatures    = pd.read_csv(os.path.join(input_path, 'features.csv.zip'))\ntest_data   = pd.read_csv(os.path.join(input_path, 'test.csv.zip'))","e272c60c":"train_data.head()","eb10ef90":"stores_data.head()","feaee69e":"features.head()","cb3289a7":"intersection_columns = ['Store', 'Date', 'IsHoliday']\ndata = pd.merge(train_data,\n                features,\n                how='left',\n                left_on=intersection_columns,\n                right_on=intersection_columns)","6142a61b":"intersection_columns = ['Store']\ndata = pd.merge(data,\n                stores_data,\n                how='left',\n                left_on=intersection_columns,\n                right_on=intersection_columns)","f83ce2d0":"print(len(train_data))\nprint(len(features))\nprint(len(data))","50d8e8f3":"'''\nPrimeiro passo:\n- Preciso converter os dados temporais (timestamp)\nem identificadores que permita maior facilidade na \nmanipula\u00e7\u00e3o dos dados.\n'''\ndata['week'] = pd.to_datetime(data.Date).dt.week\ndata['year'] = pd.to_datetime(data.Date).dt.year","e24a964e":"'''\nSegundo passo:\n- Plotar o volume de vendas semanais indicando a semana\nque ocorre o feriado com uma linha azul\n'''\nplt.figure(figsize=(20,8))\nfor year in data.year.unique():\n    var = data[data.year==year]['Weekly_Sales'].groupby(data['week']).sum()\n    sns.lineplot(var.index, var.values)\nplt.grid()\nplt.xticks(np.arange(1, 53, step=1))\nplt.legend(data.year.unique(), loc='best', fontsize=18)\nplt.title('Vendas semanais por ano', fontsize=24)\nplt.ylabel('Vendas', fontsize=21)\nplt.xlabel('Semana', fontsize=21)\nfor value in data[data.IsHoliday].week.unique():\n    plt.axvline(value, color='blue')\nplt.show()","8f800949":"'''\nPrimeiro passo:\nEstruturando o dado ao formato necess\u00e1rio para visualiza\u00e7\u00e3o.\n'''\ncorr = pd.DataFrame([])\nfor year in data.year.unique():\n    corr = pd.concat([corr, data[data.year==year]['Weekly_Sales'].groupby(data['week']).sum()], axis=1)\ncolumns_name = list()\nfor year in data.year.unique():\n    columns_name += [f'data_{year}']\ncorr.columns = columns_name","81d1051b":"'''\nCalculando o coeficiente de correla\u00e7\u00e3o das vendas semanais agrupando-as por ano.\n'''\ncorr = corr.corr()\n\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nf, ax = plt.subplots(figsize=(10, 5))\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nplt.title('Matriz de Correla\u00e7\u00e3o', fontsize=18)\n\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.show()","934b86d9":"'''\nCalculando o coeficiente de correla\u00e7\u00e3o entre as vari\u00e1veis.\n\nPara responder a quest\u00e3o acima, foi desconsiderado algumas vari\u00e1veis:\n\n- Store e Dept: o identificador da loja e do departamento s\u00e3o vari\u00e1veis categ\u00f3ricas armazenadas como n\u00fameros,\ne para analisar o coeficiente de correla\u00e7\u00e3o entre duas vari\u00e1veis voc\u00ea precisa analisar vari\u00e1veis n\u00famericas.\n- MarkDown: Existem alguns aspectos que a an\u00e1lise dessa vari\u00e1vel nesse quesito podem levar a conclus\u00f5es imprecisas.\nAs minhas duas maiores preocupa\u00e7\u00f5es s\u00e3o:\nI) Os dados est\u00e3o anonimizados e isso prejudica na interpretabilidade da an\u00e1lise. Como explicar a correla\u00e7\u00e3o\nse voc\u00ea n\u00e3o entende porque ela acontece j\u00e1 que os dados s\u00e3o an\u00f4nimos?\nII) Os dados est\u00e3o dispon\u00edveis para parte da base, ou seja, voc\u00ea pode tecer an\u00e1lises que s\u00f3 s\u00e3o v\u00e1lidas para\numa parte dos dados. Extender essas conclus\u00f5es para o resto da base pode levar a conclus\u00f5es erradas.\n\nPor tal raz\u00e3o, resolvi desconsiderar essas vari\u00e1veis na an\u00e1lise.\n'''\n\ncorr = data.drop(['Store', 'Dept', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5'], axis=1).corr()\n\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nf, ax = plt.subplots(figsize=(20, 15))\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nplt.title('Matriz de Correla\u00e7\u00e3o', fontsize=18)\n\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.show()","b1ced6e2":"plt.figure(figsize=(20,8))\nfor type_ in data.Type.unique():\n    var = data[data.Type==type_]['Weekly_Sales'].groupby(data['week']).median()\n    sns.lineplot(var.index, var.values)\nplt.grid()\nplt.xticks(np.arange(1, 53, step=1))\nplt.legend(data.Type.unique(), loc='best', fontsize=18)\nplt.title('Vendas semanais por tipo de estabelecimento', fontsize=24)\nplt.ylabel('Vendas', fontsize=21)\nplt.xlabel('Semana', fontsize=21)\nplt.show()","9c3cb0bf":"!pip install quilt\nimport missingno as msno\nmsno.matrix(data.sort_values(by='year'))","da405caa":"Eu acredito que algumas solu\u00e7\u00f5es mais criativas e rebuscadas podem apresentar desempenho superior ao que eu propus.\n\nEntretanto, acredito no princ\u00edpio de Pareto, em que voc\u00ea realiza 80% de uma tarefa em 20% do tempo.\n\nSendo assim, eu testaria a abordagem proposta e veria o resultado do modelo, ap\u00f3s isso, estudaria o custo-benef\u00edcio em aplicar mais esfor\u00e7os na abordagem proposta para melhorar o desempenho, por exemplo, usando alguma t\u00e9cnica como *oversampling* (aumento no volume de dados de maneira sint\u00e9tica), *feature generation* (gera\u00e7\u00e3o de novas vari\u00e1veis) ou *fine-tunning* (modifica\u00e7\u00e3o nos hiper-par\u00e2metros do modelo para aumentar o desempenho do modelo). Certamente, se eu tivesse mais tempo, buscaria investir esfor\u00e7os nas t\u00e9cnicas citadas aqui, pois n\u00e3o consigo pensar em algoritmos de machine-learning que pudessem apresentar solu\u00e7\u00f5es melhores. \n\n\n\n\n","72544dc7":"Com base no gr\u00e1fico acima podemos fazer as seguintes observa\u00e7\u00f5es:\n* Super bowl: \u00e9 um feriado com pouco impacto nas vendas. Se voc\u00ea observar os meses circunvizinhos, a semana que ocorre o Super Bowl pouco se destoa das demais semanas e talvez seja um feriado que ser melhor explorado pela gest\u00e3o do varejo para expandir as vendas nesse feriado.\n* Labor day: tamb\u00e9m \u00e9 um feriado com pouco destaque. Ao longo dos tr\u00eas anos poucas vendas a mais s\u00e3o feitas nessa data. Provavelmente, esse feriado \u00e9 usado para renova\u00e7\u00e3o de estoque e reabastecimento dos estoques dos novos produtos que ser\u00e3o vendidos no Natal.\n* Thanksgiving: Primeiro feriado do ano com grande impacto nas vendas. Percebe-se um grande volume de vendas nessa semana, embora que, esse volume de vendas tenha pouco variado ao longo dos anos.\n* Christmas: Poucas semanas ap\u00f3s o dia de a\u00e7\u00e3o de gra\u00e7as ocorre o Natal. Ao observar as vendas realizadas no Natal, percebe-se um baixo volume nas vendas. Entretanto, a semana que antecede o Natal apresenta um pico de vendas que supera todas as demais semanas do ano, para todos os anos fornecidos. Assim, faz-se necess\u00e1rio que o varejo se prepare antecipadamente para uma semana de vendas aquecida do Natal.\n\n\nCom base nessas an\u00e1lises, podemos perceber que os dois primeiros feriados (Super bowl e Labor day) tem um n\u00edvel de vendas constante quando comparado as semanas de seu entorno. Por outro lado, os feriados de Thanksgiving e Christmas fazem as pessoas consumir muito mais que as demais semanas. Respondendo a quest\u00e3o: Os feriados realmente impactam as vendas? Depende. Sim, ao considerar os feriados de Thanksgiving e Christmas. N\u00e3o, ao considerar os feriados de Super bowl e Labor day.","ec33d10f":"![](https:\/\/www.kaggle.com\/static\/images\/host-home\/host-home-research.png)\n\n# Planejamento de aplica\u00e7\u00e3o de Machine Learning para resolver o problema\n#### Escolher um algoritmo de Machine Learning \n#### Comentar sobre a escolha do algoritmo \n","25da899c":"Ao analisar uma matriz de correla\u00e7\u00e3o, procuramos entender quais aspectos podem explicar correla\u00e7\u00f5es fortes (sejam elas positivas ou negativas). Destaquei algumas correla\u00e7\u00f5es e busquei tecer a minha an\u00e1lise sobre elas. A vari\u00e1vel mais importante a se observar s\u00e3o as vendas semanais. Entretanto, essa vari\u00e1vel s\u00f3 possui correla\u00e7\u00e3o relevante com uma outra vari\u00e1vel que \u00e9 o tamanho da loja. Seria muito interessante ver outras vari\u00e1veis correlacionadas ao volume de vendas semanal. Mas, busquei aprofundar em cada detalhe da an\u00e1lise.\n\n**CORRELA\u00c7\u00d5ES POSITIVAS**\n* 0.78 Pre\u00e7o do combust\u00edvel vs Ano: Esse alto \u00edndice de correla\u00e7\u00e3o pode ser explicado pelo aumento no pre\u00e7o dos combust\u00edveis ao longo dos anos. Entretanto, observando o pre\u00e7o do combust\u00edvel e as vendas semanais, percebemos que mesmo o pre\u00e7o do combust\u00edvel tenha subido ao longo dos anos, as vendas semanais n\u00e3o foi impactada por esse aspecto (correla\u00e7\u00e3o muito pr\u00f3xima de zero).\n* 0.24 Tamanho da loja vs Vendas Semanais: Um aspecto bastante relevante no volume de vendas semanais deveria ser o tamanho da loja. Quanto maior a loja, maior o volume de vendas. Entretanto, esse indicador nos mostra que essa regra n\u00e3o necessariamente \u00e9 v\u00e1lido. Isto \u00e9, se quanto maior a loja, maior fosse o volume de vendas, e essa regra fosse sempre obedecida, n\u00f3s observar\u00edamos um coeficiente de correla\u00e7\u00e3o entre essas duas vari\u00e1veis igual a 1.0. No entanto, esse valor (0.24) indica uma correla\u00e7\u00e3o moderada, e que maiores lojas tendem a vender mais em boa parte dos casos.\n* 0.24 Temperatura vs Semana: Ao passar das semanas a temperatura tamb\u00e9m aumenta pois o ver\u00e3o americano acontece aproximadamente no meio do ano e temperatura volta a cair no segundo semestre. Sendo assim, essa correla\u00e7\u00e3o apresenta pouca relev\u00e2ncia para a an\u00e1lise.\n* 0.18 CPI e Temperatura + 0.14 Pre\u00e7o do Combust\u00edvel e Temperatura: Considerando o aquecimento global um fen\u00f4meno verdadeiro, ou seja, que ao longo dos anos a temperatura ir\u00e1 sempre subir, e que, \u00edndices de pre\u00e7os como o CPI tamb\u00e9m tendem a ser sempre positivos pois a infla\u00e7\u00e3o tendem a aumentar esse valor, e consequentemente tamb\u00e9m elevar o pre\u00e7o do combust\u00edvel. Vemos uma leve correla\u00e7\u00e3o entre esses aspectos. O \u00edndice de pre\u00e7os ao consumir sobe com uma correla\u00e7\u00e3o de 0.18 em rela\u00e7\u00e3o a temperatura, como tamb\u00e9m, a temperatura sobe com uma correla\u00e7\u00e3o de 0.14 com o pre\u00e7o do combust\u00edvel.\n\n**CORRELA\u00c7\u00d5ES NEGATIVAS**\n* -0.3 CPI e desemprego: Essa medida indica que o \u00edndice de pre\u00e7os ao consumidor \u00e9 menor quando o desemprego est\u00e1 mais alto. *Disclaimer*: Embora ambas as vari\u00e1veis n\u00e3o apresentem correla\u00e7\u00e3o significativa entre si e o volume de vendas semanais, esse tipo de observa\u00e7\u00e3o pode parecer fazer pouco sentido para n\u00f3s, mas vari\u00e1veis descorrelacionadas ajudam a m\u00e1quina a fazer melhores predi\u00e7\u00f5es. Imagine uma base de dados com redund\u00e2ncia nas vari\u00e1veis, por exemplo, a \u00e1rea de uma casa medida em cent\u00edmetros quadrados e metros quadrados. A correla\u00e7\u00e3o entre essas duas vari\u00e1veis seria de 100%, por outro lado, fornecer ambas as vari\u00e1veis para o modelo traria pouco ganho na capacidade preditiva. Ou seja, por mais que essas correla\u00e7\u00f5es fa\u00e7am pouco sentido para n\u00f3s, podem ser bastante interessantes para o modelo.\n* -0.24 Desemprego e Ano: Essa correla\u00e7\u00e3o indica que o desemprego teve leve queda ao longo dos anos. Por outro lado, essa rela\u00e7\u00e3o n\u00e3o impactou no volume de vendas. Entretanto, foi disponibilizado o hist\u00f3rico de apenas 3 anos. \u00c9 poss\u00edvel que as pessoas que conseguiram emprego busquem quitar as suas d\u00edvidas e posteriormente fazer mais compras. Com um hist\u00f3rico maior, tendo acesso aos anos seguintes, eu buscaria observar se houve aumento no volume de vendas que pudesse estar associado a taxa de desemprego. \u00c9 dif\u00edcil de isolar apenas esse aspecto na an\u00e1lise, mas um modelo que usa dados hist\u00f3ricos poder\u00e1 se beneficiar com essa rela\u00e7\u00e3o.\n* -0.16 CPI e Pre\u00e7o de combust\u00edvel: Percebe-se que o pre\u00e7o do combust\u00edvel subiu ao longo dos anos (correla\u00e7\u00e3o de 0.78 entre o pre\u00e7o de combust\u00edvel e o ano), e que o \u00edndice de pre\u00e7os ao consumidor \"andou de lado\" ao longo do per\u00edodo (correla\u00e7\u00e3o de 0.075 do CPI e ano). No entanto, o pre\u00e7o do combust\u00edvel t\u00eam correla\u00e7\u00e3o ligeiramente negativa em rela\u00e7\u00e3o ao CPI. Ou seja, o \u00edndice de pre\u00e7os ao consumidor deve estar associado a outras vari\u00e1veis que andam em sentidos levemente opostos ao pre\u00e7o do combust\u00edvel.\n\n","4c2a32f8":"## Quest\u00e3o 2: As vendas semanais s\u00e3o consistentes ao longo dos anos?\n\n\u00c9 comum que grandes varejistas tenham semanas com n\u00fameros excepcionais de vendas. Por exemplo, a Alibaba informou que no 'Dia dos solteiros', a movimenta\u00e7\u00e3o de vendas na China foi de R$ 300 milh\u00f5es [(fonte)](https:\/\/g1.globo.com\/economia\/noticia\/2020\/11\/11\/dia-dos-solteiros-movimenta-cerca-de-r-300-bilhoes-em-vendas-na-china.ghtml). Em rela\u00e7\u00e3o ao problema de neg\u00f3cio, a varejista precisa se preparar antecipadamente para semanas com pico de vendas, buscando evitar atrasos nas entregas, minimizar a falta de itens no estoque, expandir a disponibilidade dos servidores para evitar congestionamento no site, entre outras log\u00edsticas que permitam melhorar a experi\u00eancia do cliente e maximizar o lucro da empresa. Diante desse cen\u00e1rio, surge um importante problema t\u00e9cnico, que seria prever as vendas semanais com base em dados de vendas de anos anteriores. Para responder essa pergunta, usei uma matriz de correla\u00e7\u00e3o que analisa as vendas semanais em todos os anos registrados.","075ba2c9":"Com o aux\u00edlio dessa ferramenta, podemos visualizar os dados faltantes por c\u00e9lulas brancas nas respectivas colunas de cada vari\u00e1vel. Com isso, percebemos que as vari\u00e1vels `MarkDown` come\u00e7aram a ser registradas em 2011 ainda foi de maneira muito pequena (perceba o excesso de c\u00e9lulas em branco indicando a esparsidade dos dados). Mesmo ap\u00f3s isso, em meados de 2012, ainda existe muitas c\u00e9lulas em branco para as vari\u00e1veis `MarkDown2`, `MarkDown3` e `MarkDown4`. Nesse caso, pode-se usar algum modelo de predi\u00e7\u00e3o para preencher os dados faltantes, como por exemplo, a fatora\u00e7\u00e3o de matrizes para os dados faltantes. No entanto, n\u00e3o h\u00e1 representantes para 2010 e poucos casos para 2011, assim, o preenchimento autom\u00e1tico desses valores poder\u00e1 enviesar o modelo que treinar\u00e1 com esses dados para fazer predi\u00e7\u00f5es erradas. Sendo assim, a minha recomenda\u00e7\u00e3o seria descartar esses atributos.\n","ee5b1ae8":"Como podemos observar acima, o dataframe resultante tem o mesmo n\u00famero de inst\u00e2ncias que o dataframe de refer\u00eancia (`train_data`). Pois, nesse caso, as features de uma mesma observa\u00e7\u00e3o s\u00e3o compartilhadas por mais de uma inst\u00e2ncia e por isso, o dataframe `features` tem um tamanho menor. O importante a observar aqui \u00e9 que n\u00e3o houve desperd\u00edcio de dados.\n\n# Elaborar 4 quest\u00f5es que ajudem a compreender os dados\n\n![](https:\/\/codemyviews-blog-post-images.s3.amazonaws.com\/uploads\/machine-learning.png)\n\n## Quest\u00e3o 1: Os feriados impactam nas vendas?\n\nAo manipular os dados de vendas de uma grande varejista pela primeira vez, a principal d\u00favida que surge \u00e9: feriados realmente impactam nas vendas?\nUma informa\u00e7\u00e3o importante \u00e9 tentar observar se o n\u00edvel de oscila\u00e7\u00e3o nas vendas semanais podem indicar que uma grande n\u00famero de vendar ir\u00e1 acontecer na semana do feriado.\nPortanto, usei um gr\u00e1fico que mostra a distribui\u00e7\u00e3o de vendas para cada semana do ano nos tr\u00eas anos de vendas fornecidos. \nAl\u00e9m disso, usei uma linha em destaque para enfatizar a semana que acontece o feriado, o que gerou algumas conclus\u00f5es discutidas a seguir.\n","a79ceefd":"Observando o gr\u00e1fico acima, vemos claramente que o volume de vendas (usando a mediana) por estabelecimento de cada categoria t\u00eam sim grande impacto nas vendas. E respondendo as quest\u00f5es levantadas no primeiro trecho desse t\u00f3pico, essas categorias podem estar associadas ao volume de vendas de cada estabelecimento. Sendo os estabelecimentos mais lucrativos aqueles que pertecem a categoria A, e os menos lucrativos aqueles que pertecem a categoria C.","cd7d3c32":"## Quest\u00e3o 4: A categoria do estabelecimento influ\u00eancia nas vendas?\n\nNos dados observamos tr\u00eas categorias de estabelecimentos (A, B e C). Essas categorias de estabelecimentos podem identificar diferentes aspectos, por exemplo, o p\u00fablico-alvo do estabelecimento, ou a qualidade da estrutura f\u00edsica da loja, ou a lucratividade do empreedimento. De toda maneira, o uso apropriado dessas informa\u00e7\u00f5es podem trazer ganhos, tanto para o neg\u00f3cio quanto para o cientista de dados. Por exemplo, ao abrir uma nova loja, a empresa financia diversos planos de neg\u00f3cios que buscam maximizar os ganhos da companhia, entre elas, essa m\u00e9trica pode ajudar a definir esse melhor local. Por outro lado, o cientista de dados ao prever o n\u00edvel de vendas, poderia usar essa informa\u00e7\u00e3o ao seu favor. Ser\u00e1 que a categoria do estabelecimento tem alguma rela\u00e7\u00e3o com as vendas?","c436d9b1":"## Quest\u00e3o 3: Existe relev\u00e2ncia na correla\u00e7\u00e3o das vari\u00e1veis?\n\nAo lapidar os dados em busca de extrair insights relevantes para a tomada de decis\u00e3o, um aspecto importante a se observar \u00e9 se existem aspectos ou fen\u00f4menos que expliquem a correla\u00e7\u00e3o das vari\u00e1veis coletadas. Com isso, novas ideias podem ser implementadas ao modelo e aspectos n\u00e3o observados podem ser notados. Assim, busquei fazer uma matriz de correla\u00e7\u00e3o para todas as vari\u00e1veis que julguei fazer sentido observar a correla\u00e7\u00e3o.\n\n*Disclaimer*: Correla\u00e7\u00e3o \u00e9 uma medida que indica rela\u00e7\u00e3o entre duas vari\u00e1veis, que n\u00e3o necessariamente indicam rela\u00e7\u00e3o de causalidade entre elas. Por exemplo, suponhamos que existe uma alta correla\u00e7\u00e3o entre o n\u00famero de banheiros de uma casa e a renda bruta da fam\u00edlia que habita nessa casa. Isso quer dizer que pessoas que ganham mais, tendem a morar em casas com mais banheiros. Ou seja, h\u00e1 correla\u00e7\u00e3o positiva entre essas duas vari\u00e1veis. No entanto, se voc\u00ea construir mais banheiros em sua casa, voc\u00ea n\u00e3o ir\u00e1 aumentar a sua renda bruta. Pois essa n\u00e3o h\u00e1 rela\u00e7\u00e3o de causalidade entre essas duas vari\u00e1veis.","06c4efa0":"**Organiza\u00e7\u00e3o do documento**\n\n1. Carregando os dados de entrada e carregando bibliotecas<br>\n   1.1 Manipula\u00e7\u00e3o com os dados Iniciais<br>\n2. Elabora\u00e7\u00e3o de 4 quest\u00f5es que ajudem a compreender os dados<br>\n   2.1 Quest\u00e3o 1: Os feriados impactam nas vendas?<br>\n   2.2 Quest\u00e3o 2: As vendas semanais s\u00e3o consistentes ao longo dos anos?<br>\n   2.3 Quest\u00e3o 3: Existe relev\u00e2ncia na correla\u00e7\u00e3o das vari\u00e1veis?<br>\n   2.4 Quest\u00e3o 4: <br>\n3. Listar 1 limpeza que precisa ser feita nos dados e a executar <br>\n4. Planejamento de aplica\u00e7\u00e3o de Machine Learning para resolver o problema <br>\n   4.1 Escolher um algoritmo de Machine Learning <br>\n   4.2 Comentar sobre a escolha do algoritmo <br>\n5. Autocr\u00edtica e pr\u00f3ximos passos <br>\n   5.1 Enumerar eventuais problemas e limita\u00e7\u00f5es da estrat\u00e9gia <br>\n   5.2 Comentar sobre o que voc\u00ea teria feito se tivesse mais tempo para tratar o problema <br>\n   \n# Carregando os dados de entrada e bibliotecas","91bdb1cf":"Esse problema se caracteriza pela predi\u00e7\u00e3o no volume de vendas considerando uma linha temporal.\nSendo assim, pensamos que usar Redes Neurais Recorrentes (RNN\/LSTM) possa ser o mais indicado. \nEntretanto convido para uma an\u00e1lise\/reflex\u00e3o mais profunda sobre essa abordagem.\n\nA solu\u00e7\u00e3o proposta pode envolver duas maneiras de modelar os dados: \n    I) Considerar o volume de vendas realizadas nas \u00faltimas semanas, ou\n    II) Considerar o volume de vendas semanais realizadas nas mesmas semanas que a predita nos \u00faltimos anos.\nDetalhando um pouco mais:\n\nA solu\u00e7\u00e3o (I) indica que o volume de vendas nas \u00faltimas `n` semanas pode n\u00e3o ser \u00fatil para a modelo pois as semanas de picos s\u00e3o antecedidas por semanas de vendas comuns, e maior parte das semanas tem um volume de vendas praticamente constante. Nesse caso, o modelo poderia ter dificuldade em prever o volume de vendas nas semanas finais do ano pois poderiam ser consideradas *outliers*. Um problema cl\u00e1ssico entre especializa\u00e7\u00e3o ou generaliza\u00e7\u00e3o de modelos, um *trade-off* onde voc\u00ea melhora a acur\u00e1cia ao custo de diminuir a capacidade de generaliza\u00e7\u00e3o e vice-versa. \n\nA solu\u00e7\u00e3o (II) pode parecer mais promissora por alguns aspectos, isto \u00e9, olhando o volume de vendas realizada em uma mesma semana nos diferentes anos, voc\u00ea percebe que esse volume praticamente n\u00e3o muda. Ou seja, caso voc\u00ea queira prever o volume de vendas na semana 23, em vez de voc\u00ea olhar para as `n` semanas anteriores a semana 23. Voc\u00ea deveria observar o volume de vendas da semana 23 nos anos anteriores. No entanto, ao usar essa abordagem voc\u00ea esbarra com um problema, pois os dados s\u00e3o limitados e voc\u00ea n\u00e3o ter\u00e1 muitos exemplos para treinar o modelo, consequentemente, ele poder\u00e1 n\u00e3o apresentar um bom desempenho.\n\nNesse sentido, penso que tratar problemas envolvendo s\u00e9ries temporais, o uso de redes neurais recorrentes (do ingl\u00eas, RNN) pode fazer sentido. No entanto, o pequeno volume de dados me faz pensar que alguns problemas poder\u00e3o acontecer. Por exemplo, o modelo ter dificuldades para convergir, e isso impactar em seu desempenho. Nesse caso, eu acredito que diante o cen\u00e1rio encontrado, a solu\u00e7\u00e3o mais promissora possa ser usando \u00e1rvores de decis\u00e3o. Diante a minha experi\u00eancia, usaria alguma implementa\u00e7\u00e3o como [XGBoost](https:\/\/xgboost.readthedocs.io\/en\/latest\/) ou o [CatBoost](https:\/\/catboost.ai\/) para solucionar esse problema. Sendo este \u00faltimo algoritmo, bastante promissor por apresentar uma funcionalidade espec\u00edfica para interpreta\u00e7\u00e3o de modelos que \u00e9 o uso do [SHAP](https:\/\/github.com\/slundberg\/shap), o que permite identificar quais vari\u00e1veis est\u00e3o constribuindo para a previs\u00e3o do modelo. Esse tipo de t\u00e9cnica (SHAP) permite trazer interpretabilidade para qualquer tipo de modelo *machine-learning*, at\u00e9 mesmo os que s\u00e3o considerados *black-box*. E o CatBoost \u00e9 um modelo que traz isso implementado nativamente. \n\n\n","14f92132":"# Listar 1 limpeza que precisa ser feita nos dados e a executar\n\nUma boa limpeza de dados pode come\u00e7ar a ser feita tratando os dados faltantes. Diversas abordagens diferentes podem ser aplicadas, como por exemplo, imputar novos dados usando a m\u00e9dia, ou a predi\u00e7\u00e3o de algum modelo como uma regress\u00e3o linear ou log\u00edstica. No entando, precisamos usar alguma ferramenta adequada para essa an\u00e1lise. Vou come\u00e7ar o processo usando uma biblioteca chamada `missingno` [link](https:\/\/github.com\/ResidentMario\/missingno) que pode ser bastante \u00fatil nesses casos.","9b6a94a0":"![](https:\/\/www.vogue.pt\/media\/content\/balanco-me-too-times-up-assedio-sexual.jpg)","46e235d7":"Observando a matriz de correla\u00e7\u00e3o acima, percebemos que h\u00e1 uma forte correla\u00e7\u00e3o entre as vendas semanais para os tr\u00eas anos fornecidos.\n\nO que isso quer dizer? Com base no volume de vendas realizados nos anos anteriores, existem fortes ind\u00edcios que as oscila\u00e7\u00f5es no volume de vendas se mantenham. Isso quer dizer que o volume de vendas crescem (ou diminuem) de maneira consistente ao longo dos anos. E que essas oscila\u00e7\u00f5es podem variar pouco de um ano para outro e muito no longo prazo.\n\nObserve a correla\u00e7\u00e3o de 0.95% entre 2010 e 2011, valor extretamente alto. Outra correla\u00e7\u00e3o muito forte de 0.76 entre os anos de 2011 e 2012. Entretanto, anos mais distantes como 2010 e 2012 apresentam um n\u00edvel de correla\u00e7\u00e3o embora alto mas um pouco inferior que os outros dois casos (0.58), isso pode indicar que o n\u00edvel de vendas semanais pode se parecer muito com o ano anterior, mas que essa s\u00fatil mudan\u00e7a ao longo dos anos pode fazer com que as vendas desse ano sejam muito diferentes das vendas de 5 anos atr\u00e1s.\n\nAssim, o cientista de dados que analisa os dados dessa empresa, deve ter cuidado ao modelar o problema usando modelos com algoritmos como LSTM que usam muitos dados hist\u00f3ricos, pois os mesmos, podem usar como refer\u00eancia um volume de vendas que n\u00e3o reflete a realidade do ano atual.","0e62f445":"## Mesclando os dados de entrada\n\nObserve que `train_data` e `features` cont\u00e9m vari\u00e1veis que se complementam. Logo, faz sentido unificar esses dados em uma s\u00f3 vari\u00e1vel que permita melhor manipula\u00e7\u00e3o e an\u00e1lise dos dados.","a2f2b229":"Quando fazemos o merge de dois dataframes, \u00e9 poss\u00edvel que o dataframe resultante tenha um tamanho menor que o dataframe de refer\u00eancia, caso n\u00e3o haja correspond\u00eancia entre as colunas chaves usadas no merge. Inclusive, caso n\u00e3o seja poss\u00edvel identificar interse\u00e7\u00e3o entre nenhuma das linhas considerando as chaves de ambos os dataframes, \u00e9 poss\u00edvel que o dataframe de sa\u00edda tenha tamanho zero. Ou seja, \u00e9 importante verificar o tamanho do dataframe resultante para saber se alguma linha foi perdida no merge por falta de matching entre as chaves.","0b4e951d":"![](https:\/\/dox4euoyzny9u.cloudfront.net\/images\/blog\/uploads\/dataprocessinggdpr.jpg)","aebcef67":"![](https:\/\/cdn.corporate.walmart.com\/dims4\/WMT\/c2bbbe9\/2147483647\/strip\/true\/crop\/2389x930+0+0\/resize\/1446x563!\/quality\/90\/?url=https%3A%2F%2Fcdn.corporate.walmart.com%2Fd6%2Fe7%2F48e91bac4a8ca8f22985b3682370%2Fwalmart-logos-lockupwtag-horiz-blu-rgb.png)","47649bca":"![](https:\/\/www.kaggle.com\/static\/images\/about\/inclass\/howitworks@2x.png)\n\n# Autocr\u00edtica e pr\u00f3ximos passos \n#### Enumerar eventuais problemas e limita\u00e7\u00f5es da estrat\u00e9gia \n#### Comentar sobre o que voc\u00ea teria feito se tivesse mais tempo para tratar o problema "}}