{"cell_type":{"911e279d":"code","e90b4515":"code","6d25288f":"code","6b70acde":"code","7bb04655":"code","fa96aff8":"code","a0d04199":"code","e06cdc75":"code","e97214d9":"markdown","9e1ea7a9":"markdown","5a323ae5":"markdown"},"source":{"911e279d":"pip install gokinjo","e90b4515":"import numpy as np \nimport pandas as pd \nfrom sklearn.preprocessing import LabelEncoder\n\nfrom gokinjo import knn_kfold_extract\nfrom gokinjo import knn_extract\n\n# list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6d25288f":"# read competiton data\ndf_train = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv') \n\n# label encode the target column\nle = LabelEncoder()\ndf_train.target = le.fit_transform(df_train.target)\n\n# define X and y for training data\nX = df_train.drop(columns=[\"id\",\"target\"])\ny = df_train.target\n\n# prepare test data\nX_test=df_test.drop(columns=\"id\")\n\nprint(\"First five rows of training data:\")\ndisplay(X.head())\nprint(\"First five rows of test data:\")\ndisplay(X_test.head())","6b70acde":"# convert to numpy because gokinjo expects np arrays\nX = X.to_numpy()\ny = y.to_numpy()\nX_test = X_test.to_numpy()\n# check shapes\nprint(\"X, shape: \", np.shape(X))\nprint(\"X_test, shape: \", np.shape(X_test))","7bb04655":"# KNN feature extraction for train, as the data has not been normalized previously, let knn_kfold_extract do it\n# you can set a different value for k, just be aware about the increase in computation time\nKNN_feat_train = knn_kfold_extract(X, y, k=1, normalize='standard')\nprint(\"KNN features for training set, shape: \", np.shape(KNN_feat_train))\nKNN_feat_train[0]","fa96aff8":"# create KNN features for test set, as the data has not been normalized previously, let knn_extract do it\nKNN_feat_test = knn_extract(X, y, X_test, k=1, normalize='standard')\nprint(\"KNN features for test set, shape: \", np.shape(KNN_feat_test))\nKNN_feat_test[0]","a0d04199":"# add KNN feature to normal features\nX, X_test = np.append(X, KNN_feat_train, axis=1), np.append(X_test, KNN_feat_test, axis=1) \nprint(\"Train set, shape: \", np.shape(X))\nprint(\"Test set, shape: \", np.shape(X_test))","e06cdc75":"# store KNN features, they are computationally expensive\nnp.save('add_feat_train', KNN_feat_train)\nnp.save('add_feat_test', KNN_feat_test)\n\n# to load them in your notebook you can use:\n#new_features = np.load('add_feat_train.npy')","e97214d9":"# Boost your score with KNN Feature extraction\n\nFor TPS 6, I wanted to follow a different approach than before. Instead of focusing on modeling, I focused on feature engineering. Given the fact, that this competition has anonymized features, this might not be an obvious choise.\nHowever my [EDA](https:\/\/www.kaggle.com\/melanie7744\/tps6-eda-comparison-to-tps5) also led me to the original dataset. I.e. the dataset used to generate the synthetic one we got for TPS 6. So I researched the solutions that worked back then. Many of them did not seem to be viable for me. But one looked promising: **KNN feature extraction**. \n\nMy quest for knowledge then led me to a nice package: **fastknn**, written by [David Pinto](http:\/\/www.kaggle.com\/davidpinto\/fastknn-show-to-glm-what-knn-see-0-96#Feature-Engineering-with-KNN). It seemed to provide everything I wanted. Unfortunately it was written in R.\n\nSo the search continued until I found .... a Python implementation by Momijiame of this same package! It is called **Gokinjo**, which means neighborhood in Japanese and is available on [Github](https:\/\/github.com\/momijiame\/gokinjo). \n\n\n \n<\/div>\n        \n### What is KNN feature extraction? \n\nfrom [David Pinto](https:\/\/davpinto.github.io\/fastknn\/articles\/knn-extraction.html):\n\n<div class=\"alert alert-success\">\nThe fastknn provides a function to do feature extraction using KNN. It generates k * c new features, where c is the number of class labels. The new features are computed from the distances between the observations and their k nearest neighbors inside each class, as follows:\n<ul>\n<li>The first test feature contains the distances between each test instance and its nearest neighbor inside the first class.<\/li>\n<li>The second test feature contains the sums of distances between each test instance and its 2 nearest neighbors inside the first class.<\/li>\n<li>The third test feature contains the sums of distances between each test instance and its 3 nearest neighbors inside the first class.<\/li>\n<li>And so on.<\/li>\n<\/ul>\nThis procedure repeats for each class label, generating k * c new features. Then, the new training features are generated using a n-fold CV approach, in order to avoid overfitting. Parallelization is available. You can specify the number of threads via nthread parameter.\n<\/div>\n\nSo, let's try it out!","9e1ea7a9":"I used those extra features with an XGBoost Model: \n* My validation logloss improved from 1.75280 to 1.75089\n* My public score improved from 1.75592 to 1.75338\n* From the 10 most important features (as ranked by XGBoost), 8 were KNN features\n\nI'd be curious to know what happens if you use a better model, i.e. one that has already a lower logloss than my XGBoost.\n* Will such a model already have learned the extra insights from the KNN features and have threrfore no improvement in score? \n* Will such a model be able to use the additional KNN features more effectively and get a higher improvement in score?\n\nIf anybody tries this out, please comment below!","5a323ae5":"Note: generating the KNN features for the test set was not straight-forward for me. I did not find any sample code, so I digged into the source code of the Gokinjo package until I found the solution showed here. I hope this is how it is supposed to be done. Should anybody have experience with this package... your feedback is very welcome.\n"}}