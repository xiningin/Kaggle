{"cell_type":{"77893eaf":"code","4588b362":"code","a5bd6efb":"code","10b4f094":"code","bb649a24":"code","aede53f7":"code","76bc42e4":"code","0dd66a38":"code","ae1fb2fb":"code","86adf9c7":"code","075ec9bb":"code","20299b57":"code","43f4ac84":"code","38d112f5":"code","35e98586":"code","a06bc8be":"code","45c28576":"code","2888231b":"code","9d32bb4b":"code","d2039af3":"code","b08ecd25":"code","38de2d0a":"code","69e8067e":"code","fa24db07":"code","a10e8110":"code","1284756f":"code","4ec63734":"code","0c8cbc3b":"code","d19e6a37":"code","c60f0da7":"code","11ee9674":"code","1682ca81":"code","68cfddf6":"code","529e3c51":"code","eec345c5":"code","37072eb0":"code","a707ad85":"code","bdb82194":"code","6144de43":"code","107ea0c3":"code","bfda8105":"code","52489539":"code","238a10a2":"code","be672ef1":"code","9975c964":"code","9caf2ae8":"code","dbb3fa8b":"code","e003b281":"code","11ced7f0":"code","9832e049":"code","fb6babe9":"code","f43b813c":"code","b86a3a84":"code","9732bd67":"code","56fe9b88":"code","b37e2c24":"code","4366c82f":"code","4e102575":"code","787282a5":"code","e853acb2":"code","4017a13c":"code","02d06774":"code","fb01330a":"code","c74a88db":"code","b48930e3":"code","acfa80a4":"code","987ac5bc":"code","1f2dbc5c":"markdown","3194ee0a":"markdown","5d95f282":"markdown","05509c4a":"markdown","ebb72c0c":"markdown","09971976":"markdown","093e1de9":"markdown","63d5576f":"markdown","5b3378d5":"markdown","a981904a":"markdown","f4e729f7":"markdown"},"source":{"77893eaf":"# Importing all the tools(libraries) we need.\n# Regular EDA (Exploratory Data Analysis) and plotting libraries\/tools.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Model Evaluation Tools\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve","4588b362":"df = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\ndf.head()","a5bd6efb":"df.shape","10b4f094":"df.head()","bb649a24":"df.tail()","aede53f7":"df.target.value_counts()","76bc42e4":"df.target.value_counts().plot(kind = \"bar\"\n                             ,color = [\"salmon\", \"lightblue\"]\n                             ,figsize=(5,5))\n\nplt.xlabel(\"1 = Heart Disease, 0 = No Heart Diease\")\nplt.ylabel(\"Amount\")\nplt.xticks(rotation=0);","0dd66a38":"df.info()","ae1fb2fb":"df.isna().sum()","86adf9c7":"df.describe()","075ec9bb":"df.sex.value_counts()","20299b57":"pd.crosstab(df.sex, df.target)","43f4ac84":"pd.crosstab(df.target, df.sex).plot(kind = \"bar\"\n                                   ,color = [\"salmon\", \"lightblue\"]\n                                   ,figsize=(10,10))\n\nplt.title(\"Heart Disease Analysis Based On The Gender\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.ylabel(\"Amount\")\nplt.xticks(rotation=0)\nplt.legend([\"Female\", \"Male\"])\nplt.show()","38d112f5":"df.head()","35e98586":"df.thalach.value_counts()","a06bc8be":"plt.figure(figsize=(10,6))\nplt.scatter(df.age[df.target == 1]\n           ,df.thalach[df.target == 1]\n           ,c=\"salmon\")\n\nplt.scatter(df.age[df.target == 0]\n           ,df.thalach[df.target == 0]\n           ,c=\"lightblue\")\n\nplt.title(\"Heart Disease in function of age and max heart rate\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Thalach\")\nplt.legend([\"Disease\", \"No Disease\"])\nplt.show()","45c28576":"df.age.plot.hist()\nplt.show()","2888231b":"pd.crosstab(df.cp, df.target)","9d32bb4b":"pd.crosstab(df.cp, df.target).plot(kind = \"bar\"\n                                  ,figsize=(10,6)\n                                  ,color=[\"salmon\", \"lightblue\"])\nplt.title(\"Heart Disease Frequency per chest pain type\")\nplt.xlabel(\"Chest Pain Type\")\nplt.ylabel(\"Amount\")\nplt.legend([\"No Disease\", \"Disease\"])\nplt.xticks(rotation=0)\nplt.show()","d2039af3":"df.corr()","b08ecd25":"correlation_matrix = df.corr()\n\nfig, ax = plt.subplots(figsize=(15,10))\n\nax = sns.heatmap(correlation_matrix\n             ,annot=True\n             ,linewidth=2\n             ,fmt=\".2f\"\n             ,cmap=\"YlGnBu\")\n\n\nplt.yticks(rotation=0)\nplt.show()","38de2d0a":"df.head()","69e8067e":"X = df.drop(\"target\", axis=1)\ny = df.target","fa24db07":"np.random.seed(42) # To reproduce our results\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","a10e8110":"len(X_train), len(X_test)","1284756f":"len(y_train), len(y_test)","4ec63734":"models = {\n    \"Logistic Regression\":LogisticRegression(solver='liblinear'),\n    \"KNN\":KNeighborsClassifier(),\n    \"Random Forest\":RandomForestClassifier()\n}","0c8cbc3b":"def fit_and_score(models, X_train, X_test, y_train, y_test):\n    np.random.seed(42)\n    \n    model_scores = {}\n    \n    for model_name, model in models.items():\n        model.fit(X_train, y_train)\n        model_scores[model_name] = model.score(X_test, y_test)\n        \n\n    return model_scores","d19e6a37":"model_scores = fit_and_score(models, X_train, X_test, y_train, y_test)","c60f0da7":"# We can see that Logistic Regerssion Worked the best.(eventhough it is not in the scikit learn choosing right estimator map.)\n# Now we will proceed with improving these models by hyperparameter tuning.","11ee9674":"model_scores","1682ca81":"model_comparison = pd.DataFrame(model_scores, index=[\"accuracy\"])\nmodel_comparison = model_comparison.T","68cfddf6":"model_comparison.plot.bar()\nplt.xticks(rotation=0)\nplt.legend()\nplt.show()","529e3c51":"train_scores = []\ntest_scores = []\n\nneighbors = range(1, 21)\nknn = KNeighborsClassifier(n_jobs=-1)\n\nfor i in neighbors:\n    knn.set_params(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    test_scores.append(knn.score(X_test, y_test))\n    train_scores.append(knn.score(X_train, y_train))","eec345c5":"train_scores","37072eb0":"test_scores","a707ad85":"plt.plot(neighbors, train_scores, label=\"Train Score\")\nplt.plot(neighbors, test_scores, label=\"Test Score\")\nplt.xlabel(\"Number Of Neighbors\")\nplt.ylabel(\"KNN Model Score\")\nplt.legend()\nplt.show()","bdb82194":"print(f\"The maximum score achieved was {max(test_scores)*100:.2f}%\")","6144de43":"log_reg_grid = {\n    \"C\":np.logspace(-4, 4, 20),\n    \"solver\":[\"liblinear\"]\n}\n\nrf_grid={\n    \"n_estimators\":np.arange(10,1000,50),\n    \"max_depth\":[None, 3, 5 ,10],\n    \"min_samples_split\":np.arange(2,20,2),\n    \"min_samples_leaf\":np.arange(1,20,2)\n}","107ea0c3":"np.random.seed(42)\nlog_reg_rs = RandomizedSearchCV(estimator=LogisticRegression(),\n                               cv = 5,\n                               verbose = 0,\n                               param_distributions=log_reg_grid)\n\nrf_rs = RandomizedSearchCV(estimator = RandomForestClassifier(),\n                          cv=5,\n                          verbose=0,\n                          param_distributions=rf_grid)","bfda8105":"log_reg_rs.fit(X_train, y_train)\nrf_rs.fit(X_train, y_train)","52489539":"log_reg_rs.best_params_","238a10a2":"rf_rs.best_params_","be672ef1":"log_reg_rs.score(X_test, y_test)","9975c964":"rf_rs.score(X_test, y_test)","9caf2ae8":"gs_log_reg = GridSearchCV(LogisticRegression(), log_reg_grid, cv=5, verbose=2, n_jobs=-1)\ngs_log_reg.fit(X_train, y_train)","dbb3fa8b":"y_preds = gs_log_reg.predict(X_test)","e003b281":"plot_roc_curve(gs_log_reg, X_test, y_test)\nplt.show()","11ced7f0":"print(confusion_matrix(y_test, y_preds))","9832e049":"sns.set(font_scale=1.5)\n\ndef plot_conf_matrix(y_test, y_preds):\n    fig, ax = plt.subplots(figsize=(3,3))\n    ax = sns.heatmap(confusion_matrix(y_test,y_preds),\n                    annot=True,\n                    cbar=False)\n\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"Actual Label\")","fb6babe9":"plot_conf_matrix(y_test, y_preds)","f43b813c":"print(classification_report(y_test, y_preds))","b86a3a84":"gs_log_reg.best_params_","9732bd67":"model = LogisticRegression(C = 0.23357214690901212, solver = 'liblinear')","56fe9b88":"model.fit(X_train, y_train)","b37e2c24":"cv_acc = cross_val_score(model, X, y, cv=5, scoring=\"accuracy\")\ncv_precision = cross_val_score(model, X, y, cv=5, scoring=\"precision\")\ncv_recall = cross_val_score(model, X, y, cv=5, scoring=\"recall\")\ncv_f1 = cross_val_score(model, X, y, cv=5, scoring=\"f1\")\n\ncv_acc = np.mean(cv_acc)\ncv_recall = np.mean(cv_recall)\ncv_precision = np.mean(cv_precision)\ncv_f1 = np.mean(cv_f1)","4366c82f":"df = pd.DataFrame({\"Accuracy\":cv_acc,\n                  \"precision\":cv_precision,\n                  \"recall\":cv_recall,\n                  \"f1-score\":cv_f1},\n                 index=[0])\n\ndf.T.plot.bar(legend=False)\nplt.title(\"Cross Validated Scores\")\nplt.show()","4e102575":"gs_log_reg.best_params_","787282a5":"model = LogisticRegression(C=0.2043359, solver=\"liblinear\")\nmodel.fit(X_train, y_train)\n\nmodel.coef_","e853acb2":"df = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\nfeature_dict = dict(zip(df.columns, list(model.coef_[0])))","4017a13c":"feature_dict","02d06774":"feature_df = pd.DataFrame(feature_dict, index=[0])","fb01330a":"feature_df.T.plot.bar(title=\"Feature Importance\", legend=False)","c74a88db":"\npd.crosstab(df.sex, df.target)","b48930e3":"pd.crosstab(df.slope, df.target)","acfa80a4":"model.coef_[0]","987ac5bc":"model.coef_[0]","1f2dbc5c":"## Preparing Our Data for ML.","3194ee0a":"Here we can see that, if the sample is women there is 75% chances that she has heart disease. Similarly, we can say that if the sample is man there is 50% chances that he has heart disease. We can also say that for a sample there is 60% chance (average of 50% and 70%) we will use this as baseline and what we want to achieve is the accuracy of the model to be atleast above 60%.","5d95f282":"The max score obtained even after hyperparameter tuning is till less than the other two competitors model, so we will drop KNN.","05509c4a":"### Feature Importance","ebb72c0c":"## Exploring The Patterns in the Data","09971976":"## Preparing the tools\n\nWe're going to use Pandas, Numpy and Matplotlib for data analysis and data manipulation.","093e1de9":"## Load Data and Initial Checks.","63d5576f":"# Predicting Heart Disease Using Machine Learning \n\nThis notebook looks into using various python-based machine learning and data science libraries in an attempt to build a machine learning model capable of predicting whether or not someone has heart disease based on their medical attributes.\n\nWe're going to take the following approach:\n1. Problem Definition.\n2. Data.\n3. Evaluation.\n4. Features.\n5. Modelling.\n6. Experimentation.\n\n## 1. Problem Definition\n\n> In a statement, Given clinical parameters about of patient can we predict whether or not they have heart disease?\n\n## 2. Data\n\nThe original data came from the cleaveland data from the UCI machine learning repository.\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/heart+disease\n\nThere is also a version available on kaggle. https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\n\n## 3. Evaluation\n\n> If we can reach 95% accuracy at predicting whether or not a patient has heart disease during the proof of concept, we'll pursue the project.\n\n## 4.Features\n\n** Data Dictionary **\n* 1. age - age in years\n* 2. sex - (1 = male; 0 = female)\n* 3. cp - chest pain type\n        0: Typical angina: chest pain related decrease blood supply to the heart\n        1: Atypical angina: chest pain not related to heart\n        2: Non-anginal pain: typically esophageal spasms (non heart related)\n        3: Asymptomatic: chest pain not showing signs of disease\n* 4. trestbps - resting blood pressure (in mm Hg on admission to the hospital) anything above 130-140 is typically cause for concern\n* 5. chol - serum cholestoral in mg\/dl\n* 6. serum = LDL + HDL + .2 * triglycerides\n        above 200 is cause for concern\n        fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n        '>126' mg\/dL signals diabetes.\n* 7. restecg - resting electrocardiographic results\n        0: Nothing to note.\n        1: ST-T Wave abnormality,\n        can range from mild symptoms to severe problems\n        signals non-normal heart beat\n        2: Possible or definite left ventricular hypertrophy\n        Enlarged heart's main pumping chamber\n* 8. thalach - maximum heart rate achieved\n* 9. exang - exercise induced angina (1 = yes; 0 = no)\n* 10. oldpeak - ST depression induced by exercise relative to rest looks at stress of heart during excercise unhealthy heart will stress more\n* 11. slope - the slope of the peak exercise ST segment\n        0: Upsloping: better heart rate with excercise (uncommon)\n        1: Flatsloping: minimal change (typical healthy heart)\n        2: Downslopins: signs of unhealthy heart\n* 12. ca - number of major vessels (0-3) colored by flourosopy\n        colored vessel means the doctor can see the blood passing through\n        the more blood movement the better (no clots)\n* 13. thal - thalium stress result\n        1,3: normal\n        6: fixed defect: used to be defect but ok now\n        7: reversable defect: no proper blood movement when excercising\n* 14. target - have disease or not (1=yes, 0=no) (= the predicted attribute)","5b3378d5":"## Data Exploration (Exploratory Data Analysis or EDA)","a981904a":"### Our Focus \n- Hyperparameter tuning\n- Feature Importance \n- Confusion Matrix\n- Cross Validation\n- Precision\n- Recall\n- F1 Score\n- Classification Report\n- ROC Curve\n- Area under ROC Curve (AUC)","f4e729f7":"### Model Comparison"}}