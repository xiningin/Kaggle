{"cell_type":{"22f9cb3c":"code","f0f2ae0d":"code","25aaef06":"code","b1c74c80":"code","c9cd6231":"code","efe44996":"code","2e6af187":"code","23e9d5b7":"code","69453952":"code","97d4dedc":"code","6e62c4c3":"code","89538b56":"code","7af30cbd":"code","89148e1d":"code","0faa3d34":"code","4d6b0784":"code","f6311c98":"code","cf3cc041":"code","5ce8fc86":"code","36cbaa89":"code","e1f710f5":"code","389ed2e6":"code","c1affd05":"code","10391889":"code","a0e1d5b1":"code","fcd0ec82":"code","02480bb2":"code","d26a0bda":"code","d6d776f0":"code","f80b4f73":"code","ea481162":"code","7820f760":"code","4bd78a70":"markdown","929e050b":"markdown","f6119992":"markdown","2dadc1da":"markdown","27c07336":"markdown","0bc86e7e":"markdown","069c5a5c":"markdown","2aaa1ff6":"markdown","a34197fb":"markdown","8da27828":"markdown","4cf48813":"markdown","44266409":"markdown","1e4a2a75":"markdown","cc8f2c99":"markdown","25847c2b":"markdown","b4288941":"markdown","c149e822":"markdown","7e6f715b":"markdown","6defd691":"markdown","d5bd4609":"markdown","07f735e9":"markdown","7b7bbee0":"markdown","25041995":"markdown","5c45588d":"markdown","9cee6229":"markdown","54bca8bb":"markdown","83b2b216":"markdown","89c302b3":"markdown"},"source":{"22f9cb3c":"# unselect the following codes and run them\n# %conda install pandas\n# %conda install numpy\n# %conda install seaborn\n# %conda install -c anaconda scikit-learn\n# %conda install -c districtdatalabs yellowbrick","f0f2ae0d":"# pip install pandas\n# pip install numpy\n# pip install seaborn\n# pip install scikit-learn\n# pip install yellowbrick","25aaef06":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import datasets\n%config InlineBackend.figure_format = 'retina'","b1c74c80":"# Loading the dataset\niris = datasets.load_iris()\nX,y = iris.data,iris.target","c9cd6231":"X[:5],y[:5]","efe44996":"len(X),len(y)","2e6af187":"# converting dataset to a pandas dataframe\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf['target'] = iris['target']","23e9d5b7":"df","69453952":"# print(df.to_string())","97d4dedc":"# pd.set_option('display.height',1000)\n# pd.set_option('display.max_rows',500)\n# pd.set_option('display.max_columns',500)\n# pd.set_option('display.width',1000)","6e62c4c3":"df.describe()","89538b56":"# df['target'].value_counts()\nsns.countplot(df['target']);","7af30cbd":"sns.boxplot(y=df['sepal length (cm)'],x=df['target']);","89148e1d":"df.corr()","0faa3d34":"sns.heatmap(df.corr().abs());","4d6b0784":"df.corrwith(df[\"target\"]).plot(kind='barh');\n# df.corr().abs()['target']","f6311c98":"df.isna().sum()","cf3cc041":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n\nprint(len(df))\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)\n\nprint(len(df) - len(X_train))\nprint(len(df) - len(X_test))","5ce8fc86":"from sklearn.tree import DecisionTreeClassifier, export_graphviz\nclf = DecisionTreeClassifier().fit(X_train,y_train)\nclf","36cbaa89":"import graphviz\ndtree_viz = export_graphviz(clf, out_file=None,\n                            feature_names = df.drop('target', axis=1).columns,\n                             filled=True, rounded=True,class_names=['0','1','2'],\n                             special_characters=True,\n                             impurity=True,proportion=True,\n                            node_ids=True)\n# Draw graph\ngraph = graphviz.Source(dtree_viz)\ngraph","e1f710f5":"print('Accuracy of classifier on training set: {:.2f}'\n     .format(clf.score(X_train, y_train)))\nprint('Accuracy of classifier on test set: {:.2f}'\n     .format(clf.score(X_test, y_test)))","389ed2e6":"X_test[1],y_test[1]","c1affd05":"from sklearn.metrics import confusion_matrix\nclf.predict(X_test)","10391889":"y_test","a0e1d5b1":"clf.predict(X_test) - y_test","fcd0ec82":"confusion_matrix(y_test,clf.predict(X_test))","02480bb2":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,clf.predict(X_test))","d26a0bda":"# define one new instance\nXnew_input = [[6.7,3.0,5.2,2.3]]\n# make a prediction\nynew_predict = clf.predict(Xnew_input)\nprint(\"X=%s, Predicted=%s\" % (Xnew_input[0], ynew_predict[0]))\n# https:\/\/machinelearningmastery.com\/make-predictions-scikit-learn\/","d6d776f0":"df.tail()","f80b4f73":"import warnings\nwarnings.filterwarnings('ignore')\nfrom yellowbrick.contrib.classifier import DecisionViz\nviz = DecisionViz(clf, title=\"Dtree\",\n                  features=['sepal length (cm)', 'petal length (cm)'],\n                  classes=['0', '1','2'])\nviz.fit(X_train[:, [0, 1]], y_train)\nviz.draw(X_test[:, [0, 1]], y_test)\nplt.title('Decision Boundary')\nviz.show();","ea481162":"from yellowbrick.model_selection import LearningCurve\nvisualizer = LearningCurve(clf, scoring=\"accuracy\")\nvisualizer.fit(X,y)\nvisualizer.show();","7820f760":"from sklearn.cluster import KMeans\nfrom mpl_toolkits.mplot3d import Axes3D\nkm = KMeans(n_clusters=3,verbose=False).fit(X)\nkm.predict(X)\nprint(km)\nlabels = km.labels_\nfig = plt.figure(figsize=(7,7))\nax = Axes3D(fig, rect=[0, 0, 0.95, 1], elev=30,azim=130)\nax.scatter(X[:, 3], X[:, 0], X[:, 2],c=labels.astype(np.float), edgecolor=\"r\",cmap='viridis')\nax.set_xlabel(\"Petal width\")\nax.set_ylabel(\"Sepal length\")\nax.set_zlabel(\"Petal length\")\nplt.title(\"K Means by 3 clusters\\n\", fontsize=14);\nfig = plt.figure(figsize=(7,7))\nax = fig.add_subplot(1, 2, 2, projection='3d')\nax = Axes3D(fig, rect=[0, 0, 0.95, 1], elev=30,azim=130)\nax.scatter(X[:, 3], X[:, 0], X[:, 2],c=y, edgecolor=\"r\",cmap='viridis')\nax.set_xlabel(\"Petal width\")\nax.set_ylabel(\"Sepal length\")\nax.set_zlabel(\"Petal length\")\nplt.title(\"Ground Truth\\n\", fontsize=14)\nplt.show();","4bd78a70":"### DECISION TREES\n\nIn Decision Trees. All the data automatically divided to yes\/no questions. They could sound a bit weird from a human perspective, e.g., whether the creditor earns more than $128.12? Though, the machine comes up with such questions to split the data best at each step. The higher the branch \u2014 the broader the question. \n\n\nDecision trees are widely used in high responsibility spheres: diagnostics, medicine, and finances.\n\n\n<img src=\"https:\/\/i.vas3k.ru\/7w3.jpg\" width=600 height=600 \/>\n\n* source https:\/\/vas3k.com\/blog\/machine_learning\/#scroll70\n\n","929e050b":"## DIFFRENCE BETWEEN ML AND TRADITIONAL PROGRAMMING\n\n### TRADITIONAL PROGRAMMING\n\n\nprogramming is about giving instructions to the computer,where most of the code would be a big collection of if..else statements, mixed with imperative commands such as shoot().\n\n \n<img src=\"https:\/\/miro.medium.com\/max\/1400\/1*NdaEsVLBnp-64HR14LQFiA.png\" width=600 height=500 \/>\n\n\n### MACHINE LEARNING\n\n\nBy contrast, machine learning is about giving data to the computer, and asking it to figure out what to do.\nThe code in machine learning tells the computer how to crunch the data, so that the computer can solve the problem by itself.\n\n<img src=\"https:\/\/miro.medium.com\/max\/1400\/1*Sn2zHsHVR6hREmS8dV6fQA.png\" width=600 height=500 \/>\n\n\n* source https:\/\/medium.com\/swlh\/this-is-machine-learning-part-1-learning-vs-coding-789343df1e30","f6119992":"# UNSUPERVISED LEARNING\n## CLUSTERING\n\n\nClustering is a classification with no predefined classes. It\u2019s like dividing socks by color when you don't remember all the colors you have. Clustering algorithm trying to find similar (by some features) objects and merge them in a cluster. Those who have lots of similar features are joined in one class. With some algorithms, you even can specify the exact number of clusters you want. Searching for the centroids is convenient. Though, in real life clusters not always circles. Let's imagine you're a geologist. And you need to find some similar minerals on the map. In that case, the clusters can be weirdly shaped and even nested. Also, you don't even know how many of them to expect. 10? 100?\n\n* source https:\/\/vas3k.com\/blog\/machine_learning\/#scroll100\n\n<img src= https:\/\/i.vas3k.ru\/7w6.jpg width=600 height=600\/>","2dadc1da":"## ML ROADMAP\n\n\n\n\n<img src=\"https:\/\/developer.nvidia.com\/sites\/default\/files\/deep-learning-evolution.png\" width=600 height=500 \/>\n\n* source https:\/\/developer.nvidia.com\/deep-learning\n\n\n### GOOLGE ML TIMELINE\n\n\n<img src=\"https:\/\/storage.googleapis.com\/buildcms.google.com.a.appspot.com\/images\/Build-History-ML-Header-Final.height-380.png\" width=600 height=500 \/>\n\n\n\n\n* source https:\/\/cloud.withgoogle.com\/build\/data-analytics\/explore-history-machine-learning\/\n\n\n### ML RELATED SKILLS\n\n<img src=\"https:\/\/images.squarespace-cdn.com\/content\/v1\/5150aec6e4b0e340ec52710a\/1364352051365-HZAS3CLBF7ABLE3F5OBY\/ke17ZwdGBToddI8pDm48kB2M2-8_3EzuSSXvzQBRsa1Zw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpxPe_8B-x4gq2tfVez1FwLYYZXud0o-3jV-FAs7tmkMHY-a7GzQZKbHRGZboWC-fOc\/Data_Science_VD.png\" width=500 height=500 \/>\n\n* source https:\/\/s3.amazonaws.com\/aws.drewconway.com\/viz\/venn_diagram\/data_science.html\n","27c07336":"## EDA\n\n```Pandas``` module has a defualt restrictions in displaying the maximum amount of rows and columns, thats can be passed by the following:\n\n-  print the dataframe as a string:\n-  change the jupyternotebook display setting to a predfined settings","0bc86e7e":"---\n### USE CASES OF ML ALGORITHMS\n<img src=\"https:\/\/i.vas3k.ru\/7w1.jpg\" width=600 height=300 \/>\n\n* source https:\/\/vas3k.com\/blog\/machine_learning\/#scroll50","069c5a5c":"<img src=\"https:\/\/www.anaconda.com\/imager\/assetsdo\/Products\/8031\/open-source-logos2x_680db6b6f11f9cc710dd7defae241cd3.png\" width=600 height=600 \/>","2aaa1ff6":"### CLASSIFICATION METRICS\n\n### CONFUSION METRICS\nConfusion matrix The confusion matrix is used to have a more complete picture when assessing the performance of a model. It is defined as follows:\n\n<img src=\"https:\/\/cs230.stanford.edu\/doks-theme\/assets\/images\/section\/9\/confusion.png\" width=600 height=600 \/>\n\n\n\n","a34197fb":"## IMPORTING MODULES","8da27828":"## LOAD DATASEST\n\n### ABOUT THE DATASET:\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken from Fisher\u2019s paper. Note that it\u2019s the same as in R, but not as in the UCI Machine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the pattern recognition literature. Fisher\u2019s paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.\n\n---\n**Data Set Characteristics:**\n\n**Number of Instances:** \n\n150 (50 in each of three classes)\n\n**Number of Attributes:**\n\n4 numeric, predictive attributes and the class\n\n**Attribute Information:**\n\nsepal length in cm\n\nsepal width in cm\n\npetal length in cm\n\npetal width in cm\n\n**Class:**\n\nIris-Setosa\n\nIris-Versicolour\n\nIris-Virginica\n\n---\n\nAdditional information about the datasest: https:\/\/en.wikipedia.org\/wiki\/Iris_flower_data_set and, https:\/\/scikit-learn.org\/stable\/datasets\/index.html#iris-plants-dataset","4cf48813":"# WHAT IS ML\n## DEFINITON\n\n- In 1959, Arthur Samuel defined machine learning as a \"Field of study that gives computers the ability to learn without being explicitly programmed\".\n\n\n- A machine learning algorithm is an algorithm that is able to learn from data. But what do we mean by learning? Mitchell (1997) provides a succinct de\ufb01nition:\u201cA computer program is said to learn from experience **E** with respect to someclass of tasks **T** and performance measure **P**, if its performance at tasks in **T**, asmeasured by **P**, improves with experience **E**.\u201d\n\n\n>- Task T: driving on public four-lane highways using vision sensors\n>- Performance measure P: average distance traveled before an error (as judged by human oversteer)\n>- Training experience E: a sequence of images and steering commands recorded while observing a human drive","44266409":"# PRACTICAL PART\n<img src=\"https:\/\/media1.tenor.com\/images\/6d1b2a464d8e6e696622c2afc10143e4\/tenor.gif\" width=400 height=400\/>\n\n","1e4a2a75":"## MODELNG\n\n","cc8f2c99":"$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$ = $\\frac{16+22+19}{16+22+19+2+1}$ = 95","25847c2b":"##### USING CONDA TERMINAL","b4288941":"# SUPERVISED LEARNING","c149e822":"# ROADMAP TO SOLVE A PROBLEM\n\nThe flowchart below is designed to give users a bit of a rough guide on how to approach problems with regard to which estimators to try on your data.\n\n\n<img src=\"https:\/\/scikit-learn.org\/stable\/_static\/ml_map.png\" width=800 height=800 \/>\n\n\n\n\n\n* source: https:\/\/scikit-learn.org\/stable\/tutorial\/machine_learning_map\/index.html#\n\n","7e6f715b":"# REFRENCES\n\n## BOOKS\n\n<img src=\"http:\/\/faculty.marshall.usc.edu\/gareth-james\/ISL\/ISL%20Cover%202.jpg\" width=200 height=200\/>\n\n* source http:\/\/faculty.marshall.usc.edu\/gareth-james\/ISL\/ISLR%20Seventh%20Printing.pdf\n\n>-This course is made by the authors who wrote the book: https:\/\/online.stanford.edu\/courses\/sohs-ystatslearning-statistical-learning\n\n\n<img src=\"https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/figures\/PDSH-cover.png\" width=200 height=200\/>\n\n* source https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/index.html\n\n<img src=\"http:\/\/people.cs.ubc.ca\/~murphyk\/MLbook\/cover-lowres.jpg\" width=200 height=200\/>\n\n* source https:\/\/www.cs.ubc.ca\/~murphyk\/MLbook\/\n\n---\n\nThe Mechanics of Machine Learning book by Terence Parr and Jeremy Howard: https:\/\/mlbook.explained.ai\n\n\n## INRESTING LINKS\n\nVasily Zubarev Blog: https:\/\/vas3k.com\/blog\/machine_learning\/\n\nChris Albon Blog: https:\/\/chrisalbon.com\n\nMachine learning open course from OpenDataScience Community: https:\/\/mlcourse.ai\n\nGoogle machine learning crash course: https:\/\/developers.google.com\/machine-learning\/crash-course\n\nKaggle courses: https:\/\/www.kaggle.com\/learn\/overview\n\nStatQuest with Josh Starmer Youtube Channel: https:\/\/www.youtube.com\/user\/joshstarmer\n\nDataSchool Youtube Channel: https:\/\/www.youtube.com\/user\/dataschool\n\nAndrew NG Machine learning course: https:\/\/www.coursera.org\/learn\/machine-learning\n\nCS229 machine learning cheat sheet: https:\/\/github.com\/afshinea\/stanford-cs-229-machine-learning\n\nCS229 course notes: http:\/\/cs229.stanford.edu\/notes\/\n\n### YOU CAN ACCESS MY OTHER TUTORIALS\n\nhttps:\/\/www.kaggle.com\/ahm6644\/mushroom-classification-by-several-algorithms\n\nhttps:\/\/www.kaggle.com\/ahm6644\/movies-recommendations-by-association-rules\n","6defd691":"###  TRAIN\/TEST SPLIT\n\nOnce the model has been chosen, it is trained on the entire dataset and tested on the unseen test set.\n\n<img src=\"https:\/\/stanford.edu\/~shervine\/teaching\/cs-229\/illustrations\/train-val-test-en.png?0949795ac868562e193efdc249ae1066\" width=600 height=600 \/>\n\n* source https:\/\/stanford.edu\/~shervine\/teaching\/cs-229\/cheatsheet-machine-learning-tips-and-tricks\n\n","d5bd4609":"##### LETS COMPARE BETWEEN THE PREDICTED RESULTS AND THE TRUE RESULTS","07f735e9":"---\nLETS START\n---","7b7bbee0":"\n## JUPYTER NOTEBOOK (CODING ENVIROMENT)\n\nThe Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more.\n\n**Basic Jupyter interface**\n\n\n<img src= \"https:\/\/jupyter-notebook.readthedocs.io\/en\/stable\/_images\/blank-notebook-ui.png\" width=800 height=800\/>\n\nTry it on your browser by: https:\/\/mybinder.org\/v2\/gh\/ipython\/ipython-in-depth\/master?filepath=binder\/Index.ipynb\n\n\n* source https:\/\/jupyter-notebook.readthedocs.io\/en\/stable\/notebook.html\n","25041995":"# INTRO TO ML\n\n---\n\nThis workshop represents an introductory session on the subject of machine learning. The session will include basic definitions of machine learning, types of machine learning. use cases, applications, problem-solving framework, machine learning related software libraries. The practical part of the session will demonstrate solving a related machine learning problem using jupyter notebook and python programming.\n\n---","5c45588d":"## TOOLBOX (SOFTWARES & LIBRARIES)","9cee6229":"## TYPES OF ML\n\n### SUPERVISED LEARNING\n\n<!-- Supervised learning involves somehow modeling the relationship between measured features of data and some label associated with the data; once this model is determined, it can be used to apply labels to new, unknown data. This is further subdivided into classification tasks and regression tasks: in classification, the labels are discrete categories, while in regression, the labels are continuous quantities.\n -->\n\nSupervised learning techniques are used  when there is prior knowledge to the correct output (Y) and the goal here is to find the relation between the input x and the output y, having a new input x that need to map or predict the new output Y. \n\n\n- Classification: Models that predict labels as two or more discrete categories\n\n\n- Regression: Models that predict continuous labels\n\n\n|  \t| Regression \t| Classifier \t|\n|-\t|-\t|-\t|\n| Outcome \t| Continuous \t| Class \t|\n| Examples \t| Linear regression \t| Logistic regression, SVM, Naive Bayes \t|\n\n\n<!-- <img src=\"https:\/\/stanford.edu\/~shervine\/teaching\/cs-229\/illustrations\/discriminative-model.png?76e3b1844bdd4d887dd8abb3e14e55af\" width=200 height=200 \/>\n\n<img src=\"https:\/\/stanford.edu\/~shervine\/teaching\/cs-229\/illustrations\/generative-model.png?9450159832018fcf8f8ff47ae08a2d37\" width=200 height=200 \/>\n\n -->\n\n\n\n### UNSUPERVISED LEARNING\n\n\n<!-- Unsupervised learning involves modeling the features of a dataset without reference to any label, and is often described as \"letting the dataset speak for itself.\" These models include tasks such as clustering and dimensionality reduction. Clustering algorithms identify distinct groups of data, while dimensionality reduction algorithms search for more succinct representations of the data. We will see examples of both types of unsupervised learning in the following section. -->\n\n\nUnsupervised learning is a type of machine learning algorithm that is used to deal with input (X) data without the existence of any labeled response or output (Y). most common methods are the partitioning method or cluster analysis (which is deal mainly with distance measures) which is used to explore the data to find any hidden pattern patterns that can be used to group the data with similar features into clusters \n\n\n- Clustering: Models that detect and identify distinct groups in the data\n\n<!-- <img src=\"https:\/\/stanford.edu\/~shervine\/teaching\/cs-229\/illustrations\/k-means-en.png?9925605d814ddadebcae2ae4754ab0a4\" width=600 height=300 \/>\n\n -->","54bca8bb":"## EVALUATION","83b2b216":"### INSTALL THE FOLLWING MODULES IF YOU STILL DID NOT","89c302b3":"##### USING PIP PACKAGE INSTALLER"}}