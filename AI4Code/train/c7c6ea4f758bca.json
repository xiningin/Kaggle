{"cell_type":{"0e078892":"code","f071ae58":"code","0e96d194":"code","53b01405":"code","13a8dcea":"code","36a2cfc9":"code","a52cf24f":"code","734e271f":"code","74c512a7":"code","c06db9d5":"code","1dd41708":"code","43351b79":"code","1d1b4de7":"code","71ffcf24":"code","5f1f4264":"code","8235d98c":"code","248f0322":"code","e7227061":"code","0511eed6":"code","d32eb192":"code","449a30c7":"code","28446ad4":"code","d6ddaacf":"code","9662f24f":"code","358469c5":"code","0f7d63bf":"code","20b7a6b4":"code","0c1d3099":"code","49c25524":"code","4400cc7f":"code","7c3d5bb7":"code","6e7678d2":"code","e4a9727e":"code","ebf1f2e4":"code","bcfa7fff":"code","094e3958":"code","888d6c43":"code","2ea495ce":"code","89bd2008":"code","d75ede0f":"code","d335b54e":"code","f0c0b7c9":"code","2a6391cb":"code","0cc4b25b":"code","5a9fdaaf":"code","8fae9865":"markdown","402ac341":"markdown","5aee95bf":"markdown","eeb15de8":"markdown","0f8723ef":"markdown","28e8f658":"markdown","e0a18e27":"markdown","e27ea827":"markdown","e622fad9":"markdown","96c031a4":"markdown","6e2b84d7":"markdown","b7e9079b":"markdown","7b3a8597":"markdown","c8429c10":"markdown","7a00a4a1":"markdown","84d16fc7":"markdown","0eb918df":"markdown","aaadcda9":"markdown","e6a9b582":"markdown","a5deb0dd":"markdown","55bfe8d1":"markdown","c3d2351b":"markdown","6e3d5536":"markdown","6df0dadb":"markdown","f7e45023":"markdown","4a49283c":"markdown","2ebd7dc7":"markdown"},"source":{"0e078892":"!pip install --upgrade numpy pandas scikit-learn tensorflow Keras matplotlib seaborn opencv-python","f071ae58":"!pip install --upgrade machine-learning-datasets\n!pip install --upgrade tf-explain tf-keras-vis","0e96d194":"import math\nimport os\nimport machine_learning_datasets as mldatasets\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing, metrics\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport seaborn as sns\nfrom tensorflow import keras\nimport cv2\nfrom keras.utils.data_utils import get_file\nfrom tf_explain.core.activations import ExtractActivations\nfrom tf_keras_vis.activation_maximization import ActivationMaximization\nfrom tf_keras_vis.saliency import Saliency\nfrom tf_keras_vis.utils import normalize\nfrom tf_keras_vis.gradcam import GradcamPlusPlus\nfrom tf_explain.core.integrated_gradients import IntegratedGradients","53b01405":"print(tf.__version__)","13a8dcea":"X_train, X_test, X_val, X_val_orig, y_train, y_test, y_val, y_val_orig =\\\n                                    mldatasets.load(\"fruits-360\", prepare=True)","36a2cfc9":"print('X_train:\\t%s' % (X_train.shape,))\nprint('X_test:\\t\\t%s' % (X_test.shape,))\nprint('X_val:\\t\\t%s' % (X_val.shape,))\nprint('X_val_orig:\\t%s' % (X_val_orig.shape,))\nprint('y_train:\\t%s' % (y_train.shape,))\nprint('y_test:\\t\\t%s' % (y_test.shape,))\nprint('y_val:\\t\\t%s' % (y_val.shape,))\nprint('y_val_orig:\\t%s' % (y_val_orig.shape,))","a52cf24f":"X_train = X_train.astype('float32')\/255\nX_test = X_test.astype('float32')\/255\nX_val = X_val.astype('float32')\/255","734e271f":"ohe = preprocessing.OneHotEncoder(sparse=False)\nohe.fit(y_train)\nfruits_l = ohe.categories_[0].tolist()\nprint(fruits_l)","74c512a7":"rand = 9\nos.environ['PYTHONHASHSEED']=str(rand)\nnp.random.seed(rand)\ntf.random.set_seed(rand)","c06db9d5":"plt.subplots(figsize=(10,10))\nfor f, fruit in zip([*range(len(fruits_l))], fruits_l):\n    plt.subplot(4, 4, f+1)\n    plt.title(fruits_l[f], fontsize=12)\n    idx = np.random.choice(np.where(y_test[:,0] == fruit)[0], 1)[0]\n    plt.imshow(X_test[idx], interpolation='spline16')\n    plt.axis(\"off\")\nplt.show()","1dd41708":"plt.subplots(figsize=(10,10))\nfor f, fruit in zip([*range(len(fruits_l))], fruits_l):\n    plt.subplot(4, 4, f+1)\n    plt.title(fruits_l[f], fontsize=12)\n    idx = np.random.choice(np.where(y_val[:,0] == fruit)[0], 1)[0]\n    plt.imshow(X_val[idx], interpolation='spline16')\n    plt.axis(\"off\")\nplt.show()","43351b79":"model_path = get_file('CNN_fruits_final.hdf5',\\\n            'https:\/\/github.com\/VaibhavSxn\/cnn_model_fruits\/blob\/main\/CNN_fruits_final.hdf5?raw=true')\ncnn_fruits_mdl = keras.models.load_model(model_path)\ncnn_fruits_mdl.summary()","1d1b4de7":"train_score = cnn_fruits_mdl.evaluate(X_train, ohe.transform(y_train),\\\n                                     verbose=0)\ntest_score = cnn_fruits_mdl.evaluate(X_test, ohe.transform(y_test),\\\n                                     verbose=0)\nval_score = cnn_fruits_mdl.evaluate(X_val, ohe.transform(y_val),\\\n                                     verbose=0)\nprint('Train accuracy:\\t{:.1%}'.format(train_score[1]))\nprint('Test accuracy:\\t{:.1%}'.format(test_score[1]))\nprint('Val accuracy:\\t{:.1%}'.format(val_score[1]))","71ffcf24":"orig_plt_params = plt.rcParams\nsns.set()\ny_test_pred, y_test_prob =\\\n            mldatasets.evaluate_multiclass_mdl(cnn_fruits_mdl, X_test, y_test,\\\n                            fruits_l, ohe, plot_roc=False)","5f1f4264":"y_val_pred, y_val_prob =\\\n            mldatasets.evaluate_multiclass_mdl(cnn_fruits_mdl, X_val, y_val, fruits_l,\\\n                                    ohe, plot_roc=True, plot_roc_class=False,\\\n                                    pct_matrix=False)","8235d98c":"preds_df = pd.DataFrame({'y_true':y_val[:,0], 'y_pred':y_val_pred})\n\nprobs_df = pd.DataFrame(y_val_prob*100).round(1)\nprobs_df.loc['Total']= probs_df.sum().round(1)\nprobs_df.columns = fruits_l\nprobs_df = probs_df.sort_values('Total', axis=1, ascending=False)\nprobs_df.drop(['Total'], axis=0, inplace=True)\nprobs_final_df = probs_df.iloc[:,0:8]\n\npreds_probs_df = pd.concat([preds_df, probs_final_df], axis=1)\npd.set_option('precision', 1)\npreds_probs_df.style.apply(lambda x: ['background: lightgreen' if (x[0] == x[1])\\\n                     else '' for i in x], axis=1).\\\n    apply(lambda x: ['background: orange' if (x[0] != x[1] and x[1] == 'Grapefruit Pink')\\\n                     else '' for i in x], axis=1).\\\n    apply(lambda x: ['background: yellow' if (x[0] != x[1] and x[0] == 'Avocado')\\\n                     else '' for i in x], axis=1).\\\n    apply(lambda x: ['font-weight: bold' if isinstance(i, float) and i >= 50\\\n                     else '' for i in x], axis=1).\\\n    apply(lambda x: ['color:transparent' if i == 0.0\\\n                     else '' for i in x], axis=1)","248f0322":"avocado_FN_idxs = preds_df[(preds_df['y_true'] != preds_df['y_pred']) &\\\n                           (preds_df['y_true'] == 'Avocado')].index.to_list()\navocado_TP_idxs = preds_df[(preds_df['y_true'] == preds_df['y_pred']) &\\\n                           (preds_df['y_true'] == 'Avocado')].index.to_list()\ngrapefruit_FP_idxs = preds_df[(preds_df['y_true'] != preds_df['y_pred']) &\\\n                              (preds_df['y_pred'] == 'Grapefruit Pink')].index.to_list()\ngrapefruit_TP_idxs = preds_df[(preds_df['y_true'] == preds_df['y_pred']) &\\\n                              (preds_df['y_pred'] == 'Grapefruit Pink')].index.to_list()","e7227061":"target_layers = ['conv2d_1', 'conv2d_2', 'conv2d_3', 'conv2d_4']\nexplainer = ExtractActivations()","0511eed6":"for target_layer in target_layers:\n    for idx in avocado_TP_idxs:\n        orig_img = X_val_orig[idx]\n        viz_img = explainer.explain((np.array([X_val[idx]]), None),\\\n                                    cnn_fruits_mdl, target_layer)\n        y_true = y_val[idx,0]\n        y_pred = y_val_pred[idx]\n        probs_s = probs_df.loc[idx]\n        title = '{} Activations for Avocado #{}'.format(target_layer, idx)\n        mldatasets.compare_img_pred_viz(orig_img, viz_img, y_true,\\\n                                        y_pred, probs_s, title=title)","d32eb192":"for target_layer in target_layers:\n    for idx in grapefruit_TP_idxs:\n        orig_img = X_val_orig[idx]\n        viz_img = explainer.explain((np.array([X_val[idx]]), None),\\\n                                    cnn_fruits_mdl, target_layer)\n        #viz_img = np.uint8(cm.jet(viz_img)[..., :3] * 255)\n        y_true = y_val[idx,0]\n        y_pred = y_val_pred[idx]\n        probs_s = probs_df.loc[idx]\n        title = '{} Activations for Grapefruit #{}'.format(target_layer, idx)\n        mldatasets.compare_img_pred_viz(orig_img, viz_img, y_true,\\\n                                        y_pred, probs_s, title=title)","449a30c7":"def model_modifier(mdl):\n    global target_layer\n    target = mdl.get_layer(name=target_layer)\n    new_mdl = tf.keras.Model(inputs=mdl.inputs,\\\n                             outputs=target.output)\n    new_mdl.layers[-1].activation = tf.keras.activations.linear\n    return new_mdl\n\ndef loss(output):\n    global filter_num\n    return output[..., filter_num]","28446ad4":"#How many filters to plot Activation Maximization for in each layer\nnum_filters = 16\n\n#Compute size (width or height) of image size based on num_filters\ngridsize = math.ceil(math.sqrt(num_filters))\n\n#Iterate each target layer and..\nfor target_layer in target_layers:\n    \n    #Randomly select index of filters from total amount in layer\n    for layer in cnn_fruits_mdl.layers:\n        if layer.name == target_layer:\n            total_filters = layer.filters\n    if total_filters == num_filters or total_filters < num_filters:\n        filter_num_l = [*range(num_filters)]\n    else:\n        filter_num_l = list(np.random.choice([*range(total_filters)],\\\n                                             num_filters))\n    \n    #Compute and Plot Activation Maximization for each random filter\n    fig = plt.figure(figsize=(10,10))\n    for f, filter_num in zip([*range(len(filter_num_l))], filter_num_l):\n        plt.subplot(gridsize, gridsize, f+1)\n        plt.title('Filter #{}'.format(filter_num), fontsize=12)\n        activation_maximization = ActivationMaximization(cnn_fruits_mdl,\\\n                                            model_modifier, clone=True)\n        activation = activation_maximization(loss)\n        plt.imshow(activation[0].astype(np.uint8), interpolation='spline16')\n        plt.axis(\"off\")\n\n    fig.suptitle('{} Layer'.format(target_layer), fontsize=18, weight='bold')\n    plt.subplots_adjust(bottom=0, top=0.92)\n    plt.show()","d6ddaacf":"idxs = avocado_FN_idxs + grapefruit_FP_idxs\nX_misclass = X_val[idxs]\nprint(X_misclass.shape)","9662f24f":"enc = preprocessing.OrdinalEncoder()\nenc.fit(y_train)\ny_val_pred_exp = np.expand_dims(np.array(y_val_pred),axis=1)\ny_val_pred_enc = enc.transform(y_val_pred_exp)\nlabels_l = y_val_pred_enc[idxs].squeeze().\\\n                                    astype(int).tolist()\nprint(labels_l)","358469c5":"def model_modifier(mdl):\n    mdl.layers[-1].activation = tf.keras.activations.linear\n    return mdl\n\ndef loss(output):\n    global labels_l\n    pos_l = [*range(len(labels_l))]\n    output_l = []\n    for p, l in zip(pos_l, labels_l):\n        output_l.append(output[p][l])\n    return tuple(output_l)","0f7d63bf":"saliency = Saliency(cnn_fruits_mdl,\n                    model_modifier=model_modifier,\n                    clone=True)\n\nsaliency_maps = saliency(loss, X_misclass)\nsaliency_maps = normalize(saliency_maps)\n\nsmoothgrad_saliency_maps = saliency(loss, X_misclass,\\\n                                smooth_samples=20,\n                                smooth_noise=0.20)\nsmoothgrad_saliency_maps =\\\n                    normalize(smoothgrad_saliency_maps)","20b7a6b4":"plt.subplots(figsize=(15,5))\nplt.subplot(1, 3, 1)\nplt.imshow(X_misclass[0])\nplt.grid(b=None)\nplt.title(\"Original Image\")\nplt.subplot(1, 3, 2)\nplt.imshow(saliency_maps[0], cmap='jet')\nplt.grid(b=None)\nplt.title(\"Vanilla Saliency Map\")\nplt.subplot(1, 3, 3)\nplt.imshow(smoothgrad_saliency_maps[0], cmap='jet')\nplt.grid(b=None)\nplt.title(\"SmoothGrad Saliency Map\")\nplt.show()","0c1d3099":"print(y_val_pred[idxs[0]])","49c25524":"gradcam = GradcamPlusPlus(cnn_fruits_mdl,\\\n                          model_modifier,\\\n                          clone=True)\n\ngradcam_maps = gradcam(loss, X_misclass,\\\n                       penultimate_layer=-1)\ngradcam_maps = normalize(gradcam_maps)","4400cc7f":"plt.subplots(figsize=(15,5))\nplt.subplot(1, 3, 1)\nplt.imshow(X_misclass[0])\nplt.grid(b=None)\nplt.title(\"Original Image\")\nplt.subplot(1, 3, 2)\nheatmap = np.uint8(cm.jet(gradcam_maps[0])[..., :3] * 255)\nplt.imshow(heatmap)\nplt.grid(b=None)\nplt.title(\"Grad-CAM++\")\nplt.subplot(1, 3, 3)\nplt.imshow(X_misclass[0])\nplt.imshow(heatmap, alpha=0.5)\nplt.grid(b=None)\nplt.title(\"Grad-CAM++ Overlayed\")\nplt.show()","7c3d5bb7":"explainer = IntegratedGradients()\n\nig_maps = []\nfor i in range(len(labels_l)):\n    img = ([X_misclass[i]], None)\n    label = labels_l[i]\n    ig_map = explainer.explain(img, cnn_fruits_mdl,\\\n                               label, n_steps=25)\n    ig_maps.append(ig_map)","6e7678d2":"plt.subplots(figsize=(15,5))\nplt.subplot(1, 3, 1)\nplt.imshow(X_misclass[0])\nplt.grid(b=None)\nplt.title(\"Original Image\")\nplt.subplot(1, 3, 2)\nheatmap = np.uint8(cm.jet(ig_maps[0])[..., :3] * 255)\nplt.imshow(heatmap)\nplt.grid(b=None)\nplt.title(\"Integrated Gradients Heatmap\")\nplt.subplot(1, 3, 3)\nplt.imshow(X_misclass[0])\nplt.imshow(heatmap, alpha=0.5)\nplt.grid(b=None)\nplt.title(\"Integrated Gradients Overlayed\")\nplt.show()","e4a9727e":"for pos, idx in zip([*range(len(idxs))], idxs):\n    orig_img = X_val_orig[idx]\n    map1 = np.uint8(cm.jet(saliency_maps[pos])[..., :3] * 255)\n    map2 = np.uint8(cm.jet(smoothgrad_saliency_maps[pos])[..., :3] * 255)\n    map3 = mldatasets.heatmap_overlay(X_misclass[pos], gradcam_maps[pos])\n    map4 = mldatasets.heatmap_overlay(X_misclass[pos], ig_maps[pos])\n    viz_img = cv2.vconcat([\n            cv2.hconcat([map1, map2]),\n            cv2.hconcat([map3, map4])\n        ])\n    y_true = y_val[idx,0]\n    y_pred = y_val_pred[idx]\n    probs_s = probs_df.loc[idx]\n    title = 'Gradient-Based Attributions for Misclassification #{}'.format(pos+1)\n    mldatasets.compare_img_pred_viz(orig_img, viz_img, y_true,\\\n                                    y_pred, probs_s, title=title)","ebf1f2e4":"from skimage.segmentation import mark_boundaries\nfrom tf_explain.core.occlusion_sensitivity import OcclusionSensitivity\nimport lime\nfrom lime import lime_image\nfrom alibi.explainers import CEM\nimport shap","bcfa7fff":"print('Eager exec enabled:\\t', tf.executing_eagerly())","094e3958":"avocado_FN_idxs = preds_df[(preds_df['y_true'] != preds_df['y_pred']) &\\\n                           (preds_df['y_true'] == 'Avocado')].index.to_list()\navocado_TP_idxs = preds_df[(preds_df['y_true'] == preds_df['y_pred']) &\\\n                           (preds_df['y_true'] == 'Avocado')].index.to_list()\ngrapefruit_FP_idxs = preds_df[(preds_df['y_true'] != preds_df['y_pred']) &\\\n                              (preds_df['y_pred'] == 'Grapefruit Pink')].index.to_list()\ngrapefruit_TP_idxs = preds_df[(preds_df['y_true'] == preds_df['y_pred']) &\\\n                              (preds_df['y_pred'] == 'Grapefruit Pink')].index.to_list()","888d6c43":"idxs = avocado_TP_idxs + grapefruit_TP_idxs\nX_tp = X_val[idxs]\nprint(X_tp.shape)","2ea495ce":"labels_l = y_val_pred_enc[idxs].squeeze().\\\n                                    astype(int).tolist()\nprint(labels_l)","89bd2008":"explainer = OcclusionSensitivity()\n\nos_maps = []\nfor i in range(len(labels_l)):\n    img = ([X_tp[i]], None)\n    label = labels_l[i]\n    os_map = explainer.explain(img, cnn_fruits_mdl,\\\n                               label, 5)\n    os_maps.append(os_map)","d75ede0f":"explainer = OcclusionSensitivity()\n\nos_maps = []\nfor i in range(len(labels_l)):\n    img = ([X_tp[i]], None)\n    label = labels_l[i]\n    os_map = explainer.explain(img, cnn_fruits_mdl,\\\n                               label, 3)\n    os_maps.append(os_map)","d335b54e":"plt.subplots(figsize=(15,5))\nplt.subplot(1, 3, 1)\nplt.imshow(X_tp[2])\nplt.grid(b=None)\nplt.title(\"Original Image\")\nplt.subplot(1, 3, 2)\nplt.imshow(os_maps[2])\nplt.grid(b=None)\nplt.title(\"Occlusion Sensitivity\")\nplt.subplot(1, 3, 3)\nplt.imshow(X_tp[2])\nplt.imshow(os_maps[2], alpha=0.5)\nplt.grid(b=None)\nplt.title(\"Occlusion Sensitivity Overlayed\")\nplt.show()","f0c0b7c9":"idx = np.random.choice(np.where(y_train[:,0] == 'Grapefruit Pink')[0], 1)[0]\nos_map_train = explainer.explain(([X_train[idx]], None), cnn_fruits_mdl, label, 5)\nplt.subplots(figsize=(15,5))\nplt.subplot(1, 3, 1)\nplt.imshow(X_train[idx])\nplt.grid(b=None)\nplt.title(\"Original Image\")\nplt.subplot(1, 3, 2)\nplt.imshow(os_map_train)\nplt.grid(b=None)\nplt.title(\"Occlusion Sensitivity\")\nplt.subplot(1, 3, 3)\nplt.imshow(X_train[idx])\nplt.imshow(os_map_train, alpha=0.5)\nplt.grid(b=None)\nplt.title(\"Occlusion Sensitivity Overlayed\")\nplt.show()","2a6391cb":"explainer = lime_image.LimeImageExplainer()\n\nlime_expl = []\nfor i in range(len(labels_l)):\n    explanation = explainer.\\\n                    explain_instance(X_tp[i].astype('double'),\\\n                            cnn_fruits_mdl.predict, top_labels=5,\\\n                            hide_color=0, num_samples=1000)\n    lime_expl.append(explanation)","0cc4b25b":"#Explanation with irrelevant segments hidden\nimg_hide, mask_hide = lime_expl[2].\\\n                        get_image_and_mask(lime_expl[2].top_labels[0],\\\n                                    positive_only=True, num_features=10,\\\n                                    hide_rest=True)\nimg_hide = mark_boundaries(img_hide \/ 2 + 0.5, mask_hide)\n\n#Explanation with all segments marked for positive\/negative prediction\nimg_show, mask_show = lime_expl[2].\\\n                            get_image_and_mask(lime_expl[2].top_labels[0],\\\n                                positive_only=False, num_features=10)\nimg_show = mark_boundaries(img_show \/ 2 + 0.5, mask_show)\n\n#Heatmap explanation by segment\ndict_heatmap = dict(lime_expl[2].local_exp[lime_expl[2].top_labels[0]])\nheatmap = np.vectorize(dict_heatmap.get)(lime_expl[2].segments) ","5a9fdaaf":"plt.subplots(figsize=(15,5))\nplt.subplot(1, 3, 1)\nplt.imshow(img_hide)\nplt.grid(b=None)\nplt.title(\"Irrelevant Segments Hidden\")\nplt.subplot(1, 3, 2)\nplt.imshow(img_show)\nplt.grid(b=None)\nplt.title(\"Positive\/Negative Overlayed\")\nplt.subplot(1, 3, 3)\nplt.imshow(heatmap, alpha=0.5, cmap='RdBu')\nplt.grid(b=None)\nplt.title(\"LIME Heatmap\")\nplt.show()","8fae9865":"#### Loading the CNN Model; this model performed poorly on the real time data","402ac341":"## The Preparations","5aee95bf":"#### Creating GradCam++ Maps","eeb15de8":"## Assessing the CNN Classifier with Traditional Interpretation Methods","0f8723ef":"## Understanding Classifications with Perturbation-Based Attribution Methods","28e8f658":"#### Inspect Data","e0a18e27":"### Determining what Misclassifications to Focus On","e27ea827":"#### Data Preparation","e622fad9":"### Intermediate Activations","96c031a4":"### Saliency Maps","6e2b84d7":"## Visualizing the Learning Process with Activation-Based Methods","b7e9079b":"## Initialize and Create Explanations\n","7b3a8597":"### Understanding and Preparing the Data","c8429c10":"### Installing the Libraries","7a00a4a1":"### Tying it All Together","84d16fc7":"### Loading the Libraries","0eb918df":"# Visualizing Images in Convolutions layers : Explainability in CNN \n\n## <i>Part 1<\/i>","aaadcda9":"### Integrated Gradients","e6a9b582":"### Activation Maximization","a5deb0dd":"## Plotting Explanations","55bfe8d1":"## More Explainations","c3d2351b":"### Grad-CAM","6e3d5536":"## Extracting Image and Mask from Explanation","6df0dadb":"# Evaluating Misclassifications with Gradient-Based Attribution Methods","f7e45023":"# Occlusion Sensitivity","4a49283c":"### Determining what Misclassifications to Focus On","2ebd7dc7":"# LIME\u2019s ImageExplainer"}}