{"cell_type":{"bc685355":"code","3afeec47":"code","bdcbc879":"code","173999f1":"code","13b36a2a":"code","e2a45fdb":"code","b8f31595":"code","320df459":"code","9829612b":"code","fa5f12d7":"code","5e735efa":"code","5998df4a":"code","9ba92ace":"code","60347d45":"code","b2bd11eb":"code","b6e0d2d4":"code","80e0b228":"code","3c9ba33e":"code","d011a891":"code","f4e66fe1":"code","d24af378":"code","21318d7c":"code","8ecf07c8":"code","f47e0646":"code","8b838f77":"code","a37b86d1":"code","d49597b7":"code","b8ac46f0":"code","85d1fc06":"code","1b9e790d":"code","bc24e116":"code","5138a2fa":"code","74d4bc36":"code","6af8ac77":"code","8d4e28f1":"code","9be117d1":"code","f6d96831":"markdown","5b905f17":"markdown","66ae4cb8":"markdown","7ac62f1d":"markdown","22d03c5c":"markdown","250ed9b0":"markdown","27bad6e3":"markdown"},"source":{"bc685355":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3afeec47":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, roc_curve","bdcbc879":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","173999f1":"import pandas as pd\ntrain_df = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/train.csv')\ntest_df = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/test.csv')\ntrain_df.head()","13b36a2a":"train_df.target.value_counts()","e2a45fdb":"df_desc = train_df.describe()\ndf_desc","b8f31595":"def plot_feature_scatter(df1, df2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(4,4,figsize=(14,14))\n\n    for feature in features:\n        i += 1\n        plt.subplot(4,4,i)\n        plt.scatter(df1[feature], df2[feature], marker='+')\n        plt.xlabel(feature, fontsize=9)\n    plt.show();","320df459":"features = ['var_0', 'var_1','var_2','var_3', 'var_4', 'var_5', 'var_6', 'var_7', \n           'var_8', 'var_9', 'var_10','var_11','var_12', 'var_13', 'var_14', 'var_15', \n           ]\nplot_feature_scatter(train_df[::20],test_df[::20], features)","9829612b":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(10,10,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(10,10,i)\n        sns.distplot(df1[feature], hist=False,label=label1)\n        sns.distplot(df2[feature], hist=False,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();","fa5f12d7":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nfeatures = train_df.columns.values[2:102]\nplot_feature_distribution(t0, t1, '0', '1', features)","5e735efa":"features = train_df.columns.values[102:]\nplot_feature_distribution(t0, t1, '0', '1', features)","5998df4a":"correlations = train_df[features].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']]\ncorrelations.head(10)","9ba92ace":"correlations.tail(10)","60347d45":"## Calculating VIF\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\ntry: \n    vif_data = pd.read_csv('VIF.csv')\nexcept FileNotFoundError:\n    features = train_df.columns[2:]\n    vif_data = pd.DataFrame()\n    vif_data[\"feature\"] = features\n    # calculating VIF for each feature\n    vif_data[\"VIF\"] = [variance_inflation_factor(train_df[features].values, i)\n                              for i in range(len(features))]\n    \n    vif_data.to_csv('VIF.csv', index = False)","b2bd11eb":"vif_data.sort_values(by = 'VIF', ascending = False, inplace = True)  \nprint(vif_data.head(10))","b6e0d2d4":"print(len(vif_data.loc[vif_data.VIF > 2]))","80e0b228":"vif_data.loc[vif_data.VIF > 2]","3c9ba33e":"from sklearn import preprocessing\n\nfeatures = train_df.columns[2:]\nscaler = preprocessing.StandardScaler().fit(train_df[features])\nX_scaled = scaler.transform(train_df[features])\n\n## Creating dataframe with scaled values with same format \ntrain_df_scaled = pd.DataFrame()\ntrain_df_scaled['ID_code'] = train_df['ID_code']\ntrain_df_scaled['target'] = train_df['target']\ntrain_df_scaled[features] = X_scaled\ntrain_df_scaled.head()","d011a891":"from sklearn.linear_model import LogisticRegression\nfrom scipy.stats import ks_2samp\n\n\nkss = []\nfeatures = train_df.columns[2:]\nfor feature in features:\n    X = train_df_scaled[feature].to_numpy().reshape(-1, 1)\n    y = train_df_scaled['target']\n    clf = LogisticRegression(random_state=0).fit(X, y)\n    probs = clf.predict_proba(X)\n    res = pd.DataFrame({'p': probs[:,1], 'y' : y})\n\n    ks = ks_2samp(res.loc[res.y==0,\"p\"], res.loc[res.y==1,\"p\"]).statistic\n    kss.append(ks)\n    \nks_df = pd.DataFrame({'feature':features, 'ks': kss})\nks_df.sort_values(by = 'ks', ascending = False, inplace = True)\nprint(ks_df.head())","f4e66fe1":"## Top 10 features\nks_df.feature.to_list()[:10]","d24af378":"## Checking KS using all variables\n## Scikit Learn Logistic Regression\n\nX = train_df_scaled[features]\ny = train_df_scaled['target']\nclf = LogisticRegression(random_state=0).fit(X_scaled, y)\nprobs = clf.predict_proba(X)\nres = pd.DataFrame({'p': probs[:,1], 'y' : y})\n\nks = ks_2samp(res.loc[res.y==0,\"p\"], res.loc[res.y==1,\"p\"]).statistic\nprint('KS using Scikit and all features: ',ks)\n\n#print(res.head())","21318d7c":"import statsmodels.api as sm\n\n## statsmodels Logistic Regression\nX = train_df_scaled[features]\ny = train_df_scaled['target']\nlog_reg = sm.Logit(y, X_scaled).fit()\n\nprobs = log_reg.predict(X_scaled)\nyhat = list(map(round, probs))\nres = pd.DataFrame({'p': probs, 'y' : y})\nks = ks_2samp(res.loc[res.y==0,\"p\"], res.loc[res.y==1,\"p\"]).statistic\nprint('KS using StatsModels and all features: ',ks)","8ecf07c8":"#log_reg.summary()","f47e0646":"ks_vif_features =  vif_data.merge(ks_df, on = 'feature')\nks_vif_features = ks_vif_features.loc[ks_vif_features.VIF < 3]\nks_vif_features.sort_values(by = 'ks', ascending = False)\nks_vif_features.head(10)","8b838f77":"from statsmodels.tools.tools import add_constant\nn = 10\n#topn = ks_df.feature.to_list()[:n]\n\nfor i in range(n):\n    topn = ks_vif_features.feature.to_list()[:i+1]\n    y = train_df['target']\n    X_scaled = train_df_scaled[topn]\n    X_scaled = add_constant(X_scaled)\n                                         \n    log_reg = sm.Logit(y, X_scaled).fit()\n    probs = log_reg.predict(X_scaled)\n    \n    yhat = list(map(round, probs))\n    res = pd.DataFrame({'p': probs, 'y' : y})\n    ks = ks_2samp(res.loc[res.y==0,\"p\"], res.loc[res.y==1,\"p\"]).statistic    \n    \n    print(f'{i} {topn} ks: {ks}')\n    print('VIF: \\n',vif_data.loc[vif_data.feature.isin(topn)])","a37b86d1":"res['log_odds'] = np.log(probs\/(1-probs))\nres[topn] = train_df[topn]","d49597b7":"fig, ax = plt.subplots(nrows=4, ncols=3)\nfig.set_figheight(20)\nfig.set_figwidth(20)\ni = 0\nfor i, feature in enumerate(topn):\n    plt.subplot(4, 3, i+1)\n    plt.scatter(x = res['log_odds'], y = res[feature])\n    plt.title(feature)","b8ac46f0":"\n## Plotting multiple plots same figure\nfig, (axL, axR) = plt.subplots(2, figsize=(15, 15))\nplt.suptitle(\"Logistic Regression Residual Plots \\n using Seaborn Lowess line\")\n\n\n# Deviance Residuals\nsns.regplot(log_reg.fittedvalues, log_reg.resid_dev, ax= axL,\n            color=\"black\", scatter_kws={\"s\": 5},\n            line_kws={\"color\":\"b\", \"alpha\":1, \"lw\":2}, lowess=True)\n\naxL.set_title(\"Deviance Residuals \\n against Fitted Values\")\naxL.set_xlabel(\"Linear Predictor Values\")\naxL.set_ylabel(\"Deviance Residuals\")\n\n# Studentized Pearson Residuals\nsns.regplot(log_reg.fittedvalues, log_reg.resid_pearson, ax= axR,\n            color=\"black\", scatter_kws={\"s\": 5},\n            line_kws={\"color\":\"g\", \"alpha\":1, \"lw\":2}, lowess=True)\n\naxR.set_title(\"Studentized Pearson Residuals \\n against Fitted Values\")\naxR.set_xlabel(\"Linear Predictor Values\")\naxR.set_ylabel(\"Studentized Pearson Residuals\")\n\nplt.show()","85d1fc06":"\nfeatures = train_df.columns.values[2:202]\nunique_max_train = []\nunique_max_test = []\nfor feature in features:\n    values = train_df[feature].value_counts()\n    unique_max_train.append([feature, values.max(), values.idxmax()])\n    values = test_df[feature].value_counts()\n    unique_max_test.append([feature, values.max(), values.idxmax()])","1b9e790d":"np.transpose((pd.DataFrame(unique_max_train, columns=['Feature', 'Max duplicates', 'Value'])).\\\n            sort_values(by = 'Max duplicates', ascending=False).head(15))","bc24e116":"np.transpose((pd.DataFrame(unique_max_test, columns=['Feature', 'Max duplicates', 'Value'])).\\\n            sort_values(by = 'Max duplicates', ascending=False).head(15))","5138a2fa":"features = [c for c in train_df.columns if c not in ['ID_code', 'target']]\ntarget = train_df['target']","74d4bc36":"param = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.4,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.01,\n    'max_depth': -1,  \n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': 1\n}","6af8ac77":"folds = StratifiedKFold(n_splits=5)\noof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(train_df.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train_df.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 1000000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 3000)\n    oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test_df[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))","8d4e28f1":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:150].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(14,28))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged\/folds)')\nplt.tight_layout()","9be117d1":"folds = StratifiedKFold(n_splits=5)\noof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(train_df.iloc[trn_idx][best_features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train_df.iloc[val_idx][best_features], label=target.iloc[val_idx])\n\n    num_round = 1000000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 3000)\n    oof[val_idx] = clf.predict(train_df.iloc[val_idx][best_features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = best_features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test_df[best_features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))","f6d96831":"# Building logistic regression model against each variable and calculating ks value to rank best variables","5b905f17":"Now that we have most important features for logistic regression, next step is to build logistic regression by adding features in forward addition and keep track of KS metric. Stop when metric stops improving.\n\nFeatures requirements for logistic regression.\n1. No feature should contribute more than 50%\n2. Not correlated, VIF < 2-3","66ae4cb8":"1. Split training data in, train and validation set. \n   Check distribution of dependent in both the sets, it should align. \n   For more robust split, PSI can be calculated.\n2. **Baseline model.**\n    Feature Engineering <br>\n    Confirm linear distribution on features or transformations. (square, root, log, 1\/3) <br>\n    Cap values at floor of 0. Essentially making all negative values as 0 and only keeping positives as is.<br>\n    Note: Missing values could mean data category and could be predictive feature separating the dependent.<br>\n    When too many features and calculating correlation is not possible.<br>\n    Calculate R2\/ K-S for each independent and select top 5-7 by forward feature selection from top ~90.<br>\n    Restrictions for features; <br>\n    a. VIF < 2\/3<br>\n    b. Contribution of each feature (any shouldn't be more than 50%)<br>\n    c. Significance<br>\n    d. Rank order. - Predictions should follow rank order, if not add more features<br>\n    Else: Select one feature from each cluster of correlated features.<br>\n    Best feature from each cluster by best R2.\n    \n    \n3. **Final model.**\n    Feature Engineering\/ Selection<br>\n    Throw all the features to XGBoost or Random Forest. and reduce number of features by selecting top features, this on small sample ~100k<br>\n    Variable Reduction is done in 2 or more steps. Depends how manyvariables you are reducing. Feature selection is done based on model performance later.<br>\n    Perform broad grid search using small sample, 100k.<br>\n    Perform narrow grid search based on best parameters from broad search.<br>\n    Most important parameter **Min Child Weight** this would help take care of model overfitting. Keeping the trees shorter hence generalising better.<br>\n\n    Measure performance on validation set (ITV)<br>\n    Also on hold-out set (OTV)<br>\n    If performance within 5-10% range then model is fine.<br>\n    If underperforming on ITV, then overfitting. rebuild model by tuning parameters. Shorten tree depth. reduce learning rate, increase min child weight.<br>\n    Rank Order: If predictions not sharing rank order then re-do the model, may be add more variable. Typically adding variables will improve rank order. Alt method    would be selecting\/keeping variables in the model which do not disturb the rank order between pred vs actual\n    \n4. **Compare with linear model.**\n    Performance of final model should be better than the linear model, only reason this does not hold true is when all features are already linear and hence linear model explains everything and leaving not scope for XGBoost to shine. Or also when boosting algorithm is not adding new to the model. Model is not able to learn anything from the errors\n\n     \n     ","7ac62f1d":"## Checking for linearity with log-odds ","22d03c5c":"**Using only best 100 features to see the performance**","250ed9b0":"Maximum correlation we see is 0.009, which is not significant. Hence assuming no correlation between indepedent variables","27bad6e3":"### Scaling data"}}