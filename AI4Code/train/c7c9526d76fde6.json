{"cell_type":{"098ebc73":"code","def98593":"code","286873eb":"code","4e3f6daf":"code","b49b8668":"code","b9833c16":"code","ea05ed04":"code","ede22725":"code","b21f1eb1":"code","af7e1904":"markdown"},"source":{"098ebc73":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","def98593":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport PIL\nimport tensorflow as tf\nimport pathlib\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential","286873eb":"base_dir = '..\/input\/cloud-classification\/data\/train'\ndata_dir = pathlib.Path(base_dir)\nimage_count = len(list(data_dir.glob('*\/*.jpg')))\nprint(image_count)\n\"\"\"roses = list(data_dir.glob('Ac\/*'))\nPIL.Image.open(str(roses[0]))\n\"\"\"\ntf.test.gpu_device_name()","4e3f6daf":"batch_size = 32\nimg_height = 400\nimg_width = 400\ntest_path = \"..\/input\/cloud-classification\/data\/test\"\ntest_dir = pathlib.Path(test_path)\ntrain_generator = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"training\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)\n\nvalidation_generator = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"validation\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","b49b8668":"from tensorflow.python.keras.applications.resnet import ResNet50\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D\nnum_classes = 11\nresnet_weights_path = '..\/input\/resnet50\/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\nmy_new_model = Sequential()\nmy_new_model.add(ResNet50(include_top=False, pooling='avg', weights='imagenet'))\nmy_new_model.add(Dense(num_classes, activation='softmax'))\n\n# Say not to train first layer (ResNet) model. It is already trained\nmy_new_model.layers[0].trainable = False\n","b9833c16":"my_new_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])","ea05ed04":"from tensorflow.python.keras.applications.resnet import preprocess_input\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n\nimage_size = 400\ndata_generator = ImageDataGenerator(preprocessing_function=preprocess_input, validation_split=0.2)\n\n\ntrain_generator = data_generator.flow_from_directory(directory='..\/input\/cloud-classification\/data\/train',\n                                                     subset='training')\nvalidation_generator = data_generator.flow_from_directory(directory='..\/input\/cloud-classification\/data\/train',\n                                                   subset='validation')\nmy_new_model.add(keras.layers.Dropout(0.5))\na = my_new_model.fit_generator(\n        train_generator,\n        steps_per_epoch=15,\n        epochs = 10,\n        validation_data=validation_generator,\n        validation_steps=1)\n#print(a)","ede22725":"print(a.history.keys())\nplt.figure(1, figsize = (15,8)) \n    \nplt.subplot(221)  \nplt.plot(a.history['accuracy'])  \nplt.plot(a.history['val_accuracy'])  \nplt.title('model accuracy')  \nplt.ylabel('accuracy')  \nplt.xlabel('epoch')  \nplt.legend(['train', 'valid']) \n    \nplt.subplot(222)  \nplt.plot(a.history['loss'])  \nplt.plot(a.history['val_loss'])  \nplt.title('model loss')  \nplt.ylabel('loss')  \nplt.xlabel('epoch')  \nplt.legend(['train', 'valid']) \n\nplt.show()","b21f1eb1":"import numpy as np\nimport os\nfrom PIL import Image\nfrom keras.preprocessing import image\nfor f in os.listdir(\"..\/input\/cloud-classification\/data\/test\"):\n    img = image.load_img('..\/input\/cloud-classification\/data\/test\/' + f)\n    img  = image.img_to_array(img)\n    img  = img.reshape((1,) + img.shape)\n\n\n    img_class=my_new_model.predict_classes(img) \n\n    prediction = img_class[0]\n    classname = img_class[0]\n    print(\"Class: \",classname)","af7e1904":"### Transfer Learning\n"}}