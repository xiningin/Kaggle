{"cell_type":{"74147e75":"code","ce117d68":"code","ad47f78b":"code","632dcfac":"code","2afd8e94":"code","c262fabd":"code","f6b34f75":"code","989cce8f":"code","27807b16":"code","52fca728":"code","17a824da":"code","7b2bb628":"code","78d59d9a":"code","e97bb3b2":"code","e5a1767b":"code","60d7ba62":"code","8515e64b":"code","c5f1e49b":"markdown"},"source":{"74147e75":"import numpy as np\nimport pandas as pd\nimport os\nimport random, re, math\nimport tensorflow as tf, tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import optimizers\nfrom kaggle_datasets import KaggleDatasets\n\nprint(tf.__version__)\nprint(tf.keras.__version__)","ce117d68":"!pip install efficientnet\nimport efficientnet.tfkeras as efn","ad47f78b":"AUTO = tf.data.experimental.AUTOTUNE\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\n\n# Data access\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()","632dcfac":"train_df = pd.read_csv(\"..\/input\/imet-2020-fgvc7\/train.csv\")\ntrain_df[\"attribute_ids\"]=train_df[\"attribute_ids\"].apply(lambda x:x.split(\" \"))\ntrain_df[\"id\"]=train_df[\"id\"].apply(lambda x:x+\".png\")\ntrain_df[\"id\"]=train_df[\"id\"].apply(lambda x:'\/train\/' + x)\n\nprint(train_df.shape)\ntrain_df.head()","2afd8e94":"train_paths = train_df[\"id\"].apply(lambda x: GCS_DS_PATH + x).values","c262fabd":"from sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\n\ntrain_df_d = pd.DataFrame(mlb.fit_transform(train_df[\"attribute_ids\"]),columns=mlb.classes_, index=train_df.index)\n\nprint(train_df_d.shape)\ntrain_df_d.head()","f6b34f75":"train_df_d[:1][['448','2429','782']]","989cce8f":"train_labels = train_df_d.astype('int32').values\n\ntrain_labels","27807b16":"import gc\n\ndel train_df_d\ngc.collect()","52fca728":"BATCH_SIZE= 8 * strategy.num_replicas_in_sync\nimg_size = 32\nEPOCHS = 1\nnb_classes = 3471","17a824da":"def decode_image(filename, label=None, image_size=(img_size, img_size)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    image = tf.image.resize(image, image_size)\n    if label is None:\n        return image\n    else:\n        return image, label","7b2bb628":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(512)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n    )","78d59d9a":"gc.collect()","e97bb3b2":"def get_model():\n    base_model =  efn.EfficientNetB0(weights='imagenet', include_top=False, pooling='avg', input_shape=(img_size, img_size, 3))\n    x = base_model.output\n    predictions = Dense(nb_classes, activation=\"softmax\")(x)\n    return Model(inputs=base_model.input, outputs=predictions)","e5a1767b":"with strategy.scope():\n    model = get_model()\n    \nmodel.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])","60d7ba62":"%%time\nmodel.fit(\n    train_dataset, \n    steps_per_epoch=train_labels.shape[0] \/\/ BATCH_SIZE,\n    epochs=EPOCHS\n)","8515e64b":"model.save('model.h5')","c5f1e49b":"#### Training model using TPU and save model"}}