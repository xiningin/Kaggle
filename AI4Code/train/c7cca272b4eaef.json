{"cell_type":{"5b13ee3d":"code","9005b9b1":"code","aafd5c95":"code","774591d4":"code","e3f00699":"code","e467ac66":"code","1a3e879c":"code","77ae5c3d":"code","53bafda6":"code","d98973a2":"code","c2ce3965":"code","3aee5847":"code","b9c681ef":"code","06816eb1":"code","50360805":"code","5fa7bce5":"code","2f2280fa":"code","7af5ebea":"code","72ed6928":"code","e08c716d":"code","eb254b67":"code","c6d938aa":"code","842552e4":"code","eb6aea13":"code","8dae5eb8":"code","d1bd9d15":"code","36738647":"code","7e2a5d39":"code","cd70ed35":"code","58ee041d":"code","afa15bd6":"code","433c725a":"code","a3ffc690":"markdown","556323a6":"markdown","40b3eea3":"markdown","dc24e0ce":"markdown","fc569a2d":"markdown","219d9f2d":"markdown","9eb03c80":"markdown","f4ece04c":"markdown","b1c8dd0e":"markdown","5e17f992":"markdown","c359cecf":"markdown","5b63ef42":"markdown","fb13acb2":"markdown","3ed4d5e2":"markdown","8eaa6f08":"markdown","9bb5c10d":"markdown","e4fd1150":"markdown","66986bba":"markdown","8ac5e95e":"markdown","32e2e6ce":"markdown","59efe0d7":"markdown","d915a29b":"markdown","99ee78ff":"markdown","c3fcf09a":"markdown"},"source":{"5b13ee3d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9005b9b1":"stv = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\ns_sub = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')\ns_price = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\ncal = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\n\n","aafd5c95":"stv","774591d4":"print(\"Categories {}\".format(stv['cat_id'].unique()))\nprint(\"States {}\".format(stv['state_id'].unique()))\nprint(\"Stores {}\".format(stv['store_id'].unique()))\n\n\n","e3f00699":"cal.head(5)","e467ac66":"stv.mean(axis=1).sort_values()","1a3e879c":"# to get only days columns\nd_cols = [col for col in stv.columns if 'd_' in col]\n# and keeping ids aside we will only look into sales\nstv.loc[stv['id'] == 'FOODS_3_090_CA_3_validation'].set_index('id')[d_cols].T.plot(figsize=(15, 5),title='FOODS_3_090_CA_3 sales by \"d\" number')\nplt.legend('')\nplt.show()","77ae5c3d":"examples = ['FOODS_3_090_CA_3','FOODS_3_586_TX_3','FOODS_3_586_TX_2']\nex1= stv.loc[stv['id'] == 'FOODS_3_090_CA_3_validation'][d_cols].T\nex1 = ex1.rename(columns={8412:'FOODS_3_090_CA_3'})\nex2= stv.loc[stv['id'] == 'FOODS_3_586_TX_3_validation'][d_cols].T\nex2 = ex2.rename(columns={21104:'FOODS_3_586_TX_3'})\nex3= stv.loc[stv['id'] == 'FOODS_3_586_TX_2_validation'][d_cols].T\nex3 = ex3.rename(columns={18055:'FOODS_3_586_TX_2'})\nexamples_df = [ex1,ex2,ex3]\nex1 = ex1.reset_index().rename(columns={'index':'d'})\nex1 = ex1.merge(cal,how='left',validate='1:1')\nfor i in [0,1,2]:\n    examples_df[i] = examples_df[i].reset_index().rename(columns={'index':'d'})\n    examples_df[i] = examples_df[i].merge(cal,how='left',validate='1:1')   \n    fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(16,3))\n    examples_df[i].groupby('wday').mean()[examples[i]].plot(kind='line',title='average sale: day of week',ax=ax1)\n    examples_df[i].groupby('month').mean()[examples[i]].plot(kind='line',title='average monthly sale',ax=ax2)\n    examples_df[i].groupby('year').mean()[examples[i]].plot(kind='line',title='average yearly sale',ax=ax3)","53bafda6":"samples = stv.sample(20, random_state=200).set_index('id')[d_cols].T\\\n            .merge(cal.set_index('d')['date'],left_index=True,right_index=True,validate='1:1').set_index('date')\nfig,axs = plt.subplots(10, 2, figsize=(16, 24))\naxs = axs.flatten()\nfor i in range(len(samples.columns)):\n    samples[samples.columns[i]].plot(title=samples.columns[i],ax=axs[i])\nplt.tight_layout()\nplt.show()\n    ","d98973a2":"sns.countplot(data=stv,x='cat_id')\n","c2ce3965":"cats = ['FOODS','HOBBIES','HOUSEHOLD']\nid_cols = ['id','item_id','dept_id','store_id','state_id']\ndaily_sale = stv.groupby('cat_id').sum().T.reset_index().rename(columns={'index':'d'}).merge(cal,how='left',validate='1:1')\ndaily_sale = daily_sale.set_index('date')\nfig, axs =  plt.subplots(3, figsize=(16, 24))\naxs = axs.flatten()\nfor i in range(len(cats)):\n    daily_sale[cats[i]].plot(title= cats[i], ax = axs[i])\n","3aee5847":"from matplotlib.pyplot import figure\ndef plot_Graph3Series(series, title,labels):\n    figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n    plt.plot(series[0],'-b', label=labels[0])\n    plt.plot(series[1],'-r', label=labels[1])\n    plt.plot(series[2],'-g', label=labels[2])\n    plt.title(title)\n    plt.legend(framealpha=1, frameon=True)\n    plt.show()\n","b9c681ef":"#monthly_sale = daily_sale['FOODS'].groupby('month')\ndaily_sale = stv.groupby('cat_id').sum().T.reset_index().rename(columns={'index':'d'}).rename(columns={'index':'d'})#\nfig, axs =  plt.subplots(3, figsize=(6, 14))\naxs = axs.flatten()\nfor i in range(len(cats)):   \n    sale = daily_sale[['d',cats[i]]].merge(cal,how='left',validate='1:1').set_index('date')\n    sale.groupby('month')[cats[i]].mean().plot(title = cats[i],ax=axs[i])","06816eb1":"#daily_sale['date'] = pd.to_datetime(daily_sale['date'])\n#everydaydf.groupby([everydaydf[date_column].dt.to_period(\"M\")]).sum()\ndaily_sale = daily_sale.merge(cal,how='left',validate='1:1').set_index('date')\ndaily_sale['date'] = pd.to_datetime(daily_sale.index)\n#daily_sale = daily_sale.groupby([daily_sale['date'].dt.to_period(\"M\")])['FOODS'].sum()\n#.index = monthlyWithoutOutliersdf.index.to_timestamp()","50360805":"series = []\nfor i in range(len(cats)):\n    x = daily_sale.groupby([daily_sale['date'].dt.to_period(\"M\")])[cats[i]].sum()\n    x.index = x.index.to_timestamp()\n    series.append(x)\nplot_Graph3Series(series,'Monthly Sale in categories',cats)","5fa7bce5":"past_sales = stv.set_index('id')[d_cols] \\\n    .T \\\n    .merge(cal.set_index('d')['date'],\n           left_index=True,\n           right_index=True,\n            validate='1:1') \\\n    .set_index('date')\nfor i in stv['cat_id'].unique():\n    items_col = [c for c in past_sales.columns if i in c]\n    past_sales[items_col] \\\n        .sum(axis=1) \\\n        .plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Total Sales by Item Type')\nplt.legend(stv['cat_id'].unique())\nplt.show()","2f2280fa":"state_list = stv['state_id'].unique()\nfor s in state_list:\n    store_items = [c for c in past_sales.columns if s in c]\n    past_sales[store_items] \\\n        .sum(axis=1) \\\n        .rolling(30).mean() \\\n        .plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Rolling 90 Day Average Total Sales (10 stores)')\nplt.legend(state_list)\nplt.show()","7af5ebea":"# ----------------------------------------------------------------------------\n# Author:  Nicolas P. Rougier\n# License: BSD\n# ----------------------------------------------------------------------------\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef calmap(ax, year, data):\n    ax.tick_params('x', length=0, labelsize=\"medium\", which='major')\n    ax.tick_params('y', length=0, labelsize=\"x-small\", which='major')\n\n    # Month borders\n    xticks, labels = [], []\n    start = datetime(year,1,1).weekday()\n    for month in range(1,13):\n        first = datetime(year, month, 1)\n        last = first + relativedelta(months=1, days=-1)\n\n        y0 = first.weekday()\n        y1 = last.weekday()\n        x0 = (int(first.strftime(\"%j\"))+start-1)\/\/7\n        x1 = (int(last.strftime(\"%j\"))+start-1)\/\/7\n\n        P = [ (x0,   y0), (x0,    7),  (x1,   7),\n              (x1,   y1+1), (x1+1,  y1+1), (x1+1, 0),\n              (x0+1,  0), (x0+1,  y0) ]\n        xticks.append(x0 +(x1-x0+1)\/2)\n        labels.append(first.strftime(\"%b\"))\n        poly = Polygon(P, edgecolor=\"black\", facecolor=\"None\",\n                       linewidth=1, zorder=20, clip_on=False)\n        ax.add_artist(poly)\n    \n    ax.set_xticks(xticks)\n    ax.set_xticklabels(labels)\n    ax.set_yticks(0.5 + np.arange(7))\n    ax.set_yticklabels([\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"])\n    ax.set_title(\"{}\".format(year), weight=\"semibold\")\n    \n    # Clearing first and last day from the data\n    valid = datetime(year, 1, 1).weekday()\n    data[:valid,0] = np.nan\n    valid = datetime(year, 12, 31).weekday()\n    # data[:,x1+1:] = np.nan\n    data[valid+1:,x1] = np.nan\n\n    # Showing data\n    ax.imshow(data, extent=[0,53,0,7], zorder=10, vmin=-1, vmax=1,\n              cmap=\"RdYlBu_r\", origin=\"lower\", alpha=.75)","72ed6928":"from sklearn.preprocessing import StandardScaler\nsscale = StandardScaler()\npast_sales.index = pd.to_datetime(past_sales.index)\nfor i in stv['cat_id'].unique():\n    fig, axes = plt.subplots(3, 1, figsize=(20, 8))\n    items_col = [c for c in past_sales.columns if i in c]\n    sales2013 = past_sales.loc[past_sales.index.isin(pd.date_range('31-Dec-2012',\n                                                                   periods=371))][items_col].mean(axis=1)\n    vals = np.hstack(sscale.fit_transform(sales2013.values.reshape(-1, 1)))\n    calmap(axes[0], 2013, vals.reshape(53,7).T)\n    sales2014 = past_sales.loc[past_sales.index.isin(pd.date_range('30-Dec-2013',\n                                                                   periods=371))][items_col].mean(axis=1)\n    vals = np.hstack(sscale.fit_transform(sales2014.values.reshape(-1, 1)))\n    calmap(axes[1], 2014, vals.reshape(53,7).T)\n    sales2015 = past_sales.loc[past_sales.index.isin(pd.date_range('29-Dec-2014',\n                                                                   periods=371))][items_col].mean(axis=1)\n    vals = np.hstack(sscale.fit_transform(sales2015.values.reshape(-1, 1)))\n    calmap(axes[2], 2015, vals.reshape(53,7).T)\n    plt.suptitle(i, fontsize=30, x=0.4, y=1.01)\n    plt.tight_layout()\n    plt.show()","e08c716d":"#fig, axes = plt.subplots(3, 1, figsize=(20, 8))\n#items_col = [c for c in past_sales.columns if i in c]\nitems_col = []\nfig, axes = plt.subplots(3, 1, figsize=(12, 16))\nindex=0\nfor i in stv['state_id'].unique():\n    items_col = [c for c in past_sales.columns if i in c]\n    state_sales = past_sales[items_col].sum(axis=1)\n    state_sales.plot(ax=axes[index],title=i)\n    index = index+1","eb254b67":"cal","c6d938aa":"state_cols=[]\ncat_cols = []","842552e4":"past_sales['date'] = pd.to_datetime(past_sales.index)\n#past_sales.index","eb6aea13":"past_sales = stv.set_index('id')[d_cols].T\nfig,axs = plt.subplots(1,3,figsize=(16,3))\naxs = axs.flatten()\ni=0\nfor s in stv['state_id'].unique():\n    series = pd.DataFrame()\n    state_cols= [c for c in past_sales.columns if s in c]\n    for cat in stv['cat_id'].unique():\n        cat_cols= [c for c in state_cols if cat in c]\n        x = pd.DataFrame(past_sales[cat_cols].sum(axis=1))\n        x['d'] = past_sales[cat_cols].sum(axis=1).index \n        x= x.merge(cal.set_index('d'),how='left',left_index=True,right_index=True,validate='1:1')\n        x = x.groupby('wday').mean()[0].rename(cat)\n        series = pd.concat([series,x],axis=1) \n    series.plot(ax=axs[i],title='Average Sale on day of week for '+ s)\n    i=i+1\n       ","8dae5eb8":"fig,axs = plt.subplots(1,3,figsize=(16,3))\naxs = axs.flatten()\ni=0\nfor s in stv['state_id'].unique():\n    series = pd.DataFrame()\n    state_cols= [c for c in past_sales.columns if s in c]\n    for cat in stv['cat_id'].unique():\n        cat_cols= [c for c in state_cols if cat in c]\n        x = pd.DataFrame(past_sales[cat_cols].sum(axis=1))\n        x['d'] = past_sales[cat_cols].sum(axis=1).index \n        x= x.merge(cal.set_index('d'),how='left',left_index=True,right_index=True,validate='1:1')\n        x = x.groupby('month').mean()[0].rename(cat)\n        series = pd.concat([series,x],axis=1) \n    series.plot(ax=axs[i],title='Average monthly Sale for '+ s)\n    i=i+1\n       ","d1bd9d15":"#snap_count = cal.groupby(['weekday','snap_TX'])['snap_TX'].count()\n#snap_count['weekday'] = snap_count.index\n#snap_count = snap_count.rename({0:'off',1:'on'})\nfig,axs = plt.subplots(1,3,figsize=(16,3))\nsns.countplot(data=cal,x='weekday',hue='snap_CA', ax=axs[0])\nsns.countplot(data=cal,x='weekday',hue='snap_TX', ax=axs[1])\nsns.countplot(data=cal,x='weekday',hue='snap_WI', ax=axs[2])","36738647":"#import datetime\n#date1 = datetime.datetime.strptime('2012-31-12',\"%Y-%d-%m\") \ndate_range = pd.date_range('31-Dec-2012',periods=371)\ncal['date'] = pd.to_datetime(cal['date']) \nsales2013 = cal.loc[(cal['date'] >= min(date_range)) & (cal['date'] <= max(date_range))]['snap_TX']\nvals = np.hstack(sscale.fit_transform(sales2013.values.reshape(-1, 1)))\nfig, axes = plt.subplots(figsize=(20, 8))\ncalmap(axes, 2013, vals.reshape(53,7).T)","7e2a5d39":"\nfig,axs = plt.subplots(1,3,figsize=(16,3))\naxs = axs.flatten()\ni=0\n#for s in stv['state_id'].unique():\n#    series = pd.DataFrame()\n#    state_cols= [c for c in past_sales.columns if s in c]\nfor cat in stv['cat_id'].unique():\n    cat_cols= [c for c in past_sales.columns if cat in c]\n    x = pd.DataFrame(past_sales[cat_cols].sum(axis=1))\n    x['d'] = past_sales[cat_cols].sum(axis=1).index \n    x= x.merge(cal.set_index('d'),how='left',left_index=True,right_index=True,validate='1:1')\n    sns.boxplot(y=x[0], x=x['event_type_1'], ax=axs[i]).set_title(cat)\n    i=i+1\n       ","cd70ed35":"\nfig,axs = plt.subplots(3,3,figsize=(16,16))\n#axs = axs.flatten()\ni=0\nj=0\nfor s in stv['state_id'].unique():\n    series = pd.DataFrame()\n    state_cols= [c for c in past_sales.columns if s in c]\n    j=0\n    for cat in stv['cat_id'].unique():\n        cat_cols= [c for c in state_cols if cat in c]\n        x = pd.DataFrame(past_sales[cat_cols].sum(axis=1))\n        x['d'] = past_sales[cat_cols].sum(axis=1).index \n        x= x.merge(cal.set_index('d'),how='left',left_index=True,right_index=True,validate='1:1')\n        col = 'snap_'+s\n        sns.boxplot(y=x[0], x=x[col], ax=axs[i][j]).set_title(cat + \" \"+ s)\n        j=j+1\n    i=i+1\n       ","58ee041d":"sns.boxplot(y=x[0], x=x['event_type_1'])","afa15bd6":"fig, axes = plt.subplots(1,3,figsize=(16,6))\nsns.countplot(data=cal, x=cal['snap_CA'], ax=axes[0]) \nsns.countplot(data=cal, x=cal['snap_WI'], ax=axes[1]) \nsns.countplot(data=cal, x=cal['snap_TX'], ax=axes[2]) ","433c725a":"sns.countplot(data=cal['event_name_1'], x=cal['event_type_1']) #.isnull().sum()","a3ffc690":"Sale in CA is the most. And interestingly the curve of sale in WI and TX has similar nature of peaks and troughs. ","556323a6":"Average monthly sale in three states. Again, there is hardly any variation in hobbies sale throughtout the year. We can observe a in the sale of food and households in the month of May.","40b3eea3":"Analysing random 20 samples","dc24e0ce":"There is increase in sale of food during sporting and religious events.However, there is slight increase in sale of hobbies during cultural events. And these increase or these pattern are self explanatory.","fc569a2d":"Sale of Foods is the most and sale of hobbies is the least. This make sense because we all need Food for survival but hobbies are leisure activity which people do in free time.","219d9f2d":"Looking into average monthly sale for each category. There is a drop of sale for Food and household in the month of May and december. The reason for this drop could be that most of the people go on vacation during this month. The sale of hobbies declines in August and Sept and reaches the lowest in Sept. Reason could be that is the start of new sessions in schools and colleges and people don't have time for hobbies. (This is just a guess)","9eb03c80":"Average sale on a single day of the week in three states. This also shows that there are high sale on weekends and by the time we reach midweek .. sale drops and again there is increase in sale as move towards Friday or weekend","f4ece04c":"21104,18055,8412 ids with maximum sales overall. In terms of total sale and average sale both.So, we will try to analyze them first.\nFOODS_3_586_TX_3_validation,\nFOODS_3_586_TX_2_validation,\nFOODS_3_090_CA_3_validation","b1c8dd0e":"There are three categories \"Food\", \"Household\", \"Foods\". Three States Texas, California, Wisconsin. CA has 4 stores, TX has 3 stores and WI has 3 stores","5e17f992":"Sale based on each category","c359cecf":"There are many days when the sale is zero.","5b63ef42":"**Now, lets look into calendar data and try to find some pattern throught the year**","fb13acb2":"In the calendar heatmap, for all the categories sale is mostly concentrated in weekends or during holidays.","3ed4d5e2":"Most of the events are religious and national.","8eaa6f08":"The Sale with snap benefits in all the three states looks same for all the days of the week.","9bb5c10d":"Daily sale of three categories over the five years. An upward trend can be seen in daily food sale. \n","e4fd1150":"In the calendar data , we hadates , weekdays month year . Also, days as d1, d2, d3 given as in sales_trained data.","66986bba":"We have item ids, categories and historic sales from day1 to day 1913. There are 30490 items in the shop.And there are categories of hobbies , foods and household. There are 3 states CA, WI and TX and 11 stores distributed in these states.","8ac5e95e":"Sales per state with seasonality","32e2e6ce":"Sale of all the categories over the 5 years as a time series curve. While the sale of hobbies has not changed much in these 5 years. There is an upward trend in the sale of foods and household. there are more fluctuations in food sale, lets try to get the answer as we explore more.","59efe0d7":"The sale of every items of every category drops to zero 25th december as the shops are closed on that day.","d915a29b":"Snap benefits mostly occurs on first two weeks of the month. The graph above only shows for the year 2013.. Similarly we can look into snap benefits patterns for all the three years","99ee78ff":"Since the snap benefits are mainly for poor people. We can clearly see rise of sale of food items during snap benefits for each countries. And there is no difference between sale of hobbies during snap benefits.","c3fcf09a":"This notebook is a beginners attempt on exploring data for walmart shop. And some of the code is being inspired from notebooks of the fellow participants.  "}}