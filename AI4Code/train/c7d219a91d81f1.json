{"cell_type":{"51b70c66":"code","07057a22":"code","e67c42bd":"code","3035c372":"code","09729dca":"code","68b58ba7":"code","cc0a20e9":"code","6a148d74":"code","c0b113e5":"code","7098fec3":"code","022caeb4":"code","eb1d65cb":"code","6c551342":"markdown","7d392927":"markdown","6d3030dd":"markdown","da75caf4":"markdown","e1f3e1c8":"markdown","88ee4ff6":"markdown","30995ff0":"markdown","dcd5be65":"markdown","b5447c56":"markdown"},"source":{"51b70c66":"# -*- coding: utf-8 -*-\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n","07057a22":"def unpickle(file):\n    import pickle\n    with open(file, 'rb') as fo:\n        dict = pickle.load(fo, encoding='latin1')\n    return dict\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndata_pre_path = '\/kaggle\/input\/cifar100\/\/' # change this path\n# File paths\ndata_train_path = data_pre_path + 'train'\ndata_test_path = data_pre_path + 'test'\ndata_meta_path = data_pre_path + 'meta'\n\n# Read dictionary\ndata_train = unpickle(data_train_path)\ndata_test = unpickle(data_test_path)\ndata_meta = unpickle(data_meta_path)\n\nsubCategory = pd.DataFrame(data_meta['fine_label_names'], columns=['SubClass'])\nsubCategoryDict = subCategory.to_dict()\n\nX_train = data_train['data']\ny_train=data_train['fine_labels']\n\n# Usaremos 20% de la data de entrenamiento para validar el desempe\u00f1o de la red en cada epoch.\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size=0.8)\n\nX_train = X_train.reshape(len(X_train),3,32,32).transpose(0,2,3,1)\nX_valid = X_valid.reshape(len(X_valid),3,32,32).transpose(0,2,3,1)\n\n#transforming the testing dataset\nX_test = data_test['data']\nX_test = X_test.reshape(len(X_test),3,32,32).transpose(0,2,3,1)\ny_test = data_test['fine_labels']\n\nX_train = np.asarray(X_train)\ny_train = np.asarray(y_train)\nX_valid = np.asarray(X_valid)\ny_valid = np.asarray(y_valid)\nX_test = np.asarray(X_test)\ny_test = np.asarray(y_test)","e67c42bd":"###############################################################################\n# comprobar las labels de algunas im\u00e1genes\n###############################################################################\n\nplt.figure(figsize=(10,10))\nfor image in range(0,25):\n    i=image\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    j=i+0# a\u00f1adir de 25 en 25 para cambiar el bloque de fotos\n    data_plot = X_test[j]\n    plt.imshow(data_plot)\n    plt.xlabel(str(subCategoryDict['SubClass'][y_test[j]]))\nplt.show()","3035c372":"##############################################################################\n#plot history\n##############################################################################\n\ndef plot_history(history):\n    val_loss = history.history['val_loss' ]\n    loss =     history.history['loss' ]\n    acc =      history.history['accuracy' ]\n    val_acc =  history.history['val_accuracy' ]\n\n    epochs    = range(1,len(acc)+1,1)\n\n    plt.plot  ( epochs,     acc, 'r--', label='Training acc'  )\n    plt.plot  ( epochs, val_acc,  'b', label='Validation acc')\n    plt.title ('Training and validation accuracy')\n    plt.ylabel('acc')\n    plt.xlabel('epochs')\n    plt.legend()\n\n    plt.figure()\n\n    plt.plot  ( epochs,     loss, 'r--', label='Training loss' )\n    plt.plot  ( epochs, val_loss ,  'b', label='Validation loss' )\n    plt.title ('Training and validation loss')\n    plt.ylabel('loss')\n    plt.xlabel('epochs')\n    plt.legend()\n\n    plt.figure()","09729dca":"##############################################################################\n#MODELO KERAS\n##############################################################################\nmodel = Sequential()\nmodel.add(Conv2D(input_shape=(32, 32, 3), kernel_size=(2, 2), padding='same', strides=(2, 2), filters=32))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same'))\nmodel.add(Conv2D(kernel_size=(2, 2), padding='same', strides=(2, 2), filters=64))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same'))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(100, activation='softmax'))\n\nmodel.summary()\n\n\nopt = 'adam'\n\nmodel.compile(loss='sparse_categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])","68b58ba7":"###############################################################################\n#PREPROCESADO CON ImageDataGenerator\n###############################################################################\n\naugmenter = ImageDataGenerator(rescale=1.0\/255.,\n                             rotation_range=20,\n                             width_shift_range=0.1, \n                             height_shift_range=0.1, \n                             shear_range=0.1, \n                             zoom_range=0.2, \n                             fill_mode='nearest',\n                             horizontal_flip=True)\nrescalator=ImageDataGenerator(rescale=1.0\/255.)\n\ntrain_generator=augmenter.flow(X_train, y_train, batch_size=20)\nvalid_generator=rescalator.flow(X_valid, y_valid, batch_size=20)\ntest_generator=rescalator.flow(X_test, y_test, batch_size=20)","cc0a20e9":"############################################################################\n#ENTRENAMIENTO DEL MODELO\n############################################################################\n#EPOCHS=100\n#EPOCHS=1\nEPOCHS=30\nbatch_size=20\nsteps_per_epoch=train_generator.n\/\/batch_size\nvalidation_steps=valid_generator.n\/\/batch_size\nhistory = model.fit(train_generator,\n                        steps_per_epoch=steps_per_epoch,\n                        epochs=EPOCHS,\n                        validation_data=valid_generator,\n                        validation_steps=validation_steps\n                        )","6a148d74":"plot_history(history)\ntest_loss,test_acc=model.evaluate(test_generator, verbose=2)\nprint(\"test accuracy: \",test_acc)","c0b113e5":"###############################################################################\n# confussion matrix\n###############################################################################\ndef plot_confusion_matrix(cm, classes,\n                          normalize=True,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Reds):\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    plt.tight_layout()\n    plt.ylabel('Observaci\u00f3n')\n    plt.xlabel('Predicci\u00f3n')\nY_pred = model.predict(X_test)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred, axis = 1) \nconfusion_mtx = confusion_matrix(y_test, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(100))","7098fec3":"idx,counts =np.unique(Y_pred_classes,return_counts=True,axis=0)\nnp.unique(Y_pred_classes,return_counts=True,axis=0)\n\nprint('CLASSES CHOOSEN MANY TIMES')\nfor number in range(idx.shape[0]):\n    clase=idx[number]\n    if counts[number]>500:\n        print('clase: ' +(str(subCategoryDict['SubClass'][clase]))+' ('+str(clase) +') counts: '+str(counts[number]))","022caeb4":"print('CLASSES NOT CHOOSEN')\nfor all_classes in range(100):\n    found=False\n    for i in range(idx.shape[0]):\n        if all_classes==idx[i]:\n            found=True\n    if found==False:\n        print('clase: ' +(str(subCategoryDict['SubClass'][all_classes]))+' ('+str(all_classes) +') counts: '+str(0))","eb1d65cb":"###############################################################################\n# algunas predicciones con imagen\n###############################################################################\nplt.figure(figsize=(10,10))\nfor image in range(0,25):\n    i=image\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    j=i+100# a\u00f1adir de 25 en 25 para cambiar el bloque de fotos\n    data_plot = X_test[j]\n    plt.imshow(data_plot)\n    plt.xlabel(str(subCategoryDict['SubClass'][Y_pred_classes[j]]) + \" vs \" + str(subCategoryDict['SubClass'][y_test[j]]))\nplt.show()","6c551342":"Training of the model","7d392927":"This is the first time that I make CNN done by myself (not following the instructions of any course). I have tried many options to try to improve the accuracy of my model. I have only tried two models and some hyperparameters with each one of them.\nThis is what I got in terms of accuracy:\n\n<table style=\"width:80%\">\n  <tr>\n    <td> MODEL I <\/td>\n    <td> MODEL II <\/td> \n    <td> DATA AUGMENTATION FM <\/td>\n    <td> DATA AUGMENTATION <\/td> \n    <td> METRIC ADAM <\/td> \n    <td> METRIC SCG <\/td> \n    <td> TEST ACCURACY <\/td> \n  <\/tr>\n  <tr>\n    <td> <\/td>\n    <td>X<\/td>\n    <td> <\/td>\n    <td> <\/td>\n    <td>X<\/td>\n    <td> <\/td>\n    <td>0.2716<\/td>\n  <\/tr>\n  <tr>\n    <td> <\/td>\n    <td>X<\/td>\n    <td> <\/td>\n    <td> <\/td>\n    <td> <\/td>\n    <td>X<\/td>\n    <td>0.2920<\/td>\n  <\/tr>\n  <tr>\n    <td> <\/td>\n    <td>X<\/td>\n    <td>X<\/td>\n    <td> <\/td>\n    <td>X<\/td>\n    <td> <\/td>\n    <td>0.3679<\/td>\n  <\/tr>\n  <tr>\n    <td> <\/td>\n    <td>X<\/td>\n    <td>X<\/td>\n    <td> <\/td>\n    <td> <\/td>\n    <td>X<\/td>\n    <td>0.2934<\/td>\n  <\/tr>\n  <tr>\n    <td> <\/td>\n    <td>X<\/td>\n    <td>X<\/td>\n    <td> <\/td>\n    <td> <\/td>\n    <td>X<\/td>\n    <td>0.2934<\/td>\n  <\/tr>\n  <tr>\n    <td> <\/td>\n    <td>X<\/td>\n    <td> <\/td>\n    <td>X<\/td>\n    <td>X<\/td>\n    <td> <\/td>\n    <td>0.3867<\/td>\n  <\/tr>\n  <tr>\n    <td> <\/td>\n    <td>X<\/td>\n    <td> <\/td>\n    <td>X<\/td>\n    <td> <\/td>\n    <td>X<\/td>\n    <td>0.3054<\/td>\n  <\/tr>\n  <tr>\n    <td>X<\/td>\n    <td> <\/td>\n    <td> <\/td>\n    <td> <\/td>\n    <td>x<\/td>\n    <td> <\/td>\n    <td>0.3011<\/td>\n  <\/tr>\n  <tr>\n    <td>X<\/td>\n    <td> <\/td>\n    <td> <\/td>\n    <td> <\/td>\n    <td> <\/td>\n    <td>X<\/td>\n    <td>0.3326<\/td>\n  <\/tr>\n  <tr>\n    <td>X<\/td>\n    <td> <\/td>\n    <td>X<\/td>\n    <td> <\/td>\n    <td>X<\/td>\n    <td> <\/td>\n    <td>0.4158<\/td>\n  <\/tr>\n  <tr>\n    <td>X<\/td>\n    <td> <\/td>\n    <td>X<\/td>\n    <td> <\/td>\n    <td> <\/td>\n    <td>X<\/td>\n    <td>0.4063<\/td>\n  <\/tr>\n  <tr>\n    <td>X<\/td>\n    <td> <\/td>\n    <td> <\/td>\n    <td>X<\/td>\n    <td>X<\/td>\n    <td> <\/td>\n    <td>0.4092<\/td>\n  <\/tr>\n  <tr>\n    <td>X<\/td>\n    <td> <\/td>\n    <td> <\/td>\n    <td>X<\/td>\n    <td> <\/td>\n    <td>X<\/td>\n    <td>0.4047<\/td>\n  <\/tr>\n<\/table>\n\nDATA AUGMENTATION FM means that I added the fill_mode='nearest' to the Generator.\n\nAnd the models I trained are\n\nMODEL I: \nConv2D + MaxPooling2D + Conv2D + MaxPooling2D+ Flatten + Dense + Dense\n\nMODEL II:\nConv2D + Conv2D + MaxPooling2D + Conv2D + Conv2D + MaxPooling2D + Flatten + Dense + Dense + Dense\n\nIn my MODEL II, I was trying to measure the effect of repeating layers, because I saw that the VGG-16 works like that. So I expected MODEL II to work better than MODEL I, but it hasn't worked like that. My resoults are better with MODEL I.\n\nBetween the metric Adam and SCG, I was just trying one and the other. I can see that Adam tends to work better.\n\nI also wanted to learn to use a Generator. I think with this data set is hardly reccomended to use a Image Generator. We have 100 clases but w only have 500 images of each class. So the accuracy increases considerably when you add images to the training. I have tried the ImageGenerator and it works, but eventhough I think it's necessary more images. So I think a good option for a dataset like this should be a Transfer Learning, which I will try in another notebook. \n\nI trained all these models with 100 epochs. Now I train with less, to avoid overfitting.\n\nWhat I'm going to share is the training and test of the best combination that I found, and I will explore a little the resoults.","6d3030dd":"# Cifar-100 CNN","da75caf4":"Define the data augmentation","e1f3e1c8":"Define the model","88ee4ff6":"Define the function for plotting the history of the training of the model","30995ff0":"Whatching the Confusion matrix, we can see two things. \n\nThe diagonal points the correct predictions, where the observed image and the predicted are equal. And It is there, so many images are predicted correctly (40% attending to the accuracy of the model).\n\nBut there are some vertical lines marked. \u00bfWhat are they? These lines means that the model tends to classify many images as those clases. \n\nFor example, there are many images wrongly classified as 22, 39, ...\nIn the other side, thare are some kind of images that are never used as a prediction, for example, 27, 63. Is this something ramdom?\n\nNo it isn't. I have trained the same model 4 times and there are always the same classes predicted many times and predicted zero times. And another test was to train the model with only one epoch. Only one. And just with one epoch the model predicted the same lines and whites. \n\nSo... it's the model. It classifies many images to a few classes. Probably the model is too simple and tends to do things like classify all red things as sweet peepers (it's later).\n","dcd5be65":"I would like to see some of the images to understand what are we doing with the model","b5447c56":"Here we have some examples of images. Notice how four for the 25 images that we have here, we can see most of them classified by the model as clock, plate. Also notice that images with a strong red component, are classified as sweet peeper.\n\nFor each image, the predicted class is the first one and the wright class is the second one. "}}