{"cell_type":{"09a28f96":"code","1bf6396b":"code","999cb956":"code","3849b95a":"code","77da79ad":"code","9e1df16f":"code","b600678c":"code","8a64d3de":"code","2d004959":"code","91e8e17b":"code","a2d68bed":"code","d9779c09":"code","4f4765f1":"code","b26a9c01":"code","b38cffe2":"code","092dc508":"code","cd07316f":"code","dd357134":"code","d66cc24e":"code","1989000c":"code","7f53162d":"code","838af407":"code","cb091bf3":"markdown","3f902676":"markdown","bdae58a6":"markdown","0221bb88":"markdown","c38b5445":"markdown","fe0f5ca8":"markdown","50d26441":"markdown","29840a61":"markdown","618fab60":"markdown","9cb319d9":"markdown","894464a1":"markdown","8dd53a23":"markdown","66213136":"markdown","f94553fb":"markdown","a0f556d7":"markdown","d4acd8cb":"markdown","969dc47a":"markdown","5b4e0c92":"markdown","c9a6b4d6":"markdown","21b660e3":"markdown","0262345f":"markdown","e14a6027":"markdown","8d6ef283":"markdown","980dfb4b":"markdown","76ca39c4":"markdown","01e42d09":"markdown","0577269f":"markdown","73e09fa4":"markdown","a3d7bcbd":"markdown","e14bf3f5":"markdown","64ad19ce":"markdown","c1016563":"markdown","b5c6621c":"markdown","5591d291":"markdown","b550e48b":"markdown","97902a92":"markdown","0e49d70e":"markdown","d582b80c":"markdown","71cce3e8":"markdown","31baa999":"markdown","8f92cca6":"markdown"},"source":{"09a28f96":"from __future__ import unicode_literals, print_function, division\nfrom io import open\nimport unicodedata\nimport string\nimport re\nimport random\nimport os\nimport shutil\nimport urllib3\nimport zipfile\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","1bf6396b":"http = urllib3.PoolManager()\nurl = 'http:\/\/www.manythings.org\/anki\/fra-eng.zip'\nfilename = 'fra-eng.zip'\npath = os.getcwd()\nzipfilename = os.path.join(path, filename)\n\nwith http.request('GET', url, preload_content = False) as r, open(zipfilename, 'wb') as out_file:\n  shutil.copyfileobj(r, out_file)\n\nwith zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n  zip_ref.extractall(path)","999cb956":"!ls","3849b95a":"SOS_token = 0\nEOS_token = 1\n\nclass Lang:\n    def __init__(self, name):\n        self.name = name\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {0: 'SOS', 1: 'EOS'}\n        self.n_words = 2\n    \n    def addSentence(self, sentence):\n        for word in sentence.split(' '):\n            self.addWord(word)\n    \n    def addWord(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.n_words\n            self.word2count[word] = 1\n            self.index2word[self.n_words] = word\n            self.n_words += 1\n        else:\n            self.word2count[word] += 1","77da79ad":"# Convert Unicode strings to plain ASCII\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n    )\n\n# Remove lowercase letters, trimming, and non-character characters\ndef normalizeString(s):\n    s = unicodeToAscii(s.lower().strip())\n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n    return s","9e1df16f":"def readLangs(lang1, lang2, reverse=False):\n    print(\"Reading lines...\")\n\n    # Read the file and separate it into lines\n    lines = open('.\/%s.txt' % (lang2), encoding='utf-8').\\\n        read().strip().split('\\n')\n    \n    # Separate all lines in pairs and normalize\n    pairs = [[normalizeString(s) for s in l.split('\\t')][:2] for l in lines]\n    \n    # Flip pairs, create Lang instances\n    if reverse:\n        pairs = [list(reversed(p)) for p in pairs]\n        input_lang = Lang(lang2)\n        output_lang = Lang(lang1)\n    else:\n        input_lang = Lang(lang1)\n        output_lang = Lang(lang2)\n\n    return input_lang, output_lang, pairs\n","b600678c":"MAX_LENGTH = 10\n\neng_prefixes = (\n    \"i am \", \"i m \",\n    \"he is\", \"he s \",\n    \"she is\", \"she s \",\n    \"you are\", \"you re \",\n    \"we are\", \"we re \",\n    \"they are\", \"they re \"\n)\n\n\ndef filterPair(p):\n    return len(p[0].split(' ')) < MAX_LENGTH and \\\n        len(p[1].split(' ')) < MAX_LENGTH and \\\n        p[1].startswith(eng_prefixes)\n\n\ndef filterPairs(pairs):\n    return [pair for pair in pairs if filterPair(pair)]","8a64d3de":"def prepareData(lang1, lang2, reverse=False):\n    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n    print(\"Read %s sentence pairs\" % len(pairs))\n    pairs = filterPairs(pairs)\n    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n    print(\"Counting words...\")\n    for pair in pairs:\n        input_lang.addSentence(pair[0])\n        output_lang.addSentence(pair[1])\n    print(\"Counted words:\")\n    print(input_lang.name, input_lang.n_words)\n    print(output_lang.name, output_lang.n_words)\n    return input_lang, output_lang, pairs\n\n\ninput_lang, output_lang, pairs = prepareData('eng', 'fra', True)\nprint(random.choice(pairs))","2d004959":"class EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size)\n\n    def forward(self, input, hidden):\n        embedded = self.embedding(input).view(1, 1, -1)\n        output = embedded\n        output, hidden = self.gru(output, hidden)\n        return output, hidden\n\n    def initHidden(self):\n        return torch.zeros(1, 1, self.hidden_size, device=device)","91e8e17b":"class DecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size):\n        super(DecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n\n        self.embedding = nn.Embedding(output_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input, hidden):\n        output = self.embedding(input).view(1, 1, -1)\n        output = F.relu(output)\n        output, hidden = self.gru(output, hidden)\n        output = self.softmax(self.out(output[0]))\n        return output, hidden\n\n    def initHidden(self):\n        return torch.zeros(1, 1, self.hidden_size, device=device)","a2d68bed":"class AttnDecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n        super(AttnDecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n\n        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n        self.dropout = nn.Dropout(self.dropout_p)\n        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n        self.out = nn.Linear(self.hidden_size, self.output_size)\n\n    def forward(self, input, hidden, encoder_outputs):\n        embedded = self.embedding(input).view(1, 1, -1)\n        embedded = self.dropout(embedded)\n\n        attn_weights = F.softmax(\n            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n                                 encoder_outputs.unsqueeze(0))\n\n        output = torch.cat((embedded[0], attn_applied[0]), 1)\n        output = self.attn_combine(output).unsqueeze(0)\n\n        output = F.relu(output)\n        output, hidden = self.gru(output, hidden)\n\n        output = F.log_softmax(self.out(output[0]), dim=1)\n        return output, hidden, attn_weights\n\n    def initHidden(self):\n        return torch.zeros(1, 1, self.hidden_size, device=device)","d9779c09":"def indexesFromSentence(lang, sentence):\n    return [lang.word2index[word] for word in sentence.split(' ')]\n\n\ndef tensorFromSentence(lang, sentence):\n    indexes = indexesFromSentence(lang, sentence)\n    indexes.append(EOS_token)\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n\n\ndef tensorsFromPair(pair):\n    input_tensor = tensorFromSentence(input_lang, pair[0])\n    target_tensor = tensorFromSentence(output_lang, pair[1])\n    return (input_tensor, target_tensor)","4f4765f1":"teacher_forcing_ratio = 0.5\n\n\ndef train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n    encoder_hidden = encoder.initHidden()\n\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n\n    input_length = input_tensor.size(0)\n    target_length = target_tensor.size(0)\n\n    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n\n    loss = 0\n\n    for ei in range(input_length):\n        encoder_output, encoder_hidden = encoder(\n            input_tensor[ei], encoder_hidden)\n        encoder_outputs[ei] = encoder_output[0, 0]\n\n    decoder_input = torch.tensor([[SOS_token]], device=device)\n\n    decoder_hidden = encoder_hidden\n\n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n    if use_teacher_forcing:\n        # Teacher forcing: Feed the target as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            loss += criterion(decoder_output, target_tensor[di])\n            decoder_input = target_tensor[di]  # Teacher forcing\n\n    else:\n        # Without teacher forcing: use its own predictions as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            topv, topi = decoder_output.topk(1)\n            decoder_input = topi.squeeze().detach()  # detach from history as input\n\n            loss += criterion(decoder_output, target_tensor[di])\n            if decoder_input.item() == EOS_token:\n                break\n\n    loss.backward()\n\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n\n    return loss.item() \/ target_length","b26a9c01":"import time\nimport math\n\n\ndef asMinutes(s):\n    m = math.floor(s \/ 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s \/ (percent)\n    rs = es - s\n    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))","b38cffe2":"%matplotlib inline\ndef trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n    start = time.time()\n    plot_losses = []\n    print_loss_total = 0  # Reset every print_every\n    plot_loss_total = 0  # Reset every plot_every\n\n    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n    training_pairs = [tensorsFromPair(random.choice(pairs))\n                      for i in range(n_iters)]\n    criterion = nn.NLLLoss()\n\n    for iter in range(1, n_iters + 1):\n        training_pair = training_pairs[iter - 1]\n        input_tensor = training_pair[0]\n        target_tensor = training_pair[1]\n\n        loss = train(input_tensor, target_tensor, encoder,\n                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n        print_loss_total += loss\n        plot_loss_total += loss\n\n        if iter % print_every == 0:\n            print_loss_avg = print_loss_total \/ print_every\n            print_loss_total = 0\n            print('%s (%d %d%%) %.4f' % (timeSince(start, iter \/ n_iters),\n                                         iter, iter \/ n_iters * 100, print_loss_avg))\n\n        if iter % plot_every == 0:\n            plot_loss_avg = plot_loss_total \/ plot_every\n            plot_losses.append(plot_loss_avg)\n            plot_loss_total = 0\n\n    showPlot(plot_losses)","092dc508":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.switch_backend('agg')\nimport matplotlib.ticker as ticker\nimport numpy as np\n\n\ndef showPlot(points):\n    plt.figure()\n    fig, ax = plt.subplots()\n    \n    # this locator puts ticks at regular intervals\n    loc = ticker.MultipleLocator(base=0.2)\n    ax.yaxis.set_major_locator(loc)\n    plt.plot(points)","cd07316f":"def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n    with torch.no_grad():\n        input_tensor = tensorFromSentence(input_lang, sentence)\n        input_length = input_tensor.size()[0]\n        encoder_hidden = encoder.initHidden()\n\n        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n\n        for ei in range(input_length):\n            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n                                                     encoder_hidden)\n            encoder_outputs[ei] += encoder_output[0, 0]\n\n        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n\n        decoder_hidden = encoder_hidden\n\n        decoded_words = []\n        decoder_attentions = torch.zeros(max_length, max_length)\n\n        for di in range(max_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            decoder_attentions[di] = decoder_attention.data\n            topv, topi = decoder_output.data.topk(1)\n            if topi.item() == EOS_token:\n                decoded_words.append('<EOS>')\n                break\n            else:\n                decoded_words.append(output_lang.index2word[topi.item()])\n\n            decoder_input = topi.squeeze().detach()\n\n        return decoded_words, decoder_attentions[:di + 1]","dd357134":"def evaluateRandomly(encoder, decoder, n=10):\n    for i in range(n):\n        pair = random.choice(pairs)\n        print('>', pair[0])\n        print('=', pair[1])\n        output_words, attentions = evaluate(encoder, decoder, pair[0])\n        output_sentence = ' '.join(output_words)\n        print('<', output_sentence)\n        print('')","d66cc24e":"hidden_size = 256\nencoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\nattn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n\ntrainIters(encoder1, attn_decoder1, 75000, print_every=5000)","1989000c":"evaluateRandomly(encoder1, attn_decoder1)","7f53162d":"%matplotlib inline\noutput_words, attentions = evaluate(\n    encoder1, attn_decoder1, \"je suis trop froid .\")\nplt.matshow(attentions.numpy())","838af407":"def showAttention(input_sentence, output_words, attentions):\n    # colorbar\ub85c \uadf8\ub9bc \uc124\uc815\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(attentions.numpy(), cmap='bone')\n    fig.colorbar(cax)\n\n    # \ucd95 \uc124\uc815\n    ax.set_xticklabels([''] + input_sentence.split(' ') +\n                       ['<EOS>'], rotation=90)\n    ax.set_yticklabels([''] + output_words)\n\n    # \ub9e4 \ud2f1\ub9c8\ub2e4 \ub77c\ubca8 \ubcf4\uc5ec\uc8fc\uae30\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n    plt.show()\n\n\ndef evaluateAndShowAttention(input_sentence):\n    output_words, attentions = evaluate(\n        encoder1, attn_decoder1, input_sentence)\n    print('input =', input_sentence)\n    print('output =', ' '.join(output_words))\n    showAttention(input_sentence, output_words, attentions)\n\n\nevaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n\nevaluateAndShowAttention(\"elle est trop petit .\")\n\nevaluateAndShowAttention(\"je ne crains pas de mourir .\")\n\nevaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")","cb091bf3":"#### 2. Download and load data files <a class=\"fra-eng\" id=\"9\"><\/a>\n\nBefore writing the code, download the data provided by https:\/\/www.manythings.org\/anki\/\n","3f902676":"#### 8. Training and Evaluating <a class=\"fra-eng\" id=\"15\"><\/a>","bdae58a6":"[Back to Table of Contents](#0.1)\n\n[Back to fra-eng Table of Contents](#0.2)","0221bb88":"# \ud83d\udca5 Introduction\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$ \n<center><img src=\"https:\/\/i.ytimg.com\/vi\/d25rAmk0NVk\/maxresdefault.jpg\"\/ width=\"500\" height=\"500\" ><\/center>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$\n\nIn this notebook, we want to learn about Transformers'**Attention Mechanism**, which is used as a basic model for GPT and BERT. Despite being an important Mechanism for understanding state-of-the-art language models, tutorials for NLP beginners are difficult to find. Therefore, I try to create a notebook by combining various lecture materials and posts. Furthermore, we think that there is a lack of data utilizing Pytorch, a state-of-the-art framework, and we try to explain the overall process using Pytorch. The French-English translation process is utilized for easy access by NLP beginners.\n\nI hope this notebook will help you understand Attention Mechanism a lot.","c38b5445":"The full process for preparing the data is:\n\n* Read text file and split into lines, split lines into pairs\n* Normalize text, filter by length and content\n* Make word lists from sentences in pairs","fe0f5ca8":"#### 1. Loading Libraries <a class=\"fra-eng\" id=\"8\"><\/a>","50d26441":"#### 4. Modeling : Seq2Seq Model and Attention Decoder <a class=\"fra-eng\" id=\"11\"><\/a>","29840a61":"#### 5. Train Data Preparation <a class=\"fra-eng\" id=\"12\"><\/a>","618fab60":"To read the data file we will split the file into lines, and then split lines into pairs. The files are all English \u2192 Other Language, so if we want to translate from Other Language \u2192 English I added the reverse flag to reverse the pairs.","9cb319d9":"# 8. Conclusion <a class=\"anchor\" id=\"17\"><\/a>\n\nAttention Mechanism has been proposed in RNN to prevent deterioration in translation performance due to bottlenecks when sentence length is extended. Recently, self-attention concepts that parallelize them have emerged, and even models that use self-attention to perform better without RNN (Transformer, BERT, etc.) have been pouring out. In order to understand and learn the underlying skills of the latest models, I think it is essential to study Attention Mechanism well.\n\n[Back to Table of Contents](#0.1)\n\n[Back to fra-eng Table of Contents](#0.2)","894464a1":"* Attention Decoder\n\nAttention allows the decoder network to \u201cfocus\u201d on a different part of the encoder\u2019s outputs for every step of the decoder\u2019s own outputs. First we calculate a set of attention weights. These will be multiplied by the encoder output vectors to create a weighted combination. The result (called attn_applied in the code) should contain information about that specific part of the input sequence, and thus help the decoder choose the right output words.\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$ \n<center><img src=\"https:\/\/i.imgur.com\/1152PYf.png\"\/ width=\"300\" height=\"300\" ><\/center>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$\n\nCalculating the attention weights is done with another feed-forward layer attn, using the decoder\u2019s input and hidden state as inputs. Because there are sentences of all sizes in the training data, to actually create and train this layer we have to choose a maximum sentence length (input length, for encoder outputs) that it can apply to. Sentences of the maximum length will use all the attention weights, while shorter sentences will only use the first few.\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$ \n<center><img src=\"https:\/\/pytorch.org\/tutorials\/_images\/attention-decoder-network.png\"\/ width=\"300\" height=\"300\" ><\/center>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$","8dd53a23":"[Back to Table of Contents](#0.1)\n\n[Back to fra-eng Table of Contents](#0.2)","66213136":"[Back to Table of Contents](#0.1)\n\n[Back to fra-eng Table of Contents](#0.2)","f94553fb":"# 4. seq2seq with attention vs. traditional seq2seq <a class=\"anchor\" id=\"4\"><\/a>\n\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$ \n<center><img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcSBp7-TJBXCkCm91-SRfwloIMXgm_O9s9ZyYw&usqp=CAU\"\/ width=\"500\" height=\"500\" ><\/center>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$\n\nWe show remarkable performance on the final result graph. In the graph, 'RNNenc' stands for conventional RNN Encoder-Decoder without Attention applied, and 'RNNsearch' stands for RNN Encoder-Decoder with Attention applied. The numbers \"30\" and \"50\" mean that the network has been trained in one sentence, each consisting of words below that number. Finally, the horizontal axis of the graph represents the length of the test sentence and the vertical axis represents the BLEUS score, an indicator used to represent translation performance.\n\nFirst of all, comparing the existing RNNenc-30 and RNNsearch-30, we can see that significant improvements in translation performance have been made even if it is not a long sentence. This records higher performance than RNNenc-50 does. Secondly, comparing RNNenc-50 and RNNsearch-50 shows that, again, translation performance has been improved, and remarkably, RNNsearch-50 does not have a degradation problem with translation performance even with longer sentence lengths.\n\n[Back to Table of Contents](#0.1)","a0f556d7":"<a class=\"fra-eng\" id=\"0.2\"><\/a>\n**\u3141 Table of Contents**\n\n\ni.  [Loading Libraries](#8)\n\n\nii.  [Download and load data files](#9)\n\n\niii.  [Preparation for data preprocessing](#10)\n\n\niv. [Modeling : Seq2Seq Model and Attention Decoder](#11)\n\n   - Encoder, Decoder, Attention Decoder \n   \n\nv. [Train Data Preparation](#12)\n\n\nvi. [Training the Model](#13)\n\n\nvii. [Evaluation](#14)\n\n\nviii. [Training and Evaluating](#15) \n\n\nix. [Visualize the Attention Process](#16)","d4acd8cb":"Create a class to store downloaded data as word->index, index->words and replace rare words.","969dc47a":"# 9. References <a class=\"anchor\" id=\"18\"><\/a>\n\n* NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE : https:\/\/arxiv.org\/pdf\/1409.0473.pdf\n\n* Attention Mechanism Cleanup Notes : https:\/\/wikidocs.net\/22893\n\n* Attention Mechanism blog1 : http:\/\/blog.naver.com\/PostView.nhn?blogId=ckdgus1433&logNo=221608376139\n\n* Attention Mechanism blog2 : https:\/\/sy-programmingstudy.tistory.com\/14\n\n* Teacher Forcing : https:\/\/m.blog.naver.com\/PostView.nhn?blogId=sooftware&logNo=221790750668&categoryNo=16&proxyReferer=https:%2F%2Fwww.google.com%2F\n\n* Attention Mechanism YouTube : https:\/\/www.youtube.com\/watch?v=WsQLdu2JMgI&t=364s\n\n* Attention Mechanism using Tensorflow YouTube : https:\/\/www.youtube.com\/watch?v=aUsGQaqYYBk&t=3202s\n\n* NLP FROM SCRATCH: TRANSLATION WITH A SEQUENCE TO SEQUENCE NETWORK AND ATTENTION : https:\/\/pytorch.org\/tutorials\/intermediate\/seq2seq_translation_tutorial.html#nlp-from-scratch-translation-with-a-sequence-to-sequence-network-and-attention\n\n* fra-eng data : https:\/\/www.manythings.org\/anki\/","5b4e0c92":"You can see that the leftmost loss decreases as the learning progresses. We also show fast learning time on a large number of 75,000 data.","c9a6b4d6":"# 3. Attention's process <a class=\"anchor\" id=\"3\"><\/a>\n\nOK! If you have a rough idea of Attention, let's look at the Attention process. There are many different types of Attention, and let's understand it through Dot-Product Attention, which is the most informally easy to understand formula. Among the attractions used in seq2seq, the difference between dot-product attention and other attention is largely the difference in intermediate formulas, and the mechanism itself is almost similar. I'll explain it again after various Attentions.\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$ \n<center><img src=\"https:\/\/wikidocs.net\/images\/page\/22893\/dotproductattention1_final.PNG\"\/ width=\"500\" height=\"500\" ><\/center>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$\n\nThe third LSTM cell in the decoder shows the use of an attention mechanism when predicting output words. Suppose that the first and second LSTM cells in the decoder have already gone through the attention mechanism to predict je and suis. Before explaining the attention mechanism in detail, let's first get a sense of the overall picture above. The third LSTM cell in the decoder wants to refer to the information of all the input words in the encoder once again to predict the output words. The description of the intermediate process is currently omitted, and what is noteworthy here is the softmax function of the encoder.\n\nThe resulting value of the softmax function is a numerical value of how helpful each of the words I, am, a, and student is when predicting the output word. The figure above shows the magnitude of the resultant value of the softmax function in the size of a red rectangle. The larger the rectangle, the larger the helpful it is. When each input word is quantified and measured to help you predict the decoder, it is sent to the decoder with one piece of information. In the figure above, the green triangle corresponds to this. As a result, the decoder is more likely to predict output words more accurately. Now that we have a holistic sense of the attention mechanism.\n\nLet's look at the following as an easier example.\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$ \n<center><img src=\"https:\/\/img1.daumcdn.net\/thumb\/R1280x0\/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FG1u0d%2FbtqCl9U65jT%2F3w2vCPrTIRU0a14bWNooo1%2Fimg.png\"\/ width=\"400\" height=\"400\" ><\/center>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$\n\nFirst of all, there is a **Fully connected network (FC)**. We utilize $h_1$, $h_2$, ... $h_N$ (the states of all RNN cells from the encoder). And I put in the final H3. Because there is no value from the decoder under the current circumstances, we simply put in the state value that existed before. Output values from the fully connected network are $s_1$, $s_2$, and $s_3$, the scores on the RNN cell in each encoder.\n\nNow that we have a holistic sense of the attention mechanism, let's take a closer look at the attention mechanism.\n\n[Back to Table of Contents](#0.1)\n\n### 1. Find the attention score <a class=\"anchor\" id=\"3.1\"><\/a>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$ \n<center><img src=\"https:\/\/wikidocs.net\/images\/page\/22893\/dotproductattention2_final.PNG\"\/ width=\"500\" height=\"500\" ><\/center>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$\n\nIf the time step of the encoder is 1, 2, and N respectively, then the hidden state of time step of the encoder is  $h_1$, $h_2$, ... $h_N$ respectively. Let the decoder's hidden state at the time step t of the decoder be S. We also assume here that the hidden state of the encoder and the hidden state of the decoder are the same dimensions. For the above figure, the encoder's hidden state and the decoder's hidden state are the same dimension 4.\n\nThe attention mechanism requires a new value called Attention Value for output word prediction. Let's define the attention value to predict the t-th word as $a_t$.\n\nAn attention score is a score value that determines how similar each of the hidden states of the encoder is to the hidden state $s_t$ of the decoder at this point in time to predict the word at point t of the current decoder.\n\nDot-Product Attention will transpose $s_t$, perform each hidden state and dot product to obtain the value of this score. That is, all attention score values are scalar. For example, the method of calculating the attention score of $s_t$ and the ith hidden state of the encoder is shown below.\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$ \n<center><img src=\"https:\/\/wikidocs.net\/images\/page\/22893\/i%EB%B2%88%EC%A7%B8%EC%96%B4%ED%85%90%EC%85%98%EC%8A%A4%EC%BD%94%EC%96%B4_final.PNG\"\/ width=\"300\" height=\"300\" ><\/center>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$\n\nDefine the attention score function as follows :\n\n$$\n\\operatorname{score}(s_i, h_i) = s_{t}^Th_{i}\n$$\n\nLet's define the collection value of all hidden attention scores of $s_t$, and encoder as $e^t$. The formula for $e^t$ is as follows.\n\n$$\n\\operatorname{e^t} = [s_{t}^Th_{1}, \\operatorname{...}, s_{t}^Th_{N}]\n$$\n\n[Back to Table of Contents](#0.1)\n\n\n### 2. Attention distribution is obtained through the softmax function <a class=\"anchor\" id=\"3.2\"><\/a>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$ \n<center><img src=\"https:\/\/wikidocs.net\/images\/page\/22893\/dotproductattention3_final.PNG\"\/ width=\"500\" height=\"500\" ><\/center>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$\n\nBy applying the softmax function to $e^t$, you obtain a probability distribution that adds all values to 1. This is known as the Attention Distribution, and each value is known as the Attention Weight. In addition, the size of the attention weights in the hidden state of each encoder was visualized through the size of the rectangle. That is, the greater the attention weight, the larger the rectangle.\n\nLet me look at an easier example.\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$ \n<center><img src=\"https:\/\/img1.daumcdn.net\/thumb\/R1280x0\/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdnaBKf%2FbtqCsIuHK8B%2FGKHuV1AgOI9e8bKt90kFp0%2Fimg.png\"\/ width=\"400\" height=\"400\" ><\/center>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$\n\nThe above figure shows the probability value of taking the softmax value of \"I love you\". 90% for \"I\", 0% for \"love\", and 10% for \"you\". This is called Attention weight and these values indicate how much we will focus.\n\nWhen the attention distribution, which is a collection of attention weights at point t of the decoder, is called $a^t$, defining $a^t$ as an expression :\n\n$$\na_t = \\text{softmax}(e^t)\n$$\n\n\n[Back to Table of Contents](#0.1)\n\n\n### 3. Attention Value is obtained by weighting the attention weights and the hidden state of each encoder <a class=\"anchor\" id=\"3.3\"><\/a>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$ \n<center><img src=\"https:\/\/wikidocs.net\/images\/page\/22893\/dotproductattention4_final.PNG\"\/ width=\"500\" height=\"500\" ><\/center>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$\n\nNow it's time to combine the information we've prepared so far into one. To obtain the final result of the attention, we multiply the hidden state of each encoder and the attention weights, and finally add them all. In summary, you might say weighed Sum. Below is the final result of the attention, i.e., an expression for the attention value $a_t$, the output of the attention function.\n\n$$\na_t = \\sum_{i=1}^{N} a_{i}^{t}h_i\n$$\n\nThe attention value $a_t$ is also called the **context vector** because it often contains the context of the encoder.\n\nLet me give you an easy example.\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$ \n<center><img src=\"https:\/\/img1.daumcdn.net\/thumb\/R1280x0\/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbCGz9v%2FbtqCqNjebAu%2F77pnvKbjaNeIrPj3yM0MQ0%2Fimg.png\"\/ width=\"400\" height=\"400\" ><\/center>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$\n\ncv1 stands for context vector1. **cv1 = $h_1$ * 0.9 + $h_2$ * 0 + $h_3$ * 0.1.** In other words, the first context vector is I for 90% and you for 10%.\n\n[Back to Table of Contents](#0.1)\n\n\n### 4. Concatenate : connects the attention value with the hidden state at time t of the decoder <a class=\"anchor\" id=\"3.4\"><\/a>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$ \n<center><img src=\"https:\/\/wikidocs.net\/images\/page\/22893\/dotproductattention5_final_final.PNG\"\/ width=\"500\" height=\"500\" ><\/center>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$\n\nYou have now obtained the final value of the attention function, the attention value $a_t$. Earlier, we introduced the following expression as an expression to determine the hidden state at point t with the attention mechanism: In fact, when the attention value is obtained, the attention mechanism concatenates $a_t$ with $s_t$ to form a vector. Let's define this as $v_t$. By using this $v_t$ as the input for the $\\hat{y}$ prediction operation, we can leverage the information from the encoder to better predict $\\hat{y}$. **This is at the heart of the attention mechanism.**\n\nLet's take another easy example.\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$ \n<center><img src=\"https:\/\/img1.daumcdn.net\/thumb\/R1280x0\/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FCpNnB%2FbtqCrPOozvr%2FEXUeB68QIoA2ucSkvW5K0K%2Fimg.png\"\/ width=\"400\" height=\"400\" ><\/center>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$\n\nIn the picture above, the output 'Nan' is shown. What will the second word come from? The second word is that the current state value in the decoder goes into FC. **What we need to note is that $h_1$, $h_2$, and $h_3$ are always used.**\n\n[Back to Table of Contents](#0.1)\n\n\n### 5. Calculating \ud835\udc60\u0303 the input to output layer operation <a class=\"anchor\" id=\"3.5\"><\/a>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$ \n<center><img src=\"https:\/\/wikidocs.net\/images\/page\/22893\/st.PNG\"\/ width=\"500\" height=\"500\" ><\/center>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$\n\nIn the paper, we added another neural network operation before sending $v_t$ directly to the output layer. Multiply with the weight matrix and pass through the hyperbolic tangent function to obtain $\\hat{s}_t$, a new vector for output layer operations. In seq2seq, which does not use the attention mechanism, the input of the output layer is $s_t$ hidden at point t, while in the attention mechanism, the input of the output layer is $\\hat{s}_t$.\n\nExpressing this as an expression: In expressions, $W_c$ is the learnable weight matrix, and $b_c$ is the bias. Deflection is omitted in the figure.\n\n$$\n\\tilde{s}_{t} = \\tanh(\\mathbf{W_{c}}[{a}_t;{s}_t] + b_{c})\n$$\n\n[Back to Table of Contents](#0.1)\n\n\n### 6. Use \ud835\udc60\u0303 as the input for the output layer <a class=\"anchor\" id=\"3.6\"><\/a>\n\nUse $\\hat{s}_t$ as the input to the output layer to obtain the prediction vector.\n\n$$\n\\widehat{y}_t = \\text{Softmax}\\left( W_y\\tilde{s}_t + b_y \\right)\n$$","21b660e3":"The file is all Unicode, which converts Unicode characters to ASCII for simplicity, makes all characters lowercase, and erases most punctuation.","0262345f":"Since there are a lot of example sentences and we want to train something quickly, we\u2019ll trim the data set to only relatively short and simple sentences. Here the maximum length is 10 words (that includes ending punctuation) and we\u2019re filtering to sentences that translate to the form \u201cI am\u201d or \u201cHe is\u201d etc. (accounting for apostrophes replaced earlier).","e14a6027":"#### 7. Evaluation <a class=\"fra-eng\" id=\"14\"><\/a>\n\n\nEvaluation is mostly the same as training, but there are no targets so we simply feed the decoder\u2019s predictions back to itself for each step. Every time it predicts a word we add it to the output string, and if it predicts the EOS token we stop there. We also store the decoder\u2019s attention outputs for display later.","8d6ef283":"# 6. Additional Concepts: Teacher Forcing <a class=\"anchor\" id=\"6\"><\/a>\n\nTeacher Forcing is the technique where the target word is passed as the next input to the decoder\n\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$ \n<center><img src=\"https:\/\/mblogthumb-phinf.pstatic.net\/MjAyMDAxMzFfMTcg\/MDAxNTgwMzk4NTc3MzE2.rfhepBTuNa7UuGl7t4O2AAtVytd3Yd2d731im7KZ_jwg.kO58sY_DL9sBLx1LlZzq5A3hAplPA0gJA-6q4ZDr7Owg.PNG.sooftware\/image.png?type=w800\"\/ width=\"500\" height=\"500\" ><\/center>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$\n\n**Advantages of Teacher Forcing : Learning fast**\n\nIn the early stages of learning, the model's prediction performance is poor, so without using Teacher Forcing, Hidden State values are updated based on incorrect prediction values, which results in a slower learning rate for the model.\n\n**Disadvantages of Teacher Forcing : Exposure Bias Problem**\n\nGround Truth cannot be provided in the Inference process. Therefore, the model must continue to make predictions based on its own output values from all phases. Such differences in learning and inference stages exist, which can reduce the performance and stability of the model.\n\n[Back to Table of Contents](#0.1)","980dfb4b":"#### 3. Preparation for data preprocessing <a class=\"fra-eng\" id=\"10\"><\/a>\n\nUnlike dozens of characters in a single language, the encoding vector is very large because there are so many words in the translation. Therefore, we will refine the data to use only thousands of words per language using the method.","76ca39c4":"Plotting is done with matplotlib, using the array of loss values plot_losses saved while training.\n\n[Back to Table of Contents](#0.1)\n\n[Back to fra-eng Table of Contents](#0.2)","01e42d09":"* The Encoder\n\nThe encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word.\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$ \n<center><img src=\"https:\/\/pytorch.org\/tutorials\/_images\/encoder-network.png\"\/ width=\"300\" height=\"300\" ><\/center>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$","0577269f":"We can evaluate random sentences from the training set and print out the input, target, and output to make some subjective quality judgements","73e09fa4":"# 1. How do I translate the sentence by machine? : Appearance of Attention Mechanism <a class=\"anchor\" id=\"1\"><\/a>\n\nThe seq2seq model compresses the input sequence into one fixed-size vector representation, called the context vector, through which the decoder produces the output sequence. Through this, each word can be translated.\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$ \n<center><img src=\"https:\/\/img1.daumcdn.net\/thumb\/R1280x0\/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FprTT1%2FbtqChF6Z2BM%2FyKmITHEga1J0ZYFWEMr8Y0%2Fimg.png\"\/ width=\"500\" height=\"500\" ><\/center>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$\n\nLet's take a look at the seq2seq model using RNN. \"I\" comes first, \"love\" comes next, and \"you\" comes next. Consequently, the final RNN cell states \"I love you\". This vector is called the **context vector**.\n\n**Encoder** eventually creates a context vector by receiving each word sequentially. **Decoder** starts machine translation from this context vector. From start to end, the words in it are translated immediately.\n\nHowever, there are two main problems with seq2seq models based on these RNNs.\n\n   1. **trying to compress all the information into one fixed-size vector results in information loss.** \n   2. **there exists the Vanishing Gradient problem, which is a chronic problem of RNN.**\n\nIn other words, in the field of machine translation, if the input sentence is long, the translation quality is poor. As an alternative to this, Attention Mechanism emerges to compensate for the inaccuracy of the output sequence when the input sequence is prolonged.\n\n[Back to Table of Contents](#0.1)","a3d7bcbd":"# 5. Different types of attention <a class=\"anchor\" id=\"5\"><\/a>\n\nThere are a variety of attention types that can be used in seq2seq + attention models, but we mentioned earlier that the difference between dot-product attention and other attention is the difference in intermediate formulas. The intermediate formula here refers to the attention score function. The reason why the attention I learned above is dot-product attention is because the way to get the attention score was internal.\n\nThere are several ways to obtain an attention score, and the different types of attention score functions currently presented are as follows :\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$ \n<center><img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcSNhQFLgoJEvFvrdCRwryycEW_td77shDbn2w&usqp=CAU\"\/ width=\"600\" height=\"300\" ><\/center>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$\n\n* $s_t$ : querys (Hidden state of the decoder cell at point t)\n* $h_i$ : keys (Encoder cell hidden at all points in time)\n* $W_a$, $W_b$ : Learnable Weight Matrix\n\n[Back to Table of Contents](#0.1)","e14bf3f5":"You can see that the file has been downloaded normally. If it's not downloaded properly, check your internet connection !\n\n[Back to Table of Contents](#0.1)\n\n[Back to fra-eng Table of Contents](#0.2)","64ad19ce":"#### 8. Visualize the Attention Process <a class=\"fra-eng\" id=\"16\"><\/a>\n\nA useful property of the attention mechanism is its highly interpretable outputs. Because it is used to weight specific encoder outputs of the input sequence, we can imagine looking where the network is focused most at each time step.","c1016563":"# 2. What is Attention's idea? <a class=\"anchor\" id=\"2\"><\/a>\n\nThe basic idea of Attention is that at every time step that the decoder predicts output words, it references the entire input sentence from the encoder. However, instead of referring to all the input sentences at the same rate, you will focus more on the parts of the input words that are related to the words you need to predict at that point.\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$ \n<center><img src=\"https:\/\/img1.daumcdn.net\/thumb\/R1280x0\/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FYzQNQ%2FbtqCe5rLdtM%2FKuUvWxOTdTLlOte7lVm4i1%2Fimg.png\"\/ width=\"500\" height=\"500\" ><\/center>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$\n\nIn other words, each state from the encoder, each RNN cell, is utilized. We can summarize the advantages of Attention in two ways here.\n\n   1. **It is not a fixed size context vector. The problem that comes from one fixed-size context vector is to refresh the context vector for each state.** \n   2. **Of all the states in Encoder, we can design a mechanism that can only focus on the words we need to focus on.**\n   \n   \n[Back to Table of Contents](#0.1)","b5c6621c":"# 7. French-English Machine Translation Tutorial - Using Pytorch <a class=\"anchor\" id=\"7\"><\/a>\n\nWe implement a simple machine translation project to help NLP beginners understand the attention mechanism. Many examples using Tensorflow exist, but we recently wrote them with a popular framework, Pytorch.","5591d291":"<a class=\"anchor\" id=\"0.1\"><\/a>\n#  \ud83d\udca5 Table of Contents\ni.  [How do I translate the sentence by machine? : Appearance of Attention Mechanism](#1)\n\nii.  [What is Attention's idea?](#2)\n\niii.  [Attention's process](#3)\n\n   - [Find the attention score](#3.1)\n   - [Attention distribution is obtained through the softmax function](#3.2)\n   - [Attention Value is obtained by weighting the attention weights and the hidden state of each encoder](#3.3)\n   - [Concatenate : connects the attention value with the hidden state at time t of the decoder](#3.4)\n   - [Calculating  **\ud835\udc60\u0303** the input to output layer operation](#3.5)\n   - [Use **\ud835\udc60\u0303** as the input for the output layer](#3.6)\n\n\niv. [seq2seq with attention vs traditional seq2seq](#4)\n\n\nv. [Different types of attention](#5)\n\n\nvi. [Additional Concepts : Teacher Forcing](#6)\n\n\nvii. [French-English Machine Translation Tutorial: Using Pytorch](#7)\n\n\nviii. [conclusion](#17)\n\n\nix. [References](#18) \n","b550e48b":"For a better viewing experience we will do the extra work of adding axes and labels","97902a92":"To train, for each pair we will need an input tensor (indexes of the words in the input sentence) and target tensor (indexes of the words in the target sentence). While creating these vectors we will append the EOS token to both sequences.","0e49d70e":"* The Decoder\n\nThe decoder is another RNN that takes the encoder output vector(s) and outputs a sequence of words to create the translation.\n\nIn the simplest seq2seq decoder we use only last output of the encoder. This last output is sometimes called the context vector as it encodes context from the entire sequence. This context vector is used as the initial hidden state of the decoder.\n\nAt every step of decoding, the decoder is given an input token and hidden state. The initial input token is the start-of-string <SOS> token, and the first hidden state is the context vector (the encoder\u2019s last hidden state).\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$ \n<center><img src=\"https:\/\/pytorch.org\/tutorials\/_images\/decoder-network.png\"\/ width=\"300\" height=\"300\" ><\/center>\n\n$\\;\\;\\;\\;\\;\\;\\;\\;$","d582b80c":"The whole training process looks like this:\n\n* Start a timer\n* Initialize optimizers and criterion\n* Create set of training pairs\n* Start empty losses array for plotting\n\nThen we call train many times and occasionally print the progress (% of examples, time so far, estimated time) and average loss.","71cce3e8":"This is a helper function to print time elapsed and estimated time remaining given the current time and progress %.","31baa999":"#### 6. Train Data Preparation <a class=\"fra-eng\" id=\"13\"><\/a>\n\nTo train we run the input sentence through the encoder, and keep track of every output and the latest hidden state. Then the decoder is given the <SOS> token as its first input, and the last hidden state of the encoder as its first hidden state.\n\n\u201cTeacher forcing\u201d is the concept of using the real target outputs as each next input, instead of using the decoder\u2019s guess as the next input. Using teacher forcing causes it to converge faster but when the trained network is exploited, it may exhibit instability.\n\nYou can observe outputs of teacher-forced networks that read with coherent grammar but wander far from the correct translation - intuitively it has learned to represent the output grammar and can \u201cpick up\u201d the meaning once the teacher tells it the first few words, but it has not properly learned how to create the sentence from the translation in the first place.\n\nBecause of the freedom PyTorch\u2019s autograd gives us, we can randomly choose to use teacher forcing or not with a simple if statement. Turn teacher_forcing_ratio up to use more of it.","8f92cca6":"<h1><center>Thanks for reading<\/center><\/h1>\n<h3><center>Pls, \"UPVOTE\" if this code helped ! \ud83d\udc40<\/center><\/h3>"}}