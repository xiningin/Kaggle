{"cell_type":{"b19f0e82":"code","892482f3":"code","3f802ccc":"code","c605e3f7":"code","e0047a6b":"code","b63b8479":"code","b38f3386":"code","b33530d8":"code","b067491d":"code","40aeac4c":"code","797e8b27":"code","c62e658c":"code","aa66b14a":"code","a35ee435":"markdown","0021b32c":"markdown","82d178a8":"markdown","46b25f96":"markdown","c5588f3c":"markdown","e7cab422":"markdown","4e3d27ae":"markdown"},"source":{"b19f0e82":"!pip install -q efficientnet","892482f3":"import os\nimport re\n\nimport numpy as np\nimport pandas as pd\nimport math\n\nfrom matplotlib import pyplot as plt\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\n\nimport efficientnet.tfkeras as efn\n\nfrom kaggle_datasets import KaggleDatasets","3f802ccc":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","c605e3f7":"# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Data access\nGCS_PATH = KaggleDatasets().get_gcs_path('siim-isic-melanoma-classification')\n\n# Configuration\nDEBUG = False\nN_FOLD = 4\nEPOCHS = 1 if DEBUG else 7\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\nIMAGE_SIZE = [1024, 1024]","e0047a6b":"sub = pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/sample_submission.csv')\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/tfrecords\/train*.tfrec')\nTRAINING_FILENAMES_LIST = [TRAINING_FILENAMES[:4], TRAINING_FILENAMES[4:8], TRAINING_FILENAMES[8:12], TRAINING_FILENAMES[12:]]\nprint(TRAINING_FILENAMES_LIST)","b63b8479":"def decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image\n\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.int64),\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = tf.cast(example['target'], tf.int32)\n    return image, label\n\ndef read_unlabeled_tfrecord(example):\n    UNLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    idnum = example['image_name']\n    return image, idnum\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n    dataset = dataset.with_options(ignore_order)\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n    return dataset\n\ndef data_augment(image, label):\n    image = tf.image.random_flip_left_right(image)\n    return image, label\n\ndef get_training_dataset(train_files):\n    dataset = load_dataset(train_files, labeled=True)\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef get_validation_dataset(valid_files, ordered=False):\n    dataset = load_dataset(valid_files, labeled=True, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    #dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef get_test_dataset(test_files, ordered=False):\n    dataset = load_dataset(test_files, labeled=False, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","b38f3386":"def get_model():\n    \n    with strategy.scope():\n        model = tf.keras.Sequential([\n            efn.EfficientNetB3(\n                input_shape=(*IMAGE_SIZE, 3),\n                weights='imagenet',\n                include_top=False\n            ),\n            L.GlobalAveragePooling2D(),\n            L.Dense(1, activation='sigmoid')\n        ])\n\n    model.compile(\n        optimizer='adam',\n        loss = 'binary_crossentropy',\n        #metrics=['accuracy'],\n        metrics=[tf.keras.metrics.AUC()],\n    )\n    \n    return model","b33530d8":"def train_model(fold, debug=False):\n    if debug:\n        valid = TRAINING_FILENAMES_LIST[fold][0:1]\n        train = TRAINING_FILENAMES_LIST[fold-1][0:1]\n    else:\n        valid = TRAINING_FILENAMES_LIST[fold]\n        train = sum([TRAINING_FILENAMES_LIST[i] for i in range(N_FOLD) if i not in [fold]], [])\n    num_train = count_data_items(train)\n    steps_per_epoch = num_train \/\/ BATCH_SIZE\n    model = get_model()\n    saving_callback = tf.keras.callbacks.ModelCheckpoint(f\"fold{fold}_model.h5\", verbose=1, \n                                                         save_weights_only=True, save_best_only=True)\n    history = model.fit(\n            get_training_dataset(train), \n            steps_per_epoch=steps_per_epoch,\n            epochs=EPOCHS,\n            callbacks=[saving_callback],\n            validation_data=get_validation_dataset(valid),\n            verbose=1,\n    )\n    #model.save(f\"fold{fold}_model.h5\")","b067491d":"for fold in range(N_FOLD):\n    train_model(fold, debug=DEBUG)","40aeac4c":"def get_model():\n    \n    with strategy.scope():\n        model = tf.keras.Sequential([\n            efn.EfficientNetB3(\n                input_shape=(*IMAGE_SIZE, 3),\n                weights=None,\n                include_top=False\n            ),\n            L.GlobalAveragePooling2D(),\n            L.Dense(1, activation='sigmoid')\n        ])\n    \n    return model","797e8b27":"from tqdm import tqdm\n\noof_df = pd.DataFrame()\n\ntk0 = tqdm(range(N_FOLD), total=N_FOLD)\n\nfor fold in tk0:\n    if DEBUG:\n        test_files = TRAINING_FILENAMES_LIST[fold-1][0:1]\n    else:\n        test_files = TRAINING_FILENAMES_LIST[fold]\n    num_test = count_data_items(test_files)\n    test_ds = get_test_dataset(test_files, ordered=True)\n    test_images_ds = test_ds.map(lambda image, idnum: image)\n    model = get_model()\n    model.load_weights(f\"fold{fold}_model.h5\")\n    probabilities = model.predict(test_images_ds)\n    test_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\n    test_ids = next(iter(test_ids_ds.batch(num_test))).numpy().astype('U')\n    _oof_df = pd.DataFrame({'image_name': test_ids, 'oof': np.concatenate(probabilities)})\n    oof_df = pd.concat([oof_df, _oof_df])","c62e658c":"train_df = pd.read_csv('..\/input\/siim-isic-melanoma-classification\/train.csv')\ntrain_df = train_df.merge(oof_df, on='image_name')\ntrain_df.to_csv('oof_df.csv', index=False)","aa66b14a":"from sklearn.metrics import roc_auc_score\n\nscore = roc_auc_score(train_df['target'].values, train_df['oof'].values)\nprint(f'CV AUC: {score}')","a35ee435":"# Library","0021b32c":"# About this kernel\n- TPU tensorflow EfficientNetB3 starter code\n- 4 folds\n- References are below\n- https:\/\/www.kaggle.com\/mgornergoogle\/getting-started-with-100-flowers-on-tpu\n- https:\/\/www.kaggle.com\/xhlulu\/alaska2-efficientnet-on-tpus ","82d178a8":"# Get Model into TPU","46b25f96":"# Prepare Data & Loader","c5588f3c":"# TPU Strategy and other configs ","e7cab422":"# Training","4e3d27ae":"# CV"}}