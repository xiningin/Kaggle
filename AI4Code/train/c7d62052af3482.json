{"cell_type":{"b965e37c":"code","52e2f095":"code","04655d59":"code","0b799a7d":"code","592a560c":"code","c004e3dc":"code","d4ee0e0d":"code","3b9ae92f":"code","c48397d5":"code","92eba0d0":"code","f72ff621":"code","0424118a":"code","5a8baca6":"code","e0b6cc9e":"code","4dca3c79":"code","24997928":"code","424b8f77":"code","864a495f":"code","b897005f":"code","23ba2512":"code","329817b6":"code","1f2da612":"code","b3145e1b":"code","45350b30":"code","4f6f93da":"code","c2503a28":"code","932a54ed":"code","38bb16bc":"markdown","789c43f4":"markdown","7bb11fd7":"markdown","2106d5b5":"markdown","01e05e10":"markdown","fd70b33d":"markdown","f0a3025d":"markdown","e5ef0c69":"markdown","d10a021c":"markdown","f4f991ec":"markdown","7432e4b7":"markdown","e215f57d":"markdown","c02304a4":"markdown","429f1ac5":"markdown","a136ed00":"markdown","6a28a850":"markdown","108bbe8c":"markdown","e7bc216a":"markdown","9c2aba9a":"markdown"},"source":{"b965e37c":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import GridSearchCV, KFold ,cross_val_score\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import normalize \nfrom sklearn.feature_selection import SelectKBest, f_classif\n\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score  \n\nimport tensorflow as tf\nfrom tensorflow.python import keras\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import to_categorical\nfrom tensorflow.python.keras.utils import np_utils\nfrom tensorflow.python.keras.utils.np_utils import to_categorical\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport matplotlib as mpl\n%matplotlib inline\nmpl.style.use( 'ggplot' )\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nsns.set(context=\"notebook\", palette=\"dark\", style = 'whitegrid' , color_codes=True)","52e2f095":"train = pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_train.csv')\ntest = pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_test.csv')\nprint(f'Train:\\t {train.shape} \\nTest:\\t {test.shape}')\n      ","04655d59":"# check for null values\nprint(f'Are there null values? {train.isnull().any().sum()>0 | test.isnull().any().sum()>0}')\n# verifying the data is balanced\ntrain['label'].value_counts()\ntest['label'].value_counts()","0b799a7d":"#defining the list for labels\nclass_names = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\nlabels = {0 : \"T-shirt\/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\",\n          5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle Boot\"}","592a560c":"# Visualize some random pictures and their labels\nfig, axes = plt.subplots(5, 5, figsize = (6,6))\nfor row in axes:\n    for axe in row:\n        index = np.random.randint(60000)\n        img = train.drop('label', axis=1).values[index].reshape(28,28)\n        cloths = train['label'][index]\n        axe.imshow(img, cmap='gray')\n        axe.set_title(class_names[cloths])\n        axe.set_axis_off()","c004e3dc":"x=train.drop('label', axis=1)\n# finding pixels that are always black\ntemp_dic ={}\nfor col in x.columns:\n    count = 0\n    for val in x[col]:\n        if val > 0 :\n            count+=1\n    temp_dic[col]=count\nsort_dic = sorted(temp_dic.items(), key=lambda x: x[1])\n\n# present the least used pixels\nprint('The least used pixels are:')\nfor i in range(10):\n    print(f'{sort_dic[i][0]:<10} {sort_dic[i][1]:>10}')  ","d4ee0e0d":"#presenting heatmaps for pixels usage per category\nfig, axes = plt.subplots(3,4, figsize = (6,6))\ni = 0\nfor row in axes:\n    for axe in row:\n        if i<10:\n            selected_category = train.loc[train['label'] == i].drop('label', axis=1)\n            sns.heatmap(pd.DataFrame(selected_category).mean().values.reshape(28, 28), cmap='gray_r',cbar=False,ax=axe)\n            axe.set_title(class_names[i])\n            axe.set_axis_off()\n        else:\n            axe.axis('off')\n        i+=1","3b9ae92f":"df = train.copy()\ndf_test = test.copy()\ndf.label.unique()\nX_train = df.iloc[:,1:]\ny_train = df.iloc[:,0]","c48397d5":"pca = PCA(n_components=2)\nX_r = pca.fit(X_train).transform(X_train)\nplt.style.use('fivethirtyeight')\nfig, axarr = plt.subplots(1, 2, figsize=(8, 4))\nsns.heatmap(pca.components_[0, :].reshape(28, 28), ax=axarr[0], cmap='gray_r')\nsns.heatmap(pca.components_[1, :].reshape(28, 28), ax=axarr[1], cmap='gray_r')\naxarr[0].set_title(\"{0:.2f}% Explained Variance\".format(pca.explained_variance_ratio_[0]*100),fontsize=12)\naxarr[1].set_title(\"{0:.2f}% Explained Variance\".format(pca.explained_variance_ratio_[1]*100),fontsize=12)\naxarr[0].set_aspect('equal')\naxarr[1].set_aspect('equal')\nplt.suptitle('2-Component PCA');","92eba0d0":"# PCA\nlabel = 2\nX_train_label = X_train[y_train==label]\n\nclass PCAOutlierRemover:\n    def __init__(self, n_components=10, k_outliers=5):\n        self.n_components = n_components\n        self.k_outliers = k_outliers\n\n    def fit(self, X, y=None):\n        self.pca = PCA(n_components=self.n_components)\n        self.pca.fit(X) \n\n        self.errors_sorted_ = {}\n        for label in range(10):\n            X_label = X[y_train==label]\n            X_reduced = self.pca.transform(X_label)                                    # NumPy array [N x n_components]\n            X_reconstructed = self.pca.inverse_transform(X_reduced)                    # NumPy array [N x k_outliers]\n            X_reconstructed_df = pd.DataFrame(X_reconstructed,\n                                            index=X_label.index)                       # pandas DataFrame [N x k_outliers]\n            errors = ((X_label - X_reconstructed)**2).sum(axis=1)                      # pandas Series [N x 1, indexed by X.index]\n            self.errors_sorted_[label] = errors.iloc[errors.argsort()]                 # pandas Series [N x 1, indexed by X.index] \n        return self\n\n    def transoform(self, X):\n        X_without_outliers = []\n        for label in range(10):\n            X_label = X[y_train==label]\n            self.without_k_worst_ix = self.errors_sorted_[label][:-self.k_outliers].index.values        # NumPy array [k_outliers x 1]\n            X_without_outliers.append(X_label.loc[self.without_k_worst_ix, :])\n        return pd.concat(X_without_outliers, axis=0)\n    \n    \nmy_pca_remover = PCAOutlierRemover(n_components=10, k_outliers=5)\nmy_pca_remover.fit(X_train)\nX_train_without_pca_outliers = my_pca_remover.transoform(X_train)\nX_train_without_pca_outliers.shape","f72ff621":"label = 3 #selected category number\nX_train_label = X_train[y_train==label]\ny_train_kb = y_train==label\n\nclass SKBOutlierRemover:\n    def __init__(self, score_func=f_classif ,k=20 , y=y_train_kb, k_outliers=5):\n        self.score_func = score_func\n        self.k_outliers = k_outliers\n        self.k= k\n        self.y= y\n\n    def fit(self, X, y=None):\n        self.kbest = SelectKBest(score_func=self.score_func , k=self.k)             #commented out the [y=self.y] that was passed to the SelectKBest - not needed\n        self.kbest.fit(X, y)\n\n        self.errors_sorted_ = {}\n        for label in range(10):\n            X_label = X[y_train==label]\n            X_reduced = self.kbest.transform(X_label)                                    # NumPy array [N x n_components]\n            X_reconstructed = self.kbest.inverse_transform(X_reduced)                    # NumPy array [N x k_outliers]\n            X_reconstructed_df = pd.DataFrame(X_reconstructed,\n                                            index=X_label.index)                       # pandas DataFrame [N x k_outliers]\n            errors = ((X_label - X_reconstructed)**2).sum(axis=1)                      # pandas Series [N x 1, indexed by X.index]\n            self.errors_sorted_[label] = errors.iloc[errors.argsort()]                 # pandas Series [N x 1, indexed by X.index]\n        return self\n\n    def transform(self, X):\n        X_without_outliers = []\n        for label in range(10):\n            X_label = X[y_train==label]\n            self.without_k_worst_ix = self.errors_sorted_[label][:-self.k_outliers].index.values        # NumPy array [k_outliers x 1]\n            X_without_outliers.append(X_label.loc[self.without_k_worst_ix, :])\n        return pd.concat(X_without_outliers, axis=0)\n   \nmy_skb_remover = SKBOutlierRemover(score_func=f_classif, k=20, y=y_train.loc[X_train_without_pca_outliers.index])\nmy_skb_remover.fit(X=X_train_without_pca_outliers, y=y_train.loc[X_train_without_pca_outliers.index])\nX_train_without_skb_outliers = my_skb_remover.transform(X_train_without_pca_outliers)","0424118a":"X_train = X_train_without_skb_outliers.copy()\ny_train = y_train.loc[X_train.index]\nX_train.shape","5a8baca6":"#normalization\nX_norm = normalize(X_train.values)\ny_norm = y_train","e0b6cc9e":"# reshaping\nX_train = np.array(X_norm).reshape(-1, 28, 28, 1)\n\n#categorizing\nnum_classes = 10\ny_train = keras.utils.np_utils.to_categorical(y_norm, num_classes)","4dca3c79":"# # Model CNN 3 with dropout layers to reduce overfitting:\n# img_rows, img_cols = 28, 28\n# input_shape = (img_rows, img_cols, 1)\n# model3 = Sequential()\n# model3.add(Conv2D(32, kernel_size=(3, 3), activation='relu', kernel_initializer='he_normal', input_shape=input_shape))\n# model3.add(MaxPooling2D((2, 2)))\n# model3.add(Dropout(0.25))\n# model3.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n# model3.add(MaxPooling2D(pool_size=(2, 2)))\n# model3.add(Dropout(0.25))\n# model3.add(Conv2D(128, (3, 3), activation='relu'))\n# model3.add(Dropout(0.4))\n# model3.add(Flatten())\n# model3.add(Dense(128, activation='relu'))\n# model3.add(Dropout(0.3))\n# model3.add(Dense(num_classes, activation='softmax'))\n# model3.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n# model3.summary()","24997928":"# X_train_m3, X_val_m3, y_train_m3, y_val_m3 = train_test_split(X_train, y_train, test_size=0.2)\n# history = model3.fit(X_train_m3, y_train_m3, batch_size=256, epochs=20,validation_data=(X_val_m3,y_val_m3))\n# print(history.history.keys())\n# plt.plot(history.history['accuracy'])\n# plt.plot(history.history['val_accuracy'])\n# plt.title('model accuracy')\n# plt.ylabel('accuracy')\n# plt.xlabel('epoch')\n# plt.legend(['train', 'val'], loc='upper left')\n# plt.show()\n# # summarize history for loss\n# plt.plot(history.history['loss'])\n# plt.plot(history.history['val_loss'])\n# plt.title('model loss')\n# plt.ylabel('loss')\n# plt.xlabel('epoch')\n# plt.legend(['train', 'val'], loc='upper left')\n# plt.show()","424b8f77":"# # Model 4:\n# img_rows, img_cols = 28, 28\n# input_shape = (img_rows, img_cols, 1)\n# model4 = Sequential()\n# model4.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', strides=1, padding='same', data_format='channels_last', input_shape=input_shape))\n# model4.add(BatchNormalization())\n# model4.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', strides=1, padding='same', data_format='channels_last'))\n# model4.add(BatchNormalization())\n# model4.add(Dropout(0.25))\n# model4.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', strides=1, padding='same', data_format='channels_last'))\n# model4.add(MaxPooling2D(pool_size=(2, 2)))\n# model4.add(Dropout(0.25))\n# model4.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', strides=1, padding='same', data_format='channels_last'))\n# model4.add(BatchNormalization())\n# model4.add(Dropout(0.25))\n# model4.add(Flatten())\n# model4.add(Dense(512, activation='relu'))\n# model4.add(BatchNormalization())\n# model4.add(Dropout(0.5))\n# model4.add(Dense(128, activation='relu'))\n# model4.add(BatchNormalization())\n# model4.add(Dropout(0.5))\n# model4.add(Dense(num_classes, activation='softmax'))\n# optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999 )\n# model4.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n# model4.summary()","864a495f":"# X_train_m4, X_val_m4, y_train_m4, y_val_m4 = train_test_split(X_train, y_train, test_size=0.2)\n# history = model4.fit(X_train_m4, y_train_m4, batch_size=128, epochs=20,validation_data=(X_val_m4,y_val_m4))\n# print(history.history.keys())\n# plt.plot(history.history['accuracy'])\n# plt.plot(history.history['val_accuracy'])\n# plt.title('model accuracy')\n# plt.ylabel('accuracy')\n# plt.xlabel('epoch')\n# plt.legend(['train', 'val'], loc='upper left')\n# plt.show()\n# # summarize history for loss\n# plt.plot(history.history['loss'])\n# plt.plot(history.history['val_loss'])\n# plt.title('model loss')\n# plt.ylabel('loss')\n# plt.xlabel('epoch')\n# plt.legend(['train', 'val'], loc='upper left')\n# plt.show()","b897005f":"data= np.array(X_train).reshape(-1, 28, 28) # Transformation for Resnet50:\ndata = np.stack([data, data, data], axis=-1) #convert to 3 channel RGB\ndata.shape\nresnet_weights_path = '..\/input\/resnet50\/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n# Resnet50:\nnum_classes = 10\ndef create_model():\n        model_rn50 = Sequential()\n        model_rn50.add(ResNet50(include_top=False, pooling='avg', weights=resnet_weights_path)) # , input_shape=(28, 28, 1, 3)\n        # model_rn50.add(ResNet50(include_top=False, pooling='avg', weights='imagenet'))\n        model_rn50.add(Dropout(0.50))\n        model_rn50.add(Dense(num_classes, activation='softmax'))\n        model_rn50.layers[0].trainable = True # Indicate whether the first layer should be trained\/changed or not.\n        # model.layers[0].trainable = False\n        model_rn50.summary()\n        model_rn50.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n        return model_rn50","23ba2512":"X_train_rn50, X_val_rn50, y_train_rn50, y_val_rn50 = train_test_split(data, y_train, test_size=0.2)\n# history = create_model().fit(X_train_rn50, y_train_rn50, batch_size=128, epochs=20,validation_data=(X_val_rn50,y_val_rn50),validation_split=0.3)","329817b6":"#grid search\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nmodel = KerasClassifier(build_fn=create_model)\nparam_grid = {'epochs':[10,20,25],'batch_size':[128, 256]}\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, cv=10, scoring='roc_auc', verbose=1)\ngrid_result = grid.fit(X_train_rn50, y_train_rn50)\ngrid.estimators\n\nhistory=grid_result\nprint(history.history.keys())\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","1f2da612":"X_test_rn50 = df_test.iloc[:,1:]\ny_test_rn50 = df_test.iloc[:,0]\nX_test_norm = normalize(X_test_rn50.values)\ntest_data_rn50= np.array(X_test_norm).reshape(-1, 28, 28)\ntest_data_rn50 = X_rgb = np.stack([test_data_rn50, test_data_rn50, test_data_rn50], axis=-1)\ntest_data_rn50.shape\n","b3145e1b":"predicted_classes = model_rn50.predict_classes(test_data_rn50)\nunique_elements, counts_elements = np.unique(predicted_classes, return_counts=True)\nprint(\"Frequency of unique values of the said array:\")\nprint(np.asarray((unique_elements, counts_elements)))","45350b30":"p = pd.Series(predicted_classes[:10000])\ny = y_test_rn50\ncorrect = (p==y)\ncorrect = correct.index[correct]\nincorrect = (p!=y)\nincorrect = incorrect.index[incorrect]\nprint(\"Correct predicted classes:\",correct.shape[0])\nprint(\"Incorrect predicted classes:\",incorrect.shape[0])","4f6f93da":"target_names = [\"Class {} ({}) :\".format(i,labels[i]) for i in range(num_classes)]\nprint(classification_report(y_test_rn50, predicted_classes, target_names=target_names))","c2503a28":"img_rows, img_cols = 28, 28\ndef plot_images(data_index,cmap=\"Blues\"):   # Plot the sample images now\n    f, ax = plt.subplots(3,3, figsize=(6,6))\n    for i, indx in enumerate(data_index[:9]):\n        ax[i\/\/3, i%3].imshow(test.drop('label', axis=1).values[indx].reshape(img_rows,img_cols), cmap=cmap)\n        ax[i\/\/3, i%3].axis('off')\n        ax[i\/\/3, i%3].set_title(\"True:{}  Pred:{}\".format(labels[y_test_rn50[indx]],labels[predicted_classes[indx]]),fontsize=9)\n    plt.show()    \nplot_images(pd.Series(correct), \"Greens\")\n","932a54ed":"plot_images(incorrect, \"Reds\")","38bb16bc":"**Exploring the pixels","789c43f4":"# **Fashion MNIST**\n### **Deep Learning of image data**\n<img src=\"https:\/\/tensorflow.org\/images\/fashion-mnist-sprite.png\" width=\"200\">","7bb11fd7":"We can observe heatmaps of the typical image per category and which pixles are mostly used","2106d5b5":"**Labels:**\n* 0 - T-shirt\/top\n* 1 - Trouser\n* 2 - Pullover\n* 3 - Dress\n* 4 - Coat\n* 5 - Sandals\n* 6 - Shirt\n* 7 - Sneaker\n* 8 - Bag\n* 9 - Ankle Boots","01e05e10":"### **Training CNN Model**","fd70b33d":"### **Reading the image tabular data files**","f0a3025d":"The first component looks like some kind of large clothing object (e.g. not a shoe or accessor). The second component looks like negative space around a pair of pants.","e5ef0c69":"# EDA\nThe provided dataset contains 784 pixels (28x28) grayscale images, each with value between 0 to 255 (0=black 255=white). And one additional column for category (label).\nThe train dataset includes 60K samples and the test 10K","d10a021c":"Both test and train datasets are balanced.\nEach category has exactly 1\/10 of the data","f4f991ec":"Grid search & Cross validation","7432e4b7":"## **Outlier removal using PCA**","e215f57d":"# Preprocessing","c02304a4":"### **Transfer Learning of pre-trained ResNet50 model**","429f1ac5":"# Model Classes Inspection","a136ed00":"we see that predicting shirts (Class 6) is difficult for the model.","6a28a850":"## **Outlier removal using SelectKBest**","108bbe8c":"### **PCA: Visualizing top 2 Principle componenets**","e7bc216a":"There are no pixels that are always black, but some pixcels are rarely used","9c2aba9a":"# Models"}}