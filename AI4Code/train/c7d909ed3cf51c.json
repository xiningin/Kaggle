{"cell_type":{"a02e62dc":"code","e94dd2ac":"code","b3f1a1cc":"code","3f14e1d8":"code","e19a154b":"code","d218f2b8":"code","388e117c":"code","a84ae9ae":"code","e74d6429":"code","4fb595f8":"code","cdaf88e5":"code","9a4f75de":"code","6ab682a1":"code","d6c02079":"code","bb078027":"code","2d66b72b":"code","6864052d":"code","3d4d97ee":"code","f19577b1":"code","0bcb6d03":"code","5ea979c0":"code","977148d9":"code","a477d023":"code","f139ee3c":"code","0eb4cee6":"code","636c0539":"code","c9992bdd":"code","ef7980eb":"code","3000ec74":"code","631ff43b":"code","a7333b48":"code","f5b63e1e":"code","2bdb8e4e":"code","6e8f6d6f":"markdown","311b400a":"markdown","cfa5f954":"markdown","1d0040e8":"markdown","b0a7dab5":"markdown","fcd6f06d":"markdown","87e317d1":"markdown","dac24d75":"markdown","1015c5a2":"markdown","4c1baf13":"markdown","875bf7eb":"markdown","f30d71a8":"markdown","3d906bc9":"markdown","c11ac16e":"markdown","ea79c6f9":"markdown","617c26a4":"markdown","0a67d43c":"markdown","269ec808":"markdown"},"source":{"a02e62dc":"# Intstall the packages I'll use during the project\n\nimport pandas as pd #we import the pandas library for dataframes\nimport numpy as np \nimport os #we will need this module to get the names of the csv files\nimport folium #for the map dataviz\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#----------Import all the csv files into a dataframes----------------------\n\nli_csv=[]\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        df = pd.read_csv(os.path.join(dirname, filename)) #we go through each file in the directory\n        li_csv.append(df) #and then we append it into a single list\n\n        \nframe_csv = pd.concat(li_csv) #we use the concat method to concatenate all the csv files in a single Dataframe\n\n#----------All set, now we have a cozy little piece of code to compile all the csv files in the route into a dataframe, much easier to handle!-----","e94dd2ac":"#-------Let's take a quick glance at the dataset, just to know what we are dealing with----\nframe_csv.head(5)","b3f1a1cc":"#I already noticed something wrong and that is the data is not sorted by date in the dataframe\n\nframe_csv = frame_csv.sort_values(by='started_at') #sort dataframe by date (older to latest)\n\n#piece of 'google searched' cake","3f14e1d8":"frame_csv.columns #quick glance at the column names of the dataframe to make sure what kind of data I have at my disposition and to figure out if I am going to need to rename columns","e19a154b":"#----After 6 hours of careful study, 5:10 of which were --------------------\n#----composed of me watching Youtube videos to prevent  --------------------\n#----procastination, 00:40 of me googling a bunch of    --------------------\n#----pandas commands and 00:10 of me analyzing the      --------------------\n#----components of the dataset, I figure that I could   --------------------\n#----use the datetime data in order to get some insights-------------------- \n#----on the trends of use across time                   --------------------         \n\nframe_csv['ride_lenght'] = pd.to_datetime(frame_csv.ended_at) - pd.to_datetime(frame_csv.started_at) #I calculate the lenght of the ride in time to process the data later on\nframe_csv['Time_start'] = pd.to_datetime(frame_csv.started_at).dt.time # I formatted the started_at data into datetime and then extracted the hour data to separate it into a different column\nframe_csv['Hour_start'] = pd.to_datetime(frame_csv.started_at).dt.hour\nframe_csv['Time_end'] = pd.to_datetime(frame_csv.ended_at).dt.time # similarly to the previous step I repeated the procedure with the ended_at column\nframe_csv.ended_at = pd.to_datetime(frame_csv.ended_at).dt.to_period('D') #I formatted the date in the started_at column into datetime and then I filtered the data so only the date would show up\nframe_csv.started_at = pd.to_datetime(frame_csv.started_at).dt.to_period('D') #Likewise than inthe previous step I repeated the steps for the ended_at column\nframe_csv['day_of_week'] = frame_csv.started_at.dt.dayofweek #I want to add a column that specifies the day of the week in which the trip was performed\n\nframe_csv.head(5) #just to make sure the date and time columns where showing right","d218f2b8":"#So far all according to keikaku\n#---Next, I thought the geolocational data was going to come in handy later on   ------\n#---So I decided to process it a bit and make it more useful (I shouldn't tell   ------\n#---you this but when I was doing this from scratch I had no idea about how      ------\n#---I was gonna be using this data specifically, I was just excited to be coding)------\n\nframe_csv['start_lat'] = frame_csv['start_lat'].round(3) #It wont be necessary to have so much resolution for the latitude and longitude so I rounded the values to 3 decimal places, additionally too much reolsution may be counterproductive as the same location may be percieved as a different location due to round up errors\nframe_csv['start_lng'] = frame_csv['start_lng'].round(3)\nframe_csv['end_lat'] = frame_csv['end_lat'].round(3)\nframe_csv['end_lng'] = frame_csv['end_lng'].round(3)\n\nframe_csv.head(5) #another quick glance jsut to make sure that the column I wanted was properly added","388e117c":"max_length_total = frame_csv['ride_lenght'].max()\nmax_length_total","a84ae9ae":"#There is something *fishy* going on, now I am *hooked* into the issue, I will *fish* out a solution and..\n\nframe_csv_trial = frame_csv[frame_csv['ride_lenght'].isin(['40 days 18:40:02'])] #I want to figure out who is responsible for that 40 days record, so I filter the dataframe to show who this dude is\nframe_csv_trial","e74d6429":"#---We cleared up who was responsible for keeping the bike for 40 days (damn you Crazy Bill)  -----\n#---But that got me thinking into how many other users were keeping the bikes for periods of -----\n#---time so extended                                                                         -----\n\nframe_csv_trial = frame_csv[frame_csv['ride_lenght'] >= '10 days 12:00:00'] #I want to filter the dataframe by records with a ride_lenght longer than 10 days\nframe_csv_trial","4fb595f8":"min_lenght_total = frame_csv['ride_lenght'].min()\nmin_lenght_total","cdaf88e5":"frame_csv_trial = frame_csv[frame_csv['ride_lenght'] < '0 days 00:00:00'] #I filter by rde lenght less than 0\nframe_csv_trial","9a4f75de":"frame_csv = frame_csv[(frame_csv['ride_lenght'] > '0 days 00:00:60')]","6ab682a1":"min_lenght_total = frame_csv['ride_lenght'].min()","d6c02079":"# I wanted to check the distribution between casual users and member riders\n# A massive assumtpion I am making is that I know what the hell I am doing,\n# Another assumtpion is that each entry in the dataset is one trip by \n# one person an not multiple trips by the same individual\n\nframe_csv.groupby(['member_casual']).count()['ride_id'].plot(kind=\"pie\", fontsize=20, figsize=(8,8), colors=['r','c'])","bb078027":"#-----I calculated the statistics for the entire dataset----------\n\nmean_lenght_total = frame_csv['ride_lenght'].mean().components #I wanted t calculate the average of the lenght per ride of all the data available\nmode_day_total = frame_csv['day_of_week'].mode()[0] #I wanted to calculate which day of the week was the most common for people to ride bikes on ofa all the data available\nstd_dev_lenght_total = frame_csv['ride_lenght'].std().components #I wanted to calculate the standard deviation of the ride lenght to figure out how spread out the data is\n\n#The components method at the end is necessary to break the deltatime data \n#type into int's comonents in days, hours, minutes, etc so when we get the\n#results we will get it in a days hours:minutes:seconds format\n#at least that is what Google told me, you'll see what I mean in the stat table later on","2d66b72b":"#I had an epiphany, Pepsi is definetly better than Coke but Dr Pepper is better than both\n#Also if I separete the users from the casuals into distinct dataframes I can make an easier\n# comparison between them and the whole\n\nframe_csv_member = frame_csv[frame_csv['member_casual'].isin(['member'])] #I wanto to create another datarame whith only the memeber's data in it\nframe_csv_casual = frame_csv[frame_csv['member_casual'].isin(['casual'])] #I wanto to create another datarame whith only the casual's data in it","6864052d":"# I want to calculate some statistics of the data filtered by casual members and then paying members----------------------------\n\nmean_lenght_member = frame_csv_member['ride_lenght'].mean().components #I wanted t calculate the average of the lenght per ride of all the data available\nmode_day_member = frame_csv_member['day_of_week'].mode()[0] #I wanted to calculate which day of the week was the most common for people to ride bikes on ofa all the data available\nstd_dev_lenght_member = frame_csv_member['ride_lenght'].std().components #I wanted to calculate the standard deviation of the ride lenght to figure out how spread out the data is\nmax_lenght_member = frame_csv_member['ride_lenght'].max()\nmin_lenght_member = frame_csv_member['ride_lenght'].min()\n\nmean_lenght_casual = frame_csv_casual['ride_lenght'].mean().components #I wanted t calculate the average of the lenght per ride of all the data available\nmode_day_casual = frame_csv_casual['day_of_week'].mode()[0] #I wanted to calculate which day of the week was the most common for people to ride bikes on ofa all the data available\nstd_dev_lenght_casual = frame_csv_casual['ride_lenght'].std().components #I wanted to calculate the standard deviation of the ride lenght to figure out how spread out the data is\nmax_lenght_casual = frame_csv_casual['ride_lenght'].max()\nmin_lenght_casual = frame_csv_casual['ride_lenght'].min()","3d4d97ee":"def day_of_the_week(day_num):\n    '''\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n       This function helps to convert the\n       numeric value from the dayofweek method\n       into the equivalent in string format\n       definetly not copied from StackOverflow\n       \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n    '''\n    switcher={\n                0:'Monday',\n                1:'Tuesday',\n                2:'Wednesday',\n                3:'Thursday',\n                4:'Friday',\n                5:'Saturday',\n                6:'Sunday'\n             }\n    return switcher.get(day_num,\"Invalid day of week\")\n\n\ndata_stats = pd.DataFrame({'Total':[str(mean_lenght_total.minutes) + 'min',day_of_the_week(mode_day_total),str(std_dev_lenght_total.hours) + 'hr' + str(std_dev_lenght_total.minutes) + 'min',max_length_total,min_lenght_total],\n                           'Member':[str(mean_lenght_member.minutes) + 'min',day_of_the_week(mode_day_member),str(std_dev_lenght_member.minutes) + 'min',max_lenght_member,min_lenght_member],\n                           'Casual':[str(mean_lenght_casual.minutes) + 'min',day_of_the_week(mode_day_casual),str(std_dev_lenght_casual.hours) + 'hr' + str(std_dev_lenght_casual.minutes) + 'min',max_lenght_casual,min_lenght_casual]},\n                          ['Travel lenght average','Most common day of the week','Standard deviation','Max Lenght','Min Lenght'])\n#I created a dataframe sumarizing the data and converted the deltatime data type into string to display it properly\n","f19577b1":"data_stats","0bcb6d03":"#------- I want to create a few graphs to visualize the data and note additional trends----\n\nby_day_of_week = frame_csv.groupby(['day_of_week','member_casual']).count()['ride_id'] #in order to make it easier to visualize the use ttrends between casual and members I grouped the dataframe by the day of week and then by the user type to be able to detect user trends through the week\n\n## All of this is because I couldn't sort the values of the days of week if they were in string form when dipslayed in the graph\nnew_index = [] #In this new list the values of the days of the week in str form will be stored\nfor tup in by_day_of_week.index: #I loop trhough each tuple found in by_day_of_week index (which is a list made of tuples)\n    lis=list(tup) #I convert into a list so I can change the first index value (tuples do  not allow value to be assigned)\n    lis[0]=day_of_the_week(lis[0])\n    tup_week = tuple(lis) #I convert back into a tuple\n    new_index.append(tup_week) #I create the list of tuples\n    \nby_day_of_week.index=new_index #I reassign the new list of tuples to the index so I can graph and the labels will display the names of the days of the week\n    ","5ea979c0":"by_day_of_week.plot(kind='bar', title='Use through the days of the week', grid=False,color=['r','c'],figsize=(10,8),fontsize=12) #I plot the resulting dataframe in order to be able to visualize trends more easily","977148d9":"#---- similarly to what we did for the days of the week, I want to detect trends in monthly use of the service---\n\nframe_csv.loc[:,('month')]=frame_csv.started_at.dt.month\n# this method of accesing the index of a dataframe is recommended in the pandas documentation to avoid what it is called 'chained indexing'","a477d023":"by_month = frame_csv.groupby(['month','member_casual']).count()['ride_id'] #in order to make it easier to visualize the use ttrends between casual and members I grouped the dataframe by the day of week and then by the user type to be able to detect user trends through the week\n\ndef month(month_num):\n    '''\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n       This function helps to convert the\n       numeric value from the month into\n       a str equivalent\n       \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n    '''\n    switcher={\n                1:'Jan',\n                2:'Feb',\n                3:'Mar',\n                4:'Apr',\n                5:'May',\n                6:'Jun',\n                7:'Jul',\n                8:'Aug',\n                9:'Sep',\n                10:'Oct',\n                11:'Nov',\n                12:'Dec'\n             }\n    return switcher.get(month_num,\"Invalid day of week\")\n\n## All of this is because I couldn't sort the values of the days of week if they were in string form when dipslayed in the graph\nnew_index_month = [] #In this new list the values of the days of the week in str form will be stored\nfor tup in by_month.index: #I loop trhough each tuple found in by_day_of_week index (which is a list made of tuples)\n    lis=list(tup) #I convert into a list so I can change the first index value (tuples do  not allow value to be assigned)\n    lis[0]=month(lis[0])\n    tup_month = tuple(lis) #I convert back into a tuple\n    new_index_month.append(tup_month) #I create the list of tuples\n    \nby_month.index=new_index_month #I reassign the new list of tuples to the index so I can graph and the labels will display the names of the days of the week\n    ","f139ee3c":"by_month.plot(kind='bar', title='Use through the months', fontsize=12, figsize=(12,8),grid=False,color=['r','c'])","0eb4cee6":"#--- There is another thing I want to do with the Datetime data, and that is to find trends in daily use---\n\nby_hour=frame_csv.groupby(['Hour_start','member_casual']).count()['ride_id']\n\ndef hour_day(hour_num):\n    '''\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n       This function helps to convert the\n       numeric value from the hour into \n       the corresponding value in hr\n       format\n       \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n    '''\n    switcher={\n                0:'24:00',\n                1:'1:00',\n                2:'2:00',\n                3:'3:00',\n                4:'4:00',\n                5:'5:00',\n                6:'6:00',\n                7:'7:00',\n                8:'8:00',\n                9:'9:00',\n                10:'10:00',\n                11:'11:00',\n                12:'12:00',\n                13:'13:00',\n                14:'14:00',\n                15:'15:00',\n                16:'16:00',\n                17:'17:00',\n                18:'18:00',\n                19:'19:00',\n                20:'20:00',\n                21:'21:00',\n                22:'22:00',\n                23:'23:00'\n                \n             }\n    return switcher.get(hour_num,\"Invalid hour of day\")\n\n## All of this is because I couldn't sort the values of the days of week if they were in string form when dipslayed in the graph\nnew_index_hour = [] #In this new list the values of the days of the week in str form will be stored\nfor tup in by_hour.index: #I loop trhough each tuple found in by_day_of_week index (which is a list made of tuples)\n    lis=list(tup) #I convert into a list so I can change the first index value (tuples do  not allow value to be assigned)\n    lis[0]=hour_day(lis[0])\n    snap_back_to_reality = tuple(lis) #I convert back into a tuple\n    new_index_hour.append(snap_back_to_reality) #I create the list of tuples\n    \nby_hour.index=new_index_hour #I reassign the new list of tuples to the index so I can graph and the labels will display the names of the days of the week\n    ","636c0539":"by_hour.plot(title='Use trend throughout the day by hour, red=casual cyan=member',kind=\"bar\",color=['r','c'],fontsize=13,figsize=(15,8))","c9992bdd":"#---- I want to see what kind of trends we can get out of the coordinates data-----\n\ncolumns = ['ride_id','start_station_name','end_station_name','member_casual','start_lat','start_lng','end_lat','end_lng']\nby_station_total = frame_csv.filter(items=columns)\nby_station_member = frame_csv_member.filter(items=columns)\nby_station_casual = frame_csv_casual.filter(items=columns)","ef7980eb":"by_station_start = by_station_member.groupby(['start_station_name','member_casual']).count()['ride_id'] #in order to make it easier to visualize the use ttrends between casual and members I grouped the dataframe by the day of week and then by the user type to be able to detect user trends through the week\nby_station_start = pd.DataFrame(by_station_start)\nby_station_start.sort_values(by=['ride_id'],ascending = False).head(25).plot(kind='bar', title= 'Start station')","3000ec74":"by_station_start = by_station_casual.groupby(['start_station_name','member_casual']).count()['ride_id'] #in order to make it easier to visualize the use ttrends between casual and members I grouped the dataframe by the day of week and then by the user type to be able to detect user trends through the week\nby_station_start = pd.DataFrame(by_station_start)\nby_station_start.sort_values(by=['ride_id'],ascending = False).head(25).plot(kind='bar', title= 'Start station')","631ff43b":"#In this following section I group the dataframe by station and count \n#the number of rides that started from and ended at each one then I \n#sort by the largest to get a lsit of the most crowded stations, \n#I will use this afterwards to get the geolocational data to plot on the map\n\n##IN HERE I CAN ADJUST THE NUMBER OF CIRCELS I PLOT IN THE MAP BY ADJUSTING THE NUMBER OF ENTRIES THAT I FILTER WITH THE HEAD METHOD\nstart_station_most_popular_member=by_station_member.groupby(['start_station_name']).count()['ride_id'].sort_values(ascending = False).head(20)\nend_station_most_popular_member=by_station_member.groupby(['end_station_name']).count()['ride_id'].sort_values(ascending = False).head(20)\n\nstart_station_most_popular_casual=by_station_casual.groupby(['start_station_name']).count()['ride_id'].sort_values(ascending = False).head(20)\nend_station_most_popular_casual=by_station_casual.groupby(['end_station_name']).count()['ride_id'].sort_values(ascending = False).head(20)","a7333b48":"def get_coordinates(station_df,station_column,station_lat_lng):\n    \n    '''\n    ########################################################\n    #####This function is to get the latitude and longitude#\n    #####of the most crowded stations for arrival and      #\n    #####departure                                         #\n    ########################################################\n    '''\n    coord_list = []\n    for lat in station_df.index:\n        coord_list.append(frame_csv.loc[frame_csv[station_column] == lat][station_lat_lng].values[0])\n    return  coord_list\n\n## This series of lists summarize the lat and lng data for memebers and casuals for arrival and departure stations\nstart_lat_list_member=get_coordinates(start_station_most_popular_member,'start_station_name','start_lat')\nstart_lng_list_member=get_coordinates(start_station_most_popular_member,'start_station_name','start_lng')\nend_lat_list_member=get_coordinates(end_station_most_popular_member,'end_station_name','end_lat')\nend_lng_list_member=get_coordinates(end_station_most_popular_member,'end_station_name','end_lng')\n\nstart_lat_list_casual=get_coordinates(start_station_most_popular_casual,'start_station_name','start_lat')\nstart_lng_list_casual=get_coordinates(start_station_most_popular_casual,'start_station_name','start_lng')\nend_lat_list_casual=get_coordinates(end_station_most_popular_casual,'end_station_name','end_lat')\nend_lng_list_casual=get_coordinates(end_station_most_popular_casual,'end_station_name','end_lng')","f5b63e1e":"def station_data(name_column,name_val,qty_column,qty_val,lat_col,lat_val,lng_col,lng_val):\n    '''\n    ##########################################################\n    ##### This function is useful to create a dataframe ######\n    ##### with all the important data for plotting it on######\n    ##### map                                           ######\n    ##########################################################\n    \n    '''\n    data_stations = {\n        name_column: name_val,\n        qty_column: qty_val,\n        lat_col: lat_val,\n        lng_col:lng_val\n    }\n    \n    return pd.DataFrame(data_stations)\n\n#I make the dataframes with all the important information for plotting on the map\n\ndf_stations_start_member = station_data('start_station_name_member',start_station_most_popular_member.index, \n                            'start_station_qty_member',start_station_most_popular_member.values,\n                            'start_lat_member',start_lat_list_member,'start_lng_member',start_lng_list_member)\n\ndf_stations_end_member = station_data('end_station_name_member',end_station_most_popular_member.index, \n                            'end_station_qty_member',end_station_most_popular_member.values,\n                            'end_lat_member',end_lat_list_member,'end_lng_member',end_lng_list_member)\n\ndf_stations_start_casual = station_data('start_station_name_casual',start_station_most_popular_casual.index, \n                            'start_station_qty_casual',start_station_most_popular_casual.values,\n                            'start_lat_casual',start_lat_list_casual,'start_lng_casual',start_lng_list_casual)\n\ndf_stations_end_casual = station_data('end_station_name_casual',end_station_most_popular_casual.index, \n                            'end_station_qty_casual',end_station_most_popular_casual.values,\n                            'end_lat_casual',end_lat_list_casual,'end_lng_casual',end_lng_list_casual)","2bdb8e4e":"map_chic = folium.Map(location=[frame_csv['start_lat'].mean(), frame_csv['start_lng'].mean()],  zoom_start=15) #I create the map variable, the default display is the mean of the lat and lng of all the locations of the stations according to the original data\n\n#I iterate through each entrie in the dataframe for departure (I do not use the end stations datframe becasue earlier I learnt that the start and end hotspots are the exact same for both members and casuals) \nfor index, row in df_stations_start_member.iterrows():\n\n    folium.Circle(\n        location=[row['start_lat_member'], row['start_lng_member']], #the circle is place on the lat and lng data for each station\n        radius=(row['start_station_qty_member'])\/150, #the radius of the circle will be proportional to the quantity of the users that docked on the station\n        popup=row['start_station_name_member'],\n        color=\"blue\",\n        fill=True,\n    ).add_to(map_chic)\n\nfor index, row in df_stations_start_casual.iterrows():\n\n    folium.Circle(\n        location=[row['start_lat_casual'], row['start_lng_casual']],\n        radius=(row['start_station_qty_casual'])\/150,\n        popup=row['start_station_name_casual'],\n        color=\"red\",\n        fill=True,\n    ).add_to(map_chic)\n\n\nmap_chic","6e8f6d6f":"##### There are a couple of interesting trends to note with the stats:\n* On average the travel length for casual users is over double the value for Members, and the standard deviation for members is barely half an hour when for Casuals it is 6 hours, this speaks of more concise and focused rides for Members.\n* An interesting point to note is that keeping the bikes for long period of time is not something only members trend to do (the odometer effect), clearly casual users also keep their bikes for long periods of time.\n\n","311b400a":"# **Ask phase**\nLet's start by making a proper justification for all the work we are going to do.\n\nLet's see, this is a case study so \"I need money because I have that nasty tendency of needing food to live\" is probably not gonna make the cut, meaning we must be more flexible.\n\nIn that case we need to get in character according to the case study, all right!\nSo let's get in character, I am a jr data analyst working for Cyclistic, I am underpaid, underappreciated and probably never gonna be able to afford a home of my own, but I am here for the love of data damn it.\n\nDid I say(write) that out loud? well, since any solution lies on defining a problem first and foremost we can start from there (way easier than dwelling on an existential crisis):\n\n\n# A bit of context\nCyclistic is a (fictional) company that offers ride sharing services in the city of Chicago, their business model relies on an userbase of 2 types of people: annual subscribers (the Chads) and people who pay for day passes or single trips (the Virgins), the users can grab a bike at installed stations around various points in the city, and the company has reasons to believe that they can increase profitability by increasing the number of annual subscribers to the service, since the company already has an userbase of casual (virgins) users the marketing manager believes that the company would do well in trying to convert these users into subscribers since they already are acquainted with the service. (There are more details I could tell you to add more context, but I think this is the basic outline of the justification as for why this project came to be, besides I think that any more text would just be a waste of your time and I am not about trying to make you waste time, unless I was writing something between parenthesis).\n\n# What am I trying to solve here?\nCyclistic wants to increase the profitability of the business, and after careful analysis they found out they can do it two ways:\n* Reduce costs by enslaving it's work force\n* Increase the number of subscribers to the service for an steady flow of cash\n\nThe first option is not a moral problem but those pesky employees insist on being paid, so that leaves the company with the second choice, in the form of a nice little statement this would translate into:\n\n#### To convert casual users of the service into annual subscribers to drive profitability upward.\n\n# The solution\n#### Understand the trends in use that both types of users have in order to craft more effective marketing strategies. (reveal gaps)\n\n# The How\n#### Analyzing the historical data of users of the service can provide insights into the preferences of these users. (reveal opportunities)\n\n# The bottom Line\n#### With this analysis the company can better understand it\u2019s user base and make educated data-driven decisions. (identify options)\n\n## So, How do annual members and casual riders use Cyclistic bikes differently?","cfa5f954":"#### Welp...\n#### I have a couple of hypothesis for the negative ride duration\n1. False entries in the dataset\n2. Chronogdor, the timelord wizard that can travel in time and coincidentally lives in the Chicago area is an user of Cyclistic, grabed a bike, traveled back in time and then returned the bike... in the past.\n\n#### Well, let's take a closer look (fingers crossed for Chronogdor, dude can you imagine? that would be so cool)","1d0040e8":"After filtering the dataframe to show rides with less than 0 in duration, I came across an staggering conclusion...we are being invaded by an army of timelord wizards!!\n\nBut since the corporate world is boring, management will probably not take this as an acceptable answer (the jerks).\n\n#### According to the Divvy website one possible explanation for these entries is from potentially false starts or users trying to re-dock a bike to ensure it was secure.\n\nWith that in mind I believe that the easiest way to handle this data points is very much like any other problem in my life, pretend it doesn't exist and never look back until the last consequences.\n\nBy removing them from the dataset as something unusual happened I would only be ignoring 0.3% of all the data.\n\nI decided to filter the data for trips that lasted for longer than 60 seconds as anything lower could still be considered users trying to re-dock a bike before starting their trip, ","b0a7dab5":"# Aha! moment number 1\nThere is a clear trend in the graph above, throughout the week the member user-base (in cyan) is very consistent and always with an advantage over the casual user-base (in red), except during the weekend period, form this I can hypothesize the following:\n* Casuals use the bikes for leisure.\n* Members seem to prefer the service to make concise trips during the week more than likely to travel to their workplace, while casuals seem to prefer the service during the weekends and take longer rides, even keeping the bikes for  prolonged periods of time\n","fcd6f06d":"I thought it was weird to see a 40 day as the max ride length as I wasn't expecting to get anything larger than 1 day, so I decided to look up into it to make sure it wasn't something to worry about the dataset.\n\nAfter a quick analysis I realized that the entry was by an annual subscriber of the service, which is not so weird after all, an hypothesis of mine is a phenomena known as \"the odometer effect\" (I paid for the whole odometer, I am going to use the whole odometer).\n\nIn other words, some crazy guy decided to not return the bike after using it, and then repeated the process for 40 days in a row (at least he was an annual subscriber of the service, otherwise I would be unreasonably and unjustifiably mad).\n","87e317d1":"All right, so I used the max function to get the maximum duration of a bike ride and I... wait, what?... what the fu...? 40 days?!!! who in his right mind...?\nthat can't possibly be right","dac24d75":"# \"Google Data analytics Capstone project\" or \"This is how I spend my friday nights over the past 2 weeks\"","1015c5a2":"### This is great, but I prefer to graph it in a map to better uderstand some trends","4c1baf13":"### In general terms casual users seem to be scattered near the bay area, focused mainly on parks and locations for entertainment such as Streeter Dr and Grand Ave.\n### Members are spread deeper into the city, they can also be found on parks, but their distribution is a bit more homogenous, and they are often near residential areas, apartment complex and near bus or metro stations\n","875bf7eb":"# Prepare\nEven though Cyclistic is a fictional company (after spending so much time into its business task it feels real in my heart) the dataset is very much real and provided by Divvy under their [license agreement](https:\/\/ride.divvybikes.com\/data-license-agreement) and Divvy itself is an initiative backed by Chicago's Department of transportation, which covers pretty much all the basis for the data being reliable, original, cited and current, but in order to figure out if it is comprehensive we need to dive into it and (maybe) manipulate it a bit, so finally! enough blah, blah, blah and let's get into coding:","f30d71a8":"# Act\n## What does all of this mean for the business?\n\nJudging by the data I believe that the members users of the service (the chads) are for the most part members of the working-class person with a 9-5 job schedule. They occasionally use the service in parks or other zones meant for recreational activities, but the trend of use through the week and on a daily basis points out to this users making trips for job related commutes, reinforced by their preferred locations for grabbing a bike, which can be found near residential areas and bus stations (the users may use the bikes to compound their commute to work)\n\nWith that in mind it would be reasonable to focus on this group for a marketing campaign that seeks to convert casual users into members by highlighting how Cyclistic services fits the lifestyle and needs of the working-class fellow.\n\nAnother interesting remark is how the member user-base is most of the users of the service, offering referral benefits for members could serve as a way to strengthen loyalty and recruit the help of members to covert casuals.\n\nThe peak awareness of the service is during the summer season as evidenced by the data, so this time of year would be ideal to get the attention of new users of the service for any marketing strategy that is deemed the more reasonable\n","3d906bc9":"# Aha! moment number 3\nThere are 3 important trends I find in this graph:\n* Both users have their peaks in use at the same hour (5 PM).\n* Casual users have a very organic behavior, increasing in use as the day goes by and decreasing as the night settles in.\n* More interesting still, Members have two obvious peaks during the day: 6-9 AM and 4-6 PM, a timeframe that coincides with the 9-5 job schedule that most people have.\n\nThis leads me to believe that members tend to use the service picking up the pace in rides at around  9 AM and then again at around 5 PM because that time slot coincides with the average 9-5 working hours of the working class type fellow\n","c11ac16e":"# Aha! moment number 2\nIt seems clear that the member user-base has an edge over the casual user-base throughout the entire year, but the summer months (specifically July) it's when the casual users almost get even with the member users, I feel this helps support my original hypothesis of causal users being attracted to the service for leisure related activities \n\nThe peak shown in February is a bit though to explain but I have a hypothesis for that as well:\n* Even though the data is comprised of all 12 months, it is an overlap of 2 years (the first 3 months are from 2021 and the rest from 2020).\n* In consideration of the above, what was that thing that happened at the beginning of 2020 that shut the entire world down? \n\nSo, I feel confident in saying that the peak shown in the first 3 months is not from an increase in use during a normal year, but because the first quarter of 2020 was extremely different in circumstances from the first quarter of 2021, because of the pandemic.\n\nThe following months though show what I think is more representative of a common year for Cyclistic.\n\n### Key Takeaways:\n* The organic behavior for members is a contrast to the sudden peak that casual users have during the summer\n* The pandemic sucked\n","ea79c6f9":"# Process\n#### Now the data looks ready to get into Processing\nGood news is I have already done plenty of processing on the data already:\n* I converted the date data into Datetime\n* I separated the datetime values in different columns to make calculations easier\n* I extracted the day of the week from the Datetime data\n* I calculated the ride length with the start time and end time\n* I rounded the values of the coordinates\n* I filtered the ride length values that were less than 0\n\nSo, half the battle is done, all we need to do is some statistics calculations\n","617c26a4":"#### I decided to filter the length of the ride and found out that it is common for users to spend a significant amount of time with the bikes, as a matter of fact, over 200 users kept the bikes for a period longer than 10 days, from now on I will make 2 assumptions with the dataset: The start date and end date registries are correctly captured, and this kind of consumption pattern is simply a common trend among users\n\n#### Since I can confirm that users do in fact keep their bikes for very long periods of time and these events are not that unusual, I can carry on and calculate some stats about the dataset without having to discard any other data entry, and I can confirm I have gone through the dataset and make sure it is reliable and comprehensive\n\n#### Surely, we ran out of surprises and we can carry on with the analysis...\n","0a67d43c":"Oh...","269ec808":"# Final remarks on the course\nThe Google data analysis program offers a step by step approach for the analysis process, however I personally believe that even though the first phases of the process are very well thought as a step by step sequence, when we reach the analysis part things begin to flow better if the get mixed up a bit, I found myself going back to the Process phase often in the middle of the Analysis phase whenever I came across a part of the project where I thought of different ways in which I could leverage the dataset to find new trends, which is the reason why the Process phase and the Analysis phase are kind of mixed up in my project. \n\nI found it more comfortable to work this way and that is the reason why by the end of this mixed phase there is also the data visualizations thrown into the mix, because I just found myself eager to see what the visualization was going to reveal and I just wrote every thought I had about the trend I was seeing, maybe it is a bit unorganized, but again, that is the way I found the Google process to fit into my own pace of doing things.\n\nAll in all, the program was a mixed of intensive and novice friendly, I personally liked it and could find many correlations with similar analysis process I had already study in the past, such as six sigma, all I can say is that I would completely recommend this course for newbies and intermediates alike... also Sally from the Process phase is officially my data analysis crush.\n"}}