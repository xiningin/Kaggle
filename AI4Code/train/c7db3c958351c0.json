{"cell_type":{"896a70f6":"code","0cccd7fe":"code","bffbd570":"code","81cc4190":"code","eccc921d":"code","9a9b279b":"code","462109f1":"code","4877cd50":"code","10260c0c":"code","c6a97f56":"code","3fea1dea":"code","23e94755":"code","6cf5f9a7":"code","4ca9202a":"code","cfb2265e":"code","97437c2c":"code","bd7de3cb":"code","d0640eea":"code","23d3a218":"code","8e9c4677":"code","468e6b1f":"code","28c2d2a9":"code","951d8026":"code","0d2e9903":"code","90059201":"code","de00782d":"code","7a195148":"code","9965a2fa":"markdown","80d8436e":"markdown","65df2775":"markdown","192d8814":"markdown","90aec8e8":"markdown","fbca5bc5":"markdown","f45619bc":"markdown","9d8e3011":"markdown","aeb5c686":"markdown","30fed367":"markdown","0b6e83e3":"markdown","6178d6ac":"markdown","d04a45bf":"markdown","3c790d60":"markdown","4699ce3a":"markdown","c7719785":"markdown","6c344d15":"markdown","dc0b2dc0":"markdown","c82edcd7":"markdown","ccb49e98":"markdown","ddff0fac":"markdown","5e8857f2":"markdown"},"source":{"896a70f6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         #print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0cccd7fe":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib import style\nimport seaborn as sns\nimport os\n\nimport sklearn\nimport os \nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split,StratifiedKFold\n  \nimport os,cv2\nfrom IPython.display import Image\n\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D , BatchNormalization\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras.initializers import glorot_normal,glorot_uniform, he_normal, he_uniform\nfrom keras.optimizers import Adamax,Adam, Adadelta, Adagrad, RMSprop, Nadam\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import to_categorical\nfrom keras.regularizers import l2,l1\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import load_model\n\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications import NASNetLarge\nfrom keras import optimizers","bffbd570":"test_labels = pd.read_csv(\"..\/input\/jantahackcomputervision\/test_vc2kHdQ.csv\")\nsubmission = pd.read_csv(\"..\/input\/jantahackcomputervision\/ss.csv\")\ntrain_labels = pd.read_csv(\"..\/input\/jantahackcomputervision\/train_SOaYf6m\/train.csv\")\n\n\ntrain_labels[\"emergency_or_not\"] = train_labels[\"emergency_or_not\"].astype(str)\nsubmission[\"emergency_or_not\"] = submission[\"emergency_or_not\"].astype(str)\n\nrandom = [ '0' if i%4==0 else '1' for i in range(706)]\nsubmission[\"emergency_or_not\"]  = random","81cc4190":"train_labels.head(5)","eccc921d":"test_labels.head()","9a9b279b":"## Looking at number of classes in the training data\ntrain_labels[\"emergency_or_not\"].value_counts().plot(kind=\"bar\");\n\nprint(train_labels[\"emergency_or_not\"].value_counts())","462109f1":"from PIL import Image\ndef view_imgs(df,rows,cols):\n    IMAGE_DIR =\"..\/input\/jantahackcomputervision\/train_SOaYf6m\/images\/\"\n    axes=[]\n    fig=plt.figure(figsize=(20,12))    \n    for i in range(rows*cols):\n        idx = np.random.randint(len(df), size=1)[0]\n        image_name , label = df.loc[idx,\"image_names\"],df.loc[idx,\"emergency_or_not\"]\n        image = Image.open(IMAGE_DIR+image_name)\n        label = \"emergency\" if label=='1' else \"non-emergency\"\n        axes.append( fig.add_subplot(rows, cols, i+1))\n        subplot_title=(\"Category :\"+str(label))\n        axes[-1].set_title(subplot_title)  \n        plt.imshow(image)\n    fig.tight_layout()  \n    plt.show()\n        \n        \nview_imgs(train_labels,2,4)","4877cd50":"## Shuffling dataframe\ntrain_labels = train_labels.sample(frac=1).reset_index()\n\ndatagen = ImageDataGenerator(rescale=1.\/255)\nbatch_size=64\nimg_dir = \"..\/input\/jantahackcomputervision\/train_SOaYf6m\/images\"\n\ntrain_generator=datagen.flow_from_dataframe(dataframe=train_labels[:1318],directory=img_dir,x_col='image_names',\n                                            class_mode=None,batch_size=batch_size,\n                                            target_size=(224,224),shuffle=False)\n\n## class_mode is none since we only need the bottle_neck features \n\n\nvalidation_generator = datagen.flow_from_dataframe(dataframe=train_labels[1318:],directory=img_dir,x_col='image_names',\n                                                class_mode=None,batch_size=32,\n                                                target_size=(224,224),shuffle=False)","10260c0c":"vgg16 = VGG16(weights=\"imagenet\",include_top=False,input_shape=(224,224,3))\n\nfor layers in vgg16.layers:\n    layers.trainable = False\n    \n\n## Looking at the model structure\nvgg16.summary()","c6a97f56":"bottleneck_features_train = vgg16.predict(train_generator,verbose=1)\nbottleneck_features_validation = vgg16.predict(validation_generator,verbose=1)\nprint(bottleneck_features_train.shape,bottleneck_features_validation.shape)","3fea1dea":"train_data = bottleneck_features_train.copy()\ntrain_class = train_labels.loc[:1317,\"emergency_or_not\"]\n\nvalidation_data = bottleneck_features_validation.copy()\nvalidation_class = train_labels.loc[1318:,\"emergency_or_not\"]\n\nmodel = Sequential()\nmodel.add(Flatten(input_shape=train_data.shape[1:]))\nmodel.add(Dense(2048,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1,activation=\"sigmoid\"))\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","23e94755":"# Using checkpoints to save the best model where we are monitoring validation accuracy \n# i.e we will save the model corresponding to max validation'accuracy \n\nfrom keras.callbacks import ModelCheckpoint\nfilename = \".\/bottle_neck_best_wts.hdf5\"\nchecks = ModelCheckpoint(filename,monitor=\"val_accuracy\",verbose=1,\n                         save_best_only=True,mode=\"max\",save_weights_only=True)\n\nmodel.fit(train_data, train_class,\n          epochs=2,\n          batch_size=32,\n          validation_data=(validation_data, validation_class),\n         callbacks=[checks])\n","6cf5f9a7":"## After instantiating the VGG base and loading its weights, \n\nvgg = VGG16(weights=\"imagenet\",include_top=False,input_shape=(224,224,3))\nfor layer in vgg.layers[:-4]:\n    layer.trainable = False\n\n## we add our previously trained fully-connected classifier on top of it\nfinal_model = Sequential()\nfinal_model.add(vgg)\n\n\n## Trained Classifier\ntop_model = Sequential()\ntop_model.add(Flatten(input_shape=final_model.output_shape[1:]))\ntop_model.add(Dense(2048,activation='relu'))\ntop_model.add(Dropout(0.5))\ntop_model.add(Dense(1,activation=\"sigmoid\"))\ntop_model.summary()\n","4ca9202a":"# note that it is necessary to start with a fully-trained\n# classifier, including the top classifier,\n# in order to successfully do fine-tuning\n\ntop_model_weights_path = \"..\/input\/cv-av-models\/bottle_neck_best_wts.hdf5\"\ntop_model.load_weights(top_model_weights_path)\n\n# add the model on top of the convolutional base\nfinal_model.add(top_model)\nfinal_model.summary()","cfb2265e":"final_model.compile(loss='binary_crossentropy',\n              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n              metrics=['accuracy'])","97437c2c":"# prepare data augmentation configuration\n\ndef return_data_generators(img_width,img_height,train_labels,batch_size=16,class_mode=\"binary\"):\n    \"\"\"\n    img_width,img_height : - image dimension to be resized to this sizes during data loading\n    \n    returns ImageDatagenerators for training and testing purpose\n    \n    \"\"\"\n    \n    \n    train_labels = train_labels.sample(frac=1).reset_index()\n\n    train_datagen = ImageDataGenerator(\n            rescale=1.\/255,\n            shear_range=0.2,\n            zoom_range=0.2,\n            horizontal_flip=True)\n\n    test_datagen = ImageDataGenerator(rescale=1.\/255)\n\n\n    img_dir = \"..\/input\/jantahackcomputervision\/train_SOaYf6m\/images\"\n\n\n    train_generator = train_datagen.flow_from_dataframe(dataframe=train_labels[:1318],directory=img_dir,x_col='image_names',\n                                                y_col='emergency_or_not',class_mode=class_mode,batch_size=batch_size,\n                                                target_size=(img_width,img_height),shuffle=True)\n\n    validation_generator = test_datagen.flow_from_dataframe(dataframe=train_labels[1318:],directory=img_dir,x_col='image_names',\n                                                    y_col='emergency_or_not',class_mode=class_mode,batch_size=batch_size,\n                                                    target_size=(img_width,img_height),shuffle=True)\n    \n    test_generator = test_datagen.flow_from_dataframe(dataframe=test_labels,directory=img_dir,x_col='image_names',\n                                                class_mode=None,batch_size=batch_size,\n                                                target_size=(img_width,img_height),shuffle=False)\n    \n    return train_generator,validation_generator,test_generator","bd7de3cb":"batch_size = 32\nepochs = 30\n\ntrain_generator,validation_generator,test_generator = return_data_generators(224,224,train_labels,batch_size)\n\n## Checkpoints\nfilename = \".\/fine_tuned.hdf5\"\n\nchecks = ModelCheckpoint(filename,monitor=\"val_accuracy\",verbose=1,\n                         save_best_only=True,mode=\"max\")\n\n# fine-tune the model\nhistory_finetune = final_model.fit_generator(train_generator,epochs=epochs,\n                              validation_data=validation_generator,\n                             callbacks=[checks],verbose=1)","d0640eea":"## TRY THIS FUNCTION ONCE\n\ndef plot_loss_acc(history,epochs,filename):\n    fig = plt.figure(figsize=(14,9))\n\n    ax1 = plt.subplot2grid((2,1), (0,0))\n    ax2 = plt.subplot2grid((2,1), (1,0), sharex=ax1)\n    \n    epoch = list(range(1,epochs+1,1))\n    losses = history.history[\"loss\"]\n    val_losses = history.history[\"val_loss\"]\n\n    accuracies = history.history[\"accuracy\"]\n    val_accs = history.history[\"val_accuracy\"]\n\n    \n    ax1.plot(epoch, accuracies, label=\"acc\")\n    ax1.plot(epoch, val_accs, label=\"val_accuracy\")\n    ax1.legend(loc=2)\n    ax1.set_ylabel(\"Accuracy\")\n    \n    ax2.plot(epoch,losses, label=\"loss\")\n    ax2.plot(epoch,val_losses, label=\"val_loss\")\n    ax2.legend(loc=2)\n    ax2.set_xlabel(\"No of epochs\")\n    ax2.set_ylabel(\"Loss\")\n\n    \n    \n    plt.savefig(\".\/\"+filename+\".png\", dpi=300, bbox_inches='tight')\n    plt.show()\n\n    \nplot_loss_acc(history_finetune,30,\"keras_finetune_vgg\")","23d3a218":"## VGG16 with l2 regularization\n\n## After instantiating the VGG base and loading its weights, \n\nvgg = VGG16(weights=\"imagenet\",include_top=False,input_shape=(224,224,3))\nfor layer in vgg.layers:\n    layer.trainable = False\n\n    \nmodel_l2 = Sequential()\nmodel_l2.add(vgg)\n\n\n## Trained Classifier\ntop_model = Sequential()\ntop_model.add(Flatten(input_shape=model_l2.output_shape[1:]))\ntop_model.add(Dense(2048,activation='relu',kernel_initializer=glorot_uniform(), kernel_regularizer=l2()))\ntop_model.add(Dropout(0.5))\ntop_model.add(Dense(2,activation=\"softmax\",kernel_initializer=glorot_uniform(), kernel_regularizer=l2()))\n\n# top_model_weights_path = \"..\/input\/cv-av-models\/bottle_neck_best_wts.hdf5\"\n# top_model.load_weights(top_model_weights_path)\n\n# add the model on top of the convolutional base\nmodel_l2.add(top_model)\nmodel_l2.summary()\n\nmodel_l2.compile(loss='categorical_crossentropy',\n              optimizer=\"adam\",\n              metrics=['accuracy'])\n\n## Checkpoints\nfilename = \".\/fine_tuned_l2_augnmentation.hdf5\"\n\nchecks = ModelCheckpoint(filename,monitor=\"val_accuracy\",verbose=1,\n                         save_best_only=True,mode=\"max\")\n\ntrain_generator,validation_generator,test_generator = return_data_generators(224,224,train_labels,32,\"categorical\")\n\n# fine-tune the model\nhistory_l2 = model_l2.fit_generator(train_generator,epochs=30,validation_data=validation_generator,verbose=1,callbacks=[checks])","8e9c4677":"plot_loss_acc(history_l2,30,\"vgg_l2\")","468e6b1f":"nasnet = NASNetLarge(weights = \"imagenet\", input_shape = (331, 331, 3), include_top = False)\nfor layer in nasnet.layers:\n    layer.trainable = False\n\nnasnet_model = Sequential()\n \n\n## Adding a covolutional base with l2 regularzer and FC layer on top of it\nnasnet_model.add(nasnet)\n\nnasnet_model.add(Conv2D(1024, (3, 3), activation = \"relu\"))\nnasnet_model.add(BatchNormalization())\nnasnet_model.add(MaxPooling2D(2, 2))\n \nnasnet_model.add(Flatten())\n  \nnasnet_model.add(Dense(2048, activation = \"relu\",kernel_regularizer=l2()))\nnasnet_model.add(BatchNormalization())\nnasnet_model.add(Dropout(0.5))\n \nnasnet_model.add(Dense(256, activation = \"relu\",kernel_regularizer=l2()))\nnasnet_model.add(BatchNormalization()) \nnasnet_model.add(Dropout(0.5))\n \nnasnet_model.add(Dense(1, activation = \"sigmoid\"))\n \nnasnet_model.summary()","28c2d2a9":"## Compiling and training the model\n\n## Checkpoints\nfilename = \".\/nasnet_l2.hdf5\"\n\ntrain_generator,validation_generator,test_generator = return_data_generators(331,331,train_labels,32)\n\nchecks = ModelCheckpoint(filename,monitor=\"val_accuracy\",verbose=1,\n                         save_best_only=True,mode=\"max\")\n\nepochs = 20\nnasnet_model.compile(loss = 'binary_crossentropy',\n              optimizer = \"adam\",\n              metrics = ['accuracy']\n              )\n\nhistory_nasnet = nasnet_model.fit_generator(train_generator,epochs=epochs,\n                              validation_data=validation_generator,\n                             callbacks=[checks],verbose=1)","951d8026":"plot_loss_acc(history_nasnet,20,\"nasnet_l2\")","0d2e9903":"train_dataset = train_labels.copy()\ntest_dataset = test_labels.copy()\n\ntrain_image_name = list(train_dataset[\"image_names\"])\ntest_image_name = list(test_dataset[\"image_names\"])\n\nprint(len(train_image_name),len(test_image_name))\n\ntrain_images = list()\ntest_images = list()\ntrain_image_res = list()\n\ndirname = \"..\/input\/jantahackcomputervision\/train_SOaYf6m\/images\"\n\nfor i,filename in enumerate(train_image_name):\n    img = cv2.imread(os.path.join(dirname,filename))\n    y_val = int(train_dataset[train_dataset.image_names == filename][\"emergency_or_not\"])\n    train_image_res.append(y_val)\n    train_images.append(img)\n    #print(i+1)\n\n\nfor i,filename in enumerate(test_image_name):\n    img = cv2.imread(os.path.join(dirname,filename))\n    test_images.append(img)\n    #print(i+1)\n    \n\ntrain_images = np.array(train_images)\ntest_images = np.array(test_images)\ntrain_image_res = np.array(train_image_res)\n    \nprint(train_images.shape,test_images.shape,train_image_res.shape)","90059201":"# Creating fold_model\n\nfold_model = Sequential()\nfold_model.add(Conv2D(32,(3,3),input_shape=(224, 224,3)))\nfold_model.add(Activation('relu'))\nfold_model.add(MaxPooling2D(pool_size=(2, 2)))\n\nfold_model.add(Conv2D(32,(3,3)))\nfold_model.add(Activation('relu'))\nfold_model.add(MaxPooling2D(pool_size=(2, 2)))\n\nfold_model.add(Conv2D(128,(3,3)))\nfold_model.add(Activation('relu'))\nfold_model.add(MaxPooling2D(pool_size=(2, 2)))\n\nfold_model.add(Conv2D(512,(5,5)))\nfold_model.add(Activation('relu'))\nfold_model.add(MaxPooling2D(pool_size=(2, 2)))\n\nfold_model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\nfold_model.add(Dense(512))\nfold_model.add(Activation('relu'))\nfold_model.add(Dropout(0.5))\nfold_model.add(Dense(2))\nfold_model.add(Activation('softmax'))\n\nfold_model.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\n\nfilename = \".\/kfold.hdf5\"\ncheckpoint = ModelCheckpoint(filename, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\nearly_stopping_monitor = EarlyStopping(monitor='val_accuracy',patience=7)\ncallbacks_list = [early_stopping_monitor, checkpoint]\n","de00782d":"## Training and predictions\nK = 5\n\nskf = StratifiedKFold(n_splits=K, shuffle=True, random_state = 7)\nnew_submission = pd.DataFrame()\n\nX = train_images\nY = train_image_res\nX_test = test_images\n\nj = 1\n\ny_valid = None \nX_valid = None\n\nfor (train_data_image, test_data_image) in skf.split(X, Y):\n    \n    y_train, y_valid = Y[train_data_image], Y[test_data_image]\n    X_train, X_valid = X[train_data_image], X[test_data_image]\n    \n    y_train = to_categorical(y_train, num_classes=2)\n    y_valid = to_categorical(y_valid, num_classes=2)\n    \n    fold_model_history = fold_model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n                        epochs=25,callbacks=[callbacks_list],verbose=1)\n    \n    new_submission[\"predict\" + str(j)] = np.argmax(fold_model.predict(X_test,verbose=1), axis = 1)\n    \n    j = j + 1\n    \n\n","7a195148":"submission = pd.DataFrame()\nsubmission[\"image_names\"] = test_image_name\nsubmission[\"emergency_or_not\"] = new_submission.mode(axis=1)\nsubmission.to_csv(\".\/sub.csv\",index = False)\nsubmission.head()","9965a2fa":"## MODEL 3 : NASnet Large without Dropout and regularization\n## MODEL 4 : NASnet Large with Dropout and regularization\n","80d8436e":"**Both the classes i.e 0 and 1 are in comparable amount hence we are not dealing with imbalanced classification**","65df2775":"## Image Data Generator with augmentation","192d8814":"## INFERENCE FROM GRAPH\n- The oscillations are due to Stochastic gradient since the weights are updated after each example\n- From the graph it's obvious that our model is learning because loss is decreasing and acc is increasing with time.\n- The model starts slightly to overfit apprxoimately after 15 or 20 epochs which is quite clear in loss curve where val loss starts increasing and training loss continues to decrease","90aec8e8":"## Loading Dataset","fbca5bc5":"## Fine Tuning Last conv block of VGG with stable weights of TopClassifiers\n- Final Model = VGG(fine tune) + TopClassifier(stable wts)","f45619bc":"## Loading Images \n- ImageDataGenerator is the class used to load data and also apply any augmentation if needed all in one go\n- ImageDataGenerator has many attributes i.e flow_from_dataframe to get the file paths from data frame or flow_from_directory where in we provide the directory name from where we have to load data","9d8e3011":"## Bottleneck Predictions","aeb5c686":"### Compiling the model with a SGD\/momentum optimizer with a very slow learning rate.","30fed367":"## Creating Fully Connected layer i.e Top Classifier Architecture and training it for stable weights \n- The reason for not going directly for fine tuning is , if we try to fine tune the Top Classifier with random weights initilzation our previously pretrained weights will get destroyed and it will lead to problem of Exploding Gradients , Hence it's better to first train the top-classifier for stable weights then fine tune at last with small learing rate and using Stochastic gradients or Rmsprop so that the magnitude of updates remain small so that our peviously learned weights do not get wrecked","0b6e83e3":" ## INFERENCE FROM GRAPH\n- Accuracy curve has some oscillations at start , maybe because the model has not learnt much at that point and also because of random batches . But the oscillations become small once model has learnt after 10 epochs\n- Loss curves explains everything that the model is learning features and also after 8\/10 epochs the loss does not decreases significantly","6178d6ac":"## MODEL 1 : VGG16\n- **Using Vgg16 pre-trained network and then fine tuning the top layers of pre-trained network**\n- Below image shows the architecture of VGG16\n\n<img src=\"https:\/\/www.researchgate.net\/profile\/Clifford_Yang\/publication\/325137356\/figure\/fig2\/AS:670371271413777@1536840374533\/llustration-of-the-network-architecture-of-VGG-19-model-conv-means-convolution-FC-means.jpg\">\n\n\n- Pretrained network means we will use the weights which are provided by-default by imagenet .Since it was trained on Imagenet which has lots of classes using default weights will speed up the training process.Because the earlier layers of any CNN model recognizes basic features like edges,curves,etc then further layers combines them to make object like shape such as box,rectangle,circle,etc . Only the deeper layers are more specific to what we are building.\n\n- Since Deeper layers are specific to our problem we will fine-tune them that means we will freeze the earlier layers and only train the last Conv layer of vgg and the FC layer with very small weight updates.\n\nFor Fine Tuning I used the below keras blog as reference it explains fine tuning in depth:<br>\nhttps:\/\/blog.keras.io\/building-powerful-image-classification-models-using-very-little-data.html","d04a45bf":"### The output shape of the bottleneck features is same as the last layer of the model i.e 7,7,512 the first dimension shows no of data ","3c790d60":"## Visualising some of the images and their classes ","4699ce3a":"## MODEL 2 : VGG16 wih dropout and regularization","c7719785":"## Trying an advanced model i.e NASnet Large\n\n- 1. \" Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset.\"\n\n- 2. \" The key contribution of this work is the design of a new search space (which we call the \u201cNASNet search space\u201d) which enables transferability. In our experiments, we search for the best convolutional layer (or \u201ccell\u201d) on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we name a \u201cNASNet architecture\u201d. \"\n\n\n- We picked this model because of the first point which was discussed in paper as this model searches it's own architecture based on problem and data. Our data set is small and also the problem is difficult in itself , like we have to classify objects of same type i.e both are cars only either emergency or non-emergency so we thought it would be good to try this model.\n\n## NOTE :- \n- **We first trained NasnetLarge without regularization and Dropout which led to huge overfitting , Hence we went with NasnetLarge with regularization and dropout**","6c344d15":"## MODEL 5 :\n- A normal CNN architecture made by us which was trained for stratified 5 fold cross validation\n- Then we took the mode of this 5 predictions .\n- It was just a test and this model gave the accuracy of 0.86 on the public leaderboard","dc0b2dc0":"## Methods to improve accuracy\n- more aggresive data augmentation\n- more aggressive dropout\n- use of L1 and L2 regularization (also known as \"weight decay\")\n\n## We tried L2 rgularization\n- As there is a large gap between validation and train accuracy in attempt to reduce overfitting to  the model for better accuracy and predictions on unseen data\n","c82edcd7":"### Now for final training we used data augmentation techniques\n\n**Data Augmentation** :- In normal terms we take a single image and apply some transformations\/edit them to create new images this will help us to create more data and to significantly increase the diversity of data available for training models, without actually collecting new data.\n\n### **Augmentations Used**\n- Flipping the image horizontally\n- zooming randomly\n- shear_range : Shear Intensity (Shear angle in counter-clockwise direction in degrees)","ccb49e98":"## Data Preparation for stratified split","ddff0fac":"## PROBLEM STATEMENT\n\n**Emergency vs Non-Emergency Vehicle Classification**\n\nFatalities due to traffic delays of emergency vehicles such as ambulance & fire brigade is a huge problem. In daily life, we often see that emergency vehicles face difficulty in passing through traffic. So differentiating a vehicle into an emergency and non emergency category can be an important component in traffic monitoring as well as self drive car systems as reaching on time to their destination is critical for these services.\n\nIn Short : - **<i>Classification of  vehicle images as either belonging to the emergency vehicle or non-emergency vehicle category. Emergency vehicles usually includes police cars, ambulance and fire brigades.<\/i>**\n\n\n\n## Data Description\n- '0' class refers to non-emergency vehicles\n- '1' class refers to emergency vehicles .Eg Police car,ambulance ,etc\n","5e8857f2":"## Approach for solving the problem\n- Because of very little training data i.e 1646 in total it's very difficult to train a model with high performance and accuracy Hence we used transfer learning.\n- **Transfer Learning** (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks.\n\n\n\n<img src=\"https:\/\/miro.medium.com\/proxy\/1*1CxVzTNILTHgDs5yJO4W9A.png\" width=\"600\" height=\"400\">"}}