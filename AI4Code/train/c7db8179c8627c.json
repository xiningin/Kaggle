{"cell_type":{"1c958bc4":"code","e5acdc59":"code","cd7a61ea":"code","c3007b2a":"code","1f64ef47":"code","cc621389":"code","b178895c":"code","6378401f":"code","dc9d0ada":"code","7e5add64":"code","e9104301":"code","8cda76ab":"code","ae67ccd9":"code","80c42239":"code","5b8d4f2c":"code","e8d10500":"code","fba3f7ac":"code","1e87cee6":"code","dd366562":"code","5327d44d":"code","50a7384f":"code","ff67904d":"code","bf650a63":"code","c8e2ef79":"code","c21997bc":"code","48bf5f61":"code","f5458944":"code","fe5ffeb5":"code","30895395":"code","aeb18997":"code","fa5a1c95":"code","19a62d7b":"code","c90ace32":"code","dbfe1b61":"code","0bd0ac06":"code","da867f90":"code","30921a5f":"code","e0c06af9":"code","60595215":"code","e23f5554":"code","f156e41a":"code","9b7778ef":"code","ac038f3f":"code","fefed3bf":"code","4e693f01":"code","af8a2334":"code","8df4c85a":"code","87b9cb4e":"code","88403468":"code","f266cb97":"code","0cf146c2":"code","be8af2a2":"code","c346fa93":"code","c4ef5517":"code","62b52007":"code","5ecf1264":"code","2c9b53c5":"code","d7842e92":"code","19d2270c":"code","5d667df7":"code","e97f85e8":"code","b339a884":"code","6b0fc044":"code","9b3e8ef5":"code","a8023193":"code","e4725bb0":"code","9964a4dc":"code","28615e68":"code","a72a87ba":"code","c078107f":"code","eafdffd9":"code","d663a71b":"code","e806baac":"code","81de3200":"code","28d52e9c":"code","15a78b89":"code","5faf40c6":"code","12ac6cde":"code","34402444":"code","6ea01d07":"code","d6605e6d":"code","94c19458":"code","edd2a515":"code","a202792e":"code","2c3f5fd7":"code","3358255a":"code","8270f0ff":"code","61e2e39d":"code","ad114381":"code","7c06a77a":"code","6514b1b6":"code","0f8d6624":"code","6365cb62":"code","0e769f81":"code","9c462d3b":"code","be67df8b":"code","83f480a1":"code","666a4217":"code","9e220458":"code","3129a0d5":"code","14c3810c":"code","104fb17c":"code","6e3e0bad":"code","c5f6b4fc":"code","0c791e85":"code","bf59e128":"code","9adef526":"code","0c1bd6e2":"code","91d5a28e":"code","c7d1d5c8":"code","4e08f5ff":"code","5b7656c1":"code","752ca8c3":"code","2c5914b9":"code","3698a2e6":"code","4d73f490":"code","3dfb4131":"code","c59bb13f":"code","a0bedf34":"code","c88b6fda":"markdown","2d676e0c":"markdown","78298511":"markdown","c5cb7748":"markdown","6cf07efe":"markdown","b5395f1a":"markdown","31af434a":"markdown","97dba7d2":"markdown","4ce845d6":"markdown","53064ab6":"markdown","695b92d1":"markdown","06e479b1":"markdown","11a5d038":"markdown","d9223e45":"markdown","f4bf847b":"markdown","be263c24":"markdown","3c7ba568":"markdown","1475bd81":"markdown","aa2e2ce9":"markdown","c57ce3e1":"markdown","af535182":"markdown","dd0bd077":"markdown","16d80302":"markdown","d0e7d39c":"markdown","bcd7da8e":"markdown","9ae7ee34":"markdown","cecee736":"markdown","cb54e7a0":"markdown","7958e66b":"markdown","3d556021":"markdown","0752c120":"markdown","2f1f08e5":"markdown","ccd9e008":"markdown","21dc1332":"markdown","dc2c0119":"markdown","d34a2ca0":"markdown","e2145829":"markdown","baf5ea7d":"markdown","e9161561":"markdown"},"source":{"1c958bc4":"# for basic operations\nimport numpy as np \nimport pandas as pd \n\n# for visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pylab import rcParams\n# rcParams['figure.figsize'] = 4,4\n# plt.style.use('fivethirtyeight')\n\nfrom collections import Counter\n\n# for modeling \nimport sklearn\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report, plot_precision_recall_curve, precision_recall_curve\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets, metrics\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\n\nimport imblearn\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\n\n# to avoid warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.warn(\"this will not show\")","e5acdc59":"data = pd.read_csv('..\/input\/diabetic-data-cleaned-dummy\/diabetic_data_cleaned_dummy.csv', index_col=0).sample(5000)\ndf = data.copy()\n\ndf.head()","cd7a61ea":"from lazypredict.Supervised import LazyClassifier\n\ndf_5000 = df.sample(5000,random_state=42)\ny = df_5000['readmitted_YES']\nX = df_5000.drop('readmitted_YES', axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state =42)\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\nclf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\nmodels,predictions = clf.fit(X_train, X_test, y_train, y_test)\nmodels","c3007b2a":"ax = df['readmitted_YES'].value_counts(normalize=True).plot.bar()\ndef labels(ax):\n    for p in ax.patches:\n        ax.annotate(f\"%{p.get_height()*100:.2f}\", (p.get_x() + 0.15, p.get_height() * 1.005),size=11)\nlabels(ax)","1f64ef47":"# separating the dependent and independent data\nX = df.drop('readmitted_YES', axis=1)\ny = df['readmitted_YES']\n\n# the function train_test_split creates random data samples (default: 75-25%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\n# getting the shapes\nprint(f\"\"\"shape of X_train: {X_train.shape}\nshape of X_test\\t: {X_test.shape}\nshape of y_train: {y_train.shape}\nshape of y_test\\t: {y_test.shape}\"\"\")","cc621389":"# creating a standard scaler\nsc = StandardScaler()\n\n# fitting independent data to the model\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","b178895c":"cv_acc_train = {}\ncv_acc_test = {}\ncv_TPR = {}\ncv_FPR = {}\ncv_AUC = {}","6378401f":"def plot_result(model, name:str):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    # Evaluation based on a 10-fold cross-validation\n    scoring = ['balanced_accuracy', 'recall_macro']\n    scores_train = cross_val_score(model, X_train, y_train, cv=10, scoring = 'balanced_accuracy')\n    scores_test = cross_val_score(model, X_test, y_test, cv=10, scoring = 'balanced_accuracy')  \n    cv_acc_train[name] = round(scores_train.mean(), 4)*100  # balanced accuracy\n    cv_acc_test[name] = round(scores_test.mean(), 4)*100  # balanced accuracy\n    cv_TPR[name] = (confusion_matrix(y_test, y_pred)[1][1]\/confusion_matrix(y_test, y_pred)[1].sum())*100  # recall (Max)\n    cv_FPR[name] = (confusion_matrix(y_test, y_pred)[0][1]\/confusion_matrix(y_test, y_pred)[0].sum())*100  # fallout (Min)\n    \n    # accuracy scores\n    print('Average Balanced Accuracy (CV=10), Test Set:', scores_test.mean())  \n    print('Average Balanced Accuracy (CV=10), Training Set: ', scores_train.mean())\n\n    # print classification report\n    print(classification_report(y_test, y_pred, zero_division=0))\n\n    # Plot Confusion Matrix\n    plot_confusion_matrix(model, X_test, y_test)\n    plt.show()","dc9d0ada":"from sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import plot_confusion_matrix, classification_report, confusion_matrix","7e5add64":"dtc = DecisionTreeClassifier()\nplot_result(dtc, \"dtc\")","e9104301":"# plot tree\n# plt.figure(figsize=(16,6))\n# plot_tree(dtc, filled = True, class_names=[\"-1\", \"1\"], feature_names=X.columns, fontsize=11);","8cda76ab":"cv_acc_train, cv_acc_test, cv_TPR, cv_FPR","ae67ccd9":"from sklearn.linear_model import LogisticRegression","80c42239":"lr = LogisticRegression()\nplot_result(lr, \"lr\")","5b8d4f2c":"svc = SVC(probability=True)  # default values\nplot_result(svc, \"svc\")","e8d10500":"from sklearn.neighbors import NearestCentroid\nfrom sklearn.metrics import plot_confusion_matrix, classification_report, confusion_matrix","fba3f7ac":"nc = NearestCentroid()\nplot_result(nc, \"nc\")","1e87cee6":"from sklearn.ensemble import RandomForestClassifier","dd366562":"rfc = RandomForestClassifier()\nplot_result(rfc, \"rfc\")","5327d44d":"def plot_feature_importances(model):\n    feature_imp = pd.Series(model.feature_importances_,index=X.columns).sort_values(ascending=False)[:10]\n\n    sns.barplot(x=feature_imp, y=feature_imp.index)\n    plt.title(\"Feature Importance\")\n    plt.show()\n\n    print(f\"Top 10 Feature Importance for {str(model).split('(')[0]}\\n\\n\",feature_imp[:10],sep='')","50a7384f":"plot_feature_importances(rfc)","ff67904d":"from sklearn.ensemble import GradientBoostingClassifier","bf650a63":"gbc = GradientBoostingClassifier(random_state=42)\nplot_result(gbc, \"gbc\")","c8e2ef79":"plot_feature_importances(gbc)","c21997bc":"from sklearn.naive_bayes import GaussianNB","48bf5f61":"nb = GaussianNB()\nplot_result(nb, \"nb\")","f5458944":"from sklearn.neighbors import KNeighborsClassifier","fe5ffeb5":"knn = KNeighborsClassifier()\nplot_result(knn, \"knn\")","30895395":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score","aeb18997":"xgb = XGBClassifier(eval_metric = \"logloss\")\nplot_result(xgb, \"xgb\")","fa5a1c95":"plot_feature_importances(xgb)","19a62d7b":"from xgboost import plot_importance\nplot_importance(xgb,max_num_features=10)\nplt.xlabel('The F-Score for each features')\nplt.ylabel('Importances')\nplt.show()","c90ace32":"\ndef AUC(cv_AUC, X_test=X_test):\n    dtc_auc= roc_auc_score(y_test,dtc.predict(X_test)) #Decision Tree Classifier\n    lr_auc= roc_auc_score(y_test, lr.decision_function(X_test))#logistic regression\n    svc_auc= roc_auc_score(y_test, svc.decision_function(X_test))#Support Vector Classifier\n    nc_auc= roc_auc_score(y_test, nc.predict(X_test))#Nearest Centroid Classifier\n    rfc_auc= roc_auc_score(y_test, rfc.predict_proba(X_test)[:,1])#Randomforest Classifier\n    gbc_auc= roc_auc_score(y_test, gbc.predict_proba(X_test)[:,1])#GradientBoosting Classifier\n    nb_auc= roc_auc_score(y_test, nb.predict_proba(X_test)[:,1])#Naive Bayes Classifier\n    knn_auc= roc_auc_score(y_test, knn.predict(X_test))#KNeighbors Classifier\n    xgb_auc= roc_auc_score(y_test, xgb.predict_proba(X_test)[:,1])#XGBoost Classifier\n\n    cv_AUC={'dtc': dtc_auc,\n           'lr': lr_auc,\n           'svc':svc_auc,\n           'nc':nc_auc,\n           'rfc':rfc_auc,\n           'gbc':gbc_auc,\n           'nb':nb_auc,\n           'knn':knn_auc,\n           'xgb':xgb_auc}\n    return cv_AUC","dbfe1b61":"cv_AUC = AUC(cv_AUC)\ndf_eval = pd.DataFrame(data={'model': list(cv_acc_test.keys()), \n                             'bal_acc_train':list(cv_acc_train.values()),\n                             'bal_acc_test': list(cv_acc_test.values()), \n                             'recall': list(cv_TPR.values()), \n                             'fallout':list(cv_FPR.values()),\n                              'AUC': list(cv_AUC.values())}).round(2)\ndf_eval","0bd0ac06":"def plot_ROC(X_test=X_test, y_test=y_test):\n    fpr_dtc, tpr_dtc, thresholds = roc_curve(y_test,dtc.predict(X_test)) #Decision Tree Classifier\n    fpr_lr, tpr_lr, thresholds = roc_curve(y_test, lr.decision_function(X_test))#logistic regression\n    fpr_svc, tpr_svc, thresholds = roc_curve(y_test, svc.decision_function(X_test))#Support Vector Classifier\n    fpr_nc, tpr_nc, thresholds = roc_curve(y_test, nc.predict(X_test))#Nearest Centroid Classifier\n    fpr_rfc, tpr_rfc, thresholds = roc_curve(y_test, rfc.predict_proba(X_test)[:,1])#Randomforest Classifier\n    fpr_gbc, tpr_gbc, thresholds = roc_curve(y_test, gbc.predict_proba(X_test)[:,1])#GradientBoosting Classifier\n    fpr_nb, tpr_nb, thresholds = roc_curve(y_test, nb.predict_proba(X_test)[:,1])#Naive Bayes Classifier\n    fpr_knn, tpr_knn, thresholds = roc_curve(y_test, knn.predict(X_test))#KNeighbors Classifier\n    fpr_xgb, tpr_xgb, thresholds = roc_curve(y_test, xgb.predict_proba(X_test)[:,1])#XGBoost Classifier\n\n    #compare the ROC curve between different models\n    plt.figure(figsize=(10,10))\n    plt.plot(fpr_dtc, tpr_dtc, label='Decision Tree Classifier')\n    plt.plot(fpr_lr, tpr_lr, label='Logistic Regression')\n    plt.plot(fpr_svc, tpr_svc, label='Support Vector Classifier')\n    plt.plot(fpr_nc, tpr_nc, label='Nearest Centroid Classifier')\n    plt.plot(fpr_rfc, tpr_rfc, label='Randomforest Classifier')\n    plt.plot(fpr_gbc, tpr_gbc, label='GradientBoosting Classifier')\n    plt.plot(fpr_nb, tpr_nb, label='Naive Bayes Classifier')\n    plt.plot(fpr_knn, tpr_knn, label='KNeighbors Classifier')\n    plt.plot(fpr_xgb, tpr_xgb, label='XGBoost Classifier')\n\n    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n             label='random', alpha=.8)\n    plt.xlim([0,1])\n    plt.ylim([0,1])\n    plt.xticks(np.arange(0,1.1,0.1))\n    plt.yticks(np.arange(0,1.1,0.1))\n    plt.grid()\n    plt.legend()\n    plt.axes().set_aspect('equal')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n\nplot_ROC()","da867f90":"fig, ax = plt.subplots(1,4, figsize=(20, 4))\nsns.barplot(x=\"bal_acc_train\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0])\nax[0].set_title(\"Unbalanced Train Acc\")\nsns.barplot(x=\"bal_acc_test\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[1])\nax[1].set_title(\"Unbalanced Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[2])\nax[2].set_title(\"Unbalanced Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[3])\nax[3].set_title(\"Unbalanced Test FPR\")\nplt.show()","30921a5f":"y_test.value_counts(normalize=True)","e0c06af9":"y_train.value_counts(normalize=True)","60595215":"# pip install imblearn\nfrom imblearn import under_sampling, over_sampling\nfrom imblearn.over_sampling import SMOTE","e23f5554":"oversmote = SMOTE()\nX_train_os, y_train_os= oversmote.fit_resample(X_train, y_train)","f156e41a":"ax = y_train_os.value_counts().plot.bar(color=[\"blue\", \"red\"])\ndef labels(ax):\n    for p in ax.patches:\n        ax.annotate(f\"{p.get_height()}\", (p.get_x() + 0.15, p.get_height()+200),size=8)\nlabels(ax)\nplt.show()","9b7778ef":"X_train_os.shape","ac038f3f":"cv_acc_balance_train = {}\ncv_acc_balance_test = {}\ncv_TPR_balance = {}\ncv_FPR_balance = {}\ncv_AUC_balance = {}","fefed3bf":"def plot_result_smote(model, name:str):\n    model.fit(X_train_os, y_train_os)\n    y_pred = model.predict(X_test)\n\n    # Evaluation based on a 10-fold cross-validation\n    scoring = ['balanced_accuracy', 'recall_macro']\n    scores_train = cross_val_score(model, X_train, y_train, cv=10, scoring = 'balanced_accuracy')\n    scores_test = cross_val_score(model, X_test, y_test, cv=10, scoring = 'balanced_accuracy')\n    cv_acc_balance_train[name] = round(scores_train.mean(), 4)*100  # balanced accuracy\n    cv_acc_balance_test[name] = round(scores_test.mean(), 4)*100  # balanced accuracy\n    cv_TPR_balance[name] = (confusion_matrix(y_test, y_pred)[1][1]\/confusion_matrix(y_test, y_pred)[1].sum())*100  # recall (max)\n    cv_FPR_balance[name] = (confusion_matrix(y_test, y_pred)[0][1]\/confusion_matrix(y_test, y_pred)[0].sum())*100  # fallout (min)\n    \n    # accuracy scores\n    print('Average Balanced Accuracy (CV=10), Test Set:', scores_test.mean())  \n    print('Average Balanced Accuracy (CV=10), Training Set: ', scores_train.mean())\n\n    # print classification report\n    print(classification_report(y_test, y_pred, zero_division=0))\n\n    # Plot Confusion Matrix\n    plot_confusion_matrix(model, X_test, y_test)\n    plt.show()","4e693f01":"# Decision tree\ndtc = DecisionTreeClassifier()\n\nplot_result_smote(dtc, \"dtc\")","af8a2334":"# Logistic Regression\nlr = LogisticRegression()\nplot_result_smote(lr, \"lr\")","8df4c85a":"# NearestCentroid\nnc = NearestCentroid()\nplot_result_smote(nc, \"nc\")","87b9cb4e":"# SVC\nsvc = SVC()\nplot_result_smote(svc, \"svc\")","88403468":"# Random Forest\nrfc = RandomForestClassifier()\nplot_result_smote(rfc, \"rfc\")","f266cb97":"# Gradient Boost\ngbc = GradientBoostingClassifier(random_state=42)\nplot_result_smote(gbc, \"gbc\")","0cf146c2":"# Naive Bayes\nnb = GaussianNB()\nplot_result_smote(nb, \"nb\")","be8af2a2":"# kNN\nknn = KNeighborsClassifier()\nplot_result_smote(knn, \"knn\")","c346fa93":"# XGBOOST\nxgb = XGBClassifier(eval_metric = \"logloss\", random_state=42)\nplot_result_smote(xgb, \"xgb\")","c4ef5517":"cv_AUC_balance = AUC(cv_AUC_balance)","62b52007":"df_eval_smote = pd.DataFrame(data={'model': list(cv_acc_balance_test.keys()), \n                                   'bal_acc_train':list(cv_acc_balance_train.values()),\n                                   'bal_acc_test': list(cv_acc_balance_test.values()),\n                                   'recall': list(cv_TPR_balance.values()), \n                                   'fallout':list(cv_FPR_balance.values()),\n                                   'AUC': list(cv_AUC_balance.values())}).round(2)\ndf_eval_smote","5ecf1264":"fig, ax = plt.subplots(2,4, figsize=(20, 8))\n\nsns.barplot(x=\"bal_acc_train\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,0])\nax[0,0].set_title(\"Unbalanced Train Acc\")\nsns.barplot(x=\"bal_acc_test\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,1])\nax[0,1].set_title(\"Unbalanced Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,2])\nax[0,2].set_title(\"Unbalanced Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,3])\nax[0,3].set_title(\"Unbalanced Test FPR\")\n\nsns.barplot(x=\"bal_acc_train\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,0])\nax[1,0].set_title(\"Smote Model Train Acc\")\nsns.barplot(x=\"bal_acc_test\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,1])\nax[1,1].set_title(\"Smote Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,2])\nax[1,2].set_title(\"Smote Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,3])\nax[1,3].set_title(\"Smote Model Test FPR\")\n\nplt.tight_layout()\nplt.show()","2c9b53c5":"plot_ROC()","d7842e92":"import imblearn\nfrom imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours, NearMiss","19d2270c":"under_sampler = RandomUnderSampler(random_state=42)\nX_train_rus, y_train_rus = under_sampler.fit_sample(X_train, y_train)","5d667df7":"ax = y_train_rus.value_counts().plot.bar(color=[\"blue\", \"red\"])\nlabels(ax)\nplt.show()","e97f85e8":"cv_acc_rus_train = {}\ncv_acc_rus_test = {}\ncv_TPR_rus = {}\ncv_FPR_rus = {}\ncv_AUC_rus = {}","b339a884":"def plot_result_rus(model, name:str):\n    model.fit(X_train_rus, y_train_rus)\n    y_pred = model.predict(X_test)\n\n    # Evaluation based on a 10-fold cross-validation\n    scoring = ['balanced_accuracy', 'recall_macro']\n    scores_train = cross_val_score(model, X_train, y_train, cv=10, scoring = 'balanced_accuracy')\n    scores_test = cross_val_score(model, X_test, y_test, cv=10, scoring = 'balanced_accuracy')\n    cv_acc_rus_train[name] = round(scores_train.mean(), 4)*100  # balanced accuracy\n    cv_acc_rus_test[name] = round(scores_test.mean(), 4)*100  # balanced accuracy\n    cv_TPR_rus[name] = (confusion_matrix(y_test, y_pred)[1][1]\/confusion_matrix(y_test, y_pred)[1].sum())*100  # recall (max)\n    cv_FPR_rus[name] = (confusion_matrix(y_test, y_pred)[0][1]\/confusion_matrix(y_test, y_pred)[0].sum())*100  # fallout (min)\n    \n    # accuracy scores\n    print('Average Balanced Accuracy (CV=10), Test Set:', scores_test.mean())  \n    print('Average Balanced Accuracy (CV=10), Training Set: ', scores_train.mean())\n\n    # print classification report\n    print(classification_report(y_test, y_pred, zero_division=0))\n\n    # Plot Confusion Matrix\n    plot_confusion_matrix(model, X_test, y_test)\n    plt.show()","6b0fc044":"# Decision tree\ndtc = DecisionTreeClassifier()\n\nplot_result_rus(dtc, \"dtc\")","9b3e8ef5":"# Logistic Regression\nlr = LogisticRegression()\nplot_result_rus(lr, \"lr\")","a8023193":"# NearestCentroid\nnc = NearestCentroid()\nplot_result_rus(nc, \"nc\")","e4725bb0":"SVC\nsvc = SVC()\nplot_result_rus(svc, \"svc\")","9964a4dc":"# Random Forest\nrfc = RandomForestClassifier()\nplot_result_rus(rfc, \"rfc\")","28615e68":"# Gradient Boost\ngbc = GradientBoostingClassifier(random_state=42)\nplot_result_rus(gbc, \"gbc\")","a72a87ba":"# Naive Bayes\nnb = GaussianNB()\nplot_result_rus(nb, \"nb\")","c078107f":"# kNN\nknn = KNeighborsClassifier()\nplot_result_rus(knn, \"knn\")","eafdffd9":"# XGBOOST\nxgb = XGBClassifier(eval_metric = \"logloss\",random_state=42)\nplot_result_rus(xgb, \"xgb\");","d663a71b":"cv_AUC_rus = AUC(cv_AUC_rus)","e806baac":"df_eval_rus = pd.DataFrame(data={'model': list(cv_acc_rus_train.keys()), \n                             'bal_acc_train':list(cv_acc_rus_train.values()),\n                             'bal_acc_test': list(cv_acc_rus_test.values()), \n                             'recall': list(cv_TPR_rus.values()), \n                             'fallout':list(cv_FPR_rus.values()),\n                             'AUC': list(cv_AUC_rus.values())}).round(2)    \ndf_eval_rus","81de3200":"fig, ax = plt.subplots(3,4, figsize=(20, 12))\n\nsns.barplot(x=\"bal_acc_train\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,0])\nax[0,0].set_title(\"Unbalanced Train Acc\")\nsns.barplot(x=\"bal_acc_test\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,1])\nax[0,1].set_title(\"Unbalanced Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,2])\nax[0,2].set_title(\"Unbalanced Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,3])\nax[0,3].set_title(\"Unbalanced Test FPR\")\n\nsns.barplot(x=\"bal_acc_train\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,0])\nax[1,0].set_title(\"Smote Model Train Acc\")\nsns.barplot(x=\"bal_acc_test\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,1])\nax[1,1].set_title(\"Smote Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,2])\nax[1,2].set_title(\"Smote Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,3])\nax[1,3].set_title(\"Smote Model Test FPR\")\n\nsns.barplot(x=\"bal_acc_train\", y=\"model\", data=df_eval_rus.sort_values(by=\"recall\"), ax=ax[2,0])\nax[2,0].set_title(\"RUS_Featured Model Test Acc\")\nsns.barplot(x=\"bal_acc_test\", y=\"model\", data=df_eval_rus.sort_values(by=\"recall\"), ax=ax[2,1])\nax[2,1].set_title(\"RUS_Featured Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_rus.sort_values(by=\"recall\"), ax=ax[2,2])\nax[2,2].set_title(\"RUS_Featured Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_rus.sort_values(by=\"recall\"), ax=ax[2,3])\nax[2,3].set_title(\"RUS_Featured Model Test FPR\")\n\nplt.tight_layout()\nplt.show()","28d52e9c":"plot_ROC()","15a78b89":"from sklearn.decomposition import PCA","5faf40c6":"pca = PCA().fit(X_train_os)","12ac6cde":"fig, ax = plt.subplots(figsize=(20,8))\nxi = np.arange(0, 54, step=1)\ny = np.cumsum(pca.explained_variance_ratio_[0:160:1])\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, y, marker='.', linestyle='--', color='b')\n\nplt.xlabel('Number of Components')\nplt.xticks(np.arange(0, 54, step=2), rotation=90) #change from 0-based array index to 1-based human-readable label\nplt.ylabel('Cumulative variance (%)')\nplt.title('The number of components needed to explain variance')\n\nplt.axhline(y=0.95, color='r', linestyle='-')\nplt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n\nax.grid(axis='x')\nplt.show()","34402444":"pca = PCA(n_components=40)\npca.fit(X_train_os)\nper_var = np.round(pca.explained_variance_ratio_ * 100, 1)\nlabels = ['PC' + str(x) for x in range(1,len(per_var)+1)]\n\nplt.figure(figsize=(20,6))\nplt.bar(x=range(len(per_var)), height=per_var, tick_label=labels)\nplt.title('Total explained variance {}'.format(np.round(sum(per_var),2)))\nplt.ylabel('Explained variance in percent')\nplt.xticks(rotation=90)\nplt.show()","6ea01d07":"X_train_os_pca = pca.transform(X_train_os)\npd.DataFrame(X_train_os_pca)","d6605e6d":"# Top 20 columns that have the greatest impact\nloading_scores = pd.Series(pca.components_[0], index=X.columns)\nloading_scores.abs().sort_values(ascending=False)[:20]","94c19458":"X_test_pca = pca.transform(X_test)","edd2a515":"cv_acc_balance_train_pca = {}\ncv_acc_balance_test_pca = {}\ncv_TPR_balance_pca = {}\ncv_FPR_balance_pca = {}\ncv_AUC_balance_pca = {}","a202792e":"def plot_result_smoted_pca(model, name:str):\n    model.fit(X_train_os_pca, y_train_os)\n    y_pred = model.predict(X_test_pca)\n\n    # Evaluation based on a 10-fold cross-validation\n    scoring = ['balanced_accuracy', 'recall_macro']\n    scores_train = cross_val_score(model, X_train_os_pca, y_train_os, cv=10, scoring = 'balanced_accuracy')\n    scores_test = cross_val_score(model, X_test_pca, y_test, cv=10, scoring = 'balanced_accuracy')\n    cv_acc_balance_train_pca[name] = round(scores_train.mean(), 4)*100  # balanced accuracy\n    cv_acc_balance_test_pca[name] = round(scores_test.mean(), 4)*100  # balanced accuracy\n    cv_TPR_balance_pca[name] = (confusion_matrix(y_test, y_pred)[1][1]\/confusion_matrix(y_test, y_pred)[1].sum())*100  # recall (max)\n    cv_FPR_balance_pca[name] = (confusion_matrix(y_test, y_pred)[0][1]\/confusion_matrix(y_test, y_pred)[0].sum())*100  # fallout (min)\n\n    # accuracy scores\n    print('Average Balanced Accuracy (CV=10), Test Set:', scores_test.mean())  \n    print('Average Balanced Accuracy (CV=10), Training Set: ', scores_train.mean())\n\n    # print classification report\n    print(classification_report(y_test, y_pred, zero_division=0))\n\n    # Plot confusion matrix\n    plt.figure(figsize=(3,3))\n    plot_confusion_matrix(model, X_test_pca, y_test)\n    plt.show()","2c3f5fd7":"# Decision tree\ndtc = DecisionTreeClassifier()\nplot_result_smoted_pca(dtc, \"dtc\")","3358255a":"# Logistic Regression\nlr = LogisticRegression()\nplot_result_smoted_pca(lr, \"lr\")","8270f0ff":"# NearestCentroid\nnc = NearestCentroid()\nplot_result_smoted_pca(nc, \"nc\")","61e2e39d":"# SVC\nsvc = SVC()\nplot_result_smoted_pca(svc, \"svc\")","ad114381":"# Random Forest\nrfc = RandomForestClassifier()\nplot_result_smoted_pca(rfc, \"rfc\")","7c06a77a":"# Gradient Boost\ngbc = GradientBoostingClassifier()\nplot_result_smoted_pca(gbc, \"gbc\")","6514b1b6":"# Naive Bayes\nnb = GaussianNB()\nplot_result_smoted_pca(nb, \"nb\")","0f8d6624":"# kNN\nknn = KNeighborsClassifier()\nplot_result_smoted_pca(knn, \"knn\")","6365cb62":"# XGBOOST\nxgb = XGBClassifier(eval_metric = \"logloss\")\nplot_result_smoted_pca(xgb, \"xgb\");","0e769f81":"cv_AUC_balance_pca = AUC(cv_AUC_balance_pca, X_test_pca)\ncv_AUC_balance_pca","9c462d3b":"df_eval_smote_pca = pd.DataFrame(data={'model': list(cv_acc_balance_train_pca.keys()), \n                                       'bal_acc_train':list(cv_acc_balance_train_pca.values()),\n                                       'bal_acc_test': list(cv_acc_balance_test_pca.values()),\n                                       'recall': list(cv_TPR_balance_pca.values()), \n                                       'fallout':list(cv_FPR_balance_pca.values()),\n                                       'AUC': list(cv_AUC_rus.values())}).round(2)\ndf_eval_smote_pca","be67df8b":"fig, ax = plt.subplots(4,4, figsize=(20, 16))\n\nsns.barplot(x=\"bal_acc_train\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,0])\nax[0,0].set_title(\"Unbalanced Train Acc\")\nsns.barplot(x=\"bal_acc_test\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,1])\nax[0,1].set_title(\"Unbalanced Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,2])\nax[0,2].set_title(\"Unbalanced Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,3])\nax[0,3].set_title(\"Unbalanced Test FPR\")\n\nsns.barplot(x=\"bal_acc_train\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,0])\nax[1,0].set_title(\"Smote Model Train Acc\")\nsns.barplot(x=\"bal_acc_test\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,1])\nax[1,1].set_title(\"Smote Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,2])\nax[1,2].set_title(\"Smote Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,3])\nax[1,3].set_title(\"Smote Model Test FPR\")\n\nsns.barplot(x=\"bal_acc_train\", y=\"model\", data=df_eval_rus.sort_values(by=\"recall\"), ax=ax[2,0])\nax[2,0].set_title(\"RUS_Featured Model Test Acc\")\nsns.barplot(x=\"bal_acc_test\", y=\"model\", data=df_eval_rus.sort_values(by=\"recall\"), ax=ax[2,1])\nax[2,1].set_title(\"RUS_Featured Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_rus.sort_values(by=\"recall\"), ax=ax[2,2])\nax[2,2].set_title(\"RUS_Featured Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_rus.sort_values(by=\"recall\"), ax=ax[2,3])\nax[2,3].set_title(\"RUS_Featured Model Test FPR\")\n\nsns.barplot(x=\"bal_acc_train\", y=\"model\", data=df_eval_smote_pca.sort_values(by=\"recall\"), ax=ax[3,0])\nax[3,0].set_title(\"Smoted_PCA Model Train Acc\")\nsns.barplot(x=\"bal_acc_test\", y=\"model\", data=df_eval_smote_pca.sort_values(by=\"recall\"), ax=ax[3,1])\nax[3,1].set_title(\"Smoted_PCA Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_smote_pca.sort_values(by=\"recall\"), ax=ax[3,2])\nax[3,2].set_title(\"Smoted_PCA Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_smote_pca.sort_values(by=\"recall\"), ax=ax[3,3])\nax[3,3].set_title(\"Smoted_PCA Model Test FPR\")\n\nplt.tight_layout()\nplt.show()","83f480a1":"plot_ROC(X_test_pca)","666a4217":"df_eval[\"type\"] = \"Unbalanced\"\ndf_eval_smote[\"type\"] = \"Smote\"\ndf_eval_rus[\"type\"] = \"RUS\"\ndf_eval_smote_pca[\"type\"] = \"Smote_PCA\"","9e220458":"frames = [df_eval, df_eval_smote, df_eval_rus, df_eval_smote_pca]\ndf_result = pd.concat(frames, ignore_index=True)\ndf_result['model'] = df_result['model'].str.upper()\ndf_result[[\"recall\", \"fallout\", \"bal_acc_train\", \"bal_acc_test\",'AUC']] = df_result[[\"recall\", \"fallout\",  \"bal_acc_train\", \"bal_acc_test\",'AUC']].apply(lambda x: np.round(x, 2))","3129a0d5":"df_result","14c3810c":"sns.relplot(x=\"recall\", y=\"AUC\", hue=\"model\", size=\"bal_acc_test\", \n            sizes=(40, 400), col=\"type\", alpha=1, palette=\"bright\", height=4, legend='full', data=df_result)","104fb17c":"from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nfrom sklearn.pipeline import Pipeline","6e3e0bad":"cv_acc_balance_train_tuned = {}\ncv_acc_balance_test_tuned = {}\ncv_TPR_balance_tuned = {}\ncv_FPR_balance_tuned = {}\ncv_AUC_balance_tuned = {}","c5f6b4fc":"def plot_result_smote_tuned(model, name:str):\n    model.fit(X_train_os, y_train_os)\n    y_pred = model.predict(X_test)\n\n    # Evaluation based on a 10-fold cross-validation\n    scores_train = cross_val_score(model, X_train_os, y_train_os, cv=10, scoring = 'balanced_accuracy')\n    scores_test = cross_val_score(model, X_test, y_test, cv=10, scoring = 'balanced_accuracy')\n    cv_acc_balance_train_tuned[name] = round(scores_train.mean(), 4)*100  # balanced accuracy\n    cv_acc_balance_test_tuned[name] = round(scores_test.mean(), 4)*100  # balanced accuracy\n    cv_TPR_balance_tuned[name] = (confusion_matrix(y_test, y_pred)[1][1]\/confusion_matrix(y_test, y_pred)[1].sum())*100  # recall (max)\n    cv_FPR_balance_tuned[name] = (confusion_matrix(y_test, y_pred)[0][1]\/confusion_matrix(y_test, y_pred)[0].sum())*100  # fallout (min)\n    \n    # accuracy scores\n    print('Average Balanced Accuracy (CV=10), Test Set:', scores_test.mean())  \n    print('Average Balanced Accuracy (CV=10), Training Set: ', scores_train.mean())\n\n    # print classification report\n    print(classification_report(y_test, y_pred, zero_division=0))\n\n    # Plot confusion matrix\n    plt.figure(figsize=(3,3))\n    plot_confusion_matrix(model, X_test, y_test)\n    plt.show()","0c791e85":"# Decision Tree\nparams = {'criterion' : [\"gini\", \"entropy\"],\n          'max_depth':[2, 5, 10], \n          'min_samples_leaf':[20, 10, 1],\n          'min_samples_split':[2, 4, 8]}\n\ngrid_searcher = GridSearchCV(DecisionTreeClassifier(), params)\ngrid_searcher.fit(X_train_os, y_train_os)\ndtc = grid_searcher\nprint(grid_searcher.best_estimator_)\nplot_result_smote_tuned(grid_searcher, \"dtc\")","bf59e128":"# Logistic Regression\nparams = {'penalty' : ['l1', 'l2'],\n          'C' : np.logspace(-4, 4, 10),\n          'max_iter':[200, 300],\n          'solver' : ['liblinear']}\n\ngrid_searcher = GridSearchCV(LogisticRegression(), params)\ngrid_searcher.fit(X_train_os, y_train_os)\nlr = grid_searcher\nprint(grid_searcher.best_estimator_)\nplot_result_smote_tuned(grid_searcher.best_estimator_, \"lr\")","9adef526":"# NearestCentroid\nparams={'metric': ['euclidean', 'manhattan'],\n        'shrink_threshold' : [0.01, 0.1, 0.5]}\n\ngrid_searcher = GridSearchCV(NearestCentroid(), params)\ngrid_searcher.fit(X_train_os, y_train_os)\nnc = grid_searcher\nprint(grid_searcher.best_estimator_)\nplot_result_smote_tuned(grid_searcher.best_estimator_, \"nc\")","0c1bd6e2":"# SVC\nparams = {'kernel':(['linear','rbf']), \n          'C':[0.01,1,10], \n          'gamma':[0.01, 0.1, 1]}\n\ngrid_searcher = GridSearchCV(SVC(), params)\ngrid_searcher.fit(X_train_os, y_train_os)\nsvc = grid_searcher\nprint(grid_searcher.best_estimator_)\nplot_result_smote_tuned(grid_searcher.best_estimator_, \"svc\")","91d5a28e":"# Random Forest\nparams={'criterion': ['entropy', 'gini'],\n        'n_estimators' : [10,50,100],\n        'max_features' : [5,15,25]}\n\ngrid_searcher = GridSearchCV(RandomForestClassifier(), params)\ngrid_searcher.fit(X_train_os, y_train_os)\nrfc = grid_searcher\nprint(grid_searcher.best_estimator_)\nplot_result_smote_tuned(grid_searcher.best_estimator_, \"rfc\")","c7d1d5c8":"# Gradient Boosting Classifier\n\nparams={\n       \"learning_rate\": [1],\n      \"min_samples_split\": [50, 10, 2],\n       \"min_samples_leaf\": [1, 5, 10],\n       \"max_depth\":[3,4,5],\n       \"subsample\":[0.5, 1.0],\n       \"n_estimators\":[10, 50, 100],\n       \"random_state\":[42]}\n\ngrid_searcher = RandomizedSearchCV(GradientBoostingClassifier(), params)\ngrid_searcher.fit(X_train_os, y_train_os)\ngbc = grid_searcher\nprint(grid_searcher.best_estimator_)\nplot_result_smote_tuned(gbc, \"gbc\")","4e08f5ff":"# Naive Bayes has no tuning\nnb = GaussianNB()\nplot_result_smote_tuned(nb, \"nb\")","5b7656c1":"# kNN\nparams = {'n_neighbors':[2, 3, 5],\n         'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute']}\n\ngrid_searcher = GridSearchCV(KNeighborsClassifier(), params)\ngrid_searcher.fit(X_train_os, y_train_os)\nknn = grid_searcher\nprint(grid_searcher.best_estimator_)\nplot_result_smote_tuned(grid_searcher.best_estimator_, \"knn\")","752ca8c3":"# XGBOOST\nparams = {\"n_estimators\": [100, 300],\n          \"subsample\":[0.5,0.8,1],\n          \"max_depth\":[15,25],\n          \"learning_rate\":[0.01,0.1,0.3]}\n\ngrid_searcher = GridSearchCV(XGBClassifier(), params)\ngrid_searcher.fit(X_train_os, y_train_os)\nxgb = grid_searcher\nprint(grid_searcher.best_estimator_)\nplot_result_smote_tuned(grid_searcher.best_estimator_, \"xgb\")\n\n# xgb = XGBClassifier(eval_metric = \"logloss\",n_estimators=25, learning_rate=0.01,  max_depth=3, random_state=42)\n# plot_result_smote_tuned(xgb, \"xgb\");","2c5914b9":"cv_AUC_balance_tuned = AUC(cv_AUC_balance_tuned)","3698a2e6":"df_eval_smote_tuned = pd.DataFrame(data={'model': list(cv_acc_balance_train_tuned.keys()), \n                                               'bal_acc_train':list(cv_acc_balance_train_tuned.values()),\n                                               'bal_acc_test': list(cv_acc_balance_test_tuned.values()),\n                                               'recall': list(cv_TPR_balance_tuned.values()), \n                                               'fallout':list(cv_FPR_balance_tuned.values()),\n                                                'AUC': list(cv_AUC_balance_tuned.values())}).round(2)\ndf_eval_smote_tuned","4d73f490":"fig, ax = plt.subplots(5,4, figsize=(20, 20))\n\nsns.barplot(x=\"bal_acc_train\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,0])\nax[0,0].set_title(\"Unbalanced Train Acc\")\nsns.barplot(x=\"bal_acc_test\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,1])\nax[0,1].set_title(\"Unbalanced Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,2])\nax[0,2].set_title(\"Unbalanced Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,3])\nax[0,3].set_title(\"Unbalanced Test FPR\")\n\nsns.barplot(x=\"bal_acc_train\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,0])\nax[1,0].set_title(\"Smote Model Train Acc\")\nsns.barplot(x=\"bal_acc_test\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,1])\nax[1,1].set_title(\"Smote Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,2])\nax[1,2].set_title(\"Smote Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,3])\nax[1,3].set_title(\"Smote Model Test FPR\")\n\nsns.barplot(x=\"bal_acc_train\", y=\"model\", data=df_eval_rus.sort_values(by=\"recall\"), ax=ax[2,0])\nax[2,0].set_title(\"RUS_Featured Model Test Acc\")\nsns.barplot(x=\"bal_acc_test\", y=\"model\", data=df_eval_rus.sort_values(by=\"recall\"), ax=ax[2,1])\nax[2,1].set_title(\"RUS_Featured Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_rus.sort_values(by=\"recall\"), ax=ax[2,2])\nax[2,2].set_title(\"RUS_Featured Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_rus.sort_values(by=\"recall\"), ax=ax[2,3])\nax[2,3].set_title(\"RUS_Featured Model Test FPR\")\n\nsns.barplot(x=\"bal_acc_train\", y=\"model\", data=df_eval_smote_pca.sort_values(by=\"recall\"), ax=ax[3,0])\nax[3,0].set_title(\"Smoted_PCA Model Train Acc\")\nsns.barplot(x=\"bal_acc_test\", y=\"model\", data=df_eval_smote_pca.sort_values(by=\"recall\"), ax=ax[3,1])\nax[3,1].set_title(\"Smoted_PCA Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_smote_pca.sort_values(by=\"recall\"), ax=ax[3,2])\nax[3,2].set_title(\"Smoted_PCA Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_smote_pca.sort_values(by=\"recall\"), ax=ax[3,3])\nax[3,3].set_title(\"Smoted_PCA Model Test FPR\")\n\nsns.barplot(x=\"bal_acc_train\", y=\"model\", data=df_eval_smote_tuned.sort_values(by=\"recall\"), ax=ax[4,0])\nax[4,0].set_title(\"Smoted_Tuned Model Train Acc\")\nsns.barplot(x=\"bal_acc_test\", y=\"model\", data=df_eval_smote_tuned.sort_values(by=\"recall\"), ax=ax[4,1])\nax[4,1].set_title(\"Smoted_Tuned Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_smote_tuned.sort_values(by=\"recall\"), ax=ax[4,2])\nax[4,2].set_title(\"Smoted_Tuned Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_smote_tuned.sort_values(by=\"recall\"), ax=ax[4,3])\nax[4,3].set_title(\"Smoted_Tuned Model Test FPR\")\n\nplt.tight_layout()\nplt.show()","3dfb4131":"df_eval_smote_tuned[\"type\"] = \"Smote_Tuned\"","c59bb13f":"frames = [df_eval, df_eval_smote, df_eval_rus, df_eval_smote_pca, df_eval_smote_tuned]\ndf_result = pd.concat(frames, ignore_index=True)\ndf_result['model'] = df_result['model'].str.upper()\ndf_result[[\"recall\", \"fallout\", \"bal_acc_train\", \"bal_acc_test\",'AUC']] = df_result[[\"recall\", \"fallout\",  \"bal_acc_train\", \"bal_acc_test\",'AUC']].apply(lambda x: np.round(x, 2))\ndf_result","a0bedf34":"sns.relplot(x=\"recall\", y=\"AUC\", hue=\"model\", size=\"bal_acc_test\", \n            sizes=(40, 400), col=\"type\", alpha=1, palette=\"bright\", height=5, legend='full', data=df_result)","c88b6fda":"#### Use Algorithm","2d676e0c":"### Evaluation (iteration 1)","78298511":"#### GridSearch","c5cb7748":"### 7-Naive Bayes","6cf07efe":"### 2-Logistic Regression","b5395f1a":"## Iteration 3: (with RUS and Feature Selection)","31af434a":"For this binary classification problem, we are using Logistic Regression, Decision Tree Classifier, Random Forest Classifier, and Gradient Boosting Classifier, Gaussian Naive Bayes. To assess the accuracy, we look at the accuracy scores and classification reports.","97dba7d2":"#### Use Algorithm","4ce845d6":"## Iteration 4: (with SMOTE and PCA)","53064ab6":"![resampling-techniques-in-machine-learning-15-638.jpg](attachment:resampling-techniques-in-machine-learning-15-638.jpg)","695b92d1":"According to Smote and Feature Selection with Pearson Correlation, nb gave the better result for recall and fallout.","06e479b1":"GradientBoosting yielded the optimized result as better FPR and relative mean strong recall scores. The balance accuracy is also relatively good.","11a5d038":"### 8-kNN","d9223e45":"# General Overview - Machine Learning","f4bf847b":"## Iteration 5: (with SMOTE and hyperparameter optimization)","be263c24":"### Use algorithms","3c7ba568":"The loads (loading scores) indicate \"how high a variable X loads on a factor Y\". \n\n(The i-th principal components can be selected via i in pca.components_ [0].)","1475bd81":"### Balancing data","aa2e2ce9":"we know that the principal components explain a part of the variance. From the Scikit-learn implementation, we can get the information about the explained variance and plot the cumulative variance. The PCA algorithm is going to standardize the input data frame, calculate the covariance matrix of the features. Thanks to [Mikulski](https:\/\/www.mikulskibartosz.name\/pca-how-to-choose-the-number-of-components\/)","c57ce3e1":"- We got cleaned dataset in the [first notebook](https:\/\/www.kaggle.com\/kirshoff\/01-exploratory-data-analysis-with-diabetes-dataset)\n- We got dummied cleaned dataset in the [third notebook](https:\/\/www.kaggle.com\/kirshoff\/03-statistical-analysis-with-diabetes-dataset)\n- We use diabetic_data_cleaned_dummy.csv here.","af535182":"#### Use Algorithm","dd0bd077":"### 6-Gradient Boosting","16d80302":"### Iteration 2: (Oversampling with SMOTE)","d0e7d39c":"### 5-Random Forest","bcd7da8e":"### Data Scaling","9ae7ee34":"> NaiveBayes gave high BalanceAccuracy and TPR_Score (Recall), but it gave the poor FPR_Score (Fallout) in this unbalanced data set.","cecee736":"### Lazy Predict with 5000 samples","cb54e7a0":"### Import and Load","7958e66b":"### 1-Decision tree","3d556021":"#  Summary: \n* In this project the diabetic_data.csv dataset was analyzed by machine learning methods with 5 iterations as a classification. For each iteration one tried little by little to achieve a better model result. \n* 9 different algorithms (DecisionTree, Logistic Regression, Support Vector Machine, Random Forest, Gradient Boost, NaiveBayes, Nearest Centroid, XGBOOST and kNearestNeigbour) were used. \n* After the data cleaning and EDA process, the data set was scaled with StandartScaler because there were many large and small features. After that, something special (oversampling, FeatureSelection, FeatureExtraction, HyperParameter optimization) was applied in each iteration. \n* In the end, GradientBoost with only oversampled and scaled data set gave better results.  The accuracy of all the models are similar and ranges between 62-64%. \n* Looking at the false positives and the recall value which is approx 60% in Random forest, it gives us better results than the rest. The above visual helps us in seeing the accuracy and the ROC curver further helps us decide the performance of different models.","0752c120":"### Iteration 1: (Unbalanced data)\n![CM-1024x382.png](attachment:CM-1024x382.png)","2f1f08e5":"Smote_PCA looks better than Smote. Hyperparameter is optimized by Smote_PCA.","ccd9e008":"### Split Data","21dc1332":"In this plot it looks like GradientBoosting in Smote has a better result","dc2c0119":"### 3-SVC","d34a2ca0":"### 9-XGBOOST","e2145829":"It looks like n_components = 40 is suitable for% 95 total explained variance,","baf5ea7d":"### 4-NearestCentroid","e9161561":"According to Smote and PCA, none of the models really gave relatively good results."}}