{"cell_type":{"c717ee0c":"code","81042545":"code","1cdfd8b6":"code","20080bc2":"code","9a21aa4a":"code","f9e319bd":"code","2cfb3e1a":"code","87b19d13":"code","b8ebfcb4":"code","4ced7608":"code","4184077b":"code","b22e0242":"code","3c97af13":"code","dc21864f":"code","26b1983e":"code","fc7f3367":"code","31cb555f":"code","7ced00e8":"code","2278a3b8":"code","41c2f5c6":"code","575992a8":"code","eb46a126":"code","e5fdb61e":"code","190fca5c":"code","69d52344":"code","967efb89":"code","c28e8950":"code","ca39c358":"code","c273e367":"code","ce18762e":"code","6a299f6b":"code","954e360a":"code","bd3bc356":"code","a6395f6c":"code","8d837c90":"code","23f60a8f":"code","c17660aa":"code","39832fea":"code","961edfac":"code","5702c0b3":"code","340b4bcf":"code","cb4a21be":"code","6f2d3945":"code","2c643810":"code","00412b84":"code","826d5d4e":"code","47a64c50":"code","973efc01":"code","eb307ff4":"code","48cbd31c":"code","f1796c28":"code","1d3d82d6":"code","d0010192":"code","b835a09b":"code","f0d26706":"code","04915147":"code","605bbf9a":"code","7a99f99a":"code","7788390b":"code","dc29b1b5":"code","bc764125":"code","8f004d2a":"code","1e66ed6d":"code","24190459":"code","3d23b9e9":"code","cc2c5314":"code","e422c902":"code","e5b2418e":"code","4026533b":"code","1b239186":"code","cf8e7d7d":"code","d3b8d6fb":"code","9e669c48":"code","2018935b":"code","d2870093":"code","94efa3c0":"code","120833e7":"code","8952ca83":"code","f6328fc2":"code","4e61ab29":"code","1fab0260":"code","800d8eda":"code","cc696cbf":"code","e9cbebba":"code","47c3104e":"code","4e768863":"code","3f80994a":"code","6ee7c499":"code","5cfda588":"code","9bdaf5b4":"code","066fb0ec":"code","ce56b3bd":"code","6718cdcf":"code","7e01886d":"code","3918d831":"code","1b3d72a4":"code","eb893f12":"code","e5ab6698":"code","8ded59fc":"code","df7989c1":"code","000b81ce":"code","200cf6b3":"code","c998ed46":"code","418b04ea":"code","eb7fc151":"code","0f917fb5":"code","13ce9874":"code","d4fb93e1":"code","e12cc15f":"code","6c8fb882":"code","6aaa2d47":"code","7b8563a6":"code","87daa1ad":"code","9799258b":"code","f0f3c5c1":"code","c117b1c1":"code","8d8e88b3":"code","f96fb606":"code","70319a39":"code","56febcf7":"code","96a2e9f4":"code","61e5ee38":"code","4d2fb7cf":"code","f4d4223d":"code","6ecd09bd":"code","a72f9a9f":"code","d996a778":"code","98fda28c":"code","a65e929a":"code","3730409d":"code","fb918d42":"code","6aac7191":"code","cd36bd3f":"code","9994b4ff":"code","8a1b8add":"code","4100b0fa":"code","cf6ae84a":"code","d9fea66b":"code","3da5810f":"code","95b6d21a":"code","447465da":"code","b500e98b":"code","a44ea728":"code","cb3afff7":"code","fdbb1d82":"code","9289dc2e":"code","427e71ad":"code","eda57e01":"code","c05329d0":"markdown","552f7870":"markdown","cbed8101":"markdown","5114e9df":"markdown","80079d85":"markdown","af24b0d7":"markdown","c349efc4":"markdown","f5def51c":"markdown","b6a7f422":"markdown","c995d444":"markdown","d5d4f35a":"markdown","05e71009":"markdown","8d9add8e":"markdown","ed28dea2":"markdown","c36473fa":"markdown","6e3e3310":"markdown","7d54ab0e":"markdown","22f7de6f":"markdown","15adbe55":"markdown","151fee1d":"markdown","c7b497a6":"markdown","d03cdf1b":"markdown","360c7f62":"markdown","90e5d8a6":"markdown","a355199a":"markdown","0155aaf8":"markdown","2da167a6":"markdown","97462008":"markdown","cadfd5e3":"markdown","3964be60":"markdown","0204aa4f":"markdown","e82fac3b":"markdown","0cd8b9e2":"markdown","3e25dac8":"markdown","cb9f5a89":"markdown","2dbf661c":"markdown","296d97c7":"markdown","59fec8f8":"markdown","8d7bda18":"markdown","82d5a63f":"markdown","81535d25":"markdown","220df48e":"markdown","cee317ea":"markdown","cc2733f9":"markdown","fe511ab8":"markdown","0373f25d":"markdown","c35f5631":"markdown","32bd84af":"markdown","2fdafe0c":"markdown","c4ccb873":"markdown","e162439c":"markdown","59595131":"markdown","e58f8194":"markdown","bde7cee8":"markdown","de1b0a3b":"markdown","c9ab3bb4":"markdown","b3dab3c4":"markdown","fb9e4b80":"markdown","9faedcf0":"markdown","06e8e316":"markdown","2c39dd4a":"markdown","6c69d37a":"markdown","a8c46a98":"markdown","00174fdd":"markdown","2bcf98a7":"markdown","660ebf41":"markdown","574e854e":"markdown","a654c97c":"markdown","ad99f3d0":"markdown"},"source":{"c717ee0c":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","81042545":"df = pd.DataFrame({'bmi' : [4.5,5.8,6.5,7.0,5.0,6.0,8.6],\n                'glucose' :[190,220,280,320,200,250,380]})","1cdfd8b6":"df","20080bc2":"sns.scatterplot(df['bmi'],df['glucose'])\nplt.plot([4,9],[200,450],color = 'r',linestyle = 'dashed')\nplt.plot([4,9],[100,350],color = 'r',linestyle = 'dotted')","9a21aa4a":"n_bmi = len(df['bmi'])\nn_glucose = len(df['glucose'])","f9e319bd":"cov_bmi_glu = np.sum((df['bmi'] - np.mean(df['bmi']))*(df['glucose'] - np.mean(df['glucose'])))\/(n_bmi-1)","2cfb3e1a":"var_bmi = np.sum((df['bmi'] - np.mean(df['bmi']))**2)\/(n_bmi-1)","87b19d13":"beta1 = cov_bmi_glu\/var_bmi","b8ebfcb4":"beta1","4ced7608":"ybar = np.mean(df['glucose'])\nXbar = np.mean(df['bmi'])","4184077b":"beta0 = ybar - beta1 * Xbar","b22e0242":"beta0","3c97af13":"glu_predict = beta0 + beta1 * df['bmi']","dc21864f":"glu_predict","26b1983e":"sns.scatterplot(df['bmi'],df['glucose'])\nsns.lineplot(df['bmi'],glu_predict,color = 'g')","fc7f3367":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()","31cb555f":"lr.fit(df[['bmi']],df['glucose'])","7ced00e8":"lr.coef_","2278a3b8":"lr.intercept_","41c2f5c6":"lr.predict(df[['bmi']])","575992a8":"df['outcome'] = [0,0,1,1,0,0,1]","eb46a126":"df","e5fdb61e":"n_outcome = len(df['outcome'])","190fca5c":"cov_bmi_outcome = np.sum((df['bmi'] - np.mean(df['bmi']))*(df['outcome'] - np.mean(df['outcome'])))\/(n_bmi-1)","69d52344":"beta1 = cov_bmi_outcome\/var_bmi","967efb89":"beta1","c28e8950":"ybar = np.mean(df['outcome'])\nXbar = np.mean(df['bmi'])","ca39c358":"beta0 = ybar - beta1 * Xbar","c273e367":"beta0","ce18762e":"outcome_predict = beta0 + beta1 * df['bmi']","6a299f6b":"outcome_predict","954e360a":"outcome_pred_prob = 1\/(1+ np.exp(-outcome_predict))\noutcome_pred_prob","bd3bc356":"outcome_pred = []\nfor val in outcome_pred_prob:\n    if val > np.mean(outcome_pred_prob):\n        outcome_pred.append(1)\n    else:\n        outcome_pred.append(0)\noutcome_pred","a6395f6c":"df = pd.read_csv('..\/input\/heart-patients\/US_Heart_Patients.csv')","8d837c90":"df.shape","23f60a8f":"import statsmodels.api as sm","c17660aa":"df.head()","39832fea":"df.info()","961edfac":"df.isnull().sum()","5702c0b3":"df2 = df.dropna()","340b4bcf":"df.shape[0] - df2.shape[0]","cb4a21be":"df2.rename(columns={'male':'Gender'},inplace=True)","6f2d3945":"plt.figure(figsize=(13,7))\nsns.heatmap(df2.corr(),annot = True,cbar=False)","2c643810":"X = df2.drop('TenYearCHD',axis=1)","00412b84":"X.shape","826d5d4e":"y = df2['TenYearCHD']","47a64c50":"Xc = sm.add_constant(X)","973efc01":"model = sm.Logit(y,Xc).fit()\nmodel.summary()","eb307ff4":"def backward_elimination(df,target):\n  X = df.drop(target,axis =1)\n  Xc = sm.add_constant(X)\n  remaining_cols = list(Xc.columns)\n  y = df[target]\n  while len(remaining_cols) > 0 : \n    X_temp = Xc[remaining_cols]\n    model = sm.Logit(y,X_temp).fit()\n    pvals = list(model.pvalues)\n    max_pval = max(pvals)\n    max_pval_feature = model.pvalues.idxmax()\n    if max_pval > 0.03:\n      remaining_cols.remove(max_pval_feature)\n      print(max_pval_feature,max_pval)\n    else:\n      break\n  return remaining_cols","48cbd31c":"impt_features = backward_elimination(df2,'TenYearCHD')","f1796c28":"impt_features","1d3d82d6":"model = sm.Logit(y,Xc[impt_features]).fit()\nmodel.summary()","d0010192":"impt_features.remove('const')","b835a09b":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X[impt_features],y,test_size = 0.3,random_state = 0)","f0d26706":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)\naccuracy = metrics.accuracy_score(y_test,y_pred)\nprint(accuracy * 100)\ncm = metrics.confusion_matrix(y_test,y_pred)\nprint(cm)","04915147":"print('TPR\/Sensitivity\/Recall : ',17\/182)\nprint('Using metrics : ',metrics.recall_score(y_test,y_pred))","605bbf9a":"print('TNR\/Specificity : ',907\/916)","7a99f99a":"model.coef_","7788390b":"np.exp(model.coef_)","dc29b1b5":"backpain_df = pd.read_csv('..\/input\/backpain\/backpain - backpain.csv')","bc764125":"backpain_df.head()","8f004d2a":"backpain_df['Status'].value_counts()","1e66ed6d":"backpain_df['Status'].replace({'Abnormal':1,'Normal':0},inplace = True)","24190459":"plt.figure(figsize=(13,7))\nsns.heatmap(backpain_df.corr(),annot = True,cbar=False)","3d23b9e9":"X = backpain_df.drop('Status',axis = 1)\ny = backpain_df['Status']\nXc = sm.add_constant(X)\nmodel = sm.Logit(y,Xc).fit()\nmodel.summary()","cc2c5314":"backward_elimination(backpain_df,'Status')","e422c902":"X_final = Xc[['pelvic_incidence',\n 'sacral_slope',\n 'pelvic_radius',\n 'degree_spondylolisthesis']]\nY = y","e5b2418e":"X_train, X_test, y_train, y_test = train_test_split(X_final,Y,test_size = 0.3,random_state = 0)","4026533b":"log_reg = LogisticRegression()\nlog_reg.fit(X_train,y_train)\ny_pred = log_reg.predict(X_test)","1b239186":"log_reg.coef_","cf8e7d7d":"log_reg.intercept_","d3b8d6fb":"accuracy = metrics.accuracy_score(y_test,y_pred)\nprint(accuracy * 100)\ncm = metrics.confusion_matrix(y_test,y_pred)\nprint(cm)","9e669c48":"print('TPR\/Sensitivity\/Recall : ',52\/63)\nprint('Using metrics : ',metrics.recall_score(y_test,y_pred))","2018935b":"print('TNR\/Specificity : ',25\/30)","d2870093":"np.exp(log_reg.coef_)","94efa3c0":"['pelvic_incidence',\n 'sacral_slope',\n 'pelvic_radius',\n 'degree_spondylolisthesis']","120833e7":"df_dt = pd.DataFrame({'ID' : ['s1','s2','s3','s4','s5','s6','s7'],\n                      'Salary ($1000)': [15,17,28,10,40,50,55],\n                      'Age (yrs)': [28,25,30,35,38,32,40],\n                      'Loan Defaulter' : [1,0,1,1,0,1,0]})","8952ca83":"df_dt","f6328fc2":"sns.scatterplot(df_dt['Age (yrs)'], df_dt['Salary ($1000)'], hue = df_dt['Loan Defaulter'])\nplt.vlines(36.5,ymin = 0 , ymax = 60,label = 'Depth1',color = 'red')\nplt.legend()","4e61ab29":"plt.figure(figsize = (7,5))\nsns.scatterplot(df_dt['Age (yrs)'], df_dt['Salary ($1000)'], hue = df_dt['Loan Defaulter'])\nplt.vlines(36.5,ymin = 0 , ymax = 60,label = 'Depth1',color = 'red')\nplt.vlines(26.5,ymin = 0 , ymax = 60,label = 'Depth2',color = 'green')\nplt.legend()","1fab0260":"df_dtr = pd.DataFrame({'ID' : ['s1','s2','s3','s4','s5','s6','s7','s8','s9','s10'],\n                       'Ht' : [152,180,167,175,142,172,160,172,165,167],\n                       'Age' : [45,26,30,34,40,36,19,28,23,32],\n                       'Wt' : [77,47,55,59,72,60,40,60,45,58]})","800d8eda":"df_dtr","cc696cbf":"df_bc = pd.read_csv('..\/input\/breastcancer-dataset\/data.csv')","e9cbebba":"df_bc.drop('Unnamed: 32',axis=1,inplace=True)","47c3104e":"df_bc.shape","4e768863":"df_bc.head()","3f80994a":"df_bc['diagnosis'].value_counts()","6ee7c499":"df_bc.replace({'M':1,'B':0},inplace=True)","5cfda588":"df_bc.drop('id',axis = 1,inplace=True)","9bdaf5b4":"df_bc.head()","066fb0ec":"df_bc.isnull().sum()","ce56b3bd":"plt.figure(figsize=(17,10))\nsns.heatmap(df_bc.corr(),annot = True,cbar=False)","6718cdcf":"X = df_bc.drop('diagnosis',axis =1)\ny = df_bc['diagnosis']","7e01886d":"import warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold,cross_val_score\nLR = LogisticRegression()\nDT = DecisionTreeClassifier(criterion = 'entropy',random_state = 0)\nDT_reg = DecisionTreeClassifier(max_depth = 6,criterion = 'entropy',random_state = 0)\nRF = RandomForestClassifier(n_estimators=10,random_state = 0)","3918d831":"models = []\nmodels.append(('Logistic regression',LR))\nmodels.append(('FullyGrownDecisionTree',DT))\nmodels.append(('RegularizedDecisionTree',DT_reg))\nmodels.append(('RandomForest',RF))","1b3d72a4":"results = []\nnames = []\nfor name, model in models:\n  kfold = KFold(n_splits = 5,random_state = 0,shuffle = True)\n  cv_results = cross_val_score(model,X,y,cv = kfold,scoring='roc_auc')\n  results.append(cv_results)\n  names.append(name)\n  print(name,' : ',np.mean(cv_results),' -- ',np.var(cv_results,ddof = 1))\n\nfig = plt.figure(figsize=(10,6))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","eb893f12":"results","e5ab6698":"DT.fit(X,y)","8ded59fc":"from IPython.display import Image\nfrom os import system\nfrom sklearn import tree","df7989c1":"BC_Tree_File = open('bc_data.dot','w') # .dot --> Document Template File\ndot_data = tree.export_graphviz(DT,out_file=BC_Tree_File,feature_names=list(X),class_names=['Benign','Malignant'])\nBC_Tree_File.close()","000b81ce":"system('dot -Tpng bc_data.dot -o BC_Tree.png')\nImage('BC_Tree.png')","200cf6b3":"pd.DataFrame(DT.feature_importances_,columns=['Normalized Feature Importance'],index = X.columns).sort_values(by = 'Normalized Feature Importance',ascending = False)","c998ed46":"  df_knn = pd.DataFrame({'ID' : ['s1','s2','s3','s4'],\n                       'Age' : [25,30,50,45],\n                       'Wt' : [40,42,60,62],\n                       'Class' : ['Young','Young','Middle','Middle']})","418b04ea":"df_knn","eb7fc151":"df_knn['age_scaled'] = (df_knn['Age'] - np.mean(df_knn['Age']))\/np.std(df_knn['Age'],ddof=1)\ndf_knn['wt_scaled'] = (df_knn['Wt'] - np.mean(df_knn['Wt']))\/np.std(df_knn['Wt'],ddof=1)\ndf_knn","0f917fb5":"backpain_df.columns","13ce9874":"data = backpain_df.iloc[:,[0,1,2,3,4,5,12]]","d4fb93e1":"data.head()","e12cc15f":"from scipy.stats import ttest_ind\ndf0  = data[data['Status'] == 0]\ndf1  = data[data['Status'] == 1]","6c8fb882":"for col in data.columns[:-1]:\n  print(col,\" : \",ttest_ind(df0[col],df1[col]))","6aaa2d47":"X = data.drop('Status',axis = 1)\ny = data['Status']","7b8563a6":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nX = ss.fit_transform(X)","87daa1ad":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nknn = KNeighborsClassifier()\ndt = DecisionTreeClassifier(random_state=0)\nrf = RandomForestClassifier(random_state=0)\nparams = {\n    'n_neighbors' : np.arange(1,50),\n    'weights' : ['uniform','distance']\n}\nparam_dt = {\n    'criterion' : ['entropy','gini']\n}\nparam_rf = {\n    'n_estimators' : np.arange(1,50)\n}\ngscv = GridSearchCV(knn,params,scoring='roc_auc',cv = 3)\ngscv_dt = GridSearchCV(dt,param_dt,scoring='roc_auc',cv = 3)\ngscv_rf = GridSearchCV(rf,param_rf,scoring='roc_auc',cv = 3)\ngscv.fit(X,y)\ngscv_dt.fit(X,y)\ngscv_rf.fit(X,y)","9799258b":"print(gscv.best_params_)\nprint(gscv_dt.best_params_)\nprint(gscv_rf.best_params_)","f0f3c5c1":"knn_final = KNeighborsClassifier(**gscv.best_params_)\nLR = LogisticRegression()\ndt_final = DecisionTreeClassifier(criterion='gini',random_state=0)\nrf_final = RandomForestClassifier(n_estimators=35,random_state=0)\nmodels = []\nmodels.append(('KNNClassifier',knn_final))\nmodels.append(('Logistic Regression',LR))\nmodels.append(('DecisionTree',dt_final))\nmodels.append(('Random Forest',rf_final))","c117b1c1":"results = []\nnames = []\nfor name, model in models:\n  kfold = KFold(n_splits = 3,random_state = 0,shuffle = True)\n  cv_results = cross_val_score(model,X,y,cv = kfold,scoring='roc_auc')\n  results.append(cv_results)\n  names.append(name)\n  print(name,' : ',np.mean(cv_results),' -- ',np.var(cv_results,ddof = 1))\n\nfig = plt.figure(figsize=(10,6))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","8d8e88b3":"df_iris = pd.read_csv('..\/input\/matplotlib-datasets\/iris_dataset.csv')","f96fb606":"iris = df_iris.drop(['sepal_length','sepal_width'],axis = 1)","70319a39":"df_iris.head()","56febcf7":"sns.pairplot(iris,hue='species')","96a2e9f4":"def gaussianProbabilityDistribution(x,mean,std):\n  exponent = np.exp(-((x-mean)**2\/(2*std**2)))\n  return exponent \/ (np.sqrt(2 * np.pi) * std)","61e5ee38":"iris['species']","4d2fb7cf":"ss = StandardScaler()\nX = pd.DataFrame(ss.fit_transform(iris.iloc[:,:-1]),columns=['x1','x2'])\nX['labels'] = iris['species']","f4d4223d":"iris_setosa = X[X['labels'] == 'setosa']\niris_versicolor = X[X['labels'] == 'versicolor']\niris_virginica = X[X['labels'] == 'virginica']","6ecd09bd":"# Test Samples\ntest1 = [0.53529,0.5276]\ntest2 = [-1.398,-1.3129]\ntest3 = [1.047,1.185]","a72f9a9f":"prob_setosa = gaussianProbabilityDistribution(test1[0],iris_setosa['x1'].mean(),np.std(iris_setosa['x1'],ddof = 1)) * gaussianProbabilityDistribution(test1[1],iris_setosa['x2'].mean(),np.std(iris_setosa['x2'],ddof = 1)) * (1\/3)\nprob_versicolor = gaussianProbabilityDistribution(test1[0],iris_versicolor['x1'].mean(),np.std(iris_versicolor['x1'],ddof = 1)) * gaussianProbabilityDistribution(test1[1],iris_versicolor['x2'].mean(),np.std(iris_versicolor['x2'],ddof = 1)) * (1\/3)\nprob_virginica = gaussianProbabilityDistribution(test1[0],iris_virginica['x1'].mean(),np.std(iris_virginica['x1'],ddof = 1)) * gaussianProbabilityDistribution(test1[1],iris_virginica['x2'].mean(),np.std(iris_virginica['x2'],ddof = 1)) * (1\/3)\nprint(prob_setosa,prob_versicolor,prob_virginica)\nprint(np.argmax([prob_setosa,prob_versicolor,prob_virginica]))","d996a778":"prob_setosa = gaussianProbabilityDistribution(test2[0],iris_setosa['x1'].mean(),np.std(iris_setosa['x1'],ddof = 1)) * gaussianProbabilityDistribution(test2[1],iris_setosa['x2'].mean(),np.std(iris_setosa['x2'],ddof = 1)) * (1\/3)\nprob_versicolor = gaussianProbabilityDistribution(test2[0],iris_versicolor['x1'].mean(),np.std(iris_versicolor['x1'],ddof = 1)) * gaussianProbabilityDistribution(test2[1],iris_versicolor['x2'].mean(),np.std(iris_versicolor['x2'],ddof = 1)) * (1\/3)\nprob_virginica = gaussianProbabilityDistribution(test2[0],iris_virginica['x1'].mean(),np.std(iris_virginica['x1'],ddof = 1)) * gaussianProbabilityDistribution(test2[1],iris_virginica['x2'].mean(),np.std(iris_virginica['x2'],ddof = 1)) * (1\/3)\nprint(prob_setosa,prob_versicolor,prob_virginica)\nprint(np.argmax([prob_setosa,prob_versicolor,prob_virginica]))","98fda28c":"prob_setosa = gaussianProbabilityDistribution(test3[0],iris_setosa['x1'].mean(),np.std(iris_setosa['x1'],ddof = 1)) * gaussianProbabilityDistribution(test3[1],iris_setosa['x2'].mean(),np.std(iris_setosa['x2'],ddof = 1)) * (1\/3)\nprob_versicolor = gaussianProbabilityDistribution(test3[0],iris_versicolor['x1'].mean(),np.std(iris_versicolor['x1'],ddof = 1)) * gaussianProbabilityDistribution(test3[1],iris_versicolor['x2'].mean(),np.std(iris_versicolor['x2'],ddof = 1)) * (1\/3)\nprob_virginica = gaussianProbabilityDistribution(test3[0],iris_virginica['x1'].mean(),np.std(iris_virginica['x1'],ddof = 1)) * gaussianProbabilityDistribution(test3[1],iris_virginica['x2'].mean(),np.std(iris_virginica['x2'],ddof = 1)) * (1\/3)\nprint(prob_setosa,prob_versicolor,prob_virginica)\nprint(np.argmax([prob_setosa,prob_versicolor,prob_virginica]))","a65e929a":"  df_gb = pd.DataFrame({'ID' : ['s1','s2','s3','s4','s5','s6','s7','s8','s9'],\n                'age' : [13,14,15,25,35,49,68,71,73],\n                'likesGardening' :[0,0,0,1,0,1,1,1,1],\n                'playsVideoGames' :[1,1,1,1,1,0,1,0,0],\n                'likesHats' :[1,0,0,1,1,0,1,0,1]})","3730409d":"df_gb","fb918d42":"df_gb['y_pred_1_est'] = [19.25,19.25,19.25,57.2,19.25,57.2,57.2,57.2,57.2]\ndf_gb['residue_1st_est'] = df_gb['age'] - df_gb['y_pred_1_est']\ndf_gb","6aac7191":"df_gb['y_pred_2_est'] = [-3.58,-3.58,-3.58,-3.58,-3.58,7.12,-3.58,7.12,7.12]\ndf_gb['ensemble_pred_1_2'] = df_gb['y_pred_1_est'] + df_gb['y_pred_2_est']\ndf_gb['residue_2nd_est'] = df_gb['age'] - df_gb['ensemble_pred_1_2']\ndf_gb","cd36bd3f":"df2","9994b4ff":"# backward_elimination(df2,'TenYearCHD')","8a1b8add":"# X_parametric_model = df2[['Gender', 'age', 'cigsPerDay', 'sysBP', 'glucose']]\nX_all = df2.drop('TenYearCHD',axis = 1)\ny = df2['TenYearCHD']","4100b0fa":"ss = StandardScaler()\n# X_parametric_model_scaled = ss.fit_transform(X_parametric_model)\nX_all_scaled = ss.fit_transform(X_all)","cf6ae84a":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nknn = KNeighborsClassifier()\nparams = {\n    'n_neighbors' : np.arange(1,100),\n    'weights' : ['uniform','distance']\n}\ngscv = GridSearchCV(knn,params,scoring='roc_auc',cv = 15)\ngscv.fit(X_all_scaled,y)\nprint(gscv.best_params_)","d9fea66b":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.naive_bayes import GaussianNB\nLR = LogisticRegression()\nNB = GaussianNB()\nknn = KNeighborsClassifier(**gscv.best_params_)\nRF = RandomForestClassifier(n_estimators = 10,random_state = 0)","3da5810f":"dt = DecisionTreeClassifier(random_state=0)\nparams = {\n    'criterion' : ['entropy','gini']\n}\ngscv = GridSearchCV(dt,params,scoring='roc_auc',cv = 15)\ngscv.fit(X_all_scaled,y)\nprint(gscv.best_params_)","95b6d21a":"dt = DecisionTreeClassifier(criterion='gini',random_state=0)","447465da":"#Tuning n_estimators for Bagging\nfrom sklearn.ensemble import BaggingClassifier\nmodels = []\nmodels.append(('Logistic Regression',LR))\nmodels.append(('NaiveBayes',NB))\nmodels.append(('KNN',knn))\nmodels.append(('Decision Tree',dt))\nfor name,model in models:  \n  auc_var = []\n  for val in np.arange(1,11):\n    model_bag = BaggingClassifier(base_estimator = model,n_estimators = val,random_state = 0)\n    kfold = KFold(n_splits = 15,random_state = 0,shuffle = True)\n    results = cross_val_score(model_bag,X_all_scaled,y,cv = kfold,scoring='roc_auc')\n    auc_var.append(np.var(results,ddof = 1))\n  print(name,np.argmin(auc_var)+1)","b500e98b":"#Tuning n_estimators for Boosting\nfrom sklearn.ensemble import AdaBoostClassifier\nmodels = []\nmodels.append(('Logistic Regression',LR))\nmodels.append(('NaiveBayes',NB))\nmodels.append(('RF',RF))\nmodels.append(('Decision Tree',dt))\nfor name,model in models:  \n  auc_mean = []\n  for val in np.arange(1,51):\n    model_boost = AdaBoostClassifier(base_estimator = model,n_estimators = val,random_state = 0)\n    kfold = KFold(n_splits = 15,random_state = 0,shuffle = True)\n    results = cross_val_score(model_boost,X_all_scaled,y,cv = kfold,scoring='roc_auc')\n    auc_mean.append(np.mean(results))\n  print(name,np.argmax(auc_mean)+1)","a44ea728":"#Bagging Models\nknn_bag = BaggingClassifier(base_estimator = knn,n_estimators = 10,random_state = 0)\nLR_bag = BaggingClassifier(base_estimator = LR,n_estimators = 2,random_state = 0)\nNB_bag = BaggingClassifier(base_estimator = NB,n_estimators = 9,random_state = 0)\nDT_bag = BaggingClassifier(base_estimator = dt,n_estimators = 1,random_state = 0)\n#Boosting models\nrf_boost = AdaBoostClassifier(base_estimator = RF,n_estimators = 2,random_state = 0)\nLR_boost = AdaBoostClassifier(base_estimator = LR,n_estimators = 16,random_state = 0)\nNB_boost = AdaBoostClassifier(base_estimator = NB,n_estimators = 1,random_state = 0)\nDT_boost = AdaBoostClassifier(base_estimator = dt,n_estimators = 1,random_state = 0)","cb3afff7":"#Tuning n_estimators for Boosting\nfrom sklearn.ensemble import GradientBoostingClassifier\nauc_mean = []\nfor val in np.arange(1,51):\n  model_boost = GradientBoostingClassifier(n_estimators = val,random_state = 0)\n  kfold = KFold(n_splits = 15,random_state = 0,shuffle = True)\n  results = cross_val_score(model_boost,X_all_scaled,y,cv = kfold,scoring='roc_auc')\n  auc_mean.append(np.mean(results))\nprint(name,np.argmax(auc_mean)+1)","fdbb1d82":"GBC = GradientBoostingClassifier(n_estimators = 22,random_state = 0)","9289dc2e":"models = []\nmodels.append(('Logistic Regression',LR))\nmodels.append(('LR Bagged',LR_bag))\nmodels.append(('LR Boosted',LR_boost))\nmodels.append(('NaiveBayes',NB))\nmodels.append(('NB Bagged',NB_bag))\nmodels.append(('NB Boosted',NB_boost))\nmodels.append(('DTree',dt))\nmodels.append(('DTree Bagged',DT_bag))\nmodels.append(('DTree Boosted',DT_boost))\nmodels.append(('KNN',knn))\nmodels.append(('KNN Bagged',knn_bag))\nmodels.append(('RF',RF))\nmodels.append(('RF Boosted',rf_boost))\nmodels.append(('Gradient Boost',GBC))","427e71ad":"# results = []\n# names = []\n# for name, model in models:\n#   kfold = KFold(n_splits = 15,random_state = 0,shuffle = True)\n#   cv_results = cross_val_score(model,X_parametric_model_scaled,y,cv = kfold,scoring='roc_auc')\n#   results.append(cv_results)\n#   names.append(name)\n#   print(name,' : ',np.mean(cv_results),' -- ',np.var(cv_results,ddof = 1))\n\n# fig = plt.figure(figsize=(20,6))\n# fig.suptitle('Algorithm Comparison')\n# ax = fig.add_subplot(111)\n# plt.boxplot(results)\n# ax.set_xticklabels(names)\n# plt.show()","eda57e01":"results = []\nnames = []\nfor name, model in models:\n  kfold = KFold(n_splits = 15,random_state = 0,shuffle = True)\n  cv_results = cross_val_score(model,X_all_scaled,y,cv = kfold,scoring='roc_auc')\n  results.append(cv_results)\n  names.append(name)\n  print(name,' : ',np.mean(cv_results),' -- ',np.var(cv_results,ddof = 1))\n\nfig = plt.figure(figsize=(20,6))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","c05329d0":"# ***Random Forest (Bagged Decision Trees)***\n\n![alt text](https:\/\/i.ibb.co\/3SzFxzc\/random-forest.png)\n\nRandom forest is the only model which has built-in ensemble characteristics. \n\n**Bootstarp Sampling** : Random sampling with replacement. Using bootstrap sampling we try to reduce the variance of the data.\n\n![alt text](https:\/\/i.stack.imgur.com\/5Nzqf.png)\n\nIn random forest we use multiple bootstrapped samples and create multiple decision trees(weak learners\/less complex models\/estimators) to build a strong model.\n\n**It finally chooses the class which has been classified by majority of the weak learners\/estimators.**\n\n**Random forest handles the bias error** by using a strategy to pick only sqrt of total number of features in each decision tree, but these features are picked randomly, hence all the features get an opprtunity to be picked in some or the other estimator. This way it utilizes all the features in different estimators so the bias error is reduced in such a way.\n\n## Two perspectives for Random forest :\n1. Random forest has less bias error as compared to fully grown decision tree.\n2. Random forest has higher bias error as compared to other bagging models(eg : bagging model of logistic regression will use all features for sampling but RF uses sqrt of total number of features.)","552f7870":"# Overfitting vs Undefitting vs Ideal Model\n\n![alt text](https:\/\/miro.medium.com\/max\/1000\/1*i5BamDkqO1dnGxBVaTHtHw.png)\n\nModel 1 : High training error and high testing error\n\nModel 2 : Low training error and high testing error\n\nModel 3 : Low training error and low testing error (Rightly fit model)","cbed8101":"We can see from the pairplot that setosa is clearly separable, but there is some amount of overlapping between versicolor and virginica for both the features.","5114e9df":"**Decision tree and Random forest are non parametric models whereas Linear and Logistic regression are parametric models.**\n\n**For parametric models, we need to make sure we follow all the assumptions.**","80079d85":"### Depth level 1 (Age > 35)\n\nnode1 -> { s2,s3,s4,s7,s8,s9,s10 }\n\nybar_node1 = 52\n\n**MSE at node 1 :** \n\nMSE_node1 = 53.71\n\nnode2 -> { s1,s5,s6 }\n\nybar_node2 = 69.6\n\n**MSE at node 2 :** \n\nMSE_node2 = 50.88\n\nSo total MSE = 53.71 + 50.88 = 104.5\n\nInformation gain = MSE(root node) - MSE(current node) = 118.4 - 104.5 = 13.9","af24b0d7":"## Cost function for Classification models\n\n**Log loss :** \n\nThe summation of log loss should be minimal.\n\nMeasure of uncertainity\/ Entropy calculation\n\nEntropy :\n\n![alt text](https:\/\/www.saedsayad.com\/images\/Entropy_3.png)\n\n**Cost function of Classification --> min( sum(log loss) )**\n\n---","c349efc4":"***Please UpVote if you like the work!!!***","f5def51c":"##### We will be working on differnt models for regression and classification, but we need to choose the best model for our clients. \n\n##### So for final performance validation, for regression we use RMSE.\n\n##### For binary classification we use AUC and for multiclass classification we use F1-SCORE","b6a7f422":"# Univariate Analysis","c995d444":"As we give more and more evidence of particular data, our prediction chances will improve.","d5d4f35a":"## Estimator 2:\nAt ROOT NODE we will take all the residues from the 1st estimator instead of actual y:\n\nRoot node is a null model.\n\nand residue_1st_est = [-6.25,-5.25,-4.25,-32.2,15.75,-8.2,10.8,13.8,15.8]\n\n#### Depth level 1 (playsVideoGames)\n\nnode1_false -> { s6,s8,s9 }\n\nnode2_true -> { s1,s2,s3,s4,s5,s7 }\n\nensemble_pred = y_pred_1_est + y_pred_2_est\n\n**MSE = sum((y - ensemble_pred)^2)\/no_of_records**\n\nSo MSE = sum((age - ensemble_pred)^2)\/9 = 196.06\n\nSo using estimator 1 and 2 our Gradient Boost model can minimize the training error from 221.5 to 196.06.","05e71009":"The test3 sample will be classified as VIRGINICA.","8d9add8e":"# Characteristics : \n1. Linear Regression, Logistic Regression, Naive Bias : High bias error but low variance error.\n2. KNN with high n_neighbors : High bias error but low variance error.\n3. KNN with low n_neighbors : Low bias error but high variance error.\n4. Fully Grown Decision Tree : High bias error and high variance error.\n5. Random Forest : Moderate bias error but low variance error.\n6. Other Bagged models : Low bias error and low variance error.\n---","ed28dea2":"## Why random state is used in decision tree model?\n#### If a leaf node has multiple classes with equal probabilities for each class, there is a chance that different models will select different classes at leaf node for a test record. So to keep this selection of class uniform accross all models, we use random state.","c36473fa":"# ***K Nearest Neighbor (KNN)***\n\n![alt text](https:\/\/d1jnx9ba8s6j9r.cloudfront.net\/blog\/wp-content\/uploads\/2019\/03\/How-does-KNN-Algorithm-work-KNN-Algorithm-In-R-Edureka-528x250.png)\n\n**Laziest Algorithm** : Because it does not calculate any model parameters till the time a new test data comes.","6e3e3310":"# Reciever Operating Characteristics (ROC) curve\n\nA graph between TPR and FPR : \n\n![alt text](https:\/\/miro.medium.com\/max\/722\/1*pk05QGzoWhCgRiiFbz-oKQ.png)\n\n**For binary classification, ROC AUC curve is the best mertic.**\n\n**For multiclass classification, weighted F1 score is the best mertic.**","7d54ab0e":"glucose = beta0 + beta1 * bmi","22f7de6f":"***Please UpVote if you like the work!!!***","15adbe55":"## Bagging\n![alt text](https:\/\/i.ibb.co\/2Kg3SRk\/bagging.png)\n\nIts similar to Random Forest but the only difference is that the Bagging ensemble techinque can use any estimator and we can use all the features in Bagging as opposed to sqrt(n) features in Random forest in bootstrap samples.\n\nIn Bagging classifier as well as bagging regressor, **the learning(training) as well as prediction(testing) happens in parallel** as no estimators are dependent on each other.\n\n***Bagging has LOW VARIANCE ERROR because it uses multiple bootstrap samples as well as we can also use all the features in each estimator. But it will have a SLIGHTLY HIGH BIAS ERROR because in every estimator the training error will be large because all the bootstrap samples have equal mixture of simple samples and difficult samples***\n\nFor Bagging classifier, we use voting to get the final result from estimators whereas in Bagging regressor we get the mean of all the results from estimators.\n\nWe cannot bag a random forest model as it is already by default a bagged model.","151fee1d":"### **Euclidean distances :** \n\ned_s1_s2 = np.sqrt( (25-30)^2 + (40-42)^2 ) = 5.38\n\ned_s1_s3 = np.sqrt( (25-50)^2 + (40-60)^2 ) = 32.01\n\ned_s3_s4 = np.sqrt( (50-45)^2 + (60-62)^2 ) = 5.38\n\n### **Manhattan distances :** \n\nmd_s1_s2 = np.abs(25-30) + np.abs(40-42) = 7\n\nmd_s1_s3 = np.abs(25-50) + np.abs(40-60) = 45\n\nmd_s3_s4 = np.abs(50-45) + np.abs(60-62) = 7\n\n### **Cosine distances :** \n#### ***Cosine distance = 1 - Cosine_Similarity***\n![alt text](https:\/\/neo4j.com\/docs\/graph-algorithms\/current\/images\/cosine-similarity.png)\n\ncd_s1_s2 = 1 - [ ( (25x30)+(40x42) ) \/ ( np.sqrt(25^2 + 40^2) x np.sqrt(30^2 + 42^2) ) ] = 1 - 0.998 = 0.0018\n\ncd_s1_s3 = 1 - [ ( (25x50)+(40x60) ) \/ ( np.sqrt(25^2 + 40^2) x np.sqrt(50^2 + 60^2) ) ] = 1 - 0.990 = 0.0092\n\ncd_s3_s4 = 1 - [ ( (50x45)+(60x62) ) \/ ( np.sqrt(50^2 + 60^2) x np.sqrt(45^2 + 62^2) ) ] = 1 - 0.997 = 0.0022","c7b497a6":"# Adding a categorical value for y (dependent variable)","d03cdf1b":"y -> actual points \n\ny' -> predicted points on the line","360c7f62":"## **NOTE:**\nA general thumbrule for scaling - \n\n1. Use \"StandardScalar\" when majority of our data is continous.\n2. Use \"MinMaxScalar\" when majority of our data is categorical or there are a 50% continous and 50% categorical variables.\n3. \"MaxNormalization\" is generally used to scale pixel of image data (Convert from 0-255 grayscale to 0-1 scale)","90e5d8a6":"This 22 will give the lowest bias error.","a355199a":"### Gini Index for Decision Tree\n\n**Gini Index:**\n\n![alt text](https:\/\/miro.medium.com\/max\/415\/0*asbVp_8lwEsbfpOv.png)           ---> pj : Probability of ith class.\n","0155aaf8":"#### Sensitivity\/Recall : The fraction of ***total amount of relevent instances*** that were ***actually retrieved***.\n\nRecall_Dog = 25\/25 = 1\n\nRecall_Cat = 16\/20 = 0.8\n\nRecall_Rabbit = 12\/17 = 0.70588\n\n#### Precision : The fraction of ***relevent instances*** among ***retrieved instances***.\n\nPrecision_Dog = 25\/25 = 1\n\nPrecision_Cat = 16\/21 = 0.7619\n\nPrecision_Rabbit = 12\/16 = 0.75\n\n#### F1 score : ***Harmonic mean*** of precision and recall (2ab\/a+b)\n\nF1_Dog = 1\n\nF1_Cat = 0.78\n\nF1_Rabbit = 0.72\n\n---","2da167a6":"### NOTE : \nLinear\/parametric models works very well if we go through the statistics strictly and we should use the selected features. For non parametric models, we need not go through the feature selection and statistics.","97462008":"The test1 sample will be classified as VERSICOLOR.","cadfd5e3":"# How change in value of k affects the model?\n\n![alt text](https:\/\/i.ibb.co\/D5yyTGM\/effect-of-varing-k.png)\n\nLower the value of k\/n_neighbor, the model will overfit.\n\nHigher the value of k\/n_neighbor, the model will underfit.\n\nA weighted model will be equivalent to an unweighted overfit model as it is predicted to class similar to the class of the closest point in training set.\n\nSo its better always a good practice to build a base model with 'distance' metric so that we get a model with high variance error and low bias error. We can then further deal with the high variance error by using bagging.","3964be60":"# Breast Cancer Dataset","0204aa4f":"Every 1 unit increase in the pelvic incidence contributes to 5.58% increase in probablitiy of Abnormility.\n\nEvery 1 unit increase in the sacral slope contributes to 16.16% decrease in probablitiy of Abnormality.\n\nEvery 1 unit increase in the degree_spondylolisthesis contributes to 21% increase in probablitiy of Abnormility.\n\nEvery 1 unit increase in the pelvic radius contributes to 11.7% decrease in probablitiy of Abnormality.","e82fac3b":"# Bagging and Boosting","0cd8b9e2":"## Estimator 1:\nAt ROOT NODE :\n\nRoot node is a null model.\n\nSo age_pred = ybar = [40.33,40.33,40.33,40.33,40.33,40.33,40.33,40.33,40.33]\n\nand age = [13,14,15,25,35,49,68,71,73]\n\nMSE = 577.11\n\n#### Depth level 1 (likesGardening)\n\nnode1_false -> { s1,s2,s3,s5 }\n\nybar_node1 = 19.25\n\nnode2_true -> { s4,s6,s7,s8,s9 }\n\nybar_node2 = 57.2\n\ny_pred_1_est = [19.25,19.25,19.25,57.2,19.25,57.2,57.2,57.2,57.2]\n\n**MSE = sum((y - y_pred_1_est)^2)\/no_of_records**\n\nMSE = 221.5\n\nSo using 1 estimator our Gradient Boost model can minimize the training error from 577.11 to 221.5.","3e25dac8":"Decision tree algorithms have different metric to find the decision splits. For example, CHAID uses Chi-Square test value, ID3 and C4.5 uses entropy, CART uses GINI Index.\n\n### Feature Importance\n\nIn the below formula, the metric would correspond to which algorithm we are using. For eg, if the algo is CART, feature metric used would be GINI Index.\n\nFeature importance (FI) = Feature metric * number of instances \u2013 its left child node metric * number of instances for the left child \u2013 its right child node metric * number of instances for the right child\n\nNote that a feature can appear several times in a decision tree as a decision point. Sum of those individual decision points will be the feature importance of that feature.\n\nWe can normalize the feature importances if we divide them all with their sum.","cb9f5a89":"### Depth level 1\n\nnode1 -> { s1,s2,s3,s4,s6 } --> s2 misclassified {Still an impure node} \n\n**Entropy at node 1 :** \n\np0 : 1\/5 and p1 : 4\/5\n\nEntropy = - [ ( (4\/5) * log(4\/5) ) + ( (1\/5) * log(1\/5) ) ] = 0.7219\n\nAssume we had got some entropy at node 2 as 0.315,\n\nSo summation of both entropies would be 1.0369 which is not expected. Our summation of the entropies of the nodes should be less than its root entropy. So we use **WEIGHTED ENTROPIES**.\n\n*** Weighted Entropt of a node : entropy * (no. of points in current node\/ no. of points in the root node) ***\n\nWeighted Entropy for node 1 : 0.7219 * 5\/7 = 0.5156\n\nnode2 -> { s5,s7 } --> no misclassification - {PURE Leaf} - Entropy\/log loss is zero\n\nSo total entropy = 0.5156 + 0\n\nInformation gain = entropy(root node) - entropy(current node) = 0.9857 - 0.5156 = 0.4696","2dbf661c":"### Confusion Matrix :\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;***Predicted***\n\n***Actual*** [ TN &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; FP\/alpha\/type1 error\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FN\/beta\/type2 error &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TP]\n\n","296d97c7":"## Boosting\n### ***Gradient Boosting (Brahmastra)***\n#### Gradient Boost Regressor\n\nIn Gradient Boost, the default estimator is Decision Tree with a depth of 1, which is called as ***Decision Stump***. There is no other estimator hich we can use in Gradient boost algorithm.\n\nIn Gradient Boosting, **the learning(training) as well as prediction(testing) happens SERIALLY**.\n\nIf we grow one decision tree to the highest depth, that decision tree will suffer with high bias and high variance error. Instead in Gradient Boost, we grow horizontally rather that growing deeper. So whatever the error 1st dt will make, the 2nd dt will minimize that error and so on...\n\nNOTE :\n1. In GB, there is no Bootstrap Sampling\n2. In GB, the prediction is serial\n","59fec8f8":"*   Alpha\/ Type1 Error\/ False Positive rate = FP\/FP+TN\n*   Beta\/ Type2 Error\/ False Negative Rate = FN\/FN+TP\n*   1- Alpha\/ True Negative Rate\/ Specificity = TN\/TN+FP\n*   1- Beta\/ True Positive Rate\/ Sensitivity\/Recall = TP\/TP+FN","8d7bda18":"### Depth level 2 (Height > 170)\n\nnode11 -> { s3,s7,s9,s10 }\n\nybar_node11 = 49.25\n\n**MSE at node11 :** \n\nMSE_node11 = 53.25\n\nnode12 -> { s2,s4,s8 }\n\nybar_node12 = 55.3\n\n**MSE at node12 :** \n\nMSE_node12 = 34.89\n\nnode21 -> { s1,s5 }\n\nybar_node21 = 74.5\n\n**MSE at node21 :** \n\nMSE_node21 = 6.25\n\nnode22 -> { s6 }\n\nybar_node22 = 60\n\n**MSE at node22 :** \n\nMSE_node22 = 0\n\nSo total MSE = 53.25 + 34.89 + 6.25 + 0 = 94.38\n\nInformation gain = MSE(root node) - MSE(current node) = 104.59 - 94.38 = 10.12","82d5a63f":"## ***Squashing \/ Sigmoid \/ Logistic function***\n\n#### y = 1 \/ ( 1 + e^-(beta0 + beta1 * X) )\n\n![alt text](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/8\/88\/Logistic-curve.svg)","81535d25":"For dashed line, all residuals will be negative.\n\nFor dotted line, all residuals will be positive.\n\nbeta1 = covariance of X and y divided by variance of X\n\nbeta1 = cov(X,y)\/var(X)\n\ncov(X,y) = sum_1_to_n((X-Xbar)(y-ybar))\/n-1\n\nvar(X) = sum_1_to_n((X-Xbar)**2)\/n-1\n\nbeta0 = ybar - beta1 * Xbar","220df48e":"#### Best way to select number of splits in kfold cross validation\nSelect that n_splits value for which there is not much change in the variance error even if we change the random state.","cee317ea":"### Verification using the sklearn model","cc2733f9":"# ***Decision Trees***\n\n![alt text](https:\/\/46gyn61z4i0t1u1pnq2bbk2e-wpengine.netdna-ssl.com\/wp-content\/uploads\/2018\/07\/decision-tree.png)","fe511ab8":"***Test Data***\n\ntest_age = 28\ntest_wt = 45\n\nscaled_test_age = -0.798\nscaled_test_wt = -0.517\n\nEuclidean Distance of test data from all the training points : \n\ned_test_s1 = 0.5\n\ned_test_s2 = 0.308\n\ned_test_s3 = 2.25\n\ned_test_s4 = 2.03\n\nLets assume n_neighbors = 3\n\nSo the nearest neighbors will be, s2,s1 and s4 and are classified as Young, Young and Middle respectively. So as the majority of these classes is Young, our test data will be classified as **YOUNG**. This is called *unweighted\/uniform\/majority voting*.\n\n***Weighted\/Inverse Distance Voting***\n\n![alt text](https:\/\/i.ibb.co\/kDnfVzg\/weighted-voting.png)\n\nCloser neighbors get higher votes. Take a neighbor's vote to be inverse of its distance to the new instance. Then we sum the votes and return the class with highest votes.\n\nvote(x) = 1\/ distance(x,ith training data)\n\nSo the above green point will be classified to blue class.","0373f25d":"# Precision and Recall\n### **Example**\n![alt text](https:\/\/i.ibb.co\/HghSf5G\/conf-mat.png)","c35f5631":"### Depth level 2\n\nroot_node -> { s1,s2,s3,s4,s6 }\n\nnode1 -> { s2 } --> no misclassification - {PURE Leaf} - Entropy\/log loss is zero\n\nnode1 -> { s1,s3,s4,s6 } --> no misclassification - {PURE Leaf} - Entropy\/log loss is zero\n\nSo total entropy = 0 + 0\n\nInformation gain = entropy(root node) - entropy(current node) = 0.5156 - 0 = 0.5156\n\n**A fully grown decision tree is always overfit.**","32bd84af":"### WHY WE NEED TO PERFORM STATISTICAL TEST BEFORE SCALING THE DATA?\nAfter using Standard Scalar all the features will have a mean of zero. So there will be no Hypothesis Testing possible.","2fdafe0c":"residual = y - y'\n\nThe difference between actual(y) and the predicted(y') is called a residual","c4ccb873":"For feature selection, correlation matrix will just give us how strongly the variables are correlated with each other, but the final decision whether to keep a variable or drop a variable should be taken by p-value in the ols summary.","e162439c":"#### Using important features","59595131":"# For Classifiction Model\n\np = 1 \/ ( 1 + e^-(beta0 + beta1 * X) )\n\ny = threshold(p)\n\nLet,\n\nz = beta0 + beta1 * X\n\ny_prob = 1 \/ ( 1 + e^-(z) ) = e^z \/ ( 1 + e^z )\n\nFor interpretation of the above equation:\n\n**p \/ (1 - p)** = e^z \/ ( 1 + e^z ) \/ 1 - (e^z \/ ( 1 + e^z )) = **e^z**\n\np \/ (1 - p) --> ODDS (prob of success \/ prob of failure)\n\nlog(ODDS) = log(p \/ (1 - p)) = log(e^z) = z --> beta0 + beta1 * X","e58f8194":"#### Entropy : \nFor binary classification, entropy will be between 0 and 1.\n\nFor multiclass classification, entropy can exceed 1.\n\n#### Gini : \nFor binary classification, gini will be between 0 and 0.5.\n\nFor multiclass classification, gini can exceed 0.5.\n","bde7cee8":"Because of its greedy nature, Decision Tree does not use all the features due to which it is by default a highly biased model.\n\nSo Fully Grown Decision Tree is the only model which is simultaneously susceptible to bias as well as variance error.\n\nRegularization\/Pruning will only reduce the variance error to a certain extent for Decision Trees but will affect to a more extent towards the bias error.\n\nSo to reduce the variance error of Decision Tree we use Random Forest.","de1b0a3b":"## Boosting\n### ***AdaBoost (Adaptive Boosting)***\n\n![alt text](https:\/\/miro.medium.com\/max\/850\/0*KYszvMnr3nCtjaGy.png)\n\n**Learning happens serially but prediction happens parallely.**\n\nConsider 10 records which are input to the 1st estimator. So the probability of each record being picked by the 1st learner will be the same(0.1). Assume the 1st estimator after training misclassifies 2 records and the others are classified correctly. Now for these 2 misclassified records, the probability of getting selected by the 2nd estimator will be increased by a some margin. So to balance the effect, the probability of other records will by decreased by some margin. Now as the probability of misclassified records is higher, those records will definitely be picked up by the 2nd estimator. So the 2nd estimator will pick the difficult records from the learner 1 and some other records. This way again the 2nd estimator will follow the same process as 1st and most of the difficult records will have to be handled by the tail-end estimators gradually.\n\n***Though we say that last few estimators will be recieving the most difficult records, but right from the second estimator itself it started focussing on the difficult records, so our entire model is more focussed towards those difficult data records. So the model tends to give a SLIGHTLY HIGH VARIANCE ERROR bcoz the model will be highly trained towards the difficult records. Hence there will be low training error and this will give LOW BIAS ERROR.*** \n\nWe cannot use KNN for boosting as there is no training involved in KNN model. It directly gets test data and checks how close it is to a training record.\n\nIn Boosting, the prediction happens parallely but each estimator is weighted upon how many of the difficult records are being handled by it. So higher the handling of difficult records, greater will be the weight of those estimators. \n\nFor Adaboost classifier, the weight of those estimators classifying the test record to the same class is summed up. So the class which has a higher sum of weights will be considered as the final prediction for that test record.\n\nFor Adaboost regressor, we take the weighted average of the predicted values of the estimators and predict it as output for the test record.","c9ab3bb4":"## Intuition\n\n**Gaussian Distribution (P) Formula :** \n\n![alt text](https:\/\/cdn-5a6cb102f911c811e474f1cd.closte.com\/wp-content\/uploads\/2017\/11\/Normal-Distribution-Probability-Density-Function.png)\n\n**Prior Probability of a class = Number of records belonging to that class\/ Total number of records**\n\n*Probability that a new test data belongs to a class is given by*:\n\nP(TestPetalLength,TestPetalWidth|setosa) = P(TestPetalLength|setosa) x P(TestPetalWidth|setosa) x PriorProbability(setosa)\n\nP(TestPetalLength,TestPetalWidth|versicolor) = P(TestPetalLength|versicolor) x P(TestPetalWidth|versicolor) x PriorProbability(versicolor)\n\nP(TestPetalLength,TestPetalWidth|virginica) = P(TestPetalLength|virginica) x P(TestPetalWidth|virginica) x PriorProbability(virginica)","b3dab3c4":"For Regreesion model, we calculate the measures like rmse, r-squared and adjusted rsquared.\n\nFor binary classification, we calculate accuracy,confusion matrix,classification report and ROC AUC curve.\n\nFor multiclass classification, we calculate accuracy,confusion matrix,classification report and f1-score.","fb9e4b80":"Assume the root node is a null model.\n\nSo y_pred = [1,1,1,1,1,1,1]\n\nand y = [1,0,1,1,0,1,0]\n\np1 = 4\/7 --> as 4 of then have been correctly classified.\n\np0 = 3\/7 --> as 3 of then have been incorrectly classified.\n\nEntropy = - [ ( (4\/7) * log(4\/7) ) + ( (3\/7) * log(3\/7) ) ] = 0.9857\n\n","9faedcf0":"Assume the root node is a null model.\n\nSo y_pred = ybar = [57.3,57.3,57.3,57.3,57.3,57.3,57.3,57.3,57.3,57.3]\n\nand y = [77,47,55,59,72,60,40,60,45,58]\n\nCost function ---> MSE\n\nMSE = 118.4","06e8e316":"# ***IMPLEMENTATION***","2c39dd4a":"# Confusion Matrix\nFor a confusion matrix, the abonormility is considered as positive case and the opposite to abnormality is considered as negative case.\n\n![alt text](https:\/\/2.bp.blogspot.com\/-EvSXDotTOwc\/XMfeOGZ-CVI\/AAAAAAAAEiE\/oePFfvhfOQM11dgRn9FkPxlegCXbgOF4QCLcBGAs\/s1600\/confusionMatrxiUpdated.jpg)","6c69d37a":"The test2 sample will be classified as SETOSA.","a8c46a98":"# Pseudo R-square (Used Rarely)\n\n![alt text](https:\/\/thestatsgeek.com\/wp-content\/ql-cache\/quicklatex.com-5131a3908febd4a52f24c502dd3a83c4_l3.svg)\n\nNumerator : Log of Likelihood  when all the features are taken into consideration\n\nDenominator : Log of Likelihood  when none of the features are taken into consideration. (The null model will classify all the records to the class with higher probability)\n\nSo according to our model : \n\nPseudo R-square =  1 - [ (-1381.4)\/(-1560.6) ] = 0.1148\n\nMAXIMUM VALUE OF PSEUDO R_SQUARE IS AROUND 0.4","00174fdd":"# ***Naive Bayes***","2bcf98a7":"From the above boxplpot we can see that the Logistic regression has high mean roc auc score and low variance in roc auc score as compared to Fully grown decision tree.\n\nAlso with regularization of DT, we can see there is a mild improvement in the variance error as it has decreased as compared to fully grown decision tree.\n\nFor Random Forest we can see a big leap in improvement of the variance error.","660ebf41":"### **IMPORTANT NOTE :**\n\n1. The role of bagging is to reduce the variance error.\n2. The role of boosting is to reduce the bias error. GridSearch CV will always try to reduce the Bias error. So we need to tune the variance error manually.\n---","574e854e":"#### Using All Features","a654c97c":"## Distance calculation on scaled data\n### **Euclidean distances :** \n\ned_s1_s2 = 0.45\n\ned_s1_s3 = 2.71\n\ned_s3_s4 = 0.453\n\n### **Manhattan distances :** \n\nmd_s1_s2 = 0.592\n\nmd_s1_s3 = 3.82\n\nmd_s3_s4 = 0.59\n\n### **Cosine distances :** \n\ncd_s1_s2 = 0.011\n\ncd_s1_s3 = 1.99\n\ncd_s3_s4 = 0.052","ad99f3d0":"### Decision Tree as a Regressor"}}