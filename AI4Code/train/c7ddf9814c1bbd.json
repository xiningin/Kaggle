{"cell_type":{"fcf32889":"code","f6cf9957":"code","8c57e6a9":"code","554d99ed":"code","8a4a34f4":"code","daa7bc13":"code","7dc17583":"code","253ad40c":"code","40400f9f":"code","232b31e3":"code","17bc1057":"code","2049c811":"code","0b1386f0":"code","fcc28517":"code","5def25ef":"code","da15906b":"code","3d2ba340":"code","79b76db3":"code","f9060137":"code","fe190f15":"code","3bf930e5":"code","f5af17a0":"code","d4b92ab2":"code","1268e57e":"code","ca66ca06":"code","77c59f54":"code","73fac556":"code","6a103fb4":"code","200bf41a":"code","4ac4957b":"code","9c5cfcb2":"code","179b66ef":"code","f31e362d":"code","b4cc5a0e":"code","25ac9e61":"markdown","4fb1087b":"markdown","b8a886eb":"markdown","32ff8177":"markdown","5fffb537":"markdown","e42cc9dc":"markdown","a7cf2527":"markdown","f557e3dc":"markdown","f5f5d3a9":"markdown","8e25721c":"markdown","a3e28b81":"markdown","cce6d712":"markdown","b1708acc":"markdown","76407f92":"markdown","f818b003":"markdown","523139a0":"markdown","dd73dca9":"markdown","cd4f79dd":"markdown","069288fa":"markdown","223adbd4":"markdown","cdcda44e":"markdown","d25d76d5":"markdown","2799c8fd":"markdown","775f41b6":"markdown","18ad2256":"markdown","6a1e1b27":"markdown","07586421":"markdown","eafdfc10":"markdown","9ca69f21":"markdown","4aed2c0f":"markdown","0d4bf80e":"markdown","9dc152a4":"markdown","b1ad99be":"markdown","c1d6daa7":"markdown","bb090434":"markdown","f95d31c4":"markdown","4d252815":"markdown","8a7b6f94":"markdown","c3c10c10":"markdown","9edff21b":"markdown","7b4938ac":"markdown","b4f52b30":"markdown","6705a70a":"markdown","2e33171f":"markdown","1dd56756":"markdown","99154504":"markdown"},"source":{"fcf32889":"!pip install transformers","f6cf9957":"import os\nimport gc\nimport copy\nimport datetime\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\nimport transformers\nfrom transformers import BertTokenizer,BertForSequenceClassification, BertModel, BertConfig\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm\nfrom collections import defaultdict\nimport plotly.graph_objects as go\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, KFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","8c57e6a9":"df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\",usecols=[\"id\",\"excerpt\"])\nprint('Number of training sentences: {:,}\\n'.format(df.shape[0]))\ndf.sample(10)","554d99ed":"def prep_text(text_df):\n    text_df = text_df.str.replace(\"\\n\",\"\",regex=False) \n    return text_df.str.replace(\"\\'s\",r\"s\",regex=True).values\ndf[\"excerpt\"] = prep_text(df[\"excerpt\"])\ntest_df[\"excerpt\"] = prep_text(test_df[\"excerpt\"])","8a4a34f4":"def create_folds(data, num_splits):\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = StratifiedKFold(n_splits=num_splits)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data\n\n\n# create folds\ndf = create_folds(df, num_splits=5)","daa7bc13":"sen_length = []\n\nfor sentence in tqdm(df[\"excerpt\"]):\n   \n    token_words = CONFIG.tokenizer.encode_plus(sentence)[\"input_ids\"]\n    sen_length.append(len(token_words))\n    \nprint('maxlenth of all sentences are  ', max(sen_length))","7dc17583":"class CONFIG:\n    seed = 42\n    max_len = 331\n    train_batch = 16\n    valid_batch = 32\n    epochs = 10\n    learning_rate = 2e-5\n    splits = 5\n    scaler = amp.GradScaler()\n    model='bert-base-cased'\n    tokenizer = BertTokenizer.from_pretrained(model, do_lower_case=True)\n    tokenizer.save_pretrained('.\/tokenizer')\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","253ad40c":"def set_seed(seed = CONFIG.seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(CONFIG.seed)","40400f9f":"class BERTDataset(Dataset):\n    def __init__(self,df):\n        self.text = df['excerpt'].values\n        self.target = df['target'].values\n        self.max_len = CONFIG.max_len\n        self.tokenizer = CONFIG.tokenizer\n        \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        text = ' '.join(text.split())\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_token_type_ids=True\n        )\n\n        return {\n            'ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n            'mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n            'token_type_ids': torch.tensor(inputs[\"token_type_ids\"], dtype=torch.long),\n            'target': torch.tensor(self.target[index], dtype=torch.float)\n        }","232b31e3":"model = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels = 1,\n    output_attentions = False,\n    output_hidden_states = False, \n)\n\nmodel.cuda()","17bc1057":"param_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n     'weight_decay': 0.0001},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n     'weight_decay': 0.0}\n    ]  \n\noptimizer = AdamW(optimizer_parameters, lr=CONFIG.learning_rate)","2049c811":"def get_data(fold):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    train_dataset = BERTDataset(df_train)\n    valid_dataset = BERTDataset(df_valid)\n\n    train_loader = DataLoader(train_dataset, batch_size=CONFIG.train_batch, \n                              num_workers=4, shuffle=True, pin_memory=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG.valid_batch, \n                              num_workers=4, shuffle=False, pin_memory=True)\n    \n    return train_loader, valid_loader","0b1386f0":"train_dataloader,validation_dataloader=get_data(0)","fcc28517":"def loss_fn(output,target):\n     return torch.sqrt(nn.MSELoss()(output,target))\ndef format_time(elapsed):\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))","5def25ef":"def run(optimizer,scheduler):\n    set_seed(40)\n    scaler=CONFIG.scaler\n    training_stats = []\n    total_t0 = time.time()\n    epochs=CONFIG.epochs\n    for epoch_i in range(0, epochs):\n        print(\"\")\n        print('======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, epochs))\n        print('Training...')\n        t0 = time.time()\n        total_train_loss = 0\n        data_size=0\n        model.train()\n        for step, batch in enumerate(train_dataloader):\n            tr_loss=[]\n            b_input_ids = batch['ids'].to(CONFIG.device)\n            b_token_type_ids=batch['token_type_ids'].to(CONFIG.device)\n            b_input_mask = batch['mask'].to(CONFIG.device)\n            b_labels = batch['target'].to(CONFIG.device)\n            batch_size = b_input_ids.size(0)\n            model.zero_grad() \n            with amp.autocast(enabled=True):\n                output= model(b_input_ids,attention_mask=b_input_mask)          \n                output=output[\"logits\"].squeeze(-1)\n                loss = loss_fn(output,b_labels)\n                tr_loss.append(loss.item()\/len(output))\n            scheduler.step()\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n        avg_train_loss = np.mean(tr_loss)    \n        training_time = format_time(time.time() - t0)\n        gc.collect()\n        print(\"\")\n        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n        print(\"  Training epoch took: {:}\".format(training_time))\n        print(\"\")\n        print(\"Running Validation...\")\n\n        t0 = time.time()\n        model.eval()\n        val_loss = 0\n        allpreds = []\n        alltargets = []\n        for batch in validation_dataloader:\n            losses = []\n            with torch.no_grad():\n                device=CONFIG.device\n                ids = batch[\"ids\"].to(device)\n                mask = batch[\"mask\"].to(device)\n                tokentype = batch[\"token_type_ids\"].to(device)\n                output = model(ids,mask)\n                output = output[\"logits\"].squeeze(-1)\n                target = batch[\"target\"].to(device)\n                loss = loss_fn(output,target)\n                losses.append(loss.item()\/len(output))\n                allpreds.append(output.detach().cpu().numpy())\n                alltargets.append(target.detach().squeeze(-1).cpu().numpy())\n        allpreds = np.concatenate(allpreds)\n        alltargets = np.concatenate(alltargets)\n        losses = np.mean(losses)\n        gc.collect() \n        validation_time = format_time(time.time() - t0)\n        print(\"  Validation Loss: {0:.2f}\".format(losses))\n        print(\"  Validation took: {:}\".format(validation_time))\n        training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': losses,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    ) \n    print(\"\")\n    print(\"Training complete!\")\n    return training_stats  ","da15906b":"def Visualizations(training_stats):\n    pd.set_option('precision', 2)\n    df_stats = pd.DataFrame(data=training_stats)\n    df_stats = df_stats.set_index('epoch')\n    layout = go.Layout(template= \"plotly_dark\")\n    fig = go.Figure(layout=layout)\n    fig.add_trace(go.Scatter(x=df_stats.index, y=df_stats['Training Loss'],\n                    mode='lines+markers',\n                    name='Training Loss'))\n    fig.add_trace(go.Scatter(x=df_stats.index, y=df_stats['Valid. Loss'],\n                    mode='lines+markers',\n                    name='Validation Loss'))\n    fig.show()","3d2ba340":"# Defining LR Scheduler\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=0, \n    num_training_steps=len(train_dataloader)*CONFIG.epochs\n)\nlrs = []\nfor epoch in range(1, CONFIG.epochs + 1):\n    if scheduler is not None:\n        scheduler.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(10)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()","79b76db3":"df1=run(optimizer,scheduler)\nVisualizations(df1)","f9060137":"scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.01, patience=1, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\ndef run_model(optimizer,scheduler,epochs):\n    set_seed(40)\n    scaler=CONFIG.scaler\n    training_stats = []\n    total_t0 = time.time()\n\n    for epoch_i in range(0, epochs):\n        print(\"\")\n        print('======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, epochs))\n        print('Training...')\n        t0 = time.time()\n        total_train_loss = 0\n        data_size=0\n        model.train()\n        for step, batch in enumerate(train_dataloader):\n            tr_loss=[]\n            b_input_ids = batch['ids'].to(CONFIG.device)\n            b_token_type_ids=batch['token_type_ids'].to(CONFIG.device)\n            b_input_mask = batch['mask'].to(CONFIG.device)\n            b_labels = batch['target'].to(CONFIG.device)\n            batch_size = b_input_ids.size(0)\n            model.zero_grad() \n            with amp.autocast(enabled=True):\n                output= model(b_input_ids,attention_mask=b_input_mask)          \n                output=output[\"logits\"].squeeze(-1)\n                loss = loss_fn(output,b_labels)\n                tr_loss.append(loss.item()\/len(output))\n            scheduler.step(loss)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        avg_train_loss = np.mean(tr_loss)    \n        training_time = format_time(time.time() - t0)\n\n        gc.collect()\n        print(\"\")\n        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n        print(\"  Training epoch took: {:}\".format(training_time))\n        print(\"\")\n        print(\"Running Validation...\")\n\n        t0 = time.time()\n        model.eval()\n        val_loss = 0\n        allpreds = []\n        alltargets = []\n\n        for batch in validation_dataloader:\n            losses = []\n            with torch.no_grad():\n                device=CONFIG.device\n                ids = batch[\"ids\"].to(device)\n                mask = batch[\"mask\"].to(device)\n                tokentype = batch[\"token_type_ids\"].to(device)\n                output = model(ids,mask)\n                output = output[\"logits\"].squeeze(-1)\n                target = batch[\"target\"].to(device)\n                loss = loss_fn(output,target)\n                losses.append(loss.item()\/len(output))\n                allpreds.append(output.detach().cpu().numpy())\n                alltargets.append(target.detach().squeeze(-1).cpu().numpy())\n        allpreds = np.concatenate(allpreds)\n        alltargets = np.concatenate(alltargets)\n        losses = np.mean(losses)\n        gc.collect() \n        validation_time = format_time(time.time() - t0)\n        print(\"  Validation Loss: {0:.2f}\".format(losses))\n        print(\"  Validation took: {:}\".format(validation_time))\n        training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'lr':optimizer.param_groups[0]['lr'],\n            'Valid. Loss': losses,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    ) \n    print(\"\")\n    print(\"Training complete!\")\n    return training_stats\n    \n\n\n    ","fe190f15":"optimizer = AdamW(optimizer_parameters, lr=CONFIG.learning_rate)\nlambda1 = lambda epoch: 0.65 ** epoch\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\nlrs = []\nfor i in range(10):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(10)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()","3bf930e5":"optimizer = AdamW(optimizer_parameters, lr=CONFIG.learning_rate)\n\nlmbda = lambda epoch: 0.65 ** epoch\nscheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\nlrs = []\n\nfor i in range(10):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(10)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()\n","f5af17a0":"\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\nlrs = []\n\nfor i in range(10):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(10)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()\n","d4b92ab2":"scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[6,8,9], gamma=0.1)\nlrs = []\n\nfor i in range(10):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(10)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()\n","1268e57e":"scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\nlrs = []\nfor i in range(10):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(10)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()\n","ca66ca06":"scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\nlrs = []\nfor i in range(100):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(100)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()\n","77c59f54":"optimizer = torch.optim.SGD(model.parameters(), lr=100)\nscheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.1,step_size_up=5,mode=\"triangular\")\nlrs = []\n\n\nfor i in range(100):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(100)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()\n","73fac556":"optimizer = torch.optim.SGD(model.parameters(), lr=100)\nscheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.1,step_size_up=5,mode=\"triangular2\")\nlrs = []\n\n\nfor i in range(100):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(100)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()\n","6a103fb4":"df9=run(optimizer,scheduler)\n","200bf41a":"Visualizations(df9)","4ac4957b":"optimizer = torch.optim.SGD(model.parameters(), lr=100)\nscheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.1,step_size_up=5,mode=\"exp_range\",gamma=0.85)\nlrs = []\n\n\nfor i in range(100):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(100)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()\n","9c5cfcb2":"optimizer = torch.optim.SGD(model.parameters(), lr=100)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=10, epochs=10)\nlrs = []\n\n\nfor i in range(100):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(100)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()\n","179b66ef":"optimizer = torch.optim.SGD(model.parameters(), lr=100)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=10, epochs=10,anneal_strategy='linear')\nlrs = []\n\n\nfor i in range(100):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(100)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()\n","f31e362d":"lr_sched = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=0.001, last_epoch=-1)\n\n\nlrs = []\n\nfor i in range(100):\n    lr_sched.step()\n    lrs.append(\n        optimizer.param_groups[0][\"lr\"]\n    )\n\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(100)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()\n","b4cc5a0e":"lr_sched = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=0.01, last_epoch=-1)\n\n\nlrs = []\n\nfor i in range(300):\n    lr_sched.step()\n    lrs.append(\n        optimizer.param_groups[0][\"lr\"]\n    )\n\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(300)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()\n","25ac9e61":"# <p style=\"color:#159364; font-family:cursive;\">LOOK AT THE DATA<\/center><\/p>","4fb1087b":"# CYCLIC LR:TRIANGULAR","b8a886eb":"# COSINE ANNEALING WARM RESTARTS","32ff8177":"![Upvote!](https:\/\/img.shields.io\/badge\/Upvote-If%20you%20like%20my%20work-07b3c8?style=for-the-badge&logo=kaggle)\n","5fffb537":"# ONE CYCLE LR:LINEAR ANNEALING","e42cc9dc":"# <p style=\"color:#159364; font-family:cursive;\">LEARNING RATE SCHEDULER <\/center><\/p>","a7cf2527":"# <p style=\"color:#159364; font-family:cursive;\">TRAINING CONFIGURATION<\/center><\/p>","f557e3dc":" <h1 style=\"font-family:verdana;\"> <center>BERT BASELINE AND A WALKTHROUGH THE LEARNING RATE SCHEDULERS <\/center> <\/h1>","f5f5d3a9":"# <p style=\"color:#159364; font-family:cursive;\">MODEL:BERT FOR SEQUENCE CLASSIFICATION from \ud83e\udd17 <\/center><\/p>","8e25721c":"\n# <p><center style=\"color:#159364; font-family:cursive;\">ANALYZE LEARNING RATE VARIATION IN OTHER LEARNING RATE SCHEDULERS<\/center><\/p>","a3e28b81":"# CYCLIC LR\nSets the learning rate of each parameter group according to cyclical learning rate policy (CLR). The policy cycles the learning rate between two boundaries with a constant frequency, as detailed in the paper [Cyclical Learning Rates for Training Neural Networks](https:\/\/arxiv.org\/abs\/1506.01186). The distance between the two boundaries can be scaled on a per-iteration or per-cycle basis.\n\nCyclical learning rate policy changes the learning rate after every batch. step should be called after a batch has been used for training.\nThis class has three built-in policies, as put forth in the paper:\n\n* **\u201ctriangular\u201d**: A basic triangular cycle without amplitude scaling.\n* **\u201ctriangular2\u201d**: A basic triangular cycle that scales initial amplitude by half each cycle.\n* **\u201cexp_range\u201d**: A cycle that scales initial amplitude by \\text{gamma}^{\\text{cycle iterations}}gamma cycle iterations at each cycle iteration.","cce6d712":"* Sets the learning rate of each parameter group according to the 1 cycle learning rate policy.\n* The 1 cycle policy anneals the learning rate from an initial learning rate to some maximum learning rate and then from that maximum learning rate to some minimum learning rate much lower than the initial learning rate.","b1708acc":"# ONE CYCLE LR:COSINE ANNEALING (DEFAULT)","76407f92":"Sets the learning rate of each parameter group using a cosine annealing schedule.\nIf the learning rate is set solely by this scheduler, the learning rate at each step becomes:\n\n$$\n\\eta_{t}=\\eta_{\\min }+\\frac{1}{2}\\left(\\eta_{\\max }-\\eta_{\\min }\\right)\\left(1+\\cos \\left(\\frac{T_{c u r}}{T_{\\max }} \\pi\\right)\\right)\n$$\n\nIt has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts. Note that this only implements the cosine annealing part of SGDR, and not the restarts.https:\/\/arxiv.org\/abs\/1608.03983","f818b003":"**RUNNING OUR MODEL WITH THIS SCHEDULER**","523139a0":"Code taken from:https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds","dd73dca9":"# CYCLIC LR:EXPONENTIAL RANGE","cd4f79dd":"# Multi Step LR","069288fa":"# <p style=\"color:#159364; font-family:cursive;\">REPRODUCIBILITY<\/center><\/p>","223adbd4":"# LAMBDA LR","cdcda44e":"Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones.","d25d76d5":"**RUN ON THE SET SCHEDULER**","2799c8fd":"# ExponentialLR","775f41b6":"**LINEAR SCHEDULE WITH WARMUP**","18ad2256":"Decays the learning rate of each parameter group by gamma every epoch.","6a1e1b27":"Multiplies the learning rate of each parameter group by the factor given in the specified function. When last_epoch=-1, sets initial lr as lr.","07586421":"# <p style=\"color:#159364; font-family:cursive;\">IMPORT THE LIBRARIES<\/center><\/p>","eafdfc10":"# <p style=\"color:#159364; font-family:cursive;\">OPTIMIZER<\/center><\/p>","9ca69f21":"# <p style=\"color:#159364; font-family:cursive;\">GET THE PREPARED DATA<\/center><\/p>","4aed2c0f":"# CYCLIC LR:TRIANGUALR2","0d4bf80e":"# <p style=\"color:#159364; font-family:cursive;\">DEFINE THE DATASET CLASS<\/center><\/p>","9dc152a4":"# <p style=\"color:#159364; font-family:cursive;\">VISUALIZATION FUNCTION <\/center><\/p>","b1ad99be":"Decays the learning rate of each parameter group by gamma every step_size epochs","c1d6daa7":"\n<p style=\"color:#159364; font-family:cursive;\">INSTALL THE TRANSFORMERS PACKAGE FROM THE HUGGING FACE LIBRARY<\/center><\/p>\n","bb090434":"# <p style=\"color:#159364; font-family:cursive;\">CREATE FOLDS<\/center><\/p>","f95d31c4":"# STEP LR","4d252815":"* Reduces learning rate when a metric has stopped improving. Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This scheduler reads a metrics quantity and if no improvement is seen for a \u2018patience\u2019 number of epochs, the learning rate is reduced.\n* In this scheduler,the scheduler.step() requires the loss argument to identify any stagnation in improvement of metric,so we changed the run function","8a7b6f94":"Sets the learning rate of each parameter group to the initial lr times a given function(here it is 0.65^epoch). When last_epoch=-1, sets initial lr as lr.","c3c10c10":"# <p style=\"color:#159364; font-family:cursive;\">A BIT OF PREPROCESSING<\/center><\/p>","9edff21b":"# <p style=\"color:#159364; font-family:cursive;\">DEFINE THE FUNCTION FOR TRAINING,VALIDATION AND RUNNING<\/center><\/p>","7b4938ac":"# Cosine Annealing LR","b4f52b30":"# <p style=\"color:#159364; font-family:cursive;\">FOLD:0<\/center><\/p>","6705a70a":"# <p style=\"color:#159364; font-family:cursive;\">DEFINE LOSS AND TIME FUNCTIONS<\/center><\/p>","2e33171f":"# MULTIPLICATIVE LR","1dd56756":"Set the learning rate of each parameter group using a cosine annealing schedule, and restarts after Ti epochs.\n\n\n$$\n\\eta_{t}=\\eta_{\\min }+\\frac{1}{2}\\left(\\eta_{\\max }-\\eta_{\\min }\\right)\\left(1+\\cos \\left(\\frac{T_{\\operatorname{cur}}}{T_{i}} \\pi\\right)\\right)\n$$\n","99154504":"# 1.Reduce LR On Plateau:"}}