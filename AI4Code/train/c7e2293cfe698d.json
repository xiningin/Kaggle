{"cell_type":{"88736849":"code","4dab7eb8":"code","e331db84":"code","4c24554a":"code","358ee2ac":"code","eda2625f":"code","8676d178":"code","c344c9bf":"code","77979e1f":"code","206f57eb":"code","9abe7de5":"code","8d4c2b98":"code","38fe1b20":"code","e3219cc5":"code","c3bc4c9d":"code","923540c9":"code","1616f44e":"code","4b00ec8a":"code","4501aca1":"code","5aa62be1":"code","fb39617d":"code","fc9e420d":"code","1e290110":"code","5a4d98f6":"code","1fab3d2b":"code","dcbb0ff4":"code","fc4bfdab":"code","fff9e15a":"code","3a390735":"markdown","2576a121":"markdown","feceba1f":"markdown","b623af01":"markdown","96282779":"markdown","5336dd92":"markdown","2fdedad0":"markdown","b2c1dd77":"markdown","b7b1f28c":"markdown"},"source":{"88736849":"BS = 8\nSEED = 42","4dab7eb8":"!pip install ..\/input\/sacremoses\/sacremoses-master\/ > \/dev\/null\n!pip install ..\/input\/transformers\/transformers-master\/ > \/dev\/null","e331db84":"import numpy as np\nimport pandas as pd\nimport pickle\n\nfrom tqdm import tqdm, tqdm_notebook\ntqdm.pandas()\n\nfrom scipy import stats\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import roc_auc_score\n\nimport torch\nimport torch.utils.data\nfrom transformers import *\nimport torch.nn as nn\nfrom pytorch_transformers.modeling_bert import BertPreTrainedModel, BertModel\n\nfrom matplotlib import pyplot as plt\nimport re\nimport math\nfrom math import floor, ceil\n\ndevice = torch.device('cuda')","4c24554a":"train = pd.read_csv('\/kaggle\/input\/google-quest-challenge\/train.csv').fillna(' ')\ntest = pd.read_csv('\/kaggle\/input\/google-quest-challenge\/test.csv').fillna(' ')\nsub = pd.read_csv('..\/input\/google-quest-challenge\/sample_submission.csv').fillna(' ')","358ee2ac":"def _get_segments_xlnet(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"<sep>\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return [0] * (max_seq_length - len(tokens)) + segments\n\ndef _get_segments_bert(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_ids_xlnet(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids =  [5] * (max_seq_length-len(token_ids)) + token_ids\n    return input_ids\n\ndef _get_ids_bert(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\ndef _trim_input(title, question, answer, max_sequence_length=512-1, \n                t_max_len=70-1, q_max_len=219, a_max_len=219):\n\n\n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+4) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)\/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)\/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\" \n                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n        \n        t = t[:t_new_len]\n        q = norm_token_length(q, q_new_len)\n        a = norm_token_length(a, a_new_len)\n    \n    return t, q, a\n\ndef norm_token_length(tokens, l):\n    if len(tokens) > l:\n        head = l\/\/2\n        tail = l - head\n        return tokens[:head] + tokens[-tail:]\n    else:\n        return tokens[:l]\n\ndef _convert_to_bert_inputs(title, question, answer, cate, pretrained_weights, max_sequence_length=512):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    if \"bert-base\" in pretrained_weights:\n        stoken = [\"[CLS]\"] + [cate] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n        input_ids = _get_ids_bert(stoken, tokenizer, max_sequence_length)\n        input_segments = _get_segments_bert(stoken, max_sequence_length)    \n    elif pretrained_weights == \"xlnet-base-cased\":\n        stoken = [cate] + title + [\"<sep>\"] + question + [\"<sep>\"] + answer + [\"<sep>\", \"<cls>\"]\n        input_ids = _get_ids_xlnet(stoken, tokenizer, max_sequence_length)\n        input_segments = _get_segments_xlnet(stoken, max_sequence_length)\n        try:\n            cls_index = input_segments.index(5) - 1\n        except ValueError:\n            cls_index = -1\n        input_segments[cls_index] = 2\n    \n    return [input_ids, input_segments]\n\ndef convert_row(row, pretrained_weights):\n    if pretrained_weights == \"bert-base-uncased\":\n        c = f\"[{row['category'].lower()}]\"\n    elif pretrained_weights == \"bert-base-cased\":\n        c = f\"[{row['category']}]\"\n    elif pretrained_weights == \"xlnet-base-cased\":\n        c = f\"[{row['category']}]\"\n    t, q, a = title = row[\"question_title\"], row[\"question_body\"], row[\"answer\"]\n    t, q, a = _trim_input(t, q, a)\n    ids, segments = _convert_to_bert_inputs(t, q, a, c, pretrained_weights)\n    return np.array([[ids, segments]])","eda2625f":"class CustomBert(BertPreTrainedModel):\n    def __init__(self, config):\n        super(CustomBert, self).__init__(config)\n\n        config.num_labels = 30\n        config.output_hidden_states = True\n        self.n_use_layer = 4\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.dense1 = nn.Linear(768*self.n_use_layer, 768*self.n_use_layer)\n        self.dense2 = nn.Linear(768*self.n_use_layer, 768*self.n_use_layer)    \n        self.classifier = nn.Linear(768*self.n_use_layer, config.num_labels)\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n                position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask)\n        \n        pooled_output = torch.cat([outputs[2][-1*i][:,0] for i in range(1, self.n_use_layer+1)], dim=1)\n        pooled_output = self.dense1(pooled_output)\n        pooled_output = self.dense2(pooled_output)\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        outputs = (logits,) + outputs[2:]\n\n        return outputs","8676d178":"tokenizer = BertTokenizer.from_pretrained('..\/input\/quest-bert-swa\/')\ntokenizer.added_tokens_encoder","c344c9bf":"pretrained_weights = 'bert-base-uncased'\nX_test = test[[\"question_title\", \"question_body\", \"answer\", \"category\"]].progress_apply(lambda x: convert_row(x, pretrained_weights), axis=1).values\nX_test = np.vstack(X_test).reshape((len(X_test), 1024))\n\ntest_dataset = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BS, shuffle=False)","77979e1f":"model = CustomBert.from_pretrained(\"..\/input\/quest-bert-swa\/\")\n\nmodel = model.to(device)\nfor param in model.parameters():\n    param.requires_grad=False\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.eval()","206f57eb":"model_dir_target = \"..\/input\/quest-bertuncased-10fold\/\"\n\nbert_pred_lst = []\nfor fold in range(10):\n    bert_path = f\"{model_dir_target}\/bert-base-uncased_f{fold}_best\"\n    model.load_state_dict(torch.load(bert_path))\n    \n    lst = []\n    for i, (x_batch,)  in enumerate(test_loader):\n        input_ids = x_batch[:, :512]\n        token_ids = x_batch[:, 512:]\n        pred = model(input_ids.to(device), attention_mask=(input_ids > 0).to(device), token_type_ids=token_ids.to(device))\n        lst.append(pred[0].detach().cpu().squeeze().numpy())\n    test_pred = np.vstack(lst)\n    bert_pred_lst.append(test_pred)","9abe7de5":"tokenizer = BertTokenizer.from_pretrained('..\/input\/quest-bertcased-tokenizer\/')\ntokenizer.added_tokens_encoder","8d4c2b98":"pretrained_weights = 'bert-base-cased'\nX_test = test[[\"question_title\", \"question_body\", \"answer\", \"category\"]].progress_apply(lambda x: convert_row(x, pretrained_weights), axis=1).values\nX_test = np.vstack(X_test).reshape((len(X_test), 1024))\n\ntest_dataset = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BS, shuffle=False)","38fe1b20":"model = CustomBert.from_pretrained(\"..\/input\/quest-bert-swa\/\")\n\nmodel = model.to(device)\nfor param in model.parameters():\n    param.requires_grad=False\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.eval()","e3219cc5":"model_dir_target = \"..\/input\/quest-bertcased-10fold\/\"\n\ncased_pred_lst = []\nfor fold in range(10):\n    bert_path = f\"{model_dir_target}\/bert-base-cased_f{fold}_best\"\n    model.load_state_dict(torch.load(bert_path))\n    \n    lst = []\n    for i, (x_batch,)  in enumerate(test_loader):\n        input_ids = x_batch[:, :512]\n        token_ids = x_batch[:, 512:]\n        pred = model(input_ids.to(device), attention_mask=(input_ids > 0).to(device), token_type_ids=token_ids.to(device))\n        lst.append(pred[0].detach().cpu().squeeze().numpy())\n    test_pred = np.vstack(lst)\n    cased_pred_lst.append(test_pred)","c3bc4c9d":"class CustomXLNet(XLNetForSequenceClassification):\n    def __init__(self, config):\n        super(CustomXLNet, self).__init__(config)  \n        config.num_labels = 30\n        config.output_hidden_states = True\n        self.n_use_layer = 4\n        self.n_labels = config.num_labels\n        self.transformer = XLNetModel(config)\n        \n        self.dense1 = nn.Linear(768*self.n_use_layer, 768*self.n_use_layer)\n        self.dense2 = nn.Linear(768*self.n_use_layer, 768*self.n_use_layer)\n        self.logits_proj = nn.Linear(768*self.n_use_layer, config.num_labels)\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        mems=None,\n        perm_mask=None,\n        target_mapping=None,\n        token_type_ids=None,\n        input_mask=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n    ):\n\n        transformer_outputs = self.transformer(\n            input_ids,\n            attention_mask=attention_mask,\n            mems=mems,\n            perm_mask=perm_mask,\n            target_mapping=target_mapping,\n            token_type_ids=token_type_ids,\n            input_mask=input_mask,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n        )\n        \n        output = torch.cat([transformer_outputs[1][-1*(i+1)][:,-1] for i in range(self.n_use_layer)], dim=1)\n        output = self.dense1(output)\n        output = self.dense2(output)\n\n        logits = self.logits_proj(output)\n\n        outputs = (logits,) + transformer_outputs[1:]\n\n        return outputs ","923540c9":"tokenizer = XLNetTokenizer.from_pretrained('..\/input\/quest-xlnet-tokenizer\/')\ntokenizer.added_tokens_encoder","1616f44e":"pretrained_weights = \"xlnet-base-cased\"\n\nX_test = test[[\"question_title\", \"question_body\", \"answer\", \"category\"]].progress_apply(lambda x: convert_row(x, pretrained_weights), axis=1).values\nX_test = np.vstack(X_test).reshape((len(X_test), 1024))\n\ntest_dataset = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BS, shuffle=False)","4b00ec8a":"model = CustomXLNet.from_pretrained(\"..\/input\/quest-xlnet-tokenizer\/\")\n\nmodel = model.to(device)\nfor param in model.parameters():\n    param.requires_grad=False\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.eval()","4501aca1":"model_dir_target = \"..\/input\/quest-xlnetcased-10fold\/\"\n\nxlnet_pred_lst = []\nfor fold in range(10):\n    bert_path = f\"{model_dir_target}\/xlnet-base-cased_f{fold}_best\"\n    model.load_state_dict(torch.load(bert_path))\n    \n    lst = []\n    for i, (x_batch,)  in enumerate(test_loader):\n        input_ids = x_batch[:, :512]\n        token_ids = x_batch[:, 512:]\n        pred = model(input_ids.to(device), attention_mask=(input_ids != 5).int().to(device), token_type_ids=token_ids.to(device))\n        lst.append(pred[0].detach().cpu().squeeze().numpy())\n    test_pred = np.vstack(lst)\n    xlnet_pred_lst.append(test_pred)","5aa62be1":"def sigmoid(x):\n    return 1.0 \/ (1.0 + np.exp(-x))\n\nbert_pred = np.array(bert_pred_lst).mean(0)\ncased_pred = np.array(cased_pred_lst).mean(0)\nxlnet_pred = np.array(xlnet_pred_lst).mean(0)\n\nbert_pred = sigmoid(bert_pred)\ncased_pred = sigmoid(cased_pred)\nxlnet_pred = sigmoid(xlnet_pred)\n\ntest_pred = bert_pred*0.4 + cased_pred*0.2 + xlnet_pred*0.4","fb39617d":"lgbm_models = pickle.load(open(\"..\/input\/quest-lgbm\/lgbm_question_type_spelling.pkl\", 'rb'))\ncount_vectorizers = pickle.load(open(\"..\/input\/quest-lgbm\/tfidf_vectorizers.pkl\", 'rb'))","fc9e420d":"dfs = []\nfor idx, col_name in enumerate([\"question_title\", \"question_body\", \"answer\"]):\n    X = count_vectorizers[idx].transform(test[col_name])\n    feat = [f\"{col_name}_{c}\".encode(\"utf-8\") for c in count_vectorizers[idx].get_feature_names()]\n    df = pd.DataFrame(X.toarray(), columns=feat)\n    dfs.append(df)\ntest_x = pd.concat(dfs, axis=1)\ntest_x.shape","1e290110":"lgbm_pred = np.zeros(len(test_x))\nfor fold in range(4):\n    lgbm_pred += lgbm_models[fold].predict(test_x)\/4\nlgbm_pred.shape ","5a4d98f6":"norm_dict = {}\nfor c in sub.columns[1:]:\n  unique_values = train[c].unique()\n  unique_values = list(set(list(unique_values)+[0,1]))\n  lst = []\n  for common_num in range(90):\n    num = 90 - common_num\n    bunbo = [round((1\/num)*n, 8) for n in range(num+1)]\n    kyoutu = [round(v, 8) for v in unique_values if round(v, 8) in bunbo]\n    if len(kyoutu) == len(unique_values):\n      lst.append(num)\n  norm_dict[c] = min(lst)\nnorm_dict","1fab3d2b":"def spearman_corr(y_true, y_pred):\n    if np.ndim(y_pred) == 2:\n        corr = np.nan_to_num([stats.spearmanr(y_true[:, i], y_pred[:, i])[0] for i in range(y_true.shape[1])]).mean()\n    else:\n        corr = stats.spearmanr(y_true, y_pred)[0]\n    return corr\n\n\n# ref: https:\/\/qiita.com\/kaggle_master-arai-san\/items\/d59b2fb7142ec7e270a5\n# thank you kaggle master\u306e\u30a2\u30e9\u30a4\u3055\u3093!!\nclass OptimizedRounder(object):\n    def __init__(self,\n                 n_overall: int = 5,\n                 n_classwise: int = 5,\n                 n_classes: int = 7,\n                 metric: str = \"qwk\"):\n        self.n_overall = n_overall\n        self.n_classwise = n_classwise\n        self.n_classes = n_classes\n        self.coef = [1.0 \/ n_classes * i for i in range(1, n_classes)]\n        self.metric_str = metric\n        self.metric = spearman_corr\n\n    def _loss(self, X: np.ndarray, y: np.ndarray) -> float:\n        X_p = np.digitize(X, self.coef)\n        ll = -self.metric(y, X_p)\n        return ll\n\n    def fit(self, X: np.ndarray, y: np.ndarray):\n        golden1 = 0.618\n        golden2 = 1 - golden1\n        ab_start = [\n            (0.01, 1.0 \/ self.n_classes + 0.05),\n        ]\n        for i in range(1, self.n_classes):\n            ab_start.append((i * 1.0 \/ self.n_classes + 0.05,\n                             (i + 1) * 1.0 \/ self.n_classes + 0.05))\n        for _ in range(self.n_overall):\n            for idx in range(self.n_classes - 1):\n                # golden section search\n                a, b = ab_start[idx]\n                # calc losses\n                self.coef[idx] = a\n                la = self._loss(X, y)\n                self.coef[idx] = b\n                lb = self._loss(X, y)\n                for it in range(self.n_classwise):\n                    # choose value\n                    if la > lb:\n                        a = b - (b - a) * golden1\n                        self.coef[idx] = a\n                        la = self._loss(X, y)\n                    else:\n                        b = b - (b - a) * golden2\n                        self.coef[idx] = b\n                        lb = self._loss(X, y)\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        X_p = np.digitize(X, self.coef)\n        return X_p","dcbb0ff4":"optR_lst = pickle.load(open(\"..\/input\/quest-optimizedrounder\/optR_lst_10fold_ensemble_v3.pkl\", 'rb'))\n\nlst = []\nfor idx, optR in enumerate(optR_lst):\n    coeff = optR.predict(test_pred[:, idx])\n    lst.append(coeff\/norm_dict[sub.columns[1:][idx]])\nopt_preds = np.array(lst).T\ntest_pred = opt_preds\n\ntest_pred[:,19] = lgbm_pred","fc4bfdab":"test_pred[test[test[\"category\"] != \"CULTURE\"].index, 19] = 0.0\n\ntest[\"host_info\"] = test[\"question_user_page\"].map(lambda x: x.split(\"\/\")[2].replace(\".stackexchange.com\", \"\"))\ntest_pred[test[test[\"host_info\"].map(lambda x: x not in [\"english\", \"ell\"])].index, 19] = 0.0","fff9e15a":"sub[sub.columns[1:]] = test_pred\nsub.to_csv(\"submission.csv\", index=False)\nsub.head()","3a390735":"### Library Install & Import","2576a121":"### uncased","feceba1f":"## BERT","b623af01":"## Post Processing","96282779":"## LGBM","5336dd92":"## Prediction\n### Parameter","2fdedad0":"## Ensemble","b2c1dd77":"## XLNet","b7b1f28c":"## BERT cased"}}