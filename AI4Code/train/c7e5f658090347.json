{"cell_type":{"7b0b37a2":"code","95e2abf6":"code","d1c8a91a":"code","3caf1132":"code","00740893":"code","4b5f5827":"code","1f0f30c4":"code","a9eacba2":"code","a4433e84":"code","4f166ab1":"code","4b9579fb":"code","8ccdbf0b":"code","116dd60f":"code","6189dc5f":"code","61a67349":"code","46f1a076":"code","62259570":"code","4430d404":"code","9741cca2":"code","0c9313e1":"code","49a140a2":"code","9ae49704":"code","d270f541":"code","0d784a29":"code","81252ed1":"code","aed8146d":"code","b3153d3a":"code","73294907":"code","b0720c6c":"markdown","48c0cc8a":"markdown","5f9dda62":"markdown","dd2029b0":"markdown","1e7dfbf9":"markdown","f3df4cbe":"markdown","fa729977":"markdown","9eb3bc19":"markdown","2277aa08":"markdown","57e18193":"markdown","ac433ba6":"markdown","70a52e13":"markdown","a3eeb824":"markdown","28d8f1a6":"markdown","b4d491ec":"markdown","21a3edd3":"markdown","5d1aabc5":"markdown","7ad9d37a":"markdown","bbd48249":"markdown"},"source":{"7b0b37a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","95e2abf6":"# classics\nimport numpy as np \nimport pandas as pd \n\n# sklearn models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\n\n# sklearn bits and bobs. \nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.feature_selection import mutual_info_classif, SelectKBest\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\nfrom sklearn.inspection import permutation_importance\n\n# imblearn library for imbalanced dataset.\nfrom imblearn.pipeline import Pipeline, make_pipeline \nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# plotting\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots","d1c8a91a":"df = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\ndisplay(df.head(5))\ndisplay(df.shape)","3caf1132":"# No missing data to deal with. \ndf.isna().sum()","00740893":"# Unsurpsingly large class imbalance to deal with...\ndisplay(df['Class'].value_counts())\nfraudRatio = round(df['Class'].value_counts()[0] \/ df['Class'].value_counts()[1], 1)\nprint(f\"Ratio of non-fradulent vs fraudulent cases is: {fraudRatio}\")","4b5f5827":"# Reassuring to see some difference between classes along the PCs.\ndisplay(df.groupby('Class').mean())","1f0f30c4":"# \"Time\" can be dropped as each purchases is unrelated to one another. \n# (And if there is an apparent relationship, it is just a coincidence.)\ndf = df.drop([\"Time\"], axis=1)\ndf.head(5)","a9eacba2":"df.dtypes # already correctly set","a4433e84":"fig = make_subplots(rows=6, cols=5, subplot_titles=list(df.columns))  \n\nfor row_num in range(1, 7):\n    start_list = 0 + ((row_num-1) * 5)\n    end_list = 5 + ((row_num-1) * 5)\n    for idx, feature in enumerate(list(df.columns)[start_list: end_list]):\n        fig.add_trace(go.Violin(x=df[\"Class\"][df[\"Class\"] == 1],\n                                y=df[feature][df[\"Class\"] == 1],\n                                legendgroup=\"Fraud\", scalegroup=\"Fraud\", name=\"Fraud\",\n                                line_color=\"blue\"),\n                      row=row_num, col=(idx+1))\n        fig.add_trace(go.Violin(x=df[\"Class\"][df[\"Class\"] == 0][::100], # 1 in 100 otherwise will crash...\n                                y=df[feature][df[\"Class\"] == 0][::100], # 1 in 100 otherwise will crash...\n                                legendgroup=\"Not Fraud\", scalegroup=\"Not Fraud\", name=\"Not Fraud\",\n                                line_color='orange'),\n                      row=row_num, col=(idx+1))\n\nfig.update_traces(meanline_visible=True)\nfig.update_layout(showlegend=False, violingap=0, height=1500,\n                  title_text=\"Distributions of Fraudulent vs Not Fraudulent Transactions along each PC and Amount\")\nfig.show()","4f166ab1":"# linear correlations\ncorrM = df.corr()\nlinear_corr_target = np.around(corrM.values[29], 4) # just to the target. \n\n# normalised mutual info to target variable. (range between 0 and 1)\nx = df.to_numpy()\ny = df[\"Class\"].to_numpy()\n\nmi = mutual_info_classif(x, y, discrete_features=False)\nmi \/= np.max(mi)\n\nmat_fig = go.Figure(data=go.Heatmap(\n                z=corrM.values,\n                x=corrM.index.values,\n                y=corrM.columns.values,\n                colorscale=px.colors.diverging.RdBu,\n                zmin=-1, zmax=1\n))\nmat_fig.update_layout(title=\"Linear Correlation Matrix\")\nmat_fig.show()\n\n\nbar_df = pd.DataFrame()\nbar_df[\"Feature\"] = list(df.columns)\nbar_df[\"Linear Correlation\"] = list(linear_corr_target)\nbar_df[\"Mutual Information\"] = list(mi)\n\nbar_fig = go.Figure(data=[\n    go.Bar(name='Linear Correlation', x=bar_df[\"Feature\"], y=bar_df[\"Linear Correlation\"]),\n    go.Bar(name='Normalised MI', x=bar_df[\"Feature\"], y=bar_df[\"Mutual Information\"])\n])\nbar_fig.update_layout(barmode=\"group\", title=\"Linear Correlation vs Normalised Mutual Information for Each Feature to the Target\")\nbar_fig.show()","4b9579fb":"# First and foremost, seperate my test and training set from my validation set before anything else. \n# 15% to validation, rest to training and testing with cross validation. \n# Use of the \"stratify\" parameter is important here as this way we will have... \n# a good mix of fraudulent and not fraudulent samples in both datasets.\nX = (df.drop(\"Class\", axis=1)).to_numpy() \ny = (df[\"Class\"]).to_numpy() \n\nX_train_test, X_validation, y_train_test, y_validation = train_test_split(\n                                                            X, y, test_size = 0.15, stratify=y, random_state=1)\n\n# confirms there is a well balanced number of 1's and 0's for both sets.\nnp.average(y_train_test), np.average(y_validation) ","8ccdbf0b":"# For the outlier detection, I will put train_test data back into a dataframe so it is easier to manipulate.\ndf_train_test = pd.DataFrame(X_train_test, columns=list(df.columns[0:29])) \ndf_train_test[\"Class\"] = y_train_test\ndf_train_test.head(5)","116dd60f":"# Below 2 helper functions are for outlier removal. \ndef calc_iqr(x, cut_strength):\n    \"\"\"Calculate the interquartile range for a single feature.\n    x is the dataframe column and cut_strength tunes the strength of the cut_off used.\n    Output is the lower and upper bounds for filtering.\"\"\"\n    q25, q75 = np.percentile(x, 25), np.percentile(x, 75)\n    iqr = q75 - q25\n    # calculate the outlier cutoffs\n    cut_off = iqr * cut_strength\n    lower, upper = (q25 - cut_off), (q75 + cut_off)\n    return lower, upper   \n\ndef outlier_removal(df, feature_columns, cut_strength):\n    \"\"\"Take as input (1) the dataframe to perform outlier removal on\n    (2) the columns indexes which contain features to perform outlier removal on and\n    (3) the cutoff strength for the outlier calculation (see above function).\n    Determine the lower and upper limits for each column and append any rows which are outside the outliers. \n    Remove those rows if they are part of the not fraud class.\n    Returns the prepared dataframe and states how many rows were removed.\"\"\"\n    outlier_list = []\n    for col in feature_columns:\n        lower, upper = calc_iqr(df[col], cut_strength)\n        outliers = list( np.where( (df[col] < lower) | (df[col] > upper) ) [0])\n        outlier_list.extend(outliers)\n    outlier_set = set(outlier_list) # unique list of all rows that meet deletion requirements. \n    # prevents fraud rows from being deleted. \n    must_keep = list(np.where(df[\"Class\"] == 1)[0])\n    removal_list = [x for x in outlier_set if x not in must_keep]\n    df_ready = df.drop(removal_list, axis=\"index\")\n    print(f\"{len(removal_list)} outliers removed from the train_test dataset\")\n    return df_ready","6189dc5f":"# only those that seemed to be show a reasonable degree of correlation to the target variable are included here. \n# (Decided based on bar chart generated above.) \n# Amount also not included as seens unreasonable to remove high cost outliers especially from only one class. \nfeature_columns = [\"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\", \"V9\", \"V10\", \"V11\", \"V12\", \"V14\",\n                   \"V16\", \"V17\", \"V18\", \"V19\",\"V21\", \"V27\", \"V28\"] \n\ncut_strength = 3\ndf_train_test_outliers_v1 = outlier_removal(df_train_test, feature_columns, cut_strength)\n\ncut_strength = 5\ndf_train_test_outliers_v2 = outlier_removal(df_train_test, feature_columns, cut_strength)\n\ncut_strength = 7\ndf_train_test_outliers_v3 = outlier_removal(df_train_test, feature_columns, cut_strength)\n\n# remake the train_test arrays. \nX_train_test_outliers_v1 = (df_train_test_outliers_v1.drop(\"Class\", axis=1)).to_numpy() \ny_train_test_outliers_v1 = (df_train_test_outliers_v1[\"Class\"]).to_numpy()\n\nX_train_test_outliers_v2 = (df_train_test_outliers_v2.drop(\"Class\", axis=1)).to_numpy() \ny_train_test_outliers_v2 = (df_train_test_outliers_v2[\"Class\"]).to_numpy()\n\nX_train_test_outliers_v3 = (df_train_test_outliers_v3.drop(\"Class\", axis=1)).to_numpy() \ny_train_test_outliers_v3 = (df_train_test_outliers_v3[\"Class\"]).to_numpy()","61a67349":"# Pipeline Version 1 - Undersample the Dataset to get a 1 to 1 match between the classes.\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1) \nundersample = RandomUnderSampler(sampling_strategy=1, random_state=1) \n\npipeline_1_steps = [(\"under\", undersample),\n                    (\"scaler\", StandardScaler()),\n                    (\"feat_selector\", SelectKBest(mutual_info_classif)),\n                    (\"log_regression\", LogisticRegression(random_state=1))\n                   ]\npipeline_1 = Pipeline(pipeline_1_steps)\n\npipeline_1_params = {\n    \"scaler\": [StandardScaler(), MinMaxScaler()],\n    \"feat_selector__k\": [12, 15, 18, 20, 23, 26, 29],\n    \"log_regression__solver\": [\"liblinear\", \"lbfgs\"],\n}\n\n# Apply the pipeline defined above on all 4 different datasets. \nall_models = {\n    \"Original\" : [X_train_test, y_train_test],\n    \"Outlier Version 1\" : [X_train_test_outliers_v1, y_train_test_outliers_v1],\n    \"Outlier Version 2\" : [X_train_test_outliers_v2, y_train_test_outliers_v2],\n    \"Outlier Version 3\" : [X_train_test_outliers_v3, y_train_test_outliers_v3],\n}\n\nbest_num_feats = []\nbest_solver = []\nbest_scaler = []\nbest_score = []\nbest_stdev = []\n\nfor model_version in all_models:\n    clf_pipeline = GridSearchCV(pipeline_1, pipeline_1_params, scoring=\"roc_auc\", cv=cv, refit=True, n_jobs=-1)\n    clf_pipeline.fit(all_models[model_version][0], all_models[model_version][1])\n    \n    best_num_feats.append(clf_pipeline.best_params_[\"feat_selector__k\"])\n    best_solver.append(clf_pipeline.best_params_[\"log_regression__solver\"])\n    best_scaler.append(clf_pipeline.best_params_[\"scaler\"])\n    best_score.append(clf_pipeline.best_score_)\n    best_stdev.append(clf_pipeline.cv_results_[\"std_test_score\"][clf_pipeline.best_index_])\n    \n# Build a df to present the results. \ndf_results = pd.DataFrame(data={\"Model Version\": [\"Original\", \"Outlier Version 1\", \"Outlier Version 2\", \"Outlier Version 3\"],\n                                \"Best Number of Features\": best_num_feats,\n                                \"Best Solver\": best_solver,\n                                \"Best Scaler\": best_scaler,\n                                \"Best Score\": best_score,\n                                \"Best Score's STDev\": best_stdev\n                               })\ndf_results","46f1a076":"# Pipeline Version 2 - Over sample the Dataset to get a 1 to 1 match between the classes.\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3) \n\noversample = BorderlineSMOTE(sampling_strategy=0.05, random_state=1) # 1 in 20 ratio at this point. \nundersample = RandomUnderSampler(sampling_strategy=1, random_state=1) \n\npipeline_2_steps = [('over', oversample),\n                    ('under', undersample),\n                    ('scaler', StandardScaler()),\n                    ('feat_selector', SelectKBest(mutual_info_classif)),\n                    ('log_regression', LogisticRegression(random_state=1, max_iter=200))\n                   ]\npipeline_2 = Pipeline(pipeline_2_steps)\n\npipeline_2_params = {\n    \"scaler\": [StandardScaler(), MinMaxScaler()],\n    \"feat_selector__k\": [21, 23, 25], # number of options reduced based on observations from Pipeline Version 1\n    \"log_regression__solver\": [\"liblinear\", \"lbfgs\"],\n}\n\n# Apply the pipeline defined above on all 4 different datasets. \nall_models = {\n    \"Original\" : [X_train_test, y_train_test],\n    \"Outlier Version 1\" : [X_train_test_outliers_v1, y_train_test_outliers_v1],\n    \"Outlier Version 2\" : [X_train_test_outliers_v2, y_train_test_outliers_v2],\n    \"Outlier Version 3\" : [X_train_test_outliers_v3, y_train_test_outliers_v3],\n}\n\nbest_num_feats = []\nbest_solver = []\nbest_scaler = []\nbest_score = []\nbest_stdev = []\n\nfor model_version in all_models:\n    clf_pipeline = GridSearchCV(pipeline_2, pipeline_2_params, scoring=\"roc_auc\", cv=cv, refit=True, n_jobs=-1)\n    clf_pipeline.fit(all_models[model_version][0], all_models[model_version][1])\n    \n    best_num_feats.append(clf_pipeline.best_params_[\"feat_selector__k\"])\n    best_solver.append(clf_pipeline.best_params_[\"log_regression__solver\"])\n    best_scaler.append(clf_pipeline.best_params_[\"scaler\"])\n    best_score.append(clf_pipeline.best_score_)\n    best_stdev.append(clf_pipeline.cv_results_['std_test_score'][clf_pipeline.best_index_])\n    \n# Build a df to present the results. \ndf_results = pd.DataFrame(data={\"Model Version\": [\"Original\", \"Outlier Version 1\", \"Outlier Version 2\", \"Outlier Version 3\"],\n                                \"Best Number of Features\": best_num_feats,\n                                \"Best Solver\": best_solver,\n                                \"Best Scaler\": best_scaler,\n                                \"Best Score\": best_score,\n                                \"Best Score's STDev\": best_stdev\n                               })\ndf_results","62259570":"# Pipeline Version 3: Compare no oversampling and undersampling on a random forest model.\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=1) \nundersample = RandomUnderSampler(sampling_strategy=1, random_state=1) \n\npipeline_3_steps = [('under', undersample),\n                    ('scaler', MinMaxScaler()), # doesn't matter for random forest.\n                    ('feat_selector', SelectKBest(mutual_info_classif)),\n                    ('rand_forest', RandomForestClassifier(random_state=1))\n                   ]\npipeline_3 = Pipeline(pipeline_3_steps)\n\npipeline_3_params = {\n    \"feat_selector__k\": [18, 20], # number of options reduced based on observations from Pipeline Version 1 + 2\n    \"rand_forest__n_estimators\": [100],\n}\n\n# Apply the pipeline on only 2 datasets as testing. \nall_models = {\n    \"Original\" : [X_train_test, y_train_test],\n    \"Outlier Version 3\" : [X_train_test_outliers_v3, y_train_test_outliers_v3],\n}\n\nbest_num_feats = []\nbest_solver = []\nbest_score = []\nbest_stdev = []\n\nfor model_version in all_models:\n    clf_pipeline = GridSearchCV(pipeline_3, pipeline_3_params, scoring=\"roc_auc\", cv=cv, refit=True, n_jobs=-1)\n    clf_pipeline.fit(all_models[model_version][0], all_models[model_version][1])\n    \n    best_num_feats.append(clf_pipeline.best_params_[\"feat_selector__k\"])\n    best_solver.append(clf_pipeline.best_params_[\"rand_forest__n_estimators\"])\n    best_score.append(clf_pipeline.best_score_)\n    best_stdev.append(clf_pipeline.cv_results_[\"std_test_score\"][clf_pipeline.best_index_])\n    \n# Build a df to present the results. \ndf_results = pd.DataFrame(data={\"Model Version\": [\"Original\", \"Outlier Version 3\"],\n                                \"Best Number of Features\": best_num_feats,\n                                \"Best Solver\": best_solver,\n                                \"Best Score\": best_score,\n                                \"Best Score's STDev\": best_stdev\n                               })\n\ndisplay(\"Results for no Oversampling with a Random Forest:\")\ndisplay(df_results)","4430d404":"# Pipeline Version 4: Compare no oversampling and undersampling on a random forest model.\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=1) \noversample = BorderlineSMOTE(sampling_strategy=0.05, random_state=1) # 1 in 20 ratio at this point. \nundersample = RandomUnderSampler(sampling_strategy=1, random_state=1) \n\npipeline_4_steps = [(\"over\", oversample),\n                    (\"under\", undersample),\n                    (\"scaler\", MinMaxScaler()), # doesn't matter for random forest.\n                    (\"feat_selector\", SelectKBest(mutual_info_classif)),\n                    (\"rand_forest\", RandomForestClassifier(random_state=1))\n                   ]\npipeline_4 = Pipeline(pipeline_4_steps)\n\npipeline_4_params = {\n    \"feat_selector__k\": [18, 20], # number of options reduced based on observations from Pipeline Version 1 + 2\n    \"rand_forest__n_estimators\": [100],\n}\n\n# Apply the pipeline on only 2 datasets as testing.\nall_models = {\n    \"Original\" : [X_train_test, y_train_test],\n    \"Outlier Version 3\" : [X_train_test_outliers_v3, y_train_test_outliers_v3],\n}\n\nbest_num_feats = []\nbest_solver = []\nbest_score = []\nbest_stdev = []\n\nfor model_version in all_models:\n    clf_pipeline = GridSearchCV(pipeline_4, pipeline_4_params, scoring=\"roc_auc\", cv=cv, refit=True, n_jobs=-1)\n    clf_pipeline.fit(all_models[model_version][0], all_models[model_version][1])\n    \n    best_num_feats.append(clf_pipeline.best_params_[\"feat_selector__k\"])\n    best_solver.append(clf_pipeline.best_params_[\"rand_forest__n_estimators\"])\n    best_score.append(clf_pipeline.best_score_)\n    best_stdev.append(clf_pipeline.cv_results_[\"std_test_score\"][clf_pipeline.best_index_])\n    \n# Build a df to present the results. \ndf_results = pd.DataFrame(data={\"Model Version\": [\"Original\", \"Outlier Version 3\"],\n                                \"Best Number of Features\": best_num_feats,\n                                \"Best Solver\": best_solver,\n                                \"Best Score\": best_score,\n                                \"Best Score's STDev\": best_stdev\n                               })\n\ndisplay(\"Results for Oversampling with a Random Forest:\")\ndisplay(df_results)","9741cca2":"# Final Pipeline - Model 1: Logistic Regression. \ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1) \nundersample = RandomUnderSampler(sampling_strategy=1, random_state=1) \n\nlog_regress_pipeline_steps = [(\"under\", undersample),\n                    (\"scaler\", StandardScaler()),\n                    (\"feat_selector\", SelectKBest(mutual_info_classif)),\n                    (\"log_regress\", LogisticRegression(random_state=1))\n                   ]\nlog_regress_pipeline = Pipeline(log_regress_pipeline_steps)\n\nlog_regress_params = {\n    \"scaler\": [StandardScaler(), MinMaxScaler()],\n    \"feat_selector__k\": [18, 20, 22, 24],\n    \"log_regress__solver\": [\"liblinear\", \"lbfgs\"],\n}\n\nclf_log_regress = GridSearchCV(log_regress_pipeline, log_regress_params, scoring=\"roc_auc\", cv=cv, refit=True, n_jobs=-1)\nclf_log_regress.fit(X_train_test_outliers_v1, y_train_test_outliers_v1)\n\n# Build a df to present the results. \nlog_regress_results = {\n    \"ML Model\": [\"Logissic Regression\"], \n    \"Best Number of Features\": [clf_log_regress.best_params_[\"feat_selector__k\"]],\n    \"Best Solver\": [clf_log_regress.best_params_[\"log_regress__solver\"]],\n    \"Best Scaler\": [clf_log_regress.best_params_[\"scaler\"]], \n    \"Best Score\": [clf_log_regress.best_score_],\n    \"Best Score's STDev\": [clf_log_regress.cv_results_[\"std_test_score\"][clf_log_regress.best_index_]],\n}\n\nlog_regress_df = pd.DataFrame.from_dict(log_regress_results)\n\ndisplay(\"Best model from Logistic Regression Grid Search CV:\")\ndisplay(log_regress_df)","0c9313e1":"# Final Pipeline - Model 2: Random Forest. \ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1) \nundersample = RandomUnderSampler(sampling_strategy=1, random_state=1) \n\nrand_forest_pipeline_steps = [(\"under\", undersample),\n                              (\"scaler\", StandardScaler()),\n                              (\"feat_selector\", SelectKBest(mutual_info_classif)),\n                              (\"rand_forest\", RandomForestClassifier(random_state=1))\n                             ]\nrand_forest_pipeline = Pipeline(rand_forest_pipeline_steps)\n\nrand_forest_params = {\n    \"feat_selector__k\": [18, 20, 22, 24],\n    \"rand_forest__n_estimators\": [50, 100, 150, 200, 250],\n}\n\nclf_rand_forest = GridSearchCV(rand_forest_pipeline, rand_forest_params, scoring=\"roc_auc\", cv=cv, refit=True, n_jobs=-1)\nclf_rand_forest.fit(X_train_test_outliers_v1, y_train_test_outliers_v1)\n\n# Build a df to present the results. \nrand_forest_results = {\n    \"ML Model\": [\"Random Forest\"], \n    \"Best Number of Features\": [clf_rand_forest.best_params_[\"feat_selector__k\"]],\n    \"Best Number of Estimators\": [clf_rand_forest.best_params_[\"rand_forest__n_estimators\"]],\n    \"Best Score\": [clf_rand_forest.best_score_],\n    \"Best Score's STDev\": [clf_rand_forest.cv_results_[\"std_test_score\"][clf_rand_forest.best_index_]],\n}\n\nrand_forest_df = pd.DataFrame.from_dict(rand_forest_results)\n\ndisplay(\"Best model from Random Forest Grid Search CV:\")\ndisplay(rand_forest_df)","49a140a2":"# Final Pipeline - Model 3: Support Vector Machine. \ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1) \nundersample = RandomUnderSampler(sampling_strategy=1, random_state=1) \n\nsvm_pipeline_steps = [(\"under\", undersample),\n                    (\"scaler\", StandardScaler()),\n                    (\"feat_selector\", SelectKBest(mutual_info_classif)),\n                    (\"svm\", SVC(random_state=1))\n                   ]\nsvm_pipeline = Pipeline(svm_pipeline_steps)\n\nsvm_params = {\n    \"scaler\": [StandardScaler(), MinMaxScaler()],\n    \"feat_selector__k\": [18, 20, 22, 24],\n    \"svm__C\": [0.1, 1,10],\n    \"svm__kernel\": [\"rbf\", \"poly\", \"sigmoid\", \"linear\"],  \n}\n\nclf_svm = GridSearchCV(svm_pipeline, svm_params, scoring=\"roc_auc\", cv=cv, refit=True, n_jobs=-1)\nclf_svm.fit(X_train_test_outliers_v1, y_train_test_outliers_v1)\n\n# Build a df to present the results. \nsvm_results = {\n    \"ML Model\": [\"Support Vector Machine\"], \n    \"Best Number of Features\": [clf_svm.best_params_[\"feat_selector__k\"]],\n    \"Best kernel\": [clf_svm.best_params_[\"svm__kernel\"]],\n    \"Best Regularization parameter\": [clf_svm.best_params_[\"svm__C\"]],\n    \"Best Scaler\": [clf_svm.best_params_[\"scaler\"]], \n    \"Best Score\": [clf_svm.best_score_],\n    \"Best Score's STDev\": [clf_svm.cv_results_[\"std_test_score\"][clf_svm.best_index_]],\n}\n\nsvm_df = pd.DataFrame.from_dict(svm_results)\n\ndisplay(\"Best model from Support Vector Machine Grid Search CV:\")\ndisplay(svm_df)","9ae49704":"# Final Pipeline - Model 4: Gradient Boosting\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1) \nundersample = RandomUnderSampler(sampling_strategy=1, random_state=1) \n\nGBoost_pipeline_steps = [(\"under\", undersample),\n                    (\"scaler\", StandardScaler()),\n                    (\"feat_selector\", SelectKBest(mutual_info_classif)),\n                    (\"GBoost\", GradientBoostingClassifier(random_state=1))\n                   ]\nGBoost_pipeline = Pipeline(GBoost_pipeline_steps)\n\nGBoost_params = {\n    \"scaler\": [StandardScaler()], # shouldn't make a difference as with random forests. \n    \"feat_selector__k\": [18, 20, 22, 24], \n    \"GBoost__n_estimators\": [100, 200, 300, 400, 500],\n}\n\nclf_GBoost = GridSearchCV(GBoost_pipeline, GBoost_params, scoring=\"roc_auc\", cv=cv, refit=True, n_jobs=-1)\nclf_GBoost.fit(X_train_test_outliers_v1, y_train_test_outliers_v1)\n\n# Build a df to present the results. \nGBoost_results = {\n    \"ML Model\": [\"Gradient Boosting\"], \n    \"Best Number of Features\": [clf_GBoost.best_params_[\"feat_selector__k\"]],  \n    \"Best Score\": [clf_GBoost.best_score_],\n    \"Best Score's STDev\": [clf_GBoost.cv_results_[\"std_test_score\"][clf_GBoost.best_index_]],\n}\n\nGBoost_df = pd.DataFrame.from_dict(GBoost_results)\n\ndisplay(\"Best model from Gradient Boosting Grid Search CV:\")\ndisplay(GBoost_df)","d270f541":"yhat = clf_log_regress.predict(X_validation)\nroc_auc_log_regress = round(roc_auc_score(y_validation, yhat), 3)\n\ncm = confusion_matrix(y_validation, yhat)\n\ndisplay(f\"Logistic Regression roc_auc_Score: {roc_auc_log_regress}\")\nax = sns.heatmap(cm, annot=True, fmt=\"d\")","0d784a29":"yhat = clf_rand_forest.predict(X_validation)\nroc_auc_log_regress = round(roc_auc_score(y_validation, yhat), 3)\n\ncm = confusion_matrix(y_validation, yhat)\n\ndisplay(f\"Random Forest roc_auc_Score: {roc_auc_log_regress}\")\nax = sns.heatmap(cm, annot=True, fmt=\"d\")","81252ed1":"yhat = clf_svm.predict(X_validation)\nroc_auc_log_regress = round(roc_auc_score(y_validation, yhat), 3)\n\ncm = confusion_matrix(y_validation, yhat)\n\ndisplay(f\"Support Vector Machine roc_auc_Score: {roc_auc_log_regress}\")\nax = sns.heatmap(cm, annot=True, fmt=\"d\")","aed8146d":"yhat = clf_GBoost.predict(X_validation)\nroc_auc_log_regress = round(roc_auc_score(y_validation, yhat), 3)\n\ncm = confusion_matrix(y_validation, yhat)\n\ndisplay(f\"Gradient Boosting roc_auc_Score: {roc_auc_log_regress}\")\nax = sns.heatmap(cm, annot=True, fmt=\"d\")","b3153d3a":"ml_models = {\"Logistic Regression\": clf_log_regress,\n             \"Random Forest\": clf_rand_forest, \n             \"Support Vector Machine\": clf_svm, \n             \"Gradient Boosting\": clf_GBoost}\n\nmodel_importances = {}\nfor method, model in ml_models.items():\n    results = permutation_importance(model, X, y, scoring=\"roc_auc\", random_state=1)\n    unscaled_importances = results.importances_mean\n    \n    min_max_scaler = MinMaxScaler()\n    scaled_importances = min_max_scaler.fit_transform((unscaled_importances.reshape(-1, 1)))\n    model_importances[method] = scaled_importances.reshape(1, 29).ravel() #rearrange so plotly happy with it. ","73294907":"bar_fig = go.Figure(data=[\n    go.Bar(name=\"Logistic Regression\", x=bar_df[\"Feature\"], y=model_importances[\"Logistic Regression\"]),\n    go.Bar(name=\"Random Forest\", x=bar_df[\"Feature\"], y=model_importances[\"Random Forest\"]),\n    go.Bar(name=\"Support Vector Machine\", x=bar_df[\"Feature\"], y=model_importances[\"Support Vector Machine\"]),\n    go.Bar(name=\"Gradient Boosting\", x=bar_df[\"Feature\"], y=model_importances[\"Gradient Boosting\"]),\n])\nbar_fig.update_layout(barmode=\"group\", title=\"Feature Permutation Importance for each ML Model Evaluated\")\nbar_fig.update_layout(xaxis=dict(title=\"Feature Name\"), yaxis=dict(title=\"Normalized Feature Importance\"))\nbar_fig.show()","b0720c6c":"<a id='Model_Eval'><\/a>","48c0cc8a":"## Define the Initial Testing Pipeline with Logistic Regression\n\nI will try two approaches to handle the class imbalances present for the 4 train_test datasets above. After the over\/undersampling I will scale the data prior to model building.\n\n**Version 1:** Undersample the Dataset to get a 1 to 1 match between the classes. \n\n**Version 2:** Oversample the Dataset with Borderline-SMOTE to get a 1 to 1 match between the classes. \n\n**Version 3 + 4:** After completing the above two versions and observing no notable difference with over sampling. I decided to try a different ML model (random forest) which should be more \"data hungry\" and see if this also is insensitive to under vs oversampling.\n\nBorderline-SMOTE is a special type of oversampling technique that only makes new versions of minority class members that are rather ambiguous\/uncertain to classify. So the extra examples generated are more likely to be useful to the model (we do not need to upsample in regions that are easily classified as 1 or 0 already). \n\nIt is [recommended to combine oversampling with undersampling](https:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/), and this will also notably reduce model training time. So we will upsample the fraud examples to 1:20 and then downsample the major class afterwards to get a 1:1 ratio. \n","5f9dda62":"## Summary \nThis is a binary classification problem with a highly imbalanced dataset (ratio of approx. 1:600) that is about distinguishing fraudulent card transactions from non-fraudulent transactions. The input features are the card purchase amount and a set of 29 principal components (PCs) from principal component analysis (PCA). To tackle this challenge I attempted to use (1) both over and under sampling, (2) feature removal, (3) outlier removal (only from the not fraudulent class, because I did not want to lose any examples from the fraudulent class) and (4) various ML models and hyperparameters (with GridSearchCV).\n\n#### The Final Scores are as Follows: \n\n| Model | ROC AUC for Train\/Test Dataset | ROC AUC for Validation Dataset |\n| --- | --- |  --- |\n| Logistic Regression | 0.983 | 0.947|\n| Random Forest |  0.983 | 0.919 |\n| Support Vector Machine | 0.985 | 0.931 |\n| Gradient Boosting | 0.984 | 0.931 |\n\nMeaning the Logistic Regression model had the best performance on the validation set, but all models performed very similarly. Note that the ROC AUC is the [Area Under the Receiver Operating Characteristic Curve](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_auc_score.html), a metric that can handle class imbalances. \n\nWhen studying the above results we can see most models have highly similar performances (according to this metric). Analysis of the confusion matrices would suggest I have made a good filter to identify \"suspicious transactions\", as whilst many non-fraudulent transactions are labelled as fraud, almost all fraudulent transactions were correctly identified. I think this is likely in part due to using outlier removal on the not fraud class, meaning more uncertain cases would be trained to predict fraud over not fraud. In a practical context this may actually be ideal. One can imagine a scenario where a fairly rapid pre-filtering model (something like this) is used to identify potentially fraudulent transactions (but casts a wide enough net to ultimately catch many non-fraudulent ones too). These transactions could then be passed onto a more advanced model (that could for instance also consider a given card user's most recent purchases, location etc..) to hopefully make a more accurate decision at that point. If this was the goal, then the model developed here could be further improved by working on the outlier removal of the non-fraudulent data (or using harsher filtering criteria) and\/or altering the cost function used to make false negatives more undesirable.  \n\n\n### Links to the Different Sections of the Notebook\n\n### [Setting up the Environment and Reading in the Dataset](#Enviro_Setup)\n\n### [Exploratory Data Analysis](#Exploratory_Data_Analysis)\n\n### [Comparing the Distributions of each Feature to the Target Classes](#Feat_Distrib)\n\n### [Linear and Non-Linear Correlations to the Target Class](#Correlations)\n\n### [Some Initial ML Model Tests and Outlier Removal](#Initial_ML)\n\n### [Final Model Building](#Final_ML)\n\n### [Model Evaluations](#Model_Eval)\n\n### [Comparison of the Feature Importances for the Different Models](#Feauture_Importance)","dd2029b0":"<a id='Final_ML'><\/a>","1e7dfbf9":" <a id='Correlations'><\/a>","f3df4cbe":"<a id='Initial_ML'><\/a> ","fa729977":" <a id='Exploratory_Data_Analysis'><\/a> ","9eb3bc19":"<a id='Enviro_Setup'><\/a> ","2277aa08":" <a id='Feat_Distrib'><\/a>","57e18193":"## Reflections on the Above Results And Moving on to Final Model Training \n\n#### Reflections:\n1. Outlier removal seems to be useful, of course there is a danger here that one could overfit if the outlier removal is too aggressive (especially considering that I am only removing outliers from the not fraud class). So I will not use a too harsh outlier removal value in the upcoming model building.\n2. Reducing the number of features seems to have a beneficial impact on model quality, (this is consistent with the expectations from the exploratory data analysis performed earlier). Given that this reduces model complexity, we will take this approach forward, but use a narrower range of options than what was attempted above.\n3. No clear winner between MinMax and StandardScaler, so I will keep them both in the pipeline where relevant (not important for Random Forest or Gradient Boosting) \n4. Oversampling did not have any notable impact. At first I thought this could be due to the nature of ML model I chose to evaluate (logistic regression does not tend to require too much data to fit relative to other models), so I performed some tests with a random forest model too. But the results were repeated there, so I will just undersample. \n\n#### Model Training Time:\nNow it's time to build the final classification model. \n\n**I will try 4 classification models:**\n1. Logistic Regression\n2. Random Forest\n3. Support Vector Machine\n4. Gradient Boosting ","ac433ba6":"## Exploratory Data Analysis\nIn this section we first check to see if the data is okay (things like: is there missing data? And is the data is set to the correct data type(s)?). Then I will move on to looking at the relationships between the features and the targets.  ","70a52e13":"## Linear and Non-Linear Correlations to the Target Class: Are All of The Features Important for Prediction?\n\n* Below we will first generate a correlation matrix (that will look for only linear correlations). Given that features V1-V28 were generated from PCA it is not a surprise to see that all PCs have 0 linear correlation with each other. We can also see the largest absolute correlation value between two features (\"V2\" and \"Amount\") is 0.53, meaning we do not need to worry about any potential issues from multicollinearity.  \n\n* Perhaps a better question is do we need all these features? To help with this, below both the linear correlation and the mutual information (able to find non-linear relationships) for all features to the target is shown. These results clearly suggest that features such as \"V13\", \"V15\", \"V22\" to \"V26\" (and others) could be removed prior to machine learning. This will be investigated later.","a3eeb824":"## Evaluating the Final Model\n\nFor the 4 models trained above, we will now evaluate their performances on the validation set using the area under the receiver operating characteristic curve (\"roc_auc_score\" in sklearn). \n\nFor more insight into their performance we will also generate a confusion matrix on the validation results for each model.  \n\n##### A Reminder on How to Interpret a Confusion Matrix:\n* True Negatives (Top-Left Square): This is the number of **correct classifications** of the **No Fraud** Detected class.\n* False Negatives (Top-Right Square): This is the number of **incorrect classifications** of the **No Fraud** Detected class.\n* False Positives (Bottom-Left Square): This is the number of **incorrect classifications** of the **Yes Fraud** Detected class\n* True Positives (Bottom-Right Square): This is the number of **correct classifications** of the **Yes Fraud** Detected class.\n\n\n#### Final Model Performance Comments\nLooking at the above four models we see a general trend of... \nDo they all perform incredibly similarly? ","28d8f1a6":"## Setting up the Environment and Reading in the Dataset","b4d491ec":"## Comparison of the Feature Importances for the Different Models\nAlthough nearly all features are from PCA (so little insight possible from analysing the feature importances), it could be interesting to see how similarily each model weigths each of the features. It is of course always important to remember the caveat that feature importance does not mean feature impact.\n\nBelow are the calculated [permutation importances](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.inspection.permutation_importance.html) for each model based on the validation dataset. \n\nClearly features \"V14\" and \"V4\" were found to be the most important features for distinguishing between both classes. Further, there are some model specific differences in what features were determined to be useful.","21a3edd3":"## Comparing the Distributions of each Feature to the Target Classes\nBelow, violin plots are used to compare the distributions of each feature to the target dataset. \nBeyond showing that there are differences in the distributions between each class (good!), an argument can be made towards performing some kind of outlier removal prior to training the model. Depending on the model used, outliers can have a relatively large impact on the model (sometimes referred to as \"high leverage\").","5d1aabc5":"<a id='LogReg_pipeline'><\/a>","7ad9d37a":"## Initial Machine Learning Tests Prior to Proper Model Building.\n\n#### Above we saw that:\n1. There is a large class imbalance to be dealt with.\n2. There are potentially outliers present. Further, the violin plots showed us some of the distributions do not have a gaussian distribution. This means that the outlier removal method chosen should be okay working with non-normal distributions.\n3. Some features could be removed from the model as they do not appear to contribute much. \n\n\n#### Additional Considerations: \n1. Given what I have described above, there is a clear danger of having way too many things to try in combination with one another. I will therefore first test some of the ideas above out on a single, rapid to train model (logistic regression), and then make some decisions based on that on what to take forward for a second round with some other models. Edit: I added random forest after based on some observations. \n\n#### Therefore:\n1. Building pipelines will be valuable here due to the desire to evaluate the impact of multiple things (e.g. outlier removal, feature removal, up and down sampling). \n2. As I want to evaluate multiple protocols I will use stratified k-fold cross-validation. This means my training and testing data will be combined and I will use this approach to select my best model before final validation of this model with the \"evaluation set\".\n\n3. I will use the Interquartile Range Method to try to detect and remove outliers from the model. We have so few samples of fraud that we will keep all of them, but for not fraud samples we will apply outlier removal. \n\n4. I will evaluate my models with the roc_auc_score (as recommended by the challenge author). This provides a balanced view of model quality for both imbalanced and balanced datasets (test_train runs will always have a balanced dataset so technically not required at this stage).","bbd48249":"<a id='Feauture_Importance'><\/a>"}}