{"cell_type":{"21f52a9e":"code","d91c4792":"code","7a300f43":"code","c5496cbd":"code","ecb2861d":"code","d36b3f0e":"code","a4823c04":"code","d358746e":"code","39deef1e":"code","dce32e31":"code","52a5e040":"code","c0003acc":"markdown","a3827772":"markdown","218f03bd":"markdown","f538ed05":"markdown","9c3c7409":"markdown","35ef6460":"markdown","4c12f5cc":"markdown","0746d2b3":"markdown","00efdffc":"markdown"},"source":{"21f52a9e":"import nltk\nimport random\nimport pandas as pd\n\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords","d91c4792":"spam = pd.read_csv(\"..\/input\/spam-text-message-classification\/SPAM text message 20170820 - Data.csv\")\nspam.head()","7a300f43":"data_set = []\n\nfor index,row in spam.iterrows():\n    data_set.append((row.Message, row.Category))\n    \nprint(\"Messages : \", len(data_set))\nprint(\"--------------------------\")\ndata_set[:5]","c5496cbd":"stemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\ndef preprocess(document, stem = True):\n    \n    # Change the sentence to lower case\n    document = document.lower()\n    \n    # Tokenize into words\n    words = word_tokenize(document)\n    \n    # Remove the stop words\n    words = [word for word in words if word not in stopwords.words(\"english\")]\n    \n    if stem:\n        words = [stemmer.stem(word) for word in words]\n    else:\n        words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n        \n    # Join the words in sentence\n    document = \" \".join(words)\n    \n    return document\n\nmessage_set = []\nfor (Message, Category) in data_set:\n    words_filtered = [msg.lower() for msg in preprocess(Message).split() if len(msg) >= 3]\n    message_set.append((words_filtered, Category))\n    \nprint(message_set[:5])","ecb2861d":"# Getting words from the messages\ndef get_words_in_messages(messages):\n    all_words = []\n    \n    for (Message, Category) in messages:\n        all_words.extend(Message)\n        \n    return all_words\n\n# Getting the words using nltk library\ndef get_words_features(word_list):\n    word_list = nltk.FreqDist(word_list)\n    return word_list.keys()\n\n# Creating word features from the dataset\nword_features = get_words_features(get_words_in_messages(message_set))\nprint(\"Word Features : \", len(word_features))","d36b3f0e":"print(\"Total Length        : \", len(message_set))\nprint(\"----------------------------------------\")\n\n# Slicing into 80-20 ratio\nslice_index = int(len(message_set) * .8)\nprint(\"Slicing By          : \", slice_index)\nprint(\"----------------------------------------\")\n\n# Shuffling the message set\nrandom.shuffle(message_set)\n\ntrain_set, test_set = message_set[:slice_index], message_set[slice_index:]\nprint(\"Length of Train Set : \", len(train_set))\nprint(\"Length of Test Set  : \", len(test_set))","a4823c04":"def extract_features(document):\n    document_set = set(document)\n    features = {}\n    \n    for word in word_features:\n        features['Contains(%s)' % word] = (word in document_set)\n    \n    return features\n\ntrain_features = nltk.classify.apply_features(extract_features, train_set)\ntest_features = nltk.classify.apply_features(extract_features, test_set)\n\nprint(\"Length of Train Features : \", len(train_features))\nprint(\"Length of Test Features  : \", len(test_features))","d358746e":"# Training the classifier with NaiveBayes algorithm\nspamClassifier = nltk.NaiveBayesClassifier.train(train_features)","39deef1e":"# Analyzing the accuracy of the test set\nprint(\"Accuracy of Training Set : \", round(nltk.classify.accuracy(spamClassifier, train_features)*100, 2))","dce32e31":"# Analyzing the accuracy of the test set\nprint(\"Accuracy of Test Set    : \", round(nltk.classify.accuracy(spamClassifier, test_features)*100, 2))","52a5e040":"# Testing a example message with our newly trained classifier\nm1 = 'CONGRATULATIONS!! As a valued account holder you have been selected to receive a \u00a3900 prize reward! Valid 12 hours only.'\nm2 = 'Hi, how r u?'\n\nprint('Classification Result of 1st Example : ', spamClassifier.classify(extract_features(m1.split())))\nprint('Classification Result of 2nd Example : ', spamClassifier.classify(extract_features(m2.split())))","c0003acc":"## <font color='#4a8bad'>Preparing to Create Feature Maps<\/font>\n***\n<a id=\"feature-maps\"><\/a>","a3827772":"## <font color='#4a8bad'>Preparing to Create Train & Test Sets<\/font>\n***\n<a id=\"creating\"><\/a>","218f03bd":"## <font color='#4a8bad'>Preparing to Create Features<\/font>\n***\n<a id=\"features\"><\/a>","f538ed05":"* [Importing Libraries](#importing)\n* [Reading the File](#reading)\n* [Preprocessing](#preprocessing)\n* [Preparing to Create Features](#features)\n* [Preparing to Create Train & Test Sets](#creating)\n* [Preparing to Create Feature Maps](#feature-maps)\n* [Training](#training)\n* [Evaluation](#evaluation)","9c3c7409":"## <font color='#4a8bad'>Importing Libraries<\/font>\n***\n<a id=\"importing\"><\/a>","35ef6460":"## <font color='#4a8bad'>Reading the File<\/font>\n***\n<a id=\"reading\"><\/a>","4c12f5cc":"## <font color='#4a8bad'>Preprocessing<\/font>\n***\n<a id=\"preprocessing\"><\/a>","0746d2b3":"## <font color='#4a8bad'>Evaluation<\/font>\n***\n<a id=\"evaluation\"><\/a>","00efdffc":"## <font color='#4a8bad'>Training<\/font>\n***\n<a id=\"training\"><\/a>"}}