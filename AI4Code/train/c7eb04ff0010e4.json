{"cell_type":{"6551bf8d":"code","a27aad01":"code","098b5eea":"code","3b720c5a":"code","1f8af04c":"code","57687afe":"code","91eb3597":"code","1424cab9":"code","bd222cd2":"code","e1178b75":"code","ba823537":"code","0fb95b0a":"code","0c443e11":"code","62304c6a":"code","154d1b30":"code","1f2d07b0":"code","344ebf0c":"code","2f143290":"code","369633cb":"code","7cd49019":"markdown","ba3b4a96":"markdown","92052e33":"markdown","6f952c9a":"markdown","bd93e550":"markdown","d4bafd52":"markdown","db4dbdba":"markdown","75f4cb65":"markdown","870e2fc1":"markdown","a2abab78":"markdown","727f4ad0":"markdown","cf0e233f":"markdown","b177cdbd":"markdown","f0268f33":"markdown","e6cd2a8a":"markdown","b168a6d0":"markdown","f2f45dff":"markdown","0be1d34a":"markdown"},"source":{"6551bf8d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n# -*- coding: utf-8 -*-","a27aad01":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt","098b5eea":"df = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")","3b720c5a":"print(df.head())","1f8af04c":"print(df.isnull().sum())","57687afe":"for i in df.columns:\n    print(i,len(df[df[i] == 0]))","91eb3597":"df[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']] = df[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].replace(0,np.nan)\nprint(df.isnull().sum())","1424cab9":"df = df.fillna(method = 'ffill')\ndf = df.fillna(method = 'bfill')\nprint(df.isnull().sum())","bd222cd2":"fig1, axes1 =plt.subplots(4,2,figsize=(14, 19))\nlist1_col=['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI', 'DiabetesPedigreeFunction', 'Age']\nfor i in range(len(list1_col)):\n    row=i\/\/2\n    col=i%2\n    ax=axes1[row,col]\n    sns.boxplot(df[list1_col[i]],ax=ax).set(title='PRODUCT BY ' + list1_col[i].upper())","e1178b75":"print(df.Insulin.shape)\nprint(df.SkinThickness.shape)\nQ1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)\n\ndf = df[~((df.iloc[:,4:5] < (Q1 - 1.5 * IQR)) |(df.iloc[:,4:5] > (Q3 + 1.5 * IQR))).any(axis=1)]\ndf = df[~((df.iloc[:,3:4] < (Q1 - 1.5 * IQR)) |(df.iloc[:,3:4] > (Q3 + 1.5 * IQR))).any(axis=1)]\n\nprint(df.Insulin.shape)\nprint(df.SkinThickness.shape)","ba823537":"fig1, axes1 =plt.subplots(4,2,figsize=(14, 19))\nlist1_col=['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI', 'DiabetesPedigreeFunction', 'Age']\nfor i in range(len(list1_col)):\n    row=i\/\/2\n    col=i%2\n    ax=axes1[row,col]\n    sns.boxplot(df[list1_col[i]],ax=ax).set(title='PRODUCT BY ' + list1_col[i].upper())","0fb95b0a":"x = df.drop(\"Outcome\", axis=1)\ny = df.Outcome","0c443e11":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1) \nprint(x_train.head())","62304c6a":"from sklearn.linear_model import LogisticRegression\nlr =LogisticRegression(solver='liblinear')\nlr.fit(x_train, y_train)\ny_pred = lr.predict(x_test)\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))","154d1b30":"y_pred_proba = lr.predict_proba(x_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\nfpr \ntpr\n\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nauc\n\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")","1f2d07b0":"dt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\ny_pred1 = dt.predict(x_test)\n\nprint(classification_report(y_test, y_pred1))\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred1))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred1))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred1))","344ebf0c":"y_pred_proba = dt.predict_proba(x_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\nfpr \ntpr\n\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nauc\n\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")","2f143290":"rf = RandomForestClassifier()\nrf.fit(x_train,y_train)\ny_pred2 = rf.predict(x_test)\n\nprint(classification_report(y_test, y_pred2))\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred2))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred2))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred2))","369633cb":"y_pred_proba = rf.predict_proba(x_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\nfpr \ntpr\n\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nauc\n\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")","7cd49019":"### Observation(Outliers):\nWe can clearly observe that insulin reaching 800 which means there is soo many outliers in df.Insulin. Similarly, in SkinThickness columns also, outlier exists.\nWe have to remove them for better prediction (better accuracy).\n### Removing outlier\nNow let's identify and remove outlier. \nWe will use quantile method to remove outliers.","ba3b4a96":"### Observing outliers again with Boxplot\nLet's observe again outliers using boxplot. We are observing again to make sure that all outliers which we want to remove is removed.","92052e33":"### Please do upvotes if you found this helpful and don't forget to comment any suggestion or anything.\n## Thank you","6f952c9a":"### Removing null values","bd93e550":"# Clearning null values and zeros from data\n### Checking is there any null value\nNull values existence can affect our prediction accuracy, hence we need to remove null values if it exists.","d4bafd52":"## Logistic Regression\nLet's predict using Logistic Regression","db4dbdba":"### Train data and Test data\nLet's split data into train and test:","75f4cb65":"# Importing necessary library","870e2fc1":"## Random Forest Classifier\nLet's predict using random forest classifier and check it's accuracy, recall and precision.","a2abab78":"### Zeros detected\nAs zero were detected in columns like Pregnancies, Glucose, BloodPressure, SkinThickness, and Insulin, we need to remove it. We will do this by replacing zeros into null values then simply we will use ffill or bfill method.\nBut columns like Pregnancies can have zero so we won't remove zeros from this type of columns.","727f4ad0":"### AUC\nNow let's check its AUC","cf0e233f":"### Checking null isn't sufficient\nSo no null values were found but that's not enough, as there may be some zeros in columns like Blood Pressure which is not normal and can affect our prediction accuracy.\n### Zeros \nNow let's check is there any zeros in columns like Blood Pressure, Insulin, etc:","b177cdbd":"### Observation:\nAs we can see that outlier in insulin and skinthickness is removed so we can proceed further and predict.\n# Prediction","f0268f33":"# Final Result\nSo as we can see from above three prediction, Logistic regression has more accuracy.\nAlso Logistic regression has more auc than others but auc of Logistic Regression and auc of Random Forest Classifier are almost similar. \nWhile Logistic regression and Random forest classifier has almost similar precision (Precision of Logistic regression is slightly higher than Random forest classifier's precision)\nIf we talk about Recall, then Decision tree classifier has more recall than others.                                                                                    ","e6cd2a8a":"### AUC\nNow let's check its AUC","b168a6d0":"## Decision Tree Classifier\nLet's predict using decision tree classifier and check it's accuracy, recall and precision.","f2f45dff":"# Checking outliers\n### Boxplot\nNow let's check outliers in our data by simply using boxplot. ","0be1d34a":"### AUC\nNow let's check auc"}}