{"cell_type":{"6ad7c69d":"code","713b3b34":"code","1e95db6f":"code","d69a3da7":"code","5100df07":"code","f595c00a":"code","7317056c":"code","20ef6ae4":"code","54d8df05":"code","20d2164b":"code","760059f8":"code","c008ab05":"code","33dd8938":"code","e732f08a":"code","76addd4a":"code","0eb0a631":"code","4a13915a":"code","54c174b5":"code","3433aedf":"code","0c502a2b":"code","cdcb1cca":"code","63c3cb9a":"code","bceed60f":"code","c05ee7e9":"code","5501ccdc":"code","d05a3709":"code","2f255374":"code","98a4c2dd":"code","4b9fbe17":"code","d0ae3585":"code","7ffd633a":"code","0e6eeec0":"code","1842af83":"code","93387b27":"code","841d03a7":"code","3d257f75":"code","0a369aff":"code","5698e550":"code","3f798c2a":"code","8efc5970":"code","50283e79":"code","b11feb63":"code","8b183994":"code","8448d657":"code","d1451839":"code","992c3b62":"code","192ce320":"code","39efbbe7":"code","98d91519":"code","b20cb936":"code","63704d0a":"code","16baa217":"markdown","b114c368":"markdown","58dcd1ea":"markdown","02a4fc8f":"markdown","b6385b23":"markdown","b66fe0a9":"markdown","251d2653":"markdown","7d82f207":"markdown","45c863cc":"markdown","17e4b8d1":"markdown","2bf89611":"markdown","b9ff0fe4":"markdown","66c58ec4":"markdown","b0683b03":"markdown","82ec0a2a":"markdown","4559397e":"markdown","844b6a89":"markdown","b2f5211b":"markdown","3ff081b0":"markdown","48078862":"markdown","dae050be":"markdown","c6a81649":"markdown","5f0fd2fa":"markdown","e478f1c3":"markdown","eef108f4":"markdown","f02a2e6c":"markdown","8af8b147":"markdown","d632ca00":"markdown","b2869ff1":"markdown","49d0bd7d":"markdown","b918e89e":"markdown","6ec9a7fd":"markdown","5b0c0c6e":"markdown","8a467759":"markdown","db1b04a4":"markdown","3fbcbab4":"markdown","7c6077ac":"markdown","3a9b3e6a":"markdown","b187605e":"markdown","fce87794":"markdown","a30cec88":"markdown","85e118a3":"markdown","d7194074":"markdown","5b4785b5":"markdown","0e27e42a":"markdown","b52041e0":"markdown","ad30362e":"markdown","3afbd834":"markdown","9a12b0e6":"markdown","5afcd1a3":"markdown","f7ee5907":"markdown","081b50cd":"markdown","4069986b":"markdown","95187309":"markdown","83a1978b":"markdown","cab8fa46":"markdown","2aa78fad":"markdown"},"source":{"6ad7c69d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\n\nfrom scipy import stats\nfrom scipy.stats import norm \nimport warnings \n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit, RepeatedStratifiedKFold\nfrom sklearn import metrics\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom bayes_opt import BayesianOptimization\n\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error\nfrom sklearn.metrics.scorer import make_scorer\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n\nimport xgboost as xgb\nfrom lightgbm import LGBMRegressor\nimport gc","713b3b34":"df_train = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/train.csv.zip', parse_dates=['Date'])\ndf_test = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/test.csv.zip', parse_dates=['Date'])\n\ndf_stores = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv')\ndf_features = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/features.csv.zip', parse_dates=['Date'])","1e95db6f":"df_train.head()","d69a3da7":"df_stores.head()","5100df07":"df_features.head()","f595c00a":"#train\ndf_train_join = df_train.merge(df_stores, on='Store', how='left')\ndf_train_join = df_train_join.merge(df_features, on=['Store', 'Date', 'IsHoliday'], how='left')\n\n#test\ndf_test_join = df_test.merge(df_stores, on='Store', how='left') \ndf_test_join = df_test_join.merge(df_features, on=['Store', 'Date', 'IsHoliday'], how='left')","7317056c":"df_train_join.head()","20ef6ae4":"df_test_join.head()","54d8df05":"df_train_join.shape","20d2164b":"df_train_join.dtypes","760059f8":"df_train_join.describe()","c008ab05":"len(df_train_join[df_train_join['Weekly_Sales'] < 0])\/len(df_train_join)","33dd8938":"df_train_join.isnull().sum(axis = 0)","e732f08a":"df_test_join.isnull().sum(axis = 0)","76addd4a":"set(df_test_join[~df_test_join.Unemployment.notnull()]['Date'])\n","0eb0a631":"set(df_test_join[~df_test_join.CPI.notnull()]['Date'])","4a13915a":"df_train_join[df_train_join.isnull().any(axis=1)]['Date']","54c174b5":"df_train_join.skew()","3433aedf":"sns.kdeplot(df_train_join['Weekly_Sales'])","0c502a2b":"print(\"Skewness: {} \\nKurtosis: {}\".format(df_train_join['Weekly_Sales'].skew().round(), df_train_join['Weekly_Sales'].kurt().round()))","cdcb1cca":"#train\ndf_train_join['Year'] = df_train_join['Date'].dt.year\ndf_train_join['Month'] = df_train_join['Date'].dt.month\ndf_train_join['Week'] = df_train_join['Date'].dt.week\ndf_train_join['Day'] = df_train_join['Date'].dt.day\n\n#test\ndf_test_join['Year'] = df_test_join['Date'].dt.year\ndf_test_join['Month'] = df_test_join['Date'].dt.month\ndf_test_join['Week'] = df_test_join['Date'].dt.week\ndf_test_join['Day'] = df_test_join['Date'].dt.day","63c3cb9a":"from pylab import rcParams\nrcParams['figure.figsize'] = 8, 8","bceed60f":"sns.boxplot(x=\"Month\", y=\"Weekly_Sales\", data=df_train_join, showfliers=False)","c05ee7e9":"sns.boxplot(x=\"IsHoliday\", y=\"Weekly_Sales\", data=df_train_join, showfliers=False)","5501ccdc":"df_train_join[df_train_join['IsHoliday']==True]['Week'].unique()","d05a3709":"sns.lineplot(x=\"Week\", y=\"Weekly_Sales\", data=df_train_join, hue='Year')","2f255374":"sns.boxplot(y=\"Weekly_Sales\", x=\"Store\", data=df_train_join, showfliers=False)","98a4c2dd":"sns.boxplot(x=\"Dept\", y=\"Weekly_Sales\", data=df_train_join, showfliers=False)","4b9fbe17":"sns.boxplot(y=\"Type\", x=\"Weekly_Sales\", data=df_train_join, showfliers=False)","d0ae3585":"plt.figure(figsize=(10, 50))\nsns.boxplot(y='Dept', x='Weekly_Sales', data=df_train_join, showfliers=False, hue=\"Type\",orient=\"h\") ","7ffd633a":"sns.boxplot(x='Size', y='Weekly_Sales', data=df_train_join, showfliers=False)","0e6eeec0":"df_train_join[['Store','Type']].groupby(['Type']).nunique()","1842af83":"sns.scatterplot(x=\"Weekly_Sales\", y=\"Unemployment\", data=df_train_join)","93387b27":"corr = df_train_join.corr()\nplt.figure(figsize=(15, 10))\nsns.heatmap(corr, annot=True, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","841d03a7":"#MarkDown1 to Markdown5: null values = 0\ndf_train_join[['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']] = df_train_join[['MarkDown1', 'MarkDown2', 'MarkDown3', \n                                                                                                  'MarkDown4', 'MarkDown5']].fillna(value=0)\n\ndf_test_join[['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']] = df_test_join[['MarkDown1', 'MarkDown2', 'MarkDown3', \n                                                                                                  'MarkDown4', 'MarkDown5']].fillna(value=0)\n#MarkDown1 to Markdown5: if less than 0 = 0\ndf_train_join['MarkDown1'] = df_train_join['MarkDown1'].apply(lambda x: 0 if x < 0 else x)\ndf_train_join['MarkDown2'] = df_train_join['MarkDown2'].apply(lambda x: 0 if x < 0 else x)\ndf_train_join['MarkDown3'] = df_train_join['MarkDown3'].apply(lambda x: 0 if x < 0 else x)\ndf_train_join['MarkDown4'] = df_train_join['MarkDown4'].apply(lambda x: 0 if x < 0 else x)\ndf_train_join['MarkDown5'] = df_train_join['MarkDown5'].apply(lambda x: 0 if x < 0 else x)\n\ndf_test_join['MarkDown1'] = df_test_join['MarkDown1'].apply(lambda x: 0 if x < 0 else x)\ndf_test_join['MarkDown2'] = df_test_join['MarkDown2'].apply(lambda x: 0 if x < 0 else x)\ndf_test_join['MarkDown3'] = df_test_join['MarkDown3'].apply(lambda x: 0 if x < 0 else x)\ndf_test_join['MarkDown4'] = df_test_join['MarkDown4'].apply(lambda x: 0 if x < 0 else x)\ndf_test_join['MarkDown5'] = df_test_join['MarkDown5'].apply(lambda x: 0 if x < 0 else x)\n\n# Negative weekly sales on train data\ndf_train_join['Weekly_Sales'] = df_train_join['Weekly_Sales'].apply(lambda x: 0 if x < 0 else x)\n\n# CPI and Unemployment rate - drop (low correlation + missing values on test set)\n# We can - always - change our mind later if model performance is poor)\ndf_train_join.drop(['CPI', 'Unemployment'], axis=1, inplace=True)\ndf_test_join.drop(['CPI', 'Unemployment'], axis=1, inplace=True)\n\n# Fuel price and Temperature - Drop (low value on correlation plot)\ndf_train_join.drop(['Fuel_Price', 'Temperature'], axis=1, inplace=True)\ndf_test_join.drop(['Fuel_Price', 'Temperature'], axis=1, inplace=True)","3d257f75":"# Drop date - we transformed it into year\/month\/day\/week variables\ndf_train_join.drop(['Date'], axis=1, inplace=True)\ndf_test_join.drop(['Date'], axis=1, inplace=True)","0a369aff":"#log for skewed variables\nskewed = ['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']\ndf_train_join[skewed] = df_train_join[skewed].apply(lambda x: np.log(x + 1))\ndf_test_join[skewed] = df_test_join[skewed].apply(lambda x: np.log(x + 1))\n\n#adding easter holiday - we saw on the EDA it was important!\ndf_train_join.loc[(df_train_join.Year==2010) & (df_train_join.Week==13), 'IsHoliday'] = True\ndf_train_join.loc[(df_train_join.Year==2011) & (df_train_join.Week==16), 'IsHoliday'] = True\ndf_train_join.loc[(df_train_join.Year==2012) & (df_train_join.Week==14), 'IsHoliday'] = True\n\ndf_test_join.loc[(df_test_join.Year==2013) & (df_test_join.Week==13), 'IsHoliday'] = True\n\n#create dummy variables - store type and holliday\ndf_train_join = pd.get_dummies(df_train_join, columns=['Type'])\ndf_test_join = pd.get_dummies(df_test_join, columns=['Type'])\n\n#trasform IsHoliday\ndf_train_join['IsHoliday'] = df_train_join['IsHoliday'].apply(lambda x: 1 if x==True else 0)\ndf_test_join['IsHoliday'] = df_test_join['IsHoliday'].apply(lambda x: 1 if x==True else 0)","5698e550":"#Also apply log to weekly sales\ndf_train_join['Weekly_Sales'] = df_train_join['Weekly_Sales'].apply(lambda x: np.log(x + 1))","3f798c2a":"train_X, val_X, train_y, val_y = train_test_split(df_train_join.drop('Weekly_Sales', axis = 1), \n                                                  df_train_join['Weekly_Sales'], \n                                                  test_size = 0.2, \n                                                  random_state = 42)","8efc5970":"def wmae(pred_y, test_y, weights):\n    return 1\/sum(weights) * sum(weights * abs(test_y - pred_y))\n\ndef calculate_weights(holidays):\n    return holidays.apply(lambda x: 1 if x==0 else 5)","50283e79":"def train_predict(model, train_X, train_y, test_X, test_y, verbose=0): \n    \n    results = {}  \n    \n    model = model.fit(train_X, train_y)\n    predictions = model.predict(test_X)\n            \n    # WMAE on Test Set\n    results['WMAE'] = wmae(np.exp(test_y), \n                           np.exp(predictions), \n                           calculate_weights(test_X['IsHoliday']))\n    \n\n    importances = model.feature_importances_\n    std = np.std([model.feature_importances_ for model in model.estimators_],\n             axis=0)\n    indices = np.argsort(importances)[::-1]\n    #Print importances\n    for f in range(train_X.shape[1]):\n        print(\"%d. feature %s (%f)\" % (f + 1, train_X.columns[f], importances[indices[f]]))\n\n    # Success\n    print(\"Model Name:\", model.__class__.__name__)\n    print(\"WMAE:\", round(results['WMAE'],2))\n    \n    # Return the model & predictions\n    return (model, predictions, importances)","b11feb63":"model_rf = RandomForestRegressor(random_state=42, verbose=1)","8b183994":"m, pred_y, feature_importances = train_predict(model_rf, train_X, train_y, val_X, val_y, verbose=3)","8448d657":"params = { \n    'n_estimators': [50, 80],\n    'max_features': [None, 'auto'],\n    'bootstrap': [True, False],\n    'max_depth':[None],\n    'random_state': [42], \n    'verbose': [1]\n}","d1451839":"CV = GridSearchCV(estimator=model_rf, param_grid=params, cv=3, verbose=1)\nCV.fit(train_X, train_y)","992c3b62":"print('Best parameters: {}'.format(CV.best_params_))","192ce320":"final_model = model_rf.set_params(**CV.best_params_)","39efbbe7":"m_final, pred_y_val_final, feature_importances_final = train_predict(final_model, train_X, train_y, val_X, val_y, verbose=3)","98d91519":"y_test_pred = m_final.predict(df_test_join)","b20cb936":"df_test_join['Weekly_Sales'] = y_test_pred","63704d0a":"sns.lineplot(x=\"Week\", y=\"Weekly_Sales\", data=df_test_join)","16baa217":"## First look on our dataset","b114c368":"## Joining the datasets","58dcd1ea":"Checking missing values:","02a4fc8f":"All missing values on test set are from 2013.","b6385b23":"Markdown columns are skewed features.","b66fe0a9":"# Correlations","251d2653":"## Sales per store type","7d82f207":"Let's see our sales distribution per day:","45c863cc":"# Plotting results","17e4b8d1":"## Sales per year","2bf89611":"# Libs","b9ff0fe4":"- We have some indications of more sales on holidays, but not much.\n- We see hollidays on weeks 6 (superbowl), 36 (labor day), 47 (thanksgiving) and 52 (christmas)\n- It is missing Easter!","66c58ec4":"Let's use all out previous conclusions to move forward.","b0683b03":"Train set:\n- We only have null values on MarkDown columns. Those columns represents the Type of markdown and what quantity was available during that week.\n\nTest set:\n- Null values on CPI and Unemployment rate","82ec0a2a":"## Feature engineering","4559397e":"This erros occurs on less than 1% of the dataset. Phew.","844b6a89":"Hightly skewed, big tail (high kurtosis). Lets create more datetime columns:","b2f5211b":"It was the chosen metric for this competition.\nIt punishes mistakes based on some weight criteria, specially on hollidays.","3ff081b0":"## Sales per unenployment rate","48078862":"For now, let's plot just one of them to see if there's an impact of sales.","dae050be":"Why?\n- Robust to outliers\n- Proven performance on several competitions\n- Easy interpretability for tree based models (for non-technical stakeholders)\n- Can handle both categorical and numerical data :)\n\nWatch out for:\n- Overfitting!!!!","c6a81649":"# Tuning using Grid Search CV","5f0fd2fa":"Each store has its own sales pattern.","e478f1c3":"Negative values on weekly sales needs to be corrected or neglected. Let's see if this erros occurs on many rows.","eef108f4":"We can have some new takeaways and confirm what we already saw on the previous plots:\n- Temperature no correlation with weekly sales, but strong correlation with variable Month (as expected, of course)\n- The bigger the store, more weekly sales\n- Fuel price does not seem to have correlation with weekly sales\n- CPI and unemployment rate (with missing values on test set) have low correlation rate with weekly sales","f02a2e6c":"Big difference between department types. ","8af8b147":"We already looked into some features, but let's see how they correlate to each other:","d632ca00":"## Handling missing values","b2869ff1":"## Chosen metric: Weighted Mean Absolute Error (WMAE)","49d0bd7d":"The distribution of store types is not equal. We have more store types that tends to have more weekly sales.","b918e89e":"Yes it does. The smaller the store, the smaller the sales.","6ec9a7fd":"Let's transform some of out features into categorical values","5b0c0c6e":"## Sales per store","8a467759":"# Train and predict base model","db1b04a4":"Missing values on years 2010, 2011 and 2012 on out train set.","3fbcbab4":"Tunned model feature importances didn't change its magnitude.\nIf WMAE is good enough, the model that goes into production uses this parameters!\n<br>\n<br>\nWhat dictates if the model is good enough overall? **Business needs** (output speed + computing cost + some WMAE threshold)\n<br>\n<br>\nWhat if the error is still too big?\n- Retrain w\/o higly correlated feature\n- Use more parameters on grid search or use Bayesian Optimization (+ computing cost)\n- New features e.g countdown for important hollidays (xmas, thanksgiving...)\n- Boosting models such as lightgbm, catboost, xboost (it decreases the explainability and needs more computing power, but a  lot more powerful)","7c6077ac":"## What about hollidays?","3a9b3e6a":"We can see some seasonality here.","b187605e":"# Data split (train-test-validation)","fce87794":"## Looking at data","a30cec88":"## Does store size matter?","85e118a3":"# Chosen model: Ensemble with tree based models","d7194074":"## Data Exploration","5b4785b5":"- Peak of yearly sales on the last weeks\/holliday related\n- Sales on year before seems like a strong predictor (may become a feature)\n- Easter looks like an important holliday on 1st semester, we should include it. Our first weak model could be simply the weekly sales iin the year before +- some threshold, quick win :)","0e27e42a":"Here we can see which variables affects our model the most :)\nFirst feature seems highly correlated to the outcome, which is often **undesirable**.","b52041e0":"## Sales per department type","ad30362e":"We can't see much here. One could argue that higher unemployment rates would affect weekly sales, but we cannot see that clearly on the plot. Most of weekly sales comes from average unemployment rate locations, but that only means that most of the locations have average unenployment rates.","3afbd834":"If you have more time\/computing power, you can test lots other parameters\/values.","9a12b0e6":"## What if we combine store type and department?","5afcd1a3":"# Load datasets","f7ee5907":"We can wee clear distinction of weekly sales just by oversing those two variables combined.","081b50cd":"# Important remarks","4069986b":"# Data preprocessing","95187309":"# EDA","83a1978b":"Visually, we get a hint that there are some stores selling more than others, and store type is algo important: Store type C tends to sell less than store type B or A.","cab8fa46":"## Check","2aa78fad":"## Store type distribution"}}