{"cell_type":{"a21c4df5":"code","31962064":"code","df18671c":"code","6a91f787":"code","42a4767e":"code","4a5aebeb":"code","93d7e181":"code","673677d8":"code","bcd00f6f":"code","0f901612":"code","da9e51e6":"code","a0cf2703":"code","41ab3816":"code","286cf8c1":"code","c6329c38":"code","dac90e36":"code","c43bae36":"code","f155f02b":"code","99134263":"code","35ecdf1d":"code","eb0dff93":"code","ca508ab7":"code","46bced94":"code","071d829e":"code","34327ea7":"code","7440521e":"code","f38376c1":"code","52a99bdc":"code","c17f10e5":"code","0b65e67c":"code","66669272":"code","549abbb3":"code","28d96cfc":"code","2210e45b":"code","c627c5ab":"code","4ae55836":"code","69afe07a":"code","a52f540c":"code","6a82c397":"code","99222346":"markdown","b489e0a3":"markdown","aa7cd8be":"markdown","bb8ffb00":"markdown","340e6a85":"markdown","868de1e2":"markdown","f29cdbb6":"markdown","75d731c1":"markdown"},"source":{"a21c4df5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","31962064":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('dark_background')\nsns.set_style(\"darkgrid\")\n","df18671c":"%%time\n# Reading File\ntrain_path  = '..\/input\/train.csv'\n\n# Set columns to most suitable type to optimize for memory usage, default is float 64 but we just need float 32, it will save a lot of RAM\ntraintypes = {'fare_amount': 'float32',\n              'pickup_datetime': 'str', \n              'pickup_longitude': 'float32',\n              'pickup_latitude': 'float32',\n              'dropoff_longitude': 'float32',\n              'dropoff_latitude': 'float32',\n              'passenger_count': 'uint8'}\n\ncols = list(traintypes.keys())\n# I used 2.000.000 rows to test and 10.000.000 to commit\ntrain_df = pd.read_csv(train_path, usecols=cols, dtype=traintypes, nrows=2_000_000)","6a91f787":"%%time\n# Save into feather format, it will be faster for the next time \ntrain_df.to_feather('nyc_taxi_data_raw.feather')","42a4767e":"# load the same dataframe next time directly, without reading the csv file again!\n\ndf_train = pd.read_feather('nyc_taxi_data_raw.feather')\n","4a5aebeb":"# check datatypes\ndf_train.dtypes","93d7e181":"# check statistics of the features\ndf_train.describe()","673677d8":"len(df_train[df_train.fare_amount > 0])","bcd00f6f":"# Since they are less than 300 records so we will drop them ( for 2.000.000 rows)\ndf_train = df_train[df_train.fare_amount>=0]","0f901612":"# IQR is 3.5 so we can see most data less than 20 but we will plot data to 100 to see some outliers\nsns.distplot(df_train[df_train.fare_amount < 100].fare_amount, bins=50);","da9e51e6":"# Count the number of null value\ndf_train.isnull().sum()","a0cf2703":"# it's not too much so we will drop all row with null value\ndf_train = df_train.dropna(how = 'any', axis = 'rows')\n","41ab3816":"# read test data\ndf_test =  pd.read_csv('..\/input\/test.csv')\ndf_test.head(5)","286cf8c1":"df_test.describe()","c6329c38":"# We will paste pickup date to date type\ndf_train['pickup_datetime'] = pd.to_datetime(df_train['pickup_datetime'],format=\"%Y-%m-%d %H:%M:%S UTC\")","dac90e36":"df_train['pickup_datetime']","c43bae36":"df_test['pickup_datetime'] = pd.to_datetime(df_test['pickup_datetime'],format=\"%Y-%m-%d %H:%M:%S UTC\")","f155f02b":"def add_new_date_time_features(dataset):\n    dataset['hour'] = dataset.pickup_datetime.dt.hour\n    dataset['day'] = dataset.pickup_datetime.dt.day\n    dataset['month'] = dataset.pickup_datetime.dt.month\n    dataset['year'] = dataset.pickup_datetime.dt.year\n    dataset['day_of_week'] = dataset.pickup_datetime.dt.dayofweek\n    \n    return dataset","99134263":"df_train = add_new_date_time_features(df_train)\ndf_test = add_new_date_time_features(df_test)","35ecdf1d":"df_train.describe()","eb0dff93":"\ndf_train = df_train[df_train.pickup_longitude.between(df_test.pickup_longitude.min(), df_test.pickup_longitude.max())]\ndf_train = df_train[df_train.pickup_latitude.between(df_test.pickup_latitude.min(), df_test.pickup_latitude.max())]\ndf_train = df_train[df_train.dropoff_longitude.between(df_test.dropoff_longitude.min(), df_test.dropoff_longitude.max())]\ndf_train = df_train[df_train.dropoff_latitude.between(df_test.dropoff_latitude.min(), df_test.dropoff_latitude.max())]","ca508ab7":"df_train.shape","46bced94":"def calculate_abs_different(df):\n    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\ncalculate_abs_different(df_train)\ncalculate_abs_different(df_test)","071d829e":"# Since we are calculating this at New York, we can assign a constant, rather than using a formula\n# longitude = degrees of latitude in radians * 69.172\n#1 degree of longitude = 50 miles\ndef convert_different_miles(df):\n    df['abs_diff_longitude'] = df.abs_diff_longitude*50\n    df['abs_diff_latitude'] = df.abs_diff_latitude*69\nconvert_different_miles(df_train)\nconvert_different_miles(df_test)","34327ea7":"### Angle difference between north, and manhattan roadways\nmeas_ang = 0.506 # 29 degrees = 0.506 radians (https:\/\/en.wikipedia.org\/wiki\/Commissioners%27_Plan_of_1811)\nimport math\n\n\n## adding extra features\ndef add_distance(df):\n    df['Euclidean'] = (df.abs_diff_latitude**2 + df.abs_diff_longitude**2)**0.5 ### as the crow flies  \n    df['delta_manh_long'] = (df.Euclidean*np.sin(np.arctan(df.abs_diff_longitude \/ df.abs_diff_latitude)-meas_ang)).abs()\n    df['delta_manh_lat'] = (df.Euclidean*np.cos(np.arctan(df.abs_diff_longitude \/ df.abs_diff_latitude)-meas_ang)).abs()\n    df['distance'] = df.delta_manh_long + df.delta_manh_lat\n    df.drop(['abs_diff_longitude', 'abs_diff_latitude','Euclidean', 'delta_manh_long', 'delta_manh_lat'], axis=1, inplace=True)\nadd_distance(df_train)\nadd_distance(df_test)","7440521e":"df_train.head()","f38376c1":"def calculate_direction(pickup_lat, pickup_lon, dropoff_lat, dropoff_lon):\n    \"\"\"\n    Return distance along great radius between pickup and dropoff coordinates.\n    \"\"\"\n    #Define earth radius (km)\n    R_earth = 6371\n    #Convert degrees to radians\n    pickup_lat, pickup_lon, dropoff_lat, dropoff_lon = map(np.radians,\n                                                             [pickup_lat, pickup_lon, \n                                                              dropoff_lat, dropoff_lon])\n    #Compute distances along lat, lon dimensions\n    dlat = dropoff_lat - pickup_lat\n    dlon = pickup_lon - dropoff_lon\n    \n    #Compute bearing distance\n    a = np.arctan2(np.sin(dlon * np.cos(dropoff_lat)),np.cos(pickup_lat) * np.sin(dropoff_lat) - np.sin(pickup_lat) * np.cos(dropoff_lat) * np.cos(dlon))\n    return a","52a99bdc":"# We will convert pandas to numpy to get the best performance\ntrain_df['direction'] = calculate_direction(train_df['pickup_latitude'].values, train_df['pickup_longitude'].values, \n                                   train_df['dropoff_latitude'].values , train_df['dropoff_longitude'].values) \ndf_test['direction'] = calculate_direction(df_test['pickup_latitude'].values, df_test['pickup_longitude'].values, \n                                   df_test['dropoff_latitude'].values , df_test['dropoff_longitude'].values) ","c17f10e5":"train_df['pickup_latitude'].apply(lambda x: np.radians(x))\ntrain_df['pickup_longitude'].apply(lambda x: np.radians(x))\ntrain_df['dropoff_latitude'].apply(lambda x: np.radians(x))\ntrain_df['dropoff_longitude'].apply(lambda x: np.radians(x))\n\ndf_test['pickup_latitude'].apply(lambda x: np.radians(x))\ndf_test['pickup_longitude'].apply(lambda x: np.radians(x))\ndf_test['dropoff_latitude'].apply(lambda x: np.radians(x))\ndf_test['dropoff_longitude'].apply(lambda x: np.radians(x))","0b65e67c":"sns.jointplot(x='distance', y='fare_amount', data=df_train)\n# We can see that distance is less than 100 so it makes sense and we can use it","66669272":"# We extracted feature with day, week, month, year so we can remove pickup_datetime\ndf_train.drop(columns=['pickup_datetime'], inplace=True)\n\ny = df_train['fare_amount']\ndf_train = df_train.drop(columns=['fare_amount'])","549abbb3":"df_train.head()","28d96cfc":"from sklearn.model_selection import train_test_split\nimport lightgbm as lgbm\nx_train,x_test,y_train,y_test = train_test_split(df_train,y,random_state=123,test_size=0.1)","2210e45b":"params = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'nthread': 4,\n        'num_leaves': 31,\n        'learning_rate': 0.1,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 10000 ,\n        'bagging_freq': 10,\n        'metric': 'rmse',  \n        'zero_as_missing': True,\n        'num_rounds':50000\n    }","c627c5ab":"train_set = lgbm.Dataset(x_train, y_train, silent=False,categorical_feature=['year','month','day','day_of_week'])\nvalid_set = lgbm.Dataset(x_test, y_test, silent=False,categorical_feature=['year','month','day','day_of_week'])\nmodel = lgbm.train(params, train_set = train_set, num_boost_round=10000,early_stopping_rounds=1000,verbose_eval=500, valid_sets=valid_set)","4ae55836":"df_train.describe()","69afe07a":"test_key = df_test['key']\n\ndf_test.drop(columns=[\"pickup_datetime\",'key'], axis=1, inplace=True)","a52f540c":"prediction = model.predict(df_test, num_iteration = model.best_iteration)      \n","6a82c397":"submission = pd.DataFrame({\n        \"key\": test_key,\n        \"fare_amount\": prediction\n})\n\nsubmission.to_csv('taxi_fare_submission.csv',index=False)","99222346":"<h1>Modeling<\/h1>\n* Since I don't have time and machine so I just choose lightgbm to build model\n* It is fast and good for train a lot of data.\n* If I have time and machine I will try with Random Forest, XGBoost, do something like grid search, random search to get the best model but for this homework I just run it 1 time\n\n","b489e0a3":"<h1>Normalize lat and long<\/h1>\n* After we extract more feature from long and lat, they are less important so we will normalize them to make sure they don't impact too much to our model.\n* We will convert them to radian so their range will shink from range of degrees ( 1 -> 360) to range of radians. ","aa7cd8be":"<h1>Params<\/h1>\nI will choose some default value and modify it later if necessary, for example \n* if I see it overfitting, I will use smaller max_bin and num_leaves\n*  if I see it is low accuracy, I will use larger max_bin and num_leaves\n*  The best way is run it in a grid search with combine of paramenters but it will be slow so I will make it simple .","bb8ffb00":"<h1> Long & Lat<\/h1>\nFor longitude and lattitude, we will have some stategies:\n* We can use the max and min in testing set to make a limit to our training data\n* We can check boundaries for New York on map and filter our data base on min and max of long, lat  \n* We drop invalid long, lat in our data\n-------------------------------------------------------------------------------------------------------------------------------------------------\nIn this situation, we will set the max and min with our data test set because our target is get the best result for test dataset but in real life, I prefer check it more carefuly and make a boundary around area we want to predict ( for example 1000 miles from NYC).\n","340e6a85":"**We need to look at data with fare_amount < 0 ( It looks like a refund when passenger_count = 0). If it's too many we need to concern and try to fill them with some numbers but they are a few number, we can just drop them.**","868de1e2":"<h1>Distance<\/h1>\n\nIt is not too much meaning if we use coordinates for our model, so we will extract distance, it will be more important for our model. <br>\nWe will 3 ways to calculate the distance: Euclidean Distance, Haversine and Manhattan Distance<br>\n1. Euclidean Distance, it's the easiest way to calculate distance between 2 points but it is \"bird fly\" distance so we will can get a big different between our estimate and real life distance\n2. Haversine determines the great-circle distance between two points on a sphere given their longitudes and latitudes so it will give us the better estimate when we use radius of Earth to calculate\n3. Manhattan Distance will calculate distance between 2 point base on blocks so its estimate will be good for us (https:\/\/en.wikipedia.org\/wiki\/Taxicab_geometry)\n\n","f29cdbb6":"<h1> We will extract 1 more feature, it's direction<\/h1>\n* When we extract feture distance, it's estimate and it's not exactly the distance in reality. \n* Direction wil give us more information, if the angle is 45*, it means the distance in real life will be longer because it needs to travel though more blocks in the city.\n![image.png](attachment:image.png)","75d731c1":"We will extract more features from Pickup Datetime to hour, day, month, year, day_of_week"}}