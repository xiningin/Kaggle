{"cell_type":{"66e8367d":"code","f16abcf6":"code","107ddafa":"code","b70c0921":"code","d723b479":"code","77e636ce":"code","325a1928":"code","377e87c7":"code","6dfb5cfb":"code","a98d0d0e":"code","8741563c":"code","6fb67db1":"code","51dedd79":"code","32378158":"code","9d405c84":"code","10339102":"code","3d8c2956":"code","efba510f":"code","1c3ee18e":"code","1e956a4b":"code","250a0a7d":"markdown","aad7a2ac":"markdown","9d9da13c":"markdown","647342dc":"markdown","b383e6cc":"markdown","dba51173":"markdown","7d453fa3":"markdown","4cd8ff66":"markdown","8e0de2f2":"markdown"},"source":{"66e8367d":"import numpy as np\nimport pandas as pd","f16abcf6":"import torch","107ddafa":"data_folder = \"..\/input\/g-research-crypto-forecasting\/\"\n!ls $data_folder","b70c0921":"crypto_df = pd.read_csv(data_folder + 'train.csv')","d723b479":"dataset_ID = [crypto_df[crypto_df.Asset_ID==i] for i in range(14)]","77e636ce":"import time\n\n# auxiliary function, from datetime to timestamp\ntotimestamp = lambda s: np.int32(time.mktime(datetime.strptime(s, \"%d\/%m\/%Y\").timetuple()))","325a1928":"def get_features(df):\n    df = df[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']].copy()\n    # TODO : Try several other features here !\n    return df\n","377e87c7":"from sklearn.model_selection import train_test_split\n\ndef dataset_split(df):\n    dataset = df.dropna() # only use target is not NAN data\n    X, y = get_features(dataset), dataset[\"Target\"]\n    # For train input, only previous data than testset, set shuffle False.\n    X_train, X_test,y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle=False)\n    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5, shuffle=False)\n    X = {\"train\":X_train, \"eval\":X_val, \"test\": X_test}\n    y = {\"train\":y_train, \"eval\":y_val, \"test\": y_test}\n    return X, y","6dfb5cfb":"from xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor","a98d0d0e":"from lightgbm import early_stopping as lgbm_early_stopping\nfrom xgboost.callback import EarlyStopping as xgb_early_stopping\n# early stopping modeling fitting. Model will train until validation socre doesn;t improve.","8741563c":"def lgbm_train(X: dict, y:dict, lr, verbose=0):\n    lgbm_model = LGBMRegressor(learning_rate=lr, verbosity=verbose)\n\n    print(\"\\nLGBM fitting\\n\")\n    lgbm_model.fit(X[\"train\"], y['train'], eval_set=[(X[\"eval\"],y[\"eval\"])], \n                    callbacks=[lgbm_early_stopping(20, first_metric_only= True)])\n    \n    return lgbm_model\n\ndef xgb_train(X: dict, y:dict, lr, verbose=0):\n    xgb_model = XGBRegressor(learning_rate=lr, verbosity=verbose)\n\n    print(\"\\nXGBoost fitting\\n\")\n    xgb_model.fit(X[\"train\"], y['train'], eval_set=[(X[\"eval\"],y[\"eval\"])], \n                   callbacks=[xgb_early_stopping(20, save_best = True)])\n    \n    return xgb_model","6fb67db1":"beg = dataset_ID[0]['timestamp'].iloc[0].astype('datetime64[s]')\nend = dataset_ID[0]['timestamp'].iloc[-1].astype('datetime64[s]')\n\nprint('Asset 0 data goes from ', beg, 'to ', end)","51dedd79":"lgbm_models = []\nxgb_models = []\nfor i in range(14):\n    print(f\"Current Asset: {i}\")\n    dataset = dataset_ID[i]\n    X,y = dataset_split(dataset)\n    # Customize lr.\n    lgbm = lgbm_train(X,y, lr=0.1)\n    xgb = xgb_train(X,y, lr=0.1)\n    lgbm_models.append(lgbm)\n    xgb_models.append(xgb)","32378158":"import matplotlib.pyplot as plt","9d405c84":"# for example, asset = 5 plotting. Customize Possible. [0,13]\nasset_index = 5\n\ndataset = dataset_ID[asset_index]\nX,y = dataset_split(dataset)","10339102":"lgbm_models[asset_index].predict(X[\"test\"].to_numpy())","3d8c2956":"plt.title(\"LGBM Results\")\nplt.plot(y['test'].to_numpy(), label=\"Ground Truth\")\nplt.plot(lgbm_models[asset_index].predict(X[\"test\"].to_numpy()), label=\"Prediction\")\nplt.legend()\nplt.show()","efba510f":"plt.title(\"LGBM Results\")\nplt.plot(y['test'][100:200].to_numpy(), label=\"Ground Truth\")\nplt.plot(lgbm_models[asset_index].predict(X[\"test\"][100:200].to_numpy()), label=\"Prediction\")\nplt.legend()\nplt.show()","1c3ee18e":"plt.title(\"XGBoost Results\")\nplt.plot(y['test'].to_numpy(), label=\"Ground Truth\")\nplt.plot(xgb_models[asset_index].predict(X[\"test\"].to_numpy()), label=\"Prediction\")\nplt.legend()\nplt.show()","1e956a4b":"plt.title(\"XGBoost Results\")\nplt.plot(y['test'][100:200].to_numpy(), label=\"Ground Truth\")\nplt.plot(xgb_models[asset_index].predict(X[\"test\"][100:200].to_numpy()), label=\"Prediction\")\nplt.legend()\nplt.show()","250a0a7d":"# Evaluation","aad7a2ac":"# Utils","9d9da13c":"# Import and Load Dataset","647342dc":"# Asset By Asset, Train and predict","b383e6cc":"### dataset timestamp info","dba51173":"### Zoom In LGBM Results","7d453fa3":"### Zoom In XGBoost Results","4cd8ff66":"# Model train function - Ensemble LGBM & XGboost\n","8e0de2f2":"## Train"}}