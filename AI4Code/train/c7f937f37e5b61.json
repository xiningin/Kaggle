{"cell_type":{"af866868":"code","fddab243":"code","fc1fcb68":"code","de6e7ce6":"code","d0e7cf2d":"code","5c475408":"code","ec4c1cd1":"code","4874b64d":"code","016aca93":"code","e6a1bbca":"code","27d17994":"code","327309a9":"markdown","8bd1e202":"markdown","fc849bd7":"markdown","5b629f38":"markdown","4012c799":"markdown","e34551a5":"markdown","d10df79b":"markdown","78449b85":"markdown","343c5dbf":"markdown"},"source":{"af866868":"!pip install pmdarima","fddab243":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nfrom pmdarima.arima import auto_arima\nfrom sklearn.metrics import mean_squared_log_error\nos.listdir(\"..\/input\/kaggle-days-meetup-dubai-competition\")","fc1fcb68":"# Combine test and train data\ndftr = pd.read_csv(\"..\/input\/kaggle-days-meetup-dubai-competition\/train.csv\")\ndftr[\"date\"] = pd.to_datetime(dftr['date'])\n\n# Remove some noisy data from the start of the dataset\ndftr = dftr[dftr[\"date\"]>\"2014-08-01\"]\nprint(dftr[\"date\"].min(),dftr[\"date\"].max())\ndfte = pd.read_csv(\"..\/input\/kaggle-days-meetup-dubai-competition\/test.csv\")\ndfte[\"price\"] = np.nan\ndfte[\"date\"] = pd.to_datetime(dfte[\"date\"])\ndfte = dfte.sort_values(\"date\")\ndftr = pd.concat([dftr,dfte],ignore_index=True)\n\n# Training and test dataset are on weekly frequency\n# create a master dataset for each day of the time period and interpolate the prices between the weeks reported\ndff = pd.DataFrame({\"date\":pd.date_range(start=dftr[\"date\"].min(),end=dftr.date.max())})\ndff[\"temp\"] = 1\ndff = dff.merge(pd.DataFrame({\"type\":list(dftr[\"type\"].unique()),\\\n                             \"temp\":[1 for t in range(6)]}),how=\"left\",on=[\"temp\"]).drop([\"temp\"],axis=1)\ndff = dff.merge(dftr[[\"date\",\"type\",\"price\"]],how=\"left\",on=[\"date\",\"type\"])\ndff.loc[dff[\"price\"].isnull(),\"interp\"]=True\ndff.loc[dff[\"price\"].notnull(),\"interp\"]=False\ndff[\"price\"] = dff.sort_values('date').groupby([\"type\"])[\"price\"].transform(lambda x: x.interpolate(method='index'))\ndff.loc[dff[\"date\"]>=\"2021-01-01\",\"test\"] =True\ndff.loc[dff[\"date\"]<\"2021-01-01\",\"test\"] =False\ndff.loc[dff[\"test\"],\"price\"] = np.nan\ndff.loc[dff[\"test\"],\"interp\"] = np.nan\ndff1 = dff.set_index([\"date\",\"type\"])[\"price\"].unstack()\ndff1.head()\n","de6e7ce6":"targets = dff1.columns","d0e7cf2d":"# FEATURE ENGINEERING AND MODELING PIPELINE => ARIMA\n\nnon_interpolated_dates = dff[dff[\"interp\"]==False][\"date\"].unique()\n\n############feats\ndff2 = dff1.copy().reset_index()\ndff2[\"month\"] = dff2.date.dt.month\ndff2[\"week\"] = dff2.date.dt.week\ndff2[\"day\"] = dff2.date.dt.day\ndff2[\"day_of_week\"] = dff2.date.dt.dayofweek\nfeat_cols = [\"month\",\"week\",\"day\",\"day_of_week\"]\n############\n\n\n\ndff2 = dff2.set_index(\"date\")\ntestD = dff2.loc[\"2021-01-01\":,].reset_index()\ngts = []\npreds = []\ngts_3 = []\npreds_3 = []\n\n\n\n\nall_mses = []\nfor target_col in targets:\n    mses = []\n    for FOLD in [0,1,2,3,4]:\n        \n        if FOLD == 0:\n            trainD = dff2.loc[:\"2018-01-01\",:].reset_index()\n            validD = dff2.loc[\"2018-01-01\":\"2020-12-31\",].reset_index()\n        if FOLD == 1:\n            trainD = dff2.loc[:\"2019-01-01\",:].reset_index()\n            validD = dff2.loc[\"2019-01-01\":\"2020-12-31\",].reset_index()\n        if FOLD == 2:\n            trainD = dff2.loc[:\"2020-01-01\",:].reset_index()\n            validD = dff2.loc[\"2020-01-01\":\"2020-12-31\",].reset_index()\n        if FOLD == 3:\n            trainD = dff2.loc[:\"2020-06-01\",:].reset_index()\n            validD = dff2.loc[\"2020-06-01\":\"2020-12-31\",].reset_index()\n        if FOLD == 4:\n            trainD = dff2.loc[:\"2020-08-01\",:].reset_index()\n            validD = dff2.loc[\"2020-08-01\":\"2020-12-31\",].reset_index() \n\n\n        model = auto_arima(trainD[target_col], exogenous=trainD[feat_cols], trace=False, error_action=\"ignore\", suppress_warnings=True)\n        model.fit(trainD[target_col], exogenous=trainD[feat_cols])\n        forecast = model.predict(n_periods=len(validD), exogenous=validD[feat_cols])\n        validD[\"Famx_{}\".format(target_col)] = forecast\n        \n        non_interpvalidD = validD[validD[\"date\"].isin(non_interpolated_dates)]\n        val_mse = mean_squared_log_error(non_interpvalidD[target_col].values,\\\n                                         non_interpvalidD[f\"Famx_{target_col}\"].values)\n        gts.extend(non_interpvalidD[target_col].tolist())\n        preds.extend(non_interpvalidD[f\"Famx_{target_col}\"].to_list())\n        if FOLD ==3:\n            gts_3.extend(non_interpvalidD[target_col].tolist())\n            preds_3.extend(non_interpvalidD[f\"Famx_{target_col}\"].to_list())\n            \n        \n        mses.append(val_mse)\n    print(f\"\"\"target {target_col} mean mse {np.mean(mses)} \"\"\",mses)\n    all_mses.append(np.mean(mses))\n    \n    \n    trainD = dff2.loc[:\"2020-12-31\",:].reset_index()\n    model = auto_arima(trainD[target_col], exogenous=trainD[feat_cols], trace=False, error_action=\"ignore\", suppress_warnings=True)\n    model.fit(trainD[target_col], exogenous=trainD[feat_cols])\n    forecast = model.predict(n_periods=len(testD), exogenous=testD[feat_cols])\n    testD[\"Famx_{}\".format(target_col)] = forecast\n        \n# print(f\"****overall rmlse ; {np.mean(mses)}\")\n\nprint(\"Average RMSLE for all folds :{}\".format(mean_squared_log_error(gts,preds)))\n\nprint(\"Average RMLSE for just 2020  :{}\".format(mean_squared_log_error(gts_3,preds_3)))","5c475408":"# Visualizing the forecasts\ntestD[[\"date\"]+[f\"Famx_{t}\" for t in targets]].set_index('date').plot()","ec4c1cd1":"# preparing the outputs in submission format\ntest_dates = dfte[\"date\"].unique()\nsubtestD = testD[testD[\"date\"].isin(test_dates)]\nsubtestD = subtestD[[\"date\"]+[f\"Famx_{t}\" for t in targets]]\nsubtestD.columns = ['date', 'AFRAMAX(110K)', 'LR1(74K)', 'LR2(110K)',\n       'MR(50K)', 'SUEZMAX(160K)', 'VLCC(320K)']\nsubtestD = subtestD.set_index(\"date\").stack().reset_index()\nsubtestD = subtestD.rename(columns={\"level_1\":\"type\",0:\"price\"})\nsubtestD[\"id\"] = subtestD.apply(lambda x: str(x[\"date\"].date())+\"_\"+ x[\"type\"],axis=1)\ndfsam = pd.read_csv(\"..\/input\/kaggle-days-meetup-dubai-competition\/sample_submission.csv\")[[\"id\"]].set_index('id')\ndfsam[\"price\"] = subtestD.set_index(\"id\")[\"price\"]\ndfsam = dfsam.reset_index()\n# dfsam.isnull().sum()\n\n# dfsam.to_csv('submission_1.csv',index=None)\ndfsam.head()\n","4874b64d":"fig, axes = plt.subplots(nrows=2, ncols=1)\ndff1.plot(figsize=(5,6),title=\"price\",ax=axes[0])\ndfks = pd.read_csv('..\/input\/kaggle-days-meetup-dubai-competition\/korea_steel_plate_commodity_price_monthly.csv')\ndfks[\"date\"] = pd.to_datetime(dfks[\"date\"])\n\n\ndfks = dfks.set_index([\"date\"])[[\"price_per_tonne\"]]\ndfks_ = pd.DataFrame({\"date\":pd.date_range(start=\"2014-08-07\",end=\"2021-10-14\")}).set_index('date')\nfor col in dfks.columns:\n    dfks_[col] = dfks[col]\ndfks_.shape,dfks_.isnull().sum()\ndfks_ = dfks_.fillna(method='bfill').fillna(method='ffill')\ndfks_.loc[\"2014-08-07\":][\"price_per_tonne\"].plot(figsize=(5,6),ax=axes[1],title=\"steel price\")\nplt.show()\n\n","016aca93":"import warnings\nwarnings.filterwarnings('ignore')\nfrom scipy.stats import pearsonr\ndfpearson = pd.DataFrame({\"ship_price\":dff1.sum(1)})\ndfpearson[\"steel_price\"] = dfks_[\"price_per_tonne\"]\ndfpearson = dfpearson.loc[:\"2020-12-31\",:]\ntotal_lags = range(1,365*4)\ncorrs=[]\nfor lag in total_lags:\n    dfpearson[\"lag_sp\"] = dfpearson[\"steel_price\"].tshift(lag)\n    corr,_ = pearsonr(dfpearson.dropna()[\"lag_sp\"].values,dfpearson.dropna()[\"ship_price\"])\n    corrs.append(corr)\ndfcorr = pd.DataFrame({\"lag (days)\":(total_lags),\"correlation\":corrs})\nprint(dfcorr.isnull().sum(),dfcorr.shape)\ndfcorr.set_index('lag (days)')[\"correlation\"].plot(title=\"Cross Correlation between ship price and time shifted steel prices\")\nplt.show()","e6a1bbca":"dftemp = dff2[targets].copy()\nfor col in targets:\n#     Here we just copy the patterns from 2.2 years back => around the time of 2019 price recovery\n    dftemp[\"Famx_\"+col] = dftemp[col].tshift(int(365*2.2))\n#     We then apply a small correction factor to the forecast to adjust for recent trends\n    first_day_of_forecast,last_day_of_actual = dftemp.loc[\"2021-01-01\",\"Famx_\"+col],dftemp.loc[\"2020-12-31\",col]\n    diff = last_day_of_actual-first_day_of_forecast\n    dftemp[\"Famx_\"+col] =dftemp[\"Famx_\"+col] -diff\n\ndftemp[[f\"Famx_{t}\" for t in targets]].plot()\n","27d17994":"# preparing the outputs in submission format\ntestD__ = dftemp.loc[\"2021-01-01\":,].reset_index()\ntest_dates = dfte[\"date\"].unique()\nsubtestD = testD__[testD__[\"date\"].isin(test_dates)]\nsubtestD = subtestD[[\"date\"]+[f\"Famx_{t}\" for t in targets]]\n# subtestD = subtestD\nsubtestD.columns = ['date', 'AFRAMAX(110K)', 'LR1(74K)', 'LR2(110K)',\n       'MR(50K)', 'SUEZMAX(160K)', 'VLCC(320K)']\nsubtestD = subtestD.set_index(\"date\").stack().reset_index()\nsubtestD = subtestD.rename(columns={\"level_1\":\"type\",0:\"price\"})\nsubtestD[\"id\"] = subtestD.apply(lambda x: str(x[\"date\"].date())+\"_\"+ x[\"type\"],axis=1)\nsubtestD.head()\n\ndfsam = pd.read_csv(\"..\/input\/kaggle-days-meetup-dubai-competition\/sample_submission.csv\")[[\"id\"]].set_index('id')\ndfsam[\"price\"] = subtestD.set_index(\"id\")[\"price\"]\ndfsam = dfsam.reset_index()\n# dfsam.isnull().sum()\ndfsam.to_csv('submission_6.csv',index=None)\ndfsam.head()\n","327309a9":"##### This plot clearly confirms our theory -  there is a positive correlation between the ship price and ...\n##### and the steel price about 1-2 years before\n##### Now how can we use this info\n##### We see that in 2020, the steel price seems to have hit a slump and then has started on a upward trajectory\n##### Therefore we should be seeing a similar increasing trend in the shipping prices for 2021\n##### We theorized that this drop and recovery of steel prices in 2020 is similar to the pattern observed in 2016-2017\n##### Therefore the trend in prices for 2021 would follow a similar path as the shipping prices in 2018-19\n##### For forecasting ,We then try to replicate the trend  and make some adjustments to it","8bd1e202":"#### Looks decent right? => This is the winning solution","fc849bd7":"##### There seems to be some strong correlation between steel price and ship price. But there is a considerable lag\n##### The steel price drop in 2016 seems to have contributed to the shipping price drop in 2018\n##### To numerically test this hypothesis,....\n##### we calculate the cross correlation between ship prices and steel price over different time shifts\n","5b629f38":"### FEATURE ENGINEERING AND MODELING PIPELINE => ARIMA","4012c799":"#### Visualizing the forecasts","e34551a5":"### Shoulders of giants\n#### 1. Thank you Asyad, and their wonderful team for the business insights that helped us crack this\n#### 2. Thank you Tom Van De Wiele for the mentoring and guidance which was instrumental in our success\n#### 3. Thank you Maria, Pawel, Adam, LogicAI and Kaggle days for arranging this amazing event\n#### 4. Thank you fellow kagglers. It was an honor and true learning experience\n\n###  - Noufal & Dr.Aziza","d10df79b":"### Preparing the outputs in submission format","78449b85":"#### Submission 6 is the winning solution!!","343c5dbf":"This was our submission 1=> which gave us a score of 0.09 => our attempts to include additional features..\n\ndid not improve our validation or leaderboard standing.\n\nSo we decided to change our approach and try to find some interesting data pattern that might help us ...\n\narrive at a reasonable prediction for 2021"}}