{"cell_type":{"79c23927":"code","8fdc5999":"code","f6dac7e8":"code","015bacd3":"code","7da80592":"code","c72c8774":"code","e32a448f":"code","52402932":"code","0de36321":"code","37a32ab4":"code","9238be6c":"code","636d51f3":"code","30c7310a":"code","57db16ca":"code","a4a85ff9":"code","0f4f2612":"code","f4eee55a":"code","5de9c514":"code","3bac3246":"code","dfe393aa":"code","e5a6bac7":"code","623035fa":"code","0e314517":"code","0ce4021f":"code","40828f7c":"code","1aa11b64":"code","f9f5c286":"code","da0e2348":"code","807da44f":"code","3ccf07ba":"code","3dbebee5":"code","6184f975":"code","6567d2c8":"code","80b4aa85":"code","c2d3fc9e":"code","2a85ad9f":"code","abd900ca":"code","c95206e4":"code","00135b3d":"code","89b6cd09":"code","5f36fc1c":"code","712d04e4":"code","9c904733":"code","184d7219":"code","72cffa7a":"code","262f3e57":"markdown","01019f2d":"markdown","435b0b53":"markdown","4b8f9767":"markdown","b1bc795e":"markdown","06c1dd9e":"markdown","cb1b55e0":"markdown","ba378d86":"markdown","843c6f23":"markdown","ebea3e31":"markdown","d4b75c15":"markdown","c5302ed4":"markdown","ac11190b":"markdown","9396a1d0":"markdown","2cbca1fa":"markdown","31b2270b":"markdown","04f79aba":"markdown","dbad636a":"markdown","cab74881":"markdown","dfcab774":"markdown"},"source":{"79c23927":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8fdc5999":"pip install pyforest","f6dac7e8":"pip install sci_analysis","015bacd3":"from pyforest import *\nfrom datetime import datetime\nfrom scipy.stats import norm\nfrom scipy import stats\nfrom scipy.stats import skew\nimport matplotlib.lines as mlines\nimport warnings\nwarnings.filterwarnings('ignore')\nimport plotly.graph_objects as go\nfrom sci_analysis import analyze\nimport pandas_profiling as pp\n\nsns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\npd.options.display.float_format = '{:,.4f}'.format\npd.set_option(\"display.max_columns\", 500)\npd.set_option(\"display.max_rows\", 500)","7da80592":"class color:\n   PURPLE = '\\033[95m'\n   CYAN = '\\033[96m'\n   DARKCYAN = '\\033[36m'\n   BLUE = '\\033[94m'\n   GREEN = '\\033[92m'\n   YELLOW = '\\033[93m'\n   RED = '\\033[91m'\n   BOLD = '\\033[1m'\n   UNDERLINE = '\\033[4m'\n   END = '\\033[0m'","c72c8774":"books = pd.read_csv('..\/input\/goodreadsbooks\/books.csv',error_bad_lines=False)\nbooks.head()","e32a448f":"print(\"no. of rows: \",books.shape[0], \"\\n\"\"no. of columns: \",books.shape[1])","52402932":"books.info()","0de36321":"books.rename(columns={'  num_pages': 'num_pages'},inplace=True)","37a32ab4":"books.columns","9238be6c":"date = books.publication_date.tolist()\nMonth = []\nYear = []\nMonth_Year = []\nfor i in date:\n    a = i.split('\/')\n    a.pop(1)\n    Month.append(int(a[0]))\n    Year.append(a[1])\n    Month_Year.append(a[0]+'\/'+a[1])","636d51f3":"#change to month\nimport datetime\nMonth_name = []\nfor i in Month:\n    a=datetime.date(1900, i, 1).strftime('%B')\n    Month_name.append(a)","30c7310a":"books['Month'] = pd.Series(Month_name, index=books.index)\nbooks['Year'] = pd.Series(Year, index=books.index)\nbooks['Month_Year'] = pd.Series(Month_Year, index=books.index)","57db16ca":"book2 = books.copy()","a4a85ff9":"#Let's create a categorical column for average_rating\n\ndef create_cat(i):\n    if i >= 0 and i <=1:\n        return '0-1'\n    if i >= 1 and i <=2:\n        return '1-2'\n    if i >= 2 and i <=3:\n        return '2-3'\n    if i >= 3 and i <=4:\n        return '3-4'\n    if i >= 4 and i <=5:\n        return '4-5'","0f4f2612":"books['avg_ratings'] = books['average_rating'].apply(create_cat)\nbooks.head(3)","f4eee55a":"#Dividing numeric and categorical columns\n\nnumerical = books.select_dtypes(['float64','int64']).columns.values.tolist()\ncategorical = books.select_dtypes(['object']).columns.values.tolist()\nprint('Numerical Columns\\n\\n',numerical)\nprint()\nprint('Categorical Columns\\n\\n',categorical)\n","5de9c514":"descr_stats = books[numerical].describe().T\ndescr_stats['Variance'] = books[numerical].var()\ndescr_stats['IQR'] = descr_stats['75%']-descr_stats['25%']\ndescr_stats['Range'] = descr_stats['max']-descr_stats['min']\ndescr_stats.rename(columns={'count':'Count',\n                            'mean':'Mean',\n                            'std':'Standard Deviation',\n                            '25%':'Q1 (25%)',\n                            '50%':'Q2 (50%)',\n                            '75%':'Q3 (75%)'},inplace=True)\ndescr_stats.T","3bac3246":"for i in books.describe().columns:\n    a = skew(books[i])\n    if a > 0:\n        print('\\n',i,':\\n\\nSkewnes of the data = {}\\n\\tnot normally distributed.'.format(a))\n    elif a < 0:\n        print('\\n',i,':\\n\\nSkewnes of the data = {}\\n\\tnot normally distributed.'.format(a))\n    else:\n        print('\\n',i,':\\n\\nSkewnes of the data = {}\\n\\tnormally distributed'.format(a))","dfe393aa":"# Are there any duplicates?\ndups = books.duplicated()\nprint('Number of duplicate rows = %d' % (dups.sum()))\nbooks[dups]","e5a6bac7":"#Any missing values?\nbooks.isnull().sum().values.any()","623035fa":"#Let's check the unique values\nfor i in books.columns:\n    print('\\n',i,'\\n\\n',books[i].unique())","0e314517":"print('\\nBooks with authors name as `NOT A BOOK`:',len(books[books['authors']=='NOT A BOOK']))","0ce4021f":"books[books['authors']=='NOT A BOOK'].iloc[:,:3]","40828f7c":"books = books[books['authors']!='NOT A BOOK']","1aa11b64":"print('\\nBooks with total number of pages = 0:',len(books[books['num_pages'] == 0]))","f9f5c286":"books = books[books['num_pages'] != 0]","da0e2348":"print(\"no. of rows after removing bad data: \",books.shape[0], \"\\n\"\"no. of columns after removing bad data: \",books.shape[1])\n\nprint('total no of rows dropped:',11123-books.shape[0],'i.e.',\n      round(((11123-books.shape[0])\/11123)*100,4),'% of the original dataset')","807da44f":"print('\\nBook with maximum rating count=',books['ratings_count'].max())\n(books[books['ratings_count'] == books['ratings_count'].max()])","3ccf07ba":"print('highest average rating = ',books['average_rating'].max())\nprint('lowest average rating =',books['average_rating'].min())","3dbebee5":"books[books['average_rating'] > 5]","6184f975":"books[books['average_rating'] < 0 ]","6567d2c8":"avg_rat = books[books['average_rating'] == books['average_rating'].max()]\nprint('Total number of books with highest average rating = ',len(avg_rat))\navg_rat.iloc[:10,:3]","80b4aa85":"avg_rat = books[books['average_rating'] == books['average_rating'].min()]\nprint('Total number of books with lowest average rating = ',len(avg_rat))\navg_rat.iloc[:10,:3]","c2d3fc9e":"profile = pp.ProfileReport(books)\nprofile.to_file(\"output.html\")","2a85ad9f":"#Identifiying outliers with IQR\nsorted(books)\n\nQ1=books.quantile(0.25)\nQ3=books.quantile(0.75)\nIQR=Q3-Q1\nprint(IQR)","abd900ca":"iqr = ((books < (Q1 - 1.5 * IQR)) |(books > (Q3 + 1.5 * IQR))).any()\niqr = iqr.to_frame().reset_index().rename(columns={'index':'Columns',0:'Outliers'})\noutliers = iqr[iqr['Outliers']==True]\noutliers","c95206e4":"books = books[books['ratings_count']<books.ratings_count.quantile(.95)]","00135b3d":"print(\"no. of rows after outliers treatment: \",books.shape[0], \"\\n\"\"no. of columns after outliers treatment: \",books.shape[1])\n\nprint('total no of rows dropped:',11123-books.shape[0],'i.e.',round(((11123-books.shape[0])\/11047)*100,4),'% of the original dataset')","89b6cd09":"print('\\n\\naverage_rating')\nanalyze(books['average_rating'])\nprint('\\n\\nnum_pages')\nanalyze(books['num_pages'])\nprint('\\n\\nratings_count')\nanalyze(books['ratings_count'])\nprint('\\n\\ntext_reviews_count')\nanalyze(books['text_reviews_count'])","5f36fc1c":"most_readlang = books.language_code.value_counts().to_frame().reset_index()\nmost_readlang.rename(columns={'language_code':'Count','index':'language_code'},inplace=True)\nmost_readlang = most_readlang.iloc[:10,:]\nmost_readlang.sort_values(by='Count',ascending=False)","712d04e4":"plt.figure(figsize =[15,5])\nsns.barplot(most_readlang.language_code,most_readlang.Count,palette='BrBG')\nplt.title('Most read language',fontsize=14);","9c904733":"most_popauth = books.authors.value_counts().to_frame().reset_index()\nmost_popauth.rename(columns={'authors':'Count','index':'authors'},inplace=True)\nmost_popauth = most_popauth.iloc[:10,:]\nmost_popauth.sort_values(by='Count',ascending=False)","184d7219":"plt.figure(figsize =[15,5])\nsns.barplot(most_popauth.authors,most_popauth.Count,palette='BrBG')\nplt.title('Most published author',fontsize=14)\nplt.xticks(rotation=25);","72cffa7a":"month_aurt = pd.crosstab(books.authors,books.Month,margins=True)\nmonth_aurt.sort_values(by='All',ascending=False,axis=0,inplace=True)\nmonth_aurt.sort_values(by='All',ascending=False,axis=1,inplace=True)\nmonth_aurt.drop('All',axis=0,inplace=True)\nmonth_aurt.drop('All',axis=1,inplace=True)\nmonth_aurt = month_aurt.iloc[:10,:]\nmonth_aurt","262f3e57":"**Observations**\n\nThe numeric data is not normally distributed and the columns ratings_count 17.70, text_reviews_count 16.17 are highly skewed.\n\nnum_pages, ratings_count and text_reviews_count have a higher standard deviation and variance; which indicates that the data points are very spread out from the mean, and from one another.\n\nThe categorical data shows the most popular book and author, along with other frequency and columns. It seems that the most popular book is The Brothers Karamazov that shows the frequency of 9 and the most popular author is Stephen King which shows the frequency of 40.","01019f2d":"# Descriptive statistics","435b0b53":"**Observations**\n* The data seems to have no missing values. There are total 11123 rows with total 12 columns with the range index from 0 to 11122.\n* There are 6 numerical columns with 5 int64 values and 1 float64 and 6 categorical (object) values. We can see that the column publication_date has the dtype as object. We'll need to change that to datetime.\n* In the column names, we can see that num_pages is indented.\n* Overall the data looks in order. Memory usage: 1.0 MB","4b8f9767":"**Observations**\n\nWe have filtered out the data that has 0 num_pages leaving us with the dataset that has 11044 of total entries. That means we have dropped about 79 columns from the original dataset i.e. about 0.7102% of the data is dropped.","b1bc795e":"\n# **Observations**\n\nColumns average_rating isbn13 num_pages ratings_count text_reviews_count have outliers.\n\nWe have filtered out the data as per the 95th percentile from ratings_count leaving us with the dataset that has 10491 of total entries. That means we have dropped about 632 columns from the original dataset i.e. about 5.721% of the data is dropped.","06c1dd9e":"# **Stucture of the dataset**","cb1b55e0":"**Observation**\n\nThe highest average rating a book can get is 5.0. There are 22 books that have maximum average rating.\n\nThe lowest average rating a book can get is 0. There are 25 books that have lowest average rating.","ba378d86":"\n**Observations**\n\nWe can conclude from the Shapiro-Wilk test for normality that the data is not normally distributed and is highly skewed. Also the columns have a higher standard deviation and variance; which indicates that the data points are very spread out from the mean, and from one another.","843c6f23":"**Total number of books with highest and lowest average rating**","ebea3e31":"# Bivariate\n\n**Categorical**","d4b75c15":"# Univariate\n**Categorical**","c5302ed4":"**Authors = NOT A BOOK**","ac11190b":"# Univariate\n**Numerical**","9396a1d0":"\n**Observations**\n\nThe data does not have any missing values or duplicate values. Also there aren't any visible anomalies in the values of the dataset.","2cbca1fa":"# **Importing Libraries and dataset**","31b2270b":"\n**Observations**\n\nTwilight by Stephenie Meyer published in 9\/6\/2006 by Little Brown and Company publisher has the highest ratings_count of 4597666.\n\nAverage_rating for this books is 3.5900 and text_review_counts is 94265","04f79aba":"**Book with maximum rating count**","dbad636a":"Bad data check","cab74881":"\n**Observations**\n\nEnglish is by far the most common language the readers prefer to read in.\n\nSpanish is the 3rd popular language after eng-US","dfcab774":"The below lines were skipped from the data while importing due to the following reason:\n\n* Skipping line 3350: expected 12 fields, saw 13\n* Skipping line 4704: expected 12 fields, saw 13\n* Skipping line 5879: expected 12 fields, saw 13\n* Skipping line 8981: expected 12 fields, saw 13"}}