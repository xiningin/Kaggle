{"cell_type":{"888b30f1":"code","59d675db":"code","43d83be0":"code","b652aa2b":"code","7705c211":"code","5ed807fb":"code","458bd759":"code","daa1a167":"code","8b08ab45":"code","1f323121":"code","c25dd6ac":"code","2fb173a9":"code","b4559210":"code","a4452b98":"code","97b05cd4":"code","a6b566cc":"code","9ae3a314":"code","88999092":"code","73eaa79f":"code","c08bdf80":"code","56f890cd":"code","489c5b1a":"code","30977c52":"code","b7fbf958":"code","e3a69b0c":"code","b48e3388":"code","dc5cf13b":"code","ca6df12a":"code","17c5c12b":"code","0217c2c9":"code","f9304bb3":"code","00d53266":"code","ec6374a6":"code","2c6f79a8":"code","c7f64d18":"code","e27a68cc":"code","d1a74261":"code","653d7f61":"code","bb3a5583":"code","2d3a6060":"code","72bc9874":"code","cc7c0f04":"code","70554235":"code","bfc95cc2":"code","ee069f55":"code","bba94a47":"code","803a4ea9":"code","c0853d57":"code","867b8203":"code","51f0d76a":"code","71633fcc":"code","368002b6":"code","a0532510":"code","284f1ec6":"code","79c6f9de":"code","fb873026":"code","b56e3355":"code","05634a36":"code","05d7cca0":"code","053f48e8":"code","15d28345":"code","e060f82b":"code","7e1e5867":"code","9a1a5e4a":"code","415b51f9":"code","b3acad72":"code","c986720e":"code","f0a3fecb":"code","514feddc":"code","27e7f945":"code","a56ea532":"code","fc803de1":"code","a56e2df3":"code","ef854798":"code","9268c1b3":"markdown","7eb7d891":"markdown","736ecbad":"markdown","db7a79c1":"markdown","e0c81363":"markdown","a7a72d8d":"markdown","b0abb75e":"markdown","7d8603d5":"markdown","21348303":"markdown","d145a9b7":"markdown","ce342773":"markdown","659d1857":"markdown","79904f3c":"markdown","9d18f213":"markdown","274111fa":"markdown","30a068d5":"markdown","1cd746f1":"markdown","e0b4a902":"markdown","e286cb10":"markdown","a4ef4969":"markdown","47ffa2b1":"markdown","7ba90468":"markdown","9b66760f":"markdown","084633ec":"markdown","10f26e8a":"markdown","7b3020b7":"markdown","a9e43e3b":"markdown","8b811594":"markdown","a33879bb":"markdown","9aa992fc":"markdown","607fb658":"markdown","8ed3320c":"markdown","3e6529a1":"markdown","903c70de":"markdown","9972293f":"markdown","023bb76e":"markdown"},"source":{"888b30f1":"import numpy as np #Import all necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","59d675db":"train=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")","43d83be0":"train.head()","b652aa2b":"#Cleaning the Id columns because it doesnt have important information.\ntrain=train.drop(\"Id\",axis=1)","7705c211":"train.info()","5ed807fb":"fig=plt.figure(figsize=(8,4),dpi=100)# Draw histogram for target variable\nsns.distplot(train[\"SalePrice\"],hist_kws=dict(edgecolor=\"w\",linewidth=1),bins=25,color=\"r\")\nplt.title(\"Sales data distribution\")","458bd759":"train['saleprice'] = np.log1p(train['SalePrice'])#Use log function in numpy\n\nfig=plt.figure(figsize=(8,4),dpi=100)\nsns.distplot(train[\"saleprice\"],hist_kws=dict(edgecolor=\"w\",linewidth=1),bins=25,color=\"r\")\nplt.title(\"Sales data distribution\")","daa1a167":"#Let's check if the data set has any missing values.\n100*(train.isnull().sum()\/len(train))\ndef missing_values_percent(train):#we can use this function in all dataframes.\n    nan_percent=100*(train.isnull().sum()\/len(train))\n    nan_percent=nan_percent[nan_percent>0].sort_values()\n    return(nan_percent)\n\n","8b08ab45":"nan_percent=missing_values_percent(train)\nnan_percent","1f323121":"#Drawing barplot for these missing values.\nplt.figure(figsize=(12,6))\nsns.barplot(x=nan_percent.index,y=nan_percent)\nplt.xticks(rotation=45)","c25dd6ac":"#determine the missing values that their percentages are between 0 and 5.\nplt.figure(figsize=(12,6))\nsns.barplot(x=nan_percent.index,y=nan_percent)\nplt.xticks(rotation=45)\n\nplt.ylim(0,5)","2fb173a9":"train[train[\"Electrical\"].isnull()]","b4559210":"#can delet this row\ntrain=train.drop(labels=1379,axis=0)","a4452b98":"nan_percent=missing_values_percent(train)#see Electrical has been droped from missing data.\nplt.figure(figsize=(12,6))\nsns.barplot(x=nan_percent.index,y=nan_percent)\nplt.xticks(rotation=45)\n\nplt.ylim(0,5)","97b05cd4":"train[\"MasVnrType\"]=train[\"MasVnrType\"].fillna(\"None\")\ntrain[\"MasVnrArea\"]=train[\"MasVnrArea\"].fillna(0)","a6b566cc":"nan_percent=missing_values_percent(train)#see MasVnrType and MasVnrArea have been droped.\nplt.figure(figsize=(12,6))\nsns.barplot(x=nan_percent.index,y=nan_percent)\nplt.xticks(rotation=45)\n\nplt.ylim(0,5)","9ae3a314":"bsmt_str_cols=[\"BsmtQual\",\"BsmtCond\",\"BsmtFinType1\",\"BsmtExposure\",\"BsmtFinType2\"]\ntrain[bsmt_str_cols]=train[bsmt_str_cols].fillna(\"None\")","88999092":"gar_str_cols=[\"GarageType\",\"GarageFinish\"]#transform the object to None\ntrain[gar_str_cols]=train[gar_str_cols].fillna(\"None\")\n\ngar_num_cols=[\"GarageYrBlt\",\"GarageQual\",\"GarageCond\"]#transform the int or float to 0\ntrain[gar_num_cols]=train[gar_num_cols].fillna(0)","73eaa79f":"nan_percent=missing_values_percent(train)#see MasVnrType and MasVnrArea have been droped.\nplt.figure(figsize=(12,6))\nsns.barplot(x=nan_percent.index,y=nan_percent)\nplt.xticks(rotation=45)","c08bdf80":"train[\"FireplaceQu\"]=train[\"FireplaceQu\"].fillna(\"None\")","56f890cd":"plt.figure(figsize=(8,12))\nsns.boxplot(data=train,x=\"LotFrontage\",y=\"Neighborhood\")\n","489c5b1a":"train.groupby(\"Neighborhood\")[\"LotFrontage\"].mean()#average of Lotfrontage base on each Neighborhood","30977c52":"#replace the average of Lotfrontage base on each Neighborhood with the missing data\ntrain[\"LotFrontage\"]=train.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda val:val.fillna(val.mean()))","b7fbf958":"nan_percent=missing_values_percent(train)#see MasVnrType and MasVnrArea have been droped.\nplt.figure(figsize=(12,6))\nsns.barplot(x=nan_percent.index,y=nan_percent)\nplt.xticks(rotation=45)","e3a69b0c":"train=train.drop([\"Fence\",\"Alley\",\"MiscFeature\",\"PoolQC\"],axis=1)","b48e3388":"train.isnull().sum()#we have no missing data","dc5cf13b":"train.corr()[\"saleprice\"]#determine correlation","ca6df12a":"#we want to determine the relationship between saleprice and yearsold.\ntrain.groupby(\"YrSold\")[\"saleprice\"].mean().plot(color=\"r\")","17c5c12b":"corr = train.corr()#Top 50% Corralation train attributes with sale-price\ntop_feature = corr.index[abs(corr['saleprice']>0.5)]\nplt.subplots(figsize=(12, 8))\ntop_corr = train[top_feature].corr()\nsns.heatmap(top_corr, annot=True)","0217c2c9":"sns.scatterplot(data=train,x=\"OverallQual\",y=\"saleprice\")\n","f9304bb3":"sns.scatterplot(data=train,x=\"GrLivArea\",y=\"saleprice\")\nplt.axhline(y=11.5 , color=\"r\")\nplt.axvline(x=4000,color=\"r\")","00d53266":"train[(train[\"GrLivArea\"]>4000) & (train[\"saleprice\"]>11.5)][[\"GrLivArea\",\"saleprice\"]]#determine the index of outliers","ec6374a6":"index_drop=train[(train[\"GrLivArea\"]>4000) & (train[\"saleprice\"]>11.5)].index#drop the outliers\ntrain=train.drop(index_drop,axis=0)","2c6f79a8":"\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\nfig = plt.figure(figsize=(8,8))\n\nax = Axes3D(fig) \n\nx = train[\"YearBuilt\"]\ny = train[\"FullBath\"]\nz = train[\"OverallQual\"]\n\n\nax.scatter(x, y, z, c=x, marker='o')\nax.set_xlabel('YearBuilt')\nax.set_ylabel('FullBath')\nax.set_zlabel('OverallQual')\n","c7f64d18":"#At first,I tranform MSSubClass to str:\ntrain[\"MSSubClass\"].apply(str)","e27a68cc":"train.select_dtypes(include=\"object\")#seperate the data that are object","d1a74261":"#Divide dataframe to 2 parts(num and str)\ntrain_num=train.select_dtypes(exclude=\"object\")\ntrain_obj=train.select_dtypes(include=\"object\")","653d7f61":"train_obj=pd.get_dummies(train_obj,drop_first=True)#use one-hot encoding to transform str to int and float\ntrain_obj.shape","bb3a5583":"Final_train=pd.concat([train_num,train_obj],axis=1)","2d3a6060":"#Determine the feature and lable\nX=Final_train.drop(\"saleprice\",axis=1)\ny=Final_train[\"saleprice\"]","72bc9874":"#Split the dataset to train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)","cc7c0f04":"#train the model\nfrom sklearn.linear_model import LinearRegression\nmodel=LinearRegression()\nmodel.fit(X_train, y_train)","70554235":"#coeficient matrix\npd.DataFrame(model.coef_,X.columns,columns=[\"coeficient\"])","bfc95cc2":"#predicting test data\ny_pred=model.predict(X_test)","ee069f55":"#evaluating the model\nfrom sklearn import metrics\nMAD=metrics.mean_absolute_error(y_test,y_pred)\nMSE=metrics.mean_squared_error(y_test,y_pred)\nRMSE=np.sqrt(MSE)","bba94a47":"pd.DataFrame(data=[MAD,MSE,RMSE],index=[\"MAD\",\"MSE\",\"RMSE\"],columns=[\"LinearRegression\"])","803a4ea9":"test_residuals=y_test-y_pred#the residuals should be random and close to normal distribution.\nsns.scatterplot(x=y_test,y=y_pred,color=\"r\")\nplt.xlabel('Y-Test')\nplt.ylabel('Y-Pred')","c0853d57":"sns.scatterplot(x=y_test,y=test_residuals,color=\"r\")#test residuals should not show a clear pattern.\nplt.axhline(y=0,color=\"b\",ls=\"--\")\nplt.xlabel('Y-Test')\nplt.ylabel('residuals')","867b8203":"from sklearn.linear_model import Ridge","51f0d76a":"ridge_model=Ridge(alpha=10)","71633fcc":"ridge_model.fit(X_train,y_train)","368002b6":"y_pred=ridge_model.predict(X_test)","a0532510":"from sklearn import metrics\nMAD3=metrics.mean_absolute_error(y_test,y_pred)\nMSE3=metrics.mean_squared_error(y_test,y_pred)\nRMSE3=np.sqrt(MSE3)","284f1ec6":"data={'LinearRegression':[MAD,MSE,RMSE],\"Ridge regression\":[MAD3,MSE3,RMSE3]}\npd.DataFrame(data,index=[\"MAD\",\"MSE\",\"RMSE\"])","79c6f9de":"from sklearn.linear_model import RidgeCV","fb873026":"from sklearn.linear_model import RidgeCV","b56e3355":"ridge_cv_model=RidgeCV(alphas=(0.1, 1.0, 10.0),scoring=\"neg_mean_absolute_error\")","05634a36":"ridge_cv_model.fit(X_train,y_train)","05d7cca0":"ridge_cv_model.alpha_","053f48e8":"y_pred_ridge=ridge_cv_model.predict(X_test)","15d28345":"from sklearn import metrics\nMAD4=metrics.mean_absolute_error(y_test,y_pred_ridge)\nMSE4=metrics.mean_squared_error(y_test,y_pred_ridge)\nRMSE4=np.sqrt(MSE4)","e060f82b":"data={'LinearRegression':[MAD,MSE,RMSE],\"Ridge regression\":[MAD3,MSE3,RMSE3],\"Ridgecv\":[MAD4,MSE4,RMSE4]}\npd.DataFrame(data,index=[\"MAD\",\"MSE\",\"RMSE\"])","7e1e5867":"from sklearn.linear_model import LassoCV","9a1a5e4a":"lasso_cv_model=LassoCV(eps=0.1,n_alphas=100,cv=5)","415b51f9":"lasso_cv_model.fit(X_train,y_train)","b3acad72":"lasso_cv_model.alpha_","c986720e":"y_pred_lasso=lasso_cv_model.predict(X_test)","f0a3fecb":"from sklearn import metrics\nMAD5=metrics.mean_absolute_error(y_test,y_pred_lasso)\nMSE5=metrics.mean_squared_error(y_test,y_pred_lasso)\nRMSE5=np.sqrt(MSE5)","514feddc":"data={'LinearRegression':[MAD,MSE,RMSE],\"Ridge regression\":[MAD3,MSE3,RMSE3],\"Ridgecv\":[MAD4,MSE4,RMSE4],\"Lassocv\":[MAD5,MSE5,RMSE5]}\npd.DataFrame(data,index=[\"MAD\",\"MSE\",\"RMSE\"])","27e7f945":"from sklearn .linear_model import ElasticNetCV\nelastic_model=ElasticNetCV(l1_ratio=[0.1,0.5,0.7,0.9,0.95,0.99,1],cv=5,max_iter=100000)","a56ea532":"elastic_model.fit(X_train,y_train)\nelastic_model.l1_ratio_","fc803de1":"y_pred_elastic=elastic_model.predict(X_test)","a56e2df3":"from sklearn import metrics\nMAD6=metrics.mean_absolute_error(y_test,y_pred_elastic)\nMSE6=metrics.mean_squared_error(y_test,y_pred_elastic)\nRMSE6=np.sqrt(MSE6)","ef854798":"data={'LinearRegression':[MAD,MSE,RMSE],\"Ridge regression\":[MAD3,MSE3,RMSE3],\"Ridgecv\":[MAD4,MSE4,RMSE4],\"Lassocv\":[MAD5,MSE5,RMSE5],\"elasticnet\":[MAD6,MSE6,RMSE6]}\npd.DataFrame(data,index=[\"MAD\",\"MSE\",\"RMSE\"])","9268c1b3":"# **Exploratory data analysis**","7eb7d891":"### **Categorical data**","736ecbad":"# we see that we have 4 *outliers* which we decide about them.","db7a79c1":"**For linear regression it is a good idea to evaluate residuals(y- \u0302y)**\n","e0c81363":"**Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values to be far away from the actual values.**","a7a72d8d":"# **LinearRegression**","b0abb75e":"**From the below boxplot we understand that any neighborhood have specefic LotFrontage distribution.So we can replace the average of Lotfrontage base on each Neighborhood with the missing data.**","7d8603d5":"# **Elastic Net**","21348303":" ### **As we see ,the residuals show a clear pattern.so we can say,linear regression is not suitable for this model.**","d145a9b7":"**This dataset have 1460 rows and 81 columns.The SalePrice is the target variable that I am trying to predict.At first I should import all necessary libraries and then read and import the dataset:**","ce342773":"### Further, I drew the 3D plot to determine the relation between the saleprice, overallqual and year built. The below chart shows that the newer houses have the more overalqual but we can not say this about fullbath.","659d1857":" **After import the data,data overview and exploratory data analysis which are shown,we should determine the feature and lable.**","79904f3c":"**This is not a very big data and we do not have too many features. Thus, we have chance to plot most of them and reach some useful analytical results. Drawing charts and examining the data before applying a model is a very good practice because we may detect some possible outliers or decide to do normalization.**","9d18f213":"# **Lasso regression**","274111fa":"## **Ridgecv(cross validation)**","30a068d5":"**Elastic net is a popular type of regularized linear regression that combines two popular penalties, specifically the L1 and L2 penalty functions. ... Elastic Net is an extension of linear regression that adds regularization penalties to the loss function during training.**","1cd746f1":"**We see that the customer should pay more for high overallqual and they dont prefer to buy a house with the quality under 3.**","e0b4a902":"**As we see,we have only 4 features that have missing data.As the percentage of missing datas in these features are above 80%,the best way is to drop them from the dataframe.**","e286cb10":"## NOW I want to plot the 2 top features:","a4ef4969":"## **In this step we want to make a decision about our missing data**","47ffa2b1":"Ridge regression","7ba90468":"**In this step,I want to encoding str to int.**","9b66760f":"## **Residuals**","084633ec":"### **Target Value**","10f26e8a":"**Linear regression is probably one of the most important and widely used regression techniques. It\u2019s among the simplest regression methods. One of its main advantages is the ease of interpreting results.\nLinear regression performs the task to predict a dependent variable value (y) based on a given independent variable (x). So, this regression technique finds out a linear relationship between x (input) and y(output). Hence, the name is Linear Regression.**\n**In this step we want to predict our target valuable (saleprice) based on our features.**","7b3020b7":"### As we see ten features have the most value of correlation,**overallqual** with alomost 80% has the most value of correlation and the second one is Grlivearea with almost 70%.","a9e43e3b":"## Now,we have no missing data.","8b811594":"**Lasso regression is a regularization technique. It is used over regression methods for a more accurate prediction. This model uses shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters)**","a33879bb":"### We see that the price is decreasing between 2006 to 2010,The highest saleprice belongs to 2007 then we have the sharp decreasing in 2008,I think it is weired because the price increases mainly over the years.","9aa992fc":"# **From my opinin,as the residuals show the clear pattern in linear regression,according to the RMSE error the ridgecv is the best method in this model.**","607fb658":"**As we see the most amount of prices are between 100000 to 300000.This SalePrice distribution is not very normal so I use log for that to make more normal distribution.**","8ed3320c":" **In this notebook,I try to analysis house price dataset and predict the house prices by regression methods.**\n **I start with introdusing the dataset,then I do data analysis and house price prediction step by step.**","3e6529a1":"**Cross-validation** is a statistical method used to estimate the skill of machine learning models.\n\nIt is commonly used in applied machine learning to compare and select a model for a given predictive modeling problem because it is easy to understand, easy to implement, and results in skill estimates that generally have a lower bias than other methods.","903c70de":"# **Ridge regression**","9972293f":"# **Import the dataset:**","023bb76e":"### **Checking Missing Data**\n**Before I continue,I want to introduce the missing value briefly.The real-world data often has a lot of missing values. The cause of missing values can be data corruption or failure to record data.\nThe handling of missing data is very important during the preprocessing of the dataset as many machine learning algorithms do not support missing values.**\n**Now I want to identify the rows with the most number of missing values and drop or transform them.**"}}