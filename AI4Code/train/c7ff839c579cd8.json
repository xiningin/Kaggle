{"cell_type":{"33931cff":"code","295e7e9d":"code","ad1ab9c6":"code","20026229":"code","75ab277d":"code","a153d550":"code","6d07327b":"code","14e9f2d8":"code","2e527910":"code","b12d05ac":"code","90f9a6f5":"code","d3f3b289":"code","3f7628c8":"code","34018d19":"code","1d1e0f64":"code","623d793a":"code","c0fdf669":"code","788ec578":"code","5c8cc490":"code","89471068":"code","1f867d94":"code","6952feae":"code","39f59a73":"code","52a7b8a0":"code","0c2bf3a4":"code","e7567be9":"code","d1010774":"code","bc0dcfa6":"code","ca82c886":"code","05eedfe7":"code","26aefc1d":"code","2a0d4295":"code","85276e4f":"code","c2bd5b42":"code","7566b79f":"code","e98058f0":"code","b30595f4":"code","3f9db7c0":"code","369bfa62":"markdown","e3948647":"markdown","434e93ec":"markdown","47fb46e5":"markdown","817d3acd":"markdown","a457dfbb":"markdown","9e8810d1":"markdown","b2d27e25":"markdown","b9430168":"markdown","2722c540":"markdown","c5b085b2":"markdown","139207c5":"markdown","0c8769df":"markdown","bf7ec2e2":"markdown","5a3e6003":"markdown","8986e4d7":"markdown","5893b7ae":"markdown","97fe4f0a":"markdown","3d7368cb":"markdown","674b6bb8":"markdown","42b5e791":"markdown","75ceb623":"markdown"},"source":{"33931cff":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.rcParams[\"figure.figsize\"] = (8, 8)\nplt.rcParams[\"figure.dpi\"] = 125\nplt.rcParams[\"font.size\"] = 14\nplt.rcParams['font.family'] = ['sans-serif']\nplt.rcParams['font.sans-serif'] = ['DejaVu Sans']\nplt.style.use('ggplot')\nsns.set_style(\"whitegrid\", {'axes.grid': False})\nplt.rcParams['image.cmap'] = 'gray' # grayscale looks better\nfrom itertools import cycle\nprop_cycle = plt.rcParams['axes.prop_cycle']\ncolors = prop_cycle.by_key()['color']","295e7e9d":"# tests help notebooks stay managable\nimport doctest\nimport copy\nimport functools\n\ndef autotest(func):\n    globs = copy.copy(globals())\n    globs.update({func.__name__: func})\n    doctest.run_docstring_examples(\n        func, globs, verbose=True, name=func.__name__)\n    return func","ad1ab9c6":"from pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport os\nfrom skimage.io import imread as imread\nfrom skimage.util import montage\nfrom itertools import product\nfrom tqdm import tqdm_notebook\nfrom IPython.display import clear_output","20026229":"data_dir = Path('..') \/ 'input' \/ 'digit-recognizer'\n# Load the data\ntrain = pd.read_csv(data_dir \/ \"train.csv\")\ntest = pd.read_csv(data_dir \/ \"test.csv\")\ny_train = train[\"label\"]\n# Drop 'label' column\nX_train = train.drop(labels = [\"label\"],axis = 1) \n# Normalize the data\nX_train = X_train \/ 255.0\ntest = test \/ 255.0\n# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\nX_train = X_train.values.reshape(-1,28,28,1)\nX_test = test.values.reshape(-1,28,28,1)\ndel train\ndel test\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape)","75ab277d":"img_idx = np.random.choice(range(X_train.shape[0]), size=1)\nplt.matshow(X_train[img_idx, :, :, 0][0])","a153d550":"@autotest\ndef cut_jigsaw(\n    in_image, # type: np.ndarray\n    x_wid, # type: int\n    y_wid,# type: int\n    gap=False,\n    jitter=False,\n    jitter_dim=None, # type: Optional[int]\n):\n    # type: (...) -> List[np.ndarray]\n    \"\"\"Cuts the image into little pieces\n    :param in_image: the image to cut-apart\n    :param x_wid: the size of the piece in x\n    :param y_wid: the size of the piece in y\n    :param gap: if there is a gap between tiles\n    :param jitter: if the positions should be moved around\n    :param jitter_dim: amount to jitter (default is x_wid or y_wid\/2)\n    :return : a 4D array with tiles x x_wid x y_wid * d\n    Examples\n    >>> test_image = np.arange(20).reshape((4, 5)).astype(int)\n    >>> test_image\n    array([[ 0,  1,  2,  3,  4],\n           [ 5,  6,  7,  8,  9],\n           [10, 11, 12, 13, 14],\n           [15, 16, 17, 18, 19]])\n    >>> cut_jigsaw(test_image, 2, 2, False, False)\n    array([[[ 0,  1],\n            [ 5,  6]],\n    <BLANKLINE>\n           [[ 2,  3],\n            [ 7,  8]],\n    <BLANKLINE>\n           [[10, 11],\n            [15, 16]],\n    <BLANKLINE>\n           [[12, 13],\n            [17, 18]]])\n    >>> cut_jigsaw(test_image, 2, 2, True, False)\n    array([[[ 0,  1],\n            [ 5,  6]],\n    <BLANKLINE>\n           [[ 3,  4],\n            [ 8,  9]],\n    <BLANKLINE>\n           [[10, 11],\n            [15, 16]],\n    <BLANKLINE>\n           [[13, 14],\n            [18, 19]]])\n    >>> np.random.seed(0)\n    >>> cut_jigsaw(test_image, 2, 2, True, True, 1)\n    array([[[ 1,  2],\n            [ 6,  7]],\n    <BLANKLINE>\n           [[ 7,  8],\n            [12, 13]],\n    <BLANKLINE>\n           [[ 5,  6],\n            [10, 11]],\n    <BLANKLINE>\n           [[ 7,  8],\n            [12, 13]]])\n    \"\"\"\n    if len(in_image.shape)==2:\n        in_image = np.expand_dims(in_image, -1)\n        expand = True\n    else:\n        expand = False\n    x_size, y_size, d_size = in_image.shape\n    out_tiles = []\n    x_chunks = x_size\/\/x_wid\n    y_chunks = y_size\/\/y_wid\n    out_tiles = np.zeros((x_chunks*y_chunks, x_wid, y_wid, d_size), dtype=in_image.dtype)\n    if gap:\n        # we calculate the maximum gap and \n        x_gap = x_size-x_chunks*x_wid\n        y_gap = y_size-y_chunks*y_wid\n    else:\n        x_gap, y_gap = 0, 0\n    x_jitter = x_wid\/\/2 if jitter_dim is None else jitter_dim\n    y_jitter = y_wid\/\/2 if jitter_dim is None else jitter_dim\n    for idx, (i, j) in enumerate(product(range(x_chunks), range(y_chunks))):\n        x_start = i*x_wid+min(x_gap, i)\n        y_start = j*y_wid+min(y_gap, j)\n        if jitter:\n            x_range = max(x_start-x_jitter, 0), min(x_start+x_jitter+1, x_size-x_wid)\n            y_range = max(y_start-y_jitter, 0), min(y_start+y_jitter+1, y_size-y_wid)\n            \n            x_start = np.random.choice(range(*x_range)) if x_range[1]>x_range[0] else x_start\n            y_start = np.random.choice(range(*y_range)) if y_range[1]>y_range[0] else y_start\n            \n        out_tiles[idx, :, :, :] = in_image[x_start:x_start+x_wid, y_start:y_start+y_wid, :]\n    \n    return out_tiles[:, :, :, 0] if expand else out_tiles\n                ","6d07327b":"@autotest\ndef jigsaw_to_image(\n    in_tiles, # type: np.ndarray\n    out_x, # type: int\n    out_y, # type: int\n    gap=False\n):\n    # type: (...) -> np.ndarray\n    \"\"\"Reassembles little pieces into an image\n    :param in_tiles: the tiles to reassemble\n    :param out_x: the size of the image in x (default is calculated automatically)\n    :param out_y: the size of the image in y\n    :param gap: if there is a gap between tiles\n    :return : an image from the tiles\n    Examples\n    >>> test_image = np.arange(20).reshape((4, 5)).astype(int)\n    >>> test_image\n    array([[ 0,  1,  2,  3,  4],\n           [ 5,  6,  7,  8,  9],\n           [10, 11, 12, 13, 14],\n           [15, 16, 17, 18, 19]])\n    >>> js_pieces = cut_jigsaw(test_image, 2, 2, False, False)\n    >>> jigsaw_to_image(js_pieces, 4, 5)\n    array([[ 0,  1,  2,  3,  0],\n           [ 5,  6,  7,  8,  0],\n           [10, 11, 12, 13,  0],\n           [15, 16, 17, 18,  0]])\n    >>> js_gap_pieces = cut_jigsaw(test_image, 2, 2, True, False)\n    >>> jigsaw_to_image(js_gap_pieces, 4, 5, True)\n    array([[ 0,  1,  0,  3,  4],\n           [ 5,  6,  0,  8,  9],\n           [10, 11,  0, 13, 14],\n           [15, 16,  0, 18, 19]])\n    >>> np.random.seed(0)\n    >>> js_gap_pieces = cut_jigsaw(test_image, 2, 2, False, True)\n    >>> jigsaw_to_image(js_gap_pieces, 4, 5, False)\n    array([[ 1,  2,  6,  7,  0],\n           [ 6,  7, 11, 12,  0],\n           [ 6,  7,  7,  8,  0],\n           [11, 12, 12, 13,  0]])\n    \"\"\"\n    if len(in_tiles.shape)==3:\n        in_tiles = np.expand_dims(in_tiles, -1)\n        expand = True\n    else:\n        expand = False\n    tile_count, x_wid, y_wid, d_size = in_tiles.shape\n    x_chunks = out_x\/\/x_wid\n    y_chunks = out_y\/\/y_wid\n    out_image = np.zeros((out_x, out_y, d_size), dtype=in_tiles.dtype)\n    \n    if gap:\n        x_gap = out_x-x_chunks*x_wid\n        y_gap = out_y-y_chunks*y_wid\n    else:\n        x_gap, y_gap = 0, 0\n        \n    for idx, (i, j) in enumerate(product(range(x_chunks), range(y_chunks))):\n        x_start = i*x_wid+min(x_gap, i)\n        y_start = j*y_wid+min(y_gap, j)\n        out_image[x_start:x_start+x_wid, y_start:y_start+y_wid] = in_tiles[idx, :, :]\n    \n    return out_image[:, :, 0] if expand else out_image\n    \n    \n    ","14e9f2d8":"TILE_X = 9\nTILE_Y = 9\nJITTER_SIZE = 3\nTRAIN_TILE_COUNT = 2**14\nVALID_TILE_COUNT = 2**11\nKEEP_RANDOM_PERM = 200\nLATENT_SIZE = 8\nBIG_LATENT_SIZE = 64\n","2e527910":"fig, m_axs = plt.subplots(4, 11, figsize=(50, 10))\nfor img_idx, c_axs in enumerate(m_axs, 1):\n    c_axs[0].imshow(X_train[img_idx, :, :, 0])\n    c_axs[0].set_title('Input')\n    out_tiles = cut_jigsaw(X_train[img_idx, :, :], TILE_X, TILE_Y, gap=False) \n    for k, c_ax in zip(range(out_tiles.shape[0]), c_axs[1:]):\n        c_ax.matshow(out_tiles[k, :, :, 0])\n    recon_img = jigsaw_to_image(out_tiles, X_train.shape[1], X_train.shape[2])\n    c_axs[-1].imshow(recon_img[:, :, 0])\n    c_axs[-1].set_title('Reconstruction')","b12d05ac":"from itertools import permutations\nall_perm = np.array(list(permutations(range(out_tiles.shape[0]), out_tiles.shape[0])))\nprint('Permutation count:' , len(all_perm))\n\nnp.random.seed(2019)\n# first one is always unmessed up\nkeep_perm = all_perm[0:1, :].tolist()+all_perm[np.random.choice(range(1, len(all_perm)), KEEP_RANDOM_PERM-1), :].tolist()","90f9a6f5":"fig, m_axs = plt.subplots(5, 5, figsize=(15, 25))\nfor i, c_axs in enumerate(m_axs.T):\n    out_tiles = cut_jigsaw(X_train[8, :, :, 0], TILE_X, TILE_Y, gap=False, jitter=i>0, jitter_dim=JITTER_SIZE) \n    for j, (c_ax, c_perm) in enumerate(zip(c_axs, keep_perm)): \n        scrambled_tiles = out_tiles[c_perm]\n        recon_img = jigsaw_to_image(scrambled_tiles, X_train.shape[1], X_train.shape[2])\n        c_ax.imshow(recon_img)\n        c_ax.set_title('Permutation:#{}\\nJitter:{}'.format(j, i>0))\n        c_ax.axis('off')","d3f3b289":"out_tiles = cut_jigsaw(X_train[8, :, :], TILE_X, TILE_Y, gap=False) \n\ndef _generate_batch(in_idx, is_valid=False):\n    np.random.seed(in_idx)\n    if is_valid:\n        img_ds = X_test\n    else:\n        img_ds = X_train\n    img_idx = np.random.choice(range(img_ds.shape[0]))\n    out_tiles = cut_jigsaw(img_ds[img_idx, :, :], TILE_X, TILE_Y, gap=True, jitter=JITTER_SIZE>0, jitter_dim=JITTER_SIZE) \n    perm_idx = np.random.choice(range(len(keep_perm)))\n    c_perm = keep_perm[perm_idx]\n    return out_tiles[c_perm], perm_idx\n\ndef make_tile_group(tile_count, is_valid=False):\n    c_tiles = np.zeros((tile_count,)+out_tiles.shape, dtype='float32')\n    c_perms = np.zeros((tile_count,), dtype='int')\n    for i in tqdm_notebook(range(tile_count)):\n        # should be parallelized\n        c_tiles[i], c_perms[i] = _generate_batch(i, is_valid=is_valid)\n    return c_tiles, c_perms\ntrain_tiles, train_perms = make_tile_group(TRAIN_TILE_COUNT)\nvalid_tiles, valid_perms = make_tile_group(VALID_TILE_COUNT, is_valid=True)","3f7628c8":"from keras import models, layers\ntile_encoder = models.Sequential(name='TileEncoder')\n# we use None to make the model more usuable later\ntile_encoder.add(layers.BatchNormalization(input_shape=(None, None)+(train_tiles.shape[-1],)))\ntile_encoder.add(layers.Conv2D(8, (3,3), padding='same', activation='linear'))\ntile_encoder.add(layers.BatchNormalization())\ntile_encoder.add(layers.MaxPool2D(2,2))\ntile_encoder.add(layers.LeakyReLU(0.1))\ntile_encoder.add(layers.Conv2D(16, (3,3), padding='same', activation='linear'))\ntile_encoder.add(layers.BatchNormalization())\ntile_encoder.add(layers.MaxPool2D(2,2))\ntile_encoder.add(layers.LeakyReLU(0.1))\ntile_encoder.add(layers.Conv2D(32, (2,2), padding='valid', activation='linear'))\ntile_encoder.add(layers.BatchNormalization())\ntile_encoder.add(layers.LeakyReLU(0.1))\ntile_encoder.add(layers.Conv2D(LATENT_SIZE, (1,1), activation='linear'))\ntile_encoder.add(layers.BatchNormalization())\ntile_encoder.add(layers.LeakyReLU(0.1))\nclear_output() # some annoying loading\/warnings come up","34018d19":"tile_encoder.summary()","1d1e0f64":"print('Model Input Shape:', train_tiles.shape[2:], \n      '-> Model Output Shape:', tile_encoder.predict(np.zeros((1,)+train_tiles.shape[2:])).shape[1:])","623d793a":"big_in = layers.Input(train_tiles.shape[1:], name='All_Tile_Input')\nfeat_vec = []\nfor k in range(train_tiles.shape[1]):\n    lay_x = layers.Lambda(lambda x: x[:, k], name='Select_{}_Tile'.format(k))(big_in)\n    feat_x = tile_encoder(lay_x)\n    feat_vec += [layers.GlobalAvgPool2D()(feat_x)]\nfeat_cat = layers.concatenate(feat_vec)\nfeat_dr = layers.Dropout(0.5)(feat_cat)\nfeat_latent = layers.Dense(BIG_LATENT_SIZE)(feat_dr)\nfeat_latent_dr = layers.Dropout(0.5)(feat_latent)\nout_pred = layers.Dense(KEEP_RANDOM_PERM, activation='softmax')(feat_latent_dr)\nbig_model = models.Model(inputs=[big_in], outputs=[out_pred])\nbig_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy', 'sparse_top_k_categorical_accuracy'])","c0fdf669":"from keras.utils.vis_utils import model_to_dot\nfrom IPython.display import Image\ndot_model = model_to_dot(big_model, show_shapes=True)\ndot_model.set_rankdir('LR')\nImage(dot_model.create_png())","788ec578":"reversed_keep_perm = [[c_dict[j] for j in range(out_tiles.shape[0])]\n                      for c_dict in [{j: i for i, j in enumerate(c_perm)}\n                                     for c_perm in keep_perm]]\nfor i in range(3):\n    print('forward', keep_perm[i], 'reversed', reversed_keep_perm[i])","5c8cc490":"def show_model_output(image_count=4, perm_count=3): \n    fig, m_axs = plt.subplots(image_count, perm_count+1, figsize=(3*(perm_count+1), 3*image_count))\n    [c_ax.axis('off') for c_ax in m_axs.flatten()]\n    for img_idx, c_axs in enumerate(m_axs):\n        img_idx = np.random.choice(range(X_train.shape[0]))\n        perm_idx = np.random.choice(range(len(keep_perm)))\n        c_axs[0].imshow(X_train[img_idx, :, :, 0])\n        \n        c_axs[0].set_title('Input #{}'.format(perm_idx))\n        # generate tiles\n        out_tiles = cut_jigsaw(X_train[img_idx, :, :], TILE_X, TILE_Y, gap=True, jitter=JITTER_SIZE>0, jitter_dim=JITTER_SIZE)\n        # scramble tiles\n        \n        c_perm = keep_perm[perm_idx]\n        scr_tiles = out_tiles[c_perm]\n        # get model prediction\n        out_pred = big_model.predict(np.expand_dims(scr_tiles, 0))[0]\n        for c_ax, k_idx in zip(c_axs[1:], np.argsort(-1*out_pred)):\n            pred_rev_perm = reversed_keep_perm[k_idx]\n            recon_img = jigsaw_to_image(scr_tiles[pred_rev_perm], X_train.shape[1], X_train.shape[2])\n            c_ax.imshow(recon_img[:, :, 0])\n            c_ax.set_title('Pred: #{} ({:2.2%})'.format(k_idx, out_pred[k_idx]))\nshow_model_output()","89471068":"fit_results = big_model.fit(train_tiles, train_perms, \n                            validation_data=(valid_tiles, valid_perms),\n                                 batch_size=128,\n                                 epochs=40)\nclear_output()","1f867d94":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\nax1.semilogy(fit_results.history['loss'], label='Training')\nax1.semilogy(fit_results.history['val_loss'], label='Validation')\nax1.legend()\nax1.set_title('Loss')\nax2.plot(fit_results.history['sparse_categorical_accuracy'], label='Training')\nax2.plot(fit_results.history['val_sparse_categorical_accuracy'], label='Validation')\nax2.legend()\nax2.set_title('Accuracy')\nax2.set_ylim(0, 1)","6952feae":"show_model_output(image_count=10, perm_count=4)","39f59a73":"conv_weight_dict = {(idx, k.name): k.get_weights() for idx, k in enumerate(tile_encoder.layers) if isinstance(k, layers.Conv2D)}\nprint(conv_weight_dict.keys())\nfig, m_axs = plt.subplots(2, 2, figsize=(20, 20))\nfor c_ax, ((idx, lay_name), [W, b]) in zip(m_axs.flatten(), conv_weight_dict.items()):\n    c_ax.set_title('{} #{}\\n{}'.format(lay_name, idx, W.shape))\n    flat_W = W.reshape((W.shape[0], W.shape[1], -1)).swapaxes(0, 2).swapaxes(1,2)\n    if flat_W.shape[1]>1 or flat_W.shape[2]>1:\n        pad_W = np.pad(flat_W, [(0, 0), (1, 1), (1,1)], mode='constant', constant_values=np.NAN)\n        pad_W = montage(pad_W, fill=np.NAN, grid_shape=(W.shape[2], W.shape[3]))\n    else:\n        pad_W = W[0, 0]\n    c_ax.imshow(pad_W, vmin=-1, vmax=1, cmap='RdBu')\n    ","52a7b8a0":"gp_outputs = []\nfor k in tile_encoder.layers:\n    if isinstance(k, layers.LeakyReLU):\n        c_output = k.get_output_at(0)\n        c_smooth = layers.AvgPool2D((2, 2))(c_output)\n        c_gp = layers.GlobalMaxPool2D(name='GP_{}'.format(k.name))(c_smooth)\n        gp_outputs += [c_gp]\nactivation_tile_encoder = models.Model(inputs = tile_encoder.inputs, \n                                       outputs = gp_outputs)\nactivation_maps = dict(zip(activation_tile_encoder.output_names, activation_tile_encoder.predict(X_train, batch_size=128, verbose=True)))\n\nfor k, v in activation_maps.items():\n    print(k, v.shape)","0c2bf3a4":"keep_top_n = 5\nfig, m_axs = plt.subplots(1, len(activation_maps), figsize=(20, 20))\nfor c_ax, (k, v) in zip(m_axs.T, activation_maps.items()):\n    c_ax.set_title(k)\n    active_rows = []\n    for i in range(v.shape[1]):\n        top_idx = np.argsort(-np.abs(v[:, i]))[:keep_top_n]\n        active_rows += [X_train[top_idx, :, :, 0]]\n    c_ax.imshow(montage(np.concatenate(active_rows, 0), grid_shape=(v.shape[1], keep_top_n), padding_width=1))\n    c_ax.axis('off')","e7567be9":"print(X_train[0].shape, '->', tile_encoder.predict(X_train[0:1]).shape)","d1010774":"img_in = layers.Input(X_train.shape[1:])\nfull_feat_mat = tile_encoder(img_in)\ngap_out = layers.GlobalAvgPool2D()(full_feat_mat)\nimage_encoder = models.Model(inputs=[img_in], outputs=[gap_out], name='EncodeImage')\nimage_encoder.summary()","bc0dcfa6":"X_features = image_encoder.predict(X_train, batch_size=128)","ca82c886":"from sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, perplexity=40, verbose=2, n_iter=250, early_exaggeration=1)\nX_tsne = tsne.fit_transform(X_features)\nclear_output()","05eedfe7":"fig, ax1 = plt.subplots(1, 1, figsize=(20, 20))\nfor k in np.unique(y_train):\n    ax1.plot(X_tsne[y_train==k, 0], X_tsne[y_train==k, 1], '.', label='{}'.format(k))\nax1.legend()","26aefc1d":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC # too slow\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer","2a0d4295":"def train_and_show_model(in_model, train_size=X_train.shape[0]\/\/2, random_state=2019, show_figure=True):\n    train_imgs, test_imgs, train_feat, test_feat, train_y, test_y = train_test_split(\n        X_train,\n        X_features, \n        y_train, \n        random_state=random_state, \n        test_size=X_train.shape[0]-train_size,\n        stratify=y_train\n    )\n    # fit pixel model\n    baseline_model = make_pipeline(Normalizer(), in_model)\n    baseline_model.fit(train_imgs.reshape((-1, 784)), train_y)\n    baseline_pred_y = baseline_model.predict(test_imgs.reshape((-1, 784)))\n    \n    # fit feature model\n    feat_model = make_pipeline(Normalizer(), in_model)\n    feat_model.fit(train_feat, train_y)\n    pred_y = feat_model.predict(test_feat)\n    if show_figure:\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n\n        sns.heatmap(confusion_matrix(test_y, baseline_pred_y), annot=True, fmt='d', ax=ax1)\n        ax1.set_title('Pixel Accuracy: {:2.1%}'.format(accuracy_score(test_y, baseline_pred_y)))\n\n        sns.heatmap(confusion_matrix(test_y, pred_y), annot=True, fmt='d', ax=ax2)\n        ax2.set_title('Feature Accuracy: {:2.1%}'.format(accuracy_score(test_y, pred_y)))\n    return {\n        'train_size': train_imgs.shape[0],\n        'random_state': random_state,\n        'pixel_accuracy': accuracy_score(test_y, baseline_pred_y),\n        'feature_accuracy': accuracy_score(test_y, pred_y)\n    }","85276e4f":"train_and_show_model(LogisticRegression(solver='lbfgs', multi_class='auto'))","c2bd5b42":"train_and_show_model(RandomForestClassifier(n_estimators=100))","7566b79f":"import dask\nfrom dask import bag\nimport dask.diagnostics as diag\nfrom multiprocessing.pool import ThreadPool\nimport gc; gc.enable(); gc.collect() # memory gets very tight\nmodel_fn = lambda : RandomForestClassifier(n_estimators=25, random_state=2019, n_jobs=1)\nparm_sweep = bag.\\\n    from_sequence(product(np.logspace(1, 3, 10).astype(int), # sample size\n                          np.arange(6))).\\\n    map(lambda args: train_and_show_model(model_fn(),\n                                     train_size=args[0], \n                                     random_state=args[1],\n                                    show_figure=False))\nparm_sweep","e98058f0":"with diag.ProgressBar(), dask.config.set(pool = ThreadPool(2)):\n    parm_df = pd.DataFrame(parm_sweep.compute())\n# clean-up the output\nnice_parm_df = pd.melt(parm_df, id_vars=['train_size', 'random_state']).\\\n    rename(columns={'value': 'Test Accuracy', 'variable': 'input'})\nnice_parm_df['Test Accuracy']*=100\nnice_parm_df['input'] = nice_parm_df['input'].map(lambda x: x.split('_accuracy')[0])\nnice_parm_df['Samples Per Class'] = nice_parm_df['train_size']\/np.unique(y_train).shape[0]\nnice_parm_df.head(3) ","b30595f4":"sns.catplot(x='Samples Per Class', \n            y='Test Accuracy', \n            hue='input', \n            kind='point',\n            alpha=0.5,\n            ci='sd',\n            data=nice_parm_df)","3f9db7c0":"sns.catplot(x='Samples Per Class', \n            y='Test Accuracy', \n            hue='input', \n            kind='swarm',\n            data=nice_parm_df)","369bfa62":"# Model Building\n## Encoder Model\nWe first build the tile encoder model to come up with a feature representation of the tiles","e3948647":"# Scramble Combinations\nWe have $9!$ different possible permutations, but that is too many and is probably not a great problem to solve (since it is under-constained, there are alot of permutations where it would be hard to know what exactly is being matched to what.","434e93ec":"## Logistic Regression","47fb46e5":"## Calculate TSNE\nWe can see if the TSNE space seperates the digits well (we will quantify this later)","817d3acd":"### Show combinations\nHere we can show combinations along with various instances of jitter noise to see how much that affects the reconstruction","a457dfbb":"### Activated Neurons\nHere we show each intermediate layer (panel) with each neuron\/depth-channel (row) and the top-n images for activating that pattern (columns). Each row should more or less represent the kinds of images that particular neuron is sensitive too.","9e8810d1":"## Convert into an Image Model\nWe throw in global average pooling to turn the output of the `tile_encoder` into a single feature-vector. We can then use this feature vector as a basis classifying image.","b2d27e25":"## Increasing Training Size\nHere we try and evaluate how well the model works as we increase the sample count. We provide the `test_size` as the sample count (as an integer) and the `random_state` to allow multiple runs to be done to make sure there aren't strong sample specific dependencies.","b9430168":"# Did we learn useful intermediate representations?\nSo we have a nice pretrained model that seems to have figured out how to solve the jigsaw puzzle (sometimes). Can we do anything with it?\n- Use the model to calculate features on all of the images in MNIST\n- See if the feature space has anything meaningful","2722c540":"## Show a slightly more complicated model","c5b085b2":"## Look at the filters\nWe can examine the filters and try to see what the model was doing?","139207c5":"# Data Preparation\nIn order to train a model we need to pre-compute a whole bunch of data to train models with","0c8769df":"# Jigsaw on MNIST Images\nHere we start the actual code. We have some predefined constants below for the size of various layers and tiles. These should be optimized to be well suited for the problem at hand","bf7ec2e2":"## Find the most activating imaging channels\nWe can run all of the images through the model and record all of the intermediate points","5a3e6003":"## Comparison to Pixel-based Methods\nWe compare every model to what the performance would have been had it been trained directly on the raw pixel data.","8986e4d7":"## Show Output\nIn order to show the model output we need to descramble the image with the given scrambling code. Given that a scrambling is a mapping from $i\\rightarrow j$ we need to make a reverse mapping for each combination","5893b7ae":"# Fitting Models\nHere we try to fit models with the features created by the tile-encoder. I try logistic regression as the baseline and then see if random forest or SVMs are able to do any better.","97fe4f0a":"### Run lots of models\nRunning multiple models with multiple parameters is very time consuming so we use dask to run them in parallel.","3d7368cb":"# Overview\nExperiment with the ideas from [Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles](https:\/\/link.springer.com\/chapter\/10.1007\/978-3-319-46466-4_5) by Mehdi Noroozi and Paolo Favaro. \n\nThe core idea is to take an image, break it up into square tiles, scramble the tiles and have a model learn to unscramble them $\\rightarrow$ \"solving a jigsaw puzzle\". The features the model learns on the individual tiles should then be useful for describing the important information in the image.\n\n## Overview\n- Create and test the jigsaw puzzle code (and the reconstruction)\n- Setup scrambling\n- Build a training and validation dataset\n- Train the model to unscramble\n- Evaluate performance and visualize what the model has done\n- Use the features to try and classify digits","674b6bb8":"## Big Jigsaw Permutation Model\nHere we reuse (shared-weights) the tile-encoder to process a number of tiles and predict which permutation is most likely","42b5e791":"## To be continued\n- Benchmark the jigsaw learning against untrained models, auto-encoders, and other approaches\n- Determine how many fewer samples can be used to reach a given validation accuracy","75ceb623":"# Jigsaw Code\nHere we write the jigsaw code to break the image up into a bunch of little pieces (```cut_jigsaw```) and reassemble the pieces back into an image (```jigsaw_to_image```). The methods right now are very simple, but are fairly easy to read and include a few basic test-cases. The code is implemented in a very intuitive, but inefficient manner. This should certainly be optimized before use on real problems."}}