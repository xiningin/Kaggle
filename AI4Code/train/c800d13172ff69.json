{"cell_type":{"8e41c9e8":"code","3cf67112":"code","745caf05":"code","d0afcb92":"code","8f4e90d8":"code","0c6da836":"code","b79f4fa8":"code","97807223":"code","da33203b":"code","a25e29d4":"code","a1cb2925":"code","6da6627d":"code","69b2878c":"code","075e90c0":"code","79f570b9":"code","42d44e79":"code","0311998a":"code","96a89288":"code","26359cb4":"markdown","8364f85c":"markdown","ce26bb52":"markdown","6bf3e197":"markdown","42ad020b":"markdown","98fd3778":"markdown","f2616357":"markdown","84cc40fd":"markdown","5f1c7c79":"markdown","9956dd3a":"markdown","10b82aa5":"markdown","b9d40578":"markdown","ef9eb6bb":"markdown","236b69f3":"markdown"},"source":{"8e41c9e8":"# Set the RANDOM_STATE\nRANDOM_STATE=42\n\n# Import PCA\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html\nfrom sklearn.decomposition import PCA\n\n# Import matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Import numpy\nimport numpy as np\n\n# Import the digits dataset\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.load_digits.html\nfrom sklearn.datasets import load_digits\nX, y = load_digits(return_X_y=True)\n\n# Display the first digit in the dataset\nplt.gray()\nplt.matshow(X[1796].reshape(8,8))","3cf67112":"# Put your code for question 1 here!\n# Normalization\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler(with_mean=True, with_std=True)\nX_norm = scaler.fit(X).transform(X)\nX_pca = PCA(n_components=10,random_state=RANDOM_STATE).fit(X_norm)\nprint ('Shape of the data before applying PCA',X.shape)\nprint ('Shape of the data after applying PCA',X_pca.components_.shape)\nprint('Percentage of variance explained by each of the selected components','\\n',\n      X_pca.explained_variance_ratio_)\nprint ('''Sklearn Explained Variance Ratio is not telling the exact percentage,we will calculate through other method''')\nprint ('------------------------')\n# Calculating explained variance of each component\nexplained_variance = X_pca.explained_variance_\ntotal_explained_variance = X_pca.explained_variance_.sum()\nvariance = []\nfor var in explained_variance:\n    variance.append(var\/total_explained_variance)\nprint ('Explained Variance Ratio is', variance)\nprint('Explained Variance Ratio of all components should be equal to one',sum(variance))","745caf05":"# Code block in case you want to use it for this question\n# Lets also confirm thr explained variance through visualization\ncolors = ['purple','green','blue','orange','red','yellow','black','violet',\n          'indigo','lime']\nfor i in range(10):\n    fig = plt.figure(figsize=(14,4))\n    plt.bar(range(len(X_pca.components_[i])), X_pca.components_[i], color=colors[i])\n    plt.xlabel('Component Number')\n    plt.ylabel('Principal Component '+str(i+1)+' Value')\n    plt.title('Principal Component '+str(i+1))\n    plt.show()","d0afcb92":"# Put your code for question 3 here!\n# Transforming Features\n# Creating new featurs as a linnear combinaiton of old featurs though selected PCA.\n# Selecting 90% variance Principal Components\nX = np.dot(X,X_pca.components_[:7].T)\nprint ('Shape of the Data After Transforming',X.shape)","8f4e90d8":"def matrix_multiply(A, B):\n    m = A.shape[0]\n    n = A.shape[1]\n    p = B.shape[1]\n    C = np.zeros((m, p))\n\n    for i in range(m):\n        for j in range(p):\n            for k in range(n):\n                C[i,j] += A[i,k] * B[k,j]\n    \n    return C","0c6da836":"np.random.seed(42)\nA = np.random.random_sample((300, 100)) * 100\nB = np.random.random_sample((100, 200)) * 100","b79f4fa8":"# Uncomment this when you're ready to time the function\n%timeit matrix_multiply(A, B)","97807223":"def matrix_multiply_1(A, B):\n    m = A.shape[0]\n    n = A.shape[1]\n    p = B.shape[1]\n    C = np.zeros((m, p))\n\n    for i in range(m):\n        for j in range(p):\n            # Replace pass with your line of code!!!\n            C[i:,j:] = np.dot(A[i:],B[:,j:])\n    \n    return C","da33203b":"# Uncomment this when you're ready to time the function\n%timeit matrix_multiply_1(A, B)","a25e29d4":"def matrix_multiply_2(A, B):\n    # Replace None with your matmul function!\n    return np.matmul(A,B)","a1cb2925":"%timeit matrix_multiply_2(A, B)","6da6627d":"from numba import jit\n# Put your code below!!!\n@jit(nopython=True)\ndef matrix_multiply_3(A, B):\n    m = A.shape[0]\n    n = A.shape[1]\n    p = B.shape[1]\n    C = np.zeros((m, p))\n\n    for i in range(m):\n        for j in range(p):\n            for k in range(n):\n                C[i,j] += A[i,k] * B[k,j]\n    \n    return C","69b2878c":"# Uncomment this when you're ready to time the function\n%timeit matrix_multiply_3(A, B)","075e90c0":"%load_ext cython","79f570b9":"%%cython -a\nimport numpy as np\n# Put your code for question 4 here!\ndef matrix_multiply_4(A, B):\n    m = A.shape[0]\n    n = A.shape[1]\n    p = B.shape[1]\n    C = np.zeros((m, p))\n\n    for i in range(m):\n        for j in range(p):\n            for k in range(n):\n                C[i,j] += A[i,k] * B[k,j]\n    \n    return C","42d44e79":"# Uncomment this when you're ready to time the function\n%timeit matrix_multiply_4(A, B)","0311998a":"%%cython -a\nimport numpy as np\ncimport numpy as np\nnp.import_array()\nDTYPE = np.intc\n# Put your code for question 5 here!\ndef matrix_multiply_5(double[:, :]A, double[:, :]B,double[:,:]C):\n    cdef Py_ssize_t m = A.shape[0]\n    cdef Py_ssize_t n = A.shape[1]\n    cdef Py_ssize_t p = B.shape[1]\n    C = np.zeros((m, p))\n    cdef Py_ssize_t i,j,k\n    #cdef int i, j, k\n    for i in range(m):\n        for j in range(p):\n            for k in range(n):\n                C[i,j] += A[i,k] * B[k,j]\n    \n    return C","96a89288":"# Uncomment this when you're ready to time the function\nC = np.zeros((300, 200))\n%timeit matrix_multiply_5(A, B, C)","26359cb4":"5. Write a function named matrix_multiply_5(A, B, C) that uses Cython. This time, specify the types of all parameters and all local variables. The matrix_multiply_5(A, B, C) function should not return anything - instead, it should accept a $m$ x $p$ matrix $C$ which has been initialized to all zeroes as a parameter. When the functoin returns, $C$ should contain the matrix multiplication of $A$ and $B$. ","8364f85c":"### Part 1 - Principal Component Analysis \n\n### In Part 1 of the homework, you will use PCA to reduce the dimensionality of the Sklearn digits dataset. This dataset contains 1,797 images of the digits 0-9. Each feature vector contains 64 pixel intensities in the range 0-16.","ce26bb52":"2. Write a new function matrix_multiply_2(A,B) that just returns the result of np.matmul() on A and B. ","6bf3e197":"In Part 2 you will use different aproaches to optimize a matrix multiplication function.","42ad020b":"1. Apply PCA to the digits dataset using default hyperparameters. Make sure to pass random_state=RANDOM_STATE as a parameter to PCA. Then, print the percentage of variance explained by each of the principal components (5 pts)","98fd3778":"4. Write a function named matrix_multiply_4(A, B) that uses Cython. Leave the contents of matrix_multiply_4(A,B) the same as the original matrix_multiply(A, B) function. Since you are not specifying types of variables, the output should contain lots of yellow lines of code, indicating that Cython was not able to provide much speedup. ","f2616357":"Multiplying an $m$ x $n$ matrix $A$ by a $n$ x $p$ matrix $B$ results in an $m$ x $p$ matrix $C$. To solve for the value at $C_{ij}$ in the resulting matrix $C$, use the following formula:\n\n$C_{ij} = \\sum_{k=1}^m A_{ik} B_{kj}$\n\n\nIn this homework you will compare different implementations for matrix multiplication in Python. An inefficient implementation is provided for you in the cell below.","84cc40fd":"3. Write a function named matrix_multiply_3(A, B) that uses the just-in-time compilation decorator provided by the Numba library to speed it up. The contents of the matrix_multiply_3(A, B) function should be equivalent to the original matrix_multiply(A, B) function. ","5f1c7c79":"# Answer\n### The overall variance is distributed through each of the components. We check total components variance through two selections.\n\n### Selecting first 5 compnonets will explain 20%+16%+14%+11%+8% = 69% of variance in data.\n\n### Similarly selecting 90% variance of data, 8 components are selected. 21%+16%+14%+11%+8%+7%+7%+6% = 90%\u00a0","9956dd3a":"1. Write a function named matrix_multiply_1(A, B) which uses vectorization. Specifically, you should replace the innermost loop of the slow matrix_multiply(A,B) function with a single line that computes the value of $C[i,j]$. Use the np.dot() function and numpy array slicing to do this. ","10b82aa5":"For this assignment, you will work with large matrices $A$ and $B$. They have been generated using random floating point values in the range $[0, 100)$. As an example, the cells below show how to time the matrix multiplication of A and B using the slow matrix_multiply(A, B) function above.","b9d40578":"2. Choose how many principal components you want to keep from your dataset, and justify your answer. (Hint: There's no specific number of components that's correct, but we discussed some guidelines for this during class \/ in the slides). ","ef9eb6bb":"### Part 2 - Optimizing Python ","236b69f3":"3. Use the principal components you identified in question 2 to reduce the dimensionality of X. Do this in a single line of code, using only numpy functions. You should NOT use transform(), fit_transform(), or any other functions provided by the PCA object in this problem.\n\n(Hint: Sklearn represents the principal components as a np.ndarray of shape (n_components, n_features), but the slides describe them as (n_features, n_components). Numpy has a function that returns the transpose of a np.ndarray)\n\nPrint the shape of X after you have done this. "}}