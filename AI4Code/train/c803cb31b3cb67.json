{"cell_type":{"12200534":"code","be5874a3":"code","5eb65e6a":"code","79917956":"code","fecaa267":"code","23718850":"code","711d9ba8":"code","43990456":"code","eb088ee7":"code","29cdf5ad":"code","e5c2b07a":"code","990be4fd":"code","faeae72f":"code","60495561":"code","4f296299":"code","007286ed":"code","0a92feee":"code","7f36d40f":"code","524543ca":"code","4263df7f":"code","4c0252a0":"code","0fb3fee4":"code","2b4e2e0d":"code","84579e0b":"code","a5fb551a":"code","4c194190":"code","c973e7de":"code","6b191367":"code","c71935bd":"code","b3d61961":"code","73410b5f":"code","7be23ecd":"code","09ebb830":"code","b141bf50":"code","ead58830":"code","a4905f7e":"code","859599ed":"code","8b439843":"code","21df243b":"code","56b6b1bb":"code","e15fd307":"code","247f9d4a":"code","23925ea5":"code","fd19840a":"code","011bfc2f":"markdown","e78cd755":"markdown","c328bd51":"markdown","95eee239":"markdown","2a5451fb":"markdown","08708087":"markdown","20d86106":"markdown","d23b3e4b":"markdown","3e619605":"markdown","25548d35":"markdown","e64d2635":"markdown","a4b0e95c":"markdown","2d64a764":"markdown","e7808035":"markdown","ffe8bf5c":"markdown","28b836e3":"markdown","25341bce":"markdown","2852741e":"markdown","b7630b25":"markdown","b082a4a4":"markdown","8b7a7497":"markdown","f27cb17e":"markdown","ab0083ee":"markdown","d0b721b8":"markdown","852dd4ee":"markdown","14a6967d":"markdown","cba70424":"markdown"},"source":{"12200534":"import os\nimport math\nimport random\nimport torch\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport albumentations as A\nimport torchvision\nfrom PIL import Image\nimport torch\nimport numpy as np","be5874a3":"def set_seed(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    \n    \nSEED = 42\nset_seed(SEED)\n","5eb65e6a":"BASE_PATH = '..\/input\/cassava-leaf-disease-classification\/'\nIMAGE_PATH = os.path.join(BASE_PATH, \"train_images\")\nIMAGE = '..\/input\/cassava-leaf-disease-classification\/train_images\/469487.jpg'\nIMAGE_SIZE_AUG = 256\nmean = (0.485, 0.456, 0.406)\nstd = (0.229, 0.224, 0.225)\np=0.5","79917956":"augmentations = A.Compose(\n        [     \n        A.RandomResizedCrop(IMAGE_SIZE_AUG, IMAGE_SIZE_AUG),\n        A.Transpose(p=p),\n        A.HorizontalFlip(p=p),\n        A.VerticalFlip(p=p),\n        A.OneOf([\n        A.IAAAdditiveGaussianNoise(),\n        A.GaussNoise(),\n        ], p=p),\n        A.ShiftScaleRotate(p=p),\n        A.Normalize(mean, std, max_pixel_value=255.0, always_apply=True),  \n        ]\n    )","fecaa267":"class Cassava_Train_DS:\n    \n    def __init__(self, image_paths, targets, resize, augmentations=None):\n        self.image_paths = image_paths\n        self.targets = targets\n        self.resize = resize\n        self.augmentations = augmentations\n        \n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, item):\n        image = Image.open(self.image_paths[item])\n        targets = self.targets[item]\n        if self.resize is not None:\n            image = image.resize(\n                (self.resize[1], self.resize[0]), resample=Image.BILINEAR\n            )\n        image = np.array(image)\n        if self.augmentations is not None:\n            augmented = self.augmentations(image=image)\n            image = augmented[\"image\"]\n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        dct = {\n            \"image\": torch.tensor(image),\n            \"targets\": torch.tensor(targets),\n        }\n                \n        return dct","23718850":"def _transform_aug(\n    input_path='..\/input\/cassava-leaf-disease-classification\/',\n    image_path = '..\/input\/cassava-leaf-disease-classification\/train_images\/',\n    resize = (256, 256)\n):\n           \n    train_df = pd.read_csv(input_path+'train.csv')\n    train_images = train_df.image_id.values.tolist()\n    train_images = [os.path.join(image_path, i) for i in train_images]\n    train_targets = train_df.label.values   \n    train_dataset = Cassava_Train_DS(train_images, train_targets, resize = resize, augmentations = augmentations)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n    for idx, data in enumerate(trainloader): \n        break\n    print(data['targets'].numpy())\n    im = torchvision.utils.make_grid(data['image'], nrow=8)  # the default nrow is 8\n    inv_normalize = torchvision.transforms.Normalize(\n        mean=[-0.485\/0.229, -0.456\/0.224, -0.406\/0.225],\n        std=[1\/0.229, 1\/0.224, 1\/0.225]\n    )\n    im_inv = inv_normalize(im)\n    plt.figure(figsize=(17,15))\n    plt.imshow(np.transpose(im_inv.numpy(), (1, 2, 0)));","711d9ba8":"#Batch Visualizer\n_transform_aug()","43990456":"def batch_visualization(path, n_images, is_random=True, figsize=(16, 16)):\n    plt.figure(figsize=figsize)\n    \n    w = int(n_images ** .5)\n    h = math.ceil(n_images \/ w)\n    all_names = os.listdir(path)\n\n    image_names = all_names[:n_images]\n    if is_random:\n        image_names = random.sample(all_names, n_images)\n    \n    for ind, image_name in enumerate(image_names):\n        img = cv2.imread(os.path.join(path, image_name))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n        plt.subplot(h, w, ind + 1)\n        plt.imshow(img)\n        plt.axis(\"off\")\n    \n    plt.show()","eb088ee7":"batch_visualization(IMAGE_PATH, 1, is_random=True, figsize=(10, 6))","29cdf5ad":"batch_visualization(IMAGE_PATH, 16, is_random=True, figsize=(10, 10))","e5c2b07a":"batch_visualization(IMAGE_PATH, 100, is_random=False)","990be4fd":"def color_hist_visualization(image_path, figsize=(16, 4)):\n    plt.figure(figsize=figsize)\n    \n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n    plt.subplot(1, 4, 1)\n    plt.imshow(img)\n    plt.axis(\"off\")\n    \n    colors = [\"red\", \"green\", \"blue\"]\n    for i in range(len(colors)):\n        plt.subplot(1, 4, i + 2)\n        plt.hist(\n            img[:, :, i].reshape(-1),\n            bins=25,\n            alpha=0.5,\n            color=colors[i],\n            density=True\n        )\n        plt.xlim(0, 255)\n        plt.xticks([])\n        plt.yticks([])\n    \n    \n    plt.show()","faeae72f":"color_hist_visualization(IMAGE)\n\ncolor_hist_visualization(IMAGE)\n\ncolor_hist_visualization(IMAGE)","60495561":"def channels_visualization(image_path, figsize=(16, 4)):\n    plt.figure(figsize=figsize)\n    \n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n    plt.subplot(1, 4, 1)\n    plt.imshow(np.mean(img, axis=2), cmap=\"gray\")\n    plt.axis('off')\n    \n    for i in range(3):\n        plt.subplot(1, 4, i + 2)\n        tmp_img = np.full_like(img, 0)\n        tmp_img[:, :, i] = img[:, :, i]\n        plt.imshow(tmp_img)\n        plt.xlim(0, 255)\n        plt.xticks([])\n        plt.yticks([])\n    \n    \n    plt.show()","4f296299":"channels_visualization(IMAGE)","007286ed":"def grayscale_visualization(image_path, figsize=(8, 4)):\n    plt.figure(figsize=figsize)\n    \n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n    plt.subplot(1, 2, 1)\n    plt.imshow(img)\n    plt.axis('off')\n    \n    plt.subplot(1, 2, 2)\n    tmp_img = np.full_like(img, 0)\n    for i in range(3):\n        tmp_img[:, :, i] = img.mean(axis=-1)\n    plt.imshow(tmp_img)\n    plt.axis('off')\n    \n    \n    plt.show()","0a92feee":"grayscale_visualization(IMAGE)","7f36d40f":"def plot_simple_augmentation(image_path, transform):\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n    \n    plt.figure(figsize=(10, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.imshow(img)\n    plt.axis(\"off\")\n    \n    plt.subplot(1, 2, 2)\n    x = transform(image=img)[\"image\"]\n    plt.imshow(x)\n    plt.axis(\"off\")\n\n    plt.show()\n    \ndef plot_multiple_augmentation(image_path, transform):\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n    \n    plt.figure(figsize=(10, 10))\n    \n    plt.subplot(2, 2, 1)\n    plt.imshow(img)\n    plt.axis(\"off\")\n    \n    plt.subplot(2, 2, 2)\n    x = transform(image=img)[\"image\"]\n    plt.imshow(x)\n    plt.axis(\"off\")\n    \n    plt.subplot(2, 2, 3)\n    x = transform(image=img)[\"image\"]\n    plt.imshow(x)\n    plt.axis(\"off\")\n    \n    plt.subplot(2, 2, 4)\n    x = transform(image=img)[\"image\"]\n    plt.imshow(x)\n    plt.axis(\"off\")\n\n    plt.show()","524543ca":"transform = A.Compose(\n    [\n        A.Blur(p=1.0, blur_limit=(5, 5)),\n    ]\n)\n\nplot_simple_augmentation(IMAGE, transform)","4263df7f":"transform = A.CLAHE(p=1.0, clip_limit=(10, 10), tile_grid_size=(3, 3))\n\nplot_simple_augmentation(IMAGE, transform)","4c0252a0":"transform = A.CenterCrop(p=1.0, height=100, width=150)\n\nplot_simple_augmentation(IMAGE, transform)","0fb3fee4":"transform = A.ChannelDropout(p=1.0, channel_drop_range=(1, 2), fill_value=0)\n\nplot_multiple_augmentation(IMAGE, transform)","2b4e2e0d":"transform = A.ChannelShuffle(p=1.0)\n\nplot_multiple_augmentation(IMAGE, transform)","84579e0b":"transform = A.Crop(p=1.0, x_min=0, y_min=0, x_max=150, y_max=150)\n\nplot_simple_augmentation(IMAGE, transform)","a5fb551a":"transform = A.RandomCrop(p=1.0, height=100, width=100)\n\nplot_multiple_augmentation(IMAGE, transform)","4c194190":"transform = A.CoarseDropout(\n    max_holes=8,\n    max_height=8,\n    max_width=8, \n    min_holes=None,\n    min_height=None,\n    min_width=None, \n    fill_value=0, \n    always_apply=False,\n    p=0.5\n)\n\nplot_multiple_augmentation(IMAGE, transform)\n","c973e7de":"transform = A.Downscale(\n    p=1.0, scale_min=0.01, scale_max=0.20, interpolation=0,\n)\n\nplot_multiple_augmentation(\n    IMAGE,\n    transform,\n)","6b191367":"transform = A.Equalize(\n    p=1.0, mode='cv', by_channels=True,\n)\n\nplot_simple_augmentation(\n    IMAGE,\n    transform,\n)","c71935bd":"transform = A.HorizontalFlip(\n    p=1,\n)\n\nplot_simple_augmentation(\n    IMAGE,\n    transform,\n)","b3d61961":"transform = A.VerticalFlip(\n    p=1.0,\n)\n\nplot_simple_augmentation(\n    IMAGE,\n    transform,\n)","73410b5f":"transform = A.Flip(\n    p=1.0,\n)\n\nplot_multiple_augmentation(\n    IMAGE,\n    transform,\n)","7be23ecd":"transform = A.GaussNoise(\n    p=1.0, var_limit=(500.0, 500.0),\n)\n\nplot_simple_augmentation(\n    IMAGE,\n    transform,\n)","09ebb830":"transform = A.GridDistortion(\n    p=1.0, num_steps=15, distort_limit=(-2., 2.), \n    interpolation=0, border_mode=0, value=(0, 0, 0), mask_value=None,\n)\n\nplot_multiple_augmentation(\n    IMAGE,\n    transform,\n)","b141bf50":"transform = A.HueSaturationValue(\n    p=1.0, \n    hue_shift_limit=(-100, 100), \n    sat_shift_limit=(-100, 100), \n    val_shift_limit=(-100, 100),\n)\n\nplot_multiple_augmentation(\n    IMAGE,\n    transform,\n)","ead58830":"transform = A.ISONoise(\n    p=1.0, intensity=(0.0, 2.0), color_shift=(0.0, 1.0)\n)\n\nplot_multiple_augmentation(\n    IMAGE,\n    transform,\n)","a4905f7e":"transform = A.ImageCompression(\n    p=1.0, quality_lower=0, quality_upper=10, compression_type=0,\n)\n\nplot_multiple_augmentation(\n    IMAGE,\n    transform,\n)","859599ed":"transform = A.InvertImg(\n    p=1.0,\n)\n\nplot_simple_augmentation(\n    IMAGE,\n    transform,\n)","8b439843":"transform = A.JpegCompression(\n    p=1.0, quality_lower=0, quality_upper=10,\n)\n\nplot_multiple_augmentation(\n    IMAGE,\n    transform,\n)","21df243b":"transform = A.MotionBlur(\n    p=1.0, blur_limit=(3, 50),\n)\n\nplot_multiple_augmentation(\n    IMAGE,\n    transform,\n)","56b6b1bb":"transform = A.MultiplicativeNoise(\n    p=1.0, multiplier=(0.1, 5.0), per_channel=True, elementwise=False,\n)\n\nplot_multiple_augmentation(\n    IMAGE,\n    transform,\n)","e15fd307":"transform = A.ShiftScaleRotate(p=1)\n\nplot_multiple_augmentation(\n    IMAGE,\n    transform,\n)","247f9d4a":"transform = A.GaussNoise(p=1)\n\nplot_multiple_augmentation(\n    IMAGE,\n    transform,\n)","23925ea5":"transform = A.HueSaturationValue (hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, always_apply=False, p=1)\n\nplot_multiple_augmentation(\n    IMAGE,\n    transform,\n)","fd19840a":"transform = A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, brightness_by_max=True, always_apply=False, p=1)\nplot_multiple_augmentation(\n    IMAGE,\n    transform,\n)","011bfc2f":"# Randomly change hue, saturation and value of the input image.\n\nDefault: albumentations.augmentations.transforms.HueSaturationValue (hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, always_apply=False, p=0.5)","e78cd755":"# Blur the input image using a random-sized kernel.\n\nDefault: albumentations.augmentations.transforms.Blur (blur_limit=7, always_apply=False, p=0.5)","c328bd51":"# Crop a random part of the input.\n\nDefault: albumentations.augmentations.transforms.RandomCrop (height, width, always_apply=False, p=1.0)","95eee239":"# Apply Contrast Limited Adaptive Histogram Equalization to the input image.\n\nDefault: albumentations.augmentations.transforms.CLAHE (clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5)","2a5451fb":"# Flip the input either horizontally, vertically or both horizontally and vertically.\n\nDefault: albumentations.augmentations.transforms.Flip(p=0.5)","08708087":"# Randomly Drop Channels in the input Image.\n\nDefault: albumentations.augmentations.transforms.ChannelDropout (channel_drop_range=(1, 1), fill_value=0, always_apply=False, p=0.5)","20d86106":"# Flip the input vertically around the x-axis.\n\nDefault: albumentations.augmentations.transforms.VerticalFlip(p=0.5)","d23b3e4b":"# Crop region from image.\n\nDefault: albumentations.augmentations.transforms.Crop (x_min=0, y_min=0, x_max=1024, y_max=1024, always_apply=False, p=1.0)","3e619605":"# Randomly change brightness of the input image.\n\nDefault: albumentations.augmentations.transforms.RandomBrightness (limit=0.2, always_apply=False, p=0.5)","25548d35":"# Apply gaussian noise to the input image.\n\nDefault: albumentations.augmentations.transforms.GaussNoise (var_limit=(10.0, 50.0), mean=0, always_apply=False, p=0.5)","e64d2635":"# Randomly change hue, saturation and value of the input image.\n\nDefault: albumentations.augmentations.transforms.HueSaturationValue (hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, always_apply=False, p=0.5)","a4b0e95c":"# Multiply image to random number or array of numbers.\n\nDefault: albumentations.augmentations.transforms.MultiplicativeNoise (multiplier=(0.9, 1.1), per_channel=False, elementwise=False, always_apply=False, p=0.5)","2d64a764":"# Decreases image quality by downscaling and upscaling back.\n\nDefault: albumentations.augmentations.transforms.Downscale (scale_min=0.25, scale_max=0.25, interpolation=0, always_apply=False, p=0.5)","e7808035":"# Randomly apply affine transforms: translate, scale and rotate the input.\n\nDefault: albumentations.augmentations.transforms.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=45, interpolation=1, border_mode=4, always_apply=False, p=0.5)","ffe8bf5c":"# Cassava Augmentations\n\nBelow augmentation utility functions are copied from [ihelon](https:\/\/www.kaggle.com\/ihelon) notebook, Thanks for the excellent resource [ihelon](https:\/\/www.kaggle.com\/ihelon)\n\nPlease have a look at his work once.\n\nhttps:\/\/www.kaggle.com\/ihelon\/monet-visualization-and-augmentation","28b836e3":"# Invert the input image by subtracting pixel values from 255.\n\nDefault: albumentations.augmentations.transforms.InvertImg(p=0.5)","25341bce":"# Randomly rearrange channels of the input RGB image.\n\nDefault: albumentations.augmentations.transforms.ChannelShuffle(p=0.5)","2852741e":"# Apply gaussian noise to the input image.\n\nDefault: albumentations.augmentations.transforms.GaussNoise (var_limit=(10.0, 50.0), mean=0, always_apply=False, p=0.5)","b7630b25":"# Flip the input horizontally around the y-axis.\n\nDefault: albumentations.augmentations.transforms.HorizontalFlip(p=0.5)","b082a4a4":"# Grid Distortion\n\nDefault: albumentations.augmentations.transforms.GridDistortion (num_steps=5, distort_limit=0.3, interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=False, p=0.5)","8b7a7497":"# Equalize the image histogram.\n\nDefault: albumentations.augmentations.transforms.Equalize (mode='cv', by_channels=True, mask=None, mask_params=(), always_apply=False, p=0.5)","f27cb17e":"# CoarseDropout of the rectangular regions in the image.\n\nDefault: albumentations.augmentations.transforms.CoarseDropout (max_holes=8, max_height=8, max_width=8, min_holes=None, min_height=None, min_width=None, fill_value=0, mask_fill_value=None, always_apply=False, p=0.5)","ab0083ee":"# Apply motion blur to the input image using a random-sized kernel.\n\nDefault: albumentations.augmentations.transforms.MotionBlur(p=0.5)","d0b721b8":"# Crop the central part of the input.\n\nDefault: albumentations.augmentations.transforms.CenterCrop (height, width, always_apply=False, p=1.0)","852dd4ee":"# Decrease Jpeg compression of an image.\n\nDefault: albumentations.augmentations.transforms.JpegCompression (quality_lower=99, quality_upper=100, always_apply=False, p=0.5)","14a6967d":"# Apply camera sensor noise.\n\nDefault: albumentations.augmentations.transforms.ISONoise (color_shift=(0.01, 0.05), intensity=(0.1, 0.5), always_apply=False, p=0.5)","cba70424":"# Decrease Jpeg, WebP compression of an image.\n\nDefault: albumentations.augmentations.transforms.ImageCompression (quality_lower=99, quality_upper=100, compression_type=<ImageCompressionType.JPEG: 0>, always_apply=False, p=0.5)"}}