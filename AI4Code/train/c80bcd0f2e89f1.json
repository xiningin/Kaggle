{"cell_type":{"66ca02f3":"code","fed01d4f":"code","4f5fb9ff":"code","777ffcef":"code","bb73022e":"code","e6d2c47c":"code","d6c4478a":"code","fa4b6d9b":"code","a04fd463":"code","3d64ecb1":"code","9058412f":"code","32120c71":"code","4829acd7":"code","0bfebafc":"code","7a6e557f":"code","06fb9643":"code","12ae71e5":"code","12c3aceb":"code","4658efb3":"code","a6f6d1fc":"code","001e6b6b":"code","a4576da1":"code","64a1b2f0":"code","ec2042a4":"code","665f2048":"code","4a8f7eae":"code","cb095c67":"code","3c661f96":"code","200950c2":"code","67dfec6d":"code","b8ee1daa":"code","7a6bb433":"code","db64b213":"code","c7f28c10":"code","d04c814a":"code","f1bca090":"code","5fa35a9f":"code","bebeeb2d":"code","35f5868b":"code","b6ec2427":"code","c4517a62":"code","ffce2afe":"markdown","bf165fbe":"markdown","4d11e756":"markdown","a466958e":"markdown","1ba68d2a":"markdown","4ecda6f1":"markdown","ae2c935b":"markdown","26130564":"markdown","0575e72a":"markdown","6aed42ee":"markdown","3116330b":"markdown","a6078132":"markdown","a6127935":"markdown","b4bf3c11":"markdown","80d161f7":"markdown","e70e30ea":"markdown","bff553db":"markdown","6b24aa46":"markdown","d7ab9545":"markdown","95c6f346":"markdown","e94252e3":"markdown","36130675":"markdown","eb87904e":"markdown","fd23ca48":"markdown"},"source":{"66ca02f3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter","fed01d4f":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test  = pd.read_csv('..\/input\/test.csv')\ndf_sample= pd.read_csv('..\/input\/gender_submission.csv')","4f5fb9ff":"def detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   ","777ffcef":"Outliers_to_drop = detect_outliers(df_train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\ndf_train = df_train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","bb73022e":"dataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in df_train[\"Name\"]]\ndf_train[\"Title\"] = pd.Series(dataset_title)\ndf_train[\"Title\"] = df_train[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndf_train[\"Title\"] = df_train[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ndf_train[\"Title\"] = df_train[\"Title\"].astype(int)\ndf_train.drop(labels = [\"Name\"], axis = 1, inplace = True)","e6d2c47c":"def impute_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age):\n\n        if Pclass == 1:\n            return 37\n\n        elif Pclass == 2:\n            return 29\n\n        else:\n            return 24\n\n    else:\n        return Age","d6c4478a":"def impute_fare(cols):\n    Fare = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Fare):\n\n        if Pclass == 1:\n            return 84\n\n        elif Pclass == 2:\n            return 20\n\n        else:\n            return 13\n\n    else:\n        return Fare","fa4b6d9b":"df_train['Age'] = df_train[['Age','Pclass']].apply(impute_age,axis=1)","a04fd463":"sex = pd.get_dummies(df_train['Sex'],drop_first=True)\nembark = pd.get_dummies(df_train['Embarked'],drop_first=True)\ndf_train = pd.concat([df_train,sex,embark],axis=1)","3d64ecb1":"df_train[\"Family\"] = df_train[\"SibSp\"] + df_train[\"Parch\"] + 1\ndf_train['Single'] = df_train['Family'].map(lambda s: 1 if s == 1 else 0)\ndf_train['SmallF'] = df_train['Family'].map(lambda s: 1 if  s == 2  else 0)\ndf_train['MedF']   = df_train['Family'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ndf_train['LargeF'] = df_train['Family'].map(lambda s: 1 if s >= 5 else 0)\ndf_train['Senior'] = df_train['Age'].map(lambda s:1 if s>60 else 0)","9058412f":"dataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in df_test[\"Name\"]]\ndf_test[\"Title\"] = pd.Series(dataset_title)\ndf_test[\"Title\"] = df_test[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndf_test[\"Title\"] = df_test[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ndf_test[\"Title\"] = df_test[\"Title\"].astype(int)\ndf_test.drop(labels = [\"Name\"], axis = 1, inplace = True)","32120c71":"df_test['Age'] = df_test[['Age','Pclass']].apply(impute_age,axis=1)\nsex = pd.get_dummies(df_test['Sex'],drop_first=True)\nembark = pd.get_dummies(df_test['Embarked'],drop_first=True)\ndf_test = pd.concat([df_test,sex,embark],axis=1)\n\ndf_test['Fare'].fillna(value=df_test['Fare'].median(),inplace=True)","4829acd7":"df_test['Fare'] = df_test[['Fare','Pclass']].apply(impute_fare,axis=1)","0bfebafc":"df_test[\"Fare\"] = df_test[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)","7a6e557f":"df_test[\"Family\"] = df_test[\"SibSp\"] + df_test[\"Parch\"] + 1","06fb9643":"df_test['Single'] = df_test['Family'].map(lambda s: 1 if s == 1 else 0)\ndf_test['SmallF'] = df_test['Family'].map(lambda s: 1 if  s == 2  else 0)\ndf_test['MedF']   = df_test['Family'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ndf_test['LargeF'] = df_test['Family'].map(lambda s: 1 if s >= 5 else 0)\ndf_test['Senior'] = df_test['Age'].map(lambda s:1 if s>60 else 0)","12ae71e5":"def get_person(passenger):\n    age,sex = passenger\n    return 'child' if age < 16 else sex","12c3aceb":"df_train['Person'] = df_train[['Age','Sex']].apply(get_person,axis=1)\ndf_test['Person']  = df_test[['Age','Sex']].apply(get_person,axis=1)\n\nperson_dummies_train  = pd.get_dummies(df_train['Person'])\nperson_dummies_train.columns = ['Child','Female','Male']\nperson_dummies_train.drop(['Male'], axis=1, inplace=True)\n\nperson_dummies_test  = pd.get_dummies(df_test['Person'])\nperson_dummies_test.columns = ['Child','Female','Male']\nperson_dummies_test.drop(['Male'], axis=1, inplace=True)\n\ndf_train = df_train.join(person_dummies_train)\ndf_test  = df_test.join(person_dummies_test)\n\ndf_train.drop(['Person'],axis=1,inplace=True)\ndf_test.drop(['Person'],axis=1,inplace=True)","4658efb3":"df_train.drop('male',axis=1,inplace=True)\ndf_test.drop('male',axis=1,inplace=True)","a6f6d1fc":"df_train.drop(['Cabin','Ticket'],axis = 1, inplace= True)\ndf_test.drop(['Ticket','Cabin'],axis = 1, inplace= True)","001e6b6b":"df_train.drop(['Sex','Embarked'],axis=1,inplace=True)\ndf_test.drop(['Sex','Embarked'],axis=1,inplace=True)","a4576da1":"df_train.head()","64a1b2f0":"df_train.describe()","ec2042a4":"df_train.info()\nprint('.............................................')\ndf_test.info()","665f2048":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df_train.drop('Survived',axis=1), \n                                                    df_train['Survived'], test_size=0.15, \n                                                    random_state=101)","4a8f7eae":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import plot_tree\n\ndt = DecisionTreeClassifier()\ndt.fit(X_train,y_train);\n\nplt.figure(figsize=(18,18))\nplot_tree(dt,filled=True);","cb095c67":"from xgboost import XGBClassifier\n\nXGB = XGBClassifier(max_depth=4,learning_rate=0.005,n_estimators=500,n_jobs=-1,min_child_weight=2)\nXGB.fit(X_train,y_train)","3c661f96":"y_pred = pd.DataFrame(XGB.predict(df_test))\ny_pred['Survived'] = y_pred[0]\ny_pred.drop(0,axis=1,inplace=True)\ny_pred['PassengerId'] = df_test['PassengerId']\ny_pred_xgb = y_pred\n\ny_pred.to_csv('titanic_pred_xgb.csv',index=False)","200950c2":"from sklearn.preprocessing import StandardScaler\n\nScaler1 = StandardScaler()\nScaler2 = StandardScaler()\nX_train_scaled = Scaler1.fit_transform(X_train)\ndf_test_scaled  = Scaler2.fit_transform(df_test)","67dfec6d":"from sklearn.linear_model import LogisticRegression\n\nlogmodel = LogisticRegression(C=10).fit(X_train,y_train)\n\ny_pred = pd.DataFrame(logmodel.predict(df_test))\n\ny_pred['Survived'] = y_pred[0]\ny_pred.drop(0,axis=1,inplace=True)\ny_pred['PassengerId'] = df_test['PassengerId']\ny_pred_lr = y_pred\n\ny_pred.to_csv('titanic_pred_logistic.csv',index=False)","b8ee1daa":"from sklearn.ensemble import RandomForestClassifier\n\nRFC = RandomForestClassifier(n_estimators=500,max_depth=9,min_samples_split=3)\nRFC.fit(X_train,y_train)","7a6bb433":"y_pred = pd.DataFrame(RFC.predict(df_test))\n\ny_pred['Survived'] = y_pred[0]\ny_pred.drop(0,axis=1,inplace=True)\ny_pred['PassengerId'] = df_test['PassengerId']\ny_pred_rf = y_pred\ny_pred.to_csv('titanic_pred_rfc.csv',index=False)","db64b213":"from lightgbm import LGBMClassifier\n\nlgb = LGBMClassifier(learning_rate=0.01,max_depth=5,n_estimators=500,num_leaves=3).fit(X_train,y_train)","c7f28c10":"y_pred = pd.DataFrame(lgb.predict(df_test))\n\ny_pred['Survived'] = y_pred[0]\ny_pred.drop(0,axis=1,inplace=True)\ny_pred['PassengerId'] = df_test['PassengerId']\ny_pred_lgb = y_pred\ny_pred.to_csv('titanic_pred_lgb.csv',index=False)","d04c814a":"print(\"XGB train score:             \",round(XGB.score(X_train,y_train),2),     \"   XGB test score:           \",round(XGB.score(X_test,y_test),2))\nprint(\"Log-Reg. train score:        \",round(logmodel.score(X_train,y_train),2),\"   Log-Reg. test score:      \",round(logmodel.score(X_test,y_test),2))\nprint(\"Random Forest's train score: \",round(RFC.score(X_train,y_train),2),     \"   Random Forest test score: \",round(RFC.score(X_test,y_test),2))\nprint(\"LGBM train score:            \",round(lgb.score(X_train,y_train),2),     \"   LGB Model test score:     \",round(lgb.score(X_test,y_test),2))","f1bca090":"y_valid_xgb = XGB.predict(X_test)\ny_valid_log = logmodel.predict(X_test)\ny_valid_rfc = RFC.predict(X_test)\ny_valid_lgb = lgb.predict(X_test)","5fa35a9f":"from sklearn.metrics import roc_curve, auc, roc_auc_score\n\nfpr_xgb, tpr_xgb, thresholds_xgb = roc_curve(y_test, y_valid_xgb)\nroc_auc_xgb = auc(fpr_xgb, tpr_xgb)\n\nfpr_log, tpr_log, thresholds_log = roc_curve(y_test, y_valid_log)\nroc_auc_log = auc(fpr_log, tpr_log)\n\nfpr_rfc, tpr_rfc, thresholds_rfc = roc_curve(y_test, y_valid_rfc)\nroc_auc_rfc = auc(fpr_rfc, tpr_rfc)\n\nfpr_lgb, tpr_lgb, thresholds_lgb = roc_curve(y_test, y_valid_lgb)\nroc_auc_lgb = auc(fpr_lgb, tpr_lgb)","bebeeb2d":"from sklearn.metrics import confusion_matrix\n\nax= plt.subplot()\nsns.heatmap(confusion_matrix(y_test, y_valid_xgb), annot=True, ax = ax); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix for XGB'); \nax.xaxis.set_ticklabels(['Dead', 'Survived']); ax.yaxis.set_ticklabels(['Dead', 'Survived']);\nplt.show()\n\nax= plt.subplot()\nsns.heatmap(confusion_matrix(y_test, y_valid_log), annot=True, ax = ax); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix for Log Reg'); \nax.xaxis.set_ticklabels(['Dead', 'Survived']); ax.yaxis.set_ticklabels(['Dead', 'Survived']);\nplt.show()\n\nax= plt.subplot()\nsns.heatmap(confusion_matrix(y_test, y_valid_rfc), annot=True, ax = ax); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix for Random Forest'); \nax.xaxis.set_ticklabels(['Dead', 'Survived']); ax.yaxis.set_ticklabels(['Dead', 'Survived']);\nplt.show()\n\nax= plt.subplot()\nsns.heatmap(confusion_matrix(y_test, y_valid_lgb), annot=True, ax = ax); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix for LGBM'); \nax.xaxis.set_ticklabels(['Dead', 'Survived']); ax.yaxis.set_ticklabels(['Dead', 'Survived']);\nplt.show()","35f5868b":"sns.set('talk', 'whitegrid', 'dark', font_scale=1.2,rc={\"lines.linewidth\": 2, 'grid.linestyle': '--'})\nlw = 3\nplt.figure(figsize=(12, 8))\n\nsns.despine()\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\nplt.plot(fpr_xgb, tpr_xgb, lw=lw, label='XGB   ROC curve (AUC = %0.2f)' % roc_auc_xgb);\nplt.plot(fpr_log, tpr_log, lw=lw, label='LR      ROC curve (AUC = %0.2f)' % roc_auc_log);\nplt.plot(fpr_rfc, tpr_rfc, lw=lw, label='RF      ROC curve (AUC = %0.2f)' % roc_auc_rfc);\nplt.plot(fpr_lgb, tpr_lgb, lw=lw, label='LGBM ROC curve (AUC = %0.2f)' % roc_auc_lgb);\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([-0.1, 1.1])\nplt.ylim([-0.1, 1.1])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('All Models ROC')\nplt.legend(loc=\"lower right\");\n","b6ec2427":"y_pred_final = y_pred\ny_pred_final['Survived'] = round(  0.25 * y_pred_lgb['Survived'] \n                                 + 0.25 * y_pred_rf['Survived'] \n                                 + 0.25 * y_pred_xgb['Survived'] \n                                 + 0.25 * y_pred_lr['Survived'] )\n\ny_pred_final['PassengerId'] = df_test['PassengerId']\ny_pred_final['Survived'] = y_pred_final['Survived'].astype(int)\ny_pred_final.to_csv('titanic_pred_final.csv',index=False)","c4517a62":"y_pred_final['Survived'].value_counts()","ffce2afe":"# A simple Outlier detection function to find them","bf165fbe":"# Light GBM","4d11e756":"# Importing the necessary libraries","a466958e":"# ROC","1ba68d2a":"# we need to impute age column, basically fill in the blanks, I'm filling the blanks based on their Priority Class means","4ecda6f1":"# This notebook is not aimed for a very high score but rather to explain the simplicity of things and methods used. ","ae2c935b":"# Blend","26130564":"# General Scores","0575e72a":"# Logistic Regression","6aed42ee":"# doing the same thing to the fare column","3116330b":"# Decision Tree (only to show actual trees)","a6078132":"# Beginner's Guide to Machine Learning using Titanic Data Set\n\n![](https:\/\/api.time.com\/wp-content\/uploads\/2015\/10\/titanic-cracker.jpg)\n","a6127935":"# Confusion Matrices for 4 models","b4bf3c11":"# Random Forest","80d161f7":"# XGBOOST","e70e30ea":"# applying these functions to both columns","bff553db":"# Gotta work a little bit in the Name column, \n# Creating a Title column based on the titles found in Name column, mapping them into numbers and finally removing the Name column","6b24aa46":"# Reading the 3 files from the Titanic Data Set","d7ab9545":"# Making several new features based on the size of the family","95c6f346":"# now the gender column, getting rid of the categorical strings and making new dummy columns","e94252e3":"# Predictions","36130675":"# Train Test Split","eb87904e":"# Applying the outliers to Age, SibSp, Parch, and Fare columns","fd23ca48":"# Appying the previous stuff to the test as well"}}