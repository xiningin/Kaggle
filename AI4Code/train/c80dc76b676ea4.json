{"cell_type":{"3d1cbb60":"code","65c589fa":"code","791f5d86":"code","991894bd":"code","f3a51210":"code","d760a56a":"code","4022545e":"code","d5fcbfbe":"code","6577a73f":"code","ee1b3d5a":"code","7add02c3":"code","4f6fbc7a":"code","9a37646d":"code","81f1806a":"code","8b558d56":"code","22feb812":"code","9d17f69e":"code","f1d06291":"code","2184cba7":"code","35f1a18d":"code","f9315b37":"code","5586dd96":"code","10db9ce6":"code","e4492ab9":"code","2cd322b5":"code","5ed631f9":"code","f7a277ff":"markdown","6bf9c70c":"markdown","c13e55f5":"markdown","316c049c":"markdown","fbcf144f":"markdown","0806f933":"markdown","4266b154":"markdown","b12bae6a":"markdown","87998e92":"markdown","ba63d355":"markdown","e0cb7bd9":"markdown","13e18930":"markdown","ad804fe4":"markdown","c70f4dbe":"markdown","9a67279a":"markdown","de8138bb":"markdown"},"source":{"3d1cbb60":"# import tensorflow as tf\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior() \nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split","65c589fa":"training_data = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntesting_data = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")","791f5d86":"flat_labels = training_data.iloc[:,0].values\nprint(\" Image Labels :  \",flat_labels[0:20])\nimages = training_data.iloc[:,1:].values\nimages = images.astype(np.float32)","991894bd":"# this function is used to update the plots for each epoch and error \ndef plt_dynamic(x, y, y_1, ax,ticks, title, colors=['b']):\n    ax.plot(x, y, 'b', label=\"Train Loss\")\n    ax.plot(x, y_1, 'r', label=\"Test Loss\")\n    if len(x)==1:\n        plt.legend()\n        plt.title(title)\n    plt.yticks(ticks)\n    fig.canvas.draw()\n# the function updates loss dynamically \n\n# class Dataset(object):\n#     def __init__(self, data):\n#         self.rows = len(data.values)\n#         self.images = data.iloc[:,1:].values\n#         self.images = self.images.astype(np.float32)\n#         self.images = np.multiply(self.images, 1.0 \/ 255.0)\n#         self.labels = np.array([np.array([int(i == l) for i in range(10)]) for l in data.iloc[:,0].values]) #one-hot\n#         self.index_in_epoch = 0\n#         self.epoch = 0\n#     def next_batch(self, batch_size):\n#         start = self.index_in_epoch\n#         self.index_in_epoch += batch_size\n#         if self.index_in_epoch > self.rows:\n#             self.epoch += 1\n#             perm = np.arange(self.rows)\n#             np.random.shuffle(perm)\n#             self.images = self.images[perm]\n#             self.labels = self.labels[perm]\n#             #next epoch\n#             start = 0\n#             self.index_in_epoch = batch_size\n#         end = self.index_in_epoch\n#         return self.images[start:end] , self.labels[start:end]\n","f3a51210":"one_hot_encoder = OneHotEncoder(sparse = False)\nflat_labels = flat_labels.reshape(len(flat_labels), 1)\nlabels = one_hot_encoder.fit_transform(flat_labels)\nlabels = labels.astype(np.uint8)","d760a56a":"# print(flat_labels[3])\n# labels[3]\nx_train, x_test, y_train, y_test = train_test_split(images, labels, test_size = 0.2, random_state = 0)","4022545e":"# Network Parameters\nn_hidden_1 = 512 # 1st layer number of neurons\nn_hidden_2 = 128 # 2nd layer number of neurons\nn_input = 784 # MNIST data input (img shape: 28*28)\nn_classes = 10 # MNIST total classes (0-9 digits)","d5fcbfbe":"x = tf.placeholder(tf.float32, [None, 784])\ny_ = tf.placeholder(tf.float32, [None, 10])\n\n# keep_prob: we will be using these placeholders when we use dropouts, while testing model\nkeep_prob = tf.placeholder(tf.float32)\nkeep_prob_input = tf.placeholder(tf.float32)","6577a73f":"\nweights_sgd = {\n    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=0.039, mean=0)),    #784x512 # sqrt(2\/(784+512)) = 0.039\n    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=0.055, mean=0)), #512x128 # sqrt(2\/(512+128)) = 0.055\n    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes],stddev=0.120, mean=0))  #128x10\n}","ee1b3d5a":"weights_relu = {\n    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=0.062, mean=0)),    #784x512\n    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=0.125, mean=0)), #512x128\n    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes],stddev=0.120, mean=0))  #128x10\n}\n","7add02c3":"biases = {\n    'b1': tf.Variable(tf.random_normal([n_hidden_1])),             #512x1\n    'b2': tf.Variable(tf.random_normal([n_hidden_2])),             #128x1\n    'out': tf.Variable(tf.random_normal([n_classes]))              #10x1\n}\n\n# Parameters\ntraining_epochs = 25\nlearning_rate = 0.001\nbatch_size = 250\ndisplay_step = 1","4f6fbc7a":"# https:\/\/leonardoaraujosantos.gitbooks.io\/artificial-inteligence\/content\/multi_layer_perceptron_mnist.html\n# Create model\ndef multilayer_perceptron(x, weights, biases):\n    # Use tf.matmul instead of \"*\" because tf.matmul can change it's dimensions on the fly (broadcast)\n    print( 'x:', x.get_shape(), 'W[h1]:', weights['h1'].get_shape(), 'b[h1]:', biases['b1'].get_shape())        \n    \n    # Hidden layer with Sigmoid activation\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) #(x*weights['h1']) + biases['b1']\n    layer_1 = tf.nn.sigmoid(layer_1)\n    print( 'layer_1:', layer_1.get_shape(), 'W[h2]:', weights['h2'].get_shape(), 'b[h2]:', biases['b2'].get_shape())        \n    \n    # Hidden layer with Sigmoid activation\n    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) # (layer_1 * weights['h2']) + biases['b2'] \n    layer_2 = tf.nn.sigmoid(layer_2)\n    print( 'layer_2:', layer_2.get_shape(), 'W[out]:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())        \n    \n    # Output layer with Sigmoid activation\n    out_layer = tf.matmul(layer_2, weights['out']) + biases['out'] # (layer_2 * weights['out']) + biases['out']    \n    out_layer = tf.nn.sigmoid(out_layer)\n    print('out_layer:',out_layer.get_shape())\n\n    return out_layer","9a37646d":"# Since we are using sigmoid activations in hiden layers we will be using weights that are initalized as weights_sgd\ny_sgd = multilayer_perceptron(x, weights_sgd, biases)\n\n# https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/nn\/softmax_cross_entropy_with_logits\ncost_sgd = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y_sgd, labels = y_))\n\n# https:\/\/github.com\/amitmac\/Question-Answering\/issues\/2\n# there are many optimizers available: https:\/\/www.tensorflow.org\/versions\/r1.2\/api_guides\/python\/train#Optimizers \noptimizer_adam = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_sgd)\noptimizer_sgdc = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost_sgd)\n\nwith tf.Session() as sess:\n    tf.global_variables_initializer().run()\n    fig,ax = plt.subplots(1,1)\n    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n    xs, ytrs, ytes = [], [], []\n    for epoch in range(training_epochs):\n        train_avg_cost = 0.\n        test_avg_cost = 0.\n        total_batch = int(training_data.shape[0]\/batch_size)\n        # Loop over all batches\n        for i in range(total_batch):\n            \n            batch_start = ( i * batch_size) % (x_train.shape[0] - batch_size)\n            batch_end = batch_start + batch_size \n            batch_xs = x_train[batch_start:batch_end]\n            batch_ys = y_train[batch_start:batch_end]\n            \n#             batch_xs, batch_ys = mnist.next_batch(batch_size)\n\n            # here c: corresponds to the parameter cost_sgd\n            # w : correspondse to the parameter weights_sgd\n            # c = sess.run() return the cost after every bath during train\n            # w = sess.run() return the weights that are modified after every batch through Back prop\n            # w is dict w = {'h1': updated h1 weight vector after the current batch,\n            #                'h2': updated h2 weight vector after the current batch, \n            #                'out': updated output weight vector after the current batch, \n            #                }\n            # you check these w matrix for every iteration, and check whats happening during back prop\n            #\n            # note: sess.run() returns parameter values based on the input parameters\n            # _, c, w = sess.run([optimizer_adam, cost_sgd,weights_sgd]) it returns three parameters\n            # _, c = sess.run([optimizer_adam, cost_sgd ]) it returns two parameters\n            # _ = sess.run([optimizer_adam]) it returns one paramter (for the input optimizer it return none)\n            # c = sess.run([cost_sgd]) it returns one paramter (for the input cost return error after the batch)\n\n            # feed_dict={x: batch_xs, y_: batch_ys} here x, y_ should be placeholders\n            # x, y_ are the input parameters on which the models gets trained\n\n            # here we use AdamOptimizer\n            _, c, w = sess.run([optimizer_adam, cost_sgd,weights_sgd], feed_dict={x: batch_xs, y_: batch_ys})\n            train_avg_cost += c \/ total_batch\n            c = sess.run(cost_sgd, feed_dict={x: x_test, y_: y_test})\n            test_avg_cost += c \/ total_batch\n\n        xs.append(epoch)\n        ytrs.append(train_avg_cost)\n        ytes.append(test_avg_cost)\n        plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.3, 1.8, step=0.04), \"input-sigmoid(512)-sigmoid(128)-sigmoid(output)-AdamOptimizer\")\n\n        if epoch%display_step == 0:\n            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n    plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.3, 1.8, step=0.04), \"input-sigmoid(512)-sigmoid(128)-sigmoid(output)-AdamOptimizer\")\n\n    # we are calculating the final accuracy on the test data\n    correct_prediction = tf.equal(tf.argmax(y_sgd,1), tf.argmax(y_,1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print(\"Accuracy:\", accuracy.eval({x: x_test, y_: y_test}))","81f1806a":"import seaborn as sns\nh1_w = w['h1'].flatten().reshape(-1,1)\nh2_w = w['h2'].flatten().reshape(-1,1)\nout_w = w['out'].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.subplot(1, 3, 1)\nplt.title(\"Weight matrix\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Weight matrix \")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Weight matrix \")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","8b558d56":"# We can now launch the model in an InteractiveSession\n\n# We first have to create an operation to initialize the variables we created:\n# https:\/\/github.com\/amitmac\/Question-Answering\/issues\/2\n\n# Note: make sure you initialize variables.\n\nwith tf.Session() as sess:\n    tf.global_variables_initializer().run()\n    fig,ax = plt.subplots(1,1)\n    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n    xs, ytrs, ytes = [], [], []\n    for epoch in range(training_epochs):\n        train_avg_cost = 0.\n        test_avg_cost = 0.\n        total_batch = int(x_train.shape[0]\/batch_size)\n\n        # Loop over all batches\n        for i in range(total_batch):\n            batch_start = ( i * batch_size) % (x_train.shape[0] - batch_size)\n            batch_end = batch_start + batch_size \n            batch_xs = x_train[batch_start:batch_end]\n            batch_ys = y_train[batch_start:batch_end]\n#             batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n\n            # here we use GradientDescentOptimizer\n            _, c, w = sess.run([optimizer_sgdc, cost_sgd, weights_sgd], feed_dict={x: batch_xs, y_: batch_ys})\n            train_avg_cost += c \/ total_batch\n            c = sess.run(cost_sgd, feed_dict={x: x_test, y_: y_test})\n            test_avg_cost += c \/ total_batch\n\n        xs.append(epoch)\n        ytrs.append(train_avg_cost)\n        ytes.append(test_avg_cost)\n        plt_dynamic(xs, ytrs, ytes, ax, np.arange(2, 2.6, step=0.05), \"input-sigmoid(512)-sigmoid(128)-sigmoid(output)-GradientDescentOptimizer\")\n\n        if epoch%display_step == 0:\n            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n    plt_dynamic(xs, ytrs, ytes, ax, np.arange(2, 2.6, step=0.05), \"input-sigmoid(512)-sigmoid(128)-sigmoid(output)-GradientDescentOptimizer\")\n\n    # we are calculating the final accuracy on the test data\n    correct_prediction = tf.equal(tf.argmax(y_sgd,1), tf.argmax(y_,1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print(\"Accuracy:\", accuracy.eval({x: x_test, y_: y_test}))","22feb812":"h1_w = w['h1'].flatten().reshape(-1,1)\nh2_w = w['h2'].flatten().reshape(-1,1)\nout_w = w['out'].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.subplot(1, 3, 1)\nplt.title(\"Weight matrix\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Weight matrix \")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Weight matrix \")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","9d17f69e":"# https:\/\/leonardoaraujosantos.gitbooks.io\/artificial-inteligence\/content\/multi_layer_perceptron_mnist.html\n# Create model\ndef multilayer_perceptron_relu(x, weights, biases):\n    # Use tf.matmul instead of \"*\" because tf.matmul can change it's dimensions on the fly (broadcast)\n    print( 'x:', x.get_shape(), 'W[h1]:', weights['h1'].get_shape(), 'b[h1]:', biases['b1'].get_shape())        \n    \n    # Hidden layer with ReLu activation\n    # https:\/\/en.wikipedia.org\/wiki\/Rectifier_(neural_networks)\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) #(x*weights['h1']) + biases['b1']\n    layer_1 = tf.nn.relu(layer_1)\n    print( 'layer_1:', layer_1.get_shape(), 'W[h2]:', weights['h2'].get_shape(), 'b[h2]:', biases['b2'].get_shape())        \n    \n    # Hidden layer with ReLu activation\n    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) # (layer_1 * weights['h2']) + biases['b2'] \n    layer_2 = tf.nn.relu(layer_2)\n    print( 'layer_2:', layer_2.get_shape(), 'W[out]:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())        \n    \n    # Output layer with Sigmoid activation\n    out_layer = tf.matmul(layer_2, weights['out']) + biases['out'] # (layer_2 * weights['out']) + biases['out']    \n    out_layer = tf.nn.sigmoid(out_layer)\n    print('out_layer:',out_layer.get_shape())\n\n    return out_layer","f1d06291":"# Since we are using Relu activations in hiden layers we will be using weights that are initalized as weights_relu\nyrelu = multilayer_perceptron_relu(x, weights_relu, biases)\n\n# https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/nn\/softmax_cross_entropy_with_logits\ncost_relu = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = yrelu, labels = y_))\n# https:\/\/github.com\/amitmac\/Question-Answering\/issues\/2\n# there are many optimizers available: https:\/\/www.tensorflow.org\/versions\/r1.2\/api_guides\/python\/train#Optimizers \noptimizer_relu_adam = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_relu)\noptimizer_relu_sgdc = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost_relu)\nwith tf.Session() as sess:\n    tf.global_variables_initializer().run()\n    \n    fig,ax = plt.subplots(1,1)\n    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n    xs, ytrs, ytes = [], [], []\n    for epoch in range(training_epochs):\n        train_avg_cost = 0.\n        test_avg_cost = 0.\n        total_batch = int(x_train.shape[0]\/batch_size)\n\n        # Loop over all batches\n        for i in range(total_batch):\n            batch_start = ( i * batch_size) % (x_train.shape[0] - batch_size)\n            batch_end = batch_start + batch_size \n            batch_xs = x_train[batch_start:batch_end]\n            batch_ys = y_train[batch_start:batch_end]\n            \n\n            # here we use AdamOptimizer\n            _, c, w = sess.run([optimizer_relu_adam, cost_relu, weights_relu], feed_dict={x: batch_xs, y_: batch_ys})\n            train_avg_cost += c \/ total_batch\n            c = sess.run(cost_relu, feed_dict={x: x_test, y_: y_test})\n            test_avg_cost += c \/ total_batch\n\n        xs.append(epoch)\n        ytrs.append(train_avg_cost)\n        ytes.append(test_avg_cost)\n        plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.3, 1.8, step=0.04), \"input-ReLu(512)-ReLu(128)-sigmoid(output)-AdamOptimizer\")\n\n        if epoch%display_step == 0:\n            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n\n    # plot final results\n    plt_dynamic(xs, ytrs, ytes, ax,np.arange(1.3, 1.8, step=0.04), \"input-ReLu(512)-ReLu(128)-sigmoid(output)-AdamOptimizer\")\n\n    # we are calculating the final accuracy on the test data\n    correct_prediction = tf.equal(tf.argmax(yrelu,1), tf.argmax(y_,1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print(\"Accuracy:\", accuracy.eval({x: x_test, y_: y_test}))","2184cba7":"h1_w = w['h1'].flatten().reshape(-1,1)\nh2_w = w['h2'].flatten().reshape(-1,1)\nout_w = w['out'].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.subplot(1, 3, 1)\nplt.title(\"Weight matrix\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Weight matrix \")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Weight matrix \")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","35f1a18d":"# We can now launch the model in an InteractiveSession\n\n# We first have to create an operation to initialize the variables we created:\n# https:\/\/github.com\/amitmac\/Question-Answering\/issues\/2\n\n# Note: make sure you initialize variables after AdamOptimizer\n\nwith tf.Session() as sess:\n    tf.global_variables_initializer().run()\n    fig,ax = plt.subplots(1,1)\n    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n    xs, ytrs, ytes = [], [], []\n    for epoch in range(training_epochs):\n        train_avg_cost = 0.\n        test_avg_cost = 0.\n        total_batch = int(x_train.shape[0]\/batch_size)\n\n        # Loop over all batches\n        for i in range(total_batch):\n            batch_start = ( i * batch_size) % (x_train.shape[0] - batch_size)\n            batch_end = batch_start + batch_size \n            batch_xs = x_train[batch_start:batch_end]\n            batch_ys = y_train[batch_start:batch_end]\n\n            # here we use GradientDescentOptimizer\n            _, c, w = sess.run([optimizer_relu_sgdc, cost_relu, weights_relu], feed_dict={x: batch_xs, y_: batch_ys})\n            train_avg_cost += c \/ total_batch\n            c = sess.run(cost_relu, feed_dict={x: x_test, y_: y_test})\n            test_avg_cost += c \/ total_batch\n\n        xs.append(epoch)\n        ytrs.append(train_avg_cost)\n        ytes.append(test_avg_cost)\n        plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.5, 2.4, step=0.05), \"input-ReLu(512)-ReLu(128)-sigmoid(output)-GradientDescentOptimizer\")\n\n        if epoch%display_step == 0:\n            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n\n    # plot final results\n    plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.5, 2.4, step=0.05), \"input-ReLu(512)-ReLu(128)-sigmoid(output)-GradientDescentOptimizer\")\n\n    # we are calculating the final accuracy on the test data\n    correct_prediction = tf.equal(tf.argmax(yrelu,1), tf.argmax(y_,1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print(\"Accuracy:\", accuracy.eval({x: x_test, y_: y_test}))","f9315b37":"h1_w = w['h1'].flatten().reshape(-1,1)\nh2_w = w['h2'].flatten().reshape(-1,1)\nout_w = w['out'].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.subplot(1, 3, 1)\nplt.title(\"Weight matrix\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Weight matrix \")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Weight matrix \")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","5586dd96":"# https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/nn\/batch_normalization\n# https:\/\/r2rt.com\/implementing-batch-normalization-in-tensorflow.html\nepsilon = 1e-3\ndef multilayer_perceptron_batch(x, weights, biases):\n    # Use tf.matmul instead of \"*\" because tf.matmul can change it's dimensions on the fly (broadcast)\n    print( 'x:', x.get_shape(), 'W[h1]:', weights['h1'].get_shape(), 'b[h1]:', biases['b1'].get_shape())        \n    \n    ############################################################\n    # Hidden layer with Sigmoid activation and batch normalization\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) #(x*weights['h1']) + biases['b1']\n    \n    # https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/nn\/moments\n    # Calculate the mean and variance of x.\n    batch_mean_1, batch_var_1 = tf.nn.moments(layer_1,[0])\n    \n    scale_1 = tf.Variable(tf.ones([n_hidden_1]))\n    beta_1 = tf.Variable(tf.zeros([n_hidden_1]))\n    \n    # https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/nn\/batch_normalization\n    layer_1 = tf.nn.batch_normalization(layer_1, batch_mean_1, batch_var_1, beta_1, scale_1, epsilon)\n    layer_1 = tf.nn.sigmoid(layer_1)\n    \n    print( 'layer_1:', layer_1.get_shape(), 'W[h2]:', weights['h2'].get_shape(), 'b[h2]:', biases['b2'].get_shape())        \n    \n    #####################################################################################\n    \n    # Hidden layer with Sigmoid activation and batch normalization\n    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) # (layer_1 * weights['h2']) + biases['b2'] \n    \n    # https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/nn\/moments\n    # Calculate the mean and variance of x.\n    batch_mean_2, batch_var_2 = tf.nn.moments(layer_2, [0])\n    \n    scale_2 = tf.Variable(tf.ones([n_hidden_2]))\n    beta_2 = tf.Variable(tf.zeros([n_hidden_2]))\n    \n    layer_2 = tf.nn.batch_normalization(layer_2, batch_mean_2, batch_var_2, beta_2, scale_2, epsilon)\n    layer_2 = tf.nn.sigmoid(layer_2)\n    print( 'layer_2:', layer_2.get_shape(), 'W[out]:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())        \n    \n    ######################################################################################\n    \n    # output layer with Sigmoid activation \n    out_layer = tf.matmul(layer_2, weights['out']) + biases['out'] # (layer_2 * weights['out']) + biases['out']    \n    out_layer = tf.nn.sigmoid(out_layer)\n    print('out_layer:',out_layer.get_shape())\n\n    return out_layer","10db9ce6":"# Since we are using sigmoid activations in hiden layers we will be using weights that are initalized as weights_sgd\nybatch = multilayer_perceptron_batch(x, weights_sgd, biases)\n\n# https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/nn\/softmax_cross_entropy_with_logits\ncost_batch = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = ybatch, labels = y_))\n# https:\/\/github.com\/amitmac\/Question-Answering\/issues\/2\n# there are many optimizers available: https:\/\/www.tensorflow.org\/versions\/r1.2\/api_guides\/python\/train#Optimizers \noptimizer_batch_adam = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_batch)\noptimizer_batch_sgdc = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost_batch)\nwith tf.Session() as sess:\n    tf.global_variables_initializer().run()\n    fig,ax = plt.subplots(1,1)\n    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n    xs, ytrs, ytes = [], [], []\n    for epoch in range(training_epochs):\n        train_avg_cost = 0.\n        test_avg_cost = 0.\n        total_batch = int(x_train.shape[0]\/batch_size)\n\n        # Loop over all batches\n        for i in range(total_batch):\n            batch_start = ( i * batch_size) % (x_train.shape[0] - batch_size)\n            batch_end = batch_start + batch_size \n            batch_xs = x_train[batch_start:batch_end]\n            batch_ys = y_train[batch_start:batch_end]\n\n            # here we use AdamOptimizer\n            _, c, w = sess.run([optimizer_batch_adam, cost_batch, weights_sgd], feed_dict={x: batch_xs, y_: batch_ys})\n            train_avg_cost += c \/ total_batch\n            c = sess.run(cost_batch, feed_dict={x: x_test, y_: y_test})\n            test_avg_cost += c \/ total_batch\n\n        xs.append(epoch)\n        ytrs.append(train_avg_cost)\n        ytes.append(test_avg_cost)\n        plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.3, 1.8, step=0.04), \"input-Sigmoid(BN(512))-Sigmoid(BN(128))-Sigmoid(output)-AdamOptimizer\")\n\n        if epoch%display_step == 0:\n            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n\n    # plot final results\n    plt_dynamic(xs, ytrs, ytes, ax, np.arange(1.3, 1.8, step=0.04), \"input-Sigmoid(BN(512))-Sigmoid(BN(128))-Sigmoid(output)-AdamOptimizer\")\n\n    # we are calculating the final accuracy on the test data\n    correct_prediction = tf.equal(tf.argmax(ybatch,1), tf.argmax(y_,1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print(\"Accuracy:\", accuracy.eval({x: x_test, y_: y_test}))\n    ","e4492ab9":"h1_w = w['h1'].flatten().reshape(-1,1)\nh2_w = w['h2'].flatten().reshape(-1,1)\nout_w = w['out'].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.subplot(1, 3, 1)\nplt.title(\"Weight matrix\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Weight matrix \")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Weight matrix \")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","2cd322b5":"# https:\/\/leonardoaraujosantos.gitbooks.io\/artificial-inteligence\/content\/multi_layer_perceptron_mnist.html\n# Create model\ndef multilayer_perceptron_dropout(x, weights, biases):\n    # Use tf.matmul instead of \"*\" because tf.matmul can change it's dimensions on the fly (broadcast)\n    print( 'x:', x.get_shape(), 'W[h1]:', weights['h1'].get_shape(), 'b[h1]:', biases['b1'].get_shape())        \n    # we are adding a drop out layer after input layers with parameter keep_prob_input\n    \n    # Hidden layer with ReLu activation\n    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) #(x*weights['h1']) + biases['b1']\n    layer_1 = tf.nn.relu(layer_1)\n    # we are adding a drop out layer after the first hidden layer with parameter keep_prob\n    layer_1_drop = tf.nn.dropout(layer_1, keep_prob)\n    \n    print( 'layer_1:', layer_1.get_shape(), 'W[h2]:', weights['h2'].get_shape(), 'b[h2]:', biases['b2'].get_shape())        \n    \n    # Hidden layer with ReLu activation\n    layer_2 = tf.add(tf.matmul(layer_1_drop, weights['h2']), biases['b2']) # (layer_1 * weights['h2']) + biases['b2'] \n    layer_2 = tf.nn.relu(layer_2)\n    # we are adding a drop out layer after the first hidden layer with parameter keep_prob\n    layer_2_drop = tf.nn.dropout(layer_2, keep_prob)\n    print( 'layer_2:', layer_2.get_shape(), 'W[out]:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())        \n    \n    # Output layer with Sigmoid activation\n    out_layer = tf.matmul(layer_2_drop, weights['out']) + biases['out'] # (layer_2 * weights['out']) + biases['out']    \n    out_layer = tf.nn.sigmoid(out_layer)\n    print('out_layer:',out_layer.get_shape())\n\n    return out_layer","5ed631f9":"# Since we are using Relu activations in hiden layers we will be using weights that are initalized as weights_relu\nydrop = multilayer_perceptron_dropout(x, weights_relu, biases)\n\n# https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/nn\/softmax_cross_entropy_with_logits\ncost_drop = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = ydrop, labels = y_))\n# https:\/\/github.com\/amitmac\/Question-Answering\/issues\/2\n# there are many optimizers available: https:\/\/www.tensorflow.org\/versions\/r1.2\/api_guides\/python\/train#Optimizers \noptimizer_drop_adam = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_drop)\noptimizer_drop_sgdc = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost_drop)\n\nwith tf.Session() as sess:\n    tf.global_variables_initializer().run()\n    fig,ax = plt.subplots(1,1)\n    ax.set_xlabel('epoch') ; ax.set_ylabel('Soft Max Cross Entropy loss')\n    xs, ytrs, ytes = [], [], []\n    for epoch in range(training_epochs):\n        train_avg_cost = 0.\n        test_avg_cost = 0.\n        total_batch = int(x_train.shape[0]\/batch_size)\n\n        # Loop over all batches\n        for i in range(total_batch):\n            \n            batch_start = ( i * batch_size) % (x_train.shape[0] - batch_size)\n            batch_end = batch_start + batch_size \n            batch_xs = x_train[batch_start:batch_end]\n            batch_ys = y_train[batch_start:batch_end]\n\n            \n            # here we use AdamOptimizer\n            _, c, w = sess.run([optimizer_drop_adam, cost_drop, weights_relu], feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.5})\n            train_avg_cost += c \/ total_batch\n            c = sess.run(cost_drop, feed_dict={x: x_test, y_: y_test,  keep_prob: 1.0})\n            test_avg_cost += c \/ total_batch\n\n        xs.append(epoch)\n        ytrs.append(train_avg_cost)\n        ytes.append(test_avg_cost)\n        plt_dynamic(xs, ytrs, ytes, ax,np.arange(1, 1.8, step=0.05), \"input-ReLu(512)-Dropout-ReLu(128)-Dropout-Sigmoid(output)-AdamOptimizer\")\n\n        if epoch%display_step == 0:\n            print(\"Epoch:\", '%04d' % (epoch+1), \"train cost={:.9f}\".format(train_avg_cost), \"test cost={:.9f}\".format(test_avg_cost))\n\n    # plot final results\n    plt_dynamic(xs, ytrs, ytes, ax,np.arange(1, 1.8, step=0.05), \"input-ReLu(512)-Dropout-ReLu(128)-Dropout-Sigmoid(output)-AdamOptimizer\")\n\n    # we are calculating the final accuracy on the test data\n    correct_prediction = tf.equal(tf.argmax(ydrop,1), tf.argmax(y_,1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print(\"Accuracy:\", accuracy.eval({x: x_test, y_: y_test, keep_prob: 1.0 }))","f7a277ff":"### Defining network Architecture:","6bf9c70c":"# Model 4: Input - ReLu(512) - Dropout - ReLu(128)- Dropout -Sigmoid(output)","c13e55f5":"### Model 3 with AdamOptimizer","316c049c":"## Loading the dataset ","fbcf144f":"### Model 1 with GradientDescentOptimizer ","0806f933":"## ReLU with He Normal initialization\n- If we sample weights from a normal distribution N(0,\u03c3) we satisfy this condition with \u03c3=\u221a(2\/(ni). \n- h1 =>  \u03c3=\u221a(2\/(fan_in+1) = 0.062  => N(0,\u03c3) = N(0,0.062)\n- h2 =>  \u03c3=\u221a(2\/(fan_in+1) = 0.125  => N(0,\u03c3) = N(0,0.125)\n- out =>  \u03c3=\u221a(2\/(fan_in+1) = 0.120  => N(0,\u03c3) = N(0,0.120)\n","4266b154":"### Model 2 with Adam optimizer:","b12bae6a":"### Model 2 with GradientDescentOptimmizer ","87998e92":"### Plotting violin plots of weights to performm a sanity check\n- cheking wheather weights are not being blown out of proprotion. ","ba63d355":"### Model 4 with AdamOptimizer","e0cb7bd9":"## Implementing Tensorflow from scratch on MNIST Dataset to get a clear understanding of how different optimizers, Activation functions behave.\n\n\n### Activation Functions used :\n- Sigmoid \n- ReLU\n\n\n#### Implementing methadologies like \n- Batch Normalization \n- Dropout \n\n\n#### 4 Models used throughout the kernel :\n- Model 1: input (784) - sigmoid(512) - sigmoid(128) - softmax(output 10)\n- Model 2: input (784) - ReLu(512) - ReLu(128) - sigmoid(output 10)\n- Model 3: Input - Sigmoid(BatchNormalization(512)) - Sigmoid(BatchNormalization(128))- Sigmoid(output)\n- Model 4: Input - ReLu(512) - Dropout - ReLu(128)- Dropout -Sigmoid(output)\n\n\n### Using ADAM optimizer and GradientDescent optimizer to illustrate the significant difference in convergance with respect to epochs. \n\n\n","13e18930":"# Model 1: input (784) - sigmoid(512) - sigmoid(128) - softmax(output 10)","ad804fe4":"# Model 3: Input - Sigmoid(BatchNormalization(512)) - Sigmoid(BatchNormalization(128))- Sigmoid(output)","c70f4dbe":"## Sigmoid with Xavier\/Glorot Normal initialization.\n-  If we sample weights from a normal distribution N(0,\u03c3) we satisfy this condition with \u03c3=\u221a(2\/(ni+ni+1). \n- h1 =>  \u03c3=\u221a(2\/(fan_in+fan_out+1) = 0.039  => N(0,\u03c3) = N(0,0.039)\n- h2 =>  \u03c3=\u221a(2\/(fan_in+fan_out+1) = 0.055  => N(0,\u03c3) = N(0,0.055)\n- out =>  \u03c3=\u221a(2\/(fan_in+fan_out+1) = 0.120  => N(0,\u03c3) = N(0,0.120)\n","9a67279a":"# Model 2: input (784) - ReLu(512) - ReLu(128) - sigmoid(output 10)","de8138bb":"### Model 1 with With Adam optimizer:"}}