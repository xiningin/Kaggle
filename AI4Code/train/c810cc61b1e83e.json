{"cell_type":{"7cd60e73":"code","7e7899a9":"code","83cdd127":"code","7e8693a9":"code","eaf37b40":"code","3fc3e9c0":"code","e5a3886f":"code","a3a11129":"code","b60fa7a8":"code","322a8e18":"code","e1d26538":"code","f10b4452":"code","deaa5cc7":"code","baf97089":"code","bf2673ee":"code","75d96a2a":"code","5be8a1fb":"code","7d04151d":"code","1eb0b2de":"code","db7267e5":"code","3c042693":"code","5f4f4670":"code","7adccceb":"code","4f37e667":"markdown","2457e3c9":"markdown","01b3dc6b":"markdown","1530310e":"markdown"},"source":{"7cd60e73":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport datatable as dt  # pip install datatable\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport random\n\nfrom matplotlib.lines import Line2D\n\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import precision_recall_curve\n\n\nimport optuna\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\n# import warnings\n# warnings.simplefilter(action='ignore', category=UserWarning)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('..\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n        \n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7e7899a9":"%%time\n# Read the data\ntrain = dt.fread(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\").to_pandas().set_index(\"id\")\ntest = dt.fread(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\").to_pandas().set_index(\"id\")\n","83cdd127":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","7e8693a9":"train = reduce_memory_usage(train, verbose=True)\ntest = reduce_memory_usage(test, verbose=True)","eaf37b40":"print(\"(train, test) na --> \",(train.isna().sum().sum(), test.isna().sum().sum()))","3fc3e9c0":"is_na_train_df = train.drop(columns=\"claim\").isna().sum(axis = 1)\nprint(is_na_train_df.shape)\n\nis_na_test_df = test.isna().sum(axis = 1)\nprint(is_na_test_df.shape)","e5a3886f":"is_na_train_sum = train.drop(columns=\"claim\").isna().sum(axis=1)\nprint(is_na_train_sum.shape)\n\nis_na_test_sum = test.isna().sum(axis=1)\nprint(is_na_test_sum.shape)","a3a11129":"# from https:\/\/www.kaggle.com\/sgiuri\/sep21tp-na-feature-importance \nna_fi = ['f74', 'f91', 'f107', 'f53', 'f7', 'f12', 'f48', 'f19', 'f100', 'f69', 'f18', 'f112', 'f66', 'f17', 'f113']\nis_na_train_df = train.drop(columns=\"claim\").isna().loc[:,na_fi].astype(int)\nprint(is_na_train_df.shape)\n\nis_na_test_df = test.isna().loc[:,na_fi].astype(int)\nprint(is_na_test_df.shape)","b60fa7a8":"train = train.join(is_na_train_df, rsuffix='_isNa')\ntest = test.join(is_na_test_df, rsuffix='_isNa')","322a8e18":"train[\"isNA\"] =is_na_train_sum\nprint(train.shape)\ntest[\"isNA\"] = is_na_test_sum\nprint(test.shape)\n\n# train = train.head(1000)\n# test = test.head(1000)\n# if len(train == 1000):\n#     print(\"Train test reduced to increase speed in try and test \")\n    ","e1d26538":"x_Mm_scaler = MinMaxScaler()\nX = pd.DataFrame(x_Mm_scaler.fit_transform(train.drop(\"claim\", axis=1)),\n                 columns=train.drop(\"claim\", axis=1).columns)\ny = train.claim.astype(int)\nX_test = pd.DataFrame(x_Mm_scaler.transform(test), columns=test.columns)","f10b4452":"imputer_zeros = SimpleImputer(strategy=\"median\")\nX = pd.DataFrame(imputer_zeros.fit_transform(train.drop(\"claim\", axis=1)),\n                 columns=train.drop(\"claim\", axis=1).columns)\nX_test = pd.DataFrame(imputer_zeros.transform(test), columns=test.columns)\nX = pd.DataFrame(x_Mm_scaler.fit_transform(X),\n                 columns=train.drop(\"claim\", axis=1).columns)\nX_test = pd.DataFrame(x_Mm_scaler.transform(X_test), columns=test.columns)\nprint(\"(train, test) na --> \",(X.isna().sum().sum(), X_test.isna().sum().sum()))","deaa5cc7":"# X = reduce_memory_usage(X, verbose=True)\n# X_test = reduce_memory_usage(X_test, verbose=True)","baf97089":"# %%time\n# def objective(trial):\n#     train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.33, random_state=int(SEED), shuffle=True)\n#     train_pool = Pool(train_x, train_y)\n#     test_pool = Pool(test_x, test_y)\n    \n#     # Parameters\n#     params = {\n#         'iterations' : trial.suggest_int('iterations', 50, 300),                         \n#         'depth' : trial.suggest_int('depth', 4, 10),                                       \n#         'learning_rate' : trial.suggest_loguniform('learning_rate', 0.01, 0.3),               \n#         'random_strength' :trial.suggest_int('random_strength', 0, 100),                       \n#         'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n#         'learning_rate' :trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n#         'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter'])\n#     }\n#     # Learning\n#     model = cat.CatBoostClassifier(\n#         loss_function=\"Logloss\",\n#         eval_metric=\"AUC\",\n#         task_type=\"GPU\",\n#         l2_leaf_reg=50,\n#         random_seed=SEED,\n#         border_count=64,\n#         **params\n#     )        \n#     model.fit(train_pool)\n#     # Predict\n#     preds = model.predict(test_pool)\n#     pred_labels = np.rint(preds)\n#     y_pred_boot = resample(pred_labels, n_samples = len(train_y))\n#     # Evaluation\n#     ROC_AUC_Score = roc_auc_score(train_y, y_pred_boot)\n#     print('ROC AUC Score of CatBoost =', ROC_AUC_Score)\n#     return ROC_AUC_Score","bf2673ee":"def objective(trial):\n    train_x, valid_x, train_y, valid_y = train_test_split(X,y, test_size=0.3)\n    param = {\n            'iterations' : trial.suggest_int('iterations', 50, 300),                         \n            'depth' : trial.suggest_int('depth', 4, 10),                                       \n            'learning_rate' : trial.suggest_loguniform('learning_rate', 0.01, 0.3),               \n            'random_strength' :trial.suggest_int('random_strength', 0, 100),                       \n            'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n            'learning_rate' :trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n            'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter'])\n        }\n        # Learning\n    gbm = CatBoostClassifier(\n        loss_function=\"Logloss\",\n        eval_metric=\"AUC\",\n        task_type=\"GPU\",\n        l2_leaf_reg=50,\n        random_seed=42,\n        border_count=64,\n        **param\n    )     \n#     param = {'iterations':trial.suggest_int(\"iterations\", 4000, 25000),\n#               'od_wait':trial.suggest_int('od_wait', 500, 2300),\n#              'loss_function':'MultiClass',\n#               'task_type':\"GPU\",\n#               'eval_metric':'MultiClass',\n#               'leaf_estimation_method':'Newton',\n#               'bootstrap_type':  trial.suggest_categorical('bootstrap_type',['Bernoulli','Bayesian']),\n#               'learning_rate' : trial.suggest_uniform('learning_rate',0.02,1),\n#               'reg_lambda': trial.suggest_uniform('reg_lambda',1e-5,100),\n#               'random_strength': trial.suggest_uniform('random_strength',10,50),\n#               'depth': trial.suggest_int('depth',1,15),\n#               'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n#               'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n#                }\n\n#     if param[\"bootstrap_type\"] == \"Bayesian\":\n#         param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n#     elif param[\"bootstrap_type\"] == \"Bernoulli\":\n#         param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n\n    gbm = CatBoostClassifier(**param)\n\n    gbm.fit(train_x, train_y, eval_set=[(valid_x, valid_y)], verbose=0, early_stopping_rounds=100)\n\n    preds = gbm.predict(valid_x)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(valid_y, pred_labels)\n    return accuracy","75d96a2a":"%%time\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\ntime_limit = 3600 * 3\nn_trials = 100\n\n#optuna.logging.set_verbosity(optuna.logging.WARNING)\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=n_trials, timeout=time_limit)\n # Showing optimization results\n    \nprint('Number of finished trials:', len(study.trials))\nprint('Best trial parameters:', study.best_trial.params)\nprint('Best score:', study.best_value)\n\nprint(\"Number of finished trials: {}\".format(len(study.trials)))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n","5be8a1fb":"ctb_params = study.best_params\n# xgb_params = {'n_estimators': 10000, \n#               'learning_rate': 0.08625196792060146, \n#               'subsample': 0.5959773829663169, \n#               'colsample_bytree': 0.7603045913120982, \n#               'max_depth': 7, 'booster': 'gbtree', \n#               'tree_method': 'gpu_hist', \n#               'reg_lambda': 74.60593770387143, \n#               'reg_alpha': 33.38858560681472, \n#               'random_state': 42, \n#               'n_jobs': 4}","7d04151d":"def strati_fit(X, y, X_test, \n               splits=6, random_state=42,\n               model = CatBoostClassifier(loss_function=\"Logloss\",\n                                eval_metric=\"AUC\",\n                                task_type=\"GPU\",\n                                l2_leaf_reg=50,\n                                random_seed=42,\n                                border_count=64,\n                                **ctb_params)):\n    \n    splits = splits\n    skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=random_state)\n    oof_preds = np.zeros((X.shape[0],))\n    preds = 0\n    train_preds = 0\n    model_fi = 0\n    total_mean_rmse = 0\n    total_mean_roc_auc_score = 0\n    \n    \n    for fold, (train_indicies, valid_indicies) in enumerate(skf.split(X,y)):\n\n        X_train, X_valid = X.loc[train_indicies], X.loc[valid_indicies]\n        y_train, y_valid = y.loc[train_indicies], y.loc[valid_indicies]\n        print(fold, f\"X_train = {X_train.shape} - y_train: {y_train.shape}\")\n        print(fold, f\"X_valid = {X_valid.shape} - y_valid: {y_valid.shape}\")\n        \n        model.fit(X_train, y_train,\n              eval_set=[(X_valid, y_valid)],\n              #eval_metric=\"auc\",\n              early_stopping_rounds=100,\n              verbose=False)\n        # print(\"fitted\")\n        preds += (model.predict_proba(X_test))[:,1] \/ splits\n        train_preds += (model.predict_proba(X))[:,1] \/ splits\n        print(train_preds.shape)\n        # print(\"preds ok\")\n        model_fi += model.feature_importances_\n        # print(\"model_fi ok\")\n        oof_preds[valid_indicies] = model.predict_proba(X_valid)[:,1]\n        oof_preds[oof_preds < 0] = 0\n    #     fold_rmse = np.sqrt(mean_squared_error(y_scaler.inverse_transform(np.array(y_valid).reshape(-1,1)), y_scaler.inverse_transform(np.array(oof_preds[valid_idx]).reshape(-1,1))))\n        fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_indicies]))\n        fold_roc_auc_score = roc_auc_score(y_valid, oof_preds[valid_indicies])\n        \n        print(f\"\\nFold {fold} ROC AUC Score: {fold_roc_auc_score}\")\n        \n        print(f\"Fold {fold} RMSE: {fold_rmse}\")\n        total_mean_rmse += fold_rmse \/ splits\n        total_mean_roc_auc_score += fold_roc_auc_score \/ splits\n    return preds, model_fi, total_mean_rmse, total_mean_roc_auc_score, train_preds\n    ","1eb0b2de":"random_states = [0,3,42,69,666]\npredictions = pd.DataFrame()\npredictions[\"id\"] = test.index\ntrain_predictions = pd.DataFrame()\ntrain_predictions[\"id\"] = X.index\n\nstudy = pd.DataFrame()\n\nfor random_state in random_states:\n    \n    preds, model_fi, total_mean_rmse, total_mean_roc_auc_score, train_preds = strati_fit(X, y, X_test, \n           splits=7, random_state=random_state)\n    predictions[random_state] = preds\n    train_predictions[random_state] = train_preds\n    study[random_state] = model_fi, total_mean_rmse, total_mean_roc_auc_score\n","db7267e5":"study.to_csv('ctb_study.csv', index=False, header=study.columns)\nstudy.head()","3c042693":"train_predictions.to_csv('ctb_train_predictions.csv', index=False, header=train_predictions.columns)\ntrain_predictions.head()","5f4f4670":"predictions.to_csv('ctb_submission_pre_ensamble.csv', index=False, header=predictions.columns)\npredictions.head()","7adccceb":"submissions = pd.DataFrame()\nsubmissions[\"id\"] = predictions.id\nsubmissions[\"claim\"] = list(predictions.drop(columns=\"id\").mean(axis=1))\n\nsubmissions.to_csv('submission.csv', index=False, header=predictions.columns)\nsubmissions.head()","4f37e667":"## Data preparation: Siple Imputer + NA to median","2457e3c9":"# Libraries and Data import","01b3dc6b":"Hello everybody.","1530310e":"# Memory reducing\ntaken from: https:\/\/www.kaggle.com\/bextuychiev\/how-to-work-w-million-row-datasets-like-a-pro"}}