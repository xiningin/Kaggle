{"cell_type":{"5b382fc0":"code","cbd8fb69":"code","821e829c":"code","e2c4eb98":"code","917c8674":"code","d3bcf32b":"code","cad5a591":"code","77d577e3":"code","d82a4a2f":"code","83735bf8":"code","ef31b05b":"code","eab617dc":"code","40df297e":"code","66654c88":"code","13fd84ce":"code","bab2e390":"code","ee6887c8":"code","61a9951a":"code","abb322b0":"code","f7c6894c":"code","0c15879c":"code","48d0e3d1":"code","b07d8244":"code","ebe67b4e":"code","bed6c8e3":"code","71d72947":"code","efdf1275":"code","eac03fd7":"code","6ee40de6":"code","8d386796":"code","de079d6d":"code","37190af9":"code","0cd25cf0":"code","09d97ba6":"code","b011195c":"code","432f7b1d":"code","b93b7af2":"code","02c8cc3a":"code","4036554c":"code","9ebbdc63":"code","16e6b4d4":"code","f79573a0":"code","9ec1c6fa":"code","71637e4c":"code","1ec473f4":"markdown","2329e291":"markdown","fae108d0":"markdown","44b4390b":"markdown","fbaaf61c":"markdown","3be63ea8":"markdown","f48fdae2":"markdown","fde4815a":"markdown","bcfb40d7":"markdown","7467cdf8":"markdown","7664435b":"markdown","b31fcf31":"markdown","e9e75da3":"markdown","925179a7":"markdown","40d318dd":"markdown","dc7a8ab6":"markdown","cb1733be":"markdown","a288e3c9":"markdown","aa1d6a09":"markdown","f4574cf9":"markdown","d4fca2af":"markdown","a6a3f2ec":"markdown","d57e115b":"markdown","b76d7488":"markdown","090c148e":"markdown"},"source":{"5b382fc0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","cbd8fb69":"import pandas as pd\nimport re\n\nhn = pd.read_csv(\"\/kaggle\/input\/hacker-news-posts\/HN_posts_year_to_Sep_26_2016.csv\")\ntitles = hn['title']","821e829c":"titles","e2c4eb98":"hn.head()","917c8674":"pattern = r\"[Pp]ython\"\npython_counts = titles.str.contains(pattern).sum()\nprint(python_counts)","d3bcf32b":"pattern = r'python'\npython_counts = titles.str.contains(pattern,flags = re.I).sum()\nprint(python_counts)","cad5a591":"pattern = r'[Ss][Qq][Ll]'\nsql_counts = titles.str.contains(pattern).sum()\nprint(sql_counts)","77d577e3":"pattern = r'[Ss][Qq][Ll]'\nsql_counts = titles.str.contains(pattern,flags = re.I).sum()\nprint(sql_counts)","d82a4a2f":"pattern = r\"(SQL)\"\nsql_capitalizations = titles.str.extract(pattern, flags=re.I)\nprint(sql_capitalizations)","83735bf8":"sql_capitalizations_freq = sql_capitalizations[0].value_counts()\nprint(sql_capitalizations_freq)","ef31b05b":"pattern = r\"(\\w+SQL)\"\nsql_flavors = titles.str.extract(pattern,flags= re.I)\nsql_flavors_freq = sql_flavors[0].value_counts()\nprint(sql_flavors_freq)","eab617dc":"hn_sql = hn[hn['title'].str.contains(r\"\\w+SQL\", flags=re.I)].copy() \nhn_sql","40df297e":"hn_sql[\"flavor\"] = hn_sql[\"title\"].str.extract(r\"(\\w+SQL)\", re.I)\nhn_sql[\"flavor\"] = hn_sql[\"flavor\"].str.lower()\nsql_pivot = hn_sql.pivot_table(index=\"flavor\",values='num_comments', aggfunc='mean')","66654c88":"sql_pivot","13fd84ce":"pattern = r\"[Pp]ython ([\\d\\.]+)\"\n\npy_versions = titles.str.extract(pattern)\npy_versions_freq = dict(py_versions[0].value_counts())\npy_versions_freq","bab2e390":"def first_10_matches(pattern):\n    \"\"\"\n    Return the first 10 story titles that match\n    the provided regular expression\n    \"\"\"\n    all_matches = titles[titles.str.contains(pattern)]\n    first_10 = all_matches.head(10)\n    return first_10\n\nfirst_10_matches(r\"\\b[Cc]\\b\")","ee6887c8":"def first_10_matches(pattern):\n    all_matches = titles[titles.str.contains(pattern)]\n    first_10 = all_matches.head(10)\n    return first_10\n    \n\npattern = r\"\\b[Cc]\\b [^ . +]\"\nfirst_10_matches(pattern)\n","61a9951a":"test_cases = ['Red_Green_Blue',\n              'Yellow_Green_Red',\n              'Red_Green_Red',\n              'Yellow_Green_Blue',\n              'Green']","abb322b0":"def run_test_cases(pattern):\n    for tc in test_cases:\n        result = re.search(pattern, tc)\n        print(result or \"NO MATCH\")","f7c6894c":"run_test_cases(r\"Green(?=_Blue)\")","0c15879c":"run_test_cases(r\"Green(?!_Red)\")","48d0e3d1":"run_test_cases(r\"(?<=Red_)Green\")","b07d8244":"run_test_cases(r\"(?<!Yellow_)Green\")","ebe67b4e":"run_test_cases(r\"Green(?=.{5})\")","bed6c8e3":"first_10_matches(r\"\\b[Cc]\\b[^.+]\")","71d72947":"pattern = r\"(?<!Series\\s)\\b[Cc]\\b(?![\\+\\.])\"\nc_mentions = titles.str.contains(pattern).sum()\nc_mentions ","efdf1275":"test_cases = [\n              \"I'm going to read a book.\",\n              \"Green is my favorite color.\",\n              \"My name is Aaron.\",\n              \"No doubles here.\",\n              \"I have a pet eel.\"\n             ]\nfor tc in test_cases:\n    print(re.search(r\"(\\w)\\1\", tc))","eac03fd7":"test_cases = pd.Series(test_cases)\ntest_cases\n","6ee40de6":"print(test_cases.str.contains(r\"(\\w)\\1\"))","8d386796":"pattern = r\"\\b(\\w+)\\s\\1\\b\"\n\nrepeated_words = titles[titles.str.contains(pattern)]\nrepeated_words","de079d6d":"re.sub(pattern, repl, string, flags=0)","37190af9":"string = \"aBcDEfGHIj\"\n\nprint(re.sub(r\"[A-Z]\", \"-\", string))","0cd25cf0":"sql_variations = pd.Series([\"SQL\", \"Sql\", \"sql\"])\n\nsql_uniform = sql_variations.str.replace(r\"sql\", \"SQL\", flags=re.I)\nprint(sql_uniform)","09d97ba6":"email_variations = pd.Series(['email', 'Email', 'e Mail',\n                        'e mail', 'E-mail', 'e-mail',\n                        'eMail', 'E-Mail', 'EMAIL'])\npattern = r\"e[\\-\\s]?mail\"\nemail_uniform = email_variations.str.replace(pattern, \"email\", flags=re.I)\nemail_uniform","b011195c":"titles_clean = titles.str.replace(pattern, \"email\", flags=re.I)\ntitles_clean","432f7b1d":"test_urls = pd.Series([\n 'https:\/\/www.amazon.com\/Technology-Ventures-Enterprise-Thomas-Byers\/dp\/0073523429',\n 'http:\/\/www.interactivedynamicvideo.com\/',\n 'http:\/\/www.nytimes.com\/2007\/11\/07\/movies\/07stein.html?_r=0',\n 'http:\/\/evonomics.com\/advertising-cannot-maintain-internet-heres-solution\/',\n 'HTTPS:\/\/github.com\/keppel\/pinn',\n 'Http:\/\/phys.org\/news\/2015-09-scale-solar-youve.html',\n 'https:\/\/iot.seeed.cc',\n 'http:\/\/www.bfilipek.com\/2016\/04\/custom-deleters-for-c-smart-pointers.html',\n 'http:\/\/beta.crowdfireapp.com\/?beta=agnipath',\n 'https:\/\/www.valid.ly?param',\n 'http:\/\/css-cursor.techstream.org'\n])\npattern = r\"https?:\/\/([\\w\\-\\.]+)\"\ntest_urls_clean = test_urls.str.extract(pattern,flags=re.I)\ntest_urls_clean","b93b7af2":"domains = hn[\"url\"].str.extract(pattern,flags= re.I)\ntop_domains = domains[0].value_counts().head(5)\ntop_domains","02c8cc3a":"created_at = hn['created_at'].head()\nprint(created_at)","4036554c":"pattern = r\"(.+)\\s(.+)\"\ndates_times = created_at.str.extract(pattern)\nprint(dates_times)","9ebbdc63":"pattern = r\"(https?):\/\/([\\w\\.\\-]+)\/?(.*)\"\n\ntest_url_parts = test_urls.str.extract(pattern, flags=re.I)\ntest_url_parts","16e6b4d4":"url_parts = hn['url'].str.extract(pattern, flags=re.I)\nurl_parts","f79573a0":"#e.g\ncreated_at = hn['created_at'].head()\n\npattern = r\"(.+) (.+)\"\ndates_times = created_at.str.extract(pattern)\nprint(dates_times)","9ec1c6fa":"pattern = r\"(?P<date>.+) (?P<time>.+)\"\ndates_times = created_at.str.extract(pattern)\nprint(dates_times)","71637e4c":"pattern = r\"(?P<protocol>https?):\/\/(?P<domain>[\\w\\.\\-]+)\/?(?P<path>.*)\"\nurl_parts = hn['url'].str.extract(pattern,flags=re.I)\nurl_parts","1ec473f4":"- now use lookarounds to exclude the matches we don't want","2329e291":"- str.replace() method is used to replace simple substrings\n- We can achieve the same with regular expressions using the re.sub() function","fae108d0":"- Question adding names to our capture group from the previous screen to create a dataframe with named columns.","44b4390b":"- using backreferences for double letters HelloGoodbye\n- https:\/\/s3.amazonaws.com\/dq-content\/369\/backreference_syntax_1.svg\n- Within a regular expression, we can use a backslash followed by that integer to refer to the group:\\2\\1","fbaaf61c":"- positive lookbehind","3be63ea8":"- Question write a regular expression that will extract the URL components into individual columns of a dataframe\n- Write a regular expression that extracts URL components using three capture groups:\n* The first capture group should include the protocol text, up to but not including :\/\/.\n* The second group should contain the domain, from after :\/\/ up to but not including \/.\n* The third group should contain the page path, from after \/ to the end of the string.","f48fdae2":"- We'll use capture groups to extract these dates and times into two columns:\n- https:\/\/s3.amazonaws.com\/dq-content\/369\/multiple_capture_groups.svg\n- (.+)\\s(.+) ","fde4815a":"- https:\/\/s3.amazonaws.com\/dq-content\/369\/named_capture_groups.svg - In order to name a capture group we use the syntax ?P, where name is the name of our capture group","bcfb40d7":"- use a negative set to prevent matches for the + character and the .","7467cdf8":"- we learned is to use re.I \u2014 the ignorecase flag \u2014 to make our pattern case insensitive:","7664435b":"- while working with pandas we use, Series.str.replace(pat, repl, flags=0)","b31fcf31":"[](http:\/\/)Having extracted just the domains from the URLs, on this final screen we'll extract each of the three component parts of the URLs:\n\n* Protocol\n* Domain\n* Page path\n\nhttps:\/\/s3.amazonaws.com\/dq-content\/369\/url_examples_2_updated.svg","e9e75da3":"- capture groups to extract the version of Python https:\/\/s3.amazonaws.com\/dq-content\/369\/python_versions.svg\n[\\d\\.]+ - one or more digit or character ","925179a7":"- final task will be to name unnamed columns using named capture groups","40d318dd":"- Next up: counting the mentions of the C language.\nhttps:\/\/s3.amazonaws.com\/dq-content\/369\/c_regex_1.svg\n- \\b a word boundry ..[Cc]- the character Cc \\b a word boundry \n- re-useing the previous first_10_matches() function ","dc7a8ab6":"- Question here we are provided with email_variations,lets use the techique to make this uniofrm ","cb1733be":"- negative lookbehind","a288e3c9":"- Instead we'll need a new tool: lookarounds.( to avoid  irrelevant result line series C)\n- https:\/\/s3.amazonaws.com\/dq-content\/369\/lookarounds.svg - four types of lookarounds\n- 1. tips\n* Inside the parentheses, the first character of a lookaround is always ?.\n* If the lookaround is a lookbehind, the next character will be <, which you can think of as an arrow head pointing behind the match.\n* The next character indicates whether the lookaround is positive (=) or negative (!).","aa1d6a09":"- Using the Series.str.extract() method.\n- https:\/\/s3.amazonaws.com\/dq-content\/369\/single_capture_group.svg","f4574cf9":"- using the Series.value_counts() method to create a frequency table ","d4fca2af":"- extend this analysis by looking at titles that have letters immediately before the \"SQL,\" ","a6a3f2ec":"- HelloGoodbyeGoodbyeHello - (\\w)\\1\n- https:\/\/s3.amazonaws.com\/dq-content\/369\/backreference_syntax_2.svg","d57e115b":"- there were multiple different capitalizations for SQL in our dataset. Lets make this uniform, Series.str.replace() method and a regular expression:","b76d7488":"- repl parameter is the text that you would like to substitute for the match","090c148e":"- we'll extract components of URLs from our dataset,task we will be performing first is extracting the different components of the URLs in order to analyze them. On this screen, we'll start by extracting just the domains.https:\/\/s3.amazonaws.com\/dq-content\/369\/url_examples_1_updated.svg\n* following technique:\n\n- Using a series of characters that will match the protocol.\n- Inside a capture group, using a set that will match the character classes used in the domain.\n- Because all of the URLs either end with the domain, or continue with page path which starts with \/ (a character not found in any domains), we don't need to cater for this part of the URL in our regular expression."}}