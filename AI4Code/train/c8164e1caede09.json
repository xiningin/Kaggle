{"cell_type":{"6b7a1556":"code","3d026735":"code","2356b99e":"code","a6105ce1":"code","8d30eff1":"code","5efa15e3":"code","bfcc7182":"code","32495b52":"code","2c5a5ab0":"code","b8867ec3":"code","d2cfa877":"code","80d3c888":"code","8decf9e8":"code","37ee9af8":"code","d2054d97":"code","2e4e89e5":"code","fc0636c0":"code","aa497ba8":"code","9397236d":"code","3f4f4877":"code","443465e3":"code","a8204615":"code","4fcb582a":"code","3e1485a4":"code","590f8f1c":"code","231c4913":"code","9aa006b0":"code","5e54c0f0":"code","30f98fd3":"code","c44ba605":"code","9bc33dbf":"code","8bbc8895":"code","a3729ded":"code","9b275a70":"code","da284be5":"code","78a718e9":"code","8fb311d6":"code","18159813":"code","e36fcf2f":"code","540cfc9d":"code","2fec0a9b":"code","442788c6":"code","c18d7149":"code","f30f200d":"code","e6bddd51":"code","a11159ee":"code","f2b17a65":"code","12b3ab05":"code","500eb2f9":"code","5ca003b5":"code","a764d318":"code","cfea431e":"code","92d72f7e":"code","7cdfd5d5":"code","272a5a41":"code","87a91dc6":"code","32f2a0dd":"code","271966be":"code","33826497":"code","3a2f524d":"code","b579513e":"code","1c87f35c":"code","35d78bf7":"code","a232d316":"code","5a62d19c":"code","08f59add":"code","f346106b":"code","f56693c5":"code","a32e699b":"code","a3067b8a":"code","ae74f4fa":"code","38520f47":"code","0fb6a3b2":"code","193d155d":"code","632ff884":"code","592fdcf2":"code","319f88d5":"code","688ab364":"code","ffad4831":"code","2309de61":"code","0adbb8c6":"code","09071611":"code","4199c3d7":"code","aa455c01":"code","8415b5d2":"code","cb5216f7":"code","fbefd2a9":"code","b8f80a77":"markdown","fe60de5c":"markdown","829823f4":"markdown","9f8a05b8":"markdown","4ec6a632":"markdown","c319ddb3":"markdown","20f4267c":"markdown","4bb2b6ee":"markdown","00226786":"markdown","090db3d0":"markdown","2dd02181":"markdown","32b834ad":"markdown","cc9fe507":"markdown","56c01779":"markdown","2f999250":"markdown","39ac0c33":"markdown","f30f7e50":"markdown","b3ba450f":"markdown","9a5358b2":"markdown","38b45fe8":"markdown"},"source":{"6b7a1556":"# This block is from https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\n#load packages\nimport sys #access to system parameters https:\/\/docs.python.org\/3\/library\/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__))\n\nimport matplotlib #collection of functions for scientific and publication-ready visualization\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\n\nimport numpy as np #foundational package for scientific computing\nprint(\"NumPy version: {}\". format(np.__version__))\n\nimport scipy as sp #collection of functions for scientific computing and advance mathematics\nprint(\"SciPy version: {}\". format(sp.__version__)) \n\nimport IPython\nfrom IPython import display #pretty printing of dataframes in Jupyter notebook\nprint(\"IPython version: {}\". format(IPython.__version__)) \n\nimport sklearn #collection of machine learning algorithms\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\n\nimport seaborn as sns #collection of functions for data visualization\nprint(\"seaborn version: {}\". format(sns.__version__))\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import OneHotEncoder #OneHot Encoder\n\n#misc libraries\nimport random\nimport time\n\n%matplotlib inline\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nprint('-'*25)\n","3d026735":"train_raw = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_raw = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsample_raw = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","2356b99e":"train_raw.info()","a6105ce1":"def null_p (column):\n    num_rows = train_raw.shape[0]\n    null_percent = (column.isnull().sum() \/num_rows)*100\n    return null_percent","8d30eff1":"#drop columns with more than null_bar% of null data\ndef drop_null_col (df, null_bar):\n    col_ls = []\n    for col in df.columns:\n\n        if null_p(df[col]) > null_bar:\n            col_ls.append(col)\n        \n    return col_ls","5efa15e3":"#get a list of columns to drop\ncols_drop = drop_null_col(train_raw, 40)","bfcc7182":"cols_drop","32495b52":"#generate a df with all the cols with 40% or more null data dropped\ntrain_data_dnull = train_raw.copy().drop(cols_drop, axis = 1)","2c5a5ab0":"def null_p_df (df):\n    p_ls = []\n    type_ls = []\n    for col in df.columns:\n        n_percent = null_p(df[col])\n        p_ls.append(n_percent)\n        type_ls.append(df[col].dtype)\n    np_df = pd.DataFrame({'Column Name': df.columns, \"Null Percent\": p_ls, \"Data Type\": type_ls})\n    return np_df","b8867ec3":"np_df = null_p_df(train_data_dnull)","d2cfa877":"np_df.sort_values(by = 'Null Percent',  ascending=False).head(20)","80d3c888":"#need special attention on LotFrontage, let's take a look at the distribution \ntrain_data_dnull.LotFrontage.hist()","8decf9e8":"#let's look at how it relates to our target\ntrain_data_dnull.plot.scatter(x='SalePrice', y='LotFrontage', ylim=(0,400));","37ee9af8":"np_df.sort_values(by = 'Null Percent',  ascending=False).head(20)","d2054d97":"from sklearn.impute import SimpleImputer\n\ndef clear_na (df):\n    \n    #let split the data for more targeted handling\n    txt_cols = [cname for cname in df.columns if df[cname].dtype == \"object\"]\n\n    # Select numerical columns\n    num_cols = [cname for cname in df.columns if df[cname].dtype in ['int64', 'float64']]\n\n    \n    txt_imputer = sklearn.impute.SimpleImputer(strategy='most_frequent')\n    num_imputer = sklearn.impute.SimpleImputer(strategy='mean')\n    \n    txt_nonull = pd.DataFrame(txt_imputer.fit_transform(df[txt_cols]))\n    txt_nonull.columns = txt_cols\n    num_nonull = pd.DataFrame(num_imputer.fit_transform(df[num_cols]))\n    num_nonull.columns = num_cols\n    \n    clean_df = txt_nonull.join(num_nonull)\n    \n    return clean_df\n    \n    ","2e4e89e5":"train_clean = clear_na(train_data_dnull)","fc0636c0":"train_clean.info()","aa497ba8":"#okay, now we have a clean dataset, let's take a look at the data more closely to understand what types of data we are dealing with\n#to do this, we will break the data to txt and num, to allow easier handling\n\ndef txt_num_split(df):\n\n    #let split the data for more targeted handling\n    txt_cols = [cname for cname in df.columns if df[cname].dtype == \"object\"]\n    # Select numerical columns\n    num_cols = [cname for cname in df.columns if df[cname].dtype in ['int64', 'float64']]\n    \n    txt_df = df[txt_cols]\n    num_df = df[num_cols]\n    \n    return txt_df, num_df\n","9397236d":"train_clean_txt, train_clean_num = txt_num_split(train_clean)","3f4f4877":"train_clean_txt.describe()","443465e3":"uv_df = pd.DataFrame(train_clean_txt.describe().loc['unique'])","a8204615":"uv_df.sort_values(by = 'unique', ascending=False)","4fcb582a":"train_clean_txt.Neighborhood.value_counts()","3e1485a4":"#let's look at how it relates to our target\ntrain_clean.plot.scatter(x='SalePrice', y='Neighborhood')","590f8f1c":"import category_encoders as ce\n\n# Create the encoder\ntarget_enc = ce.CatBoostEncoder(cols=['Neighborhood'])\ntarget_enc_fitted = target_enc.fit(train_clean_txt['Neighborhood'], train_clean['SalePrice'])\nnbh_trans = target_enc_fitted.transform(train_clean_txt['Neighborhood'])\n","231c4913":"nbh_trans","9aa006b0":"train_clean.Exterior2nd.value_counts()","5e54c0f0":"train_clean.plot.scatter(x='SalePrice', y='Exterior2nd')","30f98fd3":"combine_ls = ['CBlock', 'Other', 'Stone', 'AsphShn', 'ImStucc', 'Brk Cmn']\ntrain_clean_txt.Exterior2nd = train_clean_txt.Exterior2nd.apply(lambda x: 'Other' if x in combine_ls else x)","c44ba605":"train_clean_txt.Exterior2nd.value_counts()","9bc33dbf":"train_clean_txt.Exterior1st.value_counts()","8bbc8895":"combine_ls_e1 = ['CBlock', 'Other', 'Stone', 'AsphShn', 'ImStucc', 'BrkComm']\ntrain_clean_txt.Exterior1st = train_clean_txt.Exterior1st.apply(lambda x: 'Other' if x in combine_ls_e1 else x)","a3729ded":"train_clean_txt.Exterior1st.value_counts()","9b275a70":"train_clean_num.hist(figsize = (20,20))","da284be5":"num_cat_cols = ['GarageYrBlt', 'MoSold', 'YrSold', 'GarageYrBlt', 'YearBuilt', 'YearRemodAdd']","78a718e9":"def one_hot (raw_df, cat_cols):\n    \n    df = raw_df.copy()\n    \n    #next we apply One-Hot\n    OH_en = OneHotEncoder(handle_unknown='ignore', sparse=False)\n    OH_source_df_imputed_col = OH_en.fit_transform(df[cat_cols])\n    OH_source_df_imputed_col = pd.DataFrame(OH_source_df_imputed_col)\n    \n    #alining index \n    OH_source_df_imputed_col.index = df.index\n    #alining columns\n    OH_source_df_imputed_col.columns = OH_en.get_feature_names(cat_cols)\n    return OH_source_df_imputed_col","8fb311d6":"onehot_test = one_hot(train_clean_num, num_cat_cols)","18159813":"onehot_test","e36fcf2f":"num_toscale = ['LotFrontage', 'LotArea', 'BsmtFinSF1',\n       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n       'LowQualFinSF', 'WoodDeckSF',\n       'OpenPorchSF', 'PoolArea']","540cfc9d":"from mlxtend.preprocessing import minmax_scaling\n\ndef scaler (raw_df, cols):\n    df = raw_df.copy()\n    scaled_data = minmax_scaling(df , columns=cols)\n    return scaled_data ","2fec0a9b":"scaler_test = scaler(train_clean_num, num_toscale)","442788c6":"scaler_test","c18d7149":"norm_ls = ['1stFlrSF', '2ndFlrSF', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'MSSubClass', 'OpenPorchSF', 'WoodDeckSF', 'SalePrice']","f30f200d":"# for Box-Cox Transformation\nfrom scipy import stats\n\ndef norm_trans (col_ls, target_df):\n    df = target_df.copy()\n\n    for col in df.columns:\n\n        if col in col_ls:\n            norm_data = stats.boxcox(df[col].array+0.001)\n            norm_series = pd.Series(norm_data[0])\n            norm_series.name = col\n            df[col] = norm_series\n\n    return df","e6bddd51":"train_clean_num_norm= norm_trans(norm_ls, train_clean_num)","a11159ee":"train_clean_num_norm[norm_ls].hist(bins = 20, figsize = (20,20))","f2b17a65":"#a quick a dirty way to make sure we are not dealing with outliers\n\ndef replace_outliers (raw_df, cols):\n    df = raw_df.copy()\n    for col in cols:\n        bounds = np.percentile(df[col], (0.1,99.9))\n        low_bound = bounds[0]\n        up_bound = bounds[1]\n        df[col] = df[col].apply(lambda x: low_bound if x < low_bound else (up_bound if x > up_bound else x))\n    \n    return df","12b3ab05":"df_no_outliers = replace_outliers (train_clean_num_norm, norm_ls)","500eb2f9":"df_no_outliers[norm_ls].hist(bins = 20, figsize = (20,20))","5ca003b5":"def data_clean (raw_df):\n    df = raw_df.copy()\n    cols_to_drop = drop_null_col(df, 40)\n    df = df.drop(cols_to_drop, axis = 1)\n    df = clear_na(df)\n    return df ","a764d318":"#creating combined dataframe for process\n\ntrain_raw_lable = train_raw.copy()\ntrain_raw_lable['IsTrain'] = 1\ny_train = train_raw_lable.SalePrice.copy()\ntrain_raw_lable = train_raw_lable.drop('SalePrice', axis = 1)\n\ntest_raw_lable = test_raw.copy()\ntest_raw_lable['IsTrain'] = 0\nall_data_raw = pd.concat([train_raw_lable, test_raw_lable], ignore_index=True, sort=False)\n","cfea431e":"all_data_raw","92d72f7e":"all_clean = data_clean (all_data_raw)","7cdfd5d5":"all_clean.loc[all_clean.IsTrain == 1]","272a5a41":"all_clean.isnull().sum()","87a91dc6":"X_all_txt, X_all_num = txt_num_split(all_clean)","32f2a0dd":"import category_encoders as ce\n\ndef ce_encoding (fit_df, trans_df, cols):\n    df = trans_df.copy()\n    \n    target_enc = ce.CatBoostEncoder(cols=cols)\n    target_enc_fitted = target_enc.fit(fit_df[cols], y_train)\n    tran_cols = target_enc_fitted.transform(df[cols])\n    return tran_cols","271966be":"def txt_col_preprocess (raw_df):\n    \n    df = raw_df.copy()\n    \n    #aggregate scattered values in Exterior2nd \n    combine_ls_e2 = ['CBlock', 'Other', 'Stone', 'AsphShn', 'ImStucc', 'Brk Cmn']\n    df.Exterior2nd = df.Exterior2nd.apply(lambda x: 'Other' if x in combine_ls_e2 else x)\n    \n    #aggregate scattered values in Exterior1st \n    combine_ls_e1 = ['CBlock', 'Other', 'Stone', 'AsphShn', 'ImStucc', 'BrkComm']\n    df.Exterior1st = df.Exterior1st.apply(lambda x: 'Other' if x in combine_ls_e1 else x)\n    \n    #encode cols with more than 10 unique values with category_encoders\n    top_uni_cols = ['Neighborhood', 'Exterior1st', 'Exterior2nd']\n    ce_fit = all_clean.loc[all_clean.IsTrain == 1].copy()\n    ce_cols = ce_encoding(ce_fit, df, top_uni_cols)\n    ce_cols = scaler(ce_cols, top_uni_cols)\n    df = df.drop(top_uni_cols, axis = 1).join(ce_cols)\n    \n    #onehot encode the rest\n    rest_txt_cols = df.columns.drop(top_uni_cols)\n    col_for_oh = one_hot(df, rest_txt_cols)\n    df = df.drop(rest_txt_cols, axis = 1).join(col_for_oh)\n    \n\n    \n    return df\n    \n    \n    ","33826497":"def num_col_preprocess (raw_df):\n    \n    df = raw_df.copy()\n    \n    norm_ls = ['1stFlrSF', '2ndFlrSF', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'MSSubClass', 'OpenPorchSF', 'WoodDeckSF']\n    num_toscale = ['LotFrontage', 'LotArea', 'BsmtFinSF1',\n       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n       'LowQualFinSF', 'WoodDeckSF',\n       'OpenPorchSF', 'PoolArea']\n    num_cat_cols = ['GarageYrBlt', 'MoSold', 'YrSold', 'GarageYrBlt', 'YearBuilt', 'YearRemodAdd']\n    \n    #create new features\n    df['HasPorach'] = (df['OpenPorchSF'] + df['EnclosedPorch'])>0\n    df['HouseAge'] = df['YrSold'] - df['YearBuilt']\n    df['HasPool'] = df['PoolArea'] > 0\n    new_cols = ['HasPorach', 'HasPool']\n    oh_new_cols = one_hot(df, new_cols)\n    df = df.drop(new_cols, axis = 1).join(oh_new_cols)\n    \n    #normalize and onehot other cols\n    df = replace_outliers (df, norm_ls)\n    df = norm_trans (norm_ls, df)\n    oh_trnfm_cols = one_hot(df, num_cat_cols)\n    df = df.drop(num_cat_cols, axis = 1).join(oh_trnfm_cols)\n    \n    #scale cols\n    scale_cols = scaler(df, num_toscale)\n    df = df.drop(num_toscale, axis = 1).join(scale_cols)\n    \n\n    \n    \n    return df\n    ","3a2f524d":"X_all_txt_processed = txt_col_preprocess (X_all_txt)","b579513e":"X_all_txt_processed","1c87f35c":"X_all_num_processed = num_col_preprocess(X_all_num)","35d78bf7":"X_all_num_processed","a232d316":"#lastly, we will normalize SalePrice separately\nnorm_ls = stats.boxcox(y_train.array+0.001)","5a62d19c":"y_train_norm, norm_p = norm_ls[0], norm_ls[1]","08f59add":"y_train_norm","f346106b":"X_all_processed = X_all_txt_processed.join(X_all_num_processed)","f56693c5":"X = X_all_processed.loc[X_all_processed['IsTrain']==1]","a32e699b":"X = X.drop('IsTrain', axis = 1)","a3067b8a":"X_test = X_all_processed.loc[X_all_processed['IsTrain']==0]","ae74f4fa":"X_test = X_test.drop('IsTrain', axis = 1)","38520f47":"# Loading neccesary packages for modelling.\n\nfrom sklearn.model_selection import cross_val_score, KFold, cross_validate\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom mlxtend.regressor import StackingCVRegressor # This is for stacking part, works well with sklearn and others...","0fb6a3b2":"# Setting kfold for future use.\n\nkf = KFold(10, random_state=42)","193d155d":"# Some parameters for ridge, lasso and elasticnet.\n\nalphas_alt = [15.5, 15.6, 15.7, 15.8, 15.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [\n    5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008\n]\ne_alphas = [\n    0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007\n]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\n# ridge_cv\n\nridge = make_pipeline(RobustScaler(), RidgeCV(\n    alphas=alphas_alt,\n    cv=kf,\n))\n\n# lasso_cv:\n\nlasso = make_pipeline(\n    RobustScaler(),\n    LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kf))\n\n# elasticnet_cv:\n\nelasticnet = make_pipeline(\n    RobustScaler(),\n    ElasticNetCV(max_iter=1e7,\n                 alphas=e_alphas,\n                 cv=kf,\n                 random_state=42,\n                 l1_ratio=e_l1ratio))\n\n# svr:\n\nsvr = make_pipeline(RobustScaler(),\n                    SVR(C=21, epsilon=0.0099, gamma=0.00017, tol=0.000121))\n\n# gradientboosting:\n\ngbr = GradientBoostingRegressor(n_estimators=2900,\n                                learning_rate=0.0161,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=17,\n                                loss='huber',\n                                random_state=42)\n\n# lightgbm:\n\nlightgbm = LGBMRegressor(objective='regression',\n                         n_estimators=3500,\n                         num_leaves=5,\n                         learning_rate=0.00721,\n                         max_bin=163,\n                         bagging_fraction=0.35711,\n                         n_jobs=-1,\n                         bagging_seed=42,\n                         feature_fraction_seed=42,\n                         bagging_freq=7,\n                         feature_fraction=0.1294,\n                         min_data_in_leaf=8)\n\n\n# hist gradient boosting regressor:\n\nhgrd= HistGradientBoostingRegressor(    loss= 'least_squares',\n    max_depth= 2,\n    min_samples_leaf= 40,\n    max_leaf_nodes= 29,\n    learning_rate= 0.15,\n    max_iter= 225,\n                                    random_state=42)\n\n\n#to replace xgboost for meta_regressor\nrf = RandomForestRegressor()\n\n\n# stacking regressor:\n\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr,\n                                         lightgbm,hgrd),\n                                meta_regressor=rf,\n                                use_features_in_secondary=True)","632ff884":"def model_check(X, y, estimators, cv):\n    \n    ''' A function for testing multiple estimators.'''\n    \n    model_table = pd.DataFrame()\n\n    row_index = 0\n    for est, label in zip(estimators, labels):\n\n        MLA_name = label\n        model_table.loc[row_index, 'Model Name'] = MLA_name\n\n        cv_results = cross_validate(est,\n                                    X,\n                                    y,\n                                    cv=cv,\n                                    scoring='neg_root_mean_squared_error',\n                                    return_train_score=True,\n                                    n_jobs=-1)\n\n        model_table.loc[row_index, 'Train RMSE'] = -cv_results[\n            'train_score'].mean()\n        model_table.loc[row_index, 'Test RMSE'] = -cv_results[\n            'test_score'].mean()\n        model_table.loc[row_index, 'Test Std'] = cv_results['test_score'].std()\n        model_table.loc[row_index, 'Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1\n\n    model_table.sort_values(by=['Test RMSE'],\n                            ascending=True,\n                            inplace=True)\n\n    return model_table","592fdcf2":"# Setting list of estimators and labels for them:\n\nestimators = [ridge, lasso, elasticnet, gbr, rf, lightgbm, svr, hgrd]\nlabels = [\n    'Ridge', 'Lasso', 'Elasticnet', 'GradientBoostingRegressor',\n    'RandomForestRegressor', 'LGBMRegressor', 'SVR', 'HistGradientBoostingRegressor',\n]","319f88d5":"# Executing cross validation.\n\nraw_models = model_check(X, y_train_norm, estimators, kf)\nraw_models","688ab364":"from datetime import datetime\n# Fitting the models on train data.\n\nprint('=' * 20, 'START Fitting', '=' * 20)\nprint('=' * 55)\n\nprint(datetime.now(), 'StackingCVRegressor')\nstack_gen_model = stack_gen.fit(X.values, y_train_norm)\nprint(datetime.now(), 'Elasticnet')\nelastic_model_full_data = elasticnet.fit(X, y_train_norm)\nprint(datetime.now(), 'Lasso')\nlasso_model_full_data = lasso.fit(X, y_train_norm)\nprint(datetime.now(), 'Ridge')\nridge_model_full_data = ridge.fit(X, y_train_norm)\nprint(datetime.now(), 'SVR')\nsvr_model_full_data = svr.fit(X, y_train_norm)\nprint(datetime.now(), 'GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y_train_norm)\nprint(datetime.now(), 'Hist')\nhist_full_data = hgrd.fit(X, y_train_norm)\n\nprint('=' * 20, 'FINISHED Fitting', '=' * 20)\nprint('=' * 58)","ffad4831":"def blend_models_predict(X):\n    return ((0.1 * elastic_model_full_data.predict(X)) +\n            (0.1 * lasso_model_full_data.predict(X)) +\n            (0.1 * ridge_model_full_data.predict(X)) +\n            (0.1 * svr_model_full_data.predict(X)) +\n            (0.15 * gbr_model_full_data.predict(X)) +\n            (0.1 * hist_full_data.predict(X)) +\n            (0.35 * stack_gen_model.predict(X.values)))","2309de61":"y_sub_blend_raw = blend_models_predict(X_test)","0adbb8c6":"from scipy.special import inv_boxcox\ny_sub_blend = inv_boxcox(y_sub_blend_raw, norm_p)","09071611":"y_sub_gbr_raw = gbr_model_full_data.predict(X_test)","4199c3d7":"y_sub_gbr = inv_boxcox(y_sub_gbr_raw, norm_p)","aa455c01":"sub_blend = pd.DataFrame({'ID': sample_raw['Id'], 'SalePrice': y_sub_blend})","8415b5d2":"sub_blend.to_csv('sub_blend.csv', index = False)","cb5216f7":"sub_gbr = pd.DataFrame({'ID': sample_raw['Id'], 'SalePrice': y_sub_gbr})","fbefd2a9":"sub_gbr.to_csv('sub_gbr.csv', index = False)","b8f80a77":"#### Observation\nCBlock, Other Stone, AsphShn, ImStucc, Brk Cmn are in similar ranges\n\n#### Action Plan\nlet's combine them","fe60de5c":"#### Observation\nWith the decision of handling LotFrontage na with mean, and the other null percentages are relatively small, we could process fillna for these columns in batches\n\n#### Action Plan\nWe will handle columns with null values in batches","829823f4":"# We look at num cols\nWe will what to know the cat cols, cols need normalization, cols need scaling, outliers in this analysis, as well as some new features to build","9f8a05b8":"## Function building\nnow we are going to combine our finding and approaches into functions","4ec6a632":"## With a clean dataset, we will look at txt and num data seperately, as we will apply different EDA and handling techniques accordingly","c319ddb3":"# Modeling\nthis is borrowed from https:\/\/www.kaggle.com\/datafan07\/top-1-approach-eda-new-models-and-stacking","20f4267c":"## features we can build\nhasporch? = OpenPorchSF + EnclosedPorch if zero no, else yes  <br \/>\nhouseAge = YrSold - YearBuilt <br \/>\nhasPool = PoolArea > 0 yes  <br \/>\nhasScreenP = if ScreenPorch >0  <br \/>","4bb2b6ee":"### Action Plan\nWe will apply onehot to cols with less than 10 unique values and target encoding for the ones with more than 10","00226786":"#### Observation\nDoesnt seem like there are any trend in relation to SalePrice\n\n#### Action Plan\nWe can fillna with mean","090db3d0":"#### Observation\nNeighborhood, Exterior2nd, Exterior1st have more than 10 unique values, cant use onehot\n\n#### Action Plan\nlet's see if we could further group the valuse for the above mentioned","2dd02181":"### Then we look at the rest of the cols that have null values, judging by the amount of nulls we have in each col, we will decide what to do for each of them","32b834ad":"# House Pricing Competition - A Quick A-Z\n\nThe objective is to do a quick round on the House Pricing dateset to prepare my skills for regression problems for my later projects","cc9fe507":"## Comments after submission\nThe best score obtained from this effort was 13841.83382 which put the submission to the top 3%, which is pretty good. Having that said, there is a lot more imporvements we could do, such as more detailed EDAs, other new features for txt cols, droping unrelated features, a more percise way to detect and remove outliers, etc. ","56c01779":"### Knowing the number of unique values will help us to understand the type of txt data we are dealing with, classification or other types, which will require further handling to break them down","2f999250":"## Creating Submission Files","39ac0c33":"### Data outliers\n","f30f7e50":"#### Observation\nFairly normal distributed, some outliers at 300","b3ba450f":"#### Observation\nthere is no specific trends in this data, and after reading the description, we think that there might not be a good way to furter aggregate the data\n\n#### Action Plan\nuse some sort of lable encoder, or combine with another cat data","9a5358b2":"## Observations and Next Step\nWe immediately see some cols with large nulls, let's create functions to pull them out and remove them. We will do it by looking at the percentage of null values in the respective col.","38b45fe8":"## Load the files"}}