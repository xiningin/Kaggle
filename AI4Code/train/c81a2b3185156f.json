{"cell_type":{"ec0bac64":"code","b7d3708d":"code","d675d031":"code","845c7e2d":"code","b3efaeb5":"code","84e7f6d7":"code","c6337ddf":"code","de686edb":"code","a37bd0fa":"code","12d2cdc5":"code","9ba32ad7":"code","ac911fb0":"code","4b622fca":"code","307fd9fd":"code","4756f08d":"code","2a8571e4":"code","ca37f529":"code","289f8876":"code","dc31c587":"code","4577f3bc":"code","203d4ddd":"code","bf45f096":"code","8f339ac7":"code","96dbedbc":"code","bb38b5e9":"code","afc1daf3":"code","a9ac69fc":"code","6dc6ab1d":"code","73667c1d":"code","c5512762":"code","3c5c9449":"code","a89cf5ad":"code","49c48a5e":"code","6fd4ddb9":"markdown","f16fb42f":"markdown","a36374ae":"markdown","2d1c64be":"markdown","e05a9cf6":"markdown","d74dd78e":"markdown","49e80ed0":"markdown","bd43f68b":"markdown","ec776c45":"markdown","a151c130":"markdown","5b2a7cfe":"markdown","efc7a876":"markdown","bb273169":"markdown","2ba61ea8":"markdown","2b5771a2":"markdown","e95fed30":"markdown"},"source":{"ec0bac64":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import rcParams\n%matplotlib inline\nrcParams['figure.figsize'] = 10,8\nsns.set(style='whitegrid', palette='muted',\n        rc={'figure.figsize': (15,10)})\nimport os\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\n\nfrom numpy.random import seed\nfrom tensorflow import set_random_seed\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n# print(os.listdir(\"..\/input\"))","b7d3708d":"print(os.listdir(\"..\/input\/titanic-cleaned-data\"))","d675d031":"# Load data as Pandas dataframe\ntrain = pd.read_csv('..\/input\/titanic-cleaned-data\/train_clean.csv', )\ntest = pd.read_csv('..\/input\/titanic-cleaned-data\/test_clean.csv')\ndf = pd.concat([train, test], axis=0, sort=True)","845c7e2d":"df.head()","b3efaeb5":"def display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n        display(df)\n\n        \ndisplay_all(df.describe(include='all').T)","84e7f6d7":"sns.countplot(x='Pclass', data=df, palette='hls', hue='Survived')\nplt.xticks(rotation=45)\nplt.show()","c6337ddf":"sns.countplot(x='Sex', data=df, palette='hls', hue='Survived')\nplt.xticks(rotation=45)\nplt.show()","de686edb":"sns.countplot(x='Embarked', data=df, palette='hls', hue='Survived')\nplt.xticks(rotation=45)\nplt.show()","a37bd0fa":"# convert to cateogry dtype\ndf['Sex'] = df['Sex'].astype('category')\n# convert to category codes\ndf['Sex'] = df['Sex'].cat.codes","12d2cdc5":"# subset all categorical variables which need to be encoded\ncategorical = ['Embarked', 'Title']\n\nfor var in categorical:\n    df = pd.concat([df, \n                    pd.get_dummies(df[var], prefix=var)], axis=1)\n    del df[var]","9ba32ad7":"# drop the variables we won't be using\ndf.drop(['Cabin', 'Name', 'Ticket', 'PassengerId'], axis=1, inplace=True)","ac911fb0":"df.head()","4b622fca":"continuous = ['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Family_Size']\n\nscaler = StandardScaler()\n\nfor var in continuous:\n    df[var] = df[var].astype('float64')\n    df[var] = scaler.fit_transform(df[var].values.reshape(-1, 1))","307fd9fd":"display_all(df.describe(include='all').T)","4756f08d":"X_train = df[pd.notnull(df['Survived'])].drop(['Survived'], axis=1)\ny_train = df[pd.notnull(df['Survived'])]['Survived']\nX_test = df[pd.isnull(df['Survived'])].drop(['Survived'], axis=1)","2a8571e4":"def create_model(lyrs=[8], act='linear', opt='Adam', dr=0.0):\n    \n    # set random seed for reproducibility\n    seed(42)\n    set_random_seed(42)\n    \n    model = Sequential()\n    \n    # create first hidden layer\n    model.add(Dense(lyrs[0], input_dim=X_train.shape[1], activation=act))\n    \n    # create additional hidden layers\n    for i in range(1,len(lyrs)):\n        model.add(Dense(lyrs[i], activation=act))\n    \n    # add dropout, default is none\n    model.add(Dropout(dr))\n    \n    # create output layer\n    model.add(Dense(1, activation='sigmoid'))  # output layer\n    \n    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n    \n    return model","ca37f529":"model = create_model()\nprint(model.summary())","289f8876":"# train model on full train set, with 80\/20 CV split\ntraining = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\nval_acc = np.mean(training.history['val_acc'])\nprint(\"\\n%s: %.2f%%\" % ('val_acc', val_acc*100))","dc31c587":"# summarize history for accuracy\nplt.plot(training.history['acc'])\nplt.plot(training.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","4577f3bc":"# create model\nmodel = KerasClassifier(build_fn=create_model, verbose=0)\n\n# define the grid search parameters\nbatch_size = [16, 32, 64]\nepochs = [50, 100]\nparam_grid = dict(batch_size=batch_size, epochs=epochs)\n\n# search the grid\ngrid = GridSearchCV(estimator=model, \n                    param_grid=param_grid,\n                    cv=3,\n                    verbose=2)  # include n_jobs=-1 if you are using CPU\n\ngrid_result = grid.fit(X_train, y_train)","203d4ddd":"# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","bf45f096":"# create model\nmodel = KerasClassifier(build_fn=create_model, epochs=50, batch_size=32, verbose=0)\n\n# define the grid search parameters\noptimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Nadam']\nparam_grid = dict(opt=optimizer)\n\n# search the grid\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, verbose=2)\ngrid_result = grid.fit(X_train, y_train)","8f339ac7":"# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","96dbedbc":"seed(42)\nset_random_seed(42)\n\n# create model\nmodel = KerasClassifier(build_fn=create_model, \n                        epochs=50, batch_size=32, verbose=0)\n\n# define the grid search parameters\nlayers = [[8],[10],[10,5],[12,6],[12,8,4]]\nparam_grid = dict(lyrs=layers)\n\n# search the grid\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, verbose=2)\ngrid_result = grid.fit(X_train, y_train)","bb38b5e9":"# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","afc1daf3":"# create model\nmodel = KerasClassifier(build_fn=create_model, \n                        epochs=50, batch_size=32, verbose=0)\n\n# define the grid search parameters\ndrops = [0.0, 0.01, 0.05, 0.1, 0.2, 0.5]\nparam_grid = dict(dr=drops)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, verbose=2)\ngrid_result = grid.fit(X_train, y_train)","a9ac69fc":"# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","6dc6ab1d":"# create final model\nmodel = create_model(lyrs=[8], dr=0.2)\n\nprint(model.summary())","73667c1d":"# train model on full train set, with 80\/20 CV split\ntraining = model.fit(X_train, y_train, epochs=50, batch_size=32, \n                     validation_split=0.2, verbose=0)\n\n# evaluate the model\nscores = model.evaluate(X_train, y_train)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","c5512762":"# summarize history for accuracy\nplt.plot(training.history['acc'])\nplt.plot(training.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","3c5c9449":"# calculate predictions\ntest['Survived'] = model.predict(X_test)\ntest['Survived'] = test['Survived'].apply(lambda x: round(x,0)).astype('int')\nsolution = test[['PassengerId', 'Survived']]","a89cf5ad":"solution.head(10)","49c48a5e":"solution.to_csv(\"Neural_Network_Solution.csv\", index=False)","6fd4ddb9":"## 3.1. Cross-validation\nKeras allows us to make use of cross-validation for training our model. So we will use this to train and assess our first model.","f16fb42f":"### Create neural network model\nFor this task, I have kept the model architecture pretty simple. We have one input layer with 17 nodes which feeds into a hidden layer with 8 nodes and an output layer which is used to predict a passenger's survival.   \n\nThe output layer has a sigmoid activation function, which is used to 'squash' all our outputs to be between 0 and 1.   \n\nWe are going to create a function which allows to parameterise the choice of hyperparameters in the neural network. This might seem a little overly complicated now, but it will come in super handy when we move onto tuning our parameters later.","a36374ae":"## 3.2. Grid search\n### 3.2.1. batch size and epochs\nWe can see from the graph above that we might be training our network for too long. Let's use **grid search** to find out what the optimal values for `batch_size` and `epochs` are.","2d1c64be":"### Train model\nAt this stage, we have our model. We have chosen a few hyperparameters such as the number of hidden layers, the number of neurons and the activation function.\n\nThe next step is to train the model on our training set. This step also requires us to choose a few more hyperparameters such as the loss function, the optimization algorithm, the number of epochs and the batch size.","e05a9cf6":"### Assess results","d74dd78e":"# Table of Contents:\n\n- **1. [Load packages and data](#loading)**\n- **2. [Pre-processing](#Pre-processing)**\n  - **2.1. [Variable Encoding](#encoding)**\n  - **2.2. [Variable Scaling](#scaling)**\n- **3. [Neural Network](#Neural Network)**","49e80ed0":"### 3.2.4. Dropout","bd43f68b":"## 2.2. Scale Continuous Variables\nThe continuous variables need to be scaled. This is done using a standard scaler from SkLearn.","ec776c45":"## 3.4. Output Final Predictions","a151c130":"<a id=\"loading\"><\/a>\n# 1. Load packages and data","5b2a7cfe":"## 3.3. Make Predictions on Test Set\nFinally, we can attempt to predict which passengers in the test set survived.","efc7a876":"<a id=\"neural-network\"><\/a>\n# 3. Neural Network\nNow, all that is left is to feed our data that has been cleaned, encoded and scaled to our neural network.\n\nBut first, we need to separate *data_df* back into *train* and *test* sets.","bb273169":"### 3.2.2. Optimization Algorithm","2ba61ea8":"<a id=\"pre-processing\"><\/a>\n# 2. Pre-processing\n<a id=\"encoding\"><\/a>\n## 2.1. Encode Categorical Variables\nWe need to convert all categorical variables into numeric format. The categorical variables we will be keeping are `Embarked`, `Sex` and `Title`.   \n\nThe `Sex` variable can be encoded into single 1-or-0 column, but the other variables will need to be [one-hot encoded](https:\/\/hackernoon.com\/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f). Regular label encoding assigns some category labels higher numerical values. This implies some sort of scale (Embarked = 1 is not **more** than Embarked = 0 - it's just _different_). One Hot Encoding avoids this problem.   \n\nWe will assume that there is some ordinality in the `Pclass` variable, so we will leave that as a single column.","2b5771a2":"# Titanic challenge part 3\nIn this notebook, we will be covering all of the steps required to train, tune and assess a neural network.\n\n**[Part 1](https:\/\/www.kaggle.com\/jamesleslie\/titanic-eda-wrangling-imputation)** of this series dealt with the pre-processing and manipulation of the data. This notebook will make use of the datasets that were created in the first part.\n\nWe will do each of the following:\n- train and test a neural network model\n- use grid search to optimize the hyperparameters\n- submit predictions for the test set\n\n**[Part 2](https:\/\/www.kaggle.com\/jamesleslie\/titanic-random-forest-grid-search)** covered the use of a random forest for tackling this challenge. Now let's see if we can beat that model with a neural network!\n> NOTE: make sure to use a GPU for this notebook, as it will be significantly faster to train","e95fed30":"### 3.2.3. Hidden neurons"}}