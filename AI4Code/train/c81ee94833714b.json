{"cell_type":{"709a46c4":"code","858b9bf2":"code","ab5ed7ac":"code","cd5a57fb":"code","ff74db2f":"code","42ac10c3":"code","0a470c24":"code","204bf65f":"code","22b3fcb0":"code","53ac9d73":"code","b86d8d3f":"code","aa973769":"code","35ee5f85":"code","b1928df4":"code","b0c840f7":"code","0dc72a52":"code","9307a10a":"code","94ca69b1":"code","6931103d":"code","0f4d6bd4":"code","65792bf9":"code","819b96c4":"code","2f563559":"code","97a3f090":"code","49889e11":"code","f72bb9f6":"code","53bed077":"code","a0539632":"code","d89f1081":"code","5c7a34f8":"code","61a176bd":"code","bca648ad":"code","e922aa75":"code","8cfb2648":"code","024184f9":"code","e72434e8":"code","9cddba35":"code","1e00ddfe":"code","2f3c8427":"code","e95dca68":"code","5ce609dc":"code","e3fb7790":"code","d40e6741":"code","f2b04ac7":"code","5d73cca9":"code","6b76d1a2":"code","973ac822":"code","27245e9f":"code","2b27cb25":"code","2ad72615":"code","87ac89ac":"code","9a3336dd":"code","4b8a5acf":"code","ea6ad43d":"code","dcebc658":"code","f94ee6f2":"code","d27aed90":"code","5fdc08ba":"code","2d795b9f":"code","40d04870":"code","e8a65f2c":"code","c9314477":"code","370618d9":"code","bf609e5c":"code","14d949bd":"code","bdf43810":"code","f870dd55":"code","23453269":"code","e4caa9c0":"code","9abf6fb7":"code","ff0ac8d5":"code","d9d7be05":"code","b50f562d":"code","7d3b7a8d":"code","d2398506":"code","14948d4f":"code","aa96a9af":"code","dbae87e4":"code","7d9281c5":"code","b8037327":"code","0165f5c0":"code","82877430":"markdown","e8d6537b":"markdown","a8a60b49":"markdown","d4d65456":"markdown","84635f29":"markdown","11720e31":"markdown","7b5b20fa":"markdown","9c854188":"markdown","ae5d267f":"markdown","a7d61e55":"markdown","93bce168":"markdown","506af48d":"markdown","3aa69e2c":"markdown","74b53c9a":"markdown","bdb0f286":"markdown","8958d408":"markdown","13466861":"markdown","6cbf1452":"markdown","706fe287":"markdown","d271d2a5":"markdown","c07efacc":"markdown","71cfc8c4":"markdown","640c11e8":"markdown","8538ca33":"markdown","2e72badd":"markdown","ca360dba":"markdown","79c4c1fb":"markdown","eeed2ba7":"markdown","ecad965e":"markdown","a0aa7529":"markdown","8f31f72e":"markdown","926424f9":"markdown","91a947d4":"markdown","2552dd66":"markdown","974501f3":"markdown","b555d465":"markdown","9aead403":"markdown","bdc7b871":"markdown","80b3809c":"markdown","8bb3d3e1":"markdown","8602b72b":"markdown","84445934":"markdown","484d86ae":"markdown","37b0d69e":"markdown","27e87237":"markdown","882a2ffc":"markdown","a6443bf9":"markdown"},"source":{"709a46c4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","858b9bf2":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.color_palette(\"Set2\",10)\nsns.set()","ab5ed7ac":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","cd5a57fb":"train_df = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv')","ff74db2f":"# preview data\ntrain_df.head()","42ac10c3":"test_df = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv')","0a470c24":"# preview test data\ntest_df.head()","204bf65f":"train_df.info()","22b3fcb0":"print(\"Total number of passenger records in the training set is {}\".format(train_df.shape[0]))","53ac9d73":"test_df.info()","b86d8d3f":"print(\"Total number of passenger records in the training set is {}\".format(test_df.shape[0]))","aa973769":"train_df.describe()","35ee5f85":"train_df.isnull().sum().sort_values(ascending=False)","b1928df4":"print(\"Percentage of null-values in Cabin %.2f%% \" %(train_df['Cabin'].isnull().sum()\/train_df.shape[0]*100))","b0c840f7":"print(\"Percentage of null-values in Ticket %.2f%% \" %(train_df['Ticket'].isnull().sum()\/train_df.shape[0]*100))","0dc72a52":"print(\"Percentage of null-values in Age %.2f%% \" %(train_df['Age'].isnull().sum()\/train_df.shape[0]*100))","9307a10a":"#Plot the distribution of Age\nplt.figure(figsize=(7,5))\nsns.histplot(data = train_df['Age'],bins=20,kde=True, color='coral',element=\"step\", alpha=0.4)\nplt.title('Distribution of Age')\nplt.ylabel('No of Passengers')\nplt.show()","94ca69b1":"print('The mean Age is',train_df['Age'].mean())\nprint('The median Age is',train_df['Age'].median())","6931103d":"sns.boxplot(x=train_df['Age'], color='turquoise')","0f4d6bd4":"print(\"Percentage of null-values in Fare %.2f%% \" %(train_df['Fare'].isnull().sum()\/train_df.shape[0]*100))","65792bf9":"# Visualize distribution of Fare\n\nplt.figure(figsize=(7,5))\nsns.histplot(data = train_df['Fare'],bins=50, kde=True, color='green',element=\"step\", alpha=0.3)\nplt.title('Distribution of Fare')\nplt.show()","819b96c4":"sns.boxplot(data=train_df, y=train_df['Fare'], x=train_df['Pclass'], palette='Set2')","2f563559":"print(\"Percentage of null-values in Embarked %.2f%% \" %(train_df['Embarked'].isnull().sum()\/train_df.shape[0]*100))","97a3f090":"#Visualize the distribution of Embarked\nplt.figure(figsize=(7,5))\nsns.countplot(x='Embarked',data=train_df, palette='Set2')\nplt.title('Distribution of Embarked')\nplt.ylabel('No of passengers')\nplt.show()","49889e11":"#Group based on Pclass\ntrain_pclass1 = train_df.iloc[np.where(train_df['Pclass'] == 1)]\ntrain_pclass2 = train_df.iloc[np.where(train_df['Pclass'] == 2)]\ntrain_pclass3 = train_df.iloc[np.where(train_df['Pclass'] == 3)]","f72bb9f6":"# Handling outliers where Pclass=1\nq1 = train_pclass1['Fare'].quantile(0.25)\nq3 = train_pclass1['Fare'].quantile(0.75)\nIQR = q3-q1\nm = q3 + 1.5*IQR","53bed077":"train_pclass1 = train_pclass1.iloc[np.where(train_pclass1['Fare'] < m)]","a0539632":"# Handling outliers where Pclass=2\nq1 = train_pclass2['Fare'].quantile(0.25)\nq3 = train_pclass2['Fare'].quantile(0.75)\nIQR = q3-q1\nm = q3 + 1.5*IQR\n\ntrain_pclass2 = train_pclass2.iloc[np.where(train_pclass2['Fare'] < m)]","d89f1081":"# Handling outliers where Pclass=3\nq1 = train_pclass3['Fare'].quantile(0.25)\nq3 = train_pclass3['Fare'].quantile(0.75)\nIQR = q3-q1\nm = q3 + 1.5*IQR\n\ntrain_pclass3 = train_pclass3.iloc[np.where(train_pclass3['Fare'] < m)]","5c7a34f8":"#concatenate the 3 dataframes\ntrain_df = pd.concat([train_pclass1,train_pclass2,train_pclass3], axis=0)","61a176bd":"train_df.shape","bca648ad":"#Boxplot of Fare\nplt.figure(figsize=(7,5))\nsns.boxplot(data=train_df, y=train_df['Fare'], x=train_df['Pclass'], palette='Set2')\nplt.title(\"Checking for outliers\")\nplt.show()","e922aa75":"upper = train_df.loc[train_df['Pclass'] == 1]['Survived']\nupper_percent = (sum(upper)\/len(upper))*100\nprint(\" %.2f%% of upper class passengers survived\" %upper_percent)\n\nmiddle = train_df.loc[train_df['Pclass'] == 2]['Survived']\nmiddle_percent = (sum(middle)\/len(middle))*100\nprint(\" %.2f%% of middle class passengers survived\" %middle_percent)\n\nlower = train_df.loc[train_df['Pclass'] == 3]['Survived']\nlower_percent = (sum(lower)\/len(lower))*100\nprint(\" %.2f%% of lower class passengers survived\" %lower_percent)","8cfb2648":"plt.figure(figsize=(7,5))\nsns.countplot(x='Survived',hue='Pclass',data=train_df, palette='Set2')\nplt.title('Analysing Survived passengers by Pclass')\nplt.ylabel('No of passengers')\nplt.show()","024184f9":"male = train_df.loc[train_df['Sex'] == 'male']['Survived']\nmale_percent = (sum(male)\/len(male))*100\nprint(\"Percentage of men who survived %.2f%% \" %male_percent)\n\nfemale = train_df.loc[train_df['Sex'] == 'female']['Survived']\nfemale_percent = (sum(female)\/len(female))*100\nprint(\"Percentage of women who survived %.2f%%\" %female_percent)","e72434e8":"plt.figure(figsize=(7,5))\nsns.countplot(x='Survived',hue='Sex',data=train_df, palette='Set2' )\nplt.title('Analysing Survived by Sex')\nplt.ylabel('No of Passengers')\nplt.show()","9cddba35":"plt.figure(figsize=(7,5))\nax = sns.kdeplot(train_df[train_df['Survived'] == 1]['Fare'], shade=True, legend=True, color='coral')\nax = sns.kdeplot(train_df[train_df['Survived'] == 0]['Fare'], shade=True, legend=True, color='teal')\nplt.title('Relationship between Fare and Survival')\nax.legend(['Survived','Deceased'])\nplt.show()","1e00ddfe":"# Impute train data\nage_median = train_df['Age'].median()\ntrain_df['Age'] = train_df['Age'].fillna(age_median)","2f3c8427":"# Impute test data\ntest_df['Age'] = test_df['Age'].fillna(age_median)","e95dca68":"# Impute train data\ntrain_mean_fare = lambda x: x.fillna(x.mean())\ntrain_df['Fare'] = train_df.groupby(['Pclass'])['Fare'].apply(lambda x: x.fillna(x.mean()))","5ce609dc":"# Store fare mean by Pclass for handling missing values in test data\nimpute_fare = train_df.groupby(['Pclass'])['Fare'].mean()\nimpute_fare","e3fb7790":"pclass1_mean_fare = impute_fare.iloc[0]\npclass2_mean_fare = impute_fare.iloc[1]\npclass3_mean_fare = impute_fare.iloc[2]","d40e6741":"# Impute missing values in test data\ntest_df['Fare'] = np.where(test_df['Pclass']==1, test_df['Pclass'].fillna(pclass1_mean_fare),test_df['Fare'])\ntest_df['Fare'] = np.where(test_df['Pclass']==2, test_df['Pclass'].fillna(pclass2_mean_fare),test_df['Fare'])\ntest_df['Fare'] = np.where(test_df['Pclass']==3, test_df['Pclass'].fillna(pclass3_mean_fare),test_df['Fare'])","f2b04ac7":"test_df['Fare'].isnull().sum()","5d73cca9":"# Imputing train data\nfreq_embarked = train_df['Embarked'].mode()[0]\ntrain_df['Embarked'] = train_df['Embarked'].fillna(freq_embarked)","6b76d1a2":"# Imputing test data\ntest_df['Embarked'] = test_df['Embarked'].fillna(freq_embarked)","973ac822":"# Impute train data\ntrain_df['Family'] = np.where(train_df['SibSp']+train_df['Parch'] > 0, 1,0)","27245e9f":"# Impute test data\ntest_df['Family'] = np.where(test_df['SibSp']+test_df['Parch'] > 0, 1,0)","2b27cb25":"# Impute train data\ntrain_df['Pclass'] = train_df['Pclass'].astype(str)\ntrain_onehot = pd.get_dummies(train_df[['Pclass','Sex','Embarked']], drop_first=True)","2ad72615":"train_onehot.columns","87ac89ac":"train_df = pd.concat([train_df,train_onehot], axis=1)","9a3336dd":"#  Impute test data\ntest_df['Pclass'] = test_df['Pclass'].astype(str)\ntest_onehot = pd.get_dummies(test_df[['Pclass','Sex','Embarked']], drop_first=True)","4b8a5acf":"test_onehot.columns","ea6ad43d":"test_df = pd.concat([test_df,test_onehot], axis=1)","dcebc658":"train_df.head()","f94ee6f2":"# Drop features in train data\ntrain_df.drop(['Sex','Embarked','SibSp','Parch','Pclass'], axis=1, inplace=True)\ntrain_df.drop(['PassengerId','Name','Cabin','Ticket'], axis=1, inplace=True)\n","d27aed90":"train_df.columns","5fdc08ba":"# Drop features in test data\ntest_df.drop(['Sex','Embarked','SibSp','Parch','Pclass'], axis=1, inplace=True)\ntest_df.drop(['Name','Cabin','Ticket'], axis=1, inplace=True)","2d795b9f":"test_df.columns","40d04870":"X = train_df.drop(['Survived'], axis=1)","e8a65f2c":"y = train_df['Survived']","c9314477":"scaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)","370618d9":"X_scaled","bf609e5c":"vif = pd.DataFrame()\nvif['vif'] = [variance_inflation_factor(X_scaled,i) for i in range(X_scaled.shape[1])] \nvif['Features'] = X.columns","14d949bd":"vif","bdf43810":"# Split data\nX_train, X_test, y_train, y_test = train_test_split(X_scaled,y,test_size=0.20, random_state=355)\n","f870dd55":"log_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)","23453269":"y_pred = log_reg.predict(X_test)","e4caa9c0":"log_reg.score(X_test,y_test)","9abf6fb7":"# Print confusion matrix\nconf_matrix = confusion_matrix(y_test,y_pred)\nconf_matrix","ff0ac8d5":"# Plot confusion matrix on heatmap\nplt.figure(figsize=(4,4))\nsns.heatmap(confusion_matrix(y_test,y_pred), annot=True, fmt='.0f', cbar=False, cmap='Greys')\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.show()","d9d7be05":"accuracy = accuracy_score(y_test,y_pred)\nprint(\"Accuracy of the logistic regression model is %2.3f\" % accuracy)","b50f562d":"true_positive = conf_matrix[0][0]\nfalse_positive = conf_matrix[0][1]\nfalse_negative = conf_matrix[1][0]\ntrue_negative = conf_matrix[1][1]","7d3b7a8d":"Precision = true_positive \/ (true_positive + false_positive)\nRecall = true_positive \/ (true_positive + false_negative)\nSpecificity = true_negative \/ (true_negative + false_positive)\nFalse_positive_rate = 1-Specificity\nauc = roc_auc_score(y_test,y_pred)","d2398506":"print(\"Accuracy of the logistic regression model is %2.3f\" % accuracy)\nprint(\"Precision of the logistic regression model is %2.3f\" % Precision)\nprint(\"Recall of the logistic regression model is %2.3f\" % Recall)\nprint(\"Specificity of the logistic regression model is %2.3f\" % Specificity)\nprint(\"False positive rate of the logistic regression model is %2.3f\" % False_positive_rate)\nprint(\"AUC of the logistic regression model is %2.3f\" % auc)","14948d4f":"test_df.head()","aa96a9af":"test_result = pd.DataFrame()\ntest_result['PassengerId']=test_df['PassengerId']","dbae87e4":"test_df.drop(['PassengerId'],axis=1,inplace=True)\ntest_df.shape","7d9281c5":"test_result['Survived'] = log_reg.predict(test_df)","b8037327":"test_result","0165f5c0":"test_result.to_csv('submission.csv',header=True,index=False)","82877430":"Clearly Fare is left skewed and also has lot of outliers at the upper extreme. We will remove some of the outliers using quartiles and IQR.\n\nLet's look at the outliers more clearly using boxplot","e8d6537b":"## 3. Exploratory data analysis\n\nIn this section, we will explore some more features and look for relationship with 'Survived'\n\nLet's draw some plots and gather insights","a8a60b49":"## 2. Data Preprocessing\n\n### 2.1 Explore data and its attribues","d4d65456":"Its is true that being a women increased the chances of being on life boat and survival\n\n71 % of women survived when compared to 20 % of men who survived the disaster.","84635f29":"## Introduction\n\nThis is my first ever kernal on Kaggle and also my first Machine Learning project.\nThis is based on the 'Tabular Playground Sythanic April competition'.\nI have done Exploratory data analysis of the given data, cleansed the data, applied some feature engineering techniques and then fit the data to a Logistic Regression model.\nI am open to feedback and evaluation on this kernel.\nBelow are the steps followed in the analysis\n1. Import Packages and Read data\n2. Data Preprocessing\n     2.1. Explore shape and attribues\n     2.2. Describe data\n     2.3. Check for missing values\n         2.3.1. Cabin\n         2.3.2. Ticket\n         2.3.3. Age\n         2.3.4. Fare\n         2.3.5. Embarked\n     2.4. Handle Outliers     \n3. Exploratory Data Analysis\n4. Feature Engineering\n    4.1. Handle missing values for continuos features\n        4.1.1. Impute null values in Age\n        4.1.2. Impute missing values in Fare\n    4.2. Handle missing values for categorical features\n        4.2.1. Impute null values in Embarked\n        4.2.2. Create new feature from SibSp and Parch\n        4.2.3. One-Hot Encoding for Pclass, Sex, Embarked\n    4.3. Drop unnecessary features\n5. Fit data to Logistic Regression model\n6. Prediction for Kaggle test data\n","11720e31":"## 6. Prediction for Kaggle Test data","7b5b20fa":"From the graph, its obvious that passengers belonging to upper class have higher chances of survival.\n\nAlso most passengers who didnt survive are from lower class","9c854188":"There are only 0.25% null values in Port of Embarkation. So we'll impute this with the most frequent port embarked.","ae5d267f":"### 4.2 Handle missing values for categorical features\n\nThere are 3 categorical features to be imputed.\n* Embarked\n* Pclass\n* SibSp & Parch --> combine to create new attribute\n\nWe will perform One-Hot encoding for Pclass and Embarked features.\n\nSibSp --> Presence of Sibling\/Spouse on board\nParch --> Presence of Parent\/Child on board\nWe will create a new feature based on these two, to indicate the presence of Family on board\n\nWe will impute missing in our training data and then we'll apply the same logic and impute missing values in final test data (kaggle) as well.","a7d61e55":"### 4.1 Handle missing values for continuous features\n\nAs we have already explored the different features, we have a fair idea of how to impute missing values for different features\n\nWe will impute missing in our training data and then we'll apply the same logic and impute missing values in final test data (kaggle) as well.","93bce168":"As we have already handled the extreme outliers, we can leave the outliers still present in the data untreated. ","506af48d":"The above table gives  a brief decription of the numerical attributes in training data","3aa69e2c":"Fare is the ticket fare and its based on the class of the Passenger. So we'll group passenger records by Passenger class, calculate the average fare and impute missing fare values based on this.\n\nLet's look at the distribution of Fare attribute","74b53c9a":"#### 4.1.1 Impute null values in Age","bdb0f286":"### 4.3. Drop unnecessary features\nLet's drop the features that are no longer needed after one hot encoding.","8958d408":"#### 4.2.2 One-Hot Encoding for Pclass, Sex, Embarked","13466861":"### Read Test dataset","6cbf1452":"##### 2.3.1 Cabin","706fe287":"Cabin has more than 67.87% of null values, and it doesn't make much sense to impute this attribute. So we'll drop this attribute during Feature engineering process","d271d2a5":"Ticket column refernces the ticket number and we wouldn't use it for our analysis. So we'll drop this attribute as well","c07efacc":"### Read Training dataset","71cfc8c4":"### 2.4 Handle Outliers\n\nFare column has outliers.\n\n1. Because Fare is associated with PClass, We will group the records by Pclass \n2. We can calculate IQR --> Inter Quartile Range and Quantile(0.75).\n3. Compute Quantile(0.75) + (1.5 * IQR)\n4. Any value above this can be treated as outlier and dropped.","640c11e8":"From the plot, we can infer that people who paid less fare have low probability of survival.","8538ca33":"Since the attributes are of different scales, we will scale the data using standard scaler\n\nThen we have to check for multicollinearity in the dependent variables using VIF (Variance Inflation Factor)","2e72badd":"As we can see Age is slightly left skewed, so we will consider median for imputing null values.","ca360dba":"This is transformed data after scaling","79c4c1fb":"##### 2.3.3 Age","eeed2ba7":"## 4. Feature Engineering\n1. Handle missing values for continuous features\n2. One Hot Encoding for categorical features","ecad965e":"Age will be one of the most important attribute to predict the survived.\n\nWe wil impute the missing age values with either mean or median, depending on the data.\n\nLet's look at the distribution of Age","a0aa7529":"##### 2.3.2 Ticket","8f31f72e":"#### 4.1.2 Impute missing values in Fare","926424f9":"## 5. Fit data to Logistic Regression model\n","91a947d4":"#### 4.2.1 Create new feature from SibSp and Parch","2552dd66":"We will group the passenger records by Passenger Class and then remvove the Fare outliers from each group. We will perform this in the next section","974501f3":"#### 4.2.1 Impute null values in Embarked\n* We will impute missing values with the most frequent port of Embarkment","b555d465":"Let's see if there is any relationship between PClass and Survived passengers","9aead403":"#### Is there any relationship between Fare and survival?","bdc7b871":"##### 2.3.5 Embarked","80b3809c":"### 2.3 Check for missing values\n\nLet's check the data for missing values, so that we will handle them in the Feature Engineering section","8bb3d3e1":"'Survived' is the attribute to the predicted. Hence its not part of the test dataset","8602b72b":"#### Did more women survive than men?","84445934":"## 1. Import Packages and Read data","484d86ae":"### 2.2 Describe data","37b0d69e":"All Vif values are very low --> no multicollinearity\n\nWe will split the training data into train and test data, so that we can check the accuracy of prediction\n\nThe test data provided by kaggle is for final prediction and submission\n\nSo, let's split the training data into train (80%) and test(20%)","27e87237":"6777 records are dropped as outliers in Fare attribute\n\nLet's draw the boxplot of Fare to check if the outliers are handled well","882a2ffc":"All of the attributes with null values have been explored. We will look at the distribution of all the attributes after handling outliers and null-values","a6443bf9":"##### 2.3.4 Fare"}}