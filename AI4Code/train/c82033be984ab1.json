{"cell_type":{"89c310b3":"code","221c0bb2":"code","ed3590a4":"code","23d9434b":"code","cb7fcde8":"code","11ae298c":"code","c0e4f452":"code","71460297":"code","1c7dbd0f":"code","da4f6685":"code","3a3dcf67":"code","7df149a5":"code","47f35e0b":"code","80342c63":"code","cde35872":"code","eb7fdd1e":"code","589b5654":"code","d54d0d94":"code","fc422b12":"code","fd1357c3":"code","159a6ac1":"code","37d12128":"code","909a27cf":"code","e8e5cb33":"code","f5828ca6":"markdown","3719013b":"markdown","400c8a06":"markdown","044ed00a":"markdown","32400da2":"markdown","4cdd9701":"markdown","3fc9ffeb":"markdown","c6b26eed":"markdown","f397e26b":"markdown","3b648a4c":"markdown","007592ce":"markdown"},"source":{"89c310b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","221c0bb2":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas import to_datetime\nfrom fbprophet import Prophet","ed3590a4":"df = pd.read_csv('\/kaggle\/input\/computer-network-traffic\/cs448b_ipasn.csv')","23d9434b":"print('dataframe shape {} '.format(df.shape))","cb7fcde8":"print(df.head(10))","11ae298c":"print('Number of days for which data is available {:d}'.format(df['date'].nunique()))\nprint('Unique local ip {:d}'.format(df['l_ipn'].nunique()))\nprint('Unique remote ASN {:d}'.format(df['r_asn'].nunique()))\nprint('Minimum flow count per day {:d}'.format(df['f'].min()))\nprint('Maximum flow count per day {:d}'.format(df['f'].max()))","c0e4f452":"#Missing or NAN values in dataset\ndf.isnull().sum()","71460297":"dic = {'2006-08-24':1,'2006-09-04':5,'2006-09-18':4,'2006-09-26':3,'2006-09-26':6}\nmarked_anomalies = pd.DataFrame.from_dict(dic,orient='index')\nmarked_anomalies.reset_index(inplace = True)\nmarked_anomalies.columns = ['date','l_ipn']\nprint(marked_anomalies)","1c7dbd0f":"#Aggregating daily connections\ndaily_aggregate = df.groupby(['date'])[['f']].sum()\ndaily_aggregate.reset_index(inplace = True)","da4f6685":"daily_aggregate[['f']].describe()","3a3dcf67":"daily_mean = round(daily_aggregate['f'].mean(),2)","7df149a5":"plt.figure(figsize=(15,5))\nplt.plot(daily_aggregate['date'],daily_aggregate['f'])\n[plt.axvline(x=_x, color='r' , label = 'Recorded Anomoly {}'.format(ip)) for _x,ip in list(marked_anomalies[['date','l_ipn']].to_records(index=False))]\nplt.axhline(y= daily_mean, color='g', label = 'Mean Connections')\nplt.plot(daily_aggregate['date'],daily_aggregate['f'].rolling(7).mean(), label = '7 days Rolling average')\nplt.xticks(daily_aggregate['date'][::2],  rotation='vertical')\nplt.yscale('log')\nplt.xlabel('date')\nplt.ylabel('Connection')\nplt.title('Daily Aggregate Connections')\nplt.fill_between(daily_aggregate['date'],daily_aggregate['f'],color='aqua')\nplt.legend()\nplt.show()","47f35e0b":"daily_aggregate_l_ipn = df.groupby(['l_ipn','date'])[['f']].sum()\ndaily_aggregate_l_ipn.reset_index(inplace= True)","80342c63":"import matplotlib.dates as mdates\n\nfig, axes = plt.subplots(nrows=5, ncols=2, figsize=(20, 15))\n\nplot_row = 0\nplot_col = 0\n\nfor i in range(df['l_ipn'].nunique()):\n    temp = daily_aggregate_l_ipn[daily_aggregate_l_ipn['l_ipn'] == i]\n    axes[plot_row,plot_col].set_title(i)\n    axes[plot_row,plot_col].set_xlabel('date')\n    axes[plot_row,plot_col].set_ylabel('connections')\n    \n    axes[plot_row,plot_col].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n    \n    axes[plot_row,plot_col].plot(temp['date'],temp['f'], color = 'salmon')\n    axes[plot_row,plot_col].get_xaxis().set_visible(False)\n    axes[plot_row,plot_col].fill_between(temp['date'],temp['f'], color='peachpuff')\n    \n\n    plot_col = plot_col + 1\n    if(plot_col == 2):\n        plot_row = plot_row + 1\n        plot_col = 0\nplt.show()","cde35872":"daily_aggregate_r_asn = df.groupby(['r_asn'])[['f']].sum()\ndaily_aggregate_r_asn.reset_index(inplace = True)","eb7fdd1e":"pd.options.display.float_format = '{:.2f}'.format\ndaily_aggregate_r_asn['f'].describe()","589b5654":"plt.figure(figsize=(10,5))\nplt.title(i)\nplt.xlabel('r_asn')\nplt.ylabel('connections')\nplt.xticks(rotation='vertical')\n#n_bins =  daily_aggregate_r_asn['r_asn']\n#plt.hist(daily_aggregate_r_asn['f'], n_bins, histtype ='bar')\nplt.plot(daily_aggregate_r_asn['r_asn'],daily_aggregate_r_asn['f'], color = 'salmon')\nplt.show()","d54d0d94":"def get_daily_aggregate_l_ipn(in_l_ipn):\n    temp_df = daily_aggregate_l_ipn[daily_aggregate_l_ipn['l_ipn'] == in_l_ipn].drop(['l_ipn'],axis = 1)\n    temp_df.columns = ['ds','y']\n    temp_df['ds'] = to_datetime(temp_df['ds'])\n    temp_df.reset_index(inplace=True,drop=True)\n    return temp_df","fc422b12":"def get_forecast(ts,in_l_ipn):\n    \n    model = Prophet(seasonality_mode='additive',daily_seasonality = False, yearly_seasonality = False, weekly_seasonality = True)\n    model.fit(ts)\n    forecast = model.predict(pd.DataFrame(ts['ds']))\n    \n    ts['anomaly'] = 0\n    p_color = np.full((ts.shape[0],1),'green')\n    for i in range(forecast.shape[0]):\n        if((forecast.at[i,'yhat_lower'] > ts.at[i,'y']) or (forecast.at[i,'yhat_upper'] < ts.at[i,'y'])):\n            ts.at[i,'anomaly'] = 1\n            p_color[i] = 'red'\n                \n    model.plot(forecast)\n    \n    plt.scatter(ts['ds'],ts['y'],c=p_color.ravel())\n    plt.title('Forcast plot for l_ipn %d' %in_l_ipn)\n    plt.show()\n","fd1357c3":"for i in range(df['l_ipn'].nunique()):\n    get_forecast(get_daily_aggregate_l_ipn(i),i)","159a6ac1":"pip install luminol","37d12128":"import luminol\nfrom luminol.anomaly_detector import AnomalyDetector","909a27cf":"def get_luminol_anomalies(in_df):\n    in_df['isAnomaly'] = 0\n    detector = AnomalyDetector(in_df['y'].to_dict())\n    anomalies = detector.get_anomalies()\n    time_period = ()\n    for j in range(len(anomalies)):\n        time_period = anomalies[j].get_time_window()\n        for k in time_period:\n            in_df.at[k,'isAnomaly'] = 1     \n    return(in_df)    ","e8e5cb33":"for i in range(df['l_ipn'].nunique()):\n    t_df = get_luminol_anomalies(get_daily_aggregate_l_ipn(i))\n    \n    colors = {0:'green', 1:'red'}\n   \n    plt.figure(figsize=(10,5))\n    plt.plot(t_df['ds'],t_df['y'])\n    plt.scatter(t_df['ds'],t_df['y'],c=t_df['isAnomaly'].apply(lambda x: colors[x]))\n    plt.title('Forcast plot for l_ipn %d' %i)\n    plt.show()","f5828ca6":"# Data Exploration","3719013b":"*Red dots in the graph are out of forecast so these points can be considered as anomaly*","400c8a06":"# **Notebook**\n\n***Let's get started with notebook by reading datafile and  importing required packages***","044ed00a":"***Each row of dataset shows aggregate connection between l_ipn and r_asn on a day, for example row 0 in dataset says on date 2006-07-01 l_ipn 0 and r_asn 701 has one flow. Similarly row 6 indicate there were 13 flows or connections between l_ipn 0 and r_asn 3561. Lets look at the brief descriptive statistics and clean up missing data***","32400da2":"*Anomalies are marked by red dots in graph*","4cdd9701":"# Anomaly detection using luminol","3fc9ffeb":"# *Anomaly detection using Prophet library*","c6b26eed":"***To understand connections represented in the dataset, lets look at the connection flow aggregated across all L_ipn and r_asn over for each day***","f397e26b":"*Aggregating flows on per r_asn*","3b648a4c":"Along with dataset it is mentioned\n\nReports of \"odd\" activity or suspicions about a machine's behavior triggered investigations on the following days (although the machine might have been compromised earlier)\nDate : IP <br>\n08-24 : 1 <br>\n09-04 : 5 <br>\n09-18 : 4 <br>\n09-26 : 3 6 <br>\n\nWe will keep this information in dataframe for visualisation in diffrent charts","007592ce":"# **Introduction**\n***In this notebook we will explorer timeseries of communication between local computer(l_ipn) and remote resource(r_asn). Available dataset gives per day aggregate connections count between l_ipn and r_asn for each l_ipn. This notebook uses open source forceasting and anomaly detection libraries such as Prophet(https:\/\/facebook.github.io\/prophet\/docs\/quick_start.html) and luminol https:\/\/github.com\/linkedin\/luminol. Objective of this notebook is to see if these libraries can be used to predict abnormal traffic pattern. If you would like to know general information about bots please follow this on wikipedia https:\/\/en.wikipedia.org\/wiki\/Botnet***"}}