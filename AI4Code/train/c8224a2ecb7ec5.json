{"cell_type":{"40d08f26":"code","ab89ae58":"code","3ceeb00e":"code","3366d9cf":"code","9dbbd048":"code","5bc79a92":"code","d3208022":"code","ab69ba78":"code","e284a495":"code","5cd31014":"code","a4db2e92":"code","a88cc507":"code","9d62b8d2":"code","bbbb9dff":"code","91d790fd":"code","310c80b3":"code","61a626ec":"markdown","73cf9e8c":"markdown","e5fec940":"markdown","cce7f491":"markdown","c7a4b2ca":"markdown","d682b16d":"markdown","482d80e9":"markdown","9d1cd9ef":"markdown","db987fa8":"markdown","d61a55a6":"markdown","8c8b3fc0":"markdown","cc585cbd":"markdown","aea4a46e":"markdown"},"source":{"40d08f26":"#importing basic libraries:\n\nimport pandas as pd    #for data preprocessing\nimport numpy as np     #for linear algebra\nimport matplotlib.pyplot as plt   #for data visualisation\nimport seaborn as sns      #for data visualisation\n","ab89ae58":"df = pd.read_csv('..\/input\/iris\/Iris.csv')        #load the dataset\n\ndf.head()      #extracting the head of dataset\n","3ceeb00e":"df.info()           #for checking if there are null values or inconsistent datatype in dataset.\n                    #as we can see there are no NA values so there is no need of preprocessing.","3366d9cf":"df.drop('Id',axis = 1,inplace = True)        #Drop the first column as it doesn't have any information.\ndf.head(3)                                   #extracting the 3 values from dataset.","9dbbd048":"df['Species'].value_counts()         #checking the number of class in target(species column).\n                                     #As we can see, there are 50 values of each type of species class.","5bc79a92":"sns.FacetGrid(df, hue='Species',height= 5).map(plt.scatter, 'SepalLengthCm', 'SepalWidthCm')\nplt.legend()\nplt.show()","d3208022":"sns.FacetGrid(df, hue='Species',height= 5).map(plt.scatter, 'PetalLengthCm', 'PetalWidthCm')\nplt.legend()\nplt.grid()\nplt.show()","ab69ba78":"sns.pairplot(df, hue = 'Species',height = 3)\nplt.show()\n","e284a495":"sns.heatmap(df.corr(),cmap='ocean_r',annot=True)\nplt.show()","5cd31014":"sns.violinplot(data = df ,x ='Species',y = 'PetalLengthCm', height= 15)\nplt.grid()\nplt.show()","a4db2e92":"#importing the libraries\nfrom sklearn.model_selection import train_test_split            #to split the training and testing data\nfrom sklearn.svm import SVC                                     #for support vector machine algorithm\nfrom sklearn.neighbors import KNeighborsClassifier              #for KNN algorithm\nfrom sklearn.tree import DecisionTreeClassifier                 #for DecisionTree algorithm\nfrom sklearn.metrics import confusion_matrix, accuracy_score    #for calculating accuracy\n","a88cc507":"x_train = df.iloc[:,2:4].values         #taking petal length and petal width features values\n\ny_train = df.iloc[:,4].values           #taking the target values\n","9d62b8d2":"training_x, test_x, training_y, test_y = train_test_split(x_train, y_train, test_size = .25, random_state =0)\n# splitting the dataset into training data and testing data(25%)\n","bbbb9dff":"model_1 = SVC()\nmodel_1.fit(x_train,y_train)\n\nprediction_1 = model_1.predict(test_x)\n\nprint(confusion_matrix(test_y, prediction_1))\nprint('\\n')\nprint('Accuracy score:',accuracy_score(test_y,prediction_1))","91d790fd":"model_2 = KNeighborsClassifier(n_neighbors=5)\nmodel_2.fit(x_train,y_train)\n\nprediction_2 = model_2.predict(test_x)\n\nprint(confusion_matrix(test_y, prediction_2))\nprint('\\n')\nprint('Accuracy score:',accuracy_score(test_y,prediction_2))","310c80b3":"model_3 = DecisionTreeClassifier()\nmodel_3.fit(x_train,y_train)\n\nprediction_3 = model_3.predict(test_x)\n\nprint(confusion_matrix(test_y, prediction_3))\nprint('\\n')\nprint('Accuracy score:',accuracy_score(test_y,prediction_3))","61a626ec":"Petal length and petal width gives us a more clean plot than using sepal features.This means that petal features can help us in acheiving more correct predictions.","73cf9e8c":"# K-Nearest Neighbour(KNN):-","e5fec940":"# **DATA VISUALISATION:- **","cce7f491":"# Support Vector Machine(SVM):-","c7a4b2ca":"# BUILDING THE MODEL:-\n\nI use the following ML algorithms for prediction:\n\n1. SVM(Support Vector Machine)\n2. KNN(K-Nearest Neighbour)\n3. Decision Tree Regression\n\nAccording to above visualisation, i use only petals features as they tends to give better result,\n","d682b16d":"Above graph shows that petal length and width have a better correlation between them.","482d80e9":"**Observations:**\n\nBy analysing all the plots above, we can see that petal features are being more useful for predictions.Also petal length is fairly good feature than petal width.","9d1cd9ef":"The above graph shows distribution of different species on the basis of sepal length and sepal width.It can be seen that setosa can be classify on the basis of these two features but other two classes are overlap with each other.Lets try with other remaining features.","db987fa8":"**HELLO EVERYONE!!!**\n\nI use a very basic approach for analysing the data of iris dataset using EDA(Exploratory Data Analysis).First i visualize data with the help of **Matplotlib** and **Seaborn** libraries and then apply following ML algorithms for predictions:\n\n1. SVM\n2. KNN\n3. Decision Tree Regression\n\nI also refer this notebook https:\/\/www.kaggle.com\/ash316\/ml-from-scratch-with-iris\/notebook for better understanding of iris dataset.\n\nIf you find this notebook to be useful then **Please Upvote**.\n\n\n\n","d61a55a6":"**Observations:**\n\nPetals features gives more accurate predictions than sepals as shown by heatmap correlations and by applying above algorithms we verify it.\n\n\n\nGuys this is my first notebook on kaggle.I try to make this notebook as simple as i can. I am also a learner and new to this feild. If you find this notebook to be useful then **Please Upvote**.\n\nThank you.\n","8c8b3fc0":"The violin plot shows the probablity density of each class.It also consist box plots at the centre of the curves.The white dot in the middle of the box plot shows 50th percentile of each class.The thinner the curve ,the lower is the probablity density.","cc585cbd":"# Pair Plots:-\n\nThe pair plots gives us relationship between all the features with each other.With the help of these plots we can easily visualise which feature gives us more better predictions. ","aea4a46e":"# Decision Tree Regression:-"}}