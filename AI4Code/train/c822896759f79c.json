{"cell_type":{"e4a720c3":"code","4f4c7892":"code","305742b8":"code","adc12c96":"code","948ff999":"code","b45a7857":"code","0f77b862":"code","568b709b":"code","871c3289":"markdown","aebb2cdc":"markdown","9832fcf5":"markdown","211e8b67":"markdown","fc3c9772":"markdown","fd3b0b6c":"markdown","b018d0eb":"markdown","33633af4":"markdown","956bba83":"markdown","522777ba":"markdown","88d3d81c":"markdown","bcec915d":"markdown"},"source":{"e4a720c3":"import numpy as np\n\nclass Sgd():\n    def __init__(self, learning_rate):\n        self.learning_rate = learning_rate\n    def calculate_update(self, weight_tensor, gradient_tensor):\n        updated_weight = weight_tensor - (self.learning_rate) * gradient_tensor\n        return updated_weight","4f4c7892":"class FullyConnected():\n    \n    def __init__(self, input_size, output_size):\n        self.input_size = input_size\n        self.output_size = output_size\n        weights = np.random.rand(self.input_size + 1, self.output_size)\n        self.weights = weights\n        self._opt = None\n        \n    def forward(self, input_tensor):\n        batch_size = input_tensor.shape[0]\n        bias_row = np.ones((batch_size,1))\n        X = np.concatenate((input_tensor, bias_row), axis=1)\n        W = self.weights\n        output = np.dot(X, W)\n        self.stored = X\n        self.weights = W\n        return output\n    \n    def getopt(self):\n        return self._opt\n\n    def set_optimizer(self, opt):\n        self._opt = opt\n\n    optimizer = property(getopt,set_optimizer)\n\n    def backward(self, error_tensor):\n        x = self.stored\n        errorpre = np.dot(error_tensor,self.weights[0:self.weights.shape[0]-1,:].T)\n        self.gradient_tensor = np.dot(x.T, error_tensor)\n        if self.optimizer is not None:\n            self.weights = self.optimizer.calculate_update(self.weights, self.gradient_tensor)\n        return errorpre\n\n    @property\n    def gradient_weights(self):\n        return self.gradient_tensor","305742b8":"class ReLU():\n    def __init__(self):\n        pass\n\n    def forward(self, input_tensor):\n        relu = lambda input_tensor: input_tensor * (input_tensor >= 0).astype(float)\n        out = relu(input_tensor)\n        self.store = input_tensor\n        return out\n\n    def backward(self, error_tensor):\n        x = self.store\n        output = error_tensor * (x > 0)\n        return output","adc12c96":"class SoftMax():\n    def __init__(self):\n        pass\n    def forward(self, input_tensor):\n        x = input_tensor\n        xdash = np.exp(x - np.max(x))\n        xdashsum = np.sum(xdash, axis=1)\n        self.store = xdash\/xdashsum[:, None]\n        return xdash\/xdashsum[:, None]\n    \n    def backward(self, error_tensor):\n       e = np.zeros(error_tensor.shape)\n       for i in range(error_tensor.shape[0]):\n           sum = np.sum(np.multiply(error_tensor[i,:], self.store[i,:]))\n           e[i,:]=np.multiply(self.store[i,:],np.subtract(error_tensor[i,:],sum))\n       return e","948ff999":"class CrossEntropyLoss():\n\n    def __init__(self):\n        pass\n\n    def forward(self, input_tensor, label_tensor):\n\n        loss= np.sum(label_tensor*(-np.log(input_tensor + np.finfo('float').eps)))\n        self.input = input_tensor\n        return loss\n\n    def backward(self, label_tensor):\n        back = -np.divide(label_tensor,self.input)\n        return back\n","b45a7857":"import copy\nclass NeuralNetwork():\n    def __init__(self, optimizer):\n        self.optimizer = optimizer\n        self.loss = []\n        self.layers = []\n        self.data_layer = None\n        self.loss_layer = None\n\n    def forward(self):\n        input_tensor, label_tensor = self.data_layer.forward()\n        iptensor = input_tensor\n        self.label_tensor = label_tensor\n        for layer in self.layers:\n            iptensor = layer.forward(iptensor)\n        self.output = iptensor\n        loss = self.loss_layer.forward(iptensor, self.label_tensor)\n        return loss\n\n    def backward(self):\n        error_tensor = self.loss_layer.backward(self.label_tensor)\n        for layer in reversed(self.layers):\n            error_tensor = layer.backward(error_tensor)\n\n    def append_trainable_layer(self, layer):\n        dcopy = copy.deepcopy(self.optimizer)\n        layer.optimizer = dcopy\n        self.layers.append(layer)\n\n\n    def train(self, iterations):\n        for i in range(iterations):\n            self.forward()\n            loss = self.loss_layer.forward(self.output, self.label_tensor)\n            self.loss.append(loss)\n            self.backward()\n\n    def test(self, input_tensor):\n\n        current_input_tensor = input_tensor\n        for layer in self.layers:\n            current_input_tensor = layer.forward(current_input_tensor)\n        return current_input_tensor","0f77b862":"from sklearn.datasets import load_iris\nfrom sklearn.preprocessing import OneHotEncoder\nfrom random import shuffle\nimport matplotlib.pyplot as plt\n\nclass IrisData:\n    def __init__(self, batch_size):\n        self.batch_size = batch_size\n        self._data = load_iris()\n        self._label_tensor = OneHotEncoder(sparse=False).fit_transform(self._data.target.reshape(-1, 1))\n        self._input_tensor = self._data.data\n        self._input_tensor \/= np.abs(self._input_tensor).max()\n\n        self.split = int(self._input_tensor.shape[0]*(2\/3))  # train \/ test split  == number of samples in train set\n\n        self._input_tensor, self._label_tensor = shuffle_data(self._input_tensor, self._label_tensor)\n        self._input_tensor_train = self._input_tensor[:self.split, :]\n        self._label_tensor_train = self._label_tensor[:self.split, :]\n        self._input_tensor_test = self._input_tensor[self.split:, :]\n        self._label_tensor_test = self._label_tensor[self.split:, :]\n\n        self._current_forward_idx_iterator = self._forward_idx_iterator()\n\n    def _forward_idx_iterator(self):\n        num_iterations = int(np.ceil(self.split \/ self.batch_size))\n        idx = np.arange(self.split)\n        while True:\n            this_idx = np.random.choice(idx, self.split, replace=False)\n            for i in range(num_iterations):\n                yield this_idx[i * self.batch_size:(i + 1) * self.batch_size]\n\n    def forward(self):\n        idx = next(self._current_forward_idx_iterator)\n        return self._input_tensor_train[idx, :], self._label_tensor_train[idx, :]\n\n    def get_test_set(self):\n        return self._input_tensor_test, self._label_tensor_test","568b709b":"net = NeuralNetwork(Sgd(1e-3))\ncategories = 3\ninput_size = 4\nnet.data_layer = IrisData(50)\nnet.loss_layer = CrossEntropyLoss()\n\nfcl_1 = FullyConnected(input_size, categories)\nnet.append_trainable_layer(fcl_1)\nnet.layers.append(ReLU())\nfcl_2 = FullyConnected(categories, categories)\nnet.append_trainable_layer(fcl_2)\nnet.layers.append(SoftMax())\n\nnet.train(4000)\nplt.figure('Loss function for a Neural Net on the Iris dataset using SGD')\nplt.plot(net.loss, '-x')\nplt.show()\n\ndata, labels = net.data_layer.get_test_set()\n\nresults = net.test(data)\nindex_maximum = np.argmax(results, axis=1)\none_hot_vector = np.zeros_like(results)\nfor i in range(one_hot_vector.shape[0]):\n    one_hot_vector[i, index_maximum[i]] = 1\n\n    correct = 0.\n    wrong = 0.\nfor column_results, column_labels in zip(one_hot_vector, labels):\n    if column_results[column_labels > 0].all() > 0:\n        correct += 1\n    else:\n        wrong += 1\n\naccuracy = correct \/ (correct + wrong)\nprint('\\nOn the Iris dataset, we achieve an accuracy of: ' + str(accuracy * 100) + '%')","871c3289":"ReLU is an activation function that introduces a non-linearity. It will be treated as a layer for easy implementation, hence will have a forward and a backward method for forward passing the input tensor and backward passing the error tensor respectively.","aebb2cdc":"Let's try this Neural Network on the iris dataset provided by sklearn library.","9832fcf5":"Finally, all the classes will be integrated in the neural network skeleton.","211e8b67":"<img src=\"Desktop\\0_T5jaa2othYfXZX9W_.jpg\">","fc3c9772":"<img src=\"Desktop\\1_DW0Ccmj1hZ0OvSXi7Kz5MQ.jpeg\">","fd3b0b6c":"An implementation of neural network from scratch using only numpy.","b018d0eb":"Each layers are called a Fully Connected Layer. The Fully Connected layer, optimizer, Loss, the activation functions ReLU and Softmax will be implemented seperately, and be finally called while implementing the network structure.","33633af4":"A fully Connected Layer recieves input size and output size as arguments (done for simplicity and testing). It has a forward method recieving input tensor as argument and returns the output for the next layer. The backward method passes the error tensor to the previous layer, updating the weights simultaneously. The input tensor is a (batch_size x no_of_parameters) numpy array. Weights are initialized randomly.","956bba83":"Softmax Activation function will activate the dataset into probabilities, which works best with one hot encoded labels. The softmax will also be treated as layer (the last layer of our network)","522777ba":"A stochastic gradient descent is a simple optimizer, which will be used to update the weights in the Fully Connected Layers, in every iteration. \nw(k+1) = w(k) \u00e2\u0088\u0092 \u00ce\u00b7 \u00e2\u0088\u0087L(w(k)) \nwhere \u00e2\u0088\u0087L(w(k)) is the gradient, \u00ce\u00b7 is the learning rate of our network, k is the iteration index.","88d3d81c":"Our labels (or label tensor) will be one hot encoded vectors. Following is the difference between label encoded labels and one hot encoded labels. Our label tensor is an identity matrix.","bcec915d":"After forward passing the input tensor through all the layers, softmax will give us the probability of the batches belonging to which class. Cross Entropy Loss, which is often used with Softmax will be used to calculate the error based on the label tensor. This will be the error tensor that will be propagated backwards, and will help update the weights."}}