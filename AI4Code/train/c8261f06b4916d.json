{"cell_type":{"edbf8d34":"code","45c1a97b":"code","3f11aec7":"code","2f714589":"code","96e4afaa":"code","db9e2802":"code","55d427d1":"code","03930243":"code","79c831a9":"code","4515b0dd":"code","0a11c6b8":"markdown","982f7536":"markdown","d3792abd":"markdown","519030f9":"markdown","50766762":"markdown","5df0027f":"markdown","edfc1579":"markdown","042aad4d":"markdown"},"source":{"edbf8d34":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","45c1a97b":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torch.optim as optim\n\nfrom torch.utils.data import DataLoader\n\nimport torchvision.datasets as datasets  \nimport torchvision.transforms as transforms","3f11aec7":"num_classes = 10\nlearning_rate = 1e-3\nbatch_size = 1024\nnum_epochs = 5","2f714589":"model = torchvision.models.vgg16(pretrained=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #To use the GPU if available\n\nmodel.to(device)","96e4afaa":"class Identity(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return x","db9e2802":"for prm in model.parameters():\n    prm.requires_grad = False","55d427d1":"model.avgpool = Identity()\n\nmodel.classifier = nn.Sequential(\n    nn.Linear(512, 100), \n    nn.ReLU(), \n    nn.Linear(100, num_classes)\n)\n\n\nmodel.to(device)","03930243":"train_dataset = datasets.CIFAR10(root=\"\/kaggle\/working\/dataset\/\", train=True, transform=transforms.ToTensor(), download=True)\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","79c831a9":"for epoch in range(num_epochs):\n    losses = []\n\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        \n        # If GPU is active, alter the data accordingly\n        data = data.to(device=device)\n        targets = targets.to(device=device)\n\n        # forward\n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        losses.append(loss.item())\n        \n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()\n\n    print(f\"Cost at epoch {epoch} is {sum(losses)\/len(losses):.5f}\")","4515b0dd":"def check_accuracy(loader, model):\n    \n    if loader.dataset.train:\n        print(\"Checking accuracy on training data\")\n    else:\n        print(\"Checking accuracy on test data\")\n\n    num_correct = 0\n    num_samples = 0\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n        print(\n            f\"Got {num_correct} \/ {num_samples} cases correctly with accuracy {float(num_correct)\/float(num_samples)*100:.2f}\"\n        )\n\n    model.train()\n\n\ncheck_accuracy(train_loader, model)","0a11c6b8":"### The Identity class returns the output same as the input thus preventing any changes in the input.","982f7536":"### Loading the dataset","d3792abd":"### Doing the required changes on the model","519030f9":"### The last two blocks containing the average pooling and classifier units can be altered according to our dataset. Since we have 10 classes in our dataset, we will alter the classifier unit for getting desired output of 10 classes. We will set the avgpooling layer to identity so that it cannot perform any changes on the feature map. ","50766762":"### Setting the hyperparameters","5df0027f":"## An accuracy of about 63% is obtained when the pretrained layers are freezed and only the last two units are trained for our dataset. ","edfc1579":"### We will now freeze the layers above the avgpooling layer so as to perform the backpropagation only on the last two layers while training.","042aad4d":"### In this notebook, I have used PyTorch to perform Transfer Learning on VGG16 Network and how it can be further used according to our need. CIFAR-10 dataset is used to showcase the resulting network's performance.\n\n### Most of the layers of the pretrained model are freezed and only the last two layers are trained for our dataset. It would be much clear in upcoming steps.\n\n### PS: Turn on your GPU for quick run!!"}}