{"cell_type":{"489160e4":"code","7be83189":"code","20e71b52":"code","ea1888db":"code","600d2013":"code","7a09693e":"code","054b5424":"code","5e3c983a":"code","d460c320":"code","f7ab921c":"code","66c21b6c":"code","1fcb422d":"code","482448ea":"code","1f04105a":"code","debcf74b":"code","b8975b8b":"code","ac32f78e":"code","773e09ac":"code","db11bcf5":"code","0775b1b9":"code","e897de82":"code","1a1c7258":"code","ca31cf9a":"code","f517ba71":"code","916409a9":"code","2200f23b":"code","38a86032":"code","aa640335":"code","e314c0be":"code","857c24be":"markdown","27c67423":"markdown","f4e1936e":"markdown","b696b539":"markdown"},"source":{"489160e4":"import os\nimport cv2\nimport math\nimport numpy as np\nimport pandas as pd\n\nimport scikitplot\nimport seaborn as sns\nfrom matplotlib import pyplot\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nimport tensorflow as tf\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Flatten, Dense, GlobalAvgPool2D, GlobalMaxPool2D\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.mobilenet import MobileNet\nfrom tensorflow.keras.utils import plot_model\n\nfrom keras.utils import np_utils","7be83189":"INPUT_PATH = \"..\/input\/fer13-cleaned-dataset\/\"","20e71b52":"total_images = 0\nfor dir_ in os.listdir(INPUT_PATH):\n    count = 0\n    for f in os.listdir(INPUT_PATH + dir_ + \"\/\"):\n        count += 1\n        total_images += 1\n    print(f\"{dir_} has {count} number of images\")\n    \nprint(f\"\\ntotal images are {total_images}\")","ea1888db":"TOP_EMOTIONS = [\"fear\", \"Happy\", \"Neutral\", \"Angry\"]\ntotal_images -= 380\ntotal_images","600d2013":"img_arr = np.empty(shape=(total_images,48,48,3))\nimg_label = np.empty(shape=(total_images))\nlabel_to_text = {}\n\ni = 0\ne = 0\nfor dir_ in os.listdir(INPUT_PATH):\n    if dir_ in TOP_EMOTIONS:\n        label_to_text[e] = dir_\n        for f in os.listdir(INPUT_PATH + dir_ + \"\/\"):\n            img_arr[i] = cv2.imread(INPUT_PATH + dir_ + \"\/\" + f)\n            img_label[i] = e\n            i += 1\n        print(f\"loaded all {dir_} images to numpy arrays\")\n        e += 1\n\nimg_arr.shape, img_label","7a09693e":"label_to_text","054b5424":"fig = pyplot.figure(1, (8,8))\n\nidx = 0\nfor k in label_to_text:\n    sample_indices = np.random.choice(np.where(img_label==k)[0], size=4, replace=False)\n    sample_images = img_arr[sample_indices]\n    for img in sample_images:\n        idx += 1\n        ax = pyplot.subplot(4,4,idx)\n        ax.imshow(img[:,:,0], cmap='gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(label_to_text[k])\n        pyplot.tight_layout()","5e3c983a":"img_label = np_utils.to_categorical(img_label)\nimg_label.shape","d460c320":"img_arr = img_arr \/ 255.","f7ab921c":"X_train, X_test, y_train, y_test = train_test_split(img_arr, img_label,\n                                                    shuffle=True, stratify=img_label,\n                                                    train_size=0.9, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","66c21b6c":"del img_arr\ndel img_label","1fcb422d":"img_width = X_train.shape[1]\nimg_height = X_train.shape[2]\nimg_depth = X_train.shape[3]\nnum_classes = y_train.shape[1]","482448ea":"mobile_net = MobileNet(\n    input_shape = (img_width, img_height, img_depth),\n    include_top = False,\n    weights = \"imagenet\",\n    classes = num_classes\n)\n\nx = mobile_net.layers[-14].output\nglobal_pool = GlobalMaxPool2D(name=\"global_pool\")(x)\nout = Dense(num_classes, activation=\"softmax\", name=\"out_layer\")(global_pool)\n\nmodel = Model(inputs=mobile_net.input, outputs=out)","1f04105a":"plot_model(model, show_shapes=True, show_layer_names=True, expand_nested=True, dpi=50, to_file='mobilenet.png')","debcf74b":"for layer in model.layers[:15]:\n    layer.trainable = False","b8975b8b":"train_datagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.15,\n    height_shift_range=0.15,\n    shear_range=0.15,\n    zoom_range=0.15,\n    horizontal_flip=True,\n    zca_whitening=False,\n)\ntrain_datagen.fit(X_train)","ac32f78e":"\"\"\"\nI used two callbacks one is `early stopping` for avoiding overfitting training data\nand other `ReduceLROnPlateau` for learning rate.\n\"\"\"\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    min_delta=0.00008,\n    patience=11,\n    verbose=1,\n    restore_best_weights=True,\n)\n\nlr_scheduler = ReduceLROnPlateau(\n    monitor='val_accuracy',\n    min_delta=0.0001,\n    factor=0.25,\n    patience=4,\n    min_lr=1e-7,\n    verbose=1,\n)\n\ncallbacks = [\n    early_stopping,\n    lr_scheduler,\n]","773e09ac":"batch_size = 25\nepochs = 40\n\noptims = [\n    optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07),\n    optimizers.Adam(0.01),\n]\n\nmodel.compile(\n        loss='categorical_crossentropy',\n        optimizer=optims[1],\n        metrics=['accuracy']\n)\n\nhistory = model.fit_generator(\n    train_datagen.flow(X_train, y_train, batch_size=batch_size),\n    validation_data=(X_test, y_test),\n    steps_per_epoch=len(X_train) \/ batch_size,\n    epochs=epochs,\n    callbacks=callbacks,\n    use_multiprocessing=True\n)","db11bcf5":"model_yaml = model.to_yaml()\nwith open(\"model_mobelnet.yaml\", \"w\") as yaml_file:\n    yaml_file.write(model_yaml)\n    \nmodel.save(\"model_moblenet.h5\")","0775b1b9":"sns.set()\nfig = pyplot.figure(0, (12, 4))\n\nax = pyplot.subplot(1, 2, 1)\nsns.lineplot(history.epoch, history.history['accuracy'], label='train')\nsns.lineplot(history.epoch, history.history['val_accuracy'], label='valid')\npyplot.title('Accuracy')\npyplot.tight_layout()\n\nax = pyplot.subplot(1, 2, 2)\nsns.lineplot(history.epoch, history.history['loss'], label='train')\nsns.lineplot(history.epoch, history.history['val_loss'], label='valid')\npyplot.title('Loss')\npyplot.tight_layout()\n\npyplot.savefig('epoch_history_mobilenet.png')\npyplot.show()","e897de82":"label_to_text","1a1c7258":"text_to_label = dict((v,k) for k,v in label_to_text.items())\ntext_to_label","ca31cf9a":"yhat_test = np.argmax(model.predict(X_test), axis=1)\nytest_ = np.argmax(y_test, axis=1)\n\nscikitplot.metrics.plot_confusion_matrix(ytest_, yhat_test, figsize=(7,7))\npyplot.savefig(\"confusion_matrix_mobilenet.png\")\n\ntest_accu = np.sum(ytest_ == yhat_test) \/ len(ytest_) * 100\nprint(f\"test accuracy: {round(test_accu, 4)} %\\n\\n\")\n\nprint(classification_report(ytest_, yhat_test))","f517ba71":"np.random.seed(42)\nfear_imgs = np.random.choice(np.where(y_test[:, text_to_label[\"fear\"]]==1)[0], size=9, replace=False)\nangry_imgs = np.random.choice(np.where(y_test[:, text_to_label[\"Angry\"]]==1)[0], size=9, replace=False)\n\nfig = pyplot.figure(1, (18, 4))\n\nfor i, (fear_idx, angry_idx) in enumerate(zip(fear_imgs, angry_imgs)):\n        sample_img = X_test[fear_idx,:,:,:]\n        sample_img = sample_img.reshape(1,*sample_img.shape)\n        pred = label_to_text[np.argmax(model.predict(sample_img), axis=1)[0]]\n\n        ax = pyplot.subplot(2, 9, i+1)\n        ax.imshow(sample_img[0,:,:,0], cmap='gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(f\"t:fear, p:{pred}\")\n\n        sample_img = X_test[angry_idx,:,:,:]\n        sample_img = sample_img.reshape(1,*sample_img.shape)\n        pred = label_to_text[np.argmax(model.predict(sample_img), axis=1)[0]]\n\n        ax = pyplot.subplot(2, 9, i+10)\n        ax.imshow(sample_img[0,:,:,0], cmap='gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(f\"t:angry, p:{pred}\")\n\n        pyplot.tight_layout()","916409a9":"def plot_miss_classified(emotion):\n    miss_happy_indices = np.where((ytest_ != yhat_test) & (ytest_==text_to_label[emotion]))[0]\n    print(f\"total {len(miss_happy_indices)} miss labels out of {len(np.where(ytest_==text_to_label[emotion])[0])} for emotion {emotion}\")\n\n    cols = 15\n    rows = math.ceil(len(miss_happy_indices) \/ cols)\n    fig = pyplot.figure(1, (20, rows * 2))\n\n    for i,idx in enumerate(miss_happy_indices):\n        sample_img = X_test[idx,:,:,:]\n        sample_img = sample_img.reshape(1,*sample_img.shape)\n        pred = label_to_text[np.argmax(model.predict(sample_img), axis=1)[0]]\n\n        ax = pyplot.subplot(rows,cols,i+1)\n        ax.imshow(sample_img[0,:,:,0], cmap='gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(f\"p:{pred}\")    ","2200f23b":"plot_miss_classified(emotion=\"Happy\")","38a86032":"plot_miss_classified(emotion=\"fear\")","aa640335":"plot_miss_classified(emotion=\"Angry\")","e314c0be":"plot_miss_classified(emotion=\"Neutral\")","857c24be":"The confusion matrix clearly shows that our model is doing good job on the class `happy` but it's performance is not that good on other classes. One of the reason for this could be the fact that these classes have less data as compared to `happy`.","27c67423":"One of the reason for such low accuracy is the data quality. Below I have shown all the miss-classified images. Many of them doesn't belong to their true class but are actually looking more likely to the predicted class.","f4e1936e":"As we see most of these are classified as `fear` the reason for that is that most of the fear mouths are opened as we see earlier and many of these mouths are opened like that.","b696b539":"`Splitting the data into training and validation set.`"}}