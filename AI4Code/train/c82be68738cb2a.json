{"cell_type":{"6f72482d":"code","75d67967":"code","d0ead2a9":"code","efa7f537":"code","3acfa534":"code","975b8c0e":"code","1c852f39":"code","148073bc":"code","500a5ade":"code","1b0e6829":"code","de2bd325":"code","192cde05":"code","365a5937":"code","175ea719":"code","c83293c2":"code","c8fb0b74":"code","4af7f565":"code","e68a6959":"code","feeda08b":"code","5f64a848":"code","4b84a367":"code","695f4426":"code","e371fe73":"code","efa773cd":"code","b4665e78":"code","a0d65fb5":"code","ffbe5b21":"code","8d0dafec":"code","9943b707":"code","863a6b25":"code","23bdaa70":"code","80222944":"code","263b5cf2":"code","4b394e8f":"code","63226d4a":"code","55683b7a":"code","86d7c652":"code","711a010f":"code","ac3d3526":"code","85d7facc":"code","adc20ec1":"code","7c169982":"code","b1c49173":"code","0d3631e6":"code","d92b536c":"code","361746ff":"code","5e7ee705":"code","a4565bd5":"code","0bf9ccca":"code","711b9811":"code","47fa1712":"code","f4bae1f4":"code","37a5323b":"code","127d1359":"code","1aa6c6aa":"code","dc345dca":"code","84c46c78":"code","6a5c2aab":"code","7d5b6468":"code","78142b69":"code","91adeefe":"code","1eb9c1e3":"code","5053b0f0":"code","a95f5739":"code","39a2c7c2":"code","af44dd42":"code","e17b259d":"code","6bec53c9":"code","6e5ec248":"code","e36616f5":"code","0c1315e0":"code","2ef4b8fa":"code","069cbed1":"code","710778a7":"code","44ec5ce0":"code","3b52f353":"code","dc76b612":"code","0c96c335":"code","51ff6655":"code","78f84b62":"code","e0e45eaa":"code","9f97d375":"code","540db5b1":"code","cfcbdb5f":"code","600fbb4c":"code","f382c3ae":"code","71e9ccdd":"code","0274102c":"code","b5f14c77":"code","c6f1e9e7":"code","24bf3612":"code","39ad88fa":"code","e7112ed9":"code","3a3541d9":"code","28e4aac4":"code","a0e3acf6":"code","ed406837":"code","7fccc23f":"code","2da9464a":"code","5571d90c":"code","95878a10":"code","acb4e5c8":"code","2692f2e5":"code","c1989ab6":"code","8d7d64f4":"code","8b8956ce":"code","a78dc6f9":"code","c9a9785f":"code","f14d0ffb":"code","08b9b966":"code","5f644432":"code","7616040d":"code","9fffb4fe":"code","1235a643":"code","da6a8086":"code","5d6642e1":"code","906f7644":"code","772c2345":"code","a3bca307":"code","a9fcdc95":"code","f48ae1ee":"code","0d9cb197":"code","20ed4f23":"code","02c4478b":"code","fc9a5d07":"code","5d8a1a6c":"code","504854bf":"code","d933534f":"code","6be7a465":"code","3352f2bf":"code","5f600fe5":"code","72b6b3b4":"code","d66620fc":"code","e902c338":"code","d6381f43":"code","2484e0a9":"code","6b9cb018":"code","50226fe8":"code","11c1b0d5":"code","745059c8":"code","e54d439d":"code","990bd830":"code","8e8505fa":"code","77d034de":"code","bd4c6d7a":"code","76de65d2":"code","a87666c1":"code","840adcc6":"markdown","8dce82d5":"markdown","8d7f21c8":"markdown","127efcec":"markdown","e4680bc7":"markdown","9b38427c":"markdown","f81dc22f":"markdown","1e04143b":"markdown","0eb9fddb":"markdown","42dbabfb":"markdown","3c4100bb":"markdown","1c222cb9":"markdown","71acf9f5":"markdown","14f3ac2d":"markdown","6b78cf90":"markdown","0af1381c":"markdown","9067ee87":"markdown","d9740590":"markdown","af3def01":"markdown","503976c2":"markdown","44d397c8":"markdown","fbdfcaf5":"markdown","3693f467":"markdown","eec7ab53":"markdown","a563d3d4":"markdown","bc7b653a":"markdown","244596df":"markdown","0ac1050c":"markdown","ab773f5f":"markdown","d8c92bbf":"markdown","921e16e6":"markdown","0c6f73f0":"markdown","49fb10c6":"markdown","86d2f0ed":"markdown","10013c9b":"markdown","8c45ed61":"markdown","2cf62c54":"markdown","6dc2cfa3":"markdown","aa172c28":"markdown","422d0614":"markdown","c9a9f03f":"markdown","0e61da9b":"markdown","2d94a5d6":"markdown","b3d9ff63":"markdown","d5486405":"markdown","84c927bb":"markdown","4aae9b6f":"markdown","41192324":"markdown","38a38bf6":"markdown","9b92607f":"markdown","5591e010":"markdown","de32dffc":"markdown","4375319f":"markdown","03c01058":"markdown","2cb642c8":"markdown","215a2adb":"markdown","042084a4":"markdown","2e1a1e94":"markdown","d90aa4a7":"markdown","bf503873":"markdown","ba6d8786":"markdown","fcf5fc11":"markdown","6a8c9f6a":"markdown","2f1a0b7e":"markdown","2d0fa164":"markdown","59b48925":"markdown","3d931713":"markdown","46a3fb5d":"markdown","cfbdb97d":"markdown","3e00502e":"markdown","d907d943":"markdown","567a25d8":"markdown","ff178a6e":"markdown","a4107f80":"markdown","88632291":"markdown","dca28dc1":"markdown","8cafa5af":"markdown","cee7a307":"markdown","f087ae86":"markdown","2ab46e1f":"markdown","277df5e4":"markdown","b17a6ff1":"markdown","24e171e4":"markdown","48a8222c":"markdown","f9fbf2ba":"markdown","65d713ae":"markdown","1dd0b333":"markdown","c039a4b9":"markdown","23de3a75":"markdown","21fc55d9":"markdown","8cf6b9ea":"markdown","dc0ed138":"markdown","5d3c915d":"markdown","f804ea84":"markdown","32216977":"markdown","463d5119":"markdown","53221064":"markdown","53277481":"markdown","123fc7e3":"markdown","9724e5d0":"markdown","3b54e7b9":"markdown","dd63766c":"markdown","aeb0da66":"markdown","40c64f3c":"markdown","2fc144f6":"markdown","a0ecebb9":"markdown","effdc6c8":"markdown","d55b6f00":"markdown","541ab1f7":"markdown","cde5ba5b":"markdown","c0e12f4c":"markdown","bae874f2":"markdown","e40a2193":"markdown","c1ca91b6":"markdown","0de0e411":"markdown","fd7fcc4e":"markdown","51f1c939":"markdown","102a445e":"markdown","29d49d05":"markdown","e125afab":"markdown","4ab173cf":"markdown","55d2455d":"markdown","4ca0dd34":"markdown","751243b7":"markdown","47d6c2b9":"markdown","d37d165a":"markdown","e2e901ff":"markdown"},"source":{"6f72482d":"# !pip install catboost\n# !pip install fake_useragent\n# !pip install gensim\n# !pip install mlxtend\n# !pip install nltk\n# !pip install --user keybert\n# !pip install rake-nltk","75d67967":"# Data pre-processing\nimport pandas as pd\nimport numpy as np\nimport re\nimport ast\nimport json\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom plotly.offline import (download_plotlyjs, init_notebook_mode,\n                            plot, iplot)\ninit_notebook_mode(connected=True)\n# import plotly.io as pio\n# pio.renderers.default = 'colab'\n\n# Data parsing\n# import scrapy\nimport requests as rs\n# import socks\n# import socket\nimport time\nimport concurrent.futures\nfrom bs4 import BeautifulSoup as BS\nfrom tqdm import tqdm\n# from fake_useragent import UserAgent\nfrom joblib import Parallel, delayed\nfrom itertools import chain\n# socks.set_default_proxy(socks.SOCKS5, \"localhost\", 9150)\n# socket.socket = socks.socksocket\n\n# Statistical tests, etc.\nfrom scipy.stats import (shapiro, probplot,\n                         mannwhitneyu, ttest_ind,\n                         pearsonr, f_oneway)\nfrom statsmodels.stats.multitest import multipletests\nfrom itertools import combinations\nfrom collections import Counter\n\n# Machine Learning\nfrom catboost import CatBoostRegressor\nimport xgboost as xgb\nimport lightgbm \nfrom sklearn.ensemble import (ExtraTreesRegressor, RandomForestRegressor,\n                              GradientBoostingRegressor, StackingRegressor)\nfrom sklearn.svm import LinearSVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif, f_regression\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.preprocessing import (StandardScaler, RobustScaler, OneHotEncoder,\n                                   LabelEncoder, PolynomialFeatures, FunctionTransformer)\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer, TransformedTargetRegressor\nfrom sklearn.pipeline import Pipeline\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\n\n# Working with text data\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n# from keybert import KeyBERT\n# from rake_nltk import Rake\nfrom gensim.models import Word2Vec\nfrom gensim.summarization import keywords\nfrom fuzzywuzzy import fuzz\nfrom fuzzywuzzy import process\n\n# Changing default displaying parameters\nfrom pylab import rcParams\nfrom tqdm import tqdm\nrcParams['figure.figsize'] = 12, 6\n# %config InlineBackend.figure_format = 'svg'\nsns.set(style='whitegrid')\npd.options.display.max_columns = 100\nnp.set_printoptions(suppress=True, precision=3)\nimport warnings\nwarnings.simplefilter('ignore')","d0ead2a9":"# # Checking current IP-address\n# def checkIP():\n#     ip = rs.get('http:\/\/checkip.dyndns.org').content\n#     soup = BS(ip, 'html.parser')\n#     print(soup.find('body').text)\n#     time.sleep(10)\n\n\n# for i in range(3):\n#     checkIP()\n\n# # # => Current IP Address: 51.68.212.73\n# # # => Current IP Address: 185.220.101.200\n# # # => Current IP Address: 185.220.101.12\n# # # Conclusion: Everything works properly","efa7f537":"test = pd.read_csv('..\/input\/sf-dst-car-price-prediction\/test.csv')\ntest['price'] = np.nan\ntest.head()","3acfa534":"test.info()","975b8c0e":"test.columns","1c852f39":"# page_url = 'https:\/\/auto.ru\/moskva\/cars\/{brand}\/{year}-year\/all'\n# headers = {'User-Agent': UserAgent().Chrome}\n# years = range(1960, 2021)\n\n# # Some functions to get the necessary urls\n# def get_brands():\n#     \"\"\"\n#     This function returns all\n#     car brands from auto.ru\n#     \"\"\"\n#     url = 'https:\/\/auto.ru\/catalog\/cars\/'\n#     marks_list = []\n#     response = rs.get(url)\n#     response.encoding = 'utf-8'\n#     soup = BS(response.text, 'html.parser')\n#     all_brand_tags = soup.find('div', class_='search-form-v2-list search-form-v2-list_invisible search-form-v2-list_type_all clearfix')\n#     brand_columns = all_brand_tags.find_all('div', class_='search-form-v2-list__text-item')\n#     brand_list = [brand.a['href'].split('\/')[-2] for brand in brand_columns]\n    \n#     return brand_list\n\n\n# def url_generator():\n#     \"\"\"\n#     This function returns all\n#     the urls for each year\n#     and car brand\n#     \"\"\"\n#     all_pages = []\n#     for brand in get_brands():\n#         for year in years:\n#             all_pages.append(f'https:\/\/auto.ru\/moskva\/cars\/{brand}\/{year}-year\/all\/')\n\n#     return all_pages\n\n\n# def get_total_pages(url):\n#     \"\"\"\n#     This function returns\n#     the last possible page \n#     number for each car offers\n#     \"\"\"\n#     response = rs.get(url, headers=headers)\n#     response.encoding = 'utf-8'\n#     pages = BS(response.text, 'html.parser').find_all('a',class_=\"Button Button_color_whiteHoverBlue Button_size_s Button_type_link Button_width_default ListingPagination-module__page\")\n#     try:\n#         last_page = int(pages[-1].text)\n#         return last_page\n#     except:\n#         return 1\n    \n    \n# def get_pages(url):\n#     \"\"\"\n#     Returns all the pages\n#     we need to scrape\n#     \"\"\"\n#     pages_list = []\n#     last_page = get_total_pages(url)\n#     for page_i in range(1, last_page + 1):\n#         pages_list.append(f'{url}?page={str(page_i)}&output_type=list')\n        \n#     return pages_list","148073bc":"# # Checking our links - works well\n# all_pages = url_generator()\n# print(all_pages[10000])\n# print(len(all_pages))","500a5ade":"# car_pages_lists_urls = Parallel(n_jobs=50)(delayed(get_pages)(page_link) for page_link in tqdm(all_pages))","1b0e6829":"# # # Saving links to a csv file\n# page_links = list(chain(*car_pages_lists_urls))\n# pd.Series(page_links).to_csv('links.csv', index=False)\n# page_links = list(pd.read_csv('links.csv').values.flatten())\n# page_links","de2bd325":"# import scrapy\n# import pandas as pd\n\n# urls = list(pd.read_csv('C:\/Users\/glavr\/GitHub\/skillfactory_rds\/module_5\/links.csv').values.reshape(18424))\n\n\n# class AutoruSpider(scrapy.Spider):\n#     name = 'autoru'\n#     start_urls = urls.copy()\n\n#     def parse(self, response):\n#         links = {}\n#         car_links = response.css(\"a.Link.ListingItemTitle-module__link::attr(href)\").getall()\n#         if car_links:\n#             for link in car_links:\n#                 links['car_links'] = link\n#                 yield links\n","192cde05":"# import scrapy\n# import pandas as pd\n# import numpy as np\n# import json\n# import time\n\n# urls = list(pd.read_csv('autoru_new.csv').values.flatten())\n\n\n# class CarSpider(scrapy.Spider):\n#     name = 'cars'\n#     start_urls = urls.copy()\n\n#     def parse(self, response):\n#         data = {}\n#         js = json.loads(response.css(\"script[id='initial-state']::text\").get())\n#         data['bodyType'] = response.css(\"meta[itemprop='bodyType']::attr(content)\").get()\n#         data['brand'] = response.css(\"meta[itemprop='brand']::attr(content)\").get()\n#         data['car_url'] = response.url\n#         data['color'] = response.css(\"meta[itemprop='color']::attr(content)\").get()\n#         try:\n#             data['complectation_dict'] = js['card']['vehicle_info']['complectation']['available_options']\n#         except KeyError:\n#             data['complectation_dict'] = np.nan\n#         data['description'] = response.css(\"meta[itemprop='description']::attr(content)\").get()\n#         data['engineDisplacement'] = response.css(\"meta[itemprop='engineDisplacement']::attr(content)\").get()\n#         data['enginePower'] = response.css(\"meta[itemprop='enginePower']::attr(content)\").get()\n#         try:\n#             data['equipment_dict'] = js['card']['vehicle_info']['equipment']\n#         except KeyError:\n#             data['equipment_dict'] = np.nan\n#         data['fuelType'] = response.css(\"meta[itemprop='fuelType']::attr(content)\").get()\n#         data['image'] = response.css(\"meta[itemprop='image']::attr(content)\").get()\n#         data['mileage'] = js['card']['state']['mileage']\n#         data['modelDate'] = response.css(\"meta[itemprop='modelDate']::attr(content)\").get()\n#         try:\n#             data['model_info'] = js['card']['vehicle_info']['model_info']\n#         except KeyError:\n#             data['model_info'] = np.nan\n#         data['model_name'] = response.css(\"meta[itemprop='name']::attr(content)\").get()\n#         data['name'] = response.css(\"span[itemprop='vehicleEngine'] meta::attr(content)\").get() \\\n#             .replace('\\xa0', '')\n#         data['numberOfDoors'] = response.css(\"meta[itemprop='numberOfDoors']::attr(content)\").get()\n#         data['parsing_unixtime'] = int(time.time())\n#         data['priceCurrency'] = response.css(\"meta[itemprop='priceCurrency']::attr(content)\").get()\n#         data['productionDate'] = response.css(\"meta[itemprop='productionDate']::attr(content)\").get()\n#         data['sell_id'] = response.css(\"div[title='\u0418\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u043e\u0431\u044a\u044f\u0432\u043b\u0435\u043d\u0438\u044f']::text\").get()\n#         try:\n#             data['super_gen'] = js['card']['vehicle_info']['tech_param']\n#         except KeyError:\n#             data['super_gen'] = np.nan\n#         data['vehicleConfiguration'] = response.css(\"meta[itemprop='vehicleConfiguration']::attr(content)\").get()\n#         data['vehicleTransmission'] = response.css(\"meta[itemprop='vehicleTransmission']::attr(content)\").get()\n#         try:\n#             data['vendor'] = js['card']['vehicle_info']['vendor']\n#         except KeyError:\n#             data['vendor'] = np.nan\n#         try:\n#             data['\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b'] = response.css(\"li[class='CardInfoRow CardInfoRow_ownersCount'] span::text\").getall()[-1]\n#         except IndexError:\n#             data['\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b'] = np.nan\n#         try:\n#             data['\u0412\u043b\u0430\u0434\u0435\u043d\u0438\u0435'] = response.css(\"li[class='CardInfoRow CardInfoRow_owningTime'] span::text\").getall()[-1]\n#         except IndexError:\n#             data['\u0412\u043b\u0430\u0434\u0435\u043d\u0438\u0435'] = np.nan\n#         try:\n#             data['\u041f\u0422\u0421'] = js['card']['documents']['pts']\n#         except KeyError:\n#             data['\u041f\u0422\u0421'] = np.nan\n#         try:\n#             data['\u041f\u0440\u0438\u0432\u043e\u0434'] = js['card']['vehicle_info']['tech_param']['gear_type']\n#         except KeyError:\n#             data['\u041f\u0440\u0438\u0432\u043e\u0434'] = np.nan\n#         try:\n#             data['\u0420\u0443\u043b\u044c'] = js['card']['vehicle_info']['steering_wheel']\n#         except KeyError:\n#             data['\u0420\u0443\u043b\u044c'] = np.nan\n#         try:\n#             data['\u0421\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435'] = js['card']['state']['state_not_beaten']\n#         except KeyError:\n#             data['\u0421\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435'] = np.nan\n#         try:\n#             data['\u0422\u0430\u043c\u043e\u0436\u043d\u044f'] = js['card']['documents']['custom_cleared']\n#         except KeyError:\n#             data['\u0422\u0430\u043c\u043e\u0436\u043d\u044f'] = np.nan\n#         data['price'] = response.css(\"meta[itemprop='price']::attr(content)\").get()\n\n#         yield data\n","365a5937":"train = pd.read_csv('..\/input\/auto-ru-scraped\/autoru.csv')\ntrain.head()","175ea719":"# Basic info on dataset\ntrain.info()","c83293c2":"# Descriptive statistics for train numeric features\ntrain.describe()","c8fb0b74":"# Descriptive statistics for test numeric features\ntest.describe()","4af7f565":"# Descriptive statistics for train categorical features\ntrain.describe(include=['O', 'bool'])","e68a6959":"# Descriptive statistics for test categorical features\ntest.describe(include=['O', 'bool'])","feeda08b":"# Dropping useless features\ntrain.drop(['priceCurrency', '\u0421\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435',\n            '\u0422\u0430\u043c\u043e\u0436\u043d\u044f', 'parsing_unixtime',\n            'sell_id', 'image'], axis=1, inplace=True)\n\ntest.drop(['priceCurrency', '\u0421\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435',\n           '\u0422\u0430\u043c\u043e\u0436\u043d\u044f', 'parsing_unixtime',\n           'sell_id', 'image'], axis=1, inplace=True)","5f64a848":"# Looking for duplicates - no duplicated rows\npd.concat([train, test]).duplicated().sum()","4b84a367":"# Bringing columns to a common format for convenience\ntrain.columns = ['body_type', 'brand', 'car_url', 'color', 'complectation', 'description', \n                 'engine_disp', 'engine_power', 'equipment', 'fuel_type', 'mileage',\n                 'model_date', 'model_info', 'model_name', 'name', 'doors', 'production_date',\n                 'super_gen', 'configuration', 'transmission', 'vendor', 'owners',\n                 'owning_time', 'passport', 'gear_type', 'steering_wheel', 'price']\n\ntest.columns = ['body_type', 'brand', 'car_url', 'color', 'complectation', 'description', \n                'engine_disp', 'engine_power', 'equipment', 'fuel_type', 'mileage',\n                'model_date', 'model_info', 'model_name', 'name', 'doors', 'production_date',\n                'super_gen', 'configuration', 'transmission', 'vendor', 'owners',\n                'owning_time', 'passport', 'gear_type', 'steering_wheel', 'price']","695f4426":"def show_info(data, column):\n    print(f'Number of unique values for {column}: {data[column].nunique()}\\n\\n')\n    print(f'Unique values for {column}:\\n{data[column].unique()}\\n\\n')\n    print(f'Value counts for {column}:\\n{data[column].value_counts()}\\n\\n')\n    print(f'Number of NaN-values: {data[column].isna().sum()}')\n    \n    \ndef bar(column):\n    \"\"\"\n    Plots pie chart and bar chart\n    for given categorical feature\n    \"\"\"\n    fig, axes = plt.subplots(figsize=(12, 6))\n    \n    sns.barplot(x=train[column].astype(str).value_counts().index[:5],\n                y=train[column].value_counts(normalize=True).values[:5],\n                palette='magma')\n    plt.title(f'Top train categories for \"{column}\"',\n              size=15)\n    plt.ylabel('Proportion')\n    \n    plt.xticks(rotation=90)\n    plt.subplots_adjust(wspace=0.2);\n    \n    \ndef dist_box(data, column):\n    \"\"\"\n    Plots distribution\n    and boxplot for given\n    numeric feature\n    \"\"\"\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    sns.kdeplot(data[column], ax=axes[0][0],\n                color='r', shade=True)\n    axes[0][0].set_title(f'Distribution for \"{column}\"',\n                      size=15)\n    sns.boxplot(data[column], ax=axes[0][1],\n                color='r', meanline=True)\n    axes[0][1].set_title(f'Box plot for \"{column}\"',\n                      size=15)\n    \n    sns.kdeplot(np.log(data[column] + 1), ax=axes[1][0],\n                color='violet', shade=True)\n    axes[1][0].set_title(f'Log-distribution for \"{column}\"',\n                      size=15)\n    \n    sns.boxplot(np.log(data[column] + 1), ax=axes[1][1],\n                color='violet', meanline=True)\n    axes[1][1].set_title(f'Box plot for logarithmic \"{column}\"',\n                      size=15)\n\n    plt.subplots_adjust(wspace=0.1, hspace=0.3);\n    \n    \ndef outliers(column):\n    \"\"\"\n    This function detects\n    outliers relying on\n    IQR and displays IQR bounds\n    and the number of outliers\n    \"\"\"\n    Q1 = np.quantile(column, 0.25)\n    Q3 = np.quantile(column, 0.75) \n    IQR = Q3 - Q1\n    left_bound = Q1 - 1.5*IQR\n    right_bound = Q3 + 1.5*IQR\n    bounds = pd.Series({'left bound': left_bound,\n                        'right bound': right_bound})\n    number_of_outliers = column[(column > right_bound) | (column < left_bound)]\n    return bounds, number_of_outliers.shape[0]\n    ","e371fe73":"show_info(train, 'body_type')","efa773cd":"# Checking 5 most frequent categories\nbar('body_type')","b4665e78":"# Looking at missing values\ntrain[train['body_type'].isna()]","a0d65fb5":"train.dropna(subset=['body_type'], axis=0, inplace=True)","ffbe5b21":"show_info(train, 'brand')","8d0dafec":"# Top-10 most frequent brands\nbar('brand')","9943b707":"# What if we omit very rare car brands\nbrand_counts = train['brand'].value_counts()\ntrain[train['brand'].isin(brand_counts[brand_counts <= 5].index)]","863a6b25":"show_info(train, 'car_url')","23bdaa70":"show_info(train, 'color')","80222944":"bar('color')","263b5cf2":"show_info(train, 'complectation')","4b394e8f":"# Bringing test values to the same format\ntest['complectation'] = test['complectation'].apply(lambda x: np.nan if pd.isna(x) \n                                                                     else json.loads(x)['available_options'])","63226d4a":"show_info(train, 'description')","55683b7a":"show_info(train, 'engine_disp')","86d7c652":"train[train['engine_disp'] == ' LTR']","711a010f":"train[(train['engine_disp'] == ' LTR') & (train['fuel_type'] == '\u044d\u043b\u0435\u043a\u0442\u0440\u043e')].shape == train[train['fuel_type'] == '\u044d\u043b\u0435\u043a\u0442\u0440\u043e'].shape","ac3d3526":"train[(train['engine_disp'] == ' LTR') & (train['fuel_type'] != '\u044d\u043b\u0435\u043a\u0442\u0440\u043e')]","85d7facc":"# Dropping irrelevant observation\ntrain.drop([88161], axis=0, inplace=True)\n\n\n# Replacing engine displacement for electric cars with 0\ndef replace_electro(row):\n    if row['fuel_type'] == '\u044d\u043b\u0435\u043a\u0442\u0440\u043e':\n        row['engine_disp'] = '0 LTR'\n    return row\n\n\n# Applying the function written above\ntrain = train.apply(replace_electro, axis=1)\n\n# Checking if any non-standard values left\nprint(train[train['engine_disp'] == ' LTR'].shape[0] == 0)\n\n# Converting engine displacement to float\ntrain['engine_disp'] = train['engine_disp'].str.replace(' LTR', '').astype('float')","adc20ec1":"dist_box(train, 'engine_disp')","7c169982":"# Checking outliers\ndisplay(outliers(train['engine_disp']))\ndisplay(outliers(np.log(train['engine_disp'])))","b1c49173":"show_info(train, 'engine_power')","0d3631e6":"# Converting engine power to integer format\ntrain['engine_power'] = train['engine_power'].str.replace(' N12', '').astype(np.int)","d92b536c":"dist_box(train, 'engine_power')","361746ff":"# Checking outliers\ndisplay(outliers(train['engine_power']))\ndisplay(outliers(np.log(train['engine_power'] + 1)))","5e7ee705":"show_info(train, 'equipment')","a4565bd5":"# Bringing test columns to the same format\ntest['equipment'] = test['equipment'].str.replace(':t', ':T')","0bf9ccca":"show_info(train, 'fuel_type')","711b9811":"bar('fuel_type')","47fa1712":"# Statistics for cars with alternative fuel type\ntrain.groupby(['fuel_type'])['price'].agg('mean')","f4bae1f4":"# Checking electro car brands presented in dataset\ntrain[train['fuel_type'] == '\u044d\u043b\u0435\u043a\u0442\u0440\u043e']['brand'].value_counts()","37a5323b":"# Dropping gas fuel observations\n# train = train[train['fuel_type'] != '\u0433\u0430\u0437']","127d1359":"show_info(train, 'mileage')","1aa6c6aa":"dist_box(train, 'mileage')","dc345dca":"# Checking outliers\ndisplay(outliers(train['mileage']))\ndisplay(outliers(np.log(train['mileage'] + 1)))","84c46c78":"sns.boxplot(x='vendor', y='mileage', data=train, palette='pastel')\nplt.title('Mileage depending on country of origin', size=15);","6a5c2aab":"show_info(train, 'model_date')","7d5b6468":"# Checking number of cars by model dates\nplt.figure(figsize=(25, 10))\nmodel_dates = train.groupby(['model_date'])['brand'].agg('count')\nmodel_dates.plot(color='green')\nplt.xticks(model_dates.index, rotation=90)\nplt.title('Number of cars by model year', size=18);","78142b69":"show_info(train, 'model_info')","91adeefe":"# Dropping \"model_info\" column\n# train.drop(['model_info'], axis=1, inplace=True)","1eb9c1e3":"show_info(train, 'model_name')","5053b0f0":"bar('model_name')","a95f5739":"# Dropping model name\n# train.drop(['model_name'], axis=1, inplace=True)","39a2c7c2":"train['model_name'] = train['model_info'].apply(lambda x: ast.literal_eval(x)['code'])","af44dd42":"show_info(train, 'name')","e17b259d":"# Dropping \"name\"\ntrain.drop(['name'], axis=1, inplace=True)\ntest.drop(['name'], axis=1, inplace=True)","6bec53c9":"show_info(train, 'doors')","6e5ec248":"# Changing \"doors\" data type\ntrain['doors'] = train['doors'].astype('O')\ntest['doors'] = test['doors'].astype('O')","e36616f5":"# Checking cars with zero doors\ntrain[train['doors'] == '0.0']","0c1315e0":"bar('doors')","2ef4b8fa":"show_info(train, 'production_date')","069cbed1":"# Correlation between prod and model dates\ntrain[['model_date', 'production_date']].corr()","710778a7":"plt.figure(figsize=(20, 10))\nprod_dates = train.groupby(['production_date'])['brand'].agg('count')\nprod_dates.plot()\nplt.title('Number of cars by production year', size=15)\nplt.xticks(prod_dates.index, rotation=90);","44ec5ce0":"show_info(train, 'super_gen')","3b52f353":"show_info(train, 'configuration')","dc76b612":"# Dropping \"vehicle_config\"\ntrain.drop(['configuration'], axis=1, inplace=True)\ntest.drop(['configuration'], axis=1, inplace=True)","0c96c335":"show_info(train, 'transmission')","51ff6655":"bar('transmission')","78f84b62":"show_info(train, 'vendor')","e0e45eaa":"# The brands where the \"vendor\" is missed\ntrain[train['vendor'].isna()]['brand'].unique()","9f97d375":"# The brands for each vendor\nfor vendor in train['vendor'].unique():\n    brands = train[train['vendor'] == vendor]['brand'].value_counts()\n    print(f'Unique brands for \"{vendor}\" vendor:\\n{brands}\\n\\n')","540db5b1":"# Observations for \"VENDOR_UNKNOWN\"\ntrain[train['vendor'] == 'VENDOR_UNKNOWN']['brand'].unique()","cfcbdb5f":"# Filling in missing vendor names\ndef fill_vendor(row):\n    \"\"\"\n    The function fills in missing\n    vendor values depending on car\n    brand\n    \"\"\"\n    if pd.isna(row['vendor']):\n        if row['brand'] in ['AUDI', 'BMW', 'MERCEDES',\n                            'OPEL', 'PEUGEOT', 'PORSCHE',\n                            'RENAULT', 'VOLKSWAGEN', 'VOLVO',\n                            'JAGUAR', 'LAND_ROVER', 'SAAB',\n                            'SKODA']:\n            row['vendor'] = 'EUROPEAN'\n        elif row['brand'] in ['CHERY', 'LIFAN']:\n            row['vendor'] = 'CHINESE'\n        elif row['brand'] in ['CHEVROLET', 'FORD',\n                              'CADILLAC', 'JEEP']:\n            row['vendor'] = 'AMERICAN'\n        elif row['brand'] in ['HYUNDAI', 'SSANG_YONG',\n                              'DAEWOO', 'KIA']:\n            row['vendor'] = 'KOREAN'\n        elif row['brand'] in ['VAZ', 'ZAZ','GAZ',\n                              'IG', 'UAZ']:\n            row['vendor'] = 'RUSSIAN'\n        elif row['brand'] in ['MAZDA', 'MITSUBISHI', 'NISSAN',\n                              'SUZUKI', 'TOYOTA', 'SUBARU',\n                              'HONDA', 'INFINITI',]:\n            row['vendor'] = 'JAPANESE'\n    \n    return row\n\n\n# Applying the function\ntrain = train.apply(fill_vendor, axis=1)\n\n# Dropping 'VENDOR_UNKNOWN'\n# train = train[train['vendor'] != 'VENDOR_UNKNOWN']","600fbb4c":"bar('vendor')","f382c3ae":"show_info(train, 'owners')","71e9ccdd":"# Filling in NaN-values\ntrain['owners'] = train['owners'].fillna(value='no info')\n# Cleansing owners values\ntrain['owners'] = train['owners'].str.replace('\\xa0', ' ')","0274102c":"bar('owners')","b5f14c77":"show_info(train, 'owning_time')","c6f1e9e7":"# Extracting numbers for \"owning time\"\ntrain['owning_time_float'] = train['owning_time'].apply(lambda x: x if pd.isna(x) else re.findall(r'\\d+', x))\ntest['owning_time_float'] = test['owning_time'].apply(lambda x: x if pd.isna(x) else re.findall(r'\\d+', x))\n\n# Converting number lists to total number of months\ndef get_months(x):\n    if type(x) != list:\n        return x\n    elif len(x) == 2:\n        return int(x[0])*12 + int(x[1])\n    else:\n        return int(x[0])\n      \n    \n# Applying the function\ntrain['owning_time_float'] = train['owning_time_float'].apply(get_months)\ntest['owning_time_float'] = test['owning_time_float'].apply(get_months)\n\n# Filling in missing values with zeros for now - at least zeros were better in ML than median\ntrain['owning_time_float'] = train['owning_time_float'].fillna(value=0)\ntest['owning_time_float'] = test['owning_time_float'].fillna(value=0)","24bf3612":"# Ckecking outliers\ndisplay(outliers(train['owning_time_float']))\ndisplay(outliers(np.log(train['owning_time_float'] + 1)))","39ad88fa":"show_info(train, 'passport')","e7112ed9":"# Checking observations with no vehicle passport value\ntrain[train['passport'].isna()].sample(10)","3a3541d9":"train[(train['passport'].isna()) & (train['mileage'] == 0)].shape[0]","28e4aac4":"train[(train['passport'].isna()) & (train['mileage'] != 0)]","a0e3acf6":"# # Dropping old vehicles without a passport\n# inds = df[(df['passport'].isna()) & (df['mileage'] != 0)].index.values\n# df.drop(inds, axis=0, inplace=True)","ed406837":"# Filling in vehicle passport with \"no passport\"\ntrain['passport'] = np.where(pd.isna(train['passport']), 'NO PASSPORT', train['passport'])","7fccc23f":"bar('passport')","2da9464a":"# Bringing test passport values to the same format\ntest['passport'] = test['passport'].replace({'\u041e\u0440\u0438\u0433\u0438\u043d\u0430\u043b': 'ORIGINAL', '\u0414\u0443\u0431\u043b\u0438\u043a\u0430\u0442': 'DUPLICATE'})","5571d90c":"show_info(train, 'gear_type')","95878a10":"bar('gear_type')","acb4e5c8":"# Transforming unique categories to the same format as in the training sample\ntest['gear_type'] = test['gear_type'].replace({'\u043f\u0435\u0440\u0435\u0434\u043d\u0438\u0439': 'FORWARD_CONTROL',\n                                               '\u043f\u043e\u043b\u043d\u044b\u0439': 'ALL_WHEEL_DRIVE',\n                                               '\u0437\u0430\u0434\u043d\u0438\u0439': 'REAR_DRIVE'})","2692f2e5":"show_info(train, 'steering_wheel')","c1989ab6":"# Transforming unique categories to the same format as in the training sample\ntest['steering_wheel'] = test['steering_wheel'].replace({'\u041b\u0435\u0432\u044b\u0439': 'LEFT',\n                                                         '\u041f\u0440\u0430\u0432\u044b\u0439': 'RIGHT'})","8d7d64f4":"show_info(train, 'price')","8b8956ce":"# Car offers with no price\ntrain[train['price'].isna()].car_url[10:20].unique()","a78dc6f9":"# Deleting cars with no price\ntrain.dropna(subset=['price'], axis=0, inplace=True)","c9a9785f":"# Visualizing price\ndist_box(train, 'price')","f14d0ffb":"# Checking outliers\ndisplay(outliers(train['price']))\ndisplay(outliers(np.log(train['price'] + 1)))","08b9b966":"fig, axes = plt.subplots(1, 2, figsize=(17, 6))\nsns.distplot(np.log(train['price']), color='blue', ax=axes[0])\naxes[0].set_title('Log-normal distribution of price', size=15)\nprobplot(np.log(train['price']), dist='norm', plot=axes[1])\naxes[1].set_title('QQ-Plot for price', size=15);","5f644432":"print(f\"Number of cars that cost less than 1M RUB: {train[train['price'] < 1000000].shape[0]}\")","7616040d":"# Saving pre-processed training data to a new csv file\n# train.to_csv('autoru_prep.csv', index=False)\n# test.to_csv('test_prep.csv', index=False)\n\n# Reading pre-processed data\n\ntrain = pd.read_csv('..\/input\/car-prediction-datasets\/autoru_prep.csv')\ntest = pd.read_csv('..\/input\/car-prediction-datasets\/test_prep.csv')\n\n# Labelling train and test samples \ntrain['train'] = 1\ntest['train'] = 0\n\n# Combining train and test samples\ndf = pd.concat([train, test], ignore_index=True)\ndf.info()","9fffb4fe":"# 1. Mean\/median groupby statistics for separate car groups (brands, models, body types)\nfor col in ['model_name', 'brand', 'body_type', 'engine_disp', 'engine_power']:\n    col_means = df.groupby([col])['price'].mean().to_dict()\n    df[col + '_' + 'price'] = df[col].apply(lambda x: col_means[x])\n\n# 2. Using logarithmic transformation\n# df['engine_disp'] = np.log(df['engine_disp'] + 1)\n# df['engine_power'] = np.log(df['engine_power'] + 1)\nfor col in ['price', 'model_name_price', 'brand_price', 'body_type_price', 'engine_disp_price', 'engine_power_price']:\n    df[col] = np.log(df[col])","1235a643":"# 3. Complectation length - reduced the score by ~1%\ndf['comp_length'] = df['complectation'].apply(lambda x: 0 if type(x) != str else len(x.split(',')))\n","da6a8086":"# 4. How many \"True\" values are presented in equipment dictionary - reduced MAPE\ndf['equipment'] = df['equipment'].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\ndf['equipment_length'] = df['equipment'].apply(lambda x: len(x))\n\n# 5. Equipment dummies (what attribute a certain car has)\n# equip_counter = Counter()\n# # Getting elements and their counts\n# for equipment in df['equipment_dict']:\n#     for element in equipment.keys():\n#         equip_counter[element] += 1\n\n# # Filtering to get the \"most important\" equipment attributes\n# equipment_attributes = list(\n#                             dict(\n#                                 filter(\n#                                        lambda x: x[1] < 100000,\n#                                        equip_counter.most_common()\n#                                       )\n#                                 ).keys()\n#                             )\n\n# # Creating dummies\n# for attribute in equipment_attributes:\n#     df[attribute] = df['equipment'].str.contains(attribute).astype(np.int)    \n# print(f'The number of new dummy features: {len(equipment_attributes)}')\n\n# Faster implementation of the code above\ndv = DictVectorizer()\nequipment_features = dv.fit_transform(df['equipment']).toarray()\nequipment_df = pd.DataFrame(equipment_features, columns=dv.get_feature_names())\nequipment_attributes = equipment_df.columns.values","5d6642e1":"# 6. super_gen feature extraction: acceleration, fuel_rate\n# Converting \"super_gen\" dict-like strings to dicts\ndf['super_gen'] = df['super_gen'].apply(lambda x: ast.literal_eval(x))\n\n\ndef get_acceleration(x):\n    \"\"\"\n    Extracts the acceleration\n    of a certain car\n    \"\"\"\n    try:\n        return x['acceleration']\n    except:\n        return np.nan\n    \n    \ndef get_fuel_rate(x):\n    \"\"\"\n    Extracts fuel rate\n    of a certain car\n    \"\"\"\n    try:\n        return x['fuel_rate']\n    except:\n        return np.nan\n    \n\n# Applying the functions\ndf['acceleration'] = df['super_gen'].apply(get_acceleration)\ndf['fuel_rate'] = df['super_gen'].apply(get_fuel_rate)\n\n# Getting mean values from train set \nmean_acceleration = df[df['train'] == 1]['acceleration'].mean()\nmean_fuel_rate = df[df['train'] == 1]['fuel_rate'].mean()\n\n# Imputing missing values in acceleration and fuel_rate with medians\ndf['acceleration'].fillna(value=mean_acceleration, inplace=True)\ndf['fuel_rate'].fillna(value=mean_fuel_rate, inplace=True)\n","906f7644":"# 7. Difference between production date and model date - not really significant\ndf['prod_diff'] = df['production_date'] - df['model_date']\n","772c2345":"# 8. Giving a mark to new cars\ndf['new_car'] = df['car_url'].str.contains('new').astype(np.int)\n","a3bca307":"# # 9. Owning time intervals - bad idead\n# def owning_time_intervals(x):\n#   \"\"\"\n#   This function returns\n#   categories according to\n#   the owning time\n#   \"\"\"\n#   if x == 0:\n#     return 'no info'\n#   elif 0 < x <= 12:\n#     return 'new'\n#   elif 12 < x <= 72:\n#     return 'owned'\n\n#   return 'oldtimer'\n\n\n# df['owning_time_cat'] = df['owning_time'].apply(owning_time_intervals)\n","a9fcdc95":"# # 10. Body type category reduction\n# df['body_type'].replace(regex={r'\u0432\u043d\u0435\u0434\u043e\u0440\u043e\u0436\u043d\u0438\u043a.*': '\u0432\u043d\u0435\u0434\u043e\u0440\u043e\u0436\u043d\u0438\u043a',\n#                                r'\u0441\u0435\u0434\u0430\u043d.*': '\u0441\u0435\u0434\u0430\u043d',\n#                                r'\u0445\u044d\u0442\u0447\u0431\u0435\u043a.*': '\u0445\u044d\u0442\u0447\u0431\u0435\u043a', \n#                                r'\u043a\u0443\u043f\u0435.*': '\u043a\u0443\u043f\u0435',\n#                                r'\u043f\u0438\u043a\u0430\u043f.*': '\u043f\u0438\u043a\u0430\u043f'},\n#                         inplace=True)\n","f48ae1ee":"# 11. Description text processing\n\n# Copying description feature\ntext_feature = df['description'].copy()\n# Removing numbers from the text\n# text_feature = text_feature.str.replace(r'\\d+', '')\n# Defining useless stop words\nstop_words = stopwords.words('russian')\n\n# Tokenization\ntokenizer = CountVectorizer(stop_words=stop_words).build_analyzer()\ntokenized_text_feature = text_feature.apply(tokenizer)\n\n# Description legnth as a new feature\ndf['description_length'] = tokenized_text_feature.apply(len)\n\n# # Bag of words approach with word counts\n# count_vec = CountVectorizer(stop_words=stop_words, max_features=50)\n# count_text_feature = count_vec.fit_transform(text_feature).toarray()\n\n# TF-IDF - reduced MAPE by ~0.7%\ntf_idf = TfidfVectorizer(max_features=50, stop_words=stop_words)\ntf_idf_feature = tf_idf.fit_transform(text_feature).toarray()\n# Reducing dimensions of our tf_idf matrix\npca_tfidf = PCA(n_components=2, random_state=0)\ntf_idf_pc = pca_tfidf.fit_transform(tf_idf_feature)\n\n\n# # Extracting keywords from each description with keybert - works extremely slow\n# keybert_model = KeyBERT('distilbert-base-nli-mean-tokens')\n# all_keywords = []\n# for text in tqdm(text_feature):\n#     keywords = keybert_model.extract_keywords(text,\n#                                               top_n=5,\n#                                               keyphrase_ngram_range=(1, 2),\n#                                               stop_words=stop_words)\n#     all_keywords.append(keywords)\n#     break\n\n# # Extracting key phrases with rake-nltk - works fast but doesn't make any sense for ML since the phrases are totally different\n# rake = Rake()\n# all_keywords = []\n# for text in tqdm(text_feature):\n#     rake.extract_keywords_from_text(text)\n#     all_keywords.append(rake.get_ranked_phrases()[:5])\n\n# # Extracting key phrases with gensim - faster than keybert and suits our problem well enough\n# all_keywords = []\n# for text in tqdm(text_feature):\n#     all_keywords.append(keywords(text))\n    ","0d9cb197":"# # 12. Interactions between numeric features - reduced the score a little bit\n# poly = PolynomialFeatures(degree=2, include_bias=False)\n# X_poly = poly.fit_transform(df[num_cols])\n# X_poly.shape\n","20ed4f23":"# # Word vectorization with naive Word2Vec model\n# w2v = Word2Vec(tokenized_text_feature, min_count=10, size=50, workers=4)\n\n# # Naive approach\n# def doc_vectorizer(doc, model):\n#     doc_vector = []\n#     num_words = 0\n#     for word in doc:\n#         try:\n#             if num_words == 0:\n#                 doc_vector = w2v[word]\n#             else:\n#                 doc_vector = np.add(doc_vector, w2v[word])\n#             num_words += 1\n#         except:\n#             pass\n    \n#     return np.asarray(doc_vector) \/ num_words\n\n\n# # Getting text embeddings\n# embeddings = []\n# for doc in tqdm(tokenized_text_feature):\n#     embeddings.append(doc_vectorizer(doc, w2v))\n    \n# # Imputing missing values with the means of each column\n# w2v_df = pd.DataFrame(embeddings, columns=['w_' + str(n) for n in range(30)])\n# means = w2v_df.mean(axis=0).to_dict()\n# w2v_df.fillna(value=means, inplace=True)\n\n# # Reducing dimensions of our w2v matrix\n# # t_sne = TSNE(n_components=2, random_state=0)\n# # X_w2v = t_sne.fit_transform(w2v_df)\n","02c4478b":"# Defining feature groups\ntarget = 'price'\n\nnum_cols = ['engine_disp', 'engine_power',\n            'mileage', 'production_date',\n            'model_date', 'owning_time_float',\n            'comp_length', 'equipment_length',\n            'model_name_price', 'brand_price',\n            'body_type_price', 'acceleration',\n            'fuel_rate', 'prod_diff',\n            'description_length', 'new_car',\n            'engine_disp_price', 'engine_power_price']\n\ncat_cols = ['body_type', 'fuel_type',\n            'doors', 'transmission',        \n            'vendor', 'owners',\n            'gear_type', 'steering_wheel',\n            'brand', 'color',\n            'passport']\n\nequipment = ['led-lights', 'tyre-pressure', 'electro-trunk', 'body-kit',\n             'high-beam-assist', 'black-roof', 'auto-park', 'start-button',\n             'seat-memory', 'passenger-seat-electric', 'airbag-curtain',\n             'keyless-entry', 'collision-prevention-assist', 'music-super',\n             'traffic-sign-recognition', 'apple-carplay', 'rain-sensor',\n             'navigation', 'wheel-heat', '360-camera', 'light-sensor', 'hcc',\n             'voice-recognition', 'leather', 'servo', 'auto-mirrors',\n             'paint-metallic', 'decorative-interior-lighting', 'blind-spot',\n             'multi-wheel', 'adaptive-light', 'wheel-leather',\n             'power-latching-doors', 'door-sill-panel', 'activ-suspension',\n             'front-seats-heat-vent', 'bluetooth', 'airbag-side',\n             'front-centre-armrest', 'bas', 'esp', 'isofix', 'volume-sensor',\n             'automatic-lighting-control', '20-inch-wheels',\n             'steering-wheel-gear-shift-paddles', 'air-suspension', 'panorama-roof',\n             'usb', 'start-stop-function']\n","fc9a5d07":"# Checking the correlation between all variables\nplt.figure(figsize=(18, 10))\nsns.heatmap(df[df['train'] == 1][num_cols + [target]].corr(), annot=True,\n            fmt='.2f', cmap='YlGnBu', linewidth=0.5)\nplt.title('Correlation matrix', size=15);\n","5d8a1a6c":"for num_col in num_cols:\n    p_value = pearsonr(df[df['train'] == 1][num_col], \n                       df[df['train'] == 1][target])[1]\n    # Using Bonferonni correction as well\n    if (p_value \/ len(num_cols)) < 0.05:\n        print(f'P-value={p_value} => \"{num_col}\" and \"{target}\" are correlated')\n","504854bf":"# Checking scatter plots \n# sns.pairplot(df[num_cols + [target]])\n# plt.show()","d933534f":"# dummy_cats = pd.get_dummies(df[cat_cols + [target]],\n#                             columns=cat_cols,\n#                             drop_first=True)\n# dummy_cats.shape","6be7a465":"# ANOVA in a loop for each category group\ntrain_df = df[df['train'] == 1]\nfor column in cat_cols:\n    if train_df[column].nunique() > 10:\n        ten_random_categories = np.random.choice(train_df[column].unique(), 10, replace=False)\n        combs = list(combinations(ten_random_categories, 2))\n    else:\n        combs = list(combinations(train_df[column].unique(), 2))\n    for pair in combs:\n        p_value = f_oneway(train_df[train_df[column] == pair[0]][target],\n                           train_df[train_df[column] == pair[1]][target]).pvalue\n        if p_value < 0.01:\n            print(f'\"{column}\" feature is statistically significant')\n            # Stopping the loop because at least one pair of group means differs\n            break\n        ","3352f2bf":"# ANOVA with sklearn \"f_classif\" for binary equipment elements\nf_scores, p_values = f_regression(equipment_df.iloc[train.index], df.iloc[train.index][target])\n    \n# Visualizing 50 best equipment features\nplt.figure(figsize=(20, 10))\nf_scores = pd.Series(f_scores, equipment_attributes).sort_values(ascending=False)\nbest_attributes = f_scores[:50].index\nf_scores[:50].plot(kind='barh', color='orange')\nplt.xlabel('F-Score')\nplt.title('50 best equipment features', size=16);","5f600fe5":"# Selecting 50 best equipment attributes one more time\nselector = SelectKBest(score_func=f_regression, k=50)\nbest_equips = selector.fit_transform(equipment_df.iloc[train.index], df.iloc[train.index][target])\ninds_best = selector.get_support(indices=True)\nbest_equipment = equipment_df.iloc[:, inds_best]\nbest_equipment.columns","72b6b3b4":"class TrainPreprocessing():\n    def __init__(self, df):\n        self.df = df\n        \n    def preprocess(self):\n        train = self.df.copy(deep=True)\n        # Dropping useless features\n        train.drop(['priceCurrency', '\u0421\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435',\n                    '\u0422\u0430\u043c\u043e\u0436\u043d\u044f', 'parsing_unixtime',\n                    'sell_id', 'image'], axis=1, inplace=True)\n        \n        # Bringing columns to a common format for convenience\n        train.columns = ['body_type', 'brand', 'car_url', 'color', 'complectation', 'description', \n                         'engine_disp', 'engine_power', 'equipment', 'fuel_type', 'mileage',\n                         'model_date', 'model_info', 'model_name', 'name', 'doors', 'production_date',\n                         'super_gen', 'configuration', 'transmission', 'vendor', 'owners',\n                         'owning_time', 'passport', 'gear_type', 'steering_wheel', 'price']\n        \n        # Dropping irrelevant observations\n        train.dropna(subset=['body_type'], axis=0, inplace=True)\n        train.drop([88161], axis=0, inplace=True)\n\n\n        # Replacing engine displacement for electric cars with 0\n        def replace_electro(row):\n            if row['fuel_type'] == '\u044d\u043b\u0435\u043a\u0442\u0440\u043e':\n                row['engine_disp'] = '0 LTR'\n            return row\n\n\n        # Applying the function written above\n        train = train.apply(replace_electro, axis=1)\n\n        # Converting engine displacement to float\n        train['engine_disp'] = train['engine_disp'].str.replace(' LTR', '').astype('float')\n        \n        # Converting engine power to integer format\n        train['engine_power'] = train['engine_power'].str.replace(' N12', '').astype(np.int)\n        \n        # Converting train model names to the same format as in the test\n        train['model_name'] = train['model_info'].apply(lambda x: ast.literal_eval(x)['code'])\n        \n        # Changing \"doors\" data type\n        train['doors'] = train['doors'].astype('O')\n        \n        # Dropping \"name\"\n        train.drop(['name'], axis=1, inplace=True)\n        # Dropping \"vehicle_config\"\n        train.drop(['configuration'], axis=1, inplace=True)\n        \n        # Filling in missing vendor names\n        def fill_vendor(row):\n            \"\"\"\n            The function fills in missing\n            vendor values depending on car\n            brand\n            \"\"\"\n            if pd.isna(row['vendor']):\n                if row['brand'] in ['AUDI', 'BMW', 'MERCEDES',\n                                    'OPEL', 'PEUGEOT', 'PORSCHE',\n                                    'RENAULT', 'VOLKSWAGEN', 'VOLVO',\n                                    'JAGUAR', 'LAND_ROVER', 'SAAB',\n                                    'SKODA']:\n                    row['vendor'] = 'EUROPEAN'\n                elif row['brand'] in ['CHERY', 'LIFAN']:\n                    row['vendor'] = 'CHINESE'\n                elif row['brand'] in ['CHEVROLET', 'FORD',\n                                      'CADILLAC', 'JEEP']:\n                    row['vendor'] = 'AMERICAN'\n                elif row['brand'] in ['HYUNDAI', 'SSANG_YONG',\n                                      'DAEWOO', 'KIA']:\n                    row['vendor'] = 'KOREAN'\n                elif row['brand'] in ['VAZ', 'ZAZ','GAZ',\n                                      'IG', 'UAZ']:\n                    row['vendor'] = 'RUSSIAN'\n                elif row['brand'] in ['MAZDA', 'MITSUBISHI', 'NISSAN',\n                                      'SUZUKI', 'TOYOTA', 'SUBARU',\n                                      'HONDA', 'INFINITI',]:\n                    row['vendor'] = 'JAPANESE'\n\n            return row\n\n\n        # Applying the function\n        train = train.apply(fill_vendor, axis=1)\n        \n        # Filling in NaN-values\n        train['owners'] = train['owners'].fillna(value='no info')\n        # Cleansing owners values\n        train['owners'] = train['owners'].str.replace('\\xa0', ' ')\n        \n        # Extracting numbers for \"owning time\"\n        train['owning_time_float'] = train['owning_time'].apply(lambda x: x if pd.isna(x) else re.findall(r'\\d+', x))\n        \n\n        # Converting number lists to total number of months\n        def get_months(x):\n            if type(x) != list:\n                return x\n            elif len(x) == 2:\n                return int(x[0])*12 + int(x[1])\n            else:\n                return int(x[0])\n\n\n        # Applying the function\n        train['owning_time_float'] = train['owning_time_float'].apply(get_months)\n        # Filling in missing values with zeros for now - at least zeros were better in ML than median\n        train['owning_time_float'] = train['owning_time_float'].fillna(value=0)\n        \n        # Filling in vehicle passport with \"no passport\"\n        train['passport'] = np.where(pd.isna(train['passport']), 'NO PASSPORT', train['passport'])\n        \n        # Deleting cars with no price\n        train.dropna(subset=['price'], axis=0, inplace=True)\n        \n        return train\n    \n     \nclass TestPreprocessing():\n    def __init__(self, df):\n        self.df = df\n        \n    def preprocess(self):\n        test = self.df.copy(deep=True)\n        # Dropping useless features\n        test.drop(['priceCurrency', '\u0421\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435',\n                    '\u0422\u0430\u043c\u043e\u0436\u043d\u044f', 'parsing_unixtime',\n                    'sell_id', 'image'], axis=1, inplace=True)\n        \n        # Bringing columns to a common format for convenience\n        test['price'] = np.nan\n        test.columns = ['body_type', 'brand', 'car_url', 'color', 'complectation', 'description', \n                         'engine_disp', 'engine_power', 'equipment', 'fuel_type', 'mileage',\n                         'model_date', 'model_info', 'model_name', 'name', 'doors', 'production_date',\n                         'super_gen', 'configuration', 'transmission', 'vendor', 'owners',\n                         'owning_time', 'passport', 'gear_type', 'steering_wheel', 'price']\n\n        # Bringing test values to the same format\n        test['complectation'] = test['complectation'].apply(lambda x: np.nan if pd.isna(x) \n                                                                             else json.loads(x)['available_options'])\n        # Converting to strings\n        test['complectation'] = test['complectation'].astype('str')\n        \n        \n        # Replacing engine displacement for electric cars with 0\n        def replace_electro(row):\n            if row['fuel_type'] == '\u044d\u043b\u0435\u043a\u0442\u0440\u043e':\n                row['engine_disp'] = '0 LTR'\n            return row\n\n\n        # Applying the function written above\n        test = test.apply(replace_electro, axis=1)\n\n        # Converting engine displacement to float\n        test['engine_disp'] = test['engine_disp'].str.replace(' LTR', '').astype('float')\n        \n        # Converting engine power to integer format\n        test['engine_power'] = test['engine_power'].str.replace(' N12', '').astype(np.int)\n        \n        # Bringing test columns to the same format\n        test['equipment'] = test['equipment'].str.replace(':t', ':T')\n        \n        # Changing \"doors\" data type\n        test['doors'] = test['doors'].astype('O')\n        \n        # Dropping \"name\"\n        test.drop(['name'], axis=1, inplace=True)\n        # Dropping \"vehicle_config\"\n        test.drop(['configuration'], axis=1, inplace=True)\n        \n        # Filling in NaN-values\n        test['owners'] = test['owners'].fillna(value='no info')\n        # Cleansing owners values\n        test['owners'] = test['owners'].str.replace('\\xa0', ' ')\n        \n        # Extracting numbers for \"owning time\"\n        test['owning_time_float'] = test['owning_time'].apply(lambda x: x if pd.isna(x) else re.findall(r'\\d+', x))\n        \n\n        # Converting number lists to total number of months\n        def get_months(x):\n            if type(x) != list:\n                return x\n            elif len(x) == 2:\n                return int(x[0])*12 + int(x[1])\n            else:\n                return int(x[0])\n\n\n        # Applying the function\n        test['owning_time_float'] = test['owning_time_float'].apply(get_months)\n        # Filling in missing values with zeros for now - at least zeros were better in ML than median\n        test['owning_time_float'] = test['owning_time_float'].fillna(value=0)\n        \n        # Filling in vehicle passport with \"no passport\"\n        test['passport'] = np.where(pd.isna(test['passport']), 'NO PASSPORT', test['passport'])\n        \n        # Bringing test column values to the same format\n        test['passport'] = test['passport'].replace({'\u041e\u0440\u0438\u0433\u0438\u043d\u0430\u043b': 'ORIGINAL', '\u0414\u0443\u0431\u043b\u0438\u043a\u0430\u0442': 'DUPLICATE'})\n        \n        test['gear_type'] = test['gear_type'].replace({'\u043f\u0435\u0440\u0435\u0434\u043d\u0438\u0439': 'FORWARD_CONTROL',\n                                                       '\u043f\u043e\u043b\u043d\u044b\u0439': 'ALL_WHEEL_DRIVE',\n                                                       '\u0437\u0430\u0434\u043d\u0438\u0439': 'REAR_DRIVE'})\n        \n        test['steering_wheel'] = test['steering_wheel'].replace({'\u041b\u0435\u0432\u044b\u0439': 'LEFT',\n                                                                 '\u041f\u0440\u0430\u0432\u044b\u0439': 'RIGHT'})\n        \n        return test","d66620fc":"# Reading the original data\ntrain = pd.read_csv('..\/input\/auto-ru-scraped\/autoru.csv')\ntest = pd.read_csv('..\/input\/sf-dst-car-price-prediction\/test.csv')\n\n# Pre-processing the data\ntrain_pp = TrainPreprocessing(train)\ntrain_prep = train_pp.preprocess()\n\ntest_pp = TestPreprocessing(test)\ntest_prep = test_pp.preprocess()\n\n# train_prep.to_csv('autoru_prep.csv', index=False)\n# test_prep.to_csv('test_prep.csv', index=False)\n","e902c338":"# # Reading pre-processed datasets\n# train = pd.read_csv('..\/input\/car-prediction-datasets\/autoru_prep.csv')\n# test = pd.read_csv('..\/input\/car-prediction-datasets\/test_prep.csv')\n\n# # Sample submission\n# submission = pd.read_csv('..\/input\/sf-dst-car-price-prediction\/sample_submission.csv')","d6381f43":"# Building a class for proper feature extraction\nclass FeatureEngineering():\n    def __init__(self, train, test):\n        self.train = train\n        self.test = test\n        \n    def extract_features(self):\n        train = self.train.copy(deep=True)\n        test = self.test.copy(deep=True)\n        train['train'] = 1\n        test['train'] = 0\n        df = pd.concat([train, test], ignore_index=True)\n        \n        # Defining feature groups\n        target = 'price'\n\n        num_cols = ['engine_disp', 'engine_power',\n                    'mileage', 'production_date',\n                    'model_date', 'owning_time_float',\n                    'comp_length', 'equipment_length',\n                    'model_name_price', 'brand_price',\n                    'body_type_price', 'acceleration',\n                    'engine_power_price', 'engine_disp_price',\n                    'fuel_rate', 'prod_diff',\n                    'description_length', 'new_car',\n                    'tfidf_pc1', 'tfidf_pc2']\n\n        cat_cols = ['body_type', 'fuel_type',\n                    'doors', 'transmission',        \n                    'vendor', 'owners',\n                    'gear_type', 'steering_wheel',\n                    'brand', 'color',\n                    'passport']\n        \n        equipment = ['led-lights', 'tyre-pressure', 'electro-trunk', 'body-kit',\n                     'high-beam-assist', 'black-roof', 'auto-park', 'start-button',\n                     'seat-memory', 'passenger-seat-electric', 'airbag-curtain',\n                     'keyless-entry', 'collision-prevention-assist', 'music-super',\n                     'traffic-sign-recognition', 'apple-carplay', 'rain-sensor',\n                     'navigation', 'wheel-heat', '360-camera', 'light-sensor', 'hcc',\n                     'voice-recognition', 'leather', 'servo', 'auto-mirrors',\n                     'paint-metallic', 'decorative-interior-lighting', 'blind-spot',\n                     'multi-wheel', 'adaptive-light', 'wheel-leather',\n                     'power-latching-doors', 'door-sill-panel', 'activ-suspension',\n                     'front-seats-heat-vent', 'bluetooth', 'airbag-side',\n                     'front-centre-armrest', 'bas', 'esp', 'isofix', 'volume-sensor',\n                     'automatic-lighting-control', '20-inch-wheels',\n                     'steering-wheel-gear-shift-paddles', 'air-suspension', 'panorama-roof',\n                     'usb', 'start-stop-function']\n\n        \n        # 1. Mean\/median groupby statistics for separate car groups (brands, models, body types)\n        for col in ['model_name', 'brand', 'body_type', 'engine_disp', 'engine_power']:\n            col_means = df.groupby([col])['price'].mean().to_dict()\n            df[col + '_' + 'price'] = df[col].apply(lambda x: col_means[x])\n                \n        # 2. Using logarithmic transformation\n        # df['engine_disp'] = np.log(df['engine_disp'] + 1)\n        # df['engine_power'] = np.log(df['engine_power'] + 1)\n        for col in ['price', 'model_name_price', 'brand_price', 'body_type_price', 'engine_disp_price', 'engine_power_price']:\n            df[col] = np.log(df[col])\n\n        # Deleting outliers\n        # df = df[(df['price'] > 10.310451) & (df['price'] < 16.764727)]\n\n        # 3. Complectation length - reduced the score by ~1%\n        df['comp_length'] = df['complectation'].apply(lambda x: 0 if type(x) != str else len(x.split(',')))\n\n        # 4. How many \"True\" values are presented in equipment dictionary\n        df['equipment'] = df['equipment'].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\n        df['equipment_length'] = df['equipment'].apply(len)\n        dv = DictVectorizer()\n        equipment_features = dv.fit_transform(df['equipment']).toarray()\n        equipment_df = pd.DataFrame(equipment_features, columns=dv.get_feature_names())[equipment]\n        df = pd.concat([df, equipment_df], axis=1)\n        \n        \n        # 6. super_gen feature extraction: acceleration, fuel_rate\n        # Converting \"super_gen\" dict-like strings to dicts\n        df['super_gen'] = df['super_gen'].apply(lambda x: ast.literal_eval(x))\n\n\n        def get_acceleration(x):\n            \"\"\"\n            Extracts the acceleration\n            of a certain car\n            \"\"\"\n            try:\n                return x['acceleration']\n            except KeyError:\n                return np.nan\n\n\n        def get_fuel_rate(x):\n            \"\"\"\n            Extracts fuel rate\n            of a certain car\n            \"\"\"\n            try:\n                return x['fuel_rate']\n            except KeyError:\n                return np.nan\n\n\n        # Applying the functions\n        df['acceleration'] = df['super_gen'].apply(get_acceleration)\n        df['fuel_rate'] = df['super_gen'].apply(get_fuel_rate)\n        # Getting median values from train set \n        mean_acceleration = df[df['train'] == 1]['acceleration'].median()\n        mean_fuel_rate = df[df['train'] == 1]['fuel_rate'].median()\n        # Imputing missing values in acceleration and fuel_rate with medians\n        df['acceleration'].fillna(value=mean_acceleration, inplace=True)\n        df['fuel_rate'].fillna(value=mean_fuel_rate, inplace=True)\n\n        # 7. Difference between production date and model date - not really significant\n        df['prod_diff'] = df['production_date'] - df['model_date']\n\n        # 8. Giving a mark to new cars\n        df['new_car'] = df['car_url'].str.contains('new').astype(np.int)\n\n        # 9. Description text processing\n        # Copying description feature\n        text_feature = df['description'].copy(deep=True)\n        # Removing numbers from the text\n        # text_feature = text_feature.str.replace(r'\\d+', '')\n        # Defining useless stop words\n        stop_words = stopwords.words('russian')\n\n        # Tokenization\n        tokenizer = CountVectorizer(stop_words=stop_words).build_analyzer()\n        tokenized_text_feature = text_feature.apply(tokenizer)\n\n        # Description legnth as a new feature\n        df['description_length'] = tokenized_text_feature.apply(len)\n\n        # Bag of words approach with word counts\n#         count_vec = CountVectorizer(stop_words=stop_words, max_features=50)\n#         word_counts = count_vec.fit_transform(text_feature).toarray()\n\n        # TF-IDF\n        tf_idf = TfidfVectorizer(max_features=50, stop_words=stop_words)\n        tf_idf_feature = tf_idf.fit_transform(text_feature).toarray()\n        # Reducing dimensions of our tf_idf matrix\n        pca = PCA(n_components=2, random_state=0)\n        tf_idf_pc = pca.fit_transform(tf_idf_feature)\n        df_tfidf = pd.DataFrame(tf_idf_pc, columns=['tfidf_pc1', 'tfidf_pc2'])\n        df = pd.concat([df, df_tfidf], axis=1)\n        \n        # Dropping useless columns\n        df.drop(['car_url', 'complectation', 'description',\n                 'equipment', 'model_info', 'model_name',\n                 'super_gen', 'owning_time'],\n                axis=1, inplace=True)\n        \n        # Imputing missing values in test sample with the statistics from train sample\n        df.fillna(value={'model_name_price': df[df['train'] == 1]['model_name_price'].median(),\n                         'engine_power_price': df[df['train'] == 1]['engine_power_price'].median()},\n                  inplace=True)\n        \n        # Encoding categorical data\n        le = LabelEncoder()\n        for cat_col in cat_cols:\n            df[cat_col] = le.fit_transform(df[cat_col])\n        \n        return df","2484e0a9":"# Getting full ML dataset\nfe = FeatureEngineering(train_prep, test_prep)\ndf = fe.extract_features()\ndf.info()","6b9cb018":"# Defining MAPE function since it was removed from sklearn\ndef mape(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true) * 100)","50226fe8":"# Defining features and target\nX = df[df['train'] == 1].drop([target], axis=1)\ny = df[df['train'] == 1][target]\n\n# Splitting the data\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,\n                                                  shuffle=True, random_state=0)\nX_test = df[df['train'] == 0].drop([target], axis=1)\ny_test = df[df['train'] == 0][target]\nprint(f'Train size: {X_train.shape}, Validation size: {X_val.shape}, Test size: {X_test.shape}')\n\n# Training and evaluating Random Forest\nrf = ExtraTreesRegressor(n_estimators=300, random_state=0, n_jobs=-1,\n                         bootstrap=True, verbose=1)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_val)\n\nMAPE = mape(np.exp(y_val), np.exp(y_pred))\nprint(f'Mean Absolute Percentage Error: {MAPE}')","11c1b0d5":"# # First submission\n# submission['price'] = np.exp(rf.predict(X_test))\n# submission.to_csv('submission.csv', index=False)","745059c8":"# Defining the data\nX = df[df['train'] == 1].drop([target], axis=1)\ny = df[df['train'] == 1][target]\n\n# Training the model\ncbr = CatBoostRegressor()\n\n# Grid search for catboost model\nparam_grid = {'iterations': [500],\n              'learning_rate': [0.01, 0.1, 0.25, 0.5],\n              'l2_leaf_reg': [1, 3, 5, 10],\n              'depth': np.arange(10, 14),\n              'thread_count': [4]}\n\ncbr.grid_search(param_grid=param_grid,\n                X=X,\n                y=y,\n                cv=5,\n                partition_random_seed=0,\n                calc_cv_statistics=True,\n                search_by_train_test_split=True,\n                refit=True,\n                shuffle=True,\n                stratified=None,\n                train_size=0.8,\n                verbose=True,\n                plot=True)\n\n# grid_search = GridSearchCV(cbr, param_grid, n_jobs=-1,\n#                            scoring=mape, cv=5, refit=True,\n#                            return_train_score=True, verbose=5)\n\n# grid_search.fit(X, y)","e54d439d":"# # Best model parameters\n# cbr.get_params()","990bd830":"# # Checking how cbr performs on validation sample\n# mape(np.exp(y_val), np.exp(cbr.predict(X_val)))\n\n# # # Predictions on test sample\n# # submission['price'] = np.exp(cbr.predict(X_test))\n# # submission.to_csv('submission.csv', index=False)","8e8505fa":"# Training xgboost model\nxg_reg = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.5,\n                          learning_rate=0.05, max_depth=12, alpha=1,\n                          n_estimators=1000, silent=1)\nxg_reg.fit(X_train, y_train)\nxg_red_pred = xg_reg.predict(X_val)","77d034de":"# MAPE for XGBoost\nmape(np.exp(y_val), np.exp(xg_red_pred))","bd4c6d7a":"# Cross-validation for XGBoost\n# params = {'objective': 'reg:squarederror',\n#           'colsample_bytree': 0.5,\n#           'learning_rate': 0.05,\n#           'max_depth': 10, \n#           'alpha': 1,\n#           'n_estimators': 1000}\n\n# cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=5,\n#                     num_boost_round=1000, early_stopping_rounds=10,\n#                     metrics=\"rmse\", as_pandas=True, seed=0)","76de65d2":"# Defining lightgbm hyper-parameters\nhyper_params = {\n                'task': 'train',\n                'boosting_type': 'gbdt',\n                'objective': 'regression',\n                'metric': ['mape'],\n                'learning_rate': 0.05,\n                'feature_fraction': 0.9,\n                'bagging_fraction': 0.7,\n                'bagging_freq': 10,\n                'verbose': 0,\n                \"max_depth\": 12,\n                \"num_leaves\": 128,  \n                \"max_bin\": 512,\n                \"num_iterations\": 1000,\n                \"n_estimators\": 1000\n            }\n\n# Training lightgbm model\nlgbm = lgb.LGBMRegressor(**hyper_params)\nlgbm.fit(X_train, y_train,\n         eval_set=[(X_val, y_val)],\n         eval_metric='l1',\n         early_stopping_rounds=100)\n\n# Printing the result\nmape(np.exp(y_val), np.exp(lgbm.predict(X_val)))","a87666c1":"# estimators = [('random_forest', rf),\n#               ('catboost', cbr), \n#               ('xgboost', xg_reg),\n#               ('lightgbm', lgbm)]\n\n# stack = StackingRegressor(estimators=estimators,\n#                           n_jobs=-1,\n#                           cv=3,\n#                           final_estimator=xg_reg,\n#                           verbose=100)\n\n# stack.fit(X_train, y_train).score(X_test, y_test)","840adcc6":"Getting all links to car pages:","8dce82d5":"***Baseline results:***\n\n> The baseline model in form of Random Forest (Extra Trees) with only *default numeric features* gave us **20** MAPE\n\n> The baseline model with logarithmic *price* gave us **17.98** MAPE\n\n> The baseline model with complectation and equipment length gave us **16.44** MAPE\n\n> The baseline model with mean mileage and mean engine power by car model gave us **14.44** MAPE\n\n> The baseline model with body type dummies gave us **16.63** MAPE\n\n> The baseline model with 98 most important features gave us **14.56** MAPE\n\n> **Minimum: 12.51 MAPE with num_cols + cat_cols (LE) + tf_idf most important binary and numeric features**","8d7f21c8":"##  Name","127efcec":"# Feature Engineering\n\nEarlier we got some ideas on how to extract new features that might help us to predict car prices, so it is time to implement them:","e4680bc7":"* **Result:** all the categorical features can explain the variation of the target","9b38427c":"## Color","f81dc22f":"* Nothing to pre-process\n\n* **Idea:** maybe it is better to use a new dummy which defines if a car has a popular color","1e04143b":"Starting a new scrapy project with the following cmd commands:\n\n```\nscrapy shell\n!scrapy startproject auto_ru\n```\n\n*Then moving to the folder with spiders in our project folder*","0eb9fddb":"* Most cars have 100-200 horse powers\n\n* There are extremely powerful cars in the sample dataset whose engine power is up to 1000 h. p.\n\n* Makes sense to use logarithms of this value instead, log-variable contains less outliers and is distributed closer to normal distribution","42dbabfb":"* Everything is OK","3c4100bb":"* Now the price is distributed quite normally","1c222cb9":"## Stacking","71acf9f5":"Let's see how the test sample looks like to have the general idea of what is necessary to scrape:","14f3ac2d":"**The remaining heavy work will be done by [Scrapy](https:\/\/www.smashingmagazine.com\/2019\/07\/ultimate-guide-scalable-web-scrapers-scrapy\/) Spiders**","6b78cf90":"## Complectation","0af1381c":"* The assumption was right\n\n\n* We'll probably drop one of these features later - *removal of one of these features leads to worse model results*","9067ee87":"* All the numeric features correlate with our target except 'owning_time' (used owning time intervals instead). The removal of numeric owning time gave a little worse MAPE score\n\n* Model date and production date are highly correlated as it was expected. Let's drop the model date since the production date has a little higher correlation with the target - *no need to drop any of them*\n\n* Some features have extremely high correlation coefficients between each other but, nevertheless, they all improve the model quality to some extent\n\n* Fuel rate has the lowest correlation coefficient - a candidate for removal","d9740590":"## Steering wheel","af3def01":"* Everything is OK with this feature\n\n\n* This feature is useful for **model_name transformation**","503976c2":"* Almost a half of Moscow auto market is taken by European cars\n* What is noticable: \"VENDOR_UNKNOWN\" (IRANIAN\/TAIWANESE) category simply can't be visualized due to a tiny proportion in respect to the whole dataset","44d397c8":"Since we have a relatively normal distribution of out target, let's conduct ANOVA test to define which categories are statistically significant. There are too many unique brands (body types, etc.) to make their combinations (my laptop basically explodes), so let's randomly take only ten of them:","fbdfcaf5":"* So these are racing bolides and open off-road cars which explains the absence of doors\n\n* **Idea:** drop observations with 0 doors","3693f467":"# Modules","eec7ab53":"## Mileage\n","a563d3d4":"## Body Type","bc7b653a":"## Model info","244596df":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Modules\" data-toc-modified-id=\"Modules-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Modules<\/a><\/span><\/li><li><span><a href=\"#Data-Scraping\" data-toc-modified-id=\"Data-Scraping-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Data Scraping<\/a><\/span><\/li><li><span><a href=\"#Data-Pre-processing-and-EDA\" data-toc-modified-id=\"Data-Pre-processing-and-EDA-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Data Pre-processing and EDA<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Body-Type\" data-toc-modified-id=\"Body-Type-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Body Type<\/a><\/span><\/li><li><span><a href=\"#Brand\" data-toc-modified-id=\"Brand-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;<\/span>Brand<\/a><\/span><\/li><li><span><a href=\"#Car_url\" data-toc-modified-id=\"Car_url-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;<\/span>Car_url<\/a><\/span><\/li><li><span><a href=\"#Color\" data-toc-modified-id=\"Color-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;<\/span>Color<\/a><\/span><\/li><li><span><a href=\"#Complectation\" data-toc-modified-id=\"Complectation-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;<\/span>Complectation<\/a><\/span><\/li><li><span><a href=\"#Description\" data-toc-modified-id=\"Description-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;<\/span>Description<\/a><\/span><\/li><li><span><a href=\"#Engine-displacement\" data-toc-modified-id=\"Engine-displacement-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;<\/span>Engine displacement<\/a><\/span><\/li><li><span><a href=\"#Engine-power\" data-toc-modified-id=\"Engine-power-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;<\/span>Engine power<\/a><\/span><\/li><li><span><a href=\"#Equipment\" data-toc-modified-id=\"Equipment-3.9\"><span class=\"toc-item-num\">3.9&nbsp;&nbsp;<\/span>Equipment<\/a><\/span><\/li><li><span><a href=\"#Fuel-type\" data-toc-modified-id=\"Fuel-type-3.10\"><span class=\"toc-item-num\">3.10&nbsp;&nbsp;<\/span>Fuel type<\/a><\/span><\/li><li><span><a href=\"#Mileage\" data-toc-modified-id=\"Mileage-3.11\"><span class=\"toc-item-num\">3.11&nbsp;&nbsp;<\/span>Mileage<\/a><\/span><\/li><li><span><a href=\"#Model-date\" data-toc-modified-id=\"Model-date-3.12\"><span class=\"toc-item-num\">3.12&nbsp;&nbsp;<\/span>Model date<\/a><\/span><\/li><li><span><a href=\"#Model-info\" data-toc-modified-id=\"Model-info-3.13\"><span class=\"toc-item-num\">3.13&nbsp;&nbsp;<\/span>Model info<\/a><\/span><\/li><li><span><a href=\"#Model-name\" data-toc-modified-id=\"Model-name-3.14\"><span class=\"toc-item-num\">3.14&nbsp;&nbsp;<\/span>Model name<\/a><\/span><\/li><li><span><a href=\"#Name\" data-toc-modified-id=\"Name-3.15\"><span class=\"toc-item-num\">3.15&nbsp;&nbsp;<\/span>Name<\/a><\/span><\/li><li><span><a href=\"#Doors\" data-toc-modified-id=\"Doors-3.16\"><span class=\"toc-item-num\">3.16&nbsp;&nbsp;<\/span>Doors<\/a><\/span><\/li><li><span><a href=\"#Production-date\" data-toc-modified-id=\"Production-date-3.17\"><span class=\"toc-item-num\">3.17&nbsp;&nbsp;<\/span>Production date<\/a><\/span><\/li><li><span><a href=\"#Super-gen\" data-toc-modified-id=\"Super-gen-3.18\"><span class=\"toc-item-num\">3.18&nbsp;&nbsp;<\/span>Super gen<\/a><\/span><\/li><li><span><a href=\"#Vehicle-configuration\" data-toc-modified-id=\"Vehicle-configuration-3.19\"><span class=\"toc-item-num\">3.19&nbsp;&nbsp;<\/span>Vehicle configuration<\/a><\/span><\/li><li><span><a href=\"#Vehicle-transmission\" data-toc-modified-id=\"Vehicle-transmission-3.20\"><span class=\"toc-item-num\">3.20&nbsp;&nbsp;<\/span>Vehicle transmission<\/a><\/span><\/li><li><span><a href=\"#Vendor\" data-toc-modified-id=\"Vendor-3.21\"><span class=\"toc-item-num\">3.21&nbsp;&nbsp;<\/span>Vendor<\/a><\/span><\/li><li><span><a href=\"#Owners\" data-toc-modified-id=\"Owners-3.22\"><span class=\"toc-item-num\">3.22&nbsp;&nbsp;<\/span>Owners<\/a><\/span><\/li><li><span><a href=\"#Owning-time\" data-toc-modified-id=\"Owning-time-3.23\"><span class=\"toc-item-num\">3.23&nbsp;&nbsp;<\/span>Owning time<\/a><\/span><\/li><li><span><a href=\"#Passport\" data-toc-modified-id=\"Passport-3.24\"><span class=\"toc-item-num\">3.24&nbsp;&nbsp;<\/span>Passport<\/a><\/span><\/li><li><span><a href=\"#Gear-type\" data-toc-modified-id=\"Gear-type-3.25\"><span class=\"toc-item-num\">3.25&nbsp;&nbsp;<\/span>Gear type<\/a><\/span><\/li><li><span><a href=\"#Steering-wheel\" data-toc-modified-id=\"Steering-wheel-3.26\"><span class=\"toc-item-num\">3.26&nbsp;&nbsp;<\/span>Steering wheel<\/a><\/span><\/li><li><span><a href=\"#Price-(!)\" data-toc-modified-id=\"Price-(!)-3.27\"><span class=\"toc-item-num\">3.27&nbsp;&nbsp;<\/span>Price (!)<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Feature Engineering<\/a><\/span><\/li><li><span><a href=\"#Correlation-Analysis\" data-toc-modified-id=\"Correlation-Analysis-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Correlation Analysis<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Numeric-features\" data-toc-modified-id=\"Numeric-features-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>Numeric features<\/a><\/span><\/li><li><span><a href=\"#Categorical-(binary)-features\" data-toc-modified-id=\"Categorical-(binary)-features-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>Categorical (binary) features<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Machine-Learning\" data-toc-modified-id=\"Machine-Learning-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Machine Learning<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Baseline-model\" data-toc-modified-id=\"Baseline-model-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;<\/span>Baseline model<\/a><\/span><\/li><li><span><a href=\"#Catboost\" data-toc-modified-id=\"Catboost-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;<\/span>Catboost<\/a><\/span><\/li><li><span><a href=\"#XGBoost\" data-toc-modified-id=\"XGBoost-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;<\/span>XGBoost<\/a><\/span><\/li><li><span><a href=\"#LightGBM\" data-toc-modified-id=\"LightGBM-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;<\/span>LightGBM<\/a><\/span><\/li><li><span><a href=\"#Stacking\" data-toc-modified-id=\"Stacking-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;<\/span>Stacking<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>","0ac1050c":"## Gear type","ab773f5f":"* This feature is a mixture of \"engine_disp\" and \"vehicle_transmission\" features","d8c92bbf":"* LightGBM resulted in 11.74 MAPE","921e16e6":"* So both ANOVA tests gave us the same 50 best variables\n\n* First of all, we'll try to use only these 50 best features revealed by ANOVA","0c6f73f0":"* This feature probably shows the country of origin of a car\n\n* It is possible to fill in missing values using our personal brand knowledge and google\n\n* To assure that the meaning of this feature is understood right let's check the brands corresponding to each vendor type","49fb10c6":"* Everything is OK with this feature","86d2f0ed":"* There are many empty dictionaries which should be taken into account as missing values\n\n* **Ideas:** length of equipment as a new feature, presence of equipment as a new feature, number of 'True' values as a new feature, each element of equipment dictionary as a new dummy-variable","10013c9b":"Getting initial urls:","8c45ed61":"**So the data pre-processing and EDA are done for each feature in detail**","2cf62c54":"* At least from the first look it can be assumed that American cars are driven more","6dc2cfa3":"**Take-aways:**\n\n* 126 unique car brands presented in the dataset, the most popular - **Mercedes**\n\n\n* Most popular car model is **VW Polo VI**\n\n\n* On average, car's mileage is equal to **114,500 km**\n\n\n* Most popular car color - **black**\n\n\n* The average car price is **1,576,173 RUB** and it differs from the median value (760,000 RUB) a lot which means that there are cars-outliers with extremely high prices\n\n\n* There are useless features with only one category, makes sense to drop them right now\n\n* Some features in train and test samples have different values for the same categories","aa172c28":"## Model date","422d0614":"## Doors","c9a9f03f":"* Everything is OK with this feature","0e61da9b":"* Everything is OK with this feature\n\n\n* **Idea**: apply count vectorizer\/TF-IDF\/w2v to this one later, length of description (number of words)","2d94a5d6":"Let's check how the number of produced cars differs from year to year:","b3d9ff63":"Checking whether the current ip-address rotates via Tor Browser every 10 seconds:","d5486405":"## Price (!)\n\n","84c927bb":"## Catboost","4aae9b6f":"Appealing to google once more and replacing those values:","41192324":"\n\nLet's check the other car that has a non-standard engine displacement value:","38a38bf6":"* Most cars on Moscow market are new cars produced in 2020\n\n\n* Assumption: COVID-19 decreased economy growth rate which lead to the drop in people's real income and that is why many new cars are not sold yet","9b92607f":"Let's check the **significance of correlation coefficients** using a statistical test:","5591e010":"* Complectation dictionary length feature plays an impressive role in ML\n\n* It probably makes sense to try to generate more features from complectation\/equipment","de32dffc":"* Most cars have mileage between 20 and 180 thousand km\n\n* **Idea:** delete outliers, use log-transformation","4375319f":"**The second** spider will return all the characteristics of each car we need:","03c01058":"**Notation:** the data was scraped several times to get a bigger training set","2cb642c8":"## Vendor","215a2adb":"* Stacking required too much time and calculation hardware, so it was skipped\n\n* The best result was reached by using XGBoost","042084a4":"# Correlation Analysis","2e1a1e94":"* Everything is OK\n\n\n* This feature must have a high correlation with \"model_date\"","d90aa4a7":"* Seems like this is not a country of origin but a place where the cars were produced\/assembled because it is a common knowledge that CHEVROLET or DAEWOO have\/had their own factories in Russia but they are not Russian car brands. We can mark NaN-values as \"VENDOR_UKNOWN\", drop them or define the countries of origin of brands\n\n* VENDOR_UNKNOWN contains cars from Iran and Taiwan","bf503873":"## Owners","ba6d8786":"Model names in test and train simple differ a lot. We need to bring them to the same format using information from **model_info**:","fcf5fc11":"# Machine Learning","6a8c9f6a":"* Obviously most cars cost less than 1,000,000 RUB\n\n* At the same time there are numerous outliers\n\n* Makes sense to use log-normal distribution and maybe get rid of 302 outliers ","2f1a0b7e":"## Equipment","2d0fa164":"It is interesting how mileage varies depending on country of origin of a car:","59b48925":"## Production date","3d931713":"* After transforming our categorical data to dummy format we got 4782 columns which is a large number of features\n\n* It makes sense to use dimensionality reduction technique like PCA or exclude \"model_name\" feature since it contains 4K+ unique categories\n","46a3fb5d":"# Data Pre-processing and EDA","cfbdb97d":"So we got 559 out of 562 offers without vehicle passport. Let's check the remaining 5 car offers:","3e00502e":"**The first** spider will return all the links to car offers from auto.ru:","d907d943":"## Fuel type","567a25d8":"* Everything os OK with this feature","ff178a6e":"## LightGBM","a4107f80":"This car offer has incorrect characteristics, it actually should also have another model name and body type. What is more, [this offer is from Tulskaya oblast'](https:\/\/auto.ru\/cars\/used\/sale\/uaz\/3153\/1003979736-ec79c1\/?from=searchline) which is not interesting for us.","88632291":"## Engine displacement","dca28dc1":"\n* Surprisingly, there are car offers where the price is not specified\n\n\n* Assumption: the price was lost during scrapping since all these cars are already sold","8cafa5af":"* Scatter plots show us that there is linear dependency between target and some of the numeric features\n\n\n* Production date clearly illustrates that very new and very old cars tend to be more expensive\n","cee7a307":"## Numeric features","f087ae86":"* 5 out of 10 most popular car models are Russian - LADA\n \n* This feature has 4K+ categories which means that the data for ML will have a large number of features, so it is better not to use this categorical feature itself\n\n* **Idea:** use mean\/median statistics of numeric features for each model name, use dummy variable showing that a certain model is a rare\/old one","2ab46e1f":"## Description","277df5e4":"* Well, this feature might help to extract some information that was somehow missed during web scraping, so I will leave it for now\n\n\n* Some of the urls are already invalid\n\n* **Idea:** if used is in the url then mark an observation as a used car and vice versa for new cars","b17a6ff1":"* This feature contains different important car characteristics\n\n\n* **Ideas:** fuel_rate as a new feature, acceleration as a new feature, length of super_gen","24e171e4":"## Passport","48a8222c":"## Vehicle configuration","f9fbf2ba":"## Model name","65d713ae":"* The number of cars using alternative enegry sources is so little that the charts above couldn't even visualize them properly\n\n* Unfortunately, this proves that Moscow is not adapted to electric cars and people still tend to prefer traditional fuel types\n\n* Maybe gas fuel should be omitted since it occurs rarely","1dd0b333":"* Everything is OK except zero values\n\n\n* This is a categorical feature - the dtype needs a change","c039a4b9":"* Everything is OK","23de3a75":"* Most cars are Tesla, Audi and Nissan \n\n\n* This actually explains well enough why electro cars have the highest mean price\n","21fc55d9":"* Most values are missing\n\n\n* **Idea:** length\/presence of complectation as a new feature","8cf6b9ea":"## Vehicle transmission","dc0ed138":"**Now let's walk through each feature to pre-process each one and understand the data better**\n\nBut before that let's define a few functions for convenience:","5d3c915d":"* Everything is OK with this feature","f804ea84":"* This is another duplicating feature","32216977":"* Most cars presented on Moscow auto market are models from 2019\n\n\n\n* It makes sense not to use each model year as a category but to divide years into intervals and use them as categories instead","463d5119":"* There are different variations of the same body type like an off-road with 3 or 5 doors. Maybe it would be more efficient for ML to make body types more universal\n\n\n* There are 2 missing values, let's check them","53221064":"## XGBoost ","53277481":"* Numerous unique values - this is definitely a numeric feature","123fc7e3":"## Baseline model","9724e5d0":"* 4 out of 10 most frequent car brands are German\n\n* It probably doesn't make sense to take into account rare models which are presented in less than 5 offers since there are only 66 such observations\n\n* **Idea:** mean\/median statistics of numeric features for each brand, dummy-variable for top-10 brands, dummy-variable for sport-car brands like ferrari, mclaren, etc.\n\n","3b54e7b9":"## Brand","dd63766c":"The price is not normally distributed, let's use a log-transformation since it might help to get the normal distribution:","aeb0da66":"Finally, got a dataset:","40c64f3c":"* XGBoost resulted in 10.86 MAPE","2fc144f6":"First of all, let's train a **baseline** model of **Random Forest:**","a0ecebb9":"# Data Scraping\n\nFirst of all, it is necessary to collect (scrape) training dataset **from [auto.ru](https:\/\/auto.ru\/moskva\/)**","effdc6c8":"## Categorical (binary) features","d55b6f00":"Let's try filling missing values with the countries where a certain brand was founded:","541ab1f7":"## Owning time","cde5ba5b":"* So the majority of cars have 5 or 4 doors\n\n* The proportion of cars with 0 doors is extremely little (maybe we'll drop them)\n","c0e12f4c":"For cat features we can use one of the available statistical tests to check if mean prices differ significantly for different category groups","bae874f2":"* Most values are missing, makes sense to omit this feature\n\n* Let's fill in with \"no info\" for now\n\n* It makes sense to extract the number of years and months and make other categories from owning time intervals where the NaN-values will be marked as \"no info\" or smth\n\n* Maybe it could also be sensible to check how the owning time differs from the production date and fill in missing values according to that information","e40a2193":"These two car offers have too much missing information. It is more rational to get rid of them:","c1ca91b6":"* These cars seem to be super old\n\n* Assumption: they were left in a garage for a long time which might explain the abscence of vehicle passport\n\n* Maybe it is better to omit old vehicles without a passport","0de0e411":"## Engine power","fd7fcc4e":"Assumption: cars with missing vehicle passport value are new cars with 0 mileage:","51f1c939":"**Results for CatBoost:**\n\n* Catboost gave us 11.26 MAPE on cross-validation with **only numeric features**\n\n* Catboost gave us 11.26 MAPE on cross-validation with numeric features and TF-IDF PCs","102a445e":"* Everything is OK with this column","29d49d05":"* So with 99% of confidence we can conclude that all the correlation coefficients are significant","e125afab":"Obviously, most cars above have an electric engine, so let's leave their engine displacement values as they are.","4ab173cf":"* Many missing values - let's create a new category \"no info\"\n\n* The other values need to be cleansed from \"\\xa0\"\n\n* **Idea:** use backfill, forwardfill methods to impute missing values","55d2455d":"Then running both in a terminal with the following commands:\n\n```\n!scrapy crawl autoru -o autoru_new.csv\n!scrapy crawl cars -o cars.csv\n```","4ca0dd34":"* Seems like it's better to convert this feature to float format\n\n\n* There is one value without a number, let's figure out what it is","751243b7":"* Most cars have automatic gearbox","47d6c2b9":"## Super gen","d37d165a":"## Car_url","e2e901ff":"* Most cars have an engine displacement close to 2 LTR\n\n* For now, we'll use this value as a numeric feature\n\n* Logarithmic variable contains less outliers but its distribution is still far from a normal one\n\n* **Ideas:** make categories from engine displacement intervals (i. e. 0, 0-1, 1-2, 2-3, etc.), use log-dist, replace \" LTR\" with mean\/median\/smth else"}}