{"cell_type":{"31dd7f51":"code","24535241":"code","1fc59c3a":"code","cc421ab7":"code","7b2fa0ca":"code","2e2708d3":"code","2594d7f9":"code","93c9bb04":"code","a4b2c1c1":"code","5bd9bd3d":"code","55dc0766":"code","e85d980f":"code","76cd7482":"code","1185db0d":"code","65a04b89":"code","2bbb181c":"code","16c6aa5c":"code","fad8efa8":"code","61cb93e7":"code","7b79a9ff":"code","9e903bf8":"code","a2014b75":"code","9d3e9ff2":"code","34aaaa00":"code","a5bc66ba":"code","b514032f":"code","19f4c029":"code","c7821699":"code","94c93da7":"code","d7228ee3":"code","010408d1":"code","f54c99ad":"code","4b24d02f":"code","650d34cf":"code","993dc47c":"code","2baf7ac1":"code","8502f1d7":"code","de6404e8":"code","8c7e1f67":"code","31132bdb":"code","667996f3":"code","f3394061":"code","97485542":"markdown","2d7cfc05":"markdown","de8ecf20":"markdown","57856976":"markdown","cd5c9b9f":"markdown","ed9c7876":"markdown","824dece4":"markdown","92045c99":"markdown","c5f9dc77":"markdown","e643e0ea":"markdown","1f729376":"markdown","e77b175b":"markdown","9735fb1e":"markdown","80d8945f":"markdown","d3a4949c":"markdown","10f76835":"markdown","429da5bf":"markdown","bff2bcd6":"markdown","42af5898":"markdown","16e1e0e3":"markdown","a01e2aae":"markdown","31c2bc56":"markdown","f4f47c9a":"markdown","99eddecb":"markdown","5fdedf4a":"markdown","12dc1e54":"markdown","8a2a9ced":"markdown","0a5c9140":"markdown","7d2e67b9":"markdown","2c88fefb":"markdown","6fb34bf0":"markdown"},"source":{"31dd7f51":"# Importing Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re, os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import stats\nimport warnings\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import learning_curve\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nprint(os.listdir(\"..\/input\"))","24535241":"#Bring data\ntrain = pd.read_csv('..\/input\/nassCDS1.csv')\ntrain = train.drop(['Unnamed: 0','Unnamed: 0.1','caseid'], axis=1)\ntrain = train[pd.notnull(train['injSeverity'])]\ntrain = train[pd.notnull(train['yearVeh'])]\ntrain.head()","1fc59c3a":"# Transforming Dead\nle = LabelEncoder()\ntrain.dead=le.fit_transform(train.dead)\ntrain.head()","cc421ab7":"ax = sns.barplot(x=\"frontal\", y=\"dead\", hue='sex', data=train)","7b2fa0ca":"ax = sns.barplot(x=\"seatbelt\", y=\"dead\", hue='occRole', data=train)","2e2708d3":"ax = sns.barplot(x=\"airbag\", y=\"dead\", hue='occRole', data=train)","2594d7f9":"ax = sns.barplot(x=\"abcat\", y=\"dead\", hue='occRole', data=train)","93c9bb04":"ax = sns.barplot(x=\"dvcat\", y=\"weight\", hue='dead', data=train)","a4b2c1c1":"data = pd.concat([train['weight'], train['ageOFocc']], axis=1)\ndata.plot.scatter(x='ageOFocc', y='weight');","5bd9bd3d":"# Transforming DVCAT\nle = LabelEncoder()\nle.fit(['1-9km\/h','10-24','25-39','40-54', '55+'])\ntrain.dvcat=le.transform(train.dvcat)\ntrain.head()","55dc0766":"g = sns.FacetGrid(train, col='dead')\ng.map(plt.hist, 'dvcat', bins=20)","e85d980f":"grid2 = sns.FacetGrid(train, row='seatbelt', col='dead', size=2.2, aspect=1.6)\ngrid2.map(sns.barplot, 'sex', 'dvcat', alpha=.5, ci=None)\ngrid2.add_legend()","76cd7482":"ax = sns.barplot(x=\"injSeverity\", y=\"dvcat\", hue='dead', data=train)","1185db0d":"# Transforming Airbag\nairbag  = pd.get_dummies( train.airbag , prefix='has'  )\ntrain= pd.concat([train, airbag], axis=1)  \n# we should drop one of the columns\ntrain = train.drop(['has_none','airbag'], axis=1)\ntrain.head()","65a04b89":"# Transforming seatbelt\nseatbelt  = pd.get_dummies( train.seatbelt , prefix='is'  )\ntrain= pd.concat([train, seatbelt], axis=1)  \n# we should drop one of the columns\ntrain = train.drop(['is_none','seatbelt'], axis=1)\ntrain.head()","2bbb181c":"# Transforming Abcat\nabcat  = pd.get_dummies( train.abcat , prefix='abcat'  )\ntrain= pd.concat([train, abcat], axis=1)  \n# we should drop one of the columns\ntrain = train.drop(['abcat_unavail','abcat'], axis=1)\ntrain.head()","16c6aa5c":"# Transforming occRole\noccRole  = pd.get_dummies( train.occRole , prefix='is'  )\ntrain= pd.concat([train, occRole], axis=1)  \n# we should drop one of the columns\ntrain = train.drop(['is_pass','occRole'], axis=1)\ntrain.head()","fad8efa8":"# Transforming Sex\nle = LabelEncoder()\ntrain.sex=le.fit_transform(train.sex)\ntrain.head()","61cb93e7":"train.head(10)","7b79a9ff":"corr=train.corr()\nfig = plt.figure(figsize=(10,10))\nr = sns.heatmap(corr, cmap='Purples')\nr.set_title(\"Correlation \")","9e903bf8":"#price range correlation\ncorr.sort_values(by=[\"dead\"],ascending=False).iloc[0].sort_values(ascending=False)","a2014b75":"train = train.drop(['abcat_deploy','deploy','is_driver','yearacc','abcat_nodeploy','yearVeh','has_airbag','frontal','weight','is_belted'], axis=1)\ntrain.head()","9d3e9ff2":"y = train.dead\nX_data=train.drop([\"dead\"],axis=1)\n#x = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_data,y,test_size = 0.2,random_state=1)","34aaaa00":"#DesicionTree\nfrom sklearn.tree import DecisionTreeClassifier\nmodel= DecisionTreeClassifier(random_state=1234)\n#Hyper Parameters Set\nparams = {'max_features': [None,'auto', 'sqrt', 'log2'],\n          'max_depth' : [None,1,3,5,7,9,11],\n          'min_samples_split': [2,3,4,5,6,7,8,9,10], \n          'min_samples_leaf':[1,2,3,4,5,6,7,8,9,10,11],\n          'random_state':[123]}\n#Making models with hyper parameters sets\nmodel1 = GridSearchCV(model, param_grid=params, n_jobs=-1)\n#Learning\nmodel1.fit(X_train,y_train)\n#The best hyper parameters set\nprint(\"Best Hyper Parameters:\",model1.best_params_)\n#Prediction\nprediction=model1.predict(X_test)\n#evaluation(Accuracy)\nprint(\"Accuracy:\",metrics.accuracy_score(prediction,y_test))\nprint(\"F1-Score:\",f1_score(y_test, prediction, average=\"macro\"))\nprint(\"Precision:\",precision_score(y_test, prediction, average=\"macro\"))\nprint(\"Recall:\",recall_score(y_test, prediction, average=\"macro\"))  ","a5bc66ba":"#DesicionTree\n# creating list\nmax_depth_list = [1,3,5,7,9,11,13,15]\n# empty list that will hold cv scores\ncv_scores = []\n# perform 10-fold cross validation\nfor d in max_depth_list:\n    dtc= DecisionTreeClassifier(max_depth=d, min_samples_split=2,min_samples_leaf=9, random_state=123)\n    scores = cross_val_score(dtc, X_train, y_train, cv=10, scoring='accuracy')\n    cv_scores.append(scores.mean())\n    \n# changing to misclassification error\nerrors = [1 - x for x in cv_scores]\n\n# determining best max_depth\noptimal_max_depth = max_depth_list[errors.index(min(errors))]\nprint (\"The optimal max depth is %d\" % optimal_max_depth)\n\n# plot misclassification error vs k\nplt.plot(max_depth_list, errors)\nplt.xlabel('Max Depth')\nplt.ylabel('Validation Error')\nplt.xticks(max_depth_list)\nplt.show()","b514032f":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.2, .4, 2.0, 5)):\n\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Train Size\")\n    plt.ylabel(\"Accuracy\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)*100\n    train_scores_std = np.std(train_scores, axis=1)*100\n    test_scores_mean = np.mean(test_scores, axis=1)*100\n    test_scores_std = np.std(test_scores, axis=1)*100\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\n\ntitle = r\"Decision Tree Classifier Learning Curves\"\ncv = 10\nestimator = DecisionTreeClassifier(max_depth=3, min_samples_split=2,min_samples_leaf=9, random_state=123)\nplot_learning_curve(estimator, title, X_train, y_train, (80, 100), cv=cv, n_jobs=4, train_sizes=[1,1000,2000,3000,4000,4450])\nplt.grid()\nplt.show()","19f4c029":"#SVM\nfrom sklearn import svm\n#making the instance\nmodel=svm.SVC()\n#Hyper Parameters Set\nparams = {'C': [.001,.003,.01,.03,.1,.3,1,3], \n          'kernel': ['linear','rbf'],\n          'gamma':['auto',0,.01,.1]}\n#Making models with hyper parameters sets\nmodel1 = GridSearchCV(model, param_grid=params, n_jobs=-1)\n#Learning\nmodel1.fit(X_train,y_train)\n#The best hyper parameters set\nprint(\"Best Hyper Parameters:\\n\",model1.best_params_)\n#Prediction\nprediction=model1.predict(X_test)\n#evaluation(Accuracy)\nprint(\"Accuracy:\",metrics.accuracy_score(prediction,y_test))\nprint(\"F1-Score:\",f1_score(y_test, prediction, average=\"macro\"))\nprint(\"Precision:\",precision_score(y_test, prediction, average=\"macro\"))\nprint(\"Recall:\",recall_score(y_test, prediction, average=\"macro\"))  ","c7821699":"#SVM\n# creating list\nc_list = [.001,.003,.01,.03,.1,.3,1,3]\n# empty list that will hold cv scores\ncv_scores = []\n# perform 10-fold cross validation\nfor c in c_list:\n    svc= svm.SVC(kernel='linear', C=c)\n    scores = cross_val_score(svc, X_train, y_train, cv=10, scoring='accuracy')\n    cv_scores.append(scores.mean())\n    \n# changing to misclassification error\nerrors = [1 - x for x in cv_scores]\n# determining best max_depth\noptimal_c = c_list[errors.index(min(errors))]\nprint (\"The C depth is %f\" % optimal_c)\n\n# plot misclassification error vs k\nplt.plot(c_list, errors)\nplt.xlabel('C')\nplt.ylabel('Validation Error')\nplt.xticks(c_list)\nplt.xscale(value='log')\nplt.show()","94c93da7":"#SVM\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.01, 1.0, 5)):\n\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Train Size\")\n    plt.ylabel(\"Accuracy\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)*100\n    train_scores_std = np.std(train_scores, axis=1)*100\n    test_scores_mean = np.mean(test_scores, axis=1)*100\n    test_scores_std = np.std(test_scores, axis=1)*100\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ntitle = r\"SVM Classifier Learning Curves\"\ncv = 10\nestimator = svm.SVC(kernel='linear', C=.003)\nplot_learning_curve(estimator, title, X_train, y_train, (80, 100), cv=cv, n_jobs=-1)\nplt.grid()\nplt.show()","d7228ee3":"#kNearestNeighbors\n#importing modules\nfrom sklearn.neighbors import KNeighborsClassifier\n#making the instance\nmodel = KNeighborsClassifier(n_jobs=-1)\n#Hyper Parameters Set\nparams = {'n_neighbors':[1,3,5,7,9],\n          'leaf_size':[1,2,3,5],\n          'weights':['uniform', 'distance'],\n          'algorithm':['auto', 'ball_tree','kd_tree','brute'],\n          'n_jobs':[-1]}\n#Making models with hyper parameters sets\nmodel1 = GridSearchCV(model, param_grid=params, n_jobs=1)\n#Learning\nmodel1.fit(X_train,y_train)\n#The best hyper parameters set\nprint(\"Best Hyper Parameters:\\n\",model1.best_params_)\n#Prediction\nprediction=model1.predict(X_test)\n#evaluation(Accuracy)\nprint(\"Accuracy:\",metrics.accuracy_score(prediction,y_test))\nprint(\"F1-Score:\",f1_score(y_test, prediction, average=\"macro\"))\nprint(\"Precision:\",precision_score(y_test, prediction, average=\"macro\"))\nprint(\"Recall:\",recall_score(y_test, prediction, average=\"macro\"))  ","010408d1":"#KNN\n# creating list\nn_neighbors_list = [1,3,5,7,9,11]\n# empty list that will hold cv scores\ncv_scores = []\n# perform 10-fold cross validation\nfor n in n_neighbors_list:\n    knn= KNeighborsClassifier(algorithm='brute', n_neighbors=n, n_jobs= -1, weights= 'distance')\n    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n    cv_scores.append(scores.mean())\n    \n# changing to misclassification error\nerrors = [1 - x for x in cv_scores]\n#print(errors)\n# determining best max_depth\noptimal_n = n_neighbors_list[errors.index(min(errors))]\nprint (\"The optimal N neighbor is %f\" % optimal_n)\n\n# plot misclassification error vs k\nplt.plot(n_neighbors_list, errors)\nplt.xlabel('N neighbor')\nplt.ylabel('Validation Error')\nplt.xticks(n_neighbors_list)\n#plt.xscale(value='log')\nplt.show()","f54c99ad":"#KNN\ntitle = r\"KNN Classifier Learning Curves\"\ncv = 10\nestimator = KNeighborsClassifier(algorithm='brute', n_neighbors=3, n_jobs= -1, weights= 'distance')\nplot_learning_curve(estimator, title, X_train, y_train, (80, 100), cv=cv, n_jobs=-1)\nplt.grid()\nplt.show()","4b24d02f":"#Randomforest\n#importing modules\nfrom sklearn.ensemble import RandomForestClassifier\n#making the instance\nmodel=RandomForestClassifier()\n#hyper parameters set\nparams = {'criterion':['gini','entropy'],\n          'n_estimators':[10,15,20,25,30],\n          'min_samples_leaf':[1,2,3],\n          'min_samples_split':[3,4,5,6,7], \n          'random_state':[123],\n          'n_jobs':[-1]}\n#Making models with hyper parameters sets\nmodel1 = GridSearchCV(model, param_grid=params, n_jobs=-1)\n#learning\nmodel1.fit(X_train,y_train)\n#The best hyper parameters set\nprint(\"Best Hyper Parameters:\\n\",model1.best_params_)\n#Prediction\nprediction=model1.predict(X_test)\n#evaluation(Accuracy)\nprint(\"Accuracy:\",metrics.accuracy_score(prediction,y_test))\nprint(\"F1-Score:\",f1_score(y_test, prediction, average=\"macro\"))\nprint(\"Precision:\",precision_score(y_test, prediction, average=\"macro\"))\nprint(\"Recall:\",recall_score(y_test, prediction, average=\"macro\"))  ","650d34cf":"#RandomForest\n# creating list\nestimators_list = [10,15,20,25,30]\n# empty list that will hold cv scores\ncv_scores = []\n# perform 10-fold cross validation\nfor n in estimators_list:\n    dfc= RandomForestClassifier(criterion= 'gini', n_estimators= n, n_jobs= -1, random_state= 123)\n    scores = cross_val_score(dfc, X_train, y_train, cv=10, scoring='accuracy')\n    cv_scores.append(scores.mean())\n    \n# changing to misclassification error\nerrors = [1 - x for x in cv_scores]\n#print(errors)\n# determining best max_depth\noptimal_n = estimators_list[errors.index(min(errors))]\nprint (\"The optimal N estimators is %f\" % optimal_n)\n\n# plot misclassification error vs k\nplt.plot(estimators_list, errors)\nplt.xlabel('N Estimator')\nplt.ylabel('Validation Error')\nplt.xticks(estimators_list)\n#plt.xscale(value='log')\nplt.show()","993dc47c":"#RandomForest\ntitle = r\"Random Forest Classifier Learning Curves\"\ncv = 10\nestimator = RandomForestClassifier(criterion= 'gini', n_estimators= 20, n_jobs= -1, random_state= 123)\nplot_learning_curve(estimator, title, X_train, y_train, (90, 100), cv=cv, n_jobs=-1)\nplt.grid()\nplt.show()","2baf7ac1":"import numpy as np\nfrom scipy import interp\nimport matplotlib.pyplot as plt\n\nfrom sklearn import svm, datasets\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import StratifiedKFold\n\nX = X_train\ny = y_train\nn_samples, n_features = X.shape\n\n# Classification and ROC analysis\n# Run classifier with cross-validation and plot ROC curves\ncv = StratifiedKFold(n_splits=6)\nclassifier = svm.SVC(kernel='linear', probability=True,random_state=1)\n\ntprs = []\naucs = []\nmean_fpr = np.linspace(0, 1, 100)\n\ni = 0\nfor train, test in cv.split(X, y):\n    #print(\"TRAIN:\", train, \"TEST:\", test)\n    probas_ = classifier.fit(X.iloc[train], y.iloc[train]).predict_proba(X.iloc[test])\n    # Compute ROC curve and area the curve\n    fpr, tpr, thresholds = roc_curve(y.iloc[test], probas_[:, 1])\n    tprs.append(interp(mean_fpr, fpr, tpr))\n    tprs[-1][0] = 0.0\n    roc_auc = auc(fpr, tpr)\n    aucs.append(roc_auc)\n    plt.plot(fpr, tpr, lw=1, alpha=0.3,label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n\n    i += 1\nplt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',label='Chance', alpha=.8)\n\nmean_tpr = np.mean(tprs, axis=0)\nmean_tpr[-1] = 1.0\nmean_auc = auc(mean_fpr, mean_tpr)\nstd_auc = np.std(aucs)\nplt.plot(mean_fpr, mean_tpr, color='b',\n         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n         lw=2, alpha=.8)\n\nstd_tpr = np.std(tprs, axis=0)\ntprs_upper = np.minimum(mean_tpr + std_tpr, 1)\ntprs_lower = np.maximum(mean_tpr - std_tpr, 0)\nplt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n                 label=r'$\\pm$ 1 std. dev.')\n\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()","8502f1d7":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\ndtc = DecisionTreeClassifier(max_depth=3, min_samples_split=2,min_samples_leaf=9, random_state=123)\nsvc = svm.SVC(kernel='linear', C=.003)\nknn = KNeighborsClassifier(algorithm='brute', n_neighbors=3, n_jobs= -1, weights= 'distance')\nrfc = RandomForestClassifier(criterion= 'gini', n_estimators= 20, n_jobs= -1, random_state= 123)\nMLA = [\n    dtc,\n    svc,\n    knn,\n    rfc\n]","de6404e8":"MLA_columns = []\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\nrow_index = 0\nfor alg in MLA:\n    \n    predicted = alg.fit(X_train, y_train).predict(X_test)\n    fp, tp, th = roc_curve(y_test, predicted)\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index,'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Train Accuracy'] = round(alg.score(X_train, y_train), 4)\n    MLA_compare.loc[row_index, 'MLA Test Accuracy'] = round(alg.score(X_test, y_test), 4)\n    MLA_compare.loc[row_index, 'MLA Precission'] = precision_score(y_test, predicted)\n    MLA_compare.loc[row_index, 'MLA Recall'] = recall_score(y_test, predicted)\n    MLA_compare.loc[row_index, 'MLA F1-score'] = f1_score(y_test, predicted)\n    MLA_compare.loc[row_index, 'MLA AUC'] = auc(fp, tp)\n\n    row_index+=1\n    \nMLA_compare.sort_values(by = ['MLA Test Accuracy'], ascending = False, inplace = True)    \nMLA_compare","8c7e1f67":"plt.subplots(figsize=(15,6))\nsns.barplot(x=\"MLA Name\", y=\"MLA F1-score\",data=MLA_compare,palette='hot',edgecolor=sns.color_palette('dark',7))\nplt.xticks(rotation=90)\nplt.title('MLA F1-score Comparison')\nplt.show()","31132bdb":"plt.subplots(figsize=(15,6))\nsns.barplot(x=\"MLA Name\", y=\"MLA Recall\",data=MLA_compare,palette='hot',edgecolor=sns.color_palette('dark',7))\nplt.xticks(rotation=90)\nplt.title('MLA Recall Comparison')\nplt.show()","667996f3":"index = 1\nfor alg in MLA:\n    predicted = alg.fit(X_train, y_train).predict(X_test)\n    fp, tp, th = roc_curve(y_test, predicted)\n    roc_auc_mla = auc(fp, tp)\n    MLA_name = alg.__class__.__name__\n    plt.plot(fp, tp, lw=2, alpha=0.3, label='ROC %s (AUC = %0.2f)'  % (MLA_name, roc_auc_mla))\n   \n    index+=1\n\nplt.title('ROC Curve comparison')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')    \nplt.show()","f3394061":"index = 1\nfor alg in MLA:\n    predicted = alg.fit(X_train, y_train).predict(X_test)\n    fp, tp, th = roc_curve(y_test, predicted)\n    roc_auc_mla = auc(fp, tp)\n    MLA_name = alg.__class__.__name__\n    plt.plot(fp, tp, lw=2, alpha=0.3, label='ROC %s (AUC = %0.2f)'  % (MLA_name, roc_auc_mla))\n   \n    index+=1\n\nplt.title('ROC Curve comparison')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0,0.15])\nplt.ylim([0.75,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')    \nplt.show()","97485542":"**2- Data Cleaning, Feature Selection and Feature Engineering**\n\n","2d7cfc05":"Interpretation: Most Airbags are being deployed in a car accident but they are not heavily affecting the death results","de8ecf20":"Interpretation: Random Forest is giving a better result (F1-Score)\n\n**4- AUC-ROC**\n\n\nSources:\nhttps:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_roc_crossval.html\nhttps:\/\/www.kaggle.com\/aldemuro\/comparing-ml-algorithms-train-accuracy-90","57856976":"Displaying Barplot to compare F1-score for different Algorithms","cd5c9b9f":"**SVM**","ed9c7876":"Interpretation: Decision Tree is still the best algorithm for this problem. For KNN we need more data to get better results","824dece4":"Displaying Barplot to compare Recall for different Algorithms","92045c99":"Creating table to present different metrics:\n* Train Accuracy\n* Test Accuracy\n* Precision\n* Recall\n* F1-score\n* AUC","c5f9dc77":"Interpretation: Decision Tree is giving better results than SVM.\n\n**KNN**","e643e0ea":"Displaying AUC for different Algorithms on the same plot","1f729376":"Thank You! We hope this project has been helpful to you as much as it was enjoyable for us!","e77b175b":"Interpretation: Male drivers are more cautious (wear seatbelts) but still tend to have more mortalities because of their speed.","9735fb1e":"Interpretation: Same for Airbags, having an airbag reduces the casualties.","80d8945f":"Interpretation: Injury Severity is proportional to death stats and to speed. It must be a major factor.","d3a4949c":"**Car Accident Outcome Classification**\n\n![](https:\/\/accidentlawyersarizona.com\/wp-content\/uploads\/2014\/04\/Phoenix-Accident-Injuries.jpg)","10f76835":"Interpretation: Younger drivers tend to have heigher weights on their vehicules.","429da5bf":"Zooming In:","bff2bcd6":"Loading all 4 classifiers (Decision Tree, SVM, KNN, Random Forest) in order to compare them using different metric methods and displaying AUC.","42af5898":"First let's start by importing the essential libraries.","16e1e0e3":"Interpretation: Speed is now given a numeric label. Accidents at the lowest speed did not cause any deaths.","a01e2aae":"Interpretation: Seatbelt definitely helps reduce mortalities of accidents","31c2bc56":"**1- Data Exploration and Visualization**\n\n\nNow let's import CSV File with training dataset. We will delete id columns as they are not needed and display few rows.","f4f47c9a":"Interpretation: Speed is a major factor for death.","99eddecb":"**1- Data Exploration and Visualization**\n        - Load Dataset\n        - Visualize Features according to death status\n    \n**2- Data Cleaning, Feature Selection and Feature Engineering**\n        - Null Values\n        - Encode Categorical Data\n        - Transform Features\n        - Check Corrolation\n        - Split Data to Train and Test\n\n**3- Test Different Classifiers(Parameter Tuning, Optimal Parameter, Learning Curve)**\n        - Decision Tree\n        - Support Vector Machine (SVM)\n        - KNearestNeighbors (KNN)\n        - Random Forest\n        - AUC-ROC\n        \n **4- AUC-ROC**","5fdedf4a":"AUC with SVC (using K-Folds)","12dc1e54":"**3- Test Different Classifiers(Parameter Tuning, Optimal Parameter, Learning Curve)**\n\n\n**Decision Tree**\n\nSources:\nhttps:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_learning_curve.html\nhttps:\/\/www.kaggle.com\/mayu0116\/hyper-parameters-tuning-of-dtree-rf-svm-knn?fbclid=IwAR1NxkraubETx0vcPxOWvlGev0qns7oX-vHv-QwEfbJgxBMNeRAl9PuE3eM\n","8a2a9ced":"Interpretation: Training score and cross-validation Score converge as train data size increases.","0a5c9140":"Interpretation: Random Forest is providing the best results overall the metrics while KNN is being the worst for this specific problem.","7d2e67b9":"Now we will display some data visualization in order to understand the problem and data corrolations.","2c88fefb":"Transform \"dead\" column into numeric value","6fb34bf0":"Interpretation: Mortalities happen more often when sitting in the backseat."}}