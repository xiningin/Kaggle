{"cell_type":{"f8ba12a5":"code","b7fe41ad":"code","3d003459":"code","806129eb":"code","da9ade00":"code","352bd403":"code","ebf669f9":"code","a24bbfbf":"code","285163c2":"code","084f9c28":"code","a6aafb20":"code","2dc9272a":"code","128fd8f0":"code","b163e0ef":"code","f29c6d41":"code","464827d2":"code","2328df9a":"code","b68442ad":"code","f5f6114e":"code","e1c36862":"code","a14e7e3c":"code","78d4d8f9":"code","515bfe74":"code","bc1a31d5":"code","69e7fc56":"code","814fc7cd":"code","c617d2e3":"code","f1c148c5":"code","756ae7d9":"code","792fa45c":"code","88862f65":"code","1f27ee64":"markdown","13efae73":"markdown","a91aa53a":"markdown","e7100e61":"markdown","2ca389f4":"markdown","c6785bcf":"markdown","5602dcfe":"markdown","42b257c2":"markdown","3f9fc0d1":"markdown","7c128c6d":"markdown","29542b54":"markdown","bfe5c537":"markdown","9a3d14b2":"markdown","c5f60252":"markdown","e2a1ffe2":"markdown","dd07759c":"markdown","27a3a0eb":"markdown","61f74dca":"markdown","de0f53c6":"markdown","e4b79ade":"markdown","720928a3":"markdown","beac2e23":"markdown","5b363a5c":"markdown","0e1dea29":"markdown","8b9a690e":"markdown","4be9f5c7":"markdown","ca5474b3":"markdown","5f727a61":"markdown","c6ea4f91":"markdown","8323128a":"markdown","e0db8bae":"markdown","40f0fe37":"markdown","70a4073c":"markdown","a7ab5ea0":"markdown","f3d96eae":"markdown","cd81c556":"markdown","92cca20d":"markdown","dc5dc287":"markdown","d464285c":"markdown","ba4d6bc2":"markdown"},"source":{"f8ba12a5":"import numpy as np \nimport pandas as pd \nfrom sklearn import preprocessing\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nmatplotlib.style.use('ggplot')\n\nnp.random.seed(34)","b7fe41ad":"#create columns of various distributions\ndf = pd.DataFrame({ \n    'beta': np.random.beta(5, 1, 1000) * 60,        # beta\n    'exponential': np.random.exponential(10, 1000), # exponential\n    'normal_p': np.random.normal(10, 2, 1000),      # normal platykurtic\n    'normal_l': np.random.normal(10, 10, 1000),     # normal leptokurtic\n})\n\n# make bimodal distribution\nfirst_half = np.random.normal(20, 3, 500) \nsecond_half = np.random.normal(-20, 3, 500) \nbimodal = np.concatenate([first_half, second_half])\n\ndf['bimodal'] = bimodal\n\n# create list of column names to use later\ncol_names = list(df.columns)\nprint(col_names)","3d003459":"# plot original distribution plot\nfig, (ax1) = plt.subplots(ncols=1, figsize=(10, 8))\nax1.set_title('Original Distributions')\n\nsns.kdeplot(df['beta'], ax=ax1)\nsns.kdeplot(df['exponential'], ax=ax1)\nsns.kdeplot(df['normal_p'], ax=ax1)\nsns.kdeplot(df['normal_l'], ax=ax1)\nsns.kdeplot(df['bimodal'], ax=ax1);","806129eb":"df.head()","da9ade00":"df.mean()","352bd403":"df.describe()","ebf669f9":"df.plot()","a24bbfbf":"normal_big = np.random.normal(1000000, 10000, (1000,1))  # normal distribution of large values\ndf['normal_big'] = normal_big","285163c2":"col_names.append('normal_big')","084f9c28":"df['normal_big'].plot(kind='kde')","a6aafb20":"df.normal_big.mean()","2dc9272a":"# plot original distribution plot with larger value feature\nfig, (ax1) = plt.subplots(ncols=1, figsize=(10, 8))\nax1.set_title('Original Distributions')\n\nsns.kdeplot(df['beta'], ax=ax1)\nsns.kdeplot(df['exponential'], ax=ax1)\nsns.kdeplot(df['normal_p'], ax=ax1)\nsns.kdeplot(df['normal_l'], ax=ax1)\nsns.kdeplot(df['bimodal'], ax=ax1);\nsns.kdeplot(df['normal_big'], ax=ax1);","128fd8f0":"df.plot()","b163e0ef":"df.describe()","f29c6d41":"mm_scaler = preprocessing.MinMaxScaler()\ndf_mm = mm_scaler.fit_transform(df)\nprint(col_names)\ndf_mm = pd.DataFrame(df_mm, columns=col_names)\n\nfig, (ax1) = plt.subplots(ncols=1, figsize=(10, 8))\nax1.set_title('After MinMaxScaler')\n\nsns.kdeplot(df_mm['beta'], ax=ax1)\nsns.kdeplot(df_mm['exponential'], ax=ax1)\nsns.kdeplot(df_mm['normal_p'], ax=ax1)\nsns.kdeplot(df_mm['normal_l'], ax=ax1)\nsns.kdeplot(df_mm['bimodal'], ax=ax1)\nsns.kdeplot(df_mm['normal_big'], ax=ax1);","464827d2":"df_mm['beta'].min()","2328df9a":"df_mm['beta'].max()","b68442ad":"mins = [df[col].min() for col in df.columns]\nmins","f5f6114e":"maxs = [df[col].max() for col in df.columns]\nmaxs","e1c36862":"mins = [df_mm[col].min() for col in df_mm.columns]\nmins","a14e7e3c":"maxs = [df_mm[col].max() for col in df_mm.columns]\nmaxs","78d4d8f9":"r_scaler = preprocessing.RobustScaler()\ndf_r = r_scaler.fit_transform(df)\n\ndf_r = pd.DataFrame(df_r, columns=col_names)\n\nfig, (ax1) = plt.subplots(ncols=1, figsize=(10, 8))\nax1.set_title('After RobustScaler')\n\nsns.kdeplot(df_r['beta'], ax=ax1)\nsns.kdeplot(df_r['exponential'], ax=ax1)\nsns.kdeplot(df_r['normal_p'], ax=ax1)\nsns.kdeplot(df_r['normal_l'], ax=ax1)\nsns.kdeplot(df_r['bimodal'], ax=ax1)\nsns.kdeplot(df_r['normal_big'], ax=ax1);","515bfe74":"mins = [df_r[col].min() for col in df_r.columns]\nmins","bc1a31d5":"maxs = [df_r[col].max() for col in df_r.columns]\nmaxs","69e7fc56":"s_scaler = preprocessing.StandardScaler()\ndf_s = s_scaler.fit_transform(df)\n\ndf_s = pd.DataFrame(df_s, columns=col_names)\n\nfig, (ax1) = plt.subplots(ncols=1, figsize=(10, 8))\nax1.set_title('After StandardScaler')\n\nsns.kdeplot(df_s['beta'], ax=ax1)\nsns.kdeplot(df_s['exponential'], ax=ax1)\nsns.kdeplot(df_s['normal_p'], ax=ax1)\nsns.kdeplot(df_s['normal_l'], ax=ax1)\nsns.kdeplot(df_s['bimodal'], ax=ax1)\nsns.kdeplot(df_s['normal_big'], ax=ax1);","814fc7cd":"mins = [df_s[col].min() for col in df_s.columns]\nmins","c617d2e3":"maxs = [df_s[col].max() for col in df_s.columns]\nmaxs","f1c148c5":"n_scaler = preprocessing.Normalizer()\ndf_n = n_scaler.fit_transform(df)\n\ndf_n = pd.DataFrame(df_n, columns=col_names)\n\nfig, (ax1) = plt.subplots(ncols=1, figsize=(10, 8))\nax1.set_title('After Normalizer')\n\nsns.kdeplot(df_n['beta'], ax=ax1)\nsns.kdeplot(df_n['exponential'], ax=ax1)\nsns.kdeplot(df_n['normal_p'], ax=ax1)\nsns.kdeplot(df_n['normal_l'], ax=ax1)\nsns.kdeplot(df_n['bimodal'], ax=ax1)\nsns.kdeplot(df_n['normal_big'], ax=ax1);","756ae7d9":"mins = [df_n[col].min() for col in df_n.columns]\nmins","792fa45c":"maxs = [df_n[col].max() for col in df_n.columns]\nmaxs","88862f65":"# Combined plot.\n\nfig, (ax0, ax1, ax2, ax3) = plt.subplots(ncols=4, figsize=(20, 8))\n\n\nax0.set_title('Original Distributions')\n\nsns.kdeplot(df['beta'], ax=ax0)\nsns.kdeplot(df['exponential'], ax=ax0)\nsns.kdeplot(df['normal_p'], ax=ax0)\nsns.kdeplot(df['normal_l'], ax=ax0)\nsns.kdeplot(df['bimodal'], ax=ax0)\nsns.kdeplot(df['normal_big'], ax=ax0);\n\n\nax1.set_title('After MinMaxScaler')\n\nsns.kdeplot(df_mm['beta'], ax=ax1)\nsns.kdeplot(df_mm['exponential'], ax=ax1)\nsns.kdeplot(df_mm['normal_p'], ax=ax1)\nsns.kdeplot(df_mm['normal_l'], ax=ax1)\nsns.kdeplot(df_mm['bimodal'], ax=ax1)\nsns.kdeplot(df_mm['normal_big'], ax=ax1);\n\n\nax2.set_title('After RobustScaler')\n\nsns.kdeplot(df_r['beta'], ax=ax2)\nsns.kdeplot(df_r['exponential'], ax=ax2)\nsns.kdeplot(df_r['normal_p'], ax=ax2)\nsns.kdeplot(df_r['normal_l'], ax=ax2)\nsns.kdeplot(df_r['bimodal'], ax=ax2)\nsns.kdeplot(df_r['normal_big'], ax=ax2);\n\n\nax3.set_title('After StandardScaler')\n\nsns.kdeplot(df_s['beta'], ax=ax3)\nsns.kdeplot(df_s['exponential'], ax=ax3)\nsns.kdeplot(df_s['normal_p'], ax=ax3)\nsns.kdeplot(df_s['normal_l'], ax=ax3)\nsns.kdeplot(df_s['bimodal'], ax=ax3)\nsns.kdeplot(df_s['normal_big'], ax=ax3);","1f27ee64":"The new, high-value distribution is way to the right. And here's a plot of the values.","13efae73":"## Please upvote if you find this Kernel helpful :)","a91aa53a":"Let's look at our original and transformed distributions together. We'll exclude Normalizer because you generally want to tranform your features, not your samples.","e7100e61":"You can see that after any transformation the distributions are on a similar scale. Also notice that MinMaxScaler doesn't distort the distances between the values in each feature.","2ca389f4":"Let's look at the minimums and maximums for each column prior to scaling.","c6785bcf":"Let's make several types of random distributions.","5602dcfe":"I hope you found this Kernel to be a helpful introduction to Scaling, Standardizing, and Normalizing with scikit-learn. Please upvote it if you found it helpful!","42b257c2":"Let's check the minimums and maximums for each column after RobustScaler.","3f9fc0d1":"Let's check the minimums and maximums for each column after MinMaxScaler.","7c128c6d":"Let's see what are the means are.","29542b54":"We've got a normalish distribution with a mean near 1,000,0000.","bfe5c537":"## TLDR\n\n* Use MinMaxScaler as your default\n* Use RobustScaler if you have outliers and can handle a larger range\n* Use StandardScaler if you need normalized features\n* Use Normalizer sparingly - it normalizes rows, not columns","9a3d14b2":"Let's plot our original distributions.","c5f60252":"Notice how the shape of each distribution remains the same, but now the values are between 0 and 1.","e2a1ffe2":"# Combined Plot","dd07759c":"This feature could be home prices, for example.","27a3a0eb":"If you'd like more summary statistics:","61f74dca":"# Scale, Standardize, or Normalize with scikit-learn\n## When to use MinMaxScaler, RobustScaler, StandardScaler, and Normalizer\n## By Jeff Hale","de0f53c6":"Normalizer also moved the features to similar scales. Notice that the range for our much larger feature's values is now extremely small and clustered around .9999999999. ","e4b79ade":"MinMaxScaler subtracts the column mean from each value and then divides by the range.","720928a3":"Let's check the minimums and maximums for each column after scaling.","beac2e23":"# Normalizer\n\nNote that normalizer operates on the rows, not the columns. It applies l2 normalization by default.","5b363a5c":"## Add a feature with much larger values","0e1dea29":"Qutie a nice chart, don't you think? You can see that all features now have 0 mean.","8b9a690e":"Let's check the minimums and maximums for each column after StandardScaler.","4be9f5c7":"StandardScaler is scales each column to have 0 mean and unit variance.","ca5474b3":"# StandardScaler","5f727a61":"# RobustScaler","c6ea4f91":"# Original Distributions","8323128a":"# Why scale, standardize, or normalize?\n\nMany machine learning algorithms, such as neural networks, regression-based algorithms, K-nearest neighbors, support vector machines with radial bias kernel functions, principal components analysis, and algorithms using linear discriminant analysis don't perform as well if the features are not on relatively similar scales. \n\nSometimes you'll want a more normally distributed distribution. \n\nSome of the methods below dilute the effects of outliers. ","e0db8bae":"The ranges are fairly similar to RobustScaler. \n\nNow let's look at Normalizer.","40f0fe37":"RobustScaler subtracts the column median and divides by the interquartile range.","70a4073c":"Looks close enough to 0 to 1 intervals to me. Our feature with much larger values was brought into scale with our other features. \n\nNow let's look at RobustScaler.","a7ab5ea0":"Here's a [cheat sheet I made in a google sheet](https:\/\/docs.google.com\/spreadsheets\/d\/1woVi7wq13628HJ-tN6ApaRGVZ85OdmHsDBKLAf5ylaQ\/edit?usp=sharing) to help folks keep the options straight. \n\nLet's set things up and start making some distributions!","f3d96eae":"If we put this on the same plot as the original distributions, you can't even see the earlier columns.","cd81c556":"# MinMaxScaler ","92cca20d":"Now let's look at StandardScaler.","dc5dc287":"Although the range of values for each feature is much smaller than for the original features, it's larger and varies more than for MinMaxScaler. The bimodal distribution values are now compressed into two small groups.","d464285c":"These values are all in the same ballpark.","ba4d6bc2":"Now let's see what happens when we do some scaling. Let's apply MinMax Scaler first."}}