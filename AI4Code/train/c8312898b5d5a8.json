{"cell_type":{"172b5017":"code","5a1fa7f4":"code","7f77ad83":"code","41e81e3f":"code","d6f69cd4":"code","ee083381":"code","7b921664":"code","b20f0eac":"code","6e83b7d7":"code","e99898f4":"code","2133b5c7":"code","42762f45":"code","eb797eb2":"code","6189f4cb":"code","43f2ec08":"code","5b3fe7d7":"code","db0c4697":"code","eb9d65f5":"code","935a5228":"code","65012f4e":"code","185ab535":"code","ada8f397":"code","f8b11c00":"code","de65b8c7":"code","fccd628e":"code","718bcedd":"code","6ac2cd3c":"markdown","a305db39":"markdown","d4489491":"markdown","8fc5870b":"markdown"},"source":{"172b5017":"# EDA and Profiling the data with pandas_profiler","5a1fa7f4":"#Load Libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom pandas_profiling import ProfileReport","7f77ad83":"#Raw data from the first file\ndata = pd.read_csv('..\/input\/unsw-nb15\/UNSW-NB15_1.csv')","41e81e3f":"#quick info about the data\ndata.info","d6f69cd4":"#that was hard to read....here is a prettier version, but missing some details in the middle\ndata.head(5)","ee083381":"#Well the data has no headers....we can find what they 'should be' via the features file.\n#then lets reload the data\n# and we'll make two copies of it in case we want to experiment later\ndata2 = data = pd.read_csv('..\/input\/unsw-nb15\/UNSW-NB15_1.csv', header = None, names = ['srcip','sport','dstip','dsport','proto','state','dur','sbytes','dbytes','sttl','dttl','sloss','dloss','service','Sload','Dload','Spkts','Dpkts','swin','dwin','stcpb','dtcpb','smeansz','dmeansz','trans_depth','res_bdy_len','Sjit','Djit','Stime','Ltime','Sintpkt','Dintpkt','tcprtt','synack','ackdat','is_sm_ips_ports','ct_state_ttl','ct_flw_http_mthd','is_ftp_login','ct_ftp_cmd','ct_srv_src','ct_srv_dst','ct_dst_ltm','ct_src_ ltm','ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','attack_cat','Label'])","7b921664":"df = pd.DataFrame(data)","b20f0eac":"data.head(5)","6e83b7d7":"#What were the column names again?\ndf.columns","e99898f4":"#This is a noisy data set.  Traditional netflow is a lot simpler.  \n#Let's make a smaller data set....if you don't know what these are...check out the data dictionary '....features.csv'\n\nfeatures = df[[\"sport\",\"dsport\",\"proto\",\"Dpkts\", \"Spkts\",\"Label\"]]","2133b5c7":"features.head(5)","42762f45":"features.dtypes","eb797eb2":"features['sport'] = pd.to_numeric(features['sport'], errors='coerce')","6189f4cb":"#well shoot....now the coorect way is to do a .loc, but lets try a quicker route...\n#copy\/paste.  Note this is the the 'correct' way, but it works for now\nfeatures2=features.copy()","43f2ec08":"#Machines read numbers, so let's convert to numbers\n#BTW when ran the first time sport and dsport were 'rejected', so this is a 'must'.\nfeatures2['sport'] = pd.to_numeric(features['sport'], errors='coerce')\nfeatures2['dsport'] = pd.to_numeric(features['sport'], errors='coerce')","5b3fe7d7":"#We can also do label encoding...\n#Label encoding per: https:\/\/www.datacamp.com\/community\/tutorials\/categorical-data\nfrom sklearn.preprocessing import LabelEncoder\n\nlb_make = LabelEncoder()\nfeatures2['proto'] = lb_make.fit_transform(features2['proto'])\n","db0c4697":"#run the profiler\n#https:\/\/github.com\/pandas-profiling\/pandas-profiling\nprofile = ProfileReport(features2, title = \"Features to Evaluate Data Profile\")","eb9d65f5":"#Let's look at the data from within the notebook\nprofile.to_notebook_iframe()","935a5228":"features2.isnull().sum()","65012f4e":"#drop all rows with null values\n#make a new variable so you can trace back your work when troubleshooting\nfeatures3 = features2.dropna(how='any',axis=0) ","185ab535":"#Validate this has been corrected\nfeatures3.isnull().sum()\n","ada8f397":"from sklearn import model_selection\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import MinMaxScaler","f8b11c00":"# Inspired by https:\/\/www.datacamp.com\/community\/tutorials\/ensemble-learning-python\n#Lets scale the data so all columns are relative to one another\n\nscaler = MinMaxScaler(feature_range=(0, 1))\nnormalizedData = scaler.fit_transform(features3)","de65b8c7":"print(normalizedData)","fccd628e":"#input variables = X and Y = Label.  There are 6 variables, and python starts at 0 \nX = normalizedData[:,0:5]\nY = normalizedData[:,5]","718bcedd":"# 10-fold cross-validation fold, then using decision tree clasifier with 100 trees\nkfold = model_selection.KFold(n_splits=10, random_state=7)\ncart = DecisionTreeClassifier()\nnum_trees = 100\nmodel = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=7)\nresults = model_selection.cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())","6ac2cd3c":"## **Data Prep**","a305db39":"## Data has now been profiled, cleaned, and preprocessed.  Time for actual analysis and machine learning","d4489491":"## As you can see there is a 97% overall accuracy score.  This is a problem when at scale of 100B connections, however its a great start\/filter or a great secondary back up to analyze those connections that passed through the perimeter\/firewall as a secondary check.","8fc5870b":"## Well...that's a pretty powerful profiling tool and you can interact with it dynamically!\n## Ughhh...more 'data janitor' work.....\n* sport and dsport are missing values.\n* sport and dsport are highly correlated.  Leave for now, but may need to clean up later.  \n\n## Notes....\n* ports can go up to 65K+ and the data gets close to that number, but less than 10% is unique\n* ports should never be '0'.  This is a possible hacker technique\n* ports  histograms are fairly normal, and that's wierd.  Most traffic is under the 1024 well known ports.  Anything  above that is 'wild west', so having a relatively even distribution is super wierd.  Might be  a lot of 'port scanning' in the data set\n\n### There is a lot you can do here already using statistical inference.  ML is not required to spot anomolies or identify attacks."}}