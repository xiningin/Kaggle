{"cell_type":{"fdcb7293":"code","64325b12":"code","6ec81ca9":"code","8faaeb26":"code","b2d31c4f":"code","e45a6223":"code","1786a0f9":"code","c1ef2653":"code","c09d16f1":"code","adf627c4":"code","f1f4dce6":"code","911cd0da":"code","03f1c5da":"code","9d7c6300":"code","e78e4430":"code","95eeba33":"code","fc17dac8":"code","458e3c7b":"code","325e6233":"code","ead2606e":"code","b5555f70":"code","eba1ee71":"code","47f24ccc":"code","23f7000a":"code","762f112d":"code","fe29c696":"code","d8ac1faf":"code","7d0abace":"code","f2125862":"code","45fd0710":"code","6fb696c6":"markdown","df17e2c8":"markdown","4e509369":"markdown","3d682e27":"markdown","601cd313":"markdown","defe44ab":"markdown","31c26d13":"markdown","46d3255b":"markdown","0f9626b9":"markdown","2517dc07":"markdown","8da53271":"markdown","22213823":"markdown","bec3beba":"markdown","44d610ad":"markdown","7db6afcc":"markdown","adf0fcf8":"markdown","8e3e8765":"markdown","5f396df5":"markdown","a0ac8d8c":"markdown","808dbad4":"markdown","ce6b7ab7":"markdown","cab47aa0":"markdown","45cd1c89":"markdown","16c6cfa1":"markdown","9ff33be9":"markdown","9143b545":"markdown","f9bf21da":"markdown","065c701a":"markdown","a7029782":"markdown"},"source":{"fdcb7293":"import warnings, os, math\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nfrom scipy.stats import norm","64325b12":"train=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain.head(5)","6ec81ca9":"recordId='Id'\ntarget='SalePrice'\ntrainId=train[recordId]\ntestId=test[recordId]\n\n# Dropping \"Id\" column from train and test set\ntrain.drop(recordId,axis=1,inplace=True)\ntest.drop(recordId,axis=1,inplace=True)\n\n# Checking Dataset shape\nprint('Train Set\\t %d X %d'%(train.shape[0],train.shape[1]))\nprint('Test Set\\t %d X %d'%(test.shape[0],test.shape[1]))","8faaeb26":"numericalFeatures=train.select_dtypes(include=[np.number]).columns\nnrows=6\nncols=int(len(numericalFeatures)\/nrows)\nfig,ax=plt.subplots(nrows=nrows,ncols=ncols,figsize=(20,20),sharey=True)\nfig.subplots_adjust(wspace=0.2,hspace=0.4)\nfor row in range(nrows):\n    for col in range(ncols):\n        sns.scatterplot(train[numericalFeatures[row*ncols+col]],train[target],ax=ax[row,col])","b2d31c4f":"# Scatter plot of 'LotFrontage' VS 'SalePrice' has two outliers to the right. Therefore we can delete them.\ntrain.drop(index=train['LotFrontage'].sort_values(ascending=False)[:2].index,inplace=True)\nprint('Train Set\\t %d X %d'%(train.shape[0],train.shape[1]))","e45a6223":"nTrain=train.shape[0]\nnTest=test.shape[0]\ntrainY=train[target]\nallData=pd.concat((train,test)).reset_index(drop=True)\nallData.drop(target,axis=1,inplace=True)\nprint('Train + Test Set\\t %d X %d'%(allData.shape[0],allData.shape[1]))","1786a0f9":"count=allData.isnull().sum().sort_values(ascending=False)\npercentage=(allData.isnull().sum()\/allData.isnull().count()).sort_values(ascending=False)*100\ndtypes=allData[count.index].dtypes\nmissingData=pd.DataFrame({'Count':count,'Percentage':percentage,'Type':dtypes})\nmissingData.drop(missingData[missingData['Count']==0].index,inplace=True)\n\n# Plotting\nfig,ax=plt.subplots(figsize=(10,4))\nplt.xticks(rotation='90')\nsns.barplot(x=missingData.index,y=missingData['Percentage'])\nax.set_xlabel('Features')\nax.set_title('Percentage of missing values');\nmissingData.head(10)","c1ef2653":"indices=allData[(allData['PoolQC'].isnull())&(allData['PoolArea']>0)].index\nif len(indices)>0:\n    print(allData.loc[indices,['PoolQC','PoolArea','OverallQual']])\nmapper={0:'None',1:'Po',2:'Fa',3:'TA',4:'Gd',5:'Ex'}\nfor index in indices:\n    allData.loc[index,'PoolQC']=mapper[math.ceil(allData.loc[index,'OverallQual']\/2)]\nallData['PoolQC'].fillna('None',inplace=True)    ","c09d16f1":"# Check if all 159 NAs are the same observations among all 4 garage variables.\ncols=['GarageFinish','GarageQual','GarageCond','GarageYrBlt']\nlen(allData[allData['GarageFinish'].isnull() & allData['GarageQual'].isnull() & allData['GarageCond'].isnull() & allData['GarageYrBlt'].isnull()])==159\n# Check if all 157 NAs are same observations among 159 observations in GarageQual\nlen(allData[allData['GarageType'].isnull() & allData['GarageQual'].isnull()].index)==157\n# Getting observations where GarageType is not null but GarageQual is.\nindices=allData[(allData['GarageQual'].isnull())&allData['GarageType'].notnull()].index\nallData.loc[indices,['GarageType','GarageArea','GarageCars']+cols]","adf627c4":"for index in indices:\n    # If GarageArea is not null then replace other garage variables with mode.\n    if pd.isnull(allData.loc[index,'GarageArea'])==False:\n        for feature in cols:\n            allData.loc[index,feature]=allData[feature].mode()[0]\n    else:\n        allData.loc[index,'GarageType']='None'\n# Imputing rest of the 158 observations with None or 0 values\nfor feature in cols+['GarageCars','GarageArea']:\n    allData[feature].fillna(0,inplace=True)\nallData['GarageType'].fillna('None',inplace=True)","f1f4dce6":"# check if all 79 NAs are the same observations among the variables with 80+ NAs\ncols=['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\nlen(allData[allData['BsmtFinType1'].isnull() & allData['BsmtFinType2'].isnull() & allData['BsmtQual'].isnull() & allData['BsmtExposure'].isnull()& allData['BsmtCond'].isnull()])==79\nindices=allData[allData['BsmtFinType1'].notnull() & (allData['BsmtCond'].isnull() | allData['BsmtQual'].isnull() | allData['BsmtExposure'].isnull() | allData['BsmtFinType2'].isnull())].index\nallData.loc[indices,cols]","911cd0da":"for feature in cols:\n    # Imputing mode in basement variables to fix these 9 houses\n    allData.loc[indices,feature]=allData.loc[indices,feature].fillna(allData[feature].mode()[0])\n    # Imputing 'None' for the rest of the 79 observations\n    allData[feature].fillna('None',inplace=True)\nfor feature in ('BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath'):\n    allData[feature].fillna(0,inplace=True)","03f1c5da":"# Check for houses with Masonry but 'NA' values in MasVnrType variable.\nindices=allData[(allData['MasVnrType'].isnull())&(allData['MasVnrArea']>0)].index\nif len(indices)>0:\n    print(allData.loc[indices,['MasVnrType','MasVnrArea']])\n# Fixing MasVnrType by imputing mode\nallData.loc[indices,'MasVnrType']=allData.loc[indices,'MasVnrType'].fillna(allData['MasVnrType'].value_counts().index[1])\n# Imputing rest of the observations with 'None' and 0 values\nallData['MasVnrType'].fillna('None',inplace=True)\nallData['MasVnrArea'].fillna(0,inplace=True)","9d7c6300":"# Imputing missing values 'None' or Mode values\nallData['MiscFeature'].fillna('None',inplace=True)\nallData['Alley'].fillna('None',inplace=True)\nallData['Fence'].fillna('None',inplace=True)\nallData['FireplaceQu'].fillna('None',inplace=True)\n# Imputing missing values with the median of houses in that Neighborhood\nallData['LotFrontage']=allData.groupby('Neighborhood')['LotFrontage'].transform(lambda x:x.fillna(x.median()))\nallData['MSZoning'].fillna(allData['MSZoning'].mode()[0],inplace=True)\nallData['Functional'].fillna('Typ',inplace=True)\nallData.drop('Utilities',inplace=True,axis=1)\nallData['Exterior1st'].fillna(allData['Exterior1st'].mode()[0],inplace=True)\nallData['Exterior2nd'].fillna(allData['Exterior2nd'].mode()[0],inplace=True)\nallData['KitchenQual'].fillna(allData['KitchenQual'].mode()[0],inplace=True)\nallData['Electrical'].fillna(allData['Electrical'].mode()[0],inplace=True)\nallData['SaleType'].fillna(allData['SaleType'].mode()[0],inplace=True)\nprint('Total Missing Count\\t%d'%(allData.isnull().sum().max()))","e78e4430":"fig,ax=plt.subplots(nrows=1,ncols=2,figsize=(9,3))\nfig.subplots_adjust(wspace=0.2,hspace=0.4)\n\nsns.distplot(trainY,ax=ax[0],fit=norm)\nax[0].set_ylabel('Frequency')\nax[0].set_xlabel('Skew : %.3f'%(trainY.skew()))\nax[0].set_title('Original Distribution (Skewed)')\n\n# Log Transformation\ntrainY=np.log(trainY)\n\nsns.distplot(trainY,ax=ax[1],fit=norm)\nax[1].set_ylabel('Frequency')\nax[1].set_xlabel('Skew : %.3f'%(trainY.skew()))\nax[1].set_title('Normal Distribution')\nprint('Log Transformation : ')\nprint(\"(The target variable 'SalePrice' is right skewed. Log tansfromation is done to normalize the distribution.)\")","95eeba33":"fig,ax=plt.subplots(figsize=(13,10))\ncorrMat=train.corr()\nsns.heatmap(corrMat)","fc17dac8":"# Reducing Mutlicollinearity\nallData.drop(columns=['GarageYrBlt','1stFlrSF','TotRmsAbvGrd','GarageArea'],inplace=True)","458e3c7b":"cols=['YrSold','MoSold']\nfor feature in cols:\n    allData[feature]=allData[feature].astype('str')","325e6233":"# Converting all the ordinal variables in the dataset\nmapper={\n    'Alley':{'None':0,'Grvl':1,'Pave':2},\n    'BsmtCond':{'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5},\n    'BsmtExposure':{'None':0,'No':1,'Mn':2,'Av':3,'Gd':4},\n    'BsmtFinType1':{'None':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6},\n    'BsmtFinType2':{'None':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6},\n    'BsmtQual':{'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5},\n    'ExterCond':{'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5},\n    'ExterQual':{'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5},\n    'FireplaceQu':{'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5},\n    'Functional':{'Sal':0,'Sev':1,'Maj2':2,'Maj1':3,'Mod':4,'Min2':5,'Min1':6,'Typ':7},\n    'GarageCond':{'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5},\n    'GarageFinish':{'None':0,'Unf':1,'RFn':2,'Fin':3},\n    'GarageQual':{'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5},\n    'HeatingQC':{'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5},\n    'KitchenQual':{'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5},\n    'LandSlope':{'None':0,'Sev':1,'Mod':2,'Gtl':3},\n    'LotShape':{'None':0,'IR3':1,'IR2':2,'IR1':3,'Reg':4},\n    'PavedDrive':{'None':0,'N':1,'P':2,'Y':3},\n    'PoolQC':{'None':0,'Fa':1,'TA':2,'Gd':3,'Ex':4},\n    'Street':{'None':0,'Grvl':1,'Pave':2},\n    'Utilities':{'None':0,'ELO':1,'NoSeWa':2,'NoSewr':3,'AllPub': 4}\n}\nallData=allData.replace(mapper)\nprint('Train + Test Set\\t %d X %d'%(allData.shape[0],allData.shape[1]))\nallData.sample(5)","ead2606e":"skewness=allData[allData.select_dtypes(include=[np.number]).columns].skew().sort_values(ascending=False)\nprint('Skewness : ')\nprint(skewness.head(10))\nskewness=skewness[abs(skewness)>0.5]\nprint('\\nPerforming Log transformation on %d features...'%(skewness.shape[0]))\nfor feature in skewness.index:\n    allData[feature]=np.log1p(allData[feature])\nnumericalFeatures=allData.select_dtypes(include=[np.number]).columns\ncategoricalFeatures=allData.select_dtypes(include=[np.object]).columns","b5555f70":"allData=pd.get_dummies(allData)\nprint('Train + Test Set\\t %d X %d'%(allData.shape[0],allData.shape[1]))\nallData.sample(5)","eba1ee71":"trainX=allData[:nTrain]\ntestX=allData[nTrain:]","47f24ccc":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.preprocessing import StandardScaler\n\n# Splitting training set further into training and validation set\nsubTrainX,valX,subTrainY,valY=train_test_split(trainX,trainY,test_size=0.3,random_state=42)","23f7000a":"# Training\nlr=LinearRegression()\nlr.fit(subTrainX,subTrainY)\nlog_prediction=lr.predict(subTrainX)\nerror=np.sqrt(mean_squared_error(subTrainY,log_prediction))\nscore=r2_score(subTrainY,log_prediction)\nprint('Training Accuracy : %.2f%%\\t Error : %.4f'%(score*100,error))","762f112d":"# Cross Validation\nlog_prediction=lr.predict(valX)\nerror=np.sqrt(mean_squared_error(valY,log_prediction))\nscore=r2_score(valY,log_prediction)\nprint('Validation Accuracy : %.2f%%\\t Error : %.4f'%(score*100,error))","fe29c696":"# Training\nalphas=np.arange(0.01,10, 0.1)\nerrors=[]\nscores=[]\nrlr={}\nlog_prediction={}\n\nfor alpha in alphas:\n    rlr[alpha]=Ridge(alpha)\n    rlr[alpha].fit(subTrainX,subTrainY)\n    log_prediction[alpha]=rlr[alpha].predict(subTrainX)\n    error=np.sqrt(mean_squared_error(subTrainY,log_prediction[alpha]))\n    score=r2_score(subTrainY,log_prediction[alpha])\n    errors.append(error)\n    scores.append(score)\nalpha=alphas[errors.index(min(errors))]\nerror=np.sqrt(mean_squared_error(subTrainY,log_prediction[alpha]))\nscore=r2_score(subTrainY,log_prediction[alpha])\nprint('Alpha Chosen : %.4f'%alpha)\nprint('Training Accuracy : %.2f%%\\t Error : %.4f'%(score*100,error))\nsns.scatterplot(alphas,errors)","d8ac1faf":"# Cross Validation\nlog_prediction=rlr[alpha].predict(valX)\nerror=np.sqrt(mean_squared_error(valY,log_prediction))\nscore=r2_score(valY,log_prediction)\nprint('Validation Accuracy : %.2f%%\\t Error : %.4f'%(score*100,error))","7d0abace":"# Training over the entire trainset with optimal alpha\nrlr=Ridge(alpha)\nrlr.fit(trainX,trainY)\nlog_prediction=rlr.predict(trainX)\nerror=np.sqrt(mean_squared_error(trainY,log_prediction))\nscore=r2_score(trainY,log_prediction)\nprint('Training Accuracy : %.2f%%\\t Error : %.4f'%(score*100,error))","f2125862":"prediction=np.expm1(rlr.predict(testX))\nsubmission=pd.DataFrame()\nsubmission[recordId]=testId\nsubmission[target]=prediction\nsubmission.head()","45fd0710":"submission.to_csv('submission.csv',index=False)","6fb696c6":"### Independent Variables","df17e2c8":"#### Train Validation Split on Training Data (for Cross Validation)","4e509369":"#### Let's first concatenate the train and test set for handling missing data and feature engineering","3d682e27":"## Data Preprocessing","601cd313":"#### Converting some numerical variables that really categorical","defe44ab":"# House Prices Prediction\nAuthor - Rishabh Jain","31c26d13":"#### Multicollinearity","46d3255b":"## Feature Engineering","0f9626b9":"#### Splitting dataset back to training and test set","2517dc07":"##### **Other Variables**","8da53271":"### Note : \nThere are other outliers too in the training data. However removing all of them will effect our model badly, if there are outliers in test data as well. Hence, we will just manage to make models robust with them.","22213823":"From the above correlation heatmap several features are highly correlated. This is causing **multicollinearity**. We need to drop one of them which is less correlated to SalePrice.\n1. YearBuilt and GarageYrBlt. \n2. 1stFlrSF and TotalBsmtSF.\n3. GrLivArea and TotRmsAbvGrd.\n4. GarageCars and GarageArea. ","bec3beba":"**Linear Regression seems to be overfitting. As the training accuracy is close to 95% ,whereas validation accuracy is only 90.29%. We need to use a Regularized Linear Regression (i.e. Ridge Regression) in order to reduce the overfitting**","44d610ad":"### Ridge Regression","7db6afcc":"#### Converting Categorical variables into Dummy variables","adf0fcf8":"### Dependent Variable","8e3e8765":"### For Final Submission\n\n**Now, that we know L2 regularization (i.e. Ridge Regression) seems to reduce the overfitting and give better validation accuray, we will train the model with the ENTIRE TRAINING DATA for better predictions.**\n\n#### Training","5f396df5":"##### **MasVnrType and MasVnrArea**\n> All the houses with no masonry should have 'None' and 0 for MasVnrType and MasVnrArea variables respectively.\n\n> 1 house was found with masonry area greated than 0 and MasVnrType set to 'NaN' . Fixed it by imputing the mode of the variable.","a0ac8d8c":"#### Imputation\nLet's go one by one through all the features in order to impute the missing values.","808dbad4":"##### **Basement Variables**\n >There are 11 variables that relate to the Basement of a house. It seems as if there are 79 houses without a basement, because the basement variables of the other houses with missing values are all 80% complete (missing 1 out of 5 values). I am going to impute the modes to fix those 9 houses.","ce6b7ab7":"#### Log transformation of (highly) skewed features","cab47aa0":"##### **PoolQC and PoolArea**\n>Since there are no missing values in PoolArea variable, we can use this as the source of truth for the question whether the house has pool or not. If the area of pool is equal to 0, there is no pool in the house.\n\n>I found three houses where the area of  pool was greater than 0 , yet the PoolQC was set to 'NaN'. Since there is not much relation between PoolQC and PoolArea. we can fix these 3 missing values in PoolQC by imputing the values w.r.t OverallQual variable. And the rest of the observations in PoolQC variable can be imputed with 'None' because of the absence of Pool","45cd1c89":"##### **Garage Variables**\n>There are 7 variables related to garage. We need to make sure that all the houses wihout garage should contain 'None' for all categorical and 0 for all numerical variables.\n\n>1 house found with garage area greater than 0 and missing values for other garage related features. Fixed this by imputing imputing the mode for those features. The final datset has 158 houses with no garage.","16c6cfa1":"#### Final Prediction","9ff33be9":"## Modelling","9143b545":"### Linear Regression","f9bf21da":"### Outliers","065c701a":"#### Encoding some Ordinal Features.","a7029782":"### Missing Data"}}