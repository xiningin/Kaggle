{"cell_type":{"008cf7af":"code","e9bb9e75":"code","56178dc1":"code","6cc9c0d8":"code","83a217c1":"code","8bd4273e":"code","233b0139":"code","ca64719b":"code","4abb85a9":"code","8f962bd8":"code","02486737":"code","1c8ac7ba":"code","d5b2c0a6":"code","4700793b":"code","d53dcc50":"code","f3abead0":"code","d63f28a5":"code","a406df2f":"code","ced712a2":"code","518c490c":"code","ab21153a":"code","e1b036a5":"code","cbee3e82":"code","bce0b0cc":"code","ddf78fb8":"code","e534cf13":"code","d0182929":"code","e89750fe":"code","9f7a06e2":"code","5d20e086":"code","e6843ff6":"code","db41c84e":"code","c95fb15a":"code","e5d42a8d":"code","1ef5e3ae":"code","b56d19f4":"code","e74759e7":"code","5dcac822":"code","f43ce8f3":"markdown","2b9d0dc4":"markdown","479c5d81":"markdown","2e7ea49d":"markdown","aea0bd72":"markdown","cf8d4a4a":"markdown","d2678a2c":"markdown","708ab9a6":"markdown","fe2cc434":"markdown","b1b773d4":"markdown","05bb2328":"markdown","ec373f9a":"markdown","36c7ff21":"markdown","cf3ab5b7":"markdown","b12512f8":"markdown","e50c670c":"markdown","84f12ce4":"markdown","6b159609":"markdown","2e83ee81":"markdown","3bd87e75":"markdown","e724c0dd":"markdown","6ddb7da8":"markdown","4f8fe07a":"markdown","90b1e2bf":"markdown","017885f3":"markdown","2de486ab":"markdown","8420b12e":"markdown","c2fb7c65":"markdown","8bae1455":"markdown","56655b93":"markdown","09948463":"markdown","264f4823":"markdown","703d04e4":"markdown","ba310e3b":"markdown","8d9e6116":"markdown","e7581696":"markdown","73b8be0f":"markdown","a26a6088":"markdown","3157afe9":"markdown","03b56cd7":"markdown"},"source":{"008cf7af":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport pandas as pd\nimport datetime as dt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom yellowbrick.text import TSNEVisualizer\nfrom yellowbrick.datasets import load_hobbies\nfrom yellowbrick.text import FreqDistVisualizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('all')","e9bb9e75":"data=pd.read_csv('..\/input\/reddit-india-flair-detection\/datafinal.csv')","56178dc1":"data.shape","6cc9c0d8":"data.columns","83a217c1":"data.drop('id',axis=1,inplace=True)","8bd4273e":"data.drop('Unnamed: 0', axis=1, inplace= True )","233b0139":"data.shape","ca64719b":"data.head()","4abb85a9":"data.info()","8f962bd8":"data['flair'].value_counts()","02486737":"# converting to data frame\ndf=pd.DataFrame(data)","1c8ac7ba":"y=data.columns","d5b2c0a6":"x=data.count()","4700793b":"fig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nfeatures = y \nvalues = x\nax.bar(y,x)\nplt.xticks(rotation=90)\nplt.savefig('value.png', dpi=300, bbox_inches='tight')\nplt.show()","d53dcc50":"temp = df['title'].str.len()\ntemp.hist(bins = np.arange(0,200,1))\nplt.title(\"no of words in TITLE\")\nplt.xlabel(\"length of Title (in words)\", fontsize=12)\nplt.ylabel(\"Number of posts\", fontsize=12)\nplt.savefig('title.png', dpi=300, bbox_inches='tight')\nplt.show()","f3abead0":"\ntemp = df['body'].str.len()\ntemp.hist(bins = np.arange(0,1000,10))\nplt.title(\"no of words in BODY\")\nplt.xlabel(\"length of BODY (in words)\")\nplt.ylabel(\"Number of posts\")\nplt.savefig('lenth of words in body.png', dpi=300, bbox_inches='tight')\nplt.show()","d63f28a5":"fig,ax = plt.subplots()\nax.grid()\ndf.groupby('flair').plot(x='comms_num', y='score', ax=ax, legend=False)\nplt.savefig('comments and upvotes corresponding to the flairs', dpi=300, bbox_inches='tight')\nplt.show()","a406df2f":"df2 = df.groupby(\"flair\").mean()[['score']]\n\ndf2.plot(kind='bar', legend=False, grid=True)\nplt.title(\"Average score per flair\")\n\nplt.xlabel(\"Flair\")\nplt.ylabel(\"Average score per post\")\nplt.savefig('score per flair', dpi=300, bbox_inches='tight')\nplt.show()\n","ced712a2":"df3 = df.groupby(\"flair\").mean()[['comms_num']]\n\ndf3.plot(kind='bar', legend=False, grid=True)\nplt.title(\"Average no of coments per flair\")\n\nplt.xlabel(\"Flair\")\nplt.ylabel(\"Average no of comments per post\")\nplt.savefig('comms per flair', dpi=300, bbox_inches='tight')\nplt.show()\n","518c490c":"df4=df.set_index('timestamp')","ab21153a":"sns.set(rc={'figure.figsize':(20, 8)})\ndf4['comms_num'].plot(linewidth=1);\nplt.title(\"Number of comments per post during the timestamp\")\nplt.xlabel(\"Timestamp\")\nplt.ylabel(\"Number of comments\")\nplt.savefig('no of comments per time stamp.png')\nplt.show()","e1b036a5":"sns.set(rc={'figure.figsize':(20, 8)})\ndf4['score'].plot(linewidth=1);\nplt.title(\"score per post during the timestamp\")\nplt.xlabel(\"Timestamp\")\nplt.ylabel(\"score\")\nplt.savefig('score per post during time stamp.png')\nplt.show()","cbee3e82":"df1=df.set_index('timestamp')","bce0b0cc":"df2= df1.groupby('flair')[['comms_num']]\ndf2.head()","ddf78fb8":"sns.set(rc={'figure.figsize':(20, 8)})\ndf1.groupby('flair')[['comms_num']].plot(linewidth=1);\nplt.title(\"Number of comments per post during the timestamp corresponding to the flairs\")\nplt.xlabel(\"Timestamp\")\nplt.ylabel(\"Number of comments\")\nplt.savefig('no of comments per time stamp corresponding to the flairs.png')\nplt.show()","e534cf13":"z=df['flair'].unique()","d0182929":"df['body'].fillna(\" \",inplace=True)\n","e89750fe":"\ntfidf = TfidfVectorizer()\nx = tfidf.fit_transform(df['body'])\ny = df['flair']\ntsne = TSNEVisualizer(labels=z)\ntsne.fit(x, y)\ntsne.poof()","9f7a06e2":"df['comments'].fillna(\" \",inplace=True)","5d20e086":"\ntfidf = TfidfVectorizer()\nx = tfidf.fit_transform(df['comments'])\ny = df['flair']\ntsne = TSNEVisualizer(labels=z)\ntsne.fit(x, y)\ntsne.poof()","e6843ff6":"\ntfidf = TfidfVectorizer()\nx = tfidf.fit_transform(df['title'])\ny = df['flair']\ntsne = TSNEVisualizer(labels=z)\ntsne.fit(x, y)\ntsne.poof()","db41c84e":"\ndf['combined_features'].fillna(\" \",inplace=True)","c95fb15a":"\ntfidf = TfidfVectorizer()\nx = tfidf.fit_transform(df['combined_features'])\ny = df['flair']\ntsne = TSNEVisualizer(labels=z)\ntsne.fit(x, y)\ntsne.poof()","e5d42a8d":"stopwords = set(stopwords.words('english'))","1ef5e3ae":"vectorizer = CountVectorizer(stop_words=stopwords)\ndocs = vectorizer.fit_transform(df['body'])\nfeatures = vectorizer.get_feature_names()\nvisualizer = FreqDistVisualizer(features=features)\nvisualizer.fit(docs)\nvisualizer.poof()","b56d19f4":"vectorizer = CountVectorizer(stop_words=stopwords)\ndocs = vectorizer.fit_transform(df['title'])\nfeatures = vectorizer.get_feature_names()\nvisualizer = FreqDistVisualizer(features=features)\nvisualizer.fit(docs)\nvisualizer.poof()","e74759e7":"vectorizer = CountVectorizer(stop_words=stopwords)\ndocs = vectorizer.fit_transform(df['comments'])\nfeatures = vectorizer.get_feature_names()\nvisualizer = FreqDistVisualizer(features=features)\nvisualizer.fit(docs)\nvisualizer.poof()","5dcac822":"vectorizer = CountVectorizer(stop_words=stopwords)\ndocs = vectorizer.fit_transform(df['combined_features'])\nfeatures = vectorizer.get_feature_names()\nvisualizer = FreqDistVisualizer(features=features)\nvisualizer.fit(docs)\nvisualizer.poof()","f43ce8f3":"looking for the data shape ","2b9d0dc4":"for title ","479c5d81":"obtaining the first 5 rows of data","2e7ea49d":"## In this notebook we will explore the data set and look for as many insights as we can get","aea0bd72":"combined features","cf8d4a4a":"for comments\n\n","d2678a2c":"words appearing maximum times in body","708ab9a6":"## Let's look at how people post with time what is the frequency of differnt flairs","fe2cc434":"no of comments per post during the time stamp","b1b773d4":"combined features","05bb2328":"flairs vs upvotes","ec373f9a":"It's clearly obvious unnamed: 0 and id have no use in the data so dropping them \n","36c7ff21":"Number of comments per post during the timestamp corresponding to the flairs","cf3ab5b7":"this shows that Indians have a scientific temperament and also nterested in food, sports and politics ","b12512f8":"comments are scattered all around the vector space","e50c670c":"## Importing the required libraries for analysis and visualisation \n","84f12ce4":"in comments","6b159609":"by looking at all the features they are also scattered yet large clusters can be formed this shows combined features work fine after title thus can be used for model","2e83ee81":"in title along with india corona virus, modi, pm , science occurs the most","3bd87e75":"By this we can see words in body don't cluster much together according to the flair but politics is clustered around probably because mentioning bjp, congress and other parties and personnel","e724c0dd":"this shows body is not an important parameter ","6ddb7da8":"#### graphs which we can plot to get more insights of the data \n#### 1. plots of different feature values and which feature value has missing values.\n#### 2. NO of posts and the flair they belong that have body.\n#### 3. No of words in the body corresponding to the flairs- this will help us determine is body a good parameter or not for model.\n#### 4. score of the posts with comments\n#### 5. flair which gets maximum comments - this will help us in model \n#### 6. what are thw words or persons mentioned maximum times in comments\n#### 7, what is the frequency of posts and comments.\n#### 8. what are the embeddings of different words in body, title, comments and combined features","4f8fe07a":"## Let's see different word embeddings and how the word from the corpus cluster","90b1e2bf":"this shows title is an important parameter and would affect our model ","017885f3":"## getting no of words in title","2de486ab":"## loading the data","8420b12e":"score per post during the time stamp","c2fb7c65":"### by this we find that body has a lot of null values. This shows that for most of the posts only the title suffice, there is no need of body. \n### plots can be made for the posts having body and no of words in such posts this would give more insights to the data. \n### second in the list which has missing values is comments. This is quite resonable as not all the posts get comments, next thing which can be checked is what is the score of the post with comments and what flair are the posts which get maximum comments. ","8bae1455":"## now let's look at the last part of our analysis what is the frequency of different words in the comments, body, title etc and which is the word used the most","56655b93":"## getting no of words in body","09948463":"thus India, Indian and economy government occurs many times ","264f4823":"names of columns","703d04e4":"This tells us two thing:-\n###### First the data is mostly balanced \n###### second [R]eddiquette has minimum no of posts this is may be because not most of the people post in ediquette section or things concerning this","ba310e3b":"titles are clustered according to the flair thus title is an important parameter","8d9e6116":"there is much about religion in comments might be because of people in India are most inclined to religion etc","e7581696":"comments per flair","73b8be0f":"## visualising how the flairs fare according to the comments no and score\n###### score is calculated by subtracting no of downvotes fro no of upvotes","a26a6088":"one more insight which we can do is ","3157afe9":"this shows food gets a lot of upvotes probably because we indians are foodies XD.\nsports and science and technology get a lot of upvotes, same as poilitics this shows indians are least interested in major topics like corona virus, policy economy","03b56cd7":"in combined features as well india and religion are predominant"}}