{"cell_type":{"0f9f9195":"code","5003e8e3":"code","e2daa0ff":"code","b1e40d4c":"code","ca1d90fe":"code","532e7926":"code","811e9196":"code","dc90ec78":"code","86dc180b":"code","83964746":"code","606fb31e":"code","99afe40a":"markdown","2cbca4e1":"markdown","1bfae791":"markdown","5ec31939":"markdown","79c8c343":"markdown"},"source":{"0f9f9195":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5003e8e3":"SEQUENCE_LENGTH = 40\nSTEP_SIZE = 3\n\nTRAINING_CHARS = 1000000 # The number of characters to train the model on. Greater number = better generation\n\nEPOCHS = 15\nBATCH_SIZE = 256\n\nPROMPT = None           # If you change the prompt, you must re-train the model","e2daa0ff":"import random\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import LSTM, Activation, Dense, Dropout\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import RMSprop","b1e40d4c":"# Add the alphabet to ensure that all characters are saved in the\n# training data\nALPHABET = \"qwertyuioplkjhgfdsazxcvbnmQWERTYUIOPASDFGHJKLZXCVBNM\"\n\nwith open(\"\/kaggle\/input\/latin-text-corpus\/latincorpus.txt\", \"rb\") as file:\n    text = file.read().decode().lower() + str(PROMPT) + ALPHABET\n    \ntext = text[:TRAINING_CHARS]","ca1d90fe":"charSet = sorted(set(text))\ncharToIndex = dict((c, i) for i, c in enumerate(charSet))\nindexToChar = dict((i, c) for i, c in enumerate(charSet))\n\nsentences = []\nnextChars = []\n\nfor i in range(0, len(text) - SEQUENCE_LENGTH, STEP_SIZE):\n    sentences.append(text[i: i + SEQUENCE_LENGTH])\n    nextChars.append(text[i + SEQUENCE_LENGTH])\n\n\nX = np.zeros((len(sentences), SEQUENCE_LENGTH, len(charSet)), dtype=np.bool)\ny = np.zeros((len(sentences), len(charSet)), dtype=np.bool)\n\nfor i, sentence in enumerate(sentences):\n    for j,  char in enumerate(sentence):\n        X[i, j, charToIndex[char]] = 1\n    y[i, charToIndex[nextChars[i]]] = 1","532e7926":"model = Sequential([\n    LSTM(128, input_shape=(SEQUENCE_LENGTH, len(charSet)), return_sequences=True),\n    Dropout(0.2),\n    LSTM(64),\n    Dropout(0.2),\n    Dense(len(charSet)),\n    Activation(\"softmax\")\n])\n\nmodel.compile(\n    loss=\"categorical_crossentropy\",\n    optimizer=RMSprop()\n)","811e9196":"model.summary()","dc90ec78":"history = model.fit(\n    X, y,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_split=0.2\n)","86dc180b":"from datetime import datetime\nmodel.save(f\"\/kaggle\/working\/model {str(datetime.now().date()) + str(datetime.now().time())}.h5\")","83964746":"# Helper functions\ndef sample(preds, temp=1.0):\n    preds = np.asarray(preds).astype(\"float64\")\n    preds = np.log(preds) \/ temp\n    exp_preds = np.exp(preds)\n    preds = exp_preds \/ np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)\n\ndef textGen(length, temp):\n    startIndex = random.randint(0, len(text) - SEQUENCE_LENGTH -1)\n    generated = \"\"\n    if PROMPT is None:\n        sentence = text[startIndex: startIndex + SEQUENCE_LENGTH]\n    elif len(PROMPT) < SEQUENCE_LENGTH:\n        raise ValueError(f\"Please make PROMPT at length {SEQUENCE_LENGTH} in characters.\")\n    elif len(PROMPT) > SEQUENCE_LENGTH:\n        sentence = PROMPT[:SEQUENCE_LENGTH]\n    else: sentence = PROMPT\n    generated += sentence\n    \n    for i in range(0, length):\n        X = np.zeros((1, SEQUENCE_LENGTH, len(charSet)))\n        for j, char in enumerate(sentence):\n            X[0, j, charToIndex[char]] = 1\n            \n        predictions = model.predict(X, verbose=0)[0]\n        nextIndex = sample(predictions, temp)\n        nextChar = indexToChar[nextIndex]\n        \n        generated += nextChar\n        sentence = sentence[1:] + nextChar\n    \n    return generated","606fb31e":"print(textGen(500, 0.8))","99afe40a":"Training for specified number of epochs...","2cbca4e1":"These parameters, you can change. The *EPOCHS* can be made longer for a better model.","1bfae791":"Turn the data into something the model can process.","5ec31939":"### Finally, here is the generation!\nHere is the AI generated latin text. It presently seems to generate nonsense, but with more epochs it could be improved. Play around with the temperature to see what you get generated!","79c8c343":"# Latin Generator LSTM\nThis is an LSTM that trains on the large dataset from the **latin-text-corpus** Kaggle dataset. Since it is such a large dataset with a large LSTM model, I'd recommend enabling GPU usage to speed it up a lot.\n\nOriginally, I learnt about using LSTMs for text generation from [this Neural Nine tutorial](https:\/\/www.youtube.com\/watch?v=QM5XDc4NQJo) and have gone on to develop similar models. This code can be used on any `.txt` file and I hope you like it!"}}