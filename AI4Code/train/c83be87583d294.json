{"cell_type":{"f38cc75b":"code","cd122063":"code","5be59934":"code","46e467ca":"code","660b145f":"code","7074c2c1":"code","b716e61a":"code","675af140":"code","3ef105aa":"code","185c496a":"code","9ad19c15":"code","7a67d835":"code","b9f1fdb2":"code","45ab6697":"code","053996a2":"code","79189233":"code","7b45f9e7":"code","4c793d13":"code","e765792b":"code","51cd212c":"code","7de7ce96":"code","7c9b6f41":"code","d9579a0d":"code","6ca11d30":"code","9a7c0af5":"code","3f1b0e90":"code","72266e88":"code","151982cd":"code","404242cb":"code","af71f8da":"code","c97182f1":"code","00e84fc2":"code","9b898259":"code","5cc9e94a":"code","e4bf7126":"code","36ae2308":"code","26bb38fc":"code","c3e533f0":"code","ea6b01ff":"code","75e9d79a":"code","cb26d919":"code","b117b0ce":"code","11a1cdfd":"code","292e8744":"code","7ada1638":"code","46036f47":"code","b7acebb4":"code","cf61b51c":"code","2b9b648f":"code","7eac4afb":"code","e7a16512":"code","a8d9a403":"code","7fd5d2b9":"code","d0c815cf":"code","f3e6c183":"code","18dd437e":"code","aa9c7bfd":"code","b70901a9":"code","32aca5d7":"code","02000b5b":"code","531aac60":"code","2379a6a7":"code","4ad8f13d":"markdown","28a7ff4b":"markdown","594f8119":"markdown","37065e8e":"markdown","0b629771":"markdown","24aa0a55":"markdown","1c6148c3":"markdown","41fe84ab":"markdown","9be2e6ba":"markdown","ceff5a6f":"markdown","da21e19c":"markdown","574a496d":"markdown","66f30c7a":"markdown","38348cc7":"markdown","106e1715":"markdown","2c70a83b":"markdown","8dd3cc55":"markdown","dfda5359":"markdown","89334874":"markdown","7b55363f":"markdown","5d9caceb":"markdown","52c2bbdb":"markdown","2f13c2eb":"markdown","b0ac4a30":"markdown","e442e0a7":"markdown","22af0cc7":"markdown","b5e149a9":"markdown","9249d3e6":"markdown"},"source":{"f38cc75b":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","cd122063":"# Python imports\nimport os\n\n# Maths and data imports\nimport numpy as np\nimport pandas as pd\n\n# Plot imports\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# ML modeling imports\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_error as mse\nfrom xgboost import XGBRegressor","5be59934":"%matplotlib inline\nsns.set()","46e467ca":"import warnings\nwarnings.filterwarnings('ignore')","660b145f":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id')","7074c2c1":"train_df = train.copy()\ntest_df = test.copy()","b716e61a":"train_df.head()","675af140":"train_df.shape","3ef105aa":"train_df.info()","185c496a":"# MSSubClass, OverallQual and OverallCond and categorical variable, lets convert it into one.\ntrain_df['MSSubClass'] = train_df['MSSubClass'].astype('object')\ntrain_df['OverallQual'] = train_df['OverallQual'].astype('object')\ntrain_df['OverallCond'] = train_df['OverallCond'].astype('object')\n\ntest_df['MSSubClass'] = test_df['MSSubClass'].astype('object')\ntest_df['OverallQual'] = test_df['OverallQual'].astype('object')\ntest_df['OverallCond'] = test_df['OverallCond'].astype('object')","9ad19c15":"def get_var_dtype_list(df):\n    cat_cols = []\n    num_cols = []\n\n    for col in df.columns:\n        if df[col].dtypes == 'object':\n            cat_cols.append(col)\n        else:\n            num_cols.append(col)\n    return (cat_cols, num_cols)","7a67d835":"cat_cols_train, num_cols_train = get_var_dtype_list(train_df)","b9f1fdb2":"cat_cols_test, num_cols_test = get_var_dtype_list(test_df)","45ab6697":"train_df['SalePrice'] = np.log(train_df['SalePrice'])","053996a2":"fig, axs = plt.subplots(len(num_cols_train)\/\/5, 5, figsize=(30, 30))\nfor col, ax in zip(num_cols_train[:-1], axs.flatten()):\n    sns.distplot(train_df[col], ax=ax)\n    ax.set_title(col)\n    ax.set_xlabel('')\n    ax.set_ylabel('')\nplt.show()","79189233":"right_skewed = ['LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', \n                'LowQualFinSF', 'BsmtHalfBath', 'WoodDeckSF', 'OpenPorchSF', \n                'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']","7b45f9e7":"r_skew_desc = train_df[right_skewed].describe().T\nr_skew_desc['coef_of_var'] = r_skew_desc['std']\/r_skew_desc['mean']\n\nr_skew_desc","4c793d13":"r_skew_desc[r_skew_desc['coef_of_var']>3].T.columns","e765792b":"drop_skew_cols = ['BsmtFinSF2', 'LowQualFinSF', 'BsmtHalfBath', '3SsnPorch',\n                  'ScreenPorch', 'PoolArea', 'MiscVal']\n\ntrain_df.drop(drop_skew_cols, axis=1, inplace=True)\ntest_df.drop(drop_skew_cols, axis=1, inplace=True)","51cd212c":"train_df.columns","7de7ce96":"cat_cols_train, num_cols_train = get_var_dtype_list(train_df)\ncat_cols_test, num_cols_test = get_var_dtype_list(test_df)","7c9b6f41":"def get_missing_stats(df, col_list, threshold=0):\n    total = len(df)\n    for col in col_list:\n        null = df[col].isnull().sum()\n        if null > 0 and null\/total >= threshold:\n            print(col)\n            if df[col].dtypes == 'object':\n                print(df[col].value_counts())\n            print(f'Missing values: {null} of {total}')\n            print(f'Percent missing values: {round((null*100)\/total, 2)}%\\n')","d9579a0d":"get_missing_stats(train_df, num_cols_train)","6ca11d30":"get_missing_stats(test_df, num_cols_test)","9a7c0af5":"imputer = SimpleImputer(strategy='median')\ntrain_df[num_cols_train[:-1]] = imputer.fit_transform(train_df[num_cols_train[:-1]])\ntest_df[num_cols_test] = imputer.transform(test_df[num_cols_test])","3f1b0e90":"get_missing_stats(train_df, num_cols_train)","72266e88":"get_missing_stats(test_df, num_cols_test)","151982cd":"get_missing_stats(train_df, cat_cols_train)","404242cb":"get_missing_stats(test_df, cat_cols_test)","af71f8da":"fig, axs = plt.subplots(len(cat_cols_train)\/\/5, 5, figsize=(30, 40))\nfor col, ax in zip(cat_cols_train, axs.flatten()):\n    y = train_df[col].value_counts()\n    ax.bar(y.index, y.values)\n    ax.set_title(col)\n    ax.set_xlabel('')\n    ax.set_ylabel('')\n    for tick in ax.get_xticklabels():\n        tick.set_rotation(35)\nplt.show()","c97182f1":"get_missing_stats(train_df, cat_cols_train, 0.4)","00e84fc2":"get_missing_stats(test_df, cat_cols_test, 0.4)","9b898259":"train_df.drop(['PoolQC'], axis=1, inplace=True)\ntest_df.drop(['PoolQC'], axis=1, inplace=True)","5cc9e94a":"na_cols = ['Alley','FireplaceQu','Fence','MiscFeature']\n\nna_imputer = SimpleImputer(strategy='constant', fill_value='N.A')\ntrain_df[na_cols] = na_imputer.fit_transform(train_df[na_cols])\ntest_df[na_cols] = na_imputer.transform(test_df[na_cols])","e4bf7126":"cat_cols_train, num_cols_train = get_var_dtype_list(train_df)\ncat_cols_test, num_cols_test = get_var_dtype_list(test_df)","36ae2308":"mf_imputer = SimpleImputer(strategy='most_frequent')\ntrain_df[cat_cols_train] = mf_imputer.fit_transform(train_df[cat_cols_train])\ntest_df[cat_cols_test] = mf_imputer.transform(test_df[cat_cols_test])","26bb38fc":"get_missing_stats(train_df, cat_cols_train)","c3e533f0":"get_missing_stats(test_df, cat_cols_test)","ea6b01ff":"plt.figure(figsize=(20, 20))\nsns.heatmap(train_df.drop(['SalePrice'], axis=1).corr(), annot=True)\nplt.show()","75e9d79a":"cor = train_df.drop(['SalePrice'], axis=1).corr()\nfor i, col in enumerate(cor.columns):\n    for row in cor.index[i+1:]:\n        if col != row and cor[col][row] > 0.7:\n            print(f'({row}, {col}): {cor[col][row]}')","cb26d919":"scaler = StandardScaler()\ntrain_df[num_cols_train[:-1]] = scaler.fit_transform(train_df[num_cols_train[:-1]])\ntest_df[num_cols_test] = scaler.transform(test_df[num_cols_test])","b117b0ce":"cat_cols_train","11a1cdfd":"cat_cols_train, num_cols_train = get_var_dtype_list(train_df)\ncat_cols_test, num_cols_test = get_var_dtype_list(test_df)","292e8744":"train_df = pd.get_dummies(train_df, drop_first=True, columns=cat_cols_train)\ntest_df = pd.get_dummies(test_df, drop_first=True, columns=cat_cols_test)","7ada1638":"train_df.head()","46036f47":"# check if both train and test contain same columns\ntrain_df.columns","b7acebb4":"test_df.columns","cf61b51c":"compat_list = list(set(train_df.columns).intersection(test_df.columns))","2b9b648f":"len(compat_list)","7eac4afb":"train_X, y = train_df[compat_list], train_df['SalePrice']\ntest_X = test_df[compat_list]","e7a16512":"(train_X.columns == test_X.columns).sum()","a8d9a403":"train_x, valid_x, train_y, valid_y = train_test_split(train_X, y, test_size=0.2, random_state=42)","7fd5d2b9":"linear_regressor = LinearRegression()\nlinear_regressor.fit(train_x, train_y)\npreds = linear_regressor.predict(valid_x)\nmse(valid_y, preds, squared=False)","d0c815cf":"models = {\n    'RFR': RandomForestRegressor,\n    'ADR': AdaBoostRegressor,\n    'XGB': XGBRegressor\n}","f3e6c183":"def fit_model(name, model, train_ds, valid_ds):\n    X, y = train_ds\n    X_val, y_val = valid_ds\n    \n    model.fit(X, y)\n    y_hat = model.predict(X)\n    y_hat_val = model.predict(X_val)\n    \n    mse_ = mse(y, y_hat, squared=False)\n    mse_val = mse(y_val, y_hat_val, squared=False)\n    \n    print(f'Model: {name}, Train MSE: {mse_}, Val MSE: {mse_val}')","18dd437e":"n_est = [10, 25, 50, 100, 125]\nfor i in range(len(n_est)):\n    print(f'n_estimators: {n_est[i]}')\n    for name, model in models.items():\n        model = model(n_estimators=n_est[i])\n        fit_model(name, model, (train_x, train_y), (valid_x, valid_y))\n    print('-'*20)","aa9c7bfd":"abr = AdaBoostRegressor(n_estimators=25)\nxgr = XGBRegressor(n_estimators=25)\n\nabr.fit(train_x, train_y)\nxgr.fit(train_x, train_y)\n\np1 = abr.predict(valid_x)\np2 = xgr.predict(valid_x)\np3 = linear_regressor.predict(valid_x)\n\nm1 = mse(valid_y, p1, squared=False)\nm2 = mse(valid_y, p2, squared=False)\nm3 = mse(valid_y, p3, squared=False)\nm4 = mse(valid_y, (p1+p2)\/2, squared=False)\nm5 = mse(valid_y, (p1+p2+p3)\/3, squared=False)\n\nprint(f'Ensemble MSE: \\nABR: {m1}\\nXGR: {m2}\\nLinear: {m3}\\nABR+XGR: {m4}\\nABR+XGR+Linear: {m5}')","b70901a9":"pred_1 = linear_regressor.predict(test_X)\npred_2 = (abr.predict(test_X) + xgr.predict(test_X))\/2\npred_3 = (2*pred_2 + pred_1)\/3","32aca5d7":"def create_submission(preds, name='submission.csv'):\n    preds = np.expm1(preds)\n    submission = pd.DataFrame({'Id': test_X.index, 'SalePrice': preds})\n    submission.to_csv(name, index=False)\n    return submission","02000b5b":"create_submission(pred_1, 'submission1.csv')","531aac60":"create_submission(pred_2, 'submission2.csv')","2379a6a7":"create_submission(pred_3, 'submission3.csv')","4ad8f13d":"## Data Preprocessing","28a7ff4b":"Let's now seperate the categorical and numerical columns","594f8119":"### Let check and impute missing values in continuous variables","37065e8e":"## Import libraries","0b629771":"Let's try them all, \n* linear regressor\n* Ensemble of AdaBoost and XGBoost\n* Ensemble of all of the above","24aa0a55":"Let's update out list of numerical columns list.","1c6148c3":"**Conclusion**\n\n1. There is  large variation in the scale of each continuous variables, hence, we need to scale them.\n2. Features such as YearBuilt and GarageYrBlt are left skewed, indicating that more houses were build in the later years hence, more garages were also built then. So, we might want to leave such variable for outlier check.\n3. Exploratory varibales - 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'LowQualFinSF', 'BsmtHalfBath', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal' are heavily right skewed.\n\nLet's further explore these variables.","41fe84ab":"Some columns seet to have quite a number of missing values. Lets further analyse them using barplot.","9be2e6ba":"### Scale variables","ceff5a6f":"Let's just analyse the columns with missing frequency greater than 40%.","da21e19c":"### Check train and test for compatability","574a496d":"Finally, we have dealt with missing values. Now, lets check for multicolinearity among exploratory variables.","66f30c7a":"So, the missing values for numerical columns have disappeared. Now, lets hit the categorical columns.","38348cc7":"Let's take log of SalePrice as our evaluation metric is log RMS value.","106e1715":"Hmm...From the overwhelming plots above we can see that few varibles are highly correlated, and it makes sense for them to be correlated. Hence, for now we might want to keep them.","2c70a83b":"## References:\n\nhttps:\/\/www.kaggle.com\/ankitverma2010\/house-prices-prediction-beginner-to-advanced#Exploratory-Data-Analysis","8dd3cc55":"**Task**: It is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable. \n\n**Evaluation**: Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price.","dfda5359":"`n_estimators=25` seems like a sweet spot for `AdaBoostRegressor` and `XGBRegressor`. Lets ensemble te ensembles","89334874":"We can infer two cases from above:\n1. The houses did not have these features\n2. The houses did have these features but were not reported.\n\nHowever, if we think then it seems feasible to assume that all the houses surveyed did not have all the 80 features. So, for now we can impute the missing values (with freq. > 40%) with N.A, where as missing values of columns with freq < 40% with most frequent value.\n\nI think it will be safe to assume and hypothesise that maybe the localities in which the houses were surveyed did not have the above features, thus introducing selective bias in data collection. Although, we can not be certain of it.","7b55363f":"### Lets check distribution fo continuous columns","5d9caceb":"## Model","52c2bbdb":"## Train Test split","2f13c2eb":"It does not seem to be extreme case of missing values so, we can simply replace by the median (as the data seem to be highly variable and median is a robust metric) of training set in both the training and test set.","b0ac4a30":"We can see a huge variation in lot of features. For now we can drop the columns with coefficient of variation > 3.\n\nAs the number of missing values is zero or near to zero in most of the features we would like to further analyse them (maybe take log to reduce the right skewness) and embrace the variability.\n\nBut for now lets got with dropping them off, and compare the effects in the next iteration\/followup.","e442e0a7":"### One-hot encoding of categorical columns\n\nFor now, let's simply one hot encode the categorical columns.","22af0cc7":"## EDA\n\nIn EDA we will analyse data to see if there is any - \n1. Skewness in features\n2. Check for missing values and ouliers and fix them.\n3. Check the variability of different features and scale them.\n4. Check for multicolinearity among multiple exploratory variables (features).\n5. Check if response variable is correlated to any\/may exploratory variable(s).\n6. Analyse the target\/response variable.","b5e149a9":"This notebook has been created after studying [this notebook.](https:\/\/www.kaggle.com\/ankitverma2010\/house-prices-prediction-beginner-to-advanced#Exploratory-Data-Analysis)","9249d3e6":"### Now, lets check and impute missing values of categorical columns."}}