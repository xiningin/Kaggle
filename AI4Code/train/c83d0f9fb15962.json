{"cell_type":{"0b3ca4a3":"code","a54f9241":"code","a085bc17":"code","03f30e18":"code","9eadc5df":"code","adfcd997":"code","e044df75":"code","c75051b5":"code","946f0b34":"code","19903168":"code","8c64acb2":"code","0b70eda7":"code","3ce8b0e8":"code","8d6aa6f1":"code","0ce27969":"code","ce7170bb":"code","8813f40c":"code","29d5e982":"code","9a6749f5":"code","b51fd325":"code","23422924":"code","fa75a2bb":"code","8d1f358d":"code","d909498f":"code","a4650cba":"code","f9dfd8e9":"code","4f0e58ee":"code","6a0da46c":"code","df8ca20c":"code","3fa1f035":"code","08cd7bda":"code","2949adfa":"markdown","b14518c2":"markdown","2635db8c":"markdown","916c4b9d":"markdown","619ae0ab":"markdown","6e0a7a39":"markdown","6629dea7":"markdown"},"source":{"0b3ca4a3":"# from google.colab import files\nimport pandas as pd, numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom nltk.tokenize import TweetTokenizer\n\nfrom scipy.sparse import coo_matrix, hstack\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, datasets, linear_model, metrics","a54f9241":"!du -l ..\/input\/*","a085bc17":"#importing the dataset\ntrain=pd.read_csv(\"..\/input\/fraud-finance-messages\/train.csv\")\ntest=pd.read_csv(\"..\/input\/fraud-finance-messages\/test.csv\")\nsample_submit = pd.read_csv(\"..\/input\/fraud-finance-messages\/sample_submission.csv\")\nsample_submit.head()","03f30e18":"train.tail(10)","9eadc5df":"print(f'shape(train): {train.shape}')\nprint(f'shape(test): {test.shape}')\nprint(f'shape(subm): {sample_submit.shape}')","adfcd997":"train = train.rename(columns={'class' : 'target'})","e044df75":"print(train.target.unique())","c75051b5":"test.head(3)","946f0b34":"train['text'][0]","19903168":"train.text.str.len().hist()","8c64acb2":"tknzr = TweetTokenizer()","0b70eda7":"train.text = train.text.str.lower()\ntest.text = test.text.str.lower()","3ce8b0e8":"vec_word_ngram = TfidfVectorizer(ngram_range=(1,2), tokenizer=tknzr.tokenize, max_features = 60000)","8d6aa6f1":"vec_word_ngram.fit(pd.concat([train.text, test.text], axis=0))","0ce27969":"train_tf_idf_features_word_ngram = vec_word_ngram.transform(train.text)\ntest_tf_idf_features_word_ngram = vec_word_ngram.transform(test.text)","ce7170bb":"vectorizer_char_ngram = TfidfVectorizer(analyzer='char', ngram_range=(2,4), max_features=70000)","8813f40c":"vectorizer_char_ngram.fit(pd.concat([train.text, test.text], axis=0))","29d5e982":"train_tf_idf_features_char_ngram = vectorizer_char_ngram.transform(train.text)\ntest_tf_idf_features_char_ngram = vectorizer_char_ngram.transform(test.text)","9a6749f5":"train_tf_idf_features = hstack([train_tf_idf_features_char_ngram, train_tf_idf_features_word_ngram])\ntest_tf_idf_features = hstack([test_tf_idf_features_char_ngram, test_tf_idf_features_word_ngram])","b51fd325":"X = train_tf_idf_features\ny = train.target\nX_train, X_test_local, y_train, y_test_local = train_test_split(X, y, test_size=0.5)","23422924":"# log_regressor = linear_model.LogisticRegression(C=0.5, max_iter=200, n_jobs=-1, verbose=1)\nlog_regressor = linear_model.LogisticRegression(C=0.85, max_iter=200, n_jobs=-1, verbose=1)","fa75a2bb":"log_regressor.fit(X_train, y_train)","8d1f358d":"y_test_local_proba_pred_logreg = log_regressor.predict_proba(X_test_local)","d909498f":"print(metrics.log_loss(y_test_local, y_test_local_proba_pred_logreg))","a4650cba":"#####################log_regressor = linear_model.LogisticRegression(C=0.5, max_iter=200, n_jobs=-1, verbose=1)\n# vec_word_ngram = TfidfVectorizer(ngram_range=(1,2), tokenizer=tknzr.tokenize, max_features = 400000)                        --  0.11955960706306659\n# vec_word_ngram = TfidfVectorizer(ngram_range=(1,2), tokenizer=tknzr.tokenize)                                               --  0.12205241858654657\n# vec_word_ngram = TfidfVectorizer(ngram_range=(1,2), tokenizer=tknzr.tokenize, max_features = 800000)                        --  0.12139041341987097\n# vec_word_ngram = TfidfVectorizer(ngram_range=(1,2), tokenizer=tknzr.tokenize, max_features = 300000)                        --  0.11600822519925699\n# vec_word_ngram = TfidfVectorizer(ngram_range=(1,2), tokenizer=tknzr.tokenize, max_features = 100000)                        --  0.11349224865622509\n# vec_word_ngram = TfidfVectorizer(ngram_range=(1,2), tokenizer=tknzr.tokenize, max_features = 70000)                         --  0.1100284078627325\n# vec_word_ngram = TfidfVectorizer(ngram_range=(1,2), tokenizer=tknzr.tokenize, max_features = 50000)                         --  0.1206\n# vec_word_ngram = TfidfVectorizer(ngram_range=(1,2), tokenizer=tknzr.tokenize, max_features = 60000)                         --  0.1077888727389393\n\n#GridSearchCV: {'C': 0.85, 'penalty': 'l2', 'solver': 'lbfgs'}   &    test_size=0.5)\n#####################log_regressor = linear_model.LogisticRegression(C=0.85, max_iter=200, n_jobs=-1, verbose=1)\n# vec_word_ngram = TfidfVectorizer(ngram_range=(1,2), tokenizer=tknzr.tokenize,max_features = 60000)                          --  0.09609801923245374","f9dfd8e9":"log_regressor.fit(X, y)","4f0e58ee":"y_test_pred = log_regressor.predict_proba(test_tf_idf_features)","6a0da46c":"submission = pd.concat([pd.DataFrame({'url': test[\"url\"]}), pd.DataFrame({'Expected':y_test_pred.transpose()[1]})], axis=1)","df8ca20c":"submission.head(3)","3fa1f035":"# submission.to_csv(\"\/content\/drive\/My Drive\/ML_Pump_news\/submit_optimal_C.csv\", index=False)","08cd7bda":"# !head -n3 \"\/content\/drive\/My Drive\/ML_Pump_news\/submit_optimal_C.csv\"","2949adfa":"# \u043a\u043e\u0431\u043c\u0438\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043e\u0442 char ngram \u0438 \u043e\u0442 word ngram","b14518c2":"## Tokenizing + feature extraction based on word & char ngram","2635db8c":"### Basic description\n","916c4b9d":"# train\/test split","619ae0ab":"# Test Logistic regression","6e0a7a39":"# feature extraction based on char ngram","6629dea7":"# Submission "}}