{"cell_type":{"c07ab035":"code","f53353e1":"code","5dcbf4bf":"code","a4a9d130":"code","ee830c5e":"code","068ca900":"code","af736aa5":"code","4b2a81b6":"code","5cd03e0f":"code","ae11e323":"code","49e21b7f":"code","3f4d76a7":"code","79f0716f":"code","cc6477a4":"code","ae5b716b":"code","29f8736c":"code","ee83cdc2":"code","6d07d4c8":"code","b524fa31":"code","fb88539b":"code","448bc00e":"code","98e3cbb6":"code","1a5f273b":"code","3a0d925c":"code","28ce1682":"code","a18ff0e8":"code","a94c84fb":"code","0402fc89":"code","feaa7f90":"code","2c39fcac":"code","474a9474":"code","020aa4a7":"code","d3fdce29":"code","2ffdb8b8":"code","90290a34":"code","23576ba7":"code","8eb1a11b":"code","a53436fb":"code","dd193916":"code","6a239eba":"markdown","0dbfb1d7":"markdown","422de9a8":"markdown","3dedc27d":"markdown","565c0994":"markdown","79d560c1":"markdown"},"source":{"c07ab035":"import torch\nimport numpy as np\nimport pandas as pd\nfrom torch import nn\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader, Dataset\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom torch.functional import F\nimport spacy\nimport string\nimport re\nimport numpy as np\nfrom spacy.symbols import ORTH\nfrom collections import Counter\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence","f53353e1":"batch_size = 100\nembedding_size=50\nhidden_dim=100\nepochs=30\nlearning_rate=0.001","5dcbf4bf":"PATH = Path('..\/input\/quora-question-pairs\/quora-question-pairs\/')","a4a9d130":"list(PATH.iterdir())","ee830c5e":"train_path = PATH\/'train.csv'\nval_path = PATH\/'test.csv'\n","068ca900":"train = pd.read_csv(str(train_path))\ntrain,val = train_test_split(train,test_size=0.2)\ntest = pd.read_csv(str(val_path))","af736aa5":"train=train.sample(150_000)\nval=val.sample(50_000)","4b2a81b6":"train.dropna(inplace=True)\nval.dropna(inplace=True)\ntest.dropna(inplace=True)","5cd03e0f":"re_br = re.compile(r'<\\s*br\\s*\/?>', re.IGNORECASE)\ndef sub_br(x):return re_br.sub(\"\\n\", x)\nmy_tok = spacy.load('en')\ndef spacy_tok(x): \n\n    return [tok.text for tok in my_tok.tokenizer(sub_br(x))]\n\n#         #isnan\n#         return []","ae11e323":"train['question1']=train['question1'].apply(spacy_tok)\ntrain['question2']=train['question2'].apply(spacy_tok)","49e21b7f":"val['question1']=val['question1'].apply(spacy_tok)\nval['question2']=val['question2'].apply(spacy_tok)","3f4d76a7":"counts = Counter()\nfor question_words in train['question1']:\n    counts.update(question_words)\nfor question_words in train['question2']:\n    counts.update(question_words)\nfor question_words in val['question1']:\n    counts.update(question_words)\nfor question_words in val['question2']:\n    counts.update(question_words)","79f0716f":"len(counts)","cc6477a4":"for word in list(counts):\n    if counts[word] < 3:\n        del counts[word]","ae5b716b":"vocab2index = {\"\":0, \"UNK\":1}\nwords = [\"\", \"UNK\"]\nfor word in counts:\n    vocab2index[word] = len(words)\n    words.append(word)","29f8736c":"\n\n# note that spacy_tok takes a while run it just once\ndef encode_sentence(word_list, vocab2index=vocab2index, N=embedding_size, padding_start=False):\n    x = word_list\n    enc = np.zeros(N, dtype=np.int32)\n    enc1 = np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in x])\n    l = min(N, len(enc1))\n    if padding_start:\n        enc[:l] = enc1[:l]\n    else:\n        enc[N-l:] = enc1[:l]\n    return enc, l\n\n","ee83cdc2":"train['question1']=train['question1'].apply(encode_sentence)\ntrain['question2']=train['question2'].apply(encode_sentence)","6d07d4c8":"val['question1']=val['question1'].apply(encode_sentence)\nval['question2']=val['question2'].apply(encode_sentence)","b524fa31":"num_words=len(words)\nnum_words","fb88539b":"val.head()","448bc00e":"class Question_Dataset(Dataset):\n    def __init__(self,df,train):\n    \n        self.y = torch.Tensor(df['is_duplicate'].values)\n        self.x1 = df['question1']\n        self.x2 = df['question2']\n        \n        \n    def __getitem__(self,idx):\n        x1, s1 = self.x1.loc[idx]\n        x2, s2 = self.x2.loc[idx]\n        x1=torch.Tensor(x1)\n        x2=torch.Tensor(x2)\n        return({\"x1\":x1,'x2':x2,\"s1\":s1,'s2':s2,'y':self.y[idx]})\n    def __len__(self):\n        return len(self.y)","98e3cbb6":"train.reset_index(inplace=True)\nval.reset_index(inplace=True)","1a5f273b":"train_ds = Question_Dataset(train,train=True)\nval_ds = Question_Dataset(val,train=True)","3a0d925c":"class Questionnaire(nn.Module):\n    def __init__(self):\n        super(Questionnaire,self).__init__()\n        self.lstm = nn.LSTM(embedding_size, hidden_dim, batch_first=True).cuda()\n        self.embedding =nn.Embedding(num_words,embedding_size, padding_idx=0)\n        self.dropout = nn.Dropout(0.5)\n    def forward(self,x,s):\n        s, sort_index = torch.sort(s, 0,descending=True)\n        s = s.long().cpu().numpy().tolist()\n        x=self.embedding(x)\n        x=self.dropout(x)\n        x_pack = pack_padded_sequence(x.float(), list(s), batch_first=True)\n        out_pack, (ht, ct) = self.lstm(x_pack)\n        out=ht[-1]\n        return torch.zeros_like(out).scatter_(0, sort_index.unsqueeze(1).expand(-1,out.shape[1]), out)\n         ","28ce1682":"def val_metrics(model, valid_dl,eval_metric=F.nll_loss):\n    model.eval()\n    total = 0\n    sum_loss = 0\n    sum_loss2=0\n    correct = 0 \n    rand_int = np.random.randint(len(valid_dl),size=1)\n    \n    for i, input in enumerate(valid_dl):\n        if i in rand_int:\n            x1 = input['x1'].cuda().long()\n            x2 = input['x2'].cuda().long()\n            s1 = input['s1'].cuda().long()\n            y = input['y'].cuda().float()\n\n            s2 = input['s2'].cuda().long()\n            y_hat_1 = model(x1,s1)\n            y_hat_2 = model(x2,s2)\n            DISTANCE = torch.exp(-torch.abs(y_hat_2-y_hat_1).sum(-1))\n#             DISTANCE = DISTANCE.unsqueeze(1)\n#             DISTANCE = torch.cat([1-DISTANCE,DISTANCE],1).float()\n            xt1 = [words[int(x)] for x in x1[0]]\n            xt2 = [words[int(x)] for x in x2[0]]\n            loss = F.mse_loss(DISTANCE,y)\n\n            print('Sentence 1: ',' '.join(xt1))\n            print('Sentence 2:',' '.join(xt2))\n            print('Prediction:',str(float(DISTANCE[0])))\n            print('Actual:',str(float(y[0])))\n        x1 = input['x1'].cuda().long()\n        x2 = input['x2'].cuda().long()\n        s1 = input['s1'].cuda().long()\n        s2 = input['s2'].cuda().long()\n        y_hat_1 = model(x1,s1)\n        y_hat_2 = model(x2,s2)\n\n        DISTANCE = torch.exp(-torch.abs(y_hat_2-y_hat_1).sum(-1))\n        DISTANCE = DISTANCE.unsqueeze(1)\n        DISTANCE = torch.cat([1-DISTANCE,DISTANCE],1).float()\n\n        y = input['y'].cuda().long()\n\n        loss = eval_metric(DISTANCE,y)\n        batch=y.shape[0]\n\n        sum_loss += batch*(loss.item())\n        total += batch\n    print(\"Validation Log Loss: \", sum_loss\/total)\n    return sum_loss\/total","a18ff0e8":"def train_routine(model,train_ds,valid_ds,epochs,eval_metric=F.mse_loss):\n    model.train()\n    optimizer = torch.optim.Adam(model.parameters(),learning_rate)\n    train_dl = DataLoader(train_ds,batch_size,True)\n    val_dl = DataLoader(val_ds,batch_size,True)\n    valid_errors = []\n    train_errors = []\n    for epoch in range(epochs):\n        model.train()\n\n        sum_loss=0\n        total=0\n        for i, input in enumerate(train_dl):\n            optimizer.zero_grad()\n            x1 = input['x1'].cuda().long()\n            x2 = input['x2'].cuda().long()\n            s1 = input['s1'].cuda().long()\n            s2 = input['s2'].cuda().long()\n\n            y_hat_1 = model(x1,s1)\n            y_hat_2 = model(x2,s2)\n            DISTANCE = torch.exp(-torch.abs(y_hat_2-y_hat_1).sum(-1))\n#             DISTANCE = DISTANCE.unsqueeze(1)\n#             DISTANCE = torch.cat([1-DISTANCE,DISTANCE],1).float()\n            \n            y = input['y'].float().cuda()\n            \n            loss = eval_metric(DISTANCE,y)\n            loss.backward()\n            total+=y.shape[0]\n            sum_loss+=loss.item()\n            optimizer.step()\n        print(\"Training Mean Squared error: \", sum_loss\/total)\n        train_errors.append(sum_loss\/total)\n        valid_errors.append(val_metrics(model,val_dl))\n        print()\n    return train_errors, valid_errors","a94c84fb":"model = Questionnaire().cuda()","0402fc89":"model=model.cuda()","feaa7f90":"train_errors, val_errors = train_routine(model,train_ds,val_ds,epochs,eval_metric=F.mse_loss)","2c39fcac":"plt.plot(train_errors)\n\nplt.show()\n","474a9474":"plt.plot(val_errors)\nplt.show()","020aa4a7":"learning_rate=0.001\nnewtrain_errors, newval_errors = train_routine(model,train_ds,val_ds,epochs,eval_metric=F.mse_loss)","d3fdce29":"epochs=5\nlearning_rate=0.0001\nnewesttrain_errors, newestval_errors = train_routine(model,train_ds,val_ds,epochs)","2ffdb8b8":"plt.plot(train_errors+newtrain_errors+newesttrain_errors)","90290a34":"plt.plot(val_errors+newval_errors+newestval_errors)","23576ba7":"test['question1']=test['question1'].apply(spacy_tok)\ntest['question2']=test['question2'].apply(spacy_tok)\n","8eb1a11b":"testcounts = Counter()\nfor question_words in test['question1']:\n    counts.update(question_words)\nfor question_words in test['question2']:\n    counts.update(question_words)\nfor word in list(counts):\n    if counts[word] < 3:\n        del counts[word]\nvocab2index = {\"\":0, \"UNK\":1}\nwords = [\"\", \"UNK\"]\nfor word in counts:\n    vocab2index[word] = len(words)\n    words.append(word)\ntest['question1']=test['question1'].apply(encode_sentence)\ntest['question2']=test['question2'].apply(encode_sentence)\n","a53436fb":"\npredictions = []\nfor row in range(len(test)):\n    x1, s1 = test['question1'][row]\n    x2, s2 = test['question2'][row]\n    x1=torch.Tensor(x1)\n    y_hat_1 = model(x1,s1)\n    y_hat_2 = model(x2,s2)\n    prediction = torch.exp(-torch.abs(y_hat_2-y_hat_1).sum(-1))\n    predictions.append(prediction)","dd193916":"my_submission = pd.DataFrame({'test_id': np.array( range(len(predictions))), 'is_duplicate':np.array( predictions)})\nmy_submission.to_csv('submission.csv', index=False)","6a239eba":"## Tokens!","0dbfb1d7":"## Make Counter","422de9a8":"## Dataset","3dedc27d":"## Model","565c0994":"Delete rare words","79d560c1":"number of words for embeddings"}}