{"cell_type":{"afaed805":"code","c27afb1d":"code","f8da7228":"code","98db2a1a":"code","e6137ca9":"code","3130b8b6":"code","ec3befb4":"code","308ddcb8":"code","dfc62c48":"code","e2c529c4":"code","85599891":"markdown","32ea39b2":"markdown","1e82bf9e":"markdown"},"source":{"afaed805":"import random\nimport typing as t\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm","c27afb1d":"tqdm.pandas()","f8da7228":"train_2017_df = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv')\nvalid_df = pd.read_csv('\/kaggle\/input\/jt-combined\/valid.csv')","98db2a1a":"CLS_LIST = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']","e6137ca9":"def _build_readable_label(row: t.Dict[str, int]) -> str:\n    return ' '.join([cls for cls in CLS_LIST if row[cls]])\n\n\ndef _build_bitmap_label(row: t.Dict[str, int]) -> str:\n    return ' '.join([str(row[cls]) for cls in CLS_LIST])\n\n\ndef assign_label_to_comment(df: pd.DataFrame, labels_df: pd.DataFrame) -> pd.DataFrame:\n    result_row_list = []\n    for _, row in tqdm(df.iterrows(), total=len(df)):\n        less_toxic_comment_text = str(row['less_toxic'])\n        more_toxic_comment_text = str(row['more_toxic'])\n        less_toxic_label_row_candidate_df = labels_df[labels_df['comment_text'] == less_toxic_comment_text]\n        if len(less_toxic_label_row_candidate_df):\n            less_toxic_readable_label = _build_readable_label(less_toxic_label_row_candidate_df.iloc[0])\n            less_toxic_bitmap_label = _build_bitmap_label(less_toxic_label_row_candidate_df.iloc[0])\n        else:\n            less_toxic_readable_label = ''\n            less_toxic_bitmap_label = ''\n        more_toxic_label_row_candidate_df = labels_df[labels_df['comment_text'] == more_toxic_comment_text]\n        if len(more_toxic_label_row_candidate_df):\n            more_toxic_readable_label = _build_readable_label(more_toxic_label_row_candidate_df.iloc[0])\n            more_toxic_bitmap_label = _build_bitmap_label(more_toxic_label_row_candidate_df.iloc[0])\n        else:\n            more_toxic_readable_label = ''\n            more_toxic_bitmap_label = ''\n        result_row_list.append({\n            'less_toxic': less_toxic_comment_text,\n            'less_toxic_readable_label': less_toxic_readable_label,\n            'less_toxic_bitmap_label': less_toxic_bitmap_label,\n            'more_toxic': more_toxic_comment_text,\n            'more_toxic_readable_label': more_toxic_readable_label,\n            'more_toxic_bitmap_label': more_toxic_bitmap_label,\n        })\n    return pd.DataFrame(result_row_list)","3130b8b6":"valid_with_labels_df = assign_label_to_comment(df=valid_df, labels_df=train_2017_df)","ec3befb4":"analyze_cls_label_df = valid_with_labels_df[(valid_with_labels_df['less_toxic_readable_label'] != '') & (valid_with_labels_df['more_toxic_readable_label'] != '')]","308ddcb8":"len(analyze_cls_label_df[analyze_cls_label_df['less_toxic_bitmap_label'] == analyze_cls_label_df['more_toxic_bitmap_label']]) \/ len(analyze_cls_label_df)","dfc62c48":"def get_amb_label_pair_set(df: pd.DataFrame) -> t.Set[t.Tuple[str, str]]:\n    amb_label_pair_set = set()\n    for _, row in tqdm(df.iterrows(), total=len(df)):\n        less_toxic_bitmap_label, more_toxic_bitmap_label = str(row['less_toxic_bitmap_label']), str(row['more_toxic_bitmap_label'])\n        if less_toxic_bitmap_label == more_toxic_bitmap_label:\n            continue\n        if len(df[(df['less_toxic_bitmap_label'] == more_toxic_bitmap_label) & (df['more_toxic_bitmap_label'] == less_toxic_bitmap_label)]):\n            amb_label_pair_set.add((\n                min(less_toxic_bitmap_label, more_toxic_bitmap_label),\n                max(less_toxic_bitmap_label, more_toxic_bitmap_label),\n            ))\n    return amb_label_pair_set\n\n\ndef count_rows_with_bitmap_labels(df: pd.DataFrame, bitmap_label_set: t.Set[t.Tuple[str, str]]) -> int:\n    n = 0\n    for bitmap_left, bitmap_right in tqdm(bitmap_label_set):\n        n += len(df[\n            ((df['less_toxic_bitmap_label'] == bitmap_left) & (df['more_toxic_bitmap_label'] == bitmap_right)) |\n            ((df['less_toxic_bitmap_label'] == bitmap_right) & (df['more_toxic_bitmap_label'] == bitmap_left))\n        ])\n    return n","e2c529c4":"amb_label_pair_set = get_amb_label_pair_set(analyze_cls_label_df)\ncount_rows_with_bitmap_labels(analyze_cls_label_df, amb_label_pair_set) \/ len(analyze_cls_label_df)","85599891":"Are there any samples where `less_toxic` and `more_toxic` comments have the same labels?","32ea39b2":"Just to sum up, for the 19.78% of validation samples the toxicity labels from the https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge are identical. Moreover, for the 63% of the validation samples the labels are ambigous. So there is a question of how useful those toxicity labels are for the ranking task.","1e82bf9e":"Let's consider a pair of `(less_toxic_bitmap_label, more_toxic_bitmap_label)` ambiguous if there is at least 1 pair of comments in the validation set where a comment with `less_toxic_bitmap_label` is ranked as more toxic than a comment with `more_toxic_bitmap_label`. How many ambigous samples do we have?"}}