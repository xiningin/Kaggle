{"cell_type":{"70a04843":"code","96d15a0d":"code","889933af":"code","4fa02ae8":"code","3a5db5ff":"code","6dc69f13":"code","5bddf095":"markdown","ffc9f572":"markdown","dd5a920b":"markdown","1f833023":"markdown","122895af":"markdown","d1dcf0f7":"markdown"},"source":{"70a04843":"%matplotlib notebook\n%load_ext autoreload\n%autoreload 2","96d15a0d":"import torch\nimport torch.utils.data as torchdata\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport random\nimport plotly.offline as py\nimport plotly.graph_objs as go\n\n# put plotly in Jupyter mode\npy.init_notebook_mode(connected=True)","889933af":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.numInputs = 7     # max length of input string\n        self.numOutputs = 1    # single number out\n        self.numVocab = 13     # number of unique characters\n        self.numDimensions = 3 # each embedding has three numbers\n        self.numHidden = 10    # hidden fully connected layer\n                \n        self.embedding = nn.Embedding(self.numVocab, self.numDimensions)\n        self.lin1 = nn.Linear(self.numInputs * self.numDimensions, self.numHidden)\n        self.lin2 = nn.Linear(self.numHidden, self.numOutputs)\n\n    def forward(self, input):\n        x = self.embedding(input).view(-1, self.numInputs * self.numDimensions)\n        x = F.relu(self.lin1(x))\n        x = self.lin2(x)\n        return torch.squeeze(x)\n\ndef generateData(num):\n    train = []\n    for i in range(0, num):\n        a = random.randint(0, 100)\n        b = random.randint(-100-a, 100-a)\n        train.append((f'{a}{b:+}'.ljust(7), a+b))\n    return train","4fa02ae8":"vocab = list('0123456789-+ ')\nchar2index = {char: i for i, char in enumerate(vocab)}\n\ntrain = generateData(100000)\ntrainloader = torchdata.DataLoader(train, batch_size=100)\nnet = Net()\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(net.parameters(), lr=5e-4, momentum=0.9)\nembeddingFrames = []","3a5db5ff":"# loop over the dataset multiple times\nfor epoch in range(1): \n    \n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        \n        # save emdebbings for creating the animation later\n        embeddingFrames.append(net.embedding(torch.tensor(range(0, len(vocab)))))\n        \n        # prepare batch\n        sums, actuals = data\n        input = torch.tensor([[char2index[c] for c in sum] for sum in sums], dtype=torch.long)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(input)\n        actualsTensor = actuals.type(torch.FloatTensor)\n        loss = criterion(outputs, actualsTensor)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 200 == 199:    # print every 200 mini-batches\n            print(f'[{epoch + 1}, {i + 1}] loss: {running_loss \/ 200}')\n            running_loss = 0.0\n\nprint('Finished Training')","6dc69f13":"frames = []\nfor embeddings in embeddingFrames[0:300:5]:\n    x, y, z = torch.transpose(embeddings, 0, 1).tolist()\n    data = [go.Scatter3d(x=x,y=y,z=z, mode='text', text=vocab)]\n    frame = dict(data=data)\n    frames.append(frame)\n    \nfig = dict(data=frames[0]['data'], frames=frames)\npy.iplot(fig)","5bddf095":"## Model and data\n\nLet's define our model and a function to generate random sums.","ffc9f572":"## Train\n\nAnd train the network. This is really fast so no need for GPU. Run this cell again and again to keep training if you like.","dd5a920b":"## Analyse the embeddings\n\nAll that's left is to plot the embeddings. Once the animation completes you can rotate the chart to look around.","1f833023":"Now we create our data and network. No need to create a validation set since we're only interested in the embeddings.","122895af":"# Animating embedding space\n\nThis notebook creates a toy neural net that tries to learn how to add and subtract numbers. Input is strings of the form `12+34` and the target output is a float of the result, i.e. `46.0`. Each character is represented by an embedding with three dimensions. This makes it easy to represent the embeddings as a 3D scater chart without any dimensionality reduction tricks. By saving the embedding values as we train, we can animate the chart and visualise the learning process.","d1dcf0f7":"## Observations\n- Generally you'll find that the digits `1`-`9` line up nicely and are equally spaced. Note that the line they form can be at any angle, which means that no single embedding parameter represents that dimension. \n- `0` is usually a little out of alignment, which makes sense since it is a bit different to the other digits (it never appears at the start of a number, for example). \n- Unsurprisingly, `+` and `-` end up on diametrically opposed sides of the space."}}