{"cell_type":{"4f68dc6e":"code","9b110dc0":"code","8ec72919":"code","e979158f":"code","1b0b2d20":"code","9ac66c28":"code","60262b8c":"code","f5be26bf":"code","d91612fc":"code","b72ef44d":"code","6d6797a4":"markdown","a57382fa":"markdown","64863408":"markdown","a9b77384":"markdown","fdcaed87":"markdown","420513d1":"markdown","e8bffbc6":"markdown","f45a4917":"markdown","f5236664":"markdown","d2288b39":"markdown","92ec0990":"markdown","ed3350d1":"markdown","441db073":"markdown","b65945db":"markdown","f4a81028":"markdown","14a8ca11":"markdown"},"source":{"4f68dc6e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv('\/kaggle\/input\/toyweightheight1\/weight-height.csv')\n\n\n# taking only first 50 samples\ndf = df[:50]\nsns.scatterplot(data=df, x=\"Weight\", y=\"Height\")","9b110dc0":"X = pd.DataFrame(df['Weight'])\ny = df['Height']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(5, 3))\naxes[0].plot(X_train, y_train, 'o', c='b')\naxes[1].plot(X_test, y_test, 'o', c='g')\n\naxes[0].title.set_text('Training set')\naxes[1].title.set_text('Testing set')\n\naxes[0].set_xlabel('Weight')\naxes[0].set_ylabel('Height')\nfig.tight_layout()\n","8ec72919":"from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\ny_train_linreg_pred = linreg.predict(X_train)\n\nplt.plot(X_train, y_train, 'o', c='b')\nplt.plot(X_train, y_train_linreg_pred, '-', c='r')","e979158f":"from sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor(random_state=0)\ndt.fit(X_train, y_train)\ny_train_dt_pred = dt.predict(X_train)\n\nplt.plot(X_train, y_train, 'o', c='b')\nplt.plot(X_train, y_train_dt_pred, '-', c='r')","1b0b2d20":"from sklearn.metrics import mean_squared_error\n\ndef get_bias(y, y_pred):\n    return np.round(np.mean((y_pred - y) ** 2), 2)","9ac66c28":"linreg_train_bias = get_bias(y_train, y_train_linreg_pred)\nprint('Linear Regression Training Bias = %0.2f' % linreg_train_bias)\n\ndt_train_bias = get_bias(y_train, y_train_dt_pred)\nprint('DecisionTree Regressor Training Bias = %0.2f' % dt_train_bias)","60262b8c":"y_test_linreg_pred = linreg.predict(X_test)\ny_test_dt_pred = dt.predict(X_test)\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(5, 3))\naxes[0].plot(X_test, y_test, 'o', c='g')\naxes[0].plot(X_test, y_test_linreg_pred, '-', c='r')\naxes[0].title.set_text('On Test set: Linear Regression ')\naxes[0].set_xlabel('Weight')\naxes[0].set_ylabel('Height')\n\naxes[1].plot(X_test, y_test, 'o', c='g')\naxes[1].plot(X_test, y_test_dt_pred, '-', c='r')\naxes[1].title.set_text('DecisionTree Regressor')\n\nfig.tight_layout()","f5be26bf":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(5, 3))\nfig.suptitle('Linear Regression')\naxes[0].set_title('Training set')\naxes[0].plot(X_train, y_train, 'o', c='b')\naxes[0].plot(X_train, y_train_linreg_pred, '-', c='r')\n\naxes[1].set_title('Test set')\naxes[1].plot(X_test, y_test, 'o', c='g')\naxes[1].plot(X_test, y_test_linreg_pred, '-', c='r')\n\nfig.tight_layout()","d91612fc":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(5, 3))\nfig.suptitle('DecisionTree Regressor')\naxes[0].set_title('Training set')\naxes[0].plot(X_train, y_train, 'o', c='b')\naxes[0].plot(X_train, y_train_dt_pred, '-', c='r')\n\naxes[1].set_title('Test set')\naxes[1].plot(X_test, y_test, 'o', c='g')\naxes[1].plot(X_test, y_test_dt_pred, '-', c='r')\n\nfig.tight_layout()","b72ef44d":"def sum_squares(y, y_pred):\n    return (y_pred-y)**2\n\nlinreq_train_sum_sq = sum_squares(y_train, y_train_linreg_pred)\nlinreg_test_sum_sq = sum_squares(y_test, y_test_linreg_pred)\nerror = pd.concat([linreq_train_sum_sq, linreg_test_sum_sq], ignore_index=True)\nlinreg_variance = np.round(np.var(error), 2)\n\n\ndt_train_sum_sq = sum_squares(y_train, y_train_dt_pred)\ndt_test_sum_sq = sum_squares(y_test, y_test_dt_pred)\nerror = pd.concat([dt_train_sum_sq, dt_test_sum_sq], ignore_index=True)\ndt_variance = np.round(np.var(error), 2)\n\nprint('Variance for Linear Regression: %0.2f' % linreg_variance)\nprint('Variance for DecisionTree Regressor: %0.2f' % dt_variance)","6d6797a4":"Bias is the measure of simplifying assumptions made by the model to best estimate the relationship between target and features. \n\nVariance is the measure of difference in model fits between different datasets.\n\nGoal is to find a model that has low bias and low variance.","a57382fa":"## Variance","64863408":"## Bias","a9b77384":"DecisionTree Regressor models each data point in the training set.\n\nLet us calculate the <b>Bias<\/b> each algorithm makes on the training set.","fdcaed87":"Linear Regression seems to be providing better predictions on test set as compared to DecisionTree Regresssor.\n\nLets now compare how well the models estimated test set as compared to training set.","420513d1":"Often, Machine Learning Engineers use the terms bias and variance to describe the performance of a model. Engineers may say that a model has a high variance or high bias. So, what does that mean? In general, we might say that \"high variance\" is proportional to overfitting, and \"high bias\" is proportional to underfitting.\n\nIn supervised machine learning an algorithm learns a model from training data.\n\nThe goal of any supervised machine learning algorithm is to best estimate the relationship function (f) for the target variable (Y) given the input features (X). \n\nTo understand Bias and Variance, let us start by looking into a dataset.","e8bffbc6":"Let us see how well they fit testing set.","f45a4917":"# Bias and Variance ","f5236664":"The objective is - Given the Weight of a person can we predict Height.\n\nLet us apply two machine learning algorithms, Linear Regression and Decision Tree Regressor to model the relationship between Weight and Height. \n\nFirst let us split the dataset into train and test set. ","d2288b39":"Bias is - 'Mean of sums of squares of distances between from the fit line to the data'. The error (y_pred - y) is squared to remove negative sign of distance, so that negative distance does not cancel positive distance.","92ec0990":"Linear Regression has relatively higher bias. DecisionTree regressor has zero bias as it models each data point on training set.","ed3350d1":"DecisionTree Regressor which did very well on training set, did not do so well on stesting set. \nThe difference in fits between datasets (train, test sets) is called <b>Variance<\/b>","441db073":"Apply Linear Regression on training set.","b65945db":"DecisionTree Regressor has relatively higher variance, so it is not guarenteed if it will provide good predictions on future datasets, even though its bias was zero on training set.\n\nLinear Regression has relatively lower variance, so it will give consistent predictions on future datasets if not great predictions. \n\nIf the model fits very well on training test but fits badly on test set, it is called <b>Overfitting<\/b>\n\nIdeal model needs to have low bias to accuratly model the true relationship between target and featurs and it needs to have low variance to produce consistent predictions across different datasets.","f4a81028":"Linear Regression makes an assumption that target variable is a linear function of input features and it fits a straight line to the training set. \n\nAs you can see straight line misses out lot of data points and there is a distance between data points and the line. This distance (error) is called <b>Bias<\/b>\n\nLinear Regression tries to learn the best straight line that minimizes the bias. \n\nLet us now apply Decision Tree Regressor.","14a8ca11":"## Summary:"}}