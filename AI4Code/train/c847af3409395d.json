{"cell_type":{"c7e5ca08":"code","a72fc8c6":"code","c85046a5":"code","ef8e2d71":"code","1bb0d305":"code","bc0d736c":"code","ed908993":"code","0f7cdefb":"code","11c9d909":"code","5ea21ef8":"code","b3a3416b":"code","26102e71":"code","2d969679":"code","61187a0f":"code","cac57c86":"code","78e82636":"code","a2c276ff":"code","84ccc5f7":"code","94a523c5":"code","7f7e6d7c":"code","6894d416":"code","59a1f932":"code","166c4a91":"code","5a8c4876":"markdown","2d702545":"markdown","1b2e1c91":"markdown","f1facc00":"markdown","c1127f22":"markdown","fbaaa930":"markdown","3de56e2a":"markdown","30404db9":"markdown","c952b4a0":"markdown","20b3549e":"markdown","00d64ff9":"markdown","880fae8a":"markdown","5225d6b8":"markdown","e714d04b":"markdown","03c5530d":"markdown"},"source":{"c7e5ca08":"import nltk\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom nltk.corpus import stopwords\nfrom sklearn.naive_bayes import MultinomialNB\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer","a72fc8c6":"data = pd.read_csv('\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv',encoding = 'latin-1')\ndata.head()","c85046a5":"data.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis = 1,inplace = True)\ndata.rename(columns = {'v1':'Label','v2':'Text'},inplace = True)\ndata['Label'] = data.Label.map({'ham':0,'spam':1})\ndata.head()","ef8e2d71":"sns.countplot(data.Label)","1bb0d305":"data.groupby('Label').describe()","bc0d736c":"#Dropping duplicate rows\ndata = data.drop_duplicates()\ndata.groupby('Label').describe()","ed908993":"data['Length'] = data['Text'].apply(len)\ndata.head()","0f7cdefb":"plt.figure(figsize = (10,6))\ndata[data.Label==0].Length.plot(bins = 40,kind = 'hist',color = 'green',label = 'ham messages',alpha = 0.7)\ndata[data.Label== 1].Length.plot(bins = 10,kind = 'hist',color = 'red',label = 'Spam messages',alpha = 0.8)\nplt.legend()\nplt.xlabel('Length')","11c9d909":"data[data['Label']==0].describe()","5ea21ef8":"data[data['Label']==1].describe()","b3a3416b":"stemmer = SnowballStemmer(\"english\")\nstop = stopwords.words('english')","26102e71":"#Removing Stopwords\ndata['Clean_text'] = data['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n#Adding a column to show the length of the Text after removing the Stopwords\ndata['stopword_len'] = data['Clean_text'].apply(len)\n#Removing special character,digits,...\ndata['Clean_text']= data['Clean_text'].str.replace('''[,\u201d\u201c\u2019''--.\/<\u201c>-?1234567890-=\\|+_)(*&^%$#@!`~:\"{}]''','',case=False)\n#Removing the Links,websites,...\ndata['Clean_text']= data['Clean_text'].str.replace('http\\S+|www.\\S+', '', case=False)\n#Making the text to lower case\ndata['Clean_text']= data['Clean_text'].map(lambda x: x.lower())\n#Tokenization\ndata['Clean_text']= data.apply(lambda row: nltk.word_tokenize(row['Clean_text']), axis=1)\n#Stemming\ndata['Clean_text']= data['Clean_text'].apply(lambda row:[stemmer.stem(y) for y in row])","2d969679":"data.head()","61187a0f":"data['Final_text'] = ''\nfor i in data.index:\n    text = ' '.join(data['Clean_text'][i])\n    data['Final_text'][i] = text\n    ","cac57c86":"data.drop(['Text','Length','Clean_text','stopword_len'],inplace = True,axis =1)","78e82636":"data.head()","a2c276ff":"x = data.Final_text\ny = data.Label","84ccc5f7":"size = round(len(y)*0.8)\nx_train = x[:size]\nx_test = x[size:]\ny_train = y[:size]\ny_test = y[size:]\nprint('x_train',x_train.shape)\nprint('x_test',x_test.shape)\nprint('y_train',y_train.shape)\nprint('y_test',y_test.shape)","94a523c5":"vect = CountVectorizer()\nvect.fit(x_train)","7f7e6d7c":"x_train = vect.fit_transform(x_train)\nx_train","6894d416":"x_test = vect.transform(x_test)\nx_test","59a1f932":"def tsne_plot(x, y):\n\n    # Setting the plotting background\n    sns.set(style =\"whitegrid\")\n      \n    tsne = TSNE(n_components = 2, random_state = 0)\n      \n    # Reducing the dimensionality of the data\n    X_transformed = tsne.fit_transform(x)\n      \n    plt.figure(figsize =(10, 6))\n  \n    # Building the scatter plot\n    plt.scatter(X_transformed[np.where(y == 0), 0], \n                X_transformed[np.where(y == 0), 1],\n                marker ='o', linewidth =1,\n                alpha = 0.8, label ='hem')\n    plt.scatter(X_transformed[np.where(y == 1), 0],\n                X_transformed[np.where(y == 1), 1],\n                marker ='o', linewidth =1,\n                alpha = 0.8, label ='spam')\n  \n    # Specifying the location of the legend\n    plt.legend(loc ='best')\n\n\ntsne_plot(x_train,y_train)","166c4a91":"nb = MultinomialNB()\nnb.fit(x_train, y_train)\ny_pred_class = nb.predict(x_test)\nprint('Accuracy : ',metrics.accuracy_score(y_test, y_pred_class))","5a8c4876":"Next step is to join the tokens together","2d702545":"**Importing Libraries**","1b2e1c91":"**Dropping the empty unwanted column in next step we are changing the column names for better understanding then\nMapping the label to the numeric format for the classification.**","f1facc00":"Now drop the dirty columns.","c1127f22":"Fitting vocabulary","fbaaa930":"Adding Length column which is depending upon the length of the text in the row","3de56e2a":"After cleaning the Text ","30404db9":"**Plotting the Count of datas**","c952b4a0":"**Checking for duplicate and removing those duplicate values from the data**","20b3549e":"From above After Comparing the both we have find that Spam messages have more length than Ham messages","00d64ff9":"We found that in **ham** there is 309(4825-4516)duplicates and in **spam** 94(747-653)duplicates","880fae8a":"Now need to apply the machine learning algorithm.\n\n* 1st we need to take term frequency.\n* 2nd for high frequency value we need to reduce the frequency that is known as inverse document frequency","5225d6b8":"**Reading the dataset**","e714d04b":"plotting the hist depending upon the length of the Text","03c5530d":"Splitting the data into 80% for training and 20% for testing"}}