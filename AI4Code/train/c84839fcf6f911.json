{"cell_type":{"d6a7d9a4":"code","2766926a":"code","b4c97be8":"code","6d22955b":"code","44606cb8":"code","fc523de5":"code","9e4c388a":"code","4914d48a":"code","765b88be":"code","f48a16b4":"code","6f00a01e":"code","fa70eb79":"code","1a4ef2f2":"code","50eedf1e":"code","a1188cc8":"code","141e51ec":"code","3fcec84c":"code","696f7955":"code","3809452a":"code","794e3247":"code","27e60def":"code","e9f9a988":"code","aa3f2429":"code","a89fb2e0":"code","f184ab44":"code","12f2f5eb":"code","9527acb1":"code","ba7c939b":"code","d8409bf3":"code","18281f6e":"markdown"},"source":{"d6a7d9a4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","2766926a":"df=pd.read_csv('\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv')","b4c97be8":"df.head()","6d22955b":"df.fillna(0.00,inplace=True)","44606cb8":"df.info()","fc523de5":"#f,ax=plt.subplots(2,2,figsize=(15,10))\nplt.figure(figsize=(10,5))\nsns.countplot(df['status'])\n\n","9e4c388a":"plt.figure(figsize=(10,5))\nsns.countplot(df['specialisation'])\n","4914d48a":"plt.figure(figsize=(10,5))\nsns.countplot(df['degree_t'])\n","765b88be":"plt.figure(figsize=(10,5))\nsns.countplot(df['hsc_s'])","f48a16b4":"df.head(2)","6f00a01e":"df2=df[df.status=='Placed']\ndf3=df[df.status=='Not Placed']","fa70eb79":"plt.figure(figsize=(10,8))\nplt.bar(df2['degree_t'].unique(),height=df2['degree_t'].value_counts(),color='orange',label='placed')\nplt.bar(df3['degree_t'].unique(),height=df3['degree_t'].value_counts(),color='black',label='not placed')\n\nplt.xlabel('degrees')\nplt.ylabel('frequency')\nplt.title('placements according to bachelors degree',fontsize=13)\nplt.legend()","1a4ef2f2":"f, ax=plt.subplots(2,2,figsize=(15,10))\n\nax[0,0].bar(df2['degree_t'].unique(),height=df2['degree_t'].value_counts(),color='orange',label='placed')\nax[0,0].bar(df3['degree_t'].unique(),height=df3['degree_t'].value_counts(),color='black',label='not placed')\nax[0,0].set_title('placement scenarios of degree_t')\nax[0,0].legend()\nax[0,1].bar(df2['gender'].unique(),height=df2['gender'].value_counts(),color='orange',label='placed')\nax[0,1].bar(df3['gender'].unique(),height=df3['gender'].value_counts(),color='black',label='not placed')\nax[0,1].set_xlabel('gender')\nax[0,1].set_title('placement scenarios of gender')\nax[0,1].legend()\nax[1,0].bar(df2['workex'].unique(),height=df2['workex'].value_counts(),color='orange',label='placed')\nax[1,0].bar(df3['workex'].unique(),height=df3['workex'].value_counts(),color='black',label='not placed')\nax[1,0].set_xlabel('workex')\nax[1,0].set_title('placement scenarios of workex')\nax[1,0].legend()\nax[1,1].bar(df2['specialisation'].unique(),height=df2['specialisation'].value_counts(),color='orange',label='placed')\nax[1,1].bar(df3['specialisation'].unique(),height=df3['specialisation'].value_counts(),color='black',label='not placed')\nax[1,1].set_xlabel('specialisation')\nax[1,1].set_title('placement scenarios of specialisation')\nplt.legend()\n","50eedf1e":"def change(data):\n    if data<50.00:\n        return 1\n    elif data>=50.00 and data<60.00:\n        return 2\n    elif data>=60.00 and data<70.00:\n        return 3\n    elif data>=70.00 and data<80.00:\n        return 4\n    else:\n        return 5","a1188cc8":"df['10_percent']=df.ssc_p.apply(change)\ndf['12_percent']=df.hsc_p.apply(change)\ndf['degree_percent']=df.degree_p.apply(change)\ndf['etest_percent']=df.etest_p.apply(change)\ndf['mba_percent']=df.mba_p.apply(change)","141e51ec":"df.head(2)","3fcec84c":"df2=df[df.status=='Placed']\ndf3=df[df.status=='Not Placed']","696f7955":"f, ax=plt.subplots(2,2,figsize=(15,10))\n\nax[0,0].bar(df2['10_percent'].unique(),height=df2['10_percent'].value_counts(),color='orange',label='placed')\nax[0,0].bar(df3['10_percent'].unique(),height=df3['10_percent'].value_counts(),color='black',label='not placed')\nax[0,0].set_xlabel('percentage of 10th standard')\n\nax[0,0].legend()\nax[0,1].bar(df2['12_percent'].unique(),height=df2['12_percent'].value_counts(),color='orange',label='placed')\nax[0,1].bar(df3['12_percent'].unique(),height=df3['12_percent'].value_counts(),color='black',label='not placed')\nax[0,1].set_xlabel('percentage of 12th standard')\n\nax[0,1].legend()\nax[1,0].bar(df2['degree_percent'].unique(),height=df2['degree_percent'].value_counts(),color='orange',label='placed')\nax[1,0].bar(df3['degree_percent'].unique(),height=df3['degree_percent'].value_counts(),color='black',label='not placed')\nax[1,0].set_xlabel('percentage of degree')\n\nax[1,0].legend()\nax[1,1].bar(df2['mba_percent'].unique(),height=df2['mba_percent'].value_counts(),color='orange',label='placed')\nax[1,1].bar(df3['mba_percent'].unique(),height=df3['mba_percent'].value_counts(),color='black',label='not placed')\nax[1,1].set_xlabel('percentage of mba')\n\nplt.legend()\n","3809452a":"#df2=df.drop(['10_percent','12_percent','degree_percent','etest_percent','mba_percent'],axis=1)","794e3247":"df2.head(3)","27e60def":"#For applying models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,r2_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestClassifier\n\n#for regression task use r2_score in place of accuracy_score\n\n#Algorithms\n\ndef algorithm1(X,y):\n    model=LogisticRegression()\n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\n    model.fit(X_train,y_train)\n    pred=model.predict(X_test)\n    acc_score=accuracy_score(pred,y_test)\n    print(\"accuracy score is : {}\".format(acc_score))\n    \n    \ndef algorithm2(X,y,n):\n    model=KNeighborsClassifier(n_neighbors=n)\n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\n    model.fit(X_train,y_train)\n    pred=model.predict(X_test)\n    acc_score=accuracy_score(pred,y_test)\n    #print(\"accuracy score is : {}\".format(acc_score))\n    return acc_score\n    \ndef algorithm4(X,y):\n    model=RandomForestClassifier()\n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\n    model.fit(X_train,y_train)\n    pred=model.predict(X_test)\n    acc_score=accuracy_score(pred,y_test)\n    print(\"accuracy score is : {}\".format(acc_score))","e9f9a988":"#converting categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\ns=df.dtypes=='object'\nobject_cols=list(s[s].index)\nprint(object_cols)\n\nencoder=LabelEncoder()\nfor cols in object_cols:\n    df[cols]=encoder.fit_transform(df[cols])\n","aa3f2429":"df.drop(['ssc_p','hsc_p','hsc_p','degree_p','mba_p',],axis=1,inplace=True)","a89fb2e0":"df.set_index('sl_no',inplace=True)\ndf.head()","f184ab44":"#Tried to predict salary also so it will be a regression task\ny_classification=df['status']\ny_reg=df['salary']\ndf.drop(['status','salary'],axis=1,inplace=True)","12f2f5eb":"df.drop('etest_p',axis=1,inplace=True)","9527acb1":"list_knn_class=[]\nlist_knn_reg=[]\nfor i in range(1,20):\n    list_knn_class.append(algorithm2(df,y_classification,i))\n    list_knn_reg.append(algorithm2(df,y_reg,i))","ba7c939b":"plt.figure(figsize=(15,10))\nsns.lineplot([i for i in range(1,20)],list_knn_class,marker='o')\nplt.xlabel('value of parameter')\nplt.ylabel('accuracy of classification model')","d8409bf3":"plt.figure(figsize=(15,10))\nsns.lineplot([i for i in range(1,20)],list_knn_reg,marker='o')\nplt.xlabel('value of parameter')\nplt.ylabel('accuracy of regression model')","18281f6e":"# Just read these points once\n\n- First of all keep in mind this notebook is a begginer's notebook and I don't want to make it beautiful by fancy codes and comments as you should know how your fellow friend had done with this dataset and you have to do better than him:)\n\n- It gives you a feeling of achievement when after numerous fail you succeed, This was the situation with me also I had tried lots of dataset like titanic, house prediction and some other but I was not getting a breakthrough ie. I was not developing a feeling that whether I know how to approach a dataset or not.\n\n- This dataset is perfect for beginners and what the thing I like most is it had given me a knowledge that where to start categorical variable analysis in datasets.\n\n- What I suggest you to do focus on is the graphs which I had formed and please see the codes once and try to rewrite them I bet you that you will feel upgraded after creating those graphs by yourself.\n\n- Still many of things are remaining in this like coorelation matric through heatmap and I encourage you to repeat it and implement your creativity in it or combine multiple notebook approach in one notebook that will definitely benefit you as a begginer.\n\n- Please share your notebook also it will motivate you to write a well documented code and boost your confidence.\n\n- always remember we begginers are begginers until we are not developing a problem solving pattern.\n\n- For hindi speakers - \n  \" data se lado to huzoor, upskill tumhe ye zaroor karega.\n    aj tu bhale hi beginer hai, lekin kal tu grand master banega.\"\n    \n- Please don't forgot to upvote it:)"}}