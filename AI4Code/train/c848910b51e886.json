{"cell_type":{"f91ce8ab":"code","0311d4b6":"code","0460bd96":"code","099159f9":"code","f93d3626":"code","bae366b8":"code","374e7cde":"code","f4d93a47":"code","a989b346":"code","b58585d3":"code","a38fd27d":"code","5907a32c":"code","6a186851":"code","387d36f6":"code","6a4621f9":"code","fe8ae6e9":"code","c15f24a1":"code","822bb550":"code","fe04978b":"code","bff16963":"code","3312504f":"code","b3896aae":"code","818587f1":"code","32f88090":"code","e006e47c":"code","fde98b33":"code","2f76c558":"code","bfae16b0":"code","0ee157fd":"code","ca287f35":"code","32cb8e9d":"code","b56142d2":"code","d97d52ba":"code","2d64b12b":"code","b46a1b16":"code","703595cd":"code","48505d81":"code","2672a10e":"code","dce7fd53":"code","46ce99ba":"code","53098a72":"code","b091cc05":"code","0f69b82d":"code","d843e90e":"markdown","b52961c4":"markdown","7affcb30":"markdown","0daef963":"markdown","91855ad2":"markdown","b349e1f1":"markdown","0d0ded92":"markdown","4f9b0695":"markdown","8804eddc":"markdown","a1d22988":"markdown","dab4a970":"markdown","e5dcb75f":"markdown","460f6d75":"markdown","3d6eb708":"markdown","b974f3c5":"markdown","c0d89f39":"markdown","2f6a3488":"markdown","edff2e74":"markdown","e19c22b9":"markdown","dfe50d2f":"markdown","a2fe6d4c":"markdown","470b50d2":"markdown","55972e05":"markdown","1e7c649f":"markdown","1e7c50d4":"markdown","e38e1d0b":"markdown","61b9a93f":"markdown","f73aa6a9":"markdown","f61ebb1f":"markdown","2d2865ea":"markdown","61da7b0e":"markdown","7435d288":"markdown","ec5f7000":"markdown","165d5afe":"markdown","68228add":"markdown","60675450":"markdown","dc43a10d":"markdown","0df53b68":"markdown","00a75e1c":"markdown","5ff216ba":"markdown","2b3421fc":"markdown"},"source":{"f91ce8ab":"from prettytable import PrettyTable \n  \n# Specify the Column Names while initializing the Table \nmyTable = PrettyTable([\"Type\", \"Training Dataset\", \"Testing Dataset\"], \n                     title = \"Table 1 The composition of chest X-ray dataset\") \n  \n# Add rows \nmyTable.add_row([\"Normal\", \"1349 (25.7%)\", \"234 (37.5%)\"]) \nmyTable.add_row([\"Bacteria\", \"2538 (48.5%)\", \"242 (38.7%)\"]) \nmyTable.add_row([\"Virus\", \"1343 (25.7%)\", \"148 (23.7%)\"]) \nmyTable.add_row([\"Total\", \"5232 (100%)\", \"624 (100%)\"]) \n\nprint(myTable)","0311d4b6":"# Load the numpy and panda package for linear algebra and data processing\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\n\n# Imports packages to view data\nfrom glob import glob\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nimport os\nfrom skimage.io import imread\nimport cv2\n\n# Import keras packages\nfrom keras import applications\nfrom tensorflow.keras.optimizers import RMSprop,Adam\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import optimizers\nfrom keras.models import Sequential, Model, load_model\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras.utils.data_utils import Sequence\nfrom keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\nfrom keras.layers import Conv2D, SeparableConv2D, MaxPool2D, LeakyReLU, Activation\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n\n# Load the data \ndata_dir  = '..\/input\/chest-xray-pneumonia\/chest_xray\/'\ntrain_dir = os.path.join(data_dir,'train\/')\nval_dir = os.path.join(data_dir,'val\/')\ntest_dir = os.path.join(data_dir,'test\/')\n","0460bd96":"# Get the path to the normal and pneumonia sub-directories\nnormal_cases_dir = train_dir + 'NORMAL\/'\npneumonia_cases_dir = train_dir + 'PNEUMONIA\/'\n\n# Get the list of all the images\nnormal_cases = glob(normal_cases_dir+'\/*.jpeg')\npneumonia_cases = glob(pneumonia_cases_dir+'\/*.jpeg')\n\n# An empty list. We will insert the data into this list in (img_path, label) format\ntrain_data = []\n\n# Go through all the normal cases. The label for these cases will be 0\nfor img in normal_cases:\n    train_data.append((img,0))\n\n# Go through all the pneumonia cases. The label for these cases will be 1\nfor img in pneumonia_cases:\n    train_data.append((img, 1))\n\n# Get a pandas dataframe from the data we have in our list \ntrain_data = pd.DataFrame(train_data, columns=['image', 'label'],index=None)\n\n# Shuffle the data \ntrain_data = train_data.sample(frac=1.).reset_index(drop=True)\n\n# Get few samples for both the classes\npneumonia_samples = (train_data[train_data['label']==1]['image'].iloc[:4]).tolist()\nnormal_samples = (train_data[train_data['label']==0]['image'].iloc[:4]).tolist()\n\n# Concat the data in a single list and del the above two list\nsamples = pneumonia_samples + normal_samples\ndel pneumonia_samples, normal_samples\n\n# Plot the data \nf, ax = plt.subplots(2,4, figsize=(30,10))\nfor i in range(8):\n    img = imread(samples[i])\n    ax[i\/\/4, i%4].imshow(img, cmap='gray')\n    if i<4:\n        ax[i\/\/4, i%4].set_title(\"Pneumonia\")\n    else:\n        ax[i\/\/4, i%4].set_title(\"Normal\")\n    ax[i\/\/4, i%4].axis('off')\n    ax[i\/\/4, i%4].set_aspect('auto')\nplt.show()","099159f9":"training_images = tf.io.gfile.glob('..\/input\/chest-xray-pneumonia\/chest_xray\/train\/*\/*')\nvalidation_images = tf.io.gfile.glob('..\/input\/chest-xray-pneumonia\/chest_xray\/val\/*\/*')\n\n# Data before split\nprint(f'Before division of 80:20')\nprint(f'Total number of training images = {len(training_images)}')\nprint(f'Total number of validation images = {len(validation_images)}\\n')\n\n# Merge the training and validation, to split them afterwards\ntotal_files = training_images\ntotal_files.extend(validation_images)\nprint(f'Total number of images : training_images + validation_images = {len(total_files)}\\n')\n\n# Spliting 80:20\ntrain_images, val_images = train_test_split(total_files, test_size = 0.2)\nprint(f'After division of 80:20')\nprint(f'Total number of training images = {len(train_images)}')\nprint(f'Total number of validation images = {len(val_images)}')","f93d3626":"tf.io.gfile.makedirs('\/kaggle\/working\/val_dataset\/negative\/')\ntf.io.gfile.makedirs('\/kaggle\/working\/val_dataset\/positive\/')\ntf.io.gfile.makedirs('\/kaggle\/working\/train_dataset\/negative\/')\ntf.io.gfile.makedirs('\/kaggle\/working\/train_dataset\/positive\/')","bae366b8":"# Train images\nfor ele in train_images:\n    parts_of_path = ele.split('\/')\n\n    if 'PNEUMONIA' == parts_of_path[-2]:\n        tf.io.gfile.copy(src = ele, dst = '\/kaggle\/working\/train_dataset\/positive\/' +  parts_of_path[-1])\n    else:\n        tf.io.gfile.copy(src = ele, dst = '\/kaggle\/working\/train_dataset\/negative\/' +  parts_of_path[-1])\n\n# Validation images\nfor ele in val_images:\n    parts_of_path = ele.split('\/')\n\n    if 'PNEUMONIA' == parts_of_path[-2]:\n        tf.io.gfile.copy(src = ele, dst = '\/kaggle\/working\/val_dataset\/positive\/' +  parts_of_path[-1])\n    else:\n        tf.io.gfile.copy(src = ele, dst = '\/kaggle\/working\/val_dataset\/negative\/' +  parts_of_path[-1])","374e7cde":"# Specify the Column Names while initializing the Table \nmytable = PrettyTable([\"Method\", \"Setting\"], \n                     title = \"Settings for the image augmentation.\") \n  \n# Add rows \nmytable.add_row([\"Rescale\", \"1\/255\"])\nmytable.add_row([\"Zoom Range\", \"0.3\"]) \nmytable.add_row([\"Vertical Flip\", \"True\"]) \n\nprint(mytable)","f4d93a47":"# Hyperparameters\nimage_size = 150\nepochs = 20\nbatch_size = 32","a989b346":"train_datagen = ImageDataGenerator(rescale = 1\/255,\n                                   zoom_range = 0.3,\n                                   # width_shift_range = 0.1,\n                                   # height_shift_range = 0.1, \n                                   vertical_flip=True)\n\nval_datagen = ImageDataGenerator(rescale = 1.\/255)\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ntrain_gen = train_datagen.flow_from_directory(\n    '\/kaggle\/working\/train_dataset\/',\n    target_size = (image_size, image_size),\n    batch_size = batch_size ,\n    class_mode = 'binary'\n)\n\nvalidation_gen = val_datagen.flow_from_directory(\n    '\/kaggle\/working\/val_dataset\/',\n    target_size = (image_size, image_size),\n    batch_size = batch_size ,\n    class_mode = 'binary'\n)\n\ntest_gen = test_datagen.flow_from_directory(\n     '..\/input\/chest-xray-pneumonia\/chest_xray\/test',\n    target_size = (image_size, image_size),\n    batch_size = batch_size , \n    class_mode = 'binary'\n)","b58585d3":"bias = np.log([count_pneumonia\/count_normal])\nbias\n\nweight_for_0 = (1 \/ count_normal)*(len(train_images))\/2.0 \nweight_for_1 = (1 \/ count_pneumonia)*(len(train_images))\/2.0\n\nclass_weight = {0: weight_for_0, 1: weight_for_1}\n\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))\n\n\n* [1. CNN Model ](#10)\n* [2. Creating Data Generators ](#11)\n* [4. Correction  for Data Imbalance ](#13)\n* [3. Finetune CNN Model ](#12)  \n* [3. Train CNN Model ](#12)      \n* [3. Visualise Results ](#12)","a38fd27d":"# Callbacks for Regularization\ncheckpoint = ModelCheckpoint(filepath='best_weights.CNN', \n                             save_best_only=True)\nlr_reduce = ReduceLROnPlateau(monitor='val_loss', \n                              factor=0.3, \n                              patience=2, \n                              verbose=2, \n                              mode='max')","5907a32c":"CNN = tf.keras.models.Sequential([\n    \n    # This is the first convolution\n    tf.keras.layers.Conv2D(16, (3,3), activation='relu',padding='same', input_shape=(image_size, image_size, 3)),\n    tf.keras.layers.Conv2D(16, (3,3), activation='relu',padding='same'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    \n    # The second convolution\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu',padding='same'),\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu',padding='same'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D(2,2),\n    \n    # The third convolution\n    tf.keras.layers.SeparableConv2D(64, (3,3), activation='relu',padding='same'),\n    tf.keras.layers.SeparableConv2D(64, (3,3), activation='relu',padding='same'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D(2,2),\n    \n    # The fourth convolution\n    tf.keras.layers.SeparableConv2D(128, (3,3), activation='relu',padding='same'),\n    tf.keras.layers.SeparableConv2D(128, (3,3), activation='relu',padding='same'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D(2,2),\n    \n    # The fifth convolution\n    tf.keras.layers.SeparableConv2D(256, (3,3), activation='relu',padding='same'),\n    tf.keras.layers.SeparableConv2D(256, (3,3), activation='relu',padding='same'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.2),\n    \n    # Flatten the results to feed into a DNN\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dropout(0.7),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.3),\n    \n    # Output layer, only 1 output neuron: it will contain a value from 0-1 \n    tf.keras.layers.Dense(1, activation='sigmoid'),\n    \n])","6a186851":"# Model Summary\nCNN.summary()","387d36f6":"# Compile our model\nCNN.compile(loss='binary_crossentropy',\n            optimizer='adam',\n            metrics=['accuracy','Recall', 'Precision'])","6a4621f9":"# Fitting the model\nhistCNN = CNN.fit_generator(\n    train_gen, \n    steps_per_epoch = train_gen.samples \/\/ batch_size, \n    epochs = epochs, \n    validation_data = validation_gen, \n    validation_steps = validation_gen.samples \/\/ batch_size, \n    class_weight = class_weight, \n    callbacks = [checkpoint, lr_reduce])","fe8ae6e9":"fig, ax = plt.subplots(1, 2, figsize=(10, 3))\nax = ax.ravel()\n\nfor i, met in enumerate(['accuracy', 'loss']):\n    ax[i].plot(histCNN.history[met])\n    ax[i].plot(histCNN.history['val_' + met])\n    ax[i].set_title('Model {}'.format(met))\n    ax[i].set_xlabel('epochs')\n    ax[i].set_ylabel(met)\n    ax[i].legend(['train', 'val'])","c15f24a1":"test_result_CNN = CNN.evaluate_generator(test_gen, 624)\nprint('Loss rate at test data :', test_result_CNN[0])\nprint('Accuracy rate at test data :', test_result_CNN[1])\nprint('Recall rate at test data :', test_result_CNN[2])\nprint('Precision rate at test data :', test_result_CNN[3])","822bb550":"Inception_model = tf.keras.models.load_model('\/kaggle\/working\/best_weights.CNN')\n\nwrong_predicted_image = [[],[]]\ncorrect_predicted_image = [[],[]]\ni = 0\nwhile i< 5 and len(wrong_predicted_image[0]) < 6:\n    j = 0\n    while j < 32 and len(wrong_predicted_image[0]) < 6:\n        \n        image_array = (test_gen[i][0][j]).reshape(1,150,150,3)\n        \n        prediction = Inception_model.predict(image_array)\n        \n        if int(round(prediction[0][0])) != test_gen[i][1][j]:\n            wrong_predicted_image[0].append(image_array)\n            wrong_predicted_image[1].append(int(round(prediction[0][0])))\n            \n        elif len(correct_predicted_image[0]) < 6:\n            correct_predicted_image[0].append(image_array)\n            correct_predicted_image[1].append(int(round(prediction[0][0])))\n#        print(len(correct_predicted_image[0]),len(wrong_predicted_image[0]))\n        j += 1\n        \n    i += 1","fe04978b":"import matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nrcParams['figure.figsize'] = 22 ,4\nfig, ax = plt.subplots(1,6)\n\ni = 0\nfor ele in wrong_predicted_image[0]:\n    image = tf.keras.preprocessing.image.array_to_img(ele.reshape(150,150,3))\n    ax[i].imshow(image)\n    if wrong_predicted_image[1][i] == 1:\n        ax[i].set_title(\"Classified as Pneumonia\")\n    else:\n        ax[i].set_title(\"Classified as Normal\")\n    i += 1\n\nprint(f'wrong_prediction_by_model --- {wrong_predicted_image[1]}')","bff16963":"import matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nrcParams['figure.figsize'] = 22 ,4\nfig, ax = plt.subplots(1,6)\n\ni = 0\nfor ele in correct_predicted_image[0]:\n    image = tf.keras.preprocessing.image.array_to_img(ele.reshape(150,150,3))\n    ax[i].imshow(image)\n    if correct_predicted_image[1][i] == 1:\n        ax[i].set_title(\"Classified as Pneumonia\")\n    else:\n        ax[i].set_title(\"Classified as Normal\")\n    i += 1\n\nprint(f'correct_prediction_by_model --- {correct_predicted_image[1]}')","3312504f":"\n# Deze had een betere fit, maargoed nu performed onze CNN wel beter met de oude values \n# train_datagen = ImageDataGenerator(rescale = 1\/255,\n#                                   rotation_range = 30\n#                                   zoom_range = 0.2,\n#                                   width_shift_range = 0.1,\n#                                   height_shift_range = 0.1)","b3896aae":"base_VGG = tf.keras.applications.VGG16(input_shape=(image_size, image_size, 3),\n                                       include_top=False, \n                                       weights='imagenet')\nbase_VGG.trainable = False\n\n# Add some layers which we found to be \nVGG = tf.keras.Sequential([\n        base_VGG,\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(512, activation='relu'),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(512, activation='relu'),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(1,activation='sigmoid')\n        ])\n\n# Compile our model\nVGG.compile(loss='binary_crossentropy', \n            optimizer='adam', \n            metrics=['accuracy', 'Recall', 'Precision'])\nVGG.summary()","818587f1":"print(len(base_VGG.layers))","32f88090":"# Callbacks for Regularization\ncheckpoint = ModelCheckpoint(filepath='best_weights.VGG', \n                             save_best_only=True)\nlr_reduce = ReduceLROnPlateau(monitor='val_loss', \n                              factor=0.3, \n                              patience=2, \n                              verbose=2, \n                              mode='max')","e006e47c":"histVGG = VGG.fit_generator(\n    train_gen, \n    steps_per_epoch = train_gen.samples \/\/ batch_size, \n    epochs = epochs, \n    validation_data = validation_gen, \n    validation_steps = validation_gen.samples \/\/ batch_size,\n    class_weight = class_weight,\n    callbacks= [checkpoint, lr_reduce])","fde98b33":"figure, axis = plt.subplots(1, 2, figsize=(18,5))\naxis = axis.ravel()\n\nfor i,element in enumerate(['accuracy', 'loss']):\n    axis[i].plot(histVGG.history[element])\n    axis[i].plot(histVGG.history['val_' + element])\n    axis[i].set_title('Model {}'.format(element))\n    axis[i].set_xlabel('epochs')\n    axis[i].set_ylabel(element)\n    axis[i].legend(['train', 'val'])","2f76c558":"test_result_VGG = VGG.evaluate_generator(test_gen, 624)\nprint('Loss rate at test data :', test_result_VGG[0])\nprint('Accuracy rate at test data :', test_result_VGG[1])\nprint('Recall rate at test data :', test_result_VGG[2])\nprint('Precision rate at test data :', test_result_CNN[3])","bfae16b0":"Inception_model = tf.keras.models.load_model('\/kaggle\/working\/best_weights.VGG')\n\nwrong_predicted_image = [[],[]]\ncorrect_predicted_image = [[],[]]\ni = 0\nwhile i< 5 and len(wrong_predicted_image[0]) < 6:\n    j = 0\n    while j < 32 and len(wrong_predicted_image[0]) < 6:\n        \n        image_array = (test_gen[i][0][j]).reshape(1,150,150,3)\n        \n        prediction = Inception_model.predict(image_array)\n        \n        if int(round(prediction[0][0])) != test_gen[i][1][j]:\n            wrong_predicted_image[0].append(image_array)\n            wrong_predicted_image[1].append(int(round(prediction[0][0])))\n            \n        elif len(correct_predicted_image[0]) < 6:\n            correct_predicted_image[0].append(image_array)\n            correct_predicted_image[1].append(int(round(prediction[0][0])))\n#        print(len(correct_predicted_image[0]),len(wrong_predicted_image[0]))\n        j += 1\n        \n    i += 1","0ee157fd":"import matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nrcParams['figure.figsize'] = 22 ,4\nfig, ax = plt.subplots(1,6)\n\ni = 0\nfor ele in wrong_predicted_image[0]:\n    image = tf.keras.preprocessing.image.array_to_img(ele.reshape(150,150,3))\n    ax[i].imshow(image)\n    if wrong_predicted_image[1][i] == 1:\n        ax[i].set_title(\"Classified as Pneumonia\")\n    else:\n        ax[i].set_title(\"Classified as Normal\")\n    i += 1\n\nprint(f'wrong_prediction_by_model --- {wrong_predicted_image[1]}')","ca287f35":"import matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nrcParams['figure.figsize'] = 22 ,4\nfig, ax = plt.subplots(1,6)\n\ni = 0\nfor ele in correct_predicted_image[0]:\n    image = tf.keras.preprocessing.image.array_to_img(ele.reshape(150,150,3))\n    ax[i].imshow(image)\n    if correct_predicted_image[1][i] == 1:\n        ax[i].set_title(\"Classified as Pneumonia\")\n    else:\n        ax[i].set_title(\"Classified as Normal\")\n    i += 1\n\nprint(f'correct_prediction_by_model --- {correct_predicted_image[1]}')","32cb8e9d":"# Library\nfrom tensorflow.keras.applications.resnet50 import ResNet50\n\n\nbase_resnet = ResNet50(input_shape=(image_size, image_size, 3),\n                                                     include_top=False, \n                                                     weights='imagenet')\n# base_resnet.trainable = False\n# Freazing the first 200 trained layers\nfor layers in base_resnet.layers[:100]:\n    layers.trainable = False\n\n# Add some layers which we found to be \nResNet = tf.keras.Sequential([\n        base_resnet,\n        tf.keras.layers.GlobalAveragePooling2D(),\n     #  tf.keras.layers.Flatten(),\n     #  tf.keras.layers.Dense(512, activation='relu'),\n     #  tf.keras.layers.Dropout(0.2),\n     #  tf.keras.layers.Dense(512, activation='relu'),\n     #  tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(1,activation='sigmoid')\n        ])\n\n# Compile our model\nResNet.compile(loss='binary_crossentropy', \n               optimizer='adam', \n               metrics=['accuracy', 'Recall', 'Precision'])\n\n# ResNet.summary()","b56142d2":"# Callbacks for Regularization\ncheckpoint = ModelCheckpoint(filepath='best_weights.ResNet', \n                             save_best_only=True)\nlr_reduce = ReduceLROnPlateau(monitor='val_loss', \n                              factor=0.3, \n                              patience=2, \n                              verbose=2, \n                              mode='max')","d97d52ba":"histResNet = ResNet.fit_generator(\n    train_gen, \n    steps_per_epoch = train_gen.samples \/\/ batch_size, \n    epochs = epochs, \n    validation_data = validation_gen, \n    validation_steps = validation_gen.samples \/\/ batch_size, \n    class_weight = class_weight,\n    callbacks= [checkpoint, lr_reduce])","2d64b12b":"figure, axis = plt.subplots(1, 2, figsize=(18,5))\naxis = axis.ravel()\n\nfor i,element in enumerate(['accuracy', 'loss']):\n    axis[i].plot(histResNet.history[element])\n    axis[i].plot(histResNet.history['val_' + element])\n    axis[i].set_title('Model {}'.format(element))\n    axis[i].set_xlabel('epochs')\n    axis[i].set_ylabel(element)\n    axis[i].legend(['train', 'val'])","b46a1b16":"test_result_ResNet = ResNet.evaluate_generator(test_gen, 624)\nprint('Loss rate at test data :', test_result_ResNet[0])\nprint('Accuracy rate at test data :', test_result_ResNet[1])\nprint('Recall rate at test data :', test_result_ResNet[2])\nprint('Precision rate at test data :', test_result_CNN[3])","703595cd":"Inception_model = tf.keras.models.load_model('\/kaggle\/working\/best_weights.ResNet')\n\nwrong_predicted_image = [[],[]]\ncorrect_predicted_image = [[],[]]\ni = 0\nwhile i< 5 and len(wrong_predicted_image[0]) < 6:\n    j = 0\n    while j < 32 and len(wrong_predicted_image[0]) < 6:\n        \n        image_array = (test_gen[i][0][j]).reshape(1,150,150,3)\n        \n        prediction = Inception_model.predict(image_array)\n        \n        if int(round(prediction[0][0])) != test_gen[i][1][j]:\n            wrong_predicted_image[0].append(image_array)\n            wrong_predicted_image[1].append(int(round(prediction[0][0])))\n            \n        elif len(correct_predicted_image[0]) < 6:\n            correct_predicted_image[0].append(image_array)\n            correct_predicted_image[1].append(int(round(prediction[0][0])))\n#        print(len(correct_predicted_image[0]),len(wrong_predicted_image[0]))\n        j += 1\n        \n    i += 1","48505d81":"import matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nrcParams['figure.figsize'] = 22 ,4\nfig, ax = plt.subplots(1,6)\n\ni = 0\nfor ele in wrong_predicted_image[0]:\n    image = tf.keras.preprocessing.image.array_to_img(ele.reshape(150,150,3))\n    ax[i].imshow(image)\n    if wrong_predicted_image[1][i] == 1:\n        ax[i].set_title(\"Classified as Pneumonia\")\n    else:\n        ax[i].set_title(\"Classified as Normal\")\n    i += 1\n\nprint(f'wrong_prediction_by_model --- {wrong_predicted_image[1]}')","2672a10e":"import matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nrcParams['figure.figsize'] = 22 ,4\nfig, ax = plt.subplots(1,6)\n\ni = 0\nfor ele in correct_predicted_image[0]:\n    image = tf.keras.preprocessing.image.array_to_img(ele.reshape(150,150,3))\n    ax[i].imshow(image)\n    if correct_predicted_image[1][i] == 1:\n        ax[i].set_title(\"Classified as Pneumonia\")\n    else:\n        ax[i].set_title(\"Classified as Normal\")\n    i += 1\n\nprint(f'correct_prediction_by_model --- {correct_predicted_image[1]}')","dce7fd53":"# Table with the results with different amounts of epochs\n\nmytable = PrettyTable([\"Epochs\", \"Training Accuracy (%)\", \"Validation Accuracy (%)\"], \n                     title = \"CNN model accuracy under various epochs\") \n  \n# Add rows \nmytable.add_row([\"10\", \"0.91\", \"0.925\"])\nmytable.add_row([\"20\", \"0.90\", \"0.91\"]) \nmytable.add_row([\"30\", \"0.90\", \"0.899\"]) \nmytable.add_row([\"40\", \"90.87\", \"92.6\"]) \nmytable.add_row([\"50\", \"90.77\", \"92.5\"]) \n\nprint(mytable)\n\nval = histCNN.history['val_accuracy']\nsum(val) \/ len(val)\nacc = histCNN.history['accuracy']\nsum(acc) \/ len(acc)\n\n# 20 epochs\nAccuracy rate at train data: 0.8435546875\nAccuracy rate at test data: 0.90\n    \n# 30 epochs\nLoss rate at test data : 0.2690156102180481\nAccuracy rate at test data : 0.8990384340286255\nRecall rate at test data : 0.9307692050933838\n    \n# 40 epochs\nAccuracy rate at train data : 0.9076822916666667\nAccuracy rate at test data : 0.9262820482254028\nLoss rate at test data : 0.25253385305404663\nRecall rate at test data : 0.9589743614196777\nPrecision rate at test data : 0.9257425665855408\n\n# 50 epochs\nAccuracy rate at train data : 0.9087890625\nAccuracy rate at test data : 0.9246794581413269\nLoss rate at test data : 0.20562545955181122\nRecall rate at test data : 0.9564102292060852\nPrecision rate at test data : 0.92555832862854\n    \n# 100 epochs\nAccuracy rate at train data : 0.9427281504869461\nLoss rate at test data : 0.2646242678165436\nAccuracy rate at test data : 0.9038461446762085\nRecall rate at test data : 0.9230769276618958\nPrecision rate at test data : 0.9230769276618958","46ce99ba":"results = [test_result_CNN[0], test_result_CNN[1], test_result_CNN[2], \n          [test_result_VGG[0], test_result_VGG[1], test_result_VGG[2]], \n          [test_result_ResNet[0], test_result_ResNet[1], test_result_ResNet[2]]] \n\n\nloss = [test_result_CNN[0], test_result_VGG[0], test_result_ResNet[0]]\naccuracy = [test_result_CNN[1], test_result_VGG[1], test_result_ResNet[1]]\nrecall = [test_result_CNN[2], test_result_VGG[2], test_result_ResNet[2]]\nprecision = [test_result_CNN[3], test_result_VGG[3], test_result_ResNet[3]]\n\nprint(\"Loss: 1.CNN, 2.VGG16, 3.ResNet50:\", loss)\nprint(\"Accuracy: 1.CNN, 2.VGG16, 3.ResNet50:\", accuracy)\nprint(\"Recall: 1.CNN, 2.VGG16, 3.ResNet50:\", recall)\nprint(\"Precision: 1.CNN, 2.VGG16, 3.ResNet50:\", precision)\n","53098a72":"figure, axis = plt.subplots(1, 2, figsize=(18,5))\naxis = axis.ravel()\n\nfor i,element in enumerate(['accuracy', 'loss']):\n    axis[i].plot(histCNN.history[element])\n    axis[i].plot(histCNN.history['CNN' + element])\n    axis[i].plot(histVGG.history[element])\n    axis[i].plot(histVGG.history['VGG16' + element])\n    axis[i].plot(histResNet.history[element])\n    axis[i].plot(histResnet.history['ResNet50' + element])\n    axis[i].set_title('Model {}'.format(element))\n    axis[i].set_xlabel('epochs')\n    axis[i].set_ylabel(element)\n    axis[i].legend(['CNN', 'VGG16', 'ResNet50'])","b091cc05":"#model.save('PNP.h5')","0f69b82d":"# Deleting all the directories created in the below cell as it is only created to divide the dataset.\ntf.io.gfile.rmtree('\/kaggle\/working\/val_dataset\/')\ntf.io.gfile.rmtree('\/kaggle\/working\/train_dataset\/')","d843e90e":"#### Here we compile our model","b52961c4":"# Comparing the models\n* Since this is a medical diagnosis case: The accuracy cannot be the only metric to evaluate the models.\n* In medical daignosis it is very important to correctly predict the true values.\n* We cannot incorrectly diagnose a patient as having healthy lungs (normal x-ray) after the true report of diagnosis shows that patient has pneumonia (High sensitivity: True Positives).\n* So along with the highest accuracy we also need the highest recall.\n\n**!! This was the case without regularization !!**\n### Our CNN Model performed best with an accuracy of 0.8766 and a recall of 0.9949 on the test set so we will have to save this model","7affcb30":"### Custom Directory\n\nIn the cell below, we will copy all the files from input directory -> custom directory\n- From the train_images list, transferring all the files to train_dataset directory.\n- From the val_images list, transferring all the files to val_dataset directory","0daef963":"## Predict and Evaluate on Test Data","91855ad2":"### Experts versus AI\n\nDespite the fact that pneumonia is the most common cause of serious illness and death in young children worldwide, our ability, as clinicians, to infer an infectious pathological process in the lung from specific features of the history and examination is poor (Scott et al., 2012).\n\nMisdiagnosis, arbitrary charges, annoying queues, and clinic waiting times among others are long-standing phenomena in the medical industry across the world. These factors can contribute to patient anxiety about misdiagnosis by clinicians. However, with the increasing growth in use of big data in biomedical and health care communities, the performance of artificial intelligence (Al) techniques of diagnosis is improving and can help avoid medical practice errors (Daniel et al., 2017).\n\nSome research on medical image classification by CNN has achieved performances rivaling human experts. Deep learning can be used to leverage clinician activities in different domains and applications, such as disease risk prediction, personalized prescriptions, treatment recommendations, clinical trial recruitment as well as research and data analysis. As an example, Wang et al. recently won the Parkinson\u2019s Progression Marker\u2019s Initiative data challenge on subtyping Parkinson\u2019s disease using a temporal deep learning approach (Fox, 2016). Another example, CheXNet, a CNN with 121 layers trained on a dataset with more than 100,000 frontal-view chest X-rays (ChestX-ray 14), achieved a better performance than the average performance of four radiologists (Shen et al., 2019).\n\nMost of the experts got high sensitivity but low specificity, while the CNN-based system got high values on both sensitivity and specificity (Nagendran et al., 20120). Moreover, on the average weight error measure, the CNN-based system exceeds two human experts (Liu et al., 2019).\n\nThe development of diverse AI techniques has contributed to early detections, disease diagnoses, and referral management. In addition, more than half of a randomized population group (55.8%: 428 out of 767) opted for AI diagnosis regardless of the description of the clinicians (Liu et al., 2021). \n\n** **\n- _Daniel, P., Bewick, T., Welham, S., Mckeever, T. M., & Lim, W. S. (2017). Adults miscoded and misdiagnosed as having pneumonia: results from the British Thoracic Society pneumonia audit. Thorax, 72(4), 376\u2013379. https:\/\/doi.org\/10.1136\/thoraxjnl-2016-209405_\n- _Jyotiyana, M., & Kesswani, N. (2019). Deep Learning and the Future of Biomedical Image Analysis. Studies in Big Data, 329\u2013345. https:\/\/doi.org\/10.1007\/978-3-030-33966-1_15_\n- _Liu, X., Faes, L., Kale, A. U., Wagner, S. K., Fu, D. J., Bruynseels, A., Mahendiran, T., Moraes, G., Shamdas, M., Kern, C., Ledsam, J. R., Schmid, M. K., Balaskas, K., Topol, E. J., Bachmann, L. M., Keane, P. A., & Denniston, A. K. (2019). A comparison of deep learning performance against health-care professionals in detecting diseases from medical imaging: a systematic review and meta-analysis. The Lancet Digital Health, 1(6), e271\u2013e297. https:\/\/doi.org\/10.1016\/s2589-7500(19)30123-2_\n- _Liu, T., Tsang, W., Huang, F., Lau, O. Y., Chen, Y., Sheng, J., Guo, Y., Akinwunmi, B., Zhang, C. J., & Ming, W. K. (2021). Patients\u2019 Preferences for Artificial Intelligence Applications Versus Clinicians in Disease Diagnosis During the SARS-CoV-2 Pandemic in China: Discrete Choice Experiment. Journal of Medical Internet Research, 23(2), e22841. https:\/\/doi.org\/10.2196\/22841_\n- _Nagendran, M., Chen, Y., Lovejoy, C. A., Gordon, A. C., Komorowski, M., Harvey, H., Topol, E. J., Ioannidis, J. P. A., Collins, G. S., & Maruthappu, M. (2020). Artificial intelligence versus clinicians: systematic review of design, reporting standards, and claims of deep learning studies. BMJ, m689. https:\/\/doi.org\/10.1136\/bmj.m689_\n- _Scott, J. A. G., Wonodi, C., Mo\u00efsi, J. C., Deloria-Knoll, M., DeLuca, A. N., Karron, R. A., Bhat, N., Murdoch, D. R., Crawley, J., Levine, O. S., O\u2019Brien, K. L., & Feikin, D. R. (2012). The Definition of Pneumonia, the Assessment of Severity, and Clinical Standardization in the Pneumonia Etiology Research for Child Health Study. Clinical Infectious Diseases, 54(suppl_2), S109\u2013S116. https:\/\/doi.org\/10.1093\/cid\/cir1065_\n- _Shen, J., Zhang, C. J. P., Jiang, B., Chen, J., Song, J., Liu, Z., He, Z., Wong, S. Y., Fang, P. H., & Ming, W. K. (2019). Artificial Intelligence Versus Clinicians in Disease Diagnosis: Systematic Review. JMIR Medical Informatics, 7(3), e10010. https:\/\/doi.org\/10.2196\/10010_\n- _Fox, M. J. (2016). Subtyping Parkinson\u2019s Disease with Deep Learning Models (2016 PPMI Data Challenge Winner). The Michael J. Fox Foundation for Parkinson\u2019s Research | Parkinson\u2019s Disease. Geraadpleegd op 29 november 2021, van https:\/\/www.michaeljfox.org\/grant\/subtyping-parkinsons-disease-deep-learning-models-2016-ppmi-data-challenge-winner?grant_id=1518_","b349e1f1":"## Visualise the VGG Model Performance","0d0ded92":"### Creating Directories\n\n- Creating directory in Kaggle\/working directory for training dataset and validation dataset after division of 80:20.\n- In both directory there is more two directory i.e. negative and positive\n- The directory structure is same as the structure in Input directory.","4f9b0695":"### Preparing the Data for Training\nThe given dataset has training, test and validation\/evaluation set, but test set only has 16 images whereas training set has 5216 images. To generate test samples we will do the following:  \n* So first we need to create a proper distribution set with 80% as training data and 20% as validation data.\n* So we'll merge training and validation set and then split them in the ratio of 80:20 repectively.\n\nWe used inspiration form [this](https:\/\/www.kaggle.com\/abhishekdhule\/pneumonia-detection-resnet-inception-tensorflow?scriptVersionId=40311825) and [this](https:\/\/www.kaggle.com\/csk03012\/vgg-91-inceptionnet-90-to-classify-pneumonia) notebook for the Preparing the data.\n","8804eddc":"### Importing and loading pretrained ResNet50 model","a1d22988":"## <font color='darkblue'>  Introduction of the problem <\/font>\n\n### What is Pneumonia?\n\nPneumonia is a form of acute respiratory infection that affects the lungs. The lungs are made up of small sacs called alveoli, which fill with air when a healthy person breathes. When an individual has pneumonia, the alveoli are filled with pus and fluid, which makes breathing painful and limits oxygen intake.\n\nPneumonia is the single largest infectious cause of death in children worldwide. Pneumonia killed 740 180 children under the age of 5 in 2019, accounting for 14% of all deaths of children under five years old but 22% of all deaths in children aged 1 to 5. Pneumonia affects children and families everywhere, but deaths are highest in South Asia and sub-Saharan Africa (World Health Organisation, 2021).\n\n\n![https:\/\/www.svhlunghealth.com.au\/Images\/UserUploadedImages\/3447\/4_SVH_Lung_Health_Pneumonia_final_1080p.jpg](https:\/\/www.svhlunghealth.com.au\/Images\/UserUploadedImages\/3447\/4_SVH_Lung_Health_Pneumonia_final_1080p.jpg)\n\n\n\n### The Importance of Diagnosing Pneumonia?\n\nThe risk of pneumonia is immense for many, especially in developing nations where billions face energy poverty and rely on polluting forms of energy. Over 150 million people get infected with pneumonia on an annual basis especially children under 5\u2009years old. In such regions, the problem can be further aggravated due to the dearth of medical resources and personnel. For example, in Africa\u2019s 57 nations, a gap of 2.3 million doctors and nurses exists. For these populations, accurate and fast diagnosis means everything. It can guarantee timely access to treatment and save much needed time and money for those already experiencing poverty (Stephen, Sain, Maduh, & Jeong, 2019).\n\n\n****\n- _World Health Organisation. (2021, 11th of November). Pneumonia. Consulted on 13th of December 2021, van https:\/\/www.who.int\/news-room\/fact-sheets\/detail\/pneumonia_\n- _Stephen, O., Sain, M., Maduh, U. J., & Jeong, D. U. (2019). An Efficient Deep Learning Approach to Pneumonia Classification in Healthcare. Journal of Healthcare Engineering, 2019, 1\u20137. https:\/\/doi.org\/10.1155\/2019\/4180949_\n\n","dab4a970":"## <font color='darkblue'> Exploratory Data Analysis <\/font>\n\n### Visualizing the Data\n\nWe start with looking at some chest x-rays to get a better idea about the differences between lungs suffering from pneumonia and normal lungs. When interpreting the x-ray, the radiologist will look for white spots in the lungs (called infiltrates) that identify an infection. The edges of the lung especially close to the diaphragm won't be clearly visible when there is an infection present. The same goes for the edges around the hearth and the aorta. In a study where they used chest x-rays to determine Community acquired pneumonia, there was a diagnostic accuracy of 93.1% (BRON), this is a benchmark for our model.\n\nWe used [this](https:\/\/www.kaggle.com\/donpiano\/keras-resnet-50-for-pneumonia-x-ray-images) Notebook for the visualisation\n","e5dcb75f":"#### Model loss seems to indicate a good fit, whereas Model Accuracy seems to indicate overfitting. Although it does follow the line, but it has a lot of deep spikes below and these differences between the train line and validation line in those spiked downward peeks indicate overfitting. ","460f6d75":"### Plot images that were correctly predicted by our CNN Model\n\nHere we see 6 images that were correctly classified by our CNN model.","3d6eb708":"### Code for plotting the incorrect and correct prediction images","b974f3c5":"### How do doctors distinguish between healthy and unhealthy lungs?\n\n__Figure 1.__ shows examples of chest X-rays from the dataset. The normal chest X-ray (left panel) depicts clear lungs without any areas of abnormal opacifcation in the image. Bacterial pneumonia (middle) typically exhibits a focal lobar consolidation, in the right upper lobe, whereas viral pneumonia (right) manifests with a more difuse interstitial pattern in both lungs (Kermany et al., 2018).\n\n\n\n![https:\/\/miro.medium.com\/max\/4800\/1*t-_EXQ3tlb8KOx6H7HN09A.jpeg](https:\/\/miro.medium.com\/max\/4800\/1*t-_EXQ3tlb8KOx6H7HN09A.jpeg)\n**Figure 1**\n\n\n\n** **\n_Kermany, Daniel; Zhang, Kang; Goldbaum, Michael (2018), \u201cLabeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification\u201d, Mendeley Data, V2, doi: 10.17632\/rscbjbr9sj.2_","c0d89f39":"### Code for plotting the incorrect and correct prediction images of the VGG model.","2f6a3488":"## <font color='darkblue'>  Import Libraries and Data <\/font>\nWe prepare the data which we use in our Convolutional Neural Network. The Chest X-ray data is given into three saperate folders: train, val, and test. Run following cell to set dataset path and other few variables which are used by ImageDataGenerator in next step.\n\n\n#### Our data is located in three folders\n   \n1. **Train:** the folder that contains the training images for training our model.\n2. **Val:** the folder that contains images which we will use to validate our model. A validation dataset is has its purpose to prevent our model from **Overfitting**. Overfitting is when the loss is not as low as it could be because the model learned too much noise. Therefore it can't handle data it hasn't see too well.\n3. **Test:** this folder contains the data that we use to test the model once it has learned the relationships between the images and their label (Pneumonia versus Not-Pneumonia)\n","edff2e74":"### Visualise our CNN Model Performance","e19c22b9":"## Finetune the model\n\nTo further finetune our CNN model, we used ModelCheckpoint to save the best weights of our CNN in order to plot the correctly and incorrectly predicted images which will be later in this Notebook, and so that we don't have to train the model again. This will save us time in the future.\n\nWe did not see a reason to use early stoppings. We did in the beginning, but this had a large influence on the accuracy since it stopped around 8 epochs. In the article from Stepehn and colleagues (2019) and Ikechukwu and colleagues (2021) they also did not use it, so therefore we decided to not use it in the end. \n\n\n\nFor our purposes, we'll use Keras callbacks to further finetune our model. The checkpoint callback saves the best weights of the model, so next time we want to use the model, we do not have to spend time training it. The early stopping callback stops the training process when the model starts becoming stagnant, or even worse, when the model starts overfitting. Since we set restore_best_weights to True, the returned model at the end of the training process will be the model with the best weights (i.e. low loss and high accuracy).\n\n#### Regularization\nIn order to prevent our model to overtrain we implement the following regularization measures. We used the ModelCheckpoint in order to plot the correctly and incorrectly predicted images later in this Notebook. We used [this](https:\/\/www.kaggle.com\/csk03012\/vgg-91-inceptionnet-90-to-classify-pneumonia) notebook for this.\n\n\n** **\n- _Stephen, O., Sain, M., Maduh, U. J., & Jeong, D. U. (2019). An Efficient Deep Learning Approach to Pneumonia Classification in Healthcare. Journal of Healthcare Engineering, 2019, 1\u20137. https:\/\/doi.org\/10.1155\/2019\/4180949_","dfe50d2f":"### Plot images that were incorrectly predicted by the VGG16 model\n\nHere we can see 6 images that were incorrectly classified by the VGG16 model. A difference we can see with wrongfully classified images from our model is that these also contain images classified as pneumonia whilst being normal.","a2fe6d4c":"### Plot images that were correctly predicted by The ResNet50 model\n\nThese 6 images are an example of images that were correctly classified by the model","470b50d2":"## Correction for Data Imbalance\n\nWe used [this](https:\/\/www.kaggle.com\/amyjang\/tensorflow-pneumonia-classification-on-x-rays) and [this](https:\/\/www.kaggle.com\/csk03012\/vgg-91-inceptionnet-90-to-classify-pneumonia) notebook for the correction for data imbalance.","55972e05":"# <font color='blue'>  The Challenge: Our CNN Model vs. Pretrained model <\/font>\nBuild an algorithm to automatically identify whether a patient is suffering from pneumonia or not by looking at chest X-ray images and compare the performance with a pretrained VGG-16 and ResNet50 model. We want to achieve that the algorithms are extremely accurate because lives of people are at stake.\n\n** **\n- _Victor Ikechukwu, A., Murali, S., Deepu, R., & Shivamurthy, R. (2021). ResNet-50 vs VGG-19 vs training from scratch: A comparative analysis of the segmentation and classification of Pneumonia from chest X-ray images. Global Transitions Proceedings, 2(2), 375\u2013381. https:\/\/doi.org\/10.1016\/j.gltp.2021.08.027_","1e7c649f":"#### This summary is a great way for us to see how our CNN is being set up","1e7c50d4":"# <a id=\"top\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Table of content<\/h3>\n\n<font color=\"darkblue\" size=+1><b>Introduction of the Problem<\/b><\/font>\n* [1. What is Peumonia? ](#1)\n* [2. The Importance of Diagnosing Pneumonia ](#2)\n* [2. How do doctors distinguish between healthy and unhealthy lungs? ](#3)\n* [4. Experts versus AI ](#4)    \n    \n<font color=\"darkblue\" size=+1><b>The Data<\/b><\/font>\n* [1. The X-Ray Images ](#5)\n* [2. Import Libraries and Data](#6)\n\n<font color=\"darkblue\" size=+1><b> Exploratory Data Analysis <\/b><\/font>\n* [1. Visualise the Data ](#6)\n* [1. Prepare the Data for Training ](#6)\n* [2. Create Directories ](#7)\n* [3. Custom Directory ](#8)\n    \n<font color=\"darkblue\" size=+1><b> Convolutional Neural Network From Scratch <\/b><\/font>\n* [1. CNN Model: The Architecture ](#10)\n* [2. Creating Data Generators ](#11)\n* [4. Correction for Data Imbalance ](#13)\n* [3. Finetune CNN Model ](#12)\n* [4. Train CNN Model ](#13)\n* [3. Visualise Results ](#12)\n\n<font color=\"darkblue\" size=+1><b> VGG-16 <\/b><\/font>\n* [1. VGG-16 Model: The Architecture ](#10)\n* [2. Creating Data Generators ](#11)\n* [4. Correction for Data Imbalance ](#13)\n* [3. Finetune CNN Model ](#12)\n* [4. Train CNN Model ](#13)\n* [3. Visualise Results ](#12)\n\n<font color=\"darkblue\" size=+1><b> ResNet-50 <\/b><\/font>\n* [1. CNN Model: The Architecture ](#10)\n* [2. Creating Data Generators ](#11)\n* [4. Correction for Data Imbalance ](#13)\n* [3. Finetune CNN Model ](#12)\n* [4. Train CNN Model ](#13)\n* [3. Visualise Results ](#12)\n\n<font color=\"darkblue\" size=+1><b> Others <\/b><\/font>\n* [1. Interpret the results](#14)\n* [2. Prediction Using Trained Model](#15)\n* [3. Save and Load Model](#16)\n* [4. Sources](#17)","e38e1d0b":"# Pretrained Model: ResNet-50\nResnet50 is a deep residual network developed by (He at al., 2016) and is a subclass of convolutional neural networks used for image classification. It is the winner of ILSVRC 2015. The principal innovation is the introducing of the new architecture network-in-network using residual layers. The Renset50 consists of five steps each with a convolution and Identity block and each convolution block has 3 convolution layers and each identity block also has 3 convolution layers. Resnet50 has 50 residual networks and accepts images of 224 \u00d7 224 pixels. \n\nWe again fine-tuned the ResNet-50, i.e., for Dense layers we again found 512 to be having the best result, and a Dropout of 0.2 also had the best result. \n\nWe used the article of (Victor, Murali, Deepu & Shivamurthy, 2021) as inspiration for the ResNet-50 model\n * We'll use a pre-trained model provided by keras and add some layers on the top.\n * Since it takes 224 x 244 pixels, we changed the hyperparameters. \n \n** **\n- _He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep Residual Learning for Image Recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). https:\/\/doi.org\/10.1109\/cvpr.2016.90_\n- _Victor Ikechukwu, A., Murali, S., Deepu, R., & Shivamurthy, R. (2021). ResNet-50 vs VGG-19 vs training from scratch: A comparative analysis of the segmentation and classification of Pneumonia from chest X-ray images. Global Transitions Proceedings, 2(2), 375\u2013381. https:\/\/doi.org\/10.1016\/j.gltp.2021.08.027_","61b9a93f":"### Plot images that were incorrectly predicted by our CNN Model\n\nHere we see 6 images that were incorrectly classified by our CNN model. We see that all examples were classified as pneumonia when they were actually normal x-rays.","f73aa6a9":"Below code block shows that the number of layers for base model (base_VGG) which is 19 and out of 19 we chose to freeze all 19 layers while training. This means we will not be training those 19 freezed layers.","f61ebb1f":"## Predict and Evaluate on the Test Data","2d2865ea":"This happens when you use Dropout, since the behaviour when training and testing are different.\n\nWhen training, a percentage of the features are set to zero (50% in your case since you are using Dropout(0.5)). When testing, all features are used (and are scaled appropriately). So the model at test time is more robust - and can lead to higher testing accuracies.","61da7b0e":"## Pretrained Model: VGG16\n\nProposed on 2014 by Simonyan and Zisserman VGG (Visual Geometry Group) is a convolution\nneural net (CNN) architecture and used to win ILSVR(ImageNet) competition in 2014 (K. Simonyan\nand A. Zisserman, 2014). VGG16 takes 224 x 244, since it was originally trained on 224\u00d7224 images. \n\nAll parameters below we found by experimenting many times while fine-tuning the model. i.e for Dense layers, it was trained with different no. of neurons (4096, 1024, 512, 256) but the best result found by taking 512 neurons for Dense layers and using Dropout 0.2 gave us the best result as compared to 0.3, 0.4 or 0.5 during the fine tuning. \n\nWe used different ImageDataGenerator parameters because we saw the model was very much overfitting on the validatio. data, and the loss was following the data very poorly. We saw that in the research of Ikechukwu and colleagues (2021) that different ImageDataGenerator parameters were used for the CNN model as compared to the VGG16 and ResNet Model. We used their research as inpiration for the parameters.\n\nWe used inspiration from [this](https:\/\/www.kaggle.com\/vnbhat\/pneumonia-detection-resnet50-vgg16) Notebook.\n\n** **\n- _Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image recognition. CoRR, abs\/1409.1556, 2014._","7435d288":"## 1. Our CNN Model Trained from Scratch\n\nHere we create the structure of our model, we use the following layers: based on [this](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2666285X21000558) article and [this](https:\/\/github.com\/abhinavsagar\/kaggle-notebooks\/blob\/master\/Chest_X-Ray.ipynb) github file.\n- Data augmentation to take overfitting into account\n- Normalization via rescaling of the datasets\n- Resizing the data with image_size","ec5f7000":"### Plot images that were correctly predicted by The VGG16 model\n\nHere we can see 6 images that were correctly classified by the VGG16 model.","165d5afe":"### Visualise Model Performance ResNet Model","68228add":"### Plot images that were incorrectly predicted by The ResNet50 model\n\nHere we can see 6 images that were incorrectly classified by the model.","60675450":"### Train CNN Model\nNow is the time to train out model!","dc43a10d":"### Code for plotting the incorrect and correct prediction images","0df53b68":"## Predict and Evaluate on Test Data","00a75e1c":"### Creating Data Generators \nThe dataset is highly imbalance with more of pneumonia cases versus normal cases, hence data augmentation was used to balance the dataset, thereby eliminated the possibility of overfitting the model. We used [this](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2666285X21000558) research for our Data Augmentation","5ff216ba":"## <font color='darkblue'>  Create and Train Model From Scratch <\/font>\n\n### CNN Model: The Architecture\n\nOur architecture for the CNN has been inspired by the article from Stephan and colleagues (2019), Yadav and Sjadav (2019), Ikechukwu and collegues (2021), and this [article](https:\/\/towardsdatascience.com\/deep-learning-for-detecting-pneumonia-from-x-ray-images-fc9a3d9fdba8). Their neural network architectures were specifically designed for pneumonia image classification tasks. The proposed architecture consists of the convolution, max-pooling, and classification layers combined together. We will now dive into each component and why we chose them.\n\n![https:\/\/miro.medium.com\/max\/2656\/1*18A5bLKeQKCRuOIT17pT8A.png](https:\/\/miro.medium.com\/max\/2656\/1*18A5bLKeQKCRuOIT17pT8A.png)\n\n#### Input Layer\nSince the architecture is like that of VGG Networks, the input image must be resized to the standard 224 \u00d7 224 pixels, this was needful because medical images like an X-ray, when taken from different devices come in various sizes. For efficient preprocessing, the images must be resized to 224 \u00d7 224 \u00d7 3 depicting the width, height, and channel numbers (3 for RGB) respectively.\n\n#### Convolutional Layer\nThis is the most important layer in our proposed CNN model, as it is where majority of the computations would be done. This layer's main job is to retrieve features from the image while keeping the spatial relationship between image pixels intact. This is accomplished by utilising a series of filters to learn the retrieved features. In our study, a two-dimensional convolution was performed using 10 filters, each of which was built using a 7 * 7 filter size. In addition, the filters move along the input images, calculating the dot product function, also known as convolved features.\n\n#### Batch Normal Layer\nBatch normalisation is a technique for training very deep neural networks that standardises each mini-inputs batches to a layer. This stabilises the learning process and significantly reduces the number of training epochs needed to create deep networks. It is a transformation that keeps the mean output close to 0 and the standard deviation of the output close to 1 the following hyperparameters (epsilon = 0.001 & momentum = 0.99) was used for training our model.\n\n#### ReLU Layer\nThis layer is responsible for replacing all negative values to zero while allowing positive numbers to assume their respective values from the convolved features, thereby introduces non-linearity in the feature map.\n\n#### Fully Connected (FC) Layer\nAll the activation functions from the preceding layer are related to the neurons in this layer. This layer's primary function in this study is to classify the returned convolved features from dataset images into their respective classes.\n\n#### SoftMax Layer\nAfter the fully connected layers, a proper interpretation of the probabilities is needed, and that is the function of the SoftMax layer. It simply classifies the values between \u20180\u2019 and \u20181\u2019 or 0% and 100%.\n\n#### Output Layer\nThe final layer, consisting of the two classes (Normal and Pneumonia) is presented at this layer.\n\n#### The pretrained models (VGG-16 and ResNet-50)\nVGG-16 and ResNet50: We conducted two experiments using pre-trained models as it is easier to fine-tune the parameters unlike a network trained from scratch.\n\n\n\n\n\n\n\n\n\n### <font color='darkblue'>  Hyperparameters <\/font>\n  \n### Pooling\nWe performed pooling to reduce the dimensionality. This enables us to reduce the number of parameters, which both shortens the training time and combats overfitting. Pooling layers downsample each feature map independently, reducing the height and width, keeping the depth intact.\nWe used **Max Pooling** by taking the max value in the pooling window. We saw it as important for downsampling the feature map while keeping the important information. The max-pooling layer of the convolutional neural network is effective in variant shape absorptions and comprises sparse connections in conjunction with tied weights.\n\n### Fully Connected\nAfter the convolution and pooling layers we add a couple of fully connected layers to wrap up the CNN architecture. \n\n## <font color='darkblue'>  Improve performance by preventing overfitting: <\/font>\nWe used various various strategies to increase the performance of image classifcation by preventing overfitting: \n\n**\"Only a network model with proper size and other effective methods preventing overfit, such as proper dropout rate and proper data augmentation, can get the best results.\" (Yadav & Jadhav, 2019)** \n\n### Data Augmentation\nWe employed several data augmentation methods to artificially increase the size and quality of the dataset. The idea is to alter the training data with small transformations to reproduce the variations. This process helps in solving overfitting problems and enhances the model\u2019s generalization ability during training (Stephen, Sain, Maduh, & Jeong, 2019). That is because augmentation geometrically transforms the picture, which facilitates the machine learning algorithm to learn the underground feature without the impact of rotation and scale (Yadav & Jadhav, 2019). \n\nIn addition, data augmentation can be much more helpful when the dataset is imbalanced (yadav & Jadhav, 2019) which is the case here. We can generate different samples of the undersampled class in order to try to balance the overall distribution. \n\n### Dropout\nDropout is used to prevent overfitting by temporarily \u201cdropping\u201d a neuron during training time at each iteration with probability p. Which means that all the inputs and outputs to this neuron will be disabled at the current iteration. The dropped-out neurons are resampled with probability p at every training step, so a dropped out neuron at one step can be active at the next one. The hyperparameter p is called the dropout-rate and we set it to 0.5, corresponding to 50% of the neurons being dropped out which is proposed as the best option for X-ray image classification (yadav & Jadhav, 2019).\n\nIt has to search for broad, general patterns, whose weight patterns tend to be more robust. It was found to be of high importance when it comes to classification of X-ray images (Stephan et al., 2019). \n\n### Batch Normalization\nUse batch norm with convolutions. As the network becomes deeper, batch normalization start to play an important role, to speed up the training procedure and reduce overfitting.\nThe idea is to alter the training data with small transformations to reproduce the variations, which makes learning more stable and quicker (Yadav & Jadhav, 2019).\n\n### <font color='darkblue'>  Regularization: <\/font>\nIn order to prevent our model to overtrain we implement the following regularization measures: EarlyStopping, ReduceLROnPlateau, ModelCheckpoint which was found to be very effective for X-ray image classification (Singh, Kumar, Yadav, & Kaur, 2020).\n\n- The early stopping callback stops the training process when the model starts becoming stagnant, or even worse, when the model starts overfitting. \n- We have adopted \"ReduceLROnPlateau\" as a Keras callback function to reduce the learning rate when the result stops improving. The learning rate will gradually reduce by a factor of 0.3. This function also helps the network to reduce the overfitting problem.\n- The checkpoint callback saves the best weights of the model, so next time we want to use the model, we do not have to spend time training it. \n\n### Training\nCNN is trained using backpropagation with gradient descent.\n\n** **\n- _Yadav, S. S., & Jadhav, S. M. (2019). Deep convolutional neural network based medical image classification for disease diagnosis. Journal of Big Data, 6(1). https:\/\/doi.org\/10.1186\/s40537-019-0276-2_\n- _Singh, D., Kumar, V., Yadav, V., & Kaur, M. (2020). Deep Neural Network-Based Screening Model for COVID-19-Infected Patients Using Chest X-Ray Images. International Journal of Pattern Recognition and Artificial Intelligence, 35(03), 2151004. https:\/\/doi.org\/10.1142\/s0218001421510046_\n- _Stephen, O., Sain, M., Maduh, U. J., & Jeong, D. U. (2019). An Efficient Deep Learning Approach to Pneumonia Classification in Healthcare. Journal of Healthcare Engineering, 2019, 1\u20137. https:\/\/doi.org\/10.1155\/2019\/4180949_\n- _Victor Ikechukwu, A., Murali, S., Deepu, R., & Shivamurthy, R. (2021). ResNet-50 vs VGG-19 vs training from scratch: A comparative analysis of the segmentation and classification of Pneumonia from chest X-ray images. Global Transitions Proceedings, 2(2), 375\u2013381. https:\/\/doi.org\/10.1016\/j.gltp.2021.08.027_\n","2b3421fc":"## <font color='darkblue'> The Data <\/font>\n\n### The X-Ray Images\nA total of 5,856 X-ray images of anterior-posterior chests were carefully chosen from retrospective pediatric patients between 1 and 5\u2009years old. The dataset contains two kinds of chest X-ray Images: NORMAL and PNEUMONIA, which are stored in three folders. In the PNEUMONIA folder, two types of specifc PNEUMONIA can be recognized by the fle name: BACTERIA and VIRUS.\n\n__Table 1.__ describes the composition of the dataset. The training dataset contains 5232 X-ray images, while the testing dataset contains 624 images. In the training dataset, the image in the NORMAL class only occupies one-fourth of all data. In the testing dataset, the PNEUMONIA consists of 62.5% of all data, which means the accuracy of the testing data should be higher than 62.5%.\n\n\n** **\n- _Kermany, Daniel; Zhang, Kang; Goldbaum, Michael (2018), \u201cLabeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification\u201d, Mendeley Data, V2, doi: 10.17632\/rscbjbr9sj.2_\n\n\n**_The dataset is highly imbalance with more of pneumonia cases versus normal cases, hence data augmentation was used to balance the dataset, thereby eliminated the possibility of overfitting the model._**"}}