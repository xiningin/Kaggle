{"cell_type":{"27a4fc68":"code","ddf79808":"code","e0aa5601":"code","184d2bdc":"code","59ab4200":"code","3d9358d2":"code","3df37720":"code","d7b89c57":"code","624bbff7":"code","374e3ddf":"code","85485fe4":"code","adbb6ba3":"code","12d344e4":"code","815032fb":"code","b0018836":"code","aaad4faa":"code","25feff42":"code","1cdcb0b5":"code","4d89ff6b":"code","7e2c6534":"code","c3011b84":"code","819d28ad":"code","f7fa6ee3":"code","f4f70219":"code","d15f2163":"code","5f661a62":"code","e9cdcb1a":"code","06089554":"code","d36cd0a8":"code","3c7f8715":"code","917ba21d":"code","b52c3484":"code","5771fe08":"code","a47718ab":"code","f7218080":"code","05fe56bc":"code","3fa0716c":"code","50681000":"code","c4642bc7":"code","ec49cc8c":"code","100f0eac":"code","bcd381ae":"code","5898e0b3":"code","04aa9063":"code","6c8300bd":"code","14c962b0":"code","6440755a":"code","19d6e98c":"code","7b035b1f":"code","6d4e0907":"code","70572acf":"code","42e9b338":"code","29203b7e":"code","719b228c":"markdown","257f654c":"markdown","51cd3c5f":"markdown","c49c1961":"markdown","8c4344aa":"markdown","b9e982b2":"markdown","160a9120":"markdown","9fc0e656":"markdown","00eb63bc":"markdown","a1bdc154":"markdown","b6c14b30":"markdown","b9271e3a":"markdown","9c4fe338":"markdown","e74908fa":"markdown","fd6852a9":"markdown","fdf1385a":"markdown","5a838e0e":"markdown","78bce572":"markdown","47517f0c":"markdown","784be793":"markdown","86299aab":"markdown","69e7427e":"markdown","bd5eee1f":"markdown","8c303d75":"markdown","9195b422":"markdown","9fdd4809":"markdown","a18ed46f":"markdown","86ab2ba9":"markdown","56c2770b":"markdown","52e23b5a":"markdown","890e0729":"markdown","8f2d2803":"markdown","48ebdaa5":"markdown","306a0073":"markdown","23d1cf8f":"markdown","ec7deec0":"markdown","af9eea79":"markdown","49db929f":"markdown"},"source":{"27a4fc68":"# import necessary libraries\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.metrics import classification_report","ddf79808":"# create paths to training and test data\ntrain_path = '\/kaggle\/input\/titanic\/train.csv'\ntest_path = '\/kaggle\/input\/titanic\/test.csv'\n\n# target label in dataset\ntarget = 'Survived'\n\n# to use PassengerId as index\nindex = 'PassengerId'\n\n# define explicit datatypes for each attribute\ndtypes = ({\n    'Survived': bool, \n    'Pclass':   'category', \n    'Name':     'category',\n    'Sex':      'category',\n    'Age':      float,\n    'SibSp':    int,\n    'Parch':    int,\n    'Ticket':  'category',\n    'Fare':     float,\n    'Cabin':   'category',\n    'Embarked':'category'\n})\n\n# create seperate dataframes that hold the training and test sets\ntitanic = pd.read_csv(train_path, index_col=index, dtype=dtypes)\ntitanic_test = pd.read_csv(test_path, index_col=index, dtype=dtypes)","e0aa5601":"# look at top of data\ntitanic.head()","184d2bdc":"# look at bottom of data\ntitanic.tail()","59ab4200":"# look at overall structure of data\ntitanic.info()","3d9358d2":"# look at descriptive statistics for numerical features\nnp.round(titanic.describe(),3)","3df37720":"# total number of people who survived vs. died\ntitanic['Survived'].value_counts()","d7b89c57":"# features to plot in the count plots\nattributes = ['Survived', 'Sex', 'Pclass', 'Embarked', 'SibSp', 'Parch']\n\n# plot countplots \nplt.figure(figsize=(14, 12), dpi=100)\nfor i, feature in enumerate(attributes):\n    plt.subplot(3, 3, i+1)\n    sns.countplot(data=titanic, x=feature, hue='Survived')\n    \nsns.despine()","624bbff7":"# pivot table that shows mean % of survival for each pclass split by gender\nnp.round(titanic.pivot_table(values=['Survived'], index=['Sex'], columns=['Pclass'], aggfunc=np.mean), 3)","374e3ddf":"# pivot table that shows mean % of survival for each port of embarkation split by gender\nnp.round(titanic.pivot_table(values=['Survived'], index=['Sex'], columns=['Embarked'], aggfunc=np.mean), 3)","85485fe4":"# create new feature: age bins\nbins = [0., 10., 20., 30., 40., 50., 60., np.inf]\ntitanic['age_cat'] = np.searchsorted(bins, titanic['Age'].values)","adbb6ba3":"# pivot table that shows mean % of survival for each pclass split by gender\nnp.round(titanic.pivot_table(values=['Survived'], index=['Sex'], columns=['age_cat'], aggfunc=np.mean), 3)","12d344e4":"# look at first 5 names to identify pattern in strings\ntitanic['Name'].iloc[0:5]","815032fb":"# crate function to extract a passengers title from name feature\ndef get_title(name):\n    return re.split(',|\\.', name)[1].strip()\n\n# create new title feature\ntitanic['title'] = titanic['Name'].apply(get_title)","b0018836":"# create dataframe that shows titles along with counts and mean survival rate\ntitles = titanic.groupby('title').agg({'Survived': ['count', 'mean']})\n\n# drop top level of multi-layer index from above\ntitles.columns = titles.columns.droplevel()\n\n# sort data frame by mean survival rate from highest to lowest\ntitles.sort_values('mean', ascending=False)","aaad4faa":"# list of titles to replace with 'other'\nuncommon_titles = (['Don', 'Rev', 'Dr', 'Mme', 'Ms', 'Major', \n                    'Lady', 'Sir', 'Mlle', 'Col', 'Capt', \n                    'the Countess', 'Jonkheer'])\n\n# replace uncommon titles with 'other'\ntitanic['title'] = titanic['title'].replace(uncommon_titles, 'other')","25feff42":"# count plot of titles showing portion of those that died\nsns.catplot(x='title', kind='count', hue='Survived', data=titanic, height=7)\nplt.tick_params(labelsize=12)\nplt.ylabel('Count', fontsize=14)\nplt.xlabel('Title', fontsize=14)\nplt.title('Survival by Title', fontdict={'fontsize':16});","1cdcb0b5":"# create new feature: family size\ntitanic['fam_size'] = titanic['SibSp'] + titanic['Parch']","4d89ff6b":"# count plot of family sizes showing portion of those that died\nsns.catplot(x='fam_size', kind='count', hue='Survived', data=titanic, height=8)\nplt.tick_params(labelsize=12)\nplt.ylabel('Count', fontsize=14)\nplt.xlabel('Family Size', fontsize=14)\nplt.title('Survival by Family Size', fontdict={'fontsize':16});","7e2c6534":"# create new feature: deck\ntitanic['deck'] = titanic['Cabin'].apply(lambda x: x[0])","c3011b84":"# plot countplot of those who survived vs. died for each deck\nsns.catplot(y='deck', hue='Survived', kind='count', data=titanic, \n            order=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'T'], height=7)\nplt.tick_params(labelsize=12)\nplt.ylabel('Deck', fontsize=14)\nplt.xlabel('Count', fontsize=14)\nplt.title('Survival by Deck', fontdict={'fontsize':16});","819d28ad":"# create DataFrame that contains missing values\nmissing = titanic.isnull().sum().sort_values(ascending=False) \nmissing = missing[missing > 0]\nmissing = pd.DataFrame(missing, columns=['total_missing'])\n\n# create column in missing DataFrame that shows total % of missing data\nmissing['missing_pct'] = np.round(missing['total_missing'] \/ len(titanic), 3)\nmissing","f7fa6ee3":"# create pivot table of deck and passenger class, showing counts for each\ntitanic.pivot_table(values=['Name'], index=['deck'], columns=['Pclass'], aggfunc='count')","f4f70219":"# drop deck feature that was created for purposes of data exploration\ntitanic = titanic.drop('deck', axis=1)","d15f2163":"# histogram of ages\ntitanic['Age'].hist();","5f661a62":"# get summary statistics for age feature\ntitanic['Age'].describe()","e9cdcb1a":"titanic['Embarked'].value_counts()","06089554":"# age bins for age category feature\nbins = [0., 10., 20., 30., 40., 50., 60., np.inf]\n\n# create class that will create custom features in pipeline\nclass CustomFeatures(BaseEstimator, TransformerMixin):\n    \"\"\"Represents the custom features added to the Titanic dataset\"\"\"\n    \n    def __init__(self, add_deck_feature=False):\n        self.add_deck_feature = add_deck_feature\n    \n    def fit(self, X, y=None):\n        return self    # nothing else to do\n    \n    def transform(self, X, y=None):\n        # create copy of dataset to avoid changes to original\n        X = X.copy()\n        \n        # age category feature\n        X['age_cat'] = np.searchsorted(bins, X['Age'].values)\n\n        # title feature\n        X['title'] = X['Name'].apply(get_title)\n        \n        # family size feature\n        X['fam_size'] = X['SibSp'] + X['Parch']\n        \n        # option to add deck feature if wanted\n        if self.add_deck_feature:\n            X['deck'] = X['Cabin'].apply(lambda x: x[0])\n        else:\n            pass\n        \n        return X","d36cd0a8":"# create instance of custom features transformer\ncustom_features = CustomFeatures()\n\n# add custom features to dataset\ntitanic = custom_features.transform(titanic)","3c7f8715":"# create clean training set (split out labels into seperate DataFrame) before data cleaning and transformation\ntitanic_labels = titanic['Survived'].copy()\ntitanic = titanic.drop('Survived', axis=1)","917ba21d":"# create list of numerical features\ntitanic_num = titanic.select_dtypes(include=[np.number])\ntitanic_num = titanic_num.drop(['age_cat'], axis=1)\nnum_attr = list(titanic_num)\n\n# create list of ordinal features\nord_attr = ['Pclass']\n\n# create list of categorical features\ncat_attr = ['Sex', 'Embarked', 'age_cat', 'title']","b52c3484":"# create pipleline for numerical features\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('std_scaler', StandardScaler())\n])\n\n# create pipleline for ordinal features\nord_pipeline = Pipeline([\n    ('encoder', OrdinalEncoder(handle_unknown='ignore')),\n])\n\n# create pipleline for catgorical features\ncat_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n# create full pipeline to prepare and transform dataset\nfull_pipeline = ColumnTransformer([\n    ('numerical', num_pipeline, num_attr),\n    ('ordinal', ord_pipeline, ord_attr),\n    ('categorical', cat_pipeline, cat_attr)],\n    remainder='drop')","5771fe08":"# create prepared dataset by applying pipeline\ntitanic_prepared = full_pipeline.fit_transform(titanic)\n\n# cast to dense numpy array (models were getting error w\/ sparse matrix?)\ntitanic_prepared = titanic_prepared.todense()","a47718ab":"# create base model - always predicts that someone dies\nclass AlwaysDieClassifier(BaseEstimator):\n    def fit(self, X, y=None):\n        return self\n    def predict(self, X):\n        return np.zeros((len(X), 1), dtype=bool)","f7218080":"# create base model classifier\nad_classifier = AlwaysDieClassifier()\nad_pred = ad_classifier.predict(titanic_prepared)","05fe56bc":"# classification report for base model\nprint(classification_report(y_true=titanic_labels, y_pred=ad_pred, zero_division=0))","3fa0716c":"# create logistic regression classifier\nlog_classifier = LogisticRegression(random_state=42)\nlog_classifier.fit(titanic_prepared, titanic_labels)\nlog_pred = cross_val_predict(log_classifier, X=titanic_prepared, y=titanic_labels, cv=5)","50681000":"# classification report for logistic regression model\nprint(classification_report(y_true=titanic_labels, y_pred=log_pred))","c4642bc7":"# create SVM regression classifier\nsvm_classifier = SVC(random_state=42)\nsvm_classifier.fit(titanic_prepared, titanic_labels)\nsvm_pred = cross_val_predict(svm_classifier, X=titanic_prepared, y=titanic_labels, cv=5)","ec49cc8c":"# classification report for SVM classifier model\nprint(classification_report(y_true=titanic_labels, y_pred=svm_pred))","100f0eac":"# create Random Forest classifier\nrf_classifier = RandomForestClassifier(random_state=42)\nrf_classifier.fit(titanic_prepared, titanic_labels)\nrf_pred = cross_val_predict(rf_classifier, X=titanic_prepared, y=titanic_labels, cv=5)","bcd381ae":"# classification report for random forest classifier model\nprint(classification_report(y_true=titanic_labels, y_pred=rf_pred))","5898e0b3":"#parameters for SVM parameter grid\nC = [0.1, 0.5, 0.75, 1, 2, 5, 10]\nkernel = ['linear', 'rbf', 'sigmoid']\ngamma = ['scale', 'auto']\n\n# create SVM parameter grid\nsvm_param_grid = [{'C':C,\n                   'kernel':kernel,\n                   'gamma':gamma}]\n\n# create SVM classifier\nsvm_classifier = SVC(random_state=42)\n\n# create grid search for SVM model\nsvm_grid_search = GridSearchCV(svm_classifier, param_grid=svm_param_grid, cv=5,\n                               return_train_score=True)\n\n# fit grid search\nsvm_grid_search.fit(titanic_prepared, titanic_labels)","04aa9063":"# get best parameters for SVM grid search\nsvm_grid_search.best_params_","6c8300bd":"# save the best model\nsvm_classifier_best = svm_grid_search.best_estimator_","14c962b0":"# classification report for SVM classifier model\nsvm_pred = cross_val_predict(svm_classifier, X=titanic_prepared, y=titanic_labels, cv=5)\nprint(classification_report(y_true=titanic_labels, y_pred=svm_pred))","6440755a":"#parameters for rf parameter grid\nn_estimators = [2, 4, 8, 16, 32 ,64, 128, 256]\nmax_features = [2, 3, 4, 5, 6, 7, 8]\nbootstrap = [True, False]\n\n# create rf parameter grid\nrf_param_grid = {'n_estimators':n_estimators,\n                 'max_features':max_features,\n                 'bootstrap':bootstrap}\n\n# create rf classifier\nrf_classifier = RandomForestClassifier(random_state=42)\n\n# create grid search for rf model\nrf_grid_search = GridSearchCV(rf_classifier, param_grid=rf_param_grid, cv=5,\n                               return_train_score=True)\n\n# fit grid search\nrf_grid_search.fit(titanic_prepared, titanic_labels)","19d6e98c":"# get best parameters for rf grid search\nrf_grid_search.best_params_","7b035b1f":"# save the best model\nrf_classifier_best = rf_grid_search.best_estimator_","6d4e0907":"# classification report for rf classifier model\nrf_pred = cross_val_predict(rf_classifier, X=titanic_prepared, y=titanic_labels, cv=5)\nprint(classification_report(y_true=titanic_labels, y_pred=rf_pred))","70572acf":"voting_classifier = VotingClassifier(estimators=[('svm', svm_classifier_best), ('rf', rf_classifier_best)])\n\nvoting_classifier.fit(titanic_prepared, titanic_labels)","42e9b338":"# X test set\nX_test = titanic_test\n\n# add custom features to the test set\nX_test = custom_features.transform(X_test)\n\n# transform X test set\nX_test_prepared = full_pipeline.transform(X_test)\n\n# cast to dense numpy array (models were getting error w\/ sparse matrix?)\nX_test_prepared = X_test_prepared.todense()","29203b7e":"# get final predictions\npredictions = voting_classifier.predict(X_test_prepared).astype(int)\n\n# create output dataframe\noutput = pd.DataFrame({'PassengerId': titanic_test.index,\n                       'Survived': predictions})\n\n# store outputs to csv\noutput.to_csv('my_submission.csv', index=False)","719b228c":"### 1.9 Missing Data\n- Finally, I look at what missing data we have, and decide what to do with each category where there are missing data.","257f654c":"## 3.3: Model 3: SVM classifer\n- Here I'll test out a SVM classifier model.","51cd3c5f":"## 3.1: Base Model Always Predict a Passenger Dies\n- First I'll create and run a base model that always predicts that someone dies.","c49c1961":"### 1.3 Survival rate: gender and passenger class\n- Below, I look at survival by gender and further split by passenger class\n- Immediately, it's important to notice that on average, females have a much higher survival rate compared to males, especially in classes 1 & 2.","8c4344aa":"It looks like there are several titles where there are only 1 - 10 passengers with the title.  I'll group these into one category called 'other' to reduce cardinality.","b9e982b2":"## 2.1: Create transformer for custom features\n- First, I'll create a custom transformer with sklearn to create the custom features that I created previously in the data exploration section (age category, title, family size).  The custom transformer will leverage sklearns BaseEstimator and TransformerMixin classes.\n- I'll also include an option to turn on the deck feature - however I won't be doing this due to the volume of missing data.  This is rather to show how you can add hyper parameters to custom transformers which could aid in fine tuning an ML model.","160a9120":"### 3.5.1: SVM Classifier Tuning","9fc0e656":"# Titanic Classification Project","00eb63bc":"### 1.8 New Feature: Deck\n- Lastly, I create one more new categorical feature: deck\n- I derived this from the cabin column by taking just the deck letter (e.g. Deck A, B... and so on).","a1bdc154":"# 3. Model Selection, Tuning, and Emsembling\n- In this section, I'll train a few models and select a winner for further tuning.","b6c14b30":"**Conclusion**: \n- There's only 2 datapoints missing in the Embarked feature\n- The most common port of embarkation was 'S', so I'll fill the missing data with this value.","b9271e3a":"### 1.7 New Feature: Total family size\n- Below, I create a new categorical feature: total family size\n- This feature is a combination of the features sibsp (siblings\/spouse) and parch (parents\/children)\n- A family size of 0 means that a passenger was alone on the ride\n- It looks like survival rates were higher for families of size 1, 2, 3.","9c4fe338":"### 1.1 Data import and inspection\n- First I've imported the data and done some preliminary inspections\n- There appears to be missing values in the age, cabin, and embarked columns.  I'll deal with these in section 2 of this kernel.","e74908fa":"#### 1.9.3: Missing Data: Embarked\n- Lastly I'll look at the missing data in the Embarked feature","fd6852a9":"## 2.2: Create data pipeline\n- Now I'll build a data pipeline with sklearn.  \n- Pipelines are great as they are extremely efficient.  For example, once the pipeline is built, we can simply call it on the test set and it will perform all the feature engineering etc. for us all at once, rather than having to re-write custom code.\n- I'll also split out the labels from the training set in this section.","fdf1385a":"### 3.5.2: Random Forest Classifier Tuning","5a838e0e":"#### 1.9.2: Missing Data: Age\n- Next I'll look at the missing data in the Age feature","78bce572":"### 1.5 New Feature: Age bins\n- Below, I create a new categorical feature: age bins\n- More specifically I bin the ages into groups of 10 from 0 to 60, and then the remaining older folks into a group of 60+\n- Then I look at a pivot table of survival rates for each age bin\n- Based on the above, a few key conclusions:\n    - Again, notice that females have a much higher survival rate, in group 7 (ages 50-59) 100% survived\n    - It looks like for most groups ~80% of men die, besides group 1(ages 0-9).\n    - It's looking like age is going to be a very strong predictive feature.","47517f0c":"## 3.6 Model ensembling\n- Finally, I'll ensemble the  best SVM and RF classifier models tuned above and create a voting classifier.","784be793":"#### 1.9.2: Missing Data: Cabin (and Deck)\n- First I'll look at the missing data in the Cabin feature\n- Note: I created the 'deck' feature previously in section 1.8.  This contains the same information as the Cabin feature, except simplified in the sense that the 'deck' feature is what deck a passenger was on (A - T), wheras the Cabin feature was a passengers deck and room number.","86299aab":"# 1. Exploratory Data Analysis and Feature Engineering\nFirst, we'll perform some explatory data analysis and engineer a few new features.","69e7427e":"### 0.3 Library imports\nBelow I've imported the libraries I'll be using throughout this kernel.","bd5eee1f":"## 3.5: Model Tuning\n- Based on the above results, I will tune an SVM classifier and a Random Forest classifier to try to improve performance.\n- I'll then ensemble them together into a voting classifier model.","8c303d75":"**Conclusion**: \n- Overall, almost 80% of the data in the Cabin\/deck features is missing.  This is a significant amount of missing data.  \n- I thought I may be able to fill values by drawing some relation to a passengers class, but even based on this it would be difficult to come up with an accurate strategy to fill the missing values.  \n- Therefore, the cabin, and the deck feature will be dropped.  Since the deck feature was custom, I've dropped it above.  I'll leave in the cabin feature which will be dropped by our Pipeline I create later in this kernel.","9195b422":"## 3.4: Model 4: Random Forest\n- Here I'll test out a Random Forest classifier model.","9fdd4809":"# 2. Data preparation and pipeline construction\nNow I'll create a custom transformer and a pipeline for our data","a18ed46f":"### 1.4 Survival rate: gender and port of embarkation\n- Below, I look at survival by gender and further split by port of embarkation\n- Similar to section 1.3, it looks like on average women have a higher survival rate, especially for those who departed from Cherbourg.","86ab2ba9":"### 0.2 Overall goal\nOur goal will be to train a machine learning model that can predict (classify) whether or not a passenger of the titanic will survive or not when the ship crashes.  Our measurement of interest will be accuracy.  I will aim for an model that produces an accuracy of at least 80% on the train set (and hopefully the same, or higher, on the test set!)","56c2770b":"# 4. Final Predictions\n- First I'll run X_test through the pipeline and then use the voting classifier from above to get the final model predictions.","52e23b5a":"# 0. Introduction","890e0729":"# Thank you!\nThanks for reviewing my kernel!  This was my first kaggle project and I'm looking forward to doing more.  Any comments\/feedback is appreciated!\n\nCheers,\nNico","8f2d2803":"#### 1.9.1 Missing Data - Create summary\n- First I created a DataFrame that contains the categories of missing data\n- There are two columns, one for the total number of missing datapoints, and one for the total % of missing data in terms of the respective feature.","48ebdaa5":"### 1.2 Overall survival rate\n- First I'll look at the overall number of people that survived vs. died\n- Then I'll look at a few countplots which demonstrate the number of people who survived\/died with detail of a few features","306a0073":"### 0.1 Introduction to dataset\n\nThe dataset we will be working with is a collection of data on the passengers of the Titanic ship.  There are various features for each passenger, and our goal is to predict whether or not a passenger survived when the ship crashed.\n\nBelow is a a table that contains the features of the dataset, their definition, a key if applicable, and the respective data type of each.\n\n| Featute  | Definition                                | Key                                             | Data Type   |\n|----------|-------------------------------------------|-------------------------------------------------|-------------|\n| Survival | Label - If someone survives or not        | 0 = No, 1 = Yes                                 | Categorical |\n| Pclass   | Class of a passengers ticket              | 1 = 1st, 2 = 2nd, 3 = 2rd                       | Ordinal     |\n| Sex      | A passengers sex (male\/female)            |                                                 | Categorical |\n| Sibsp    | # of passengers siblings and\/or spouses   |                                                 | Numerical   |\n| Parch    | # of a passengers parents and\/or children |                                                 | Numerical   |\n| Ticket   | Ticket number                             |                                                 | Categorical |\n| Fare     | Passenger fare                            |                                                 | Numerical   |\n| Cabin    | Cabin number                              |                                                 | Categorical |\n| Embarked | Port of Embarkation                       | C = Cherbourg, Q = Queenstown, S = Southhampton | Categorical |","23d1cf8f":"# Table of Contents\n* [0. Introduction](#0.-Introduction)\n    * [0.1 Introduction to dataset](#0.1-Introduction-to-dataset)\n    * [0.2 Overall goal](#0.2-Overall-goal)\n    * [0.3 Library imports](#0.3-Library-imports)\n* [1. Explatory Data Analysis and Feature Engineering](#1.-Exploratory-Data-Analysis-and-Feature-Engineering)\n    * [1.1 Data import and inspection](#1.1-Data-import-and-inspection)\n    * [1.2 Overall survival rate](#1.2-Overall-survival-rate)\n    * [1.3 Survival rate: gender and passenger class](#1.3-Survival-rate:-gender-and-passenger-class)\n    * [1.4 Survival rate: gender and port of embarktion](#1.4-Survival-rate:-gender-and-port-of-embarkation)\n    * [1.5 New Feature: Age bins](#1.5-New-feature:-Age-bins)\n    * [1.6 New Feature: Title](#1.6-New-Feature:-Title)\n    * [1.7 New Feature: Total Family Size](#1.7-New-Feature:-Total-family-size)\n    * [1.8 New Feature: Deck](#1.8-New-Feature:-Deck)\n* [2. Data preparation and pipeline construction](#2.-Data-preparation-and-pipeline-construction)\n    * [2.1: Create transformer for custom features](#2.1:-Create-transformer-for-custom-features)\n    * [2.2: Create data pipeline](#2.2:-Create-data-pipeline)\n* [3. Model Selection, Tuning, and Emsembling](#3.-Model-Selection,-Tuning,-and-Emsembling)\n    * [3.1: Base Model Always Predict a Passenger Dies](#3.1:-Base-Model-Always-Predict-a-Passenger-Dies)\n    * [3.2: Model 2: Logistic regression](#3.2:-Model-2:-Logistic-regression)\n    * [3.3: Model 3: SVM classifer](#3.3:-Model-3:-SVM-classifer)\n    * [3.4: Model 4: Random Forest](#3.4:-Model-4:-Random-Forest)\n    * [3.5: Model Tuning](#3.5:-Model-Tuning)\n        * [3.5.1: SVM Classifier Tuning](#3.5.1:-SVM-Classifier-Tuning)\n        * [3.5.2: Random Forest Classifier Tuning](#3.5.2:-Random-Forest-Classifier-Tuning)\n    * [3.6 Model ensembling](#3.6-Model-ensembling)\n* [4. Final Predictions](#4.-Final-Predictions)","ec7deec0":"### 1.6 New Feature: Title\n- Below, I create a new categorical feature: title.\n- This was extracted from each passengers name.\n- Some examples would be 'Mr', 'Mrs.', etc.\n- A key conclusion is that those with titles 'Mrs', 'Miss', and 'Master' had higher survival rates that 'Mr'.","af9eea79":"## 3.2: Model 2: Logistic regression\n- Here I'll test out a standard logistic regression classifier.","49db929f":"**Conclusion**: \n- About 20% of the age data is missing.  I'll fill this with the mean age value (will be built into the pipeline later)."}}