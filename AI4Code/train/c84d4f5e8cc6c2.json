{"cell_type":{"4ee36d25":"code","a7555b0a":"code","ad0c799a":"code","a29ed430":"code","355cbfed":"code","a6cff2d6":"code","90f0d7a7":"code","4b63b9b1":"code","d249b339":"code","3d7adaa8":"code","f92047df":"code","3ed9b8a5":"code","2c061850":"code","f8d93610":"code","cc724d5e":"code","4f13a4b9":"code","02ad114e":"code","1712c172":"code","01df42c1":"code","c4a8dc35":"code","0f4c543a":"code","413c9208":"code","34fba13c":"code","b5ee8cf9":"code","697527d9":"code","5b19041f":"markdown","373670e4":"markdown","8de47bf2":"markdown"},"source":{"4ee36d25":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a7555b0a":"# Octopus ML pakage - github.com\/gershonc\/octopus-ml\n!pip install octopus-ml","ad0c799a":"import warnings\nwarnings.simplefilter(\"ignore\")\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport time\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport tracemalloc\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.metrics import classification_report\n%matplotlib inline\nsns.set_style(\"whitegrid\")\n\npd.set_option('display.max_columns', None)  # or 1000\npd.set_option('display.max_rows', None)  # or 1000\npd.set_option('display.max_colwidth', -1)  # or 199\n\n#check out https:\/\/github.com\/gershonc\/octopus-ml\nimport octopus_ml as oc","a29ed430":"train_df = pd.read_csv ( \"..\/input\/tabular-playground-series-mar-2021\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/test.csv\")\n","355cbfed":"# Data shape \nprint (\"Train set: \",train_df.shape)\nprint (\"Test set: \",test_df.shape)","a6cff2d6":"# DataFrane Summary by pandas summary package (extension of pandas.describe method) \ndfs = DataFrameSummary(train_df)\ndfs.summary()","90f0d7a7":"# Top 10 sparse features, mainly labs results \npd.Series(1 - train_df.count() \/ len(train_df)).sort_values(ascending=False).head(10)","4b63b9b1":"# Categorical features\n\ncategorical_features=[]\nfor c in train_df.columns:\n    col_type = train_df[c].dtype\n    if col_type == 'object' or col_type.name == 'category':\n        train_df[c] = train_df[c].astype('category')\n        categorical_features.append(c)\nprint (categorical_features)","d249b339":"# Target distribution analysis\nfig, ax =plt.subplots(1,2)\n\n\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(3,4))\nsns.set_context(\"paper\", font_scale=1.2)                                                  \nsns.countplot('target',data=train_df, ax=ax[0])\ntrain_df['target'].value_counts().plot.pie(explode=[0,0.2],autopct='%1.2f%%',ax=ax[1])\nfig.show()","3d7adaa8":"sns.displot(data = train_df, kind = 'hist', x = 'cont1', hue = 'target', multiple = 'stack',bins=25,height = 4, aspect = 1.7)","f92047df":"sns.displot(data = train_df, kind = 'hist', x = 'cont2', hue = 'target', multiple = 'stack',bins=25,height = 4, aspect = 1.7)","3ed9b8a5":"features=train_df.columns.to_list()\nprint ('Number of features ', len(features))","2c061850":"features_remove=['target']\nfor f in features_remove:\n    features.remove(f)","f8d93610":"X=train_df[features]\ny=train_df['target']","cc724d5e":"%%time\n\nfrom bayes_opt import BayesianOptimization\n\n\ndef bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=6, verbose=-1, random_seed=6, n_estimators=10, output_process=False):\n    # prepare data\n    train_data = lgb.Dataset(data=X, label=y, free_raw_data=False)\n    # parameters\n    def lgb_eval(learning_rate,num_leaves, feature_fraction, bagging_fraction, max_depth, max_bin, min_data_in_leaf,min_sum_hessian_in_leaf,subsample,scale_pos_weight):\n        params = {'application':'binary', 'metric':'auc'}\n        params['learning_rate'] = max(min(learning_rate, 1), 0)\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['max_depth'] = int(round(max_depth))\n        params['max_bin'] = int(round(max_depth))\n        params['verbose']=-1\n        params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n        params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n        params['subsample'] = max(min(subsample, 1), 0)\n        params['scale_pos_weight']=scale_pos_weight\n        \n        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['auc'])\n        return np.mean(cv_result['auc-mean'])\n     \n    lgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.01, 0.15),\n                                            'num_leaves': (1, 500),\n                                            'feature_fraction': (0.1, 0.9),\n                                            'bagging_fraction': (0.5, 1),\n                                            'max_depth': (2, 20),\n                                            'max_bin':(10,30),\n                                            'min_data_in_leaf': (20, 80),\n                                            'min_sum_hessian_in_leaf':(0,100),\n                                            'subsample': (0.01, 1.0),\n                                            'scale_pos_weight': (0.1,10)\n                                           }, \n                                             random_state=200)\n\n    \n    #n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n    #init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n    \n    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n    \n    model_auc=[]\n    for model in range(len( lgbBO.res)):\n        model_auc.append(lgbBO.res[model]['target'])\n    \n    # return best parameters\n    return lgbBO.res[pd.Series(model_auc).idxmax()]['target'],lgbBO.res[pd.Series(model_auc).idxmax()]['params']\n\nopt_params = bayes_parameter_opt_lgb(X, y, init_round=5, opt_round=150, n_folds=5, random_seed=6,n_estimators=300)","4f13a4b9":"opt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\nopt_params[1]['max_depth'] = int(round(opt_params[1]['max_depth']))\nopt_params[1]['min_data_in_leaf'] = int(round(opt_params[1]['min_data_in_leaf']))\nopt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\nopt_params[1]['objective']='binary'\nopt_params[1]['metric']='auc'\n#opt_params[1]['is_unbalance']=True\nopt_params[1]['boost_from_average']=False\nopt_params=opt_params[1]\nopt_params","02ad114e":"params = {\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'metric': 'auc',\n        'learning_rate': 0.01,\n        'subsample': 1,\n        'colsample_bytree': 0.2,\n        'reg_alpha': 3,\n        'reg_lambda': 1,\n        'scale_pos_weight': 4,\n        'n_estimators': 1000,\n        'verbose': 1,\n        'max_depth': -1,\n        'seed':100, \n        'force_col_wise': True\n\n}\n\n#params.update(opt_params)\n\n\nclf,arr_f1_weighted,arr_f1_macro,arr_f1_positive,prediction_folds,preds_folds,y_folds= oc.cv(X,y,0.5,1000,shuffle=True,params=params)","1712c172":"oc.cv_plot(arr_f1_weighted,arr_f1_macro,arr_f1_positive,'TBS match 2021 - Kaggle compatition')\n","01df42c1":"print(classification_report(y_folds, prediction_folds))\n","c4a8dc35":"oc.roc_curve_plot(y_folds,preds_folds)\n","0f4c543a":"feature_imp_list=oc.plot_imp(clf,X,'LightGBM Mortality Kaggle',num=20)\n","413c9208":"top_features=feature_imp_list.sort_values(by='Value', ascending=False).head(20)\ntop_features","34fba13c":"list_for_correlations=top_features['Feature'].to_list()\nlist_for_correlations.append('target')\noc.correlations(train_df,list_for_correlations)","b5ee8cf9":"def Kaggle_submission(file_name,model,test_data,ids_list):\n    if TARGET in test_data.columns:\n        test_data.drop([TARGET],axis=1,inplace=True)\n    #test_pred=model.predict(test_data[features])[:,1]\n    test_pred=model.predict(test_data[features])\n    print (test_pred[1:2])\n\n    submit=pd.DataFrame()\n    submit['id'] = ids_list\n    submit['target'] = test_pred\n    submit.to_csv(file_name,index=False)\n    return submit","697527d9":"# Categorical features on testset\n\ncategorical_features=[]\nfor c in test_df.columns:\n    col_type = train_df[c].dtype\n    if col_type == 'object' or col_type.name == 'category':\n        test_df[c] = test_df[c].astype('category')\n        categorical_features.append(c)\nprint (categorical_features)\n\nTARGET=\"diabetes_mellitus\"\nsubmit=Kaggle_submission(\"LGBM_baseline_v15.csv\",clf,test_df,test_df['id'].tolist())","5b19041f":"## Data pre-processing\n","373670e4":"## EDA","8de47bf2":"## HPO (Hyper-parameter Tuning)"}}