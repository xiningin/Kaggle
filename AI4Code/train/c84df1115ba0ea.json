{"cell_type":{"af01e2e6":"code","f35de1ca":"code","0847617c":"code","8169c895":"code","ed543951":"code","a2454774":"code","fac677c5":"code","7d884b67":"code","72f570c0":"code","6c8c981b":"code","8fa15e39":"code","ae8ea95f":"code","8b6ba9d2":"code","0a335437":"code","3a02a6a1":"code","7b6737ae":"code","b6d33273":"code","04c3be93":"code","6db46f0a":"code","6121811b":"code","d1d5687b":"code","409c9c22":"code","a3794e71":"code","ed3286fe":"code","d2e163ce":"code","82f73948":"code","3ea71681":"code","eb361a25":"code","172d1e72":"code","6ae7c138":"code","941452c4":"markdown","369c2106":"markdown","387ea45f":"markdown","6b1ddcf8":"markdown","efeecc51":"markdown"},"source":{"af01e2e6":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.impute import KNNImputer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import Lasso,Ridge,BayesianRidge,ElasticNet,HuberRegressor,LinearRegression,LogisticRegression,SGDRegressor","f35de1ca":"df=pd.read_csv('..\/input\/synchronous-machine-dataset\/SynchronousMachine.csv')\ndf.rename(columns = {'I_y':'Load Current', 'PF':'Power Factor',\n                              'e_PF':'Power Factor Error','d_if':'Excitation Current Change','I_f':'Excitation Current'}, inplace = True)\ndf.head()","0847617c":"def describe(df):                        # Function to explore major elements in a Dataset\n                                         # Will help to find null values present and deal with them\n  columns=df.columns.to_list()           # Function will help to directly find numerical and categorical columns\n  ncol=df.describe().columns.to_list()\n  ccol=[]\n  for i in columns:\n    if(ncol.count(i)==0):\n      ccol.append(i)\n    else:\n      continue\n  print('Name of all columns in the dataframe:')\n  print(columns)\n  print('')\n  print('Number of columns in the dataframe:')\n  print(len(columns))\n  print('')\n  print('Name of all numerical columns in the dataframe:')\n  print(ncol)\n  print('')\n  print('Number of numerical columns in the dataframe:')\n  print(len(ncol))\n  print('')\n  print('Name of all categorical columns in the dataframe:')\n  print(ccol)\n  print('')\n  print('Number of categorical columns in the dataframe:')\n  print(len(ccol))\n  print('')\n  print('------------------------------------------------------------------------------------------------')\n  print('')\n  print('Number of Null Values in Each Column:')\n  print('')\n  print(df.isnull().sum())\n  print('')\n  print('')\n  print('Number of Unique Values in Each Column:')\n  print('')\n  print(df.nunique())\n  print('')\n  print('')\n  print('Basic Statistics and Measures for Numerical Columns:')\n  print('')\n  print(df.describe().T)\n  print('')\n  print('')\n  print('Other Relevant Metadata Regarding the Dataframe:')\n  print('')\n  print(df.info())\n  print('')\n  print('')","8169c895":"describe(df)","ed543951":"import warnings\nwarnings.filterwarnings(\"ignore\")\n# We are creating 3 categories for better vizualisation\n# Split was chosen after Personal research and is subject to change \n\ndf['Load Current Range']=0\nfor i in range(0,len(df)):\n  if(df['Load Current'][i]>5):\n    df['Load Current Range'][i]='High'\n  elif(df['Load Current'][i]<4):\n    df['Load Current Range'][i]='Low'\n  else:\n    df['Load Current Range'][i]='Fair'\n\n    ","a2454774":"oe=['g','y','r']\nplt.tight_layout()\nplt.style.use('ggplot')\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.set_context('talk')\nplt.title('Load Current vs Excitation Current')\nsns.scatterplot( x=\"Load Current\",y='Excitation Current', hue=\"Load Current Range\",data=df,palette=oe)\nplt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, title='Load Current Range')","fac677c5":"oe=['g','y','r']\nplt.tight_layout()\nplt.style.use('ggplot')\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.set_context('talk')\nplt.title('Power Factor vs Excitation Current')\nsns.scatterplot( x=\"Power Factor\",y='Excitation Current', hue=\"Load Current Range\",data=df,palette=oe)\nplt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, title='Load Current Range')","7d884b67":"oe=['g','y','r']\nplt.tight_layout()\nplt.style.use('ggplot')\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.set_context('talk')\nplt.title('Power Factor Error vs Excitation Current')\nsns.scatterplot( x=\"Power Factor Error\",y='Excitation Current', hue=\"Load Current Range\",data=df,palette=oe)\nplt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, title='Load Current Range')","72f570c0":"oe=['g','y','r']\nplt.tight_layout()\nplt.style.use('ggplot')\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.set_context('talk')\nplt.title('Excitation Current Change vs Excitation Current')\nsns.scatterplot( x=\"Excitation Current Change\",y='Excitation Current', hue=\"Load Current Range\",data=df,palette=oe)\nplt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, title='Load Current Range')","6c8c981b":"import statsmodels.formula.api as sm","8fa15e39":"from statsmodels.api import OLS\nreg=OLS(df['Excitation Current'],df['Load Current'] ).fit()\nprint(reg.summary())\nprint('')\nprint('')\nreg=OLS(df['Excitation Current'],df['Power Factor'] ).fit()\nprint(reg.summary())\nprint('')\nprint('')\nreg=OLS(df['Excitation Current'],df['Power Factor Error'] ).fit()\nprint(reg.summary())\nprint('')\nprint('')\nreg=OLS(df['Excitation Current'],df['Excitation Current Change'] ).fit()\nprint(reg.summary())\nprint('')\nprint('')\n","ae8ea95f":"corr = df.corr()\noe=['r','y','g']\nfig, ax = plt.subplots(figsize=(8, 6))\ndropSelf = np.zeros_like(corr)\ndropSelf[np.triu_indices_from(dropSelf)] = True\nsns.set_style(\"white\")\nsns.heatmap(corr, cmap=oe, linewidths=.5, annot=True, fmt=\".2f\", mask=dropSelf)\nplt.show()","8b6ba9d2":"def outliers(df_column):\n  oe=['r','y','g']\n  q75, q25 = np.percentile(df_column, [75 ,25]) \n  iqr = q75 - q25\n  print('q75: ',q75)\n  print('q25: ',q25)\n  print('Inter Quartile Range: ',round(iqr,2))\n  print('Outliers lie before', q25-1.8*iqr, 'and beyond', q75+1.8*iqr) \n\n  # Usually 1.5 times IQR is considered, but we have used 1.8 for broader range since datapoints are very less\n\n  print('Number of Rows with Left Extreme Outliers:', len(df[df_column <q25-1.8*iqr]))\n  print('Number of Rows with Right Extreme Outliers:', len(df[df_column>q75+1.8*iqr]))\n  fig, ax = plt.subplots(figsize=(8, 6))\n  plt.tight_layout()\n  plt.style.use('ggplot')\n  sns.set_context('talk')\n  sns.histplot(data=df, x=df_column, hue=\"Load Current Range\",multiple=\"stack\",palette=oe)\n  sns.move_legend(ax, \"lower center\", bbox_to_anchor=(.5, 1), ncol=3, title='Load Current Range', frameon=False)\n  print('')\n  \n","0a335437":"outliers(df['Load Current'])","3a02a6a1":"outliers(df['Power Factor'])","7b6737ae":"outliers(df['Power Factor Error'])","b6d33273":"outliers(df['Excitation Current Change'])","04c3be93":"vif = df.copy()\nvif.drop(columns=['Excitation Current','Load Current Range'],axis=1,inplace=True)\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = vif.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(vif.values, i)\n                          for i in range(len(vif.columns))]","6db46f0a":"vif_data","6121811b":"# Scale Data For Higher Efficiency\nfrom sklearn.preprocessing import StandardScaler # Converts Columnar Data into Standard Normal Distribution\nscaler=StandardScaler()\nscaler.fit(vif)\nscaled_data=scaler.transform(vif)\nscaled_data","d1d5687b":"from sklearn.decomposition import PCA # Reduce Dimensions by Principal Component Analysis To Compensate for Variables with High VIF\npca=PCA(n_components=2)\npca.fit(scaled_data)\nx_pca=pca.transform(scaled_data)\nx_pca","409c9c22":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_pca, df['Excitation Current'], test_size=0.2, random_state=42)\nreg = LinearRegression()\nreg.fit(x_train, y_train)\nprint('Test Accuracy of Linear Regression: ',round(100*reg.score(x_test, y_test),2),'%')\nprint('')\nprint('Train Accuracy of Linear Regression:',round(100*reg.score(x_train, y_train),2),'%')\nprint('')\ny_pred=reg.predict(x_test)\nprint('Mean Squared Error (MSE): ',round(np.sqrt(mean_squared_error(y_test,y_pred)),4))","a3794e71":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_pca, df['Excitation Current'], test_size=0.2, random_state=27)\nreg=Lasso(alpha=0.001)\nreg.fit(x_train, y_train)\nprint('Test Accuracy of Lasso Regression: ',round(100*reg.score(x_test, y_test),2),'%')\nprint('')\nprint('Train Accuracy of Lasso Regression:',round(100*reg.score(x_train, y_train),2),'%')\nprint('')\ny_pred=reg.predict(x_test)\nprint('Mean Squared Error (MSE): ',round(np.sqrt(mean_squared_error(y_test,y_pred)),4))","ed3286fe":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_pca, df['Excitation Current'], test_size=0.2, random_state=27)\nreg=Ridge(alpha=0.002)\nreg.fit(x_train, y_train)\nprint('Test Accuracy of Ridge Regression: ',round(100*reg.score(x_test, y_test),2),'%')\nprint('')\nprint('Train Accuracy of Ridge Regression:',round(100*reg.score(x_train, y_train),2),'%')\nprint('')\ny_pred=reg.predict(x_test)\nprint('Mean Squared Error (MSE): ',round(np.sqrt(mean_squared_error(y_test,y_pred)),4))","d2e163ce":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import ElasticNet\nx_train, x_test, y_train, y_test = train_test_split(x_pca, df['Excitation Current'], test_size=0.2, random_state=27)\nreg=ElasticNet(alpha=0.005)\nreg.fit(x_train, y_train)\nprint('Test Accuracy of ElacticNet Regression: ',round(100*reg.score(x_test, y_test),2),'%')\nprint('')\nprint('Train Accuracy of ElacticNet Regression:',round(100*reg.score(x_train, y_train),2),'%')\nprint('')\ny_pred=reg.predict(x_test)\nprint('Mean Squared Error (MSE): ',round(np.sqrt(mean_squared_error(y_test,y_pred)),4))","82f73948":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_pca, df['Excitation Current'], test_size=0.2, random_state=4)\nreg=DecisionTreeRegressor()\nreg.fit(x_train, y_train)\nprint('Test Accuracy of DecisionTree Regression: ',round(100*reg.score(x_test, y_test),2),'%')\nprint('')\nprint('Train Accuracy of DecisionTree Regression:',round(100*reg.score(x_train, y_train),2),'%')\nprint('')\ny_pred=reg.predict(x_test)\nprint('Mean Squared Error (MSE): ',round(np.sqrt(mean_squared_error(y_test,y_pred)),4))","3ea71681":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 103, stop = 300, num = 5)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 5)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","eb361a25":"rf = RandomForestRegressor()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(x_train, y_train)","172d1e72":"rf_random.best_params_","6ae7c138":"x_train, x_test, y_train, y_test = train_test_split(x_pca, df['Excitation Current'], test_size=0.2, random_state=4)\nreg=RandomForestRegressor(n_estimators=103, min_samples_split= 2, min_samples_leaf= 1, max_features= 'sqrt',max_depth= None, bootstrap= True)\nreg.fit(x_train, y_train)\nprint('Test Accuracy of RandomForestRegressor: ',round(100*reg.score(x_test, y_test),2),'%')\nprint('')\nprint('Train Accuracy of RandomForestRegressor :',round(100*reg.score(x_train, y_train),2),'%')\nprint('')\ny_pred=reg.predict(x_test)\nprint('Mean Squared Error (MSE): ',round(np.sqrt(mean_squared_error(y_test,y_pred)),4))","941452c4":"Findings from the above tables:\n\n1. R\u00b2 and Adjusted R\u00b2 range from **0.81 to 0.96**. This means that **all Independent variables can explain upto (81,96)% of variance in the dependent variables (excitation current)**. In layman terms, all seem to be a **good fit** for our regression.\n\n2. **A high F-Statistic in sync with a very low (0 or close to 0) Prob(F-Statistic)** Basically proves that our independent variable is a better predictor than just the intercept term (coeff) with a **very high degree of certainty**.\n\n3.  High t-test value in-sync with P>|t| being (0,0.05) also corroborates for **strong significance**. ","369c2106":"Relationship between all independent variables to the target variable see pretty linear, Next we can look up the \nStatistics for ols regression to check for p value, F-Statistics and regression coefficients for each variable \nto see how much change each independent variable bring to the target variable (i.e. excitation current)","387ea45f":"**NO OUTLIERS** found hence, we can directly move on to the next segment","6b1ddcf8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","efeecc51":"The Following Code Explores Various Models and **Effective Vizualisations** That are Easy to Understand\n\nDetailed Explanation on **Statistical Concepts**\n\nThe Best Mean Squared Error **(MSE)** Numeric Obtained By a Few Models was: **0.0317** and an Accuray of **97.11%**\n\nConcepts Like **VIF and PCA** have been used to transform the dataset, Vizualizations Include Scatter Plots and Hist Plots\n\nModels are Subject to Betterment with Stringent Hyper parameter tuning, Like using **GridSearchCV** to Run through multiple combinations of potential hyperparameters"}}