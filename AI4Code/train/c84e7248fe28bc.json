{"cell_type":{"786b7ce9":"code","362c483a":"code","2e6fc08c":"code","c14c085c":"code","da424830":"code","db3575e6":"code","4d00af07":"code","55a9277e":"code","76689add":"code","5f4cf19c":"code","801417a5":"code","37274d6c":"code","411dbcfe":"code","4d51b158":"code","cc81d73b":"code","542d4c88":"code","3a3051d1":"code","5761742f":"code","f481f7f7":"code","ff2cd88c":"code","fcb9ca84":"code","5306cb55":"code","89e42b79":"code","6e7b22d0":"code","3c63d2b0":"code","6fd1ea7b":"code","3c84b764":"code","54e2b340":"code","4b3dc635":"code","a411aebf":"code","0ce9028c":"code","aba42cb1":"code","c366da77":"code","5dd1c49f":"code","798b2585":"code","d465fc3c":"code","80491873":"code","dc64d808":"code","c8754fa9":"code","d2e4f32f":"code","5e2e9f39":"code","06951a34":"code","97fce303":"code","4d17246d":"code","9b3fde94":"code","13feb8ad":"code","c690fbc0":"markdown","143501cd":"markdown","c74b5cea":"markdown","205eadea":"markdown","0c9a6917":"markdown","4e812d4c":"markdown","29fbac9f":"markdown","16065b9e":"markdown","dd400570":"markdown","390085c2":"markdown","bef528c8":"markdown","87563c63":"markdown","551eafc6":"markdown","2cd95aa4":"markdown","44b40a09":"markdown","a6f25722":"markdown","c270e9bb":"markdown","e145d199":"markdown","70e06ae5":"markdown","91129344":"markdown","34cc76af":"markdown","89e4e322":"markdown","3455f2e3":"markdown","738c12f2":"markdown"},"source":{"786b7ce9":"!pip install -q split-folders","362c483a":"#import required libraries \nimport os\nimport zipfile\nimport splitfolders \nimport time\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport cv2\n\nimport tensorflow as tf\nimport keras\nimport keras_preprocessing\nfrom keras_preprocessing import image\nfrom keras_preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications import  MobileNetV2, ResNet50\nfrom tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_input_resnet50\nfrom tensorflow.keras.applications.vgg16 import preprocess_input as preprocess_input_vgg16\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input as preprocess_input_mobilenetv2","2e6fc08c":"!ls ..\/input\/flowers-recognition\/flowers","c14c085c":"# Split the Dataset folders into train test val folders\nsplitfolders.ratio(\"..\/input\/flowers-recognition\/flowers\", output=\"output\", seed=100, ratio=(.8, .1, .1), group_prefix=None) ","da424830":"!ls  .\/output","db3575e6":"#create paths for folders \n\n\ndaisy_dir = os.path.join('.\/output\/train\/daisy')\ndandelion_dir = os.path.join('.\/output\/train\/dandelion')\nrose_dir = os.path.join('.\/output\/train\/rose')\nsunflower_dir = os.path.join('.\/output\/train\/sunflower')\ntulip_dir = os.path.join('.\/output\/train\/tulip')\n\n#count number of files in each directory\nprint('Total training Daisy images :',len(os.listdir(daisy_dir)))\nprint('Total training Dandelion images :',len(os.listdir(dandelion_dir)))\nprint('Total training Rose images :',len(os.listdir(rose_dir)))\nprint('Total training Sunflower images :',len(os.listdir(sunflower_dir)))\nprint('Total training Tulip images :',len(os.listdir(tulip_dir)))\n\nprint('\\n')\ndaisy_files = os.listdir(daisy_dir)\ndandelion_files = os.listdir(dandelion_dir)\nrose_files = os.listdir(rose_dir)\nsunflower_files = os.listdir(sunflower_dir)\ntulip_files = os.listdir(tulip_dir)\n\nprint(\"Total len of training images\",len(daisy_files+dandelion_files+rose_files+sunflower_files+tulip_files))","4d00af07":"#count of images available in test dataset for each category\nprint('Total Test Daisy images :',len(os.listdir('output\/test\/daisy')))\nprint('Total Test Dandelion images :',len(os.listdir('output\/test\/dandelion')))\nprint('Total Test Rose images :',len(os.listdir('output\/test\/rose')))\nprint('Total Test Sunflower images :',len(os.listdir('output\/test\/sunflower')))\nprint('Total Test Tulip images :',len(os.listdir('output\/test\/tulip')))","55a9277e":"#to plot images get complete paths for images\npic_index = 10\n\n#get paths for some of the immages from each folder\ndaisy_fewimg = [os.path.join(daisy_dir,fname) for fname in daisy_files[pic_index-10:pic_index]]\ndandelion_fewimg = [os.path.join(dandelion_dir,fname) for fname in dandelion_files[pic_index-10:pic_index]]\nrose_fewimg = [os.path.join(rose_dir,fname) for fname in rose_files[pic_index-10:pic_index]]\nsunflower_fewimg = [os.path.join(sunflower_dir,fname) for fname in sunflower_files[pic_index-10:pic_index]]\ntulip_fewimg = [os.path.join(tulip_dir,fname) for fname in tulip_files[pic_index-10:pic_index]]\n","76689add":"#plot random image \nimport PIL.Image as Image\n\nprint(cv2.imread(daisy_fewimg[0]).shape)\nImage.open(daisy_fewimg[0])","5f4cf19c":"Image.open(daisy_fewimg[0]).resize((224,224))","801417a5":"#function to convert BGR image to RGB\ndef cvtRGB(img):\n    return cv2.cvtColor(img.copy(), cv2.COLOR_BGR2RGB)","37274d6c":"#plot smaple of images function\ndef plot_img(imgpath_list):\n\n  fig, ax = plt.subplots(3,3,figsize=(10,10))\n  k = 0\n  for j in range(3):\n    for i in range(3):\n        img = cv2.resize(cv2.imread(imgpath_list[k]),(224,224))\n        k=k+1\n        ax[i,j].imshow(cvtRGB(img));\n        ax[i,j].axis('off');","411dbcfe":"print('\\t\\t\\t\\tDaisy')\nplot_img(daisy_fewimg)","4d51b158":"print('\\t\\t\\t\\tDadelion')\nplot_img(dandelion_fewimg)","cc81d73b":"print('\\t\\t\\t\\tRose')\nplot_img(rose_fewimg)","542d4c88":"print('\\t\\t\\t\\tTulip')\nplot_img(tulip_fewimg)","3a3051d1":"print('\\t\\t\\t\\tSunflower')\nplot_img(sunflower_fewimg)","5761742f":"#image Augumentation using ImageDataGenerator on train data \n#Data augmentation is used to increase the size of training set and to get more different images\ntraining_dir = '.\/output\/train\/'\ntraining_datagen = ImageDataGenerator(\n    rescale = 1.\/255, #normalization\n    rotation_range = 40,\n    width_shift_range = 0.4,\n    height_shift_range = 0.4,\n    shear_range = 0.2,\n    zoom_range = 0.1,\n    fill_mode = 'nearest',\n    horizontal_flip = True)","f481f7f7":"#do not perform augmentation on validation and test data\/validation set ,this might mislead the results\nvalidation_dir = '.\/output\/val'\nvalidation_datagen = ImageDataGenerator(\n    rescale=1.\/255\n)\n\ntest_dir = '.\/output\/test'\ntest_datagen = ImageDataGenerator(\n    rescale=1.\/255\n)","ff2cd88c":"#flow_from_directory() method allows you to read the images directly from the directory and augment them while the neural network model is learning on the training data.\ntrain_generator = training_datagen.flow_from_directory(\n    training_dir,\n    target_size = (224,224), #rescale images to fixed size\n    class_mode = 'categorical',\n    batch_size = 32\n)\nvalidation_generator = validation_datagen.flow_from_directory(\n    validation_dir,\n    target_size = (224,224),\n    class_mode = 'categorical',\n    batch_size = 32\n)\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size = (224,224),\n    class_mode = 'categorical',\n    batch_size = 32\n)","fcb9ca84":"#tf.keras.backend.clear_session()","5306cb55":"# Build CNN sequential model and train from scratch \ndef model_from_scratch():\n  #Build model\n  model = tf.keras.models.Sequential([\n              tf.keras.layers.Conv2D(32, (3,3), activation = 'relu', input_shape = (224,224,3)),\n              tf.keras.layers.MaxPool2D(2,2),\n              tf.keras.layers.Dropout(0.5),\n              tf.keras.layers.Conv2D(64, (3,3), activation = 'relu'),\n              tf.keras.layers.MaxPool2D(2,2),\n              tf.keras.layers.Dropout(0.5),\n              tf.keras.layers.Conv2D(128, (3,3), activation = 'relu'),\n              tf.keras.layers.MaxPool2D(2,2),\n              tf.keras.layers.Dropout(0.5),\n              tf.keras.layers.Flatten(),\n              tf.keras.layers.Dropout(0.5),\n              tf.keras.layers.Dense(512, activation = 'relu'),\n              tf.keras.layers.Dense(5, activation = 'softmax')\n  ])\n\n  #model compilation\n  model.compile(loss='categorical_crossentropy',\n              optimizer = tf.keras.optimizers.Adam(),\n              metrics = ['accuracy']\n            )\n  \n  return model\n\nmodel_from_scratch().summary() #Model Summary","89e42b79":"#fit model to train dataset \nepochs = 25\nstart = time.time()\n\nmodel = model_from_scratch()\nhistory1 = model.fit(train_generator,\n                    epochs=epochs,\n                    validation_data=validation_generator,\n                    validation_steps=5,\n                    verbose=2\n                    )\n\nend = time.time()\nduration = end - start\nprint ('\\n Model built from scratch training took %0.2f seconds (%0.1f minutes) to train for %d epochs'%(duration, duration\/60, epochs) )","6e7b22d0":"#function to get accuracy and loss from history \ndef get_history_data(history):\n    \n  train_acc = history.history['accuracy']\n  val_acc = history.history['val_accuracy']\n\n  train_loss = history.history['loss']\n  val_loss = history.history['val_loss']\n\n  return train_acc,val_acc,train_loss,val_loss","3c63d2b0":"train_acc,val_acc,train_loss,val_loss = get_history_data(history1)","6fd1ea7b":"#plot Accuracy graph\nepochs = range(len(train_acc))\nplt.figure(figsize=(7,7));\nplt.plot(epochs,train_acc,label ='Train accuracy');\nplt.plot(epochs,val_acc,label ='Validation accuracy');\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend();\nplt.title('Training Vs Validation Accuracy');","3c84b764":"#plot validation graph\nplt.figure(figsize=(7,7));\nplt.plot(epochs,train_loss,label ='Train Loss');\nplt.plot(epochs,val_loss,label ='Validation Loss');\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend();\nplt.title('Training Vs Validation Loss');","54e2b340":"model.save('.\/model_from_stratch.h5')","4b3dc635":"#check how model performs on test data(unseen data)\nmodel.evaluate(test_generator)","a411aebf":"#tf.keras.backend.clear_session()","0ce9028c":"#use VGG16 pre-trained model\ndef create_model_from_VGG16():\n   \n    model = VGG16(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3))\n    \n    # don't train existing weights \n    for layer in model.layers:\n      layer.trainable = False\n\n    #Adding custom Layers \n    x = model.output\n    x = keras.layers.Flatten()(x)\n    x = keras.layers.Dense(128, activation=\"relu\")(x)\n    #x = keras.layers.Dropout(0.5)(x)\n    #x = keras.layers.Dense(1024, activation=\"relu\")(x)\n    predictions = keras.layers.Dense(5, activation=\"softmax\")(x)\n    \n    # creating the final model \n    final_model = keras.models.Model(inputs = model.input, outputs = predictions)\n    \n    final_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n    \n    return final_model\n\ncreate_model_from_VGG16().summary()","aba42cb1":"epochs = 10\nstart = time.time()\n\nmodel_from_vgg16 = create_model_from_VGG16()\nhistory2 = model_from_vgg16.fit(train_generator,\n                    epochs=epochs,\n                    validation_data=validation_generator,\n                    validation_steps=5,\n                    verbose=2\n                     )\n\nend = time.time()\nduration = end - start\nprint ('\\n model from trained with VGG16 took %0.2f seconds (%0.1f minutes) to train for %d epochs'%(duration, duration\/60, epochs) )","c366da77":"pd.DataFrame(history2.history).plot(figsize=(8,5))\nplt.show()","5dd1c49f":"model.save('.\/model_from_VGG16.h5')","798b2585":"model_from_vgg16.evaluate(test_generator)","d465fc3c":"#tf.keras.backend.clear_session()","80491873":"#use ResNet50 pre-trained model\ndef create_model_from_ResNet50():\n   \n    model = ResNet50(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3))\n    \n    # don't train existing weights\n    for layer in model.layers:\n      layer.trainable = False\n\n    #Adding custom Dense Layers \n    x = model.output\n    x = keras.layers.Flatten()(x)\n    #x = keras.layers.Dense(128, activation=\"relu\")(x)\n    #x = keras.layers.Dropout(0.5)(x)\n    #x = keras.layers.Dense(512, activation=\"relu\")(x)\n    x = keras.layers.Dropout(0.5)(x)\n    x = keras.layers.Dense(256, activation=\"relu\")(x)\n    predictions = keras.layers.Dense(5, activation=\"softmax\")(x)\n    \n    # creating the final model \n    final_model = keras.models.Model(inputs = model.input, outputs = predictions)\n    \n    final_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n    \n    return final_model\n\ncreate_model_from_ResNet50().summary()","dc64d808":"#epochs = 10\n#start = time.time()\n\n#model_resNet50 = create_model_from_ResNet50()\n#history3 = model_resNet50.fit(train_generator,\n#                    epochs=epochs,\n#                    validation_data=validation_generator,\n#                    validation_steps=5,\n#                    verbose=2\n#                     )\n\n#end = time.time()\n#duration = end - start\n#print ('\\n model from trained from ResNet50 took %0.2f seconds (%0.1f minutes) to train for %d epochs'%(duration, duration\/60, epochs) )","c8754fa9":"#model_resNet50.evaluate(test_generator)","d2e4f32f":"#using preprocess function of pretarined model so that model is not overfitted and trained properly\ntraining_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_mobilenetv2)\ntest_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_mobilenetv2)\n\ntrain_generator = training_datagen.flow_from_directory(\n    training_dir,\n    target_size = (224,224),\n    class_mode = 'categorical',\n    batch_size = 32\n)\nvalidation_generator = validation_datagen.flow_from_directory(\n    validation_dir,\n    target_size = (224,224),\n    class_mode = 'categorical',\n    batch_size = 32\n)\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size = (224,224),\n    class_mode = 'categorical',\n    batch_size = 32\n)","5e2e9f39":"#use Rmobilenetv2 pre-trained model\ndef create_model_from_mobilenetv2():\n   \n    model = MobileNetV2(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3))\n    \n    # don't train existing weights\n    for layer in model.layers:\n      layer.trainable = False\n\n    #Adding custom Dense Layers \n    x = model.output\n    x = keras.layers.Flatten()(x)\n    x = keras.layers.Dense(128, activation=\"relu\")(x)\n    x = keras.layers.Dropout(0.5)(x)\n    x = keras.layers.Dense(512, activation=\"relu\")(x)\n    predictions = keras.layers.Dense(5, activation=\"softmax\")(x)\n    \n    # creating the final model \n    final_model = keras.models.Model(inputs = model.input, outputs = predictions)\n    \n    final_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n    \n    return final_model\n\ncreate_model_from_mobilenetv2().summary()","06951a34":"epochs = 10\nstart = time.time()\n\nmodel_mobilenetV2 = create_model_from_mobilenetv2()\nhistory4 = model_mobilenetV2.fit(train_generator,\n                    epochs=epochs,\n                    validation_data=validation_generator,\n                    validation_steps=5,\n                    verbose=2\n                     )\n\nend = time.time()\nduration = end - start\nprint ('\\n model from trained from MobileNetV2 took %0.2f seconds (%0.1f minutes) to train for %d epochs'%(duration, duration\/60, epochs) )","97fce303":"model_mobilenetV2.evaluate(test_generator)","4d17246d":"!pip install -q gradio","9b3fde94":"import gradio as gr \nimport tensorflow as tf\nimport numpy as np\nimport requests\nfrom keras.preprocessing import image","13feb8ad":"#used model trained using vgg16 as it performed well on both train and test data\nlabels = ['Daisy','Dadelion','Rose','Sunflower','Tulip'] #classes\n\ndef classify_image(inp):\n  img = inp.reshape((-1,224,224,3)) #reshape input image\n  prediction = model_from_vgg16.predict(img).flatten() #prediction\n  return {labels[i]: float(prediction[i]) for i in range(5)} #return classes\n\nimage = gr.inputs.Image(shape=(224, 224))\nlabel = gr.outputs.Label(num_top_classes=1)\n\ngr.Interface(fn=classify_image, inputs=image, outputs=label, capture_session=True).launch(debug=True,share=True)","c690fbc0":"##### results\n**loss: 0.5707 - accuracy: 0.7871 - val_loss: 0.6541 - val_accuracy: 0.7875**\nBetter than the perevious model (model built from scratch)","143501cd":"![screenshot.png](attachment:e6b1321f-1f0c-462f-a1c3-ae65114442a5.png)\n\n**Actual : Rose**\n\n**Prediction : Rose**","c74b5cea":"###### the flucation on validation accuarcy is nothing but overfitting , which can be probaly because of the noise present in the data","205eadea":"### Test model by uploading random images and check results","0c9a6917":"## Transfer Learning : Using Pre-trained model to improve model performance","4e812d4c":"![screenshot (4).png](attachment:7b23bbea-341c-4caa-93c2-620549e514f6.png)\n\n**Actual : Daisy**\n\n**Prediction: Daisy**","29fbac9f":"![image.png](attachment:ef0681e9-c2bc-4871-8347-7ef7cb99426b.png)","16065b9e":"\n### Author : Kalyani Avhale\n# Flower Recognition \ud83c\udf38 \n  - Task : Recognize what kind of flower is that?\n  - This dataset contains 4242 images of flowers.\n  - The pictures are divided into five classes: chamomile, tulip, rose, sunflower, dandelion.\n  - For each class there are about 800 photos. Photos are not high resolution, about 320x240 pixels. Photos are not reduced to a single size, they have different proportions!\n  \n  ###### Dataset : https:\/\/www.kaggle.com\/alxmamaev\/flowers-recognition","dd400570":"### Import required libraries and Load the dataset","390085c2":"**loss: 0.8467 - accuracy: 0.7034** \n\n**Model has performed well on test data**","bef528c8":"### Visualization","87563c63":"![screenshot (3).png](attachment:4fce91a7-a8c4-4f0a-9e5a-d667f5ff4612.png)\n\n**Actual : Dadelion**\n\n**Prediction : Daisy**\n\n\n![screenshot (5).png](attachment:d8ac75d3-1ca8-467f-bf42-b4e4241ca7e3.png)\n\n**Actual : Tulip**\n\n**Prediction : Tulip**\n\n\n![screenshot (6).png](attachment:4f7b953a-58b3-43ea-b637-372e2bed63d1.png)\n\n**Actual : Daisy**\n\n**Prediction : Daisy**","551eafc6":"#### MobileNetV2","2cd95aa4":"##### Model Performance :\n**loss: 0.4698 - accuracy: 0.8473 - val_loss: 0.9422 - val_accuracy: 0.7875**","44b40a09":"### ResNet50","a6f25722":"##### val_accuracy:0.6625\n##### val_loss : 0.8049 \n##### Model has been trained well with 66% accuracy , but will try increasing it further with Transfer learning","c270e9bb":"**Please upvote my notebook if you found it informative and helpful. If you want to give any suggestions and queries regarding notebook please feel free to mention it in comments**","e145d199":"### Pre-Processing ","70e06ae5":"**model performance on test data:**\n**loss: 0.5357 - accuracy: 0.8276**","91129344":"#### Visualize model performance","34cc76af":"### Model Building","89e4e322":"Few images has other objects present","3455f2e3":"### VGG16 : ","738c12f2":"##### Performance on Test Data:\n**loss: 0.7838 - accuracy: 0.8483**"}}