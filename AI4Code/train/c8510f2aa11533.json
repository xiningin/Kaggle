{"cell_type":{"071b9305":"code","179d640c":"code","331985b4":"code","7709a85e":"code","2f105275":"code","db87a00e":"code","df3f9ece":"code","1ce42f5a":"code","bb58254a":"code","5c03748e":"code","444e697f":"code","22d342c7":"code","2fa13ce7":"code","6d148ba1":"code","292a6438":"code","29c4446a":"code","5595d669":"code","054f02b4":"code","81082a68":"code","0d578175":"code","07cc0b01":"code","5ea6a62f":"code","2b0c2a24":"code","94099edf":"code","74cc7528":"code","9ba48882":"code","c0aaabf3":"code","cb9f76e5":"code","bc508ea9":"code","20f05b3b":"code","fcc843c2":"code","2129cfad":"code","c437d0b2":"markdown","1a6bb1d3":"markdown","e35fb06c":"markdown","cc73819c":"markdown","c902287b":"markdown","907b9b09":"markdown","1a320ffb":"markdown","bc0efcb5":"markdown","85ad8894":"markdown","a4a387e8":"markdown","ca3092df":"markdown","631a7ca0":"markdown","d139c656":"markdown","5902f9d0":"markdown","29a265c4":"markdown","a05776ef":"markdown","546e8446":"markdown","5b90f884":"markdown","1ec08dca":"markdown","59ea1bd4":"markdown","8c4552c6":"markdown","6247ed78":"markdown"},"source":{"071b9305":"#Install needed packages and NLP models\n!pip install -U pysolr\n!pip install -U scispacy\n!pip install -U gensim\n!pip install -U jsonpath-ng\n!pip install -U pyvis\n!pip install -U pyLDAvis\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_md-0.2.4.tar.gz \n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_ner_craft_md-0.2.4.tar.gz\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_ner_jnlpba_md-0.2.4.tar.gz\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_ner_bc5cdr_md-0.2.4.tar.gz\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_ner_bionlp13cg_md-0.2.4.tar.gz    ","179d640c":"!wget -O solr-8.5.0.zip \"https:\/\/archive.apache.org\/dist\/lucene\/solr\/8.5.0\/solr-8.5.0.zip\";\n!unzip solr-8.5.0.zip","331985b4":"!ls","7709a85e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport os\nimport json\nimport math\nimport glob\nimport re\nimport time\nimport pysolr\nimport csv\nimport time\nimport scipy\nimport spacy\nimport scispacy\nimport multiprocessing\nimport pandas as pd\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport ipywidgets as widgets\nfrom os import path\nfrom pandas import ExcelWriter\nfrom pandas import ExcelFile\nfrom jsonpath_ng.ext import parse\nfrom collections import Counter\nfrom collections import OrderedDict\nfrom IPython.core.display import display, HTML\nfrom IPython.display import IFrame\nfrom pyvis.network import Network\nfrom datetime import date\nimport dateutil.parser as dparser\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport concurrent.futures\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nimport nltk\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nimport plotly.express as px\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly as py\nimport plotly.graph_objs as go\n\ninit_notebook_mode(connected=True) #do not miss this line\n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nstopwords = nltk.corpus.stopwords.words('english')\n\ncustom_stop_words = [\n    'doi', 'medRxiv','MedRxiv','preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI',\n    '-PRON-', '-', 'medRxiv preprint'\n]\n\nstopwords.extend(custom_stop_words)","2f105275":"def clean_text(text) :\n    text = re.sub(r\" ?\\([^)]*\\)\", \"\", text)\n    text = re.sub(r\" ?\\[[^)]*\\]\", \"\", text)\n    text = re.sub(r\"http\\S+\", \"\", text)\n    text = re.sub(r\"  \", \" \", text)\n    return text.strip()","db87a00e":"def preprocess_data() :\n    print(\"Starting preprocessing of Data....\\n\",\"Please be patient, custom pre-processing can take approximately 25-30 mins....\")\n    start = time.time()\n\n    meta_df = pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv', sep=',', header=0)\n    meta_columns = list(meta_df.columns)\n    meta_df.fillna('', inplace=True)\n\n    # Filter to pick only needed sections\n    include_set = ['Abstract','Introduction', 'background', 'Discussion', 'Results', 'Results and Discussion', 'methods,results']\n\n    # Throw execption is the Section Heads excel file is not available\n    section_df = pd.DataFrame()\n    try :\n        print('Loading SECTIONHEADERS4PCKR...')\n        section_df = pd.read_excel('\/kaggle\/input\/sectionheaders4pckr\/SECTIONHEADERS4PCKR.xlsx', sheet_name='Sheet1')\n    except Exception as ex :\n        print('Preprocessing cannot run without SECTIONHEADERS4PCKR.xlsx:', ex)\n        raise\n        \n    section_df.dropna(subset=['Section Heads'], inplace=True)\n    section_df = section_df[section_df['Section Heads'].isin(include_set)]\n        \n    # Categories\/Targets to pick from Section Heads column\n    introduction_categories = [\"Introduction\"]\n    discussion_categories = [\"Discussion\"]\n    result_categories = [\"Results\", \"Results and Discussion\", 'methods,results']\n\n    introduction_df = section_df[section_df['Section Heads'].isin(introduction_categories)]\n    discussion_df = section_df[section_df['Section Heads'].isin(discussion_categories)]\n    result_df = section_df[section_df['Section Heads'].isin(result_categories)]\n\n    intro_list = introduction_df.iloc[:, 0].tolist()\n    discussion_list = discussion_df.iloc[:, 0].tolist()\n    result_list = result_df.iloc[:, 0].tolist()\n\n    intro_list = list(map(lambda x: str(x).strip(), intro_list))\n    discussion_list = list(map(lambda x: str(x).strip(), discussion_list))\n    result_list = list(map(lambda x: str(x).strip(), result_list))\n\n    path = '\/kaggle\/input\/CORD-19-research-challenge\/'\n    paths = [p for p in glob.glob(path + \"**\/*.json\", recursive=True)]\n    files_size = len(paths)\n\n    col_names = ['paper_id','title','source', 'abstract','introduction','result','discussion','body', 'publish_time', 'has_covid','url']\n    clean_df = pd.DataFrame(columns=col_names)\n\n    covid_syns = ['COVID-19','COVID19','2019-nCoV','2019nCoV','Coronavirus','SARS-CoV-2','SARSCov2','novel Coronavirus']\n\n    target_empty_count = 0\n\n    abstract_expr = parse('$.abstract[*].text')\n\n    for path in paths:\n        with open(path) as f:\n            intro_text_list = list()\n            discussion_text_list = list()\n            result_text_list = list()\n\n            data = json.load(f)\n\n            abstract_texts = [match.value for match in abstract_expr.find(data)]\n\n            body_nodes = data['body_text']\n\n            for entry in body_nodes :\n                section_name = entry['section']\n                section_name = section_name.strip().lower()\n                entry_text = entry['text']\n\n                if section_name.strip() in intro_list:\n                    intro_text_list.append(entry_text)\n\n                if section_name.strip() in discussion_list:\n                    discussion_text_list.append(entry_text)\n\n                if section_name.strip() in result_list:\n                    result_text_list.append(entry_text)\n\n            if len(intro_text_list) == 0 and len(discussion_text_list) == 0 and len(result_text_list) == 0 :\n                target_empty_count = target_empty_count + 1\n\n\n            id = data['paper_id']\n            title = data['metadata']['title']\n            \n            url=''\n            try :                 \n                url = meta_df.loc[meta_df['sha'] == id, 'url'].iloc[0]\n            except Exception as ex:\n                pass            \n            \n            pubtime_df = meta_df[meta_df.sha == id]['publish_time']\n            pubtime_dict = pubtime_df.to_dict()\n            pubtime = ''\n            for pubtime_field_key in pubtime_dict.keys():\n                temp_pubtime_str = pubtime_dict.get(pubtime_field_key)\n                orig_temp_pubtime_str = temp_pubtime_str\n                try:\n                    temppubdate = dparser.parse(orig_temp_pubtime_str,fuzzy=True).date()\n                    pubtime = temppubdate.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n                except Exception as e:\n                    temp_pubtime_str_parts = temp_pubtime_str.split(' ')\n                    if len(temp_pubtime_str_parts) > 2 :\n                        try :\n                            temp_pubtime_str = temp_pubtime_str_parts[0] + ' ' + temp_pubtime_str_parts[1] + ' ' + temp_pubtime_str_parts[2]\n                            temppubdate = dparser.parse(temp_pubtime_str,fuzzy=True).date()\n                            pubtime = temppubdate.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n                        except Exception as ex:\n                            pubtime = ''\n                    else:\n                        pubtime = ''             \n\n            sha_df = meta_df[meta_df.sha == id]['source_x']\n            meta_dict = sha_df.to_dict()\n            source = ''\n            for meta_field_key in meta_dict.keys():\n                source = meta_dict.get(meta_field_key)\n\n            if not source:\n                title_df = meta_df[meta_df.title == title]['source_x']\n                meta_dict = title_df.to_dict()\n                for meta_field_key in meta_dict.keys():\n                    source = meta_dict.get(meta_field_key)\n\n            abstract = clean_text(\" \".join(abstract_texts))\n            introduction = clean_text(\" \".join(intro_text_list))\n            discussion = clean_text(\" \".join(discussion_text_list))\n            result = clean_text(\" \".join(result_text_list))\n            body = \" \".join([introduction, discussion, result])\n\n            has_covid = 'false'\n\n            res = [ele for ele in covid_syns if (ele.lower() in body.lower())]\n            if(len(res)  > 0):\n                has_covid = 'true'\n\n            if len(body.strip()) > 0 or len(abstract) > 0:\n                new_row = {'paper_id': id, 'title': title.strip(), 'source': source,'abstract': abstract.strip(),\n                           'introduction': introduction.strip(),'result': result.strip(),'discussion': discussion.strip(),\n                            'body': body.strip(), 'publish_time': pubtime,'has_covid': has_covid, 'url':url}\n                clean_df = clean_df.append(new_row, ignore_index=True)\n\n    # Drop duoplicate papers\n    clean_df.drop_duplicates(subset=['title','abstract'], keep='first', inplace=False)\n    clean_df.to_csv('\/kaggle\/working\/CORD-19.csv', index=True)\n\n    print('Final DataFrame Shape - ', clean_df.shape)\n    print(\"Papers that dont have Intro, Discussion or Result  - \", target_empty_count)\n    print('Total Papers processed - ', files_size)\n\n    print('Time Elaspsed - ', time.time() - start)\n    ","df3f9ece":"!solr-8.5.0\/bin\/solr start -force","1ce42f5a":"!solr-8.5.0\/bin\/solr create -c covid19 -s 1 -rf 1 -force","bb58254a":"# Using _default configset with data driven schema functionality. NOT RECOMMENDED for production use.\n!solr-8.5.0\/bin\/solr config -c covid19 -p 8983 -action set-user-property -property update.autoCreateFields -value false","5c03748e":"#Set Up Synonyms\n\n!echo 'COVID-19,covid19,2019-nCoV,2019nCoV,Coronavirus,SARS-CoV-2,SARSCov2,novel Coronavirus' > solr-8.5.0\/server\/solr\/covid19\/conf\/synonyms.txt;\n!echo 'heart,cardiac,tachycardia,myocardial' >> solr-8.5.0\/server\/solr\/covid19\/conf\/synonyms.txt;\n!echo 'pulmonary,respiratory' >> solr-8.5.0\/server\/solr\/covid19\/conf\/synonyms.txt;","444e697f":"!cat solr-8.5.0\/server\/solr\/covid19\/conf\/synonyms.txt","22d342c7":"#Reload the covid19 core\/collection because we added new synonyms. Need reload as it will affect index\n#Whenever new synonyms are added we need to reindex as synonyms are applied both on index and query analyzers\n!curl 'http:\/\/localhost:8983\/solr\/admin\/cores?action=RELOAD&core=covid19'","2fa13ce7":"#Add custom field Type that wont tokenize phrases for fields like source etc\n!curl -X POST -H 'Content-type:application\/json' --data-binary '{\"add-field-type\" : {\"name\":\"keywordText\",\"class\":\"solr.TextField\", \"positionIncrementGap\":\"100\", \"indexAnalyzer\" : {\"tokenizer\":{\"class\":\"solr.KeywordTokenizerFactory\" }, \"filters\":[{\"class\":\"solr.TrimFilterFactory\"},{\"class\":\"solr.StopFilterFactory\", \"ignoreCase\":true, \"words\":\"lang\/stopwords_en.txt\"},{\"class\":\"solr.ManagedSynonymGraphFilterFactory\", \"managed\":\"english\" },{\"class\":\"solr.RemoveDuplicatesTokenFilterFactory\"},{\"class\":\"solr.FlattenGraphFilterFactory\"}]},\"queryAnalyzer\" : {\"tokenizer\":{\"class\":\"solr.KeywordTokenizerFactory\" },\"filters\":[{\"class\":\"solr.TrimFilterFactory\"},{\"class\":\"solr.StopFilterFactory\", \"ignoreCase\":true, \"words\":\"lang\/stopwords_en.txt\"},{\"class\":\"solr.ManagedSynonymGraphFilterFactory\", \"managed\":\"english\" },{\"class\":\"solr.RemoveDuplicatesTokenFilterFactory\"}]}}}' http:\/\/localhost:8983\/solr\/covid19\/schema","6d148ba1":"#Create SOLR field definitions\n!curl -X POST -H 'Content-type:application\/json' --data-binary '{\"add-field\": {\"name\":\"title\", \"type\":\"text_en_splitting_tight\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http:\/\/localhost:8983\/solr\/covid19\/schema;\n!curl -X POST -H 'Content-type:application\/json' --data-binary '{\"add-field\": {\"name\":\"abstract\", \"type\":\"text_en_splitting_tight\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http:\/\/localhost:8983\/solr\/covid19\/schema;\n!curl -X POST -H 'Content-type:application\/json' --data-binary '{\"add-field\": {\"name\":\"source\", \"type\":\"keywordText\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http:\/\/localhost:8983\/solr\/covid19\/schema;\n!curl -X POST -H 'Content-type:application\/json' --data-binary '{\"add-field\": {\"name\":\"introduction\", \"type\":\"text_en_splitting_tight\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http:\/\/localhost:8983\/solr\/covid19\/schema;\n!curl -X POST -H 'Content-type:application\/json' --data-binary '{\"add-field\": {\"name\":\"discussion\", \"type\":\"text_en_splitting_tight\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http:\/\/localhost:8983\/solr\/covid19\/schema;\n!curl -X POST -H 'Content-type:application\/json' --data-binary '{\"add-field\": {\"name\":\"result\", \"type\":\"text_en_splitting_tight\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http:\/\/localhost:8983\/solr\/covid19\/schema;\n!curl -X POST -H 'Content-type:application\/json' --data-binary '{\"add-field\": {\"name\":\"body\", \"type\":\"text_en_splitting_tight\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http:\/\/localhost:8983\/solr\/covid19\/schema;\n!curl -X POST -H 'Content-type:application\/json' --data-binary '{\"add-field\": {\"name\":\"publish_time\", \"type\":\"pdate\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http:\/\/localhost:8983\/solr\/covid19\/schema;\n!curl -X POST -H 'Content-type:application\/json' --data-binary '{\"add-field\": {\"name\":\"url\", \"type\":\"string\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http:\/\/localhost:8983\/solr\/covid19\/schema;        \n!curl -X POST -H 'Content-type:application\/json' --data-binary '{\"add-field\": {\"name\":\"has_covid\", \"type\":\"boolean\", \"multiValued\":false, \"stored\":true, \"indexed\":true}}' http:\/\/localhost:8983\/solr\/covid19\/schema;","292a6438":"solr = pysolr.Solr('http:\/\/localhost:8983\/solr\/covid19\/', timeout=10)","29c4446a":"generic_model = spacy.load('en_core_sci_md')\n\n#Load preprocessed CSV data\npublic_csv_path = '\/kaggle\/input\/cord19s4pckr\/CORD19S4PCKR.csv'\ncsv_path = '\/kaggle\/working\/CORD19S4PCKR.csv'\n\nif not path.exists(public_csv_path):\n    print('Calling Preprocessing...')\n    preprocess_data()\nelse :\n    csv_path = public_csv_path\n    print('Dataset Path - ', csv_path)\n    \ndf = pd.read_csv(csv_path, sep=',', header=0)\n#df.dropna(axis=0, how='all', thresh=None, subset=['title','abstract','body'], inplace=False)\ndf.fillna('', inplace=True)\ndf.drop_duplicates(subset=['title','abstract'], keep='first', inplace=True)\ndf = df[df[['abstract','body']].ne('').any(axis=1)]\nprint('DF candidate_list size - ', df.shape)\n\ndf.head(2)","5595d669":"# Index each pandas row as a document into SOLR search engine\n\ncovid_syns = ('SARSCoV2','SARS-CoV-2', '2019-nCoV','2019nCoV','COVID-19', 'COVID19','coronavirus', 'corona virus' 'novel coronavirus')\n\nlist_for_solr=[]\ncounter = 0\nfor index, row in df.iterrows():\n    id = row['paper_id']\n    title = row[\"title\"]\n    source = row[\"source\"]    \n    abstract = row[\"abstract\"]\n    introduction = row[\"introduction\"]\n    discussion = row[\"discussion\"]\n    result = row[\"result\"]\n    publish_time = row[\"publish_time\"]\n    body = row[\"body\"]  # Cocatenated text of all text fields abstract, introduction, discussion, result\n    \n    if((title and title.isspace()) and (abstract and abstract.isspace()) and (body and body.isspace())):\n        continue\n        \n    has_covid = 'false'\n    if any(words in body for words in covid_syns):\n        has_covid = 'true'\n    \n    solr_content = {}\n    solr_content['id'] = id\n    solr_content['title'] = title\n    solr_content['source'] = source\n    solr_content['abstract'] = abstract\n    solr_content['introduction'] = introduction\n    solr_content['discussion'] = discussion\n    solr_content['result'] = result    \n    solr_content['body'] = body  \n    solr_content['has_covid'] = has_covid\n    \n    if publish_time != '':\n        solr_content['publish_time'] = publish_time    \n        \n    list_for_solr.append(solr_content)\n    \n    if index % 1000 == 0:\n        solr.add(list_for_solr)\n        list_for_solr = []\n        counter = counter + 1000\n        print('Indexed Papers - ', counter)\n        \n#Commit is very costly use it sparingly        \nsolr.commit()\nprint('Indexing Finished !')","054f02b4":"def extract_entities(scimodels, text) :\n    entities = {}\n    \n    for nlp in scimodels :\n        doc = nlp(text)\n        for ent in doc.ents:\n            entity = ent.text\n            if ent.label_ in entities :\n                if entities[ent.label_].count(ent.text) == 0:\n                    entities[ent.label_].append(ent.text)\n            else :\n                entities[ent.label_] = [ent.text]\n\n    return entities","81082a68":"def initilize_nlp_models(model_names):\n    scimodels = {}\n    for name in model_names:\n        scimodels[name] = spacy.load(name)\n    \n    print('Models Loaded')\n    return scimodels","0d578175":"def search_task_answers(search_results, display_on=False) :\n    answers_list = list()\n    \n    for search_result in search_results:\n        doc_hl_dict = {}\n        \n        id = search_result.get('id', \"MISSING\")\n        title = search_result.get('title', \"MISSING\")\n        source = search_result.get('source', \"MISSING\")        \n        publish_time = search_result.get('publish_time')\n        url = search_result.get('url', 'MISSING')\n        \n        doc_highlights = search_results.highlighting[id]\n        \n        doc_hl_dict['id'] = id\n        doc_hl_dict['title'] = title\n        doc_hl_dict['source'] = source\n        doc_hl_dict['publish_time'] = publish_time\n        doc_hl_dict['url'] = url        \n                \n        if len(doc_highlights) > 0 and display_on:\n            display(HTML(f'<h4><i>{title}\\n<\/i><\/h4>'))\n\n        for doc_hl_field in doc_highlights:\n            hl_snippets = doc_highlights[doc_hl_field]\n        \n            if len(hl_snippets) > 0 :\n                answer_snippet = ''\n                \n                if display_on :\n                    display(HTML(f'\\t<h5>{doc_hl_field}\\n<\/h5>\\n'))\n            \n                for index, snippet in enumerate(hl_snippets, start=1):\n                    answer_snippet = answer_snippet.strip() + \" \" + snippet.strip()\n                    \n                    if display_on :\n                        display(HTML(f'<blockquote>{index}. {snippet.strip()}\\n<\/blockquote>'))\n                                  \n                doc_hl_dict[doc_hl_field] = answer_snippet.strip()\n                    \n        if len(doc_hl_dict) > 0:\n            answers_list.append(doc_hl_dict)\n        \n    return answers_list","07cc0b01":"def search(query, rows=5, mark=False):\n    # Search for data\n    if mark :\n        search_results = solr.search(query, **{\n        'fq':'has_covid:true',\n        'rows' : rows,\n        'qf':'title^50.0 abstract^40.0 introduction^30.0 discussion^20.0 result^50.0 body^10.0',\n        'pf':'title^60.0 abstract^50.0 introduction^40.0 discussion^30.0 result^60.0 body^20.0',\n        'hl': 'true',\n        'hl.bs.type': 'SENTENCE',\n        'hl.method' : 'unified',\n        'hl.snippets' : 5,\n        'hl.usePhraseHighlighter': 'true',\n        'hl.highlightMultiTerm' : 'true',\n        'hl.tag.pre':'<mark>',\n        'hl.tag.post':'<\/mark>',\n        'df':'body',\n        'hl.fl':'introduction,discussion,result'\n        })\n    else:\n        search_results = solr.search(query, **{\n        'fq':'has_covid:true',\n        'rows' : rows,\n        'qf':'title^50.0 abstract^40.0 introduction^30.0 discussion^20.0 result^50.0 body^10.0',\n        'pf':'title^60.0 abstract^50.0 introduction^40.0 discussion^30.0 result^60.0 body^20.0',\n        'hl': 'true',\n        'hl.bs.type': 'SENTENCE',\n        'hl.method' : 'unified',\n        'hl.snippets' : 5,\n        'hl.usePhraseHighlighter': 'true',\n        'hl.highlightMultiTerm' : 'true',\n        'hl.tag.pre':'',\n        'hl.tag.post':'',\n        'df':'body',\n        'hl.fl':'introduction,discussion,result'\n        })        \n\n    num_docs_found = search_results.hits\n    num_search_results = len(search_results)\n    \n    return num_docs_found, search_results","5ea6a62f":"# Map entities existence in intro-result-discussion respectively to label values\nlabel_def = {'111':'prior-newdata','101':'prior-strong','100':'prior','001':'speculative','010':'unknown', '011':'novel'}\n\nmodel_names = ['en_ner_craft_md','en_ner_jnlpba_md','en_ner_bc5cdr_md','en_ner_bionlp13cg_md']\nscimodels = initilize_nlp_models(model_names)","2b0c2a24":"def populate_labels(task_answers) :\n    field_list = ['introduction','discussion', 'result']\n\n    for doc_answer_dict in task_answers:\n        all_entities = set()\n        intro_entities = set()\n        discussion_entities = set()\n        result_entities = set() \n\n        for field_name, answer_text in doc_answer_dict.items():\n            if field_name in field_list:\n                chosen_models = list(scimodels.values())\n                \n                ent_dict = extract_entities(chosen_models, answer_text)\n                ent_list = set()\n                \n                for ner_type, model_ent_list in ent_dict.items():\n                    ent_list.update(model_ent_list)                \n\n                all_entities.update(ent_list)\n                if field_name == 'introduction' :\n                    intro_entities.update(ent_list)\n                elif field_name == 'discussion' :\n                    discussion_entities.update(ent_list)\n                else :\n                    result_entities.update(ent_list)\n\n        # Now set up labels for entities\n        prior_newdata_entities = set()\n        prior_strong_entities = set()\n        prior_entities = set()\n        speculative_entities = set()\n        unknown_entities = set()    \n        novel_entities = set()\n\n        for a_ent in all_entities :\n            if a_ent in intro_entities and a_ent in result_entities and a_ent in discussion_entities : \n                prior_newdata_entities.add(a_ent)\n            elif a_ent in intro_entities and a_ent not in result_entities and a_ent in discussion_entities :\n                prior_strong_entities.add(a_ent)\n            elif a_ent in intro_entities and a_ent not in result_entities and a_ent not in discussion_entities :\n                prior_entities.add(a_ent)\n            elif a_ent not in intro_entities and a_ent in result_entities and a_ent not in discussion_entities : \n                unknown_entities.add(a_ent)\n            elif a_ent not in intro_entities and a_ent in result_entities and a_ent in discussion_entities :\n                novel_entities.add(a_ent)\n            else :\n                pass\n            \n        if(len(prior_newdata_entities) > 0) :\n            doc_answer_dict['prior-newdata'] = list(prior_newdata_entities)\n\n        if(len(prior_strong_entities) > 0) :\n            doc_answer_dict['prior-strong'] = list(prior_strong_entities)          \n\n        if(len(prior_entities) > 0) :\n            doc_answer_dict['prior'] = list(prior_entities)             \n\n        if(len(speculative_entities) > 0) :\n            doc_answer_dict['speculative'] = list(speculative_entities)\n\n        if(len(unknown_entities) > 0) :\n            doc_answer_dict['unknown'] = list(unknown_entities)           \n\n        if(len(novel_entities) > 0) :\n            doc_answer_dict['novel'] = list(novel_entities)           \n\n    return task_answers","94099edf":"tasks = ['Smoking and pre-existing pulmonary disease', \n         'Co-infections, co-morbidities and respiratory infections',\n         'Neonates and pregnant women',\n         'Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.',\n         'Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors',\n         'Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups',\n         'Susceptibility of populations',\n         'Public health mitigation measures that could be effective for control'\n        ]\n\nqueries = ['(\"Smoking COVID-19\"~10 OR \"tobacco COVID-19\"~10 OR \"nicotine COVID-19\"~10 OR \"pulmonary disease\"~10)', \n           '(\"Co-infections COVID-19\"~10 OR \"co-morbidities COVID-19\"~10 OR \"respiratory infections COVID-19\"~10)',\n           '(\"Neonates COVID-19\"~10 OR \"pregnant women COVID-19\"~10)',\n           '(Socio-economic OR \"behavioral factors\"~10 OR \"economic impact\"~10)',\n           '(\"Transmission dynamics\"~10 OR \"reproductive number\"~3 OR \"incubation period\"~3 OR \"serial interval\"~10 OR \"modes of transmission\"~10 OR \"environmental factors\"~10)',\n           '(\"Severity of disease\"~10 OR \"risk of fatality\"~10 OR \"symptomatic patients\"~10 OR \"high-risk patient\"~10)',\n           'Susceptibility of populations',\n           '(\"mitigation measures\"~10 OR \"effective control\"~10)'\n        ]\n\n# Map entities existence in intro-result-discussion respectively to label values\nlabel_def = {'111':'prior-newdata','101':'prior-strong','100':'prior','001':'speculative','010':'unknown', '011':'novel'}","74cc7528":"# Label based Bubble chart over time\nlabel_def_list = list(label_def.values())\nlabel_weights = {\"prior-newdata\":6, \"prio-strong\":5, \"prior\":4, \"novel\":3, \"speculative\":2, \"unknown\":1}\nlabel_weight_values = list(label_weights.values())\nbcol_names = ['title','source', 'entity', 'label','weight','publish_time']\n\nfor task,query in zip(tasks, queries):\n    \n    label_pca_df = pd.DataFrame()\n    label_df = pd.DataFrame(columns=bcol_names)\n    \n    label_entities = []\n    num_docs_found, search_results = search(query, 300, False)\n    num_search_results = len(search_results)\n    \n    all_entities_set = set()\n    \n    if num_docs_found > 0:\n        task_answers = search_task_answers(search_results, False)\n        task_answers = populate_labels(task_answers)\n        \n        for task_answer in task_answers :\n            title = task_answer['title']\n            source = task_answer['source']\n            publish_time = task_answer['publish_time']\n            \n            present_labels = set(list(task_answer.keys()))&set( list(label_weights.keys()))\n                        \n            for present_label in present_labels :\n                label_entities = task_answer[present_label]\n                \n                all_entities_set.update(label_entities)\n                \n                for ent in label_entities :\n                    ldict = {'title':title,'source':source, 'entity':ent, \n                             'label':present_label, 'weight':label_weights[present_label], \n                             'publish_time':publish_time}\n                    label_df = label_df.append(ldict, ignore_index=True)\n        \n        \n        label_pca_df = pd.DataFrame(0, index=np.arange(len(task_answers)), columns=all_entities_set)\n        title_list = []\n        for l_index, task_answer in enumerate(task_answers) :\n            title_list.append(task_answer['title'])\n            \n            present_labels = set(list(task_answer.keys()))&set( list(label_weights.keys()))\n            for present_label in present_labels :\n                label_entities = task_answer[present_label]            \n                for ent in label_entities :\n                    label_pca_df.set_value(l_index, ent, label_weights[present_label])\n        \n\n        # PCA on entities with Labels like prior, novel etc#\n        ###################################################\n        l_pca = PCA(n_components=2)\n        l_data = l_pca.fit_transform(label_pca_df)\n        \n        # Scree Variance Plot\n        l_var_df = pd.DataFrame({\"variance\": l_pca.explained_variance_ratio_, \"PC\":[\"PC1\", \"PC2\"]})       \n        \n        lvar_fig = px.bar(l_var_df, x='PC', y='variance', \n                          labels={'variance':'Variance Explained', 'PC':'Principal Components'} ,\n                          title=\"Scree Plot - \" + task)\n        py.offline.iplot(lvar_fig)\n        \n#         #Elbow\n#         sum_squared_distances = []\n#         K = range(1,12)\n#         for k in K:\n#             km = KMeans(n_clusters=k, init = 'k-means++', random_state = 13)\n#             km = km.fit(l_data)\n#             print(km.inertia_)\n#             sum_squared_distances.append(km.inertia_)  \n\n#         plt.plot(K, sum_squared_distances, 'bx-')\n#         plt.xlabel('k')\n#         plt.ylabel('Sum_of_squared_distances')\n#         plt.title('Elbow Method For Optimal k')\n#         plt.show()        \n        \n        # KMeans Cluster Plot. The Elbow method showed k = 3 as best\n        l_kmeans_indices = KMeans(n_clusters=3, init = 'k-means++', random_state = 13).fit_predict(l_data)\n        colors = ['b', 'g', 'r', 'c', 'y', 'm']\n        l_pca_dict = {'PC1': l_data[:,0], 'PC2': l_data[:,1], 'Title':title_list, 'Color':[colors[d] for d in l_kmeans_indices]}\n        l_pca_df = pd.DataFrame(l_pca_dict)        \n        l_fig = px.scatter(l_pca_df, x=\"PC1\", y=\"PC2\", color='Color', hover_name=\"Title\", title=task)        \n        py.offline.iplot(l_fig)\n    \n    \n    # Bubble Plot\n    label_df[\"weight\"].fillna(1, inplace = True)\n    label_df[\"publish_time\"]= pd.to_datetime(label_df[\"publish_time\"])\n\n    bubble_time = px.scatter(label_df.sort_values(by='publish_time',ascending=True),\n                        x=\"publish_time\", y=\"entity\", color=\"label\", \n                        size=\"weight\",hover_name=\"title\",\n                        title=task)\n    bubble_time.update_yaxes(showticklabels=False)\n    py.offline.iplot(bubble_time)    \n    \n    bubble_entity = px.scatter(label_df.sort_values(by='publish_time',ascending=True),\n                        x=\"entity\", y=\"label\", color=\"source\", \n                        size=\"weight\",hover_name=\"title\",\n                        title=task) \n    bubble_entity.update_xaxes(showticklabels=False)\n    py.offline.iplot(bubble_entity)\n    ","9ba48882":"label_def_list = list(label_def.values())\n\nfor task,query in zip(tasks, queries):\n    label_entities = []\n    numDocsFound, search_results = search(query,10, True)\n    num_search_results = len(search_results)\n    \n    display(HTML(f'<h3 style=\"color:red\">Task - {task} \\n<\/h3>'))\n    display(HTML(f'<h3 style=\"color:blue\">Top {num_search_results} search result(s) of {numDocsFound} total. \\n<\/h3>'))\n    if numDocsFound > 0:\n        task_answers = search_task_answers(search_results, True)\n        task_answers = populate_labels(task_answers)","c0aaabf3":"# PCA via DOC-TERM MATRIX \nfor task,query in zip(tasks, queries):\n\n    num_docs_found, search_results = search(query, 100, False)\n    uniq_entity_list = set()\n    \n    q_entity_list = []\n    title_list = []\n    body_list = []\n    \n    for index,search_result in enumerate(search_results):\n        title = search_result.get('title', \"\")        \n        title_list.append(title.strip())\n        \n        body = search_result.get('body', \"\")\n        body_list.append(body)\n                \n        doc_ent_list = []\n        \n        for scimodel in scimodels.values(): \n            body_doc = scimodel(body)\n            doc_ent_list = doc_ent_list + [e.text.lower() for e in body_doc.ents]\n            \n        uniq_entity_list.update(doc_ent_list)\n        q_entity_list.extend(doc_ent_list)\n    \n    doc_df = pd.DataFrame(data=body_list, columns=['documents'])\n    \n    vectorizer = CountVectorizer(vocabulary=uniq_entity_list, analyzer='word', min_df=2, \n                                 max_features = 5000, preprocessor=None, tokenizer=None, \n                                 lowercase=True, stop_words=stopwords)\n    \n    vectors = vectorizer.fit_transform(doc_df['documents'].values)\n    \n    title_df = pd.DataFrame(data=title_list, columns=['title'])\n    \n    matrix_df = pd.DataFrame(data=vectors.toarray(), columns=vectorizer.get_feature_names())\n    \n    pca = PCA(n_components=3)\n    pca_components = pca.fit_transform(vectors.toarray())\n    \n    # Scree Variance Plot\n    var_df = pd.DataFrame({\"variance\": pca.explained_variance_ratio_, \"PC\":[\"PC1\", \"PC2\", \"PC3\"]})       \n        \n    var_fig = px.bar(var_df, x='PC', y='variance', \n                      labels={'variance':'Variance Explained', 'PC':'Principal Components'} ,\n                      title=\"Scree Plot - \" + task)\n    py.offline.iplot(var_fig)    \n\n    x_axis = [o[0] for o in pca_components]\n    y_axis = [o[1] for o in pca_components]\n    z_axis = [o[2] for o in pca_components] \n    \n    colors = [\"r\", \"b\", \"c\", \"y\", \"m\", ]\n    \n    #The Elbow method showed k = 3 as best\n    kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 13)\n    kmean_indices = kmeans.fit_predict(pca_components)    \n    \n    pca_dict = {'PC1': x_axis, 'PC2': y_axis, 'PC3': z_axis,'Title':title_list, 'Color':[colors[d] for d in kmean_indices]}\n    pca_df = pd.DataFrame(pca_dict)\n    fig = px.scatter_3d(pca_df, x=\"PC1\", y=\"PC2\", z='PC3',color='Color', hover_name=\"Title\", title=task)\n    py.offline.iplot(fig)\n    \n    #Bar Graphs of Entity Frequencies\n    nerCntr = Counter(q_entity_list)\n    freq_ners = nerCntr.most_common(50)\n\n    x,y = zip(*freq_ners)\n    x,y = list(x),list(y)\n\n    plt.figure(figsize=(15,10))\n    ax= sns.barplot(x=x, y=y,palette = sns.cubehelix_palette(len(x)))\n    plt.xlabel('Entity')\n    plt.xticks(rotation=90)\n    plt.ylabel('Frequency')\n    plt.title(task)\n    plt.show()    \n    ","cb9f76e5":"def search_custom_query(query, num_docs=10, mark=False, display_on=False):\n    numDocsFound, search_results = search(query, num_docs, mark)\n    if numDocsFound > 0:\n        task_answers = search_task_answers(search_results, display_on)\n        task_answers = populate_labels(task_answers)\n        \n        return search_results,task_answers","bc508ea9":"search_terms = 'Pregnant women'\nsearchbar = widgets.interactive(search_custom_query, query=search_terms, num_docs=5,  mark=False, display_on=False)\nsearchbar","20f05b3b":"temp_query = searchbar.kwargs\nsearched_query = temp_query['query']\n\nif searched_query:\n    s,a = search_custom_query(searched_query, num_docs=50, mark=False, display_on=False)\nelse:\n    s,a = search_custom_query('(\"Smoking COVID-19\"~10 OR \"Nicotine COVID-19\"~10 OR \"tobacco COVID-19\"~10 OR \"pulmonary disease\"~10)', num_docs=200, mark=False, display_on=False)\n\nsearch_results = s\ntask_answers = a\nOR","fcc843c2":"def cosine_score(x):\n        d = []\n        for i in range(len(x)):\n            for j in range(i+1,len(x)):\n                doc1= generic_model(x[i])\n                doc2= generic_model(x[j])\n                d.append({\n\n                    'Title1': x[i],\n                    'Title2': x[j],\n                    'Score': doc1.similarity(doc2)*30\n                }\n            )\n\n        return d\n\nclass network_graph:\n    \n    def __init__(self,search_result):\n        data = pd.DataFrame(search_result)\n        self.data = data\n        col_list = data.columns\n        drop_list = ['introduction','discussion','result']\n        self.new_cols = []\n        for cols in col_list:\n            if cols not in drop_list:\n                self.new_cols.append(cols)\n\n        \n    def extract_titles(self,search_result):\n        title_list = []\n        for title in range(len(search_result)):\n            title_list.append(search_result[title]['title'])\n            \n        title_dict_temp = cosine_score(title_list)\n        self.title_df = pd.DataFrame(title_dict_temp)\n        title_df_temp1 = self.title_df[['Title1']]\n        title_df_temp1.rename(columns={'Title1':'Title'},inplace=True)\n        title_df_temp2 = self.title_df[['Title2']]\n        title_df_temp2.rename(columns={'Title2':'Title'},inplace=True)\n        self.merged_titles = pd.concat([title_df_temp1,title_df_temp2],axis=0)\n        \n    \n    def extract_words(self,search_result):\n        \n        word_df = self.data[self.new_cols]\n        \n        word_df_temp = word_df.drop(['id','title'],axis=1)\n        \n        col_list = list(word_df_temp.columns)\n        \n        self.word_df_prior =pd.DataFrame()\n        self.word_df_prior_strong =pd.DataFrame()\n        self.word_df_prior_newdata =pd.DataFrame()\n        self.word_df_speculative =pd.DataFrame()\n        self.word_df_unknown =pd.DataFrame()\n        self.word_df_novel =pd.DataFrame()\n        \n        word_df_prior_1 =pd.DataFrame()\n        word_df_prior_strong_1 =pd.DataFrame()\n        word_df_prior_newdata_1 =pd.DataFrame()\n        word_df_speculative_1 =pd.DataFrame()\n        word_df_unknown_1 =pd.DataFrame()\n        word_df_novel_1 =pd.DataFrame()\n            \n        for col in col_list:\n            if col =='prior':\n                self.word_df_prior = word_df[['id','title','prior']]\n                self.word_df_prior = self.word_df_prior.explode('prior')\n                self.word_df_prior.dropna(subset = [\"prior\"], inplace=True)\n                self.word_df_prior['Weight'] = 8\n                \n                word_df_prior_1 = self.word_df_prior[['prior']]\n                word_df_prior_1.drop_duplicates(inplace=True)\n                word_df_prior_1['Color'] = 'tomato'\n                word_df_prior_1['Size'] = 10\n                word_df_prior_1.columns = ['Words','Color','Size']\n                \n            elif col== 'prior-strong':\n                self.word_df_prior_strong = word_df[['id','title','prior-strong']]\n                self.word_df_prior_strong = self.word_df_prior_strong.explode('prior-strong')\n                self.word_df_prior_strong.dropna(subset = [\"prior-strong\"], inplace=True)\n                self.word_df_prior_strong['Weight'] = 6\n                \n                word_df_prior_strong_1 = self.word_df_prior_strong[['prior-strong']]\n                word_df_prior_strong_1.drop_duplicates(inplace=True)\n                word_df_prior_strong_1['Color'] = 'sienna'\n                word_df_prior_strong_1['Size'] = 10\n                word_df_prior_strong_1.columns = ['Words','Color','Size']\n                \n                \n            elif col == 'prior_newdata':\n                self.word_df_prior_newdata = word_df[['id','title','prior_newdata']]\n                self.word_df_prior_newdata = self.word_df_prior_newdata.explode('prior_newdata')\n                self.word_df_prior_newdata.dropna(subset = [\"prior_newdata\"], inplace=True)\n                self.word_df_prior_newdata['Weight'] = 4\n                \n                word_df_prior_newdata_1 = self.word_df_prior_newdata[['prior_newdata']]\n                word_df_prior_newdata_1.drop_duplicates(inplace=True)\n                word_df_prior_newdata_1['Color'] = 'bisque'\n                word_df_prior_newdata_1['Size'] = 10\n                word_df_prior_newdata_1.columns = ['Words','Color','Size']\n                \n                \n            elif col =='speculative':\n                self.word_df_speculative = word_df[['id','title','speculative']]\n                self.word_df_speculative = self.word_df_speculative.explode('speculative')\n                word_df_speculative.dropna(subset = [\"speculative\"], inplace=True)\n                self.word_df_speculative['Weight'] = 10\n                \n                word_df_speculative_1 = self.word_df_speculative[['speculative']]\n                word_df_speculative_1.drop_duplicates(inplace=True)\n                word_df_speculative_1['Color'] = 'dimgray'\n                word_df_speculative_1['Size'] = 10\n                word_df_speculative_1.columns = ['Words','Color','Size']\n                \n            elif col =='unknown':\n                self.word_df_unknown = word_df[['id','title','unknown']]\n                self.word_df_unknown = self.word_df_unknown.explode('unknown')\n                self.word_df_unknown.dropna(subset = [\"unknown\"], inplace=True)\n                self.word_df_unknown['Weight'] = 30\n                \n                word_df_unknown_1 = self.word_df_unknown[['unknown']]\n                word_df_unknown_1.drop_duplicates(inplace=True)\n                word_df_unknown_1['Color'] = 'black'\n                word_df_unknown_1['Size'] = 10\n                word_df_unknown_1.columns = ['Words','Color','Size']\n                \n            elif col =='novel':\n                self.word_df_novel = word_df[['id','title','novel']]\n                self.word_df_novel = self.word_df_novel.explode('novel')\n                self.word_df_novel.dropna(subset = [\"novel\"], inplace=True)\n                self.word_df_novel['Weight'] = 2\n                \n                word_df_novel_1 = self.word_df_novel[['novel']]\n                word_df_novel_1.drop_duplicates(inplace=True)\n                word_df_novel_1['Color'] = 'purple'\n                word_df_novel_1['Size'] = 10\n                word_df_novel_1.columns = ['Words','Color','Size']\n                \n    # Take all the titles and categorize them into one group\n    \n        title_df_1 = self.merged_titles[['Title']]\n        title_df_1.drop_duplicates(inplace=True,keep='first')\n        title_df_1['Color'] = 'firebrick'\n        title_df_1['Size'] = 30\n        title_df_1.columns = ['Words','Color','Size']\n        \n        # Combining all the dataframe together in one place\n\n        frames = [title_df_1,word_df_prior_1,word_df_prior_strong_1,word_df_prior_newdata_1,word_df_speculative_1,word_df_unknown_1,word_df_novel_1]\n         \n        self.merged_df = pd.concat(frames)\n        \n        \n    # Changing the index of merged_df to 'Words' so that can combine it with node\n\n        self.merged_df_index = self.merged_df.drop_duplicates(subset='Words',keep='first')\n        self.merged_df_index.set_index('Words',inplace=True)\n        \n        \n    def network_creation(self):\n        \n        i = nx.Graph()\n        \n        \n\n        if (self.title_df.empty ==False):\n            for row in self.title_df.iterrows():\n                i.add_edge(row[1]['Title1'], row[1]['Title1'], weight=row[1]['Score'])\n\n        if (self.word_df_prior.empty ==False):\n            for row in self.word_df_prior.iterrows():\n                i.add_edge(row[1]['title'], row[1]['prior'], weight=row[1]['Weight'])\n\n        if (self.word_df_prior_strong.empty ==False):\n            for row in self.word_df_prior_strong.iterrows():\n                i.add_edge(row[1]['title'], row[1]['prior-strong'], weight=row[1]['Weight'])\n            \n        if (self.word_df_novel.empty ==False):\n            for row in self.word_df_novel.iterrows():\n                i.add_edge(row[1]['title'], row[1]['novel'], weight=row[1]['Weight'])\n                \n        if (self.word_df_speculative.empty ==False):\n            for row in self.word_df_speculative.iterrows():\n                i.add_edge(row[1]['title'], row[1]['speculative'], weight=row[1]['Weight'])\n                \n        if (self.word_df_prior_newdata.empty ==False):\n            for row in self.word_df_prior_newdata.iterrows():\n                i.add_edge(row[1]['title'], row[1]['prior_newdata'], weight=row[1]['Weight'])\n                \n        if (self.word_df_unknown.empty ==False):\n            for row in self.word_df_unknown.iterrows():\n                i.add_edge(row[1]['title'], row[1]['unknown'], weight=row[1]['Weight'])\n            \n        merged_df_clr = self.merged_df_index.reindex(i.nodes())\n\n        merged_df_clr['Color']=pd.Categorical(merged_df_clr['Color'])\n\n        merged_df_clr['Color'].cat.codes\n        \n        \n        #Plotting the force directed graph\n        \n        plt.figure(figsize=(22, 22))\n        degrees = nx.degree(i)\n        pos_node = nx.spring_layout(i,k=0.5)\n        nx.draw_networkx(i,pos=pos_node,node_color=merged_df_clr['Color'].cat.codes, cmap=plt.cm.Set2,node_size=[(degrees[v] + 1) * 100 for v in i.nodes()],alpha = 0.7)\n        \n    def force_directed_graphs(self):\n        \n        net = Network(notebook=True,height = '720px',width = '1200px')\n#         self.path = os.path.dirname(__file__) + \"\/templates\/template.html\"\n        \n        temp1 = self.merged_df\n        temp2 = temp1.reset_index()\n\n        temp2.drop(columns=['index'],inplace=True)\n\n        net.add_nodes(temp2['Words'],title = temp2['Words'],color=temp2['Color'],size=temp2['Size'].to_list())\n\n        if (self.title_df.empty ==False):\n            for row in self.title_df.iterrows():\n                net.add_edge(row[1]['Title1'], row[1]['Title1'], weight=row[1]['Score'])\n\n        if (self.word_df_prior.empty ==False):\n            for row in self.word_df_prior.iterrows():\n                net.add_edge(row[1]['title'], row[1]['prior'], weight=row[1]['Weight'])\n\n        if (self.word_df_prior_strong.empty ==False):\n            for row in self.word_df_prior_strong.iterrows():\n                net.add_edge(row[1]['title'], row[1]['prior-strong'], weight=row[1]['Weight'])\n            \n        if (self.word_df_novel.empty ==False):\n            for row in self.word_df_novel.iterrows():\n                net.add_edge(row[1]['title'], row[1]['novel'], weight=row[1]['Weight'])\n                \n        if (self.word_df_speculative.empty ==False):\n            for row in self.word_df_speculative.iterrows():\n                net.add_edge(row[1]['title'], row[1]['speculative'], weight=row[1]['Weight'])\n                \n        if (self.word_df_prior_newdata.empty ==False):\n            for row in self.word_df_prior_newdata.iterrows():\n                net.add_edge(row[1]['title'], row[1]['prior_newdata'], weight=row[1]['Weight'])\n                \n        if (self.word_df_unknown.empty ==False):\n            for row in self.word_df_unknown.iterrows():\n                net.add_edge(row[1]['title'], row[1]['unknown'], weight=row[1]['Weight'])\n\n        net.save_graph(\"mygraph2.html\")\n        return net\n#         display(net.show(\"mygraph.html\"))","2129cfad":"#Calling all the functions\nng= network_graph(task_answers)\nng.extract_titles(task_answers)\nng.extract_words(task_answers)\na = ng.force_directed_graphs()\na.show(\"mygraph2.html\")","c437d0b2":"## IV.III.I Create Search Index\n\n> A search engine is set up using Apache Solr, wherein one can search for a term of interest and be shown the papers that have information related to the query with keywords highlighted. By default, the result of a query is at the most five papers but this can be changed to another number. The medical entities that are mapped by the Scispacy models in the search results are also highlighted in the final result shown.\n> \n> More information regarding Solr can be found in the Solr documentation (see References).\n","1a6bb1d3":"## IV.X. Visualization of Force Directed Graph codes\n\n> Aim - The aim of these graphs is to identify the key relations between the important entities present in the documents with other documents. This will help researchers quickly identify associated papers and the relationship between them based on the importance of the entities present on the graphs\n> \t\n> Creating a variable such that if no query is given to the system, the directed graph will be formed on the query \u201cpregnant women\u201d\n> \n> Using Pyvis, the following are the key steps followed in the code given below -\n\n1.  Adding nodes - All top 5 result titles along with their key entities are added as nodes\n\n\n2. Node Sizes - <br\/>\nTitle nodes - Title nodes are given a big size (size = 30) so as to easily identify them <br\/>\nEntity nodes - Entity nodes are given a smaller size (size = 10) as compared to title nodes\n\n3. Node colour - Depending on the classification of the entity, the colours have been associated with the entities (Important entities have been highlighted in a lighter shade of red as compared to less important entities which have been highlighted in a darker shade of blue)\n\n    Prior New Data - Bisque (lighter shade) <br\/>\n    Prior Strong - Sienna (lighter shade) <br\/>\n    Prior - Tomato (lighter shade) <br\/>\n    Novel - Purple (darker shade) <br\/>\n    Speculative - Dimgray (darker shade) <br\/>\n    Unknown - Black (darker shade) <br\/>\n\n4. Node Edge - Node edges (weight) have been given based on the entity classification\n\n5. Distance between titles - Cosine scores between titles (based on their title names)  have been calculated to map the distance between the title nodes\n","e35fb06c":"## IV.VII. Initialize NLP models\n\nThis function takes a list of the names of models, loads them one by one and returns the names of the models. More specifically, four spaCy NER models are loaded here and their encompassing entities are listed below.\n\n<table style=\"width:100%\">\n  <tr>\n    <th><center>Model<\/center><\/th>\n    <th><center>Training Corpus<\/center><\/th> \n    <th><center>Entity Type<\/center><\/th> \n  <\/tr>\n  <tr>\n    <td><center>en_ner_craft_md<\/center><\/td>\n    <td><center>CRAFT<\/center><\/td> \n    <td>GGP, SO, TAXON, CHEBI, GO, CL<\/td> \n  <\/tr>\n  <tr>\n    <td><center>en_ner_jnlpba_md<\/center><\/td>\n    <td><center>JNLPBA<\/center><\/td>\n    <td>DNA, CELL_TYPE, CELL_LINE, RNA, PROTEIN<\/td>  \n  <\/tr>\n  <tr>\n    <td><center>en_ner_bc5cdr_md<\/center><\/td>\n    <td><center>BC5CDR<\/center><\/td> \n    <td>DISEASE, CHEMICAL<\/td> \n  <\/tr>\n  <tr>\n    <td><center>en_ner_bionlp13cg_md<\/center><\/td>\n    <td><center>BIONLP13CG<\/center><\/td>\n    <td>CANCER, ORGAN, TISSUE, ORGANISM, CELL, AMINO_ACID, GENE_OR_GENE_PRODUCT, SIMPLE_CHEMICAL, ANATOMICAL_SYSTEM, IMMATERIAL_ANATOMICAL_ENTITY, MULTI-TISSUE_STRUCTURE, DEVELOPING_ANATOMICAL_STRUCTURE, ORGANISM_SUBDIVISION, CELLULAR_COMPONENT<\/td>  \n<\/tr>\n<\/table>\n","cc73819c":"# IV. Code ","c902287b":"> # II. Approach\n> \n> Information retrieval, in a text mining context, involves a user submitting a query to a search engine and receiving relevant results aligning with their submitted query in return. In a pandemic context, information extraction from medical literature involves the identification of entities such as diseases, as well as the identification of complex relationships between these entities. Query results extracted from the literature may be used for the purposes of populating databases or data curation. From these extractions, knowledge bases can be built that contain the collected statements with references to the literature. Knowledge discovery involves identifying undiscovered or hidden knowledge by applying data-mining algorithms to the collection of facts gathered from the literature. From here, text mining results may be used to suggest new hypotheses which can be used to either validate or disprove existing hypotheses or to help direct future research. \n> \n> For the purpose of the task, we formed a team including **data scientists, software engineers, clinicians and medical researchers** to enable a credible and informed approach in developing the text-mining model. Our text-mining model automates the knowledge discovery process aiding researchers and clinicians in their pursuit of appropriate treatment and management of COVID-19 cases. This process is achieved by identifying whether a given medical article is related to COVID-19, and it\u2019s relevance to the competition task of identifying clinical risk factors embedded in the literature. The assumption here is that supplied databases collectively have relevant information suitable for extraction. While the tool we developed here was customised to automatically identify COVID19 related risk factors, this model can be potentially expanded to extract useful information from medical literature and building knowledge bases. Key steps involved in building the solution are -\n> \n>     1. All documents provided in the CORD19 Data were pre-processed, by grabbing all sections within documents and categorising them appropriately into Introduction, Discussion, Results OR Results and Discussions fields. \n> \n>     2. Introduction, Discussion and Results were merged to form another field\/column called the Body.\n> \n>     3. All documents that don't have Abstract and Body were dropped - which means they don\u2019t have any of Intro\/Discussion\/Body.\n> \n>     4. All documents were indexed into a search engine (acting as the data corpus from which information is extracted).\n> \n>     5. Once the tasks are queried- results are displayed and a knowledge discovery process is applied to the extracted information to gain relevant entities and valuable insights. Using those entities - algorithms are applied and graphs like Principal Component Analysis and Temporal Value Affinity charts are plotted. \n> ","907b9b09":"## IV.V. Indexing row as a document in Solr Search engine\n\n> A search engine is set up using Apache Solr, wherein one can search for a term of interest and be shown the papers that have information related to the query with keywords highlighted. By default, the result of a query is at the most five papers but this can be changed to another number. The medical entities that are mapped by the Scispacy models in the search results are also highlighted in the final result shown.\n> \n> More information regarding Solr can be found in the Solr documentation (see References).\n","1a320ffb":"## Task Results\n\n> Extraction of text segments that contain entities related to the search terms organised by their location in the document using our classification algorithm - introduction, methods, results or discussion. \nThe top 5 search results out of 2362 are shown here.","bc0efcb5":"## IV.III. Configure Search Engine\n\n> Solr, an open-source enterprise search platform, was employed as a foundational query search engine for the text mining model. Solr provides distributed indexing and load-balanced querying, which suits the challenge objectives. The search platform was enjoined with scispaCy, a package recommended by the organisers, to process the search results for biomedical or clinical terms. Initially, key terms aligning with COVID-19 including all its associated synonyms were built into the search process to filter results. The search process then incorporated the SciSpacy NER model to further filter the results. ","85ad8894":"## IV.IV. Load Data\n\n> To expedite the run time for processing code, we have included the .csv file (Cord-19.csv) that is generated after the pre-processing of all the JSON files are completed. If the CSV file is not present, the pre-processing code will run (~25-30 mins) and the CSV file will be regenerated","a4a387e8":"## IV.VIII. Clustering and Entity classification over time\n\n#### Label\n\n> Refer to \u2018Table 2  Extraction Process for Structured Articles\u2019. Each label is assigned a weight as per \u2018Table 3 Weightage for labels\u2019.\n\n#### Clustering documents based on the weightage associated with every entity (using PCA + K means)\n\n>**Aim: The aim here is to cluster documents based on the weightage of words (which we have assigned above) present in the document.**\n\n> As the number of entities is much larger than the number of documents, in order to generate readable results, it is crucial to reduce the dimensionality of the data before clustering it. Therefore, for each of the tasks, Principal Component Analysis is applied to each document to generate a Term-Label weight matrix.\n\n> Table 5: \n\n<table style=\"width:100%\">\n  <tr>\n    <th><center><\/center><\/th>\n    <th><center>Term 1<\/center><\/th> \n    <th><center>Term 2<\/center><\/th> \n    <th><center>Term 3<\/center><\/th> \n  <\/tr>\n  <tr>\n    <th><center>Document 1<\/center><\/th>\n    <td><center>6<\/center><\/td> \n    <td><center>3<\/center><\/td> \n    <td><center>1<\/center><\/td> \n  <\/tr>\n  <tr>\n    <th><center>Document 2<\/center><\/th>\n    <td><center>6<\/center><\/td> \n    <td><center>5<\/center><\/td> \n    <td><center>1<\/center><\/td> \n  <\/tr>\n  <tr>\n    <th><center>Document 3<\/center><\/th>\n    <td><center>3<\/center><\/td> \n    <td><center>2<\/center><\/td> \n    <td><center>4<\/center><\/td> \n  <\/tr>\n<\/table>\n<br\/><br\/>\n\n> K-means clustering is then applied on the vector to show the segregation of principal components. The number of components for clustering or the optimal k-value is determined as 3 by the k-means elbow method. The entity weights are leveraged to form their clusters across documents.\n> \n> A scatter plot of PC1 by PC2 while showing the clusters generated above is created. The papers that have reference to COVID-19 or its synonyms and entities with similar weights are clustered together. Densely packed clusters indicate papers with greater similarity and more relevance to the searched term.\n\n#### Entity Classification over Time\n\n> For each of the tasks, the occurrence of relevant medical entities across all papers are plotted over a period of time as bubbles, wherein the colour of the entity indicates the label of the entity as per its occurrence in the document and the size of the bubble indicates the weightage of that entity.\n\n#### Scree Plot\n> A scree plot is then plotted to show the Eigenvalues on the y-axis and the number of principal components on the x-axis for the Principal Component Analysis.\n","ca3092df":"## IV.VI. Extract Entities\n\nThis function takes in a list of NLP models (sciscpaCy) and text as parameters and runs each model on the text to generate entities and appends them to the list. This list is then returned. It is worthwhile to note that since all the four models are running on the text, each entity created will have multiple tags associated with it. This is particularly useful as each model can diagnose a particular entity with a different type.\n","631a7ca0":"# Team\n\n**Technical Team**\n* Ravi Kiran Bhaskar \n* Puru Kushwah \n* Smitan Pradhan \n* Saumya Sinha \n* Rani Lahoti \n<br>\n<br>\n\n**Medical Team**\n* Sandosh Padmanabhan \n* Chaitanya Mamillapalli \n* Sandeep Reddy \n","d139c656":"## IV. I. Installing necessary packages and libraries\n\n> Ensure that your internet is turned 'on' from the 'Settings'","5902f9d0":"## IV.IX. Document Clustering and Entity Frequency graphs\n\n> Aim - To cluster documents based on the count of words present in each document\n\n> Paper Clustering - To cluster the documents we have used Principal Component Analysis along with K means clustering to group the papers together. In this methodology, bodies of all the papers related to the search query are passed through the SciSpacy Biomedical models to extract specialised medical entities from them. These terms are then converted to a \u201cdocument - entity\u201d matrix wherein each value of the matrix would display the count of the particular entity in a paper. Each entity would become a feature of the document and since it is not necessary that all entities would be present in all documents, we get a sparse matrix ( a matrix in which most of the elements are 0) as an output\n\n<table style=\"width:100%\">\n  <tr>\n    <th><center><\/center><\/th>\n    <th><center>Virus<\/center><\/th> \n    <th><center>Pulmonary<\/center><\/th> \n    <th><center>Gene Sequences<\/center><\/th> \n    <th><center>16S rRNA<\/center><\/th> \n    <th><center>Amino Acid<\/center><\/th> \n  <\/tr>\n  <tr>\n    <th><center>Document 1<\/center><\/th>\n    <td><center>0<\/center><\/td> \n    <td><center>0<\/center><\/td> \n    <td><center>1<\/center><\/td> \n    <td><center>1<\/center><\/td> \n    <td><center>0<\/center><\/td> \n  <\/tr>\n  <tr>\n    <th><center>Document 2<\/center><\/th>\n    <td><center>2<\/center><\/td> \n    <td><center>0<\/center><\/td> \n    <td><center>1<\/center><\/td> \n    <td><center>0<\/center><\/td> \n    <td><center>0<\/center><\/td> \n  <\/tr>\n  <tr>\n    <th><center>Document 3<\/center><\/th>\n    <td><center>0<\/center><\/td> \n    <td><center>0<\/center><\/td> \n    <td><center>0<\/center><\/td> \n    <td><center>0<\/center><\/td> \n    <td><center>1<\/center><\/td> \n  <\/tr>\n<\/table>\n<br\/><br\/>\n\n> This sparse matrix is then passed through K means to cluster them into 5 different groups. Since each document is a combination of multiple entities (in other words, multiple features), to visualize each document, we passed the output through PCA to reduce the dimensionality of the features to 3 main principal component axes which can then be visualized easily.\n> \n> **Entity frequency graphs - Top 50 most occurring medical terms across papers related to the search query are then plotted. **","29a265c4":"> We applied our pipeline on Kaggle tasks as specific use cases. Running each of the Kaggle tasks through the search query in our pipeline produces results that are pesented in the panel below. We explain in detail the results for one of the tasks and present just the results of the other tasks.\n> \n> **Graphical Results for Task - Smoking and pre-existing pulmonary disease**\n> \n> **Figure 1:** We clustered documents based on the similarity of their contained curated entities using Kmeans clustering and PCA. The results show three papers (coloured green) that are likely to be less informative and the majority of the rest cluster together in two groups. Whilst the variance explained by the first 2 principal components are low, this reflects the ubiquitousness of the search terms which are present in a majority of publications related to COVID. Neverthless, our pipeline produces a prioritised set of publication of high relevance to the search terms.\n> \n> **Figure 2:**  We next present a bubble plot of all the biological\/medical entities extracted from these documents. The plot shows an increase in publications related to smoking and pulmonary diseases from 2000 onwards with major proportion of the entities indicating prior knowledge of those entities. There are a large number of publications with entities showing unknown or novel tags indicating potential new signals that merit further research.\n> \n> **Figure 3:** The second bubble plot depicts the source of the papers.\n> \n> **Figure 4:** This is another representation of document similarity, but different from figure 3 in using text frequency as a metric for each entity. Here we used three principle components to depict the clustering of publications based on entity frequency.\n> \n> **Figure 5:** A bar plot of top entities by their frequency is plotted here.\n> \n> **Figure 6:** Force directed graph showing details of each publication and their included entities each coded using our algorithm categorising them based on current understanding at publication time.","a05776ef":"# V. Results\n\n\n","546e8446":"### What do we know about COVID-19 risk factors?\n\nTask Details\n\nWhat do we know about COVID-19 risk factors? What have we learned from epidemiological studies?\nSpecifically, we want to know what the literature reports about:\n\n* Data on potential risks factors\n    1. Smoking, pre-existing pulmonary disease\n    2. Co-infections (determine whether co-existing respiratory\/viral infections make the virus more transmissible or virulent) and other co-morbidities\n    3. Neonates and pregnant women\n    4. Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.\n* Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\n* Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\n* Susceptibility of populations\n* Public health mitigation measures that could be effective for control\n","5b90f884":"# The pros and cons of our platform\n\n**Pros:** The pros of the platform are that it draws upon a multi-disciplinary team of data scientists, clinicians, medical researchers and software engineers to develop a well-rounded and powerful knowledge extraction process. This emulates the real-world clinical inductive reasoning that is utilised to evaluate, extract and prioritise insights from biomedical literature so as to inform translational medicine. \n\nThe other valuable aspect to the platform is its combinatorial knowledge extraction process, which involves both a robust information retrieval engine with a biomedical named entity recognition library. Knowledge representation was achieved through dimensionality reduction and feature affinity clustering algorithms. Further to this the garnered discernments are presented in a visually aesthetic and readable manners that ensure pertinent insights are conveyed to a wide spectrum of healthcare professionals. \n\nWe have received early and direct feedback about the value of our platform to frontline clinicians who have expressed the distinct value our platform offers to their clinical practice and research. We strongly believe because of these features the platform informs both frontline clinical delivery and backend laboratory and epidemiological research. \n\n**Cons:**\nWhile our search platform intends to extract pertinent entities from the biomedical literature, limitations associated with the biomedical named entity recognition library has restricted the ability to extract all the necessary biomedical aligned entities. The architecture of the model was customised to the submission platform specifications as opposed to the full extent we would have liked to present. With more time on our hands, we would have been in a position to present a more scalable and refined model. However, with further availability of time we have the confidence these limitations can be addressed both through a process of refinement of the named entity recognition library and streamlined programming and submission.\n","1ec08dca":"##  IV. II. Data Preprocessing\n\n> The papers present as JSON files are read into a data frame, cleaned and transformed and then converted into a CSV file, which is used for further analysis. Subject matter expertise determined that the information critical to risk factors of COVID-19 is present in certain sections of the literature. <br><br>\nAs part of the analysis, these sections were then categorized into seven categories: 'Abstract', 'Introduction', 'background', 'Discussion', 'Results', 'Results and Discussion'  and  'methods,results'. Further, it was determined that content in **'Introduction', 'Discussion', 'Results' and 'Results and Discussion' is most relevant** to the analysis. \u2018Results\u2019 and \u2018Results and Discussion\u2019 are treated under \u2018Result\u2019.\n\n<table style=\"width:100%\">\n  <tr>\n    <th><center>Paper ID<\/center><\/th>\n    <th><center>Introduction<\/center><\/th> \n    <th><center>Discussion<\/center><\/th>\n    <th><center>Result<\/center><\/th>\n  <\/tr>\n  <tr>\n    <td><center>PMC5652284<\/center><\/td>\n    <td>Zika virus is a single-stranded RNA virus prim...<\/td> \n    <td>Recently, Xu et al. performed a screen of ~ 60...<\/td>\n    <td>Of the 16 EBOV entry inhibitors reported previ...<\/td>\n  <\/tr>\n<\/table>\n<br\/><br\/>\n\n> Individual papers are processed and their paper IDs, titles, abstracts and introductions, discussions and results from the body-text are stored in separate columns. From the metadata file, for every paper based on the paper ID, the publish time is taken, transformed into the desired format and stored in a column. Similarly, the source of the papers is looked up in the metadata file based on the paper ID, and the title in the absence of paper ID, and stored as the source of the paper. \n> \n> The \u2018introduction\u2019, \u2018discussion\u2019, \u2018results\u2019 and \u2018abstract\u2019 columns are cleaned up for the removal of unnecessary non-alphanumeric characters and hyperlinks and stripped off whitespaces from both ends of the text. A new column \u2018body\u2019 is created as the concatenation of \u2018introduction\u2019, \u2018discussion\u2019 and \u2018results\u2019.\n> \n> A list of words that are synonymous with COVID-19 (refer Table 4) is defined and then searched for in the body of all papers. Papers in which any of these words are found are flagged with \u2018has_covid\u2019 = True and others are flagged with \u2018has_covid\u2019 = False.\n> \n> The data frame is then treated to drop duplicate rows (or papers), wherein duplicates are identified based on the combination of title and abstract.\n> \n> The final dataframe then has columns: 'paper_id', 'title', 'source', 'abstract', 'introduction', 'discussion', 'result', 'body',  'publish_time' and 'has_covid'; and is written to a CSV file \u2018CORD-19.csv\u2019.\n","59ea1bd4":"# III. Methodology\n\n## III.I. Section headers segmentation\n\n> We considered presenting search query results in the format of structured abstracts, which researchers and clinicians are used to when reviewing literature but also the results are segmented for ease of review and understanding. The results are segmented as introduction, results, methods and discussion as per below logic.\n\n> Table 1.Organisation of Headings\n<table style=\"width:100%\">\n  <tr>\n    <th><center>Normal Paper Headings<\/center><\/th>\n    <th><center>Result Headings<\/center><\/th> \n  <\/tr>\n  <tr>\n    <td>Introduction, Background, Challenges, Overview, epidemiology, Objectives, Purpose of the study<\/td>\n    <td><center>Introduction<\/center><\/td> \n  <\/tr>\n  <tr>\n <td>Methodology, Material and methods, Ethics Statement, Data Analysis, Statistical Analysis, Study population, Experimental design, Case description, virus detection,  RNA extraction, RT-PCR,  immunoblot analysis, data management and analysis, viral sequencing, immunohistochemistry<\/td>\n   <td><center>Introduction<\/center><\/td>\n  <\/tr>\n  <tr>\n    <td>Results,  diagnostic features, case report, sensitivity analysis,  demographic and clinical characteristics,  patient characteristics,  supplementary materials,  clinical outcomes,  secondary outcomes,  mortality, adverse events, <\/td>\n  <td><center>Results<\/center><\/td> \n  <\/tr>\n  <tr>\n <td>Discussion, conclusions,  commentary, summary, limitations, strengths and limitations, perspectives, key points, general comments, recommendations, future directions, review,  key messages, synopsis<\/td>\n  <td><center>Discussion<\/center><\/td>\n<\/tr>\n<\/table>\n\n<br\/>\n<br\/>\n\n## III.II. Creating labels and associated weights\n\n> To ensure the relevancy of results for users we created **labels** outlined in the table. The logic behind the labelling is that relevant terms presented in the introduction, results and discussion would indicate that the association is well established, and the study would be adding additional evidence. If risk factors were absent in the introduction but were present in the results and discussion this would indicate a novel finding. Additional combinations are stated in table 2.\n\n> Table 2: Label Definition\n <table style=\"width:100%\">\n  <tr>\n    <th><center>Introduction<\/center><\/th>\n    <th><center>Results<\/center><\/th> \n    <th><center>Discussion<\/center><\/th> \n    <th><center>Label<\/center><\/th> \n  <\/tr>\n  <tr>\n    <td><center>Present<\/center><\/td>\n    <td><center>Present<\/center><\/td> \n    <td><center>Present<\/center><\/td> \n    <td><center>prior-newdata<\/center><\/td> \n  <\/tr>\n  <tr>\n    <td><center>Present<\/center><\/td>\n    <td><center>Absent<\/center><\/td> \n    <td><center>Present<\/center><\/td> \n    <td><center>prior-strong<\/center><\/td> \n  <\/tr>\n  <tr>\n    <td><center>Present<\/center><\/td>\n    <td><center>Absent<\/center><\/td> \n    <td><center>Absent<\/center><\/td> \n    <td><center>prior<\/center><\/td> \n  <\/tr>\n  <tr>\n    <td><center>Absent<\/center><\/td>\n    <td><center>Absent<\/center><\/td> \n    <td><center>Present<\/center><\/td> \n    <td><center>speculative<\/center><\/td> \n  <\/tr>\n  <tr>\n    <td><center>Absent<\/center><\/td>\n    <td><center>Present<\/center><\/td> \n    <td><center>Present<\/center><\/td> \n    <td><center>novel<\/center><\/td> \n  <\/tr>\n  <tr>\n    <td><center>Absent<\/center><\/td>\n    <td><center>Present<\/center><\/td> \n    <td><center>Absent<\/center><\/td> \n    <td><center>unknown<\/center><\/td> \n  <\/tr>\n  <tr>\n    <td><center>Present<\/center><\/td>\n    <td colspan=\"2\"><center>Present<\/center><\/td> \n    <td><center>prior-newdata<\/center><\/td> \n  <\/tr>\n  <tr>\n    <td><center>Present<\/center><\/td>\n    <td colspan=\"2\"><center>Absent<\/center><\/td> \n    <td><center>prior<\/center><\/td> \n  <\/tr>\n  <tr>\n    <td><center>Absent<\/center><\/td>\n    <td colspan=\"2\"><center>Present<\/center><\/td> \n    <td><center>novel<\/center><\/td> \n  <\/tr>\n<\/table>\n<br\/>\n<br\/>\n\n> Table 3: Label weights\n<table style=\"width:100%\">\n  <tr>\n    <th><center>Labels<\/center><\/th>\n    <th><center>Weights<\/center><\/th> \n  <\/tr>\n  <tr>\n    <td><center>Prior-newdata<\/center><\/td>\n    <td><center>6<\/center><\/td> \n  <\/tr>\n  <tr>\n <td><center>Prior-strong<\/center><\/td>\n   <td><center>5<\/center><\/td>\n  <\/tr>\n  <tr>\n    <td><center>Prior<\/center><\/td>\n  <td><center>4<\/center><\/td> \n  <\/tr>\n  <tr>\n <td><center>Novel<\/center><\/td>\n  <td><center>3<\/center><\/td>\n<\/tr>\n  <tr>\n <td><center>Speculative<\/center><\/td>\n  <td><center>2<\/center><\/td>\n<\/tr>\n  <tr>\n <td><center>Unknown<\/center><\/td>\n  <td><center>1<\/center><\/td>\n<\/tr>\n<\/table>\n\n<br\/>\n<br\/>\n\n\n## III.III. Synonyms of COVID-19\n> The following synonyms of COVID-19 have been taken into consideration to ensure that the search results are exhaustive and thorough.<br\/>\n>\n> Table 4: Synonyms of COVID-19\n>\n> * COVID-19\n> * COVID19\n> * 2019-nCoV\n> * 2019nCoV\n> * Coronavirus\n> * SARS-CoV-2\n> * SARSCov2\n> * novel Coronavirus\n\n\n## III.IV. Techniques applied for visualization\n  \n  > **Principal Component Analysis**<br><br>\n  Throughout the model, principal component analysis has been performed at multiple places to reduce dimensionality. Principal component analysis (PCA) is a technique for reducing the dimensionality of large datasets, increasing interpretability but at the same time minimizing information loss.You can read more about PCA [here](https:\/\/medium.com\/@aptrishu\/understanding-principle-component-analysis-e32be0253ef0).\n  \n  > **K Means Clustering**<br><br>\n  Kmeans algorithm is an iterative algorithm that tries to partition the dataset into K pre-defined distinct non-overlapping subgroups (clusters) where each data point belongs to only one group. It tries to make the inter-cluster data points as similar as possible while also keeping the clusters as different (far) as possible. It assigns data points to a cluster such that the sum of the squared distance between the data points and the cluster\u2019s centroid (arithmetic mean of all the data points that belong to that cluster) is at the minimum. The less variation we have within clusters, the more homogeneous (similar) the data points are within the same cluster.<br>\n  [Here](https:\/\/www.youtube.com\/watch?v=4b5d3muPQmA) is a small video by StatQuest explaining K means algorithm with video \n","8c4552c6":"![Results_Figures.png](attachment:Results_Figures.png)","6247ed78":"# I. Introduction\n  \n> In a pandemic situation, clinicians and researchers are in urgent need of rapid and quality information that will help them to inform diagnostics and therapeutics relating to the disease. Traditional research models producing trustworthy and methodologically sound results takes time, which does not fit well with a pandemic context where research has to be fast-tracked. The ongoing coronavirus disease 2019 (COVID-19) pandemic has demonstrated the volume and velocity of scientific information that can be produced in a short period of time. For COVID-19, some of these traditional delays have been circumvented as many medical journals have prioritized publications related to COVID-19 and there is greater use of preprint servers to make research findings immediately available in an open format. \n> \n> Whilst not everything posing as trustworthy research is truly so, medical literature (pre-print and otherwise) is yet an important source of information in the pandemic context. However, staying current with the growing body of literature is a challenge. Generally, when publishing their research, the authors indicate what other work has been done in the past, describe their data and results and cite previous literature to support or refute their interpretation of the data. \n> \n> In the context of COVID-19 pandemic, researchers and clinicians require a reliable model to mine published literature for novel insights, emerging risk factors and therapeutics to inform their work in combating the COVID-19 pandemic. In this context, we present an innovative text mining and analytical tool that will aid clinicians and researchers in extracting valuable insights from large datasets of literature. In the following sections, we describe the principles, techniques and results of our approach."}}