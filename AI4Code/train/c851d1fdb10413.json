{"cell_type":{"c79dc439":"code","2a431d68":"code","57da2e9c":"code","3ddbd575":"code","cc936c34":"code","5b0c229a":"code","f7b6bcf6":"code","87d712e8":"code","76ccd100":"code","2187601e":"code","1b41a7f4":"code","ba97e083":"code","d66cc6f6":"code","e62ad6cc":"code","89b6ebae":"code","9569402f":"code","6d03d82c":"code","17d8dc55":"code","5295dd9a":"code","6b34a6df":"code","7a118ea4":"code","8783d1d3":"code","b7e2e2e3":"code","f45b0264":"code","f78da02c":"code","d717ab8b":"code","71184596":"markdown","200fd6a5":"markdown","dd6f9627":"markdown","151e0432":"markdown","f9bf9aba":"markdown","7fb2e6da":"markdown","110db1f8":"markdown","3f7e4e0a":"markdown","8c02e538":"markdown","d2f5d993":"markdown","6b87b015":"markdown","82597e82":"markdown","9d0c582b":"markdown","0d9cc974":"markdown","545bfff6":"markdown","ff955853":"markdown","f80120d1":"markdown","ff41f4ff":"markdown","8bce56e2":"markdown","25e8bdc8":"markdown","bc86691e":"markdown","7441c400":"markdown","7b062130":"markdown"},"source":{"c79dc439":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt","2a431d68":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV","57da2e9c":"import warnings\nwarnings.filterwarnings(\"ignore\")","3ddbd575":"path = '\/kaggle\/input\/iris\/'\nos.listdir(path)","cc936c34":"data = pd.read_csv(path+'Iris.csv', index_col=0)","5b0c229a":"print('number of samples:', len(data.index))\nprint('number of features:', len(data.columns))","f7b6bcf6":"data.head()","87d712e8":"data['Species'].value_counts()","76ccd100":"def plot_features(data):\n    fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n    fig.subplots_adjust(hspace = 0.5, wspace=0.2)\n    axs = axs.ravel()\n    colors = ['red', 'orange', 'blue']\n    species = data['Species'].unique()\n    features = ['Sepal', 'Petal']\n    for feature in range(2):\n        for i in range(3):\n            df_species = data[data['Species']==species[i]]\n            axs[feature].scatter(df_species[features[feature]+'LengthCm'],\n                                 df_species[features[feature]+'WidthCm'],\n                                 c=colors[i],\n                                 label=species[i],\n                                 alpha=0.5)\n        axs[feature].set_title(features[feature])\n        axs[feature].legend(loc='lower right')\n        axs[feature].set_xlabel('lenght (cm)')\n        axs[feature].set_ylabel('width (cm)')\n        axs[feature].grid()","2187601e":"def calculate(X, y, max_num_features):\n    \"\"\" Calulate the best score of grid search \"\"\"\n    \n    knn = KNeighborsClassifier()\n    knn_params = {'n_neighbors': [i+1 for i in range(max_num_features)]}\n    grid = GridSearchCV(knn, knn_params, verbose=1)\n    grid.fit(X, y)\n    print('best score:', grid.best_score_)\n    print('best param:', grid.best_params_)","1b41a7f4":"def rename_species(s):\n    return s.split('-')[1]","ba97e083":"data['Species'] = data['Species'].apply(rename_species)","d66cc6f6":"plot_features(data)","e62ad6cc":"df_features = data[data.columns[:-1]]\ndf_features -= df_features.mean() ","89b6ebae":"df_cov_matrix = np.cov(df_features.T)\ndf_cov_matrix","9569402f":"eig_vals, eig_vecs = np.linalg.eig(df_cov_matrix)\nprint('\\nEigenvalues \\n%s' %eig_vals)\nprint('Eigenvectors \\n%s' %eig_vecs)","6d03d82c":"eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\nprint('Eigenvalues in descending order:')\nfor i in eig_pairs:\n    print(i[0])","17d8dc55":"total = sum(eig_vals)\nvar_exp = [(i \/ total)*100 for i in sorted(eig_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\nprint('Variance captured by each component is \\n',var_exp)\nprint('Cumulative variance captured as we travel with each component \\n',cum_var_exp)","5295dd9a":"two_eigenvectors = eig_vecs[:, :2].T\ntwo_eigenvectors","6b34a6df":"df_transformed = np.dot(df_features, two_eigenvectors.T)","7a118ea4":"print('original shape:', df_features.shape)\nprint('transformed shape:', df_transformed.shape)","8783d1d3":"number_of_components = 2\ncomponent_names = ['PC'+str(i+1) for i in range(number_of_components)]\npca = PCA(n_components = number_of_components)\npcs = pca.fit_transform(df_features)\ndf_transformed = pd.DataFrame(data=pcs, columns=component_names)","b7e2e2e3":"X = data[data.columns[:-1]]\ny = data['Species']\ncalculate(X, y, 4)","f45b0264":"number_of_components = 2\ncomponent_names = ['PC'+str(i+1) for i in range(number_of_components)]\npca = PCA(n_components = number_of_components)\npcs = pca.fit_transform(X)\nX_pca = pd.DataFrame(data=pcs, columns=component_names)\ncalculate(X_pca, y, number_of_components)","f78da02c":"X_scaled_mean = X-X.mean()\nnumber_of_components = 2\ncomponent_names = ['PC'+str(i+1) for i in range(number_of_components)]\npca = PCA(n_components = number_of_components)\npcs = pca.fit_transform(X_scaled_mean)\nX_scaled_mean = pd.DataFrame(data=pcs, columns=component_names)\ncalculate(X_scaled_mean, y, number_of_components)","d717ab8b":"X_scaled = StandardScaler().fit_transform(df_features)\nnumber_of_components = 2\ncomponent_names = ['PC'+str(i+1) for i in range(number_of_components)]\npca = PCA(n_components = number_of_components)\npcs = pca.fit_transform(X_scaled)\nX_scaled = pd.DataFrame(data=pcs, columns=component_names)\ncalculate(X_scaled, y, number_of_components)","71184596":"Scaled by mean:","200fd6a5":"# PCA With Scikit-learn","dd6f9627":"Scaled with Standard Scaler:","151e0432":"Standardization of the data:","f9bf9aba":"By multiplying the orginial data with the two eigenvectors will give us the transformed data with reduced dimensions:","7fb2e6da":"Calculating the eigenvalues and eigenvectors of the covariance matrix:","110db1f8":"# Visualization","3f7e4e0a":"# Principal Component Analysis\nFor more informations about PCA we consider [this](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis). ","8c02e538":"# Summary\nThe reduction of dimension can help to handle big datasets. In the examples above the reduction by PCA reduces also the accuracy of prediction. ","d2f5d993":"# Functions","6b87b015":"# Libraries","82597e82":"PCA without scaling of the origin data:","9d0c582b":"Selecting the number of principal components:","0d9cc974":"# Load Data","545bfff6":"Calculate the covariance matrix:","ff955853":"# Rename Species","f80120d1":"Original data:","ff41f4ff":"As we can see the first two componentes account nearly 98% of the total variance. If we only use this two components we are able to shrink the size of the dataset by half (2 instead of 4 columns).","8bce56e2":"# Analyse Impact Of PCA\nWe analyse the impact of the PCA with different kinds of scaling and compare with the results based on the original data. We use a simple classifier.","25e8bdc8":"# Overview","bc86691e":"# Intro\nWelecome to the famous [Iris](https:\/\/www.kaggle.com\/uciml\/iris) dataset.\n![](https:\/\/storage.googleapis.com\/kaggle-datasets-images\/19\/19\/default-backgrounds\/dataset-cover.jpg)\n\nThe flowers are classified by the features\n* sepal lenght in cm,\n* sepal width in cm,\n* pedal lenth in cm,\n* pedal lenght in cm.\n\nThe different species are (click on the link for more informations):\n* [setosa](https:\/\/en.wikipedia.org\/wiki\/Iris_setosa)\n* [virginica](https:\/\/en.wikipedia.org\/wiki\/Iris_virginica),\n* [versicolor](https:\/\/en.wikipedia.org\/wiki\/Iris_versicolor).\n\nIn this notebook we focus on the reduction of the dimensions.\n\n<span style=\"color: royalblue;\">Please vote the notebook up if it helps you. Thank you. <\/span>","7441c400":"Sorting the list of eigenvalues in descending order:","7b062130":"# Path"}}