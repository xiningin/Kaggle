{"cell_type":{"8290e5cd":"code","12433344":"code","4e19ed75":"code","bc1b2d3e":"code","79833abe":"code","81588e90":"code","0d50b03b":"code","bfc588f6":"code","2b3f419c":"code","58984355":"code","39fda83b":"code","7f861bee":"code","d6282554":"code","a55b1352":"code","906fbbcc":"code","d2cf9216":"code","e094d782":"code","78db4ba4":"code","9b96d5a4":"code","19aed9cf":"code","7cc4f126":"code","c18b3867":"code","2f7a7eaa":"code","acdb29d9":"code","5dfd82e0":"code","4b7a2e66":"code","45e29840":"code","1a7c7787":"code","9fe98eb4":"code","0adcfbc7":"code","0bd45991":"code","67e889c2":"code","1829a02f":"code","2d246ddd":"code","51b62372":"code","693a5c1b":"code","07f212ee":"code","e9751e52":"code","0e7d6033":"code","e3c9b46d":"code","02781ebc":"code","d2a86548":"code","9fa9f148":"code","12f10a82":"code","2a48246f":"code","df91ac91":"code","26e3a414":"code","54bfdd89":"code","7c92a562":"code","1dc71520":"code","d0bcfbca":"code","fa5afcde":"code","1b0ebb88":"code","01f201fb":"code","5afb7f01":"code","83d701d6":"code","47d5020e":"code","f844602c":"code","580aaaf0":"code","2541774c":"code","ed51346b":"code","f27588b0":"code","d6dddb8e":"code","30da91da":"code","5b6125b6":"code","269952b1":"code","8054d4ac":"code","ec7f30f8":"code","ff2d4eeb":"code","668a5978":"code","edcee041":"code","06a7038f":"code","6770d9a1":"code","1bcaeb6f":"code","ba8b073a":"code","30770b28":"code","b8975855":"code","704eaf1a":"code","4e8e2e5e":"code","8feece0a":"code","38d7bc14":"code","86faf429":"code","6298583f":"code","1d603ebf":"code","ef3c8454":"code","28f9cfe4":"markdown","06b9f9ef":"markdown","14879598":"markdown","7046a69a":"markdown","6334c9d9":"markdown","c2739912":"markdown","3df3e9c0":"markdown","d4d5c62c":"markdown","4ef01de2":"markdown","05852764":"markdown","0f8d363c":"markdown","936269e8":"markdown","c964a8c6":"markdown","f1aee6c1":"markdown","49b6f6d8":"markdown","c7ca3be7":"markdown","3032c7b8":"markdown","2075d592":"markdown","7f048b50":"markdown","d976dbaa":"markdown","9977215c":"markdown","b96e5fea":"markdown","6727938b":"markdown","0a6a9234":"markdown","2bc6faa6":"markdown","9c798644":"markdown","69d66276":"markdown","d1038618":"markdown","3fecfded":"markdown","dd844cc3":"markdown","44b41aa0":"markdown","3b85499e":"markdown","2b50b84c":"markdown","09c21dfc":"markdown","71f55fe9":"markdown","6fd29725":"markdown","b0bc3754":"markdown","468bc173":"markdown","84fc7589":"markdown","8dd88c84":"markdown","b63f1922":"markdown","d52d8fc2":"markdown","a0d96491":"markdown","97c14c6e":"markdown","5557b46e":"markdown","3abbf1fe":"markdown","b1685f0b":"markdown","24a2c058":"markdown","13dd1813":"markdown","57b658c8":"markdown","13bfee06":"markdown","47deb39e":"markdown"},"source":{"8290e5cd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","12433344":"! pip install transformers==3.2.0 --user\n! pip install tensorflow==2.2 --user","4e19ed75":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport pandas as pd\nimport string as s\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport transformers\nfrom wordcloud import WordCloud\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\n%matplotlib inline\nfrom sklearn.feature_extraction.text  import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn.metrics  import f1_score, accuracy_score, confusion_matrix, recall_score, precision_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport sklearn\nfrom lightgbm import LGBMClassifier\nimport tensorflow as tf\nfrom keras.utils import to_categorical","bc1b2d3e":"#bert large uncased pretrained tokenizer\ntokenizer = transformers.BertTokenizer.from_pretrained('bert-large-uncased')","79833abe":"# Loading category list\ncategory_list_df = pd.read_csv(\"\/kaggle\/input\/draup-news-classification\/categories.csv\")\ncategory_list_df","81588e90":"# Loading category mapping list\ncategory_mapping_df = pd.read_excel(\"\/kaggle\/input\/draup-news-classification\/category_mapping.xlsx\")\ncategory_mapping_df.head()","0d50b03b":"# Loading news data\nnews_details_df = pd.read_excel(\"\/kaggle\/input\/draup-news-classification\/news_details.xlsx\")\nnews_details_df = news_details_df.drop_duplicates( keep='first')\nnews_details_df.head()","bfc588f6":"# Mapping news_id with category name\ncategory_df = pd.merge(category_mapping_df, category_list_df, left_on='category_id', right_on=\"id\").drop('id', axis=1)\ncategory_df = category_df.drop_duplicates( keep='first')\ncategory_df.head()","2b3f419c":"# Mapping news_id with category_id\nfinal_df = news_details_df.merge(category_df, on='news_id')\nfinal_df.head()","58984355":"# Printing Sample Snippet\nfinal_df.iloc[3][1]","39fda83b":"# Printing Sample Title\nfinal_df.iloc[3][2]","7f861bee":"# Printing Sample News Description\nfinal_df.iloc[3][3]","d6282554":"# Checking for missing snippets\/titles\/descriptions\nfinal_df.info()","a55b1352":"# Replacing NAs with empty string\nfinal_df = final_df.fillna('')\nfinal_df.head()","906fbbcc":"# Converting each of snippet, title, and news description into lower case.\nfinal_df['snippet'] = final_df['snippet'].apply(lambda snippet: str(snippet).lower())\nfinal_df['title'] = final_df['title'].apply(lambda title: str(title).lower())\nfinal_df['news_description'] = final_df['news_description'].apply(lambda news_description: str(news_description).lower())","d2cf9216":"#calculating the length of snippets, titles and descriptions\nfinal_df['snippet_len'] = final_df['snippet'].apply(lambda x: len(str(x).split()))\nfinal_df['title_len'] = final_df['title'].apply(lambda x: len(str(x).split()))\nfinal_df['news_description_len'] = final_df['news_description'].apply(lambda x: len(str(x).split()))","e094d782":"final_df.describe()","78db4ba4":"def fx(x):\n    if len(x['news_description'])==0:\n        return x['title'] + \" \" + x['snippet']\n    else:\n        return x['title'] + \" \" + x['news_description']   \nfinal_df['text']=final_df.apply(lambda x : fx(x),axis=1)","9b96d5a4":"final_df.head()","19aed9cf":"def tokenization(text):\n    lst=text.split()\n    return lst","7cc4f126":"def remove_new_lines(lst):\n    new_lst=[]\n    for i in lst:\n        i=i.replace(r'\\n', ' ').replace(r'\\r', ' ').replace(r'\\u', ' ')\n        new_lst.append(i.strip())\n    return new_lst","c18b3867":"def remove_punctuations(lst):\n    new_lst=[]\n    for i in lst:\n        for  j in s.punctuation:\n            i=i.replace(j,' ')\n        new_lst.append(i.strip())\n    return new_lst","2f7a7eaa":"def remove_numbers(lst):\n    nodig_lst=[]\n    new_lst=[]\n    for i in  lst:\n        for j in  s.digits:\n            i=i.replace(j,' ')\n        nodig_lst.append(i.strip())\n    for i in  nodig_lst:\n        if  i!='':\n            new_lst.append(i.strip())\n    return new_lst","acdb29d9":"def remove_stopwords(lst):\n    stop=stopwords.words('english')\n    new_lst=[]\n    for i in lst:\n        if i not in stop:\n            new_lst.append(i.strip())\n    return new_lst","5dfd82e0":"lemmatizer=nltk.stem.WordNetLemmatizer()\ndef lemmatization(lst):\n    new_lst=[]\n    for i in lst:\n        i=lemmatizer.lemmatize(i)\n        new_lst.append(i.strip())\n    return new_lst","4b7a2e66":"def remove_urls(text):\n    return re.sub(r'http\\S+', ' ', text)","45e29840":"def split_words(text):\n    return ' '.join(text).split()","1a7c7787":"def remove_single_chars(lst):\n    new_lst=[]\n    for i in lst:\n        if len(i)>1:\n            new_lst.append(i.strip())\n    return new_lst","9fe98eb4":"# Cleaning Text\ndef denoise_text(text):\n    text = remove_urls(text)\n    text = tokenization(text)\n    text = remove_new_lines(text)\n    text = remove_punctuations(text)\n    text = remove_numbers(text)\n    text = remove_stopwords(text)\n    text = split_words(text)\n    text = remove_single_chars(text)\n    text = lemmatization(text)\n    return text\n\nfinal_df['text'] = final_df['text'].apply(lambda x: denoise_text(x))","0adcfbc7":"ax = final_df.groupby('category')['news_id'].count().plot(kind='barh', figsize=(10,6), fontsize=13)\nax.set_alpha(0.8)\nax.set_title(\"Count of Each News Category\", fontsize=18)\nax.set_ylabel(\"Category\", fontsize=15)\nax.set_xlabel(\"Count\", fontsize=15)\nfor p in ax.patches:\n    ax.annotate(str(p.get_width()), (p.get_x() + p.get_width(), p.get_y()), xytext=(5, 10), textcoords='offset points')\nplt.show()","0bd45991":"layoff = final_df[final_df.category=='Layoff']['text'].apply(lambda x:' '.join(x))\nplt.figure(figsize = (15,20))\nwordcloud = WordCloud(min_font_size = 3,  max_words = 2500 , width = 1200 , height = 800).generate(\" \".join(layoff))\nplt.imshow(wordcloud,interpolation = 'bilinear')\nplt.show()","67e889c2":"mergers_acquisitions = final_df[final_df.category=='Mergers and Acquisitions']['text'].apply(lambda x: ' '.join(x))\nplt.figure(figsize = (15,20))\nwordcloud = WordCloud(min_font_size = 3,  max_words = 2500 , width = 1200 , height = 800).generate(\" \".join(mergers_acquisitions))\nplt.imshow(wordcloud,interpolation = 'bilinear')\nplt.show()","1829a02f":"mass_hiring = final_df[final_df.category=='Mass Hiring']['text'].apply(lambda x: ' '.join(x))\nplt.figure(figsize = (15,20))\nwordcloud = WordCloud(min_font_size = 3,  max_words = 2500 , width = 1200 , height = 800).generate(\" \".join(mass_hiring))\nplt.imshow(wordcloud,interpolation = 'bilinear')\nplt.show()","2d246ddd":"executive_movement = final_df[final_df.category=='Executive Movement']['text'].apply(lambda x: ' '.join(x))\nplt.figure(figsize = (15,20))\nwordcloud = WordCloud(min_font_size = 3,  max_words = 2500 , width = 1200 , height = 800).generate(\" \".join(executive_movement))\nplt.imshow(wordcloud,interpolation = 'bilinear')\nplt.show()","51b62372":"setup_expansion = final_df[final_df.category=='Centre Setup and Expansion']['text'].apply(lambda x: ' '.join(x))\nplt.figure(figsize = (15,20))\nwordcloud = WordCloud(min_font_size = 3,  max_words = 2500 , width = 1200 , height = 800).generate(\" \".join(setup_expansion))\nplt.imshow(wordcloud,interpolation = 'bilinear')\nplt.show()","693a5c1b":"deals = final_df[final_df.category=='Deals']['text'].apply(lambda x: ' '.join(x))\nplt.figure(figsize = (15,20))\nwordcloud = WordCloud(min_font_size = 3,  max_words = 2500 , width = 1200 , height = 800).generate(\" \".join(deals))\nplt.imshow(wordcloud,interpolation = 'bilinear')\nplt.show()","07f212ee":"partnerships = final_df[final_df.category=='Partnerships']['text'].apply(lambda x: ' '.join(x))\nplt.figure(figsize = (15,20))\nwordcloud = WordCloud(min_font_size = 3,  max_words = 2500 , width = 1200 , height = 800).generate(\" \".join(partnerships))\nplt.imshow(wordcloud,interpolation = 'bilinear')\nplt.show()","e9751e52":"final_df['text_char_len'] = final_df['text'].apply(lambda x: len(' '.join(x)))","0e7d6033":"fig, ax=plt.subplots(4,2,figsize=(16,16))\ncount = 1\nfor i in ax:\n    for j in i:\n        if count==8:\n            j.axis('off')\n            break\n        text_len=final_df[final_df['category_id']==count]['text_char_len'] \n        j.hist(text_len, bins=20)\n        j.set_title(final_df[final_df['category_id']==count]['category'].values[0])\n        count+=1","e3c9b46d":"final_df['text_word_len'] = final_df['text'].str.len() ","02781ebc":"fig, ax=plt.subplots(4,2,figsize=(16,16))\ncount = 1\nfor i in ax:\n    for j in i:\n        if count==8:\n            j.axis('off')\n            break\n        text_len=final_df[final_df['category_id']==count]['text_word_len'] \n        j.hist(text_len, bins=20)\n        j.set_title(final_df[final_df['category_id']==count]['category'].values[0])\n        count+=1","d2a86548":"final_df.text_word_len","9fa9f148":"# KDE plot for words and characters length\n# fig, ax=plt.subplots(1,2,figsize=(16,6))\n# sns.kdeplot(data=final_df, x='text_word_len', hue='category', ax=ax[0])\n# sns.kdeplot(data=final_df, x='text_char_len', hue='category', ax=ax[1], multiple='stack')\n# plt.show()","12f10a82":"final_df['text_avg_word_len'] = final_df['text'].apply(lambda x : np.mean([len(i) for i in x]))","2a48246f":"fig, ax=plt.subplots(4,2,figsize=(16,16), constrained_layout=True)\ncount = 1\nfor i in ax:\n    for j in i:\n        if count==8:\n            j.axis('off')\n            break\n        word=final_df[final_df['category_id']==count]['text_avg_word_len']\n        sns.distplot(word, ax=j)\n        j.set_title(final_df[final_df['category_id']==count]['category'].values[0])\n        count+=1\n        j.set_xlabel('')","df91ac91":"# Word Corpus\ndef get_corpus(text):\n    words = []\n    for i in text:\n        for j in i:\n            words.append(j.strip())\n    return words\ncorpus = get_corpus(final_df.text)\ncorpus[:5]","26e3a414":"# Most common words\nfrom collections import Counter\ncounter = Counter(corpus)\nmost_common = counter.most_common(10)\nmost_common = dict(most_common)\nmost_common","54bfdd89":"from sklearn.feature_extraction.text import CountVectorizer\ndef get_top_text_ngrams(corpus, n, g):\n    vec = CountVectorizer(ngram_range=(g, g)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","7c92a562":"plt.figure(figsize = (16,9))\nmost_common_uni = get_top_text_ngrams(final_df.text.apply(lambda x: ' '.join(x)),10,1)\nmost_common_uni = dict(most_common_uni)\nsns.barplot(x=list(most_common_uni.values()), y=list(most_common_uni.keys()))\nplt.show()","1dc71520":"plt.figure(figsize = (16,9))\nmost_common_bi = get_top_text_ngrams(final_df.text.apply(lambda x: ' '.join(x)),10,2)\nmost_common_bi = dict(most_common_bi)\nsns.barplot(x=list(most_common_bi.values()),y=list(most_common_bi.keys()))\nplt.show()","d0bcfbca":"plt.figure(figsize = (16,9))\nmost_common_tri = get_top_text_ngrams(final_df.text.apply(lambda x: ' '.join(x)),10,3)\nmost_common_tri = dict(most_common_tri)\nsns.barplot(x=list(most_common_tri.values()),y=list(most_common_tri.keys()))\nplt.show()","fa5afcde":"#label encoding the categories. After this each category would be mapped to an integer.\nencoder = LabelEncoder()\nfinal_df['categoryEncoded'] = encoder.fit_transform(final_df['category'])","1b0ebb88":"X_train, X_test, y_train, y_test = train_test_split(final_df['text'], final_df['categoryEncoded'], random_state = 43, test_size = 0.2)","01f201fb":"def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_mask=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])","5afb7f01":"# Tokenizing the news descriptions and converting the categories into one hot vectors using tf.keras.utils.to_categorical\nXtrain_encoded = regular_encode(X_train.apply(lambda x: ' '.join(x)).astype('str'), tokenizer, maxlen=256)\nXtest_encoded = regular_encode(X_test.apply(lambda x: ' '.join(x)).astype('str'), tokenizer, maxlen=256)\nytrain_encoded = to_categorical(y_train, dtype='int32')\nytest_encoded = to_categorical(y_test, dtype='int32')","83d701d6":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","47d5020e":"train_x=X_train.apply(lambda x: ''.join(i+' ' for i in x))\ntest_x=X_test.apply(lambda x: ''.join(i+' '  for i in x))","f844602c":"tfidf=TfidfVectorizer(max_features=10000,min_df=6)\ntrain_1=tfidf.fit_transform(train_x)\ntest_1=tfidf.transform(test_x)\nprint(\"No. of features extracted:\", len(tfidf.get_feature_names()))\nprint(tfidf.get_feature_names()[:20])\n\ntrain_arr=train_1.toarray()\ntest_arr=test_1.toarray()","580aaaf0":"NB_MN=MultinomialNB()\nNB_MN.fit(train_arr,y_train)\npred=NB_MN.predict(test_arr)\n\nprint(\"first 20 actual labels\")\nprint(y_test.tolist()[:20])\nprint(\"first 20 predicted labels\")\nprint(pred.tolist()[:20])","2541774c":"def eval_model(y,y_pred):\n    print(\"Recall score of the model:\", round(recall_score(y_test, pred, average='weighted'), 3))\n    print(\"Precision score of the model:\", round(precision_score(y_test, pred, average='weighted'), 3))\n    print(\"F1 score of the model:\", round(f1_score(y,y_pred,average='micro'), 3))\n    print(\"Accuracy of the model:\", round(accuracy_score(y,y_pred),3))\n    print(\"Accuracy of the model in percentage:\", round(accuracy_score(y,y_pred)*100,3),\"%\")","ed51346b":"def confusion_mat(color):\n    cof=confusion_matrix(y_test, pred)\n    cof=pd.DataFrame(cof, index=[i for i in range(1,8)], columns=[i for i in range(1,8)])\n    sns.set(font_scale=1.5)\n    plt.figure(figsize=(8,8));\n\n    sns.heatmap(cof, cmap=color,linewidths=1, annot=True,square=True, fmt='d', cbar=False,xticklabels=list(encoder.classes_),yticklabels=list(encoder.classes_));\n    plt.xlabel(\"Predicted Classes\");\n    plt.ylabel(\"Actual Classes\");\n    ","f27588b0":"eval_model(y_test,pred)\n    \na=round(accuracy_score(y_test,pred)*100,3)","d6dddb8e":"confusion_mat('YlGnBu')","30da91da":"DT=DecisionTreeClassifier()\nDT.fit(train_arr,y_train)\npred=DT.predict(test_arr)\n\nprint(\"first 20 actual labels\")\nprint(y_test.tolist()[:20])\nprint(\"first 20 predicted labels\")\nprint(pred.tolist()[:20])","5b6125b6":"eval_model(y_test,pred)\nb=round(accuracy_score(y_test,pred)*100,3)","269952b1":"confusion_mat('Blues')","8054d4ac":"NB=GaussianNB()\nNB.fit(train_arr,y_train)\npred=NB.predict(test_arr)","ec7f30f8":"eval_model(y_test,pred)\n    \nc=round(accuracy_score(y_test,pred)*100,3)","ff2d4eeb":"confusion_mat('Greens')","668a5978":"SGD=SGDClassifier()\nSGD.fit(train_arr,y_train)\npred=SGD.predict(test_arr)","edcee041":"eval_model(y_test,pred)\n    \nd=round(accuracy_score(y_test,pred)*100,3)","06a7038f":"confusion_mat('Reds')","6770d9a1":"lgbm=LGBMClassifier()\nlgbm.fit(train_arr,y_train)\npred=lgbm.predict(test_arr)","1bcaeb6f":"eval_model(y_test,pred)\n\ne=round(accuracy_score(y_test,pred)*100,3)","ba8b073a":"confusion_mat('YlOrBr')","30770b28":"def build_model(transformer, loss='categorical_crossentropy', max_len=512):\n    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    #adding dropout layer\n    x = tf.keras.layers.Dropout(0.3)(cls_token)\n    #using a dense layer of 7 neurons as the number of unique categories is 7. \n    out = tf.keras.layers.Dense(7, activation='softmax')(x)\n    model = tf.keras.Model(inputs=input_word_ids, outputs=out)\n    #using categorical crossentropy as the loss as it is a multi-class classification problem\n    model.compile(tf.keras.optimizers.Adam(lr=3e-5), loss=loss, metrics=['accuracy'])\n    return model","b8975855":"#building the model on tpu\nwith strategy.scope():\n    transformer_layer = transformers.TFAutoModel.from_pretrained('bert-large-uncased')\n    model = build_model(transformer_layer, max_len=256)\nmodel.summary()","704eaf1a":"#creating the training and testing dataset.\nBATCH_SIZE = 32*strategy.num_replicas_in_sync\nAUTO = tf.data.experimental.AUTOTUNE \ntrain_dataset = (tf.data.Dataset.from_tensor_slices((Xtrain_encoded, ytrain_encoded)).repeat().shuffle(2048).batch(BATCH_SIZE)\n                 .prefetch(AUTO))\ntest_dataset = (tf.data.Dataset.from_tensor_slices(Xtest_encoded).batch(BATCH_SIZE))","4e8e2e5e":"#training for 10 epochs\nn_steps = Xtrain_encoded.shape[0] \/\/ BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    epochs=10\n)","8feece0a":"#making predictions\npreds = model.predict(test_dataset,verbose = 1)\n#converting the one hot vector output to a linear numpy array.\npred = np.argmax(preds, axis = 1)","38d7bc14":"#extracting the classes from the label encoder\nencoded_classes = encoder.classes_\n#mapping the encoded output to actual categories\npredicted_category = [encoded_classes[x] for x in pred]\ntrue_category = [encoded_classes[x] for x in y_test]","86faf429":"result_df = pd.DataFrame({'description':X_test,'true_category':true_category, 'predicted_category':predicted_category})\nresult_df.head()","6298583f":"eval_model(y_test,pred)\n\nf=round(accuracy_score(y_test,pred)*100,3)","1d603ebf":"confusion_mat('YlOrBr')","ef3c8454":"sns.set()\nfig = plt.figure(figsize=(10,6))\nax = fig.add_axes([0,0,1,1])\nModels = ['MultinomialNB', 'DecisionTree', 'GaussianNB', 'SGD','LGBM', 'BERT']\nAccuracy=[a,b,c,d,e,f]\nax.bar(Models,Accuracy);\nfor i in ax.patches:\n    ax.text(i.get_x()+.1, i.get_height()-5.5, str(round(i.get_height(),2))+'%', fontsize=13, color='white')\nplt.title('Comparison of Different Classification Models');\nplt.ylabel('Accuracy');\nplt.xlabel('Classification Models');\n\nplt.show();","28f9cfe4":"#### Deals","06b9f9ef":"## Comparison of Accuracies of Different Models","14879598":"#### Layoff","7046a69a":"#### Mass Hiring","6334c9d9":"The given news classifier is trained on using BERT model. Since, BERT is a very large model, it requires gpu's and tpu's to train quickly. Here, I have used TPU's provided by Kaggle to train this model.","c2739912":"#### Average word length in a text for each category","3df3e9c0":"#### Partnerships","d4d5c62c":"#### Removal of Stopwords","4ef01de2":"#### Tokenization of Data","05852764":"#### Split words","0f8d363c":"#### Confusion Matrix","936269e8":"### Model 3 - Gaussian Naive Bayes","c964a8c6":"#### Bigram Analysis","f1aee6c1":"#### Creating Corpus of Words in Text","49b6f6d8":"#### Removal of Punctuation Symbols","c7ca3be7":"#### Removal of Numbers(digits)","3032c7b8":"#### Lemmatization of Data","2075d592":"#### Unigram Analysis","7f048b50":"#### Number of characters in texts for each category","d976dbaa":"#### Training Model","9977215c":"### Model 6 - BERT","b96e5fea":"**Function for Displaying the Confusion Matrix**\n\nThis function displays the confusion matrix of the model","6727938b":"### Model Building","0a6a9234":"### Evaluation of Results","2bc6faa6":"#### Remove single letter characters","9c798644":"#### Configuration for TPUs","69d66276":"#### Executive Movement","d1038618":"#### Removing URL's","3fecfded":"As we can see, that there are a lot of samples that have a description length of 0, however almost all articles have a snippet and every articles have title. Going with the intuition that the title is often more descriptive of the category of the news, as well as to provide more text data to the model:\n- Add news snippet to the missing news description \n- Add title to news description","dd844cc3":"# Training of Model\n\n### Model 1- Multinomial Naive Bayes","44b41aa0":"#### Category wise count of news","3b85499e":"## Feature Extraction\n \n Features are extracted from the dataset and TF-IDF(Term Frequency - Inverse Document Frequency) is used for this purpose.","2b50b84c":"#### Replace new lines","09c21dfc":"#### Trigram Analysis","71f55fe9":"**Function for evaluation of model**\n\nThis function finds the F1-score and Accuracy of the trained model","6fd29725":"### Model 4 - Stochastic Gradient Descent Classifier","b0bc3754":"### Evaluation of Model","468bc173":"### Model 5 - Light Gradient Boosting Classifier","84fc7589":"We can observe, BERT classifier gives the highest accuracy on test data.","8dd88c84":"### Evaluation of Results","b63f1922":"### Data Pre-Processing","d52d8fc2":"### Evaluation of Model","a0d96491":"#### Centre Setup and Expansion","97c14c6e":"### Importing Necessary Libraries","5557b46e":"#### Evaluation","3abbf1fe":"The data is preprocessed, in NLP it is also known as text normalization. Some of the most common methods of text normalization are:\n- Tokenization\n- Lemmatization\n- Stemming","b1685f0b":"#### Mergers and Acquisitions","24a2c058":"#### Number of words in texts for each category","13dd1813":"### Model 2 - Decision Tree Classifier","57b658c8":"### Evaluation of Results","13bfee06":"### Wordcloud For Different Categories","47deb39e":"#### Train-Test Split"}}