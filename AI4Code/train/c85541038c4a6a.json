{"cell_type":{"24afc1ea":"code","430e3548":"code","7474c99e":"code","5b6d836f":"code","f5a53cd9":"code","343efcc3":"code","dca0eb04":"code","6d9bcf8c":"code","e4060b26":"code","30d75314":"code","18e7cb41":"code","ac1bfca6":"code","f722d92e":"code","b7f4aaf9":"code","37932994":"code","8e8bdf4c":"code","d919e99c":"code","7ceb2910":"code","4d6f6f3e":"code","114dabf2":"code","8cca8b4e":"code","9ed45e7e":"code","5a184fa8":"code","53e184bd":"code","cd27af8c":"code","7404197e":"code","f71768cb":"code","d28cbccd":"code","3c01059e":"code","97eaf27e":"markdown","24535fa6":"markdown","a475ccaf":"markdown","afc3e008":"markdown","2b1e4f7f":"markdown","719698d7":"markdown","eab1f44b":"markdown","383fd096":"markdown","c29f48d1":"markdown","44a76560":"markdown","a8526ce0":"markdown","ee879bba":"markdown","2d267f6a":"markdown","f645aa49":"markdown","915a2db8":"markdown","93a6123b":"markdown","4adb4cb0":"markdown","cad6e550":"markdown","d16fd74d":"markdown","42c11f86":"markdown"},"source":{"24afc1ea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\nfrom matplotlib import pyplot as plt # graph plots\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.impute import KNNImputer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","430e3548":"data_train = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-1\/train.csv\")\ndata_test = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-1\/test.csv\")","7474c99e":"data_train_copy = data_train.copy()\ndata_train.head()","5b6d836f":"data_train.info()","f5a53cd9":"data_train.describe()","343efcc3":"data_train.corr()","dca0eb04":"data_test = data_test[data_test[\"HHS Region\"] == \"United States\"]\ndata_test = data_test.drop(\"HHS Region\", axis=1)\n\ndata_test = data_test[data_test[\"Group\"] == \"By Week\"]\ndata_test = data_test.drop(\"Group\", axis=1)\n\ntest_id_vals = data_test[\"id\"]\ndata_test = data_test.drop(\"id\", axis=1)\ndata_test = data_test.drop(\"Month\", axis=1)\ndata_test = data_test.drop(\"Data As Of\", axis=1)\ndata_test = data_test.drop(\"Week-Ending Date\", axis=1)\ndata_test = data_test.drop(\"MMWR Week\", axis=1)\ndata_test = data_test.drop(\"Year\", axis=1)","6d9bcf8c":"data_train = data_train[data_train[\"HHS Region\"] == \"United States\"]\ndata_train = data_train.drop(\"HHS Region\", axis=1)\n\ndata_train = data_train[data_train[\"Group\"] == \"By Week\"]\ndata_train = data_train.drop(\"Group\", axis=1)\n\ndata_train = data_train.drop(\"id\", axis=1)\ndata_train = data_train.drop(\"Footnote\", axis=1)\ndata_train = data_train.drop(\"Month\", axis=1)\ndata_train = data_train.drop(\"Data As Of\", axis=1)\ndata_train = data_train.drop(\"Week-Ending Date\", axis=1)\ndata_train = data_train.drop(\"MMWR Week\", axis=1)\ndata_train = data_train.drop(\"Year\", axis=1)\n\ndata_train","e4060b26":"data_test = pd.get_dummies(data_test, drop_first=True, columns=[\"Race and Hispanic Origin Group\", \"Age Group\"])","30d75314":"#create dummies for the categorical columns (Race and Age)\ndata_train = pd.get_dummies(data_train, drop_first=True, columns=[\"Race and Hispanic Origin Group\", \"Age Group\"])","18e7cb41":"data_train.info()","ac1bfca6":"data_test[\"Start Date\"] = pd.to_datetime(data_test[\"Start Date\"])\ndata_test[\"End Date\"] = pd.to_datetime(data_test[\"End Date\"])\ndata_test[\"Duration\"] = (data_test[\"Start Date\"] - data_test[\"End Date\"]).apply(lambda ele: np.abs(ele.days))","f722d92e":"data_train[\"Start Date\"] = pd.to_datetime(data_train[\"Start Date\"])\ndata_train[\"End Date\"] = pd.to_datetime(data_train[\"End Date\"])\ndata_train[\"Duration\"] = (data_train[\"Start Date\"] - data_train[\"End Date\"]).apply(lambda ele: np.abs(ele.days))","b7f4aaf9":"fig, ax = plt.subplots()\nax.scatter(data_train[\"Start Date\"], data_train[\"COVID-19 Deaths\"])\nax.axhline(data_train[\"COVID-19 Deaths\"].mean() + data_train[\"COVID-19 Deaths\"].std()*3, linestyle='--', c=\"red\")\nplt.show()","37932994":"data_test[\"Year\"] = pd.DatetimeIndex(data_test[\"Start Date\"]).year\ndata_test[\"Month\"] = pd.DatetimeIndex(data_test[\"Start Date\"]).month\ndata_test[\"Quarter\"] = pd.DatetimeIndex(data_test[\"Start Date\"]).quarter\n\ndata_test = pd.get_dummies(data_test, drop_first=True, columns=[\"Year\", \"Month\", \"Quarter\"])\n\ndata_test = data_test.drop(\"Start Date\", axis=1)\ndata_test = data_test.drop(\"End Date\", axis=1)","8e8bdf4c":"data_train[\"Year\"] = pd.DatetimeIndex(data_train[\"Start Date\"]).year\ndata_train[\"Month\"] = pd.DatetimeIndex(data_train[\"Start Date\"]).month\ndata_train[\"Quarter\"] = pd.DatetimeIndex(data_train[\"Start Date\"]).quarter\n\ndata_train = pd.get_dummies(data_train, drop_first=True, columns=[\"Year\", \"Month\", \"Quarter\"])\n\ndata_train = data_train.drop(\"Start Date\", axis=1)\ndata_train = data_train.drop(\"End Date\", axis=1)","d919e99c":"for column in data_test.columns:\n    if column not in data_train.columns:\n        data_train[column] = pd.Series([0] * len(data_train.index))","7ceb2910":"for column in data_train.columns:\n    if column not in data_test.columns and column != \"COVID-19 Deaths\":\n        data_test[column] = pd.Series([0] * len(data_test.index))","4d6f6f3e":"data_train.info()","114dabf2":"data_test.info()","8cca8b4e":"data_train.isna().sum()","9ed45e7e":"# get X and y\nX = data_train.drop([\"COVID-19 Deaths\"], axis=1)\ny = data_train[\"COVID-19 Deaths\"]\n\ntimesplit = TimeSeriesSplit(n_splits = 50, max_train_size=int(len(data_train.index)*0.8))\n\n# build models\nlr = Pipeline(steps=[\n                    ('scale', StandardScaler()),\n                    ('knnimp', KNNImputer(n_neighbors=2, weights=\"uniform\")),\n                    ('lr',LinearRegression())])\n\n\nridge = Pipeline(steps=[\n                    ('scale', StandardScaler()),\n                    ('knnimp', KNNImputer(n_neighbors=2, weights=\"uniform\")),\n                    ('ridge',Ridge())])\n\n\nlasso = Pipeline(steps=[\n                    ('scale', StandardScaler()),\n                    ('knnimp', KNNImputer(n_neighbors=2, weights=\"uniform\")),\n                    ('lasso',Lasso())])\n\n\nnet = Pipeline(steps=[\n                    ('scale', StandardScaler()),\n                    ('knnimp', KNNImputer(n_neighbors=2, weights=\"uniform\")),\n                    ('net',ElasticNet())])\n\nforest = Pipeline(steps=[\n                    ('scale', StandardScaler()),\n                    ('knnimp', KNNImputer(n_neighbors=2, weights=\"uniform\")),\n                    ('forest',RandomForestRegressor(n_estimators = 50, max_depth = 10, random_state=688))])\n\nada = Pipeline(steps=[\n                    ('scale', StandardScaler()),\n                    ('knnimp', KNNImputer(n_neighbors=2, weights=\"uniform\")),\n                    ('ada', AdaBoostRegressor(n_estimators = 100))])\n\ngrad = Pipeline(steps=[\n                    ('scale', StandardScaler()),\n                    ('knnimp', KNNImputer(n_neighbors=2, weights=\"uniform\")),\n                    ('grad', GradientBoostingRegressor(n_estimators = 100))])\n\n#params = {\"net__alpha\": [0.1, 0.01, 0.001], \"net__max_iter\": [1000, 2000]}\n#grid = RandomizedSearchCV(net, param_distributions = params, n_iter=6, scoring = \"neg_root_mean_squared_error\", cv=2)\n","5a184fa8":"\"\"\"\"\n#mostly a test to how much random seeds impact score\n#used to look at a larger distribution across random seeds\n\nmean_rmse = 1000\nstate = 0\nwhile mean_rmse > 110:\n    forest_rmse = []\n    state = random.randrange(1000)\n    \n    timesplit = TimeSeriesSplit(n_splits = 50, max_train_size=int(len(data_train.index)*0.8))\n\n    forest = Pipeline(steps=[\n                    ('scale', StandardScaler()),\n                    ('knnimp', KNNImputer(n_neighbors=2, weights=\"uniform\")),\n                    ('forest',RandomForestRegressor(n_estimators = 50, max_depth = 10, random_state=state))])\n\n    for train_index, test_index in timesplit.split(X):\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        forest.fit(X_train, y_train)\n        forest_pred = forest.predict(X_test)\n        forest_rmse.append(np.sqrt(mean_squared_error(y_test, forest_pred)))\n        \n    mean_rmse = np.mean(forest_rmse)\n    print(f\"{np.mean(forest_rmse)}, {state}\")\n\"\"\"","53e184bd":"lr_rmse = []\nridge_rmse = []\nlasso_rmse = []\nnet_rmse = []\nforest_rmse = []\nada_rmse = []\ngrad_rmse = []\n\nfor train_index, test_index in timesplit.split(X):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    \n    lr.fit(X_train, y_train)\n    lr_pred = lr.predict(X_test)\n    lr_rmse.append(np.sqrt(mean_squared_error(y_test, lr_pred)))\n\n    ridge.fit(X_train, y_train)\n    ridge_pred = ridge.predict(X_test)\n    ridge_rmse.append(np.sqrt(mean_squared_error(y_test, ridge_pred)))\n\n    lasso.fit(X_train, y_train)\n    lasso_pred = lasso.predict(X_test)\n    lasso_rmse.append(np.sqrt(mean_squared_error(y_test, lasso_pred)))\n\n    net.fit(X_train, y_train)\n    net_pred = net.predict(X_test)\n    net_rmse.append(np.sqrt(mean_squared_error(y_test, net_pred)))\n    \n    forest.fit(X_train, y_train)\n    forest_pred = forest.predict(X_test)\n    forest_rmse.append(np.sqrt(mean_squared_error(y_test, forest_pred)))\n    \n    ada.fit(X_train, y_train)\n    ada_pred = ada.predict(X_test)\n    ada_rmse.append(np.sqrt(mean_squared_error(y_test, ada_pred)))\n    \n    grad.fit(X_train, y_train)\n    grad_pred = grad.predict(X_test)\n    grad_rmse.append(np.sqrt(mean_squared_error(y_test, grad_pred)))\n    \n    #grid.fit(X_train, y_train)\n    #grid_pred = grid.predict(X_test)\n    #grid_rmse.append(np.sqrt(mean_squared_error(y_test, net_pred)))\n    \n\nprint(f\"Linear Min and Max:\\t{np.min(lr_rmse)}, {np.max(lr_rmse)}\\n\")\nprint(f\"Linear Mean and STD:\\t{np.mean(lr_rmse)}, {np.std(lr_rmse)}\\n\")\n\nprint(f\"Ridge Min and Max:\\t{np.min(ridge_rmse)}, {np.max(ridge_rmse)}\\n\")\nprint(f\"Ridge Mean and STD:\\t{np.mean(ridge_rmse)}, {np.std(ridge_rmse)}\\n\")\n\nprint(f\"Lasso Min and Max:\\t{np.min(lasso_rmse)}, {np.max(lasso_rmse)}\\n\")\nprint(f\"Lasso Mean and STD:\\t{np.mean(lasso_rmse)}, {np.std(lasso_rmse)}\\n\")\n\nprint(f\"Net Min and Max:\\t{np.min(net_rmse)}, {np.max(net_rmse)}\\n\")\nprint(f\"Net Mean and STD:\\t{np.mean(net_rmse)}, {np.std(net_rmse)}\\n\")\n\nprint(f\"Forest Min and Max:\\t{np.min(forest_rmse)}, {np.max(forest_rmse)}\\n\")\nprint(f\"Forest Mean and STD:\\t{np.mean(forest_rmse)}, {np.std(forest_rmse)}\\n\")\n\nprint(f\"AdaBoost Min and Max:\\t{np.min(ada_rmse)}, {np.max(ada_rmse)}\\n\")\nprint(f\"AdaBoost Mean and STD:\\t{np.mean(ada_rmse)}, {np.std(ada_rmse)}\\n\")\n\nprint(f\"Gradient Min and Max:\\t{np.min(grad_rmse)}, {np.max(grad_rmse)}\\n\")\nprint(f\"Gradient Mean and STD:\\t{np.mean(grad_rmse)}, {np.std(grad_rmse)}\\n\")\n\n#print(f\"Grid:\\t{grid_rmse}\\n\\n\")","cd27af8c":"fig, ax = plt.subplots()\nax.scatter(range(len(net_rmse)), net_rmse)\nax.scatter(range(len(forest_rmse)), forest_rmse, c='g')\nax.scatter(range(len(grad_rmse)), grad_rmse, c='r')\nplt.show()","7404197e":"fig, ax = plt.subplots()\nax.boxplot([net_rmse, forest_rmse, grad_rmse])\nplt.show()","f71768cb":"grad.fit(X,y)","d28cbccd":"print(X.shape)\nprint(data_test.shape)","3c01059e":"#predict on the test set with the best model\ny_submission_predict = grad.predict(data_test)\n\n#submit data\nsubmission_data = pd.DataFrame({\"id\": test_id_vals, \"COVID-19 Deaths\": y_submission_predict})\nsubmission_data.to_csv(\"submission.csv\", index=False)","97eaf27e":"#### Make sure the training data and test data have the same dummy columns. If they don't then create zeroed out columns for them\n#### This is necessary for predicting on the test set","24535fa6":"#### Did not find missing values in either the test or training set (added a KNNImputer to the models just in case anyway)","a475ccaf":"### Generating Submission Data","afc3e008":"#### Basic exploration of the data","2b1e4f7f":"### Building Models","719698d7":"#### Isolate the target column, COVID-19 Deaths, from the training set\n#### Create a TimeSeriesSplit for later (since we're dealing with timeseries data)\n#### Create our models: Linear Regression, Ridge Regression, Lasso Regression and an Elastic Net\n#### Use transformations with these models via pipelines (A standard scaler to scale features and a KNNImputer even though we have no missing values)","eab1f44b":"#### Get dummies for our original categorical columns (Race and Age)","383fd096":"# US Covid-19 Deaths\n### By Myles Caesar","c29f48d1":"#### Plot the RMSE for a few regressors\n##### The scatter plot resembles the COVID-19 Deaths over time plot","44a76560":"##### Tried a RandomizedSearch CV with the Elastic Net estimator but it did not work out well (part of this is because it has automatic CV)\n##### Tried other transformers like the Polynomial transformer but they did not work well (model would not converge)\n##### Looked into ARIMA and Exponential Smoothing but the concepts and python implementation went over my head somewhat\n##### Looked through my notes for other Regression models after getting poor RMSE fiddling with the other models and tried a few other models I found","a8526ce0":"#### Check the shape of the test set to make sure we can predict on it","ee879bba":"### Scoring Models","2d267f6a":"#### Create the features Year, Month and Quarter based on the Start Time. Get dummies for these categorical features\n#### Now that we have these new features we do not need Start Date and End Date anymore","f645aa49":"### Imports\/Loading Data","915a2db8":"#### Try each set of timeseries splits, fit on the training values for the split and predict on the test values\n#### Add the RMSE of each model each split to a list. Display the Minimum, Maximum, Mean and Standard Deviation for these lists","93a6123b":"#### Decided to keep the outliers. It would be difficult to identify them since trends depend a lot on the categorical variables.\n##### Outliers in terms of the whole data set might not be an outlier from the perspective of the observation's race, age or other categorical feature\n##### Fitting a model for each race and age grouping would be ideal but very expensive","4adb4cb0":"#### Turn Start Date and End Date into datetime columns and extract a new feature, Duration, from them (sounds like a good feature)\n#### Since Start Date is now a datetime column I can plot COVID-19 deaths over time","cad6e550":"#### Fit the best estimator to the entire train data set (better results most likely)","d16fd74d":"#### Only get the data entries that correspond to the United States and are grouped by week. The other data entries for these columns are redundant.\n#### Drop the columns for Month, Data As Of, Week-Ending Date, MMWR Week and Year. These columns are either redundant or will be replaced later\n#### We store id seperate from the test data set so we can match it later with the predictions. We do not need it for fiting though","42c11f86":"### EDA"}}