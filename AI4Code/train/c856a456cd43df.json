{"cell_type":{"3fa46215":"code","3903bf69":"code","e02f570f":"markdown","0fb6fdf6":"markdown"},"source":{"3fa46215":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n    None\n\n# Any results you write to the current directory are saved as output.","3903bf69":"def _sigmoid(x):\n    \n    \n    y = torch.clamp(x.sigmoid_(), min=1e-4, max=1-1e-4)\n    return y\n\nclass focal_loss(nn.Module):\n  def __init__(self, gamma=2.0):\n        super().__init__()\n  def forward(self,pred, gt):\n    ''' Modified focal loss. Exactly the same as CornerNet.\n        Runs faster and costs a little bit more memory\n      Arguments:\n        pred (batch x c x h x w)\n        gt_regr (batch x c x h x w)\n    '''\n    pred=_sigmoid(pred)\n    pos_inds = gt.eq(1).float()\n    pos_inds=pos_inds.unsqueeze(1)\n    #print(pos_inds.size())\n    neg_inds = gt.lt(1).float().unsqueeze(1)\n\n    neg_weights = torch.pow(1 - gt, 4).unsqueeze(1)\n\n    loss = 0\n    #print(neg_weights)\n    pos_loss = torch.log(pred+1e-7) * torch.pow(1 - pred, 2) * pos_inds\n    neg_loss = torch.log(1 - pred+1e-7) * torch.pow(pred, 2) * neg_weights * neg_inds\n\n    \n    #.float().sum()\n    pos_loss = pos_loss.view(pred.size(0),-1).sum(-1)\n    neg_loss = neg_loss.view(gt.size(0),-1).sum(-1)\n    #neg_loss.sum(-1)\n    num_pos  = pos_inds.sum()\n    if num_pos == 0:\n      loss = loss - neg_loss\n    else:\n      loss = loss - (pos_loss + neg_loss) #\/ num_pos\n    num_pos  = pos_inds.view(gt.size(0),-1).sum(-1)\n    #print('loss',loss.size(),pos_loss.size(),loss.size(),'loss_sum',loss.sum(-1).mean(0),num_pos.size())\n    return loss.mean(0)\n","e02f570f":"**Here is the some route ahead after you are stuck between 0.04 and 0.05. I am not going to publish the kernel as it would be against the ethics of this competition at this stage.**\n* 1. I used Hicops Kernel only ,same Dataset ,image scaling ,did dataset cleaning using approach in point 7 \n\n* 2. I used below version of Focal Loss\n* 3. I used effnetb2 in hicops model , You have to   replace 1280+1024 with 1408+1024\n* 4. I used higher image size 512,2048\n* 5. I used One Cycle LR scheduler max lr =1e-3 ,div_factor=8  and monitored mAP for saving best weights.This will help you prevernt overfitting.\n* 6. Stop the Training if you see Mask loss is rising more.\n* 7. I modified the Drop out rate for effnet as per https:\/\/www.kaggle.com\/isakev\/rb-s-centernet-baseline-pytorch-without-dropout","0fb6fdf6":"**Dont Forget to Upvote if you get benefitted by the approach**\nI will keep updating if i find more approaches."}}