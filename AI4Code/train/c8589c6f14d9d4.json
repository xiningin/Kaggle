{"cell_type":{"76752881":"code","bc50f97f":"code","9bb59166":"code","61c9e66d":"code","7d445e05":"code","28211637":"code","99b93145":"code","ff4dc94f":"code","9b2fed7a":"code","6b44774b":"code","aae77827":"code","2f27cde8":"code","cdd0acdc":"code","42aad4f9":"code","cb6a6147":"code","eb6d0b00":"code","e84192e0":"markdown","fe2939d6":"markdown","5e6d892b":"markdown","d87bb311":"markdown","22938c1e":"markdown","e92a3372":"markdown","6e6ccc5c":"markdown","7700d742":"markdown","a9c0cea6":"markdown","e75c95ee":"markdown"},"source":{"76752881":"%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd    \nimport matplotlib.pyplot as plt\n!pip install lifelines\nfrom lifelines import NelsonAalenFitter, CoxPHFitter, KaplanMeierFitter\nfrom lifelines.statistics import logrank_test\n\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\ncredit = pd.read_csv('..\/input\/credit-card-approval-prediction\/credit_record.csv')  \napplication = pd.read_csv('..\/input\/credit-card-approval-prediction\/application_record.csv') \ncredit","bc50f97f":"application","9bb59166":"len(set(application['ID'])) # how many unique ID in application record?","61c9e66d":"len(set(credit['ID'])) # how many unique ID in credit record?","7d445e05":"len(set(application['ID']).intersection(set(credit['ID']))) # how many IDs do two tables share?","28211637":"grouped = credit.groupby('ID')\n### convert credit data to wide format which every ID is a row\npivot_tb = credit.pivot(index = 'ID', columns = 'MONTHS_BALANCE', values = 'STATUS')\npivot_tb['open_month'] = grouped['MONTHS_BALANCE'].min() # smallest value of MONTHS_BALANCE, is the month when loan was granted\npivot_tb['end_month'] = grouped['MONTHS_BALANCE'].max() # biggest value of MONTHS_BALANCE, might be observe over or canceling account\npivot_tb['ID'] = pivot_tb.index\npivot_tb = pivot_tb[['ID', 'open_month', 'end_month']]\npivot_tb['window'] = pivot_tb['end_month'] - pivot_tb['open_month'] # calculate observe window\npivot_tb.reset_index(drop = True, inplace = True)\ncredit = pd.merge(credit, pivot_tb, on = 'ID', how = 'left') # join calculated information\ncredit0 = credit.copy()\ncredit = credit[credit['window'] > 20] # delete users whose observe window less than 20\ncredit['status'] = np.where((credit['STATUS'] == '2') | (credit['STATUS'] == '3' )| (credit['STATUS'] == '4' )| (credit['STATUS'] == '5'), 1, 0) # analyze > 60 days past due \ncredit['status'] = credit['status'].astype(np.int8) # 1: overdue 0: not\ncredit['month_on_book'] = credit['MONTHS_BALANCE'] - credit['open_month'] # calculate month on book: how many months after opening account\ncredit.sort_values(by = ['ID','month_on_book'], inplace = True)\n\n##### denominator\ndenominator = pivot_tb.groupby(['open_month']).agg({'ID': ['count']}) # count how many users in every month the account was opened\ndenominator.reset_index(inplace = True)\ndenominator.columns = ['open_month','sta_sum']\n\n##### ventage table\nvintage = credit.groupby(['open_month','month_on_book']).agg({'ID': ['count']}) \nvintage.reset_index(inplace = True)\nvintage.columns = ['open_month','month_on_book','sta_sum'] \nvintage['due_count'] = np.nan\nvintage = vintage[['open_month','month_on_book','due_count']] # delete aggerate column\nvintage = pd.merge(vintage, denominator, on = ['open_month'], how = 'left') # join sta_sum colun to vintage table\nvintage","99b93145":"for j in range(-60,1): # outer loop: month in which account was opened\n    ls = []\n    for i in range(0,61): # inner loop time after the credit card was granted\n        due = list(credit[(credit['status'] == 1) & (credit['month_on_book'] == i) & (credit['open_month'] == j)]['ID']) # get ID which satisfy the condition\n        ls.extend(due) # As time goes, add bad customers\n        vintage.loc[(vintage['month_on_book'] == i) & (vintage['open_month'] == j), 'due_count'] = len(set(ls)) # calculate non-duplicate ID numbers using set()\n        \nvintage['sta_rate']  = vintage['due_count'] \/ vintage['sta_sum'] # calculate cumulative % of bad customers\nvintage        ","ff4dc94f":"### Vintage wide table\nvintage_wide = vintage.pivot(index = 'open_month',\n                             columns = 'month_on_book',\n                             values = 'sta_rate')\nvintage_wide","9b2fed7a":"# plot vintage line chart\nplt.rcParams['figure.facecolor'] = 'white'\nvintage0 = vintage_wide.replace(0,np.nan)\nlst = [i for i in range(0,61)]\nvintage_wide[lst].T.plot(legend = False, grid = True, title = 'Cumulative % of Bad Customers (> 60 Days Past Due)')\n#plt.axvline(30)\n#plt.axvline(25)\n#plt.axvline(20)\nplt.xlabel('Months on Books')\nplt.ylabel('Cumulative % > 60 Days Past Due')\nplt.show()","6b44774b":"lst = []\nfor i in range(0,61):\n    ratio = len(pivot_tb[pivot_tb['window'] < i]) \/ len(set(pivot_tb['ID']))\n    lst.append(ratio)\n    \npd.Series(lst).plot(legend = False, grid = True, title = ' ')\nplt.xlabel('Observe Window')\nplt.ylabel('account ratio')\nplt.show()","aae77827":"def calculate_observe(credit, command):\n    '''calculate observe window\n    '''\n    id_sum = len(set(pivot_tb['ID']))\n    credit['status'] = 0\n    exec(command)\n    #credit.loc[(credit['STATUS'] == '4' )| (credit['STATUS'] == '5'), 'status'] = 1\n    credit['month_on_book'] = credit['MONTHS_BALANCE'] - credit['open_month']\n    minagg = credit[credit['status'] == 1].groupby('ID')['month_on_book'].min()\n    minagg = pd.DataFrame(minagg)\n    minagg['ID'] = minagg.index\n    obslst = pd.DataFrame({'month_on_book':range(0,61), 'rate': None})\n    lst = []\n    for i in range(0,61):\n        due = list(minagg[minagg['month_on_book']  == i]['ID'])\n        lst.extend(due)\n        obslst.loc[obslst['month_on_book'] == i, 'rate'] = len(set(lst)) \/ id_sum \n    return obslst['rate']\n\ncommand = \"credit.loc[(credit['STATUS'] == '0') | (credit['STATUS'] == '1') | (credit['STATUS'] == '2') | (credit['STATUS'] == '3' )| (credit['STATUS'] == '4' )| (credit['STATUS'] == '5'), 'status'] = 1\"   \nmorethan1 = calculate_observe(credit, command)\ncommand = \"credit.loc[(credit['STATUS'] == '1') | (credit['STATUS'] == '2') | (credit['STATUS'] == '3' )| (credit['STATUS'] == '4' )| (credit['STATUS'] == '5'), 'status'] = 1\"   \nmorethan30 = calculate_observe(credit, command)\ncommand = \"credit.loc[(credit['STATUS'] == '2') | (credit['STATUS'] == '3' )| (credit['STATUS'] == '4' )| (credit['STATUS'] == '5'), 'status'] = 1\"\nmorethan60 = calculate_observe(credit, command)\ncommand = \"credit.loc[(credit['STATUS'] == '3' )| (credit['STATUS'] == '4' )| (credit['STATUS'] == '5'), 'status'] = 1\"\nmorethan90 = calculate_observe(credit, command)\ncommand = \"credit.loc[(credit['STATUS'] == '4' )| (credit['STATUS'] == '5'), 'status'] = 1\"\nmorethan120 = calculate_observe(credit, command)\ncommand = \"credit.loc[(credit['STATUS'] == '5'), 'status'] = 1\"\nmorethan150 = calculate_observe(credit, command)","2f27cde8":"obslst = pd.DataFrame({'past due more than 30 days': morethan30,\n                       'past due more than 60 days': morethan60,\n                       'past due more than 90 days': morethan90,\n                       'past due more than 120 days': morethan120,\n                       'past due more than 150 days': morethan150\n                        })\n\nobslst.plot(grid = True, title = 'Cumulative % of Bad Customers Analysis')\nplt.xlabel('Months on Books')\nplt.ylabel('Cumulative %')\nplt.show()","cdd0acdc":"def calculate_rate(pivot_tb, command): \n    '''calculate bad customer rate\n    '''\n    credit0['status'] = None\n    exec(command) # excuate input code\n    sumagg = credit0.groupby('ID')['status'].agg(sum)\n    pivot_tb = pd.merge(pivot_tb, sumagg, on = 'ID', how = 'left')\n    pivot_tb.loc[pivot_tb['status'] > 1, 'status'] = 1\n    rate = pivot_tb['status'].sum() \/ len(pivot_tb)\n    return round(rate, 5)\n\ncommand = \"credit0.loc[(credit0['STATUS'] == '0') | (credit0['STATUS'] == '1') | (credit0['STATUS'] == '2') | (credit0['STATUS'] == '3' )| (credit0['STATUS'] == '4' )| (credit0['STATUS'] == '5'), 'status'] = 1\"   \nmorethan1 = calculate_rate(pivot_tb, command)\ncommand = \"credit0.loc[(credit0['STATUS'] == '1') | (credit0['STATUS'] == '2') | (credit0['STATUS'] == '3' )| (credit0['STATUS'] == '4' )| (credit0['STATUS'] == '5'), 'status'] = 1\"   \nmorethan30 = calculate_rate(pivot_tb, command)\ncommand = \"credit0.loc[(credit0['STATUS'] == '2') | (credit0['STATUS'] == '3' )| (credit0['STATUS'] == '4' )| (credit0['STATUS'] == '5'), 'status'] = 1\"\nmorethan60 = calculate_rate(pivot_tb, command)\ncommand = \"credit0.loc[(credit0['STATUS'] == '3' )| (credit0['STATUS'] == '4' )| (credit0['STATUS'] == '5'), 'status'] = 1\"\nmorethan90 = calculate_rate(pivot_tb, command)\ncommand = \"credit0.loc[(credit0['STATUS'] == '4' )| (credit0['STATUS'] == '5'), 'status'] = 1\"\nmorethan120 = calculate_rate(pivot_tb, command)\ncommand = \"credit0.loc[(credit0['STATUS'] == '5'), 'status'] = 1\"\nmorethan150 = calculate_rate(pivot_tb, command)\n\nsummary_dt = pd.DataFrame({'situation':['past due more than 1 day',\n                               'past due more than 30 days',\n                               'past due more than 60 days',\n                               'past due more than 90 days',\n                               'past due more than 120 days',\n                               'past due more than 150 days'],\n                      'bad customer ratio':[morethan1,\n                               morethan30,\n                               morethan60,\n                               morethan90, \n                               morethan120,\n                               morethan150, \n                      ]})\nsummary_dt","42aad4f9":"credit['truncate'] = credit.groupby('ID')['status'].transform('max')\ntruncate = credit.loc[credit['truncate']==1,]\nnot_trunc = credit.loc[credit['truncate']==0,]\n\ndt1 = not_trunc.groupby('ID').agg({'month_on_book': ['max'],\n                            'truncate':['max']\n                           })\ndt1.reset_index(inplace = True)\ndt1.columns = ['ID','month_on_book','truncate']\ndt2 = truncate.loc[truncate['status']==1,].groupby('ID').agg({'month_on_book': ['min'],\n                            'truncate':['max']\n                           })\ndt2.reset_index(inplace = True)\ndt2.columns = ['ID','month_on_book','truncate'] \ndt = dt1.append(dt2)\n\nprint(dt.head())\n\nkmf = KaplanMeierFitter()\nkmf.fit(dt['month_on_book'], event_observed=dt['truncate'], label='overdue')\nkmf.plot()","cb6a6147":"application = application[['ID', 'CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE']]\ndt = pd.merge(dt, application, on = 'ID', how = 'left')\ndt.fillna('unknown',inplace=True)\n\ndef plot_km(dt, timevar, truncate, groupby):\n    '''plot km by subgroup\n    '''\n    plt.rcParams['figure.facecolor'] = 'white'\n    kmf = KaplanMeierFitter()\n    t = np.linspace(0, 60, 61)\n    for i in dt[groupby].value_counts().index:\n        kmf.fit(dt[dt[groupby] == i][timevar], event_observed=dt[dt[groupby] == i][truncate], timeline=t, label=i)\n        ax = kmf.plot()\n\nplot_km(dt, 'month_on_book', 'truncate', 'CODE_GENDER')","eb6d0b00":"kmf = KaplanMeierFitter()\nt = np.linspace(0, 60, 61)\nkmf.fit(dt[dt['NAME_FAMILY_STATUS'] == 'Married']['month_on_book'], event_observed=dt[dt['NAME_FAMILY_STATUS'] == 'Married']['truncate'], timeline=t, label=\"Married\")\nax = kmf.plot()\n\nkmf.fit(dt[dt['NAME_FAMILY_STATUS'] == 'Single \/ not married']['month_on_book'], event_observed=dt[dt['NAME_FAMILY_STATUS'] == 'Single \/ not married']['truncate'], timeline=t, label=\"Single \/ not married\")\nax = kmf.plot()","e84192e0":"# Observe Window Analysis\n\nBecause of two reasons, account cancellation and observe over, our observe on accounts will be truncated. Observe window is a significant parameter to be considered. If observe window is too short, users' behavior will not fully show off, which will bring unnecessary noise to our data.\n\nIn order to observe how many accounts increase as observe window extend, we plot this. ","fe2939d6":"Using `pivot` to convert long data to wide data:","5e6d892b":"# Survival Analysis","d87bb311":"# Overall Past-due Ratio\n\nCalculating overall past-due rate. Respectively, we analyze 1 day past due, 20 days past due, 60 days past due, 90 days past due, 120 days past due, 150 days past due. This analysis could help us to define who are *bad customers*. We could see that almost 87% users have past due more than 1 day, which is too common, thus it's inappropriate to be a standard. What about 150 days overdue? Only 0.4% of accounts appear to past due that long. If we use that, we will left many *bad customers* in our scrutiny. A table like that will help you to determine what  will be the most suitable standard of *bad customers*.","22938c1e":"This is a standard vingtage analysis table. The rows represent months of opening accounts, columns represent months after openning acounts, and values are accumlate past-due rate. As open-month closes to 0, the observe window shortens, thus the bottom half triangular field is filled by nan.","e92a3372":"<style>\ncode, kbd, pre, samp {\n    \/*font-family:'consolas', Lucida Console, SimSun, Fira Code, Monaco !important;*\/\n    font-size: 11pt !important;\n}\n\ndiv.output_area pre {\n    font-family: 'consolas', Lucida Console, SimSun, Fira Code, Monaco !important;\n    font-size:  10pt !important;\n}\n\ndiv.output_area img, div.output_area svg {\n    background-color: #FFFFFF !important;\n}\n<\/style>\n\n<font size=5>EDA & Vintage Analysis<\/font>\n\n[Xiao Song](https:\/\/xsong.ltd\/en)\n\nFor chinese reader, hope you check my wechat official account article: [\u4fe1\u7528\u8bc4\u5206\u6a21\u578b\u4e2d\u7684Vintage\u5206\u6790](https:\/\/mp.weixin.qq.com\/s\/peb3XhXLCSTQYOa5e6u6TQ). \n\nVintage analysis is a widely-used method for managing credit risk, it illustrate the behavior after an account was opened. Based on same origination period, it calculates charge-off ratio of a loan portfolio. \n\n\n\n\n\n\nHere comes our datasets, in which `credit_record.csv` contains loan accounts' credit records, the detailed data explanation is here:\n\n|  credit_record.csv           | \u3000                        | \u3000                                                                                                                                                                                                                            |\n|:-----------------------:|---------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Feature name          | Explanation               | Remarks                                                                                                                                                                                                                       |\n| ID                    | Client number             | \u3000                                                                                                                                                                                                                            |\n| MONTHS_BALANCE        | Record month              | The month of the extracted data is the starting point,  backwards, 0 is the current month, -1 is the previous month,  and so on                                                                                               |\n| STATUS               | Status                    | 0: 1-29 days past due 1: 30-59 days past due   2: 60-89 days overdue 3: 90-119 days overdue    4: 120-149 days overdue 5: Overdue or bad debts, write-offs for more than 150 days    C: paid off that month X: No loan for the month  |\n\n\n\n-----\n\n\n\nwhile the `application_record.csv` contains appliers' features, such as gender, income. This notebook will show some necessary EDA on credit_record table.","6e6ccc5c":"# Vintage Analysis\n\nDetailed explanation could be seen here [here](https:\/\/www.listendata.com\/2019\/09\/credit-risk-vintage-analysis.html). ","7700d742":"We could see that a 60 months observe window covers all appliers, while 20 months window contains about 52% records.","a9c0cea6":"# Summary\n\nHere shows some EDA on credit_record table, including vintage analysis, bad customer definition and observe window analysis. For credit card application data analysis, the response variable sometimes not clear, thus we need to extract information from credit record. This notebook does not contain all possible methods for sure. There definitly has more methods, I expect more valuable analysis based on this data :)","e75c95ee":"This plot could be seen as a average (across open-month) version of vintage plot.\nFor longer past due date, it needs longer observe window. For example, more than 150 days past due needs at least 5 months until first *bad customer* appears. For most situation, a 20-months observe window could cover most *bad customer*. However, For 30 days past due, we could see that after 30 months on books, there still are new *bad customer* join in the list. So a 20 MOB observe window will be appropriate. Those who exists shorter than the observe window should be excluded from our analysis, thus you could see I deleted users whose observe window less than 20 on last section (the window could be changed)."}}