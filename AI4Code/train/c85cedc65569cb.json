{"cell_type":{"4f7ee26d":"code","41e9c2e0":"code","b64df601":"code","2f1fa35f":"code","6a5b1683":"code","f8da9ed6":"code","c9197c68":"code","e9cb9442":"code","2c071434":"code","8ebf62a2":"code","17e97fab":"code","da1f8575":"code","8544255d":"code","229c2120":"code","708e5017":"code","4222843f":"code","cdfe8381":"code","84168d58":"code","075674f8":"code","975e2cc6":"code","dbbb2caa":"code","6334acf6":"code","34cab8f1":"code","49b26f42":"code","af85e759":"code","50c6f499":"code","3ae67fe7":"code","197b0770":"code","794467d9":"code","bcf9579d":"code","a6281d77":"code","ddaf92e5":"code","6d1011c6":"code","fc909ea5":"code","fe4b180b":"code","4eaf0241":"code","9a3357f5":"code","ae9cd66f":"code","68f74f52":"code","4c3dddfa":"code","1bd20b3f":"code","81eed4ae":"code","105b0c81":"code","d9fbb2ea":"code","ea36d2e8":"code","d40386b6":"code","952a4ec0":"code","27a8fb45":"code","880634b6":"code","d072fd69":"code","8afdba4a":"code","28de2a41":"code","a7c4984b":"code","9120b05b":"code","579b5102":"code","7f978be1":"code","74c0d86a":"code","024aee09":"code","df884e17":"code","6c654bc1":"code","a05187dd":"code","c1a463d8":"code","99ae135e":"code","b2595318":"code","768203c4":"code","5a0a0581":"code","b85389b8":"code","2ec6117e":"code","24b5602b":"code","447c02ed":"code","0180af7f":"code","b2397c31":"code","2e52ba5f":"code","a7074417":"code","ba7efd27":"code","0363e640":"code","c4dae309":"code","53fc2a60":"code","e9878ebf":"code","25fd25a7":"code","afe5606e":"code","f883d438":"code","f6d62666":"code","039ff3e8":"code","a57a3eb5":"code","b6d2b042":"code","16119487":"code","22f0de1e":"code","025bc31d":"code","252dc870":"code","ecc3c460":"markdown","c585dc64":"markdown","b4fa65b9":"markdown","99252fde":"markdown","74a7d285":"markdown","615fba0a":"markdown","3126dfa6":"markdown","0b27ddfa":"markdown","f7ee5d08":"markdown","fe143be7":"markdown","9e22286e":"markdown","7b62c5ab":"markdown","c92c17e7":"markdown","595666e0":"markdown","e296cd7e":"markdown","83a63623":"markdown","be0349d1":"markdown","02f076b3":"markdown","23df9eca":"markdown","b6351691":"markdown","89c5694b":"markdown","22d285f7":"markdown","c8fe3640":"markdown","beadbd1a":"markdown","d7597cd2":"markdown","4f83e661":"markdown","84d36dcb":"markdown","bb86f776":"markdown","59ce64ef":"markdown","198090b2":"markdown","c7c7bf3a":"markdown","44aef050":"markdown","ba66f841":"markdown","56156d6b":"markdown","19129316":"markdown","626c4130":"markdown","736dfc23":"markdown","9bb9ddfa":"markdown","943df894":"markdown","b8a435c5":"markdown","cbeba627":"markdown","502c5bf2":"markdown","762c23c1":"markdown","df174995":"markdown","3613b28b":"markdown","c5d72392":"markdown","7f85a679":"markdown","60afa397":"markdown","04d8aa2b":"markdown","37953743":"markdown","6d195123":"markdown","effe45cf":"markdown","41ead0ce":"markdown","3073dff1":"markdown","fd96b465":"markdown","73b43259":"markdown","75467689":"markdown","09f23cb0":"markdown","9d3728c0":"markdown","25ac553e":"markdown","f28db2ec":"markdown","5889cdd5":"markdown"},"source":{"4f7ee26d":"import warnings\nwarnings.filterwarnings(\"ignore\")\nfrom nltk.corpus import stopwords\nimport re\nimport math\nfrom collections import Counter, defaultdict\nfrom scipy.sparse import hstack\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import normalize\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import StackingClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression","41e9c2e0":"df = pd.read_csv('\/kaggle\/input\/msk-redefining-cancer-treatment\/training_variants.zip')\nprint('Dataframe shape: ', df.shape)\nprint('Features names: ', df.columns.values)\ndf.head()","b64df601":"df_text =pd.read_csv(\"\/kaggle\/input\/msk-redefining-cancer-treatment\/training_text.zip\",sep=\"\\|\\|\",engine=\"python\",names=[\"ID\",\"TEXT\"],skiprows=1)\nprint('Text data shape: ', df_text.shape)\nprint('Features names : ', df_text.columns.values)\ndf_text.head()","2f1fa35f":"stop_words = set(stopwords.words('english'))\nfor i,text in enumerate(df_text[\"TEXT\"]):\n    if type(df_text[\"TEXT\"][i]) is not str:\n        print(\"no text description available at index : \", i)\n    else:\n        string = \"\"\n        df_text[\"TEXT\"][i] = str(df_text[\"TEXT\"][i]).lower()\n        df_text[\"TEXT\"][i] = re.sub(\"\\W\",\" \",df_text[\"TEXT\"][i])\n        df_text[\"TEXT\"][i] = re.sub('\\s+',' ', df_text[\"TEXT\"][i])\n        for word in df_text[\"TEXT\"][i].split():\n            if not word in stop_words:\n                string += word + \" \"\n        df_text[\"TEXT\"][i] = string","6a5b1683":"df[\"Gene\"] = df[\"Gene\"].str.replace('\\s+', '_')\ndf[\"Variation\"] = df[\"Variation\"].str.replace('\\s+', '_')","f8da9ed6":"final_df = pd.merge(df, df_text,on='ID', how='left')\nfinal_df.head()","c9197c68":"final_df[final_df.isna().any(axis=1)]","e9cb9442":"#imputing the missing values\nfinal_df.loc[final_df['TEXT'].isna(),'TEXT'] = final_df['Gene'] +' '+final_df['Variation']","2c071434":"y_label = final_df[\"Class\"].values\nX_train, X_test, y_train, y_test = train_test_split(final_df, y_label, stratify=y_label, test_size=0.2)\nX_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2)","8ebf62a2":"print('Training data size :', X_train.shape)\nprint('test data size :', X_test.shape)\nprint('Validation data size :', X_cv.shape)","17e97fab":"def dist_class(df, name):\n    sns.countplot(df[\"Class\"])\n    plt.title(\"Bar plot of Class using {} data\".format(name))\n    print(\"Frequency of each class in {} data\".format(name))\n    for i in df[\"Class\"].value_counts().index:\n        print(\"Number of observations in class \", i,\" is : \",df[\"Class\"].value_counts()[i], \"(\", np.round((df[\"Class\"].value_counts()[i] \/ len(df[\"Class\"]))*100,2), \"%)\")","da1f8575":"dist_class(X_train,\"training\")","8544255d":"dist_class(X_cv,\"validation\")","229c2120":"dist_class(X_test,\"test\")","708e5017":"#user defined function to plot confusion matrix, precision and recall for a ML model\ndef plot_confusion_recall_precision(cm):\n    labels = [1,2,3,4,5,6,7,8,9]\n    print(\"=\"*30, \"Confusion matrix\", \"=\"*30)\n    plt.figure(figsize=(16,8))\n    sns.heatmap(cm, annot=True, cmap=plt.cm.Blues, xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n    \n    precision_matrix =(cm\/cm.sum(axis=0))\n    print(\"=\"*30, \"Precision matrix (columm sum=1)\", \"=\"*30)\n    plt.figure(figsize=(16,8))\n    sns.heatmap(precision_matrix, annot=True, cmap=plt.cm.Blues, xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n    \n    recall_matrix =(((cm.T)\/(cm.sum(axis=1))).T)\n    print(\"=\"*30, \"Recall matrix (row sum=1)\", \"=\"*30)\n    plt.figure(figsize=(16,8))\n    sns.heatmap(recall_matrix, annot=True, cmap=plt.cm.Blues, xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()","4222843f":"test_len = X_test.shape[0]\ncv_len = X_cv.shape[0]\ncv_y_pred = np.zeros((cv_len,9))\nfor i in range(cv_len):\n    rand_probs = np.random.rand(1,9)\n    cv_y_pred[i] = ((rand_probs\/sum(sum(rand_probs)))[0])\nprint(\"Log loss on validation data using Random Model\",log_loss(y_cv,cv_y_pred, eps=1e-15))\ntest_y_pred = np.zeros((test_len,9))\nfor i in range(test_len):\n    rand_probs = np.random.rand(1,9)\n    test_y_pred[i] = ((rand_probs\/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Test Data using Random Model\",log_loss(y_test,test_y_pred, eps=1e-15))\ny_pred = np.argmax(test_y_pred, axis=1)\ncm = confusion_matrix(y_test, y_pred+1)\nplot_confusion_recall_precision(cm)","cdfe8381":"#user defined functions to fet feature representations\ndef get_column_fea_dict(alpha, column, df):\n    freq = X_train[column].value_counts()\n    column_dict = dict()\n    for i, denominator in freq.items():\n        vec = []\n        for k in range(1,10):\n            subset = X_train.loc[(X_train['Class']==k) & (X_train[column]==i)]\n            vec.append((subset.shape[0] + alpha*10)\/ (denominator + 90*alpha))\n        column_dict[i]=vec\n    return column_dict\ndef get_column_feature(alpha, column, df):\n    column_dict = get_column_fea_dict(alpha, column, df)\n    freq = X_train[column].value_counts()\n    column_fea = []\n    for index, row in df.iterrows():\n        if row[column] in dict(freq).keys():\n            column_fea.append(column_dict[row[column]])\n        else:\n            column_fea.append([1\/9,1\/9,1\/9,1\/9,1\/9,1\/9,1\/9,1\/9,1\/9])\n    return column_fea","84168d58":"print('Number of Unique Genes :', X_train[\"Gene\"].nunique())\nfreq_genes = X_train['Gene'].value_counts()\nprint(\"Top 10 genes with highest frequency\")\nfreq_genes.head(10)","075674f8":"plt.figure(figsize = (20,8))\nsns.countplot(freq_genes)\nplt.xticks(rotation = 90)\nplt.xlabel('Index of a Gene based on their decreasing order of frequency')\nplt.title('Bar plot of most oftenly occuring Genes')","975e2cc6":"alpha = 1\ntrain_gene_feat_resp_coding = np.array(get_column_feature(alpha, \"Gene\", X_train))\nval_gene_feat_resp_coding = np.array(get_column_feature(alpha, \"Gene\", X_cv))\ntest_gene_feat_resp_coding = np.array(get_column_feature(alpha, \"Gene\", X_test))","dbbb2caa":"print(\"shape of training gene feature after response coding :\", train_gene_feat_resp_coding.shape)","6334acf6":"gene_vectorizer = CountVectorizer()\ntrain_gene_feat_onehot_en = gene_vectorizer.fit_transform(X_train['Gene'])\nval_gene_feat_onehot_en = gene_vectorizer.transform(X_cv['Gene'])\ntest_gene_feat_onehot_en = gene_vectorizer.transform(X_test['Gene'])","34cab8f1":"gene_vectorizer.get_feature_names()","49b26f42":"print(\"shape of training gene feature after one hot encoding :\", train_gene_feat_onehot_en.shape)","af85e759":"alpha = [10 ** x for x in range(-5, 1)]\nval_log_loss_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_gene_feat_onehot_en, y_train)\n    calib_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    calib_clf.fit(train_gene_feat_onehot_en, y_train)\n    y_pred = calib_clf.predict_proba(val_gene_feat_onehot_en)\n    val_log_loss_array.append(log_loss(y_cv, y_pred, labels=clf.classes_, eps=1e-15))\n    print('For alpha = ', i, \"The log loss is:\",log_loss(y_cv, y_pred, labels=clf.classes_, eps=1e-15))\nplt.plot(alpha, val_log_loss_array)\nfor i, logloss in enumerate(np.round(val_log_loss_array,3)):\n    plt.annotate((alpha[i],np.round(logloss,3)), (alpha[i],logloss))\nplt.grid()\nplt.title(\"Validation log loss for different values of alpha\")\nplt.xlabel(\"Alpha\")\nplt.ylabel(\"Log loss\")","50c6f499":"best_alpha = np.argmin(val_log_loss_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_gene_feat_onehot_en, y_train)\ncalib_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\ncalib_clf.fit(train_gene_feat_onehot_en, y_train)\n\ny_pred = calib_clf.predict_proba(train_gene_feat_onehot_en)\nprint('For the best alpha of alpha = ', alpha[best_alpha], \"The log loss on training data is : \",log_loss(y_train, y_pred, labels=clf.classes_, eps=1e-15))\ny_pred = calib_clf.predict_proba(val_gene_feat_onehot_en)\nprint('For the best alpha of alpha = ', alpha[best_alpha], \"The log loss on validation data is: \",log_loss(y_cv, y_pred, labels=clf.classes_, eps=1e-15))\ny_pred = calib_clf.predict_proba(test_gene_feat_onehot_en)\nprint('For the best alpha of alpha = ', alpha[best_alpha], \"The log loss on test data is : \",log_loss(y_test, y_pred, labels=clf.classes_, eps=1e-15))","3ae67fe7":"print(\"Number of observations in test and validation datasets covered by the unique \", X_train[\"Gene\"].nunique(), \" genes in train dataset\")\n\ntest_cover=X_test[X_test['Gene'].isin(list(X_train['Gene'].unique()))].shape[0]\nvalidation_cover=X_cv[X_cv['Gene'].isin(list(X_train['Gene'].unique()))].shape[0]\n\nprint('In test data',test_cover, 'out of',X_test.shape[0], \":\",(test_cover\/X_test.shape[0])*100)\nprint('2. In cross validation data',validation_cover, 'out of ',X_cv.shape[0],\":\" ,(validation_cover\/X_cv.shape[0])*100)","197b0770":"print('Number of Unique Variations :', X_train[\"Variation\"].nunique())\nfreq_variations = X_train['Variation'].value_counts()\nprint(\"Top 10 variations with highest frequency\")\nfreq_variations.head(10)","794467d9":"total_variations = sum(freq_variations.values)\nfraction = freq_variations.values\/total_variations\nplt.plot(fraction, label=\"Histrogram of Variations\")\nplt.xlabel('Index of a variations based on their decreasing order of frequency')\nplt.ylabel('Frequency')\nplt.legend()\nplt.grid()","bcf9579d":"alpha = 1\ntrain_variation_feat_resp_coding = np.array(get_column_feature(alpha, \"Variation\", X_train))\nval_variation_feat_resp_coding = np.array(get_column_feature(alpha, \"Variation\", X_cv))\ntest_variation_feat_resp_coding = np.array(get_column_feature(alpha, \"Variation\", X_test))","a6281d77":"print(\"shape of training variation feature after response coding :\", train_variation_feat_resp_coding.shape)","ddaf92e5":"variation_vectorizer = CountVectorizer()\ntrain_variation_feat_onehot_en = variation_vectorizer.fit_transform(X_train['Variation'])\nval_variation_feat_onehot_en = variation_vectorizer.transform(X_cv['Variation'])\ntest_variation_feat_onehot_en = variation_vectorizer.transform(X_test['Variation'])","6d1011c6":"variation_vectorizer.get_feature_names()","fc909ea5":"print(\"shape of training varaition feature after one hot encoding :\", train_variation_feat_onehot_en.shape)","fe4b180b":"alpha = [10 ** x for x in range(-5, 1)]\nval_log_loss_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_variation_feat_onehot_en, y_train)\n    calib_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    calib_clf.fit(train_variation_feat_onehot_en, y_train)\n    y_pred = calib_clf.predict_proba(val_variation_feat_onehot_en)\n    val_log_loss_array.append(log_loss(y_cv, y_pred, labels=clf.classes_, eps=1e-15))\n    print('For alpha = ', i, \"The log loss is:\",log_loss(y_cv, y_pred, labels=clf.classes_, eps=1e-15))\nplt.plot(alpha, val_log_loss_array)\nfor i, logloss in enumerate(np.round(val_log_loss_array,3)):\n    plt.annotate((alpha[i],np.round(logloss,3)), (alpha[i],logloss))\nplt.grid()\nplt.title(\"Validation log loss for different values of alpha\")\nplt.xlabel(\"Alpha\")\nplt.ylabel(\"Log loss\")","4eaf0241":"best_alpha = np.argmin(val_log_loss_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_variation_feat_onehot_en, y_train)\ncalib_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\ncalib_clf.fit(train_variation_feat_onehot_en, y_train)\n\ny_pred = calib_clf.predict_proba(train_variation_feat_onehot_en)\nprint('For the best alpha of alpha = ', alpha[best_alpha], \"The log loss on training data is:\",log_loss(y_train, y_pred, labels=clf.classes_, eps=1e-15))\ny_pred = calib_clf.predict_proba(val_variation_feat_onehot_en)\nprint('For the best alpha of alpha = ', alpha[best_alpha], \"The log loss on validation data is:\",log_loss(y_cv, y_pred, labels=clf.classes_, eps=1e-15))\ny_pred = calib_clf.predict_proba(test_variation_feat_onehot_en)\nprint('For the best alpha of alpha = ', alpha[best_alpha], \"The log loss on test data is:\",log_loss(y_test, y_pred, labels=clf.classes_, eps=1e-15))","9a3357f5":"print(\"Number of observations in test and validation datasets covered by the unique \", X_train[\"Variation\"].nunique(), \" variations in train dataset\")\n\ntest_cover=X_test[X_test['Variation'].isin(list(X_train['Variation'].unique()))].shape[0]\nvalidation_cover=X_cv[X_cv['Variation'].isin(list(X_train['Variation'].unique()))].shape[0]\n\nprint('In test data',test_cover, 'out of',X_test.shape[0], \":\",(test_cover\/X_test.shape[0])*100)\nprint('2. In cross validation data',validation_cover, 'out of ',X_cv.shape[0],\":\" ,(validation_cover\/X_cv.shape[0])*100)","ae9cd66f":"#using Bag of words approach\ntext_vectorizer = CountVectorizer(min_df=3)\ntrain_text_feat_onehot_en = text_vectorizer.fit_transform(X_train['TEXT'])\ntrain_text_features= text_vectorizer.get_feature_names()\ntrain_text_feat_counts = train_text_feat_onehot_en.sum(axis=0).A1\ntext_feat_dict = dict(zip(list(train_text_features),train_text_feat_counts))\nprint(\"Total number of unique words in TEXT feature of training data :\", len(train_text_features))","68f74f52":"def get_word_count_dictionary(df_cls):\n    dic = defaultdict(int)\n    for index, row in df_cls.iterrows():\n        for word in row['TEXT'].split():\n            dic[word] +=1\n    return dic","4c3dddfa":"def get_text_resp_coding(df):\n    text_feat_resp_coding = np.zeros((df.shape[0],9))\n    for i in range(0,9):\n        row_index = 0\n        for index, row in df.iterrows():\n            total_prob = 0\n            for word in row['TEXT'].split():\n                total_prob += math.log(((dic_list[i].get(word,0)+10 )\/(total_dic.get(word,0)+90)))\n            text_feat_resp_coding[row_index][i] = math.exp(total_prob\/len(row['TEXT'].split()))\n            row_index += 1\n    return text_feat_resp_coding","1bd20b3f":"dic_list = []\nfor i in range(1,10):\n    subset_cls = X_train[X_train['Class']==i]\n    dic_list.append(get_word_count_dictionary(subset_cls))\ntotal_dic = get_word_count_dictionary(X_train)","81eed4ae":"train_text_feat_resp_coding  = get_text_resp_coding(X_train)\nval_text_feat_resp_coding  = get_text_resp_coding(X_cv)\ntest_text_feat_resp_coding  = get_text_resp_coding(X_test)","105b0c81":"train_text_feat_resp_coding = (train_text_feat_resp_coding.T\/train_text_feat_resp_coding.sum(axis=1)).T\nval_text_feat_resp_coding = (val_text_feat_resp_coding.T\/val_text_feat_resp_coding.sum(axis=1)).T\ntest_text_feat_resp_coding = (test_text_feat_resp_coding.T\/test_text_feat_resp_coding.sum(axis=1)).T","d9fbb2ea":"train_text_feat_onehot_en = normalize(train_text_feat_onehot_en, axis=0)\ntest_text_feat_onehot_en = text_vectorizer.transform(X_test['TEXT'])\ntest_text_feat_onehot_en = normalize(test_text_feat_onehot_en, axis=0)\nval_text_feat_onehot_en = text_vectorizer.transform(X_cv['TEXT'])\nval_text_feat_onehot_en = normalize(val_text_feat_onehot_en, axis=0)","ea36d2e8":"sorted_text_feat_dict = dict(sorted(text_feat_dict.items(), key=lambda x: x[1] , reverse=True))\nsorted_text_occur = np.array(list(sorted_text_feat_dict.values()))","d40386b6":"# Number of words to a given frequency\nprint(Counter(sorted_text_occur))","952a4ec0":"alpha = [10 ** x for x in range(-5, 1)]\nval_log_loss_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_text_feat_onehot_en, y_train)\n    calib_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    calib_clf.fit(train_text_feat_onehot_en, y_train)\n    y_pred = calib_clf.predict_proba(val_text_feat_onehot_en)\n    val_log_loss_array.append(log_loss(y_cv, y_pred, labels=clf.classes_, eps=1e-15))\n    print('For alpha = ', i, \"The log loss is:\",log_loss(y_cv, y_pred, labels=clf.classes_, eps=1e-15))\nplt.plot(alpha, val_log_loss_array)\nfor i, logloss in enumerate(np.round(val_log_loss_array,3)):\n    plt.annotate((alpha[i],np.round(logloss,3)), (alpha[i],logloss))\nplt.grid()\nplt.title(\"Validation log loss for different values of alpha\")\nplt.xlabel(\"Alpha\")\nplt.ylabel(\"Log loss\")\n","27a8fb45":"best_alpha = np.argmin(val_log_loss_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_text_feat_onehot_en, y_train)\ncalib_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\ncalib_clf.fit(train_text_feat_onehot_en, y_train)\n\ny_pred = calib_clf.predict_proba(train_text_feat_onehot_en)\nprint('For the best alpha of alpha = ', alpha[best_alpha], \"The log loss on training data is:\",log_loss(y_train, y_pred, labels=clf.classes_, eps=1e-15))\ny_pred = calib_clf.predict_proba(val_text_feat_onehot_en)\nprint('For the best alpha of alpha = ', alpha[best_alpha], \"The log loss on validation data is:\",log_loss(y_cv, y_pred, labels=clf.classes_, eps=1e-15))\ny_pred = calib_clf.predict_proba(test_text_feat_onehot_en)\nprint('For the best alpha of alpha = ', alpha[best_alpha], \"The log loss on test data is:\",log_loss(y_test, y_pred, labels=clf.classes_, eps=1e-15))","880634b6":"def get_common_word(df):\n    text_vectorizer = CountVectorizer(min_df=3)\n    df_text_feat_onehot_en = text_vectorizer.fit_transform(df['TEXT'])\n    df_text_features = text_vectorizer.get_feature_names()\n\n    df_text_feat_counts = df_text_feat_onehot_en.sum(axis=0).A1\n    df_text_fea_dict = dict(zip(list(df_text_features),df_text_feat_counts))\n    df_len = len(set(df_text_features))\n    common_words_len = len(set(train_text_features) & set(df_text_features))\n    return df_len,common_words_len","d072fd69":"cv_len,common_words_len = get_common_word(X_cv)\nprint(np.round((common_words_len\/cv_len)*100, 3), \"% of word of validation appeared in train data\")\ntest_len,common_words_len = get_common_word(X_test)\nprint(np.round((common_words_len\/test_len)*100, 3), \"% of word of test data appeared in train data\")","8afdba4a":"#user defined function to calculate confusion matrix, precision and recall and also to plot\ndef predict_and_plot_confusion_recall_precision(X_train, y_train,X_test, y_test, classifier):\n    classifier.fit(X_train, y_train)\n    calib_clf = CalibratedClassifierCV(classifier, method=\"sigmoid\")\n    calib_clf.fit(X_train, y_train)\n    y_pred = calib_clf.predict(X_test)\n\n    # for calculating log_loss we willl provide the array of probabilities belongs to each class\n    print(\"Log loss :\",log_loss(y_test, calib_clf.predict_proba(X_test)))\n    # calculating the number of data points that are misclassified\n    print(\"Number of mis-classified points :\", np.count_nonzero((y_pred- y_test))\/y_test.shape[0])\n    cm = confusion_matrix(y_test, y_pred)\n    plot_confusion_recall_precision(cm)","28de2a41":"#user defined function to calculate log loss\ndef calculate_log_loss(X_train, y_train,X_test, y_test, classifier):\n    classifier.fit(X_train, y_train)\n    calib_clf = CalibratedClassifierCV(classifier, method=\"sigmoid\")\n    calib_clf.fit(X_train, y_train)\n    calib_clf_probs = calib_clf.predict_proba(X_test)\n    return log_loss(y_test, calib_clf_probs, eps=1e-15)","a7c4984b":"# user defined function to get important feature \ndef get_impfeature_names(indices, text, gene, var, no_features):\n    gene_count_vectorizer = CountVectorizer()\n    var_count_vectorizer = CountVectorizer()\n    text_count_vectorizer = CountVectorizer(min_df=3)\n    \n    gene_vec_onehot = gene_count_vectorizer.fit(X_train['Gene'])\n    var_vec_onehot  = var_count_vectorizer.fit(X_train['Variation'])\n    text_vec_onehot = text_count_vectorizer.fit(X_train['TEXT'])\n    \n    feat1_len = len(gene_count_vectorizer.get_feature_names())\n    feat2_len = len(var_count_vectorizer.get_feature_names())\n    \n    word_present = 0\n    for i,v in enumerate(indices):\n        if (v < feat1_len):\n            word = gene_count_vectorizer.get_feature_names()[v]\n            flag = True if word == gene else False\n            if flag:\n                word_present += 1\n                print(i, \"Gene feature [{}] present in test data point [{}]\".format(word,flag))\n        elif (v < feat1_len+feat2_len):\n            word = var_count_vectorizer.get_feature_names()[v-(feat1_len)]\n            flag = True if word == var else False\n            if flag:\n                word_present += 1\n                print(i, \"variation feature [{}] present in test data point [{}]\".format(word,flag))\n        else:\n            word = text_count_vectorizer.get_feature_names()[v-(feat1_len+feat2_len)]\n            flag = True if word in text.split() else False\n            if flag:\n                word_present += 1\n                print(i, \"Text feature [{}] present in test data point [{}]\".format(word,flag))\n\n    print(\"Out of the top \",no_features,\" features \", word_present, \"are present in query point\")","9120b05b":"train_gene_and_var_onehot_en = hstack((train_gene_feat_onehot_en,train_variation_feat_onehot_en))\nval_gene_and_var_onehot_en = hstack((val_gene_feat_onehot_en,val_variation_feat_onehot_en))\ntest_gene_and_var_onehot_en = hstack((test_gene_feat_onehot_en,test_variation_feat_onehot_en))","579b5102":"X_train_onehotCoding = hstack((train_gene_and_var_onehot_en, train_text_feat_onehot_en)).tocsr()\ny_train = np.array(X_train['Class'])\nX_test_onehotCoding = hstack((test_gene_and_var_onehot_en, test_text_feat_onehot_en)).tocsr()\ny_test = np.array(X_test['Class'])\nX_cv_onehotCoding = hstack((val_gene_and_var_onehot_en, val_text_feat_onehot_en)).tocsr()\ny_cv = np.array(X_cv['Class'])","7f978be1":"train_gene_and_var_responseCoding = np.hstack((train_gene_feat_resp_coding,train_variation_feat_resp_coding))\ntest_gene_and_var_responseCoding = np.hstack((test_gene_feat_resp_coding,test_variation_feat_resp_coding))\nval_gene_and_var_responseCoding = np.hstack((val_gene_feat_resp_coding,val_variation_feat_resp_coding))\n\nX_train_responseCoding = np.hstack((train_gene_and_var_responseCoding, train_text_feat_resp_coding))\nX_test_responseCoding = np.hstack((test_gene_and_var_responseCoding, test_text_feat_resp_coding))\nX_cv_responseCoding = np.hstack((val_gene_and_var_responseCoding, val_text_feat_resp_coding))","74c0d86a":"print(\"Overview about one hot encoding features : \")\nprint(\"Size of one hot encoded train data : \", X_train_onehotCoding.shape)\nprint(\"Size of one hot encoded test data : \", X_test_onehotCoding.shape)\nprint(\"Size of one hot encoded validation data : \", X_cv_onehotCoding.shape)","024aee09":"print(\" Overview about response coded features :\")\nprint(\"Size of response coded train data : \", X_train_responseCoding.shape)\nprint(\"Size of response coded test data : \", X_test_responseCoding.shape)\nprint(\"Size of response coded validation data \", X_cv_responseCoding.shape)","df884e17":"alpha = [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100,1000]\nval_log_loss_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = MultinomialNB(alpha=i)\n    clf.fit(X_train_onehotCoding, y_train)\n    calib_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    calib_clf.fit(X_train_onehotCoding, y_train)\n    calib_clf_probs = calib_clf.predict_proba(X_cv_onehotCoding)\n    val_log_loss_array.append(log_loss(y_cv, calib_clf_probs, labels=clf.classes_, eps=1e-15))\n    print(\"Log Loss :\",log_loss(y_cv, calib_clf_probs)) \nplt.plot(np.log10(alpha), val_log_loss_array)\nfor i, logloss in enumerate(np.round(val_log_loss_array,3)):\n    plt.annotate((alpha[i],str(logloss)), (np.log10(alpha[i]),logloss))\nplt.grid()\nplt.xticks(np.log10(alpha))\nplt.title(\"Validation log loss for different values of alpha\")\nplt.xlabel(\"Alpha\")\nplt.ylabel(\"Log loss\")\n\nbest_alpha = np.argmin(val_log_loss_array)\nclf = MultinomialNB(alpha=i)\nclf.fit(X_train_onehotCoding, y_train)\ncalib_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\ncalib_clf.fit(X_train_onehotCoding, y_train)\n\ny_pred = calib_clf.predict_proba(X_train_onehotCoding)\nprint('For the best alpha of alpha = ', alpha[best_alpha], \"The log loss on training data is:\",log_loss(y_train, y_pred, labels=clf.classes_, eps=1e-15))\ny_pred = calib_clf.predict_proba(X_cv_onehotCoding)\nprint('For the best alpha of alpha = ', alpha[best_alpha], \"The log loss on validation data is:\",log_loss(y_cv, y_pred, labels=clf.classes_, eps=1e-15))\ny_pred = calib_clf.predict_proba(X_test_onehotCoding)\nprint('For the best alpha of alpha = ', alpha[best_alpha], \"The log loss on test data is:\",log_loss(y_test, y_pred, labels=clf.classes_, eps=1e-15))","6c654bc1":"best_alpha = np.argmin(val_log_loss_array)\nclf = MultinomialNB(alpha=alpha[best_alpha])\npredict_and_plot_confusion_recall_precision(X_train_onehotCoding, y_train,X_cv_onehotCoding, y_cv, clf)","a05187dd":"random_index = 1\nno_feature = 100\npred_cls = calib_clf.predict(X_test_onehotCoding[random_index])\nprint(\"Predicted Class :\", pred_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(calib_clf.predict_proba(X_test_onehotCoding[random_index]),4))\nprint(\"Actual Class :\", y_test[random_index])\nindices=np.argsort(abs(-clf.coef_))[pred_cls-1][:,:no_feature]\nprint(\"=\"*50)\nget_impfeature_names(indices[0], X_test['TEXT'].iloc[random_index],X_test['Gene'].iloc[random_index],X_test['Variation'].iloc[random_index], no_feature)","c1a463d8":"random_index = 100\nno_feature = 100\npred_cls = calib_clf.predict(X_test_onehotCoding[random_index])\nprint(\"Predicted Class :\", pred_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(calib_clf.predict_proba(X_test_onehotCoding[random_index]),4))\nprint(\"Actual Class :\", y_test[random_index])\nindices=np.argsort(abs(-clf.coef_))[pred_cls-1][:,:no_feature]\nprint(\"=\"*50)\nget_impfeature_names(indices[0], X_test['TEXT'].iloc[random_index],X_test['Gene'].iloc[random_index],X_test['Variation'].iloc[random_index], no_feature)","99ae135e":"alpha = [5, 11, 15, 21, 31, 41, 51, 99]\nval_log_loss_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = KNeighborsClassifier(n_neighbors=i)\n    clf.fit(X_train_responseCoding, y_train)\n    calib_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    calib_clf.fit(X_train_responseCoding, y_train)\n    calib_clf_probs = calib_clf.predict_proba(X_cv_responseCoding)\n    val_log_loss_array.append(log_loss(y_cv, calib_clf_probs, labels=clf.classes_, eps=1e-15))\n    print(\"Log Loss :\",log_loss(y_cv, calib_clf_probs)) \nplt.plot(alpha, val_log_loss_array)\nfor i, logloss in enumerate(np.round(val_log_loss_array,3)):\n    plt.annotate((alpha[i],str(logloss)), (alpha[i],logloss))\nplt.grid()\nplt.xticks(alpha)\nplt.title(\"Validation log loss for different values of alpha\")\nplt.xlabel(\"Alpha\")\nplt.ylabel(\"Log loss\")\n\nbest_alpha = np.argmin(val_log_loss_array)\nclf = KNeighborsClassifier(n_neighbors=i)\nclf.fit(X_train_responseCoding, y_train)\ncalib_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\ncalib_clf.fit(X_train_responseCoding, y_train)\n\ny_pred = calib_clf.predict_proba(X_train_responseCoding)\nprint('For the best alpha of alpha = ', alpha[best_alpha], \"The log loss on training data is:\",log_loss(y_train, y_pred, labels=clf.classes_, eps=1e-15))\ny_pred = calib_clf.predict_proba(X_cv_responseCoding)\nprint('For the best alpha of alpha = ', alpha[best_alpha], \"The log loss on validation data is:\",log_loss(y_cv, y_pred, labels=clf.classes_, eps=1e-15))\ny_pred = calib_clf.predict_proba(X_test_responseCoding)\nprint('For the best alpha of alpha = ', alpha[best_alpha], \"The log loss on test data is:\",log_loss(y_test, y_pred, labels=clf.classes_, eps=1e-15))","b2595318":"best_alpha = np.argmin(val_log_loss_array)\nclf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\npredict_and_plot_confusion_recall_precision(X_train_responseCoding, y_train,X_cv_responseCoding, y_cv, clf)","768203c4":"random_index = 1\npred_cls = calib_clf.predict(X_test_responseCoding[random_index].reshape(1,-1))\nprint(\"Predicted Class :\", pred_cls[0])\nprint(\"Actual Class :\", y_test[random_index])\nnearest_neighbors = clf.kneighbors(X_test_responseCoding[random_index].reshape(1, -1), alpha[best_alpha])\nprint(\"The \",alpha[best_alpha],\" nearest neighbours of the random test point belongs to classes\",y_train[nearest_neighbors[1][0]])\nprint(\"Fequency of nearest points :\",Counter(y_train[nearest_neighbors[1][0]]))","5a0a0581":"random_index = 100\npred_cls = calib_clf.predict(X_test_responseCoding[random_index].reshape(1,-1))\nprint(\"Predicted Class :\", pred_cls[0])\nprint(\"Actual Class :\", y_test[random_index])\nnearest_neighbors = clf.kneighbors(X_test_responseCoding[random_index].reshape(1, -1), alpha[best_alpha])\nprint(\"The \",alpha[best_alpha],\" nearest neighbours of the random test point belongs to classes\",y_train[nearest_neighbors[1][0]])\nprint(\"Fequency of nearest points :\",Counter(y_train[nearest_neighbors[1][0]]))","b85389b8":"alpha = [10 ** x for x in range(-6, 3)]\nval_log_loss_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = SGDClassifier(class_weight='balanced', alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(X_train_onehotCoding, y_train)\n    calib_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    calib_clf.fit(X_train_onehotCoding, y_train)\n    calib_clf_probs = calib_clf.predict_proba(X_cv_onehotCoding)\n    val_log_loss_array.append(log_loss(y_cv, calib_clf_probs, labels=clf.classes_, eps=1e-15))\n    print(\"Log Loss :\",log_loss(y_cv, calib_clf_probs)) \nplt.plot(alpha, val_log_loss_array)\nfor i, logloss in enumerate(np.round(val_log_loss_array,3)):\n    plt.annotate((alpha[i],str(logloss)), (alpha[i],logloss))\nplt.grid()\nplt.xticks(alpha)\nplt.title(\"Validation log loss for different values of alpha\")\nplt.xlabel(\"Alpha\")\nplt.ylabel(\"Log loss\")\n\nbest_alpha = np.argmin(val_log_loss_array)\nclf = SGDClassifier(class_weight='balanced',alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(X_train_onehotCoding, y_train)\ncalib_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\ncalib_clf.fit(X_train_onehotCoding, y_train)\n\ny_pred = calib_clf.predict_proba(X_train_onehotCoding)\nprint('For the best alpha of alpha = ', alpha[best_alpha], \"The log loss on training data is:\",log_loss(y_train, y_pred, labels=clf.classes_, eps=1e-15))\ny_pred = calib_clf.predict_proba(X_cv_onehotCoding)\nprint('For the best alpha of alpha = ', alpha[best_alpha], \"The log loss on validation data is:\",log_loss(y_cv, y_pred, labels=clf.classes_, eps=1e-15))\ny_pred = calib_clf.predict_proba(X_test_onehotCoding)\nprint('For the best alpha of alpha = ', alpha[best_alpha], \"The log loss on test data is:\",log_loss(y_test, y_pred, labels=clf.classes_, eps=1e-15))","2ec6117e":"best_alpha = np.argmin(val_log_loss_array)\nclf = SGDClassifier(class_weight='balanced',alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusion_recall_precision(X_train_onehotCoding, y_train,X_cv_onehotCoding, y_cv, clf)","24b5602b":"random_index = 1\nno_feature = 500\npred_cls = calib_clf.predict(X_test_onehotCoding[random_index])\nprint(\"Predicted Class :\", pred_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(calib_clf.predict_proba(X_test_onehotCoding[random_index]),4))\nprint(\"Actual Class :\", y_test[random_index])\nindices=np.argsort(abs(-clf.coef_))[pred_cls-1][:,:no_feature]\nprint(\"=\"*50)\nget_impfeature_names(indices[0], X_test['TEXT'].iloc[random_index],X_test['Gene'].iloc[random_index],X_test['Variation'].iloc[random_index], no_feature)","447c02ed":"random_index = 100\nno_feature = 500\npred_cls = calib_clf.predict(X_test_onehotCoding[random_index])\nprint(\"Predicted Class :\", pred_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(calib_clf.predict_proba(X_test_onehotCoding[random_index]),4))\nprint(\"Actual Class :\", y_test[random_index])\nindices=np.argsort(abs(-clf.coef_))[pred_cls-1][:,:no_feature]\nprint(\"=\"*50)\nget_impfeature_names(indices[0], X_test['TEXT'].iloc[random_index],X_test['Gene'].iloc[random_index],X_test['Variation'].iloc[random_index], no_feature)","0180af7f":"alpha = [10 ** x for x in range(-6, 1)]\nval_log_loss_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(X_train_onehotCoding, y_train)\n    calib_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    calib_clf.fit(X_train_onehotCoding, y_train)\n    calib_clf_probs = calib_clf.predict_proba(X_cv_onehotCoding)\n    val_log_loss_array.append(log_loss(y_cv, calib_clf_probs, labels=clf.classes_, eps=1e-15))\n    print(\"Log Loss :\",log_loss(y_cv, calib_clf_probs)) \nplt.plot(alpha, val_log_loss_array)\nfor i, logloss in enumerate(np.round(val_log_loss_array,3)):\n    plt.annotate((alpha[i],str(logloss)), (alpha[i],logloss))\nplt.grid()\nplt.xticks(alpha)\nplt.title(\"Validation log loss for different values of alpha\")\nplt.xlabel(\"Alpha\")\nplt.ylabel(\"Log loss\")\n\nbest_alpha = np.argmin(val_log_loss_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(X_train_onehotCoding, y_train)\ncalib_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\ncalib_clf.fit(X_train_onehotCoding, y_train)\n\ny_pred = calib_clf.predict_proba(X_train_onehotCoding)\nprint('For the best alpha of alpha = ', alpha[best_alpha], \"The log loss on training data is:\",log_loss(y_train, y_pred, labels=clf.classes_, eps=1e-15))\ny_pred = calib_clf.predict_proba(X_cv_onehotCoding)\nprint('For the best alpha of alpha = ', alpha[best_alpha], \"The log loss on validation data is:\",log_loss(y_cv, y_pred, labels=clf.classes_, eps=1e-15))\ny_pred = calib_clf.predict_proba(X_test_onehotCoding)\nprint('For the best alpha of alpha = ', alpha[best_alpha], \"The log loss on test data is:\",log_loss(y_test, y_pred, labels=clf.classes_, eps=1e-15))","b2397c31":"best_alpha = np.argmin(val_log_loss_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusion_recall_precision(X_train_onehotCoding, y_train,X_cv_onehotCoding, y_cv, clf)","2e52ba5f":"random_index = 1\nno_feature = 500\npred_cls = calib_clf.predict(X_test_onehotCoding[random_index])\nprint(\"Predicted Class :\", pred_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(calib_clf.predict_proba(X_test_onehotCoding[random_index]),4))\nprint(\"Actual Class :\", y_test[random_index])\nindices=np.argsort(abs(-clf.coef_))[pred_cls-1][:,:no_feature]\nprint(\"=\"*50)\nget_impfeature_names(indices[0], X_test['TEXT'].iloc[random_index],X_test['Gene'].iloc[random_index],X_test['Variation'].iloc[random_index], no_feature)","a7074417":"random_index = 100\nno_feature = 500\npred_cls = calib_clf.predict(X_test_onehotCoding[random_index])\nprint(\"Predicted Class :\", pred_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(calib_clf.predict_proba(X_test_onehotCoding[random_index]),4))\nprint(\"Actual Class :\", y_test[random_index])\nindices=np.argsort(abs(-clf.coef_))[pred_cls-1][:,:no_feature]\nprint(\"=\"*50)\nget_impfeature_names(indices[0], X_test['TEXT'].iloc[random_index],X_test['Gene'].iloc[random_index],X_test['Variation'].iloc[random_index], no_feature)","ba7efd27":"alpha = [10 ** x for x in range(-5, 3)]\nval_log_loss_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = SGDClassifier(class_weight='balanced',alpha=i, penalty='l2', loss='hinge', random_state=42)\n    clf.fit(X_train_onehotCoding, y_train)\n    calib_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    calib_clf.fit(X_train_onehotCoding, y_train)\n    calib_clf_probs = calib_clf.predict_proba(X_cv_onehotCoding)\n    val_log_loss_array.append(log_loss(y_cv, calib_clf_probs, labels=clf.classes_, eps=1e-15))\n    print(\"Log Loss :\",log_loss(y_cv, calib_clf_probs)) \nplt.plot(alpha, val_log_loss_array)\nfor i, logloss in enumerate(np.round(val_log_loss_array,3)):\n    plt.annotate((alpha[i],str(logloss)), (alpha[i],logloss))\nplt.grid()\nplt.xticks(alpha)\nplt.title(\"Validation log loss for different values of alpha\")\nplt.xlabel(\"Alpha\")\nplt.ylabel(\"Log loss\")\n\nbest_alpha = np.argmin(val_log_loss_array)\nclf = SGDClassifier(class_weight='balanced',alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\nclf.fit(X_train_onehotCoding, y_train)\ncalib_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\ncalib_clf.fit(X_train_onehotCoding, y_train)\n\ny_pred = calib_clf.predict_proba(X_train_onehotCoding)\nprint('For the best alpha of alpha = ', alpha[best_alpha], \"The log loss on training data is:\",log_loss(y_train, y_pred, labels=clf.classes_, eps=1e-15))\ny_pred = calib_clf.predict_proba(X_cv_onehotCoding)\nprint('For the best alpha of alpha = ', alpha[best_alpha], \"The log loss on validation data is:\",log_loss(y_cv, y_pred, labels=clf.classes_, eps=1e-15))\ny_pred = calib_clf.predict_proba(X_test_onehotCoding)\nprint('For the best alpha of alpha = ', alpha[best_alpha], \"The log loss on test data is:\",log_loss(y_test, y_pred, labels=clf.classes_, eps=1e-15))","0363e640":"best_alpha = np.argmin(val_log_loss_array)\nclf = SGDClassifier(class_weight='balanced',alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\npredict_and_plot_confusion_recall_precision(X_train_onehotCoding, y_train,X_cv_onehotCoding, y_cv, clf)","c4dae309":"random_index = 1\nno_feature = 500\npred_cls = calib_clf.predict(X_test_onehotCoding[random_index])\nprint(\"Predicted Class :\", pred_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(calib_clf.predict_proba(X_test_onehotCoding[random_index]),4))\nprint(\"Actual Class :\", y_test[random_index])\nindices=np.argsort(abs(-clf.coef_))[pred_cls-1][:,:no_feature]\nprint(\"=\"*50)\nget_impfeature_names(indices[0], X_test['TEXT'].iloc[random_index],X_test['Gene'].iloc[random_index],X_test['Variation'].iloc[random_index], no_feature)","53fc2a60":"random_index = 100\nno_feature = 500\npred_cls = calib_clf.predict(X_test_onehotCoding[random_index])\nprint(\"Predicted Class :\", pred_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(calib_clf.predict_proba(X_test_onehotCoding[random_index]),4))\nprint(\"Actual Class :\", y_test[random_index])\nindices=np.argsort(abs(-clf.coef_))[pred_cls-1][:,:no_feature]\nprint(\"=\"*50)\nget_impfeature_names(indices[0], X_test['TEXT'].iloc[random_index],X_test['Gene'].iloc[random_index],X_test['Variation'].iloc[random_index], no_feature)","e9878ebf":"alpha = [100,200,500,1000,2000]\nmax_depth = [5, 10]\nval_log_loss_array = []\nfor i in alpha:\n    for j in max_depth:\n        print(\"for n_estimators =\", i,\"and max depth = \", j)\n        clf = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j, random_state=42)\n        clf.fit(X_train_onehotCoding, y_train)\n        calib_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n        calib_clf.fit(X_train_onehotCoding, y_train)\n        calib_clf_probs = calib_clf.predict_proba(X_cv_onehotCoding)\n        val_log_loss_array.append(log_loss(y_cv, calib_clf_probs, labels=clf.classes_, eps=1e-15))\n        print(\"Log Loss :\",log_loss(y_cv, calib_clf_probs)) \n\nbest_alpha = np.argmin(val_log_loss_array)\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha\/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42)\nclf.fit(X_train_onehotCoding, y_train)\ncalib_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\ncalib_clf.fit(X_train_onehotCoding, y_train)","25fd25a7":"y_pred = calib_clf.predict_proba(X_train_onehotCoding)\nprint('For the best alpha of alpha = ', alpha[int(best_alpha\/2)], \"The log loss on training data is:\",log_loss(y_train, y_pred, labels=clf.classes_, eps=1e-15))\ny_pred = calib_clf.predict_proba(X_cv_onehotCoding)\nprint('For the best alpha of alpha = ', alpha[int(best_alpha\/2)], \"The log loss on validation data is:\",log_loss(y_cv, y_pred, labels=clf.classes_, eps=1e-15))\ny_pred = calib_clf.predict_proba(X_test_onehotCoding)\nprint('For the best alpha of alpha = ', alpha[int(best_alpha\/2)], \"The log loss on test data is:\",log_loss(y_test, y_pred, labels=clf.classes_, eps=1e-15))","afe5606e":"clf = RandomForestClassifier(n_estimators=alpha[int(best_alpha\/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42)\npredict_and_plot_confusion_recall_precision(X_train_onehotCoding, y_train,X_cv_onehotCoding, y_cv, clf)","f883d438":"random_index = 1\nno_feature = 100\npred_cls = calib_clf.predict(X_test_onehotCoding[random_index])\nprint(\"Predicted Class :\", pred_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(calib_clf.predict_proba(X_test_onehotCoding[random_index]),4))\nprint(\"Actual Class :\", y_test[random_index])\nindices=np.argsort(-clf.feature_importances_)\nprint(\"=\"*50)\nget_impfeature_names(indices[:no_feature], X_test['TEXT'].iloc[random_index],X_test['Gene'].iloc[random_index],X_test['Variation'].iloc[random_index], no_feature)","f6d62666":"random_index = 100\nno_feature = 100\npred_cls = calib_clf.predict(X_test_onehotCoding[random_index])\nprint(\"Predicted Class :\", pred_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(calib_clf.predict_proba(X_test_onehotCoding[random_index]),4))\nprint(\"Actual Class :\", y_test[random_index])\nindices=np.argsort(-clf.feature_importances_)\nprint(\"=\"*50)\nget_impfeature_names(indices[:no_feature], X_test['TEXT'].iloc[random_index],X_test['Gene'].iloc[random_index],X_test['Variation'].iloc[random_index], no_feature)","039ff3e8":"alpha = [10,50,100,200,500,1000]\nmax_depth = [2,3,5,10]\nval_log_loss_array = []\nfor i in alpha:\n    for j in max_depth:\n        print(\"for n_estimators =\", i,\"and max depth = \", j)\n        clf = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j, random_state=42)\n        clf.fit(X_train_responseCoding, y_train)\n        calib_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n        calib_clf.fit(X_train_responseCoding, y_train)\n        calib_clf_probs = calib_clf.predict_proba(X_cv_responseCoding)\n        val_log_loss_array.append(log_loss(y_cv, calib_clf_probs, labels=clf.classes_, eps=1e-15))\n        print(\"Log Loss :\",log_loss(y_cv, calib_clf_probs)) \n\nbest_alpha = np.argmin(val_log_loss_array)\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha\/4)], criterion='gini', max_depth=max_depth[int(best_alpha%4)], random_state=42)\nclf.fit(X_train_responseCoding, y_train)\ncalib_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\ncalib_clf.fit(X_train_responseCoding, y_train)\n\ny_pred = calib_clf.predict_proba(X_train_responseCoding)\nprint('For the best alpha of alpha = ', alpha[int(best_alpha\/4)], \"The log loss on training data is:\",log_loss(y_train, y_pred, labels=clf.classes_, eps=1e-15))\ny_pred = calib_clf.predict_proba(X_cv_responseCoding)\nprint('For the best alpha of alpha = ', alpha[int(best_alpha\/4)], \"The log loss on validation data is:\",log_loss(y_cv, y_pred, labels=clf.classes_, eps=1e-15))\ny_pred = calib_clf.predict_proba(X_test_responseCoding)\nprint('For the best alpha of alpha = ', alpha[int(best_alpha\/4)], \"The log loss on test data is:\",log_loss(y_test, y_pred, labels=clf.classes_, eps=1e-15))","a57a3eb5":"clf = RandomForestClassifier(n_estimators=alpha[int(best_alpha\/4)], criterion='gini', max_depth=max_depth[int(best_alpha%4)], random_state=42)\npredict_and_plot_confusion_recall_precision(X_train_responseCoding, y_train,X_cv_responseCoding, y_cv, clf)","b6d2b042":"random_index = 1\npred_cls = calib_clf.predict(X_test_responseCoding[random_index].reshape(1,-1))\nprint(\"Predicted Class :\", pred_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(calib_clf.predict_proba(X_test_responseCoding[random_index].reshape(1,-1)),4))\nprint(\"Actual Class :\", y_test[random_index])\nindices = np.argsort(-clf.feature_importances_)\nprint(\"=\"*50)\nfor i in indices:\n    if i<9:\n        print(\"Gene is important feature\")\n    elif i<18:\n        print(\"Variation is important feature\")\n    else:\n        print(\"Text is important feature\")","16119487":"random_index = 100\npred_cls = calib_clf.predict(X_test_responseCoding[random_index].reshape(1,-1))\nprint(\"Predicted Class :\", pred_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(calib_clf.predict_proba(X_test_responseCoding[random_index].reshape(1,-1)),4))\nprint(\"Actual Class :\", y_test[random_index])\nindices = np.argsort(-clf.feature_importances_)\nprint(\"=\"*50)\nfor i in indices:\n    if i<9:\n        print(\"Gene is important feature\")\n    elif i<18:\n        print(\"Variation is important feature\")\n    else:\n        print(\"Text is important feature\")","22f0de1e":"clf1 = SGDClassifier(alpha=0.001, penalty='l2', loss='log', class_weight='balanced', random_state=42)\nclf1.fit(X_train_onehotCoding, y_train)\ncalib_clf1 = CalibratedClassifierCV(clf1, method=\"sigmoid\")\n\nclf2 = SGDClassifier(alpha=1, penalty='l2', loss='hinge', class_weight='balanced', random_state=42)\nclf2.fit(X_train_onehotCoding, y_train)\ncalib_clf2 = CalibratedClassifierCV(clf2, method=\"sigmoid\")\n\n\nclf3 = MultinomialNB(alpha=0.001)\nclf3.fit(X_train_onehotCoding, y_train)\ncalib_clf3 = CalibratedClassifierCV(clf3, method=\"sigmoid\")\n\ncalib_clf1.fit(X_train_onehotCoding, y_train)\nprint(\"Logistic Regression :  Log Loss: %0.2f\" % (log_loss(y_cv, calib_clf1.predict_proba(X_cv_onehotCoding))))\ncalib_clf2.fit(X_train_onehotCoding, y_train)\nprint(\"SVM : Log Loss: %0.2f\" % (log_loss(y_cv, calib_clf2.predict_proba(X_cv_onehotCoding))))\ncalib_clf3.fit(X_train_onehotCoding, y_train)\nprint(\"Naive Bayes : Log Loss: %0.2f\" % (log_loss(y_cv, calib_clf3.predict_proba(X_cv_onehotCoding))))\nprint(\"=\"*50)\nalpha = [0.0001,0.001,0.01,0.1,1,10] \nbest_alpha = 999\nfor i in alpha:\n    lr = LogisticRegression(C=i)\n    stack_clf = StackingClassifier(classifiers=[calib_clf1, calib_clf2, calib_clf3], meta_classifier=lr, use_probas=True)\n    stack_clf.fit(X_train_onehotCoding, y_train)\n    print(\"Stacking Classifer : for the value of alpha: %f Log Loss: %0.3f\" % (i, log_loss(y_cv, stack_clf.predict_proba(X_cv_onehotCoding))))\n    logloss =log_loss(y_cv, stack_clf.predict_proba(X_cv_onehotCoding))\n    if best_alpha > logloss:\n        best_alpha = logloss","025bc31d":"lr = LogisticRegression(C=best_alpha)\nstack_clf = StackingClassifier(classifiers=[calib_clf1, calib_clf2, calib_clf3], meta_classifier=lr, use_probas=True)\nstack_clf.fit(X_train_onehotCoding, y_train)\n\nlogloss = log_loss(y_train, stack_clf.predict_proba(X_train_onehotCoding))\nprint(\"Log loss of training data using the stacking classifier :\",logloss)\n\nlogloss = log_loss(y_cv, stack_clf.predict_proba(X_cv_onehotCoding))\nprint(\"Log loss of validation data using the stacking classifier :\",logloss)\n\nlogloss = log_loss(y_test, stack_clf.predict_proba(X_test_onehotCoding))\nprint(\"Log loss of test data using the stacking classifier :\",logloss)\n\nprint(\"Number of missclassified point :\", np.count_nonzero((stack_clf.predict(X_test_onehotCoding)- y_test))\/y_test.shape[0])\ncm = confusion_matrix(y_test, stack_clf.predict(X_test_onehotCoding))\nplot_confusion_recall_precision(cm)","252dc870":"voting_clf = VotingClassifier(estimators=[('lr', calib_clf1), ('svc', calib_clf2), ('rf', calib_clf3)], voting='soft')\nvoting_clf.fit(X_train_onehotCoding, y_train)\nprint(\"Log loss (train) on the VotingClassifier :\", log_loss(y_train, voting_clf.predict_proba(X_train_onehotCoding)))\nprint(\"Log loss (CV) on the VotingClassifier :\", log_loss(y_cv, voting_clf.predict_proba(X_cv_onehotCoding)))\nprint(\"Log loss (test) on the VotingClassifier :\", log_loss(y_test, voting_clf.predict_proba(X_test_onehotCoding)))\nprint(\"Number of missclassified point :\", np.count_nonzero((voting_clf.predict(X_test_onehotCoding)- y_test))\/y_test.shape[0])\ncm = confusion_matrix(y_test, voting_clf.predict(X_test_onehotCoding))\nplot_confusion_recall_precision(cm)","ecc3c460":"## Base Line Model\n\n### 7.1 Naive Bayes \n\n#### 7.1.1 Hyperparameter tuning","c585dc64":"#### 6.c.2.2 Featurization of Text feature using one hot encoding","b4fa65b9":"#### 6.c.2 Featurization of Text feature\n#### 6.c.2.1 Featurization of Text feature using Response coding","99252fde":"#### 7.3.a.4 Feature importance check with incorrectly classified point","74a7d285":"#### 6.b.2 Distribution of Variation categories","615fba0a":"### 7.4 Linear SVM\n\n#### 7.4.1 Hyperparameter tuning","3126dfa6":"#### 7.1.2 Testing the model agianst best hyperparameters ","0b27ddfa":"### 5. Fitting a Random model","f7ee5d08":"#### 4.b Distribution of ${y}_{i}$  in train, validation and test datasets","fe143be7":"#### 7.5.b.3 Feature importance check with correctly classified point","9e22286e":"#### 7.3.a.2 Testing the model agianst best hyperparameters ","7b62c5ab":"#### 7.3.a.3 Feature importance check with correctly classified point","c92c17e7":"#### 3.b Merging both the data frames","595666e0":"#### 6.a.4.2 Checking stability of Gene feature across all the datasets","e296cd7e":"## 7. Classical Machine learning models","83a63623":"### Notebook - Table of Content\n\n1. [**Importing necessary libraries**](#1.-Importing-necessary-libraries)   \n2. [**Loading data**](#2.-Loading-data)   \n    2.a [**Reading gene and variation data**](#2.a-Reading-gene-and-variation-data)  \n    2.b [**Reading text data**](#2.b-Reading-text-data)  \n3. [**Text preprocessing**](#3.-Text-preprocessing)  \n    3.a [**Removing non word characters, whitespaces and stop words**](#3.a-Removing-non-word-characters,-whitespaces-and-stop-words)  \n    3.b [**Merging both the data frames**](#3.b-Merging-both-the-data-frames)  \n    3.c [**Checking for missing values**](#3.c-Checking-for-missing-values)  \n4. [**Train, validation and test split**](#4.-Train,-validation-and-test-split)  \n    4.a [**Splitting data into a ratio of 64:20:16 for train, validation and test set**](#4.a-Splitting-data-into-a-ratio-of-64:20:16-for-train,-validation-and-test-set)  \n    4.b [**Distribution of ${y}_{i}$  in train, validation and test datasets**](#4.b-Distribution-of-${y}_{i}$-in-train,-validation-and-test-datasets)  \n5. [**Fitting a Random model**](#5.-Fitting-a-Random-model)  \n6. [**Univariate Analysis**](#6.-Univariate-Analysis)   \n    6.a [**Univariate Analysis of Gene feature**](#6.a-Univariate-Analysis-of-Gene-feature)  \n    6.a.1 [**Number of uniques genes and their distribution**](#6.a.1-Number-of-uniques-genes-and-their-distribution)  \n    6.a.2 [**Distribution of Gene categories**](#6.a.2-Distribution-of-Gene-categories)  \n    6.a.3 [**Featurization of Gene feature**](#6.a.3-Featurization-of-Gene-feature)    \n    6.a.3.1 [**Featurization of Gene feature using Response coding**](#6.a.3.1-Featurization-of-Gene-feature-using-Response-coding)   \n    6.a.3.2 [**Featurization of Gene feature using One hot encoding**](#6.a.3.2-Featurization-of-Gene-feature-using-One-hot-encoding)   \n    6.a.4 [**Checking significance of Gene feature in predicting ${y}_{i}$**](#6.a.4-Checking-significance-of-Gene-feature-in-predicting-${y}_{i}$)  \n    6.a.4.1 [**Checking through Logistic Regresion model**](#6.a.4.1-Checking-through-Logistic-Regresion-model)   \n    6.a.4.2 [**Checking stability of Gene feature across all the datasets**](#6.a.4.2-Checking-stability-of-Gene-feature-across-all-the-datasets)  \n    6.b [**Univariate Analysis of Variation feature**](#6.b-Univariate-Analysis-of-Variation-feature)  \n    6.b.1 [**Number of uniques variations and their distribution**](#6.b.1-Number-of-uniques-variations-and-their-distribution)  \n    6.b.2 [**Distribution of Variation categories**](#6.b.2-Distribution-of-Variation-categories)  \n    6.b.3 [**Featurization of Variation feature**](#6.b.3-Featurization-of-Variation-feature)    \n    6.b.3.1 [**Featurization of Variation feature using Response coding**](#6.b.3.1-Featurization-of-Variation-feature-using-Response-coding)   \n    6.b.3.2 [**Featurization of Variation feature using one hot encoding**](#6.b.3.2-Featurization-of-Variation-feature-using-one-hot-encoding)   \n    6.b.4 [**Checking significance of Variation feature in predicting ${y}_{i}$**](#6.b.4-Checking-significance-of-Variation-feature-in-predicting-${y}_{i}$)  \n    6.b.4.1 [**Checking through Logistic Regresion model**](#6.b.4.1-Checking-through-Logistic-Regresion-model)   \n    6.b.4.2 [**Checking stability of Variation feature across all the datasets**](#6.b.4.2-Checking-stability-of-Variation-feature-across-all-the-datasets)  \n    6.c [**Univariate Analysis of Text feature**](#6.c-Univariate-Analysis-of-Text-feature)  \n    6.c.1 [**Number of uniques words and their distribution**](#6.c.1-Number-of-uniques-words-and-their-distribution)  \n    6.c.2 [**Featurization of Text feature**](#6.c.2-Featurization-of-Text-feature)    \n    6.c.2.1 [**Featurization of Text feature using Response coding**](#6.c.2.1-Featurization-of-Text-feature-using-Response-coding)   \n    6.c.2.2 [**Featurization of Text feature using one hot encoding**](#6.c.2.2-Featurization-of-Text-feature-using-one-hot-encoding)   \n    6.c.3 [**Checking significance of Text feature in predicting ${y}_{i}$**](#6.c.3-Checking-significance-of-Text-feature-in-predicting-${y}_{i}$)  \n    6.c.3.1 [**Checking through Logistic Regresion model**](#6.c.3.1-Checking-through-Logistic-Regresion-model)   \n    6.c.3.2 [**Checking stability of Text feature across all the datasets**](#6.c.3.2-Checking-stability-of-Text-feature-across-all-the-datasets) \n7. [**Classical Machine learning models**](#7.-Classical-Machine-learning-models)  \n    7.1 [**Naive Bayes**](#7.1-Naive-Bayes)  \n    7.2 [**kNN**](#7.2-kNN)  \n    7.3 [**Logistic Regression**](#7.3-Logistic-Regression)  \n    7.4 [**Linear SVM**](#7.4-Linear-SVM)   \n    7.5 [**Random Forest**](#7.5-Random-Forest)  \n8. [**Stacking the models**](#8.-Stacking-the-models) \n9. [**Majority Voting classifier**](#9.-Majority-Voting-classifier)","be0349d1":"#### 6.b.4.2 Checking stability of Variation feature across all the datasets","02f076b3":"#### 6.b.3 Featurization of Variation feature\n\nTwo ways to do this\n- One hot encoding\n- Response coding\n\n#### 6.b.3.1 Featurization of Variation feature using Response coding","23df9eca":"#### 3.c Checking for missing values","b6351691":"#### 7.3.b.3 Feature importance check with incorrectly classified point","89c5694b":"#### 7.5.b.4 Feature importance check with incorrectly classified point","22d285f7":"#### 7.4.2 Testing the model agianst best hyperparameters ","c8fe3640":"#### 7.2.3 Feature importance check with correctly classified point","beadbd1a":"#### 7.3.b.2 Feature importance check with correctly classified point","d7597cd2":"## 6. Univariate Analysis","4f83e661":"#### 6.b.3.2 Featurization of Variation feature using one hot encoding","84d36dcb":"#### 6.c.3.2 Checking stability of Text feature across all the datasets","bb86f776":"#### 6.a.3.2 Featurization of Gene feature using one hot encoding","59ce64ef":"### 2. Loading Data\n\n#### 2.a Reading gene and variation data","198090b2":"### 6.a.4 Checking significance of Gene feature in predicting ${y}_{i}$ \n\n#### 6.a.4.1 Checking through Logistic Regresion model","c7c7bf3a":"### 7.5.b.1 Hyper paramter tuning (with Response coding)","44aef050":"#### 6.a.3 Featurization of Gene feature\n\nTwo ways to do this\n- One hot encoding\n- Response coding\n\n#### 6.a.3.1 Featurization of Gene feature using Response coding","ba66f841":"#### 7.5.a.4 Feature importance check with incorrectly classified point","56156d6b":"#### 7.5.a.2 Testing the model agianst best hyperparameters using one hot encoding","19129316":"### 1. Importing necessary libraries","626c4130":"#### 2.b Reading text data","736dfc23":"**Additional NOTE**\n\nIf you are interested in learning or exploring more about importance of feature selection in machine learning, then refer to my below blog offering.\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/a-comprehensive-guide-to-feature-selection-using-wrapper-methods-in-python\/","9bb9ddfa":"#### 7.5.a.3 Feature importance check with correctly classified point","943df894":"### 6.c.3 Checking significance of Text feature in predicting ${y}_{i}$ \n\n#### 6.c.3.1 Checking through Logistic Regresion model","b8a435c5":"#### 6.a.2 Distribution of Gene categories","cbeba627":"#### 7.2.2 Testing the model agianst best hyperparameters ","502c5bf2":"#### 7.3.b.1 Testing the model agianst best hyperparameters ","762c23c1":"### 8.2 Testing the model against best hyperparameters ","df174995":"### 7.3.b LR without Class balancing","3613b28b":"## 7.5 Random Forest\n\n### 7.5.a.1 Hyperparamter tuning (with One hot encoding)","c5d72392":"## 7.3 Logistic Regression\n\n### 7.3.a LR with class balancing\n\n#### 7.3.a.1 Hyperparameter tuning","7f85a679":"### 9. Majority Voting classifier","60afa397":"#### 7.1.3 Feature importance check with correctly classified point","04d8aa2b":"#### 7.4.4 Feature importance check with incorrectly classified point","37953743":"#### 7.1.4 Feature importance check with incorrectly classified point","6d195123":"### 6.b.4 Checking significance of Variation feature in predicting ${y}_{i}$ \n\n#### 6.b.4.1 Checking through Logistic Regresion model","effe45cf":"#### 7.5.b.2 Testing the model agianst best hyperparameters using response coding","41ead0ce":"### 7.2 kNN \n\n#### 7.2.1 Hyperparameter tuning ","3073dff1":"### 6.a Univariate Analysis of Gene feature\n\n#### 6.a.1 Number of uniques genes and their distribution","fd96b465":"### 6.b Univariate analysis of Variation feature\n\n#### 6.b.1 Number of uniques variations and their distribution","73b43259":"### 4. Train, validation and test split\n\n#### 4.a Splitting data into a ratio of 64:20:16 for train, validation and test set","75467689":"#### 7.4.3 Feature importance check with correctly classified point","09f23cb0":"## 8. Stacking the models\n\n### 8.1 Testing the model for best hyperparameters ","9d3728c0":"### 6.c Univariate Analysis of Text feature\n\n#### 6.c.1 Number of uniques words and their distribution","25ac553e":"### 3. Text preprocessing\n\n#### 3.a Removing non word characters, whitespaces and stop words","f28db2ec":"#### 7.2.4 Feature importance check with incorrectly classified point","5889cdd5":"### Using all the three features together"}}