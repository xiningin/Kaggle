{"cell_type":{"2b689602":"code","f28ec397":"code","f3bb7500":"code","6745e2a3":"code","4c9dc557":"code","71900d57":"code","02ad5c23":"code","7a5c894e":"code","7a337f86":"code","4fc219c5":"code","cdeff471":"markdown","f2b61f3a":"markdown","a1c31864":"markdown","cab78e2e":"markdown","5a3fb489":"markdown","e7e2954a":"markdown","49fab5da":"markdown","162df7f7":"markdown","875a592b":"markdown","83011b8e":"markdown","388a8156":"markdown","ea32eaa1":"markdown","1b284d38":"markdown"},"source":{"2b689602":"import pandas as pd # \ub370\uc774\ud130 \uc804\ucc98\ub9ac\nimport numpy as np # \ub370\uc774\ud130 \uc804\ucc98\ub9ac\nimport random #\ub370\uc774\ud130 \uc804\ucc98\ub9ac\n\nfrom pandas import DataFrame #\ub370\uc774\ud130 \uc804\ucc98\ub9ac\nfrom collections import Counter #\ub370\uc774\ud130 \uc804\ucc98\ub9ac\n\nimport re\nimport nltk\nnltk.download('words')\nfrom nltk.corpus import words, brown","f28ec397":"word_dictionary = list(set(words.words()))\nfor alphabet in \"bcdefghjklmnopqrstuvwxyz\":\n    word_dictionary.remove(alphabet)\n\ndef split_hashtag_to_words_all_possibilities(hashtag):\n    all_possibilities = []\n\n    split_posibility = [hashtag[:i] in word_dictionary for i in reversed(range(len(hashtag)+1))]\n    possible_split_positions = [i for i, x in enumerate(split_posibility) if x == True]\n\n    for split_pos in possible_split_positions:\n        split_words = []\n        word_1, word_2 = hashtag[:len(hashtag)-split_pos], hashtag[len(hashtag)-split_pos:]\n\n        if word_2 in word_dictionary:\n            split_words.append(word_1)\n            split_words.append(word_2)\n            all_possibilities.append(split_words)\n\n            another_round = split_hashtag_to_words_all_possibilities(word_2)\n\n            if len(another_round) > 0:\n                all_possibilities = all_possibilities + [[a1] + a2 for a1, a2, in zip([word_1]*len(another_round), another_round)]\n        else:\n            another_round = split_hashtag_to_words_all_possibilities(word_2)\n\n            if len(another_round) > 0:\n                all_possibilities = all_possibilities + [[a1] + a2 for a1, a2, in zip([word_1]*len(another_round), another_round)]\n                \n    return all_possibilities","f3bb7500":"def print_3(original):\n    word_space=[]\n    for i in original:\n        if len(i)<=3:\n            word_space.append(i)\n    return word_space","6745e2a3":"def print_er(original_word):\n    word_space2=[]\n    for j in original_word:\n        if (len(j)==3) & ( len(j[-1])<=3 ):\n            temp=[]\n            temp.append( j[0] )\n            temp.append( j[1]+j[2])\n            word_space2.append(temp)\n        else:\n            word_space2.append(j)\n    word_space=[]\n    for i in word_space2:\n        if 'er' in i:\n            pass\n        else:\n            word_space.append(i)\n    return [list(t) for t in set(tuple(element) for element in word_space)]\n\ndef print_ing(original_word):\n    q=p = re.compile('ing$')\n    word_space2=[]\n    for j in original_word:\n        if (len(j)==3) & ( len(j[-1])<=4 ):\n            temp=[]\n            temp.append( j[0] )\n            temp.append( j[1]+j[2])\n            word_space2.append(temp)\n        elif (len(j)==3) & ( q.findall(j[-2])==['ing'] ):\n            temp=[]\n            temp.append( j[0]+j[1] )\n            temp.append( j[2])\n            word_space2.append(temp)\n        else:\n            word_space2.append(j)\n    word_space=[]\n    for i in word_space2:\n        if 'ing' in i:\n            pass\n        else:\n            word_space.append(i)\n    return [list(t) for t in set(tuple(element) for element in word_space)]\n\ndef print_ed(original_word):\n    word_space2=[]\n    for j in original_word:\n        if (len(j)==3) & ( len(j[-1])<=3 ):\n            temp=[]\n            temp.append( j[0] )\n            temp.append( j[1]+j[2])\n            word_space2.append(temp)\n        else:\n            word_space2.append(j)\n    word_space=[]\n    for i in word_space2:\n        if 'ed' in i:\n            pass\n        else:\n            word_space.append(i)\n    return [list(t) for t in set(tuple(element) for element in word_space)]","4c9dc557":"def print_man(original_word):\n    raw=''.join(original_word[0])\n    original_word.append( [ raw[:-3],raw[-3:] ]  )\n\n    return [list(t) for t in set(tuple(element) for element in original_word)]\n\ndef print_wm(original_word):\n    raw=''.join(original_word[0])\n    original_word.append( [ raw[:-5],raw[-5:] ]  )\n\n    return [list(t) for t in set(tuple(element) for element in original_word)]","71900d57":"count_v = pd.read_csv('..\/input\/english-word-frequency\/unigram_freq.csv')\ncount_v['type'] = [type(i) for i in count_v['word']]\ncount_v2=count_v[count_v['type']==str]\ncount_v2['len'] = [len(i) for i in count_v2['word']]\n\ncount_v2=count_v2[count_v2['len']>=2]","02ad5c23":"count_v2.head(10)","7a5c894e":"remove_list=['to','in','by','go','of','in','on','as','the','and','up']\ndef regulatoin_list(next_word):\n    word_space=[]\n    if next_word==[]:\n        return [next_word]\n    else:\n        for list1 in next_word:\n            word_space2 = [len(i) for i in list1]\n            if (1 in word_space2) :  #\uae38\uc774\uac00 \ud558\ub098\ub77c\ub3c4 1\uc778 \uacbd\uc6b0\n                pass\n            elif (word_space2.count(2)==2) :  # 2\uac1c \ub2e8\uc5b4\uc758 \uae38\uc774\uac00 2\uc778 \uacbd\uc6b0\ub294 \ube44\uc815\uc0c1\uc801\uc774\ubbc0\ub85c \uc81c\uc678\n                pass\n            else:\n                word_space.append(list1)\n\n        if len(word_space)>=2:\n            sum_list=[]\n            real_list=[]\n            for splitting in word_space:\n                if len(splitting)==2:\n                    if ( (len(splitting[-1])==2) & ( splitting[-1] not in remove_list ) ) | ( ( len(splitting[-2])==2 ) & ( splitting[-2] not in remove_list ) ) :\n                        pass\n                    else:\n                        real_list.append(splitting)\n                else:\n                    if ( len(splitting[-1])==2 ) | ( ( len(splitting[-2])==2 ) & ( splitting[-2] not in remove_list ) ) | ( ( len(splitting[-3])==2 ) & ( splitting[-3] not in remove_list ) ) :\n                        pass\n                    else:\n                        real_list.append(splitting)\n\n            for j in real_list:\n                sum1 = 1\n                for y in range(len(j)):\n                    try:\n                        sum1 += count_v[count_v['word']==j[y]].index[0]\n                    except:\n                        sum1 += 99999999\n                sum_list.append(sum1)\n            return real_list[ sum_list.index(min(sum_list)) ]\n            \n        elif len(word_space)==0:\n            return []\n\n        else:\n            return word_space[0]","7a337f86":"def word_space(j):\n    p = re.compile('er$')\n    \n    if p.findall(j)==['er']:\n        try:\n            return regulatoin_list( print_er( print_3( split_hashtag_to_words_all_possibilities(j) ) ) ) \n        except:\n            return [j]\n\n    elif j.find(\"ing\")>(-1):\n        try:\n            return regulatoin_list( print_ing( print_3( split_hashtag_to_words_all_possibilities(j) ) ) )\n        except:\n            return [j]\n\n    elif j[-5:]==\"woman\":\n        try:\n            return regulatoin_list( print_wm( print_3( split_hashtag_to_words_all_possibilities(j) ) ) )\n        except:\n            return [j]\n\n    elif (j[-3:]==\"man\") &  (j[-5:]!=\"woman\"):\n        try:\n            return regulatoin_list( print_man( print_3( split_hashtag_to_words_all_possibilities(j) ) ) )\n        except:\n            return [j]\n    elif j[-2:]==\"ed\" :\n        try:\n            return regulatoin_list( print_ed( print_3( split_hashtag_to_words_all_possibilities(j) ) ) )\n        except:\n            return [j]\n    \n    else:\n        try:\n            return regulatoin_list( print_3( split_hashtag_to_words_all_possibilities( j ) ) )\n        except:\n            return [j]","4fc219c5":"print( word_space('snowman') )\n\nprint( word_space('longwinded') )\n\nprint( word_space('hashtagsplit') )\n\nprint( word_space('strawberry') )\n\nprint( word_space('strawberrycake') )\n\nprint( word_space('blueberrycake') )\n\nprint( word_space('watermelonsugar'))\n\nprint( word_space('watermelonsugarsalt'))\n\nprint( word_space('themselves'))","cdeff471":"# 1. \uc0ac\uc804 \uc124\uba85 \ubc0f \ud328\ud0a4\uc9c0 import\n\n\ud574\ub2f9 notebook\uc758 \uacbd\uc6b0, \ubcf5\ud569\uc801\uc778 \uc601\uc5b4 \ub2e8\uc5b4\ub97c \ub2e8\uc21c\ud55c \ub2e8\uc5b4\ub85c \ubd84\ub9ac\ub97c \uc2dc\ud0a4\ub294 \uacfc\uc81c\uc5d0 \ub300\ud55c \ud480\uc774\uc785\ub2c8\ub2e4.\n\n\ub370\uc774\ud130\ub294 \ube44\uacf5\uac1c\uc774\uae30 \ub54c\ubb38\uc5d0, \uc608\uc2dc\uc758 \ub2e8\uc5b4\ub4e4\ub85c\ub9cc \uacb0\uacfc\ub97c \ubcf4\uc5ec\ub4dc\ub9ac\uba70,\n\n\uc2e4\uc81c \ub370\uc774\ud130\ub97c \uac00\uc9c0\uace0 \ud55c \uacb0\uacfc\uc5d0\uc11c\ub294 \ud574\ub2f9 \uc54c\uace0\ub9ac\uc998\ub9cc \uac00\uc9c0\uace0\ub294 500\uac1c\uc758 \ub2e8\uc5b4\ub97c \uac00\uc9c0\uace0\n\n\uc57d 97%\uc758 \uc815\ud655\ub3c4\ub97c \uac00\uc84c\uc5c8\uc2b5\ub2c8\ub2e4.\n\n\ud604\uc7ac\ub294 \ub178\ud2b8\ubd81\uc5d0\uc11c \ud568\uc218\ub85c\ub9cc \uc2e4\ud589\uc774 \uac00\ub2a5\ud558\uc9c0\ub9cc, \ub098\uc911\uc5d0\ub294 class\ub85c \uad6c\ud604\ud558\uc5ec\uc11c\n\n\uac04\ud3b8\ud558\uac8c \ubd88\ub7ec\uc624\uace0, cmd \ucc3d\uc5d0\uc11c \ubc14\ub85c \uc2e4\ud589\uc774 \uac00\ub2a5\ud558\ub3c4\ub85d \uc54c\uace0\ub9ac\uc998\uc744 \uad6c\ud604\ud558\ub294 \uac83\uc744 \uc0dd\uac01\uc740 \ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.","f2b61f3a":"\ub098\uc911\uc5d0\ub294 man, woman\uc774\ub77c\ub294 \ub2e8\uc5b4\ub85c \ub05d\ub098\ub294 \uacbd\uc6b0\uc5d0 \ub300\ud55c \uad6c\ubd84\ub3c4 \ucd94\uac00\ud558\uc600\uc2b5\ub2c8\ub2e4.","a1c31864":"a, b, c \uc640 \uac19\uc740 \ud55c \uae00\uc790 \ub2e8\uc5b4\ub294 \uc81c\uc678\ud558\uace0 \uc0ac\uc6a9\ud558\uba70,\n\nto, in, by \ub4f1\uacfc \uac19\uc740 \uc804\uce58\uc0ac, \uc811\uc18d\uc0ac \ub4f1\uc758 \ub2e8\uc5b4\ub4e4\uc740 remove_list\ub97c \ub9cc\ub4e4\uc5b4\uc11c \uc81c\uc678\ud558\ub294 \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4.","cab78e2e":"# 4. English Word Frequency \ud65c\uc6a9\n\n\ud558\uc9c0\ub9cc, \uadf8\ub807\uac8c \ud558\ub354\ub77c\ub3c4 \ucd5c\uc801\uc758 \uacbd\uc6b0\uac00 \ub098\uc624\uc9c0 \uc54a\ub294 \uacbd\uc6b0\uac00 \uc788\uc5c8\ub294\ub370\n\n\uac00\uc7a5 \ud070 \uac83\uc740 3\uac1c\uc758 \ub2e8\uc5b4\ub85c \uad6c\ubd84\uc744 \ud560 \uacbd\uc6b0, \uacbd\uc6b0\uc758 \uc218\uac00 2\uac00\uc9c0\uac00 \ub098\uc624\ub294 \uac83\uc774\uc5c8\uc2b5\ub2c8\ub2e4.\n\n\uc608\ub97c\ub4e4\uc5b4 blueberrycake\ub77c\ub294 \ub2e8\uc5b4\uc758 \uacbd\uc6b0,\n\nblue berry cake, blueberry cake\ub85c \ub098\ub260 \uc218\uac00 \uc788\ub294\ub370\n\n\uc5b4\ub5a0\ud55c \uac83\uc774 \ucd5c\uc801\uc758 \uacbd\uc6b0\uc778\uc9c0 \ucc3e\ub294 \uc694\uc18c\ub85c https:\/\/www.kaggle.com\/rtatman\/english-word-frequency \uc758 \ub2e8\uc5b4 \ube48\ub3c4 \ub370\uc774\ud130\ub97c \ud65c\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4.","5a3fb489":"# 5. word_frequecy \uc544\uc774\ub514\uc5b4 \uc801\uc6a9\n\n\ud574\ub2f9 \ud568\uc218\ub4e4\uc744 \ub9cc\ub4e0 \ub2e4\uc74c\uc5d0, \ub05d\uc5d0 \ub05d\ub098\ub294 \uae00\uc790\ub4e4\uc744\uc744 \ud1b5\ud574\uc11c \uac01\uac01 \ub2e4\ub978 \ud568\uc218\uac00 \uc801\uc6a9\uc774 \ub418\ub3c4\ub85d \ud558\uc600\uc2b5\ub2c8\ub2e4.\n\n\uc911\uac04\uc5d0 \ub0b4\uc6a9\uc744 \ucd94\uac00\ud558\ub2e4\ubcf4\ub2c8 \ubd88\ud544\uc694\ud558\uac8c if, elif \ub4f1\uc73c\ub85c \ubc18\ubcf5\ub418\ub294 \ub0b4\uc6a9\uc774 \ub4e4\uc5b4\uac00\uae34 \ud588\uc9c0\ub9cc\n\n3\uc77c\uc548\uc5d0 \ud574\ub2f9 \uc54c\uace0\ub9ac\uc998\uc744 \ucc98\ub9ac\ub97c \ud558\uc5ec\uc57c \ub42c\uae30 \ub54c\ubb38\uc5d0 \uad6c\ud604\uc5d0\ub9cc \ucd08\uc810\uc744 \ub450\uc5c8\uc2b5\ub2c8\ub2e4.","e7e2954a":"# 2. \ud575\uc2ec \uc54c\uace0\ub9ac\uc998\n\n\uc778\ud130\ub137\uc5d0\uc11c \ucc3e\uc740 hashtag splitter\uc758 \ud568\uc218\uc785\ub2c8\ub2e4.","49fab5da":"\uadf8\ub807\uae30 \ub54c\ubb38\uc5d0, \ucd5c\uc801\uc758 \uacbd\uc6b0\uc758 \uc218\ub97c \ucc3e\ub294 \uac83\uc744 \uc0dd\uac01\ud558\uc600\uace0,\n\n\uac00\uc7a5 \uc88b\uc740 \uacbd\uc6b0\ub294 \ub2e8\uc5b4\uac00 \ucd5c\ub300 3\uac1c\ub85c \ub098\ub204\uc5b4\uc9c0\uace0, \ub9c8\uc9c0\ub9c9 \uae00\uc790\uc5d0 \ub530\ub77c\uc11c \ud328\ud134\ub9cc \uc870\uc815\ud558\ub294 \ubc29\uc2dd\uc744 \uc120\ud0dd\ud558\uc600\uc2b5\ub2c8\ub2e4.","162df7f7":"\ud574\ub2f9 \uc54c\uace0\ub9ac\uc998\uc758 \ud575\uc2ec\uc740, \uc774\ubbf8 \ub2e8\uc5b4\uac00 \ubd84\ub9ac\uac00 \ub41c \uc0c1\ud0dc\uc5d0\uc11c\n\nremove_list\uc5d0 \uc788\ub294 \ub2e8\uc5b4\ub4e4\ub85c\ub294 \ubd84\ub9ac\uac00 \ub418\uc9c0 \uc54a\uc73c\uba74\uc11c, word_frequecy \ub370\uc774\ud130\uc758 \ube48\ub3c4\ub97c \ud65c\uc6a9\ud558\uc5ec\uc11c\n\n\ucd5c\ub300\ud55c \uac00\uc7a5 \uc798 \uc0ac\uc6a9\uc774 \ub418\ub294 \ub2e8\uc5b4\uac00 \ucd5c\uc801\uc758 \uacbd\uc6b0\uc758 \uc218\uac00 \ub418\ub3c4\ub85d \ubd84\ub9ac\ub97c \ud558\ub294 \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud55c \uac83\uc785\ub2c8\ub2e4.","875a592b":"\ucd5c\ub300 3\uac1c\ub9cc \uc120\ud0dd\ud558\uac8c \ud55c \uc774\ud6c4\uc5d0\ub294, er, ing, ed\ub85c \ub05d\ub098\ub294 \ub2e8\uc5b4\uc5d0 \ub300\ud55c \uad6c\ubd84\uc744 \ucd94\uac00\ud558\uc600\uace0","83011b8e":"# 0. Explanation\n\ngithub version :  https:\/\/github.com\/bluemumin\/word_frequency_with_hashtag_split\n\n\uc774 \ud1a0\uc774 \ud504\ub85c\uc81d\ud2b8\ub294, 6\uac1c\uc6d4 \uc804\uc5d0 \uc784\uc2dc\uc801\uc73c\ub85c \uacfc\uc81c\ub97c \ubd80\uc5ec\ubc1b\uc544 \uc218\ud589\ud55c \ub0b4\uc6a9\uc785\ub2c8\ub2e4.\n\n\ud574\ub2f9 \uac8c\uc2dc\ubb3c\uc744 \uac8c\uc2dc\ud558\ub294 \uc774\uc720\ub294, \ud574\ub2f9 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec\uc11c \uc601\ub2e8\uc5b4 \ubd84\ub9ac\ub97c \uc218\ud589\ud558\uc600\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.\n\n\uc6d0\ub798\ub294 \ubcf5\ud569 \ub2e8\uc5b4 csv \ud30c\uc77c\uc744 \uc774\uc6a9\ud558\uc5ec\uc11c \uacb0\uacfc\ub97c \ub3c4\ucd9c\ud558\uc600\uc9c0\ub9cc, \n\n\ud574\ub2f9 \ub370\uc774\ud130\ub294 \uc0ac\uc815\uc0c1 \uac8c\uc2dc\uac00 \ubd88\uac00\ub2a5\ud558\uc5ec \ub9c8\uc9c0\ub9c9\uc5d0 \uc608\uc2dc\ub9cc \ucca8\ubd80\ud569\ub2c8\ub2e4.\n\n\uc81c\uac00 \ud55c\uad6d\uc778\uc774\ub77c, \ub098\uba38\uc9c0 \ub0b4\uc6a9\uc758 \uc8fc\uc11d\uc740 \ubc88\uc5ed\uae30\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc8fc\uc2dc\uba74 \uac10\uc0ac\ub4dc\ub9ac\uaca0\uc2b5\ub2c8\ub2e4.\n\n\ucc38\uace0\ub85c, \ud574\ub2f9 \ud568\uc218\ub294 \ucd5c\uc801\uc758 \uacbd\uc6b0\uac00 \ub2e8\uc77c \ub2e8\uc5b4\uc5ec\ub3c4 \ubd84\ub9ac\uac00 \uc9c4\ud589\uc774 \ub418\uba70, \uacbd\uc6b0\uc758 \uc218\uac00 \uc5c6\ub294 \uacbd\uc6b0\uc5d0\ub294 \ubd84\ub9ac\uac00 \uc9c4\ud589\ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\n\n( [[]]\ub77c\ub294 \uacb0\uacfc\ub85c \ubc18\ud658\uc774 \ub429\ub2c8\ub2e4.)\n\nThis toy project was carried out by receiving a temporary assignment six months ago.\n\nThe reason for posting this post is that we used the data to separate English words.\n\nOriginally, the results were derived using a compound word csv file, \n\nbut the data cannot be posted due to reasons, so only examples are attached at the end.\n\nSince I am Korean, please use a translator for Korean comments in the article.\n\nFor reference, even if the optimal case is a single word, separation proceeds, and if there are no cases, separation does not proceed.\n\n( It is returned as a result of [[]]. )","388a8156":"# 3. \ub2e8\uc5b4 \ud328\ud134 \uad6c\ubd84\n\n\ud558\uc9c0\ub9cc \uc774\ub97c \uc774\ub300\ub85c \uc0ac\uc6a9\ud558\uae30\uc5d0\ub294 \ubb34\ub9ac \uc600\ub358 \uc810\uc774,\n\n\ud574\ub2f9 \ud568\uc218\ub294 \ubaa8\ub4e0 \uacbd\uc6b0\uc758 \uc218\ub97c \ubc18\ud658\ud574\uc8fc\uae30 \ub54c\ubb38\uc5d0, \ub9e4\ubc88 \uc0ac\uc6a9\uc790\uac00 \ud655\uc778\ub3c4 \ud574\uc918\uc57c\ub418\uace0 \ub9cc\uc57d\uc5d0 \ub2e8\uc5b4\uc758 \uac2f\uc218\uac00 \uc5c4\uccad\ub098\uac8c \ub9ce\ub2e4\uba74\n\n\uc774\ub97c \uc81c\ub300\ub85c \ud65c\uc6a9\ud558\uc9c0 \ubabb\ud558\uac8c \ub429\ub2c8\ub2e4.","ea32eaa1":"# 6. \uacb0\uacfc \ud655\uc778","1b284d38":"\uc77c\ub2e8 \ub2e8\uc5b4\uac00 \ub4e4\uc5b4\uac00\uba74 \ubb34\uc870\uac74 \ubd84\ub9ac\uac00 \uc77c\uc5b4\ub098\ub3c4\ub85d \uc9c4\ud589\uc774 \ub418\uc5c8\uc73c\uba70,\n\n\ub2e8\uc5b4 4\uac1c \uc774\uc0c1\uc73c\ub85c \ubd84\ub9ac\ub418\ub294 \uacbd\uc6b0\ub294 \uc5b4\uca54\uc218 \uc5c6\uc774 \ucd5c\ub300 3\uac1c\uac00 \ub418\ub3c4\ub85d \ucc98\ub9ac\ub97c \ud558\uc600\uc73c\uba70\n\n\ubd84\ub9ac\uac00 \ubd88\uac00\ub2a5\ud55c, \uc989 \uc544\uc608 \uacbd\uc6b0\uc758 \uc218\uac00 \uc5c6\ub294 \uacbd\uc6b0\uc5d0\ub294 \ube48 \uac12\uc774 \ub098\uc624\ub3c4\ub85d \ub418\uc5b4\uc788\uc2b5\ub2c8\ub2e4.\n\n\ube48 \uac12\uc774 \ub098\uc624\uac8c \ud55c \uc774\uc720\ub294 \ud574\ub2f9 \ub2e8\uc5b4\ub294 \uc9c1\uc811 \ucc98\ub9ac\ub97c \ud574\uc57c\ub428\uc744 \uc758\ubbf8\ud558\uba70, \n\n\uc774\ub294 \ud568\uc218 \uc801\uc6a9\uc2dc [[]]\uac00 \ub098\uc624\uba74 \uc6d0\ubcf8\uc744 \ubc18\ud658\ud558\ub3c4\ub85d \ud558\ub294 \uac83\uc744 \uad6c\ud604\ud558\uba74\n\n\uac04\ub2e8\ud558\uac8c \ud574\uacb0\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4."}}