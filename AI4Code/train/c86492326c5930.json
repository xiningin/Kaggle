{"cell_type":{"a229ac27":"code","31f2c686":"code","f988bea4":"code","fa5246eb":"code","0d3fddc8":"code","b36778f5":"code","6e8dd970":"code","9b093b0f":"code","6d5a423e":"code","4437d101":"code","eb0e7bcf":"code","cbcb9f2e":"code","cbaad9d8":"code","be131096":"code","ecdbb92b":"code","31202364":"code","713a78a2":"code","adf7960d":"code","e59d4a06":"code","5c9f09c7":"code","b8ee1f44":"code","4c52ca6e":"code","49f8d163":"code","47ae7889":"code","8c9ad2b0":"code","bc4c2f99":"code","97f523c5":"code","f1fb99a2":"code","8b522624":"code","ff4e6723":"markdown","577de970":"markdown","f59f82f9":"markdown","d0290eb2":"markdown","731b08f4":"markdown","a27ab4bb":"markdown","a1865ade":"markdown","7494920f":"markdown","bf01269a":"markdown","bd730192":"markdown","d4bc6094":"markdown","89034b1c":"markdown","06b2f2c5":"markdown","226ad36d":"markdown"},"source":{"a229ac27":"!pip install -q efficientnet_pytorch","31f2c686":"import time\nimport random\nimport datetime\nimport os\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import model_selection\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport efficientnet_pytorch\n\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport matplotlib.pyplot as plt","f988bea4":"SEED = 42","fa5246eb":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)","0d3fddc8":"class DataLoaderConfig:\n    batch_size = 64\n    num_workers = 8\n\n\nclass TrainConfig:\n    criterion = nn.CrossEntropyLoss \n    n_epochs = 10\n    lr = 0.001\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau\n    scheduler_params = dict(\n        mode='min',\n        factor=0.5,\n        patience=1,\n        verbose=False, \n        threshold=0.0001,\n        threshold_mode='abs',\n        cooldown=0, \n        min_lr=1e-8,\n        eps=1e-08\n    )\n    \n\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","b36778f5":"df = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\nprint(df.shape)\ndf.head()","6e8dd970":"y = df['label'].values\nX = df.drop(['label'], axis=1).values","9b093b0f":"X_train, X_valid, y_train, y_valid = model_selection.train_test_split(X, y, test_size=0.2)\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape","6d5a423e":"class DatasetRetriever(Dataset):\n    def __init__(self, X, y, transforms=None):\n        super().__init__()\n        self.X = X.reshape(-1, 28, 28).astype(np.float32)\n        self.y = y\n        self.transforms = transforms\n\n    def __getitem__(self, index):\n        image, target = self.X[index], self.y[index]\n        image = np.stack([image] * 3, axis=-1)\n        image \/= 255.\n        if self.transforms:\n            image = self.transforms(image=image)['image']\n            \n        return image, torch.tensor(target, dtype=torch.long)\n\n    def __len__(self):\n        return self.y.shape[0]","4437d101":"def get_train_transforms():\n    return A.Compose(\n        [\n            A.Rotate(limit=10, border_mode=cv2.BORDER_REPLICATE, p=0.5),\n            A.Cutout(num_holes=8, max_h_size=2, max_w_size=2, fill_value=0, p=0.5),\n            A.Cutout(num_holes=8, max_h_size=1, max_w_size=1, fill_value=1, p=0.5),\n            A.Resize(32, 32, p=1.),\n            ToTensorV2(p=1.0),\n        ], \n        p=1.0)\n\ndef get_valid_transforms():\n    return A.Compose(\n        [\n            A.Resize(32, 32, p=1.),\n            ToTensorV2(p=1.0),\n        ], \n        p=1.0\n    )","eb0e7bcf":"train_dataset = DatasetRetriever(\n    X = X_train,\n    y = y_train,\n    transforms=get_train_transforms(),\n)\n\nvalid_dataset = DatasetRetriever(\n    X = X_valid,\n    y = y_valid,\n    transforms=get_valid_transforms(),\n)","cbcb9f2e":"plt.figure(figsize=(16, 6))\n\nfor i in range(10):    \n    image, target = train_dataset[random.randint(0, len(train_dataset))]\n    numpy_image = image.permute(1, 2, 0).cpu().numpy()\n\n    plt.subplot(2, 5, i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.title(target.cpu().numpy(), fontsize=15)\n    plt.imshow(numpy_image);","cbaad9d8":"train_loader = DataLoader(\n    train_dataset,\n    batch_size=DataLoaderConfig.batch_size,\n    shuffle=True,\n    num_workers=DataLoaderConfig.num_workers,\n)\n\nvalid_loader = DataLoader(\n    valid_dataset, \n    batch_size=DataLoaderConfig.batch_size,\n    shuffle=False,\n    num_workers=DataLoaderConfig.num_workers,\n)","be131096":"class LossMeter:\n    def __init__(self):\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n        \nclass AccMeter:\n    def __init__(self):\n        self.true_count = 0\n        self.all_count = 0\n        self.avg = 0\n        \n    def update(self, y_true, y_pred):\n        y_true = y_true.cpu().numpy().astype(int)\n        y_pred = y_pred.cpu().numpy().argmax(axis=1).astype(int)\n        self.true_count += (y_true == y_pred).sum()\n        self.all_count += y_true.shape[0]\n        self.avg = self.true_count \/ self.all_count","ecdbb92b":"class Fitter:\n    def __init__(\n        self, model, device, criterion, n_epochs, \n        lr, sheduler=None, scheduler_params=None\n    ):\n        self.epoch = 0\n        self.n_epochs = n_epochs\n        self.base_dir = '.\/'\n        self.log_path = f'{self.base_dir}\/log.txt'\n        self.best_summary_loss = np.inf\n\n        self.model = model\n        self.device = device\n\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n        \n        if sheduler:\n            self.scheduler = sheduler(self.optimizer, **scheduler_params)\n            \n        self.criterion = criterion().to(self.device)\n        \n        self.log(f'Fitter prepared. Device is {self.device}')\n\n    def fit(self, train_loader, valid_loader):\n        for e in range(self.n_epochs):\n            current_lr = self.optimizer.param_groups[0]['lr']\n            self.log(f'\\n{datetime.datetime.utcnow().isoformat()}\\nLR: {current_lr}')\n\n            t = int(time.time())\n            summary_loss, final_scores = self.train_one_epoch(train_loader)\n            self.log(\n                f'[RESULT]: Train. Epoch: {self.epoch}, ' + \\\n                f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n                f'final_score: {final_scores.avg:.5f}, ' + \\\n                f'time: {int(time.time()) - t} s'\n            )\n\n            t = int(time.time())\n            summary_loss, final_scores = self.validation(valid_loader)\n            self.log(\n                f'[RESULT]: Valid. Epoch: {self.epoch}, ' + \\\n                f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n                f'final_score: {final_scores.avg:.5f}, ' + \\\n                f'time: {int(time.time()) - t} s'\n            )\n            \n            f_best = 0\n            if summary_loss.avg < self.best_summary_loss:\n                self.best_summary_loss = summary_loss.avg\n                f_best = 1\n\n            \n            self.scheduler.step(metrics=summary_loss.avg)\n                \n            self.save(f'{self.base_dir}\/last-checkpoint.bin')\n            \n            if f_best:\n                self.save(f'{self.base_dir}\/best-checkpoint.bin')\n                print('New best checkpoint')\n\n            self.epoch += 1\n\n    def validation(self, val_loader):\n        self.model.eval()\n        summary_loss = LossMeter()\n        final_scores = AccMeter()\n        \n        t = int(time.time())\n        for step, (images, targets) in enumerate(val_loader):\n            print(\n                f'Valid Step {step}\/{len(val_loader)}, ' + \\\n                f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n                f'final_score: {final_scores.avg:.5f}, ' + \\\n                f'time: {int(time.time()) - t} s', end='\\r'\n            )\n            \n            with torch.no_grad():\n                targets = targets.to(self.device)\n                images = images.to(self.device)\n                batch_size = images.shape[0]\n                \n                outputs = self.model(images)\n                loss = self.criterion(outputs, targets)\n                \n                final_scores.update(targets, outputs)\n                summary_loss.update(loss.detach().item(), batch_size)\n\n        return summary_loss, final_scores\n\n    def train_one_epoch(self, train_loader):\n        self.model.train()\n        summary_loss = LossMeter()\n        final_scores = AccMeter()\n        \n        t = int(time.time())\n        for step, (images, targets) in enumerate(train_loader):\n            print(\n                f'Train Step {step}\/{len(train_loader)}, ' + \\\n                f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n                f'final_score: {final_scores.avg:.5f}, ' + \\\n                f'time: {int(time.time()) - t} s', end='\\r'\n            )\n            \n            targets = targets.to(self.device)\n            images = images.to(self.device)\n            batch_size = images.shape[0]\n\n            self.optimizer.zero_grad()\n            outputs = self.model(images)\n            \n            loss = self.criterion(outputs, targets)\n            loss.backward()\n\n            final_scores.update(targets, outputs.detach())\n            summary_loss.update(loss.detach().item(), batch_size)\n            \n            self.optimizer.step()\n\n        return summary_loss, final_scores\n    \n    def save(self, path):\n        self.model.eval()\n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'best_summary_loss': self.best_summary_loss,\n            'epoch': self.epoch,\n        }, path)\n\n    def load(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        self.best_summary_loss = checkpoint['best_summary_loss']\n        self.epoch = checkpoint['epoch'] + 1\n        \n    def log(self, message):\n        print(message)\n        with open(self.log_path, 'a+') as logger:\n            logger.write(f'{message}\\n')","31202364":"def get_net():\n    net = efficientnet_pytorch.EfficientNet.from_pretrained('efficientnet-b7')\n    net._fc = nn.Linear(in_features=2560, out_features=10, bias=True)\n    return net\n\nnet = get_net().to(DEVICE)","713a78a2":"fitter = Fitter(\n    model=net, \n    device=DEVICE, \n    criterion=TrainConfig.criterion, \n    n_epochs=TrainConfig.n_epochs, \n    lr=TrainConfig.lr, \n    sheduler=TrainConfig.scheduler, \n    scheduler_params=TrainConfig.scheduler_params\n)","adf7960d":"fitter.fit(train_loader, valid_loader)","e59d4a06":"checkpoint = torch.load('..\/working\/best-checkpoint.bin')\nnet.load_state_dict(checkpoint['model_state_dict']);\nnet.eval();","5c9f09c7":"df = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\nprint(df.shape)\ndf.head()","b8ee1f44":"X = df.values","4c52ca6e":"class DatasetRetriever(Dataset):\n    def __init__(self, X, transforms=None):\n        super().__init__()\n        self.X = X.reshape(-1, 28, 28).astype(np.float32)\n        self.transforms = transforms\n\n    def __getitem__(self, index):\n        image = self.X[index]\n        image = np.stack([image] * 3, axis=-1)\n        image \/= 255.\n        if self.transforms:\n            image = self.transforms(image=image)['image']\n            \n        return image\n\n    def __len__(self):\n        return self.X.shape[0]","49f8d163":"test_dataset = DatasetRetriever(\n    X = X,\n    transforms=get_valid_transforms(),\n)","47ae7889":"test_loader = DataLoader(\n    test_dataset, \n    batch_size=DataLoaderConfig.batch_size,\n    shuffle=False,\n    num_workers=DataLoaderConfig.num_workers\n)","8c9ad2b0":"result = []\nfor step, images in enumerate(test_loader):\n    print(step, end='\\r')\n    \n    y_pred = net(images.to(DEVICE)).detach().cpu().numpy().argmax(axis=1).astype(int)\n    \n    result.extend(y_pred)","bc4c2f99":"plt.figure(figsize=(16, 6))\n\nfor i in range(10):    \n    image = test_dataset[i]\n    numpy_image = image.permute(1, 2, 0).cpu().numpy()\n\n    plt.subplot(2, 5, i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.title(f'Predict: {result[i]}', fontsize=15)\n    plt.imshow(numpy_image);","97f523c5":"sub = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv', index_col=0)\nsub.head()","f1fb99a2":"sub['Label'] = result","8b522624":"sub.to_csv('submission.csv', index=True)","ff4e6723":"### 4.2 Train function <a class=\"anchor\" id=\"4.2\"><\/a>","577de970":"## 1. Configuration <a class=\"anchor\" id=\"1\"><\/a>","f59f82f9":"## 2. Split data <a class=\"anchor\" id=\"2\"><\/a>","d0290eb2":"### 3.4 Create train and valid dataloaders <a class=\"anchor\" id=\"3.4\"><\/a>","731b08f4":"## 3. Create DataLoader <a class=\"anchor\" id=\"3\"><\/a>","a27ab4bb":"### 4.4 Train model <a class=\"anchor\" id=\"4.4\"><\/a>","a1865ade":"### 4.1 Initialize loss and accuracy classes <a class=\"anchor\" id=\"4.1\"><\/a>","7494920f":"## 4. Build Model <a class=\"anchor\" id=\"4\"><\/a>","bf01269a":"### 3.2 Initialize Augmentations <a class=\"anchor\" id=\"3.2\"><\/a>","bd730192":"## 5. Inference Model <a class=\"anchor\" id=\"5\"><\/a>","d4bc6094":"1. [Configuration](#1)\n2. [Split data](#2)\n3. [Create DataLoader](#3)\n    - [Initialize Dataset](#3.1)\n    - [Initialize Augmentations](#3.2)\n    - [Create train and valid datasets](#3.3)\n    - [Create train and valid dataloaders](#3.4)\n4. [Build Model](#4)\n    - [Initialize loss and accuracy classes](#4.1)\n    - [Train function](#4.2)\n    - [Load MobileNetV2](#4.3)\n    - [Train model](#4.4)\n5. [Inference Model](#5)","89034b1c":"### 4.3 Load Model <a class=\"anchor\" id=\"4.3\"><\/a>","06b2f2c5":"### 3.1 Initialize Dataset <a class=\"anchor\" id=\"3.1\"><\/a>","226ad36d":"### 3.3 Create train and valid datasets <a class=\"anchor\" id=\"3.3\"><\/a>"}}