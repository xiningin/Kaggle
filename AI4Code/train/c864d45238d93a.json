{"cell_type":{"98c9ce87":"code","5e4f0950":"code","9ca85dce":"code","cc33434d":"code","d4c44b12":"code","61074253":"code","dfeaca87":"code","03081753":"code","967e2e70":"code","d8f8fe98":"code","6774fb96":"code","f269ea47":"markdown","20fe7b22":"markdown","eafbc3dd":"markdown","8f274d4a":"markdown","2a308703":"markdown","8dfc3beb":"markdown","02ccf84e":"markdown","aa79fedc":"markdown","883af065":"markdown","5e98d193":"markdown","4915df93":"markdown"},"source":{"98c9ce87":"## import libraries \nfrom collections import Counter \nimport pandas as pd \nimport numpy as np \nimport string \n\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nimport plotly.plotly as py\nfrom plotly import tools\nimport seaborn as sns\ninit_notebook_mode(connected=True)\nfrom itertools import zip_longest\nimport string \nimport re\n\nfrom nltk.corpus import stopwords \nfrom nltk.util import ngrams\nimport nltk \nstopwords = stopwords.words('english')\n\n## dataset preparation\nmessages = pd.read_csv(\"..\/input\/ForumMessages.csv\")\nmessages['CreationDate'] = pd.to_datetime(messages['PostDate'])\nmessages['CreationYear'] = messages['CreationDate'].dt.year\nmessages['CreationMonth'] = messages['CreationDate'].dt.month\nmessages['CreationMonth'] = messages['CreationMonth'].apply(lambda x : \"0\"+str(x) if len(str(x)) < 2 else x)\nmessages['CreationDay'] = \"29\"\nmessages['KernelDate'] = messages[\"CreationYear\"].astype(str) +\"-\"+ messages[\"CreationMonth\"].astype(str) +\"-\"+ messages[\"CreationDay\"].astype(str)\nmessages['Message'] = messages['Message'].fillna(\" \")\n\n## function to remove html entities from text\ndef striphtml(data):\n    p = re.compile(r'<.*?>')\n    return p.sub('', data)\n\n## function to clean a text\ndef clntxt(text):\n    text = text.lower()\n    text = striphtml(text)\n    text = \" \".join([c for c in text.split() if c not in stopwords])\n    for c in string.punctuation:\n        text = text.replace(c, \" \")\n    text = \" \".join([c for c in text.split() if c not in stopwords])\n    \n    words = []\n    ignorewords = [\"&nbsp;\", \"quot\", \"quote\", \"www\", \"http\", \"com\"]\n    for wrd in text.split():\n        if len(wrd) <= 2: \n            continue\n        if wrd in ignorewords:\n            continue\n        words.append(wrd)\n    text = \" \".join(words)    \n    return text\n\n## function to get top ngrams for a given year\ndef get_top_ngrams(yr, n, limit):\n    # get relevant text\n    temp = messages[messages['CreationYear'] == yr]\n    text = \" \".join(temp['Message']).lower()\n    \n    # cleaning\n    text = striphtml(text)\n    text = \" \".join([c for c in text.split() if c not in stopwords])\n    for c in string.punctuation:\n        text = text.replace(c, \" \")\n    text = \" \".join([c for c in text.split() if c not in stopwords])\n    \n    # ignore \n    words = []\n    ignorewords = [\"&nbsp;\", \"quot\", \"quote\", \"www\", \"http\", \"com\"]\n    for wrd in text.split():\n        if len(wrd) <= 2: \n            continue\n        if wrd in ignorewords:\n            continue\n        words.append(wrd)\n    text = \" \".join(words)\n    \n    # tokenize\n    token = nltk.word_tokenize(text)\n    grams = ngrams(token, n)\n    grams = [\" \".join(c) for c in grams]\n    return dict(Counter(grams).most_common(limit))\n\ndef check_presence(txt, wrds):    \n    cnt = 0\n    txt = \" \"+txt+\" \"\n    for wrd in wrds.split(\"|\"):\n        if \" \"+wrd+\" \" in txt:\n            cnt += 1 \n    return cnt\n\nmessages['CMessage'] = messages['Message'].apply(lambda x : clntxt(x))\n\n# ## get top unigrams\n# unigrams = {}\n# for i in range(2010, 2019):\n#     unigrams[i] = get_top_ngrams(i, 1, 1000)\n    \n# ## get top bigrams \n# bigrams = {}\n# for i in range(2010, 2019):\n#     bigrams[i] = get_top_ngrams(i, 2, 1000)\n\n\n\nmessages['CreationDay'] = \"21\"\nmessages['KernelDate'] = messages[\"CreationYear\"].astype(str) +\"-\"+ messages[\"CreationMonth\"].astype(str) +\"-\"+ messages[\"CreationDay\"].astype(str)\n","5e4f0950":"def plotthem(listed, title):    \n    traces = []\n    for model in listed:\n        temp = messages.groupby('KernelDate').agg({model : \"sum\"}).reset_index()\n        trace = go.Scatter(x = temp[\"KernelDate\"], y = temp[model], name=model.split(\"|\")[0].title(), line=dict(shape=\"spline\", width=2), mode = \"lines\")\n        traces.append(trace)\n\n    layout = go.Layout(\n        paper_bgcolor='#fff',\n        plot_bgcolor=\"#fff\",\n        legend=dict(orientation=\"h\", y=1.1),\n        title=title,\n        xaxis=dict(\n            gridcolor='rgb(255,255,255)',\n            range = ['2010-01-01','2018-06-01'],\n            showgrid=True,\n            showline=False,\n            showticklabels=True,\n            tickcolor='rgb(127,127,127)',\n            ticks='outside',\n            zeroline=False\n        ),\n        yaxis=dict(\n            title=\"Number of Kaggle Discussions\",\n            gridcolor='rgb(255,255,255)',\n            showgrid=False,\n            showline=False,\n            showticklabels=True,\n            tickcolor='rgb(127,127,127)',\n            ticks='outside',\n            zeroline=False\n        ),\n    )\n\n    fig = go.Figure(data=traces, layout=layout)\n    iplot(fig)\n    \n## linear vs logistic regression\nmodels = [\"linear regression\", \"logistic regression\"]\nfor col in models:\n    messages[col] = messages[\"CMessage\"].apply(lambda x : check_presence(x, col))\nplotthem(models, \"Kaggle Discussions: Linear vs Logistic\")    \n","9ca85dce":"models = [\"decision tree\",\"random forest\", \"xgboost|xgb\", \"lightgbm|lgb\", \"catboost\"]\nfor col in models:\n    messages[col] = messages[\"CMessage\"].apply(lambda x : check_presence(x, col))\nplotthem(models, \"Kaggle Discussions: Tree based models\")    \n","cc33434d":"models = [\"neural network\", \"deep learning\"]\nfor col in models:\n    messages[col] = messages[\"CMessage\"].apply(lambda x : check_presence(x, col))\nplotthem(models, \"Kaggle Discussions: Neural Networks vs Deep Learning\")    ","d4c44b12":"models = [\"scikit\", \"tensorflow|tensor flow\", \"keras\", \"pytorch\"]\nfor col in models:\n    messages[col] = messages[\"CMessage\"].apply(lambda x : check_presence(x, col))\nplotthem(models, \"Kaggle Discussions: ML Tools\")    ","61074253":"models = [\"xgboost|xgb\", \"keras\"]\nfor col in models:\n    messages[col] = messages[\"CMessage\"].apply(lambda x : check_presence(x, col))\nplotthem(models, \"Kaggle Discussions: Xgboost vs Deep Learning\")    ","dfeaca87":"models = [\"matplotlib\", \"seaborn\", \"plotly\"]\nfor col in models:\n    messages[col] = messages[\"CMessage\"].apply(lambda x : check_presence(x, col))\nplotthem(models, \"Kaggle Discussions: Data Visualization Libraries\")    ","03081753":"models = [\"exploration|explore|eda\" , 'feature engineering', 'parameter tuning|hyperparameter tuning|model tuning|tuning', \"ensembling|ensemble\"]\nfor col in models:\n    messages[col] = messages[\"CMessage\"].apply(lambda x : check_presence(x, col))\nplotthem(models, \"Kaggle Discussions: Important Data Science Techniques\")    ","967e2e70":"models = [\"dataset\" , 'kernel', 'competition', 'learn']\nfor col in models:\n    messages[col] = messages[\"CMessage\"].apply(lambda x : check_presence(x, col))\nplotthem(models, \"What is hottest on Kaggle\")    ","d8f8fe98":"models = [\"xgboost|xgb\", \"lightgbm|lgb\"]\nfor col in models:\n    messages[col] = messages[\"CMessage\"].apply(lambda x : check_presence(x, col))\nplotthem(models, \"Kaggle Discussions: Xgboost vs lightgbm\")   ","6774fb96":"models = [\"lightgbm|lgb\", \"keras\"]\nfor col in models:\n    messages[col] = messages[\"CMessage\"].apply(lambda x : check_presence(x, col))\nplotthem(models, \"Kaggle Discussions: lightgbm vs keras\")   ","f269ea47":"> - Scikit Learn was the only library used on kaggle for machine learning tasks, but since 2015 tensorflow gained populartiy. \n> - Among the ML tools, Keras is the most popular because of the simplistic deep learning implementation.  \n\n## 5. XgBoost vs Keras","20fe7b22":"> - Among both the popular techniques on Kaggle - xgboost and deeplearning, xgboost has remained on top because it is faster and requires **less computational infrastructure** than very complex and deeper neural networks. \n\n\n## 6. What Kagglers are using for Data Visualizations ?","eafbc3dd":"## 5. LightGBM vs Keras","8f274d4a":"> - From the above graph, we can observe that there were always been **more discussions related to logistic regression** than linear regression. The generel trend is that number of discussions are increasing every month. \n> - One indication is that there are more number of classification problems than regression problems on Kaggle including the most popular **Titanic Survival Prediction competition**. This competition has most number of discussions and is one of the longest running compeition on Kaggle. There is a regression competition as well : House Prices advanced regression, but people more often start it after titanic only.     \n> - The number of logistic regression discussions on forums, kernel comments, and replies boomed to high numbers in October 2017 and March 2018. One of the reason is the the **Toxic Comments Classification Competition\"** in which a number of authors shared excellent information related to classification models including logistic regression. \n\n## 2. The dominance of xgboost","2a308703":"## Historical Data Science Trends on Kaggle \n\nA number of trends have changed over the years in the field of Data Science. Kaggle is the largest and the most popular data science community across the globe. In this kernel, I am using Kaggle Meta Data to explore the Data Science trends over the years. \n\n## 1. Linear Vs Logistic Regression\n\nLets look at the comparison of linear regression and logistic regression discussions on forums, kernels and replies on kaggle. ","8dfc3beb":"> - Among the important data science steps, kagglers focus alot on **Model Ensembling** since many winning solutions on kaggle competitions are ensemble models - the blends and stacked models.  In almost every regression or classification kernels, one can notice the ensemblling kernels. Just for an example - in Toxic Comment Classification Competition, massively large number of ensemling kernels were shared.   \n> - **Data Exploration** is the important technique and people have started stressing on the importance of exploration in the EDA kernels.  \n> - Surprizing to see that discussions related to **Feature Engineering and Model Tuninig are less than Ensembling**. These two tasks have the most important significance in the best and accurate models.  People tend to forget that ensembling is only the last stage of any modelling process but a considerable amount of time should be given to feature engineering and model tuning tasks.  \n\n## 8. Kaggle Components : What people talks about the most ","02ccf84e":"> - Plotly has gained so much popularity since 2017 and is one of the most used data visualization library among the kernels. The second best is seaborn which is used extensively as well. Some of the high quality visualization kernels by kaggle grandmasters such as SRK and Anistropic are created with plotly. Personally, I am a big fan of plotly as well. :P\n\n## 7. Important Data Science Techniques ","aa79fedc":"## 5. XgBoost vs LightGBM","883af065":"> - Before 2014, Linear Models, Decision Trees, and Random Forests were very popular. But when XgBoost was open sourced in 2014, it gained popularty quickly and **dominated the kaggle competitions and kernels**. Today, xgboost is still used exhaustively in compeitions and is the part of the winning models of many competitions. Some examples are **Otto Group Classification Competition** in which first place solution made use of xgboost. \n> - However with the arrival of **Lightgbm in 2016**, the useage of xgboost dipped to some extent and popularity of lightgbm started rising very quickly. Based on the recent increasing trend of lightgbm (shown in red), one can forecast that it will dominate next few years as well, unless any other company opensources a better model.  For example, lightgbm was used in the winning solution of **Porto Seguro\u2019s Safe Driver Prediction** . One of the reason for light gbm popularity is the faster implementation and simple interface as compared to xgboost.  \n> - For instance, Catboost was recently released and is starting gaining popularity.  \n\n\n## 3. Trends of Neural Networks and Deep Learning","5e98d193":"> - Kaggle communitiy has shared a number of competition related discussions in fourms and are increasing in general.  \n> - With the launch of kernels in 2016, their useage increased to a great extent. Firstly kagglers shared kernels in competitions only, but with a more focus on **kaggle datasets, kernel awards**, the number of discussions related to kernels started rising and have surpassed the discussions related to competitions.  Also, a number of **Data Science for Good Challenges** and **Kernels only competitions** have been launched on kaggle which are one of the reason of kernels popularity. \n> - Kaggle also launched the awesome **Kaggle Learn** section which is becoming popular and popular but still it is behind than the compeitions, kernels, and discussions. This is because its primarily audience is the novice and begineers, but for sure in coming years and with the more addition of courses, kaggle learn section will reach the similar levels as competitions and kernels. ","4915df93":"> - Neural networks were present in the industry since the decades but in recent years trends changed because of the access to much larger data and computational power.  \n> - The era of deep learning started in 2014 with the arrival of libraries such as theano, tensorflow in 2015, and keras in 2016. The number of discussions related to deep learning is increasing regularly and are always more than neural networks. Also, many cloud instance providers such as **Amazon AWS, Google cloud** etc showcases their capabilities of training very deep neural networks on clouds.  \n> - The deeplearning models also became popular because of a number of Image Classification competitions on Kaggle such as :  **Data Science Bowl**, competitions from Google etc. Also, deeplearning models became popular for text classification problems for example **Quora Duplicate Questions Classification**.  \n> - Deep learning is also become populary every month because of different variants of models such as RNNs, CNNs have shown great improvements in the kernels. Also, **transfer learning and pre-trained models** have shown great results in competitions.  \n> - Kaggle can launch more competitions \/ playgrounds related to Image Classification Modelling as people wants to learn from them alot.  Not to forget that Kaggle have added the GPU support in kernels which facilitates the Deep Learning useage on kaggle.  \n\n## 4. ML Tools used on Kaggle"}}