{"cell_type":{"d174f7dc":"code","a8e2067f":"code","3ee894e7":"code","cc1cf5eb":"code","1ecde77d":"code","9aa378a3":"code","dfc58254":"code","b3de194d":"code","2b6f5842":"code","35152b1b":"code","b0be9650":"code","422b0531":"code","740084ed":"code","16316be1":"code","78745d5a":"code","a594dc46":"code","8960f575":"code","b2dd558d":"code","2b6b1c36":"code","b850aaf6":"code","cc2adb70":"code","86a1db57":"code","6d35a237":"code","33d8fbb7":"code","0b979687":"code","b5145d90":"code","fb88dd14":"code","1b86888a":"code","3f6c228a":"code","3d6d657d":"code","52dc57a1":"code","e8567400":"code","73a36ff1":"code","1e19c55f":"code","8e3a244a":"code","b5c507c2":"code","526474b4":"code","46bb44df":"code","7e30172c":"code","e4965a2a":"markdown","59ad7b0c":"markdown","ad038fb6":"markdown","291352b5":"markdown","d9e881f8":"markdown"},"source":{"d174f7dc":"%%capture\n!python -m spacy download en\n!python -m spacy download de","a8e2067f":"import os\nimport re\nimport time\nimport math\nimport random\nimport unicodedata\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\n\nimport spacy\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom torch import nn, optim\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn.functional as F\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nfrom torchtext.data.metrics import bleu_score","3ee894e7":"SEED = 28\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","cc1cf5eb":"data_df = pd.read_csv('..\/input\/english-to-german\/deu.txt', sep='\\t', usecols=[0, 1])\ndata_df.columns = ['en', 'de']\ndata_df.head()","1ecde77d":"data_df.shape","9aa378a3":"plt.figure(figsize=(12, 6))\nplt.style.use('ggplot')\nplt.subplot(1, 2, 1)\nsns.distplot(data_df['en'].str.split().apply(len))\nplt.title('Distribution of English sentences length')\nplt.xlabel('Length')\n\nplt.style.use('ggplot')\nplt.subplot(1, 2, 2)\nsns.distplot(data_df['de'].str.split().apply(len))\nplt.title('Distribution of German sentences length')\nplt.xlabel('Length')\nplt.show()","dfc58254":"seq_len_en = 20\nseq_len_de = 20","b3de194d":"train_df, valid_df = train_test_split(data_df, test_size=0.1, shuffle=True, random_state=28)\n\ntrain_df = train_df.reset_index(drop=True)\nvalid_df = valid_df.reset_index(drop=True)\n\nprint(train_df.shape)\nprint(valid_df.shape)","2b6f5842":"for i in range(len(train_df)-5, len(train_df)):\n    print(f'ENGLISH:\\n{train_df.iloc[i][\"en\"]},\\nGERMAN:\\n{train_df.iloc[i][\"de\"]}\\n{\"=\"*92}')","35152b1b":"class Vocabulary:\n    def __init__(self, freq_threshold=2, language='en', preprocessor=None, reverse=False):\n        self.itos = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\", 3: \"<unk>\"}\n        self.stoi = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n        self.tokenizer = spacy.load(language)\n        self.freq_threshold = freq_threshold\n        self.preprocessor = preprocessor\n        self.reverse = reverse\n\n    def __len__(self):\n        return len(self.itos)\n\n    def tokenize(self, text):\n        if self.reverse:\n            return [token.text.lower() for token in self.tokenizer.tokenizer(text)][::-1]\n        else:\n            return [token.text.lower() for token in self.tokenizer.tokenizer(text)]\n\n    def build_vocabulary(self, sentence_list):\n        frequencies = {}\n        idx = len(self.itos)\n\n        for sentence in sentence_list:\n            # Preprocess the sentence using given preprocessor.\n            if self.preprocessor:\n                sentence = self.preprocessor(sentence)\n\n            for word in self.tokenize(sentence):\n                if word in frequencies:\n                    frequencies[word] += 1\n                else:\n                    frequencies[word] = 1\n\n                if frequencies[word] == self.freq_threshold:\n                    self.stoi[word] = idx\n                    self.itos[idx] = word\n                    idx += 1\n\n    def numericalize(self, text):\n        tokenized_text = self.tokenize(text)\n\n        return [\n            self.stoi[token] if token in self.stoi else self.stoi[\"<unk>\"]\n            for token in tokenized_text\n        ]","b0be9650":"# Converts the unicode file to ascii\ndef unicode_to_ascii(s):\n    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n\ndef preprocess_sentence(w):\n    w = unicode_to_ascii(w.lower().strip())\n\n    # creating a space between a word and the punctuation following it\n    # eg: \"he is a boy.\" => \"he is a boy .\"\n    # Reference:- https:\/\/stackoverflow.com\/questions\/3645931\/python-padding-punctuation-with-white-spaces-keeping-punctuation\n    w = re.sub(r\"([?.!,\u00bf])\", r\" \\1 \", w)\n    w = re.sub(r'[\" \"]+', \" \", w)\n\n    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n    w = re.sub(r\"[^a-zA-Z?.!,\u00bf]+\", \" \", w)\n\n    w = w.strip()\n    return w","422b0531":"%%time\n# Build vocab using training data\nfreq_threshold = 2\nen_vocab = Vocabulary(freq_threshold=freq_threshold, language=\"en\", preprocessor=preprocess_sentence, reverse=False)\nde_vocab = Vocabulary(freq_threshold=freq_threshold, language=\"de\", preprocessor=preprocess_sentence, reverse=True)\n\n# build vocab for both english and german\nen_vocab.build_vocabulary(train_df[\"en\"].tolist())\nde_vocab.build_vocabulary(train_df[\"de\"].tolist())","740084ed":"class CustomTranslationDataset(Dataset):    \n    def __init__(self, df, en_vocab, de_vocab):\n        super().__init__()\n        self.df = df\n        self.en_vocab = en_vocab\n        self.de_vocab = de_vocab\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def _get_numericalized(self, sentence, vocab):\n        \"\"\"Numericalize given text using prebuilt vocab.\"\"\"\n        numericalized = [vocab.stoi[\"<sos>\"]]\n        numericalized.extend(vocab.numericalize(sentence))\n        numericalized.append(vocab.stoi[\"<eos>\"])\n        return numericalized\n\n    def __getitem__(self, index):\n        en_numericalized = self._get_numericalized(self.df.iloc[index][\"en\"], self.en_vocab)\n        de_numericalized = self._get_numericalized(self.df.iloc[index][\"de\"], self.de_vocab)\n\n        # Also include the length of the source sentence\n        return torch.tensor(de_numericalized), len(de_numericalized), torch.tensor(en_numericalized)","16316be1":"class CustomCollate:\n    def __init__(self, pad_idx):\n        self.pad_idx = pad_idx\n\n    def __call__(self, batch):\n        # Sort the batch w.r.t source length in decreasing order as sorted batch\n        # is required in packed padded sequence function.\n        batch = sorted(batch, key=lambda x: -x[1])\n        src = [item[0] for item in batch]\n        src = pad_sequence(src, batch_first=False, padding_value=self.pad_idx)\n        \n        src_lens = torch.tensor([item[1] for item in batch])\n        \n        targets = [item[2] for item in batch]\n        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n\n        return src, src_lens, targets","78745d5a":"BATCH_SIZE = 128\n\n# Define dataset and dataloader\ntrain_dataset = CustomTranslationDataset(train_df, en_vocab, de_vocab)\nvalid_dataset = CustomTranslationDataset(valid_df, en_vocab, de_vocab)\n\ntrain_loader = DataLoader(\n    dataset=train_dataset,\n    batch_size=BATCH_SIZE,\n    num_workers=4,\n    shuffle=False,\n    collate_fn=CustomCollate(pad_idx=en_vocab.stoi[\"<pad>\"])\n)\n\nvalid_loader = DataLoader(\n    dataset=valid_dataset,\n    batch_size=BATCH_SIZE,\n    num_workers=4,\n    shuffle=False,\n    collate_fn=CustomCollate(pad_idx=en_vocab.stoi[\"<pad>\"])\n)","a594dc46":"fun_de = np.vectorize(lambda x: de_vocab.itos[x])\nfun_en = np.vectorize(lambda x: en_vocab.itos[x])","8960f575":"print(f\"Unique tokens in source (de) vocabulary: {len(de_vocab)}\")\nprint(f\"Unique tokens in target (en) vocabulary: {len(en_vocab)}\")","b2dd558d":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","2b6b1c36":"class Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout=0.2):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.gru = nn.GRU(emb_dim, hidden_dim, n_layers, bidirectional=True, dropout=0.0 if n_layers==1 else dropout)\n        self.fc = nn.Linear(2*hidden_dim, hidden_dim)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, src_len):\n        x = self.embedding(x)\n        x = self.dropout(x)\n        \n        # pack the sequence before passing to RNN\n        packed_x = nn.utils.rnn.pack_padded_sequence(x, src_len)\n        \n        packed_outputs, hidden_state = self.gru(packed_x)\n        \n        # pad packed output sequence\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n        \n        # Combine the forward and backward RNN's hidden states to be input to decoder\n        hidden_state = torch.tanh(self.fc(torch.cat((hidden_state[-2, :, :], hidden_state[-1, :, :]), dim=1)))\n        return outputs, hidden_state\n\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        self.attention = nn.Linear(3*hidden_dim, hidden_dim)\n        self.v = nn.Linear(hidden_dim, 1, bias=False)\n    \n    def forward(self, hidden_state, encoder_outputs, mask):\n        batch_size = encoder_outputs.shape[1]\n        src_len = encoder_outputs.shape[0]\n        \n        # repeat decoder hidden state src_len times\n        hidden_state = hidden_state.unsqueeze(1).repeat(1, src_len, 1)\n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n\n        energy = torch.tanh(self.attention(torch.cat((hidden_state, encoder_outputs), dim = 2))) \n        attention = self.v(energy).squeeze(2)\n        # apply the mask so that the model don't pay the attention to paddings.\n        # it's applied before softmax so that the masked values which are very\n        # small will be zero'ed out after softmax\n        attention = attention.masked_fill(mask == 0, -1e10)\n        \n        return F.softmax(attention, dim=1)\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, attention, dropout=0.2):\n        super().__init__()\n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n        self.attention = attention\n        self.n_layers = n_layers\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.gru = nn.GRU(emb_dim+(hidden_dim*2), hidden_dim, n_layers, dropout=0.0 if n_layers==1 else dropout)\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden_dim*3 + emb_dim, output_dim)\n    \n    def forward(self, input, hidden, encoder_outputs, mask):\n        input = input.unsqueeze(0)\n        \n        #input = [1, batch size]\n        \n        embedded = self.dropout(self.embedding(input))\n        \n        #embedded = [1, batch size, emb dim]\n        \n        a = self.attention(hidden, encoder_outputs, mask)\n                \n        #a = [batch size, src len]\n        \n        a = a.unsqueeze(1)\n        \n        #a = [batch size, 1, src len]\n        \n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n        \n        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n        \n        weighted = torch.bmm(a, encoder_outputs)\n        \n        #weighted = [batch size, 1, enc hid dim * 2]\n        \n        weighted = weighted.permute(1, 0, 2)\n        \n        #weighted = [1, batch size, enc hid dim * 2]\n        \n        rnn_input = torch.cat((embedded, weighted), dim = 2)\n        \n        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n            \n        output, hidden = self.gru(rnn_input, hidden.unsqueeze(0))\n        \n        #output = [seq len, batch size, dec hid dim * n directions]\n        #hidden = [n layers * n directions, batch size, dec hid dim]\n        \n        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n        #output = [1, batch size, dec hid dim]\n        #hidden = [1, batch size, dec hid dim]\n        #this also means that output == hidden\n        assert (output == hidden).all()\n        \n        embedded = embedded.squeeze(0)\n        output = output.squeeze(0)\n        weighted = weighted.squeeze(0)\n        \n        prediction = self.fc(torch.cat((output, weighted, embedded), dim = 1))\n        \n        #prediction = [batch size, output dim]\n        \n        # also return attention vector as it's required at inference time to vizualize\n        # attention mechanism.\n        return prediction, hidden.squeeze(0), a.squeeze(1)\n\nclass EncoderDecoder(nn.Module):\n    def __init__(self, encoder, decoder, pad_idx):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.pad_idx = pad_idx\n        \n        assert self.encoder.hidden_dim == decoder.hidden_dim\n        assert self.encoder.n_layers == decoder.n_layers\n    \n    def _create_mask(self, x):\n        return (x != self.pad_idx).permute(1, 0)\n    \n    def forward(self, x, src_len, y, teacher_forcing_ratio=0.75):\n        \n        target_len = y.shape[0]\n        batch_size = y.shape[1]\n        target_vocab_size = self.decoder.output_dim  # Output dim\n        \n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        \n        # Encode the source text using encoder. Last hidden state of encoder is context vector.\n        encoder_outputs, hidden_state = self.encoder(x, src_len)\n        \n        # First input is <sos>\n        input = y[0,:]\n        \n        mask = self._create_mask(x)\n        \n        # Decode the encoded vector using decoder\n        for t in range(1, target_len):\n            # attention vector returned by decoder is not required while training\n            output, hidden_state, _ = self.decoder(input, hidden_state, encoder_outputs, mask)\n            outputs[t] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            pred = output.argmax(1)\n            input = y[t] if teacher_force else pred\n        \n        return outputs","b850aaf6":"# Initialize all models\ninput_dim = len(de_vocab)\noutput_dim = len(en_vocab)\nemb_dim = 256\nhidden_dim = 512\nn_layers = 1\ndropout = 0.5\npad_idx = de_vocab.stoi['<pad>']\n\nattention = Attention(hidden_dim)\nencoder = Encoder(input_dim, emb_dim, hidden_dim, n_layers, dropout)\ndecoder = Decoder(output_dim, emb_dim, hidden_dim, n_layers, attention, dropout)\nmodel = EncoderDecoder(encoder, decoder, pad_idx).to(device)","cc2adb70":"def init_weights(m):\n    for name, param in m.named_parameters():\n        if 'weight' in name:\n            nn.init.normal_(param.data, mean=0, std=0.01)\n        else:\n            nn.init.constant_(param.data, 0)\n            \nmodel.apply(init_weights)","86a1db57":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","6d35a237":"optimizer = optim.Adam(model.parameters())\ncriterion = nn.CrossEntropyLoss(ignore_index=en_vocab.stoi[\"<pad>\"])","33d8fbb7":"def train(model, iterator, optimizer, criterion, clip):\n    model.train()\n    epoch_loss = 0\n    \n    for i, batch in tqdm(enumerate(iterator), total=len(iterator), position=0, leave=True):\n        src = batch[0].to(device)\n        src_len = batch[1].to(device)\n        trg = batch[2].to(device)\n\n        optimizer.zero_grad()\n        \n        output = model(src, src_len, trg)\n        \n        #trg = [trg len, batch size]\n        #output = [trg len, batch size, output dim]\n        \n        output_dim = output.shape[-1]\n        output = output[1:].view(-1, output_dim)\n        trg = trg[1:].view(-1)\n        \n        #trg = [(trg len - 1) * batch size]\n        #output = [(trg len - 1) * batch size, output dim]\n        \n        loss = criterion(output, trg)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        epoch_loss += loss.item()\n        \n    return epoch_loss \/ len(iterator)","0b979687":"def evaluate(model, iterator, criterion):\n    model.eval()    \n    epoch_loss = 0\n\n    with torch.no_grad():\n        for i, batch in tqdm(enumerate(iterator), total=len(iterator), position=0, leave=True):\n            src = batch[0].to(device)\n            src_len = batch[1].to(device)\n            trg = batch[2].to(device)\n\n            output = model(src, src_len, trg, 0)  # turn off teacher forcing\n\n            #trg = [trg len, batch size]\n            #output = [trg len, batch size, output dim]\n\n            output_dim = output.shape[-1]\n            output = output[1:].view(-1, output_dim)\n            trg = trg[1:].view(-1)\n\n            #trg = [(trg len - 1) * batch size]\n            #output = [(trg len - 1) * batch size, output dim]\n\n            loss = criterion(output, trg)\n            epoch_loss += loss.item()\n        \n    return epoch_loss \/ len(iterator)","b5145d90":"def inference(model, sentence, src_len):\n    model.eval()\n    result, attentions = [], []\n\n    with torch.no_grad():\n        sentence = sentence.to(device)\n        encoder_outputs, hidden_state = model.encoder(sentence, src_len)\n\n        # First input to decoder is \"<sos>\"\n        inp = torch.tensor([en_vocab.stoi[\"<sos>\"]]).to(device)\n        \n        mask = model._create_mask(sentence)\n\n        # Decode the encoded vector using decoder until max length is reached or <eos> is generated.\n        for t in range(1, seq_len_en):\n            output, hidden_state, attention = model.decoder(inp, hidden_state, encoder_outputs, mask)\n            pred = output.argmax(1)\n            if pred == en_vocab.stoi[\"<eos>\"]:\n                break\n            result.append(en_vocab.itos[pred.item()])\n            inp = pred\n            attentions.append(attention.cpu().detach().numpy().ravel())\n\n    return \" \".join(result), attentions","fb88dd14":"def epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time \/ 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","1b86888a":"for sample_batch in valid_loader:\n    break","3f6c228a":"N_EPOCHS = 10\nCLIP = 1\n\nbest_valid_loss = float('inf')\n\nsample_source = ' '.join([word for word in fun_de(sample_batch[0][:, 101]) if word not in [\"<pad>\", \"<sos>\", \"<eos>\"]])\nsample_target = ' '.join([word for word in fun_en(sample_batch[2][:, 101]) if word not in [\"<pad>\", \"<sos>\", \"<eos>\"]])\n\nfor epoch in range(N_EPOCHS):\n    \n    start_time = time.time()\n    \n    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n    valid_loss = evaluate(model, valid_loader, criterion)\n    \n    end_time = time.time()\n    \n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'best_model.pt')\n    \n    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\t Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n    print(f'\\t Sample Source (German): {sample_source}')\n    print(f'\\t Sample Target (English): {sample_target}')\n    print(f'\\t Generated: {inference(model, sample_batch[0][:, 10].reshape(-1, 1), torch.tensor([sample_batch[1][0]]))[0]}\\n')","3d6d657d":"# Load the best model.\nmodel_path = \".\/best_model.pt\"\nmodel.load_state_dict(torch.load(model_path))","52dc57a1":"for idx in range(20):\n    print(f'ACTUAL GERMAN: {\" \".join([word for word in fun_de(sample_batch[0][:, idx]) if word not in [\"<pad>\", \"<sos>\", \"<eos>\"]])}')\n    print(f'ACTUAL: ENGLISH: {\" \".join([word for word in fun_en(sample_batch[2][:, idx]) if word not in [\"<pad>\", \"<sos>\", \"<eos>\"]])}')\n    print(f'GENERATED BY MODEL: {inference(model, sample_batch[0][:, idx].reshape(-1, 1), torch.tensor([sample_batch[1][0]]))[0]}')\n    print(\"=\"*92)","e8567400":"def plot_attention_map(sentence):\n    sentence = sentence.reshape(-1, 1)\n    translated, attentions = inference(model, sentence, torch.tensor([sample_batch[1][0]]))\n    sentence = sentence.cpu().detach().numpy().ravel()\n    sent_len = len(sentence[sentence != 0])\n    \n    fig, ax = plt.subplots(1, 1)\n    fig.set_figheight(8)\n    fig.set_figwidth(8)\n    ax.imshow(np.array(attentions)[:, :sent_len], cmap='hot', interpolation='nearest')\n    ax.tick_params(labelsize=12)\n    ax.set_xticklabels(['']+[word for word in fun_de(sentence) if word not in [\"<pad>\"]], rotation=45)\n    ax.set_yticklabels(['']+translated.split())\n\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.grid(False)\n    plt.show()","73a36ff1":"plot_attention_map(sample_batch[0][:, 0])","1e19c55f":"plot_attention_map(sample_batch[0][:, 1])","8e3a244a":"plot_attention_map(sample_batch[0][:, 15])","b5c507c2":"plot_attention_map(sample_batch[0][:, 19])","526474b4":"plot_attention_map(sample_batch[0][:, 2])","46bb44df":"def calculate_bleu(sample_batch):\n    \n    trgs = []\n    pred_trgs = []\n    \n    for idx in range(sample_batch[0].shape[1]):\n        pred, _ = inference(model, sample_batch[0][:, idx].reshape(-1, 1), torch.tensor([sample_batch[1][0]]))\n        pred_trgs.append(pred.split())\n        trgs.append([[word for word in fun_en(sample_batch[2][:, idx]) if word not in [\"<pad>\", \"<sos>\", \"<eos>\"]]])\n    return bleu_score(pred_trgs, trgs)","7e30172c":"calculate_bleu(sample_batch)","e4965a2a":"## Attention plots\n\nAttention plot shows the amount of attention paid by the model to the source sequence words at the time of translation of each of the target word.","59ad7b0c":"## Calculating validation BLEU Score","ad038fb6":"## Results","291352b5":"In this notebook, I've implemented improved version of attention based Seq2Seq model. Most of the modeling and training part are referenced from part-IV of this great tutorial series: https:\/\/github.com\/bentrevett\/pytorch-seq2seq.\n\nBut in this series, the preprocessed data is being used for training\/evaluation (because pytorch's Multi30k class provides all the heavy lifting), so it's bit difficult to generalize the structure for custom dataset implementation. So in this notebook, I've implemented data preprocessing like tokenization, padding etc. from scratch using spacy and pure pytorch. I've also plotted attention plots for some of the validation sentences. Which seems interesting as it helps us to understand the underlying attention mechanism.\n\nHere are some other references I've used:\n\n* [Original research paper: Neural Machine Translation by Jointly Learning to Align and Translate](https:\/\/arxiv.org\/pdf\/1409.0473.pdf)\n* [Creating custom dataset for NLP tasks](https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/blob\/22635a65d8cf462aa44199357928e61c0ecda000\/ML\/Pytorch\/more_advanced\/image_captioning\/get_loader.py)","d9e881f8":"## Modeling"}}