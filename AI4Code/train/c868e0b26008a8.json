{"cell_type":{"b65ad9a2":"code","3738557f":"code","bd1f3df1":"code","01307382":"code","1d77509b":"code","ca241817":"code","c5349d0c":"code","302e559b":"code","06d9cbd7":"code","5cf7face":"code","45ae31fc":"code","e4ff62f4":"code","a28aa1c3":"code","0845f2e7":"code","4c07980f":"code","823a8486":"code","72ae9aaa":"code","36ffba82":"code","21125be5":"code","f3a9179c":"code","6fa60290":"code","8b0f79a6":"code","65342a44":"code","dbd7d15e":"code","1c6a2d72":"code","9667eaed":"code","91ba776e":"code","e6470f9f":"code","5f070115":"code","a5d44c33":"code","38e62e77":"code","c8b37b91":"code","572fb91e":"code","90d6f708":"code","4a621646":"code","c4605b9e":"code","ab700307":"code","51137b45":"code","ae463696":"code","986bce8b":"code","6968196b":"code","0409534d":"code","a61ee3ae":"code","639f80de":"markdown","a0170c12":"markdown","8a1b9bdf":"markdown","55ef9ac7":"markdown","3cf31c1c":"markdown","284a894c":"markdown","b6f2fc89":"markdown","1569b9ab":"markdown","1261c0ff":"markdown","804b5d93":"markdown","3967631b":"markdown","4cfc553c":"markdown","c42a2dbf":"markdown","ecdcb13f":"markdown","303b4c4d":"markdown","3eb3b069":"markdown","6862ff11":"markdown","4330384b":"markdown","61dc4996":"markdown","a521f3e8":"markdown","086561a8":"markdown","44954660":"markdown","a151bba6":"markdown","8de3d1e0":"markdown","e7d879a4":"markdown","a78def7a":"markdown","c563689b":"markdown","ac339c03":"markdown","23475cf4":"markdown","2151b2bf":"markdown","9a292cb0":"markdown","fae08a95":"markdown","9a575856":"markdown","536a6764":"markdown","4d4bf55c":"markdown","986d59c7":"markdown","1ff5ab5f":"markdown","c05e820d":"markdown","86688783":"markdown","9c20813d":"markdown"},"source":{"b65ad9a2":"!pip install optuna","3738557f":"# Core\nimport pandas as pd\nimport numpy as np\nimport gc\n\n# Data Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","bd1f3df1":"# Path of the file to read.\nfile_path = '..\/input\/california-house-prices\/train.csv'\n\n# Load data into a pandas DataFrame. Note: 1st column is ID\nhome_data = pd.read_csv(file_path, index_col=0)","01307382":"home_data.head()","1d77509b":"home_data.tail()","ca241817":"home_data.shape","c5349d0c":"home_data.select_dtypes(exclude=['object']).columns","302e559b":"len(home_data.select_dtypes(exclude=['object']).columns)","06d9cbd7":"home_data.select_dtypes(exclude=['object']).describe().round(decimals=2)","5cf7face":"home_data.select_dtypes(include=['object']).columns","45ae31fc":"len(home_data.select_dtypes(include=['object']).columns)","e4ff62f4":"home_data.select_dtypes(include=['object']).describe().round()","a28aa1c3":"target = home_data['Sold Price']\nplt.figure()\nsns.distplot(target)\nplt.title('Distribution of Sold Price')\nplt.show()","0845f2e7":"sns.distplot(np.log(target))\nplt.title('Distribution of Log-transformed Sold Price')\nplt.xlabel('log(Sold Price)')\nplt.show()","4c07980f":"print('Sold Proice has a skew of ' \\\n      + str(target.skew().round(decimals=2)) \\\n      + ' while the log-transformed Sold Price improves the skew to ' \\\n      + str(np.log(target).skew().round(decimals=2)))","823a8486":"num_attributes = home_data.select_dtypes(exclude='object').drop('Sold Price', axis=1).drop('Lot', axis=1).copy()\n\nfig = plt.figure(figsize=(12,18))\nfor i in range(len(num_attributes.columns)):\n    fig.add_subplot(6,3,i+1)\n    sns.distplot(num_attributes.iloc[:,i].dropna().round(decimals=2))\n    plt.xlabel(num_attributes.columns[i])\n\nplt.tight_layout()\nplt.show()","72ae9aaa":"fig = plt.figure(figsize=(12, 18))\n\nfor i in range(len(num_attributes.columns)):\n    fig.add_subplot(6, 3, i+1)\n    sns.boxplot(y=num_attributes.iloc[:,i])\n\nplt.tight_layout()\nplt.show()","36ffba82":"f = plt.figure(figsize=(12,20))\n\nfor i in range(len(num_attributes.columns)):\n    f.add_subplot(6, 3, i+1)\n    sns.scatterplot(num_attributes.iloc[:,i], target)\n    \nplt.tight_layout()\nplt.show()","21125be5":"correlation = home_data.corr()\n\nf, ax = plt.subplots(figsize=(14,12))\nplt.title('Correlation of numerical attributes', size=16)\nsns.heatmap(correlation)\nplt.show()","f3a9179c":"correlation['Sold Price'].sort_values(ascending=False).head(15)","6fa60290":"num_columns = home_data.select_dtypes(exclude='object').columns\ncorr_to_price = correlation['Sold Price']\nn_cols = 5\nn_rows = 4\nfig, ax_arr = plt.subplots(n_rows, n_cols, figsize=(16,20), sharey=True)\nplt.subplots_adjust(bottom=-0.8)\nfor j in range(n_rows):\n    for i in range(n_cols):\n        plt.sca(ax_arr[j, i])\n        index = i + j*n_cols\n        if index < len(num_columns):\n            plt.scatter(home_data[num_columns[index]], home_data['Sold Price'])\n            plt.xlabel(num_columns[index])\n            plt.title('Corr to Sold Price = '+ str(np.around(corr_to_price[index], decimals=3)))\nplt.show()","8b0f79a6":"# Show columns with most null values:\nnum_attributes.isna().sum().sort_values(ascending=False).head()","65342a44":"cat_columns = home_data.select_dtypes(include='object').columns\nprint(cat_columns)","dbd7d15e":"home_data[cat_columns].isna().sum().sort_values(ascending=False)","1c6a2d72":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split","9667eaed":"# Create copy of dataset  ====================================\nhome_data_copy = home_data.copy()\n\n# Dealing with missing\/null values ===========================\n# Numerical columns:\nhome_data_copy.Region = home_data_copy.Region.fillna(0)\n# HOW TO TREAT LotFrontage - 259 missing values??\n\n# Categorical columns:\ncat_cols_fill_none = ['Last Sold On', 'Middle School', 'Laundry features', 'Appliances included', 'Cooling features',\n                     'Flooring', 'Cooling', 'Heating features', 'Heating',\n                     'High School', 'Elementary School', 'Parking features', 'Bedrooms', 'Parking',\n                     'Summary']\nfor cat in cat_cols_fill_none:\n    home_data_copy[cat] = home_data_copy[cat].fillna(\"None\")","91ba776e":"# Check for outstanding missing\/null values\n# Scikit-learn's Imputer will be used to address these\nhome_data_copy.isna().sum().sort_values(ascending=False).head()","e6470f9f":"# Remove outliers based on observations on scatter plots against Sold Price:\nhome_data_copy = home_data_copy.drop(home_data_copy['Year built']\n                                     [home_data_copy['Year built']>9000][home_data_copy['Year built']==0].index)\nhome_data_copy = home_data_copy.drop(home_data_copy['Total interior livable area']\n                                     [home_data_copy['Total interior livable area']>1.7*1e8].index)\nhome_data_copy = home_data_copy.drop(home_data_copy['Total spaces']\n                                     [home_data_copy['Total spaces']>400].index)\nhome_data_copy = home_data_copy.drop(home_data_copy['Garage spaces']\n                                     [home_data_copy['Garage spaces']>400].index)\nhome_data_copy = home_data_copy.drop(home_data_copy['Middle School Distance']\n                                     [home_data_copy['Middle School Distance']>40].index)\nhome_data_copy = home_data_copy.drop(home_data_copy['High School Distance']\n                                     [home_data_copy['High School Distance']>60].index)\nhome_data_copy = home_data_copy.drop(home_data_copy['Tax assessed value']\n                                     [home_data_copy['Tax assessed value']>4].index)\nhome_data_copy = home_data_copy.drop(home_data_copy['Annual tax amount']\n                                     [home_data_copy['Annual tax amount']>40000].index)\nhome_data_copy = home_data_copy.drop(home_data_copy['Listed Price']\n                                     [home_data_copy['Listed Price']>3*1e8].index)\nhome_data_copy = home_data_copy.drop(home_data_copy['Last Sold Price']\n                                     [home_data_copy['Last Sold Price']>8*1e7].index)\nhome_data_copy = home_data_copy.drop(home_data_copy['Zip']\n                                     [home_data_copy['Zip']<86000].index)","5f070115":"home_data_copy['Sold Price'] = np.log(home_data_copy['Sold Price'])\nhome_data_copy = home_data_copy.rename(columns={'Sold Price': 'SoldPrice_log'})","a5d44c33":"transformed_corr = home_data_copy.corr()\nplt.figure(figsize=(12,10))\nsns.heatmap(transformed_corr)","38e62e77":"transformed_corr['SoldPrice_log'].sort_values(ascending=False)","c8b37b91":"# Remove attributes that were identified for excluding when viewing scatter plots & corr values\nattributes_drop = ['SoldPrice_log', 'Last Sold Price', 'Lot', 'Total spaces', 'Year built', 'Zip', \n                   'Elementary School Distance', 'High School Distance', 'Middle School Distance',\n                   'Bathrooms', 'High School Score', 'Tax assessed value']\nX = home_data_copy.drop(attributes_drop, axis=1)\n\n# Create target object and call it y\ny = home_data_copy.SoldPrice_log\n\n# One-hot-encoding to transform all categorical data\nX = pd.get_dummies(X, dummy_na=False)","572fb91e":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom optuna import create_study\nfrom optuna.samplers import TPESampler\nfrom optuna.integration import XGBoostPruningCallback","90d6f708":"def inv_y(transformed_y):\n    return np.exp(transformed_y)","4a621646":"def objective(\n    trial,\n    X,\n    y,\n    random_state=42,\n    n_splits=5,\n    n_repeats=1,\n    n_jobs=1,\n    early_stopping_rounds=50,\n):\n    CV_score_array = []\n    # XGBoost parameters\n    params = {\n        \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n        \"objective\": \"reg:squarederror\",\n        \"n_estimators\": 100*trial.suggest_int(\"n_estimators\", 5, 20),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 6, 12),\n        \"learning_rate\": trial.suggest_int(\"learning_rate\", 0.005, 0.5),\n        \"colsample_bytree\": trial.suggest_loguniform(\"colsample_bytree\", 0.2, 0.6),\n        \"subsample\": 0.1*trial.suggest_int(\"subsample\", 4, 8),\n        \"alpha\": trial.suggest_loguniform(\"alpha\", 0.01, 10.0),\n        \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-8, 10.0),\n        \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-8, 10.0),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 10, 1000),\n        \"seed\": random_state,\n        \"n_jobs\": n_jobs,\n    }\n\n    pruning_callback = XGBoostPruningCallback(trial, \"validation_0-rmse\")\n    rkf = RepeatedKFold(\n        n_splits=n_splits, n_repeats=n_repeats, random_state=random_state\n    )\n    y_pred = np.zeros_like(y)\n    for train_index, test_index in rkf.split(imputed_X):\n        X_train, X_valid = X[train_index], X[test_index]\n        y_train, y_valid = y[train_index].reshape(-1,1), y[test_index].reshape(-1,1)\n        model = XGBRegressor(**params)\n        model.fit(\n            X_train,\n            y_train,\n            eval_set=[(X_valid, y_valid)],\n            eval_metric=\"rmse\",\n            verbose=0,\n            callbacks=[pruning_callback],\n            early_stopping_rounds=early_stopping_rounds,\n        )\n        y_pred[test_index] += model.predict(X_valid)\n    y_pred \/= n_repeats\n    \n    return np.sqrt(mean_squared_error(y, y_pred))","c4605b9e":"imputer = SimpleImputer()\nimputed_X = np.array(imputer.fit_transform(X))\nsampler = TPESampler(seed=42)#, multivariate=True)\nstudy = create_study(direction=\"minimize\", sampler=sampler)\nstudy.optimize(\n    lambda trial: objective(\n        trial,\n        imputed_X,\n        y.values,\n        random_state=42,\n        n_splits=5,\n        n_repeats=1,\n        n_jobs=8,\n        early_stopping_rounds=50,\n    ),\n    n_trials=150,\n    n_jobs=1,\n)\n\n# display params\nhp = study.best_params\nfor key, value in hp.items():\n    print(f\"{key:>20s} : {value}\")\nprint(f\"{'best objective value':>20s} : {study.best_value}\")","ab700307":"params = {\n        \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n        \"objective\": \"reg:squarederror\",\n        \"n_estimators\": 100*study.best_params[\"n_estimators\"],\n        \"max_depth\": study.best_params[\"max_depth\"],\n        \"learning_rate\": study.best_params[\"learning_rate\"],\n        \"colsample_bytree\": study.best_params[\"colsample_bytree\"],\n        \"subsample\": 0.1*study.best_params[\"subsample\"],\n        \"alpha\": study.best_params[\"alpha\"],\n        \"lambda\": study.best_params[\"lambda\"],\n        \"gamma\": study.best_params[\"gamma\"],\n        \"min_child_weight\": study.best_params[\"min_child_weight\"],\n        \"seed\": 42,\n        \"n_jobs\": 8,\n    }\nfinal_xgb_model = XGBRegressor(**params)","51137b45":"del study, sampler","ae463696":"# final_model = XGBRegressor(n_estimators=1500, learning_rate=0.03)\nfinal_train_imputed = imputed_X\n\n# Fit the model using all the data - train it on all of X and y\nfinal_xgb_model.fit(final_train_imputed, y)","986bce8b":"# path to file you will use for predictions\ntest_data_path = '..\/input\/california-house-prices\/test.csv'\n\n# read test data file using pandas\ntest_data = pd.read_csv(test_data_path)","6968196b":"# create test_X which to perform all previous pre-processing on\ntest_X = test_data.copy()\n\n# Repeat treatments for missing\/null values =====================================\n# Numerical columns:\ntest_X.Region = test_X.Region.fillna(0)\n\n# Categorical columns: \nfor cat in cat_cols_fill_none:\n    test_X[cat] = test_X[cat].fillna(\"None\")\n\n# Remove attributes that were identified for excluding when viewing scatter plots & corr values\nattributes_drop = ['Last Sold Price', 'Lot', 'Total spaces', 'Year built', 'Zip', \n                   'Elementary School Distance', 'High School Distance', 'Middle School Distance',\n                   'Bathrooms', 'High School Score', 'Tax assessed value'] # high corr with other attributes\n\ntest_X = test_data.drop(attributes_drop, axis=1)\n\n# Repeat dropping of chosen attributes ==========================================\nif 'SoldPrice_log' in attributes_drop:\n    attributes_drop.remove('SoldPrice_log')\n\n# One-hot encoding for categorical data =========================================\ntest_X = pd.get_dummies(test_X, dummy_na=False)\n\n# ===============================================================================\n# Ensure test data is encoded in the same manner as training data with align command\nfinal_train, final_test = X.align(test_X, join='left', axis=1)\n\n# Imputer for all other missing values in test data. Note: Do not 'fit_transform'\nfinal_test_imputed_1 = imputer.transform(final_test[:final_test.shape[0]\/\/4])","0409534d":"# make predictions which we will submit. \ntest_preds_1 = final_xgb_model.predict(final_test_imputed_1)\n# test_preds_12 = final_gbr_model.predict(final_test_imputed_1)\n# test_preds_1 = np.add(0.5*test_preds_11, 0.5*test_preds_12)\ndel final_test_imputed_1\nfinal_test_imputed_2 = imputer.transform(final_test[final_test.shape[0]\/\/4:final_test.shape[0]\/\/2])\ntest_preds_2 = final_xgb_model.predict(final_test_imputed_2)\n# test_preds_22 = final_gbr_model.predict(final_test_imputed_2)\n# test_preds_2 = np.add(0.5*test_preds_21, 0.5*test_preds_22)\ndel final_test_imputed_2\nfinal_test_imputed_3 = imputer.transform(final_test[final_test.shape[0]\/\/2:3*final_test.shape[0]\/\/4])\ntest_preds_3 = final_xgb_model.predict(final_test_imputed_3)\n# test_preds_22 = final_gbr_model.predict(final_test_imputed_2)\n# test_preds_2 = np.add(0.5*test_preds_21, 0.5*test_preds_22)\ndel final_test_imputed_3\nfinal_test_imputed_4 = imputer.transform(final_test[3*final_test.shape[0]\/\/4:])\ntest_preds_4 = final_xgb_model.predict(final_test_imputed_4)\n# test_preds_22 = final_gbr_model.predict(final_test_imputed_2)\n# test_preds_2 = np.add(0.5*test_preds_21, 0.5*test_preds_22)\ndel final_test_imputed_4\ntest_preds = np.concatenate((test_preds_1, test_preds_2), axis=0)\ntest_preds = np.concatenate((test_preds, test_preds_3), axis=0)\ntest_preds = np.concatenate((test_preds, test_preds_4), axis=0)\n# The lines below shows you how to save your data in the format needed to score it in the competition\n# Reminder: predictions are in log(SalePrice). Need to inverse-transform.\noutput = pd.DataFrame({'Id': test_data.Id,\n                       'Sold Price': inv_y(test_preds)})\n\noutput.to_csv('submission.csv', index=False)","a61ee3ae":"print(output)","639f80de":"#### Use scikit-learn's function to grid search for the best combination of hyperparameters","a0170c12":"#### Perform feature selection, and encoding of categorical columns","8a1b9bdf":"#### Reminder: target is now log(SalePrice). After prediction call, need to inverse-transform to obtain SalePrice!","55ef9ac7":"The above shows that there appears to be 37 numerical columns, including the target, Sold Price.\n\nNotes:\n- Some potential anomalies in certain rows of the dataset may cause the column data type to become an 'object'. This may lead to an error in distinguishing between numerical and categorical columns. How can this be checked efficiently?\n- It is possible that there are numerical columns that have data in the form of discrete, and limited number of values. Such columns may also be interpreted as categorical data.\n\nThe 19 numerical columns have the following general characteristics:","3cf31c1c":"Import modules","284a894c":"### 1.3 Exploring categorical columns\n\n#### Examples of box plots of SalePrice versus categorical values","b6f2fc89":"#### Univariate analysis - box plots for numerical attributes","1569b9ab":"#### Notes for Data Cleaning & Preprocessing\nBased on a first viewing of the scatter plots against Sold Price, there appears to be:\n\n- A few outliers on the Year built (say, >9000 or =0) and Total Interior livable area (>1.75*1e8)\n\n- Total spaces (>400) and Garage spaces (>400)\n\n- Middle School Distance (>40) and High School Distance (>60)\n\n- Tax assessed value (>4) and Annual tax amount (>40000)\n\n- Listed Price (>3*1e8) and Last Sold Price (>8*1e7)\n\n- Zip (<86000)","1261c0ff":"## 2. Data Cleaning & Preprocessing","804b5d93":"With reference to the target Sold Price, the top correlated attributes are:","3967631b":"### 1.1 Preliminary observations\n\nShow examples from the dataset","4cfc553c":"#### Repeat pre-processing defined previously","c42a2dbf":"#### Numerical columns within the dataset","ecdcb13f":"#### Skew of target column\n\nIt appears to be good practice to minimise the skew of the dataset. The reason often given is that skewed data adversely affects the prediction accuracy of regression models.\n\nNote: While important for linear regression, correcting skew is not necessary for Decisions Trees and Random Forests.","303b4c4d":"#### Assess correlations amongst attributes\n\nThe linear correlation between two columns of data is shown below. There are various correlation calculation methods, but the Pearson correlation is often used and is the default method. It may be useful to note that:\n\n1. A combination of the correlation figure and a scatter plot can support the understanding of whether there is a non-linear correlation (i.e. depending on the data, this may result in a low value of linear correlation, but the variables may still be strongly correlated in a non-linear fashion)\n\n2. Correlation values may be heavily influenced by single outliers!\n\nSeveral authors have suggested that \"to use linear regression for modelling, it is necessary to remove correlated variables to improve your model\", and \"it's a good practice to remove correlated variables during feature selection\"\n\nBelow is a heatmap of the correlation of the numerical columns:","3eb3b069":"### 2.1 Dealing with missing\/null values","6862ff11":"## 5. Selection of Best Algorithm(s) & Fine-Tuning\n\n#### Create a Model and Make Predictions for the Competition\nRead the file of \"test\" data. And apply best model to make predictions\n\n#### Reminders:\n\n- Need to perform all transformations and normalisation, etc. to test data similar to when training the data\n- Make sure to inverse transform predictions to get predicted Sold Price","4330384b":"#### Make predictions for submission","61dc4996":"## 4. Preliminary Assessment of Machine Learning Algorithms","a521f3e8":"#### Notes for Data Cleaning & Preprocessing:\n\n- Perform log transformation on Sold Price\n- Feature variables that are skewed should also be investigated to asssess whether they require transformation","086561a8":"## 1. EDA\n\nThis part is working to:\n\n1. Gain a preliminary understanding of data\n\n2. Check missing\/null values","44954660":"### 1.2 Exploring numerical columns","a151bba6":"The \"shape\" of the dataset shows that it has 47439 rows\/instances, with data from 40 attributes.\nOut of the 40 attributes, one is the target (Sold Price) that the model should predict.\nHence, there are 39 attributes that may be used for feature selection\/engineering.","8de3d1e0":"#### Categorical columns within the dataset","e7d879a4":"Highly-correlated attributes include (left attribute has higher correlation with SoldPrice_log):\n\n- Listed Price and Last Sold Price (0.904)\n- Anual tax amount and Lot (1.000)\n- Total spaces and Garage spaces (0.966)\n- Elementary School Distance and Middle Scool Distance (0.821)\n- Full bathrooms and Bathrooms (0.868)\n\nPerhaps choose to drop the column with the lower correlation against SalePrice_log from the above pairs with more than 0.5 correlation.","a78def7a":"#### Distributions of attributes","c563689b":"##### Notes for Data Cleaning & Processing:\n\n- Not yet clear what to do with Full bathrooms missing values. Simple imputation with median? Full bathrooms correlation with bathrooms?\n- Last Sold Price is highly correlated with Listed Price.\n- Middle School Score has 16705 missing values, the same number as missing Middle Schooll Distance values. Drop?\n- High School Score has 5219 missing values, and the correlation between it and Sold Price is low.","ac339c03":"Show scatter plots for each numerical attribute (again, but different, less-efficient code) and show correlation value:","23475cf4":"### 2.3 Transforming data to reduce skew\n\nFor the moment, this is restricted to the target variable.","2151b2bf":"#### Missing\/null values in categorical columns","9a292cb0":"Reading iin input file","fae08a95":"##### Notes for Data Cleaning & Preprocessing:\n\nFor the moment, assume that Last Sold On to Summary attributes are missing as the houses do not have them. Hence, the missing values could be filled in with \"None\".\n\nRegion has 2 missing values,. Likely not to have masonry veneer. Hence, fill with 'None'","9a575856":"## 3. Feature Selection & Engineering\n\n#### Considering highly-correlated features\n\nFeeding highly-correlated features to machine algorithms may cause a reduction in performance. Hence, these are addressed below:","536a6764":"#### Import machine learning modules","4d4bf55c":"### 2.2 Addressing outliers","986d59c7":"#### Bivariate analysis - scatter plots for target versus numerical attributes","1ff5ab5f":"#### Missing\/null values in numerical columns","c05e820d":"#### Find Outliers\n\nVisualisation of data may support the discovery of possible outliers within the data. Examples of how this can be done include:\n\n1. Within univariate analysis, for example through using box plots. Outliers are observations more than a multiple (1.5-3) of the IQR (inter-quartile range) beyond the upper or lower quartile. (If data is skewed, it may be helpful to transform them first to a more symmetric distribution shape)\n\n2. Within bivariate analysis, for example scatterplots. Outliers have y-values that are unusual in relation to other observations with similar x-values. Alternatively, plots of the residuals from fitted least square line of bivariate regression can also indicate outliers.\n\nThe consensus is that all outliers should be carefully examined:\n\n- Go back to original data to check for recording or transcription errors\n\n- If no such errors, look carefully for unusual features of the individual unit to explain difference. This may lead to new theory\/discoveries\n\n- If data cannot be checked further, outlier is usually (often) dropped from the dataset.\n\nThe scatterplots of Sold Price against each numerical attribute is shown below, with the aim of employing method 2 above with bivariate analysis.","86688783":"Notes for Feature Selection & Engineering: Based on the scatter plots and correlation figures above, consider:\n\n- Excluding Tax assessed value - highly (0.99) and Last Sold Price -highly (0.88) correlated with  Annual tax amount, which has a higher corr with Price\n- Excluding Bathrooms - highly (0.87) correlated with Full bathrooms, which has a higher corr with Price\n- Excluding Listed Price - highly (0.81) correlated with Last Sold Price, which has a higher corr with Price\n- Excluding all attributes with low corr with Price and unclear non-linear correlation - e.g. Garage spaces Total spaces, Total interior livable area, Lot, Year built?","9c20813d":"##### Notes for Data Cleaning & Preprocessing:\n\nUni-modal, skewed distributions could potentially be log transformed:\n\nElementary School Distance, Middle School Distance, High School Distance, Tax assessed value, Annual tax amount, Last Sold Price\n\nAfter-note: This will be a future addition."}}