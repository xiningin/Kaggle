{"cell_type":{"7aaed2b6":"code","5befcbd7":"code","87bd8893":"code","40d3bd30":"code","65613ec6":"code","60ce69a8":"code","fa92548a":"code","a2ab252c":"code","55ec7e1a":"code","243ed024":"code","b98fc40c":"code","be8dc8bb":"code","3111655a":"code","3ca16721":"code","ded0c485":"code","5d7023ae":"code","a4c99698":"code","1ef81ca0":"code","5c6a29cf":"code","e726cee1":"code","51b09ffa":"code","1ef2e48d":"code","da3c7c7a":"code","1c89f0ea":"markdown","73a6483f":"markdown","a48e4c52":"markdown","72acce2f":"markdown","ae0942e3":"markdown","b687fc16":"markdown","53e99f41":"markdown","490b644b":"markdown","3c32c0ed":"markdown","b1f71409":"markdown","e50a95bc":"markdown","ff42a4d4":"markdown","ee34460d":"markdown","cddd5321":"markdown","58fb022d":"markdown","7f0dfd76":"markdown","5d8087b0":"markdown","ac2e5766":"markdown"},"source":{"7aaed2b6":"!pip install pmdarima","5befcbd7":"#Importing Libraries\n\n# linear algebra\nimport numpy as np \nimport math\n\n# data processing\nimport pandas as pd\n\n# data visualization(for EDA)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('ggplot')\nsns.set(color_codes=True)\nimport plotly.express as px\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n#For lstm model\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n\n#for Arima model\nfrom pmdarima.arima import auto_arima\nfrom statsmodels.tsa.stattools import adfuller\n\n","87bd8893":"df=pd.read_csv('..\/input\/rainfall-in-pakistan\/Rainfall_1901_2016_PAK.csv')\ndf.head()","40d3bd30":"#creating index column\ndf['Date']=pd.to_datetime(df[' Year'].astype(str)  + df['Month'], format='%Y%B').dt.to_period('m')\ndf = df.set_index('Date')\ndf = df.rename(columns = {'Rainfall - (MM)':'Rainfall',' Year':'Year'})\ndf.head()","65613ec6":"ax=df.groupby([df.Year]).mean()['Rainfall']\nfig = px.line(ax, x=ax.index, y='Rainfall', title='Annual rainfall in Pakistan from 1901 to 2016')\nfig.update_traces(mode='lines+markers',line=dict(color='Orange'))\nfig.update(layout=dict(title=dict(x=0.5)))\nfig.show()","60ce69a8":"ax=df.groupby('Year').mean()['Rainfall'].rolling(10).mean()\nfig = px.line(ax, x=ax.index, y='Rainfall', title='Rolling average of 10 years of Rainfall')\nfig.update_traces(mode='lines+markers',line=dict(color='Black'))\nfig.update(layout=dict(title=dict(x=0.5)))\nfig.show()","fa92548a":"ax=df.groupby([df.Month]).mean()['Rainfall']\nfig = px.bar(ax,y='Rainfall',title='Monthly Rainfall in Pakistan',category_orders={\"Month\": [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n      \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]},color_discrete_sequence=px.colors.qualitative.D3)\nfig.update(layout=dict(title=dict(x=0.5)))\nfig.show()","a2ab252c":"winter=df.query('Month==\"December\" or Month==\"January\" or Month==\"February\"').groupby([df.Year]).mean()['Rainfall']\nspring=df.query('Month==\"March\"or Month==\"April\"').groupby([df.Year]).mean()['Rainfall']\nsummer=df.query('Month==\"May\" or Month==\"June\" or Month==\"July\" or Month==\"August\"or Month==\"September\"').groupby([df.Year]).mean()['Rainfall']\nAutumn=df.query('Month==\"October\" or Month==\"November\"').groupby([df.Year]).mean()['Rainfall']\ndata=pd.DataFrame({ 'Winter': winter, 'Spring': spring,'Summer': summer, 'Autumn': Autumn })\ndata.plot(figsize=(17,8));\nplt.title('Seasonal Rainfall in Pakistan from 1901 to 2016',fontsize=20);\n\n","55ec7e1a":"y=data.mean()\nx=data.columns\nfig = px.bar(x=x,y=y,color=x,title='Season wise Rainfall in Pakistan')\nfig.update(layout=dict(title=dict(x=0.5)))\nfig.show()","243ed024":"ax=df.groupby([df.index.year]).agg({'Rainfall':sum})\nprint('The largest amount of rain was recorded in the following years')\nax['Rainfall'].nlargest(5)","b98fc40c":"#Data preprcessing\ndataset = df.drop(columns = ['Month','Year'])\n# normalize the dataset\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset = scaler.fit_transform(dataset)","be8dc8bb":"# convert an array of values into a dataset matrix\ndef create_dataset(dataset, look_back=12):\n\tdataX, dataY = [], []\n\tfor i in range(len(dataset)-look_back-1):\n\t\ta = dataset[i:(i+look_back), 0]\n\t\tdataX.append(a)\n\t\tdataY.append(dataset[i + look_back, 0])\n\treturn np.array(dataX), np.array(dataY)\n","3111655a":"\n\n# split into train and test sets\ntraining_size=int(len(dataset)*0.90)\ntest_size=len(dataset)-training_size\ntrain, test =dataset[0:training_size,:],dataset[training_size:len(dataset),:1]\n\n#reshape into X=t and Y=t+1\nlook_back =12\ntrainX, trainY = create_dataset(train, look_back)\ntestX, testY = create_dataset(test, look_back)\n","3ca16721":"# reshape input to be [samples, time steps, features]\ntrainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))","ded0c485":"\n# create and fit the LSTM network\nmodel=Sequential()\nmodel.add(LSTM(50,return_sequences=True,input_shape=(1,look_back)))\nmodel.add(LSTM(50,return_sequences=True))\nmodel.add(LSTM(50))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nhistory = model.fit(trainX, trainY, epochs=600, batch_size=200, validation_data=(testX, testY), shuffle=False)\n","5d7023ae":"\n# plot train and validation loss\nimport matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model train vs validation loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()\n","a4c99698":"\ntestPredict = model.predict(testX)\n# invert predictions\ntest_Predict = scaler.inverse_transform(testPredict)\ntest_Y = scaler.inverse_transform([testY])\n# calculate root mean squared error\ntestScore = math.sqrt(mean_squared_error(test_Y[0], test_Predict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","1ef81ca0":"\ndata = df.drop(columns = ['Month','Year'])\ntestindex=data[-(testY.size):]\nfuture_forecast = pd.DataFrame(test_Predict[:,0],index =testindex.index,columns=['Prediction'])\npd.concat([testindex,future_forecast],axis=1).iplot()","5c6a29cf":"test_result=adfuller(df['Rainfall'])\ndef adfuller_test(sales):\n    result=adfuller(sales)\n    labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations']\n    for value,label in zip(result,labels):\n        print(label+' : '+str(value) )\n\n    if result[1] <= 0.05:\n        print(\"strong evidence against the null hypothesis(Ho), reject the null hypothesis. Data is stationary\")\n    else:\n        print(\"weak evidence against null hypothesis,indicating it is non-stationary \")\n\nadfuller_test(df['Rainfall'])\n","e726cee1":"# split into train and test sets\ntraining_size=int(len(df)*0.90)\ntest_size=len(df)-training_size\ntrain,valid=df.iloc[0:training_size,:],df.iloc[training_size:len(df),:1]\ntraining = train['Rainfall']\nvalidation = valid['Rainfall']","51b09ffa":"model = auto_arima(training, start_p=0, start_q=0,max_p=3, max_q=3, m=12,start_P=0,start_Q=0,max_P=1,max_Q=1, seasonal=True,d=0, D=1, trace=True,error_action='ignore',suppress_warnings=True)\nmodel.fit(training)","1ef2e48d":"\nfuture_forecast = model.predict(len(valid))\nfuture_forecast = pd.DataFrame(future_forecast,index =valid.index,columns=['Prediction'])\npd.concat([valid,future_forecast],axis=1).iplot()","da3c7c7a":"testScore = math.sqrt(mean_squared_error(valid,future_forecast))\nprint('Test Score: %.2f RMSE' % (testScore))","1c89f0ea":" To analysis the progression level of rainfall, I took the rolling average of 10 years.","73a6483f":"ARIMA has three components \u2013 AR (autoregressive term), I (differencing term) and MA (moving average term). \n\n1. AR term refers to the past values used for forecasting the next value and it represented by p.\n1. MA term is used to defines number of past forecast errors used to predict the future values and it represented by q.\n1. Order of differencing  specifies the number of times the differencing operation is performed on series to make it stationary and it represented by d.\n\n**I'm using auto arima in which i don't have to select the combination of p, q, d.Because the model select the best combination of these parameters.**","a48e4c52":"The LSTM network expects the input data (X) to be provided with a specific array structure in the form of: (samples, time steps, features) but Currently, our data is in the form: (samples, features)","72acce2f":"<h1 style=\"background-color:#ADD8E6; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">LSTM Model<\/h1>\n","ae0942e3":"\nThe dataset contains 116-year rainfall data of Pakistan.To predict the rainfall i'm using the ARIMA and LSTM models. The parameters considered for the evaluation of the performance and the efficiency of the proposed rainfall prediction model are Root Mean Square Error (RMSE).Notebook summary\n* Importing and Data cleaning\n* Exploratory Data Analysis\n* Forecast across the test set using an ARIMA model.\n* Forecast across the test set using an LSTM model and examine\n","b687fc16":"We are now ready to design and fit our LSTM network.","53e99f41":"<h1 style=\"background-color:#ADD8E6; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">ARIMA Model<\/h1>","490b644b":"Converting  an array of values into a dataset matrix. The function takes two arguments: the dataset, which is a NumPy array that we want to convert into a dataset, and the look_back, which is the number of previous time steps to use as input variables to predict the next time period and in our case look back is 1.","3c32c0ed":"<h1 style=\"background-color:#ADD8E6; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">Introduction<\/h1>","b1f71409":"###  A plot of learning curves \nTrain and Validation Learning Curves Showing a Good Fit because The plot of validation loss decreases and has a small gap with the training loss.","e50a95bc":"<h1 style=\"background-color:#ADD8E6; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">Exploratory Data Analysis<\/h1>\n","ff42a4d4":"As we are working on univariate time series and Data is also stationary.So we can use Arima model.","ee34460d":"Pakistan has four seasons:  winter from December through February; spring from March through May; the summer rainy season, or southwest monsoon period, from June through September; and the Autumn period is in October and November.\nFrom the graph, it is clear that Pakistan received more rainfall in the summer and spring seasons.\n","cddd5321":"From the graph, we can see that the highest average rainfall in Pakistan was recorded in the year 1944.","58fb022d":"From the figure we can see that the majority of the rainfall is received in the months of July and August which is the monsoon season while in October and November the least Rainfall is recorded.","7f0dfd76":"<h1 style=\"background-color:#ADD8E6; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">Importing and Data cleaning<\/h1>\n","5d8087b0":"The model is fit, now we can estimate the performance of the model on the test datasets.","ac2e5766":"**ARIMA models work on the following assumptions \u2013**\n\n1. The data series is stationary, which means that the mean and variance should not vary with time.\n1. The data provided as input must be a univariate series, since arima uses the past values to predict the future values.\n\n**To identify the nature of data, we will be using the null hypothesis.**\n\n* Ho: It is non-stationary\n* H1: It is stationary\nWe will be considering the null hypothesis that data is not stationary and the alternate hypothesis that data is stationary.\n\n"}}