{"cell_type":{"2c559f5f":"code","21d698c4":"code","2db237d5":"code","b7ce66dd":"code","44554a21":"code","7aa576c3":"code","d558b6dd":"code","18336075":"code","1e89da30":"code","d6befa6d":"code","21ec74c4":"code","9d54ef3b":"code","9d9ef86b":"code","fb0a7af4":"code","f61281df":"code","c19b24cb":"code","13359b7f":"code","28a108ba":"code","aff3d0d7":"code","d2dfa7ab":"code","1bf7f9a5":"code","4bd564a8":"code","c7030ec7":"code","df2309ce":"code","1d052a29":"code","4cfd6bde":"code","53fed06d":"code","2bd3180b":"code","1b173ba0":"code","c175e561":"code","4e4d4190":"code","96d39669":"code","7040b484":"code","f5d1fc8e":"code","b4503661":"code","a504537d":"code","1911d1e0":"code","b20df5e1":"code","9dd83b1f":"code","70e28980":"code","13952b0f":"code","0082392d":"code","5b8403bd":"code","69d8ae4d":"code","818cffb2":"code","6431212e":"code","7c5ed70e":"code","2e795aee":"code","a4883455":"code","a1feb53f":"code","c687d193":"code","5b6b25aa":"code","8c1a2e10":"code","8dea8019":"code","1a319fc3":"code","892690f6":"code","3826be9c":"code","3ff7b673":"code","7a78939d":"code","28d25cfe":"code","3f880901":"code","b3e82a0b":"code","ece0f054":"code","869c55b5":"code","65e27309":"code","11e41604":"code","d1ddf3ee":"code","043e31e5":"code","13314727":"code","4fa46abe":"code","93226b05":"code","6a9ecb0d":"code","8fa23f39":"code","c794f44c":"code","7ec79e37":"code","e2f4455a":"code","657ed1a9":"code","21d6ea48":"code","e1de5005":"code","9e7a20ec":"code","42588fa6":"code","6afb0090":"code","381dffd7":"code","0e301d23":"code","0b454843":"code","2e5cbd5c":"code","90df466e":"code","aceee8d1":"code","94a3d88f":"code","5fe9579c":"code","d579aef6":"code","d9db951d":"code","79972591":"code","e3a25381":"code","f5686907":"code","3d11a1cc":"code","58644cb1":"code","6ab7c9b8":"code","0f3cab5d":"code","f40e0246":"code","8736d66a":"code","e8e393f6":"code","6c950a01":"code","fdde9042":"code","ce53ced6":"code","e5147006":"code","f2757740":"code","8b204642":"code","51f16e58":"code","8e0ddf2c":"code","eb9f2d46":"code","3c5a9211":"code","03f9fd3d":"code","1510a6d5":"code","3b852c9a":"code","de27b61e":"code","a4a705c7":"code","ae825400":"code","70fccb4a":"code","63fada5e":"code","262c81ec":"code","ee2b5b02":"code","255a7da8":"code","94af5ac5":"code","eee86442":"code","98e79959":"markdown","dbf65390":"markdown","99fbe017":"markdown","104062d7":"markdown","5f76a84c":"markdown","90ccb5dd":"markdown","fc3571b0":"markdown","d81c204d":"markdown","7196795a":"markdown","70aab3b1":"markdown","fd48da27":"markdown","ca04e254":"markdown","7fc20dfe":"markdown","51e05d33":"markdown","276f62c9":"markdown","b9a27b82":"markdown","15a5ee15":"markdown","ff26824a":"markdown","eefef70e":"markdown","94635bc5":"markdown","3417d8f1":"markdown","6b0acabb":"markdown","3c3626e8":"markdown","a2644c72":"markdown"},"source":{"2c559f5f":"# Import libraries\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","21d698c4":"df=pd.read_csv('..\/input\/machine-learning-24-hrs-hackathon\/train_SJC.csv')\n# import data","2db237d5":"df.info()\n# Columns need to be named correctly","b7ce66dd":"df.columns = df.iloc[0]\n# Rename all the columns with first row","44554a21":"df.head()","7aa576c3":"column_indices = [2,7,11]\nnew_names = ['DateReported','DependentsOther','DaysWorkedPerWeek']\nold_names = df.columns[column_indices]\ndf.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n# Rename three columns whose column names were nan","d558b6dd":"df.head()","18336075":"df = df.iloc[1:]\n# Delete first row","1e89da30":"df.drop_duplicates(inplace=True)\n# No duplicate rows in this case","d6befa6d":"df.info()\n# data has missing values and data types are not appropriate","21ec74c4":"df[['Age', 'DependentChildren','WeeklyWages','HoursWorkedPerWeek','InitialIncurredCalimsCost','UltimateIncurredClaimCost']] = df[['Age', 'DependentChildren','WeeklyWages','HoursWorkedPerWeek','InitialIncurredCalimsCost','UltimateIncurredClaimCost']].astype(float)\n\n# While downloading dataset, due to presence of first row many column data types are not correct","9d54ef3b":"df[\"DateTimeOfAccident\"]=pd.to_datetime(df[\"DateTimeOfAccident\"])\ndf[\"DateReported\"]=pd.to_datetime(df[\"DateReported\"])","9d9ef86b":"df.dtypes","fb0a7af4":"df.loc[:,['DateTimeOfAccident','DateReported']]","f61281df":"# define functions\n\n# Create Data audit Report for continuous variables\ndef continuous_var_summary(x):\n    return pd.Series([x.count(), x.isnull().sum(), x.sum(), x.mean(), x.median(),  \n                      x.std(), x.var(), x.min(), x.quantile(0.01), x.quantile(0.05),\n                          x.quantile(0.10),x.quantile(0.25),x.quantile(0.50),x.quantile(0.75), \n                              x.quantile(0.90),x.quantile(0.95), x.quantile(0.98), x.quantile(0.99).round(2),x.max()], \n                  index = ['N', 'NMISS', 'SUM', 'MEAN','MEDIAN', 'STD', 'VAR', 'MIN', 'P1', \n                            'P5' ,'P10' ,'P25' ,'P50' ,'P75' ,'P90' ,'P95' ,'P98','P99' ,'MAX'])\n\n# Create Data audit Report for categorical variables\ndef categorical_var_summary(x):\n    Mode = x.value_counts().sort_values(ascending = False)[0:1].reset_index()\n    return pd.Series([x.count(), x.isnull().sum(), Mode.iloc[0, 0], Mode.iloc[0, 1], \n                          round(Mode.iloc[0, 1] * 100\/x.count(), 2)], \n                  index = ['N', 'NMISS', 'MODE', 'FREQ', 'PERCENT'])\n    \n\n# An utility function to create dummy variable\ndef create_dummies(df, colname):\n    col_dummies = pd.get_dummies(df[colname], prefix = colname, drop_first = True)\n    df = pd.concat([df, col_dummies], axis = 1)\n    df.drop(colname, axis = 1, inplace = True )\n    return df\n\n# Missing value imputation for categorical and continuous variables\ndef missing_imputation(x, stats = 'mean'):\n    if (x.dtypes == 'float64'):   # | (x.dtypes == 'int64')\n        x = x.fillna(x.mean()) #  if stats == 'mean' else x.fillna(x.median()\n    else:\n        x = x.fillna(x.mode())\n    return x","c19b24cb":"# Continuous features \ndf.describe().T\n# We aren't able to observe change in last few percentiles, so we will create a function to get detailed analysis","13359b7f":"df[df['HoursWorkedPerWeek']>500].shape","28a108ba":"sns.boxplot(df[\"HoursWorkedPerWeek\"])","aff3d0d7":"df[df['WeeklyWages']>5000.0].shape\n# Since there are 1800 rows which shows that we will not be able to predict high claim cost if we do outlier treatment, so we can ","d2dfa7ab":"sns.boxplot(df[\"WeeklyWages\"])","1bf7f9a5":"df[df['UltimateIncurredClaimCost']>1373660.7]\n","4bd564a8":"sns.boxplot(df[\"UltimateIncurredClaimCost\"])\n# Since one data point gives a very high value we will treat it as outlier and remove it from data","c7030ec7":"df = df[df.ClaimNumber != 'WC9901999']","df2309ce":"df.drop(['ClaimNumber'],axis=1,inplace=True)\n# since it is unique, this can be dropped","1d052a29":"df_conti = df.loc[:, (df.dtypes == 'float64') | (df.dtypes == 'int64')]\ndf_cat = df.loc[:,(df.dtypes == 'object')|(df.dtypes==\"category\")]","4cfd6bde":"df_conti.apply(continuous_var_summary).T.round(1) ","53fed06d":"df_conti= df_conti.apply(lambda x: x.clip(lower = x.dropna().quantile(0.01), upper = x.quantile(0.99)))\ndf_conti.apply(continuous_var_summary).T.round(1)\n# Outlier treatment at 99th percentile","2bd3180b":"df.isnull().sum().plot(kind='bar')\nplt.show()","1b173ba0":"df.columns[df.isna().any()]","c175e561":"# df['WeeklyWages'] = df['WeeklyWages'].fillna((df['WeeklyWages'].mean()))\n# df['HoursWorkedPerWeek'] = df['HoursWorkedPerWeek'].fillna((df['HoursWorkedPerWeek'].mean()))\n# df['MaritalStatus'] = df['MaritalStatus'].fillna((df['MaritalStatus'].mode()[0]))\n\n# Individual treatment of columns may create problem in pre-processing of test data, so we will try to use something more general","4e4d4190":"df_cat['MaritalStatus'] = df_cat['MaritalStatus'].fillna('U')\n# Because of presence of a category U, we will replace nan with U","96d39669":"from statistics import mode\ndf_conti= df_conti.apply(missing_imputation)\ndf_cat = df_cat.apply(missing_imputation)","7040b484":"df.info()\n# No missing values","f5d1fc8e":"# Categorical features \ndf.describe(include=['object']).T","b4503661":"df_new = pd.concat([df_conti, df_cat,df.loc[:,['DateTimeOfAccident','DateReported']]], axis=1)","a504537d":"df_new.head()","1911d1e0":"df_new.columns","b20df5e1":"df_processed=df_new.copy()","9dd83b1f":"# df_new=df_processed.copy()","70e28980":"df_new.drop(['ClaimDescription','DateTimeOfAccident', 'DateReported'], axis=1, inplace=True)","13952b0f":"df_new.head()","0082392d":"target_train=df_new['UltimateIncurredClaimCost']\ntarget_train","5b8403bd":"x_df=df_new.loc[:, df_new.columns != 'UltimateIncurredClaimCost']","69d8ae4d":"import sklearn.preprocessing as pre\nimport sklearn.model_selection as ms\nimport sklearn.linear_model as lm","818cffb2":"le=pre.LabelEncoder()\nfor x in x_df.select_dtypes(include='object').columns.tolist():\n    x_df[x]=le.fit_transform(x_df[x])","6431212e":"X_scale=pre.minmax_scale(x_df)\nY=target_train","7c5ed70e":"x_train,x_test,y_train,y_test=ms.train_test_split(X_scale,Y,test_size=0.3,random_state=1234457)","2e795aee":"x_train.shape,x_test.shape,y_train.shape,y_test.shape","a4883455":"glm=lm.LinearRegression()","a1feb53f":"glm.fit(x_train,y_train)","c687d193":"glm.score(x_test,y_test)","5b6b25aa":"glm.score(x_train,y_train)","8c1a2e10":"import sklearn.metrics as mt\na=mt.mean_squared_error(y_pred=glm.predict(x_test),y_true=y_test)\nimport math\nmath.sqrt(a)","8dea8019":"import sklearn.neighbors as NN","1a319fc3":"KNN=NN.KNeighborsRegressor(n_neighbors=7)","892690f6":"KNN.fit(x_train,y_train)","3826be9c":"KNN.predict(x_test)","3ff7b673":"import sklearn.metrics as mt\n\nimport math\n\na=mt.mean_squared_error(y_pred=KNN.predict(x_test),y_true=y_test)\nmath.sqrt(a)","7a78939d":"# Import the model we are using\nfrom sklearn.ensemble import RandomForestRegressor\n# Instantiate model with 500 decision trees\nrf = RandomForestRegressor(n_estimators = 500, random_state = 42351, max_depth=5)\n# Train the model on training data\nrf.fit(x_train,y_train)\n# ,max_leaf_nodes=7,min_samples_split=50","28d25cfe":"# Use the forest's predict method on the test data\npredictions = rf.predict(x_test)","3f880901":"a=mt.mean_squared_error(y_pred=rf.predict(x_test),y_true=y_test)\nmath.sqrt(a)","b3e82a0b":"rf.score(x_train,y_train)","ece0f054":"rf.score(x_test,y_test)","869c55b5":"df_1=df_processed.copy()","65e27309":"df_1['YearOfAccident']  = pd.DatetimeIndex(df_1['DateTimeOfAccident']).year\ndf_1['MonthOfAccident']  = pd.DatetimeIndex(df_1['DateTimeOfAccident']).month\ndf_1['DayOfAccident']  = pd.DatetimeIndex(df_1['DateTimeOfAccident']).day\ndf_1['WeekdayOfAccident']  = pd.DatetimeIndex(df_1['DateTimeOfAccident']).day_name()\ndf_1['HourOfAccident']  = pd.DatetimeIndex(df_1['DateTimeOfAccident']).hour\ndf_1['YearReported']  = pd.DatetimeIndex(df_1['DateReported']).year\ndf_1['DaysDelayed'] = (pd.DatetimeIndex(df_1['DateReported']).date - pd.DatetimeIndex(df_1['DateTimeOfAccident']).date)","11e41604":"df_1.dtypes","d1ddf3ee":"df_1['DaysDelayed']=df_1['DaysDelayed'].dt.days","043e31e5":"df_1.head()","13314727":"df_1.drop(['ClaimDescription','DateTimeOfAccident', 'DateReported'], axis=1, inplace=True)","4fa46abe":"df_1.head()","93226b05":"target_train=df_1['UltimateIncurredClaimCost']\ntarget_train","6a9ecb0d":"x_df=df_1.loc[:, df_1.columns != 'UltimateIncurredClaimCost']","8fa23f39":"le=pre.LabelEncoder()\nfor x in x_df.select_dtypes(include='object').columns.tolist():\n    x_df[x]=le.fit_transform(x_df[x])","c794f44c":"x_df.head()","7ec79e37":"X_scale=pre.minmax_scale(x_df)\nY=target_train","e2f4455a":"x_train,x_test,y_train,y_test=ms.train_test_split(x_df,Y,test_size=0.3,random_state=1234457)","657ed1a9":"x_train.shape,x_test.shape,y_train.shape,y_test.shape","21d6ea48":"import sklearn.preprocessing as pre\nimport sklearn.model_selection as ms\nimport sklearn.linear_model as lm","e1de5005":"glm_=lm.LinearRegression()","9e7a20ec":"glm_.fit(x_train,y_train)","42588fa6":"a=mt.mean_squared_error(y_pred=glm_.predict(x_test),y_true=y_test)\nmath.sqrt(a)","6afb0090":"glm_.score(x_test,y_test)\n","381dffd7":"glm_.score(x_train,y_train)","0e301d23":"# Import the model we are using\nfrom sklearn.ensemble import RandomForestRegressor\n# Instantiate model with 1000 decision trees\nrf_ = RandomForestRegressor(n_estimators = 500, random_state = 42351, max_depth=5)\n# Train the model on training data\nrf_.fit(x_train,y_train);","0b454843":"# Use the forest's predict method on the test data\npredictions = rf_.predict(x_test)","2e5cbd5c":"a=mt.mean_squared_error(y_pred=rf_.predict(x_test),y_true=y_test)\nmath.sqrt(a)","90df466e":"rf_.score(x_train,y_train)","aceee8d1":"rf_.score(x_test,y_test)","94a3d88f":"df_11=df_processed.copy()","5fe9579c":"df_11.columns","d579aef6":"df_11['HoursWorkedPerWeek']=pd.cut(df_11['HoursWorkedPerWeek'],bins=[1,20,35,40,80],labels=['less','mid','okay','high'])\ndf_11['DaysWorkedPerWeek']=pd.cut(df_11['DaysWorkedPerWeek'],bins=[1,4,5,7],labels=['less','okay','high'])\ndf_11['Age']=pd.cut(df_11['Age'],bins=[1,20,35,40,80],labels=['less','mid','okay','high'])\n# transformation will help in better prediction","d9db951d":"df_11['YearOfAccident']  = pd.DatetimeIndex(df_11['DateTimeOfAccident']).year\ndf_11['MonthOfAccident']  = pd.DatetimeIndex(df_11['DateTimeOfAccident']).month\ndf_11['DayOfAccident']  = pd.DatetimeIndex(df_11['DateTimeOfAccident']).day\ndf_11['WeekdayOfAccident']  = pd.DatetimeIndex(df_11['DateTimeOfAccident']).day_name()\ndf_11['HourOfAccident']  = pd.DatetimeIndex(df_11['DateTimeOfAccident']).hour\ndf_11['YearReported']  = pd.DatetimeIndex(df_11['DateReported']).year\ndf_11['DaysDelayed'] = (pd.DatetimeIndex(df_11['DateReported']).date - pd.DatetimeIndex(df_11['DateTimeOfAccident']).date)","79972591":"df_11['DaysDelayed']=df_11['DaysDelayed'].dt.days","e3a25381":"df_11.dtypes","f5686907":"df_11.corr()","3d11a1cc":"# high correlation 'YearReported','YearOfAccident'","58644cb1":"df_11.drop(['DateTimeOfAccident','DateReported','ClaimDescription','YearOfAccident'],axis=1,inplace=True)","6ab7c9b8":"df_11['MonthOfAccident']=df_11['MonthOfAccident'].astype('str')\ndf_11['WeekdayOfAccident']=df_11['WeekdayOfAccident'].astype('str')\ndf_11['DependentChildren']=df_11['DependentChildren'].astype('str')\ndf_11['DependentsOther']=df_11['DependentsOther'].astype('str')","0f3cab5d":"df_11.columns","f40e0246":"df_conti = df_11.loc[:, (df_11.dtypes == 'float64') | (df_11.dtypes == 'int64')]\ndf_cat = df_11.loc[:,(df_11.dtypes == 'object')|(df_11.dtypes==\"category\")]","8736d66a":"df_conti.columns","e8e393f6":"for c_feature in df_cat:\n    df_cat[c_feature] = df_cat[c_feature].astype('category')\n    df_cat = create_dummies(df_cat,c_feature)","6c950a01":"df_cat.columns","fdde9042":"df_11 = pd.concat([df_conti, df_cat], axis=1)","ce53ced6":"df_11.dtypes","e5147006":"df_11.head()","f2757740":"target_train=df_11['UltimateIncurredClaimCost']\ntarget_train","8b204642":"x_df=df_11.loc[:, df_11.columns != 'UltimateIncurredClaimCost']","51f16e58":"# le=pre.LabelEncoder()\n# for x in x_df.select_dtypes(include='object').columns.tolist():\n#     x_df[x]=le.fit_transform(x_df[x])","8e0ddf2c":"x_df.head()","eb9f2d46":"# X_scale=pre.minmax_scale(x_df)\nY=target_train","3c5a9211":"x_train,x_test,y_train,y_test=ms.train_test_split(x_df,Y,test_size=0.3,random_state=1234457)","03f9fd3d":"x_train.shape,x_test.shape,y_train.shape,y_test.shape","1510a6d5":"import sklearn.preprocessing as pre\nimport sklearn.model_selection as ms\nimport sklearn.linear_model as lm","3b852c9a":"glm__=lm.LinearRegression()","de27b61e":"glm__.fit(x_train,y_train)","a4a705c7":"glm__.score(x_test,y_test)\n","ae825400":"glm__.score(x_train,y_train)","70fccb4a":"# Import the model we are using\nfrom sklearn.ensemble import RandomForestRegressor\n# Instantiate model with 1000 decision trees\nrf__ = RandomForestRegressor(n_estimators = 500, random_state = 42351, max_depth=15,max_leaf_nodes=7,min_samples_split=50)\n# Train the model on training data\nrf__.fit(x_train,y_train);","63fada5e":"import sklearn.metrics as mt\na=mt.mean_squared_error(y_pred=rf__.predict(x_test),y_true=y_test)\nimport math\nmath.sqrt(a)","262c81ec":"rf__.score(x_train,y_train)","ee2b5b02":"rf__.score(x_test,y_test)","255a7da8":"df_test=pd.read_csv('..\/input\/machine-learning-24-hrs-hackathon\/Test_SJC.csv')","94af5ac5":"# Changing data type\ndf_test[['Age', 'DependentChildren','WeeklyWages','HoursWorkedPerWeek','InitialIncurredCalimsCost']] = df_test[['Age', 'DependentChildren','WeeklyWages','HoursWorkedPerWeek','InitialIncurredCalimsCost']].astype(float)\ndf_test[\"DateTimeOfAccident\"]=pd.to_datetime(df_test[\"DateTimeOfAccident\"])\ndf_test[\"DateReported\"]=pd.to_datetime(df_test[\"DateReported\"])\ndf_test.drop(['ClaimNumber'],axis=1,inplace=True)\n\n# Outlier treatment\ndf_test_conti = df_test.loc[:, (df_test.dtypes == 'float64') | (df_test.dtypes == 'int64')]\ndf_test_cat = df_test.loc[:,(df_test.dtypes == 'object')|(df_test.dtypes==\"category\")]\n\ndf_test_conti= df_test_conti.apply(lambda x: x.clip(lower = x.dropna().quantile(0.01), upper = x.quantile(0.99)))\n# df_test_conti.apply(continuous_var_summary).T.round(1)\n# Outlier treatment at 99th percentile\n\n# Missing value treatment\ndf_test_cat['MaritalStatus'] = df_test_cat['MaritalStatus'].fillna('U')\n\ndf_test_conti= df_test_conti.apply(missing_imputation)\ndf_test_cat = df_test_cat.apply(missing_imputation)\n\ndf_new_test = pd.concat([df_test_conti, df_test_cat,df_test.loc[:,['DateTimeOfAccident','DateReported']]], axis=1)\n\n# Removing some variables\ndf_new_test.drop(['ClaimDescription','DateTimeOfAccident', 'DateReported'], axis=1, inplace=True)\n\n\n# Label-encoding for categorical variables\n# le=pre.LabelEncoder()\nfor x in df_new_test.select_dtypes(include='object').columns.tolist():\n    df_new_test[x]=le.fit_transform(df_new_test[x])\n    \nX_scale_test=pre.minmax_scale(df_new_test)\n\n# This model has been used for prediction","eee86442":"predictions = glm.predict(X_scale_test)\ncsv = pd.read_csv(\"..\/input\/machine-learning-24-hrs-hackathon\/sample_submission.csv\")\ncsv[\"UltimateIncurredClaimCost\"]=predictions\ncsv.to_csv(\"20BDA68.csv\", index = False)","98e79959":"## Model made by dropping columns","dbf65390":"## Renaming Columns","99fbe017":"# ETL","104062d7":"## Random Forest","5f76a84c":"# data for modelling","90ccb5dd":"# Creating columns from Date-time columns","fc3571b0":"## KNN","d81c204d":"# data for modelling","7196795a":"## Testing dataset","70aab3b1":"### Here, I have tried to use both linear regression and random forest model on different sets of pre-processed data, but I finalized my model based on simplicity to explain model to client. And even though we increase complexity results doesn't differ much. So, I have chosen the least complex model.\n","fd48da27":"# Other models that I tried but haven't been used for final prediction","ca04e254":"## Linear regression model","7fc20dfe":"# data for modelling","51e05d33":"### Creating dummy variables","276f62c9":"## Linear regression model","b9a27b82":"## Random Forest","15a5ee15":"## Outlier treatment","ff26824a":"## Missing value treatment","eefef70e":"# Date-time column Transformations","94635bc5":"## Random Forest","3417d8f1":"## Linear regression model","6b0acabb":"## Changing data type","3c3626e8":"## Removing Duplicate rows","a2644c72":"# Data Visualization"}}