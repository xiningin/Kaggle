{"cell_type":{"e2c5ebf3":"code","0f14e0b4":"code","e515b412":"code","6e636b69":"code","08bb0fae":"code","a9667395":"code","7fd9d30f":"code","b0e6ee01":"code","892b6db9":"code","25400f95":"code","4ee910ec":"code","423ba5d1":"code","1b4f73c1":"code","21af7e2f":"code","51c82324":"code","c8b1753b":"code","f75ddb2e":"code","0d3274c3":"code","494638ee":"code","9f9d48bb":"code","2de624dd":"code","af7dedde":"code","895afc1d":"code","3eb82f2f":"code","5584473a":"code","d3a2bb9c":"code","7972c20a":"markdown","b840e097":"markdown","d1482d6d":"markdown","4065e9a6":"markdown","ccb5f9a3":"markdown","61fcefc2":"markdown","8d7fbcff":"markdown","6805e1d6":"markdown"},"source":{"e2c5ebf3":"!pip3 install transformers==4.11.2","0f14e0b4":"import os\nimport os.path as osp\n\nimport pandas as pd\n\nimport torch\n\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering","e515b412":"INPUT_PATH = '..\/input\/chaii-hindi-and-tamil-question-answering\/'\n\ntrain = pd.read_csv(osp.join(INPUT_PATH, 'train.csv'))\ntest = pd.read_csv(osp.join(INPUT_PATH, 'test.csv'))\nsub = pd.read_csv(osp.join(INPUT_PATH, 'sample_submission.csv'))","6e636b69":"train_df, val_df = train[:round(len(train) * 0.8)], train[round(len(train) * 0.8):]","08bb0fae":"# https:\/\/huggingface.co\/transformers\/usage.html#extractive-question-answering\n\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n\ntext = r\"\"\"\n\ud83e\udd17 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\narchitectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet\u2026) for Natural Language Understanding (NLU) and Natural\nLanguage Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\nTensorFlow 2.0 and PyTorch.\n\"\"\"\n\nquestions = [\n    \"How many pretrained models are available in Transformers?\",\n    \"What does Transformers provide?\",\n    \"Transformers provides interoperability between which frameworks?\",\n]\n\nfor question in questions:\n    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].tolist()[0]\n\n    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    result = model(**inputs)\n    answer_start_scores = result['start_logits']\n    answer_end_scores = result['end_logits']\n\n    answer_start = torch.argmax(\n        answer_start_scores\n    )  # Get the most likely beginning of answer with the argmax of the score\n    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n\n    print(f\"Question: {question}\")\n    print(f\"Answer: {answer}\\n\")","a9667395":"# https:\/\/huggingface.co\/transformers\/custom_datasets.html#question-answering-with-squad-2-0\n\ndef read_dataset(df:pd.DataFrame):\n    \n    contexts = []\n    questions = []\n    answers = []\n    \n    for i, data in df.iterrows():\n        contexts.append(data['context'])\n        questions.append(data['question'])\n        \n        answer = {}\n        answer['text'] = data['answer_text']\n        answer['answer_start'] = data['answer_start']\n        answer['answer_end'] = data['answer_start'] + len(data['answer_text'])\n        answers.append(answer)\n    \n    return contexts, questions, answers","7fd9d30f":"train_contexts, train_questions, train_answers = read_dataset(train_df)\nval_contexts, val_questions, val_answers = read_dataset(val_df)","b0e6ee01":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('deepset\/xlm-roberta-large-squad2')","892b6db9":"from transformers import AutoTokenizer\n\ntrain_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\nval_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)","25400f95":"def add_token_positions(encodings, answers):\n    start_positions = []\n    end_positions = []\n    for i in range(len(answers)):\n        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n\n        # if start position is None, the answer passage has been truncated\n        if start_positions[-1] is None:\n            start_positions[-1] = tokenizer.model_max_length\n        if end_positions[-1] is None:\n            end_positions[-1] = tokenizer.model_max_length\n\n    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n\nadd_token_positions(train_encodings, train_answers)\nadd_token_positions(val_encodings, val_answers)","4ee910ec":"# https:\/\/huggingface.co\/transformers\/custom_datasets.html#question-answering-with-squad-2-0\n\nimport torch\n\nclass ChaiiDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\n    def __len__(self):\n        return len(self.encodings.input_ids)\n\ntrain_dataset = ChaiiDataset(train_encodings)\nval_dataset = ChaiiDataset(val_encodings)","423ba5d1":"# model = AutoModelForQuestionAnswering.from_pretrained(\"deepset\/xlm-roberta-base-squad2\")","1b4f73c1":"# # https:\/\/huggingface.co\/transformers\/custom_datasets.html#question-answering-with-squad-2-0\n\n# from torch.utils.data import DataLoader\n# from transformers import AdamW\n\n# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# model.to(device)\n# model.train()\n\n# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\n# optim = AdamW(model.parameters(), lr=5e-5)","21af7e2f":"# from tqdm import tqdm\n\n# for epoch in range(30):\n#     pbar = tqdm(train_loader)\n#     total_loss = 0\n#     for batch in pbar:\n#         optim.zero_grad()\n#         input_ids = batch['input_ids'][:,:512].to(device)\n#         attention_mask = batch['attention_mask'][:,:512].to(device)\n#         start_positions = batch['start_positions'].to(device)\n#         end_positions = batch['end_positions'].to(device)\n#         outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n#         loss = outputs[0]\n#         total_loss += loss\n#         loss.backward()\n#         optim.step()\n        \n#         pbar.set_description(f\"Loss : {round(loss.item(), 3)}\")\n#     print(f\"[{epoch+1} EPOCH] Total Loss : {round((total_loss \/ len(pbar)).item(), 4)}\\n\")\n","51c82324":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"\/kaggle\/input\/chaiick\/checkpoint_all\")\nmodel.to(device)","c8b1753b":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","f75ddb2e":"def find_index(target, base):\n    for i in range(len(base)):\n        if target == base[i:i + len(target)]:\n            return i,i+len(target)\n    return -1,-1","0d3274c3":"model.eval()\n\n\ndef test_idx(\n    df:pd.DataFrame,\n    idx:int, \n    is_valid:bool = False, \n    log:bool = False\n):\n    question = df.loc[idx]['question']\n    text = df.loc[idx]['context']\n    \n    if is_valid:\n        answer = df.loc[idx]['answer_text']\n    \n    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n    \n    if is_valid:\n        start, end = find_index(tokenizer(answer)[\"input_ids\"][1:-1], tokenizer(text)[\"input_ids\"])\n    input_ids = inputs[\"input_ids\"][:, :512].to(device)\n    attention_mask = inputs[\"attention_mask\"][:, :512].to(device)\n\n    result = model(input_ids, attention_mask=attention_mask)\n    answer_start_scores = result['start_logits']\n    answer_end_scores = result['end_logits']\n\n\n    answer_start = torch.argmax(\n        answer_start_scores\n    )  # Get the most likely beginning of answer with the argmax of the score\n    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n\n    answer = tokenizer.convert_tokens_to_string(\n        tokenizer.convert_ids_to_tokens(\n            input_ids[0][answer_start:answer_end]\n        )\n    )\n    \n    # delete bad tokens\n    bad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"\u2013\",  \",\", \";\"]\n    bad_endings = [\"...\", \"-\", \"(\", \")\", \"\u2013\", \",\", \";\"]\n    if any([answer.startswith(token) for token in bad_starts]):\n        answer = answer[1:]\n    if any([answer.endswith(token) for token in bad_endings]):\n        answer = answer[:-1]\n    \n    if is_valid:\n        score = jaccard(df.loc[idx][\"answer_text\"], answer)\n    df.loc[idx, 'predict'] = answer\n    if is_valid:\n        df.loc[idx, 'ans_start'] = start\n        df.loc[idx, 'ans_end'] = end\n        df.loc[idx, 'score'] = score\n    df.loc[idx, 'pred_start'] = find_index(tokenizer(answer)[\"input_ids\"][1:-1], tokenizer(text)[\"input_ids\"])[0]\n    \n\n    if log:\n        print(f'Answer[{df.iloc[idx][\"answer_start\"]} - {df.iloc[idx][\"answer_start\"] + len(val_df.iloc[idx][\"answer_text\"])}] : {df.iloc[idx][\"answer_text\"]}')\n        print(f'Prediction[{int(answer_start)} - {int(answer_end)}] : {answer}\\n')\n        print(f'Score : {score}\\n\\n')","494638ee":"val_df = val_df.reset_index(drop=True)\nval_df[['predict', 'ans_start', 'ans_end', 'pred_start', 'score']] = 0\n\nfor i in range(len(val_df)):\n    test_idx(val_df, i, is_valid=True)","9f9d48bb":"val_df.sort_values('score', ascending=False).head(15)","2de624dd":"print(f'Validation Score : {round(val_df.score.mean(), 2)}')","af7dedde":"test","895afc1d":"test = test.reset_index(drop=True)\ntest[['predict', 'pred_start']] = 0\n\nfor i in range(len(test)):\n    test_idx(test, i, is_valid=False)","3eb82f2f":"test","5584473a":"for i, data in test.iterrows():\n    sub.loc[sub.id == data.id, 'PredictionString'] = data['predict']","d3a2bb9c":"sub.to_csv('submission.csv')","7972c20a":"# Training Your Own GPU\nIt needs 24GB memory, so can't training in kaggle.","b840e097":"# Inference Example","d1482d6d":"# Fine-tuning","4065e9a6":"# Validation","ccb5f9a3":"We wants to solve open-domain QA task.\n\nMy process is as follows:\n\n#### 1. [Tokenization](https:\/\/www.kaggle.com\/adldotori\/tokenizing-hindi-and-tamil-language-nlp-step-1)\n#### 2. [Demo](https:\/\/www.kaggle.com\/adldotori\/demo-training-nlp-step-2\/)\n* ver 1 : init (2021\/10\/03)\n* ver 2 : update validation (2021\/10\/05)\n* ver 3 : validation score 0.45 (2021\/10\/11)\n\n#### 3. Research QA Model\n#### 4. Training\n#### 5. Inference","61fcefc2":"Jaccard Similiarity.","8d7fbcff":"# Submission","6805e1d6":"This checkpoint based by deepset\/xlm-roberta-base-squad2 model."}}