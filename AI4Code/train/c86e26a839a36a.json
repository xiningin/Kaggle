{"cell_type":{"e341492c":"code","0c2350b8":"code","14224483":"code","0a15d6cb":"code","a80dbc93":"code","16a21228":"code","47d46572":"code","43053911":"code","f3a8cf83":"code","6d27d59d":"code","0d66385f":"code","cef88916":"code","f92cc344":"code","22e2faa6":"code","3be80fe0":"code","14896fd6":"code","1e90975c":"code","6aa973d4":"code","69d2db72":"code","059c2d2e":"code","79d82a26":"code","c9982cc3":"code","8ebe31d8":"code","f62d27a1":"code","7fcfd139":"code","0ca82b34":"code","a08b7581":"code","39216f64":"code","31c80453":"code","b5338898":"code","360ed298":"code","6f8e4a4f":"code","1f5169b8":"code","47bdcc78":"code","aec908db":"code","f0418cd8":"code","9de882f5":"code","123c176c":"code","8c256a48":"code","373f4167":"code","6e72e2f6":"code","5492b241":"code","974a2bd5":"code","30ac79b7":"code","1c8937a3":"markdown","24e649fa":"markdown","4a65f970":"markdown","982b5d9b":"markdown","cb34ed2f":"markdown","1e8d1c0a":"markdown","87735477":"markdown","9aa1c3ca":"markdown","ff0a13f2":"markdown","fa4080c0":"markdown","0c2f7972":"markdown","6611d47f":"markdown","cfaf33c6":"markdown","06fed38c":"markdown","ed15af4d":"markdown","7795cd9e":"markdown","63749f69":"markdown","c2b8a484":"markdown"},"source":{"e341492c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nimport pickle\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n        \n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n#in order to speed up the demo, I've pre-generated the metadata.csv file and the image vector file.\n#let's copy the metadata file and the image vector file from the input dir to the working dir\nif os.path.exists('\/kaggle\/input\/drinking-waste-classification-metadata\/metadata.csv') and not os.path.exists('\/kaggle\/working\/metadata.csv'):\n    os.popen('cp \/kaggle\/input\/drinking-waste-classification-metadata\/metadata.csv \/kaggle\/working\/metadata.csv') \n    print('metadata copied to working dir')\n\nif os.path.exists('\/kaggle\/input\/drinking-waste-classification-metadata\/vector.pickle') and not os.path.exists('\/kaggle\/working\/vector.pickle'):\n    os.popen('cp \/kaggle\/input\/drinking-waste-classification-metadata\/vector.pickle \/kaggle\/working\/vector.pickle')     \n    print('image vectors copied to working dir')","0c2350b8":"'''\nparse the annoyingly formatted filenames with commas in them\n'''\n\nfname = '\/kaggle\/input\/drinking-waste-classification\/Images_of_Waste\/rawimgs\/Glass\/Glass1,196.JPG'\nprint(fname.split('\/')[-1])\nint(''.join([s for s in fname.split('\/')[-1] if s.isdigit()]))","14224483":"'''\nWhat datatypes are we looking at? What is the file structure?\n'''\n\nrawdir = '\/kaggle\/input\/drinking-waste-classification\/Images_of_Waste\/rawimgs\/'\n\nclasses = sorted(os.listdir(rawdir))\nprint(classes)\n\nn_files = {}\nfiletypes = {}\nfor dirname, _, filenames in os.walk(rawdir):\n    n_files[dirname] = 0\n    for filename in filenames:\n        n_files[dirname] += 1   \n        extension = filename.split('.')[-1]\n        if extension in filetypes.keys():\n            filetypes[extension] += 1\n        else:    \n            filetypes[extension] = 1\n\nfor directory, counts in n_files.items():\n    print(f'number of files {directory} {counts}') \n\nfor filetype, counts in filetypes.items():\n    print(f'number of files of filetype {filetype} {counts}')     ","0a15d6cb":"'''\nload some sample images\n'''\nfnames = sorted(os.listdir(rawdir+classes[0]))\nimgs = []\nfor i in range(18):\n    imgs.append(Image.open(rawdir+classes[0] + '\/'+ fnames[i]))\n","a80dbc93":"f, axarr = plt.subplots(6, 3,figsize=(20,60))\nfor i in range(18):\n    axarr[i\/\/3,i%3].imshow(imgs[i])","16a21228":"imgs[0]._getexif()","47d46572":"generate_metadata = False #only need to generate metadata.csv once - after that just need to load the file\n\nif generate_metadata or not os.path.exists('\/kaggle\/working\/metadata.csv'):\n    metadata = pd.DataFrame(columns=['filepath', 'filename', 'label', 'camera', 'timestamp', 'coord_ns', 'coord_ew', 'width', 'height'])\n\n    i = -1\n    for dirname, _, filenames in os.walk(rawdir):\n        print(dirname)\n        #print(len(filenames))\n        label = dirname.split(rawdir)[-1]\n        print(label)\n        \n        #sort the filenames by the numeric value at the end\n        filenames = {fname: int(''.join([s for s in fname.split('\/')[-1] if s.isdigit()])) for fname in filenames}\n        filenames = {k: v for k, v in sorted(filenames.items(), key=lambda item: item[1])}\n\n        for filename in filenames.keys(): \n            if filename.split('.')[-1] in ['jpg', 'JPG']: #exclude .HEIC files for now\n                i += 1\n                metadata.loc[i, 'label'] = label\n                img = Image.open(dirname + '\/' + filename)\n                metadata.loc[i, 'width'] = np.array(img).shape[0]\n                metadata.loc[i, 'height'] = np.array(img).shape[1]\n                info = img._getexif()\n\n                metadata.loc[i, 'filepath'] = dirname + '\/' + filename\n                metadata.loc[i, 'filename'] = filename\n                try:\n                    metadata.loc[i, 'camera'] = info[42036]\n                except:\n                    metadata.loc[i, 'camera'] = None\n\n                try:\n                    metadata.loc[i, 'timestamp'] = info[36867]\n                except:\n                    metadata.loc[i, 'timestamp'] = None    \n\n                try:\n                    metadata.loc[i, 'coord_ns'] = info[34853][2]\n                    metadata.loc[i, 'coord_ew'] = info[34853][4]\n                except:\n                    metadata.loc[i, 'coord_ns'] = None \n                    metadata.loc[i, 'coord_ew'] = None \n                    \n    metadata.to_csv('\/kaggle\/working\/metadata.csv', index=False)  \n    \nelse:\n    metadata = pd.read_csv('\/kaggle\/working\/metadata.csv')","43053911":"'''\nHow many images are there for each class?\n'''\nprint(metadata.label.value_counts(dropna=False))","f3a8cf83":"'''\nHow many unique cameras were used to generate the data set? \n... note that the labels don't have the same fraction of photos for each device - \nthe iPhone was used to take a much higher fraction of the AluCan photos than the Glass photos.\n'''\nmetadata.groupby(['label', 'camera'], dropna=False).size()","6d27d59d":"metadata.groupby(['label', 'width', 'height', 'camera'], dropna=False).size()","0d66385f":"'''\nconvert the time stamp to a time delta (days since the first photo was taken)\n\n137 days between the first photo and the last photo!\n\n679 of the images don't include the timestamp metadata (unixtime is NaN)\n'''\n\nimport datetime\nmetadata.loc[:, 'unixtime'] = metadata.loc[:, 'timestamp'].apply(lambda x: datetime.datetime.strptime(str(x), \"%Y:%m:%d %H:%M:%S\").timestamp() if not (x is None or x is np.NaN) else None) \nmetadata.loc[:, 'unixtime'] -=  metadata.loc[:, 'unixtime'].min()\nmetadata.loc[:, 'unixtime'] \/\/= (3600*24) #round unix timestamp to day\nmetadata.head()\nprint(metadata.unixtime.value_counts(dropna=False))","cef88916":"'''\nimport the pretrained models\n'''\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision import transforms, datasets, models","f92cc344":"#model = torch.hub.load('pytorch\/vision:v0.6.0', 'resnet18', pretrained=True)\nresnet = torchvision.models.resnet18(pretrained=True)\nmodel = torchvision.models.resnet18(pretrained=True)\n\n'''\ntake a peek at the layers in your model\nhttps:\/\/github.com\/pytorch\/vision\/blob\/195c41ee7ae3fff8f64a4fda67632259dd4f7454\/torchvision\/models\/resnet.py#L201\n'''","22e2faa6":"class Identity(torch.nn.Module):\n    def __init__(self):\n        super(Identity, self).__init__()\n        \n    def forward(self, x):\n        return x\n    \nclass Flatten(torch.nn.Module):\n    def __init__(self):\n        super(Flatten, self).__init__()\n        \n    def forward(self, x):\n        return x.reshape(x.size(0), -1)  ","3be80fe0":"'''\nExercise:\nexperiment with:\n- making the last layer a max pooling layer?\n- setting model.layer3 to the Identity() module?\n- try resnet50 instead of resnet18?\n'''\noutput_pools = (1, 1)\nmodel.layer4 = Identity()\nmodel.avgpool = torch.nn.AdaptiveAvgPool2d(output_size=output_pools)\nmodel.fc = Identity() \n\n#set model.eval() so that pytorch doesn't allocate memory for gradients and batchnorm is fixed and dropout isn't used \nmodel = model.eval()\nprint(model.forward)","14896fd6":"resize = transforms.Resize(384) #resize so that smallest dim is 384\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\ntransforms = transforms.Compose([resize, torchvision.transforms.ToTensor(), normalize])","1e90975c":"class WasteDataset(Dataset):\n    \"\"\"waste dataset.\"\"\"\n\n    def __init__(self, metadata, transforms=None):\n        \"\"\"\n        Args:\n            metadata: pandas dataframe with image data\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.metadata = metadata\n        self.transforms = transforms\n\n    def __len__(self):\n        return self.metadata.shape[0]\n\n    def __getitem__(self, idx):\n        img = Image.open(self.metadata.loc[idx, 'filepath'])\n        label = self.metadata.loc[idx, 'label']\n        \n        return {'img': self.transforms(img)}, label","6aa973d4":"'''\ncheck the dataset class output \n'''\nds = WasteDataset(metadata, transforms=transforms)\nx, y = ds.__getitem__(8)\nprint(x['img'].shape)\n#plt.imshow(x['img'].numpy().swapaxes(0,2))","69d2db72":"'''\ncheck the model output\n'''\nx, y = ds.__getitem__(4000)\nv = model.forward(x['img'].unsqueeze(0))\nprint(v.size())\nprint(metadata.shape)","059c2d2e":"'''\nExercise: use code profiling to figure out what processes take the longest in this code. \n   Is loading the data more time-consuming? Or is executing the model the slowest step?\n   What about if you switch from a model based on resnet18 to one based on resnet50?\n   (more on code profiling: https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/01.07-timing-and-profiling.html)\nExercise: does using larger input images make a difference to the image vectors (and resulting clusters?)  \nExercise: does using resnet50 vs resnet18 make a difference to the image vectors (and resulting clusters?)\nExercise: make this code more efficient by replacing the for loop with a pytorch data loader?\n'''\n\n#set calc_vectors to True to recalculate the image vectors\ncalc_vectors = False\nn = metadata.shape[0] #200 \nclasses_dict = {c: i for i, c in enumerate(classes)}\ny = torch.tensor(metadata.label.apply(lambda x: classes_dict[x]).values)\n\nif calc_vectors or not os.path.exists('\/kaggle\/working\/vector.pickle'):\n    #vectors = torch.zeros(n, v.size(1)*output_pools[0]*output_pools[1])\n    vectors = torch.zeros(n, v.size(1))\n\n    #ensure that model is in eval mode (model = model.eval())\n    for i in tqdm(range(n)):\n        with torch.no_grad(): #save memory by not storing gradients\n            x, label = ds.__getitem__(i)\n            vectors[i] = model(x['img'].unsqueeze(0)).reshape(1, -1)\n\n    torch.save(vectors, '\/kaggle\/working\/vector.pickle')\nelse:\n    vectors = torch.load('\/kaggle\/working\/vector.pickle')  ","79d82a26":"'''\ncheck a histogram of some values in the image vectors\n- mostly looking to see that we have a distribution where we don't just have one value \n(which would suggest that something went wrong  or the input images are not normalized correctly.)\n'''  \nprint(vectors.size())\nplt.hist(vectors[:, 2].numpy(), bins=40)","c9982cc3":"'''\nto efficiently calculate the distance matrix describing how similar or dissimilar the vectors are from each other\nbegin by calculating the dot product of all the vectors with each other\n'''\nab = torch.matmul(vectors, vectors.T)\nprint(ab.size())","8ebe31d8":"#a few possible definitions for the correlation matrix \n#exercise: what are the values on the diagonal of the correlation matix for 1) and 3)?\n#exercise: why might you pick one choice over another?\n\n#1. distance matrix is the difference between ||v[i] and v[j]||\ncorr = torch.sqrt(torch.clamp(ab.diag().unsqueeze(0).repeat(n, 1) -2*ab + ab.diag().unsqueeze(1).repeat(1, n), 0.0, None))\n\n#2. distance matrix is the square difference ||v[i] and v[j]||**2\n#corr = torch.clamp(ab.diag().unsqueeze(0).repeat(n, 1) -2*ab + ab.diag().unsqueeze(1).repeat(1, n), 0.0, None)\n\n#3. correlation matrix is the cosine overlap between v[i] and v[j]\n#corr = 0.5 + 0.5ab \/ torch.sqrt(ab.diag().unsqueeze(0).repeat(n, 1) * ab.diag().unsqueeze(1).repeat(1, n))\n\n#guarantee that corr is symmetric (asymmetries are due to floating point errors)\n#corr = 0.5*corr + 0.5*corr.T","f62d27a1":"'''\nplot the image vector distance matrix as ordered by the filename\n'''\nplt.figure(figsize = (16,16))\nplt.imshow(corr, cmap='gist_rainbow')\nplt.xlabel('correlation matrix for image vectors (ordered by filename)')\nplt.ylabel('correlation matrix for image vectors (ordered by filename)')\ncbar = plt.colorbar(shrink=0.8)\ncbar.set_label('distance between image pairs')","7fcfd139":"'''\nplot the correlation matrix with a sidebar plotting the class label\n'''\nplt.figure(figsize = (16,16))\nplt.imshow(torch.cat([torch.max(corr)\/3.5*y.unsqueeze(1).repeat(1,40), np.nan*torch.zeros(y.size(0), 15), corr], dim=1), vmin=0, cmap='gist_rainbow')\nplt.ylabel('class label')\nplt.xlabel('correlation matrix for image vectors (ordered by filename)')","0ca82b34":"'''\ninstall the fastcluster package\n'''\ntry: fc is None\nexcept NameError: \n    !pip install fastcluster","a08b7581":"'''\nsorting a distance matrix with agglomerative clustering\nworks like sklearn's agglomerative clustering, but it just orders the image vectors instead of \nassigning them to clusters.\nhttps:\/\/gmarti.gitlab.io\/ml\/2017\/09\/07\/how-to-sort-distance-matrix.html\n\nalso check out Faiss: a library for efficient similarity search and clustering of dense vectors.\nhttps:\/\/github.com\/facebookresearch\/faiss\n'''\nimport fastcluster as fc\nimport scipy.spatial.distance as dst #import pdist, squareform\n\ndef seriation(Z,N,cur_index):\n    '''\n        input:\n            - Z is a hierarchical tree (dendrogram)\n            - N is the number of points given to the clustering process\n            - cur_index is the position in the tree for the recursive traversal\n        output:\n            - order implied by the hierarchical tree Z\n            \n        seriation computes the order implied by a hierarchical tree (dendrogram)\n    '''\n    if cur_index < N:\n        return [cur_index]\n    else:\n        left = int(Z[cur_index-N,0])\n        right = int(Z[cur_index-N,1])\n        return (seriation(Z,N,left) + seriation(Z,N,right))\n    \ndef compute_serial_matrix(dist_mat,method=\"ward\"):\n    '''\n        input:\n            - dist_mat is a distance matrix\n            - method = [\"ward\",\"single\",\"average\",\"complete\"]\n        output:\n            - seriated_dist is the input dist_mat,\n              but with re-ordered rows and columns\n              according to the seriation, i.e. the\n              order implied by the hierarchical tree\n            - res_order is the order implied by\n              the hierarhical tree\n            - res_linkage is the hierarhical tree (dendrogram)\n        \n        compute_serial_matrix transforms a distance matrix into \n        a sorted distance matrix according to the order implied \n        by the hierarchical tree (dendrogram)\n    '''\n    N = len(dist_mat)\n    flat_dist_mat = dst.squareform(dist_mat)\n    res_linkage = fc.linkage(flat_dist_mat, method=method,preserve_input=True)\n    res_order = seriation(res_linkage, N, N + N-2)\n    seriated_dist = np.zeros((N,N))\n    a,b = np.triu_indices(N,k=1)\n    seriated_dist[a,b] = dist_mat[ [res_order[i] for i in a], [res_order[j] for j in b]]\n    seriated_dist[b,a] = seriated_dist[a,b]\n    \n    return seriated_dist, res_order, res_linkage","39216f64":"#use corr + corr.T to guarantee that the correlation matrix is symmetric\n#compute_serial_matrix generates an ordering of the images, ror, which places similar images close together\ncorr_rord, rord, linkage = compute_serial_matrix((corr + corr.T).numpy(), method=\"ward\") #try different values for method","31c80453":"plt.figure(figsize = (18,18))\n#plt.imshow(np.sqrt(corr[:, rord][rord, :]))\nplt.imshow(torch.cat([torch.max(corr)\/3.5*y[rord].unsqueeze(1).repeat(1,25), np.nan*torch.zeros(y.size(0), 15), corr[:, rord][rord, :]], dim=1), vmin=0, cmap='gist_rainbow')\n\nplt.ylabel('class label')\nplt.xlabel('image vector distance matrix')","b5338898":"'''\nthe clustering algorithm groups images with width=384 (images from the NaN camera)!\nexercise: make the colourbar less crappy :-) \nexercise: try making this plot with the seaborn plotting package\n'''\n\nwidth_dict = {w:i for i, w in enumerate(sorted(metadata['width'].unique()))}\nplt.figure(figsize = (18,18))\n#plt.imshow(np.sqrt(corr[:, rord][rord, :]))\nplot = plt.imshow(torch.cat([torch.max(corr)\/len(width_dict)*torch.tensor(metadata.loc[rord, 'width'].apply(lambda x: width_dict[x])).unsqueeze(1).repeat(1,25), np.nan*torch.zeros(y.size(0), 15), corr[:, rord][rord, :]], dim=1), cmap='gist_rainbow')\n\nplt.ylabel('image width group')\nplt.xlabel('image vector distance matrix')\ncbarplot = plt.colorbar(plot, ticks=torch.max(corr)\/len(width_dict)*np.array(list(width_dict.values())), shrink=0.8)\ncbarplot.ax.set_yticklabels(list(width_dict.keys()))\ncbarplot.set_label('image vector pairwise distance AND image width (pixels)', rotation=90)","360ed298":"'''\nfix metadata loaded from csv with incorrect data type\n'''\nfor feature in ['coord_ns', 'coord_ew']:\n    if metadata[feature].dtype in ['str', 'object']:\n        metadata[feature] = metadata[feature].apply(lambda x: np.nan if x in [None, np.nan] else eval(str(x)))\n\ncoord_ns = metadata['coord_ns'].apply(lambda x: np.nan if x in [None, np.nan] else x[2]).values\ncoord_ew = metadata['coord_ew'].apply(lambda x: np.nan if x in [None, np.nan] else x[2]).values","6f8e4a4f":"plt.plot(coord_ew, coord_ns, '.')\nplt.xlabel('ew latitude (seconds only)')\nplt.ylabel('ns longitude (seconds only)')","1f5169b8":"'''\nnote: this is the WRONG way to calculate the distance between two nearby points given their latitude and longitude\nhowever, the WRONG way is very fast and not super wrong for points that are close together.\n\nthe correct way is slow: https:\/\/blog.batchgeo.com\/manipulating-coordinates-in-excel\/\n'''\ncoord_ew = torch.tensor(coord_ew)\ncoord_ns = torch.tensor(coord_ns)\n\n'''\nuse dist = |a-b|^2 = |a|^2 - 2|ab| + |b|^2\n'''\nabdx = torch.matmul(coord_ew.unsqueeze(1), coord_ew.unsqueeze(0))\nabdy = torch.matmul(coord_ns.unsqueeze(1), coord_ns.unsqueeze(0))\ndist = -2*(abdx + abdy) \ndist += abdx.diag().unsqueeze(0).repeat(n, 1) + abdy.diag().unsqueeze(0).repeat(n, 1)\ndist += abdx.diag().unsqueeze(1).repeat(1, n) + abdy.diag().unsqueeze(1).repeat(1, n)\n#dist = torch.sqrt(dist)\ndist = torch.log(dist)\ndist += torch.max(dist[~torch.isnan(dist) * ~torch.isinf(dist)])\ndist \/= -torch.min(dist[~torch.isnan(dist) * ~torch.isinf(dist)])","47bdcc78":"plt.figure(figsize = (18, 18))\nimg = (torch.max(corr[~torch.isnan(corr)])\/1*dist).float()\nimg2 = torch.cat([torch.max(img[~torch.isnan(img)])\/3.5*y.float().unsqueeze(1).repeat(1,40), np.nan*torch.zeros(y.size(0), 15), img], dim=1)\nplt.imshow(img2, vmin=0, cmap='gist_rainbow')\nplt.ylabel('image label')\nplt.xlabel('log(physical distance) between the location of photo i and photo j (ordered by filename)')","aec908db":"'''\nphysical distance between photo i and photo j (ordered by image vector clustering)\n'''\n\nplt.figure(figsize = (18, 18))\nimg2 = torch.cat([torch.max(img[~torch.isnan(img)])\/3.5*y[rord].float().unsqueeze(1).repeat(1,40), np.nan*torch.zeros(y.size(0), 15), img[:, rord][rord]], dim=1)\nplt.imshow(img2, vmin=0, cmap='gist_rainbow')\nplt.ylabel('image label')\nplt.xlabel('physical distance between photo i and photo j (ordered by image vector clustering)')","f0418cd8":"'''\nKmeans is useful for grouping the images into a fixed number of clusters\n(if the clusters are fairly balanced in size, this can be used to create split the data into 6 folds)\nif the clusters have imbalanced sizes, try fiddling with the exponent on the corr (distance) matrix\nexercise: how do the clusters selected by kmeans compare to the clusters you would have arbitrarily defined \nby splitting the corr matrix ordered by fastcluster?\n'''\nfrom sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering\npred = KMeans(n_clusters=6).fit_predict(corr.numpy()**0.25) \n#pred = 1+DBSCAN(eps=0.8, metric='precomputed').fit_predict(0.5*corr.numpy()**0.25 + 0.5*corr.numpy().T**0.25)\n#pred = SpectralClustering(3, affinity='precomputed', n_init=100, assign_labels='kmeans').fit_predict(corr.numpy())\n","9de882f5":"'''\nquick histogram to check the cluster sizes\n'''\nnp.histogram(pred, bins=range(0,1+len(set(pred))))","123c176c":"'''\ncompare cluster assignments from kmeans vs the order created by fastcluster\n'''\nplt.figure(figsize = (16,16))\n#order plot by filename\n#plt.imshow(torch.cat([torch.max(corr)\/len(set(pred))*torch.tensor(pred).unsqueeze(1).repeat(1,40), np.nan*torch.zeros(y.size(0), 15), corr], dim=1), vmin=0, cmap='gist_rainbow')\n#plt.xlabel('distance matrix for image vectors (ordered by filename)')\n\n#order plot with fastcluster\nplt.imshow(torch.cat([torch.max(corr)\/len(set(pred))*torch.tensor(pred[rord]).unsqueeze(1).repeat(1,40), np.nan*torch.zeros(y.size(0), 15), corr[:, rord][rord]], dim=1), vmin=0, cmap='gist_rainbow')\nplt.xlabel('distance matrix for image vectors (ordered by fastcluster)')\nplt.ylabel('cluster assignment by kmeans')","8c256a48":"\nplt.figure(figsize = (16,16))\n#add extra (arbitrary) distance between images that don't have the same target label\ncorr2 = corr + 1*torch.max(corr)*(y.unsqueeze(0).repeat(n, 1) != y.unsqueeze(1).repeat(1, n))\nplt.imshow(corr2, vmin=0, cmap='gist_rainbow')\nplt.xlabel('distance matrix for image vectors (ordered by filename)')\nplt.ylabel('distance matrix for image vectors (ordered by filename)')","373f4167":"'''\ngroup similar images using the the corr2, the distance matrix with extra\ndistance added to images pairs which don't have the same label\n\nClustering will give us a way to group images that are likely of similar objects \nthen we can weight the training and validation data so that an item with multiple\nimages will not be weighted more than an item with only a few images: ie a bottle \nwith 6 images has 1\/6 weight for each image\n\nThis clustering process is a bit of an art and will depend on how the vectors for\neach image are constructed. \n\nExercise: try changing the value of the distance_threshold parameter: how big can you\nmake it before the algorithm starts grouping images of different objects together?\nExercise: Try max vs avg pooling, different models or different output pool\nsizes ((3,3) instead of (1,1)) for the pretrained network?\n'''\npred = AgglomerativeClustering(n_clusters=None, distance_threshold=2.0, linkage='ward').fit_predict(0.5*corr2.numpy() + 0.5*corr2.numpy().T)","6e72e2f6":"'''\nHow many clusters do we have? How big are they? Is the biggest cluster made of adjacent image files?\n'''\n\ncounts, _ = np.histogram(pred, bins=range(0,1+len(set(pred))))\nprint(f'number of images in the largest cluster: {np.max(counts)}')\nprint(f'image indices for the largest cluster: {np.where(pred==np.argmax(counts))}')\nprint(f'how many images are in a cluster alone? {np.sum(counts==1)}')\nprint(f'number of clusters: {counts.shape}')","5492b241":"# plt.figure(figsize = (16,16))\n# #order plot by filename\n# plt.imshow(torch.cat([torch.max(corr)\/len(set(pred))*torch.tensor(pred).unsqueeze(1).repeat(1,40), np.nan*torch.zeros(y.size(0), 15), corr], dim=1), vmin=0, cmap='gist_rainbow')\n# plt.xlabel('distance matrix for image vectors (ordered by filename)')","974a2bd5":"'''\nload some images that were clustered together\n'''\n\nidx = np.where(pred==np.argmax(counts))[0]\nprint(idx)\nimgs = []\nfor i in idx:\n    imgs.append(Image.open(metadata.loc[i, 'filepath']))\n","30ac79b7":"'''\nimages that were clustered together are of the same object!\nExercise: tinker with the distance threshold to see if you can find more \nimages of this bottle without adding images of a different bottle to the cluster.\n'''\nf, axarr = plt.subplots(6, 3,figsize=(20,60))\nfor i in range(len(idx)):\n    axarr[i\/\/3,i%3].imshow(imgs[i])","1c8937a3":"# 5. Identifying multiple images of the same object\n\nIn addition to making a good train-val split, when we eventually train a classifier model, we would also like \nto correctly weight images so that an object with multiple images is weighted the same as an object with only \none image in the data set.\n\nThis means that we'd like to group images of the same object together, but *not* group together objects from \ndifferent classes that were photographed on the same background.\n\nTo stop the clustering algorithm from grouping images that belong to different classes, let's `add the maximum distance in the correlation matrix to image pair distances where the two images don't belong to the same class`****.","24e649fa":"Plot some sample images.... we can already see that there are multiple images of the same can....","4a65f970":"Let's extract the camera info, time stamp, GPS coords and the image height and width.\n\nPut the image metadata into a pandas dataframe","982b5d9b":"I've started with resnet18 and I've removed final fully-connect (fc) layers. I also \"remove\" the final conv block + bottleneck (layer4) by replacing it with an `Identity` layer that just returns the input. The `Flatten` layer is needed to convert the output of the last convolutional layer from 2D to 1D (so that each item in the batch is represented by a 1D vector).","cb34ed2f":">> ","1e8d1c0a":"All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]. ","87735477":"# 2. Using a pretrained network to generate vectors representing the images\n\nNow that we've had a peek at the image metadata, let's do some simple image clustering using a pretrained nn.\n\nThe idea is to use a pretrained resnet nn as an unsupervised clustering algorith. We do this by stripping off the final dense layers and maybe the last few conv layers. The model will generate a vector that describes each image - and we can use that vector to evaluate how similar images are to each other! \n\nNote that there is nothing particularly magical about the cropped-down version of the model I've created here: depending on how similar your images are to each other (and on how similar they are to the imagenet data that the resnet models are trained on) you might get more instructive image similarities by leaving more layers or leaving fewer layers.\n\nOnce we have generated a vector describing each image, we can used `torch.matmul` to efficiently calculate the distance between pairs of image vectors. We can then use these distances to find groups of similar images.","9aa1c3ca":"Now we can write the Dataset class","ff0a13f2":"Pairs of photos that have similar GPS coords are indicated in red. Pairs of photos that have IDENTICAL GPS coords are in white - (log(0) is NaN). Plotting the correlation matrix with the images ordered by filename makes it clear when clusters of images were taken in the same location.","fa4080c0":"The image order in the above plot can be used to split the images into folds: a manual split allows you to trade off having folds with similar numbers of images, while at the same time keeping similar images together. You could also use Kmeans to group the image vectors into 5 or 6 clusters, but you would give up the ability to make the clusters contain similar numbers of images.\n\nNow let's investigate whether the clustering algorithm grouped together images with the same aspect ratio - or images from the same camera.","0c2f7972":"# 4. clustering the image vectors in order to place the images into folds with similar images","6611d47f":"How are image dimensions distributed for each class?\n\nAha! Only PET images have width=512 and height=384.... And different classes have different ratios of images with width 384 and width 683.... **Watch out that images in a batch are not padded such that your algorithm is just learning what aspect ratio corresponds to a given class!!!**","cfaf33c6":"Now we are ready to compute all the vectors describing each image!\nThis will be time-consuming, so either load the pre-generated vectors or use a tqdm progress bar to keep track of the remaining time required.","06fed38c":"# Drinking waste data exploration and cross-validation design\n\nHow do you design a good training-validation split? When is a random split undesirable? This notebook explores the Drinking Waste dataset and develops some tools that can help to automate the process of identifying similar images. There are two goals:\n\n1. to place similar images in the same fold so that extrapolating from one fold to another mimics the extrapolation tasks that the algorithm might have to deal with in (unseen) production data. \n2. to identify multiple images of the same object so that an item with 50 images can be undersampled during training relative to an item with 2 images.\n\nThe processes that are demonstrated include:\n\n1. extracting image metadata<\/a>\n2. using a pretrained network to generate vectors representing the images\n3. efficiently calculating the distance between image vectors\n4. different approaches to clustering the image vectors in order to place the images into folds with similar images\n5. an approach for clustering the images in order to identify multiple images of the same object","ed15af4d":"# 3. Efficiently calculating the distance between image vectors\n\nNow that we have computed the vectors describing each image, we would like to evaluate the distances (or correlations) between all the vectors. Similar images will be closer together (distance -> 0) or more strongly correlated (correlation -> large)","7795cd9e":"Since the distance matrix plotted above is ordered by the file name, we can immediately appreciate that there are already clumps of similar images where photos were taken one after the other. We can also see that there are cases where images assigned to one class are similar to images in another class - perhaps these are images from the same camera or with the same background? We'll investigate that more in a bit.\n\nFor now let's use the fastcluster package to cluster the images: we can use the order generated by the fastcluster sorting process as the basis for splitting the images into folds. ","63749f69":"Now let's try using Kmeans to generate 6 clusters that we can use to assign similar images to the same CV fold. \n","c2b8a484":"# 1. Extracting image metadata\n\n<a id=\"chap1\"><\/a>Now that we can open the files, the goal of further data exploration is to look for features that are not distributed in the way we expect in production data.\n\nLet's start by examining the image metadata using the `_getexif()` routine, which is available for images loaded with the PIL library.\n\n(HINT: if you're making a dataset public, consider scrubbing your GPS coords from the metadata!)\n\nExercise: find some documentation that explains what some of the other entries in the metadata mean."}}