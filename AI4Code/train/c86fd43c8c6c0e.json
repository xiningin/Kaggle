{"cell_type":{"a23d181b":"code","d39ce97a":"code","9303416b":"code","813c7b0c":"code","ca78f71f":"code","b9aabc11":"code","61ee81fd":"code","7e476ea2":"code","e21aa30f":"code","dfb92677":"code","63d868e7":"code","f329d69d":"code","da7408b4":"code","b0231212":"code","5ecfffe0":"code","18722231":"code","71aa7133":"code","7dd4e2fe":"code","8ca9760f":"code","6b2a38dd":"code","bb5b2624":"code","bd123e94":"code","7b768e35":"code","091c9081":"code","f406c495":"code","df2a5952":"code","bae7146f":"code","42eb4835":"code","39b2acdb":"code","1edfc037":"code","f9259e6a":"code","1a2b75a9":"code","ed8a1910":"code","bd9e6c6a":"code","c398d419":"code","c6533629":"code","ad7d3363":"code","f66c3a25":"code","b8dee1e4":"code","a455eaca":"code","eb255314":"code","19a7b2cf":"code","f2679f57":"code","52e23057":"code","d5179cca":"code","f303da00":"code","aed97170":"code","88fe52b8":"code","8e5b77e1":"code","23e38162":"code","ae70aa70":"code","38532c75":"code","87feacd9":"code","235e61da":"code","4356df69":"code","a5fa49eb":"code","cc538076":"code","19220f34":"code","009bd532":"code","dabfafcb":"code","80e067c5":"code","ab68371c":"code","c92c28f7":"code","167b36b5":"code","ecf0b3e0":"code","f08c0bad":"code","2974dbdd":"code","bfd4706d":"code","cb0e1430":"code","bcb6310b":"code","7d67e2c0":"code","e343d9f6":"code","c3a7a7e3":"code","88c144c9":"code","d7961f91":"code","f8984a23":"code","6ee99da3":"code","015e1581":"code","9b4d2bc1":"code","4b39418f":"code","f7e05dbb":"code","ac95b764":"code","ae30c03c":"code","0440149b":"code","f8bb01ce":"code","9cb17d8d":"code","79e72192":"code","ad2e56cb":"code","a05cb5c2":"code","ae2a17b2":"code","c412fa50":"code","bc128095":"code","1545010c":"code","48de658c":"code","cc37035a":"markdown","32990f92":"markdown","8bdc8e04":"markdown","abcd6d3d":"markdown","cde3994a":"markdown","8127b3b3":"markdown","489efc27":"markdown","b66d89b4":"markdown","da6d33ed":"markdown","a2b84239":"markdown","ad5489b6":"markdown","bc307197":"markdown","851d56f9":"markdown","e705cbdf":"markdown","0e7d3fd6":"markdown","dc69c3e3":"markdown","c75c6d47":"markdown","4e2686cc":"markdown","b1d9bc66":"markdown","affcd84c":"markdown","99eb4bcb":"markdown","197a5557":"markdown","39b865c9":"markdown","dbef9544":"markdown","e541f7a1":"markdown","71c0939a":"markdown","9bfea4a3":"markdown","eb67a45e":"markdown","73fd6b23":"markdown","05b7f8f1":"markdown","bf934994":"markdown","50eacc2c":"markdown","b3fe5089":"markdown","63b9d250":"markdown","a73d382f":"markdown","13a9533d":"markdown","fe572b83":"markdown","f6d7fdfe":"markdown","8fb0b045":"markdown","932bd0c7":"markdown","797c1579":"markdown","e7fa6bf9":"markdown","a4b175ba":"markdown","85b8f817":"markdown","f921eed2":"markdown","40801446":"markdown","91403abc":"markdown","8ba0ba2a":"markdown","10c312d0":"markdown","87457dbf":"markdown","08c40a9f":"markdown","97d86738":"markdown","1facde86":"markdown"},"source":{"a23d181b":"!pip install visualkeras","d39ce97a":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport cv2 \n\nimport matplotlib.pyplot as plt\nimport plotly.offline as pyo\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport seaborn as sb\n\n\nfrom keras.utils import plot_model\nimport visualkeras\nimport tensorflow as tf\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import confusion_matrix\nimport pickle\n\nimport warnings\n\nsb.set()\npyo.init_notebook_mode()\nwarnings.filterwarnings(\"ignore\")","9303416b":"tf.__version__","813c7b0c":"def Create_Directory_DataFrame(basedir):\n    df = pd.DataFrame(columns=['Class','Location'])\n    # basedir\n    for Class in os.listdir(basedir+'\/'):\n        for location in os.listdir(basedir+'\/'+Class+'\/'):\n            df = df.append({'Class':Class,'Location':basedir+'\/'+Class+'\/'+location},ignore_index=True)\n    return df","ca78f71f":"# Path to train images\ntrain_dir = '..\/input\/intel-image-classification\/seg_train\/seg_train'\n\n# Path to validation images\ntest_dir = '..\/input\/intel-image-classification\/seg_test\/seg_test'\n\ntrain_df = Create_Directory_DataFrame(train_dir)\ntest_df = Create_Directory_DataFrame(test_dir)","b9aabc11":"train_df.head()","61ee81fd":"test_df.head()","7e476ea2":"# Create a dictionary encoding the classes to a numeric value in alphabetical order\n\nCLASS_MAP = {\n    0: 'buildings',\n    1: 'forest',\n    2: 'glacier',\n    3: 'mountain',\n    4: 'sea',\n    5: 'street'\n}","e21aa30f":"count = 1\nf = plt.figure(figsize=(50,20))\nfor Class in train_df['Class'].unique():\n    seg = train_df[train_df['Class']==Class]\n    address =  seg.sample().iloc[0]['Location']\n    img = cv2.imread(address)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    ax = f.add_subplot(2, 5,count)\n    ax = plt.imshow(img)\n    ax = plt.title(Class,fontsize= 30)\n    count = count + 1\nplt.suptitle(\"Scenery Images\", size = 32)\nplt.show()","dfb92677":"sb.countplot(train_df[\"Class\"])","63d868e7":"sb.countplot(test_df[\"Class\"])","f329d69d":"training_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   shear_range = 0.2,\n                                   zoom_range = 0.2,\n                                   horizontal_flip = True) \n\ntraining_set = training_datagen.flow_from_dataframe(\n    dataframe=train_df,\n    directory=None,\n    x_col=\"Location\",\n    y_col=\"Class\",\n    weight_col=None,\n    target_size=(150, 150),\n    color_mode=\"rgb\",\n    classes=None,\n    class_mode=\"categorical\",\n    batch_size=32,\n    shuffle=True,\n    seed=None,\n    save_to_dir=None,\n    save_prefix=\"\",\n    save_format=\"png\",\n    subset=None,\n    interpolation=\"nearest\",\n    validate_filenames=True,\n)","da7408b4":"test_datagen = ImageDataGenerator(rescale = 1.\/255)\ntest_set = test_datagen.flow_from_dataframe(\n    dataframe=test_df,\n    directory=None,\n    x_col=\"Location\",\n    y_col=\"Class\",\n    weight_col=None,\n    target_size=(150, 150),\n    color_mode=\"rgb\",\n    classes=None,\n    class_mode=\"categorical\",\n    batch_size=32,\n    shuffle=False,\n    seed=None,\n    save_to_dir=None,\n    save_prefix=\"\",\n    save_format=\"png\",\n    subset=None,\n    interpolation=\"nearest\",\n    validate_filenames=True,\n)","b0231212":"optimizers = ['Adam', 'RMSprop']\nMETRICS = ['accuracy', tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall')] ","5ecfffe0":"loss_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.75,\n                              patience=2, min_lr=0.0001)\nacc_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.75,\n                              patience=2, min_lr=0.0001)","18722231":"num_of_train_samples = 14034\nnum_of_test_samples = 3000\nbatch_size = 32","71aa7133":"def PlotModel(history):\n    epochs = range(1,len(history.history['loss']) + 1)\n    epochs = list(epochs)\n    stats = []\n    for stat in history.history:\n        stats.append(stat)\n    num_rows = len(stats) \/\/ 4 + 1\n    fig = make_subplots(rows=num_rows, cols=4,subplot_titles=stats)\n    \n    row=0\n    col=0\n    for stat in stats:\n        if col%4==0:\n            row+=1\n            col=1\n        else:\n            col+=1\n        fig.add_trace(go.Scatter(x=epochs, y=history.history[stat]), row=row, col=col)\n    fig.update_layout(showlegend=False,height=row*500, width=1200, title_text=\"Statistics\")\n    pyo.iplot(fig)","7dd4e2fe":"def plot_cm(y_pred, test):\n    y_pred_class = np.argmax(y_pred, axis=1)\n    try:\n        cm = confusion_matrix(test.classes, y_pred_class)\n    except:\n        cm = confusion_matrix(test, y_pred_class)\n    conf_matrix = pd.DataFrame(data=cm,index = ['Predicted ' + CLASS_MAP[key] for key in CLASS_MAP],\n                  columns = ['Actual ' + CLASS_MAP[key] for key in CLASS_MAP])\n    f, axes = plt.subplots(1, 1, figsize=(24, 24))\n    sb.heatmap(conf_matrix,\n              annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, cmap=\"YlGnBu\").set_title(\"Confusion Matrix\")\n    \n    ","8ca9760f":"def create_model():\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.Input(shape=(150 , 150 , 3)))\n    model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n    model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n    model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(units=256, activation='selu'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dropout(0.2))\n    model.add(tf.keras.layers.Dense(units=128, activation='selu'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dropout(0.2))\n    model.add(tf.keras.layers.Dense(units=64, activation='selu'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dropout(0.2))\n    model.add(tf.keras.layers.Dense(units=6, activation='softmax'))\n    \n    return model","6b2a38dd":"cnn = create_model()","bb5b2624":"cnn.summary()","bd123e94":"plot_model(cnn,show_shapes=True)","7b768e35":"visualkeras.layered_view(cnn)","091c9081":"results = {'Adam': None, 'RMSprop': None}\npreds = {'Adam': None, 'RMSprop': None}\nfor opt in optimizers:\n    cnn = create_model()\n    cnn.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = METRICS)\n    history = cnn.fit(x = training_set, validation_data = test_set, epochs = 10, callbacks=[loss_callback, acc_callback])\n    results[opt] = history\n    preds[opt] = cnn.predict_generator(test_set, num_of_test_samples \/\/ batch_size +1)","f406c495":"PlotModel(results[\"Adam\"])","df2a5952":"PlotModel(results[\"RMSprop\"])","bae7146f":"plot_cm(preds['Adam'], test_set)","42eb4835":"plot_cm(preds['RMSprop'], test_set)","39b2acdb":"performance = {\n    'VGG16': [0, 0],\n    'InceptionV3': [0, 0],\n    'ResNet152V2': [0, 0],\n    'Xception': [0, 0]\n}\n\nEPOCHS = 10","1edfc037":"from keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input as vgg","f9259e6a":"train_datagen = ImageDataGenerator(preprocessing_function=vgg)\n\nvgg_training_set = train_datagen.flow_from_dataframe(\n    dataframe=train_df,\n    directory=None,\n    x_col=\"Location\",\n    y_col=\"Class\",\n    weight_col=None,\n    target_size=(150, 150),\n    color_mode=\"rgb\",\n    classes=None,\n    class_mode=\"categorical\",\n    batch_size=32,\n    shuffle=True,\n    seed=None,\n    save_to_dir=None,\n    save_prefix=\"\",\n    save_format=\"png\",\n    subset=None,\n    interpolation=\"nearest\",\n    validate_filenames=True,\n)\n\ntest_datagen = ImageDataGenerator(preprocessing_function=vgg)\n\nvgg_test_set = test_datagen.flow_from_dataframe(\n    dataframe=test_df,\n    directory=None,\n    x_col=\"Location\",\n    y_col=\"Class\",\n    weight_col=None,\n    target_size=(150, 150),\n    color_mode=\"rgb\",\n    classes=None,\n    class_mode=\"categorical\",\n    batch_size=32,\n    shuffle=False,\n    seed=None,\n    save_to_dir=None,\n    save_prefix=\"\",\n    save_format=\"png\",\n    subset=None,\n    interpolation=\"nearest\",\n    validate_filenames=True,\n)","1a2b75a9":"vgg16 = VGG16(include_top=False, input_shape=(150, 150, 3))\n\n# Freeze the layers\nfor layer in vgg16.layers:\n    layer.trainable = False\nflat1 = tf.keras.layers.Flatten()(vgg16.layers[-1].output)\nl1 = tf.keras.layers.Dense(units=1024, activation='relu')(flat1)\nd1 = tf.keras.layers.Dropout(0.2)(l1)\nl2 = tf.keras.layers.Dense(units=512, activation='relu')(d1)\noutput = tf.keras.layers.Dense(units=6, activation='softmax')(l2)\n\nvgg16 = tf.keras.Model(inputs=vgg16.inputs, outputs=output)","ed8a1910":"vgg16.summary()","bd9e6c6a":"#visualkeras.layered_view(vgg16)","c398d419":"vgg16.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\nhistory = vgg16.fit(x = vgg_training_set, validation_data = vgg_test_set, epochs = EPOCHS)","c6533629":"train_loss, train_acc = vgg16.evaluate(vgg_training_set)\nval_loss, val_acc = vgg16.evaluate(vgg_test_set)\n\n\nprint(f\"Train Loss: {train_loss}\")\nprint(f\"Train Accuracy: {train_acc*100}\")\nprint()\nprint(f\"Validation Loss: {val_loss}\")\nprint(f\"Validation Accuracy: {val_acc*100}\")\n\nperformance['VGG16'] = [train_acc, val_acc]","ad7d3363":"PlotModel(history)","f66c3a25":"vgg_pred = vgg16.predict_generator(vgg_test_set, num_of_test_samples \/\/ batch_size +1)\nplot_cm(vgg_pred, vgg_test_set)","b8dee1e4":"from keras.applications import InceptionV3\nfrom keras.applications.inception_v3 import preprocess_input as inc","a455eaca":"train_datagen = ImageDataGenerator(preprocessing_function=inc)\n\nincep_training_set = train_datagen.flow_from_dataframe(\n    dataframe=train_df,\n    directory=None,\n    x_col=\"Location\",\n    y_col=\"Class\",\n    weight_col=None,\n    target_size=(150, 150),\n    color_mode=\"rgb\",\n    classes=None,\n    class_mode=\"categorical\",\n    batch_size=32,\n    shuffle=True,\n    seed=None,\n    save_to_dir=None,\n    save_prefix=\"\",\n    save_format=\"png\",\n    subset=None,\n    interpolation=\"nearest\",\n    validate_filenames=True,\n)\n\ntest_datagen = ImageDataGenerator(preprocessing_function=inc)\n\nincep_test_set = test_datagen.flow_from_dataframe(\n    dataframe=test_df,\n    directory=None,\n    x_col=\"Location\",\n    y_col=\"Class\",\n    weight_col=None,\n    target_size=(150, 150),\n    color_mode=\"rgb\",\n    classes=None,\n    class_mode=\"categorical\",\n    batch_size=32,\n    shuffle=False,\n    seed=None,\n    save_to_dir=None,\n    save_prefix=\"\",\n    save_format=\"png\",\n    subset=None,\n    interpolation=\"nearest\",\n    validate_filenames=True,\n)","eb255314":"incep = InceptionV3(include_top=False, input_shape=(150, 150, 3))\nfor layer in incep.layers:\n    layer.trainable = False\nflat1 = tf.keras.layers.Flatten()(incep.layers[-1].output)\nl1 = tf.keras.layers.Dense(units=1024, activation='relu')(flat1)\nd1 = tf.keras.layers.Dropout(0.2)(l1)\nl2 = tf.keras.layers.Dense(units=512, activation='relu')(d1)\noutput = tf.keras.layers.Dense(units=6, activation='softmax')(l2)\n\nincep = tf.keras.Model(inputs=incep.inputs, outputs=output)","19a7b2cf":"incep.summary()","f2679f57":"#visualkeras.layered_view(incep)","52e23057":"incep.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\nhistory = incep.fit(x = incep_training_set, validation_data = incep_test_set, epochs = EPOCHS)","d5179cca":"train_loss, train_acc = incep.evaluate(incep_training_set)\nval_loss, val_acc = incep.evaluate(incep_test_set)\n\n\nprint(f\"Train Loss: {train_loss}\")\nprint(f\"Train Accuracy: {train_acc*100}\")\nprint()\nprint(f\"Validation Loss: {val_loss}\")\nprint(f\"Validation Accuracy: {val_acc*100}\")\n\nperformance['InceptionV3'] = [train_acc, val_acc]","f303da00":"PlotModel(history)","aed97170":"pred = incep.predict_generator(incep_test_set, num_of_test_samples \/\/ batch_size +1)\nplot_cm(pred, incep_test_set)","88fe52b8":"from keras.applications import ResNet152V2\nfrom keras.applications.resnet_v2 import preprocess_input as res","8e5b77e1":"train_datagen = ImageDataGenerator(preprocessing_function=res)\n\nres_training_set = train_datagen.flow_from_dataframe(\n    dataframe=train_df,\n    directory=None,\n    x_col=\"Location\",\n    y_col=\"Class\",\n    weight_col=None,\n    target_size=(150, 150),\n    color_mode=\"rgb\",\n    classes=None,\n    class_mode=\"categorical\",\n    batch_size=32,\n    shuffle=True,\n    seed=None,\n    save_to_dir=None,\n    save_prefix=\"\",\n    save_format=\"png\",\n    subset=None,\n    interpolation=\"nearest\",\n    validate_filenames=True,\n)\n\ntest_datagen = ImageDataGenerator(preprocessing_function=res)\n\nres_test_set = test_datagen.flow_from_dataframe(\n    dataframe=test_df,\n    directory=None,\n    x_col=\"Location\",\n    y_col=\"Class\",\n    weight_col=None,\n    target_size=(150, 150),\n    color_mode=\"rgb\",\n    classes=None,\n    class_mode=\"categorical\",\n    batch_size=32,\n    shuffle=False,\n    seed=None,\n    save_to_dir=None,\n    save_prefix=\"\",\n    save_format=\"png\",\n    subset=None,\n    interpolation=\"nearest\",\n    validate_filenames=True,\n)","23e38162":"resnet = ResNet152V2(include_top=False, input_shape=(150, 150, 3))\nfor layer in resnet.layers:\n    layer.trainable = False\nflat1 = tf.keras.layers.Flatten()(resnet.layers[-1].output)\nl1 = tf.keras.layers.Dense(units=1024, activation='relu')(flat1)\nd1 = tf.keras.layers.Dropout(0.2)(l1)\nl2 = tf.keras.layers.Dense(units=512, activation='relu')(d1)\noutput = tf.keras.layers.Dense(units=6, activation='softmax')(l2)\n\nresnet = tf.keras.Model(inputs=resnet.inputs, outputs=output)","ae70aa70":"resnet.summary()","38532c75":"#visualkeras.layered_view(resnet)","87feacd9":"resnet.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\nhistory = resnet.fit(x = res_training_set, validation_data = res_test_set, epochs = EPOCHS)","235e61da":"train_loss, train_acc = resnet.evaluate(res_training_set)\nval_loss, val_acc = resnet.evaluate(res_test_set)\n\n\nprint(f\"Train Loss: {train_loss}\")\nprint(f\"Train Accuracy: {train_acc*100}\")\nprint()\nprint(f\"Validation Loss: {val_loss}\")\nprint(f\"Validation Accuracy: {val_acc*100}\")\n\nperformance['ResNet152V2'] = [train_acc, val_acc]","4356df69":"PlotModel(history)","a5fa49eb":"pred = resnet.predict_generator(res_test_set, num_of_test_samples \/\/ batch_size +1)\nplot_cm(pred, res_test_set)","cc538076":"from keras.applications import Xception\nfrom keras.applications.xception import preprocess_input as xcp","19220f34":"train_datagen = ImageDataGenerator(preprocessing_function=xcp)\n\nxcp_training_set = train_datagen.flow_from_dataframe(\n    dataframe=train_df,\n    directory=None,\n    x_col=\"Location\",\n    y_col=\"Class\",\n    weight_col=None,\n    target_size=(150, 150),\n    color_mode=\"rgb\",\n    classes=None,\n    class_mode=\"categorical\",\n    batch_size=32,\n    shuffle=True,\n    seed=None,\n    save_to_dir=None,\n    save_prefix=\"\",\n    save_format=\"png\",\n    subset=None,\n    interpolation=\"nearest\",\n    validate_filenames=True,\n)\n\ntest_datagen = ImageDataGenerator(preprocessing_function=xcp)\n\nxcp_test_set = test_datagen.flow_from_dataframe(\n    dataframe=test_df,\n    directory=None,\n    x_col=\"Location\",\n    y_col=\"Class\",\n    weight_col=None,\n    target_size=(150, 150),\n    color_mode=\"rgb\",\n    classes=None,\n    class_mode=\"categorical\",\n    batch_size=32,\n    shuffle=False,\n    seed=None,\n    save_to_dir=None,\n    save_prefix=\"\",\n    save_format=\"png\",\n    subset=None,\n    interpolation=\"nearest\",\n    validate_filenames=True,\n)","009bd532":"xception = Xception(include_top=False, input_shape=(150, 150, 3))\nfor layer in xception.layers:\n    layer.trainable = False\nflat1 = tf.keras.layers.Flatten()(xception.layers[-1].output)\nl1 = tf.keras.layers.Dense(units=1024, activation='relu')(flat1)\nd1 = tf.keras.layers.Dropout(0.2)(l1)\nl2 = tf.keras.layers.Dense(units=512, activation='relu')(d1)\noutput = tf.keras.layers.Dense(units=6, activation='softmax')(l2)\n\nxception = tf.keras.Model(inputs=xception.inputs, outputs=output)","dabfafcb":"xception.summary()","80e067c5":"xception.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\nhistory = xception.fit(x = xcp_training_set, validation_data = xcp_test_set, epochs = EPOCHS)","ab68371c":"train_loss, train_acc = xception.evaluate(xcp_training_set)\nval_loss, val_acc = xception.evaluate(xcp_test_set)\n\n\nprint(f\"Train Loss: {train_loss}\")\nprint(f\"Train Accuracy: {train_acc*100}\")\nprint()\nprint(f\"Validation Loss: {val_loss}\")\nprint(f\"Validation Accuracy: {val_acc*100}\")\n\nperformance['Xception'] = [train_acc, val_acc]","c92c28f7":"PlotModel(history)","167b36b5":"pred = xception.predict_generator(xcp_test_set, num_of_test_samples \/\/ batch_size +1)\nplot_cm(pred, xcp_test_set)","ecf0b3e0":"model_scores_df=pd.DataFrame(performance).T\nmodel_scores_df.columns = ['Train', 'Validation']\nmodel_scores_df.plot(kind='bar', figsize=(12, 8), color=['black', 'red'], rot=15)                                       \nplt.title(\"Comparison Graph\")\nplt.xlabel(\"Model\")\nplt.ylabel(\"Accuracy\")\nplt.show()","f08c0bad":"base1 = VGG16()\nbase1.trainable = False\nbase1 = tf.keras.Model(inputs=base1.inputs, outputs=base1.layers[-2].output)\n\nbase2 = InceptionV3()\nbase2.trainable = False\nbase2 = tf.keras.Model(inputs=base2.inputs, outputs=base2.layers[-2].output)\n\nbase3 = ResNet152V2()\nbase3.trainable = False\nbase3 = tf.keras.Model(inputs=base3.inputs, outputs=base3.layers[-2].output)\n\nbase4 = Xception()\nbase4.trainable = False\nbase4 = tf.keras.Model(inputs=base4.inputs, outputs=base4.layers[-2].output)\n\nmodels = [\n     base1,\n     base2,\n     base3,\n     base4\n]\n\n#filename = \"Models\"\n#pickle.dump(models, open(filename, 'wb'))","2974dbdd":"preprocess = [\n    vgg,\n    inc,\n    res,\n    xcp\n]\n\nsizes = [\n    (224, 224),\n    (299, 299),\n    (224, 224),\n    (229, 229)\n]","bfd4706d":"train = []\nvalidation = []\n\nfor i in range(len(models)):\n    pre = preprocess[i]\n    size = sizes[i]\n    \n    datagen = ImageDataGenerator(preprocessing_function=pre)\n\n    training_set = datagen.flow_from_dataframe(\n        dataframe=train_df,\n        directory=None,\n        x_col=\"Location\",\n        y_col=\"Class\",\n        weight_col=None,\n        target_size=size,\n        color_mode=\"rgb\",\n        classes=None,\n        class_mode=\"categorical\",\n        batch_size=32,\n        shuffle=False,\n        seed=None,\n        save_to_dir=None,\n        save_prefix=\"\",\n        save_format=\"png\",\n        subset=None,\n        interpolation=\"nearest\",\n        validate_filenames=True,\n    )\n\n\n    validation_set = datagen.flow_from_dataframe(\n        dataframe=test_df,\n        directory=None,\n        x_col=\"Location\",\n        y_col=\"Class\",\n        weight_col=None,\n        target_size=size,\n        color_mode=\"rgb\",\n        classes=None,\n        class_mode=\"categorical\",\n        batch_size=32,\n        shuffle=False,\n        seed=None,\n        save_to_dir=None,\n        save_prefix=\"\",\n        save_format=\"png\",\n        subset=None,\n        interpolation=\"nearest\",\n        validate_filenames=True,\n    )\n    \n    \n    train.append(training_set)\n    validation.append(validation_set)","cb0e1430":"train_features = []\nval_features = []\n\nfor i in range(len(models)):\n    model = models[i]\n    train_f = model.predict(train[i])\n    val_f = model.predict(validation[i])\n    train_features.append(train_f)\n    val_features.append(val_f)","bcb6310b":"for item in train_features:\n    print(item.shape)","7d67e2c0":"X_train = None\nX_val = None\n\nfor i in range(0, len(train_features)):\n    if X_train is None:\n        X_train = train_features[i]\n        X_val = val_features[i]\n    else:\n        X_train = np.concatenate((X_train, train_features[i]), axis=1)\n        X_val = np.concatenate((X_val, val_features[i]), axis=1)","e343d9f6":"print(\"Shape of train: \" , X_train.shape)\nprint(\"Shape of validation: \" , X_val.shape)","c3a7a7e3":"y_train = train[0].classes\ny_val = validation[0].classes","88c144c9":"#train_df = pd.DataFrame(X_train)\n#val_df = pd.DataFrame(X_val)\n#train_df['Class'] = y_train\n#val_df['Class'] = y_val\n\n#train_df.to_csv(\"train.csv\", index=False)\n#val_df.to_csv(\"val.csv\", index=False)","d7961f91":"pickle.dump(X_train, open(\"X_train\", 'wb'))\npickle.dump(y_train, open(\"y_train\", 'wb'))\npickle.dump(X_val, open(\"X_val\", 'wb'))\npickle.dump(y_val, open(\"y_val\", 'wb'))","f8984a23":"from sklearn.model_selection import GridSearchCV\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score","6ee99da3":"#params = {\n#    \"C\": [0.1, 1, 10, 100]\n#}","015e1581":"#grid = GridSearchCV(\n#    estimator=svm.SVC(kernel='rbf'),\n#    param_grid=params,\n#    refit=True,\n#    verbose=3\n#)","9b4d2bc1":"#train = pd.read_csv(\".\/train.csv\") \n#val = pd.read_csv(\".\/val.csv\")\n\n#X_train = train.iloc[:, 0:-1].values\n#y_train = train.iloc[:, -1].values\n\n#X_val = val.iloc[:, 0:-1].values\n#y_val = val.iloc[:, -1].values","4b39418f":"#grid.fit(X_train, y_train)","f7e05dbb":"#grid.best_params_","ac95b764":"#grid.best_estimator_","ae30c03c":"#val_pred = grid.predict(X_val)","0440149b":"#print(f\"Validation Accuracy: {accuracy_score(y_val, val_pred)}\")","f8bb01ce":"#filename = \"SVM\"\n#pickle.dump(gird, open(filename, 'wb'))","9cb17d8d":"svc = svm.SVC(kernel='rbf', C=1)","79e72192":"svc.fit(X_train, y_train)","ad2e56cb":"val_pred = svc.predict(X_val)","a05cb5c2":"print(f\"Validation Accuracy: {accuracy_score(y_val, val_pred)}\")","ae2a17b2":"test = []\n\nfor i in range(len(models)):\n    pre = preprocess[i]\n    size = sizes[i]\n    \n    datagen = ImageDataGenerator(preprocessing_function=pre)\n    \n    test_set = datagen.flow_from_directory('..\/input\/intel-image-classification\/seg_pred',\n                                          target_size=size,\n                                          color_mode='rgb',\n                                          class_mode=None,\n                                          shuffle=False,\n                                          seed=None,\n                                          batch_size=32)\n    \n    test.append(test_set)\n    \ntest_features=[]\n\nfor i in range(len(models)):\n    model = models[i]\n    test_f = model.predict(test[i])\n    test_features.append(test_f)\n    \nX_test = None\n\nfor i in range(0, len(test_features)):\n    if X_test is None:\n        X_test = test_features[i]\n    else:\n        X_test = np.concatenate((X_test, test_features[i]), axis=1)","c412fa50":"print(\"Shape of test: \" , X_test.shape)","bc128095":"test_pred = svc.predict(X_test)","1545010c":"test_pred","48de658c":"pickle.dump(X_test, open(\"X_test\", 'wb'))\npickle.dump(test_pred, open(\"final_pred\", \"wb\"))","cc37035a":"#### Creating the Data Generator with the appropriate preprocessing function","32990f92":"---","8bdc8e04":"#### Training 2 CNNs with different optimizers to see the effect of the optimizer ","abcd6d3d":"#### Imports","cde3994a":"# Intel Image Classification","8127b3b3":"#### Create DataFrame containing the **Location** of the image and the **Class** that the image belongs to.","489efc27":"### The Intel Image Classification dataset contains images belonging to 1 of the following 6 classes:\n\n   * buildings\n   * forest\n   * glacier\n   * mountain\n   * sea\n   * street","b66d89b4":"# TRANSFER LEARNING","da6d33ed":"#### Function to plot the model statistics","a2b84239":"Since shuffle is set to false, we can extract the classes like this","ad5489b6":"## Creating the ImageDataGenerator","bc307197":"### Stack Features","851d56f9":"---","e705cbdf":"#### Function to plot the confusion matrix","0e7d3fd6":"#### Create the classifier","dc69c3e3":"## InceptionV3","c75c6d47":"#### Save the data just in case you need it to tune the SVM someother time","4e2686cc":"#### Imports","b1d9bc66":"#### Creating the data generator with the appropriate preprocessing function","affcd84c":"#### Install necessary libraries","99eb4bcb":"#### Imports","197a5557":"#### Install visualkeras to view the model architectures","39b865c9":"# Final Prediction","dbef9544":"#### Creating the data generator with the appropriate preprocessing function","e541f7a1":"#### Imports","71c0939a":"#### Freeze the layers and add new fully-connected layers","9bfea4a3":"Instead of using 1 or 2 models to extract features, all 4 models have been used to extract features and the features extracted from the different models are stacked together.\n\n(Using 1 model to extract features is more than enough)","eb67a45e":"## **VGG16**","73fd6b23":"### Models","05b7f8f1":"---","bf934994":"### Creating callbacks to decrease the learning rate if a specific value fluctuates","50eacc2c":"### Check the distibution of classes for both train and validation images","b3fe5089":"#### Create the data generators using the appropriate preprocessing function","63b9d250":"#### Freeze the model and add new fully-connected layers","a73d382f":"---","13a9533d":"A higher accuracy can be achieved by tuning the values of the hyper-parameters.\nHoweverm this process takes a long time and the lines have been commented.","fe572b83":"## ResNet152V2","f6d7fdfe":"### Extract Features","8fb0b045":"--- ","932bd0c7":"# Feature Extraction","797c1579":"## Xception","e7fa6bf9":"Transfer Learning can also be used to extract features from the images. The features can then be used to train any classifer to predict on these images","a4b175ba":"#### Freeze the layers of the model and add new fully-connected layers","85b8f817":"---","f921eed2":"---","40801446":"#### Freeze the layers and add new Dense Layers","91403abc":"### Plot 1 image from each of the 6 classes","8ba0ba2a":"# Baseline CNN Model","10c312d0":"---","87457dbf":"### The optimizers that we will be using are:\n1. Adam\n2. RMSprop","08c40a9f":"# Model Performance Comparison","97d86738":"# SVM","1facde86":"To improve the accuracy, transfer learning will be used.\n\nThe following pretrained models will be compared:\n* VGG16\n* InceptionV3\n* ResNet152V2\n* Xception"}}