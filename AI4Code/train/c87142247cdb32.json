{"cell_type":{"1ba320b5":"code","0b9ba0fb":"code","55a0613a":"code","79122bfe":"code","9481d891":"code","17ec03f8":"code","ab539483":"code","a113366e":"code","b6538461":"code","c711ba25":"code","3190b595":"code","b4a8e3a3":"code","cd7c6da1":"code","7c4a9aec":"code","ead3e2cd":"code","3575a04d":"code","8ecb622c":"code","ccc75cdf":"code","23784551":"code","477aeefa":"code","63d51195":"code","f792bcfe":"code","2ad749f3":"code","71c40101":"code","4f594203":"code","0a6a5245":"code","bfaff7f4":"code","fbb49ea5":"code","d2c7fd3e":"code","4537c20d":"code","f399bd78":"code","495f09b4":"code","1d254641":"code","a8cbda1d":"code","00528268":"code","51a1aa5e":"code","8faf6a83":"code","605563ff":"code","695c3c15":"code","f400aee6":"code","f495f7ff":"code","aa11250b":"code","b41471ce":"code","80fb0d90":"code","c6b00395":"code","0b230ecf":"code","f777230d":"code","0bb3de72":"code","54f96f2b":"code","8500dd65":"code","5cdffe10":"code","6614214c":"code","ddd99faa":"code","98996140":"code","b4ea9e4b":"code","96a1f0d5":"code","9b75daf5":"code","0d2bdcac":"code","31e53429":"code","aac8f3e6":"code","f93cf235":"code","5665dbfb":"code","dd73ff73":"code","662f4da8":"code","601f4e70":"code","b64c7de7":"code","cabb07de":"code","b6ad4d4d":"code","66271b2f":"code","ddd38664":"code","b9b40946":"code","61dbffa7":"code","a299c0b2":"code","ad74965e":"code","bd37379d":"code","ce6b098c":"code","2835ca35":"code","097e9ac7":"code","037ebd31":"code","a5a2625b":"code","6eb0b3e3":"code","7f694a34":"code","25096059":"code","c2a17e66":"code","cf47869d":"code","20c69356":"code","398f0b3c":"code","4a744818":"code","d6249305":"code","922a6893":"code","4b99d329":"code","2d8cff3c":"code","9bfd6846":"code","2a3e7685":"code","c8403be7":"code","56027122":"code","13664265":"code","59cfb359":"code","3d100b69":"code","2e057160":"code","152f9168":"code","126a449b":"markdown","38880247":"markdown","0b21d07c":"markdown","5c09c8a7":"markdown","40eaa5d7":"markdown","842858b6":"markdown","b2d08e9b":"markdown","3d8c45c9":"markdown","a4d6f70a":"markdown","27e5e61d":"markdown","f4c8e399":"markdown","e0dbc7bb":"markdown","82659659":"markdown","6713545a":"markdown","0231e6b0":"markdown","12d7cc61":"markdown","c4ba1735":"markdown","51e9e7ef":"markdown","96f2ecaf":"markdown","c1ee749c":"markdown","c9c0ade3":"markdown","73b6996c":"markdown","9a5df48c":"markdown","874610a9":"markdown","e69dae46":"markdown","5e07955a":"markdown","cf6dba78":"markdown","8e009884":"markdown","6e377fce":"markdown","74e63cbf":"markdown","e3720b4b":"markdown","fbf5004d":"markdown","106c0378":"markdown","08f1ae11":"markdown","9e4c0043":"markdown","ebf5a9a5":"markdown","fdac3c42":"markdown"},"source":{"1ba320b5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n","0b9ba0fb":"# Encoding \nfrom sklearn.preprocessing import LabelEncoder , OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n# Grid Search (Hyperparameter tuning)\nfrom sklearn.model_selection import GridSearchCV\n\n# ML Models\nfrom sklearn.svm import SVC\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\nfrom sklearn import model_selection\nimport warnings\nwarnings.filterwarnings(\"ignore\")","55a0613a":"dataset_raw = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndataset_traincsv = dataset_raw.copy(deep = True)\n\ndataset_testcsv = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndataset_testcsv_copy = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\ndataset_train_test = [dataset_traincsv , dataset_testcsv]","79122bfe":"dataset_traincsv.head()","9481d891":"dataset_testcsv.head() # Note no 'Survived' column","17ec03f8":"dataset_traincsv.shape","ab539483":"dataset_testcsv.shape","a113366e":"dataset_traincsv.columns","b6538461":"dataset_testcsv.columns","c711ba25":"dataset_traincsv.sample(10)","3190b595":"dataset_testcsv.sample(10)","b4a8e3a3":"dataset_traincsv.info()","cd7c6da1":"dataset_testcsv.info()","7c4a9aec":"dataset_traincsv.describe()","ead3e2cd":"dataset_testcsv.describe()","3575a04d":"dataset_traincsv[\"Sex\"].value_counts(dropna = False) #Nan values will also be counted","8ecb622c":"dataset_traincsv['Pclass'].value_counts(dropna = False)","ccc75cdf":"dataset_traincsv['Survived'].value_counts(dropna = False)","23784551":"dataset_traincsv['Embarked'].value_counts(dropna = False)","477aeefa":"dataset_traincsv.corr()","63d51195":"print('Train.csv columns with null values:\\n', dataset_traincsv.isnull().sum())","f792bcfe":"print('Test.csv columns with null values:\\n', dataset_testcsv.isnull().sum())","2ad749f3":"for dataset in dataset_train_test:\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)","71c40101":"dataset_traincsv.info()","4f594203":"dataset_testcsv.info()","0a6a5245":"# Dropping the 'PassengerId','Cabin' and 'Ticket' columns as they don't have any impact on result.\n\ndrop_column = ['PassengerId','Cabin', 'Ticket']\nfor dataset in dataset_train_test :\n    dataset.drop(drop_column, axis=1, inplace = True)","bfaff7f4":"dataset_traincsv.isnull().sum() #No Nan values in dataframe now","fbb49ea5":"dataset_testcsv.isnull().sum()","d2c7fd3e":"\nfor dataset in dataset_train_test:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1 \n\n    dataset['IsAlone'] = 1\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0\n\n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n    title_names = (dataset['Title'].value_counts() < 10)\n    dataset['Title'] = dataset['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)","4537c20d":"dataset_traincsv.sample(10)","f399bd78":"dataset_testcsv.sample(10)","495f09b4":"dataset_traincsv['Title'].value_counts()","1d254641":"dataset_testcsv['Title'].value_counts()","a8cbda1d":"dataset_traincsv.info()","00528268":"dataset_testcsv.info()","51a1aa5e":"dataset_traincsv['Sex'].value_counts()","8faf6a83":"dataset_traincsv['Embarked'].value_counts()","605563ff":"dataset_traincsv['Title'].value_counts()","695c3c15":"# So we will need Label Encoder for 'Sex' (As only 2 values) and OneHotEncoder for 'Embarked' and 'Title'.\n# We could actually use Label encoder for 'Title' too as there is a hierarchy i.e Masters,Mrs,Miss are more likely to survive .\n# Please let me know if you get the same accuracy. I will be sticking ot OneHotEncoder for 'Title'.\n\nlabelencoder = LabelEncoder()\nfor dataset in dataset_train_test:\n    dataset['Sex'] = labelencoder.fit_transform(dataset['Sex'])","f400aee6":"# Dropping the 'Name' column as it does not have any impact on result\n\nfor dataset in dataset_train_test:\n    dataset.drop('Name' , axis =1 , inplace = True)","f495f7ff":"dataset_traincsv.head()","aa11250b":"dataset_testcsv.head()","b41471ce":"# Now that 'Sex' is encoded check the co-relation matrix again :\n\ndataset_traincsv.corr()","80fb0d90":"# Re-ordering the Survived column to the last location just to increase the redability\nSurvived = dataset_traincsv['Survived']\ndataset_traincsv.drop(labels = ['Survived'] , axis = 1 , inplace = True )\ndataset_traincsv.insert(10 , 'Survived' , Survived)\ndataset_traincsv.sample(10)","c6b00395":"# for train.csv\nx = dataset_traincsv.iloc[: , [0,1,2,5,6,7,8,9]].values\ny = dataset_traincsv.iloc[: , 10].values\n\n# for test.csv\nx_2 = dataset_testcsv.iloc[: , [0,1,2,5,6,7,8,9]].values","0b230ecf":"x[0:9]","f777230d":"y[0:9]","0bb3de72":"x_2[0:9]","54f96f2b":"ct_x = ColumnTransformer([('encoder' , OneHotEncoder() , [4,7])] , remainder= 'passthrough')\nx = np.array(ct_x.fit_transform(x),dtype = float )\nx_2 = np.array(ct_x.fit_transform(x_2) , dtype = float)","8500dd65":"# Avoiding the Dummy variable trap (This is automatically cared for in ML models. However in some algos like backprop it is not taken care of.)\n# x = x[: , 1:]\n# x_2 = x_2[: , 1:]","5cdffe10":"x.shape # It is a matrix","6614214c":"y.shape # It is a vector","ddd99faa":"x[0] # Just checking if OneHotEncoding was done on not , by checking first row of the array","98996140":"x_2[0]","b4ea9e4b":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x , y , test_size = 0.25)","96a1f0d5":"x_train.shape","9b75daf5":"x_test.shape","0d2bdcac":"y_train.shape","31e53429":"y_test.shape","aac8f3e6":"from sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\nx_train = sc_x.fit_transform(x_train)\nx_test = sc_x.transform(x_test) # Only transform as we have applied fit in training set already in above line\n\n# Feature Scaling of x_2\nx_2 = sc_x.transform(x_2)","f93cf235":"x_train[0]","5665dbfb":"x_2[0]","dd73ff73":"classifier_base = SVC(kernel='linear' ,random_state= 0 )\nclassifier_base.fit(x_train , y_train)","662f4da8":"# Applying 10 fold cross validation to check the accuracy\naccuracies_base = model_selection.cross_validate(estimator=classifier_base , X=x_train , y= y_train , cv = 10 ) ","601f4e70":"accuracies_base","b64c7de7":"accuracies_base['test_score'].mean()*100","cabb07de":"parameters_hyper = [\n    \n    {'C' : [ i for i in range(1,10,1)] , \n     'gamma' : [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09 , 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1], \n     'probability' : [False , True],\n     'kernel' : ['linear']\n    } ,\n    \n    {'C' : [ i for i in range(1,10,1)] , \n     'gamma' : [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09 , 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1], \n     'probability' : [False , True],\n     'kernel' : ['rbf']\n    }\n]\n\ngrid_search_hyper = GridSearchCV(estimator=classifier_base, param_grid= parameters_hyper , scoring= 'accuracy' , cv= 10 , n_jobs= -1)\ngrid_search_hyper = grid_search_hyper.fit(x_train , y_train)\nbest_accuracy_hyper = grid_search_hyper.best_score_\nbest_parameters_hyper = grid_search_hyper.best_params_ ","b6ad4d4d":"best_accuracy_hyper","66271b2f":"best_parameters_hyper","ddd38664":"C_hyper = best_parameters_hyper.get('C')\ngamma_hyper = best_parameters_hyper.get('gamma')\nkernel_hyper = best_parameters_hyper.get('kernel')","b9b40946":"classifier_hyper = SVC(kernel=kernel_hyper ,C= C_hyper , gamma = gamma_hyper, random_state= 0 ) \nclassifier_hyper.fit(x_train , y_train)","61dbffa7":"# Checking the results with 10 Fold cross validation\n\naccuracies_hyper = model_selection.cross_validate(estimator=classifier_hyper , X=x_train , y= y_train , cv = 10 ) \n","a299c0b2":"accuracies_hyper","ad74965e":"accuracies_hyper['test_score'].mean()*100","bd37379d":"classifier_xgb = XGBClassifier()\nclassifier_xgb.fit(x_train , y_train)\n\n#  10 Fold Cross Validate\naccuracies_xgb = model_selection.cross_validate(estimator=classifier_xgb , X=x_train , y= y_train , cv = 10 )","ce6b098c":"accuracies_xgb","2835ca35":"accuracies_xgb['test_score'].mean()*100","097e9ac7":"classifier_rf = ensemble.RandomForestClassifier(n_estimators= 500, criterion='entropy' , random_state= 0)\nclassifier_rf.fit(x_train , y_train)\n\n# 10 fold Cross Validate\naccuracies_rf = model_selection.cross_validate(estimator=classifier_rf , X=x_train , y= y_train , cv = 10 )\n","037ebd31":"accuracies_rf","a5a2625b":"accuracies_rf['test_score'].mean()*100","6eb0b3e3":"import tensorflow as tf ","7f694a34":"# TensorFlow version check\ntf.__version__","25096059":"# Initializing the ANN\nclassifier_ann = tf.keras.models.Sequential() #We will add layers afterwards\n\n# Adding the input layer and first hidden layer\n\"\"\"nodes = number of output nodes (input nodes are taken care automatically) , activation - activation funct used  \"\"\"\nclassifier_ann.add(tf.keras.layers.Dense(units = 7 , activation='relu')) \n\n# Adding second hidden layer\nclassifier_ann.add(tf.keras.layers.Dense(units = 7 , activation='relu')) \n\n# Adding the output layer (We want to have probabilities as output)\n\"\"\"If no of categories is 3 or more then output_dim = 3 (or more) , activation = softmax\"\"\" \nclassifier_ann.add(tf.keras.layers.Dense(units = 1 , activation='sigmoid')) \n\n# Compile ANN (Applying SGD) - The backpropagation step\n\"\"\"For more than 3 classifiers use loss = categorical_crossentropy\"\"\"\nclassifier_ann.compile(optimizer='adam', loss='binary_crossentropy' , metrics= ['accuracy'] )\n\n# Fitting the ANN to the training set\nclassifier_ann.fit(x_train , y_train, batch_size= 10 , epochs= 400) \n","c2a17e66":"# Predict\ny_pred_ann = classifier_ann.predict(x_test)\ny_pred_ann = (y_pred_ann > 0.5)\n\n# Making the confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\ncm_ann = confusion_matrix(y_test , y_pred_ann)","cf47869d":"cm_ann","20c69356":"from sklearn.metrics import accuracy_score\n\naccuracy_score(y_test , y_pred_ann)*100","398f0b3c":"vote_est = [\n    #Ensemble Methods: http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassifier()),\n\n    #Gaussian Processes: http:\/\/scikit-learn.org\/stable\/modules\/gaussian_process.html#gaussian-process-classification-gpc\n    ('gpc', gaussian_process.GaussianProcessClassifier()),\n    \n    #GLM: http:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#logistic-regression\n    ('lr', linear_model.LogisticRegressionCV()),\n    \n    #Navies Bayes: http:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html\n    ('bnb', naive_bayes.BernoulliNB()),\n    ('gnb', naive_bayes.GaussianNB()),\n    \n    #Nearest Neighbor: http:\/\/scikit-learn.org\/stable\/modules\/neighbors.html\n    ('knn', neighbors.KNeighborsClassifier()),\n    \n    #SVM: http:\/\/scikit-learn.org\/stable\/modules\/svm.html\n    ('svc', svm.SVC(probability=True)),\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n   ('xgb', XGBClassifier())\n\n]\n\n#Hard Vote or majority rules\nvote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\n\n# Cross Validate\nvote_hard_cv = model_selection.cross_validate(vote_hard, x_train, y_train , cv = 10)\n","4a744818":"vote_hard_cv","d6249305":"vote_hard.fit(x_train , y_train)","922a6893":"vote_hard_cv['test_score'].mean()*100","4b99d329":"#Soft Vote or weighted probabilities\nvote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\n\n# Cross validate\nvote_soft_cv = model_selection.cross_validate(vote_soft, x_train , y_train , cv  = 10)","2d8cff3c":"vote_soft.fit(x_train, y_train)","9bfd6846":"vote_soft_cv","2a3e7685":"vote_soft_cv['test_score'].mean()*100","c8403be7":"# Predicting the values and setting the threshold for 1 as greater than 0.5\ny_pred_testcsv_ann = classifier_ann.predict(x_2)\ny_pred_testcsv_ann = (y_pred_testcsv_ann > 0.5) #returns values in True \/ False in a list of lists format\n\n# Converting True and False values to int\ny_pred_testcsv_ann = y_pred_testcsv_ann.astype(int)\n\n# Coverting list of list to 1 flat list\ny_predtestcsv_ann = [item for sublist in y_pred_testcsv_ann for item in sublist]\n\n# Converting the flat list to np array\ny_predtestcsv_ann = np.asarray(y_predtestcsv_ann , dtype = int)\n\n","56027122":"y_predtestcsv_ann","13664265":"y_pred_testcsv = vote_soft.predict(x_2)","59cfb359":"y_pred_testcsv","3d100b69":"dataset_testcsv_copy['Survived'] = y_predtestcsv_ann","2e057160":"dataset_testcsv_copy.info()","152f9168":"submit = dataset_testcsv_copy[['PassengerId','Survived']]\nsubmit.to_csv(\"..\/working\/submit.csv\", index=False)\n\nprint('Validation Data Distribution: \\n', dataset_testcsv_copy['Survived'].value_counts(normalize = True))\nsubmit.sample(10)","126a449b":"Using TensorFlow.Keras as keras now comes integrated to TensorFlow. Previous versions of my code used standalone Keras.","38880247":"Hard Vote","0b21d07c":"# Just exploring different dataframe functions ","5c09c8a7":"# 3. Creating (Feature Engineering)","40eaa5d7":"No need to split for x_2 as the whole dataset is x_test.","842858b6":"For Hyperparameter tuned SVC model","b2d08e9b":"# SVC model","3d8c45c9":"Hyperparameter tuning SVC","a4d6f70a":"# Cleaning - Correcting , Completing , Creating and Converting","27e5e61d":"# Optimizations","f4c8e399":"# Applying ANN","e0dbc7bb":"# Submit file","82659659":"# Splitting the data to Test and Training set","6713545a":"# Applying XGBoost","0231e6b0":"# Corelation matrix","12d7cc61":"This Titanic Dataset has 2 files train.csv and test.csv . However,test.csv does not have the 'Survived' column to cross verify our accuracy. So we will just use split train.csv to training and test sets and get the accuracy.","c4ba1735":"# 2. Cleaning","51e9e7ef":"# Feature Scaling","96f2ecaf":"Please refer  https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\/notebook#Table-of-Contents for more info on this section.\n\nHere i am creating new features :\n\n1. Title - This feature will be extracted from the 'Name' feature. The Name feature in itself is not much of use but the title of people (Master , Mrs ,Miss etc) was detremental in their survival.\n\n2. FamilySize - This feature is added to remove the SibSp and Parch features. We are basically trying to reduce the redundant features.\n\n3. IsAlone - This feature is for us to determine if a person who came alone (FamilySize = 1) had a lower or more chance of survival. Spoiler Alert : Person had a lower chance of survival as we will see in the coorelation matrix. ","c1ee749c":"We will be doing OneHotEncoding after the splitting is done as OneHotEncoder returns a nparray and it would have been difficult read if i had applied it right now.","c9c0ade3":"#                  1. Correcting","73b6996c":"No correction required","9a5df48c":"# Applying Random Forest\n","874610a9":"Soft vote","e69dae46":"# Encoding 'Sex' using Label Encoder","5e07955a":"# Splitting dataframe to independent and dependent variable","cf6dba78":"You can get a premature idea of what affect each variable has to Survived column.\nSince it only has numerical values and not words we cannot get the full picture. Here for eg we will see that 'Sex' had the biggest role to play but since it is a word variable it can't be seen yet. Once we use Label Encoding to convert to numerical values we can see the corelation. \n\nHere we can see 'Pclass' and 'Fare' had a big role to play.","8e009884":"Trying out these parameters","6e377fce":"Here we are treating the null values of our columns.","74e63cbf":"# Predicting values using ANN model and Tuned SVC (As it has the highest accuracy) on test.csv","e3720b4b":"# Importing some essential libraries","fbf5004d":"# Finding out how many categories are there for each categorical variable","106c0378":"For ANN","08f1ae11":"# Now doing the OneHotEncoder on 'Embarked' and 'Title' columns","9e4c0043":"We can further optimize this model by applying all the models and find the optimal hyperparameters of all of them .\nThis will take a considerable amount of time.","ebf5a9a5":"# Voting Classifiers","fdac3c42":"As seen above the 'Sex' has the most affect in survivng rate followed by 'Pclass' and 'Fare'"}}