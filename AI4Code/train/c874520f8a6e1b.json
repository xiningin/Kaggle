{"cell_type":{"dba34c88":"code","da79483a":"code","d2719eba":"code","b48d336a":"code","96acd589":"code","c507b8fb":"code","d4a59197":"code","9c751017":"code","15633baf":"code","48060cb4":"code","2f73c00a":"code","56e61e2e":"code","7c6eff82":"code","317b1f36":"code","435055a1":"code","0ae116e0":"code","55a1bdc0":"code","c35c9056":"code","8dc65a77":"code","2fa09b14":"code","64f1fe55":"code","36e5e4a8":"code","3b0d4b82":"code","3277e6d6":"code","0b15eeb0":"code","cfaaf279":"code","6789fdbf":"code","631f7876":"code","153c0326":"code","281a9143":"code","173bda78":"code","f5695e81":"markdown","5cb61d75":"markdown","67091cd2":"markdown","dc71a94f":"markdown","b4d3bf5b":"markdown","a71aac90":"markdown","24734b3f":"markdown","2ebf23bc":"markdown","12f14671":"markdown","c0b059fb":"markdown","b5c08d7d":"markdown","4efecf9f":"markdown","fa429c97":"markdown","d922f0fc":"markdown","e330f683":"markdown","d411334a":"markdown","9a61818f":"markdown","013d0eb7":"markdown","dabe1ee6":"markdown","347f232e":"markdown","3efc8c11":"markdown","cf4ec074":"markdown","7d7d792e":"markdown","74727743":"markdown","2c95e00a":"markdown","a43c403f":"markdown","942ddd11":"markdown","474c8b2a":"markdown","74d24141":"markdown","c5672266":"markdown","0fb46bdf":"markdown","8cf256b7":"markdown","d73baf0f":"markdown","215b097b":"markdown","674a29bf":"markdown","cd1f07fb":"markdown","817330e3":"markdown","0f3c7123":"markdown","686016d9":"markdown"},"source":{"dba34c88":"import numpy as np\nimport pandas as pd\nfrom scipy.optimize import curve_fit\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as colors\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff","da79483a":"in_videos = pd.read_csv('..\/input\/INvideos.csv')\nin_videos_categories = pd.read_json('..\/input\/IN_category_id.json')","d2719eba":"in_videos.head(1)","b48d336a":"in_videos.info()","96acd589":"in_videos = in_videos.drop(['description'], axis = 1)\n","c507b8fb":"################################### Use only once (Fails after 1st Attempt) ##################################\n# Transforming Trending date column to datetime format\nin_videos['trending_date'] = pd.to_datetime(in_videos['trending_date'], format='%y.%d.%m').dt.date\n\n# Transforming Trending date column to datetime format and splitting into two separate ones\npublish_time = pd.to_datetime(in_videos['publish_time'], format='%Y-%m-%dT%H:%M:%S.%fZ')\nin_videos['publish_date'] = publish_time.dt.date\nin_videos['publish_time'] = publish_time.dt.time\nin_videos['publish_hour'] = publish_time.dt.hour","d4a59197":"in_videos.head(1)","9c751017":"################################### Use only once (Fails after 1st Attempt) ##################################\n# We'll use a very nice python featur - dictionary comprehension, to extract most important data from IN_category_id.json\ncategories = {category['id']: category['snippet']['title'] for category in in_videos_categories['items']}\n\n# Now we will create new column that will represent name of category\nin_videos.insert(4, 'category', in_videos['category_id'].astype(str).map(categories))\nin_videos.tail(3)","15633baf":"in_videos_first = in_videos.copy() \nin_videos_first['dislike_percentage'] = in_videos['dislikes'] \/ (in_videos['dislikes'] + in_videos['likes'])\nprint(in_videos_first['dislike_percentage'].head(5))","48060cb4":"# Helper function\ndef numberOfUpper(string):\n    i = 0\n    for word in string.split():\n        if word.isupper():\n            i += 1\n    return(i)\n\nin_videos_first[\"all_upper_in_title\"] = in_videos[\"title\"].apply(numberOfUpper)\nprint(in_videos_first[\"all_upper_in_title\"].tail(5))","2f73c00a":"in_videos_first['likes_log'] = np.log(in_videos['likes'] + 1)\nin_videos_first['views_log'] = np.log(in_videos['views'] + 1)\nin_videos_first['dislikes_log'] = np.log(in_videos['dislikes'] + 1)\nin_videos_first['comment_log'] = np.log(in_videos['comment_count'] + 1)\n\nplt.figure(figsize = (12,6))\n\nplt.subplot(221)\ng1 = sns.distplot(in_videos_first['views_log'])\ng1.set_title(\"VIEWS LOG DISTRIBUITION\", fontsize=16)\n\nplt.subplot(224)\ng2 = sns.distplot(in_videos_first['likes_log'],color='green')\ng2.set_title('LIKES LOG DISTRIBUITION', fontsize=16)\n\nplt.subplot(223)\ng3 = sns.distplot(in_videos_first['dislikes_log'], color='r')\ng3.set_title(\"DISLIKES LOG DISTRIBUITION\", fontsize=16)\n\nplt.subplot(222)\ng4 = sns.distplot(in_videos_first['comment_log'])\ng4.set_title(\"COMMENTS LOG DISTRIBUITION\", fontsize=16)\n\nplt.subplots_adjust(wspace = 0.2, hspace = 0.4,top = 0.9)\n\nplt.show()","56e61e2e":"in_videos_last = in_videos.drop_duplicates(subset=['video_id'], keep='last', inplace=False)\nin_videos_first = in_videos.drop_duplicates(subset=['video_id'], keep='first', inplace=False)\nprint(in_videos_last.head(2))","7c6eff82":"print(\"in_videos dataset contains {} videos\".format(in_videos.shape[0]))\nprint(\"in_videos_first dataset contains {} videos\".format(in_videos_first.shape[0]))\nprint(\"in_videos_last dataset contains {} videos\".format(in_videos_last.shape[0]))","317b1f36":"in_videos[\"days_before_trend\"] = (in_videos.trending_date - in_videos.publish_date) \/ np.timedelta64(1, 'D')\nin_videos[\"days_before_trend\"] = in_videos[\"days_before_trend\"].astype(int)\nin_videos.tail(3)","435055a1":"in_videos.isnull().sum()","0ae116e0":"null_data = in_videos[in_videos[\"category\"].isnull()]\nnull_data.head(2)","55a1bdc0":"in_videos[\"category\"].fillna(\"Nonprofits & Activism\", inplace = True) \nin_videos[in_videos[\"category_id\"]  == 29]\nin_videos[in_videos[\"category_id\"]  == 29].tail(3)","c35c9056":"in_videos.loc[(in_videos['days_before_trend'] < 1), 'days_before_trend'] = 1\nin_videos[\"views_per_day\"] = in_videos[\"views\"].astype(int) \/ in_videos[\"days_before_trend\"]\nin_videos[\"views_per_day\"] = in_videos[\"views_per_day\"]\nin_videos.tail(3)","8dc65a77":"in_videos.isnull().sum()","2fa09b14":"in_videos.to_csv('preprocessedIndia.csv',index=False)","64f1fe55":"# Initialization of the list storing counters for subsequent publication hours\npublish_h = [0] * 24\n\nfor index, row in in_videos_first.iterrows():\n    publish_h[row[\"publish_hour\"]] += 1\n    \nvalues = publish_h\nind = np.arange(len(values))\n\n\n# Creating new plot\nfig = plt.figure(figsize=(20,10))\nax = fig.add_subplot(111)\nax.yaxis.grid()\nax.xaxis.grid()\nbars = ax.bar(ind, values)\n\n# Sampling of Colormap\nfor i, b in enumerate(bars):\n    b.set_color(plt.cm.viridis((values[i] - min(values))\/(max(values)- min(values))))\n    \nplt.ylabel('Number of videos that got trending', fontsize=20)\nplt.xlabel('Time of publishing', fontsize=20)\nplt.title('Best time to publish video', fontsize=35, fontweight='bold')\nplt.xticks(np.arange(0, len(ind), len(ind)\/6), [0, 4, 8, 12, 16, 20])\n\nplt.show()","36e5e4a8":"h_labels = [x.replace('_', ' ').title() for x in \n            list(in_videos.select_dtypes(include=['number', 'bool']).columns.values)]\n\nfig, ax = plt.subplots(figsize=(10,6))\n_ = sns.heatmap(in_videos.corr(), annot=True, xticklabels=h_labels, yticklabels=h_labels, cmap=sns.cubehelix_palette(as_cmap=True), ax=ax)","3b0d4b82":"from IPython.display import HTML, display\n\n# We choose the 10 most trending videos\nselected_columns = ['title', 'channel_title', 'thumbnail_link', 'publish_date', 'category']\n\nmost_frequent = in_videos.groupby(selected_columns)['video_id'].agg(\n    {\"code_count\": len}).sort_values(\n    \"code_count\", ascending=False\n).head(10).reset_index()\n\n# Construction of HTML table with miniature photos assigned to the most popular movies\ntable_content = ''\nmax_title_length = 50\n\nfor date, row in most_frequent.T.iteritems():\n    HTML_row = '<tr>'\n    HTML_row += '<td><img src=\"' + str(row[2]) + '\"style=\"width:100px;height:100px;\"><\/td>'\n    HTML_row += '<td>' + str(row[1]) + '<\/td>'\n    HTML_row += '<td>' + str(row[0])  + '<\/td>'\n    HTML_row += '<td>' + str(row[4]) + '<\/td>'\n    HTML_row += '<td>' + str(row[3]) + '<\/td>'\n    \n    table_content += HTML_row + '<\/tr>'\n\ndisplay(HTML(\n    '<table><tr><th>Photo<\/th><th>Channel Name<\/th><th style=\"width:250px;\">Title<\/th><th>Category<\/th><th>Publish Date<\/th><\/tr>{}<\/table>'.format(table_content))\n)","3277e6d6":"max_title_length = 20\nnumber_of_creators = 20\n\ntop_creators = in_videos.groupby(['channel_title'])['channel_title'].agg(\n    {\"code_count\": len}).sort_values(\n    \"code_count\", ascending=False\n).head(number_of_creators).reset_index()\n\ntrace1 = go.Bar(\n    y = [(x if len(x) <= max_title_length else x[:max_title_length] + \"...\") for x in top_creators.channel_title.values][::-1],\n    x = top_creators['code_count'].tolist()[::-1],\n    name = \"Top creators\",\n    orientation = 'h',\n    marker=dict(\n        color='rgba(55, 128, 191, 0.7)',\n        line=dict(\n            color='rgba(55, 128, 191, 1.0)',\n            width=2,\n        )\n    ),\n)\n\ndata = [trace1]\n\nlayout = go.Layout(\n    title = 'Most influential creators',\n    width=900,\n    height=600,\n    margin=go.Margin(\n        l=180,\n        r=50,\n        b=80,\n        t=80,\n        pad=10\n    ),\n    paper_bgcolor='rgb(244, 238, 225)',\n    plot_bgcolor='rgb(244, 238, 225)',\n    yaxis = dict(\n        anchor = 'x',\n        rangemode='tozero',\n        tickfont=dict(\n            size=10\n        ),\n        ticklen=1\n    ), \n    xaxis = dict(\n        title= 'Number of times video made by creator got trending',\n        anchor = 'x',\n        rangemode='tozero'\n    ), \n    legend=dict(x=0.6, y=0.07)\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","0b15eeb0":"max_title_length = 30\nnumber_of_creators = 12\n\ntop_creators = in_videos.groupby(['category'])['category'].agg(\n    {\"code_count\": len}).sort_values(\n    \"code_count\", ascending=False\n).head(number_of_creators).reset_index()\n\ntrace1 = go.Bar(\n    y = [(x if len(x) <= max_title_length else x[:max_title_length] + \"...\") for x in top_creators.category.values][::-1],\n    x = top_creators['code_count'].tolist()[::-1],\n    name = \"Top categories\",\n    orientation = 'h',\n    marker=dict(\n        color='rgba(55, 128, 191, 0.7)',\n        line=dict(\n            color='rgba(55, 128, 191, 1.0)',\n            width=2,\n        )\n    ),\n)\n\ndata = [trace1]\n\nlayout = go.Layout(\n    title = 'Most popular categories',\n    width=900,\n    height=600,\n    margin=go.Margin(\n        l=180,\n        r=50,\n        b=80,\n        t=80,\n        pad=10\n    ),\n    paper_bgcolor='rgb(244, 238, 225)',\n    plot_bgcolor='rgb(244, 238, 225)',\n    yaxis = dict(\n        anchor = 'x',\n        rangemode='tozero',\n        tickfont=dict(\n            size=10\n        ),\n        ticklen=1\n    ), \n    xaxis = dict(\n        title= 'The number of times the video of a given category was trending',\n        anchor = 'x',\n        rangemode='tozero'\n    ), \n    legend=dict(x=0.6, y=0.07)\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","cfaaf279":"# Average time interval between published and trending\nin_videos['interval'] = (pd.to_datetime(in_videos['trending_date']).dt.date - pd.to_datetime(in_videos['publish_date']).dt.date).astype('timedelta64[D]')\ndf_t = pd.DataFrame(in_videos['interval'].groupby(in_videos['category']).mean())\nplt.figure(figsize = (32,12))\nplt.plot(df_t, color='skyblue', linewidth=2)\nplt.title(\"Average Days to be trending video\", fontsize=25)\nplt.xlabel('Category',fontsize=22)\nplt.ylabel('Average Time Interval',fontsize=22)\nplt.tick_params(labelsize=14)\nplt.show();\n","6789fdbf":"print(type(in_videos[\"video_id\"]))","631f7876":"# dropping passed values \n\n#in_videos.drop(in_videos.video_id == '\"zUZ1z7FwLc8\",\"CLl1RbxDRAs\",\"z3V9LUA6VQM\", \"jElRtesCnlA\", \"qP67alYxSiU\", \"JSkOecmAFFo\", \"l3fRny54z4U\", \"UTVFNrRwL1o\", \"K6JyjjNnTlg\", \"4tEqzEo5uKY\", \"8vBjlhp73hU\", \"KskjXRkmJW4\", \"NTiSvK7c810\", \"sOwXjFMy17Y\", \"h6Z9mmSNJcw\"'), inplace = True) \nmax_title_length = 20\nnumber_of_late_bloomers = 15\nin_videos_first[\"days_before_trend\"]= in_videos[\"days_before_trend\"].astype(float)\nlate_bloomers = in_videos_first.sort_values([\"days_before_trend\"], ascending=False).head(number_of_late_bloomers)\nlate_bloomers_title = [(x if len(x) <= max_title_length else x[:max_title_length] + \"...\") for x in late_bloomers.title.values]\nlate_bloomers_days = late_bloomers.days_before_trend.values\nlate_bloomers_views = late_bloomers.views.values\n\ntrace1 = go.Bar(\n    x = late_bloomers_title,\n    y = late_bloomers_days,\n    name='Number of days',\n    marker=dict(\n        color='rgba(55, 128, 191, 0.7)',\n        line=dict(\n            color='rgba(55, 128, 191, 1.0)',\n            width=2,\n        )\n    )\n)\ntrace2 = go.Bar(\n    x = late_bloomers_title,\n    y = late_bloomers_views,\n    name='total views',\n    marker=dict(\n        color='rgba(219, 64, 82, 0.7)',\n        line=dict(\n            color='rgba(219, 64, 82, 1.0)',\n            width=2,\n        )\n    ),\n    yaxis='y2'\n)\n\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    barmode='group',\n    title = 'Late bloomers',\n    width=900,\n    height=500,\n    margin=go.Margin(\n        l=75,\n        r=75,\n        b=120,\n        t=80,\n        pad=10\n    ),\n    paper_bgcolor='rgb(244, 238, 225)',\n    plot_bgcolor='rgb(244, 238, 225)',\n    yaxis = dict(\n        title= 'Number of days until becoming trending',\n        anchor = 'x',\n        rangemode='tozero'\n    ),   \n    yaxis2=dict(\n        title='Total number of views',\n        titlefont=dict(\n            color='rgb(148, 103, 189)'\n        ),\n        tickfont=dict(\n            color='rgb(148, 103, 189)'\n        ),\n        overlaying='y',\n        side='right',\n        anchor = 'x',\n        rangemode = 'tozero',\n        dtick = 61000\n    ),\n    #legend=dict(x=-.1, y=1.2)\n    legend=dict(x=0.1, y=0.05)\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","153c0326":"max_title_length = 20\nnumber_of_late_bloomers = 10\nin_videos_first[\"dislikes\"]= in_videos[\"dislikes\"]\nin_videos_first['dislike_percentage'] = in_videos['dislikes'] \/ (in_videos['dislikes'] + in_videos['likes'])\nmost_disliked = in_videos_first.sort_values([\"dislikes\"], ascending=False).head(number_of_late_bloomers)\nmost_disliked_title = [(x if len(x) <= max_title_length else x[:max_title_length] + \"...\") for x in late_bloomers.title.values]\nmost_disliked_l_number = most_disliked.likes.values\nmost_disliked_dl_number = most_disliked.dislikes.values\nmost_disliked_dl_percentage = most_disliked.dislike_percentage.values\n\ntrace1 = go.Bar(\n    x = most_disliked_title,\n    y = most_disliked_l_number,\n    name='Number of likes',\n    marker=dict(\n        color='rgba(55, 128, 191, 0.7)',\n        line=dict(\n            color='rgba(55, 128, 191, 1.0)',\n            width=2,\n        )\n    )\n)\ntrace2 = go.Bar(\n    x = most_disliked_title,\n    y = most_disliked_dl_number,\n    name='Number of dislikes',\n    marker=dict(\n        color='rgba(219, 64, 82, 0.7)',\n        line=dict(\n            color='rgba(219, 64, 82, 1.0)',\n            width=2,\n        )\n    )\n)\n\ntrace3 = go.Scatter(\n    x = most_disliked_title,\n    y = most_disliked_dl_percentage,\n    name='Dislike percentage',\n    mode = 'markers',\n    marker=dict(\n        symbol=\"hexagon-dot\",\n        size=15\n    ),\n    yaxis='y2'\n)\n\ndata = [trace1, trace2, trace3]\nlayout = go.Layout(\n    barmode='group',\n    title = 'No such thing as bad press, right?',\n    width=900,\n    height=500,\n    margin=go.Margin(\n        l=75,\n        r=75,\n        b=120,\n        t=80,\n        pad=10\n    ),\n    paper_bgcolor='rgb(244, 238, 225)',\n    plot_bgcolor='rgb(244, 238, 225)',\n    yaxis = dict(\n        title= 'Number of likes\/dislikes',\n        anchor = 'x',\n        rangemode='tozero'\n    ),   \n    yaxis2=dict(\n        title='Dislike percentage',\n        titlefont=dict(\n            color='rgb(148, 103, 189)'\n        ),\n        tickfont=dict(\n            color='rgb(148, 103, 189)'\n        ),\n        overlaying='y',\n        side='right',\n        anchor = 'x',\n        rangemode = 'tozero',\n        dtick = 0.165\n    ),\n    legend=dict(x=0.75, y=1)\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","281a9143":"from wordcloud import WordCloud, STOPWORDS\nfrom PIL import Image\nimport urllib\nimport requests\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nmask = np.array(Image.open(requests.get('http:\/\/www.clker.com\/cliparts\/O\/i\/x\/Y\/q\/P\/yellow-house-hi.png', stream=True).raw))\n\n# This function takes in your text and your mask and generates a wordcloud. \ndef generate_wordcloud(mask):\n    word_cloud = WordCloud(width = 512, height = 512, background_color='white', stopwords=STOPWORDS, mask=mask).generate(str(in_videos[\"tags\"]))\n    plt.figure(figsize=(10,8),facecolor = 'white', edgecolor='blue')\n    plt.imshow(word_cloud)\n    plt.axis('off')\n    plt.tight_layout(pad=0)\n    plt.show()\n    \n#Run the following to generate your wordcloud\ngenerate_wordcloud(mask)","173bda78":"in_videos_first['likes_log'] = np.log(in_videos['likes'] + 1)\nin_videos_first['dislikes_log'] = np.log(in_videos['dislikes'] + 1)\nhist_data = [in_videos_first[\"dislikes_log\"].values, in_videos_first[\"likes_log\"].values]\n\ngroup_labels = ['Dislikes log distribution', 'Likes log distribution']\ncolors = ['#A6ACEC', '#63F5EF']\n\n# Create distplot with curve_type set to 'normal'\nfig = ff.create_distplot(hist_data, group_labels, colors=colors,\n                         bin_size=0.5, show_rug=False)\n\n# Add title\nfig['layout'].update(title='Likes vs dislikes', legend=dict(x=0.65, y=0.8))\n\n# Plot!\npy.iplot(fig, filename='Hist and Curve')","f5695e81":"# 1. Importing dataset and data preprocessing\n<a id=\"importing_dataset_and_data_preprocessing\"><\/a>","5cb61d75":"<b>NOTE:<\/b> Next we will connect the ```category``` with the ```category_id``` they belong to. We will associate the information in two files: ```INvideos.csv``` and ```IN_category_id.json``` .","67091cd2":"### 1.4. Data preprocessing and feature engineering\n<a id=\"data_preprocessing_and_feature_engineering\"><\/a>","dc71a94f":"### 1.4.7. What about duplicates? (For Visualization Section)\n<a id=\"what_about_duplicates\"><\/a>","b4d3bf5b":"### 1.2. Importing dataset\n<a id=\"importing_datasets\"><\/a>","a71aac90":"If you really enjoyed above kernel, then you might want to take a close look at following interesting kernels  :-\n\n1. House Prices : Visualization & Prediction - https:\/\/www.kaggle.com\/iamrohitsingh\/house-prices-visualization-prediction\n\n2. Titanic : Visualization & Prediction - https:\/\/www.kaggle.com\/iamrohitsingh\/titanic-visualization-prediction\n\n3. Flight Crash Investigation - https:\/\/www.kaggle.com\/iamrohitsingh\/flight-crash-investigation\/ ","24734b3f":"### 1.4.4. Dislike percentage (For Visualization Section)\n<a id=\"dislike_percentage\"><\/a>","2ebf23bc":"### 2.1. Best time to publish video\n<a id=\"best_time_to_publish_video\"><\/a>","12f14671":"# Table of Contents\n\n* [1. Importing dataset and data preprocessing](#importing_dataset_and_data_preprocessing) <br>\n * [1.1. Importing essential libraries](#importing_essential_libraries) <br>\n * [1.2. Importing datasets](#importing_datasets) <br>\n * [1.3. Let's summarize the datasets](#lets_summarize_the_dataset) <br>\n * [1.4. Data preprocessing and feature engineering](#data_preprocessing_and_feature_engineering) <br>\n   * [1.4.1. Removing Column 'Description'](#description-removing) <br>\n   * [1.4.2. Datetime format of Trending date and Publish time](#datetime_format_of_trending_date_and_publish_time) <br>\n   * [1.4.3. Assignment of the film category](#assignment_of_the_film_category) <br>\n   * [1.4.4. Dislike percentage](#dislike_percentage) <br>\n   * [1.4.5. Number of words with all upper case in title](#number_of_words_with_all_upper_case_in_title) <br>\n   * [1.4.6. Distribution of basic parameters](#distribution_of_basic_parameters) <br>\n   * [1.4.7. What about duplicates?](#what_about_duplicates) <br>\n   * [1.4.8. Days before trend](#time_to_trend) <br>\n   * [1.4.9. Missing Value for Category Columns](#missing_value) <br>\n   * [1.4.10. Addition of column 'Views per day'](#views_per_day) <br>\n   * [1.4.11. Outputing the file in CSV Format](#output_file) <br>\n<br>\n* [2. Data Visualization](#data_visualization) <br>\n * [2.1. Best time to publish video](#best_time_to_publish_video) <br>\n * [2.2. Correlation between dataset variables](#correlation) <br>\n * [2.3. It got viral](#it_got_viral) <br>\n * [2.4. Most influential creators](#most_influential_creators) <br>\n * [2.5. Variety of topics](#variety_of_topics) <br>\n * [2.6. Average time interval](#avg_time_interval) <br>\n * [2.7. Late bloomers](#late_bloomers) <br>\n * [2.8. No such thing as bad press, right?](#no_such_thing_as_bad_press_right) <br>\n * [2.9. Tags wordcloud](#tags_wordcloud) <br>\n * [2.10. Likes vs dislikes distribution!](#likes_vs_dislikes_distribution) <br>\n\n* [3. References](#refer) <br>","c0b059fb":"# 2. Data Visualization\n<a id=\"data_visualization\"><\/a>","b5c08d7d":"### 1.4.1. Removing Column 'Description'\n<a id=\"description-removing\"><\/a>\nSummary and Titles are creating some issues with shifting the whole row into some next cell in the final csv output file. We are ignoring rows of data of that nature.","4efecf9f":"### 1.3. Let's summarize the dataset\n<a id=\"lets_summarize_the_dataset\"><\/a>","fa429c97":"### 1.4.11. Outputing the file in CSV Format \n<a id=\"output_file\"><\/a>","d922f0fc":"### 1.4.3. Addition of column 'category'\n<a id=\"assignment_of_the_film_category\"><\/a>","e330f683":"### 1.4.5. Number of words with all upper case in title (For Visualization Section)\n<a id=\"number_of_words_with_all_upper_case_in_title\"><\/a>","d411334a":"### 2.7. Late bloomers\n<a id=\"late_bloomers\"><\/a>\nThis section dedicated to videos that waited the longest before they became trending. I also checked how many views they had when they hit the YouTube home page.","9a61818f":"<b>NOTE:<\/b> Because many of the films have been trending you several times, we will create a separate datasets in which we will get rid of repetitions. Still, we leave the original dataset, because there is a lot of interesting information in it.","013d0eb7":"### 2.10. Likes vs Dislikes distribution!\n<a id=\"likes_vs_dislikes_distribution\"><\/a>","dabe1ee6":"<b>NOTE:<\/b> Firstly we will transform ```trending_date``` as well as ```publish_time``` from string to datetime format. This will allow us to easily perform arithmetic operations and compare these values. ```publish_time``` column will be divided into three separate ones ```publish_date```, ```publish_time``` and  ```publish_hour``` .","347f232e":"Inspirations are drawn from various Kaggle projects but majorly incentive is from the following :\n\n1. https:\/\/www.kaggle.com\/residentmario\/creating-reading-and-writing\n2. https:\/\/www.kaggle.com\/skalskip\/youtube-data-exploration-and-plotly-visualization\n3. https:\/\/www.kaggle.com\/kabure\/extensive-usa-youtube-eda \n\nOur GitHub Project Link - https:\/\/github.com\/RohitLearner\/Youtube_India_Data_Exploration ","3efc8c11":"### 2.9. Tags wordcloud\n<a id=\"tags_wordcloud\"><\/a>\nThis section dedicated to tags that support the videos to reach the trending list.","cf4ec074":"<b>NOTE:<\/b> Producing output as processed file after preprocessing step in the pipeline as .csv file. The next step will take preprocessed csv file as input for model to train. ","7d7d792e":"### 1.4.6. Distribution of basic parameters (For Visualization Section)\n<a id=\"distribution_of_basic_parameters\"><\/a>","74727743":"### 2.3. It got sensational viral\n<a id=\"it_got_viral\"><\/a>\nIt take few minutes to get the thumbnail from the internet (at live) for top 10 trending video from the dataset.","2c95e00a":"### 1.1. Importing essential libraries\n<a id=\"importing_essential_libraries\"><\/a> ","a43c403f":"### 2.4. Most influential creators (By Channel)\n<a id=\"most_influential_creators\"><\/a>","942ddd11":"# YouTube India Data Exploration\n### Data Exploration and Visualization With Python ( \u2b50\ufe0f Upvote my Notebook \u2014 it helps! )\n***\n\n","474c8b2a":"# 3. Acknowledgments\n<a id=\"refer\"><\/a>","74d24141":"### 2.5. Variety of topics\n<a id=\"variety_of_topics\"><\/a>","c5672266":"### 2.8. No such thing as bad press, right?\n<a id=\"no_such_thing_as_bad_press_right\"><\/a>\nThis section dedicated to videos that has gained popularity on YT by being disliked.","0fb46bdf":"### 2.2. Correlation between dataset variables\n<a id=\"correlation\"><\/a>\nNow let's see how the dataset variables are correlated with each other: for example, we would like to see how views and likes are correlated, meaning do views and likes increase and decrease together (positive correlation)? Does one of them increase when the other decrease and vice versa (negative correlation)? Or are they not correlated?\n\nCorrelation is represented as a value between -1 and +1 where +1 denotes the highest positive correlation, -1 denotes the highest negative correlation, and 0 denotes that there is no correlation.\n\nLet's visualize the correlation table between our dataset variables using a heatmap.","8cf256b7":"<img src='https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/e1\/Logo_of_YouTube_%282015-2017%29.svg'>","d73baf0f":"### 1.4.2. Datetime format of Trending date and Publish time\n<a id=\"datetime_format_of_trending_date_and_publish_time\"><\/a>","215b097b":"### 1.4.10. Addition of column 'Views per day'\n<a id=\"views_per_day\"><\/a>","674a29bf":"### 1.4.9. Missing Value for Category Columns\n<a id=\"missing_value\"><\/a>","cd1f07fb":"<b>NOTE:<\/b> We also count what percentage of assessments are negative ratings.","817330e3":"<b>NOTE:<\/b> Lastly we will create new feature ```days_before_trend``` representing the time (in days) between publication and the day when it became trending.","0f3c7123":"### 2.6. Average time interval\n<a id=\"avg_time_interval\"><\/a>\nThe average time interval for each category describes on average how fast a video can show up on the trending board. This is also a important criterion that which need to be cared about, because the longer time interval is, the larger the time cost will be.","686016d9":"### 1.4.8. Addition of column 'Days before trend'\n<a id=\"time_to_trend\"><\/a>"}}