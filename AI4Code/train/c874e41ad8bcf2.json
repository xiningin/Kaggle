{"cell_type":{"a7149af5":"code","cce8df27":"code","2c43dd83":"code","600e26a6":"code","5ec1b34d":"code","ab98797d":"code","ce113d1e":"code","f89b6220":"code","6697662a":"code","b615d04f":"code","84e41f8d":"code","df621ba6":"code","5c12ed1b":"code","60233390":"code","44a8ed3f":"code","15dec280":"code","deeab795":"code","b9b884cb":"code","7e04cfc0":"code","262beda2":"code","0ab43e0d":"code","9fc780f4":"code","adec2fe4":"code","2de3356a":"code","05fe4e31":"code","fcdf0148":"code","5b1a88ee":"code","4f55eee6":"code","fc551357":"code","860a429c":"code","fa56c5c4":"code","4875bf8f":"code","7a010937":"code","60bc650e":"code","747d2384":"code","df851c35":"code","78f27e75":"code","3bb9850f":"code","44e02ba1":"code","66a85f1a":"code","c8cd7094":"code","80385a21":"code","a16ce254":"code","f033294d":"code","18c4e617":"code","76979036":"code","e8ec1150":"code","d6609617":"code","8ef3a009":"code","4f32532f":"code","4bd986d5":"code","51ad591c":"code","0a607063":"code","0fa0081c":"code","b0717124":"code","9a67fa3f":"code","1fdad807":"code","1b018cf8":"code","9f58187b":"code","688e86aa":"code","0e5e952d":"code","3469590a":"code","5afaa8ac":"code","8f72acab":"code","27a5bd26":"code","3f15a27f":"code","48239abf":"code","328cbe58":"code","15a86888":"code","47286caa":"code","41f36251":"code","71fb01ce":"code","5e3ca9ce":"code","1bd5a943":"code","a42f913a":"code","8982f9ef":"code","59613be7":"code","9528a1d1":"code","4ea7394c":"code","7131f34b":"code","f85d927e":"code","d620e984":"code","b944547a":"code","c608e7cb":"code","a96a6990":"code","fd96660d":"code","9aa20c81":"code","8ca32e41":"code","376435db":"code","47e85a77":"code","20159735":"code","34f55efc":"code","8b98bf4d":"code","efaadfae":"code","d836c842":"code","ad963231":"code","ae071d2a":"code","732e61bb":"markdown","c0e12263":"markdown","c54488a7":"markdown","389c823b":"markdown","f64fb393":"markdown","f89ee0cf":"markdown","1081fdce":"markdown","f6c12a13":"markdown","5d4dbc61":"markdown","cf0a80cf":"markdown","c5ed32a1":"markdown","73294ed0":"markdown","451f95d4":"markdown","6c1f333f":"markdown","af6ad17d":"markdown","4dc4f8ec":"markdown","edb27fc9":"markdown","0e8c69f0":"markdown","2e98a551":"markdown","c835ca98":"markdown","84a11b7a":"markdown","42012f3d":"markdown","e03805f0":"markdown","2afc7fa2":"markdown","43c70ced":"markdown","b1d897fb":"markdown","c25de388":"markdown","e2e70f8f":"markdown","ff161c61":"markdown","47796116":"markdown","db6cb75a":"markdown","0bcf164a":"markdown","3fdd6dd6":"markdown","c8f5f160":"markdown","69de25ca":"markdown","fe614065":"markdown","1887710e":"markdown","43d58480":"markdown","c38d1b9d":"markdown","98e11102":"markdown","eb4338a1":"markdown","d6126be2":"markdown","525b7d86":"markdown","799da6d1":"markdown","3fafe9a8":"markdown","83838ca9":"markdown","d31640cc":"markdown","a4414576":"markdown","1b97f08e":"markdown","63929d9b":"markdown","1efa6353":"markdown","86566cdc":"markdown","ca98b022":"markdown","e4aa3ab2":"markdown","d93c2c94":"markdown","3c43da13":"markdown","c26c28cf":"markdown","7fc2e923":"markdown","b38cf61f":"markdown","561421e3":"markdown","54d446af":"markdown","9d9fd205":"markdown","bb345ea9":"markdown","cd0e9237":"markdown","d37441d2":"markdown","a5f024f9":"markdown","a0232d5b":"markdown","61e8f49d":"markdown","566a276a":"markdown","d8abe6e2":"markdown","d304796e":"markdown","48c54de9":"markdown","b65dd636":"markdown","40794457":"markdown","d08fd7ff":"markdown","9381eefa":"markdown","03009bc7":"markdown","90b87daa":"markdown","97f4864d":"markdown","e31dc2ca":"markdown","bdf73706":"markdown","46e05b35":"markdown","a713c357":"markdown","62d7152a":"markdown","931d5c28":"markdown","e84e6b12":"markdown","9387721b":"markdown","65563f24":"markdown","e8e23b67":"markdown","f2a33b1e":"markdown","cdb25b9f":"markdown","99a4d2cc":"markdown","3f6703b6":"markdown","e9d4eeaa":"markdown","0c17beea":"markdown","12e62dfc":"markdown","2f2a0090":"markdown","a1838bac":"markdown","0847fc87":"markdown","38ab225e":"markdown","55a50543":"markdown","89143e9d":"markdown","38859ab5":"markdown"},"source":{"a7149af5":"import pandas as pd\nimport numpy as np\nfrom sklearn import metrics\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nfrom statistics import mean,stdev,median\nfrom prettytable import PrettyTable","cce8df27":"def mis_val_treatment(data,string,percentage):\n    \"\"\"Treat the missing values.\n\n    * This function select the features which have a number of missing values less than a given threshold\n    * Numeric features : Replace missing values selected with mean\/median of the feature\n    * categorical features : Replace missing values with a new class named \"missing value\"\n\n    :param data: data to treat\n    :param string: mean\/median\n    :param percentage: percentage to define a threshold for missing values\n    :return: data treated\n    \n    \"\"\"\n    var = data.isnull().sum()\n    threshold = round(len(data.index) \/ percentage)\n    data_mis_val = data[var[var!=0][var<threshold].index].copy()\n    names_data = list(data_mis_val.columns.values)\n    names_data_numeric = list(data_mis_val.describe().columns.values)\n    for name in names_data:\n        s = data_mis_val[name]\n        if name in names_data_numeric:\n            if string == \"mean\":\n                s = s.fillna(s.mean())\n            if string == \"median\":\n                s = s.fillna(s.median())\n        else:\n            s = s.fillna(\"missing_value\")\n        data_mis_val[name] = s\n    return data_mis_val\n\n\ndef data_model_constuct(data,data2):\n    '''\n    Reconstruct data after missing values treatment and one hot encoding\n\n    :param data: data to reconstruct\n    :param data2: result of mis val_treatment\n    :return: data model constructed\n    '''\n    var = data.isnull().sum()\n    data1 = data[var[var==0].index].copy()\n    data_model = pd.concat([data1,data2],axis=1)\n    data_model = pd.get_dummies(data_model)\n    return data_model\n\ndef train_val_size(data,val_size,test_size):\n    '''\n    Split data into train validation and test datasets\n    \n    :param data:  data to split\n    :param val_size: percentage of validation dataset size [0,1]\n    :param test_size: percentage of test dataset size [0,1] \n    :return: dictionary containing the 3 datasets\n    \n    '''\n    y=data['TARGET']\n    data_train, data_valtest, y_train, y_valtest = train_test_split(data, y, test_size=val_size+test_size)\n    data_val, data_test, y_val, y_test = train_test_split(data_valtest, y_valtest, test_size=val_size\/(val_size+test_size))\n    return {'data_train':data_train,'data_val':data_val,'data_test':data_test}\n\n\ndef prepare_data(path_to,string=\"mean\",percentage=10,val_size=0.2,test_size=0.2):\n    data = pd.read_csv(path_to)\n    data_mis_val = mis_val_treatment(data,string,percentage)\n    data_model = data_model_constuct(data, data_mis_val)\n    dict = train_val_size(data_model,val_size,test_size)\n    return dict\n","2c43dd83":"dict_data = prepare_data(path_to=\"..\/input\/home-credit-default-risk\/application_train.csv\")","600e26a6":"data_val = dict_data['data_val']\ny_val = data_val['TARGET']\ndata_val_model = data_val.drop(['TARGET'], axis=1)","5ec1b34d":"model = LogisticRegression()","ab98797d":"def reglog_model_results(model,data_test,y_test):\n    '''\n    Performance model results\n    \n    :param model: model to implement\n    :param data_test: data to test\n    :param y_test: target variable \n    :return: dictionary of performance results\n    '''\n\n    # Calculate Class Probabilities\n    probability = model.predict_proba(data_test)\n\n    # Predicted Class Labels\n    y_predicted = model.predict(data_test)\n\n    # Evaluate The Model\n\n    ### Confusion Matrix\n    Confusion_Matrix = metrics.confusion_matrix(y_test, y_predicted)\n\n    ### Classification Report\n    Classification_Report = metrics.classification_report(y_test, y_predicted)\n\n    ### Model Accuracy\n    Accuracy = model.score(data_test, y_test)\n\n    ### AUC\n    y_pred_proba = probability[:, 1]\n    [fpr, tpr, thr] = metrics.roc_curve(y_test, y_pred_proba)\n    auc = metrics.auc(fpr, tpr)\n\n    return {'Class_Probabilities':probability,'Predicted_Class_Labels':y_predicted,'Confusion_Matrix':Confusion_Matrix,'Classification_Report':Classification_Report,'Accuracy':Accuracy, 'AUC':auc}\n\n\n# Show Confusion Matrix\ndef confusion_matrix(cm):\n    tab = PrettyTable([' ', 'Predicted 0', 'Predicted 1'])\n    tab.add_row([\"Actual 0\", cm[0][0], cm[0][1]])\n    tab.add_row([\"Actual 1\", cm[1][0], cm[1][1]])\n    print(tab)\n\n    \n# Show the ROC_CURVE\ndef roc_curve_show(model,data_test,y_test):\n\n    result_model = reglog_model_results(model, data_test, y_test)\n    y_pred_proba = result_model['Class_Probabilities'][:, 1]\n    [fpr, tpr, thr] = metrics.roc_curve(y_test, y_pred_proba)\n    idx = np.min(np.where(tpr > 0.95))  # index of the first threshold for which the sensibility > 0.95\n    plt.figure()\n    plt.plot(fpr, tpr, color='coral', label='ROC curve (area = %0.3f)' % metrics.auc(fpr, tpr))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.plot([0, fpr[idx]], [tpr[idx], tpr[idx]], 'k--', color='blue')\n    plt.plot([fpr[idx], fpr[idx]], [0, tpr[idx]], 'k--', color='blue')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\n    plt.ylabel('True Positive Rate (recall)', fontsize=14)\n    plt.title('Receiver operating characteristic (ROC) curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n","ce113d1e":"def results_model_dict(model,dict_data):\n    '''\n        apply the model and get results\n        :param model: model to apply\n        :param dict_data: dictionary including model datasets\n        :return: dict of model results\n        '''\n\n    # datasets :\n    data_train = dict_data['data_train']\n    data_val = dict_data['data_val']\n    data_test = dict_data['data_test']\n\n    # Get target variable from each data\n    y_train = data_train['TARGET']\n    y_val = data_val['TARGET']\n    y_test = data_test['TARGET']\n\n    # drop target variable from each dataset\n    data_train_model = data_train.drop(['TARGET'], axis=1)\n    data_val_model = data_val.drop(['TARGET'], axis=1)\n    data_test_model = data_test.drop(['TARGET'], axis=1)\n\n    ##########  fit model   #############\n    model1 = model.fit(data_train_model, y_train)\n\n    ### model results\n    results_model1 = reglog_model_results(model1, data_val_model, y_val)\n    return results_model1\n","f89b6220":"logreg_results = results_model_dict(model,dict_data)","6697662a":"logreg_results['AUC']","b615d04f":"logreg_results['Accuracy']","84e41f8d":"dist = y_val.value_counts()\ndist","df621ba6":"cm = logreg_results['Confusion_Matrix']\nconfusion_matrix(cm)","5c12ed1b":"tp = round(float(cm[0][0])\/float(dist[0])*100,2)\n\"True Postive en % = \"+str(tp)+\" %\"","60233390":"tn = round(float(cm[1][1])\/float(dist[1])*100,2)\n\"True Negative en % = \"+str(tn)+\" %\"","44a8ed3f":"roc_curve_show(model,data_val_model,y_val)","15dec280":"model_balanced = LogisticRegression(class_weight='balanced')","deeab795":"logreg_results_balanced = results_model_dict(model_balanced,dict_data)","b9b884cb":"logreg_results_balanced['AUC']","7e04cfc0":"logreg_results_balanced['Accuracy']","262beda2":"dist = y_val.value_counts()\ndist","0ab43e0d":"cm_balanced = logreg_results_balanced['Confusion_Matrix']\nconfusion_matrix(cm_balanced)","9fc780f4":"tp_balanced = round(float(cm_balanced[0][0])\/float(dist[0])*100,2)\n\"True Postive en % = \"+str(tp_balanced)+\" %\"","adec2fe4":"tn_balanced = round(float(cm_balanced[1][1])\/float(dist[1])*100,2)\n\"True Negative en % = \"+str(tn_balanced)+\" %\"","2de3356a":"roc_curve_show(model_balanced,data_val_model,y_val)","05fe4e31":"def resmpling_data(data,val,string=\"percentage\"):\n    '''\n    Resampling data with given a value of percentage or number for output rows number\n\n    :param data: pandas data_train to resample\n    :param val: value of percentage or rows number\n    :param string: percentage or number\n    :return: dataset resampled\n    '''\n    y = data['TARGET']\n    data_0 = data[y == 0]\n    data_1 = data[y == 1]\n    nrows = data_0.shape[0]\n    if string == \"percentage\":\n        sample = int(round(val*nrows\/100))\n        boot = resample(data_0, replace=False, n_samples=sample)\n    else:\n        boot = resample(data_0, replace=False, n_samples=val)\n\n    data_boot = boot.append(data_1)\n    return data_boot\n","fcdf0148":"data = dict_data['data_train']\ny = data['TARGET']\npercentage_to_sample = round(float(y.value_counts()[1])\/float(y.value_counts()[0])*100)\n\"Percentage_to_sample = \"+str(percentage_to_sample)+\" %\"","5b1a88ee":"data_resampled = resmpling_data(data,9)","4f55eee6":"model = LogisticRegression()","fc551357":"data_train = dict_data['data_train']\n\n#resample data\ndata_resampled = resmpling_data(data_train,9,string=\"percentage\")\n\n\n# datasets :\ndata_val =  dict_data['data_val']\ndata_test = dict_data['data_test']\n\n# Get target variable from each data\ny_train = data_resampled['TARGET']\ny_val = data_val['TARGET']\ny_test = data_test['TARGET']\n\n# drop target variable from each dataset\ndata_train_model = data_resampled.drop(['TARGET'], axis=1)\ndata_val_model = data_val.drop(['TARGET'], axis=1)\ndata_test_model = data_test.drop(['TARGET'], axis=1)\n","860a429c":"model_subsample = model.fit(data_train_model, y_train)\nlogreg_results_resampled = reglog_model_results(model_subsample, data_val_model, y_val)\n","fa56c5c4":"logreg_results_resampled['AUC']","4875bf8f":"logreg_results_resampled['Accuracy']","7a010937":"dist = y_val.value_counts()\ndist","60bc650e":"cm_resampled = logreg_results_resampled['Confusion_Matrix']\nconfusion_matrix(cm_resampled)","747d2384":"tp_resampled = round(float(cm_resampled[0][0])\/float(dist[0])*100,2)\n\"True Postive en % = \"+str(tp_resampled)+\" %\"","df851c35":"tn_resampled = round(float(cm_resampled[1][1])\/float(dist[1])*100,2)\n\"True Negative en % = \"+str(tn_resampled)+\" %\"","78f27e75":"tab = PrettyTable(['Model', 'Accuracy','AUC','Sensitivity : TP %','Specificity : TN %'])\ntab.add_row([\"model_unbalanced\",round(logreg_results['Accuracy'],5),round(logreg_results['AUC'],5),tp,tn])\ntab.add_row([\"model_balanced\",round(logreg_results_balanced['Accuracy'],5),round(logreg_results_balanced['AUC'],5),tp_balanced,tn_balanced])\ntab.add_row([\"model_subsample\",round(logreg_results_resampled['Accuracy'],5),round(logreg_results_resampled['AUC'],5),tp_resampled,tn_resampled])\nprint(tab)","3bb9850f":"data = pd.read_csv(\"..\/input\/home-credit-default-risk\/application_train.csv\")\ndata_mis_val = mis_val_treatment(data, string=\"mean\",percentage=10)\ndata_model = data_model_constuct(data, data_mis_val)","44e02ba1":"def results_model_data(model,data_model,val_size=0.2,test_size=0.2):\n    '''\n    Split data-model into train, val and test datasets and then apply the model and finally get results\n    :param model: model to apply\n    :param data: data_model\n    :param val_size:\n    :param test_size:\n    :return:  model results and random data split result\n    '''\n\n    dict_data = train_val_size(data_model, val_size, test_size)\n    # datasets :\n    data_train = dict_data['data_train']\n    data_val = dict_data['data_val']\n    data_test = dict_data['data_test']\n\n    # Get target variable from each data\n    y_train = data_train['TARGET']\n    y_val = data_val['TARGET']\n    y_test = data_test['TARGET']\n\n    # drop target variable from each dataset\n    data_train_model = data_train.drop(['TARGET'], axis=1)\n    data_val_model = data_val.drop(['TARGET'], axis=1)\n    data_test_model = data_test.drop(['TARGET'], axis=1)\n\n    ##########  fit model   #############\n    model1 = model.fit(data_train_model, y_train)\n\n    ### model results\n    results_model1 = reglog_model_results(model1, data_val_model, y_val)\n    results_model1.update(dict_data)\n    return results_model1\n","66a85f1a":"auc_results = list()\naccuracy_results = list()","c8cd7094":"for i in range(0,10):\n    auc = results_model_data(model_balanced,data_model)['AUC']\n    accuracy = results_model_data(model_balanced,data_model)['Accuracy'] \n    auc_results.append(auc)\n    accuracy_results.append(accuracy)\n","80385a21":"t = PrettyTable(['Model', 'accuracy','AUC'])","a16ce254":"for i in range(0,10):\n    m = \"model\"+\" \"+str(i+1)\n    t.add_row([m,round(accuracy_results[i],5),round(auc_results[i],5)])\n\nt.add_row([\"Mean\",mean(accuracy_results),mean(auc_results)])\nt.add_row([\"Standard deviation\",stdev(accuracy_results),stdev(auc_results)])\nt.add_row([\"Median\",median(accuracy_results),median(auc_results)])\nprint(t)","f033294d":"logreg_results_balanced_test = reglog_model_results(model_balanced, data_test_model, y_test)","18c4e617":"logreg_results_balanced_test['AUC']","76979036":"logreg_results_balanced_test['Accuracy']","e8ec1150":"dist = y_test.value_counts()\ndist","d6609617":"cm_balanced_test = logreg_results_balanced_test['Confusion_Matrix']\nconfusion_matrix(cm_balanced_test)","8ef3a009":"tp_balanced_test = round(float(cm_balanced_test[0][0])\/float(dist[0])*100,2)\n\"True Postive en % = \"+str(tp_balanced_test)+\" %\"","4f32532f":"tn_balanced_test = round(float(cm_balanced_test[1][1])\/float(dist[1])*100,2)\n\"True Negative en % = \"+str(tn_balanced_test)+\" %\"","4bd986d5":"tab = PrettyTable(['Dataset', 'Accuracy','AUC','Sensitivity : TP %','Specificity : TN %'])\ntab.add_row([\"Validation data\",round(logreg_results_balanced['Accuracy'],5),round(logreg_results_balanced['AUC'],5),tp_balanced,tn_balanced])\ntab.add_row([\"Test data\",round(logreg_results_balanced_test['Accuracy'],5),round(logreg_results_balanced_test['AUC'],5),tp_balanced_test,tn_balanced_test])\nprint(tab)","51ad591c":"dict_data = prepare_data(path_to=\"..\/input\/home-credit-default-risk\/application_train.csv\")","0a607063":"data_val = dict_data['data_val']\ny_val = data_val['TARGET']\ndata_val_model = data_val.drop(['TARGET'], axis=1)","0fa0081c":"model = GradientBoostingClassifier()","b0717124":"gbm_results = results_model_dict(model,dict_data)","9a67fa3f":"gbm_results['AUC']","1fdad807":"gbm_results['Accuracy']","1b018cf8":"dist = y_val.value_counts()\ndist","9f58187b":"cm = gbm_results['Confusion_Matrix']\nconfusion_matrix(cm)","688e86aa":"\"True Postive en % = \"+str(round(float(cm[0][0])\/float(dist[0])*100,2))+\" %\"","0e5e952d":"\"True Negative en % = \"+str(round(float(cm[1][1])\/float(dist[1])*100,2))+\" %\"","3469590a":"# Import datasets\ndata_train = dict_data['data_train']","5afaa8ac":"\n#resample data\ndata_resampled = resmpling_data(data_train,9,string=\"percentage\")\n\n# datasets :\ndata_val =  dict_data['data_val']\ndata_test = dict_data['data_test']\n\n# Get target variable from each data\ny_train = data_resampled['TARGET']\ny_val = data_val['TARGET']\ny_test = data_test['TARGET']\n\n# drop target variable from each dataset\ndata_train_model = data_resampled.drop(['TARGET'], axis=1)\ndata_val_model = data_val.drop(['TARGET'], axis=1)\ndata_test_model = data_test.drop(['TARGET'], axis=1)","8f72acab":"# built model\nmodel_gbm_resample = model.fit(data_train_model, y_train)\n\n### model results\nresults_model_gbm_resample = reglog_model_results(model_gbm_resample, data_val_model, y_val)\n","27a5bd26":"results_model_gbm_resample['AUC']","3f15a27f":"results_model_gbm_resample['Accuracy']","48239abf":"dist = y_val.value_counts()\ndist","328cbe58":"cm_gbm_resample = results_model_gbm_resample['Confusion_Matrix']\nconfusion_matrix(cm_gbm_resample)","15a86888":"\"True Postive en % = \"+str(round(float(cm_gbm_resample[0][0])\/float(dist[0])*100,2))+\" %\"","47286caa":"\"True Negative en % = \"+str(round(float(cm_gbm_resample[1][1])\/float(dist[1])*100,2))+\" %\"","41f36251":"# Resampling with stratification\ndata_input = dict_data['data_train']\ny = data_input['TARGET']\ndata_input_0 = data_input[y == 0]\n# kmeans\nkmeans = KMeans(n_clusters=6).fit(data_input_0)\nd = pd.Series(kmeans.labels_)\nprint(\"Clustering with Kmeans : \\n\")\nprint(d.value_counts())\ndict = {}\nfor i in range(0, 6):\n    rows = list(d[d == i].index)\n    data = data_input_0.iloc[rows, :]\n    number_to_sample = data.shape[0] * 9 \/ 100\n    number_to_sample = round(number_to_sample)\n    data_sample = resample(data, replace=False, n_samples=number_to_sample)\n    key = str(i)\n    dict.update({key: data_sample})\n\ndata_output = pd.concat([dict['0'], dict['1'], dict['2'], dict['3'], dict['4'], dict['5']])\n\n# reconstruct data\ndata_input_1 = data_input[y == 1]\ndata_train_output = data_output.append(data_input_1)\ny_train = data_train_output['TARGET']\ndata_train_model = data_train_output.drop(['TARGET'], axis=1)\n\n# built model\nmodel_gbm_kmeans = model.fit(data_train_model, y_train)\n\n### model results\nresults_model_gbm_kmeans = reglog_model_results(model_gbm_kmeans, data_val_model, y_val)\n","71fb01ce":"results_model_gbm_kmeans['AUC']","5e3ca9ce":"results_model_gbm_kmeans['Accuracy']","1bd5a943":"dist = y_val.value_counts()\ndist","a42f913a":"cm_gbm_kmeans = results_model_gbm_kmeans['Confusion_Matrix']\nconfusion_matrix(cm_gbm_kmeans)","8982f9ef":"\"True Postive en % = \"+str(round(float(cm_gbm_kmeans[0][0])\/float(dist[0])*100,2))+\" %\"","59613be7":"\"True negative en % = \"+str(round(float(cm_gbm_kmeans[1][1])\/float(dist[1])*100,2))+\" %\"","9528a1d1":"random_search_output = pd.read_csv(\"..\/input\/output-random-search\/output_random_search.csv\")","4ea7394c":"output_ranked = random_search_output.sort_values(\"rank_test_score\")\noutput_ranked","7131f34b":"params = list(output_ranked[\"params\"])\nparams[0]","f85d927e":"model_search_output = GradientBoostingClassifier(n_estimators=500, max_depth=10,learning_rate=0.01)","d620e984":"# Import datasets\ndata_train = dict_data['data_train']\n\n# resample data\ndata_resampled = resmpling_data(data_train,9,string=\"percentage\")\n\n# target\ny_train = data_resampled['TARGET']\n\n# drop target variable from each dataset\ndata_train_model = data_resampled.drop(['TARGET'], axis=1)\n\n# built model\nmodel_search_output_resample = model_search_output.fit(data_train_model, y_train)\n\n### model results\nresults_model_search_resample = reglog_model_results(model_search_output_resample, data_val_model, y_val)\n","b944547a":"results_model_search_resample['AUC']","c608e7cb":"results_model_search_resample['Accuracy']","a96a6990":"dist = y_val.value_counts()\ndist","fd96660d":"cm_gbm_search = results_model_search_resample['Confusion_Matrix']\nconfusion_matrix(cm_gbm_search)","9aa20c81":"tp_gbm_search = round(float(cm_gbm_search[0][0])\/float(dist[0])*100,2)\n\"True Postive en % = \"+str(tp_gbm_search)+\" %\"","8ca32e41":"tn_gbm_search = round(float(cm_gbm_search[1][1])\/float(dist[1])*100,2)\n\"True negative en % = \"+str(tn_gbm_search)+\" %\"","376435db":"results_model_search_resample_test = reglog_model_results(model_search_output_resample, data_test_model, y_test)","47e85a77":"results_model_search_resample_test['AUC']","20159735":"results_model_search_resample_test['Accuracy']","34f55efc":"dist = y_val.value_counts()\ndist","8b98bf4d":"cm_gbm_search_test = results_model_search_resample_test['Confusion_Matrix']\nconfusion_matrix(cm_gbm_search_test)","efaadfae":"tp_gbm_search_test = round(float(cm_gbm_search_test[0][0])\/float(dist[0])*100,2)\n\"True Postive en % = \"+str(tp_gbm_search_test)+\" %\"","d836c842":"tn_gbm_search_test= round(float(cm_gbm_search_test[1][1])\/float(dist[1])*100,2)\n\"True negative en % = \"+str(tn_gbm_search_test)+\" %\"","ad963231":"tab = PrettyTable(['Dataset', 'Accuracy','AUC','Sensitivity : TP %','Specificity : TN %'])\ntab.add_row([\"Validation data\",round(results_model_search_resample_test['Accuracy'],5),round(results_model_search_resample_test['AUC'],5),tp_gbm_search,tn_gbm_search])\ntab.add_row([\"Test data\",round(results_model_search_resample_test['Accuracy'],5),round(results_model_search_resample_test['AUC'],5),tp_gbm_search_test,tn_gbm_search_test])\nprint(tab)","ae071d2a":"%%javascript\n$.getScript('https:\/\/kmahelona.github.io\/ipython_notebook_goodies\/ipython_notebook_toc.js')","732e61bb":"### First simple model : Unbalanced classes","c0e12263":"* <strong> TARGET distribution : <\/strong>","c54488a7":"### Second model : Balanced classes by Undersampling data","389c823b":"* <strong> Confusion Matrix <\/strong>","f64fb393":"   <font size=3 color=green  ><strong>  Confusion Matrix <\/strong> <\/font> ","f89ee0cf":"<strong> Function to implement model <\/strong>","1081fdce":"   <font size=3 color=green  ><strong>  Accuracy <\/strong> <\/font> ","f6c12a13":"* <strong>  Confusion Matrix <\/strong>","5d4dbc61":"#### Random search output data ","cf0a80cf":"#### Model results :","c5ed32a1":"   <font size=3 color=green  ><strong>  AUC <\/strong> <\/font> ","73294ed0":"<h1 id=\"tocheading\" >Table of Contents<\/h1>\n<div id=\"toc\"><\/div>","451f95d4":"<strong> List of auc result models <\/strong>","6c1f333f":"We built a random search algorithm with specific values of hyperparameters and we the following data of combinations :","af6ad17d":"* <strong>  Confusion Matrix <\/strong>","4dc4f8ec":"## Gradient Boosting Machine","edb27fc9":"#### Model results :","0e8c69f0":"   <font size=3 color=green  ><strong>  Confusion Matrix <\/strong> <\/font> ","2e98a551":"####  Validation model vs test model","c835ca98":"* <strong> Confusion Matrix <\/strong>","84a11b7a":"* <strong> TARGET distribution : <\/strong>","42012f3d":"Then, we check the <strong> standard deviation of 10 models <\/strong> implemented with Logistic Regression.","e03805f0":"##  Import Librarires","2afc7fa2":"   <font size=3 color=green  ><strong>  AUC <\/strong> <\/font> ","43c70ced":"* <strong> TARGET distribution <\/strong>","b1d897fb":"<strong> Fonction to get results from a radomly data split <\/strong>","c25de388":"* <strong> TARGET distribution : <\/strong>","e2e70f8f":"In order to improve the model, we try to implement it with <strong> balanced classes <\/strong>","ff161c61":"   <font size=3 color=green  ><strong>  ROC CURVE <\/strong> <\/font> ","47796116":"Validation data","db6cb75a":"So, we tried the model with the new hyperparameters values :","0bcf164a":"#### Model results","3fdd6dd6":"So, we can conclude that the model with balanced classes is the most appropriate one for the Logistic Regression algorithm.","c8f5f160":"* <strong> Confusion Matrix : <\/strong>","69de25ca":"### Validate the model and try results on data test ","fe614065":"Implement the GBM model with subsampling data :","1887710e":"   <font size=3 color=green  ><strong>  Accuracy <\/strong> <\/font> ","43d58480":"#### Model results :","c38d1b9d":"   <font size=3 color=green  ><strong>  ROC CURVE <\/strong> <\/font> ","98e11102":"We <strong> sort the data by rank test score <\/strong> to get the best combination of hyperparameters.","eb4338a1":"* <strong> TARGET distribution <\/strong>","d6126be2":"   <font size=3 color=green  ><strong>  Accuracy <\/strong> <\/font> ","525b7d86":"#### Model results :","799da6d1":"*  <strong>  Confusion Matrix : <\/strong>","3fafe9a8":"* <strong> TARGET ditribution : <\/strong>","83838ca9":"Running the <strong> Logistic Regression algorithm with balanced classes <\/strong> on the test data gives approximately the same results (i.e. Accuracy, AUC...) as with the test data. So, we can conclude the implemented algorithm supports many datasets.","d31640cc":"#### Model results :","a4414576":"Implement the model with stratified subsampling data using Kmeans algorithm :","1b97f08e":"### Prepare Data","63929d9b":"####  Validation model vs test model","1efa6353":"The accuracy is very high and the AUC > 0.5 but the model is bad because <strong> the class \"1\" is unpredicted. <\/strong> . So the model is unable to detect risk of unrepaiment loan.","86566cdc":"### Third Model : Balanced classes by undersampling data","ca98b022":"   <font size=3 color=green  ><strong>  Confusion Matrix <\/strong> <\/font> ","e4aa3ab2":"   <font size=3 color=green  ><strong>  Confusion Matrix <\/strong> <\/font> ","d93c2c94":"* <strong> Confusion Matrix : <\/strong>","3c43da13":"* <strong> TARGET distribution <\/strong>","c26c28cf":"* <strong> Confusion Matrix <\/strong>","7fc2e923":"First of all, we implement the standart logistic regression model : <strong> unbalanced classes <\/strong>","b38cf61f":"   <font size=3 color=green  ><strong>  Confusion Matrix  <\/strong> <\/font> ","561421e3":"### Test random effect on selected model ","54d446af":"### Models comparison table : ","9d9fd205":"The <strong> AUC increased <\/strong> compared to the first model but the <strong> accuracy highly decreased <\/strong>. This model can predict the class \"1\" with a percentage of 63.13 %. We can say that this model is better than the previous one.","bb345ea9":"Validation data","cd0e9237":"   <font size=3 color=green  ><strong>  AUC <\/strong> <\/font> ","d37441d2":"* <strong> TARGET distribution : <\/strong>","a5f024f9":"# Results presentation :","a0232d5b":"   <font size=3 color=green  ><strong>  Accuracy <\/strong> <\/font> ","61e8f49d":"   <font size=3 color=green  ><strong>  AUC <\/strong> <\/font> ","566a276a":"   <font size=3 color=green  ><strong>  AUC <\/strong> <\/font> ","d8abe6e2":"### Fourth model : model selected + Hyperparmeters optimization using Random Search algorithm","d304796e":"<strong> Function to resample data <\/strong>","48c54de9":"We prepared 3 datasets : Train, Validation and Test datasets and we put them into a dictionary.","b65dd636":" <strong> Functions to show performance incators of the models. <\/strong>","40794457":"   <font size=3 color=green  ><strong>  Accuracy <\/strong> <\/font> ","d08fd7ff":"   <font size=3 color=green  ><strong>  Confsion Matrix <\/strong> <\/font> ","9381eefa":"#### Prepare dataset","03009bc7":"   <font size=3 color=green  ><strong>  AUC <\/strong> <\/font> ","90b87daa":"We tried the Logistic Regression Model with <strong> subsampling data <\/strong>.<br\/>\nWe get a sample from \"0\" class with the same number of elements for the class \"1\".","97f4864d":"We choose finally the GBM model with resampling data, and we will look for best hyperparameters' combination with a random search algorithm to implement the model and to get the best results. ","e31dc2ca":"   <font size=3 color=green  ><strong>  Confusion Matrix <\/strong> <\/font> ","bdf73706":"### First simple model : Unbalanced classes","46e05b35":"   <font size=3 color=green  ><strong> AUC <\/strong> <\/font>","a713c357":"### Third model : Balanced classes by stratified undersampling data ( Using Kmeans algorithm )","62d7152a":"   *  <strong> TARGET distribution : <\/strong>","931d5c28":"#### Model results :","e84e6b12":"   <font size=3 color=green  ><strong> Confusion Matrix <\/strong> <\/font> ","9387721b":"   <font size=3 color=green  ><strong>  Accuracy <\/strong> <\/font> ","65563f24":"#### Random effect results table :","e8e23b67":"   <font size=3 color=green  ><strong>  AUC <\/strong> <\/font> ","f2a33b1e":"#### Model results :","cdb25b9f":"<strong> Percentage to sample : <\/strong>","99a4d2cc":"   <font size=3 color=green  ><strong>  Accuracy <\/strong> <\/font> ","3f6703b6":"   <font size=3 color=green  ><strong>  Accuracy <\/strong> <\/font> ","e9d4eeaa":"### Second model :  Balanced classes by modifying weight of each class","0c17beea":"#### Model results","12e62dfc":"### Preparing data :","2f2a0090":"### Validate the model and try results on data test ","a1838bac":"## Logistic Regression","0847fc87":"   <font size=3 color=green  ><strong>  Confusion Matrix <\/strong> <\/font> ","38ab225e":"   <font size=3 color=green  ><strong>  AUC <\/strong> <\/font> ","55a50543":"* <strong> Confusion Matrix <strong>","89143e9d":"The best combination of hyperparameters returned by Random Search","38859ab5":"   <font size=3 color=green  ><strong> Accuracy <\/strong> <\/font>"}}