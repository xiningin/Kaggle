{"cell_type":{"87b93c5a":"code","b8aa7855":"code","cfbc212b":"code","3dc01f9f":"code","7a7c6c5c":"code","e1a30a97":"code","b7508f3a":"code","487908a9":"code","b5404669":"code","cea0f08d":"code","3b9919fb":"code","bbe80112":"code","1410749e":"code","afb1faa0":"code","de25dcd3":"code","b00ab947":"code","55254efc":"code","9f7add5e":"code","76886c90":"code","c6b30def":"code","c8ea023e":"code","d8183051":"code","9983661e":"code","904cdda2":"code","e8eed577":"code","b54f9cae":"code","5c5c434a":"code","d67f773e":"code","01d99c82":"code","75d69bba":"code","291a405b":"code","0607e5bd":"code","6fdebb64":"code","df4aebf3":"code","f7877490":"code","d8787544":"code","2ed25715":"code","f4914c02":"code","e7656592":"code","9748fa98":"code","ca514d7f":"code","b4dea848":"code","f5d01cc4":"code","ada84e1b":"code","05fd7ce6":"code","4b6d0aeb":"code","591508cb":"code","1797e9d9":"code","1883fd5a":"code","b38d4e33":"code","462bac62":"code","01b30163":"code","278ec181":"code","c0b180ea":"code","0d00aeee":"code","ac5eef3b":"code","a53e4ca6":"code","40fbfcd5":"code","12c85f31":"code","df27d0ee":"code","a855aca1":"code","315c1120":"code","5b00113f":"code","81bf5130":"code","d8a60b90":"code","80cee58d":"code","44ec606c":"code","67229c22":"code","0a2664c3":"code","34cf9924":"code","3c6de079":"code","80c97e91":"code","dc1394f9":"code","031aa0f2":"code","3d5c7af3":"code","5bce402d":"code","76885a99":"code","b0ad1fca":"code","6d9d9aa2":"markdown","f97490eb":"markdown","106c474b":"markdown","7e735514":"markdown","ab00b570":"markdown","d1154adb":"markdown","b240be37":"markdown","506b6ef3":"markdown","0ce8fd11":"markdown","264ffdb9":"markdown","6288c5dd":"markdown","7be154e8":"markdown","ec9c3f97":"markdown","a89e4388":"markdown","c965bbdf":"markdown","e0f029a4":"markdown","c314c9be":"markdown","db35a006":"markdown","3c237f68":"markdown","784c321b":"markdown","e1f57874":"markdown","a5a2bc3d":"markdown","c214aea7":"markdown"},"source":{"87b93c5a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score,f1_score,confusion_matrix,roc_auc_score,roc_curve\nsns.set()\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b8aa7855":"data = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')","cfbc212b":"data.head()","3dc01f9f":"data['Class'].value_counts()","7a7c6c5c":"plt.figure(figsize=(20,2))\nsns.countplot(y=data['Class'])\nplt.savefig('countofdata.png')\nplt.show()","e1a30a97":"print(f'Percentage of data where class = 1 is : {(len(data[data.Class == 1])\/ len(data[data.Class == 0]))*100}')","b7508f3a":"#Create train and test dataset\nfrom sklearn.model_selection import train_test_split\nX = data.drop('Class',axis=1)\ny = data['Class']","487908a9":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)","b5404669":"print(f'Shape of X_train :{X_train.shape}')\nprint(f'Shape of X_train :{X_test.shape}')\nprint(f'Shape of X_train :{y_train.shape}')\nprint(f'Shape of X_train :{y_test.shape}')","cea0f08d":"# DummyClassifier to predict only target 0\nfrom sklearn.dummy import DummyClassifier","3b9919fb":"dummy = DummyClassifier(strategy='most_frequent')\ndummy.fit(X_train,y_train)","bbe80112":"dummy_pred = dummy.predict(X_test)\n#Check for the unique labels\nprint(f'Unique predicted labels : {np.unique(dummy_pred)}')","1410749e":"#Check the accuracy for unique predicated labels\nprint(f'Accuracy for the test dataset : {accuracy_score(dummy_pred,y_test)}')","afb1faa0":"#Now let's use the Logistic regression model to check the accuracy on inbalanced data set\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()","de25dcd3":"#Predict the value by Logistic Regression\nlr.fit(X_train,y_train)","b00ab947":"#Predict on test dataset\nlr_pred = lr.predict(X_test)","55254efc":"#check the accuracy\naccuracy_score(lr_pred,y_test)","9f7add5e":"# Checking unique values\npredictions = pd.DataFrame(lr_pred)\npredictions[0].value_counts()","76886c90":"from sklearn.metrics import confusion_matrix,f1_score,classification_report,recall_score","c6b30def":"f1_score(y_test,lr_pred)","c8ea023e":"pd.DataFrame(confusion_matrix(y_test,lr_pred))","d8183051":"sns.heatmap(confusion_matrix(y_test,lr_pred),cmap='viridis',annot=True,fmt='.2f')\nplt.savefig('lr.png')\nplt.show()","9983661e":"recall_score(y_test,lr_pred)","904cdda2":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC","e8eed577":"random_forest = RandomForestClassifier()\nsvc = SVC()","b54f9cae":"random_forest.fit(X_train,y_train)","5c5c434a":"random_forest_pred = random_forest.predict(X_test)","d67f773e":"accuracy_score(random_forest_pred,y_test)","01d99c82":"f1_score(random_forest_pred,y_test)","75d69bba":"pd.DataFrame(confusion_matrix(random_forest_pred,y_test))","291a405b":"sns.heatmap(confusion_matrix(y_test,random_forest_pred),cmap='viridis',annot=True,fmt='.2f')\nplt.savefig('random.png')\nplt.show()","0607e5bd":"recall_score(random_forest_pred,y_test)","6fdebb64":"print(classification_report(random_forest_pred,y_test))","df4aebf3":"from tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.utils import plot_model","f7877490":"model = Sequential([\n    Dense(units=16,input_dim = 30,activation='relu'),\n    Dense(units=24,activation='relu'),\n    Dropout(0.5),\n    Dense(units=20,activation='relu'),\n    Dense(units=24,activation='relu'),\n    Dense(1,activation='sigmoid')\n])","d8787544":"plot_model(model)","2ed25715":"model.summary()","f4914c02":"##Training of model\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\nmodel.fit(X_train,y_train,batch_size=15,epochs=5)","e7656592":"score = model.evaluate(X_test,y_test)","9748fa98":"y_pred = model.predict(X_test)\ny_test_copy = pd.DataFrame(y_test)","ca514d7f":"y_pred = y_pred.astype(int)","b4dea848":"pd.DataFrame(confusion_matrix(y_test,y_pred))","f5d01cc4":"sns.heatmap(confusion_matrix(y_test,y_pred),annot=True,cmap='YlOrBr',fmt='.2f')\nplt.savefig('deep.png')\nplt.show()","ada84e1b":"from sklearn.utils import resample\n\n#Seprate the input feature and target \nX = data.drop('Class',axis=1)\ny= data['Class']","05fd7ce6":"\n#As told split the data into train and test dataset\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42,test_size=0.25)\nprint(f'Shape of X_train :{X_train.shape}')\nprint(f'Shape of X_train :{X_test.shape}')\nprint(f'Shape of X_train :{y_train.shape}')\nprint(f'Shape of X_train :{y_test.shape}')","4b6d0aeb":"# concatenate our training data back together\nX = pd.concat([X_train, y_train], axis=1)\nX.head()","591508cb":"#Seperate minority and majority class:\nnot_fraud = X[X['Class'] == 0]\nfraud = X[X['Class'] == 1]\nprint(f'Total sample which are not fraud : {len(not_fraud)}')\nprint(f'Total Fraud samples : {len(fraud)}')","1797e9d9":"#Now use the oversampling techniques\nrandom_sampling = resample(fraud,\n                          replace=True,\n                           n_samples = len(not_fraud),\n                           random_state = 42\n                          )\n\n#combine minority and upsample data\nupsample = pd.concat([not_fraud,random_sampling])\n\n#Check new values are balances for the both classes or not\nupsample['Class'].value_counts()","1883fd5a":"X_train = upsample.drop('Class',axis=1)\ny_train = upsample['Class']\n\nlr_model = LogisticRegression()\nlr_model.fit(X_train,y_train)","b38d4e33":"upsampled_pred = lr_model.predict(X_test)","462bac62":"#check for the accuracy_score\naccuracy_score(y_test,upsampled_pred)","01b30163":"#F1 score is\nf1_score(y_test, upsampled_pred)","278ec181":"sns.heatmap(confusion_matrix(y_test,upsampled_pred),annot=True,fmt='.2f',cmap='YlOrBr')\nplt.savefig('ligit_after_oversample.png')\nplt.show()","c0b180ea":"print(classification_report(y_test,upsampled_pred))","0d00aeee":"# downsample majority\nnot_fraud_downsampled = resample(not_fraud,\n                                replace = False, # sample without replacement\n                                n_samples = len(fraud), # match minority n\n                                random_state = 27) # reproducible results\n\n# combine minority and downsampled majority\ndownsampled = pd.concat([not_fraud_downsampled, fraud])\n\n# checking counts\ndownsampled.Class.value_counts()","ac5eef3b":"# trying logistic regression again with the undersampled dataset\n\ny_train = downsampled.Class\nX_train = downsampled.drop('Class', axis=1)\n\nundersampled = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n\nundersampled_pred = undersampled.predict(X_test)","a53e4ca6":"accuracy_score(y_test, undersampled_pred)\n","40fbfcd5":"# f1 score\nf1_score(y_test, undersampled_pred)","12c85f31":"acc","df27d0ee":"print(classification_report(y_test,undersampled_pred))","a855aca1":"#Build the model for the Random Forest\nrandom_forest_undersampled = RandomForestClassifier()\nrandom_forest_undersampled.fit(X_train,y_train)","315c1120":"#Predict the value by using random forest\nrandom_forest_undersampled_pred = undersampled.predict(X_test)","5b00113f":"accuracy_score(y_test,random_forest_undersampled_pred)","81bf5130":"f1_score(y_test,random_forest_undersampled_pred)","d8a60b90":"sns.heatmap(confusion_matrix(y_test,random_forest_undersampled_pred),annot=True,fmt='.2f',cmap='YlGnBu')\nplt.savefig('rand_after_oversample.png')\nplt.show()","80cee58d":"print(classification_report(y_test,random_forest_undersampled_pred))","44ec606c":"#import the libaray for SMOTE\nfrom imblearn.over_sampling import SMOTE\n\n# Separate input features and target\ny = data[\"Class\"]\nX = data.drop('Class', axis=1)\n\n# setting up testing and training sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nsm = SMOTE(random_state=27,sampling_strategy=1.0)\nX_train, y_train = sm.fit_sample(X_train, y_train)","67229c22":"smote_logistic = LogisticRegression()\nsmote_logistic.fit(X_train,y_train)","0a2664c3":"smote_pred = smote_logistic.predict(X_test)","34cf9924":"# Checking accuracy\naccuracy_score(y_test, smote_pred)","3c6de079":"# f1 score\nf1_score(y_test, smote_pred)","80c97e91":"sns.heatmap(confusion_matrix(y_test,smote_pred),annot=True,fmt='.2f',cmap='YlGnBu')\nplt.savefig('rand_after_oversample.png')\nplt.show()","dc1394f9":"smote_random_forest = RandomForestClassifier()\nsmote_random_forest.fit(X_train,y_train)","031aa0f2":"smote_rand_pred = smote_random_forest.predict(X_test)","3d5c7af3":"sns.heatmap(confusion_matrix(y_test,smote_rand_pred),annot=True,fmt='.2f',cmap='YlGnBu')\nplt.savefig('rand_after_smote.png')\nplt.show()","5bce402d":"accuracy_score(y_test,smote_rand_pred)","76885a99":"#F1_score is\nf1_score(y_test,smote_rand_pred)","b0ad1fca":"print(classification_report(y_test,smote_rand_pred))","6d9d9aa2":"As we can see the Type-II error is to much high for the fradulent data.As the model is biased towards the majority class.and to reduce the error model simply ignoring the fradulent trasactions","f97490eb":"Now Try the logistic Regression on this data","106c474b":"## 1. **Resampling Technique:** \nIn the Resampling technique, our main goal is to either increase the frequency of minority class or decrease the frequency of the majority class. This is done in order to obtain the same number of instances of both classes.\n    Following are the few Resampling techniques:\n### 1. **Random Over-sampling\u00a0:**\nRandom Over-sampling can be defined as adding More copies of the minority class. but the oversampling choice is good when you don't have a ton of data to work.\n    We will use the resampling module from scikit-learn to randomly replicate the samples from minority class.To perform oversampling it's important that we should have split the data into train and test dataset.\n    Now the question will come why should we have to split before the oversampling. The answer is easy, oversampling allows us the exact same observations to be present into both the train and test dataset.\n","7e735514":"## **Check by using Different Metrices**\n* Accuracy metric is not best metric to use when evaluating imbalanced class as it can be mislead for the classification.\n* Following are some metrics give us the good insights on the imbalanced dataset\n\n    ### 1. **Confusion Metrics : ** \n    confusion metrics show the clearly classification of the predicted class vs actual class.We can also see how many data point wrongly classified.\n    ### 2. **Precision : **\n    We can get the precision by number of all positive classified value divided by all positive predicted value.It's measure the classifier's exactness.Low presicion indicates the high number of false positive.\n    ### 3. **Recall : **\n    Recall is a metric that quantifies the number of correct positive predictions made out of all positive predictions that could have been made.\n    Unlike precision that only comments on the correct positive predictions out of all positive predictions, recall provides an indication of missed positive predictions.\n    ### 4. **F1-score : **\n    The F1-measure, which weights precision and recall equally, is the variant most often used when learning from imbalanced data.\n    ### 5. **Classification Report : **\n    All above metioned things are auto-generated in the classication report","ab00b570":"As we can the Logistic regression outperformed on the Dummy Classifier.Becuase here our model identied that the 90 trascations are from class 1 i.e. fraudulent.\nLet's see by using SVM and Random Forest Can we increase the Accuracy","d1154adb":"As we already know that the data is highly unbalanced let's check the class column","b240be37":"As we Can see by using random forest we are getting good score and recall is also good enough.Let's print the classification report for the above","506b6ef3":"# Approaches to Handling Imbalanced Dataset:","0ce8fd11":"The datasets contains transactions made by credit cards in September 2013 by european cardholders.\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.","264ffdb9":"## Random Forest","6288c5dd":"## Logistic Regression ","7be154e8":"# **SMOTE :**\nA technique is similar to upsampling is to create synthetic samples. Here we use SMOTE (Synthetic Minority Oversampling Technique) technique.\nSMOTE (Synthetic Minority Oversampling Technique) works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space, and drawing a new sample at a point along that line.\nBy above we can easily understand the SMOTE working is similar to the nearest neighbor algorithm. Yeah you are right!! SMOTE uses the nearest neighbor algorithm to generate new and synthetic data we can use to train our model.","ec9c3f97":"Please anyone to balance an imbalanced dataset. According to your data check which one working good with the advantages and disadvantages of every technique.","a89e4388":"We got an accuracy score of 99.8% \u2014\nAnd without even training a model! Let\u2019s compare this to logistic regression, an actual trained classifier.","c965bbdf":"## **Baseline models**","e0f029a4":"# **Generate Synthetic Samples:**","c314c9be":"Let's Try with Random Forest Also,","db35a006":"# **Credit Card Fraud Detection**","3c237f68":"## **Random Forest After Under-Sampling**","784c321b":"As you can see by te above graph we can interpret how data is highly unbalanced","e1f57874":"As we can see the our accuracy matrix score is near about 99% but the f1 score is only 58%","a5a2bc3d":"## Deep Leaning Approch","c214aea7":"Our accuracy score decreased after upsampling, but the model is now predicting both classes more equally, making it an improvement over our plain logistic regression above."}}