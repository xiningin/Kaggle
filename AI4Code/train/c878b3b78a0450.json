{"cell_type":{"4ba4e8de":"code","bf4ab5bf":"code","6e25c9c4":"code","bf6c738b":"code","db4f8005":"code","8b11101f":"code","471f15c9":"code","b0f23cd0":"code","494b0b5f":"code","c074c620":"code","339c55b5":"code","cdb51fae":"code","cbbd0c7f":"code","dde88efa":"code","c0dc3296":"code","4e14c2cf":"code","690a9db5":"code","8f19c392":"code","8453e50f":"code","f3605b17":"code","08b4c4ad":"code","eb2c06c8":"code","129faa2c":"code","cf4e294d":"code","389237c4":"code","d2caa8fa":"code","86e8c755":"code","d9be8cbd":"code","70e4c42a":"code","bd99e5b2":"code","df4ddbdb":"code","aa9a5bb6":"code","e50d46ae":"code","4c10edd3":"code","73d5f66b":"code","dd49cd84":"code","b7c22ab0":"code","ea918937":"code","f8e6d024":"code","bd878deb":"code","ce66f256":"code","25d503e4":"code","167b0d6e":"code","426808d0":"code","8402a59d":"code","29c4a7f4":"code","d66a3969":"code","5028be37":"code","f50954f2":"code","b1402a29":"code","5bbedfb9":"code","e20afebb":"code","9deeb156":"code","a1ccf374":"code","f769cf6f":"code","7eee021e":"code","bb8b5b81":"code","7ac2774c":"code","8c616a80":"code","8ca2b77f":"code","c5d29112":"code","5e629742":"code","45b68616":"code","5c7bcfa7":"code","0395a3fb":"code","5eb3759c":"code","ec881b3e":"code","7dd3844f":"markdown","36ed56d6":"markdown","ac5a21d2":"markdown","25632bca":"markdown","92e54e9f":"markdown","f3d604b4":"markdown","b4987f53":"markdown","aa454266":"markdown","17a8c588":"markdown","2585ef44":"markdown"},"source":{"4ba4e8de":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bf4ab5bf":"#Importing the required libraries to read,visualize and model the givn dataset files\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport missingno as msno \nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport re\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nimport nltk\nfrom nltk.tokenize import word_tokenize,RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom platform import python_version\nprint (python_version())","6e25c9c4":"# Read the dataset csv files and create pandas datframes\ntrain_df=pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest_df=pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\nprint(\"Train and Test data sets are imported successfully\")","bf6c738b":"def explore_data(df):\n    \n    '''Input- df= pandas dataframes to be explored\n       Output- print shape, info and first 5 records of the dataframe \n    '''\n    \n    print(\"-\"*50)\n    print('Shape of the dataframe:',df.shape)\n    print(\"Number of records in train data set:\",df.shape[0])\n    print(\"Information of the dataset:\")\n    df.info()\n    print(\"-\"*50)\n    print(\"First 5 records of the dataset:\")\n    return df.head()\n    print(\"-\"*50)\nexplore_data(train_df)","db4f8005":"explore_data(test_df)","8b11101f":"#Calculate count and percentage of missing values in the dataframe\n\ndef missing_values(df):\n    \n    '''Input- df=pandas dataframe\n       Output- print missing records count and % of the input dataframe and visualize using MSNO\n    '''\n    \n    print(\"Number of records with missing location:\",df.location.isnull().sum())\n    print(\"Number of records with missing keywords:\",df.keyword.isnull().sum())\n    print('{}% of location values are missing from Total Number of Records.'.format(round((df.location.isnull().sum())\/(df.shape[0])*100),2))\n    print('{}% of keywords values are missing from Total Number of Records.'.format(round((df.keyword.isnull().sum())\/(df.shape[0])*100),2))\n    msno.matrix(df);","471f15c9":"missing_values(train_df)","b0f23cd0":"missing_values(test_df)","494b0b5f":"sns.countplot(x ='target',data =train_df)","c074c620":"sns.countplot(x ='keyword',data =train_df)","339c55b5":"train_df['keyword'].value_counts()","cdb51fae":"train_df.loc[train_df['text'].str.contains('disaster',na =False,case =False)].target.value_counts()","cbbd0c7f":"train_df['location'].value_counts()","dde88efa":"loc_dict={'United States':'USA','New York':'USA',\"London\":'UK',\"Los Angeles, CA\":'USA',\"Washington, D.C.\":'USA',\n          \"California\":'USA',\"Chicago, IL\":'USA',\"Chicago\":'USA',\"New York, NY\":'USA',\"California, USA\":'USA',\n          \"FLorida\":'USA',\"Nigeria\":'Africa',\"Kenya\":'Africa',\"Everywhere\":'Worldwide',\"San Francisco\":'USA',\n          \"Florida\":'USA',\"United Kingdom\":'UK',\"Los Angeles\":'USA',\"Toronto\":'Canada',\"San Francisco, CA\":'USA',\n          \"NYC\":'USA',\"Seattle\":'USA',\"Earth\":'Worldwide',\"Ireland\":'UK',\"London, England\":'UK',\"New York City\":'USA',\n          \"Texas\":'USA',\"London, UK\":'UK',\"Atlanta, GA\":'USA',\"Mumbai\":\"India\"}\n\ntrain_df['location'].replace(loc_dict,inplace =True)\n\n","c0dc3296":"sns.barplot(y=train_df['location'].value_counts()[:10].index,x=train_df['location'].value_counts()[:10],\n            orient='h');","4e14c2cf":"# Drop the column 'location' from the training dataset\ntrain_df=train_df.drop(['location'],axis=1)","690a9db5":"# Lets find the length of the 'text' for each row and add a new cloumn to train dataframe \ntrain_df['text_length'] = train_df['text'].apply(lambda x : len(x))\ntrain_df.head(5)","8f19c392":"train_df.text_length.describe()","8453e50f":"sns.distplot(train_df['text_length'])\nplt.title('Distribution of Tweet text Length')","f3605b17":"f, (ax1, ax2) = plt.subplots(1, 2, sharex=True,figsize=(10,6))\nsns.distplot(train_df[(train_df['target'] == 1)]['text_length'], ax=ax1, kde=False, color='blue',label='Disater Tweets')\nsns.distplot(train_df[(train_df['target'] == 0)]['text_length'],ax=ax2, kde=False, color='red',label='Non-Disater Tweets');\nf.suptitle('Tweet length distribution')\nf.legend(loc='upper right')\nax1.grid()\nax2.grid()\nplt.show()","08b4c4ad":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ndis_tweet=train_df[train_df['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(dis_tweet,color='blue')\nax1.set_title('Disaster tweets')\nax1.grid()\nnondis_tweet=train_df[train_df['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(nondis_tweet,color='red')\nax2.set_title('Non-disaster tweets')\nax2.grid()\nfig.suptitle('Words in a tweet')\nplt.show()","eb2c06c8":"# A disaster tweet exmaple\ntrain_df[train_df['target']==1]['text'][10:20]","129faa2c":"#A non-disaster tweet example\ntrain_df[train_df['target']==0]['text'][10:20]","cf4e294d":"#Lets have a quick look of the text data\ntrain_df['text'][:5]","389237c4":"#Define Function to clean text data\ndef clean_text(text):\n\n\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","d2caa8fa":"# Lets apply the clean_text function to both test and training datasets copies\ntrain_df1=train_df.copy()\ntest_df1=test_df.copy()\ntrain_df1['text'] = train_df1['text'].apply(lambda x: clean_text(x))\ntest_df1['text'] = test_df1['text'].apply(lambda x: clean_text(x))","86e8c755":"#Lets look cleaned text data\ndef text_after_preprocess(before_text,after_text):\n    \n    '''\n    Input- before_text=text column before cleanup\n              after_text= text column after cleanup\n       Output- print before and after text to compare how it looks after cleanup\n       \n    '''\n    print('-'*60)\n    print('Text before cleanup')\n    print('-'*60)\n    print(before_text.head(5))\n    print('-'*60)\n    print('Text after cleanup')\n    print('-'*60)\n    print(after_text.head(5))","d9be8cbd":"text_after_preprocess(train_df.text,train_df1.text)","70e4c42a":"text_after_preprocess(test_df.text,test_df1.text)","bd99e5b2":"test_df.text[1]","df4ddbdb":"text = \"Heard about #earthquake is different cities, stay safe everyone.\"\ntokenizer1 = nltk.tokenize.WhitespaceTokenizer()\ntokenizer2 = nltk.tokenize.TreebankWordTokenizer()\ntokenizer3 = nltk.tokenize.WordPunctTokenizer()\ntokenizer4 = nltk.tokenize.RegexpTokenizer(r'\\w+')\nprint(\"-\"*100)\nprint(\"Example Text: \",text)\nprint(\"-\"*100)\nprint(\"Tokenization by whitespace:- \",tokenizer1.tokenize(text))\nprint(\"Tokenization by words using Treebank Word Tokenizer:- \",tokenizer2.tokenize(text))\nprint(\"Tokenization by punctuation:- \",tokenizer3.tokenize(text))\nprint(\"Tokenization by regular expression:- \",tokenizer4.tokenize(text))","aa9a5bb6":"# Lets Tokenize the training and the test dataset copies with RegEx tokenizer\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrain_df1['text'] = train_df1['text'].apply(lambda x: tokenizer.tokenize(x))\ntest_df1['text'] = test_df1['text'].apply(lambda x: tokenizer.tokenize(x))","e50d46ae":"#Lets check tokenized text\ntrain_df1['text'].head()","4c10edd3":"#Lets check tokenized text\ntest_df1['text'].head()","73d5f66b":"#Create a funtion to remove stopwords\ndef remove_stopwords(text):\n    \n    \"\"\"\n    Input- text=text from which english stopwprds will be removed\n    Output- return text without english stopwords \n    \n    \"\"\"\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words","dd49cd84":"train_df1['text'] = train_df1['text'].apply(lambda x : remove_stopwords(x))\ntest_df1['text'] = test_df1['text'].apply(lambda x : remove_stopwords(x))","b7c22ab0":"train_df1.text.head()","ea918937":"test_df1.text.head()","f8e6d024":"# Stemming and Lemmatization examples\ntext =  \"ran deduced dogs talking studies\"\n\ntokenizer = nltk.tokenize.TreebankWordTokenizer()\ntokens = tokenizer.tokenize(text)\n\n# Stemmer\nstemmer = nltk.stem.PorterStemmer()\nprint(\"Stemming the sentence: \", \" \".join(stemmer.stem(token) for token in tokens))\n\n# Lemmatizer\nlemmatizer=nltk.stem.WordNetLemmatizer()\nprint(\"Lemmatizing the sentence: \", \" \".join(lemmatizer.lemmatize(token) for token in tokens))","bd878deb":"def combine_text(text):\n    \n    '''\n    Input-text= list cleand and tokenized text\n    Output- Takes a list of text and returns combined one large chunk of text.\n    \n    '''\n    all_text = ' '.join(text)\n    return all_text","ce66f256":"train_df1['text'] = train_df1['text'].apply(lambda x : combine_text(x))\ntest_df1['text'] = test_df1['text'].apply(lambda x : combine_text(x))","25d503e4":"train_df1.head()","167b0d6e":"\n# Create a function to pre-process the tweets\ndef pre_process_text_combined(text):\n    \n    \"\"\"\n    Input- text= text to be pre-processed\n    \n    Oputput- return cleaned and combined text to be vectrorized for Machine learning.\n\n    \"\"\"\n    #Initiate a tokenizer\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    # Clean the text using clean_text function\n    cleaned_txt = clean_text(text)\n    tokenized_text = tokenizer.tokenize(cleaned_txt)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(remove_stopwords)\n    return  combined_text","426808d0":"# Create a function to pre-process the tweets\ndef pre_process_text(text):\n    \"\"\"\n    Input- text= text to be pre-processed\n    \n    Oputput- return cleaned text to be vectrorized for Machine learning.\n\n    \"\"\"\n    #Initiate a tokenizer\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    # Clean the text using clean_text function\n    cleaned_txt = clean_text(text)\n    tokenized_text = tokenizer.tokenize(cleaned_txt)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    return remove_stopwords","8402a59d":"# Text before pre-processing\ntrain_df.text.head()","29c4a7f4":"# Lets pre-process train data text\ntrain_df2=train_df.copy()\ntrain_df2['text'] = train_df2['text'].apply(lambda x : pre_process_text_combined(x))","d66a3969":"# Text after pre-processing the text column\ntrain_df2.head()","5028be37":"# Lets pre-process test data text\ntest_df2=test_df.copy()\ntest_df2['text'] = test_df2['text'].apply(lambda x : pre_process_text_combined(x))","f50954f2":"# Text after pre-processing the text column\ntest_df2.head(10)","b1402a29":"# Lets pre-process train data text\ntrain_df3=train_df.copy()\ntrain_df3['text'] = train_df3['text'].apply(lambda x : pre_process_text(x))","5bbedfb9":"train_df3.head()","e20afebb":"#Lets have a quick look of the tweets in wordcloud\nfrom wordcloud import WordCloud\nfig, ax = plt.subplots(figsize=[10, 6])\nwordcloud = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(train_df2.text))\nax.imshow(wordcloud)\nax.axis('off')\nax.set_title('Disaster Tweets',fontsize=40);","9deeb156":"# Vectorize the text using CountVectorizer\ncount_vectorizer = CountVectorizer()\ntrain_cv = count_vectorizer.fit_transform(train_df2['text'])\ntest_cv = count_vectorizer.transform(test_df2[\"text\"])\n\n## Keeping only non-zero elements to preserve space \nprint(train_cv[0].todense())","a1ccf374":"# Vectorize the text using TFIDF\ntfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_tf = tfidf.fit_transform(train_df2['text'])\ntest_tf = tfidf.transform(test_df2[\"text\"])","f769cf6f":"#Split the CountVector vectorized data into train and test datasets for model training and testing\nX_train_cv, X_test_cv, y_train_cv, y_test_cv =train_test_split(train_cv,train_df.target,test_size=0.2,random_state=2020)","7eee021e":"#Define a function to fit and predict on training and test data sets\ndef fit_and_predict(model,X_train,y_train,X_test,y_test):\n    \n    '''Input- model=model to be trained\n              X_train, y_train= traing data set\n              X_test,  y_test = testing data set\n       Output- Print accuracy of model for training and test data sets   \n    '''\n    \n    # Fitting a simple Logistic Regression on Counts\n    clf = model\n    clf.fit(X_train, y_train)\n    predictions=clf.predict(X_test)\n    confusion_matrix(y_test,predictions)\n    print(classification_report(y_test,predictions))\n    print('-'*50)\n    print(\"{}\" .format(model))\n    print('-'*50)\n    print('Accuracy of classifier on training set:{}%'.format(round(clf.score(X_train, y_train)*100)))\n    print('-'*50)\n    print('Accuracy of classifier on test set:{}%' .format(round(accuracy_score(y_test,predictions)*100)))\n    print('-'*50)","bb8b5b81":"# Create a list of the regression models to be used\nmodels=[LogisticRegression(C=1.0),SVC(),MultinomialNB(),DecisionTreeClassifier(),\n        KNeighborsClassifier(n_neighbors=5),RandomForestClassifier()]","7ac2774c":"# Loop through the list of models and use 'fit_and_predict()' function to trian and make predictions\nfor model in models:\n    fit_and_predict(model,X_train_cv, y_train_cv,X_test_cv,y_test_cv)","8c616a80":"# Split the TFDIF vectorized data into train and test datasets for model training and testing\nX_train_tf, X_test_tf, y_train_tf, y_test_tf =train_test_split(train_tf,train_df.target,test_size=0.2,random_state=2020)","8ca2b77f":"# Loop through the list of models and use 'fit_and_predict()' function to train and make predictions on the TFDIF vectororized data\nfor model in models:\n    fit_and_predict(model,X_train_tf, y_train_tf,X_test_tf,y_test_tf)","c5d29112":"# Fitting 'LogisticRegression()' with CountVectorizer() fit dataset\nclf_logreg = LogisticRegression(C=1.0)\nclf_logreg.fit(X_train_cv, y_train_cv)\npred=clf_logreg.predict(X_test_cv)\nconfusion_matrix(y_test_cv,pred)\nprint(classification_report(y_test_cv,pred))\nprint('Accuracy of classifier on training set:{}%'.format(round(clf_logreg.score(X_train_cv, y_train_cv)*100)))\nprint('Accuracy of classifier on test set:{}%' .format(round(accuracy_score(y_test_cv,pred)*100)))","5e629742":"clf_logreg","45b68616":"# Create the list of various hyper parameters to try\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]\nlogreg= LogisticRegression()\n\n# Define and fit grid search\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=logreg, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train_cv, y_train_cv)\n\n# Summarize and print results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","5c7bcfa7":"# Lets apply pre-processing function to clean and pre-process text data before vectorizing\ntest_df_final=test_df.copy()\ntest_df_final['text'] = test_df_final['text'].apply(lambda x : pre_process_text_combined(x))","0395a3fb":"# Lets fit the test data with Countvectorizer() method to vectroize the data\ntest_vector = count_vectorizer.transform(test_df_final[\"text\"])","5eb3759c":"# Define a function to generate predictions and store in a.csv file for submission on Kaggle\ndef submission(sub_file,model,test_vector):\n    \n    '''Input- sub_file=Location of the file submission file\n              model=final fit model to be used for predictions\n              test_vector=pre-processed and vectorized test dataset\n       Output- submission file in .csv format with predictions       \n    \n    '''\n    sub_df = pd.read_csv(sub_file)\n    sub_df[\"target\"] = model.predict(test_vector)\n    sub_df.to_csv(\"submission.csv\", index=False)","ec881b3e":"# Use Submission() function to generate submission file for posting on Kaggle\nsub_file = \".\/submission.csv\"\ntest_vector=test_vector\nsubmission(sub_file,clf_logreg,test_vector)","7dd3844f":"The distribution of both kind of tweets seems to be almost same.About 120 to 140 characters in a tweet are the most common among both.","36ed56d6":"**Let's check how often the word 'disaster' come in the 'text' feature in the dataset and whether this help us in determining whether a tweet belongs to a disaster' category or not.**","ac5a21d2":"> ****Both the Datasets contains missing values in the columns 'keyword' and 'location'","25632bca":"**It seems even though word disaster is part of the tweets but that doesn't mean tweet announces a real disaster.**","92e54e9f":"It doesn't seem that location feature has any value in our analysis and could be dropped from the dataframe.****","f3d604b4":"> Text Pre-processing Function\nPutting all above functions together, below will be final function to pre-process the text before modeling with ML algorithms.****","b4987f53":"***Stemming and Lemmatization\nStemming: Removing and replacing suffixes to get to the root form of the word, which is called the stem for instance cats - cat, wolves - wolv\nLemmatization : Returns the base or dictionary form of a word, which is known as the lemma\nIt is important to note that stemming and lemmatization sometimes doesnt necessarily improve results as sometimes we dont want to trim words rather preserve their original form.Its usage from problem to problem and for this problem it wouldnt be good idea to use it.***","aa454266":"Even though the column 'location' has lots of missing values, let's check the top 20 locations present in the dataset. Since some of the locations are duplicated, lets clean the duplicate locations and plot them.****","17a8c588":"**From the observations we can conclude that location column can be dropped from the dataset as it contains lot of missing values.******","2585ef44":"# #  Build a Text Classification Machine Learning model\nLets create a simple classification model using commonly used calssification algorithms and check how the models performs."}}