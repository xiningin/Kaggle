{"cell_type":{"a53191c3":"code","7bcdd07b":"code","86bdd8e4":"code","6e2a4b6c":"code","e2c5d54f":"code","50673dd0":"code","70d6a388":"code","d0dfb58d":"code","88683e13":"code","a88e97a8":"code","72eae31f":"code","9e6c05b9":"code","d8726609":"code","b55dfc5e":"code","6c17f601":"code","37da169d":"code","de0c0792":"code","123cbb82":"code","8dd25e16":"code","269309a5":"code","55b06aa6":"code","19b8c789":"code","227909e0":"code","2b03357e":"code","32c25c3b":"code","e9a16c86":"code","7833d34a":"code","6ec43b2f":"code","49665a4a":"code","b9232789":"code","515092ae":"code","f37462ab":"code","3ee0aba5":"code","38e26d2d":"code","a8542257":"code","83333be0":"code","fa8f844e":"code","9f2eba6f":"code","d26e4ad7":"code","d9e89966":"code","7bf76b2c":"code","7aa8f094":"code","28a5ac15":"code","898dde2a":"code","31975b76":"code","e31f51a6":"code","7d263a2d":"code","4a4d3515":"code","2d5919d2":"code","da598382":"code","5f794219":"code","cd8f4010":"code","e7c13de5":"code","f556e48e":"code","df744621":"code","a54e2346":"code","a6b5abcc":"code","98c0a818":"code","d6b5622f":"code","d6546c84":"code","26bc1801":"code","bedf636e":"markdown","bdbc5e29":"markdown","a885a341":"markdown","133dcf69":"markdown","400e59e7":"markdown","51d1b89f":"markdown","bef221f2":"markdown","42366426":"markdown","6b5be8ce":"markdown","4186b91f":"markdown","c3794cf4":"markdown","1caa853d":"markdown","5d6eaa45":"markdown","a58ee3bb":"markdown","7eb2dc4b":"markdown","3b2c5137":"markdown","9cb5f335":"markdown","6639a0cb":"markdown","1c7eca59":"markdown","4efc60eb":"markdown","cc584b65":"markdown","c6facbc7":"markdown","f4c53e4f":"markdown","413579cc":"markdown","d32ee649":"markdown","53813bce":"markdown","9f1c8c73":"markdown","0b0a6411":"markdown","04e0cd21":"markdown","597c9795":"markdown","fef48177":"markdown","fc68f39d":"markdown","6756f95a":"markdown","d5a31a71":"markdown","5f90a9f6":"markdown","2db58ccd":"markdown","54dab281":"markdown","b42f3ea5":"markdown","dc3a1ea4":"markdown","cdb4fdc6":"markdown","eebe0333":"markdown","65b0df03":"markdown"},"source":{"a53191c3":"# import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set()\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score","7bcdd07b":"# read data\ntrain_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')","86bdd8e4":"train_df.head()","6e2a4b6c":"test_df.head()","e2c5d54f":"train_df.info()","50673dd0":"# drop unuseful columns\ntrain_df.drop(['PassengerId', 'Name', 'Ticket'], axis= 1, inplace = True)\ntrain_df.head()","70d6a388":"train_df.Age.describe()","d0dfb58d":"# Check Age Distribution\ntrain_df.Age.hist();","88683e13":"train_df['Age'].fillna(train_df['Age'].mean(), inplace=True)\ntrain_df.Age.isnull().sum()","a88e97a8":"train_df.Age.hist();","72eae31f":"train_df.Age.describe()","9e6c05b9":"train_df.Cabin.nunique()","d8726609":"train_df.Cabin.describe()","b55dfc5e":"train_df.Cabin.value_counts().nlargest(10)","6c17f601":"train_df.Cabin.isnull().sum()","37da169d":"train_df.drop('Cabin', axis= 1, inplace = True)\ntrain_df.head()","de0c0792":"train_df.Embarked.unique()","123cbb82":"train_df.Embarked.isnull().sum()","8dd25e16":"train_df.Embarked.value_counts()","269309a5":"train_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace = True)\ntrain_df['Embarked'].isnull().sum()","55b06aa6":"train_df['Embarked'].replace({'C' : 'Cherbourg', 'Q' : 'Queenstown', 'S' : 'Southampton'}, inplace = True)\ntrain_df.head()","19b8c789":"train_df.head()","227909e0":"def explore_feature(feature):\n    group= train_df.groupby(feature).mean()\n    plots = group.Survived.sort_values().plot(kind = 'bar', fontsize=15)\n    plots.title.set_size(40);\n    plt.xlabel(feature, fontsize=15);\n    plt.ylabel('% Survived', fontsize=15);\n    plt.title(\"% Survived according to {} feature\\n\".format(feature), fontsize=20)\n    for bar in plots.patches:\n        plots.annotate(format(bar.get_height(), '.2f'), \n                       (bar.get_x() + bar.get_width() \/ 2, \n                        bar.get_height()), ha='center', va='center',\n                       size=15, xytext=(0, 8),\n                       textcoords='offset points')\n    ","2b03357e":"explore_feature('Pclass')","32c25c3b":"sns.countplot(x=\"Pclass\", hue = 'Survived', data=train_df);","e9a16c86":"explore_feature('Sex')","7833d34a":"train_df.Age.hist();","6ec43b2f":"# Age Categorizing\ntrain_df.loc[(train_df.Age < 18),'AgeGroup'] = 'Children'\ntrain_df.loc[(train_df.Age >=18) & (train_df.Age <30),'AgeGroup'] = 'Youth'\ntrain_df.loc[(train_df.Age >=30) & (train_df.Age <60),'AgeGroup'] = 'Adult'\ntrain_df.loc[(train_df.Age >=60),'AgeGroup'] = 'Senior'\ntrain_df.head()","49665a4a":"explore_feature('AgeGroup')","b9232789":"explore_feature('Embarked')","515092ae":"train_df.Fare.hist()","f37462ab":"# Fare Categorizing\ntrain_df.loc[(train_df.Fare < 50),'FareGroup'] = 'Low'\ntrain_df.loc[(train_df.Fare >= 50) & (train_df.Fare < 100),'FareGroup'] = 'Medium'\ntrain_df.loc[(train_df.Fare >= 100),'FareGroup'] = 'High'\ntrain_df.head()","3ee0aba5":"explore_feature('FareGroup')","38e26d2d":"train_df.SibSp.value_counts()","a8542257":"explore_feature('SibSp')","83333be0":"explore_feature('Parch')","fa8f844e":"train_df.head()","9f2eba6f":"features= train_df[['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']]\ntarget = train_df['Survived']","d26e4ad7":"features = pd.get_dummies(features, drop_first=True)\nfeatures.head()","d9e89966":"target.head()","7bf76b2c":"X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=42)","7aa8f094":"scaler = StandardScaler()\nX_train_sc = scaler.fit_transform(X_train)\nX_test_sc = scaler.transform(X_test)","28a5ac15":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.model_selection import cross_validate\n\n# array of methods\nmodels=[LogisticRegression(),\n        RandomForestClassifier(),\n        SVC(),\n        DecisionTreeClassifier(),\n        KNeighborsClassifier()]\n\n\nfor model in models:\n    # fit the model with cross validation\n    results=cross_validate(model,X_train_sc,y_train,cv=10)\n    # accuracy\n    r2=results['test_score'].mean()\n    # print the result\n    m=str(model)\n    print('r2 for',m[:m.index('(')],'=',r2)","898dde2a":"# Logistic Regression\n\nparam_lr={'penalty':['l1','l2'],\n          'C' : [0.01,0.1,1,10,50,100,200,300],\n         'solver':['liblinear', 'saga']}\n\ngs_lr = GridSearchCV(LogisticRegression(),param_grid = param_lr, scoring=\"accuracy\",n_jobs=-1)\ngs_lr.fit(X_train_sc,y_train)\nbest_lr=gs_lr.best_estimator_\nprint(best_lr)\nprint('score=',gs_lr.best_score_)","31975b76":"# Random Forest\nparam_rf={'max_features': [1, 2, 3, 5, 10],\n          'min_samples_split': [2, 3, 5, 7, 10],\n          'min_samples_leaf': [1, 3, 5, 7, 10],\n          'bootstrap': [False],\n          'n_estimators' :[100,200,300]}\n\ngs_rf = GridSearchCV(RandomForestClassifier(),param_grid = param_rf, scoring=\"accuracy\",n_jobs=-1)\ngs_rf.fit(X_train_sc,y_train)\nbest_rf=gs_rf.best_estimator_\nprint(best_rf)\nprint('score=',gs_rf.best_score_)","e31f51a6":"# Support Vector Machine\nparam_sv={'probability':[True],\n          'gamma': [ 0.001, 0.01, 0.1, 1],\n          'C': [1, 10, 50, 100, 200, 300, 1000]}\n\ngs_sv = GridSearchCV(SVC(),param_grid = param_sv, scoring=\"accuracy\",n_jobs=-1)\ngs_sv.fit(X_train_sc,y_train)\nbest_sv=gs_sv.best_estimator_\nprint(best_sv)\nprint('score=',gs_sv.best_score_)","7d263a2d":"# Decision Tree\nparam_dt={'max_features': [1, 2, 3, 5, 6, 7, 8, 9, 10, 15],\n          'min_samples_split': [2, 3, 4, 5, 6, 7, 10, 15],\n          'min_samples_leaf': [1, 2, 3, 5, 6, 7, 8, 10, 15],\n          'splitter':['best']}\n\ngs_dt = GridSearchCV(DecisionTreeClassifier(),param_grid = param_dt, scoring=\"accuracy\",n_jobs=-1)\ngs_dt.fit(X_train_sc,y_train)\nbest_dt=gs_dt.best_estimator_\nprint(best_dt)\nprint('score=',gs_dt.best_score_)","4a4d3515":"# KNN\nparam_kn={'n_neighbors':[1,2,3,5,7,10,14,15]}\n\ngs_kn = GridSearchCV(KNeighborsClassifier(),param_grid = param_kn, scoring=\"accuracy\",n_jobs=-1)\ngs_kn.fit(X_train_sc,y_train)\nbest_kn=gs_kn.best_estimator_\nprint(best_kn)\nprint('score=',gs_kn.best_score_)","2d5919d2":"from sklearn.ensemble import VotingClassifier\nvote=VotingClassifier(estimators=[('lr',best_lr),\n                                  ('rfc', best_rf),\n                                  ('svc',best_sv),\n                                  ('dtc',best_dt),\n                                  ('knc',best_kn)],\n                      voting='soft', n_jobs=-1)\n\nvote = vote.fit(X_train_sc, y_train)","da598382":"from xgboost.sklearn import XGBClassifier\n\ngbm = XGBClassifier(learning_rate = 0.02, n_estimators= 2000, max_depth= 4)\n\ngbm.fit(X_train_sc, y_train)","5f794219":"y_pred_vote=vote.predict(X_test_sc)\ny_pred_gbm=gbm.predict(X_test_sc)","cd8f4010":"print('Accuracy of Vote Classifier: ', accuracy_score(y_test, y_pred_vote))\nprint('Accuracy of Xgboot Classifier: ', accuracy_score(y_test, y_pred_gbm))","e7c13de5":"test_df.head()","f556e48e":"test_df['Embarked'].replace({'C' : 'Cherbourg', 'Q' : 'Queenstown', 'S' : 'Southampton'}, inplace = True)\ntest_features = test_df[['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']]\ntest_features = pd.get_dummies(test_features, drop_first=True)\ntest_features.head()","df744621":"test_features.isnull().sum()","a54e2346":"test_features.fillna(test_features.mean(), inplace= True)\ntest_features.isnull().sum()","a6b5abcc":"X_pred = scaler.transform(test_features)\nX_pred","98c0a818":"submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsubmission.head()","d6b5622f":"best_rf = RandomForestClassifier(bootstrap=False, max_features=2, min_samples_leaf=3, min_samples_split=7)\nbest_rf.fit(X_train_sc, y_train)\n\nsubmission['Survived'] = best_rf.predict(X_pred)\nsubmission.to_csv('submission_best_rf.csv', index = False)","d6546c84":"submission['Survived'] = gbm.predict(X_pred)\nsubmission.to_csv('submission_gbm.csv', index = False)","26bc1801":"submission['Survived'] = vote.predict(X_pred)\nsubmission.to_csv('submission_vote.csv', index = False)","bedf636e":"I'll try to build several models using different methods and choose the best one.","bdbc5e29":"### Splitting Features and Target data into training set and test set","a885a341":"### So, it is obvious that the higher your class level on Titanic, the higher probability to be survived","133dcf69":"Nearly, the same shape and close values for \"Age\"","400e59e7":"### The bigger fare is, the more chances to survive you have. I think the fare was connected with the class.","51d1b89f":"<div style=\" color:#069; border:1px solid #b3deff; padding: 20px\">\n<h2 id='Prediction'>Prediction<\/h2>\n<\/div>","bef221f2":"<h1>Titanic competition<\/h1>","42366426":"<div style=\" color:#069; border:1px solid #b3deff; padding: 20px\">\n<h2 id='Data-Analysis'>Data Analysis<\/h2>\n<\/div>","6b5be8ce":"# Random Forest Prediction","4186b91f":"## 3. Handling Missing Values in \"Embarked\"","c3794cf4":"### Splitting Training data into Features and Target","1caa853d":"<div style=\" color:#069; border:1px solid #b3deff; padding: 20px\">\n<h2 id='Data-Preprocessing'>Data Preprocessing<\/h2>\n<\/div>","5d6eaa45":"## \"Pclass\" Exploration","a58ee3bb":"<div style=\" color:#069; border:1px solid #b3deff; padding: 20px\">\n<h2 id='Modelling'>Modelling<\/h2>\n<\/div>","7eb2dc4b":"So, Missing values are nearly 3 times more than present values in \"Cabin\"\n\nAlso, Values counts are very close, so, it is not a good idea to impute missing values with \"mode\"\n\nSo, I will drop this column!","3b2c5137":"So, We dropped *PassengerId*, *name* and *Cabin* columns\n\nand imputed missing values in *Age* and *Embarked* columns\n\nand We have another task, to handle categorical data in *Sex* and *Embarked* columns , but after some EDA","9cb5f335":"### People from Cherbourg have more chances than from Queenstown and Southampton.\n","6639a0cb":"### Handling Categorical Features","1c7eca59":"## 2. Handling Missing Values in \"Cabin\"","4efc60eb":"### So, the children (Age < 18) had the higher probability to survive, whereas Seniors (Age > 60) had the lower probability","cc584b65":"### So, it is obvious that females on Titanic had higher probability than males to be survived","c6facbc7":"From the describtion and visualization, the mean and median values are very close.\n\nSo, We can impute missing values in \"Age\" by mean","f4c53e4f":"# Vote Classifier Prediction","413579cc":"### The passengers with No. of \"Parch\" in range(1,3) have close chances (50-60 %)","d32ee649":"## \"Sex\" Exploration","53813bce":"Replace letters in 'Embarked' Column with their meanings","9f1c8c73":"## 1. Handling Missing Values in \"Age\"","0b0a6411":"## General Exploration Function","04e0cd21":"So, We have some unuseful columns : PassengerId, Name and Ticket  >> To be dropped\n\nand Non-numeric Useful columns: Sex, Cabin and Embarked  >> To be encoded\n\nand Columns with missing values: Age, Cabin and Embarked  >> To be imputed","597c9795":"## \"Embarkment\" Exploration","fef48177":"## \"Fare\" Exploration","fc68f39d":"## Testing data Preprocessing","6756f95a":"<div style=\" border:1px solid #b3deff; padding: 20px\">\n<h2>Table of content<\/h2>\n<ul>    \n<li><a href=\"#Data-Analysis\">Data Analysis<\/a><\/li>\n<li><a href=#Data-Preprocessing>Data Preprocessing<\/a><\/li>\n<li><a href=#Modelling>Modelling<\/a><\/li>\n<li><a href=#Prediction>Prediction<\/a><\/li>\n<\/ul>\n<\/div>","d5a31a71":"# Xgboost Prediction","5f90a9f6":"### Scaling","2db58ccd":"### Excluding the single passenger, the less No. of siblings\/spouses, the more chance to survive","54dab281":"# Handling Missing Values","b42f3ea5":"# Let's Explore Training Data","dc3a1ea4":"# EDA","cdb4fdc6":"## \"Parch\" Exploration\n> * From Date Overview:\n    * Parent = mother, father\n    * Child = daughter, son, stepdaughter, stepson\n    * Some children travelled only with a nanny, therefore parch=0 for them.","eebe0333":"## \"Age\" Exploration","65b0df03":"## \"SibSp\" Exploration\n> * From Date Overview:\n    * Sibling = brother, sister, stepbrother, stepsister\n    * Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)"}}