{"cell_type":{"b95e3ee3":"code","b6b77010":"code","cc78e06d":"code","20a75f8f":"code","3ed7b342":"code","00610b10":"code","f115725a":"code","604b1c2c":"code","7c721bcf":"code","589fa168":"code","79bb2860":"code","a137cc9e":"markdown","8d38c053":"markdown","f337b68a":"markdown","03c47860":"markdown","0e0d0677":"markdown","1bd18169":"markdown"},"source":{"b95e3ee3":"!pip install mglearn fulltext\n\nimport os\nimport re\nimport nltk\nimport IPython\nimport sklearn\nimport mglearn\nimport fulltext\nimport pyLDAvis\nimport threading\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nfrom pandas import Series\nfrom tabulate import tabulate\nfrom bs4 import BeautifulSoup\nfrom string import punctuation\nfrom nltk.corpus import stopwords\nfrom IPython.display import display, clear_output\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nprint('complete')\nthreading.activeCount()\n\nstop_words = set(stopwords.words('english'))\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b6b77010":"# TRAINING DATA\n# Three articles about 3 related (auto), but different topics; Road Policing, The Auto Industry & Winter Driving\nauto_article_path_1 = \"..\/input\/toronto-star-sample-articles\/toronto-police-try-a-newold-trick.txt\"\nauto_article_path_2 = \"..\/input\/toronto-star-sample-articles\/canadian-sales-down-two-per-cent-year-over-year-in-october-while-multiple-automakers-set-records.html.txt\"\nauto_article_path_3 = \"..\/input\/toronto-star-sample-articles\/winter-car-prep-done-right.txt\"\n\n# TESTING DATA (stuff the model hasn't seen before but can hopefully recognize because of training)\n# One related article about Winter Driving\nauto_article_path_4 = \"..\/input\/toronto-star-sample-articles\/winter-of-2020-is-coming-are-you-ready.txt\"","cc78e06d":"# URLS:\n# Article 1: https:\/\/www.thestar.com\/autos\/opinion\/2020\/11\/18\/toronto-police-try-a-newold-trick.html\n# Article 2: https:\/\/www.thestar.com\/autos\/2020\/11\/16\/canadian-sales-down-two-per-cent-year-over-year-in-october-while-multiple-automakers-set-records.html\n# Article 3: https:\/\/www.thestar.com\/autos\/advice\/2020\/11\/20\/what-ive-learned-after-15-winters-of-testing-tires.html\n# Article 4: https:\/\/www.thestar.com\/autos\/advice\/2020\/11\/27\/winter-of-2020-is-coming-are-you-ready.html\n\nwith open(auto_article_path_1, 'rb') as this_file:\n    auto_article_text_1 = this_file.read()\nprint(\"---- ARTICLE 1 RAW TEXT ----\")\nprint(auto_article_text_1[0:300])\n\nwith open(auto_article_path_2, 'rb') as this_file:\n    auto_article_text_2 = this_file.read()\nprint(\"\\n---- ARTICLE 2 RAW TEXT ----\")\nprint(auto_article_text_2[0:300])\n\nwith open(auto_article_path_3, 'rb') as this_file:\n    auto_article_text_3 = this_file.read()\nprint(\"\\n---- ARTICLE 3 RAW TEXT ----\")\nprint(auto_article_text_3[0:300])\n\nwith open(auto_article_path_4, 'rb') as this_file:\n    auto_article_text_4 = this_file.read()\nprint(\"\\n---- ARTICLE 4 RAW TEXT ----\")\nprint(auto_article_text_4[0:300])","20a75f8f":"def clean_up(text):\n    \n    result = re.sub(r'\\\\x?..', ' ', str(text))\n    result = str(result).replace(',', '')\n    result = str(result).replace('|', '')\n    result = str(result).replace('\"', '')\n    result = str(result).replace('*', '')\n    result = re.sub(r'\\s.\\s', ' ', str(result))\n    result = re.sub(r'\\s.{1}\\s', ' ', str(result))\n    result = str(result).replace('-', ' ')\n    result = str(result).replace('_', '')\n    result = str(result).replace('\\n', ' ')\n    result = str(result).replace('\\t', ' ')\n    result = str(result).replace('\\r', ' ')\n    result = str(result).replace('&nbsp;', ' ')\n        \n    result = str(result.lower())\n    \n    result = ''.join([c for c in result if c not in punctuation])\n    \n    result = ' '.join([w for w in result.split() if w not in stop_words])\n    \n    # Remove Multiple Spaces\n    result = re.sub(' +', ' ', str(result))\n        \n    # Remove duplicate consecutive words\n    result = re.sub(r'\\b(\\w+)\\s+\\1\\b', '', str(result)) \n    \n    result = result[1:len(result)]\n    \n    return result\n\narticle_1_clean = clean_up(auto_article_text_1)\narticle_2_clean = clean_up(auto_article_text_2)\narticle_3_clean = clean_up(auto_article_text_3)\narticle_4_clean = clean_up(auto_article_text_4)\n\nprint(\"---- ARTICLE 1 CLEAN TEXT ----\")\nprint(article_1_clean[0:300])\nprint(\"\\n---- ARTICLE 2 CLEAN TEXT ----\")\nprint(article_2_clean[0:300])\nprint(\"\\n---- ARTICLE 3 CLEAN TEXT ----\")\nprint(article_3_clean[0:300])\nprint(\"\\n---- ARTICLE 4 CLEAN TEXT ----\")\nprint(article_4_clean[0:300])","3ed7b342":"# Unique Words Present In Both Articles\noverlap_4_1 = len(set(article_4_clean.split()) & set(article_1_clean.split()))\noverlap_4_2 = len(set(article_4_clean.split()) & set(article_2_clean.split()))\noverlap_4_3 = len(set(article_4_clean.split()) & set(article_3_clean.split()))\n\nprint(\"Article 4 (about winter driving) Contains \" + str(overlap_4_1) + \" Words That Are Also In Article 1 (about road policing)\")\nprint(\"Article 4 (about winter driving) Contains \" + str(overlap_4_2) + \" Words That Are Also In Article 2 (about the auto industry)\")\nprint(\"Article 4 (about winter driving) Contains \" + str(overlap_4_3) + \" Words That Are Also In Article 3 (about winter driving)\")\n\nall_duplicates = overlap_4_1 + overlap_4_2 + overlap_4_3\n\nprint(\"Weight for Artile 1: \", round(overlap_4_1 \/ all_duplicates, 2), \"%\")\nprint(\"Weight for Artile 2: \", round(overlap_4_2 \/ all_duplicates, 2), \"%\")\nprint(\"Weight for Artile 3: \", round(overlap_4_3 \/ all_duplicates, 2), \"%\")","00610b10":"data_corpus = [ article_1_clean, article_2_clean, article_3_clean ]\n\nvect = CountVectorizer(input='content', binary=False, min_df=.0005, max_df=0.35, ngram_range=(1,4))\n\ndtm = vect.fit_transform(data_corpus)\n\nmatrix = pd.DataFrame(dtm.toarray(), columns=vect.get_feature_names())\n\nprint(matrix.shape)","f115725a":"feature_names = vect.get_feature_names()\n\ndef runLDA(dtm, vect, n_components, alpha=None, beta=None):\n\n    print('=== LDA MODEL : ' + str(n_components) + ' TOPICS ===')\n    \n    lda_model = LatentDirichletAllocation(n_components=n_components,\n                                          doc_topic_prior=alpha,\n                                          topic_word_prior=beta,\n                                          max_iter=10, \n                                          learning_method='batch', \n                                          random_state=123,\n                                          n_jobs=-1,\n                                          verbose=1)\n    \n    lda_output = lda_model.fit(dtm)\n\n    ll = lda_model.score(dtm)                            # Log Likelyhood: Higher the better\n    perp = lda_model.perplexity(dtm)                     # Perplexity: Lower the better.\n    sorting = np.argsort(lda_model.components_)[:, ::-1] # sorted terms\n    theta = pd.DataFrame(lda_model.transform(dtm))       # document-topic matrix\n    beta = pd.DataFrame(lda_model.components_)           # components_ = topic-term matrix\n    \n    # Build Custom Topic Summary\n    no_top_words = 1000\n    weight = theta.sum(axis=0)\n    support50 = (theta > 0.5).sum(axis=0)\n    support10 = (theta > 0.1).sum(axis=0)\n    termss = list()\n    for topic_id, topic in enumerate(lda_model.components_):\n        terms = \" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n        termss.append(terms)\n    topic_summary = pd.DataFrame({'TopicID': range(0, len(termss)), \n                                  'Support50': support50, \n                                  'Support10': support10, \n                                  'Weight': weight, \n                                  'Terms': termss})\n    return {'model': lda_model, \n            'theta': theta, \n            'beta': beta, \n            'topic_summary': topic_summary, \n            'll': ll, \n            'perp': perp,\n            'sorting': sorting,\n            'n_components': n_components}\n\n\nlda_2 = runLDA(dtm, vect, 2)\nlda_3 = runLDA(dtm, vect, 3)\nlda_4 = runLDA(dtm, vect, 4)\nlda_5 = runLDA(dtm, vect, 5)\nlda_10 = runLDA(dtm, vect, 10)\nprint('done')","604b1c2c":"# Perplexity (lower the better?)\npp = [lda_2['perp'], lda_3['perp'], lda_4['perp'], lda_5['perp'], lda_10['perp']]\nplt.title('PERPLEXITY')\nplt.ylabel('Perplexity')\nplt.xlabel('Number Of Topics')\nplt.plot([2, 3, 4, 5, 10], pp)\nplt.show()\n\n# Log Likelyhood (higher the better?)\nll = [lda_2['ll'], lda_3['ll'], lda_4['ll'], lda_5['ll'], lda_10['ll']]\nplt.title('LOG LIKELIHOOD')\nplt.ylabel('Log Likelihood')\nplt.xlabel('Number Of Topics')\nplt.plot([2, 3, 4, 5, 10], ll)\nplt.show()\n\n# when in doubt, go for the elbow","7c721bcf":"lda_choice = lda_3\nn_topics = lda_choice['n_components']","589fa168":"# Word cloud for each topic\nfor t in range(n_topics):\n\n    # Extract terms from this LDA topic\n    topic_data = lda_3['topic_summary'].loc[t, 'Terms']\n    print(topic_data)\n    # Create and generate a word cloud\n    wordcloud = WordCloud(max_font_size = 75,\n                          max_words = 250,\n                          background_color = 'white',\n                          width = 600,\n                          height = 400).generate(str(topic_data))\n\n    # Display the generated image:\n    plt.figure(figsize=(10, 15))\n    plt.title('TOPIC ' + str(t) + ' WORDCLOUD')\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()","79bb2860":"unseen_document = article_4_clean\n\nprint(lda_output.transform(vect.transform([unseen_document])))","a137cc9e":"### The Topic Modelling Way\n\n1) Mash all articles together into one 'corpus'.\n\n2) Put corpus through the 'LDA' model to determine how many topics are present.\n\n3) Label each topic logically.\n\n4) Model can then be used to output topic weights on new articles it hasn't seen.\n","8d38c053":"^ so above it looks like:\n* the first topic is winter driving\n* the second topic is about policing\n* the thrid topic is about the auto industry","f337b68a":"### The Keyword Overlap or 'Bag of Words' Method\n\nThis method requires someone to label all articles.","03c47860":"^ Boom, that's what you'd expect. Article 3 is about Winter Driving and so it our 4th test article.","0e0d0677":"^ If we put test article 4 through this model it gives topic 1 (index 0), which we have labelled as 'winter driving', a weight of 0.78.","1bd18169":"## HTF does Topic Modelling Work?\n\n[WIKIPEDIA](https:\/\/en.wikipedia.org\/wiki\/Topic_model) In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: \"dog\" and \"bone\" will appear more often in documents about dogs, \"cat\" and \"meow\" will appear in documents about cats, and \"the\" and \"is\" will appear approximately equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The \"topics\" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is."}}