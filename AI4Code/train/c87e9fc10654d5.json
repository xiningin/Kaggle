{"cell_type":{"aa327b66":"code","9cc078e8":"code","6c591c26":"code","aae86806":"code","ef2484b8":"code","c2afd921":"code","0a165820":"code","4fcaca75":"code","9959a9da":"code","790d22e8":"code","08e3d913":"code","52f8dadc":"code","e0f2fc33":"code","a5fa2232":"code","cb0a9bb6":"code","cf4cb213":"code","197f72cd":"code","0aa1a868":"code","f044f5fe":"code","8c283b81":"code","06b744dc":"code","3819b55a":"code","cefb3f46":"code","e0a223a8":"code","f1298364":"code","410039b9":"code","539266b0":"code","ef638d21":"markdown","627da929":"markdown","2125dcf5":"markdown","3c93b519":"markdown","f0a6c363":"markdown","4ec884d6":"markdown","136d674a":"markdown","53eaec76":"markdown","deabe589":"markdown","2e7a4cff":"markdown","44c8820b":"markdown","ed906bd7":"markdown","313c3935":"markdown","dc002037":"markdown","cec3f8f0":"markdown","28228206":"markdown","4e7b1c79":"markdown","31af862a":"markdown","d8262743":"markdown","138c4483":"markdown","e8681113":"markdown","318c167e":"markdown"},"source":{"aa327b66":"!pip install neural-structured-learning\n!pip install tensorflow-text","9cc078e8":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\n\nimport neural_structured_learning as nsl\n\nimport tensorflow as tf\nimport tensorflow_text as text\nimport tensorflow_hub as tfh\n\ntf.keras.backend.clear_session()","6c591c26":"df = pd.read_csv('\/kaggle\/input\/disaster-tweets-cleaned\/df.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/disaster-tweets-cleaned\/test_df.csv')\n\nX_tr, X_val, y_tr, y_val = train_test_split(\n    df['ctext'].values, df['target'].values,\n    test_size = 0.15,\n    shuffle = True, stratify = df['target']\n)\nX_test = test_df['ctext'].values\ny_test = test_df['target'].values\ny_tr = np.reshape(y_tr, (-1, 1))\ny_val = np.reshape(y_val, (-1, 1))\ny_test = np.reshape(y_test, (-1, 1))\nprint(X_tr.shape, y_tr.shape)\nprint(X_val.shape, y_val.shape)\nprint(X_test.shape, y_test.shape)","aae86806":"preprocessor = tfh.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3\")\nembedding_handler = 'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-128_A-2\/2'\nembedding_layer = tfh.KerasLayer(embedding_handler, trainable = True, name = 'embedder')","ef2484b8":"def make_model(seq_length = 40 ):\n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n    encoder_inputs = preprocessor(text_input)\n    x = embedding_layer(encoder_inputs,)\n    x = x['pooled_output']\n    output = tf.keras.layers.Dense(1, activation = 'sigmoid')(x)\n    model = tf.keras.Model(text_input, output)\n    model.compile(loss = 'binary_crossentropy', \n                  optimizer = tf.keras.optimizers.Adam(1e-4), \n                  metrics = ['acc'])\n    return model\n\nmodel = make_model()\nmodel.fit(x = X_tr, y = y_tr, epochs = 3, validation_data = (X_val, y_val))","c2afd921":"text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\nencoder_inputs = preprocessor(text_input)\nembedd = model.get_layer('embedder')(encoder_inputs)\nembedder = tf.keras.Model(text_input, embedd)","0a165820":"embed_X_tr = embedder(X_tr)['pooled_output'].numpy()\nembed_X_val = embedder(X_val)['pooled_output'].numpy()\nembed_X_test = embedder(X_test)['pooled_output'].numpy()\n\nnp.save('X_tr', embed_X_tr)\nnp.save('X_val', embed_X_val)\nnp.save('X_test', embed_X_test)","4fcaca75":"embed_X_tr = np.load('X_tr.npy')\nembed_X_val = np.load('X_val.npy')\nembed_X_test = np.load('X_test.npy')","9959a9da":"def _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value = value.tolist()))\ndef _bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value = [value.encode('utf-8')]))\ndef _float_feature(value):\n    return tf.train.Feature(float_list=tf.train.FloatList(value = value.tolist()))","790d22e8":"def create_embedding_example(sent_embed, record_id, layer_id = 'pooled_output'):\n    features = {\n        'id': _bytes_feature(str(record_id)),\n        'embedding': _float_feature(sent_embed)\n    }\n    return tf.train.Example(features = tf.train.Features(feature = features))\n\ndef create_embeddings(sent_embeds, output_path, strarting_id = 0):\n    record_id = int(strarting_id)\n    with tf.io.TFRecordWriter(output_path) as writer:\n        for sent_embed in sent_embeds:\n            example = create_embedding_example(sent_embed, record_id)\n            record_id += 1\n            writer.write(example.SerializeToString())\n    return record_id","08e3d913":"!mkdir -p tmp\/tweets\ncreate_embeddings(embed_X_tr, 'tmp\/tweets\/embeddings.tfr', 0)","52f8dadc":"graph_builder_config = nsl.configs.GraphBuilderConfig(similarity_threshold=0.97, \n                                                      lsh_splits=0, \n                                                      lsh_rounds=0, \n                                                      random_seed=12345)\nnsl.tools.build_graph_from_config(['tmp\/tweets\/embeddings.tfr'],\n                                  'tmp\/tweets\/graph.tsv',\n                                  graph_builder_config)","e0f2fc33":"!wc -l tmp\/tweets\/graph.tsv","a5fa2232":"graph = pd.read_csv('tmp\/tweets\/graph.tsv', \n                    delimiter = '\\t', \n                    header = None,\n                   names = ['node1', 'node2', 'weight'])\ngraph.head()","cb0a9bb6":"id1 = graph.node1.values[0]\ng_tmp = graph[graph['node1'] == id1]\ng_tmp.head()","cf4cb213":"print('Node1:', id1)\nprint(X_tr[id1])\nprint()\nprint('Neighboors:')\nfor txt in g_tmp.node2.values:\n    print(X_tr[txt])","197f72cd":"def create_example(record, label, record_id):\n    features={\n        'id': _bytes_feature(str(record_id)),\n        'label': _int64_feature(label),\n        'embed': _float_feature(record),\n    }\n    return tf.train.Example(features = tf.train.Features(feature=features))\n\ndef create_records(sent_embeds, labels, record_path, start_id = 0):\n    record_id = int(start_id)\n    with tf.io.TFRecordWriter(record_path) as writer:\n        for sent_embed, label in zip(sent_embeds, labels):\n            example = create_example(sent_embed, label, record_id)\n            record_id += 1\n            writer.write(example.SerializeToString())\n    return record_id","0aa1a868":"next_record_id = create_records(embed_X_tr, y_tr, 'tmp\/tweets\/train.tfr', 0)\nnext_record_id","f044f5fe":"max_nbrs = 4\nnsl.tools.pack_nbrs(\n    'tmp\/tweets\/train.tfr',\n    '',\n    'tmp\/tweets\/graph.tsv',\n    'tmp\/tweets\/nsl_train.tfr',\n    add_undirected_edges=True,\n    max_nbrs=max_nbrs)","8c283b81":"NBR_FEATURE_PREFIX = 'NL_nbr_'\nNBR_WEIGHT_SUFFIX = '_weight'\ndef parse_example(val):\n    def pad_sequence(sequence, max_seq_length):\n        pad_size = tf.maximum([0], max_seq_length - tf.shape(sequence)[0])\n        padded = tf.concat(\n            [sequence.values,\n             tf.fill((pad_size), tf.cast(0, sequence.dtype))],\n            axis=0)\n        return tf.slice(padded, [0], [max_seq_length])\n    \n    feature_spec = {\n        'embed': tf.io.VarLenFeature(dtype = tf.float32),\n        'label': tf.io.FixedLenFeature((), tf.int64, default_value=-1),\n    }\n    for i in range(max_nbrs):\n        nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'embed')\n        nbr_weight_key = '{}{}{}'.format(NBR_FEATURE_PREFIX, i,\n                                         NBR_WEIGHT_SUFFIX)\n        feature_spec[nbr_feature_key] = tf.io.VarLenFeature(dtype = tf.float32)\n        feature_spec[nbr_weight_key] = tf.io.FixedLenFeature(\n            [1], tf.float32, default_value=tf.constant([0.0]))\n\n    features = tf.io.parse_single_example(val, feature_spec)\n    features['embed'] = pad_sequence(features['embed'], 128)\n    for i in range(max_nbrs):\n        nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'embed')\n        features[nbr_feature_key] = pad_sequence(features[nbr_feature_key],\n                                                 128)\n\n    return features, features.pop('label')","06b744dc":"tmp_ds = tf.data.TFRecordDataset(['tmp\/tweets\/nsl_train.tfr'])\ntmp_ds = tmp_ds.map(parse_example)\nfor t, l in tmp_ds.take(1):\n    for n in t.keys():\n        print(n)","3819b55a":"train_ds = tf.data.TFRecordDataset(['tmp\/tweets\/nsl_train.tfr'])\ntrain_ds = train_ds.map(parse_example)\ntrain_ds = train_ds.batch(256)\n\nval_ds = tf.data.Dataset.from_tensor_slices((embed_X_val, y_val))\nval_ds = val_ds.batch(256)","cefb3f46":"def make_feed_forward_model():\n    inputs = tf.keras.Input(shape=(128,), dtype='float32', name='embed')\n    dense_layer = tf.keras.layers.Dense(128, activation='relu')(inputs)\n    dense_layer = tf.keras.layers.Dense(32, activation='relu')(dense_layer)\n    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dense_layer)\n    return tf.keras.Model(inputs=inputs, outputs=outputs)\n\nmodel = make_feed_forward_model()\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['acc'])\nmodel.fit(embed_X_tr, y_tr, validation_data = (embed_X_val, y_val), epochs = 15, verbose = 1)","e0a223a8":"results = model.evaluate(embed_X_test, y_test)\nprint(results)","f1298364":"base_model = make_feed_forward_model()\ngraph_reg_config = nsl.configs.make_graph_reg_config(\n    max_neighbors=2,\n    multiplier=10,\n    distance_type=nsl.configs.DistanceType.L1,\n    sum_over_axis=-1)\ngraph_reg_model = nsl.keras.GraphRegularization(base_model,\n                                                graph_reg_config)","410039b9":"graph_reg_model.compile(\n    optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\ngraph_reg_model.fit(train_ds, validation_data = val_ds, epochs = 15, verbose = 1)","539266b0":"results = graph_reg_model.evaluate(embed_X_test, y_test)\nprint(results)","ef638d21":"# Transform data to TFRecord","627da929":"# Read augmented data","2125dcf5":"# Get FeatureExtractor from finetuned Bert","3c93b519":"# BertTokenizer\/BertLayer","f0a6c363":"# Neural Graph Learning","4ec884d6":"# Read data\/shuffle\/split","136d674a":"# Create regularized model","53eaec76":"# Description\nIn this notebook I will demonstrate how to use [**Neural Graph Learning**](https:\/\/storage.googleapis.com\/pub-tools-public-publication-data\/pdf\/bbd774a3c6f13f05bf754e09aa45e7aa6faa08a8.pdf) to improve your model accuracy. The Idea of this method is to leverage prior knowledge about how your data is interconnected to make the training more reboust and accurate.\n\nIn other words, If you have a graph where each node is an example from your data and the edges between two nodes represent how similar those two nodes, then you can use this graph to make your model predicts similar label to examples with high weighted edge.\nTo understand this idea have a look at this objective function:\n\n$\\mathcal{L}(\\theta)=\\alpha_1\\sum_{(u, v)\\in V}{w_{uv}d(h_\\theta(x_u), h_\\theta(x_v))}+\\alpha_2\\sum_{i}^Ncost(g_\\theta(x_i), y_i)$\n\n*  V: labeled nodes in the graph\n*  $d:$ distance function\n*  $w_{uv}$: weight of the edge between u and v\n*  $h_\\theta:$ activation of some hidden layer in the model\n*  $g_\\theta:$ activation of the last layer of your model\n*  $cost:$: crossentopy, MSE, ...\n\nThe first term in previous equation acts as a regularization term. When the weight between u and v is high (similar examples) the model will minimize the distance between u and v or it will be punched. In other words the previous objective encourges the model to assign similar labels to similar examples. However this method generalizes to semi-supervised learning. We can incorporte unlabel nodes in the above equation by adding their distances to the objective function. More details ([**Neural Graph Learning**](https:\/\/storage.googleapis.com\/pub-tools-public-publication-data\/pdf\/bbd774a3c6f13f05bf754e09aa45e7aa6faa08a8.pdf))\n\n## How can we build a graph from our data?\nTensorflow provides a framework called **neural-structured-learning** that allows us to build structured graphs from our data. But we need to transform the data to a format that is acceptable by this framework.\n\n*  Create an ID and embeddings for each textual example in our dataset(Labeled and unlabeled)\n*  Transform each example to TFRecord format\n*  Serialize the transfomed data to the hard\n*  Use **neural-structured-learning** to generate a graph of our data based on (Cosine similarity)\n*  Transform training data to TFRecord fomate and serialize it to the hard\n*  Use **neural-structured-learning** to augment your training data\n\nAt the end of this stage, we will get a dataset where each record is a dictionary consists of:\n*  Orginal training example\n*  N Nieghboors of the example\n*  weight of each neighboor\n\n## How to build the regularized model?\n**neural-structured-learning** provides functionalities to wrap any base model and compute all necessary terms.\nThe framework compute pairwise distance function between each example in our data and its neighboors then add add the results to the cost function defined by the base model.\n## After using this method, you will see that the accuracy is increased by: ~1.15%","deabe589":"# Transform each example in the data to Node with id","2e7a4cff":"*  **lsh_splits:** locality sensitive hashing (in this case 0 because the size of the data is small. No need to group the data into smaller buckets)\n*  **lsh_rounds:** locality senitive hashing running times(in this case 0 because we are not bucketizing the data)","44c8820b":"# Create Base Model","ed906bd7":"# Import libs","313c3935":"# Create embeddings for the data","dc002037":"# Build Graph from the Data","cec3f8f0":"# Associalte neighboors to each Node","28228206":"## Base model evaluation","4e7b1c79":"# Create Batched data pipeline","31af862a":"*  layers: 4\n*  hidden state size: 128\n*  Attention heads: 2","d8262743":"# Fine Tune Bert","138c4483":"# Install libs","e8681113":"## Regularized Model evaluation","318c167e":"## Explore the Graph"}}