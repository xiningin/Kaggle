{"cell_type":{"2246c219":"code","ae83c008":"code","9e69c4f4":"code","9e231cd3":"code","fa05db95":"code","8e3dfadf":"code","6c143aaa":"code","ee8b28e8":"code","aabe920e":"code","abbb7267":"code","144e8503":"code","ad64289c":"code","a2fd5b15":"code","67d5cff9":"code","165c6117":"code","a8200d26":"code","fdac464c":"code","f3cd4707":"code","447cc043":"code","39a2f72b":"code","4abec8ca":"code","ac02e20a":"code","c59345f7":"code","0d93c8d2":"code","aca451db":"code","6f05f410":"code","2d04a8b4":"code","0f86606f":"code","e789f93d":"code","91e1e8c4":"code","5a3062ea":"code","a01fd488":"code","f22b13de":"code","26b92cb5":"code","d385130d":"markdown","e7b872c1":"markdown","f8dcee71":"markdown","3a8732de":"markdown"},"source":{"2246c219":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport os\nimport random\nimport pickle\nimport time\n\nfrom tensorflow import keras as K\nfrom tensorflow.keras import layers as L\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\n\nimport matplotlib.pyplot as plt","ae83c008":"def seed_all(seed = 20):\n    os.environ['PYTHONHASHSEED']=str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    # # 5. For layers that introduce randomness like dropout, make sure to set seed values:\n    # model.add(Dropout(0.25, seed=seed_value))\n    # #6 Configure a new global `tensorflow` session: \n    # from keras import backend as K \n    # session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1) \n    # sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n    # K.set_session(sess)\n    \nseed_all(20)","9e69c4f4":"#Import raw data and copy\n# train = pd.read_csv('\/kaggle\/input\/osic-pulmonary-fibrosis-progression\/train.csv')\n# test = pd.read_csv('\/kaggle\/input\/osic-pulmonary-fibrosis-progression\/test.csv')\n\n# size_image = pd.read_csv('\/kaggle\/input\/prep-data\/size_image.csv')\n# list_files = pd.read_csv('\/kaggle\/input\/prep-data\/list_files.csv')\n\ntrain = pd.read_csv('\/kaggle\/input\/data-preparation-for-osic\/train.csv')\n\nraw_test = pd.read_csv('\/kaggle\/input\/osic-pulmonary-fibrosis-progression\/test.csv')\nX_prediction = pd.read_csv('\/kaggle\/input\/osic-pulmonary-fibrosis-progression\/sample_submission.csv')","9e231cd3":"ID='Patient_Week'\nPINBALL_QUANTILE = [0.255, 0.50, 0.745]\nLAMBDA_LOSS = 0.585\nEPOCH = [54, 55, 20, 60, 23]\nBATCH_SIZE = 128\n\nNFOLD = 5","fa05db95":"C1, C2 = tf.constant(70, dtype='float32'), tf.constant(1000, dtype=\"float32\")\n#=============================#\ndef score(y_true, y_pred):\n#     y_true=tf.dtypes.cast(y_true, tf.float32)\n#     y_pred=tf.dtypes.cast(y_pred, tf.float32)\n    sigma = y_pred[:, 2] - y_pred[:, 0]\n    fvc_pred = y_pred[:, 1]\n    \n    #sigma_clip = sigma + C1\n    sigma_clip = tf.maximum(sigma, C1)\n    delta = tf.abs(y_true[:, 0] - fvc_pred)\n    delta = tf.minimum(delta, C2)\n    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype=tf.float32) )\n    metric = (delta \/ sigma_clip)*sq2 + tf.math.log(sigma_clip* sq2)\n    return K.backend.mean(metric)\n#============================#\ndef qloss(y_true, y_pred):\n    # Pinball loss for multiple quantiles\n    qs = PINBALL_QUANTILE\n    q = tf.constant(np.array([qs]), dtype=tf.float32)\n    e = y_true - y_pred\n    v = tf.maximum(q*e, (q-1)*e)\n    return K.backend.mean(v)\n#=============================#\ndef mloss(_lambda):\n    def loss(y_true, y_pred):\n        return _lambda * qloss(y_true, y_pred) + (1 - _lambda)*score(y_true, y_pred)\n    return loss","8e3dfadf":"def eval_score(y_true, y_pred):\n    y_true = tf.dtypes.cast(y_true, tf.float32)*(data_prep.fvc_max-data_prep.fvc_min)+data_prep.fvc_min\n    y_pred = tf.dtypes.cast(y_pred, tf.float32)*(data_prep.fvc_max-data_prep.fvc_min)+data_prep.fvc_min\n    sigma = y_pred[:, 2] - y_pred[:, 0]\n    fvc_pred = y_pred[:, 1]\n    \n    #sigma_clip = sigma + C1\n    sigma_clip = tf.maximum(sigma, C1)\n    delta = tf.abs(y_true - fvc_pred)\n    delta = tf.minimum(delta, C2)\n    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype=tf.float32) )\n    metric = (delta \/ sigma_clip)*sq2 + tf.math.log(sigma_clip* sq2)\n    return -K.backend.mean(metric)","6c143aaa":"SELECTED_COLUMNS = ['Weeks', 'Percent', 'Age', 'Sex', 'Min_week', 'Base_FVC','Base_week', '_Currently smokes', '_Ex-smoker', '_Never smoked']\ndef create_model(lambda_loss):\n    model_input = K.Input(shape=(len(SELECTED_COLUMNS),))\n    x = L.Dense(500, activation=\"selu\", name='dense_to_freeze1')(model_input)\n    x = L.Dense(100, activation=\"selu\", name='dense_to_freeze2')(x)\n#     FVC = L.Dense(3)(x)\n    p1 = L.Dense(3, activation=\"linear\", name=\"p1\")(x)\n    p2 = L.Dense(3, activation=\"selu\", name=\"p2\")(x)\n    FVC = L.Lambda(lambda x: x[0] + tf.cumsum(x[1], axis=1), \n                         name=\"FVC\")([p1, p2])\n\n    model = K.Model(\n        inputs=model_input,\n        outputs=[FVC],\n    )\n#     boundaries = [150, 250, 350]\n#     values = [0.1, 0.08, 0.01, 0.001]\n#     learning_rate_fn = K.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\n#     optimizer=K.optimizers.Adam(learning_rate=learning_rate_fn, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False)\n    model.compile(\n        optimizer=K.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False),\n        loss=mloss(lambda_loss),\n        metrics=score,\n    )\n    return model\n\nmodel = create_model(LAMBDA_LOSS)\n# tf.keras.utils.plot_model(model)\nmodel.summary()","ee8b28e8":"X_prediction['Patient'] = X_prediction['Patient_Week'].str.extract(r'(.*)_.*')\nX_prediction['Weeks'] = X_prediction['Patient_Week'].str.extract(r'.*_(.*)').astype(int)\nX_prediction = X_prediction[['Patient', 'Weeks', 'Patient_Week']]\nrename_cols = {'Weeks_y':'Min_week', 'Weeks_x': 'Weeks', 'FVC':'Base_FVC'}\nX_prediction = X_prediction.merge(raw_test, how='left', left_on='Patient', right_on='Patient').rename(columns=rename_cols)[['Patient', 'Min_week', 'Base_FVC', 'Percent', 'Age', 'Sex', 'SmokingStatus', 'Weeks', 'Patient_Week']].reset_index(drop=True)","aabe920e":"X_prediction['Base_week'] = X_prediction['Weeks'] - X_prediction['Min_week']","abbb7267":"# rename_cols = {'Weeks_x':'Base_week', 'FVC_x': 'Base_FVC', 'Percent_x': 'Base_percent', 'Age_x': 'Age', 'SmokingStatus_x': 'SmokingStatus', 'Sex_x':'Sex', 'Weeks_y':'Weeks', 'FVC_y': 'FVC'}\n# drop_cols = ['Age_y', 'Sex_y', 'SmokingStatus_y', 'Percent_y']\n# test = test.merge(test, how='left', left_on='Patient', right_on='Patient').rename(columns=rename_cols).drop(columns=drop_cols)\n# test[ID] = test['Patient'].astype(str) + '_' + test['Weeks'].astype(str)\n# test = test[['Patient', 'Base_week', 'Base_FVC', 'Base_percent', 'Age', 'Sex', 'SmokingStatus', 'Weeks', 'Patient_Week', 'FVC']].reset_index(drop=True)","144e8503":"from sklearn.preprocessing import OneHotEncoder as SklearnOneHotEncoder\n\n#Override OneHotEncoder to have the column names created automatically (Ex-smoker, Never Smoked...) \nclass OneHotEncoder(SklearnOneHotEncoder):\n    def __init__(self, **kwargs):\n        super(OneHotEncoder, self).__init__(**kwargs)\n        self.fit_flag = False\n\n    def fit(self, X, **kwargs):\n        out = super().fit(X)\n        self.fit_flag = True\n        return out\n\n    def transform(self, X, categories, index='', name='', **kwargs):\n        sparse_matrix = super(OneHotEncoder, self).transform(X)\n        new_columns = self.get_new_columns(X=X, name=name, categories=categories)\n        d_out = pd.DataFrame(sparse_matrix.toarray(), columns=new_columns, index=index)\n        return d_out\n\n    def fit_transform(self, X, categories, index, name, **kwargs):\n        self.fit(X)\n        return self.transform(X, categories=categories, index=index, name=name)\n\n    def get_new_columns(self, X, name, categories):\n        new_columns = []\n        for j in range(len(categories)):\n            new_columns.append('{}_{}'.format(name, categories[j]))\n        return new_columns\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.exceptions import NotFittedError\n\ndef standardisation(x, u, s):\n    return (x-u)\/s\n\ndef normalization(x, ma, mi):\n    return (x-mi)\/(ma-mi)\n\nclass data_preparation():\n    def __init__(self, bool_normalization=True,bool_standard=False):\n        self.enc_sex = LabelEncoder()\n        self.enc_smok = LabelEncoder()\n        self.onehotenc_smok = OneHotEncoder()\n        self.standardisation = bool_standard\n        self.normalization = bool_normalization\n        \n        \n    def __call__(self, data_untransformed):\n        data = data_untransformed.copy(deep=True)\n        \n        #For the test set\/Already fitted\n        try:\n            data['Sex'] = self.enc_sex.transform(data['Sex'].values)\n            data['SmokingStatus'] = self.enc_smok.transform(data['SmokingStatus'].values)\n            data = pd.concat([data.drop(columns=['SmokingStatus']), self.onehotenc_smok.transform(data['SmokingStatus'].values.reshape(-1,1), categories=self.enc_smok.classes_, name='', index=data.index).astype(int)], axis=1)\n            \n            #Standardisation\n            if self.standardisation:\n                data['Base_week'] = standardisation(data['Base_week'],self.base_week_mean,self.base_week_std)\n                data['Base_FVC'] = standardisation(data['Base_FVC'],self.base_fvc_mean,self.base_fvc_std)\n                data['Base_percent'] = standardisation(data['Base_percent'],self.base_percent_mean,self.base_percent_std)\n                data['Age'] = standardisation(data['Age'],self.age_mean,self.age_std)\n                data['Weeks'] = standardisation(data['Weeks'],self.weeks_mean,self.weeks_std)\n            \n            #Normalization\n            if self.normalization:\n                data['Base_week'] = normalization(data['Base_week'],self.base_week_max,self.base_week_min)\n                data['Base_FVC'] = normalization(data['Base_FVC'],self.base_fvc_max,self.base_fvc_min)\n                data['Percent'] = normalization(data['Percent'],self.base_percent_max,self.base_percent_min)\n                data['Age'] = normalization(data['Age'],self.age_max,self.age_min)\n                data['Weeks'] = normalization(data['Weeks'],self.weeks_max,self.weeks_min)\n                data['Min_week'] = normalization(data['Min_week'],self.base_week_max,self.base_week_min)\n\n        #For the train set\/Not yet fitted    \n        except NotFittedError:\n            data['Sex'] = self.enc_sex.fit_transform(data['Sex'].values)\n            data['SmokingStatus'] = self.enc_smok.fit_transform(data['SmokingStatus'].values)\n            data = pd.concat([data.drop(columns=['SmokingStatus']), self.onehotenc_smok.fit_transform(data['SmokingStatus'].values.reshape(-1,1), categories=self.enc_smok.classes_, name='', index=data.index).astype(int)], axis=1)\n            \n            #Standardisation\n            if self.standardisation:\n                self.base_week_mean = data['Base_week'].mean()\n                self.base_week_std = data['Base_week'].std()\n                data['Base_week'] = standardisation(data['Base_week'],self.base_week_mean,self.base_week_std)\n\n                self.base_fvc_mean = data['Base_FVC'].mean()\n                self.base_fvc_std = data['Base_FVC'].std()\n                data['Base_FVC'] = standardisation(data['Base_FVC'],self.base_fvc_mean,self.base_fvc_std)\n\n                self.base_percent_mean = data['Base_percent'].mean()\n                self.base_percent_std = data['Base_percent'].std()\n                data['Base_percent'] = standardisation(data['Base_percent'],self.base_percent_mean,self.base_percent_std)\n\n                self.age_mean = data['Age'].mean()\n                self.age_std = data['Age'].std()\n                data['Age'] = standardisation(data['Age'],self.age_mean,self.age_std)\n\n                self.weeks_mean = data['Weeks'].mean()\n                self.weeks_std = data['Weeks'].std()\n                data['Weeks'] = standardisation(data['Weeks'],self.weeks_mean,self.weeks_std)\n\n                \n            #Normalization\n            if self.normalization:\n                self.base_week_min = data['Base_week'].min()\n                self.base_week_max = data['Base_week'].max()\n                data['Base_week'] = normalization(data['Base_week'],self.base_week_max,self.base_week_min)\n\n                self.base_fvc_min = data['Base_FVC'].min()\n                self.base_fvc_max = data['Base_FVC'].max()\n                data['Base_FVC'] = normalization(data['Base_FVC'],self.base_fvc_max,self.base_fvc_min)\n\n                self.base_percent_min = data['Percent'].min()\n                self.base_percent_max = data['Percent'].max()\n                data['Percent'] = normalization(data['Percent'],self.base_percent_max,self.base_percent_min)\n\n                self.age_min = data['Age'].min()\n                self.age_max = data['Age'].max()\n                data['Age'] = normalization(data['Age'],self.age_max,self.age_min)\n\n                self.weeks_min = data['Weeks'].min()\n                self.weeks_max = data['Weeks'].max()\n                data['Weeks'] = normalization(data['Weeks'],self.weeks_max,self.weeks_min)\n                \n                self.base_week_min = data['Min_week'].min()\n                self.base_week_max = data['Min_week'].max()\n                data['Min_week'] = normalization(data['Min_week'],self.base_week_max,self.base_week_min)\n\n            \n        return data","ad64289c":"pickefile = open('\/kaggle\/input\/data-preparation-for-osic\/data_prep', 'rb')\ndata_prep = pickle.load(pickefile)\npickefile.close()","a2fd5b15":"X_prediction = data_prep(X_prediction)\n# test = train[train['Patient']=='ID00009637202177434476278']\n# train = train[~(train['Patient']=='ID00009637202177434476278')]","67d5cff9":"# list_patient_score=[]\n# for i in train.Patient.unique():\n#     model = create_model(LAMBDA_LOSS)\n#     print(i)\n#     history = model.fit(x=train[~train.Patient.isin([i])][SELECTED_COLUMNS], y=train[~train.Patient.isin([i])][['FVC']], validation_data=(train[train.Patient.isin([i])][SELECTED_COLUMNS], train[train.Patient.isin([i])][['FVC']]), epochs=250)\n#     list_patient_score.append([i, history.history['val_score']])","165c6117":"pickefile = open('\/kaggle\/input\/trained-cnn-mlp-for-osic\/list_patient_score', 'rb')\nlist_patient_score = pickle.load(pickefile)\npickefile.close()","a8200d26":"# pickefile = open('list_patient_score', 'wb')\n# pickle.dump(list_patient_score, pickefile)\n# pickefile.close()","fdac464c":"# list_mean_score=[]\n# for i in list_patient_score:\n#     list_mean_score.append(np.mean(i[1]))","f3cd4707":"# count=0\n# for i in list_patient_score:\n#     if np.mean(i[1])>8:\n#         plt.plot(i[1])\n#         count+=1    \n# print(count)","447cc043":"list_patient_weight = []\nfor i in list_patient_score:\n    if 6.4>np.mean(i[1]):\n        list_patient_weight.append([i[0], 3])\n    elif 6.8>np.mean(i[1])>6.4:\n        list_patient_weight.append([i[0], 50])\n    elif 7.1>np.mean(i[1])>6.8:\n        list_patient_weight.append([i[0], 100])\n    elif 50>np.mean(i[1])>7.6:\n        list_patient_weight.append([i[0], 3])\n    else:\n        list_patient_weight.append([i[0], 3])\n        \ntrain['Weight'] = train.Patient.map(dict(list_patient_weight))","39a2f72b":"pickefile = open('list_patient_weight', 'wb')\npickle.dump(list_patient_weight, pickefile)\npickefile.close()","4abec8ca":"#Selection for KFOLD\nlist_patient_KFOLD=[]\nfor i in list_patient_score:\n    if np.mean(i[1]) < 6.26:\n        list_patient_KFOLD.append([i[0], 0])\n    elif 6.26<=np.mean(i[1]) < 6.43:\n        list_patient_KFOLD.append([i[0], 1])\n    elif 6.43<=np.mean(i[1]) < 6.74:\n        list_patient_KFOLD.append([i[0], 2])\n    elif 6.74<=np.mean(i[1]) < 7.15:\n        list_patient_KFOLD.append([i[0], 3])\n    elif 7.15<=np.mean(i[1]) < 50:\n        list_patient_KFOLD.append([i[0], 4])","ac02e20a":"# model = create_model(LAMBDA_LOSS)\n# # list_patients = train.Patient.sample(12).to_list()\n# # history = model.fit(x=train[~train.Patient.isin(list_patients)][SELECTED_COLUMNS], y=train[~train.Patient.isin(list_patients)][['FVC']], validation_data=(train[train.Patient.isin(list_patients)][SELECTED_COLUMNS], train[train.Patient.isin(list_patients)][['FVC']]), epochs=EPOCH, sample_weight=train[~train.Patient.isin(list_patients)].Weight, verbose=0)\n# history = model.fit(x=train[SELECTED_COLUMNS], y=train[['FVC']], epochs=EPOCH, sample_weight=train.Weight, verbose=0)\n# model.save('model')\n# # plt.plot(history.history['val_score'])\n# plt.plot(history.history['score'])","c59345f7":"# for i in range(1,6):\n#     model = create_model(LAMBDA_LOSS)\n#     list_patients = train.Patient.unique()[np.random.randint(0, len(train.Patient.unique())-1, size=round(len(train.Patient.unique())\/NFOLD))]\n#     history = model.fit(x=train[~train.Patient.isin(list_patients)][SELECTED_COLUMNS], y=train[~train.Patient.isin(list_patients)][['FVC']], validation_data=(train[train.Patient.isin(list_patients)][SELECTED_COLUMNS], train[train.Patient.isin(list_patients)][['FVC']]), epochs=EPOCH, sample_weight=train[~train.Patient.isin(list_patients)].Weight, verbose=0)\n#     # model.save('model')\n#     plt.figure(figsize=(20,20))\n#     plt.subplot(3,3,i)\n#     plt.plot(history.history['val_score'])\n#     plt.plot(history.history['score'])","0d93c8d2":"# pe = np.zeros((X_prediction.shape[0], 3))\n# pred = np.zeros((train.shape[0], 3))\n# i=0\n# EPOCH = [54, 55, 56, 57, 58]\n# for j in range(NFOLD):\n#     print(f\"FOLD {i}\")\n#     model = create_model(LAMBDA_LOSS)\n#     list_patients = [j[0] for j in list_patient_KFOLD if j[1]==i]\n# #     list_patients = train.Patient.unique()[np.random.randint(0, len(train.Patient.unique())-1, size=round(len(train.Patient.unique())\/NFOLD))]\n#     history = model.fit(x=train[~train.Patient.isin(list_patients)][SELECTED_COLUMNS], y=train[~train.Patient.isin(list_patients)][['FVC']], validation_data=(train[train.Patient.isin(list_patients)][SELECTED_COLUMNS], train[train.Patient.isin(list_patients)][['FVC']]), epochs=EPOCH[j], sample_weight=train[~train.Patient.isin(list_patients)].Weight, verbose=0)\n#     print(\"train\", model.evaluate(train[~train.Patient.isin(list_patients)][SELECTED_COLUMNS], train[~train.Patient.isin(list_patients)][['FVC']], verbose=0, batch_size=BATCH_SIZE))\n#     print(\"val\", model.evaluate(train[train.Patient.isin(list_patients)][SELECTED_COLUMNS], train[train.Patient.isin(list_patients)][['FVC']], verbose=0, batch_size=BATCH_SIZE))\n#     pred[train[train.Patient.isin(list_patients)].index] = model.predict(train[train.Patient.isin(list_patients)][SELECTED_COLUMNS], batch_size=BATCH_SIZE, verbose=0)\n#     pe += model.predict(X_prediction[SELECTED_COLUMNS], batch_size=BATCH_SIZE, verbose=0) \/ NFOLD\n#     model.save('model_' + str(i))","aca451db":"KFOLD_confidence = [0.05, 0.15, 0.2, 0.25, 0.35]","6f05f410":"pe = np.zeros((X_prediction.shape[0], 3))\npred = np.zeros((train.shape[0], 3))\nfor i in range(NFOLD):\n    print(f\"FOLD {i}\")\n    model = create_model(LAMBDA_LOSS)\n    list_patients = [j[0] for j in list_patient_KFOLD if j[1]==i]\n#     list_patients = train.Patient.unique()[np.random.randint(0, len(train.Patient.unique())-1, size=round(len(train.Patient.unique())\/NFOLD))]\n    history = model.fit(x=train[~train.Patient.isin(list_patients)][SELECTED_COLUMNS], y=train[~train.Patient.isin(list_patients)][['FVC']], validation_data=(train[train.Patient.isin(list_patients)][SELECTED_COLUMNS], train[train.Patient.isin(list_patients)][['FVC']]), epochs=EPOCH[i], sample_weight=train[~train.Patient.isin(list_patients)].Weight, verbose=0)\n    print(\"train\", model.evaluate(train[~train.Patient.isin(list_patients)][SELECTED_COLUMNS], train[~train.Patient.isin(list_patients)][['FVC']], verbose=0, batch_size=BATCH_SIZE))\n    print(\"val\", model.evaluate(train[train.Patient.isin(list_patients)][SELECTED_COLUMNS], train[train.Patient.isin(list_patients)][['FVC']], verbose=0, batch_size=BATCH_SIZE))\n    pred[train[train.Patient.isin(list_patients)].index] = model.predict(train[train.Patient.isin(list_patients)][SELECTED_COLUMNS], batch_size=BATCH_SIZE, verbose=0)\n    pe += model.predict(X_prediction[SELECTED_COLUMNS], batch_size=BATCH_SIZE, verbose=0)* KFOLD_confidence[i]\n    model.save('model_' + str(i))","2d04a8b4":"# loss_value = [0.58, 0.585, 0.59, 0.595]\n# for loss in loss_value:\n#     print('Loss_value; %f' % loss)\n#     res_train = np.zeros((5,1))\n#     res_val = np.zeros((5,1))\n#     for k in range(3):\n#         for i in range(NFOLD):\n#             model = create_model(LAMBDA_LOSS)\n#             list_patients = [j[0] for j in list_patient_KFOLD if j[1]==i]\n#             history = model.fit(x=train[~train.Patient.isin(list_patients)][SELECTED_COLUMNS], y=train[~train.Patient.isin(list_patients)][['FVC']], validation_data=(train[train.Patient.isin(list_patients)][SELECTED_COLUMNS], train[train.Patient.isin(list_patients)][['FVC']]), epochs=EPOCH[i], sample_weight=train[~train.Patient.isin(list_patients)].Weight, verbose=0)\n#             res_train[i] += model.evaluate(train[~train.Patient.isin(list_patients)][SELECTED_COLUMNS], train[~train.Patient.isin(list_patients)][['FVC']], verbose=0, batch_size=BATCH_SIZE)[1]\/3\n#             res_val[i] += model.evaluate(train[train.Patient.isin(list_patients)][SELECTED_COLUMNS], train[train.Patient.isin(list_patients)][['FVC']], verbose=0, batch_size=BATCH_SIZE)[1]\/3\n#     for p in range(5):\n#         print('Train')\n#         print(res_train[p])\n#         print('Val')\n#         print(res_val[p])\n            \n        ","0f86606f":"# NFOLD = 3\n# kf = KFold(n_splits=NFOLD)\n# pe = np.zeros((X_prediction.shape[0], 3))\n# pred = np.zeros((train.shape[0], 3))\n\n# cnt = 0\n# EPOCHS = 200\n# for tr_idx, val_idx in kf.split(train):\n#     cnt += 1\n#     print(f\"FOLD {cnt}\")\n#     net = create_model(LAMBDA_LOSS)\n#     net.fit(train.loc[tr_idx, SELECTED_COLUMNS], train.loc[tr_idx, 'FVC'], batch_size=BATCH_SIZE, epochs=EPOCHS,\n#             validation_data=(train.loc[val_idx, SELECTED_COLUMNS], train.loc[val_idx, 'FVC']), sample_weight=train.loc[tr_idx, 'Weight'], verbose=0)\n#     print(\"train\", net.evaluate(train.loc[tr_idx, SELECTED_COLUMNS], train.loc[tr_idx, 'FVC'], verbose=0, batch_size=BATCH_SIZE))\n#     print(\"val\", net.evaluate(train.loc[val_idx, SELECTED_COLUMNS], train.loc[val_idx, 'FVC'], verbose=0, batch_size=BATCH_SIZE))\n#     print(\"predict val...\")\n#     pred[val_idx] = net.predict(train.loc[val_idx, SELECTED_COLUMNS], batch_size=BATCH_SIZE, verbose=0)\n#     print(\"predict test...\")\n#     pe += net.predict(X_prediction[SELECTED_COLUMNS], batch_size=BATCH_SIZE, verbose=0) \/ NFOLD\n#     model.save('model_' + str(cnt))","e789f93d":"sigma_opt = mean_absolute_error(train[['FVC']], pred[:, 1])\nunc = pred[:,2] - pred[:, 0]\nsigma_mean = np.mean(unc)\nprint(sigma_opt, sigma_mean)\n\nX_prediction['FVC1'] = 0.996*pe[:, 1]\nX_prediction['Confidence1'] = pe[:, 2] - pe[:, 0]","91e1e8c4":"subm = X_prediction.copy()\nsubm['FVC'] = 3020\nsubm['Confidence'] = 100\n\nsubm.loc[~subm.FVC1.isnull(),'FVC'] = subm.loc[~subm.FVC1.isnull(),'FVC1']\nif sigma_mean<70:\n    subm['Confidence'] = sigma_opt\nelse:\n    subm.loc[~subm.FVC1.isnull(),'Confidence'] = subm.loc[~subm.FVC1.isnull(),'Confidence1']","5a3062ea":"otest = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/test.csv')\nfor i in range(len(otest)):\n    subm.loc[subm['Patient_Week']==otest.Patient[i]+'_'+str(otest.Weeks[i]), 'FVC'] = otest.FVC[i]\n    subm.loc[subm['Patient_Week']==otest.Patient[i]+'_'+str(otest.Weeks[i]), 'Confidence'] = 0.1\nsubm[[\"Patient_Week\",\"FVC\",\"Confidence\"]].to_csv(\"submission.csv\", index=False)","a01fd488":"# SELECTED_COLUMNS = ['Base_week', 'Base_FVC', 'Base_percent', 'Age', 'Sex','Weeks', '_Currently smokes', '_Ex-smoker', '_Never smoked']\n# history = model.fit(x=train[SELECTED_COLUMNS], y=train[['FVC']], validation_data=(test[SELECTED_COLUMNS], test[['FVC']]), epochs=EPOCH)\n# model.save('model')","f22b13de":"# SELECTED_COLUMNS = ['Base_week', 'Base_FVC', 'Base_percent', 'Age', 'Sex','Weeks', '_Currently smokes', '_Ex-smoker', '_Never smoked']\n# history=[]\n# for i in np.linspace(0,1,11):\n#     print('Lambda %f' % i)\n#     model = model = create_model(i)\n#     history.append(model.fit(x=train[SELECTED_COLUMNS], y=train[['FVC']], validation_data=(test[SELECTED_COLUMNS], test[['FVC']]), epochs=200))\n# # model.save('model')","26b92cb5":"# import matplotlib.pyplot as plt\n# plt.figure(figsize=(30,10))\n# for i in range(1,11):\n#     plt.subplot(3,4,i)\n#     plt.plot(history[i].history['score'])\n#     plt.plot(history[i].history['val_score'])","d385130d":"#### Librairies","e7b872c1":"# Model","f8dcee71":"# Data Prep","3a8732de":"#### Set seed"}}