{"cell_type":{"e490118c":"code","c519c1a8":"code","0e2c3483":"code","2d7bd9fd":"code","dbffa8fb":"code","2bc8db91":"code","5dfc5c77":"code","d2af2170":"code","d6be8372":"code","59ad672e":"code","85a4c46f":"code","12bf810c":"code","1aad796b":"code","32c736be":"markdown","60cdc05a":"markdown","1665c4b0":"markdown","a0b6883a":"markdown","250defb0":"markdown","4672a70c":"markdown","0efce32f":"markdown"},"source":{"e490118c":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom datetime import date, datetime, timedelta\nimport seaborn as sns\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import QuantileTransformer,OneHotEncoder, StandardScaler\nfrom sklearn import set_config \n\nimport lightgbm as lgb\n\npd.options.display.max_rows = 50\npd.options.display.float_format = \"{:.4f}\".format\npd.set_option('max_columns', 200)\nplt.style.use('ggplot')","c519c1a8":"train_source = pd.read_parquet('..\/input\/kaggle-pog-series-s01e01\/train.parquet')\ntest_source = pd.read_parquet('..\/input\/kaggle-pog-series-s01e01\/test.parquet')\nss = pd.read_csv('..\/input\/kaggle-pog-series-s01e01\/sample_submission.csv')","0e2c3483":"category_id_map = {\n    1: \"Film & Animation\",\n    2: \"Autos & Vehicles\",\n    10: \"Music\",\n    15: \"Pets & Animals\",\n    17: \"Sports\",\n    19: \"Travel & Events\",\n    20: \"Gaming\",\n    22: \"People & Blogs\",\n    23: \"Comedy\",\n    24: \"Entertainment\",\n    25: \"News & Politics\",\n    26: \"Howto & Style\",\n    27: \"Education\",\n    28: \"Science & Technology\",\n    29: \"Nonprofits & Activism\",\n}\n\ndef addvariables(df):\n    df['publishedAt'] = pd.to_datetime(df['publishedAt'])\n    df['trending_date'] = pd.to_datetime(df['trending_date'], utc=True)\n    \n    df['category_name'] = df['categoryId'].map(category_id_map)\n    \n    average_duration = df.groupby('categoryId')['duration_seconds'].mean().to_dict()\n    df['cat_average_duration'] = df['categoryId'].map(average_duration)\n    df['duration_seconds'] = df['duration_seconds'].fillna(df['cat_average_duration'])\n    \n    # Feature 1 - Age of video\n    df['daystotrend'] = abs(df['trending_date'] - df['publishedAt']) \\\n        .dt.total_seconds().astype('int')\n\n    df['age'] = (pd.to_datetime(date(2021, 12, 31), utc=True)-df['publishedAt']) \\\n        .dt.total_seconds().astype('int')\n    \n    # Trending day of week As a category\n    df['trending_dow'] = df['trending_date'].dt.day_name()\n    df['trending_dow']= df['trending_dow'].astype('category')\n    \n    df['published_dow'] = df['publishedAt'].dt.day_name()\n    df['published_dow']= df['published_dow'].astype('category')\n    \n    df['categoryId'] = df['categoryId'].astype('category')\n    \n    df['channel_occurance'] = df['channelId'].map(\n        df['channelId'].value_counts().to_dict())\n\n    df['channel_unique_video_count'] = df['channelId'].map(\n        df.groupby('channelId')['video_id'].nunique().to_dict())\n    \n    df['video_occurance_count'] = df.groupby('video_id')['trending_date'] \\\n        .rank().astype('int')\n    df['day_of_week'] = df['publishedAt'].dt.day_name()\n    \n    # Try to capture the curvelinear distribution \n    df['duration_seconds2'] = df['duration_seconds']**2\n    df['daystotrend2'] = df['daystotrend']**2\n    \n#     #Transform\n    df['daystotrend_qt'] = QuantileTransformer(n_quantiles=100).fit_transform(df[['daystotrend']])\n    df['duration_seconds_qt'] = QuantileTransformer(n_quantiles=100).fit_transform(df[['duration_seconds']])\n    \n    return df","2d7bd9fd":"train_df = addvariables(train_source)\ntest_df = addvariables(test_source)","dbffa8fb":"features = ['age', \n            'categoryId', \n            'daystotrend', \n            'duration_seconds',\n            'channel_occurance',\n            'channel_unique_video_count',\n            'video_occurance_count',\n            'daystotrend2',\n            'duration_seconds2',\n#       'sent_neg',\n#       'sent_pos',\n#       'sent_neu',\n#       'daystotrend_qt',\n#       'duration_seconds_qt'\n     ]\ntarget = ['target']","2bc8db91":"train_df.describe().transpose()","5dfc5c77":"# %%time\n# set_config(print_changed_only=False) \n# reg = RandomForestRegressor(n_estimators=200, random_state=0, verbose=1, n_jobs=2)\n# # print(reg)\n# reg.fit(train_df[features], train_df['target'])","d2af2170":"# %%time\n# results = pd.DataFrame()\n# finaltestdf = pd.DataFrame()\n# train_df_cats = pd.DataFrame()\n\n# for cat in train_df['categoryId'].unique():\n#     if cat!=29 and cat!=15: \n#         df = train_df.query('categoryId==@cat').copy().reset_index(drop=True)\n#         reg1 = RandomForestRegressor(n_estimators=200, random_state=0, n_jobs=2)\n#         reg1.fit(df[features], df['target'])\n        \n#         testdf = test_df.query('categoryId==@cat').copy().reset_index(drop=True)\n#         testdf['target_m2'] = reg1.predict(testdf[features])\n#         finaltestdf = finaltestdf.append(testdf, ignore_index=True)\n        \n#         traindfcats = train_df.query('categoryId==@cat').copy().reset_index(drop=True)\n#         traindfcats['target_m2'] = reg1.predict(traindfcats[features])\n        \n#         train_df_cats = train_df_cats.append(traindfcats, ignore_index=True)       ","d6be8372":"# train_df_cats['target_m1'] = reg.predict(train_df_cats[features])\n# finaltestdf['target_m1'] = reg.predict(finaltestdf[features])\n# m1score = mean_absolute_error(train_df_cats['target'],train_df_cats['target_m1'])\n# m2score = mean_absolute_error(train_df_cats['target'],train_df_cats['target_m2'])\n# print(f\"Model 1 mae: {m1score}\")\n# print(f\"Model 2 mae: {m2score}\")","59ad672e":"# finaltestdf = finaltestdf.rename(columns={'target_m2':'target'})\n# finaltestdf[['id', 'target']].to_csv('submission.csv', index=False)","85a4c46f":"cfg = {\n    'TARGET' : 'target',\n    'N_FOLDS' : 5,\n    'RANDOM_STATE': 529,\n    'N_ESTIMATORS' : 50_000,\n    'LEARNING_RATE': 0.1\n}","12bf810c":"regs = []\nfis = []\n\nsubmission_df = pd.DataFrame()\n\nfor cat in train_df['categoryId'].unique():\n    if cat!=29 and cat!=15: \n        df = train_df.query('categoryId==@cat').copy().reset_index(drop=True)\n        third = int(df.shape[0]\/3)\n        twothird = third*3\n        X_tr = df[features][:twothird]\n        y_tr = df[target][:twothird]\n        X_val = df[features][third:]\n        y_val = df[target][third:]\n        \n        print(f'===== Running for CategoryId {cat} {twothird} =====')\n        print(X_tr.shape, y_tr.shape, X_val.shape, y_val.shape)\n        \n        reg = lgb.LGBMRegressor(n_estimators=cfg['N_ESTIMATORS'],\n                            learning_rate=cfg['LEARNING_RATE'],\n                            objective='mae',\n                            metric=['mae'],\n                            importance_type='gain'\n                           )\n        # Fit our model\n        reg.fit(X_tr, y_tr,\n            eval_set=(X_val, y_val),\n            early_stopping_rounds=500,\n            verbose=10000,\n           )\n        \n        # Predicting on validation set\n        df['preds'] = reg.predict(df[features],\n                                 num_iteration=reg.best_iteration_)\n\n        # Score validation set\n        fold_score = mean_absolute_error(\n            df['target'],\n                df['preds']\n        )\n\n        # Creating a feature importance dataframe\n        fi = pd.DataFrame(index=reg.feature_name_,\n                     data=reg.feature_importances_,\n                     columns=[f'{cat}_importance'])\n\n        # Predicting on test\n        cat_test = test_df.query('categoryId==@cat').copy().reset_index(drop=True)\n        cat_test[f'pred_{cat}'] = reg.predict(cat_test[features],\n                    num_iteration=reg.best_iteration_)\n        submission_df = submission_df.append(cat_test, ignore_index=True)\n        print(f'Score of this fold is {fold_score:0.6f}')","1aad796b":"pred_cols = [c for c in submission_df.columns if c.startswith('pred_')]\n\nsubmission_df['target'] = submission_df[pred_cols].mean(axis=1)\nsubmission_df[['id','target']] \\\n    .to_csv('submission.csv', index=False)","32c736be":"# Compare Models","60cdc05a":"# Some Descriptive Statistics\n- Check out the [Basic Linear Regression](https:\/\/www.kaggle.com\/impostorengineer\/basic-linear-regression) notebook for more. ","1665c4b0":"# RandomForestRegressor Model\n- See [scikit](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html) for more info, this is my first attempt.","a0b6883a":"# Let's build another one\n- I built a predictor based on categories in [Basic Linear Regression](https:\/\/www.kaggle.com\/impostorengineer\/basic-linear-regression).\n- Can we do the same?","250defb0":"# LGBM","4672a70c":"# Features","0efce32f":"# POG #001\n- Goal is to predict Likes\/View Ratio for trending videos. \n- Tried linear regression in another notebook. \n- Let's try RandomForestRegressor in this notebook."}}