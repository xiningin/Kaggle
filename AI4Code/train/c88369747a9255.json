{"cell_type":{"324e0247":"code","d91cdc03":"code","de738062":"code","9a333b39":"code","e849d4d9":"code","61066ca5":"code","7d2c6d2b":"code","c9f6c537":"code","27132d49":"code","cae23042":"code","8eb13a98":"code","d268ba4e":"code","f5b885ed":"code","60d0b9b2":"code","fc3e8662":"code","b35dd5b4":"code","3f58a91b":"code","83336042":"code","d2c85a74":"code","1e8e380a":"code","a0f6e857":"code","566d3d95":"code","434e3249":"code","72b2b209":"code","40637073":"code","8709abc8":"code","39d02f76":"code","87c4057c":"code","a69e90ad":"code","46625904":"code","e5f35468":"code","92f35359":"code","45e50a92":"code","d6c6c73a":"code","20c64545":"code","cd3d85b3":"code","33961f45":"code","79ffef57":"code","55503615":"code","83db60ac":"code","03cefca8":"code","ab75184c":"markdown","b1a56496":"markdown","cc4f7bce":"markdown","decf3df1":"markdown","5e1434a8":"markdown","92379e29":"markdown","1cd7410c":"markdown","c45920c7":"markdown","ffbd23aa":"markdown","391ad2c9":"markdown","577455d2":"markdown","d47bd5c3":"markdown","d685a15f":"markdown","19e73230":"markdown","015bc071":"markdown","e455ae9b":"markdown","c752894f":"markdown","58c5a07b":"markdown","fae18173":"markdown","8a6e7b95":"markdown","260da96b":"markdown","16d54f60":"markdown","867275ed":"markdown","f8290573":"markdown","f7d632e8":"markdown","bd4806f5":"markdown","d7073bc8":"markdown","1e13cec5":"markdown","4fc69401":"markdown","569de5ab":"markdown","b2189e90":"markdown","8d169ead":"markdown","66acc460":"markdown","9bf38f3d":"markdown","87874de4":"markdown","50f56ec4":"markdown","d3a699ce":"markdown","d18a4286":"markdown","281bc7ea":"markdown","463dcef1":"markdown","5a9b643d":"markdown","aafe0291":"markdown","47b9ad55":"markdown","30aa26a4":"markdown","41156e7b":"markdown","c58316c5":"markdown","df5912f5":"markdown","1ce822f8":"markdown","971894ab":"markdown"},"source":{"324e0247":"import math\nimport numpy as np\n\nimport pandas as pd\npd.set_option('display.max_columns', 60)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom tensorflow.keras.utils import plot_model\n\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn import metrics\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report, accuracy_score\nfrom scipy import stats","d91cdc03":"INT8_MIN = np.iinfo(np.int8).min\nINT8_MAX = np.iinfo(np.int8).max\nINT16_MIN = np.iinfo(np.int16).min\nINT16_MAX = np.iinfo(np.int16).max\nINT32_MIN = np.iinfo(np.int32).min\nINT32_MAX = np.iinfo(np.int32).max\n\nFLOAT16_MIN = np.finfo(np.float16).min\nFLOAT16_MAX = np.finfo(np.float16).max\nFLOAT32_MIN = np.finfo(np.float32).min\nFLOAT32_MAX = np.finfo(np.float32).max\n\n\ndef memory_usage(data, detail = 1):\n    if detail:\n        display(data.memory_usage())\n    memory = data.memory_usage().sum() \/ (1024 * 1024)\n    print(\"Memory usage : {0:.2f}MB\".format(memory))\n    return memory\n\n\ndef compress_dataset(data):\n    memory_before_compress = memory_usage(data, 0)\n    print()\n    print('=' * 50)\n    for col in data.columns:\n        col_dtype = data[col][:100].dtype\n\n        if col_dtype != 'object':\n            print(\"Name: {0:24s} Type: {1}\".format(col, col_dtype))\n            col_series = data[col]\n            col_min = col_series.min()\n            col_max = col_series.max()\n\n            if col_dtype == 'float64':\n                print(\" variable min: {0:15s} max: {1:15s}\".format(str(np.round(col_min, 4)), str(np.round(col_max, 4))))\n                if (col_min > FLOAT16_MIN) and (col_max < FLOAT16_MAX):\n                    data[col] = data[col].astype(np.float16)\n                    print(\"  float16 min: {0:15s} max: {1:15s}\".format(str(FLOAT16_MIN), str(FLOAT16_MAX)))\n                    print(\"compress float64 --> float16\")\n                elif (col_min > FLOAT32_MIN) and (col_max < FLOAT32_MAX):\n                    data[col] = data[col].astype(np.float32)\n                    print(\"  float32 min: {0:15s} max: {1:15s}\".format(str(FLOAT32_MIN), str(FLOAT32_MAX)))\n                    print(\"compress float64 --> float32\")\n                else:\n                    pass\n                memory_after_compress = memory_usage(data, 0)\n                print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) \/ memory_before_compress))\n                print('=' * 50)\n\n            if col_dtype == 'int64':\n                print(\" variable min: {0:15s} max: {1:15s}\".format(str(col_min), str(col_max)))\n                type_flag = 64\n                if (col_min > INT8_MIN \/ 2) and (col_max < INT8_MAX \/ 2):\n                    type_flag = 8\n                    data[col] = data[col].astype(np.int8)\n                    print(\"     int8 min: {0:15s} max: {1:15s}\".format(str(INT8_MIN), str(INT8_MAX)))\n                elif (col_min > INT16_MIN) and (col_max < INT16_MAX):\n                    type_flag = 16\n                    data[col] = data[col].astype(np.int16)\n                    print(\"    int16 min: {0:15s} max: {1:15s}\".format(str(INT16_MIN), str(INT16_MAX)))\n                elif (col_min > INT32_MIN) and (col_max < INT32_MAX):\n                    type_flag = 32\n                    data[col] = data[col].astype(np.int32)\n                    print(\"    int32 min: {0:15s} max: {1:15s}\".format(str(INT32_MIN), str(INT32_MAX)))\n                    type_flag = 1\n                else:\n                    pass\n                memory_after_compress = memory_usage(data, 0)\n                print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) \/ memory_before_compress))\n                if type_flag == 32:\n                    print(\"compress (int64) ==> (int32)\")\n                elif type_flag == 16:\n                    print(\"compress (int64) ==> (int16)\")\n                else:\n                    print(\"compress (int64) ==> (int8)\")\n                print('=' * 50)\n\n    print()\n    memory_after_compress = memory_usage(data, 0)\n    print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) \/ memory_before_compress))\n    \n    return data","de738062":"df_train = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv')","9a333b39":"df_train.head()","e849d4d9":"df_train.drop('Id', axis = 1, inplace = True)","61066ca5":"print(f'Train set shape:   {df_train.shape}')","7d2c6d2b":"df_train.info()","c9f6c537":"df_train.describe()","27132d49":"df_train.drop(['Soil_Type7', 'Soil_Type15'], axis = 1, inplace = True)","cae23042":"df_train.isnull().sum().max() != 0","8eb13a98":"df_train.duplicated().unique()","d268ba4e":"df_test.head()","f5b885ed":"df_test.drop('Id', axis = 1, inplace = True)","60d0b9b2":"print(f'Test set shape:   {df_test.shape}')","fc3e8662":"df_test.info()","b35dd5b4":"df_test.describe()","3f58a91b":"df_test.drop(['Soil_Type7', 'Soil_Type15'], axis = 1, inplace = True)","83336042":"df_test.isnull().sum().max() != 0","d2c85a74":"df_test.duplicated().unique()","1e8e380a":"df_train['Cover_Type'].unique()","a0f6e857":"sns.countplot(x = df_train['Cover_Type'])\nplt.grid()","566d3d95":"df_train['Cover_Type'].value_counts()","434e3249":"df_train.drop(df_train[df_train['Cover_Type'] == 5].index, axis = 0, inplace = True)","72b2b209":"non_binary_columns = list(df_train.columns[:10])\nsns.heatmap(df_train[non_binary_columns].corr())","40637073":"def aspect(x):\n    if x < 0:\n        return x + 360\n    elif (x >= 0) & (x < 360):\n        return x\n    else:\n        return x - 360","8709abc8":"df_train['Aspect'] = df_train['Aspect'].map(aspect)\ndf_test['Aspect'] = df_train['Aspect'].map(aspect)","39d02f76":"def hillshade(x):\n    if x < 0:\n        return 0\n    elif (x >= 0) & (x < 256):\n        return x\n    else:\n        return 255","87c4057c":"df_train['Hillshade_9am'] = df_train['Hillshade_9am'].map(hillshade)\ndf_train['Hillshade_Noon'] = df_train['Hillshade_Noon'].map(hillshade)\ndf_train['Hillshade_3pm'] = df_train['Hillshade_3pm'].map(hillshade)\n\ndf_test['Hillshade_9am'] = df_test['Hillshade_9am'].map(hillshade)\ndf_test['Hillshade_Noon'] = df_test['Hillshade_Noon'].map(hillshade)\ndf_test['Hillshade_3pm'] = df_test['Hillshade_3pm'].map(hillshade)","a69e90ad":"df_train['L1_distance'] = np.abs(df_train['Horizontal_Distance_To_Hydrology']) + np.abs(df_train['Vertical_Distance_To_Hydrology'])\ndf_test['L1_distance'] = np.abs(df_test['Horizontal_Distance_To_Hydrology']) + np.abs(df_test['Vertical_Distance_To_Hydrology'])\n\ndf_train['L2_distance'] = np.sqrt(np.square(df_train['Horizontal_Distance_To_Hydrology']) + np.square(df_train['Vertical_Distance_To_Hydrology']))\ndf_test['L2_distance'] = np.sqrt(np.square(df_test['Horizontal_Distance_To_Hydrology']) + np.square(df_test['Vertical_Distance_To_Hydrology']))","46625904":"soil_type_cols = [col for col in df_train.columns if 'Soil' in col]\n\ndf_train['Soil_Type_sum'] = df_train[soil_type_cols].sum(axis = 1)\ndf_test['Soil_Type_sum'] = df_test[soil_type_cols].sum(axis = 1)\n\nwilderness_area_cols = [col for col in df_train.columns if 'Wilderness' in col]\n\ndf_train['Wilderness_Area_sum'] = df_train[wilderness_area_cols].sum(axis = 1)\ndf_test['Wilderness_Area_sum'] = df_test[wilderness_area_cols].sum(axis = 1)","e5f35468":"scale_cols = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n              'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', \n              'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'L1_distance',\n              'L2_distance', 'Soil_Type_sum', 'Wilderness_Area_sum']\n\nscaler = RobustScaler()\n\ndf_train[scale_cols] = scaler.fit_transform(df_train[scale_cols])\ndf_test[scale_cols] = scaler.transform(df_test[scale_cols])","92f35359":"df_train = compress_dataset(df_train)\ndf_test = compress_dataset(df_test)","45e50a92":"le = LabelEncoder()\ndf_train['Cover_Type'] = le.fit_transform(df_train['Cover_Type'])","d6c6c73a":"feats = [col for col in df_train.columns if 'Cover_Type' not in col]\nX = df_train[feats]\ny = df_train['Cover_Type']","20c64545":"def model_cnn():\n    \n    model = Sequential()\n    model.add(Dense(128, activation = 'relu', kernel_initializer = 'uniform', input_shape = [X.shape[1]]))\n    model.add(Dropout(0.3))\n    model.add(BatchNormalization())\n    model.add(Dense(64, activation = 'relu', kernel_initializer = 'uniform'))\n    model.add(Dropout(0.3))\n    model.add(BatchNormalization())\n    model.add(Dense(32, activation = 'relu', kernel_initializer = 'uniform'))\n    model.add(Dropout(0.3))\n    model.add(BatchNormalization())\n    model.add(Dense(16, activation = 'relu', kernel_initializer = 'uniform'))\n    model.add(Dropout(0.3))\n    model.add(BatchNormalization())\n    model.add(Dense(8, activation = 'relu', kernel_initializer = 'uniform'))\n    model.add(Dropout(0.3))\n    model.add(BatchNormalization())\n    model.add(Dense(6, activation = 'softmax'))\n\n    model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n\n    return model\n\nearly_stop = EarlyStopping(monitor = 'val_accuracy', patience = 10, \n                           verbose = 1, mode = 'max', restore_best_weights = True)\nred_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = 5, verbose = 1)","cd3d85b3":"model_cnn().summary()","33961f45":"model = KerasClassifier(build_fn = model_cnn, epochs = 100, batch_size = 1024, \n                        verbose = 1, callbacks = [early_stop, red_lr])","79ffef57":"plot_model(model_cnn(), show_shapes = True, show_layer_names = True)","55503615":"preds = []\naccuracy = []\n\ncv = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 12)\n\nfor fold, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n    \n    history = model.fit(X_train, y_train, validation_data = (X_test, y_test))\n    y_preds = model.predict(X_test)\n    y_preds = le.inverse_transform(y_preds)\n    y_true = le.inverse_transform(y_test)\n    acc = accuracy_score(y_true, y_preds)\n    accuracy.append(acc)\n    \n    df_test_preds = model.predict(df_test)\n    df_test_preds = le.inverse_transform(df_test_preds)\n    preds.append(df_test_preds)\n    \n    print('*' * 20)\n    print(f'*****    Summary of Fold {fold + 1}    *****')\n    print('*' * 20)\n    print(f'Acuuracy: {acc}')\n    \n    print('*' * 20)\n    print(classification_report(y_true, y_preds, zero_division = 0))\n    \n    print('*' * 20)\n    cm = confusion_matrix(y_true, y_preds)\n    fig, ax = plt.subplots(figsize = (10,10))\n    cmd = ConfusionMatrixDisplay(cm, display_labels = set(y_true))\n    cmd.plot(cmap = plt.cm.Blues, ax = ax)\n    plt.show()\n    \n    print('*' * 20)\n    plt.figure(figsize = (15, 5))\n\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n\n    plt.title(f'Fold {fold + 1} - Model Accuracy', size = 16)\n    plt.xlabel('Epoch')\n    plt.legend(['Train accuracy', 'Test accuracy'], loc = 4)\n    plt.grid()\n    plt.show()\n\n    plt.figure(figsize = (15, 5))\n\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n\n    plt.title(f'Fold {fold + 1} - Model loss', size = 16)\n    plt.xlabel('Epoch')\n    plt.legend(['Train loss', 'Test loss'], loc = 7)\n    plt.grid()\n    plt.show()\n    \nprint('*' * 20)\nprint(f' Mean accuracy: {np.mean(accuracy)}')","83db60ac":"sub = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv')\nsub['Cover_Type'] = stats.mode(preds, axis = 0)[0].T\nsub.head()","03cefca8":"sub.to_csv(f'cnn_{np.mean(accuracy):0.7}.csv', index = False)","ab75184c":"## Correlation","b1a56496":"## Import necessary libraries and datasets","cc4f7bce":"Let's find out something more about data. ","decf3df1":"`Id` column **is redundant**. Let's remove it.","5e1434a8":"Let's build self-normalizing neural network model.","92379e29":"At the end let's check dataset has duplicated rows.","1cd7410c":"Let's see a distribution of each column.","c45920c7":"## Scaler","ffbd23aa":"There is **no correlation between non-binary feature**.","391ad2c9":"## Feature engineering","577455d2":"Datasets are very large and use huge quantity of memory, so we need to convert type of columns to ones using less memory.","d47bd5c3":"We need to modify target by encoding labels in following way:\n* 1 -> 0\n* 2 -> 1\n* 3 -> 2\n* 4 -> 3\n* 6 -> 4\n* 7 -> 5","d685a15f":"In this version of notebook we are verifying the best model indicaded by GridSearchCV, which was built in my other notebook:\nhttps:\/\/www.kaggle.com\/christoforum\/tps-dec-2021-neural-network-with-gridsearchcv ","19e73230":"Let's see what a test set looks like","015bc071":"There are **no missing values** in test dataset.","e455ae9b":"Let's find out something more about data. ","c752894f":"Now let's analyse `Hillshade` columns. Hillshading computes surface illumination as values from 0 to 255 based on a given compass direction to the sun (azimuth) and a certain altitude above the horizon (altitude). All our hillshade's values are between -53 and 301 in train set and between -51 and 296 in test set. Therefore we will replace all negative numbers with 0 and all numbers greater than 255 with 255.","58c5a07b":"## Submission","fae18173":"Let's define our features and target.","8a6e7b95":"Let's see what a train set looks like","260da96b":"## Memory releasing","16d54f60":"We create a new columns based on the `Horizontal_Distance_To_Hydrology` and `Vertical_Distance_To_Hydrology` columns. First column will contain L1 distance and second column will contain L2 distance.","867275ed":"Let's check dataset has some missing values.","f8290573":"Let's check a number of classes in target column.","f7d632e8":"Let's check dataset has some missing values.","bd4806f5":"All columns consist of **integers** and the set is huge - it using a lot of memory. Just like in train set.","d7073bc8":"Our test set has **1 000 000 rows** and **54 columns**.","1e13cec5":"## Train set summary","4fc69401":"All columns consist of **integers** and the set is huge - it using a lot of memory.","569de5ab":"Finally, we create one column with sum of `Soil_Type` columns and the other one with sum of `Wilderness_Area` columns.","b2189e90":"Our train set has **4 000 000 rows** and **55 columns**.","8d169ead":"Let's check how big is our data.","66acc460":"Unfortunatelly, **classes are imbalanced**. Class no. 5 appears only once. We'll remove it.","9bf38f3d":"## CNN","87874de4":"Let's check how big is our data.","50f56ec4":"`Id` column **is redundant**. Let's remove it.","d3a699ce":"We'll carry out exactly the same steps as above.","d18a4286":"We have **7** classes. We need to check that classes are balanced or not.","281bc7ea":"Let's scale and match our datasets. We use RobustScaler algotithm.","463dcef1":"There are **no missing values** in train dataset.","5a9b643d":"Let's look at the `Aspect` column. It is the compass direction that a terrain faces and it is expressed in degrees. Values are contained in range [-33, 407] (train set) and [-33, 400] (test set), so to all values less than 0 we add 360 and to all values greater than or equal to 360 we substract 360.","aafe0291":"Columns `Soil_Type7` and `Soil_Type15` contain only **one value - 0**. They don't introduce a variability, so we can remove them. All remaining columns`Soil_Type` has **two values - 0 and 1**. Just like in train set.","47b9ad55":"Columns `Soil_Type7` and `Soil_Type15` contain only **one value - 0**. They don't introduce a variability, so we can remove them. All remaining columns`Soil_Type` has **two values - 0 and 1**.","30aa26a4":"Let's see a distribution of each column.","41156e7b":"## Test set summary","c58316c5":"There are **no duplicated rows** in test dataset.","df5912f5":"At the end let's check dataset has duplicated rows.","1ce822f8":"There are **no duplicated rows** in train dataset.","971894ab":"## Target summary"}}