{"cell_type":{"b17c5ae6":"code","9479bb2b":"code","1a435860":"code","e98c7e41":"code","c24d970a":"code","7cc17d5b":"code","4bdf8cac":"code","9e66b1d5":"code","d2d07496":"code","373f2f91":"code","91766ae1":"code","c7830ddc":"code","2925e48f":"code","4edf2237":"code","4fee4db2":"code","9ffd6288":"code","eaa58c6b":"code","f4099190":"code","b5580949":"code","4e723d9e":"code","0f70a41a":"code","be4078b4":"code","e8491145":"code","eb194737":"code","68b00fbf":"code","9cb447c8":"code","70f8567a":"code","fb4e03b1":"code","b5181ba8":"code","fa013b8e":"code","4f5eec96":"code","4a716b17":"code","781b483a":"code","9b79fac7":"code","02de55d1":"code","7177ad99":"code","1ccc7604":"code","8deb347a":"code","2912b2c2":"code","6b90d106":"markdown","f397d1d8":"markdown","72a9cab6":"markdown","746e6480":"markdown","147bce4b":"markdown","78f39a50":"markdown","e059029b":"markdown","a66a822b":"markdown","4b53e3aa":"markdown","199cf34f":"markdown","8de30bab":"markdown","4f87bcab":"markdown","ab987a65":"markdown","20d798b9":"markdown","0d5f9b9c":"markdown","f9c958b5":"markdown","2b453c04":"markdown","dbdf44f6":"markdown","c93d4759":"markdown","9e791725":"markdown","da727844":"markdown","0aa0046d":"markdown","9e6aa149":"markdown","bfd6838b":"markdown","ac9840d9":"markdown","7aefdfd2":"markdown","87599a70":"markdown","c9e5907e":"markdown","6b0f2a2f":"markdown","2dbc6415":"markdown"},"source":{"b17c5ae6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9479bb2b":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom keras.preprocessing import text, sequence\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nimport keras\nimport gensim\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,Dropout,Bidirectional,GRU\nimport tensorflow as tf\nfrom keras.callbacks import ReduceLROnPlateau\nfrom sklearn.feature_extraction.text import CountVectorizer","1a435860":"df = pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\",encoding = \"latin1\")\ndf.head()","e98c7e41":"df.isna().sum()","c24d970a":"df = df.rename(columns = {'v1':'category','v2':'text'})\ndf.head()","7cc17d5b":"df.fillna(\"\",inplace = True)\ndf['text'] = df['text'] + ' ' + df['Unnamed: 2'] + ' ' + df['Unnamed: 3'] + ' ' + df['Unnamed: 4']\ndel df['Unnamed: 2']\ndel df['Unnamed: 3']\ndel df['Unnamed: 4']","4bdf8cac":"df.head()","9e66b1d5":"sns.set_style(\"darkgrid\")\nsns.countplot(df.category)","d2d07496":"df.category.replace(\"ham\",0,inplace = True)\ndf.category.replace(\"spam\",1,inplace = True)","373f2f91":"stop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)","91766ae1":"def strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n#Removing the square brackets\ndef remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n# Removing URL's\ndef remove_between_square_brackets(text):\n    return re.sub(r'http\\S+', '', text)\n#Removing the stopwords from text\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            if i.strip().isalpha():\n                final_text.append(i.strip())\n    return \" \".join(final_text)\n#Removing the noisy text\ndef denoise_text(text):\n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    text = remove_stopwords(text)\n    return text\n#Apply function on review column\ndf['text']=df['text'].apply(denoise_text)","c7830ddc":"plt.figure(figsize = (20,20)) # Text that is not Spam\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(df[df.category == 0].text))\nplt.imshow(wc , interpolation = 'bilinear')","2925e48f":"plt.figure(figsize = (20,20)) # Text that is Spam\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(df[df.category == 1].text))\nplt.imshow(wc , interpolation = 'bilinear')","4edf2237":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntext_len=df[df['category']==1]['text'].str.len()\nax1.hist(text_len,color='red')\nax1.set_title('Spam text')\ntext_len=df[df['category']==0]['text'].str.len()\nax2.hist(text_len,color='green')\nax2.set_title('Not Spam text')\nfig.suptitle('Characters in texts')\nplt.show()","4fee4db2":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntext_len=df[df['category']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(text_len,color='red')\nax1.set_title('Spam text')\ntext_len=df[df['category']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(text_len,color='green')\nax2.set_title('Not Spam text')\nfig.suptitle('Words in texts')\nplt.show()","9ffd6288":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))\nword=df[df['category']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('Spam')\nword=df[df['category']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Not Spam')\nfig.suptitle('Average word length in each text')","eaa58c6b":"def get_corpus(text):\n    words = []\n    for i in text:\n        for j in i.split():\n            words.append(j.strip())\n    return words\ncorpus = get_corpus(df.text)\ncorpus[:5]","f4099190":"from collections import Counter\ncounter = Counter(corpus)\nmost_common = counter.most_common(10)\nmost_common = dict(most_common)\nmost_common","b5580949":"sns.barplot(x=list(most_common.values()),y=list(most_common.keys()))","4e723d9e":"def get_top_text_ngrams(corpus, n, g):\n    vec = CountVectorizer(ngram_range=(g, g)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\nplt.figure(figsize = (16,9))\nmost_common_bi = get_top_text_ngrams(df.text,10,2)\nmost_common_bi = dict(most_common_bi)\nsns.barplot(x=list(most_common_bi.values()),y=list(most_common_bi.keys()))","0f70a41a":"plt.figure(figsize = (16,9))\nmost_common_tri = get_top_text_ngrams(df.text,10,3)\nmost_common_tri = dict(most_common_tri)\nsns.barplot(x=list(most_common_tri.values()),y=list(most_common_tri.keys()))","be4078b4":"x_train,x_test,y_train,y_test = train_test_split(df.text,df.category,random_state = 0)","e8491145":"max_features = 4000\nmaxlen = 50","eb194737":"tokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(x_train)\ntokenized_train = tokenizer.texts_to_sequences(x_train)\nx_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)","68b00fbf":"tokenized_test = tokenizer.texts_to_sequences(x_test)\nx_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)","9cb447c8":"EMBEDDING_FILE = '..\/input\/glove-twitter\/glove.twitter.27B.50d.txt'","70f8567a":"def get_coefs(word, *arr): \n    return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))","fb4e03b1":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\n#change below line if computing normal stats is too slow\nembedding_matrix = embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, 50))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","b5181ba8":"batch_size = 64\nepochs = 5\nembed_size = 50","fa013b8e":"#Defining Neural Network\nmodel = Sequential()\n#Non-trainable embeddidng layer\nmodel.add(Embedding(max_features, output_dim=embed_size, weights=[embedding_matrix], input_length = maxlen, trainable=True))\n#LSTM \nmodel.add(Bidirectional(LSTM(units=128, return_sequences = True)))\nmodel.add(Bidirectional(GRU(units=32)))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","4f5eec96":"model.summary()","4a716b17":"history = model.fit(x_train, y_train, batch_size = batch_size , validation_data = (x_test,y_test) , epochs = 10)","781b483a":"print(\"Accuracy of the model on Testing Data is - \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")","9b79fac7":"epochs = [i for i in range(10)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\nax[0].set_title('Training & Testing Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')\nax[1].set_title('Training & Testing Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Loss\")\nplt.show()","02de55d1":"pred = model.predict_classes(x_test)\npred[:5]","7177ad99":"print(classification_report(y_test, pred, target_names = ['Not Spam','Spam']))","1ccc7604":"cm = confusion_matrix(y_test,pred)\ncm","8deb347a":"cm = pd.DataFrame(cm , index = ['Not Spam','Spam'] , columns = ['Not Spam','Spam'])","2912b2c2":"plt.figure(figsize = (10,10))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['Not Spam','Spam'] , yticklabels = ['Not Spam','Spam'])","6b90d106":"# ANALYSIS AFTER TRAINING OF MODEL","f397d1d8":"**Number of characters in texts**","72a9cab6":"**Making a matrix using pre-trained word vectors in GloVe for all the words in the corpus**","746e6480":"**Number of words in each text**","147bce4b":"**WORDCLOUD FOR TEXT THAT IS NOT SPAM**","78f39a50":"**Splitting the data into 2 parts - Training and Testing**","e059029b":"**WORDCLOUD FOR SPAM TEXT**","a66a822b":"**Tokenizing Text -> Repsesenting each word by a number**\n\n**Mapping of orginal word to number is preserved in word_index property of tokenizer**\n\n**Tokenized applies basic processing like changing it to lower case, explicitely setting that as False**\n\n**Lets keep all news to 50, add padding to news with less than 50 words and truncating long ones**","4b53e3aa":"**Unigram Analysis**","199cf34f":"**The distribution of both seems to be completely different.60 to 70 characters in text is the most common in spam category while around 50-70 characters in text are most common in Not Spam \/ Ham category.**","8de30bab":"# PLS UPVOTE THIS NOTEBOOK IF YOU LIKE IT! THANKS FOR YOUR TIME !","4f87bcab":"**Basic Data Cleaning**","ab987a65":"**WHAT ARE STOPWORDS?**\n\n**Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like the, he, have etc. Such words are already captured this in corpus named corpus. We first download it to our python environment.**","20d798b9":"**Some Model Parameters**","0d5f9b9c":"**Replacing Spam category with 1 and ham category with 0**","f9c958b5":"**SO, WE CAN SEE THAT THE DATASET IS HIGHLY IMBALANCED**","2b453c04":"# LOADING THE DATASET","dbdf44f6":"# DATA VISUALIZATION AND PREPROCESSING","c93d4759":"**Bigram Analysis**","9e791725":"**Average word length in a text**","da727844":"**Renaming the columns.**","0aa0046d":"**Combining all the 3 unnamed columns with the text column**","9e6aa149":"# Introduction to GloVe\n**GloVe method is built on an important idea, You can derive semantic relationships between words from the co-occurrence matrix. Given a corpus having V words, the co-occurrence matrix X will be a V x V matrix, where the i th row and j th column of X, X_ij denotes how many times word i has co-occurred with word j. An example co-occurrence matrix might look as follows.**\n![image.png](attachment:image.png)","bfd6838b":"**Nearly all the values of these 3 unnamed columns are empty.**","ac9840d9":"# TRAINING THE MODEL","7aefdfd2":"**Trigram Analysis**","87599a70":"**The co-occurrence matrix for the sentence \u201cthe cat sat on the mat\u201d with a window size of 1. As you probably noticed it is a symmetric matrix. How do we get a metric that measures semantic similarity between words from this? For that, you will need three words at a time. Let me concretely lay down this statement.**\n![image.png](attachment:image.png)\n**The behavior of P_ik\/P_jk for various words Consider the entity P_ik\/P_jk where P_ik = X_ik\/X_i Here P_ik denotes the probability of seeing word i and k together, which is computed by dividing the number of times i and k appeared together (X_ik) by the total number of times word i appeared in the corpus (X_i). You can see that given two words, i.e. ice and steam, if the third word k (also called the \u201cprobe word\u201d), is very similar to ice but irrelevant to steam (e.g. k=solid), P_ik\/P_jk will be very high (>1), is very similar to steam but irrelevant to ice (e.g. k=gas), P_ik\/P_jk will be very small (<1), is related or unrelated to either words, then P_ik\/P_jk will be close to 1 So, if we can find a way to incorporate P_ik\/P_jk to computing word vectors we will be achieving the goal of using global statistics when learning word vectors.**\n\n**Source Credits - https:\/\/towardsdatascience.com\/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010**","c9e5907e":"**Checking for Nan values if any**","6b0f2a2f":"# LOADING THE NECESSARY LIBRARIES","2dbc6415":"# OVERVIEW OF DATASET\n**The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.The files contain one message per line. Each line is composed by two columns: v1 contains the label (ham or spam) and v2 contains the raw text.This corpus has been collected from free or free for research sources from the Internet. Our aim to correctly predict a given piece of text as Spam or Ham.**\n![image.png](attachment:image.png)"}}