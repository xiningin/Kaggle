{"cell_type":{"75bea8b0":"code","1e84815d":"code","aa7a82e9":"code","d5e033ca":"code","4c53747c":"code","faf5258d":"code","1ffbdfa6":"code","9e2eec18":"code","0d09f9e1":"code","6945ed36":"code","d020d5fc":"code","8b1ce78f":"code","f443368d":"code","de88b3cd":"code","45c712c8":"code","67ce8795":"code","1c94bd80":"code","0deff722":"code","7af6cef1":"code","d311f2b9":"code","f727ffed":"code","9642dea3":"code","eae68e4c":"code","7c8cdf01":"code","b95aec27":"code","9bce1bfc":"code","f4c70149":"code","6ffaf93e":"code","e587bf6d":"code","0aba134b":"code","4a43107b":"code","a535b3eb":"code","2b0a0ecd":"code","58a5df2e":"code","e54a3964":"code","5d636788":"code","6796cfb9":"markdown","8d638a2b":"markdown","b00a3d41":"markdown","6e4e9573":"markdown","410d4170":"markdown","132f9804":"markdown","9f51110b":"markdown","0f1a63d2":"markdown","5dc4df0c":"markdown","3aaab255":"markdown","f00ee3d3":"markdown","eb601a96":"markdown","0fb2c0e4":"markdown","bbb512cb":"markdown","c814ca2c":"markdown","df793c05":"markdown","6ba7807f":"markdown"},"source":{"75bea8b0":"# we will be using various libraries like os for taking the input,etc\n# I have used some libraries like seaborn, wordcloud, matplotlib for data visualization so\n# you can skip them if you don't understand \n\nimport os\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport operator\nimport seaborn as sns\nfrom wordcloud import WordCloud,STOPWORDS\n\n# re is used for cleaning the dataset \n\nimport re\n\n\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow import keras\n\n# callbacks are important here as sometimes you get the best accuracy earlies and then it \n# goes down so as to stop the training there you need to use them\n\n\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding,Conv1D,LSTM,GRU,BatchNormalization,Flatten,Dense\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","1e84815d":"df= pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\ndf.head()","aa7a82e9":"df.info()","d5e033ca":"df.isnull().sum()","4c53747c":"sns.countplot(x=df['sentiment'])\nplt.grid()","faf5258d":"sentences=df['review']\nle=LabelEncoder()\ndf['sentiment']= le.fit_transform(df['sentiment'])","1ffbdfa6":"stopwords = set(STOPWORDS) \n\npos=' '.join(map(str,sentences[df['sentiment']==1]))\nneg=' '.join(map(str,sentences[df['sentiment']==0]))\n  \nwordcloud1 = WordCloud(width = 800, height = 800, \n                background_color ='black', \n                stopwords = stopwords, \n                min_font_size = 10).generate(pos) \n\nplt.figure(figsize=(8,8))\nplt.imshow(wordcloud1)\nplt.title('Positive Sentiment')\nplt.axis('off')","9e2eec18":"plt.figure(figsize=(8,8))\nwordcloud2 = WordCloud(width = 800, height = 800, \n                background_color ='black', \n                stopwords = stopwords, \n                min_font_size = 10).generate(neg) \n\nplt.imshow(wordcloud2)\nplt.title('Negative Sentiment')\nplt.axis('off')\n\nplt.show() ","0d09f9e1":"labels=to_categorical(df['sentiment'],num_classes=2)\nX_train,X_test,Y_train,Y_test = train_test_split(df['review'],labels,test_size=0.1,random_state=10)","6945ed36":"glove_embeddings= np.load('..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl',\n                          allow_pickle=True)","d020d5fc":"def vocab_build(review):\n    \n    comments = review.apply(lambda s: s.split()).values\n    vocab={}\n    \n    for comment in comments:\n        for word in comment:\n            try:\n                vocab[word]+=1\n                \n            except KeyError:\n                vocab[word]=1\n    return vocab","8b1ce78f":"def embedding_coverage(review,embeddings):\n    \n    vocab=vocab_build(review)\n    \n    covered={}\n    word_count={}\n    oov={}\n    covered_num=0\n    oov_num=0\n    \n    for word in vocab:\n        try:\n            covered[word]=embeddings[word]\n            covered_num+=vocab[word]\n            word_count[word]=vocab[word]\n        except:\n            oov[word]=vocab[word]\n            oov_num+=oov[word]\n    \n    vocab_coverage=len(covered)\/len(vocab)*100\n    text_coverage = covered_num\/(covered_num+oov_num)*100\n    \n    sorted_oov=sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n    sorted_word_count=sorted(word_count.items(), key=operator.itemgetter(1))[::-1]\n    \n    return sorted_word_count,sorted_oov,vocab_coverage,text_coverage\n        ","f443368d":"train_covered,train_oov,train_vocab_coverage,train_text_coverage=embedding_coverage(X_train,glove_embeddings)\ntest_covered,test_oov, test_vocab_coverage, test_text_coverage = embedding_coverage(X_test,glove_embeddings)\n\nprint(f\"Glove embeddings cover {round(train_vocab_coverage,2)}% of vocabulary and {round(train_text_coverage,2)}% text in training set\")\nprint(f\"Glove embeddings cover {round(test_vocab_coverage,2)}% of vocabulary and {round(test_text_coverage,2)}% text in testing set\")","de88b3cd":"train_oov[:10]","45c712c8":"def clean_sentences(line):\n    \n    line=re.sub('<.*?>','',line) # removing html tags\n    \n    #removing contractions\n    line=re.sub(\"isn't\",'is not',line)\n    line=re.sub(\"he's\",'he is',line)\n    line=re.sub(\"wasn't\",'was not',line)\n    line=re.sub(\"there's\",'there is',line)\n    line=re.sub(\"couldn't\",'could not',line)\n    line=re.sub(\"won't\",'will not',line)\n    line=re.sub(\"they're\",'they are',line)\n    line=re.sub(\"she's\",'she is',line)\n    line=re.sub(\"There's\",'there is',line)\n    line=re.sub(\"wouldn't\",'would not',line)\n    line=re.sub(\"haven't\",'have not',line)\n    line=re.sub(\"That's\",'That is',line)\n    line=re.sub(\"you've\",'you have',line)\n    line=re.sub(\"He's\",'He is',line)\n    line=re.sub(\"what's\",'what is',line)\n    line=re.sub(\"weren't\",'were not',line)\n    line=re.sub(\"we're\",'we are',line)\n    line=re.sub(\"hasn't\",'has not',line)\n    line=re.sub(\"you'd\",'you would',line)\n    line=re.sub(\"shouldn't\",'should not',line)\n    line=re.sub(\"let's\",'let us',line)\n    line=re.sub(\"they've\",'they have',line)\n    line=re.sub(\"You'll\",'You will',line)\n    line=re.sub(\"i'm\",'i am',line)\n    line=re.sub(\"we've\",'we have',line)\n    line=re.sub(\"it's\",'it is',line)\n    line=re.sub(\"don't\",'do not',line)\n    line=re.sub(\"that\u00b4s\",'that is',line)\n    line=re.sub(\"I\u00b4m\",'I am',line)\n    line=re.sub(\"it\u2019s\",'it is',line)\n    line=re.sub(\"she\u00b4s\",'she is',line)\n    line=re.sub(\"he\u2019s'\",'he is',line)\n    line=re.sub('I\u2019m','I am',line)\n    line=re.sub('I\u2019d','I did',line)\n    line=re.sub(\"he\u2019s'\",'he is',line)\n    line=re.sub('there\u2019s','there is',line)\n    \n    #special characters and emojis\n    line=re.sub('\\x91The','The',line)\n    line=re.sub('\\x97','',line)\n    line=re.sub('\\x84The','The',line)\n    line=re.sub('\\uf0b7','',line)\n    line=re.sub('\u00a1\u00a8','',line)\n    line=re.sub('\\x95','',line)\n    line=re.sub('\\x8ei\\x9eek','',line)\n    line=re.sub('\\xad','',line)\n    line=re.sub('\\x84bubble','bubble',line)\n    \n    # remove concated words\n    line=re.sub('trivialBoring','trivial Boring',line)\n    line=re.sub('Justforkix','Just for kix',line)\n    line=re.sub('Nightbeast','Night beast',line)\n    line=re.sub('DEATHTRAP','Death Trap',line)\n    line=re.sub('CitizenX','Citizen X',line)\n    line=re.sub('10Rated','10 Rated',line)\n    line=re.sub('_The','_ The',line)\n    line=re.sub('1Sound','1 Sound',line)\n    line=re.sub('blahblahblahblahblahblahblahblahblahblahblahblahblahblahblahblahblahblah','blah blah',line)\n    line=re.sub('ResidentHazard','Resident Hazard',line)\n    line=re.sub('iameracing','i am racing',line)\n    line=re.sub('BLACKSNAKE','Black Snake',line)\n    line=re.sub('DEATHSTALKER','Death Stalker',line)\n    line=re.sub('_is_','is',line)\n    line=re.sub('10Fans','10 Fans',line)\n    line=re.sub('Yellowcoat','Yellow coat',line)\n    line=re.sub('Spiderbabe','Spider babe',line)\n    line=re.sub('Frightworld','Fright world',line)\n    \n    #removing punctuations\n    \n    punctuations = '@#!~?+&*[]-%._-:\/\u00a3();$=><|{}^' + '''\"\u201c\u00b4\u201d'`'''\n    for p in punctuations:\n        line = line.replace(p, f' {p} ')\n        \n    line=re.sub(',',' , ',line)\n        \n    # ... and ..\n    line = line.replace('...', ' ... ')\n    \n    if '...' not in line:\n        line = line.replace('..', ' ... ')\n        \n    return line\n    ","67ce8795":"X_train=X_train.apply(lambda s: clean_sentences(s))\nX_test=X_test.apply(lambda s: clean_sentences(s))\n\ntrain_covered,train_oov,train_vocab_coverage,train_text_coverage=embedding_coverage(X_train,glove_embeddings)\nprint(f\"Glove embeddings cover {round(train_vocab_coverage,2)}% of vocabulary and {round(train_text_coverage,2)}% text in training set\")\n\ntest_covered,test_oov,test_vocab_coverage,test_text_coverage=embedding_coverage(X_test,glove_embeddings)\nprint(f\"Glove embeddings cover {round(test_vocab_coverage,2)}% of vocabulary and {round(test_text_coverage,2)}% text in training set\")\n","1c94bd80":"punctuations = '@#!~?+&*[]-%._-:\/\u00a3();$=><|{},^' + '''\"\u201c\u00b4\u201d'`'''\ntrain_word=[]\ntrain_count=[]\n\ni=1\nfor word,count in train_covered: \n    if word not in punctuations:\n        train_word.append(word)\n        train_count.append(count)\n        i+=1\n    if(i==15):\n        break","0deff722":"test_word=[]\ntest_count=[]\n\ni=1\nfor word,count in test_covered: \n    if word not in punctuations:\n        test_word.append(word)\n        test_count.append(count)\n        i+=1\n    if(i==15):\n        break","7af6cef1":"plt.figure(figsize=(12,8))\nsns.barplot(x=train_count,y=train_word).set_title('Count of 15 most used word in training set')\nplt.grid()","d311f2b9":"plt.figure(figsize=(12,8))\nsns.barplot(x=test_count,y=test_word).set_title('Count of 15 most used word in testing set')\nplt.grid()","f727ffed":"del glove_embeddings,train_oov,test_oov\ngc.collect()","9642dea3":"num_words=80000\nembeddings=256","eae68e4c":"tokenizer=Tokenizer(num_words=num_words,oov_token='<OOV>')\ntokenizer.fit_on_texts(X_train)\nword_index=tokenizer.word_index\ntotal_vocab=len(word_index)","7c8cdf01":"print(\"Vocabulary of the dataset is : \",total_vocab)","b95aec27":"sequences_train=tokenizer.texts_to_sequences(X_train)\nsequences_test=tokenizer.texts_to_sequences(X_test)\n\nmax_len=max(max([len(x) for x in sequences_train]),max([len(x) for x in sequences_test]))\n\ntrain_padded=pad_sequences(sequences_train,maxlen=max_len)\ntest_padded=pad_sequences(sequences_test,maxlen=max_len)\n","9bce1bfc":"X_train,X_val,Y_train,Y_val=train_test_split(train_padded,Y_train,\n                                             test_size=0.05,random_state=10)","f4c70149":"model= keras.Sequential()\nmodel.add(Embedding(num_words,embeddings,input_length=max_len))\nmodel.add(Conv1D(256,10,activation='relu'))\nmodel.add(keras.layers.Bidirectional(LSTM(128,return_sequences=True)))\nmodel.add(LSTM(64))\nmodel.add(keras.layers.Dropout(0.4))\nmodel.add(Dense(2,activation='softmax'))","6ffaf93e":"model.summary()","e587bf6d":"model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy']\n             )","0aba134b":"es= EarlyStopping(monitor='val_accuracy',\n                  patience=2\n                 )\n\ncheckpoints=ModelCheckpoint(filepath='.\/',\n                            monitor=\"val_accuracy\",\n                            verbose=0,\n                            save_best_only=True\n                           )\n\ncallbacks=[es,checkpoints]","4a43107b":"history=model.fit(X_train,Y_train,validation_data=(X_val,Y_val),epochs=5,callbacks=callbacks)","a535b3eb":"def plot_graph(history,string):\n    \n    plt.plot(history.history[string],label='training '+string)\n    plt.plot(history.history['val_'+string],label='validation '+string)\n    plt.legend()\n    plt.xlabel('epochs')\n    plt.ylabel(string)\n    plt.title(string+' vs epochs')\n    plt.show()","2b0a0ecd":"plot_graph(history,'loss')","58a5df2e":"plot_graph(history,'accuracy')","e54a3964":"model.save('imdb_model.h5')","5d636788":"print(\"Model Performance on test set\")\nresult = model.evaluate(test_padded,Y_test)\nprint(dict(zip(model.metrics_names, result)))","6796cfb9":"<b>Let's find if the data contains any missing value<\/b>","8d638a2b":"# Model Evaluation","b00a3d41":"<b>Data visualization using word cloud for finding the most used words for each type of sentiment<\/b>","6e4e9573":"<h4 style='color:blue'><span style='color:red'>Note: <\/span>In this model I will be using glove embeddings.It has a large vocabulary and we can find the words from our data which are not present in the glove( these words are contractions, misspelled words, concated words or emojis which can decrease our model's performance. We will then use re library to remove these words from the dataset.<\/h4>","410d4170":"**We will delete the embeddings as it takes too much memory**","132f9804":"# Model Building","9f51110b":"<b>We will build vocabulary and count of each vocabulary using the below function<\/b>","0f1a63d2":"<span style='color:green'>After cleaning the dataset we can see that now our vocabulary covers almost 87% on training set and 95.5% on testing set which initially was far less.<\/span>","5dc4df0c":"# Data Cleaning","3aaab255":"# Importing Libraries","f00ee3d3":"**using seaborn's barplot let's find out the count of 10 most used words in training and testing set**","eb601a96":"<b>train_oov shows the words which we need to preprocess<\/b>","0fb2c0e4":"**Callbacks are really helpful as they stop our model when the validation accuracy of our model starts decreasing for consecutive 2 epochs as well save the best possible weights which gives highest validation accuracy**","bbb512cb":"<b>We will find the count of each type of sentiment in the dataset using seaborn library<\/b>","c814ca2c":"# Data Preprocessing","df793c05":"<b>Embedding Coverage tells how much percentage of the words in our data are covered by the vocabulary.<br>\n<i>sorted_oov<\/i> is the list of words which we need to do text cleaning on. <\/b>","6ba7807f":"**We will 2 LSTM layers and Conv1D layer for training the model.<br>\nUsing Dropout reduces the overfitting by decreasing the bias and is a must since there is lot of variance seen.**\n"}}