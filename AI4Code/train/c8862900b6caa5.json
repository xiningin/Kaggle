{"cell_type":{"5f612d44":"code","e384fb12":"code","c5a302cd":"code","003773bd":"code","e3428f68":"code","62dddc3e":"code","7b1c898f":"code","7ecf52e3":"code","cf2e683c":"code","d044a32d":"code","c785c60a":"code","f879a4a6":"code","e6c47467":"code","392d6b5e":"code","a8bb7bb3":"code","43e667e3":"code","4758312c":"code","d011d111":"code","b3f7d569":"code","6be9f173":"code","2d12cb85":"code","c281a727":"code","d58f31b2":"code","0f288465":"code","5a0bb84a":"code","8bdae121":"code","f2c0375a":"code","4e1cea16":"code","12fabb28":"code","02519828":"code","23f63def":"code","8521cbbf":"code","9ff64227":"code","0d1e38ba":"markdown","4cd160e1":"markdown","6b0f358d":"markdown","8644ad66":"markdown","f8c6171f":"markdown","6cb8f013":"markdown","fbbbe380":"markdown","4f696c92":"markdown","beaa50a6":"markdown","fb02d9bd":"markdown","0463d4f5":"markdown","386d3ca9":"markdown","513a23f3":"markdown"},"source":{"5f612d44":"!pip install tensorflow_hub\n!pip install bert-for-tf2\n!pip install tensorflow\n!pip install sentencepiece\n!pip install transformers","e384fb12":"try:\n    %tensorflow_version 2.x\nexcept Exception:\n    pass\nimport tensorflow as tf\n\nimport tensorflow_hub as hub\nimport bert\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizer, TFBertModel, BertConfig\nfrom transformers import DistilBertModel,DistilBertTokenizer\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tensorflow.keras.layers import Dense, Input,Concatenate,concatenate\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras import backend as K\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import train_test_split\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c5a302cd":"BertTokenizer = bert.bert_tokenization.FullTokenizer\nbert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\", trainable=False)\nvocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\nto_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = BertTokenizer(vocabulary_file, to_lower_case)","003773bd":"import re\nimport emoji\nimport nltk\n#import stopwords corpus from nltk\nfrom nltk.corpus import stopwords\nimport string #load punctuation charachers\n\n\n#remove stopwords and punctuations\nstopwrds = set(stopwords.words('english'))\n\nTAG_RE = re.compile(r'<[^>]+>')\n\ndef remove_tags(text):\n    return TAG_RE.sub('', text)\n\ndef preprocess_text(sen):\n    #translate emojis \n    sentence = emoji.demojize(sen)\n    \n    #Remove URLs\n    sentence = re.sub(r\"http:\\S+\",'',sentence)\n    \n    #remove stop words\n    sentence = ' '.join([x for x in nltk.word_tokenize(sentence) if x not in stopwrds])\n    \n    # Removing html tags\n    sentence = remove_tags(sentence)\n\n    # Remove punctuations and numbers\n    #sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n    sentence = re.sub(r'['+string.punctuation+']','',sentence)\n\n    # Single character removal\n    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n\n    # Removing multiple spaces\n    sentence = re.sub(r'\\s+', ' ', sentence)\n\n    return sentence.strip()","e3428f68":"def tokenize_bert(data):\n    tokenized = data.apply((lambda x: tokenizer.convert_tokens_to_ids(['[CLS]']) + tokenizer.convert_tokens_to_ids(tokenizer.tokenize(x))))\n    return tokenized\ndef pad_mask(data_tokenized,max_len):\n    padded = tf.keras.preprocessing.sequence.pad_sequences(data_tokenized, maxlen=max_len, dtype='int32', padding='post',value=0.0)\n    masked = np.where(padded!=0,1,0)\n    return padded, masked\ndef get_max_len(data):\n    max_len = 0\n    for val in data:\n        tmp = len(tokenizer.tokenize(val))\n        if tmp > max_len:\n            max_len = tmp\n    return max_len","62dddc3e":"def summarize_model(history):\n    pyplot.subplot(211)\n    pyplot.title('Loss')\n    pyplot.plot(history.history['loss'], label='train')\n    pyplot.plot(history.history['val_loss'], label='test')\n    pyplot.legend()\n    # plot accuracy during training\n    pyplot.subplot(212)\n    pyplot.title('Accuracy')\n    pyplot.plot(history.history['accuracy'], label='train')\n    pyplot.plot(history.history['val_accuracy'], label='test')\n    pyplot.legend()\n    pyplot.show()","7b1c898f":"def encode(df):\n    tweet = tf.ragged.constant([tokenizer.convert_tokens_to_ids(tokenizer.tokenize(s)) for s in df])\n    cls1 = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*tweet.shape[0]\n    input_word_ids = tf.concat([cls1, tweet], axis=-1)\n    input_mask = tf.ones_like(input_word_ids).to_tensor()\n    type_cls = tf.zeros_like(cls1)\n    type_tweets = tf.zeros_like(tweet)\n    input_type_ids = tf.concat([type_cls, type_tweets], axis=-1).to_tensor()\n    \n    inputs = {\n      'input_ids': input_word_ids.to_tensor(),\n      'input_mask': input_mask,\n      'input_type_ids': input_type_ids}\n    return inputs","7ecf52e3":"finetune_train = pd.read_csv('\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv',encoding=\"utf-8\")\nfinetune_test = pd.read_csv('\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/test.csv',encoding=\"utf-8\")\n#extract hashtags\nfinetune_train[\"hashtags\"]=finetune_train[\"tweet\"].apply(lambda x:re.findall(r\"#(\\w+)\",x.lower()))\nfinetune_test[\"hashtags\"]=finetune_test[\"tweet\"].apply(lambda x:re.findall(r\"#(\\w+)\",x.lower()))\nfinetune_train[\"hashtags\"]=finetune_train[\"hashtags\"].apply(lambda x: ' '.join(x))\nfinetune_test[\"hashtags\"]=finetune_test[\"hashtags\"].apply(lambda x: ' '.join(x))\nfinetune_train = finetune_train.rename(columns={'label':'target'})\nfinetune_test = finetune_test.rename(columns={'label':'target'})\nfinetune_train.info()","cf2e683c":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\",encoding=\"utf-8\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\",encoding=\"utf-8\")\n#extract hashtags\ntrain[\"hashtags\"]=train[\"text\"].apply(lambda x:re.findall(r\"#(\\w+)\",x.lower()))\ntest[\"hashtags\"]=test[\"text\"].apply(lambda x:re.findall(r\"#(\\w+)\",x.lower()))\ntrain[\"hashtags\"]=train[\"hashtags\"].apply(lambda x: ' '.join(x))\ntest[\"hashtags\"]=test[\"hashtags\"].apply(lambda x: ' '.join(x))\ntrain.info()","d044a32d":"finetune_train[\"clean\"]  = finetune_train[\"tweet\"].apply(lambda x: preprocess_text(x.lower()) )\nfinetune_test[\"clean\"]  = finetune_test[\"tweet\"].apply(lambda x: preprocess_text(x.lower()) )\n\nprint(\"length of finetune train set:\",len(finetune_train))\nprint(\"length of finetune test set:\",len(finetune_test))","c785c60a":"train[\"clean\"]  = train[\"text\"].apply(lambda x: preprocess_text(x.lower()) )\ntest[\"clean\"]  = test[\"text\"].apply(lambda x: preprocess_text(x.lower()) )\n\nprint(\"length of train set:\",len(train))\nprint(\"length of test set:\",len(test))","f879a4a6":"def extract_features(df,test_df):\n    #Uigram Frequency distribution for disaster tweets\n    #convert disaster tweets into single string\n    txt=' '.join(df[df[\"target\"]==1][\"clean\"])\n    disaster_unigram=nltk.FreqDist(nltk.word_tokenize(txt))\n\n    #Uigram Frequency distribution for non disaster tweets\n    #convert non disaster tweets into single string\n    txt=' '.join(df[df[\"target\"]==0][\"clean\"])\n    nondisaster_unigram=nltk.FreqDist(nltk.word_tokenize(txt))\n\n    #Bigram Frequency distribution for disaster tweets\n    #convert disaster tweets into single string\n    txt=' '.join(df[df[\"target\"]==1][\"clean\"])\n    disaster_bigram=nltk.FreqDist(nltk.bigrams(nltk.word_tokenize(txt)))\n\n    #Bigram Frequency distribution for non disaster tweets\n    #convert non disaster tweets into single string\n    txt=' '.join(df[df[\"target\"]==0][\"clean\"])\n    nondisaster_bigram=nltk.FreqDist(nltk.bigrams(nltk.word_tokenize(txt)))\n\n    #Uigram Frequency distribution for disaster hashtags\n    #convert disaster hashtags into single string\n    txt=' '.join(df[df[\"target\"]==1][\"hashtags\"])\n    disaster_unigram_hash=nltk.FreqDist(nltk.word_tokenize(txt))\n\n    #Uigram Frequency distribution for non disaster hashtags\n    #convert non disaster hashtags into single string\n    txt=' '.join(df[df[\"target\"]==0][\"hashtags\"])\n    nondisaster_unigram_hash=nltk.FreqDist(nltk.word_tokenize(txt))\n\n    #compute unigram feature vector for tweet likelihood to disaster\n    df[\"unigram_disas\"]=df[\"clean\"].apply(lambda x: sum([disaster_unigram.get(wrd) for wrd in nltk.word_tokenize(x) if disaster_unigram.get(wrd)!=None])\/len(disaster_unigram))\n    test_df[\"unigram_disas\"]=test_df[\"clean\"].apply(lambda x: sum([disaster_unigram.get(wrd) for wrd in nltk.word_tokenize(x) if disaster_unigram.get(wrd)!=None])\/len(disaster_unigram))\n\n    #compute unigram feature vector for tweet likelihood to non disaster\n    df[\"unigram_nondisas\"]=df[\"clean\"].apply(lambda x: sum([nondisaster_unigram.get(wrd) for wrd in nltk.word_tokenize(x) if nondisaster_unigram.get(wrd)!=None])\/len(nondisaster_unigram))\n    test_df[\"unigram_nondisas\"]=test_df[\"clean\"].apply(lambda x: sum([nondisaster_unigram.get(wrd) for wrd in nltk.word_tokenize(x) if nondisaster_unigram.get(wrd)!=None])\/len(nondisaster_unigram))\n\n    #compute unigram feature vector for hashtags likelihood to disaster\n    df[\"unigram_disas_hash\"]=df[\"hashtags\"].apply(lambda x: sum([disaster_unigram_hash.get(wrd) for wrd in nltk.word_tokenize(x) if disaster_unigram_hash.get(wrd)!=None])\/len(disaster_unigram_hash))\n    test_df[\"unigram_disas_hash\"]=test_df[\"hashtags\"].apply(lambda x: sum([disaster_unigram_hash.get(wrd) for wrd in nltk.word_tokenize(x) if disaster_unigram_hash.get(wrd)!=None])\/len(disaster_unigram_hash))\n\n    #compute unigram feature vector for hashtags likelihood to non disaster\n    df[\"unigram_nondisas_hash\"]=df[\"hashtags\"].apply(lambda x: sum([nondisaster_unigram_hash.get(wrd) for wrd in nltk.word_tokenize(x) if nondisaster_unigram_hash.get(wrd)!=None])\/len(nondisaster_unigram_hash))\n    test_df[\"unigram_nondisas_hash\"]=test_df[\"hashtags\"].apply(lambda x: sum([nondisaster_unigram_hash.get(wrd) for wrd in nltk.word_tokenize(x) if nondisaster_unigram_hash.get(wrd)!=None])\/len(nondisaster_unigram_hash))\n\n    #compute bigram feature vector for tweet likelihood to disaster\n    df[\"bigram_disas\"]=df[\"clean\"].apply(lambda x: sum([disaster_bigram.get(wrd) for wrd in nltk.bigrams(nltk.word_tokenize(x)) if disaster_bigram.get(wrd)!=None])\/len(disaster_bigram) if x.strip()!='' else 0)\n    test_df[\"bigram_disas\"]=test_df[\"clean\"].apply(lambda x: sum([disaster_bigram.get(wrd) for wrd in nltk.bigrams(nltk.word_tokenize(x)) if disaster_bigram.get(wrd)!=None])\/len(disaster_bigram) if x.strip()!='' else 0)\n\n    #compute bigram feature vector for tweet likelihood to non disaster\n    df[\"bigram_nondisas\"]=df[\"clean\"].apply(lambda x: sum([nondisaster_bigram.get(wrd) for wrd in nltk.bigrams(nltk.word_tokenize(x)) if nondisaster_bigram.get(wrd)!=None])\/len(nondisaster_bigram) if x.strip()!='' else 0)\n    test_df[\"bigram_nondisas\"]=test_df[\"clean\"].apply(lambda x: sum([nondisaster_bigram.get(wrd) for wrd in nltk.bigrams(nltk.word_tokenize(x)) if nondisaster_bigram.get(wrd)!=None])\/len(nondisaster_bigram) if x.strip()!='' else 0)\n    \n    return df,test_df","e6c47467":"finetune_train,finetune_test = extract_features(finetune_train,finetune_test)\nfinetune_train.head(2)","392d6b5e":"train,test = extract_features(train,test)\ntrain.head(2)","a8bb7bb3":"def build_bert(max_len):\n    input_ids = keras.layers.Input(shape=(max_len,), name=\"input_ids\", dtype=tf.int32)\n    input_typ = keras.layers.Input(shape=(max_len,), name=\"input_type_ids\", dtype=tf.int32)\n    input_mask = keras.layers.Input(shape=(max_len,), name=\"input_mask\", dtype=tf.int32)\n    input_features = keras.layers.Input(shape=(6,), name=\"input_features\", dtype=tf.float32)\n    bert_inputs = {\"input_ids\": input_ids, \"input_mask\": input_mask,'input_type_ids':input_typ}\n    ## BERT encoder\n    bert_model = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\", trainable=True,name='keraslayer')\n    pooled_output, _ = bert_model([input_ids, input_mask,input_typ])\n    \n    merge_two = concatenate([pooled_output, input_features])\n    out =  keras.layers.Dense(1, activation='sigmoid')(merge_two)\n    model = keras.Model(inputs=[bert_inputs,input_features],outputs=out)\n    \n    return model","43e667e3":"all_df = pd.concat([finetune_train,finetune_test])\n#get max length of input data\nmax_len = get_max_len(all_df[\"clean\"]) + 1\n#encode and prepare data for Bert input\nencode_ds_all = encode(all_df[\"clean\"])","4758312c":"encode_ds_tr = {'input_ids':encode_ds_all[\"input_ids\"][0:31962,:],\n                'input_mask':encode_ds_all[\"input_mask\"][0:31962,:],\n                'input_type_ids':encode_ds_all[\"input_type_ids\"][0:31962,:]}","d011d111":"features = ['unigram_disas','unigram_nondisas','unigram_disas_hash','unigram_nondisas_hash','bigram_disas','bigram_nondisas']\nencode_features_tr = all_df[features].iloc[0:31962,:]","b3f7d569":"model = build_bert(max_len)\nmodel.summary()","6be9f173":"y_enc = finetune_train[\"target\"]\nloss = tf.keras.losses.BinaryCrossentropy (from_logits=False)\noptimizer = keras.optimizers.Adam(lr=1e-3,decay=1e-3\/64)\nmodel.compile(optimizer=optimizer, loss=[loss, loss],metrics=[\"accuracy\"])\ncheckpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True)\nearlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy')\nfine_history = model.fit([encode_ds_tr,encode_features_tr], y_enc, validation_split=0.34,shuffle=True,epochs=2,batch_size=64,verbose=1)","2d12cb85":"summarize_model(fine_history)","c281a727":"all_df = pd.concat([train,test])\n#get max length of input data\nmax_len = get_max_len(train[\"clean\"]) + 1\n#encode and prepare data for Bert input\nencode_ds_all = encode(all_df[\"clean\"])","d58f31b2":"encode_ds_tr = {'input_ids':encode_ds_all[\"input_ids\"][0:7613,:],\n                'input_mask':encode_ds_all[\"input_mask\"][0:7613,:],\n                'input_type_ids':encode_ds_all[\"input_type_ids\"][0:7613,:]}\nencode_ds_tr","0f288465":"encode_features_tr = all_df[features].iloc[0:7613,:]","5a0bb84a":"model = build_bert(max_len)\nmodel.summary()","8bdae121":"tf.keras.utils.plot_model(model, show_shapes=True, dpi=48)","f2c0375a":"y_enc = train[\"target\"]\nloss = tf.keras.losses.BinaryCrossentropy (from_logits=False)\noptimizer = keras.optimizers.Adam(lr=1e-5,decay=1e-5\/64)\nmodel.compile(optimizer=optimizer, loss=[loss, loss],metrics=[\"accuracy\"])\ncheckpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True)\nearlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy')\nfine_history = model.fit([encode_ds_tr,encode_features_tr], y_enc, validation_split=0.34,shuffle=True,epochs=3,batch_size=64,verbose=1)","4e1cea16":"summarize_model(fine_history)","12fabb28":"y_pred=model.predict([encode_ds_tr,encode_features_tr])\ny_pred = y_pred.round()\nprint(classification_report(y_enc,y_pred))","02519828":"encode_ds_ts = {'input_ids':encode_ds_all[\"input_ids\"][7613:,:],\n                'input_mask':encode_ds_all[\"input_mask\"][7613:,:],\n                'input_type_ids':encode_ds_all[\"input_type_ids\"][7613:,:]}\nencode_ds_ts","23f63def":"encode_features_ts = all_df[features].iloc[7613:,:]","8521cbbf":"#save in submission dataframe\ny_pred=model.predict([encode_ds_ts,encode_features_ts])\ny_pred= y_pred.round()\nsubmission=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\nsubmission['id']=test['id']\nsubmission['target']=y_pred\nsubmission['target']=submission['target'].astype(int)\nsubmission.head(10)\n","9ff64227":"submission.to_csv('sample_submission.csv',index=False)","0d1e38ba":"# Install Needed libraries","4cd160e1":"# Build Bert Model","6b0f358d":"# Adjust bert model ","8644ad66":"# FineTuning Bert using Sentiment Analysis Twitter Dataset","f8c6171f":"# Data Statistics","6cb8f013":"# Evaluation","fbbbe380":"# Load Data","4f696c92":"# Prepare Disaster Data for Bert ","beaa50a6":"# load Bert tokenizer","fb02d9bd":"# Build Model","0463d4f5":"Transfer learning is when a model developed for one task is reused to work on a second task. \nFine tuning is one approach to transfer learning.\n\nBERT (Bidirectional Encoder Representations from Transformers) is a big neural network architecture, with a huge number of parameters, that can range from 100 million to over 300 million. So, training a BERT model from scratch on a small dataset would result in overfitting.\n\nSo, it is better to use a pre-trained BERT model that was trained on a huge dataset, as a starting point. We can then further train the model on our relatively smaller dataset and this process is known as model fine-tuning.\n\u201cBERT stands for Bidirectional Encoder Representations from Transformers. It is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of NLP tasks.\u201d\nDifferent Fine-Tuning Techniques\nTrain the entire architecture \u2013 We can further train the entire pre-trained model on our dataset and feed the output to a softmax layer. In this case, the error is back-propagated through the entire architecture and the pre-trained weights of the model are updated based on the new dataset.\nTrain some layers while freezing others \u2013 Another way to use a pre-trained model is to train it partially. What we can do is keep the weights of initial layers of the model frozen while we retrain only the higher layers. We can try and test as to how many layers to be frozen and how many to be trained.\nFreeze the entire architecture \u2013 We can even freeze all the layers of the model and attach a few neural network layers of our own and train this new model. Note that the weights of only the attached layers will be updated during model training.\n","386d3ca9":"# Import Needed Libraries","513a23f3":"# Helper Functions"}}