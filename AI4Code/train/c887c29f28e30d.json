{"cell_type":{"3707087d":"code","b757c931":"code","efb8c663":"code","8cea1066":"code","d3204a50":"code","62a115f8":"code","5597ffc9":"code","10234edf":"code","c7ace25f":"code","5d6e5635":"code","d59c817f":"code","bffadeb7":"code","3d12d612":"code","0e533c98":"markdown","6cf477b8":"markdown","0b62ba3a":"markdown"},"source":{"3707087d":"import sys\nimport os\nsys.path.append('..\/input\/torchez\/')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nfrom transformers import logging\nlogging.set_verbosity(50)\n\nimport pandas as pd\nimport numpy as np\nimport transformers\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer, AdamW, AutoModelForTokenClassification\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport ast\nfrom tqdm import tqdm\nimport gc; gc.enable()\nimport torchez as ez","b757c931":"df = pd.read_csv('..\/input\/feedback-ner-train\/train_folds.csv')","efb8c663":"le = LabelEncoder()\nall_tags=[]\nfor i in tqdm(range(len(df))):\n    all_tags.extend(set(ast.literal_eval(df['entities'].values[i])))\n    \nunique_tags = list(set(all_tags))","8cea1066":"le.fit_transform(unique_tags)\ndict(zip(le.transform(le.classes_), le.classes_))","d3204a50":"class Config:\n    batch_size = 4\n    lr = 1e-5\n    max_len = 512\n    num_class = 15\n    weight_decay=0.01\n    model_name = '..\/input\/pt-longformer-base'\n    fold = 1\n    submission = True ## For submission ---> True For CV ---> False\n    cross_val = False if submission else True # Not need to change this","62a115f8":"LABEL_ALL_SUBTOKENS = True\n\nclass EntityDataset:\n    def __init__(self, dataframe, tokenizer, max_len, get_wids):\n        self.len = len(dataframe)\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.get_wids = get_wids # for validation\n\n    def __getitem__(self, index):\n        # GET TEXT AND WORD LABELS \n        text = self.data['text'].values[index]        \n        word_labels = ast.literal_eval(self.data['entities'].values[index]) if not self.get_wids else None\n\n        # TOKENIZE TEXT\n        encoding = self.tokenizer(text.split(),\n                             is_split_into_words=True,\n                             #return_offsets_mapping=True, \n                             padding='max_length', \n                             truncation=True, \n                             max_length=self.max_len)\n        word_ids = encoding.word_ids()  \n\n        # CREATE TARGETS\n        if not self.get_wids:\n            previous_word_idx = None\n            label_ids = []\n            for word_idx in word_ids:                            \n                if word_idx is None:\n                    label_ids.append(-100)\n                elif word_idx != previous_word_idx:              \n                    label_ids.append( le.transform([word_labels[word_idx]]) )\n                else:\n                    if LABEL_ALL_SUBTOKENS:\n                        label_ids.append( le.transform([word_labels[word_idx]]) )\n                    else:\n                        label_ids.append(-100)\n                previous_word_idx = word_idx\n            encoding['labels'] = label_ids\n\n        # CONVERT TO TORCH TENSORS\n        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n        if self.get_wids: \n            word_ids2 = [w if w is not None else -1 for w in word_ids]\n            item['wids'] = torch.as_tensor(word_ids2)\n\n        return item\n\n    def __len__(self):\n        return self.len","5597ffc9":"class FeedbackModel(ez.Model):\n    def __init__(self):\n        super(FeedbackModel, self).__init__()\n        self.config = AutoConfig.from_pretrained(args.model_name)\n        self.config.update({'output_hidden_states':False, 'return_dict':False})\n        self.config.num_labels = args.num_class\n        self.roberta = AutoModelForTokenClassification.from_pretrained(args.model_name, config=self.config)\n        \n    def forward(self, ids, mask, labels=None):\n        if labels is not None:\n            loss, logits = self.roberta(ids, attention_mask=mask, labels=labels)\n            return loss, logits\n        else:\n            logits = self.roberta(ids, attention_mask=mask)\n            return logits\n    \n    def prediction_step(self, input_ids, attention_mask, wids):\n        outputs = self(input_ids, attention_mask)\n\n        return outputs[0]\n    ","10234edf":"def get_predictions(preds, test_dataset, df, min_select=1):\n    predictions = []\n    for k,text_preds in tqdm(enumerate(preds)):\n        token_preds = le.inverse_transform(text_preds)\n        prediction = []\n        word_ids = test_dataset[k]['wids'].cpu().detach().numpy()\n        previous_word_idx = -1\n        for idx,word_idx in enumerate(word_ids):                            \n            if word_idx == -1:\n                pass\n            elif word_idx != previous_word_idx:              \n                prediction.append(token_preds[idx])\n                previous_word_idx = word_idx\n        predictions.append(prediction)\n    final_preds2 = []\n    \n    for i in range(len(df)):\n\n        idx = df.id.values[i]\n        pred = predictions[i] # Leave \"B\" and \"I\"\n        preds = []\n        j = 0\n        while j < len(pred):\n            cls = pred[j]\n            if cls == 'O': j += 1\n            else: cls = cls.replace('B','I') # spans start with B\n            end = j + 1\n            while end < len(pred) and pred[end] == cls:\n                end += 1\n\n            if cls != 'O' and cls != '' and end - j > min_select:\n                final_preds2.append((idx, cls.replace('I-',''),\n                                     ' '.join(map(str, list(range(j, end))))))\n\n            j = end\n\n    oof = pd.DataFrame(final_preds2)\n    oof.columns = ['id','class','predictionstring']\n    \n    gc.collect()\n    return oof","c7ace25f":"# from Rob Mulla @robikscube\n# https:\/\/www.kaggle.com\/robikscube\/student-writing-competition-twitch\ndef calc_overlap(row):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    set_pred = set(row.predictionstring_pred.split(' '))\n    set_gt = set(row.predictionstring_gt.split(' '))\n    # Length of each and intersection\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter \/ len_gt\n    overlap_2 = inter\/ len_pred\n    return [overlap_1, overlap_2]\n\n\ndef score_feedback_comp(pred_df, gt_df):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n        \n    Uses the steps in the evaluation page here:\n        https:\/\/www.kaggle.com\/c\/feedback-prize-2021\/overview\/evaluation\n    \"\"\"\n    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df = pred_df[['id','class','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df['pred_id'] = pred_df.index\n    gt_df['gt_id'] = gt_df.index\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(gt_df,\n                           left_on=['id','class'],\n                           right_on=['id','discourse_type'],\n                           how='outer',\n                           suffixes=('_pred','_gt')\n                          )\n    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n\n    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n\n\n    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n    tp_pred_ids = joined.query('potential_TP') \\\n        .sort_values('max_overlap', ascending=False) \\\n        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n\n    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    #calc microf1\n    my_f1_score = TP \/ (TP + 0.5*(FP+FN))\n    return my_f1_score","5d6e5635":"train_df = pd.read_csv('..\/input\/feedback-prize-2021\/train.csv')","d59c817f":"args= Config()\n\nif args.cross_val:\n    for f in range(2):\n        gc.collect()\n        model = FeedbackModel()\n        tokenizer = AutoTokenizer.from_pretrained(args.model_name, add_prefix_space=True)\n        args.fold = f\n        validdf = df[df['kfold']==args.fold]\n\n        validation_df = pd.DataFrame()\n        essays=[]\n        ids=[]\n        for idx in tqdm(validdf.id.unique()):\n            essays.append(open(f'..\/input\/feedback-prize-2021\/train\/{idx}.txt', 'r').read())\n            ids.append(idx)\n        validation_df['id'] = ids\n        validation_df['text'] = essays\n\n        test_dataset = EntityDataset(validation_df, tokenizer, args.max_len, get_wids=True)\n        model = FeedbackModel()\n        preds = model.predict(test_dataset, batch_size=128, device='cuda', model_path=f'..\/input\/feedback-rob-l\/model_f{args.fold}.bin')\n        preds = [np.argmax(preds[i], axis=1) for i in range(len(preds))]\n\n        print('='*50)\n        print(f'Fold : {args.fold}')\n        print('='*50)\n\n        # VALID TARGETS\n        valid = train_df.loc[train_df['id'].isin(validdf.id.values)]\n        # OOF PREDICTIONS\n        oof = get_predictions(preds, test_dataset, validation_df, min_select=6)\n\n        # COMPUTE F1 SCORE\n        f1s = []\n        CLASSES = oof['class'].unique()\n        print()\n        for c in CLASSES:\n            pred_df = oof.loc[oof['class']==c].copy()\n            gt_df = valid.loc[valid['discourse_type']==c].copy()\n            f1 = score_feedback_comp(pred_df, gt_df)\n            print(c,f1)\n            f1s.append(f1)\n\n        del model\n        gc.collect()\n        print()\n        print('Overall',np.mean(f1s))\n        print()","bffadeb7":"essays=[]\nIDS = []\nfor file in os.listdir('..\/input\/feedback-prize-2021\/test\/'):\n    essays.append(open(f'..\/input\/feedback-prize-2021\/test\/{file}', 'r').read())\n    IDS.append(file.split('.')[0])","3d12d612":"from IPython.display import display\nargs=Config()\ntokenizer = AutoTokenizer.from_pretrained(args.model_name, add_prefix_space=True)\n\nargs.submission=True\nif args.submission:\n    testdf = pd.DataFrame()\n\n    essays=[]\n    ids = []\n    for file in os.listdir('..\/input\/feedback-prize-2021\/test'):\n        essays.append(open(f'..\/input\/feedback-prize-2021\/test\/{file}', 'r').read())\n        ids.append(file.split('.')[0])\n\n    testdf['id'] = ids\n    testdf['text'] = essays\n    \n    test_dataset = EntityDataset(testdf, tokenizer, args.max_len, get_wids=True)\n    model = FeedbackModel()\n    preds=[]\n    for f in range(1):\n        args.fold=f\n        preds.append(np.array(model.predict(test_dataset, batch_size=128, device='cuda', model_path=f'..\/input\/pytorch-longformer-1024-train\/model_f1.bin')))\n    preds = sum(preds)\/1\n    preds = [np.argmax(preds[i], axis=1) for i in range(len(preds))]\n    preddf = get_predictions(preds, test_dataset, testdf, min_select=6)\n    preddf.to_csv('submission.csv', index=False)\n    display(preddf.head())","0e533c98":"# OOF","6cf477b8":"# Essays!! NER-dy Inference and Cross-Validation (KFolds)\n\nThis notebook is basically based on [this](https:\/\/www.kaggle.com\/cdeotte\/pytorch-bigbird-ner-cv-0-615) notebook by @cdeotte where he has used PyTorch and BigBird. \n\nThis code is just shorter and easier to debug and I have used **TorchEZ**, a PyTorch Wrapper to make the train, validation and prediction codes resuable, rather than writing them again and again.\n\nTorchEZ is still under development. Currently, I have not incorporated the use of schedulers (will update soon). The basic usage can be seen in my training notebook [here](https:\/\/www.kaggle.com\/kishalmandal\/essays-nerdy-indeed-train-kfolds).\n\nMoreover, I have tried to use the **folds** to get outputs. (Note: you cannot combine models with different sequence lengths)\n\nHere it is basically inferencing **Roberta-L 2folds**. (Will update since training the Longformer drained my GPU quota)\n\n**About the Cross-Validation** : The metric calculation has been taken from [this](https:\/\/www.kaggle.com\/robikscube\/student-writing-competition-twitch-stream) notebook by @robikscube \n\n***If you like it please do UPVOTE! :)***","0b62ba3a":"# Sub"}}