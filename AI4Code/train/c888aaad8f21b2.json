{"cell_type":{"0bb2ead1":"code","47a4016b":"code","1da3039a":"code","7f2de93f":"code","ff2a86a8":"code","f2c84a7b":"code","c785bfc5":"code","1af06469":"code","c672be88":"code","4cf6dc5b":"code","7fafc6be":"code","62f2a4a0":"code","99ca676f":"code","ccc8764a":"code","e6bc0836":"code","10fcb80d":"code","ca2831b7":"code","13f9a6c1":"code","c1687fb9":"code","32c5894c":"code","78292d45":"code","703e5ca4":"code","3e567760":"code","032790ca":"code","93ae4e76":"code","5881c59d":"code","cf031d62":"code","72327752":"code","02cec95a":"code","fd0ede97":"code","d475c779":"code","26cd529d":"code","0dd18c81":"code","31c16647":"code","af7743b7":"code","3f36add9":"code","f52f7b95":"code","4f748e13":"code","da152712":"code","d634e726":"code","91cf619b":"code","0a6a3987":"code","9c92a8aa":"code","8775eb89":"code","fbf5d58f":"code","55426b5c":"code","d9e43c08":"code","5766af03":"code","e1287cd3":"code","82ed3357":"code","bcab8e20":"code","6a76aad6":"code","38858186":"code","89840283":"code","c5350963":"markdown","5b6f1d9d":"markdown","5db96e6f":"markdown","3feefff5":"markdown","71eeaf84":"markdown","58cd30ac":"markdown","9bf17ef7":"markdown","8b8dca9e":"markdown","01f1fafe":"markdown","5ac861e0":"markdown","0a9c53a3":"markdown","c3dbf512":"markdown","4608f2ff":"markdown"},"source":{"0bb2ead1":"import sys\nimport re\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\npd.set_option('display.max_colwidth', -1)\npd.set_option('display.max_columns', None)\nsns.set_style('ticks', {'axes.grid': True, 'axes.spines.right': False, 'axes.spines.top': False})\nsns.set_palette(sns.color_palette(\"RdBu\", n_colors=21))\nwarnings.filterwarnings('ignore')\n\nfrom scipy.special import boxcox1p, inv_boxcox1p\nfrom scipy.stats import kruskal, skew, boxcox_normmax\n\nfrom sklearn.preprocessing import LabelEncoder, RobustScaler, PolynomialFeatures\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.model_selection import cross_val_score, KFold, GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet\nfrom sklearn.metrics import make_scorer, mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor, VotingRegressor\nfrom sklearn.pipeline import Pipeline\nfrom xgboost import XGBRegressor\nfrom vecstack import stacking","47a4016b":"# utility class for updating both train and test sets\nclass Dataset:\n    target = 'sale_price'\n    \n    def __init__(self):\n        _train = pd.read_csv('..\/input\/train.csv')\n        _train.columns = _train.columns.map(self.format_column_name)\n        \n        _test = pd.read_csv('..\/input\/test.csv')\n        _test.columns = _test.columns.map(self.format_column_name)\n        self.test_id = _test.id\n        \n        _train.drop('id', axis=1, inplace=True)\n        _test.drop('id', axis=1, inplace=True)\n        \n        self.update(_train, _test)\n\n    def __getitem__(self, key):\n        return self.dataset[key]\n    \n    def __setitem__(self, key, args):\n        if self.target in list(key):\n            raise\n        \n        if isinstance(args, tuple):\n            self.train[key] = self.train[key].pipe(*args)      \n            self.test[key] = self.test[key].pipe(*args)\n            self.update(self.train, self.test)\n        else:\n            self.dataset[key] = args\n            self.update()\n    \n    def __len__(self):\n        return len(self.dataset)\n    \n    def format_column_name(self, s):\n        s = re.findall('[A-Z, 0-9][a-z, 0-9]*', s)\n        s = '_'.join(s).lower()\n        return s\n    \n    def update(self, *args):\n        if len(args):\n            self.train, self.test = args\n            self.dataset = pd.concat([self.train, self.test], axis=0)\n        else:\n            self.train = self.dataset[:len(self.train)]\n            self.test = self.dataset[len(self.train):]\n        \n        for key in self.dataset.columns:\n            self.__dict__[key] = self.dataset[key]\n\n        global train, test\n        train = self.train\n        test = self.test\n\n\ndataset = Dataset()","1da3039a":"pd.DataFrame([train.columns, train.dtypes, train.isnull().sum(), train.nunique()])","7f2de93f":"pd.DataFrame([test.columns, test.dtypes, test.isnull().sum(), test.nunique()])","ff2a86a8":"# manual check for features\nordinals = ['exter_qual', 'exter_cond', 'bsmt_qual', 'bsmt_cond', 'heating_q_c',\n            'kitchen_qual', 'fireplace_qu', 'garage_qual', 'garage_cond', 'pool_q_c', \n            'overall_qual', 'overall_cond', 'm_s_sub_class']\nyears = ['year_built', 'year_remod_add', 'garage_yr_blt', 'yr_sold']\nnominals = list(set(train.select_dtypes('object').columns) - set(ordinals) - set(years)) + ['mo_sold']\ncontinuous = list(set(train.select_dtypes(['int64', 'float64']).columns) - set(ordinals) - set(years) - set(nominals))\ncontinuous.remove('sale_price')\n\nlen(ordinals) + len(years) + len(nominals) + len(continuous) + 1 == len(train.columns)","f2c84a7b":"# normalize sale_price\nfig, ax = plt.subplots(1, 2, figsize=(15, 5))\nsns.distplot(train.sale_price, ax=ax[0])\n\nboxcox_lambda = boxcox_normmax(train['sale_price'] + 1)\ntrain['sale_price'] = boxcox1p(train['sale_price'], boxcox_lambda)\nsns.distplot(train.sale_price, ax=ax[1])\n\ndataset.update(train, test)","c785bfc5":"data = dataset[ordinals].isnull().sum() \/ len(dataset)\ndata = data[data != 0].sort_values()\nplt.figure(figsize=(10, 5))\nsns.barplot(data.values, data.index, orient='h') \n# fill NA appropriately","1af06469":"for feat in ordinals:\n    print(f'{feat}: {sorted(train[feat].dropna().unique())}')","c672be88":"dataset['kitchen_qual'] = pd.Series.fillna, train.kitchen_qual.mode()[0]\ndataset[data.index] = pd.DataFrame.fillna, 'NA'\n\n# map\/encode ordinals\nmapping = {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\ndataset[ordinals[:-3]] = pd.DataFrame.applymap, lambda x: mapping.get(x)\ndataset[ordinals[-3:]] = pd.DataFrame.apply, LabelEncoder().fit_transform","4cf6dc5b":"fig, ax = plt.subplots(3, 5, figsize=(15, 10))\nfig.tight_layout()\nax = iter(ax.flatten())\nfor feat in ordinals:\n    data = train[[feat, 'sale_price']]\n    sns.boxplot(x=feat, y='sale_price', data=data, ax=next(ax))\n# the higher the quality, the higher the price","7fafc6be":"plt.figure(figsize=(15, 5))\nsns.heatmap(train[ordinals + ['sale_price']].corr(), annot=True)","62f2a4a0":"data = dataset[years].isnull().sum() \/ len(dataset)\ndata = data[data != 0].sort_values()\nprint(data)\n# fill NA appropriately","99ca676f":"# fill with min decade\ndataset['garage_yr_blt'] = pd.Series.fillna, 0","ccc8764a":"fig, ax = plt.subplots(2, 2, figsize=(30, 20))\nfig.tight_layout()\nax = iter(ax.flatten())\nfor feat in years:\n    sns.boxplot(x='sale_price', y=feat, orient='h', data=train, ax=next(ax))","e6bc0836":"fig, ax = plt.subplots(2, 2, figsize=(15, 5))\nfig.tight_layout()\nax = iter(ax.flatten())\nfor feat in years:\n    sns.lineplot(x=feat, y='sale_price', data=train[train[feat] != 0], ax=next(ax))","10fcb80d":"# try visualizing by decade\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\nfig.tight_layout()\nax = iter(ax.flatten())\nfor feat in years[:-1]:\n#     data = train.groupby(train[feat] \/\/ 10 * 10).agg({'sale_price': 'mean'}).reset_index()\n    data = train.copy(deep=True)\n    data[feat]= data[feat] \/\/ 10 * 10\n    sns.lineplot(x=feat, y='sale_price', data=data[data[feat] != 0], ax=next(ax))","ca2831b7":"fig, ax = plt.subplots(1, 3, figsize=(15, 5))\nfig.tight_layout()\nax = iter(ax.flatten())\nfor feat in years[:-1]:\n    data = train[[feat, 'sale_price']]\n    data[feat] = data[feat] \/\/ 10 * 10\n    sns.boxplot(x='sale_price', y=feat, orient='h', data=data, ax=next(ax))","13f9a6c1":"# bin years to decade\ndataset[years[:-1]] = pd.DataFrame.apply, lambda x: x \/\/ 10 * 10","c1687fb9":"data = dataset[continuous].isnull().sum() \/ len(dataset)\ndata = data[data != 0].sort_values()","32c5894c":"data = dataset[continuous].isnull().sum() \/ len(dataset)\ndata = data[data != 0].sort_values()\nplt.figure(figsize=(10, 5))\nsns.barplot(data.values, data.index, orient='h') \n# fill NA appropriately","78292d45":"dataset['lot_frontage'] = pd.Series.fillna, train.lot_frontage.median()\ndataset[data.index] = pd.DataFrame.fillna, 0","703e5ca4":"fig, ax = plt.subplots(6, 5, figsize=(15, 10))\nfig.tight_layout()\nax = iter(ax.flatten())\nfor feat in sorted(continuous):\n    sns.scatterplot(x=feat, y='sale_price', data=train, ax=next(ax))","3e567760":"plt.figure(figsize=(25, 10))\nsns.heatmap(train[continuous + ['sale_price']].corr(), annot=True)","032790ca":"data = dataset[nominals].isnull().sum() \/ len(dataset)\ndata = data[data != 0].sort_values()\nplt.figure(figsize=(10, 5))\nsns.barplot(data.values, data.index, orient='h')\n# fill NA appropriately","93ae4e76":"# fill train and test with train mode\nmode_fills = ['m_s_zoning', 'functional', 'utilities', 'exterior1st', 'electrical', 'exterior2nd', 'sale_type']\ndataset[mode_fills] = pd.DataFrame.fillna, train[mode_fills].mode().iloc[0]\n\n# fill the rest with NA\ndataset[data.index] = pd.DataFrame.fillna, 'NA'","5881c59d":"fig, ax = plt.subplots(7, 5, figsize=(15, 25))\nfig.tight_layout()\nax = iter(ax.flatten())\nfor feat in nominals:\n    data = train[[feat, 'sale_price']]\n    # sort by median for better visualization\n    labels = data.groupby(feat)['sale_price'].median().sort_values().index\n    sns.boxplot(x='sale_price', y=feat, orient='h', data=data, order=labels, ax=next(ax))","cf031d62":"dataset['total_sf'] = dataset.gr_liv_area + dataset.total_bsmt_s_f\ndataset['bsmt_comp'] = dataset.bsmt_qual * dataset.bsmt_cond * dataset.total_bsmt_s_f\ndataset['garage_comp'] = dataset.garage_cars * dataset.garage_qual * dataset.garage_cond\ndataset['total_bath'] = dataset.full_bath + dataset.bsmt_full_bath + dataset.half_bath + dataset.bsmt_half_bath\ndataset['total_porch_sf'] = dataset.open_porch_s_f + dataset['3_ssn_porch'] + dataset.enclosed_porch + dataset.screen_porch + dataset.wood_deck_s_f\ncontinuous.extend(['total_sf', 'bsmt_comp', 'garage_comp', 'total_bath', 'total_porch_sf'])","72327752":"def normalize(df, feats):\n    skewed = df[feats].apply(skew)\n    normalized = list()\n    for feat, skewness in skewed.items():\n        if skewness > 0.5:\n            n = boxcox1p(df[feat], boxcox_normmax(df[feat] + 1)).tolist()\n            normalized.append(n)\n        else:\n            normalized.append(df[feat].tolist())\n    return np.array(normalized).T\n\ndef get_outlier_indices(df, feats):\n    indices = list()\n    for feat in feats:\n        col = df[feat]\n        Q1 = np.percentile(col, 25)\n        Q3 = np.percentile(col, 75)\n        IQR = Q3 - Q1\n        step = IQR * 1.5\n        idx = col[(col < Q1 - step) | (col > Q3 + step)].index\n        indices.extend(idx)\n    return list(set(indices))\n\ndef get_kruskals(df, feats):\n    # non-parametric one way anova to measure association \n    # between nominal and continuous variables\n    stats = list()\n    target = 'sale_price'\n    for feat in feats:\n        groups = df.groupby(feat).agg({target: list})[target].tolist()\n        stat = kruskal(*groups)\n        stats.append([feat, stat.statistic, stat.pvalue])\n    stats = pd.DataFrame(stats, columns=['feat', 'stat', 'pvalue'])\n    return stats.sort_values('stat', ascending=False)","02cec95a":"# normalize\nnormalize_feats = continuous + ordinals\ndataset[normalize_feats] = normalize(dataset, normalize_feats)\n\n# drop outliers\noutlier_feats = ['total_sf']\ntrain = train[~train.index.isin(get_outlier_indices(train, outlier_feats))]\ndataset.update(train, test)","fd0ede97":"continuous = train[continuous + ['sale_price']].corr()['sale_price'].sort_values(ascending=False)\ncontinuous = continuous[continuous > 0.3].index.tolist()[1:]\nprint(continuous)","d475c779":"ordinals = train[ordinals + ['sale_price']].corr()['sale_price'].sort_values(ascending=False)\nordinals = ordinals[ordinals > 0.3].index.tolist()[1:]\nprint(ordinals)","26cd529d":"stats = get_kruskals(train, nominals)\nprint(stats[:10])\n\nnominals = stats.feat.tolist()[:10]\n\ndataset['has_bsmt'] = dataset['total_bsmt_s_f'] > 0\ndataset['has_garage'] = dataset['garage_area'] > 0\ndataset['has_fireplace'] = dataset['fireplaces'] > 0\nnominals.extend(['has_bsmt', 'has_garage', 'has_fireplace'])","0dd18c81":"# try to check if there's a trend over mo-yr sold, otherwise exclude feature\ndata =  train[['yr_sold', 'mo_sold', 'sale_price', 'overall_qual']]\ndata['yr_mo_sold'] = pd.to_datetime(train.yr_sold.astype(str) + '-' + train.mo_sold.astype(str)).dt.to_period('M')\ndata = data.sort_values('yr_mo_sold')\nplt.figure(figsize=(25, 10))\nsns.boxplot(x='sale_price', y='yr_mo_sold', orient='h', data=data)\n\nyears = years[:-1]","31c16647":"# check other feats\nsns.boxplot(x='lot_config', y='sale_price', hue='lot_shape', data=train)\nnominals.extend(['lot_config', 'lot_shape'])","af7743b7":"# label encode\ndataset[years] = pd.DataFrame.apply, LabelEncoder().fit_transform\ndataset[nominals] = pd.DataFrame.apply, LabelEncoder().fit_transform\n\n# onehot encode\nyears_encoded = pd.get_dummies(dataset[years].astype(object), drop_first=True)\nnominals_encoded = pd.get_dummies(dataset[nominals].astype(object), drop_first=True)","3f36add9":"dataset_preprocessed = pd.concat([\n    dataset[continuous], dataset[ordinals],\n    nominals_encoded, years_encoded\n], axis=1)","f52f7b95":"dataset_preprocessed.shape","4f748e13":"X_train = dataset_preprocessed[:len(train)]\ny_train = train.sale_price\n\nX_test = dataset_preprocessed[len(train):]","da152712":"kfold = KFold(10, shuffle=True)","d634e726":"rmse_scorer = make_scorer(lambda y, y0: np.sqrt(mean_squared_error(y, y0)), greater_is_better=False)","91cf619b":"regressors = [\n    Ridge(random_state=0),\n    Pipeline([('poly', PolynomialFeatures()), ('lasso', Lasso(random_state=0))]),\n    Pipeline([('poly', PolynomialFeatures()), ('el', ElasticNet(random_state=0))]),\n    RandomForestRegressor(random_state=0),\n    ExtraTreesRegressor(random_state=0),\n    AdaBoostRegressor(random_state=0),\n    GradientBoostingRegressor(random_state=0),\n    XGBRegressor(random_state=0)\n]\n\nresults = list()\nfor reg in regressors:\n    if isinstance(reg, Pipeline):\n        name = reg.steps[1][1]\n    else:\n        name = reg\n    name = type(name).__name__\n    print(f'Validating {name}..')\n    cv = cross_val_score(reg, X_train, y_train, scoring=rmse_scorer, n_jobs=-1, cv=kfold)\n    results.append([name, cv])\n\nresults = pd.DataFrame(results, columns=['model', 'cv'])\nresults = results.set_index('model')['cv'].apply(pd.Series)\nresults['mean'] = results.mean(axis=1)\nresults['std'] = results.std(axis=1)\nresults","0a6a3987":"data = results.unstack().drop(['mean', 'std']).reset_index().rename(columns={0: 'score'})\nfig, ax = plt.subplots(2, 1, figsize=(10, 5))\nfig.tight_layout()\nax = iter(ax.flatten())\nsns.lineplot(x='level_0', y='score', hue='model', data=data, ax=next(ax))\nsns.boxplot(x='score', y='model', orient='h', data=data, ax=next(ax))","9c92a8aa":"ridge_params = {\n    'alpha': [30, 20, 10, 5, 1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-8, 1e-10, 1e-15]\n}\n\nlasso_params = {\n    'lasso__alpha': [30, 20, 10, 5, 1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-8, 1e-10, 1e-15]\n}\n\nel_params = {\n    'el__alpha': [30, 20, 10, 5, 1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-8, 1e-10, 1e-15]\n}\n\nrf_params = {\n    'max_depth': [5, 10, 15],\n    'min_samples_split': [2, 5, 10],\n    'n_estimators': [50, 100, 200, 250, 300]\n}\n\net_params = {\n    'max_depth': [5, 10, 15],\n    'min_samples_split': [2, 5, 10],\n    'n_estimators': [50, 100, 200, 250, 300]\n}\n\ngb_params = {\n    'learning_rate': [0.01, 0.03, 0.05, 0.09, 0.1, 0.2],\n    'max_depth': [3, 4, 5, 6],\n    'min_samples_split': [2, 5, 10],\n    'n_estimators': [100, 200, 300, 400, 500]\n}\n\nxgb_params = {\n    'learning_rate': [0.01, 0.03, 0.05, 0.09, 0.1, 0.2],\n    'max_depth': [3, 4, 5, 6],\n    'min_child_weight': [1, 3, 5, 7],\n    'gamma': [0.0, 0.1, 0.2 , 0.3, 0.4],\n    'colsample_bytree': [0.4, 0.5 , 0.7],\n    'n_estimators': [100, 200, 300, 400, 500]\n}\n\nparams = [ridge_params, lasso_params, el_params, rf_params, et_params, gb_params, xgb_params]","8775eb89":"results = list()\nfor reg, param in zip(regressors[:-3], params[:-2]):\n    if isinstance(reg, Pipeline):\n        name = reg.steps[1][1]\n    else:\n        name = reg\n    name = type(name).__name__\n    print(f'Grid search {name}..')\n    reg = GridSearchCV(reg, param, rmse_scorer, n_jobs=-1, cv=kfold, verbose=3)\n    reg.fit(X_train, y_train)\n    results.append(reg)\n\nfor reg, param in zip(regressors[-2:], params[-2:]):\n    name = type(reg).__name__\n    print(f'Grid search {name}..')\n    reg = RandomizedSearchCV(reg, param_distributions=param, scoring=rmse_scorer, n_iter=100, n_jobs=-1, cv=kfold, verbose=3)\n    reg.fit(X_train, y_train)\n    results.append(reg)","fbf5d58f":"preds = list()\nfor cv in results:\n    reg = cv.best_estimator_\n    reg.fit(X_train, y_train)\n    pred = inv_boxcox1p(reg.predict(X_train), boxcox_lambda)\n    preds.append(pred)\n    \nfig, ax = plt.subplots(4, 2, figsize=(15, 15))\nfig.tight_layout()\nax = iter(ax.flatten())\nfor pred in preds:\n    sns.regplot(x=inv_boxcox1p(y_train, boxcox_lambda), y=pred, ax=next(ax))","55426b5c":"fis = list()\nfor cv in results[3:]:\n    fi = cv.best_estimator_.feature_importances_\n    fis.append(fi)\n\nfis = pd.DataFrame(fis, columns=X_train.columns)\nfis.index = [type(r.best_estimator_).__name__ for r in results[3:]]\nfis = fis.unstack().reset_index()\nfis = fis.sort_values(['level_1', 0], ascending=[False, False]).groupby('level_1', sort=False).head(5)\n\nplt.figure(figsize=(10, 5))\nsns.barplot(x=0, y='level_0', hue='level_1', orient='h', data=fis)","d9e43c08":"names = ['ridge', 'lasso', 'el', 'rf', 'et', 'gb', 'xgb']\nmodels = dict(zip(names, results))","5766af03":"S_train_1, S_test_1 = stacking(\n    [models['rf'].best_estimator_, models['xgb'].best_estimator_, models['el'].best_estimator_], \n    X_train, y_train, X_test, regression=True, n_folds=10, shuffle=True, random_state=0, verbose=2\n)\n\nS_train_2, S_test_2 = stacking(\n    [models['gb'].best_estimator_, models['et'].best_estimator_, models['ridge'].best_estimator_, models['lasso'].best_estimator_],\n    X_train, y_train, X_test, regression=True, n_folds=10, shuffle=True, random_state=0, verbose=2\n)","e1287cd3":"stack_reg_1 = GridSearchCV(Ridge(random_state=0), ridge_params, rmse_scorer, n_jobs=-1, cv=kfold, verbose=3)\nstack_reg_1.fit(S_train_1, y_train)\n\nstack_reg_1 = stack_reg_1.best_estimator_.fit(S_train_1, y_train)\nstack_1_pred = stack_reg_1.predict(S_test_1)","82ed3357":"stack_reg_2 = RandomizedSearchCV(XGBRegressor(random_state=0), param_distributions=xgb_params, scoring=rmse_scorer, n_iter=100, n_jobs=-1, cv=kfold, verbose=3)\nstack_reg_2.fit(S_train_2, y_train)\n\nstack_reg_2 = stack_reg_2.best_estimator_.fit(S_train_2, y_train)\nstack_2_pred = stack_reg_2.predict(S_test_2)","bcab8e20":"blended_pred = (\n    (0.6 * inv_boxcox1p(stack_1_pred, boxcox_lambda)) +\n    (0.4 * inv_boxcox1p(stack_2_pred, boxcox_lambda))\n)","6a76aad6":"# np.random.seed(42)\n# weights =x np.random.randint(0, 1000, (50, 4)) # generate 100 weights\n# weights = [\n#     [round(w, 2) for w in weight \/ weight.sum()]\n#     for weight in weights\n# ]\n# params = {'weights': weights}\n# voting_reg = GridSearchCV(VotingRegressor(estimators), params, rmse_scorer, n_jobs=-1, verbose=3)\n# voting_reg.fit(X_train, y_train)\n\n# print(voting_reg.best_score_)\n# print(voting_reg.best_params_)\n\n# voting_reg = voting_reg.best_estimator_\n# voting_reg.fit(X_train, y_train)\n# y_pred = inv_boxcox1p(voting_reg.predict(X_test), boxcox_lambda)","38858186":"submit = pd.DataFrame({\n    'Id': dataset.test_id,\n    'SalePrice': blended_pred\n})","89840283":"from IPython.display import FileLink\nsubmit.to_csv('house_prices_submit.csv', index=False)\nFileLink('house_prices_submit.csv')","c5350963":"### Years","5b6f1d9d":"# Modeling","5db96e6f":"### Grid Search","3feefff5":"### Nominal","71eeaf84":"### Feature Engg \/ Feature Selection \/ Preprocessing","58cd30ac":"### Target","9bf17ef7":"### Continuous","8b8dca9e":"# Init ","01f1fafe":"# EDA \/ Cleaning","5ac861e0":"### Preprocess","0a9c53a3":"### Model Selection","c3dbf512":"### Stack Blending","4608f2ff":"### Ordinal"}}