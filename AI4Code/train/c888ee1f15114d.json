{"cell_type":{"4d351b17":"code","7a1a66e6":"code","8c37a833":"code","5c1943a1":"code","d2e64387":"code","199df32b":"code","ccd41cc8":"code","535f0ee3":"code","56537349":"code","961e57d4":"code","83977a7e":"code","9f6cb373":"code","e9604224":"code","eeb021a5":"code","1555c68d":"code","2d96a011":"code","257e08bb":"code","d9ca8b39":"code","b9fa435e":"code","166d0b08":"code","bf25f439":"code","6e0036ed":"code","ce7c62f2":"code","8c06a3cd":"code","9ea90b2a":"code","744443b6":"code","7fc8d764":"code","8bfbce5d":"code","624414b4":"code","8ac94be4":"code","54a59580":"code","05265d35":"code","479c9640":"code","03d2c6ac":"code","9cadd548":"code","ba4ab034":"code","73119373":"code","97c899f9":"code","f3b33c6d":"code","5871c3e6":"code","a107225c":"code","1f1c4063":"code","08895175":"code","88c184f2":"code","977421e9":"markdown","d1b32d89":"markdown","0a95af5c":"markdown","0813a954":"markdown","84d7b29c":"markdown","98ed0f41":"markdown","3170d22b":"markdown","6c15b68b":"markdown","61e92cba":"markdown","9c9bf9e1":"markdown","16a9aae2":"markdown","a33ca220":"markdown","b7372a92":"markdown","fe265a08":"markdown","2af75596":"markdown","1765ba24":"markdown"},"source":{"4d351b17":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7a1a66e6":"df = pd.read_csv('..\/input\/telecom-churn\/telecom_churn.csv')","8c37a833":"df.columns","5c1943a1":"df.isnull().any()","d2e64387":"df.describe() #\u0e14\u0e39\u0e20\u0e32\u0e1e\u0e23\u0e27\u0e21\u0e02\u0e2d\u0e07\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e2b\u0e32\u0e04\u0e27\u0e32\u0e21\u0e1c\u0e34\u0e14\u0e1b\u0e01\u0e15\u0e34","199df32b":"y = df['Churn'].values \nX = df.drop(['Churn'],axis = 1).values","ccd41cc8":"len(y)","535f0ee3":"y.sum() #\u0e04\u0e48\u0e32\u0e17\u0e35\u0e48 label \u0e40\u0e1b\u0e47\u0e19 0","56537349":"len(y) - y.sum() #\u0e04\u0e48\u0e32\u0e17\u0e35\u0e48 label \u0e40\u0e1b\u0e47\u0e19 0","961e57d4":"from sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0)","83977a7e":"X_train.shape","9f6cb373":"X_test.shape","e9604224":"y_train.shape","eeb021a5":"y_test.shape","1555c68d":"y_train.sum() #label = 1","2d96a011":"len(y_train) - y_train.sum() #label = 0","257e08bb":"from sklearn.preprocessing import MinMaxScaler #normalization\nfrom sklearn.ensemble import RandomForestClassifier #classifier\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.pipeline import Pipeline\n\n#from imblearn.under_sampling import RandomUnderSampler #imbalanced\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\n#Imbalanced\n#undersample = RandomUnderSampler(sampling_strategy='majority') \n#X_res, y_res = undersample.fit_resample(X_train, y_train)\n\n#Pipeline\nclf = Pipeline([ \n    ('scaler',MinMaxScaler(feature_range=(0,1))), #normalization\n    ('feature_selection',SelectKBest(f_classif)), #select feature\n    ('classification',RandomForestClassifier(random_state=0))#classifier\n])\n\n\n#Tune GridSearchCV\nparams = { \n    'feature_selection__k':[3,5,7],\n    'classification__n_estimators': [10,20,50,100,200],\n}\nbest_clf = GridSearchCV(clf, params, cv=10)\n\nbest_clf.fit(X_train, y_train)","d9ca8b39":"acc = best_clf.best_score_\nprint(\"10CV accuracy: \"+str(acc))","b9fa435e":"best_clf.best_params_","166d0b08":"yp = best_clf.predict(X_test)\nacc = sum(yp == y_test)\/len(y_test)\nprint(\"Test Training accuracy: \"+str(acc))","bf25f439":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ntarget_names = ['negative(0)', 'positive(1)']\nC = confusion_matrix(y_test,yp) \nC = C \/ C.astype(np.float).sum(axis=1)*100\nsns.heatmap(C, annot=True, fmt=\"f\",cmap=\"GnBu\",xticklabels=target_names, yticklabels=target_names)\nplt.ylabel(\"True Label\")\nplt.xlabel(\"Predicted Label\")\nplt.show()","6e0036ed":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,yp,target_names=target_names)) ","ce7c62f2":"from sklearn.preprocessing import MinMaxScaler #normalization\nfrom sklearn.svm import SVC #classifier\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.pipeline import Pipeline\n\n#from imblearn.under_sampling import RandomUnderSampler #imbalanced\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\n#Imbalanced\n#undersample = RandomUnderSampler(sampling_strategy='majority') \n#X_res, y_res = undersample.fit_resample(X_train, y_train)\n\n#Pipeline\nclf = Pipeline([ \n    ('scaler',MinMaxScaler(feature_range=(0,1))), #normalization\n    ('feature_selection',SelectKBest(f_classif)), #select feature\n    ('classification',SVC(random_state=0))#classifier\n])\n\n\n#Tune GridSearchCV\nparams = { \n    'feature_selection__k':[3,5,7],\n    'classification__C': [1,2,4,8,16,32],\n    'classification__gamma' :[0.0625,0.0125,0.025,0.05,0.01,0,1,2,4,8,16,32]\n}\nbest_clf = GridSearchCV(clf, params, cv=10)\n\nbest_clf.fit(X_train, y_train)","8c06a3cd":"acc = best_clf.best_score_\nprint(\"10CV accuracy: \"+str(acc))","9ea90b2a":"best_clf.best_params_","744443b6":"yp = best_clf.predict(X_test)\nacc = sum(yp == y_test)\/len(y_test)\nprint(\"Test Training accuracy: \"+str(acc))","7fc8d764":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ntarget_names = ['negative(0)', 'positive(1)']\nC = confusion_matrix(y_test,yp) \nC = C \/ C.astype(np.float).sum(axis=1)*100\nsns.heatmap(C, annot=True, fmt=\"f\",cmap=\"GnBu\",xticklabels=target_names, yticklabels=target_names)\nplt.ylabel(\"True Label\")\nplt.xlabel(\"Predicted Label\")\nplt.show()","8bfbce5d":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,yp,target_names=target_names)) ","624414b4":"from sklearn.preprocessing import MinMaxScaler #normalization\nfrom sklearn.ensemble import ExtraTreesClassifier #classifier\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.pipeline import Pipeline\n\n#from imblearn.under_sampling import RandomUnderSampler #imbalanced\n\n#Select feature\n#from sklearn.feature_selection import SelectKBest\n#from sklearn.feature_selection import f_classif\n\n#Imbalanced\n#undersample = RandomUnderSampler(sampling_strategy='majority') \n#X_res, y_res = undersample.fit_resample(X_train, y_train)\n\n#Pipeline\nclf = Pipeline([ \n    ('scaler',MinMaxScaler(feature_range=(0,1))), #normalization\n    #('feature_selection',SelectKBest(f_classif)), #select feature\n    ('classification',ExtraTreesClassifier(random_state=0))#classifier\n])\n\n\n#Tune GridSearchCV\nparams = { \n    #'feature_selection__k':[3,5,7],\n    'classification__n_estimators': [10,20,50,100,200]\n}\nbest_clf = GridSearchCV(clf, params, cv=10)\n\nbest_clf.fit(X_train, y_train)","8ac94be4":"acc = best_clf.best_score_\nprint(\"10CV accuracy: \"+str(acc))","54a59580":"best_clf.best_params_","05265d35":"yp = best_clf.predict(X_test)\nacc = sum(yp == y_test)\/len(y_test)\nprint(\"Test Training accuracy: \"+str(acc))","479c9640":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ntarget_names = ['negative(0)', 'positive(1)']\nC = confusion_matrix(y_test,yp) \nC = C \/ C.astype(np.float).sum(axis=1)*100\nsns.heatmap(C, annot=True, fmt=\"f\",cmap=\"GnBu\",xticklabels=target_names, yticklabels=target_names)\nplt.ylabel(\"True Label\")\nplt.xlabel(\"Predicted Label\")\nplt.show()","03d2c6ac":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,yp,target_names=target_names)) ","9cadd548":"best_clf.best_estimator_.steps[1][1].feature_importances_","ba4ab034":"from imblearn.over_sampling import SMOTE #imbalanced\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler #normalization\nfrom sklearn.feature_selection import SelectKBest #select feature\nfrom sklearn.feature_selection import f_classif #select feature\nfrom sklearn.ensemble import RandomForestClassifier #classifier\nfrom sklearn.model_selection import GridSearchCV \n\n#Imbalanced\nsm = SMOTE(random_state=1)\nX_res, y_res = sm.fit_resample(X_train, y_train)\n\n#Pipeline\nclf = Pipeline([ \n    ('scaler',MinMaxScaler(feature_range=(0,1))), #normalization\n    ('feature_selection',SelectKBest(f_classif)), #select feature\n    ('classification',RandomForestClassifier(random_state=0)) #classifier \n])\n\n#Tune GridSearchCV\nparams = { \n    'feature_selection__k':[3,5,7,10],\n    'classification__n_estimators': [10,100,1000],\n    'classification__max_depth' : [1,2,3,4,5,6,7,8,8,9,10,11,12,13,14,15,16,17,18,19,20]\n}\nbest_clf = GridSearchCV(clf, params, cv=10)\n\nbest_clf.fit(X_res, y_res)","73119373":"best_clf.best_params_","97c899f9":"acc = best_clf.best_score_\nprint(\"10CV accuracy: \"+str(acc))","f3b33c6d":"yp = best_clf.predict(X_test)\nacc = sum(yp == y_test)\/len(y_test)\nprint(\"Test Training accuracy: \"+str(acc))","5871c3e6":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ntarget_names = ['negative(0)', 'positive(1)']\nC = confusion_matrix(y_test,yp) \nC = C \/ C.astype(np.float).sum(axis=1)*100\nsns.heatmap(C, annot=True, fmt=\"f\",cmap=\"GnBu\",xticklabels=target_names, yticklabels=target_names)\nplt.ylabel(\"True Label\")\nplt.xlabel(\"Predicted Label\")\nplt.show()","a107225c":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,yp,target_names=target_names)) ","1f1c4063":"f = best_clf.best_estimator_.steps[-1][1].feature_importances_","08895175":"f_idx = np.argsort(-f)\nf_idx","88c184f2":"df.columns.values[f_idx]","977421e9":"**Training**","d1b32d89":"# **Training**","0a95af5c":"**Testing**","0813a954":"**Preproessing**","84d7b29c":"___________________________________________________________________________________","98ed0f41":"**ExtraTrees**","3170d22b":"**Feature Selected**","6c15b68b":"**RandomForest**","61e92cba":"1 \u0e04\u0e37\u0e2d\u0e25\u0e39\u0e01\u0e04\u0e49\u0e32\u0e22\u0e49\u0e32\u0e22\u0e04\u0e48\u0e32\u0e22 0 \u0e04\u0e37\u0e2d\u0e25\u0e39\u0e01\u0e04\u0e49\u0e32\u0e2d\u0e22\u0e39\u0e48\u0e1a\u0e23\u0e34\u0e01\u0e32\u0e23\u0e40\u0e14\u0e34\u0e21","9c9bf9e1":"# **Telecom-churn**","16a9aae2":"____________________________________________________________________________________","a33ca220":"**SVC**","b7372a92":"_________________________________________________________________________________","fe265a08":"# **ASSIGNMENT**","2af75596":"\u0e40\u0e0a\u0e47\u0e04\u0e08\u0e32\u0e01 mean min max \u0e41\u0e25\u0e30 percentile \u0e08\u0e30\u0e40\u0e2b\u0e47\u0e19\u0e27\u0e48\u0e32\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e1a\u0e49\u0e02\u0e27\u0e32\u0e21\u0e32\u0e01\u0e46 \/ \u0e16\u0e49\u0e32\u0e04\u0e48\u0e32 std \u0e40\u0e1b\u0e47\u0e19 0 \u0e41\u0e2a\u0e14\u0e07\u0e27\u0e48\u0e32\u0e44\u0e21\u0e48\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e41\u0e1b\u0e23\u0e1b\u0e23\u0e27\u0e19\u0e40\u0e25\u0e22 = \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e2b\u0e21\u0e37\u0e2d\u0e19\u0e01\u0e31\u0e19\u0e2b\u0e21\u0e14","1765ba24":"**Cleansing**"}}